{"cell_type":{"75ec4c7a":"code","1203ebc8":"code","1dca6b97":"code","eb0ea704":"code","ca55f0ee":"code","b0c05f8c":"code","5ce0652b":"code","7576c23c":"code","a335986f":"code","009013d6":"code","a63ee354":"code","e6054a25":"code","a10144b5":"code","e820365f":"code","574fdbfc":"code","b4ad87f6":"code","89af5418":"code","7762a577":"code","846dcd92":"code","9bb55109":"code","58c5823b":"code","d233ce0a":"code","23bd1b9e":"code","174bf544":"markdown","974f9f06":"markdown","bcdbaf72":"markdown","dd05151a":"markdown","78755ecc":"markdown","a37cac48":"markdown","d0eac842":"markdown","810d8c1e":"markdown","ba16b559":"markdown","1bf19bcf":"markdown","d0980f99":"markdown","6348f844":"markdown","8cd6eb99":"markdown","7eddbe5e":"markdown","e3935329":"markdown","1d7beab7":"markdown","f84dd336":"markdown","569176b0":"markdown","aede8da6":"markdown","9db91a83":"markdown","c2712cf6":"markdown"},"source":{"75ec4c7a":"import numpy as np \nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom plotly.offline import init_notebook_mode, iplot\ninit_notebook_mode()\nimport plotly.graph_objs as go\nfrom sklearn.preprocessing import StandardScaler\nimport tensorflow as tf\nfrom sklearn.decomposition import PCA\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Activation,Dropout, BatchNormalization\nfrom sklearn.model_selection import KFold,StratifiedKFold\nimport lightgbm as lgb\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.model_selection import  train_test_split\nfrom keras import backend as K\nfrom keras import optimizers\nimport keras as k\nimport time\nfrom keras.callbacks import EarlyStopping\nfrom sklearn.utils import class_weight\nimport warnings\nwarnings.filterwarnings('ignore')","1203ebc8":"sampledf = True #Uses a specific amount of rows , use this for faster training and testing functionalities and features\nfrac_sample = 0.1 #fraction of the data to use\naugmnt = False #use an augmented data set\nfold_train = True\nagmnt_between = False #augment training data between folds\nkfold_shuffle = False\nuse_perc = False #using percentiles in feature engineering\nlog_transf = False\nsq_data = True\ndim_red = False\nn_components = 60\nstandradize=True\ntrain_between = True\n#--------------------------------------------------------------\n#Keras options\n#Weighted Classes when training \nweighted = False\nbalanced = False #balanced weights\n#-------------------\n#train test Split\ntst_size = 0.2\n\nsub_name = 'submission'\nprint('Options Active: \\n\\t SampledDF: {} frac: {} \\n\\t Augmentation: {}\\n\\t Weighted: {}\\n\\t Balanced: {}\\\n      \\n\\t agmnt_between:{}\\n\\t Percentiles: {}\\n\\t LOG_Transform: {}\\n\\t PCA: {}\\n\\t Square: {}\\n\\t Standradize: {}\\n\\t Train_Between: {}'.format(sampledf,frac_sample,augmnt,\n                                                                                                                             weighted,balanced,agmnt_between,use_perc,log_transf\n                                                                                                                            ,dim_red,sq_data,standradize,train_between))","1dca6b97":"%time\ndf_t = pd.read_csv('..\/input\/train.csv')\ndf_tst = pd.read_csv('..\/input\/test.csv')\n\nif sampledf:\n    sub_name = sub_name+'_sampled'\n    df_train = df_t.sample(frac=(frac_sample))\n    df_test = df_tst.sample(frac=(frac_sample))\n    print('Loading Sampled df..')\nelse:\n    df_train = df_t.copy()\n    df_test = df_tst.copy()\n\nprint('Training df shape',df_train.shape)\nprint('Test df shape',df_test.shape)","eb0ea704":"df_train.head()","ca55f0ee":"if standradize:\n    print('Standradizing the data..')\n    #inf values can result from squaring\n    scaler = StandardScaler()\n    df_train.iloc[:,2:] = scaler.fit_transform(df_train.iloc[:,2:])\n    df_test.iloc[:,1:] = scaler.fit_transform(df_test.iloc[:,1:])\n    print('Data Standradized!')","b0c05f8c":"df_train.head()","5ce0652b":"def vis_classes(labels,values,title='Target Percentages'):\n    trace=go.Pie(labels=labels,values=values)\n    layout = go.Layout(\n        title=title,\n        height=600,\n        margin=go.Margin(l=0, r=200, b=100, t=100, pad=4)   # Margins - Left, Right, Top Bottom, Padding\n        )\n    fig = go.Figure(data=[trace], layout=layout)\n    iplot(fig)\n    \n    \nlabels = [str(x) for x in list(df_train['target'].unique())]\nvalues = [(len(df_train[df_train['target'] == 0])\/len(df_train))*100,(len(df_train[df_train['target'] > 0])\/len(df_train))*100]    \nvis_classes(labels,values)","7576c23c":"labels","a335986f":"values","009013d6":"df_ones = df_train[df_train['target'] > 0]\nprint('Ones',df_ones.shape)\ndf_zeros = df_train[df_train['target'] == 0].sample(frac=0.25)\nprint('Zeros',df_zeros.shape)\n#we concat both to the sampling dataframe\ndf_sampling = pd.concat([df_ones, df_zeros]).sample(frac=1) #shuffling\nprint(df_sampling.shape)","a63ee354":"df_sampling","e6054a25":"#part of it Inspired by Gabriel Preda 's Kernel'\n\n\n\n# Notice axis = 1 here. that means we are taking sum , min , max , etc. of each ROW , now column \n# percentile can be used and is optional (ex. 50th percentile would be the median )\ndef feature_creation(df,idx,use_perc,perc_list,name_num='_1'):\n    #data metrics\n    print('  * Loading new data metrics: ', name_num)\n    df['sum'+name_num] = df[idx].sum(axis=1)  \n    df['min'+name_num] = df[idx].min(axis=1)\n    df['max'+name_num] = df[idx].max(axis=1)\n    df['mean'+name_num] = df[idx].mean(axis=1)\n    df['std'+name_num] = df[idx].std(axis=1)\n    df['skew'+name_num] = df[idx].skew(axis=1)\n    df['kurt'+name_num] = df[idx].kurtosis(axis=1)\n    df['med'+name_num] = df[idx].median(axis=1)\n    #moving average\n    print('  * Loading moving average metric: ', name_num)\n    df['ma'+name_num] =  df[idx].apply(lambda x: np.ma.average(x), axis=1)\n    #percentiles\n    if use_perc:\n        print('  * Loading percentiles: ', name_num)\n        for i in perc_list:\n            df['perc_'+str(i)] =  df[idx].apply(lambda x: np.percentile(x, i), axis=1)\n\n    #interactions\n    #coming..\n\n   \nperc_size  = 0\nperc_list = [1,2,5,10,25,50,60,75,80,85,95,99]\nif use_perc:\n    perc_size = len(perc_list)\nstart_time = time.time()\n\nfor i,df in enumerate([df_train,df_test]):\n    print('Loading more features for df: {}\/{}'.format(i+1,3))\n    print('Creating Metrics Part 1')\n    features_1 = df_train.columns.values[2:202]\n    feature_creation(df,features_1,use_perc,perc_list,name_num='_1') #adding columns using the train features (#200)\n    print('Creating Metrics Part 2')\n    features_2 = df_train.columns.values[2:211+perc_size] #all features included the ones added\n    feature_creation(df,features_2,use_perc,perc_list,name_num='_2') #adding columns using the train features + the new features\n    #drop repeated columns\n    df.drop(['min_2','max_2'],axis=1,inplace=True)\n    print('-'*50)\n    \nif log_transf:\n    print('Loading log transformations')\n    for i in range(200):\n        df_train['var_log_'+str(i)] = np.log(df_train['var_'+str(i)])\n        df_test['var_log_'+str(i)] = np.log(df_test['var_'+str(i)])\n\nif sq_data:\n    print('Loading Squared data..')\n    for i in range(200):\n        df_train['var_sq_'+str(i)] = np.square(df_train['var_'+str(i)])\n        df_test['var_sq_'+str(i)] = np.square(df_test['var_'+str(i)])\n    \nprint('Features loaded !')\nprint(\"Execution --- %s seconds ---\" % (time.time() - start_time))\nprint('Train df: ', df_train.columns)\nprint('Test df: ', df_test.columns)\nprint('Number of Features: ', len(df_train.columns[2:]))","a10144b5":"#thanks to https:\/\/www.kaggle.com\/jiweiliu\/lgb-2-leaves-augment\ndef augment(x,y,t=2):\n    xs,xn = [],[]\n    for i in range(t):\n        mask = y>0\n        x1 = x[mask].copy()\n        ids = np.arange(x1.shape[0])\n        for c in range(x1.shape[1]):\n            np.random.shuffle(ids)\n            x1[:,c] = x1[ids][:,c]\n        xs.append(x1)\n\n    for i in range(t\/\/2):\n        mask = y==0\n        x1 = x[mask].copy()\n        ids = np.arange(x1.shape[0])\n        for c in range(x1.shape[1]):\n            np.random.shuffle(ids)\n            x1[:,c] = x1[ids][:,c]\n        xn.append(x1)\n\n    xs = np.vstack(xs)\n    xn = np.vstack(xn)\n    ys = np.ones(xs.shape[0])\n    yn = np.zeros(xn.shape[0])\n    x = np.vstack([x,xs,xn])\n    y = np.concatenate([y,ys,yn])\n    return x,y","e820365f":"#del augment","574fdbfc":"X = df_train.iloc[:,2:]\nY = df_train['target']\nX_target = df_test.iloc[:,1:]\n\n# To use augmnt , set as 1 \naugmnt = 1\n\nif augmnt:\n    print('Data Augmentation: Enabled')\n    X,Y = augment(X.values,Y.values,t=2)\n    X = pd.DataFrame(X,columns=df_train.columns[2:])\n    Y = pd.Series(Y)\n    print('Augmentation Succeeded')\n    labels = [\"0\",\"1\"]\n    values = [(sum(Y == 0)\/len(Y))*100,(sum(Y > 0)\/len(Y))*100]    \n    vis_classes(labels,values,title ='Target Percentages After Augmentation')\n    sub_name = sub_name+'_agmted'    ","b4ad87f6":"if dim_red :\n    print('Reducing Dimension to:',n_components)\n    pca = PCA(n_components= n_components)\n    xnew =pca.fit_transform(X)\n    xtest = pca.fit_transform(X_target)\n    X =pd.DataFrame(nnew_df,columns=['pc_'+str(i) for i in range(n_components)])\n    X_target = pd.DataFrame(xtest,columns=['pc_'+str(i) for i in range(n_components)])\n    print(X.shape)\n    X.head()","89af5418":"#test train split\nX_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=tst_size, random_state=6666)\n\n#Sampling train test split\nX_smple = df_sampling.iloc[:,2:]\ny_smple = df_sampling['target']\nX_train_smple, X_test_smple, y_train_smple, y_test_smple = train_test_split(X_smple, y_smple, test_size=0.4, random_state=6)\n\nprint(\"X_train: \", X_train.shape)\nprint(\"X_test: \" ,X_test.shape)","7762a577":"#Model LGBM \nparam = {\n    'bagging_freq': 4, #handling overfitting\n    'bagging_fraction': 0.2,#handling overfitting - adding some noise\n    #'boost': 'dart',\n    #'boost': 'goss',\n     'boost_from_average' :False,\n     'boost': 'gbdt',   \n    'feature_fraction': 0.15, #handling overfitting\n    'learning_rate': 0.01, #the changes between one auc and a better one gets really small thus a small learning rate performs better\n    'max_depth': 2, \n    'metric':'auc',\n    'num_threads': 8,\n    'tree_learner': 'serial',\n    'objective': 'xentropy', \n    'verbosity':0,\n    \"bagging_seed\" : 12,\n    \"seed\": 2,\n    }\ndef create_model_lgbm(param,X_train,y_train,X_val=None,y_val=None):\n    dtrain = lgb.Dataset(X_train,label=y_train)\n    if not X_val is None:\n        dval = lgb.Dataset(X_val,label=y_val)\n        valid_sets = (dtrain,dval)\n        valid_names = ['train','valid']\n        num_boost_round = 200000\n    else:\n        valid_sets = (dtrain)\n        valid_names = ['train']\n        num_boost_round = 60000\n    model = lgb.train(param,dtrain,num_boost_round=num_boost_round,valid_sets=valid_sets,valid_names=valid_names,\n                      verbose_eval=3000,\n                     early_stopping_rounds=3000)\n    return model\n\n\n#Setting up\nlgbm_test_x = X_target\n\npredictions = df_test[['ID_code']]\nprint('Using Params:\\n',param)\nval_aucs = []\nval_pred = np.zeros(X.shape[0])\ntarget_pred = 0\nimportand_folds = 0\nsub_train_n =2\nkf = StratifiedKFold(n_splits=5,shuffle = kfold_shuffle, random_state=546)\nif fold_train:\n    for _fold, (trn_idx, val_idx) in enumerate(kf.split(X.values, Y.values)):\n            Xtrn, ytrn = X.iloc[trn_idx], Y.iloc[trn_idx]\n            Xval, y_val = X.iloc[val_idx], Y.iloc[val_idx]\n            #Just for info\n            ones_train = (sum(ytrn>0) \/ len(ytrn))*100\n            ones_val = (sum(y_val>0) \/ len(y_val))*100\n            print('-'*50)\n            print(\"Fold num:{}\".format(_fold + 1))\n            print('\\tTrain Perc: 1: {:.2f}%, 0: {:.2f}%'.format(ones_train,100-ones_train))\n            print('\\tValid Perc: 1 : {:.2f}%, 0:{:.2f}%'.format(ones_val,100-ones_val))\n            #augmentation for each training \n            #thanks to https:\/\/www.kaggle.com\/jiweiliu\/lgb-2-leaves-augment\n            if train_between:\n                val_pred = 0\n                target_pred = 0\n                for i in range(sub_train_n):\n                    print('\\tSub-Train: {}'.format(i+1))\n                    if agmnt_between:\n                        X_t, y_t = augment(Xtrn.values, ytrn.values)\n                        print('\\tAugmentation Succeeded..')\n                        X_t = pd.DataFrame(X_t)\n                        X_t = X_t.add_prefix('var_')\n                        print('\\tFitting Model')\n                        clf = create_model_lgbm(param,X_t,y_t,Xval,y_val)\n                    else:\n                         clf = create_model_lgbm(param,Xtrn,ytrn,Xval,y_val)\n                    \n                    target_pred += clf.predict(lgbm_test_x)\n                    val_pred += clf.predict(Xval)\n            #this part could be used when the augmentation is fully applied to the training data\n            else:\n                clf = create_model_lgbm(param,Xtrn,ytrn,Xval,y_val)\n                val_pred  += clf.predict(Xval) \/ kf.n_splits\n                target_pred += clf.predict(df_test.iloc[:,1:]) \/ kf.n_splits\n            \n            print('-' * 50)\n            \n            importand_folds += clf.feature_importance()\n            if train_between:\n                val_score = roc_auc_score(y_val, val_pred\/sub_train_n)\n                predictions['fold{}'.format(_fold+1)] = target_pred\/sub_train_n\n            else:\n                val_score = roc_auc_score(y_val, val_pred)\n                predictions['fold{}'.format(_fold+1)] = target_pred\n                \n            val_aucs.append(val_score)\n            print('\\tVal CV score : {}'.format(val_score))\n            \n\nmean_cv_score = np.mean(val_aucs)\nprint ('-----   Mean CV Score: {:.2} ------'.format(mean_cv_score))","846dcd92":"clf_non_cv = create_model_lgbm(param,X_train,y_train,X_test,y_test)\nlgbm_pred_noncv = clf_non_cv.predict(df_test.iloc[:,1:])\n#full val score\nprint('LGBM NO-CV Val Score: {}'.format(roc_auc_score(y_test,clf_non_cv.predict(X_test))))","9bb55109":"num_features = 60\nif fold_train:\n    indxs = np.argsort(importand_folds\/ kf.n_splits)[:num_features]\nelse:\n    indxs = np.argsort(clf.feature_importance())[:num_features]\n    \nfeature_imp = pd.DataFrame(sorted(zip(clf.feature_importance()[indxs],X.columns[indxs])), columns=['Value','Feature'])\n\nplt.figure(figsize=(20, 10))\nsns.barplot(x=\"Value\", y=\"Feature\", data=feature_imp.sort_values(by=\"Value\", ascending=False))\nplt.title('Top {} LightGBM Features accorss folds'.format(num_features))\nplt.tight_layout()\nplt.show()","58c5823b":"def auc(y_true, y_pred):\n    auc = tf.metrics.auc(y_true, y_pred)[1]\n    K.get_session().run(tf.local_variables_initializer())\n    return auc\n\n#Model NN definition\ndef create_model_nn(in_dim,layer_size=200):\n    model = Sequential()\n    model.add(Dense(layer_size,input_dim=in_dim, kernel_initializer='normal'))\n    if not standradize:\n        model.add(BatchNormalization())\n    model.add(Activation('relu'))\n    model.add(Dropout(0.3))\n    for i in range(2):\n        model.add(Dense(layer_size))\n        if not standradize:\n            model.add(BatchNormalization())\n        model.add(Activation('relu'))\n        model.add(Dropout(0.3))\n    model.add(Dense(1, activation='sigmoid'))\n    adam = optimizers.Adam(lr=0.001)\n    model.compile(optimizer=adam,loss='binary_crossentropy',metrics = [auc])    \n    return model\n\n#Class weights to handle the unbalanced dataset\n\nclass_weights = None\nif weighted:\n    sub_name = sub_name+'_weighted'\n    if balanced:\n        class_weights = class_weight.compute_class_weight('balanced',\n                                                 np.unique(y_train),\n                                                 y_train)\n    else:\n        class_weights = {\n            1:50, \n            0:1\n                }\n\nmodel_nn = create_model_nn(X_train.shape[1])\ncallback = EarlyStopping(monitor=\"val_auc\", patience=50, verbose=0, mode='max')\nhistory = model_nn.fit(X_train, y_train, validation_data = (X_test ,y_test),epochs=100,batch_size=64,verbose=1,callbacks=[callback],class_weight=class_weights)\ntarget_pred_nn = model_nn.predict(df_test.iloc[:,1:])[:,0]\nprint('\\n Validation Max score : {}'.format(np.max(history.history['val_auc'])))","d233ce0a":"#Ditribution Plots from both models \nnn_val_pred = model_nn.predict(X_test,batch_size=256)[:,0]\npredictions['target'] = np.mean(predictions[[col for col in predictions.columns if col not in ['ID_code', 'target']]].values, axis=1) \npredictions['target_noncv'] = lgbm_pred_noncv\npredictions['target_keras'] = target_pred_nn\n\ncomb_approach_test = (0.1*target_pred_nn)+(0.9*predictions['target'])\ncomb_approach_test[comb_approach_test>1]=1\ncomb_approach_test[comb_approach_test<0]=0\n\nif fold_train:\n    plt.figure(figsize=(13, 9))\n    #validations sets\n    sns.distplot(nn_val_pred,label='NN Val Score:{:.3f}'.format(roc_auc_score(y_test,nn_val_pred)))\n    sns.distplot(val_pred,label='LGBM Val Score : {:.3f}'.format(mean_cv_score))\n    plt.title('Validation set target predictions')\n    plt.legend()\n    plt.show()\n    plt.savefig('combination_val.png')\n\nplt.figure(figsize=(13, 9))\n#target final test set\nsns.distplot(target_pred_nn,label='NN Target')\nsns.distplot(predictions['target'],label='LGBM Target')\nsns.distplot(lgbm_pred_noncv,label='Non-CV LGBM Target')\nsns.distplot(comb_approach_test,label='Combination Prediction Target')\nplt.title('Test set target predictions')\nplt.legend()\nplt.show()\nplt.savefig('combination_target_test.png')","23bd1b9e":"def sub_pred(preds,df_test,name='submission.csv'):\n    sub_df = pd.DataFrame({'ID_code':df_test['ID_code'],'target':preds})\n    sub_df.to_csv(name, index=False)\n\nsub_file =  sub_name +'.csv'\nsub_pred(predictions['target'],df_test,name=sub_file)\nprint(sub_file+'   --submitted successfully')\n\nprint('Submitting Combination File..')\nsub_pred(comb_approach_test,df_test,name='comb_submission.csv')\nprint('comb_submission.csv   --submitted successfully')\n\n\n\nprint('Submitting non-cv File..')\nsub_pred(predictions['target_noncv'] ,df_test,name='lgbm_noncv_submission.csv')\nprint('lgbm_noncv_submission.csv   --submitted successfully')","174bf544":"#### here , we take all 1's (10%) and 25% of 0's\n\n#### hence , final distribution = \n* 10\\*1 + 90\\*(1\/4) = 32 % of all data\n* in this 32 % , 10 % are 1's\n* remaining 22% are 0's\n* So now , 1's are 10\/32 * 100 = 31.25 %","974f9f06":"## Running Models Options\n<a id='options'><\/a>\nThe options here helps you check the different combinations for training and check which fits best.","bcdbaf72":"### This is another augmentation strategy for demonstration , we won't be using it here.","dd05151a":"As we can see , many of the engineered features are present within the top 60 important features","78755ecc":"### One can also use the above function to do feature engineering however he wants","a37cac48":"### Feature engineering\n\n* Notice that axis = 1 here. that means we are taking sum , min , max , etc. of each **ROW** , not column \n![](https:\/\/www.stechies.com\/userfiles\/images\/num-11.jpg)\n### Axis = 1 can be seen here.\n* percentile can be used and is optional (ex. 50th percentile would be the median )\n\n* And this way , no. of columns goes up from 202 to 416","d0eac842":"## Let's see how this pie chart was made : ","810d8c1e":"## Simple Feature Engineering\n**Features Used:**\n* sum\n* min\n* max\n* mean\n* std\n* skew\n* kurt\n* med\n* Moving Average\n* percentiles\n<a id='fe'><\/a>","ba16b559":"### So , Basically we can just take that function , and create our own pie charts !","1bf19bcf":"## Combination Vis\n<a id='vis'><\/a>","d0980f99":"# 1. LGBM Model\n<a id='lgbm'><\/a>\n\n* This is a plug-and-play model. You can copy it , understand how it works and use it on your own . BUT UNDERSTAND IT FIRST. These days , it is hard win a competition without using lgbm.","6348f844":"# Dimensionality Reduction\n<a id='dim'><\/a>","8cd6eb99":"### Now , this is really interesting . Let's see how this is done and the final result .\n\n* Below the output , I will expalin the process of doing this feature engineering.","7eddbe5e":"# 2.Keras NN Model\n<a id='keras'><\/a>","e3935329":"### Non-CV LGBM Approach","1d7beab7":"As you can see we are dealing with an unbalanced targets (10% vs 90%)","f84dd336":"## Sampling from the full dataset (more work on this later)\n<a id='sample'><\/a>","569176b0":"# Deep Learning + LGBM + Weighted Combination\n\nThis kernel will always be running with different parameters and approaches until before the competition deadline.\n\nFeel free to upvote,fork and test the presented models with different training options, to see if a better score with the following models is possible.\n\nIf forked Please try the different combinations:\n- Only Feature Engineering ( omitting some features maybe)\n- Only Augmented\n- Augmented + Feature Engineering (Augment before or after FE)\n- Augmented + Feature Engineering + folds\n- Augmented + Feature Engineering + full\n- Combination of different prediction weights\n- etc..\n\nYou can also check here for weighted CV approach that will make a minor better prediction that you might need in the competition:\nhttps:\/\/www.kaggle.com\/hjd810\/introducing-weighted-cross-validation\n\nEnjoy ! \n\nAny comments are appreciated (added motivation <3)\n\n1. [Training Options](#options)\n2. [Sampling](#sample)\n3. [Feature Engineering](#fe)\n4. [Dim Reduction](#dim)\n5. [LGBM Model](#lgbm)\n6. [Keras Model](#keras)\n7. [Combination Vis](#vis)\n8. [Submission](#sub)","aede8da6":"**DistPlot Analysis:** \n\nWe can see from the plots how the predictions for the validation sets, and the final target test set follows closely a similar distribution.\nWhich tells us that the test set can be a resemblance of validation sets we are using. \nThen we can proceed with improving our scores in the validation set knowing that there is a high chance they will also improve in the test set.\n\n**Combination Analysis**\n\nWe can also see that the combination of both is adding some noise to the prediction, which in some cases can prove helpful when each model\nwas able to predict with some features better than the others\n\nmore testing is going on here to see how effecient a combination model can get.","9db91a83":"# Submission\n<a id='sub'><\/a>","c2712cf6":"We will Reduce our dimensions to n number of components and test whether this approach can also help in providing better results"}}