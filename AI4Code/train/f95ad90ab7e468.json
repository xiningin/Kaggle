{"cell_type":{"4f5b2d1b":"code","990af54f":"code","7dd2b182":"code","00cbeaf7":"code","28463cf9":"code","c1a8ec16":"code","63b6bcde":"code","9c38130e":"code","2d23c7da":"code","0663f25d":"code","4f26dfe6":"code","6af3e32f":"code","3f8d7f8d":"code","286e8f6f":"code","fa25ad16":"code","c1c0314c":"code","ac37792c":"markdown","1d400629":"markdown","3635e1ce":"markdown","0143493e":"markdown","f636349f":"markdown","f42ac587":"markdown","632b8503":"markdown","7aa914fe":"markdown","f7e2dd27":"markdown","ccb125aa":"markdown","000179fa":"markdown","db6f1ff5":"markdown","60e65007":"markdown","3873bb10":"markdown","29b799fa":"markdown","b5e83f69":"markdown","21c313d3":"markdown","33ca481a":"markdown","4d799bda":"markdown","2641ce6f":"markdown"},"source":{"4f5b2d1b":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","990af54f":"# BIBLIOTECAS NECESS\u00c1RIAS #                 \nimport pandas as pd\nimport numpy as np\nfrom scipy import stats\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split, RandomizedSearchCV, GridSearchCV\nimport xgboost as xgb\nfrom sklearn import metrics\nimport seaborn as sns","7dd2b182":"train_data = pd.read_csv('\/kaggle\/input\/risco-de-credito\/treino.csv')\n\nprint(\"Tamanho do dataset de treino: \", train_data.shape)\n\npd.concat([train_data.head(3),train_data.sample(4),train_data.tail(3)])","00cbeaf7":"zero_count = train_data['inadimplente'][train_data['inadimplente'] == 0].count()\none_count = train_data['inadimplente'][train_data['inadimplente'] == 1].count()\nproportion = zero_count\/one_count\nprint('A propor\u00e7\u00e3o entre inadimplentes e adimplentes \u00e9 de 1 :', int(proportion))","28463cf9":"test_data = pd.read_csv('\/kaggle\/input\/risco-de-credito\/teste.csv')\nprint(\"Tamanho do dataset de teste: \", test_data.shape)\n\npd.concat([test_data.head(3),test_data.sample(4),test_data.tail(3)])","c1a8ec16":"# An\u00e1lise Estat\u00edstica Descritiva dos Dados:\ntrain_data.describe().T.round(1)","63b6bcde":"count = train_data.isnull().sum().sort_values(ascending = False)\npercent = (train_data.isnull().sum()\/train_data.isnull().count()*100).sort_values(ascending = False)\nmissing_train_data  = pd.concat([count, percent], axis=1, keys=['Count', '%'])\nmissing_train_data.T.round(1)","9c38130e":"count = test_data.isnull().sum().sort_values(ascending = False)\npercent = (test_data.isnull().sum()\/test_data.isnull().count()*100).sort_values(ascending = False)\nmissing_test_data  = pd.concat([count, percent], axis=1, keys=['Count', '%'])\nmissing_test_data.T.round(1)","2d23c7da":"fig, axes = plt.subplots(3, 1, tight_layout=False, figsize=(20,15))\nfig.suptitle('Outliers Analysis\\n', fontsize=22)\nsns.boxplot(ax=axes[0], data=train_data['util_linhas_inseguras'], orient='h')\nsns.boxplot(ax=axes[1], data=train_data['razao_debito'], orient='h')\nsns.boxplot(ax=axes[2], data=train_data['salario_mensal'], orient='h')\naxes[0].set_xlabel('Linhas de Cr\u00e9dito N\u00e3o Seguradas', fontsize='xx-large')\naxes[1].set_xlabel('Raz\u00e3o D\u00edvidas\/Patrim\u00f4nio', fontsize='xx-large')\naxes[2].set_xlabel('Sal\u00e1rio Mensal', fontsize='xx-large')\nplt.show()\n\nQ1 = train_data[['util_linhas_inseguras','razao_debito','salario_mensal']].quantile(0.25)\nQ3 = train_data[['util_linhas_inseguras','razao_debito','salario_mensal']].quantile(0.75)\nIQR = Q3 - Q1\n\nn_outliers = ((train_data[['util_linhas_inseguras','razao_debito','salario_mensal']] < (Q1 - 1.5 * IQR)) | (\n               train_data[['util_linhas_inseguras','razao_debito','salario_mensal']] > (Q3 + 1.5 * IQR))).sum()\npercent_outliers = n_outliers\/train_data[['util_linhas_inseguras','razao_debito','salario_mensal']].count()*100\npd.concat([n_outliers, percent_outliers], axis=1, keys=['Count', '%']).T.round(1)","0663f25d":"fig, axes = plt.subplots(2, 5, figsize=(20,8), tight_layout=False)\nfor i in range(2):\n    for j in range(5):\n        sns.scatterplot(ax=axes[i,j],data=train_data,x=train_data.iloc[:,(5*i+j+1)].sample(500), y=train_data.iloc[:,0])      ","4f26dfe6":"corr_train = train_data.corr()\nplt.figure(figsize = (14, 10))\nsns.heatmap(corr_train, cmap = \"coolwarm\", vmin = -0.25, annot = True, vmax = 0.6)\nplt.title('Correlation Heatmap');","6af3e32f":"X = train_data.iloc[:,1:]\ny = train_data.iloc[:,0]\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1, test_size=0.2)","3f8d7f8d":"model = xgb.XGBClassifier(use_label_encoder=False, eval_metric= 'aucpr', base_score=(1-1\/proportion), scale_pos_weight=proportion)\nmodel.fit(X_train, y_train)\npredicted = model.predict(X_test)\nprobs=model.predict_proba(X_test)\nexpected = y_test\nprint(\"Extreme Gradient Boosting Model Metrics:\\n\")\nprint(metrics.classification_report(expected, predicted))\nprint(\"\\n Confusion Matrix:\\n [[TP       FN]\\n [FP       TN]]\\n\", metrics.confusion_matrix(expected, predicted))\nprint(\"\\nProbability:\\n\", probs[:,1])","286e8f6f":"# Hyper parameters tuning: *** Comentado para reduzir o tempo de c\u00e1lculo do notebook, j\u00e1 que este processo leva horas para ser conclu\u00eddo**\n#parameters = {\n#    'eta':[0.01,0.02,0.03,0.04,0.05,0.1,0.3,0.5,0.75,1],\n#    'alpha':[0,1,5,10,50,75,100]}\n\n#model = xgb.XGBClassifier(use_label_encoder=False, eval_metric= 'aucpr', base_score=(1-1\/proportion), scale_pos_weight=proportion)\n#cv = RandomizedSearchCV(model,parameters,cv=4, n_jobs=-1, verbose=1, n_iter=50)\n#cv.fit(X_train,y_train)\n#cv.best_params_","fa25ad16":"# Modelo Refinado:\nmodel = xgb.XGBClassifier(eval_metric= 'aucpr', use_label_encoder=False, base_score=(1-1\/proportion), scale_pos_weight=proportion, \n                          eta=0.02, alpha=100)\nmodel.fit(X_train, y_train)\npredicted = model.predict(X_test)\nprobs = model.predict_proba(X_test)\nexpected = y_test\nprint(\"Extreme Gradient Boosting Model Metrics:\\n\")\nprint(metrics.classification_report(expected, predicted))\nprint(\"\\n Confusion Matrix:\\n [[TP       FN]\\n [FP       TN]]\\n\", metrics.confusion_matrix(expected, predicted))\nprint(\"\\nProbability:\\n\", probs[:,1])","c1c0314c":"predicted = model.predict(test_data)\nprobs = model.predict_proba(test_data)[:,1]*100\noutput = {'inadimplente': predicted, 'probabilidade %': probs}\noutput = pd.DataFrame(data=output)\noutput.to_csv('output.csv',index=False)\npredicted = pd.DataFrame({'inadimplente': predicted})\nSubmission_File = predicted.join(test_data)\nSubmission_File.to_csv('Submission_File.csv',index=False)\n","ac37792c":"Agora o dataset de teste:","1d400629":"Pela estat\u00edstica descritiva apresentada, tamb\u00e9m \u00e9 not\u00e1vel que h\u00e1 outliers nas features util_linhas_inseguras, razao_debito e salario_mensal. \u00c9 poss\u00edvel que sejam alguns inputs incoerentes, conforme salientado na descri\u00e7\u00e3o do problema. Vamos verificar pelos gr\u00e1ficos de caixa e analiticamente:","3635e1ce":"Algumas vari\u00e1veis apresentam baix\u00edssima correla\u00e7\u00e3o entre si, por\u00e9m, outras, como as vari\u00e1veis relacionadas ao n\u00famero de dias de atraso no pagamento do empr\u00e9stimo, est\u00e3o quase que perfeitamente correlacionadas.","0143493e":"# **Modelo Creditae**: Modelo Preditivo para Aprova\u00e7\u00e3o de Cr\u00e9dito\n\n**Objetivo:** Calcular a probabilidade de inadimpl\u00eancia de cada novo pedido de cr\u00e9dito\n\n**Autor:** Caio Vellosa\n","f636349f":"# **An\u00e1lise Explorat\u00f3ria dos Dados**","f42ac587":"Realmente, h\u00e1 um quantidade consider\u00e1vel de dados de salario mensal faltando. Adiante veremos a relev\u00e2ncia desta feature para determinar seu desfecho, seja excluir esta coluna ou preencher as lacunas com alguma t\u00e9cnica preditiva. As linhas com dados faltantes de numero de dependentes pode ser exclu\u00edda sem perda substancial de informa\u00e7\u00e3o, \u00e0 princ\u00edpio.","632b8503":"Percebe-se que o modelo teve um desempenho satisfat\u00f3rio em classificar os tomadores de cr\u00e9dito, na maioria das vezes alocando na classifica\u00e7\u00e3o certa: 84% entre os reais adimplentes e 69% entre os reais inadimplentes (coluna recall). Dada a natureza do problema, sup\u00f5e-se seja mais interessante ter um acerto maior entre os inadimplentes, mesmo que isso signifique uma maior classifica\u00e7\u00e3o errada entre os adimplentes. Assim sendo, vamos tentar refinar o modelo, ajustando seus hiperpar\u00e2metros e visando aumentar sua Especificidade.","7aa914fe":"Pela caracter\u00edstica do problema, \u00e9 de se esperar que as classifica\u00e7\u00f5es (adimplente ou inadimplente) estejam desbalanceadas. Vamos checar a propor\u00e7\u00e3o exata:","f7e2dd27":"Implementando o modelo XGBoost, somente o configurando para considerar o dataset desbalanceado e indicando uma m\u00e9trica mais adequada, devido ao desbalanceamento do dataset.","ccb125aa":"Como poss\u00edvel sugest\u00f5es de melhoria do modelo, destaco:\n- Reavalia\u00e7\u00e3o de parte do dataset de treino a fim de encontrar classifica\u00e7\u00f5es erradas. Talvez at\u00e9 separar uma por\u00e7\u00e3o razo\u00e1vel dele, reavaliar e treinar o modelo somente com esse peda\u00e7o;\n- Ir mais a fundo nas possibilidades de refino dos hiperpar\u00e2metros do modelo XGBoost, j\u00e1 que existem in\u00fameros par\u00e2metros n\u00e3o explorados devido falta de recursos computacionais\/tempo.\n- Testar a performance de outros modelos, al\u00e9m do XGBoost;","000179fa":"Como o dataset \u00e9 grande, vamos separar 20% dele para a valida\u00e7\u00e3o e obten\u00e7\u00e3o dos m\u00e9tricas de acur\u00e1cia:","db6f1ff5":"Aplicando o modelo ao dataset de teste:","60e65007":"Ap\u00f3s o refino do modelo, principalmente ao reduzir o par\u00e2metro \"eta\" de 0.3 (padr\u00e3o) para 0.02, a *Especificidade* (TN\/(FP+TN)) se tornou mais interessante, mesmo ao custo de penalizar a *Sensibilidade* (recall). Cabe \u00e0 casa de empr\u00e9stimos avaliar qual a exposi\u00e7\u00e3o aceit\u00e1vel de devedores inadimplentes.","3873bb10":"Pela contagem das features, parece haver dados faltantes nas colunas salario_mensal e numero_de_dependentes, vamos checar:","29b799fa":"A partir de uma amostra de 500 pontos, a visualiza\u00e7\u00e3o da dispers\u00e3o dos dados n\u00e3o demonstra uma linearidade suficiente para ajustar bem uma fun\u00e7\u00e3o log\u00edstica em nenhuma feature.\n","b5e83f69":"Para complementar a an\u00e1lise, vamos checar as correla\u00e7\u00f5es entre as vari\u00e1veis:","21c313d3":"A propor\u00e7\u00e3o de outliers \u00e9 bem significativa na coluna razao_debito. Como retir\u00e1-los acarretaria em uma perda grande de informa\u00e7\u00e3o, vamos mant\u00ea-los por enquanto e verificar posteriormente a performance do modelo com e sem os outliers.","33ca481a":"Vamos explorar as caracter\u00edsticas do dataset.\n\nCome\u00e7ando pelo dataset de treino:","4d799bda":"As primeiras impress\u00f5es do dataset s\u00e3o:\n- Trata-se de um problema supervisionado de classifica\u00e7\u00e3o bin\u00e1ria (Adimplente=0 e Inadimplente=1);\n- Todas as features tem valores num\u00e9ricos, sendo 3 delas com valores cont\u00ednuos e 4 com valores discretos;\n- O dataset \u00e9 grande;\n- Existem muitos outliers;\n- N\u00e3o demonstra correla\u00e7\u00e3o linear com a vari\u00e1vel dependente;\n- Algumas features est\u00e3o altamente correlacionadas;\n- Classifica\u00e7\u00e3o desbalanceada;\n\n# Modelo\nDiante deste cen\u00e1rio, modelar uma regress\u00e3o log\u00edstica n\u00e3o parece ser uma boa id\u00e9ia. Talvez algum modelo baseado em \u00c1rvores de Decis\u00e3o seja mais adequado, j\u00e1 que eles s\u00e3o mais tolerantes \u00e0 datasets ins\u00f3litos, al\u00e9m de n\u00e3o necessitar normaliza\u00e7\u00e3o dos dados. \nO modelo Extreme Gradient Boosting, abreviado como XGBoost, parece ser uma boa escolha inicialmente, pois, al\u00e9m de ser o estado-da-arte do aprendizado de m\u00e1quina baseado em \u00c1rvores de Decis\u00e3o, tamb\u00e9m \u00e9 tolerante com dados faltantes. Assim, poderemos manter todas as colunas dos dataset, retendo mais poder de aprendizado.","2641ce6f":"Agora, vamos visualizar a dispers\u00e3o dos dados de cada coluna:"}}