{"cell_type":{"7b73e8ae":"code","13bc7b87":"code","a33eca98":"code","99bd5135":"code","571d9f53":"code","f06ad450":"code","79b22158":"code","7458b758":"code","5d1141d1":"markdown"},"source":{"7b73e8ae":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","13bc7b87":"\n\"\"\"\"\n***************************** TOPIC : Line of best fit ******************************************************\n\n\nLine of best fit is line through data points that best express the relation among these data points.\n\nFor Line of best fit  sum of square of residual errors between data point and indivisual line is at minimum .\n    \n  In term of   mathematical equation :\n        y=mx+c \n        m: slope\n        c: intercept\n       m=n* (sum (x*y)-sum(x)sum(y))\/(n* sum (x*x)- (sum(x))^2)\n       n: no of data point on scatter plot\n       \n    c:(sum(y)- m* sum(x))\/n\n\"\"\"","a33eca98":"\"\"\"\"\n\n ******************************************** TOPIC  : RMSE *****************************************************\n\nRoot mean square error\n\nRMSE tells us how data point is around line of best fit. \nin other word how residual are spread among best fit line . How actual values are diff. from predicted value .\nRMSE is SD(standard deviation) of residuals .\n\nResiduals : prediction error \/ or how much data point are away from regression line \n    \n  RMSE =  sqrt((y(predict)-y(actual))^2\/n)\n\n*************************************************************************\nTo remember : error diff. ->then square -> take mean -> square root\n************************************************************************\n\nRMSE is always betwwn 0 and 1 .\n\n*****  As much Lower the  RMSE value better the model prediction *******\n\n\"\"\"","99bd5135":"\"\"\"\n********************************************* TOPIC : R square *************************************************\n\nR square determine the  proportion of variance of  dependent variable that is  explained by the independent variable \nin a regression model\nin other word , to what extent variance of one varibale explained by other variable .\nif R^2  value is 0.50 that means 50% variance of dependent variable can be explained by model input(independent variable)\n\nR^2 value range from 0 to 1 or  0% to 100% .\n\nwe can't totaly relie on r^2 for test model prediction is good or not , we need other statistical term as well .\nHigher  the value of R^2  -> good the model prediction ..this statement is not always true ... \n\nHigher\/Lower value of  R^2  does not tell anything about model prediction .\n\nR^2 =1-(Explained variation)\/(Total Variation)\n\n\n********************** ******************** TOPIC : Adjusted R^2  **********************************************************\n\nR^2 only works well in case of simple linear regression with one explainatory variable .\nin case of regression made up of multiple independent variable R^2 must be adjusted .\n\nEvery  independent variable added to model always increase R^2 value  whether added variable actually enchancing the model or not .\nso in that case model with more term may seem better fit just for fact that it has more term.\n\nHere Adjusted R^2 play its role . \nit adjust the value and increase only when addition of new term enchance the model .\n\nso in short , increasing the term \/ addition of variable  always increase R^2 value whereas in adjusted R^2 \nit checks whethere model is being enchanced or not then accordingly increase the value otherwise not .\n\nFormula :\n\nadj R^2 = 1-[(1-R^2)(n-1)\/(n-k-1)]\n\nn= no. of data point\nk= no. of independent variable\n\n\"\"\"\n\n\n\n","571d9f53":"\"\"\"\n\n****************************************** TOPIC :- P-value  **********************************************************\n\nP value :-\n\nit represent the probability of occurance of given event .\n\nLower the p value  that mean we can reject null hypothesis .\n\n*******************************************************\n\nNull Hypothesis :- the event which are on trial is called null hypothesis .\n\nAlternative Hypothesis :-it is the one which you will believe when null hypothesis is supposed to be false .\n\n**************************************************************************\nfor ex :- \n\npizza shop claim it deliver pizza in 30 mnt or less than 30 mnt .\n\nbut you have doubt on it you think its more than 30 mnt , then you conduct hypothesis test .\n\nnull hypothesis :- delivery time is max 30 mnt .(normal claim \/or event on trial)\n\nAlternative hypothesis :- deliver time is more than 30 mnt .\n\nNow we calculate P-value .its found  P=0.001 .\nHence its very less, we can reject null hypothesis  .\nLower the P-value , mean Hypothesis is in favor of Alternative Hypothesis .\nHence pizz is deliver in more than 30 mnt .\n\n\n*******************************************************\n\nType 1 error : Incorrect rejection of null hypothesis \nType 2 error  : Incorect acceptance of null hypothesis\n\n****************************************************\n\n\"\"\"\n","f06ad450":"\"\"\"\nF1 Score\n\nF1 score : It being use to measure test accuracy .\n    its Harmonic mean of test precision and recall .\n    \n    F1= HM(recall ,Precision)\n     =  2* (precision *Recall) \/(Precision+Recall)\n        \n        \nPrecision : - precision is define as accuracy of judgement  , how precise  the value is .\n              (true postive) \/(true positive+ false positive)\n        \nRecall :-   recall is called sensitivity , is the ability to identify the number of samples that would really count \n            positive for specific attribute.\n            \n            = (true positive)\/(true positive+ False negative)\n            \n\n\n\"\"\"\n","79b22158":"\"\"\"\n***************************************** TOPIC :- ROC and AUC ***************************************\n\n    ROC :- Receiver operating characteristics\n           It is perfromance measure of classification model , which tell how model can distiguish between\n           true positive and true negative .\n           \n            It  is created by plotting true positive rate against false positive rate .\n             plotting  sensitivity  against 1-specificity\n             plotting TPR again FPR\n             \n        ROC is curve and AUC(area under curve) is measure of seperability .\n        HIgher the AUC better the model is predicting 0s as 0 and 1s as 1.\n        Excellent model has AUC=1 which means good measure of seperability . prdeciting 0 as 0 and 1 as 1\n        \n        AUC=0 , which means worst measure of seperability . predicting 0 as 1 and 1 as 0\n        \n        \"\"\"\n        \n        \n            \n    ","7458b758":"\"\"\"\"\n************************** confusion Matrix ******************\n\nconfusion matrix use to predict performance of classification model .\nit is table of combination of 4 values ,\n\nTP : true positive   predicted positive and it is true\nFP : false positive  predicted positive and it is false (type 1 error) \nTN : true negative  predicted negative and it is true\nFN : false negative predicted negative and it is false (type 2 error)\n    \n    confusion matrix used to calculate precision , recall , F1-score\n    \n    \"\"\"","5d1141d1":"Basic Statistic  use in ML"}}