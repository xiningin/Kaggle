{"cell_type":{"cedca339":"code","fb2aa56e":"code","45f7f2ba":"code","17da368c":"code","c1df5b88":"code","c160c390":"markdown"},"source":{"cedca339":"import pandas as pd\nimport numpy as np\nfrom fbprophet import Prophet\nfrom tqdm import tqdm, tnrange\nfrom multiprocessing import Pool, cpu_count","fb2aa56e":"calendar_df = pd.read_csv('\/kaggle\/input\/m5-forecasting-accuracy\/calendar.csv')\nsales_train =  pd.read_csv('\/kaggle\/input\/m5-forecasting-accuracy\/sales_train_validation.csv')\nsell_prices = pd.read_csv('\/kaggle\/input\/m5-forecasting-accuracy\/sell_prices.csv')\nsubmission = pd.read_csv('\/kaggle\/input\/m5-forecasting-accuracy\/sample_submission.csv')","45f7f2ba":"def run_prophet(timeserie):\n    model = Prophet(uncertainty_samples=False)\n    model.fit(timeserie)\n    future = model.make_future_dataframe(periods=28, include_history=False)\n    forecast = model.predict(future)\n    return forecast","17da368c":"start_from_ob = 800\nfor i in tnrange(sales_train.shape[0]):\n    temp_series = sales_train.iloc[i,start_from_ob:]\n    temp_series.index = calendar_df['date'][start_from_ob:start_from_ob+len(temp_series)]\n    temp_series =  pd.DataFrame(temp_series)\n    temp_series = temp_series.reset_index()\n    temp_series.columns = ['ds', 'y']\n\n    with Pool(cpu_count()) as p:\n        forecast1 = p.map(run_prophet, [temp_series])\n\n    submission.iloc[i,1:] = forecast1[0]['yhat'].values\n\nsubmission.iloc[:,1:][submission.iloc[:,1:]<0]=0","c1df5b88":"submission.to_csv('submission.csv', index=False)","c160c390":"## Goal of the notebook \n- Use Facebook's Prophet to predict each time series \n- Cut down running times \n\n### The problems \nFBprophet is a great tool for predicting univariate times series that can take into consideration external factors such as calendar effects, special events, etc. However the large number of time series to predict in the competition leads to increased running times due to a) the sequential nature of the for loop and b) the fact that FBprophet does not really perform parallelization across all cpu threads by default. \n\n### The solutions \nTo reduce computational times there are some tricks we might use: \n1. Avoid producing confidence intervals for our prediction. Since we do not have to provide a confidence in our predictions to our bosses, simply dropping them speeds up training a lot (kudos to @tita1708 for that). \n2. Avoid producing in-sample prediction. By default, FBprophet produces a prediction that covers your entire training sample plus whatever horizon you tell it to predict (e.g. 28 days). Since we dont neccessarily need the in-sample values (at least not to submit them), we can drop them to save some time. \n3. Avoid using the full length of the time series. In this notebook I start from observation 800, dropping the ones before that point. Maybe not the optimal, it is strictly selected just to curtail running times. Could possibly be the correct choice if the was a data shift between old and recent data but that needs to be examined\n4. Parallelize the process. I have use the standard 'multiprocessing' python library here to force all available CPU threads to work on each prediction. CAUTION: this only parallelizes each single iteration of the for loop across CPUs, it does not parallelize the entire for loop itself. \n5. Parallelize the for loop (not yet implemented). Starbucks are currently implementing FBprophet parallelization to perform daily predictions across all codes, of all their stores (a very large number of time series). They accomplish this by splitting the process across distributed systems using spark (more: https:\/\/databricks.com\/p\/webinar\/starbucks-forecast-demand-at-scale-facebook-prophet-azure-databricks) \n\n#### Please don't forget...\n- To consider upvoting if you find this kernel helpful\n- To contribute your own insights! Enjoy!"}}