{"cell_type":{"fe0e164c":"code","242732d0":"code","8dcdfbb8":"code","c93cbe25":"code","20bc1742":"code","d581f2a9":"code","5a691fe9":"code","35616840":"code","a3f034a2":"code","eb89b154":"code","46b03f63":"code","33ef1ecd":"code","61b17b3b":"code","1e7be196":"code","bb56cee4":"code","5fa1750a":"code","8bf13532":"code","70244d08":"code","6d33989a":"code","23e44ceb":"code","80301760":"code","0373b538":"markdown","81693881":"markdown"},"source":{"fe0e164c":"import pandas as pd\nimport numpy as np\nimport re #regular expression\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.stem import PorterStemmer\nimport operator\nimport pylab as pl\nfrom wordcloud import WordCloud\nimport matplotlib.pyplot as plt\n","242732d0":"#pre-precessing methods\nps = PorterStemmer()\nlemmatizer = WordNetLemmatizer() \n\n#install stopwords\nstopwords = set(stopwords.words('english'))\nstopwords.update(['.', ',', '\"', \"'\", ':', ';', '(', ')', '[', ']', '{', '}'])","8dcdfbb8":"def calculate_prob(a):\n    read_dict = {}\n    for read in a:\n        for r in read:\n            if r in read_dict:\n                val_old = read_dict[r]\n                val_new = val_old + 1\n                read_dict[r] = val_new\n            else:\n                read_dict[r] = 1\n    return read_dict","c93cbe25":"#if we moved words have low frequecny, increase the accuracy \ndef delete_low_frequency(a):\n    b = a.copy()\n    for read in a:\n        if a[read] < 3:\n            b.pop(read)\n    return b\n","20bc1742":"#For only test\ndef calculate_error_zero(test):\n    probability_zero = 1\n    smooth_zero = 1\/(len(label_train_zero_class) + len(label_train)) #it is so important to use smooting factor..\n    for keys in range(len(test)):\n        probability_zero = probability_zero * prob_zero.get(test[keys], smooth_zero)\n    return probability_zero * zero_prior_pb\n\n\ndef calculate_error_one(test):\n    probability_one = 1\n    smooth_one = 1\/(len(label_train_one_class) + len(label_train)) \n    for keys in range(len(test)):\n        probability_one = probability_one * prob_one.get(test[keys],smooth_one)\n    return probability_one * one_prior_pb","d581f2a9":"def tokenize(message):\n    message = message.lower()\n    all_words = re.findall(\"[a-zA-z+]+\", message)\n   # print(all_words)\n    all_words_alpha = [word.lower() for word in all_words if word.isalpha()]\n   # print(all_words_alpha)\n    all_words_lemmate = [ps.stem(lemmatizer.lemmatize(w)) for w in all_words_alpha]\n   # print(all_words_lemmate)\n    return all_words_lemmate","5a691fe9":"def punc_ques(df):   #we decide to use ? and ! they are meaningfull to sarcatism.. \n    punc = \"?!\"\n    replace_with=[' '+i+' ' for i in punc]\n    to_replace=[re.escape(i) for i in punc]\n    df = pd.DataFrame(df).replace(to_replace,replace_with,regex=True)\n    return df","35616840":"def count_punc_ques(data):\n    punc_count = 0\n    ques_count = 0\n    if type(data) == pd.core.frame.DataFrame: #for train data\n        for i in data[\"headline\"]:\n            for j in i:\n                if j == \"?\":\n                    ques_count+=1\n                elif j == \"!\":\n                    punc_count+=1\n    elif type(data) == str:             #for test data\n        for j in data:\n            if j == \"?\":\n                ques_count+=1\n            elif j == \"!\":\n                punc_count+=1\n        \n    return punc_count,ques_count","a3f034a2":"df = pd.read_json(\"..\/input\/news-headlines-dataset-for-sarcasm-detection\/Sarcasm_Headlines_Dataset.json\", lines=True)\ndf.head()","eb89b154":"df = df.drop(['article_link'], axis=1)\n\ntrain_filenames = df[:20000]\n\nlabel_train = train_filenames['is_sarcastic']\n\nzero_class = train_filenames[label_train == 0]\none_class = train_filenames[label_train == 1]\n\nzero_class = zero_class.sample(frac=1).reset_index(drop=True)\none_class = one_class.sample(frac=1).reset_index(drop=True)\n\nlabel_train_zero_class = zero_class['is_sarcastic']\nlabel_train_one_class = one_class['is_sarcastic']\n\nnumberofelement = label_train.count()\nzero_prior_pb = (label_train[label_train == 0].count()) \/ numberofelement   #prior probab\none_prior_pb = (label_train[label_train == 1].count()) \/ numberofelement\n\nprint(\"Neutral Prior Prob : %s\" %zero_prior_pb)\nprint(\"Sarcasm Prior Prob : %s\"%one_prior_pb)","46b03f63":"tokenized_zero_class = []\ntokenized_one_class = []\na = []\n\none_class = punc_ques(one_class)   #for ? and !\nzero_class = punc_ques(zero_class)  #for ? and !\none_punc, one_ques = count_punc_ques(one_class)\nzero_punc, zero_ques = count_punc_ques(zero_class)\n\nfor i in range(label_train_zero_class.count()):\n    tokenized_zero_class.append(tokenize(zero_class['headline'][i]))\n\nfor i in range(label_train_one_class.count()):\n    tokenized_one_class.append(tokenize(one_class['headline'][i]))\n\ntokenized_zero_class_prob = []\nfor doc in tokenized_zero_class:\n    tokenized_zero_class_prob.append([i.lower() for i in doc if i.lower() not in stopwords])\n\ntokenized_zero_class_prob_clean = delete_low_frequency(calculate_prob(tokenized_zero_class_prob))\ntokenized_zero_class_prob_clean[\"!\"] = zero_punc \ntokenized_zero_class_prob_clean[\"?\"] = zero_ques \n\ntokenized_one_class_prob = []\nfor doc in tokenized_one_class:\n    tokenized_one_class_prob.append([i.lower() for i in doc if i.lower() not in stopwords])\n\ntokenized_one_class_prob_clean = delete_low_frequency(calculate_prob(tokenized_one_class_prob))\ntokenized_one_class_prob_clean[\"!\"] = one_punc \ntokenized_one_class_prob_clean[\"?\"] = one_ques \n\nprob_zero = {}\nfor read in tokenized_zero_class_prob_clean.keys():\n    prob_zero[read] = tokenized_zero_class_prob_clean[read] \/ sum(tokenized_zero_class_prob_clean.values())\n\nprob_one = {}\nfor read in tokenized_one_class_prob_clean.keys():\n    prob_one[read] = tokenized_one_class_prob_clean[read] \/ sum(tokenized_one_class_prob_clean.values())","33ef1ecd":"#example of probabilty dictionary\nfor e in range(5):\n    print(([x for x in prob_zero.keys()][e], [x for x in prob_zero.values()][e]))","61b17b3b":"test_filenames = df[20000:]\ntest_data = test_filenames.sample(frac=1).reset_index(drop=True)\nlabel_test = test_data['is_sarcastic']\nTP = TN = FP = FN = 0\nprecision = recall = f1_score = accurancy = specificity = 0\nsample_right_zero = []\nsample_wrong_zero = []\nsample_right_one = []\nsample_wrong_one = []\n\ntest_data = punc_ques(test_data)  #for ? and !\n\nfor i in range(label_test.count()):\n    test_sample = test_data['headline'][i]\n    test_punc, test_ques = count_punc_ques(test_sample)\n    tokenized_test = tokenize(test_sample)\n    tokenized_test_prob = []\n    tokenized_test_prob = [i.lower() for i in tokenized_test if i.lower() not in stopwords]\n    if test_punc != 0 :\n        tokenized_test_prob.append(\"!\")\n    if test_ques != 0:\n        tokenized_test_prob.append(\"?\")\n    calculated_zero_prob = calculate_error_zero(tokenized_test_prob)\n    calculated_one_prob = calculate_error_one(tokenized_test_prob)\n    if calculated_zero_prob > calculated_one_prob:\n        if label_test[i] == 1:\n            FN += 1\n            sample_wrong_one.append(test_data['headline'][i])\n        elif label_test[i] == 0:\n            TN += 1\n            sample_right_zero.append(test_data['headline'][i])\n    else:\n        if label_test[i] == 1:\n            TP += 1\n            sample_right_one.append(test_data['headline'][i])\n        elif label_test[i] == 0:\n            FP += 1\n            sample_wrong_zero.append(test_data['headline'][i])","1e7be196":"accurancy = (TN+TP) \/ label_test.count()\nspecificity = TN \/ (TN + FP)\nprecision = TP \/ (TP + FP)\nrecall = TP \/ (TP + FN)\nf1_score = 2*((precision*recall)\/(precision+recall))\n\nprint(\"Accuracy is :\", accurancy)\nprint(\"Precision is :\", precision)\nprint(\"Recall is :\", recall)\nprint(\"Specificity is :\", specificity)\nprint(\"F1-score is :\", f1_score)","bb56cee4":"print(\"Right Samples of Neutral Headlines are :\")\nfor i in range (5) :\n    print(sample_right_zero[i])\n","5fa1750a":"print(\"Wrong Samples of Neutral Headlines are :\")\nfor i in range (5) :\n    print(sample_wrong_zero[i])","8bf13532":"print(\"Right Samples of Sarcastic Headlines are :\")\nfor i in range (5) :\n    print(sample_right_one[i])","70244d08":"print(\"Wrong Samples of Sarcastic Headlines are :\")\nfor i in range (5) :\n    print(sample_wrong_one[i])","6d33989a":"one_most  = max(tokenized_one_class_prob_clean.items(), key=operator.itemgetter(1))[0]\nzero_most = max(tokenized_zero_class_prob_clean.items(), key=operator.itemgetter(1))[0]\nprint(\"The most accurated word for sarcastic headlines is: \",one_most,\n      \" and it repeats \",tokenized_one_class_prob_clean[one_most], \"times.\")\n","23e44ceb":"print(\"The most accurated word for neutral headlines is: \",zero_most,\n      \" and it repeats \",tokenized_zero_class_prob_clean[zero_most], \"times.\")","80301760":"pl.figure(figsize =(10,10))\nnot_sarcastic = str(tokenized_zero_class_prob_clean)\nwordCloud = WordCloud(background_color=\"white\").generate(not_sarcastic)\nplt.imshow(wordCloud, interpolation='bilinear')\nplt.axis('off')\nplt.title('Not Sarcasm - Words', fontsize=15)","0373b538":"* > ****Test****","81693881":"****In this study, it is analyzed whether the headlines obtained from various sites are sarcastic or not.****\nNaive Bayes is a simple \"probabilistic classifiers\" and based on Bayes theorem. It can be expressed as: \n\\begin{equation*}\nP(c\/x)   = \\frac{ P(x\/c) * P(c)}{P(x)}\n\\end{equation*}\n\n\n*  P(c) = Prior probabilty\n*  P(x\/c) = Likelihood\n*  P(c\/x) = Posterior probability\n \nIt is assumed that all the features have an equal effect on the outcome and features  has unrelated to the presence of any other feature.\n\n"}}