{"cell_type":{"83908a31":"code","68e03b92":"code","4aaa8334":"code","fbdd6258":"code","e2444687":"code","c9e2e5de":"code","adacd262":"code","6f1075ed":"code","e13ba025":"code","95e02f1b":"code","8d037e32":"code","c1bef066":"code","cdc4bf26":"code","4d956384":"code","cd5884a6":"code","90b236ef":"code","75fd79b9":"code","b8e0aacf":"code","82e8be43":"code","13f51064":"code","dd41e5c6":"code","7bced6fd":"code","22ac64db":"code","63c18f2b":"code","03fdc383":"code","0fc7f538":"code","8c6c9366":"markdown","4dec3323":"markdown","3fa0081b":"markdown","d1e65da1":"markdown","97cfe32a":"markdown","e504acc1":"markdown","45995e6b":"markdown","ae2bb5a9":"markdown","5f1770de":"markdown","7664d8df":"markdown","31752588":"markdown","308842bc":"markdown","1051b5b2":"markdown","18b36cc7":"markdown","c48e2945":"markdown","f840da7d":"markdown","c591df09":"markdown","1944de0e":"markdown","8255b49a":"markdown","d93a870f":"markdown","75f20859":"markdown","fea4dfeb":"markdown","89eed58f":"markdown","77e81314":"markdown","8a83f44a":"markdown","c9e732ca":"markdown","5778caa4":"markdown","b53a30d8":"markdown"},"source":{"83908a31":"class Config:\n    vocab_size = 15000 # Vocabulary Size\n    sequence_length = 100 # Length of sequence\n    batch_size = 1024\n    validation_split = 0.15\n    embed_dim = 256\n    latent_dim = 256\n    epochs = 50 # Number of Epochs to train\n    best_auc_model_path = \"model_best_auc.tf\"\n    best_acc_model_path = \"model_best_acc.tf\"\n    lastest_model_path = \"model_latest.tf\"\nconfig = Config()","68e03b92":"import pandas as pd\nimport tensorflow as tf\nimport pathlib\nimport random\nimport string\nimport re\nimport sys\nimport numpy as np\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nimport os\nimport sklearn\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom nltk.tokenize import TweetTokenizer \nfrom nltk.stem.porter import PorterStemmer\nfrom nltk.stem import WordNetLemmatizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom scipy.stats import rankdata\nimport json","4aaa8334":"validation_data = pd.read_csv(\"\/kaggle\/input\/jigsaw-toxic-severity-rating\/validation_data.csv\")\nvalidation_data.head()","fbdd6258":"train = pd.read_csv(\"..\/input\/jigsaw-toxic-comment-classification-challenge\/train.csv\")\ntrain.head()","e2444687":"use_external_dataset = True\nif use_external_dataset:\n    train = train[[\"comment_text\", \"toxic\"]]\n    train.columns = [\"text\", \"label\"]\n    # Add More toxic data to mitigate class imbalance problem\n    train = train.append(pd.DataFrame({\"text\": validation_data[\"more_toxic\"], \"label\": [1] * len(validation_data)}))\nelse:\n    data = pd.DataFrame({\"text\": validation_data[\"less_toxic\"], \"label\": [0] * len(validation_data)})\n    data = data.append(pd.DataFrame({\"text\": validation_data[\"more_toxic\"], \"label\": [1] * len(validation_data)}))\n    text = data[\"text\"].unique()\n    grouped = data.groupby(\"text\")\n    label = list(grouped.mean()[\"label\"])\n    text_label_dict = dict({key: value for key, value in zip(text, label)})\n    index_label = sorted(grouped.mean()[\"label\"].unique())\n    data[\"average_value\"] = data[\"text\"].apply(lambda text: text_label_dict[text])\n    data[\"class\"] = data[\"average_value\"].apply(lambda value: index_label.index(value))\n    classes = sorted(data[\"class\"].unique())\n    print(\"Classes:\", classes)\n    train = data[[\"text\", \"label\"]]","c9e2e5de":"def custom_standardization(input_data):\n    lowercase = tf.strings.lower(input_data)\n    stripped_html = tf.strings.regex_replace(lowercase, \"<br \/>\", \" \")\n    text = tf.strings.regex_replace(\n        stripped_html, f\"[{re.escape(string.punctuation)}]\", \"\"\n    )\n    text = tf.strings.regex_replace(text, f\"[0-9]+\", \" \")\n    text = tf.strings.regex_replace(text, f\"[ ]+\", \" \")\n    text = tf.strings.strip(text)\n    return text","adacd262":"tfidf_vectozier = layers.TextVectorization(\n    standardize=custom_standardization, \n    max_tokens=config.vocab_size, \n    output_mode=\"tf-idf\", \n    ngrams=2\n)\nwith tf.device(\"CPU\"):\n    # A bug that prevents this from running on GPU for now.\n    tfidf_vectozier.adapt(list(train[\"text\"]))","6f1075ed":"word2vec_vectozier = layers.TextVectorization(\n    standardize=custom_standardization, \n    max_tokens=config.vocab_size, \n    output_sequence_length=config.sequence_length\n)\nwith tf.device(\"CPU\"):\n    word2vec_vectozier.adapt(train[\"text\"])","e13ba025":"X_train, X_val, y_train, y_val = train_test_split(train[\"text\"], train[\"label\"], test_size=config.validation_split)","95e02f1b":"X_train.shape, y_train.shape, X_val.shape, y_val.shape","8d037e32":"def make_dataset(X, y, batch_size, mode):\n    dataset = tf.data.Dataset.from_tensor_slices((X, y))\n    if mode == \"train\":\n       dataset = dataset.shuffle(256) \n    dataset = dataset.batch(batch_size)\n    dataset = dataset.cache().prefetch(16).repeat(1)\n    return dataset","c1bef066":"train_ds = make_dataset(X_train, y_train, batch_size=config.batch_size, mode=\"train\")\nvalid_ds = make_dataset(X_val, y_val, batch_size=config.batch_size, mode=\"valid\")","cdc4bf26":"for batch in train_ds.take(1):\n    print(batch)","4d956384":"class_weight =  1 \/ train[\"label\"].value_counts(normalize=True)\nclass_weight = dict(class_weight \/ class_weight.sum())\nclass_weight","cd5884a6":"class FNetEncoder(layers.Layer):\n    def __init__(self, embed_dim, dense_dim, dropout_rate=0.1, **kwargs):\n        super(FNetEncoder, self).__init__(**kwargs)\n        self.embed_dim = embed_dim\n        self.dense_dim = dense_dim\n        self.dense_proj = keras.Sequential(\n            [\n                layers.Dense(dense_dim, activation=\"relu\"),\n                layers.Dense(embed_dim),\n            ]\n        )\n        self.layernorm_1 = layers.LayerNormalization()\n        self.layernorm_2 = layers.LayerNormalization()\n\n    def call(self, inputs):\n        # Casting the inputs to complex64\n        inp_complex = tf.cast(inputs, tf.complex64)\n        # Projecting the inputs to the frequency domain using FFT2D and\n        # extracting the real part of the output\n        fft = tf.math.real(tf.signal.fft2d(inp_complex))\n        proj_input = self.layernorm_1(inputs + fft)\n        proj_output = self.dense_proj(proj_input)\n       \n        layer_norm = self.layernorm_2(proj_input + proj_output)\n        return layer_norm","90b236ef":"class PositionalEmbedding(layers.Layer):\n    def __init__(self, sequence_length, vocab_size, embed_dim, **kwargs):\n        super(PositionalEmbedding, self).__init__(**kwargs)\n        self.token_embeddings = layers.Embedding(\n            input_dim=vocab_size, output_dim=embed_dim\n        )\n        self.position_embeddings = layers.Embedding(\n            input_dim=sequence_length, output_dim=embed_dim\n        )\n        self.sequence_length = sequence_length\n        self.vocab_size = vocab_size\n        self.embed_dim = embed_dim\n\n    def call(self, inputs):\n        length = tf.shape(inputs)[-1]\n        positions = tf.range(start=0, limit=length, delta=1)\n        embedded_tokens = self.token_embeddings(inputs)\n        embedded_positions = self.position_embeddings(positions)\n        return embedded_tokens + embedded_positions\n\n    def compute_mask(self, inputs, mask=None):\n        return tf.math.not_equal(inputs, 0)\n","75fd79b9":"def get_word2vec_model(config, inputs):\n    x = word2vec_vectozier(inputs)\n    x = PositionalEmbedding(config.sequence_length, config.vocab_size, config.embed_dim)(x)\n    x = FNetEncoder(config.embed_dim, config.latent_dim)(x)\n    x = layers.GlobalAveragePooling1D()(x)\n    x = layers.Dropout(0.5)(x)\n    for i in range(3):\n        x = layers.Dense(100, activation=\"relu\")(x)\n        x = layers.Dropout(0.3)(x)\n    return x","b8e0aacf":"def get_tfidf_model(config, inputs):\n    x = tfidf_vectozier(inputs)\n    x = layers.Dense(256, activation=\"relu\", kernel_regularizer=\"l2\")(x)\n    x = layers.Dense(100, activation=\"relu\", kernel_regularizer=\"l2\")(x)\n    return x","82e8be43":"def get_model(config):\n    inputs = keras.Input(shape=(None, ), dtype=\"string\", name=\"inputs\")\n    word2vec_x = get_word2vec_model(config, inputs)\n    tfidf_x = get_tfidf_model(config, inputs)\n    x = layers.Concatenate()([word2vec_x, tfidf_x])\n    output = layers.Dense(1, activation=\"sigmoid\")(x)\n    model = keras.Model(inputs, output, name=\"model\")\n    return model","13f51064":"model = get_model(config)","dd41e5c6":"model.summary()","7bced6fd":"keras.utils.plot_model(model, show_shapes=True)","22ac64db":"model.compile(\n    \"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\", tf.keras.metrics.AUC()]\n)","63c18f2b":"acc_checkpoint = keras.callbacks.ModelCheckpoint(config.best_acc_model_path, monitor=\"val_accuracy\",save_weights_only=True, save_best_only=True)\nauc_checkpoint = keras.callbacks.ModelCheckpoint(config.best_auc_model_path, monitor=\"val_auc\",save_weights_only=True, save_best_only=True)\nearly_stopping = keras.callbacks.EarlyStopping(patience=10)\nreduce_lr = keras.callbacks.ReduceLROnPlateau(patience=5, min_delta=1e-4, min_lr=1e-6)\nmodel.fit(train_ds, epochs=config.epochs, validation_data=valid_ds, callbacks=[acc_checkpoint, auc_checkpoint, reduce_lr], class_weight=class_weight)\nmodel.save_weights(config.lastest_model_path)","03fdc383":"from sklearn.metrics import classification_report\ny_pred = np.array(model.predict(valid_ds) > 0.5, dtype=int)\ncls_report = classification_report(y_val, y_pred)\nprint(cls_report)","0fc7f538":"test = pd.read_csv(\"\/kaggle\/input\/jigsaw-toxic-severity-rating\/comments_to_score.csv\")\nsample_submission = pd.read_csv(\"\/kaggle\/input\/jigsaw-toxic-severity-rating\/sample_submission.csv\")\ntest_ds = tf.data.Dataset.from_tensor_slices((test[\"text\"])).batch(config.batch_size).cache().prefetch(1)\nscores = []\nfor path in [config.best_acc_model_path, config.best_auc_model_path, config.lastest_model_path]:\n    model.load_weights(path)\n    score = model.predict(test_ds).reshape(-1)\n    scores.append(score)\nscore = np.mean(scores, axis=0)\nprint(score.shape)\nsample_submission[\"score\"] = rankdata(score, method='ordinal')\nsample_submission.to_csv(\"submission.csv\", index=False)\nsample_submission.head()","8c6c9366":"<a id=\"6.6\"><\/a>\n### 6.6 Model Training","4dec3323":"Let's visualize the Model.","3fa0081b":"<a id=\"6.2\"><\/a>\n### 6.2 Positional Embedding","d1e65da1":"<a id=\"4.\"><\/a>\n## 4. Import datasets","97cfe32a":"#### Classification Report","e504acc1":"<a id=\"3.\"><\/a>\n## 3. Setup","45995e6b":"<a id=\"6.3\"><\/a>\n### 6.3 Word2Vec FNet Model","ae2bb5a9":"<a id=\"5.\"><\/a>\n## 5. EDA & Preprocessing","5f1770de":"<a id=\"5.5\"><\/a>\n### 5.5 Train Validation Split","7664d8df":"<a id=\"5.1\"><\/a>\n### 5.1 Select Traning Data","31752588":"<a id=\"5.7\"><\/a>\n### 5.7  Calculate Class weight","308842bc":"<a id=\"5.4\"><\/a>\n### 5.4 Word2Vec Vectorization","1051b5b2":"<a id=\"6.7\"><\/a>\n### 6.7 Evaluation","18b36cc7":"<a id=\"5.2\"><\/a>\n### 5.2 Text Preprocessing Function ","c48e2945":"<a id=\"7.\"><\/a>\n## 7. Submission","f840da7d":"\n<a id=\"8.\"><\/a>\n## 8. References\n- [FNet: Mixing Tokens with Fourier Transforms](https:\/\/arxiv.org\/abs\/2105.03824v3)\n- [Attention Is All You Need](https:\/\/arxiv.org\/abs\/1706.03762v5)\n- [Text Generation using FNet](https:\/\/keras.io\/examples\/nlp\/text_generation_fnet\/)\n- [English-Spanish Translation: FNet](https:\/\/www.kaggle.com\/lonnieqin\/english-spanish-translation-fnet)","c591df09":"<a id=\"1.\"><\/a>\n## 1. Overview\nIn my previous notebooks, I build Jigsaw Toxicity Model with [FNet](https:\/\/www.kaggle.com\/lonnieqin\/jigsaw-toxicity-training-with-fnet) using Word2Vec Vectorizatoin and [DNN](https:\/\/www.kaggle.com\/lonnieqin\/tf-idf-vectorization-with-keras) using TFIDF Vectorzation. How about combining this two Models together? I will try it in this notebook.\n\nThis Model is a binary classfication Model, ranking of toxicity can be calcualated via probability of binary classficiation.\n\nI use dataset from [Toxic Comment Classification Challenge](https:\/\/www.kaggle.com\/c\/jigsaw-toxic-comment-classification-challenge) and combine with more toxic data in this dataset for training.\n\nI will keep track of three Model: Model with Best Accuracy, Model with Best AUC, and Latest Model. So I can use them for inference and try different ensemble method to get a better score.","1944de0e":"Let's see what this data look like.","8255b49a":"<a id=\"5.6\"><\/a>\n### 5.6 Create TensorFlow Dataset","d93a870f":"<a id=\"5.3\"><\/a>\n### 5.3 TF-IDF Vectorization","75f20859":"# Jigsaw Toxicity: Word2Vec+TFIDF Training\n## Table of Contents\n* [1. Overview](#1.)\n* [2. Configuration](#2.)\n* [3. Setup](#3.)\n* [4. Import datasets](#4.)\n* [5. EDA & Preprocessing](#5.)\n    * [5.1 Select trainng data](#5.1)\n    * [5.2 Text Preprocessing Function](#5.2)\n    * [5.3 TF-IDF Vectorization](#5.3)\n    * [5.4 Word2Vec Vectorization](#5.4)\n    * [5.5 Train Validation Split](#5.5)\n    * [5.6 Create TensorFlow Dataset](#5.6)\n    * [5.7 Calculate Class weight](#5.7)\n* [6. Model Development](#6.)\n    * [6.1 FNet Encoder](#6.1)\n    * [6.2 Positional Embedding](#6.2)\n    * [6.3 Word2Vec FNet Model](#6.3)\n    * [6.4 TFIDF DNN Model](#6.4)\n    * [6.5 The Whole Model](#6.5)\n    * [6.6 Model Training](#6.6)\n    * [6.7 Evaluation](#6.7)\n* [7. Submission](#7.)\n* [8. References](#8.)","fea4dfeb":"<a id=\"6.4\"><\/a>\n### 6.4 TFIDF DNN Model","89eed58f":"<a id=\"6.1\"><\/a>\n### 6.1 FNet Encoder","77e81314":"<a id=\"6.\"><\/a>\n## 6. Model Development","8a83f44a":"One of the way is to label `less_toxic` as 0 and `more_toxic` as 1, and FNet can get 0.749 score. I tried grouping the duplicated comment together and replace the label with average value, but got a worse 0.49 score instead. I also tried to convert the average value to a class value, but still can't learn any important information from it. So I am going to keep every variable we may use in the future to a data table.\n\n\nAnother way is to use external dataset from [Toxic Comment Classification Challenge](https:\/\/www.kaggle.com\/c\/jigsaw-toxic-comment-classification-challenge). Since there is a class imbalance problem, I also add more_toxic data from this dataset and label it as 1.","c9e732ca":"<a id=\"2.\"><\/a>\n## 2. Configuration","5778caa4":"<a id=\"6.5\"><\/a>\n### 6.5 The Whole Model","b53a30d8":"<font color=\"red\" size=\"3\">If you found it useful and would like to back me up, just upvote.<\/font>"}}