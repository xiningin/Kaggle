{"cell_type":{"254833a1":"code","c2b15796":"code","c1d958a4":"code","9d238e32":"code","f191246b":"code","3eaaa118":"code","40bb64e1":"code","b8fbb2cb":"code","7efb1a38":"code","6fec6944":"code","daa7c35a":"code","92c8bf76":"code","125fe1e2":"markdown","716c0f0d":"markdown","ee9a0c3e":"markdown","d54391e2":"markdown"},"source":{"254833a1":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n'''\n    \n'''\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\nimport glob\nglob.glob('..\/input\/flowers-recognition\/flowers\/flowers\/*')\n# Any results you write to the current directory are saved as output.","c2b15796":"import matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nimport numpy as np\nimport cv2\nimport glob\nimport tensorflow as tf\nfrom sklearn.utils import shuffle\nimport pandas as pd\nfrom sklearn.metrics import accuracy_score","c1d958a4":"def readImages():\n    images = []\n    folders = glob.glob('..\/input\/flowers-recognition\/flowers\/flowers\/*')\n    data = []\n    labels = []\n    for folder in folders:\n        img_path = glob.glob(folder+\"\/*\")\n        for img in img_path:\n            if img.split('.')[-1] != \"jpg\":\n                continue\n            im = cv2.imread(img)\n            b,g,r = cv2.split(im)\n            rgb_img = cv2.merge([r,g,b]) \n            data.append(cv2.resize(rgb_img, (100, 100)))\n            labels.append(img.split(\"\/\")[-2])\n    print(\"Normalizing Images and reshaping them into 100*100\")\n    return data, labels","9d238e32":"data, orig_labels = readImages()\ndata, orig_labels = shuffle(data, orig_labels)\nlabels = pd.get_dummies(orig_labels).values\ndata = np.array(data)\/255.0\ntotal_labels = [ 'daisy', 'dandelion', 'rose', 'sunflower','tulip']\nprint('Labels are:',total_labels)","f191246b":"plt.figure(figsize=(20,10))\ncolumns = 5\nfor i, image in enumerate(data[:10]):\n    plt.subplot(5 \/ columns + 1, columns, i + 1)\n    plt.imshow(image)\n    ","3eaaa118":"train_X, test_X, train_y, test_y = train_test_split(data,labels, test_size = .20)\nprint(\"Train Size : X:\",train_X.shape,\" Y:\",train_y.shape,\" \\nTest Size : X:\",test_X.shape,\" Y:\",test_y.shape)","40bb64e1":"tf.reset_default_graph()\ntraining_epoch = 10\nlearning_rate = 0.003\ninput_size = 100\nn_class = labels.shape[1]\nx = tf.placeholder('float32', [None, input_size, input_size, 3])\ny = tf.placeholder('float32', [None, n_class])\ntraining = tf.placeholder('bool')\nprint(\"Training Epoch : \", training_epoch, \"\\nLearning Rate : \", learning_rate,\"\\nInput Size : \",input_size, \"\\nClasses : \",n_class)","b8fbb2cb":"def model(x,training=False):\n\n    conv1 = tf.layers.conv2d(x, strides=1, filters=16, kernel_size=[3,3], padding='SAME', activation = tf.nn.relu, kernel_initializer=tf.contrib.layers.variance_scaling_initializer())\n    print(conv1)\n    conv1 = tf.layers.max_pooling2d(conv1, pool_size=[2,2], strides=2, padding='SAME')\n    print(conv1)\n    \n    conv1 = tf.layers.conv2d(conv1, strides=1, filters=32, kernel_size=[3,3], padding='SAME', activation = tf.nn.relu, kernel_initializer=tf.contrib.layers.variance_scaling_initializer())\n    print(conv1)\n    conv1 = tf.layers.max_pooling2d(conv1, pool_size=[2,2], strides=2, padding='SAME')\n    print(conv1)\n    \n    conv1 = tf.layers.conv2d(conv1, strides=2, filters=64, kernel_size=[3,3], padding='SAME', activation = tf.nn.relu, kernel_initializer=tf.contrib.layers.variance_scaling_initializer())\n    print(conv1)\n    conv1 = tf.layers.max_pooling2d(conv1, pool_size=[2,2], strides=2, padding='SAME')\n    print(conv1)\n    \n    fc1 = tf.contrib.layers.flatten(conv1)\n    print(fc1)\n    fc1 = tf.layers.dropout(fc1,rate=0.2,seed=24,training=training)\n    print(fc1)\n    fc1 = tf.layers.dense(fc1, 100, activation=tf.nn.relu)\n    print(fc1)\n    out = tf.layers.dense(fc1, n_class)\n    print(out)\n    return out\n    ","7efb1a38":"pred_y = model(x,training)\nget_out = tf.nn.softmax(pred_y)\ncost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits = pred_y, labels = y))\noptimize = tf.train.AdamOptimizer(learning_rate).minimize(cost)","6fec6944":"init = tf.global_variables_initializer()","daa7c35a":"sess = tf.Session()\nsess.run(init)\nbatch_size = 10\ntrain_loss_mean = []\nfor itr in range(training_epoch):\n    for batch in range(len(data)\/\/batch_size):\n        batch_x = data[batch*batch_size:min((batch+1)*batch_size,len(data))]\n        batch_y = labels[batch*batch_size:min((batch+1)*batch_size,len(data))]    \n        _, train_loss = sess.run([optimize, cost], feed_dict={x: batch_x, y: batch_y, training:True})\n        train_loss_mean.append(train_loss)\n        #print(loss)\n    train_pred = sess.run(pred_y, feed_dict={x:train_X,training:False})\n    train_acc = accuracy_score(np.argmax(train_pred,1), np.argmax(train_y,1))\n    \n    test_pred, test_loss = sess.run([pred_y, cost], feed_dict={x: test_X, y: test_y, training:False})\n    test_acc = accuracy_score(np.argmax(test_pred,1), np.argmax(test_y,1))\n    \n    print(\"\\n\\nEpoch :\",itr+1,\"\\tTraining Cost :\\t\",np.mean(train_loss_mean),\"\\tTrain Accuracy :\\t\",train_acc)\n    print(\"Epoch :\",itr+1,\"\\tTest Cost :\\t\",test_loss,\"\\tTest Accuracy :\\t\\t\",test_acc)","92c8bf76":"rand = np.random.randint(len(test_X))\nplt.imshow(test_X[rand])\npredict = sess.run(get_out, feed_dict={x:[test_X[rand]], training:False})\npredict = predict.reshape(-1)\nprint('True Value :',total_labels[np.argmax(test_y[rand])])\nprint('\\n\\nPredicted Values :\\n')\nsorted_predict, sorted_labels = zip(*sorted(zip(predict, total_labels),reverse=True))\nfor i in range(len(predict)):\n    print(sorted_labels[i],'{0:15.2f}'.format(float(sorted_predict[i])*100)+'%')\n\nindex = np.argmax(predict)","125fe1e2":"### Image Preprocessing","716c0f0d":"### Visualization Of Images","ee9a0c3e":"### Importing Libraries","d54391e2":"### Predicting Some Random Images From Dataset"}}