{"cell_type":{"926171d4":"code","ff414dd8":"code","83c9b0f4":"code","84cdadcc":"code","40390a3c":"code","b39397ed":"code","18474083":"code","53ab13a7":"code","34941dd6":"code","3bc96a19":"code","0032270a":"code","977d9715":"code","92ce998c":"code","b3c98190":"markdown","efe51504":"markdown","a0955f20":"markdown","030792f3":"markdown","87dc5b7e":"markdown","b79ed37e":"markdown","287c1a83":"markdown","6e9b9d92":"markdown","f71a8077":"markdown"},"source":{"926171d4":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","ff414dd8":"#our hypothesis function \n\ndef hypothesis(theta, X):\n    h = np.ones((X.shape[0],1))\n    for i in range(0,X.shape[0]):\n        x = np.concatenate((np.ones(1), np.array([X[i]])), axis = 0)\n        h[i] = float(np.matmul(theta, x))\n    h = h.reshape(X.shape[0])\n    return h","83c9b0f4":"#our SGD function\n\ndef SGD(theta, alpha, num_iters, h, X, y):\n    for i in range(0,num_iters):\n        theta[0] = theta[0] - (alpha) * (h - y)\n        theta[1] = theta[1] - (alpha) * ((h - y) * X)\n        h = theta[1]*X + theta[0] \n    return theta","84cdadcc":"#SGD function integrated with Linear Regression\n\ndef sgd_linear_regression(X, y, alpha, num_iters):\n    # initializing the parameter vector...\n    theta = np.zeros(2)\n    # hypothesis calculation....\n    h = hypothesis(theta, X)    \n    # returning the optimized parameters by Gradient Descent...\n    for i in range(0, X.shape[0]):\n        theta = SGD(theta,alpha,num_iters,h[i],X[i],y[i])\n    theta = theta.reshape(1, 2)\n    return theta","40390a3c":"#our BGD function\n\ndef BGD(theta, alpha, num_iters, h, X, y):\n    cost = np.ones(num_iters)\n    theta_0 = np.ones(num_iters)\n    theta_1 = np.ones(num_iters)\n    for i in range(0,num_iters):\n        theta[0] = theta[0] - (alpha\/X.shape[0]) * sum(h - y)\n        theta[1] = theta[1] - (alpha\/X.shape[0]) * sum((h - y) * X)\n        h = hypothesis(theta, X)\n        cost[i] = (1\/X.shape[0]) * 0.5 * sum(np.square(h - y))\n        theta_0[i] = theta[0]\n        theta_1[i] = theta[1]\n    theta = theta.reshape(1,2)\n    return theta, theta_0, theta_1, cost","b39397ed":"#BGD function integrated with linear Regression\n\ndef linear_regression(X, y, alpha, num_iters):\n    # initializing the parameter vector...\n    theta = np.zeros(2)\n    # hypothesis calculation....\n    h = hypothesis(theta, X)    \n    # returning the optimized parameters by Gradient Descent...\n    theta,theta_0,theta_1,cost= BGD(theta,alpha,num_iters,h,X,y)\n    return theta, theta_0, theta_1, cost","18474083":"# loading our uni-variate dataset\n\ndata = pd.read_csv('\/kaggle\/input\/dataset\/data.csv',header=None)\ndata.head()","53ab13a7":"# extracting features and labels\n\nX = data.iloc[:,0].values #the feature_set\ny = data.iloc[:,1].values#the labels","34941dd6":"#visualising my features and labels\n\nimport matplotlib.pyplot as plt\nplt.scatter(X,y)\nplt.xlabel('Population of City in 10,000s')\nplt.ylabel('Profit in $10,000s')","3bc96a19":"import matplotlib.pyplot as plt \n# getting the predictions...\ntheta = sgd_linear_regression(X, y, 0.0001, 10000)\ntraining_predictions = hypothesis(theta, X)\nscatter = plt.scatter(X, y, label=\"training data\")\nregression_line = plt.plot(X, training_predictions\n                           , label=\"linear regression\")\nplt.legend()\nplt.xlabel('X axis')\nplt.ylabel('y axis')\nplt.title('Regression line with SGD')","0032270a":"from sklearn.metrics import r2_score   #for performance analysis\nprint('R2 score for SGD',r2_score(y,training_predictions))","977d9715":"theta,theta_0,theta_1,cost=linear_regression(X,y,0.0001,250)\n\n#predictions\ntraining_predictions = hypothesis(theta, X)\nscatter = plt.scatter(X, y, label=\"training data\")\nregression_line = plt.plot(X, training_predictions, label=\"linear regression\")\nplt.legend()\nplt.xlabel('X axis')\nplt.ylabel('y axis')\nplt.title('Regression line with BGD')","92ce998c":"from sklearn.metrics import r2_score   #for performance analysis\nprint('R2 score for BGD',r2_score(y,training_predictions))","b3c98190":"There are 2 approaches of gradient descent. They are:\n\n* Stochastic Gradient Descent\n* Batch Gradient Descent ","efe51504":"**Stochastic Gradient Descent Algorithm** takes current values of theta_0 and theta_1, alpha, number of iterations (num_iters), hypothesis value(h), feature set (X) and Target Variable set (y) as input and outputs the optimized theta (theta_0 and theta_1) at each iteration characterized by an instance.\n\n**Batch Gradient Descent Algorithm** takes current values of theta_0 and theta_1, alpha, number of iterations (num_iters), list of hypothesis values of all samples(h), feature set (X) and Target Variable set (y) as input and outputs the optimized theta (theta_0 and theta_1), theta_0 history (theta_0) and theta_1 history (theta_1) i.e., the value of theta_0 and theta_1 at each iteration and finally the cost history which contains the value of the cost function over all the iterations. ","a0955f20":"**The hypothesis for Uni-Variate Linear Regression is given below:**\n\n![image.png](attachment:image.png)","030792f3":"# Conclusion\n\n**For uni-variate data, BGD is preferred over SGD because:**\n\n* BGD had a better performance over SGD\n* SGD took more time than BGD","87dc5b7e":"We will find the difference between the two","b79ed37e":"# Winner is Batch Gradient Descent (BGD)","287c1a83":"These 2 parameters, theta_0 and theta_1 has to assume such values that the value of this cost function (i.e., the cost) assumes a minimum value possible. \nSo, now we have to find the values of theta_0 and theta_1 for which the cost is minimum or to simply find the minima of the Cost Function.\n\n**Gradient Descent is one of the most prominent Convex Optimization Technique using which minima of the functions can be found. The Gradient Descent Algorithm is given below:**\n\n![image.png](attachment:image.png)","6e9b9d92":"*Cost function is dependent on theta values*\n\n**Cost function is defined as :**\n\n![image.png](attachment:image.png)","f71a8077":"**The above hypothesis can also be written in Matrix Multiplication format or in terms of Vector Algebra as:**\n\n![image.png](attachment:image.png)"}}