{"cell_type":{"42a3f709":"code","c83df770":"code","06cf6fc6":"code","1881004b":"code","bc9554d8":"code","4104e169":"code","0f450b6a":"code","042d69e1":"code","08270a05":"code","788e54a3":"code","70d35fda":"code","ee129cf4":"code","c3da2735":"code","deaf9735":"code","10d6101f":"code","d78a5c81":"code","c3723b19":"code","d32c7fbd":"code","ada10303":"code","203d58b1":"code","5a39e55f":"code","777954bf":"code","bbdcf635":"code","d5669c23":"code","f01dfee5":"code","9c839c51":"code","18d7e6a9":"code","0177e765":"code","d6733162":"code","b29e1c4c":"code","10ec1125":"markdown","bc705c48":"markdown","7ff22fee":"markdown","1d542677":"markdown","879667cb":"markdown","be5d1da1":"markdown","26f80340":"markdown","fef5215d":"markdown","d265e16b":"markdown","fd9932e8":"markdown","2c6bf3d3":"markdown"},"source":{"42a3f709":"!pip install -q tf-models-official==2.2.0\n!pip install -q nlp","c83df770":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport tensorflow as tf\n\nfrom official.nlp import bert\nfrom official import nlp\n\nimport official.nlp.bert.bert_models\nimport official.nlp.bert.tokenization\nimport official.nlp.bert.configs\nimport official.nlp.optimization\n\nimport nlp as an_nlp\n\nfrom kaggle_datasets import KaggleDatasets\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\nfrom collections import Counter\nimport json\nimport time\nimport unicodedata\nimport re\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","06cf6fc6":"GCS_PATH_TO_SAVEDMODEL = KaggleDatasets().get_gcs_path(\"multi-cased-l12-h768-a12\")","1881004b":"try:\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\nexcept ValueError:\n    strategy = tf.distribute.get_strategy()","bc9554d8":"seed = 123\ntf.random.set_seed(seed)\nnp.random.seed(seed)\n\nstart_time = time.time()","4104e169":"data = pd.read_csv('\/kaggle\/input\/contradictory-my-dear-watson\/train.csv')\ndata.head(10)","0f450b6a":"data.info()","042d69e1":"fig, ax = plt.subplots(figsize=(6, 15))\nsns.countplot(y='language', hue='label', data=data)","08270a05":"dataset = data[['premise', 'hypothesis', 'label']]","788e54a3":"multigenre_data = an_nlp.load_dataset(path='glue', name='mnli')","70d35fda":"multigenre_data","ee129cf4":"premise = []\nhypothesis = []\nlabel = []\n\nfor example in multigenre_data['train']:\n    premise.append(example['premise'])\n    hypothesis.append(example['hypothesis'])\n    label.append(example['label'])\n\nmultigenre_df = pd.DataFrame(data={\n    'premise': premise,\n    'hypothesis': hypothesis,\n    'label': label\n})","c3da2735":"adversarial_data = an_nlp.load_dataset(path='anli')","deaf9735":"premise = []\nhypothesis = []\nlabel = []\n\nfor example in adversarial_data['train_r1']:\n    premise.append(example['premise'])\n    hypothesis.append(example['hypothesis'])\n    label.append(example['label'])\n    \nfor example in adversarial_data['train_r2']:\n    premise.append(example['premise'])\n    hypothesis.append(example['hypothesis'])\n    label.append(example['label'])\n    \nfor example in adversarial_data['train_r3']:\n    premise.append(example['premise'])\n    hypothesis.append(example['hypothesis'])\n    label.append(example['label'])\n    \nadversarial_df = pd.DataFrame(data={\n    'premise': premise,\n    'hypothesis': hypothesis,\n    'label': label\n})","10d6101f":"train = dataset.sample(frac=0.99)\nval = dataset[~dataset.index.isin(train.index)]","d78a5c81":"train = pd.concat([train, multigenre_df, adversarial_df])\ntrain.info()","c3723b19":"tokenizer = bert.tokenization.FullTokenizer(vocab_file=os.path.join(GCS_PATH_TO_SAVEDMODEL, \"vocab.txt\"), do_lower_case=True, split_on_punc=True)\nprint('Vocab size', len(tokenizer.vocab))","d32c7fbd":"tokenizer.tokenize('and these comments were considered in formulat')","ada10303":"def unicode_to_ascii(s):\n    return ''.join(c for c in unicodedata.normalize('NFD', s) if unicodedata.category(c) != 'Mn')\n\ndef preprocessing_text(s):\n    s = unicode_to_ascii(s.lower().strip())\n    s = re.sub(r'[\" \"0-9]+', \" \", s)\n    s = s.rstrip().strip()\n    return s\n\ndef encode_sentence(s, tokenizer):\n    s = preprocessing_text(s)\n    tokens = list(tokenizer.tokenize(s))\n    tokens.append('[SEP]')\n    return tokenizer.convert_tokens_to_ids(tokens)\n\ndef bert_encode(glue_dict, tokenizer):\n    sentence1 = tf.ragged.constant([encode_sentence(s, tokenizer) for s in np.array(glue_dict['premise'])])\n    sentence2 = tf.ragged.constant([encode_sentence(s, tokenizer) for s in np.array(glue_dict['hypothesis'])])\n    \n    cls = [tokenizer.convert_tokens_to_ids(['[CLS]'])]*sentence1.shape[0]\n    input_word_ids = tf.concat([cls, sentence1, sentence2], axis=-1)\n    \n    input_mask = tf.ones_like(input_word_ids).to_tensor()\n    \n    type_cls = tf.zeros_like(cls)\n    type_s1 = tf.zeros_like(sentence1)\n    type_s2 = tf.ones_like(sentence2)\n    input_type_ids = tf.concat([type_cls, type_s1, type_s2], axis=-1).to_tensor()\n    \n    return {\n        \"input_word_ids\": input_word_ids.to_tensor(),\n        \"input_mask\": input_mask,\n        \"input_type_ids\": input_type_ids\n    }","203d58b1":"glue_train = bert_encode(train, tokenizer)\nglue_train_labels = train['label']\n\nglue_val = bert_encode(val, tokenizer)\nglue_val_labels = val['label']\n\nfor key, value in glue_train.items():\n    print(f\"{key:15s} shape: {value.shape}\")\nprint(f\"glue_train_labels shape: {glue_train_labels.shape}\")","5a39e55f":"bert_config_file = os.path.join(GCS_PATH_TO_SAVEDMODEL, 'bert_config.json')\nconfig_dict = json.loads(tf.io.gfile.GFile(bert_config_file).read())\nconfig_dict['attention_probs_dropout_prob'] = 0.1\nconfig_dict['hidden_dropout_prob'] = 0.1\n\nbert_config = bert.configs.BertConfig.from_dict(config_dict)\nprint(config_dict)","777954bf":"with strategy.scope():\n    multi_bert_classifier, bert_encoder = bert.bert_models.classifier_model(bert_config, num_labels=3, max_seq_length=None)\n\n    checkpoint = tf.train.Checkpoint(model=bert_encoder)\n    checkpoint.restore(os.path.join(GCS_PATH_TO_SAVEDMODEL, 'bert_model.ckpt')).assert_consumed()","bbdcf635":"epochs = 3\nbatch_size = 256\n\ntrain_data_size = len(glue_train_labels)\nsteps_per_epoch = int(train_data_size \/ batch_size)\nnum_train_steps = steps_per_epoch * epochs\nwarmup_steps = int(epochs * train_data_size * 0.1 \/ batch_size)","d5669c23":"with strategy.scope():\n    optimizer = nlp.optimization.create_optimizer(2e-5, num_train_steps=num_train_steps, num_warmup_steps=warmup_steps)\n\n    metrics = [tf.keras.metrics.SparseCategoricalAccuracy('accuracy', dtype=tf.float32)]\n    loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n\n    multi_bert_classifier.compile(optimizer=optimizer, loss=loss, metrics=metrics)","f01dfee5":"checkpoint_path = os.path.join(\"\/kaggle\/working\",\"tmp\/cp.ckpt\")\ncp_callback = tf.keras.callbacks.ModelCheckpoint(\n    filepath=checkpoint_path, \n    monitor='val_accuracy',\n    mode='max',\n    save_weights_only=True,\n    save_best_only=True)","9c839c51":"multi_bert_classifier.fit(glue_train, glue_train_labels, validation_data=(glue_val, glue_val_labels), batch_size=batch_size, epochs=epochs)","18d7e6a9":"test = pd.read_csv('\/kaggle\/input\/contradictory-my-dear-watson\/test.csv')\ntest.info()","0177e765":"data_language = dict(Counter(data['language']).most_common())\ntest_language = dict(Counter(test['language']).most_common())\n\nlanguage_name = np.concatenate([list(data_language.keys()), list(test_language.keys())])\nlanguage_name = list(set(language_name))\n\ndata_num = []\ntest_num = []\nfor index, value in enumerate(language_name):\n    if value in data_language.keys():\n        data_num.append(data_language[value])\n    else:\n        data_num.append(0)\n    \n    if value in test_language.keys():\n        test_num.append(test_language[value])\n    else:\n        test_num.append(0)\n\nlanguage_num = pd.DataFrame({'language': language_name, 'data': data_num, 'test': test_num})\n\nfig, ax = plt.subplots(figsize=(6, 8))\nsns.set_color_codes(\"pastel\")\nsns.barplot(x='data' , y='language', data=language_num, label=\"data\", color=\"b\")\nsns.set_color_codes(\"muted\")\nsns.barplot(x='test' , y='language', data=language_num, label=\"test\", color=\"b\")\nax.legend(ncol=2, loc=\"lower right\", frameon=True)\nsns.despine(left=True, bottom=True)\nplt.show()","d6733162":"glue_test = bert_encode(test, tokenizer)\n#multi_bert_classifier.load_weights(checkpoint_path)\npredictions = multi_bert_classifier.predict(glue_test)","b29e1c4c":"predictions = tf.math.argmax(predictions, axis=-1)\nresults = pd.DataFrame({'id': test['id'], 'prediction': predictions.numpy()})\nresults.to_csv('\/kaggle\/working\/submission.csv', index=False)","10ec1125":"### creates an optimizer with learning rate schedule","bc705c48":"# Augment dataset","7ff22fee":"# \u8bbe\u7f6e\u4f18\u5316\u5668","1d542677":"# read test","879667cb":"### set up epochs and steps","be5d1da1":"# split train and val","26f80340":"# load multilingual bert","fef5215d":"# Preprocessing text","d265e16b":"# \u6784\u5efa\u6a21\u578b","fd9932e8":"# set seed","2c6bf3d3":"# detect and init the TPU and instantiate a distribution strategy"}}