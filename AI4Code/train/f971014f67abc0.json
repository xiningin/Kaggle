{"cell_type":{"6e39e7fe":"code","4629a126":"code","406ec30d":"code","a1ac95c7":"code","b13f19d4":"code","2a83613a":"code","115b9e1e":"code","45c5c8aa":"code","cb53ce22":"code","fc37dbcc":"code","ff3aea66":"code","a260c783":"code","8a6441de":"code","791ea3ca":"code","98f2e699":"code","5f53714d":"code","0872d308":"code","59bb5bf4":"code","0842e777":"code","e9e91c9c":"code","ae705b1d":"code","aa664efb":"code","a744241c":"code","769e73b3":"code","2b303633":"code","2ed169b4":"code","04015c05":"code","c3483b3c":"code","f8cc7d7c":"code","a279160f":"code","6ed10f84":"code","bc290455":"code","91abe57e":"code","4c6e934a":"markdown","20b06496":"markdown","06795cbd":"markdown","08902f01":"markdown","f249a62a":"markdown","59dd59d3":"markdown","4a41fd3a":"markdown","19c6cfad":"markdown","b374a5ac":"markdown","b5b3443c":"markdown","2756db9a":"markdown","e6c990e3":"markdown","6ef5685d":"markdown","1cee11e0":"markdown"},"source":{"6e39e7fe":"#import\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nimport tensorflow.keras\nfrom  tensorflow.keras.optimizers import  Adam\nfrom  tensorflow.keras.models import Sequential, Model, model_from_json\nfrom  tensorflow.keras.layers import Dense, Conv2D, Activation, MaxPool2D, Flatten, Dropout, BatchNormalization\nfrom  tensorflow.keras.utils import load_img, img_to_array, array_to_img, to_categorical\nfrom  tensorflow.keras.preprocessing import image_dataset_from_directory\nfrom  tensorflow.keras.callbacks import ModelCheckpoint","4629a126":"# Reading dataset with pandas and import to notebook\ndata= pd.read_csv(\"..\/input\/facial-expression-recognitionferchallenge\/fer2013\/fer2013\/fer2013.csv\") \n\n# checking data size\ndata.shape","406ec30d":"# shows all columns and first 5 rows of dataset\ndata.head() ","a1ac95c7":"# The dataset is divided into three as Training, PuplicTest and PrivateTest. Let's count them..\n\ndata[\"Usage\"].value_counts()","b13f19d4":"# Let's take the data separated as \"training\" in the dataset. We will carry out our training process on this data.\n\nnp.unique(data[\"Usage\"].values.ravel())  # --np.ravel-- Return a contiguous flattened array.\n\ntrain_data= data[data.Usage==\"Training\"]  # let's assign the training examples to another variable\n\ntrain_data.shape  # let's check the number of training data","2a83613a":"# In the fer2013 dataset, the pixel values in the \"pixels\" column were created with spaces between them. let's arrange\n\ntrain_pixels= train_data.pixels.str.split(\" \").tolist() # We remove the spaces and add the pixel values to the list.\n\ntrain_pixels= pd.DataFrame(train_pixels, dtype= int) # let's convert the pixel list to dataframe","115b9e1e":"train_images = train_pixels.values #Let's assign the pixel values to a new variable\n\ntrain_images = train_images.astype(np.float)  #Let's change the type of the variable\n\nprint(train_images.shape)  #let's see the variable size","45c5c8aa":"#edit figure size\nplt.figure(figsize=(14,7))  \n\n# Let's transform the pixel values into a 48x48 image and visualize it. \n#i used for loop to visualize top 10 data from dataset\n\nfor i in range (10):\n  plt.subplot(2,5,1+i)\n  plt.imshow(train_images[10+i].reshape(48,48), cmap=\"gray\")\n","cb53ce22":"#Let's determine how many classes are in the training set. namely different emotion labeled\n\ntrain_labels_flat= train_data[\"emotion\"].values.ravel()  #--np.ravel-- Return a contiguous flattened array.\ntrain_labels_count= np.unique(train_labels_flat).shape[0] # how many unique labels are there\nprint(\"number of label :\",train_labels_count )","fc37dbcc":"# Let's determine the class of the data in the training set with onehot.\n# Let the emotion of the data be 1 and the remaining emotions be 0.\n\ndef dense_to_one_hot (labels_dense, num_classes):\n  num_labels= labels_dense.shape[0]\n  index_offset = np.arange(num_labels)*num_classes\n  labels_one_hot= np.zeros((num_labels, num_classes))\n  labels_one_hot.flat[index_offset + labels_dense.ravel()]=1\n  return labels_one_hot","ff3aea66":"# Let's determine the class values of the training data\ny_train = dense_to_one_hot(train_labels_flat, train_labels_count)\n\ny_train = y_train.astype(np.uint8)\n\nprint(y_train.shape)\nprint(y_train[15])","a260c783":"# Let's take the data separated as \"PuplicTest\" in the dataset. We will carry out our test process on this data.\n\nnp.unique(data[\"Usage\"].values.ravel())  # --np.ravel-- Return a contiguous flattened array.\n\ntest_data= data[data.Usage==\"PublicTest\"]  # let's assign the test examples to another variable\n\ntest_data.shape  # let's check the number of test data","8a6441de":"# In the fer2013 dataset, the pixel values in the \"pixels\" column were created with spaces between them. let's arrange\n\ntest_pixels= test_data.pixels.str.split(\" \").tolist() # We remove the spaces and add the pixel values to the list.\n\ntest_pixels= pd.DataFrame(test_pixels, dtype= int) # let's convert the pixel list to dataframe\n","791ea3ca":"test_images = test_pixels.values #Let's assign the pixel values to a new variable\n\ntest_images = test_images.astype(np.float)  #Let's change the type of the variable\n\nprint(test_images.shape)  #let's see the variable size","98f2e699":"#edit figure size\nplt.figure(figsize=(14,7))  \n\n# Let's transform the pixel values into a 48x48 image and visualize it. \n#i used for loop to visualize top 10 data from dataset\n\nfor i in range (10):\n  plt.subplot(2,5,1+i)\n  plt.imshow(test_images[i].reshape(48,48), cmap=\"gray\")","5f53714d":"#Let's determine how many classes are in the training set. namely different emotion labeled\n\ntest_labels_flat= test_data[\"emotion\"].values.ravel()  #--np.ravel-- Return a contiguous flattened array.\ntest_labels_count= np.unique(test_labels_flat).shape[0] # how many unique labels are there\nprint(\"number of label :\",test_labels_count )","0872d308":"# Let's determine the class values of the training data\ny_test = dense_to_one_hot(test_labels_flat, test_labels_count)\n\ny_test = y_test.astype(np.uint8)\n\nprint(y_test.shape)\nprint(y_test[15])","59bb5bf4":"from tensorflow.python.keras import activations\nmodel= Sequential()  #Create an empty neural network model and assign it to variable \"model\"\n\n# first layer\n\n# \"channels_last\" corresponds to inputs with shape (batch_size, height, width, channels) \n# \"input_shape: (height, width, channels)\"\nmodel.add(Conv2D(64,3, data_format=\"channels_last\", kernel_initializer = \"he_normal\", input_shape= (48, 48, 1))) # filters: 64, kernel_size :3\nmodel.add(BatchNormalization()) #Batch normalization applies a transformation that maintains the mean output close to 0 and the output standard deviation close to 1.\nmodel.add(Activation(\"relu\"))\n\n\n# Second layer\n\nmodel.add(Conv2D(64,3))\nmodel.add(BatchNormalization())\nmodel.add(Activation(\"relu\"))\nmodel.add(MaxPool2D(pool_size=(2,2), strides=1)) #Downsamples the input along its spatial dimensions (height and width) by taking the maximum value over an input window (of size defined by pool_size) for each channel of the input.\nmodel.add(Dropout(0.6)) #The Dropout layer randomly sets input units to 0 with a frequency of rate at each step during training time, which helps prevent overfitting.\n\n\n# Third layer\n\nmodel.add(Conv2D(32,3))\nmodel.add(BatchNormalization())\nmodel.add(Activation(\"relu\"))\n\n\n# Fourth layer\n\n# fifth layer\n\nmodel.add(Conv2D(32,3))\nmodel.add(BatchNormalization())\nmodel.add(Activation(\"relu\"))\nmodel.add(MaxPool2D(pool_size=(2,2), strides=1)) \nmodel.add(Dropout(0.5))\n\n\n# Full connected layers\nmodel.add(Flatten()) #Flattens the input. Does not affect the batch size.\nmodel.add(Dense(128))\nmodel.add(BatchNormalization())\nmodel.add(Activation('relu'))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(64))\nmodel.add(BatchNormalization())\nmodel.add(Activation('relu'))\nmodel.add(Dropout(0.5))\n\n\n# output layer\nmodel.add(Dense(train_labels_count))\nmodel.add(Activation(\"softmax\"))\nmodel.summary()\n","0842e777":"#compile\n\noptimizer = Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.999) # let's use  \"Adam\" optimizer and edit optimization parameters\n\nmodel.compile(loss=\"categorical_crossentropy\", optimizer= optimizer, metrics=[\"accuracy\"]) #loss function: \"categorical_crossentropy\", ","e9e91c9c":"# Let's bring the training and test datasets into a format suitable for training and prediction operations\n\nx_train= train_images.reshape(train_images.shape[0],48,48,1)\nx_test= test_images.reshape(test_images.shape[0],48,48,1)\n\nprint(\"x_train shape :\", x_train.shape)\nprint(\"y_train shape :\", y_train.shape) # number of classes\nprint(\"x_test shape :\", x_test.shape)\nprint(\"y_test shape :\", y_test.shape) # number of classes","ae705b1d":"# save most successful weights (\"accuracy\")\n\ncheckpointer = ModelCheckpoint(filepath = \".\/face_model.h5\", verbose=1, save_best_only=True) \ncallbacks_list = [checkpointer]\n\n# hyperparameters\nepochs= 15\nbatch_size= 150\n","aa664efb":"# run the model\n\nhist = model.fit(x_train, y_train,\n                 epochs=epochs,\n                 shuffle=True,\n                 batch_size=batch_size,\n                 validation_data=(x_test, y_test),\n                 callbacks= callbacks_list,\n                 verbose=1)","a744241c":"# save model to json\nmodel_json = model.to_json()\nwith open('.\/face_model.json', 'w') as json_file:\n  json_file.write(model_json)","769e73b3":"#visualize\n\nplt.figure(figsize=(7,4))\n\nplt.plot(hist.history[\"accuracy\"], label= \"train acc\", color=\"g\")\nplt.plot(hist.history[\"val_accuracy\"], label= \"validation acc\",color=\"r\")\nplt.legend()\nplt.title(\"Model Accuracy\")\nplt.xlabel(\"Epochs\")\nplt.ylabel(\"Accuracy\")\nplt.savefig('face_recognition_accuracy.png')\nplt.show()\n\nplt.figure(figsize=(7,4))\nplt.plot(hist.history[\"loss\"], label= \"train loss\",color=\"g\")\nplt.plot(hist.history[\"val_loss\"], label= \"validation loss\",color=\"r\")\nplt.legend()\nplt.title(\"Model Loss\")\nplt.xlabel(\"Epochs\")\nplt.ylabel(\"Loss(Cost)\")\nplt.savefig('face_recognition_loss.png')\nplt.show()\n\n\nplt.show()","2b303633":"#Saving\nmodel_test=model.save(\".\/face_model.h5\")","2ed169b4":"# We will evaluate model performance for privatetest data\n\ntest= data[[\"emotion\",\"pixels\"]][data[\"Usage\"]== \"PrivateTest\"] #Let's specify the PrivateTest data\ntest[\"pixels\"] = test[\"pixels\"].apply(lambda im: np.fromstring (im, sep= \" \")) # --numpy.fromstring- A new 1-D array initialized from text data in a string.\ntest.head()","04015c05":"x_test_private = np.vstack(test[\"pixels\"].values) # Stack arrays in sequence vertically (row wise).\ny_test_private = np.array(test[\"emotion\"]) # convert \"emotion\" data to array","c3483b3c":"# Let's convert the arrays to appropriate sizes\n\nx_test_private = x_test_private.reshape(-1, 48,48,1) \ny_test_private = to_categorical(y_test_private) \n\nprint(\"x_test_private shape :\", x_test_private.shape)\nprint(\"y_test_private shape :\", y_test_private.shape)","f8cc7d7c":"score = model.evaluate(x_test_private,y_test_private, verbose=0) # evaluating dataset\n\nprint(\" PrivateTest loss: \", score[0]) # PrivateTest Loss\nprint(\" PrivateTest Accuracy: \",\"%\", score[1]*100, ) # PrivateTest Accuracy","a279160f":"#let's load the modules to load and size the images\n\nfrom tensorflow.keras.models import load_model\nfrom PIL import Image\nfrom tensorflow.keras.preprocessing import image","6ed10f84":"# we recorded the best weights obtained in the training process. \n# We will use these weights\n\nmodel_best= load_model(\".\/face_model.h5\")","bc290455":"# let's upload face photo for emotion recognition\n\n\nreal_image_path= \"..\/input\/emotion-recognition-images\/sener_sen.jpg\" #file location of image\n\noriginal_image= image.load_img(real_image_path) #original image\n\nreal_image= image.load_img(real_image_path, target_size= (48,48), grayscale=True) # upload photo and convert to grayscale\n\nreal_data= image.img_to_array(real_image) # convert image to array\n\nreal_data= np.expand_dims(real_data, axis=0)  # -- np.expand_dims--Insert a new axis that will appear at the axis position in the expanded array shape.\nreal_data= np.vstack([real_data]) # # Stack arrays in sequence vertically (row wise).\n\nresults= model_best.predict(real_data, batch_size) #predict the image, add predictions to \"results\" list\n\nplt.imshow(original_image) #Let's show you the original photo\nplt.title(\"Original image\") \nplt.axis(\"off\")\nplt.show()","91abe57e":"# Visualize\n\nclass_name=[\"Angry\", \"Disgust\", \"Fear\", \"Happy\", \"Sad\", \"Surprised\", \"Natural\"] # emotion class names\n\nemotion= class_name[np.argmax(results)] # highest rate emotion\n\nplt.figure(figsize=(14,8)) # Figure size\nplt.plot(class_name, results[0], label= emotion, linewidth=5, markersize=12) # draw the graph\nplt.legend(fontsize=14)\nplt.xlabel(\"Emotions\", fontsize=20) # x axis name\nplt.ylabel(\"classification score\", fontsize=20) # y axis name\nplt.xticks(rotation= 45, fontsize= 14)\nplt.yticks(fontsize= 14)\nplt.show()\n","4c6e934a":"This study was carried out to determine emotional states from facial photographs using facial expression recognition (fer2013)  dataset.","20b06496":"**Visualizing and examining the dataset**","06795cbd":"**Train Data preprocessing**","08902f01":"**Traning**","f249a62a":"# **Real World Samples**","59dd59d3":"# **Loading the dataset**","4a41fd3a":"# **Creating a deep convolutional neural network model**","19c6cfad":"**Test emotion labels**","b374a5ac":"**Train emotion labels**","b5b3443c":"# **Test Data preprocessing**\n\nwe will use \"PuplicTest\" data to test the deep learning model","2756db9a":"# PrivateTest","e6c990e3":"**Test Dataset Visualizing**","6ef5685d":"# **Installing related packages**\n\nUploading the necessary modules for data preparation, visualization and training to the notebook","1cee11e0":"# **Train Datasets Visualizing**"}}