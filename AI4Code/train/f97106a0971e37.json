{"cell_type":{"129be02c":"code","853cb68f":"code","9b79a137":"code","6504e64e":"code","82ff91ef":"code","8fd120ac":"code","9a69e0ff":"code","d1ba529e":"code","191a9b11":"code","40a646a3":"code","8a0de87e":"code","054d8004":"code","851955fb":"code","9d6d594f":"code","c9a308f9":"code","7706951a":"code","8b0e8fc7":"code","5b35dd21":"code","6e0f3236":"code","3b5be47f":"code","8e2649ef":"code","5405b1af":"code","f2a625d1":"code","0b7caaf1":"code","0eb8e285":"code","927606f3":"code","6af6474d":"code","78e63e11":"code","a1125b34":"code","17be9b68":"code","98d231c5":"code","65849ae8":"code","d41693d9":"code","f743d6ed":"code","2bcc7f68":"code","62d2db9f":"code","3d1ae0b5":"code","0ca59d8d":"code","e8f0bb23":"code","bdc09dad":"code","58f3bc73":"code","aeeebb47":"code","33e00af3":"code","8de192de":"code","9b448cf2":"code","a70dbb71":"code","9359f510":"code","70466529":"markdown","89c1d5f2":"markdown","faee1dac":"markdown","dcd90e24":"markdown","64342140":"markdown","20a8783e":"markdown","cb3e79c8":"markdown","959105c4":"markdown","40274b8e":"markdown","9c8ee9a3":"markdown","8e3c9441":"markdown","41539413":"markdown","6ec59d3f":"markdown","f88560a8":"markdown","d1a858ce":"markdown","d0de73ea":"markdown","ffb00287":"markdown","e2a6f365":"markdown","cebd40fa":"markdown","234da83e":"markdown","dee1a0cc":"markdown","72ead5c5":"markdown","b278b62b":"markdown","b86d144b":"markdown","96de52b3":"markdown","ba940e30":"markdown","a4d8b35c":"markdown","51449e14":"markdown","fcce5f24":"markdown","65c448e3":"markdown","580db04a":"markdown","e2ffa52e":"markdown","33d6caa1":"markdown","8091d205":"markdown","ec7b8f09":"markdown"},"source":{"129be02c":"# Computational imports\nimport numpy as np   # Library for n-dimensional arrays\nimport pandas as pd  # Library for dataframes (structured data)\n\n# Helper imports\nimport os \nimport re\nimport warnings\nfrom tqdm import tqdm\nimport datetime as dt\n\n# ML\/DL imports\nfrom sklearn.preprocessing import MinMaxScaler, OrdinalEncoder, LabelEncoder\nfrom sklearn.metrics import mean_squared_error, mean_squared_log_error\nfrom sklearn.model_selection import train_test_split\n\nimport tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import LSTM, Dense, Dropout, RepeatVector, TimeDistributed\nfrom tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n\n# Plotting imports\nimport matplotlib.pyplot as plt\nimport matplotlib.dates as dates\nimport plotly.express as px\nimport plotly.graph_objects as go\nimport plotly.figure_factory as ff\nfrom plotly.subplots import make_subplots\nfrom plotly.offline import init_notebook_mode, iplot\n\n%matplotlib inline\nwarnings.filterwarnings(\"ignore\")\ninit_notebook_mode(connected=True)\n\n# Set seeds to make the experiment more reproducible.\nfrom numpy.random import seed\nseed(1)\n\n# Allows us to see more information regarding the DataFrame\npd.set_option(\"display.max_rows\", 500)\npd.set_option(\"display.max_columns\", 500)","853cb68f":"def show_shapes(Sequences, Targets): # this'll use inputs; can make yours to use local variable values\n    print(\"Expected: (num_samples, timesteps, channels)\")\n    print(\"Sequences: {}\".format(Sequences.shape))\n    print(\"Targets:   {}\".format(Targets.shape))   ","9b79a137":"def downcast_dtypes(df):\n    float_cols = [c for c in df if df[c].dtype == \"float64\"]\n    int_cols = [c for c in df if df[c].dtype in [\"int64\", \"int32\"]]\n    df[float_cols] = df[float_cols].astype(np.float32)\n    df[int_cols] = df[int_cols].astype(np.int16)\n    return df","6504e64e":"def basic_eda(df):\n    print(\"-------------------------------TOP 5 RECORDS-----------------------------\")\n    print(df.head(5))\n    print()\n    \n    print(\"-------------------------------INFO--------------------------------------\")\n    print(df.info())\n    print()\n    \n    print(\"-------------------------------Describe----------------------------------\")\n    print(df.describe())\n    print()\n    \n    print(\"-------------------------------Columns-----------------------------------\")\n    print(df.columns)\n    print()\n    \n    print(\"-------------------------------Data Types--------------------------------\")\n    print(df.dtypes)\n    print()\n    \n    print(\"----------------------------Missing Values-------------------------------\")\n    print(df.isnull().sum())\n    print()\n    \n    print(\"----------------------------NULL values----------------------------------\")\n    print(df.isna().sum())\n    print()\n    \n    print(\"--------------------------Shape Of Data---------------------------------\")\n    print(df.shape)\n    print()\n    \n    print(\"============================================================================ \\n\")","82ff91ef":"def split_sequences(sequences, timesteps, horizon):\n    Sequences, Targets = list(), list()\n    for i in range(len(sequences)):\n        # find the end of this pattern\n        end_ix = i + timesteps\n        out_end_ix = end_ix + horizon-1\n        # check if we are beyond the dataset\n        if out_end_ix > len(sequences):\n            break\n        # gather input and output parts of the pattern\n        seq_x, seq_y = sequences[i:end_ix, :-1], sequences[end_ix-1:out_end_ix, -1]\n        Sequences.append(seq_x)\n        Targets.append(seq_y)\n        show_shapes()\n    return array(X), array(y)","8fd120ac":"def transform(data):\n    \n    nan_features = ['event_name_1', 'event_type_1', 'event_name_2', 'event_type_2']\n    for feature in nan_features:\n        data[feature].fillna('unknown', inplace = True)\n        \n    cat = ['event_name_1','event_type_1','event_name_2','event_type_2','snap_CA','snap_TX','snap_WI']\n    for feature in cat:\n        encoder = LabelEncoder()\n        data[feature] = encoder.fit_transform(data[feature])\n    \n    return data","9a69e0ff":"def Normalize(list):\n    list = np.array(list)\n    low, high = np.percentile(list, [0, 100])\n    delta = high - low\n    if delta != 0:\n        for i in range(0, len(list)):\n            list[i] = (list[i]-low)\/delta\n    return  list,low,high\n\ndef FNoramlize(list,low,high):\n    delta = high - low\n    if delta != 0:\n        for i in range(0, len(list)):\n            list[i] = list[i]*delta + low\n    return list\n\ndef Normalize2(list,low,high):\n    list = np.array(list)\n    delta = high - low\n    if delta != 0:\n        for i in range(0, len(list)):\n            list[i] = (list[i]-low)\/delta\n    return  list","d1ba529e":"path = '..\/input\/m5-forecasting-accuracy\/'\n\ntrain_data = pd.read_csv(path+'sales_train_validation.csv')\ncalendar = pd.read_csv('\/kaggle\/input\/m5-forecasting-accuracy\/calendar.csv')\nsell_prices = pd.read_csv('\/kaggle\/input\/m5-forecasting-accuracy\/sell_prices.csv')\nsubmission_file = pd.read_csv('\/kaggle\/input\/m5-forecasting-accuracy\/sample_submission.csv')\n","191a9b11":"train_data.describe()","40a646a3":"days = range(1, 1970)\ntime_series_columns = [f'd_{i}' for i in days]\ntransfer_cal = pd.DataFrame(calendar[['event_name_1','event_type_1','event_name_2','event_type_2','snap_CA','snap_TX','snap_WI']].values.T, index=['event_name_1','event_type_1','event_name_2','event_type_2','snap_CA','snap_TX','snap_WI'], columns= time_series_columns)\ntransfer_cal = transfer_cal.fillna(0)\nevent_name_1_se = transfer_cal.loc['event_name_1'].apply(lambda x: x if re.search(\"^\\d+$\", str(x)) else np.nan).fillna(10)\nevent_name_2_se = transfer_cal.loc['event_name_2'].apply(lambda x: x if re.search(\"^\\d+$\", str(x)) else np.nan).fillna(10)","8a0de87e":"transfer_cal.head()","054d8004":"event_name_1_se.head()","851955fb":"calendar['date'] = pd.to_datetime(calendar['date'])\ncalendar = calendar[calendar['date']>= '2016-2-01']  # reduce memory\ncalendar= transform(calendar)\n# Attempts to convert events into time series data.\ntransfer_cal = pd.DataFrame(calendar[['event_name_1','event_type_1','event_name_2','event_type_2','snap_CA','snap_TX','snap_WI']].values.T,\n                            index=['event_name_1','event_type_1','event_name_2','event_type_2','snap_CA','snap_TX','snap_WI'])\ntransfer_cal","9d6d594f":"price_fea = calendar[['wm_yr_wk','date']].merge(sell_prices, on = ['wm_yr_wk'], how = 'left')\nprice_fea['id'] = price_fea['item_id']+'_'+price_fea['store_id']+'_validation'\nprice_fea.head()","c9a308f9":"df = price_fea.pivot('id','date','sell_price')\ndf.head()","7706951a":"price_df = train_data.merge(df,on=['id'],how= 'left').iloc[:,-140:] # -145: starts dataframe column at 2016-01-27 \nprice_df.index = train_data.id\nprice_df.head()","8b0e8fc7":"train_data.info()","5b35dd21":"train_data = downcast_dtypes(train_data)\ntrain_data.info()","6e0f3236":"train_data = train_data.iloc[:, -140:]\ntrain_data.head(10)","3b5be47f":"time_series_col1 = train_data.columns\ntime_series_col2 = price_df.columns\ntime_series_col3 = transfer_cal.columns\n\nprint(len(time_series_col1),len(time_series_col2),len(time_series_col3))","8e2649ef":"price_df.columns = time_series_col1\ntransfer_cal.columns = time_series_col1\n\ntrain_data.shape, price_df.shape, transfer_cal.shape","5405b1af":"full_train_data = pd.concat([train_data, transfer_cal, price_df], axis=0)\nfull_train_data.tail(10)","f2a625d1":"# Litle bit of exploration of data\nprint(\"=================================train_data=================================\")\nbasic_eda(full_train_data)","0b7caaf1":"full_train_data.fillna(method='backfill', axis=1, inplace=True)\nnp.sum(full_train_data.isnull().sum())","0eb8e285":"full_train_data_transposed = full_train_data.T\nfull_train_data_transposed.head()","927606f3":"object_cols = [cname for cname in full_train_data_transposed.columns \n               if full_train_data_transposed[cname].dtype == \"object\" \n               and cname != \"date\"]\n\nprint(\"Categorical Columns:\")\nlen(object_cols)","6af6474d":"num_cols = [cname for cname in full_train_data_transposed.columns \n            if full_train_data_transposed[cname].dtype in ['int64', 'float64', 'int16', 'float32']\n            and cname not in ['event_name_1','event_type_1','event_name_2','event_type_2','snap_CA','snap_TX','snap_WI']]\n\nprint(\"Numerical Columns:\")\nlen(num_cols)","78e63e11":"all_cols = num_cols + object_cols\nprint(\"All Columns:\")\nlen(all_cols)","a1125b34":"timesteps = 28\nhorizon = 28\n\nfull_train_data_sequenced = []   \n\nfor i in tqdm(range(train_data.shape[0])):      # Using tqdm to visualize the progress\n\n    full_train_data_sequenced.append([list(t) for t in zip(full_train_data_transposed['event_name_1'][-(100+14):-(14)],\n                                       full_train_data_transposed['event_type_1'][-(100+14):-(14)],\n                                       full_train_data_transposed['event_name_2'][-(100+14):-(14)],     \n                                       full_train_data_transposed['event_type_2'][-(100+14):-(14)],\n                                       full_train_data_transposed['snap_CA'][-(100+14):-(14)],\n                                       full_train_data_transposed['snap_TX'][-(100+14):-(14)],\n                                       full_train_data_transposed['snap_WI'][-(100+14):-(14)],\n                                       price_df.iloc[i][-100:],\n                                       train_data.iloc[i][-100:])]) \n\nfull_train_data_sequenced = np.asarray(full_train_data_sequenced, dtype=np.float32)","17be9b68":"norm_full_train_data, train_low, train_high = Normalize(full_train_data_sequenced[:,-(timesteps*2):,:])","98d231c5":"print(norm_full_train_data.shape)\nprint(train_low)\nprint(train_high)","65849ae8":"num_features = 9\n\nX_train = norm_full_train_data[:,-28*2:-28,:]\ny_train = norm_full_train_data[:,-28:,8] \n\nX_train = X_train.reshape((X_train.shape[0], X_train.shape[1], num_features))\ny_train = y_train.reshape((y_train.shape[0], y_train.shape[1], 1))\n\nshow_shapes(X_train, y_train)","d41693d9":"def encoder_decoder_model():\n    \n    # Use Keras sequential model\n    model = Sequential()\n    \n    # Encoder LSTM layer with Dropout regularisation; Set return_sequences to False since we are feeding last output to decoder layer\n    model.add(LSTM(units = 100, activation='relu', input_shape = (X_train.shape[1], X_train.shape[2])))\n    model.add(Dropout(0.2))\n    \n    # The fixed-length output of the encoder is repeated, once for each required time step in the output sequence with the RepeatVector wrapper\n    model.add(RepeatVector(horizon))\n    \n    # Decoder LSTM layer with Dropout regularisation; Set return_sequences to True to feed each output time step to a Dense layer\n    model.add(LSTM(units = 100, activation='relu', return_sequences=True))\n    model.add(Dropout(0.2))\n    \n    # Same dense layer is repeated for each output timestep with the TimeDistributed wrapper\n    model.add(TimeDistributed(Dense(units=1, activation = \"linear\")))\n    \n    return model","f743d6ed":"model = encoder_decoder_model()\nmodel.summary()","2bcc7f68":"model.compile(optimizer='adam', loss='mean_squared_error', metrics = ['accuracy'])","62d2db9f":"his=model.fit(X_train,y_train,epochs=15,batch_size=1000,verbose=2)","3d1ae0b5":"plt.plot(his.history['loss'])\nplt.plot(his.history['accuracy'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['loss','accuracy'])\nplt.show()","0ca59d8d":"# # serialize model to JSON\n# model_json = model.to_json()\n# with open(\"model2.json\", \"w\") as json_file:\n#     json_file.write(model_json)\n# # serialize weights to HDF5\n# model.save_weights(\"model2.h5\")\n# print(\"Saved model to disk\")","e8f0bb23":"# # load json and create model\n# json_file = open('model.json', 'r')\n# loaded_model_json = json_file.read()\n# json_file.close()\n# loaded_model = model_from_json(loaded_model_json)\n# # load weights into new model\n# loaded_model.load_weights(\"model.h5\")\n# print(\"Loaded model from disk\")","bdc09dad":"# Take last 28 days in the past to predict the next 28 days in the future\ntest_input = np.array(X_train[:, -timesteps:, :]) # Here timesteps=28\ntest_input = test_input.reshape((X_train.shape[0], timesteps, num_features)) # Very important to reshape to assure that the test input has the correct shape (# samples, # timesteps, # features)\nprint(test_input.shape)\n\n# Predict the next 28 days \ny_test = model.predict(test_input[:,-timesteps:, :], verbose=2)\n\n# Concatenate prediction with past timesteps\ntest_forecast= np.concatenate((test_input[:,:,8].reshape(test_input.shape[0],test_input.shape[1]), \n                           y_test.astype(np.float32).reshape(test_input.shape[0],test_input.shape[1])),axis=1).reshape((test_input.shape[0],test_input.shape[1]+28,1))\nprint(y_test)\nprint(test_forecast.shape)","58f3bc73":"print(y_test.shape)\nprint(test_forecast.shape)","aeeebb47":"# Reverse normalize to obtain human interpratable values\ntest_forecast = FNoramlize(test_forecast,train_low,train_high)\n\n# Round values\ntest_forecast = np.rint(test_forecast)","33e00af3":"# Transform into DataFrame and keep only the predictions\nforecast = pd.DataFrame(test_forecast.reshape(test_forecast.shape[0],test_forecast.shape[1])).iloc[:,-28:]\nforecast.columns = [f'F{i}' for i in range(1, forecast.shape[1] + 1)]\nforecast[forecast < 0] = 0\nforecast.head()","8de192de":"train_data = pd.read_csv(path+'sales_train_validation.csv')\nvalidation_ids = train_data['id'].values\nevaluation_ids = [i.replace('validation', 'evaluation') for i in validation_ids]","9b448cf2":"ids = np.concatenate([validation_ids, evaluation_ids])","a70dbb71":"predictions = pd.DataFrame(ids, columns=['id'])\nforecast = pd.concat([forecast]*2).reset_index(drop=True)\npredictions = pd.concat([predictions, forecast], axis=1)","9359f510":"predictions.to_csv('submission.csv', index=False)  #Generate the csv file.","70466529":"# Taking care of NaN (missing values) in the dataframe\nThere is multiple ways of doing this, I have chosen do it with the following method:","89c1d5f2":"You can also choose to drop NaN columns\/rows, frontfill or even fill with the mean. A lot of the time, it is best to try many methods and just simply choose the best.","faee1dac":"We notice that the DataFrame has some missing values that we want to take care of.","dcd90e24":"# Preparing the Data\nLet's prepare our data. We will manipulate and transform the data and make it more convenient for us to use.","64342140":"# Testing the Model\nNow that we have validated that the model does pretty well on our training data, we can move to some more serious stuff... TEST DATA\nWhy is this important you might ask? Well if you score plenty of goals in your practice (good job I guess), but none at the real game... there must be something wrong.\n\nThat is why we need test data to confirm that our model does well on unseen data.","20a8783e":"Let us now use summary method to validate our network.","cb3e79c8":"## Combining all datas","959105c4":"## Training\/Fitting time\nWe can finally train our model with our training data. Let's see how it does.","40274b8e":"## Split Sequences\nA key component of time-series problem is splitting our input data into sequences that we can feed to our LSTM network. This sequences depend on the required timesteps and horizons. ","9c8ee9a3":"## Obtaining all CAT and NUM columns\nThis is important if you want to do some feature scaling or encoding. ","8e3c9441":"Here we prep the ids to conform with the sample_submission.csv. We want to have both validation and evaluation ids","41539413":"# Reading and Preparing the Data\nLet's start by reading our data. We will store it in many dataframes.","6ec59d3f":"# Helper Functions\nThese are some helper functions that allow us to simplyfy our code and re-use some functionalities.","f88560a8":"## Saving and Loading Models\nThe next two cells allows you to save and load models. This would save you a lot of time whenever training takes a good chunk of time and computational power.","d1a858ce":"## Splitting the training data into sequences\nIn this section, we split the training data into sequences that we can further feed into LSTM network. Notice that each sequence has many variables\/features making it a multivariate problem. To predict the next 14 days (our horizon), we are going to use the events that occureed 14-54 days ago (not 1-14 ays ago, we keep a lag of 14 days since we believe that it takes a while for the events to show effect onto the price and number of items sold). ","d0de73ea":"# Training the model\nBefore we start training, we have to do some intermediate steps.","ffb00287":"## Normalize the training data\nHere we normalize our training data with our own in-house functions. This will increase modela accuracy and speed.","e2a6f365":"# Split into sequence and target\nAfter sequencing and normalzing the data, we slice the data to create the input sequences and output targets.","cebd40fa":"# Final Remarks\nThank you for going through this notebook. Please feel free to show support and comment on the notebooks with advice or improvements. If you found it useful, please let me know as well :)","234da83e":"## Normalization \nThese functions are used to normalize our data. This aids with model performance and speed. You can also use the scikit-learn MinMaxScaler if you wish, it is up to you.","dee1a0cc":"## Show Shapes\nThis functions is used to quickly check the shapes of our numpy arrays. This is especially important to assure we have the right shape for our LSTM network.","72ead5c5":"# Submit Predictions\nHere we submit the predictions and pray to god we did well :)","b278b62b":"## Sell Price Data","b86d144b":"# Exploring the DataFrame\nHere we explore the DataFrames and look for anything unusual.","96de52b3":"## Exploraty Data Analysis for pandas\nThis functions is used to quickly check the basic attributes of our pandas DataFrame.","ba940e30":"## Sales Data","a4d8b35c":"## Calendar Data","51449e14":"## Downcasting\nThis functions is used to downcast our variables to types that take less memory. This helps with model performance and speed.","fcce5f24":"## Event data transform\nThis function is specific to this competition and is used to manipulate and transform the competiton input data.","65c448e3":"# Introduction \n`V1.0.1`\n### Who am I\nJust a fellow Kaggle learner. I was creating this Notebook as practice and thought it could be useful to some others \n### Who is this for\nThis Notebook is for people that learn from examples. Forget the boring lectures and follow along for some fun\/instructive time :)\n### What can I learn here\nYou learn all the basics needed to create a rudimentary RNN\/LSTM encoder-decoder Network. I go over a multitude of steps with explanations. Hopefully with these building blocks,\nyou can go ahead and build much more complex models.\n\n### Things to remember\n+ Please Upvote\/Like the Notebook so other people can learn from it\n+ Feel free to give any recommendations\/changes. \n+ I will be continuously updating the notebook. Look forward to many more upcoming changes in the future.\n\n### You can also refer to these notebooks that have helped me as well:\n+ https:\/\/www.kaggle.com\/li325040229\/eda-and-an-encoder-decoder-lstm-with-9-features#Build-a-LSTM-Model-\n\n+ https:\/\/www.kaggle.com\/yashvi\/time-series-forecasting-using-lstm-m5\/notebook","580db04a":"# Imports\nFirst let us start by importing the relevant libraries that we need.","e2ffa52e":"Now we set our compiler and our optimatization mechanism. We will be using the Adam optimazation method since it is widely used and performs much better than regular gradient descent.","33d6caa1":"# Creating the LSTM Network\nWe are going to be creating a multivariate encoder-decoder LSTM Network with a dense layer at the end. We are using dropout as a regularisation method to combat overfitting.","8091d205":"## Plotting model accuracy and loss\nThis step is very important since it allows you to see if your model is performing well as you train it. If it isn't, you will rather have to create new features, tune hyperparameters, modify the RNN network or cry.","ec7b8f09":"## Tranposing the DataFrame\nWe do this so we can have the index as the timesteps (days) and the columns as our features."}}