{"cell_type":{"a112967d":"code","10e69eec":"code","142ceb59":"code","b836de30":"code","3662ff5d":"code","d8f30ed7":"code","504883b5":"code","bf973595":"code","a919bae3":"code","a92bfb7e":"code","25dd31e3":"code","e32473c4":"code","e4108d04":"code","20639600":"code","8e307105":"code","4bd85189":"code","615fbb7f":"code","e7383ed4":"code","4c4aa516":"code","d691d7ee":"code","5d168fd1":"code","4229a4e0":"code","590ec19d":"markdown","01cffb6b":"markdown","497e0a2d":"markdown","12d4a061":"markdown","508c1628":"markdown","3d7e5aaf":"markdown","842fd6bc":"markdown","ffb64ccd":"markdown","85d8e521":"markdown","ccbaf7b8":"markdown","4c9f5de2":"markdown","19070bc3":"markdown","5a825335":"markdown","220ce13d":"markdown","0186e397":"markdown","a186e5cd":"markdown","04c7e703":"markdown","b45e9c74":"markdown","630143a4":"markdown","26817879":"markdown","24502970":"markdown","4397384e":"markdown"},"source":{"a112967d":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nwarnings.simplefilter(action='ignore', category=FutureWarning)\n\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","10e69eec":"%%time\ntrain = pd.read_csv('..\/input\/cat-in-the-dat\/train.csv')\ntest = pd.read_csv('..\/input\/cat-in-the-dat\/test.csv')","142ceb59":"target = train['target']\ntrain_id = train['id']\ntest_id = test['id']\n\ntrain.drop(['target', 'id'], axis=1, inplace=True)\ntest.drop('id', axis=1, inplace=True)","b836de30":"df = pd.concat([train, test], axis=0, sort=False )","3662ff5d":"bin_dict = {'T':1, 'F':0, 'Y':1, 'N':0}\ndf['bin_3'] = df['bin_3'].map(bin_dict)\ndf['bin_4'] = df['bin_4'].map(bin_dict)\n","d8f30ed7":"print(f'Shape before dummy transformation: {df.shape}')\ndf = pd.get_dummies(df, columns=['nom_0', 'nom_1', 'nom_2', 'nom_3', 'nom_4'],\n                    prefix=['nom_0', 'nom_1', 'nom_2', 'nom_3', 'nom_4'], \n                    drop_first=True)\nprint(f'Shape after dummy transformation: {df.shape}')","504883b5":"from pandas.api.types import CategoricalDtype \n\nord_1 = CategoricalDtype(categories=['Novice', 'Contributor','Expert', \n                                     'Master', 'Grandmaster'], ordered=True)\nord_2 = CategoricalDtype(categories=['Freezing', 'Cold', 'Warm', 'Hot',\n                                     'Boiling Hot', 'Lava Hot'], ordered=True)\nord_3 = CategoricalDtype(categories=['a', 'b', 'c', 'd', 'e', 'f', 'g',\n                                     'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o'], ordered=True)\nord_4 = CategoricalDtype(categories=['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I',\n                                     'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R',\n                                     'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z'], ordered=True)\n\ndf.ord_1 = df.ord_1.astype(ord_1)\ndf.ord_2 = df.ord_2.astype(ord_2)\ndf.ord_3 = df.ord_3.astype(ord_3)\ndf.ord_4 = df.ord_4.astype(ord_4)\n\ndf.ord_1 = df.ord_1.cat.codes\ndf.ord_2 = df.ord_2.cat.codes\ndf.ord_3 = df.ord_3.cat.codes\ndf.ord_4 = df.ord_4.cat.codes\n","bf973595":"def date_cyc_enc(df, col, max_vals):\n    df[col + '_sin'] = np.sin(2 * np.pi * df[col]\/max_vals)\n    df[col + '_cos'] = np.cos(2 * np.pi * df[col]\/max_vals)\n    return df\n\ndf = date_cyc_enc(df, 'day', 7)\ndf = date_cyc_enc(df, 'month', 12)","a919bae3":"%%time\nfrom sklearn.preprocessing import LabelEncoder\n\n# Label Encoding\nfor f in ['nom_5', 'nom_6', 'nom_7', 'nom_8', 'nom_9', 'ord_5']:\n    lbl = LabelEncoder()\n    lbl.fit(df[f])\n    df[f'le_{f}'] = lbl.transform(df[f])","a92bfb7e":"\ndf.drop(['nom_5','nom_6','nom_7','nom_8','nom_9', 'ord_5'] , axis=1, inplace=True)","25dd31e3":"def reduce_mem_usage(df, verbose=True):\n    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    start_mem = df.memory_usage().sum() \/ 1024**2    \n    for col in df.columns:\n        col_type = df[col].dtypes\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)    \n    end_mem = df.memory_usage().sum() \/ 1024**2\n    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) \/ start_mem))\n    return df","e32473c4":"df = reduce_mem_usage(df)","e4108d04":"train = df[:train.shape[0]]\ntest = df[train.shape[0]:]\n\ntrain.shape","20639600":"%%time\nfrom xgboost import XGBClassifier\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_val, y_train, y_val = train_test_split(\n    train, target, test_size=0.2, random_state=2019\n)\n\nxgb_clf = XGBClassifier(learning_rate=0.05,\n    n_estimators=2000,\n    seed=42,\n    eval_metric='auc',\n)\n\nxgb_clf.fit(\n    X_train, \n    y_train, \n    eval_set=[(X_train, y_train), (X_val, y_val)],\n    early_stopping_rounds=20,\n    verbose=20\n)\n\n","8e307105":"results = xgb_clf.evals_result()\nepochs = len(results['validation_0']['auc'])\nx_axis = range(0, epochs)\n\n# plot log loss\nplt.figure(figsize=(15, 7))\nplt.plot(x_axis, results['validation_0']['auc'], label='Train')\nplt.plot(x_axis, results['validation_1']['auc'], label='Val')\nplt.legend()\nplt.ylabel('AUC')\nplt.xlabel('# of iterations')\nplt.title('XGBoost AUC')\nplt.show()","4bd85189":"%%time\nimport shap\n\nshap.initjs()\n\nexplainer = shap.TreeExplainer(xgb_clf)\nshap_values = explainer.shap_values(train)","615fbb7f":"shap.summary_plot(shap_values, train, plot_type=\"bar\")","e7383ed4":"shap.summary_plot(shap_values, train)","4c4aa516":"shap.dependence_plot(\"le_ord_5\", shap_values, train)","d691d7ee":"shap.dependence_plot(\"ord_4\", shap_values, train)","5d168fd1":"shap.dependence_plot(\"day\", shap_values, train)","4229a4e0":"shap.force_plot(explainer.expected_value, shap_values[0,:], train.iloc[0,:])","590ec19d":"### Feature Importance\n\nYou can extract `Feature Importance` from boosting models\n\n**importance_type**\n\n- `weight` - the number of times a feature is used to split the data across all trees.\n- `gain` - the average gain across all splits the feature is used in.\n- `cover` - the average coverage across all splits the feature is used in.\n- `total_gain` - the total gain across all splits the feature is used in.\n- `total_cover` - the total coverage across all splits the feature is used in.","01cffb6b":"### ETC (Label Encoding)","497e0a2d":"## Read Files & Library","12d4a061":"## Reduce Memory Usage & Train_test split","508c1628":"### Simple XGBoost\n","3d7e5aaf":"There are many plot in `SHAP` package \n\n- summary plot \n- dependence plot\n- force plot\n- decision plot \n- etc\n\nIn this notebook, I will introduce `How to read a SHAP's plot`.","842fd6bc":"### SHAP Value\n\nThe main idea of SHAP value is :\n\n*How does this prediction $i$ change when variable $j$ is removed from this model*\n\n### Tree Explainer\n\n**TreeSHAP**, a variant of SHAP for tree-based machine learning models such as decision trees, random forests and gradient boosted trees. TreeSHAP is fast, computes exact Shapley values, and correctly estimates the Shapley values when features are dependent.","ffb64ccd":"First of all, I will separate the target value.","85d8e521":"### Data (Cycle Encoding)","ccbaf7b8":"## Categorical Encoding\n\n\n### Binary Feature (map\/apply)\n","4c9f5de2":"**Type 1 : bar plot**","19070bc3":"### Drop 'object' features (Remaining features)","5a825335":"## Models\n\nFor more parameters, I recommend the following site.\n\n- [Laurae++](https:\/\/sites.google.com\/view\/lauraepp\/parameters)","220ce13d":"### SHAP Dependence Plot\n\nSHAP feature dependence might be the simplest global interpretation plot\n\nSHAP **dependence plots** are an alternative to partial dependence plots and accumulated local effects. ","0186e397":"### SHAP Summary Plot\n\nThe **summary plot** combines feature importance with feature effects. \n\nEach point on the summary plot is a Shapley value for a feature and an instance. ","a186e5cd":"To make encoding easier, let's connect to concat for a while.","04c7e703":"- `Feature importance`: Variables are ranked in descending order.\n- `Impact`: The horizontal location shows whether the effect of that value is associated with a higher or lower prediction.\n- `Original value`: Color shows whether that variable is high (in red) or low (in blue) for that observation.\n- `Correlation`","b45e9c74":"**Type 2 : default**","630143a4":"## Interpretable XGBoost \/ LGBM \/ Catboost\n\nWith SHAP, Let's interpret Tree-based Model.\n\n- XGBoost \n- LightGBM (TBD)\n- Catboost (TBD)\n\n### Reference\n\n- [Interpretable Machine Learning](https:\/\/christophm.github.io\/interpretable-ml-book\/index.html)","26817879":"### Nominal Feature (One-Hot Encoding)","24502970":"### Ordinal Feature (~Label Encoding)","4397384e":"### SHAP Force Plot \n\nYou can visualize feature attributions such as Shapley values as \"**forces**\". Each feature value is a force that either increases or decreases the prediction. \nThe prediction starts from the baseline. \nThe baseline for Shapley values is the average of all predictions. \n\nIn the plot, each Shapley value is an arrow that pushes to increase (positive value) or decrease (negative value) the prediction. These forces balance each other out at the actual prediction of the data instance."}}