{"cell_type":{"3a05ea10":"code","f2ee6622":"code","52ff507c":"code","9b00fdb6":"code","3575f80b":"code","5dd079cc":"code","9eb81dfe":"code","d6e182e6":"code","1877169a":"code","991124b2":"code","a276c795":"code","8ca25d63":"code","1c528c10":"code","42bfe6ad":"code","4ee3eac1":"code","24cf39a0":"code","74273735":"code","cbdf31d2":"code","002463af":"code","5273597b":"code","142c2437":"code","5af4ad5c":"code","3e521416":"code","d13a0335":"code","74ee4ad4":"code","91bea50c":"code","2252b106":"code","a892bb92":"code","97beedc2":"code","c623ac4f":"code","553ff4c5":"code","fde8597d":"code","588173bb":"code","8338a1eb":"code","d00eee2e":"code","a6c1efb8":"code","e5720ae8":"code","59d7aac2":"code","41c05511":"code","c103e5ad":"code","73bda69f":"code","54f60ddf":"code","93556579":"code","6513ff56":"code","396c3ae3":"code","f53bfc57":"code","15a6f3a4":"code","e4b998a9":"code","316561d9":"code","42bb2858":"code","bda06903":"code","46262637":"code","e2c6c91c":"markdown","355957f9":"markdown","054a299f":"markdown","8864ddda":"markdown","0bf984c2":"markdown","f6c26559":"markdown","2b920a41":"markdown","b34df5b3":"markdown","cec26765":"markdown"},"source":{"3a05ea10":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport os,sys\nimport numpy as np\nfrom random import shuffle\nimport cv2\nfrom tqdm import tqdm\n\nimport os\nprint(os.listdir(\"..\/\"))","f2ee6622":"df = pd.read_csv(\"..\/input\/imet-2019-fgvc6\/train.csv\")\ndata=[]\nmax_lenght=0\nid_max_lenght=0\nfor i in range(len(df)):\n    img_path=df.iloc[i][\"id\"]\n    lb=df.iloc[i][\"attribute_ids\"].split(' ')\n    if max_lenght<len(lb):\n        max_lenght=len(lb)\n        id_max_lenght=img_path\n    else:\n        max_lenght=max_lenght\n    label=[int(i) for i in lb]\n    data.append([img_path,label])","52ff507c":"data","9b00fdb6":"print(\"max_lenght:\",max_lenght)\nprint(\"id_max_lenght:\",id_max_lenght)","3575f80b":"def creat_square(in_img,value=0):\n    img = cv2.imread(\"..\/input\/imet-2019-fgvc6\/train\/\"+in_img+'.png')\n    #grey_img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n    grey_img=img\n    h, w = grey_img.shape[:2]\n    edge_square = max(h,w)  \n    ground_square=np.ones(shape=[edge_square,edge_square,3])*value\n    if h<w:\n        x1=np.floor((w-h)\/2).astype(int)\n        x2=np.floor((w-h)\/2+h).astype(int)\n        ground_square[x1:x2,:]=grey_img   \n    elif h>w:\n        x1=np.floor((h-w)\/2).astype(int)\n        x2=np.floor((h-w)\/2+w).astype(int)\n        ground_square[:,x1:x2]=grey_img\n    else:\n        ground_square=grey_img\n\n    return ground_square.astype(np.uint8)","5dd079cc":"# convert to gray images and resize to 224x224\ndef resize(img,x_scale=224,y_scale=224):\n    img=cv2.resize(img,(x_scale,y_scale))\n    #data.astype('uint8')\n    return img","9eb81dfe":"shuffle(data)\ndata","d6e182e6":"data_trainning=data[:int(len(data)*70\/100)]\ndata_val=data[int(len(data)*70\/100):]\n#data_test=data[int(len(data)*85\/100):]","1877169a":"print(\"training:\",np.array(data_trainning).shape)\nprint(\"validation:\",np.array(data_val).shape)\n#print(\"test:\",np.array(data_test).shape)","991124b2":"from keras.preprocessing.image import ImageDataGenerator","a276c795":"# construct the training image generator for data augmentation\naug = ImageDataGenerator(horizontal_flip=False,\n                         rotation_range=90,\n                         brightness_range=(0.5,1.3),\n                         fill_mode=\"nearest\")","8ca25d63":"# categorical label\ndef categorical_label(arr):\n    lb=np.zeros(1103)\n    for i in arr:\n        lb[i]=1\n    return lb","1c528c10":"def generator(data_txt,batchsize=32,augmentation=False):    \n    #map_name=list(zip(list_image,list_label))\n    shuffle_data=True\n    if shuffle_data:\n        shuffle(data_txt)\n    #list_image, list_label = zip(*map_name)\n    n = len(data_txt)\n    i = 0\n    while True:\n        if i==0:\n            shuffle(data_txt)\n        X_train=[]\n        Y_train=[]       \n        for b in range(batchsize):\n            img_path=data_txt[i][0]\n            #print(img_path)\n            img=creat_square(img_path)\n            img=resize(img,224,224)\n            #img=img.reshape(224,224,1)\n            \n            label=categorical_label(data_txt[i][1])\n            \n            X_train.append(img)\n            Y_train.append(label)\n            \n            i = (i+1) % n\n        if augmentation is True:\n            (images, labels) = next(aug.flow(np.array(X_train),Y_train, batch_size=batchsize))\n        else:\n            images=X_train\n            labels=Y_train\n        \n        images=np.array(images)\n        labels=np.array(labels)\n        \n        \n        images  = images.astype('float32')\n        labels  = labels.astype('float32')\n\n        images = images\/255\n        #print(\"X_train:\",X_train.shape)\n        yield (images,labels)","42bfe6ad":"from matplotlib import pyplot\na,b=next(generator(data,10,augmentation=True))\nfor i in range(0, 9):\n    pyplot.subplot(330 + 1 + i)\n    pyplot.imshow(a[i], cmap=pyplot.get_cmap('gray'))\npyplot.show()","4ee3eac1":"from matplotlib import pyplot\na,b=next(generator(data,10,augmentation=False))\nfor i in range(0, 9):\n    pyplot.subplot(330 + 1 + i)\n    pyplot.imshow(a[i], cmap=pyplot.get_cmap('gray'))\npyplot.show()","24cf39a0":"import keras\nfrom keras.models import Sequential\nfrom keras_preprocessing.image import ImageDataGenerator\nfrom keras.layers import Dense, Activation, Flatten, Dropout, BatchNormalization\nfrom keras.layers import Conv2D, MaxPooling2D,ZeroPadding2D,add,GlobalAveragePooling2D\nfrom keras import regularizers, optimizers\nfrom keras import applications\nfrom keras.models import Model,load_model\nfrom keras.optimizers import RMSprop,Adam,SGD\nfrom keras.callbacks import ModelCheckpoint,ReduceLROnPlateau, EarlyStopping\nfrom keras.callbacks import  TensorBoard\nfrom keras import backend as K\nimport tensorflow as tf","74273735":"def creat_model():\n    base_model=applications.ResNet50(weights=None,include_top=False,input_shape=(224, 224, 3))\n    base_model.load_weights(\"..\/input\/pretrain\/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5\")\n    \n    out = base_model.output\n    out = Dropout(0.5)(out)\n    out = BatchNormalization()(out)\n    out = GlobalAveragePooling2D()(out)\n    out = BatchNormalization()(out)\n    out = Dropout(0.5)(out)\n    out = BatchNormalization()(out)\n    predictions = Dense(1103, activation= 'sigmoid')(out)\n    custom_model = Model(inputs = base_model.input, outputs = predictions)\n    return custom_model","cbdf31d2":"model=creat_model()","002463af":"model.summary()","5273597b":"for layer in model.layers[:-7]:\n    layer.trainable = False","142c2437":"for layer in model.layers[-7:]:\n    layer.trainable = True","5af4ad5c":"model.summary()","3e521416":"## precision, recall, and f measure\n#def precision(y_true, y_pred):\n#    # Calculates the precision\n#    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n#    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n#    precision = true_positives \/ (predicted_positives + K.epsilon())\n#    return precision\n#\n#def recall(y_true, y_pred):\n#    # Calculates the recall\n#    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n#    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n#    recall = true_positives \/ (possible_positives + K.epsilon())\n#    return recall\n#\n#def fbeta_score(y_true, y_pred, beta=2):\n#    # Calculates the F score, the weighted harmonic mean of precision and recall.\n#    if beta < 0:\n#        raise ValueError('The lowest choosable beta is zero (only precision).')\n#    \n#    # If there are no true positives, fix the F score at 0 like sklearn.\n#    if K.sum(K.round(K.clip(y_true, 0, 1))) == 0:\n#        return 0\n#\n#    p = precision(y_true, y_pred)\n#    r = recall(y_true, y_pred)\n#    bb = beta ** 2\n#    fbeta_score = (1 + bb) * (p * r) \/ (bb * p + r + K.epsilon())\n#    return fbeta_score\n#\n#def fmeasure(y_true, y_pred):\n#    # Calculates the f-measure, the harmonic mean of precision and recall.\n#    return fbeta_score(y_true, y_pred, beta=2)","d13a0335":"beta_f2=2\n\n# if gamma == 0.0:\n#     F2_THRESHOLD = 0.1\n# elif gamma == 1.0:\n#     F2_THRESHOLD = 0.2\n# else:\n#     F2_THRESHOLD = 0.3\n\n# print(F2_THRESHOLD)\n    \ndef f2(y_true, y_pred):\n    #y_pred = K.round(y_pred)\n#     y_pred = K.cast(K.greater(K.clip(y_pred, 0, 1), F2_THRESHOLD), K.floatx())\n    tp = K.sum(K.cast(y_true*y_pred, 'float'), axis=1)\n    tn = K.sum(K.cast((1-y_true)*(1-y_pred), 'float'), axis=1)\n    fp = K.sum(K.cast((1-y_true)*y_pred, 'float'), axis=1)\n    fn = K.sum(K.cast(y_true*(1-y_pred), 'float'), axis=1)\n\n    p = tp \/ (tp + fp + K.epsilon())\n    r = tp \/ (tp + fn + K.epsilon())\n\n    f2 = (1+beta_f2**2)*p*r \/ (p*beta_f2**2 + r + K.epsilon())\n    f2 = tf.where(tf.is_nan(f2), tf.zeros_like(f2), f2)\n    return K.mean(f2)","74ee4ad4":"#from keras import backend as K\n#import tensorflow as tf\n#\n#import dill\n#\n#\n#def binary_focal_loss(gamma=2., alpha=0.25):\n#\n#    def binary_focal_loss_fixed(y_true, y_pred):\n#        \n#        pt_1 = tf.where(tf.equal(y_true, 1), y_pred, tf.ones_like(y_pred))\n#        pt_0 = tf.where(tf.equal(y_true, 0), y_pred, tf.zeros_like(y_pred))\n#\n#        epsilon = K.epsilon()\n#        # clip to prevent NaN's and Inf's\n#        pt_1 = K.clip(pt_1, epsilon, 1. - epsilon)\n#        pt_0 = K.clip(pt_0, epsilon, 1. - epsilon)\n#\n#        return -K.sum(alpha * K.pow(1. - pt_1, gamma) * K.log(pt_1)) \\\n#               -K.sum((1 - alpha) * K.pow(pt_0, gamma) * K.log(1. - pt_0))\n#\n#    return binary_focal_loss_fixed","91bea50c":"# focal loss\n#def focal_loss(gamma=2., alpha=4.):#\n\n#    gamma = float(gamma)\n#    alpha = float(alpha)\n#\n#    def focal_loss_fixed(y_true, y_pred):\n#        epsilon = 1.e-7\n#        y_true = tf.convert_to_tensor(y_true, tf.float32)\n#        y_pred = tf.convert_to_tensor(y_pred, tf.float32)\n#\n#        model_out = tf.add(y_pred, epsilon)\n#        ce = tf.multiply(y_true, -tf.log(model_out))\n#        weight = tf.multiply(y_true, tf.pow(tf.subtract(1., model_out), gamma))\n#        fl = tf.multiply(alpha, tf.multiply(weight, ce))\n#        reduced_fl = tf.reduce_max(fl, axis=1)\n#        return tf.reduce_mean(reduced_fl)\n#    return focal_loss_fixed\n\ndef focal_loss(gamma=2., alpha=2):\n    gamma = 2.0\n    epsilon = K.epsilon()\n    def focal_loss(y_true, y_pred):\n        pt = y_pred * y_true + (1-y_pred) * (1-y_true)\n        pt = K.clip(pt, epsilon, 1-epsilon)\n        CE = -K.log(pt)\n        FL = K.pow(1-pt, gamma) * CE\n        loss = K.sum(FL, axis=1)\n        return loss\n    return focal_loss","2252b106":"import tensorflow as tf\nimport keras.backend as K\n\ndef f1_macro(y_true, y_pred):\n    y_pred = K.round(y_pred)\n    tp = K.sum(K.cast(y_true*y_pred, 'float'), axis=0)\n    # tn = K.sum(K.cast((1-y_true)*(1-y_pred), 'float'), axis=0)\n    fp = K.sum(K.cast((1-y_true)*y_pred, 'float'), axis=0)\n    fn = K.sum(K.cast(y_true*(1-y_pred), 'float'), axis=0)\n\n    p = tp \/ (tp + fp + K.epsilon())\n    r = tp \/ (tp + fn + K.epsilon())\n\n    f1 = 2*p*r \/ (p+r+K.epsilon())\n    f1 = tf.where(tf.is_nan(f1), tf.zeros_like(f1), f1)\n    return K.mean(f1)","a892bb92":"os.mkdir(\"..\/checkpoint\/\")","97beedc2":"os.listdir(\"..\/\")","c623ac4f":"model.compile(optimizer=Adam(lr=1e-3),\n                     loss=\"binary_crossentropy\",\n                     metrics=['accuracy',f1_macro,f2])\n#custom_model.compile(optimizer=Adam(lr=1e-3), loss=focal_loss(), metrics=['accuracy',f1_macro,f2])","553ff4c5":"tensorboard = TensorBoard(log_dir ='..\/training',\n                          write_graph=True,\n                          write_images=True,\n                          update_freq='batch'\n                            )\nreduce_lr = ReduceLROnPlateau(monitor='val_acc', \n                              factor=0.5,patience=5, min_lr=0.0001)\nearly_stopping = EarlyStopping(monitor='acc', min_delta=0, patience=10, verbose=1)\ncheckpoint = ModelCheckpoint('..\/checkpoint\/weights.{epoch:02d}-{val_acc:.5f}_binary_crossentropy_f1_macro.hdf5',\n                             monitor='val_acc', verbose=1, save_best_only=True, mode='auto')","fde8597d":"batch_size_train=32\nbatch_size_val=32\nhistory=model.fit_generator(generator(data_trainning,batch_size_train),\n                steps_per_epoch=max(1, len(data_trainning)\/\/batch_size_train),\n                validation_data=generator(data_val,batch_size_val),#\n                validation_steps=max(1, len(data_val)\/\/batch_size_val),#\n                epochs=1,\n                callbacks=[reduce_lr,checkpoint,tensorboard,reduce_lr])","588173bb":"os.mkdir(\"..\/model\")","8338a1eb":"model.save(\"..\/model\/model.h5\")","d00eee2e":"os.listdir(\"..\/model\/\")","a6c1efb8":"model_s2=creat_model()","e5720ae8":"model_s2.load_weights(\"..\/model\/model.h5\")","59d7aac2":"for layer in model_s2.layers[:]:\n    layer.trainable = True","41c05511":"model_s2.summary()","c103e5ad":"model_s2.compile(optimizer=Adam(lr=1e-3),\n                     loss=\"binary_crossentropy\",\n                     metrics=['accuracy',f1_macro,f2])\n#custom_model.compile(optimizer=Adam(lr=1e-3), loss=focal_loss(), metrics=['accuracy',f1_macro,f2])","73bda69f":"batch_size_train=32\nbatch_size_val=32\nhistory=model_s2.fit_generator(generator(data_trainning,batch_size_train),\n                steps_per_epoch=max(1, len(data_trainning)\/\/batch_size_train),\n                validation_data=generator(data_val,batch_size_val),#\n                validation_steps=max(1, len(data_val)\/\/batch_size_val),#\n                epochs=10,\n                callbacks=[reduce_lr,checkpoint,tensorboard,reduce_lr])","54f60ddf":"model_s2.save(\"..\/model_0.3478.h5\")","93556579":"#print(K.eval(model_s2.optimizer.lr))\n#lr=(K.eval(model_s2.optimizer.lr\n#model_s2.compile(optimizer=Adam(lr=lr),\n#                     loss=focal_loss(),\n#                     metrics=['accuracy',f1_macro,f2])\n#tensorboard = TensorBoard(log_dir ='training',\n#                          write_graph=True,\n#                          write_images=True,\n#                          update_freq='batch'\n#                            )\n#\n#reduce_lr = ReduceLROnPlateau(monitor='val_loss', \n#                              factor=0.5,patience=5, min_lr=0.0001)\n#\n#early_stopping = EarlyStopping(monitor='loss', min_delta=0, patience=10, verbose=1)\n#checkpoint = ModelCheckpoint('..\/weights.{epoch:02d}-{val_acc:.5f}_focal_f1_macro.hdf5',\n#                             monitor='val_loss', verbose=1, save_best_only=True, mode='auto')\n#batch_size_train=32\n#batch_size_val=32\n#history=model_s2.fit_generator(generator(data_trainning,batch_size_train),\n#                steps_per_epoch=max(1, len(data_trainning)\/\/batch_size_train),\n#                validation_data=generator(data_val,batch_size_val),#\n#                validation_steps=max(1, len(data_val)\/\/batch_size_val),#\n#                epochs=10,\n#                callbacks=[reduce_lr,checkpoint,tensorboard,reduce_lr])","6513ff56":"def my_f2(y_true, y_pred):\n    assert y_true.shape[0] == y_pred.shape[0]\n\n    tp = np.sum((y_true == 1) & (y_pred == 1),axis=1)\n    tn = np.sum((y_true == 0) & (y_pred == 0),axis=1)\n    fp = np.sum((y_true == 0) & (y_pred == 1),axis=1)\n    fn = np.sum((y_true == 1) & (y_pred == 0),axis=1)\n    \n    p = tp \/ (tp + fp + K.epsilon())\n    r = tp \/ (tp + fn + K.epsilon())\n\n    f2 = (1+beta_f2**2)*p*r \/ (p*beta_f2**2 + r + 1e-15)\n\n    return np.mean(f2)\n\ndef find_best_fixed_threshold(preds, targs, do_plot=True):\n    score = []\n    thrs = np.arange(0, 0.5, 0.01)\n    for thr in tqdm(thrs):\n        score.append(my_f2(targs, (preds > thr).astype(int) ))\n    score = np.array(score)\n    pm = score.argmax()\n    best_thr, best_score = thrs[pm], score[pm].item()\n    print(f'thr={best_thr:.3f}', f'F2={best_score:.3f}')\n    if do_plot:\n        plt.plot(thrs, score)\n        plt.vlines(x=best_thr, ymin=score.min(), ymax=score.max())\n        plt.text(best_thr+0.03, best_score-0.01, f'$F_{2}=${best_score:.3f}', fontsize=14);\n        plt.show()\n    return best_thr, best_score","396c3ae3":"def creat_square_test(in_img,value=0):\n    img = cv2.imread(\"..\/input\/imet-2019-fgvc6\/test\/\"+in_img+'.png')\n    #grey_img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n    grey_img=img\n    h, w = grey_img.shape[:2]\n    edge_square = max(h,w)  \n    ground_square=np.ones(shape=[edge_square,edge_square,3])*value\n    if h<w:\n        x1=np.floor((w-h)\/2).astype(int)\n        x2=np.floor((w-h)\/2+h).astype(int)\n        ground_square[x1:x2,:]=grey_img   \n    elif h>w:\n        x1=np.floor((h-w)\/2).astype(int)\n        x2=np.floor((h-w)\/2+w).astype(int)\n        ground_square[:,x1:x2]=grey_img\n    else:\n        ground_square=grey_img\n\n    return ground_square.astype(np.uint8)","f53bfc57":"model_s2.load_weights(\"..\/model_0.3478.h5\")","15a6f3a4":"batch=512\nn_val = round(len(data_val))\/\/batch\nfullValGen =generator(data_val,batch)\nlastFullValPred = np.empty((0, 1103))\nlastFullValLabels = np.empty((0, 1103))\nfor i in tqdm(range(n_val+1)): \n    #print(i)\n    im, lbl = next(fullValGen)\n    scores = model_s2.predict(im)\n    lastFullValPred = np.append(lastFullValPred, scores, axis=0)\n    lastFullValLabels = np.append(lastFullValLabels, lbl, axis=0)\nprint(lastFullValPred.shape, lastFullValLabels.shape)\n","e4b998a9":"best_thr, best_score = find_best_fixed_threshold(lastFullValPred, lastFullValLabels, do_plot=False)\nprint(\"best_thr={}, best_score={}\".format(best_thr, best_score))","316561d9":"from tqdm import tqdm\nsubmit = pd.read_csv('..\/input\/imet-2019-fgvc6\/sample_submission.csv')\nresult_predict=[]\nfor i, name in tqdm(enumerate(submit['id'])):\n    image = creat_square_test(name)\n    image = resize(image)\n    score_predict = model_s2.predict(image[np.newaxis]\/255)\n    # print(score_predict)\n    label_predict = np.arange(1103)[score_predict[0]>=best_thr]\n    print(\"label_predict:\",label_predict)\n    str_predict_label = ' '.join(str(l) for l in label_predict)\n    result_predict.append(str_predict_label)","42bb2858":"result_predict","bda06903":"submit = pd.read_csv('..\/input\/imet-2019-fgvc6\/sample_submission.csv')\nsubmit['attribute_ids'] = result_predict\nsubmit.to_csv('submission.csv', index=False)","46262637":"submit","e2c6c91c":"# Read image and resize","355957f9":"# Model","054a299f":"# Evalution","8864ddda":"# Training with focal loss","0bf984c2":"# Training","f6c26559":"# Custom loss funtion and F score","2b920a41":"# Creat train-val-test data","b34df5b3":"# Data Generator with augmentation data","cec26765":"# Data"}}