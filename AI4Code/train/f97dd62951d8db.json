{"cell_type":{"9f9e86e8":"code","78681597":"code","aeb00a41":"code","b01f00e3":"code","5ebff0bf":"code","d06bd685":"code","b0ea736b":"code","75423e27":"code","937bb2d2":"code","4052a991":"code","26d3f7f1":"code","7b0f1a08":"code","5704e841":"code","5ae87118":"code","7d490d1f":"code","e40cf327":"code","4e2e5d0f":"code","53a2ecb4":"code","3bb9bca2":"code","cd650f59":"markdown","a6358884":"markdown","5368677d":"markdown","204230c7":"markdown","fb5e8a56":"markdown","04e76a65":"markdown","719a2b6f":"markdown","62499eae":"markdown","3d5954dd":"markdown","b6bee27f":"markdown","fc64697c":"markdown","a83f4220":"markdown","615a88ea":"markdown","2aef3202":"markdown"},"source":{"9f9e86e8":"# You just became the leader of a country and your main goal is to increase the life expectancy of your citizens. \n# What factors are most important for achieving this goal?","78681597":"### Load the Python environment that is loaded on Kaggle's servers\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n    \n### Load (import) the tools that will be used to explore the data.\n#   We might not know exactly what tools we will need, therefore, they can be called upon as we need them.\n#   The data analysis will dictate this.\n    \nimport numpy as np # tool for linear algebra\nimport pandas as pd # tool for data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt # tool for data visualization\nimport seaborn as sns # tool for advanced data visualization\nfrom scipy import stats # tool for statistics\n\ndf = pd.read_csv(\"\/kaggle\/input\/life-expectancy-who\/Life Expectancy Data.csv\") # df defines the entire dataset\n\ndf.head()","aeb00a41":"print(\"Original Shape: \", df.shape)\n\n### RENAME COLUMNS\n\ndf.columns=['Country', 'Year', 'Status', 'Life_Expectancy', 'Adult_Mortality', 'Infant_Deaths', 'Alcohol', \n            'Percent_Expenditure', 'Hep_B', 'Measles', 'BMI', 'Under_5_Deaths', 'Polio', 'Total_Expenditure', \n            'Diptheria', 'AIDS', 'GDP', 'Population', 'Thinness', 'Thinness_5-9yrs', \n            'Income', 'Schooling'] \n\n### DROP UNNECESSARY COLUMNS\n\ndf = df.drop(columns = ['Adult_Mortality', 'Infant_Deaths','Under_5_Deaths','Percent_Expenditure',\n                        'Measles','Thinness_5-9yrs', 'Country','Year', 'Status']) \n\n### CHANGE INCOME SCALE FROM 0-1 TO 1-100\ndf.Income = df.Income*100 \n\nprint(\"Post Changes Shape: \", df.shape)\n\ndf.head()","b01f00e3":"### MISSING DATA DETECTION AND REMOVAL\n\nprint(\"Pre Missing Values Qty: \", df.isnull().sum()) \n\ndf = df.dropna(thresh=13) # drop any rows having less than 13 columns with non-missing values\n\nprint(\"Post Missing Values Qty: \", df.isnull().sum()) \n\nprint(\"Post Missing Data Shape: \", df.shape)","5ebff0bf":"### CONVERT ALL CONTINUOUS NUMERICAL VALUES (FLOAT) TO DISCRETE NUMERICAL VALUES (INTEGER)\n\ndf2 = df.select_dtypes(include=['float64']) # detects columns defined as float\nfor col in df2.columns.values: df2[col] = df2[col].astype('int64') # changes them to integer\n\ndf2.info()\n    \ndf2.head()","d06bd685":"#   We will only remove outliers for Life_Expectancy because it takes into account a combination of all the \n#   other independent variables. Outliers here would signify a combination of extraordinary generational \n#   circumstances (war, epidemic, natural disaster, nuclear disaster, etc.). Outliers in the independent \n#   variables are left alone because any single variable wouldn't extremely affect the Life_Expectancy.\n\nfig, ax = plt.subplots(figsize=(3, 1.25)) # visualization size\nsns.boxplot(x=df2['Life_Expectancy']) # forms a box and whisker plot","b0ea736b":"print(\" Pre Outliers Shape: \", df2.shape) # shows 1649 observations\n\nQ1 = df2.quantile(0.25) # defines 25th %ile\nQ3 = df2.quantile(0.75) # defines 75th %ile\nIQR = Q3 - Q1 # defines interquartile range\nUpper = Q3 + 1.5*IQR # defines upper limit for outliers\nLower = Q1 - 1.5*IQR # defines lower limit for outliers\n\nprint(\"UPPER LIMIT\", Upper) # shows the value of upper limit so that we can use it\nprint(\"LOWER LIMIT\", Lower) # shows the value of the lower limit so that we can use it\n\noutlier = df2[(df2['Life_Expectancy'] >= 91)|(df2['Life_Expectancy'] <= 49)].index # identifies the outlier values\ndf2.drop(outlier, inplace=True) # drops the outliers\n\nprint(\"After Outliers Shape: \", df2.shape) \n\nfig, ax = plt.subplots(figsize=(3, 1.25)) # visualization size\nsns.boxplot(x=df2['Life_Expectancy']) # box and whisker plot","75423e27":"### ADDITIONAL BOX & WHISKER PLOTS\n#   To explore more of the variables in the data set.\n\nfig, ax = plt.subplots(figsize=(3, 1.25))\nsns.boxplot(x=df2['Income'])\nfig, ax = plt.subplots(figsize=(3, 1.25))\nsns.boxplot(x=df2['BMI'])\nfig, ax = plt.subplots(figsize=(3, 1.25))\nsns.boxplot(x=df2['GDP'])","937bb2d2":"### LOOKING AT THE DATA\n#   A small variety of views of the data as well as basic descriptive statistics.\n\nprint(df2.head()) # shows first 5 rows of every column\n\nprint()\ndf2.info() # shows column names, number of recorded values, data type\n\nprint()\nprint(df2) # shows the entire data set with the first and last 5 rows\n\n## Descriptive Statistics\n\nprint()\ndf2.describe() # count, min, max, mean, median, Q1, Q3, STD","4052a991":"## Correlation\n\ndf2.corr() # shows the numerical correlation fit among all the variables (columns)","26d3f7f1":"## Correlation\n\nfig, ax = plt.subplots(figsize=(6, 4.5))\nsns.heatmap(df2.corr(),center=0,vmin=-1.0,vmax=1.0,cmap=\"PiYG\") # Uses color instead of values to show corr","7b0f1a08":"### FROM THE HEAT MAP OF CORRELATIONS \n#   These variables seemed either highly correlated or inspired interesting questions.\n\nfig, ax = plt.subplots(figsize=(5, 3))\nsns.regplot(x=\"Schooling\", y=\"Life_Expectancy\", data=df2, scatter_kws={\"color\": \"gray\"}, line_kws={\"color\": \"green\"})\n\nfig, ax = plt.subplots(figsize=(5, 3))\nsns.regplot(x=\"Income\", y=\"Life_Expectancy\", data=df2, scatter_kws={\"color\": \"gray\"}, line_kws={\"color\": \"green\"})\n\nfig, ax = plt.subplots(figsize=(5, 3))\nsns.regplot(x=\"BMI\", y=\"Life_Expectancy\", data=df2, scatter_kws={\"color\": \"gray\"}, line_kws={\"color\": \"green\"})\n\nfig, ax = plt.subplots(figsize=(5, 3))\nsns.regplot(x=\"Thinness\", y=\"Life_Expectancy\", data=df2, scatter_kws={\"color\": \"gray\"}, line_kws={\"color\": \"green\"})\n\nfig, ax = plt.subplots(figsize=(5, 3))\nsns.regplot(x=\"GDP\", y=\"Life_Expectancy\", data=df2, logx=True, scatter_kws={\"color\": \"gray\"}, line_kws={\"color\": \"green\"})\n\nfig, ax = plt.subplots(figsize=(5, 3))\nsns.regplot(x=\"Life_Expectancy\", y=\"AIDS\", data=df2, logx=True, scatter_kws={\"color\": \"gray\"}, line_kws={\"color\": \"green\"})\n","5704e841":"### FROM SCATTERPLOTS \n#   Greater Alcohol consumption and BMI correlate with having a longer life\n#   This was unexpected and deserves further investigation. \n#   Perhaps income has something to do with it.\n\nfig, ax = plt.subplots(figsize=(5, 3))\nsns.regplot(x=\"Alcohol\", y=\"Income\", data=df, logx=True, scatter_kws={\"color\": \"black\"}, line_kws={\"color\": \"pink\"})\n\nfig, ax = plt.subplots(figsize=(5, 3))\nsns.regplot(x=\"BMI\", y=\"Income\", data=df, scatter_kws={\"color\": \"black\"}, line_kws={\"color\": \"pink\"})\n\nfig, ax = plt.subplots(figsize=(5, 3))\nsns.regplot(x=\"Schooling\", y=\"Income\", data=df, scatter_kws={\"color\": \"black\"}, line_kws={\"color\": \"pink\"})\n\nfig, ax = plt.subplots(figsize=(5, 3))\nsns.regplot(x=\"GDP\", y=\"Income\", data=df, logx=True, scatter_kws={\"color\": \"black\"}, line_kws={\"color\": \"pink\"})","5ae87118":"import sklearn # used to perform supervised and unsupervised machine learning\nfrom sklearn.svm import SVC \nfrom sklearn.model_selection import train_test_split # used to split data into training and testing\n\n# SPLITTING THE DATA\ny = df2.Life_Expectancy # define dependent variable \nprint(y)\nx = df2.drop(\"Life_Expectancy\", axis=1) # define the independent variables\nprint(x)\n\nx_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.2) # split the data into 20% test & 80% train\nprint(x_train.info())\nprint(x_test.info())\nprint(y_train.describe())\nprint(y_test.describe())","7d490d1f":"### FEATURE SCALING\n\nfrom sklearn.preprocessing import StandardScaler # tool for scaling values\n\nsc = StandardScaler()\nx_train = sc.fit_transform(x_train)\nx_test = sc.transform(x_test)\n\n### BUILD THE RFR MODEL\n\nfrom sklearn.ensemble import RandomForestRegressor # tool for building random forest models\n\nRFR = RandomForestRegressor(n_estimators=50, random_state=0, max_depth=3)\nRFR.fit(x_train, y_train)\ny_pred = RFR.predict(x_test)\n\n### EVALUATE PERFORMANCE\n\nfrom sklearn.model_selection import cross_val_score # tool for evaluating training model using testing data\n\nRFRscore = cross_val_score(RFR,x_train,y_train,cv=5) # plugs test data into the RFR model and measures the success rate\n\nprint('RFR Scores: ', RFRscore)\nprint('RFR Cross Val Score: ', RFRscore.mean())","e40cf327":"### SHOW TABLE OF VARIABLE IMPORTANCE\n\nfrom sklearn.ensemble import RandomForestClassifier # tool for building random forest classifier\n\nRFC = RandomForestClassifier()\nRFC.fit(x, y)\npd.DataFrame({'Variable':x.columns,\n              'Importance':RFC.feature_importances_}).sort_values('Importance', ascending=False)","4e2e5d0f":"# VISUALIZE A SIGLE RANDOM FOREST WALK\n\nfrom sklearn import tree # tool for compiling tree diagrams of forest walks\n\nlen(RFC.estimators_)\nplt.figure(figsize=(25,25))\n_ = tree.plot_tree(RFC.estimators_[0], feature_names=x.columns, filled=True)","53a2ecb4":"### FEATURE SCALING\n\nfrom sklearn.preprocessing import StandardScaler # tool for scaling data\n\nsc = StandardScaler()\nx_train = sc.fit_transform(x_train)\nx_test = sc.transform(x_test)\n\n### BUILD THE LR MODEL\n\nfrom sklearn.linear_model import LinearRegression # tool for building linear regression models\n\nLR = LinearRegression().fit(x_train, y_train) \n\nprint(\"intercept: \", LR.intercept_)\nprint(\"coefficient: \", LR.coef_)\n\n### EVALUATE PERFORMANCE\n\nfrom sklearn.model_selection import cross_val_score # tool for evaluating training model using testing data\n\nLRscore = cross_val_score(LR,x_train,y_train,cv=5) # plugs test data to the LR model and measures it's success rate\n\nprint('LR Scores: ', LRscore)\nprint('LR Score Mean: ', LRscore.mean())","3bb9bca2":"# Ideally, this is something that is done thruought the data analysis process \n# as we have done in this walkthrough. Other visuals can also be compiled here if they haven't been already.","cd650f59":"### Random Forest Regression\nThis model is good for establishing which independent variables are most important when in achieving the desired goal for the dependent variable.","a6358884":"# STEP 2 - COLLECT RAW DATA\nThe purpose of this step is to collect reliable and robust data that will help understand the problem. Sometimes we need to collect it ourselves and need the proper resources (time, money, talent). Other times we can get it from a reliable third party (World Health Organization, US Census)","5368677d":"# STEP 5 - IN DEPTH ANALYSIS\nThe purpose of this step is to dive deeply into a few interesting correlations among variables.","204230c7":"## a. Bivariate Scatterplots","fb5e8a56":"## b. Machine Learning","04e76a65":"### Missing Data","719a2b6f":"# STEP 6 - COMMUNICATION AND VISUALIZATION\nThe purpose of this step is to build effective deliverables and visuals that clearly and effectively communicate our findings from the data analysis to interested stakeholders.","62499eae":"# STEP 4 - EXPLORATORY DATA ANALYSIS\nThe purpose of this step is to understand the main characteristics of the data. This approach enables us to begin mapping out a rough plan for analyzing the data.","3d5954dd":"### Outliers","b6bee27f":"### Other Cleanups","fc64697c":"### Linear Regression\nOnce the important independent variables are established, we can use this regression model and use it to guide decision-making.","a83f4220":"### Data Characteristics","615a88ea":"# STEP 3 - PROCESS THE DATA\nThe purpose of this step is to prepare the dataframe for data analysis. We address outliers, missing data, unnecessary\/duplicate columns, value input errors, renaming, etc.","2aef3202":"# STEP 1 - FRAME THE PROBLEM\nThe purpose is to establish a reason for collecting and exploring a set of data."}}