{"cell_type":{"c360cc88":"code","8883c353":"code","01a63bcc":"code","0c24c555":"code","c2f71d66":"code","36eb6cd9":"code","b6c1f005":"code","d039b8c7":"code","891ec62b":"code","efdb711d":"code","dcb3e2f1":"code","479e23c2":"code","0bfefcd5":"code","86c5846d":"code","25971a5f":"code","56fc0950":"code","3b32179c":"code","e62faf45":"code","268f7d5e":"code","db0ca174":"code","b7fc79ca":"code","483ac95b":"code","48a36674":"code","15d9425c":"code","9f39d7c5":"code","60c7e3a1":"code","b85c84a5":"code","750c3967":"code","8cd8d261":"code","1112bf47":"code","5cf79517":"code","2ac40e57":"code","63e339d3":"code","b40d7d4a":"code","0bacbe88":"code","74f1e9af":"code","725190c6":"code","1ec285ec":"code","4c027f66":"code","d07834ba":"code","d1ccfd71":"code","bc662b38":"code","9e39734a":"code","8b2ca8ae":"code","a496c36d":"code","e6ea09f9":"code","3c3b759c":"code","937fce92":"code","563602c8":"code","f4545e7d":"code","0bff9270":"code","b836a724":"code","d911f8c8":"code","26411c1a":"code","24dd6f69":"code","18a627ba":"code","ade39c4e":"code","f13cbd0e":"code","675889e7":"code","1f3b7327":"code","ac6a855d":"code","0349fea9":"code","e53517c7":"code","0b4cf786":"code","2975bf97":"code","801863cc":"code","20d7bdf2":"code","bc3a7d88":"code","13833922":"code","036a7282":"code","2ad37ba0":"code","146db38a":"code","023e4c8f":"code","78d6233a":"code","304d65e4":"code","f8219385":"code","54c72735":"code","b2f3fd9f":"code","0aaf81ec":"code","0d6446c2":"code","17e0a3cb":"code","fb0bdf59":"code","ecdef26f":"code","2e4e014d":"markdown","9595491b":"markdown","2a92ade5":"markdown","3527d3d9":"markdown","3ec0eaf1":"markdown","1591349e":"markdown","858d0fdc":"markdown","e3e74f23":"markdown","fc5cfc9e":"markdown","a68979e5":"markdown","8577d2a3":"markdown","370ca8af":"markdown","5bbce43f":"markdown","63a6bbe6":"markdown","8ca31b03":"markdown","bfdfb2b6":"markdown","9834f3c4":"markdown","dace6334":"markdown","d4574911":"markdown","60bf9684":"markdown","ee9dc2d3":"markdown","8c91f918":"markdown","bf69318a":"markdown","080651fc":"markdown","d46e5e83":"markdown","e5e4d7c8":"markdown","ed230ab2":"markdown","d84d52d1":"markdown","3fcb30a2":"markdown","0274126e":"markdown","395908c0":"markdown"},"source":{"c360cc88":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","8883c353":"import re \nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n%matplotlib inline \nimport seaborn as sns\nfrom plotly import graph_objs as go\nimport plotly.figure_factory as ff\nimport string\nfrom collections import Counter\nfrom wordcloud import WordCloud,STOPWORDS\nimport plotly.express as px\nimport nltk\nfrom nltk.corpus import stopwords","01a63bcc":"train = pd.read_csv(\"\/kaggle\/input\/tweet-sentiment-extraction\/train.csv\")\ntest = pd.read_csv(\"\/kaggle\/input\/tweet-sentiment-extraction\/test.csv\")\nsubmission = pd.read_csv(\"\/kaggle\/input\/tweet-sentiment-extraction\/sample_submission.csv\")","0c24c555":"train.head()","c2f71d66":"print(train.shape)\nprint(test.shape)","36eb6cd9":"train.info()","b6c1f005":"test.info()","d039b8c7":"test.head()","891ec62b":"train.dropna(inplace=True)","efdb711d":"train.describe()","dcb3e2f1":"sentiment = train.groupby('sentiment').count()['text'].reset_index().sort_values(by='text',ascending= False)\nsentiment.style.background_gradient(cmap='viridis')","479e23c2":"plt.figure(figsize=(12,6))\nsns.countplot(x='sentiment',data=train)","0bfefcd5":"fig = go.Figure(go.Funnelarea(\n    text =sentiment.sentiment,\n    values = sentiment.text,\n    title = {\"position\": \"top center\", \"text\": \"Funnel-Chart of Sentiment Distribution\"}\n    ))\nfig.show()","86c5846d":"# we are going to consider only text column and selected text column \ndef gen_freq(text):\n    #Will store the list of words\n    word_list = []\n\n    #Loop over all the tweets and extract words into word_list\n    for tw_words in text.split():\n        word_list.extend(tw_words)\n\n    #Create word frequencies using word_list\n    word_freq = pd.Series(word_list).value_counts()\n\n    #Print top 20 words\n    word_freq[:20]\n    \n    return word_freq\n\ngen_freq(train.text.str)\n","25971a5f":"gen_freq(train.selected_text.str)","56fc0950":"import matplotlib.pyplot as plt\nfrom wordcloud import WordCloud,STOPWORDS\n\n#Generate word frequencies\nword_freq = gen_freq(train.text.str)\n\n#Generate word cloud\nwc = WordCloud(width=400, height=330, max_words=100, background_color='white').generate_from_frequencies(word_freq)\n\nplt.figure(figsize=(12, 8))\nplt.imshow(wc, interpolation='bilinear')\nplt.axis('off')\nplt.show()","3b32179c":"#Generate word frequencies\nword_freq = gen_freq(train.selected_text.str)\n\n#Generate word cloud\nwc = WordCloud(width=400, height=330, max_words=100, background_color='white').generate_from_frequencies(word_freq)\n\nplt.figure(figsize=(12, 8))\nplt.imshow(wc, interpolation='bilinear')\nplt.axis('off')\nplt.show()","e62faf45":"def jaccard(str1, str2): \n    a = set(str1.lower().split()) \n    b = set(str2.lower().split())\n    c = a.intersection(b)\n    return float(len(c)) \/ (len(a) + len(b) - len(c))","268f7d5e":"results_jaccard=[]\n\nfor ind,row in train.iterrows():\n    sentence1 = row.text\n    sentence2 = row.selected_text\n\n    jaccard_score = jaccard(sentence1,sentence2)\n    results_jaccard.append([sentence1,sentence2,jaccard_score])","db0ca174":"jaccard = pd.DataFrame(results_jaccard,columns=[\"text\",\"selected_text\",\"jaccard_score\"])\ntrain = train.merge(jaccard,how='outer')","b7fc79ca":"train['Num_words_ST'] = train['selected_text'].apply(lambda x:len(str(x).split())) #Number Of words in Selected Text\ntrain['Num_word_text'] = train['text'].apply(lambda x:len(str(x).split())) #Number Of words in main text\ntrain['difference_in_words'] = train['Num_word_text'] - train['Num_words_ST'] #Difference in Number of words text and Selected Text","483ac95b":"train.head()","48a36674":"hist_data = [train['Num_words_ST'],train['Num_word_text']]\n\ngroup_labels = ['Selected_Text','Text']\n\n# Create distplot with custom bin_size\nfig = ff.create_distplot(hist_data, group_labels,show_curve=False)\nfig.update_layout(title_text='Distribution of Number Of words')\nfig.update_layout(\n    autosize=False,\n    width=900,\n    height=700,\n    paper_bgcolor=\"LightSteelBlue\",\n)\nfig.show()","15d9425c":"#Kernal Distribution\nplt.figure(figsize=(12,6))\np1=sns.kdeplot(train['Num_words_ST'], shade=True, color=\"g\").set_title('Kernel Distribution of Number Of words')\np1=sns.kdeplot(train['Num_word_text'], shade=True, color=\"r\")\n","9f39d7c5":"plt.figure(figsize=(12,6))\np1=sns.kdeplot(train[train['sentiment']=='positive']['difference_in_words'], shade=True, color=\"b\").set_title('Kernel Distribution of Difference in Number Of words')\np2=sns.kdeplot(train[train['sentiment']=='negative']['difference_in_words'], shade=True, color=\"r\")","60c7e3a1":"plt.figure(figsize=(12,6))\nsns.distplot(train[train['sentiment']=='neutral']['difference_in_words'],kde=False)","b85c84a5":"plt.figure(figsize=(12,6))\np1=sns.kdeplot(train[train['sentiment']=='positive']['jaccard_score'], shade=True, color=\"b\").set_title('KDE of Jaccard Scores across different Sentiments')\np2=sns.kdeplot(train[train['sentiment']=='negative']['jaccard_score'], shade=True, color=\"r\")\nplt.legend(labels=['positive','negative'])","750c3967":"plt.figure(figsize=(12,6))\nsns.distplot(train[train['sentiment']=='neutral']['jaccard_score'],kde=False)","8cd8d261":"k = train[train['Num_word_text']<=2]","1112bf47":"k.groupby('sentiment').mean()['jaccard_score']","5cf79517":"k[k['sentiment']=='positive']","2ac40e57":"# Text cleaning \nimport re \n\ndef clean_text(text):\n    '''Make text lowercase, remove text in square brackets,remove links,remove punctuation\n    and remove words containing numbers.'''\n    text = str(text).lower()\n    text = re.sub('\\[.*?\\]', '', text)\n    text = re.sub('https?:\/\/\\S+|www\\.\\S+', '', text)\n    text = re.sub('<.*?>+', '', text)\n    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n    text = re.sub('\\n', '', text)\n    text = re.sub('\\w*\\d\\w*', '', text)\n    return text","63e339d3":"train['text'] = train['text'].apply(lambda x:clean_text(x))\ntrain['selected_text'] = train['selected_text'].apply(lambda x:clean_text(x))","b40d7d4a":"train.head()","0bacbe88":"# Remove Stop Words \n\nprint(STOPWORDS)","74f1e9af":"text = train.text.apply(lambda x: clean_text(x))\nword_freq = gen_freq(text.str)*100\nword_freq_txt = word_freq.drop(labels=STOPWORDS,errors = 'ignore')\n\n#generate word cloud \nwc = WordCloud(width=450, height=330, max_words=200, background_color='white').generate_from_frequencies(word_freq_txt)\n\nplt.figure(figsize=(12, 14))\nplt.imshow(wc, interpolation='bilinear')\nplt.axis('off')\nplt.show()","725190c6":"ST = train.selected_text.apply(lambda x: clean_text(x))\nword_freq = gen_freq(ST.str)*100\nword_freq_st = word_freq.drop(labels=STOPWORDS,errors = 'ignore')\n\n#generate word cloud \nwc = WordCloud(width=450, height=330, max_words=200, background_color='white').generate_from_frequencies(word_freq_st)\n\nplt.figure(figsize=(12, 14))\nplt.imshow(wc, interpolation='bilinear')\nplt.axis('off')\nplt.show()","1ec285ec":"train['temp_list'] = train['selected_text'].apply(lambda x:str(x).split())\ntop = Counter([item for sublist in train['temp_list'] for item in sublist])\ntemp = pd.DataFrame(top.most_common(20))\ntemp.columns = ['Common_words','count']\ntemp.style.background_gradient(cmap='Reds')","4c027f66":"fig = px.bar(temp, x=\"count\", y=\"Common_words\", title='Commmon Words in Selected Text', orientation='h', \n             width=700, height=700,color='Common_words')\nfig.show()","d07834ba":"def remove_stopword(x):\n    return [y for y in x if y not in stopwords.words('english')]\ntrain['temp_list'] = train['temp_list'].apply(lambda x:remove_stopword(x))","d1ccfd71":"top = Counter([item for sublist in train['temp_list'] for item in sublist])\ntemp = pd.DataFrame(top.most_common(20))\ntemp = temp.iloc[1:,:]\ntemp.columns = ['Common_words','count']\ntemp.style.background_gradient(cmap='Purples')","bc662b38":"fig = px.treemap(temp, path=['Common_words'], values='count',title='Tree of Most Common Words')\nfig.show()","9e39734a":"train['temp_list1'] = train['text'].apply(lambda x:str(x).split()) #List of words in every row for text\ntrain['temp_list1'] = train['temp_list1'].apply(lambda x:remove_stopword(x)) #Removing Stopwords","8b2ca8ae":"top = Counter([item for sublist in train['temp_list1'] for item in sublist])\ntemp = pd.DataFrame(top.most_common(25))\ntemp = temp.iloc[1:,:]\ntemp.columns = ['Common_words','count']\ntemp.style.background_gradient(cmap='Blues')","a496c36d":"fig = px.bar(temp, x=\"count\", y=\"Common_words\", title='Commmon Words in Text', orientation='h', \n             width=700, height=700,color='Common_words')\nfig.show()","e6ea09f9":"fig = px.treemap(temp, path=['Common_words'], values='count',title='Tree of Most Common Words')\nfig.show()","3c3b759c":"Positive_sent = train[train['sentiment']=='positive']\nNegative_sent = train[train['sentiment']=='negative']\nNeutral_sent = train[train['sentiment']=='neutral']","937fce92":"#MosT common positive words\ntop = Counter([item for sublist in Positive_sent['temp_list'] for item in sublist])\ntemp_positive = pd.DataFrame(top.most_common(20))\ntemp_positive.columns = ['Common_words','count']\ntemp_positive.style.background_gradient(cmap='Greens')","563602c8":"fig = px.bar(temp_positive, x=\"count\", y=\"Common_words\", title='Most Commmon Positive Words', orientation='h', \n             width=700, height=700,color='Common_words')\nfig.show()","f4545e7d":"#MosT common negative words\ntop = Counter([item for sublist in Negative_sent['temp_list'] for item in sublist])\ntemp_negative = pd.DataFrame(top.most_common(20))\ntemp_negative = temp_negative.iloc[1:,:]\ntemp_negative.columns = ['Common_words','count']\ntemp_negative.style.background_gradient(cmap='Reds')","0bff9270":"fig = px.bar(temp_negative, x=\"count\", y=\"Common_words\", title='Most Commmon Negative Words', orientation='h', \n             width=700, height=700,color='Common_words')\nfig.show()","b836a724":"fig = px.treemap(temp_negative, path=['Common_words'], values='count',title='Tree Of Most Common Negative Words')\nfig.show()","d911f8c8":"#MosT common Neutral words\ntop = Counter([item for sublist in Neutral_sent['temp_list'] for item in sublist])\ntemp_neutral = pd.DataFrame(top.most_common(20))\ntemp_neutral = temp_neutral.loc[1:,:]\ntemp_neutral.columns = ['Common_words','count']\ntemp_neutral.style.background_gradient(cmap='Reds')","26411c1a":"fig = px.bar(temp_neutral, x=\"count\", y=\"Common_words\", title='Most Commmon Neutral Words', orientation='h', \n             width=700, height=700,color='Common_words')\nfig.show()","24dd6f69":"fig = px.treemap(temp_neutral, path=['Common_words'], values='count',title='Tree Of Most Common Neutral Words')\nfig.show()","18a627ba":"raw_text = [word for word_list in train['temp_list1'] for word in word_list]","ade39c4e":"def words_unique(sentiment,numwords,raw_words):\n    '''\n    Input:\n        segment - Segment category (ex. 'Neutral');\n        numwords - how many specific words do you want to see in the final result; \n        raw_words - list  for item in train_data[train_data.segments == segments]['temp_list1']:\n    Output: \n        dataframe giving information about the name of the specific ingredient and how many times it occurs in the chosen cuisine (in descending order based on their counts)..\n\n    '''\n    allother = []\n    for item in train[train.sentiment != sentiment]['temp_list1']:\n        for word in item:\n            allother .append(word)\n    allother  = list(set(allother ))\n    \n    specificnonly = [x for x in raw_text if x not in allother]\n    \n    mycounter = Counter()\n    \n    for item in train[train.sentiment == sentiment]['temp_list1']:\n        for word in item:\n            mycounter[word] += 1\n    keep = list(specificnonly)\n    \n    for word in list(mycounter):\n        if word not in keep:\n            del mycounter[word]\n    \n    Unique_words = pd.DataFrame(mycounter.most_common(numwords), columns = ['words','count'])\n    \n    return Unique_words","f13cbd0e":"Unique_Positive= words_unique('positive', 10, raw_text)\nprint(\"The top 10 unique words in Positive Tweets are:\")\nUnique_Positive.style.background_gradient(cmap='Greens')","675889e7":"fig = px.treemap(Unique_Positive, path=['words'], values='count',title='Tree Of Unique Positive Words')\nfig.show()","1f3b7327":"from palettable.colorbrewer.qualitative import Pastel1_7\nplt.figure(figsize=(16,10))\nmy_circle=plt.Circle((0,0), 0.7, color='white')\nplt.pie(Unique_Positive['count'], labels=Unique_Positive.words, colors=Pastel1_7.hex_colors)\np=plt.gcf()\np.gca().add_artist(my_circle)\nplt.title('DoNut Plot Of Unique Positive Words')\nplt.show()","ac6a855d":"Unique_Negative= words_unique('negative', 10, raw_text)\nprint(\"The top 10 unique words in Negative Tweets are:\")\nUnique_Negative.style.background_gradient(cmap='Blues')","0349fea9":"fig = px.treemap(Unique_Negative, path=['words'], values='count',title='Tree Of Unique Negative Words')\nfig.show()","e53517c7":"from palettable.colorbrewer.qualitative import Pastel1_7\nplt.figure(figsize=(16,10))\nmy_circle=plt.Circle((0,0), 0.7, color='white')\nplt.pie(Unique_Negative['count'], labels=Unique_Negative.words, colors=Pastel1_7.hex_colors)\np=plt.gcf()\np.gca().add_artist(my_circle)\nplt.title('DoNut Plot Of Unique Negative Words')\nplt.show()","0b4cf786":"Unique_Neutral= words_unique('neutral', 10, raw_text)\nprint(\"The top 10 unique words in Neutral Tweets are:\")\nUnique_Neutral.style.background_gradient(cmap='Reds')","2975bf97":"fig = px.treemap(Unique_Neutral, path=['words'], values='count',title='Tree Of Unique Neutral Words')\nfig.show()","801863cc":"from palettable.colorbrewer.qualitative import Pastel1_7\nplt.figure(figsize=(16,10))\nmy_circle=plt.Circle((0,0), 0.7, color='white')\nplt.pie(Unique_Neutral['count'], labels=Unique_Neutral.words, colors=Pastel1_7.hex_colors)\np=plt.gcf()\np.gca().add_artist(my_circle)\nplt.title('DoNut Plot Of Unique Neutral Words')\nplt.show()","20d7bdf2":"import re\nfrom nltk import sent_tokenize\nfrom nltk.tokenize import word_tokenize\nfrom nltk.corpus import stopwords\n\nall_tokens = []\n\nfor idx, row in train.iterrows():\n    for word in word_tokenize(row.selected_text):\n        all_tokens.append(word)\n    \nprint(len(all_tokens), all_tokens)","bc3a7d88":"all_tokens_unique = set(all_tokens)\nprint(len(all_tokens_unique), all_tokens_unique)","13833922":"number_of_class_labels=len(train['sentiment'].unique())\nnumber_of_class_labels","036a7282":"class_prob_df = pd.DataFrame(columns=['sentiment', 'probability'], index=range(number_of_class_labels))\nclass_prob_df","2ad37ba0":"Count_Row=train.shape[0] \nCount_Col=train.shape[1] \nprint(Count_Col)\nprint(Count_Row)\nprint(train.shape)","146db38a":"i=0\nfor val, cnt in train['sentiment'].value_counts().iteritems():\n    print ('value', val, 'was found', cnt, 'times')\n    class_prob_df.loc[i].sentiment = val\n    class_prob_df.loc[i].probability = cnt\/Count_Row\n    i = i +1\n    \nclass_prob_df","023e4c8f":"stop_words = set(stopwords.words('english'))\n\ntokens = [w for w in all_tokens_unique if not w in stop_words]\nprint(len(tokens), tokens)\n\ntokens1=[]\ntokens = [word for word in tokens if word.isalpha()]\nprint(len(tokens), tokens)","78d6233a":"word = ['@', 'rr', '!', '$', '@', 'jfjf', '&','(', ')', ',']\nfor word in word:\n    if word.isalpha():\n        print(\"yes it is alpha: \", word)","304d65e4":"train.values","f8219385":"group_train = train.groupby('sentiment')['selected_text'].apply(' '.join).reset_index()\n\ngroup_train","54c72735":"for idx, row in group_train.iterrows():\n    \n    temp1_tokens = []\n    for word in word_tokenize(row.selected_text):\n        temp1_tokens.append(word)\n    \n    temp1_tokens = set(temp1_tokens)\n         \n    temp2_tokens = []\n    for word in temp1_tokens:\n        if not word in stop_words:\n            temp2_tokens.append(word)           \n    \n    temp3_tokens = []\n    for word in temp2_tokens:\n        if word.isalpha():\n            temp3_tokens.append(word)\n            \n    print(temp3_tokens)\n    temp4_tokens = \" \".join(temp3_tokens)\n    print(temp4_tokens)\n    \n    group_train.at[idx, 'selected_text'] = temp4_tokens\n    group_train.at[idx, 'no_of_words_in_category'] = len(temp3_tokens)","b2f3fd9f":"group_train.head()","0aaf81ec":"group_train = pd.merge(group_train, class_prob_df[['sentiment', 'probability']], on='sentiment')\ngroup_train","0d6446c2":"final_df = pd.DataFrame()\n\nrow_counter = 0\n\nfor idx, row in group_train.iterrows():\n    for token in tokens:\n        # find the number of occurances of the token in the current category of documents\n        no_of_occurances = row.selected_text.count(token)\n        no_of_words_in_category = row.no_of_words_in_category\n        no_unique_words_all = len(tokens)\n        \n        prob_of_token = (no_of_occurances+ 1)\/ (no_of_words_in_category+ no_unique_words_all)\n        #print(row.class_label, token, no_of_occurances, prob_of_token)\n        final_df.at[row_counter, 'Result'] = row.sentiment\n        final_df.at[row_counter, 'token'] = token\n        final_df.at[row_counter, 'no_of_occurances'] = no_of_occurances\n        final_df.at[row_counter, 'no_of_words_in_category'] = no_of_words_in_category\n        final_df.at[row_counter, 'no_unique_words_all'] = no_unique_words_all\n        final_df.at[row_counter, 'prob_of_token_category'] = prob_of_token\n        \n        row_counter = row_counter + 1","17e0a3cb":"final_df","fb0bdf59":"for idx, row in test.iterrows():\n    \n    # tokenize & unique words\n    temp1_tokens = []\n    for word in word_tokenize(row.sentiment):\n        temp1_tokens.append(word)\n        #temp1_tokens = set(temp1_tokens)\n        \n    # remove stop words\n    temp2_tokens = []\n    for word in temp1_tokens:\n        if not word in stop_words:\n            temp2_tokens.append(word)\n          \n    # remove punctuations\n    temp3_tokens = []\n    for word in temp2_tokens:\n        if word.isalpha():\n            temp3_tokens.append(word)\n            \n    #temp4_tokens = \" \".join(temp3_tokens)\n    #print(temp4_tokens)\n    \n    prob = 1 \n    \n    # process for each class_label\n    for idx1, row1 in group_train.iterrows():\n        print(\"class: \"+ row1.sentiment)\n        for token in temp3_tokens:\n            # find the token in final_df for the given category, get the probability\n            # row1.class_label & token\n        \n            print(\"      : \"+ token)  \n        \n            temp_df = final_df[(final_df['Result'] == row1.sentiment) & (final_df['token'] == token)]\n\n            # process for exception\n            if (temp_df.shape[0] == 0):\n                token_prob = 1\/(row1.no_of_words_in_category+ no_unique_words_all)\n                print(\"       no token found prob :\", token_prob)\n                prob = prob * token_prob\n            else:\n                token_prob = temp_df.get_value(temp_df.index[0],'prob_of_token_category')\n                print(\"       token prob          :\", token_prob)\n                prob = prob * token_prob\n\n            prob = prob * row1.probability\n\n        col_at = 'prob_'+row1.sentiment\n\n        test.at[idx, col_at] = prob\n\n\ntest","ecdef26f":"test.to_csv('submission.csv', index=False)","2e4e014d":"The number of words plot is really interesting ,the tweets having number of words greater than 25 are very less and thus the number of words distribution plot is right skewed","9595491b":"#  Read train and test data","2a92ade5":"Now It will be more interesting to see the differnce in number of words and jaccard_scores across different Sentiments","3527d3d9":"Lets look the distribuion of meta - features","3ec0eaf1":"Identify the shape and strucure of the train and test dataset ","1591349e":"Funnel chart","858d0fdc":"# Prepare Submission File","e3e74f23":"Lets check the word cloud for the frequency of the words ","fc5cfc9e":"Model Building \n\nLets consider only Selected Text and Sentiment for Model Building, Later we consider text and Sentiment for Model Building and Seleced Text and Sentiment for Model Buiding ","a68979e5":"Here also content is missing stop word are more frequent. before cleaning the stop words lets generate the meta feature\n\n* Difference In Number Of words of Selected_text and Text\n* Jaccard Similarity Scores between text and Selected_text","8577d2a3":"There is no null value in the test data ","370ca8af":"# Let's Look at Unique Words in each Segment\n# We will look at unique words in each segment in the Following Order:","5bbce43f":"Most Common Words in our Target-Selected text","63a6bbe6":"# **Tweets Sentiment Analysis - EDA, Probability determination**","8ca31b03":"Lets Remove the stop words ","bfdfb2b6":"We have one null Value in the train , as the test field for value is NAN we will just remove it","9834f3c4":"Lets remove the null observation from train set","dace6334":"let us find the frequency of words in selected text and text ","d4574911":"Lets see the disrtubution in the graphical view","60bf9684":"Most common words Sentiments Wise","ee9dc2d3":"To find the distribution of sentiment from train data ","8c91f918":"Exploratary Data Analysis(EDA)","bf69318a":"Tree for most common selected text","080651fc":"Lets understand the dataset, Textid is alpha numereic number generated for each row, Text is raw data and selected text is subset of the text data and sentiment is about postive, negative or neutral.","d46e5e83":"Unable to plot kde plot for neutral sentiment because most of the values for difference in number of words were zero. We can see it clearly now ,if we had used the feature in the starting we would have known that text and selected text are mostly the same for neutral sentiment,thus its always important to keep the end goal in mind while performing EDA","e5e4d7c8":"Lets view in Visulaization Form","ed230ab2":"# **Importing Necesseties**","d84d52d1":"we have 27481 obseravation and 4 varaiables in the Train dataset and 3534 observations and 3 variables in Test data, This is because one variable sentiment which is need to be predicted is missing in the test data\n\nLets find the structure of the data ","3fcb30a2":"Based on the probability we can determine the sentiment.\n\nPlease upvote if you find usefull. ","0274126e":"**Conclusion Of EDA**\n\nWe can see from the jaccard score plot that there is peak for negative and positive plot around score of 1 .That means there is a cluster of tweets where there is a high similarity between text and selected texts ,if we can find those clusters then we can predict text for selected texts for those tweets irrespective of segment\n\n\nLet's see if we can find those clusters,one interesting idea would be to check tweets which have number of words lesss than 3 in text, because there the text might be completely used as text","395908c0":"To, the, and, a are stop words which have more frequency which is not meaningful, unable to identify the content of the text, let us also look for selected text "}}