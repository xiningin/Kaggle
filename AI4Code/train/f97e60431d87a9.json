{"cell_type":{"d7a2626e":"code","0fdeb811":"code","dacd3ccf":"code","69205a40":"code","d3e53dfa":"code","fc52c988":"code","1c70ffaf":"code","ee6b69c0":"code","1668a95e":"code","e2f215a2":"code","506f05d4":"code","b697ccf1":"code","49b1858d":"code","263ee44c":"code","0f41d693":"code","ad54b8e3":"code","b97e9418":"code","b8a52e18":"code","95b6935b":"code","10be8da8":"markdown","24caed53":"markdown","927913f6":"markdown","e766b7c4":"markdown","180fe8aa":"markdown","91bbd3a6":"markdown","11cca2c0":"markdown","f35de592":"markdown","4c6a5f8f":"markdown","bbb7fa2b":"markdown"},"source":{"d7a2626e":"!pip install -q tensorflow==2.6.0","0fdeb811":"import tensorflow as tf\nimport numpy as np\nfrom sklearn import metrics\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nimport pandas as pd\nimport re\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split","dacd3ccf":"class Config:\n    batch_size = 256\n    epochs = 50\n    embed_dim = 256\n    validation_split = 0.15\n    maxlen = 128\n    vocab_size = 30000\n    num_heads = 8\n    labels = np.array([\"neg\", \"pos\"])\n    num_classes = len(labels)\n    label_display_names = np.array([\"Negative\", \"Positive\"])\n    mask_word = \"[mask]\"\n    mask_model_path = \"bert_mlm_imdb.h5\"\n    ff_dim = 128\n    num_layers = 1\n    learning_rate = 1e-3\nconfig = Config()","69205a40":"data = pd.read_csv(\"\/kaggle\/input\/imdb-review-dataset\/imdb_master.csv\", encoding='unicode_escape')\ndata.head()","d3e53dfa":"data = data[(data.label == \"neg\") | (data.label == \"pos\")]","fc52c988":"def standardization(input_data):\n    lowercase = tf.strings.lower(input_data)\n    stripped_html = tf.strings.regex_replace(lowercase, \"<br \/>\", \" \")\n    return tf.strings.regex_replace(\n        stripped_html, \"[%s]\" % re.escape(\"!#$%&'()*+,-.\/:;<=>?@\\^_`{|}~\"), \"\"\n    )","1c70ffaf":"vectorizer = layers.TextVectorization(\n    max_tokens=config.vocab_size, \n    standardize=standardization,\n    output_sequence_length=config.maxlen\n)\nvectorizer.adapt(data[\"review\"] + [config.mask_word])","ee6b69c0":"config.mask_token_id = vectorizer([config.mask_word])[0][0].numpy()\nconfig.mask_token_id","1668a95e":"def encode(texts):\n    encoded_texts = vectorizer(texts)\n    return encoded_texts.numpy()\n\ndef get_masked_input_and_labels(encoded_texts):\n    # 15% BERT masking\n    inp_mask = np.random.rand(*encoded_texts.shape) < 0.15\n    # Do not mask special tokens\n    inp_mask[encoded_texts <= 2] = False\n    # Set targets to -1 by default, it means ignore\n    labels = -1 * np.ones(encoded_texts.shape, dtype=int)\n    # Set labels for masked tokens\n    labels[inp_mask] = encoded_texts[inp_mask]\n\n    # Prepare input\n    encoded_texts_masked = np.copy(encoded_texts)\n    # Set input to [MASK] which is the last token for the 90% of tokens\n    # This means leaving 10% unchanged\n    inp_mask_2mask = inp_mask & (np.random.rand(*encoded_texts.shape) < 0.90)\n    encoded_texts_masked[\n        inp_mask_2mask\n    ] = config.mask_token_id  # mask token is the last in the dict\n\n    # Set 10% to a random token\n    inp_mask_2random = inp_mask_2mask & (np.random.rand(*encoded_texts.shape) < 1 \/ 9)\n    encoded_texts_masked[inp_mask_2random] = np.random.randint(\n        3, config.mask_token_id, inp_mask_2random.sum()\n    )\n\n    # Prepare sample_weights to pass to .fit() method\n    sample_weights = np.ones(labels.shape)\n    sample_weights[labels == -1] = 0\n\n    # y_labels would be same as encoded_texts i.e input tokens\n    y_labels = np.copy(encoded_texts)\n\n    return encoded_texts_masked, y_labels, sample_weights","e2f215a2":"# Prepare data for masked language model\nx_all_review = encode(data.review.values)\nx_masked_train, y_masked_labels, sample_weights = get_masked_input_and_labels(\n    x_all_review\n)\nmlm_ds = tf.data.Dataset.from_tensor_slices(\n    (x_masked_train, y_masked_labels, sample_weights)\n)\nmlm_ds = mlm_ds.shuffle(1000).batch(config.batch_size)","506f05d4":"for item in mlm_ds.take(1):\n    print(item[0])\n    print(item[1])\n    print(item[2])","b697ccf1":"def bert_module(query, key, value, i):\n    # Multi headed self-attention\n    attention_output = layers.MultiHeadAttention(\n        num_heads=config.num_heads,\n        key_dim=config.embed_dim \/\/ config.num_heads,\n        name=\"encoder_{}\/multiheadattention\".format(i),\n    )(query, key, value)\n    attention_output = layers.Dropout(0.1, name=\"encoder_{}\/att_dropout\".format(i))(\n        attention_output\n    )\n    attention_output = layers.LayerNormalization(\n        epsilon=1e-6, name=\"encoder_{}\/att_layernormalization\".format(i)\n    )(query + attention_output)\n\n    # Feed-forward layer\n    ffn = keras.Sequential(\n        [\n            layers.Dense(config.ff_dim, activation=\"relu\"),\n            layers.Dense(config.embed_dim),\n        ],\n        name=\"encoder_{}\/ffn\".format(i),\n    )\n    ffn_output = ffn(attention_output)\n    ffn_output = layers.Dropout(0.1, name=\"encoder_{}\/ffn_dropout\".format(i))(\n        ffn_output\n    )\n    sequence_output = layers.LayerNormalization(\n        epsilon=1e-6, name=\"encoder_{}\/ffn_layernormalization\".format(i)\n    )(attention_output + ffn_output)\n    return sequence_output\n\n\ndef get_pos_encoding_matrix(max_len, d_emb):\n    pos_enc = np.array(\n        [\n            [pos \/ np.power(10000, 2 * (j \/\/ 2) \/ d_emb) for j in range(d_emb)]\n            if pos != 0\n            else np.zeros(d_emb)\n            for pos in range(max_len)\n        ]\n    )\n    pos_enc[1:, 0::2] = np.sin(pos_enc[1:, 0::2])  # dim 2i\n    pos_enc[1:, 1::2] = np.cos(pos_enc[1:, 1::2])  # dim 2i+1\n    return pos_enc\n\n\nloss_fn = keras.losses.SparseCategoricalCrossentropy(\n    reduction=tf.keras.losses.Reduction.NONE\n)\nloss_tracker = tf.keras.metrics.Mean(name=\"loss\")\n\n\nclass MaskedLanguageModel(tf.keras.Model):\n    def train_step(self, inputs):\n        if len(inputs) == 3:\n            features, labels, sample_weight = inputs\n        else:\n            features, labels = inputs\n            sample_weight = None\n\n        with tf.GradientTape() as tape:\n            predictions = self(features, training=True)\n            loss = loss_fn(labels, predictions, sample_weight=sample_weight)\n\n        # Compute gradients\n        trainable_vars = self.trainable_variables\n        gradients = tape.gradient(loss, trainable_vars)\n\n        # Update weights\n        self.optimizer.apply_gradients(zip(gradients, trainable_vars))\n\n        # Compute our own metrics\n        loss_tracker.update_state(loss, sample_weight=sample_weight)\n\n        # Return a dict mapping metric names to current value\n        return {\"loss\": loss_tracker.result()}\n\n    @property\n    def metrics(self):\n        # We list our `Metric` objects here so that `reset_states()` can be\n        # called automatically at the start of each epoch\n        # or at the start of `evaluate()`.\n        # If you don't implement this property, you have to call\n        # `reset_states()` yourself at the time of your choosing.\n        return [loss_tracker]\n\n\ndef create_masked_language_bert_model():\n    inputs = layers.Input((config.maxlen,), dtype=tf.int64)\n\n    word_embeddings = layers.Embedding(\n        config.vocab_size, config.embed_dim, name=\"word_embedding\"\n    )(inputs)\n    position_embeddings = layers.Embedding(\n        input_dim=config.maxlen,\n        output_dim=config.embed_dim,\n        weights=[get_pos_encoding_matrix(config.maxlen, config.embed_dim)],\n        name=\"position_embedding\",\n    )(tf.range(start=0, limit=config.maxlen, delta=1))\n    embeddings = word_embeddings + position_embeddings\n\n    encoder_output = embeddings\n    for i in range(config.num_layers):\n        encoder_output = bert_module(encoder_output, encoder_output, encoder_output, i)\n\n    mlm_output = layers.Dense(config.vocab_size, name=\"mlm_cls\", activation=\"softmax\")(\n        encoder_output\n    )\n    mlm_model = MaskedLanguageModel(inputs, mlm_output, name=\"masked_bert_model\")\n\n    optimizer = keras.optimizers.Adam(learning_rate=config.learning_rate)\n    mlm_model.compile(optimizer=optimizer)\n    return mlm_model\n\n\nid2token = dict(enumerate(vectorizer.get_vocabulary()))\ntoken2id = {y: x for x, y in id2token.items()}\n\n\nclass MaskedTextGenerator(keras.callbacks.Callback):\n    def __init__(self, sample_tokens, top_k=5):\n        self.sample_tokens = sample_tokens\n        self.k = top_k\n\n    def decode(self, tokens):\n        return \" \".join([id2token[t] for t in tokens if t != 0])\n\n    def convert_ids_to_tokens(self, id):\n        return id2token[id]\n    \n    def mlm_predict(self, sample_tokens):\n        prediction = self.model.predict(sample_tokens)\n\n        masked_index = np.where(sample_tokens == config.mask_token_id)\n        masked_index = masked_index[1]\n        mask_prediction = prediction[0][masked_index]\n\n        top_indices = mask_prediction[0].argsort()[-self.k :][::-1]\n        values = mask_prediction[0][top_indices]\n        retsults = []\n        for i in range(len(top_indices)):\n            p = top_indices[i]\n            v = values[i]\n            tokens = np.copy(sample_tokens[0])\n            tokens[masked_index[0]] = p\n            result = {\n                \"input_text\": self.decode(sample_tokens[0]),\n                \"prediction\": self.decode(tokens),\n                \"probability\": v,\n                \"predicted mask token\": self.convert_ids_to_tokens(p),\n            }\n            retsults.append(result)\n        return retsults\n\n    def on_epoch_end(self, epoch, logs=None):\n        results = self.mlm_predict(self.sample_tokens)\n        for result in results:\n            print(result)\n            \n\n\nsample_tokens = vectorizer([\"I have watched this [mask] and it was awesome\"])\ngenerator_callback = MaskedTextGenerator(sample_tokens.numpy())\n\nbert_masked_model = create_masked_language_bert_model()\nbert_masked_model.summary()","49b1858d":"keras.utils.plot_model(bert_masked_model, show_shapes=True)","263ee44c":"checkpoint = keras.callbacks.ModelCheckpoint(config.mask_model_path, save_best_only=True)\nearly_stop = keras.callbacks.EarlyStopping(patience=5)\nbert_masked_model.fit(mlm_ds, epochs=config.epochs, callbacks=[generator_callback, checkpoint, early_stop])","0f41d693":"result = generator_callback.mlm_predict(encode([\"i love this [mask] so much\"]))[0]\nprint(result)","ad54b8e3":"result = generator_callback.mlm_predict(encode([\"I enjoy this [mask] very much\"]))[0]\nprint(result)","b97e9418":"result = generator_callback.mlm_predict(encode([\"He is [mask] kind\"]))[0]\nprint(result)","b8a52e18":"result = generator_callback.mlm_predict(encode([\"I [mask] this movie\"]))[0]\nprint(result)","95b6935b":"result = generator_callback.mlm_predict(encode([\"I love this [mask]\"]))[0]\nprint(result)","10be8da8":"## Table of Contents\n* Overview\n* Setup\n* Loading data\n* Model Development\n* Model Training\n* Make Predictions","24caed53":"## Make Predictions","927913f6":"# IMDB Reviews: Masked Language Modeling","e766b7c4":"## Overview\nIn this notebook I will implement a Mask Language Modeling(MLM) Model with BERT. Masked Language Modeling is a kind a fill-in-the-blank task.","180fe8aa":"### Visualize the architecture","91bbd3a6":"## Setup","11cca2c0":"### Build a TextVectorization layer","f35de592":"## Model Training","4c6a5f8f":"## Model Development","bbb7fa2b":"## Loading data"}}