{"cell_type":{"050d5346":"code","e9bb3287":"code","8df49b59":"code","ed47aca3":"code","c3d61c97":"code","2ffad2c6":"code","da72971f":"code","cb59ef6b":"code","870a9f95":"code","f62d8eb2":"code","a76e9cd1":"code","f0cc7dc4":"code","52adbe96":"code","5881c542":"code","370a0af9":"code","4ff617fc":"code","062d2e96":"code","2453b21d":"code","13b5768e":"code","10ae3b9d":"code","2b3a87d0":"code","5df3df14":"code","ac17f912":"code","92106eb1":"code","f7b02cbd":"code","37fc9d03":"code","b34b89d1":"code","78045d4e":"code","4e79189d":"code","e06c3628":"code","f3f2caf4":"code","f9ba5872":"code","6ca86861":"code","fbf7fabc":"code","cf99368b":"code","a687e0a9":"markdown","a02200eb":"markdown","f61ddcab":"markdown","734bbbbf":"markdown","685c32f0":"markdown","32f29358":"markdown","0ff0ab4a":"markdown","da5709ac":"markdown","1ba7cfa7":"markdown","c4202421":"markdown","25a156ac":"markdown","cc1a6fff":"markdown","a53b47b0":"markdown","63f1011a":"markdown"},"source":{"050d5346":"#commit version 10 \n#this sho","e9bb3287":"import pandas as pd\nimport numpy as np\nfrom datetime import datetime\nimport warnings  \nwarnings.filterwarnings('ignore')\n\n%load_ext autoreload\n%autoreload 2\n\n%matplotlib inline\n\nimport time\nimport lightgbm as lgb\nimport xgboost as xgb\nimport seaborn as sns\n\n\nfrom sklearn.metrics import r2_score, median_absolute_error, mean_absolute_error\nfrom sklearn.metrics import median_absolute_error, mean_squared_error, mean_squared_log_error\nfrom sklearn.model_selection import KFold\nfrom scipy import stats\n\n\nimport statsmodels.api as sm\n\ndef mean_absolute_percentage_error(y_true, y_pred): \n    return np.mean(np.abs((y_true - y_pred) \/ y_true)) * 100\n\n\n\n\nlocations = [\"shingyi\", \"nanggang\", \"daan\",\"nehoo\" ]","8df49b59":"#todo: \n1. how to deal with test train split \/\n2. dealing with missing vals \/\n3. predition using only trend \/\n4.establish baseline prediction\/ \n5. calculate smape score \/\n\n\n#data visualizations \n1.time serties of each item at each store \/\n2.time series of every item in each store \/\n3.time series of each item at every stoer combines \/ \n4.all items time series \/\n5. decompostion of trend . seasonaty , noise \/\n\n\n\n#model predicitons \n1.arima\/\n2.exponetial smoothing \/ \n3.stationaty . naive \/\nmoving average\n4.neural network. pending \n5.fb prophet \/ \n6. mean\/median of past vsits \/\n7. xgboost \n8. light gbm \/ pending \n\n\n#chanllenges \n1.not enough data points for good train\/valid split \n2.no seasonality and however trend is possible \n","ed47aca3":"#Load the datasets\ndf = pd.read_csv('..\/input\/fm.csv')\ndf_og = df.copy()\ndf.date = pd.to_datetime(df.date, format='%Y')\ndf = df.set_index('date')\n#show dimenion \nprint(\"train shape: \", df.shape)\n# Set of features we have are: date\tlocation\titem\tvisits\ndisplay(df.sample(10))\n\n","c3d61c97":"print(df.head(5))\n","2ffad2c6":"#change column names to make it match sales prediction dataset, this makes our life easer \nprint(df.columns)\ndf.columns = ['store','item','sales']\nprint(df.head(5))","da72971f":"locations = df.store.unique()\nprint(locations)\nprint(\"number of locations: \", len(locations))","cb59ef6b":"df_sns = df.copy()\ndf_sns[\"date\"] = df_sns.index\nprint(df_sns.head()) ","870a9f95":"    print(\"all exams in all location combines (all exam)\")\n    ax = sns.lineplot(x = \"date\", y = \"sales\",data = df_sns)\n#     legend = ax.legend()\n#     legend.texts[0].set_text(\"Whatever else\")","f62d8eb2":"print(\"all exams in every location\")\nlocations = df_sns['store'].unique()\nax = sns.lineplot(x=\"date\", y=\"sales\", hue=\"store\",data=df_sns)","a76e9cd1":"print(\"all exams in every location\")\nlocations = df_sns['store'].unique()\nax = sns.lineplot(x=\"date\", y=\"sales\", hue=\"store\",data=df_sns[df_sns['store'].isin(locations[:4])])","f0cc7dc4":"print(\"all exams in every location\")\nlocations = df_sns['store'].unique()\nax = sns.lineplot(x=\"date\", y=\"sales\", hue=\"store\",data=df_sns[df_sns['store'].isin(locations[4:8])])","52adbe96":"print(\"all exams in every location\")\nlocations = df_sns['store'].unique()\nax = sns.lineplot(x=\"date\", y=\"sales\", hue=\"store\",data=df_sns[df_sns['store'].isin(locations[8:12])])","5881c542":"print(\"all exams in every location\")\nlocations = df_sns['store'].unique()\nax = sns.lineplot(x=\"date\", y=\"sales\", hue=\"store\",data=df_sns[df_sns['store'].isin(locations[12:16])])","370a0af9":"import matplotlib.pyplot as plt\nlocations = df_sns['store'].unique() \nfor i, loc in enumerate(locations): \n    plt.figure(i)\n    ax = sns.lineplot(x=\"date\", y=\"sales\", hue=\"item\", style=\"store\", data=df_sns[df_sns[\"store\"]==loc])\n    ","4ff617fc":"import matplotlib.pyplot as plt\nexams  = df_sns['item'].unique() \nfor i, exam in enumerate(exams): \n    plt.figure(i)\n    ax = sns.lineplot(x=\"date\", y=\"sales\", hue=\"store\", style=\"item\", data=df_sns[df_sns[\"item\"]==exam])","062d2e96":"#### Seasonality Check\n# preparation: input should be float type\ndf_raw = df.copy()\ndf_raw['sales'] = df_raw['sales'] * 1.0\n\n\nstores = []\n\nfor loc in locations: \n    stores.append(df_raw[df_raw.store == loc]['sales'].sort_index(ascending = True))\n\nf, axs = plt.subplots(len(locations), figsize = (15, 52))\nc = '#386B7F'\n\n# store types\nfor i in range(len(locations)): \n    stores[i].resample('W').sum().plot(color = c, ax = axs[i],title = \"                               \"+locations[i])\n        \n\n#There seems to be no seasonality \n#daan showed increasinf trend \n#nehoo showed decreasing trend \n","2453b21d":"date_sales = df_raw.drop(['store','item'], axis=1).copy() #it's a temporary DataFrame.. Original is Still intact..\n\ndate_sales.get_ftype_counts()\ny = date_sales['sales'].resample('YS').mean() \nprint(y['2011':]) #sneak peak\n\nimport statsmodels.api as sm\n#We can also visualize our data using a method called time-series decomposition that allows us to decompose our time series into three distinct components: \n#trend, seasonality, and noise.\ndecomposition = sm.tsa.seasonal_decompose(y, model='additive')\n\nprint(\"mean visits for every exam in every location\")\ndecomposition.plot();\n#The plot shows no seasonality which is to be expected. we should focus on trend prediction instead . \n#espceialy at locations ","13b5768e":"#We can also visualize our data using a method called time-series decomposition that allows us to decompose our time series into three distinct components: \n#trend, seasonality, and noise.\ndecomposition = sm.tsa.seasonal_decompose(y, model='multiplicative')\ndecomposition.plot();\n#The plot above clearly shows that the sales is unstable, along with its obvious seasonality.;","10ae3b9d":"plt.ylim(0, 400)\nsns.boxplot(x=\"date\", y=\"visits\", data=df_og)","2b3a87d0":"#imputing methods. \n#impute using mean \n#impute using median \n#impute with rolling mean \ndf_imputed = df.copy()\ndf_imputed[\"mean\"] = df_imputed[\"sales\"]\ndf_imputed[\"median\"] = df_imputed[\"sales\"]\n\nfor i in  range(df_imputed.shape[0]): \n    row = df_imputed.iloc[i, :]\n    if row[2] == 0:\n       #find time series of particualt item and store \n#         print(row[3])\n        historical_vals_df =  df_imputed[(df_imputed.store == row[0]) ]\n        historical_vals_df =   historical_vals_df[( historical_vals_df.item == row[1]) ]\n#         print(historical_vals_df)\n        row[3] = int(np.mean(historical_vals_df.sales))\n#         print(row[3])\n        row[4] = int(np.median(historical_vals_df.sales))\n#         print(row[4])\n    df_imputed.iloc[i, :] = row\n       \n\n","5df3df14":"#train valid split \n#use 2011 to 2017 data to predict 2018 data \n#theorectically we need  a trian\/valid\/test set but i fear we do not have enough data points \ntrain = df_imputed.loc['2011-01-01':'2017-02-01']\nvalid = df_imputed.loc['2017-02-01':'2018-02-01']\n\nvalid_x = valid.iloc[:,:-1]\nvalid_y = valid.iloc[:,-1]\nprint(train.head())\nprint(valid.head())","ac17f912":"df = df_imputed","92106eb1":"#error from 0 -100 %\ndef smape(y_true, y_pred): \n    denominator = (np.abs(y_true) + np.abs(y_pred)) \/ 100.0\n    diff = np.abs(y_true - y_pred) \/ denominator\n    diff[denominator == 0] = 0.0\n    return np.nanmean(diff)\n\n\ndef score(y_label,y_pred,verbose = 0,target = \"sales\"):\n    if verbose == 1: \n        print(\"you predictied: \",y_pred)\n        print(\"label: \",y_label)\n    y_pred = [x if x > 0 else 0 for x in y_pred]\n    print(\"smape score(\",target,\"):\",smape(y_label,y_pred))\n    #plot here ","f7b02cbd":"y_true = np.array(3)\ny_pred = np.ones(1)\nx = np.linspace(0,10,1000)\nres = [smape(y_true, i * y_pred) for i in x]\nplt.plot(x, res)","37fc9d03":"def mean_prediction(valid_x, train,target = \"sales\"):\n    #from valid_x find average of all item,store combo \n    preds = []\n    for index, row in valid_x.iterrows():\n        preds.append(mean_pred_single(row.values[0],row.values[1],train, target = target))\n    return preds\n        \n    \n\ndef mean_pred_single(store, item ,train, target = \"sales\"):\n    historical_vals_df =  train[(train.store == store) ]\n    historical_vals_df =   historical_vals_df[( historical_vals_df.item == item) ]\n    return np.mean(historical_vals_df[target])\n\npred = mean_prediction(valid_x,train)\nscore(valid_y,pred, verbose =0 )\n    \npred = mean_prediction(valid_x,train, target  = \"median\")\nscore(valid_y,pred, verbose =0 ,target  = \"median\")\n\npred = mean_prediction(valid_x,train, target = \"mean\")\nscore(valid_y,pred, verbose =0 , target = \"mean\")\n","b34b89d1":"def median_prediction(valid_x, train, target = \"sales\"):\n    #from valid_x find average of all item,store combo \n    preds = []\n    for index, row in valid_x.iterrows():\n        preds.append(median_pred_single(row.values[0],row.values[1],train, target = target ))\n    return preds\n        \n    \n\ndef median_pred_single(store, item ,train, target = \"sales\"):\n    historical_vals_df =  train[(train.store == store) ]\n    historical_vals_df =   historical_vals_df[( historical_vals_df.item == item) ]\n    return np.median(historical_vals_df[target])\n\n\npred = median_prediction(valid_x,train)\nscore(valid_y,pred, verbose =0 )\n    \npred = median_prediction(valid_x,train, target  = \"median\")\nscore(valid_y,pred, verbose =0 ,target  = \"median\")\n\npred = median_prediction(valid_x,train, target = \"mean\")\nscore(valid_y,pred, verbose =0 , target = \"mean\")\n    \n","78045d4e":"def last_prediction(valid_x, train):\n    #from valid_x find average of all item,store combo \n    preds = []\n    for index, row in valid_x.iterrows():\n        preds.append(last_pred_single(row.values[0],row.values[1],train))\n    return preds\n        \n    \n\ndef last_pred_single(store, item ,train):\n    historical_vals_df =  train[(train.store == store) ]\n    historical_vals_df =   historical_vals_df[( historical_vals_df.item == item) ]\n    historical_vals_df=historical_vals_df.sort_index()\n#     print(historical_vals_df)\n#     print( historical_vals_df.iloc[-1,2])\n    \n    return historical_vals_df.iloc[-1,2]\n\npred = last_prediction(valid_x,train)\n\nscore(valid_y,pred,verbose = 0)\n    ","4e79189d":"def exponential_prediction(valid_x, train, target = 'sales'):\n    #from valid_x find average of all item,store combo \n    preds = []\n    for index, row in valid_x.iterrows():\n        preds.append(exponential_pred_single(row.values[0],row.values[1],train, target = target ))\n    return preds\n        \n    \n\ndef exponential_pred_single(store, item ,train, target = \"sales\"):\n    historical_vals_df =  train[(train.store == store) ]\n    historical_vals_df =   historical_vals_df[( historical_vals_df.item == item) ]\n    historical_vals_df=historical_vals_df.sort_index()\n    p = historical_vals_df[target].ewm(alpha = 0.9).mean()[-1]\n    return p\n\npred = exponential_prediction(valid_x,train)\n\nscore(valid_y,pred,verbose = 0)\n\npred = exponential_prediction(valid_x,train, target  = \"median\")\nscore(valid_y,pred, verbose =0 ,target  = \"median\")\n\npred = exponential_prediction(valid_x,train, target = \"mean\")\nscore(valid_y,pred, verbose =0 , target = \"mean\")","e06c3628":"from statsmodels.tsa.holtwinters import ExponentialSmoothing\n\ntrain_exp = train.copy()\ntrain_exp = train.replace(0,0)\nimport warnings  \nwarnings.filterwarnings('ignore')\n\ndef exponential_prediction(valid_x, train, target = \"sales\"):\n    #from valid_x find average of all item,store combo \n    preds = []\n    for index, row in valid_x.iterrows():\n        preds.append(exponential_pred_single(row.values[0],row.values[1],train,target = target))\n    return preds\n        \n    \n\ndef exponential_pred_single(store, item ,train, target = \"sales\"):\n    historical_vals_df =  train[(train.store == store) ]\n    historical_vals_df =   historical_vals_df[( historical_vals_df.item == item) ]\n    historical_vals_df=historical_vals_df.sort_index()\n    model = ExponentialSmoothing(historical_vals_df[target],trend = 'add',damped = True ,seasonal = None )\n    model_fit = model.fit(smoothing_level = 1, smoothing_slope  = 1)\n    p = model_fit.predict(8,8)\n    return p.tolist()[0]\n\npred =exponential_prediction(valid_x,train)\nscore(valid_y,pred, verbose =0 )\n    \npred = exponential_prediction(valid_x,train, target  = \"median\")\nscore(valid_y,pred, verbose =0 ,target  = \"median\")\n\npred = exponential_prediction(valid_x,train, target = \"mean\")\nscore(valid_y,pred, verbose =0 , target = \"mean\")","f3f2caf4":"from statsmodels.tsa.holtwinters import ExponentialSmoothing\n\n\n\n\ndef prediction(valid_x, train, target = 'sales'):\n    #from valid_x find average of all item,store combo \n    preds = []\n    for index, row in valid_x.iterrows():\n        preds.append(pred_single(row.values[0],row.values[1],train, target = target\n                                ))\n    return preds\n        \n    \n\ndef pred_single(store, item ,train, target = 'sales'):\n    historical_vals_df =  train[(train.store == store) ]\n    historical_vals_df =   historical_vals_df[( historical_vals_df.item == item) ]\n    historical_vals_df=historical_vals_df.sort_index()\n    vals = historical_vals_df.sales.tolist() \n    return vals[-1] + (vals[-1]-vals[-2])\n\npred = prediction(valid_x,train)\n\nscore(valid_y,pred,verbose = 0)\n\npred = prediction(valid_x,train, target  = \"median\")\nscore(valid_y,pred, verbose =0 ,target  = \"median\")\n\npred =prediction(valid_x,train, target = \"mean\")\nscore(valid_y,pred, verbose =0 , target = \"mean\")","f9ba5872":"np.__version__","6ca86861":"from fbprophet import Prophet\nimport matplotlib.pyplot as plt \n\n\n\ndef fb_prediction(valid_x, train):\n    #from valid_x find average of all item,store combo \n    preds = []\n    for index, row in valid_x.iterrows():\n        preds.append(fb_pred_single(row.values[0],row.values[1],train))\n    return preds\n        \n    \n\ndef fb_pred_single(store, item ,train):\n    historical_vals_df =  train[(train.store == store) ]\n    historical_vals_df =   historical_vals_df[( historical_vals_df.item == item) ]\n    historical_vals_df=historical_vals_df.sort_index()\n    df_fb = pd.DataFrame()\n    df_fb[\"Y\"] = historical_vals_df[\"median\"]\n    df_fb[\"DS\"] = df_fb.index\n    df_fb = df_fb[['DS', 'Y']]\n    df_fb.columns = ['ds','y']\n    m = Prophet()\n#     print(df_fb)\n    m.fit(df_fb)\n    future = m.make_future_dataframe(periods=1,freq='Y')\n#     print(future.tail())\n    forecast = m.predict(future)\n    print(forecast[['ds', 'yhat', 'yhat_lower', 'yhat_upper']].tail())\n    fig1 = m.plot(forecast)\n    fig2 = m.plot_components(forecast)\n\n#     plt.show()\n    print(forecast[\"yhat\"].tolist()[-1])\n    return forecast[\"yhat\"].tolist()[-1]\n    \npred = fb_prediction(valid_x,train)\n\nscore(valid_y,pred,verbose = 0)","fbf7fabc":"\nnp.seterr(divide='ignore', invalid='ignore')\n\nstores = train.store.unique()\nitems = train.item.unique()\n# print(stores)\n\ntrain_arima = train.copy()\ntrain_arima = train_arima.replace(0,1)\n# print(train_arima)\n\ncounter = 0\nsuccess = 0\npred = []\nfor store in stores: \n    for item in items:\n        print(store,item)\n        counter += 1\n        train_df = train_arima[(train_arima['store']==store) &(train_arima['item']==item)]\n        try:\n            arima = sm.tsa.ARIMA(train_df.sales, (1,1,0)).fit(disp=False)\n#             print(arima.summary())\n            p = arima.forecast(1)[0]\n            pred.append(p[0])\n            print(arima.forecast(1)[0])\n            success += 1\n        except Exception as e:\n            print(\"shit\")\n            pred.append(median_pred_single(store, item, train))\n            \nprint(counter, success)\nprint(pred)\n\nscore(valid_y,pred, verbose =0 )","cf99368b":"df_imputed","a687e0a9":"<h1>mean prediction<\/h1> \n","a02200eb":"<h1> Train valid split<\/h1>","f61ddcab":"<h1>double exponential smoothing<\/h1> ","734bbbbf":"<h1>median prediction: a baseline likely hard to beat <\/h1>","685c32f0":"<h1>Does imputing missing values help? <\/h1> ","32f29358":"<h1>Does imputing missing values help? <\/h1> ","0ff0ab4a":"train\/valid split \n\n","da5709ac":"<h1>exponential smoothing <\/h1>","1ba7cfa7":"<h1> ARIMA<\/h1>","c4202421":"* <h1>data exploration<\/h1>[](http:\/\/)","25a156ac":"#train a arima model for every time series. \n#caveats : \n1.","cc1a6fff":"<h1> prediciton section <\/h1>","a53b47b0":"<h1>naive baseline : same as last year. <\/h1>","63f1011a":"<h1>FB prophet !!! <\/h1>"}}