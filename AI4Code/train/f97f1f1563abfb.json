{"cell_type":{"f34f0752":"code","6b95d58c":"code","0e40feeb":"code","964ce4d2":"code","e842acbc":"code","63684a88":"code","c8b47ca8":"code","9e2006b8":"code","d9006a16":"code","0c3efea6":"code","5c121191":"code","cdcaaa3a":"code","47bc754f":"code","3db33dc5":"code","d7fa1f4b":"code","dfe8b669":"code","b0bf69b5":"code","d68cb9bb":"code","6c74ea5e":"code","8b3ffa5f":"code","208a4975":"markdown","cb71406f":"markdown","afad19b5":"markdown","00dfcc5a":"markdown","7faff639":"markdown","07cac12c":"markdown","a104b99f":"markdown","f18026e6":"markdown","aefe5260":"markdown","31569edf":"markdown","d18df165":"markdown","caa51b36":"markdown","2fe603c7":"markdown"},"source":{"f34f0752":"# Let's import needed libraries\nimport albumentations as A\nimport cv2\nimport json\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport os\nfrom skimage.color import label2rgb\n%matplotlib inline","6b95d58c":"IMG_SIZE = 512\nMAX_SIZE = 1120\nIMAGE_ID = '0461935888bad18244f11e67e7d3b417.jpg'","0e40feeb":"input_path = '\/kaggle\/input\/examples-for-augs'\nimage_filepath = os.path.join(input_path,IMAGE_ID)\nannot_filepath = os.path.join(input_path,'annotations.csv')","964ce4d2":"# load image\nimage = cv2.imread(image_filepath, cv2.IMREAD_UNCHANGED) \nimage = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) # I am still wondering why OpenCV changed to BGR by default\nheight, width, channels = image.shape\nheight, width","e842acbc":"# load annotations\nanns = pd.read_csv(annot_filepath)\nanns.head()","63684a88":"def img_masks_targets(df, img_id):\n    \"\"\"Select all targets of one image as an array of numbers\n       Select all masks of one image as an array of RLE strings\n       Output: \n            masks and targets for an image\n    \"\"\"\n    # select all targets of one image as an array of numbers\n    targets = df[df['ImageId'] == img_id][\"ClassId\"].values\n    # select all masks of one image as an array is strings\n    rles = df[df['ImageId'] == img_id][\"EncodedPixels\"].values\n    return targets, rles","c8b47ca8":"labels, rles = img_masks_targets(anns, img_id = IMAGE_ID)\nnum_instances = len(rles)\nprint(f'Number of instances on the image {len(rles)}')","9e2006b8":"def rle_decode(rle_str: str, mask_shape: tuple, mask_dtype=np.uint8):\n    \"\"\"Helper to decode RLE string to a binary mask\"\"\"\n    s = rle_str.split()\n    starts, lengths = [np.asarray(x, dtype=int) for x in (s[0:][::2], s[1:][::2])]\n    starts -= 1\n    ends = starts + lengths\n    mask = np.zeros(np.prod(mask_shape), dtype=mask_dtype)\n    for lo, hi in zip(starts, ends):\n        mask[lo:hi] = 1\n    return mask.reshape(mask_shape[::-1]).T\n\n\ndef rle_encode(mask):\n    \"\"\"Helper to encode binary mask to RLE string\"\"\"\n    pixels = mask.T.flatten()\n    pixels = np.concatenate([[0], pixels, [0]])\n    rle = np.where(pixels[1:] != pixels[:-1])[0] + 1\n    rle[1::2] -= rle[::2]\n    return rle.tolist()","d9006a16":"masks = np.zeros((len(rles), height, width), dtype = np.uint8)\nfor num in range(num_instances):\n    masks[num, :, :] = rle_decode(rles[num], (height, width), np.uint8)\n\nprint(masks.shape) ","0c3efea6":"def visualize_bbox(img, bbox, color=(255, 255, 0), thickness=2):  \n    \"\"\"Helper to add bboxes to images \n    Args:\n        img : image as open-cv numpy array\n        bbox : boxes as a list or numpy array in pascal_voc fromat [x_min, y_min, x_max, y_max]  \n        color=(255, 255, 0): boxes color \n        thickness=2 : boxes line thickness\n    \"\"\"\n    x_min, y_min, x_max, y_max = bbox\n    x_min, y_min, x_max, y_max = int(x_min), int(y_min), int(x_max), int(y_max)\n    cv2.rectangle(img, (x_min, y_min), (x_max, y_max), color=color, thickness=thickness)\n    return img\n\n\ndef plot_image_anns(image, masks, boxes=None):\n    \"\"\"Helper to plot images with bboxes and masks \n    Args:\n        image: image as open-cv numpy array, original and augmented\n        masks: setof binary masks, original and augmented \n        bbox : boxes as a list or numpy array, original and augmented       \n    \"\"\"\n    # glue binary masks together\n    one_mask = np.zeros_like(masks[0])\n    for i, mask in enumerate(masks):\n        one_mask += (mask > 0).astype(np.uint8) * (11-i) # (11-i) so my inner artist is happy with the masks colors     \n      \n    if boxes is not None:\n        for box in boxes:\n            image = visualize_bbox(image, box)\n            \n    # for binary masks we get one channel and need to convert to RGB for visualization\n    mask_rgb = label2rgb(one_mask, bg_label=0)            \n        \n    f, ax = plt.subplots(1, 2, figsize=(16, 16))             \n    ax[0].imshow(image)\n    ax[0].set_title('Original image')     \n    ax[1].imshow(mask_rgb, interpolation='nearest')\n    ax[1].set_title('Original mask')\n    f.tight_layout()\n    plt.show() \n\n\ndef plot_image_aug(image, image_aug, masks, aug_masks, boxes, aug_boxes):\n    \"\"\"Helper to plot images with bboxes and masks and their augmented versions \n    Args:\n        image, image_aug: image as open-cv numpy array, original and augmented\n        masks, aug_masks:setof binary masks, original and augmented \n        bbox, aug_boxes : boxes as a list or numpy array, original and augmented       \n    \"\"\"\n    # glue masks together\n    one_mask = np.zeros_like(masks[0])\n    for i, mask in enumerate(masks):\n        one_mask += (mask > 0).astype(np.uint8) * (11-i)       \n    \n    one_aug_mask = np.zeros_like(aug_masks[0])\n    for i, augmask in enumerate(aug_masks):\n        one_aug_mask += (augmask > 0).astype(np.uint8) * (11-i) \n    \n    for box in boxes:\n        image = visualize_bbox(image, box)\n    for augbox in aug_boxes:\n        image_aug = visualize_bbox(image_aug, augbox)    \n        \n    # for binary masks we get one channel and need to convert to RGB for visualization\n    mask_rgb = label2rgb(one_mask, bg_label=0)            \n    mask_aug_rgb = label2rgb(one_aug_mask, bg_label=0) \n    \n    f, ax = plt.subplots(2, 2, figsize=(16, 16))             \n    ax[0, 0].imshow(img)\n    ax[0, 0].set_title('Original image')        \n    ax[0, 1].imshow(image_aug)\n    ax[0, 1].set_title('Augmented image')     \n    ax[1, 0].imshow(mask_rgb, interpolation='nearest')\n    ax[1, 0].set_title('Original mask')\n    ax[1, 1].imshow(mask_aug_rgb, interpolation='nearest')\n    ax[1, 1].set_title('Augmented mask')\n    f.tight_layout()\n    plt.show() ","5c121191":"plot_image_anns(image, masks)","cdcaaa3a":"def get_boxes_from_masks(masks):\n    \"\"\" Helper, gets bounding boxes from masks \"\"\"\n    coco_boxes = []\n    for mask in masks:\n        pos = np.nonzero(mask)\n        xmin = np.min(pos[1])\n        xmax = np.max(pos[1])\n        ymin = np.min(pos[0])\n        ymax = np.max(pos[0])             \n        coco_boxes.append([xmin, ymin, xmax, ymax])  \n    coco_boxes = np.asarray(coco_boxes, dtype=np.float32) \n    \n    return coco_boxes","47bc754f":"img = image.copy()\nboxes = get_boxes_from_masks(masks)\nplot_image_anns(img, masks, boxes)","3db33dc5":"D4_transforms = [A.Resize(height\/\/2, width\/\/2, interpolation=cv2.INTER_LINEAR, p=1), \n                # D4 Group augmentations\n                A.HorizontalFlip(p=1),\n                A.VerticalFlip(p=0.5),\n                A.RandomRotate90(p=0.5),\n                A.Transpose(p=0.5),                   \n                #A.Normalize()\n                ]\n\ngeom_transforms =  [A.ShiftScaleRotate(shift_limit=0.0625, scale_limit=0.1, rotate_limit=15, \n                       interpolation=cv2.INTER_LINEAR, border_mode=cv2.BORDER_CONSTANT, value=0, mask_value=0, p=0.5),\n                    # D4 Group augmentations\n                    A.HorizontalFlip(p=0.5),\n                    A.VerticalFlip(p=0.5),\n                    A.RandomRotate90(p=0.5),\n                    A.Transpose(p=0.5),\n                    # crop and resize  \n                    A.RandomSizedCrop((MAX_SIZE-100, MAX_SIZE), height\/\/2, width\/\/2, w2h_ratio=1.0, \n                                        interpolation=cv2.INTER_LINEAR, always_apply=False, p=0.5),  \n                    A.Resize(height\/\/2, width\/\/2, interpolation=cv2.INTER_LINEAR, p=1), \n                    #A.Normalize(),\n                    ]\n                 \nheavy_transforms = [A.RandomRotate90(),\n                    A.Flip(),\n                    A.Transpose(),\n                    A.GaussNoise(),\n                    A.ShiftScaleRotate(shift_limit=0.0625, scale_limit=0.1, rotate_limit=45, p=0.7),                    \n                    A.OneOf([\n                        A.MotionBlur(p=0.2),\n                        A.MedianBlur(blur_limit=3, p=0.1),\n                        A.Blur(blur_limit=3, p=0.1),\n                    ], p=0.5),                    \n                    A.OneOf([                        \n                        A.IAASharpen(),\n                        A.IAAEmboss(),\n                        A.RandomBrightnessContrast(),\n                    ], p=0.5),\n                    A.HueSaturationValue(p=0.3),\n                    #A.Normalize(),\n                    ]","d7fa1f4b":"bbox_params={'format':'pascal_voc', 'min_area': 0, 'min_visibility': 0, 'label_fields': ['category_id']} ","dfe8b669":"boxes = list(boxes) # you need to send bounding boxes to a list\nimg = image.copy()\n\naugs = A.Compose(D4_transforms, bbox_params=bbox_params, p=1)       \naugmented = augs(image=img, masks=masks, bboxes=boxes, category_id=labels)     \naug_img = augmented['image']\naug_masks = augmented['masks']\naug_boxes = augmented['bboxes']\n\nplot_image_aug(img, aug_img, masks, aug_masks, boxes, aug_boxes)","b0bf69b5":"img = image.copy()\n\naugs = A.Compose(geom_transforms, bbox_params=bbox_params, p=1)       \naugmented = augs(image=img, masks=masks, bboxes=boxes, category_id=labels)     \naug_img = augmented['image']\naug_masks = augmented['masks']\naug_boxes = augmented['bboxes']\n\nplot_image_aug(img, aug_img, masks, aug_masks, boxes, aug_boxes)","d68cb9bb":"for i in range(5):\n    img = image.copy()\n\n    augs = A.Compose(heavy_transforms, bbox_params=bbox_params, p=1)       \n    augmented = augs(image=img, masks=masks, bboxes=boxes, category_id=labels)     \n    aug_img = augmented['image']\n    aug_masks = augmented['masks']\n    aug_boxes = augmented['bboxes']\n\n    plot_image_aug(img, aug_img, masks, aug_masks, boxes, aug_boxes)","6c74ea5e":"import torch\nimport torch.utils.data\n\nclass DatasetAugs(torch.utils.data.Dataset):\n    \"\"\"\n    My Dummy dataset for instance segmentation with augs\n    \n        :param fold: integer, number of the fold\n        :param df: Dataframe with sample tokens\n        :param debug: if True, runs the debugging on few images\n        :param img_size: the desired image size to resize to        \n        :param input_dir: directory with imputs and targets (and maps, optionally)   \n        :param transforms: list of transformations\n        \n        \"\"\"    \n    def __init__(self, fold: int, df: pd.DataFrame,                  \n                 debug: bool, img_size: int, \n                 input_dir: str, transforms = None, \n                 ):\n\n        super(DatasetAugs, self).__init__()  # inherit it from torch Dataset\n        self.fold = fold\n        self.df = df        \n        self.debug = debug\n        self.img_size = img_size\n        self.input_dir = input_dir\n        self.transforms = transforms\n        self.classes = df.classes.unique()\n    \n        if self.debug:\n            self.df = self.df.head(16)\n            print('Debug mode, samples: ', self.df.samples)  \n        self.samples = list(self.df.samples)\n\n    def __len__(self):\n        return len(self.samples)\n    \n    def __getitem__(self, idx):\n        sample = self.samples[idx]\n        input_filepath = '{}\/{}'.format(self.input_dir, sample)        \n        # load image\n        im = cv2.imread(input_filepath, cv2.IMREAD_UNCHANGED) \n        im = cv2.cvtColor(im, cv2.COLOR_BGR2RGB)     \n\n        # get annotations\n        labels, rles = img_masks_targets(df, img_id = sample)\n        masks = np.zeros((len(rles), height, width), dtype = np.uint8)\n        for num in range(num_instances):\n            masks[num, :, :] = rle_decode(rles[num], (height, width), np.uint8)        \n         \n        # get boxes from masks\n        boxes = get_boxes_from_masks(masks)    \n        boxes = list(boxes)    \n        \n        # augment image and targets\n        if self.transforms is not None:\n            bbox_params={'format':'pascal_voc', 'min_area': 5, 'min_visibility': 0.5, 'label_fields': ['category_id']}\n            augs = A.Compose(self.transforms, bbox_params=bbox_params, p=1)       \n            augmented = augs(image=im, masks=masks, bboxes=boxes, category_id=labels)     \n            im = augmented['image']\n            masks = augmented['masks']\n            boxes = augmented['bboxes']       \n                                                    \n        # targets to tensor\n        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n        labels = torch.as_tensor(labels, dtype=torch.int64)\n        masks = torch.as_tensor(masks, dtype=torch.uint8)\n        image_id = torch.tensor([idx])\n        area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n        iscrowd = torch.zeros((len(boxes),), dtype=torch.int64)  \n\n        target = {}\n        target[\"boxes\"] = boxes\n        target[\"labels\"] = labels\n        target[\"masks\"] = masks\n        target[\"image_id\"] = image_id\n        target[\"area\"] = area\n        target[\"iscrowd\"] = iscrowd  \n        \n        im = torch.from_numpy(im.transpose(2,0,1)) # channels first\n          \n        return im, target","8b3ffa5f":"print('Thanks for reading and happy halloween !!!')","208a4975":"Let's test some other augmentations sets","cb71406f":"For instance segmentation models, like Mask-R-CNN and Co., you also need bounding boxes. COCO dataset provides them, but often you'll get only masks. \nNo problem, we will generate bboxes from the masks then.","afad19b5":"Now let's create a list of transforms we want to apply for an image and annotations.\n\nThe choice of augmentations depends on your task. The common sets include: \n\n    D4 symmetry group augmentations for satellite imaging data; \n    \n    crops, rotation and scales with varying brightness and colors for classification; \n    \n    weather comditions simulation for camera-based self-driving tasks; \n    \n    noise, blur and mild rotations for X-Ray chest scans, and so on.\n\nHere we will make some lists for experimenting. In practice, it's better to create transforms.py file and put it all there. \n\nPlease note: A.Normalize function -- normalizes image as in ImageNet dataset training. It's recommended to do it before sending data to your model, especially if you use pretrained on ImageNet backbone. Here I commented it for the visualization puproses (so that colors look right, my inner artist again :)) ","00dfcc5a":"We need to choose the bbox format, please refer to the library docs here: https:\/\/albumentations.readthedocs.io\/en\/latest\/api\/core.html#albumentations.core.composition.BboxParams","7faff639":"Set the path to the data. I uploaded an image and annotations here for convenience","07cac12c":"The last but not least, you probably want to augment your images on the fly during training. In this case you may want to include them in your Dataset. \nOn PyTorch it it something like that:\n\n","a104b99f":"Define some helper functions to visualize data and labels","f18026e6":"Masks for the image often come in the run-length encoded format (RLE). We need to convert them to binary masks to plot and\/or augment. \nHere are some helpers for RLE encoding and decoding taken from here: https:\/\/www.kaggle.com\/paulorzp\/run-length-encode-and-decode","aefe5260":"Let's load an image with annotations","31569edf":"In this notebook we will use popular augmentations libruary, **Albumentations** (https:\/\/github.com\/albu\/albumentations) to perform augmentations of images together with multiple annotations, similar to those in the coco dataset. You can simply install it locally using: \n\npip install albumentations\n\nWe consider an image for instance segmentation with several objects, each one having a lable, a bounding box (bbox), and a binary mask.","d18df165":"Set common constants. Hint: in practice you may create config.py file, add them there and then just do \n\nfrom configs import ...","caa51b36":"Now we are ready to apply augmentations to the image, set of bboxes, and set of masks. It requires bboxes as a list, so we send it to the list format.","2fe603c7":"Let's look at our image and corresponding masks"}}