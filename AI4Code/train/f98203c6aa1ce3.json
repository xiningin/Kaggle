{"cell_type":{"a32278e5":"code","75eda8b0":"code","4bdfce74":"code","fb112ce6":"code","783612a3":"code","a7e23e8c":"code","72aa7556":"code","0fbc3b4c":"code","f35b78f6":"code","ae7e794a":"markdown","0df9ef62":"markdown","3732744e":"markdown","e70c0c67":"markdown","750710f1":"markdown","be1a6d63":"markdown","d075723a":"markdown","e8835155":"markdown","83f52e64":"markdown"},"source":{"a32278e5":"import os\nimport time\nimport pickle\nimport random\nimport numpy as np\nimport pandas as pd\nfrom tqdm import tqdm\nfrom sklearn.metrics import log_loss, roc_auc_score\n\nimport torch\nimport torch.nn as nn\nfrom torch.autograd import Variable\nfrom torch.utils.data import DataLoader\nfrom torch.nn import CrossEntropyLoss, MSELoss\nfrom torch.nn.modules.loss import _WeightedLoss\nimport torch.nn.functional as F\n\npd.set_option('display.max_columns', 100)\npd.set_option('display.max_rows', 100)\n\nDATA_PATH = '..\/input\/jane-street-market-prediction\/'\n\nNFOLDS = 5\n\nTRAIN = False\nCACHE_PATH = '..\/input\/mlp012003weights'\nXGBOOST_PATH = '..\/input\/xgboost'\n\ndef save_pickle(dic, save_path):\n    with open(save_path, 'wb') as f:\n    # with gzip.open(save_path, 'wb') as f:\n        pickle.dump(dic, f)\n\ndef load_pickle(load_path):\n    with open(load_path, 'rb') as f:\n    # with gzip.open(load_path, 'rb') as f:\n        message_dict = pickle.load(f)\n    return message_dict\n\nfeat_cols = [f'feature_{i}' for i in range(130)]\n\ntarget_cols = ['action', 'action_1', 'action_2', 'action_3', 'action_4']\n\nf_mean = np.load(f'{CACHE_PATH}\/f_mean_online.npy')\n\n\n##### Making features\nall_feat_cols = [col for col in feat_cols]\nall_feat_cols.extend(['cross_41_42_43', 'cross_1_2'])\n\n##### Model&Data fnc\nclass Model(nn.Module):\n    def __init__(self):\n        super(Model, self).__init__()\n        self.batch_norm0 = nn.BatchNorm1d(len(all_feat_cols))\n        self.dropout0 = nn.Dropout(0.2)\n\n        dropout_rate = 0.2\n        hidden_size = 256\n        self.dense1 = nn.Linear(len(all_feat_cols), hidden_size)\n        self.batch_norm1 = nn.BatchNorm1d(hidden_size)\n        self.dropout1 = nn.Dropout(dropout_rate)\n\n        self.dense2 = nn.Linear(hidden_size+len(all_feat_cols), hidden_size)\n        self.batch_norm2 = nn.BatchNorm1d(hidden_size)\n        self.dropout2 = nn.Dropout(dropout_rate)\n\n        self.dense3 = nn.Linear(hidden_size+hidden_size, hidden_size)\n        self.batch_norm3 = nn.BatchNorm1d(hidden_size)\n        self.dropout3 = nn.Dropout(dropout_rate)\n\n        self.dense4 = nn.Linear(hidden_size+hidden_size, hidden_size)\n        self.batch_norm4 = nn.BatchNorm1d(hidden_size)\n        self.dropout4 = nn.Dropout(dropout_rate)\n\n        self.dense5 = nn.Linear(hidden_size+hidden_size, len(target_cols))\n\n        self.Relu = nn.ReLU(inplace=True)\n        self.PReLU = nn.PReLU()\n        self.LeakyReLU = nn.LeakyReLU(negative_slope=0.01, inplace=True)\n        # self.GeLU = nn.GELU()\n        self.RReLU = nn.RReLU()\n\n    def forward(self, x):\n        x = self.batch_norm0(x)\n        x = self.dropout0(x)\n\n        x1 = self.dense1(x)\n        x1 = self.batch_norm1(x1)\n        # x = F.relu(x)\n        # x = self.PReLU(x)\n        x1 = self.LeakyReLU(x1)\n        x1 = self.dropout1(x1)\n\n        x = torch.cat([x, x1], 1)\n\n        x2 = self.dense2(x)\n        x2 = self.batch_norm2(x2)\n        # x = F.relu(x)\n        # x = self.PReLU(x)\n        x2 = self.LeakyReLU(x2)\n        x2 = self.dropout2(x2)\n\n        x = torch.cat([x1, x2], 1)\n\n        x3 = self.dense3(x)\n        x3 = self.batch_norm3(x3)\n        # x = F.relu(x)\n        # x = self.PReLU(x)\n        x3 = self.LeakyReLU(x3)\n        x3 = self.dropout3(x3)\n\n        x = torch.cat([x2, x3], 1)\n\n        x4 = self.dense4(x)\n        x4 = self.batch_norm4(x4)\n        # x = F.relu(x)\n        # x = self.PReLU(x)\n        x4 = self.LeakyReLU(x4)\n        x4 = self.dropout4(x4)\n\n        x = torch.cat([x3, x4], 1)\n\n        x = self.dense5(x)\n\n        return x\n\nif True:\n    #if torch.cuda.is_available():\n    device = torch.device(\"cuda:0\")\n    #else:\n    #    device = torch.device(\"cpu\")\n\n    model_list = []\n    tmp = np.zeros(len(feat_cols))\n    for _fold in range(NFOLDS):\n        torch.cuda.empty_cache()\n        model = Model()\n        model.to(device)\n        model_weights = f\"{CACHE_PATH}\/online_model{_fold}.pth\"\n        model.load_state_dict(torch.load(model_weights))\n        model.eval()\n        model_list.append(model)\n\n","75eda8b0":"from tensorflow.keras.layers import Input, Dense, BatchNormalization, Dropout, Concatenate, Lambda, GaussianNoise, Activation\nfrom tensorflow.keras.models import Model, Sequential\nfrom tensorflow.keras.losses import BinaryCrossentropy\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom tensorflow.keras.layers.experimental.preprocessing import Normalization\nimport tensorflow as tf\nimport tensorflow_addons as tfa\n\nimport numpy as np\nimport pandas as pd\nfrom tqdm import tqdm\nfrom random import choices\n\n\nSEED = 1111\n\nnp.random.seed(SEED)\n\n# fit\ndef create_mlp(\n    num_columns, num_labels, hidden_units, dropout_rates, label_smoothing, learning_rate\n):\n\n    inp = tf.keras.layers.Input(shape=(num_columns,))\n    x = tf.keras.layers.BatchNormalization()(inp)\n    x = tf.keras.layers.Dropout(dropout_rates[0])(x)\n    for i in range(len(hidden_units)):\n        x = tf.keras.layers.Dense(hidden_units[i])(x)\n        x = tf.keras.layers.BatchNormalization()(x)\n        x = tf.keras.layers.Activation(tf.keras.activations.swish)(x)\n        x = tf.keras.layers.Dropout(dropout_rates[i + 1])(x)\n    \n    x = tf.keras.layers.Dense(num_labels)(x)\n    out = tf.keras.layers.Activation(\"sigmoid\")(x)\n\n    model = tf.keras.models.Model(inputs=inp, outputs=out)\n    model.compile(\n        optimizer=tfa.optimizers.RectifiedAdam(learning_rate=learning_rate),\n        loss=tf.keras.losses.BinaryCrossentropy(label_smoothing=label_smoothing),\n        metrics=tf.keras.metrics.AUC(name=\"AUC\"),\n    )\n\n    return model\n\nepochs = 200\nbatch_size = 4096\nhidden_units = [160, 160, 160]\ndropout_rates = [0.2, 0.2, 0.2, 0.2]\nlabel_smoothing = 1e-2\nlearning_rate = 1e-3\n\ntf.keras.backend.clear_session()\ntf.random.set_seed(SEED)\nclf = create_mlp(\n    len(feat_cols), 5, hidden_units, dropout_rates, label_smoothing, learning_rate\n    )\nclf.load_weights('..\/input\/jane-street-with-keras-nn-overfit\/model.h5')\n\ntf_models = [clf]","4bdfce74":"import joblib\nfrom xgboost import XGBClassifier\nimport xgboost as xgb","fb112ce6":"XGBOOST_PATH = '..\/input\/xgboost'\n\n# Load median, a pandas series, from csv. I am not sure if there is another simpler way to this\nmedian_df = pd.read_csv(XGBOOST_PATH+'\/median_pd_130_features.csv', index_col=False, header=0);\nmedian_df.columns = range(median_df.shape[1])\nmedian_df = median_df.transpose()\nmedian_df.columns = median_df.iloc[0]\nmedian_df.drop(median_df.index[0], inplace=True)\n#median_df.reset_index(drop=True)\nmedian = median_df.iloc[0]","783612a3":"xgb_file_suffix = \"-n-500-d-8-sub-0.9-lr-0.05.joblib\"\nxgb_clfs = []\nfor i in range(5):\n    xgb_clf = joblib.load(XGBOOST_PATH + \"\/xgb\" + str(i) + xgb_file_suffix)\n    xgb_clfs.append(xgb_clf)","a7e23e8c":"from sklearn.model_selection import train_test_split\nimport datatable\n\nLOCAL_TEST = True\n\nif LOCAL_TEST:    \n\n    datatable_frame = datatable.fread('..\/input\/jane-street-market-prediction\/train.csv')\n    \n    df_raw = datatable_frame.to_pandas()\n    \n    del datatable_frame\n\n    df_raw = df_raw.query('date > 85').reset_index(drop = True) \n    df_raw = df_raw[df_raw['weight'] != 0]\n    \n    df_raw['action'] = ((df_raw['resp'].values) > 0).astype(int)\n\n    \n    df_train, df_test = train_test_split(df_raw, test_size=0.2, shuffle=True, random_state=150)\n    \n    features = [c for c in df_train.columns if \"feature\" in c]\n    all_feat_cols = [col for col in features]\n    \n    del df_raw, df_train\n    \n    neutral_values = median\n    df_test.fillna(neutral_values,inplace=True)\n\n    X_test = df_test.loc[:, df_test.columns.str.contains('feature')]\n    \n    X_test_extended = df_test.loc[:, df_test.columns.str.contains('feature')]\n    X_test_extended['cross_41_42_43'] = X_test_extended['feature_41'] + X_test_extended['feature_42'] + X_test_extended['feature_43']\n    X_test_extended['cross_1_2'] = X_test_extended['feature_1'] \/ (X_test_extended['feature_2'] + 1e-5)\n    \n    all_feat_cols.extend(['cross_41_42_43', 'cross_1_2'])\n\n    #y_test = np.stack([(df_test[c] > 0).astype('int') for c in resp_cols]).T\n\n    y_action_test = df_test['action'].to_numpy()\n    \n    del df_test\n    \n\n\n    ","72aa7556":"from torch.utils.data import Dataset, DataLoader\n\nif LOCAL_TEST:\n    class TestDataset(Dataset):\n        def __init__(self, df):\n            self.features = df[all_feat_cols].values\n\n            #self.label = df[target_cols].values.reshape(-1, len(target_cols))\n\n        def __len__(self):\n            return len(self.features)\n\n        def __getitem__(self, idx):\n            return {\n                'features': torch.tensor(self.features[idx], dtype=torch.float)\n                #'label': torch.tensor(self.label[idx], dtype=torch.float)\n            }","0fbc3b4c":"from sklearn import metrics\n\nif LOCAL_TEST:\n\n    # ResNet with Pytorch\n    BATCH_SIZE = 8192\n    test_set = TestDataset(X_test_extended)\n    test_loader = DataLoader(test_set, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)\n    \n    for model in model_list:\n        model.eval()\n        \n    torch_preds = []\n    for data in test_loader:\n        feature_data = data['features'].to(device)\n        multiple_preds = np.zeros((len(feature_data), len(model_list)))\n        for model in model_list:\n            multiple_preds += model(torch.tensor(feature_data, dtype=torch.float).to(device)).sigmoid().detach().cpu().numpy() \/ NFOLDS\n            \n        torch_pred = np.median(multiple_preds, axis=1)\n        torch_preds.append(torch_pred)\n        \n    resnet_preds = np.concatenate(torch_preds)\n    \n    \n    # MLP with Tensorflow\n    mlp_preds = np.median(tf_models[0](X_test.values, training = False).numpy(), axis=1)  # currently we only have one tf model\n    \n\n    # XGBoost\n    five_preds = []\n\n    for i in range(5):  #len(resp_cols)\n        pred_prob = xgb_clfs[i].predict_proba(X_test)[:,1]    # arr[0] is the probability for class 0, arr[1] is the probability for class 1\n        five_preds.append(pred_prob)\n    five_preds = np.array(five_preds).T\n    xgboost_preds = np.median(five_preds, axis=1)\n\n\n    # Blend the three models\n    th = 0.5\n\n    preds = np.mean(np.vstack([resnet_preds, mlp_preds, xgboost_preds]).T, axis=1)\n    actions_predicted = np.where(preds >= th, 1, 0).astype(int)\n\n    print(preds.shape)\n    print(actions_predicted.shape)\n\n    print(metrics.accuracy_score(y_action_test, actions_predicted))","f35b78f6":"if not LOCAL_TEST:\n    import janestreet\n    env = janestreet.make_env()\n    env_iter = env.iter_test()\n\n    th = 0.5\n    for (test_df, pred_df) in tqdm(env_iter):\n        if test_df['weight'].item() > 0:\n            x_tt = test_df.loc[:, feat_cols].values\n            if np.isnan(x_tt.sum()):\n                x_tt = np.nan_to_num(x_tt) + np.isnan(x_tt) * f_mean\n\n            cross_41_42_43 = x_tt[:, 41] + x_tt[:, 42] + x_tt[:, 43]\n            cross_1_2 = x_tt[:, 1] \/ (x_tt[:, 2] + 1e-5)\n            feature_inp = np.concatenate((\n                x_tt,\n                np.array(cross_41_42_43).reshape(x_tt.shape[0], 1),\n                np.array(cross_1_2).reshape(x_tt.shape[0], 1),\n            ), axis=1)\n\n            # torch_pred\n            torch_pred = np.zeros((1, len(target_cols)))\n            for model in model_list:\n                torch_pred += model(torch.tensor(feature_inp, dtype=torch.float).to(device)).sigmoid().detach().cpu().numpy() \/ NFOLDS\n            torch_pred = np.median(torch_pred)\n            \n            # tf_pred\n            tf_pred = np.median(np.mean([model(x_tt, training = False).numpy() for model in tf_models],axis=0))\n            \n            # xgboost\n            x_tt = test_df.loc[:, feat_cols].values\n            if np.isnan(x_tt.sum()):\n                x_tt = np.nan_to_num(x_tt) + np.isnan(x_tt) * median.values\n            five_preds = []\n\n            for i in range(5):\n                pred_prob = xgb_clfs[i].predict_proba(x_tt)[:,1]    # arr[0] is the probability for class 0, arr[1] is the probability for class 1\n                five_preds.append(pred_prob)\n            five_preds = np.array(five_preds).T\n            xgb_pred = np.median(five_preds, axis=1)\n            \n            \n            # avg\n            pred = (torch_pred + tf_pred + xgb_pred) \/ 3.0\n            \n            pred_df.action = np.where(pred >= th, 1, 0).astype(int)\n        else:\n            pred_df.action = 0\n        env.predict(pred_df)","ae7e794a":"## Inference with online testing API","0df9ef62":"## Local accuracy test","3732744e":"## MLP part","e70c0c67":"### ResNet train code:\nhttps:\/\/www.kaggle.com\/a763337092\/neural-network-starter-pytorch-version<br\/>\nhttps:\/\/www.kaggle.com\/a763337092\/pytorch-resnet-starter-training<br\/>\n\n### MLP training code:\nhttps:\/\/www.kaggle.com\/code1110\/jane-street-with-keras-nn-overfit<br\/>\n\n### XGBoost training code: \nhttps:\/\/www.kaggle.com\/dongwenjian\/key-notebook-xgboost\/edit<br\/>","750710f1":"In this notebook, first we use Pytorch to implement a ResNet model. If run alone, we find this ResNet model could get a score 5k+ on the public test set. \n\nThen we use Tensorflow to implement a MLP model. If run alone, the model could get a score 9k+ on the public test plus. \n\nThen we add the XGBoost model. If run alone, the model could get a score 7k+ on the public test set. \n\nWhen we blend them together to form an ensemble model, we find we can a score 10k+ on the public test set. We believe this ensembled model will solve better the overfitting problem on the private dataset. \n\nNote that, in this notebook, we load the pre-trained model from other notebooks, and we do not do the training part in this notebook. \n\nAlso note that, for ResNet and MLP, we use mean value of each column to fill the missing value. ","be1a6d63":"The ResNet part and the MLP part are based on the amazing notebook of Lindada\u7131\u7131\u7131 :\n\nhttps:\/\/www.kaggle.com\/a763337092\/blending-tensorflow-and-pytorch\n\nWe add our xgboost model for better solving the overfitting problem.","d075723a":"## Blending ResNet, MLP, & XGBoost","e8835155":"## XGBoost Part","83f52e64":"## ResNet part"}}