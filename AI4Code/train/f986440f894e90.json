{"cell_type":{"b4234392":"code","42214505":"code","873edf9d":"code","6843a751":"code","54f54658":"code","38fae198":"code","9bcb70af":"code","41e2dd63":"code","25f3d975":"code","eb45afaa":"code","f14ef408":"code","63bfeb26":"code","680ad2a8":"code","8fc5952a":"code","1370a641":"code","b42c99fb":"code","a6930999":"code","fb29c00b":"code","21db609b":"code","866a0b1b":"code","ab1ae67b":"code","143e6cc4":"markdown","ca84f927":"markdown","b767a3f9":"markdown","ce7bcab1":"markdown","91b62ead":"markdown","ef957685":"markdown","2b3d0b8e":"markdown","4fed08a4":"markdown","d57e7aa4":"markdown"},"source":{"b4234392":"import numpy as np #\nimport pandas as pd \nimport matplotlib.pyplot as plt\n\nimport random\nimport shutil \nimport torch\nimport torchvision\nimport os\nimport copy\nfrom tqdm import tqdm\nrandom.seed(6)\nnp.random.seed(6)\ntorch.manual_seed(6)\ntorch.cuda.manual_seed(6)\ntorch.backends.cudnn.deterministic = True","42214505":"data_root = '..\/input\/plates\/plates\/'\nprint(os.listdir(data_root))","873edf9d":"train_dir = 'train'\n\nclass_names = ['cleaned', 'dirty']\n\nfor class_name in class_names:\n    os.makedirs(os.path.join(train_dir, class_name), exist_ok=True)\n\nfor class_name in class_names:\n    source_dir = os.path.join(data_root, 'train', class_name)\n    for i, file_name in enumerate(tqdm(os.listdir(source_dir))):\n        dest_dir = os.path.join(train_dir, class_name) \n        shutil.copy(os.path.join(source_dir, file_name),\n                    os.path.join(dest_dir, file_name))","6843a751":"from torchvision import transforms, models\n   \naug_lvl_1 = transforms.Compose([              #!!!\n    transforms.RandomHorizontalFlip(p=0.75),                                \n    transforms.TenCrop(224, vertical_flip=True),\n    transforms.Lambda(\n        lambda crops: torch.stack([aug_lvl_2(crop) for crop in crops]))\n])    \n\n\naug_lvl_2 = transforms.Compose([              #!!!            \n    transforms.ColorJitter(\n        brightness = 0.175,   \n        contrast   = 0.175,   \n        saturation = 0.195,   \n        hue        = (0.1, 0.25)),  \n    transforms.RandomRotation(360),\n    transforms.ToTensor(),\n    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n    transforms.Lambda(\n        lambda x: x[np.random.permutation(3), :, :])                            \n])\n\ntrain_transforms = transforms.Compose([  \n    transforms.Resize((224, 224)),\n    transforms.TenCrop(224),\n    transforms.Lambda(\n        lambda crops: torch.stack([aug_lvl_1(crop) for crop in crops]))\n])\n\n\"\"\"\n\u041e\u0431\u0440\u0430\u0442\u0438\u0442\u0435 \u0432\u043d\u0438\u043c\u0430\u043d\u0438\u0435 \u043d\u0430 \u0441\u0442\u0440\u043e\u043a\u0438, \u043f\u043e\u043c\u0435\u0447\u0435\u043d\u043d\u044b\u0435 #!!!. \u041e\u043d\u0438 \u0438\u0433\u0440\u0430\u044e\u0442 \u0432\u0430\u0436\u043d\u0443\u044e \u0440\u043e\u043b\u044c \u0432 \u043f\u0440\u043e\u0446\u0435\u0441\u0441\u0435\n\u0430\u0443\u0433\u043c\u0435\u043d\u0442\u0430\u0446\u0438\u0438.\u0421\u043d\u0430\u0447\u0430\u043b\u0430 \u043a\u0430\u0440\u0442\u0438\u043d\u043a\u0430 \u0443\u043c\u0435\u043d\u044c\u0448\u0430\u0435\u0442\u0441\u044f \u0434\u043e \u0440\u0430\u0437\u043c\u0435\u0440\u043e\u0432 224x224, \u0437\u0430\u0442\u0435\u043c \u0440\u0430\u0437\u0431\u0438\u0432\u0430\u0435\u0442\u0441\u044f\n\u043d\u0430 10 \u043a\u0430\u0440\u0442\u0438\u043d\u043e\u043a, \u0430 \u043f\u043e\u0442\u043e\u043c \u043a\u0430\u0436\u0434\u0430\u044f \u0438\u0437 \u043d\u0438\u0445 \u0440\u0430\u0437\u0431\u0438\u0432\u0430\u0435\u0442\u0441\u044f \u0435\u0449\u0435 \u043d\u0430 10 \u043a\u0430\u0440\u0442\u0438\u043d\u043e\u043a \u0432 aug_lvl_1.\n\u0412 aug_lvl_2 \u0440\u0430\u043d\u0434\u043e\u043c\u043d\u043e \u0438\u0437\u043c\u0435\u043d\u044f\u0435\u0442\u0441\u044f \u044f\u0440\u043a\u043e\u0441\u0442\u044c, \u043a\u043e\u043d\u0442\u0440\u0430\u0441\u043d\u043e\u0441\u0442\u044c \u0438 \u043d\u0430\u0441\u044b\u0449\u0435\u043d\u0438\u0435 \u043a\u0430\u0440\u0442\u0438\u043d\u043a\u0438\n\u0441 \u043f\u043e\u0441\u043b\u0435\u0434\u0443\u044e\u0449\u0435\u0439 \u0442\u0440\u0430\u043d\u0444\u043e\u0440\u043c\u0430\u0446\u0438\u0435\u0439 \u0432 \u0442\u0435\u043d\u0437\u043e\u0440 \u0438 \u0435\u0433\u043e \u043d\u043e\u0440\u043c\u0430\u043b\u0438\u0437\u0430\u0446\u0438\u0435\u0439.\n\u041a\u0430\u0436\u0434\u0430\u044f \u043a\u0430\u0440\u0442\u0438\u043d\u043a\u0430 \u0438\u0437 train \u0442\u0440\u0430\u043d\u0441\u0444\u043e\u0440\u043c\u0438\u0440\u0443\u0435\u0442\u0441\u044f \u0432 \u0442\u0435\u043d\u0437\u043e\u0440 \u0440\u0430\u0437\u043c\u0435\u0440\u043d\u043e\u0441\u0442\u0438 [10, 10, 3, 224, 224]\n\"\"\"","54f54658":"def create_target_set(variable, size_target, dtype = torch.long): #\u042d\u0442\u0430 \u0444\u0443\u043d\u043a\u0438\u044f \u043d\u0443\u0436\u043d\u0430, \u0447\u0442\u043e\u0431\u044b \"\u0440\u0430\u0437\u043c\u043d\u043e\u0436\u0438\u0442\u044c\" \u043c\u0435\u0442\u043a\u0438. \n\n    target = torch.tensor([variable], dtype = dtype, requires_grad=False)\n    target = torch.nn.functional.pad(target,\n                                     (size_target\/\/2, size_target\/\/2 - 1),\n                                     \"constant\",\n                                     variable)\n    \n    return target\n\ndef augmentation(dir_data, dir_transforms, batch_size, shuffle, num_workers):  #\u041a\u043b\u044e\u0447\u0435\u0432\u0430\u044f \u0444\u0443\u043d\u043a\u0446\u0438\u044f \u0432 \u0434\u0430\u043d\u043d\u043e\u043c \u0440\u0435\u0448\u0435\u043d\u0438\u0438. \u041e\u043d\u0430 \u043f\u043e\u0437\u0432\u043e\u043b\u044f\u0435\u0442\n                                                                               #40 \u043a\u0430\u0440\u0442\u0438\u043d\u043e\u043a \u043f\u043e\u043b\u0443\u0447\u0438\u0442\u044c 4000 \u0442\u0435\u043d\u0437\u043e\u0440\u043e\u0432!\n    data = []\n    target = []\n\n    #\u0421\u043e\u0437\u0434\u0430\u043d\u0438\u0435 \u043d\u0430\u0431\u043e\u0440\u0430 \u0434\u0430\u043d\u043d\u044b\u0445\n    dataset = torchvision.datasets.ImageFolder(dir_data, dir_transforms)\n\n    #\u0418\u0437\u043c\u0435\u043d\u0435\u043d\u0438\u0435 \u0440\u0430\u0437\u043c\u0435\u0440\u043d\u043e\u0441\u0442\u0435\u0439 \u0442\u0435\u043d\u0437\u043e\u0440\u043e\u0432 \u0438 \"\u0440\u0430\u0437\u043c\u043d\u043e\u0436\u0435\u043d\u0438\u0435\" \u043c\u0435\u0442\u043e\u043a\n    for dt, trgt in tqdm(dataset):\n        data.append(dt.resize_(100, 3, 224, 224))\n        target.append(create_target_set(trgt, 100))\n\n    #\u041e\u0431\u044a\u0435\u0434\u0438\u043d\u0435\u043d\u0438\u0435 \u0442\u0435\u043d\u0437\u043e\u0440\u043e\u0432\n    dtst = list(\n        zip(torch.cat(data , dim = 0), torch.cat(target, dim = 0))\n        )\n    \n    #\u0421\u043e\u0437\u0434\u0430\u043d\u0438\u0435 dataloader\n    dataloader = torch.utils.data.DataLoader(\n        dtst,\n        batch_size = batch_size,\n        shuffle = shuffle,\n        num_workers = num_workers\n        )\n    \n    return dataloader","38fae198":"batch_size = 25\n\ntrain_dataloader = augmentation(\n    train_dir,\n    train_transforms,\n    batch_size = batch_size,\n    shuffle = True,\n    num_workers = batch_size\n    )\n","9bcb70af":"len(train_dataloader)","41e2dd63":"mean = np.array([0.485, 0.456, 0.406])\nstd = np.array([0.229, 0.224, 0.225])\ndef show_input(input_tensor, title=''):\n    image = input_tensor.permute(1, 2, 0).numpy()\n    image = std * image + mean\n    plt.imshow(image.clip(0, 1))\n    plt.title(title)\n    plt.show()\n    plt.pause(0.001)\n\n\nX_batch, y_batch = next(iter(train_dataloader))\n\nfor x_item, y_item in zip(X_batch, y_batch):\n    show_input(x_item, title=class_names[int(y_item)])","25f3d975":"def train_model(model, loss, optimizer, scheduler, num_epochs):\n    loss_hist = []\n    accuracy_hist = []\n\n    for epoch in range(num_epochs):\n        print('Epoch {}\/{}:'.format(epoch + 1, num_epochs), flush=True)\n\n        dataloader = train_dataloader\n        model.train()  \n            \n        running_loss = 0.\n        running_acc  = 0.\n\n        #\u041f\u0435\u0440\u0435\u0431\u043e\u0440 \u0434\u0430\u043d\u043d\u044b\u0445\n        for inputs, labels in tqdm(dataloader):\n            inputs = inputs.to(device)\n            labels = labels.to(device)\n\n            optimizer.zero_grad()\n            \n            #\u041f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u0430\u043d\u0438\u0435\n            preds = model(inputs)\n            loss_value = loss(preds, labels)\n            preds_class = preds.argmax(dim=1)\n\n            #\u0412\u044b\u0447\u0438\u0441\u043b\u0435\u043d\u0438\u0435 \u0438 \u0448\u0430\u0433 \u0433\u0440\u0430\u0434\u0438\u0435\u043d\u0442\u0430\n            loss_value.backward()\n            optimizer.step()\n\n            #\u0421\u0442\u0430\u0442\u0438\u0441\u0442\u0438\u043a\u0430\n            running_loss += loss_value.item()\n            running_acc += (preds_class == labels.data).float().mean().data.cpu().numpy()\n\n        scheduler.step() #\u041f\u0435\u0440\u0435\u043c\u0435\u0441\u0442\u0438\u043b \u0434\u0430\u043d\u043d\u044b\u0439 \u043e\u0431\u044a\u0435\u043a\u0442 \u0432 \u043a\u043e\u043d\u0435\u0446 \u043f\u0440\u043e\u0433\u0440\u0430\u043c\u043c\u044b \u0441\u043e\u0433\u043b\u0430\u0441\u043d\u043e \u0440\u0435\u043a\u043e\u043c\u0435\u043d\u0434\u0430\u0446\u0438\u0438 \u0432 \u0434\u043e\u043a\u0443\u043c\u0435\u043d\u0442\u0430\u0446\u0438\u0438 pytorch\n\n        epoch_loss = running_loss \/ len(dataloader)\n        epoch_acc = running_acc \/ len(dataloader)\n\n        loss_hist.append(epoch_loss)\n        accuracy_hist.append(epoch_acc)\n\n        print('\\nLoss: {:.4f} Acc: {:.4f}'.format(epoch_loss, epoch_acc), flush=True)\n\n    return loss_hist, accuracy_hist, model","eb45afaa":"model = torchvision.models.vgg13_bn(pretrained=True)","f14ef408":"for param in model.parameters():\n    param.requires_grad = True  \n\nmodel.classifier = torch.nn.Sequential(   \n    torch.nn.Linear(512 * 7 * 7, 8),      \n    torch.nn.ReLU(True),                  \n    torch.nn.Dropout(0.5),                \n    torch.nn.Linear(8, 8),\n    torch.nn.ReLU(True),\n    torch.nn.Dropout(0.5),\n    torch.nn.Linear(8, 2)\n)\n\"\"\"\n\u041a\u043b\u0430\u0441\u0441\u0438\u0444\u0438\u043a\u0430\u0442\u043e\u0440 \u0441\u0435\u0442\u0438 VGG \u043e\u0442\u043b\u0438\u0447\u0430\u0435\u0442\u0441\u044f \u043e\u0442 \u043a\u043b\u0430\u0441\u0441\u0438\u0444\u0438\u043a\u0430\u0442\u043e\u0440\u0430 ResNet. \u0421\u043e\u0433\u043b\u0430\u0441\u043d\u043e \n\u0434\u043e\u043a\u0443\u043c\u0435\u043d\u0442\u0430\u0446\u0438\u0438 pytorch, \u043a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u043e \u043d\u0435\u0439\u0440\u043e\u043d\u043e\u0432 \u0432 \u0441\u043a\u0440\u044b\u0442\u044b\u0445 \u043f\u043e\u043b\u043d\u043e\u0441\u0432\u044f\u0437\u043d\u044b\u0445 \u0441\u043b\u043e\u044f\u0445 \u0440\u0430\u0432\u043d\u043e 4096.\n\u0414\u043b\u044f \u0434\u0430\u043d\u043d\u043e\u0439 \u0437\u0430\u0434\u0430\u0447\u0438 \u043d\u0435\u0442 \u043d\u0435\u043e\u0431\u0445\u043e\u0434\u0438\u043c\u043e\u0441\u0442\u0438 \u043e\u0431\u0443\u0447\u0430\u0442\u044c \u0442\u0430\u043a\u043e\u0435 \u043a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u043e \u043d\u0435\u0439\u0440\u043e\u043d\u043e\u0432.\n\u0412\u043e\u0437\u044c\u043c\u0435\u043c \u0447\u0438\u0441\u043b\u043e, \u043f\u0440\u043e\u043f\u043e\u0440\u0446\u0438\u043e\u043d\u0430\u043b\u044c\u043d\u043e\u0435 \u043a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u0443 \u043a\u043b\u0430\u0441\u0441\u043e\u0432. 4096 - 1000 \u043a\u043b\u0430\u0441\u0441\u043e\u0432, \n\u0441\u043b\u0435\u0434\u043e\u0432\u0430\u0442\u0435\u043b\u044c\u043d\u043e \u0434\u043b\u044f 2\u0445 \u043a\u043b\u0430\u0441\u0441\u043e\u0432 \u044d\u0442\u043e 8-9 \u043d\u0435\u0439\u0440\u043e\u043d\u043e\u0432.\n\"\"\"\n\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\nmodel = model.to(device)\n\nloss = torch.nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr = 1.0e-5, amsgrad=True)\n\n# \u0421\u043a\u043e\u0440\u043e\u0441\u0442\u044c \u043e\u0431\u0443\u0447\u0435\u043d\u0438\u044f \u043f\u043e\u0441\u043b\u0435 \u043a\u0430\u0436\u0434\u043e\u0439 4\u043e\u0439 \u044d\u043f\u043e\u0445\u0438 \u0443\u043c\u043d\u043e\u0436\u0430\u0435\u0442\u0441\u044f \u043d\u0430 0.001.\nscheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size = 4, gamma = 0.001)","63bfeb26":"loss_history = []\naccuracy_histoty = []\nloss_history, accuracy_histoty, trained_model = \\\n     train_model(model, loss, optimizer, scheduler, num_epochs = 4);","680ad2a8":"fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10 ,7))\n\nax1.plot(accuracy_histoty)\nax1.set_title('Train Accuracy')\nfig.tight_layout()\n\nax2.plot(loss_history)\nax2.set_title('Train Loss');\nfig.tight_layout()","8fc5952a":"test_dir = 'test'\nshutil.copytree(os.path.join(data_root, 'test'), \n                os.path.join(test_dir, 'unknown'))","1370a641":"class ImageFolderWithPaths(torchvision.datasets.ImageFolder):\n    def __getitem__(self, index):\n        original_tuple = super(ImageFolderWithPaths, self).__getitem__(index)\n        path = self.imgs[index][0]\n        tuple_with_path = (original_tuple + (path,))\n        return tuple_with_path","b42c99fb":"test_transforms = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(),\n    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n])\n\n\ntest_dataset = ImageFolderWithPaths(test_dir, test_transforms)\n\ntest_dataloader = torch.utils.data.DataLoader(\n    test_dataset, batch_size=batch_size, shuffle=False, num_workers=batch_size)","a6930999":"model.eval()\n\ntest_predictions = []\ntest_img_paths = []\nfor inputs, labels, paths in tqdm(test_dataloader):\n    inputs = inputs.to(device)\n    labels = labels.to(device)\n    with torch.set_grad_enabled(False):\n        preds = trained_model(inputs)\n    test_predictions.append(\n        torch.nn.functional.softmax(preds, dim=1)[:,1].data.cpu().numpy())\n    test_img_paths.extend(paths)\n    \ntest_predictions = np.concatenate(test_predictions)","fb29c00b":"inputs, labels, paths = next(iter(test_dataloader))\n\nfor img, pred in zip(inputs, test_predictions):\n    show_input(img, title=pred)","21db609b":"submission_df = pd.DataFrame.from_dict({'id': test_img_paths, 'label': test_predictions})\nsubmission_df['label'] = submission_df['label'].map(lambda pred: 'dirty' if pred > 0.5 else 'cleaned')\nsubmission_df['id'] = submission_df['id'].str.replace('test\/unknown\/', '')\nsubmission_df['id'] = submission_df['id'].str.replace('.jpg', '')\nsubmission_df.set_index('id', inplace=True)\nsubmission_df.head(n=6)","866a0b1b":"submission_df.to_csv('submission.csv')","ab1ae67b":"!rm -rf train val test","143e6cc4":"\u0412\u0438\u0437\u0443\u0430\u043b\u0438\u0437\u0430\u0446\u0438\u044f \u0442\u0440\u0435\u043d\u0438\u0440\u043e\u0432\u043e\u0447\u043d\u044b\u0445 \u0434\u0430\u043d\u043d\u044b\u0445.","ca84f927":"\u0417\u0430\u043f\u0443\u0441\u043a\u0430\u0435\u043c \u043f\u0440\u043e\u0446\u0435\u0441\u0441 \u043e\u0431\u0443\u0447\u0435\u043d\u0438\u044f!","b767a3f9":"\u041f\u043e\u0441\u043a\u043e\u043b\u044c\u043a\u0443 \u0432\u0430\u043b\u0438\u0434\u0430\u0446\u0438\u043e\u043d\u043d\u043e\u0439 \u0432\u044b\u0431\u043e\u0440\u043a\u0438 \u043d\u0435\u0442, \u0438\u0437 \u0444\u0443\u043d\u043a\u0446\u0438\u0438 train_model \u044f \u0443\u0434\u0430\u043b\u0438\u043b \u0444\u0430\u0437\u044b train  \u0438 val.","ce7bcab1":"\u0422\u0435\u043f\u0435\u0440\u044c, \u043f\u043e\u0436\u0430\u043b\u0443\u0439, \u043e\u0434\u043d\u0430 \u0438\u0437 \u0433\u043b\u0430\u0432\u043d\u044b\u0445 \u0447\u0430\u0441\u0442\u0435\u0439 \u043a\u043e\u0434\u0430!","91b62ead":"\u0414\u0430\u043b\u0435\u0435 \u043f\u043e\u0441\u0442\u0440\u043e\u0438\u043c \u0433\u0440\u0430\u0444\u0438\u043a\u0438 Train Accuracy \u0438 Train Loss. \u0412\u0438\u0434\u043d\u043e, \u0447\u0442\u043e \u0444\u0443\u043d\u043a\u0446\u0438\u044f \u043f\u043e\u0442\u0435\u0440\u044c \u0435\u0449\u0435 \u043d\u0435 \u0432\u044b\u0448\u043b\u0430 \u043d\u0430 \u043f\u043b\u0430\u0442\u043e.","ef957685":"\u0414\u0430\u043d\u043d\u044b\u0439 \u043a\u043e\u0434 \u0440\u0430\u0437\u0440\u0430\u0431\u043e\u0442\u0430\u043d \u0432 \u0440\u0430\u043c\u043a\u0430\u0445 [\u043a\u0443\u0440\u0441\u0430 \u043f\u043e \u043d\u0435\u0439\u0440\u043e\u043d\u043d\u044b\u043c \u0441\u0435\u0442\u044f\u043c \u0438 \u043a\u043e\u043c\u043f\u044c\u0442\u0435\u0440\u043d\u043e\u043c\u0443 \u0437\u0440\u0435\u043d\u0438\u044e](https:\/\/stepik.org\/course\/50352\/syllabus). \u0421\u043f\u0430\u0441\u0438\u0431\u043e \u0431\u043e\u043b\u044c\u0448\u043e\u0435 \u0418\u0433\u043e\u0440\u044e \u0421\u043b\u0438\u043d\u044c\u043a\u043e \u0437\u0430 \u0435\u0433\u043e [\u043a\u043e\u0434](https:\/\/www.kaggle.com\/yellowduck\/baseline-in-pytorch) \u043d\u0430 Kaggle. \u0418\u043c\u0435\u043d\u043d\u043e \u043d\u0430 \u0435\u0433\u043e \u043e\u0441\u043d\u043e\u0432\u0435 \u0432 Google Colab \u0440\u0430\u0437\u0440\u0430\u0431\u043e\u0442\u0430\u043d\u043e \u0434\u0430\u043d\u043d\u043e\u0435 \u0440\u0435\u0448\u0435\u043d\u0438\u0435 \u0437\u0430\u0434\u0430\u0447\u0438. \u0422\u0430\u043a, \u043f\u043e\u0435\u0445\u0430\u043b\u0438!\n\u0418\u043c\u043f\u043e\u0440\u0442\u0438\u0440\u0443\u0435\u043c \u0432\u0441\u0435 \u043d\u0435\u043e\u0431\u0445\u043e\u0434\u0438\u043c\u044b\u0435 \u0431\u0438\u043b\u0438\u043e\u0442\u0435\u043a\u0438.","2b3d0b8e":"\u0422\u0435\u043f\u0435\u0440\u044c \u0437\u0430\u0433\u0440\u0443\u0437\u0438\u043c \u043f\u0440\u0435\u0434\u043e\u0431\u0443\u0447\u0435\u043d\u043d\u0443\u044e \u043d\u0435\u0439\u0440\u043e\u0441\u0435\u0442\u044c. \u041e\u0431\u0443\u0447\u0430\u0442\u044c \u0431\u0443\u0434\u0435\u043c \u0441\u0435\u0442\u044c [VGG-13 \u0441 \u0431\u0430\u0442\u0447 \u043d\u043e\u0440\u043c\u0430\u043b\u0438\u0437\u0430\u0446\u0438\u0435\u0439](https:\/\/pytorch.org\/docs\/stable\/_modules\/torchvision\/models\/vgg.html#vgg13). \u0422\u0440\u0435\u043d\u0438\u0440\u043e\u0432\u043e\u0447\u043d\u044b\u0445 \u0434\u0430\u043d\u043d\u044b\u0445 \u0434\u043e\u0441\u0442\u0430\u0442\u043e\u0447\u043d\u043e \u043c\u043d\u043e\u0433\u043e. \u041e\u0431\u0443\u0447\u0438\u043c \u0436\u0435 \u0432\u0441\u0435 \u0441\u043b\u043e\u0438 \u0441\u0435\u0442\u0438!","4fed08a4":"\u0414\u0430\u043b\u0435\u0435 \u0440\u0430\u0431\u043e\u0442\u0430\u0435\u043c \u0441 \u0442\u0435\u0441\u0442\u043e\u0432\u044b\u043c\u0438 \u0434\u0430\u043d\u043d\u044b\u043c\u0438 \u043f\u043e \u0430\u043d\u0430\u043b\u043e\u0433\u0438\u0438 \u0441 [\u043a\u043e\u0434\u043e\u043c \u0432 \u0443\u0440\u043e\u043a\u0435](https:\/\/stepik.org\/lesson\/241449\/step\/1?unit=213793).","d57e7aa4":"\u0421\u043e\u0437\u0434\u0430\u0435\u043c \u043b\u043e\u043a\u0430\u043b\u044c\u043d\u0443\u044e \u043f\u0430\u043f\u043a\u0443 train. \u041f\u0430\u043f\u043a\u0430 val \u0443\u0434\u0430\u043b\u0435\u043d\u0430, \u0442\u0430\u043a \u043a\u0430\u043a \u043e\u043d\u0430 \u043e\u0441\u043e\u0431\u043e \u043d\u0438 \u043d\u0430 \u0447\u0442\u043e \u043d\u0435 \u0432\u043b\u0438\u044f\u0435\u0442. \u0417\u0430\u0442\u043e \u0442\u0435\u043f\u0435\u0440\u044c \u0431\u043e\u043b\u044c\u0448\u0435 \u0434\u0430\u043d\u043d\u044b\u0445 \u0434\u043b\u044f \u043e\u0431\u0443\u0447\u0435\u043d\u0438\u044f."}}