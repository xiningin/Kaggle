{"cell_type":{"c80f8433":"code","6ce1c276":"code","1dc21efc":"code","ceb5bcd3":"code","f8cd8f70":"code","962f1440":"code","a75b5752":"code","cc120d00":"code","60dfea59":"code","d02c0a62":"code","11185660":"code","b65a9248":"code","06fc587f":"code","f52c40ad":"code","3b010d32":"code","44dcabfb":"code","4192966e":"code","d9e23297":"code","fe46e004":"code","ca6bca54":"code","f7e8ddde":"code","1fa7bac6":"code","b8df1933":"markdown","117e8269":"markdown","10f2a2b1":"markdown","b19f0a4c":"markdown","95459924":"markdown","d11c224e":"markdown","fb9bf1cd":"markdown","176e816e":"markdown","8d1be9c4":"markdown","2a00dd4f":"markdown","e3bac113":"markdown","267317fe":"markdown","43605f84":"markdown","52e30624":"markdown","ec10f8db":"markdown","55373389":"markdown","e6d12550":"markdown"},"source":{"c80f8433":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Load test data\ntrain_fp = \"..\/input\/train.csv\"\ntrain_data = pd.read_csv(train_fp)\n\ntest_fp = \"..\/input\/test.csv\"\ntest_data = pd.read_csv(test_fp)\n\ny_full = train_data.SalePrice\nX_full = train_data.drop([\"SalePrice\", \"Id\"],axis=1)\n\n# Seperate categorical and numberical data. Some categorical data uses numbers and isn't identified here e.g MSSubClass,\n# OverallQual, OverallCond etc. These are dealt with on a case by case basis.\nnumerical_cols = [col for col in X_full.columns if X_full.dtypes[col] != 'object']\ntext_cols = [col for col in X_full.columns if X_full.dtypes[col] == 'object']\n\n# As discussed above, MSSubClass is definitely categorical. Quality ratings don't disturb me for the moment as they are\n# quasi-numberical and have clear ranking\/order\nnumerical_cols.remove('MSSubClass')\ntext_cols.insert(0,'MSSubClass')\n\nprint(\"Numerical column list:\\n\", numerical_cols, \"\\n\")\nprint(\"Text column list:\\n\",text_cols, \"\\n\")","6ce1c276":"#Create correlation heatmap of numberical values\ncor_list = numerical_cols.copy()\ncor_list.insert(0, 'SalePrice')\n\nnum_corr = train_data[cor_list].corr(method='spearman')\nfig,ax = plt.subplots(figsize=(12,12))\nsns.heatmap(data=num_corr, square=True)","1dc21efc":"#Sort correlation with sale price in order and display\nnum_corr.sort_values(['SalePrice'], ascending=False, inplace=True)\nprint(num_corr.SalePrice)","ceb5bcd3":"#Display more detailed version looking at top-n variable\nk = 20\ncols = num_corr.nlargest(k,'SalePrice').index\nnum_corr_top = train_data[cols].corr(method='spearman')\nfig,ax = plt.subplots(figsize=(12,12))\nsns.heatmap(data=num_corr_top, square=True, annot=True)\n","f8cd8f70":"#Plot scatter plots of numerical data vs price to look at general trends\nf= pd.melt(train_data,id_vars=['SalePrice'], value_vars=numerical_cols)\ng = sns.FacetGrid(f, col='variable', col_wrap=2, sharey=False, sharex=False, height = 4, aspect = 1.5)\ng = g.map(sns.scatterplot, \"value\", \"SalePrice\")","962f1440":"# Examing a few different suggestions from above\n# Combining basement area and living area for a total area \nfrom scipy.stats import spearmanr\ntotal_area = train_data['GrLivArea']+train_data['TotalBsmtSF']\ntotal_area2 = total_area + train_data['GarageArea']\n\ncorr,p_value = spearmanr(train_data['GrLivArea'],y_full)\nprint('Total Area (GrLivArea) Spearman:',corr)\ncorr,p_value = spearmanr(total_area,y_full)\nprint('Total Area (GrLivArea and TotalBsmtSF) Spearman:',corr)\ncorr,p_value = spearmanr(total_area2,y_full)\nprint('Total Area (GrLivArea, TotalBsmtSF and GarageArea:',corr)\n\nsns.scatterplot(x=total_area2,y=y_full)","a75b5752":"# Given that a house is unlikely to have all porch types, it may just make sense to look at them all together.\nporch_area = train_data[['WoodDeckSF', 'OpenPorchSF', 'EnclosedPorch', '3SsnPorch', 'ScreenPorch']].sum(axis=1)\ncorr,p_value = spearmanr(porch_area,y_full)\nprint('Total Porch Area Spearman:',corr)\n\nsns.scatterplot(x=porch_area,y=y_full)","cc120d00":"total_baths = train_data[['FullBath', 'HalfBath', 'BsmtFullBath', 'BsmtHalfBath']].sum(axis=1)\ncorr,p_value = spearmanr(total_baths,y_full)\nprint('Total Baths Spearman:',corr)\n\nsns.scatterplot(x=total_baths,y=y_full)","60dfea59":"# NEXT: CATEGORICAL DATA\n# Plot scatter plots of numerical data vs price to look at general trends\nf= pd.melt(train_data.fillna('MISSING'),id_vars=['SalePrice'], value_vars=text_cols)\ng = sns.FacetGrid(f, col='variable', col_wrap=2, sharey=False, sharex=False, height = 4, aspect = 1.5)\ng = g.map(sns.boxplot, \"value\", \"SalePrice\")\n\n#for i in range(len(text_cols)):\n #   var = text_cols[i]\n  #  plt.title(var)\n   # #Fills in NA with \"MISSING\" placeholder so they are displayed on the graph\n    #sns.boxplot(x=X_full[var].fillna('MISSING'),y=y_full)\n    #plt.show()","d02c0a62":"# Create a copy of the training data to start processing it.\ntrain_data_copy = train_data.copy()","11185660":"nbrhd_means = train_data_copy['SalePrice'].groupby(train_data_copy['Neighborhood']).mean().sort_values()\nnbrhd_index = nbrhd_means.index.values\n#print(nbrhd_index)\n\nfor i in range(len(nbrhd_index)):\n    train_data_copy = train_data_copy.replace({'Neighborhood':{nbrhd_index[i]:i}})\n\nsns.scatterplot(train_data_copy['Neighborhood'],train_data_copy['SalePrice'])\ncorr,p_value = spearmanr(train_data_copy['Neighborhood'],train_data_copy['SalePrice'])\nprint('Neighborhood Ranked Spearman:',corr)","b65a9248":"style_means = train_data_copy['SalePrice'].groupby(train_data_copy['HouseStyle']).mean().sort_values()\nstyle_index = style_means.index.values\n\nfor i in range(len(style_index)):\n     train_data_copy = train_data_copy.replace({'HouseStyle':{style_index[i]:i}})\n\nsns.scatterplot(train_data_copy['HouseStyle'],train_data_copy['SalePrice'])\ncorr,p_value = spearmanr(train_data_copy['HouseStyle'],y_full)\nprint('House Style Ranked Spearman:',corr)","06fc587f":"# Need to handle filling in missing values before doing this stuff\n# Originally from https:\/\/www.kaggle.com\/juliencs\/a-study-on-regression-applied-to-the-ames-dataset, with some edits\n# Have removed some categories where I don't feel that they necesarrilly correspond to a measure of quality, and in turn can't\n# really be ranked\n\n# For these ranked categories, fill in the missing values first so that rankings are applied to the complete data set. Various\n# assumptions made about what missing values mean\ntrain_data_copy = train_data_copy.fillna({\n    \"Alley\" : \"No\", #NAs defined as not present\n    \"BsmtCond\" : \"No\",\n    \"BsmtExposure\" : \"No\",\n    \"BsmtFinType1\" :\"No\",\n    \"BsmtFinType2\" :\"No\",\n    \"BsmtQual\" : \"No\",\n    \"ExterCond\" :\"TA\", #assume that if no quality measure, quality is average\n    \"ExterQual\" :\"TA\",\n    \"FireplaceQu\" :\"TA\",\n    \"Functional\" : \"Typ\", #assume typical unless dedcutions are warranted\n    \"GarageCond\" : \"No\", #assume no garage present if NA\n    \"GarageQual\" : \"No\",\n    \"GarageFinish\" :\"No\",\n    \"HeatingQC\" : \"TA\", #assume average condition\n    \"KitchenQual\" : \"TA\",\n    \"LotShape\" : \"Reg\", #assume regular\n    \"PavedDrive\" : \"Y\", #dataset suggests the majority of drives are paved\n    \"PoolQC\" : \"No\",\n    \"Street\" : \"Pave\", #assume that most streets are paved (this plays out in the dataset)\n    \"Utilities\" : \"AllPub\", #dataset suggests that majority of properties have all utilities \n    \"CentralAir\" : \"N\",\n    \"Electrical\" : \"SBrkr\",\n    \"LandSlope\" : \"Gtl\"\n})\n\n# Convert text based rankings to numerical ones\ntrain_data_copy = train_data_copy.replace({\n    \"Alley\" : {\"No\": 0, \"Grvl\" : 1, \"Pave\" : 2},\n    \"BsmtCond\" : {\"No\" : 0, \"Po\" : 1, \"Fa\" : 2, \"TA\" : 3, \"Gd\" : 4, \"Ex\" : 5},\n    \"BsmtExposure\" : {\"No\" : 0, \"Mn\" : 1, \"Av\": 2, \"Gd\" : 3},\n    \"BsmtFinType1\" : {\"No\" : 0, \"Unf\" : 1, \"LwQ\": 2, \"Rec\" : 3, \"BLQ\" : 4,\"ALQ\" : 5, \"GLQ\" : 6},\n    \"BsmtFinType2\" : {\"No\" : 0, \"Unf\" : 1, \"LwQ\": 2, \"Rec\" : 3, \"BLQ\" : 4,\"ALQ\" : 5, \"GLQ\" : 6},\n    \"BsmtQual\" : {\"No\" : 0, \"Po\" : 1, \"Fa\" : 2, \"TA\": 3, \"Gd\" : 4, \"Ex\" : 5},\n    \"ExterCond\" : {\"Po\" : 1, \"Fa\" : 2, \"TA\": 3, \"Gd\": 4, \"Ex\" : 5},\n    \"ExterQual\" : {\"Po\" : 1, \"Fa\" : 2, \"TA\": 3, \"Gd\": 4, \"Ex\" : 5},\n    \"FireplaceQu\" : {\"No\" : 0, \"Po\" : 1, \"Fa\" : 2, \"TA\" : 3, \"Gd\" : 4, \"Ex\" : 5},\n    \"Functional\" : {\"Sal\" : 1, \"Sev\" : 2, \"Maj2\" : 3, \"Maj1\" : 4, \"Mod\": 5, \"Min2\" : 6, \"Min1\" : 7, \"Typ\" : 8},\n    \"GarageCond\" : {\"No\" : 0, \"Po\" : 1, \"Fa\" : 2, \"TA\" : 3, \"Gd\" : 4, \"Ex\" : 5},\n    \"GarageQual\" : {\"No\" : 0, \"Po\" : 1, \"Fa\" : 2, \"TA\" : 3, \"Gd\" : 4, \"Ex\" : 5},\n    \"GarageFinish\" : {\"No\" : 0, \"Unf\":1, \"RFn\":2, \"Fin\":3},\n    \"HeatingQC\" : {\"Po\" : 1, \"Fa\" : 2, \"TA\" : 3, \"Gd\" : 4, \"Ex\" : 5},\n    \"KitchenQual\" : {\"Po\" : 1, \"Fa\" : 2, \"TA\" : 3, \"Gd\" : 4, \"Ex\" : 5},\n    \"LotShape\" : {\"IR3\" : 4, \"IR2\" : 3, \"IR1\" : 2, \"Reg\" : 1},\n    \"PavedDrive\" : {\"N\" : 0, \"P\" : 1, \"Y\" : 2},\n    \"PoolQC\" : {\"No\" : 0, \"Fa\" : 1, \"TA\" : 2, \"Gd\" : 3, \"Ex\" : 4},\n    \"Street\" : {\"Grvl\" : 1, \"Pave\" : 2},\n    \"Utilities\" : {\"ELO\" : 1, \"NoSeWa\" : 2, \"NoSewr\" : 3, \"AllPub\" : 4},\n    \"CentralAir\" : {\"N\" : 0, \"Y\":1},\n    \"Electrical\" : {\"Mix\": 1, \"FuseP\":2, \"FuseF\":3, \"FuseA\":4, \"SBrkr\":5},\n    \"LandSlope\" :{\"Sev\":1, \"Mod\":2, \"Gtl\":3}\n})","f52c40ad":"#Check for columns where data is missing after 'valid' NAs are processed above\nmissing = train_data_copy.isnull().sum()\nmissing = missing[missing>0]\nmissing.sort_values(ascending=False, inplace=True)\n\nprint(len(missing),\"columns have missing values:\\n\")\nprint(missing,\"\\n\")","3b010d32":"# Drop non-reqired columns now to avoid generating excessive entries\ndrop_list = ['Id','MSSubClass', \n             'Heating', 'RoofMatl', 'Exterior1st', 'Exterior2nd', 'Condition2', 'MiscFeature',\n             'WoodDeckSF', 'OpenPorchSF', 'EnclosedPorch', '3SsnPorch', 'ScreenPorch'#cheeky hardcoding to remove features that aren't present in final test datset\n             #'Fence', 'GarageYrBlt', #dropping missing columns that definitely won't be used\n             #'YrSold', 'BsmtFinSF2', 'MiscVal', 'LowQualFinSF', 'OverallCond',\n             #'KitchenAbvGr' ,#remove low correlation numerical vars (-ve)\n             #'PoolArea', 'MoSold', 'BsmtUnfSF', 'PoolQC', 'BsmtCond', 'PavedDrive',#remove low correlation numerical vars (+ve)\n             #'ExterCond','Utilities', 'Street', 'BsmtFinType2','Functional', 'Electrical','FireplaceQu',  #remove low correlations cats(+ve)\n             #'LandSlope','Alley',# remove low correlation cat(-ve)\n             #'LotConfig', 'SaleCondition', 'SaleType','RoofStyle', 'Condition1', 'MSZoning',\n             #'LandContour', 'BldgType', 'Foundation', 'GarageType', 'MasVnrType',#remove 1H encoded variables, before they are encoded\n             #'BsmtFinSF1','LotFrontage', '1stFlrSF', '2ndFlrSF', 'GarageCond', 'BedroomAbvGr',\n             #'BsmtQual', 'ExterQual', # remove variable that seem to correlate highly within dataset\n             #\n            ]\n\ntrain_data_final = train_data_copy.drop(drop_list, axis=1)\n\ntrain_data_final['TotalArea'] = train_data_copy[['GrLivArea','TotalBsmtSF', 'GarageArea']].sum(axis=1)\ntrain_data_final['TotalBaths'] = train_data_copy[['FullBath', 'HalfBath', 'BsmtFullBath', 'BsmtHalfBath']].sum(axis=1)\n\ntrain_data_final['OverallQual'] = train_data_final['OverallQual'] ** 2 \ntrain_data_final['Neighborhood'] = train_data_final['Neighborhood'] ** 2 #weighting these more highly as they appear to follow a quadratic relationship\n\n# Drop data that has been used above to calculate total quantities\ntrain_data_final = train_data_final.drop(['GrLivArea','TotalBsmtSF', 'GarageArea',\n                                        'FullBath', 'HalfBath', 'BsmtFullBath', 'BsmtHalfBath'],axis=1)\n\n# One hot encode data\nOH_train_data_final = pd.get_dummies(train_data_final)\n\n# Remove outliers identified earlier\nOH_train_data_final = OH_train_data_final[OH_train_data_final.TotalArea < 8000]","44dcabfb":"#Look at correlations in shortened dataset with modified category values\ncorrelated_complete = OH_train_data_final.corr(method='spearman')\n\ncorrelated_complete.sort_values(['SalePrice'], ascending=False, inplace=True)\nprint(correlated_complete.SalePrice)\n\nk = 50\ncols_complete = correlated_complete.nlargest(k,'SalePrice').index\ncorrelated_complete_top = OH_train_data_final[cols_complete].corr(method='spearman')\n\n#print(correlated_complete_top.SalePrice)\nfig,ax = plt.subplots(figsize=(16,16))\nsns.heatmap(data=correlated_complete_top, square=True, annot=True)","4192966e":"import xgboost as xgb\nfrom xgboost import XGBRegressor\nfrom sklearn.model_selection import train_test_split\nimport math\nimport numpy as np\nfrom sklearn.metrics import mean_absolute_error\n\nX_full = OH_train_data_final.drop(['SalePrice'],axis=1)\ny_full = OH_train_data_final['SalePrice']\n\n# Run basic imputation to fill in missing values\nfrom sklearn.impute import SimpleImputer\nimputer = SimpleImputer()\n\nX_full_imp = pd.DataFrame(imputer.fit_transform(X_full))\nX_full_imp.columns = X_full.columns\n\ndtrain = xgb.DMatrix(X_full_imp, label = np.log(y_full))\nparams = {\"max_depth\":2, \"eta\":0.1} #parameters tuned by trial and error, would like to have a more systematic approach\nmodel = xgb.cv(params, dtrain,  num_boost_round=2000, early_stopping_rounds=50, metrics=['rmse'])\nmodel.loc[50:,[\"test-rmse-mean\", \"train-rmse-mean\"]].plot()\nmodel.tail()","d9e23297":"X_train, X_valid, y_train, y_valid = train_test_split(X_full,y_full, train_size=0.8, test_size = 0.2, random_state=0)\n\nX_train_imp = pd.DataFrame(imputer.fit_transform(X_train))\nX_valid_imp = pd.DataFrame(imputer.transform(X_valid))\nX_train_imp.columns = X_train.columns\nX_valid_imp.columns = X_valid.columns\n\nmy_model = XGBRegressor(max_depth = 2, n_estimators=2000, learning_rate=0.1, random_state=0)\nmy_model.fit(X_train, np.log(y_train),early_stopping_rounds= 50, eval_set=[(X_valid,np.log(y_valid))], verbose=False)\n\npredictions = np.exp(my_model.predict(X_valid))\nprint(\"Mean Absolute Error: \" + str(mean_absolute_error(predictions, y_valid)))","fe46e004":"#Train model on full training dataset\nmy_model_final = XGBRegressor(max_depth = 2, n_estimators = 355, learning_rate = 0.1) #params taken from XGBcv\nmy_model_final.fit(X_full_imp, np.log(y_full))","ca6bca54":"#THIS IS NASTY AND SHOULD VERY MUCH BE CONTAINED IN FUNCTIONS, APOLOGIES\n#Preprocessing of test_data\ntest_data_proc = test_data.copy()\nfor i in range(len(nbrhd_index)):\n    test_data_proc = test_data_proc.replace({'Neighborhood':{nbrhd_index[i]:i}})\n\nfor i in range(len(style_index)-1): #slightly cheeky hard code as last index isn't in test set\n    test_data_proc = test_data_proc.replace({'HouseStyle':{style_index[i]:i}})\n\ntest_data_proc = test_data_proc.fillna({\n    \"Alley\" : \"No\", #NAs defined as not present\n    \"BsmtCond\" : \"No\",\n    \"BsmtExposure\" : \"No\",\n    \"BsmtFinType1\" :\"No\",\n    \"BsmtFinType2\" :\"No\",\n    \"BsmtQual\" : \"No\",\n    \"ExterCond\" :\"TA\", #assume that if no quality measure, quality is average\n    \"ExterQual\" :\"TA\",\n    \"FireplaceQu\" :\"TA\",\n    \"Functional\" : \"Typ\", #assume typical unless dedcutions are warranted\n    \"GarageCond\" : \"No\", #assume no garage present if NA\n    \"GarageQual\" : \"No\",\n    \"GarageFinish\" :\"No\",\n    \"HeatingQC\" : \"TA\", #assume average condition\n    \"KitchenQual\" : \"TA\",\n    \"LotShape\" : \"Reg\", #assume regular\n    \"PavedDrive\" : \"Y\", #dataset suggests the majority of drives are paved\n    \"PoolQC\" : \"No\",\n    \"Street\" : \"Pave\", #assume that most streets are paved (this plays out in the dataset)\n    \"Utilities\" : \"AllPub\", #dataset suggests that majority of properties have all utilities \n    \"CentralAir\" : \"N\",\n    \"Electrical\" : \"SBrkr\",\n    \"LandSlope\" : \"Gtl\"\n})\n\ntest_data_proc = test_data_proc.replace({\n    \"Alley\" : {\"No\": 0, \"Grvl\" : 1, \"Pave\" : 2},\n    \"BsmtCond\" : {\"No\" : 0, \"Po\" : 1, \"Fa\" : 2, \"TA\" : 3, \"Gd\" : 4, \"Ex\" : 5},\n    \"BsmtExposure\" : {\"No\" : 0, \"Mn\" : 1, \"Av\": 2, \"Gd\" : 3},\n    \"BsmtFinType1\" : {\"No\" : 0, \"Unf\" : 1, \"LwQ\": 2, \"Rec\" : 3, \"BLQ\" : 4,\"ALQ\" : 5, \"GLQ\" : 6},\n    \"BsmtFinType2\" : {\"No\" : 0, \"Unf\" : 1, \"LwQ\": 2, \"Rec\" : 3, \"BLQ\" : 4,\"ALQ\" : 5, \"GLQ\" : 6},\n    \"BsmtQual\" : {\"No\" : 0, \"Po\" : 1, \"Fa\" : 2, \"TA\": 3, \"Gd\" : 4, \"Ex\" : 5},\n    \"ExterCond\" : {\"Po\" : 1, \"Fa\" : 2, \"TA\": 3, \"Gd\": 4, \"Ex\" : 5},\n    \"ExterQual\" : {\"Po\" : 1, \"Fa\" : 2, \"TA\": 3, \"Gd\": 4, \"Ex\" : 5},\n    \"FireplaceQu\" : {\"No\" : 0, \"Po\" : 1, \"Fa\" : 2, \"TA\" : 3, \"Gd\" : 4, \"Ex\" : 5},\n    \"Functional\" : {\"Sal\" : 1, \"Sev\" : 2, \"Maj2\" : 3, \"Maj1\" : 4, \"Mod\": 5, \"Min2\" : 6, \"Min1\" : 7, \"Typ\" : 8},\n    \"GarageCond\" : {\"No\" : 0, \"Po\" : 1, \"Fa\" : 2, \"TA\" : 3, \"Gd\" : 4, \"Ex\" : 5},\n    \"GarageQual\" : {\"No\" : 0, \"Po\" : 1, \"Fa\" : 2, \"TA\" : 3, \"Gd\" : 4, \"Ex\" : 5},\n    \"GarageFinish\" : {\"No\" : 0, \"Unf\":1, \"RFn\":2, \"Fin\":3},\n    \"HeatingQC\" : {\"Po\" : 1, \"Fa\" : 2, \"TA\" : 3, \"Gd\" : 4, \"Ex\" : 5},\n    \"KitchenQual\" : {\"Po\" : 1, \"Fa\" : 2, \"TA\" : 3, \"Gd\" : 4, \"Ex\" : 5},\n    \"LotShape\" : {\"IR3\" : 4, \"IR2\" : 3, \"IR1\" : 2, \"Reg\" : 1},\n    \"PavedDrive\" : {\"N\" : 0, \"P\" : 1, \"Y\" : 2},\n    \"PoolQC\" : {\"No\" : 0, \"Fa\" : 1, \"TA\" : 2, \"Gd\" : 3, \"Ex\" : 4},\n    \"Street\" : {\"Grvl\" : 1, \"Pave\" : 2},\n    \"Utilities\" : {\"ELO\" : 1, \"NoSeWa\" : 2, \"NoSewr\" : 3, \"AllPub\" : 4},\n    \"CentralAir\" : {\"N\" : 0, \"Y\":1},\n    \"Electrical\" : {\"Mix\": 1, \"FuseP\":2, \"FuseF\":3, \"FuseA\":4, \"SBrkr\":5},\n    \"LandSlope\" :{\"Sev\":1, \"Mod\":2, \"Gtl\":3}\n})\n\ntest_data_proc = test_data_proc.drop(drop_list, axis=1)\n\ntest_data_proc['TotalArea'] = test_data_proc[['GrLivArea','TotalBsmtSF', 'GarageArea']].sum(axis=1)\ntest_data_proc['TotalBaths'] = test_data_proc[['FullBath', 'HalfBath', 'BsmtFullBath', 'BsmtHalfBath']].sum(axis=1)\n\ntrain_data_final['OverallQual'] = train_data_final['OverallQual'] ** 2 \ntrain_data_final['Neighborhood'] = train_data_final['Neighborhood'] ** 2 \n\ntest_data_proc = test_data_proc.drop(['GrLivArea','TotalBsmtSF', 'GarageArea',\n                                        'FullBath', 'HalfBath', 'BsmtFullBath', 'BsmtHalfBath'],axis=1)\n\ntest_data_proc = pd.get_dummies(test_data_proc)\ntest_data_proc.index = test_data[\"Id\"]","f7e8ddde":"#Impute any missing values\ntest_data_proc_imp = pd.DataFrame(imputer.transform(test_data_proc))\ntest_data_proc_imp.columns = test_data_proc.columns","1fa7bac6":"\ntest_data_proc_imp.index = test_data[\"Id\"]\n\npreds_test = np.exp(my_model_final.predict(test_data_proc_imp))\n\n# Save test predictions to file\noutput = pd.DataFrame({'Id': test_data_proc_imp.index,\n                       'SalePrice': preds_test})\noutput.to_csv('submission.csv', index=False)","b8df1933":"Most of the work here is based on the Kaggle tutorials along with some reading of recommended links. There's still lots of optimisation to be done and it's a little scruffy. In particular I would want a better understanding of how to optimise learning parameters.","117e8269":"Whilst the correlation is improved, it's still not significant and continues to have a distinctive line of 0 values. Apparently not all houses have porches and there appears to be quite a lot of variation in any case.","10f2a2b1":"First let's look at quality evaluations","b19f0a4c":"All neighborhoods are contained in the test dataset, so it might make sense to asign each neighborhood a label which is ranked according to its average price in the set","95459924":"Using the results from a single XGB run seems to work better, though I'm a little unclear why. Have added a log transform of the target data to normalise the skew.","d11c224e":"It makes sense to consider all baths rather than basement and living baths seperately. Improved Spearman Correlation results.","fb9bf1cd":"Currently the tested mae for the cross-val implementation has been wildly different to that calculated in the submission set. Unsure whether this is an issue with my CV code as it didn't seem to occur when optimising which just one train data split as below...","176e816e":"Strongly correlated variable GarageCars-GarageAreas, TotalBsmtSF-1stFlrSF, YearBuilt-GarageYrBlt indicate that these describe similar characteristics? Likely means that some should be dropped in the final model.","8d1be9c4":"Below: had been experimenting with dropping features with low correlations to improve scores. However consistently get best results when using the whole dataset. Here it's not a huge issue as the dataset is relatively small, but would like to look at Feature Engineering and Selection in a lot more detail!","2a00dd4f":"One thing that stands out is that 0-surface area measurements are prevalent across a range of prices. I.e. there will be a range of houses that don't have a basement i.e TotalBsmtSF = 0, but there are a range of prices corresponding to that value. Is it worth converting this into a binary value, i.e. the house has a certain feature or it doesn't?\n\nSeveral of the SF area measure could be combined eliminating zero value i.e. taking a value for the combined surface area of Living, Basement and Garage area could exhibit a more defined trend. However this likely means that data that describes effectively the same thing (1stFlrSF etc.) should be removed.\n\nWith the zero values, could introduce a flag that only considers this data, if, for example a pool, 2nd floor, garage etc is present, so these values are only present then. This would likely require generating a different model for different scenarios. Not sure this is worth doing here, especially considering that this is a relatively small dataset.\n\nCould also look at combining the number of bathrooms.\n* BsmtFullBath, BsmtHalfBath, FullBath, HalfBath \n\nThere are also some variables which exhibit outliers or don't seem to add much value to the data. Candidates for modification:\n* MiscVal to be removed as there is not enough data \n* PoolArea replace with a binary option\n* ScreenPorch, 3SsnPorch, EnclosedPorch, OpenPorchSF, WoodDeckSF could all be combined into one surface area measure\n\nThere are also some outliers that should be examined:\n* LotFrontage, LotArea, GrLivArea\n\nSome data doesn't exhibit any obvious correlations:\n* YrSold, MoSold, \n\nAnd some exhibits some unexpected correlations:\n* Fireplaces, KitchenAbvGr","e3bac113":"In terms of correlations, likely makes sense to use the top results (maybe top-ten) to feed into predictor.\n\nSlightly surprised by a few correlations, particularly KitchenAbvGr and ","267317fe":"Next step is to encode categorical variables. For quality ratings it's probably easiest to just assign labels, for everything else one-hot encoding is probably required. Have considered running an automated labelling system, but training data doesn't contain all the categorical variables that are present in test dataset.","43605f84":"No significant concerns in terms of missing values. The, NAs largely mean a feature isn't present rather than indicating that ","52e30624":"MiscFeature will be removed anyway GarageYrBlt conflicts with YearBuilt so can be removed, GarageType, Fence, MasVnrArea\/Type NAs mean these features aren't present.","ec10f8db":"There are some categorical variables that can be label encoded as there is a clear order\/ranking to the variable. For the rest, one-hot encoding will likely be the best option.\n\nSome comments on relevant variable:\n* MSSubClass - it's unclear how best to encode this as it contains various bits of information. Probably best to remove as the information encoded is also present in other variables (e.g. number of stories, age etc.)\n* LotShape has quite a lot of outliers and doesn't necesarrily follow an expected trend. Might be worth removing\n* Condition1 and Condition2 don't have clear interpretation which is somewhat surprising\n* HouseStyle seems to show quite distinct categories\n* Neighborhood has a pronounced effect on value (unsurprisingly)\n\nCandidates for removal:\n\n","55373389":"Combining surface area improves correlation, which is not that surprising given that a key metric of homes is how many square feet of living space they have.\n\nIf total area will be used, then the components will be removed. Appears to follow a quadratic relationship? There are two outliers which should potentially be removed. ","e6d12550":"Ranking the neighborhoods appears to result in quite a good correlation.\n\nWe can do the same for the house style"}}