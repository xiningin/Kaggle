{"cell_type":{"beb5d326":"code","9086e523":"code","60612baa":"code","c41f8bac":"code","ce4d34e2":"code","87f2b0bd":"code","b850540e":"code","149532a3":"code","02b0d555":"code","2f2ae608":"code","5a2c4cee":"code","48ab0b1b":"code","d79f5d08":"code","5f56a4a9":"code","61806cb6":"code","5dc3c6fd":"code","49aa1f83":"code","0d14fe99":"code","6f2798c3":"code","7b4c6164":"code","a0d79d61":"code","3d82d47c":"code","4a300132":"code","a76df5cd":"code","beec1f9b":"code","d200d6bd":"code","dd2c1680":"code","ed0c1587":"code","0d2337d9":"code","2522b418":"code","0cf730b3":"code","1007637b":"code","73eac3c5":"code","7254e44b":"code","fc15b49c":"code","61677388":"code","860ae03f":"code","84dcbbda":"code","69c625c3":"code","d4454c22":"code","c60cf3ef":"code","7c2f40a6":"code","3fa658a3":"code","645539d0":"code","fa352234":"code","432a546a":"code","e5aa1ef7":"code","d87782fe":"code","06fb900d":"code","814485b7":"code","2edcf67e":"code","35643684":"markdown","a267f3b5":"markdown","a40a564c":"markdown","e1cf313e":"markdown","6b3bd27d":"markdown","fc1e6d5a":"markdown","a7466d02":"markdown","6b4d0698":"markdown","a9faa915":"markdown","58b299cb":"markdown","fc13d5c4":"markdown","2d56b0ca":"markdown","14c3f955":"markdown","21f78e6a":"markdown","ca8f9029":"markdown","e4a8b0c7":"markdown","785d5549":"markdown","aad85890":"markdown","dd2ea1ce":"markdown","78dcc1f2":"markdown","cd332001":"markdown","e319e63c":"markdown","16d4f944":"markdown","30792010":"markdown","f13fa45c":"markdown","616c06e4":"markdown","1d269739":"markdown","f0459b31":"markdown","d7ec27b2":"markdown"},"source":{"beb5d326":"import sys\nimport os\nos.chdir('\/kaggle\/')\n!git clone https:\/\/www.github.com\/matterport\/Mask_RCNN.git Mask_RCNN\nos.chdir('\/kaggle\/Mask_RCNN')\nos.makedirs('logs')","9086e523":"#!git clone https:\/\/github.com\/matterport\/Mask_RCNN.git\n#os.chdir(\"\/kaggle\/working\/Mask_RCNN\")\n#!rm -r samples images README.md assets LICENSE MANIFEST.in\n#!ls\n#shutil.rmtree('\/kaggle\/working\/logs')\nos.walk('\/kaggle\/input')","60612baa":"!pip install -r requirements.txt\n!pip install tensorflow-gpu==1.15\n!pip install keras==2.2.5","c41f8bac":"!conda install -c anaconda --yes cudatoolkit\n!conda install -c anaconda --yes cudnn","ce4d34e2":"ROOT_PATH='\/kaggle\/'\nMASK_PATH='\/kaggle\/Mask_RCNN\/'\nMODEL_PATH='\/kaggle\/Mask_RCNN\/logs\/'\nTRAIN_PATH='\/kaggle\/input\/start-train\/'\nTEST_PATH='\/kaggle\/input\/transfer-learning-test\/'\nVAL_PATH='\/kaggle\/input\/start-val\/'\nJSON_PATH='\/kaggle\/input\/json-label\/'\nTRANSFER_TRAIN_PATH = '\/kaggle\/input\/transfer-train\/'\nTRANSFER_VAL_PATH = '\/kaggle\/input\/transfer-val\/'\nTRANSFER_TEST_PATH = '\/kaggle\/input\/transfer-test\/'","87f2b0bd":"#Einmalig genutzte Befehle\n#os.makedirs('\/kaggle\/working\/mrcnn')\n#os.makedirs('\/kaggle\/working\/Mask_RCNN\/')\n#os.makedirs('\/kaggle\/working\/Mask_RCNN\/logs')\n#os.makedirs('\/kaggle\/input\/transfer_learning_test')\n#os.makedirs('\/kaggle\/input\/transfer_learning_val')\n#os.rmdir('\/kaggle\/working\/Mask_RCNN\/transfer_learning_test')\n\n#1x gebraucht\n#os.makedirs('\/kaggle\/working\/Mask_RCNN\/logs')\n#os.makedirs('\/kaggle\/working\/logs')\n#os.makedirs('\/kaggle\/logs')\n","b850540e":"#for folder in os.listdir('\/'):\n#    print (folder)","149532a3":"import json\nimport gc\nimport datetime\nimport numpy as np\nimport skimage.draw\nfrom imgaug import augmenters as iaa\nimport pandas as pd\nimport matplotlib.image as mpimg\nimport matplotlib.pyplot as plt\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n# Root directory of the project\nROOT_DIR = os.path.abspath(ROOT_PATH)\n\n# Import Mask RCNN\nsys.path.append(ROOT_DIR)  # To find local version of the library\nfrom mrcnn.config import Config\nfrom mrcnn import model as modellib, utils\n\n# Path to trained weights file\nCOCO_WEIGHTS_PATH = os.path.join(ROOT_DIR, \"mask_rcnn.h5\")\n\n# Directory to save logs and model checkpoints, if not provided\n# through the command line argument --logs\nDEFAULT_LOGS_DIR = os.path.join(ROOT_DIR, \"logs\")","02b0d555":"class OCTConfig(Config):\n   \n    NAME = \"OCT\"\n\n    # We use a GPU with 12GB memory, which can fit two images.\n    # Adjust down if you use a smaller GPU.\n    IMAGES_PER_GPU = 2\n\n    # Number of classes (including background)\n    # TODO CHANGE\n    NUM_CLASSES = 1 + 2 + 3 # Background + ObereSchicht + UntereSchicht + druse + hr + pseudo\n\n    # Number of training steps per epoch\n    STEPS_PER_EPOCH = 150\n    #STEPS_PER_EPOCH = 30\n\n    DETECTION_MIN_CONFIDENCE = 0.75\n\n    TRAIN_ROIS_PER_IMAGE = 100\n    \n    BACKBONE = \"resnet50\"\n    \n    MAX_GT_INSTANCES = 100\n    \n    IMAGE_MIN_DIM = 512\n    IMAGE_MAX_DIM = 512","2f2ae608":"class OCTDataset(utils.Dataset):\n\n    def load_oct(self, subset):\n\n        self.add_class(\"oct\", 1, \"RPE\")  # adjusted here\n        self.add_class(\"oct\", 2, \"BM\")  # adjusted here\n        self.add_class(\"oct\", 3, \"druse\")  # adjusted here\n        self.add_class(\"oct\", 4, \"hr\")  # adjusted here\n        self.add_class(\"oct\", 5, \"pseudo\")  # adjusted here\n\n        dataset_dir = \"\"\n        # Train or validation dataset?\n        if subset == \"start_train\":\n            dataset_dir=TRAIN_PATH\n        if subset == \"start_val\":\n            dataset_dir=VAL_PATH\n        if subset == \"transfer_train\":\n            dataset_dir=TRANSFER_TRAIN_PATH\n        if subset == \"transfer_val\":\n            dataset_dir=TRANSFER_VAL_PATH\n        if subset == \"transfer_test\":\n            dataset_dir=TRANSFER_TEST_PATH\n            \n            numberImages = 0\n            for filename in os.listdir('\/kaggle\/input\/transfer-test\/'):\n                image_path = os.path.join(dataset_dir, filename)\n\n                image = skimage.io.imread(image_path)\n                height, width = image.shape[:2]\n                numberImages = numberImages+1\n                class_name_nums = []\n\n                self.add_image(\n                    \"oct\",\n                    image_id=filename,  # use file name as a unique image id\n                    path=image_path,\n                    width=width, height=height,\n                    polygons=[],\n                    class_list=np.array(\n                        class_name_nums))\n                \n            self.dataset_size = numberImages\n        \n        if(subset != \"transfer_test\"):\n            annotations = json.load(open(os.path.join(JSON_PATH, \"via_export_json_\"+subset+\".json\")))\n            annotations = list(annotations.values())  # don't need the dict keys\n            annotations = [a for a in annotations if a['regions']]\n\n            numberImages = 0\n\n            # Add images\n            for a in annotations:\n\n                if type(a['regions']) is dict:\n                    polygons = [r['shape_attributes'] for r in a['regions'].values()]\n                else:\n                    polygons = [r['shape_attributes'] for r in a['regions']]\n\n                class_names_str = [r['region_attributes']['shape'] for r in a['regions']]\n                class_name_nums = []\n\n                for i in class_names_str:\n                    if i == 'RPE':\n                        class_name_nums.append(1)\n                    if i == 'BM':\n                        class_name_nums.append(2)\n                    if i == 'druse':\n                        class_name_nums.append(3)\n                    if i == 'hr':\n                        class_name_nums.append(4)\n                    if i == 'pseudo':\n                        class_name_nums.append(5)\n\n                image_path = os.path.join(dataset_dir, a['filename'])\n\n                image = skimage.io.imread(image_path)\n                height, width = image.shape[:2]\n                numberImages = numberImages+1\n\n                self.add_image(\n                    \"oct\",\n                    image_id=a['filename'],  # use file name as a unique image id\n                    path=image_path,\n                    width=width, height=height,\n                    polygons=polygons,\n                    class_list=np.array(\n                        class_name_nums))  # UNSURE IF  I CAN JUST ADD THIS  HERE. OTHERWISE NEED  TO MODIFY DATASET UTIL\n\n                self.dataset_size = numberImages\n\n    def load_mask(self, image_id):\n        # If not a balloon dataset image, delegate to parent class.\n        image_info = self.image_info[image_id]\n        if image_info[\"source\"] != \"oct\":  # adjusted here\n            return super(self.__class__, self).load_mask(image_id)\n\n        # Convert polygons to a bitmap mask of shape\n        # [height, width, instance_count]\n        info = self.image_info[image_id]\n        mask = np.zeros([info[\"height\"], info[\"width\"], len(info[\"polygons\"])],\n                        dtype=np.uint8)\n        for i, p in enumerate(info[\"polygons\"]):\n            # Get indexes of pixels inside the polygon and set them to 1\n            rr, cc = skimage.draw.polygon(p['all_points_y'], p['all_points_x'])\n            mask[rr, cc, i] = 1\n\n        class_array = info['class_list']\n        return mask.astype(np.bool), class_array\n\n    def image_reference(self, image_id):\n        \"\"\"Return the path of the image.\"\"\"\n        info = self.image_info[image_id]\n        if info[\"source\"] == \"oct\":  # adjusted here\n            return info[\"path\"]\n        else:\n            super(self.__class__, self).image_reference(image_id)","5a2c4cee":"weights = \"coco\"\nlogs= \"logs\"\ncommand = \"train\"\n\n# Configurations\nif command == \"train\":\n    config = OCTConfig()\nconfig.display()\n\n# Create model\nif command == \"train\":\n    model = modellib.MaskRCNN(mode=\"training\", \n                              config=config, \n                              model_dir=logs)\n\nmodel.keras_model.summary()\n\n# Select weights file to load\nif weights.lower() == \"coco\":\n    weights_path = COCO_WEIGHTS_PATH\n    # Download weights file\n    if not os.path.exists(weights_path):\n        utils.download_trained_weights(weights_path)\nelif weights.lower() == \"last\":\n    # Find last trained weights\n    weights_path = model.find_last()\nelse:\n    weights_path = weights\n\n# Load weights\nprint(\"Loading weights \", weights_path)\nif weights.lower() == \"coco\":\n    # Exclude the last layers because they require a matching\n    # number of classes\n    model.load_weights(weights_path, by_name=True, exclude=[\n        \"mrcnn_class_logits\", \"mrcnn_bbox_fc\",\n        \"mrcnn_bbox\", \"mrcnn_mask\"])\nelse:\n    model.load_weights(weights_path, by_name=True)\n","48ab0b1b":"augmentation = iaa.SomeOf((0, 2), [\n        iaa.Flipud(0.5),\n        iaa.Fliplr(0.5),  # horizontal flips\n        iaa.Crop(percent=(0, 0.1)),  # random crops\n\n        # Make some images brighter and some darker.\n        # In 20% of all cases, we sample the multiplier once per channel,\n        # which can end up changing the color of the images.\n        iaa.Multiply((0.8, 1.2), per_channel=0.2),\n\n        # Apply affine transformations to each image.\n        # Scale\/zoom them, translate\/move them, rotate them and shear them.\n        iaa.Affine(\n            scale={\"x\": (0.8, 1.2), \"y\": (0.8, 1.2)},\n            translate_percent={\"x\": (-0.2, 0.2), \"y\": (-0.2, 0.2)},\n            rotate=(-25, 25),\n            shear=(-8, 8)\n        )\n    ], random_order=True)\n\nimg=mpimg.imread(TRAIN_PATH + \"DRUSEN-9689334-1.jpeg\")\nimggrid = augmentation.draw_grid(img, cols=5, rows=2)\nplt.figure(figsize=(30, 12))\n_ = plt.imshow(imggrid.astype(int))","d79f5d08":"# Training dataset.\nprint('preparing training set')\ndataset_train = OCTDataset()\ndataset_train.load_oct(\"start_train\")\ndataset_train.prepare()\n\n# Validation dataset\nprint('preparing val set')\ndataset_val = OCTDataset()\ndataset_val.load_oct(\"start_val\")\ndataset_val.prepare()","5f56a4a9":"# starting_epoch = model.epoch\nepoch = int(dataset_train.dataset_size \/ (config.STEPS_PER_EPOCH * config.BATCH_SIZE) * 1)\n#epoch = 5\n#epochs_warmup = 1 * epoch\nepochs_warmup = 20 #Die anzahl wird beim vortraining genutzt\n#epochs_warmup = 5\nepochs_heads = 7 * epoch  # + starting_epoch\nepochs_stage4 = 7 * epoch  # + starting_epoch\n#epochs_all = 7 * epoch  # + starting_epoch\nepochs_all = 70 #die anzahl wird beim transfer genutzt\n#epochs_all = 10 #die anzahl wird beim transfer genutzt\nepochs_breakOfDawn = 5 * epoch\n\nprint(\"> Training Schedule: \\\n    \\nwarmup: {} epochs \\\n    \\nheads: {} epochs \\\n    \\nstage4+: {} epochs \\\n    \\nall layers: {} epochs \\\n    \\ntill the break of Dawn: {} epochs\".format(\n    epochs_warmup,epochs_heads,epochs_stage4,epochs_all,epochs_breakOfDawn))","61806cb6":"%%time\n# Training - Stage 1\nprint(\"> Training network heads\")\nmodel.train(dataset_train, dataset_val,\n            learning_rate=0.001,\n            epochs= epochs_warmup,\n            layers='all',\n            augmentation=augmentation)\n\nhistory = model.keras_model.history.history","5dc3c6fd":"\n# copy only the last version of .h5 to output\nu_ordner = os.listdir('\/kaggle\/Mask_RCNN\/logs\/')[0]\nprint(u_ordner)\n\nos.chdir(\"\/kaggle\/Mask_RCNN\/logs\/\"+u_ordner+\"\/\")\n#teste welche Datei die neuste ist\nneusterFilename = 'newFilename'\nfileCreateDate = int(0)\nfor file in os.listdir(\"\/kaggle\/Mask_RCNN\/logs\/\"+u_ordner+\"\/\"):\n    if file.endswith(\".h5\"):\n        if(int((os.path.getmtime(file))) > fileCreateDate):\n            fileCreateDate = int(os.path.getmtime(file))\n            neusterFilename = file\n            \n#kopiere Modell in output-Ordner\nkopierZeile = 'cp \/kaggle\/Mask_RCNN\/logs\/'+ u_ordner + '\/' + neusterFilename + ' \/kaggle\/working\/'\nos.system(kopierZeile)\n\nos.chdir('\/kaggle\/Mask_RCNN')","49aa1f83":"# clean vars\n#del dataset_train\n#del dataset_val\ngc.collect()","0d14fe99":"%%time\n# Anpassung der Trainings- und Testdaten (Ordner)!\n# Training dataset.\nprint('preparing training set')\ndataset_train = OCTDataset()\ndataset_train.load_oct(\"transfer_train\")\ndataset_train.prepare()\n\n# Validation dataset\nprint('preparing val set')\ndataset_val = OCTDataset()\ndataset_val.load_oct(\"transfer_val\")\ndataset_val.prepare()","6f2798c3":"augmentation = iaa.SomeOf((0, 2), [\n        iaa.Flipud(0.5),\n        iaa.Fliplr(0.5),  # horizontal flips\n        iaa.Crop(percent=(0, 0.1)),  # random crops\n\n        # Make some images brighter and some darker.\n        # In 20% of all cases, we sample the multiplier once per channel,\n        # which can end up changing the color of the images.\n        iaa.Multiply((0.8, 1.2), per_channel=0.2),\n\n        # Apply affine transformations to each image.\n        # Scale\/zoom them, translate\/move them, rotate them and shear them.\n        iaa.Affine(\n            scale={\"x\": (0.8, 1.2), \"y\": (0.8, 1.2)},\n            translate_percent={\"x\": (-0.2, 0.2), \"y\": (-0.2, 0.2)},\n            rotate=(-25, 25),\n            shear=(-8, 8)\n        )\n    ], random_order=True)\n\nimg=mpimg.imread(\"\/kaggle\/input\/transfer-train\/DRUSEN-95633-1.jpeg\")\nimggrid = augmentation.draw_grid(img, cols=5, rows=2)\nplt.figure(figsize=(30, 12))\n_ = plt.imshow(imggrid.astype(int))\n\ngc.collect()","7b4c6164":"# transfer learning - dursen trainieren\nmodel.train(dataset_train, dataset_val,\n            learning_rate=0.001,\n            epochs= epochs_all,\n            layers='heads',\n            augmentation=augmentation)\n\nhistory = model.keras_model.history.history","a0d79d61":"# copy only the last version of .h5 to output\nu_ordner = os.listdir('\/kaggle\/Mask_RCNN\/logs\/')[0]\nprint(u_ordner)\n\nos.chdir(\"\/kaggle\/Mask_RCNN\/logs\/\"+u_ordner+\"\/\")\n#teste welche Datei die neuste ist\nneusterFilename = 'newFilename'\nfileCreateDate = int(0)\nfor file in os.listdir(\"\/kaggle\/Mask_RCNN\/logs\/\"+u_ordner+\"\/\"):\n    if file.endswith(\".h5\"):\n        if(int((os.path.getmtime(file))) > fileCreateDate):\n            fileCreateDate = int(os.path.getmtime(file))\n            neusterFilename = file\n            \n#kopiere Modell in output-Ordner\nkopierZeile = 'cp \/kaggle\/Mask_RCNN\/logs\/'+ u_ordner + '\/' + neusterFilename + ' \/kaggle\/working\/'\nos.system(kopierZeile)\n\nos.chdir('\/kaggle\/Mask_RCNN')","3d82d47c":"epochs = range(1, len(history['loss'])+1)\npd.DataFrame(history, index=epochs)","4a300132":"plt.figure(figsize=(21,11))\n\nplt.subplot(231)\nplt.plot(epochs, history[\"loss\"], label=\"Train loss\")\nplt.plot(epochs, history[\"val_loss\"], label=\"Valid loss\")\nplt.legend()\nplt.subplot(232)\nplt.plot(epochs, history[\"rpn_class_loss\"], label=\"Train RPN class ce\")\nplt.plot(epochs, history[\"val_rpn_class_loss\"], label=\"Valid RPN class ce\")\nplt.legend()\nplt.subplot(233)\nplt.plot(epochs, history[\"rpn_bbox_loss\"], label=\"Train RPN box loss\")\nplt.plot(epochs, history[\"val_rpn_bbox_loss\"], label=\"Valid RPN box loss\")\nplt.legend()\nplt.subplot(234)\nplt.plot(epochs, history[\"mrcnn_class_loss\"], label=\"Train MRCNN class ce\")\nplt.plot(epochs, history[\"val_mrcnn_class_loss\"], label=\"Valid MRCNN class ce\")\nplt.legend()\nplt.subplot(235)\nplt.plot(epochs, history[\"mrcnn_bbox_loss\"], label=\"Train MRCNN box loss\")\nplt.plot(epochs, history[\"val_mrcnn_bbox_loss\"], label=\"Valid MRCNN box loss\")\nplt.legend()\nplt.subplot(236)\nplt.plot(epochs, history[\"mrcnn_mask_loss\"], label=\"Train Mask loss\")\nplt.plot(epochs, history[\"val_mrcnn_mask_loss\"], label=\"Valid Mask loss\")\nplt.legend()\n\nplt.show()\n\nbest_epoch = np.argmin(history[\"val_loss\"])\nscore = history[\"val_loss\"][best_epoch]\nprint(f'Best Epoch:{best_epoch+1} val_loss:{score}')","a76df5cd":"import os\nimport sys\nimport random\nimport math\nimport re\nimport time\nimport numpy as np\nimport tensorflow as tf\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as patches\nimport matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\nfrom glob import glob\n\n# Import Mask RCNN\nsys.path.append(ROOT_DIR)  # To find local version of the library\nfrom mrcnn import utils\nfrom mrcnn import visualize\nfrom mrcnn.visualize import display_images\nimport mrcnn.model as modellib\nfrom mrcnn.model import log\n\n%matplotlib inline \n\n# Directory to save logs and trained model\nMODEL_DIR = MODEL_PATH","beec1f9b":"config = OCTConfig()\nOCT_DIR = os.path.join(ROOT_DIR, \"\")\nclass InferenceConfig(config.__class__):\n    # Run detection on one image at a time\n    GPU_COUNT = 1\n    IMAGES_PER_GPU = 1\n\nconfig = InferenceConfig()\nconfig.DETECTION_MIN_CONFIDENCE = 0.80\nconfig.display()\n\nDEVICE = \"\/cpu:0\"  # \/cpu:0 or \/gpu:0\nTEST_MODE = \"inference\"\n\ndef get_ax(rows=1, cols=1, size=16):\n    _, ax = plt.subplots(rows, cols, figsize=(size*cols, size*rows))\n    return ax","d200d6bd":"# Load validation dataset\ndataset = OCTDataset()\ndataset.load_oct(\"transfer_val\")\n\n# Must call before using the dataset\ndataset.prepare()\n\nprint(\"Images: {}\\nClasses: {}\".format(len(dataset.image_ids), dataset.class_names))\n\nprint(dataset.image_ids)","dd2c1680":"with tf.device(DEVICE):\n    model = modellib.MaskRCNN(mode=\"inference\", model_dir='\/kaggle\/Mask_RCNN\/logs\/',\n                              config=config)\n\nweights_path = model.find_last()\n\n# Load weights\nprint(\"Loading weights \", weights_path)\nmodel.load_weights(weights_path, by_name=True)","ed0c1587":"#Display results\nnumberPictures=min(len(dataset.image_ids),10)\nax = get_ax(rows=numberPictures,cols=2)\n\nfor i in range(0,numberPictures):\n    image_id = random.choice(dataset.image_ids)\n    image, image_meta, gt_class_id, gt_bbox, gt_mask =\\\n        modellib.load_image_gt(dataset, config, image_id, use_mini_mask=False)\n    info = dataset.image_info[image_id]\n    print(\"image ID: {}.{} ({}) {}\".format(info[\"source\"], info[\"id\"], image_id, \n                                           dataset.image_reference(image_id)))\n    \n    # Run object detection\n    results = model.detect([image], verbose=1)\n\n    r = results[0]\n\n    visualize.display_instances(image, r['rois'], r['masks'], r['class_ids'], \n                                dataset.class_names, r['scores'], ax=ax[i,0],\n                                title=\"Predictions \")\n    \n    image2=mpimg.imread(dataset.image_reference(image_id))\n    ax[i,1].set_title(\"Blank\")\n    ax[i,1].imshow(image2)\n        \n    log(\"gt_class_id\", gt_class_id)\n    log(\"gt_bbox\", gt_bbox)\n    log(\"gt_mask\", gt_mask)","0d2337d9":"# Generate RPN trainig targets\n# target_rpn_match is 1 for positive anchors, -1 for negative anchors\n# and 0 for neutral anchors.\n#target_rpn_match, target_rpn_bbox = modellib.build_rpn_targets(\n#    image.shape, model.anchors, gt_class_id, gt_bbox, model.config)\n#log(\"target_rpn_match\", target_rpn_match)\n#log(\"target_rpn_bbox\", target_rpn_bbox)\n\n#positive_anchor_ix = np.where(target_rpn_match[:] == 1)[0]\n#negative_anchor_ix = np.where(target_rpn_match[:] == -1)[0]\n#neutral_anchor_ix = np.where(target_rpn_match[:] == 0)[0]\n#positive_anchors = model.anchors[positive_anchor_ix]\n#negative_anchors = model.anchors[negative_anchor_ix]\n#neutral_anchors = model.anchors[neutral_anchor_ix]\n#log(\"positive_anchors\", positive_anchors)\n#log(\"negative_anchors\", negative_anchors)\n#log(\"neutral anchors\", neutral_anchors)\n\n# Apply refinement deltas to positive anchors\n#refined_anchors = utils.apply_box_deltas(\n#    positive_anchors,\n#    target_rpn_bbox[:positive_anchors.shape[0]] * model.config.RPN_BBOX_STD_DEV)\n#log(\"refined_anchors\", refined_anchors, )\n\n#visualize.draw_boxes(image, boxes=positive_anchors, refined_boxes=refined_anchors, ax=get_ax())\n","2522b418":"# Run RPN sub-graph\n#pillar = model.keras_model.get_layer(\"ROI\").output  # node to start searching from\n\n# TF 1.4 and 1.9 introduce new versions of NMS. Search for all names to support TF 1.3~1.10\n#nms_node = model.ancestor(pillar, \"ROI\/rpn_non_max_suppression:0\")\n#if nms_node is None:\n#    nms_node = model.ancestor(pillar, \"ROI\/rpn_non_max_suppression\/NonMaxSuppressionV2:0\")\n#if nms_node is None: #TF 1.9-1.10\n#    nms_node = model.ancestor(pillar, \"ROI\/rpn_non_max_suppression\/NonMaxSuppressionV3:0\")\n\n#rpn = model.run_graph([image], [\n#    (\"rpn_class\", model.keras_model.get_layer(\"rpn_class\").output),\n#    (\"pre_nms_anchors\", model.ancestor(pillar, \"ROI\/pre_nms_anchors:0\")),\n#    (\"refined_anchors\", model.ancestor(pillar, \"ROI\/refined_anchors:0\")),\n#    (\"refined_anchors_clipped\", model.ancestor(pillar, \"ROI\/refined_anchors_clipped:0\")),\n#    (\"post_nms_anchor_ix\", nms_node),\n#    (\"proposals\", model.keras_model.get_layer(\"ROI\").output),\n#])\n\n#limit = 100\n#sorted_anchor_ids = np.argsort(rpn['rpn_class'][:,:,1].flatten())[::-1]\n#visualize.draw_boxes(image, boxes=model.anchors[sorted_anchor_ids[:limit]], ax=get_ax())","0cf730b3":"# Show top anchors with refinement. Then with clipping to image boundaries\n#limit = 50\n#ax = get_ax(1, 2)\n#pre_nms_anchors = utils.denorm_boxes(rpn[\"pre_nms_anchors\"][0], image.shape[:2])\n#refined_anchors = utils.denorm_boxes(rpn[\"refined_anchors\"][0], image.shape[:2])\n#refined_anchors_clipped = utils.denorm_boxes(rpn[\"refined_anchors_clipped\"][0], image.shape[:2])\n#visualize.draw_boxes(image, boxes=pre_nms_anchors[:limit],\n#                     refined_boxes=refined_anchors[:limit], ax=ax[0])\n#visualize.draw_boxes(image, refined_boxes=refined_anchors_clipped[:limit], ax=ax[1])","1007637b":"# Show refined anchors after non-max suppression\n#limit = 50\n#ixs = rpn[\"post_nms_anchor_ix\"][:limit]\n#visualize.draw_boxes(image, refined_boxes=refined_anchors_clipped[ixs], ax=get_ax())","73eac3c5":"# Show final proposals\n# These are the same as the previous step (refined anchors \n# after NMS) but with coordinates normalized to [0, 1] range.\n#limit = 50\n# Convert back to image coordinates for display\n#h, w = config.IMAGE_SHAPE[:2]\n#proposals = rpn['proposals'][0, :limit] * np.array([h, w, h, w])\n#visualize.draw_boxes(image, refined_boxes=proposals, ax=get_ax())","7254e44b":"# Get input and output to classifier and mask heads.\n#mrcnn = model.run_graph([image], [\n#    (\"proposals\", model.keras_model.get_layer(\"ROI\").output),\n#    (\"probs\", model.keras_model.get_layer(\"mrcnn_class\").output),\n#    (\"deltas\", model.keras_model.get_layer(\"mrcnn_bbox\").output),\n#    (\"masks\", model.keras_model.get_layer(\"mrcnn_mask\").output),\n#    (\"detections\", model.keras_model.get_layer(\"mrcnn_detection\").output),\n#])\n","fc15b49c":"# Get detection class IDs. Trim zero padding.\n#det_class_ids = mrcnn['detections'][0, :, 4].astype(np.int32)\n#det_count = np.where(det_class_ids == 0)[0][0]\n#det_class_ids = det_class_ids[:det_count]\n#detections = mrcnn['detections'][0, :det_count]\n\n#print(\"{} detections: {}\".format(\n#    det_count, np.array(dataset.class_names)[det_class_ids]))\n\n#captions = [\"{} {:.3f}\".format(dataset.class_names[int(c)], s) if c > 0 else \"\"\n#            for c, s in zip(detections[:, 4], detections[:, 5])]\n#visualize.draw_boxes(\n#    image, \n#    refined_boxes=utils.denorm_boxes(detections[:, :4], image.shape[:2]),\n#    visibilities=[2] * len(detections),\n#    captions=captions, title=\"Detections\",\n#    ax=get_ax())","61677388":"# Get input and output to classifier and mask heads.\n#mrcnn = model.run_graph([image], [\n#    (\"proposals\", model.keras_model.get_layer(\"ROI\").output),\n#    (\"probs\", model.keras_model.get_layer(\"mrcnn_class\").output),\n#    (\"deltas\", model.keras_model.get_layer(\"mrcnn_bbox\").output),\n#    (\"masks\", model.keras_model.get_layer(\"mrcnn_mask\").output),\n#    (\"detections\", model.keras_model.get_layer(\"mrcnn_detection\").output),\n#])\n","860ae03f":"# Get detection class IDs. Trim zero padding.\n#det_class_ids = mrcnn['detections'][0, :, 4].astype(np.int32)\n#det_count = np.where(det_class_ids == 0)[0][0]\n#det_class_ids = det_class_ids[:det_count]\n#detections = mrcnn['detections'][0, :det_count]\n\n#print(\"{} detections: {}\".format(\n#    det_count, np.array(dataset.class_names)[det_class_ids]))\n\n#captions = [\"{} {:.3f}\".format(dataset.class_names[int(c)], s) if c > 0 else \"\"\n#            for c, s in zip(detections[:, 4], detections[:, 5])]\n#visualize.draw_boxes(\n#    image, \n#    refined_boxes=utils.denorm_boxes(detections[:, :4], image.shape[:2]),\n#    visibilities=[2] * len(detections),\n#    captions=captions, title=\"Detections\",\n#    ax=get_ax())\n","84dcbbda":"# Proposals are in normalized coordinates. Scale them\n# to image coordinates.\n#h, w = config.IMAGE_SHAPE[:2]\n#proposals = np.around(mrcnn[\"proposals\"][0] * np.array([h, w, h, w])).astype(np.int32)\n\n# Class ID, score, and mask per proposal\n#roi_class_ids = np.argmax(mrcnn[\"probs\"][0], axis=1)\n#roi_scores = mrcnn[\"probs\"][0, np.arange(roi_class_ids.shape[0]), roi_class_ids]\n#roi_class_names = np.array(dataset.class_names)[roi_class_ids]\n#roi_positive_ixs = np.where(roi_class_ids > 0)[0]\n\n# How many ROIs vs empty rows?\n#print(\"{} Valid proposals out of {}\".format(np.sum(np.any(proposals, axis=1)), proposals.shape[0]))\n#print(\"{} Positive ROIs\".format(len(roi_positive_ixs)))\n\n# Class counts\n#print(list(zip(*np.unique(roi_class_names, return_counts=True))))","69c625c3":"# Display a random sample of proposals.\n# Proposals classified as background are dotted, and\n# the rest show their class and confidence score.\n#limit = 200\n#ixs = np.random.randint(0, proposals.shape[0], limit)\n#captions = [\"{} {:.3f}\".format(dataset.class_names[c], s) if c > 0 else \"\"\n#            for c, s in zip(roi_class_ids[ixs], roi_scores[ixs])]\n#visualize.draw_boxes(image, boxes=proposals[ixs],\n#                     visibilities=np.where(roi_class_ids[ixs] > 0, 2, 1),\n#                     captions=captions, title=\"ROIs Before Refinement\",\n#                     ax=get_ax())","d4454c22":"# Class-specific bounding box shifts.\n#roi_bbox_specific = mrcnn[\"deltas\"][0, np.arange(proposals.shape[0]), roi_class_ids]\n#log(\"roi_bbox_specific\", roi_bbox_specific)\n\n# Apply bounding box transformations\n# Shape: [N, (y1, x1, y2, x2)]\n#refined_proposals = utils.apply_box_deltas(\n#    proposals, roi_bbox_specific * config.BBOX_STD_DEV).astype(np.int32)\n#log(\"refined_proposals\", refined_proposals)\n\n# Show positive proposals\n# ids = np.arange(roi_boxes.shape[0])  # Display all\n#limit = 5\n#ids = np.random.randint(0, len(roi_positive_ixs), limit)  # Display random sample\n#captions = [\"{} {:.3f}\".format(dataset.class_names[c], s) if c > 0 else \"\"\n#            for c, s in zip(roi_class_ids[roi_positive_ixs][ids], roi_scores[roi_positive_ixs][ids])]\n#visualize.draw_boxes(image, boxes=proposals[roi_positive_ixs][ids],\n#                     refined_boxes=refined_proposals[roi_positive_ixs][ids],\n#                     visibilities=np.where(roi_class_ids[roi_positive_ixs][ids] > 0, 1, 0),\n#                     captions=captions, title=\"ROIs After Refinement\",\n#                     ax=get_ax())","c60cf3ef":"#keep = np.where(roi_class_ids > 0)[0]\n#print(\"Keep {} detections:\\n{}\".format(keep.shape[0], keep))\n\n# Remove low confidence detections\n#keep = np.intersect1d(keep, np.where(roi_scores >= config.DETECTION_MIN_CONFIDENCE)[0])\n#print(\"Remove boxes below {} confidence. Keep {}:\\n{}\".format(\n#    config.DETECTION_MIN_CONFIDENCE, keep.shape[0], keep))","7c2f40a6":"# Apply per-class non-max suppression\n#pre_nms_boxes = refined_proposals[keep]\n#pre_nms_scores = roi_scores[keep]\n#pre_nms_class_ids = roi_class_ids[keep]\n\n#nms_keep = []\n#for class_id in np.unique(pre_nms_class_ids):\n    # Pick detections of this class\n#    ixs = np.where(pre_nms_class_ids == class_id)[0]\n    # Apply NMS\n#    class_keep = utils.non_max_suppression(pre_nms_boxes[ixs], \n#                                            pre_nms_scores[ixs],\n#                                            config.DETECTION_NMS_THRESHOLD)\n    # Map indicies\n#    class_keep = keep[ixs[class_keep]]\n#    nms_keep = np.union1d(nms_keep, class_keep)\n#    print(\"{:22}: {} -> {}\".format(dataset.class_names[class_id][:20], \n#                                   keep[ixs], class_keep))\n\n#keep = np.intersect1d(keep, nms_keep).astype(np.int32)\n#print(\"\\nKept after per-class NMS: {}\\n{}\".format(keep.shape[0], keep))","3fa658a3":"# Show final detections\n#ixs = np.arange(len(keep))  # Display all\n# ixs = np.random.randint(0, len(keep), 10)  # Display random sample\n#captions = [\"{} {:.3f}\".format(dataset.class_names[c], s) if c > 0 else \"\"\n#            for c, s in zip(roi_class_ids[keep][ixs], roi_scores[keep][ixs])]\n#visualize.draw_boxes(\n#    image, boxes=proposals[keep][ixs],\n#    refined_boxes=refined_proposals[keep][ixs],\n#    visibilities=np.where(roi_class_ids[keep][ixs] > 0, 1, 0),\n#    captions=captions, title=\"Detections after NMS\",\n#    ax=get_ax())","645539d0":"#display_images(np.transpose(gt_mask, [2, 0, 1]), cmap=\"Blues\")","fa352234":"# Get predictions of mask head\n#mrcnn = model.run_graph([image], [\n#    (\"detections\", model.keras_model.get_layer(\"mrcnn_detection\").output),\n#    (\"masks\", model.keras_model.get_layer(\"mrcnn_mask\").output),\n#])\n\n# Get detection class IDs. Trim zero padding.\n#det_class_ids = mrcnn['detections'][0, :, 4].astype(np.int32)\n#det_count = np.where(det_class_ids == 0)[0][0]\n#det_class_ids = det_class_ids[:det_count]\n\n#print(\"{} detections: {}\".format(\n#    det_count, np.array(dataset.class_names)[det_class_ids]))","432a546a":"# Masks\n#det_boxes = utils.denorm_boxes(mrcnn[\"detections\"][0, :, :4], image.shape[:2])\n#det_mask_specific = np.array([mrcnn[\"masks\"][0, i, :, :, c] \n#                              for i, c in enumerate(det_class_ids)])\n#det_masks = np.array([utils.unmold_mask(m, det_boxes[i], image.shape)\n#                      for i, m in enumerate(det_mask_specific)])\n#log(\"det_mask_specific\", det_mask_specific)\n#log(\"det_masks\", det_masks)","e5aa1ef7":"#display_images(det_mask_specific[:4] * 255, cmap=\"Blues\", interpolation=\"none\")#","d87782fe":"#display_images(det_masks[:4] * 255, cmap=\"Blues\", interpolation=\"none\")","06fb900d":"# Get activations of a few sample layers\n#activations = model.run_graph([image], [\n#    (\"input_image\",        tf.identity(model.keras_model.get_layer(\"input_image\").output)),\n#    (\"res2c_out\",          model.keras_model.get_layer(\"res2c_out\").output),\n#    (\"res3c_out\",          model.keras_model.get_layer(\"res3c_out\").output),\n#    (\"res4w_out\",          model.keras_model.get_layer(\"res4w_out\").output),  # for resnet100\n#    (\"rpn_bbox\",           model.keras_model.get_layer(\"rpn_bbox\").output),\n#    (\"roi\",                model.keras_model.get_layer(\"ROI\").output),\n#])","814485b7":"# Input image (normalized)\n#_ = plt.imshow(modellib.unmold_image(activations[\"input_image\"][0],config))","2edcf67e":"# Backbone feature map\n#display_images(np.transpose(activations[\"res2c_out\"][0,:,:,:4], [2, 0, 1]), cols=4)","35643684":"# Transfer Part","a267f3b5":"# Import","a40a564c":"## Load Model","e1cf313e":"## Filter Low Confidence Detections","6b3bd27d":"## Augmentation","fc1e6d5a":"## Generating Masks","a7466d02":"%%time\n## Training - WarmUp Stage\nprint(\"> Warm Up all layers\")\nmodel.train(dataset_train, dataset_val,\n            learning_rate=config.LEARNING_RATE \/ 10,\n            epochs=epochs_warmup,\n            layers='all',\n            augmentation=augmentation)\n\nhistory = model.keras_model.history.history","6b4d0698":"## Run Detection","a9faa915":"## RPN Targets","58b299cb":"## Apply Bounding Box Refinement","fc13d5c4":"# Dataset","2d56b0ca":"## Pfade","14c3f955":"## Metrics","21f78e6a":"%%time\n# Training - Stage 2\n# Finetune layers  stage 4 and up\nprint(\"> Fine tune {} stage 4 and up\".format(config.BACKBONE))\nmodel.train(dataset_train, dataset_val,\n            learning_rate=config.LEARNING_RATE,\n            epochs=epochs_stage4,\n            layers=\"4+\",\n            augmentation=augmentation)\n\nnew_history = model.keras_model.history.history\nfor k in new_history: history[k] = history[k] + new_history[k]","ca8f9029":"**Helpful-Links:**\n* https:\/\/github.com\/matterport\/Mask_RCNN","e4a8b0c7":"#  Configurations","785d5549":"## RPN Predictions","aad85890":"%%time\n# Training - Stage 3\n# Fine tune all layers\nprint(\"> Fine tune all layers\")\nmodel.train(dataset_train, dataset_val,\n            learning_rate=config.LEARNING_RATE \/ 10,\n            epochs=epochs_all,\n            layers='all',\n            augmentation=augmentation)\n\nnew_history = model.keras_model.history.history\nfor k in new_history: history[k] = history[k] + new_history[k]","dd2ea1ce":"# Result","78dcc1f2":"## Per-Class Non-Max Suppression","cd332001":"#  Training","e319e63c":"## Visualize Activations\n","16d4f944":"# Identifikation von Drusenarten in OCT-Images\n### Unter der Verwendung des matterplot Mask-R-CNN Ansatzes","30792010":"## Load Model","f13fa45c":"## Load Validation Dataset\n","616c06e4":"## Step by Step Detection","1d269739":"## Configuration","f0459b31":"##  Proposal Classification","d7ec27b2":"# Setup Umgebung"}}