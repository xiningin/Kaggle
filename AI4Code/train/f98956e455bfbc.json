{"cell_type":{"857b35de":"code","dbdb2f66":"code","5670594b":"code","645013ab":"code","61d6b311":"code","dbd4bf43":"code","084ff0e3":"code","92f838db":"markdown","bfec8fdb":"markdown","a2acc576":"markdown","3c8b55c0":"markdown","656aedc0":"markdown"},"source":{"857b35de":"from keras.layers import Input, Dense, Dropout, Conv1D, Flatten, MaxPooling1D, Concatenate, BatchNormalization, GlobalAveragePooling1D, LeakyReLU\nfrom keras.models import Model, Sequential\nfrom keras import regularizers\nfrom sklearn.model_selection import KFold \nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.model_selection import StratifiedKFold\nfrom keras.callbacks import Callback\nfrom sklearn.linear_model import LogisticRegression, BayesianRidge\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import classification_report, accuracy_score\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.manifold import TSNE\nimport lightgbm as lgb\nimport math\nfrom sklearn.feature_selection import f_regression\nfrom sklearn.preprocessing import StandardScaler\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import LinearRegression\nfrom scipy import stats\nimport pandas as pd \nimport numpy as np\nfrom sklearn.metrics import roc_curve, auc\nimport seaborn as sns\nsns.set(style=\"whitegrid\")\nimport matplotlib\nfrom sklearn.naive_bayes import GaussianNB\nnp.random.seed(203)\nimport math\nfrom scipy.stats import ks_2samp, normaltest\nfrom sklearn.ensemble import RandomForestRegressor\n\nimport os\nprint(os.listdir(\"..\/input\"))","dbdb2f66":"df_train = pd.read_csv(\"..\/input\/train.csv\")\ndf_test = pd.read_csv(\"..\/input\/test.csv\")","5670594b":"scaler = StandardScaler()\nX = df_train.drop([\"target\", \"ID_code\"], axis=1).values\nX = scaler.fit_transform(X)\nX = X.reshape(200000, 200, 1)\ntest = df_test.drop([\"ID_code\"], axis=1).values\ntest = scaler.transform(test)\ntest = test.reshape(200000, 200, 1)\ny = df_train[\"target\"].values","645013ab":"class roc_callback(Callback):\n    def __init__(self,training_data,validation_data):\n        self.x = training_data[0]\n        self.y = training_data[1]\n        self.x_val = validation_data[0]\n        self.y_val = validation_data[1]\n\n    def on_train_begin(self, logs={}):\n        return\n\n    def on_train_end(self, logs={}):\n        return\n\n    def on_epoch_begin(self, epoch, logs={}):\n        return\n\n    def on_epoch_end(self, epoch, logs={}):\n        y_pred = self.model.predict(self.x)\n        roc = roc_auc_score(self.y, y_pred)\n        y_pred_val = self.model.predict(self.x_val)\n        roc_val = roc_auc_score(self.y_val, y_pred_val)\n        print('\\rroc-auc: %s - roc-auc_val: %s' % (str(round(roc,4)),str(round(roc_val,4))),end=100*' '+'\\n')\n        return\n\n    def on_batch_begin(self, batch, logs={}):\n        return\n\n    def on_batch_end(self, batch, logs={}):\n        return","61d6b311":"def init_model():\n\n    input1 = Input(shape = (200, 1))\n    a = Conv1D(22, 2, activation=\"relu\", kernel_initializer=\"uniform\")(input1)\n    a = BatchNormalization()(a)\n    a = Flatten()(a)\n    a = Dropout(0.6)(a)\n    a = Dense(50, activation = \"relu\", kernel_initializer=\"uniform\")(a)\n    a = Dropout(0.6)(a)\n    output = Dense(1, activation = \"sigmoid\", kernel_initializer=\"uniform\")(a)\n    model = Model(input1, output)\n    \n    return model","dbd4bf43":"folds = StratifiedKFold(n_splits=12, shuffle=True, random_state=4590)\noof = np.zeros(len(df_train))\ntest_predictions = np.zeros(len(df_test))\n\nfor fold_, (trn_idx, val_idx) in enumerate(folds.split(X,y)):\n    X_train, X_test = X[trn_idx], X[val_idx]\n    y_train, y_test = y[trn_idx], y[val_idx]\n    \n    model = init_model()\n    \n    model.compile(loss = \"binary_crossentropy\", optimizer = \"adam\", metrics = [\"accuracy\"])\n    model.fit(X_train, y_train, validation_data = (X_test, y_test), epochs=10, batch_size=512, shuffle=True, callbacks=[roc_callback(training_data=(X_train, y_train),validation_data=(X_test, y_test))])\n    oof[val_idx] = model.predict(X_test).reshape(X_test.shape[0],)\n    test_predictions+= model.predict(test).reshape(test.shape[0],)\/12\nroc_auc_score(y, oof)","084ff0e3":"sub = pd.DataFrame()\nsub[\"ID_code\"] = df_test[\"ID_code\"]\nsub[\"target\"] = test_predictions\nsub.to_csv(\"submission.csv\", index=False)","92f838db":"The following code is necessary for viewing AUCROC during training (Credits to: Tom on https:\/\/stackoverflow.com\/questions\/41032551\/how-to-compute-receiving-operating-characteristic-roc-and-auc-in-keras)","bfec8fdb":"It turns out that model performs a lot better with BatchNormalization and High Dropout. ","a2acc576":"We need to scale and reshape the data fot the Neural Network.","3c8b55c0":"Since there's already so many kernels that try out different parameters in lgbm models, I thougth I'd try out a different type of model and see how high an AUCROC I could get. It turns out (for me atleast) Convolutional 1 Dimensional Networks achieve a higher accuracy than standard Neural Nets. ","656aedc0":"The code can still be optimized by increasing the number of epochs, adding early stopping and saving the models best weights. In this way, the model doesn't end on a bad epoch. \n\nIf you found this code useful, feel free to leave an upvote :)"}}