{"cell_type":{"d768f861":"code","fb8bb833":"code","3864800b":"code","dc7c913c":"code","75b5d0b5":"code","5a36c922":"code","97524973":"code","9d26849c":"code","98c5b0b3":"code","a75d92c4":"code","40f9542e":"code","2e7b4f7d":"markdown","f75a3d86":"markdown","3bfa87c5":"markdown"},"source":{"d768f861":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport os\nimport json\nimport matplotlib\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom tqdm import tqdm_notebook\n%matplotlib inline \n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\n\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","fb8bb833":"import pandas as pd\nsample_submission_df = pd.read_csv(\"..\/input\/data-science-bowl-2019\/sample_submission.csv\")\nspecs_df = pd.read_csv(\"..\/input\/data-science-bowl-2019\/specs.csv\")\ntest_df = pd.read_csv(\"..\/input\/data-science-bowl-2019\/test.csv\")\ntrain_df = pd.read_csv(\"..\/input\/data-science-bowl-2019\/train.csv\")\ntrain_labels_df = pd.read_csv(\"..\/input\/data-science-bowl-2019\/train_labels.csv\")","3864800b":"print(f\"train shape: {train_df.shape}\")\nprint(f\"test shape: {test_df.shape}\")\nprint(f\"train labels shape: {train_labels_df.shape}\")\nprint(f\"specs shape: {specs_df.shape}\")\nprint(f\"sample submission shape: {sample_submission_df.shape}\")","dc7c913c":"train_df.head()","75b5d0b5":"test_df.head()","5a36c922":"train_labels_df.head()","97524973":"pd.set_option('max_colwidth',150)\nspecs_df.head()","9d26849c":"sample_submission_df.head()","98c5b0b3":"print(f\"train installation id: {train_df.installation_id.nunique()}\")\nprint(f\"test installation id: {test_df.installation_id.nunique()}\")\nprint(f\"test & submission installation ids identical: {set(test_df.installation_id.unique()) == set(sample_submission_df.installation_id.unique())}\")","a75d92c4":"def missing_data(data):\n    total = data.isnull().sum()\n    percent = (data.isnull().sum()\/data.isnull().count()*100)\n    tt = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\n    types = []\n    for col in data.columns:\n        dtype = str(data[col].dtype)\n        types.append(dtype)\n    tt['Types'] = types\n    return(np.transpose(tt))","40f9542e":"missing_data(train_df)","2e7b4f7d":"## <a id=\"31\">Missing values<\/a> \n\nWe define a function to calculate the missing values and also show the type of each column.","f75a3d86":"**We have 17K different installation_id in train and 1K in test sets (these are similar with the ones in sample_submission).**","3bfa87c5":"# Data exploration"}}