{"cell_type":{"0f2144e2":"code","51986db9":"code","1d8e161b":"code","fd05e07d":"code","818b4dfd":"code","b1a6824f":"code","b89eb4bb":"code","240f109e":"code","76c79c54":"code","afb3eb11":"code","1e705e83":"code","ecedb6ea":"code","61289502":"code","844b3463":"code","62492f2a":"code","18c492e3":"code","c4180876":"code","07ad6b2b":"code","b141cb62":"code","1a93e9d6":"code","1be55a51":"code","d31c444e":"code","0e54b0bb":"code","023a8e97":"code","d3dee15c":"markdown","ee5dd613":"markdown","ed47bd85":"markdown","961261e6":"markdown","656238f5":"markdown","8d4d4637":"markdown","3934d0a1":"markdown","200f1edc":"markdown","850bcb40":"markdown","007cb33b":"markdown","7f484a4f":"markdown","10e3f202":"markdown","e375b104":"markdown","19122a52":"markdown","627a0479":"markdown","d5374139":"markdown","26ac8e62":"markdown","4b571f05":"markdown","366f6fd0":"markdown","be875b6f":"markdown","742adf71":"markdown"},"source":{"0f2144e2":"import pandas as pd \nimport numpy as np\nimport matplotlib.pyplot as plt","51986db9":"train = pd.read_csv('..\/input\/competitive-data-science-predict-future-sales\/sales_train.csv')\nitems = pd.read_csv('..\/input\/competitive-data-science-predict-future-sales\/items.csv')\ncategories = pd.read_csv('..\/input\/competitive-data-science-predict-future-sales\/item_categories.csv')\nshops = pd.read_csv('..\/input\/competitive-data-science-predict-future-sales\/shops.csv')\ntest = pd.read_csv('..\/input\/competitive-data-science-predict-future-sales\/test.csv')\nsubmission = pd.read_csv('..\/input\/competitive-data-science-predict-future-sales\/sample_submission.csv')","1d8e161b":"# Delete outliers.\n\ntrain = train[train['item_cnt_day'] < 2000]\ntrain = train[train['item_price'] < 300000]\n\n# Delete negative item price values.\n\ntrain = train[train.item_price > 0].reset_index(drop = True)\ntrain.loc[train.item_cnt_day < 1, \"item_cnt_day\"] = 0\n\n# CLEANING SHOPS\n\nshops_train = train['shop_id'].nunique()\nshops_test = test['shop_id'].nunique()\n\n\n\n# Some stores with the same name have different ID. We need to fix this.\ntrain.loc[train['shop_id'] == 0, 'shop_id'] = 57\ntest.loc[test['shop_id'] == 0, 'shop_id'] = 57\n\ntrain.loc[train['shop_id'] == 1, 'shop_id'] = 58\ntest.loc[test['shop_id'] == 1, 'shop_id'] = 58\n\ntrain.loc[train['shop_id'] == 10, 'shop_id'] = 11\ntest.loc[test['shop_id'] == 10, 'shop_id'] = 11\n\n# Add a city and a shop category.\nshops.loc[ shops.shop_name == '\u0421\u0435\u0440\u0433\u0438\u0435\u0432 \u041f\u043e\u0441\u0430\u0434 \u0422\u0426 \"7\u042f\"',\"shop_name\" ] = '\u0421\u0435\u0440\u0433\u0438\u0435\u0432\u041f\u043e\u0441\u0430\u0434 \u0422\u0426 \"7\u042f\"'\nshops[\"city\"] = shops.shop_name.str.split(\" \").map( lambda x: x[0] )\nshops[\"category\"] = shops.shop_name.str.split(\" \").map( lambda x: x[1] )\nshops.loc[shops.city == \"!\u042f\u043a\u0443\u0442\u0441\u043a\", \"city\"] = \"\u042f\u043a\u0443\u0442\u0441\u043a\"\n\n# If there are less than 5 stores in one category, we will make them the category \"other\".\ncategory = []\nfor cat in shops.category.unique():\n    if len(shops[shops.category == cat]) >= 5:\n        category.append(cat)\nshops.category = shops.category.apply( lambda x: x if (x in category) else \"other\" )\n\n\n# Let's transform the category and city of the store into a numeric attribute.\nfrom sklearn import preprocessing\nle = preprocessing.LabelEncoder()\n\nshops[\"shop_category\"] = le.fit_transform(shops.category)\nshops[\"shop_city\"] = le.fit_transform(shops.city)\nshops = shops[[\"shop_id\", \"shop_category\", \"shop_city\"]]\n\n\n# CLEANING CATEGORIES\nitems_train = train['item_id'].nunique()\nitems_test = test['item_id'].nunique()\n\n\n# Select the category and subcategory of the product and convert it to a numeric attribute.\nmain_categories = categories['item_category_name'].str.split('-')\ncategories['main_category_id'] = main_categories.map(lambda row: row[0].strip())\ncategories['main_category_id'] = le.fit_transform(categories['main_category_id'])\n\n# Some items don't have sub-categories. For those, we will use the main category as a sub-category\ncategories['sub_category_id'] = main_categories.map(lambda row: row[1].strip() if len(row) > 1 else row[0].strip())\ncategories['sub_category_id'] = le.fit_transform(categories['sub_category_id'])\n\n\n# CLEANING ITEMS\nimport re\ndef name_correction(x):\n    x = x.lower() # all letters lower case\n    x = x.partition('[')[0] # partition by square brackets\n    x = x.partition('(')[0] # partition by curly brackets\n    x = re.sub('[^A-Za-z0-9\u0410-\u042f\u0430-\u044f]+', ' ', x) # remove special characters\n    x = x.replace('  ', ' ') # replace double spaces with single spaces\n    x = x.strip() # remove leading and trailing white space\n    return x\n\n\n# Clean item names\n# split item names by first bracket\nitems[\"name1\"], items[\"name2\"] = items.item_name.str.split(\"[\", 1).str\nitems[\"name1\"], items[\"name3\"] = items.item_name.str.split(\"(\", 1).str\n\n# replace special characters and turn to lower case\nitems[\"name2\"] = items.name2.str.replace('[^A-Za-z0-9\u0410-\u042f\u0430-\u044f]+', \" \").str.lower()\nitems[\"name3\"] = items.name3.str.replace('[^A-Za-z0-9\u0410-\u042f\u0430-\u044f]+', \" \").str.lower()\n\n# fill nulls with '0'\nitems = items.fillna('0')\n\nitems[\"item_name\"] = items[\"item_name\"].apply(lambda x: name_correction(x))\n\n# return all characters except the last if name 2 is not \"0\" - the closing bracket\nitems.name2 = items.name2.apply( lambda x: x[:-1] if x !=\"0\" else \"0\")\n\n\n\n# Clean item type\nitems[\"type\"] = items.name2.apply(lambda x: x[0:8] if x.split(\" \")[0] == \"xbox\" else x.split(\" \")[0] )\nitems.loc[(items.type == \"x360\") | (items.type == \"xbox360\") | (items.type == \"xbox 360\") ,\"type\"] = \"xbox 360\"\nitems.loc[ items.type == \"\", \"type\"] = \"mac\"\nitems.type = items.type.apply( lambda x: x.replace(\" \", \"\") )\nitems.loc[ (items.type == 'pc' )| (items.type == 'p\u0441') | (items.type == \"pc\"), \"type\" ] = \"pc\"\nitems.loc[ items.type == '\u0440s3' , \"type\"] = \"ps3\"\n\ngroup_sum = items.groupby([\"type\"]).agg({\"item_id\": \"count\"})\ngroup_sum = group_sum.reset_index()\ndrop_cols = []\nfor cat in group_sum.type.unique():\n    if group_sum.loc[(group_sum.type == cat), \"item_id\"].values[0] <40:\n        drop_cols.append(cat)\nitems.name2 = items.name2.apply( lambda x: \"other\" if (x in drop_cols) else x )\nitems = items.drop([\"type\"], axis = 1)\n\nitems.name2 = le.fit_transform(items.name2)\nitems.name3 = le.fit_transform(items.name3)\n\nitems.drop([\"item_name\", \"name1\"],axis = 1, inplace= True)\nitems.head()\n\n\n# Convert the date to \"datetime\" format.\ntrain['date'] =  pd.to_datetime(train['date'], format='%d.%m.%Y')\n\n\n\nfrom itertools import product\nfrom tqdm import tqdm_notebook\n\ndef downcast_dtypes(df):\n    '''\n        Changes column types in the dataframe: \n                \n                `float64` type to `float32`\n                `int64`   type to `int32`\n    '''\n    \n    # Select columns to downcast\n    float_cols = [c for c in df if df[c].dtype == \"float64\"]\n    int_cols =   [c for c in df if df[c].dtype == \"int64\"]\n    \n    # Downcast\n    df[float_cols] = df[float_cols].astype(np.float32)\n    df[int_cols]   = df[int_cols].astype(np.int32)\n    \n    return df\n\n\n\n\n# Generating prodcuct of Shop-Item pairs for each month in the training data\nmonths = train['date_block_num'].unique()\n\ncartesian = []\nfor month in months:\n    shops_in_month = train.loc[train['date_block_num']==month, 'shop_id'].unique()\n    items_in_month = train.loc[train['date_block_num']==month, 'item_id'].unique()\n    cartesian.append(np.array(list(product(*[shops_in_month, items_in_month, [month]])), dtype='int32'))\n\ncartesian_df = pd.DataFrame(np.vstack(cartesian), columns = ['shop_id', 'item_id', 'date_block_num'], dtype=np.int32)\n\n\n# Add revenue to the train dataset.\ntrain[\"revenue\"] = train[\"item_cnt_day\"] * train[\"item_price\"]\n\n\n# Aggregating sales to a monthly level and clipping target variable\nx = train.groupby(['shop_id', 'item_id', 'date_block_num'])['item_cnt_day'].sum().rename('item_cnt_month').reset_index()\nx.head()\n\n\n# Now we need to merge our two dataframes.\nnew_train = pd.merge(cartesian_df, x, on=['shop_id', 'item_id', 'date_block_num'], how='left').fillna(0)\n\nnew_train.head()\n\n\n# Now we need to merge our two dataframes. For the intersecting, we will simply put the values that exist in the dataframe x. \n# For the remaining rows, we will sub in zero. Remember, the columns you want to merge on are the intersection of shop_id, item_id, and date_block_num\nnew_train['item_cnt_month'] = np.clip(new_train['item_cnt_month'], 0, 20)\n\n\ndel x\ndel cartesian_df\ndel cartesian\n\n\nnew_train.sort_values(['date_block_num','shop_id','item_id'], inplace = True)\n\n\n# APPENDING TEST SET TO TRAINING SET\n\n# First, let's insert the date_block_num feature for the test set! Using insert method of pandas to place this new column at a specific index. \n# This will allow us to concatenate the test set easily to the training set before we generate mean encodings and lag features.\n\ntest.insert(loc=3, column='date_block_num', value=34)\n\ntest['item_cnt_month'] = 0\n\n\nnew_train = new_train.append(test.drop('ID', axis = 1))\n\n# Merge shops, items and categories dataframes with new_train\n\nnew_train = pd.merge(new_train, shops, on=['shop_id'], how='left')\n\nnew_train = pd.merge(new_train, items, on=['item_id'], how='left')\n\nnew_train = pd.merge(new_train, categories.drop('item_category_name', axis = 1), on=['item_category_id'], how='left')\n\n\n# Generating Lag Features and Mean-Encodings\ndef lag_feature( df,lags, cols ):\n    for col in cols:\n        tmp = df[[\"date_block_num\", \"shop_id\",\"item_id\",col ]]\n        for i in lags:\n            shifted = tmp.copy()\n            shifted.columns = [\"date_block_num\", \"shop_id\", \"item_id\", col + \"_lag_\"+str(i)]\n            shifted.date_block_num = shifted.date_block_num + i\n            df = pd.merge(df, shifted, on=['date_block_num','shop_id','item_id'], how='left')\n    return df\n\n\ndel items\ndel categories\ndel shops\n\n\nnew_train = downcast_dtypes(new_train)\n\nimport gc\ngc.collect()\n\n\n# Add item_cnt_month lag features.\nnew_train = lag_feature( new_train, [1,2,3], [\"item_cnt_month\"] )\n\n\n# Add the previous month's average item_cnt.\ngroup = new_train.groupby( [\"date_block_num\"] ).agg({\"item_cnt_month\" : [\"mean\"]})\ngroup.columns = [\"date_avg_item_cnt\"]\ngroup.reset_index(inplace = True)\n\nnew_train = pd.merge(new_train, group, on = [\"date_block_num\"], how = \"left\")\nnew_train.date_avg_item_cnt = new_train[\"date_avg_item_cnt\"].astype(np.float16)\nnew_train = lag_feature( new_train, [1], [\"date_avg_item_cnt\"] )\nnew_train.drop( [\"date_avg_item_cnt\"], axis = 1, inplace = True )\n\n\n# Add lag values of item_cnt_month for month \/ item_id.\ngroup = new_train.groupby(['date_block_num', 'item_id']).agg({'item_cnt_month': ['mean']})\ngroup.columns = [ 'date_item_avg_item_cnt' ]\ngroup.reset_index(inplace=True)\n\nnew_train = pd.merge(new_train, group, on=['date_block_num','item_id'], how='left')\nnew_train.date_item_avg_item_cnt = new_train['date_item_avg_item_cnt'].astype(np.float16)\nnew_train = lag_feature(new_train, [1,2,3], ['date_item_avg_item_cnt'])\nnew_train.drop(['date_item_avg_item_cnt'], axis=1, inplace=True)\n\n\n# Add lag values for item_cnt_month for every month \/ shop combination.\ngroup = new_train.groupby( [\"date_block_num\",\"shop_id\"] ).agg({\"item_cnt_month\" : [\"mean\"]})\ngroup.columns = [\"date_shop_avg_item_cnt\"]\ngroup.reset_index(inplace = True)\n\nnew_train = pd.merge(new_train, group, on = [\"date_block_num\",\"shop_id\"], how = \"left\")\nnew_train.date_avg_item_cnt = new_train[\"date_shop_avg_item_cnt\"].astype(np.float16)\nnew_train = lag_feature( new_train, [1,2,3], [\"date_shop_avg_item_cnt\"] )\nnew_train.drop( [\"date_shop_avg_item_cnt\"], axis = 1, inplace = True )\n\n\n# Add lag values for item_cnt_month for month\/shop\/item.\ngroup = new_train.groupby( [\"date_block_num\",\"shop_id\",\"item_id\"] ).agg({\"item_cnt_month\" : [\"mean\"]})\ngroup.columns = [\"date_shop_item_avg_item_cnt\"]\ngroup.reset_index(inplace = True)\n\nnew_train = pd.merge(new_train, group, on = [\"date_block_num\",\"shop_id\",\"item_id\"], how = \"left\")\nnew_train.date_avg_item_cnt = new_train[\"date_shop_item_avg_item_cnt\"].astype(np.float16)\nnew_train = lag_feature( new_train, [1,2,3], [\"date_shop_item_avg_item_cnt\"] )\nnew_train.drop( [\"date_shop_item_avg_item_cnt\"], axis = 1, inplace = True )\n\n\n# Add lag values for item_cnt_month for month\/shop\/item subtype.\ngroup = new_train.groupby(['date_block_num', 'shop_id', 'sub_category_id']).agg({'item_cnt_month': ['mean']})\ngroup.columns = ['date_shop_subtype_avg_item_cnt']\ngroup.reset_index(inplace=True)\n\nnew_train = pd.merge(new_train, group, on=['date_block_num', 'shop_id', 'sub_category_id'], how='left')\nnew_train.date_shop_subtype_avg_item_cnt = new_train['date_shop_subtype_avg_item_cnt'].astype(np.float16)\nnew_train = lag_feature(new_train, [1], ['date_shop_subtype_avg_item_cnt'])\nnew_train.drop(['date_shop_subtype_avg_item_cnt'], axis=1, inplace=True)\n\n\n# Add lag values for item_cnt_month for month\/city\ngroup = new_train.groupby(['date_block_num', 'shop_city']).agg({'item_cnt_month': ['mean']})\ngroup.columns = ['date_city_avg_item_cnt']\ngroup.reset_index(inplace=True)\n\nnew_train = pd.merge(new_train, group, on=['date_block_num', \"shop_city\"], how='left')\nnew_train.date_city_avg_item_cnt = new_train['date_city_avg_item_cnt'].astype(np.float16)\nnew_train = lag_feature(new_train, [1], ['date_city_avg_item_cnt'])\nnew_train.drop(['date_city_avg_item_cnt'], axis=1, inplace=True)\n\n# Add lag values for item_cnt_month for month\/city\/item.\ngroup = new_train.groupby(['date_block_num', 'item_id', 'shop_city']).agg({'item_cnt_month': ['mean']})\ngroup.columns = [ 'date_item_city_avg_item_cnt' ]\ngroup.reset_index(inplace=True)\n\nnew_train = pd.merge(new_train, group, on=['date_block_num', 'item_id', 'shop_city'], how='left')\nnew_train.date_item_city_avg_item_cnt = new_train['date_item_city_avg_item_cnt'].astype(np.float16)\nnew_train = lag_feature(new_train, [1], ['date_item_city_avg_item_cnt'])\nnew_train.drop(['date_item_city_avg_item_cnt'], axis=1, inplace=True)\n\n# Add average item price on to matix df.\n# Add lag values of item price per month.\n# Add delta price values - how current month average pirce relates to global average.\n\ngroup = train.groupby( [\"item_id\"] ).agg({\"item_price\": [\"mean\"]})\ngroup.columns = [\"item_avg_item_price\"]\ngroup.reset_index(inplace = True)\n\nnew_train = new_train.merge( group, on = [\"item_id\"], how = \"left\" )\nnew_train[\"item_avg_item_price\"] = new_train.item_avg_item_price.astype(np.float16)\n\n\ngroup = train.groupby( [\"date_block_num\",\"item_id\"] ).agg( {\"item_price\": [\"mean\"]} )\ngroup.columns = [\"date_item_avg_item_price\"]\ngroup.reset_index(inplace = True)\n\nnew_train = new_train.merge(group, on = [\"date_block_num\",\"item_id\"], how = \"left\")\nnew_train[\"date_item_avg_item_price\"] = new_train.date_item_avg_item_price.astype(np.float16)\nlags = [1, 2, 3]\nnew_train = lag_feature( new_train, lags, [\"date_item_avg_item_price\"] )\nfor i in lags:\n    new_train[\"delta_price_lag_\" + str(i) ] = (new_train[\"date_item_avg_item_price_lag_\" + str(i)]- new_train[\"item_avg_item_price\"] )\/ new_train[\"item_avg_item_price\"]\n\ndef select_trends(row) :\n    for i in lags:\n        if row[\"delta_price_lag_\" + str(i)]:\n            return row[\"delta_price_lag_\" + str(i)]\n    return 0\n\nnew_train[\"delta_price_lag\"] = new_train.apply(select_trends, axis = 1)\nnew_train[\"delta_price_lag\"] = new_train.delta_price_lag.astype( np.float16 )\nnew_train[\"delta_price_lag\"].fillna( 0 ,inplace = True)\n\nfeatures_to_drop = [\"item_avg_item_price\", \"date_item_avg_item_price\"]\nfor i in lags:\n    features_to_drop.append(\"date_item_avg_item_price_lag_\" + str(i) )\n    features_to_drop.append(\"delta_price_lag_\" + str(i) )\nnew_train.drop(features_to_drop, axis = 1, inplace = True)\n\n# Add total shop revenue per month to matix df.\n# Add lag values of revenue per month.\n# Add delta revenue values - how current month revenue relates to global average.\n\ngroup = train.groupby( [\"date_block_num\",\"shop_id\"] ).agg({\"revenue\": [\"sum\"] })\ngroup.columns = [\"date_shop_revenue\"]\ngroup.reset_index(inplace = True)\n\nnew_train = new_train.merge( group , on = [\"date_block_num\", \"shop_id\"], how = \"left\" )\nnew_train['date_shop_revenue'] = new_train['date_shop_revenue'].astype(np.float32)\n\ngroup = group.groupby([\"shop_id\"]).agg({ \"date_block_num\":[\"mean\"] })\ngroup.columns = [\"shop_avg_revenue\"]\ngroup.reset_index(inplace = True )\n\nnew_train = new_train.merge( group, on = [\"shop_id\"], how = \"left\" )\nnew_train[\"shop_avg_revenue\"] = new_train.shop_avg_revenue.astype(np.float32)\nnew_train[\"delta_revenue\"] = (new_train['date_shop_revenue'] - new_train['shop_avg_revenue']) \/ new_train['shop_avg_revenue']\nnew_train[\"delta_revenue\"] = new_train[\"delta_revenue\"]. astype(np.float32)\n\nnew_train = lag_feature(new_train, [1], [\"delta_revenue\"])\nnew_train[\"delta_revenue_lag_1\"] = new_train[\"delta_revenue_lag_1\"].astype(np.float32)\nnew_train.drop( [\"date_shop_revenue\", \"shop_avg_revenue\", \"delta_revenue\"] ,axis = 1, inplace = True)\n\n# Add month and number of days in each month to matrix df.\n\nnew_train[\"month\"] = new_train[\"date_block_num\"] % 12\ndays = pd.Series([31,28,31,30,31,30,31,31,30,31,30,31])\nnew_train[\"days\"] = new_train[\"month\"].map(days).astype(np.int8)\n\n# Add holidays in dataset.\nholiday_dict = {\n    0: 6,\n    1: 3,\n    2: 2,\n    3: 8,\n    4: 3,\n    5: 3,\n    6: 2,\n    7: 8,\n    8: 4,\n    9: 8,\n    10: 5,\n    11: 4,\n}\n\nnew_train['holidays_in_month'] = new_train['month'].map(holiday_dict)\n\n\n# Add the month of each shop and item first sale.\nnew_train[\"item_shop_first_sale\"] = new_train[\"date_block_num\"] - new_train.groupby([\"item_id\",\"shop_id\"])[\"date_block_num\"].transform('min')\nnew_train[\"item_first_sale\"] = new_train[\"date_block_num\"] - new_train.groupby([\"item_id\"])[\"date_block_num\"].transform('min')\n\n# Delete first three months from matrix. They don't have lag values.\n\nnew_train = new_train[new_train[\"date_block_num\"] > 3]\n\nnew_train.head()\n\ndef fill_na(df):\n    for col in df.columns:\n        if ('_lag_' in col) & (df[col].isnull().any()):\n            df[col].fillna(0, inplace=True)         \n    return df\n\nnew_train = downcast_dtypes(new_train)","fd05e07d":"new_train = pd.read_csv('..\/input\/new-train\/new_train.csv', index_col='Unnamed: 0')","818b4dfd":"x_train = new_train[new_train.date_block_num < 34].drop(['item_cnt_month'], axis=1)\ny_train = new_train[new_train.date_block_num < 34]['item_cnt_month']\n\nx_val = new_train[new_train.date_block_num == 33].drop(['item_cnt_month'], axis=1)\ny_val = new_train[new_train.date_block_num == 33]['item_cnt_month']\n\nx_test = new_train[new_train.date_block_num == 34].drop(['item_cnt_month'], axis=1)","b1a6824f":"from sklearn.preprocessing import StandardScaler, MinMaxScaler\nscaler_x = MinMaxScaler()\nscaler_y = StandardScaler()","b89eb4bb":"scaler_x.fit(x_train)\nx_train = scaler_x.transform(x_train)\nx_val = scaler_x.transform(x_val)\nx_test = scaler_x.transform(x_test)","240f109e":"scaler_y.fit(y_train.to_numpy().reshape(-1, 1))\ny_train = scaler_y.transform(y_train.to_numpy().reshape(-1, 1)).ravel()\ny_val = scaler_y.transform(y_val.to_numpy().reshape(-1, 1)).ravel()","76c79c54":"!pip install scikit-learn-intelex -q --progress-bar off","afb3eb11":"from sklearnex import patch_sklearn\npatch_sklearn()","1e705e83":"from sklearn.linear_model import Lasso, ElasticNet, Ridge\nfrom sklearn.ensemble import StackingRegressor\nfrom sklearn.metrics import mean_squared_error\nimport optuna","ecedb6ea":"def get_stacking_regressor( alpha4=None,\n                            alpha1=None, alpha2=None, alpha3=None,\n                            l1_ratio=None, l1_ratio2=None\n                            ):\n    elastic = ElasticNet(alpha=alpha1, l1_ratio=l1_ratio, random_state=0)\n    lasso2 = Lasso(alpha=alpha2, random_state=0)\n    ridge = Ridge(alpha=alpha3, random_state=0)\n\n    \n    elastic_f = ElasticNet(alpha=alpha4, l1_ratio=l1_ratio2, random_state=0)\n    stacking_estimators = [\n        ('elastic', elastic),\n        ('lasso2', lasso2),\n        ('ridge', ridge),\n    ]\n    \n    return StackingRegressor(estimators=stacking_estimators, final_estimator=elastic_f)","61289502":"def objective_stack(trial):\n    params ={\n        'alpha4': trial.suggest_float('alpha4', 0.0, 0.02969371481087929),\n        'alpha1': trial.suggest_float('alpha1', 0.0, 0.027694846519887552),\n        'alpha2': trial.suggest_float('alpha2', 0.0, 0.31557621736570013),\n        'alpha3': trial.suggest_float('alpha3', 0.0,  0.029221357138328012),\n        'l1_ratio': trial.suggest_float('l1_ratio', 0.0, 0.31140039770607025),\n        'l1_ratio2': trial.suggest_float('l1_ratio2', 0.0, 0.09864359696600125),\n\n    }\n    model = get_stacking_regressor(**params).fit(x_train, y_train)\n    y_pred = model.predict(x_val)\n    loss = np.sqrt(mean_squared_error(y_val, y_pred))\n    return loss","844b3463":"study = optuna.create_study(sampler=optuna.samplers.TPESampler(seed=123),\n                            direction=\"minimize\",\n                            pruner=optuna.pruners.HyperbandPruner())","62492f2a":"%%time\nstudy.optimize(objective_stack, n_trials=10)","18c492e3":"x_train_full = np.concatenate((x_train, x_val), axis=0)\ny_train_full = np.concatenate((y_train, y_val), axis=0)","c4180876":"%%time\nfinal_model = get_stacking_regressor(**study.best_params).fit(x_train_full, y_train_full)","07ad6b2b":"%%time\ny_pred = final_model.predict(x_test)\ny_pred = scaler_y.inverse_transform(y_pred)","b141cb62":"submission['item_cnt_month'] = y_pred\nsubmission.to_csv('submission.csv', index=False)\nsubmission.head(10)","1a93e9d6":"from sklearnex import unpatch_sklearn\nunpatch_sklearn()","1be55a51":"from sklearn.linear_model import Lasso, ElasticNet, Ridge","d31c444e":"study = optuna.create_study(sampler=optuna.samplers.TPESampler(seed=123),\n                            direction=\"minimize\",\n                            pruner=optuna.pruners.HyperbandPruner())","0e54b0bb":"%%time\nstudy.optimize(objective_stack, n_trials=10)","023a8e97":"%%time\nfinal_model = get_stacking_regressor(**study.best_params).fit(x_train_full, y_train_full)","d3dee15c":"<big><strong>Select parameters<\/strong><\/big>","ee5dd613":"<big>Let's see the execution time with Intel(R) Extension for Scikit-learn.<\/big>","ed47bd85":"<big><strong>Select parameters<\/strong><\/big>","961261e6":"<big>Save the results in 'submission.csv'.<\/big>","656238f5":"<big><strong>Prediction.<\/strong><\/big>","8d4d4637":"<big>Normalize data.<\/big>","3934d0a1":"# Split data to a train and test sets","200f1edc":"<big>Let's see the execution time without patch.<\/big>","850bcb40":"<big><strong>Training the model with the selected parameters.<\/strong><\/big>","007cb33b":"# Now we use the same algorithms with original scikit-learn","7f484a4f":"<h2>Conclusions<\/h2>\n<big>We can see that using only one classical machine learning algorithm may give you a pretty hight accuracy score. We also use well-known libraries Scikit-learn and Optuna, as well as the increasingly popular library Intel\u00ae Extension for Scikit-learn. Noted that Intel\u00ae Extension for Scikit-learn gives you opportunities to:<\/big>\n\n* <big>Use your Scikit-learn code for training and inference without modification.<\/big>\n* <big>Speed up selection of parameters <strong>from 27 minutes to 12 minutes.<\/strong><\/big>\n* <big>Get predictions of the similar quality.<\/big>","10e3f202":"<big>I took preprocessing from  <a href='https:\/\/www.kaggle.com\/gordotron85\/future-sales-xgboost-top-3'>here<\/a> and <a href='https:\/\/www.kaggle.com\/sarthakbatra\/predicting-sales-tutorial'>here<\/a> <\/big><br><br> \n<big>The main steps:<\/big><br><br>\n<ol>\n<li><big>Cleaning \"shops\": fix store identifiers, adding categories and cities and convert it to a numeric attribute.<\/big><\/li><br>\n<li><big>Cleaning \"categories\": select the category and subcategory of the product and convert it to a numeric attribute.<\/big><\/li><br>\n<li><big>Cleaning \"items\": clean item names and types.<\/big><\/li><br>\n<li><big>Generating prodcuct of Shop-Item pairs for each month in the training data.<\/big><\/li><br>\n<li><big>Merge shops, items and categories dataframes with new_train set.<\/big><\/li><br>\n<li><big>Generating Lag Features and Mean-Encodings.<\/big><\/li><br>\n<\/ol>","e375b104":"<big>The process of selecting the parameters is too long and computationally intensive, so I selected the parameters in advance.<\/big>","19122a52":"<big>Let\u2019s run the same code with original scikit-learn and compare its execution time with the execution time of the patched by Intel(R) Extension for Scikit-learn.<\/big>","627a0479":"# Installing Intel(R) Extension for Scikit-learn\n\n<big>Use Intel\u00ae Extension for Scikit-learn* for fast compute Scikit-learn estimators.<\/big>","d5374139":"<big>Patch original scikit-learn.<\/big>","26ac8e62":"<big><strong>Training the model with the selected parameters.<\/strong><\/big>","4b571f05":"<big>For classical machine learning algorithms, we often use the most popular Python library, Scikit-learn. With Scikit-learn you can fit models and search for optimal parameters, but\u202fit\u202fsometimes works for hours.<\/big><br><br>\n\n<big>I want to show you how to use Scikit-learn library and get the results faster without changing the code. To do this, we will make use of another Python library, <strong>\u202f<a href='https:\/\/github.com\/intel\/scikit-learn-intelex'>Intel\u00ae Extension for Scikit-learn*<\/a><\/strong>.<\/big><br><br>\n\n<big>I will show you how to <strong>speed up your kernel more than 2 times<\/strong> without changing your code!<\/big><big>","366f6fd0":"# Using optuna to select parameters for Stacking algorithm\n<big>Stacking or generalization is an ensemble of machine learning algorithms.\n\nThis generalization consists of output combination of individual estimators and the final prediction based on it. Stacking allows to use the strength of each individual estimator by using their output as input of a final estimator.<\/big><br><br>\n<big>We adjust hyperparameters for the best result.<\/big><br><br>\n\n<big>Parameters that we select:<\/big><br>\n<big>* <code>alpha<\/code> - Regularization parameter. Regularization improves the solution and reduces the variance of estimates.<br> <\/big>\n<big>* <code>l1_ratio<\/code> - Regularization parameter. For the penalty is a combination of L1 and L2 regularization.<br> <\/big>","be875b6f":"# Preprocessing","742adf71":"# Importing data"}}