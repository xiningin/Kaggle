{"cell_type":{"8b3f503b":"code","65c8ebef":"code","65a56a5d":"code","1b36465e":"code","9d722f07":"code","df75a7e0":"code","34980cdb":"code","d25e08a2":"code","b11706b5":"code","26d70ca2":"code","a13dd175":"code","25b06b22":"code","1686c93a":"code","a1fa14bb":"code","019e918c":"code","2ee05f0b":"code","bc5ae4fa":"code","8e1a0e6b":"code","2f79843e":"code","95ede50e":"code","7c82f090":"code","68647e82":"code","2ac830ee":"code","1aac2797":"code","eaa324ed":"code","a61ee6fe":"code","1c41ddf8":"code","0ff3cc13":"code","e8e07710":"code","468e3a13":"code","7d1a5e24":"code","58df163b":"code","543c24b5":"code","3a3b0d81":"code","55791b05":"code","99db8054":"code","460d4a52":"code","1e04b641":"code","0f826f30":"code","f00c06e2":"code","02206821":"code","18a11284":"code","95b99565":"code","a83fca43":"code","5d5fec55":"code","69e0ccee":"code","5b3a6a4c":"code","c9f4cf1f":"code","44efec24":"code","de510b1d":"code","3e6ceea6":"code","af94f160":"code","693064bb":"code","46591e6b":"code","4ec4297d":"code","0ed8a983":"code","9002561c":"code","79d5670e":"code","762a57a0":"code","1b37c0c6":"code","406d2392":"code","7a953f6f":"code","2e085529":"code","2c5a34e7":"code","fd2736e0":"code","e0e09360":"code","c47e6200":"code","f208826f":"code","69da4d28":"code","6228c1b0":"code","7640bdae":"code","2be5e62e":"code","799e0f85":"code","32cb14f8":"code","da899748":"code","1904a111":"code","1a323c1b":"code","9f56ca6f":"code","1d918906":"code","3d99bd15":"code","8abc330d":"code","8689b920":"code","34e92c7c":"code","f6224ea7":"code","13aa89a3":"code","f8256bc5":"code","988ac457":"code","746808aa":"code","c5c499a9":"code","3097a82a":"code","2c79a0de":"code","3dff013b":"code","cdbed737":"code","0e1db2ad":"code","921feca3":"code","a1702a38":"code","47bd3a7c":"code","681ba2a3":"code","42d99884":"code","ad35f0c9":"code","5a2dba0c":"code","5c66aa3c":"code","e70773f1":"code","be5ac3fa":"code","1737ae20":"code","dcdc972a":"code","104d0a71":"code","66198940":"code","7f568659":"code","4cca7bf8":"code","9ff4a88d":"code","09e914c2":"code","bd1c2cc4":"code","b9afa3e9":"code","23505f34":"code","a3284d9b":"code","d2b67e9a":"code","ab59785b":"code","07abc84c":"code","3169fbc4":"code","36f76889":"code","5eb9831c":"code","5ba6eda6":"code","dd499306":"code","cb111964":"code","5ac12682":"code","1f06127e":"code","c4a89659":"code","40b2b9f8":"code","e8f092a9":"code","03e663cc":"code","0597fc87":"code","be34237a":"code","f3cf737d":"markdown","c5911244":"markdown","dc16daf4":"markdown","ec274bac":"markdown","62776386":"markdown","15f2a1a7":"markdown","168e65ed":"markdown","83b3c86c":"markdown","53610889":"markdown","15af4f02":"markdown","d8999d85":"markdown","8f00fe55":"markdown","b3a4a4f7":"markdown","0aaa7900":"markdown","98bbc262":"markdown","91206bd9":"markdown","26007b7c":"markdown","7fd7bb9c":"markdown","d600afb1":"markdown","92c3d419":"markdown","4b184eec":"markdown","585026c5":"markdown","55afbc3c":"markdown","e07cd409":"markdown","edc4ebf0":"markdown","c105cef5":"markdown","90fefebd":"markdown","172613ff":"markdown","d535694d":"markdown","c69d1e68":"markdown","a0b26b60":"markdown","3e280c5d":"markdown","dd93debb":"markdown","5e4a1ac1":"markdown","a827bb08":"markdown","eb505a14":"markdown","f2f84254":"markdown","d5606b4a":"markdown","cb4e6865":"markdown","446c4993":"markdown","42c2ce5f":"markdown","1863c8a4":"markdown","3be56706":"markdown","ddca0b44":"markdown","af6fd92b":"markdown","a7c43279":"markdown","fb96abb6":"markdown","1ef813c8":"markdown","e7d943d5":"markdown","f5ab96b0":"markdown","df632f63":"markdown","6dcdd7f6":"markdown","70e535a9":"markdown","462bfac1":"markdown","14dfbea5":"markdown","5b9424a1":"markdown","b438c3b4":"markdown","66415a76":"markdown","8749d9f2":"markdown","ea6824f1":"markdown","58cde7c3":"markdown","22119cb1":"markdown","c7e60190":"markdown","cf36d30b":"markdown","bde7db8f":"markdown","8fd16d68":"markdown","93acbdfa":"markdown","d16c1996":"markdown","25330e69":"markdown","927c8d4a":"markdown","98336ede":"markdown","08a6e814":"markdown","b020231b":"markdown","aa5bd529":"markdown","a833e4c0":"markdown","2bec2b12":"markdown","803ca80d":"markdown","26320ae0":"markdown","958caf34":"markdown","536ad8d4":"markdown","f788ace3":"markdown","f93b86b6":"markdown","eac0bb1d":"markdown","64cc8bf2":"markdown","13f4f7b2":"markdown","1b6dfae6":"markdown","c212635d":"markdown","ba2db54d":"markdown","c03b5c4c":"markdown","fd82191d":"markdown","e0f7dace":"markdown","d9335d8e":"markdown","7dd44a5a":"markdown","a446851f":"markdown","b28bd5f7":"markdown","5d9adf94":"markdown","f25985c1":"markdown","29ed7012":"markdown","fb68d41e":"markdown","be8c3cc0":"markdown","ac44b7a6":"markdown","96851603":"markdown","d29e7f12":"markdown","ae7baf23":"markdown","af902a3e":"markdown","3e8735b6":"markdown","00b18f71":"markdown","a07b0b5d":"markdown","afadfa20":"markdown","e4b1e4d9":"markdown","a55200f7":"markdown","ba9884d5":"markdown","78bbc06b":"markdown","5d001f30":"markdown","a9cef5e7":"markdown","06014e0e":"markdown","5f97ecca":"markdown","d55fb0a2":"markdown","093c2336":"markdown","086d400e":"markdown","80aeaedd":"markdown","8b9ccd9f":"markdown","1ab861ec":"markdown","a2d5b52e":"markdown","5fe2ae86":"markdown","18e439d1":"markdown","035286b7":"markdown","6fc42438":"markdown","9e246e7f":"markdown","5e7cab9a":"markdown","b87c7733":"markdown","402cdc79":"markdown","ec988141":"markdown","9602e624":"markdown"},"source":{"8b3f503b":"import numpy as np\nimport pandas as pd\n\n#visualizations\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nimport seaborn as sns\n\nimport plotly.offline as py\npy.init_notebook_mode(connected = True)\nimport plotly.graph_objs as go\nimport plotly.figure_factory as ff\n\n#metrics and split\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix, accuracy_score, classification_report\nfrom sklearn.metrics import precision_score, recall_score, f1_score\n\n#models\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom xgboost import XGBClassifier\n\nimport warnings\nwarnings.filterwarnings('ignore')\n","65c8ebef":"df = pd.read_csv('..\/input\/german-credit-data-with-risk\/german_credit_data.csv')","65a56a5d":"df.head()","1b36465e":"df.columns","9d722f07":"df.shape","df75a7e0":"df.info()","34980cdb":"df.describe()","d25e08a2":"df.isnull().sum()","b11706b5":"df['Saving accounts'].isnull().sum()","26d70ca2":"#mode of Saving Accounts attribute\n\nmode_sav = df['Saving accounts'].mode()\nmode_sav","a13dd175":"df['Saving accounts'].fillna(mode_sav[0], inplace=True)","25b06b22":"df['Saving accounts'].isnull().sum()","1686c93a":"df['Checking account'].isnull().sum()","a1fa14bb":"#mode of Checking Accouny\nmode_check = df['Checking account'].mode()\nmode_check","019e918c":"df['Checking account'].fillna(mode_check[0], inplace=True)","2ee05f0b":"df['Checking account'].isnull().sum()","bc5ae4fa":"df.drop(columns=['Unnamed: 0'], inplace=True)","8e1a0e6b":"df.columns","2f79843e":"df.info()","95ede50e":"df.describe()","7c82f090":"#labels\nlab = df[\"Risk\"].value_counts().keys().tolist()\n#values\nval = df[\"Risk\"].value_counts().values.tolist()\n\ntrace = go.Pie(labels = lab, values = val, marker = dict(colors = [ 'royalblue','lime'], line = dict(color =\"white\",width =2)),\n               rotation = 100,hoverinfo = \"label+value+text\", textinfo='label+percent',hole = .5)\n\nlayout = go.Layout(dict(title = \"Risk Count\",paper_bgcolor = \"rgb(243,243,243)\", plot_bgcolor  = \"rgb(243,243,243)\"))\n                        \ndata = [trace]\n\nfig = go.Figure(data = data,layout = layout)\n\npy.iplot(fig)","68647e82":"#Defining funtion for ploting categorical attributes\n\ndef categorical_plots(var, data):\n        \n    #Adjustment of plots, bigger size and space b\/w subplots\n    \n    fig = plt.figure(figsize=(15,5))\n    fig.subplots_adjust(wspace=0.3)\n    \n    plt.style.use('ggplot')\n        \n    #1st Plot:  Bar plot     \n        \n    plt.subplot(1,2,1)\n    sns.countplot(x=var, data= data)\n    plt.xticks(rotation = 45, horizontalalignment='right')\n    plt.xlabel(var.name + ' Distribution')\n    \n\n    #2nd Plot: PIE Chart\n    \n    labels =var.value_counts().index  #Labels that will be written against slices in pie charts\n    \n    #For the slice with highest value to be exploded, explode parameter is passed. Using for loop to make a tuple of \n    # number of slice using len(unique) and exploding the first slice by mentioning 0.1 at first index. Atlast converted list to tuple\n    \n    a=[0.1]\n    for i in range ((len(var.unique()))-1):\n        a.append(0)\n\n    explode1= tuple(a)\n    #if var.name != 'Customer Name':\n    ax1 = plt.subplot(1,2,2)\n    ax1.pie(var.value_counts(), labels=labels,autopct='%1.1f%%', shadow=True,explode= explode1 )\n    ax1.axis('equal')\n    plt.xlabel(var.name + ' Distribution')\n    plt.legend()\n        \n    show=plt.show()\n    \n    return(show)","2ac830ee":"#FOR NUMERICL PLOTS WE WILL BE USING THE FOLLOWING FUNCTION\n\ndef numerical_plots(var):\n    \n    #Adjustment of plots, bigger size and space b\/w subplots\n    \n    fig = plt.figure(figsize=(15,4))\n    fig.subplots_adjust(wspace=0.3)\n    \n    #1st Plot:  Histogram with KDE plot          \n \n    plt.subplot(1,3,1)\n    sns.distplot(var, color='b')\n    plt.xlabel(var.name + ' Distribution')\n    \n    \n    #2nd Plot:  Box plot\n    \n    plt.subplot(1,3,2)\n    sns.boxplot(y=var)\n    plt.xlabel(var.name + ' Distribution')\n    \n\n    #3rd Plot:  Histogram    \n\n    plt.subplot(1,3,3)\n    var.hist(bins=15, color='steelblue', edgecolor='black', linewidth=1.0, xlabelsize=8, ylabelsize=8, grid=False)\n    plt.xlabel(var.name + ' Distribution')\n    #plt.grid(color='white')\n    \n    \n    show=plt.show()\n    \n    return(show)","1aac2797":"numerical_plots(df.Age)","eaa324ed":"categorical_plots(df.Sex, df)","a61ee6fe":"categorical_plots(df.Job,df)","1c41ddf8":"categorical_plots(df.Housing,df)","0ff3cc13":"categorical_plots(df['Saving accounts'],df)","e8e07710":"categorical_plots(df['Checking account'],df)","468e3a13":"numerical_plots(df['Credit amount'])","7d1a5e24":"numerical_plots(df.Duration)","58df163b":"categorical_plots(df.Purpose,df)","543c24b5":"categorical_plots(df.Risk,df)","3a3b0d81":"fig = plt.figure(figsize=(15,10))\nfig.subplots_adjust(wspace=0.5, hspace=0.5)\n\nc=1\nfor i in df.columns:\n    if df[i].dtype =='O' and i!='Risk':\n        plt.subplot(2,3,c)\n        sns.countplot(x=df[i], hue=df.Risk, data=df)\n        plt.xticks(rotation = 45, horizontalalignment='right')\n        c+=1\nplt.show()","55791b05":"fig = plt.figure(figsize=(15,5))\nfig.subplots_adjust(wspace=0.3)\n\nc=1\nfor i in df.columns:\n    if df[i].dtype !='O' and i!='Job':\n        plt.subplot(1,3,c)\n        sns.boxplot(y=df[i], x=df.Risk)\n        #plt.xticks(rotation = 45, horizontalalignment='right')\n        c+=1\nplt.show()\n#for i in df.columns:\n #   if df[i].dtype !='O':\n  #      sns.boxplot(y=df[i], x=df.Risk)\n   #     plt.show()","99db8054":"plt.style.use('seaborn')\nsns.pairplot(df)","460d4a52":"plt.style.use('default')\n\nfig = plt.figure(figsize=(10,4))\nfig.subplots_adjust(wspace=0.5)\n\nplt.subplot(1,3,1)\nsns.boxplot(x=df.Sex,y=df['Credit amount'], hue=df.Risk, data=df)\n\n#Age2=np.where(df.Age)\nAge2 = np.where((df.Age<20),'TeenAger', np.where((df.Age>=20) & (df.Age<=30),'Adult',np.where((df.Age>30) & (df.Age<=50),'Middle-Aged','Old')))\nplt.subplot(1,3,2)\nsns.boxplot(x=Age2,y=df['Credit amount'], data=df)\nplt.xticks(rotation=45, horizontalalignment='right');\n\nplt.subplot(1,3,3)\nsns.boxplot(x=df.Purpose,y=df['Credit amount'], hue=df.Risk, data=df)\nplt.xticks(rotation=45, horizontalalignment='right');","1e04b641":"plt.style.use('default')\n\nfig = plt.figure(figsize=(10,4))\nfig.subplots_adjust(wspace=0.5)\n\nplt.subplot(1,3,1)\nsns.boxplot(x=df.Sex,y=df['Age'], hue=df.Risk, data=df)\n\n#Age2=np.where(df.Age)\n#Age2 = np.where((df.Age<20),'TeenAger', np.where((df.Age>=20) & (df.Age<=30),'Adult',np.where((df.Age>30) & (df.Age<=50),'Middle-Aged','Old')))\nplt.subplot(1,3,2)\nsns.boxplot(x=Age2,y=df['Duration'], hue=df.Risk, data=df)\nplt.xticks(rotation=45, horizontalalignment='right');\n\nplt.subplot(1,3,3)\nsns.boxplot(x=df.Purpose,y=df['Age'], hue=df.Risk, data=df)\nplt.xticks(rotation=45, horizontalalignment='right');","0f826f30":"df.Risk = np.where(df.Risk =='good',1,0)","f00c06e2":"from yellowbrick.features import rank1d, rank2d\n\nfig, axes = plt.subplots(1, 2,figsize=(15,5))\n\n#heatmap using seaborn\ncorr = df.corr()\nplt.subplot(1,2,1)\nsns.heatmap(corr, annot=True)\n\n#2 dimensional Ranking using yellow brick\ndf2 =df.drop(columns=['Sex','Housing','Saving accounts','Checking account','Purpose'])\nrank2d(df2, ax=axes[1]);","02206821":"import scipy.stats as s\n\ndef chi2(data,target,alpha):\n    \n    for i in df.columns:\n    \n        if df[i].dtype == 'O' and i != target:\n            col = i\n\n            ov = pd.crosstab(data[col], data[target])\n            #max_least_income = ov.loc[ov[' <=50K'].idxmax()].name\n            #max_highest_income = ov.loc[ov[' >50K'].idxmax()].name\n            plt.style.use('ggplot')\n            ov.plot(kind='bar', figsize=(5,5), stacked=True)\n            plt.xlabel(i.title())\n                 \n            chi = s.chi2_contingency(ov)\n            chi2_s = chi[0]\n            p_value = chi[1]\n            dof = chi[2]\n            critical_value = s.chi2.ppf(q=1-alpha, df=dof)\n            \n            print('\\n\\033[1m\\033[4m', col.upper(),':\\033[0m \\n')\n            print('Significance Level = ', alpha)\n            print('Degree of Freedom = ', dof)\n            print('chi2 = ', chi2_s)\n            print('Critical Value = ',critical_value)\n            print('p-value = ', p_value)\n\n            if chi2_s >=critical_value or p_value <= alpha :\n                print('\\nWe reject the null hypotheses, there is a relationship between the two variables \\n')\n            else:\n                print('\\nThere is no relationship between the two variables and the null hypotheses is retained \\n')\n            \n            plt.show()\n            #print('\\033[1mThe bar chart shows that', max_least_income,i,'has the highest number of people with <=50k income and',max_highest_income,i,'has the highest number of people having income >50K \\n')","18a11284":"chi2(df, 'Risk', 0.05)","95b99565":"df.drop(columns = ['Purpose'], inplace=True) #Since there is no relation ship with the target variable","a83fca43":"df.head()","5d5fec55":"outliers = np.log(df[['Age', 'Duration','Credit amount']])","69e0ccee":"fig, (ax1, ax2, ax3) = plt.subplots(1,3, figsize=(8,3))\nsns.distplot(outliers.Age, ax=ax1)\nsns.distplot(outliers.Duration, ax=ax2)\nsns.distplot(outliers[\"Credit amount\"], ax=ax3)\n\nplt.tight_layout()","5b3a6a4c":"df.Sex =  np.where(df.Sex =='male',1,0)","c9f4cf1f":"df.Dummies_Housing = pd.get_dummies(df.Housing)\ndf.Dummies_Housing.head()","44efec24":"df.Dummies_Saving = pd.get_dummies(df['Saving accounts'])\ndf.Dummies_Saving.head()\n","de510b1d":"df.Dummies_Checking = pd.get_dummies(df['Checking account'])\ndf.Dummies_Checking.head()","3e6ceea6":"df = pd.concat([df,df.Dummies_Housing,df.Dummies_Saving,df.Dummies_Checking], axis=1)\ndf.drop(columns=['Housing', 'Saving accounts', 'Checking account'], inplace=True)\ndf.head()","af94f160":"y = df['Risk']\n\nx = df.drop(columns=['Risk'])\nx.head()","693064bb":"from sklearn.preprocessing import StandardScaler\nzscore = StandardScaler()\n\nfor i in x.columns:\n    df[i] = zscore.fit_transform(df[[i]])","46591e6b":"x = x.to_dict(orient='records')\n\nfrom sklearn.feature_extraction import DictVectorizer\nvec = DictVectorizer()\nx = vec.fit_transform(x).toarray()\nx","4ec4297d":"y =np.asarray(y)","0ed8a983":"from sklearn.model_selection import StratifiedKFold\nskf = StratifiedKFold(n_splits=5, random_state=None)\n\nfor train_index, val_index in skf.split(x,y): \n    #print(\"Train:\", train_index, \"Validation:\", val_index) \n    xtrain, xtest = x[train_index], x[val_index] \n    ytrain, ytest = y[train_index], y[val_index]\n","9002561c":"# Defining a function to be used for evaluation for all algorithms\n\nfrom sklearn.metrics import roc_auc_score,roc_curve,scorer, classification_report, confusion_matrix\nimport plotly.tools as tls\n\ndef evaluation(algorithm):\n    #Classification Report\n    print (\"\\n \\033[1m Classification report : \\033[0m\\n\",classification_report(ytest,algorithm ))\n\n    #Accuracy\n    print (\"\\033[1mAccuracy Score   : \\033[0m\",accuracy_score(ytest, algorithm))\n\n    #conf_matrix\n    conf_matrix = confusion_matrix(ytest,algorithm)\n\n\n    #roc_auc_score\n    model_roc_auc = round(roc_auc_score(ytest, algorithm),3) \n    print (\"\\033[1mArea under curve : \\033[0m\",model_roc_auc)\n    fpr,tpr,thresholds = roc_curve(ytest,probabilities[:,1])\n\n    # roc curve plot\n    trace1 = go.Scatter(x = fpr,y = tpr,\n                        name = \"Roc : \" + str(model_roc_auc),\n                        line = dict(color = ('rgb(22, 96, 167)'),width = 2),\n                       )\n    trace3 = go.Scatter(x = [0,1],y=[0,1],\n                        line = dict(color = ('rgb(205, 12, 24)'),width = 2,\n                        dash = 'dot'))\n    #confusion matrix plot\n    trace2 = go.Heatmap(z = conf_matrix ,\n                        x = [\"Bad Risk \",\"Good Risk\"],\n                        y = [\"Bad Risk\",\"Good Risk\"],\n                        colorscale = \"Viridis\",name = \"matrix\" )\n    #subplots\n    fig = tls.make_subplots(rows=1, cols=2, horizontal_spacing = 0.40,subplot_titles=('ROC Curve','Confusion Matrix'))\n\n    fig.append_trace(trace1,1,1)\n    fig.append_trace(trace3,1,1)\n    fig.append_trace(trace2,1,2)\n\n\n    fig['layout'].update(showlegend=False, title=\"Model performance\" ,\n                         autosize = False,height = 400,width = 800,\n                         plot_bgcolor = 'rgba(240,240,240, 0.95)',\n                         paper_bgcolor = 'rgba(240,240,240, 0.95)',\n                         xaxis = dict(title = \"false positive rate\",\n                                 gridcolor = 'rgb(255, 255, 255)',\n                                 domain=[0, 0.6],\n                                 ticklen=5,gridwidth=2),\n                        yaxis = dict(title = \"true positive rate\",\n                                  gridcolor = 'rgb(255, 255, 255)',\n                                  zerolinewidth=1),\n                        margin = dict(b = 20))\n\n    py.iplot(fig)\n","79d5670e":"#Defining a function using Yellow brick library to be used for evaluation\n\nfrom yellowbrick.classifier import ConfusionMatrix, ClassificationReport, ROCAUC\nfrom yellowbrick.features import FeatureImportances\n\n\n\ndef visualize(model):\n\n    \n    fig, axes = plt.subplots(1, 3,figsize=(15,5))\n    fig.subplots_adjust(wspace=0.7)\n    \n    visualgrid = [\n        #FeatureImportances(model,ax=axes[0][0]),\n        ROCAUC(model, ax=axes[1],cmap='RdYlBu'),\n        ConfusionMatrix(model,cmap='BuPu', ax=axes[2]),\n        ClassificationReport(model, cmap='PuBu',ax=axes[0])\n        \n    ]\n\n    for viz in visualgrid:\n        viz.fit(xtrain, ytrain)\n        viz.score(xtest, ytest)\n        viz.finalize()\n\n    plt.show()","762a57a0":"from sklearn.linear_model import LogisticRegression\nfrom sklearn import metrics\n\n#Model Training\nmodel_LG =LogisticRegression()\nmodel_LG.fit(xtrain, ytrain);\n\n#Prediction\ny_pred_LG = model_LG.predict(xtest)\nprobabilities = model_LG.predict_proba(xtest)\nfpr,tpr,thresholds = roc_curve(ytest,probabilities[:,1])\n","1b37c0c6":"evaluation(y_pred_LG)","406d2392":"visualize(model_LG)","7a953f6f":"from sklearn.neighbors import KNeighborsClassifier\n\n#Model Traning\nmodel_knn = KNeighborsClassifier()\nmodel_knn.fit(xtrain,ytrain);\n\n#Prediction\ny_pred_knn = model_knn.predict(xtest)\nprobabilities = model_knn.predict_proba(xtest)\nfpr,tpr,thresholds = roc_curve(ytest,probabilities[:,1])","2e085529":"#print (\"\\n \\033[1m Classification report : \\033[0m\\n\",classification_report(ytest, y_pred_knn))\n#print (\"\\033[1mAccuracy Score   : \\033[0m\",accuracy_score(ytest, y_pred_knn))\nevaluation(y_pred_knn)","2c5a34e7":"visualize(model_knn)","fd2736e0":"from sklearn.naive_bayes import GaussianNB\n\n#Model Training\nmodel_nb = GaussianNB()\nmodel_nb.fit(xtrain, ytrain);\n\n# Model Prediction\ny_pred_nb = model_nb.predict(xtest)\nprobabilities = model_nb.predict_proba(xtest)\nfpr,tpr,thresholds = roc_curve(ytest,probabilities[:,1])","e0e09360":"evaluation(y_pred_nb)","c47e6200":"visualize(model_nb)","f208826f":"from sklearn import tree\n\n# Model Traning\nmodel_DT = tree.DecisionTreeClassifier()\nmodel_DT.fit(xtrain,ytrain)\n\n#Predictions\ny_pred_DT = model_DT.predict(xtest)\nprobabilities = model_DT.predict_proba(xtest)\nfpr,tpr,thresholds = roc_curve(ytest,probabilities[:,1])","69da4d28":"evaluation(y_pred_DT)","6228c1b0":"visualize(model_DT)","7640bdae":"from sklearn.ensemble import RandomForestClassifier\n#Model Training\nmodel_rfc = RandomForestClassifier()\nmodel_rfc.fit(xtrain, ytrain);\n\n#Prediction\ny_pred_rfc = model_rfc.predict(xtest)\nprobabilities = model_rfc.predict_proba(xtest)\nfpr,tpr,thresholds = roc_curve(ytest,probabilities[:,1])","2be5e62e":"evaluation(y_pred_rfc)","799e0f85":"visualize(model_rfc)","32cb14f8":"import xgboost as xgb\n\n#Model Training\nmodel_xgb  = xgb.XGBClassifier()\nmodel_xgb.fit(xtrain, ytrain);\n\n#Prediction\ny_pred_xgb = model_xgb.predict(xtest)\nprobabilities = model_xgb.predict_proba(xtest)\nfpr,tpr,thresholds = roc_curve(ytest,probabilities[:,1])","da899748":"#print (\"\\n \\033[1m Classification report : \\033[0m\\n\",classification_report(ytest, y_pred_xgb))\n#print (\"\\n \\033[1m Accuracy : \\033[0m\\n\",metrics.accuracy_score(ytest, y_pred_xgb))\nevaluation(y_pred_xgb)","1904a111":"visualize(model_xgb)","1a323c1b":"from sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.tree import DecisionTreeClassifier\n\n#Model Training\nmodel_adaboost = AdaBoostClassifier()\nmodel_adaboost.fit(xtrain, ytrain);\n\n#Prediction\ny_pred_ada = model_adaboost.predict(xtest)\nprobabilities = model_adaboost.predict_proba(xtest)\nfpr,tpr,thresholds = roc_curve(ytest,probabilities[:,1])\n","9f56ca6f":"evaluation(y_pred_ada)","1d918906":"visualize(model_adaboost)","3d99bd15":"from sklearn.metrics import f1_score\n\ndef model_report(model,training_x,testing_x,training_y,testing_y,name) :\n    model.fit(training_x,training_y)\n    predictions  = model.predict(testing_x)\n    accuracy     = accuracy_score(testing_y,predictions)\n    recallscore  = recall_score(testing_y,predictions)\n    precision    = precision_score(testing_y,predictions)\n    f1score      = f1_score(testing_y,predictions) \n    roc_auc      = roc_auc_score(testing_y,predictions)\n    \n    \n    df = pd.DataFrame({\"Model\"           : [name],\n                       \"Accuracy_score\"  : [accuracy],\n                       \"Recall_score\"    : [recallscore],\n                       \"Precision\"       : [precision],\n                       \"f1_score\"        : [f1score],\n                       \"roc-auc\"         : [roc_auc]\n                      })\n    return df\n\nmodel1 = model_report(model_LG,xtrain,xtest,ytrain,ytest,\n                      \"Logistic Reg. \")\n\n\nmodel2 = model_report(model_rfc,xtrain,xtest,ytrain,ytest,\n                     \"Random Forest Classifier\")\n\n\nmodel3 = model_report(model_xgb,xtrain,xtest,ytrain,ytest,\n                     \"XGBoost.\")\n\nmodel4 = model_report(model_knn,xtrain,xtest,ytrain,ytest,\n                      \"KNN Classifier\")\n\nmodel5 = model_report(model_nb,xtrain,xtest,ytrain,ytest,\n                      \"Naive Bayes\")\n\nmodel6 = model_report(model_DT,xtrain,xtest,ytrain,ytest,\n                      \"Decision Tree\")\n\nmodel7 = model_report(model_adaboost,xtrain,xtest,ytrain,ytest,\n                      \"AdaBoost\")\n\n\nmodel_performances = pd.concat([model1,model4,model6,model5,model2, model3, model7],axis = 0).reset_index()\n\nmodel_performances = model_performances.drop(columns = \"index\",axis =1)\n\ntable  = ff.create_table(np.round(model_performances,4))\n\npy.iplot(table)","8abc330d":"from sklearn.model_selection import GridSearchCV\nfrom yellowbrick.model_selection import ValidationCurve\nfrom yellowbrick.model_selection import LearningCurve\nfrom sklearn.model_selection import validation_curve","8689b920":"#We will define three functions: One for validation curve using yellowbrick and other using scikit learn\n# Third Function for Learning Curve using yellowbrick\nplt.style.use('ggplot')\n#Function1 (Validation Curve using Yellow Brick)\n\ndef validation(model, meter, range1):\n    i=0\n    fig, axes = plt.subplots(2, 2,figsize=(15,10))\n    fig.subplots_adjust(wspace=0.3, hspace=0.3)\n    \n    cv = StratifiedKFold(n_splits=5) #To avoid class imbalance problem, using Stratified cross validation\n\n    visualgrid = [\n                ValidationCurve(model, param_name=meter,param_range = range1, cv=cv, scoring=\"accuracy\",ax=axes[0][0]),\n                \n                ValidationCurve(model, param_name= meter, param_range= range1, cv=cv, scoring=\"precision\",ax=axes[0][1]),\n             \n                ValidationCurve(model, param_name= meter, param_range= range1, cv=cv, scoring=\"recall\",ax=axes[1][0]),\n                \n                ValidationCurve(model, param_name= meter, param_range= range1, cv=cv, scoring=\"f1\",ax=axes[1][1])    ]\n    \n    score=['Accuracy','Precision','Recall','f1']\n    \n    print('The plots scores are in following order\\n:')\n    \n    for viz in visualgrid:\n        print('\\n',i+1,'.',score[i])\n        i+=1\n        viz.fit(x, y)\n        viz.finalize()\n            \n    plt.show()\n\n\n#Function3 (Learning Curve using Yellow Brick)\n\ndef learning (model):\n    \n    i=0\n    fig, axes = plt.subplots(2, 2,figsize=(15,10))\n    fig.subplots_adjust(wspace=0.3, hspace=0.3)\n    \n    cv = StratifiedKFold(n_splits=5) #To avoid class imbalance problem, using Stratified cross validation\n    sizes = np.linspace(0.3, 1.0, 10)\n\n    visualgrid = [\n                LearningCurve(model, cv=cv, scoring=\"accuracy\",train_sizes=sizes,ax=axes[0][0]),\n                \n                LearningCurve(model, cv=cv, scoring=\"precision\",train_sizes=sizes,ax=axes[0][1]),\n             \n                LearningCurve(model, cv=cv, scoring=\"recall\",train_sizes=sizes,ax=axes[1][0]),\n                \n                LearningCurve(model, cv=cv, scoring=\"f1\",train_sizes=sizes,ax=axes[1][1])    ]\n    \n    score=['Accuracy','Precision','Recall','f1']\n    \n    print('The plots scores are in following order\\n:')\n    \n    for viz in visualgrid:\n        print('\\n',i+1,'.',score[i])\n        i+=1\n        viz.fit(x, y)\n        viz.finalize()\n            \n    plt.show()\n    ","34e92c7c":"plt.style.use('ggplot')\n#Function 2 (Validation curve using scikit learn)\n\ndef validation_plots(name,algorithm, feature, range3):\n    \n    param_range = range3\n    #np.logspace(-5, 3)\n\n    cv = StratifiedKFold(n_splits=5) #To avoid class imbalance problem, using Stratified cross validation\n\n    train_scores, test_scores = validation_curve(algorithm, x, y, feature, param_range=param_range, cv=cv)\n\n    train_scores_mean = np.mean(train_scores, axis=1)\n    train_scores_std = np.std(train_scores, axis=1)\n    test_scores_mean = np.mean(test_scores, axis=1)\n    test_scores_std = np.std(test_scores, axis=1)\n\n    plt.title(\"Validation Curve with\"+name)\n    plt.xlabel(feature)\n    plt.ylabel(\"Score\")\n    plt.ylim(0.0, 1.1)\n    #plt.xlim(-10,10)\n    \n    #scaling on x axis and plotting the mean score\n    plt.plot(param_range, train_scores_mean, label=\"Training score\", color=\"green\", lw=2, linestyle='dashdot')\n        \n    plt.plot(param_range, test_scores_mean, label=\"Cross-validation score\", color=\"navy\", lw=2, linestyle='dashdot')\n        \n    plt.legend(loc=\"best\")\n    \n    plt.show()","f6224ea7":"LG= LogisticRegression(penalty='l2')\nhyperparameters = {'dual':[True,False],'max_iter':[100,110,120,130,140],'C':[0.001,1.0,1.5,2.0,2.5,100]}\nLG_classifier =  GridSearchCV(LG, hyperparameters, refit=True, cv=5)\nLG_classifier.fit(xtrain,ytrain);\n#LG.fit(X_train,y_train);","13aa89a3":"print(\"The best fit value is found out to be :\" ,LG_classifier.best_params_)","f8256bc5":"y_pred_LG2 = LG_classifier.predict(xtest)\n#y_pred_LG2 = LG.predict(X_test)","988ac457":"print (\"\\n \\033[1m Classification report : \\033[0m\\n\",classification_report(ytest, y_pred_LG2))\nprint (\"\\033[1mAccuracy Score   : \\033[0m\",accuracy_score(ytest, y_pred_LG2))","746808aa":"validation_plots('Logistic Regression',LogisticRegression(penalty='l2'),'C',np.arange(0.0001,100))","c5c499a9":"validation(LogisticRegression(penalty='l2'),'C',np.logspace(-5, 3))\n\n# Following are plots with scoring in the following order:\n\n# 1. Accuracy  2 . Precision  3. Recall  4. f1\n","3097a82a":"learning(LogisticRegression(penalty='l2'))","2c79a0de":"knn = KNeighborsClassifier()\nhyperparameters = {'n_neighbors' : list(range(1,40)), 'p':[1,2]}\nknn_classifier =  GridSearchCV(knn, hyperparameters, refit=True, cv=5)\nknn_classifier.fit(xtrain,ytrain);\n#knn.fit(X_train,y_train);","3dff013b":"print(\"The best fit value is found out to be :\" ,knn_classifier.best_params_)","cdbed737":"y_pred_knn2 = knn_classifier.predict(xtest)\n#y_pred_knn2 = knn.predict(X_test)","0e1db2ad":"print (\"\\n \\033[1m Classification report : \\033[0m\\n\",classification_report(ytest, y_pred_knn2))\nprint (\"\\033[1mAccuracy Score   : \\033[0m\",accuracy_score(ytest, y_pred_knn2))","921feca3":"#Validation Curve using Scikit Learn\nvalidation_plots('KNN',KNeighborsClassifier(),'n_neighbors',np.arange(1, 40))","a1702a38":"validation(KNeighborsClassifier(),'n_neighbors',np.arange(1, 40))","47bd3a7c":"learning(KNeighborsClassifier())","681ba2a3":"DT = tree.DecisionTreeClassifier()\nhyperparameters = {'criterion' :['gini','entropy'], 'max_features':[2,4,6,8],'max_depth':[0.001,1,10, 20, 25, 30, 40, 50]},\nDT_classifier =  GridSearchCV(DT, hyperparameters, refit=True, cv=5)\nDT_classifier.fit(xtrain,ytrain);","42d99884":"print(\"The best fit value is found out to be :\" ,DT_classifier.best_params_)","ad35f0c9":"y_pred_DT2 = DT_classifier.predict(xtest)","5a2dba0c":"print (\"\\n \\033[1m Classification report : \\033[0m\\n\",classification_report(ytest, y_pred_DT2))\nprint (\"\\033[1mAccuracy Score   : \\033[0m\",accuracy_score(ytest, y_pred_DT2))","5c66aa3c":"validation_plots('DT',tree.DecisionTreeClassifier(),'max_depth',np.arange(0.001, 40))","e70773f1":"validation(tree.DecisionTreeClassifier(),'max_depth',np.arange(0.001, 10))","be5ac3fa":"learning(DT)","1737ae20":"NB = GaussianNB()\nhyperparameters = {'var_smoothing':np.logspace(0,-9, num=100)}\n\nNB_classifier =  GridSearchCV(NB, hyperparameters, refit=True, cv=5)\nNB_classifier.fit(xtrain,ytrain);","dcdc972a":"print(\"The best fit value is found out to be :\" ,NB_classifier.best_params_)","104d0a71":"y_pred_NB2 = NB_classifier.predict(xtest)","66198940":"print (\"\\n \\033[1m Classification report : \\033[0m\\n\",classification_report(ytest, y_pred_NB2))\nprint (\"\\033[1mAccuracy Score   : \\033[0m\",accuracy_score(ytest, y_pred_NB2))","7f568659":"validation_plots('NB',GaussianNB(),'var_smoothing',np.logspace(0,-100, num=100))\n","4cca7bf8":"validation(GaussianNB(),'var_smoothing',np.logspace(0,-9, num=100))","9ff4a88d":"learning(GaussianNB())","09e914c2":"RF= RandomForestClassifier()\nhyperparameters = {'n_estimators' :[1,2,5,10,15,25,30], 'max_features':[2,4,6,8],'max_depth':[0.01,1,10, 20, 25,30, 40, 50]}\nRF_classifier =  GridSearchCV(RF, hyperparameters, refit=True, cv=5)\nRF_classifier.fit(xtrain,ytrain);","bd1c2cc4":"print(\"The best fit value is found out to be :\" ,RF_classifier.best_params_)","b9afa3e9":"y_pred_rfc2 = RF_classifier.predict(xtest)","23505f34":"print (\"\\n \\033[1m Classification report : \\033[0m\\n\",classification_report(ytest, y_pred_rfc2))\nprint (\"\\033[1mAccuracy Score   : \\033[0m\",accuracy_score(ytest, y_pred_rfc2))","a3284d9b":"validation_plots('Random Forest',RandomForestClassifier(),'max_depth',np.arange(0.001, 40))","d2b67e9a":"validation(RandomForestClassifier(),'max_depth',np.arange(0.001, 10))","ab59785b":"learning(RandomForestClassifier())","07abc84c":"XGB= xgb.XGBClassifier()\nhyperparameters = {\"learning_rate\": [0.05, 0.10, 0.15, 0.20, 0.25, 0.30 ],\"max_depth\": [ 3, 4, 5, 6, 8, 10, 12, 15], \n                    \"gamma\":[ 0.0, 0.1, 0.2 , 0.3, 0.4 ]}\n                   \nXGB_classifier =  GridSearchCV(XGB, hyperparameters, refit=True, cv=5)\nXGB_classifier.fit(xtrain,ytrain);","3169fbc4":"print(\"The best fit value is found out to be :\" ,XGB_classifier.best_params_)","36f76889":"y_pred_xgb2 = RF_classifier.predict(xtest)","5eb9831c":"print (\"\\n \\033[1m Classification report : \\033[0m\\n\",classification_report(ytest, y_pred_xgb2))\nprint (\"\\033[1mAccuracy Score   : \\033[0m\",accuracy_score(ytest, y_pred_xgb2))","5ba6eda6":"validation_plots('XGBoost',xgb.XGBClassifier(),'learning_rate', np.arange(0,100))","dd499306":"validation(xgb.XGBClassifier(),'learning_rate',np.arange(0,100))","cb111964":"learning(xgb.XGBClassifier())","5ac12682":"ADA = AdaBoostClassifier()\n#ADA2= AdaBoostClassifier(learning_rate=0.25)\nhyperparameters = {\"learning_rate\": [0.0001,0.05, 0.10, 0.15, 0.20, 0.25,0.26,0.27, 0.30 ]}\n\nADA_classifier =  GridSearchCV(ADA, hyperparameters, refit=True, cv=5)\nADA_classifier.fit(xtrain,ytrain);\n#ADA2.fit(xtrain,ytrain)","1f06127e":"print(\"The best fit value is found out to be :\" ,ADA_classifier.best_params_)","c4a89659":"y_pred_ada2 = ADA_classifier.predict(xtest)\n#y_pred_ada3 = ADA2.predict(xtest)","40b2b9f8":"print (\"\\n \\033[1m Classification report : \\033[0m\\n\",classification_report(ytest, y_pred_ada2))\nprint (\"\\033[1mAccuracy Score   : \\033[0m\",accuracy_score(ytest, y_pred_ada2))","e8f092a9":"validation_plots('AdaBoost',AdaBoostClassifier(),'learning_rate',np.arange(1,10))","03e663cc":"validation(AdaBoostClassifier(),'learning_rate',np.arange(1,10))","0597fc87":"learning(AdaBoostClassifier())","be34237a":"def model_report(model,training_x,testing_x,training_y,testing_y,name) :\n    model.fit(training_x,training_y)\n    predictions  = model.predict(testing_x)\n    accuracy     = accuracy_score(testing_y,predictions)\n    recallscore  = recall_score(testing_y,predictions)\n    precision    = precision_score(testing_y,predictions)\n    f1score      = f1_score(testing_y,predictions) \n    roc_auc      = roc_auc_score(testing_y,predictions)\n    \n    \n    df = pd.DataFrame({\"Model\"           : [name],\n                       \"Accuracy_score\"  : [accuracy],\n                       \"Recall_score\"    : [recallscore],\n                       \"Precision\"       : [precision],\n                       \"f1_score\"        : [f1score],\n                       \"roc_auc\"         : [roc_auc]\n                       \n                      })\n    return df\n\nmodel1 = model_report(LG_classifier,xtrain,xtest,ytrain,ytest,\n                      \"Logistic Reg. \")\n\n\nmodel2 = model_report(RF_classifier,xtrain,xtest,ytrain,ytest,\n                     \"Random Forest Classifier\")\n\n\nmodel3 = model_report(XGB_classifier,xtrain,xtest,ytrain,ytest,\n                     \"XGBoost.\")\n\nmodel4 = model_report(knn_classifier,xtrain,xtest,ytrain,ytest,\n                      \"KNN Classifier\")\n\nmodel5 = model_report(NB_classifier,xtrain,xtest,ytrain,ytest,\n                      \"Naive Bayes\")\n\nmodel6 = model_report(DT_classifier,xtrain,xtest,ytrain,ytest,\n                      \"Decision Tree\")\n\nmodel7 = model_report(ADA_classifier,xtrain,xtest,ytrain,ytest,\n                      \"AdaBoost\")\n\n    \nmodel_performances = pd.concat([model1,model4,model6,model5,model2, model3,model7],axis = 0).reset_index()\n\nmodel_performances = model_performances.drop(columns = \"index\",axis =1)\n\ntable  = ff.create_table(np.round(model_performances,4))\n\npy.iplot(table)","f3cf737d":"#### 11.1. Manual Encoding","c5911244":"## 14.2. <u>KNN Classifer<\/u>","dc16daf4":"People aged between 20 to 40 tend to apply for credits more as compared to other age groups. The average of people who apply for loan is 35 years.\n\nMales apply for credits\/loans more than females. 69% of the people who applied for credit were male, while female were 31%.\n\nFrom 1000 people who applied for loan, highly skilled people were more than 600. That accounts for 63% of the total people who applied for credit.\n\nCustomers who have their own properties\/houses are in greater number who applied for loan.\n\nCustomers with little savings and little amounts in checking accounts apply for loan more often as compared to people who have better or more savings.\n\nAplications for credit amount less than 5000 are more as compared to higher loan applications.\n\nThe max Credit amount granted to customer was 18424 while minimum was 250. The average credit amount granted was 3271. <\/b>\n\nMajority of the application for loan are for a duration of 10-25 months.\n\nMost of the loan applications are for cars. That accounts for 33.7% application. Interestingly the second most common purpose for loan is radio\/TV.\n\nA credit loan aplication of amount greater than 3000 will have more probability of being a Bad Risk as compared to Good.\n\nWhen a loan is taken for a duration greater than 25 months then the probability of being a Bad Risk is high as compared to being Good.","ec274bac":"## 14.3. <u>Decision Tree<\/u>","62776386":"### 14.6.3. Learning Curves for Random Forest (Using YellowBrick):","15f2a1a7":"### Decision Tree Evaluation Using YellowBrick","168e65ed":"#### <b>We will be utilizing GridSearch to find best parameters for our model and train our model on the best found hyperparamters. \n\n<b><br>Then we will plot validation curve of any one paramaters for range of hypermaters for every model. \n    \n   <br>And lastly we will plot learning curves for every model to see if more data provided to the model will result in better performance or not.\n    ","83b3c86c":"<b> All 4 curves show that increasing depth of random forest will make the model overfit. The model will be more generalized when the depth is less than 0.","53610889":"## <u>12.3. Stratified Cross Validation<\/u>","15af4f02":"### Applying Naive Bayes Classifier","d8999d85":"<b>All the 4 plots show that the training and validation score converge with increasing instances. i.e the models performace will be better as more data is provided to the model.<\/b>","8f00fe55":"### XGBoost Evaluation Using YellowBrick","b3a4a4f7":"Too many records are missing and for the better analysis of the data set with total 1000 records it will be appropiate to fill these values rather than dropping them","0aaa7900":"<b> Its clear from all the four plots that models performance gets better or more generalized as the number of neighbors increase.<\/b>","98bbc262":"<b> The learning curves show that there will no positive impact on models performance with increase in number of instances","91206bd9":"### 14.3.1. Validation Curve for HyperParameter max_depth(Using Plotly):","26007b7c":"## 7. <b>Summary:<\/b>","7fd7bb9c":"## 6. BiVariate Analysis","d600afb1":"<b> The Validation curve shows that with increase in depth of the tree the model will tend t be overfit. With lesser depths model will generalize well.","92c3d419":"### 14.3.2. Validation Curve for HyperParameter max_depth(Using YellowBricks):","4b184eec":"## 2. Loading Data","585026c5":"### 14.4.2. Validation Curve for HyperParameter var_smoothing(Using YellowBrick):","55afbc3c":"#### 5.1.6. Checking account","e07cd409":"## 12.6. <u>KNN Algorithm<\/u>","edc4ebf0":"#### 5.1.1. Age Attribute","c105cef5":"### KNN Evaluation Using YellowBrick","90fefebd":"<b> The Validation curve shows that with increase in depth of the model the model will tend t be overfit. With lesser depths model will generalize well.\n","172613ff":"### 14.4.1.Validation Curve for HyperParameter var_smoothing(Using Plotly):","d535694d":"## 12.9. <u>Random Forest Classifier<\/u>","c69d1e68":"### 14.1.1. Validation Curve for Hyperameter C (Using Scikit Learn):","a0b26b60":"### 14.2.2. Validation Curve for Hyperparameter n_neighbors(Using YellowBricks):","3e280c5d":"### XGBoost Evaluation Using Plotly:","dd93debb":"## 12.8. <u>Decision Tree Classifer<\/u>","5e4a1ac1":"#### 5.1.2.Sex Attribute","a827bb08":"<b> When more data is provided to the model, it will generalize well which is clear from the learning curves as the training and validation scores converge with increasing data","eb505a14":"#### 5.1.4. Housing Attribute","f2f84254":"#### 5.1.5. Saving Accounts attribute","d5606b4a":"The plots clearly show that most of the loan applications are for buying cars. That accounts for 33.7% application. Interestingly the second most common purpose for loan is radio\/TV.","cb4e6865":"## 12.11. <u>AdaBoost Classifier<\/u>","446c4993":"### 14.6.1. Validation Curve for HyperParameter max_depth (Using Plotly):","42c2ce5f":"### Naive Bayes Evaluation using YellowBrick","1863c8a4":"## 4. Data Quality Assessment & Statistical Analysis","3be56706":"Contents\n\n1.\tImporting Libraries\n2.\tLoading Data\n3.\tData Description\n4.\tData Quality Assessment and Statitical Analysis\n5.\tUnivariate Analysis\n6.\tBivariate Analysis\n7.\tSummary\n8.\tCorrelation of Variables\n9.\tChi Square Statistics\n10.\tOutliers Removal\n11.\tFeature Encoding \n12.\tMachine Learning\n    12.1.\tTransforming Features and Target Variable into Array\n    12.3.\tStratified Cross Validation\n    12.4.\tDefining Functions for Evaluation\n    12.5.\tLogistic Regression\n    12.6.\tKnn Algorithm\n    12.7.\tNa\u00efve Bayes classifier\n    12.8.\tDecision Tree Classifier\n    12.9.\tRandom Forest Classifier\n    12.10.\tXGBoost\n    12.11.\tAdaBoost Classifier\n13.\tModel Metrics Comparision\n14.\tHyperparameter Tunning\n    14.1.\tLogistic Regression\n    14.2.\tKNN Classifier\n    14.3.\tDecision Tree\n    14.4.\tNa\u00efve Bayes\n    14.5.\tRandom Forest\n    14.6.\tXGBoost\n    14.7.\tAdaBoost\n15.\tModel Metrics After Hyperparameter Tunning\n","ddca0b44":"### 14.2.1. Validation Curve for Hyperparameter n_neighbors (Using Scikit Learn):","af6fd92b":"People who have little amount in their checking accounts are the one in greater number who applied for loan.","a7c43279":"## 11. Feature Encoding & Data Normalization","fb96abb6":"### AdaBoost Evaluation Using Yellow Brick:","1ef813c8":"# <center><u>12. Machine Learning<\/u><\/center>","e7d943d5":"### Random Forest Classifier Evaluation Using Yellow Brick","f5ab96b0":"## 14.7. <u>XGBoost<\/u>","df632f63":"From 1000 people who applied for loan, highly skilled people were more than 600. That accounts for 63% of the total people who applied for credit.\n","6dcdd7f6":"### Naive Bayes Evaluation Using Plotly","70e535a9":"### Logistic Regression Evaluation Using Plotly","462bfac1":"\n<b>From the model metrics summary above we can see that Logistic Regression performed well when considering all the scores as compared to other algorithms. It has the highest f1 score and accuracy. Precision, recall and and roc_auc is also comparable with other algorithms. Hence for our case Logistic Regression performed best.","14dfbea5":"<b>The Learning curves show that there will no positive impact on models performance with increasing data.","5b9424a1":"# <center><u>German Credit Data Set<\/u><\/center>","b438c3b4":"# <center> **--End --**<\/center>","66415a76":"## 12.10. <u>XGBoost<\/u>","8749d9f2":"### 14.6.2. Validation Curve for HyperParameter max_depth (Using YellowBricks):","ea6824f1":"## 14.8. <u>AdaBoost<\/u>","58cde7c3":"### 12.4. Defining Functions for evaluation","22119cb1":"<b>The Validation curve seconds the best paramter found out using gridsearch i.e. k =31 as we can see the training score and validation score converge when the number of neighbors are 31 and more.<\/b>","c7e60190":"<b> The validation curve shows that the model us generalized well on a wide range of learning rate values. The performance stablizes when learning rate reaches near 50.","cf36d30b":"### Logistic Regression Evaluation using YellowBrick","bde7db8f":"#### 4.1.2. Checking account Attribute","8fd16d68":"#### 4.1.1 Savings account Attribute","93acbdfa":"<b>All categorical variables have a relationship with the target variable except 'Purpose' attribute. Among all the related variables saving account has the highest affect on the Risk.<\/b>","d16c1996":"## 10. Outliers Removal ","25330e69":"## 1. Importing Libraries","927c8d4a":"### 12.1. Transforming Features and Target Variable into Array","98336ede":"### 14.8.1. Validation Curve for Hyperparameter Learning Rate (using Plotly):","08a6e814":"## 14.4. <u>Naive Bayes<\/u>","b020231b":"<b>The validation curve shows that although model generalized well for learning rate >=2 but the overall score will be greaty reduced for both training and validation.","aa5bd529":"<b>The learning curves show that with incresing number of instances the model generalizes.","a833e4c0":"The plot shows that people who have little savings apply for loan more often as compared to people who have better or more savings.","2bec2b12":"## 12.7. <u>Naive Bayes Classifier<\/u>","803ca80d":"## 8. Correlation of Variables ","26320ae0":"The plot shows that males apply more than female. Our data shows that 69% of the people who applied for credit were male, while female were 31%.","958caf34":"### 14.7.2. Validation Curves for HyperParamter Learning Rate (Using YellowBricks):","536ad8d4":"<b>From the plots we can observe:\n    <br>With an increase in age(>45) the number of credit applications decreases.\n    <br>Similarly, middle aged and old aged customers tend to apply for loan applications for shorter durations compared to adults.\n    ","f788ace3":"<b> All 4 curves show that increasing depth of tree will make the model overfit. The tree will be more generalized when the depth is less than 0.","f93b86b6":"The distribution shows that aplications for credit amount less than 5000 are more as compared to higher loan applications.\n<br> Around 50% application were for credit amount =2500","eac0bb1d":"# <center>14. <u> Hyper Parameter Tunning<\/u><\/center>","64cc8bf2":"### Applying KNN","13f4f7b2":"A credit loan aplication of amount greater than 3000 will have more probability of being a Bad Risk as compared to Good.\n<br>When a loan is taken for a duration greater than 25 months then the probability of being a Bad Risk is high as compared to being Good.\n","1b6dfae6":"#### 5.1.10. Risk Attribute","c212635d":"<b>The validation curve for hyperparameter C clearly shows that model is well generalized on a wide rang of values for C i.e. if we select -1 or 100, for both the values Logistic Regression will have a score af around 0.70-0.71.<\/b>","ba2db54d":"## 14.1 <u> Logistic Regression<\/u>","c03b5c4c":"<b>The above plot shows that our data has class imbalance and therefore when using machine learning algorithms we will utilize stratified k fold cross validation that is specifically used to address class imbalance.","fd82191d":"### KNN Evaluation Using Plotly","e0f7dace":"### Best Model","d9335d8e":"<b>The learning curves show that training and validation score converge with increase in data and therfore model will generalize will with incresing number of instances.","7dd44a5a":"### 14.8.3. Learning Curves for AdaBoost (Using yellowBricks):\n","a446851f":"### 14.2.3. Learning Curves for KNN (Using YellowBricks):","b28bd5f7":"### 14.3.3. Learning Curve for Decision Tree(Using YellowBricks):","5d9adf94":"### Applying Decision Tree","f25985c1":"### Decision Tree Evaluation Using Plotly","29ed7012":"#### 12.1.2. Data Normalization","fb68d41e":"## 14.6. <u>Random Forest <\/u>","be8c3cc0":"## 3. Data Description","ac44b7a6":"#### 5.1.9. Purpose Attribute","96851603":"Total number of records is 1000 out of which 183 are null values in the Savings Account attrobute which accounts for 18.3% missing records. We therefore will fill these values using MCT","d29e7f12":"### 14.7.1. Validation Curve for HyperParamter Learning Rate (Using Plotly):","ae7baf23":"<b>In the univariate analysis we saw that age, credit amount and Duration attributes not only had outliers but were also positivey skewed. Therefore, before feeding the data to model we will remove reduce the skewness and remove the outliers using logarithimic transformation.","af902a3e":"<b> All the learning curve show that there will be no positive impact on the performance of the decision tree even if more data is provided.","3e8735b6":"Customers who have their own properties\/houses are in greater number who apply for loan.","00b18f71":"#### 11.2. One Hot Encoding","a07b0b5d":"### 14.1.3. Learning Curves for Logistic Regression (Using YellowBricks)","afadfa20":"The distribution shows that majority of the application are loan are for a duration between 10-25 months.","e4b1e4d9":"## 12.5.<u> Logistic Regression<\/u>","a55200f7":"### 14.7.3. Learning Curves for XGBoost (Using yellowBricks):","ba9884d5":"#### 5.1.3. Job Attribute","78bbc06b":"#### 5.1.7. Credit Amount","5d001f30":"## 15. Model Metrics After Hyper Parameter Tunning","a9cef5e7":"<b> All the validation curve shows that the model us generalized well on a wide range of learning rate values. The performance stablizes when learning rate reaches near 50","06014e0e":"The plots show that people aged between 20 to 40 tend to apply for credits more as compared to other age groups.\n<br>Among the customers who applied for credit, 50% or less were aged around 35 years.\n","5f97ecca":"<b>The given data set has 4 categorical and 4 numeric attributes\/features. The target variable is also categorical.\n<br>\n<br>\nYoungest customer in our data set is 19 years old and the oldest is 75 years old. The average age of people in the given data set is 35. \n<br>\n<br>\nMajority of the customers lie in skilled class.\n<br>\n<br>\nThe max Credit amount granted to customer is 18424 while minimum was 250. The average credit amount granted is 3271. <\/b>","d55fb0a2":"Saving accounts and Checking account attributes have missing values","093c2336":"<b> Validation curve shows that for learning rate =1 the model has some differnce in the training and validation scores which is acceptable as the overall score is good. But as the learning rate increases the models genrelizes at the same time its overall score decreases.","086d400e":"### 14.8.2. Validation Curve for Hyperparameter Learning Rate (using YellowBricks):","80aeaedd":"#### 5.1.8. Duration attribute","8b9ccd9f":"Note: Class imbalance has been solved using Stratified CV which is mentioned in when we performed CV.","1ab861ec":"### 4.1. Filling missing Values","a2d5b52e":"Around 70% of the total application were classified as good risk. ","5fe2ae86":"<b>To cater the problems associated with class imbalance, we will use stratifed cross validation which will split data having equal portions of our target class in every split.<\/b>","18e439d1":"## 13. Model Metrics Comparision","035286b7":"## 9. Chi Square Statistics","6fc42438":"### Random Forest Classifier Evaluation Using Plotly:","9e246e7f":"### AdaBoost Evaluation Using Plotly:","5e7cab9a":"### 14.1.2. Validation Curves for HyperParameter C (Using YellowBrick):","b87c7733":"## 5.Univariate Analysis","402cdc79":"<b> The learning curves show that the model performance will remain same even if more data is provided to the model.","ec988141":"### 14.4.3. Learning Curves for Gaussian Naive Bayes (Using YellowBricks):","9602e624":"<b> 1. Credit amount and Duration attributes have a strong postive relationship. Greater the credit amount, greater will be the duration.\n    \n<br>2. Credit amount and Duration show a negative correlation with the target variabe Risk i.e. Larger credit loan applications may have higher probabilty of risk compared to smaller.\n\n<br>3. Similarly larger duration of loan tend towards Bad Risk applications.<\/b>"}}