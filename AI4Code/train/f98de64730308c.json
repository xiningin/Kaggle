{"cell_type":{"85f5d3a3":"code","3fdd50a2":"code","917c34a0":"code","59c0f749":"code","2e222e97":"code","7c9a169d":"code","3d40305b":"code","37ba440e":"code","6e058029":"code","0d815409":"code","dbb40826":"code","3f8e12ab":"code","903458c3":"code","da84a569":"code","31a82bc2":"code","8903c5a4":"code","baebe577":"code","3d86f8dd":"code","72b5c014":"code","90f67bd3":"code","d9703913":"code","b0f055da":"code","295c82fb":"code","37ade834":"code","5633c656":"code","886d88e2":"code","406290c7":"code","901fb0f4":"code","ca01d85d":"code","65d72829":"code","25143ff5":"code","c9236a68":"code","cb2db3f0":"code","77c4b41d":"code","6bd8c968":"code","702d328f":"code","9f7a4b50":"code","4a492a4f":"code","267a02b9":"markdown","f1b866c2":"markdown","40bbfa05":"markdown","1a9c3c05":"markdown","32e3d84c":"markdown","1ddd378c":"markdown","49894240":"markdown"},"source":{"85f5d3a3":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","3fdd50a2":"import seaborn as sns\nimport matplotlib.pyplot as plt","917c34a0":"df = pd.read_csv('\/kaggle\/input\/health-care-data-set-on-heart-attack-possibility\/heart.csv')\ndf.head()","59c0f749":"df.info()","2e222e97":"df.describe()","7c9a169d":"sns.countplot(df['target'])","3d40305b":"sns.countplot(df['sex'])","37ba440e":"sns.pairplot(data=df)","6e058029":"plt.figure(figsize=(12,10))\nsns.heatmap(df.corr(), cmap='Blues')","0d815409":"df.isna().sum()","dbb40826":"y = df['target']\nX = df.drop('target', axis=1)","3f8e12ab":"from sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state = 0)","903458c3":"from sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\n\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)","da84a569":"# example of grid searching key hyperparametres for logistic regression\nfrom sklearn.model_selection import RepeatedStratifiedKFold\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.linear_model import LogisticRegression\n\nmodel = LogisticRegression()\nsolvers = ['newton-cg', 'lbfgs', 'liblinear']\npenalty = ['l2']\nc_values = [100, 10, 1.0, 0.1, 0.01]\n# define grid search\ngrid = dict(solver=solvers,penalty=penalty,C=c_values)\n\ngrid_search = GridSearchCV(estimator=model, param_grid=grid, n_jobs=-1, cv=3, scoring='accuracy',error_score=0)\ngrid_result = grid_search.fit(X_train, y_train)","31a82bc2":"# summarize results\nprint(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\nmeans = grid_result.cv_results_['mean_test_score']\nstds = grid_result.cv_results_['std_test_score']\nparams = grid_result.cv_results_['params']\nfor mean, stdev, param in zip(means, stds, params):\n    print(\"%f (%f) with: %r\" % (mean, stdev, param))","8903c5a4":"best_model1 = LogisticRegression(C=0.01, solver='liblinear', penalty = 'l2')\nbest_model1.fit(X_train, y_train)\ny_pred = best_model1.predict(X_test)","baebe577":"from sklearn.metrics import confusion_matrix,accuracy_score,roc_curve\n\nprint(confusion_matrix(y_test, y_pred))\nprint('Accuracy of our model is: ', accuracy_score(y_test, y_pred))","3d86f8dd":"lr_acc = accuracy_score(y_test, y_pred)*100\nlr_acc","72b5c014":"from sklearn.naive_bayes import GaussianNB, MultinomialNB","90f67bd3":"nb1 = GaussianNB()\nnb1.fit(X_train, y_train)\ny_pred = nb1.predict(X_test)\n\nprint(confusion_matrix(y_test, y_pred))\nprint('Accuracy of our model is: ', accuracy_score(y_test, y_pred))","d9703913":"nb_acc = accuracy_score(y_test, y_pred)*100\nnb_acc","b0f055da":"from sklearn.ensemble import RandomForestClassifier","295c82fb":"rf1 = RandomForestClassifier()","37ade834":"n_estimators = [200, 300, 500]\nmax_depth = [4,6,5, 8]\nmin_samples_split = [8,9,7]\n\nhyperF = dict(n_estimators = n_estimators, max_depth = max_depth,  \n              min_samples_split = min_samples_split)\n\ngridF = GridSearchCV(rf1, hyperF, cv = 3, verbose = 1, \n                      n_jobs = -1)\nbestF = gridF.fit(X_train, y_train)","5633c656":"# summarize results\nprint(\"Best: %f using %s\" % (bestF.best_score_, bestF.best_params_))\nmeans = bestF.cv_results_['mean_test_score']\nstds = bestF.cv_results_['std_test_score']\nparams = bestF.cv_results_['params']\nfor mean, stdev, param in zip(means, stds, params):\n    print(\"%f (%f) with: %r\" % (mean, stdev, param))","886d88e2":"rf_best = RandomForestClassifier(max_depth= 8, min_samples_split= 8, n_estimators= 500)","406290c7":"rf_best.fit(X_train, y_train)\ny_pred = rf_best.predict(X_test)\n\nprint(confusion_matrix(y_test, y_pred))\nprint('Accuracy of our model is: ', accuracy_score(y_test, y_pred))","901fb0f4":"rf_acc = accuracy_score(y_test, y_pred)*100\nrf_acc","ca01d85d":"df.columns","65d72829":"importance = rf_best.feature_importances_\nindices = np.argsort(importance)[::-1]\nfeature_names = X.columns # e.g. ['A', 'B', 'C', 'D', 'E']\n\nf, ax = plt.subplots(figsize=(11, 9))\nplt.title(\"Feature ranking\", fontsize = 20)\nplt.bar(range(X.shape[1]), importance[indices],\n    color=\"b\", \n    align=\"center\")\nplt.xticks(range(X.shape[1]), feature_names)\nplt.xlim([-1, X.shape[1]])\nplt.ylabel(\"importance\", fontsize = 18)\nplt.xlabel(\"index of the feature\", fontsize = 18)\n","25143ff5":"from sklearn.svm import SVC \n\nfrom sklearn.model_selection import GridSearchCV \n\n# defining parameter range \nparam_grid = {'C': [ 1, 10, 100], \n            'gamma': [ 0.1, 0.01, 0.001,], \n            'kernel': ['rbf', 'poly']} \n\ngrid = GridSearchCV(SVC(), param_grid, refit = True, verbose = 3, cv=3) \n\n# fitting the model for grid search \ngrid.fit(X_train, y_train) \n","c9236a68":"# print best parameter after tuning \nprint(grid.best_params_) \n\n# print how our model looks after hyper-parameter tuning \nprint(grid.best_estimator_) \n","cb2db3f0":"y_test.shape","77c4b41d":"y_pred = grid.predict(X_test)\n\nprint(confusion_matrix(y_test, y_pred))\nprint('Accuracy of our model is: ', accuracy_score(y_test, y_pred))\n\n","6bd8c968":"svm_acc = accuracy_score(y_test, y_pred)*100\nsvm_acc","702d328f":"m1 = 'Logistic Regression'\nm2 = 'Gaussian Naive Bayes'\nm3 = 'Random Forests'\nm4 = 'Support Vector Classifiers'","9f7a4b50":"colors = [\"orange\", \"green\", \"magenta\", \"red\"]\nacc = [lr_acc,nb_acc,rf_acc, svm_acc]\nm = [m1,m2,m3, m4]\nplt.figure(figsize=(10,5))\nplt.yticks(np.arange(0,100,10))\nplt.title(\"barplot Represent Accuracy of different models\")\nplt.ylabel(\"Accuracy %\")\nplt.xlabel(\"Algorithms\")\nsns.barplot( y=acc,x=m, palette=colors)\nplt.show()","4a492a4f":"print(acc)","267a02b9":"* We shall need to normalize\/scale the data since it looks highly skewed","f1b866c2":"Sneek-Peek","40bbfa05":"1. Both SVM and Random Forests have the same accuracy scores, they differ in their predictions by one data points\n\n**Credits - https:\/\/www.kaggle.com\/nareshbhat\/complete-data-analysis-a-to-z\/notebook**","1a9c3c05":"In this kernel we shall look into the 'Heart-healthcare' dataset in which we shall predict possibilty of Heart Attack given the different predictors. \nI have used 4 different classification algorithms namely:\n1. Logistic Regression\n2. Gaussian Naive Bayes\n3. Support Vector Machines\n4. Random Forests Classifier\n\nI have also used extensive Grid Search CV for hyperparameter tuning.\nHope my notebook helps. \ud83e\udd17\ud83e\udd17","32e3d84c":"Absolutely Clean data","1ddd378c":"Train_Test Split","49894240":"Please upvote if you like and comment for any explanations "}}