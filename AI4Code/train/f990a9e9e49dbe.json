{"cell_type":{"e0f0acb2":"code","e30ce020":"code","bcbfc4b2":"code","9cc2c184":"code","4b6f9ca5":"code","13a05acd":"code","b3457570":"code","4a35dff3":"code","0a2c14a7":"code","b9089093":"code","23add3c2":"code","d1ca3595":"code","f8fa4d29":"code","0b3fa211":"code","e9386a83":"code","1851d5ba":"code","353d5417":"code","973fe4cc":"code","2870f245":"code","ec66327d":"code","49573e4f":"code","c8a1141a":"code","21eb4195":"code","9463c8f5":"code","3a5ac7a3":"code","c4a54070":"code","185ed2d8":"code","86f0e253":"code","1b89aa90":"code","8cf331bc":"code","1d034692":"code","dffb43d1":"code","5e8bb098":"code","8cb397fa":"code","107f0b7c":"code","8d6514d5":"code","61cdd5a1":"code","7acbd4fb":"code","6ffc7b7e":"code","b290acb7":"code","c45b2856":"code","116c2ef7":"code","6ac9c59e":"code","4674478b":"code","2113873c":"code","ff480fe0":"code","ea4b7668":"code","72c08967":"code","7d84244e":"code","46e31365":"code","0d633c77":"code","199bf586":"code","d82f6dc1":"code","cbd3517b":"code","36066a19":"code","59c577bc":"code","ad0396d6":"code","63a9e8cc":"code","1f931519":"code","50fa07c4":"code","fbfa24f1":"code","68ae266c":"code","6fb8949b":"code","23a90b93":"code","e738115c":"code","e9c832c0":"code","cd23d6d2":"code","65197da2":"code","93218e46":"code","8f749787":"code","60971712":"code","7703553a":"code","6574be10":"markdown","7c5553ae":"markdown","5f068f99":"markdown","899f51e0":"markdown","eacdec9c":"markdown","9c5d4a23":"markdown","33b6572b":"markdown","0dcc0e8b":"markdown","24224fa9":"markdown","7087887d":"markdown","27d93c61":"markdown","71b0e0c1":"markdown","c0924169":"markdown","f8dfecbf":"markdown","c6a8b95b":"markdown","4c1c74bc":"markdown","6f7f8dac":"markdown","c2021568":"markdown","6582403d":"markdown","2b1c0e1b":"markdown","ee17c6b3":"markdown","a7806d83":"markdown","9ffb83d0":"markdown","bd2a01e8":"markdown","ea217a87":"markdown","20f3d1a7":"markdown","46d8d669":"markdown","a0243630":"markdown","01e52a1d":"markdown","dc09ba7e":"markdown","cdb62661":"markdown","c2b595dc":"markdown","e0d330b2":"markdown","6a147e1d":"markdown","578b01a7":"markdown","c71b8587":"markdown","0a5762db":"markdown","0a3243b8":"markdown","37cda40a":"markdown","9d75c23f":"markdown","1c43ad33":"markdown","b5da52e4":"markdown","f6bd1442":"markdown","8104e7b4":"markdown","69de0887":"markdown","bf5069ad":"markdown","d8643990":"markdown","2eac02f9":"markdown","9caf0e48":"markdown","d6c19755":"markdown","e513ef70":"markdown","4a0a729c":"markdown","e0f571d5":"markdown","7298bb8b":"markdown","901edae6":"markdown","ef8bcccc":"markdown"},"source":{"e0f0acb2":"import pandas as pd\nfrom sklearn.feature_selection import VarianceThreshold\ndf=pd.read_csv('..\/input\/santander-customer-satisfaction\/train.csv',nrows=10000)","e30ce020":"df.shape","bcbfc4b2":"## top 10 data\ndf.head(10)","9cc2c184":"### Define the dataset into dependent and independent feature\nX=df.drop(labels=['TARGET'], axis=1)\ny=df['TARGET']","4b6f9ca5":"from sklearn.model_selection import train_test_split\n# separate dataset into train and test\nX_train, X_test, y_train, y_test = train_test_split(\n    df.drop(labels=['TARGET'], axis=1),\n    df['TARGET'],\n    test_size=0.3,\n    random_state=0)\n\n\n### we have 370 features as our independent features\nX_train.shape, X_test.shape","13a05acd":"var_thres=VarianceThreshold(threshold=0)\nvar_thres.fit(X_train)","b3457570":"var_thres.get_support()","4a35dff3":"### lets find non constant feature\nlen(X_train.columns[var_thres.get_support()])","0a2c14a7":"constant_columns = [column for column in X_train.columns\n                    if column not in X_train.columns[var_thres.get_support()]]\n\nprint(len(constant_columns))\n","b9089093":"for column in constant_columns:\n    print(column)","23add3c2":"X_train.drop(constant_columns,axis=1)","d1ca3595":"#importing libraries\nfrom sklearn.datasets import load_boston\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline","f8fa4d29":"#Loading the dataset\ndata = load_boston()\ndf = pd.DataFrame(data.data, columns = data.feature_names)\ndf[\"MEDV\"] = data.target","0b3fa211":"data.feature_names","e9386a83":"df.head()","1851d5ba":"X = df.drop(\"MEDV\",axis=1)   #Feature Matrix\ny = df[\"MEDV\"]","353d5417":"X.head()","973fe4cc":"y.head()","2870f245":"# separate dataset into train and test\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(\n    X,\n    y,\n    test_size=0.3,\n    random_state=0)\n\nX_train.shape, X_test.shape","ec66327d":"X_train.corr()","49573e4f":"import seaborn as sns\n#Using Pearson Correlation\nplt.figure(figsize=(12,10))\ncor = X_train.corr()\nsns.heatmap(cor, annot=True, cmap=plt.cm.CMRmap_r)\nplt.show()","c8a1141a":"# with the following function we can select highly correlated features\n# it will remove the first feature that is correlated with anything other feature\n\ndef correlation(dataset, threshold):\n    col_corr = set()  # Set of all the names of correlated columns\n    corr_matrix = dataset.corr()\n    for i in range(len(corr_matrix.columns)):\n        for j in range(i):\n            if abs(corr_matrix.iloc[i, j]) > threshold: # we are interested in absolute coeff value\n                colname = corr_matrix.columns[i]  # getting the name of column\n                col_corr.add(colname)\n    return col_corr","21eb4195":"corr_features = correlation(X_train, 0.7)\nlen(set(corr_features))","9463c8f5":"corr_features","3a5ac7a3":"X_train.drop(corr_features,axis=1)\nX_test.drop(corr_features,axis=1)","c4a54070":"import pandas as pd","185ed2d8":"df=pd.read_csv('https:\/\/gist.githubusercontent.com\/tijptjik\/9408623\/raw\/b237fa5848349a14a14e5d4107dc7897c21951f5\/wine.csv')\ndf.head()","86f0e253":"df['Wine'].unique()","1b89aa90":"df.info()","8cf331bc":"### Train test split to avoid overfitting\nfrom sklearn.model_selection import train_test_split\nX_train,X_test,y_train,y_test=train_test_split(df.drop(labels=['Wine'], axis=1),\n    df['Wine'],\n    test_size=0.3,\n    random_state=0)","1d034692":"X_train.head()","dffb43d1":"from sklearn.feature_selection import mutual_info_classif\n# determine the mutual information\nmutual_info = mutual_info_classif(X_train, y_train)\nmutual_info","5e8bb098":"mutual_info = pd.Series(mutual_info)\nmutual_info.index = X_train.columns\nmutual_info.sort_values(ascending=False)","8cb397fa":"#let's plot the ordered mutual_info values per feature\nmutual_info.sort_values(ascending=False).plot.bar(figsize=(20, 8))","107f0b7c":"from sklearn.feature_selection import SelectKBest","8d6514d5":"#No we Will select the  top 5 important features\nsel_five_cols = SelectKBest(mutual_info_classif, k=5)\nsel_five_cols.fit(X_train, y_train)\nX_train.columns[sel_five_cols.get_support()]","61cdd5a1":"import pandas as pd\nhousing_df=pd.read_csv('..\/input\/housepricesadvancedregressiontechniquestrain\/train.csv')","7acbd4fb":"housing_df.head()","6ffc7b7e":"housing_df.info()","b290acb7":"housing_df.isnull().sum()","c45b2856":"\nnumeric_lst=['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\nnumerical_cols = list(housing_df.select_dtypes(include=numeric_lst).columns)","116c2ef7":"numerical_cols","6ac9c59e":"housing_df=housing_df[numerical_cols]","4674478b":"housing_df.head()","2113873c":"housing_df=housing_df.drop(\"Id\",axis=1)","ff480fe0":"### It is always a good practice to split train and test data to avoid\n#overfitting\nfrom sklearn.model_selection import train_test_split\nX_train,X_test,y_train,y_test=train_test_split(housing_df.drop(labels=['SalePrice'], axis=1),\n    housing_df['SalePrice'],\n    test_size=0.3,\n    random_state=0)","ea4b7668":"X_train","72c08967":"X_train.isnull().sum()","7d84244e":"from sklearn.feature_selection import mutual_info_regression\n# determine the mutual information\nmutual_info = mutual_info_regression(X_train.fillna(0), y_train)\nmutual_info","46e31365":"mutual_info = pd.Series(mutual_info)\nmutual_info.index = X_train.columns\nmutual_info.sort_values(ascending=False)","0d633c77":"mutual_info.sort_values(ascending=False).plot.bar(figsize=(15,5))","199bf586":"from sklearn.feature_selection import SelectPercentile","d82f6dc1":"## Selecting the top 20 percentile\nselected_top_columns = SelectPercentile(mutual_info_regression, percentile=20)\nselected_top_columns.fit(X_train.fillna(0), y_train)","cbd3517b":"selected_top_columns.get_support()","36066a19":"X_train.columns[selected_top_columns.get_support()]","59c577bc":"import seaborn as sns\ndf=sns.load_dataset('titanic')\nimport numpy as np","ad0396d6":"df.head()","63a9e8cc":"df.info()","1f931519":"##['sex','embarked','alone','pclass','Survived']\ndf=df[['sex','embarked','alone','pclass','survived']]\ndf.head()","50fa07c4":"df['sex']=np.where(df['sex']==\"male\",1,0)\ndf.head()","fbfa24f1":"### Let's perform label encoding on sex column\nimport numpy as np\n### let's perform label encoding on embarked\nordinal_label = {k: i for i, k in enumerate(df['embarked'].unique(), 0)}\ndf['embarked'] = df['embarked'].map(ordinal_label)","68ae266c":"df.head()","6fb8949b":"### let's perform label encoding on alone\ndf['alone']=np.where(df['alone']==True,1,0)","23a90b93":"df.head()","e738115c":"### train Test split is usually done to avaoid overfitting\nfrom sklearn.model_selection import train_test_split\nX_train,X_test,y_train,y_test=train_test_split(df[['sex','embarked','alone','pclass']],\n                                              df['survived'],test_size=0.3,random_state=100)","e9c832c0":"X_train.head()","cd23d6d2":"X_train['sex'].unique()","65197da2":"X_train.isnull().sum()","93218e46":"## Perform chi2 test\n### chi2 returns 2 values\n### Fscore and the pvalue\nfrom sklearn.feature_selection import chi2\nf_p_values=chi2(X_train,y_train)","8f749787":"f_p_values","60971712":"import pandas as pd\np_values=pd.Series(f_p_values[1])\np_values.index=X_train.columns\np_values","7703553a":"p_values.sort_index(ascending=False)","6574be10":"\n# Import selectkbest function to pick top features","7c5553ae":"# Make a series of these p_values","5f068f99":"# let's check how many unique value we have","899f51e0":"# Performing label encoding on each and every column.","eacdec9c":"# Fisher Score- Chisquare Test For Feature Selection\nCompute chi-squared stats between each non-negative feature and class.\n\nThis score should be used to evaluate categorical variables in a classification task.\nThis score can be used to select the n_features features with the highest values for the test chi-squared statistic from X, which must contain only non-negative features such as booleans or frequencies (e.g., term counts in document classification), relative to the classes.\n\nRecall that the chi-square test measures dependence between stochastic variables, so using this function \u201cweeds out\u201d the features that are the most likely to be independent of class and therefore irrelevant for classification. The Chi Square statistic is commonly used for testing relationships between categorical variables.\n\nIt compares the observed distribution of the different classes of target Y among the different categories of the feature, against the expected distribution of the target classes, regardless of the feature categories.","9c5d4a23":"# Loading the dataset","33b6572b":"# Observation\nSex Column is the most important column when compared to the output feature Survived","0dcc0e8b":"We will choose only top percentile fatures. SelectPercentile helps us to choose top feature out of all the features.","24224fa9":"# There are total 284 non constant features out of 370 features.","7087887d":"# Converting the information of features into series","27d93c61":"here we are trying to find out the best features based on the specific sales price. And ales price is a continuous target variable.","71b0e0c1":"# We will take only top 5 features as our independent features.","c0924169":"# Dividing our independent(x) and dependent feature (y)","f8dfecbf":"# You can see tax and rad features both are are 91% highly co-related with each other, so we will drop one of them. threshold(90)","c6a8b95b":"# Lets apply the variance threshold","4c1c74bc":"Creating a data frame for categorical features.we need to compare all the categories with the output category (Survived)","6f7f8dac":"Sort the series in ascending order","c2021568":"Before going ahead you need to have some statistical test knowledge like annova test, t test , chi square test, p value test","6582403d":"# Check all the values are integers or not","2b1c0e1b":"# Mutual Information\nEstimate mutual information for a continuous target variable.\n\nMutual information (MI) between two random variables is a non-negative value, which measures the dependency between the variables. It is equal to zero if and only if two random variables are independent, and higher values mean higher dependency.\n\nThe function relies on nonparametric methods based on entropy estimation from k-nearest neighbors distances\n\nMutual information is calculated between two variables and measures the reduction in uncertainty for one variable given a known value of the other variable.\n\nInshort\n\nA quantity called mutual information measures the amount of information one can obtain from one random variable given another.\n\nThe mutual information between two random variables X and Y can be stated formally as follows:\n\nI(X ; Y) = H(X) \u2013 H(X | Y) Where I(X ; Y) is the mutual information for X and Y, H(X) is the entropy for X and H(X | Y) is the conditional entropy for X given Y. The result has the units of bits.","ee17c6b3":"Chi2 gives us two value-\n\nFscore - fscore needs to be higher, the more the value of fscore the more important feature is\n\nPvalue - lesser the pvalue the more important the feature is","a7806d83":"# In this technique, we compare two features together and if both features are highly co-related with each other then we will drop anyone features from\u00a0both.","9ffb83d0":"So,i am considering categorical features and will try to find out the top important features.","bd2a01e8":"# We perform all the operations on X_train dataset and after do the same for X_test.","ea217a87":"Note - Remove all the null values from train and test dataset  before applying  mutual_info_classify","20f3d1a7":"# Mutual Information\nMI Estimate mutual information for a discrete target variable.\n\nMutual information (MI) between two random variables is a non-negative value, which measures the dependency between the variables. It is equal to zero if and only if two random variables are independent, and higher values mean higher dependency.\n\nThe function relies on nonparametric methods based on entropy estimation from k-nearest neighbors distances.\n\nInshort\n\nA quantity called mutual information measures the amount of information one can obtain from one random variable given another.\n\nThe mutual information between two random variables X and Y can be stated formally as follows:\n\nI(X ; Y) = H(X) \u2013 H(X | Y) Where I(X ; Y) is the mutual information for X and Y, H(X) is the entropy for X and H(X | Y) is the conditional entropy for X given Y. The result has the units of bits.","46d8d669":"In below code, false means that particular feature is not belonging to top 20 percentile. ","a0243630":"# Highly co-related features","01e52a1d":"# Printing our constant columns","dc09ba7e":"# Variance Threshold\nVariance threshold is a function inside Feature selector.\nFeature selector that removes all low-variance features.\n\nThis feature selection algorithm looks only at the features (X), not the desired outputs (y), and can thus be used for unsupervised learning.","cdb62661":"# Filling null values with zero. \nHigher the value you get for any feature ,the more better it is nd more dependent to target feature.","c2b595dc":"# Converting all the values into series","e0d330b2":"# Drop highly co-related features","6a147e1d":"1st array values is of fscore\n\n2nd array values is of pvalue","578b01a7":"# We dropped highly co-related features after comparing each and every column with each other.","c71b8587":"# 4- Features selection Using Information Gain For Regression","0a5762db":"# Taking only numerical variable to apply mutual information.","0a3243b8":"In the below code, true indicates that a particular feature is very important and false indicates that a particular feature is not so important with respect to the target feature.","37cda40a":"With the following function we can select highly correlated features\nIt will remove the first feature that is correlated with anything other feature\n","9d75c23f":"# 3- Features selection Using Information Gain For Classification","1c43ad33":"# 2- Feature selection with correlation","b5da52e4":"Mutual information (MI) [1] between two random variables is a non-negative value, which measures the dependency between the variables. It is equal to zero if and only if two random variables are independent, and higher values mean higher dependency.As the value near to one the more dependent that particular feature is.","f6bd1442":"# Checking for null values","8104e7b4":"# We have to find out mutual information with respect to each and every feature along with sales price.","69de0887":"# In this particular notebook ,we are going to see 5 different techniques for feature selection.\n1 Dropping Constant Features- Variance Threshold- Unsupervised learning \n2 Feature selection with correlation\n3 Features selection Using Information Gain For Classification \n4 Features selection Using Information Gain For Regression \n5 Feature Selection Using Chi2 Statistical Analysis","bf5069ad":"# 1- Dropping Constant Features- Variance Threshold","d8643990":"# 5- Feature Selection Using Chi2 Statistical Analysis","2eac02f9":"# Getting top most important feature","9caf0e48":"# 86 features are our constant features","d6c19755":"# Numericals columns","e513ef70":"It will remove all those features which have zero threshold value or zero variance feature . It applies only on independent feature.","4a0a729c":"# Here, we are dropping the constant columns.","e0f571d5":"# Ceating a dataframe for all the numerical_cols","7298bb8b":"# Here we are calling our fn. and passing the dataset and threshold value","901edae6":"From above function, we get all the features which are important for our dataset","ef8bcccc":"# High value for any feature mean that particular feature is the best feature "}}