{"cell_type":{"193aac89":"code","e92ce365":"code","0c1be42a":"code","916ef603":"code","ea0c42c5":"code","fe68d93b":"code","1e435624":"code","3f6be7b5":"code","2defd22f":"code","a08689aa":"code","3d2876bd":"code","90e0c97f":"code","de7456ec":"code","cb8c0d50":"code","8986a152":"code","0b47f4a9":"code","06dfd23d":"code","c60ae5b5":"code","88159247":"code","5937f1d6":"code","2b430607":"code","6da31dcd":"code","6765d3c4":"code","4d91ef34":"code","821e7066":"code","e1bf6693":"code","f29c9082":"code","b6b4fa62":"code","da0b9416":"code","1cb79d3e":"code","0d62bb24":"code","16cde8dd":"code","aa308fb5":"code","d71d3e0b":"code","f96199c3":"code","36cb9081":"code","e0f59c3d":"code","0d9cfb31":"code","df02dde6":"code","0a4649bc":"code","3dafd476":"code","28a2eb85":"code","5ea8b391":"code","48032b07":"code","1d922373":"code","3a935d94":"code","f99ecce8":"code","cd976086":"code","3f331cf6":"code","b414d288":"code","8b79f9bd":"code","2a12cf21":"code","f56c66fb":"code","0bae9cce":"code","685646f4":"code","3b01eb01":"code","1f9f8568":"code","52b9b0e7":"code","a46f8e74":"code","05490db2":"code","ee7bf73b":"code","6a45c3ec":"code","7800285f":"code","c7e30669":"code","3e313f70":"code","3fac8762":"code","5b458aeb":"code","ca5ac6a4":"markdown"},"source":{"193aac89":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.\n\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\nfrom category_encoders import OrdinalEncoder, TargetEncoder\nfrom tqdm import tqdm_notebook as tqdm\nfrom lightgbm import LGBMClassifier\n\nfrom sklearn.ensemble import GradientBoostingClassifier\nimport itertools\n\nfrom numba import jit\nfrom numba.errors import NumbaDeprecationWarning, NumbaPendingDeprecationWarning\nimport warnings\n\nwarnings.simplefilter('ignore', category=NumbaDeprecationWarning)\nwarnings.simplefilter('ignore', category=NumbaPendingDeprecationWarning)\n\nimport scipy as sp\nfrom pandas import DataFrame, Series\n\nimport matplotlib.pyplot as plt\nplt.style.use('ggplot')\n%matplotlib inline","e92ce365":"df_train = pd.read_csv('..\/input\/homework-for-students2\/train.csv', index_col=0, parse_dates=['issue_d','earliest_cr_line'])\ndf_test = pd.read_csv('..\/input\/homework-for-students2\/test.csv', index_col=0, parse_dates=['issue_d','earliest_cr_line'])\n\nX_train = df_train\nX_test = df_test\n\n## 2014\u5e74\u4ee5\u964d\u3092\u898b\u308b\nX_train = X_train[X_train.issue_d.dt.year>=2014]\n\ny_train = X_train.loan_condition.values\nX_train = X_train.drop(['loan_condition'], axis=1 )\n\nlen_X_train = len(X_train)\nlen_X_test = len(X_test)\n\n## \u30c6\u30a3\u30ba\u30cb\u30fc\u7528:\u6b63\u898f\u5316\u3057\u305f\u6570\u5024\u7279\u5fb4\u91cf\u306b\u95a2\u3057\u3066\u51e6\u7406\u3059\u308b\u3002\n# Disney_train = X_train\n# Disney_test = X_test","0c1be42a":"# cats = []\n# for col in Disney_train.columns:\n#     if Disney_train[col].dtype == 'object':\n#         cats.append(col)\n        \n#         print(col, Disney_train[col].nunique())","916ef603":"# Disney_train.drop(cats, axis=1, inplace=True)\n# Disney_test.drop(cats, axis=1, inplace=True)\n\n# Disney_train.drop(['mths_since_last_delinq', 'mths_since_last_record', 'mths_since_last_major_derog'], axis=1, inplace=True)\n# Disney_test.drop(['mths_since_last_delinq', 'mths_since_last_record', 'mths_since_last_major_derog'], axis=1, inplace=True)\n\n# Disney_japan = pd.concat([Disney_train, Disney_test],  ignore_index=True)","ea0c42c5":"# log_money = ('loan_amnt', 'annual_inc', 'tot_cur_amt' ,'tot_cur_bal')\n\n# for i in log_money:\n#     Disney_japan[i] = np.log1p(Disney_japan[i])","fe68d93b":"# Disney_col = Disney_train.columns\n# Disney_col","1e435624":"# scaler = StandardScaler()\n\n# for i in Disney_col:\n#     Disney_japan[Disney_col] = scaler.fit_transform(Disney_japan[Disney_col])\n    \n# Disney_japan.head()","3f6be7b5":"# for i in Disney_japan.columns:\n#     print(i, Disney_japan[i].isnull().sum())","2defd22f":"# drops = ['delinq_2yrs', 'inq_last_6mths', 'open_acc', 'pub_rec', 'revol_util', 'collections_12_mths_ex_med', 'acc_now_delinq']","a08689aa":"# Disney_japan.drop(drops, axis=1, inplace=True)\n# Disney_japan.head()","3d2876bd":"# Disney_japan.fillna(0, inplace=True)","90e0c97f":"# tsne = TSNE(n_jobs=5)\n# Disney_Tsney = tsne.fit_transform(Disney_japan)\n# \u30c6\u30a3\u30ba\u30cb\u30fc\u3057\u3088\u3046\u3068\u601d\u3063\u305f\u304c\u3001\u7d50\u5c40\u8a08\u7b97\u6642\u9593\u304c\u9577\u3059\u304e\u3066\u8ae6\u3081\u308b\u3002","de7456ec":"## \u6b20\u640d\u5024\u306e\u60c5\u5831\u3092\u306e\u3053\u3057\u3066\u304a\u304f\nnan_train = X_train.isnull().sum(axis=1)\nnan_test = X_test.isnull().sum(axis=1)\n\nX_train['nan_num']=nan_train\nX_test['nan_num']=nan_test\n\n## \u6b20\u640d\u3067\u3042\u308b\u3053\u3068\u304c\u30d7\u30e9\u30b9\u306a\u306e\u304b\u30de\u30a4\u30ca\u30b9\u306a\u306e\u304b\u3088\u304f\u308f\u304b\u3089\u306a\u3044\u3082\u306e\nnan_que_train = X_train.loc[:, ['mths_since_last_delinq', 'mths_since_last_record', 'mths_since_last_major_derog']].isnull().sum(axis=1)\nnan_que_test = X_test.loc[:, ['mths_since_last_delinq', 'mths_since_last_record', 'mths_since_last_major_derog']].isnull().sum(axis=1)\n\nX_train['nan_que'] = nan_que_train\nX_test['nan_que'] = nan_que_test","cb8c0d50":"## \u30c6\u30ad\u30b9\u30c8\u5909\u63db\u4e88\u5b9a\u306e\u3082\u306e\u3092\u907f\u96e3\u3055\u305b\u308b\nTXT_train1 = X_train.emp_title.copy()\nTXT_test1 = X_test.emp_title.copy()\n\nX_train.drop(['emp_title'], axis=1, inplace=True)\nX_test.drop(['emp_title'], axis=1, inplace=True)\n\nTXT_train2= X_train.title.copy()\nTXT_test2 = X_test.title.copy()\n\nX_train.drop(['title'], axis=1, inplace=True)\nX_test.drop(['title'], axis=1, inplace=True)","8986a152":"## \u304a\u91d1\u306b\u95a2\u308f\u3063\u3066\u3044\u305d\u3046\u306a\u3082\u306e\u306e\u6bd4\u3092\u3068\u308b\n## \u8a08\u7b97\u5931\u6557\u3057\u305f\u3082\u306e\u306f\u898b\u306a\u304b\u3063\u305f\u3053\u3068\u306b...\nmoneys = ['loan_amnt', 'installment', 'annual_inc', 'revol_bal', 'tot_cur_bal']\niter_moneys = list(itertools.combinations(moneys,2))\n\nfor i, j in iter_moneys: \n    X_train['ratio_' + i + '_' + j] = X_train[i]\/X_train[j]\n    X_test['ratio_' + i + '_' + j] = X_test[i]\/X_test[j]\n    X_train['ratio_' + i + '_' + j].fillna(-9999, inplace=True)\n    X_test['ratio_' + i + '_' + j].fillna(-9999, inplace=True)\n    \n## debt to income dti, to money    \nX_train['dti_2_money'] = (X_train['dti']\/100)*12*X_train['annual_inc']\nX_test['dti_2_money'] = (X_test['dti']\/100)*12*X_test['annual_inc']\n    \nX_train['annual_inc'].fillna(-9999, inplace=True)\nX_test['annual_inc'].fillna(-9999, inplace=True)","0b47f4a9":"X_train.head()","06dfd23d":"X_test.head()","c60ae5b5":"cats = []\nfor col in X_train.columns:\n    if X_train[col].dtype == 'object':\n        cats.append(col)\n        \n        print(col, X_train[col].nunique())","88159247":"# X_train.drop(['initial_list_status'], axis=1, inplace=True)\n# X_test.drop(['initial_list_status'], axis=1, inplace=True)\n\n# X_train.drop(['collections_12_mths_ex_med'], axis=1, inplace=True)\n# X_test.drop(['collections_12_mths_ex_med'], axis=1, inplace=True)\n\n# X_train.drop(['mths_since_last_delinq'], axis=1, inplace=True)\n# X_test.drop(['mths_since_last_delinq'], axis=1, inplace=True)\n\n# X_train.drop(['application_type'], axis=1, inplace=True)\n# X_test.drop(['application_type'], axis=1, inplace=True)\n\n# X_train.drop(['delinq_2yrs'], axis=1, inplace=True)\n# X_test.drop(['delinq_2yrs'], axis=1, inplace=True)","5937f1d6":"##\u4ee3\u5165\nX_train['year_d']=df_train.issue_d.dt.year\nX_train['month_d']=df_train.issue_d.dt.month\nX_test['year_d']=df_test.issue_d.dt.year\nX_test['month_d']=df_test.issue_d.dt.month\n\nX_train['year_cr']=df_train.earliest_cr_line.dt.year\nX_train['month_cr']=df_train.earliest_cr_line.dt.month\nX_test['year_cr']=df_test.earliest_cr_line.dt.year\nX_test['month_cr']=df_test.earliest_cr_line.dt.year","2b430607":"df_spi = pd.read_csv('..\/input\/homework-for-students2\/spi.csv', parse_dates=['date'])\ndf_spi['year_d']=df_spi.date.dt.year\ndf_spi['month_d']=df_spi.date.dt.month","6da31dcd":"df_temp =df_spi.groupby(['year_d', 'month_d'], as_index=False)['close'].mean()","6765d3c4":"X_train = X_train.merge(df_temp, on=['year_d', 'month_d'], how='left')\nX_test = X_test.merge(df_temp, on=['year_d', 'month_d'], how='left')","4d91ef34":"X_train.drop(['issue_d'], axis=1, inplace=True)\nX_test.drop(['issue_d'], axis=1, inplace=True)\nX_train.drop(['earliest_cr_line'], axis=1, inplace=True)\nX_test.drop(['earliest_cr_line'], axis=1, inplace=True)","821e7066":"gdp_data = pd.read_csv('..\/input\/homework-for-students2\/US_GDP_by_State.csv', index_col=0)\nstate_data = pd.read_csv('..\/input\/homework-for-students2\/statelatlong.csv', index_col=0)","e1bf6693":"gdp_ind = gdp_data.index\nstate_ind = state_data.index","f29c9082":"gdp_data['City']=gdp_ind\nstate_data['addr_state']=state_ind\n\ngdp_data['per_gdp'] = (gdp_data['Gross State Product']-gdp_data['State & Local Spending'])\/gdp_data['Population (million)']","b6b4fa62":"gdp_data.drop(['Gross State Product'], axis=1, inplace=True)\ngdp_data.drop(['Real State Growth %'], axis=1, inplace=True)\ngdp_data.drop(['Population (million)'], axis=1, inplace=True)\ngdp_data.drop(['State & Local Spending'], axis=1, inplace=True)\n\nmean_state_gdp = gdp_data.groupby(['City'], as_index=False)['per_gdp'].mean()\ngdp_state_data = state_data.merge(mean_state_gdp, on=['City'], how='left')\ngdp_state_data.drop(['City'], axis=1, inplace=True)","da0b9416":"gdp_state_data.head()","1cb79d3e":"X_train=X_train.merge(gdp_state_data, on=['addr_state'], how='left')\nX_test=X_test.merge(gdp_state_data, on=['addr_state'], how='left')","0d62bb24":"X_train.drop(['zip_code'], axis=1, inplace=True)\nX_test.drop(['zip_code'], axis=1, inplace=True)","16cde8dd":"cnt_enc = ['sub_grade', 'home_ownership', 'purpose', 'addr_state' ]\n\nlen_X_train = len(X_train)\nX_japan = pd.concat([X_train, X_test], ignore_index=True)\n\nfor i in cnt_enc:\n    X_japan['cnt_' + i] = X_japan[i].map(X_japan[i].value_counts())\n    X_train['cnt_' + i] = X_japan['cnt_' + i][:len_X_train]\n    X_test['cnt_' + i]  = np.array(X_japan['cnt_' + i][len_X_train:])","aa308fb5":"X_test.head()","d71d3e0b":"X_train.head()","f96199c3":"# @jit\n# def counting (XX):\n#     amnt_class_ = []\n#     for ind in range( len(XX) ):\n#         if XX['loan_amnt'][ind]<=5000:\n#             amnt_class_.append(5000)\n#         elif XX['loan_amnt'][ind]>5000 and XX['loan_amnt'][ind]<= 10000:\n#             amnt_class_.append(10000)\n#         elif XX['loan_amnt'][ind]>10000 and XX['loan_amnt'][ind]<= 15000:\n#             amnt_class_.append(15000)\n#         elif XX['loan_amnt'][ind]>15000 and XX['loan_amnt'][ind]<= 20000:\n#             amnt_class_.append(20000)\n#         elif XX['loan_amnt'][ind]>20000 and XX['loan_amnt'][ind]<= 25000:\n#             amnt_class_.append(25000)\n#         elif XX['loan_amnt'][ind]>25000 and XX['loan_amnt'][ind]<= 30000:\n#             amnt_class_.append(30000)\n#         elif XX['loan_amnt'][ind]>30000 and XX['loan_amnt'][ind]<= 35000:\n#             amnt_class_.append(35000)\n#         elif XX['loan_amnt'][ind] >35000 and XX['loan_amnt'][ind]<= 40000:\n#             amnt_class_.append(40000)\n#         else: \n#             amnt_class_.append(-9999)\n    \n#     return amnt_class_","36cb9081":"# amnt_class = counting(X_japan)\n# X_japan['amnt_class']=np.array(amnt_class)","e0f59c3d":"# X_japan['cnt_amnt_class'] = X_japan['amnt_class'].map( X_japan['amnt_class'].value_counts() )\n\n# X_train['cnt_amnt_class'] = X_japan['cnt_amnt_class'][:len_X_train]\n# X_test['cnt_amnt_class'] =  np.array( X_japan['cnt_amnt_class'][len_X_train:] )","0d9cfb31":"X_japan.head()","df02dde6":"X_train.head()","0a4649bc":"X_test.head()","3dafd476":"emp_length_mapping = {'6 years':6, '4 years':4, '< 1 year':0, '10+ years':10, '3 years':3, '2 years':2,\n '8 years':8, '1 year':1, '9 years':9, '7 years':7, '5 years':5}\n\nX_train['emp_length']=X_train['emp_length'].map(emp_length_mapping)\nX_test['emp_length']=X_test['emp_length'].map(emp_length_mapping)\nX_train['emp_length'].fillna(-9999, inplace=True)\nX_test['emp_length'].fillna(-9999, inplace=True)","28a2eb85":"grade_mapping ={'A':7, 'B':6, 'C':5, 'D':4, 'E':3, 'F':2, 'G':1}\nX_train['grade']=X_train['grade'].map(grade_mapping)\nX_test['grade']=X_test['grade'].map(grade_mapping)","5ea8b391":"sub_grade_mapping ={'A1':35, 'A2':34, 'A3':33, 'A4':32, 'A5':31, 'B1':30, 'B2':29, 'B3':28,\n                'B4':27, 'B5':26, 'C1':25, 'C2':24, 'C3':23, 'C4':22, 'C5':21, 'D1':20,\n                'D2':19, 'D3':18, 'D4':17, 'D5':16, 'E1':15, 'E2':14, 'E3':13, 'E4':12,\n                'E5':11, 'F1':10, 'F2':9, 'F3':8, 'F4':7, 'F5':6, 'G1':5, 'G2':4, 'G3':3,\n                'G4':2, 'G5':1 }\n\nX_train['sub_grade']=X_train['sub_grade'].map(sub_grade_mapping)\nX_test['sub_grade']=X_test['sub_grade'].map(sub_grade_mapping)","48032b07":"home_mapping ={'OWN':5, 'MORTGAGE':4, 'RENT':3, 'OTHER':2, 'NONE':1, 'ANY':-9999 }\n\nX_train['home_ownership']=X_train['home_ownership'].map(home_mapping)\nX_test['home_ownership']=X_test['home_ownership'].map(home_mapping)","1d922373":"## \u52b9\u3044\u3066\u305d\u3046\u306a\u91cf\u3092\u5e8f\u5217\u3068\u304b\u3051\u307e\u304f\u308b\nmulti = ['grade', 'sub_grade', 'home_ownership', 'emp_length']\npassive = ['loan_amnt', 'annual_inc', 'installment', 'revol_bal', 'tot_cur_bal', 'year_cr']\n\nfor i in multi:\n    for j in passive:\n        X_train[ i + '_mul_' + j] = X_train[i]*X_train[j]\n        X_test[ i + '_mul_' + j] = X_test[i]*X_test[j]","3a935d94":"X_train.head()","f99ecce8":"X_test.head()","cd976086":"## \u6b20\u640d\u57cb\u3081\nX_train['month_cr'].fillna(-9999, inplace=True)\nX_test['month_cr'].fillna(-9999, inplace=True)\nX_train['year_cr'].fillna(-9999, inplace=True)\nX_test['year_cr'].fillna(-9999, inplace=True)\n\n## \u6574\u6570\u5024\u306b\u5909\u63db\nX_train['month_cr']=X_train['month_cr'].astype(int)\nX_test['month_cr']=X_test['month_cr'].astype(int)\nX_train['year_cr']=X_train['year_cr'].astype(int)\nX_test['year_cr']=X_test['year_cr'].astype(int)","3f331cf6":"X_train['sub_mul_length'] = X_train['sub_grade']*X_train['emp_length']\nX_test['sub_mul_length'] = X_test['sub_grade']*X_test['emp_length']","b414d288":"X_train.head()","8b79f9bd":"X_test.head()","2a12cf21":"## \u30c0\u30e1\u62bc\u3057\u306e\u30ab\u30c6\u30b4\u30ea\u30ab\u30eb\u30a8\u30f3\u30b3\u30fc\u30c9\uff08\u30a8\u30e9\u30fc\u56de\u907f\uff09\ncats = []\nfor col in X_train.columns:\n    if X_train[col].dtype == 'object':\n        cats.append(col)\n        \n        print(col, X_train[col].nunique())","f56c66fb":"encoder = OrdinalEncoder(cols=cats)\nX_train[cats]=encoder.fit_transform(X_train[cats])\nX_test[cats]=encoder.transform(X_test[cats])","0bae9cce":"TXT_train1.fillna('#', inplace=True)\nTXT_test1.fillna('#', inplace=True)","685646f4":"# counts_vec = CountVectorizer(max_features=15000)\n# Txitter1 = pd.concat([TXT_train1, TXT_test1])\n# counts_vec.fit(Txitter1)\ntfidf = TfidfVectorizer(max_features=10000)\nTxitter1 = pd.concat([TXT_train1, TXT_test1])\ntfidf.fit(Txitter1)","3b01eb01":"# TXT_train1_trans = counts_vec.transform(TXT_train1)\n# TXT_test1_trans = counts_vec.transform(TXT_test1)\n\nTXT_train1_trans = tfidf.transform(TXT_train1)\nTXT_test1_trans = tfidf.transform(TXT_test1)","1f9f8568":"X_train = sp.sparse.hstack([X_train, TXT_train1_trans])\nX_test = sp.sparse.hstack([X_test, TXT_test1_trans])","52b9b0e7":"TXT_train2.fillna('#', inplace=True)\nTXT_test2.fillna('#', inplace=True)","a46f8e74":"# counts_vec = CountVectorizer(max_features=10000)\n# Txitter2 = pd.concat([TXT_train2, TXT_test2])\n# counts_vec.fit(Txitter2)\n\ntfidf = TfidfVectorizer(max_features=10000)\nTxitter2 = pd.concat([TXT_train2, TXT_test2])\ntfidf.fit(Txitter2)","05490db2":"TXT_train2_trans = tfidf.transform(TXT_train2)\nTXT_test2_trans = tfidf.transform(TXT_test2)\n\n# TXT_train2_trans = counts_vec.transform(TXT_train2)\n# TXT_test2_trans = counts_vec.transform(TXT_test2)","ee7bf73b":"X_train = sp.sparse.hstack([X_train, TXT_train2_trans])\nX_test = sp.sparse.hstack([X_test, TXT_test2_trans])","6a45c3ec":"## \u758e\u884c\u5217\u306e\u307e\u307eLight GBM\u306b\u6295\u3052\u308b\nX_train = X_train.tocsr()\nX_test  = X_test.tocsr()","7800285f":"# X_train","c7e30669":"# X_test","3e313f70":"# \u30a2\u30f3\u30b5\u30f3\u30d6\u30eb\u7684\u306a\u3082\u306e\n\n# scores = []\nskf = StratifiedKFold(n_splits=10, random_state=71, shuffle=True)\ny_pred_test = np.zeros(len_X_test)\n\nfor i, (train_ix, test_ix) in tqdm(enumerate(skf.split(X_train, y_train))):\n    X_train_, y_train_ = X_train[train_ix], y_train[train_ix]\n    X_val, y_val = X_train[test_ix], y_train[test_ix]\n    \n    clf = LGBMClassifier(\n        learning_rate = 0.05,\n        num_leaves=31,\n        colsample_bytree=0.9,\n        subsample=0.9,\n        n_estimators=9999,\n        random_state=71,\n        importance_type='gain')\n\n    clf.fit(X_train_, y_train_, \n            early_stopping_rounds=100,\n            eval_metric='auc',\n            eval_set=[(X_val, y_val)])\n#     y_pred = clf.predict_proba(X_val)[:,1]\n    y_pred_test += clf.predict_proba(X_test)[:,1]\n\n#     score = roc_auc_score(y_val, y_pred)\n#     scores.append(score)\n#     print('CV Score of Fold_%d is %f' % (i, score))\n    \n    y_pred_test += clf.predict_proba(X_test)[:,1]\n    y_pred_test \/= 10","3fac8762":"submission = pd.read_csv('..\/input\/homework-for-students2\/sample_submission.csv', index_col=0)\n\n# y_pred = clf.predict_proba(X_test)[:,1]\nsubmission.loan_condition = y_pred_test\nsubmission.to_csv('submission.csv')","5b458aeb":"# pd.set_option('display.max_columns', 100)\n# pd.set_option('display.max_rows', 100)\n\n# importance = pd.DataFrame(clf.feature_importances_, index=X_train.columns, columns=['importance'])\n# importance = importance.sort_values('importance', ascending=False)\n# display(importance)","ca5ac6a4":"\u5e8f\u5217\u304c\u3042\u308a\u305d\u3046\u306a\u3082\u306e\u3092\u30de\u30c3\u30d4\u30f3\u30b0\u3057\u3066\u3044\u304f"}}