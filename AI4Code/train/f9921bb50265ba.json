{"cell_type":{"70800d96":"code","5c93eb1a":"code","d22f9550":"code","b2c7f92d":"code","04b25a1e":"code","c1de49e0":"code","c9bdfeb7":"code","21906aa9":"code","82a8462f":"code","6a72acd4":"code","c30ab957":"code","852b5308":"code","62847df0":"code","5d9df23b":"code","8def449a":"markdown","074aca5b":"markdown","f4782465":"markdown","24f6fe5d":"markdown","ac310251":"markdown","ad09703f":"markdown","025af9f2":"markdown","8db584bb":"markdown","61d74c4e":"markdown","347adea0":"markdown","bdc6b5e0":"markdown","db531fed":"markdown","77d3c9b9":"markdown","63696677":"markdown"},"source":{"70800d96":"# Data manipulation\nimport numpy as np\nimport pandas as pd\n# Data visualisation\nimport matplotlib.pyplot as plt\n# Text pre-processing\nfrom string import punctuation\nfrom nltk.corpus import stopwords\nimport spacy\n# Splitting data into train, evaluation and test\nfrom sklearn.model_selection import train_test_split\n# Na\u00efve Bayes\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.svm import SVC\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\n# Evaluation\nfrom sklearn.metrics import classification_report\n# Vectorization\nfrom sklearn.feature_extraction.text import TfidfVectorizer","5c93eb1a":"reviews = pd.read_csv('\/kaggle\/input\/imdb-dataset-of-50k-movie-reviews\/IMDB Dataset.csv')\n\nprint('Number of reviews :', len(reviews))\nreviews.head(10)","d22f9550":"s = reviews['sentiment'].value_counts()\ns = (s\/s.sum())*100\n\nplt.figure()\nbars = plt.bar(s.index, s.values, color = ['green', 'red'], alpha = .6)\nplt.xticks(s.index, ['Positive', 'Negative'], fontsize = 15)\nplt.tick_params(bottom = False, top = False, left = False, right = False, labelleft = False)\nfor spine in plt.gca().spines.values():\n    spine.set_visible(False)\nfor bar in bars:\n    plt.text(bar.get_x() + bar.get_width()\/2, bar.get_height() - 5, s = str(bar.get_height())[:2] + '%', ha = 'center', fontsize = 15)\nplt.title('Reviews polarity', fontsize = 17)\nplt.show()","b2c7f92d":"nlp = spacy.load('en')\nstopwords_ = set(stopwords.words('english'))\nkeep_track = 1\n\ndef remove_stopwords(text, stopwords):\n    tokens = text.split(' ')\n    tokens = [token for token in tokens if token not in stopwords]\n    return ' '.join(tokens)\n\ndef lemmatize(text):\n    global keep_track\n    \n    doc = nlp(text)\n    lemms = []\n    for token in doc:\n        if token.lemma_.startswith('-'):\n            lemms.append(str(token).lower())\n        else:\n            lemms.append(token.lemma_)\n    lemms = [lemm for lemm in lemms if not lemm.startswith('-')]\n    if keep_track % 100 == 0:\n        print(str(np.round((keep_track\/50000)*100, 2)) + '%', end = '\\r')\n    keep_track += 1\n    return ' '.join(lemms) ","04b25a1e":"print('Pre-processing ...')\n# Removing line breakers ...\nreviews['review'] = reviews['review'].str.replace('<br \/>', ' ')\n# Removing digits ...\nreviews['review'] = reviews['review'].str.replace('\\d+', ' ')\n# Lower casing ...\nreviews['review'] = reviews['review'].str.lower()\nprint('Done! ')\n\nprint('Removing stopwords ...')\nreviews['review'] = reviews['review'].apply(lambda review: remove_stopwords(review, stopwords_))\nprint('Done! ')\n\nprint('Lemmatizing ...')\nreviews['review'] = reviews['review'].apply(lemmatize)\nprint('Done! ')\n\nprint('Removing punctuation ...')\nreviews['review'] = reviews['review'].str.replace('[' + punctuation +']', ' ', regex = True)\n# Squeezing white spaces ...\nreviews['review'] = reviews['review'].str.replace('\\s+', ' ')\nprint('Done! ')\nreviews['review'].head(10)","c1de49e0":"X_train, X_test, y_train, y_test = train_test_split(reviews['review'], reviews['sentiment'], train_size = .9)\n\nprint('Training dataset : {} reviews'.format(X_train.shape[0]))\nprint('Testing dataset : {} reviews'.format(X_test.shape[0]))","c9bdfeb7":"vect = TfidfVectorizer(ngram_range = (1, 2), min_df = .01)\nX_train_vect = vect.fit_transform(X_train)","21906aa9":"model = MultinomialNB(alpha = 0.001).fit(X_train_vect, y_train)\ny_pred_test = model.predict(vect.transform(X_test))\ny_pred_train = model.predict(X_train_vect)\n\nprint('Training data')\nprint(classification_report(y_train, y_pred_train))\nprint('Test data')\nprint(classification_report(y_test, y_pred_test))","82a8462f":"model = LogisticRegression().fit(X_train_vect, y_train)\ny_pred_test = model.predict(vect.transform(X_test))\ny_pred_train = model.predict(X_train_vect)\n\nprint('Training data')\nprint(classification_report(y_train, y_pred_train))\nprint('Test data')\nprint(classification_report(y_test, y_pred_test))","6a72acd4":"model = RandomForestClassifier().fit(X_train_vect, y_train)\ny_pred_test = model.predict(vect.transform(X_test))\ny_pred_train = model.predict(X_train_vect)\n\nprint('Training data')\nprint(classification_report(y_train, y_pred_train))\nprint('Test data')\nprint(classification_report(y_test, y_pred_test))","c30ab957":"model = LogisticRegression().fit(X_train_vect, y_train)\ny_pred_test = model.predict(vect.transform(X_test))","852b5308":"df = pd.DataFrame()\ndf['reviews'] = X_test\ndf['predicted sentiment'] = y_pred_test\ndf['actual sentiment'] = y_test\ndf = df[df['actual sentiment'] != df['predicted sentiment']]\n\ndf","62847df0":"reviews['review'][33066]","5d9df23b":"reviews['review'][7428]","8def449a":"One of the most common issues encountered while making ML model or DL model is mis-labeled data and on the example above we can clearly notice an example. The review reflected negative feedback with term such as \"weak script\", \"slow pace\", \"exhaustingly long\", ...\n\nThis doesn't change the fact that some reviews are wrongly classified (example below)","074aca5b":"Random Forest seems to be overfitting.\n\nLogistic Regression is giving the best accuracy (89%) which is still low","f4782465":"## Loading data","24f6fe5d":"## Vectorizing reviews for training\n\nHaving a large vocabulary usually makes ML models overfit the training data. Thus, we will only be using terms that appear in more than 1% of our documents (reviews).\n\nNote : I did go through multiple values of `min_df` and 1% seems to be the optimal value to balance between overfitting and underfitting.","ac310251":"## Splitting data for training","ad09703f":"### Logistic Regression","025af9f2":"### Random Forest","8db584bb":"Looking at review 33066 which was labeled positive and predicted negative","61d74c4e":"## Text pre-processing","347adea0":"## Libraries","bdc6b5e0":"### Multinomial Na\u00efve Bayes","db531fed":"## Training Machine learning model","77d3c9b9":"## Going through wrong predictions (Logistic Regression)","63696677":"<center>\n    <h1>Movie Review Classification<\/h1>\n<\/center>"}}