{"cell_type":{"351ecc8c":"code","8a399090":"code","91703ba4":"code","76d9e843":"code","eb7e31f7":"code","98c700e9":"code","8150800d":"code","aae36154":"code","e065d26e":"code","42065d7a":"code","e462879d":"code","99bc896b":"code","2a64f8ec":"code","9477f9df":"code","0953abca":"code","c2bda0b3":"code","8347ba0b":"code","e1f8c8d9":"code","05215f72":"code","7f50e49a":"code","a4fd2455":"code","567cfb84":"code","9e8243a3":"code","f9c8e31d":"code","438cc219":"code","db199b31":"code","2ffd8089":"code","aa9dc84f":"code","e827360e":"code","91e306ee":"code","3104d3ba":"code","d4ced403":"code","c616b0d9":"code","8471a605":"code","db3ac8c0":"code","901d5ab1":"code","fc53db6b":"code","7bc9e95b":"code","c66758c2":"code","ca0f41a1":"code","e06cabb0":"code","9b4b2dd5":"code","c0adebcf":"code","17ed0ac0":"code","f42b8f16":"code","d2f6d064":"code","98f8b31b":"code","eb01e22f":"code","5499c9d2":"code","63eeb436":"code","c3a01d4d":"markdown","cecef528":"markdown","2b640f7a":"markdown","758b96ac":"markdown","358847f3":"markdown","de724e5d":"markdown","ff1762ff":"markdown","864b5745":"markdown","3e6bd64f":"markdown","0fc3a57f":"markdown","93768d2a":"markdown","2fb8b466":"markdown","68057bcd":"markdown"},"source":{"351ecc8c":"import numpy as np\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set(style=\"darkgrid\")\n\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.preprocessing import OneHotEncoder, LabelEncoder, StandardScaler\nfrom sklearn.metrics import roc_curve, auc\nfrom sklearn.model_selection import StratifiedKFold\n\nimport string\nimport warnings\nwarnings.filterwarnings('ignore')\n\nSEED = 42","8a399090":"def concat_df(train_data, test_data):\n    # Returns a concatenated df of training and test set on axis 0\n    return pd.concat([train_data, test_data], sort=True).reset_index(drop=True)\n\ndef divide_df(all_data):\n    # Returns divided dfs of training and test set\n    return all_data.loc[:890], all_data.loc[891:].drop(['Survived'], axis=1)\n\ndf_train = pd.read_csv('..\/input\/titanic\/train.csv')\ndf_test = pd.read_csv('..\/input\/titanic\/test.csv')\ndf_all = concat_df(df_train, df_test)\n\ndf_train.name = 'Training Set'\ndf_test.name = 'Test Set'\ndf_all.name = 'All Set' \n\ndfs = [df_train, df_test]\n\nprint('Number of Training Examples = {}'.format(df_train.shape[0]))\nprint('Number of Test Examples = {}\\n'.format(df_test.shape[0]))\nprint('Training X Shape = {}'.format(df_train.shape))\nprint('Training y Shape = {}\\n'.format(df_train['Survived'].shape[0]))\nprint('Test X Shape = {}'.format(df_test.shape))\nprint('Test y Shape = {}\\n'.format(df_test.shape[0]))\nprint(df_train.columns)\nprint(df_test.columns)","91703ba4":"def display_missing(df):    \n    for col in df.columns.tolist():          \n        print('{} column missing values: {}'.format(col, df[col].isnull().sum()))\n    print('\\n')\n    \nfor df in dfs:\n    print('{}'.format(df.name))\n    display_missing(df)","76d9e843":"df_all_corr = df_all.corr().abs().unstack().sort_values(kind=\"quicksort\", ascending=False).reset_index()\ndf_all_corr.rename(columns={\"level_0\": \"Feature 1\", \"level_1\": \"Feature 2\", 0: 'Correlation Coefficient'}, inplace=True)\ndf_all_corr[df_all_corr['Feature 1'] == 'Age']","eb7e31f7":"age_by_pclass_sex = df_all.groupby(['Sex', 'Pclass']).median()['Age']\n\nfor pclass in range(1, 4):\n    for sex in ['female', 'male']:\n        print('Median age of Pclass {} {}s: {}'.format(pclass, sex, age_by_pclass_sex[sex][pclass]))\nprint('Median age of all passengers: {}'.format(df_all['Age'].median()))\n\n# Filling the missing values in Age with the medians of Sex and Pclass groups\ndf_all['Age'] = df_all.groupby(['Sex', 'Pclass'])['Age'].apply(lambda x: x.fillna(x.median()))","98c700e9":"df_all[df_all['Embarked'].isnull()]","8150800d":"# Filling the missing values in Embarked with S\ndf_all['Embarked'] = df_all['Embarked'].fillna('S')","aae36154":"med_fare = df_all.groupby(['Pclass', 'Parch', 'SibSp']).Fare.median()[3][0][0]\n# Filling the missing value in Fare with the median Fare of 3rd class alone passenger\ndf_all['Fare'] = df_all['Fare'].fillna(med_fare)","e065d26e":"# Creating Deck column from the first letter of the Cabin column (M stands for Missing)\ndf_all['Deck'] = df_all['Cabin'].apply(lambda s: s[0] if pd.notnull(s) else 'M')\n\ndf_all_decks = df_all.groupby(['Deck', 'Pclass']).count().drop(columns=['Survived', 'Sex', 'Age', 'SibSp', 'Parch', \n                                                                        'Fare', 'Embarked', 'Cabin', 'PassengerId', 'Ticket']).rename(columns={'Name': 'Count'}).transpose()\n\ndef get_pclass_dist(df):\n    \n    # Creating a dictionary for every passenger class count in every deck\n    deck_counts = {'A': {}, 'B': {}, 'C': {}, 'D': {}, 'E': {}, 'F': {}, 'G': {}, 'M': {}, 'T': {}}\n    decks = df.columns.levels[0]    \n    \n    for deck in decks:\n        for pclass in range(1, 4):\n            try:\n                count = df[deck][pclass][0]\n                deck_counts[deck][pclass] = count \n            except KeyError:\n                deck_counts[deck][pclass] = 0\n                \n    df_decks = pd.DataFrame(deck_counts)    \n    deck_percentages = {}\n\n    # Creating a dictionary for every passenger class percentage in every deck\n    for col in df_decks.columns:\n        deck_percentages[col] = [(count \/ df_decks[col].sum()) * 100 for count in df_decks[col]]\n        \n    return deck_counts, deck_percentages\n\ndef display_pclass_dist(percentages):\n    \n    df_percentages = pd.DataFrame(percentages).transpose()\n    deck_names = ('A', 'B', 'C', 'D', 'E', 'F', 'G', 'M', 'T')\n    bar_count = np.arange(len(deck_names))  \n    bar_width = 0.85\n    \n    pclass1 = df_percentages[0]\n    pclass2 = df_percentages[1]\n    pclass3 = df_percentages[2]\n    \n    plt.figure(figsize=(20, 10))\n    plt.bar(bar_count, pclass1, color='#b5ffb9', edgecolor='white', width=bar_width, label='Passenger Class 1')\n    plt.bar(bar_count, pclass2, bottom=pclass1, color='#f9bc86', edgecolor='white', width=bar_width, label='Passenger Class 2')\n    plt.bar(bar_count, pclass3, bottom=pclass1 + pclass2, color='#a3acff', edgecolor='white', width=bar_width, label='Passenger Class 3')\n\n    plt.xlabel('Deck', size=15, labelpad=20)\n    plt.ylabel('Passenger Class Percentage', size=15, labelpad=20)\n    plt.xticks(bar_count, deck_names)    \n    plt.tick_params(axis='x', labelsize=15)\n    plt.tick_params(axis='y', labelsize=15)\n    \n    plt.legend(loc='upper left', bbox_to_anchor=(1, 1), prop={'size': 15})\n    plt.title('Passenger Class Distribution in Decks', size=18, y=1.05)   \n    \n    plt.show()    \n\nall_deck_count, all_deck_per = get_pclass_dist(df_all_decks)\ndisplay_pclass_dist(all_deck_per)","42065d7a":"# Passenger in the T deck is changed to A\nidx = df_all[df_all['Deck'] == 'T'].index\ndf_all.loc[idx, 'Deck'] = 'A'","e462879d":"df_all_decks_survived = df_all.groupby(['Deck', 'Survived']).count().drop(columns=['Sex', 'Age', 'SibSp', 'Parch', 'Fare', \n                                                                                   'Embarked', 'Pclass', 'Cabin', 'PassengerId', 'Ticket']).rename(columns={'Name':'Count'}).transpose()\n\ndef get_survived_dist(df):\n    \n    # Creating a dictionary for every survival count in every deck\n    surv_counts = {'A':{}, 'B':{}, 'C':{}, 'D':{}, 'E':{}, 'F':{}, 'G':{}, 'M':{}}\n    decks = df.columns.levels[0]    \n\n    for deck in decks:\n        for survive in range(0, 2):\n            surv_counts[deck][survive] = df[deck][survive][0]\n            \n    df_surv = pd.DataFrame(surv_counts)\n    surv_percentages = {}\n\n    for col in df_surv.columns:\n        surv_percentages[col] = [(count \/ df_surv[col].sum()) * 100 for count in df_surv[col]]\n        \n    return surv_counts, surv_percentages\n\ndef display_surv_dist(percentages):\n    \n    df_survived_percentages = pd.DataFrame(percentages).transpose()\n    deck_names = ('A', 'B', 'C', 'D', 'E', 'F', 'G', 'M')\n    bar_count = np.arange(len(deck_names))  \n    bar_width = 0.85    \n\n    not_survived = df_survived_percentages[0]\n    survived = df_survived_percentages[1]\n    \n    plt.figure(figsize=(20, 10))\n    plt.bar(bar_count, not_survived, color='#b5ffb9', edgecolor='white', width=bar_width, label=\"Not Survived\")\n    plt.bar(bar_count, survived, bottom=not_survived, color='#f9bc86', edgecolor='white', width=bar_width, label=\"Survived\")\n \n    plt.xlabel('Deck', size=15, labelpad=20)\n    plt.ylabel('Survival Percentage', size=15, labelpad=20)\n    plt.xticks(bar_count, deck_names)    \n    plt.tick_params(axis='x', labelsize=15)\n    plt.tick_params(axis='y', labelsize=15)\n    \n    plt.legend(loc='upper left', bbox_to_anchor=(1, 1), prop={'size': 15})\n    plt.title('Survival Percentage in Decks', size=18, y=1.05)\n    \n    plt.show()\n\nall_surv_count, all_surv_per = get_survived_dist(df_all_decks_survived)\ndisplay_surv_dist(all_surv_per)","99bc896b":"df_all['Deck'] = df_all['Deck'].replace(['A', 'B', 'C'], 'ABC')\ndf_all['Deck'] = df_all['Deck'].replace(['D', 'E'], 'DE')\ndf_all['Deck'] = df_all['Deck'].replace(['F', 'G'], 'FG')\n\ndf_all['Deck'].value_counts()","2a64f8ec":"# Dropping the Cabin feature\ndf_all.drop(['Cabin'], inplace=True, axis=1)\n\ndf_train, df_test = divide_df(df_all)\ndfs = [df_train, df_test]\n\nfor df in dfs:\n    display_missing(df)","9477f9df":"survived = df_train['Survived'].value_counts()[1]\nnot_survived = df_train['Survived'].value_counts()[0]\nsurvived_per = survived \/ df_train.shape[0] * 100\nnot_survived_per = not_survived \/ df_train.shape[0] * 100\n\nprint('{} of {} passengers survived and it is the {:.2f}% of the training set.'.format(survived, df_train.shape[0], survived_per))\nprint('{} of {} passengers didnt survive and it is the {:.2f}% of the training set.'.format(not_survived, df_train.shape[0], not_survived_per))\n\nplt.figure(figsize=(10, 8))\nsns.countplot(df_train['Survived'])\n\nplt.xlabel('Survival', size=15, labelpad=15)\nplt.ylabel('Passenger Count', size=15, labelpad=15)\nplt.xticks((0, 1), ['Not Survived ({0:.2f}%)'.format(not_survived_per), 'Survived ({0:.2f}%)'.format(survived_per)])\nplt.tick_params(axis='x', labelsize=13)\nplt.tick_params(axis='y', labelsize=13)\n\nplt.title('Training Set Survival Distribution', size=15, y=1.05)\n\nplt.show()","0953abca":"df_train_corr = df_train.drop(['PassengerId'], axis=1).corr().abs().unstack().sort_values(kind=\"quicksort\", ascending=False).reset_index()\ndf_train_corr.rename(columns={\"level_0\": \"Feature 1\", \"level_1\": \"Feature 2\", 0: 'Correlation Coefficient'}, inplace=True)\ndf_train_corr.drop(df_train_corr.iloc[1::2].index, inplace=True)\ndf_train_corr_nd = df_train_corr.drop(df_train_corr[df_train_corr['Correlation Coefficient'] == 1.0].index)\n\ndf_test_corr = df_test.corr().abs().unstack().sort_values(kind=\"quicksort\", ascending=False).reset_index()\ndf_test_corr.rename(columns={\"level_0\": \"Feature 1\", \"level_1\": \"Feature 2\", 0: 'Correlation Coefficient'}, inplace=True)\ndf_test_corr.drop(df_test_corr.iloc[1::2].index, inplace=True)\ndf_test_corr_nd = df_test_corr.drop(df_test_corr[df_test_corr['Correlation Coefficient'] == 1.0].index)","c2bda0b3":"# Training set high correlations\ncorr = df_train_corr_nd['Correlation Coefficient'] > 0.1\ndf_train_corr_nd[corr]","8347ba0b":"# Test set high correlations\ncorr = df_test_corr_nd['Correlation Coefficient'] > 0.1\ndf_test_corr_nd[corr]","e1f8c8d9":"fig, axs = plt.subplots(nrows=2, figsize=(20, 20))\n\nsns.heatmap(df_train.drop(['PassengerId'], axis=1).corr(), ax=axs[0], annot=True, square=True, cmap='coolwarm', annot_kws={'size': 14})\nsns.heatmap(df_test.drop(['PassengerId'], axis=1).corr(), ax=axs[1], annot=True, square=True, cmap='coolwarm', annot_kws={'size': 14})\n\nfor i in range(2):    \n    axs[i].tick_params(axis='x', labelsize=14)\n    axs[i].tick_params(axis='y', labelsize=14)\n    \naxs[0].set_title('Training Set Correlations', size=15)\naxs[1].set_title('Test Set Correlations', size=15)\n\nplt.show()","05215f72":"cont_features = ['Age', 'Fare']\nsurv = df_train['Survived'] == 1\n\nfig, axs = plt.subplots(ncols=2, nrows=2, figsize=(20, 20))\nplt.subplots_adjust(right=1.5)\n\nfor i, feature in enumerate(cont_features):    \n    # Distribution of survival in feature\n    sns.distplot(df_train[~surv][feature], label='Not Survived', hist=True, color='#e74c3c', ax=axs[0][i])\n    sns.distplot(df_train[surv][feature], label='Survived', hist=True, color='#2ecc71', ax=axs[0][i])\n    \n    # Distribution of feature in dataset\n    sns.distplot(df_train[feature], label='Training Set', hist=False, color='#e74c3c', ax=axs[1][i])\n    sns.distplot(df_test[feature], label='Test Set', hist=False, color='#2ecc71', ax=axs[1][i])\n    \n    axs[0][i].set_xlabel('')\n    axs[1][i].set_xlabel('')\n    \n    for j in range(2):        \n        axs[i][j].tick_params(axis='x', labelsize=20)\n        axs[i][j].tick_params(axis='y', labelsize=20)\n    \n    axs[0][i].legend(loc='upper right', prop={'size': 20})\n    axs[1][i].legend(loc='upper right', prop={'size': 20})\n    axs[0][i].set_title('Distribution of Survival in {}'.format(feature), size=20, y=1.05)\n\naxs[1][0].set_title('Distribution of {} Feature'.format('Age'), size=20, y=1.05)\naxs[1][1].set_title('Distribution of {} Feature'.format('Fare'), size=20, y=1.05)\n        \nplt.show()","7f50e49a":"cat_features = ['Embarked', 'Parch', 'Pclass', 'Sex', 'SibSp', 'Deck']\n\nfig, axs = plt.subplots(ncols=2, nrows=3, figsize=(20, 20))\nplt.subplots_adjust(right=1.5, top=1.25)\n\nfor i, feature in enumerate(cat_features, 1):    \n    plt.subplot(2, 3, i)\n    sns.countplot(x=feature, hue='Survived', data=df_train)\n    \n    plt.xlabel('{}'.format(feature), size=20, labelpad=15)\n    plt.ylabel('Passenger Count', size=20, labelpad=15)    \n    plt.tick_params(axis='x', labelsize=20)\n    plt.tick_params(axis='y', labelsize=20)\n    \n    plt.legend(['Not Survived', 'Survived'], loc='upper center', prop={'size': 18})\n    plt.title('Count of Survival in {} Feature'.format(feature), size=20, y=1.05)\n\nplt.show()","a4fd2455":"df_all = concat_df(df_train, df_test)\ndf_all.head()","567cfb84":"df_all['Fare'] = pd.qcut(df_all['Fare'], 13)\n\nfig, axs = plt.subplots(figsize=(22, 9))\nsns.countplot(x='Fare', hue='Survived', data=df_all)\n\nplt.xlabel('Fare', size=15, labelpad=20)\nplt.ylabel('Passenger Count', size=15, labelpad=20)\nplt.tick_params(axis='x', labelsize=10)\nplt.tick_params(axis='y', labelsize=15)\n\nplt.legend(['Not Survived', 'Survived'], loc='upper right', prop={'size': 15})\nplt.title('Count of Survival in {} Feature'.format('Fare'), size=15, y=1.05)\n\nplt.show()","9e8243a3":"df_all['Age'] = pd.qcut(df_all['Age'], 10)\n\nfig, axs = plt.subplots(figsize=(22, 9))\nsns.countplot(x='Age', hue='Survived', data=df_all)\n\nplt.xlabel('Age', size=15, labelpad=20)\nplt.ylabel('Passenger Count', size=15, labelpad=20)\nplt.tick_params(axis='x', labelsize=15)\nplt.tick_params(axis='y', labelsize=15)\n\nplt.legend(['Not Survived', 'Survived'], loc='upper right', prop={'size': 15})\nplt.title('Survival Counts in {} Feature'.format('Age'), size=15, y=1.05)\n\nplt.show()","f9c8e31d":"df_all['Family_Size'] = df_all['SibSp'] + df_all['Parch'] + 1\n\nfig, axs = plt.subplots(figsize=(20, 20), ncols=2, nrows=2)\nplt.subplots_adjust(right=1.5)\n\nsns.barplot(x=df_all['Family_Size'].value_counts().index, y=df_all['Family_Size'].value_counts().values, ax=axs[0][0])\nsns.countplot(x='Family_Size', hue='Survived', data=df_all, ax=axs[0][1])\n\naxs[0][0].set_title('Family Size Feature Value Counts', size=20, y=1.05)\naxs[0][1].set_title('Survival Counts in Family Size ', size=20, y=1.05)\n\nfamily_map = {1: 'Alone', 2: 'Small', 3: 'Small', 4: 'Small', 5: 'Medium', 6: 'Medium', 7: 'Large', 8: 'Large', 11: 'Large'}\ndf_all['Family_Size_Grouped'] = df_all['Family_Size'].map(family_map)\n\nsns.barplot(x=df_all['Family_Size_Grouped'].value_counts().index, y=df_all['Family_Size_Grouped'].value_counts().values, ax=axs[1][0])\nsns.countplot(x='Family_Size_Grouped', hue='Survived', data=df_all, ax=axs[1][1])\n\naxs[1][0].set_title('Family Size Feature Value Counts After Grouping', size=20, y=1.05)\naxs[1][1].set_title('Survival Counts in Family Size After Grouping', size=20, y=1.05)\n\nfor i in range(2):\n    axs[i][1].legend(['Not Survived', 'Survived'], loc='upper right', prop={'size': 20})\n    for j in range(2):\n        axs[i][j].tick_params(axis='x', labelsize=20)\n        axs[i][j].tick_params(axis='y', labelsize=20)\n        axs[i][j].set_xlabel('')\n        axs[i][j].set_ylabel('')\n\nplt.show()","438cc219":"df_all['Ticket_Frequency'] = df_all.groupby('Ticket')['Ticket'].transform('count')\n\nfig, axs = plt.subplots(figsize=(12, 9))\nsns.countplot(x='Ticket_Frequency', hue='Survived', data=df_all)\n\nplt.xlabel('Ticket Frequency', size=15, labelpad=20)\nplt.ylabel('Passenger Count', size=15, labelpad=20)\nplt.tick_params(axis='x', labelsize=15)\nplt.tick_params(axis='y', labelsize=15)\n\nplt.legend(['Not Survived', 'Survived'], loc='upper right', prop={'size': 15})\nplt.title('Count of Survival in {} Feature'.format('Ticket Frequency'), size=15, y=1.05)\n\nplt.show()","db199b31":"df_all['Title'] = df_all['Name'].str.split(', ', expand=True)[1].str.split('.', expand=True)[0]\ndf_all['Is_Married'] = 0\ndf_all['Is_Married'].loc[df_all['Title'] == 'Mrs'] = 1\n\nfig, axs = plt.subplots(nrows=2, figsize=(20, 20))\nsns.barplot(x=df_all['Title'].value_counts().index, y=df_all['Title'].value_counts().values, ax=axs[0])\n\naxs[0].tick_params(axis='x', labelsize=10)\naxs[1].tick_params(axis='x', labelsize=15)\n\nfor i in range(2):    \n    axs[i].tick_params(axis='y', labelsize=15)\n\naxs[0].set_title('Title Feature Value Counts', size=20, y=1.05)\n\ndf_all['Title'] = df_all['Title'].replace(['Miss', 'Mrs','Ms', 'Mlle', 'Lady', 'Mme', 'the Countess', 'Dona'], 'Miss\/Mrs\/Ms')\ndf_all['Title'] = df_all['Title'].replace(['Dr', 'Col', 'Major', 'Jonkheer', 'Capt', 'Sir', 'Don', 'Rev'], 'Dr\/Military\/Noble\/Clergy')\n\nsns.barplot(x=df_all['Title'].value_counts().index, y=df_all['Title'].value_counts().values, ax=axs[1])\naxs[1].set_title('Title Feature Value Counts After Grouping', size=20, y=1.05)\n\nplt.show()","2ffd8089":"def extract_surname(data):    \n    \n    families = []\n    \n    for i in range(len(data)):        \n        name = data.iloc[i]\n\n        if '(' in name:\n            name_no_bracket = name.split('(')[0] \n        else:\n            name_no_bracket = name\n            \n        family = name_no_bracket.split(',')[0]\n        title = name_no_bracket.split(',')[1].strip().split(' ')[0]\n        \n        for c in string.punctuation:\n            family = family.replace(c, '').strip()\n            \n        families.append(family)\n            \n    return families\n\ndf_all['Family'] = extract_surname(df_all['Name'])\ndf_train = df_all.loc[:890]\ndf_test = df_all.loc[891:]\ndfs = [df_train, df_test]","aa9dc84f":"# Creating a list of families and tickets that are occuring in both training and test set\nnon_unique_families = [x for x in df_train['Family'].unique() if x in df_test['Family'].unique()]\nnon_unique_tickets = [x for x in df_train['Ticket'].unique() if x in df_test['Ticket'].unique()]\n\ndf_family_survival_rate = df_train.groupby('Family')['Survived', 'Family','Family_Size'].median()\ndf_ticket_survival_rate = df_train.groupby('Ticket')['Survived', 'Ticket','Ticket_Frequency'].median()\n\nfamily_rates = {}\nticket_rates = {}\n\nfor i in range(len(df_family_survival_rate)):\n    # Checking a family exists in both training and test set, and has members more than 1\n    if df_family_survival_rate.index[i] in non_unique_families and df_family_survival_rate.iloc[i, 1] > 1:\n        family_rates[df_family_survival_rate.index[i]] = df_family_survival_rate.iloc[i, 0]\n\nfor i in range(len(df_ticket_survival_rate)):\n    # Checking a ticket exists in both training and test set, and has members more than 1\n    if df_ticket_survival_rate.index[i] in non_unique_tickets and df_ticket_survival_rate.iloc[i, 1] > 1:\n        ticket_rates[df_ticket_survival_rate.index[i]] = df_ticket_survival_rate.iloc[i, 0]","e827360e":"mean_survival_rate = np.mean(df_train['Survived'])\n\ntrain_family_survival_rate = []\ntrain_family_survival_rate_NA = []\ntest_family_survival_rate = []\ntest_family_survival_rate_NA = []\n\nfor i in range(len(df_train)):\n    if df_train['Family'][i] in family_rates:\n        train_family_survival_rate.append(family_rates[df_train['Family'][i]])\n        train_family_survival_rate_NA.append(1)\n    else:\n        train_family_survival_rate.append(mean_survival_rate)\n        train_family_survival_rate_NA.append(0)\n        \nfor i in range(len(df_test)):\n    if df_test['Family'].iloc[i] in family_rates:\n        test_family_survival_rate.append(family_rates[df_test['Family'].iloc[i]])\n        test_family_survival_rate_NA.append(1)\n    else:\n        test_family_survival_rate.append(mean_survival_rate)\n        test_family_survival_rate_NA.append(0)\n        \ndf_train['Family_Survival_Rate'] = train_family_survival_rate\ndf_train['Family_Survival_Rate_NA'] = train_family_survival_rate_NA\ndf_test['Family_Survival_Rate'] = test_family_survival_rate\ndf_test['Family_Survival_Rate_NA'] = test_family_survival_rate_NA\n\ntrain_ticket_survival_rate = []\ntrain_ticket_survival_rate_NA = []\ntest_ticket_survival_rate = []\ntest_ticket_survival_rate_NA = []\n\nfor i in range(len(df_train)):\n    if df_train['Ticket'][i] in ticket_rates:\n        train_ticket_survival_rate.append(ticket_rates[df_train['Ticket'][i]])\n        train_ticket_survival_rate_NA.append(1)\n    else:\n        train_ticket_survival_rate.append(mean_survival_rate)\n        train_ticket_survival_rate_NA.append(0)\n        \nfor i in range(len(df_test)):\n    if df_test['Ticket'].iloc[i] in ticket_rates:\n        test_ticket_survival_rate.append(ticket_rates[df_test['Ticket'].iloc[i]])\n        test_ticket_survival_rate_NA.append(1)\n    else:\n        test_ticket_survival_rate.append(mean_survival_rate)\n        test_ticket_survival_rate_NA.append(0)\n        \ndf_train['Ticket_Survival_Rate'] = train_ticket_survival_rate\ndf_train['Ticket_Survival_Rate_NA'] = train_ticket_survival_rate_NA\ndf_test['Ticket_Survival_Rate'] = test_ticket_survival_rate\ndf_test['Ticket_Survival_Rate_NA'] = test_ticket_survival_rate_NA","91e306ee":"for df in [df_train, df_test]:\n    df['Survival_Rate'] = (df['Ticket_Survival_Rate'] + df['Family_Survival_Rate']) \/ 2\n    df['Survival_Rate_NA'] = (df['Ticket_Survival_Rate_NA'] + df['Family_Survival_Rate_NA']) \/ 2    ","3104d3ba":"non_numeric_features = ['Embarked', 'Sex', 'Deck', 'Title', 'Family_Size_Grouped', 'Age', 'Fare']\n\nfor df in dfs:\n    for feature in non_numeric_features:        \n        df[feature] = LabelEncoder().fit_transform(df[feature])","d4ced403":"cat_features = ['Pclass', 'Sex', 'Deck', 'Embarked', 'Title', 'Family_Size_Grouped']\nencoded_features = []\n\nfor df in dfs:\n    for feature in cat_features:\n        encoded_feat = OneHotEncoder().fit_transform(df[feature].values.reshape(-1, 1)).toarray()\n        n = df[feature].nunique()\n        cols = ['{}_{}'.format(feature, n) for n in range(1, n + 1)]\n        encoded_df = pd.DataFrame(encoded_feat, columns=cols)\n        encoded_df.index = df.index\n        encoded_features.append(encoded_df)\n\ndf_train = pd.concat([df_train, *encoded_features[:6]], axis=1)\ndf_test = pd.concat([df_test, *encoded_features[6:]], axis=1)","c616b0d9":"df_all = concat_df(df_train, df_test)\ndrop_cols = ['Deck', 'Embarked', 'Family', 'Family_Size', 'Family_Size_Grouped', 'Survived',\n             'Name', 'Parch', 'PassengerId', 'Pclass', 'Sex', 'SibSp', 'Ticket', 'Title',\n            'Ticket_Survival_Rate', 'Family_Survival_Rate', 'Ticket_Survival_Rate_NA', 'Family_Survival_Rate_NA']\n\ndf_all.drop(columns=drop_cols, inplace=True)\n\ndf_all.head()","8471a605":"feature_stores = df_all.columns.values\nFEATURES = feature_stores\nFEATURES","db3ac8c0":"X_train = StandardScaler().fit_transform(df_train.drop(columns=drop_cols))\ny_train = df_train['Survived'].values\nX_test = StandardScaler().fit_transform(df_test.drop(columns=drop_cols))\n\nprint('X_train shape: {}'.format(X_train.shape))\nprint('y_train shape: {}'.format(y_train.shape))\nprint('X_test shape: {}'.format(X_test.shape))\n# Justin taking over\n# back to pandas dataframes\nX_train = pd.DataFrame(X_train)\ny_train = pd.DataFrame(y_train)\nX_test = pd.DataFrame(X_test)\n# write back the columns\n\nX_train.columns = feature_stores\nX_test.columns = feature_stores\nX_train.head()","901d5ab1":"# visualize feature importance\n#correlation heatmap of dataset\ndef correlation_heatmap(df):\n    _ , ax = plt.subplots(figsize =(14, 12))\n    colormap = sns.diverging_palette(220, 10, as_cmap = True)\n    \n    _ = sns.heatmap(\n        df.corr(), \n        cmap = colormap,\n        square=True, \n        cbar_kws={'shrink':.9 }, \n        ax=ax,\n        annot=True, \n        linewidths=0.1,vmax=1.0, linecolor='white',\n        annot_kws={'fontsize':12 }\n    )\n    \n    plt.title('Pearson Correlation of Features', y=1.05, size=15)\n\ncorrelation_heatmap(df_train)\ntmp = X_train \ntmp[\"Survived\"] = y_train\n\n\ncorrs = pd.DataFrame(tmp.corr()[\"Survived\"].sort_values(ascending=False))\n\ntop20 = corrs.loc[corrs[\"Survived\"] > 0.2]\n\ncorrs\n\n\n","fc53db6b":"# imports for later\n#Common Model Algorithms\nfrom sklearn import svm, tree, linear_model, neighbors, naive_bayes, ensemble, discriminant_analysis, gaussian_process\nfrom xgboost import XGBClassifier\n\n#Common Model Helpers\nfrom sklearn.preprocessing import OneHotEncoder, LabelEncoder\nfrom sklearn import feature_selection\nfrom sklearn import model_selection\nfrom sklearn import metrics","7bc9e95b":"# The first two models are taken from the forked notebook. \"Leaderboard_model\" are already \n# the settings deducted from the experiments below.\n\nsingle_best_model = RandomForestClassifier(criterion='gini', \n                                           n_estimators=1100,\n                                           max_depth=5,\n                                           min_samples_split=4,\n                                           min_samples_leaf=5,\n                                           max_features='auto',\n                                           oob_score=True,\n                                           random_state=SEED,\n                                           n_jobs=-1,\n                                           verbose=1)\n\nold_leaderboard_model = RandomForestClassifier(criterion='gini',\n                                           n_estimators=1750,\n                                           max_depth=7,\n                                           min_samples_split=6,\n                                           min_samples_leaf=6,\n                                           max_features='auto',\n                                           oob_score=True,\n                                           random_state=SEED,\n                                           n_jobs=-1,\n                                           verbose=0) \n\nleaderboard_model = RandomForestClassifier(criterion='gini',\n                                           n_estimators=20,\n                                           max_depth=3,\n                                           min_samples_split=2,\n                                           min_samples_leaf=1,\n                                           max_features='auto',\n                                           oob_score=True,\n                                           random_state=42,\n                                           n_jobs=-1,\n                                           verbose=0) \n\n\n\n#prepare to split dataset in cross-validation with this splitter class: http:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.model_selection.ShuffleSplit.html#sklearn.model_selection.ShuffleSplit\n#note: this is an alternative to train_test_split\ncv_split = model_selection.ShuffleSplit(n_splits = 10, test_size = .3, train_size = .6, random_state = 0 ) # run model 10x with 60\/30 split intentionally leaving out 10%","c66758c2":"## DISABLED FOR NOW BECAUSE IT TAKES TOO MUCH TIME\n\n# feature selection\n\n# takes quite some time, so maybe choose a smaller nr of trees or run only if you have the time\n#basem = ensemble.RandomForestClassifier()\n# again added return_train_score\n# MODEL = leaderboard_model\n# base_results = model_selection.cross_validate(MODEL, X_train[FEATURES], y_train, cv  = cv_split,return_train_score=True )\n# MODEL.fit(X_train[FEATURES], y_train)\n\n# print(FEATURES.shape)\n# print('BEFORE DT RFE Training Columns Old: ', FEATURES)\n# print(\"BEFORE DT RFE Training w\/bin score mean: {:.2f}\". format(base_results['train_score'].mean()*100)) \n# print(\"BEFORE DT RFE Test w\/bin score mean: {:.2f}\". format(base_results['test_score'].mean()*100))\n# print(\"BEFORE DT RFE Test w\/bin score 3*std: +\/- {:.2f}\". format(base_results['test_score'].std()*100*3))\n# print('-'*10)\n\n\n\n# # #feature selection\n# MODEL = feature_selection.RFECV(MODEL, step = 1, scoring = 'accuracy', cv = cv_split)\n# MODEL.fit(X_train, y_train)\n\n# # #transform x&y to reduced features and fit new model\n# # #alternative: can use pipeline to reduce fit and transform steps: http:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.pipeline.Pipeline.html\n# X_rfe = X_train.columns.values[MODEL.get_support()]\n# rfe_results = model_selection.cross_validate(MODEL, X_train, y_train, cv  = cv_split,return_train_score=True)\n\n# print(X_rfe.shape)\n# print('AFTER DT RFE Training Columns New: ', X_rfe)\n\n# print(\"AFTER DT RFE Training w\/bin score mean: {:.2f}\". format(rfe_results['train_score'].mean()*100)) \n# print(\"AFTER DT RFE Test w\/bin score mean: {:.2f}\". format(rfe_results['test_score'].mean()*100))\n# print(\"AFTER DT RFE Test w\/bin score 3*std: +\/- {:.2f}\". format(rfe_results['test_score'].std()*100*3))\n# print('-'*10)\n\n# # AFTER DT RFE Training Columns New:  np.array(['Age', 'Deck_1', 'Deck_2', 'Deck_3', 'Deck_4', 'Embarked_1', 'Embarked_2'\n#  'Embarked_3', 'Family_Siiize_Grouped_1', 'Family_Size_Grouped_2'\n#  'Family_Size_Grouped_3', 'Family_Size_Grouped_4', 'Fare', 'Pclass_1'\n#  'Pclass_2', 'Sex_1', 'Sex_2', 'Survival_Rate', 'Survival_Rate_NA'\n#  'Ticket_Frequency', 'Title_1', 'Title_3', 'Title_4'])","ca0f41a1":"X_rfe = None # uncomment if you are executing the code above","e06cabb0":"# # implementation with pipeline\n# # way way faster than the version above\n# # pure copypaste https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.pipeline.Pipeline.html\n\n# from sklearn.datasets import samples_generator\n# from sklearn.feature_selection import SelectKBest\n# from sklearn.feature_selection import f_regression\n# from sklearn.pipeline import Pipeline\n\n# MODEL = leaderboard_model\n\n# m_ensemble = ('rfc',MODEL)\n# # ANOVA \n# anova_filter = SelectKBest(f_regression, k=10)\n# clf = MODEL\n# anova_rfc = Pipeline([('anova', anova_filter), m_ensemble])\n# # You can set the parameters using the names issued\n# # For instance, fit using a k of 10 in the SelectKBest\n# anova_rfc.set_params(anova__k=10).fit(X_train[FEATURES], y_train)\n\n# prediction = anova_rfc.predict(X_train[FEATURES])\n# an_score = anova_rfc.score(X_train[FEATURES], y_train)  \n# # getting the selected features chosen by anova_filter\n# new_features = anova_rfc['anova'].get_support()\n# del_features = ~new_features\n\n# print(\"score: %f, delted features: %s\" % (an_score,FEATURES[del_features]))\n# print(\"New features: %s\" % FEATURES[new_features])\n# FEATURES_pipeline = FEATURES[new_features]","9b4b2dd5":"FEATURES_pipeline = np.array(['Deck_1', 'Deck_4', 'Embarked_2', 'Family_Size_Grouped_1'\n 'Family_Size_Grouped_3', 'Fare', 'Pclass_1', 'Survival_Rate_NA'\n 'Ticket_Frequency', 'Title_4'])","c0adebcf":"myrun_bestresults = np.array(['Age', 'Deck_1', 'Deck_2', 'Deck_4', 'Embarked_1', 'Embarked_2',\n       'Embarked_3', 'Family_Size_Grouped_1', 'Family_Size_Grouped_3',\n       'Family_Size_Grouped_4', 'Fare', 'Is_Married', 'Pclass_1',\n       'Pclass_2', 'Pclass_3', 'Sex_1', 'Survival_Rate',\n       'Survival_Rate_NA', 'Ticket_Frequency', 'Title_2', 'Title_3',\n       'Title_4'])","17ed0ac0":"# set features to the newly discovered features if you want to use them, if you want to stay with the old set\n# just uncomment \nchoose = [FEATURES,X_rfe,FEATURES_pipeline,myrun_bestresults]\n# 0 = default features\n# 1 = Version 1\n# 2 = Version 2 (fast)\n# 3 = My calculated results with the above algorithms\nFEATURES = choose[3]","f42b8f16":"#Machine Learning Algorithm (MLA) Selection and Initialization\nMLA = [\n    #Ensemble Methods\n    ensemble.AdaBoostClassifier(),\n    ensemble.BaggingClassifier(),\n    ensemble.ExtraTreesClassifier(),\n    ensemble.GradientBoostingClassifier(),\n    leaderboard_model, # our model\n\n    #Gaussian Processes\n    gaussian_process.GaussianProcessClassifier(),\n    \n    #GLM\n    linear_model.LogisticRegressionCV(),\n    linear_model.PassiveAggressiveClassifier(),\n    linear_model.RidgeClassifierCV(),\n    linear_model.SGDClassifier(),\n    linear_model.Perceptron(),\n    \n    #Navies Bayes\n    naive_bayes.BernoulliNB(),\n    naive_bayes.GaussianNB(),\n    \n    #Nearest Neighbor\n    neighbors.KNeighborsClassifier(),\n    \n    #SVM\n    svm.SVC(probability=True),\n    svm.NuSVC(probability=True),\n    svm.LinearSVC(),\n    \n    #Trees    \n    tree.DecisionTreeClassifier(),\n    tree.ExtraTreeClassifier(),\n    \n    #Discriminant Analysis\n    discriminant_analysis.LinearDiscriminantAnalysis(),\n    discriminant_analysis.QuadraticDiscriminantAnalysis(),\n\n    \n    #xgboost: http:\/\/xgboost.readthedocs.io\/en\/latest\/model.html\n    XGBClassifier()    \n    ]\n\n#create table to compare MLA metrics\nMLA_columns = ['MLA Name', 'MLA Parameters','MLA Train Accuracy Mean', 'MLA Test Accuracy Mean', 'MLA Test Accuracy 3*STD' ,'MLA Time']\nMLA_compare = pd.DataFrame(columns = MLA_columns)\n\n#create table to compare MLA predictions\nMLA_predict = pd.DataFrame()\n\n#index through MLA and save performance to table\nrow_index = 0\nfor alg in MLA:\n\n    #set name and parameters\n    MLA_name = alg.__class__.__name__\n    MLA_compare.loc[row_index, 'MLA Name'] = MLA_name\n    MLA_compare.loc[row_index, 'MLA Parameters'] = str(alg.get_params())\n    \n    #score model with cross validation: http:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.model_selection.cross_validate.html#sklearn.model_selection.cross_validate\n    # remark: need to state return_train_score explicitely\n    cv_results = model_selection.cross_validate(alg, X_train[FEATURES], y_train, cv  = cv_split, return_train_score=True)\n    MLA_compare.loc[row_index, 'MLA Time'] = cv_results['fit_time'].mean()\n    MLA_compare.loc[row_index, 'MLA Train Accuracy Mean'] = cv_results['train_score'].mean()\n    MLA_compare.loc[row_index, 'MLA Test Accuracy Mean'] = cv_results['test_score'].mean()   \n    #if this is a non-bias random sample, then +\/-3 standard deviations (std) from the mean, should statistically capture 99.7% of the subsets\n    MLA_compare.loc[row_index, 'MLA Test Accuracy 3*STD'] = cv_results['test_score'].std()*3   #let's know the worst that can happen!\n    \n\n    #save MLA predictions - see section 6 for usage\n    alg.fit( X_train[FEATURES], y_train)\n    MLA_predict[MLA_name] = alg.predict( X_train[FEATURES])\n    \n    row_index+=1\n\n    \n#print and sort table: https:\/\/pandas.pydata.org\/pandas-docs\/stable\/generated\/pandas.DataFrame.sort_values.html\nMLA_compare.sort_values(by = ['MLA Test Accuracy Mean'], ascending = False, inplace = True)\nMLA_compare\n#MLA_predict\n\n","d2f6d064":"#barplot using https:\/\/seaborn.pydata.org\/generated\/seaborn.barplot.html\nsns.barplot(x='MLA Test Accuracy Mean', y = 'MLA Name', data = MLA_compare, color = 'm')\n\n#prettify using pyplot: https:\/\/matplotlib.org\/api\/pyplot_api.html\nplt.title('Machine Learning Algorithm Accuracy Score \\n')\nplt.xlabel('Accuracy Score (%)')\nplt.ylabel('Algorithm')","98f8b31b":"#compare algorithm predictions with each other, where 1 = exactly similar and 0 = exactly opposite\n#there are some 1's, but enough blues and light reds to create a \"super algorithm\" by combining them\ncorrelation_heatmap(MLA_predict)\n","eb01e22f":"# test different parameters\n\n# ATTENTION: Really really costly, it cost me ~8 hours\n# Output:\n# The best parameter for RandomForestClassifier is {'criterion': 'gini', 'max_depth': 3, \n#  'min_samples_leaf': 1, 'min_samples_split': 2, \n#  'n_estimators': 20, 'n_jobs': -1, 'oob_score': True, 'random_state': 42}\n# Total optimization time was 477.18 minutes.\n# ----------\n#               RandomForestClassifier - http:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.ensemble.RandomForestClassifier.html#sklearn.ensemble.RandomForestClassifier\n#             'n_estimators': [20],\n#             'criterion': [\"gini\"],\n#             'max_depth': [3], \n#             'min_samples_split': [2],\n#             'min_samples_leaf': [1],\n#             'oob_score': [True],\n#             'random_state': [42],\n#             'n_jobs': [-1]\n\n\nimport time\ngrid_n_estimator = [250,500,1000, 10, 20, 40,50, 100, 1750, 2000, 1500] #10, 50, 100, 300, 500, 750, 1000, 1250, 1500, 1750, 2000\ngrid_ratio = [.1, .25, .5, .75, 1.0]\ngrid_learn = [.01, .03, .05, 0.75,.1,1.25, .25]\ngrid_max_depth = [1,2,3, 4,5,6,7,8,9,10] # None\ngrid_min_samples = [5, 10, .03, .05, .10]\ngrid_criterion = ['gini', 'entropy']\ngrid_bool = [True, False]\ngrid_seed = [0]\nmin_sample_split = [6,4,8]\nmin_sample_leaf = [6,4,8]\nSEED=np.arange(1,100,1)\n\n\nvote_est = [      \n    ('rfc', ensemble.RandomForestClassifier()) ,\n#     ('gbc', ensemble.GradientBoostingClassifier()),\n#     ('xgb', XGBClassifier()),\n]                          \n\ngrid_param = [\n#     [{\n#             #GradientBoostingClassifier - http:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.ensemble.GradientBoostingClassifier.html#sklearn.ensemble.GradientBoostingClassifier\n#             'loss': ['deviance'],   #'exponential'], #default=\u2019deviance\u2019\n#             'learning_rate': grid_learn, #grid_learn, #default=0.1 -- 12\/31\/17 set to reduce runtime -- The best parameter for GradientBoostingClassifier is {'learning_rate': 0.05, 'max_depth': 2, 'n_estimators': 300, 'random_state': 0} with a runtime of 264.45 seconds.\n#             'n_estimators': [95], #default=100 -- 12\/31\/17 set to reduce runtime -- The best parameter for GradientBoostingClassifier is {'learning_rate': 0.05, 'max_depth': 2, 'n_estimators': 300, 'random_state': 0} with a runtime of 264.45 seconds.\n#             'criterion': ['friedman_mse'],     #, 'mse', 'mae'], #default=\u201dfriedman_mse\u201d\n#             'max_depth': [3], #default=3   \n#             'random_state': grid_seed\n#              }],\n            [{\n                #RandomForestClassifier - http:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.ensemble.RandomForestClassifier.html#sklearn.ensemble.RandomForestClassifier\n            'n_estimators': [20], #default=10\n            'criterion': [\"gini\"], #default=\u201dgini\u201d\n            'max_depth': [3], #default=Nonie\n            'min_samples_split': [2],\n            'min_samples_leaf': [1],\n            'oob_score': [True], #default=False -- 12\/31\/17 set to reduce runtime -- The best parameter for RandomForestClassifier is {'criterion': 'entropy', 'max_depth': 6, 'n_estimators': 100, 'oob_score': True, 'random_state': 0} with a runtime of 146.35 seconds.\n            'random_state': [42],\n            'n_jobs': [-1]\n             }]\n#      [{\n#             #XGBClassifier - http:\/\/xgboost.readthedocs.io\/en\/latest\/parameter.html\n#             'learning_rate': grid_learn, #default: .3\n#             'max_depth': [1,2,4,6,8,10], #default 2\n#             'n_estimators': grid_n_estimator, \n#             'seed': grid_seed  \n#              }]  \n    \n]\n    \n\nstart_total = time.perf_counter() #https:\/\/docs.python.org\/3\/library\/time.html#time.perf_counter\nfor clf, param in zip (vote_est, grid_param): #https:\/\/docs.python.org\/3\/library\/functions.html#zip\n\n    #print(clf[1]) #vote_est is a list of tuples, index 0 is the name and index 1 is the algorithm\n    #print(param)\n    \n    \n    start = time.perf_counter()        \n    best_search = model_selection.GridSearchCV(estimator = clf[1], param_grid = param, cv = cv_split, scoring = 'roc_auc')\n    best_search.fit(X_train[FEATURES], y_train)\n    run = time.perf_counter() - start\n\n    best_param = best_search.best_params_\n    print('The best parameter for {} is {} with a runtime of {:.2f} seconds.'.format(clf[1].__class__.__name__, best_param, run))\n    clf[1].set_params(**best_param) \n\n\nrun_total = time.perf_counter() - start_total\nprint('Total optimization time was {:.2f} minutes.'.format(run_total\/60))\n\nprint('-'*10)","5499c9d2":"#Hard Vote or majority rules w\/Tuned Hyperparameters\ngrid_hard = ensemble.VotingClassifier(estimators = vote_est , voting = 'hard')\ngrid_hard_cv = model_selection.cross_validate(grid_hard, X_train[FEATURES], y_train, cv  = cv_split, return_train_score=True)\ngrid_hard.fit(X_train[FEATURES], y_train)\n\nprint(\"Hard Voting w\/Tuned Hyperparameters Training w\/bin score mean: {:.2f}\". format(grid_hard_cv['train_score'].mean()*100)) \nprint(\"Hard Voting w\/Tuned Hyperparameters Test w\/bin score mean: {:.2f}\". format(grid_hard_cv['test_score'].mean()*100))\nprint(\"Hard Voting w\/Tuned Hyperparameters Test w\/bin score 3*std: +\/- {:.2f}\". format(grid_hard_cv['test_score'].std()*100*3))\nprint('-'*10)\n\n#Soft Vote or weighted probabilities w\/Tuned Hyperparameters\ngrid_soft = ensemble.VotingClassifier(estimators = vote_est , voting = 'soft')\ngrid_soft_cv = model_selection.cross_validate(grid_soft, X_train[FEATURES], y_train, cv  = cv_split, return_train_score=True)\ngrid_soft.fit(X_train[FEATURES], y_train)\n\nprint(\"Soft Voting w\/Tuned Hyperparameters Training w\/bin score mean: {:.2f}\". format(grid_soft_cv['train_score'].mean()*100)) \nprint(\"Soft Voting w\/Tuned Hyperparameters Test w\/bin score mean: {:.2f}\". format(grid_soft_cv['test_score'].mean()*100))\nprint(\"Soft Voting w\/Tuned Hyperparameters Test w\/bin score 3*std: +\/- {:.2f}\". format(grid_soft_cv['test_score'].std()*100*3))\nprint('-'*10)\n","63eeb436":"submission_df = pd.DataFrame(columns=['PassengerId', 'Survived'])\nsubmission_df['PassengerId'] = df_test['PassengerId']\nsubmission_df['Survived'] = grid_hard.predict(X_test[FEATURES])\n\n# submission doesnt like float\nsubmission_df[\"Survived\"] = submission_df[\"Survived\"].astype(int)\n\nsubmission_df.to_csv('submission.csv', header=True, index=False)\n\n\nsubmission_df.head(10)","c3a01d4d":"Unfortunately the above throws an error with Kaggle's Sklearn version. Anyways, an example output is for example:\n`\nscore: 0.860831, delted features: ['Age' 'Deck_2' 'Embarked_1' 'Embarked_3' 'Family_Size_Grouped_4'\n 'Is_Married' 'Pclass_2' 'Pclass_3' 'Sex_1' 'Survival_Rate' 'Title_2'\n 'Title_3']\nNew features: ['Deck_1' 'Deck_4' 'Embarked_2' 'Family_Size_Grouped_1'\n 'Family_Size_Grouped_3' 'Fare' 'Pclass_1' 'Survival_Rate_NA'\n 'Ticket_Frequency' 'Title_4']\n `","cecef528":"### Let's have a look at the correlations again to estimate which features we might be able to replace. \nIf two features correlate with each other we might be able to remove one","2b640f7a":"### Which are the best algorithms for our use case?","758b96ac":"#### Version 1: transforming x&y","358847f3":"# Model tuning with GridCV\n\nNow GridCV is a nice tool to quickly test different parameters of algorithms. My recommendation is to set most of the values to a fixed value, because testing too many settings at once results in a huge calculation time","de724e5d":"# **Advanced Feature Engineering Tutorial with Titanic**","ff1762ff":"### Choosing the right algorithms\n\nNow comes the hard part. It would be easy to think that the best results are done by just choosing the best performing algorithms. The problem is they are very likely just overfitting on our data, meaning briefly explained, that they may show high results on our test data, but will perform bad with the \"unseen\" test data. \n\nHave a look at \"LD Freeman's\" Notebook for a deeper analysis regarding model selection: https:\/\/www.kaggle.com\/ldfreeman3\/a-data-science-framework-to-achieve-99-accuracy\n\nNow a broad explanation to find the best models would be to choose some that are showing a high prediction value and that are also not too similar. \n\nE.g. \nDecisionTreeClassifier and BaggingClassifier both show high values, but correlate to each other quite much, meaning they are almost the same and not the best choice for a voting algorithm. \n\nFor now, as mentioned above, I will just proceed with the RandomForest Classifier because it showed the best results","864b5745":"* > ## **3. Machine Learning, Justin's Additions**","3e6bd64f":"# Introduction\n\n\n\nHey there, I'm happy to join the Kaggle community!\n\nTLDR: I improved the score of 0.8110047846889952 to 0.8181818181818182 and greatly improved the runtime\n\nFirst things first: This notebook is based on the amazing work of \"Nida G\u00fcler\", https:\/\/www.kaggle.com\/nidaguler\/advanced-feature-engineering-tutorial-with-titanic and contains parts of\n\"LD Freeman\" (https:\/\/www.kaggle.com\/ldfreeman3\/a-data-science-framework-to-achieve-99-accuracy) \n\nThis is why I just removed the exploratory stuff, head over to https:\/\/www.kaggle.com\/nidaguler\/advanced-feature-engineering-tutorial-with-titanic  to see how this has been concluded. My work starts form the \"Machine Learning\" section 3.\n\n## Content\n1. Preparation - Head to Nida G\u00fcler's Notebook for explanations\n2. My Additions (3. Machine learning)\n\n## What did I change?\n\nThe features were already good, I tried to add own Features, but it did not improve the score. I noticed that the \"leaderboard_model\" takes quite some time to execute, which is why I decided to start a GridsearchCV to find possible alternatives to the algorithm, or features that are quicker yielding the same results. \n\nThe next thing is that there were not really any features selected. Usually models perform better the less features there are, which is why I did an analysis of possible features to drop. \n\nSummarized my methods resulted in a shorter feature selection \n\n`['Age', 'Deck_1', 'Deck_2', 'Deck_4', 'Embarked_1', 'Embarked_2',\n       'Embarked_3', 'Family_Size_Grouped_1', 'Family_Size_Grouped_3',\n       'Family_Size_Grouped_4', 'Fare', 'Is_Married', 'Pclass_1',\n       'Pclass_2', 'Pclass_3', 'Sex_1', 'Survival_Rate',\n       'Survival_Rate_NA', 'Ticket_Frequency', 'Title_2', 'Title_3',\n       'Title_4']`\n\n[](http:\/\/)And a way faster algorithm, mostly due to reducing the nr of estimators (formerly 1750, now just 20) and reducing the max_depth\n\n`\n     RandomForestClassifier(criterion='gini',\n\t                                           n_estimators=20,\n\t                                           max_depth=3,\n\t                                           min_samples_split=2,\n\t                                           min_samples_leaf=1,\n\t                                           max_features='auto',\n\t                                           oob_score=True,\n\t                                           random_state=SEED, # 42\n\t                                           n_jobs=-1,\n\t                                           verbose=0)\n`\n\n## Contact\n\nFeel free to add further suggestions, I did not manage to get past ~0.82. I guess the algorithm is at it's max, the only addition would be in the form of additional features. Any ideas?\nhttps:\/\/www.linkedin.com\/in\/justin-guese\/\n","0fc3a57f":"#### Version 2: Implementation with Pipeline (faster)","93768d2a":"### Feature reduction\n\nBelow you will find two ways to reduce features. The second version performs way faster, but was in my opinion less accurate","2fb8b466":"# Submission\n\nUse the voting results (soft or hard) to calculate the survival score","68057bcd":"## Model optimization\n\nNow a good approach would be to use several models and to use a voting functionality, but at least in my results the rfc could not be outperformed and just made the results worse. This is why I will just use one model, feel free to submit some guesses and tipps;)"}}