{"cell_type":{"699ef0d9":"code","fd19b916":"code","8b58b24e":"code","d2c69b26":"code","14434539":"code","d81b842a":"code","e5eb79be":"code","38a94500":"code","36ef51a4":"code","8c530ff2":"code","b799cc0a":"code","4e53e23c":"code","d9ef5eba":"code","c8a73efc":"code","622c0386":"code","6928c5dc":"code","114401c0":"code","a09c40ad":"code","333cba0a":"code","b26c351d":"code","f67f7e41":"code","7d1820c9":"code","d93cbf3a":"code","91f413fe":"code","10215355":"code","ff5f7a70":"code","3b82fea2":"code","ac644ea3":"code","d0025e6f":"code","24548dcd":"code","620c7356":"code","293d45b2":"code","101db3ca":"code","6ab65962":"code","42ecee02":"code","a5a8ddc2":"code","26b918ea":"code","94cc7542":"code","99562855":"code","acc7ae26":"code","32122d45":"markdown","70fc3611":"markdown","d459192d":"markdown","4e1e66b3":"markdown","8c2ca9d8":"markdown","6bea4b02":"markdown","53dcad96":"markdown","6d269d62":"markdown","d713f40e":"markdown","6a51580e":"markdown","342d8d4e":"markdown","a551b52c":"markdown","926ce283":"markdown","6517b981":"markdown","e1052593":"markdown","f1e4c4fd":"markdown","36e82679":"markdown","4551bf3c":"markdown","5ba4c411":"markdown","49c14eab":"markdown","c2291342":"markdown","6f61ad4f":"markdown","4ef0ce1e":"markdown","17a4fb5a":"markdown","fc2cdcf0":"markdown","a1a280d6":"markdown"},"source":{"699ef0d9":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","fd19b916":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly.express as px\n\n\nfrom sklearn.model_selection import StratifiedKFold \n# It means that you want to import and use layered K-Fold cross-validation.\n#(\uacc4\uce35\ud654\ub41c K-Fold \uad50\ucc28 \uac80\uc99d\uc744 import \ud574\uc11c \uc0ac\uc6a9\ud558\uaca0\ub2e4\ub294 \uc758\ubbf8 )\n\nfrom sklearn.preprocessing import StandardScaler , LabelEncoder \n# StandardScaler: Standardizes features by removing the mean and adjusting for unit variance.\n# (StandardScaler : \ud3c9\uade0\uc744 \uc81c\uac70\ud558\uace0 \ub2e8\uc704 \ubd84\uc0b0\uc5d0 \ub9de\uac8c \uc870\uc815\ud558\uc5ec \uae30\ub2a5\uc744 \ud45c\uc900\ud654.)\n# LabelEncoder : Encodes the target label with a value between 0 and n_classes-1.\n# (LabelEncoder : 0\uacfc n_classes-1 \uc0ac\uc774\uc758 \uac12\uc73c\ub85c \ub300\uc0c1 label\uc744 \uc778\ucf54\ub529.)\n\nfrom sklearn.metrics import accuracy_score    \n# accuracy_score: Indicates the accuracy classification score.\n#(accuracy_score: \uc815\ud655\ub3c4 \ubd84\ub958 \uc810\uc218\ub97c \uc758\ubbf8\ud55c\ub2e4.)\n\nfrom sklearn.model_selection import train_test_split\nfrom scipy.stats import mode # Meaning to return an array of modal (most common) values from the passed array.\n                             #(\uc804\ub2ec\ub41c \ubc30\uc5f4\uc5d0\uc11c \ubaa8\ub2ec(\uac00\uc7a5 \uc77c\ubc18\uc801\uc778) \uac12\uc758 \ubc30\uc5f4\uc744 \ubc18\ud658\ud55c\ub2e4\ub294 \uc758\ubbf8.)\n\n\nfrom lightgbm import LGBMClassifier\n\n\nfrom matplotlib import ticker\nimport time\nimport warnings\npd.set_option('display.max_rows', None)\npd.set_option('display.max_columns', None)\npd.set_option('float_format', '{:f}'.format)\nwarnings.filterwarnings('ignore')","8b58b24e":"train = pd.read_csv(\"..\/input\/tabular-playground-series-dec-2021\/train.csv\")\ntest = pd.read_csv(\"..\/input\/tabular-playground-series-dec-2021\/test.csv\")\nsubmission = pd.read_csv(\"..\/input\/tabular-playground-series-dec-2021\/sample_submission.csv\")\n\n\ntrain.drop([\"Id\"] , axis = 1 , inplace = True) \n# If 'drop' is used, the name of the specified column is deleted from \n# the row or column, and 'axis = 1' is used to designate the column. \n#(If '0', the row is designated.)\n#('drop'\uc744 \uc0ac\uc6a9\ud558\uba74 \ud589\uc774\ub098 \uc5f4\uc5d0\uc11c \uc9c0\uc815\ub41c \uceec\ub7fc\uc758 \uc774\ub984\uc744 \uc0ad\uc81c, 'axis = 1'\uc740 \uc5f4\uc744 \uc9c0\uc815\ud558\ub294 \uc5ed\ud560\uc744 \ud55c\ub2e4.('0'\uc774\uba74 \ud589\uc744 \uc9c0\uc815\ud55c\ub2e4.))\n\ntest.drop([\"Id\"] , axis = 1 , inplace = True)\nTARGET = 'Cover_Type'  \nFEATURES = [col for col in train.columns if col not in ['id', TARGET]]\n# 'List comprehension' is one of the ways to initialize a list, and it is used to initialize a list called 'FEATURES'.\n# ('\ub9ac\uc2a4\ud2b8 \ucef4\ud50c\ub9ac\ud5e8\uc158'\uc740 \ub9ac\uc2a4\ud2b8\ub97c \ucd08\uae30\ud654 \ud558\ub294 \ubc29\ubc95 \uc911 \ud558\ub098\uc778\ub370 \uc774\uac78 \uc0ac\uc6a9\ud574\uc11c 'FEATURES'\ub77c\ub294 \ub9ac\uc2a4\ud2b8\ub97c \ucd08\uae30\ud654 \ud574\uc90c.)\n\nRANDOM_STATE = 12 ","d2c69b26":"train.head() # Use head() to output only the top 5 data in index order.\n             # (head() \ub97c \uc0ac\uc6a9\ud558\uc5ec \uc778\ub371\uc2a4 \uc21c\uc73c\ub85c \uc0c1\uc704 5\uac1c\uc758 \ub370\uc774\ud130\ub9cc \ucd9c\ub825.)","14434539":"print(f'Number of rows in train data: {train.shape[0]}') \n# If f is used, 'Number of rows in test data:' is output as characters, \n# and if '.shape' is used, the total number of columns or rows is returned.\n#(f\ub97c \uc0ac\uc6a9\ud558\uba74 'Number of rows in test data:'\ub97c \ubb38\uc790\ub85c \ucd9c\ub825\ud558\uace0, '.shape'\ub97c \uc0ac\uc6a9\ud558\uba74 \uc804\uccb4 \uc5f4\uc774\ub098 \ud589\uc758 \uac1c\uc218\ub97c \ubc18\ud658\ud55c\ub2e4.)\n\nprint(f'Number of columns in train data: {train.shape[1]}')\nprint(f'No of missing values in train data: {sum(train.isna().sum())}')\n# If 'isna().sum()' is used, the sum of 'missing values' is displayed.\n# ('isna().sum()' \uc744 \uc0ac\uc6a9\ud558\uba74 '\uacb0\uce21\uce58'\ub97c \ud569\ud55c \uac12\uc744 \ub098\ud0c0\ub0b4\uc900\ub2e4.)","d81b842a":"train.describe() # 'describe()' serves to output summary statistics\n                 # ('describe()'\ub294 \uc694\uc57d \ud1b5\uacc4\ub7c9\uc744 \ucd9c\ub825\ud558\ub294 \uc5ed\ud560)","e5eb79be":"test.head()","38a94500":"print(f'Number of rows in test data: {test.shape[0]}')\nprint(f'Number of columns in test data: {test.shape[1]}')\nprint(f'No of missing values in test data: {sum(test.isna().sum())}')","36ef51a4":"submission.head()","8c530ff2":"print(\"\\n Name of the columns train data \\n \", train.columns.tolist(), '\\n')  # let's see the name of the columns train data\nprint('\\n Name of the columns test data \\n', test.columns.tolist(), '\\n')  # let's see the name of the columns test data","b799cc0a":"print('total number of columns train data', len(train.columns.tolist()))  # total number of columns (\uceec\ub7fc\uc758 \uc218)\nprint('total number of columns test data', len(test.columns.tolist()))  # total number of columns (\uceec\ub7fc\uc758 \uc218)","4e53e23c":"train.head()  ","d9ef5eba":"train.info()","c8a73efc":"train.describe()","622c0386":"df1 = train[0:50]\nplt.figure(figsize=(30, 30))\n\nsns.lineplot(data=df1) # Line graph visualization using seaborn (seaborn\uc744 \uc774\uc6a9\ud55c \uc120 \uadf8\ub798\ud504 \uc2dc\uac01\ud654)","6928c5dc":"# train.value_counts()  # Counting the number of values (train \uac12\uc758 \uac1c\uc218)","114401c0":"train.isnull().sum() #Check if there'is null values (\ub110 \uac12\uc774 \uc788\ub294\uc9c0 \ud655\uc778)","a09c40ad":"train.iloc[:,:].hist(figsize=(18,18))  # Visualize the entire value of the train data as a histogram\nplt.show()                               #(train \ub370\uc774\ud130\uc758 \uc804\uccb4 \uac12\uc744 \ud788\uc2a4\ud1a0\uadf8\ub7a8\uc73c\ub85c \uc2dc\uac01\ud654)","333cba0a":"cols = [e for e in test.columns if e not in ('Id')]\ncontinous_features = cols[:10]\ncategorical_features = cols[10:]\n# plot continous features  \ni = 1\nplt.figure()\nfig, ax = plt.subplots(2, 5,figsize=(20, 12))\nfor feature in continous_features:\n    plt.subplot(2, 5,i)\n    sns.histplot(train[feature],color=\"blue\", kde=True,bins=100, label='train_'+feature)\n    sns.histplot(test[feature],color=\"olive\", kde=True,bins=100, label='test_'+feature)\n    plt.xlabel(feature, fontsize=9); plt.legend()\n    i += 1\nplt.show() ","b26c351d":"test.head()","f67f7e41":"test.info()","7d1820c9":"test.describe()","d93cbf3a":"test.iloc[:,:].hist(figsize=(18,18))\nplt.show()","91f413fe":"df2 = test[0:50]\nplt.figure(figsize=(30, 30))\n\nsns.lineplot(data=df2)","10215355":"test.isnull().sum()","ff5f7a70":"sns.catplot(x=\"Cover_Type\", kind=\"count\", palette=\"ch:.25\", data=train)","3b82fea2":"train.Cover_Type.value_counts()","ac644ea3":"fig, ax = plt.subplots()\nsns.countplot(x='Cover_Type', data=train, order=sorted(train['Cover_Type'].unique()), ax=ax)\nax.set_ylim(0, 2563000)\nax.set_title('Cover_Type Distribution', weight='bold')\nplt.show()","d0025e6f":"train.iloc[:, :-1].describe().T.sort_values(by='std' , ascending = False)\\\n                     .style.background_gradient(cmap='turbo')\\\n                     .bar(subset=[\"max\"], color='#3296B1')\\\n                     .bar(subset=[\"mean\"], color='#1B71B1')\\","24548dcd":"df = pd.concat([train[FEATURES], test[FEATURES]], axis=0)\n\ncat_features = [col for col in FEATURES if df[col].nunique() < 9]\ncont_features = [col for col in FEATURES if df[col].nunique() >= 9]\n\ndel df\nprint(f'Total number of features: {len(FEATURES)}')\nprint(f'Number of categorical features: {len(cat_features)}')\nprint(f'Number of continuos features: {len(cont_features)}')\n\nplt.pie([len(cat_features), len(cont_features)],\n       labels=['Categorical', 'Continuos'],\n       colors=['#B14F3A', '#07E0C5'],\n       textprops={'fontsize' : 15},\n       autopct='%1.1f%%')\n\nplt.show()","620c7356":"ncols = 5\nnrows = int(len(cont_features) \/ ncols + (len(FEATURES) % ncols > 0))-1\n\nfig, axes = plt.subplots(nrows, ncols, figsize=(18, 8), facecolor='#A171E0')\n\nfor r in range(nrows) :\n    for c in range(ncols) :\n        col = cont_features[r*ncols+c]\n        sns.kdeplot(x=train[col], ax=axes[r, c], color='#6F8CB1', label='Train data')\n        sns.kdeplot(x=test[col], ax=axes[r, c], color='#4A6BB1', label='Test data')\n        axes[r, c].set_ylabel('')\n        axes[r, c].set_xlabel(col, fontsize=10, fontweight='bold')\n        axes[r, c].tick_params(labelsize=5, width=0.5)\n        axes[r, c].xaxis.offsetText.set_fontsize(4)\n        axes[r, c].yaxis.offsetText.set_fontsize(4)\n        \nplt.show()","293d45b2":"if len(cat_features) == 0 :\n    print(\"No Categorical features\")\nelse:\n    ncols = 5\n    nrows = int(len(cat_features) \/ ncols + (len(FEATURES) % ncols > 0))\n    \n    fig, axes = plt.subplots(nrows, ncols, figsize=(18, 45), facecolor='#DB92DB')\n    \n    for r in range(nrows):\n        for c in range(ncols):\n            if r*ncols+c >= len(cat_features):\n                break\n            col = cat_features[r*ncols+c]\n            sns.countplot(x=train[col], ax=axes[r, c], color='#97A0DB', label='Train Data')\n            sns.countplot(x=train[col], ax=axes[r, c], color='#DB48A1', label='Test Data')\n            axes[r, c].set_ylabel('')\n            axes[r, c].set_xlabel(col, fontsize=10, fontweight='bold')\n            axes[r, c].tick_params(labelsize=5, width=0.6)\n            axes[r, c].xaxis.offsetText.set_fontsize(4)\n            axes[r, c].yaxis.offsetText.set_fontsize(4)\n            \nplt.show()","101db3ca":"target_df = pd.DataFrame(train[TARGET].value_counts()).reset_index()\ntarget_df.columns = [TARGET, 'count']\nfig = px.bar(data_frame = target_df,\n            x = 'Cover_Type',\n            y = 'count',\n            color = \"count\",\n            color_continuous_scale=\"Emrld\")\n\nfig.show()\ntarget_df.sort_values(by =TARGET , ignore_index = True)","6ab65962":"train = train.drop(index = int(np.where(train[\"Cover_Type\"] == 5 )[0]))    \ntrain = train.drop(labels = [\"Soil_Type7\" , \"Soil_Type8\", \"Soil_Type15\"] ,axis = 1)\nFEATURES.remove('Soil_Type7') \nFEATURES.remove('Soil_Type8')\nFEATURES.remove('Soil_Type15')","42ecee02":"train.corr(method = \"pearson\").style.background_gradient(cmap='Blues')","a5a8ddc2":"corr=train.corr() \nv=10\ncolmn = corr.nlargest(v, 'Cover_Type')['Cover_Type'].index \nxm = np.corrcoef(train[colmn].values.T)\nsns.set(font_scale=1.25)\nplt.figure(figsize=(18, 18))\nhm = sns.heatmap(xm, cbar=True, annot=True, square=True, fmt='.2f', annot_kws={'size': 10},yticklabels=colmn.values, xticklabels=colmn.values)\nplt.show()","26b918ea":"train[\"mean\"] = train[FEATURES].mean(axis=1)\ntrain[\"std\"] = train[FEATURES].std(axis=1)\ntrain[\"min\"] = train[FEATURES].min(axis=1)\ntrain[\"max\"] = train[FEATURES].max(axis=1)\n\ntest[\"mean\"] = test[FEATURES].mean(axis=1)\ntest[\"std\"] = test[FEATURES].std(axis=1)\ntest[\"min\"] = test[FEATURES].min(axis=1)\ntest[\"max\"] = test[FEATURES].max(axis=1)\n\nFEATURES.extend(['mean', 'std', 'min', 'max'])","94cc7542":"scaler = StandardScaler()\nfor col in FEATURES:\n    train[col] = scaler.fit_transform(train[col].to_numpy().reshape(-1,1))\n    test[col] = scaler.transform(test[col].to_numpy().reshape(-1,1))\n    \nX = train[FEATURES].to_numpy().astype(np.float32)\ny = train[TARGET].to_numpy().astype(np.float32)\nX_test = test[FEATURES].to_numpy().astype(np.float32)","99562855":"lgb_params = {\n    'objective' : 'multiclass',\n    'metric' : 'multi_logloss',\n    'device' : 'gpu',\n}\n\n\nlgb_predictions = []\nlgb_scores = []\n\nkf = StratifiedKFold(n_splits=5, shuffle=True, random_state=RANDOM_STATE)\nfor fold, (train_idx, valid_idx) in enumerate(kf.split(X = X, y = y)):\n\n    print(10*\"=\", f\"Fold={fold+1}\", 10*\"=\")\n    start_time = time.time()\n    x_train = X[train_idx, :]\n    x_valid = X[valid_idx, :]\n    y_train = y[train_idx]\n    y_valid = y[valid_idx]\n    \n    model = LGBMClassifier(**lgb_params)\n    model.fit(x_train, y_train,\n          early_stopping_rounds=200,\n          eval_set=[(x_valid, y_valid)],\n          verbose=0)\n    \n    preds_valid = model.predict(x_valid)\n    acc = accuracy_score(y_valid,  preds_valid)\n    lgb_scores.append(acc)\n    run_time = time.time() - start_time\n    print(f\"Fold={fold+1}, acc: {acc:.8f}, Run Time: {run_time:.2f}\")\n    test_preds = model.predict(X_test)\n    lgb_predictions.append(test_preds)\n    \nprint(\"Mean Accuracy :\", np.mean(lgb_scores))","acc7ae26":"lgb_submission = submission.copy()\nlgb_submission['Cover_Type'] = np.squeeze(mode(np.column_stack(lgb_predictions),axis = 1)[0]).astype('int')\nlgb_submission.to_csv(\"lgb-subs.csv\",index=None)\nlgb_submission.head()","32122d45":"#### There are total 7 different output classes (\ucd1d 7\uac1c\uc758 \ub2e4\ub978 \ud074\ub798\uc2a4 \ucd9c\ub825\uc774 \uc788\uc2b5\ub2c8\ub2e4.)","70fc3611":"### code source : https:\/\/www.kaggle.com\/odins0n\/tps-dec-eda-modeling","d459192d":"#### Target distibution (\ub300\uc0c1 \ubd84\ud3ec)","4e1e66b3":"<a id=\"4\"><\/a>\n# EDA","8c2ca9d8":"## Target Distribution (\ub300\uc0c1 \ubd84\ud3ec)","6bea4b02":"# Exploring Test Data (Test \ub370\uc774\ud130 \ud0d0\uc0c9)","53dcad96":"## Quick view of Test Data (Test \ub370\uc774\ud130\uc758 \ube68\ub9ac\ubcf4\uae30)","6d269d62":"# Modeling (\ubaa8\ub378\ub9c1)","d713f40e":"# Import Library","6a51580e":"# LGBM Classifier (Light BGM \ubd84\ub958\uae30)","342d8d4e":"<a id=\"3.3\"><\/a>\n# Submission File","a551b52c":"# T.P.S Submission File","926ce283":"#### Soil_type7 and Soil_Type15 are all zero values (\ud0c0\uc7857\uacfc \ud0c0\uc78515\uc5d0\ub294 \uac12\uc774 0\uc774\ub2e4.)","6517b981":"## Quick view of Train Data (Train \ub370\uc774\ud130 \ube68\ub9ac \ubcf4\uae30)","e1052593":"# Feature Engineering (\ub370\uc774\ud130 \uc804\ucc98\ub9ac)","f1e4c4fd":"### Learn more about the train data and test data, and do more visualization and preprocessing.\n(data source(\ub370\uc774\ud130 \ucd9c\ucc98) : https:\/\/www.kaggle.com\/andrej0marinchenko\/tps12-21-data-visualization)","36e82679":"### LGBM Classifier Submission (Light BGM \ubd84\ub958\uae30 \uc81c\ucd9c)","4551bf3c":"### Removing Unwanted Rows and columns (\uac12\uc774 \uc5c6\uac70\ub098 \ud544\uc694\uc5c6\ub294 \ud589,\uc5f4 \uc81c\uac70)","5ba4c411":"# Submission (\uc81c\ucd9c)","49c14eab":"# Data Loading and Preperation (\ub370\uc774\ud130 \ub85c\ub4dc \ubc0f \uc900\ube44)","c2291342":"## Basic statistics of training data (\ud6c8\ub828 \ub370\uc774\ud130\uc758 \uae30\ubcf8 \ud1b5\uacc4)","6f61ad4f":"## Feature Distribution of Continous Features (\uc5f0\uc18d\ud615 \ub370\uc774\ud130\uc758 \ud2b9\uc9d5 \ubd84\ud3ec)","4ef0ce1e":"<a id=\"3.1\"><\/a>\n# Exploring Train Data (Train \ub370\uc774\ud130 \ud0d0\uc0c9)","17a4fb5a":"## Continuos and Categorical Data Distribution (\uc5f0\uc18d\ud615 \ub370\uc774\ud130\uc640 \uba85\ubaa9\ud615 \ub370\uc774\ud130 \ubd84\ud3ec)","fc2cdcf0":"## Overview of Data (\ub370\uc774\ud130 \uac1c\uc694)","a1a280d6":"## Feature Distribution of Categorical Features (\uba85\ubaa9\ud615 \ub370\uc774\ud130\uc758 \ud2b9\uc9d5 \ubd84\ud3ec)"}}