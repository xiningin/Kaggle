{"cell_type":{"6bd60e6e":"code","5ff84617":"code","d81e4f93":"code","dd4db193":"code","c026fb16":"code","308e91c7":"code","7b953c1b":"code","6ca19bb2":"code","7da3eb6a":"code","d6e1c739":"code","972d9906":"code","52765799":"code","ad507281":"code","0a7176e9":"code","a3dcc4b7":"code","dfb06cfa":"code","abe35c4a":"code","1d73bfaa":"code","5423336a":"code","9623b1c0":"code","e461a2cf":"code","205f5bfd":"code","33963d0a":"code","aeba2130":"code","c98dcc97":"code","e4adc7d6":"code","41cca76e":"code","520adcc2":"code","583c6642":"code","353ddb9b":"code","847acca9":"code","49ca1f03":"code","8da746df":"code","af00a80d":"code","a37bfc54":"code","e32a1a62":"code","d2185ea7":"code","08e58a84":"code","a8af0b17":"code","cff6070e":"code","edb541ef":"code","7d28c0b8":"code","d4ad5581":"code","e8e33227":"code","e533d216":"code","0a85c693":"code","7d9fc899":"code","9e8c734a":"code","af1eb6b4":"markdown","d324e3e5":"markdown","3aa9b6db":"markdown","e42b106d":"markdown","b60b6953":"markdown","6234c027":"markdown","1021a959":"markdown","d57e6dc4":"markdown","97d0b027":"markdown","c9832590":"markdown","08cecf18":"markdown","23a05277":"markdown","125f1713":"markdown","58a500f1":"markdown","1a9ff33f":"markdown","0677a300":"markdown","2ae49e57":"markdown","37b2ebf8":"markdown","2640a229":"markdown","024e490a":"markdown","b79162e3":"markdown","8c9cb64d":"markdown","87ea10f1":"markdown","568d2bf0":"markdown","298e044a":"markdown","7ca5f79b":"markdown","c5b2b917":"markdown","fc6eea48":"markdown","3d63f409":"markdown","c7fc03e3":"markdown","4ce653fd":"markdown","7aef74a5":"markdown","37250383":"markdown","f4125324":"markdown","8cb96e81":"markdown","3248f80d":"markdown","e345bc7a":"markdown"},"source":{"6bd60e6e":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport matplotlib as mlt\nimport seaborn as sns\n\n# Warnings\nimport warnings\nwarnings.filterwarnings('ignore')","5ff84617":"%matplotlib inline\n%precision 3\nplt.style.use('ggplot')\nsns.set_style('white')","d81e4f93":"import os\nprint(os.listdir(\"..\/input\"))","dd4db193":"df_origin = pd.read_csv('..\/input\/mushroom-classification\/mushrooms.csv')\ndf = df_origin.copy()","c026fb16":"df.sample(6)","308e91c7":"df.info()","7b953c1b":"# Let's see the shape of our dataframe\nprint(\"Shape of the data =\", df.shape)","6ca19bb2":"df.describe(include='all')","7da3eb6a":"del df['veil-type']","d6e1c739":"df['class'].value_counts().to_frame().style.background_gradient('summer')\n\n# Looks like we have a balanced dataset.","972d9906":"sorted_counts = df['class'].value_counts()\nplt.pie(sorted_counts, labels=['Edible', 'Poisonous'], startangle=90, counterclock=False, autopct='%1.0f%%')\nplt.axis('square');","52765799":"# First, I'll change the label for each column instead of a letter to a word, just for the sake of visualization,\n# then I'll return it back, in the data preprocessing.\ndf_visual = df.copy()\n    \ncap_shape = dict(b='bell', c='conical', x='convex', f='flat', k='knobbed', s='sunken')\ncap_surface = dict(f='fibrous', g='grooves', y='scaly', s='smooth')\ncap_color = dict(n='brown', b='buff', c='cinnamon', g='gray', r='green', p='pink', u='purple', e='red', w='white', y='yellow')\n\ndf_visual['cap-shape'] = df_visual['cap-shape'].map(cap_shape)\ndf_visual['cap-surface'] = df_visual['cap-surface'].map(cap_surface)\ndf_visual['cap-color'] = df_visual['cap-color'].map(cap_color)","ad507281":"df_visual['class'] = df_visual['class'].map(dict(p='Poisonous', e='Edible'))","0a7176e9":"plt.figure(figsize=(18, 6))\nplt.subplot(131)\nsns.countplot(x='cap-shape', data=df_visual, hue='class')\nplt.title('Cap Shape')\nplt.xticks(rotation=15)\n\nplt.subplot(132)\nsns.countplot(x='cap-surface', data=df_visual, hue='class')\nplt.title('Cap Surface')\n\nplt.subplot(133)\nsns.countplot(x='cap-color', data=df_visual, hue='class');\nplt.title('Cap Color')\nplt.xticks(rotation=40)\nmlt.rc('xtick', labelsize=14) \n\nplt.subplots_adjust(wspace=.2)","a3dcc4b7":"# changing the label for each column instead of a letter to a word, just for the sake of visualization.\n\ngill_attachment = dict(a='Attached', d='Descending', f='Free', n='Notched')\ngill_spacing = dict(c='Close', w='Crowded', d='Distant')\ngill_size = dict(b='Broad', n='Narrow')\ngill_color = dict(n='Brown', b='Buff', g='Gray', r='Green', p='Pink', u='Purple', o='Orange',\n                  h='Chocolate', k='Black', e='Red', w='White', y='Yellow')\n\ndf_visual['gill-attachment'] = df_visual['gill-attachment'].map(gill_attachment)\ndf_visual['gill-spacing'] = df_visual['gill-spacing'].map(gill_spacing)\ndf_visual['gill-size'] = df_visual['gill-size'].map(gill_size)\ndf_visual['gill-color'] = df_visual['gill-color'].map(gill_color)","dfb06cfa":"plt.figure(figsize=(16, 10))\n\nplt.subplot(221)\nsns.countplot(data=df_visual, x='gill-attachment', hue='class')\nplt.title('Gill Attachment')\n\nplt.subplot(222)\nsns.countplot(data=df_visual, x='gill-spacing', hue='class')\nplt.title('Gill Spacing')\n\nplt.subplot(223)\nsns.countplot(data=df_visual, x='gill-size', hue='class')\nplt.title('Gill Size')\n\nplt.subplot(224)\nsns.countplot(data=df_visual, x='gill-color', hue='class')\nplt.title('Gill Color')\nplt.xticks(rotation=60)\nmlt.rc('xtick', labelsize=12)\n\n\nplt.subplots_adjust(wspace=.2, hspace=.4)","abe35c4a":"# changing the label for each column instead of a letter to a word, just for the sake of visualization. \nstalk_shape              = dict(e='enlarging',t='tapering')\nstalk_root               = {'b':'bulbous','c':'club','u':'cup','e':'equal','z':'rhizomorphs','r':'rooted', '?':'missing'}\nstalk_surface_above_ring = dict(f='fibrous',y='scaly',k='silky',s='smooth')\nstalk_surface_below_ring = dict(f='fibrous',y='scaly',k='silky',s='smooth')\nstalk_color_above_ring   = dict(n='brown', b='buff', c='cinnamon', g='gray', o='orange', p='pink', e='red',\n                                w='white', y='yellow')\nstalk_color_below_ring   = dict(n='brown', b='buff', c='cinnamon', g='gray', o='orange', p='pink', e='red',\n                                w='white', y='yellow')\n\n# Map all the new labels to its corresponding column\ndf_visual['stalk-shape']              = df_visual['stalk-shape'].map(stalk_shape)\ndf_visual['stalk-root']               = df_visual['stalk-root'].map(stalk_root)\ndf_visual['stalk-surface-above-ring'] = df_visual['stalk-surface-above-ring'].map(stalk_surface_above_ring)\ndf_visual['stalk-surface-below-ring'] = df_visual['stalk-surface-below-ring'].map(stalk_surface_below_ring)\ndf_visual['stalk-color-above-ring']   = df_visual['stalk-color-above-ring'].map(stalk_color_above_ring)\ndf_visual['stalk-color-below-ring']   = df_visual['stalk-color-below-ring'].map(stalk_color_below_ring)","1d73bfaa":"plt.figure(figsize=(18, 10))\n\nplt.subplot(231)\nsns.countplot(data=df_visual, x='stalk-shape', hue='class')\nplt.title('Stalk Shape')\n\nplt.subplot(232)\nsns.countplot(data=df_visual, x='stalk-root', hue='class')\nplt.title('Stalk Root')\n\nplt.subplot(233)\nsns.countplot(data=df_visual, x='stalk-surface-above-ring', hue='class')\nplt.title('Stalk Surface Above Ring')\n\n\nplt.subplot(234)\nsns.countplot(data=df_visual, x='stalk-surface-below-ring', hue='class')\nplt.title('Stalk Surface Below Ring')\n\n\nplt.subplot(235)\nsns.countplot(data=df_visual, x='stalk-color-above-ring', hue='class')\nplt.title('Stalk Color Above Ring')\nplt.xticks(rotation=40)\n\nplt.subplot(236)\nsns.countplot(data=df_visual, x='stalk-color-below-ring', hue='class')\nplt.title('Stalk Color Below Ring')\nplt.xticks(rotation=40)\nmlt.rc('xtick', labelsize=14)\n\nplt.subplots_adjust(wspace=.3, hspace=.4)","5423336a":"# changing the label for each column instead of a letter to a word, just for the sake of visualization.\n\nveil_color  = dict(n='brown', o='orange', w='white', y='yellow')\nring_number = dict(n='none', o='one', t='two')\nring_type   = dict(c='cobwebby', e='evanescent', f='flaring', l='large', n='none', p='pendant',\n                   s='sheathing', z='zone')\n\ndf_visual['veil-color']  = df_visual['veil-color'].map(veil_color)\ndf_visual['ring-number'] = df_visual['ring-number'].map(ring_number)\ndf_visual['ring-type']   = df_visual['ring-type'].map(ring_type)","9623b1c0":"plt.figure(figsize=(16, 6))\n\nplt.subplot(131)\nsns.countplot(data=df_visual, x='veil-color', hue='class')\nplt.title('Veil Color')\n\nplt.subplot(132)\nsns.countplot(data=df_visual, x='ring-number', hue='class')\nplt.title('Ring Number')\n\nplt.subplot(133)\nsns.countplot(data=df_visual, x='ring-type', hue='class')\nplt.title('Ring Type')\nplt.xticks(rotation=15)\nmlt.rc('xtick', labelsize=12)\n\n\nplt.subplots_adjust(wspace=.2, hspace=.4)","e461a2cf":"bruises           = dict(t='true', f='false')\nodor              = dict(a='almond', l='anise', c='creosote', y='fishy', f='foul', m='musty', n='none', \n                         p='pungent', s='spicy')\nspore_print_color = dict(k='black', n='brown', b='buff', h='chocoloate', r='green', o='Orange', u='Purple',\n                        w='White', y='Yellow')\npopulation        = dict(a='Abundant', c='Clustered', n='Numerous', s='Scattered', v='Several', y='Solitary')\nhabitat           = dict(g='Grasses', l='Leaves', m='Meadows', p='Paths', u='Urban', w='Waste', d='Woods')\n\n\ndf_visual['bruises']             = df_visual['bruises'].map(bruises)\ndf_visual['odor']                = df_visual['odor'].map(odor)\ndf_visual['spore-print-color']   = df_visual['spore-print-color'].map(spore_print_color)\ndf_visual['population']          = df_visual['population'].map(population)\ndf_visual['habitat']             = df_visual['habitat'].map(habitat)","205f5bfd":"plt.figure(figsize=(18, 10))\n\nlabels = ['bruises', 'odor', 'spore-print-color', 'population', 'habitat']\n\nfor i, label in enumerate(labels):\n    plt.subplot(2, 3, i+1)\n    sns.countplot(data=df_visual, x=label, hue='class')\n    plt.title(label.capitalize())\n    plt.xticks(rotation=40)\n\nmlt.rc('xtick', labelsize=14)\nplt.subplots_adjust(wspace=.3, hspace=.4)","33963d0a":"# Splite the featuers and target in two variables.\nX = df.drop('class', axis=1)\ny = df['class']\n\n# For the sake of visualization, then I'll return it by hot-encoding.\ny = y.map({'p': 'Posionous', 'e': 'Edible'}) \n\nprint('Shape of X =', X.shape)\nprint('Shape of y =', y.shape)","aeba2130":"# Get dummy variables for each column in X.\nX_dummy = pd.get_dummies(X, drop_first=True)\nX_dummy.sample(5)","c98dcc97":"print('Number of columns generated from dummy variables =', len(X_dummy.columns.values),'Columns')","e4adc7d6":"from sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA","41cca76e":"X_std = StandardScaler().fit_transform(X_dummy)\nX_std","520adcc2":"# Reduce 95 features to 2 features to visualize it.\nX_pca = PCA(n_components=2).fit_transform(X_std)\n\nprint('Original Number of features =',  X_dummy.shape[1] ,'columns')\nprint('Reduced Number of features =', X_pca.shape[1], 'columns')","583c6642":"# Attach the label for each 2-d data point\nX_pca = np.vstack((X_pca.T, y)).T\n\ndf_pca = pd.DataFrame(X_pca, columns=['First_Component',\n                                      'Second_Component',\n                                      'class'])\nsns.FacetGrid(data=df_pca, hue='class', size=6)\\\n   .map(plt.scatter, 'First_Component', 'Second_Component')\\\n   .add_legend();","353ddb9b":"# I'll create a CDF for number of features, to answer that question\npca = PCA(n_components=95)\nX_pca = pca.fit_transform(X_std)\n\npercent_var_explained = pca.explained_variance_ \/ np.sum(pca.explained_variance_)\n\ncum_var_explained = np.cumsum(percent_var_explained)\n\n# Plot the PCA Spectrum\nplt.figure(1, figsize=(12, 6))\nplt.clf()\nplt.plot(cum_var_explained, linewidth=2)\nplt.axis('tight')\nplt.grid()\nplt.xlabel('n_components')\nplt.ylabel('Cumulative Explained Variance');","847acca9":"# Create a PCA that will retain 97 of the variance\npca = PCA(n_components=0.97, whiten=True)\n\n# Conduct PCA\nX_pca = pca.fit_transform(X_std)","49ca1f03":"# Show the result\nprint('Original number of features:', X_dummy.shape[1])\nprint('Reduced number of features:', X_pca.shape[1])","8da746df":"from sklearn.manifold import TSNE","af00a80d":"# I'm staying with default parameter for the initial trial.\n# Number of components = 2\n# default perplexity = 30 (Minmum number of points for one neighbor)\n# deafult epsilon = 200 (The learning rate)\n# default Maximum number of iterations for optimization = 1000\n# NOTE: the code here will take a while to run, take a walk till it finishs\ntsne = TSNE(n_components=2, random_state=0)\n\nX_tsne = tsne.fit_transform(X_std)","a37bfc54":"# Create the dataframe to plot\nX_tsne_data = np.vstack((X_tsne.T, y)).T\ndf_tsne = pd.DataFrame(X_tsne_data, columns=['Dim1', 'Dim2', 'class'])\n\n# Plot the 2 components from t-SNE\nsns.FacetGrid(df_tsne, hue='class', size=6)\\\n   .map(plt.scatter, 'Dim1', 'Dim2')\\\n   .add_legend();","e32a1a62":"tsne = TSNE(n_components=2, random_state=0, perplexity=50)\nX_tsne = tsne.fit_transform(X_std)","d2185ea7":"# Plot the resultant 2 dim from t-SNE\nX_tsne_data = np.vstack((X_tsne.T, y)).T\ndf_tsne = pd.DataFrame(data=X_tsne_data, columns=['Dim1', 'Dim2', 'class'])\n\nsns.FacetGrid(df_tsne, hue='class', size=6)\\\n   .map(plt.scatter, 'Dim1', 'Dim2')\\\n   .add_legend()\nplt.title('With Perplexity = 50');","08e58a84":"tsne = TSNE(n_components=2, random_state=0, perplexity=50, n_iter=5000)\nX_tsne = tsne.fit_transform(X_std)","a8af0b17":"# Create a dataframe to plot\ntsne_data = np.vstack((X_tsne.T, y)).T\ndf_tsne = pd.DataFrame(data=tsne_data, columns=['Dim_1', 'Dim_2', 'Class'])\n\n# Plot the dataframe\nsns.FacetGrid(data=df_tsne, hue='Class', size=6)\\\n   .map(plt.scatter, 'Dim_1', 'Dim_2')\\\n   .add_legend()\nplt.title('With Perplexity = 50, n_iter = 5000');","cff6070e":"tsne = TSNE(n_components=2, random_state=0, perplexity=50, n_iter=5000)\nX_tsne = tsne.fit_transform(X_std)","edb541ef":"# Create a dataframe to plot\ntsne_data = np.vstack((X_tsne.T, y)).T\ndf_tsne = pd.DataFrame(data=tsne_data, columns=['Dim_1', 'Dim_2', 'Class'])\n\n# Plot the dataframe\nsns.FacetGrid(data=df_tsne, hue='Class', size=6)\\\n   .map(plt.scatter, 'Dim_1', 'Dim_2')\\\n   .add_legend()\nplt.title('With Perplexity = 50, n_iter = 5000');","7d28c0b8":"tsne = TSNE(n_components=2, random_state=0, perplexity=100, n_iter=1000)\nX_tsne = tsne.fit_transform(X_std)","d4ad5581":"# Create a dataframe to plot\ntsne_data = np.vstack((X_tsne.T, y)).T\ndf_tsne = pd.DataFrame(data=tsne_data, columns=['Dim_1', 'Dim_2', 'Class'])\n\n# Plot the dataframe\nsns.FacetGrid(data=df_tsne, hue='Class', size=6)\\\n   .map(plt.scatter, 'Dim_1', 'Dim_2')\\\n   .add_legend()\nplt.title('With Perplexity = 70, n_iter = 1000');","e8e33227":"# First let's label encode the class column\ny = y.map({'Posionous':1, 'Edible':0})\ny.value_counts()","e533d216":"from sklearn import svm, tree, linear_model, neighbors, naive_bayes, ensemble, discriminant_analysis, gaussian_process\n# from xgboost import XGBClassifier","0a85c693":"# Machine Learning Algorithm (MLA) Selection and Initialization\nMLA = [\n    # Ensemble Methods\n    ensemble.AdaBoostClassifier(),\n    ensemble.BaggingClassifier(),\n    ensemble.ExtraTreesClassifier(),\n    ensemble.GradientBoostingClassifier(),\n    ensemble.RandomForestClassifier(),\n    \n    # Gaussian Processes\n    gaussian_process.GaussianProcessClassifier(),\n    \n    # GLM\n    linear_model.LogisticRegressionCV(),\n    linear_model.PassiveAggressiveClassifier(),\n    linear_model.RidgeClassifierCV(),\n    linear_model.SGDClassifier(),\n    linear_model.Perceptron(),\n    \n    # Naives Bayes\n    naive_bayes.BernoulliNB(),\n    naive_bayes.GaussianNB(),\n    \n    # Nearest Neighbor\n    neighbors.KNeighborsClassifier(),\n    \n    # SVM\n    svm.SVC(probability=True),\n    svm.NuSVC(probability=True),\n    svm.LinearSVC(),\n    \n    # Trees\n    tree.DecisionTreeClassifier(),\n    tree.ExtraTreeClassifier(),\n    \n    # Discriminant Analysis\n    discriminant_analysis.LinearDiscriminantAnalysis(),\n    discriminant_analysis.QuadraticDiscriminantAnalysis(),\n    \n    # XGBoost\n#     XGBClassifier()\n]","7d9fc899":"# Splite dataset in cross-validation with this splitter class:\n# NOTE: this is an alternative to train_test_split\nfrom sklearn.model_selection import ShuffleSplit, cross_validate\n\n# Run the model 10x with 60\/30 split intentionally leaving out 10% of the data\ncv_split = ShuffleSplit(n_splits=10, test_size=.3, train_size=.6, random_state=0)\n\n# Create table to compare MLA metrices\nMLA_columns = ['MLA Name', 'MLA Parameters','MLA Test Accuracy Mean', \n               'MLA Test Accuracy 3*STD', 'MLA Time']\nMLA_compare = pd.DataFrame(columns=MLA_columns)\n\n# Create table to compare MLA Predictions\nMLA_predict = {}\n\n# Index through MLA and save performance to table\nfor row_idx, alg in enumerate(MLA):\n    # Set name and parameters\n    MLA_name = alg.__class__.__name__\n    MLA_compare.loc[row_idx, 'MLA Name'] = MLA_name\n    MLA_compare.loc[row_idx, 'MLA Parameters'] = str(alg.get_params())\n    \n    # Score model with cross validation\n    cv_results = cross_validate(alg, X_pca, y.astype('int'), cv=cv_split)\n    \n    MLA_compare.loc[row_idx, 'MLA Time'] = cv_results['fit_time'].mean()\n#     MLA_compare.loc[row_idx, 'MLA Train Accuracy Mean'] = cv_results['train_score'].mean() \n    MLA_compare.loc[row_idx, 'MLA Test Accuracy Mean'] = cv_results['test_score'].mean()\n    \n    # If this is a non-bias random sample, then +\/-3 standard deviations from the mean\n    # should statistically capture 99.7% of the subsets.\n    MLA_compare.loc[row_idx, 'MLA Test Accuracy 3*STD'] = cv_results['test_score'].std()*3\n    \n    # Save MLA predictions\n    alg.fit(X_pca, y.astype('int'))\n    MLA_predict[MLA_name] = alg.predict(X_pca)\n    \n# Print and Sort table.\nMLA_compare.sort_values(by=['MLA Test Accuracy Mean'], ascending=False,\n                       inplace=True)\nMLA_compare","9e8c734a":"plt.figure(figsize=(16, 8))\nbase_color = sns.color_palette()[1]\nsns.barplot(x='MLA Test Accuracy Mean', y='MLA Name', data=MLA_compare, color=base_color)\nplt.title('Machine Learning Algorithm Accuracy Score')\nplt.xlabel('Accuracy Score (%)')\nplt.ylabel('Algorithm Name')\nmlt.rc('ytick', labelsize=14)","af1eb6b4":"#### Observations:\n- `veil-color`:\n    - This feature doesn't tell us much, since `Orange` and `Brown` are edible, but we have a fewer records to build a verdit based on it.\n- `ring-number`:\n    - It the mushroom has 2 rings, it's likely to be safe.\n- `ring-type`:\n    - If the mushroom has ring with type `Pendant` or `flaring`, then it's most likely to be edible.\n    - If the mushroom has ring with type `evanescent` or `large`, then it's most likely to be poisonous.","d324e3e5":"### Descriptive Statistics:","3aa9b6db":"### Problem Statement\nOur task here, is to be able to classify the mushroom whether it's _edible_ or _poisionous_ (Binary Classification), based on the given features.\n<br>\nHere's some inspirational questions, that I'm going to discover:\n- What types of machine learning models perform best on this dataset?\n- Which features are most indicative of a poisonous mushroom?\n- Which features are most indicative of a edible mushroom?\n<br>\n\n**Point of View:** In my opinion, the last 2 questions are very important, because as a layman, he can take a look at the mushroom and determine whether it's edible or poisonous, I know, It's not that easy and it's very tricky to determine between them, but at least we can make a better decision based on data better than a decision based on nothing.","e42b106d":"#### Observations:\nAs you can see from the figure, when `n_components` = 60, it contains roughly 98% of our data, which is really good. <br>\n\nBut luckly, we can make PCA pick how many percentage of our data we need to fit our model. As illustrated in the coming code.","b60b6953":"### Project Steps\n- Import Libraries & Read Data:\n- Explore the data:\n    - I'll try a bar chart with each feature and how it relates to class\/target feature.\n- Preprocess the data:\n    - Create dummy variables for each column.\n    - Use PCA or t-SNE to reduce the number of dimensions.\n    - ...\n- Model the data:\n    - Benchmark.\n    - ...\n- Evaluation:\n    - ...\n- Summary:\n    - ...","6234c027":"#### Attribute Information:\nNow, let's look at the attributes of dataset, to know if we can understand now or not:\n<br>\n- **classes:** edible=e, poisonous=p\n\n####  Features related to `Cap`:\n- **cap-shape:** bell=b, conical=c, convex=x, flat=f, knobbed=k, sunken=s\n- **cap-surface:** fibrous=f, grooves=g, scaly=y, smooth=s\n- **cap-color:** brown=n,buff=b,cinnamon=c,gray=g,green=r,pink=p,purple=u,red=e,white=w,yellow=y\n\n#### Features related to `Gill`:\n- **gill-attachment:** attached=a, descending=d, free=f, notched=n\n- **gill-spacing:** close=c, crowded=w, distant=d\n- **gill-size:** broad=b, narrow=n\n- **gill-color:** black=k, brown=n, buff=b, chocolate=h, gray=g, green=r, orange=o, pink=p, purple=u, red=e, white=w, yellow=y\n\n#### Features related to `Stalk`:\n- **stalk-shape:** enlarging=e,tapering=t\n\n- **stalk-root:** bulbous=b,club=c,cup=u,equal=e,rhizomorphs=z,rooted=r,missing=?\n\n- **stalk-surface-above-ring:** fibrous=f,scaly=y,silky=k,smooth=s\n\n- **stalk-surface-below-ring:** fibrous=f,scaly=y,silky=k,smooth=s\n\n- **stalk-color-above-ring:** brown=n,buff=b,cinnamon=c,gray=g,orange=o,pink=p,red=e,white=w,yellow=y\n\n- **stalk-color-below-ring:** brown=n,buff=b,cinnamon=c,gray=g,orange=o,pink=p,red=e,white=w,yellow=y\n\n#### Features related to `Veil`:\n- **veil-type:** partial=p,universal=u\n\n- **veil-color:** brown=n,orange=o,white=w,yellow=y\n\n- **ring-number:** none=n,one=o,two=t\n\n- **ring-type:** cobwebby=c,evanescent=e,flaring=f,large=l,none=n,pendant=p,sheathing=s,zone=z\n\n#### `Misc` Features:\n- **bruises:** bruises=t,no=f\n\n- **odor:** almond=a,anise=l,creosote=c,fishy=y,foul=f,musty=m,none=n,pungent=p,spicy=s\n\n- **spore-print-color:** black=k,brown=n,buff=b,chocolate=h,green=r,orange=o,purple=u,white=w,yellow=y\n\n- **population:** abundant=a,clustered=c,numerous=n,scattered=s,several=v,solitary=y\n\n- **habitat:** grasses=g,leaves=l,meadows=m,paths=p,urban=u,waste=w,woods=d","1021a959":"#### Observations:\n- `Bruises`:\n    - It the mushroom has bruises, it's most likey to be edible, and vice versa.\n- `Odor`:\n    - If the mushroom has no smell, or the smell of `Almond` or `Anise`, then it's edible.\n    - If the mushroom has the smell of `Pungent`, `foul`, `creosote`, `fishy`, `spicy`, or `musty`, then it's poisonous.\n- `Spore Print Color`:\n    - If the color is `Black` or `Brown`, then it's most likely to be edible.\n    - If the color is `Chocoloate` or `White`, then it's most likely to be poisonous.\n    - For the colors `Purple`, `Orange`, `Yellow`, `Buff`, or `Green`, we don't have enough records to build a verdit.\n- `Population`:\n    - If it's `Numerous` or `Abundant`, then it's edible.\n    - If it's `Scattered` or `Solitary`, then it's most likely to be edible.\n    - If it's `Several`, then it's most likely to be poisonous.\n- `Habitat`:\n    - It's most likely to be edible, if the habitat of the mushroom is `Grasses`, `Woods`, or `Waste`.\n    - It's most likely to be poisonous, if the habitat of the mushroom is `Paths`, `Leaves`, or `Urban`.","d57e6dc4":"### Read Data","97d0b027":"### Data Modeling\nHere' I'm going to use multiple models for this problem, and see which one is the best among them.\nHere's the ML Classification Algorithms, that I'm going to use:\n- [Ensemble Methods](https:\/\/scikit-learn.org\/stable\/modules\/classes.html#module-sklearn.ensemble)\n- [Generalized Linear Models (GLM](http:\/\/scikit-learn.org\/stable\/modules\/classes.html#module-sklearn.linear_model)\n- [Naive Bayes](http:\/\/scikit-learn.org\/stable\/modules\/classes.html#module-sklearn.naive_bayes)\n- [Nearest Neighbors](http:\/\/scikit-learn.org\/stable\/modules\/classes.html#module-sklearn.neighbors)\n- [Support Vector Machines (SVM](http:\/\/scikit-learn.org\/stable\/modules\/classes.html#module-sklearn.svm)\n- [Decision Trees](http:\/\/scikit-learn.org\/stable\/modules\/classes.html#module-sklearn.tree)\n- [Discriminant Analysis](http:\/\/scikit-learn.org\/stable\/modules\/classes.html#module-sklearn.discriminant_analysis)","c9832590":"#### Observations:\n- `Gill Attachment`:\n    - If it's `Attached`, it's mostly safe, but it's hard to tell.\n    - If it's `Free`, it doesn't add up much information.\n- `Gill Spacing`:\n    - If it's `Crowded`, then it's mostly edible.\n    - It would be much safer, if you found mushroom with `Close Gill Spacing`, and didn't eat it.\n- `Gill Size`:\n    - If it's `Narrow`, then it's mostly poisonous.\n    - If it's `Broad`, then it's mostly safe to eat, but take care.\n- `Gill Color`:\n    - The most poisonous color for gill mushroom is `Buff`, it's not safe at all.\n    - The most safer color for gill is `Brown`, `White`, `Black`, and `Purple`.\n    - It's hard to tell that `Orange`, `Red`, and `Yellow`, because we have a few data of them.","08cecf18":"# Mushroom Classification\n\nMosaab Muhammad <br>\nAugust 10th, 2019","23a05277":"**Let's take the `Stalk` Feature, and see how it relates to edible or poisonous:** [`stalk-shape`, `stalk-root`, `stalk-surface-above-ring`, `stalk-surface-below-ring`, `stalk-color-above-ring`, `stalk-color-below-ring`]","125f1713":"### Metrics\n- Since we are concered about **Type II Error**, (false-negatives), meaning we don't want to say about the poisouous mushroom that it's safe, so I'm going to measure how well the predictions are based on the `Recall`.\n- For this project, I'm going to use `Accuracy` to measure how well the prediction of the model is.\n- For some reason, I'm going to use `ROC_AUC` Curve to measure how well the model is.","58a500f1":"Let's see if the class column is balanced or not","1a9ff33f":"**Time for the `Misc` Features: [`bruises`, `odor`, `spore-print-color`, `population`, `habitat`]**","0677a300":"#### Observations:\nAs you can see, they look very similar. <br>\n\n**The final trial, let's make n_iter = 5000**","2ae49e57":"#### NOTE:\nFrom the observations above, you noticed that most of them are wrote using `IF` condition, which means that we can build a model, which likely to be a decision tree to make a prediction based on those conditions for each feature.","37b2ebf8":"#### Observations:\nAs we can see here, that PCA did fairly a poor job, distinguishing between 2 clusters, because there's a lot of overlapping between the 2 clusters, I know some of them are clustered well, but we're looking here to get the best prediction. <br>\n\nWhen we increase the number of principle component, PCA will do a better job distinguishing between them and reduce the overlapping between them, but unfortunatly we won't be able to visualize. <br>\n\n**Now, let's see how many number of features has the 97% of the data, to fit our model with it?**","2640a229":"### PCA:\nI'm going to convert the columns from 95 to 2 columns just for the sake of visualization. <br>\n\nThen I'm going to see how many columns contain most of the data, to train our model. <br>\n\n**And of course, before using PCA, we need to Standardize our data.**","024e490a":"#### Observations:\n- `Stalk Root`:\n    - If the root is equal, club, or rooted, then it's mostly safe.\n    \n- `Stalk Surface`:\n    - There are more safe mushrooms that has smooth stalk surface, than the poisnous ones.\n    \n- `Stalk Color`:\n    - The more safer color for stalk is `White` and `Gray`.\n    - The `Orange` is hard to tell, whether it's safe or not, since it has fewer records.\n    - The most dangerous colors is `pink`, `buff`, and `brown`.","b79162e3":"#### Observation:\nSince all the columns are `Categorical`, and we have 23 columns. We need to dummy those columns and use PCA or t-SNE to reduce those columns.","8c9cb64d":"### t-SNE:\nt-SNE is used for demonsionality reduction, and it's the state-of-the-art in industry. <br>\n\nI'm going to reduce the 95 features, and compare the result between PCA and t-SNE.<br>\n\nI'll do the same as I did for PCA, first I'll try to reduce the dimensions to 2 features, then visualize it, to see which one is better. <br>\nThen play with perplexity and number of iterations until I get a stable shape.","87ea10f1":"**Please Upvote, if you found it helpful!**","568d2bf0":"**We can see now that we have 95 columns**","298e044a":"#### Observations:\nAs you can notice here, another magnificent result. The overlapping between 2 clusters continuing to decrease when we increase the number of perplexity to 50. <br>\n\nNow, let's try to decrease the number of iterations, to see we can get the same shape or not. In other words, to see that the shape is stable or not.<br>\n\n**Let's make n_iterations = 500**","7ca5f79b":"## III. Methodology\n\n### Data Preprocessing\nSince we have 21 features and every feature in that 21 features is _categorical_, then I'm going to use dummy variable to convert these features into numerical features, and as a result, the number of columns will increase based how unique values in each feature.\n<br>\n\nThen to reduce those n features, I'm going to use PCA and t-SNE (which is the state-of-the-art in reducing dimensionality).\n<br>\n\nAfter reducing some dimensionalities, I'm going to fit those new features to the models, and see which one gives best results.\n<br>\n\n### Dummy Variables:","c5b2b917":"**Let's take the `Gill` Feature, and see how we can get anything from it:** [`gill-attachment`, `gill-spacing`, `gill-size`, `gill-color`]","fc6eea48":"## II. Analysis\n### Import Libraries","3d63f409":"## Exploratory Data Analysis (EDA)\nIn order to answer the first 2 questions, I'll take features one by one and see how it relates to the target class.\n<br>\n\n**Let's start with `Cap` Features:** [`cap-shape`, `cap-surface`, `cap-color`]","c7fc03e3":"**Since in this dataset, the `veil-type` has only one value for each row, so I'm going to remove it.**","4ce653fd":"**Let's take the `Veil` and `Ring` Features, and see how it relates to edible or poisonous:** [`veil-type`, `veil-color`, `ring-number`, `ring-type`]","7aef74a5":"#### Observation:\n1. `Cap Shape`:\n    - there are a lot of `Bell` shaped mushrooms that are edible.\n    - All the `Sunken` shaped mushrooms _in our data_ are edible, since our data has a fewer samples of them.\n    - If you see `knobbed` shaped mushroom, don't take it, since a lot of them are poisonous.\n    - There is no records for `Conical` shaped mushrooms in our data.\n2. `Cap Surface`:\n    - The `Smooth` and `Scaly` surface are more poisonous than the edible ones.\n    - There are a lot of `Fibrous` surface mushrooms that are safe.\n    - There is no records for `Grooves` surface mushrooms in our data.\n3. `Cap Color`:\n    - A lot of mushrooms have `brown` and `gray` cap color, and more of them are safe.\n    - The `Yellow` and `Red` cap colors are the most dangours among all colors in the dataset.\n    - The `White` cap color is the safest mushroom among all the colors in the dataset.\n    - The `Green` and `Purple` have fewer data, and all of them are safe.\n\n**NOTE:**\n- Since finding an edible mushroom is a tricky issue, in the above observations, I'm talking by percent, how much this shape or color is safe and so on.\n- The observations here is not ready to be applied, the applied observations will be at the end of this notebook.","37250383":"#### Observations:\nThe best model based on the test score is `QuadraticDiscriminantAnalysis`, he got everything right!!","f4125324":"#### Observations:\nAs you can see, it has the same shape, then it's stable, we'll go with it.","8cb96e81":"From the image, we can say that we have 5 main features of the mushroom:\n1. **Cap:** which is the top part of the mushroom from above, which takes the umberlla shape.\n2. **Gill:** which is the underside of the cap\n3. **Ring:** which is this little shape that surround the stem (stalk).\n4. **Stalk (Stem):** is what makes the mushroom standing, it's this vertical cylindrical.\n5. **Veil (Volva):** which is the bottom shape of the mushroom, that covers the bottom part of the mushroom.\n<br><br>\n\nThere are other features that is included in the dataset, such as:\n- **Bruises**: Does the mushroom has bruises or not.\n- **Spore Print Color**: Shows the color of the mushroom spores if viewed en masse. The image below shows how it looks like.[[1]](https:\/\/en.wikipedia.org\/wiki\/Spore_print) \n- **Oder**: Desribes the smell or the odor of the mushroom.\n- **Population**: Shows what kind of populations that this mushroom grows.\n- **Habitat**: Shows where it lives.\n![Spore Print Color.png](attachment:image.png)","3248f80d":"#### Observations:\n**WOW!** Just Wow, with just 2 components, we clustered 2 clusters without any overlapping. <br>\n\nYou can see now, which result is better between PCA and t-SNE! <br>\n\nNow, let's play with the parameter, to know where we can get the stable shape.<br>\n\n**For the second trial, let's start with perplextiy = 50**","e345bc7a":"## I. Definition\n### Project Overview\n#### Project Origin:\nThis problem is taken from **Kaggle**, named as [Mushroom Classification](https:\/\/www.kaggle.com\/uciml\/mushroom-classification), and collected by **UCI Machine Learning**.\n#### Problem Domain:\nBefore delving into the problem, let me introduce you to the problem domain, which is related to mushroom classification, to determine whether it's _poisonous_ or _edible_. \n<br>\nSince the collected data shows the mushroom features, let me make them crisp and clear for you, to be able to understand the big picture. \n<br>\nI'll start by an image, showing the most important features for mushroom: \n<br>\n![Mushroom Features.png](attachment:image.png)\n"}}