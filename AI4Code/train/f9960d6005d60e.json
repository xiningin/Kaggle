{"cell_type":{"ab8055b1":"code","193a16c5":"code","0be34d49":"code","1adf47bb":"code","3810ecbb":"code","3c21e17d":"code","3891614f":"code","bfec3d06":"code","9352fbf1":"code","9749969e":"code","3343cbc8":"code","9aef4b7f":"code","f790f2ef":"code","74ce35d2":"code","c682319d":"code","a3102652":"code","832aba5e":"code","f54727c8":"code","cf22c001":"code","66468c26":"code","7d007495":"code","2e99513d":"code","aa424b4c":"code","051ba3b3":"code","73054ebe":"code","4b7add76":"code","35ee70a5":"code","4387d087":"code","1506f9ee":"code","1a970896":"code","6829eeff":"code","ffe054ef":"code","3b1f3133":"code","367cd0cf":"code","562bae50":"code","327a9e22":"code","47b0f113":"code","3930ca79":"code","e7047078":"code","faaa1852":"code","4185c199":"code","34149fb2":"code","bfc22ecb":"code","a97e3632":"code","24c62fe2":"code","ea162cc5":"code","0e27505c":"code","65d1d576":"code","887f30e0":"code","28f6b8c4":"code","d99c7daa":"code","46902d2c":"code","4c2e4872":"markdown","fb0a8235":"markdown","1b1586ae":"markdown","787377c3":"markdown","988cdb45":"markdown","cbcedcc4":"markdown","f0e5a8b2":"markdown","d2481c07":"markdown","5b0deedd":"markdown","2da0c179":"markdown","d8f8895a":"markdown","a5fe5867":"markdown","7d917f13":"markdown","270b4d23":"markdown","f3da5287":"markdown","01ce9cac":"markdown","6a430e33":"markdown","7f1a1131":"markdown","2a14eec8":"markdown","92b1be00":"markdown","4babf81f":"markdown","266a40f5":"markdown","acae3b22":"markdown","ee539860":"markdown","347db6dd":"markdown"},"source":{"ab8055b1":"# Data processing\nimport numpy as np\nimport pandas as pd\n\n# Data visualization\nimport seaborn as sns\nsns.set()\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n# Modelling\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import learning_curve\nfrom sklearn.model_selection import validation_curve\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.linear_model import SGDRegressor\nfrom sklearn.preprocessing import StandardScaler\n\n# Eliminating unnecessary sklearn warnings\nimport warnings\nwarnings.filterwarnings('ignore')","193a16c5":"# Create table for missing data analysis\ndef draw_missing_data_table(df):\n    total = df.isnull().sum().sort_values(ascending=False)\n    percent = (df.isnull().sum()\/df.isnull().count()).sort_values(ascending=False)\n    missing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\n    return missing_data","0be34d49":"def plot_learning_curve(estimator, title, X, y, ylim=None, cv=None,\n                        n_jobs=1, train_sizes=np.linspace(.1, 1.0, 5)):\n    \"\"\"\n    Generate a simple plot of the test and training learning curve.\n\n    Parameters\n    ----------\n    estimator : object type that implements the \"fit\" and \"predict\" methods\n        An object of that type which is cloned for each validation.\n\n    title : string\n        Title for the chart.\n\n    X : array-like, shape (n_samples, n_features)\n        Training vector, where n_samples is the number of samples and\n        n_features is the number of features.\n\n    y : array-like, shape (n_samples) or (n_samples, n_features), optional\n        Target relative to X for classification or regression;\n        None for unsupervised learning.\n\n    ylim : tuple, shape (ymin, ymax), optional\n        Defines minimum and maximum yvalues plotted.\n\n    cv : int, cross-validation generator or an iterable, optional\n        Determines the cross-validation splitting strategy.\n        Possible inputs for cv are:\n          - None, to use the default 3-fold cross-validation,\n          - integer, to specify the number of folds.\n          - An object to be used as a cross-validation generator.\n          - An iterable yielding train\/test splits.\n\n        For integer\/None inputs, if ``y`` is binary or multiclass,\n        :class:`StratifiedKFold` used. If the estimator is not a classifier\n        or if ``y`` is neither binary nor multiclass, :class:`KFold` is used.\n\n        Refer :ref:`User Guide <cross_validation>` for the various\n        cross-validators that can be used here.\n\n    n_jobs : integer, optional\n        Number of jobs to run in parallel (default 1).\n    \"\"\"\n    plt.figure()\n    plt.title(title)\n    if ylim is not None:\n        plt.ylim(*ylim)\n    plt.xlabel(\"Training examples\")\n    plt.ylabel(\"Score\")\n    train_sizes, train_scores, test_scores = learning_curve(\n        estimator, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes)\n    train_scores_mean = np.mean(train_scores, axis=1)\n    train_scores_std = np.std(train_scores, axis=1)\n    test_scores_mean = np.mean(test_scores, axis=1)\n    test_scores_std = np.std(test_scores, axis=1)\n    plt.grid()\n\n    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n                     train_scores_mean + train_scores_std, alpha=0.1,\n                     color=\"r\")\n    plt.fill_between(train_sizes, test_scores_mean - test_scores_std,\n                     test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n    plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n             label=\"Training score\")\n    plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n             label=\"Cross-validation score\")\n\n    plt.legend(loc=\"best\")\n    return plt","1adf47bb":"# Plot validation curve\ndef plot_validation_curve(estimator, title, X, y, param_name, param_range, ylim=None, cv=None,\n                        n_jobs=1, train_sizes=np.linspace(.1, 1.0, 5)):\n    train_scores, test_scores = validation_curve(estimator, X, y, param_name, param_range, cv)\n    train_mean = np.mean(train_scores, axis=1)\n    train_std = np.std(train_scores, axis=1)\n    test_mean = np.mean(test_scores, axis=1)\n    test_std = np.std(test_scores, axis=1)\n    plt.plot(param_range, train_mean, color='r', marker='o', markersize=5, label='Training score')\n    plt.fill_between(param_range, train_mean + train_std, train_mean - train_std, alpha=0.15, color='r')\n    plt.plot(param_range, test_mean, color='g', linestyle='--', marker='s', markersize=5, label='Validation score')\n    plt.fill_between(param_range, test_mean + test_std, test_mean - test_std, alpha=0.15, color='g')\n    plt.grid() \n    plt.xscale('log')\n    plt.legend(loc='best') \n    plt.xlabel('Parameter') \n    plt.ylabel('Score') \n    plt.ylim(ylim)","3810ecbb":"df = pd.read_csv('..\/input\/train.csv') # Load the data and save as df\ndf_raw = df.copy() # Save the data as raw format just in case\ndf.head()","3c21e17d":"df.describe()","3891614f":"# Draw a data table showing the missing data as percentage of total\ndraw_missing_data_table(df)","bfec3d06":"df.dtypes","9352fbf1":"# Define date as date.\ndf['date'] = pd.to_datetime(df['date'])\ndf.drop('date', axis=1, inplace=True)","9749969e":"# Define store and item as categorical\ndf['store'] = pd.Categorical(df['store'])\ndf['item'] = pd.Categorical(df['item'])\ndf.head()","3343cbc8":"# Transform categorical variables into dummy variables\ndf = pd.get_dummies(df, drop_first=True)  # To avoid dummy trap\ndf.head()","9aef4b7f":"# Create data set to train data imputation methods\nX = df[df.loc[:, df.columns != 'sales'].columns]\ny = df['sales']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.2, random_state=1)","f790f2ef":"# Debug\nprint('Inputs: \\n', X_train.head())\nprint('Outputs: \\n', y_train.head())\nprint(X.head())","74ce35d2":"# Fit linear regression\nsgdreg = SGDRegressor()\nsgdreg.fit(X_train, y_train)","c682319d":"# Model performance\nscores = cross_val_score(sgdreg, X_train, y_train, cv=10)\nprint('CV accuracy: %.3f +\/- %.3f' % (np.mean(scores), np.std(scores)))","a3102652":"# Plot learning curves\ntitle = \"Learning Curves (Linear Regression)\"\ncv = 10\nplot_learning_curve(estimator=sgdreg, title=title, X=X_train, y=y_train, cv=cv, n_jobs=1);","832aba5e":"# Plot validation curve\ntitle = 'Validation Curve (Linear Regression)'\nparam_name = 'alpha'\nparam_range = [0.0001, 0.0003, 0.001, 0.003, 0.01, 0.03, 0.1, 0.3] \ncv = 10\nplot_validation_curve(estimator=sgdreg, title=title, X=X_train, y=y_train, param_name=param_name, param_range=param_range);","f54727c8":"# Restart data set\ndf = df_raw.copy()\ndf.head()","cf22c001":"df.dtypes","66468c26":"# Define date as date.\ndf['date'] = pd.to_datetime(df['date'])","7d007495":"# Define store and item as categorical\ndf['store'] = pd.Categorical(df['store'])\ndf['item'] = pd.Categorical(df['item'])\ndf.dtypes","2e99513d":"plt.rcParams['figure.figsize']=(20,5)\nsns.barplot(df['item'], df['sales'])","aa424b4c":"plt.rcParams['figure.figsize']=(10,5)\nsns.barplot(df['store'], df['sales'])","051ba3b3":"g = sns.FacetGrid(df, col=\"item\", col_wrap=5)\ng = g.map(plt.scatter, \"date\", \"sales\", marker=\"o\", s=1, alpha=.5)","73054ebe":"df['month'] = df.date.dt.month\ndf['month'] = pd.Categorical(df['month'])\ndf.head()","4b7add76":"sns.barplot(df['month'],df['sales']);","35ee70a5":"df.dtypes","4387d087":"# Add SMA to dataframe\nsma = df.groupby(['item', 'store'], as_index=False).apply(lambda x: x['sales'].rolling(90).mean().shift(365))\ndf['sma'] = sma.reset_index(level=0, drop=True)\n\n# Add EMA to dataframe\nema = df.groupby(['item', 'store'], as_index=False).apply(lambda x: x['sales'].ewm(span=90, adjust=False).mean().shift(365))\ndf['ema'] = ema.reset_index(level=0, drop=True)\n\ndf.head()","1506f9ee":"df_latest_year_one_item = df.loc[(df['date'] > '2014-12-31') & (df['item'] == 1) & (df['store'] == 10)]\nplt.figure(figsize=(20,5))\nplt.plot(df_latest_year_one_item[['sales','sma', 'ema']])\nplt.legend(['sales','sma','ema'])\nplt.show()","1a970896":"df.isna()['sma'].sum()","6829eeff":"df.dropna(inplace=True)\ndf.isna()['sma'].sum()","ffe054ef":"# Drop date\ndf.drop('date', axis=1, inplace=True)","3b1f3133":"# Transform categorical variables into dummy variables\ndf = pd.get_dummies(df, drop_first=True)  # To avoid dummy trap","367cd0cf":"# Create data set to train data imputation methods\nX = df[df.loc[:, df.columns != 'sales'].columns]\ny = df['sales']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.2, random_state=1)","562bae50":"# Debug\nprint('Inputs: \\n', X_train.head())\nprint('Outputs: \\n', y_train.head())","327a9e22":"# Scale the features\ncolumns_to_scale = ['sma', 'ema']\nmean = X_train[columns_to_scale].mean()\nstd = X_train[columns_to_scale].std()\nX_train[columns_to_scale] = (X_train[columns_to_scale] - mean) \/ std\n\n# Fit linear regression\nsgdreg = SGDRegressor(alpha=0.0001)\nsgdreg.fit(X_train, y_train)","47b0f113":"# Model performance\nscores = cross_val_score(sgdreg, X_train, y_train, cv=10)\nprint('CV accuracy: %.3f +\/- %.3f' % (np.mean(scores), np.std(scores)))","3930ca79":"# Plot learning curves\ntitle = \"Learning Curves (Linear Regression)\"\ncv = 10\nplot_learning_curve(estimator=sgdreg, title=title, X=X_train, y=y_train, cv=cv, n_jobs=1);","e7047078":"# Plot validation curve\ntitle = 'Validation Curve (Linear Regression)'\nparam_name = 'alpha'\nparam_range = [0.0001, 0.0003, 0.001, 0.003, 0.01, 0.03, 0.1, 0.3] \ncv = 10\nplot_validation_curve(estimator=sgdreg, title=title, X=X_train, y=y_train, param_name=param_name, param_range=param_range);","faaa1852":"# Restart data set\ndf = df_raw.copy()\ndf_test_raw = pd.read_csv('..\/input\/test.csv') # Load the test data and save as df_test\ndf_test = df_test_raw.copy()\n\ncombine = [df, df_test]","4185c199":"# Define date as date.\nfor dataset in combine:\n    dataset['date'] = pd.to_datetime(dataset['date'])","34149fb2":"# Define store and item as categorical\nfor dataset in combine:\n    dataset['store'] = pd.Categorical(dataset['store'])\n    dataset['item'] = pd.Categorical(dataset['item'])","bfc22ecb":"# Define month as categorical\nfor dataset in combine:\n    dataset['month'] = dataset.date.dt.month\n    dataset['month'] = pd.Categorical(dataset['month'])\n\ndf_test.dtypes","a97e3632":"# Add SMA to dataframe\nsma = df.groupby(['item', 'store'], as_index=False).apply(lambda x: x['sales'].rolling(90).mean())\ndf['sma'] = sma.reset_index(level=0, drop=True)\n\n# Add EMA to dataframe\nema = df.groupby(['item', 'store'], as_index=False).apply(lambda x: x['sales'].ewm(span=90, adjust=False).mean())\ndf['ema'] = ema.reset_index(level=0, drop=True)","24c62fe2":"# Adding last 3 months of sma and ema from training to test data\nsma_test = df.loc[(df['date'] < '2017-04-01') & (df['date'] >= '2017-01-01')]['sma'].reset_index(drop=True)\ndf_test['sma'] = sma_test\nema_test = df.loc[(df['date'] < '2017-04-01') & (df['date'] >= '2017-01-01')]['ema'].reset_index(drop=True)\ndf_test['ema'] = ema_test\n\n# Shifting SMA and EMA on training data\ndf['sma'] = df['sma'].shift(365)\ndf['ema'] = df['ema'].shift(365)\n\ndf_test.head()","ea162cc5":"# Drop date and id\ndf.drop('date', axis=1, inplace=True)\ndf.dropna(inplace=True)\ndf_test.drop('date', axis=1, inplace=True)\ndf_test.drop('id', axis=1, inplace=True)","0e27505c":"# Transform categorical variables into dummy variables\ndf = pd.get_dummies(df, drop_first=True)  # To avoid dummy trap\ndf_test = pd.get_dummies(df_test, drop_first=True)  # To avoid dummy trap\n\n# Add month 4-12 to df_test\ndf_test = df_test.join(pd.DataFrame(\n    {\n        'month_4': 0,\n        'month_5': 0,\n        'month_6': 0,\n        'month_7': 0,\n        'month_8': 0,\n        'month_9': 0,\n        'month_10': 0,\n        'month_11': 0,\n        'month_12': 0\n    }, index=df_test.index\n))\ndf_test.head()","65d1d576":"# Prepare data for model\nX_train = df[df.loc[:, df.columns != 'sales'].columns]\ny_train = df['sales']\nX_test = df_test\n\n# Scale the features\ncolumns_to_scale = ['sma', 'ema']\nmean = X_train[columns_to_scale].mean()\nstd = X_train[columns_to_scale].std()\nX_train[columns_to_scale] = (X_train[columns_to_scale] - mean) \/ std\nX_test[columns_to_scale] = (X_test[columns_to_scale] - mean) \/ std","887f30e0":"# Run SGD Regression\nsgdreg = SGDRegressor(alpha=0.0001)\nsgdreg.fit(X_train, y_train)\n\n# Get prediction\nprediction = sgdreg.predict(X_test)","28f6b8c4":"# Add to submission\nsubmission = pd.DataFrame({\n        \"id\": df_test_raw['id'],\n        \"sales\": prediction\n})","d99c7daa":"# Quick look at the submission\nsubmission.head()","46902d2c":"# Save submission\nsubmission.to_csv('submission.csv',index=False)","4c2e4872":"# Let the real fun begin!\nWe now have our basic model, which is modelled using the SGDRegressor. The SGDRegressor performs very well using large datasets.\n\nFor now, let's do some explorative analysis.","fb0a8235":"The data does indeed look very simple. Here's my definition and quick thoughts:\n- **Date:** Defined as a date. There may be a seasonal effect.\n- **Store:** The store number of where the sales were made, which goes from 1-10. Some stores may be better at selling than other stores (e.g. better local marketing, better staff etc.).\n- **Item:** The item number which goes from 1-50. Some items may have bigger sales than others (e.g. lower price, quality etc.).\n- **Sales:** The number of sales. This is our target variable.\n\nQuick follow-up: Date may indicate seasonal effects; Some stores may be better at selling; Some items may be easier to sell.\n\nLet's take a closer look at the data, namely the descriptives of the features.","1b1586ae":"- Date should be of date type, however this need to be parsed before used in modelling, so we drop it for now.\n- Store and item should be of categorical type.\n- Sales will not be considered because it is our target variable.","787377c3":"It seems like every product has a peak time during mid year, so the date should have some predictive power. Let's zoom closer and see if we can define which periods demand is peaking and stagnant. We use the most recent year historical data for this.","988cdb45":"So far I have just worked with the features that were given in the dataset. Further feature extraction should be examined and exploited if usable.\n\n## Moving Averages\nMoving averages is commonly used in time series to smooth out the daily noise of the data, and can highlight short- and long term trends.\n\nI will make use of a few different moving averages for further analysis. The types will include:\n- Simple Moving Average\n- Exponential Moving Average\n\nAs the pattern of sales seems to be pretty similiar for each year, I will shift the moving averages by a year, so the test data will have these data available.\nI choose a 3 month period (90 days), as this is the goal of prediction.","cbcedcc4":"From the learning curve we see that the training and validation score eventually converges as the more training examples are used.\nThis means that our model seems to fit well and isn't overfitted nor underfitted.\n\nLet's check out the validation curve and see if we can optimize through parameter tuning.","f0e5a8b2":"As we can see from the above plot, the Simple Moving Average smooths all the noise, whereas our exponential reacts a little bit faster to trend movements.\nNote that the moving averages were shifted 365 days, but still it aligns pretty close to the actual means.\n\nI would expect that these moving averages would be informative for our model.\n\nOne drawdown of the shifted effect, is that it leaves our first year with NaN values. I don't see much else to do, than dropping the first year.\nThis shouldn't be so much of a problem, since we saw that our Minimum Viable Model were pretty good fit already after just 200.000 examples.","d2481c07":"**Count:** there is obviously no missing data, as the count is equal for each column.\n\n**Mean:** Nothing out of the ordinary here.\n\n**Min & Max:** Nothing out of the ordinary here, as minimum sales aren't negative, nor do we have store or item below 1. Same goes for Max.\n\nOverall the data looks simple and clean as the competition description also stated.\n\nTo confirm that there is no missing data, I can use Pedro Marcelino's function which draws a data table for the missing data.","5b0deedd":"## Functions\nThe following function are taken from [Pedro Marcelino](https:\/\/www.kaggle.com\/pmarcelino)'s kernel, [Data analysis and feature extraction with Python\n](https:\/\/www.kaggle.com\/pmarcelino\/data-analysis-and-feature-extraction-with-python\/notebook).","2da0c179":"## The data and the objective\nIn this competition there is provided a relatively simple and clean dataset for 50 different items at 10 different stores.\nThe data includes a training set with 5 years of historical data as well as a test set for 3 months.\n\n*The objective is to predict the future 3 months of demand.*\n\nLet's dig into the data.","d8f8895a":"There is indeed no gaps to fill.\n\n# Minimal Viable Model\nAs in any other case, it is always good to have some sort of beta version of your product, so that you can test it and benchmark it as development continues.\n\nI start by preparing the data, then fitting the data to a multivariate regression model and finally analyze the performance of the model through learning and validation curves.\n\n## Preparing the data","a5fe5867":"# Motivation\nI recently just got a job at a big pharmaceutical company. My task is mainly to challenge the forecasts that the financial department has made for the budgets.\nThe company is distributing over 100 different products, so the task is likely to be comparable to this competition.\n\nI hope I will gain some more insight in the forecast modelling and keep improving my skill as a forecast analyst.\n\nAny feedback is much appreciated.","7d917f13":"Seems like the logic hold for this one as well. I keep the 'store' feature as it is for now.\n\n## Date\nThe last feature to check is the date. Let's plot the date and sales and see whether there's a seasonal effect.","270b4d23":"It seems like the peak season is during April through August. Maybe I could set a categorical feature for peak time, but I will keep the monthly category as it is for now.","f3da5287":"## The beginning\nAs with any other task, one is going to need some basic tools to get started.\n### Libraries","01ce9cac":"## So how's the model now?\nSo let's make a quick recap. The following thing has changed since we ran our Minimum Viable Model.\n- **Month:** was added as a categorical feature, going from 1-12.\n- **Moving Averages:** Simple and Exponential Moving Averages were added and shifted 365 days.\nLet's run the model and see how it performs.","6a430e33":"Clearly some items sell better than others. We keep 'item' as a categorical feature as it is for now.\n\n## Stores\nLet's do the same for stores. Some stores may sell better due to geographic position, marketing, better staff etc.","7f1a1131":"# Last Words\nSo I recently just started digging into data analysis, and this is what I've learned so far. There is probably multiply ways to improve my model, and if you as a reader got any suggestions, please let me know.\n\nAll feedback is much appreciated. Thank you for taking your time to read my analysis.","2a14eec8":"# Conclusion\n- The Minimal Viable Model performs with an 66.5% +- 0.2% accuracy, whereas the final model perform with 85.3% +- 0.1% accuracy on the training data.\n- The cross validation indicates a good fit, so neither under- or overfit.\n- We have added a month category and an SMA and EMA.\n\nLet's finish the model and submit the results.","92b1be00":"Let's have a look how the SMA and EMA follows the sale volume.","4babf81f":"We will be dropping 227.000 rows. I may have to sit down and think about how this can be avoided.","266a40f5":"## Assessing Model Performance","acae3b22":"### Conclusion so far\nWith the basic features our model achieves 66.5% +- 0.2%. Our model seems to be fit well, and perform best when alpha is lower than .001.","ee539860":"## Items\nSome items may have a higher sell volume, due to greater demand or lower pricing. Let's see if some of the items sells better than some other.","347db6dd":"## Launching the model\nLet's get ready for take off!"}}