{"cell_type":{"4844aef8":"code","40cd4c9d":"code","cc6a0f8f":"code","2148b287":"code","385755dd":"code","6b45ca85":"code","1eab3f7d":"code","d204a9f3":"code","2a07a175":"code","17a5fdee":"code","6a72b0d8":"code","02a17c43":"code","4b05ae16":"code","f1e40daa":"code","b47bc21d":"code","9cd8bfc8":"code","bc40a31a":"code","5e63fe23":"code","cb16c354":"code","9a18bbb7":"code","1b2dad92":"code","53d418a7":"code","11daae16":"code","bc9c9a62":"code","e3161a13":"code","aa5b577f":"code","e09099ff":"code","df397776":"code","7036c847":"code","4ceda628":"code","3e06fa2f":"code","cf3c2881":"code","0aa4e123":"code","58ca721c":"code","a429357c":"code","d435363d":"code","1f7f1766":"code","aed04658":"code","6fdd0436":"code","29b18342":"code","15ed7986":"code","b067bc46":"code","a3f2f306":"code","a4e915a0":"code","ef1dcba7":"code","3bfe29c9":"code","aa74177f":"code","1f124fca":"code","8cdcb39c":"code","977055b4":"code","5944ee65":"code","d6634895":"code","947a26d1":"code","93e524eb":"code","e6ae0e65":"code","ef162216":"code","c32aafcd":"code","5fe09441":"code","a7c2e889":"code","22fbdf26":"code","fb7c41ae":"code","c4a8f743":"code","10fa763d":"code","ae657725":"code","156c54eb":"code","7397e26b":"code","487dac2a":"code","33c46f12":"code","513d9780":"code","a791d6f7":"code","dce45609":"code","ab668f5f":"code","28c53f81":"code","a3fdcd1e":"code","7f05c14f":"code","66db1d86":"code","1461ddde":"code","2012ccd3":"code","e2288599":"code","f94073f4":"code","dccd63ab":"code","ee2d54bf":"code","a76f258f":"code","15847edb":"code","1a7804aa":"code","858dac71":"code","be6e70eb":"code","257feeab":"code","5d458d02":"code","019618df":"code","6de48fee":"code","c01b6c90":"code","a9a682f5":"code","6936c13d":"code","1786ac0f":"code","6d241cbe":"code","5e8c065d":"code","814b2b2a":"code","0c092213":"code","f3014e46":"code","cc6e15e4":"code","900ba335":"code","12518131":"code","34a837c3":"code","77075638":"code","acc0e7e5":"code","eaf037e3":"code","bb891f06":"markdown","fb27058e":"markdown","b92e50a5":"markdown","10812fc6":"markdown","6ce3ce68":"markdown","4e5bda93":"markdown","7e4e05e2":"markdown","8b4ee96b":"markdown","a97a8c17":"markdown","86a58792":"markdown","dd664e4c":"markdown","defa1d75":"markdown","f743aea0":"markdown","f3acea7b":"markdown","9ab4b7a5":"markdown","6158e480":"markdown","694b3b3c":"markdown","77a4f36a":"markdown","339e450a":"markdown","63eb4774":"markdown","5890b510":"markdown","1aad150e":"markdown","60f48650":"markdown","3903c174":"markdown","8df5124d":"markdown","61825daf":"markdown","b2815fab":"markdown","d5e3f96d":"markdown","123bd2d3":"markdown","6bacef31":"markdown","19af3ebd":"markdown","3dc1d5fc":"markdown","98dcd81e":"markdown","f08a92d0":"markdown","6ce66440":"markdown","54b44f3f":"markdown","e63624ea":"markdown","683389ea":"markdown","7bbac40f":"markdown","335d1440":"markdown","421dc050":"markdown","96d5c179":"markdown","a1e5085f":"markdown","052e3cf8":"markdown","1421b996":"markdown","47859000":"markdown","1148cacc":"markdown","263b00f6":"markdown","8b5a08c9":"markdown","bd80d9e8":"markdown","4879d6d4":"markdown","fcd29783":"markdown","42b515b8":"markdown","d281a077":"markdown","033410b5":"markdown","8089a22b":"markdown","5a3a9e7e":"markdown","b85f5856":"markdown","5594acca":"markdown","18d03a32":"markdown","9653e967":"markdown","8afcafcd":"markdown","38e02c31":"markdown","f56adfa8":"markdown","c723d0bb":"markdown","265207b2":"markdown","418f5395":"markdown","9f417ecb":"markdown","344a99ba":"markdown","f52839d0":"markdown","a007e52a":"markdown","6c977c8c":"markdown","5151a8e2":"markdown","667f3f23":"markdown","1399988c":"markdown","3fec7eb9":"markdown","06e98907":"markdown","74c3c8c5":"markdown","5d392a78":"markdown","21309b86":"markdown","e6e32c09":"markdown","66506bda":"markdown","b46f85d7":"markdown","8c562dd2":"markdown","1b634725":"markdown","cc2f71d7":"markdown","449f8aa6":"markdown","8e0c7e24":"markdown","166fcf9b":"markdown"},"source":{"4844aef8":"import numpy as np\nimport pandas as pd\nimport pandas_profiling as pp\nimport math\nimport random\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n# preprocessing\nimport sklearn\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler, MinMaxScaler, RobustScaler\nfrom sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold, learning_curve, ShuffleSplit\nfrom sklearn.model_selection import cross_val_predict as cvp\nfrom sklearn import metrics\nfrom sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error, accuracy_score, confusion_matrix, explained_variance_score\nfrom sklearn.feature_selection import VarianceThreshold\nfrom sklearn.feature_selection import SelectFromModel, SelectKBest, RFE, chi2\n\n# models\nfrom sklearn.linear_model import LinearRegression, LogisticRegression, Perceptron, RidgeClassifier, SGDClassifier, LassoCV\nfrom sklearn.svm import SVC, LinearSVC, SVR\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, ExtraTreesClassifier \nfrom sklearn.ensemble import BaggingClassifier, AdaBoostClassifier, VotingClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.gaussian_process import GaussianProcessClassifier\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.tree import DecisionTreeClassifier, plot_tree\nfrom sklearn import metrics\nimport xgboost as xgb\nfrom xgboost import XGBClassifier\nimport lightgbm as lgb\nfrom lightgbm import LGBMClassifier\n\n# NN models\nimport keras\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras import optimizers\nfrom keras.wrappers.scikit_learn import KerasClassifier\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","40cd4c9d":"# Autoviz for automatic EDA\n!pip install autoviz\nfrom autoviz.AutoViz_Class import AutoViz_Class","cc6a0f8f":"cv_n_split = 3\nrandom_state = 40\ntest_train_split_part = 0.2","2148b287":"metrics_all = {1 : 'r2_score', 2: 'acc', 3 : 'rmse', 4 : 're'}\nmetrics_now = [1, 2, 3, 4] # you can only select some numbers of metrics from metrics_all","385755dd":"data = pd.read_csv(\"..\/input\/heart-disease-uci\/heart.csv\")","6b45ca85":"data.head(3)","1eab3f7d":"data.describe([.05, .95])","d204a9f3":"# data = data[(data['chol'] <= 326.9) & (data['oldpeak'] <=3.4)].reset_index(drop=True)\n# data","2a07a175":"data.describe()","17a5fdee":"data.info()","6a72b0d8":"pp.ProfileReport(data)","02a17c43":"data = data.drop_duplicates()\ndata.shape","4b05ae16":"data.describe()","f1e40daa":"data","b47bc21d":"def fe_creation(df):\n    df['age2'] = df['age']\/\/10\n    df['trestbps2'] = df['trestbps']\/\/10 #10\n    df['chol2'] = df['chol']\/\/40\n    df['thalach2'] = df['thalach']\/\/40\n    df['oldpeak2'] = df['oldpeak']\/\/0.4\n    for i in ['sex', 'age2', 'fbs', 'restecg', 'exang','thal', ]:\n        for j in ['cp','trestbps2', 'chol2', 'thalach2', 'oldpeak2', 'slope', 'ca']:\n            df[i + \"_\" + j] = df[i].astype('str') + \"_\" + df[j].astype('str')\n    return df\n\ndata = fe_creation(data)","9cd8bfc8":"pd.set_option('max_columns', len(data.columns)+1)\nlen(data.columns)","bc40a31a":"# Determination categorical features\ncategorical_columns = []\nnumerics = ['int8', 'int16', 'int32', 'int64', 'float16', 'float32', 'float64']\nfeatures = data.columns.values.tolist()\nfor col in features:\n    if data[col].dtype in numerics: continue\n    categorical_columns.append(col)\ncategorical_columns","5e63fe23":"# Encoding categorical features\nfor col in categorical_columns:\n    if col in data.columns:\n        le = LabelEncoder()\n        le.fit(list(data[col].astype(str).values))\n        data[col] = le.transform(list(data[col].astype(str).values))","cb16c354":"data.head(3)","9a18bbb7":"data.shape","1b2dad92":"train = data.copy()\ntarget = train.pop('target')\ntrain.head(2)","53d418a7":"num_features_opt = 25   # the number of features that we need to choose as a result\nnum_features_max = 35   # the somewhat excessive number of features, which we will choose at each stage\nfeatures_best = []","11daae16":"# Threshold for removing correlated variables\nthreshold = 0.9\n\ndef highlight(value):\n    if value > threshold:\n        style = 'background-color: pink'\n    else:\n        style = 'background-color: palegreen'\n    return style\n\n# Absolute value correlation matrix\ncorr_matrix = data.corr().abs().round(2)\nupper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))\nupper.style.format(\"{:.2f}\").applymap(highlight)","bc9c9a62":"# Select columns with correlations above threshold\ncollinear_features = [column for column in upper.columns if any(upper[column] > threshold)]\nfeatures_filtered = data.drop(columns = collinear_features)\nprint('The number of features that passed the collinearity threshold: ', features_filtered.shape[1])\nfeatures_best.append(features_filtered.columns.tolist())","e3161a13":"lsvc = LinearSVC(C=0.1, penalty=\"l1\", dual=False).fit(train, target)\nmodel = SelectFromModel(lsvc, prefit=True)\nX_new = model.transform(train)\nX_selected_df = pd.DataFrame(X_new, columns=[train.columns[i] for i in range(len(train.columns)) if model.get_support()[i]])\nfeatures_best.append(X_selected_df.columns.tolist())","aa5b577f":"lasso = LassoCV(cv=3).fit(train, target)\nmodel = SelectFromModel(lasso, prefit=True)\nX_new = model.transform(train)\nX_selected_df = pd.DataFrame(X_new, columns=[train.columns[i] for i in range(len(train.columns)) if model.get_support()[i]])\nfeatures_best.append(X_selected_df.columns.tolist())","e09099ff":"# Visualization from https:\/\/towardsdatascience.com\/feature-selection-techniques-in-machine-learning-with-python-f24e7da3f36e\n# but to k='all'\nbestfeatures = SelectKBest(score_func=chi2, k='all')\nfit = bestfeatures.fit(train, target)\ndfscores = pd.DataFrame(fit.scores_)\ndfcolumns = pd.DataFrame(train.columns)\n\n#concat two dataframes for better visualization \nfeatureScores = pd.concat([dfcolumns,dfscores],axis=1)\nfeatureScores.columns = ['Feature','Score']  #naming the dataframe columns\nfeatures_best.append(featureScores.nlargest(num_features_max,'Score')['Feature'].tolist())\nprint(featureScores.nlargest(len(dfcolumns),'Score')) ","df397776":"rfe_selector = RFE(estimator=LogisticRegression(), n_features_to_select=num_features_max, step=10, verbose=5)\nrfe_selector.fit(train, target)\nrfe_support = rfe_selector.get_support()\nrfe_feature = train.loc[:,rfe_support].columns.tolist()\nprint(str(len(rfe_feature)), 'selected features')","7036c847":"features_best.append(rfe_feature)","4ceda628":"embeded_rf_selector = SelectFromModel(RandomForestClassifier(n_estimators=200), threshold='1.25*median')\nembeded_rf_selector.fit(train, target)","3e06fa2f":"embeded_rf_support = embeded_rf_selector.get_support()\nembeded_rf_feature = train.loc[:,embeded_rf_support].columns.tolist()\nprint(str(len(embeded_rf_feature)), 'selected features')","cf3c2881":"features_best.append(embeded_rf_feature)","0aa4e123":"# Check whether all features have a sufficiently different meaning\nselector = VarianceThreshold(threshold=10)\nnp.shape(selector.fit_transform(data))\nfeatures_best.append(list(np.array(data.columns)[selector.get_support(indices=False)]))","58ca721c":"features_best","a429357c":"# The element is in at least one list of optimal features\nmain_cols_max = features_best[0]\nfor i in range(len(features_best)-1):\n    main_cols_max = list(set(main_cols_max) | set(features_best[i+1]))\nmain_cols_max","d435363d":"len(main_cols_max)","1f7f1766":"# The element is in all lists of optimal features\nmain_cols_min = features_best[0]\nfor i in range(len(features_best)-1):\n    main_cols_min = list(set(main_cols_min).intersection(set(features_best[i+1])))\nmain_cols_min","aed04658":"# Most common items in all lists of optimal features\nmain_cols = []\nmain_cols_opt = {feature_name : 0 for feature_name in data.columns.tolist()}\nfor i in range(len(features_best)):\n    for feature_name in features_best[i]:\n        main_cols_opt[feature_name] += 1\ndf_main_cols_opt = pd.DataFrame.from_dict(main_cols_opt, orient='index', columns=['Num'])\ndf_main_cols_opt.sort_values(by=['Num'], ascending=False).head(num_features_opt)","6fdd0436":"main_cols = df_main_cols_opt.nlargest(num_features_opt, 'Num').index.tolist()\nif not 'target' in main_cols:\n    main_cols.append('target')\nmain_cols","29b18342":"pd.set_option('max_columns', len(main_cols)+1)\nlen(main_cols)","15ed7986":"data.to_csv('data_EDA.csv', index=False)","b067bc46":"AV = AutoViz_Class()\ndata = pd.read_csv('.\/data_EDA.csv')\ndf = AV.AutoViz(filename=\"\",sep=',', depVar='target', dfte=data, header=0, verbose=2, lowess=False, \n                chart_format='svg',  max_cols_analyzed=30)","a3f2f306":"pp.ProfileReport(data[main_cols])","a4e915a0":"data[main_cols].describe()","ef1dcba7":"# Target\ntarget_name = 'target'\ntarget0 = data[target_name]\ntrain0 = data[main_cols].drop([target_name], axis=1)","3bfe29c9":"# For boosting model\ntrain0b = train0.copy()\n\n# Synthesis valid as \"test\" for selection models\ntrainb, testb, targetb, target_testb = train_test_split(train0b, target0, test_size=test_train_split_part, random_state=random_state)","aa74177f":"# For models from Sklearn\nscaler = MinMaxScaler()\ntrain0 = pd.DataFrame(scaler.fit_transform(train0), columns = train0.columns)\n#scaler2 = StandardScaler()\nscaler2 = RobustScaler()\ntrain0 = pd.DataFrame(scaler2.fit_transform(train0), columns = train0.columns)","1f124fca":"# Synthesis valid as test for selection models\ntrain, test, target, target_test = train_test_split(train0, target0, test_size=test_train_split_part, random_state=random_state)","8cdcb39c":"train.head(3)","977055b4":"test.head(3)","5944ee65":"train.info()","d6634895":"test.info()","947a26d1":"# list of accuracy of all model - amount of metrics_now * 2 (train & test datasets)\nnum_models = 20\nacc_train = []\nacc_test = []\nacc_all = np.empty((len(metrics_now)*2, 0)).tolist()\nacc_all","93e524eb":"acc_all_pred = np.empty((len(metrics_now), 0)).tolist()\nacc_all_pred","e6ae0e65":"# Splitting train data for model tuning with cross-validation\ncv_train = ShuffleSplit(n_splits=cv_n_split, test_size=test_train_split_part, random_state=random_state)","ef162216":"def acc_d(y_meas, y_pred):\n    # Relative error between predicted y_pred and measured y_meas values\n    return mean_absolute_error(y_meas, y_pred)*len(y_meas)\/sum(abs(y_meas))\n\ndef acc_rmse(y_meas, y_pred):\n    # RMSE between predicted y_pred and measured y_meas values\n    return (mean_squared_error(y_meas, y_pred))**0.5","c32aafcd":"def plot_cm(target, train_pred, target_test, test_pred):\n    # Building the confusion matrices\n    \n    def cm_calc(y_true, y_pred):\n        cm = confusion_matrix(y_true, y_pred, labels=np.unique(y_true))\n        cm_sum = np.sum(cm, axis=1, keepdims=True)\n        cm_perc = cm \/ cm_sum.astype(float) * 100\n        annot = np.empty_like(cm).astype(str)\n        nrows, ncols = cm.shape\n        for i in range(nrows):\n            for j in range(ncols):\n                c = cm[i, j]\n                p = cm_perc[i, j]\n                if i == j:\n                    s = cm_sum[i]\n                    annot[i, j] = '%.1f%%\\n%d\/%d' % (p, c, s)\n                elif c == 0:\n                    annot[i, j] = ''\n                else:\n                    annot[i, j] = '%.1f%%\\n%d' % (p, c)\n        cm = pd.DataFrame(cm, index=np.unique(y_true), columns=np.unique(y_true))\n        cm.index.name = 'Actual'\n        cm.columns.name = 'Predicted'\n        return cm, annot\n\n    \n    # Building the confusion matrices\n    fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(16, 6), sharex=True)\n    \n    # Training data\n    ax = axes[0]\n    ax.set_title(\"for training data\")\n    cm0, annot0 = cm_calc(target, train_pred)    \n    sns.heatmap(cm0, cmap= \"YlGnBu\", annot=annot0, fmt='', ax=ax)\n    \n    # Test data\n    ax = axes[1]\n    ax.set_title(\"for test (validation) data\")\n    cm1, annot1 = cm_calc(target_test, test_pred)\n    sns.heatmap(cm1, cmap= \"YlGnBu\", annot=annot1, fmt='', ax=ax)\n    \n    fig.suptitle('CONFUSION MATRICES')\n    plt.show()","5fe09441":"def acc_metrics_calc(num,model,train,test,target,target_test):\n    # The models selection stage\n    # Calculation of accuracy of model by different metrics\n    global acc_all\n\n    ytrain = model.predict(train).astype(int)\n    ytest = model.predict(test).astype(int)\n    if num != 17:\n        print('target = ', target[:5].values)\n        print('ytrain = ', ytrain[:5])\n        print('target_test =', target_test[:5].values)\n        print('ytest =', ytest[:5])\n\n    num_acc = 0\n    for x in metrics_now:\n        if x == 1:\n            #r2_score criterion\n            acc_train = round(r2_score(target, ytrain) * 100, 2)\n            acc_test = round(r2_score(target_test, ytest) * 100, 2)\n        elif x == 2:\n            #accuracy_score criterion\n            acc_train = round(metrics.accuracy_score(target, ytrain) * 100, 2)\n            acc_test = round(metrics.accuracy_score(target_test, ytest) * 100, 2)\n        elif x == 3:\n            #rmse criterion\n            acc_train = round(acc_rmse(target, ytrain) * 100, 2)\n            acc_test = round(acc_rmse(target_test, ytest) * 100, 2)\n        elif x == 4:\n            #relative error criterion\n            acc_train = round(acc_d(target, ytrain) * 100, 2)\n            acc_test = round(acc_d(target_test, ytest) * 100, 2)\n        \n        print('acc of', metrics_all[x], 'for train =', acc_train)\n        print('acc of', metrics_all[x], 'for test =', acc_test)\n        acc_all[num_acc].append(acc_train) #train\n        acc_all[num_acc+1].append(acc_test) #test\n        num_acc += 2\n    \n    #  Building the confusion matrices\n    plot_cm(target, ytrain, target_test, ytest)","a7c2e889":"def acc_metrics_calc_pred(num,model,name_model,train,test,target):\n    # The prediction stage\n    # Calculation of accuracy of model for all different metrics and creates of the main submission file for the best model (num=0)\n    global acc_all_pred\n\n    ytrain = model.predict(train).astype(int)\n    ytest = model.predict(test).astype(int)\n\n    print('**********')\n    print(name_model)\n    if num != 17:\n        print('target = ', target[:15].values)\n        print('ytrain = ', ytrain[:15])\n        print('ytest =', ytest[:15])\n    \n    num_acc = 0\n    for x in metrics_now:\n        if x == 1:\n            #r2_score criterion\n            acc_train = round(r2_score(target, ytrain) * 100, 2)\n        elif x == 2:\n            #accuracy_score criterion\n            acc_train = round(metrics.accuracy_score(target, ytrain) * 100, 2)\n        elif x == 3:\n            #rmse criterion\n            acc_train = round(acc_rmse(target, ytrain) * 100, 2)\n        elif x == 4:\n            #relative error criterion\n            acc_train = round(acc_d(target, ytrain) * 100, 2)\n\n        print('acc of', metrics_all[x], 'for train =', acc_train)\n        acc_all_pred[num_acc].append(acc_train) #train\n        num_acc += 1\n    \n    # Save the submission file\n    submission[target_name] = ytest\n    submission.to_csv('submission_' + name_model + '.csv', index=False)    ","22fbdf26":"# Thanks to https:\/\/scikit-learn.org\/stable\/auto_examples\/model_selection\/plot_learning_curve.html#sphx-glr-auto-examples-model-selection-plot-learning-curve-py\ndef plot_learning_curve(estimator, title, X, y, cv=None, axes=None, ylim=None, \n                        n_jobs=None, train_sizes=np.linspace(.1, 1.0, 5), random_state=0):\n    \"\"\"\n    Generate 2 plots: \n    - the test and training learning curve, \n    - the training samples vs fit times curve.\n\n    Parameters\n    ----------\n    estimator : object type that implements the \"fit\" and \"predict\" methods\n        An object of that type which is cloned for each validation.\n\n    title : string\n        Title for the chart.\n\n    X : array-like, shape (n_samples, n_features)\n        Training vector, where n_samples is the number of samples and\n        n_features is the number of features.\n\n    y : array-like, shape (n_samples) or (n_samples, n_features), optional\n        Target relative to X for classification or regression;\n        None for unsupervised learning.\n\n    axes : array of 3 axes, optional (default=None)\n        Axes to use for plotting the curves.\n\n    ylim : tuple, shape (ymin, ymax), optional\n        Defines minimum and maximum yvalues plotted.\n\n    cv : int, cross-validation generator or an iterable, optional\n        Determines the cross-validation splitting strategy.\n        Possible inputs for cv are:\n\n          - None, to use the default 5-fold cross-validation,\n          - integer, to specify the number of folds.\n          - :term:`CV splitter`,\n          - An iterable yielding (train, test) splits as arrays of indices.\n\n        For integer\/None inputs, if ``y`` is binary or multiclass,\n        :class:`StratifiedKFold` used. If the estimator is not a classifier\n        or if ``y`` is neither binary nor multiclass, :class:`KFold` is used.\n\n        Refer :ref:`User Guide <cross_validation>` for the various\n        cross-validators that can be used here.\n\n    train_sizes : array-like, shape (n_ticks,), dtype float or int\n        Relative or absolute numbers of training examples that will be used to\n        generate the learning curve. If the dtype is float, it is regarded as a\n        fraction of the maximum size of the training set (that is determined\n        by the selected validation method), i.e. it has to be within (0, 1].\n        Otherwise it is interpreted as absolute sizes of the training sets.\n        Note that for classification the number of samples usually have to\n        be big enough to contain at least one sample from each class.\n        (default: np.linspace(0.1, 1.0, 5))\n    \n    random_state : random_state\n    \n    \"\"\"\n    fig, axes = plt.subplots(2, 1, figsize=(20, 10))\n    \n    if axes is None:\n        _, axes = plt.subplots(1, 2, figsize=(20, 5))\n\n    axes[0].set_title(title)\n    if ylim is not None:\n        axes[0].set_ylim(*ylim)\n    axes[0].set_xlabel(\"Training examples\")\n    axes[0].set_ylabel(\"Score\")\n\n    cv_train = ShuffleSplit(n_splits=cv_n_split, test_size=test_train_split_part, random_state=random_state)\n    \n    train_sizes, train_scores, test_scores, fit_times, _ = \\\n        learning_curve(estimator=estimator, X=X, y=y, cv=cv,\n                       train_sizes=train_sizes,\n                       return_times=True)\n    train_scores_mean = np.mean(train_scores, axis=1)\n    train_scores_std = np.std(train_scores, axis=1)\n    test_scores_mean = np.mean(test_scores, axis=1)\n    test_scores_std = np.std(test_scores, axis=1)\n    fit_times_mean = np.mean(fit_times, axis=1)\n    fit_times_std = np.std(fit_times, axis=1)\n\n    # Plot learning curve\n    axes[0].grid()\n    axes[0].fill_between(train_sizes, train_scores_mean - train_scores_std,\n                         train_scores_mean + train_scores_std, alpha=0.1,\n                         color=\"r\")\n    axes[0].fill_between(train_sizes, test_scores_mean - test_scores_std,\n                         test_scores_mean + test_scores_std, alpha=0.1,\n                         color=\"g\")\n    axes[0].plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n                 label=\"Training score\")\n    axes[0].plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n                 label=\"Cross-validation score\")\n    axes[0].legend(loc=\"best\")\n\n    # Plot n_samples vs fit_times\n    axes[1].grid()\n    axes[1].plot(train_sizes, fit_times_mean, 'o-')\n    axes[1].fill_between(train_sizes, fit_times_mean - fit_times_std,\n                         fit_times_mean + fit_times_std, alpha=0.1)\n    axes[1].set_xlabel(\"Training examples\")\n    axes[1].set_ylabel(\"fit_times\")\n    axes[1].set_title(\"Scalability of the model\")\n\n    plt.show()\n    return","fb7c41ae":"# Linear Regression\nlinreg = LinearRegression()\nlinreg_CV = GridSearchCV(linreg, param_grid={}, cv=cv_train, verbose=False)\nlinreg_CV.fit(train, target)\nacc_metrics_calc(0,linreg_CV,train,test,target,target_test)","c4a8f743":"# Building learning curve of model\nplot_learning_curve(linreg, \"Linear Regression\", train, target, cv=cv_train)","10fa763d":"# Support Vector Machines\n\nsvr = SVC()\nsvr_CV = GridSearchCV(svr, param_grid={'kernel': ['linear', 'poly', 'rbf', 'sigmoid'],\n                                       'tol': [1e-3]}, \n                      cv=cv_train, verbose=False)\nsvr_CV.fit(train, target)\nprint(svr_CV.best_params_)\nacc_metrics_calc(1,svr_CV,train,test,target,target_test)","ae657725":"# Building learning curve of model\nplot_learning_curve(svr, \"Support Vector Machines\", train, target, cv=cv_train)","156c54eb":"# Linear SVR\n\nlinear_svc = LinearSVC()\nparam_grid = {'dual':[False],\n              'C': np.linspace(1, 15, 15)}\nlinear_svc_CV = GridSearchCV(linear_svc, param_grid=param_grid, cv=cv_train, verbose=False)\nlinear_svc_CV.fit(train, target)\nprint(linear_svc_CV.best_params_)\nacc_metrics_calc(2,linear_svc_CV,train,test,target,target_test)","7397e26b":"# Building learning curve of model\nplot_learning_curve(linear_svc, \"Linear SVR\", train, target, cv=cv_train)","487dac2a":"%%time\n# MLPClassifier\n\nmlp = MLPClassifier()\nparam_grid = {'hidden_layer_sizes': [i for i in range(2,5)],\n              'solver': ['sgd'],\n              'learning_rate': ['adaptive'],\n              'max_iter': [1000]\n              }\nmlp_GS = GridSearchCV(mlp, param_grid=param_grid, cv=cv_train, verbose=False)\nmlp_GS.fit(train, target)\nprint(mlp_GS.best_params_)\nacc_metrics_calc(3,mlp_GS,train,test,target,target_test)","33c46f12":"# Building learning curve of model\nplot_learning_curve(mlp, \"MLP Classifier\", train, target, cv=cv_train)","513d9780":"# Stochastic Gradient Descent\n\nsgd = SGDClassifier(early_stopping=True)\nparam_grid = {'alpha': [0.035, 0.04, 0.45]}\nsgd_CV = GridSearchCV(sgd, param_grid=param_grid, cv=cv_train, verbose=False)\nsgd_CV.fit(train, target)\nprint(sgd_CV.best_params_)\nacc_metrics_calc(4,sgd_CV,train,test,target,target_test)","a791d6f7":"# Building learning curve of model\nplot_learning_curve(sgd, \"Stochastic Gradient Descent\", train, target, cv=cv_train)","dce45609":"# Decision Tree Classifier\n\ndecision_tree = DecisionTreeClassifier()\nparam_grid = {'min_samples_leaf': [i for i in range(2,10)]}\ndecision_tree_CV = GridSearchCV(decision_tree, param_grid=param_grid, cv=cv_train, verbose=False)\ndecision_tree_CV.fit(train, target)\nprint(decision_tree_CV.best_params_)\nacc_metrics_calc(5,decision_tree_CV,train,test,target,target_test)","ab668f5f":"# Building learning curve of model\nplot_learning_curve(decision_tree_CV, \"Decision Tree\", train, target, cv=cv_train)","28c53f81":"%%time\n# Random Forest\n# Parameters of model (param_grid) taken from the notebook https:\/\/www.kaggle.com\/morenovanton\/titanic-random-forest\n\nrandom_forest = RandomForestClassifier()\nparam_grid = {'n_estimators': [40, 50, 60], 'min_samples_split': [40, 50, 60, 70], 'min_samples_leaf': [12, 13, 14, 15, 16, 17], \n              'max_features': ['auto'], 'max_depth': [3, 4, 5, 6], 'criterion': ['gini'], 'bootstrap': [False]}\nrandom_forest_CV = GridSearchCV(estimator=random_forest, param_grid=param_grid, \n                             cv=cv_train, verbose=False)\nrandom_forest_CV.fit(train, target)\nprint(random_forest_CV.best_params_)\nacc_metrics_calc(6,random_forest_CV,train,test,target,target_test)","a3fdcd1e":"# Building learning curve of model\nplot_learning_curve(random_forest, \"Random Forest\", train, target, cv=cv_train)","7f05c14f":"%%time\n# XGBoost Classifier\nxgb_clf = xgb.XGBClassifier(objective='reg:squarederror') \nparameters = {'n_estimators': [50, 60, 70, 80, 90], \n              'learning_rate': [0.09, 0.1, 0.15, 0.2],\n              'max_depth': [3, 4, 5]}\nxgb_reg = GridSearchCV(estimator=xgb_clf, param_grid=parameters, cv=cv_train).fit(trainb, targetb)\nprint(\"Best score: %0.3f\" % xgb_reg.best_score_)\nprint(\"Best parameters set:\", xgb_reg.best_params_)\nacc_metrics_calc(7,xgb_reg,trainb,testb,targetb,target_testb)","66db1d86":"# Building learning curve of model\nplot_learning_curve(xgb_clf, \"XGBoost Classifier\", trainb, targetb, cv=cv_train)","1461ddde":"#%% split training set to validation set\nXtrain, Xval, Ztrain, Zval = train_test_split(trainb, targetb, test_size=test_train_split_part, random_state=random_state)\nmodelL = lgb.LGBMClassifier(n_estimators=1000, num_leaves=40)\nmodelL.fit(Xtrain, Ztrain, eval_set=[(Xval, Zval)], early_stopping_rounds=50, verbose=True)","2012ccd3":"acc_metrics_calc(8,modelL,trainb,testb,targetb,target_testb)","e2288599":"fig =  plt.figure(figsize = (10,10))\naxes = fig.add_subplot(111)\nlgb.plot_importance(modelL,ax = axes,height = 0.5)\nplt.show();\nplt.close()","f94073f4":"# Gradient Boosting Classifier\n\ngradient_boosting = GradientBoostingClassifier()\nparam_grid = {'learning_rate' : [0.05, 0.06, 0.07, 0.08, 0.09],\n              'max_depth': [i for i in range(2,5)],\n              'min_samples_leaf': [i for i in range(3,10)]}\ngradient_boosting_CV = GridSearchCV(estimator=gradient_boosting, param_grid=param_grid, \n                                    cv=cv_train, verbose=False)\ngradient_boosting_CV.fit(train, target)\nprint(gradient_boosting_CV.best_params_)\nacc_metrics_calc(9,gradient_boosting_CV,train,test,target,target_test)","dccd63ab":"# Building learning curve of model\nplot_learning_curve(gradient_boosting_CV, \"Gradient Boosting Classifier\", train, target, cv=cv_train)","ee2d54bf":"# Ridge Classifier\n\nridge = RidgeClassifier()\nridge_CV = GridSearchCV(estimator=ridge, param_grid={'alpha': np.linspace(.1, 1.5, 15)}, cv=cv_train, verbose=False)\nridge_CV.fit(train, target)\nprint(ridge_CV.best_params_)\nacc_metrics_calc(10,ridge_CV,train,test,target,target_test)","a76f258f":"# Building learning curve of model\nplot_learning_curve(ridge_CV, \"Ridge Classifier\", train, target, cv=cv_train)","15847edb":"# Bagging Classifier\n\nbagging = BaggingClassifier()\nparam_grid={'max_features': [0.85, 0.9, 0.95],\n            'n_estimators': [3, 4, 5],\n            'warm_start' : [False],\n            'random_state': [random_state]}\nbagging_CV = GridSearchCV(estimator=bagging, param_grid=param_grid, cv=cv_train, verbose=False)\nbagging_CV.fit(train, target)\nprint(bagging_CV.best_params_)\nacc_metrics_calc(11,bagging_CV,train,test,target,target_test)","1a7804aa":"# Building learning curve of model\nplot_learning_curve(bagging_CV, \"Bagging Classifier\", train, target, cv=cv_train)","858dac71":"# Extra Trees Classifier\n\netr = ExtraTreesClassifier()\netr_CV = GridSearchCV(estimator=etr, param_grid={'min_samples_leaf' : [11, 12, 13, 14]}, cv=cv_train, verbose=False)\netr_CV.fit(train, target)\nprint(etr_CV.best_params_)\nacc_metrics_calc(12,etr_CV,train,test,target,target_test)","be6e70eb":"# Building learning curve of model\nplot_learning_curve(etr, \"Extra Trees Classifier\", train, target, cv=cv_train)","257feeab":"# AdaBoost Classifier\n\nAda_Boost = AdaBoostClassifier()\nAda_Boost_CV = GridSearchCV(estimator=Ada_Boost, param_grid={'learning_rate' : [0.095, 0.1, 0.101, 0.102, 0.105]}, cv=cv_train, verbose=False)\nAda_Boost_CV.fit(train, target)\nprint(Ada_Boost_CV.best_params_)\nacc_metrics_calc(13,Ada_Boost_CV,train,test,target,target_test)","5d458d02":"# Building learning curve of model\nplot_learning_curve(Ada_Boost, \"AdaBoost Classifier\", train, target, cv=cv_train)","019618df":"# Logistic Regression\n\nlogreg = LogisticRegression()\nlogreg_CV = GridSearchCV(estimator=logreg, param_grid={'C' : [.2, .3, .4]}, cv=cv_train, verbose=False)\nlogreg_CV.fit(train, target)\nprint(logreg_CV.best_params_)\nacc_metrics_calc(14,logreg_CV,train,test,target,target_test)","6de48fee":"# Building learning curve of model\nplot_learning_curve(logreg, \"Logistic Regression\", train, target, cv=cv_train)","c01b6c90":"# KNN - k-Nearest Neighbors algorithm\n\nknn = KNeighborsClassifier()\nknn_CV = GridSearchCV(estimator=knn, param_grid={'n_neighbors': range(2, 7)}, \n                      cv=cv_train, verbose=False).fit(train, target)\nprint(knn_CV.best_params_)\nacc_metrics_calc(15,knn_CV,train,test,target,target_test)","a9a682f5":"# Building learning curve of model\nplot_learning_curve(knn, \"KNN\", train, target, cv=cv_train)","6936c13d":"# Gaussian Naive Bayes\n\ngaussian = GaussianNB()\nparam_grid={'var_smoothing': [1e-4, 1e-5, 1e-6]}\ngaussian_CV = GridSearchCV(estimator=gaussian, param_grid=param_grid, cv=cv_train, verbose=False)\ngaussian_CV.fit(train, target)\nprint(gaussian_CV.best_params_)\nacc_metrics_calc(16,gaussian_CV,train,test,target,target_test)","1786ac0f":"# Building learning curve of model\nplot_learning_curve(gaussian, \"Gaussian Naive Bayes\", train, target, cv=cv_train)","6d241cbe":"# Thanks to https:\/\/www.kaggle.com\/skrudals\/modification-of-neural-network-around-90\ndef build_nn(optimizer='adam'):\n\n    # Initializing the NN\n    nn = Sequential()\n\n    # Adding the input layer and the first hidden layer of the NN\n    nn.add(Dense(units=32, kernel_initializer='he_normal', activation='relu', input_shape=(len(train0.columns),)))\n    # Adding the output layer\n    nn.add(Dense(units=1, kernel_initializer='he_normal', activation='sigmoid'))\n\n    # Compiling the NN\n    nn.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n\n    return nn\n\nXtrain, Xval, Ztrain, Zval = train_test_split(train, target, test_size=test_train_split_part, random_state=random_state)\nnn_model = build_nn(optimizers.Adam(lr=0.0001))\nnn_model.fit(Xtrain, Ztrain, batch_size=16, epochs=100, validation_data=(Xval, Zval))\nacc_metrics_calc(17,nn_model,train,test,target,target_test)","5e8c065d":"# Gaussian Process Classification\n\ngpc = GaussianProcessClassifier()\nparam_grid = {'max_iter_predict': [70, 80, 90],\n              'warm_start': [False],\n              'n_restarts_optimizer': range(2,4)}\ngpc_CV = GridSearchCV(estimator=gpc, param_grid=param_grid, cv=cv_train, verbose=False)\ngpc_CV.fit(train, target)\nprint(gpc_CV.best_params_)\nacc_metrics_calc(18,gpc_CV,train,test,target,target_test)","814b2b2a":"# Building learning curve of model\nplot_learning_curve(gpc, \"Gaussian Process Classification\", train, target, cv=cv_train)","0c092213":"# Voting Classifier\n\nVoting_ens = VotingClassifier(estimators=[('log', logreg_CV), ('mlp', mlp_GS ), ('svc', linear_svc_CV)])\nVoting_ens.fit(train, target)\nacc_metrics_calc(19,Voting_ens,train,test,target,target_test)","f3014e46":"models = pd.DataFrame({\n    'Model': ['Linear Regression', 'Support Vector Machines', 'Linear SVC', \n              'MLP Classifier', 'Stochastic Gradient Decent', \n              'Decision Tree Classifier', 'Random Forest Classifier',  'XGB Classifier', 'LGBM Classifier',\n              'Gradient Boosting Classifier', 'RidgeCV', 'Bagging Classifier', 'ExtraTrees Classifier', \n              'AdaBoost Classifier', 'Logistic Regression',\n              'KNN', 'Naive Bayes', 'NN model', 'Gaussian Process Classification',\n              'VotingClassifier']})","cc6e15e4":"for x in metrics_now:\n    xs = metrics_all[x]\n    models[xs + '_train'] = acc_all[(x-1)*2]\n    models[xs + '_test'] = acc_all[(x-1)*2+1]\n    if xs == \"acc\":\n        models[xs + '_diff'] = models[xs + '_train'] - models[xs + '_test']\n#models","900ba335":"print('Prediction accuracy for models')\nms = metrics_all[metrics_now[1]] # the first from metrics\nmodels[['Model', ms + '_train', ms + '_test', 'acc_diff']].sort_values(by=[(ms + '_test'), (ms + '_train')], ascending=False)","12518131":"pd.options.display.float_format = '{:,.2f}'.format","34a837c3":"for x in metrics_now:   \n    # Plot\n    xs = metrics_all[x]\n    xs_train = metrics_all[x] + '_train'\n    xs_test = metrics_all[x] + '_test'\n    plt.figure(figsize=[15,6])\n    xx = models['Model']\n    plt.tick_params(labelsize=14)\n    plt.plot(xx, models[xs_train], label = xs_train)\n    plt.plot(xx, models[xs_test], label = xs_test)\n    plt.legend()\n    plt.title(str(xs) + ' criterion for ' + str(num_models) + ' popular models for train and test datasets')\n    plt.xlabel('Models')\n    plt.ylabel(xs + ', %')\n    plt.xticks(xx, rotation='vertical')\n    plt.show()","77075638":"# Choose the number of metric by which the best models will be determined =>  {1 : 'r2_score', 2: 'accuracy_score', 3 : 'relative_error', 4 : 'rmse'}\nmetrics_main = 2 \nxs = metrics_all[metrics_main]\nxs_train = metrics_all[metrics_main] + '_train'\nxs_test = metrics_all[metrics_main] + '_test'\nprint('The best models by the',xs,'criterion:')\ndirect_sort = False if (metrics_main >= 2) else True\nmodels_sort = models.sort_values(by=[xs_test, xs_train], ascending=direct_sort)","acc0e7e5":"# Selection the best models except VotingClassifier\nmodels_best = models_sort[(models_sort.acc_diff < 10) & (models_sort.acc_test > 86)]\nmodels_best[['Model', ms + '_train', ms + '_test']].sort_values(by=['acc_test'], ascending=False)","eaf037e3":"# Selection the best models from the best\nmodels_best_best = models_best[(models_best.acc_test > 90)]\nmodels_best_best[['Model', ms + '_train', ms + '_test']].sort_values(by=['acc_test'], ascending=False)","bb891f06":"### 5.6 Decision Tree Classifier<a class=\"anchor\" id=\"5.6\"><\/a>\n\n[Back to Table of Contents](#0.1)","fb27058e":"**XGBoost** is an ensemble tree method that apply the principle of boosting weak learners (CARTs generally) using the gradient descent architecture. XGBoost improves upon the base Gradient Boosting Machines (GBM) framework through systems optimization and algorithmic enhancements. Reference [Towards Data Science.](https:\/\/towardsdatascience.com\/https-medium-com-vishalmorde-xgboost-algorithm-long-she-may-rein-edd9f99be63d)","b92e50a5":"### 5.7 Random Forest Classifier<a class=\"anchor\" id=\"5.7\"><\/a>\n\n[Back to Table of Contents](#0.1)","10812fc6":"**Linear SVC** is a similar to SVM method. Its also builds on kernel functions but is appropriate for unsupervised learning. Reference [Wikipedia](https:\/\/en.wikipedia.org\/wiki\/Support-vector_machine#Support-vector_clustering_(svr).","6ce3ce68":"<a class=\"anchor\" id=\"0\"><\/a>\n\n# Advanced EDA, FE and selection the best from the 20 popular models with visualization in the dataset [Heart Disease UCI data](https:\/\/www.kaggle.com\/ronitf\/heart-disease-uci)","4e5bda93":"The analysis showed that the available features are poorly divided according to the target values. It is advisable to generate a number of new features.","7e4e05e2":"**Stochastic gradient descent** (often abbreviated **SGD**) is an iterative method for optimizing an objective function with suitable smoothness properties (e.g. differentiable or subdifferentiable). It can be regarded as a stochastic approximation of gradient descent optimization, since it replaces the actual gradient (calculated from the entire data set) by an estimate thereof (calculated from a randomly selected subset of the data). Especially in big data applications this reduces the computational burden, achieving faster iterations in trade for a slightly lower convergence rate. Reference [Wikipedia](https:\/\/en.wikipedia.org\/wiki\/Stochastic_gradient_descent).","8b4ee96b":"**Random Forest** is one of the most popular model. Random forests or random decision forests are an ensemble learning method for classification, regression and other tasks, that operate by constructing a multitude of decision trees (n_estimators= [100, 300]) at training time and outputting the class that is the mode of the classes (classification) or mean prediction (regression) of the individual trees. Reference [Wikipedia](https:\/\/en.wikipedia.org\/wiki\/Random_forest).","a97a8c17":"**Light GBM** is a fast, distributed, high-performance gradient boosting framework based on decision tree algorithms. It splits the tree leaf wise with the best fit whereas other boosting algorithms split the tree depth wise or level wise rather than leaf-wise. So when growing on the same leaf in Light GBM, the leaf-wise algorithm can reduce more loss than the level-wise algorithm and hence results in much better accuracy which can rarely be achieved by any of the existing boosting algorithms. Also, it is surprisingly very fast, hence the word \u2018Light\u2019. Reference [Analytics Vidhya](https:\/\/www.analyticsvidhya.com\/blog\/2017\/06\/which-algorithm-takes-the-crown-light-gbm-vs-xgboost\/).","86a58792":"### 5.20 Voting Classifier <a class=\"anchor\" id=\"5.20\"><\/a>\n\n[Back to Table of Contents](#0.1)","dd664e4c":"I hope you find this kernel useful and enjoyable.","defa1d75":"### 5.11 Ridge Classifier <a class=\"anchor\" id=\"5.11\"><\/a>\n\n[Back to Table of Contents](#0.1)","f743aea0":"### 3.3.2. Pandas Profiling<a class=\"anchor\" id=\"3.3.2\"><\/a>\n\n[Back to Table of Contents](#0.1)","f3acea7b":"### 3.3.1. AutoViz<a class=\"anchor\" id=\"3.3.1\"><\/a>\n\n[Back to Table of Contents](#0.1)","9ab4b7a5":"### 5.3 Linear SVC <a class=\"anchor\" id=\"5.3\"><\/a>\n\n[Back to Table of Contents](#0.1)","6158e480":"### 3.2.2. Feature selection<a class=\"anchor\" id=\"3.2.2\"><\/a>\n\n[Back to Table of Contents](#0.1)","694b3b3c":"### 5.15 Logistic Regression <a class=\"anchor\" id=\"5.15\"><\/a>\n\n[Back to Table of Contents](#0.1)","77a4f36a":"Thanks to https:\/\/www.kaggle.com\/liananapalkova\/automated-feature-engineering-for-titanic-dataset","339e450a":"### 3.2.2.7 FS by the VarianceThreshold<a class=\"anchor\" id=\"3.2.2.7\"><\/a>\n\n[Back to Table of Contents](#0.1)","63eb4774":"### 5.14 AdaBoost Classifier <a class=\"anchor\" id=\"5.14\"><\/a>\n\n[Back to Table of Contents](#0.1)","5890b510":"**Logistic Regression** is a useful model to run early in the workflow. Logistic regression measures the relationship between the categorical dependent variable (feature) and one or more independent variables (features) by estimating probabilities using a logistic function, which is the cumulative logistic distribution. Reference [Wikipedia](https:\/\/en.wikipedia.org\/wiki\/Logistic_regression).","1aad150e":"### 5.1 Linear Regression <a class=\"anchor\" id=\"5.1\"><\/a>\n\n[Back to Table of Contents](#0.1)","60f48650":"## 2. Download datasets <a class=\"anchor\" id=\"2\"><\/a>\n\n[Back to Table of Contents](#0.1)","3903c174":"## Dataset [Heart Disease UCI](https:\/\/www.kaggle.com\/ronitf\/heart-disease-uci): the application of advanced techniques for automation EDA & FE & Model selection with visualization \n\n(In more detail, the technology and results of modeling and forecasting are described in my post: [Automation and visualization of EDA & FE & Model selection](https:\/\/www.kaggle.com\/getting-started\/187917) in **\"Getting Started\"** Discussion)\n\n### I. Feature engineering (FE)\nNew features and different combinations of feature pairs are formed. \nThere are many techniques for **selection features**, see the example:\n- [a collection of notebooks for FE](https:\/\/www.kaggle.com\/vbmokin\/data-science-for-tabular-data-advanced-techniques#3)\n- [Titanic - Featuretools (automatic FE&FS)](https:\/\/www.kaggle.com\/vbmokin\/titanic-featuretools-automatic-fe-fs)\n- [sklearn library documentation](https:\/\/scikit-learn.org\/stable\/modules\/classes.html#module-sklearn.feature_selection)\n\n             \nI apply the 7 techniques of features selection and **automatic selection** the best features from them:\n - by the Pearson correlation\n - by the SelectFromModel with LinearSVC\n - by the SelectFromModel with Lasso\n - by the SelectKBest with Chi-2\n - by the Recursive Feature Elimination (RFE) with Logistic Regression\n - by the Recursive Feature Elimination (RFE) with Random Forest\n - by the VarianceThreshold\n\n\n### II. Automatic EDA\nI used packages and methods of automatic EDA with good visualization:\n\n* AV.AutoViz\n* pandas-profiling.ProfileReport\n* pandas.describe\n\n\n### III. Preprocessing \nFor models from Sklearn library, **scaling and standardization** are applied.\n\n\n### IV. Model selection \nModeling is carried out with 20 model-classifier (with tuning and cross-validation):\n- Linear Regression, Logistic Regression\n- Naive Bayes \n- k-Nearest Neighbors algorithm\n- Neural network with Keras\n- Support Vector Machines and Linear SVC\n- Stochastic Gradient Descent, Gradient Boosting Classifier, RidgeCV, Bagging Classifier\n- Decision Tree Classifier, Random Forest Classifier, AdaBoost Classifier, XGB Classifier, LGBM Classifier, ExtraTrees Classifier \n- Gaussian Process Classification\n- MLP Classifier (Deep Learning)\n- Voting Classifier\n\nFor each model, the following are calculated and built:\n- **learning curve plot**\n- **confusion matrices** for train and test data\n\n4 metrics are automatically calculated for each model and the best models are selected with the highest accuracy on test data and the smallest (less 10) difference between the forecast accuracy of the training and test data at the same time - this choice of model reduces the risks of choosing a model with overfitting.\n\nYour comments, votes and feedback are most welcome.","8df5124d":"The next code from in my kernel [FE & EDA with Pandas Profiling](https:\/\/www.kaggle.com\/vbmokin\/fe-eda-with-pandas-profiling)","61825daf":"### 5.12 BaggingClassifier <a class=\"anchor\" id=\"5.12\"><\/a>\n\n[Back to Table of Contents](#0.1)","b2815fab":"### 5.2 Support Vector Machines <a class=\"anchor\" id=\"5.2\"><\/a>\n\n[Back to Table of Contents](#0.1)","d5e3f96d":"### 5.13 Extra Trees Classifier <a class=\"anchor\" id=\"5.13\"><\/a>\n\n[Back to Table of Contents](#0.1)","123bd2d3":"Thanks to [sklearn.feature_selection.VarianceThreshold](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.feature_selection.VarianceThreshold.html#sklearn.feature_selection.VarianceThreshold) - Feature selector that removes all low-variance features.","6bacef31":"### 3.2.2.8 Selection the best features<a class=\"anchor\" id=\"3.2.2.8\"><\/a>\n\n[Back to Table of Contents](#0.1)","19af3ebd":"### The best models:","3dc1d5fc":"<a class=\"anchor\" id=\"0.1\"><\/a>\n\n## Table of Contents\n\n1. [Import libraries](#1)\n1. [Download datasets](#2)\n1. [EDA & FE](#3)\n    -  [Initial EDA - for FE](#3.1)\n    -  [FE](#3.2)\n        -  [Feature creation](#3.2.1)\n        -  [Automatic feature selection (FS)](#3.2.2)\n             -  [FS with the Pearson correlation](#3.2.2.1)\n             -  [FS by the SelectFromModel with LinearSVC](#3.2.2.2) \n             -  [FS by the SelectFromModel with Lasso](#3.2.2.3) \n             -  [FS by the SelectKBest with Chi-2](#3.2.2.4)\n             -  [FS by the Recursive Feature Elimination (RFE) with Logistic Regression](#3.2.2.5) \n             -  [FS by the Recursive Feature Elimination (RFE) with Random Forest](#3.2.2.6)\n             -  [FS by the VarianceThreshold](#3.2.2.7)             \n             -  [Selection the best features](#3.2.2.8)\n    -  [EDA - for Model selection](#3.3)\n        -  [AutoViz](#3.3.1)\n        -  [Pandas Profiling](#3.3.2)\n        -  [Pandas Describe](#3.3.3)\n1. [Preparing to modeling](#4)\n1. [Tuning models with GridSearchCV](#5)\n    -  [Linear Regression](#5.1)\n    -  [Support Vector Machines](#5.2)\n    -  [Linear SVC](#5.3)\n    -  [MLP Classifier](#5.4)\n    -  [Stochastic Gradient Descent](#5.5)\n    -  [Decision Tree Classifier](#5.6)\n    -  [Random Forest Classifier](#5.7)\n    -  [XGB Classifier](#5.8)\n    -  [LGBM Classifier](#5.9)\n    -  [Gradient Boosting Classifier](#5.10)\n    -  [Ridge Classifier](#5.11)\n    -  [Bagging Classifier](#5.12)\n    -  [Extra Trees Classifier](#5.13)\n    -  [AdaBoost Classifier](#5.14)\n    -  [Logistic Regression](#5.15)\n    -  [k-Nearest Neighbors (KNN)](#5.16)\n    -  [Naive Bayes](#5.17)\n    -  [Neural network (NN) with Keras](#5.18)\n    -  [Gaussian Process Classification](#5.19)\n    -  [Voting Classifier](#5.20)   \n1. [Models evaluation](#6)\n1. [Conclusion](#7)\n","98dcd81e":"## 7. Conclusion <a class=\"anchor\" id=\"7\"><\/a>\n\n[Back to Table of Contents](#0.1)","f08a92d0":"Thanks to:\n* https:\/\/www.kaggle.com\/sz8416\/6-ways-for-feature-selection\n* https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.feature_selection.RFE.html","6ce66440":"**ExtraTreesClassifier** implements a meta estimator that fits a number of randomized decision trees (a.k.a. extra-trees) on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting. The default values for the parameters controlling the size of the trees (e.g. max_depth, min_samples_leaf, etc.) lead to fully grown and unpruned trees which can potentially be very large on some data sets. To reduce memory consumption, the complexity and size of the trees should be controlled by setting those parameter values. Reference [sklearn documentation](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.ensemble.ExtraTreesClassifier.html). \n\nIn extremely randomized trees, randomness goes one step further in the way splits are computed. As in random forests, a random subset of candidate features is used, but instead of looking for the most discriminative thresholds, thresholds are drawn at random for each candidate feature and the best of these randomly-generated thresholds is picked as the splitting rule. This usually allows to reduce the variance of the model a bit more, at the expense of a slightly greater increase in bias. Reference [sklearn documentation](https:\/\/scikit-learn.org\/stable\/modules\/ensemble.html#Extremely%20Randomized%20Trees).","54b44f3f":"### 5.19 Gaussian Process Classification <a class=\"anchor\" id=\"5.19\"><\/a>\n\n[Back to Table of Contents](#0.1)","e63624ea":"### 3.2.1. Feature creation<a class=\"anchor\" id=\"3.2.1\"><\/a>\n\n[Back to Table of Contents](#0.1)","683389ea":"### 5.17 Naive Bayes <a class=\"anchor\" id=\"5.17\"><\/a>\n\n[Back to Table of Contents](#0.1)","7bbac40f":"### 5.4 MLP Classifier<a class=\"anchor\" id=\"5.4\"><\/a>\n\n[Back to Table of Contents](#0.1)","335d1440":"## 1. Import libraries <a class=\"anchor\" id=\"1\"><\/a>\n\n[Back to Table of Contents](#0.1)","421dc050":"Tikhonov Regularization, colloquially known as **Ridge Classifier**, is the most commonly used regression algorithm to approximate an answer for an equation with no unique solution. This type of problem is very common in machine learning tasks, where the \"best\" solution must be chosen using limited data. If a unique solution exists, algorithm will return the optimal value. However, if multiple solutions exist, it may choose any of them. Reference [Brilliant.org](https:\/\/brilliant.org\/wiki\/ridge-regression\/).","96d5c179":"The **MLPClassifier** optimizes the squared-loss using LBFGS or stochastic gradient descent by the Multi-layer Perceptron regressor. Reference [Sklearn documentation](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.neural_network.MLPRegressor.html#sklearn.neural_network.MLPRegressor).","a1e5085f":"**Linear Regression** is a linear approach to modeling the relationship between a scalar response (or dependent variable) and one or more explanatory variables (or independent variables). The case of one explanatory variable is called simple linear regression. For more than one explanatory variable, the process is called multiple linear regression. Reference [Wikipedia](https:\/\/en.wikipedia.org\/wiki\/Linear_regression).\n\nNote the confidence score generated by the model based on our training dataset.","052e3cf8":"There is **VotingClassifier**. The idea behind the VotingClassifier is to combine conceptually different machine learning classifiers and use a majority vote (hard vote) or the average predicted probabilities (soft vote) to predict the class labels. Such a classifier can be useful for a set of equally well performing model in order to balance out their individual weaknesses. Reference [sklearn documentation](https:\/\/scikit-learn.org\/stable\/modules\/ensemble.html#voting-classifier).","1421b996":"## 5. Tuning models and test for all features <a class=\"anchor\" id=\"5\"><\/a>\n\n[Back to Table of Contents](#0.1)","47859000":"### 5.10 Gradient Boosting Classifier<a class=\"anchor\" id=\"5.10\"><\/a>\n\n[Back to Table of Contents](#0.1)","1148cacc":"Thanks to https:\/\/www.kaggle.com\/startupsci\/titanic-data-science-solutions\n\nNow we are ready to train a model and predict the required solution. There are 60+ predictive modelling algorithms to choose from. We must understand the type of problem and solution requirement to narrow down to a select few models which we can evaluate. Our problem is a classification problem. With these two criteria - Supervised Learning, we can narrow down our choice of models to a few. These include:\n\n- Linear Regression, Logistic Regression\n- Naive Bayes \n- k-Nearest Neighbors algorithm\n- Neural network with Keras\n- Support Vector Machines and Linear SVC\n- Stochastic Gradient Descent, Gradient Boosting Classifier, RidgeCV, Bagging Classifier\n- Decision Tree Classifier, Random Forest Classifier, AdaBoost Classifier, XGB Classifier, LGBM Classifier, ExtraTrees Classifier \n- Gaussian Process Classification\n- MLP Classifier (Deep Learning)\n- Voting Classifier\n\nEach model is built using cross-validation (except LGBM). The parameters of the model are selected to ensure the maximum matching of accuracy on the training and validation data. A plot is being built for this purpose with [learning_curve](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.model_selection.learning_curve.html?highlight=learning_curve#sklearn.model_selection.learning_curve) from sklearn library.","263b00f6":"We can now rank our evaluation of all the models to choose the best one for our problem.","8b5a08c9":"This code is based on my kernel [Autoselection from 20 classifier models & L_curves](https:\/\/www.kaggle.com\/vbmokin\/autoselection-from-20-classifier-models-l-curves)","bd80d9e8":"Thanks to:\n\n* FE from the https:\/\/www.kaggle.com\/liananapalkova\/automated-feature-engineering-for-titanic-dataset\n\n* Visualization from the https:\/\/www.kaggle.com\/vbmokin\/three-lines-of-code-for-titanic-top-20","4879d6d4":"In pattern recognition, the **k-Nearest Neighbors algorithm** (or k-NN for short) is a non-parametric method used for classification and regression. A sample is classified by a majority vote of its neighbors, with the sample being assigned to the class most common among its k nearest neighbors (k is a positive integer, typically small). Reference [Wikipedia](https:\/\/en.wikipedia.org\/wiki\/K-nearest_neighbors_algorithm).","fcd29783":"### 3.2.2.6 FS by the Recursive Feature Elimination (RFE) with Random Forest<a class=\"anchor\" id=\"3.2.2.6\"><\/a>\n\n[Back to Table of Contents](#0.1)","42b515b8":"Larger values **r2_score_diff** mean overfitting.","d281a077":"From the notebook [Modification of neural network(around 90%)](https:\/\/www.kaggle.com\/skrudals\/modification-of-neural-network-around-90) - thanks to the author [@skrudals](https:\/\/www.kaggle.com\/skrudals) for improving the model I used in earlier versions of my notebook","033410b5":"1. Pandas Profiling","8089a22b":"### 5.18 Neural network (NN) with Keras <a class=\"anchor\" id=\"5.18\"><\/a>\n\n[Back to Table of Contents](#0.1)","5a3a9e7e":"Bootstrap aggregating, also called **Bagging**, is a machine learning ensemble meta-algorithm designed to improve the stability and accuracy of machine learning algorithms used in statistical classification and regression. It also reduces variance and helps to avoid overfitting. Although it is usually applied to decision tree methods, it can be used with any type of method. Bagging is a special case of the model averaging approach. Bagging leads to \"improvements for unstable procedures\", which include, for example, artificial neural networks, classification and regression trees, and subset selection in linear regression. On the other hand, it can mildly degrade the performance of stable methods such as K-nearest neighbors. Reference [Wikipedia](https:\/\/en.wikipedia.org\/wiki\/Bootstrap_aggregating).","b85f5856":"**Gradient Boosting** builds an additive model in a forward stage-wise fashion; it allows for the optimization of arbitrary differentiable loss functions. In each stage n_classes_ regression trees are fit on the negative gradient of the binomial or multinomial deviance loss function. Binary classification is a special case where only a single regression tree is induced. The features are always randomly permuted at each split. Therefore, the best found split may vary, even with the same training data and max_features=n_features, if the improvement of the criterion is identical for several splits enumerated during the search of the best split. To obtain a deterministic behaviour during fitting, random_state has to be fixed. Reference [sklearn documentation](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.ensemble.GradientBoostingClassifier.html).","5594acca":"## 3. EDA & FE<a class=\"anchor\" id=\"3\"><\/a>\n\n[Back to Table of Contents](#0.1)","18d03a32":"**Support Vector Machines** are supervised learning models with associated learning algorithms that analyze data used for classification and regression analysis. Given a set of training samples, each marked as belonging to one or the other of two categories, an SVM training algorithm builds a model that assigns new test samples to one category or the other, making it a non-probabilistic binary linear classifier. Reference [Wikipedia](https:\/\/en.wikipedia.org\/wiki\/Support_vector_machine).","9653e967":"The analysis shows different patterns, but most importantly, it confirms that the features are quite diverse, there are no too strongly correlated. Some features clustering target values quite well, but there are none that do it with 100% accuracy. Those, a good dataset has been formed, but it is impossible to unambiguously choose the optimal model. There is little data, so any model can overfit.","8afcafcd":"## 4. Preparing to modeling <a class=\"anchor\" id=\"4\"><\/a>\n\n[Back to Table of Contents](#0.1)","38e02c31":"### 3.2.2.1. FS with the Pearson correlation<a class=\"anchor\" id=\"3.2.2.1\"><\/a>\n\n[Back to Table of Contents](#0.1)","f56adfa8":"### 3.2.2.2. FS by the SelectFromModel with LinearSVC <a class=\"anchor\" id=\"3.2.2.2\"><\/a>\n\n[Back to Table of Contents](#0.1)","c723d0bb":"Your comments and feedback are most welcome.","265207b2":"### 3.2.2.4. FS by the SelectKBest with Chi-2 <a class=\"anchor\" id=\"3.2.2.4\"><\/a>\n\n[Back to Table of Contents](#0.1)","418f5395":"### 5.9 LGBM Classifier <a class=\"anchor\" id=\"5.9\"><\/a>\n\n[Back to Table of Contents](#0.1)","9f417ecb":"### 3.3.3. Pandas Describe<a class=\"anchor\" id=\"3.3.3\"><\/a>\n\n[Back to Table of Contents](#0.1)","344a99ba":"The **GaussianProcessClassifier** implements Gaussian processes (GP) for classification purposes, more specifically for probabilistic classification, where test predictions take the form of class probabilities. GaussianProcessClassifier places a GP prior on a latent function, which is then squashed through a link function to obtain the probabilistic classification. The latent function is a so-called nuisance function, whose values are not observed and are not relevant by themselves. Its purpose is to allow a convenient formulation of the model. GaussianProcessClassifier implements the logistic link function, for which the integral cannot be computed analytically but is easily approximated in the binary case.\n\nIn contrast to the regression setting, the posterior of the latent function is not Gaussian even for a GP prior since a Gaussian likelihood is inappropriate for discrete class labels. Rather, a non-Gaussian likelihood corresponding to the logistic link function (logit) is used. GaussianProcessClassifier approximates the non-Gaussian posterior with a Gaussian based on the Laplace approximation. Reference [Sklearn documentation](https:\/\/scikit-learn.org\/stable\/modules\/gaussian_process.html#gaussian-process-classification-gpc).","f52839d0":"### 5.16 k-Nearest Neighbors (KNN)<a class=\"anchor\" id=\"5.16\"><\/a>\n\n[Back to Table of Contents](#0.1)","a007e52a":"### 5.8 XGB Classifier<a class=\"anchor\" id=\"5.8\"><\/a>\n\n[Back to Table of Contents](#0.1)","6c977c8c":"2. Pandas Describe","5151a8e2":"The core principle of **AdaBoost** (\"Adaptive Boosting\") is to fit a sequence of weak learners (i.e., models that are only slightly better than random guessing, such as small decision trees) on repeatedly modified versions of the data. The predictions from all of them are then combined through a weighted majority vote (or sum) to produce the final prediction. The data modifications at each so-called boosting iteration consist of applying N weights to each of the training samples. Initially, those weights are all set to 1\/N, so that the first step simply trains a weak learner on the original data. For each successive iteration, the sample weights are individually modified and the learning algorithm is reapplied to the reweighted data. At a given step, those training examples that were incorrectly predicted by the boosted model induced at the previous step have their weights increased, whereas the weights are decreased for those that were predicted correctly. As iterations proceed, examples that are difficult to predict receive ever-increasing influence. Each subsequent weak learner is thereby forced to concentrate on the examples that are missed by the previous ones in the sequence. Reference [sklearn documentation](https:\/\/scikit-learn.org\/stable\/modules\/ensemble.html#adaboost).","667f3f23":"This code for AutoViz used EDA tool from the kernel [Data Visualization in just one line of code!!](https:\/\/www.kaggle.com\/nareshbhat\/data-visualization-in-just-one-line-of-code)","1399988c":"There are many techniques for **selection features**, see example:\n- [sklearn library documentation](https:\/\/scikit-learn.org\/stable\/modules\/classes.html#module-sklearn.feature_selection)\n- [a collection of notebooks](https:\/\/www.kaggle.com\/vbmokin\/data-science-for-tabular-data-advanced-techniques#3)\n- my notebook [Titanic - Featuretools (automatic FE&FS)](https:\/\/www.kaggle.com\/vbmokin\/titanic-featuretools-automatic-fe-fs)\n- my notebook [Merging FE & Prediction - xgb, lgb, logr, linr](https:\/\/www.kaggle.com\/vbmokin\/merging-fe-prediction-xgb-lgb-logr-linr)","3fec7eb9":"This model uses a **Decision Tree** as a predictive model which maps features (tree branches) to conclusions about the target value (tree leaves). Tree models where the target variable can take a finite set of values are called classification trees; in these tree structures, leaves represent class labels and branches represent conjunctions of features that lead to those class labels. Decision trees where the target variable can take continuous values (typically real numbers) are called regression trees. Reference [Wikipedia](https:\/\/en.wikipedia.org\/wiki\/Decision_tree_learning).","06e98907":"### 3.2. FE<a class=\"anchor\" id=\"3.2\"><\/a>\n\n[Back to Table of Contents](#0.1)","74c3c8c5":"### 3.3. EDA for Model selection<a class=\"anchor\" id=\"3.3\"><\/a>\n\n[Back to Table of Contents](#0.1)","5d392a78":"### 5.5 Stochastic Gradient Descent <a class=\"anchor\" id=\"5.5\"><\/a>\n\n[Back to Table of Contents](#0.1)","21309b86":"[Go to Top](#0)","e6e32c09":"Thanks to:\n* https:\/\/www.kaggle.com\/sz8416\/6-ways-for-feature-selection\n* https:\/\/towardsdatascience.com\/feature-selection-techniques-in-machine-learning-with-python-f24e7da3f36e","66506bda":"The analysis revealed the presence of one duplicate line. Let's remove it.","b46f85d7":"### 3.1. Initial EDA for FE<a class=\"anchor\" id=\"3.1\"><\/a>\n\n[Back to Table of Contents](#0.1)","8c562dd2":"## 6. Models evaluation <a class=\"anchor\" id=\"6\"><\/a>\n\n[Back to Table of Contents](#0.1)","1b634725":"### 3.2.2.3. FS by the SelectFromModel with Lasso <a class=\"anchor\" id=\"3.2.2.3\"><\/a>\n\n[Back to Table of Contents](#0.1)","cc2f71d7":"### 3.2.2.5. FS by the Recursive Feature Elimination (RFE) with Logistic Regression<a class=\"anchor\" id=\"3.2.2.5\"><\/a>\n\n[Back to Table of Contents](#0.1)","449f8aa6":"The next code from in my kernel [FE & EDA with Pandas Profiling](https:\/\/www.kaggle.com\/vbmokin\/fe-eda-with-pandas-profiling)","8e0c7e24":"In machine learning, **Naive Bayes classifiers** are a family of simple probabilistic classifiers based on applying Bayes' theorem with strong (naive) independence assumptions between the features. Naive Bayes classifiers are highly scalable, requiring a number of parameters linear in the number of variables (features) in a learning problem. Reference [Wikipedia](https:\/\/en.wikipedia.org\/wiki\/Naive_Bayes_classifier).","166fcf9b":"This kernel is based on the my kernels:\n* [Titanic (0.83253) - Comparison 20 popular models](https:\/\/www.kaggle.com\/vbmokin\/titanic-0-83253-comparison-20-popular-models)\n* [FE & EDA with Pandas Profiling](https:\/\/www.kaggle.com\/vbmokin\/fe-eda-with-pandas-profiling)\n* [Autoselection from 20 classifier models & L_curves](https:\/\/www.kaggle.com\/vbmokin\/autoselection-from-20-classifier-models-l-curves)\n* FE from [Titanic - Featuretools (automatic FE&FS)](https:\/\/www.kaggle.com\/vbmokin\/titanic-featuretools-automatic-fe-fs)\n\nAlso the kernel used EDA tool from the kernel [Data Visualization in just one line of code!!](https:\/\/www.kaggle.com\/nareshbhat\/data-visualization-in-just-one-line-of-code)"}}