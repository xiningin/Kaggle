{"cell_type":{"5aee9dcc":"code","44aa8fa5":"code","3a77f228":"code","dfa3741b":"code","0d543b57":"code","f37bbc74":"code","ae817f07":"code","cc6e0f38":"code","37e85f9e":"code","c9e87f34":"code","cf8157d5":"code","4a6f5607":"code","519102ec":"code","143f57fb":"code","887c0933":"code","bcb92178":"code","239dbd00":"code","dd4406c8":"code","4663a127":"code","6697c0c6":"code","c2972323":"code","56888683":"code","227a8840":"code","ff44e18f":"code","d7b46831":"code","f20f0911":"code","7a3b1f91":"code","31bfa38f":"code","4d18411e":"code","d9dda596":"code","07d3c797":"code","d195e769":"code","67e39806":"code","c99852cf":"code","c6550ee5":"code","31bdbcce":"code","023faa6f":"code","70ca28d8":"code","35bbc0f6":"code","4c32d0f2":"code","6fc000a6":"code","db976807":"code","864f5f98":"code","27b62e36":"code","f32d85ec":"code","1cd839db":"code","53bdccd7":"code","a0fd2115":"code","75d11078":"code","989e7815":"code","57cf04cd":"code","9034a6e8":"code","4b386edd":"code","e9304ae5":"code","209b160e":"code","f3179f85":"code","6aabe482":"code","3b4c2906":"code","e5f11cd6":"code","93bfd859":"code","7d68d0a5":"code","8935b40d":"code","31379038":"code","4ed17fe0":"code","22afba06":"code","0d6e49f5":"code","de2eb889":"code","6b3cf9f0":"code","3a3c4303":"code","30c8237d":"code","16c1d0e3":"code","4dd56bba":"code","5b6b2f5b":"code","715ecfc4":"code","8fd9306e":"code","898859f5":"code","ca562675":"markdown","5861d31f":"markdown","999501a4":"markdown","af3499ee":"markdown","037de613":"markdown","43dae6f1":"markdown","7a449ee3":"markdown","83094e4b":"markdown","a4753bb6":"markdown","05837ad1":"markdown","b9492636":"markdown","98af89f4":"markdown","d36a45fc":"markdown","8b6930f9":"markdown","1784affc":"markdown","6202ef19":"markdown","d64b2cc6":"markdown","ca0a3481":"markdown","660e41ee":"markdown","aafd7710":"markdown","e354e6d8":"markdown","e2ba3125":"markdown","f4a74f6f":"markdown","993c2048":"markdown","8561bd97":"markdown","2168ed0a":"markdown","b54882ab":"markdown","a383fd9c":"markdown","f16846a2":"markdown","724ae39f":"markdown","b848d6c7":"markdown","cfc77f22":"markdown","8e751e7e":"markdown"},"source":{"5aee9dcc":"# kaggle data\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","44aa8fa5":"import pandas as pd\nimport numpy as np\nimport matplotlib as plot\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\nimport sklearn\nimport plotly as px","3a77f228":"# data used is from https:\/\/www.divvybikes.com\/data-license-agreement under the licence 'Motivate hereby grants to you a non-exclusive, royalty-free, limited, perpetual license to access, reproduce, analyze, copy, modify, distribute in your product or service and use the Data for any lawful purpose (\u201cLicense\u201d).'\n\n# local data\n#q1_2019 = pd.read_csv('\/Users\/cjpw\/Documents\/jupyter\/bike coursera capstone\/Divvy_Trips_2019_Q1.csv')\n#q2_2019 = pd.read_csv('\/Users\/cjpw\/Documents\/jupyter\/bike coursera capstone\/Divvy_Trips_2019_Q2.csv')\n#q3_2019 = pd.read_csv('\/Users\/cjpw\/Documents\/jupyter\/bike coursera capstone\/Divvy_Trips_2019_Q3.csv')\n#q4_2019 = pd.read_csv('\/Users\/cjpw\/Documents\/jupyter\/bike coursera capstone\/Divvy_Trips_2019_Q4.csv')\n\n# kaggle data\nq1_2019 = pd.read_csv('..\/input\/trip-data\/Divvy_Trips_2019_Q1.csv')\nq2_2019 = pd.read_csv('..\/input\/divvy-trips\/Divvy_Trips_2019_Q2.csv')\nq3_2019 = pd.read_csv('..\/input\/divvy-trips\/Divvy_Trips_2019_Q3.csv')\nq4_2019 = pd.read_csv('..\/input\/divvy-trips\/Divvy_Trips_2019_Q4.csv')\nq1_2020 = pd.read_csv('..\/input\/divvy-trips\/Divvy_Trips_2020_Q1.csv')\n\n\n","dfa3741b":"q1_2019.head()","0d543b57":"# check that all the files have the same columns\nq2_2019.head(5)","f37bbc74":"q3_2019.head(5)","ae817f07":"q4_2019.head(4)","cc6e0f38":"# assign the column Id's to q2 2019 so all tables match\nq2_2019.columns = q1_2019.columns","37e85f9e":"# lets check that the tables use the same data type\nprint(q1_2019.dtypes)\nprint(q2_2019.dtypes)\nprint(q3_2019.dtypes)\nprint(q4_2019.dtypes)\n# all good, everything matches","c9e87f34":"#now lets combine the tables\nto_merge = q1_2019, q2_2019,q3_2019,q4_2019\ndf = pd.concat(to_merge)\ndf.head()","cf8157d5":"# take a look at the data types\ndf.dtypes","4a6f5607":"# lets fix the data types\n\n# converting date time to correct type\ndf.start_time = pd.to_datetime(df.start_time)\ndf.end_time = pd.to_datetime(df.end_time)\n\n# converting trip duration to int, data is in seconds with no miliseconds so int is fine\ndf.tripduration = df.tripduration.str.replace('\\W', '')\ndf.tripduration = df.tripduration.astype(int)\n\n# need to change nan's to a number before we can convert float to int\ndf = df.fillna(0)\n# changing birth year from float to int\ndf.birthyear = df.birthyear.astype(int)\n\n","519102ec":"df.shape","143f57fb":"# checking for duplicates\ndf.drop_duplicates(inplace = True)\ndf.shape\n# no duplicates detected\n\n","887c0933":"# checking trip_id's are unique\ndf.trip_id.nunique()\n# all values are unique","bcb92178":"# checking the earliest start time\ndf.sort_values(by='start_time')\n# earlisest start time is in the correct, 1st jan","239dbd00":"# che# checking the latest start time\ndf.sort_values(by='start_time', ascending=False).head(4)\n# latest start time is in the correct, not after 31st december","dd4406c8":"# checking for the earliest end time\n\ndf.sort_values(by='end_time', ascending=True).head(5)\n# earliest start time is in 2019\n","4663a127":"# checking for the latest end time\n\ndf.sort_values(by='end_time', ascending=False).head(5)\n# some trips ended in 2020 early in the morning on the first of jan,\n# but some trips ended many days in january, thus the trip length was VERY long\n# these might be items that need removing \n# but we will look at it more when looking at trip duration\n","6697c0c6":"# converting the trip duration into mins from secondsseconds, originally i only divided the minutes by 60 and the minutes here didnnt match the minutes from minusing end time from start time. So i came back after the next cell  and added the divide by a further 10.\n\ndf['duration_mins'] = (df.tripduration\/60)\/10\ndf['duration_mins'] = df['duration_mins'].round(2)\ndf.head(5)\n","c2972323":"# im not sure what format the trip_duration is in, \n# so i will calculate the trip duaraion in minutes using start and end time data, and then work out what the official trip\n# duration is\n\nimport datetime\nfrom dateutil.relativedelta import relativedelta\nfrom datetime import date\n\n# getting difference in start to end tome\ndf['calc_duration_mins'] = df.end_time - df.start_time\n\n# converting the difference to minutes\ndf['calc_duration_mins'] = df.calc_duration_mins \/ np.timedelta64(1,'m')\ndf['calc_duration_mins'] = df['calc_duration_mins'].round(2)","56888683":"# checking the times match\ndf.iloc[1000:1100]","227a8840":"# lets chech the mean and median trip length\nprint(df.duration_mins.mean())\nprint(df.duration_mins.median())\nprint(df.duration_mins.describe().round(2))","ff44e18f":"df.sort_values(by = 'duration_mins', ascending = False).head(10)\n\n# okay, i figured out that the negative times are daylight saving time.","d7b46831":"# we can see that the max duration is huge.\n# lets have a look at that data\n\nlong_trips = df.loc[df['duration_mins'] > 500 ]\nlong_trips.sort_values('duration_mins', ascending=False).head()\n# exporting to sheets to have a closer look\nlong_trips.to_csv('long_trips.csv')","f20f0911":"long_trips.shape","7a3b1f91":"# lets try and look at them on a hist plot\n# sns.histplot(x = df.duration_mins)","31bfa38f":"id_3846 = df.loc[df['bikeid']== 3846]\nid_3846.sort_values('start_time', ascending=True)\npd.set_option('display.max_rows', 500)\n#print(id_3846)\n\n# can confirm that 3846 hat the bike was not in action during the time it was booked","4d18411e":"id_1354 = df.loc[df['bikeid']== 1354]\nid_1354.sort_values('start_time', ascending=True)\npd.set_option('display.max_rows', 500)\nid_1354.head(5)\n#confirmed, we can now assume that the trip times and durations are accurate","d9dda596":"# i downloaded the long trips to excel to get an overview\n# aafter checking the long trips in excel we can see that there are entries to the repair centers that should be removed from the data set\n\ndf = df.loc[df['to_station_name'] != 'HUBBARD ST BIKE CHECKING (LBS-WH-TEST)']\ndf = df.loc[df['to_station_name']!= 'DIVVY CASSETTE REPAIR MOBILE STATION']\ndf = df.loc[df['to_station_name']!= 'DIVVY Map Frame B\/C Station']\ndf.shape\n","07d3c797":"\ndf.sort_values('duration_mins', ascending=False)","d195e769":"df.loc[df['duration_mins'] > 600 ].count\n","67e39806":"df.duration_mins.describe().round(2)","c99852cf":"# trying to bin the values so later we can see the few outliers\ndf['bins'] = pd.qcut(df['duration_mins'], q=20)","c6550ee5":"# checking for nulls \/ nan's\ndf.isnull().sum()\n# saying none, but i know there are some","31bdbcce":"df.usertype.value_counts()\n# all good here","023faa6f":"df.gender.value_counts()\n# got a lot of missing valuesd here\n","70ca28d8":"# we could share the unknowns between Male and Female in the same proportion but we know how sensitive gender is, so we wont\ndf.gender = df.gender.replace({0: 'unknown'})","35bbc0f6":"print(df.birthyear.value_counts())\n","4c32d0f2":"df.birthyear.replace(0, np.nan, inplace=True)\ndf.birthyear.isna().sum()","6fc000a6":"# got this function from Maxim Dsouza on stack overflow. it will proportionaly share out the nan's\ndef replace_with_proportion(frame, column):\n     isnull = frame[column].isna()\n     sample = frame[column].dropna()\n     sample=set(sample)\n     countframe=pd.DataFrame(columns=[\"name\", \"count\", \"proportion\"])\n     for x in sample:\n          countframe.loc[-1] = [x,frame.loc[frame[column] == x].shape[0], 0 ]\n          countframe.index = countframe.index + 1\n     countframe['proportion']=countframe['count']\/countframe['count'].sum()\n     to_fill=np.random.choice(countframe['name'], size=(frame[column].isnull().sum(),), p=countframe['proportion'])\n     frame.loc[isnull, column] = to_fill\n     return frame","db976807":"df = replace_with_proportion(df, 'birthyear')","864f5f98":"# great all the nans have been shared out\nprint(df.birthyear.isna().sum())\nprint(df.birthyear.isnull().sum())","27b62e36":"# add day of week to data frame\ndf['day_of_week'] = df['start_time'].dt.day_name()","f32d85ec":"# add month of year to data frame\ndf['month'] = df['start_time'].dt.month","1cd839db":"from datetime import date\nimport holidays # https:\/\/pypi.org\/project\/holidays\/\n","53bdccd7":"# trying to add holiday dates but it crashes the kernel\n# df['holiday'] = pd.Series(df.index).apply(lambda x: # holidays.CountryHoliday('US',prov='IL').get(x)).values","a0fd2115":"df.head()","75d11078":"# lets bring in the geo data. it will help us with visualisations\n\n\ngeo_data = q1_2020\ngeo_data.head()\n","989e7815":"# getting columns\ngeo_data.columns","57cf04cd":"# Just keepnig the data we need\ngeo_data = geo_data.drop(['ride_id', 'rideable_type', 'started_at', 'ended_at',\n       'start_station_name','end_station_name',\n       'end_station_id', 'end_lat', 'end_lng',\n       'member_casual'], axis = 1)\ngeo_data.head()","9034a6e8":"# Renaming the columns so we can bring in the lat and long to the main dataset}\ngeo_data.head()\ngeo_data.columns = 'from_station_id', 'start_lat', 'start_lng'","4b386edd":"# checking the data\nprint(geo_data.head())\ngeo_data.shape\n","e9304ae5":"# we just need to match each station with the lat and long, so we can drop all the duplicates\ngeo_data.drop_duplicates(inplace=True)\n# checking the shape of data without duplicstes\ngeo_data.shape","209b160e":"# merging the lat and long for START station into the main df\ndf = pd.merge(df, geo_data, how=\"outer\", on='from_station_id')\n# checking the shape\ndf.head(5)","f3179f85":"# lets bring in the geo data. it will help us with visualisations\ngeo_data = pd.read_csv('\/Users\/cjpw\/Documents\/jupyter\/bike coursera capstone\/Divvy_Trips_2020_Q1.csv')\ngeo_data.columns\n\n# Just keepnig the data we need\ngeo_data = geo_data.drop(['ride_id', 'rideable_type', 'started_at', 'ended_at',\n       'start_station_name','end_station_name',\n       'end_station_id','end_lat', 'end_lng',\n       'member_casual'], axis = 1)\ngeo_data.head()\n\n\n# Renaming the columns so we can bring in the lat and long to the main dataset}\ngeo_data.head()\ngeo_data.columns = 'to_station_id', 'end_lat', 'end_lng'\n\n# checking the data\nprint(geo_data.head())\ngeo_data.shape\n\n# we just need to match each station with the lat and long, so we can drop all the duplicates\ngeo_data.drop_duplicates(inplace=True)\n# checking the shape of data without duplicstes\ngeo_data.shape\n\n# merging the lat and long for END station into the main df\ndf = pd.merge(df, geo_data, how='inner', on='to_station_id')\n# checking the shape\ndf.tail()\n\n","6aabe482":"\n# bringing start in again to check it, renaming columns to prep merge\ngeo_data.head()\ngeo_data.columns = 'from_station_id', 'start_lat_2', 'start_lng_2'","3b4c2906":"# merging the lat and long for START station into the main df\ndf = pd.merge(df, geo_data, how='inner', on='from_station_id')\n# checking the shape\ndf.tail()","e5f11cd6":"df.isna().sum()","93bfd859":"df = df.sort_index()\ndf.head()","7d68d0a5":"# would be good to get this to work\n# https:\/\/www.py4u.net\/discuss\/260045\n\n# note = Geodesic Distance: It is the length of the shortest path between 2 points on any surface. In our case, the surface is the earth. Below program illustrates how to calculate geodesic distance from latitude-longitude data.\n\nimport geopy.distance\nfrom geopy.distance import geodesic\nfrom geopy import Point\n\ndf['start_point'] = df.apply(lambda row: Point(latitude=row['start_lat'], longitude=row['start_lng']), axis=1)\ndf['end_point'] = df.apply(lambda row: Point(latitude=row['end_lat'], longitude=row['end_lng']), axis=1)","8935b40d":"\nfrom geopy.distance import distance\n\ndf['distance_km'] = df.apply(lambda row: distance(row['start_point'], row['end_point']).km if row['end_point'] is not None else float('nan'), axis=1)\n","31379038":"# df.to_csv('output_with_distance_traveled_back_up.csv')","4ed17fe0":"df = pd.read_csv('output_with_distance_traveled_back_up.csv')","22afba06":"df['speed'] = df.distance_km \/ df.duration_mins","0d6e49f5":"df['user_age'] = 2019 - df['birthyear']\ndf.tail(10)","de2eb889":"df = df[df.user_age <80]\ndf = df[df.user_age > 15]","6b3cf9f0":"df.columns","3a3c4303":"df.drop(['Unnamed: 0','tripduration', 'birthyear', 'calc_duration_mins', 'bins','start_lat_2_x', 'start_lng_2_x', 'start_lat_2_y','start_lng_2_y', 'start_point', 'end_point'], axis = 1, inplace = True)","30c8237d":"df.head()","16c1d0e3":"df.trip_id = df.trip_id.astype(int)\ndf.bikeid = df.bikeid.astype(int)\ndf.to_station_id = df.to_station_id.astype(int)\ndf.user_age = df.user_age.astype(int)\n","4dd56bba":"df.head()","5b6b2f5b":"# exporting the csv so we have fresh data\n\ndf.to_csv('output_with_distance_traveled.csv')\n# clearing all the variables from memory as we will reload the new csv\nimport sys\nsys.modules[__name__].__dict__.clear()","715ecfc4":"import pandas as pd\nimport numpy as np\nimport matplotlib as plot\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\nimport sklearn\nimport plotly as px","8fd9306e":"path = \"\/Users\/cjpw\/Documents\/jupyter\/bike coursera capstone\/output_with_distance_traveled.csv\n\ndf = pd.read_csv(path)\n\ndf.head()\n","898859f5":"df.duration_mins.max()","ca562675":"We will drop columns we dont need","5861d31f":"i spoke to the data engineer and the long trips are users that didnt end their trips properley.I will keep the long trips in the dataset but filter it out for analysis. ","999501a4":"We will fix the value types","af3499ee":"Lets convert the year of birth to an age","037de613":"Lets drop the impossible ages\n\n","43dae6f1":"Okay, so i think we have finished checking the data integrity\nnext to do:\n\n* \/\/ bin the data\n* \/\/ deal with nan's\n* \/\/ import the geo data\n* \/\/ add days of week to the df \n* \/\/ add month\n* add holidays # this crashes my kernel :( \n* \/\/ mark the really long trips [in the bins]\n* try again to make a hist plot and adjutst for skewed data, would be good to show how the trips are skewed\n* Now we need to start to analyse the data, i think we need to keep the long trips in the system so we can see how they are made up by subscribers and regualr customers\n* maybe we do the analysis in Tableau\n","7a449ee3":"## Now we need to deal with NaNs and nulls ","83094e4b":"the data from the calculated trip time shows that we have some negative trip times, someone has a time machine! or the columns are mixed up or somehting else. could it be the changing of the clocks? ","a4753bb6":"# ** STARTING ANALYSE STAGE HERE, RUN FROM HERE **","05837ad1":"Lets look a the trip duration to see if it makes sense, we already know there are some super long trips\n\ni tried to chart the durations but it is useless as there are a small number of huge duration items","b9492636":"# Process","98af89f4":"# Ask \n* are there only short term rentals\n* do they know why some of the durations are very long\n* checking the data issues\n* read the divvy website https:\/\/www.divvybikes.com\/","d36a45fc":"We will add the geo data from the 2020 q1 dataset","8b6930f9":"[Presentaion](http:\/\/docs.google.com\/presentation\/d\/1SIznRK_ksGkpeppPQCsd2kL0vjdfnBldU1aHdT3Ptfo\/edit?usp=sharing) , [Report](https:\/\/docs.google.com\/document\/d\/1wTu-7GOiWx6F5B34_Ot3t6oegMq_MtClV_7nq_uEoAA\/edit?usp=sharing)","1784affc":"# Guiding questions for determining marketing strategy\n\n1. How do annual members and casual riders use Cyclistic bikes differently?\n2. Why would casual riders buy Cyclistic annual memberships?\n3. How can Cyclistic use digital media to influence casual riders to become members?","6202ef19":"# Importing libraries and data","d64b2cc6":"We wil tidy the data up before we export to csv so we dont have to run everything above each time","ca0a3481":"Whats going on with the crazy lon duration times?","660e41ee":"Lets deal with NaN's","aafd7710":"Let's try and get the end lat and end long so we can try and calculate trip distance. To do this we need to match the to_station_id field to the geo data table we made. previously.","e354e6d8":"# Statement of Business task\n\nIdentify how annual members and casual riders use Cyclistic bikes\ndifferently in order to help Cyclistic develop a marketing strategy that will convert casual riders to subscribers. \n\n","e2ba3125":"# will check the data out in tableau now","f4a74f6f":"## checking for internal data integrity \n\n* \/\/ check for duplicate rows\n* \/\/ check number of unique trip_id matches number of rows\n* \/\/ check earliest and latest dates in start_time, end_time\n* \/\/ check length of each trip and select the longest and shortest to make sure the numbers are sensible\n* check station id's mathch station names\n* check for nan's blanks or invaid entries in user type and gender\n* \/\/ do all the data sets calculate tip duration in deciseconds, q2 says that the time is in seconds","993c2048":"## Let's start to preapre the data to analyse\nWe will add additional fields that will lets us filter and find patterns, day of week, month, if the day was a holiday and the geo data. The geo data will allow us to make some cool visualisations in Tableau","8561bd97":"these long durations are super confusing, i will check if the duration is a mistake by checking if a bike is used for any other trips during the time it is out\n\nbike id 3846 was out between 2019-02-14 14:44:13 AND\t2019-06-17 16:04:35","2168ed0a":"![Cat](https:\/\/www.chicago.gov\/content\/dam\/city\/depts\/cdot\/Divvy\/divvydockweb.jpg)","b54882ab":"# Intro\n\n* Moreno has set a clear goal: Design marketing strategies aimed at converting casual riders into annual members.\n* In order to do that, however, the marketing analyst team needs to better understand how annual members and casual riders differ\n* why casual riders would buy a membership, and how digital media could affect their marketing tactics\n* Moreno and her team are interested in analyzing the Cyclistic historical bike trip data to identify trends.","a383fd9c":"We have a lot of 0's, so we need to do something with them, we will disribute them proportinaly amoing the other values","f16846a2":"So, the data in the columns is a bit of a mess, time periods dont match. i will need to put some effort into merging the datasets.\n* i will rename the columns so the names match on all df's\n* Reorder the columns, then merge the rows","724ae39f":"checking one more to be sure\n\nBike ID 1354 was booked for three hours - 2019-06-15 14:00:53\t2019-06-15 17:20:19\t","b848d6c7":"checking the data in the files","cfc77f22":"Lets get the speed of the rides","8e751e7e":"# Cyclistic Bikes Analysis\n\n## Identify how annual members and casual riders use Cyclistic bikes differently in order to help Cyclistic develop a marketing strategy that will convert casual riders to subscribers."}}