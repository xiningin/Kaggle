{"cell_type":{"4cb8db82":"code","1cae181f":"code","2843d284":"code","99c97ab8":"code","e0486698":"code","cbc26f0a":"code","f51d4ab1":"code","103e2686":"code","52e7ea7c":"code","31ca3e66":"code","95d5ba4e":"code","dd512085":"code","41420e03":"code","e426cc63":"code","4ecb09ba":"code","b40f8a50":"code","458275b4":"code","eda380a8":"code","715d44cc":"code","5e2d6d99":"code","cf4e05a3":"code","40a1be7e":"code","16e08be1":"code","f8907f3b":"code","6cd4f344":"code","77aa1404":"code","46020e7d":"code","a3e00992":"code","2d1204af":"code","720888b5":"code","a356a488":"code","fe84432a":"code","6a361bdf":"code","f1d54454":"code","acf19d66":"code","95dc1790":"code","9a4ab248":"code","b94139c2":"code","f1f758ee":"code","dadd4e42":"code","13d96842":"markdown","713d59f3":"markdown","94b6209a":"markdown","1f325259":"markdown","423135a2":"markdown","2a17517a":"markdown","231f95cc":"markdown","78795e87":"markdown","1477a062":"markdown","513becc6":"markdown","c8ecdf4b":"markdown","0bf0d1b5":"markdown","e3173e17":"markdown","b52d67e5":"markdown","b4f7896e":"markdown","7e722769":"markdown","8977cb94":"markdown","082f153b":"markdown","a7b68ca0":"markdown","8912bb5a":"markdown","eda85144":"markdown","347a0135":"markdown","dce804a5":"markdown","c7eada4f":"markdown","72dc50d1":"markdown","d2fddcb3":"markdown","ad2e673a":"markdown","4439e052":"markdown"},"source":{"4cb8db82":"cd '\/kaggle\/input\/elliptic-data-set\/elliptic_bitcoin_dataset\/'","1cae181f":"import kmapper as km\nimport community as ct\nfrom sklearn.model_selection import learning_curve\nfrom sklearn.ensemble import RandomForestClassifier\n\nmapper = km.KeplerMapper(verbose=2)\n\nimport sklearn\nimport matplotlib.pyplot as plt\nfrom kmapper import jupyter\nfrom sklearn.decomposition import PCA\nfrom kmapper.cover import Cover\nfrom sklearn import ensemble\nfrom sklearn.cross_decomposition import PLSRegression\n\nimport pandas as pd\nimport numpy as np\n\nfrom sklearn.neighbors import KernelDensity\n\nfrom sklearn.metrics import roc_auc_score\n\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import LocalOutlierFactor\nfrom sklearn.ensemble import IsolationForest\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import BaggingClassifier\n\nfrom sklearn.decomposition import PCA\nfrom sklearn.neighbors import NeighborhoodComponentsAnalysis\nfrom sklearn.cross_decomposition import PLSRegression\nfrom sklearn.kernel_approximation import RBFSampler\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis","2843d284":"df_classes = pd.read_csv('elliptic_txs_classes.csv')\ndf_features = pd.read_csv('elliptic_txs_features.csv')\ndf_transaction_edge = pd.read_csv('elliptic_txs_edgelist.csv')\n\n\nmerged_df = df_classes.merge(df_features, left_on = 'txId', right_on = '230425980')\nmerged_df = merged_df.rename(columns = {'class' : 'target', '1' : 'time_step'})\n\n\nmerged_df = merged_df.rename(columns = {'class' : 'target', '1' : 'time_step'})\nmerged_df['target'] = merged_df['target'].replace({'1': 1, '2': 0, 'unknown' : -1})\nmerged_df = merged_df.drop(['txId', '230425980'], axis = 1)","99c97ab8":"X_train = merged_df.query('time_step < 35 and target != -1').drop(['target'], axis =1)\ny_train = merged_df.query('time_step < 35 and target != -1')['target']\nunlabeled_df_train = merged_df.query('time_step < 35 and target == -1').drop(['target'], axis =1)\n\n\nX_test = merged_df.query('time_step >= 35 and target != -1').drop(['target'], axis = 1)\ny_test = merged_df.query('time_step >= 35 and target != -1')['target']","e0486698":"model = ensemble.IsolationForest(random_state= 42)\nmodel.fit(X_train)\nlens1 = model.decision_function(X_train).reshape((X_train.shape[0], 1))\n\nlens2 = mapper.fit_transform(X_train, projection = \"l2norm\")\n\n\n# Combine lenses pairwise to get a 2-D lens i.e. [Isolation Forest, L^2-Norm] lens\nlens = np.c_[lens1, lens2]","cbc26f0a":"'''\ngraph = mapper.map(lens1,\n                   X_train,\n                   cover=km.Cover(n_cubes=15, perc_overlap=0.5),\n                   clusterer=sklearn.cluster.KMeans(n_clusters = 2,\n                                                    random_state= 42))\n\n# Visualization\ngrafico = mapper.visualize(graph,\n                 path_html=\"\/kaggle\/working\/elliptic-dataset_EDA.html\",\n                 title=\" Illicit Transaction - Elliptic Dataset - EDA with Heatmap True Label',\n                 color_function= np.array(y_train))\n'''","f51d4ab1":"from hyperopt import hp\nfrom hyperopt.pyll.stochastic import sample\n\n\n{feature: hp.choice('feature_activation', [0, 1]) for feature in X_train.columns}\n\n\ndef wrapper(df, features = None):\n\n    if features == None:\n        return df\n    \n    else:\n        active_features = []\n        for col in features.keys():\n            if features[col] == 1:\n                active_features.append(col)\n            \n    return df[set(active_features + ['time_step'])]","103e2686":"parametrizacao = {'feature_activation': {'-0.0031749902209437055': 0, '-0.004094090363360379': 0, '-0.004194370240563965': 0, '-0.004357614038573012': 0, '-0.0106685610738475': 1, '-0.010763009512837094': 1, '-0.012005182118417812': 0, '-0.012107451777478418': 0, '-0.013281614870058885': 1, '-0.014658775691506424': 0, '-0.015070900001830685': 0, '-0.017031675880349647': 0, '-0.01764011962990492': 0, '-0.018848712954381765': 1, '-0.0230451563960962': 1, '-0.02466883065625352': 0, '-0.02605968882058813': 1, '-0.026214655177430907': 0, '-0.027659714028485525': 0, '-0.028741285856664783': 0, '-0.030026235277791913': 1, '-0.031272390486630317': 1, '-0.035390552600813516': 1, '-0.039143978122140564': 0, '-0.03914869407758738': 0, '-0.042955299258028254': 1, '-0.04387454791734898': 0, '-0.04970696439403985': 0, '-0.057194633660791555': 0, '-0.05901305478547111': 1, '-0.06158379407303222': 1, '-0.07525553154622772': 0, '-0.08014673584659182': 0, '-0.08014726965335221': 1, '-0.08345878401951694': 0, '-0.0874901561101501': 0, '-0.08879772536984745': 0, '-0.0904370818830761': 1, '-0.09314472701426248': 0, '-0.09540268927284089': 1, '-0.09752359377152515': 1, '-0.09771859912616487': 1, '-0.0978946778840727': 0, '-0.11300200928476244': 0, '-0.11642460389073964': 0, '-0.1168167152870516': 0, '-0.11979245961251665': 1, '-0.12061340670311574': 1, '-0.1209504155034722': 1, '-0.12196959975910057': 0, '-0.12746225048300552': 1, '-0.13115530389558736': 0, '-0.13454647495572797': 1, '-0.13732327709079195': 1, '-0.13973120192279553': 0, '-0.13973300164963834': 0, '-0.14033462388732307': 1, '-0.14076322246003314': 1, '-0.14370672448675942': 0, '-0.14890718609704007': 0, '-0.1489118870463073': 1, '-0.15246437130048263': 0, '-0.1556613943053229': 1, '-0.15566142432803598': 1, '-0.15973245222462992': 0, '-0.16092533142876464': 1, '-0.16093221729544638': 0, '-0.16209679981659642': 1, '-0.16311462864045503': 0, '-0.16312596586207964': 0, '-0.16440217329951': 1, '-0.16654994856468994': 0, '-0.16793302645225652': 0, '-0.1691595024198962': 1, '-0.16960915015560768': 1, '-0.17115370708833458': 0, '-0.1714692896288031': 0, '-0.17160149344087147': 0, '-0.1728840699195216': 0, '-0.17289511037923946': 1, '-0.1744725474413385': 1, '-0.1766172824237854': 0, '-0.18381594178200059': 1, '-0.18466755143291433': 0, '-0.18799267262971858': 1, '-0.19147194592540379': 1, '-0.19481661068542533': 1, '-0.19914489882884956': 1, '-0.20158367565021204': 0, '-0.21294750674116758': 1, '-0.21653637857822206': 0, '-0.21681436061843226': 1, '-0.22720332382768585': 1, '-0.22721544644782224': 1, '-0.23255262057859216': 1, '-0.23495151756654098': 0, '-0.2358964234692872': 0, '-0.23936836871437037': 1, '-0.24323609254239772': 1, '-0.2505232075535023': 1, '-0.25511064604426475': 1, '-0.2591940456978486': 0, '-0.2623679155254754': 0, '-0.2637032528675492': 0, '-0.26437568843944076': 0, '-0.29377256255240464': 0, '-0.29797501416794375': 1, '-0.3499327749819623': 0, '-0.37571453141267186': 1, '-0.4140054550165004': 0, '-0.4235879631572251': 1, '-0.44088282778065335': 1, '-0.4581618412195745': 1, '-0.46755439410398614': 0, '-0.48834041329936734': 1, '-0.5621534802884299': 1, '-0.6009988905192808': 1, '-0.6056310578523486': 1, '-0.7683292540299814': 1, '-0.968902874660573': 0, '-1.0159633735173061': 0, '-1.0162304272806977': 1, '-1.0963356939258175': 0, '-1.2013688016765636': 0, '-1.230440801051298': 1, '-1.2673399140326196': 0, '-1.315375381290753': 1, '-1.3153883882753692': 1, '-1.3163333780274464': 0, '-1.316342283664295': 1, '-1.3547347717590499': 0, '-1.3714598276027399': 1, '-1.3736571773938961': 0, '-1.3737824923785267': 0, '-1.4036981050621322': 1, '-1.4579532008792007': 1, '-1.4859719767717499': 0, '-1.494056831372085': 1, '0.001427813709709475': 1, '0.0014826437872997916': 0, '0.0024263098865888125': 0, '0.0031432964315106413': 0, '0.01827940003744589': 0, '0.034038660711490365': 1, '0.0365766503766988': 1, '0.0374680287743863': 0, '0.04234513480691014': 0, '0.043444221333941775': 1, '0.048766817854885264': 1, '0.05295606452358398': 0, '0.759748200156594': 0, '1.0637874876466933': 0, '1.0642045463479908': 1, '1.1255896087380572': 0, '1.1280380290020882': 0, '1.1335266830129689': 0, '1.135278702458467': 1, '1.135522751490673': 0, '1.1359467007331425': 0, '1.3407327277377104': 1, '1.342003002186918': 0, '1.4613303209554889': 1, '1.4613689382001922': 0, '1.487932476808418': 0, '1.4881129012284324': 0, 'time_step': 1}, 'hyperparams': {'max_depth': 11, 'max_features': 0.48930003686115775, 'max_samples': 0.7692800266308553, 'n_estimators': 250, 'n_jobs': -1}}\n\nparametrizacao['hyperparams']['n_estimators'] = 300\n\ncolunas_escolhidas = list(wrapper(X_train, parametrizacao['feature_activation']).columns)","52e7ea7c":"rf = RandomForestClassifier(**parametrizacao['hyperparams'], random_state = 1)\nsampled_df = X_train[colunas_escolhidas].sample(frac = 0.97, random_state = 1)\ny_sampled = y_train.loc[sampled_df.index]\nrf.fit( sampled_df, y_sampled )\ny_labeled_score1 = rf.predict_proba( unlabeled_df_train[colunas_escolhidas] )\ny_labeled_score1 = pd.DataFrame( y_labeled_score1, index = unlabeled_df_train.index ).rename(columns = {1 : 'y_labeled_score1'})\ny_label1 = rf.predict( unlabeled_df_train[colunas_escolhidas] )\ny_label1 = pd.DataFrame( y_label1, index = unlabeled_df_train.index ).rename(columns = {0 : 'y_label1'})\nlabels_1 = y_labeled_score1.join(y_label1)[['y_labeled_score1', 'y_label1']]\n\n\nrf = RandomForestClassifier(**parametrizacao['hyperparams'], random_state = 2)\nsampled_df = X_train[colunas_escolhidas].sample(frac = 0.97, random_state = 2)\ny_sampled = y_train.loc[sampled_df.index]\nrf.fit( sampled_df, y_sampled )\ny_labeled_score2 = rf.predict_proba( unlabeled_df_train[colunas_escolhidas] )\ny_labeled_score2 = pd.DataFrame( y_labeled_score2, index = unlabeled_df_train.index ).rename(columns = {1 : 'y_labeled_score2'})\ny_label2 = rf.predict( unlabeled_df_train[colunas_escolhidas] )\ny_label2 = pd.DataFrame( y_label2, index = unlabeled_df_train.index ).rename(columns = {0 : 'y_label2'})\nlabels_2 = y_labeled_score2.join(y_label2)[['y_labeled_score2', 'y_label2']]\n\n\nrf = RandomForestClassifier(**parametrizacao['hyperparams'], random_state = 3)\nsampled_df = X_train[colunas_escolhidas].sample(frac = 0.97, random_state = 3)\ny_sampled = y_train.loc[sampled_df.index]\nrf.fit( sampled_df, y_sampled )\ny_labeled_score3 = rf.predict_proba( unlabeled_df_train[colunas_escolhidas] )\ny_labeled_score3 = pd.DataFrame( y_labeled_score3, index = unlabeled_df_train.index ).rename(columns = {1 : 'y_labeled_score3'})\ny_label3 = rf.predict( unlabeled_df_train[colunas_escolhidas] )\ny_label3 = pd.DataFrame( y_label3, index = unlabeled_df_train.index ).rename(columns = {0 : 'y_label3'})\nlabels_3 = y_labeled_score3.join(y_label3)[['y_labeled_score3', 'y_label3']]","31ca3e66":"labels_df = labels_1.join(labels_2).join(labels_3)\nlabels_df['target'] = -1\nlabels_df.loc[ labels_df.query('y_labeled_score1 > 0.8 and y_labeled_score2 > 0.8 and y_labeled_score3 > 0.8').index, ['target'] ] = 1\nlabels_df.loc[ labels_df.query('y_labeled_score1 <= 0.1 and y_labeled_score2 <= 0.1 and y_labeled_score3 <= 0.1').index, ['target'] ] = 0\n\nunlabeled_df_train['target'] = labels_df['target']","95d5ba4e":"rf = RandomForestClassifier(**parametrizacao['hyperparams'], random_state = 10)\nrf.fit(X_train[colunas_escolhidas], y_train)\n\ny_score = rf.predict_proba(X_test[colunas_escolhidas])\ny_score = pd.DataFrame( y_score )[1]\n\nroc_auc_score(y_test, y_score)\n","dd512085":"rf = RandomForestClassifier(**parametrizacao['hyperparams'], random_state = 1)\nrf.fit(X_train[colunas_escolhidas].append(unlabeled_df_train.query('target != -1')[colunas_escolhidas]), y_train.append(unlabeled_df_train.query('target != -1')['target']))\n\ny_score = rf.predict_proba(X_test[colunas_escolhidas])\ny_score = pd.DataFrame( y_score, index = X_test.index )[1].rename('y_score')\n\nroc_auc_score(y_test, y_score)","41420e03":"try:\n    X_train.drop(['erro_tipo_1', 'erro_tipo_2', 'lda_transform_erro_1_test', 'lda_transform_erro_2_test'], axis = 1, inplace = True)\n    X_test.drop(['erro_tipo_1', 'erro_tipo_2', 'lda_transform_erro_1_test', 'lda_transform_erro_2_test'], axis = 1, inplace = True)\nexcept:\n    None\n\n\n\nfrom sklearn.preprocessing import Normalizer\n\n\nnorm = Normalizer().fit(X_train)  # fit does nothing.\n\nX_train_norm = pd.DataFrame( norm.transform(X_train), index = X_train.index)\n\n\nlof = LocalOutlierFactor(n_neighbors = 10, n_jobs = -1, novelty = False)\n\ny_outlier = lof.fit_predict(X_train_norm)\n\n\n###############################################################3\n\n\nfrom sklearn.cross_decomposition import PLSRegression\n\npls = PLSRegression(n_components= 15)\npls.fit(X_train, y_train)\n\n\nX_train_transformed = pd.DataFrame( pls.transform(X_train), index = X_train.index)","e426cc63":"from sklearn.neighbors import NearestNeighbors\nfrom sklearn.model_selection import StratifiedShuffleSplit\n\nsss = StratifiedShuffleSplit(n_splits = 5, test_size=0.333, random_state=0)\nsss.get_n_splits(X_train_transformed, y_train)\n\ntrust_score_final = []\ntrust_score_df = (X_train_transformed.join(y_train).reset_index()).join(pd.Series (y_outlier).rename('outlier_treino') )\nfor train_index, test_index in sss.split(X_train_transformed, y_train):\n\n    searched_df = trust_score_df.iloc[train_index].query('outlier_treino != -1')\n    searched_df = searched_df.drop(['index', 'outlier_treino'], axis = 1)\n    \n    searched_df_target0 = searched_df.query(y_train.name + \" == 0\")\n    neigh_target0 = NearestNeighbors(n_neighbors = 2, n_jobs = -1)\n    neigh_target0.fit(searched_df_target0.drop([y_train.name], axis =1))\n\n    searched_df_target1 = searched_df.query(y_train.name + \" == 1\")\n    neigh_target1 = NearestNeighbors(n_neighbors = 2, n_jobs = -1)\n    neigh_target1.fit(searched_df_target1.drop([y_train.name], axis =1))\n\n    # Busca 2 n_neighbours, de forma a retornar pela funcao abaixo o valor minimo diferente de zero,\n    # caso exista duplicata de elemento de X_test em X_train, em que distancia seria 0\n\n    distance_class_0 = list(map( lambda x: np.min(x[np.nonzero(x)]), neigh_target0.kneighbors(pls.transform(X_test))[0]))\n\n    distance_class_1 = list(map( lambda x: np.min(x[np.nonzero(x)]), neigh_target1.kneighbors(pls.transform(X_test))[0]))\n\n    # trust score = distancia para classe diferente da prevista \/ distancia para classe prevista\n\n    trust_score = []\n    for i in range (0, len(y_test)):\n        trust_score.append(  distance_class_0[i] \/ distance_class_1[i] if y_score.tolist()[i] > 0.5 else\n                             distance_class_1[i] \/ distance_class_0[i])\n\n    trust_score = list(map( float, trust_score ))\n    trust_score_final.append(trust_score)\ntrust_score_final_df = pd.DataFrame( trust_score_final, columns = X_test.index ).mean().rename('trust_score')","4ecb09ba":"query_erros = '(target == 1 and y_score < 0.5) or (target == 0 and y_score > 0.5)'\n\n\nprint('A quantidade de previs\u00f5es erradas \u00e9 : ' + str( len(X_test.join(y_test).join(trust_score_final_df).join(y_score).query( query_erros )) ) )\nprint('M\u00e9dia de trust_score em s\u00e3o previs\u00f5es erradas : ' + str(X_test.join(y_test).join(trust_score_final_df).join(y_score).query( query_erros )[['trust_score']].mean()[0]) )\nprint('Desvio padr\u00e3o do Trust Score em previs\u00f5es erradas : ' + str(X_test.join(y_test).join(trust_score_final_df).join(y_score).query( query_erros )[['trust_score']].std()[0]) )","b40f8a50":"query_acertos = '(y_score > 0.5 and target == 1) or (y_score < 0.5 and target == 0)'\n\n\nprint('A quantidade de previs\u00f5es acertadas \u00e9 : ' + str( len(X_test.join(y_test).join(trust_score_final_df).join(y_score).query( query_acertos )) ) )\nprint('M\u00e9dia de trust_score em s\u00e3o previs\u00f5es certas : ' + str(X_test.join(y_test).join(trust_score_final_df).join(y_score).query( query_acertos )[['trust_score']].mean()[0]) )\nprint('Desvio padr\u00e3o do Trust Score em previs\u00f5es certas : ' + str(X_test.join(y_test).join(trust_score_final_df).join(y_score).query( query_acertos )[['trust_score']].std()[0]) )","458275b4":"# Query que seleciona todas classificacoes erradas\nquery1_inner = '(target == 1 and y_score < 0.5) or (target == 0 and y_score > 0.5) '\n\ncentro_intervalo_trust_score_erros = X_test.join(y_test).join(trust_score_final_df).join(y_score).query( query1_inner )[['trust_score']].mean()[0] - 0.05\namplitude_trust_score_erros = X_test.join(y_test).join(trust_score_final_df).join(y_score).query( query1_inner )[['trust_score']].std()[0] - 0.1\n\nquery2_inner = ' (trust_score > ' + str(centro_intervalo_trust_score_erros - amplitude_trust_score_erros) + ' and trust_score < ' + str(centro_intervalo_trust_score_erros + amplitude_trust_score_erros) + ')'\n\nprint('Selecao de todas classificacoes erradas em intervalo definido : ' + str([centro_intervalo_trust_score_erros - amplitude_trust_score_erros, centro_intervalo_trust_score_erros + amplitude_trust_score_erros]))\nprint('Quantidade de erros nesse intervalo : ' + str(len(X_test.join(y_test).join(trust_score_final_df).join(y_score).query(query1_inner + ' and ' + query2_inner)[['y_score', 'target', 'trust_score']])))\nprint('Quantidade de transa\u00e7\u00f5es nesse intervalo : ' + str(len(X_test.join(y_test).join(trust_score_final_df).join(y_score).query(query2_inner))))","eda380a8":"query1_outer = '(target == 1 and y_score < 0.5) or (target == 0 and y_score > 0.5) '\nquery2_outer = ' (trust_score < ' + str(centro_intervalo_trust_score_erros - amplitude_trust_score_erros) + ' or trust_score > ' + str(centro_intervalo_trust_score_erros + amplitude_trust_score_erros) + ')'\n\nprint('Selecao de todas classificacoes erradas disjuntas ao intervalo acima')\nprint('Quantidade de erros nesse intervalo : ' + str(len(X_test.join(y_test).join(trust_score_final_df).join(y_score).query(query1_outer + ' and ' + query2_outer)[['y_score', 'target', 'trust_score']])))\nprint('Quantidade de transa\u00e7\u00f5es nesse intervalo : ' + str(len(X_test.join(y_test).join(trust_score_final_df).join(y_score).query(query2_outer))))","715d44cc":"try:\n    X_train.drop(['erro_tipo_1', 'erro_tipo_2', 'lda_transform_erro_1_test', 'lda_transform_erro_2_test'], axis = 1, inplace = True)\n    X_test.drop(['erro_tipo_1', 'erro_tipo_2', 'lda_transform_erro_1_test', 'lda_transform_erro_2_test'], axis = 1, inplace = True)\nexcept:\n    None\n\n\nfrom sklearn.preprocessing import Normalizer\n\nX_train_reduced = X_train.query('time_step <= 23')[colunas_escolhidas]\ny_train_reduced = y_train.loc[X_train_reduced.index]\nX_new_test = X_train.query('time_step >= 28')[colunas_escolhidas]\ny_new_test = y_train.loc[X_new_test.index]\n\n\nrf.fit(X_train_reduced, y_train_reduced)\ny_predicted = rf.predict_proba(X_new_test)\ny_predicted = pd.DataFrame( y_predicted, index = X_new_test.index)[1].rename('y_predicted')\n\n\n\n\n\nnorm = Normalizer().fit(X_train_reduced)  # fit does nothing.\n\nX_train_norm = pd.DataFrame( norm.transform(X_train_reduced), index = X_train_reduced.index)\n\n\nlof = LocalOutlierFactor(n_neighbors = 10, n_jobs = -1, novelty = False)\n\ny_outlier = lof.fit_predict(X_train_norm)\n\n\n###############################################################3\n\n\nfrom sklearn.cross_decomposition import PLSRegression\n\npls = PLSRegression(n_components= 15)\npls.fit(X_train_reduced, y_train_reduced)\n\n\n\n\n\nX_train_transformed = pd.DataFrame( pls.transform(X_train_reduced), index = X_train_reduced.index)","5e2d6d99":"from sklearn.neighbors import NearestNeighbors\nfrom sklearn.model_selection import StratifiedShuffleSplit\n\nsss = StratifiedShuffleSplit(n_splits = 5, test_size=0.333, random_state=0)\nsss.get_n_splits(X_train_transformed, y_train_reduced)\n\ntrust_score_final = []\ntrust_score_df = (X_train_transformed.join(y_train_reduced).reset_index()).join(pd.Series (y_outlier).rename('outlier_treino') )\nfor train_index, test_index in sss.split(X_train_transformed, y_train_reduced):\n\n    searched_df = trust_score_df.iloc[train_index].query('outlier_treino != -1')\n    searched_df = searched_df.drop(['index', 'outlier_treino'], axis = 1)\n    \n    searched_df_target0 = searched_df.query(y_train_reduced.name + \" == 0\")\n    neigh_target0 = NearestNeighbors(n_neighbors = 2, n_jobs = -1)\n    neigh_target0.fit(searched_df_target0.drop([y_train_reduced.name], axis =1))\n\n    searched_df_target1 = searched_df.query(y_train_reduced.name + \" == 1\")\n    neigh_target1 = NearestNeighbors(n_neighbors = 2, n_jobs = -1)\n    neigh_target1.fit(searched_df_target1.drop([y_train_reduced.name], axis =1))\n\n    # Busca 2 n_neighbours, de forma a retornar pela funcao abaixo o valor minimo diferente de zero,\n    # caso exista duplicata de elemento de X_test em X_train, em que distancia seria 0\n\n    distance_class_0 = list(map( lambda x: np.min(x[np.nonzero(x)]), neigh_target0.kneighbors(pls.transform(X_new_test))[0]))\n    \n    distance_class_1 = list(map( lambda x: np.min(x[np.nonzero(x)]), neigh_target1.kneighbors(pls.transform(X_new_test))[0]))\n\n    # trust score = distancia para classe diferente da prevista \/ distancia para classe prevista\n\n    trust_score = []\n    for i in range (0, len(y_new_test)):\n        trust_score.append(  distance_class_0[i] \/ distance_class_1[i] if y_predicted.tolist()[i] > 0.5 else\n                             distance_class_1[i] \/ distance_class_0[i])\n\n    trust_score = list(map( float, trust_score ))\n    trust_score_final.append(trust_score)\ntrust_score_final_df = pd.DataFrame( trust_score_final, columns = X_new_test.index ).mean().rename('trust_score')","cf4e05a3":"query_acertos = '(y_predicted > 0.5 and target == 1) or (y_predicted < 0.5 and target == 0)'\n\nprint('A quantidade de previs\u00f5es acertadas \u00e9 : ' + str( len(X_new_test.join(y_new_test).join(trust_score_final_df).join(y_predicted).query( query_acertos )) ) )\nprint('M\u00e9dia de trust_score em s\u00e3o previs\u00f5es certas : ' + str( X_new_test.join(y_new_test).join(trust_score_final_df).join(y_predicted).query( query_acertos )['trust_score'].mean()) )\nprint('Desvio padr\u00e3o do Trust Score em previs\u00f5es certas : ' + str(X_new_test.join(y_new_test).join(trust_score_final_df).join(y_predicted).query( query_acertos )['trust_score'].std()) )","40a1be7e":"query_erros = '(y_predicted > 0.5 and target == 0) or (y_predicted < 0.5 and target == 1)'\n\nprint('A quantidade de previs\u00f5es erradas \u00e9 : ' + str( len( X_new_test.join(y_new_test).join(trust_score_final_df).join(y_predicted).query( query_erros ) ) ))\nprint('M\u00e9dia de trust_score em s\u00e3o previs\u00f5es equivocadas : ' + str( X_new_test.join(y_new_test).join(trust_score_final_df).join(y_predicted).query( query_erros )['trust_score'].mean() ))\nprint('Desvio padr\u00e3o do Trust Score em previs\u00f5es equivocadas : ' + str( X_new_test.join(y_new_test).join(trust_score_final_df).join(y_predicted).query( query_erros )['trust_score'].std()) )","16e08be1":"from sklearn.preprocessing import Normalizer\n\nnorm = Normalizer().fit(X_train)  # fit does nothing.\n\nX_train_norm = pd.DataFrame( norm.transform(X_train), index = X_train.index)","f8907f3b":"predicted_score_true_label = []\n\npredicted_score_predicted_label = []\nfor i in range(0 , len(y_score)):\n    predicted_score_true_label.append( y_score.tolist()[i] if y_test.tolist()[i] == 1 else 1 - y_score.tolist()[i] )\n    predicted_score_predicted_label.append( max([y_score.tolist()[i], 1 - y_score.tolist()[i]]) )\n\n\nfrom sklearn.decomposition import PCA\n\npca = PCA(n_components = 1)\npca.fit(X_train)\nlens_pca = pca.transform(X_test)\n\nlens = np.c_[lens_pca, y_test, predicted_score_true_label, predicted_score_predicted_label]","6cd4f344":"graph = mapper.map(lens,\n                   norm.transform(X_test),\n                   cover=km.Cover(n_cubes= 5, perc_overlap=0.5),\n                   remove_duplicate_nodes = True,\n                   clusterer=sklearn.cluster.KMeans(n_clusters=2,\n                                                    random_state= 42))\n\n\n# Visualization\ngrafico = mapper.visualize(graph,\n                 path_html=\"\/kaggle\/working\/elliptic-dataset  Erro Analysis Prediction Score.html\",\n                 title=\" Illicit Transaction - Elliptic Dataset - Error Analysis with Heatmap Prediction Score\",\n                 color_function= np.array(y_score))\n\n\n# Visualization\ngrafico = mapper.visualize(graph,\n                 path_html=\"\/kaggle\/working\/elliptic-dataset Error Analysis True Label.html\",\n                 title=\" Illicit Transaction - Elliptic Dataset -  Error Analysis with Heatmap True Label\",\n                 color_function= np.array(y_test))","77aa1404":"\nfrom sklearn.preprocessing import Normalizer\n\nX_train_reduced = X_train.query('time_step <= 23')[colunas_escolhidas]\ny_train_reduced = y_train.loc[X_train_reduced.index]\nX_new_test = X_train.query('time_step >= 28')[colunas_escolhidas]\ny_new_test = y_train.loc[X_new_test.index]\n\n\nrf.fit(X_train_reduced, y_train_reduced)\ny_predicted = rf.predict_proba(X_new_test)\ny_predicted = pd.DataFrame( y_predicted, index = X_new_test.index)[1].rename('y_predicted')\n\n\nfrom sklearn.preprocessing import Normalizer\n\nnorm = Normalizer().fit(X_train)  # fit does nothing.\n\nX_train_norm = pd.DataFrame( norm.transform(X_train), index = X_train.index)","46020e7d":"predicted_score_true_label = []\n\npredicted_score_predicted_label = []\nfor i in range(0 , len(y_predicted)):\n    predicted_score_true_label.append( y_predicted.tolist()[i] if y_new_test.tolist()[i] == 1 else 1 - y_predicted.tolist()[i] )\n    predicted_score_predicted_label.append( max([y_predicted.tolist()[i], 1 - y_predicted.tolist()[i]]) )\n\n\nfrom sklearn.decomposition import PCA\n\npca = PCA(n_components = 1)\npca.fit(X_train_reduced)\nlens_pca = pca.transform(X_new_test)\n\nlens = np.c_[lens_pca, y_new_test, predicted_score_true_label, predicted_score_predicted_label]","a3e00992":"graph = mapper.map(lens,\n                   norm.transform(X_new_test),\n                   cover=km.Cover(n_cubes= 5, perc_overlap=0.5),\n                   remove_duplicate_nodes = True,\n                   clusterer=sklearn.cluster.KMeans(n_clusters=2,\n                                                    random_state= 42))\n\n\n# Visualization\ngrafico = mapper.visualize(graph,\n                 path_html=\"\/kaggle\/working\/elliptic-dataset  Erro Analysis Prediction Score new_test.html\",\n                 title=\" Illicit Transaction - Elliptic Dataset - Heatmap Prediction Score new_test\",\n                 color_function= np.array(y_predicted))\n\n\n# Visualization\ngrafico = mapper.visualize(graph,\n                 path_html=\"\/kaggle\/working\/elliptic-dataset Error Analysis True Label new_test.html\",\n                 title=\" Illicit Transaction - Elliptic Dataset - Heatmap True Label new_test\",\n                 color_function= np.array(y_new_test))","2d1204af":"adapted_graph = km.adapter.to_networkx(graph)\n\nfrom matplotlib.pyplot import figure\nfigure(figsize=(20, 10))\n\n\nimport community as community_louvain\nimport networkx as nx\nimport matplotlib.cm as cm\n\n#G = nx.erdos_renyi_graph(100, 0.01)\npartition = community_louvain.best_partition(adapted_graph, resolution = 1.3, random_state = 0)\n\n\n# draw the graph\npos = nx.spring_layout(adapted_graph)\n# color the nodes according to their partition\ncmap = cm.get_cmap('viridis', max(partition.values()) + 1)\nnx.draw_networkx_nodes(adapted_graph, pos, partition.keys(), node_size=40, \n                        cmap=cmap, node_color=list(partition.values()))\nnx.draw_networkx_edges(adapted_graph, pos, alpha=0.5)","720888b5":"lista_ids_comunity_2 = [512, 1031, 523, 1036, 524, 2061, 1040, 528, 529, 1041, 1045, 1046, 535, 536, 1572, 3622, 1066, 1067, 4144, 4145, 570, 572, 578, 579, 1096, 585, 1614, 1117, 1119, 1120, 611, 612, 1123, 2148, 617, 3700, 2679, 2177, 2178, 645, 646, 1160, 140, 2188, 1168, 659, 155, 1182, 1695, 1696, 1697, 163, 164, 1190, 171, 1207, 1212, 2239, 2248, 1738, 4302, 4303, 723, 724, 1254, 1262, 1264, 1266, 3319, 252, 1279, 1285, 1801, 1802, 4881, 1303, 792, 1306, 794, 1308, 285, 1313, 1317, 1321, 298, 826, 1851, 2366, 1351, 841, 842, 1355, 1365, 1885, 863, 2404, 1384, 1385, 4458, 1387, 878, 1904, 371, 883, 884, 374, 887, 1403, 386, 900, 904, 2441, 1420, 1422, 920, 939, 4524, 2477, 433, 1972, 2488, 442, 3518, 1471, 1472, 456, 972, 461, 462, 470, 471, 1496, 2525, 993, 488, 489, 490, 2028, 2550, 2553]\n\nlista_ids_comunity_0 = [4108, 4119, 4126, 2083, 4135, 2100, 4162, 2129, 4186, 4187, 2144, 2164, 2171, 128, 2183, 4235, 2200, 2204, 192, 4288, 2249, 4301, 209, 2258, 4315, 2269, 2271, 2277, 2280, 2284, 2296, 2319, 2322, 2339, 2342, 2345, 2369, 4419, 2373, 2375, 2383, 2386, 339, 2388, 347, 2406, 362, 2415, 4468, 4469, 2422, 2426, 2454, 2455, 4506, 2461, 4509, 4527, 4528, 2481, 2485, 2494, 4547, 4548, 4551, 4559, 2518, 2528, 4579, 4583, 2536, 4586, 2541, 2542, 4597, 4603, 4624, 538, 2591, 4645, 2604, 2611, 4662, 2625, 4674, 2629, 4680, 4681, 2634, 2635, 595, 596, 4705, 4718, 4729, 4759, 2717, 2720, 2721, 4776, 4790, 4796, 4802, 2757, 2758, 2773, 4824, 2780, 4848, 4854, 2818, 2828, 2829, 782, 2836, 2850, 4905, 2863, 2864, 2867, 2868, 2884, 2893, 2894, 2902, 2903, 4954, 2907, 2909, 4969, 2930, 2935, 2959, 2965, 2966, 2979, 2980, 2982, 2983, 2988, 2990, 943, 2991, 2999, 3003, 960, 964, 3012, 3015, 3016, 3020, 3025, 980, 3029, 1018, 3091, 3092, 3093, 3101, 3124, 3125, 3136, 3140, 1100, 3154, 3155, 3157, 3159, 3168, 3178, 3180, 3189, 3201, 3202, 3204, 3205, 3206, 3207, 3212, 3213, 3218, 3219, 3221, 1174, 3224, 3241, 3253, 3254, 3269, 3270, 1228, 3280, 3286, 3293, 3298, 3301, 3303, 3304, 3322, 3327, 3332, 3335, 3336, 3348, 3355, 3373, 3375, 3376, 1335, 3395, 3396, 3405, 3406, 3409, 3413, 1371, 3422, 3446, 3447, 3450, 3451, 3456, 3457, 3465, 3466, 3478, 3482, 1435, 3483, 3485, 3490, 3491, 3493, 3494, 3504, 1480, 3532, 3550, 3556, 1509, 1510, 3559, 3577, 3580, 3583, 3591, 3595, 3598, 3606, 3607, 1566, 1576, 3656, 3664, 3669, 3671, 3672, 3677, 3682, 3697, 3715, 3733, 3734, 3737, 3741, 3742, 3765, 3782, 3787, 3788, 3790, 3791, 3797, 3803, 3804, 3805, 3806, 3808, 3811, 3813, 3815, 3817, 3823, 3824, 3829, 3834, 3839, 3844, 3849, 3857, 3861, 3862, 3870, 1825, 3878, 3886, 3891, 3895, 3896, 3908, 3936, 3938, 3964, 1923, 1931, 3991, 3994, 1952, 1957, 4006, 1962, 1967, 4017, 1971, 4050, 2007, 4057, 4058, 2012, 2016, 4064, 4070, 4076, 2033, 2041]","a356a488":"X_train['erro_tipo_1'] = 0\nindex_community_2 = list(X_train.iloc[lista_ids_comunity_2].index)\nX_train.loc[index_community_2, ['erro_tipo_1']] = 1\n\nX_train['erro_tipo_2'] = 0\nindex_community_0 = list(X_train.iloc[lista_ids_comunity_0].index)\nX_train.loc[index_community_0, ['erro_tipo_2']] = 1","fe84432a":"try:\n    X_train.drop(['lda_transform_erro_1_test', 'lda_transform_erro_2_test'], axis = 1, inplace = True)\nexcept:\n    None\n\n\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n\npls_erro_1 = PLSRegression(n_components= 50)\npls_erro_1.fit(X_train.drop(['erro_tipo_1', 'erro_tipo_2'], axis = 1), X_train['erro_tipo_1'])\n\nlda_erro_1 = LinearDiscriminantAnalysis()\nlda_transform_erro_tipo_1 = lda_erro_1.fit_transform(pls_erro_1.transform( X_train.drop(['erro_tipo_1', 'erro_tipo_2'], axis = 1) ), X_train['erro_tipo_1'])\nlda_transform_erro_tipo_1 = pd.DataFrame(lda_transform_erro_tipo_1, index = X_train.index)[0].rename('lda_transform_erro_1_test')\nlda_proba_erro_tipo_1 = pd.DataFrame( lda_erro_1.predict_proba(pls_erro_1.transform( X_train.drop(['erro_tipo_1', 'erro_tipo_2'], axis = 1) )) )[1].rename('lda_prob_erro_tipo_1')\n\n#############################################################\n\n\npls_erro_2 = PLSRegression(n_components= 50)\npls_erro_2.fit(X_train.drop(['erro_tipo_1', 'erro_tipo_2'], axis = 1), X_train['erro_tipo_2'])\n\nlda_erro_2 = LinearDiscriminantAnalysis()\nlda_transform_erro_tipo_2 = lda_erro_2.fit_transform(pls_erro_2.transform( X_train.drop(['erro_tipo_1', 'erro_tipo_2'], axis = 1) ), X_train['erro_tipo_2'])\nlda_transform_erro_tipo_2 = pd.DataFrame(lda_transform_erro_tipo_2, index = X_train.index)[0].rename('lda_transform_erro_2_test')\nlda_proba_erro_tipo_2 = pd.DataFrame( lda_erro_1.predict_proba(pls_erro_1.transform( X_train.drop(['erro_tipo_1', 'erro_tipo_2'], axis = 1) )) )[1].rename('lda_prob_erro_tipo_2')\n\n\n##############################################################\n\nX_train['lda_transform_erro_1_test'] = lda_transform_erro_tipo_1\nX_train['lda_transform_erro_2_test'] = lda_transform_erro_tipo_2","6a361bdf":"rf = RandomForestClassifier(**parametrizacao['hyperparams'], random_state = 10)\nrf.fit(X_train[colunas_escolhidas + ['lda_transform_erro_1_test']], y_train)\n\n\n# Criacao das variaveis indicadoras de erro tipo 1 e 2 no teste\n\nlda_transform_erro_1_test = lda_erro_1.transform(pls_erro_1.transform( X_test ) )\nlda_transform_erro_1_test = pd.DataFrame( lda_transform_erro_1_test, index = X_test.index)[0].rename('lda_transform_erro_1_test')\n\n#lda_transform_erro_2_test = lda_erro_2.transform(pls_erro_2.transform( X_test ) )\n#lda_transform_erro_2_test = pd.DataFrame( lda_transform_erro_2_test, index = X_test.index)[0].rename('lda_transform_erro_2_test')\n\n#lda_transform_erros = pd.DataFrame(lda_transform_erro_1_test).join(lda_transform_erro_2_test)\n\n###################################\n\ny_score = rf.predict_proba(X_test[colunas_escolhidas].join(lda_transform_erro_1_test))\ny_score = pd.DataFrame( y_score )[1]\n\nroc_auc_score(y_test, y_score)","f1d54454":"rf = RandomForestClassifier(**parametrizacao['hyperparams'], random_state = 10)\nrf.fit(X_train[colunas_escolhidas + ['lda_transform_erro_2_test']], y_train)\n\n\n# Criacao das variaveis indicadoras de erro tipo 1 e 2 no teste\n\n#lda_transform_erro_1_test = lda_erro_1.transform(pls_erro_1.transform( X_test ) )\n#lda_transform_erro_1_test = pd.DataFrame( lda_transform_erro_1_test, index = X_test.index)[0].rename('lda_transform_erro_1_test')\n\nlda_transform_erro_2_test = lda_erro_2.transform(pls_erro_2.transform( X_test ) )\nlda_transform_erro_2_test = pd.DataFrame( lda_transform_erro_2_test, index = X_test.index)[0].rename('lda_transform_erro_2_test')\n\nlda_transform_erros = pd.DataFrame(lda_transform_erro_1_test).join(lda_transform_erro_2_test)\n\n###################################\n\ny_score = rf.predict_proba(X_test[colunas_escolhidas].join(lda_transform_erro_2_test))\ny_score = pd.DataFrame( y_score )[1]\n\nroc_auc_score(y_test, y_score)","acf19d66":"lda_transform_erro_1_unlabeled = lda_erro_1.transform( pls_erro_1.transform( unlabeled_df_train.drop(['target'], axis = 1) ) ) \nlda_transform_erro_1_unlabeled = pd.DataFrame( lda_transform_erro_1_unlabeled, index = unlabeled_df_train.index)[0].rename('lda_transform_erro_1_test')","95dc1790":"try:\n    unlabeled_df_train.drop(['target'], axis = 1, inplace = True)\nexcept:\n    None\n\nlda_transform_erro_1_unlabeled = lda_erro_1.transform(pls_erro_1.transform( unlabeled_df_train ) ) \nlda_transform_erro_1_unlabeled = pd.DataFrame( lda_transform_erro_1_unlabeled, index = unlabeled_df_train.index)[0].rename('lda_transform_erro_1_test')\n\nnovas_colunas_escolhidas = colunas_escolhidas + ['lda_transform_erro_1_test']\n\n\nrf = RandomForestClassifier(**parametrizacao['hyperparams'], random_state = 1)\nsampled_df = X_train[novas_colunas_escolhidas].sample(frac = 0.97, random_state = 1)\ny_sampled = y_train.loc[sampled_df.index]\nrf.fit( sampled_df, y_sampled )\ny_labeled_score1 = rf.predict_proba( unlabeled_df_train.join(lda_transform_erro_1_unlabeled)[novas_colunas_escolhidas] )\ny_labeled_score1 = pd.DataFrame( y_labeled_score1, index = unlabeled_df_train.index ).rename(columns = {1 : 'y_labeled_score1'})\ny_label1 = rf.predict( unlabeled_df_train.join(lda_transform_erro_1_unlabeled)[novas_colunas_escolhidas] )\ny_label1 = pd.DataFrame( y_label1, index = unlabeled_df_train.index ).rename(columns = {0 : 'y_label1'})\nlabels_1 = y_labeled_score1.join(y_label1)[['y_labeled_score1', 'y_label1']]\n\n\nrf = RandomForestClassifier(**parametrizacao['hyperparams'], random_state = 2)\nsampled_df = X_train[novas_colunas_escolhidas].sample(frac = 0.97, random_state = 2)\ny_sampled = y_train.loc[sampled_df.index]\nrf.fit( sampled_df, y_sampled )\ny_labeled_score2 = rf.predict_proba( unlabeled_df_train.join(lda_transform_erro_1_unlabeled)[novas_colunas_escolhidas] )\ny_labeled_score2 = pd.DataFrame( y_labeled_score2, index = unlabeled_df_train.index ).rename(columns = {1 : 'y_labeled_score2'})\ny_label2 = rf.predict( unlabeled_df_train.join(lda_transform_erro_1_unlabeled)[novas_colunas_escolhidas] )\ny_label2 = pd.DataFrame( y_label2, index = unlabeled_df_train.index ).rename(columns = {0 : 'y_label2'})\nlabels_2 = y_labeled_score2.join(y_label2)[['y_labeled_score2', 'y_label2']]\n\n\nrf = RandomForestClassifier(**parametrizacao['hyperparams'], random_state = 3)\nsampled_df = X_train[novas_colunas_escolhidas].sample(frac = 0.97, random_state = 3)\ny_sampled = y_train.loc[sampled_df.index]\nrf.fit( sampled_df, y_sampled )\ny_labeled_score3 = rf.predict_proba( unlabeled_df_train.join(lda_transform_erro_1_unlabeled)[novas_colunas_escolhidas] )\ny_labeled_score3 = pd.DataFrame( y_labeled_score3, index = unlabeled_df_train.index ).rename(columns = {1 : 'y_labeled_score3'})\ny_label3 = rf.predict( unlabeled_df_train.join(lda_transform_erro_1_unlabeled)[novas_colunas_escolhidas] )\ny_label3 = pd.DataFrame( y_label3, index = unlabeled_df_train.index ).rename(columns = {0 : 'y_label3'})\nlabels_3 = y_labeled_score3.join(y_label3)[['y_labeled_score3', 'y_label3']]\n\nlabels_df = labels_1.join(labels_2).join(labels_3)\nlabels_df['target'] = -1\nlabels_df.loc[ labels_df.query('y_labeled_score1 > 0.8 and y_labeled_score2 > 0.8 and y_labeled_score3 > 0.8').index, ['target'] ] = 1\nlabels_df.loc[ labels_df.query('y_labeled_score1 <= 0.1 and y_labeled_score2 <= 0.1 and y_labeled_score3 <= 0.1').index, ['target'] ] = 0\n\nunlabeled_df_train['target'] = labels_df['target']","9a4ab248":"rf = RandomForestClassifier(**parametrizacao['hyperparams'], random_state = 42)\nrf.fit(X_train.append(unlabeled_df_train.join(lda_transform_erro_1_unlabeled).query('target != -1'))[novas_colunas_escolhidas], y_train.append(unlabeled_df_train.query('target != -1')['target']))\n\n\n# Criacao das variaveis indicadoras de erro tipo 1 e 2 no teste\n\nlda_transform_erro_1_test = lda_erro_1.transform(pls_erro_1.transform( X_test ) )\nlda_transform_erro_1_test = pd.DataFrame( lda_transform_erro_1_test, index = X_test.index)[0].rename('lda_transform_erro_1_test')\n\n###################################\n\ny_score = rf.predict_proba(X_test.join(lda_transform_erro_1_test)[novas_colunas_escolhidas])\ny_score = pd.DataFrame( y_score )[1]\n\nroc_auc_score(y_test, y_score)","b94139c2":"predicted_score_true_label = []\n\npredicted_score_predicted_label = []\nfor i in range(0 , len(y_score)):\n    predicted_score_true_label.append( y_score.tolist()[i] if y_score.tolist()[i] == 1 else 1 - y_score.tolist()[i] )\n    predicted_score_predicted_label.append( max([y_score.tolist()[i], 1 - y_score.tolist()[i]]) )\n\n\nfrom sklearn.decomposition import PCA\n\npca = PCA(n_components = 1)\npca.fit(X_train.drop(['erro_tipo_1', 'erro_tipo_2', 'lda_transform_erro_1_test', 'lda_transform_erro_2_test'], axis =1))\nlens_pca = pca.transform(X_test)\n\nlens = np.c_[lens_pca, y_test, predicted_score_true_label, predicted_score_predicted_label]","f1f758ee":"graph = mapper.map(lens,\n                   norm.transform(X_test),\n                   cover=km.Cover(n_cubes= 5, perc_overlap=0.5),\n                   remove_duplicate_nodes = True,\n                   clusterer=sklearn.cluster.KMeans(n_clusters=2,\n                                                    random_state= 42))\n\n\n# Visualization\ngrafico = mapper.visualize(graph,\n                 path_html=\"\/kaggle\/working\/elliptic-dataset  Erro Analysis Prediction Score RESULTADO FINAL.html\",\n                 title=\" Illicit Transaction - Elliptic Dataset - Heatmap Prediction Score - resultado final\",\n                 color_function= np.array(y_test))\n\n\n# Visualization\ngrafico = mapper.visualize(graph,\n                 path_html=\"\/kaggle\/working\/elliptic-datasetn Error Analysis True Label RESULTADO FINAL.html\",\n                 title=\" Illicit Transaction - Elliptic Dataset - Heatmap True Label - resultado final\",\n                 color_function= np.array(y_score))","dadd4e42":"import nbconvert","13d96842":"1. Chamando todos os dados\n2. Definindo X_train e X_test\n3. Arrumando valores de target e coluna que usava palavra reservada","713d59f3":"# Primeira fase de resultado : Modelo State-of-art\n1. Random Forest","94b6209a":"# Reflex\u00f5es sobre Trust Score","1f325259":"# Abaixo s\u00e3o geradas as duas variaveis novas - produzidas em dois passos simples:\n1. A aplica\u00e7\u00e3o de um Partial Least Square no dataset, com formato one-versus-all, com target sendo todas transa\u00e7\u00f5es que foram agrupadas na mesma comunidade com os principais erros de seu pr\u00f3prio tipo (seja tipo 1 ou tipo 2)\n2. O uso da redu\u00e7\u00e3o de dimensionalidade por um LDA, aplicado sobre as dimens\u00f5es n\u00e3o correlacionadas geradas pela PLS, reduzindo efetivamente para uma dimens\u00e3o para cada tipo de erro\n3. Obtemos assim duas novas variaveis a serem adicionadas na Random Forest, no lugar da cria\u00e7\u00e3o de uma nova camada de ensembles sobre as decis\u00f5es feitas por ela, complexificando seus resultados\n\n\n** Obs: os dois m\u00e9todos aplicados (PLS e LDA) assumem pressupostos sobre os dados, que podem n\u00e3o valer; por isso, testamos todo o processo anterior com essas novas duas variaveis produzidas*","423135a2":"# 1) Otimiza\u00e7\u00e3o bayesiana de modelos state-of-art (wrapper + hyperparametros)\n\n\n*Obs1: Busca utilizando cross-validation temporal com 4 folds (20 time_steps para treino - pula 4 - todo restante at\u00e9 time_step 34 para teste), avan\u00e7ando treino e teste um time_step para cada fold*\n\n*Obs2: Uma busca de hyperparametros \u00f3timos se torna enviesada por features que j\u00e1 tenham sido definidas; assim como uma busca por melhores features, sem varia\u00e7\u00e3o de hiperparametros enviesa quais features ser\u00e3o mais propensas a serem escolhidas.*","2a17517a":"# Visualiza\u00e7\u00e3o Final\n\n* Relembrando que essas visualiza\u00e7\u00f5es podem ser realizadas a cada time step de forma a verificar como se encontra o comportamento do modelo - ou especificamente quando ocorrem varia\u00e7\u00f5es consideraveis em sua performance no tempo, orientando os problemas recentes e como corrigir as altera\u00e7\u00f5es, seja de forma manual ou autom\u00e1tica","231f95cc":"* Essa segunda testagem, em outro periodo de tempo, sugere a existencia de estabilidade temporal na m\u00e9dia e desvio padr\u00e3o desse padr\u00e3o observado no trust scores das classifica\u00e7\u00f5es erradas, com esses valores tendo sido obtidos por uma segunda vez muito pr\u00f3ximos dos obtidos inicialmente, embora com testagem feita em outro conjunto de meses - com um intervalo gerado a partir desses valores podendo ser usado para indica\u00e7\u00e3o de confiabilidade das classifica\u00e7\u00f5es","78795e87":"%%capture\n\n# Global variable\nglobal  ITERATION\n\nITERATION = 0\n\n# Run optimization\nbest = fmin(fn = objective, space = space, algo = tpe.suggest, \n            max_evals = 1500, trials = bayes_trials, rstate = np.random.RandomState(50))","1477a062":"# Hipoteses futuras\n* A utiliza\u00e7\u00e3o de um processo semi-supervisionado mais refinado pode trazer mais ganho do que o obtido, junto desses procedimentos\n* A utiliza\u00e7\u00e3o de um intervalo no entorno da m\u00e9dia de trust score das classifica\u00e7\u00f5es erradas deve auxiliar como filtro no processo semi-supervisionado, diminuindo ruido adicionado retreino do dataset\n* Em modelos com performance n\u00e3o t\u00e3o alta, impacto de corre\u00e7\u00e3o das piores zonas de erro do modelo deve ser consideravelmente maior, assim como a utiliza\u00e7\u00e3o de ensembles para isso (SVMs ou outros) que foquem nessa detec\u00e7\u00e3o a partir diversos pressupostos diferentes, em vez da gera\u00e7\u00e3o de uma nova feature para ser adicionada no modelo antigo - como feito aqui, em vistas da discuss\u00e3o sobre interpretabilidade e busca de utiliza\u00e7\u00e3o desse m\u00e9todo sem adicionar uma nova camada de complexidade sobre o problema\n* Em casos mais complexos, como multiclasses, as zonas de confus\u00e3o e erro sistematico do modelo devem ser mais espalhadas e com ganho de performance mais sensivel ao se utilizar esse procedimento\n* Um estudo voltado a estabilidade temporal das variaveis pode ser importante para aplica\u00e7\u00e3o desse m\u00e9todo com mais eficiencia, dado que a detec\u00e7\u00e3o de erro pode ser construida em cima de features que n\u00e3o ir\u00e3o manter sua capacidade de previs\u00e3o pelo tempo - assim como variaveis utilizadas no pr\u00f3prio modelo","513becc6":"* Performance base com hiperparametros e uso de 78 features das 166 ficou ao redor de 0.912 - 0.913","c8ecdf4b":"# Com esses resultados, todo o processo anterior de aprendizado semi-supervisionado foi repetido - com adi\u00e7\u00e3o de v\u00e1riavel de identifica\u00e7\u00e3o para erro tipo I","0bf0d1b5":"# M\u00e9dia e desvio padr\u00e3o do trust score das classifica\u00e7\u00f5es erradas - test set","e3173e17":"# Trust Score - test dataset\n\nMedida proposta inicialmente nos artigos \"Identifying unreliable predictions in clinical risk models\" e \"To trust or not a classifier\", desenvolvida na \u00e1rea da saude para quantificar a confiabilidade que deveria ser dada as previs\u00f5es de um modelo para cada caso espec\u00edfico; dado que podem existir subpopula\u00e7\u00f5es que o modelo erra sistematicamente, embora performe bem globalmente, com erros de tipo 1 ou tipo 2 tendo muito peso devido a suas implica\u00e7\u00f5es m\u00e9dicas\n\nA nossa hipotese, discutida abaixo foi sua exten\u00e7\u00e3o para \u00e1rea de previs\u00e3o de transa\u00e7\u00f5es ilicitas; em que erros de tipo I e tipo II tambem tem graves consequencias para os individuos envolvidos. Pensamos em 3 aplica\u00e7\u00f5es possiveis dessa m\u00e9trica\n1. A diretamente proposta pelos artigos, de medida de confiabilidade das previs\u00f5es do modelo para novas previs\u00f5es realizadas (que assume importancia grande, dado estarmos em um cenario semi-supervisionado, em que as previs\u00f5es podem ter implica\u00e7\u00f5es grandes e n\u00e3o temos a classifica\u00e7\u00e3o direta sobre a maior parte dos dados)\n2. A de possivel filtro a ser aplicado nas observa\u00e7\u00f5es de um modelo semi-supervisionado - em que observa\u00e7\u00f5es que tivessem trust scores muito baixos seriam cortadas, de maneira aos exemplos inicialmente n\u00e3o classificados e com baixa confiabilidade sobre as previs\u00f5es n\u00e3o serem utilizados na etapa de retreino do modelo, na qual utiliza dados classificados por ele mesmo em seu conjunto de treino\n3. Como medida possivel sobre performance do modelo pelo tempo; dado o cen\u00e1rio de que podem existir meses com pouquissimas observa\u00e7\u00f5es classificadas, mas ainda sim podendo ter uma m\u00e9trica que oriente no\u00e7\u00f5es gerais sobre a sua estabilidade no tempo","b52d67e5":"* A hip\u00f3tese inicial, pensada a partir dos artigos \"Identifying unreliable predictions in clinical risk models\" e \"To trust or not a classifier\"  era de que o comportamento do score de confiabilidade teria uma rela\u00e7\u00e3o linear, ou ao menos monotonica, com a imprecis\u00e3o medida nas probabilidades geradas pelos modelos - com os menores valores concentrando as previs\u00f5es menos confiaveis\n\n* Essa hipotese estava equivocada - impedindo obten\u00e7\u00e3o de melhora de performance no modelo pelo uso do trust score como um filtro das classifica\u00e7\u00f5es para retreino do modelo semi-supervisionado - mas nessa \u00e9tapa - embora tenhamos posteriormente descobertos que existe uma rela\u00e7\u00e3o n\u00e3o linear, indicada abaixo\n\n* A m\u00e9dia e desvio padr\u00e3o dos Trust Scores de previs\u00f5es certas e erradas s\u00e3o sensivelmente diferentes, com o trust score de previs\u00f5es erradas sendo bastante mais concentrado e tendo desvio padr\u00e3o 3 vezes menor frente ao desvio padr\u00e3o das acertadas","b4f7896e":"# Lentes usadas em proje\u00e7\u00f5es para estudo por an\u00e1lise topol\u00f3gica\n* Confirmamos visualmente a partir de estudos a coloca\u00e7\u00e3o feita em \"Machine learning methods to detect money laundering in theBitcoin blockchain in the presence of label scarcity\" sobre a baixa eficiencia de modelos de anomaly detection nesse problema, ao utilizarmos o score de anomalia produzido por uma Isolation Forest (sendo um dos principais algoritmos dessa \u00e1rea) e observando uma visualiza\u00e7\u00e3o dos dados a partir dessa lente que apresenta diferencia\u00e7\u00e3o consideravelmente menor do que a obtida no uso de uma lente como o primeiro componente de um PLS, com seus componentes voltados a maximiza\u00e7\u00e3o da variancia linear em X que produza maior variancia em Y","7e722769":"# Foi observado o impacto dos resultados abaixo com teste em diversas seeds diferentes para constatar varia\u00e7\u00f5es aleat\u00f3rias\n* A nova variavel criada para os principais erros de tipo 1 retornou resultados consistentemente acima dos resultados sem ela, com AUC ficando entre 0.917 a 0.921 (comparavel com resultado obtido anteriormente em situa\u00e7\u00e3o ap\u00f3s retreino do modelo com expan\u00e7\u00e3o do dataset de treino com novas classifica\u00e7\u00e3o n\u00e3o inicialmente classificadas)\n* A nova variavel criada para os principais erros de tipo 2 n\u00e3o retornou resultados de melhora como a de tipo 1 - permanecendo na zona de AUC anteriora do modelo; e inclusive diminuindo a performance do modelo quando combinada com a variavel criada para identifica\u00e7\u00e3o de erro de tipo 1 (provavelmente devido a overfitting de algum tipo ou varia\u00e7\u00e3o temporal do comportamento desse erro) ","8977cb94":"* Performance com dataset de treino expandido adicionado ficou ao redor de 0.920 - 0.921","082f153b":"bayes_trials_results = sorted(bayes_trials.results, key = lambda x: x['loss'])\nbayes_trials_results","a7b68ca0":"# Acima se encontra a primeira visualiza\u00e7\u00e3o obtida do dataset de teste - uma contendo heatmap a partir da classe real e outra contendo heatmap a partir da probabilidade gerada de ser ilicita pelo modelo\n\n* Abaixo, as principais zonas que contem os principais erros de tipo 1 e tipo 2 ser\u00e3o agrupadas a partir de um algoritmo de community detection em grafos\n* Em seguida, uso os index dos membros que pertencem a essas zonas para criar duas novas features a serem adicionadas no modelo - voltadas especificamente para a Random Forest consegua ter acesso a v\u00e1riaveis que indiquem quando as previs\u00f5es que esta realizando s\u00e3o semelhantes a erros cometidos; assim podendo os corrigir","8912bb5a":"# 2) Aprendizagem semi-supervisionada\n\nDevemos espremer o m\u00e1ximo possivel do cenario do problema - em que pouquissimas transa\u00e7\u00f5es est\u00e3o classificadas entre a popula\u00e7\u00e3o total\n\nOs artigos existentes sobre o dataset elliptic disponiveis na internet se focam exclusivamente em seu estudo por uma abordagem supervisionada ou de anomaly detection, n\u00e3o utilizando t\u00e9cnicas de semi-supervisined learning, como forma de aumentar o dataset de treino a partir de transa\u00e7\u00f5es j\u00e1 disponiveis, porem n\u00e3o classificadas inicialmente, e diminuir o problema de vari\u00e2ncia diagnosticado.\n\n","eda85144":"'''\nParticao de erro tipo 1\nModelo indica ser classe 1 mas era classe 0\n'''\n\nlista = [] for i in partition.keys(): if partition[i] == 2: lista.append(i)\n\nids = [] for elemento in lista: if elemento in graph['nodes'].keys(): ids.append(graph['nodes'][elemento])\n\nlista_ids_comunity_2 = list( set([item for sublist in ids for item in sublist]) ) print(lista_ids_comunity_2) len(lista_ids_comunity_2)\n\n\n#########################################################################\n\n'''\nParticao de erro tipo 2\nModelo indica ser classe 0 mas era classe 1\n'''\n\nlista = [] for i in partition.keys(): if partition[i] == 0: lista.append(i)\n\nids = [] for elemento in lista: if elemento in graph['nodes'].keys(): ids.append(graph['nodes'][elemento])\n\nlista_ids_comunity_0 = list( set([item for sublist in ids for item in sublist]) ) print(lista_ids_comunity_0) len(lista_ids_comunity_0)","347a0135":"* Parametriza\u00e7\u00e3o abaixo escolhida a partir de 500 intera\u00e7\u00f5es, sendo a mais simples obtida nos 5 primeiros resultados\n* -0.848119159778934 roc_auc de teste na cross-validation - a mesma m\u00e9trica avaliada no treino fica ao redor de 1.0, fornecendo diagnostico de muita vari\u00e2ncia","dce804a5":"# Essas informa\u00e7\u00f5es indicam que a possibilidade de estabelecer um intervalo de valores a partir do trust score medido para cada amostra, na qual se concentram os erros de previs\u00e3o realizados\n\n\n* Como tentativa da verifica\u00e7\u00e3o sobre a possivel estabilidade temporal nessa hipotese, realizamos mais um experimento; em que o modelo foi retreinado apenas nos time_steps 1 at\u00e9 23 e mesma analise foi realizada com a separa\u00e7\u00e3o para teste dos steps de 28 at\u00e9 34\n* A confirma\u00e7\u00e3o segue abaixo, em que tambem nesses meses, mesmo com modelo sendo treinado em menor quantidade de dados e tendo menor performance, \u00e9 indicada a existencia de estabilidade temporal nessa informa\u00e7\u00e3o, dado que os resultados se mantem ao redor de um desvio padr\u00e3o de distancia dos resultados obtidos anteriormente, assim como tendo um desvio padr\u00e3o consideravelmente baixo frente aos desvios padr\u00f5es obtidos pelas transa\u00e7\u00f5es acertadas","c7eada4f":"# Abaixo \u00e9 o resultado final - AUC se mantendo estavelmente ao redor de 0.922 ao ser testado com v\u00e1rias seeds","72dc50d1":"# 3) Lentes para proje\u00e7\u00e3o de forma a explicitar erros sistem\u00e1ticos do modelo na visualiza\u00e7\u00e3o\n\n* Abaixo segue o terceiro passo planejado; a visualiza\u00e7\u00e3o dos dados a partir de lentes que explicitem as zonas de erro e agrupem dados classificados erroneamente de maneira semelhante\n* Para isso, utilizamos quatro fun\u00e7\u00f5es combinadas para gerar uma lente; a probabilidade produzida para a verdadeira classe de cada transa\u00e7\u00e3o, a probabilidade produzida para a classe prevista da transa\u00e7\u00e3o, a classe real das transa\u00e7\u00f5es (para separar claramente o comportamento nas duas classes) e o primeiro componente de um PCA (para obten\u00e7\u00e3o de formas mais detalhadas)\n","d2fddcb3":"import csv\nfrom hyperopt import STATUS_OK\nfrom timeit import default_timer as timer\nfrom hyperopt import Trials\n\n\n# Keep track of results\nbayes_trials = Trials()\n\nout_file = '\/kaggle\/working\/hyperopt_trials_random_forest.csv'\nof_connection = open(out_file, 'w')\nwriter = csv.writer(of_connection)\n\n# Write the headers to the file\nwriter.writerow(['loss', 'params', 'iteration', 'estimators', 'train_time'])\nof_connection.close()\n\n\n\n\n####################################################################################################################\n\n\n\ndef objective(params):\n    \"\"\"Objective function for Hyperparameter Optimization\"\"\"\n    \n    # Keep track of evals\n    global ITERATION\n    \n    ITERATION += 1\n\n    \n    start = timer()\n    \n    # Calls the estimator\n    \n    for parameter_name in ['max_depth']:\n        params['hyperparams'][parameter_name] = int(params['hyperparams'][parameter_name])\n    \n\n    \n    wrapped_df = wrapper(X_train, params['feature_activation'])\n        \n        \n    # Declare classifier\n    \n    classifier = RandomForestClassifier(**params['hyperparams'])\n    \n    # Run the fit\n    \n    average_auc = []\n    for i in range(0,4):\n        cross_validation_train = (wrapped_df.join(y_train)).query(str(i) + ' < time_step <= ' + str(i+20))\n        cross_validation_test = (wrapped_df.join(y_train)).query('time_step >= ' + str(i + 24))\n        classifier.fit(cross_validation_train.drop(['target'], axis = 1), cross_validation_train['target'])\n    \n        y_score = classifier.predict_proba(cross_validation_test.drop(['target'], axis = 1))\n        y_score = pd.DataFrame( y_score )[1]\n        \n        average_auc.append( roc_auc_score(cross_validation_test['target'], y_score) )\n    \n    \n    # loss function\n    loss = - np.array(average_auc).mean()\n    \n    \n    \n  ##############################################################################################################   \n    \n    \n    print(ITERATION)\n    \n    #registers the time took\n    run_time = timer() - start\n    \n    # Write to the csv file ('a' means append)\n    of_connection = open(out_file, 'a')\n    writer = csv.writer(of_connection)\n    writer.writerow([loss, params, ITERATION, run_time])\n    \n    # Dictionary with information for evaluation\n    return {'loss': loss, 'params': params, 'iteration': ITERATION,\n            'train_time': run_time, 'status': STATUS_OK}\n\n\n\n\n\n###################################################################################################################\nfrom hyperopt import hp\nfrom hyperopt.pyll.stochastic import sample\n\n# Define the search space\nspace = {\n    \n    'feature_activation' : {feature : hp.choice(feature, [0, 1]) for feature in X_train.columns},\n    \n    'hyperparams' :  {\n                    'n_estimators' : 75,\n                    'max_samples' : hp.uniform('max_samples', 0.1, 1),\n                    'max_features' : hp.uniform('max_features', 0.05, 1),\n                    'max_depth' : hp.quniform('max_depth', 9, 35, 1),\n                    'n_jobs' : -1\n                     }\n        }\n    \nparams = sample(space)\nparams\n\n\n\n##################################################################################################################\n\n\n\n\nfrom hyperopt import tpe\n# optimization algorithm\ntpe_algorithm = tpe.suggest\n\n\n\n\n###################################################################################################################\n\n\n\nfrom hyperopt import fmin\n\n\nfrom hyperopt import Trials\n# Keep track of results\nbayes_trials = Trials()\n","ad2e673a":"* A checagem abaixo sobre intervalos gerados pela m\u00e9dia e desvio padr\u00e3o do Trust Score revela uma informa\u00e7\u00e3o interessante\n* A partir dessas duas informa\u00e7\u00f5es, \u00e9 possivel gerar um intervalo com propriedade interessante pelos dados com Trust Score dentro dele -  em um intervalo que contem 25% (4000 observa\u00e7\u00f5es) de todo dataset de teste, est\u00e3o concentradas aproximamente de 50% de todos erros de previs\u00e3o cometidos pelo modelo; enquanto os outros 75% (12000 observa\u00e7\u00f5es) representam aproximadamente os 50% restantes - informa\u00e7\u00e3o interessante","4439e052":"# M\u00e9dia e desvio padr\u00e3o do trust score das classifica\u00e7\u00f5es certas - test set"}}