{"cell_type":{"9e54633b":"code","817f3ab9":"code","ad098def":"code","46fe49cd":"code","fd34f7e8":"code","2a103b7b":"code","e0ec76da":"code","4c36ede7":"code","46760d92":"code","3be87e30":"code","06d4258a":"markdown","27b3471e":"markdown","ad0e1faa":"markdown","54f48732":"markdown","64b2a427":"markdown","a9023b1a":"markdown","72dc459f":"markdown","242e3050":"markdown","c5f49d5d":"markdown","c922f176":"markdown"},"source":{"9e54633b":"import matplotlib.pyplot as plt\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.linear_model import LinearRegression","817f3ab9":"df = pd.read_csv(\"\/kaggle\/input\/one-year-of-r-india\/one-year-of-r-india-comments.csv\").dropna(subset=[\"body\"])\ndf.head()","ad098def":"corpus = df[\"body\"]\ncv = CountVectorizer(min_df=150, max_df=0.9)\nX = cv.fit_transform(corpus)","46fe49cd":"l = LinearRegression().fit(X, df[\"score\"])\nl.coef_","fd34f7e8":"cv.get_feature_names()[10::1000]","2a103b7b":"coef_df = pd.DataFrame({\"coef\": l.coef_})\ncoef_df.reset_index(inplace=True)\n\ntokens = cv.get_feature_names()\n\ncoef_df[\"token\"] = coef_df.apply(lambda x: tokens[int(x[\"index\"])], axis=1)\ncoef_df = coef_df.drop([\"index\"], axis=1)\ncoef_df","e0ec76da":"coef_df = coef_df.sort_values(by=\"coef\")","4c36ede7":"plt.rcParams['figure.figsize'] = [20, 15]  # Makes charts bigger","46760d92":"sns.barplot(x=\"token\", y=\"coef\", data=coef_df.iloc[:16])","3be87e30":"sns.barplot(x=\"token\", y=\"coef\", data=coef_df.iloc[-16:])","06d4258a":"Token on the right, and how much it contributes to the score on the left. Nice!\n\nLet's do some visualization, while we are at it.\n\n### Sort our tokens","27b3471e":"The big list of words right here, of course!\n\n### Let's put them together.","ad0e1faa":"### Draw the top 16","54f48732":"### Draw the bottom 16","64b2a427":"We use `min_df` (a word must be found at least 150 times) and `max_df` (only the bottom 90% of the words count) to remove some junk data already.\n\n## Let's fit a linear model to word counts and upvotes\n\nThey are very simple, but easy to explain. See below:","a9023b1a":"That's the coefficients we get.\n\nFor what, you ask?","72dc459f":"### Read our dataset","242e3050":"And there you have it! That's (very roughly) what people of \/r\/India like to talk about.\n\n### Things to do, if you want to take a deeper look\n\n - A lot of these words are numbers, link parts, or other bad data. Can you figure out a better scheme to clean the comments and split them into words?\n - You can do a similar analysis with post text. Will the distribution be any different?\n - There are better ways to predict upvotes. Look into Language Modeling - a whole world awaits.\n \nGood luck!","c5f49d5d":"# Many comments are posted in \/r\/India.\n\nWe set out with a question - which comments have the highest chance of being upvoted?\n\nLet's try looking at the words the comment has.\n\nBut first, some pre-processing.\n\n### Import our libraries","c922f176":"Looks good.\n\n### Now, create a bag-of-words model from the comment bodies.\n\nIt just counts which words were used - nothing more."}}