{"cell_type":{"796fc15b":"code","0edcf38f":"code","ecadd095":"code","fabb20f2":"code","52457808":"code","9b679c1a":"code","dd67abc6":"code","14ef395d":"code","82e55287":"markdown","3b5fc4c3":"markdown","12e7d8c9":"markdown"},"source":{"796fc15b":"import numpy as np\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\nfrom tensorflow.contrib.layers import fully_connected\nfrom tensorflow.contrib.layers import batch_norm\nimport pandas as pd","0edcf38f":"df_train = pd.read_csv(\"..\/input\/train.csv\").iloc[:]\ny_train = df_train.label\nX_train = df_train.drop([\"label\"],axis=1)\ndf_test = pd.read_csv(\"..\/input\/test.csv\").iloc[:]\nclass fetch_mnist:\n    def __init__(self,X=X_train,y=y_train,Xt=df_test):\n        self.X_train = X\/255\n        self.y_train = y\n        self.X_test = Xt\/255\n    def fetch_train_batch(self,n,cnn=False):\n        rand_indx = np.random.permutation(len(self.X_train))\n        self.X_train = self.X_train.iloc[rand_indx]\n        self.y_train = self.y_train.iloc[rand_indx]\n        if cnn==False:\n            return(self.X_train[:n],self.y_train[:n])\n        if cnn==True:\n            shape = np.r_[n,28,28,1]\n            data=self.X_train[:n]\n            X_out = np.zeros(shape)\n            X_out[:,:,:,0] = [np.array(data.iloc[i]).reshape(28,28) for i in range(data.shape[0])]\n            return(X_out,self.y_train[:n])\n    def fetch_test_batch(self,n,cnn=False):\n        if cnn==False:\n            return(self.X_test[:n])\n        if cnn==True:\n            shape = np.r_[n,28,28,1]\n            data=self.X_train[:n]\n            X_out = np.zeros(shape)\n            X_out[:,:,:,0] = [np.array(data.iloc[i]).reshape(28,28) for i in range(data.shape[0])]\n            return(X_out,self.y_train[:n])\n        return(self.X_test[:n],self.y_test[:n])","ecadd095":"def update_weights(n_layers,parameters):\n    a=[variable.name for variable in tf.trainable_variables()]\n    a=[a[i][:len(a[i])-2] for i in range(len(a))]\n    for i in range(n_layers*2):\n        ind = a[i].find('\/')\n        with tf.variable_scope(a[i][:ind],reuse=True):\n            var = tf.get_variable(a[i][ind+1:])\n        forge = tf.assign(var,best_weights)\n    return forge.eval(feed_dict={best_weights:parameters[i]})","fabb20f2":"#Definition of the constants\nn_feature = 28*28\nearly_stop_step=6\nn_output = 20\nhd_nuer = [300,100,n_output]\nlr=0.01\nn_epoch=10\nbatch_size=8000\ntrain_batch_size=1000\nmnist = fetch_mnist()\naccuracy_test_1=np.empty((1,1))","52457808":"#Tensorflow graph construction\ntf.reset_default_graph()\nX = tf.placeholder(dtype=tf.float32,shape=(None,n_feature),name=\"X\")\nY = tf.placeholder(dtype=tf.int32,shape=(None),name=\"Y\")\nbest_weights = tf.placeholder(dtype=tf.float32)\nis_training=tf.placeholder(tf.bool,shape=(),name=\"is_training\")\nbn_params={'is_training':is_training,'decay':0.75,'updates_collections':None}\n# hid_init = tf.initializers.random_normal(stddev=0.5)\nhidden = fully_connected(X,hd_nuer[0],normalizer_fn=batch_norm,normalizer_params=bn_params)\nfor i in range(1,len(hd_nuer)-1):\n    hidden = fully_connected(hidden,hd_nuer[i],normalizer_fn=batch_norm,normalizer_params=bn_params)\nlogits = fully_connected(hidden,hd_nuer[len(hd_nuer)-1],activation_fn=None,normalizer_fn=batch_norm,normalizer_params=bn_params)\nxentry=tf.nn.sparse_softmax_cross_entropy_with_logits(labels=Y,logits=logits)\nloss = tf.reduce_sum(xentry,axis=0)\noptimizer = tf.train.AdamOptimizer(learning_rate=lr)\ntraining_op = optimizer.minimize(loss)\ncorrect=tf.nn.in_top_k(logits,Y,1)\nal_in = tf.reduce_mean(tf.cast(correct,tf.float32))\ninit = tf.global_variables_initializer()\nsaver = tf.train.Saver()\nparams = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES,scope=\"fully_connected\")\nparams_1 = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES,scope=\"fully_connected\/weights\")","9b679c1a":"#Tensorflow graph execution\nwith tf.Session() as sess:\n    sess.run(init)\n    # # logits_val = X.eval(feed_dict={X:x[1:3,:],Y:y[1:3]})\n    # X_batch, y_batch = mnist.train.next_batch(batch_size)\n    # print(X_batch)\n    best_val=0\n    step=0\n    ep = 0\n    for ep in range(n_epoch):\n        for iteration in range(70):\n            X_batch, y_batch = mnist.fetch_train_batch(batch_size)\n            sess.run(training_op, feed_dict={is_training:True,X: X_batch, Y: y_batch})\n            acc_train = al_in.eval(feed_dict={is_training:False,X: X_batch,Y: y_batch})\n            print(ep, \"Train accuracy:\", acc_train)\n            parama = sess.run(params_1)\n            if step>early_stop_step:break\n            if (acc_train >= best_val):\n                best_val=acc_train\n                parameters = sess.run(params)\n                step=0\n            else:\n                step+=1\n        if step>early_stop_step:break\n        ep+=1\n    update_weights(len(hd_nuer),parameters)\n    save_path=saver.save(sess,\"..\/working\/mnist_class.ckpt\")\n    sess.close()\n","dd67abc6":"#Testing the damn! code\nwith tf.Session() as sess:\n    saver.restore(sess,\"..\/working\/mnist_class.ckpt\")\n    X_test= mnist.fetch_test_batch(28000)\n    acc_test = logits.eval(feed_dict={is_training:False,X:X_test})\n    out_put=np.argmax(acc_test,axis=1)\n    sess.close()","14ef395d":"data = pd.DataFrame({'ImageId':range(1,len(X_test)+1),'Label':out_put})\ndata.to_csv(\"..\/working\/output.csv\",index=False)\nprint(data.head())","82e55287":"Low level tensorflow classifier using only fully connected layers!!!","3b5fc4c3":"Class *fetch_mnist* for importing the data and using it likewise","12e7d8c9":"Function *update_weights* to store the final weights evolving out of early stop implementation"}}