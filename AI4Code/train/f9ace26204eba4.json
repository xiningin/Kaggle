{"cell_type":{"9c5b35af":"code","8bc99455":"code","3c7e12d8":"code","02dd4748":"code","b8410b18":"code","a58c5323":"code","beb5c73c":"code","4b963e7b":"code","6c439e38":"markdown","f06c9e9c":"markdown","0dc99ef7":"markdown","870bb566":"markdown","a38109a8":"markdown","255d87e8":"markdown","d5a288a6":"markdown","58ee1b88":"markdown"},"source":{"9c5b35af":"import numpy as np\nnp.random.seed(42)\nX = np.random.rand(100,1)\ny = 2 + X+np.random.randn(100,1)\/7.5","8bc99455":"import matplotlib.pyplot as plt\nplt.plot(X, y, 'ro')\nplt.show()","3c7e12d8":"def computeModelParameters(X,y):\n    X_b = np.c_[np.ones((100,1)), X] # concatenate a weight of 1 to each instance\n    optimal_theta = np.linalg.inv(X_b.T.dot(X_b)).dot(X_b.T).dot(y)\n    return optimal_theta\n\ntheta = computeModelParameters(X,y)\ntheta","02dd4748":"def predictY(x, theta): # predicts a single y value\n    return theta[0]+theta[1]*x\n\ndef predictAllY(X, theta): # predicts all y values of a matrix\n    X_b = np.c_[np.ones((len(X),1)), X] # concatenate 1's for theta_0 * x_0 (because x_0 doesn't exist in our data)\n    y_predict = X_b.dot(theta)\n    return y_predict\n    \ny_pred = predictAllY(X, theta)    ","b8410b18":"plt.plot(X, y, 'ro')\nplt.plot(X, y_pred, '-')\nplt.show()","a58c5323":"from sklearn.linear_model import LinearRegression\nlin_reg = LinearRegression()\nlin_reg.fit(X,y)\nprint('Theta 0:', lin_reg.intercept_)\nprint('Theta 1:', lin_reg.coef_[0])","beb5c73c":"def calculateMSE(X, y, theta):\n    sum = 0\n    m = len(X)\n    X_b = np.c_[np.ones((m,1)), X] # concatenate 1's for theta_0 * x_0 (because x_0 doesn't exist in our data)\n    for i in range(m):\n        # Create Prediction Value\n        pred = theta.T.dot(X_b[i])\n        # Find the Error\n        error = pred - y[i]\n        # Square the Error\n        sqrError = error**2\n        # Add the sqrError Up\n        sum += sqrError\n    return (1\/m)*sum[0]\n\ncalculateMSE(X,y,theta)","4b963e7b":"from sklearn.metrics import mean_squared_error\n\nmean_squared_error(y_pred=y_pred, y_true=y)","6c439e38":"Next we'll take a look at the Scikit-Learn MSE function.","f06c9e9c":"## Comparing to Scikit-Learn\nNow let's compare our model to the Linear Regression model from the Scikit-Learn. ","0dc99ef7":"## Conclusion\nBy creating our own Linear Regression model we have explored the basics of how a linear machine learning model works. We've computed the weights\/parameters and succesfully predicted our data. Then we looked at the Scikit-Learn version of the model which computed the same weights\/parameters as we did. After this we created our own Mean Square Error calculator function which also gave the same result as the Scikit-Learn version. This means, unless Scikit-Learn is wrong, that we've correctly used both formulas to implement our own Linear Regression model and MSE metric.\n\n### Next Kernel:\n[What Is Gradient Descent?](https:\/\/www.kaggle.com\/veleon\/what-is-gradient-descent)","870bb566":"Just as before, the function we've created gives (almost) the same result! ","a38109a8":"Its exactly the same!\n\n## Measuring Performance\nTo know how well our Linear Regression algorithm performs we'll need a performance measure. The Root Mean Square Error (RMSE) is commonly used for Regression Algorithms. So we'll need to find the values of $\\theta$ that minimize the RMSE. But instead of doing that we'll minimize the Mean Square Error (MSE) because this is simpler and still leads to the same result because a smaller mean also equals a smaller root mean.\n\nThe MSE equation:\n\n$MSE(X, h_\\theta)=\\frac{1}{m}\\displaystyle\\sum_{i=1}^{m}(\\theta^Tx^{(i)}-y^{(i)})^2$\n\n* $MSE(X, h_\\theta)$ = the cost function measured on the set of examples using the hyptothesis\n* $X$ = matrix combining all feature values (excluding labels) of all instances in the dataset\n* $h_\\theta$ = the systems prediction function, also called the hyptothesis\n* $m$ = the number of instances in the dataset\n* $\\displaystyle\\sum_{i=1}^{m}$ = sum of $(\\theta^Tx^{(i)}-y^{(i)})^2$ for $i=1$ to $i=m$\n* $\\theta^T$ = transposed vector of model parameters \n* $x$ = vector of feature values (excluding labels) of the $i^{th}$ instance\n* $y$ = label of the $i^{th}$ instance\n","255d87e8":"# Linear Regression\n[Index](https:\/\/www.kaggle.com\/veleon\/in-depth-look-at-machine-learning-models)\n\nLinear Regression creates a linear model that makes a prediction by computing a weighted sum of the input features, plus a constant (called _bias term_). These weights combined with the constant create a formula that outputs the predictions.\n\nThe equation:\n\n$ y = \\theta_0+\\theta_1x_1+\\theta_2x_2+\\dotsb+\\theta_nx_n$\n\n* $y$ = predicted value\n* $n$ = number of features\n* $x_i$ = the $i^{th}$ feature value\n* $\\theta_j$ = the $j^{th}$ model parameter\n\nThe equation using vectors:\n$ y = \\begin{bmatrix}\\theta_0\\\\\\theta_1\\\\\\vdots\\\\\\theta_n\\end{bmatrix}\\cdot\\begin{bmatrix}x_0\\\\x_1\\\\\\vdots\\\\x_n\\end{bmatrix}$\n\n\n## Creating a Dataset\nTo try our Linear Regression model we'll need data to use. Using the numpy library we'll generate some random points.","d5a288a6":"## Computing Model Parameters\/Weights \nNow lets compute $\\theta$ using the Normal Equation.\n\nThe Normal Equation:\n\n$ \\hat{\\theta} = (X^TX)^{-1}X^Ty $\n\n* $\\hat{\\theta}$ = the value of $\\theta$ that minimizes the cost function\n* $y$ = the vector of target values containing $y^1$ to $y^m$\n* $X$ = the matrix combining all feature values (excluding labels) of all instances in the dataset","58ee1b88":"So what did we expect? Our formula was $y = 2+x+noise$\n\nExpectation:\n* $\\theta_0 = 2$\n* $\\theta_1 = 1$\n\nOutput:\n* $\\theta_0 = 2.03$\n* $\\theta_1 = 0.94$\n\nI would say that's pretty close! The noise definitely made it harder to get our exact parameters but with some more data we should be able to get even closer to the original parameters.\n\n## Making Predictions\nNow that we have computed $\\hat{\\theta}$ we can try and predict our generated dataset. "}}