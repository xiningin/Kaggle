{"cell_type":{"8a2d1356":"code","1be7b456":"code","3258ba54":"code","ea0d2ce6":"code","820ca766":"code","d7ec520c":"code","a1a1e306":"code","fcfc53bf":"code","f2c6b278":"code","91f3011c":"code","55f7f766":"code","a891492a":"code","72610963":"code","4b6716e0":"code","a97e11dd":"code","81905cda":"code","08e125cd":"code","ea6826a5":"code","6c780c28":"code","ace3330b":"code","16f5ba85":"code","7fdf2530":"code","8db89361":"code","d38f1057":"code","4686a416":"code","57100d59":"code","fcde608d":"code","735f88ab":"code","0bf05e03":"code","68c41734":"code","7a8ddb8f":"code","826d44bf":"code","8b112ad2":"code","17db5196":"code","ca74e84b":"code","36988405":"code","c55130f8":"code","3beb21e2":"code","dae8d38a":"code","565f7735":"code","2e04c344":"code","e34a51fd":"code","f07929e7":"code","21a565ce":"code","92e31ef8":"code","ddf06e17":"code","79981744":"code","07ea93b7":"code","7b6dcf06":"code","62923acc":"code","8f5d588f":"markdown","b1535f88":"markdown","260fc97c":"markdown","a3422c1c":"markdown","ca31081f":"markdown","66c4621a":"markdown","4e658cd7":"markdown","60064a5a":"markdown","0d73afdc":"markdown","be1f245b":"markdown"},"source":{"8a2d1356":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","1be7b456":"data_test = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/test.csv',index_col='Id')\ndata_train = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/train.csv',index_col='Id')\ndata_test.tail()","3258ba54":"data_train.describe()","ea0d2ce6":"# Get  columns whose data type is object i.e. string\n###filteredColumns_Train = data_train.dtypes[data_train.dtypes == np.object]\n###filteredColumns_Test = data_test.dtypes[data_test.dtypes == np.object]\n# list of columns whose data type is object i.e. string\n#print(filteredColumns_Test)\n###all_Columns_Object= list(filteredColumns_Test.index) + list(filteredColumns_Train.index)\n###print(all_Columns_Object)\n#listOfColumnNames = list(all_Columns_Object.index)\n#print(listOfColumnNames)\n###data_train.drop(all_Columns_Object, axis=1,inplace=True)\n###data_test.drop(all_Columns_Object, axis=1,inplace=True)","820ca766":"cols_with_missing = [col for col in data_train.columns if data_train[col].isnull().any()]\n\n# Fill in the lines below: drop columns in training and validation data\nreduced_X_train = data_train.drop(cols_with_missing, axis=1,inplace=True)\nreduced_X_valid = data_test.drop(cols_with_missing, axis=1,inplace=True)","d7ec520c":"#print(cols_with_missing)","a1a1e306":"set(data_train) - set(data_test)","fcfc53bf":"SalePrice = data_train['SalePrice']","f2c6b278":"data_train.drop(columns='SalePrice',axis=1,inplace=True)\ndata_test.tail()","91f3011c":"from sklearn.impute import SimpleImputer\n\n# Imputation\nmy_imputer = SimpleImputer(strategy='most_frequent')\nimputed_X_train = pd.DataFrame(my_imputer.fit_transform(data_train))\nimputed_X_valid = pd.DataFrame(my_imputer.transform(data_test))\n\n# Imputation removed column names; put them back\nimputed_X_train.columns = data_train.columns\nimputed_X_valid.columns = data_test.columns\n\n","55f7f766":"data_train = imputed_X_train\ndata_test = imputed_X_valid","a891492a":"data_test.tail()","72610963":"s = (data_train.dtypes == 'object')\nobject_cols = list(s[s].index)\n\nprint(\"Categorical variables:\")\nprint(object_cols)","4b6716e0":"##unique_Columns = list()\n##for i in object_cols:\n ##   if len(data_train[i].unique()) > 20:\n           #print(i ,': ',len(data_train[i].unique()))\n   ##         unique_Columns.append(i)\n##data_train.drop(columns=unique_Columns,axis=1,inplace=True)\n##data_test.drop(columns=unique_Columns,axis=1,inplace=True)","a97e11dd":"s = (data_train.dtypes == 'object')\nobject_cols = list(s[s].index)\n\nprint(\"Categorical variables:\")\nprint(object_cols)","81905cda":"for i in object_cols:\n    if len(data_train[i].unique()) > 20:\n           print(i ,': ',len(data_train[i].unique()))","08e125cd":"set(data_train) - set(data_test)\ndata_test.tail()","ea6826a5":"from sklearn.preprocessing import LabelEncoder\n\n# Make copy to avoid changing original data \nlabel_X_train = data_train.copy()\nlabel_X_valid = data_test.copy()\n\n# Apply label encoder to each column with categorical data\nlabel_encoder = LabelEncoder()\nfor col in object_cols:\n    label_encoder.fit(pd.concat([data_train[col], data_test[col]], axis=0, sort=False))\n    label_X_train[col] = label_encoder.transform(data_train[col])\n    label_X_valid[col] = label_encoder.transform(data_test[col])\n","6c780c28":"data_train = label_X_train\ndata_test = label_X_valid\n#label_X_train\n#print(len(OH_X_train.columns))\n#print(len(data_train.columns))\ndata_test","ace3330b":"set(data_train) - set(data_test)","16f5ba85":"#counter = 0\ncolumns_have_missing_Train = []\nfor i in data_train.columns:\n    if data_train[i].isnull().sum() > 0:\n        #counter = counter  + 1\n        columns_have_missing_Train.append(i)\n        print(i,': ',data_train[i].isnull().sum())\n##################################\n#counter = 0\ncolumns_have_missing_Test = []\nfor i in data_test.columns:\n    if data_test[i].isnull().sum() > 0:\n        #counter = counter  + 1\n        columns_have_missing_Test.append(i)\n        print(i,': ',data_test[i].isnull().sum())","7fdf2530":"###setColumne = columns_have_missing_Test + columns_have_missing_Train\n###set(setColumne)","8db89361":"###for i in setColumne:\n   ### data_train[i].fillna(data_train[i].mean(), inplace=True)\n    ###data_test[i].fillna(data_test[i].mean(), inplace=True)   ","d38f1057":"###data_train","4686a416":"#SalePrice = data_train['SalePrice']\n#SalePrice","57100d59":"#data_train.drop(columns='SalePrice',axis=1,inplace=True)","fcde608d":"#from sklearn.impute import SimpleImputer\n\n# Imputation\n##my_imputer = SimpleImputer()\n##imputed_X_train = pd.DataFrame(my_imputer.fit_transform(data_train))\n##imputed_X_test = pd.DataFrame(my_imputer.transform(data_test))\n\n# Imputation removed column names; put them back\n##imputed_X_train.columns = data_train.columns\n##imputed_X_test.columns = data_test.columns\n#############################################\n##data_train = imputed_X_train\n##data_test = imputed_X_test","735f88ab":"##SalePrice.index -= 1\n##SalePrice\n##data_train['SalePrice'] = SalePrice\n","0bf05e03":"#data_train","68c41734":"###cols_with_missing_train = [col for col in data_train.columns\n   ###                  if data_train[col].isnull().any()]\n###cols_with_missing_test = [col for col in data_test.columns\n   ###                  if data_test[col].isnull().any()]\n#print(cols_with_missing_train)\n#print('----------------------')\n#print(cols_with_missing_test)\n#print(set(cols_with_missing_test) - set(cols_with_missing_train))\n\n###all_missing_columns = cols_with_missing_train + cols_with_missing_test\n###print(len(all_missing_columns))\n\n#Drop columns in training and validation data\n\n###data_train.drop(all_missing_columns, axis=1,inplace=True)\n###data_test.drop(all_missing_columns, axis=1,inplace=True)\n\n#set(data_test)-set(data_train)","7a8ddb8f":"# Get  columns whose data type is object i.e. string\n###filteredColumns = data_train.dtypes[data_train.dtypes == np.object]\n# list of columns whose data type is object i.e. string\n#print(filteredColumns.index)\n###listOfColumnNames = list(filteredColumns.index)\n###print(listOfColumnNames)\n###data_train.drop(listOfColumnNames, axis=1,inplace=True)\n###data_test.drop(listOfColumnNames, axis=1,inplace=True)","826d44bf":"###data_train","8b112ad2":"#for i in data_train.columns:    \n #   print(i ,': ',len(data_train[i].unique()))\n#len(data_train.Name.unique)","17db5196":"#data_train.drop(columns='SalePrice',axis=1,inplace=True)","ca74e84b":"#data_train.join(SalePrice = list(SalePrice))","36988405":"#SalePrice\ndata_train.insert(0, 'SalePrice',  list(SalePrice))","c55130f8":"data_train.SalePrice\n#data_train","3beb21e2":"y = data_train.SalePrice\n#############################\nX = data_train.drop(columns=['SalePrice'])","dae8d38a":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y)","565f7735":"from xgboost import XGBRegressor\nfrom sklearn.metrics import mean_absolute_error ,explained_variance_score, mean_squared_error","2e04c344":"parameters = {'learning_rate':  [0.02,0.05,0.07,0.09], #so called `eta` value\n              'max_depth':  list(range(6, 30, 10)),\n              'n_estimators': list(range(100, 1001, 100))}\n","e34a51fd":"from sklearn.model_selection import GridSearchCV\n\ngsearch = GridSearchCV(estimator=XGBRegressor(),\n                       param_grid = parameters, \n                       scoring='neg_mean_absolute_error',\n                       n_jobs=4,cv=5,verbose=7)\n\ngsearch.fit(X_train, y_train)\n\n","f07929e7":"print(gsearch.best_params_.get('n_estimators'))\nprint(gsearch.best_params_.get('learning_rate'))\nprint(gsearch.best_params_.get('max_depth'))\n#print(gsearch.best_params_.get('subsample'))","21a565ce":"parameters_final = {'learning_rate': gsearch.best_params_.get('learning_rate'), #so called `eta` value\n              'max_depth': gsearch.best_params_.get('max_depth'),\n              'n_estimators': gsearch.best_params_.get('n_estimators')}","92e31ef8":"my_model = XGBRegressor(learning_rate = gsearch.best_params_.get('learning_rate'),\n                         max_depth = gsearch.best_params_.get('max_depth'),\n              n_estimators = gsearch.best_params_.get('n_estimators'),random_state=1, n_jobs=4)\nmy_model.fit(X_train, y_train)\npredictions = my_model.predict(X_test)\nmean_Error = mean_absolute_error(y_true=y_test,y_pred = predictions)\nprint(mean_Error)","ddf06e17":"def getBestScore(n_est):\n    my_model = XGBRegressor(n_estimators=n_est,random_state=1,learning_rate=0.05, n_jobs=4)\n    my_model.fit(X_train, y_train)\n    predictions = my_model.predict(X_test)\n    mean_Error = mean_absolute_error(y_true=y_test,y_pred = predictions)\n    return mean_Error \n","79981744":"#explained_variance_score\n###range_Estimation = getBestScore(1)\n###minEstim = 1\n###for i in range(1,100,1):\n    #print(getBestScore(i),'*-*',i)\n   ### if range_Estimation > getBestScore(i):\n      ###  minEstim = i\n###print(range_Estimation,'>>>',minEstim)\n##### 196 is the best...'''","07ea93b7":"final_model = XGBRegressor(learning_rate = gsearch.best_params_.get('learning_rate'),\n                         max_depth = gsearch.best_params_.get('max_depth'),\n              n_estimators = gsearch.best_params_.get('n_estimators'),random_state=1, n_jobs=4)\nfinal_model.fit(X, y)\npredictions = final_model.predict(X)\n#print(predictions)\n#mean_absolute_error(y_true=y , y_pred = predictions)\n#print(predictions[:5])\n#print(y[:5])","7b6dcf06":"data_test","62923acc":"test_preds = final_model.predict(data_test)\n#output = pd.DataFrame({'Id': data_test.index,\n #                      'SalePrice': test_preds})\n#output.to_csv('submission.csv', index=False)\n#############################################3\nsamplesubmission = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/sample_submission.csv')\noutput = pd.DataFrame({'Id': samplesubmission.Id, 'SalePrice': test_preds})\noutput.to_csv('submission.csv', index=False)\nprint('Done')","8f5d588f":"# Read DataSet & Show data_train","b1535f88":"# Drop columns with missing values","260fc97c":"# Drop diffrent columns betwwen train data & test data","a3422c1c":"# Show describe()","ca31081f":"# SimpleImputer","66c4621a":"# Drop any columns have datatype = object","4e658cd7":"# Drop any column content datatype is String or object","60064a5a":"# View the sum of empty values in each column.","0d73afdc":"# Drop any column have missing value","be1f245b":"# The Best Values is :\n##700\n##0.02\n##6\n##0.7"}}