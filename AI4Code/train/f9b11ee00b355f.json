{"cell_type":{"2e6521ec":"code","c12a407d":"code","66f0cf0d":"code","16a726b1":"code","c06f7665":"code","f9560e75":"code","1202daf8":"code","ba638587":"code","746d9307":"code","60eb52b1":"code","f1d6810a":"code","d742ff6e":"code","06244012":"code","630056ef":"code","ef2d31e7":"code","15d3800b":"code","f4b82860":"code","67bb3284":"code","36b0dad2":"code","b1028fca":"code","f0ff9770":"code","49b4e3e1":"code","ca0a78a0":"code","886c23d2":"code","76f06a11":"code","ca1d80d3":"code","8f2b4e86":"code","4a407959":"code","e0bfa4b7":"code","bb28548d":"code","b844bc16":"code","99f6548b":"code","5576cd75":"code","25f631b9":"code","8531c820":"code","fd14159e":"code","22cf879b":"code","77595d40":"code","77d69cae":"code","d053a2d8":"markdown","2e72bebc":"markdown","7a61b4bf":"markdown","7b5a782c":"markdown","e4eed27b":"markdown","a8035598":"markdown","1bbea84f":"markdown","3c56a2db":"markdown","c2001970":"markdown","8827f0a6":"markdown","f8265669":"markdown","96691425":"markdown","302e225f":"markdown","dcf35fce":"markdown","f19b8cfb":"markdown","c37e89ab":"markdown","0fcd56d9":"markdown","f5341ec3":"markdown","92069682":"markdown","cfdca515":"markdown","fccf941e":"markdown","b2ac47c6":"markdown","c412324d":"markdown","f7dfd9a8":"markdown","44519018":"markdown","4a7118f4":"markdown","1ff5e0e6":"markdown","7e22b01a":"markdown","3bb14885":"markdown","f346d46c":"markdown","96cbd6fb":"markdown","c3da0478":"markdown","2fcbb06e":"markdown","78c2ef1d":"markdown","c088a830":"markdown","99e2aa2f":"markdown","d758c8f1":"markdown","9aa0c7e7":"markdown","e5815d85":"markdown","91ac53b7":"markdown","1f1a2716":"markdown","9de75160":"markdown","ca839239":"markdown","47545592":"markdown","b45b2ae0":"markdown","c771ac93":"markdown","fd079d87":"markdown","b3a096ab":"markdown","2b06ddc8":"markdown","091124bf":"markdown","de730612":"markdown","142c420d":"markdown","03e4b0f6":"markdown","9ca8ef32":"markdown","d59540ee":"markdown","deb8da11":"markdown","629e0821":"markdown","bc48c5e5":"markdown","0796fbca":"markdown","f91c609f":"markdown","93e73ef6":"markdown","aa590f25":"markdown"},"source":{"2e6521ec":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport keras\nimport plotly.express as px\nimport plotly.graph_objects as go\nimport plotly.figure_factory as ff\n\nfrom sklearn.model_selection import train_test_split \nfrom sklearn.preprocessing import MinMaxScaler \nfrom sklearn.metrics import confusion_matrix, accuracy_score, recall_score, precision_score, f1_score, roc_curve, auc\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\nfrom keras.models import Sequential # Initializing ANN\nfrom keras.layers import Dense, Dropout # Layers and dropout to avoid overfitting\nsns.set()\n\n%matplotlib inline","c12a407d":"dataset = pd.read_csv('..\/input\/churn-modelling\/Churn_Modelling.csv')\ndataset.head()","66f0cf0d":"dataset.describe().T","16a726b1":"print(pd.isnull(dataset).sum()) # Is there any missing value?","c06f7665":"dataset2 = dataset.copy() # Copying before dropping columns\ndataset2 = dataset2.drop(['Surname','RowNumber','CustomerId'], axis = 1) ","f9560e75":"Genre = pd.DataFrame(dataset2['Gender'].value_counts()).reset_index()\nGenre.columns = ['Gender','Total']\nfig = px.pie(Genre, values = 'Total', names = 'Gender', title='Gender percentage', hole=.4, color = 'Gender',width=800, height=400)\nfig.show()","1202daf8":"fig = px.bar(Genre, x = 'Gender', y='Total', title='Amount per genre', color='Gender',width=600, height=500)\nfig.show()","ba638587":"Male = dataset2[dataset2[\"Gender\"] == 'Male'][['Gender','Age','EstimatedSalary']]\ntemp = pd.DataFrame(Male['Age'].value_counts().reset_index())\ntemp.columns = ['Age','Total']\n\nFemale = dataset2[dataset2[\"Gender\"] == 'Female'][['Gender','Age','EstimatedSalary']]\ntemp2 = pd.DataFrame(Female['Age'].value_counts().reset_index())\ntemp2.columns = ['Age','Total']","746d9307":"hist_data = [Male['Age'],Female['Age']]\ngroup_labels = ['Male Age','Female Age']\n\nfig = ff.create_distplot(hist_data, group_labels, bin_size=[5,5], colors = ['Blue', 'Red'])\nfig.update_layout(title_text='Male and Female age distribution')\nfig.show()","60eb52b1":"fig = go.Figure()\nfig.add_trace(go.Bar(\n    x = temp['Age'],\n    y = temp['Total'],\n    name='Male',\n    marker_color='rgba(94, 144, 175, 0.8)'\n))\nfig.add_trace(go.Bar(\n    x = temp2['Age'],\n    y = temp2['Total'],\n    name='Female',\n    marker_color='rgba(249, 70, 10, 0.9)'\n))\n\n# Here we modify the tickangle of the xaxis, resulting in rotated labels.\nfig.update_layout(title = 'Age per genre', barmode = 'group', xaxis_tickangle=-45)\nfig.show()","f1d6810a":"Exited = dataset2.query('Exited == 1')\n\nMale = Exited[Exited[\"Gender\"] == 'Male'][['Gender','Age','EstimatedSalary']]\ntemp = pd.DataFrame(Male['Age'].value_counts().reset_index())\ntemp.columns = ['Age','Total']\n\nFemale = Exited[Exited[\"Gender\"] == 'Female'][['Gender','Age','EstimatedSalary']]\ntemp2 = pd.DataFrame(Female['Age'].value_counts().reset_index())\ntemp2.columns = ['Age','Total']","d742ff6e":"fig = go.Figure()\nfig.add_trace(go.Bar(\n    x = temp['Age'],\n    y = temp['Total'],\n    name='Male',\n    marker_color='rgba(94, 144, 175, 0.8)'\n))\nfig.add_trace(go.Bar(\n    x = temp2['Age'],\n    y = temp2['Total'],\n    name='Female',\n    marker_color='rgba(249, 70, 10, 0.9)'\n))\n\n# Here we modify the tickangle of the xaxis, resulting in rotated labels.\nfig.update_layout(title = 'Exited members', barmode = 'group', xaxis_tickangle=-45)\nfig.show()","06244012":"Exited_value = pd.DataFrame(Exited['Gender'].value_counts()).reset_index()\nExited_value.columns = ['Gender','Total']\n\nfig = px.pie(Exited_value, values = 'Total', names = 'Gender', title='Exited gender percentage', hole=.4, color = 'Gender',width=800, height=400)\nfig.show()","630056ef":"Product = Exited['NumOfProducts'].value_counts()\nProduct = pd.DataFrame(Product).reset_index() # Creating DF\nProduct.columns = ['NumOfProducts','Total']","ef2d31e7":"fig = px.bar(Product, x = 'NumOfProducts', y = 'Total' ,\n             hover_data = ['NumOfProducts','Total'] , color='NumOfProducts', height=400, title = 'Exited members and prouts')\nfig.show()","15d3800b":"CreditCard = Exited['HasCrCard'].value_counts()\nCreditCard = pd.DataFrame(CreditCard).reset_index() # Creating DF\nCreditCard.columns = ['CreditCard','Total']\n\nfig = px.bar(CreditCard, x = 'CreditCard', y = 'Total' ,\n             hover_data = ['CreditCard','Total'] , color='CreditCard', height=400, title = 'Exited members and credit card')\nfig.show()","f4b82860":"Active = Exited['IsActiveMember'].value_counts()\nActive = pd.DataFrame(Active).reset_index() # Creating DF\nActive.columns = ['Active','Total']\n\nfig = px.bar(Active, x = 'Active', y = 'Total' ,\n             hover_data = ['Active','Total'] , color='Active', height=400, title = 'Active exited members')\nfig.show()","67bb3284":"Tenure = Exited['Tenure'].value_counts()\nTenure = pd.DataFrame(Tenure).reset_index() # Creating DF\nTenure.columns = ['Tenure','Total']\n\nfig = px.bar(Tenure, x = 'Tenure', y = 'Total' ,\n             hover_data = ['Tenure','Total'] , color='Tenure', height=400, title = 'Exited members and tenure')\nfig.show()","36b0dad2":"Non_Exited = dataset2.query('Exited == 0')\n\nhist_data = [Exited['CreditScore'], Non_Exited['CreditScore']]\ngroup_labels = [ 'Exited Credit Score','Non Exited Credit Score']\n\nfig = ff.create_distplot(hist_data, group_labels, bin_size=[10, 10], colors = ['#F66095', '#2BCDC1'])\nfig.update_layout(title_text='Credit score')\nfig.show()","b1028fca":"Countries = pd.DataFrame(dataset['Geography'].value_counts()).reset_index()\nCountries.columns = ['Country','Total clients']","f0ff9770":"fig = px.bar(Countries, x = 'Country', y = 'Total clients', color = 'Country', title='Countries and total clients')        \nfig.show()","49b4e3e1":"x = [\"CreditScore\", \"Age\", \"Tenure\",\"Balance\", \"NumOfProducts\", \"HasCrCard\",\"IsActiveMember\",\"EstimatedSalary\",\"Exited\"]\nheat = go.Heatmap(z =dataset2.corr(),\n                  x = x,\n                  y=x,\n                  xgap=1, ygap=1,\n                  colorbar_thickness=20,\n                  colorbar_ticklen=3,\n                  hovertext = dataset2.corr(),\n                  hoverinfo='text',colorscale=[[0.0, '#F5FFFA'], \n                         [0.2, '#ADD8E6'], \n                         [0.4, '#87CEEB'],\n                         [0.6, '#87CEFA'], \n                         [0.8, '#40E0D0'], \n                         [1.0, '#00CED1']]\n                   )\n\ntitle = 'Correlation Matrix'               \n\nlayout = go.Layout(title_text=title, title_x=0.5, \n                   width=600, height=600,\n                   xaxis_showgrid=False,\n                   yaxis_showgrid=False,\n                   yaxis_autorange='reversed')\n   \nfig=go.Figure(data=[heat], layout=layout)        \nfig.show() ","ca0a78a0":"data_dummie = pd.get_dummies(dataset2)","886c23d2":"MinMaxScaler = MinMaxScaler() \ndata_dummie_Scaled = MinMaxScaler.fit_transform(data_dummie)\ndata_dummie_Scaled = pd.DataFrame(data_dummie_Scaled, columns = data_dummie.columns)\ndata_dummie_Scaled.head(3)","76f06a11":"X = data_dummie_Scaled.drop(['Exited'], axis = 1).values\ny = data_dummie_Scaled['Exited'].values\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2)","ca1d80d3":"classifier = Sequential() # Initializing model","8f2b4e86":"classifier.add(Dense(units = 7, activation = 'relu', input_dim = X_train.shape[1], kernel_initializer = \"glorot_uniform\")) \nclassifier.add(Dropout(rate = 0.1))","4a407959":"classifier.add(Dense(units = 7, activation='relu', kernel_initializer=\"glorot_uniform\"))\nclassifier.add(Dropout(rate = 0.1))\nclassifier.add(Dense(units = 7, activation='relu', kernel_initializer=\"glorot_uniform\"))\nclassifier.add(Dropout(rate = 0.1))\nclassifier.add(Dense(units = 7, activation='relu', kernel_initializer=\"glorot_uniform\"))\nclassifier.add(Dropout(rate = 0.1))","e0bfa4b7":"classifier.add(Dense(units = 1, activation='tanh', kernel_initializer=\"uniform\"))","bb28548d":"classifier.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])","b844bc16":"history = classifier.fit(X_train, y_train, batch_size=100, epochs=150)","99f6548b":"y_pred = classifier.predict(X_test)\ny_pred = (y_pred > 0.5) # if y_pred is larger than 0.5 it returns true(1) else false(2)\ny_pred","5576cd75":"y_pred = y_pred.astype(int).reshape(X_test.shape[0])\ny_pred","25f631b9":"Prediction_y = pd.DataFrame({'Prediction_y': y_pred})\n\nY = pd.DataFrame({'Y test': y_test}) # Df with real values and predicted values\nComparation = Y.join(Prediction_y) \nComparation.sample(10)","8531c820":"cm = confusion_matrix(y_test, y_pred)\n\ngroup_names = ['True Neg','False Pos','False Neg','True Pos']\ngroup_counts = ['{0:0.0f}'.format(value) for value in\n                cm.flatten()]\ngroup_percentages = ['{0:.2%}'.format(value) for value in\n                     cm.flatten()\/np.sum(cm)]\n\nlabels = [f'{v1}\\n{v2}\\n{v3}' for v1, v2, v3 in\n          zip(group_names,group_counts,group_percentages)]\n\nlabels = np.asarray(labels).reshape(2,2)\n\nsns.heatmap(cm, annot = labels, fmt = '', cmap = 'Blues', cbar = False)\nplt.gcf().set_size_inches(5, 5)\nplt.title('Confusion Matrix NN', fontsize = 20)\nplt.show()","fd14159e":"print('Accuracy Score : ' + str(accuracy_score(y_test,y_pred)))\nprint('Precision Score : ' + str(precision_score(y_test,y_pred)))\nprint('Recall Score : ' + str(recall_score(y_test,y_pred)))\nprint('F1 Score : ' + str(f1_score(y_test,y_pred)))","22cf879b":"# length of the test data \ntotal = len(y_test) \n  \n# Counting '1' labels in test data \none_count = np.sum(y_test) \n  \n# counting '0' lables in test data  \nzero_count = total - one_count \n\nplt.figure(figsize = (10, 6)) \n  \n# x-axis ranges from 0 to total people on y_test  \n# y-axis ranges from 0 to the total positive outcomes. \n\n# NN classifier \n\nNN = [y for _, y in sorted(zip(y_pred, y_test), reverse = True)] \n\nx = np.arange(0, total + 1) # Shape of Y_test\ny = np.append([0], np.cumsum(NN)) # Y values\n\nplt.plot(x, y, c = 'red', label = 'NN', linewidth = 2)\n\n\n# Random Model plot\n  \nplt.plot([0, total], [0, one_count], c = 'blue',  \n         linestyle = '--', label = 'Random Model') \n\n# Perfect model plot\n\nplt.plot([0, one_count, total], [0, one_count, one_count], \n         c = 'grey', linewidth = 2, label = 'Perfect Model') \n\nplt.title('Cumulative Accuracy Profile of NN', fontsize = 20)\nplt.xlabel('Total y_test observations', fontsize = 15)\nplt.ylabel('N\u00b0 class 1 scores', fontsize = 15)\nplt.legend() \nplt.show()","77595d40":"# Area under Random Model\na = auc([0, total], [0, one_count])\n\n# Area between Perfect and Random Model\naP = auc([0, one_count, total], [0, one_count, one_count]) - a\n\n# Area NN\n\naNN = auc(x, y) - a\nprint(\"Accuracy Rate for NN: {}\".format(aNN \/ aP))","77d69cae":"fpr, tpr, thresholds = roc_curve(y_test, y_pred)\n\nroc_auc = auc(fpr, tpr)\n\nplt.figure(figsize = (10, 6)) \n\nplt.plot(fpr, tpr, c = 'red', linewidth = 2, label = 'NN AUC:' + ' {0:.2f}'.format(roc_auc))\nplt.plot([0,1], [0,1], c = 'blue', linestyle = '--')\n\nplt.xlabel('False Positive Rate', fontsize = 15)\nplt.ylabel('True Positive Rate', fontsize = 15)\nplt.title('ROC', fontsize = 20)\nplt.legend(loc = 'lower right', fontsize = 13)\nplt.show()","d053a2d8":"There are no missing values in the set.","2e72bebc":"## Loss function","7a61b4bf":"'IsActiveMember' and 'Age' are the best predictors for an exited member as well as the categorical features.","7b5a782c":"As the NN has almost the same AUC as the perfect model, we have achieved a really accurate one.","e4eed27b":"## ANN modeling","a8035598":"## Sigmoid and Tanh","1bbea84f":"## Dropout","3c56a2db":"Optimizers are algorithms or methods used to **change the attributes of your neural network such as weights and learning rate in order to reduce the losses.** Some of them are:\n\n**Gradient Descent** is the most basic but most used optimization algorithm. It\u2019s used heavily in linear regression and classification algorithms. Backpropagation in neural networks also uses a gradient descent algorithm. Some disadvantages of this optimization algorithm are:\n\n- Gradient Descent may trap at **local minima.**\n- Weights are changed after calculating gradient on the **whole dataset.** So, if the dataset is too large than this may take years to converge to the minima.\n- Requires large memory to calculate gradient on the whole dataset.\n\n**Mini-Batch Gradient Descent** it\u2019s best among all the variations of gradient descent algorithms. The dataset is divided into **various batches** and after every batch, the parameters are updated. It requires more computing power than gradient descent.\n\n**Momentum** accelerates the **convergence towards the relevant direction** and reduces the fluctuation to the irrelevant direction. It reduces the oscillations and high variance of the parameters and converges faster than gradient descent.\n\n**One more hyper-parameter** is added which needs to be selected manually and accurately.\n\n**Adam** is a newer and improved optimizer. Its is an adaptive method compared to the gradient descent which maintains a single learning rate for all weight updates and the learning rate does not change.\n\nAdam has the advantage over the GradientDescent of using the running average (momentum) of the gradients (mean) as well as the running average of the gradient squared. You can learn more in https:\/\/machinelearningmastery.com\/adam-optimization-algorithm-for-deep-learning\/","c2001970":"## Relu\n\nThe main advantage of using the ReLU function over other activation functions is that **it does not activate all the neurons at the same time.**\n\nThis means that the neurons will only **be deactivated if the output of the linear transformation is less than 0.**\n\nI'll use Relu activation function in every layer except in the output layer.\n\nComparing Relu to Sigmoid, one major benefit is the reduced likelihood of the gradient to vanish. This arises when x>0. In this regime the gradient has a constant value. In contrast, the gradient of sigmoids becomes increasingly small as the absolute value of x increases. The constant gradient of ReLUs results in faster learning.\n\n**Relu** is preferable for hidden layers (or Leaky Relu for instance).","8827f0a6":"## CAP curve","f8265669":"**Gender and age**","96691425":"Exited members credit score has a normal distribution around a lower value than non exited ones. Non Exited members have higher credit scores.","302e225f":"## Countries","dcf35fce":"## Output layer","f19b8cfb":"Most exited members are from Germany and France! Followed by Spain.","c37e89ab":"![image.png](attachment:image.png)","0fcd56d9":"Gender and geography are important features for predicting an exited member, so I'll not encode them yet.\n\nLet's see the correlation between numerical features.","f5341ec3":"Sigmoid activation function translates the input ranged in (-inf,+inf) to the range in (0; 1), while tanh does it to the range in (-1;1). \n\nThe derivatives of the tanh are larger than the derivatives of the sigmoid. In other words, you minimize your cost function faster if you use tanh as an activation fuction. I'll use tanh function for the output layer.","92069682":"For the number of units (input nodes) , I'll select the number of final outputs (1), add the number of input_dim (number of features to enter the network, in this case 13) and half that number.\n\nIn this case I'll use 7 units. This is an iterative process, and trial and error is the best way to optimize the number of nodes and layers.","cfdca515":"![image.png](attachment:image.png)","fccf941e":"**Conclusions:**\n- There are more men than women in the bank dataset, and both they ages are normally distributed around 36 years.\n- As expected, most of the bank products are directed for people mostly from 25 to 45 years.\n- 56% of exited members are women, men are more loyal to the bank.\n- Most exited members had only 1 product in the bank and having credit card didn't affect them in their decission.\n- Most exited members were unnactive.\n- The number of tenures is not decisive for leaving the bank.","b2ac47c6":"The objective of this analysis is to see whether a customer will stop having an account in a bank, using NN in the process.","c412324d":"Receiver Operating Characteristic curve is another way to evaluate the accuracy of classification models and also to compare between them.\n\nIt plots the True Positive Rate in the Y-axis and False Positive Rate in the X-axis. It is a way to summarize information that could be obtained from many confusion matrices.\n\nAs the AUC gets bigger, the model is better at classifying.","f7dfd9a8":"## Feature Engineering","44519018":"## Training the model","4a7118f4":"## Confusion matrix","1ff5e0e6":"The aim of **weight initialization** is to **prevent layer activation outputs from exploding or vanishing during the course of a forward pass through a deep neural network.** If either occurs, loss gradients will either be too large or too small to flow backwards beneficially, and the network will take longer to converge, if it is even able to do so at all.\n\nThere are many initializers that could be used in Keras: Glorot uniform (default), uniform and glorot normal among others. Also this initializer is a trial and error procedure.\n\nBias will be initialized in 0 (default)","7e22b01a":"## Correlation Matrix","3bb14885":"Now that we have explained all the layers, optimizers and function it's time to train the NN.","f346d46c":"**Artificial neural network**, which has input layer, output layer, and **two or more trainable weight layers** (constisting of Perceptrons) is called multilayer perceptron or **MLP.**\n\nInformation flows through a neural network in two ways. When it's learning (being trained) or operating normally (after being trained), patterns of information are fed into the network via the input units, which trigger the layers of hidden units, and these in turn arrive at the output units. This common design is called a **feedforward network.**\n\n**Backpropagation**\n\nBy a feedback process called backpropagation, the network compares the predicted output with the real output value, and it uses this difference to learn and update it's weights.","96cbd6fb":"**Checking missing values**","c3da0478":"## Metrics","2fcbb06e":"An **artificial neural network (ANN)** is the piece of a computing system designed to simulate the way the human brain analyzes and processes information. It is the foundation of artificial intelligence (AI) and solves problems that would prove impossible or difficult by human or statistical standards. ANNs have self-learning capabilities that enable them to produce better results as more data becomes available.\n\n**Perceptron is a single layer neural network**, it consists of a **layer of inputs(corresponds to columns of a dataframe)**. Each input has a weight which controls the magnitude of an input. The summation of the products of these input values and weights is fed to the activation function. Activation functions are really important for a Artificial Neural Network to learn and make sense of something really complicated and Non-linear complex functional mappings between the inputs and response variable.","78c2ef1d":"## Credit score","c088a830":"## Batch size and Epochs","99e2aa2f":"This function will essentially calculate **how poorly our model is performing by comparing what the model is predicting with the actual value it is supposed to output.** If Y_pred is very **far** off from Y, the Loss value will be **very high.** However if both values are almost **similar**, the Loss value will be **very low.**\n\nThis function will be different whether the problem is a regression or a classification for example. In this case, as we are trying to predict a class we'll be using binary cross entropy.","d758c8f1":"It\u2019s a regularizer technique that **reduces the odds of overfitting by dropping out neurons at random, during every epoch** (or, when using a minibatch approach, during every minibatch). I'll use this in every layer.","9aa0c7e7":"## Input Layer","e5815d85":"## ROC curve","91ac53b7":"## Optimizers","1f1a2716":"The ROC curve and AUC is not the best we could have, we should improve the recall to achieve a better model.","9de75160":"## AUC (Area Under the Curve)","ca839239":"## Adding layers","47545592":"## Initializing NN sequential model","b45b2ae0":"This NN will have an input layer, 3 hidden layers and an output layer. As the number of nodes in each layer, the number of layers is another hyperparameter you should iterate over and check accuracy.","c771ac93":"## Hidden layers","fd079d87":"- The **batch size** is a hyperparameter of gradient descent that controls the **number of training samples to work through before the model\u2019s internal parameters are updated.**\n\n- The number of **epochs** is a hyperparameter of gradient descent that controls the **number of complete passes through the training dataset.**","b3a096ab":"![image.png](attachment:image.png)","2b06ddc8":"![image.png](attachment:image.png)","091124bf":"## NN modeling","de730612":"## EDA and visualizations","142c420d":"**The set has:**\n- Info of 1000 different customers.\n- Three categorical columns: 'Surname', 'Geography' and 'Gender'. Surname will be dropped as it doesn't give any additional information.\n- 11 numerical columns, two of them will be dropped, as 'Row Number' and 'Customer Id' don't give any useful info.","03e4b0f6":"## Exited members","9ca8ef32":"## Initializer","d59540ee":"**Splitting dataset**","deb8da11":"## Compiling NN","629e0821":"Cumulative Accuracy Profile curve is a tool that will help evaluate the performance and accuracy of the classification model.  I'll compare how CAP curves for NN algorithm relate to a random model and to an ideal model.\n\nAs the CAP curve gets more and more similar to the ideal model, the accuracy of the algorithm improves.","bc48c5e5":"As I'll create a simple NN, first the dataset will be **scaled.** The accuracy of ANN improves considerably when scaling the data accordingly.","0796fbca":"![image.png](attachment:image.png)","f91c609f":"![image.png](attachment:image.png)","93e73ef6":"**OHE for categorical columns**","aa590f25":"**Activation function**"}}