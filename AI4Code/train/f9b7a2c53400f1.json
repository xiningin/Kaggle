{"cell_type":{"e1ebc8ab":"code","e1ab7c3a":"code","698d6ae7":"code","adfe6cc6":"code","c1f238d3":"code","8924c96c":"code","784561db":"code","ff49e570":"code","ddd18cd1":"code","d186f638":"code","43b1c6eb":"code","dacf9c4f":"code","c211af0e":"code","c6a03d49":"code","a9f0a003":"code","2078388d":"code","3f57af53":"code","c8b194aa":"code","6ed46d3e":"code","ac083da4":"code","d78c31bf":"code","3ed4c2d1":"code","76eaf328":"code","83cd40d4":"code","525b41de":"code","2f6c91a8":"code","861f7940":"code","583bd806":"code","fd08c703":"code","f7421017":"code","9901f20d":"code","c0a809cd":"code","81b3c199":"code","0639cd00":"code","f55b08e3":"code","11866e5d":"code","fa7a4445":"code","a8651aa5":"code","56c59067":"markdown","c6a87491":"markdown","59c39e16":"markdown","fc38a1e8":"markdown","15a39a44":"markdown","b05becf2":"markdown","600cb194":"markdown","b99db77f":"markdown","8b510746":"markdown","fcc733bb":"markdown","c50d3712":"markdown","d12b04b1":"markdown","d0491992":"markdown","f858cfc4":"markdown","91d34d4b":"markdown","216c461a":"markdown","8a95d970":"markdown","1298934f":"markdown","da7ad0ec":"markdown"},"source":{"e1ebc8ab":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nfrom sklearn.metrics import r2_score, mean_squared_error\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.tree import DecisionTreeRegressor, plot_tree\nfrom sklearn.ensemble import RandomForestRegressor\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","e1ab7c3a":"data = pd.read_csv(\"\/kaggle\/input\/biomechanical-features-of-orthopedic-patients\/column_2C_weka.csv\")","698d6ae7":"data","adfe6cc6":"data.info()","c1f238d3":"list = ['blue' if i=='Abnormal' else 'red' for i in data.loc[:,'class']]\npd.plotting.scatter_matrix(data.loc[:,data.columns != 'class'],\n                          c=list,\n                          figsize=[15,15],\n                          diagonal='hist',\n                          alpha=0.5,\n                          s=200,\n                          marker='*',\n                          edgecolor='black')\nplt.show()","8924c96c":"data.corr()","784561db":"from sklearn.linear_model import LinearRegression\n\nlinear_regression = LinearRegression()\n\nx = data[\"lumbar_lordosis_angle\"].values.reshape(-1,1)\ny = data.sacral_slope.values.reshape(-1,1)","ff49e570":"x_train,x_test,y_train,y_test = train_test_split(x,y, test_size=0.2, random_state=40)","ddd18cd1":"linear_regression.fit(x_train,y_train)","d186f638":"b0 = linear_regression.intercept_\nb1 = linear_regression.coef_\nprint(\"intercept: \",b0)\nprint(\"coef: \",b1)","43b1c6eb":"y_head = linear_regression.predict(x_test)\ny_head_df = pd.DataFrame(y_head, columns=[\"Predicted Response\"])\ny_test_df = pd.DataFrame(y_test, columns=[\"Reals Values\"])\n\npd.concat([y_head_df,y_test_df],axis=1)","dacf9c4f":"plt.scatter(x_test,y_test,color=\"red\")\nplt.plot(x_test,y_head, color=\"orange\")\nplt.xlabel(\"lumbar_lordosis_angle\")\nplt.ylabel(\"sacral_slope\")\nplt.show()","c211af0e":"\nx=data.iloc[:,[0,1,3,4,5]].values\ny=data.lumbar_lordosis_angle.values.reshape(-1,1)","c6a03d49":"x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.30, random_state=0)","a9f0a003":"multiple_lr = LinearRegression()\nmultiple_lr.fit(x_train, y_train)","2078388d":"print(\"intercept: \",multiple_lr.intercept_)\nprint(\"coef: \",multiple_lr.coef_)","3f57af53":"y_head = multiple_lr.predict(x_test)\ny_head_df = pd.DataFrame(y_head,columns=[\"Predicted Response\"])\ny_test_df = pd.DataFrame(y_test,columns=[\"Real Values\"])\npd.concat([y_head_df,y_test_df],axis=1)","c8b194aa":"score = r2_score(y_test,y_head)\nMSE = mean_squared_error(y_test,y_head)\n\nprint(\"R^2 Score: \",score)\nprint(\"MSE: \",MSE)","6ed46d3e":"x = data[\"lumbar_lordosis_angle\"].values.reshape(-1,1)\ny = data.sacral_slope.values.reshape(-1,1)","ac083da4":"poly = PolynomialFeatures(degree=6)\nx_poly = poly.fit_transform(x)","d78c31bf":"x_train, x_test, y_train,y_test = train_test_split(x_poly,y,test_size=0.2,random_state=10)","3ed4c2d1":"poly_reg = LinearRegression()\npoly_reg.fit(x_train,y_train)","76eaf328":"print(\"intercept: \",poly_reg.intercept_)\nprint(\"coef: \",poly_reg.coef_)","83cd40d4":"y_head = poly_reg.predict(x_test)\ny_head_df = pd.DataFrame(y_head,columns=[\"Predicted Response\"])\ny_test_df = pd.DataFrame(y_test,columns=[\"Real Values\"])\npd.concat([y_head_df,y_test_df],axis=1)","525b41de":"score = r2_score(y_test,y_head)\nMSE = mean_squared_error(y_test,y_head)\n\nprint(\"R2 Score : {}\".format(score))\nprint(\"MSE : {}\".format(MSE))","2f6c91a8":"y_head = poly_reg.predict(x_poly)\nsorted_zip = sorted(zip(x,y))\nx,y = zip(*sorted_zip)\n\nplt.scatter(x,y, color=\"blue\")\nplt.plot(x,y_head,color=\"red\")\nplt.xlabel(\"lumbar_lordosis_angle\")\nplt.ylabel(\"sacral_slope\")\nplt.show()","861f7940":"data","583bd806":"x = data.drop([\"lumbar_lordosis_angle\",\"class\"],axis=1)\n#x=data.iloc[:,[0,1,3,4,5]].values\n\ny = data[\"lumbar_lordosis_angle\"].values.reshape(-1,1)","fd08c703":"x_train,x_test,y_train,t_test = train_test_split(x,y,test_size=0.2,random_state=20)","f7421017":"tree = DecisionTreeRegressor()\ndtree = tree.fit(x_train,y_train)","9901f20d":"y_head = dtree.predict(x_test)\ny_head_df = pd.DataFrame(y_head, columns=[\"Predicted Response\"])\ny_test_df = pd.DataFrame(y_test, columns=[\"Real Values\"])\npd.concat([y_head_df,y_test_df],axis=1)","c0a809cd":"fig, axes = plt.subplots(nrows = 1,ncols = 1,figsize = (17,10) , dpi=500 )\nplot_tree(dtree);\n","81b3c199":"score = r2_score(y_test,y_head)\nMSE = mean_squared_error(y_test, y_head)\n\nprint(\"R2 Score : {}\".format(score))\nprint(\"MSE : {}\".format(MSE))","0639cd00":"x, y = data.drop([\"pelvic_incidence\",\"class\"] , axis=1) , data[\"pelvic_incidence\"].values.reshape(-1,1)","f55b08e3":"x_train,x_test,y_train,t_test = train_test_split(x,y,test_size=0.2,random_state=20)","11866e5d":"model_rf = RandomForestRegressor(n_estimators=100)\nmodel_rf.fit(x_train, y_train)","fa7a4445":"y_pred = model_rf.predict(x_test)\ny_pred_df = pd.DataFrame(y_pred, columns=[\"Predicted Response\" ])\ny_test_df = pd.DataFrame(y_test, columns=[\"Real Values\"])\npd.concat([y_test_df , y_pred_df] , axis=1)","a8651aa5":"score = r2_score(y_test, y_pred)\nMSE = mean_squared_error(y_test, y_pred)\n\nprint(\"R2 Score : {}\".format(score))\nprint(\"MSE : {}\".format(MSE))","56c59067":"## Predict","c6a87491":"<a id = '1'><\/a><br>\n## Load and Check Data","59c39e16":"## Fit","fc38a1e8":"## Fit","15a39a44":"<a id = '4'><\/a><br>\n# Polynomial Regression","b05becf2":"## Visualize","600cb194":"pd.plotting.scatter_matrix:\n* red: normal and blue: abnormal\n* c: color\n* figsize: figure size\n* diagonal: histohram of each features\n* alpha: opacity\n* s: size of marker\n* marker: marker type","b99db77f":"## Get Results","8b510746":"## Predict","fcc733bb":"<a id = '2'><\/a><br>\n## Linear Regression","c50d3712":"## Visualize","d12b04b1":"# Random Forest","d0491992":"## Fit","f858cfc4":"## Fit","91d34d4b":"<a id = '3'><\/a><br>\n# Multiple Linear Regression","216c461a":"## Fit","8a95d970":"# Decision Tree","1298934f":"# Introduction\n\n1. [Load and Check Data](#1)\n1. [Linear Regression](#2)\n1. [Multiple Linear Regression](#3)\n1. [Polynomial Regression](#4)","da7ad0ec":"## Visualize"}}