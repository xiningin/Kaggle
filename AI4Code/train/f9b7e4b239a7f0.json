{"cell_type":{"42d508bd":"code","daa98207":"code","9f56119a":"code","bcc1969c":"code","eccbaeb7":"code","7c7afbea":"code","ff1e2465":"code","e88270ae":"code","4441f786":"code","7a572515":"code","73a28b01":"code","dc826e49":"code","18552256":"code","9542d53d":"code","fa9847a5":"code","ccb5851f":"code","04e4c969":"code","b712bf87":"code","46a54ded":"code","9b28a015":"code","5b706e98":"code","007553da":"code","cd9107f5":"code","d60ac873":"code","bba7e2ae":"code","d1564227":"code","5d2bb689":"code","748278ea":"code","7b01c60f":"code","616d2496":"code","62ba4349":"code","0ef06570":"code","fcb51140":"code","62ad82ed":"code","0bc7133c":"code","801e0cd3":"code","b76cebd5":"code","84f23c66":"code","da839cf9":"code","a2020092":"code","57e2d3f7":"code","d41808eb":"code","d1520d8d":"code","c56c9c7f":"code","4b8a2f9f":"code","eb80bb84":"code","fd17fe1c":"code","64304fb6":"code","08d952f4":"code","5fa34448":"code","41b46c3a":"code","c9ca1ad1":"code","af843f2e":"code","ca441b1b":"code","30ca1d67":"code","9d0de6ea":"code","046762b9":"code","ab1955d3":"code","194b8719":"code","8929a746":"code","7e820b3b":"code","0d38a532":"code","8408e2b9":"code","34293c53":"code","4c2d9a2f":"code","eb762b93":"code","0d3d89d5":"code","8d443d54":"code","169b7584":"code","bf98272a":"markdown","43e867fd":"markdown","af51f66d":"markdown","e4f8ca5e":"markdown","accdd45f":"markdown","1afd4b75":"markdown","066de2dc":"markdown","a99d19de":"markdown","f7d03db8":"markdown","d451104a":"markdown","03816b31":"markdown","b1496415":"markdown","e3f44251":"markdown","a0163702":"markdown","635ac576":"markdown","2d328147":"markdown","85bdfe53":"markdown","89652485":"markdown","d92b6a76":"markdown","4bdcab0e":"markdown","7146fde1":"markdown","dbc70d5e":"markdown","b655f2f4":"markdown","886398b6":"markdown","8e2bcd07":"markdown","4442d8a3":"markdown","f0a6b40b":"markdown","7c64d943":"markdown","3607473a":"markdown","f3bf824d":"markdown","a974024f":"markdown","d490e389":"markdown","e2dcee39":"markdown"},"source":{"42d508bd":"%load_ext autoreload\n%autoreload 2\n\n%matplotlib inline","daa98207":"#export\n# from exp.nb_00 import *\nimport operator\n\ndef test(a,b,cmp,cname=None):\n    if cname is None: cname=cmp.__name__\n    assert cmp(a,b),f\"{cname}:\\n{a}\\n{b}\"\n\ndef test_eq(a,b): test(a,b,operator.eq,'==')","9f56119a":"#export\nfrom pathlib import Path\nfrom IPython.core.debugger import set_trace\nfrom fastai import datasets\nimport pickle, gzip, math, torch, matplotlib as mpl\nimport matplotlib.pyplot as plt\nfrom torch import tensor\n\nMNIST_URL='http:\/\/deeplearning.net\/data\/mnist\/mnist.pkl'","bcc1969c":"path = datasets.download_data(MNIST_URL, ext='.gz'); path","eccbaeb7":"with gzip.open(path, 'rb') as f:\n    ((x_train, y_train), (x_valid, y_valid), _) = pickle.load(f, encoding='latin-1')","7c7afbea":"x_train,y_train,x_valid,y_valid = map(tensor, (x_train,y_train,x_valid,y_valid))\nn,c = x_train.shape\nx_train, x_train.shape, y_train, y_train.shape, y_train.min(), y_train.max()","ff1e2465":"assert n==y_train.shape[0]==50000 #chek that n have the sam shape as y_train[0]=50000\ntest_eq(c,28*28)                  #test_eq is a self made fuction defined in the beginning and here we check if the columns are 28*28\ntest_eq(y_train.min(),0)          #check that y_train.mib are 0 \ntest_eq(y_train.max(),9)          #check that y_train.max are 9","e88270ae":"# print a image\nmpl.rcParams['image.cmap'] = 'gray' \n#first image\nimg = x_train[0]\n\n#image size\nimg.view(28,28).type()\n#show image\nplt.imshow(img.view((28,28)));","4441f786":"weights = torch.randn(784,10) #784 by 10 matrix because we got 748 comming in and 10 going out","7a572515":"bias = torch.zeros(10) #for bias we are just staring with 10 zeros ","73a28b01":"##almost everything we do in deep learning are matrix multipication or somethig close to it \n# This will only work if the number of rows on one matrix is the same as the number of columns of the secound matrix \n\ndef matmul(a,b):\n    ar,ac = a.shape # n_rows * n_cols: ar=row, ac=columns\n    br,bc = b.shape #br=row, bc=columns\n    # number of columns in 1. matrix == number of rows in 2. matrix\n    assert ac==br\n    c = torch.zeros(ar, bc) #then lets create a new matrix of size ar and bc so it has enough columns and rows with zeros in\n    for i in range(ar): #for each row in 'a' matrix\n        for j in range(bc): #for each column in 'b' matrix\n            for k in range(ac): # for each column in 'a' matrix\n                c[i,j] += a[i,k] * b[k,j] #This is the part that does the actual calulation where fx where the first row in 'a'\n                # are [1,2,1] an the firs column in 'b' are [2,6,1] now the column in ac wich just is for the first row. \n                # the calulation will then look like this ?c[1,2]+=a[1,1]*b[1,2]? if (i\u00b4m) confused look at the pictures above \n    return c","dc826e49":"x_valid","18552256":"m1 = x_valid[:5] #take the 5 first rows in validaionse \nm2 = weights  #take the weight matrix ","9542d53d":"m1.shape,m2.shape #show shape","fa9847a5":"%time t1=matmul(m1, m2) #marix mutiplication","ccb5851f":"t1.shape #here we see that it go the input shape af m1 and the output shape of 10 by m2 in other words 5 rows by 10 columns output\n","04e4c969":"len(x_train)","b712bf87":"a = tensor([10., 6, -4])  #we create to tensors with 3 dimensions \nb = tensor([2., 8, 7])\na,b","46a54ded":"a + b #then we add hem together an we see that each row are being added together","9b28a015":"(a < b).float().mean()","5b706e98":"m = tensor([[1., 2, 3], [4,5,6], [7,8,9]]); m","007553da":"(m*m).sum().sqrt()  #this is the code of the above formular could also be writen as m.pow(2).sum().sqrt()","cd9107f5":"def matmul(a,b):\n    ar,ac = a.shape\n    br,bc = b.shape\n    assert ac==br\n    c = torch.zeros(ar, bc)\n    for i in range(ar):\n        for j in range(bc):\n            # Any trailing \",:\" can be removed\n            c[i,j] = (a[i,:] * b[:,j]).sum() #a[i,:] are all the rows and b[:,j] are all the columns \n    return c","d60ac873":"%timeit -n 10 _=matmul(m1, m2) ","bba7e2ae":"890.1\/5 #it is faster since the above formular uses C","d1564227":"#export test if the floats are neer since we cant just compare floats with eachother \ndef near(a,b): return torch.allclose(a, b, rtol=1e-3, atol=1e-5)\ndef test_near(a,b): test(a,b,near)","5d2bb689":"test_near(t1,matmul(m1, m2))","748278ea":"a","7b01c60f":"a > 0","616d2496":"a + 1","62ba4349":"m","0ef06570":"2*m","fcb51140":"c = tensor([10.,20,30]); c","62ad82ed":"m","0bc7133c":"m.shape,c.shape","801e0cd3":"m + c","b76cebd5":"c + m","84f23c66":"t = c.expand_as(m)","da839cf9":"t","a2020092":"m + t","57e2d3f7":"t.storage()","d41808eb":"t.stride(), t.shape","d1520d8d":"c.unsqueeze(0)","c56c9c7f":"c.unsqueeze(1)","4b8a2f9f":"m","eb80bb84":"c.shape, c.unsqueeze(0).shape,c.unsqueeze(1).shape  #you can change rows to columns and change i dimentions hereby","fd17fe1c":"c.shape, c[None].shape,c[:,None].shape  #None means squeeze a new axis in here and note tha unsqueeze and None does he same","64304fb6":"c[None].shape,c[...,None].shape","08d952f4":"c[:,None].expand_as(m)","5fa34448":"m + c[:,None]","41b46c3a":"c[:,None]","c9ca1ad1":"def matmul(a,b):\n    ar,ac = a.shape\n    br,bc = b.shape\n    assert ac==br\n    c = torch.zeros(ar, bc)\n    for i in range(ar):\n#       c[i,j] = (a[i,:]          * b[:,j]).sum() # previous\n        c[i]   = (a[i  ].unsqueeze(-1) * b).sum(dim=0)\n        # c[i] set the entyre row same as c[i,:] \n        #.unsqueeze(-1) changes it to a rank two tensor -1 because it is the last dimension same as a[i, None] so it \n        # is of shape ar , 1 since a[i  ].unsqueeze(-1) is the rows. And 'b' is the entyraty of our tensor so it is also rank 2\n        #this return a rank 2 tensor but will sum it up over the rows so we use .sum(dim=0) (dim=0 wha axis you want to sum over)\n        \n    return c","af843f2e":"%timeit -n 10 _=matmul(m1, m2)","ca441b1b":"885000\/277","30ca1d67":"test_near(t1, matmul(m1, m2))","9d0de6ea":"c[None,:]","046762b9":"c[None,:].shape","ab1955d3":"c[:,None]","194b8719":"c[:,None].shape","8929a746":"c[None,:] * c[:,None]","7e820b3b":"c[None] > c[:,None]","0d38a532":"# c[i,j] += a[i,k] * b[k,j]\n# c[i,j] = (a[i,:] * b[:,j]).sum()\ndef matmul(a,b): return torch.einsum('ik,kj->ij', a, b)","8408e2b9":"%timeit -n 10 _=matmul(m1, m2)","34293c53":"885000\/55","4c2d9a2f":"test_near(t1, matmul(m1, m2))","eb762b93":"%timeit -n 10 t2 = m1.matmul(m2)","0d3d89d5":"# time comparison vs pure python:\n885000\/18","8d443d54":"t2=m1@m2 # '@' means matrix multiplication ","169b7584":"t2.shape ","bf98272a":"# **Einstein summation**\n\nEinstein summation (einsum) is a compact representation for combining products and sums in a general way. From the numpy docs:\n\n\"The subscripts string is a comma-separated list of subscript labels, where each label refers to a dimension of the corresponding operand. Whenever a label is repeated it is summed, so np.einsum('i,i', a, b) is equivalent to np.inner(a,b). If a label appears only once, it is not summed, so np.einsum('i', a) produces a view of a with no changes.\"","43e867fd":"This is kinda slow - what if we could speed it up by 50,000 times? Let's try!","af51f66d":"### **Broadcasting a vector to a matrix**\n\nWe can also broadcast a vector to a matrix:","e4f8ca5e":"\nWe want to use PyTorch because it is using C. We do this by using elementwise operators \nElementwise ops\nOperators (+,-,*,\/,>,<,==) are usually element-wise.\n\nExamples of element-wise operations:\n\n","accdd45f":"Linear model:\n    \n    y = ax + b\n\na = weights\n\nb = bias","1afd4b75":"![image.png](attachment:image.png)\n\nSo we flip the matrix with fewest columns and put and the side with the matrix with most columns. Then we do 3 loop where we multiply each row with the given value for row one and add them all together (se pictures below). We do this throught 3 loop ","066de2dc":"**Elementwise matmul** (matrix multiplication)","a99d19de":"We don't really copy the rows, but it looks as if we did. In fact, the rows are given a stride of 0.","f7d03db8":"we are now operation on C or cuda speed not python speed ","d451104a":"Frobenius norm:\n\n$$\\| A \\|_F = \\left( \\sum_{i,j=1}^n | a_{ij} |^2 \\right)^{1\/2}$$\nHint: you don't normally need to write equations in LaTeX yourself, instead, you can click 'edit' in Wikipedia and copy the LaTeX from there (which is what I did for the above equation). Or on arxiv.org, click \"Download: Other formats\" in the top right, then \"Download source\"; rename the downloaded file to end in .tgz if it doesn't already, and you should find the source there, including the equations to copy and paste.","03816b31":"## This is most of fastai part 2 lesson 8, though it is mixed with my notes, eksperiments and what I found usefull, if you want the pure version, check fastai github or the following link: https:\/\/github.com\/fastai\/course-v3\/tree\/master\/nbs\/dl2","b1496415":"# Matrix multiplication","e3f44251":"\n![image.png](attachment:image.png)\n### and so on \nThis comes from matrixmultiplication.xyz","a0163702":"\nYou can always skip trailling ':'s. And '...' means 'all preceding dimensions'","635ac576":"\ntensor([1, 1, 0], dtype=torch.uint8)\nHow are we able to do a > 0? 0 is being broadcast to have the same dimensions as a.\n\nFor instance you can normalize our dataset by subtracting the mean (a scalar) from the entire data set (a matrix) and dividing by the standard deviation (another scalar), using broadcasting.\n\nOther examples of broadcasting with a scalar:","2d328147":"### **Matmul with broadcasting**","85bdfe53":"# **pytorch op**\n\nWe can use pytorch's function or operator directly for matrix multiplication.","89652485":"# Matrix multiplication from foundations\n\nThe *foundations* we'll assume throughout this course are:\n\n* Python\n* Python modules (non-DL)\n* pytorch indexable tensor, and tensor creation (including RNGs - random number generators)\n* fastai.datasets","d92b6a76":"When importing the dataset the above code just set an error if we do it wrong.","4bdcab0e":"## **Broadcasting**\n\nThe term **broadcasting** describes how arrays with different shapes are treated during arithmetic operations. The term broadcasting was first used by Numpy.\n\nFrom the Numpy Documentation:\n\n    The term broadcasting describes how numpy treats arrays with \n    different shapes during arithmetic operations. Subject to certain \n    constraints, the smaller array is \u201cbroadcast\u201d across the larger \n    array so that they have compatible shapes. Broadcasting provides a \n    means of vectorizing array operations so that looping occurs in C\n    instead of Python. It does this without making needless copies of \n    data and usually leads to efficient algorithm implementations.\n\nIn addition to the efficiency of broadcasting, it allows developers to write less code, which typically leads to fewer errors.\n\nThis section was adapted from Chapter 4 of the fast.ai Computational Linear Algebra course.\n","7146fde1":"# Initial python model","dbc70d5e":"You can index with the special value [None] or use unsqueeze() to convert a 1-dimensional array into a 2-dimensional array (although one of those dimensions has value 1).","b655f2f4":"### **Broadcasting with a scalar**","886398b6":"I dont know how or if you even can make your own liberies in kaggle, so we will not use it here","8e2bcd07":"#### **This notebook will go to prove, that you have to use other program languages than only python. This could be Pytorch or something else. In the end we will se that when we do matrix multipication it will be about 50000 times faster to use Pytorch then python. This is usefull when in ML but even more usefull in DL, because we do matrix multiplication very time we add and activation. **","4442d8a3":"\nWhen operating on two arrays\/tensors, Numpy\/PyTorch compares their shapes element-wise. It starts with the trailing dimensions, and works its way forward. Two dimensions are compatible when\n\n* they are equal, or\n* one of them is 1, in which case that dimension is broadcasted to make it the same size\n\nArrays do not need to have the same number of dimensions. For example, if you have a 256*256*3 array of RGB values, and you want to scale each color in the image by a different value, you can multiply the image by a one-dimensional array with 3 values. Lining up the sizes of the trailing axes of these arrays according to the broadcast rules, shows that they are compatible:\n\n    Image  (3d array): 256 x 256 x 3\n    Scale  (1d array):             3\n    Result (3d array): 256 x 256 x 3\n\nThe numpy documentation includes several examples of what dimensions can and can not be broadcast together.","f0a6b40b":"when there are no number from a rank 1 to rank 3 it will inset 1 at the missing places. ","7c64d943":"![image.png](attachment:image.png)","3607473a":"### **Broadcasting Rules**","f3bf824d":"'assert' is a statement that works like a debugging aid that tests a condition. If the condition is true, the program just continues to execute. But if the assert condition evaluates to false, it raises an AssertionError exception with an optional error message.","a974024f":"# Get data","d490e389":"![image.png](attachment:image.png)","e2dcee39":"# Check imports"}}