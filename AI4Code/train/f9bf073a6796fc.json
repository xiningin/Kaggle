{"cell_type":{"f55af4f2":"code","a696fb5d":"code","1a69b709":"code","e74188a2":"code","44e1e2b5":"code","903f5d74":"code","30af0870":"code","20ec56a4":"code","06b2ba20":"code","ff8674e8":"code","1814bbe1":"code","89d499d8":"code","dce351d1":"code","fb15326b":"code","9b2827f2":"code","a50e1bfc":"code","ef64098a":"code","3b42d1a6":"code","95210973":"code","450efd86":"code","a24019d0":"code","022f94d5":"code","29447b13":"code","e6c61177":"code","138569f0":"code","32dcf893":"code","208f64d9":"code","4ac79bbb":"code","990b5a82":"code","e31ef400":"code","8ea48e59":"code","4a69ac18":"code","4468c603":"code","39d7a425":"code","739bc1fa":"code","1dd5743a":"code","5ef0d507":"code","0dffed44":"code","dc414c0f":"code","9241359b":"code","67c6ec02":"code","cc70c8e8":"code","a16d0670":"code","94c9735f":"markdown","ce06a8c3":"markdown","3e92d47e":"markdown","ebe06b2a":"markdown","850f21a3":"markdown","c83a4f1a":"markdown","0f6e1e59":"markdown","b5b949f9":"markdown","5083f168":"markdown","9e5b4869":"markdown","35c59f7f":"markdown","31baf476":"markdown"},"source":{"f55af4f2":"import os\nimport gc\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n\nimport numpy as np\nimport pandas as pd\nimport cudf \n\nfrom xgboost import XGBClassifier\n\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\nfrom sklearn.feature_selection import mutual_info_regression\nfrom sklearn.cluster import MiniBatchKMeans\nfrom sklearn.ensemble import IsolationForest\nfrom scipy import stats\n\n%matplotlib inline\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set_style('darkgrid')","a696fb5d":"%%time\ntrain = cudf.read_csv('..\/input\/tabular-playground-series-dec-2021\/train.csv').set_index(\"Id\")\ntest = cudf.read_csv('..\/input\/tabular-playground-series-dec-2021\/test.csv').set_index(\"Id\")\npseudo = cudf.read_csv('..\/input\/tps12-pseudolabels\/tps12-pseudolabels_v2.csv').set_index(\"Id\")\n\nsample_submission = pd.read_csv(\"..\/input\/tabular-playground-series-dec-2021\/sample_submission.csv\")\nfeature_cols = test.columns.tolist()","1a69b709":"plt.figure(figsize=(10,5))\naxs = sns.countplot(x=train[\"Cover_Type\"].to_pandas())\naxs.bar_label(axs.containers[0])\nplt.show()","e74188a2":"cnt_cols = [col for col in feature_cols if (not col.startswith(\"Soil_Type\")) and (not col.startswith(\"Wilderness_Area\"))]\ntrain[cnt_cols].describe().T","44e1e2b5":"for col in feature_cols:\n    if col in cnt_cols:\n        train[col] = train[col].astype(\"float32\")\n        pseudo[col] = pseudo[col].astype(\"float32\")\n        test[col] = test[col].astype(\"float32\")\n    else:\n        train[col] = train[col].astype(\"bool\")\n        pseudo[col] = pseudo[col].astype(\"bool\")\n        test[col] = test[col].astype(\"bool\")\n        \ntrain[\"Cover_Type\"] = train[\"Cover_Type\"].astype(\"int8\")","903f5d74":"train = train.to_pandas()\ntest = test.to_pandas()\npseudo = pseudo.to_pandas()\nall_df = pd.concat([train.assign(ds=0), pseudo.assign(ds=1)])","30af0870":"%%time\nisf = IsolationForest(random_state=42)\nall_df[\"outlier_isf\"] = isf.fit_predict(all_df[feature_cols])\ntest[\"outlier_isf\"] = isf.predict(test[feature_cols])\n\nprint(all_df[\"outlier_isf\"].value_counts())\nprint(test[\"outlier_isf\"].value_counts())","20ec56a4":"all_df[\"outlier_isf\"] = all_df[\"outlier_isf\"].astype(\"int8\")\ntest[\"outlier_isf\"] = test[\"outlier_isf\"].astype(\"int8\")","06b2ba20":"plt.figure(figsize=(10,5))\naxs = sns.countplot(x=train.loc[all_df.outlier_isf==-1,\"Cover_Type\"])\naxs.bar_label(axs.containers[0])\nplt.title(\"Outliers Count Isolation Forest\")\nplt.show()","ff8674e8":"del isf\n_ = gc.collect()","1814bbe1":"feature_cols.append(\"outlier_isf\")","89d499d8":"sc = StandardScaler()\nx = all_df.copy()\nt = test.copy()\nx[cnt_cols] = sc.fit_transform(x[cnt_cols])\nt[cnt_cols] = sc.transform(t[cnt_cols])","dce351d1":"%%time\nn_clusters = 14\ncd_feature = False # cluster distance instead of cluster number  \n\nkmeans = MiniBatchKMeans(n_clusters=n_clusters, max_iter=300, batch_size=256*5, random_state=42)\n\nif cd_feature:\n    cluster_cols = [f\"cluster{i+1}\" for i in range(n_clusters)]\n    \n    X_cd = kmeans.fit_transform(x[feature_cols])\n    X_cd = pd.DataFrame(X_cd, columns=cluster_cols, index=x.index)\n    all_df = all_df.join(X_cd)\n    \n    X_cd = kmeans.transform(t[feature_cols])\n    X_cd = pd.DataFrame(X_cd, columns=cluster_cols, index=t.index)\n    test = test.join(X_cd)\n\nelse:\n    cluster_cols = [\"cluster\"]  \n    all_df[\"cluster\"] = kmeans.fit_predict(x[feature_cols])\n    test[\"cluster\"] = kmeans.predict(t[feature_cols])\n    \n\nfeature_cols += cluster_cols\n\ntrain.head()","fb15326b":"plt.figure(figsize=(20,8))\nax = sns.countplot(x=\"cluster\", data=all_df, hue=\"Cover_Type\")\nplt.xlabel(\"Clusters\")\nplt.show()","9b2827f2":"x[cluster_cols] = all_df[cluster_cols].copy()\nt[cluster_cols] = test[cluster_cols].copy()","a50e1bfc":"pca = PCA(n_components=2, random_state=42)\nX_pca = pca.fit_transform(x[feature_cols])\nT_pca = pca.transform(t[feature_cols])\n\npca_cols = [f\"PC{i+1}\" for i in range(X_pca.shape[1])]\n\nX_pca = pd.DataFrame(X_pca, columns=pca_cols, index=x.index)\nT_pca = pd.DataFrame(T_pca, columns=pca_cols, index=t.index)\n\nall_df = pd.concat([all_df, X_pca], axis=1)\ntest = pd.concat([test, T_pca], axis=1)\nall_df.head()","ef64098a":"del x, t, X_pca, T_pca\n_ = gc.collect()","3b42d1a6":"loadings = pd.DataFrame(pca.components_, index=pca_cols, columns=all_df[feature_cols].columns)\nloadings.style.bar(align='mid', color=['#d65f5f', '#5fba7d'], vmin=-1.0, vmax=1.0)","95210973":"plt.figure(figsize=(15,8))\nsns.scatterplot(data=all_df, x=\"PC1\", y=\"PC2\", hue=\"Cover_Type\", alpha=0.8, palette=\"deep\")\nplt.show()","450efd86":"feature_cols += [\"PC1\", \"PC2\"]","a24019d0":"all_df[\"likely_type3\"] = all_df[\"PC2\"] < -2.2\nall_df[\"likely_type2\"] = (all_df[\"PC2\"] < 0) & (all_df[\"PC2\"] > -2.2)\nall_df[\"likely_type7\"] = all_df[\"PC2\"] > 3.9\nall_df[\"likely_type1\"] = (all_df[\"PC2\"] > 1) & (all_df[\"PC2\"] < 3.9)\n\ntest[\"likely_type3\"] = test[\"PC2\"] < -2.2\ntest[\"likely_type2\"] = (test[\"PC2\"] < 0) & (test[\"PC2\"] > -2.2)\ntest[\"likely_type7\"] = test[\"PC2\"] > 3.9\ntest[\"likely_type1\"] = (test[\"PC2\"] > 1) & (test[\"PC2\"] < 3.9)","022f94d5":"feature_cols += [\"likely_type3\", \"likely_type2\", \"likely_type7\", \"likely_type1\"]","29447b13":"def r(x):\n    if x+180>360:\n        return x-180\n    else:\n        return x+180\n\nall_df['Aspect2'] = all_df.Aspect.map(r)\ntest['Aspect2'] = test.Aspect.map(r)\n\nall_df.loc[all_df[\"Aspect\"] < 0, \"Aspect\"] += 360\ntest.loc[test[\"Aspect\"] < 0, \"Aspect\"] += 360\n\nall_df.loc[all_df[\"Aspect\"] > 359, \"Aspect\"] -= 360\ntest.loc[test[\"Aspect\"] > 359, \"Aspect\"] -= 360","e6c61177":"all_df['Highwater'] = all_df.Vertical_Distance_To_Hydrology < 0\ntest['Highwater'] = test.Vertical_Distance_To_Hydrology < 0\n\nall_df['DistHydro'] = all_df.Horizontal_Distance_To_Hydrology < 0\ntest['DistHydro'] = test.Horizontal_Distance_To_Hydrology < 0\n\nall_df['DistRoad'] = all_df.Horizontal_Distance_To_Roadways < 0\ntest['DistRoad'] = test.Horizontal_Distance_To_Roadways < 0\n\nall_df['DistFire'] = all_df.Horizontal_Distance_To_Fire_Points < 0\ntest['DistFire'] = test.Horizontal_Distance_To_Fire_Points < 0\n\nall_df['Hillshade_3pm_is_zero'] = all_df.Hillshade_3pm == 0\ntest['Hillshade_3pm_is_zero'] = test.Hillshade_3pm == 0","138569f0":"all_df['EHiElv'] = all_df['Horizontal_Distance_To_Roadways'] * all_df['Elevation']\ntest['EHiElv'] = test['Horizontal_Distance_To_Roadways'] * test['Elevation']\n\nall_df['EViElv'] = all_df['Vertical_Distance_To_Hydrology'] * all_df['Elevation']\ntest['EViElv'] = test['Vertical_Distance_To_Hydrology'] * test['Elevation']","32dcf893":"all_df['EVDtH'] = all_df.Elevation-all_df.Vertical_Distance_To_Hydrology\ntest['EVDtH'] = test.Elevation-test.Vertical_Distance_To_Hydrology\n\nall_df['EHDtH'] = all_df.Elevation-all_df.Horizontal_Distance_To_Hydrology*0.2\ntest['EHDtH'] = test.Elevation-test.Horizontal_Distance_To_Hydrology*0.2","208f64d9":"all_df['Distanse_to_Hydrolody'] = (all_df['Horizontal_Distance_To_Hydrology']**2+all_df['Vertical_Distance_To_Hydrology']**2)**0.5\ntest['Distanse_to_Hydrolody'] = (test['Horizontal_Distance_To_Hydrology']**2+test['Vertical_Distance_To_Hydrology']**2)**0.5\n\nall_df['Hydro_Fire_1'] = all_df['Horizontal_Distance_To_Hydrology']+all_df['Horizontal_Distance_To_Fire_Points']\ntest['Hydro_Fire_1'] = test['Horizontal_Distance_To_Hydrology']+test['Horizontal_Distance_To_Fire_Points']\n\nall_df['Hydro_Fire_2'] = abs(all_df['Horizontal_Distance_To_Hydrology']-all_df['Horizontal_Distance_To_Fire_Points'])\ntest['Hydro_Fire_2'] = abs(test['Horizontal_Distance_To_Hydrology']-test['Horizontal_Distance_To_Fire_Points'])\n\nall_df['Hydro_Road_1'] = abs(all_df['Horizontal_Distance_To_Hydrology']+all_df['Horizontal_Distance_To_Roadways'])\ntest['Hydro_Road_1'] = abs(test['Horizontal_Distance_To_Hydrology']+test['Horizontal_Distance_To_Roadways'])\n\nall_df['Hydro_Road_2'] = abs(all_df['Horizontal_Distance_To_Hydrology']-all_df['Horizontal_Distance_To_Roadways'])\ntest['Hydro_Road_2'] = abs(test['Horizontal_Distance_To_Hydrology']-test['Horizontal_Distance_To_Roadways'])\n\nall_df['Fire_Road_1'] = abs(all_df['Horizontal_Distance_To_Fire_Points']+all_df['Horizontal_Distance_To_Roadways'])\ntest['Fire_Road_1'] = abs(test['Horizontal_Distance_To_Fire_Points']+test['Horizontal_Distance_To_Roadways'])\n\nall_df['Fire_Road_2'] = abs(all_df['Horizontal_Distance_To_Fire_Points']-all_df['Horizontal_Distance_To_Roadways'])\ntest['Fire_Road_2'] = abs(test['Horizontal_Distance_To_Fire_Points']-test['Horizontal_Distance_To_Roadways'])","4ac79bbb":"all_df[\"new_f1\"] = all_df[\"Elevation\"] + all_df[\"Horizontal_Distance_To_Roadways\"] + all_df[\"Horizontal_Distance_To_Fire_Points\"]\nall_df[\"new_f2\"] = (all_df[\"Hillshade_Noon\"] + all_df[\"Hillshade_3pm\"]) - all_df[\"Hillshade_9am\"]\n\ntest[\"new_f1\"] = test[\"Elevation\"] + test[\"Horizontal_Distance_To_Roadways\"] + test[\"Horizontal_Distance_To_Fire_Points\"]\ntest[\"new_f2\"] = (test[\"Hillshade_Noon\"] + test[\"Hillshade_3pm\"]) - test[\"Hillshade_9am\"]","990b5a82":"all_df.loc[all_df[\"Hillshade_9am\"] < 0, \"Hillshade_9am\"] = 0\ntest.loc[test[\"Hillshade_9am\"] < 0, \"Hillshade_9am\"] = 0\n\nall_df.loc[all_df[\"Hillshade_Noon\"] < 0, \"Hillshade_Noon\"] = 0\ntest.loc[test[\"Hillshade_Noon\"] < 0, \"Hillshade_Noon\"] = 0\n\nall_df.loc[all_df[\"Hillshade_3pm\"] < 0, \"Hillshade_3pm\"] = 0\ntest.loc[test[\"Hillshade_3pm\"] < 0, \"Hillshade_3pm\"] = 0\n\nall_df.loc[all_df[\"Hillshade_9am\"] > 255, \"Hillshade_9am\"] = 255\ntest.loc[test[\"Hillshade_9am\"] > 255, \"Hillshade_9am\"] = 255\n\nall_df.loc[all_df[\"Hillshade_Noon\"] > 255, \"Hillshade_Noon\"] = 255\ntest.loc[test[\"Hillshade_Noon\"] > 255, \"Hillshade_Noon\"] = 255\n\nall_df.loc[all_df[\"Hillshade_3pm\"] > 255, \"Hillshade_3pm\"] = 255\ntest.loc[test[\"Hillshade_3pm\"] > 255, \"Hillshade_3pm\"] = 255","e31ef400":"feature_cols += [\"new_f1\", \"new_f2\", \"Aspect2\", \"Highwater\", \"EVDtH\", \"EHDtH\",  'EHiElv', 'EViElv', 'Hillshade_3pm_is_zero',\n                 \"Distanse_to_Hydrolody\", \"Hydro_Fire_1\", \"Hydro_Fire_2\", \"Hydro_Road_1\", \"Hydro_Road_2\", \"Fire_Road_1\", \"Fire_Road_2\"]","8ea48e59":"%%time\nx = all_df.iloc[:5000,:][feature_cols].copy()\ny = train.iloc[:5000,:]['Cover_Type'].copy()\nmi_scores = mutual_info_regression(x, y)\nmi_scores = pd.Series(mi_scores, name=\"MI Scores\", index=x.columns)\nmi_scores = mi_scores.sort_values(ascending=False)","4a69ac18":"top = 20\nplt.figure(figsize=(20,10))\nsns.barplot(x=mi_scores.values[:top], y=mi_scores.index[:top], palette=\"summer\")\nplt.title(f\"Top {top} Strong Relationships Between Feature Columns and Target Column\")\nplt.xlabel(\"Relationship with Target\")\nplt.ylabel(\"Feature Columns\")\nplt.show()","4468c603":"all_df = all_df.append([all_df[all_df[\"Cover_Type\"]==5]]*9, ignore_index=True)","39d7a425":"train = all_df.loc[all_df.ds == 0].drop(columns=['ds'])\npseudo = all_df.loc[all_df.ds == 1].drop(columns=['ds'])\ntrain = train.reset_index(drop=True)","739bc1fa":"del all_df","1dd5743a":"folds = 5\ntrain[\"kfold\"] = -1\npseudo[\"kfold\"] = -1\nkf = StratifiedKFold(n_splits=folds, shuffle=True, random_state=42)\n\nfor fold, (train_indicies, valid_indicies) in enumerate(kf.split(train,train[\"Cover_Type\"])):\n    train.loc[valid_indicies, \"kfold\"] = fold","5ef0d507":"train[\"Cover_Type\"] = train[\"Cover_Type\"] - 1\npseudo[\"Cover_Type\"] = pseudo[\"Cover_Type\"] - 1","0dffed44":"%%time\nfinal_test_predictions = []\nscores = []\n\nfor fold in range(folds):\n    x_train = train[train.kfold != fold].copy()\n    x_valid = train[train.kfold == fold].copy()\n    x_test  = test[feature_cols].copy()\n    \n    x_train = pd.concat([x_train, pseudo],axis=0)\n    \n    y_train = x_train['Cover_Type']\n    y_valid = x_valid['Cover_Type']\n\n    x_train = x_train[feature_cols]\n    x_valid = x_valid[feature_cols]\n\n    xgb_params = {\n        'objective': 'multi:softmax',\n        'tree_method': 'gpu_hist', \n        'use_label_encoder':False,\n        'seed': 42, \n        'eval_metric': ['mlogloss', 'merror'],\n        'predictor': 'gpu_predictor',\n        'learning_rate': .09,\n        'max_depth': 0,\n        'subsample': .15,\n        'sampling_method': 'gradient_based',\n        'seed': 42,\n        'grow_policy': 'lossguide',\n        'max_leaves': 255,\n        'lambda': 100,\n    }\n\n\n    xgb_model = XGBClassifier(**xgb_params)\n    xgb_model.fit(x_train, y_train, eval_set=[(x_valid, y_valid)], verbose=False)\n\n    preds_train = xgb_model.predict(x_train)\n    preds_valid = xgb_model.predict(x_valid)\n    acc_train = accuracy_score(y_train, preds_train)\n    acc = accuracy_score(y_valid, preds_valid)\n    print(f\"Fold {fold}, train: {acc_train:.6f}, valid: {acc:.6f}\")\n    scores.append(acc)\n\n    preds_test = xgb_model.predict(x_test)\n    final_test_predictions.append(preds_test)\n\nprint(\"AVG ACC:\",np.mean(scores))","dc414c0f":"d = pd.DataFrame(np.array([feature_cols,list(xgb_model.feature_importances_)]).T, columns=['feature','importance'])\nd[\"importance\"] = pd.to_numeric(d[\"importance\"])\nd = d.sort_values('importance', ascending=False)","9241359b":"plt.figure(figsize=(20,8))\nsns.barplot(y=\"feature\", x=\"importance\", data=d.iloc[:10])\nplt.show()","67c6ec02":"preds_test = stats.mode(np.array(final_test_predictions))[0]\npreds_test = preds_test.squeeze() + 1","cc70c8e8":"plt.figure(figsize=(10,5))\nax = sns.countplot(x=preds_test)\nplt.title(\"Predictions\")\nplt.xlabel(\"Cover Type\")\nax.bar_label(ax.containers[0])\nplt.show()","a16d0670":"sample_submission['Cover_Type'] = preds_test\nsample_submission.to_csv(\"submission.csv\", index=False)\nsample_submission.head()","94c9735f":"# Plot Prediction","ce06a8c3":"# Submission","3e92d47e":"# Feature Importance","ebe06b2a":"# PCA","850f21a3":"## Add Other Features\n\nThanks to Luca for [this discussion](https:\/\/www.kaggle.com\/c\/tabular-playground-series-dec-2021\/discussion\/291839).","c83a4f1a":"# Scale Data","0f6e1e59":"# Load Data","b5b949f9":"# XGBoost","5083f168":"# Mutual Information","9e5b4869":"# Isolation Forest (Outlier Detection)\nThe IsolationForest \u2018isolates\u2019 observations by randomly selecting a feature and then randomly selecting a split value between the maximum and minimum values of the selected feature.","35c59f7f":"# Reduce Memory Usage","31baf476":"# MiniBatch KMeans\n\nThe MiniBatchKMeans is a variant of the KMeans algorithm which uses mini-batches to reduce the computation time, while still attempting to optimise the same objective function. Mini-batches are subsets of the input data, randomly sampled in each training iteration. These mini-batches drastically reduce the amount of computation required to converge to a local solution. In contrast to other algorithms that reduce the convergence time of k-means, mini-batch k-means produces results that are generally only slightly worse than the standard algorithm."}}