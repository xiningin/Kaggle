{"cell_type":{"ea82c7fe":"code","4283967b":"code","e27a5aad":"code","f88fdd6d":"code","3e30abcd":"code","88f96d97":"code","913ecb01":"code","820b8d86":"code","360e881f":"code","3c2c1479":"code","0f3876e6":"code","f465dfd1":"code","6ef0ee8f":"code","f8338c1d":"markdown","68c4abbc":"markdown","dda8b22b":"markdown","2d57c272":"markdown","592e9ded":"markdown","41a9789a":"markdown","10804abd":"markdown","28059345":"markdown","d620695e":"markdown","d8bde622":"markdown","e8f906b9":"markdown","8493d94b":"markdown","7c1cd459":"markdown","fbab38b0":"markdown"},"source":{"ea82c7fe":"import random\nrandom.seed(123)\n\nimport pandas as pd\nimport numpy as np\nimport datatable as dt\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n# importing evaluation and data split packages\n\nfrom sklearn.model_selection import StratifiedKFold, train_test_split\nfrom sklearn.metrics import roc_auc_score\n\n# importing modelling packages\n\nfrom sklearn.ensemble import RandomForestClassifier","4283967b":"# taking only 10000 rows as sample\n\ntrain = pd.read_csv(r'..\/input\/tabular-playground-series-oct-2021\/train.csv',nrows=10000)\ntest = pd.read_csv(r'..\/input\/tabular-playground-series-oct-2021\/test.csv',nrows=10000)\nsub = pd.read_csv(r'..\/input\/tabular-playground-series-oct-2021\/sample_submission.csv',nrows=10000)","e27a5aad":"def reduce_mem_usage(df, verbose=True):\n    numerics = ['int16', 'int32', 'int64','float16', 'float32', 'float64']\n    start_mem = df.memory_usage().sum() \/ 1024**2    \n    for col in df.columns:\n        col_type = df[col].dtypes\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                else:\n                    df[col] = df[col].astype(np.float32)    \n    end_mem = df.memory_usage().sum() \/ 1024**2\n    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) \/ start_mem))\n    return df","f88fdd6d":"train = reduce_mem_usage(train)\ntest  = reduce_mem_usage(test)","3e30abcd":"X = train.drop(columns=[\"id\", \"target\"]).copy()\ny = train[\"target\"].copy()\ntest_for_model = test.drop(columns=[\"id\"]).copy()\n\n# freeing up some memory\n\ndel train\ndel test \n\n# splitting into training and validation data\n\nX_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.3,random_state=2021,stratify = y)","88f96d97":"model = RandomForestClassifier(n_estimators=100, criterion='gini', max_depth=None,\n                               min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0,\n                               max_features='auto', max_leaf_nodes=None, min_impurity_decrease=0.0, \n                               bootstrap=True, oob_score=False, n_jobs=None,\n                               random_state=None, verbose=0, warm_start=False,\n                               class_weight=None, ccp_alpha=0.0, max_samples=None)\n\n# we will focus on the key ones like n_estimators, max_depth, max_features, criterion, max samples and bootstrap\n# have fixed random_state as 2021 to control bootstrapping randomness.","913ecb01":"estimators = [100,200,500,1000,1500]\n\nfor est in estimators:\n    model = RandomForestClassifier(n_estimators=est,random_state=2021)\n    model.fit(X_train,y_train)\n    print('No. of trees: ',est,\" \",'AUC: ',roc_auc_score(y_test,model.predict_proba(X_test)[:,1]))","820b8d86":"depth = [None,10,20,30,40,50,100]\n\nfor d in depth:\n    model = RandomForestClassifier(max_depth = d,random_state=2021)\n    model.fit(X_train,y_train)\n    print('Max Depth: ',d,\" \",'AUC: ',roc_auc_score(y_test,model.predict_proba(X_test)[:,1]))","360e881f":"max_features = ['auto','log2',100, None]\n\nfor m in max_features:\n    model = RandomForestClassifier(max_features = m,random_state=2021)\n    model.fit(X_train,y_train)\n    print('Max Features: ',m,\" \",'AUC: ',roc_auc_score(y_test,model.predict_proba(X_test)[:,1]))","3c2c1479":"criterion = ['gini','entropy']\n\nfor c in criterion:\n    model = RandomForestClassifier(criterion = c,random_state=2021)\n    model.fit(X_train,y_train)\n    print('Criterion: ',c,\" \",'AUC: ',roc_auc_score(y_test,model.predict_proba(X_test)[:,1]))","0f3876e6":"bootstrap = [True,False]\n\nfor b in bootstrap:\n    model = RandomForestClassifier(bootstrap = b,random_state=2021)\n    model.fit(X_train,y_train)\n    print('Bootstrap: ',b,\" \",'AUC: ',roc_auc_score(y_test,model.predict_proba(X_test)[:,1]))","f465dfd1":"samples = [None,0.1,0.2,0.4,0.8,1]\n\nfor s in samples:\n    model = RandomForestClassifier(max_samples = s,random_state=2021)\n    model.fit(X_train,y_train)\n    print('Max Samples: ',s,\" \",'AUC: ',roc_auc_score(y_test,model.predict_proba(X_test)[:,1]))","6ef0ee8f":"min_samples_leaf = [0.1,0.2,0.3,0.4,0.5]\n\nfor s in min_samples_leaf:\n    model = RandomForestClassifier(min_samples_leaf = s,random_state=2021)\n    model.fit(X_train,y_train)\n    print('Min Samples in Leaf: ',s,\" \",'AUC: ',roc_auc_score(y_test,model.predict_proba(X_test)[:,1]))","f8338c1d":"In this month's TPS, I am diving deep into some of the popular bagging and boosting models and I am starting with Random Forest. The purpose of this exercise is to become better at hyper-parameter tuning. Since there are already a lot of codes on Optuna and others being implemented with Random Forest, we will just look at the basic impact of those parameters in this notebook.\n\nI would make changes to the important parameters and mention their impact. **Please note that these parameter observations are made independent of each other and only for the current data we have**. For speed, I am choosing a simple test split of 30% size on 10000 samples. \nFollowing is the link to the parameters' documentation - https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.ensemble.RandomForestClassifier.html\n\n**Feel free to run your own experiments and upvote if you find this code useful :)**","68c4abbc":"<div style=\"background-color:rgba(210, 129, 21, 0.5);\">\n    <h1><center>Memory Reduction<\/center><\/h1>\n<\/div>","dda8b22b":"<div style=\"background-color:rgba(210, 129, 21, 0.5);\">\n    <h1><center>Understand the Models You Love<\/center><\/h1>\n<\/div>","2d57c272":"# **Bootstrap** - default is true (samples taken with replacement), else the whole data would be taken for building a tree\n\nRecommended value is true, which does better as well.","592e9ded":"# **Criterion** - For splitting the tree based on information gain or Gini Impurity\n\nQuite similar results, but entropy does better.","41a9789a":"<div style=\"background-color:rgba(210, 129, 21, 0.5);\">\n    <h1><center>Your Turn<\/center><\/h1>\n<\/div>","10804abd":"# **Min Samples Leaf** - The minimum number of samples required to be at a leaf node.\n\nThis may have the effect of smoothing the model, especially in regression.","28059345":"# **Max Depth** - The depth of the trees, until we get pure leaves.\n\nBy default, a tree is split until no further information gain is obtained from the leaves.\n'None' is the ideal value, but I have used some others to compare performance.\nMaybe all leaves became pure after 50 as depth, hence we see no change in 50 to 100.","d620695e":"# **Max Samples** - Number of samples taken from data, to build a tree\n\nIf none, it takes all. Otherwise, a value between 0 to 1 is mentioned (as fraction of total)\nAll considered, gave us the best results. There was improvement on its increase until 1.0.","d8bde622":"# **Max Features** - Number of features to be considered when splitting.\n\nBy default, we go with the sqrt of the number of features.\nI have used 'auto' (same as sqrt), log2 (of n_features), None (all features) and 100 as trial values.\n100 gave the best value, so we can consider setting some random values for this parameter when tuning.","e8f906b9":"<div style=\"background-color:rgba(210, 129, 21, 0.5);\">\n    <h1><center>Data Splitting<\/center><\/h1>\n<\/div>","8493d94b":"Please use this notebook to understand the parameters involved and then run your own trials, using Optuna or other optimization packages.\n**I hope you found this useful, please upvote if you did :)**","7c1cd459":"<div style=\"background-color:rgba(210, 129, 21, 0.5);\">\n    <h1><center>Random Forest<\/center><\/h1>\n<\/div>","fbab38b0":"# **Estimators** - The number of trees in the forest\n\nHigher the number of trees, more accuracy, but more training time too.\nAlso, it might lead to overfitting, so have to be careful when setting a high number. The improvement in accuracy also drops beyond a certain number."}}