{"cell_type":{"373f72ba":"code","f5a67d21":"code","227706dc":"code","f9fb96f3":"code","a2b9b23b":"code","cc59a7a0":"code","17a85931":"code","5fb48a25":"code","c21deb0a":"code","683a10d4":"code","03001eac":"code","9d8bb105":"code","f9386bb2":"code","32182f16":"code","4999ffcb":"markdown","36507331":"markdown","029c9b9c":"markdown","1188710c":"markdown","a3cf4cf4":"markdown","eb69349c":"markdown","2618ecb4":"markdown","24c7f541":"markdown","0f796851":"markdown","504f7c3e":"markdown"},"source":{"373f72ba":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file \n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n\nfrom sklearn import preprocessing\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import LabelEncoder\nimport matplotlib.gridspec as gridspec\n%matplotlib inline\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport gc\ngc.enable()\n\nimport os\nos.chdir('\/kaggle\/input\/ieeecis-fraud-detection') # Set working directory\nprint(os.listdir('\/kaggle\/input\/ieeecis-fraud-detection'))","f5a67d21":"%%time\ntrain_transaction = pd.read_csv('train_transaction.csv', index_col='TransactionID')\ntest_transaction = pd.read_csv('test_transaction.csv', index_col='TransactionID')\ntrain_identity = pd.read_csv('train_identity.csv', index_col='TransactionID')\ntest_identity = pd.read_csv('test_identity.csv', index_col='TransactionID')\nprint (\"Data is loaded!\")","227706dc":"print('train_transaction shape is {}'.format(train_transaction.shape))\nprint('test_transaction shape is {}'.format(test_transaction.shape))\nprint('train_identity shape is {}'.format(train_identity.shape))\nprint('test_identity shape is {}'.format(test_identity.shape))","f9fb96f3":"%%time\ntrain_df = pd.merge(train_transaction, train_identity, on = \"TransactionID\", how = \"left\")\nprint(\"Tain: \",train_df.shape)\ndel train_transaction, train_identity\ngc.collect()","a2b9b23b":"%%time\ntest_df = pd.merge(test_transaction, test_identity, on = \"TransactionID\", how = \"left\")\nprint(\"Test: \",test_df.shape)\ntest_df[\"isFraud\"] = 0\ndel test_transaction, test_identity\ngc.collect()","cc59a7a0":"emails = {\n'gmail': 'google', \n'att.net': 'att', \n'twc.com': 'spectrum', \n'scranton.edu': 'other', \n'optonline.net': 'other', \n'hotmail.co.uk': 'microsoft',\n'comcast.net': 'other', \n'yahoo.com.mx': 'yahoo', \n'yahoo.fr': 'yahoo',\n'yahoo.es': 'yahoo', \n'charter.net': 'spectrum', \n'live.com': 'microsoft', \n'aim.com': 'aol', \n'hotmail.de': 'microsoft', \n'centurylink.net': 'centurylink',\n'gmail.com': 'google', \n'me.com': 'apple', \n'earthlink.net': 'other', \n'gmx.de': 'other',\n'web.de': 'other', \n'cfl.rr.com': 'other', \n'hotmail.com': 'microsoft', \n'protonmail.com': 'other', \n'hotmail.fr': 'microsoft', \n'windstream.net': 'other', \n'outlook.es': 'microsoft', \n'yahoo.co.jp': 'yahoo', \n'yahoo.de': 'yahoo',\n'servicios-ta.com': 'other', \n'netzero.net': 'other', \n'suddenlink.net': 'other',\n'roadrunner.com': 'other', \n'sc.rr.com': 'other', \n'live.fr': 'microsoft',\n'verizon.net': 'yahoo', \n'msn.com': 'microsoft', \n'q.com': 'centurylink', \n'prodigy.net.mx': 'att', \n'frontier.com': 'yahoo', \n'anonymous.com': 'other', \n'rocketmail.com': 'yahoo',\n'sbcglobal.net': 'att',\n'frontiernet.net': 'yahoo', \n'ymail.com': 'yahoo',\n'outlook.com': 'microsoft',\n'mail.com': 'other', \n'bellsouth.net': 'other',\n'embarqmail.com': 'centurylink',\n'cableone.net': 'other', \n'hotmail.es': 'microsoft', \n'mac.com': 'apple',\n'yahoo.co.uk': 'yahoo',\n'netzero.com': 'other', \n'yahoo.com': 'yahoo', \n'live.com.mx': 'microsoft',\n'ptd.net': 'other',\n'cox.net': 'other',\n'aol.com': 'aol',\n'juno.com': 'other',\n'icloud.com': 'apple'\n}\n\n# number types for filtering the columns\nint_types = [\"int8\", \"int16\", \"int32\", \"int64\", \"float\"]","17a85931":"# Let's check how many missing values has each column.\n\ndef check_nan(df, limit):\n    '''\n    Check how many values are missing in each column.\n    If the number of missing values are higher than limit, we drop the column.\n    '''\n    \n    total_rows = df.shape[0]\n    total_cols = df.shape[1]\n    \n    total_dropped = 0\n    col_to_drop = []\n    \n    for col in df.columns:\n\n        null_sum = df[col].isnull().sum()\n        perc_over_total = round((null_sum\/total_rows), 2)\n        \n        if perc_over_total > limit:\n            \n            print(\"The col {} contains {} null values.\\nThis represents {} of total rows.\"\\\n                  .format(col, null_sum, perc_over_total))\n            \n            print(\"Dropping column {} from the df.\\n\".format(col))\n            \n            col_to_drop.append(col)\n            total_dropped += 1            \n    \n    df.drop(col_to_drop, axis = 1, inplace = True)\n    print(\"We have dropped a total of {} columns.\\nIt's {} of the total\"\\\n          .format(total_dropped, round((total_dropped\/total_cols), 2)))\n    \n    return df","5fb48a25":"def binarizer(df_train, df_test):\n    '''\n    Work with cat features and binarize the values.\n    Works with 2 dataframes at a time and returns a tupple of both.\n    '''\n    cat_cols = df_train.select_dtypes(exclude=int_types).columns\n\n    for col in cat_cols:\n        \n        # creating a list of unique features to binarize so we dont get and value error\n        unique_train = list(df_train[col].unique())\n        unique_test = list(df_test[col].unique())\n        unique_values = list(set(unique_train + unique_test))\n        \n        enc = LabelEncoder()\n        enc.fit(unique_values)\n        \n        df_train[col] = enc.transform((df_train[col].values).reshape(-1 ,1))\n        df_test[col] = enc.transform((df_test[col].values).reshape(-1 ,1))\n    \n    return (df_train, df_test)","c21deb0a":"def cathegorical_imputer(df_train, df_test, strategy, fill_value):\n    '''\n    Replace all cathegorical features with a constant or the most frequent strategy.\n    '''\n    cat_cols = df_train.select_dtypes(exclude=int_types).columns\n    \n    for col in cat_cols:\n        print(\"Working with column {}\".format(col))\n        \n        # select the correct inputer\n        if strategy == \"constant\":\n            # input a fill_value of -999 to all nulls\n            inputer = SimpleImputer(strategy=strategy, fill_value=fill_value)\n        elif strategy == \"most_frequent\":\n            inputer = SimpleImputer(strategy=strategy)\n        \n        # replace the nulls in train and test\n        df_train[col] = inputer.fit_transform(X = (df_train[col].values).reshape(-1, 1))\n        df_test[col] = inputer.transform(X = (df_test[col].values).reshape(-1, 1))\n        \n    return (df_train, df_test)","683a10d4":"def numerical_inputer(df_train, df_test, strategy, fill_value):\n    '''\n    Replace NaN in the numerical features.\n    Works with 2 dataframes at a time (train & test).\n    Return a tupple of both.\n    '''\n    \n    # assert valid strategy\n    message = \"Please select a valid strategy (mean, median, constant (and give a fill_value) or most_frequent)\"\n    assert strategy in [\"constant\", \"most_frequent\", \"mean\", \"median\"], message\n    \n    # int_types defined earlier in the kernel\n    num_cols = df_train.select_dtypes(include = int_types).columns\n    \n    for col in num_cols:\n\n        print(\"Working with column {}\".format(col))\n\n        # select the correct inputer\n        if strategy == \"constant\":\n            inputer = SimpleImputer(strategy=strategy, fill_value=fill_value)\n        elif strategy == \"most_frequent\":\n            inputer = SimpleImputer(strategy=strategy)\n        elif strategy == \"mean\":\n            inputer = SimpleImputer(strategy=strategy)\n        elif strategy == \"median\":\n            inputer = SimpleImputer(strategy=strategy)\n\n        # replace the nulls in train and test\n        try:\n            df_train[col] = inputer.fit_transform(X = (df_train[col].values).reshape(-1, 1))\n            df_test[col] = inputer.transform(X = (df_test[col].values).reshape(-1, 1))\n        except:\n            print(\"Col {} gave and error.\".format(col))\n            \n    return (df_train, df_test)","03001eac":"def pipeline(df_train, df_test):\n    '''\n    We define a personal pipeline to process the data and fill with processing functions.\n    NOTE: modifies the df in place.\n    '''\n    print(\"Shape of train is {}\".format(df_train.shape))\n    print(\"Shape of test is {}\".format(df_test.shape))\n    # We have set the limit of 70%. If a column contains more that 70% of it's values as NaN\/Missing values we will drop the column\n    # Since it's very unlikely that it will help our future model.\n    print(\"Checking for nan values\\n\")\n    df_train = check_nan(df_train, limit=0.7)\n    \n    # Select the columns from df_train with less nulls and asign to test.\n    df_test = df_test[list(df_train.columns)]\n          \n    print(\"Shape of train is {}\".format(df_train.shape))\n    print(\"Shape of test is {}\".format(df_test.shape))\n          \n    # mapping emails\n    print(\"Mapping emails \\n\")\n    df_train[\"EMAILP\"] = df_train[\"P_emaildomain\"].map(emails)\n    df_test[\"EMAILP\"] = df_test[\"P_emaildomain\"].map(emails)\n\n    print(\"Shape of train is {}\".format(df_train.shape))\n    print(\"Shape of test is {}\".format(df_test.shape))\n          \n    # replace nulls from the train and test df with a value of \"Other\"\n    print(\"Working with cathegorical values\\n\")\n    df_train, df_test = cathegorical_imputer(df_train, df_test, strategy = \"constant\", fill_value = \"Other\")\n    \n    print(\"Shape of train is {}\".format(df_train.shape))\n    print(\"Shape of test is {}\".format(df_test.shape))\n          \n    # now we will make a one hot encoder of these colums\n    print(\"Binarazing values\\n\")\n    df_train, df_test = binarizer(df_train, df_test)\n    \n    print(\"Shape of train is {}\".format(df_train.shape))\n    print(\"Shape of test is {}\".format(df_test.shape))\n          \n    # working with null values in numeric columns\n    print(\"Working with numerical columns. NAN values\\n\")\n    df_train, df_test = numerical_inputer(df_train, df_test, strategy = \"constant\", fill_value=-999)\n        \n    print(\"Shape of train is {}\".format(df_train.shape))\n    print(\"Shape of test is {}\".format(df_test.shape))\n          \n    return (df_train, df_test)","9d8bb105":"# before preprocesing\nprint(\"Train before preprocesing: \",train_df.shape)\nprint(\"Test before preprocesing: \",test_df.shape)\n\ntrain_df, test_df = pipeline(train_df, test_df)\n\n# after preprocesing\nprint(\"Train after preprocesing: \",train_df.shape)\nprint(\"Test after preprocesing: \",test_df.shape)","f9386bb2":"# check for null values\ncolumns = train_df.columns\nfor col in  columns:\n    total_nulls = train_df[col].isnull().sum()\n    if total_nulls > 0:\n        print(col, total_nulls)\n        \ncolumns = test_df.select_dtypes(exclude=int_types).columns\ntrain_df[columns]\n\ncolumns = test_df.select_dtypes(include=int_types).columns\ntrain_df[columns]","32182f16":"train_df.to_pickle('\/kaggle\/working\/train_df.pkl')\ntest_df.to_pickle('\/kaggle\/working\/test_df.pkl')","4999ffcb":"# <a style=\"color:#6699ff\"> III. Preprocessing<\/a>","36507331":"**Pourquoi la d\u00e9tection de fraude ?**\n> La fraude est un commerce d'un milliard de dollars et elle augmente chaque ann\u00e9e. L'enqu\u00eate mondiale de PwC sur la criminalit\u00e9 \u00e9conomique de 2018 a r\u00e9v\u00e9l\u00e9 que la moiti\u00e9 (49 %) des 7 200 entreprises interrog\u00e9es avaient \u00e9t\u00e9 victimes d'une fraude quelconque. C'est une augmentation par rapport \u00e0 l'\u00e9tude PwC de 2016, dans laquelle un peu plus d'un tiers des organisations interrog\u00e9es (36 %) avaient \u00e9t\u00e9 victimes de la criminalit\u00e9 \u00e9conomique.\n\n\nCette comp\u00e9tition est un probl\u00e8me de **classification binaire** - c'est-\u00e0-dire que notre variable cible est un attribut binaire (l'utilisateur qui fait le clic est-il frauduleux ou non ?) et notre objectif est de classer les utilisateurs en \"frauduleux\" ou \"non frauduleux\" le mieux possible.","029c9b9c":"**Load data**","1188710c":"#  <a style=\"color:#6699ff\"> Team <\/a>\n- <a style=\"color:#6699ff\">Mohamed NIANG <\/a>\n- <a style=\"color:#6699ff\">Fernanda Tchouacheu <\/a>\n- <a style=\"color:#6699ff\">Sokhna Penda Toure <\/a>\n- <a style=\"color:#6699ff\">Hypolite Chokki <\/a>","a3cf4cf4":"# <a style=\"color:#6699ff\">  Table of Contents<\/a> \n\n<a style=\"color:#6699ff\"> I. Introduction<\/a>\n\n<a style=\"color:#6699ff\"> II. Descriptive Statistics & Visualization<\/a>\n\n<a style=\"color:#6699ff\"> III. Preprocessing<\/a>\n\n<a style=\"color:#6699ff\"> IV. Machine Learning Models<\/a>","eb69349c":"## Pipeline of preprocessing","2618ecb4":"<img src=\"https:\/\/github.com\/DataCampM2DSSAF\/suivi-du-data-camp-equipe-tchouacheu_toure_niang_chokki\/blob\/master\/img\/credit-card-fraud-detection.png?raw=true\" width=\"800\" align=\"center\">","24c7f541":"# <a style=\"color:#6699ff\"> I. Introduction<\/a>","0f796851":"## Merge transaction & identity ","504f7c3e":"<h1 align=\"center\" style=\"color:#6699ff\"> DataCamp IEEE Fraud Detection <\/h1>"}}