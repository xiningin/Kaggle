{"cell_type":{"aeb61ce8":"code","9a1efac7":"code","b9dfce3a":"code","b56d226b":"code","35cf4692":"code","a7af377a":"code","ccb841f6":"code","28c6e902":"code","a91ef778":"code","a879561d":"code","31a06908":"code","fde4eb8c":"code","94a3c95d":"code","ea2a3fb9":"code","f25f2256":"markdown","18c32d83":"markdown","6289c820":"markdown","2a41aa3a":"markdown","2e7dedb1":"markdown","0886df30":"markdown","ed6ed8c7":"markdown","0f2e8e9c":"markdown","0fbae51e":"markdown","a76a2f2c":"markdown","e66b8fb8":"markdown","7d4317cb":"markdown","3da74c0e":"markdown","f1abdcf7":"markdown","d66a80b7":"markdown"},"source":{"aeb61ce8":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt #Ploting\nimport seaborn as sns #Ploting\nfrom sklearn.model_selection import KFold\nimport lightgbm as lgb\n\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","9a1efac7":"train = pd.read_csv('\/kaggle\/input\/diamonds-ds-ft-2109\/diamonds_train.csv')\ntest = pd.read_csv('\/kaggle\/input\/diamonds-ds-ft-2109\/diamonds_test.csv')\n\n# Change 'Unnamed:0' to a more clarity name.\ntrain.rename(columns={'Unnamed: 0':'id'},inplace=True)\ntest.rename(columns={'Unnamed: 0':'id'},inplace=True)\n\ntrain.head()","b9dfce3a":"fig, ax_arr = plt.subplots(3,2, figsize=(10,5),dpi=200)\nsns.histplot(train['carat'],ax = ax_arr[0][0])\nsns.histplot(train['table'],ax = ax_arr[0][1])\nsns.histplot(train['depth'],ax = ax_arr[1][0])\nsns.histplot(train['x'],ax = ax_arr[1][1])\nsns.histplot(train['y'],ax = ax_arr[2][0])\nsns.histplot(train['z'],ax = ax_arr[2][1])\nplt.tight_layout()","b56d226b":"fig, ax_arr = plt.subplots(2,3, figsize=(10,6),dpi=200)\nax1 = ax_arr[0][0]\nax1.scatter(train['carat'],train['price']\/1000,marker='+',color='tomato',alpha=0.5,s=20)\nax1.set_xlabel('carat',size=14)\nax1.set_ylabel('price(1000$)',size=14)\n\nax2 = ax_arr[0][1]\nax2.scatter(train['x'],train['price']\/1000,marker='+',color='steelblue',alpha=0.5,s=20)\nax2.set_xlabel('x(mm)',size=14)\nax2.set_ylabel('price(1000$)',size=14)\n\nax3 = ax_arr[1][0]\nax3.scatter(train['y'],train['price']\/1000,marker='+',color='plum',alpha=0.5,s=20)\nax3.set_xlabel('y(mm)',size=14)\nax3.set_ylabel('price(1000$)',size=14)\n\nax4 = ax_arr[1][1]\nax4.scatter(train['z'],train['price']\/1000,marker='+',color='skyblue',alpha=0.5,s=20)\nax4.set_xlabel('z(mm)',size=14)\nax4.set_ylabel('price(1000$)',size=14)\n\nax5 = ax_arr[0][2]\nax5.scatter(train['depth'],train['price']\/1000,marker='+',color='orange',alpha=0.5,s=20)\nax5.set_xlabel('depth',size=14)\nax5.set_ylabel('price(1000$)',size=14)\n\nax6 = ax_arr[1][2]\nax6.scatter(train['table'],train['price']\/1000,marker='+',color='tan',alpha=0.5,s=20)\nax6.set_xlabel('table',size=14)\nax6.set_ylabel('price(1000$)',size=14)\n\nplt.tight_layout()","35cf4692":"# Cleaning...Round 1, to replace the outliers in y data.\ny_err = train[train['y']>10]\nidy = list(y_err['id'])\nfor i in idy:\n    train.loc[i,'y'] = train.loc[i,'x']\n\n\n# Cleaning...Round 2, to fill the missing value with 'x' or 'y' != 0.\nz0 = train[train['z']==0]\nidz = list(z0['id'])\nfor i in idz:\n    if (train.loc[i,'x'] == 0 or train.loc[i,'y'] == 0):\n        if (train.loc[i,'x'] == 0 and train.loc[i,'y'] == 0):\n            pass\n        else:\n            temp = max(train.loc[i,'x'], train.loc[i,'y'])\n            train.loc[i,'x'] = temp\n            train.loc[i,'y'] = temp\n    temp = train.loc[i,'depth']*(train.loc[i,'x'] + train.loc[i,'y'])\/2\/100\n    train.loc[i,'z'] = temp\n\n# Cleaning...Round 3, to fill the missing value with both 'x' and 'y' == 0. \n\n# Use curve_fit function to fill the missing 'x', 'y' and 'z'.\nfrom scipy.optimize import curve_fit\ndef func(x, a, b):\n    return a*pow(x, b)\npopt, pcov = curve_fit(func,train['carat'],train['x'])\nax = popt[0]\nbx = popt[1]\npopt, pcov = curve_fit(func,train['carat'],train['y'])\nay= popt[0]\nby = popt[1]\n\npopt, pcov = curve_fit(func,train['z'],train['price'])\nap= popt[0]\nbp = popt[1]\n\nz0 = train[train['z']==0]\nidz = list(z0['id'])\n\n\nfor i in idz:\n    carat = train.loc[i,'carat']\n    train.loc[i,'x'] = func(carat, ax, bx)\n    train.loc[i,'y'] = func(carat, ay, by)\n    temp = train.loc[i,'depth']*(train.loc[i,'x'] + train.loc[i,'y'])\/2\/100\n    train.loc[i,'z'] = temp","a7af377a":"fig, ax_arr = plt.subplots(2,2, figsize=(10,6),dpi=200)\nax1 = ax_arr[0][0]\nax1.scatter(train['carat'],train['price']\/1000,marker='+',color='tomato',alpha=0.5,s=20)\nax1.set_xlabel('carat',size=14)\nax1.set_ylabel('price(1000$)',size=14)\n\nax2 = ax_arr[0][1]\nax2.scatter(train['x'],train['price']\/1000,marker='+',color='steelblue',alpha=0.5,s=20)\nax2.set_xlabel('x(mm)',size=14)\nax2.set_ylabel('price(1000$)',size=14)\n\nax3 = ax_arr[1][0]\nax3.scatter(train['y'],train['price']\/1000,marker='+',color='plum',alpha=0.5,s=20)\nax3.set_xlabel('y(mm)',size=14)\nax3.set_ylabel('price(1000$)',size=14)\n\nax4 = ax_arr[1][1]\nax4.scatter(train['z'],train['price']\/1000,marker='+',color='skyblue',alpha=0.5,s=20)\nax4.set_xlabel('z(mm)',size=14)\nax4.set_ylabel('price(1000$)',size=14)\n\nplt.tight_layout()","ccb841f6":"cut = {'Fair':1,'Good':np.sqrt(2),'Very Good':np.sqrt(3),\n       'Premium':np.sqrt(4),'Ideal':np.sqrt(5)}\ncolor = {'D':np.sqrt(7),'E':np.sqrt(6),'F':np.sqrt(5),\n         'G':np.sqrt(4),'H':np.sqrt(3),'I':np.sqrt(2),'J':1}\nclarity = {'I1':1,'SI2':np.sqrt(2),'SI1':np.sqrt(3),\n           'VS2':np.sqrt(4),'VS1':np.sqrt(5),'VVS2':np.sqrt(6),\n           'VVS1':np.sqrt(7),'IF':np.sqrt(8)}","28c6e902":"#Encoding\nfor i in range(0,train.shape[0]):\n    train.loc[i,'cut'] = cut[train.loc[i,'cut']]\n    train.loc[i,'color'] = color[train.loc[i,'color']]\n    train.loc[i,'clarity'] = clarity[train.loc[i,'clarity']]\n    if (i%1000==0):\n        print('Encoding the train data,',i\/train.shape[0]*100, '%')\n\nfor i in range(0,test.shape[0]):\n    test.loc[i,'cut'] = cut[test.loc[i,'cut']]\n    test.loc[i,'color'] = color[test.loc[i,'color']]\n    test.loc[i,'clarity'] = clarity[test.loc[i,'clarity']]\n    if (i%1000==0):\n        print('Encoding the test data,',i\/test.shape[0]*100, '%')","a91ef778":"train.head()","a879561d":"# Data Norm Function\ndef Norm(x):\n    mi = np.mean(x)\n    ma = np.std(x)\n    return (x-mi)\/ma\n\n# Data Reshaping...\nall_data = pd.concat((train,test))\nall_data = all_data.drop(['id'],axis=1)\n\nall_data['cd'] = Norm(all_data['table']*all_data['depth']\/all_data['carat'])\nall_data['xyzc'] = Norm(all_data['x']*all_data['y']*all_data['z']\/\n                        all_data['carat'])\n\nall_data['xy'] = Norm((all_data['x']\/all_data['y']))\n\nall_data['xy2'] = Norm((all_data['x']\/all_data['y'])**2)\n\nall_data['td'] = Norm(all_data['depth']\/all_data['table'])\n\nall_data['depth'] = Norm(all_data['depth']) \nall_data['table'] = Norm(all_data['table'])\n\nall_data['cut'] = all_data['cut'].astype('float32')\nall_data['color'] =all_data['color'].astype('float32')\nall_data['clarity'] = all_data['clarity'].astype('float32')\n\n\nall_data['ctca'] = Norm(all_data['cut']*all_data['color'])\nall_data['cyca'] = Norm(all_data['clarity']*all_data['color'])\nall_data['crca'] = Norm(all_data['color']*all_data['cut'])\n\nall_data['ctyr'] = Norm(all_data['cut']*all_data['color']*all_data['clarity'])\n\nall_data['cut'] = Norm(all_data['cut'])\nall_data['color'] = Norm(all_data['color'])\nall_data['clarity'] = Norm(all_data['clarity'])\n\n\nall_data['carat'] = Norm(all_data['carat']**2)\nall_data['x'] = Norm(all_data['x']**2)\nall_data['y'] = Norm(all_data['y']**2)\nall_data['z'] = Norm(all_data['z']**2)","31a06908":"X_train = all_data[:train.shape[0]]\nX_test = all_data[train.shape[0]:]\nX_train = X_train.drop(['price'],axis=1)\nX_test = X_test.drop(['price'],axis=1)\ny = train['price']\nX_train.head()","fde4eb8c":"folds = KFold(n_splits=20, shuffle=True, random_state=2300)\nparams = {'num_leaves':64,\n          'min_child_samples':6,\n          'objective':'regression',\n          'learning_rate':0.01,\n          'boosting_type':'gbdt',\n          'metric':'rmse',\n          'max_depth':6,\n          'verbose': -1}\n\ni1=0\nfor trn_idx, val_idx in folds.split(X_train, y):\n    i1 += 1\n    trn_df, trn_label = X_train.iloc[trn_idx,:], y[trn_idx]\n    val_df, val_label = X_train.iloc[val_idx,:], y[val_idx]\n    dtrn = lgb.Dataset(trn_df,label=trn_label)\n    dval = lgb.Dataset(val_df,label=val_label)\n    bst = lgb.train(params,dtrn,num_boost_round=1000,valid_sets=[dtrn,dval],\n                    early_stopping_rounds=200,verbose_eval=100)\n    print('-------------------',i1,'---------------------')","94a3c95d":"sub = pd.read_csv(\"\/kaggle\/input\/diamonds-ds-ft-2109\/sample_submission.csv\")\nypred = bst.predict(X_test)\nsub['price'] = ypred\nsub.to_csv(\"submission.csv\",index=False)","ea2a3fb9":"y_train = bst.predict(X_train)\n\nfig, ax_arr = plt.subplots(2,2, figsize=(10,6),dpi=200)\nax1 = ax_arr[0][0]\nax1.scatter(train['carat'],train['price']\/1000,marker='+',color='gray',alpha=0.5,s=20)\nax1.scatter(train['carat'],y_train\/1000,marker='+',color='tomato',alpha=0.5,s=20)\nax1.set_xlabel('carat',size=14)\nax1.set_ylabel('price(1000$)',size=14)\n\nax2 = ax_arr[0][1]\nax2.scatter(train['x'],train['price']\/1000,marker='+',color='gray',alpha=0.5,s=20)\nax2.scatter(train['x'],y_train\/1000,marker='+',color='steelblue',alpha=0.5,s=20)\nax2.set_xlabel('x(mm)',size=14)\nax2.set_ylabel('price(1000$)',size=14)\n\nax3 = ax_arr[1][0]\nax3.scatter(train['y'],train['price']\/1000,marker='+',color='gray',alpha=0.5,s=20)\nax3.scatter(train['y'],y_train\/1000,marker='+',color='plum',alpha=0.5,s=20)\nax3.set_xlabel('y(mm)',size=14)\nax3.set_ylabel('price(1000$)',size=14)\n\nax4 = ax_arr[1][1]\nax4.scatter(train['z'],train['price']\/1000,marker='+',color='gray',alpha=0.5,s=20)\nax4.scatter(train['z'],y_train\/1000,marker='+',color='skyblue',alpha=0.5,s=20)\nax4.set_xlabel('z(mm)',size=14)\nax4.set_ylabel('price(1000$)',size=14)\n\nplt.tight_layout()","f25f2256":"# Train Model","18c32d83":"Note that the distribution function of 'carat', 'x', 'y' and 'z' are very similar. It's easy to understand. Because diamonds are made of pure carbon, their density is constant. Therefore, the weight of a diamond is directly proportional to its size. But the distribution of these features are not 'normal'. We need to normalizing it later.\n\nOn the other hand, the distribution functions of \u2018table\u2019 and \u2018depth\u2019 are closer to the normal distribution.  But their order of magnitude is higher than other features. By scaling them to [-1,1], our model may perform better.\n\nThen we focus on the relationship between features. We all kown that the heavier the diamond, the more expensive it is. But $price(carat)$ is not a linear mapping. As the scatter plots shows below:","6289c820":"Referring to the table above, we use the following encoding method\uff1a","2a41aa3a":"# Feature Engineering\n## Encode 'cut', 'color' and 'clarity'\nWe need to encode 'color','cut' and 'clarity', to convert them to numeric features. Because the price of diamonds is related to these features, one-hot coding is not suitable. Fortunately, we got a *Rapaport Diamond Price List*.\n\n![Rapaport Diamond Price List](https:\/\/pic3.zhimg.com\/80\/e81bd4d4371c6cf8ab3c423a84e48c22_1440w.jpg)\n\n","2e7dedb1":"After encoding, we got the train data:","0886df30":"## Construct New Features\nThe price of a diamond is related to its shape. Usually, the more \"symmetrical\" the diamond, the higher the price. In order to measure the symmetry of diamonds, we construct new features\uff1a\n\n* $x\/y$\n* $x\\times y\\times z\/carat$\n* $depth\/table$\n* $depth \\times table\/carat$\n\nConsidering that 'cut', 'color' and 'clarity' have synergistic effect on price. We have:\n* $cut\\times color$\n* $cut\\times clarity$\n* $color \\times clarity$\n* $cut\\times color \\times clarity$\n\nAfter construct, we normlize all features by:\n$$\nx_{norm}=\\frac{x-x_{avg}}{x_{std}}\n$$","ed6ed8c7":"Submission:","0f2e8e9c":"After cleaning, we have a better dataset:","0fbae51e":"It is noted that the difference is mainly in the high-price position. This needs to be improved.\n\n**Thanks for reading, Have a nice day! :)**","a76a2f2c":"The Compare between predict and real price:","e66b8fb8":"Divide training data and test data:","7d4317cb":"# Input Data","3da74c0e":"The mappings 'carat-price', 'x'-'price', 'y'-'price' and 'z'-'price' are nearly exponential. But there are two outliers in the 'y' data (over 20mm). And 'x', 'y', 'z' data all have missing values. We need to replace these outliers in data cleaning step.","f1abdcf7":"# Exploratory Data Analysis\nThere are 6 numerical features (carat, depth, table, x, y, z) and 3 category features (color, clarity, cut) in the dataset. First we give the distplot of these numerical features:","d66a80b7":"# Data Cleaning"}}