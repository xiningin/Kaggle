{"cell_type":{"21b5636f":"code","9767f12a":"code","24b950e5":"code","0062b8f8":"code","eb4eafd7":"code","896266d0":"code","05c9b939":"code","7ce65402":"code","9ec73dd6":"code","0d3077f7":"code","ef633d17":"code","ee0d18d9":"code","e468ba14":"code","bf484121":"code","309adfae":"code","25a81a85":"code","783fecdb":"code","3d7a364b":"markdown","85cefd91":"markdown","768aa0e1":"markdown","f8037b56":"markdown","36cecdb4":"markdown","26fe172f":"markdown","ad7b2535":"markdown","3cf9d315":"markdown","7d91512f":"markdown"},"source":{"21b5636f":"# Familiar imports\nimport numpy as np\nimport pandas as pd\n\n# For ordinal encoding categorical variables, splitting data\nfrom sklearn.preprocessing import OrdinalEncoder\nfrom sklearn.model_selection import train_test_split\n\n# For training random forest model\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_squared_error","9767f12a":"# Load the training data\ntrain = pd.read_csv(\"..\/input\/30-days-of-ml\/train.csv\", index_col=0)\ntest = pd.read_csv(\"..\/input\/30-days-of-ml\/test.csv\", index_col=0)\n\n# Preview the data\ntrain.head()","24b950e5":"# Separate target from features\ny = train['target']\nfeatures = train.drop(['target'], axis=1)\n\n# Preview features\nfeatures.head()","0062b8f8":"# List of categorical columns\nobject_cols = [col for col in features.columns if 'cat' in col]\n\n# ordinal-encode categorical columns\nX = features.copy()\nX_test = test.copy()\nordinal_encoder = OrdinalEncoder()\nX[object_cols] = ordinal_encoder.fit_transform(features[object_cols])\nX_test[object_cols] = ordinal_encoder.transform(test[object_cols])\n\n# Preview the ordinal-encoded features\nX.head()","eb4eafd7":"# from sklearn.preprocessing import OneHotEncoder\n# # List of categorical columns\n# object_cols = [col for col in features.columns if 'cat' in col]\n\n# # ordinal-encode categorical columns\n# X = features.copy()\n# X_test = test.copy()\n\n# onehot_encoder=OneHotEncoder(handle_unknown=\"ignore\",sparse=False)\n\n# X= pd.DataFrame(onehot_encoder.fit_transform(features[object_cols]))\n# X_test = pd.DataFrame(onehot_encoder.transform(test[object_cols]))\n\n# X.index=features.index\n# X_test.index=test.index\n\n# X.head()","896266d0":"from sklearn.preprocessing import LabelEncoder","05c9b939":"# # List of categorical columns\n# object_cols = [col for col in features.columns if 'cat' in col]\n\n# # ordinal-encode categorical columns\n# X = features.copy()\n# X_test = test.copy()\n\n# label_encoder = LabelEncoder()\n\n# X[object_cols] = label_encoder.fit_transform(features[object_cols])\n# X_test[object_cols] = label_encoder.transform(test[object_cols])\n\n# # X.index=features.index\n# # X_test.index=test.index\n\n# # Preview the ordinal-encoded features\n# X.head()","7ce65402":"X_train, X_valid, y_train, y_valid = train_test_split(X, y, random_state=0)","9ec73dd6":"# # Define the model \n# model = RandomForestRegressor(random_state=1,n_estimators=200,max_depth=20,n_jobs=5)\n\n# # Train the model (will take about 10 minutes to run)\n# model.fit(X_train, y_train)\n# preds_valid = model.predict(X_valid)\n# print(mean_squared_error(y_valid, preds_valid, squared=False))","0d3077f7":"# RandomForestRegressor?","ef633d17":"from xgboost import XGBRegressor\nfrom xgboost import XGBClassifier,XGBRFRegressor","ee0d18d9":"# model_2 = XGBRegressor(n_estimators=2000,random_state=0,learning_rate=0.02\n#                           ,max_depth=5,n_jobs=5,tree_method='gpu_hist')\n\n# model_2.fit(X_train,y_train,early_stopping_rounds=20,\n#                eval_set=[(X_valid,y_valid)],verbose=False)\n\n# preds_valid_1 = model_2.predict(X_valid)\n# print(mean_squared_error(y_valid, preds_valid_1, squared=False))","e468ba14":"from xgboost import cv\n","bf484121":"param={\"n_estimators\":8000,\"random_state\":0,\"learning_rate\":0.0305677,\"max_depth\":3,\n       \"n_jobs\":4,\"tree_method\":'gpu_hist',\n       'reg_lambda': 0.0005,'reg_alpha': 23.13,'gamma':0.002,'subsample': 0.78,\n    'colsample_bytree': 0.2,'booster':'gbtree','num_parallel_tree':10,'predictor': 'gpu_predictor','subsample':1}\n\nmodel_2 = XGBRegressor(**param)\nmodel_2.fit(X_train,y_train,early_stopping_rounds=20,\n               eval_set=[(X_valid,y_valid)],verbose=500)\n\npreds_valid_1 = model_2.predict(X_valid)\nprint(mean_squared_error(y_valid, preds_valid_1, squared=False))","309adfae":"# xgb_cv = cv(dtrain=X_train, params=param, nfold=3)","25a81a85":"XGBRegressor?","783fecdb":"# Use the model to generate predictions\npredictions = model_2.predict(X_test)\n\n# Save the predictions to a CSV file\noutput = pd.DataFrame({'Id': X_test.index,\n                       'target': predictions})\noutput.to_csv('submission.csv', index=False)","3d7a364b":"Next, we break off a validation set from the training data.","85cefd91":"Once you have run the code cell above, follow the instructions below to submit to the competition:\n1. Begin by clicking on the **Save Version** button in the top right corner of the window.  This will generate a pop-up window.  \n2. Ensure that the **Save and Run All** option is selected, and then click on the **Save** button.\n3. This generates a window in the bottom left corner of the notebook.  After it has finished running, click on the number to the right of the **Save Version** button.  This pulls up a list of versions on the right of the screen.  Click on the ellipsis **(...)** to the right of the most recent version, and select **Open in Viewer**.  This brings you into view mode of the same page. You will need to scroll down to get back to these instructions.\n4. Click on the **Output** tab on the right of the screen.  Then, click on the file you would like to submit, and click on the **Submit** button to submit your results to the leaderboard.\n\nYou have now successfully submitted to the competition!\n\nIf you want to keep working to improve your performance, select the **Edit** button in the top right of the screen. Then you can change your code and repeat the process. There's a lot of room to improve, and you will climb up the leaderboard as you work.","768aa0e1":"In the code cell above, we set `squared=False` to get the root mean squared error (RMSE) on the validation data.\n\n# Step 5: Submit to the competition\n\nWe'll begin by using the trained model to generate predictions, which we'll save to a CSV file.","f8037b56":"# Step 6: Keep Learning!\n\nIf you're not sure what to do next, you can begin by trying out more model types!\n1. If you took the **[Intermediate Machine Learning](https:\/\/www.kaggle.com\/learn\/intermediate-machine-learning)** course, then you learned about **[XGBoost](https:\/\/www.kaggle.com\/alexisbcook\/xgboost)**.  Try training a model with XGBoost, to improve over the performance you got here.\n\n2. Take the time to learn about **Light GBM (LGBM)**, which is similar to XGBoost, since they both use gradient boosting to iteratively add decision trees to an ensemble.  In case you're not sure how to get started, **[here's a notebook](https:\/\/www.kaggle.com\/svyatoslavsokolov\/tps-feb-2021-lgbm-simple-version)** that trains a model on a similar dataset.","36cecdb4":"Welcome to the **[30 Days of ML competition](https:\/\/www.kaggle.com\/c\/30-days-of-ml\/overview)**!  In this notebook, you'll learn how to make your first submission.\n\nBefore getting started, make your own editable copy of this notebook by clicking on the **Copy and Edit** button.\n\n# Step 1: Import helpful libraries\n\nWe begin by importing the libraries we'll need.  Some of them will be familiar from the **[Intro to Machine Learning](https:\/\/www.kaggle.com\/learn\/intro-to-machine-learning)** course and the **[Intermediate Machine Learning](https:\/\/www.kaggle.com\/learn\/intermediate-machine-learning)** course.","26fe172f":"# Step 3: Prepare the data\n\nNext, we'll need to handle the categorical columns (`cat0`, `cat1`, ... `cat9`).  \n\nIn the **[Categorical Variables lesson](https:\/\/www.kaggle.com\/alexisbcook\/categorical-variables)** in the Intermediate Machine Learning course, you learned several different ways to encode categorical variables in a dataset.  In this notebook, we'll use ordinal encoding and save our encoded features as new variables `X` and `X_test`.","ad7b2535":"# Step 2: Load the data\n\nNext, we'll load the training and test data.  \n\nWe set `index_col=0` in the code cell below to use the `id` column to index the DataFrame.  (*If you're not sure how this works, try temporarily removing `index_col=0` and see how it changes the result.*)","3cf9d315":"# Step 4: Train a model\n\nNow that the data is prepared, the next step is to train a model.  \n\nIf you took the **[Intro to Machine Learning](https:\/\/www.kaggle.com\/learn\/intro-to-machine-learning)** courses, then you learned about **[Random Forests](https:\/\/www.kaggle.com\/dansbecker\/random-forests)**.  In the code cell below, we fit a random forest model to the data.","7d91512f":"The next code cell separates the target (which we assign to `y`) from the training features (which we assign to `features`)."}}