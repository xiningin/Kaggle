{"cell_type":{"24c58e18":"code","00651fbd":"code","d7a55c54":"code","947b90b1":"code","66a8a86f":"code","a2043eee":"code","054a61bd":"code","23ca1bc4":"code","36751f98":"code","d618e3ca":"code","94487d98":"code","fbc893dd":"code","1763cd08":"code","dc26e773":"code","83ecab81":"code","98c85b0f":"code","ac829817":"code","c88d2a56":"code","ac936fd2":"code","8ab1744c":"code","78d6352c":"code","4e171868":"code","a951176f":"code","0ebf6fe2":"code","238ba284":"code","346ce970":"code","05dbff1c":"code","0c7e774c":"code","8e15316d":"code","9326e149":"code","c62c8d88":"code","0bdc4a4f":"code","a3e0b347":"code","9fd3cd20":"code","6a3ce2d4":"markdown","12be44ac":"markdown","9f555de4":"markdown","92cccf4d":"markdown","2d65ad6c":"markdown","7f40c11c":"markdown","865a05c9":"markdown","1463f19d":"markdown","a6f0341e":"markdown","55ef3775":"markdown","f31ece66":"markdown","fe3a12b9":"markdown","fc35ab83":"markdown","fff097f9":"markdown","44016523":"markdown","05090eda":"markdown","ead0f547":"markdown","a9809948":"markdown","c0ce87a6":"markdown","9346552e":"markdown","d955fc2e":"markdown","b79f3a02":"markdown","e8262d7e":"markdown","4acfd229":"markdown","9332464b":"markdown","497eda39":"markdown","965bf082":"markdown","b29931c7":"markdown","fd60abfe":"markdown","9a5c35e2":"markdown","2c20e5c3":"markdown"},"source":{"24c58e18":"import numpy as np\nimport pandas as pd\nimport datetime\nimport random\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport warnings\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics import mean_squared_error\nimport matplotlib.pyplot as plt\nplt.rcParams.update({'font.size': 18})\nplt.style.use('ggplot')\nimport seaborn as sns\nfrom scipy import stats\nfrom sklearn.impute import SimpleImputer\nimport shap\nfrom sklearn.preprocessing import StandardScaler\nwarnings.filterwarnings(\"ignore\", category=FutureWarning)","00651fbd":"#code copied from https:\/\/www.kaggle.com\/usharengaraju\/wids2022-lgbm-starter-w-b\ntrain = pd.read_csv(\"..\/input\/widsdatathon2022\/train.csv\")\ntest = pd.read_csv(\"..\/input\/widsdatathon2022\/test.csv\")\nprint(\"Number of train samples are\",train.shape)\nprint(\"Number of test samples are\",test.shape)\ncategorical_features = ['State_Factor', 'building_class', 'facility_type']\nnumerical_features=train.select_dtypes('number').columns","d7a55c54":"train.head()","947b90b1":"plt.figure(figsize = (25,11))\nsns.heatmap(train.isna().values, xticklabels=train.columns)\nplt.title(\"Missing values in training Data\", size=20)","66a8a86f":"#code copied from https:\/\/www.kaggle.com\/shrutisaxena\/wids2022-starter-code\nmissing_columns = [col for col in train.columns if train[col].isnull().any()]\nmissingvalues_count =train.isna().sum()\nmissingValues_df = pd.DataFrame(missingvalues_count.rename('Null Values Count')).loc[missingvalues_count.ne(0)]\nmissingValues_df .style.background_gradient(cmap=\"Pastel1\")\n\ntrain['year_built'] =train['year_built'].replace(np.nan, 2022)\ntest['year_built'] =test['year_built'].replace(np.nan, 2022)\nnull_col=['energy_star_rating','direction_max_wind_speed','direction_peak_wind_speed','max_wind_speed','days_with_fog']\nimputer = SimpleImputer()\nimputer.fit(train[null_col])\ndata_transformed = imputer.transform(train[null_col])\ntrain[null_col] = pd.DataFrame(data_transformed)\ntest_data_transformed = imputer.transform(test[null_col])\ntest[null_col] = pd.DataFrame(test_data_transformed)","a2043eee":"plt.figure(figsize = (25,11))\nsns.heatmap(train.isna().values, xticklabels=train.columns)\nplt.title(\"Missing values in training Data\", size=20)","054a61bd":"le = LabelEncoder()\nfor col in categorical_features:\n    train[col] = le.fit_transform(train[col])\n    test[col] = le.fit_transform(test[col])","23ca1bc4":"train.head()\n","36751f98":"train.describe().style.background_gradient()\n","d618e3ca":"import copy\n#code copied from https:\/\/www.kaggle.com\/usharengaraju\/wids2022-lgbm-starter-w-b\ny = train[\"site_eui\"]\ntrain = train.drop([\"site_eui\",\"id\"],axis =1)\ntest = test.drop([\"id\"],axis =1)\ntrainnames = copy.deepcopy(train)\nscaler = StandardScaler()\ntrain = scaler.fit_transform(train)\ntest = scaler.transform(test)","94487d98":"train","fbc893dd":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(train, y, test_size = 0.2, random_state = 50)","1763cd08":"X_test_ft=pd.DataFrame(X_test,columns=trainnames.columns)","dc26e773":"X_test_ft","83ecab81":"import xgboost\nxgboost_model = xgboost.XGBRegressor(n_estimators=200, learning_rate=0.02, gamma=0, subsample=0.75,\n                           colsample_bytree=0.4, max_depth=5)\nxgboost_model.fit(X_train,y_train)","98c85b0f":"import eli5\nfrom eli5.sklearn import PermutationImportance\n\nperm = PermutationImportance(xgboost_model, random_state=1).fit(X_test, y_test)\neli5.show_weights(perm, feature_names = trainnames.columns.tolist(), top=63)","ac829817":"explainer = shap.Explainer(xgboost_model)\nshap_values = explainer(X_test_ft)","c88d2a56":"shap.summary_plot(shap_values, X_test_ft,plot_type=\"bar\")","ac936fd2":"shap.plots.bar(shap_values,max_display=12)","8ab1744c":"shap.summary_plot(shap_values, X_test_ft)\n","78d6352c":"shap_interaction_values = explainer.shap_interaction_values(X_test_ft)\nshap.summary_plot(shap_interaction_values, X_test_ft)","4e171868":"shap.plots.waterfall(shap_values[0])","a951176f":"shap.plots.waterfall(shap_values[1])","0ebf6fe2":"explainer = shap.TreeExplainer(xgboost_model)\nshap_values = explainer.shap_values(X_test_ft)\nshap.initjs()\ndef p(j):\n    return(shap.force_plot(explainer.expected_value, shap_values[j,:], X_test_ft.iloc[j,:]))\np(0)","238ba284":"p(1)","346ce970":"expected_value = explainer.expected_value\nshap_values = explainer.shap_values(X_test_ft)[0]\nshap.decision_plot(expected_value, shap_values, X_test_ft)","05dbff1c":"shap_values = explainer.shap_values(X_test_ft)[1]\nshap.decision_plot(expected_value, shap_values, X_test_ft)","0c7e774c":"import lime\nimport lime.lime_tabular\n\nexplainer = lime.lime_tabular.LimeTabularExplainer(X_train,\n                    feature_names=trainnames.columns, \n                    class_names=['site_eui'], \n                    categorical_features=categorical_features, \n                    verbose=True, mode='regression')","8e15316d":"exp = explainer.explain_instance(X_test_ft.iloc[0], \n     xgboost_model.predict, num_features=10)\nexp.as_pyplot_figure()","9326e149":"pd.DataFrame(exp.as_list())\n","c62c8d88":"exp.show_in_notebook(show_table=True, show_all=False)\n","0bdc4a4f":"exp = explainer.explain_instance(X_test_ft.iloc[1], xgboost_model.predict)\nexp.show_in_notebook(show_table=True, show_all=False)","a3e0b347":"from sklearn.metrics import explained_variance_score\npredictions = xgboost_model.predict(X_test)\nprint(explained_variance_score(predictions,y_test))","9fd3cd20":"res = xgboost_model.predict(test)\nsub = pd.read_csv(\"..\/input\/widsdatathon2022\/sample_solution.csv\")\nsub[\"site_eui\"] = res\nsub.to_csv(\"submission.csv\", index = False)","6a3ce2d4":"# LOCAL INTERPRETABILITY\n**Waterfall Plot**","12be44ac":"\n<h4 style = \"font-size:40px;font-family:verdana;text-align: center\">EXPLAINABLE AI<\/h4>\n\n\n\n<h2 style = \"font-size:30px;font-family:verdana;text-align: center\">WHAT IS EXPLAINABLE AI?<\/h2>\n\nExplainability in machine learning is the process of explaining to a human why and how a machine learning model made a decision. Model explainability means the algorithm and its decision or output can be understood by a human. It is the process of analysing machine learning model decisions and results to understand the reasoning behind the system\u2019s decision. This is an important concept with \u2018black box\u2019 machine learning models, which develop and learn directly from the data without human supervision or guidance.  \n\n![image.png](attachment:03d23cf5-2338-43f9-85bf-7576631c7c47.png)\n\n<h2 style = \"font-size:30px;font-family:verdana;text-align: center\">WHY IS MODEL EXPLAINABILITY IMPORTANT?<\/h2>\n\nMany of the ML models, though achieving high-level of precision, are not easily understandable for how a recommendation is made. This is especially the case in a deep learning model. As humans, we must be able to fully understand how decisions are being made so that we can trust the decisions of AI systems. We need ML models to function as expected, to produce transparent explanations, and to be visible in how they work.\n\nIn this notebook,I will be walking you through the three important model explainability methods :**their internal working**,**the insights they provide** and **how we can interpret their results**:\n\n1. PERMUTATION IMPORTANCE\n2. SHAP\n3. LIME\n\nIn our problem statement,we have to predict the energy consumption of a building and the model we trained does so.It predicts ,for each sample in the test data,the energy consumption.But how can we get an answer to questions like why our model is predicting the value it is predicting?what are the variables positively correlated with the target?what are negatively correlated with the target?what variables have the highest importance in making predictions for the whole dataset\/for a single example?\nWith the help of these methods we can successfully answer all these WHYs and WHATs related to our model.Great, isn't it?\n\n","9f555de4":"**Decision Plot**\n\nUsed as an alternative for waterfall plot when the number of predictors is high,to get a non-clumsy plot.","92cccf4d":"* The LIME model intercept: 79.40720440515193\n* The LIME model prediction: \"Prediction_local [94.854192]\"\n* The original XGBOOST model prediction: \"Right: 74.22026\"\n\nThe local lime prediction =intercept+sum of coefficients","2d65ad6c":"**INFERENCE:**\n\nThis plot is made of all the dots in the train data. It demonstrates the following information:\n\n* **Feature importance**: Variables are ranked in descending order.\n* **Impact**: The horizontal location shows whether the effect of that value is associated with a higher or lower prediction.\n* **Original value**: Color shows whether that variable is high (in red) or low (in blue) for that observation.\n* **Correlation**: A low value of the \u201cenergy star rating\u201d  has a high and positive impact on the building energy consumption. The \u201clow\u201d comes from the blue color, and the \u201cpositive\u201d impact is shown on the X-axis(the horizontal location). Similarly, \"building class\" and \"facility type\" are also  negatively correlated with the target variable(site_eui_id).","7f40c11c":"# LOCAL EXPLANATION","865a05c9":"We can infer that on the global level ,the top features in the descending order of their importance are:**Energy Star Rating>Facility Type> Floor area**","1463f19d":"The plot looks beautiful,but what do we infer from this?\n\n* **The output value**:In the above plot the number in bold ,i.e. 58.78 is the value our model predicted for the first observation in the validation set.\n* **The base value**: The base value is \u201cthe value that would be predicted if we did not know any features for the current output.\u201d In other words, it is the mean prediction, or mean(yhat).\n* **Red\/blue**: Features that push the prediction higher (to the right) are shown in red, and those pushing the prediction lower are in blue.\n* The values corresponding to the features are mentioned for the observation.\n\nEnergy Star Rating: has a negative impact on the building energy consumption. The energy_star rating value is 1.255 for this observation,which is higher than the average value in the train set. So it pushes the prediction to the left.\nSimilarly we can calculate for other features of the plot as well.\n","a6f0341e":"References:\n* https:\/\/www.kaggle.com\/shrutisaxena\/wids2022-starter-co\n* https:\/\/www.kaggle.com\/usharengaraju\/wids2022-lgbm-starter-w-b\n* https:\/\/www.kaggle.com\/dansbecker\/permutation-importance\n* https:\/\/medium.com\/dataman-in-ai\/explain-your-model-with-lime-5a1a5867b423\n* https:\/\/towardsdatascience.com\/explain-your-model-with-the-shap-values-bc36aac4de3d","55ef3775":"**Force plot**","f31ece66":"The value at the bottom of the plot is E[f(X)] is the base value that is,\u201cthe value that would be predicted if we did not know any features for the current output.\u201d In other words, it is the mean prediction, or mean(yhat),then we move upwards to see how much each feature is affecting the final prediction,f(X) for the given observation,either in the positive or negative direction.This gives a good local interpretation of the model's prediction.","fe3a12b9":"**THE SUMMARY PLOT**:\n\nThe plot below shows the positive and negative relationship of a feature with the target variable prediction based on their SHAP values.Till now,we just saw the magnitude of the impact and not the nature of the impact.We'll see that now:\n","fc35ab83":"# RESULTS IN NOTEBOOK FORMAT:","fff097f9":"**Interaction Plot**","44016523":"# ADVANTAGE OF LIME OVER SHAP:\nWe know that SHAP can give us local +global interpretability and LIME is just giving local interpretability,then why use LIME?It is important to note that the two algorithms are very different.The SHAP calculates the SHAP values for each feature first and to accomplish that it has to train models across all the possible feature combinations globally to get local interpretability.\nBut,the LIME algorithm is simpler and less time consuming when computing local interpretations.It does not go into global calculations in the process.So,the speed is an important factor.\n\n","05090eda":" <h1 style = \"font-size:40px;font-family:verdana;text-align: center;background-color:#DCDCDC\">WIDS DATATHON 2022<\/h1>\n \n![image.png](attachment:37446a0c-3f6f-4358-a6e7-4ff82b7e1842.png)\n\n<h2 style = \"font-size:30px;font-family:verdana;text-align: center\">Problem Statement<\/h2>\n\n<h3 style=\"font-size:18px;font-family:courier\">The WiDS Datathon 2022 focuses on a prediction task involving roughly 100k observations of building energy usage records collected over 7 years and a number of states within the United States. The dataset consists of building characteristics (e.g. floor area, facility type etc), weather data for the location of the building (e.g. annual average temperature, annual total precipitation etc) as well as the energy usage for the building and the given year, measured as Site Energy Usage Intensity (Site EUI). Each row in the data corresponds to the a single building observed in a given year. Your task is to predict the Site EUI for each row, given the characteristics of the building and the weather data for the location of the building.<\/h3>\n\n\n\n**This notebook is used for presentation in WIDS DATATHON 2022 Talk organized by WIDS Mysuru Community on the topic :A Walthrough Of Model Explainability and Interpretability Methods**\n","ead0f547":"**Coefficients of the LIME model by as_list():**","a9809948":"# MISSING VALUE\n","c0ce87a6":"#  GLOBAL INTERPRETABILITY\n\n**Variable Importance Plot**","9346552e":"# 3.LIME:\n\nLocal Interpretable Model-Agnostic Explanations (LIME) can explain the predictions of any classifier in \u201can interpretable and faithful manner, by learning an interpretable model locally around the prediction\u201d. Their approach is to gain the trust of users for individual predictions and then to trust the model as a whole.\n\nOne of the most important and novel feature of **LIME** is:\n\n**LOCAL FIDELITY**:\nThe LIME model is all about interpreting local predictions in a faithful manner.The interpretation of a model's individual observation should correspond to atleast the points in it's vicinity.And,as you can see further how this is implemented in the LIME algorithm and how we can see this in the interpretation itself.\nA model can be trained on 100s of features but while making a prediction for an individual observation,only a subset of those features play a key role and LIME provides us that insight.That is, the local interpretability.LIME,as the name suggests is concerned with local interpretation only.\n\n![image.png](attachment:e804e349-5d12-4446-8dbf-bd0358683163.png)\n\nThe graph above was provided by the authors of LIME.The graph gives an intuition into the working of LIME model.The original model we want to explain(xgboost in our example) is represented by the blue\/pink background.The prediction we need to explain is the bold red cross in the image above.So,LIME:\n\n1. Generates new samples in the vicinity of the red cross instance and then gets their predictions using the original model.\n2. Weighting of the newly generated samples is done by the distance of the samples from the instance to be explained.\n3. Finally,after getting the predictions for the newly generated samples including the red cross,a linear regression model is fitted,the black dashed line in the image above is the one used to explain our original model.This makes sure that the explanation is locally faithful as these explanations are generated from a linear regression model built taking the vicinity points +the original point into account.\n","d955fc2e":"# 1.PERMUTATION IMPORTANCE USING ELI5 LIBRARY:\n\nPermutation Importance is a method of understanding which features are of highest importance to our model.It ranks the features according to the weights in descending order of their importance.\n\nThe algorithm that Permutation Importance uses to get the insights is very interesting and novel.It's easy to understand and it overcomes the shortcomings of other feature importance methods.\n\n1. First we train our model,here the xgboost model in our case on the train set.\n2. Then we use the model as it is to get the predictions on our validation set.Then the rmse is calculated.\n3. Further,the column whose feature importance has to be calculated is randomly shuffled.So,now our validation set is shuffled on 1 column.Then Step 2 is repeated on the shuffled validation set.The new rmse is compared to the old rmse and if there is an increase in the error,the feature importance is high,i.e. the shuffled feature was crucial in making predictions.It may also be possible that rmse decreases on shuffling or there is no change,then the shuffled feature is of nil importance to the model.The decrease in rmse after shuffling can be attributed to chance \/luck or lower size of dataset that means higher probability of luck.\n4. The validation set is then unshuffled to it's original state and the same steps are repeated for other features (one at a time) and then we obtain the feature importances.\n\n","b79f3a02":"# SUBMISSISON","e8262d7e":"* Green\/Red color: features that have positive correlations with the target are shown in green, otherwise red.\n* Energy Star Rating>0.65:High energy star rating negatively correlate with high energy consumption.\n* Facility Type<=0.09:Lower Facility Type positively correlate with high energy consumption.\n\nWe can understand the other features by using the same logic.\n","4acfd229":"# DATA","9332464b":"# VALIDATION LOSS","497eda39":"# LABEL ENCODING","965bf082":"# MODEL:","b29931c7":"# FEATURE SCALING","fd60abfe":"# MISSING VALUE IMPUTATION","9a5c35e2":"**INFERENCE:**\n\nFrom this Variable Importance plot,we get similar results as the Permutation Importance.The features are arranged in descending order of their importance here.On the X-axis,we have the mean SHAP value for a feature.As the SHAP values are calculated per observation of the validation set ,to get a global interpretability across the dataset we need to have a mean of the calculated SHAP values.Here the positive or negative impact is not considered,just the magnitude of impact of a feature on the final predictions is taken into account.The below graph is also similar,just the impact is quantified by mentioning the mean(|SHAP|).","2c20e5c3":"# 2. SHAP \n\n\nSHAP \u2014 which stands for SHapley Additive exPlanations \u2014 is an algorithm was first published in 2017 by Lundberg and Lee and it is a brilliant way to reverse-engineer the output of any predictive algorithm. \n\n\n**WORKING**\n\nWe will see what the Shapley value is and how the SHAP (SHapley Additive exPlanations) value emerges from the Shapley concept.Understanding the idea behind the calculation of SHAP values is crucial to make sense of their outcome. We will go through the theoretical foundation of SHapley Additive exPlanations described in the article by Slundberg and Lee, and see why SHAP values are computed the way they are computed.\n\n[This](https:\/\/towardsdatascience.com\/shap-explained-the-way-i-wish-someone-explained-it-to-me-ab81cc69ef30) article provides a very great explanation of SHAP values calculation.\n\n\nImportant features of SHAP:\n * Global interpretability \n * Local interpretability\n"}}