{"cell_type":{"6a4e1a2a":"code","c8cff6f1":"code","98a9780e":"code","030a1116":"code","b4c2287a":"code","f0152a8e":"code","f5cb28cf":"code","5e01ca0d":"code","de26b676":"code","fc1c083d":"code","5be0841c":"markdown","591867a6":"markdown","c52ee287":"markdown"},"source":{"6a4e1a2a":"#pre-processing imports\nimport nltk\nnltk.download('stopwords')\nnltk.download('punkt')\nfrom nltk.tokenize import word_tokenize\nfrom nltk.corpus import stopwords\nfrom string import punctuation\n\n#imports related to modeling\nimport numpy as np\nfrom gensim.models import Word2Vec, KeyedVectors\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report","c8cff6f1":"pretrain_w2v = \"..\/input\/nlpword2vecembeddingspretrained\/GoogleNews-vectors-negative300.bin\"\nw2v_model = KeyedVectors.load_word2vec_format(pretrain_w2v, binary=True)","98a9780e":"filenames = ['amazon_cells_labelled.txt', 'imdb_labelled.txt', 'yelp_labelled.txt']\n\nwith open('.\/sentiment_sentences.txt', 'w') as outfile:\n    for fname in filenames:\n        with open('..\/input\/sentiment-labelled-sentences-data-set\/sentiment labelled sentences\/' + fname) as infile:\n            outfile.write(infile.read())\nprint(\"File created\")","030a1116":"texts = []\ncats = []\nfh = open(\".\/sentiment_sentences.txt\")\nfor line in fh:\n    text, sentiment = line.split(\"\\t\")\n    texts.append(text)\n    cats.append(sentiment)","b4c2287a":"len(texts)","f0152a8e":"def preprocess_corpus(texts):\n    mystopwords = set(stopwords.words(\"english\"))\n    def remove_stops_digits(tokens):\n        #Nested function that lowercases, removes stopwords and digits from a list of tokens\n        return [token.lower() for token in tokens if token.lower() not in mystopwords and not token.isdigit()\n               and token not in punctuation]\n    #This return statement below uses the above function to process twitter tokenizer output further. \n    return [remove_stops_digits(word_tokenize(text)) for text in texts]\n\ntexts_processed = preprocess_corpus(texts)\nprint(len(cats), len(texts_processed))\nprint(texts_processed[1])\nprint(cats[1])","f5cb28cf":"def embedding_feats(list_of_lists):\n    DIMENSION = 300\n    zero_vector = np.zeros(DIMENSION)\n    feats = []\n    for tokens in list_of_lists:\n        feat_for_this =  np.zeros(DIMENSION)\n        count_for_this = 0 + 1e-5 # to avoid divide-by-zero \n        for token in tokens:\n            if token in w2v_model:\n                feat_for_this += w2v_model[token]\n                count_for_this +=1\n        if(count_for_this!=0):\n            feats.append(feat_for_this\/count_for_this) \n        else:\n            feats.append(zero_vector)\n    return feats\n\ntrain_vectors = embedding_feats(texts_processed)\nprint(len(train_vectors))","5e01ca0d":"texts_processed[0]","de26b676":"train_vectors[0].shape","fc1c083d":"classifier = LogisticRegression(random_state=1234)\ntrain_data, test_data, train_cats, test_cats = train_test_split(train_vectors, cats)\nclassifier.fit(train_data, train_cats)\nprint(\"Accuracy: \", classifier.score(test_data, test_cats))\npreds = classifier.predict(test_data)\nprint(classification_report(test_cats, preds))","5be0841c":"import the pretrained word2vec model","591867a6":"get all the texts","c52ee287":"average embeddings for each word"}}