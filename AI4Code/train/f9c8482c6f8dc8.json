{"cell_type":{"5c930e15":"code","d1e5b5f5":"code","823269d4":"code","87c0b8e6":"code","2bc9b0e6":"code","19b46a56":"code","a7452eeb":"code","ae4d4d50":"code","f655b8ec":"code","c13450f8":"code","871f8cd2":"code","bb008ba9":"code","088b5a04":"code","2966d88a":"code","79a74dd0":"code","ecca787b":"code","7ce0c40d":"code","a3ab7644":"code","166eef08":"markdown","209e3f0a":"markdown","a0cbe960":"markdown","2d6d2bff":"markdown","2cda3158":"markdown","721f4092":"markdown","e6c49047":"markdown","b61da2e1":"markdown","a4c6e914":"markdown","c7e581a5":"markdown","1c181ac4":"markdown","db2ad77a":"markdown","9cf891db":"markdown","05add467":"markdown","6f4ff95c":"markdown","1be145dd":"markdown","ea9eafaa":"markdown"},"source":{"5c930e15":"import os\nos.environ['DGLBACKEND'] = 'tensorflow'\n\n!pip -q install dgl-cu101\n!pip -q install tfdlpack-gpu  # when using tensorflow gpu version\n!export TF_FORCE_GPU_ALLOW_GROWTH=true # and add this to your .bashrc\/.zshrc file if needed\n!pip -q install rdflib\nimport dgl\nprint(dgl.backend.backend_name)\n\n","d1e5b5f5":"# general import\nimport time\n\n# datascince import\nimport numpy as np\n\n# tf import\nimport tensorflow as tf\nfrom tensorflow.keras import layers\n\n# dgl import\nfrom dgl import DGLGraph\nfrom dgl.contrib.data import load_data\n# If you don't know it and there was a function to react(reaction, someone) I will make a special function from it partial(react, 'haha') for you now.\nfrom functools import partial","823269d4":"data = load_data('aifb', bfs_level=3, relabel=False)","87c0b8e6":"num_nodes = data.num_nodes\nnum_rels = data.num_rels\nnum_classes = data.num_classes\nlabels = data.labels\ntrain_idx = data.train_idx\ntest_idx = data.test_idx","2bc9b0e6":"# number of nodes\nnum_nodes\n\n# number of edges\nlen(data.edge_norm)\n\n# Edges have normalized values, I don't care..I guess it's just scaling.. If an edge is between three people, \n# it should take a higher weight than another one between a thousand person \nprint(data.edge_norm)","19b46a56":"# number of train relations, and test relations, they sum to 176\nprint(len(train_idx),len(test_idx))\n\n# print the class of each node in the train ids\nprint(labels[train_idx])","a7452eeb":"val_idx = train_idx[:len(train_idx) \/\/ 5]\ntrain_idx = train_idx[len(train_idx) \/\/ 5 :]","ae4d4d50":"# since the nodes are featureless, the input feature is then the node id.\nfeats = tf.range(num_nodes, dtype=tf.int64)\nprint(feats)","f655b8ec":"edge_type = tf.convert_to_tensor(data.edge_type)\n\n# number of edge types is 89\n# In the above it says number of relations is 91. \n# they should be the same\n# Because in the end I will have a semi-supervised problem to solve. I don't care about anything else\n# And some news said, that they removed two relations like employes, affiliated to . because based on them we induced the four classes\n#(The four classes are just affiliations of each node to one of four research groups) \nprint(len(np.unique(edge_type)))","c13450f8":"# edge norm -> we talked about it before\nedge_norm = tf.expand_dims(tf.convert_to_tensor(data.edge_norm), 1)\n\n# labels \n# everything but the 176 nodes have 0 labels.\nlabels = tf.reshape(tf.convert_to_tensor(labels), (-1, ))\nprint(labels)","871f8cd2":"# create graph\ng = DGLGraph()\ng.add_nodes(num_nodes)\ng.add_edges(data.edge_src, data.edge_dst)","bb008ba9":"class RelGraphConv(layers.Layer):\n    def __init__(self,\n                 in_feat,\n                 out_feat,\n                 num_rels, \n                 num_bases):\n        super(RelGraphConv, self).__init__()\n        self.in_feat = in_feat\n        self.out_feat = out_feat\n        self.num_rels = num_rels\n        self.num_bases = num_bases\n      \n        # initialize the weights for a number of linear models we choose above (num_bases)\n        xinit = tf.keras.initializers.glorot_uniform()\n        self.weight = tf.Variable(initial_value=xinit(shape=(self.num_bases, self.in_feat, self.out_feat),dtype='float32'), trainable=True)\n        \n        # linear combination coefficient\n        # This will reduce the weights from num_rels (89 models) to a lower number (num_bases)\n\n        self.w_comp = tf.Variable(initial_value=xinit(shape=(self.num_rels, self.num_bases), dtype='float32'), trainable=True)\n        \n        # message func, similar to any convolution graph layer , google pagerank algorithm for more information.\n        self.message_func = self.basis_message_func\n        \n        \n    def basis_message_func(self, edges):\n        # generate all weights from bases models\n        weight = tf.reshape(self.weight, (self.num_bases, self.in_feat * self.out_feat))\n        \n        # extract back the 91 models.. just for training .. we will combose them again.\n        weight = tf.reshape(tf.matmul(self.w_comp, weight), (self.num_rels, self.in_feat, self.out_feat))\n        \n        # get the features of the source nodes (just the id) of all the nodes in the edges. A.shape is (65439,)..because this is \\\n        # tensorflow, we are going to do all the computations at once\n        A = edges.src['h']\n        \n        # and get their type (one of 91). It's shape is also (65439,)\n        index = edges.data['type']\n\n        # weight.shape is supposed to be (91, 8285, out_feat) i.e. 8285 weights for each relation type i.e. 91 models\n        \n\n        if len(A.shape) == 1:\n            # for the input layer, when we input the feats(just one dimension) to the first layer it will be just the feats. \n            # I will collect all the weights for each realation type \n            # I will put them all together 91 * 8285 = 753935\n            # It's shape will be (753935, out_feats)\n            weight = tf.reshape(weight, (-1, weight.shape[2]))\n            \n            # as you know the index i.e. type of each node is useless except for the 176 nodes we want to classify. \n            # For every other node, the index will be zero\n            # index * weight.shape[1] -> if the index is zero, it will just keep it's feature (id) only if it's in the src edges (A) of the node \\\n            # we are working one ...\n            flatidx = index * weight.shape[1] + A\n            \n            # grather just slices the weights to ...I don't care\n            msg = tf.gather(weight, flatidx)\n            \n            # In the end, I don't care about this.. \n            # What I care about is that msg has this shape (65439, out_feat) which means .. if the out_feat is 16 the above stuff helped me \n            # collect 16 different messages from neighbors for the source node of all the edges (65439 edge).   \n\n        else:\n            # For next layers .. The weight is already taken from the previous layer its shape is (91 models, 16 messages from previous layer, out_feats)\n            weight_slice = tf.gather(weight, index)\n            msg = tf.squeeze(tf.matmul(tf.expand_dims(A, 1), weight_slice))\n            # In the end, I don't care about this.. \n            # What I care about is that msg has this shape (65439, out_feat) which means .. if the out_feat is 16 the above stuff helped me \n            # collect 16 different messages from neighbors for the source node of all the edges (65439 edge). \n            \n            \n            # In the last layer the output will be one of four classes.. So I will rewrite again\n            \n            # In the end, I don't care about this.. \n            # What I care about is that msg has this shape (65439, 4) which means .. if the out_feat is 4 the above stuff helped me \n            # collect 4 different messages from neighbors for the source node of all the edges (65439 edge). \n            \n            # And with these four messages we will calculate loss (just a simple categorical loss) and optimizer the weights to reduce loss\n            \n        # some normalization .. just scaling.\n        msg = msg * edges.data['norm']\n        return {'msg': msg}\n    \n\n    def call(self, g, feats, edge_type, edge_norm):\n        # create a local copy .. for memory optimization ..\n        g = g.local_var()\n        \n        # feats as we said as just the id for each node\n        g.ndata['h'] = feats\n        # type is one of the 91 relations\n        g.edata['type'] = tf.cast(edge_type, tf.int64)\n        \n        # some normalization factor for edge we have in advance. could be the mean of each relation type.just scaling.\n        g.edata['norm'] = edge_norm\n        # message passing very opimized in one step. Without infinite loops thanks to dgl\n        g.update_all(self.message_func, dgl.function.sum(msg='msg', out='h'))\n        # return output features\n        node_repr = g.ndata['h']\n\n        return node_repr","088b5a04":"class EntityClassifier(tf.keras.Model):\n    def __init__(self, num_nodes, h_dim, out_dim, num_rels, num_bases):\n        super(EntityClassifier, self).__init__()\n        \n        # create rgcn layers\n        self.input_to_hidden = RelGraphConv(num_nodes, h_dim, num_rels, num_bases)\n        self.hidden_to_hidden = RelGraphConv(h_dim, h_dim, num_rels, num_bases)\n        self.hidden_to_output = RelGraphConv(h_dim, out_dim, num_rels, num_bases)\n\n        self.output_activation = partial(tf.nn.softmax, axis=1)\n        \n    def call(self, g, feats, edge_type, edge_norm):\n        feats = tf.nn.relu(self.input_to_hidden(g, feats, edge_type, edge_norm))\n        feats = tf.nn.relu(self.hidden_to_hidden(g, feats, edge_type, edge_norm))\n        feats = self.output_activation(self.hidden_to_output(g, feats, edge_type, edge_norm))\n        return feats","2966d88a":"# create model\nmodel = EntityClassifier(len(g), h_dim=16, out_dim=num_classes, num_rels=num_rels, num_bases=25)\n\n# optimizer .. same for any simple model\noptimizer = tf.keras.optimizers.Adam(learning_rate=1e-2)\nloss_func = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False)\n","79a74dd0":"def acc(logits, labels, mask):\n    logits = tf.gather(logits, mask)\n    labels = tf.gather(labels, mask)\n    indices = tf.math.argmax(logits, axis=1)\n    acc = tf.reduce_mean(tf.cast(indices == labels, dtype=tf.float32))\n    return acc","ecca787b":"# start the grad , make a loop for some more accuray\nfor i in range(10):\n    with tf.GradientTape() as tape:\n        # enter the features , get predictions\n        logits = model(g, feats, edge_type, edge_norm)\n        # compare predictions to true values to get the loss\n        loss = loss_func(tf.gather(labels, train_idx), tf.gather(logits, train_idx))\n\n        # some manual weight decay suff.. just regularize the weights with l2 \n        for weight in model.trainable_weights:\n            loss = loss + 5e-4 * tf.nn.l2_loss(weight)\n\n        # same for all models, use optimizer to update weights\n        grads = tape.gradient(loss, model.trainable_weights)\n        optimizer.apply_gradients(zip(grads, model.trainable_weights))\n\n        # get the accuracy to print them\n        train_acc = acc(logits, labels, train_idx)\n        val_loss = loss_func(tf.gather(labels, val_idx), tf.gather(logits, val_idx))\n        val_acc = acc(logits, labels, val_idx)\n\n        print(\"Train Accuracy: {:.4f} | Train Loss: {:.4f} | Validation Accuracy: {:.4f} | Validation loss: {:.4f}\".\n                    format(train_acc, loss.numpy().item(), val_acc, val_loss.numpy().item()))\n\n","7ce0c40d":"# testing\nlogits = model(g, feats, edge_type, edge_norm)\ntest_loss = loss_func(tf.gather(labels, test_idx), tf.gather(logits, test_idx))\ntest_acc = acc(logits, labels, test_idx)\nprint(\"Test Accuracy: {:.4f} | Test loss: {:.4f}\".format(test_acc, test_loss.numpy().item()))\n","a3ab7644":"model.summary()","166eef08":"# Step 1 -> upvote it now please! , so I recursively know that you need more so I work more.\nYour support will notify me that there is an interest in the field. Your support will lead me to :\n\nImprove this kernel to summarize all stuff about node classification to be in 100 educational steps and I guarantee it will be very well explained\nWrite additional kernels on graph classification and edge classfication or link prediction (please comment with your email , if you want to allow me to notify you when these kernels are ready.\n\n# Step 2 -> Introduction\n\n|  | <font style=\"color:red\">Graph convolution networks<\/font> | <font style=\"color:red\"> Relational Graph convolution networks<\/font> |\n| - | - | - |\n| Type of netwok that it works on | Simple just some nodes connected with edges | A directed labeled network  |\n| Example | papers citing each others | knowledge graph (google it)\n| Weights | Just make a dense layer for each node and pass messages from each node to the other | make a dense layer for each relation type since we have many types of relation e.g. ahmed employes(relation type) mohamed, John visits (relation type) [Dahab](https:\/\/www.youtube.com\/watch?v=CheLNATs4IA&t=33s) because it's awesome and cheap and the author can invite you for dinner there.","209e3f0a":"# Step 13 -> Learn more about the model","a0cbe960":"# Step 10 -> Not needed. Create a fancy accuracy function.\nGiven predicted labels    [1,0,2,1,2,3,0] \n\nand true values [0,1,1,1,3,3,0]\n\nBelow is a nice acc function .. very fast with tf functions and amazingly accepts logits instead of predicted labels.. haha Man just use argmax. no big deal.  \n","2d6d2bff":"# Step 12 -> Test the model","2cda3158":"# Step 5 -> Load the data\n\nThe AIFB dataset describes the AIFB research institute in terms of its staff, research group, and publications. In (Bloehdorn et al 2007) the dataset was first used to predict the affiliation (i.e., research group) for people in the dataset. The AIFB Dataset is an RDF dataset pertaining to the Institute for Applied Informatics and Formal Description Methods at the Karlsruhe Institute of Technology6 . It describes the inter-relationships between persons (e.g., Professors,Students), research topics, projects, publications etc.  Refer to the Bloehdorn paper to learn more.\n\n\nThe dataset contains 178 members of a research group 4 classes.\n\n","721f4092":"# Step 6 -> It's relational not simple.\n\nIn the table at the start,  we know that we are taking about relational graphs. Edges are labeled and directed.\n\nYou can't treat all the edges the same.\n\nWe have to have weights for each type of edges. E.g. nodes that have a \"lives_in\" edge should be multiplied by a weight (that will be trained) different from the weight that will be mulitplied by \"plays\" edge.","e6c49047":"# Step 5 -> define the problem\n\n\nAn example for a relation is like this ('ahmed', likes, 'dahab'). The takes the form of (node, predicate, node). \n\nI have a trainable classification regarding 'ahmed' , I know that 'ahmed' is classified as a person.\n\nNow I want to build a model that I can train on :\n\n|X| Y(class)|\n|-|-|\n|('ahmed', likes, 'dahab')| person\n|('John', drinks, 'tea')| person\n|('tina', eats, 'raw_fish') | cat\n|('pato', has, 'tail') | cat\n|('tony', drinks, 'coffee') | person\n\nYou will also have many other relations without classes (your model has to learn from them but doesn't have to classify them at all) There unlabeled data will make our model \"semi-supervised\"\n\n|X|\n|-|\n|('coffee', similar_to, 'tea')|\n|('ahmed', loves, 'tina')|\n|('dahab', is_in, 'Egypt')|\n|('Mohamed', loves, 'Mona')|\n|('Egypt', visited_by, 'Mohamed)|\n\n\nNow I want your model to tell me from this realation\n('mona', drinks, 'coffee')   if 'mona' is a cat or a person ? \n\n<font color='green'> Seriously. This is awesome. Because I will know something about mona without having to even ask her or her friends. I will ask friends of friends of friends of her friends. I am spying on her. So smart. Really. <\/font>\n\n<font color='red'> This is awesome because it causes [combinatorial generalization](https:\/\/arxiv.org\/pdf\/1806.01261.pdf).<\/font>.\n \n \nBased on the above case, \nWe are going to do semi-supervised learning. \n\nData are split in two parts...\n\n\n## Part one -> so many unlabeled relations...\n(node, predicate, node)\n","b61da2e1":"# Where are the features ?\nAs we don't know much about each node......\nFor example, we don't know anything about mona or ahmed except that they are in a relation. We only know they love each other. So treat nodes id itself as a feature. We have 8285 nodes and 8285 features.","a4c6e914":"# Step 11 -> train the model","c7e581a5":"# Step 9 -> Create the model\n\nJust a simple three layered model.\n__init__ part defines the layers\ncall part runs them ..\nfor the last layer we use softmax.. similar to any linear model","1c181ac4":"# Step 4 -> Import everything you need\n","db2ad77a":"# Step 8  -> Create the relational graph convolutional layer\n\nThe Relational Graph Convolution layer is provided by the dgl library. I prefer to create it on my own.\n\n\nFor any layer I need to \n1. Know my input and output \n2. Do some function by multiplying the input by some weights (define the function in the __init__ part and use it in the call part.\n3. The function should do some message passing ...why ? because this a network\n\n## Input\n\n|Input| why| shape|\n|-|-|\n|in_feat| each node has only one feature. Its id| 8285|  \n|out_feat| The classes we want to predit| 4|\n|num_rels| because each relation type of the 89 types will have it's weights separately.| 89 model !!!!!!|\n|num_bases| because it's not possible to have 89 models, we will do linear decomposition to reduce them to let's say 16 models | a number of models you choose e.g. 16 models| ","9cf891db":"# Step 7 -> build the graph","05add467":"Additionally, create a validation set","6f4ff95c":"# Step 3 -> Decide which library to use\nI will use the dgl library (similar to keras) with tensorflow as backend\n\ndgl is responsible for the message passing part and everything else is conducted by tensorflow","1be145dd":"## Part two -> the labeled relations \nOn this part we are  going to train and classfiy the nodes. It's very little. Only 176 relations. For each relation we know the class that the first node in the relation belongs to. Any node of the nodes that appear in these 176 relations will belong to one of four classes ( 0, 1, 2, 3).\n","ea9eafaa":"# Step 3 -> Try your best to understand message passing\n## The best ever explanation of message passing\n\n\nIf though about each node as an independent input, you can build a simple dense layer.   Remember the most simple classification problem you ever learnt.\nYou have some users for each user you know [age, gender, height, weight, education, etc] and you want to predict his salary.\nAnother example, Imaging some users on twitter writing tweets (words) and you would like to predict if they are happy or not (sentiment).\n\n- I would do this \n> y = wx + b \n- If I ignore bias, I would do this :\n> y = wx \n- With tensorflow, I would do : (don't think about A (refers to some features) or weight_slice for now )\n> tf.matmul(tf.expand_dims(A, 1), weight_slice)\n\nBecause this a network...\nYou are the summation of your connections\nThis means Your connections features * weights ALL of them will be passed to you in addition to your own featurs and weights.\nSo the weights and features will be enter a message passing function which will take all edges of a node as input and sum all its surrounding nodes.\n\n    def basis_message_func(self, edges):\n        # generate all weights from bases\n        weight = tf.reshape(self.weight, (self.num_bases, self.in_feat * self.out_feat))\n        weight = tf.reshape(tf.matmul(self.w_comp, weight), (self.num_rels, self.in_feat, self.out_feat))\n        \n        A = edges.src['h']\n        \n        index = edges.data['type']\n        if len(A.shape) == 1:\n            # for the input layer\n            weight = tf.reshape(weight, (-1, weight.shape[2]))\n            flatidx = index * weight.shape[1] + A\n            msg = tf.gather(weight, flatidx)\n        else:\n            weight_slice = tf.gather(weight, index)\n            msg = tf.squeeze(tf.matmul(tf.expand_dims(A, 1), weight_slice))\n            \n            \n        msg = msg * edges.data['norm']\n        return {'msg': msg}\n   \nIn the end, it collects messages from all edges around the node (message passing). The next step is to sum them up.\n\n        g.ndata['h'] = feats\n        g.edata['type'] = tf.cast(edge_type, tf.int64)\n        g.edata['norm'] = edge_norm\n        # message passing \n        g.update_all(self.message_func, dgl.function.sum(msg='msg', out='h'))\n        node_repr = g.ndata['h']\n        \nFor more explaination, Choose from the following: \n\n<input type=\"radio\" disabled> [<strike>Read the paper<\/strike>](https:\/\/arxiv.org\/pdf\/1703.06103.pdf)\n<input type=\"radio\" disabled> [<strike>Check the dgl tutorial<\/strike>](https:\/\/docs.dgl.ai\/en\/0.4.x\/tutorials\/models\/1_gnn\/4_rgcn.html)\n<input type=\"radio\" disabled> [<strike>Learn more about the pagerank algorithm<\/strike>](https:\/\/docs.dgl.ai\/en\/0.4.x\/tutorials\/basics\/3_pagerank.html)\n\n<input type=\"radio\" checked> Upvote this kernel, So I work hard to make it awesome\n\n\n![](https:\/\/raw.githubusercontent.com\/benedekrozemberczki\/APPNP\/master\/ppnp.jpg) photo taken from [this](https:\/\/github.com\/benedekrozemberczki\/APPNP)"}}