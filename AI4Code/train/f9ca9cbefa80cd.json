{"cell_type":{"a594f8f9":"code","300cb868":"code","25e266de":"code","35d26cfc":"code","8e96cbea":"code","58680aeb":"code","fa5b30ec":"code","1bf3fcbe":"code","60420edf":"code","9ba37995":"code","c8ef0dd7":"code","42725a87":"code","3352ad9f":"code","b747193d":"code","b9808285":"code","4a7cbfcd":"code","6b57a47a":"markdown","61c227d9":"markdown","dc4257e0":"markdown","c1636751":"markdown","38a7b66b":"markdown","b8cb3809":"markdown","bda567ac":"markdown","f3c0d466":"markdown","eb03c853":"markdown","396ae1d2":"markdown","1c7c93a6":"markdown","5206d9be":"markdown"},"source":{"a594f8f9":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt # plotting\nimport seaborn as sns\nimport tensorflow\n\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.random import set_random_seed\nfrom tensorflow.keras.applications import VGG16\nfrom tensorflow.keras.layers import Flatten, Dense, Dropout\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.optimizers import RMSprop, SGD, Adam, Nadam\nfrom tensorflow.keras.regularizers import l1, l2, L1L2\nfrom tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n\nset_random_seed(0)\nnp.random.seed(0)","300cb868":"path = \"..\/input\/split-garbage-dataset\/split-garbage-dataset\"","25e266de":"train_datagen = ImageDataGenerator(\n        rescale = 1.\/255,\n        rotation_range = 20,\n        width_shift_range = 0.2,\n        height_shift_range = 0.2,\n        horizontal_flip = True,\n        vertical_flip = True,\n        fill_mode='nearest'\n)\nvalidation_datagen = ImageDataGenerator(\n        rescale = 1.\/255\n)\ntest_datagen = ImageDataGenerator(\n        rescale = 1.\/255\n)","35d26cfc":"img_shape = (224, 224, 3) # default values\n\ntrain_batch_size = 256\nval_batch_size = 32\n\ntrain_generator = train_datagen.flow_from_directory(\n            path + '\/train',\n            target_size = (img_shape[0], img_shape[1]),\n            batch_size = train_batch_size,\n            class_mode = 'categorical',)\n\nvalidation_generator = validation_datagen.flow_from_directory(\n            path + '\/valid',\n            target_size = (img_shape[0], img_shape[1]),\n            batch_size = val_batch_size,\n            class_mode = 'categorical',\n            shuffle=False)\n\ntest_generator = test_datagen.flow_from_directory(\n            path + '\/test',\n            target_size = (img_shape[0], img_shape[1]),\n            batch_size = val_batch_size,\n            class_mode = 'categorical',\n            shuffle=False,)","8e96cbea":"vgg = VGG16(weights = 'imagenet',\n              include_top = False,\n              input_shape = img_shape)","58680aeb":"# Freeze the layers except the last 3 layers\nfor layer in vgg.layers[:-3]:\n    layer.trainable = False","fa5b30ec":"# Create the model\nmodel = Sequential()\n \n# Add the vgg convolutional base model\nmodel.add(vgg)\n \n# Add new layers\nmodel.add(Flatten())\nmodel.add(Dense(1024, activation='relu'))\nmodel.add(Dropout(0.2))\nmodel.add(Dense(6, activation='softmax'))","1bf3fcbe":"model.summary()","60420edf":"# Compile the model\nmodel.compile(loss='categorical_crossentropy',\n              optimizer=Nadam(lr=1e-4),\n              metrics=['acc'])","9ba37995":"# Train the model\nes = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=10)\nmc = ModelCheckpoint('VGG16 Garbage Classifier.h5', monitor='val_acc', mode='max', verbose=1, save_best_only=True)\n\nhistory = model.fit_generator(\n    train_generator,\n    steps_per_epoch=train_generator.samples\/train_generator.batch_size ,\n    epochs=30,\n    validation_data=validation_generator,\n    validation_steps=validation_generator.samples\/validation_generator.batch_size,\n    verbose=0,\n    callbacks = [es, mc],)","c8ef0dd7":"train_acc = history.history['acc']\nval_acc = history.history['val_acc']\ntrain_loss = history.history['loss']\nval_loss = history.history['val_loss']\n\nepochs = range(1, len(train_acc) + 1)\n\nplt.plot(epochs, train_acc, 'b*-', label = 'Training accuracy')\nplt.plot(epochs, val_acc, 'r', label = 'Validation accuracy')\nplt.title('Training and validation accuracy')\nplt.legend()\n\nplt.figure()\n\nplt.plot(epochs, train_loss, 'b*-', label = 'Training loss')\nplt.plot(epochs, val_loss, 'r', label = 'Validation loss')\nplt.title('Training and validation loss')\nplt.legend()\n\nplt.show()","42725a87":"data = np.load('..\/input\/test-data\/test_data.npz')\nx_test, y_test = data['x_test'], data['y_test']\ny_pred = model.predict(x_test)","3352ad9f":"acc = np.count_nonzero(np.equal(np.argmax(y_pred, axis=1), np.argmax(y_test, axis=1)))\/x_test.shape[0]\nprint(acc)","b747193d":"confusion_matrix = np.zeros((6,6), dtype=np.uint8)\nper_class_acc = np.zeros(6)\nfor i in range(y_test.shape[1]):\n    idxs = np.argmax(y_test, axis=1)==i\n    this_label = y_test[idxs]\n    num_samples_per_class = np.count_nonzero(idxs)\n    one_hot = tensorflow.one_hot(np.argmax(model.predict(x_test[idxs]), axis=1), depth=6).eval(session=tensorflow.Session())\n    confusion_matrix[i] = np.sum(one_hot, axis=0)\n    per_class_acc[i] = confusion_matrix[i,i]\/num_samples_per_class","b9808285":"LABELS=['Cardboard', 'Glass', 'Metal', 'Paper', 'Plastic', 'Trash']\nplt.figure(figsize=(10,8))\nsns.heatmap(confusion_matrix, xticklabels=LABELS, yticklabels=LABELS, annot=True, fmt = 'd')\nplt.title('Confusion Matrix')\nplt.ylabel('True class')\nplt.xlabel('Predicted class')\nplt.show()","4a7cbfcd":"dict(zip(LABELS, per_class_acc))","6b57a47a":"# Data preprocessing","61c227d9":"## Fine-tuning","dc4257e0":"## Test set accuracy","c1636751":"# Building model","38a7b66b":"# Train the model","b8cb3809":"# Model evaluation","bda567ac":"## Confusion Matrix and per-class accuracy","f3c0d466":"### Model definition","eb03c853":"# Prediction on test set","396ae1d2":"### Freeze VGG layers","1c7c93a6":"## Pretrained Convolutional Base (VGG16)","5206d9be":"### Training history"}}