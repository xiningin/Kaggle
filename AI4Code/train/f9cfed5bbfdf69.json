{"cell_type":{"f1a7bda0":"code","8e396926":"code","04058ef4":"code","b9692409":"code","b226e3eb":"code","af22aace":"code","725fa180":"code","8a7d85c4":"code","e929cd80":"code","55d76941":"code","a4275eb4":"code","b6027f5e":"code","6a163a29":"code","7ff8e3de":"code","0446992e":"code","6ab73296":"code","2309f197":"code","513b1844":"code","cc9c978d":"code","eb05c801":"code","f79f3ca3":"code","5afdb5eb":"code","230fc4cf":"markdown","22ca3ca5":"markdown","1879c049":"markdown","26cccda7":"markdown","e9ed8a6e":"markdown","e4eca9d4":"markdown","3df04f85":"markdown","b4e1a69d":"markdown","72dd1739":"markdown","0b655de6":"markdown","f607bdb6":"markdown","d0c14ee8":"markdown"},"source":{"f1a7bda0":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","8e396926":"# import essentials\nimport numpy as np\nimport pandas as pd\n\n# set randomness for reproducability\nseed = 42\nnp.random.seed(seed)","04058ef4":"# get our data\ndata = pd.read_csv(\"..\/input\/mushroom-classification\/mushrooms.csv\")\n# give it a quick overview\nprint(data.describe())","b9692409":"# check data types of our data\nprint('\\ndata types of our dataset')\nprint(str(data.dtypes) + '\\n')","b226e3eb":"# grab our column names to iterate over\ncolumns = data.keys()\n# changing almost all to categorical variables\n# will make a few exceptions\ncolumn_exceptions = ['class','bruises']","af22aace":"newData = pd.DataFrame()\nnewData['class'] = (data['class'] == 'p')\nnewData['class'] = newData['class'].astype(int)\nnewData['bruises'] = data['bruises'] == 't'","725fa180":"data.drop(columns=['class','bruises'])\ndummy_data = pd.get_dummies(data,drop_first=True, dtype='int')\nnewData = pd.concat([newData['class'],newData['bruises'],dummy_data],axis=1)\ndata = newData","8a7d85c4":"# check dtypes again\nprint('\\nCleaned data types')\nprint(data.dtypes)\n","e929cd80":"from sklearn.model_selection import train_test_split\n\ndata_independent = data.iloc[:, 1:]  # X\ndata_dependent = data.iloc[:, 0] # y\nX_train, X_test, y_train, y_test = train_test_split(\n    data_independent,data_dependent.values,shuffle=True,\n    random_state=seed, test_size=.2, stratify=data_dependent)","55d76941":"# import scoring metrics for evaluation\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import KFold","a4275eb4":"\n# our cross validation strategy\ndef log_loss(model, X=X_train, y=y_train, _scoring='neg_log_loss') :\n    _kfold = KFold(n_splits=5)\n    _score = -cross_val_score(model, X, y, cv=_kfold, scoring=_scoring)\n    return _score","b6027f5e":"# and the ratio that we predicted correctly\ndef final_accuracy(model,_X_train=X_train,_y_train=y_train,\n                   _X_test=X_test,_y_test=y_test) :\n    model.fit(_X_train,_y_train)\n    _y_hat = model.predict(_X_test)\n    _final_score = np.sum(_y_hat == _y_test) \/ len(_y_test)\n    return _final_score","6a163a29":"# now make linear estimations and evaluate if we need\n# a more complex model\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier","7ff8e3de":"\nprint('\\nLoading and scoring models...')","0446992e":"logit = LogisticRegression(random_state=seed)\nsvc = SVC(probability=True,random_state=seed,kernel='rbf')\nforest = RandomForestClassifier(random_state=seed)","6ab73296":"# using log loss, what score do we get on our test set?\nscore = log_loss(logit)\nprint('\\nLogit score: {:.4f} ({:.4f})'\n      .format(score.mean(),score.std()))\nscore = log_loss(svc)\nprint('\\nSVC score: {:.4f} ({:.4f})'\n      .format(score.mean(),score.std()))\nscore = log_loss(forest)\nprint('\\nRandom Forest score: {:.4f} ({:.4f})'\n      .format(score.mean(),score.std()))","2309f197":"\naccuracy = final_accuracy(logit)\nprint('Logit final accuracy: {:.4f}'.format(accuracy))\naccuracy = final_accuracy(svc)\nprint('\\nSVC final accuracy: {:.4f}'.format(accuracy))\naccuracy = final_accuracy(forest)\nprint('\\nRandom Forest final accuracy: {:.4f}'.format(accuracy))","513b1844":"logit.fit(X_train,y_train)\npredictions = logit.predict(X_test)","cc9c978d":"\nfrom sklearn.decomposition import PCA","eb05c801":"# create our PCA and fit it to the data\npca = PCA(n_components=2, random_state=seed)\npca.fit(X_test)\nX_test_pca = pca.transform(X_test)","f79f3ca3":"# create our PCA dataframes\npca_df = pd.DataFrame(data=X_test_pca, columns=['PCA 1', 'PCA 2'])\ny_test_series = pd.DataFrame(y_test, columns=['target'])\nfinal_df = pd.concat([pca_df, y_test_series],axis=1)","5afdb5eb":"# import our plotting methods\nimport matplotlib.pyplot as plt\n\n# create our figure\nfig :plt.Figure = plt.figure(figsize=(8,8))\nax = fig.add_subplot(1,1,1)\nax.set_xlabel('PCA 1')\nax.set_ylabel('PCA 2')\nax.set_title('PCA Graph', fontsize=20)\ntargets = [0, 1]\ncolors = ['b','r']\n\nfor target, color in zip(targets, colors) :\n    kept_indicies = predictions == target\n    ax.scatter(final_df.loc[kept_indicies, 'PCA 1'],\n               final_df.loc[kept_indicies, 'PCA 2'],\n               c = color,\n               s= 50)\nax.legend(['Non-Poisonous','Poisonous'])\nax.grid()\nplt.show()","230fc4cf":"# next we split into train \/ test data for evaluation","22ca3ca5":"# define models","1879c049":"# convert our target variable \"class\" to False for\n# non-poisonous and True for poisonous\n# do the same for bruises\n","26cccda7":"# Success!","e9ed8a6e":"# classify mushrooms as either poisonous or non-poisonous\n# using data from https:\/\/www.kaggle.com\/uciml\/mushroom-classification","e4eca9d4":"# extremely accurate scores on all. What's our prediction rate?","3df04f85":"# now perform PCA to view how our machine \n# seperates the data","b4e1a69d":"# use the documentation to clean our data","72dd1739":"# change all non-exception columns to categorical vars\n","0b655de6":"# 100% accuracy on several methods! Wow, machines are great mycologists!\n# let's analyze the data to get some intuitions on our results\n# let's use our logostic regression since it got everything right\n# while being quite fast","f607bdb6":"# define our custom scoring methods","d0c14ee8":"# unfortunately our variables are all classified as\n# objects with string names, but we need most of them as categories"}}