{"cell_type":{"68bf2ef8":"code","50a97b24":"code","c0d65744":"code","7c5845b5":"code","c760dd1b":"code","c4f5114a":"code","913d901b":"code","8fc164c0":"code","d76902a2":"code","80b7a077":"code","c93003cc":"code","154e092e":"code","9022bf27":"code","9b907eb0":"code","2579f8a1":"code","a22fec49":"code","30a8d45e":"code","10f02250":"code","18be4b24":"code","9c327ae0":"code","36ce7e9e":"code","e47947ac":"code","da27ee1c":"code","1d6144a9":"code","3d456cbb":"code","802928f2":"code","fadfa028":"code","0b766174":"code","68789114":"code","0773832f":"code","af29f7c6":"code","62e53677":"code","60315c6b":"code","94419268":"code","bbd37a14":"code","67ad1a6f":"code","117365f8":"code","8533dda5":"code","5541d1af":"code","67c32f83":"code","a48fa778":"code","01be0ceb":"code","20449e02":"code","7a413dbf":"code","36de46c1":"code","24e47a6b":"code","cadbdbba":"code","7b19f040":"code","82861691":"code","f360855c":"code","e42fa8ef":"code","0690c40c":"markdown","3709e28d":"markdown","20488202":"markdown","f9cc80f2":"markdown","e5a42de7":"markdown","24ecb392":"markdown","cebe678a":"markdown","ef4c331e":"markdown","b562bf71":"markdown","2d485150":"markdown","4575d223":"markdown","cb23dc1c":"markdown","cda684e3":"markdown","87bafd6d":"markdown","a0fadb56":"markdown","b85c17c5":"markdown","204093e1":"markdown","6a612840":"markdown","653757fa":"markdown","5197318a":"markdown","23951566":"markdown","608bf74a":"markdown","0d97978c":"markdown","71056a20":"markdown","d8e9d550":"markdown"},"source":{"68bf2ef8":"import numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\nplt.style.use('ggplot')\nfrom statsmodels.tsa.stattools import adfuller\nfrom statsmodels.graphics.tsaplots import plot_acf,plot_pacf\nfrom statsmodels.tsa.arima_model import ARIMA\nfrom statsmodels.tsa.statespace.sarimax import SARIMAX\nfrom statsmodels.tsa.seasonal import seasonal_decompose \nimport warnings\nwarnings.filterwarnings('ignore')\nfrom sklearn.metrics import make_scorer \nimport math\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import train_test_split,GridSearchCV\nfrom xgboost import XGBRegressor\nfrom sklearn.preprocessing import MinMaxScaler\nfrom keras.layers import LSTM,Dropout,Dense,Input\nfrom keras.models import Sequential,Model\nfrom keras.callbacks import EarlyStopping,ReduceLROnPlateau\nimport os\nimport xgboost as xgb\nfrom fbprophet import Prophet\nfrom keras import backend\nprint(os.listdir(\"..\/input\"))","50a97b24":"data=pd.read_csv('..\/input\/GOOGL_2006-01-01_to_2018-01-01.csv',parse_dates=['Date'],index_col='Date')","c0d65744":"data.head()","7c5845b5":"data.shape","c760dd1b":"data.info()","c4f5114a":"plt.subplots(2,2,figsize=(10,10))\nplt.subplot(2,2,1)\ndata['Open'].plot()\nplt.title('Open')\nplt.subplot(2,2,2)\ndata.Close.plot()\nplt.title('Close')\nplt.subplot(2,2,3)\ndata.High.plot()\nplt.title('High')\nplt.subplot(2,2,4)\ndata.Low.plot()\nplt.title('Low')\nplt.tight_layout()\nplt.show()","913d901b":"#Let us create model for predicting Low. \n\nlow=data['Low'].asfreq('M')\nlow.dropna(inplace=True)","8fc164c0":"#Using rolling mean and rolling standard deviation.\n# I am using 'Low' values from previous 7 days (1 week) for rolling mean\n\ndef plot_rolling(data,window=7):\n    rolling_mean=data.rolling(window).mean()\n    rolling_std=data.rolling(window).std()\n    plt.figure(figsize=(10,5))\n    plt.plot(data,label='original',color='red')\n    plt.plot(rolling_mean,label='rolling mean',color='black')\n    plt.plot(rolling_std,label='rolling std',color='green')\n    plt.legend(loc='best')\n    plt.show()","d76902a2":"plot_rolling(low)","80b7a077":"#Lets check stationarity using Augmented Dickey-Fuller test\n\ndef test_stationarity(data):\n    result=adfuller(data)\n    print('ADF : ' + str(result[0]))\n    print('pvalue : ' + str(result[1]))\n    print('Number of lags used : ' + str(result[2]))\n    print('Number of obs used : ' + str(result[3]))\n    print('Critical value at 1% :' + str(result[4]['1%']))\n    print('Critical value at 5% :' + str(result[4]['5%']))\n    print('Critical value at 10% :' + str(result[4]['10%']))","c93003cc":"test_stationarity(low)","154e092e":"#Rolling mean subtraction from original time series\nwindow=2\nlow_rolling=low.rolling(window).mean()\nlow_rolling_diff=low-low_rolling\nlow_rolling_diff=low_rolling_diff.dropna()\n\ntest_stationarity(low_rolling_diff)\nplot_rolling(low_rolling_diff)","9022bf27":"# Subtraction of shifted time series from original time series\nshift=1\nlow_shifted=low.shift(shift)\nlow_shift_diff=low-low_shifted\nlow_shift_diff=low_shift_diff.dropna()\ntest_stationarity(low_shift_diff)\nplot_rolling(low_shift_diff)","9b907eb0":"def plot_acf_pacf(data,lags=50):\n    plot_acf(low_shift_diff,lags=lags)\n    plot_pacf(low_shift_diff,lags=lags)\n    plt.show()  ","2579f8a1":"plot_acf_pacf(low_shift_diff,lags=10)","a22fec49":"def fit(data,d=0,p=2,q=2):\n    model=ARIMA(data,(p,d,q))\n    model_fit=model.fit(disp=0)\n    fitted_values=model_fit.fittedvalues\n    score=math.sqrt(mean_squared_error(data,fitted_values))\n    return fitted_values,score\n        \ndef plot_values(predictions,data,score):\n    plt.figure(figsize=(10,5))\n    plt.plot(data,label='original')\n    plt.plot(predictions,label='Fitted values',color='black')\n    plt.title('RMSE : '+str(score))\n    plt.legend(loc='best')\n    plt.show()  ","30a8d45e":"fitted_values,score=fit(low_shift_diff)\nplot_values(fitted_values,low_shift_diff,score)","10f02250":"def original_scale(fitted_values,data):\n    fitted_values_cumsum=fitted_values.cumsum()\n    new_series=pd.Series(data[0],index=data.index)\n    original_scale_fit=new_series.add(fitted_values_cumsum,fill_value=0)\n    return original_scale_fit","18be4b24":"original_scale_fit=original_scale(fitted_values,low)\noriginal_score=math.sqrt(mean_squared_error(original_scale_fit,low))\nplot_values(original_scale_fit,low,original_score)","9c327ae0":"fitted_values_ar,score_ar=fit(low_shift_diff,p=2,d=0,q=0)\nplot_values(fitted_values_ar,low_shift_diff,score_ar)","36ce7e9e":"original_scale_fit_ar=original_scale(fitted_values_ar,low)\noriginal_score_ar=math.sqrt(mean_squared_error(original_scale_fit_ar,low))\nplot_values(original_scale_fit_ar,low,original_score_ar)","e47947ac":"fitted_values_ma,score_ma=fit(low_shift_diff,p=0,d=0,q=2)\nplot_values(fitted_values_ma,low_shift_diff,score_ma)","da27ee1c":"original_scale_fit_ma=original_scale(fitted_values_ma,low)\noriginal_score_ma=math.sqrt(mean_squared_error(original_scale_fit_ma,low))\nplot_values(original_scale_fit_ma,low,original_score_ma)","1d6144a9":"low_date=data['Low']\nlow_date=pd.DataFrame(low_date,columns=['Low'])\nlow_date['Day']=low_date.index.day\nlow_date['Month']=low_date.index.month\nlow_date['Year']=low_date.index.year\nlow_date.reset_index(drop=True,inplace=True)","3d456cbb":"train_size=0.8\ntrain_index=int(len(low_date)*train_size)\ntrain=low_date.iloc[:train_index,:]\nval=low_date.iloc[train_index:,:]\ntrain_X=train.drop('Low',axis=1)\ntrain_y=train['Low']\nval_X=val.drop('Low',axis=1)\nval_y=val['Low']","802928f2":"xgb_model=XGBRegressor(random_state=3)\nxgb_model.fit(train_X,train_y)\npred=xgb_model.predict(val_X)\nrmse=math.sqrt(mean_squared_error(val_y,pred))\nval_size=val_X.shape[0]\nprint('RMSE for validation : '+ str(rmse\/val_size)) #Normarlized with the length of validation set","fadfa028":"train_pred=xgb_model.predict(train_X)\ntrain_rmse=math.sqrt(mean_squared_error(train_y,train_pred))\ntrain_size=train_X.shape[0]\nprint('RMSE for train : ' + str(train_rmse\/train_size)) #Normalized with the length of train set","0b766174":"xgb1=XGBRegressor(random_state=5)\nparams={'n_estimators':np.arange(100,600,100),\n       'learning_rate':np.arange(0.01,0.11,0.03),\n       'gamma':np.arange(0,11,2),\n       'subsample':[0.8]}\n\ngrid=GridSearchCV(xgb1,params,cv=5,scoring='neg_mean_squared_error',verbose=0)\ngrid.fit(train_X,train_y)\n","68789114":"print(grid.best_params_)\nprint(grid.best_score_)","0773832f":"best_model=grid.best_estimator_\nbest_model.fit(train_X,train_y)\nbest_pred_train=best_model.predict(train_X)\ntrain_rmse=math.sqrt(mean_squared_error(train_y,best_pred_train))\nprint('RMSE for training :' + str(train_rmse\/train_size)) #Normalize with the length of train set\nbest_pred_val=best_model.predict(val_X)\nval_rmse=math.sqrt(mean_squared_error(val_y,best_pred_val))\nprint('RMSE for validation :' + str(val_rmse\/val_size)) #Normalize with the length of val set","af29f7c6":"low_df=pd.DataFrame(low).reset_index()\nlow_df=low_df.rename(columns={'Date':'ds','Low':'y'})","62e53677":"train_size=0.8\ntrain_index=int(len(low_df)*train_size)\ntrain=low_df.iloc[:train_index,:]\nval=low_df.iloc[train_index:,:]\nval_X=val.drop('y',axis=1)\nval_y=val['y']","60315c6b":"# set the uncertainty interval to 95%\nprophet_model=Prophet(interval_width=0.95)\nprophet_model.fit(train)\npro_predictions=prophet_model.predict(val_X)\n","94419268":"pro_rmse_val=math.sqrt(mean_squared_error(val_y,pro_predictions['yhat']))\nprint('RMSE for Prophet validation : ' + str(pro_rmse_val\/len(val_y))) # #Normalize with the length of val set","bbd37a14":"prophet_model.plot(pro_predictions,uncertainty=True)\nplt.show()","67ad1a6f":"prediction_ts=pro_predictions.loc[:,['ds','yhat']].set_index('ds')\nval_ts=val.set_index('ds')","117365f8":"plt.figure(figsize=(10,5))\nplt.plot(val_ts,label='original')\nplt.plot(prediction_ts,label='prediction',color='green')\nplt.legend(loc='best')\nplt.show()","8533dda5":"low=data['Low'].values\nlow=np.array(low).reshape(-1,1)","5541d1af":"train_size=0.8\ntrain_index=int(len(low)*train_size)\ntrain=low[:train_index]\nval=low[train_index:]","67c32f83":"scl=MinMaxScaler(feature_range=(0,1))\ntrain_scl=scl.fit_transform(train)\nval_scl=scl.transform(val)","a48fa778":"def prepare_dataset(data,window=30):\n    X=[]\n    Y=[]\n    for i in range(len(data)-window):\n        dummy_X=data[i:i+window]\n        dummy_y=data[window+i]\n        X.append(dummy_X)\n        Y.append(dummy_y)\n    return np.array(X),np.array(Y)\n\n    ","01be0ceb":"train_X,train_y=prepare_dataset(train_scl)\nval_X,val_y=prepare_dataset(val_scl)","20449e02":"def rmse(true_y,pred_y):\n    return backend.sqrt(backend.mean(backend.square(true_y-pred_y),axis=1))","7a413dbf":"window=30\ninput_layer=Input(shape=(window,1))\nx=LSTM(4)(input_layer)\noutput=Dense(1,activation='linear')(x)\nlstm_model=Model(input_layer,output)\nlstm_model.compile(loss='mean_squared_error',optimizer='adam',metrics=[rmse])","36de46c1":"result=lstm_model.fit(x=train_X,y=train_y,epochs=200,validation_data=[val_X,val_y])","24e47a6b":"lstm_train_rmse=result.history['rmse'][-1]\nlstm_val_rmse=result.history['val_rmse'][-1]\nprint('RMSE for train :' + str(lstm_train_rmse\/len(train_y)))  #Normalize with the length of train set\nprint('RMSE for validation :' + str(lstm_train_rmse\/len(val_y)))  #Normalize with the length of val set","cadbdbba":"plt.subplots(2,1,figsize=(10,5))\nplt.subplot(2,1,1)\nplt.plot(result.history['rmse'],label='training')\nplt.plot(result.history['val_rmse'],label='validation',color='green')\nplt.legend(loc='best')\nplt.subplot(2,1,2)\nplt.plot(result.history['loss'],label='training')\nplt.plot(result.history['val_loss'],label='validation',color='green')\nplt.legend(loc='best')\n","7b19f040":"est=EarlyStopping(monitor='val_loss',patience=20,restore_best_weights=True)\nrlr=ReduceLROnPlateau(monitor='val_loss',patience=10,min_lr=0.0001,factor=0.1)\ncall_backs=[est,rlr]","82861691":"result_callbacks=lstm_model.fit(x=train_X,y=train_y,epochs=200,validation_data=[val_X,val_y],callbacks=call_backs)","f360855c":"lstm_train_rmse=result_callbacks.history['rmse'][-1]\nlstm_val_rmse=result_callbacks.history['val_rmse'][-1]\nprint('RMSE for train :' + str(lstm_train_rmse\/len(train_y)))  #Normalize with the length of train set\nprint('RMSE for validation :' + str(lstm_train_rmse\/len(val_y))) #Normalize with the length of val set","e42fa8ef":"plt.subplots(2,1,figsize=(10,5))\nplt.subplot(2,1,1)\nplt.plot(result_callbacks.history['rmse'],label='training')\nplt.plot(result_callbacks.history['val_rmse'],label='validation',color='green')\n\nplt.subplot(2,1,2)\nplt.plot(result_callbacks.history['loss'],label='training')\nplt.plot(result_callbacks.history['val_loss'],label='validation',color='green')\n","0690c40c":"**Data preparation for LSTM**","3709e28d":"Correlation represents the strength of relationship between 2 variables. We are using Pearson's correlation coefficient to determine correlation. It ranges from -1 to 1. 0 signifies no correlation. Autocorrelation is correlation between an observation in time series and a lagged observation in the same time series. Autocorrelation takes into account the direct correlations as well as indirect correlations due to the intervening observations.Partial Correlation function does not take indirect correlations into account and we can know direct correlation between two observations in time series. Source : https:\/\/machinelearningmastery.com\/gentle-introduction-autocorrelation-partial-autocorrelation\/","20488202":"ADF value is lesser than Critical value at 1%. That means we are more than 99% sure that this is stationary time series. Even its rolling mean and rolling std are more or less constant","f9cc80f2":"We will use ARIMA model for prediction. \n* **AR: Autoregression**. A model that uses the dependent relationship between an observation and some number of lagged observations.\n* **I: Integrated**. The use of differencing of raw observations (e.g. subtracting an observation from an observation at the previous time step) in order to make the time series stationary.\n* **MA: Moving Average**. A model that uses the dependency between an observation and a residual error from a moving average model applied to lagged observations.\nARIMA has three parameters (p,d,q).\n* p: The number of lag observations included in the model, also called the lag order.\n* d: The number of times that the raw observations are differenced, also called the degree of differencing.\n* q: The size of the moving average window, also called the order of moving average.\n\nSource : https:\/\/machinelearningmastery.com\/arima-for-time-series-forecasting-with-python\/\n\nI will follow you through a method to choose (p,d,q)","e5a42de7":"**AR MODEL**","24ecb392":"Let us check whether the time series is stationary or not. Stationary time series means constant mean and constant standard deviation and autocovariance that does not depend on time. So, we can check whether mean and standard deviation is constant.","cebe678a":"**Preprocessing for XGBoost**","ef4c331e":"Let us use callbacks.","b562bf71":"Hello there. In this kernel, I have used ARIMA model and Prophet for time series modelling. Then I went on to use XGBoost, followed by LSTM. I hope you enjoy this kernel.** Please upvote if you like the kernel**. Your upvotes will motivate me to code more. ","2d485150":"**MA MODEL**","4575d223":"2 methods to make time series stationary. First, you can subtract the rolling mean of time series from the the time series itself. Second, you can shift the time series and subtract it from the original time series itself. ","cb23dc1c":"Now we have to convert it in original scale. We can convert so by cumulative addition of fitted_vallues followed by addition of whole series to another series of base number.","cda684e3":"Source : https:\/\/www.digitalocean.com\/community\/tutorials\/a-guide-to-time-series-forecasting-with-prophet-in-python-3","87bafd6d":"We see that the ADF value is much higher than even Critival value at 10%. This is means there is not even 90% chance that this is stationary time series.","a0fadb56":"Stationarity can also be checked with Augmented Dickey Fuller test.  ","b85c17c5":"I guess it is still overfitting, but better than previous. You can try to optimize the parameters further. ","204093e1":"**Prophet**","6a612840":"Similar result is obtained. You can use any one of the methods, just make sure that the time series thus obtained is stationary.","653757fa":"If value of correlation coefficient is above the red band, then it is considered to be significant. The value of p is lag value just after which the autocorrelations becomes insignificant for the first time. The value of q is lag value just after which the partial autocorrelations becomes insignificant for the first time. So, here p=2 and q=2. This is not a hard and fast rule. You can play with values of p and q around 2 and see what changes occur. \n","5197318a":"**ARIMA MODEL**","23951566":"**Please upvote if you liked the kernel.** Your suggestions are always welcomed. \n\n\nThings you can do further :\n* Optimize ARIMA parameters\n* Optimize XGBoost hyperparameters\n* Update the LSTM network. \n* Use different callbacks","608bf74a":"**Importing modules**","0d97978c":"**LSTM **","71056a20":"We can see that the model is overfitting. You can use lower learning rate, tune number of estimators and use gamma for regularization to reduce overfitting. I am using GridSearch to find best parameters. I am not sure whether this approach is correct or not because in GridSearch, we do crossvalidation and so, the order in the data is lost. Please provide your suggestions in the comments. ","d8e9d550":"I shall create model for data frequency of 1 Month. I have chosen the feature 'LOW'. There is no particular reason for choosing frequency of 1 Month and the 'LOW' feature."}}