{"cell_type":{"0366099e":"code","372260a8":"code","db9082bd":"code","be45ecac":"code","2fb220b0":"code","77241cdb":"code","888b8892":"code","a6781dcf":"code","69b2a946":"code","8df4409a":"code","606fb216":"code","c15e9237":"code","e0a37e4c":"code","51fc2b5b":"markdown","8514125b":"markdown","38a32bdf":"markdown","bf2a3c20":"markdown","dde4fa5f":"markdown","8c097bfd":"markdown","6413a2ca":"markdown","c44de21b":"markdown","14ec169a":"markdown"},"source":{"0366099e":"#Machine Learning to predict titanic survivors\nimport pandas as pd\nimport numpy as np\n\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\n\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import GridSearchCV\nimport sklearn.metrics\n\n#Suppress all warnings (risky business!)\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n#getting system path\nimport os\npath1 = os.getcwd()\ntrain_path = path1.rsplit('\\\\',1)[0]+'\\\\Original_Data\\\\train.csv'\ntest_path = path1.rsplit('\\\\',1)[0]+'\\\\Original_Data\\\\test.csv'","372260a8":"#reading csv\ndf_raw = pd.read_csv(\"..\/input\/titanic\/train.csv\")\ndf_test_raw = pd.read_csv(\"..\/input\/titanic\/test.csv\")","db9082bd":"# paramters\ntest_train_split_size = 0.2","be45ecac":"df1 = df_raw.copy()\ndf1.head()","2fb220b0":"df1.isna().sum()","77241cdb":"def get_unique_list(df1,col_name,threshold):\n    #function to get list of unique items in a columns that has value count above threshold percentage\n    unique_list = df1[col_name].value_counts().index.tolist()\n    for index,values in df1[col_name].value_counts().items():\n        if values<=(df1.shape[0])*threshold:\n            unique_list.remove(index)\n            \n    return(unique_list)","888b8892":" def data_cleaner(df1):\n    df1.Age = df1[['Pclass','Sex','Age']].groupby(['Sex','Pclass']).transform(lambda x: x.fillna(x.mean()))\n    df1.Fare = df1[['Pclass','Fare']].groupby(['Pclass']).transform(lambda x: x.fillna(x.mean()))\n    df1.Embarked.fillna(method='ffill',inplace=True)\n    \n    df1['Has_cabin'] = np.where(df1.Cabin.notnull(),1,0)\n    \n    #code to get title from name\n    liste = df1.Name.str.split(',').tolist()\n    liste = [e[1] for e in liste]\n    liste = pd.Series(liste).str.split(\".\")\n    liste = [e[0] for e in liste]\n    df1['Prefix'] = liste\n\n    #code to merge similar titles for instance: 'Mlle.' is 'Miss.' in French\n    df1['Prefix'] = df1['Prefix'].replace({' Mlle':' Miss',' Mme':' Mrs',' Ms':' Miss'})\n \n    unique_list = get_unique_list(df1,'Prefix',0.03)\n    df1['Prefix'][~(df1['Prefix'].isin(unique_list))] = 'Rare'\n    \n    #adding a new column - 'relatives'\n    df1['relatives'] = df1['SibSp'] + df1['Parch']\n    #new column - 'Alone'\n    df1['Alone'] = np.where(df1.relatives==0,1,0)\n    df1.drop(['PassengerId','Name','Ticket','SibSp','Cabin'],axis=1,inplace=True)\n\n    df1 = pd.get_dummies(df1,columns=['Sex','Pclass','Embarked','Prefix'],drop_first=True)\n    \n    \n    return(df1)","a6781dcf":"df1 = df_raw.copy()\ndf_test = df_test_raw.copy()\n\ndf1 = data_cleaner(df1)\ndf1_test = data_cleaner(df_test)","69b2a946":"df1.head()","8df4409a":"X_train, X_test, y_train, y_test = train_test_split(df1.drop(['Survived'],axis=1), df1.Survived, test_size=test_train_split_size)","606fb216":"lr = LogisticRegression()\nparams = {'penalty':['l1','l2'],'C':[0.001,0.01,0.1,1,10,100,1000]}\n\ngslr = GridSearchCV(lr,params,cv=10,scoring='accuracy')\ngslr.fit(X_train,y_train)\n\nprint(\"best parameter = \", gslr.best_params_)\nprint(\"best score = \",gslr.best_score_)\n\nprint(\"\\naccuracy score on test set:\", sklearn.metrics.accuracy_score(y_test,gslr.predict(X_test)))","c15e9237":"#shows importance of each feature\nfor i in zip(gslr.best_estimator_.coef_[0].tolist(),X_train.columns.tolist()):\n    print(i)","e0a37e4c":"# path1 = os.getcwd()+'\\\\lr_v10.csv'\npd.DataFrame(data={'PassengerId':df_test_raw.PassengerId,'Survived':gslr.predict(df1_test).astype(int)}).to_csv('titanic_LR',index=False)","51fc2b5b":"Creating functions to help with data cleaning - since we need to clean the train data as well as the test data","8514125b":"# Prediction","38a32bdf":"Submitting the Logistic Regression score!","bf2a3c20":"Works perfect, Has a good CV accuracy score (0.834) and a similar test set score.","dde4fa5f":"Got a accuracy of 0.789 on Kaggle!","8c097bfd":"## Data cleaning","6413a2ca":"#### Logistic Regression","c44de21b":"![image.png](attachment:image.png)","14ec169a":"Looks like Age & Cabin has missing values!"}}