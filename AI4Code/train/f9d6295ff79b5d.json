{"cell_type":{"3eb3bea0":"code","7db0a8d3":"code","f6dd0f8c":"code","58295732":"code","e723f1e3":"code","67362800":"code","dce0b7ac":"code","ad73faf0":"code","a9d7ed2e":"code","c1cd97f0":"code","afe45433":"code","ab4899a1":"code","8ac09725":"code","7c2195f0":"code","f76da1f3":"code","d52fdcfa":"code","3012c70b":"code","d84f1cd3":"code","0090c5a3":"code","12d7eed4":"code","7df50412":"code","e658a018":"code","0c499703":"markdown","db7666ce":"markdown","d62e806d":"markdown","0bc5cd6e":"markdown","2d64a433":"markdown","7d9000a8":"markdown","12431e9d":"markdown","5eee3ae6":"markdown","73106ac1":"markdown","0ff00e95":"markdown","3a88370a":"markdown","2967c15c":"markdown","5cb5c468":"markdown","5f487360":"markdown","83c151f3":"markdown","18d9978e":"markdown"},"source":{"3eb3bea0":"import pandas as pd\nimport numpy as np\nimport re\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import accuracy_score, f1_score, recall_score, precision_score, confusion_matrix, roc_auc_score","7db0a8d3":"import tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras.layers import *\nfrom tensorflow.keras.metrics import *\nfrom tensorflow.keras.losses import *\nfrom tensorflow.keras.models import Model, Sequential\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.utils import plot_model\nfrom tensorflow.keras.callbacks import ModelCheckpoint\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences","f6dd0f8c":"df_train = pd.read_csv('..\/input\/imdb-dataset\/train.csv', usecols = ['review','sentiment'])\ndf_val = pd.read_csv('..\/input\/imdb-dataset\/val.csv', usecols = ['review','sentiment'])\ndf_test = pd.read_csv('..\/input\/imdb-dataset\/test.csv', usecols = ['review','sentiment'])","58295732":"print(df_train.info())\ndf_train","e723f1e3":"print(df_val.info())\ndf_val","67362800":"print(df_test.info())\ndf_test","dce0b7ac":"def get_metrics(y_test, y_pred_proba):\n    print('ACCURACY_SCORE: ', round(accuracy_score(y_test, y_pred_proba >= 0.5), 4))\n    print('ROC_AUC_SCORE: ', round(roc_auc_score(y_test, y_pred_proba), 4))\n    print('CONFUSION_MATRIX:\\n', confusion_matrix(y_test, y_pred_proba >= 0.5),'\\n')","ad73faf0":"def LSTM_V0(embedding):\n    #...\n    return outputs","a9d7ed2e":"def CNN_V0(embedding):\n    net = Conv1D(128, 7, activation='relu',padding='same')(embedding)\n    net = MaxPooling1D()(net)\n    net = Conv1D(256, 5, activation='relu',padding='same')(net)\n    net = MaxPooling1D()(net)\n    net = Conv1D(512, 3, activation='relu',padding='same')(net)\n    net = MaxPooling1D()(net)\n    net = Flatten()(net)\n    net = Dense(128, activation='relu')(net)\n    net = Dropout(0.5)(net)\n    outputs = Dense(1, activation='sigmoid', name='classifier')(net) \n    return outputs","c1cd97f0":"def BiLSTM_V0(embedding):\n    net = Bidirectional(LSTM(units=32, return_sequences=True))(embedding)\n    net = GlobalAveragePooling1D()(net)\n    net = Dense(20, activation='relu')(net)\n    net = Dropout(rate=0.5)(net)\n    outputs = Dense(1, activation='sigmoid', name='classifier')(net) \n    return outputs","afe45433":"def CNN_LSTM_V0(embedding):\n    net = Dropout(0.3)(embedding)\n    net = Conv1D(200, 5, activation='relu')(net)\n    net = MaxPooling1D(pool_size=2)(net)\n    net = LSTM(100)(net)\n    net = Dropout(0.3)(net)\n    net = Dense(16,activation='relu')(net)\n    outputs = Dense(1, activation='sigmoid', name='classifier')(net)\n    return outputs\n\ndef CNN_LSTM_V1(embedding):\n\n    # channel 1\n    net = Conv1D(filters=128, kernel_size=3*32, activation='relu')(embedding)\n    net = MaxPooling1D(pool_size=2)(net)\n    net = Dropout(0.5)(net)\n    net = BatchNormalization()(net)\n    a = LSTM(128)(net)\n\n    # channel 2\n    net = Conv1D(filters=128, kernel_size=5*32, activation='relu')(embedding)\n    net = MaxPooling1D(pool_size=2)(net)\n    net = Dropout(0.5)(net)\n    net = BatchNormalization()(net)\n    b = LSTM(128)(net)\n\n    # channel 3\n    net = Conv1D(filters=128, kernel_size=7*32, activation='relu')(embedding)\n    net = MaxPooling1D(pool_size=2)(net)\n    net = Dropout(0.5)(net)\n    net = BatchNormalization()(net)\n    c =LSTM(128)(net)\n\n    # channel 4\n    net = Conv1D(filters=128, kernel_size=9*32, activation='relu')(embedding)\n    net = MaxPooling1D(pool_size=2)(net)\n    net = Dropout(0.5)(net)\n    net = BatchNormalization()(net)\n    d=LSTM(128)(net)\n\n    merged = concatenate([a,b,c,d])\n    dense = Dense(100, activation='relu')(merged)\n    drop = Dropout(0.2)(dense)\n    outputs = Dense(1, activation='sigmoid')(merged)\n    return outputs","ab4899a1":"def LSTM_CNN_V0(embedding):\n    net = Bidirectional(LSTM(64, return_sequences=True))(embedding)\n    net = Conv1D(128, 7, activation='relu',padding='same')(net)\n    net = MaxPooling1D()(net)\n    net = Conv1D(256, 5, activation='relu',padding='same')(net)\n    net = MaxPooling1D()(net)\n    net = Conv1D(512, 3, activation='relu',padding='same')(net)\n    net = MaxPooling1D()(net)\n    net = Flatten()(net)\n    net = Dense(128, activation='relu')(net)\n    net = Dropout(0.5)(net)\n    outputs = Dense(1, activation='sigmoid', name='classifier')(net) \n    return outputs","8ac09725":"def create_model(model_name, model_ver, max_seq_len, max_features, embed_size, embedding_matrix):\n\n    ## Creat dictionary\n    choose_model = {'LSTM':{},\n                    'CNN':{0: CNN_V0,},\n                    'BiLSTM':{0: BiLSTM_V0,},\n                    'CNN+LSTM':{0: CNN_LSTM_V0, 1: CNN_LSTM_V1},\n                    'LSTM+CNN':{0: LSTM_CNN_V0}}\n    \n    ## Embedding\n    inputs = Input(shape=(max_seq_len,))\n    embedding = Embedding(max_features,embed_size,weights=[embedding_matrix])(inputs)\n    \n    outputs = choose_model[model_name][model_ver](embedding)\n    model = keras.Model(inputs, outputs)\n        \n    return model","7c2195f0":"max_seq_len = 500\nmax_features = 20000\n\ntokenizer = Tokenizer(num_words=max_features)\ntokenizer.fit_on_texts(list(df_train['review']))\nX_train = tokenizer.texts_to_sequences(df_train['review'])\nX_val = tokenizer.texts_to_sequences(df_val['review'])\nX_test = tokenizer.texts_to_sequences(df_test['review'])","f76da1f3":"X_train = pad_sequences(X_train, maxlen=max_seq_len)\nX_val = pad_sequences(X_val, maxlen=max_seq_len)\nX_test = pad_sequences(X_test, maxlen=max_seq_len)\ny_train = df_train['sentiment']\ny_val = df_val['sentiment']\ny_test = df_test['sentiment']","d52fdcfa":"EMBEDDING_FILE = '..\/input\/glove-global-vectors-for-word-representation\/glove.6B.200d.txt'\ndef get_coefs(word,*arr): return word, np.asarray(arr, dtype='float32')\nembeddings_index = dict(get_coefs(*o.split(\" \")) for o in open(EMBEDDING_FILE) if len(o)>100)\n\nall_embs = np.stack(embeddings_index.values())\nemb_mean,emb_std = all_embs.mean(), all_embs.std()\nembed_size = all_embs.shape[1]\n\nword_index = tokenizer.word_index\nnb_words = min(max_features, len(word_index))\nembedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, embed_size))\nfor word, i in word_index.items():\n    if i >= max_features: continue\n    embedding_vector = embeddings_index.get(word)\n    if embedding_vector is not None: embedding_matrix[i] = embedding_vector","3012c70b":"model_name = \"LSTM+CNN\"\nmodel_ver = 0\nLR = 1e-3\nloss = BinaryCrossentropy(from_logits=True)\noptimizer = Adam(learning_rate = LR)\nmetrics = [BinaryAccuracy(), AUC()]\n\nmodel = create_model(model_name, model_ver, max_seq_len, max_features, embed_size, embedding_matrix)\nmodel.compile(loss=loss, optimizer=optimizer, metrics=metrics)\nmodel.summary()","d84f1cd3":"# Plot architecture model\ntf.keras.utils.plot_model(model, show_shapes=True, dpi=96)","0090c5a3":"# Save model\nmodel_ckpt_path = f\"[Glove-200d]{model_name}-V{model_ver}\"+\"_epoch{epoch:02d}.hdf5\"\ncheckpoint = ModelCheckpoint(model_ckpt_path, monitor='val_binary_accuracy', mode='max', verbose=1, save_best_only=True, save_weights_only=True)\ncallbacks_list = [checkpoint]\n\n# Training\nprint(f\"Training model with [Glove-200d]{model_name}-V{model_ver}\\n\")\ntrain_history = model.fit(X_train, y_train, validation_data=(X_val,y_val), epochs=5, batch_size=64, verbose=1, callbacks=callbacks_list)","12d7eed4":"# Plot accuracy and loss\nhistory_dict = train_history.history\nprint(history_dict.keys())\n\nacc = history_dict['binary_accuracy']\nval_acc = history_dict['val_binary_accuracy']\nloss = history_dict['loss']\nval_loss = history_dict['val_loss']\n\nepochs = range(1, len(acc) + 1)\nfig = plt.figure(figsize=(10, 6))\nfig.tight_layout()\n\nplt.subplot(2, 1, 1)\nplt.plot(epochs, loss, 'r', label='Training loss')\nplt.plot(epochs, val_loss, 'b', label='Validation loss')\nplt.title('Training and validation loss')\nplt.ylabel('Loss')\nplt.legend()\n\nplt.subplot(2, 1, 2)\nplt.plot(epochs, acc, 'r', label='Training acc')\nplt.plot(epochs, val_acc, 'b', label='Validation acc')\nplt.title('Training and validation accuracy')\nplt.xlabel('Epochs')\nplt.ylabel('Accuracy')\nplt.legend(loc='lower right')","7df50412":"ls -d *hdf5","e658a018":"list_path = [\"01\",\"02\"]\nfor path in list_path:\n    print(f\"Epoch {path} \\n\")\n    model_ckpt_path = f\"[Glove-200d]{model_name}-V{model_ver}_epoch{path}.hdf5\"\n    model.load_weights(model_ckpt_path)\n    y_pred_proba = model.predict(X_test)\n    get_metrics(y_test, y_pred_proba)","0c499703":"## Evaluate","db7666ce":"## Model","d62e806d":"### CNN","0bc5cd6e":"### LSTM","2d64a433":"## Data Cleaning\nRef: https:\/\/www.kaggle.com\/colearninglounge\/nlp-data-preprocessing-and-cleaning\n\nRaw text gives better results than preprocessed text","7d9000a8":"## Creat model","12431e9d":"### Choose model","5eee3ae6":"### LSTM + CNN","73106ac1":"## Tokenzie","0ff00e95":"## Model training","3a88370a":"## Load Dataset\nTrain \/ Validation \/ Test = 7 \/ 1 \/ 2","2967c15c":"### BiLSTM","5cb5c468":"## Setup","5f487360":"### CNN + LSTM","83c151f3":"## Metrics","18d9978e":"## Embeddings\nRef: https:\/\/www.kaggle.com\/colearninglounge\/nlp-model-building-transformers-attention-more#Build-a-Static-Semantic-Embedding-Neural-Network(LSTM)-Baseline"}}