{"cell_type":{"bef41622":"code","94ecae6b":"code","6d89b782":"code","7c768ed1":"code","e86d01c3":"code","5a9f18e1":"code","289bce37":"code","ad855356":"code","9997f85d":"code","4ab98ddc":"code","50831b46":"code","80f70baf":"code","966ecca2":"code","cc26004d":"code","5d433d38":"code","44102ea2":"code","951fb3de":"code","f265a333":"code","2354118b":"code","000b1cfd":"code","4dfc64a4":"code","1b47b253":"code","a56197f7":"code","05a43dea":"code","a9263fc7":"code","8506a911":"code","4287aa09":"code","aac3484e":"code","03638c42":"code","5028194d":"code","6b980113":"code","3d0995ce":"code","f1a2d9f9":"code","ceb2c353":"code","ff43ad87":"code","63a386c3":"code","53d6c967":"code","1beb0d8c":"code","4c7f81b3":"code","ddf9489e":"code","a9313088":"code","72f45cf9":"code","76af98e6":"code","1f1b6864":"code","e829d73b":"code","4156f827":"code","aa90b348":"code","a917fb83":"code","a426df6e":"code","c8ed39b4":"code","9eb4bee6":"code","3232f6ae":"code","4ba33ad2":"markdown","e616bd12":"markdown","b684a065":"markdown","c0bebfa4":"markdown","2b13fbbf":"markdown","e6c4cc6b":"markdown","9d6cf107":"markdown","7bcd09c8":"markdown","8579f6ff":"markdown","5d71805e":"markdown","9a6fcabb":"markdown","01061c4f":"markdown","12a14eea":"markdown","745367bd":"markdown","75b029e2":"markdown","e106b3c0":"markdown","93053490":"markdown","4d274dc8":"markdown","42844b64":"markdown","aa665efd":"markdown","87f566ad":"markdown","3850c498":"markdown","f3231dfb":"markdown","d48b80e8":"markdown","b1848e72":"markdown","77b14b46":"markdown","1aaec75b":"markdown","346fed63":"markdown","9c707ee9":"markdown","a8ef7796":"markdown","87a8ef41":"markdown","42466799":"markdown","f35ce08d":"markdown","cd96eeb1":"markdown","4066510e":"markdown","6a21e20a":"markdown","6df9e6f3":"markdown","a864b61e":"markdown","5391cec5":"markdown","5e92f4a5":"markdown","ed20d238":"markdown","eedbfed5":"markdown","38a8a582":"markdown","53552c0e":"markdown","a32317c9":"markdown","665500e9":"markdown","573c2b6b":"markdown","f06fa09e":"markdown","4d628e5b":"markdown","e969d40a":"markdown","8dc4f43e":"markdown","49b6e23c":"markdown","d9453c8d":"markdown","272d2969":"markdown","ea7b7b34":"markdown","9ac46b80":"markdown","799c01d4":"markdown","cc4fe6a1":"markdown","1d97caaf":"markdown","4f821f37":"markdown","a6ebcd0f":"markdown","7b0734eb":"markdown","6507a78d":"markdown","e94ccadf":"markdown","689f7e4e":"markdown","535a17ac":"markdown","826b3a68":"markdown","ec93b2dd":"markdown","59db6528":"markdown","a20e1904":"markdown","5c6444c5":"markdown","1470375a":"markdown","3cd5e38f":"markdown","4d08df69":"markdown","7d7bbe50":"markdown","f907087a":"markdown"},"source":{"bef41622":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","94ecae6b":"# ! pip uninstall tensorflow -y","6d89b782":"# ! pip install tensorflow-gpu==2.6","7c768ed1":"import numpy as np\nimport pandas as pd\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, LSTM, Dropout\nfrom tensorflow.keras.layers import *\nfrom tensorflow.keras.optimizers import Adam, SGD\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom tensorflow.keras import backend as K\nimport tensorflow_probability as tfp\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error\nfrom sklearn.model_selection import train_test_split\nfrom scipy.stats import pearsonr, spearmanr\nimport seaborn as sns\nimport requests\nimport missingno as msno","e86d01c3":"import tensorflow as tf\ntf.__version__","5a9f18e1":"!pip install openpyxl ","289bce37":"import torch\n\nis_cuda = torch.cuda.is_available()\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\ndevice\nif is_cuda:\n    device = torch.device(\"cuda\")\nelse:\n    device = torch.device(\"cpu\")","ad855356":"x_features = pd.read_excel(\"..\/input\/datas-features\/Features data set.xlsx\") # features \ny_sales = pd.read_excel('..\/input\/datas-features\/sales data-set.xlsx') # labels\nz_stores = pd.read_csv(\"..\/input\/datas-features\/stores data-set.csv\") # stores","9997f85d":"print(x_features.head())\nprint(y_sales.head())\nprint(z_stores.head())\nprint('-'*40)\nprint('\\n')","4ab98ddc":"# First - Optimize the sales via store in a unique date with all Deparments to be preper\ny_sales['Date'] = pd.to_datetime(y_sales['Date'])\n# we have to repeat its again to fit x_features y_sales\ny_sales = y_sales.groupby(['Store', 'Date']).sum()\n# Merging of x_features and z_stores\nx_features = pd.merge(x_features, z_stores)","50831b46":"# Secondly I  transform categorical 'Type' column' feature needed to be number.\ndef Type2num(x):\n     if x == 'A':\n         return 0\n     if x == 'B':\n         return 1\n     if x == 'C':\n         return 2\nx_features['Type'] = x_features['Type'].apply(Type2num) \n\n\nx_features['Date'] = pd.to_datetime(x_features['Date'])\nx_features = x_features.groupby(['Store', 'Date']).sum()\nprint(x_features.dtypes)\nprint(y_sales.dtypes)\nprint(z_stores.dtypes)","80f70baf":"x_features['Type'].value_counts() #categorical variable\nx_features.Type.value_counts().plot.barh();","966ecca2":"Combined_table = pd.merge(x_features, y_sales['Weekly_Sales'], how='inner', right_index=True, left_index=True)\nCombined_table.isna().sum()\nCombined_table.info\nCombined_table['Weekly_Sales'].describe()\n\n# Very well... It seems that your minimum price is larger than zero. Excellent!","cc26004d":"# Adding a colum and calculating profit per m^2\nCombined_table['M^2_per_profit'] = Combined_table['Weekly_Sales']\/Combined_table['Size']\nCombind_graf = Combined_table.copy()","5d433d38":"Combined_table.isnull().sum()\nCombined_table.describe(percentiles=[0.25, 0.5, 0.75, 0.9, 0.95, 0.99])\nmsno.bar(Combined_table)   # checking for null ? by plotting.","44102ea2":"print(\"Skewness: %f\" % Combined_table['Weekly_Sales'].skew())\nprint(\"Kurtosis: %f\" % Combined_table['Weekly_Sales'].kurt())","951fb3de":"# let see how Weekly_Sales corellative to other features ?\n# sns.distplot(Combined_table['Weekly_Sales'])\nsns.displot(data = Combined_table, x = 'Weekly_Sales', kde=True)","f265a333":"# first convert the store from index into a column\n# Takea a look over each sotre outlayers via \"Weekly_Sales'.\nCombined_table = Combined_table.reset_index(level=0)\nvar = 'Store'\ndata = pd.concat([Combined_table['Weekly_Sales'], Combined_table[var]], axis=1)\nf, ax = plt.subplots(figsize=(8, 6))\nfig = sns.boxplot(x=var, y=\"Weekly_Sales\", data=data)\nfig.axis(ymin = 180000, ymax = 4000000)","2354118b":"# Let display the relationship between two numerical variables\n# For any combination features. \nsns.set()\ncols = ['Temperature', 'Fuel_Price', 'CPI', 'Unemployment', 'MarkDown2', 'MarkDown3', 'MarkDown4', 'MarkDown5', 'Weekly_Sales']\nsns_plot = sns.pairplot(Combind_graf[cols].sample(100), height = 2.5) \nplt.show()","000b1cfd":"# I try to see if it's any corralation between the features\nCombind_graf[['Temperature', 'Fuel_Price', 'CPI', 'Unemployment', 'MarkDown1', 'MarkDown2', 'MarkDown3', 'MarkDown4', 'MarkDown5', 'Type', 'Size', 'Weekly_Sales', 'M^2_per_profit']].plot(subplots=True, figsize=(20,15))\nplt.show()","4dfc64a4":"# In the following Matrix we will see how mach the features are confusing?.\n# As we see not all features are not corelative. \ng = sns.heatmap(Combind_graf[['Temperature', 'Fuel_Price', 'CPI', 'Unemployment', 'MarkDown1', 'MarkDown2', 'MarkDown3', 'MarkDown4', 'MarkDown5', 'Weekly_Sales', 'M^2_per_profit', 'Type', 'Size']].corr(),annot=True, fmt = \".2f\", cmap = \"coolwarm\")\nplt.show()","1b47b253":"# Since I have two Prameters with I correlasition, I drop 'MarkDown1', Why ?\n# To prevent  Linkage featurs that are corelatived.\n# Even that I havesome specifics correlative feautures that I will not drop like: 'Type', 'Size'. and 'Weekly_ales', why ??\n# Why? Because each of those features stay alone with meaning that should present.\ng = sns.heatmap(Combind_graf[['Temperature', 'Fuel_Price', 'CPI', 'Unemployment', 'MarkDown2', 'MarkDown3', 'MarkDown4', 'MarkDown5', 'Weekly_Sales', 'M^2_per_profit', 'Type', 'Size']].corr(),annot=True, fmt = \".2f\", cmap = \"coolwarm\")\nplt.show()","a56197f7":"Combined_table = Combined_table.drop(['MarkDown1'], axis = 1)","05a43dea":"print(\"Dats shape = {}\".format(Combined_table.shape))\nprint()\nprint(\"Lets see some feature:\")\nprint(Combined_table[1:10])","a9263fc7":"# I try to see if it's any corralation between those tow features\nCombind_graf[['Type', 'Size', 'Weekly_Sales']].plot(subplots=True, figsize=(20,15))\nplt.show()","8506a911":"# I try to see if it's any corralation between 'Type' and 'Weekly_Sales'\nCombind_graf[['Type', 'Weekly_Sales']].plot(subplots=True, figsize=(20,15))\nplt.show()","4287aa09":"# I try to see if it's any correlation between 'Size' and 'Weekly_Sales'\nCombind_graf[['Size', 'Weekly_Sales']].plot(subplots=True, figsize=(20,15))\nplt.show()","aac3484e":"# I try to see if it's any corralation between 'M^2_per_profit' and 'Weekly_Sales'\nCombind_graf[['M^2_per_profit', 'Weekly_Sales']].plot(subplots=True, figsize=(20,15))\nplt.show()","03638c42":"# Till nuw what we saw ? I saw that 'M2^_per_profit' feature is in correlation with \n#  'Type'----> 40% and 'Size'----> 30%. 'M2^_per_profit' feature was create to check profit.\n# Once I did my exploration with 'M2^_per_profit' and it's artificial feature I need drop it.\n# Reducing features revoke dimensional Data Curse problem.\n# This feature 'M2^_per_profit' could be predicable like 'Size' or 'Type' but both of them are better.\nCombined_table = Combined_table.drop(['M^2_per_profit'], axis=1)\n# Let plot it.\n\ng = sns.heatmap(Combind_graf[['Temperature', 'Fuel_Price', 'CPI', 'Unemployment', 'MarkDown2', 'MarkDown3', 'MarkDown4', 'MarkDown5', 'Weekly_Sales', 'Type', 'Size']].corr(),annot=True, fmt = \".2f\", cmap = \"coolwarm\")\nplt.show()","5028194d":"# Definition of Y (label) and X (inputs)\n# Define label for the new merge table: \"combined_table\"\n# The 'Weekly_Sales' is Indexial so it dosn't take as a label\ny = Combined_table['Weekly_Sales']\n# Define features for the new merge table: \"combined_table\"\nx = Combined_table.drop(['Weekly_Sales'], axis=1)","6b980113":"scaler = MinMaxScaler()\nx_scaled = scaler.fit_transform(x)\ny.shape\ny = y.values.reshape(6435, 1)\ny_scaled = scaler.fit_transform(y)\nx.head()\nx.tail()","3d0995ce":"# we need to take x_scaled after being notmalized\n# In the first step we will split the data in training and remaining dataset\n# random_state = 0 to keep order time as best as I can\nx_train, x_valid, y_train, y_valid = train_test_split(x_scaled, y, train_size=0.80, random_state=0)\n","f1a2d9f9":"# Now since we want the valid and test size to be equal (10% each of overall data). \n# we have to define valid_size=0.5 (that is 50% of remaining data)\ntest_size = 0.5\nx_test, x_valid, y_test, y_valid = train_test_split(x_scaled,y, test_size=0.5)\n# Finally splitting will be : 80%-10%-10% as train-val-test\n","ceb2c353":"x_train = x_train.reshape(x_train.shape[0],1,x_train.shape[1]) \nx_valid = x_valid.reshape(x_valid.shape[0],1,x_valid.shape[1])\nx_test = x_test.reshape(x_test.shape[0],1,x_test.shape[1]) \n\nTest_Data = (x_test, y_test)\nprint(x_train.shape), print(y_train.shape)\nprint(x_valid.shape), print(y_valid.shape)\nprint(x_test.shape), print(y_test.shape)","ff43ad87":"# Training basic LSTM model# Initializing the Recurrent Neural Network AS LSTM\ninputs = tf.random.normal([32, 50, 11])\nmodel = Sequential()","63a386c3":"# Adding the first LSTM layer with a sigmoid activation function and some\n# Dropout regularization\n# Units - dimensionality of the output space\nmodel.add(LSTM(units = 32, return_sequences = False, input_shape =(1,x_train.shape[2])))\n# Adding the output layer\nmodel.add(Dense(units = 128, activation=\"relu\"))\nmodel.add(Dense(units = 64, activation=\"relu\"))\nmodel.add(Dense(units = 1, activation=\"relu\", input_shape=(4,)))\nmodel.summary()","53d6c967":"opt = tf.keras.optimizers.Adam(learning_rate = 0.1, beta_1 = 0.9, beta_2 = 0.999, epsilon = 1e-08, decay = 0.0)\n# Best learning_rate was found to be 0.1  .\n# Metrics=[soft_acc], optimizer = opt","1beb0d8c":"model.compile(loss = 'mse', optimizer = opt)","4c7f81b3":"# A logger was created for logs the best whieghts of the training to be saved.\nmy_callbacks = [tf.keras.callbacks.ModelCheckpoint(save_best_only = True, filepath = 'model.{epoch:02d}-{val_loss:.2f}.h5'),\n    tf.keras.callbacks.TensorBoard(log_dir = '.\/logs')]\nlogger = tf.keras.callbacks.TensorBoard(log_dir = 'logs', write_graph = True,\n    histogram_freq = 5)","ddf9489e":"# Use `rankdir='LR'` to make the graph horizontal.\ntf.keras.utils.plot_model(model, show_shapes=True, rankdir=\"LR\")\n","a9313088":"history = model.fit(x_train,y_train,epochs = 200, batch_size = 16, validation_data = (x_valid,y_valid), callbacks = my_callbacks)\np = history.history['loss']\nprint('-'*40)\nprint('\\n')\n# list all data in history\nprint(history.history.keys())","72f45cf9":"# summarize history for loss\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['x_train', 'x_valid'], loc='upper left')\nplt.legend(['y_train', 'y_valid'], loc='upper left')","76af98e6":"preds_val = model.predict(x_valid)\npreds_val = preds_val.squeeze()\nresult_val = y_valid - preds_val","1f1b6864":"preds_test = model.predict(x_test)\npreds_test = preds_test.squeeze()\nresult_test = y_test - preds_val","e829d73b":"def correlation_coefficient_var(y_valid, preds_coef_val):\n    pearson_r_val = tfp.stats.correlation(preds_coef_val, y_valid)\n    return(pearson_r_val)\n    print(pearson_r_val)","4156f827":"def correlation_coefficient_test(y_test, preds_coef_test):\n    pearson_r_test = tfp.stats.correlation(preds_coef_test, y_test)\n    return(pearson_r_test)\n    print(pearson_r_test)\n    \nprint('-'*40)\nprint('\\n')","aa90b348":"preds_val = preds_val.reshape(-1, 1)\ny_valid = y_valid.astype('float32')\nprint(correlation_coefficient_var(y_valid, preds_val))","a917fb83":"preds_test = preds_test.reshape(-1, 1)\ny_test = y_test.astype('float32')\nprint(correlation_coefficient_test(y_test, preds_test))","a426df6e":"# Plotting the label prediction of sales.\nCombind_graf = Combined_table.copy()\nCombind_graf[['Weekly_Sales']].plot(subplots=True, figsize=(20,15))\nplt.title('model Weekly sales predict')\nplt.ylabel('Income')\nplt.xlabel('Weekly Time Sales')\nplt.show()","c8ed39b4":"(print(preds_val))\ncorr, p_val = pearsonr(y_valid.squeeze(), preds_val.squeeze())\nprint('Pearson corr validation')\nprint(corr)\nprint('-'*40)\nprint('\\n')","9eb4bee6":"(print(preds_test))\ncorr, p_test = pearsonr(y_test.squeeze(), preds_test.squeeze())\nprint('Pearson corr test')\nprint(corr)","3232f6ae":"print('This inquiry seem to be applicable for the majority Retail marketing network \\\n      - sales. This vision and methods work used in this kernel should therefore \\\n          be useful for large range of such problems likes these types of sles predict.')","4ba33ad2":"# **2. Libraries to import.**","e616bd12":"**12.2 Plotting 'Loss'**","b684a065":"**6.1.1**","c0bebfa4":"Code for Cpu or Gpu","2b13fbbf":"**16.2 Print pridict of test.**","e6c4cc6b":"**conclusion: Finally The linckage is no so big wether we drop or not 'MarkDown1'\nbut I need to reduce features best as i can.**","9d6cf107":"**3.4.2 Inquery by Plotting isnull ?**","7bcd09c8":"**8.4 we need to add an additional dimantion to get numpy arrey for keras shape**","8579f6ff":"**3.4 Merging all Data**","5d71805e":"**8.2 splitting the Datas to train @ test : 80%-20%**","9a6fcabb":"# **4. Inquery by Plotting**","01061c4f":"**14.3 prediction for validetion.**","12a14eea":"**14.4 prediction for test.**","745367bd":"**Now I check my small data with Pearson as a statistic problem.\nForecasting Accuracy = Pearson.**","75b029e2":"# **8. Modeling.**","e106b3c0":"**6.2 Drop unuse feature**","93053490":"**5.1 Plotting Confusional Correlation matrix between numerical values**","4d274dc8":"**Flow the plot above the sales along the time dosn't chang**","42844b64":"**16.3 Conclusion.**","aa665efd":"# **3. Load and check data**","87f566ad":"    1.  Table of Contents\n    2.  Libraries to import.\n    3.  Load and check data.\n    4.  Inquery by Plotting.\n    5.  Feature analysis.**\n    6.  Core business analysis and inquery by plotting\n    7.  Feature engineering.\n    8.  Modeling.\n    9.  Training basic LSTM model.\n    10. Addional LSTM to Fully conected model deep.\n    11. Hyperparameters.\n    12. Fitting + plotting 'Loss'.\n    13. Prediction.\n    14. Pearson.\n    15. Checking @ comparing all features to be predictable.\n    16. Conclusion.","3850c498":"**16.1 Print pridict of validetion.**","f3231dfb":"**By Pearson on Validation and Test verificatuin after  lots of epocs.**\n**WE could declair that the model prediction fits the Data training in** \n**Aproximitly around 95%-96% Pearson accurecy.**\n**In both model we could see same conclusion.**\n**In other words : \"Type\" and Size columns could be priditable for sales.**\n**The model Fully conected (deep) was crossover with Pearson and we have same results.**\n**Since we have no Change in sales along the time - that will be the predict future**\n**for 95%-96%**","d48b80e8":"**12.3 Mini conclusion**","b1848e72":"# **0. Instruction:**","77b14b46":"Adam was config as Adaptive Learning Rate Methods","1aaec75b":"**6.1.3**","346fed63":"**Retail chain efficiency is measured as efficiency per square meter of profit. \nLet's see what the efficiency is per square meter? What is the rate of increase \nin efficiency? Next we will examine whether it is possible to predict the rate \nof sales growth based on this figure, and what is the percentage of accuracy of \nthe model?**","9c707ee9":"# **15. Check comparing all features to be predictable.**","a8ef7796":"**Finally after take a look on the Loss,'y_train' became fitting with 'y_valid even**\n1) All most all the stores are with outlayers.                                    \n2) Correlation graph are not a simetric Gaussian.                                 \n3) Small Data.                                                                    \n4) Very big numbers sales that cause big differances and complex calaulation.\n5) We have small lost.     \nIt is very good but I will do a crossover with Pearson.","87a8ef41":"**3.5.1 We have appreciable positive skewness.**","42466799":"**4.2 Inquery of outlier by box plot store \/ Weekly_Sales**","f35ce08d":"**As an inner mean - marge just the common Datas.**","cd96eeb1":"**We could see above that 'Size' store and 'Type' store could be good predictable \nfor business growing marketing. \nI need to see if those two features could be implemented with deep learning model.\nFinally as showing plotting above 'M^2_per_profit' have no good correlation \nwith other features, and along the time the profit per m^2 get down. So m^2 \nand can not pridictable.\nTo be on the safe side I take correlasition Pearson Metric as another referance\nbias the FC (fuly conected) model.**","4066510e":"**8.3 splitting the test to test @ val : 50%-50%**","6a21e20a":"**3.3 Groupby working**","6df9e6f3":"# **1. Table of Contents:**","a864b61e":"# **7. Feature engineering.**","5391cec5":"**12.1 Fitting**","5e92f4a5":"**3.5.2 We have appreciable positive Kurtosis.**","ed20d238":"**4.1 histogram plot**","eedbfed5":"# **14. Pearson.**","38a8a582":"# **16. Conclusion.**","53552c0e":"# **9. Training basic model LSTM model.**","a32317c9":"**6.1.2**","665500e9":"**13.1. For Validation.**","573c2b6b":"**5.2 Plotting Confusional Correlation matrix with no features correlative**","f06fa09e":"**3.3.1 Plotting 'Type' categorical variable above**","4d628e5b":"**'Size' feature could be predictable with out any deep learning mode just in case of this Data for 80% accurecy.If we take all the features of this data we could get 95%-96% accurecy how? It's happen because all features are synergists one to each other. In other case of data we need to aplic all the code (since it's other features the synergists could change and accurecy too) .Even that the code above could be aplicable for any data sale pridict.**","e969d40a":"**3.4.1 let Calculating profitability per square meter**","8dc4f43e":"# **13. Prediction (Verification of the prediction).**","49b6e23c":"**3.5 inquery by: Skewness @ skew**","d9453c8d":"# **5. Feature analysis**","272d2969":"**3.2 Inquering all Data**","ea7b7b34":"# **6. Core business analysis and inquery by plotting.**","9ac46b80":"**The groupby drop all missing rows**","799c01d4":"**After looking the data we have a problem with missing data\nSo to fix it we need apply the dates in both columns to same type date.\nI will choose as best as I can the featurs that fit the economic aspects predict.**","cc4fe6a1":"**14.1 Checking for the validetion coefficient.**","1d97caaf":"**Here we have a retail chain that I try to predict weekly sales.\nI will try to find a feature that could be preditable.\nUsually - the profit is mesure by M^2. could this feature be predictable ?\nFor this question above I will inquery the profit and sales.\nThen, wether Fully conected (Deep Learning model with LSTM could be good model ?**\n\n**Could small data and problems of sequences arrangement fit Lstm and fully \nconected deep model?\nAt the end I will do a crossover with Pearson to see if what was obtained by \nfully conected is good.**\n\n**Finally**\n\n**This inquiry seem to be applicable for the majority Retail marketing network \nsales, but not only. This vision and methods work used in this kernel should therefore \\\nbe useful for large range of such problems sales or any business growth.**","4f821f37":"**11.4 Let's visualize the connectivity graph:**","a6ebcd0f":"**8.1 preprocessing normalization values between 0 and 1**","7b0734eb":"**4.4 Plotting all features via the label (prediction of sales)**","6507a78d":"# **12. Fitting + plotting 'Loss'.**","e94ccadf":"**11.1 Optimiztion.**","689f7e4e":"**5.3 Let see the first 10 variable (for example)**","535a17ac":"**4.3 scatterplot - Data Visualization**","826b3a68":"**14.2 Checking for the test coefficient.**","ec93b2dd":"**13.2 for Test.**","59db6528":"**But in any other data we must pay attention to the synergy between the features. Once the features change then the accurecy could be changed too. even that it's applicable.**  ","a20e1904":"# **11. Hyperparameters.**","5c6444c5":"**6.1 Plotting 'M^2_per_profit' 'Type' and 'Size\" via the label (prediction of main sales features)**","1470375a":"**11.2 Compilation.**","3cd5e38f":"\n\nCreated on Sun Dec 11 09:54:32 2021\n\n@author: Rony Keller - Algorithm Engineer (Data Science),technologies Manager @ Economic investigation (Electronic practical Engineer & Graduate of Business Administration) ML and Deep Learning. \"\"\"\n","4d08df69":"**11.3 Create logger and Tensorboard for analyzing.**","7d7bbe50":"# **10. Addional LSTM to Fully conected model deep.**","f907087a":"**3.1 Load data**"}}