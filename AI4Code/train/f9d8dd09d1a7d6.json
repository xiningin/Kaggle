{"cell_type":{"5e743b86":"code","558232be":"code","5060049f":"code","000f326a":"code","aa6d6e23":"code","f1a4361d":"code","dc1be0f9":"code","e3d13a13":"code","63f668f1":"code","eea6d115":"code","4e04990f":"code","69c532ba":"code","874527d4":"code","7e3ae0af":"code","e2b82b6a":"code","b6c81d44":"code","0fea215b":"code","562226be":"code","eaf780b6":"code","41cd3209":"code","765aa631":"code","17000d68":"code","885a94e0":"code","9ebdab33":"code","0ce05515":"code","5d21eb2f":"code","b578836f":"code","3bde1b27":"markdown","3f7b5658":"markdown","5c36f7db":"markdown","a7bd35a1":"markdown","ce66741a":"markdown","64930a1f":"markdown","9266c0cf":"markdown","c2c1e0b4":"markdown","6a71c44c":"markdown","dcf1d238":"markdown","c0c72b16":"markdown","e2febb7e":"markdown","8645153d":"markdown"},"source":{"5e743b86":"import pandas as pd\nimport numpy as np\nfrom datetime import datetime\nimport cudf\n\nfrom tqdm import tqdm\ntqdm.pandas()\n\nfrom catboost import CatBoostClassifier\nfrom sklearn.metrics import roc_auc_score\n\nimport warnings\nwarnings.filterwarnings('ignore')","558232be":"def reduce_memory(df):\n    for col in df.columns:\n        col_type = df[col].dtypes\n        if col_type != object:\n            cmin = df[col].min()\n            cmax = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if cmin > np.iinfo(np.int8).min and cmax < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif cmin > np.iinfo(np.int16).min and cmax < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif cmin > np.iinfo(np.int32).min and cmax < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif cmin > np.iinfo(np.int64).min and cmax < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)\n            else:\n                if cmin > np.finfo(np.float16).min and cmax < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif cmin > np.finfo(np.float32).min and cmax < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n    return df","5060049f":"customer_df = cudf.read_csv(\"..\/input\/1-data-preprocessing-do-u-otomotiv\/FINAL_CUSTOMER_DATATHON.csv\")\ncustomer_related_df = cudf.read_csv(\"..\/input\/1-data-preprocessing-do-u-otomotiv\/FINAL_CUSTOMER_RELATED_TABLE_FOR_DATATHON.csv\")\nsales_df = cudf.read_csv(\"..\/input\/1-data-preprocessing-do-u-otomotiv\/FINAL_SALES_FILE_DATATHON.csv\")\nbought_df = cudf.read_csv(\"..\/input\/1-data-preprocessing-do-u-otomotiv\/FINAL_SIFIR_ARAC_ALANLAR_DATATHON.csv\")\nvehicle_df = cudf.read_csv(\"..\/input\/1-data-preprocessing-do-u-otomotiv\/FINAL_VEHICLE_TABLE_DATATHON.csv\")\nservice_df = cudf.read_csv(\"..\/input\/1-data-preprocessing-do-u-otomotiv\/MASK_SERVIS_BAKIM_DATATHON_FINAL.csv\")\nusd_df = cudf.read_csv(\"..\/input\/1-data-preprocessing-do-u-otomotiv\/usd_monthly.csv\")\ntufe_df = cudf.read_csv(\"..\/input\/1-data-preprocessing-do-u-otomotiv\/tufe.csv\")\nsales_df_3mo_total = cudf.read_csv(\"..\/input\/1-data-preprocessing-do-u-otomotiv\/sales_df_3mo_total.csv\").drop(\"Unnamed: 0\", 1)\n\nsubm_df = cudf.read_csv(\"..\/input\/dogus-datathon-otomotiv\/sample_submission.csv\")","000f326a":"base_and_vanilla_cids = customer_df[[\"CUSTOMER_ID\", \"BASE_CUSTOMER_ID\"]].copy()\n\nsales_df = sales_df.merge(base_and_vanilla_cids, how=\"left\", on=\"CUSTOMER_ID\")\nsales_df.drop(\"CUSTOMER_ID\",1,inplace=True)\n\nbought_df = bought_df.merge(base_and_vanilla_cids, how=\"left\", on=\"CUSTOMER_ID\")\nbought_df.drop(\"CUSTOMER_ID\",1,inplace=True)\n\ncustomer_df.drop(\"CUSTOMER_ID\", 1, inplace=True)\ncustomer_df = customer_df.drop_duplicates(subset=['BASE_CUSTOMER_ID'], keep='first')","aa6d6e23":"service_df[\"CREATE_DATE\"] = cudf.to_datetime(service_df[\"CREATE_DATE\"])\ncustomer_related_df[\"START_DATE\"] = cudf.to_datetime(customer_related_df[\"START_DATE\"])\ncustomer_related_df[\"END_DATE\"] = cudf.to_datetime(customer_related_df[\"END_DATE\"])\nbought_df[\"CREATE_DATE\"] = cudf.to_datetime(bought_df[\"CREATE_DATE\"])\nvehicle_df[\"TRAFFIC_DATE\"] = cudf.to_datetime(vehicle_df[\"TRAFFIC_DATE\"])\nsales_df[\"SF_CREATE_DATE\"] = cudf.to_datetime(sales_df[\"SF_CREATE_DATE\"])\nsales_df_3mo_total[\"time_index\"] = cudf.to_datetime(sales_df_3mo_total[\"time_index\"])\ntufe_df[\"TIME\"] = cudf.to_datetime(tufe_df[\"TIME\"])","f1a4361d":"datelist = pd.date_range(\"2010-01-01\", end=\"2021-09-01\", freq=\"1M\").tolist()\n\ndate_windows = []\n\nfor date_i in range(len(datelist)):\n    try:\n        date_windows.append([datelist[date_i] + pd.DateOffset(1), datelist[date_i+3] + pd.DateOffset(1)])\n    except:\n        pass\n    \ndate_windows","dc1be0f9":"def np_mean(x):\n    try:\n        return np.diff(x).mean()\n    except:\n        return np.nan","e3d13a13":"whole_data_df = []\n\nfor date_window in tqdm(date_windows):\n    \n    ## PREPARING VEHICLE DATA\n    # Selecting the last car is being owned for each customer\n    \n    wrapped_customer_related_mask = (customer_related_df.START_DATE < date_window[0]) &\\\n                                    (customer_related_df.END_DATE >= date_window[0])\n    wrapped_customer_related_passive_mask = (customer_related_df.END_DATE < date_window[0])\n    \n    wrapped_customer_related_df = customer_related_df[wrapped_customer_related_mask]\n    \n    ########################################\n    # AUTOCORRELATION TABANLI \u00d6ZELL\u0130KLER (\u0130\u015fe pek yaramad\u0131, veri olu\u015fturma s\u00fcresini uzatt\u0131\u011f\u0131 i\u00e7in yoruma ald\u0131m)\n    \n#     wrapped_sales_df_3mo_total = sales_df_3mo_total[sales_df_3mo_total.BASE_CUSTOMER_ID.isin(wrapped_customer_related_df.BASE_CUSTOMER_ID)].to_pandas()\n\n#     wrapped_sales_df_3mo_total_lag_3mo = wrapped_sales_df_3mo_total[wrapped_sales_df_3mo_total.time_index < date_window[0]+pd.DateOffset(-90)]\n#     acorr_3_mo = wrapped_sales_df_3mo_total_lag_3mo.groupby(['BASE_CUSTOMER_ID'])['3mo_count'].progress_apply(pd.Series.autocorr, lag=3).reset_index()\\\n#                         .rename(columns={\"3mo_count\": \"acorr_3_mo\"})\n\n#     wrapped_sales_df_3mo_total_lag_6mo = wrapped_sales_df_3mo_total[wrapped_sales_df_3mo_total.time_index < date_window[0]+pd.DateOffset(-180)]\n#     acorr_6_mo = wrapped_sales_df_3mo_total_lag_6mo.groupby(['BASE_CUSTOMER_ID'])['3mo_count'].progress_apply(pd.Series.autocorr, lag=6).reset_index()\\\n#                         .rename(columns={\"3mo_count\": \"acorr_6_mo\"})\n\n#     wrapped_sales_df_3mo_total_lag_12mo = wrapped_sales_df_3mo_total[wrapped_sales_df_3mo_total.time_index < date_window[0]+pd.DateOffset(-365)]\n#     acorr_12_mo = wrapped_sales_df_3mo_total_lag_12mo.groupby(['BASE_CUSTOMER_ID'])['3mo_count'].progress_apply(pd.Series.autocorr, lag=12).reset_index()\\\n#                         .rename(columns={\"3mo_count\": \"acorr_12_mo\"})\n\n#     wrapped_customer_related_df = wrapped_customer_related_df.merge(cudf.DataFrame.from_pandas(acorr_3_mo), how=\"left\", on=\"BASE_CUSTOMER_ID\")\n#     wrapped_customer_related_df.acorr_3_mo.fillna(0, inplace=True)\n#     wrapped_customer_related_df = wrapped_customer_related_df.merge(cudf.DataFrame.from_pandas(acorr_6_mo), how=\"left\", on=\"BASE_CUSTOMER_ID\")\n#     wrapped_customer_related_df.acorr_6_mo.fillna(0, inplace=True)\n#     wrapped_customer_related_df = wrapped_customer_related_df.merge(cudf.DataFrame.from_pandas(acorr_12_mo), how=\"left\", on=\"BASE_CUSTOMER_ID\")\n#     wrapped_customer_related_df.acorr_12_mo.fillna(0, inplace=True)\n\n    ########################################\n    \n    active_car_count = wrapped_customer_related_df.groupby(\"BASE_CUSTOMER_ID\")[\"VEHICLE_ID\"].count().reset_index()\\\n                        .rename(columns={\"VEHICLE_ID\": \"active_car_count\"})\n    \n    passive_car_count = customer_related_df[wrapped_customer_related_passive_mask].\\\n                                groupby(\"BASE_CUSTOMER_ID\")[\"VEHICLE_ID\"].count().reset_index()\\\n                                .rename(columns={\"VEHICLE_ID\": \"passive_car_count\"})\n    \n    wrapped_customer_related_df = wrapped_customer_related_df.merge(active_car_count, how=\"left\", on=\"BASE_CUSTOMER_ID\")\n    wrapped_customer_related_df = wrapped_customer_related_df.merge(passive_car_count, how=\"left\", on=\"BASE_CUSTOMER_ID\")\n    wrapped_customer_related_df.active_car_count.fillna(0, inplace=True)\n    wrapped_customer_related_df.passive_car_count.fillna(0, inplace=True)\n    \n    wrapped_customer_related_df[\"months_since_own\"] =\\\n                ((date_window[0] - wrapped_customer_related_df.START_DATE)\/np.timedelta64(1, 'M'))\n    \n    # Getting the details of the last car owned by each customer\n    wrapped_vehicle_df = vehicle_df[vehicle_df.VEHICLE_ID.isin(wrapped_customer_related_df.VEHICLE_ID)]\n    # Calculating the 'months since traffic'\n    wrapped_vehicle_df[\"months_since_traffic\"] =\\\n                ((date_window[0] - wrapped_vehicle_df.TRAFFIC_DATE)\/np.timedelta64(1, 'M'))   \n    # Merging car details with 'ruhsat' data\n    wrapped_customer_related_df = wrapped_customer_related_df.merge(wrapped_vehicle_df, how=\"left\", on=\"VEHICLE_ID\")\n    \n    ##########################################\n    # Checking sales of Dogus\n    wrapped_bought_df = bought_df[bought_df.CREATE_DATE < date_window[0]]\n    # Extracting bought age\n    wrapped_bought_df[\"months_since_bought\"] =\\\n                ((date_window[0] - wrapped_bought_df.CREATE_DATE)\/np.timedelta64(1, 'M')).round()\n\n    # Total car bought since that time by each customer\n    total_car_bought_for_base_customer = wrapped_bought_df.groupby(\"BASE_CUSTOMER_ID\")\\\n                                    [\"months_since_bought\"].count().reset_index()\\\n                                    .rename(columns={\"months_since_bought\": \"total_car_bought_from_dogus\"})\n    wrapped_customer_related_df = wrapped_customer_related_df.merge(total_car_bought_for_base_customer, how=\"left\", on='BASE_CUSTOMER_ID')\n    wrapped_customer_related_df.total_car_bought_from_dogus.fillna(0, inplace=True)\n\n    # Checking if the car is bought from Dogus\n    customer_vehicle_bought_duos = wrapped_bought_df.groupby(['BASE_CUSTOMER_ID', 'VEHICLE_ID']).size().reset_index()\\\n                                    .rename(columns={0: \"bought_from_dogus\"})\n    wrapped_customer_related_df = wrapped_customer_related_df.merge(customer_vehicle_bought_duos, how=\"left\", on=['BASE_CUSTOMER_ID', 'VEHICLE_ID'])\n    wrapped_customer_related_df.bought_from_dogus.fillna(0, inplace=True)\n    \n\n    #########################\n    # Regresyon tipi tahmin de denendi fakat bir ba\u015far\u0131 g\u00f6zlenmedi.\n    \n    # Regression Label for SF\n#     reg_wrapped_sales_df_first = sales_df[sales_df.SF_CREATE_DATE >= date_window[0]]\n#     reg_wrapped_sales_df_first[\"next_sf_months_from_now\"] =\\\n#                 ((reg_wrapped_sales_df_first.SF_CREATE_DATE - date_window[0])\/np.timedelta64(1, 'M'))\n#     closest_next_sf_for_base_customer = reg_wrapped_sales_df_first.sort_values(by='next_sf_months_from_now', ascending=True)\\\n#                         .drop_duplicates(subset='BASE_CUSTOMER_ID', keep=\"first\")\\\n#                         .drop(\"SF_CREATE_DATE\", 1)[[\"BASE_CUSTOMER_ID\", \"next_sf_months_from_now\"]] \n#     wrapped_customer_related_df = wrapped_customer_related_df.merge(closest_next_sf_for_base_customer, how=\"left\", on='BASE_CUSTOMER_ID')\n    #wrapped_customer_related_df.next_sf_months_from_now.fillna(1000, inplace=True)\n    #########################\n    \n    # Month since last sale-file for each customer\n    wrapped_sales_df_first = sales_df[sales_df.SF_CREATE_DATE < date_window[0]]\n    wrapped_sales_df_first[\"months_since_sf\"] =\\\n                ((date_window[0] - wrapped_sales_df_first.SF_CREATE_DATE)\/np.timedelta64(1, 'M'))\n    \n    since_last_sf_for_base_customer = wrapped_sales_df_first.sort_values(by='months_since_sf', ascending=True)\\\n                        .drop_duplicates(subset='BASE_CUSTOMER_ID', keep=\"first\")\\\n                        .rename(columns={\"months_since_sf\": \"since_sf_in_months\"})\n    \n    # Month number of the last sales-file creation\n    since_last_sf_for_base_customer[\"last_sf_monthnum\"] = since_last_sf_for_base_customer.SF_CREATE_DATE.dt.month\n    since_last_sf_for_base_customer = since_last_sf_for_base_customer.\\\n                                        drop(labels=\"SF_CREATE_DATE\", axis=1)[[\"BASE_CUSTOMER_ID\", \"since_sf_in_months\", \"last_sf_monthnum\"]]\n    \n    wrapped_customer_related_df = wrapped_customer_related_df.merge(since_last_sf_for_base_customer, how=\"left\", on='BASE_CUSTOMER_ID')\n    wrapped_customer_related_df.since_sf_in_months.fillna(-1, inplace=True)\n    wrapped_customer_related_df.last_sf_monthnum.fillna(-1, inplace=True)\n\n    # Total amount of sale-file for each customer\n    total_sf_for_base_customer = wrapped_sales_df_first.groupby(\"BASE_CUSTOMER_ID\")\\\n                                [\"months_since_sf\"].count().reset_index()\\\n                                .rename(columns={\"months_since_sf\": \"total_sf_for_base_customer\"})\n    wrapped_customer_related_df = wrapped_customer_related_df.merge(total_sf_for_base_customer, how=\"left\", on='BASE_CUSTOMER_ID')\n    wrapped_customer_related_df.total_sf_for_base_customer.fillna(0, inplace=True)\n\n    \n    #########################\n\n    # Mean car amount per sale-file for each customer\n    mean_car_amount_sf_for_base_customer = wrapped_sales_df_first.groupby(\"BASE_CUSTOMER_ID\")\\\n                                [\"car_count_per_salefile\"].mean().reset_index()\\\n                                .rename(columns={\"car_count_per_salefile\": \"mean_car_amount_sf_for_base_customer\"})\n    wrapped_customer_related_df = wrapped_customer_related_df.merge(mean_car_amount_sf_for_base_customer, how=\"left\", on='BASE_CUSTOMER_ID')\n    wrapped_customer_related_df.mean_car_amount_sf_for_base_customer.fillna(0, inplace=True)\n    #########################\n    \n    ## EXTRACTING SERVICE STATS \n    wrapped_service_df = service_df[(service_df.CREATE_DATE < date_window[0]) &\\\n                                   service_df[\"VEHICLE_ID\"].isin(wrapped_vehicle_df[\"VEHICLE_ID\"])]\n\n    wrapped_service_df[\"months_since_sanayi\"] =\\\n            ((date_window[0] - wrapped_service_df.CREATE_DATE)\/np.timedelta64(1, 'M'))\n    \n    \n    total_service_payment = wrapped_service_df[wrapped_service_df[\"IS_MAINTENANCE\"]==\"Servis\"]\\\n                            .groupby(\"VEHICLE_ID\")[\"TOTAL_AMOUNT_TL\"].sum().reset_index()\\\n                            .rename(columns={\"TOTAL_AMOUNT_TL\": \"total_service_payment\"})\n    count_service_payment = wrapped_service_df[wrapped_service_df[\"IS_MAINTENANCE\"]==\"Servis\"]\\\n                            .groupby(\"VEHICLE_ID\")[\"TOTAL_AMOUNT_TL\"].count().reset_index()\\\n                            .rename(columns={\"TOTAL_AMOUNT_TL\": \"count_service_payment\"})    \n    months_since_last_service_payment = wrapped_service_df[wrapped_service_df[\"IS_MAINTENANCE\"]==\"Servis\"]\\\n                            .groupby(\"VEHICLE_ID\")[\"months_since_sanayi\"].min().reset_index()\\\n                            .rename(columns={\"months_since_sanayi\": \"months_since_last_service_payment\"})  \n    \n    total_care_payment = wrapped_service_df[wrapped_service_df[\"IS_MAINTENANCE\"]==\"Bak\u0131m\"]\\\n                            .groupby(\"VEHICLE_ID\")[\"TOTAL_AMOUNT_TL\"].sum().reset_index()\\\n                            .rename(columns={\"TOTAL_AMOUNT_TL\": \"total_care_payment\"})\n    count_care_payment = wrapped_service_df[wrapped_service_df[\"IS_MAINTENANCE\"]==\"Bak\u0131m\"]\\\n                            .groupby(\"VEHICLE_ID\")[\"TOTAL_AMOUNT_TL\"].count().reset_index()\\\n                            .rename(columns={\"TOTAL_AMOUNT_TL\": \"count_care_payment\"})    \n    months_since_last_care_payment = wrapped_service_df[wrapped_service_df[\"IS_MAINTENANCE\"]==\"Bak\u0131m\"]\\\n                            .groupby(\"VEHICLE_ID\")[\"months_since_sanayi\"].min().reset_index()\\\n                            .rename(columns={\"months_since_sanayi\": \"months_since_last_care_payment\"})  \n    \n    \n    wrapped_customer_related_df = wrapped_customer_related_df.merge(total_service_payment, how=\"left\", on=[\"VEHICLE_ID\"])\n    wrapped_customer_related_df = wrapped_customer_related_df.merge(count_service_payment, how=\"left\", on=[\"VEHICLE_ID\"])\n    wrapped_customer_related_df = wrapped_customer_related_df.merge(months_since_last_service_payment, how=\"left\", on=[\"VEHICLE_ID\"])\n    \n    wrapped_customer_related_df = wrapped_customer_related_df.merge(total_care_payment, how=\"left\", on=[\"VEHICLE_ID\"])\n    wrapped_customer_related_df = wrapped_customer_related_df.merge(count_care_payment, how=\"left\", on=[\"VEHICLE_ID\"])\n    wrapped_customer_related_df = wrapped_customer_related_df.merge(months_since_last_care_payment, how=\"left\", on=[\"VEHICLE_ID\"])\n    \n    wrapped_customer_related_df.total_service_payment.fillna(0, inplace=True)\n    wrapped_customer_related_df.count_service_payment.fillna(0, inplace=True)\n    wrapped_customer_related_df.total_care_payment.fillna(0, inplace=True)\n    wrapped_customer_related_df.count_care_payment.fillna(0, inplace=True)\n    \n    wrapped_customer_related_df.months_since_last_service_payment.fillna(-1, inplace=True)\n    wrapped_customer_related_df.months_since_last_care_payment.fillna(-1, inplace=True)\n    \n    #########################\n    \n    ## INCLUDING CUSTOMER DETAILS\n    wrapped_customer_related_df = wrapped_customer_related_df.merge(customer_df, how=\"left\", on=\"BASE_CUSTOMER_ID\")\n    #########################\n    \n    ## AGE\n    wrapped_customer_related_df[\"customer_age\"] = date_window[0].year - wrapped_customer_related_df.mod_birth\n    wrapped_customer_related_df.drop(labels=\"mod_birth\", axis=1, inplace=True)\n    wrapped_customer_related_df[\"customer_age\"].fillna(-1, inplace=True)\n    #########################    \n    \n    ## EXTRACTING THE QUERY DATETIME INFO  \n    wrapped_customer_related_df[\"query_Month\"] = date_window[0].month\n    wrapped_customer_related_df[\"query_Year\"] = date_window[0].year\n    #########################\n\n    ## EXTRA FEATURES\n    \n    # wrapped_customer_related_df[\"CORONA\"] = 1 if (date_window[0]>pd.Timestamp(\"20200401\")) else 0\n    \n    usd_df[\"day\"] = 25\n    usd_time_index = pd.to_datetime(usd_df.to_pandas()[[\"year\", \"month\", \"day\"]])\n\n    wrapped_customer_related_df[\"usd_diff\"] = usd_df[\"usd_diff\"][usd_time_index<date_window[0]].values[-1]\n    wrapped_customer_related_df[\"usd_raw_diff\"] = usd_df[\"usd_raw_diff\"][usd_time_index<date_window[0]].values[-1]\n    wrapped_customer_related_df[\"usd_Close\"] = usd_df[\"Close\"][usd_time_index<date_window[0]].values[-1]\n    \n    wrapped_customer_related_df[\"TUFE_DIFF\"] = tufe_df[\"TUFE_DIFF\"][tufe_df.TIME<date_window[0]].values[-1]\n    wrapped_customer_related_df[\"TUFE_RAW_DIFF\"] = tufe_df[\"TUFE_RAW_DIFF\"][tufe_df.TIME<date_window[0]].values[-1]\n    \n    #########################\n    \n    wrapped_customer_related_df = wrapped_customer_related_df.to_pandas()  \n        \n    sf_period = wrapped_sales_df_first.sort_values(by='months_since_sf', ascending=True).to_pandas()\\\n                                    .groupby(\"BASE_CUSTOMER_ID\")[\"months_since_sf\"]\\\n                                    .apply(lambda x: np_mean(x)).reset_index()\\\n                                    .rename(columns={\"months_since_sf\" : \"sf_period\"})\n    \n    wrapped_customer_related_df = wrapped_customer_related_df.merge(sf_period, how=\"left\", on=\"BASE_CUSTOMER_ID\")\n    wrapped_customer_related_df[\"sf_period\"].fillna(-1, inplace=True)\n    \n    ## SETTING THE GT LABEL\n    wrapped_sales_df = sales_df[(sales_df[\"SF_CREATE_DATE\"] >= date_window[0]) &\\\n                               (sales_df[\"SF_CREATE_DATE\"] < date_window[1])]\n    \n    wrapped_customer_related_df[\"WILL_CREATE_SF\"] = wrapped_customer_related_df[\"BASE_CUSTOMER_ID\"].isin(wrapped_sales_df[\"BASE_CUSTOMER_ID\"].to_pandas()).astype(int)\n\n    wrapped_customer_related_df.drop(\n    labels=[\n        \"START_DATE\",\n        \"END_DATE\",\n        \"FK_RELATION_STATUS_EXPLANATION\",\n        \"TRAFFIC_DATE\",\n        \"BASEMODEL_CODE\",\n    ], axis=1, inplace=True)\n    \n    wrapped_customer_related_df = reduce_memory(wrapped_customer_related_df)\n    \n    # Leaving only the last-car-bought for each customer\n    wrapped_customer_related_df = wrapped_customer_related_df.sort_values(by='months_since_own', ascending=True)\\\n                    .drop_duplicates(subset='BASE_CUSTOMER_ID', keep=\"first\")\n      \n    whole_data_df.append(wrapped_customer_related_df)\n    \n    del wrapped_vehicle_df\n    ","63f668f1":"whole_data_df = pd.concat(whole_data_df, ignore_index=True)","eea6d115":"banned_months = [4,5,6]\nbanned_years = [2021]\n\ntrain_mask = ~((whole_data_df[\"query_Month\"].isin(banned_months)) & (whole_data_df[\"query_Year\"].isin(banned_years)))\ntest_mask = ((whole_data_df[\"query_Month\"].isin([6])) & (whole_data_df[\"query_Year\"].isin([2021])))","4e04990f":"train_df = whole_data_df[train_mask]","69c532ba":"subm_df = pd.read_csv(\"..\/input\/dogus-datathon-otomotiv\/sample_submission.csv\")\ntest_df = whole_data_df[test_mask]\ntest_df = test_df[test_df.BASE_CUSTOMER_ID.isin(subm_df.Id.values)]","874527d4":"time_df = pd.DataFrame({'year': train_df.query_Year,\n                   'month': train_df.query_Month,\n                       'day': 1})\ntime_index = pd.to_datetime(time_df).astype(int).values","7e3ae0af":"dropcols = [\n            \"VEHICLE_ID\",\n            \"query_Year\",\n            \"query_Month\"\n           ]\n\ncids = train_df[\"BASE_CUSTOMER_ID\"]\n\ntrain_df.drop(dropcols\\\n                    + [\"BASE_CUSTOMER_ID\"]\\\n                    ,1,inplace=True)","e2b82b6a":"catcols = [\n    \"BRAND_CODE\",\n   \"MOTOR_GAS_TYPE\",\n   \"GEAR_BOX_TYPE\",\n    \"mod_marital\",\n    \"mod_occupation\",\n    \"mod_city\",\n    \"mod_gender\",\n    \"TOPMODEL_CODE\"\n]\ntrain_df[catcols] = train_df[catcols].astype(str).astype('category')\ntrain_df[\"WILL_CREATE_SF\"] = train_df[\"WILL_CREATE_SF\"].astype(int)","b6c81d44":"for c in train_df.columns:\n    col_type = train_df[c].dtype\n    print(\"'\"+c+\"'\", \":\", col_type)","0fea215b":"X = train_df.drop([\"WILL_CREATE_SF\"], 1)\ny = train_df[\"WILL_CREATE_SF\"]\n\ndel train_df","562226be":"time_indexes = np.unique(time_index)\n\ncustom_ts_splits = []\nTS_CV_COUNT = 5\n\nto_be_iterated = time_indexes[:-7][-TS_CV_COUNT:]\n\nfor time_index_unq_i, time_index_unq in tqdm(enumerate(to_be_iterated)):\n    train_time_idxs = time_indexes[time_indexes<=time_index_unq]\n    \n    train_time_idxs = train_time_idxs[list(range(time_index_unq_i,len(train_time_idxs),5))]\n    test_time_idxs = time_indexes[-4:]\n\n    train_idxs = X[np.isin(time_index, train_time_idxs)].index\n    test_idxs = X[np.isin(time_index, test_time_idxs)].index\n    custom_ts_splits.append((train_idxs, test_idxs))\n    \ncustom_ts_splits","eaf780b6":"# import optuna\n# from catboost import Pool, cv\n\n# cat_feats_list = np.argwhere(X.columns.isin(catcols)).flatten()\n# cv_dataset = Pool(data=X,\n#                   label=y,\n#                   cat_features=cat_feats_list)","41cd3209":"# def objective(trial):\n    \n#     param = {\n#         'iterations' : trial.suggest_int('iterations', 1, 1),                         \n#         'depth' : trial.suggest_int('depth', 4, 12),                                                   \n#         'random_strength' :trial.suggest_int('random_strength', 0, 100),                       \n#         'bagging_temperature' :trial.suggest_loguniform('bagging_temperature', 0.01, 100.00),\n#         'learning_rate' :trial.suggest_loguniform('learning_rate', 1e-3, 1e-1),\n#         \"loss_function\": trial.suggest_categorical(\"loss_function\", [\"Logloss\", \"CrossEntropy\"]),\n#         \"bootstrap_type\": trial.suggest_categorical(\"bootstrap_type\", [\"Poisson\"]),     \n#         \"border_count\": trial.suggest_int('border_count', 8, 255),\n#         #\"colsample_bylevel\": trial.suggest_uniform(\"colsample_bylevel\", 0.1, 1),\n#         'od_type' : \"Iter\",\n#         'od_wait' : 30,\n#     }\n    \n    \n#     params = {\n#       \"eval_metric\": \"AUC\",\n#        \"hints\":\"skip_train~false\",\n#       \"auto_class_weights\": \"Balanced\",\n#         \"task_type\":\"GPU\",\n#      }\n    \n#     param.update(params)\n    \n#     try:\n#         shutil.rmtree(\".\/catboost_info\")\n#     except:\n#         pass\n        \n#     scores = cv(cv_dataset,\n                \n#             param,\n#             as_pandas=True,\n#             fold_count=3,        \n#             plot=False, verbose=True)\n    \n\n#     return scores['test-F1-mean'].max()\n\n# study = optuna.create_study(direction=\"maximize\")\n# study.optimize(objective, n_trials=60, timeout=100000)","765aa631":"cat_feats_list = np.argwhere(X.columns.isin(catcols)).flatten()\n\ncb_params = {\n    \"eval_metric\": \"AUC\",\n    \"auto_class_weights\": \"Balanced\",\n    \"random_seed\": 1773,\n    \"task_type\":\"GPU\",\n    \"loss_function\": \"Logloss\",\n    \"bootstrap_type\": \"Poisson\",\n    'od_type' : \"Iter\",\n    'od_wait' : 300,\n    'iterations': 20000, \n    'max_ctr_complexity':1,\n}\n\nscore_list = []\ncv_model_list = []\n\nfor fold_num, (train_index, test_index) in enumerate(custom_ts_splits):\n    \n    try:\n        shutil.rmtree(\".\/catboost_info\")\n    except:\n        pass\n    \n    X_train, X_val = X.iloc[train_index], X.iloc[test_index]\n    y_train, y_val = y.iloc[train_index], y.iloc[test_index]\n        \n    cb_model = CatBoostClassifier(\n        cat_features=cat_feats_list,\n        **cb_params)\n    \n    \n    cb_model.fit(\n        X = X_train,\n        y = y_train,\n        eval_set=(X_val, y_val),\n        use_best_model=True\n    )\n\n    val_pred_cb = cb_model.predict_proba(X_val)[:,1]\n    val_auc = roc_auc_score(y_val, val_pred_cb)\n    print(f\"CB AUC Fold-{fold_num} : {val_auc}\")\n    \n    score_list.append(val_auc) \n    cv_model_list.append(cb_model) ","17000d68":"imps = []\n\n\nfor model in cv_model_list:\n    imps.append(model.get_feature_importance())\n\nimps = np.array(imps).mean(axis=0)\n\n\npd.DataFrame({'feature_importance': imps, \n              'feature_names': X.columns}).sort_values(by=['feature_importance'], \n                                                           ascending=False)","885a94e0":"test_df[catcols] = test_df[catcols].astype(str).astype('category')\ntest_df.drop(dropcols,1,inplace=True)\ntest_df = test_df.rename(columns={\"BASE_CUSTOMER_ID\": \"Id\"})","9ebdab33":"preds = None\n\nsplit_list = list(range(1,TS_CV_COUNT+1))\n\ntotal_weight = sum(split_list)\n\nfor model_i in range(len(cv_model_list)):\n    \n    cb_pred = cv_model_list[model_i].predict_proba(test_df[X.columns])[:,1]\n    weight = 1 \/ TS_CV_COUNT\n    print(\"Model\", model_i, \"| Predicting with weight:\", weight)\n    if type(preds) is not np.ndarray:\n        preds = cb_pred * weight\n    else:\n        preds += cb_pred * weight\n\ntest_df[\"Expected\"] = preds","0ce05515":"subm_df = pd.read_csv(\"..\/input\/dogus-datathon-otomotiv\/sample_submission.csv\").drop(\"Expected\", 1)\nsubm_df = subm_df.merge(test_df[[\"Id\", \"Expected\"]], how='left', on='Id')","5d21eb2f":"subm_df.to_csv(\"submission_out.csv\", index=False)","b578836f":"subm_df[\"Expected\"].hist(bins=50, alpha=0.8)","3bde1b27":"## Verinin e\u011fitim ve test olarak b\u00f6l\u00fcnmesi\nSonraki 3 ay hakk\u0131nda ground-truth i\u00e7ermeyen d\u00f6nemler e\u011fitim setine al\u0131nmad\u0131. Test verisi i\u00e7in sadece submission dosyas\u0131ndaki kullan\u0131c\u0131lar\u0131n may\u0131s ay\u0131 verileri kullan\u0131ld\u0131.","3f7b5658":"B\u00fct\u00fcn verilerin okunmas\u0131","5c36f7db":"Verilerdeki CUSTOMER_ID'lerin unique BASE_CUSTOMER_ID ile de\u011fi\u015ftirilmesi.","a7bd35a1":"Input ve prediction time-series splitlerin tan\u0131mlanmas\u0131.","ce66741a":"## Tahminlerin olas\u0131l\u0131k da\u011f\u0131l\u0131m\u0131","64930a1f":"## Ortalama Feature Importance","9266c0cf":"## Time-Series Splitting (CV)\n\nVeri 3 ayl\u0131k gelece\u011fe ait sat\u0131n alma durumunu i\u00e7erece\u011fi i\u00e7in, b\u00f6l\u00fcnecek zaman dilimlerinin 3 ayl\u0131k GT k\u0131s\u0131mlar\u0131 kesi\u015fmemelidir. Aksi takdirde information-leak ya\u015fan\u0131r. A\u015fa\u011f\u0131daki \u00f6zelle\u015ftirilmi\u015f splitteri yazd\u0131\u011f\u0131mda train-test-public korelasyonu b\u00fcy\u00fck \u00f6l\u00e7\u00fcde iyile\u015fti.","c2c1e0b4":"## Optuna Hyper-Parameter Optimizasyonu","6a71c44c":"Verilerdeki tarih s\u00fctunlar\u0131 veri tipi olarak da tarih'e cast edildi.","dcf1d238":"# E\u011fitim verisinin olu\u015fturulmas\u0131","c0c72b16":"## CV ile tahmin","e2febb7e":"# Tahmin A\u015famas\u0131","8645153d":"## CatBoost E\u011fitimi"}}