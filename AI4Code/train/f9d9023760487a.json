{"cell_type":{"1fffa7e2":"code","5c81b56a":"code","02282ca2":"code","250b153a":"code","4e9c5624":"code","f67abbdd":"code","0adfde3b":"code","ab091300":"code","18cdce91":"code","449c4577":"code","57e84d58":"code","d1751f88":"code","5e653748":"code","774baa9a":"code","045cbb59":"code","64e73e5b":"code","62f3f7d5":"code","51f2ea62":"code","1eb5d996":"code","66203cba":"code","fda33a09":"code","5cea0324":"code","cab684e2":"code","ed53c7c5":"code","d8d97082":"code","6f41f2f7":"code","803ba61f":"code","2c72783e":"code","977e711c":"markdown","c996a491":"markdown","50938dd0":"markdown","1625702c":"markdown","2f297d12":"markdown","66ba6c9e":"markdown","a2b38d5c":"markdown","658f359d":"markdown","c9f36cf6":"markdown","73aedf1f":"markdown","19c33580":"markdown","8e048115":"markdown","4b9384ae":"markdown","528d4055":"markdown","263c47c5":"markdown","8c4e44af":"markdown","636f8081":"markdown","7d8c40a4":"markdown","463819be":"markdown","220cf1d7":"markdown"},"source":{"1fffa7e2":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n#Visualization\nimport matplotlib.pyplot as plt\n\n#Machine Learning\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.decomposition import PCA\nfrom sklearn.svm import SVC\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn import metrics\n\n#System\nimport os\nprint(os.listdir(\"..\/input\"))","5c81b56a":"import warnings\nwarnings.filterwarnings('ignore')\nprint(\"Warnings ignored!!\")","02282ca2":"data=np.load(\"..\/input\/olivetti_faces.npy\")\ntarget=np.load(\"..\/input\/olivetti_faces_target.npy\")","250b153a":"print(\"There are {} images in the dataset\".format(len(data)))\nprint(\"There are {} unique targets in the dataset\".format(len(np.unique(target))))\nprint(\"Size of each image is {}x{}\".format(data.shape[1],data.shape[2]))\nprint(\"Pixel values were scaled to [0,1] interval. e.g:{}\".format(data[0][0,:4]))","4e9c5624":"print(\"unique target number:\",np.unique(target))","f67abbdd":"def show_40_distinct_people(images, unique_ids):\n    #Creating 4X10 subplots in  18x9 figure size\n    fig, axarr=plt.subplots(nrows=4, ncols=10, figsize=(18, 9))\n    #For easy iteration flattened 4X10 subplots matrix to 40 array\n    axarr=axarr.flatten()\n    \n    #iterating over user ids\n    for unique_id in unique_ids:\n        image_index=unique_id*10\n        axarr[unique_id].imshow(images[image_index], cmap='gray')\n        axarr[unique_id].set_xticks([])\n        axarr[unique_id].set_yticks([])\n        axarr[unique_id].set_title(\"face id:{}\".format(unique_id))\n    plt.suptitle(\"There are 40 distinct people in the dataset\")","0adfde3b":"show_40_distinct_people(data, np.unique(target))","ab091300":"def show_10_faces_of_n_subject(images, subject_ids):\n    cols=10# each subject has 10 distinct face images\n    rows=(len(subject_ids)*10)\/cols #\n    rows=int(rows)\n    \n    fig, axarr=plt.subplots(nrows=rows, ncols=cols, figsize=(18,9))\n    #axarr=axarr.flatten()\n    \n    for i, subject_id in enumerate(subject_ids):\n        for j in range(cols):\n            image_index=subject_id*10 + j\n            axarr[i,j].imshow(images[image_index], cmap=\"gray\")\n            axarr[i,j].set_xticks([])\n            axarr[i,j].set_yticks([])\n            axarr[i,j].set_title(\"face id:{}\".format(subject_id))\n    ","18cdce91":"#You can playaround subject_ids to see other people faces\nshow_10_faces_of_n_subject(images=data, subject_ids=[0,5, 21, 24, 36])","449c4577":"#We reshape images for machine learnig  model\nX=data.reshape((data.shape[0],data.shape[1]*data.shape[2]))\nprint(\"X shape:\",X.shape)","57e84d58":"X_train, X_test, y_train, y_test=train_test_split(X, target, test_size=0.3, stratify=target, random_state=0)\nprint(\"X_train shape:\",X_train.shape)\nprint(\"y_train shape:{}\".format(y_train.shape))","d1751f88":"y_frame=pd.DataFrame()\ny_frame['subject ids']=y_train\ny_frame.groupby(['subject ids']).size().plot.bar(figsize=(15,8),title=\"Number of Samples for Each Classes\")","5e653748":"import mglearn","774baa9a":"mglearn.plots.plot_pca_illustration()","045cbb59":"from sklearn.decomposition import PCA\npca=PCA(n_components=2)\npca.fit(X)\nX_pca=pca.transform(X)","64e73e5b":"number_of_people=10\nindex_range=number_of_people*10\nfig=plt.figure(figsize=(10,8))\nax=fig.add_subplot(1,1,1)\nscatter=ax.scatter(X_pca[:index_range,0],\n            X_pca[:index_range,1], \n            c=target[:index_range],\n            s=10,\n           cmap=plt.get_cmap('jet', number_of_people)\n          )\n\nax.set_xlabel(\"First Principle Component\")\nax.set_ylabel(\"Second Principle Component\")\nax.set_title(\"PCA projection of {} people\".format(number_of_people))\n\nfig.colorbar(scatter)","62f3f7d5":"pca=PCA()\npca.fit(X)\n\nplt.figure(1, figsize=(12,8))\n\nplt.plot(pca.explained_variance_, linewidth=2)\n \nplt.xlabel('Components')\nplt.ylabel('Explained Variaces')\nplt.show()","51f2ea62":"n_components=90","1eb5d996":"pca=PCA(n_components=n_components, whiten=True)\npca.fit(X_train)","66203cba":"fig,ax=plt.subplots(1,1,figsize=(8,8))\nax.imshow(pca.mean_.reshape((64,64)), cmap=\"gray\")\nax.set_xticks([])\nax.set_yticks([])\nax.set_title('Average Face')","fda33a09":"number_of_eigenfaces=len(pca.components_)\neigen_faces=pca.components_.reshape((number_of_eigenfaces, data.shape[1], data.shape[2]))\n\ncols=10\nrows=int(number_of_eigenfaces\/cols)\nfig, axarr=plt.subplots(nrows=rows, ncols=cols, figsize=(15,15))\naxarr=axarr.flatten()\nfor i in range(number_of_eigenfaces):\n    axarr[i].imshow(eigen_faces[i],cmap=\"gray\")\n    axarr[i].set_xticks([])\n    axarr[i].set_yticks([])\n    axarr[i].set_title(\"eigen id:{}\".format(i))\nplt.suptitle(\"All Eigen Faces\".format(10*\"=\", 10*\"=\"))","5cea0324":"X_train_pca=pca.transform(X_train)\nX_test_pca=pca.transform(X_test)","cab684e2":"print(X_train_pca.shape)\nprint(y_train.shape)\nprint(X_test_pca.shape)\nprint(y_test.shape)\nprint(y_train)\n\nfrom keras.utils import to_categorical\n\ny_train=to_categorical(y_train,40)\ny_test=to_categorical(y_test,40)\nprint(y_train.shape)","ed53c7c5":"from keras.models import Sequential\nfrom keras.layers import Dense, Activation\n\nfrom keras.optimizers import Adam\nfrom keras.callbacks import ReduceLROnPlateau\nfrom keras.layers import Dropout\n\nfrom keras import regularizers","d8d97082":"model=Sequential()\nmodel.add(Dense(256, activation='relu', input_dim=90))\nmodel.add(Dropout(0.2))\nmodel.add(Dense(128, activation='relu'))\nmodel.add(Dropout(0.1))\nmodel.add(Dense(128, activation='relu'))\nmodel.add(Dropout(0.05))\nmodel.add(Dense(40, activation='softmax'))\n\nepochs=100\nbatch_size=128\nred_lr=ReduceLROnPlateau(monitor='val_acc', factor=0.1, min_delta=0.0001, patience=2, verbose=1)\nmodel.compile(optimizer=Adam(lr=1e-3),loss='categorical_crossentropy',metrics=['accuracy'])\n\nmodel.summary()","6f41f2f7":"History = model.fit(X_train_pca,y_train, epochs = epochs, validation_data = (X_test_pca,y_test),batch_size=128, verbose = 1)","803ba61f":"plt.plot(History.history['acc'])\nplt.plot(History.history['val_acc'])\nplt.title('Model Accuracy')\nplt.ylabel('Accuracy')\nplt.xlabel('Epochs')\nplt.legend(['train', 'test'])\nplt.show()","2c72783e":"plt.plot(History.history['loss'])\nplt.plot(History.history['val_loss'])\nplt.title('Model Loss')\nplt.ylabel('Loss')\nplt.xlabel('Epochs')\nplt.legend(['train', 'test'])\nplt.show()","977e711c":"[Go to Contents Menu](#0.)  |  Quick Links: [1. ](#1.)|[3.](#2.)|[3.1. ](#2.1.)|[3.2.](#2.2.)|[4.](#3.)|[4.1.](#3.1.)|[4.2.](#3.2.)|[4.3.](#3.3.)|[4.4. ](#3.4.) [4.5. ](#3.5.)|[4.6. ](#3.5e1.)|[4.7. ](#3.6.)|[4.8. ](#3.7.)|[4.9. ](#3.8.)|[4.10.](#3.9.)|[4.11.](#3.10.)|[4.12. ](#3.11.)\n\n<a class=\"anchor\" id=\"3.1.\"><\/a>**4.1. Split data and target into Random train and test Subsets** \n\nThe data set contains 10 face images for each subject. Of the face images, 70 percent will be used for training, 30 percent for testing. Uses stratify feature to have equal number of training and test images for each subject. Thus, there will be 7 training images and 3 test images for each subject. You can play with training and test rates.","c996a491":"Many thanks for upvotes and feedbacks ^ __ ^ !\n\n<a class=\"anchor\" id=\"0.\"><\/a>**Contents**\n* [1. Summary](#1.)\n* [2. Face Recognition](#1e.)\n* [3. Olivetti Face Dataset](#2.)\n* * [3.1. Show 48 Disticnt People in the Olivetti Dataset](#2.1.)\n* * [3.2. Show 10 Face Images of Selected Target](#2.2.)\n* [4. Machine Learning Model fo Face Recognition](#3.)\n* * [4.1. Split data and target into Random train and test Subsets](#3.1.)\n* * [4.2. Principle Component Analysis](#3.2.)\n* * [4.3. PCA Projection of Defined Number of Target](#3.3.)\n* * [4.4. Finding Optimum Number of Principle Component](#3.4.)\n* * [4.5. Show Average Face](#3.5.)\n* * [4.6. Show Eigen Faces](#3.5e1.)\n* * [4.7. Classification Result](#3.6.)\n* * [4.8. More Results](#3.7.)\n* * [4.9. Validated Results](#3.8.)\n* * [4.10. More Validated Results: Leave One Out vross-validation](#3.9.)\n* * [4.11. Hyperparameter Tunning: GridSearcCV](#3.10.)\n* * [4.12. Precision-Recall-ROC Curves](#3.11.)\n* [5. Linear Discriminant Analysis \u0130le Boyut Azaltma](#5.)\n* [6. Machine Learning Automated Workflow: Pipeline](#6.)\n\n[Go to Content Menu](#0.)|[1. ](#1.)|[3.](#2.)|[3.1. ](#2.1.)|[3.2.](#2.2.)|[4.](#3.)|[4.1.](#3.1.)|[4.2.](#3.2.)|[4.3.](#3.3.)|[4.4. ](#3.4.) [4.5. ](#3.5.)|[4.6. ](#3.5e1.)|[4.7. ](#3.6.)|[4.8. ](#3.7.)|[4.9. ](#3.8.)|[4.10.](#3.9.)|[4.11.](#3.10.)|[4.12. ](#3.11.)","50938dd0":"[Go to Contents Menu](#0.)  |  Quick Links: [1. ](#1.)|[3.](#2.)|[3.1. ](#2.1.)|[3.2.](#2.2.)|[4.](#3.)|[4.1.](#3.1.)|[4.2.](#3.2.)|[4.3.](#3.3.)|[4.4. ](#3.4.) [4.5. ](#3.5.)|[4.6. ](#3.5e1.)|[4.7. ](#3.6.)|[4.8. ](#3.7.)|[4.9. ](#3.8.)|[4.10.](#3.9.)|[4.11.](#3.10.)|[4.12. ](#3.11.)\n\n<a class=\"anchor\" id=\"3.\"><\/a>**4. Machine Learning Model fo Face Recognition**\n\nMachine learning models can work on vectors. Since the image data is in the matrix form, it must be converted to a vector.","1625702c":"The above illustration shows a simple example on a synthetic two-dimensional data set. The first drawing shows the original data points colored to distinguish points. The algorithm first proceeds by finding the direction of the maximum variance labeled \"Component 1\". This refers to the direction in which most of the data is associated, or in other words, the properties that are most related to each other.\n\nThen, when the algorithm is orthogonal (at right angle), it finds the direction that contains the most information in the first direction. There are only one possible orientation in two dimensions at a right angle, but there will be many orthogonal directions (infinite) in high dimensional spaces.","2f297d12":"As seen in the photo gallery above, the data set has 40 different person-owned, facial images.","66ba6c9e":"In the figure above, it can be seen that 90 and more PCA components represent the same data. Now let's make the classification process using 90 PCA components.","a2b38d5c":"[Go to Contents Menu](#0.)  |  Quick Links: [1. ](#1.)|[3.](#2.)|[3.1. ](#2.1.)|[3.2.](#2.2.)|[4.](#3.)|[4.1.](#3.1.)|[4.2.](#3.2.)|[4.3.](#3.3.)|[4.4. ](#3.4.) [4.5. ](#3.5.)|[4.6. ](#3.5e1.)|[4.7. ](#3.6.)|[4.8. ](#3.7.)|[4.9. ](#3.8.)|[4.10.](#3.9.)|[4.11.](#3.10.)|[4.12. ](#3.11.)\n\n<a  class=\"anchor\" id=\"2.2.\"><\/a>**3.2. Show 10 Face Images of Selected Target** ","658f359d":"[Go to Contents Menu](#0.)  |  Quick Links: [1. ](#1.)|[3.](#2.)|[3.1. ](#2.1.)|[3.2.](#2.2.)|[4.](#3.)|[4.1.](#3.1.)|[4.2.](#3.2.)|[4.3.](#3.3.)|[4.4. ](#3.4.) [4.5. ](#3.5.)|[4.6. ](#3.5e1.)|[4.7. ](#3.6.)|[4.8. ](#3.7.)|[4.9. ](#3.8.)|[4.10.](#3.9.)|[4.11.](#3.10.)|[4.12. ](#3.11.)\n\n<a class=\"anchor\" id=\"3.5.\"><\/a>**4.5. Show Average Face**  ","c9f36cf6":"[Go to Contents Menu](#0.)  |  Quick Links: [1. ](#1.)|[3.](#2.)|[3.1. ](#2.1.)|[3.2.](#2.2.)|[4.](#3.)|[4.1.](#3.1.)|[4.2.](#3.2.)|[4.3.](#3.3.)|[4.4. ](#3.4.) [4.5. ](#3.5.)|[4.6. ](#3.5e1.)|[4.7. ](#3.6.)|[4.8. ](#3.7.)|[4.9. ](#3.8.)|[4.10.](#3.9.)|[4.11.](#3.10.)|[4.12. ](#3.11.)\n\n<a class=\"anchor\" id=\"3.2.\"><\/a>**4.2.Principle Component Analysis**\n\nMachine learning methods are divided into two: supervised learning and unsupervised learning. In supervised learning, the data set is divided into two main parts: 'data' and 'output'. The data holds the values of the sample in the data set, while the 'output' holds the class (for classification) or the target value (for regression). In unsupervised learning, the data set consists of only the data section.\n\nNon-supervised learning is generally divided into two: data transformation and clustering. In this study, the transformation of the data will be carried out using unsupervised learning. Unsupervised transformation methods allow for easier interpretation of data by computers and people.\n\nThe most common unsupervised transformation  applications is to reduce data size. In the size reduction process, the dimension of the data reduced. \n\nPrinciple Component Analysis (PCA) is a method that allows data to be represented in a lesser size. According to this method, the data is transformed to new components and the size of the data is reduced by selecting the most important components.","73aedf1f":"[Go to Contents Menu](#0.)  |  Quick Links: [1. ](#1.)|[3.](#2.)|[3.1. ](#2.1.)|[3.2.](#2.2.)|[4.](#3.)|[4.1.](#3.1.)|[4.2.](#3.2.)|[4.3.](#3.3.)|[4.4. ](#3.4.) [4.5. ](#3.5.)|[4.6. ](#3.5e1.)|[4.7. ](#3.6.)|[4.8. ](#3.7.)|[4.9. ](#3.8.)|[4.10.](#3.9.)|[4.11.](#3.10.)|[4.12. ](#3.11.)\n\n<a class=\"anchor\" id=\"3.6.\"><\/a>**4.7. Classification Results**  ","19c33580":"[Go to Contents Menu](#0.)  |  Quick Links: [1. ](#1.)|[3.](#2.)|[3.1. ](#2.1.)|[3.2.](#2.2.)|[4.](#3.)|[4.1.](#3.1.)|[4.2.](#3.2.)|[4.3.](#3.3.)|[4.4. ](#3.4.) [4.5. ](#3.5.)|[4.6. ](#3.5e1.)|[4.7. ](#3.6.)|[4.8. ](#3.7.)|[4.9. ](#3.8.)|[4.10.](#3.9.)|[4.11.](#3.10.)|[4.12. ](#3.11.)\n\n<a class=\"anchor\" id=\"3.5e1.\"><\/a>**4.6. Show Eigen Faces** ","8e048115":"Each face of a subject has different characteristic in context of varying lighting, facial express and facial detail(glasses, beard)","4b9384ae":"Let's verify above information","528d4055":"<a class=\"anchor\" id=\"2.2.\">","263c47c5":"[Go to Contents Menu](#0.)  |  Quick Links: [1. ](#1.)|[3.](#2.)|[3.1. ](#2.1.)|[3.2.](#2.2.)|[4.](#3.)|[4.1.](#3.1.)|[4.2.](#3.2.)|[4.3.](#3.3.)|[4.4. ](#3.4.) [4.5. ](#3.5.)|[4.6. ](#3.5e1.)|[4.7. ](#3.6.)|[4.8. ](#3.7.)|[4.9. ](#3.8.)|[4.10.](#3.9.)|[4.11.](#3.10.)|[4.12. ](#3.11.)\n\n<a class=\"anchor\" id=\"3.3.\"><\/a>**4.3. PCA Projection of Defined Number of Target**","8c4e44af":"[Go to Contents Menu](#0.)  |  Quick Links: [1. ](#1.)|[3.](#2.)|[3.1. ](#2.1.)|[3.2.](#2.2.)|[4.](#3.)|[4.1.](#3.1.)|[4.2.](#3.2.)|[4.3.](#3.3.)|[4.4. ](#3.4.) [4.5. ](#3.5.)|[4.6. ](#3.5e1.)|[4.7. ](#3.6.)|[4.8. ](#3.7.)|[4.9. ](#3.8.)|[4.10.](#3.9.)|[4.11.](#3.10.)|[4.12. ](#3.11.)\n\n<a class=\"anchor\" id=\"1e.\"><\/a>**2.Face Recognition**\n\nThe first study on automatic facial recognition systems was performed by Bledsoe between 1964 and 1966. This study was semi-automatic. The feature points on the face are determined manually and placed in the table called RAND. Then, a computer would perform the recognition process by classifying these points. However, a fully functional facial recognition application was performed in 1977 by Kanade. A feature-based approach was proposed in the study. After this date, two-dimensional (2D) face recognition have studied intensively. Three-dimensional (3D) face studies were started to be made after the 2000s.\n\n3D facial recognition approaches developed in a different way than 2D facial recognition approaches. Therefore, it will be more accurate to categorize in 2D and 3D  when discussing face recognition approaches.\n\nWe can classify the face recognition researches carried out with 2D approach in three categories; analytical (feature-based, local), global (appearance) and hybrid methods. While analytical approaches want to  recognize by comparing the properties of the facial components, global approaches try to achieve a recognition with data derived from all the face. Hybrid approaches, together with local and global approaches, try to obtain data that expresses the face more accurately.\n\nFace recognition performed in this kernel can assessed under global face recognition approaches.\n\nIn analytical approaches, the distance of the determined feature points and the angles between them, the shape of the facial features or the variables containing the regional features are obtained from the face image are used in face recognition. Analytical methods examine the face images in two different ways according to the pattern and geometrical properties. In these methods, the face image is represented by smaller size data, so the big data size problem that increases the computation cost in face recognition  is solved.\n\nGlobal-based methods are applied to face recognition by researchers because they perform facial recognition without feature extraction which is troublesome in feature based methods. Globally based methods have been used in face recognition since the 1990s, since they significantly improve facial recognition efficiency. Kirby and Sirovich (1990) first developed a method known as Eigenface, which is used in facial representation and recognition based on Principal Component Analysis . With this method, Turk and Pentland transformed the entire face image into vectors and computed eigenfaces with a set of samples. PCA was able to obtain data representing the face at the optimum level with the data obtained from the image. The different facial and illumination levels of the same person were evaluated as the weakness point of PCA.\n\nThe face recognition performend in this kernel totally based on Turk and Pentland work. ","636f8081":"[Go to Contents Menu](#0.)  |  Quick Links: [1. ](#1.)|[3.](#2.)|[3.1. ](#2.1.)|[3.2.](#2.2.)|[4.](#3.)|[4.1.](#3.1.)|[4.2.](#3.2.)|[4.3.](#3.3.)|[4.4. ](#3.4.) [4.5. ](#3.5.)|[4.6. ](#3.5e1.)|[4.7. ](#3.6.)|[4.8. ](#3.7.)|[4.9. ](#3.8.)|[4.10.](#3.9.)|[4.11.](#3.10.)|[4.12. ](#3.11.)\n\n<a class=\"anchor\" id=\"2.\"><\/a>**3. Olivetti Dataset** \n\nBrief information about Olivetti Dataset:\n\n* Face images taken between April 1992 and April 1994.\n* There are ten different image of each of 40 distinct people\n* There are 400 face images in the dataset\n* Face images were taken at different times, variying ligthing, facial express and facial detail\n* All face images have black background\n* The images are gray level\n* Size of each image is 64x64\n* Image pixel values were scaled to [0, 1] interval\n* Names of 40 people were encoded to an integer from 0 to 39\n","7d8c40a4":"[Go to Contents Menu](#0.)  |  Quick Links: [1. ](#1.)|[3.](#2.)|[3.1. ](#2.1.)|[3.2.](#2.2.)|[4.](#3.)|[4.1.](#3.1.)|[4.2.](#3.2.)|[4.3.](#3.3.)|[4.4. ](#3.4.) [4.5. ](#3.5.)|[4.6. ](#3.5e1.)|[4.7. ](#3.6.)|[4.8. ](#3.7.)|[4.9. ](#3.8.)|[4.10.](#3.9.)|[4.11.](#3.10.)|[4.12. ](#3.11.)\n\n<a class=\"anchor\" id=\"1.\"><\/a>**1. Summary** \n\nIn this study, face recognition was performed using the face images in the Olivetti data set. The steps for face recognition are as follows:\n* Principal  components of face images were obtained by PCA.\n* Adequate number of principal  components determined\n* According to three different classification models, accuracy score obtained.\n* According to three different classification models, cross-validation accuracy score were obtained.\n* Parameter optimization of the best model has been made.","463819be":"[Go to Contents Menu](#0.)  |  Quick Links: [1. ](#1.)|[3.](#2.)|[3.1. ](#2.1.)|[3.2.](#2.2.)|[4.](#3.)|[4.1.](#3.1.)|[4.2.](#3.2.)|[4.3.](#3.3.)|[4.4. ](#3.4.) [4.5. ](#3.5.)|[4.6. ](#3.5e1.)|[4.7. ](#3.6.)|[4.8. ](#3.7.)|[4.9. ](#3.8.)|[4.10.](#3.9.)|[4.11.](#3.10.)|[4.12. ](#3.11.)\n\n<a class=\"anchor\" id=\"2.1.\"><\/a>**3.1. Show 48 Disticnt People in the Olivetti Dataset** ","220cf1d7":"[Go to Contents Menu](#0.)  |  Quick Links: [1. ](#1.)|[3.](#2.)|[3.1. ](#2.1.)|[3.2.](#2.2.)|[4.](#3.)|[4.1.](#3.1.)|[4.2.](#3.2.)|[4.3.](#3.3.)|[4.4. ](#3.4.) [4.5. ](#3.5.)|[4.6. ](#3.5e1.)|[4.7. ](#3.6.)|[4.8. ](#3.7.)|[4.9. ](#3.8.)|[4.10.](#3.9.)|[4.11.](#3.10.)|[4.12. ](#3.11.)\n\n<a class=\"anchor\" id=\"3.4.\"><\/a>**4.4. Finding Optimum Number of Principle Component**"}}