{"cell_type":{"42e7ae4a":"code","49b6b40d":"code","261fa068":"code","6e4f4237":"code","e5890eaf":"code","f5d266e3":"code","2be12d87":"code","0ba9efb7":"code","d0c4f549":"code","11493f07":"code","0c5a5efe":"code","3b8506a3":"code","1930b037":"code","5ae0974f":"code","3c1e6a01":"code","40f8adf6":"code","bc021e80":"code","9d71a760":"code","dd1a88f9":"code","9ea631f7":"code","36cf0891":"code","4ca80883":"code","b8485a66":"code","64b02288":"code","06a75ae7":"code","ace89642":"code","ec7c1bdb":"code","2d1cff42":"code","a9783033":"code","c3247b96":"markdown","378d7830":"markdown","7cf8e1bb":"markdown","b8156e98":"markdown","ca6cc4ba":"markdown","bf2a60b3":"markdown","f57a1e84":"markdown","e3a3329f":"markdown","96ddf37a":"markdown","901ad2aa":"markdown","5f476ee7":"markdown","3a0593e7":"markdown"},"source":{"42e7ae4a":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import GroupKFold\nimport gc\nfrom joblib import dump,load\nfrom tqdm.auto import tqdm\npd.set_option('display.max_columns', None)\n\nimport tensorflow as tf\nimport tensorflow_addons as tfa\nfrom tensorflow.keras.utils import Sequence","49b6b40d":"# dummy class to store all the hyperparameters throughout the notebook\nclass Params: pass\nparams=Params()","261fa068":"# # loading original train.csv\n# !pip install datatable > \/dev\/null\n# import datatable as dt\n# train_dt = dt.fread('..\/input\/jane-street-market-prediction\/train.csv')\n# full_df = train_dt.to_pandas()\n\n# # converting float64 columns to float32\n# float64_cols=[col for col in full_df.columns if full_df[col].dtype=='float64']\n# full_df[float64_cols]=full_df[float64_cols].astype('float32')\n\n# # saving in feather format for future use\n# full_df.to_feather('train.feather')","6e4f4237":"train_df=pd.read_feather('..\/input\/js-files\/train.feather')\ntrain_df.info()","e5890eaf":"train_df['resp_sum']=(train_df.resp+train_df.resp_1+train_df.resp_2+train_df.resp_3)\/4","f5d266e3":"targets=['resp','resp_1','resp_2','resp_3','resp_sum']\ntrain_df['targ']=train_df['resp'] #storing the original `resp` away for Utility score calculation\ntrain_df[targets]=train_df[targets]>0 #converting targets for binary classification","2be12d87":"# original features\nfeatures = [col for col in train_df.columns if 'feature' in col]","0ba9efb7":"train_df[train_df.date==0].feature_64.plot(style='.',xlabel='timestamp',ylabel='feature_64',title='Day 0');","d0c4f549":"train_df['part_of_day']=(train_df.feature_64>1).astype('float32')\ntrain_df['min_ts']=train_df.groupby(['date','part_of_day'])['ts_id'].transform(min)\ntrain_df['trades']=train_df.ts_id-train_df.min_ts\n\nparams.grad_64_lag=50\ntrain_df['grad_64']=train_df.feature_64.diff(1)\ntrain_df.loc[train_df.min_ts==train_df.ts_id,'grad_64']=0\ntrain_df['lag_64']=train_df.grad_64.rolling(params.grad_64_lag).sum().fillna(0)\ntrain_df['lag_64']=train_df['lag_64'].astype('float32')\n\ntrain_df.drop(columns=['min_ts','grad_64'],axis=1,inplace=True)","11493f07":"features = features + ['part_of_day','trades','lag_64']","0c5a5efe":"train_df[train_df.date<100].feature_0.cumsum().plot(xlabel='timestamp',ylabel='feature_0',title='Days 0-99')","3b8506a3":"params.lag_features=['feature_0']\nparams.n_lags=20\nparams.lag=200\nlag_cols=[]\nfor i in range(1,params.n_lags):\n    col=f'lag_{i*params.lag}'\n    lag_cols.append(col)\n    train_df[col]=train_df[params.lag_features].rolling(i*params.lag).sum().fillna(0).astype('float32')","1930b037":"features = features + lag_cols","5ae0974f":"train_df=train_df[train_df.weight>0].reset_index(drop=True)","3c1e6a01":"# grouping features based on NaN patterns\ntmp=pd.DataFrame(train_df[features].isnull().sum())\ntmp=tmp[tmp[0]>0].reset_index()\ntmp.columns=['feat','cnt']\ntmp=tmp.sort_values('cnt')\nfeat_groups=dict(tmp.groupby('cnt')['feat'].agg(lambda x:list(x)))\nfeat_groups","40f8adf6":"# store away the names of features that will be used to repeat this step during inference (can be any feature from each group)\nparams.nan_cols=[v[0] for k,v in feat_groups.items() if k>2000]\nparams.nan_cols","bc021e80":"nan_names=[f'nan_{i}' for i in range(len(params.nan_cols))]\ntrain_df[nan_names]= train_df[params.nan_cols].isnull().astype('float32')","9d71a760":"features = features + nan_names","dd1a88f9":"len(features)","9ea631f7":"means=train_df[features].mean().astype('float32')\nstds=train_df[features].std().astype('float32')","36cf0891":"train_df[features]=train_df[features].fillna(means)\ntrain_df[features]=(train_df[features]-means)\/stds","4ca80883":"dump(means,'means.joblib')\ndump(stds,'stds.joblib');","b8485a66":"def create_model(n_in, n_out, layers, dropout_rate, optimizer, metrics):\n    \n    inp = tf.keras.layers.Input(shape = (n_in, ))  \n        \n    x=inp\n    for i,hidden_units in enumerate(layers): \n        x = tf.keras.layers.BatchNormalization()(x)\n        if i>0:   \n            x = tf.keras.layers.Dropout(dropout_rate)(x)    \n        else: \n            x = tf.keras.layers.Dropout(.01)(x)    \n        x = tf.keras.layers.Dense(hidden_units)(x)\n        x = tf.keras.layers.Activation('relu')(x)\n        \n    x = tf.keras.layers.Dense(n_out)(x)\n    out = tf.keras.layers.Activation('sigmoid')(x)\n    \n    model = tf.keras.models.Model(inputs = inp, outputs = out)\n    model.compile(optimizer = optimizer,\n                  loss = tf.keras.losses.BinaryCrossentropy(), \n                  metrics = metrics, \n#                   run_eagerly=True\n                 )\n    \n    return model","64b02288":"def utility_score_bincount(date, weight, resp, action):\n    count_i = len(np.unique(date))\n    Pi = np.bincount(date, weight * resp * action)\n    t = np.sum(Pi) \/ np.sqrt(np.sum(Pi ** 2)) * np.sqrt(250 \/ count_i)\n    u = np.clip(t, 0, 6) * np.sum(Pi)\n    return t,u","06a75ae7":"from sklearn.metrics import roc_auc_score\nclass ValScore(tf.keras.callbacks.Callback):\n    def __init__(self, validation_data,dates,weights,targs):\n        super().__init__()\n        self.X_val, self.y_val = validation_data\n        self.dates, self.weights, self.targs = dates, weights, targs\n\n    def on_epoch_end(self, epoch, logs={}):\n        y_pred=self.model(self.X_val,training = False).numpy()\n        aoc = roc_auc_score(self.y_val, y_pred,average=None)\n        action=(y_pred.mean(1)>0.5).astype('int8')\n        score=utility_score_bincount(self.dates,self.weights,self.targs,action)\n        print(f\"AOC scores: {aoc}, t: {score[0]:.2f}, Utility score: {score[1]:.0f}\")","ace89642":"### model parameters\nparams.layers = [500,350,200]\nparams.dropout_rate = 0.35\n\n###training parameters\nparams.bs = 8192\nparams.lr = 0.002\nparams.epochs = 30 \nparams.wd = 0.02\n\n### adding overall AuC as a metric\n### for early stopping I only look at resp and resp_sum because they start overfitting earlier\nmetrics = [tf.keras.metrics.AUC(label_weights=[1,0,0,0,1],name='auc_10001'),\n           tf.keras.metrics.AUC(name='auc')\n          ] ","ec7c1bdb":"#training loop\n!mkdir models\nparams.n_folds=5\nparams.days_in_group=[50]*3 \nparams.n_runs=len(params.days_in_group)\npred_cols=[f'pred{i}_{j}' for i in range(params.n_runs) for j in range(len(targets))]\ntrain_df[pred_cols]=0\ntrain_df[pred_cols]=train_df[pred_cols].astype('float32')\n\nfor i,days_in_group in enumerate(params.days_in_group):\n    gkf=GroupKFold(params.n_folds)\n    params.n_groups=500\/\/days_in_group\n    train_df['group']=train_df['date']\/\/days_in_group\n    for j,splits in enumerate(gkf.split(train_df[features],train_df[targets],\n                          train_df['group'])):\n        \n        print(f'Run:{i}, Fold:{j}')\n        \n        X_train, X_val = train_df.loc[splits[0], features].values, train_df.loc[splits[1], features].values\n        y_train, y_val = train_df.loc[splits[0], targets].values, train_df.loc[splits[1], targets].values\n      \n        dates=train_df.loc[splits[1],'date'].values\n        weights=train_df.loc[splits[1],'weight'].values\n        targs=train_df.loc[splits[1],'targ'].values\n        cbs=[ValScore((X_val, y_val),dates,weights,targs),\n                 tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.3,\n                              patience=3, verbose=1),\n                 tf.keras.callbacks.EarlyStopping(\n                                monitor='val_auc_10001', patience=5, verbose=1,\n                                mode='max', restore_best_weights=True\n                            )\n            ]\n        \n        model=create_model(len(features), len(targets), params.layers, params.dropout_rate, \n                           optimizer=tfa.optimizers.Lookahead(\n                               tfa.optimizers.LAMB(learning_rate=params.lr,weight_decay_rate=params.wd)\n                           ),\n                           metrics=metrics)\n        model.fit(X_train, y_train, validation_data = (X_val, y_val), epochs = params.epochs, \n                    batch_size = params.bs, validation_batch_size=500_000,\n                    callbacks = [cbs], verbose = 2)           \n        model.save_weights(f'models\/saved_model_{i}_{j}.hdf5')\n        \n        preds=model(X_val,training = False).numpy()\n        train_df.loc[splits[1],[f'pred{i}_{j}' for j in range(len(targets))]]=preds\n        gc.collect()\n        tf.keras.backend.clear_session()      ","2d1cff42":"dump(params,'params.joblib');   ","a9783033":"dump(train_df,'train_df_oof.joblib');   ","c3247b96":"Given the anonymous nature of the dataset, this whole FE logic could very easily be misguided. Still, adding these features resulted in noticeable performance gains across various models and training regimes that I tried.","378d7830":"# Target Engineering","7cf8e1bb":"# Model & Training","b8156e98":"This gave me an idea for a few new features:\n* Binary feature represing part of the trading day (before\/after lunch)\n* Number of trades suggested by JS algorithm earlier today (for the first part of the day) or after lunch (for the second part of the day) - the intution here that together with 'clock' this feature could represent a market condition (e.g. more trade opportunities = more volatility)\n* 'Gradient' of feature_64 with respect to timestamp - similar intuition to the previous point","ca6cc4ba":"Lesson learned in the MoA competition - for a multilabel classification task it might be useful to track the validation metric for each target separately because they might exhibit different overfitting behaviour. It's not possible to notice overfitting of specific targets just looking at the the combined metric. \nI implemented it via callback because I don't need this calculated for the training data, just for validation.","bf2a60b3":"Rows with zero weight do not contribute to the competition metric calculation - I chose to discard them.","f57a1e84":"Another set of features is based on the behaviour of the binary feature_0. There is no obvious pattern there but it's also definitely not completely random. Again, my guess is that it could be indicative of some market condition - this could be used to construct lag features.","e3a3329f":"It has been hypothesized that feature_64 represents some sort of clock:\n* It's always increasing throughout the day \n* Every day it follows very similar pattern\n* There is a gap in the middle that can be interpreted as a lunch break (feature of several Asian markets)","96ddf37a":"Like many others, I have noticed that treating this task as multi-label classification leads to better results compared to trying to predict just one label `resp`. I have made a couple of adjustments compared to most public notebooks:\n* I do not use `resp_4` - my CV always goes down when I try to add it. This could be to some extent explained by the fact (conjecture?) that the time horizon of `resp_4` is longer than that of `resp`.\n* Instead, I add the mean value of `resp`, `resp_1`, `resp_2` and `resp_3` as a separate target which does improve the CV score. This feature can be thought as a proxy for general direction of returns over the whole `resp` time horizon.","901ad2aa":"# Final Data Preprocessing","5f476ee7":"Finally, we need to deal with missing values:\n* Replace NaNs with mean values. I tried other methods (median, `ffill`) but did not see any performance gains.\n* Add new columns indicating if the replacement took place. Missing values follow a pattern - so we only need new columns for groups of features not for each individual feature with missing values.","3a0593e7":"# Feature Engineering"}}