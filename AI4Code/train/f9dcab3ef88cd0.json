{"cell_type":{"e45b5a64":"code","26e995d3":"code","e0a76128":"code","21c41f8d":"code","40acdaae":"code","ba68ee9a":"code","8d289cce":"code","ecb1e969":"code","5a5dc210":"code","66abd692":"code","44351c01":"markdown","332f5ad3":"markdown","3c168b96":"markdown","2806a149":"markdown","b076587b":"markdown","ce5080e8":"markdown","4cc2301f":"markdown","f4bdda05":"markdown","ef2080bf":"markdown","930ee4e0":"markdown","36f363c7":"markdown"},"source":{"e45b5a64":"import numpy as np\nimport pandas as pd \nimport tensorflow as tf\npd.set_option('max.columns',80)\n\npreprocessed_train = pd.read_csv('..\/input\/preprocessed-train-data\/preprocessed_train_data.csv')\ntest = pd.read_csv('..\/input\/preprocessed-test-data\/preprocessed_test_data.csv')\ndisplay(preprocessed_train.head())\n'''Split the train data into features and target'''\nx_train, y_train = preprocessed_train[preprocessed_train.columns[:-1]], preprocessed_train[preprocessed_train.columns[-1]]","26e995d3":"'''Kernel initializer denotes the distribution in which the weights of the neural networks are initialized'''\nmodel = tf.keras.Sequential([\n    tf.keras.layers.Dense(128, kernel_initializer='normal', activation='relu'),\n    tf.keras.layers.Dense(256, kernel_initializer='normal',activation='relu'),\n    tf.keras.layers.Dense(256, kernel_initializer='normal',activation='relu'),\n    tf.keras.layers.Dense(256, kernel_initializer='normal',activation='relu'),\n    tf.keras.layers.Dense(384, kernel_initializer='normal',activation='relu'),\n    tf.keras.layers.Dense(384, kernel_initializer='normal',activation='relu'),\n    tf.keras.layers.Dense(1, kernel_initializer='normal',activation='linear')\n])\n\nloss_fn = tf.keras.losses.MeanSquaredLogarithmicError()\nmodel.compile(loss= loss_fn, optimizer='adam', metrics=[loss_fn])\nmodel.fit(x_train.values, y_train.values, epochs=15, batch_size=64, validation_split = 0.2, verbose =0)","e0a76128":"'''Predict the training data'''\npredictions = model.predict(x_train.values).squeeze()","21c41f8d":"def mean_absolute_error(y_train, predictions):\n    \"\"\"\n    Returns Mean Absolute Error \n    \"\"\"\n    abs_error = abs(y_train - predictions)\n    mean_abs_error = abs_error.mean()\n    return mean_abs_error\n\nmean_absolute_error(y_train, predictions)","40acdaae":"def mean_squared_error(y_train, predictions):\n    \"\"\"\n    Returns Mean Squared Error \n    \"\"\"\n    square_error = (y_train - predictions) ** 2\n    mean_square_error = square_error.mean()\n    return mean_square_error\n\nmean_squared_error(y_train, predictions)","ba68ee9a":"def root_mean_squared_error(y_train, predictions):\n    \"\"\"\n    Returns Root Mean Squared Error\n    \"\"\"\n    square_error = (y_train - predictions) ** 2\n    mean_square_error = square_error.mean()\n    rmse = np.sqrt(mean_square_error)\n    return rmse\n\nroot_mean_squared_error(y_train, predictions)","8d289cce":"def root_mean_squared_log_error(y_train, predictions):\n    \"\"\"\n    Returns Root Mean Square Log Error\n    \"\"\"\n    square_error = (np.log(y_train+1) - np.log(predictions+1)) ** 2\n    mean_square_log_error = square_error.mean()\n    rmsle = np.sqrt(mean_square_log_error)\n    return rmsle\n\nroot_mean_squared_log_error(y_train, predictions)","ecb1e969":"\"\"\"Use Stochastic Gradient Descent\"\"\"\noptimizer = tf.keras.optimizers.SGD()\nmodel.compile(loss= loss_fn, optimizer= optimizer, metrics=[loss_fn])\nmodel.fit(x_train.values, y_train.values, epochs=15, batch_size=64, validation_split = 0.2, verbose = 0)","5a5dc210":"optimizer = tf.keras.optimizers.RMSprop()\nmodel.compile(loss= loss_fn, optimizer= optimizer, metrics=[loss_fn])\nmodel.fit(x_train.values, y_train.values, epochs=15, batch_size=64, validation_split = 0.2, verbose = 0)","66abd692":"optimizer = tf.keras.optimizers.Adam()\nmodel.compile(loss= loss_fn, optimizer= optimizer, metrics=[loss_fn])\nmodel.fit(x_train.values, y_train.values, epochs=15, batch_size=64, validation_split = 0.2, verbose = 0)","44351c01":"## RMS Prop (Root Mean Squared Prop)\nRMS Prop is an optimization algorithm that is very similar to Gradient Descent but the gradients are smoothed and squared and then updated by taking mean to attain the global optima soon. \n\nOn each iteration t,\n* Cost of the data is found.\n* The partial differentiation of cost function with respect to weights and bias are computed. \n* The weights and bias are smoothed and then updated.\n* Follow these steps until specified number of iterations are completed.\n\n### Math behind RMS Prop\n\nW = {$w_0, w_1, w_2, ... , w_n$}\n\nb = $bias$\n\nC(W,b) = Cost function involving parameters W and b.\n\n#### Partial differentiation of Cost function with respect to weights \n\n$dW = \\frac{\\partial{}}{\\partial{W}} C(W,b)$  \n\n#### Partial differentiation of Cost function with respect to bias\n\n$db = \\frac{\\partial{}}{\\partial{b}} C(W,b)$\n\n#### Smooth the parameters gradients\n\n$S_{dW} = \\beta S_{dW} + (1-\\beta)dW^2$\n\n$S_{db} = \\beta S_{db} + (1-\\beta)db^2$\n\n$\\beta = 0.9$\n\n$S_{dW} = 0$ initially, $S_{db} = 0$ initially\n\n#### Update parameters W and b with the smoothed gradients\n\n$W = W - (\\alpha \\times \\frac{dW}{\\sqrt{S_{dW}}})$\n\n$b = b - (\\alpha \\times \\frac{db}{\\sqrt{S_{db}}})$\n","332f5ad3":"# Load the preprocessed data\nThe data you feed to the **ANN** must be preprocessed thouroughly to yield reliable results. The training data has been preprocessed already. The preprocessing steps involved are,\n\n* MICE Imputation\n* Log transformation\n* Square root transformation\n* Ordinal Encoding\n* Target Encoding\n* Z-Score Normalization\n\nFor detailed implementation of the above mentioned steps refer my notebook on data preprocessing\n\n[Notebook Link](https:\/\/www.kaggle.com\/srivignesh\/data-preprocessing-for-house-price-prediction)","3c168b96":"## Gradient Descent\n\nGradient Descent algorithm makes use of gradients of cost function to find the optimal value for the parameters. Gradient descent is an iterative algorithm. It attempts to find the global minima.\n\nOn each iteration t, \n* Cost of the data is found.\n* The partial differentiation of cost function with respect to weights and bias are computed. \n* The weights and bias are then updated.\n* Follow these steps until specified number of iterations are completed.\n\n### Math behind Gradient Descent\nW = {$w_0, w_1, w_2, ... , w_n$}\n\nb = $bias$\n\nC(W,b) = Cost function involving parameters W and b.\n\n#### Partial differentiation of Cost function with respect to weights \n\n$ dW = \\frac{\\partial{}}{\\partial{W}} C(W,b)$\n\n#### Partial differentiation of Cost function with respect to bias\n\n$db = \\frac{\\partial{}}{\\partial{b}} C(W,b)$\n\n#### Update parameters W and b\n\n$W = W - (\\alpha \\times dW)$\n\n$b = b - (\\alpha \\times db)$","2806a149":"# Mean Squared Error (MSE)\n**Mean Squared Error(MSE)** is the squared difference of the actual and predicted values. MSE penalizes high errors caused by outliers. By penalizing high errors it helps optimization algorithms to find the optimal parameters.\n* The drawback of MSE is that it is very sensitive to outliers. When high errors (which are caused by outliers in the target) are squared it becomes even more a larger error.\n* MSE is used when high errors are undesirable.\n\n***Mean Squared Loss*** = $(y - \\hat{y})^2$\n\n***Mean Squared Error*** =$ \\frac{1}{n}\\sum\\limits_{i=1}^{n} (y_i - \\hat{y_i})^2 $\n\n$y$ = *Actual value*\n\n$\\hat{y} = b + w_1x_1 + w_2x_2 + .... + w_nx_n$ \n\n* $b $ = *Bias*\n\n* $w_i $ = *Weights\/ coefficients*\n\n* $x_i$ = *Features*","b076587b":"# Root Mean Squared Logarithmic Error\nRoot Mean Squared Logarithmic Error is very similar to RMSE but log is applied before calculating the difference between actual and predicted values. RMSLE is used when the target is not normalized\/ scaled.\n* RMSLE is less sensitive to outliers as compared to RMSE. It relaxes the penalization of high errors due to the presence of log.\n\n***Root Mean Squared Logarithmic Loss*** = $  \\sqrt{(log(y_i + 1) - log(\\hat{y_i}+1))^2}$\n\n***Root Mean Squared Logarithmic Error*** =$  \\sqrt{\\frac{1}{n} \\sum\\limits_{i =1}^{n}(log(y_i + 1) - log(\\hat{y_i}+1))^2}$\n\n$y$ = *Actual value*\n\n$\\hat{y} = b + w_1x_1 + w_2x_2 + .... + w_nx_n$ \n\n* $b $ = *Bias*\n\n* $w_i $ = *Weights\/ coefficients*\n\n* $x_i$ = *Features*","ce5080e8":"# Loss functions\nLoss functions are used to gauge the performance of the model. Model performance is a predominant part because it reveals the quality of predictions and set standards. Different loss functions impact the model performance differently.\n\n## Loss functions versus Cost functions\n* A Loss is calculated for a single instance of data. A function used to calculate loss is called loss function.\n* Cost is calculated for the entire instances of data. A function used to calculate cost is called cost function.\n","4cc2301f":"# Cost function Optimization Algorithms\nCost function optimization algorithms attempt to find the optimal values for the parameters by considering cost functions.\nThe various algorithms available are,\n* Gradient Descent\n* Gradient Descent with momentum\n* RMS Prop\n* Adam","f4bdda05":"# Mean Absolute Error (MAE)\n\nMean Absoluter Error(MAE) is the **Absolute** difference between the actual and the predicted values. \n* MAE is more robust to outliers. MAE does not penalize high errors caused by outliers\n* The drawback of MAE is that it isn't differentiable at zero and many **Loss function Optimization algorithms** involve differentiation.\n\nA loss refers to the residual of a single instance of data.\n\n***Mean Absolute Loss*** = $| y - \\hat{y}| $\n\nWhen all the residuals of the data are considered it is called errors.\n\n***Mean Absolute Error*** =$ \\frac{1}{n} \\sum\\limits_{i=1}^{n}| y_i - \\hat{y_i}| $\n\n$y$ = *Actual value*\n\n$\\hat{y} = b + w_1x_1 + w_2x_2 + .... + w_nx_n$ **(in case of regression)**\n\n* $b $ = *Bias*\n\n* $w_i $ = *Weights\/ coefficients*\n\n* $x_i$ = *Features*","ef2080bf":"# Root Mean Squared Error (RMSE)\nRoot Mean Squared Error is the squared mean of the difference between actual and predicted values. RMSE is used when we want to penalize high errors but not as much as MSE does.\n* RMSE is highly sensitive to outliers as well. The square root makes sure that the error term is penalized but not as much as MSE.\n\n***Root Mean Squared Loss*** = $\\sqrt{ (y_i - \\hat{y_i})^2}$\n\n***Root Mean Squared Error*** =$  \\sqrt{\\frac{1}{n} \\sum\\limits_{i =1}^{n}(y_i - \\hat{y_i})^2}$\n\n$y$ = *Actual value*\n\n$\\hat{y} = b + w_1x_1 + w_2x_2 + .... + w_nx_n$ \n\n* $b $ = *Bias*\n\n* $w_i $ = *Weights\/ coefficients*\n\n* $x_i$ = *Features*","930ee4e0":"# Train the model with ANN\nRefer to my notebook on [Introduction to ANN in Tensorflow](https:\/\/www.kaggle.com\/srivignesh\/introduction-to-ann-in-tensorflow) for more details.","36f363c7":"## Adam (Adaptive Moment Estimation)\n\nAdam (Adaptive Moment Estimation) is an algorithm that emerged by combining Gradient Descent with momentum and RMS Prop.\n\nOn each iteration t,\n* Cost of the data is found.\n* The partial differentiation of cost function with respect to weights and bias are computed. \n* The weights and bias are smoothed with the technique used in RMS Prop and Gradient Descent with momentum and then updated.\n* Follow these steps until specified number of iterations are completed.\n\n\n### Math behind Adam\nW = {$w_0, w_1, w_2, ... , w_n$}\n\nb = $bias$\n\nC(W,b) = Cost function involving parameters W and b.\n\n#### Partial differentiation of Cost function with respect to weights \n\n$dW = \\frac{\\partial{}}{\\partial{W}} C(W,b)$  \n\n#### Partial differentiation of Cost function with respect to bias\n\n$db = \\frac{\\partial{}}{\\partial{b}} C(W,b)$\n\n#### Smooth the gradients with the technique used in Gradient Descent with momentum and correct it\n$v_{dW} = \\beta_1 v_{dW} + (1-\\beta_1)dW$\n\n$v_{db} = \\beta_1 v_{db} + (1-\\beta_1)db$\n\n$v_{dW} = 0$ , $v_{db} = 0$ initially\n\n**Bias Correction**:\nBias correction is done to obtain accurate values of parameters.\n\n* $v_{dW}^c$ = $\\frac{v_{dW}}{(1-\\beta_1^t)}$\n* $v_{db}^c$ = $\\frac{v_{db}}{(1-\\beta_1^t)}$\n\n#### Smooth the gradients with the technique used in RMS Prop and correct it\n\n$S_{dW} = \\beta_2 S_{dW} + (1-\\beta_2)dW^2$\n\n$S_{db} = \\beta_2 S_{db} + (1-\\beta_2)db^2$\n\n$S_{dW} = 0$ initially, $S_{db} = 0$ initially\n\n**Bias Correction**:\n\n* $S_{dW}^c$ = $\\frac{S_{dW}}{(1-\\beta_2^t)}$\n* $S_{db}^c$ = $\\frac{S_{db}}{(1-\\beta_2^t)}$\n\n#### Update parameters W and b with the smoothed gradients\n\n$W = W - (\\alpha \\times \\frac{v_{dW}^c}{\\sqrt{S_{dW}^c}})$\n\n$b = b - (\\alpha \\times \\frac{v_{db}^c}{\\sqrt{S_{db}^c}})$\n"}}