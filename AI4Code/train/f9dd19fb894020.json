{"cell_type":{"c69a9e23":"code","72f3f874":"code","db6d4d92":"code","17a406a6":"code","927abe02":"code","e72e63cd":"code","c40a8783":"code","ca2e891c":"code","2d214a18":"code","37ffa523":"code","a7a7cef0":"code","383775df":"code","7c2c1357":"code","0bc01df3":"code","5df37db3":"code","d03eb7cc":"code","ffa81d0d":"code","f1d6561c":"code","4ba7f12c":"code","5ef3b818":"markdown","7a838811":"markdown","b7019eb5":"markdown","c3e0ea56":"markdown","e9d7cf95":"markdown","d47f2b97":"markdown"},"source":{"c69a9e23":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n        \n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","72f3f874":"df=pd.read_csv('..\/input\/housepricing\/HousePrices_HalfMil.csv')\ndf.head()","db6d4d92":"df.info()","17a406a6":"df.describe()","927abe02":"import matplotlib.pyplot as plt\nimport scipy.stats as stats\nimport seaborn as sns","e72e63cd":"plt.figure(figsize=(20,20)) \nsns.heatmap(df.corr(), annot=True, cmap = 'RdBu', center=0);","c40a8783":"# price distribution\nplt.hist(df['Prices'],bins=30, color ='green',density= True,edgecolor = 'red',label=\"hist\")\nkde = stats.kde.gaussian_kde(df['Prices'])\nx = np.linspace(0, 100000, 100)\ny = kde(x)\nplt.plot(x,y,c=\"black\",linewidth=3,label=\"k-density\")\nplt.xlabel('Sale price')\nplt.legend()\nplt.show()","ca2e891c":"l=list()\nfor i in range(1,4):\n    l.append(df[df['City']==i]['Prices'])\nplt.boxplot(l)\nplt.xlabel('City')\nplt.ylabel('Price')\nplt.ylim(0,100000)\nplt.show()","2d214a18":"fig = plt.figure(figsize=(15,6))\nsns.lineplot(x = df[\"Area\"] , y = df[\"Prices\"])\n","37ffa523":"fig = plt.figure(figsize=(15,6))\n\n\ncolor=['blue','red','green']\nfor i in range(1,4):\n    plt.scatter(df[df['City']==i]['Area'],df[df['City']==i]['Prices'],color=color[i-1],label=i, alpha = 0.5)\n    plt.legend()\n    plt.xlabel(\"Area\")\n    plt.ylabel(\"Price\")\n\nplt.show()","a7a7cef0":"fig = plt.figure(figsize=(15,6))\n\n\ncolor=['blue','red','green']\nfor i in range(1,4):\n    plt.scatter(df[df['City']==i]['Floors'],df[df['City']==i]['Prices'],color=color[i-1],label=i, alpha = 0.5)\n    plt.legend()\n    plt.xlabel(\"Floors\")\n    plt.ylabel(\"Price\")\n\nplt.show()","383775df":"from sklearn import model_selection, preprocessing\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import train_test_split,cross_val_score\n\ntarget = df['Prices']\ndata = df.drop('Prices', axis =1)\n\nX_train, X_test, y_train, y_test = train_test_split(data, target, test_size=0.2, random_state=789)","7c2c1357":"lr = LinearRegression()\nlr.fit(X_train, y_train)\n","0bc01df3":"#Display the intercept as well as the coefficients of each variable estimated by the model\n\ncoeffs = list(lr.coef_)\ncoeffs.insert(0, lr.intercept_)\n\nfeats = list(data.columns)\nfeats.insert(0, 'intercept')\n\npd.DataFrame({'valeur estim\u00e9e': coeffs}, index = feats)","5df37db3":"print(\"Coefficient de d\u00e9termination du mod\u00e8le :\", lr.score(X_train, y_train))\nprint(\"Coefficient de d\u00e9termination obtenu par Cv :\", cross_val_score(lr,X_train,y_train).mean())","d03eb7cc":"lr.score(X_test,y_test)","ffa81d0d":"#Display in a graph the scatter plot between pred_test and y_test\npred_test = lr.predict(X_test)\nplt.scatter(pred_test, y_test)\nplt.plot((y_test.min(),y_test.max()), (y_test.min(),y_test.max()))","f1d6561c":"#Display the scatter plot representing the residuals as a function of the values of y_train.\npred_train = lr.predict(X_train)\nresidus = pred_train - y_train\n\nplt.scatter(y_train, residus, color = '#980a10', s=15)\nplt.plot((y_train.min(),y_train.max()), (0,0), lw=3, color = '#0a5798')","4ba7f12c":"residus_norm = (residus-residus.mean())\/residus.std()\n\nstats.probplot(residus_norm, plot=plt)\n\nplt.show()","5ef3b818":"## Data Visualization","7a838811":"### Thanks for reading!","b7019eb5":"### The normality assumption is plausible, with the points lining up approximately along the line.","c3e0ea56":"### The points are perfectly close to this line, so the prediction is very good.","e9d7cf95":"## Multiple Linear Regression","d47f2b97":"### The probplot function of scipy.stats makes it possible to display a Quantile-Quantile plot or (Q-Q plot) which makes it possible to evaluate the relevance of the fit of a given distribution to a theoretical model (often a reduced centered Gaussian law). Thus the normality of the residuals (once centered reduced) is easily validated: if the points are aligned on the first bisector, it is because the distribution of the residuals probably follows a normalized Gaussian law."}}