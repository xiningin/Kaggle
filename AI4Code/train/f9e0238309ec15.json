{"cell_type":{"cbd16e3b":"code","e4be0c64":"code","9ebc676d":"code","a0c69a94":"code","22480c9b":"code","244807a5":"code","9d06acfa":"code","033a63a5":"code","34bb502c":"code","63a04bb7":"markdown","9da33da3":"markdown","92b3056b":"markdown","cfe5dfd8":"markdown","0cda11d8":"markdown","df15460a":"markdown","fa90cfd6":"markdown","c185938b":"markdown","d4616330":"markdown","7bece59f":"markdown","71ff3ef8":"markdown","b24bd0a2":"markdown","aa79294d":"markdown","09752d24":"markdown","4d568359":"markdown","bb43f76a":"markdown","acbdd321":"markdown","4abccec2":"markdown","4fb6b0f6":"markdown","39ece810":"markdown","00322426":"markdown","8872b037":"markdown","84f51671":"markdown","8553d274":"markdown","f2438b1a":"markdown","f7f2a97e":"markdown","cea11de7":"markdown","cb3c28f8":"markdown"},"source":{"cbd16e3b":"#EDA for the data\nimport pandas_profiling as pp\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom sklearn.preprocessing import LabelEncoder #label encoding the features\nfrom sklearn.model_selection import train_test_split #Splitting the data as train and test\nfrom sklearn.ensemble import RandomForestClassifier #Random Forrest\nfrom sklearn import metrics #accuracy calculation\n","e4be0c64":"\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","9ebc676d":"data=pd.read_csv(\"\/kaggle\/input\/iris\/Iris.csv\")","a0c69a94":"report = pp.ProfileReport(data)\nreport","22480c9b":"data[\"Species\"]=LabelEncoder().fit_transform(data[\"Species\"].astype(str)) ","244807a5":"X=data[[\"PetalLengthCm\",\"PetalWidthCm\",\"SepalLengthCm\",\"SepalWidthCm\"]]  # Features\ny=data['Species']  # Labels\n\n# Split dataset into training set and test set\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3) # 70% training and 30% test","9d06acfa":"#Create a Gaussian Classifier\nclf=RandomForestClassifier(n_estimators=100)\n\n#Train the model using the training sets y_pred=clf.predict(X_test)\nclf.fit(X_train,y_train)\n\ny_pred=clf.predict(X_test)","033a63a5":"# Model Accuracy, how often is the classifier correct?\nprint(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))","34bb502c":"from sklearn.datasets import load_iris\niris = load_iris()\n\n# Model (can also use single decision tree)\nfrom sklearn.ensemble import RandomForestClassifier\nmodel = RandomForestClassifier(n_estimators=10)\n\n# Train\nmodel.fit(iris.data, iris.target)\n# Extract single tree\nestimator = model.estimators_[5]\n\nfrom sklearn.tree import export_graphviz\n# Export as dot file\nexport_graphviz(estimator, out_file='tree.dot', \n                feature_names = iris.feature_names,\n                class_names = iris.target_names,\n                rounded = True, proportion = False, \n                precision = 2, filled = True)\n\n# Convert to png using system command (requires Graphviz)\nfrom subprocess import call\ncall(['dot', '-Tpng', 'tree.dot', '-o', 'tree.png', '-Gdpi=600'])\n\n# Display in jupyter notebook\nfrom IPython.display import Image\nImage(filename = 'tree.png')","63a04bb7":"## References\n\n* https:\/\/towardsdatascience.com\/understanding-random-forest-58381e0602d2\n\n* https:\/\/dataaspirant.com\/2017\/05\/22\/random-forest-algorithm-machine-learing\/\n\n* https:\/\/becominghuman.ai\/ensemble-learning-bagging-and-boosting-d20f38be9b1e\n\n* https:\/\/medium.com\/greyatom\/decision-trees-a-simple-way-to-visualize-a-decision-dc506a403aeb\n\n* https:\/\/www.analyticsvidhya.com\/blog\/2015\/06\/tuning-random-forest-model\/\n\n* https:\/\/towardsdatascience.com\/how-to-visualize-a-decision-tree-from-a-random-forest-in-python-using-scikit-learn-38ad2d75f21c","9da33da3":"**Random forest prediction pseudocode:**\n\n* To perform prediction using the trained random forest algorithm uses the below pseudocode.\n\n* Takes the test features and use the rules of each randomly created decision tree to predict the oucome and stores the predicted outcome (target)\n\n* Calculate the votes for each predicted target.\n\n* Consider the high voted predicted target as the final prediction from the random forest algorithm.","92b3056b":"**Decision Trees**\n    ![image.png](attachment:image.png)\n    \n    \n A decision tree is a decision support tool that uses a tree-like graph or model of decisions and their possible consequences, including chance event outcomes, resource costs, and utility. It is one way to display an algorithm that only contains conditional control statements.","cfe5dfd8":"**Why Random Forest?**\n\n","0cda11d8":"**Fitting the Random Forest**","df15460a":"Hi Kagglers, As i have mentioned in one of my [disscussion's](https:\/\/www.kaggle.com\/questions-and-answers\/110036) of making a series on breaking down the black box of complex algorithm's I will be exploring the Random Forest in this series.\n\nSince our main objective is to explore the **Random Forest** I have taken a very simple dataset **Iris Sepcies**\nand I am going to explain both the intuition of the algorithm and how to use it effectively in python\n\nPlease feel free to ask your queries in the comments","fa90cfd6":"**Random Forest pseudocode:**\n\n* Randomly select \u201ck\u201d features from total \u201cm\u201d features.\n        * Where k << m\n* Among the \u201ck\u201d features, calculate the node \u201cd\u201d using the best split point.\n\n* Split the node into daughter nodes using the best split.\n\n* Repeat 1 to 3 steps until \u201cl\u201d number of nodes has been reached.\n\n* Build forest by repeating steps 1 to 4 for \u201cn\u201d number times to create \u201cn\u201d number of trees.","c185938b":"**Prerequisite**\n\n                Before getting to know what is **Random Forest** there are two basic concepts we need to:\n                                1) Decision Trees\n                                2) Ensembling\n                                3) Bagging","d4616330":"**Features which make predictions of the model better**\n\n* ** max_features:**\n    These are the maximum number of features Random Forest is allowed to try in individual tree. \n\n* ** n_estimators:**\n     This is the number of trees you want to build before taking the maximum voting or averages of predictions. \n    \n* ** min_sample_leaf:**\n    Leaf is the end node of a decision tree. A smaller leaf makes the model more prone to capturing noise in train data.However, you should try multiple leaf sizes to find the most optimum for your use case.\n  ","7bece59f":"## Visualizing Random Forest","71ff3ef8":"Parameters in random forest are either to increase the predictive power of the model or to make it easier to train the model.","b24bd0a2":"**What is Bagging?**\n\n\n1)Bootstrap Aggregation (or Bagging for short), is a simple and very powerful ensemble method.\n\n2)Bootstrap Aggregation is a general procedure that can be used to reduce the variance for those algorithm that have high variance. \n\n3)When bagging with decision trees, we are less concerned about individual trees overfitting the training data.\n\n4)These trees will have both high variance and low bias. These are important characterize of sub-models when combining predictions using bagging.","aa79294d":"**Random forest**\n\nIt is a supervised classification algorithm. As the name suggest, this algorithm creates the forest with a number of trees.\n\nIn general, the more trees in the forest the more robust the forest looks like. In the same way in the random forest classifier, the higher the number of trees in the forest gives the high accuracy results.","09752d24":"## Random Forest","4d568359":"** Features which will make the model training easier **\n\n* **  n_jobs :**\n     This parameter tells the engine how many processors is it allowed to use.\n     \n* **  random_state :**\n    This parameter makes a solution easy to replicate. A definite value of random_state will always produce same results if given with same parameters and training data. \n    \n* ** oob_score :**\n    This is a random forest cross validation method. It is very similar to leave one out validation technique, however, this is so much faster. ","bb43f76a":"## **What is Bootstrap?**\n![image.png](attachment:image.png)\n\n\n1)Bootstrap refers to random sampling with replacement. \n\n2)Bootstrap allows us to better understand the bias and the variance with the dataset. \n\n3)Bootstrap involves random sampling of small subset of data from the dataset.\n\n4)This subset can be replaced. \n\n5)The selection of all the example in the dataset has equal probability.\n\n6)This method can help to better understand the mean and standand deviation from the dataset.","acbdd321":"**Please leave a upvote if you like this kernel**\n\n**Feel Free to ask your doubts in the comments**","4abccec2":"\n![image.png](attachment:image.png)\n\n\n","4fb6b0f6":"The error produced by the machine learning algorithm can be broken down into three parts:\n\n   * Bias Error\n   * Variance Error\n   * Irreducible Error \n   \n**Bias Error**\n  \n   * It is based on the assumptions we have about the target functions.\n   * Higher the assumptions ---> Higher the bias\n   * Models with high bias are less flexible\n   \n   \n**Variance Error**\n\n   * Variance error is variability of a target function's form with respect to different training sets. \n   * Models with small variance error will not change much if you replace couple of samples in training set. \n   * Models with high variance might be affected even with small changes in training set.\n   \n**Irreducible Error**\n\n  * As the name implies, is an error component that we cannot correct, regardless of algorithm and it's parameter     selection. \n  * Irreducible error is due to complexities which are simply not captured in the training set.\n  \n  \nNow, it's also straightforward to see that decision trees are example of model with low bias and high variance. The tree makes almost no assumptions about target function but it is highly susceptible to variance in data. Therefore, we can use Random Forest which is a ensembling algorithm of decision tree which aim to reduce variance at the small cost of bias in decision tree.\n  ","39ece810":"Eventhough, the main objective here is to understand the usage of **Random Forest**, We should understand the nature of the data for better modelling.\n\nI have used a package named **Pandas Profiling**. It gives a detailed analysis both visually as well as statistically.\n\nFor better understanding of particular column use **Toggle details** in the report.","00322426":"**About the data**\n\n\n    ","8872b037":"## Parameters of Random Forest","84f51671":"## About the kernel","8553d274":"**Major Takeaways**\n\n1) Intiution of the Random Forest\n\n2) Optimal Useage of Random Froest in Python","f2438b1a":"## Exploratory Data Analysis","f7f2a97e":"The Iris dataset was used in R.A. Fisher's classic 1936 paper, The Use of Multiple Measurements in Taxonomic Problems, and can also be found on the UCI Machine Learning Repository.\n\nIt includes three iris species with 50 samples each as well as some properties about each flower. One flower species is linearly separable from the other two, but the other two are not linearly separable from each other.\n\nThe columns in this dataset are:\n\n1)Id\n\n2)SepalLengthCm\n\n3)SepalWidthCm\n\n4)PetalLengthCm\n\n5)PetalWidthCm\n\n6)Species\n","cea11de7":"**What is Ensembling?**\n\nEnsemble is the art of combining diverse set of learners (individual models) together to improvise on the stability and predictive power of the model. In the above example, the way we combine all the predictions together will be termed as Ensemble Learning.\n","cb3c28f8":"## Importing the Packages and Reading the Data"}}