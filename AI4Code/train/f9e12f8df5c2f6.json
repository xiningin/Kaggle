{"cell_type":{"8d664210":"code","9ed319e2":"code","28cd968b":"code","18ff3baf":"code","50a6fb0a":"code","4d63d549":"code","daa1ffbc":"code","ceb8f922":"code","12b9d95c":"code","a92b9b9e":"code","df266734":"markdown","6585f403":"markdown","ac1c604e":"markdown","8c823a2b":"markdown","c02fdd24":"markdown","873ab1e0":"markdown","831c8c92":"markdown","169ce8bd":"markdown","cd2fe842":"markdown","91160440":"markdown","b272c427":"markdown","4c21b385":"markdown","eb260085":"markdown","d9bca082":"markdown","fcce22ac":"markdown","e5f0d5f3":"markdown"},"source":{"8d664210":"import pandas as pd\nimport numpy as np","9ed319e2":"dataset = pd.read_csv('..\/input\/breast-cancer-csv\/breastCancer.csv')\ndataset = dataset.replace('?', np.nan)","28cd968b":"dataset.isnull().sum()","18ff3baf":"X = dataset.iloc[:, 1:-1].values\ny = dataset.iloc[:, -1].values","50a6fb0a":"from sklearn.impute import SimpleImputer\nimputer = SimpleImputer(missing_values= np.nan, strategy = 'mean')\nimputer.fit(X)\nX = imputer.transform(X)","4d63d549":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25)","daa1ffbc":"from sklearn.linear_model import LogisticRegression\nlr = LogisticRegression()\nlr.fit(X_train, y_train)","ceb8f922":"y_pred = lr.predict(X_test)","12b9d95c":"from sklearn.metrics import confusion_matrix\nconfusion_matrix(y_test, y_pred)","a92b9b9e":"from sklearn.model_selection import cross_val_score\nacc = cross_val_score(estimator = lr, X = X_train, y = y_train, cv = 10)\nprint(\"Accuracy: {:.2f}%\".format(acc.mean()*100))\nprint(\"Standard Deviation: {:.2f}%\".format(acc.std()*100))","df266734":"### 1. Importing the libraries\n\nWe begin with importing the two most crucial libraries, **pandas** and **numpy** library. \n\n**pandas** helps in reading, storing, and manipulating data for our analysis.\n\n**numpy** is used for scientific computing, such as handling arrays and matrices.\n","6585f403":"### 3. Separating the Explanatory variable and Response variable\n\nIn this step, we separate the explanatory and response variables. Since we have to predict the class variable using all other variables, it becomes the response variable 'y'.\n\n**Note** - The first variable, 'id', only acts as a primary key (SQL reference) to identify all the observations. It doesn't have an effect on the class variable. Hence we will exclude this from our independent variables set 'X'. Finally, 'X' has all other variables except 'id' and 'class'.","ac1c604e":"### 4. Handling missing values\n\nWe know that there are a total of 16 missing values in the dataset (only in one column, according to our analysis). We impute these missing values using the **SimpleImputer** class of sklearn.impute\n\nWe replace the null values with the **mean** of the entire column.","8c823a2b":"### 2. Importing the dataset\n\nWe use **pandas** to read our breast cancer dataset. This is done by using the 'read_csv()' function of **pandas**.\n\nWe go a step further and also replace the missing values in the dataset depicted by '?'. We replace it with the 'nan' value of a **numpy** array. ","c02fdd24":"We get a good accuracy of the predictions at 97.13%. Also, since the standard deviation is not much, we can confirm that most of the accuracy scores lie near to 97.13%.","873ab1e0":"# Using Logistic Regression to predict Breast Cancer","831c8c92":"**The Dataset** - The dataset contains 9 features determining the overall breast condition of 699 observations. The class column tells us whether a tumour has been classified as 2-benign or 4-malignant.\n\n**The goal** - To build and evaluate a Logistic Regression model that predicts the class of our dataset.\n\n*Before you go ahead, a little disclaimer about this project - it outlines just the basics of model development. The analysis is not limited to the following steps.","169ce8bd":"### 6. Making the Confusion Matrix\n\nWe finally evaluate our model by forming a Confusion Matrix, which gives us a contingency table of the correct and incorrect predictions.","cd2fe842":"Now let's check for the total null values in each column of our dataset -","91160440":"### 4. Training the Logistic Regression model on the training set","b272c427":"From the above evaluation, we have 114+54 correct predictions and only 5+2 incorrect predictions. Hence, Logistic Regression seems to have worked well.","4c21b385":"### 7. Computing the accuracy with k-Fold Cross Validation\n\nK-fold Cross validation is a popular method of evaluation that uses k-divisions of the dataset. In this method, each division is used as a test set once while other divisions are used to train the model.\n\nWe determine the accuracy of the model predictions by finding the mean of all the cross-validation scores found through k-fold cross validation.\n\nWe also determine the standard deviation to confirm that there is not too much variance in all the cross-validation scores.","eb260085":"As can be seen from the above output, there are 16 null values in the **bare_nucleoli** column.","d9bca082":"### 3. Splitting the dataset into the Training set and Test set\n\nA common practice for determining a good model, we split the dataset into training and test set. \n\nHere, we use the sci-kit learn library and call the model_selection class. From this class, we import the 'train_test_split()' function. We take the test set size as 25% of the total dataset.","fcce22ac":"So, after almost a year of exploring Data Science, this is my first attempt at a Kaggle notebook! I sincerely hope you find my explanations useful.","e5f0d5f3":"### 5. Predicting the test results\n\nWe predict the test results and put them in a separate array named **y_pred**"}}