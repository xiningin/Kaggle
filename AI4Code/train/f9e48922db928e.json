{"cell_type":{"6cb984a0":"code","5b23b4d7":"code","f2a03271":"code","f779353f":"code","8d312ad6":"code","6279a2fd":"code","f1599a62":"code","7067fa8a":"code","6dacb1ee":"code","ceefafb5":"code","13d2a729":"code","eb749ae1":"code","292bb0ef":"code","371374df":"code","93806eb3":"code","21271aea":"code","42da0e00":"code","b8cb9269":"code","65c397cc":"code","99f00436":"code","a19d8058":"code","e625442c":"code","55528e35":"code","200d8c1b":"markdown"},"source":{"6cb984a0":"import pandas as pd\nimport transformers\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nfrom sklearn.model_selection import train_test_split\nfrom torch import nn, optim\nfrom torch.utils.data import Dataset, DataLoader\nimport inspect\nimport torch.nn as nn\nimport torch\nfrom tqdm import tqdm\nimport gc\n\nwarnings.filterwarnings('ignore')","5b23b4d7":"BATCH_SIZE = 16\nMAX_LEN = 60\nEPOCHS = 1","f2a03271":"DIR = '..\/input\/tweet-sentiment-extraction\/'","f779353f":"train = pd.read_csv(DIR+'train.csv')\n#test = pd.read_csv(DIR+'train.csv')\n#ss = pd.read_csv(DIR+'sample_submission.csv')","8d312ad6":"train.head()","6279a2fd":"sns.countplot(train.sentiment)","f1599a62":"train.sentiment[0]","7067fa8a":"mapping = {'positive':2,'negative':0,'neutral':1}\ntrain.replace({'sentiment':mapping},inplace=True)\n##test.replace({'sentiment':mapping},inplace=True)","6dacb1ee":"tokenizer = transformers.BertTokenizer.from_pretrained('bert-base-uncased')","ceefafb5":"class TweetReviewDataset(Dataset):\n    def __init__(self,review,tokenizer,targets,max_len):\n        self.review = review\n        self.tokenizer = tokenizer\n        self.targets = targets\n        self.max_len = max_len\n        \n    def __len__(self):\n        return len(self.review)\n    \n    def __getitem__(self,index):\n        review = str(self.review[index])\n        target = self.targets[index]\n        \n        encoding = self.tokenizer.encode_plus(\n        review,\n        add_special_tokens=True,\n        max_length=self.max_len,\n        return_token_type_ids=False,\n        pad_to_max_length=True,\n        return_attention_mask=True,\n        return_tensors='pt'\n        )\n        \n        return {'review_text':review,'input_ids': encoding['input_ids'].flatten(),\n      'attention_mask': encoding['attention_mask'].flatten(),\n      'targets': torch.tensor(target, dtype=torch.long)\n    }","13d2a729":"df_train , df_val = train_test_split(train,test_size = 0.2,random_state = 23)","eb749ae1":"del train\ngc.collect()","292bb0ef":"def create_data_loader(df, tokenizer, max_len, batch_size):\n    \n    ds = TweetReviewDataset(\n    review=df.text.to_numpy(),\n    targets=df.sentiment.to_numpy(),\n    tokenizer=tokenizer,\n    max_len=max_len\n  )\n    return DataLoader(\n    ds,\n    batch_size=batch_size,\n    num_workers=0\n  )","371374df":"train_len = len(df_train)\nval_len = len(df_val)\n\ntrain_loader = create_data_loader(df_train,tokenizer,MAX_LEN,BATCH_SIZE)\nval_loader = create_data_loader(df_val,tokenizer,MAX_LEN,BATCH_SIZE)","93806eb3":"del df_train\ndel df_val\n\ngc.collect()","21271aea":"class SentimentClassifier(nn.Module):\n    def __init__(self,n_classes):\n        super(SentimentClassifier,self).__init__()\n        self.bert = transformers.BertModel.from_pretrained('bert-base-uncased')\n        self.drop = nn.Dropout(0.3)\n        self.out = nn.Linear(self.bert.config.hidden_size,n_classes)\n        \n    def forward(self,input_ids,attention_mask):\n        _,pooled_output = self.bert(\n      input_ids=input_ids,\n      attention_mask=attention_mask\n    )\n        \n        output = self.drop(pooled_output)\n        return self.out(output)","42da0e00":"model = SentimentClassifier(3)","b8cb9269":"optimizer = transformers.AdamW(model.parameters(), lr=2e-5)\ntotal_steps = len(train_loader) * EPOCHS\n\nscheduler = transformers.get_linear_schedule_with_warmup(\n  optimizer,\n  num_warmup_steps=0,\n  num_training_steps=total_steps\n)\n\nloss_fn = nn.CrossEntropyLoss()","65c397cc":"def train_epoch(model,data_loader,loss_fn,optimizer,scheduler,n_examples):\n    model = model.train()\n    \n    losses = []\n    correct_predictions = 0\n    \n    for i,d in tqdm(enumerate(data_loader)):\n        \n        input_ids = d['input_ids']\n        attention_mask = d['attention_mask']\n        targets = d[\"targets\"]\n        \n        outputs = model(input_ids = input_ids,attention_mask = attention_mask)\n        \n        _,preds = torch.max(outputs,dim=1)\n        loss = loss_fn(outputs, targets)\n        \n        correct_predictions += torch.sum(preds == targets)\n        losses.append(loss.item())\n\n        \n        loss.backward()\n        nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n        optimizer.step()\n        scheduler.step()\n        optimizer.zero_grad()\n    return correct_predictions.double() \/ n_examples, np.mean(losses)\n        \n        ","99f00436":"def eval_model(model, data_loader,loss_fn, n_examples):\n    \n    model = model.eval()\n    losses = []\n    correct_predictions = 0\n    with torch.no_grad():\n        \n        for i,d in tqdm(enumerate(data_loader)):\n            \n            input_ids = d[\"input_ids\"]\n            attention_mask = d[\"attention_mask\"]\n            targets = d[\"targets\"]\n            \n            outputs = model(\n            input_ids=input_ids,\n            attention_mask=attention_mask\n          )\n            _, preds = torch.max(outputs, dim=1)\n            loss = loss_fn(outputs, targets)\n            correct_predictions += torch.sum(preds == targets)\n            losses.append(loss.item())\n    return correct_predictions.double() \/ n_examples, np.mean(losses)","a19d8058":"\nfor epoch in range(EPOCHS):\n    \n    print(f'Epoch {epoch + 1}\/{EPOCHS}')\n    print('-' * 10)\n    train_acc, train_loss = train_epoch(\n    model,\n    train_loader,\n    loss_fn,\n    optimizer,\n    scheduler,\n    train_len\n  )\n    \n    print(f'Train loss {train_loss} accuracy {train_acc}')\n    val_acc, val_loss = eval_model(\n    model,\n    val_loader,\n    loss_fn,\n    val_len\n  )\n    print(f'Val   loss {val_loss} accuracy {val_acc}')\n    print()","e625442c":"PATH = 'model.pth'","55528e35":"torch.save(model,PATH)","200d8c1b":"this is only till training of model i will do the submission code as soon as possible"}}