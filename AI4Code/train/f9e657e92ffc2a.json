{"cell_type":{"0ad4e435":"code","844308fd":"code","f0539eae":"code","a47373f8":"code","c87a1209":"code","358fbc11":"code","1334cf12":"code","c4181399":"code","b0d52859":"code","548f9c60":"code","0f5d4985":"code","521c80a9":"code","7542e710":"code","3354b4c0":"code","da9cea3e":"code","fd8ccd5b":"code","6fba15ba":"code","18862d33":"code","d21bfad0":"code","16a5ee4a":"code","e22b4757":"code","63f509fb":"code","db6d5ac3":"code","9345b7e2":"code","94613ab5":"code","22a16edf":"code","fc0edd63":"code","9f65bcdb":"code","4d5d8099":"code","a8512cb2":"code","20adaeac":"code","f73744fb":"code","f2664590":"code","4a28e397":"code","153e0a4c":"code","4bbea3ac":"code","383b1d7c":"code","885b576b":"code","9897d758":"code","f8d65251":"code","bacd122f":"code","5db9b655":"code","3ad6ce7e":"code","c361adeb":"code","2c82ca04":"code","89a7357d":"code","9e7fc4ca":"code","0ca09b54":"code","e1c6a0a7":"markdown","43c8d672":"markdown","5b8c0cba":"markdown","5b1c93c1":"markdown","1c1da0e9":"markdown","c214ac78":"markdown","7bf85ef4":"markdown","54374dfd":"markdown","d8795749":"markdown","1ab93323":"markdown","0fd9ebae":"markdown","4c47b1a9":"markdown","695c4064":"markdown","c19a152a":"markdown","eb31aab1":"markdown","d59ac2f0":"markdown","74520900":"markdown","e19d3b20":"markdown","84b6116b":"markdown"},"source":{"0ad4e435":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.preprocessing import OneHotEncoder, LabelEncoder, StandardScaler\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn import svm\n\nfrom sklearn.metrics import confusion_matrix, precision_recall_curve, fbeta_score\nfrom sklearn.metrics import classification_report, accuracy_score, recall_score, f1_score, precision_score\nfrom sklearn.metrics import roc_curve, auc, roc_auc_score\n\nfrom xgboost import XGBClassifier\n\nfrom tensorflow.keras.callbacks import ModelCheckpoint\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Activation, Flatten\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.utils import to_categorical\n\npd.set_option('display.max_columns', 100)","844308fd":"df = df_credit = pd.read_csv(\"..\/input\/german-credit-risk-labelled\/credit_risk_labelled.csv\",index_col=0)","f0539eae":"df.isna().sum()","a47373f8":"print(df['Saving accounts'].unique())\ndf['Saving accounts'][df['Saving accounts'].isna()] = 'None'\nprint(df['Saving accounts'].unique())","c87a1209":"print(df['Checking account'].unique())\ndf['Checking account'][df['Checking account'].isna()] = 'None'\nprint(df['Checking account'].unique())","358fbc11":"df.isna().sum()","1334cf12":"df.nunique()","c4181399":"sns.countplot(x=df['Risk'])\nprint('proportion of good credit: ', sum(sum([df['Risk']=='good']))\/len(df))\nprint('proportion of bad credit: ', sum(sum([df['Risk']=='bad']))\/len(df))","b0d52859":"plt.figure(figsize = (12, 7))\nsns.distplot(df['Age'][df['Risk']=='bad'], label = 'risk=bad')\nsns.distplot(df['Age'][df['Risk']=='good'], label = 'risk=good')\nsns.distplot(df['Age'], label='all')\nplt.legend()","548f9c60":"g = sns.FacetGrid(df, col=\"Risk\", height=5, aspect=1)\ng.map(sns.distplot, \"Age\")","0f5d4985":"plt.figure(figsize = (10, 5))\nsns.countplot(data=df, x='Job', hue='Risk', palette=[(.4, .8, .2), (.9, .1, .3)])\ncategories = df['Job'].unique()\ncategories.sort()\n\nfor category in categories:\n    fail = len((df)[(df['Job']==category) & (df['Risk']=='bad')]) \/ len((df)[(df['Job']==category)])\n    print('proportion of fail for category {}: {}%'.format(category, round(fail*100, 1)))","521c80a9":"plt.figure(figsize = (10, 5))\nsns.boxplot(data=df, x='Job', y='Credit amount', hue='Risk', palette=[(.4, .8, .2), (.9, .1, .3)])","7542e710":"plt.figure(figsize = (10, 5))\nsns.countplot(x='Housing', hue='Risk', data=df, palette=[(.4, .8, .2), (.9, .1, .3)])\n\ncategories = df['Housing'].unique()\nfor category in categories:\n    fail = len((df)[(df['Housing']==category) & (df['Risk']=='bad')]) \/ len((df)[(df['Housing']==category)])\n    print('proportion of fail for category {}: {}%'.format(category, round(fail*100, 1)))","3354b4c0":"plt.figure(figsize = (10, 5))\nsns.boxplot(x='Housing', hue='Risk', y='Credit amount', data=df, palette=[(.4, .8, .2), (.9, .1, .3)])","da9cea3e":"plt.figure(figsize = (10, 5))\nsns.countplot(x='Saving accounts', hue='Risk', data=df, palette=[(.4, .8, .2), (.9, .1, .3)])\n\ncategories = df['Saving accounts'].unique()\nfor category in categories:\n    fail = len((df)[(df['Saving accounts']==category) & (df['Risk']=='bad')]) \/ len((df)[(df['Saving accounts']==category)])\n    print('proportion of fail for category {}: {}%'.format(category, round(fail*100, 1)))","fd8ccd5b":"plt.figure(figsize = (10, 5))\nsns.boxplot(x='Saving accounts', hue='Risk', y='Credit amount', data=df, palette=[(.4, .8, .2), (.9, .1, .3)])","6fba15ba":"plt.figure(figsize = (13, 5))\nsns.countplot(data=df, x='Purpose', hue='Risk', palette=[(.4, .8, .2), (.9, .1, .3)])\n\ncategories = df['Purpose'].unique()\nfor category in categories:\n    fail = len((df)[(df['Purpose']==category) & (df['Risk']=='bad')]) \/ len((df)[(df['Purpose']==category)])\n    print('proportion of fail for category {}: {}%'.format(category, round(fail*100, 1)))","18862d33":"plt.figure(figsize = (13, 5))\nsns.boxplot(data=df, x='Purpose', hue='Risk', y='Credit amount', palette=[(.4, .8, .2), (.9, .1, .3)])","d21bfad0":"plt.figure(figsize = (12, 7))\nsns.distplot(df['Credit amount'][df['Risk']=='bad'], label = 'risk=bad')\nsns.distplot(df['Credit amount'][df['Risk']=='good'], label = 'risk=good')\nplt.legend()","16a5ee4a":"plt.figure(figsize = (12, 7))\nsns.distplot(df['Duration'][df['Risk']=='bad'], label = 'risk=bad')\nsns.distplot(df['Duration'][df['Risk']=='good'], label = 'risk=good')\nplt.legend()","e22b4757":"plt.figure(figsize = (13, 5))\nsns.countplot(data=df, x='Sex', hue='Risk', palette=[(.4, .8, .2), (.9, .1, .3)])\n\ncategories = df['Sex'].unique()\nfor category in categories:\n    fail = len((df)[(df['Sex']==category) & (df['Risk']=='bad')]) \/ len((df)[(df['Sex']==category)])\n    print('proportion of fail for category {}: {}%'.format(category, round(fail*100, 1)))","63f509fb":"plt.figure(figsize = (13, 5))\nsns.barplot(data=df, x='Sex', hue='Risk', y='Credit amount', palette=[(.4, .8, .2), (.9, .1, .3)])","db6d5ac3":"df_inter=pd.DataFrame()\ncategorical_features = ['Sex', 'Job', 'Housing', 'Saving accounts', 'Checking account', 'Purpose', 'Risk']\n\nfor col in categorical_features:\n    dummies = pd.get_dummies(df[col],prefix=col)\n    df_inter = pd.concat([df_inter, dummies], axis=1)\n    \ndf = pd.concat([df, df_inter], axis=1)\ndf = df.drop(categorical_features, axis=1)\n\n\n    \ncontinuous_features = ['Age', 'Credit amount', 'Duration']\nfor col in continuous_features:\n    column = np.array(df[col])\n    column = column.reshape(len(column), 1)\n    sc = StandardScaler()\n    sc.fit(column)\n    df[col] = sc.transform(column)\n    \n    \n\ny = df['Risk_good']\nX=df.drop(['Risk_good', 'Risk_bad'], axis=1)","9345b7e2":"y = df['Risk_bad']","94613ab5":"df.head()","22a16edf":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.25, random_state=42)","fc0edd63":"#Seting the Hyper Parameters\nparam_grid = {\"max_depth\": [3,5, 7, 10,None],\n              \"n_estimators\":[3,5,10,25,50],\n              \"max_features\": [4,7,15,20]}\n\n#Creating the classifier\nmodel = RandomForestClassifier(random_state=2)\n\ngrid_search = GridSearchCV(model, param_grid=param_grid, cv=5, scoring='recall', verbose=4)\ngrid_search.fit(X_train, y_train)","9f65bcdb":"print(grid_search.best_score_)\nprint(grid_search.best_params_)","4d5d8099":"rf = RandomForestClassifier(max_depth=None, max_features=20, n_estimators=5, random_state=0)\nrf.fit(X_train, y_train)","a8512cb2":"prediction = rf.predict(X_test)","20adaeac":"#Testing the model \n#Predicting using our  model\ny_pred = rf.predict(X_test)\n\n# Validation of the results\nprint(accuracy_score(y_test,prediction))\nprint(\"\\n\")\nprint(confusion_matrix(y_test, prediction))\nprint(\"\\n\")\nprint(fbeta_score(y_test, prediction, beta=2))\nprint(\"\\n\")\nprint(classification_report(y_test, prediction))","f73744fb":"XGBModel = XGBClassifier()\nXGBModel.fit(X_train, y_train , verbose=1)\n\n#Testing the model \n#Predicting using our  model\nXGBpredictions = XGBModel.predict(X_test)","f2664590":"# Validation of the results\nprint(accuracy_score(y_test,XGBpredictions))\nprint(\"\\n\")\nprint(confusion_matrix(y_test, XGBpredictions))\nprint(\"\\n\")\nprint(fbeta_score(y_test, XGBpredictions, beta=2))\nprint(\"\\n\")\nprint(classification_report(y_test, XGBpredictions))","4a28e397":"GNBModel = GaussianNB()\nGNBModel.fit(X_train, y_train)\n\n#Testing the model \n#Predicting using our  model\nGNBpredictions = GNBModel.predict(X_test)","153e0a4c":"# Validation of the results\nprint(accuracy_score(y_test,GNBpredictions))\nprint(\"\\n\")\nprint(confusion_matrix(y_test, GNBpredictions))\nprint(\"\\n\")\nprint(fbeta_score(y_test, GNBpredictions, beta=2))\nprint(\"\\n\")\nprint(classification_report(y_test, GNBpredictions))","4bbea3ac":"y_test_categorical = to_categorical(\n    y_test, num_classes=2, dtype='float32')\ny_train_categorical = to_categorical(\n    y_train, num_classes=2, dtype='float32')","383b1d7c":"def nn_model(learning_rate):\n    NN_model = Sequential()\n\n    # The Input Layer :\n    NN_model.add(Dense(128, kernel_initializer='normal',input_dim = X_train.shape[1], activation='relu'))\n\n    # The Hidden Layers :\n    NN_model.add(Dense(256, kernel_initializer='normal',activation='relu'))\n    NN_model.add(Dense(256, kernel_initializer='normal',activation='relu'))\n    NN_model.add(Dense(256, kernel_initializer='normal',activation='relu'))\n\n    # The Output Layer :\n    NN_model.add(Dense(2, kernel_initializer='normal',activation='sigmoid'))\n\n    # Compile the network :\n    optimizer = Adam(learning_rate=1e-5)\n    NN_model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['acc'])\n    NN_model.summary()\n    return NN_model","885b576b":"NN_model = nn_model(1e-4)\nnb_epochs = 200\nNN_model.fit(X_train, y_train_categorical, epochs=nb_epochs, batch_size=32)","9897d758":"NNpredictions = NN_model.predict(X_test)\n\nNN_prediction = list()\nfor i in range(len(NNpredictions)):\n    NN_prediction.append(np.argmax(NNpredictions[i]))","f8d65251":"# Validation of the results\nprint(accuracy_score(y_test, NN_prediction))\nprint(\"\\n\")\nprint(confusion_matrix(y_test, NN_prediction))\nprint(\"\\n\")\nprint(fbeta_score(y_test, NN_prediction, beta=2))\nprint(\"\\n\")\nprint(classification_report(y_test, NN_prediction))","bacd122f":"# Generate ROC curve values: fpr, tpr, thresholds\nfpr, tpr, thresholds = roc_curve(y_test, NNpredictions[:, 1])\n\nlr_auc = roc_auc_score(y_test, NNpredictions[:, 1])\n\n# Plot ROC curve\nplt.plot([0, 1], [0, 1], 'k--', label='No Skill: ROC AUC=%.3f' % (0.5))\nplt.plot(fpr, tpr, label='Logistic: ROC AUC=%.3f' % (lr_auc))\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('ROC Curve')\nplt.legend()\nplt.show()","5db9b655":"NN_model = nn_model(1e-4)\nnb_epochs = 200\nNN_model.fit(X_train, y_train_categorical, epochs=nb_epochs, batch_size=32)\n\noptimizer = Adam(learning_rate=5e-5)\nNN_model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['acc'])\nnb_epochs = 200\nNN_model.fit(X_train, y_train_categorical, epochs=nb_epochs, batch_size=32)","3ad6ce7e":"NNpredictions = NN_model.predict(X_test)\n\nNN_prediction = list()\nfor i in range(len(NNpredictions)):\n    NN_prediction.append(np.argmax(NNpredictions[i]))","c361adeb":"# Validation of the results\nprint(accuracy_score(y_test, NN_prediction))\nprint(\"\\n\")\nprint(confusion_matrix(y_test, NN_prediction))\nprint(\"\\n\")\nprint(fbeta_score(y_test, NN_prediction, beta=2))\nprint(\"\\n\")\nprint(classification_report(y_test, NN_prediction))","2c82ca04":"# Generate ROC curve values: fpr, tpr, thresholds\nfpr, tpr, thresholds = roc_curve(y_test, NNpredictions[:, 1])\n\nlr_auc = roc_auc_score(y_test, NNpredictions[:, 1])\n\n# Plot ROC curve\nplt.plot([0, 1], [0, 1], 'k--', label='No Skill: ROC AUC=%.3f' % (0.5))\nplt.plot(fpr, tpr, label='Logistic: ROC AUC=%.3f' % (lr_auc))\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('ROC Curve')\nplt.legend()\nplt.show()","89a7357d":"clf = svm.SVC(kernel='poly')\nclf.fit(X_train, y_train)","9e7fc4ca":"SVMpredictions = clf.predict(X_test)","0ca09b54":"# Validation of the results\nprint(accuracy_score(y_test,SVMpredictions))\nprint(\"\\n\")\nprint(confusion_matrix(y_test, SVMpredictions))\nprint(\"\\n\")\nprint(fbeta_score(y_test, SVMpredictions, beta=2))\nprint(\"\\n\")\nprint(classification_report(y_test, SVMpredictions))","e1c6a0a7":"70% of the credits in the dataset are labelled 'bad'","43c8d672":"Males engage more credits than females, but they fail less often in proportion than females  \nMales also tend to borrow larger amounts, and also fail on the largest ones.  \nThe variation of amounts is similar for males and females","5b8c0cba":"## SVM model","5b1c93c1":"### Find best parameters for Random Forest","1c1da0e9":"People owning their own house tend to make more credits, and have a lower proportion of fail  \nThose who are free housing make larger credits","c214ac78":"## Neural Network Model","7bf85ef4":"### Gaussian Naive Bayes model","54374dfd":"Most of the credits are made by people in job category 2, but the proportion of bad credits is similar for each category  \nNonetheless, the highest credits are made by job category 3, and the largest variation is made by category 0   \nPeople in category 3 fail more often","d8795749":"High credit amounts fail more often than low credit. ","1ab93323":"### Test with XGBoost","0fd9ebae":"## Data Analysis","4c47b1a9":"People with little saving account make more credits than other categories, with a higher proportion a failing.  \nPoeple with moderate saving fail quite often as well.  \nNonetheless, each category makes credit of similar amount.  \nRich people have a larger variation, meaning they are more susceptible to borrow large amounts","695c4064":"## Filling missing data","c19a152a":"# Credit Risk Analysis and prediction","eb31aab1":"Prediction with RF","d59ac2f0":" Younger people appear to be more at risk for bad credit than older people","74520900":"Very interesting plot, as credits made for vacation have the largest proportion of failing. \nBusiness is second in raking of failing purpose  \nPeople tend to make more credits to buy radio\/TV and cars.  \nEvery category is more susceptible to lead to a failing credit than to success...","e19d3b20":"Credits engaged on a long duration happen to fail more than on short duration, which follows the previous plot: larger credits lead to longer duration, hence failing more ofter","84b6116b":"The Neural Network is the best model found so far.. reaching accuracy of 74%.  "}}