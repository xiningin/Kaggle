{"cell_type":{"beb251a7":"code","1469d321":"code","e4814954":"code","8e9d09a8":"code","afb0a17e":"code","7e6696b3":"code","8cce2690":"code","1e32b1b2":"code","34b3e65e":"code","776a2ecc":"code","c391da9d":"code","38d04e7c":"code","762bdb49":"code","defb9d60":"code","50c59882":"code","1cc1bb6b":"code","65faa23d":"code","d5ddaf7f":"code","311b6d0a":"code","757f1a72":"code","7726c78a":"code","8a630e0e":"code","33d9e6d0":"code","7131ac04":"code","0e59fa7c":"code","23d376e4":"code","bfda24c2":"code","ee4f17e4":"code","02f9243c":"code","9ed8dac7":"code","e4aa51a1":"code","c2382177":"code","008c0820":"code","da15107d":"code","2fa1e6ed":"code","03e17d54":"code","ef4efb1e":"code","620db481":"code","b451f6a3":"code","d25cf5b5":"code","a8247feb":"code","2e659379":"code","2e09ece4":"code","59657a6b":"code","1f33b47e":"markdown","2014b0a2":"markdown","fb793162":"markdown","0ea693e7":"markdown","7aa02eef":"markdown","f41e6a66":"markdown","58d22a15":"markdown","14b89b41":"markdown","1ae0610f":"markdown","e7bed64f":"markdown","53d2554b":"markdown","3bb4ebc2":"markdown","7a478449":"markdown","630ebe16":"markdown","037e3d82":"markdown","f4daf926":"markdown","6af82dee":"markdown","07e53933":"markdown","2a99dc0b":"markdown","1b18a561":"markdown","921e7872":"markdown","ae5c1b76":"markdown","15bc23df":"markdown","68e23984":"markdown","a4d487b1":"markdown"},"source":{"beb251a7":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","1469d321":"import warnings\nwarnings.filterwarnings(\"ignore\")\nfrom collections import Counter\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.preprocessing import LabelEncoder, MinMaxScaler","e4814954":"df = pd.read_csv(\"\/kaggle\/input\/weather-dataset-rattle-package\/weatherAUS.csv\")","8e9d09a8":"df.head()","afb0a17e":"df.info()","7e6696b3":"df[\"Date\"] = pd.to_datetime(df[\"Date\"])\ndf.drop(\"RISK_MM\",axis=1,inplace = True)\ndf[\"RainTomorrow\"] = [1 if each == \"Yes\" else 0 for each in df[\"RainTomorrow\"]]\ndf[\"RainToday\"] = [1 if each == \"Yes\" else 0 for each in df[\"RainToday\"]]","8cce2690":"df.describe()","1e32b1b2":"cat_cols = []\nnum_cols = []\nother_cols = []\n\nfor each in df.columns:\n    if df[each].dtype == \"object\":\n        cat_cols.append(each)\n    elif df[each].dtype == \"float64\":\n        num_cols.append(each)\n    else:\n        other_cols.append(each)\nprint(\"Categorical Columns: \",cat_cols)\nprint(\"Numerical Columns: \",num_cols)\nprint(\"Other Columns: \",other_cols)","34b3e65e":"def ctgplt(variable,to):\n    \n    \"Function for visualization of categorical variables.\"\n    \n    var = df[variable]\n    values=var.value_counts()\n    \n    f, ax = plt.subplots(figsize = (8,8))\n    g = sns.barplot(x = variable, y = to, data = df)\n    g.set_xticklabels(g.get_xticklabels(),rotation = 90)\n    plt.show()\n    \n    print(\"{}:\\n{}\".format(variable,values))\n\ndef numplt(data,variable,to):\n  \n  \"Function for visualization of numerical variables.\"\n\n  c = sns.FacetGrid(data,col=to,height=6)\n  c.map(sns.distplot,variable,bins=25)\n  plt.show()\n","776a2ecc":"for i in cat_cols:\n    ctgplt(i, \"RainTomorrow\")","c391da9d":"for k in num_cols:\n    numplt(df, k, \"RainTomorrow\")","38d04e7c":"sns.boxplot(x = df[\"Rainfall\"])\nplt.show()","762bdb49":"sns.boxplot(x= df[\"Evaporation\"])\nplt.show()","defb9d60":"corr = df.corr()\nmask = np.triu(np.ones_like(corr, dtype=np.bool))\nf, ax = plt.subplots(figsize=(12, 8))\ncmap = sns.diverging_palette(220, 10, as_cmap=True)\nsns.heatmap(corr, annot=True, mask=mask, cmap=cmap, ax=ax)\nplt.show()","50c59882":"df.drop(columns = [\"Temp3pm\", \"Temp9am\", \"Pressure9am\"], axis=1, inplace = True)","1cc1bb6b":"corr = df.corr()\nmask = np.triu(np.ones_like(corr, dtype=np.bool))\nf, ax = plt.subplots(figsize=(12, 8))\ncmap = sns.diverging_palette(220, 10, as_cmap=True)\nsns.heatmap(corr, annot=True, mask=mask, cmap=cmap, ax=ax)\nplt.show()","65faa23d":"# I removed the columns that i just deleted from dataframe.\nto_remove = (\"Temp3pm\", \"Temp9am\", \"Pressure9am\")\nnum_cols = [each for each in num_cols if each not in to_remove]","d5ddaf7f":"Q3 = df[\"Rainfall\"].quantile(0.75)\nQ1 = df[\"Rainfall\"].quantile(0.25)\n\nIQR = Q3 - Q1\nstep = IQR * 3\n\nmaxm = Q3 + step\nminm = Q1 - step\n\ndf = df[df[\"Rainfall\"].fillna(1) < (maxm)]\n\nQ3 = df[\"Evaporation\"].quantile(0.75)\nQ1 = df[\"Evaporation\"].quantile(0.25)\n\nIQR = Q3 - Q1\nstep = IQR * 3\n\nmaxm = Q3 + step\nminm = Q1 - step\n\ndf = df[df[\"Evaporation\"].fillna(1) < (maxm)]","311b6d0a":"sns.distplot(df[\"Evaporation\"])\nplt.show()","757f1a72":"df[\"RainTomorrow\"].value_counts()","7726c78a":"sns.countplot(x = \"RainTomorrow\", data=df, palette = \"RdBu\")\nplt.show()","8a630e0e":"def missing_values_table(data):\n        # Total missing values\n        mis_val = data.isnull().sum()\n        \n        # Percentage of missing values\n        mis_val_percent = 100 * data.isnull().sum() \/ len(data)\n        \n        # Make a table with the results\n        mis_val_table = pd.concat([mis_val, mis_val_percent], axis=1)\n        \n        # Rename the columns\n        mis_val_table_ren_columns = mis_val_table.rename(\n        columns = {0 : 'Missing Values', 1 : '% of Total Values'})\n        \n        # Sort the table by percentage of missing descending\n        mis_val_table_ren_columns = mis_val_table_ren_columns[\n            mis_val_table_ren_columns.iloc[:,1] != 0].sort_values(\n        '% of Total Values', ascending=False).round(1)\n        \n        # Return the dataframe with missing information\n        return mis_val_table_ren_columns","33d9e6d0":"missing_values_table(df)","7131ac04":"for i in cat_cols:\n    df[i].fillna(value=df[i].mode()[0],inplace=True)\n\nfor k in num_cols:\n    df[k].fillna(value=df[k].median(),inplace=True)","0e59fa7c":"df.isnull().sum()","23d376e4":"df[\"Year\"] = df[\"Date\"].dt.year\n\ndf[\"Month\"] = df[\"Date\"].dt.month\n\ndf[\"Day\"] = df[\"Date\"].dt.day\n\ndf.drop(\"Date\",axis=1,inplace=True)","bfda24c2":"df.head()","ee4f17e4":"le = LabelEncoder()\nmms = MinMaxScaler()\n\nfor each in cat_cols:\n    df[each] = le.fit_transform(df[each])\n\ndf[df.columns] = mms.fit_transform(df[df.columns])","02f9243c":"df.head()","9ed8dac7":"df.describe()","e4aa51a1":"from sklearn.model_selection import train_test_split, StratifiedKFold, GridSearchCV\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom xgboost import XGBClassifier\nfrom lightgbm import LGBMClassifier\nfrom sklearn.metrics import classification_report, confusion_matrix, f1_score","c2382177":"X = df.drop(\"RainTomorrow\",axis=1)\ny = df[\"RainTomorrow\"]\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42, stratify = y)","008c0820":"lr = LogisticRegression()\n\nlr.fit(X_train, y_train)\npreds = lr.predict(X_test)\nprint(\"train_score\",lr.score(X_train, y_train))\nprint(\"test_score\",lr.score(X_test,y_test))","da15107d":"cf_matrix = confusion_matrix(y_test, preds)","2fa1e6ed":"sns.heatmap(cf_matrix,annot = True, fmt=\"g\",cmap=\"Greens\")\nplt.show()","03e17d54":"xgb = XGBClassifier(objective = \"binary:logistic\")\nxgb.fit(X_train,y_train)\npred = xgb.predict(X_test)\nprint(xgb.score(X_train,y_train))\nprint(xgb.score(X_test,y_test))","ef4efb1e":"# I will not use a huge parameter grid because it will took to long to train, so here few parameters that could be useful.\nparams = {\n  'min_child_weight':[1,2],\n  'max_depth': [3,5],\n  'n_estimators':[200,300],\n  'colsample_bytree':[0.7,0.8],\n  'scale_pos_weight':[1.1,1.2]  \n}\n\nmodel = GridSearchCV(estimator=XGBClassifier(objective=\"binary:logistic\"), param_grid=params, cv=StratifiedKFold(n_splits=5), scoring=\"f1_macro\", n_jobs=-1, verbose=3)\nmodel.fit(X_train, y_train)\n\nprint(\"Best Score: \",model.best_score_)\nprint(\"Best Estimator: \",model.best_estimator_)","620db481":"mat = confusion_matrix(y_test,model.predict(X_test))\nsns.heatmap(mat,annot=True,cmap=\"Greens\", fmt=\"g\")\nplt.show()","b451f6a3":"print(classification_report(y_test,model.predict(X_test)))","d25cf5b5":"importances = pd.Series(data=xgb.feature_importances_,\n                        index= X_train.columns)\n\nimportances_sorted = importances.sort_values()\nplt.figure(figsize=(8,8))\nimportances_sorted.plot(kind='barh', color='lightgreen')\nplt.title('Features Importances')\nplt.show()","a8247feb":"from imblearn.over_sampling import SMOTE\n\nmethod = SMOTE()\n\nX_resampled, y_resampled = method.fit_sample(X_train, y_train)","2e659379":"xgb.fit(X_resampled, y_resampled)\npred1 = xgb.predict(X_test)\nprint(\"Train Score: \", xgb.score(X_resampled,y_resampled))\nprint(\"Test Score: \", xgb.score(X_test,y_test))","2e09ece4":"mat = confusion_matrix(y_test,pred1)\nsns.heatmap(mat,annot=True,cmap=\"Greens\", fmt=\"g\")\nplt.show()","59657a6b":"print(classification_report(y_test,pred1))","1f33b47e":"* We can see that we have many missing values.\n* And we have 23 columns. (21 if we exclude target feature and RISK_MM)\n* We are going to drop RISK_MM because our target feature is based on this feature, like it says in dataset description.\n* Date feature needs to be converted to datetime type. Currently it's object.\n* And lastly in RainTomorrow column i'll replace No with 0 and Yes with 1. I'll do same to RainToday column too.","2014b0a2":"* It's looking better now.","fb793162":"* We have some extreme outliers in Ranfall and Evaporation column, we'll inspect these outliers later.","0ea693e7":"* So now that we filled our missing values we can prepare out data for modeling.\n* i will drop date column after i seperated it to 3 pieces (year,month,day).\n* Label encode the categorical features.\n* Min max scale the numerical features.","7aa02eef":"* Most of the missing value percentages is between 0 - 10.\n* Cloud columns, Evaporation and Sunshine features have 40% missing data.\n* I will fill categorical variables with mode and numerical variables with median.","f41e6a66":"### Missing Values","58d22a15":"## Modeling","14b89b41":"* We cant even see the distribution. ","1ae0610f":"* First i'll define two functions to visualize categorical and numerical features to target feature. After that i'll look at relations between features.","e7bed64f":"* Rain rates for some cities are very low, but most of them are around 0.2.\n* Wind is coming from Northwest if its going to rain, mostly.","53d2554b":"* We get a better result with XGBoost but because we did not specify the parameters. Now lets apply Grid Search with xgb classifier.","3bb4ebc2":"* Recall is pretty low because we missclassified most of the zeros as ones. This is caused because of the imbalance.\n* I will use over-sampling with imblearn library to overcome this problem.\n* To learn what is over-sampling you can read here: https:\/\/imbalanced-learn.readthedocs.io\/en\/stable\/user_guide.html\n* Other then over-sampling what can we do?\n* We could do missing value imputation with KNN or MICE. These both are available in sklearn library. \n* We could do more feature engineering.\n","7a478449":"* Distribution looks better without outliers.","630ebe16":"### That's all. Thank you for reading, i hope you like it. ","037e3d82":"### **Features and what do they mean**\n* Date: The date of observation\n* Location: The common name of the lcoation of the weather station\n* MinTemp: The minimum temperature in degrees celcius\n* MaxTemp: The maximum temperature in degrees celsius\n* Rainfall: The amount of rainfall recorded for the day in mm\n* Evaporation: The so-called Class A pan evaporation (mm) in the 24 hours to 9am\n* Sunshine: The number of hours of bright sunshine in the day.\n* WindGustDir: The direction of the strongest wind gust in the 24 hours to midnight\n* WindGustSpeed: The speed (km\/h) of the strongest wind gust in the 24 hours to midnight\n* WindDir9am: Direction of the wind at 9am\n* WindDir3pm: Direction of the wind at 3pm\n* WindSpeed9am: Wind speed (km\/hr) averaged over 10 minutes prior to 9am\n* WindSpeed3pm: Wind speed (km\/hr) averaged over 10 minutes prior to 3pm\n* Humidity9am: Humidity (percent) at 9am\n* Humidity3pm: Humidity (percent) at 3pm\n* Pressure9am: Atmospheric pressure (hpa) reduced to mean sea level at 9am\n* Pressure3pm: Atmospheric pressure (hpa) reduced to mean sea level at 3pm\n* Cloud9am: Fraction of sky obscured by cloud at 9am. This is measured in \"oktas\", which are a unit of eigths. It records how many eigths of the sky are obscured by cloud. A 0 measure indicates completely clear sky whilst an 8 indicates that it is completely overcast.\n* Cloud3pm: Fraction of sky obscured by cloud (in \"oktas\": eighths) at 3pm. See Cload9am for a description of the values\n* Temp9am: Temperature (degrees C) at 9am\n* Temp3pm: Temperature (degrees C) at 3pm\n* RainToday: 1 if precipitation (mm) in the 24 hours to 9am exceeds 1mm, otherwise 0\n* RainTomorrow: The target variable. Did it rain tomorrow?","f4daf926":"* We have some highly correlated features. This collinear features wont do any good to our models. So we need to drop one of them.\n* Columns to drop: Temp3pm, Temp9am, Pressure9am. Lets drop these columns and look our correlation matrix again.","6af82dee":"* We have a imbalanced data.","07e53933":"* First i will fit a baseline model with logistic regression, then i will use xgboost.","2a99dc0b":"### Outlier Removal","1b18a561":"\n### UPDATE\n\n* After some time i checked the notebook and i realized that i resampled in the all data, so because of that accuracy is increasing. But now i fixed it and accuracy went down a little with over sampling.","921e7872":"* We can see that humidity and clouds are the most important features. But ratios are pretty close to each other.","ae5c1b76":"* So grid search is did not do better then default parameters. Let's look at the confusion matrix with default parameters.","15bc23df":"* This one is not bad as the Rainfall column but still has some serious outliers.","68e23984":"# Exploratory Data Analysis (EDA)","a4d487b1":"* Lets look at the graphs and comment the graps:\n* MinTemps are nicely distributed around 10 Degrees for RainTomorrow = 0. But for RainTomorrow = 1 its grouped around 15 degrees. And we have some more values around 25 - 30 degrees.\n* MaxTemps are stuck at 20 ish degrees for RainTomorrow = 1 and some in 30 - 35 degrees. But we can see that its warmer if tomorrow is a rainy day.\n* Like i said before Rainfall and Evaporation has some extreme outliers that makes impossible to commentate the graphs. I'll drop outliers and look at these graphs again.\n* Weather is mostly cloudy if its going to rain. (as expected) But no clouds if its not.\n* Wind is more distributed for RainTomorrow = 1 it's getting stronger if its going to rain.\n* 9 am wind speeds are almost same.\n* 3 pm wind speeds are a little bit strongter if its going to rain.\n* Humidity is more (as normal) if its going to rain. Especially in early hours of the day.\n* Atmospheric pressures distributions are almost same.\n* More clouds for rainy days.\n* Temperatures are not changed so much, like i said earlier most days grouped around 20 degrees for rainy days."}}