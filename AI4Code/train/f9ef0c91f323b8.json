{"cell_type":{"d7fb749f":"code","75aded94":"code","2b49218c":"code","c8db9eb4":"code","6c931d76":"code","1676dc68":"code","f7760015":"code","1b8b579e":"code","5a9f8d81":"code","7acef82d":"code","702ecfcf":"code","36cc2b1f":"code","2f46226d":"code","75e2dd92":"code","823df57b":"code","0be6c1eb":"code","428ef5d1":"code","5245d44e":"code","db69cd4a":"code","97f30b25":"code","7df45010":"code","671d41ee":"code","10458bd0":"code","24a6fc54":"code","28d23f00":"markdown","46eae082":"markdown","23080f0d":"markdown"},"source":{"d7fb749f":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","75aded94":"'''\n# Cleaning the data\n# read data in pandas dataframe\ndata = pd.read_csv(\"austin_weather.csv\")\n# drop (delete) the unnecessary columns in the data\ndata = data.drop(\n    ['Events', 'Date', 'SeaLevelPressureHighInches', 'SeaLevelPressureLowInches'], axis=1)\ndata = data.replace('T', 0.0)\ndata = data.replace('-', 0.0)\n# Save the data in a csv file\ndata.to_csv('austin_final.csv')\n'''\n\n# import the libraries\nimport pandas as pd\nimport numpy as np\nimport sklearn as sk\nfrom sklearn.linear_model import LinearRegression\nimport matplotlib.pyplot as plt\n\n","2b49218c":"# Cleaning the data\n# read data in pandas dataframe\ndata = pd.read_csv(\"\/kaggle\/input\/austin-weather\/austin_weather.csv\")\n# drop (delete) the unnecessary columns in the data\ndata = data.drop(\n    ['Events', 'Date', 'SeaLevelPressureHighInches', 'SeaLevelPressureLowInches'], axis=1)\ndata = data.replace('T', 0.0)\ndata = data.replace('-', 0.0)\n# Save the data in a csv file\ndata.to_csv('austin_final.csv')","c8db9eb4":"data.info(),data['PrecipitationSumInches'].describe()","6c931d76":"# read the cleaned data\n#data = pd.read_csv(\"\/kaggle\/input\/austin-weather\/austin_weather.csv\")\n\nX = data.drop(['PrecipitationSumInches'], axis=1)\n\nY = data['PrecipitationSumInches'].values.astype('float32')\nY = Y.reshape(-1, 1)\n\n","1676dc68":"X","f7760015":"day_index = 798\ndays = [i for i in range(Y.size)]\n#print(days)","1b8b579e":"from sklearn.model_selection import cross_val_score\n#from sklearn.metrics import tr\nfrom sklearn.model_selection import cross_val_score, train_test_split\n\nfrom sklearn.metrics import r2_score as r2\nimport xgboost as xg \nfrom sklearn.model_selection import train_test_split \nfrom sklearn.metrics import mean_squared_error as MSE \nfrom catboost import CatBoostRegressor\nfrom sklearn.svm import SVR\nfrom sklearn.ensemble import RandomForestRegressor\nfrom keras.models import Sequential\nfrom keras.models import load_model\nfrom keras.layers import Dense\nfrom sklearn.metrics import mean_squared_error as MSE\nfrom sklearn.metrics import mean_absolute_error as MAE\n","5a9f8d81":"def LR(X,y,k=10):\n    scores=[]\n    mse=[]\n    mae=[]\n    for i in range(k):\n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, random_state=0)\n        reg = LinearRegression()\n        reg.fit(X_train,y_train)\n        y_pred=reg.predict(X_test)\n        scores.append(r2(y_test,y_pred))\n        mse.append(MSE(y_test,y_pred))\n        mae.append(MAE(y_test,y_pred))\n    fig ,ax =plt.subplots()\n    ax.scatter(y_test,y_pred,edgecolors=(0,0,0))\n    ax.plot([y_test.min(),y_test.max()],[y_test.min(),y_test.max()],'k--',lw=4)\n    ax.set_label('Actual')\n    ax.set_label('Predicted')\n    plt.title('Test data viz (LR)')\n    plt.show()\n    return scores,mse,mse\n\ndef SVMRegressor(X,y,k=10):\n    scores=[]\n    mse=[]\n    mae=[]\n    for i in range(k):\n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, random_state=0)\n        reg = SVR()\n        reg.fit(X_train,y_train)\n        y_pred=reg.predict(X_test)\n        scores.append(r2(y_test,y_pred))\n        mse.append(MSE(y_test,y_pred))\n        mae.append(MAE(y_test,y_pred))\n    fig ,ax =plt.subplots()\n    ax.scatter(y_test,y_pred,edgecolors=(0,0,0))\n    ax.plot([y_test.min(),y_test.max()],[y_test.min(),y_test.max()],'k--',lw=4)\n    ax.set_label('Actual')\n    ax.set_label('Predicted')\n    plt.title('Test data viz (SVR)')\n    plt.show()\n    return scores,mse,mse\n    \n\ndef RF(X,y,k=10):\n    scores=[]\n    mse=[]\n    mae=[]\n    for i in range(k):\n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, random_state=0)\n        reg = RandomForestRegressor(max_depth=4, random_state=0)\n        reg.fit(X_train,y_train)\n        y_pred=reg.predict(X_test)\n        scores.append(r2(y_test,y_pred))\n        mse.append(MSE(y_test,y_pred))\n        mae.append(MAE(y_test,y_pred))\n    fig ,ax =plt.subplots()\n    ax.scatter(y_test,y_pred,edgecolors=(0,0,0))\n    ax.plot([y_test.min(),y_test.max()],[y_test.min(),y_test.max()],'k--',lw=4)\n    ax.set_label('Actual')\n    ax.set_label('Predicted')\n    plt.title('Test data viz (RF)')\n    plt.show()\n    return scores,mse,mse\n   \n\ndef ANN(X,y,k=10):\n    scores=[]\n    mse=[]\n    mae=[]\n    for i in range(k):\n        \n        # fit the keras model on the dataset\n\n        X_train , X_test, y_train, y_test = train_test_split(X, y, test_size = 0.15)\n        model = Sequential()\n        model.add(Dense(20, input_dim=X.shape[1],kernel_initializer='normal', activation='relu'))\n        #model.add(Dense(32,kernel_initializer='normal', activation='relu'))\n        model.add(Dense(16,kernel_initializer='normal', activation='relu'))\n        model.add(Dense(1, activation='relu'))\n        # Compile model\n        model.compile(loss='mean_squared_error', optimizer='adam')\n        model.fit(X_train.astype('float32'), y_train.astype('float32'),epochs=50, batch_size=5, verbose=0)\n        y_pred = model.predict(np.array(X_test.astype('float32')))\n        if len(scores)==0:\n            model.save('best_model.h5')\n        elif r2(y_test.astype('float32'),y_pred) > max(scores):\n            model.save('best_model.h5')\n        scores.append(r2(y_test.astype('float32'),y_pred))\n        mse.append(MSE(y_test,y_pred))\n        mae.append(MAE(y_test,y_pred))\n    model= load_model('best_model.h5')\n    X_train , X_test, y_train, y_test = train_test_split(X, y, test_size = 0.15)\n    y_pred = model.predict(np.array(X_test.astype('float32')))\n    fig ,ax =plt.subplots()\n    ax.scatter(y_test,y_pred,edgecolors=(0,0,0))\n    ax.plot([y_test.min(),y_test.max()],[y_test.min(),y_test.max()],'k--',lw=4)\n    ax.set_label('Actual')\n    ax.set_label('Predicted')\n    plt.title('Test data viz (ANN)')\n    plt.show()\n    return scores,mse,mse\n    \n\ndef Catboost(X,y,k=10):\n    scores=[]\n    mse=[]\n    mae=[]\n    for i in range(k):\n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, random_state=0)\n        model=CatBoostRegressor(iterations=100, depth=10, learning_rate=0.01,verbose=False, loss_function='RMSE')\n        model.fit(X_train,y_train ,plot=False)\n        y_pred=model.predict(X_test)\n        scores.append(r2(y_test,y_pred))\n        mse.append(MSE(y_test,y_pred))\n        mae.append(MAE(y_test,y_pred))\n    fig ,ax =plt.subplots()\n    ax.scatter(y_test,y_pred,edgecolors=(0,0,0))\n    ax.plot([y_test.min(),y_test.max()],[y_test.min(),y_test.max()],'k--',lw=4)\n    ax.set_label('Actual')\n    ax.set_label('Predicted')\n    plt.show()\n    return scores,mse,mse\n\ndef Xgboost(X,y,k=10):\n    scores=[]\n    mse=[]\n    mae=[]\n    for i in range(k):\n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=0)\n        xgb = xg.XGBRegressor(objective ='reg:linear', n_estimators = 3, seed = 123) \n        xgb.fit(X_train, y_train) \n        y_pred=xgb.predict(X_test)\n        scores.append(r2(y_test,y_pred))\n        mse.append(MSE(y_test,y_pred))\n        mae.append(MAE(y_test,y_pred))\n    return scores,mse,mse\n    \n\n","7acef82d":"print(\"Linear Regressor\")\nlr,lrmse,lrmae=LR(X,Y)\nprint(\"R2 score: \"+str(sum(lr)\/len(lr)))\nprint(\"MSE: \"+str(sum(lrmse)\/len(lrmse)))\nprint(\"MAE: \"+str(sum(lrmae)\/len(lrmae)))","702ecfcf":"print(\"CatBoost Regressor\")\ncb,cbmse,cbmae=Catboost(X,Y)\nprint(\"R2 score : \" +str(sum(cb)\/len(cb)))\nprint(\"MSE: \"+str(sum(cbmse)\/len(cbmse)))\nprint(\"MAE: \"+str(sum(cbmae)\/len(cbmae)))","36cc2b1f":"print(\"Support Vector Machine Regressor\")\nsvr,svrmse,svrmae=SVMRegressor(X,Y)\nprint(\"R2 score: \" +str(sum(svr)\/len(svr)))\nprint(\"MSE: \"+str(sum(svrmse)\/len(svrmse)))\nprint(\"MAE: \"+str(sum(svrmae)\/len(svrmae)))","2f46226d":"print(\"Random Forest Regressor\")\nrf,rfmse,rfmae=RF(X,Y)\nprint(\"R2 score: \" +str(sum(rf)\/len(rf)))\nprint(\"MSE: \"+str(sum(rfmse)\/len(rfmse)))\nprint(\"MAE: \"+str(sum(rfmae)\/len(rfmae)))","75e2dd92":"print(\"Regression with Artificial Neural Networks (ANN)\")\nann,annmse,annmae=ANN(X,Y)\nprint(\"R2 score: \" +str((sum(ann)\/len(ann))))\nprint(\"MSE: \"+str(sum(annmse)\/len(annmse)))\nprint(\"MAE: \"+str(sum(annmae)\/len(annmae)))","823df57b":"import numpy as np\nimport matplotlib.pyplot as plt","0be6c1eb":"models= {'LinearRegressor':lr,'CatBoost':cb,'SVR':svr,'RandomForest':rf,'ANN':ann}\n\nfig, ax = plt.subplots()\nplt.title('R2 scores')\nax.boxplot(models.values())\nax.set_xticklabels(models.keys())","428ef5d1":"models= {'LinearRegressor':lrmse,'CatBoost':cbmse,'SVR':svrmse,'RandomForest':rfmse,'ANN':annmse}\n\nfig, ax = plt.subplots()\nplt.title('Mean Squared Error')\nax.boxplot(models.values())\nax.set_xticklabels(models.keys())","5245d44e":"models= {'LinearRegressor':lrmae,'CatBoost':cbmae,'SVR':svrmae,'RandomForest':rfmae,'ANN':annmae}\n\nfig, ax = plt.subplots()\nplt.title('Mean Absolute Error')\nax.boxplot(models.values())\nax.set_xticklabels(models.keys())","db69cd4a":"model=CatBoostRegressor(iterations=100, depth=10, learning_rate=0.01, loss_function='RMSE')\n","97f30b25":"model.fit(X,Y ,plot=True)","7df45010":"inp = np.array([[74], [60], [45], [67], [49], [43], [33], [45],\n                [57], [29.68], [10], [7], [2], [0], [20], [4]])\n\ninp = inp.reshape(1, -1)\n\n# Print output\nprint('The precipitation in inches for the input is:', model.predict(inp))\n\nprint('The precipitation trend graph: ')\nplt.scatter(days, Y, color='g')\nplt.scatter(days[day_index], Y[day_index], color='r')\nplt.title('Precipitation level')\nplt.xlabel('Days')\nplt.ylabel('Precipitation in inches')\n\n# Plot a graph of precipitation levels vs n# of days\nplt.show()\n","671d41ee":"xgb = xg.XGBRegressor(objective ='reg:linear', \n                  n_estimators = 53, seed = 123) \n  \nxgb.fit(X.values, Y) \n  \n\n  ","10458bd0":"pred = xgb.predict(inp) ","24a6fc54":"inp = np.array([[74], [60], [45], [67], [49], [43], [33], [45],\n                [57], [29.68], [10], [7], [2], [0], [20], [4]])\n\ninp = inp.reshape(1, -1)\n\n# Print output\nprint('The precipitation in inches for the input is:', xgb.predict(inp))\n\nprint('The precipitation trend graph: ')\nplt.scatter(days, Y, color='g')\nplt.scatter(days[day_index], Y[day_index], color='r')\nplt.title('Precipitation level')\nplt.xlabel('Days')\nplt.ylabel('Precipitation in inches')\n\n# Plot a graph of precipitation levels vs n# of days\nplt.show()\n","28d23f00":"XGBOOST","46eae082":"**PLEASE IGNORE THE BELOW CODE **","23080f0d":"CATBOOST"}}