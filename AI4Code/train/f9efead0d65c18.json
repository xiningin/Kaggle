{"cell_type":{"868ffb0c":"code","d1ea3fa0":"code","e14760de":"code","ad94896f":"code","78c869b7":"code","a960b717":"code","4138db9b":"code","150efc8d":"code","50852ecc":"code","fc25ae57":"code","60dc0c51":"code","f5c641be":"code","fc65999e":"code","a0636d37":"code","135b2328":"code","80475ae7":"code","6b5465f9":"code","6bed7347":"code","653d79a2":"code","f344d4f9":"code","1f022c39":"code","bb291283":"code","287d565c":"code","38cd8d53":"code","29eae9d9":"code","28f535c9":"code","d4f68393":"code","beaddf0a":"code","c71674b1":"code","afd1a995":"code","0361d9ef":"code","5a157bdf":"code","061f6fc4":"code","36e36f25":"code","fe915966":"code","d152c8d9":"code","38161665":"code","4f258699":"code","66dda071":"markdown","35365e4a":"markdown","e6221872":"markdown","254d0db4":"markdown"},"source":{"868ffb0c":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","d1ea3fa0":"summary = pd.read_csv('\/kaggle\/input\/news-summary\/news_summary.csv', encoding='iso-8859-1')\nraw = pd.read_csv('\/kaggle\/input\/news-summary\/news_summary_more.csv', encoding='iso-8859-1')","e14760de":"summary.head(2)","ad94896f":"raw.head(2)","78c869b7":"pre1 =  raw.iloc[:,0:2].copy()\npre1['head + text'] = pre1['headlines'].str.cat(pre1['text'], sep =\" \") \n\npre2 = summary.iloc[:,0:6].copy()\npre2['head + text'] = pre2['author'].str.cat(pre2['date'].str.cat(pre2['headlines'].str.cat(pre2['read_more'].str.cat(pre2['text'].str.cat(pre2['ctext'], sep = \" \"), sep =\" \"),sep= \" \"), sep = \" \"),sep= \" \")","a960b717":"pre1.head(2)","4138db9b":"pre2.head(2)","150efc8d":"pre = pd.DataFrame()\npre['head + text'] = pd.concat([pre1['head + text'], pre2['head + text']], ignore_index=True)","50852ecc":"pre.head(2)","fc25ae57":"import re  # For preprocessing\nimport pandas as pd  # For data handling\nfrom time import time  # To time our operations\nfrom collections import defaultdict  # For word frequency\n\nimport spacy  # For preprocessing\n\nimport logging  # Setting up the loggings to monitor gensim\nlogging.basicConfig(format=\"%(levelname)s - %(asctime)s: %(message)s\", datefmt= '%H:%M:%S', level=logging.INFO)","60dc0c51":"#DATA CLEANSING\n#Scope for Improvement by loading lg model\n\nnlp = spacy.load('en', disable=['ner', 'parser']) # disabling Named Entity Recognition for speed\n\ndef cleaning(doc):\n    # Lemmatizes and removes stopwords\n    # doc needs to be a spacy Doc object\n    txt = [token.lemma_ for token in doc if not token.is_stop]\n    # Word2Vec uses context words to learn the vector representation of a target word,\n    # if a sentence is only one or two words long,\n    # the benefit for the training is very small\n    if len(txt) > 2:\n        return ' '.join(txt)","f5c641be":"#Removes non-alphabetic characters:\ndef text_strip(column):\n    for row in column:\n        \n        #ORDER OF REGEX IS VERY VERY IMPORTANT!!!!!!\n        \n        row=re.sub(\"(\\\\t)\", ' ', str(row)).lower() #remove escape charecters\n        row=re.sub(\"(\\\\r)\", ' ', str(row)).lower() \n        row=re.sub(\"(\\\\n)\", ' ', str(row)).lower()\n        \n        row=re.sub(\"(__+)\", ' ', str(row)).lower()   #remove _ if it occors more than one time consecutively\n        row=re.sub(\"(--+)\", ' ', str(row)).lower()   #remove - if it occors more than one time consecutively\n        row=re.sub(\"(~~+)\", ' ', str(row)).lower()   #remove ~ if it occors more than one time consecutively\n        row=re.sub(\"(\\+\\++)\", ' ', str(row)).lower()   #remove + if it occors more than one time consecutively\n        row=re.sub(\"(\\.\\.+)\", ' ', str(row)).lower()   #remove . if it occors more than one time consecutively\n        \n        row=re.sub(r\"[<>()|&\u00a9\u00f8\\[\\]\\'\\\",;?~*!]\", ' ', str(row)).lower() #remove <>()|&\u00a9\u00f8\"',;?~*!\n        \n        row=re.sub(\"(mailto:)\", ' ', str(row)).lower() #remove mailto:\n        row=re.sub(r\"(\\\\x9\\d)\", ' ', str(row)).lower() #remove \\x9* in text\n        row=re.sub(\"([iI][nN][cC]\\d+)\", 'INC_NUM', str(row)).lower() #replace INC nums to INC_NUM\n        row=re.sub(\"([cC][mM]\\d+)|([cC][hH][gG]\\d+)\", 'CM_NUM', str(row)).lower() #replace CM# and CHG# to CM_NUM\n        \n        \n        row=re.sub(\"(\\.\\s+)\", ' ', str(row)).lower() #remove full stop at end of words(not between)\n        row=re.sub(\"(\\-\\s+)\", ' ', str(row)).lower() #remove - at end of words(not between)\n        row=re.sub(\"(\\:\\s+)\", ' ', str(row)).lower() #remove : at end of words(not between)\n        \n        row=re.sub(\"(\\s+.\\s+)\", ' ', str(row)).lower() #remove any single charecters hanging between 2 spaces\n        \n        #Replace any url as such https:\/\/abc.xyz.net\/browse\/sdf-5327 ====> abc.xyz.net\n        try:\n            url = re.search(r'((https*:\\\/*)([^\\\/\\s]+))(.[^\\s]+)', str(row))\n            repl_url = url.group(3)\n            row = re.sub(r'((https*:\\\/*)([^\\\/\\s]+))(.[^\\s]+)',repl_url, str(row))\n        except:\n            pass #there might be emails with no url in them\n        \n\n        \n        row = re.sub(\"(\\s+)\",' ',str(row)).lower() #remove multiple spaces\n        \n        #Should always be last\n        row=re.sub(\"(\\s+.\\s+)\", ' ', str(row)).lower() #remove any single charecters hanging between 2 spaces\n\n        \n        \n        yield row","fc65999e":"brief_cleaning = text_strip(pre['text'])","a0636d37":"#Taking advantage of spaCy .pipe() method to speed-up the cleaning process:\n\nt = time()\n\n#Batch the data points into 5000 and run on all cores for faster preprocessing\ntxt = [cleaning(doc) for doc in nlp.pipe(brief_cleaning, batch_size=10000, n_threads=-1)]\n\n#Takes 7-8 mins\nprint('Time to clean up everything: {} mins'.format(round((time() - t) \/ 60, 2)))","135b2328":"txt[0]","80475ae7":"pre_clean = pd.DataFrame({'cleaned head + text': txt})\npre_clean = pre_clean.dropna().drop_duplicates()\npre_clean.shape","6b5465f9":"# Bigrams:\n# We are using Gensim Phrases package to automatically detect common phrases (bigrams) from a list of sentences. https:\/\/radimrehurek.com\/gensim\/models\/phrases.html\n\n# The main reason we do this is to catch words like \"t_mobile\" or \"nbc_universal\" !\n\n\nfrom gensim.models.phrases import Phrases, Phraser\n\n#Split the words by space\nsent = [row.split() for row in pre_clean['cleaned head + text']]\n\n#They should appear in the joint form minimum 50 times\n#Phrases() takes a list of list of words as input:\n\nphrases = Phrases(sent, min_count=50, progress_per=10000)\n","6bed7347":"#The goal of Phraser() is to cut down memory consumption of Phrases(), \n#by discarding model state not strictly needed for the bigram detection task:\nbigram = Phraser(phrases)","653d79a2":"sentences  = bigram[sent]","f344d4f9":"#Sentences is basically a list of list fed to w2v for training\nsentences[0]","1f022c39":"#Sort the words by thier frequency and print them\n\nword_freq = defaultdict(int)\nfor sent in sentences:\n    for i in sent:\n        word_freq[i] += 1\nprint(len(word_freq))\n\nprint(sorted(word_freq, key=word_freq.get, reverse=True)[:10])\n","bb291283":"#---------------------Training the W2V Model-----------------------#\n\nimport multiprocessing\nfrom gensim.models import Word2Vec\n\ncores = multiprocessing.cpu_count() # Count the number of cores in a computer\n\n#initialize w2v parameters\nw2v_model = Word2Vec(min_count=30,\n                     window=2,\n                     size=200,\n                     sample=6e-5, \n                     alpha=0.03, \n                     min_alpha=0.0007, \n                     negative=20,\n                     workers=cores-1)","287d565c":"#Build the Vocab of the w2v , but right now it is not trained on the sentences yet\nt = time()\n\nw2v_model.build_vocab(sentences, progress_per=10000)\n\nprint('Time to build vocab: {} mins'.format(round((time() - t) \/ 60, 2)))","38cd8d53":"#Now actually train model on the list of list of words\nt = time()\n\nw2v_model.train(sentences, total_examples=w2v_model.corpus_count, epochs=30, report_delay=1)\n\n#Takes 10 mins for 79mil words.\nprint('Time to train the model: {} mins'.format(round((time() - t) \/ 60, 2)))","29eae9d9":"#This makes the model memory-efficient\nw2v_model.init_sims(replace=True)\n","28f535c9":"#Save the Model to disk\nw2v_model.save(fname_or_handle = \"w2v_text_summ_200d_09162019\")","d4f68393":"#-------------------------DATA VISUALIZATION--------------------------#\nimport numpy as np\nimport pandas as pd\nfrom sklearn.datasets import fetch_mldata\nfrom sklearn.decomposition import PCA\nfrom sklearn.manifold import TSNE\nimport gensim\nfrom gensim.models import Word2Vec\n%matplotlib inline\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D\n","beaddf0a":"#LOAD A W2V Model\nMODEL_NAME = 'w2v_text_summ_200d_09162019'\nEMAIL_W2V =  MODEL_NAME\nemail_model =  gensim.models.Word2Vec.load(EMAIL_W2V)\n","c71674b1":"import time\n\ndef tsne_plot(model):\n    start =  time.time()\n    \"Creates and TSNE model and plots it\"\n    labels = []\n    tokens = []\n\n    for word in model.wv.vocab:\n        tokens.append(model[word])\n        labels.append(word)\n    \n    tsne_model = TSNE(perplexity=40, n_components=2, init='pca', n_iter=2500, random_state=23)\n    new_values = tsne_model.fit_transform(tokens)\n\n    x = []\n    y = []\n    for value in new_values:\n        x.append(value[0])\n        y.append(value[1])\n    end = time.time()\n    print('{} seconds taken to index the w2v model'.format(end-start))\n\n\n    \n    return x,y,labels","afd1a995":"def matplot_plot(x,y,labels):\n    plt.figure(figsize=(16, 16)) \n    for i in range(len(x)):\n        plt.scatter(x[i],y[i])\n        plt.annotate(labels[i],\n                     xy=(x[i], y[i]),\n                     xytext=(5, 2),\n                     textcoords='offset points',\n                     ha='right',\n                     va='bottom')\n    plt.show()","0361d9ef":"#--------------BOKEH PLOT------------------#\nfrom bokeh.plotting import *\nfrom bokeh.models import *\nfrom bokeh.layouts import *\nfrom bokeh.io import *\nfrom bokeh.embed import *\nfrom bokeh.resources import *\nimport pandas as pd\noutput_notebook() \ndef bokehplotw2v(x,y,labels,MODEL_NAME):\n    df = pd.DataFrame(list(zip(x, y, labels)),columns =['X', 'Y', 'LABELS']) \n    source = ColumnDataSource(data=df)\n    labeled_w2v_plot = figure(plot_width=1500,title=\"Labelled W2V graph\")\n    labels = LabelSet(x='X', y='Y', text='LABELS', level='glyph',source=source,x_offset=5, y_offset=5)\n    labeled_w2v_plot.circle(x, y)   \n    labeled_w2v_plot.add_layout(labels)\n    simple_w2v_plot = figure(plot_width=labeled_w2v_plot.plot_width, x_range=labeled_w2v_plot.x_range, y_range=labeled_w2v_plot.y_range,title=\"Simple W2V graph\")\n    simple_w2v_plot.circle(x, y)\n    plot = column(labeled_w2v_plot, simple_w2v_plot)\n    html_name = MODEL_NAME + '.html'\n    output_file(html_name,mode='inline')\n    show(plot)\n    save(plot)\n\n\n","5a157bdf":"import numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\n \nimport seaborn as sns\nsns.set_style(\"darkgrid\")\n\nfrom sklearn.decomposition import PCA\nfrom sklearn.manifold import TSNE\n\n\ndef tsnescatterplot(model, word, list_names,VECTOR_DIMENTION):\n    \"\"\" Plot in seaborn the results from the t-SNE dimensionality reduction algorithm of the vectors of a query word,\n    its list of most similar words, and a list of words.\n    \"\"\"\n    arrays = np.empty((0, VECTOR_DIMENTION), dtype='f')\n    word_labels = [word]\n    color_list  = ['red']\n\n    # adds the vector of the query word\n    arrays = np.append(arrays, model.wv.__getitem__([word]), axis=0)\n    \n    # gets list of most similar words\n    close_words = model.wv.most_similar([word])\n    \n    # adds the vector for each of the closest words to the array\n    for wrd_score in close_words:\n        wrd_vector = model.wv.__getitem__([wrd_score[0]])\n        word_labels.append(wrd_score[0])\n        color_list.append('blue')\n        arrays = np.append(arrays, wrd_vector, axis=0)\n    \n    # adds the vector for each of the words from list_names to the array\n    for wrd in list_names:\n        wrd_vector = model.wv.__getitem__([wrd])\n        word_labels.append(wrd)\n        color_list.append('green')\n        arrays = np.append(arrays, wrd_vector, axis=0)\n        \n    # Reduces the dimensionality from 300 to 50 dimensions with PCA\n    reduc = PCA(n_components=12).fit_transform(arrays)\n    \n    # Finds t-SNE coordinates for 2 dimensions\n    np.set_printoptions(suppress=True)\n    \n    Y = TSNE(n_components=2, random_state=0, perplexity=15).fit_transform(reduc)\n    \n    # Sets everything up to plot\n    df = pd.DataFrame({'x': [x for x in Y[:, 0]],\n                       'y': [y for y in Y[:, 1]],\n                       'words': word_labels,\n                       'color': color_list})\n    \n    fig, _ = plt.subplots()\n    fig.set_size_inches(9, 9)\n    \n    # Basic plot\n    p1 = sns.regplot(data=df,\n                     x=\"x\",\n                     y=\"y\",\n                     fit_reg=False,\n                     marker=\"o\",\n                     scatter_kws={'s': 40,\n                                  'facecolors': df['color']\n                                 }\n                    )\n    \n    # Adds annotations one by one with a loop\n    for line in range(0, df.shape[0]):\n         p1.text(df[\"x\"][line],\n                 df['y'][line],\n                 '  ' + df[\"words\"][line].title(),\n                 horizontalalignment='left',\n                 verticalalignment='bottom', size='medium',\n                 color=df['color'][line],\n                 weight='normal'\n                ).set_size(15)\n\n    \n    plt.xlim(Y[:, 0].min()-50, Y[:, 0].max()+50)\n    plt.ylim(Y[:, 1].min()-50, Y[:, 1].max()+50)\n            \n    plt.title('t-SNE visualization for {}'.format(word.title()))\n    \n","061f6fc4":"#Index Entire Vocab (takes 16 mins for 79mil words)\nx,y,labels = tsne_plot(email_model)","36e36f25":"# #Plot w2v in matplotlib\n# matplot_plot(x,y,labels)","fe915966":"#Plot w2v in Bokeh\nbokehplotw2v(x,y,labels,MODEL_NAME)","d152c8d9":"#10 Most similar words vs. n Random words:\nWORD ='pay'\nVECTOR_DIMENTION=200\nlist_names=['salary']\ntsnescatterplot(email_model,WORD.lower(),list_names,VECTOR_DIMENTION)","38161665":"#10 Most similar words vs. 10 Most dissimilar\n\nWORD ='salary'\nVECTOR_DIMENTION=200\nlist_names=[i[0] for i in email_model.wv.most_similar(negative=[WORD.lower()])]\ntsnescatterplot(email_model,WORD.lower(),list_names,VECTOR_DIMENTION )","4f258699":"#10 Most similar words vs. 11th to 20th Most similar words:\n    \nWORD ='kant'\nVECTOR_DIMENTION=200\nlist_names=[t[0] for t in email_model.wv.most_similar(positive=[WORD.lower()], topn=20)][10:]\ntsnescatterplot(email_model, WORD.lower(),list_names,VECTOR_DIMENTION)","66dda071":"1. **(((STEP 1))) -  Create a W2V model more creating word vectors**\n![main-qimg-88f6b1f2c060b6625c86218284bb0620.png](attachment:main-qimg-88f6b1f2c060b6625c86218284bb0620.png)\n#Resource - https:\/\/www.kaggle.com\/pierremegret\/gensim-word2vec-tutorial","35365e4a":"**The below are the methods which generate the plots**","e6221872":"**Data Visualization**\n\n![data-visualization.jpg](attachment:data-visualization.jpg)","254d0db4":"**Generate a W2V model**\n![word2vec-skip-gram.png](attachment:word2vec-skip-gram.png)"}}