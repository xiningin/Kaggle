{"cell_type":{"7eb3d971":"code","8b6be465":"code","c155727c":"code","0965c9b2":"code","fad881aa":"code","991a1009":"code","d11f8696":"code","ca5902f4":"code","7f277b0a":"code","00a2ab52":"code","cf44ff9d":"code","f025d339":"code","fd395eb6":"code","4199a708":"code","7d469c9e":"code","91f604e9":"code","d924ed9a":"code","eededdfc":"code","de5f48a0":"code","7b11909d":"code","0fae784a":"code","c0febb3e":"code","52b82865":"code","1a0e0eaa":"code","8c3f412e":"code","7ba3939f":"code","9f53cc3b":"markdown","e268f2aa":"markdown"},"source":{"7eb3d971":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport nltk\nimport re\nfrom sklearn.model_selection import train_test_split\nimport os\nimport pathlib\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport seaborn as sns\nimport tensorflow as tf\n\nfrom tensorflow.keras.layers.experimental import preprocessing\nfrom tensorflow.keras import layers\nfrom tensorflow.keras import models\nfrom IPython import display\nfrom tensorflow.compat.v1 import ConfigProto\nfrom tensorflow.compat.v1 import InteractiveSession\nfrom collections import Counter\nconfig = ConfigProto()\nconfig.gpu_options.allow_growth = True\nsession = InteractiveSession(config=config)","8b6be465":"data=pd.read_csv('..\/input\/ukrainian-ostotextdataset42-dataframe\/Ukrainian_Open_Speech_To_Text Dataset.csv')\n\ndata['number_of_words'] = data['text'].apply(lambda x: len(str(x).split()))\ndata['words'] = data['text'].apply(lambda x: str(x).split())\nprint(data)\n","c155727c":"pd.set_option('display.max_columns', None)\npd.set_option('display.max_colwidth', -1)\nprint(data.shape)\n\nprint(data.loc[data['number_of_words'] == 1])\n","0965c9b2":"datasets=data['dataset'].unique()\ndata=data.dropna()#.fillna(value='', inplace=True)\n#data=data.loc[data['dataset'] == 'VR\/']\n\n#data=data.loc[data['dataset'] != '1TVUKRAINIAN\/']\n#data=data.loc[data['dataset'] != 'GROSHI\/']\n#data=data.loc[data['dataset'] != 'MON\/']\n\ndata=data.loc[data['number_of_words'] == 1]\ndata['count'] = data.groupby('text')['text'].transform('count')\n\ndata=data.loc[data['count'] > 10]\ndata=data[['path','text']]\ndata['text'] = data['text'].str.replace(',','-')\nprint('kaggle datasets:',datasets)\ncommands=data['text'].unique()\nprint('commands:',len(commands))\n","fad881aa":"#data=data.dropna()#.fillna(value='', inplace=True)","991a1009":"print(len(data))\nval_size=0.2#0.2\ntest_size=0.1\n#val_size=round(val_size*len(data))\n#test_size=round(test_size*len(data))\n\nfrom sklearn.model_selection import train_test_split\n\ntrain_val_files_pd,test_files_pd,_,__= train_test_split(data,data['text'],test_size=test_size,stratify=data['text'], random_state=10)\n\ntrain_files_pd,val_files_pd,_,__= train_test_split(train_val_files_pd,train_val_files_pd['text'],test_size=val_size,stratify=train_val_files_pd['text'], random_state=11)\n#train_files_pd = data[:len(data)-val_size-test_size]\n#val_files_pd = data[len(data)-val_size-test_size: len(data)-test_size]\n#test_files_pd = data[len(data)-test_size:]\n\ntrain_files=train_files_pd.to_numpy()\nval_files=val_files_pd.to_numpy()\ntest_files=test_files_pd.to_numpy()\n\nprint('Training set size', len(train_files))\nprint('Validation set size', len(val_files))\nprint('Test set size', len(test_files))","d11f8696":"def get_label(file_path):\n  parts = tf.strings.split(file_path, os.path.sep)\n\n  # Note: You'll use indexing here instead of tuple unpacking to enable this \n  # to work in a TensorFlow graph.\n  return parts[-2]","ca5902f4":"def decode_audio(audio_binary):\n  audio, _ = tf.audio.decode_wav(audio_binary)\n  return tf.squeeze(audio, axis=-1)","7f277b0a":"def get_waveform_and_label(file_path):\n    label = file_path[1]\n    audio_binary = tf.io.read_file(file_path[0])\n    print(audio_binary)\n    waveform = decode_audio(audio_binary)\n    return waveform, label","00a2ab52":"AUTOTUNE = tf.data.AUTOTUNE\nfiles_ds = tf.data.Dataset.from_tensor_slices(train_files)\nprint(files_ds)\nwaveform_ds = files_ds.map(get_waveform_and_label, num_parallel_calls=AUTOTUNE)\nprint(files_ds)\n","cf44ff9d":"rows = 3\ncols = 3\nn = rows*cols\nfig, axes = plt.subplots(rows, cols, figsize=(10, 12))\nfor i, (audio, label) in enumerate(waveform_ds.take(n)):\n  r = i \/\/ cols\n  c = i % cols\n  ax = axes[r][c]\n  ax.plot(audio.numpy())\n  ax.set_yticks(np.arange(-1.2, 1.2, 0.2))\n  label = label.numpy().decode('utf-8')\n  ax.set_title(label)\n\nplt.show()","f025d339":"def get_spectrogram(waveform):\n  # Padding for files with less than 16000 samples\n  #if waveform.get_shape().as_list()[0]:\n  print (([50000] -tf.shape(waveform))<0)\n  if ([50000] -tf.shape(waveform))<0:\n    waveform=tf.slice(waveform, [0], [50000])\n  zero_padding = tf.zeros([50000] - tf.shape(waveform), dtype=tf.float32)\n  #print(waveform.get_shape().as_list()[0])  \n  #print(tf.slice(waveform, [0], [100000], name=None\n\n\n  # Concatenate audio with padding so that all audio clips will be of the \n  # same length\n  waveform = tf.cast(waveform, tf.float32)\n  equal_length = tf.concat([waveform, zero_padding], 0)\n  spectrogram = tf.signal.stft(\n      equal_length, frame_length=255, frame_step=128)\n\n  spectrogram = tf.abs(spectrogram)\n\n  return spectrogram","fd395eb6":"for waveform, label in waveform_ds.take(1):\n  label = label.numpy().decode('utf-8')\n  spectrogram = get_spectrogram(waveform)\n\nprint('Label:', label)\nprint('Waveform shape:', waveform.shape)\nprint('Spectrogram shape:', spectrogram.shape)\nprint('Audio playback')\ndisplay.display(display.Audio(waveform, rate=16000))","4199a708":"def plot_spectrogram(spectrogram, ax):\n  # Convert to frequencies to log scale and transpose so that the time is\n  # represented in the x-axis (columns).\n  log_spec = np.log(spectrogram.T)\n  height = log_spec.shape[0]\n  width = log_spec.shape[1]\n  X = np.linspace(0, np.size(spectrogram), num=width, dtype=int)\n  Y = range(height)\n  ax.pcolormesh(X, Y, log_spec)\n\n\nfig, axes = plt.subplots(2, figsize=(12, 8))\ntimescale = np.arange(waveform.shape[0])\naxes[0].plot(timescale, waveform.numpy())\naxes[0].set_title('Waveform')\naxes[0].set_xlim([0, 16000])\nplot_spectrogram(spectrogram.numpy(), axes[1])\naxes[1].set_title('Spectrogram')\nplt.show()","7d469c9e":"def get_spectrogram_and_label_id(audio, label):\n  spectrogram = get_spectrogram(audio)\n  spectrogram = tf.expand_dims(spectrogram, -1)\n  label_id = tf.argmax(label == commands)\n  return spectrogram, label_id","91f604e9":"spectrogram_ds = waveform_ds.map(\n    get_spectrogram_and_label_id, num_parallel_calls=AUTOTUNE)","d924ed9a":"rows = 3\ncols = 3\nn = rows*cols\nfig, axes = plt.subplots(rows, cols, figsize=(10, 10))\nfor i, (spectrogram, label_id) in enumerate(spectrogram_ds.take(n)):\n  r = i \/\/ cols\n  c = i % cols\n  ax = axes[r][c]\n  plot_spectrogram(np.squeeze(spectrogram.numpy()), ax)\n  ax.set_title(commands[label_id.numpy()])\n  ax.axis('off')\n\nplt.show()","eededdfc":"def preprocess_dataset(files):\n  files_ds = tf.data.Dataset.from_tensor_slices(files)\n  output_ds = files_ds.map(get_waveform_and_label, num_parallel_calls=AUTOTUNE)\n  output_ds = output_ds.map(\n      get_spectrogram_and_label_id,  num_parallel_calls=AUTOTUNE)\n  return output_ds","de5f48a0":"train_ds = spectrogram_ds\nval_ds = preprocess_dataset(val_files)\ntest_ds = preprocess_dataset(test_files)","7b11909d":"batch_size = 128\ntrain_ds = train_ds.batch(batch_size)\nval_ds = val_ds.batch(batch_size)","0fae784a":"for spectrogram, _ in spectrogram_ds.take(1):\n  input_shape = spectrogram.shape\nprint('Input shape:', input_shape)\nnum_labels = len(commands)\nprint('num_labels:', num_labels)\nnorm_layer = preprocessing.Normalization()\n#norm_layer.adapt(spectrogram_ds.map(lambda x, _: x))\n\nmodel = models.Sequential([\n    layers.Input(shape=input_shape),\n    preprocessing.Resizing(96, 96), \n    norm_layer,\n    layers.Conv2D(512, 3, activation='relu'),\n    layers.MaxPooling2D(),\n    layers.Dropout(0.25),\n    layers.Conv2D(256, 3, activation='relu'),\n    layers.MaxPooling2D(),\n    layers.Dropout(0.25),\n    layers.Flatten(),\n    layers.Dense(256, activation='relu'),\n    layers.Dropout(0.5),\n    layers.Dense(num_labels),\n])\n\nmodel.summary()","c0febb3e":"model.compile(\n    optimizer=tf.keras.optimizers.Adam(),\n    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n    metrics=['accuracy'],\n)","52b82865":"EPOCHS = 30\nhistory = model.fit(\n    train_ds, \n    #steps_per_epoch =20,\n    validation_data=val_ds,  \n    epochs=EPOCHS,\n    #callbacks=tf.keras.callbacks.EarlyStopping(verbose=1, patience=2),\n)","1a0e0eaa":"metrics = history.history\nplt.plot(history.epoch, metrics['loss'], metrics['val_loss'])\nplt.legend(['loss', 'val_loss'])\nplt.show()","8c3f412e":"test_audio = []\ntest_labels = []\n\nfor audio, label in test_ds:\n  test_audio.append(audio.numpy())\n  test_labels.append(label.numpy())\n\ntest_audio = np.array(test_audio)\ntest_labels = np.array(test_labels)","7ba3939f":"y_pred = np.argmax(model.predict(test_audio), axis=1)\ny_true = test_labels\n\ntest_acc = sum(y_pred == y_true) \/ len(y_true)\nprint(f'Test set accuracy: {test_acc:.0%}')","9f53cc3b":"Based on this example: https:\/\/www.tensorflow.org\/tutorials\/audio\/simple_audio","e268f2aa":"Hello everyone! This is an example of building a simple language recognition model for a Ukrainian Open Speech To Text Dataset"}}