{"cell_type":{"b9fa0fde":"code","c6915886":"code","49aa76c5":"code","f97ef55f":"code","d5e64017":"code","94023d70":"code","b0d09e45":"code","9af999ea":"code","bfcd315d":"code","caa92cef":"code","d87a6610":"code","aa780f8c":"code","323f0cd8":"code","42b7f743":"code","bf19cde8":"code","76252cf0":"code","a843a2db":"code","7726739a":"code","7be24d40":"code","5f85e3c1":"code","1a9c14cc":"code","9d1abaad":"code","ad25d03f":"code","74049a18":"code","c6472200":"code","3a9ebcba":"code","99040f94":"markdown","304b1eca":"markdown","4d0b9adc":"markdown","c9c7f66f":"markdown","bf5b6fa4":"markdown","01495d6f":"markdown","ac2333a6":"markdown","d1f656b4":"markdown","8d9941c9":"markdown"},"source":{"b9fa0fde":"!pip install pymorphy2[fast]","c6915886":"import re\nfrom collections import Counter\nimport pandas as pd\nimport numpy as np\nfrom gensim import corpora, models\nimport pymorphy2\nfrom wordcloud import WordCloud\nimport matplotlib.pyplot as plt","49aa76c5":"morph = pymorphy2.MorphAnalyzer()","f97ef55f":"def lemmatize(token):\n    return morph.parse(token)[0].normal_form","d5e64017":"news_df = pd.read_csv('\/kaggle\/input\/russian-news-2020\/news.csv')","94023d70":"news_df.head()","b0d09e45":"news_df.shape","9af999ea":"news_df.loc[news_df['source'] == 'ria.ru', 'publication_date'] = (news_df.loc[news_df['source'] == 'ria.ru', 'publication_date'].str\n                                                              .extract(r'(?P<date>\\d{2}\\.\\d{2}\\.\\d{4})', expand=False)\n                                                              .apply(lambda x: '-'.join(reversed(x.split('.'))) if type(x) is str else x))","bfcd315d":"news_df.loc[news_df['source'] == 'lenta.ru', 'publication_date'] = news_df.loc[news_df['source'] == 'lenta.ru', 'publication_date'].str.split('T').str.get(0)","caa92cef":"month_mapper = {\n    '\u044f\u043d\u0432\u0430\u0440\u044f': '01',\n    '\u0444\u0435\u0432\u0440\u0430\u043b\u044f': '02',\n    '\u043c\u0430\u0440\u0442\u0430': '03',\n    '\u0430\u043f\u0440\u0435\u043b\u044f': '04',\n    '\u043c\u0430\u044f': '05',\n    '\u0438\u044e\u043d\u044f': '06',\n    '\u0438\u044e\u043b\u044f': '07',\n    '\u0430\u0432\u0433\u0443\u0441\u0442\u0430': '08',\n    '\u0441\u0435\u043d\u0442\u044f\u0431\u0440\u044f': '09',\n    '\u043e\u043a\u0442\u044f\u0431\u0440\u044f': '10',\n    '\u043d\u043e\u044f\u0431\u0440\u044f': '11',\n    '\u0434\u0435\u043a\u0430\u0431\u0440\u044f': '12'\n}\nnews_df.loc[news_df['source'] == 'meduza.io', 'publication_date'] = (news_df.loc[news_df['source'] == 'meduza.io', 'publication_date']\n                                                                     .apply(lambda x: f'{x.split()[3]}-{month_mapper[x.split()[2]]}-{x.split()[1].zfill(2)}' if type(x) is str else x))","d87a6610":"news_df.loc[news_df['source'] == 'tjournal.ru', 'publication_date'] = pd.to_datetime(news_df.loc[news_df['source'] == 'tjournal.ru', 'publication_date'], unit='s').dt.strftime('%Y-%m-%d')","aa780f8c":"news_df.loc[news_df['source'] == 'tjournal.ru', 'text'] = news_df.loc[news_df['source'] == 'tjournal.ru', 'text'].str.replace('\\n', '').str.replace(r'\\s+', ' ')","323f0cd8":"news_df.loc[news_df['source'] == 'tjournal.ru', 'tags'] = news_df.loc[news_df['source'] == 'tjournal.ru', 'text'].str.findall(r'#\\w+').str.join(', ').str.replace('#', '')","42b7f743":"news_df.loc[news_df['source'] == 'tjournal.ru', 'text'] = news_df.loc[news_df['source'] == 'tjournal.ru', 'text'].apply(lambda x: x[:x.find('#')])","bf19cde8":"documents = news_df.text.tolist()","76252cf0":"texts = [\n    [lemmatize(word) for word in re.findall(r'\\w+', document.lower()) if len(word) > 2]\n    for document in documents\n]","a843a2db":"dictionary = corpora.Dictionary(texts)\ndictionary.filter_extremes(no_below=5, no_above=0.25, keep_n=25000)\ncorpus = [dictionary.doc2bow(text) for text in texts]","7726739a":"ldamodel = models.ldamulticore.LdaMulticore(corpus, id2word=dictionary, num_topics=100, passes=50, alpha='symmetric', eta=None, decay=0.5)","7be24d40":"perplexity = ldamodel.log_perplexity(corpus)\nprint(2**(-perplexity))","5f85e3c1":"for t, top_words in ldamodel.print_topics(num_topics=-1, num_words=10):\n    print(\"Topic\", t, \":\", top_words)\n    print()","1a9c14cc":"news_df['topic'] = [max(i, key=lambda x: x[1])[0] for i in ldamodel[corpus]]","9d1abaad":"for i in range(news_df.topic.max()):\n    print(f'Topic: {i}')\n    counts = news_df[news_df.topic == i].rubric.value_counts()\n    print(counts[counts > 5])\n    print()","ad25d03f":"for i in range(news_df.topic.max()):\n    print(f'Topic: {i}')\n    counts = news_df[news_df.topic == i].subrubric.value_counts()\n    print(counts[counts > 5])\n    print()","74049a18":"for i in range(news_df.topic.max()):\n    print(f'Topic: {i}')\n    tags = []\n    for i in news_df[news_df.topic == i].tags.dropna():\n        tags += i.split(', ')\n    counts = Counter(tags)\n    print('\\n'.join(map(str, counts.most_common()[:5])))\n    print()","c6472200":"for i in range(news_df.topic.max()):\n    print(f'Topic: {i}')\n    frequencies = dict(ldamodel.show_topic(i, topn=100))\n    wordcloud = WordCloud(max_font_size=50, max_words=100, background_color=\"white\").generate_from_frequencies(frequencies)\n    plt.figure(figsize=(10, 5))\n    plt.imshow(wordcloud, interpolation=\"bilinear\")\n    plt.axis(\"off\")\n    plt.show()","3a9ebcba":"f = plt.figure()\nf, ax = plt.subplots(100, 1, figsize=(75, 900))\n\nfor i, topic_name in enumerate(range(news_df.topic.max())):\n    counts = news_df[news_df.topic == topic_name]['publication_date'].dropna().value_counts().to_dict()\n    ax[i].bar(news_df['publication_date'].dropna().drop_duplicates().sort_values(), news_df['publication_date'].dropna().drop_duplicates().sort_values().map(counts))\n    ax[i].set_title(topic_name)\n    ax[i].tick_params(labelrotation=90)","99040f94":"# Perplexity\n\n\n![perplexity](https:\/\/wikimedia.org\/api\/rest_v1\/media\/math\/render\/svg\/fc7974a9bf394db8698fb76c0fa060c6c21068ed)","304b1eca":"# Analysis of the resulting topics\nLet's see the resulting topics and their most frequent words.","4d0b9adc":"# Training model\n![topic_modeling](https:\/\/miro.medium.com\/max\/1200\/1*IJw8N-HSEzLpwJDS6JVs-w.png)","c9c7f66f":"Let's see the distribution of rubrics, subrubrics and tags by topics","bf5b6fa4":"# Wordcloud\n\n\nVisualizing each topic with a word cloud","01495d6f":"# Data cleaning","ac2333a6":"Let's create a dictionary of words from our texts. Let's leave only words that occur at least 5 times and no more than 25% of documents.","d1f656b4":"# Text Preprocessing\nSplit the text into tokens, bring the tokens to normal form and take only tokens longer than two characters.","8d9941c9":"# Distribution of topics over time"}}