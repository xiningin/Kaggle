{"cell_type":{"9d7bdd63":"code","7f071384":"code","71c1cd11":"code","2b0693c6":"code","51c21220":"code","666cb8bd":"code","ede397e9":"code","1845d49c":"code","4d503081":"code","f2a66fd9":"code","ebfe26bb":"code","24247314":"markdown","fb37ff9a":"markdown","98d72e3f":"markdown","3c2cc01b":"markdown","06c54394":"markdown","a663330c":"markdown","bed090f5":"markdown","342f8224":"markdown"},"source":{"9d7bdd63":"!pip install pydotplus\n!pip install googletrans\n!pip install gTTS\n ","7f071384":"import pydotplus\nfrom IPython.display import display, Image\nimport googletrans  \nfrom gtts import gTTS\nimport IPython.display as ipd\n \n \n ","71c1cd11":"myplan=\"\"\"digraph { \n                Load_VGG16_Model_Restructure -> \n                Load_Pretring_Model -> \n                Show_image -> \n                Input_preprocess -> \n                Generate_English_Description -> \n                Translate_English_Description_To_Arabic -> \n                Read_Arabic_Description_by_gTTS \n        }\"\"\"\nmygraph=pydotplus.graph_from_dot_data(myplan)\nmygraph.write_png(\"myplan.png\")\ndisplay(Image(filename='.\/myplan.png'))\n","2b0693c6":"from pickle import load\nfrom numpy import argmax\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.applications.vgg16 import VGG16\nfrom keras.preprocessing.image import load_img\nfrom keras.preprocessing.image import img_to_array\nfrom keras.applications.vgg16 import preprocess_input\nfrom keras.models import Model\nfrom keras.models import load_model\n \n# extract features from each photo in the directory\ndef extract_features(filename):\n\t# load the model\n\tmodel = VGG16()\n\t# re-structure the model\n\tmodel = Model(inputs=model.inputs, outputs=model.layers[-2].output)\n\t# load the photo\n\timage = load_img(filename, target_size=(224, 224))\n\t# convert the image pixels to a numpy array\n\timage = img_to_array(image)\n\t# reshape data for the model\n\timage = image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))\n\t# prepare the image for the VGG model\n\timage = preprocess_input(image)\n\t# get features\n\tfeature = model.predict(image, verbose=0)\n\treturn feature\n \n# map an integer to a word\ndef word_for_id(integer, tokenizer):\n\tfor word, index in tokenizer.word_index.items():\n\t\tif index == integer:\n\t\t\treturn word\n\treturn None\n \n# generate a description for an image\ndef generate_desc(model, tokenizer, photo, max_length):\n\t# seed the generation process\n\tin_text = 'startseq'\n\t# iterate over the whole length of the sequence\n\tfor i in range(max_length):\n\t\t# integer encode input sequence\n\t\tsequence = tokenizer.texts_to_sequences([in_text])[0]\n\t\t# pad input\n\t\tsequence = pad_sequences([sequence], maxlen=max_length)\n\t\t# predict next word\n\t\tyhat = model.predict([photo,sequence], verbose=0)\n\t\t# convert probability to integer\n\t\tyhat = argmax(yhat)\n\t\t# map integer to word\n\t\tword = word_for_id(yhat, tokenizer)\n\t\t# stop if we cannot map the word\n\t\tif word is None:\n\t\t\tbreak\n\t\t# append as input for generating the next word\n\t\tin_text += ' ' + word\n\t\t# stop if we predict the end of the sequence\n\t\tif word == 'endseq':\n\t\t\tbreak\n\treturn in_text\n \n# load the tokenizer\ntokenizer = load(open('..\/input\/imagecaptiongenerator\/tokenizer.pkl', 'rb'))\n# pre-define the max sequence length (from training)\nmax_length = 34\n# load the model\nmodel = load_model('..\/input\/imagecaptiongenerator\/model_19.h5')\n\n\n","51c21220":"path = '..\/input\/flicker8k-dataset\/Flickr8k_Dataset\/Flicker8k_Dataset\/1019077836_6fc9b15408.jpg'\n# load and prepare the photograph\nphoto = extract_features(path)\n# generate description\nenglish_text = generate_desc(model, tokenizer, photo, max_length)\n \n","666cb8bd":"display(Image(filename=path))","ede397e9":"english_text=english_text.replace(\"startseq\", \"\").replace(\"endseq\", \"\")\nprint(english_text)","1845d49c":" # print(googletrans.LANGUAGES)","4d503081":"\ntranslator = googletrans.Translator()\narabic_text = translator.translate(english_text,dest='ar').text\n\nprint(arabic_text)\n","f2a66fd9":"tts = gTTS(arabic_text, lang='ar')\ntts.save('test.mp3')","ebfe26bb":"audio_path=\"test.mp3\"\n\nipd.Audio(audio_path, autoplay=True)","24247314":"\n<a id=\"8\"><\/a>\n\n<h2 style='background:blue; border:0; color:white'><center> References <\/center><h2>\n\n    \n* https:\/\/machinelearningmastery.com\/develop-a-deep-learning-caption-generation-model-in-python\/\n* https:\/\/www.kaggle.com\/wikiabhi\/image-caption-generator\n* http:\/\/pydotplus.readthedocs.org\/\n* https:\/\/github.com\/carlos-jenkins\/pydotplus\n* https:\/\/gtts.readthedocs.io\/en\/latest\/module.html","fb37ff9a":"\n\n<a id=\"1\"><\/a>\n<a id=\"2\"><\/a>\n<h2 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='background:blue; border:0; color:white' role=\"tab\" aria-controls=\"home\"><center>\n  Load_VGG16_Model&Restructure<br\/>\n  Load_Pretring_Model\n    <\/center><\/h2>\n ","98d72e3f":"\n\n<a id=\"7\"><\/a>\n\n<h2 style='background:blue; border:0; color:white'><center>  Read_Arabic_Description_by_gTTS  <\/center><h2>\n","3c2cc01b":"\n<a id=\"5\"><\/a>\n\n<h2 style='background:blue; border:0; color:white'><center> Generate_English_Description <\/center><h2>\n\n","06c54394":"# Thankyou for Reading and Do Upvote If you liked !!!","a663330c":"\n<a id=\"3\"><\/a>\n<h2 style='background:blue; border:0; color:white'><center>Show_image<\/center><h2>","bed090f5":" \n<a id=\"6\"><\/a>\n\n<h2 style='background:blue; border:0; color:white'><center> Translate_English_Description_To_Arabic <\/center><h2>\n\n\n","342f8224":"<div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n<h2 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='background:blue; border:0; color:white' role=\"tab\" aria-controls=\"home\"><center>Quick navigation<\/center><\/h2>\n\n* [1.Load_VGG16_Model&Restructure](#1)\n    \n* [2.Load_Pretring_Model](#2)\n    \n* [3.Show_image](#3)\n    \n* [4.Input preprocess](#4)\n    \n* [5.Generate_English_Description](#5)\n    \n* [6.Translate_English_Description_To_Arabic](#6)\n    \n* [7.Read_Arabic_Description_by_gTTS](#7)\n        \n* [8.References](#8)\n\n    \n    "}}