{"cell_type":{"5328b263":"code","b6abeca6":"code","9ddc9bdf":"code","ac1852d1":"code","75044a03":"code","8efdc814":"code","e77be197":"code","af686d65":"code","e53a83a3":"code","2e8be9fd":"code","ee6d9397":"code","19a68882":"code","c0bac208":"code","263eee2c":"code","24d6a97b":"code","add387d4":"code","622952c2":"code","3220228b":"code","83b5f366":"code","b9c67eb6":"code","9e063f7e":"code","09140fb0":"code","aff2deba":"code","121117b1":"code","b3f5c7c6":"code","c22c0df7":"code","ae6acaeb":"code","649137f1":"code","d342c8fb":"code","11cd3c76":"code","1538b433":"code","7e0eb49b":"code","1d20cc75":"code","23f1638d":"code","c33e37e5":"code","b73b927a":"code","318a7715":"code","157ed782":"markdown","5ed5d883":"markdown","30d393f3":"markdown","b726c7a7":"markdown","ff1ff2ca":"markdown","6012d066":"markdown","3ed97ce8":"markdown","ee92e781":"markdown","3778d09e":"markdown","fa61b993":"markdown","8ff1aef2":"markdown","97ae6bb5":"markdown","fc493bb0":"markdown","44e46074":"markdown","17ec399f":"markdown","a699b24d":"markdown","a4451059":"markdown","45c1d411":"markdown","c5495b27":"markdown","052fd3ad":"markdown","177e4619":"markdown","a29bc624":"markdown","4938bab0":"markdown","d39d9673":"markdown","692e50bf":"markdown","1017f782":"markdown","e2d0c4e1":"markdown","1a605c52":"markdown","a1226eef":"markdown","54d3ce2f":"markdown","daae1dcb":"markdown","a7f6b75d":"markdown","4eb335b6":"markdown","95e84751":"markdown","529d8835":"markdown","bf9b4102":"markdown","22cb975a":"markdown","c03f33b0":"markdown"},"source":{"5328b263":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns \n\nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.preprocessing import StandardScaler \nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom xgboost import XGBClassifier\nfrom sklearn.metrics import confusion_matrix, classification_report, accuracy_score \n\nimport warnings\nwarnings.filterwarnings('ignore')","b6abeca6":"df = pd.read_csv(\"..\/input\/pima-indians-diabetes-database\/diabetes.csv\")","9ddc9bdf":"# check the first five rows of data\ndf.head()","ac1852d1":"df.shape","75044a03":"# checking the distribution of outcomes\ndf[\"Outcome\"].value_counts()","8efdc814":"# checking null values in data\ndf.isnull().sum()","e77be197":"#checking correlation between features\nplt.figure(figsize=(10, 6))\nsns.heatmap(df.corr(), annot=True, linewidths=2,cmap='plasma')\nplt.show()","af686d65":"#checking datatypes of features\ndf.dtypes","e53a83a3":"# Summary Statistics\ndf.describe()","2e8be9fd":"#replacing zero values with NaN \nfeatures_with_zero = [\"Glucose\", \"BloodPressure\", \"SkinThickness\",\"Insulin\", \"BMI\"]\nfor col in features_with_zero:\n    df[col].replace(0, np.nan, inplace=True)","ee6d9397":"df.head()","19a68882":"# Now check again the count of Null values \ndf.isnull().sum()","c0bac208":"#Imputing mean instead of null values\nfor col in features_with_zero:\n    df[col].replace(np.nan, df[col].mean(), inplace=True)","263eee2c":"# summary statistics\ndf.describe()","24d6a97b":"X = df.drop('Outcome',axis=1)\ny = df['Outcome']","add387d4":"X_train, X_test, y_train, y_test = train_test_split(X, y,\n                                                   test_size=0.2,\n                                                   random_state=50)","622952c2":"ss = StandardScaler()\nX_train = ss.fit_transform(X_train)\nX_test = ss.transform(X_test)","3220228b":"#Helper functions to check the performance of different classifiers\n\n#function to display confusion matrix\ndef displayConfusionMatrix(y_test, y_pred):\n    \"\"\"Displays the confusion matrix in the form of heatmap.\n    \n    Parameters:\n    y_test (array-like): list of true labels\n    y_pred (array-like): list of predicted labels\n    \n    Returns:\n    acc_score (float): Accuracy score \n    \"\"\"\n    acc_score = accuracy_score(y_test, y_pred)\n    cm = confusion_matrix(y_test, y_pred)\n    sns.heatmap(cm, annot=True, fmt=\"d\",cmap='plasma')\n    plt.title(f\"Accuracy: {acc_score:0.3f}\")\n    plt.xlabel(\"Predicted labels\")\n    plt.ylabel(\"Actual labels\")\n    plt.show()\n    return acc_score\n\n#function to build model and display classification report of a classifer\ndef model(classifier, X_train=X_train, y_train=y_train,\n              X_test=X_test, y_test=y_test):\n    \"\"\"Fits the `classifier` to `X_train`, `y_train` and generate an elegant \n    classification report using `X_test` and `y_test`.\n    \n    Parameters:\n    classifer : classifier obj implementing 'fit' method.\n    X_train (array-like): 2D-array of input features of Training Set.\n    y_train (array-like): list of target features of Training Set.\n    X_test  (array-like): 2D-array of input features of Testing Set.\n    y_test  (array-like): list of target features of Testing Set.\n    \n    Returns:\n    acc_score (float): Accuracy score \n    \"\"\"\n    classifier.fit(X_train, y_train)\n    y_pred = classifier.predict(X_test)\n    acc_score = displayConfusionMatrix(y_test, y_pred)\n    print(classification_report(y_test, y_pred))\n    return acc_score","83b5f366":"log = LogisticRegression(random_state = 50)\nlog_accuracy = model(log)","b9c67eb6":"knn= KNeighborsClassifier(n_neighbors=7)\nknn_accuracy = model(knn)","9e063f7e":"linear_svm = SVC(kernel=\"linear\", random_state=50)\nlsvm_accuracy = model(linear_svm)","09140fb0":"radial_svm = SVC(kernel=\"rbf\", random_state=50)\nrsvm_accuracy = model(radial_svm)","aff2deba":"nb = GaussianNB()\nnb_accuracy = model(nb)","121117b1":"dt = DecisionTreeClassifier(criterion=\"entropy\",\n                                             random_state=50)\ndt_accuracy = model(dt)","b3f5c7c6":"rf = RandomForestClassifier(n_estimators=150,\n                                            criterion=\"entropy\",\n                                            random_state=50)\nrf_accuracy = model(rf)","c22c0df7":"xgb= XGBClassifier(use_label_encoder=False,\n                       verbosity=0)\nxgb_accuracy = model(xgb)","ae6acaeb":"models = pd.DataFrame({\n    'Model': [\"Logistic Regression\", \"KNN\", \"SVM-Linear\", \"SVM-RBF\", \n             \"Naive Bayes\", \"Decision Tree\", \"Random Forest\", \"XGBoost\"],\n    'Accuracy Score': [log_accuracy, knn_accuracy, lsvm_accuracy, rsvm_accuracy, \n                       nb_accuracy, dt_accuracy, rf_accuracy, xgb_accuracy]\n})\n\nmodels.sort_values(by = 'Accuracy Score', ascending = False, ignore_index=True)","649137f1":"def perform_kfold(clf, X_train=X_train, y_train=y_train):\n    \"\"\"Performs k-fold cross validation on given data(X_train, y_train) using \n    the `clf` (aka classifier)\n    \n    Parameters:\n    classifer : classifier obj implementing 'fit' method.\n    X_train (array-like): 2D-array of input features of Training Set.\n    y_train (array-like): list of target features of Training Set.\n    \n    Returns:\n    mean_score (float): Mean of Accuracy scores after operation.\n    std_score  (float): Standard Deviation of Accuracy scores.\n    \"\"\"\n    scores = cross_val_score(estimator=clf, X=X_train, y=y_train, \n                            cv=10\n                            )\n    \n    mean_score = scores.mean()\n    std_score = scores.std()\n    print(f\"Mean Accuracy: {mean_score*100:0.3f} %\")\n    print(f\"Standard Deviation: {std_score*100:0.3f} %\")\n    \n    return mean_score, std_score","d342c8fb":"log_macc, log_std = perform_kfold(log)","11cd3c76":"knn_macc, knn_std = perform_kfold(knn)","1538b433":"lsvm_macc, lsvm_std = perform_kfold(linear_svm)","7e0eb49b":"rsvm_macc, rsvm_std = perform_kfold(radial_svm)","1d20cc75":"nb_macc, nb_std = perform_kfold(nb)","23f1638d":"dt_macc, dt_std = perform_kfold(dt)","c33e37e5":"rf_macc, rf_std = perform_kfold(rf)","b73b927a":"xgb_macc, xgb_std = perform_kfold(xgb)","318a7715":"cross_validated_models = pd.DataFrame({\n    \"Model\": [\"Logistic Regression\", \"KNN\", \"SVM-Linear\", \"SVM-RBF\", \n             \"Naive Bayes\", \"Decision Tree\", \"Random Forest\", \"XGBoost\"],\n    \"Mean Accuracy Score\": [log_macc, knn_macc, lsvm_macc, rsvm_macc, \n                       nb_macc, dt_macc, rf_macc, xgb_macc],\n    \"Standard Deviation\": [log_std, knn_std, lsvm_std, rsvm_std, \n                       nb_std, dt_std, rf_std, xgb_std]\n})\n\ncross_validated_models.sort_values(by = 'Mean Accuracy Score', ascending = False,\n                     ignore_index=True)","157ed782":"## Model Building","5ed5d883":"### Random Forest","30d393f3":"* **We can see there are lot of null values in SkinThickness and Insulin column.**","b726c7a7":"#### Splitting the Dataset into Training and Test Set","ff1ff2ca":"#### Radial-Support Vector Machine(SVM)","6012d066":"#### Comparing Models after K-fold cross validation","3ed97ce8":"## Data Cleaning","ee92e781":"**Now we can see,mean of Insulin and SkinThickness has increased.**","3778d09e":"**We have 768 observations and 9 features.**","fa61b993":"#### Load the Data","8ff1aef2":"### Naive Bayes","97ae6bb5":"**As we can see that there is no missing values.**","fc493bb0":"### Decision Tree","44e46074":"#### Decision Tree","17ec399f":"#### Linear-Support Vector Machine(SVM)","a699b24d":"* **So, it turns out LogisticRegression is the winners after K-fold Crossvalidation.**","a4451059":"## EDA","45c1d411":"* **It seems like Logistic Regression and XGBoost performs best!**\n* **But before we jump into any conclusions let's perform K-fold cross validation.**","c5495b27":"### Model Comparison","052fd3ad":"* **Pregnancies,Glucose,BMI,Age have positive correlation with Outcome(target variable).**\n* **SkinThickness and Insulin having correlation with each other.**\n* **Age and Pregnancies are correlated.**","177e4619":"## Data PreProcessing","a29bc624":"#### Feature Scaling","4938bab0":"**Important Observation(s):**<br>\n* It seems like null values are present in the form of zeros because almost all the features have minimun value 0.<br>\n* It's not possible to have Glucose, BloodPressure, SkinThickness, Insulin, BMI to be zero. So, we have to handle this.<br>\n    **Let's check, how many zeroes are present in each feature**.","d39d9673":"#### XGBoost","692e50bf":"#### Logistic Regression","1017f782":"#### Separating dependent and independent features","e2d0c4e1":"### KNN","1a605c52":"#### Naive Bayes","a1226eef":"### Logistic Regression","54d3ce2f":"### K-Fold Cross Validation","daae1dcb":"### Radial-Support Vector Machine(SVM)","a7f6b75d":"### Linear-Support Vector Machine(SVM)","4eb335b6":"## Importing Necessary Libraries\n","95e84751":"#### KNN","529d8835":"#### Helper Function","bf9b4102":"### XGBoost","22cb975a":"#### Helper Function","c03f33b0":"#### Random Forest"}}