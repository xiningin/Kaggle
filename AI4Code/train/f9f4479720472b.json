{"cell_type":{"c6fc6b47":"code","a6ffcfc2":"code","73cdcc7f":"code","2b5fe20f":"code","4125a92c":"code","059ebdb3":"code","b5077b6a":"code","5ff7b9b8":"code","b194621c":"code","2447e706":"code","8ac5a7bf":"code","f17882a7":"code","0ec5225c":"code","fadfa14c":"code","b2393f3c":"code","3a00602a":"code","12967e8d":"code","ec31c6a0":"code","0887cb62":"code","c6634bf0":"code","9152f3e6":"code","2b74714a":"code","729a1481":"code","6993ea5f":"code","1e8b7312":"code","3aec7d57":"code","7db1ae44":"code","7f7c4aa6":"code","f4f89724":"code","531d0638":"code","ec4463c7":"code","9b5bf02e":"code","890b9f0c":"code","37a41d02":"code","af42714c":"code","cc8da500":"code","d37cf335":"code","6dfb015a":"code","fb16fe0c":"code","7a67b459":"code","c158b851":"code","bc64fd66":"code","d40b6f8f":"code","4da3d15f":"code","22967fbd":"code","cb123c5a":"code","bf521ea8":"code","e6c1bbe2":"code","ac0a67cc":"code","de3548c9":"code","2757decf":"code","81361ff6":"code","973806dd":"code","225cff6b":"code","eefdc5ea":"code","35ccc1aa":"code","a98e48e9":"markdown","74ecfba7":"markdown","08e3bce6":"markdown","1b7c318e":"markdown","4255dd84":"markdown","3c7f47ed":"markdown","0d41545d":"markdown","a75d0ead":"markdown","64756041":"markdown","2fa2ce8c":"markdown","bb2c2db9":"markdown","3715542c":"markdown","6b26258f":"markdown","b3ce86ef":"markdown","0967e6e6":"markdown","5fc2caa3":"markdown","b9a829ed":"markdown","ede143cd":"markdown","0a9dc519":"markdown","9bda2b2e":"markdown","2d2f82b4":"markdown","885372c5":"markdown","93ffa6f7":"markdown","a7e36ac2":"markdown","7c2840e8":"markdown","5a0c70b3":"markdown","b5bc06e3":"markdown","52882350":"markdown","72f32d8a":"markdown","1a42b416":"markdown","4e9c3c38":"markdown","a0c05d0f":"markdown","47c46c4a":"markdown"},"source":{"c6fc6b47":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport holoviews as hv\nfrom holoviews import opts\nhv.extension('bokeh')\nimport altair as alt\nimport seaborn as sns\nfrom matplotlib import pyplot as plt\n\n\nfrom fuzzywuzzy import fuzz \nfrom fuzzywuzzy import process \n\n#Apriori libraries \nfrom mlxtend.frequent_patterns import apriori\nfrom mlxtend.frequent_patterns import association_rules\n\nimport nltk\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","a6ffcfc2":"df = pd.read_csv(\"..\/input\/gufhtugu-publications-dataset-challenge\/GP Orders - 5.csv\", delimiter=',')\ndf.head()","73cdcc7f":"df.shape","2b5fe20f":"df.describe()","4125a92c":"df.info()","059ebdb3":"df = df.rename(columns={'Order Number': 'order_number',\"Order Status\":\"order_status\", \"Book Name\":\"book_name\",\"Order Date & Time\":\"order_date\",\"City\":\"billing_city\", \"Payment Method\":\"payment_method\", \"Total items\":\"total_books\", \"Total weight (grams)\":\"grams\"})\ndf.head()","b5077b6a":"df.isnull().sum().sort_values(ascending = False)","5ff7b9b8":"df[df['book_name'].isna()]","b194621c":"df[df['billing_city'].isna()]","2447e706":"df.dropna(inplace=True)","8ac5a7bf":"df.isnull().sum()","f17882a7":"df.order_status.unique()","0ec5225c":"df.order_status.value_counts()","fadfa14c":"df.order_status.hist()","b2393f3c":"split_col = df['book_name'].str.split('\/', expand=True).stack()\n\n# Melting dataframe so that we have one book in each row\nsplit_col.index = split_col.index.droplevel(-1) # to line up with df's index\nsplit_col.name = 'book_name' # needs a name to join\n\ndf = df.drop(columns='book_name').join(split_col)\ndf.head(10)","3a00602a":"df['book_name'] = df['book_name'].str.lower()\ndf['billing_city'] = df['billing_city'].str.lower()","12967e8d":"#pd.set_option('display.max_rows', df.shape[0]+1)\ndf['billing_city'].head(10)","ec31c6a0":"df['billing_city'].nunique()","0887cb62":"# df['billing_city'] = df['billing_city'].replace({'KHI':'KARACHI', 'KHI':'KARACHI'})\n# df['billing_city'] = df['billing_city'].replace({'GAGOO MANDI TEHSIL BUREWALA DISTRICT VEHARI':'VEHARI'})\n# df['billing_city'] = df['billing_city'].replace({'ZILA TOBA TAK SING TAHSEEL GOJJRA':'GOJRA'})\n# df['billing_city'] = df['billing_city'].replace({'WAH CANTT, TAXILA, RAWALPINDI':'RAWALPINDI'})\n# df['billing_city'] = df['billing_city'].replace({'MIDEL TOWN LAHORE':'LAHORE'})\n# df['billing_city'] = df['billing_city'].replace({'TEHSIL BARNALA, DUST. BHIMBER':'BHIMBER'})\n# df['billing_city'] = df['billing_city'].replace({'ADDA ZAKHEERA(DUNYA PUR)':'DUNYAPUR'})\n# df['billing_city'] = df['billing_city'].replace({'JAUHARABAD , DISTT KHUSHAB':'KHUSHAB'})\n# df['billing_city'] = df['billing_city'].replace({'ADIYALA ROAD, RAWALPINDI':'RAWALPINDI'})\n# df['billing_city'] = df['billing_city'].replace({'UPPER CHITRAL':'CHITRAL'})\n# df['billing_city'] = df['billing_city'].replace({'MUSTUFA TOWN, WAHDAT ROAD, LAHORE.':'LAHORE'})\n# df['billing_city'] = df['billing_city'].replace({'AHMED PUR SIAL JHANG':'JHANG'})\n# df['billing_city'] = df['billing_city'].replace({'KALLAR SYEDAN DIST RAWALPINDI':'RAWALPINDI'})\n# df['billing_city'] = df['billing_city'].replace({'BAHWALNAGAR':'BAHAWALNAGAR'})\n# df['billing_city'] = df['billing_city'].replace({'LAHORE, PAKISTAN':'LAHORE'})\n# df['billing_city'] = df['billing_city'].replace({'MADINA TOWN, FAISALABAD':'FAISALABAD'})\n# df['billing_city'] = df['billing_city'].replace({'ALI RAZA ABAD 5KM RAIWIND ROAD LAHORE':'LAHORE'})\n# df['billing_city'] = df['billing_city'].replace({'MUZAFFARABAD AZAD KASHMIR':'MUZAFFARABAD'})\n# df['billing_city'] = df['billing_city'].replace({'TALUKA HALA DISTRICT MATIARI':'MATIARI'})\n# df['billing_city'] = df['billing_city'].replace({'AGRICS TOWN, RAIWAND ROAD LAHORE':'LAHORE'})\n# df['billing_city'] = df['billing_city'].replace({'LYARI, KARACHI':'KARACHI'})\n# df['billing_city'] = df['billing_city'].replace({'NORTH NAZIMABAD KARACHI':'KARACHI'})\n# df['billing_city'] = df['billing_city'].replace({'SAADI TOWN\/KARACHI':'KARACHI'})\n# df['billing_city'] = df['billing_city'].replace({'KORANGI, KARACHI':'KARACHI'})\n# df['billing_city'] = df['billing_city'].replace({'RAWALPINDI CANT':'RAWALPINDI'})\n# df['billing_city'] = df['billing_city'].replace({'GULSHAN E IQBAL BLOCK 10 A, KARACHI':'KARACHI'})\n# df['billing_city'] = df['billing_city'].replace({'MALIR, KARACHI':'KARACHI'})\n# df['billing_city'] = df['billing_city'].replace({'SABZAZAR, MULTAN ROAD,LAHORE':'LAHORE'})\n# df['billing_city'] = df['billing_city'].replace({'TOWNSHIP LAHORE':'LAHORE'})\n# df['billing_city'] = df['billing_city'].replace({'CHAKLALA RAWALPINDI':'RAWALPINDI'})\n# df['billing_city'] = df['billing_city'].replace({'TEHSIL PHALIA DISTRICT MANDI BHUDDIN':'MANDI BAHAUDDIN'})\n# df['billing_city'] = df['billing_city'].replace({'VILLAGE SHUKHDARA TEHSIL MATTA DISTRICT SWAT':'SWAT'})\n# df['billing_city'] = df['billing_city'].replace({'DHA,EME SOCIETY MULTAN ROAD LAHORE PAKISTAN.':'LAHORE'})\n# df['billing_city'] = df['billing_city'].replace({'PAHAR PUR D I KHAN':'D I KHAN'})\n# df['billing_city'] = df['billing_city'].replace({'MANDI BAHAHUDIN':'MANDI BAHAUDDIN'})\n# df['billing_city'] = df['billing_city'].replace({'FAZAL COLONY NEAR EMCO FACTORY SHEIKHUPURA ROA':'SHEIKHUPURA'})\n# df['billing_city'] = df['billing_city'].replace({'SADIQ ABAD DISTRICT RAHIM YAR KHAN':'RAHIM YAR KHAN'})\n# df['billing_city'] = df['billing_city'].replace({'CHOUPERHATTA KABIRWALA KHANEWAL':'KHANEWAL'})\n# df['billing_city'] = df['billing_city'].replace({'ACADEMY TOWN, PESHAWAR':'PESHAWAR'})\n# df['billing_city'] = df['billing_city'].replace({'LATIFABAD, HYDERABAD':'HYDERABAD'})\n# df['billing_city'] = df['billing_city'].replace({'G-8\/1, ISLAMABAD':'ISLAMABAD'})\n# df['billing_city'] = df['billing_city'].replace({'ORANGI TOWN KARACHI':'KARACHI'})\n# df['billing_city'] = df['billing_city'].replace({'CHASHMA, KUNDIAN, DISTRICT, MIANWALI':'MIANWALI'})\n# df['billing_city'] = df['billing_city'].replace({'MALAKWAL DISTRICT MANDI BAHAUDDIN':'MANDI BAHAUDDIN'})\n# df['billing_city'] = df['billing_city'].replace({'GULISTAN E JAUHAR KARACHI':'KARACHI'})\n# df['billing_city'] = df['billing_city'].replace({'JABOKA OKARA':'OKARA'})\n# df['billing_city'] = df['billing_city'].replace({'GHOURI TOWN, ISLAMABAD':'ISLAMABAD'})\n# df['billing_city'] = df['billing_city'].replace({'GULZAR E HIJRI':'KARACHI'})\n# df['billing_city'] = df['billing_city'].replace({'GULBERG TOWN KARACHI CENTRAL':'KARACHI'})\n# df['billing_city'] = df['billing_city'].replace({'KAHNA NAU LAHORE':'LAHORE'})\n# df['billing_city'] = df['billing_city'].replace({'FAZAL COLONY NEAR EMCO FACTORY SHEIKHUPURA ROAD LAHORE':'LAHORE'})\n# df['billing_city'] = df['billing_city'].replace({'ISLAAM ABAD':'ISLAMABAD'})\n# df['billing_city'] = df['billing_city'].replace({'TEHSIL ALLAI DISST BATTAGRAM':'BATTAGRAM'})\n# df['billing_city'] = df['billing_city'].replace({'SWABI PESHAWAR':'SWABI'})\n# df['billing_city'] = df['billing_city'].replace({'FEROZ PUR ROAD LAHORE':'LAHORE'})\n# df['billing_city'] = df['billing_city'].replace({'SAKRAND DIST:NAWABSHAH':'NAWABSHAH'})\n# df['billing_city'] = df['billing_city'].replace({'SARGODHA SILANWALI ROAD HAMEED TOWN':'SARGODHA'})\n# df['billing_city'] = df['billing_city'].replace({'BAGH AZAD KASHMIR':'BAGH'})\n# df['billing_city'] = df['billing_city'].replace({'KAMEER DARBAR HAZRAT BABA MUHAMMAD PANHA DISTRICT SAHIWAL':'SAHIWAL'})\n# df['billing_city'] = df['billing_city'].replace({'RAWAL PINDI':'RAWALPINDI'})\n# df['billing_city'] = df['billing_city'].replace({'FAISAL ABAD':'FAISALABAD'})\n# df['billing_city'] = df['billing_city'].replace({'SIALKOT PASRUR':'SIALKOT'})\n# df['billing_city'] = df['billing_city'].replace({'PESHAWAR GULBAHAR NO.1':'PESHAWAR'})\n# df['billing_city'] = df['billing_city'].replace({'TAXILA, RAWALPINDI':'RAWALPINDI'})\n# df['billing_city'] = df['billing_city'].replace({'NAWAB SHAH':'NAWABSHAH'})\n# df['billing_city'] = df['billing_city'].replace({'CHAKDARA LOWER DIR':'DIR'})\n# df['billing_city'] = df['billing_city'].replace({'LALIAN DISTRICT CHINIOT':'CHINIOT'})\n# df['billing_city'] = df['billing_city'].replace({'BAHWALPUR':'BAHAWALPUR'})\n# df['billing_city'] = df['billing_city'].replace({'DASKA, SIALKOT':'SIALKOT'})\n# df['billing_city'] = df['billing_city'].replace({'UPPER DIR':'DIR'})\n# df['billing_city'] = df['billing_city'].replace({'MAKHAI TIMERGATA':'TIMERGATA'})\n# df['billing_city'] = df['billing_city'].replace({'HAZARA TOWN QUETTA':'QUETTA'})\n# df['billing_city'] = df['billing_city'].replace({'QILA DIDAR SINGH, GUJRANWALA':'GUJRANWALA'})\n# df['billing_city'] = df['billing_city'].replace({'F-17\/2 ISLAMABAD':'ISLAMABAD'})\n# df['billing_city'] = df['billing_city'].replace({'ALKHIDMAT RAAZI HOSPITAL CBR TOWN':'ISLAMABAD'})\n# df['billing_city'] = df['billing_city'].replace({'ZAFARWAL ZILLA NAROWAL':'NAROWAL'})\n# df['billing_city'] = df['billing_city'].replace({'DINGA, TEHSIL KHARIAN,DISTRICT GUJRAT.':'GUJRAT'})\n# df['billing_city'] = df['billing_city'].replace({'BALDIA TOWN':'KARACHI'})\n# df['billing_city'] = df['billing_city'].replace({'TEH: DASKA\/ DISTT: SIALKOT':'SIALKOT'})\n# df['billing_city'] = df['billing_city'].replace({'MUZAFFARGARH PAKISTAN':'MUZAFFARGARH'})\n# df['billing_city'] = df['billing_city'].replace({'JOHAR TOWN, LAHORE':'LAHORE'})\n# df['billing_city'] = df['billing_city'].replace({'B?GH AZAD KASHMIR':'BAGH'})\n# df['billing_city'] = df['billing_city'].replace({'ARIFWALA DISTRICT PAKPATTAN':'ARIFWALA'})\n# df['billing_city'] = df['billing_city'].replace({'LYYAH':'LAYYAH'})\n# df['billing_city'] = df['billing_city'].replace({'M. B DIN':'MANDI BAHAUDDIN'})\n# df['billing_city'] = df['billing_city'].replace({'MULTAN\/LODHRAN':'MULTAN'})\n# df['billing_city'] = df['billing_city'].replace({'MIRPUR AZAD KASHMIR':'MIRPUR'})\n# df['billing_city'] = df['billing_city'].replace({'FATEH PUR DISTRICT LAYYAH':'LAYYAH'})\n# df['billing_city'] = df['billing_city'].replace({'PESHAWAR RAHATABAD':'PESHAWAR'})\n# df['billing_city'] = df['billing_city'].replace({'USTA MUHAMMAD DISTRICT JAFFERABAD':'USTA MUHAMMAD'})\n# df['billing_city'] = df['billing_city'].replace({'BERON YAKATOOT ZARGAR ABAD PESHAWAR CITY':'PESHAWAR'})\n# df['billing_city'] = df['billing_city'].replace({'KOTLA JAM DISTRICT BHAKKAR':'BHAKKAR'})\n# df['billing_city'] = df['billing_city'].replace({'USMAN WALA DISTRICT KASUR PUNJAB PAKISTAN':'KASUR'})\n# df['billing_city'] = df['billing_city'].replace({'BALAKOT DISTRICT MANSEHRA':'MANSEHRA'})\n# df['billing_city'] = df['billing_city'].replace({'AHMAD PUR SIAL\/JHANG':'JHANG'})\n# df['billing_city'] = df['billing_city'].replace({'KARACHI\/MALIR\/QUAIDABAD':'KARACHI'})\n# df['billing_city'] = df['billing_city'].replace({'USLAMABAF':'ISLAMABAD'})\n# df['billing_city'] = df['billing_city'].replace({'BHIMBER AZAD KASHMIR':'BHIMBER'})\n# df['billing_city'] = df['billing_city'].replace({'SITA ROAD RAHMANI NAGHAR DISTRICT DADU':'DADU'})\n# df['billing_city'] = df['billing_city'].replace({'MANGAT MANDI BAHA UD DIN':'MANDI BAHAUDDIN'})\n# df['billing_city'] = df['billing_city'].replace({'TEH:DASKA\/ DISTT: SIALKOT':'SIALKOT'})\n# df['billing_city'] = df['billing_city'].replace({'PINDI GHEB DISTRICT ATTOCK':'ATTOCK'})","c6634bf0":"# @arindam et al.\n\n# Source: https:\/\/simplemaps.com\/data\/pk-cities\n\npakistan_top_cities = ['karachi', 'lahore', 'sialkot', 'faisalabad', 'rawalpindi',\n       'peshawar', 'saidu sharif', 'multan', 'gujranwala', 'islamabad',\n       'quetta', 'bahawalpur', 'sargodha', 'new mirpur', 'chiniot',\n       'sukkur', 'larkana', 'shekhupura', 'jhang', 'rahimyar khan',\n       'gujrat', 'kasur', 'mardan', 'mingaora', 'dera ghazi khan',\"dgk\"\n       'nawabshah', 'sahiwal', 'mirpur khas', 'okara', 'burewala',\n       'jacobabad', 'saddiqabad', 'kohat', 'muridke', 'muzaffargarh',\n       'khanpur', 'gojra', 'bahauddin', 'abbottabad', 'dadu',\n       'khuzdar', 'pakpattan', 'tando allahyar', 'vihari', 'jaranwala',\n       'kamalia', 'kot addu', 'nowshera', 'swabi', 'dera ismail khan',\n       'chaman', 'charsadda', 'kandhkot', 'hasilpur', 'muzaffarabad',\n       'mianwali', 'jalalpur\",\"jattan', 'bhakkar', 'zhob', 'kharian',\n       'mian channun', 'jamshoro', 'pattoki', 'harunabad',\n       'toba tek singh', 'shakargarh', 'hujra\", \"shah\", \"muqim', 'kabirwala',\n       'mansehra', 'lala musa', 'nankana sahib', 'bannu', 'timargara',\n       'parachinar', 'gwadar', 'abdul hakim', 'hassan\", \"abdal', 'tank',\n       'hangu', 'risalpur cantonment', 'karak', 'kundian', 'umarkot',\n       'chitral', 'dainyor', 'kulachi', 'kotli', 'gilgit',\n       'hyderabad', 'narowal', 'khairpur', \"mir\u2019s\", 'khanewal', 'jhelum',\n       'haripur', 'shikarpur', 'rawala kot', 'hafizabad', 'lodhran',\n       'malakand', 'attock', 'batgram', 'matiari', 'ghotki',\n       'firoz','naushahro', 'alpurai', 'bagh', 'daggar', 'bahawalnagar',\n       'leiah', 'tando muhammad khan', 'chakwal', 'khushab', 'badin',\n       'lakki', 'rajanpur', 'dera allahyar', 'shahdad kot', 'pishin',\n       'sanghar', 'upper dir', 'thatta', 'dera murad jamali', 'kohlu',\n       'mastung', 'dasu', 'athmuqam', 'loralai', 'barkhan',\n       'musa khel bazar', 'ziarat', 'gandava', 'sibi', 'dera bugti',\n       'eidgah', 'turbat', 'uthal', 'chilas', 'kalat', 'panjgur', 'gakuch',\n       'qila', 'saifullah', 'kharan', 'aliabad', 'awaran', 'dalbandin']\n\n","9152f3e6":"single_word_cities = df[df[\"billing_city\"].str.split().apply(len) == 2][\"billing_city\"].unique()\nsingle_word_cities[:20]","2b74714a":"def clean_city(row):\n    address = row.billing_city.split()\n    add = set()\n    for a in address:\n        a = a.strip()\n        if a:\n            add.add(a)\n    for city in pakistan_top_cities:\n        if row.billing_city.__contains__(city):\n            return city\n        \n    for a in add:\n        for c in pakistan_top_cities:\n            if nltk.edit_distance(a, c) <= 5: # considering spelling mistakes upto 5 letters\n                return c\n    return row.billing_city","729a1481":"# Number of cities before cleaning\ndf[\"billing_city\"].nunique()","6993ea5f":"df[\"billing_city\"] = df.apply(clean_city, axis=1)","1e8b7312":"df['billing_city'].head()","3aec7d57":"df['billing_city'].nunique()","7db1ae44":"df.head()","7f7c4aa6":"# df['order_date']= pd.to_datetime(df['order_date'])\n \n# #Extracting year,month and day\n# df['year'] = df['order_date'].apply(lambda x : x.year)\n# df['month'] = df['order_date'].apply(lambda x : x.month_name)\n# df['day'] = df['order_date'].apply(lambda x : x.day)\n# df['weekday'] = df['order_date'].apply(lambda x : x.weekday())\n\n# #Rearranging the columns\n# df_new=df[['order_number', 'order_status', 'book_name', 'order_date', 'billing_city', 'year', 'month', 'day','weekday']]\n# df_new.head()","f4f89724":"\ndf['order_date']= pd.to_datetime(df['order_date'])\n \n#Extracting year,month and day\ndf['year'] = df['order_date'].apply(lambda x : x.year)\ndf['month'] = df['order_date'].apply(lambda x : x.month)\ndf['day'] = df['order_date'].apply(lambda x : x.day_name())\ndf['weekday'] = df['order_date'].apply(lambda x : x.weekday())\n\n#Rearranging the columns\ndf_new=df[['order_number', 'order_status', 'book_name', 'order_date', 'billing_city', 'year', 'month', 'day','weekday']]\ndf_new.head()","531d0638":"df_new.year.value_counts()","ec4463c7":"# \ndaily_sales = df.groupby([\"day\"])[\"book_name\"].agg([\"count\"]).reset_index()\ndaily_sales.sort_values(\"day\",ascending = True)","9b5bf02e":"plt.figure(figsize = (12,8))\nsns.barplot(x =daily_sales[\"day\"], y =daily_sales[\"count\"],color = \"Blue\",label = \"count\")\nplt.xlabel(\"Days\")\nplt.ylabel(\"item\")\nplt.title(\"Sales by Days\")\nplt.xticks(rotation = 60)\nplt.legend()\nplt.show()","890b9f0c":"# \nmonth_sales = df.groupby([\"month\"])[\"book_name\"].agg([\"count\"]).reset_index()\nmonth_sales.sort_values(\"month\",ascending = True)","37a41d02":"plt.figure(figsize = (12,8))\nsns.barplot(x =month_sales[\"month\"], y =month_sales[\"count\"],color = \"Blue\",label = \"count\")\nplt.xlabel(\"month\")\nplt.ylabel(\"Total Number of Books Buy\")\nplt.title(\"Sales by Month\")\nplt.xticks(rotation = 60)\nplt.legend()\nplt.show()","af42714c":"import holoviews as hv\nfrom holoviews import opts\nhv.extension('bokeh')\n\ndf1=df_new.groupby(['year']).filter(lambda x: (x['year'] == 2020).any())\ndf2=df_new.groupby(['year']).filter(lambda x: (x['year'] == 2021).any())\ndf3=df_new.groupby(['year']).filter(lambda x: (x['year'] == 2019).any())\n\n#Plotting monthly data of number of quantity purchased in 2020 and 2021 \nsales_2019=hv.Bars(df3.groupby(['month'])['book_name'].count()).opts(ylabel=\"# of items\", title='# of items sold in 2019')\nsales_2020=hv.Bars(df1.groupby(['month'])['book_name'].count()).opts(ylabel=\"# of items\", title='# of items sold in 2020')\nsales_2021=hv.Bars(df2.groupby(['month'])['book_name'].count()).opts(ylabel=\"# of items\", title='# of items sold in 2021')\n\n#Merging both plots\n(sales_2019 + sales_2020 + sales_2021).opts(opts.Bars(width=380, height=300,tools=['hover'],show_grid=True))","cc8da500":"import altair as alt\n\n#Converting weekday variable to category\ntemp=df_new.copy()\ntemp['qty_purchased']=df_new['order_number'].map(df_new['order_number'].value_counts())\n\n#Slicing first 5000 rows as altair library can't plot any data which has record beyond that\ntemp1=temp[:5000]\ntemp1.columns\ntemp1.weekday = temp1.weekday.astype('category') \n\n#Creating a new dataframe which has the frequency of weekdays\nweekday_bin=temp1['weekday'].value_counts().to_frame().reset_index().rename(columns={'index':'weekday','weekday':'count'})\n\n#Plotting bar chart\nbars = alt.Chart(weekday_bin).mark_bar(color=\"darkorange\").encode(\n    x='weekday',\n    y=alt.Y(\"count\",title='Number of purchases')\n)\n\n#Adding data labels\ntext = bars.mark_text(\n    align='center',\n    baseline='middle',\n    dy=-7 ,\n    size=15,\n).encode(\n    text='count',\n    tooltip=[alt.Tooltip('weekday'),\n            alt.Tooltip('count')]\n)\n\n#Combining both\n(bars + text).properties(\n    width=800,\n    height=400,\n    title=\"Number of quantity purchases across weekdays\"\n)","d37cf335":"import seaborn as sns\n\n#Setting plot style\nplt.figure(figsize = (15, 8))\nplt.style.use('seaborn-white')\n\n#Top 10 fast moving products\nplt.subplot(1,2,1)\nax=sns.countplot(y=\"book_name\", hue=\"year\", data=df_new, palette=\"pastel\",\n              order=df_new.book_name.value_counts().iloc[:10].index)\n\nax.set_xticklabels(ax.get_xticklabels(),fontsize=11,rotation=40, ha=\"right\")\nax.set_title('Top 10 book sales',fontsize= 22)\nax.set_xlabel('Total # of items purchased',fontsize = 20) \nax.set_ylabel('Top 10 items', fontsize = 20)\nplt.tight_layout()\n\n","6dfb015a":"#Bottom 10 fast moving products\nplt.subplot(1,2,2)\nax=sns.countplot(y=\"book_name\", hue=\"year\", data=df_new, palette=\"pastel\",\n              order=df_new.book_name.value_counts().iloc[-10:].index)\nax.set_xticklabels(ax.get_xticklabels(),fontsize=11,rotation=40, ha=\"right\")\nax.set_title('10 Books sale by bottom',fontsize= 22)\nax.set_xlabel('Total # of books purchased',fontsize = 20) \nax.set_ylabel('Bottom 10 book', fontsize = 20)\nplt.tight_layout()","fb16fe0c":"#df_new.assign(Book=df_new.book_name.str.split(\"\/\")).explode('book_name')","7a67b459":"from wordcloud import WordCloud,STOPWORDS\n\n#Wordcloud\nwordcloud = WordCloud(\n    width = 3000,\n    height = 2000,\n    background_color = 'white').generate(\"\".join(str(df_new['book_name'])))\nfig = plt.figure(\n    figsize = (50, 30),\n    facecolor = 'k',\n    edgecolor = 'k')\n\n#Display plot\nplt.imshow(wordcloud, interpolation = 'bilinear')\nplt.axis('off')\nplt.tight_layout(pad=0)\nplt.show()","c158b851":"#df_new.assign(book_name_seperate=df_new.book_name.str.split(\"\/\")).explode('book_name')","bc64fd66":"city_sales1 = df_new['billing_city'].value_counts()[:10].index.tolist()\ncity_sales2 = df_new['billing_city'].value_counts().unique()\ncity_sales = list(zip(city_sales1, city_sales2)) \ncity_sales = pd.DataFrame(city_sales, \n                  columns = ['billig_city', 'counts']) \ncity_sales","d40b6f8f":"plt.figure(figsize = (12,8))\nsns.barplot(x =city_sales[\"billig_city\"], y =city_sales[\"counts\"],color = \"Blue\",label = \"count\")\nplt.xlabel(\"City\")\nplt.ylabel(\"Total numbers of sales\")\nplt.title(\"Sales by City\")\nplt.xticks(rotation = 60)\nplt.legend()\nplt.show()\n","4da3d15f":"# import plotly.express as px\n\n# fig = px.sunburst(df_new, path=['year', 'month', 'day', 'book_name'],title=\"Dont Forget to Click Chart to Examine Deeply \")\n# fig.show()","22967fbd":"# targets = list(dict(df_new['book_name'].value_counts()).keys())\n# values = list(dict(df_new['book_name'].value_counts()).values())\n\n# fig = px.pie(\n#     values=values, \n#     names=targets,\n#     title='Book Name',\n#     color_discrete_sequence=['darkcyan', 'lawngreen']\n# )\n# fig.show()","cb123c5a":"# import squarify\n\n# plt.figure(figsize = (50, 30))\n# squarify.plot(sizes = df_new.book_name.value_counts().values, alpha = 0.8,\n#               label = df_new.book_name.unique(), text_kwargs={'fontsize':18})\n# plt.title('Book Name', fontsize = 30)\n# plt.axis('off')\n# plt.show()","bf521ea8":"hot_encoded_df = df_new.groupby(['order_number', 'book_name'])['book_name'].count().unstack().reset_index().fillna(0).set_index('order_number')\n","e6c1bbe2":"hot_encoded_df.head()","ac0a67cc":"def encode_units(x):\n    if x <= 0:\n        return 0\n    if x >= 1:\n        return 1\nhot_encoded_df = hot_encoded_df.applymap(encode_units)","de3548c9":"frequent_itemsets = apriori(hot_encoded_df, min_support=0.01, use_colnames=True)","2757decf":"rules = association_rules(frequent_itemsets, metric='lift', min_threshold=1)\nrules.head(10)","81361ff6":"rules[ (rules['lift'] >= 1) & (rules['confidence'] >= 0.5)]","973806dd":"support = rules['support'] #rules.to_numpy(columns=['support']) #as_matrix\nconfidence = rules['confidence'] #rules.to_numpy(columns=['confidence']) \nimport seaborn as sns\n\nfor i in range (len(support)):\n    support[i] = support[i]\n    confidence[i] = confidence[i]\n    \nplt.title('Assonciation Rules')\nplt.xlabel('support')\nplt.ylabel('confidance')\nsns.regplot(x=support, y=confidence, fit_reg=False)","225cff6b":"df_new.head()","eefdc5ea":"df_new.shape","35ccc1aa":"df_new.describe()","a98e48e9":"# Top 10 book sales","74ecfba7":"**Check NaN in book_name**","08e3bce6":"Now, We have no NaN values.","1b7c318e":"# 10 Books sale by bottom","4255dd84":"# Check missing value in dataset","3c7f47ed":"# Top 10 City","0d41545d":"# Week Days Sale","a75d0ead":"# Apriori Algorithm","64756041":"**We only want to see the rules where confidence is greater than or equal to 50% so:**","2fa2ce8c":"# Convert the 'order_date' column to datetime format","bb2c2db9":"**Try to clean the cities column**","3715542c":"# Sales by month and year","6b26258f":"**Make book_name and billing_city to lowercase**","b3ce86ef":"# find out how much bought is total on which month","0967e6e6":"# Data Cleaning","5fc2caa3":"**We have 3 order status**","b9a829ed":"**Now, we need to run apriori algorithm to get insight that if a customer buys one item which item he\/she buys next.**","ede143cd":"# What's Next?","0a9dc519":"# Which words used mostly ","9bda2b2e":"**We have 3518  total cities**","2d2f82b4":"# find out how much bought is total on which day","885372c5":"For instance from the last rule we can see that (\u0645\u0634\u06cc\u0646 \u0644\u0631\u0646\u0646\u06af, ARTIFICIAL INTELLIGENCE) and (\u0688\u06cc\u0679\u0627 \u0633\u0627\u0626\u0646\u0633) are commonly bought together. This makes sense since people who purchase (\u0645\u0634\u06cc\u0646 \u0644\u0631\u0646\u0646\u06af, ARTIFICIAL INTELLIGENCE) would like to have (\u0688\u06cc\u0679\u0627 \u0633\u0627\u0626\u0646\u0633) with it.\n\nThe support value for the this rule is 0.016109. This number is calculated by dividing the number of transactions containing toast divided by total number of transactions. The confidence level for the rule is 0.04995 which shows that out of all the transactions that contain toast , 76.69% of the transactions also contain (\u0688\u06cc\u0679\u0627 \u0633\u0627\u0626\u0646\u0633). Finally, the lift of 15.34 tells us that (\u0645\u0634\u06cc\u0646 \u0644\u0631\u0646\u0646\u06af, ARTIFICIAL INTELLIGENCE) is 15.34 times more likely to be bought by the customers who buy (\u0645\u0634\u06cc\u0646 \u0644\u0631\u0646\u0646\u06af, ARTIFICIAL INTELLIGENCE) compared to the default likelihood of the sale of (\u0688\u06cc\u0679\u0627 \u0633\u0627\u0626\u0646\u0633).\n","93ffa6f7":"# Import Dataset and check the rows and columns","a7e36ac2":"* Support is an indication of how frequently the itemset appears in the dataset.\n* Confidence is an indication of how often the rule has been found to be true.\n","7c2840e8":"**Split the book_name column with '\/'and make a new row**","5a0c70b3":"# Seperate Book with \"\/\"","b5bc06e3":"**Check NaN in billing_city**","52882350":"Delete all NaN from dataframe","72f32d8a":"* Try to clean city column with fuzzywuzzy\n* Predict next hour order\n* Predict next day order","1a42b416":"**Above lineAbove line of code is transfrom data to make items as columns and each transaction as a row and count same Items bought in one transaction but fill other cloumns of the row with 0 to represent item which are not bought.**","4e9c3c38":"# Convert the 'day' column to day_name","a0c05d0f":"# Shape of dataset <br\/>\nWe have 19187 rows and 5 columns","47c46c4a":"# Rename the column of dataset for easy"}}