{"cell_type":{"c1142b6e":"code","31b1f101":"code","9a57ee82":"code","22a9bae5":"code","d426deee":"code","4cf7dcc0":"code","27d54459":"code","eea6e1d7":"code","2a846c83":"code","7768175c":"code","21bc95bb":"code","74d92eb9":"code","7291b641":"code","226a0509":"code","e29100c5":"code","3a80ca43":"code","53f8901f":"code","6794834f":"code","ea5184ed":"code","5c5fd8a8":"code","4678ebe9":"code","40ca941c":"code","dea34a75":"code","1d0fb4c7":"code","9f60d5b1":"code","6c34ffa6":"code","2e1a0138":"code","e11a42e6":"code","b173c597":"code","8c86be04":"code","2e903daa":"code","8d49b3ec":"code","242a2416":"code","f8601eab":"code","0ef08c45":"code","f38fac41":"code","40c024ff":"code","0558d9ee":"code","f79f9687":"markdown","fdf419df":"markdown","95fb3430":"markdown","d345eaf9":"markdown","a0258184":"markdown","5362ffa6":"markdown","c1ea4196":"markdown","2e14de30":"markdown","124ef44f":"markdown","a2c47c1d":"markdown","fb2eb430":"markdown","3006316e":"markdown","3ad5cda0":"markdown","29d435f9":"markdown","77835175":"markdown","360060d5":"markdown","5c593da9":"markdown","0e9d709a":"markdown","3fd7ef29":"markdown","19e8186c":"markdown","b1c2a902":"markdown","a5aa9bbc":"markdown","3936e07c":"markdown","496998c1":"markdown","c2403b3f":"markdown"},"source":{"c1142b6e":"import pandas as pd\nimport numpy as np\nimport lightgbm as lgb\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom joblib import parallel, delayed\nimport gc\nimport sys\nimport pytz\nimport warnings\nimport time\nimport inspect\nimport datetime\nfrom itertools import chain\nfrom datetime import date, timedelta\nfrom kaggle.competitions import twosigmanews\nwarnings.filterwarnings(\"ignore\",category=DeprecationWarning)\n#______________________________________________________________\nenv = twosigmanews.make_env()\n(market_train, news_train) = env.get_training_data()","31b1f101":"def reduce_mem_usage(df, verbose=True):\n    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    start_mem = df.memory_usage().sum() \/ 1024**2    \n    for col in df.columns:\n        col_type = df[col].dtypes\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)    \n    end_mem = df.memory_usage().sum() \/ 1024**2\n    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) \/ start_mem))\n    return df","9a57ee82":"def show_mem_usage():\n    ipython_vars = ['In', 'Out', 'exit', 'quit', 'get_ipython', 'ipython_vars']\n    list_objects=[]\n    mem = 0\n    for x in globals():\n        if x.startswith('_'): continue\n        if x in sys.modules: continue \n        if x in ipython_vars: continue\n        if isinstance(globals().get(x), pd.DataFrame):\n            mem = sys.getsizeof(globals().get(x))\/1e+6\n            if mem > 1:\n                list_objects.append([x, mem ])\n        else:\n            for o in dir(globals().get(x)):\n                if o.startswith('__'): continue\n                mem = sys.getsizeof(getattr(globals().get(x), o))\/1e+6\n                if mem > 1:\n                    list_objects.append(['.'.join([x,o]), mem ])\n    return sorted(list_objects, key=lambda x: x[1], reverse=True)","22a9bae5":"def timeit(method):\n    def timed(*args, **kw):\n        ts = time.time()\n        result = method(*args, **kw)\n        te = time.time()\n        if TIME_FUNCTIONS:\n            dt = (te - ts)*1000\n            if dt < 1000:\n                print ('{:<40}  {:>20.2f} ms'.format(method.__name__, dt))\n            else:\n                print ('{:<40}  {:>20.2f} s'.format(method.__name__, dt\/1000))\n        return result\n    return timed","d426deee":"gc.collect()\nmemory_used = show_mem_usage()\nprint(\"approximate memory usage: {:>5.2f}GB\".format(sum([s[1] for s in memory_used])\/1000))\nmemory_used","4cf7dcc0":"class model2SigmaStockPrizes():\n    #_______________________\n    # class initialisation\n    @timeit\n    def __init__(self, market, news, verbose=False):\n        self.market = market.sort_values('time')\n        self.news = news.sort_values('time')\n        self.market = reduce_mem_usage(self.market, verbose)\n        self.news = reduce_mem_usage(self.news, verbose)\n        self.verbose = verbose\n        self._format_dates()\n        self._convert_booleans()\n    #________________________________________________________________________\n    # detection of columns with dates and times, then reduced to single dates\n    @timeit\n    def _format_dates(self):\n        for df in [self.market, self.news]:\n            datetime_cols = [c for c in df.columns if 'date' in str(df[c].dtypes)]\n            for col in datetime_cols:\n                df[col] = df[col].dt.normalize()\n                if self.verbose: \n                    print (\"Content of column:'{}' set as date\".format(col))\n    #_________________________________\n    # convert booleans columns to int\n    @timeit\n    def _convert_booleans(self):\n        for col in self.news.columns:\n            if self.news[col].dtype == bool: \n                self.news[col] = self.news[col].astype(int)","27d54459":"TIME_FUNCTIONS = True\ntwo_sigma_model =  model2SigmaStockPrizes(market_train, news_train, verbose=True)","eea6e1d7":"del news_train\ndel market_train\ngc.collect()\nshow_mem_usage()","2a846c83":"@timeit\ndef select_dates(self, first_date):\n    self.market = self.market.loc[self.market['time'] >= first_date]\n    self.news = self.news.loc[self.news['time'] >= first_date]\n    if self.verbose:\n        print(\"data before '{}' has been removed\".format(first_date))\n#______________________________________________________________________\ntwo_sigma_model.select_dates = select_dates.__get__(two_sigma_model)\ntwo_sigma_model.select_dates(datetime.datetime(2009, 1, 1, 0, 0, 0, 0, pytz.UTC))","7768175c":"gc.collect()\nshow_mem_usage()","21bc95bb":"@timeit\ndef column_combinations(self):\n    self.market['price_diff'] = self.market['close'] - self.market['open']\n\n@timeit\ndef asset_codes_encoding(self):\n    self.news['assetCodesLen'] = self.news['assetCodes'].map(lambda x: len(eval(x)))\n    self.news['assetCodes'] = self.news['assetCodes'].str.findall(f\"'([\\w\\.\/]+)'\").apply(lambda x: x[0])\n\n@timeit\ndef news_count(self):\n    t = self.news.groupby(['time', 'assetName']).size().reset_index(name='news_count')\n    self.news = pd.merge(self.news, t, on=['time', 'assetName'])\n#______________________________________________________________________\ntwo_sigma_model.column_combinations = column_combinations.__get__(two_sigma_model)\ntwo_sigma_model.asset_codes_encoding = asset_codes_encoding.__get__(two_sigma_model)\ntwo_sigma_model.news_count = news_count.__get__(two_sigma_model)\n\ntwo_sigma_model.column_combinations()\ntwo_sigma_model.asset_codes_encoding()\ntwo_sigma_model.news_count()    ","74d92eb9":"@timeit\ndef aggregate_numericals(self):\n    numerics = ['int8', 'int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    self.news_numerical = self.news.select_dtypes(include=numerics).copy()\n    self.news_numerical_columns = self.news_numerical.columns\n    self.news_numerical.loc[:, 'assetCodes'] = self.news['assetCodes'] \n    self.news_numerical.loc[:, 'time'] = self.news['time']\n\n    agg_func = {\n        'takeSequence': ['sum', 'mean', 'max', 'min', 'std'],\n        'bodySize': ['sum', 'mean', 'max', 'min', 'std'],\n        'marketCommentary': ['sum', 'mean'],\n        'sentenceCount': ['sum', 'mean', 'max', 'min', 'std'],\n        'wordCount': ['sum', 'mean', 'max', 'min', 'std'],\n        'relevance': ['sum', 'mean', 'max', 'min', 'std'],\n        'firstMentionSentence': ['sum', 'mean', 'max', 'min', 'std'],\n        'sentimentNegative': ['sum', 'mean', 'max', 'min', 'std'],\n        'sentimentNeutral': ['sum', 'mean', 'max', 'min', 'std'],\n        'sentimentPositive': ['sum', 'mean', 'max', 'min', 'std'],\n        'sentimentWordCount': ['sum', 'mean', 'max', 'min', 'std'],\n        'noveltyCount12H': ['sum', 'mean', 'max', 'min', 'std'],\n        'noveltyCount24H': ['sum', 'mean', 'max', 'min', 'std'],\n        'noveltyCount3D': ['sum', 'mean', 'max', 'min', 'std'],\n        'noveltyCount5D': ['sum', 'mean', 'max', 'min', 'std'],\n        'noveltyCount7D': ['sum', 'mean', 'max', 'min', 'std'],\n        'volumeCounts12H': ['sum', 'mean', 'max', 'min', 'std'],\n        'volumeCounts24H': ['sum', 'mean', 'max', 'min', 'std'],\n        'volumeCounts3D': ['sum', 'mean', 'max', 'min', 'std'],\n        'volumeCounts5D': ['sum', 'mean', 'max', 'min', 'std'],\n        'volumeCounts7D': ['sum', 'mean', 'max', 'min', 'std'],\n        'news_count': ['max']\n        }\n\n    self.news_numerical = self.news_numerical.groupby(['assetCodes', 'time']).agg(agg_func)\n    self.news_numerical.columns = ['_'.join(col).strip()\n                                   for col in self.news_numerical.columns.values]\n    self.news_numerical.reset_index(inplace=True)\n#______________________________________________________________________\ntwo_sigma_model.aggregate_numericals = aggregate_numericals.__get__(two_sigma_model)\n#two_sigma_model.aggregate_numericals() ","7291b641":"@timeit\ndef aggregate_categoricals(self): \n    categ_columns = [col for col in self.news.columns\n                     if col not in self.news_numerical_columns]\n    if self.verbose:\n        print(\"news's categorical columns:\\n\", categ_columns)\n    temp = self.news[self.news['news_count'] > 1][categ_columns].copy()\n    multiple_articles = temp.groupby(['assetCodes', 'time']).tail(1)\n    single_articles = self.news[self.news['news_count'] == 1][categ_columns]\n    self.news_categ = pd.concat([multiple_articles, single_articles])\n#________________________________________________________________________________________\ntwo_sigma_model.aggregate_categoricals = aggregate_categoricals.__get__(two_sigma_model)\n#two_sigma_model.aggregate_categoricals() ","226a0509":"@timeit\ndef merge_news(self):\n    self.news = pd.merge(self.news_numerical,\n                         self.news_categ,\n                         on = ['assetCodes', 'time'],\n                         how='left')\n    # freeing memory\n    self.news_categ = 0\n    self.news_numerical = 0\n\n@timeit\ndef merge_market_news(self, keep_news=False):\n    if keep_news:\n        self.merged_df = pd.merge(self.market,\n                                  self.news,\n                                  left_on=['time', 'assetCode'],\n                                  right_on=['time', 'assetCodes'],\n                                  how='left')\n    else:\n        self.merged_df = self.market\n    # freeing memory\n    self.news = 0\n    self.market = 0\n    self.merged_df = reduce_mem_usage(self.merged_df, self.verbose)\n    if self.verbose:\n        print(\"merged_df's shape is: {}\".format(self.merged_df.shape))\n#_________________________________________________________________________\ntwo_sigma_model.merge_news = merge_news.__get__(two_sigma_model)\ntwo_sigma_model.merge_market_news = merge_market_news.__get__(two_sigma_model)\n\n#two_sigma_model.merge_news()\ntwo_sigma_model.merge_market_news() ","e29100c5":"gc.collect()\nshow_mem_usage()","3a80ca43":"@timeit\ndef set_labels(self, labeled_columns, define_index=False):\n    #____________________________________________________\n    # indexation during training stage\n    if define_index:\n        self.indexer = {}\n        for col in labeled_columns:\n            _, self.indexer[col] = pd.factorize(self.merged_df[col])\n    # label encoding\n    self.categorical_columns = labeled_columns\n    if self.verbose:\n        print(\"categorical variables: {}\".format(labeled_columns))\n    for col in labeled_columns:\n        self.merged_df[col] = self.indexer[col].get_indexer(self.merged_df[col])\n#_____________________________________________________________________________\ntwo_sigma_model.set_labels = set_labels.__get__(two_sigma_model)\n#two_sigma_model.set_labels(['headlineTag', 'provider'], define_index=True)    \ntwo_sigma_model.set_labels([], define_index=True)    ","53f8901f":"@timeit\ndef save_history(self):\n    self.common_lag_features = ['time', 'assetCode'] \n    self.used_for_news_lag = [] # ['sentimentNegative_mean']\n    self.used_for_lag = ['price_diff', 'close']\n    self.history_df =  self.merged_df[self.common_lag_features +\n                                      self.used_for_lag +\n                                      self.used_for_news_lag].copy()\n#_____________________________________________________________________________\ntwo_sigma_model.save_history = save_history.__get__(two_sigma_model)\ntwo_sigma_model.save_history()    ","6794834f":"gc.collect()\nshow_mem_usage()","ea5184ed":"def run_parallel(grouped, method, verbose):\n    with parallel.Parallel(n_jobs=-1, verbose=verbose) as par:\n        segments = par(delayed(method)(df_seg) for df_seg in grouped)\n    return segments\n\ndef account_for_past(df_code, n_lag=[7, 14, 21], shift_size=1):\n    features = [c for c in df_code.columns if c not in ['assetCode']]\n    lag_columns = []\n    for col in features:\n        for window in n_lag:\n            if LAST_DATE_ONLY:\n                tmp = df_code[df_code.index.max() - 2*timedelta(window):df_code.index.max()]\n                rolled = tmp[col].shift(shift_size, freq='D').rolling(window=window)\n            else:\n                rolled = df_code[col].shift(shift_size, freq='D').rolling(window=window)\n                \n#             for fct in [np.mean, min, max]:\n#                 col_name = '{}_{}_past{}'.format(col, fct.__name__, window)\n#                 lag_columns.append(col_name)\n#                 df_code[col_name] = rolled.apply(fct)\n\n                col_name = '{}_median_past{}'.format(col, window)\n                lag_columns.append(col_name)\n                df_code[col_name] = rolled.median()\n            \n                col_name = '{}_max_past{}'.format(col, window)\n                lag_columns.append(col_name)\n                df_code[col_name] = rolled.max()\n                \n                col_name = '{}_min_past{}'.format(col, window)\n                lag_columns.append(col_name)\n                df_code[col_name] = rolled.min()\n                \n    return lag_columns, df_code.drop(features, axis=1)\n\ndef create_returns(df_code, n_days=[3, 5, 7, 12]):\n    features = [c for c in df_code.columns if c not in ['assetCode']]\n    lag_columns = []\n    for col in features:\n        for days in n_days:\n            col_shift = 'shift_{}_{}'.format(col, days)  \n            col_name = 'returns_{}_PrevRaw{}'.format(col, days)\n            lag_columns.append(col_name)\n            df_code.loc[:, col_shift] = df_code[col].shift(days, freq='D')\n            df_code[col_name] = (df_code[col_shift] - df_code[col]) \/ df_code[col_shift]\n            df_code.drop(col_shift, axis=1, inplace=True)\n    return lag_columns, df_code.drop(features, axis=1)\n\n@timeit\ndef define_lagged_var(self):\n    start_time = time.time()\n    list_df = [d[1][self.common_lag_features + ['price_diff']].set_index('time').copy()\n               for d in self.history_df.groupby('assetCode', sort=False)]\n    if self.verbose:\n        print('assetCodes: {}, time to group: {:<5.2f}s'.format(len(list_df), time.time() - start_time))\n        \n    grouped = run_parallel(list_df, account_for_past, int(self.verbose))\n    self.lag_columns = grouped[0][0]\n    self.merged_df = pd.merge(self.merged_df,\n                              pd.concat([d[1] for d in grouped]).reset_index(),\n                              on=self.common_lag_features,\n                              how='left')\n    \n@timeit\ndef define_lagged_return(self):\n    start_time = time.time()\n    list_df = [d[1][self.common_lag_features + ['close']].set_index('time').copy()\n               for d in self.history_df.groupby('assetCode', sort=False)]\n    if self.verbose:\n        print('assetCodes: {}, time to group: {:<5.2f}s'.format(len(list_df), time.time() - start_time))\n        \n    grouped = run_parallel(list_df, create_returns, int(self.verbose))\n    self.lag_columns = grouped[0][0]\n    self.merged_df = pd.merge(self.merged_df,\n                              pd.concat([d[1] for d in grouped]).reset_index(),\n                              on=self.common_lag_features,\n                              how='left')\n        \n    \n#_____________________________________________________________________________\ntwo_sigma_model.define_lagged_var = define_lagged_var.__get__(two_sigma_model)\ntwo_sigma_model.define_lagged_return = define_lagged_return.__get__(two_sigma_model)\nLAST_DATE_ONLY = False\ntwo_sigma_model.define_lagged_var()      \ntwo_sigma_model.define_lagged_return()      ","5c5fd8a8":"@timeit\ndef news_moving_averages(self, col, window):\n    col_name = 'news_mva_'+ col + '_' + str(window) + 'days'\n    if self.verbose:\n        print(\"column '{}' has been created\".format(col_name))\n    avg_col = \\\n        self.history_df[self.common_lag_features+[col]].set_index('time').\\\n        groupby('assetCode').rolling(window=window, freq='D').mean().reset_index()\n    avg_col.rename(columns = {col: col_name}, inplace = True)\n    self.merged_df = pd.merge(self.merged_df,\n                              avg_col[self.common_lag_features + [col_name]],\n                              on=self.common_lag_features,\n                              how='left')\n    return col_name\n\n@timeit\ndef calc_news_moving_averages(self):    \n    self.news_lag_columns = []\n    for col in ['sentimentNegative_mean']:\n        new_col = self.news_moving_averages(col, 7)\n        self.news_lag_columns.append(new_col)\n#_____________________________________________________________________________\ntwo_sigma_model.news_moving_averages = news_moving_averages.__get__(two_sigma_model)\ntwo_sigma_model.calc_news_moving_averages = calc_news_moving_averages.__get__(two_sigma_model)\n#two_sigma_model.calc_news_moving_averages()   ","4678ebe9":"@timeit\ndef select_variables(self):\n    removed_columns = [\n        'assetCode', 'assetCodes', 'assetCodesLen', 'assetName_x', 'assetName_y', 'assetName',\n        'audiences', 'firstCreated', 'headline', 'returnsOpenNextMktres10',\n        'sourceId', 'subjects', 'time', 'universe','sourceTimestamp']\n    self.selected_variables = [c for c in self.merged_df.columns if c not in removed_columns]\n    \n    if self.verbose:\n        print(\"variables kept: {}\".format(self.selected_variables))\n#_____________________________________________________________________________\ntwo_sigma_model.select_variables = select_variables.__get__(two_sigma_model)\ntwo_sigma_model.select_variables()  ","40ca941c":"@timeit\ndef define_time_cv(self):\n    X = self.merged_df[self.selected_variables]\n    target = self.merged_df['returnsOpenNextMktres10']\n\n    time = self.merged_df['time']\n    universe = self.merged_df['universe']\n\n    n_train = int(X.shape[0] * 0.8)\n    self.X_train, self.y_train = X.iloc[:n_train], target[:n_train]\n    self.X_valid, self.y_valid = X.iloc[n_train:], target[n_train:]\n    self.t_valid = time.iloc[n_train:]\n\n    # For valid data, keep only those with universe > 0. This will help calculate the metric\n    u_valid = (universe.iloc[n_train:] > 0)\n    self.t_valid = time.iloc[n_train:]\n\n    self.X_valid = self.X_valid[u_valid]\n    self.y_valid = self.y_valid[u_valid]\n    self.t_valid = self.t_valid[u_valid]\n    del u_valid\n#________________________________________________________________________________\ntwo_sigma_model.define_time_cv = define_time_cv.__get__(two_sigma_model)\ntwo_sigma_model.define_time_cv()  ","dea34a75":"@timeit\ndef prep_lgbm_data(self):\n    \n    self.dtrain = lgb.Dataset(\n        self.X_train.values, self.y_train,\n        feature_name = self.selected_variables,\n        categorical_feature = self.categorical_columns,\n        free_raw_data = False)\n    \n    self.dvalid = lgb.Dataset(\n        self.X_valid.values, self.y_valid,\n        feature_name = self.selected_variables,\n        categorical_feature = self.categorical_columns,\n        free_raw_data = False)\n    \n    self.dvalid.params = {'extra_time': self.t_valid.factorize()[0]}\n#________________________________________________________________________________\ntwo_sigma_model.prep_lgbm_data = prep_lgbm_data.__get__(two_sigma_model)\ntwo_sigma_model.prep_lgbm_data() ","1d0fb4c7":"def sigma_score(preds, valid_data):\n    df_time = valid_data.params['extra_time']\n    labels = valid_data.get_label()\n    val = pd.DataFrame()\n    val['time'] = df_time\n    val['y'] = preds * labels.values\n    output = val.groupby('time').sum()\n    score = output['y'].mean() \/ output['y'].std()\n    return 'sigma_score', score, True","9f60d5b1":"def clean_frames(self):\n    del self.merged_df\n#________________________________________________________________________________\ntwo_sigma_model.clean_frames = clean_frames.__get__(two_sigma_model)\ntwo_sigma_model.clean_frames()","6c34ffa6":"two_sigma_model.X_train[-15:]","2e1a0138":"gc.collect()\nshow_mem_usage()","e11a42e6":"@timeit\ndef train_model(self, lgb_params):\n    evals_result = {}\n    self.model = lgb.train(\n        lgb_params,\n        self.dtrain,\n        num_boost_round= 10000,\n        valid_sets=(self.dvalid,),\n        valid_names=('valid',),\n        verbose_eval=100,\n        early_stopping_rounds=200,\n        feval=sigma_score,\n        evals_result=evals_result\n    )\n    df_result = pd.DataFrame(evals_result['valid'])\n    self.num_boost_round, valid_score = \\\n        df_result['sigma_score'].idxmax()+1, df_result['sigma_score'].max()\n    \n    print(f'Best score was {valid_score:.5f} on round {self.num_boost_round}')\n    \n    del self.X_train\n    del self.y_train\n    del self.X_valid\n    del self.y_valid\n#________________________________________________________________________________\ntwo_sigma_model.train_model = train_model.__get__(two_sigma_model)\n\nlgb_params = dict(\n    objective = 'regression_l1',\n    learning_rate = 0.01,\n    num_leaves = 51,\n    max_depth = 8,\n    bagging_fraction = 0.9,\n    bagging_freq = 1,\n    feature_fraction = 0.9,\n    lambda_l1 = 0.0,\n    lambda_l2 = 1.0,\n    metric = 'None', \n    seed = 42)\n\n\ntwo_sigma_model.train_model(lgb_params) ","b173c597":"# two_sigma_model.model.feature_name()","8c86be04":"# liste = list(zip(two_sigma_model.model.feature_name(),\n#     two_sigma_model.model.feature_importance('gain')))\n# liste.sort(key = lambda x:x[1], reverse=True)\n# [x[0] for x in liste[:50]]","2e903daa":"def feat_importances(self):\n    fig, ax = plt.subplots(1, 1, figsize=(11, 20))\n    lgb.plot_importance(self.model, ax, importance_type='gain')\n    fig.tight_layout()\n#________________________________________________________________________________\ntwo_sigma_model.feat_importances = feat_importances.__get__(two_sigma_model)\ntwo_sigma_model.feat_importances()","8d49b3ec":"@timeit\ndef data_prep(self, market, news):\n    self.verbose = False\n    self.market = market\n    self.news = news\n    self._format_dates()\n    self._convert_booleans()\n    self.column_combinations()\n    self.asset_codes_encoding()\n    self.news_count()   \n    #self.aggregate_numericals()\n    #self.aggregate_categoricals() \n    #self.merge_news()\n    self.merge_market_news()\n    #self.set_labels(['headlineTag', 'provider'])  \n    self.set_labels([])  \n#________________________________________________________________________________\ntwo_sigma_model.data_prep = data_prep.__get__(two_sigma_model)","242a2416":"days = env.get_prediction_days()","f8601eab":"@timeit\ndef add_new_date(self):\n    self.history_df = pd.concat([self.history_df,\n                                 self.merged_df[\n                                      self.common_lag_features +\n                                      self.used_for_lag +\n                                      self.used_for_news_lag]])\n    if self.verbose:\n        print(\"data for lagged quantities from {} to {}\".format(\n            self.lag_df['time'].min().date(),\n            self.lag_df['time'].max().date()))\n#_______________________________________________________________\ntwo_sigma_model.add_new_date = add_new_date.__get__(two_sigma_model)","0ef08c45":"@timeit\ndef predict(self, date, pred_template):\n    \n    df = self.merged_df[self.merged_df['time'] == date]\n    \n    predictions = self.model.predict(\n        df[self.selected_variables].values,\n        ntree_limit = self.num_boost_round\n    )\n    \n    preds = pd.DataFrame({'assetCode': df['assetCode'],\n                          'confidence': np.clip(predictions, -1, 1)})\n    \n    self.formated_pred = \\\n        pred_template.merge(preds, how='left').\\\n        drop('confidenceValue', axis=1).\\\n        fillna(0).\\\n        rename(columns={'confidence': 'confidenceValue'})\n#_______________________________________________________________\ntwo_sigma_model.predict = predict.__get__(two_sigma_model)","f38fac41":"two_sigma_model.history_df = \\\ntwo_sigma_model.history_df[two_sigma_model.history_df['time'] > (two_sigma_model.history_df['time'].max() - timedelta(30))]","40c024ff":"TIME_FUNCTIONS = False\nn_days = 0\nfor (market_obs_df, news_obs_df, predictions_template_df) in days:\n    n_days +=1\n    start_time = time.time()\n    print(n_days, end=' ')\n    two_sigma_model.data_prep(market_obs_df, news_obs_df)\n    date = two_sigma_model.merged_df['time'].max()\n    #_________________________________\n    two_sigma_model.add_new_date()\n    two_sigma_model.define_lagged_var()\n    two_sigma_model.define_lagged_return()\n    #two_sigma_model.calc_news_moving_averages()\n    two_sigma_model.select_variables()\n    two_sigma_model.predict(date, predictions_template_df)\n    env.predict(two_sigma_model.formated_pred)","0558d9ee":"env.write_submission_file()","f79f9687":"Finally, we introduce a decorator that allow to time functions:","fdf419df":"<a id=\"9\"><\/a> <br>\n#### 1.3 Feature selection\nBefore training, we select the variables we want to keep in our model:","95fb3430":"<a id=\"5\"><\/a> <br>\n#### 1.2.1 Engineering  the `News` data\nThe `news` data contains a few numerical columns. For each instrument and each day, we aggregate the corresponding variables with common statistics:","d345eaf9":"and a method that performs the predictions:","a0258184":"Finally, we define a method that adds a new day to the data currently available:","5362ffa6":"to finally train the model:","c1ea4196":"Concerning the categorical columns, we only consider the last entry when for an instrument, there are multiple entries on a single day:","2e14de30":"<a id=\"2\"><\/a> <br>\n## 1. Class definition\nIn this section, I develop a class that contains the feature engineering and training of the model. For a didactic purpose, the various methods are are introduced consecutively and are thus added to the instance that contains the model. In production, all the methods would be merged while first defining the class.","124ef44f":"<a id=\"10\"><\/a> <br>\n## 2. Training the model\n\nAs a first step, we split the data set in order to cross-validate the model according to time:","a2c47c1d":"<a id=\"6\"><\/a> <br>\n#### 1.2.2 merging the `News` and `Market` data","fb2eb430":"The second helps tracking memory usage:","3006316e":"<a id=\"7\"><\/a> <br>\n####  1.2.3 Indexing categorical columns\nAt this stage, we define the categorical columns we will subsequently introduce in LGBM as `categorical_features`. These index is set during training and then reloaded during the prediction stage:","3ad5cda0":"We can inspect the feature importance:","29d435f9":"We are then able to loop over the test set days and create our prediction: ","77835175":"We can check in which objects the memory is currently allocated: ","360060d5":"<a id=\"4\"><\/a> <br>\n### 1.2 Feature engineering\nA first step consists in performing some basic feature engineering:","5c593da9":"and then prepare the data for LGBM:","0e9d709a":"We create the class instance:","3fd7ef29":"<a id=\"11\"><\/a> <br>\n## 3. Model pipeline\nDuring the prediction stage, the observations of each day are given one after the other and we have to make the prediction before loading the data of the next day.\nHence, in order to perform time averages or calculate lagged values, we have to enrich our data while it comes. We then define a method that concatentate the whole process described above:","19e8186c":"<a id=\"12\"><\/a> <br>\n## 4. Test set predictions","b1c2a902":"<a id=\"8\"><\/a> <br>\n#### 1.2.4 Lagged information\nA way to introduce the time is to create variables that contain past information (either time averages or lagged quantities). Below, we consider the returns on the `close` and `open` variables with respect to past values:","a5aa9bbc":"<a id=\"3\"><\/a> <br>\n### 1.1 Selecting training dates\nTo reduce the time spent to pre-process the data, and ease the FE exploration, we can limit the number of dates during the exploration. Moreover, as outlined in a few discussions and kernels, the data before 2009 may false the models and it is safe to forget about this period. Hence, the following method allows to select the first date to account for:","3936e07c":"We define a function that encodes the competition's metric:","496998c1":"<a id=\"1\"><\/a> <br>\n## 1. Utility functions\nWe first load some utility functions. The first, written by [Guillaume Martin](https:\/\/www.kaggle.com\/gemartin\/load-data-reduce-memory-usage#), allows to manage memory:","c2403b3f":"This notebook presents a LGBM model where both the `market` and `news` data are merged. In order to facilitate the data preparation and the subsequent update during prediction stage, the model is embedded in a python Class. In the first part of the notebook, the Class funcionalities are presented, step-by-step, by adding new methods to the class instance. In a second stage, the real training is performed.\n\n<a id=\"top\"><\/a> <br>\n## Notebook  Content\n1. [Utility functions](#1)\n1. [Class definition](#2)\n    1. [Selecting training dates](#3)\n    1. [Feature engineering](#4)\n        1. [Engineering the News data](#5)\n        1. [merging the News and Market data](#6)\n        1. [ Indexing categorical columns](#7)\n        1. [ Lagged information](#8)\n    1. [Feature selection](#9)\n1. [Training the model](#10)\n1. [Model pipeline](#11)\n1. [Test set predictions](#12)"}}