{"cell_type":{"a39b7cda":"code","258b913a":"code","335d4e5b":"code","4b4efb16":"code","71b8b3f5":"code","71a80bc7":"code","b6fb13cf":"code","44d8d9ea":"code","5af169ba":"code","1b071296":"code","fdcf54ba":"code","f94deb84":"code","d84ccce9":"code","62232b5e":"code","b0bd8c42":"markdown","d77be09b":"markdown","839d3077":"markdown","d15ad194":"markdown","6a00609f":"markdown","3f6ff35c":"markdown","e0f371fe":"markdown","6a27be37":"markdown","a505755d":"markdown","a18d1501":"markdown","eb64671b":"markdown","e39b4e3b":"markdown","113ddeb5":"markdown","862148ef":"markdown"},"source":{"a39b7cda":"! cp ..\/input\/german-word2vec\/myutils.py .\n! ls -l","258b913a":"# Importing packages\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport matplotlib as mpl\nmpl.rcParams['figure.figsize'] = (20.0, 15.0)\nimport mpld3\n\nimport gensim\nfrom gensim import corpora\nfrom gensim.corpora import WikiCorpus\nfrom gensim.models import Word2Vec\nfrom gensim.models.word2vec import LineSentence\nfrom gensim.models import KeyedVectors\nfrom gensim.scripts.glove2word2vec import glove2word2vec\nfrom gensim.test.utils import datapath, get_tmpfile\n\nimport networkx as nx\n\nfrom sklearn.decomposition import PCA\nfrom sklearn.manifold import TSNE\n\nfrom ipywidgets import IntProgress\nfrom IPython.display import display\n\nimport myutils","335d4e5b":"# get trained model, files without a suffix, .bin or .model are treated as binary files\ntrained_model1 = gensim.models.KeyedVectors.load_word2vec_format('..\/input\/german-word2vec\/german.model', binary=True)\n# remove original vectors to free up memory\ntrained_model1.init_sims(replace=True)","4b4efb16":"word = \"Fruehstueck\"\n[k for k,w in trained_model1.vocab.items() if k.startswith(word)][0:20]","71b8b3f5":"word = \"Wien\"\ntrained_model1[word]","71a80bc7":"print(trained_model1.similarity('kopf', 'blau'))\nprint(trained_model1.distance('kopf', 'blau'))","b6fb13cf":"words = ['Werkzeug', 'blau', 'rot', 'kopf', 'Gewerbe']\nseries = []\nfor word in words:\n    series.append(pd.DataFrame(trained_model1.most_similar(word, topn=10), columns=[f\"Similar_{word}\", \"similarity\"]))\ndf = pd.concat(series, axis=1)\ndf.head(10)","44d8d9ea":"#word1, word2, word3, word4, word5 = 'blau','rot','feld','gruen','gelb'\n#word1, word2, word3, word4, word5 = 'Fruehstueck', \"Fenster\", 'Abendessen','Mittagessen', \"Soupe\"\nword1, word2, word3, word4, word5 = \"Vater\", \"Mutter\", \"Sohn\", \"Tochter\", \"Oma\"\n#word1, word2, word3, word4, word5 = \"Frankreich\",\"England\",\"Deutschland\",\"Berlin\",\"Oesterreich\"\n\n\n\nprint(trained_model1.doesnt_match([word1, word2, word3, word4, word5]))","5af169ba":"#positive_vectors = ['Koenig', 'frau']\n#negative_vectors = ['mann']\n\npositive_vectors = ['frau', 'blau']\nnegative_vectors = ['mann']\n\nfor result in trained_model1.most_similar(positive=positive_vectors, \n                                          negative=negative_vectors):\n    print(result)","1b071296":"wordpairs = [\"Mann\", \"Vater\",\n             \"Frau\",  \"Mutter\",\n             \"Mutter\", \"Oma\",\n             \"Vater\", \"Grossvater\",\n             \"Junge\", \"Mann\",\n             \"Maedchen\", \"Frau\",\n            ]\n\nmyutils.draw_words(trained_model1, wordpairs, True, True, True, -2.5, 2.5, -2.5, 2.5, r'$PCA Visualisierung:')","fdcf54ba":"# plot currencies\nwordpairs = [\"Schweiz\", \"Franken\",\n             \"Deutschland\", \"Euro\",\n             \"Grossbritannien\", \"britische_Pfund\",\n             \"Japan\", \"Yen\",\n             \"Russland\", \"Rubel\",\n             \"USA\", \"US-Dollar\",\n             \"Kroatien\", \"Kuna\",\n             \"Oesterreich\", \"Euro\",]\n\nmyutils.draw_words(trained_model1, wordpairs, True, True, True, -2, 2, -2, 2, r'$PCA Visualisierung:')","f94deb84":"wordpairs = [\"duerfen\", \"koennen\",  # change this words and run the cell again\n             \"moegen\", \"muessen\",\n            ]\n\nmyutils.draw_words(trained_model1, wordpairs, True, True, True, -2.5, 2.5, -2.5, 2.5, r'$PCA Visualisierung:')","d84ccce9":"#word = 'Oesterreich'\n#word = 'Akademie'\n#word = \"Kant\"\nword = \"Rock\"\n\nG = myutils.build_neighbors(word, trained_model1, 10) # number of similar words to display\npos = nx.spring_layout(G, iterations=100)\nnx.draw_networkx(G,\n                 pos=pos, \n                 node_color=nx.get_node_attributes(G,'color').values(),\n                 node_size=1000, \n                 alpha=0.8, \n                 font_size=12,\n                )","62232b5e":"! ls ..\/input\/cookbooks\/","b0bd8c42":"### Some exercises:  \n\n+ Let's supose you want to find out if the German Wikipedia has any gender biases using vectorial analogies. Could you think of a test?  \nhint: see [this paper](https:\/\/papers.nips.cc\/paper\/6228-man-is-to-computer-programmer-as-woman-is-to-homemaker-debiasing-word-embeddings.pdf)\n\n+ Can you train your own word2vec model?  \nhint: see [this blog post](https:\/\/kavita-ganesan.com\/gensim-word2vec-tutorial-starter-code\/#.XkXhPOGj9p8)  \n\n    - Use the collection of cookbooks for model training (see folder access below).   \n    - Train word2vec model with 200 dimensions, 12 words window and 5 iterations.  \n    - Find out what beef dish is most similar mutton chops \ud83d\ude09  \n    - Find top 10 most similar words for \u2018sweet\u2019 and \u2018sour\u2019.   \n    - Plot them with similarity to \u2018sweet\u2019 following the previous examples  ","d77be09b":"### There are different [metrics](http:\/\/mkusner.github.io\/publications\/WMD.pdf) for vector distances  ","839d3077":"### Checking the words that are present in the model:  \n\n#### Change the word, as you like ","d15ad194":"## Word embeddings ","6a00609f":"### What are the words most similar to each word of the list?   \n#### Change the words, as you like ","3f6ff35c":"### Now we are going to build a graph with similar words...  \n\n#### Change the word, as you like!  ","e0f371fe":"### [Using German Word2vec trained on the German Wikipedia (15th May 2015) and German news articles (15th May 2015)](https:\/\/devmount.github.io\/GermanWordEmbeddings\/#download)  ","6a27be37":"#### Now it is time to build your own projection...  \n#### Change the words, as you like!  ","a505755d":"### Let's make some vectorial operations with words?  \n#### Change the words, as you like! ","a18d1501":"### Examining the vector representation of a word","eb64671b":"### Prepare\n* Jupyter notebook:\n    - either install locally: `pip install jupyter`\n    - or go to kaggle.com, login and clone: https:\/\/www.kaggle.com\/rsouza\/winterschule-ai-14-02-2020-11-00-12-30\/","e39b4e3b":"Word embedding is one of the most popular representation of document vocabulary. It is capable of capturing context of a word in a document, semantic and syntactic similarity, relation with other words, etc.\nWhat are word embeddings exactly? Loosely speaking, they are vector representations of a particular word. Let's first see the traditional way to represent a word in vectors, a technique called \"Bag of Words\":\n\n + Document 1 - \u201cI am feeling very happy today\u201d\n + Document 2 - \u201cI am not well today\u201d\n + Document 3 - \u201cI wish I could go to play\u201d\n\nFirst, it creates a vocabulary using unique words from all the documents:\n\nVocabulary: [I, am, feeling, very, happy, today, not, well, wish, could, go, to, play]\n\nThen, for each word the frequency of the word in the corresponding document is inserted\n![Vector representation of documents](https:\/\/qph.fs.quoracdn.net\/main-qimg-054edc66e34ec439241ea1637350286f-c)\n\n![Vector space](http:\/\/3.bp.blogspot.com\/_tOOi3R89e74\/TUeyueig7ZI\/AAAAAAAAAJQ\/QHL-VLEWook\/s1600\/vector_space.png)\n\nBut if the vocabulary is huge, it may lead to very sparse vectors (mainly composed by zeros).  \n\nWord Embeddings is an efficient and effective way of representing words as vectors. The whole body of the text is encapsulated in some space of much lower dimension. In this space, all words are represented by a smaller set of vectors (usually around 300) and it is possible to explicitly define their relationship with each other. This is done using a neural network architecture like these:\n\n![Word2vec CBOW and Skip-gram architectures](https:\/\/miro.medium.com\/max\/594\/1*cktg-1KKmTKFb7Qfyj4QWg.png)\n\nThe resulting vector space can be illustrated by the following figure, that depicts how this space is able to capture relationships:\n\n![Geometrical relationships of embedded words](https:\/\/miro.medium.com\/max\/1077\/1*i-aWU_fjKblzRG4OTgmCkA.png)  \n\nNow let's see it in practice!","113ddeb5":"### Let's try reducing the dimensionality of the space and see a 2D projection of the vectors...  ","862148ef":"### What is the word that does not fit?  \n\n#### Change the words, as you like "}}