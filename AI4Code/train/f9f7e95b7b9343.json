{"cell_type":{"f8023d46":"code","8ee2ce80":"code","e0de9573":"code","34b28e98":"code","ab62d5bb":"code","83b32b54":"code","00e422fc":"code","2d29329a":"code","124ab43a":"code","4c3a7a8b":"code","e68d3bf1":"code","0c68ddd0":"code","89829898":"code","41dd4d06":"code","1dab1942":"code","6dcde4fd":"code","9b777689":"code","33603401":"code","78e05af8":"code","e3d7eb83":"code","e1ab240b":"markdown","743367c6":"markdown","0e1ca663":"markdown","a0c24dc1":"markdown","32059b7d":"markdown","99c36714":"markdown","a0c9071f":"markdown","fee26f0a":"markdown","1bf56003":"markdown","93f50085":"markdown","9a73b97b":"markdown","d2e38816":"markdown","ed9d067e":"markdown","951f2a2e":"markdown","57cf1388":"markdown","54b337cd":"markdown","39726eb4":"markdown","db045604":"markdown"},"source":{"f8023d46":"!pip install tensorflow==1.14","8ee2ce80":"import tensorflow as tf\nimport time\nfrom PIL import Image\nprint(tf.__version__)\nimport sys\nsys.path.append('..\/input\/handpose\/Hand Pose')\nroot='..\/input\/handpose\/Hand Pose\/'\nimport numpy as np\nimport sys\nimport os\n\nimport detection_rectangles\nfrom utils import draw_util\nfrom matplotlib import pyplot as plt\nimport cv2","e0de9573":"def Reformat_Image(img,im_width, im_height):\n\n    from PIL import Image\n    from skimage.transform import resize\n\n    width = img.shape[1]\n    height = img.shape[0]\n    image = Image.fromarray(img)\n    image_size = image.size\n    print(image_size)\n    bigsidew = im_width\n    bigsideh = im_height\n\n    background = Image.new('RGB', (bigsidew, bigsideh), (255, 255, 255))\n    offset = (int(round(((bigsidew - width) \/ 2), 0)), int(round(((bigsideh - height) \/ 2),0)))\n    print(\"offset: \",offset)\n    background.paste(image, offset)\n    print(background.size)\n    result= np.array(background)\n    #result= resize(result, (224,224,3))\n    plt.imshow(result)\n    plt.savefig('result.png')\n    print(result.shape)\n    print(\"Image has been resized !\")\n    return result","34b28e98":"def plot_multiple_img(img_matrix_list, title_list, ncols, main_title=\"Hand Pose\"):\n    fig, myaxes = plt.subplots(figsize=(20, 15), nrows=2, ncols=ncols, squeeze=True)\n    fig.suptitle(main_title, fontsize = 30)\n    #fig.subplots_adjust(wspace=0.3)\n    #fig.subplots_adjust(hspace=0.3)\n    for i, (img, title) in enumerate(zip(img_matrix_list, title_list)):\n        myaxes[i \/\/ ncols][i % ncols].imshow(img)\n        myaxes[i \/\/ ncols][i % ncols].set_title(title, fontsize=15)\n    plt.show()","ab62d5bb":"#image_np= Image.open(root+'body.jpg')\nfile='..\/input\/hand-pose-estimation\/Hand Data\/Multiple hands\/t2.jpg'\nimage_np= cv2.imread(file)\nimage_np = cv2.cvtColor(image_np, cv2.COLOR_BGR2RGB)\nim_width = image_np.shape[1]\nim_height = image_np.shape[0]\nprint(im_width,im_height)","83b32b54":"detection_graph, sess = detection_rectangles.load_inference_graph()","00e422fc":"relative_boxes, scores, classes = detection_rectangles.detect_objects(image_np, detection_graph, sess)","2d29329a":"box_relative2absolute = lambda box: (box[1] * im_width, box[3] * im_width, box[0] * im_height, box[2] * im_height)\nhand_boxes = []\n\n###Take the best 2 boxes for 2 hands\nhand_boxes.append(box_relative2absolute(relative_boxes[0]))\nhand_boxes.append(box_relative2absolute(relative_boxes[1]))\nprint(hand_boxes)","124ab43a":"image=draw_util.draw_box_on_image(hand_boxes,  image_np)\nplt.imshow(image)","4c3a7a8b":"protoFile = root+\"hand\/pose_deploy.prototxt\"\nweightsFile = root+\"hand\/pose_iter_102000.caffemodel\"\nnPoints = 22\nPOSE_PAIRS = [ [0,1],[1,2],[2,3],[3,4],[0,5],[5,6],[6,7],[7,8],[0,9],[9,10],[10,11],[11,12],[0,13],[13,14],[14,15],[15,16],[0,17],[17,18],[18,19],[19,20] ]\nnet = cv2.dnn.readNetFromCaffe(protoFile, weightsFile)","e68d3bf1":"count=1\nimg_matrix_list=[]\nfor box in hand_boxes:\n  xmin= int(box[0])\n  xmax= int(box[1])\n  ymin= int(box[2])\n  ymax= int(box[3])\n  print(ymin,ymax,xmin,xmax)\n  result= image_np[ymin:ymax,xmin:xmax,:]\n  im_width = 2*result.shape[1]\n  im_height = 2*result.shape[0]\n  frame= Reformat_Image(result,im_width,im_height)\n\n  frameCopy = np.copy(frame)\n  print(frame.shape)\n  frameWidth = frame.shape[1]\n  frameHeight = frame.shape[0]\n  print(frameWidth,frameHeight)\n  aspect_ratio = frameWidth\/frameHeight\n\n  threshold = 0.5\n\n  t = time.time()\n  # input image dimensions for the network\n  inHeight = 368\n  inWidth = int(((aspect_ratio*inHeight)*8)\/\/8)\n  inpBlob = cv2.dnn.blobFromImage(frame, 1.0 \/ 255, (inWidth, inHeight), (0, 0, 0), swapRB=False, crop=False)\n  print(inpBlob.shape)\n  net.setInput(inpBlob)\n\n  output = net.forward()\n  #print(output)\n  print(\"time taken by network : {:.3f}\".format(time.time() - t))\n\n    # Empty list to store the detected keypoints\n  points = []\n\n  for i in range(nPoints):\n    # confidence map of corresponding body's part.\n    probMap = output[0, i, :, :]\n    probMap = cv2.resize(probMap, (frameWidth, frameHeight))\n\n    # Find global maxima of the probMap.\n    minVal, prob, minLoc, point = cv2.minMaxLoc(probMap)\n    #print(probMap)\n    #print(point)\n    #print()\n        \n    if prob > threshold :\n        point= list(point)\n        addx= point[0]+xmin\n        addy= point[1]+ymin\n        point= tuple(point)\n        cv2.circle(frameCopy, (int(point[0]), int(point[1])), 2, (27, 74, 29), thickness=1, lineType=cv2.FILLED)\n        cv2.putText(frameCopy, \"{}\".format(i), (int(point[0]), int(point[1])), cv2.FONT_HERSHEY_SIMPLEX, 0.25, (0, 0, 255), 2, lineType=cv2.LINE_AA)\n\n\n        #cv2.circle(image_np, (int(addx), int(addy)), 8, (27, 74, 29), thickness=1, lineType=cv2.FILLED)\n        # Add the point to the list if the probability is greater than the threshold\n        points.append((int(point[0]), int(point[1])))\n        print(int(point[0]),int(point[1]))\n    else :\n        points.append(None)\n  \n  img_matrix_list.append(frameCopy)\n\n  # Draw Skeleton\n  for pair in POSE_PAIRS:\n    partA = pair[0]\n    partB = pair[1]\n\n    if points[partA] and points[partB]:\n        cv2.line(frame, points[partA], points[partB], (0, 255, 255), 8)\n        cv2.circle(frame, points[partA], 2, (0, 0, 255), thickness=2, lineType=cv2.FILLED)\n        cv2.circle(frame, points[partB], 2, (0, 0, 255), thickness=2, lineType=cv2.FILLED)\n\n  img_matrix_list.append(frame)\n  nymin= frame.shape[0] \/\/ 4\n  nymax= nymin + frame.shape[0] \/\/ 2\n  nxmin= frame.shape[1] \/\/ 4\n  nxmax= nxmin + frame.shape[1] \/\/ 2\n  #r_imgs.append(frame[nymin:nymax,nxmin:nxmax,:])\n\n  count=count+2\n","0c68ddd0":"titles_list = [\"Hand 1\",\"Hand1\",\"Hand 2\",\"Hand 2\"]\nplot_multiple_img(img_matrix_list, titles_list, ncols = 2)","89829898":"#https:\/\/stackoverflow.com\/questions\/20925818\/algorithm-to-check-if-two-boxes-overlap\n# Overlap check for 1D rectangle\ndef isOverlapping1D(box1,box2):\n    #print(box1,box2)\n    xmax1= box1[1]\n    xmin1= box1[0]\n    xmax2= box2[1]\n    xmin2= box2[0]\n    return xmax1 >= xmin2 and xmax2 >= xmin1\n\ndef isOverlapping2D(box1,box2):\n    return isOverlapping1D(box1['x'], box2['x']) and isOverlapping1D(box1['y'], box2['y'])","41dd4d06":"box1 = {'x':(hand_boxes[0][0],hand_boxes[0][1]),'y':(hand_boxes[0][2],hand_boxes[0][3])}\nbox2 = {'x':(hand_boxes[1][0],hand_boxes[1][1]),'y':(hand_boxes[1][2],hand_boxes[1][3])}\n\nif isOverlapping2D(box1,box2):\n    print(\"The bounding boxes are overlapping.\")\nelse:\n    print(\"The bounding boxes are not ovelapping\")\n","1dab1942":"def adjust_bbox(hand_boxes):\n    updated_hand_boxes=[]\n    for box in hand_boxes:\n        xmin= int(box[0])\n        xmax= int(box[1])\n        ymin= int(box[2])\n        ymax= int(box[3])\n        \n        xmin= xmin - (xmax-xmin)\/2\n        xmax= xmax + (xmax-xmin)\/2\n        \n        updated_box= [xmin,xmax,ymin,ymax]\n        updated_hand_boxes.append(updated_box)\n    \n    return updated_hand_boxes","6dcde4fd":"updated_hand_boxes= adjust_bbox(hand_boxes)\nprint(updated_hand_boxes)\nprint(hand_boxes)","9b777689":"count=1\nimg_matrix_list=[]\nr_imgs=[]\nfor box in updated_hand_boxes:\n  xmin= int(box[0])\n  xmax= int(box[1])\n  ymin= int(box[2])\n  ymax= int(box[3])\n  print(ymin,ymax,xmin,xmax)\n  result= image_np[ymin:ymax,xmin:xmax,:]\n  #im_width = 2*result.shape[1]\n  #im_height = 2*result.shape[0]\n  #frame= Reformat_Image(result,im_width,im_height)\n  frame= result\n  frameCopy = np.copy(frame)\n  print(frame.shape)\n  frameWidth = frame.shape[1]\n  frameHeight = frame.shape[0]\n  print(frameWidth,frameHeight)\n  aspect_ratio = frameWidth\/frameHeight\n\n  threshold = 0.5\n\n  t = time.time()\n  # input image dimensions for the network\n  inHeight = 368\n  inWidth = int(((aspect_ratio*inHeight)*8)\/\/8)\n  inpBlob = cv2.dnn.blobFromImage(frame, 1.0 \/ 255, (inWidth, inHeight), (0, 0, 0), swapRB=False, crop=False)\n  print(inpBlob.shape)\n  net.setInput(inpBlob)\n\n  output = net.forward()\n  #print(output)\n  print(\"time taken by network : {:.3f}\".format(time.time() - t))\n\n    # Empty list to store the detected keypoints\n  points = []\n\n  for i in range(nPoints):\n    # confidence map of corresponding body's part.\n    probMap = output[0, i, :, :]\n    probMap = cv2.resize(probMap, (frameWidth, frameHeight))\n\n    # Find global maxima of the probMap.\n    minVal, prob, minLoc, point = cv2.minMaxLoc(probMap)\n    #print(probMap)\n    #print(point)\n    #print()\n        \n    if prob > threshold :\n        point= list(point)\n        addx= point[0]+xmin\n        addy= point[1]+ymin\n        point= tuple(point)\n        cv2.circle(frameCopy, (int(point[0]), int(point[1])), 2, (27, 74, 29), thickness=1, lineType=cv2.FILLED)\n        cv2.putText(frameCopy, \"{}\".format(i), (int(point[0]), int(point[1])), cv2.FONT_HERSHEY_SIMPLEX, 0.25, (0, 0, 255), 2, lineType=cv2.LINE_AA)\n\n\n        #cv2.circle(image_np, (int(addx), int(addy)), 8, (27, 74, 29), thickness=1, lineType=cv2.FILLED)\n        # Add the point to the list if the probability is greater than the threshold\n        points.append((int(point[0]), int(point[1])))\n        print(int(point[0]),int(point[1]))\n    else :\n        points.append(None)\n  \n  img_matrix_list.append(frameCopy)\n\n  # Draw Skeleton\n  for pair in POSE_PAIRS:\n    partA = pair[0]\n    partB = pair[1]\n\n    if points[partA] and points[partB]:\n        cv2.line(frame, points[partA], points[partB], (0, 255, 255), 8)\n        cv2.circle(frame, points[partA], 2, (0, 0, 255), thickness=2, lineType=cv2.FILLED)\n        cv2.circle(frame, points[partB], 2, (0, 0, 255), thickness=2, lineType=cv2.FILLED)\n\n  img_matrix_list.append(frame)\n  #nymin= frame.shape[0] \/\/ 4\n  #nymax= nymin + frame.shape[0] \/\/ 2\n  #nxmin= frame.shape[1] \/\/ 4\n  #nxmax= nxmin + frame.shape[1] \/\/ 2\n  r_imgs.append(frame)\n\n  count=count+2\n","33603401":"titles_list = [\"Hand 1-Keypoint\",\"Hand1-Skeleton\",\"Hand 2-keypoint\",\"Hand 2-skeleton\"]\nplot_multiple_img(img_matrix_list, titles_list, ncols = 2)","78e05af8":"background = Image.fromarray(image_np)\nfor i in range(len(r_imgs)):\n  os2= (int(updated_hand_boxes[i][0]),int(updated_hand_boxes[i][2]))\n  print(os2)\n  background.paste(Image.fromarray(r_imgs[i]), os2)\n\nresult= np.array(background)\nplt.imshow(result)","e3d7eb83":"cv2.imwrite('Final_result.jpg',result)","e1ab240b":"# Adjust bounding box for better detection","743367c6":"**Load the detection graph**","0e1ca663":"Detect Keypoints from bounding boxes","a0c24dc1":"Import the libraries","32059b7d":"Load Keypoint Detector","99c36714":"# Check to see if two boxes overlap or not","a0c9071f":"**Function to check overlapping of two bounding boxes**","fee26f0a":"**Check overlap**","1bf56003":"Get bounding boxes and scores for the input image","93f50085":"Get the bounding boxes.","9a73b97b":"**Install the dependies**","d2e38816":"**Load An Image as Input**","ed9d067e":"**In this notebook, I tried to detect hand keypoints where there were two hands in the input images. I also predicted the bounding boxes for each hands. \nThe pre-trained models and some of the codes were taken from the following links:**\n\n 1. https:\/\/www.learnopencv.com\/hand-keypoint-detection-using-deep-learning-and-opencv\/\n 2. https:\/\/github.com\/victordibia\/handtracking\n \n**Future work**:\n \nAdjust bounding box to make sure the best keypoints are detected.","951f2a2e":"**Function for plotting multiple images**","57cf1388":"**Plot the results**","54b337cd":"**Save the output**","39726eb4":"**Function for placing a bounding box on a white background**","db045604":"**Place the detected bounding boxes on the original image**"}}