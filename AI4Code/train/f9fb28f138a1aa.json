{"cell_type":{"f31e2f25":"code","057ab383":"code","6fcfc72f":"code","0c3f9009":"code","b1568257":"code","378f7a0a":"code","63430d5c":"code","ad0b2706":"code","7d7a16cb":"code","a95f54af":"code","cd2b9514":"code","72445e5a":"code","4664a8d5":"code","ebae22bd":"code","790c355c":"code","46f47a84":"code","89968c1a":"code","e2c8bacf":"code","d9240d51":"markdown","3a3b8740":"markdown","82e699e9":"markdown","ae1fc5ee":"markdown","7dc294ed":"markdown","cb1b0557":"markdown","3e86cfae":"markdown","6e9dc72d":"markdown","a9dfed4a":"markdown","cdcd7a65":"markdown","74e2e572":"markdown","bd3d0ea1":"markdown","cf655298":"markdown","7c14852b":"markdown","e1f1cccd":"markdown","2ba50f87":"markdown","7e338948":"markdown","deea9080":"markdown","c7197e24":"markdown","6162529c":"markdown"},"source":{"f31e2f25":"import numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt\n","057ab383":"dfTrain = pd.read_csv('..\/input\/Train.csv')   #Training Dataset\ndfTest = pd.read_csv('..\/input\/Test.csv')   #Test Dataset\ndfValid = pd.read_csv('..\/input\/Valid.csv') #Validation Dataset\ndfTrain.head()","6fcfc72f":"dfTrain.plot(x='X',y='Y',kind='scatter')","0c3f9009":"def extractFeatures(df):\n    df_Features=df.iloc[:,0:1]\n    df_Label=df.iloc[:,1:2]\n    X=df_Features.values\n    Y=df_Label.values\n    return X,Y","b1568257":"X,Y=extractFeatures(dfTrain)","378f7a0a":"SMean=np.mean(X)\nSDev=np.std(X)\ndef NormalizeInput(X,SMean,SDev):   \n    XNorm=(X-SMean)\/SDev\n    return XNorm","63430d5c":"XNorm=NormalizeInput(X,SMean,SDev)","ad0b2706":"def mapFeature(X,degree):\n    \n    sz=X.shape[1]\n    if (sz==2):\n        sz=(degree+1)*(degree+2)\/2\n        sz=int(sz)\n    else:\n         sz=degree+1\n    out=np.ones((X.shape[0],sz))     #Adding Bias W0\n\n    sz=X.shape[1]\n    if (sz==2):\n        X1=X[:, 0:1]\n        X2=X[:, 1:2]\n        col=1\n        for i in range(1,degree+1):        \n            for j in range(0,i+1):\n                out[:,col:col+1]= np.multiply(np.power(X1,i-j),np.power(X2,j))    \n                col+=1\n        return out\n    else:\n        for i in range(1,degree+1):        \n            out[:,i:i+1]= np.power(X,i)\n    \n    return out","7d7a16cb":"degree=8\ninputX=mapFeature(XNorm,degree)  ","a95f54af":"batchSize=len(Y)         #no of Examples\niterations = 1000\nalpha = 0.01\nfeatureCount=inputX.shape[1] \nweights=np.zeros((featureCount, 1)) #initialize Weight Paramters\nlossList=np.zeros((iterations,1),dtype=float)  #for plotting loss curve","cd2b9514":"\nfor k in range(iterations):\n    #Hypothesis\n    hypothesis=np.matmul( inputX,weights)           \n    \n    #Loss\n    loss=hypothesis-Y  \n    \n    \n    # derivative\n    dW=np.matmul(inputX.T,loss)  #Derivative\n   \n    #gradient Update\n    weights=weights - (alpha\/batchSize)*dW              \n    \n    #Compute Loss for Plotting\n    newLoss=np.matmul( inputX,weights)-Y\n    newLossSqr=np.multiply(newLoss,newLoss)\n    lossList[k]=(1.0\/(2.0*batchSize))* np.sum(newLossSqr)\n","72445e5a":"plt.plot(lossList,color='r')","4664a8d5":"def predict(X,weights,SMean,SDev,degree):\n    XNorm=NormalizeInput(X,SMean,SDev)\n    inputX=mapFeature(XNorm,degree)\n    PY=np.matmul(inputX, weights)\n    return PY","ebae22bd":"def getRMSE(aY,pY):\n    Error=aY- pY\n    ErrorSqr=Error**2\n    MSE=ErrorSqr.mean()\n    RMSE=np.sqrt(MSE)\n    return RMSE","790c355c":"X,Y=extractFeatures(dfTrain)\npY=predict(X, weights,SMean,SDev,degree)  # Predict with bias feature added\nprint(getRMSE(Y, pY))","46f47a84":"vX,vY=extractFeatures(dfValid)\npY=predict(vX, weights,SMean,SDev,degree)  # Predict with bias feature added\nprint(getRMSE(vY, pY))","89968c1a":"tX,tY=extractFeatures(dfTest)\npY=predict(tX, weights,SMean,SDev,degree)  # Predict with bias feature added\nprint(getRMSE(tY, pY))","e2c8bacf":"x_min, x_max = X[:, 0].min() - 5, X[:, 0].max() + 5\ncurveX = np.linspace(x_min, x_max, 100)\ncurveX.shape=(len(curveX),1) \ncurveY=predict(curveX, weights,SMean,SDev,degree)  # Predict with bias feature added\nplt.scatter(X,Y)\nplt.plot(curveX, curveY,color='r')","d9240d51":"<h5> Initialization","3a3b8740":"<h5>Add Polynomial Features\n    <\/h2>","82e699e9":"[](http:\/\/)<h2> Polynomial Hypothesis<\/h2>\n\n<p>\n$h(x) = w_0 + w_1 x + w_2 x^2 + w_3 x^3 + w_4 x^4 + w_5 x^5 + w_6 x^6 + w_7 x^7 + w_8 x^8$\n<\/p> ","ae1fc5ee":"<h2>Extract Input Feature to <b>X <\/b>and Label to <b>y<\/b>","7dc294ed":"<H1>Read Data from CSV","cb1b0557":"<h5> RMSE on Validation Data","3e86cfae":"<h1>Plot Hypothesis","6e9dc72d":"We minimize Loss by taking the derivative (the tangential line to a function) of our cost\/loss function. The slope of the tangent is the derivative at that point and it will give us a direction to move towards. We make steps down the cost\/loss function in the direction with the steepest descent. The size of each step is determined by the parameter \u03b1($alpha$), which is called the learning rate. The direction in which the step is taken is determined by the partial derivative of $L(w_0,w_1,..,w_n)$. \n\nThe gradient descent algorithm is:\n\nrepeat until convergence:<p>\n{<p>\n&nbsp;&nbsp;    $w_0 := w_0 - \\alpha \\frac{\\partial}{\\partial w_0} L(w_0, w_1,....,w_n) $<p>\n&nbsp;&nbsp;    $w_1 := w_1 - \\alpha \\frac{\\partial}{\\partial w_1} L(w_0, w_1,....,w_n) $<p>\n&nbsp;&nbsp;    ......<p>\n&nbsp;&nbsp;    $w_n := w_n - \\alpha \\frac{\\partial}{\\partial w_n} L(w_0, w_n,....,w_n) $<p>\n}\n","a9dfed4a":"<H5>Improve RMSE by adding polynomial features and Normalizing Input Data<\/h5>\n    <p> Try without Normalization, you will find it difficult to train and converge.","cdcd7a65":"<h1> Training","74e2e572":"<h5>RMSE on Training Data","bd3d0ea1":"<h5> Gradient Descent Updates","cf655298":"<h1>Polynomial Regression<\/h1>\n<p>\nTrying to fit 2D curve to data using Gradient Descent Algorithm\n    ","7c14852b":"<h5> Visualize Data","e1f1cccd":"<h1>Plot Loss Curve","2ba50f87":"<h2>Cost\/Loss Function<\/h2>\nWe can measure the accuracy of our hypothesis function by using a cost function. This takes an average difference (actually a fancier version of an average) of all the results of the hypothesis with inputs from x's and the actual output y's.\n\n$L(W) = \\dfrac {1}{2n} \\displaystyle \\sum _{i=1}^n \\left ( \\hat{Y}_{i}- Y_{i} \\right)^2$\n<p>\n$L(w_0, w_1,...w_n)  = \\dfrac {1}{2n} \\displaystyle \\sum _{i=1}^n \\left (h(x_{i}) - y_{i} \\right)^2$\n\nThis Loss\/cost function is also called the \"Squared error function\", or \"Mean squared error\". The mean is halved $\\left(\\frac{1}{2}\\right)$as a convenience for the computation of the gradient descent, as the derivative term of the square function will cancel out the $\\frac{1}{2}$  term.","7e338948":"<h1> Prediction\/RMSE Evaluation","deea9080":"<h3> Gradient Descent Algorithm <\/h3>\n<p>\nWe start with assumpution equation of degree 8 (Called hypothesis) which can fit above data points.   \n<p>\n$h(x) = w_0 + w_1 x + w_2 x^2 + w_3 x^3 + w_4 x^4 + w_5 x^5 + w_6 x^6 + w_7 x^7 + w_8 x^8$\n<\/p> \nThe two coefficients with initial guess (i.e. $w_0$, $w_1$...$w_n$) of $h(x)$ will be fed into the algorithm.\nThen Program will start from initial guess and then iterate steps to find the best fit.\n\n<p>\n Our objective is to minimize Loss.\n    <p>\n $ L(W)=   \\hat{Y}-Y$  Where  $\\hat{Y}=h(X)$\n <\/p>\nSince Loss can negative or postive, we need to minimize the absolute values ( OR Mean squared) Loss so we define Loss\/Cost function as follows","c7197e24":"<h5> RMSE on Test Data","6162529c":"<h5>Normalize Input\n    <\/h2>\n    "}}