{"cell_type":{"6dad7ea9":"code","d6d428f1":"code","9b9b38e8":"code","39e15b21":"code","3a0a9532":"code","df0f0bb1":"code","330a32d4":"code","c12ec96a":"code","59eb7c8f":"code","0c3bd9ca":"code","7cc96fa7":"code","65a7b7b2":"code","c58dbae3":"code","ad35f1e0":"code","56614e29":"code","7440e7c7":"code","094d82f4":"code","95350623":"code","29dcdfec":"code","c94c945b":"code","9872be45":"code","2a8778eb":"code","a1033208":"markdown","868c4372":"markdown","3ed9d53a":"markdown","405d9a62":"markdown","544f9a7f":"markdown","1c1b5aec":"markdown","c232a3f6":"markdown","eec4d3f7":"markdown","e670f44b":"markdown","0832fb77":"markdown"},"source":{"6dad7ea9":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n%matplotlib inline\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n!pip install tensorflow==2.0.0-alpha0\nimport tensorflow as tf\n\nimport matplotlib.pyplot as plt\nfrom matplotlib import cm\nfrom pandas.plotting import scatter_matrix\nimport seaborn as sns\nfrom mpl_toolkits.axes_grid1 import make_axes_locatable\nimport random\n\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.preprocessing import OneHotEncoder, LabelEncoder, scale\nfrom sklearn.model_selection import train_test_split, StratifiedKFold\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.dummy import DummyClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","d6d428f1":"music_df = pd.read_csv('..\/input\/data.csv')\nmusic_df.head(3)","9b9b38e8":"features = list(music_df.columns)\nfeatures.remove('filename')\nfeatures.remove('label')\nprint(features)\n\nlabeled_groups = music_df.groupby('label')\nlabels = list(music_df['label'].unique())\n\n# Group class labels by median value of feature\nfor feat in features:\n    feat_groups = labeled_groups[feat]\n    feat_med_by_group = [(group[0], group[1].median()) for group in list(feat_groups)]\n    feat_med_by_group = sorted(feat_med_by_group, key=lambda x: x[1])\n    feat_labels_ordered_by_median, ordered_medians = zip(*feat_med_by_group)\n","39e15b21":"# Standardize data\nmusic_features_df = music_df[features]\nprint(music_features_df.head(3))\nmusic_features_norm_df = pd.DataFrame(scale(music_features_df))\nprint(music_features_norm_df.head(3))","3a0a9532":"# Encode the labels for genre\nle = LabelEncoder()\nnew_labels = pd.DataFrame(le.fit_transform(music_df['label']))\nmusic_df['label'] = new_labels\nprint(music_df.head(3))","df0f0bb1":"# Put the data together with the encoded labels\n# We start with the standardized data\nmodel_ready_df = music_features_norm_df.copy()\nmodel_ready_df['label'] = music_df['label']\n","330a32d4":"# Splits the data into 10 different folds, each containing the whole set\n# The folds contain two parts:\n# index:0 the larger (9\/10's) piece\n# index:1 the smaller (1\/10's) piece\nfolds = 10\nrandom_state = random_state = random.randint(1, 65536)\ncv = StratifiedKFold(n_splits=folds,\n                     shuffle=True,\n                     random_state=random_state,\n                     )\n\ndata = list(cv.split(music_features_df, music_df['label']))","c12ec96a":"# This was an idea, but it's very ineffecient as you load all the fold data\n# into program memory, which is bascally a n x data array\ndef generate_data_from_fold_indices(data):\n    folds = []\n    for i, indices in enumerate(data):\n        train_index, test_index = indices\n        train_data = model_ready_df.iloc[train_index]\n        train_labels = model_ready_df['label'].iloc[train_index]\n        test_data = model_ready_df.iloc[test_index]\n        test_labels = model_ready_df['label'].iloc[test_index]\n        full_data = (train_data, train_labels, test_data, test_labels)\n        folds.append(full_data)\n    return folds\n    ","59eb7c8f":"print(tf.__version__)\nwith tf.Graph().as_default() as g:\n    a = tf.constant(3.0)\n    b = tf.constant(4.0)\n    total = a + b\n    print(a)\n    print(b)\n    print(total)","0c3bd9ca":"from tensorflow import keras","7cc96fa7":"model = keras.Sequential([\n    keras.layers.Dense(28, activation='relu'),\n    keras.layers.Dense(19, activation='relu'),\n    keras.layers.Dense(10, activation='softmax')\n])\n","65a7b7b2":"model.compile(optimizer='adam',\n              loss='sparse_categorical_crossentropy',\n              metrics=['accuracy'])\n","c58dbae3":"# Let's start with the first fold of data just to see that everything works\nfirst_fold = data[0]\ntrain_indices, test_indices = first_fold[0], first_fold[1]\ntrain_data = music_features_norm_df.iloc[train_indices]\ntrain_labels = music_df['label'].iloc[train_indices]\ntest_data = music_features_norm_df.iloc[test_indices]\ntest_labels = music_df['label'].iloc[test_indices]","ad35f1e0":"history = model.fit(train_data.values, train_labels.values, epochs=150)","56614e29":"test_loss, test_acc = model.evaluate(test_data.values, test_labels.values)\n\nprint('\\nTest accuracy:', test_acc)","7440e7c7":"# list all data in history\nprint(history.history.keys())\n# summarize history for accuracy\nplt.plot(history.history['accuracy'])\nplt.title('model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train'], loc='upper left')\nplt.show()\n# summarize history for loss\nplt.plot(history.history['loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train'], loc='upper left')\nplt.show()","094d82f4":"fig, axs = plt.subplots(2,1, figsize=(12,9), constrained_layout=True)\n\nfor i, fold_ind in enumerate(data[:]):\n    print('Training on fold {} ...'.format(i))\n    train_indices, test_indices = fold_ind[0], fold_ind[1]\n    train_data = music_features_norm_df.iloc[train_indices]\n    train_labels = music_df['label'].iloc[train_indices]\n    test_data = music_features_norm_df.iloc[test_indices]\n    test_labels = music_df['label'].iloc[test_indices]\n    \n    model = keras.Sequential([\n        keras.layers.Dense(28, activation='relu'),\n        keras.layers.Dense(19, activation='relu'),\n        keras.layers.Dense(10, activation='softmax')\n    ])\n\n    model.compile(optimizer='adam',\n                  loss='sparse_categorical_crossentropy',\n                  metrics=['accuracy'])\n    \n    history = model.fit(train_data.values,\n                        train_labels.values,\n                        epochs=100,\n                        batch_size=16,\n                        validation_data=(test_data.values, test_labels.values),\n                        verbose=0\n                       )\n    \n    # summarize history for accuracy\n    axs[0].plot(history.history['accuracy'], label='acc_'+str(i))\n    axs[0].plot(history.history['val_accuracy'], label='val_acc_'+str(i))\n\n    # summarize history for loss\n    axs[1].plot(history.history['loss'], label='loss_'+str(i))\n    axs[1].plot(history.history['val_loss'], label='val_loss_'+str(i))\n\naxs[0].set_title('model accuracy')\naxs[0].set_ylabel('accuracy')\naxs[0].set_xlabel('epoch')\naxs[0].grid(True, which='major')\n# axs[0].legend(loc='upper left')\n\naxs[1].set_title('model loss')\naxs[1].set_ylabel('loss')\naxs[1].set_xlabel('epoch')\naxs[1].grid(True, which='major')\n# axs[1].legend(loc='upper left')\n","95350623":"from tensorflow.keras import layers","29dcdfec":"fig, axs = plt.subplots(2,1, figsize=(12,9), constrained_layout=True)\n\nfor i, fold_ind in enumerate(data[:]):\n    print('Training on fold {} ...'.format(i))\n    train_indices, test_indices = fold_ind[0], fold_ind[1]\n    train_data = music_features_norm_df.iloc[train_indices]\n    train_labels = music_df['label'].iloc[train_indices]\n    test_data = music_features_norm_df.iloc[test_indices]\n    test_labels = music_df['label'].iloc[test_indices]\n    \n    model = keras.Sequential([\n        keras.layers.Dense(28, activation='relu'),\n        keras.layers.Dropout(0.2),\n        keras.layers.Dense(19, activation='relu'),\n        keras.layers.Dropout(0.2),\n        keras.layers.Dense(10, activation='softmax')\n    ])\n\n    model.compile(optimizer='adam',\n                  loss='sparse_categorical_crossentropy',\n                  metrics=['accuracy'])\n    \n    history = model.fit(train_data.values,\n                        train_labels.values,\n                        epochs=100,\n                        batch_size=16,\n                        validation_data=(test_data.values, test_labels.values),\n                        verbose=0\n                       )\n    \n    # summarize history for accuracy\n    axs[0].plot(history.history['accuracy'], label='acc_'+str(i))\n    axs[0].plot(history.history['val_accuracy'], label='val_acc_'+str(i))\n\n    # summarize history for loss\n    axs[1].plot(history.history['loss'], label='loss_'+str(i))\n    axs[1].plot(history.history['val_loss'], label='val_loss_'+str(i))\n\naxs[0].set_title('model accuracy')\naxs[0].set_ylabel('accuracy')\naxs[0].set_xlabel('epoch')\naxs[0].grid(True, which='major')\n# axs[0].legend(loc='upper left')\n\naxs[1].set_title('model loss')\naxs[1].set_ylabel('loss')\naxs[1].set_xlabel('epoch')\naxs[1].grid(True, which='major')\n# axs[1].legend(loc='upper left')\n","c94c945b":"fig, axs = plt.subplots(2,1, figsize=(12,9), constrained_layout=True)\n\nfor i, fold_ind in enumerate(data[:]):\n    print('Training on fold {} ...'.format(i))\n    train_indices, test_indices = fold_ind[0], fold_ind[1]\n    train_data = music_features_norm_df.iloc[train_indices]\n    train_labels = music_df['label'].iloc[train_indices]\n    test_data = music_features_norm_df.iloc[test_indices]\n    test_labels = music_df['label'].iloc[test_indices]\n    \n    model = keras.Sequential([\n        keras.layers.Dense(28, activation='relu'),\n        keras.layers.Dropout(0.2),\n        keras.layers.Dense(19, activation='relu'),\n        keras.layers.Dropout(0.2),\n        keras.layers.Dense(10, activation='softmax')\n    ])\n\n    model.compile(optimizer='adam',\n                  loss='sparse_categorical_crossentropy',\n                  metrics=['accuracy'])\n    \n    history = model.fit(train_data.values,\n                        train_labels.values,\n                        epochs=500,\n                        batch_size=16,\n                        validation_data=(test_data.values, test_labels.values),\n                        verbose=0\n                       )\n    \n    # summarize history for accuracy\n    axs[0].plot(history.history['accuracy'], label='acc_'+str(i))\n    axs[0].plot(history.history['val_accuracy'], label='val_acc_'+str(i))\n\n    # summarize history for loss\n    axs[1].plot(history.history['loss'], label='loss_'+str(i))\n    axs[1].plot(history.history['val_loss'], label='val_loss_'+str(i))\n\naxs[0].set_title('model accuracy')\naxs[0].set_ylabel('accuracy')\naxs[0].set_xlabel('epoch')\naxs[0].grid(True, which='major')\n# axs[0].legend(loc='upper left')\n\naxs[1].set_title('model loss')\naxs[1].set_ylabel('loss')\naxs[1].set_xlabel('epoch')\naxs[1].grid(True, which='major')\n# axs[1].legend(loc='upper left')\n","9872be45":"%load_ext tensorboard.notebook","2a8778eb":"# Start TENSORBOARD\n%tensorboard --logdir logs","a1033208":"### Let's try to use regularization & dropout and see if that gives us even better results.","868c4372":"### The violin plots in the Music EDA notebook showed that the distributions of data did not resemble a gaussian for most features\/classes. Nonetheless, we standardize and use this as an assumption of our analysis.","3ed9d53a":"### Let's start with a simple Keras model.","405d9a62":"### Testing Tensorflow download and import in Kaggle:","544f9a7f":"### Below is an attempt at tensorboard integration, ignore for now.","1c1b5aec":"### A notebook exploring Deep Learning on Musical genre data","c232a3f6":"### The above plots have no legend, but the labels are somewhat obvious. The tighter bunches correspond to training data, the looser to validation. As you can see, the folds generalize well with 30-50 epochs. Some folds generlize better with more training.","eec4d3f7":"### Train the model on the k folds. See what kinds of profiles the loss and accuracy curves take","e670f44b":"### Poor generalization shown above. Training for 100 epochs is currently the best case.","0832fb77":"### Train the network for a bit longer... It looks like we may be able to eek more performance out, while regularizing. "}}