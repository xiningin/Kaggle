{"cell_type":{"9723fb02":"code","73fa0d4f":"code","5a7a0a00":"code","c92b25a8":"code","3a4a2b7c":"code","12563ab9":"code","356c2062":"code","78cab7a7":"code","4f7791e1":"code","235a8c17":"code","6b73badd":"code","19760775":"code","af280b85":"code","27a2e51f":"code","235c24a5":"code","50c5e8c8":"code","651d0535":"code","3ba69cfd":"code","ef91a818":"code","e4cbff58":"code","3c4035f5":"code","71875758":"code","82838337":"code","6558a5a2":"code","6873edb5":"code","f31ef9f2":"code","aa2adb82":"code","89f7305a":"code","d9ecf0da":"code","4451a67c":"code","9d571914":"code","575c7acc":"code","d0948e48":"code","e85fc906":"code","f231e0a6":"code","4b02c287":"code","388590d9":"code","00d6de54":"code","0a653b40":"code","eed12b0f":"code","e77fc61d":"code","a9c557f8":"code","28b97996":"code","a07e741a":"code","9714162b":"code","2b0446b2":"code","ecd05ce5":"markdown","ad1c2ab8":"markdown","a3447bce":"markdown","e2e5c810":"markdown","c8d86a8d":"markdown","c6989b4c":"markdown","c38f1f58":"markdown","4457b953":"markdown","40dfd0a6":"markdown","801c9f2d":"markdown","cdaaca9f":"markdown","0f909956":"markdown","dcedddb4":"markdown","fc0f2062":"markdown","9fe0e3a7":"markdown","cb220643":"markdown","f64eb9af":"markdown","85781806":"markdown","f3143c58":"markdown","526a9be0":"markdown","6414f0af":"markdown","8b6a53ca":"markdown","a7e02c48":"markdown","d0c8288d":"markdown","a2575f66":"markdown","037533dc":"markdown","3fc0828a":"markdown","cee7eabd":"markdown","87010e71":"markdown","b3dd1c64":"markdown","7c573372":"markdown","ffb57d63":"markdown","0b76a64e":"markdown","532cc921":"markdown","42066133":"markdown","07714356":"markdown","a3556bdd":"markdown","f059e553":"markdown","4d3f3143":"markdown","c1e02136":"markdown","943881f0":"markdown","659bd0ed":"markdown","3f5d5434":"markdown","79c78d5f":"markdown","1e5cb3a3":"markdown"},"source":{"9723fb02":"from warnings import filterwarnings\nfilterwarnings(\"ignore\")","73fa0d4f":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib.colors import ListedColormap\nimport seaborn as sns\nimport statsmodels.api as sm\nimport statsmodels.formula.api as smf\nfrom sklearn.linear_model import LogisticRegression,LogisticRegressionCV\nfrom sklearn.model_selection import train_test_split,cross_val_score,cross_val_predict,ShuffleSplit,GridSearchCV\nfrom sklearn.decomposition import PCA\nfrom sklearn import preprocessing\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler,scale, RobustScaler\nfrom sklearn.metrics import confusion_matrix,accuracy_score, roc_auc_score,roc_curve, classification_report,mean_squared_error,f1_score,recall_score,precision_score\nfrom sklearn.neighbors import KNeighborsClassifier\nimport time","5a7a0a00":"pd.set_option('display.max_rows', 1000)\npd.set_option('display.max_columns', 1000)\npd.set_option('display.width', 1000)","c92b25a8":"df = pd.read_csv(\"..\/input\/pima-indians-diabetes-database\/diabetes.csv\")\ndf.head()","3a4a2b7c":"df.shape","12563ab9":"df.describe()","356c2062":"X = df.drop(\"Outcome\",axis=1)\ny= df[\"Outcome\"] #We will predict Outcome(diabetes) ","78cab7a7":"X.shape","4f7791e1":"y.shape","235a8c17":"X_train = X.iloc[:600]\nX_test = X.iloc[600:]\ny_train = y[:600]\ny_test = y[600:]\n\nprint(\"X_train Shape: \",X_train.shape)\nprint(\"X_test Shape: \",X_test.shape)\nprint(\"y_train Shape: \",y_train.shape)\nprint(\"y_test Shape: \",y_test.shape)","6b73badd":"k_nearest_neighbor = KNeighborsClassifier().fit(X_train,y_train)","19760775":"k_nearest_neighbor","af280b85":"# Default neighbor size\nk_nearest_neighbor.n_neighbors","27a2e51f":"# Default distance metric\nk_nearest_neighbor.metric","235c24a5":"# Default p\nk_nearest_neighbor.p","50c5e8c8":"k_nearest_neighbor","651d0535":"y_pred = k_nearest_neighbor.predict(X_test)","3ba69cfd":"cm = confusion_matrix(y_test,y_pred)","ef91a818":"cm","e4cbff58":"print(\"Our Accuracy is: \", (84+33)\/(84+24+27+33))","3c4035f5":"accuracy_score(y_test,y_pred)","71875758":"recall_score(y_test,y_pred)","82838337":"precision_score(y_test,y_pred)","6558a5a2":"f1_score(y_test,y_pred)","6873edb5":"print(classification_report(y_test,y_pred))","f31ef9f2":"k_nearest_neighbor","aa2adb82":"accuracies= cross_val_score(estimator=k_nearest_neighbor,\n                            X=X_train,y=y_train,\n                            cv=10)\nprint(\"Average Accuracy: {:.2f} %\".format(accuracies.mean()*100))\nprint(\"Standart Deviation of Accuracies: {:.2f} %\".format(accuracies.std()*100))","89f7305a":"k_nearest_neighbor.predict(X_test)[:10]","d9ecf0da":"results =pd.DataFrame(k_nearest_neighbor.predict_proba(X_test)[:10],\n             columns=[\"Possibility of 0\",\"Possibility of 1\"])\n\nresults[\"Class\"]=[1 if i>0.5 else 0 for i in results[\"Possibility of 1\"]]","4451a67c":"results.head()","9d571914":"knn_params ={\"n_neighbors\":np.arange(1,70)}","575c7acc":"knn = KNeighborsClassifier()\nk_nearest_neighbor_cv = GridSearchCV(knn,knn_params,cv=8)","d0948e48":"start_time = time.time()\n\nk_nearest_neighbor_cv.fit(X_train,y_train)\n\nelapsed_time = time.time() - start_time\n\nprint(f\"Elapsed time for k_nearest_neighbor_cv cross validation: \"\n      f\"{elapsed_time:.3f} seconds\")","e85fc906":"#best score\nk_nearest_neighbor_cv.best_score_","f231e0a6":"#best parameters\nk_nearest_neighbor_cv.best_params_","4b02c287":"knn_tuned = KNeighborsClassifier(n_neighbors=10).fit(X_train,y_train)","388590d9":"knn_tuned","00d6de54":"y_pred = knn_tuned.predict(X_test)","0a653b40":"cm = confusion_matrix(y_test,y_pred)","eed12b0f":"cm","e77fc61d":"print(\"Our Accuracy is: \", (94+28)\/(94+28+32+14))","a9c557f8":"accuracy_score(y_test,y_pred)","28b97996":"recall_score(y_test,y_pred)","a07e741a":"precision_score(y_test,y_pred)","9714162b":"f1_score(y_test,y_pred)","2b0446b2":"print(classification_report(y_test,y_pred))","ecd05ce5":"## Resources","ad1c2ab8":"As you can see *predict()* function gives us directly classes. If we want to get probabilites of each classes, we can use **predict_proba()** function. We can also give manual treshold to classify.","a3447bce":"## Importing Libraries","e2e5c810":"### Model Tuning & Validation","c8d86a8d":"Precision gives us the answer of this question : \n\n**What proportion of positive identifications was actually correct?**\n\nIt is defined as follows: TP \/ (TP+FP)","c6989b4c":"Now we will tune our model with GridSearch.","c38f1f58":"Some use-cases:\n\n- Mail classification (spam or not)\n\n- Diagnosis of the sicknesses\n\n- Customer buying prediction (if customer will buy or not)","4457b953":"- **The Elements of  Statistical Learning** - Trevor Hastie,  Robert Tibshirani, Jerome Friedman -  Data Mining, Inference, and Prediction (Springer Series in Statistics) \n\n- [**Logistic Regression by Statquest**](https:\/\/www.youtube.com\/watch?v=yIYKR4sgzI8&ab_channel=StatQuestwithJoshStarmer)\n\n- [**The Ultimate Guide to Regression & Classification**](https:\/\/www.superdatascience.com\/blogs\/the-ultimate-guide-to-regression-classification)\n\n- [**Logistic Regression for Machine Learning**](https:\/\/machinelearningmastery.com\/logistic-regression-for-machine-learning\/)\n\n- [**Logistic Regression by Stanford University**](https:\/\/web.stanford.edu\/class\/stats202\/notes\/Classification\/Logistic-regression.html)\n\n- [**What is a Confusion Matrix in Machine Learning?**](https:\/\/machinelearningmastery.com\/confusion-matrix-machine-learning\/)\n\n- [**Classification: Precision and Recall**](https:\/\/developers.google.com\/machine-learning\/crash-course\/classification\/precision-and-recall)\n\n- [**Classification: ROC Curve and AUC**](https:\/\/developers.google.com\/machine-learning\/crash-course\/classification\/roc-and-auc)\n\n- [**AUC-ROC Curve in Machine Learning Clearly Explained**](https:\/\/www.analyticsvidhya.com\/blog\/2020\/06\/auc-roc-curve-machine-learning\/)","40dfd0a6":"For a real world example, we will work with **Pima Indians Diabetes** dataset by UCI Machine Learning as before.\n\nIt can be downloaded [here](https:\/\/www.kaggle.com\/uciml\/pima-indians-diabetes-database).\n\nThis dataset is originally from the National Institute of Diabetes and Digestive and Kidney Diseases. The objective of the dataset is to diagnostically predict whether or not a patient has diabetes, based on certain diagnostic measurements included in the dataset. Several constraints were placed on the selection of these instances from a larger database. In particular, all patients here are females at least 21 years old of Pima Indian heritage.\n\nWe will try to predict whether the patient has diabetes or not.","801c9f2d":"Other popular distance measures include:\n\n- Hamming Distance: Calculate the distance between binary vectors.\n\n- Manhattan Distance: Calculate the distance between real vectors using the sum of their absolute difference. Also called City Block Distance.\n\n- Minkowski Distance: Generalization of Euclidean and Manhattan distance.\n","cdaaca9f":"Recall gives us the answer of this question :\n\n**What proportion of actual positives was identified correctly?**\n\nIt is defined as follows: TP \/ (TP+FN)","0f909956":"The Area Under the Curve (AUC) is the measure of the ability of a classifier to distinguish between classes and is used as a summary of the ROC curve. The higher the AUC, the better the performance of the model at distinguishing between the positive and negative classes.\n\nThis is an example of AUC:\n\n![image.png](attachment:image.png)","dcedddb4":"#### Confusion Matrix","fc0f2062":"### Model","9fe0e3a7":"### Evaluation Metrics","cb220643":"### Theory","f64eb9af":"All hyperparameters can be found [here](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.neighbors.KNeighborsClassifier.html).","85781806":"Classification is the process of finding or discovering a model or function which helps in separating the data into multiple categorical classes i.e. discrete values. In classification, data is categorized under different labels according to some parameters given in input and then the labels are predicted for the data. \nThe derived mapping function could be demonstrated in the form of \u201cIF-THEN\u201d rules. The classification process deal with the problems where the data can be divided into binary or multiple discrete labels. ","f3143c58":"Because we are doing a classification case, we will create a **confusion matrix** in order to evaluate out model.","526a9be0":"- **true positive**: These are cases in which we predicted positive, and they are actually positive.\n- **false positive (Type 1 Error)**: We predicted postive, but they are actually negative. (Also known as a \"Type 1 error.\")\n- **true negative**: We predicted negative, and they are actually negative.\n- **false negative (Type 2 Error)**: We predicted negative, but they are actually postive. (Also known as a \"Type 2 error.\")","6414f0af":"![image.png](attachment:image.png)\n\nPhoto is cited by [here](https:\/\/www.google.com\/url?sa=i&url=https%3A%2F%2Ftowardsdatascience.com%2Fconfusion-matrix-for-your-multi-class-machine-learning-model-ff9aa3bf7826&psig=AOvVaw29atdmY9s4wmI-rc0qQZZb&ust=1628435461495000&source=images&cd=vfe&ved=0CAsQjRxqFwoTCKj1g_2Yn_ICFQAAAAAdAAAAABAD).","8b6a53ca":"Accuracy is one metric for evaluating classification models. Informally, accuracy is **the fraction of predictions our model got right**.\n\nFormally, accuracy has the following definition: All correct predictions \/ all predictions\n\nFor binary classification, accuracy can also be calculated in terms of positives and negatives as follow: (TP+TN) \/ (TP+FP+FN+TN)","a7e02c48":"### Classification","d0c8288d":"Because we are doing a classification case, we will create a **confusion matrix** in order to evaluate our model.  Confusion matrix is a summary of prediction results on a classification problem. The number of correct and incorrect predictions are summarized with count values and broken down by each class. This is the key to the confusion matrix.","a2575f66":"Now we will try to tune our model by using **K-Fold Cross Validation**.","037533dc":"#### Recall","3fc0828a":"If you want to see other algorithms such as:\n\n- Logistic Regression (Theory - Model- Tuning)\n\n- Decision Tree Classification (Theory - Model- Tuning)\n\n- Support Vector Machines(SVC) - Linear Kernel (Theory - Model- Tuning)\n\n- Support Vector Machines(SVC) - Radial Basis Kernel (Theory - Model- Tuning)\n\n- Ensemble Learning - Random Forests Classification (Theory - Model- Tuning)\n\n- Naive Bayes Classification (Theory - Model)\n\n- XGBoost(Extreme Gradient Boosting) Classification (Theory - Model- Tuning)\n\nPlease visit my [Classification tutorial](https:\/\/github.com\/berkayalan\/Data-Science-Tutorials\/blob\/master\/Classification\/Classification.ipynb)","cee7eabd":"#### Precision","87010e71":"#### Accuracy","b3dd1c64":"### Prediction","7c573372":"To compute the points in an ROC curve, we could evaluate a classification model many times with different classification thresholds, but this would be inefficient. Fortunately, there's an efficient, sorting-based algorithm that can provide this information for us, called AUC.","ffb57d63":"#### AUC (Area under Curve)","0b76a64e":"## K - Nearest Neighbors(KNN) ","532cc921":"Now we're going to split our dataset to train and test set. We will choose almost 20% of dataset as test size.","42066133":"#### F1 - Score","07714356":"The F1 score can be interpreted as a harmonic mean of the precision and recall, where an F1 score reaches its best value at 1 and worst score at 0. The relative contribution of precision and recall to the F1 score are equal. \n\nThe formula for the F1 score is: 2 * (precision * recall) \/ (precision + recall)","a3556bdd":"In order to see all rows and columns, we will increase max display numbers of dataframe.","f059e553":"#### ROC Curve (Receiver Operating Characteristic Curve)","4d3f3143":"The k-nearest neighbors (KNN) algorithm is a non-linear supervised machine learning algorithm that can be used to solve both classification and regression problems. The KNN algorithm assumes that similar things exist in close proximity. In other words, similar things are near to each other.\n\nKNN captures the idea of similarity (sometimes called distance, proximity, or closeness) with some mathematics; calculating the distance between points on a graph.\n\nFor real-valued input variables, the most popular distance measure is *Euclidean distance*.\n\nEuclidean distance is calculated as the square root of the sum of the squared differences between a new point (x) and an existing point (xi) across all input attributes j. The formula as below:","c1e02136":"An ROC curve (receiver operating characteristic curve) is a graph showing the performance of a classification model at all classification thresholds. \n\nAn ROC curve plots TP rates vs. FP rares at different classification thresholds. Lowering the classification threshold classifies more items as positive, thus increasing both False Positives and True Positives. The following figure shows a typical ROC curve.\n\n![image.png](attachment:image.png)","943881f0":"![image.png](attachment:image.png)\n\nPhoto is cited by [here](https:\/\/www.google.com\/url?sa=i&url=https%3A%2F%2Fmccormickml.com%2F2013%2F08%2F15%2Fthe-gaussian-kernel%2F&psig=AOvVaw3oFhYy6YidyaXAGxKY20A5&ust=1626817405823000&source=images&cd=vfe&ved=0CAsQjRxqFwoTCPjG0bCN8PECFQAAAAAdAAAAABAD).","659bd0ed":"**Notation**: TP = True Positives, TN = True Negatives, FP = False Positives, and FN = False Negatives.","3f5d5434":"Predictions are made for a new instance by searching through the entire training set for the K most similar instances (the neighbors) and summarizing the output variable for those K instances. For regression this might be the mean output variable, in classification this might be the mode (or most common) class value.","79c78d5f":"## Classification and Evaluation Metrics","1e5cb3a3":"**Created by Berkay Alan**\n\n**Classification | KNN**\n\n**17 January 2022**\n\n**For more Tutorial:** https:\/\/github.com\/berkayalan"}}