{"cell_type":{"6b46f75e":"code","b6403bda":"code","6ab04153":"code","083d793b":"code","9d90f76c":"code","9c0fc3c3":"code","d8033f36":"code","9a9e92f6":"code","ba4e498d":"code","487c6d56":"code","a12bb81f":"code","1d5101ca":"code","dd8fa919":"code","eeb9872d":"code","ab6a3637":"code","c960f034":"code","c7b65ff0":"markdown","41ee0d75":"markdown","f6835169":"markdown","8f48419d":"markdown","c808b49c":"markdown","065a72bc":"markdown","395e6bfa":"markdown","11360843":"markdown","f3f0c92c":"markdown","501de34e":"markdown"},"source":{"6b46f75e":"# Data analysis and wrangling\nimport numpy as np\nimport pandas as pd\n\n# Visualization\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Machine learning\nimport lightgbm as lgb\nfrom sklearn import preprocessing\nfrom sklearn import model_selection\nfrom sklearn.svm import LinearSVC\nfrom sklearn.svm import NuSVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import RandomizedSearchCV\n\n# File handling\nimport os\nprint(os.listdir(\"..\/input\"))","b6403bda":"training_df = pd.read_csv(\"..\/input\/train.csv\")\ntesting_df = pd.read_csv(\"..\/input\/test.csv\")\nsample_submission = pd.read_csv(\"..\/input\/sample_submission.csv\")","6ab04153":"# preview the data\ntraining_df.head()","083d793b":"training_df.describe()","9d90f76c":"training_df.isnull().sum().sum()","9c0fc3c3":"print ('Train set:', training_df.shape)\nprint ('Test set:', testing_df.shape)","d8033f36":"training_df[\"mean_image\"] = training_df.mean(axis=1)\ntesting_df[\"mean_image\"] = testing_df.mean(axis=1)\ntraining_df.head()","9a9e92f6":"features = list(training_df.columns)\nfeatures.remove(\"label\")\n\nfor feature in features:\n    if training_df[feature].mean() == 0 and testing_df[feature].mean() == 0:\n        training_df = training_df.drop([feature], axis=1)\n        testing_df = testing_df.drop([feature], axis=1)\n        \nprint ('Train set:', training_df.shape)\nprint ('Test set:', testing_df.shape)","ba4e498d":"# Memory saving function credit to https:\/\/www.kaggle.com\/gemartin\/load-data-reduce-memory-usage\ndef reduce_mem_usage(df):\n    \"\"\" iterate through all the columns of a dataframe and modify the data type\n        to reduce memory usage.        \n    \"\"\"\n    start_mem = df.memory_usage().sum() \/ 1024**2\n    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n\n    for col in df.columns:\n        col_type = df[col].dtype\n\n        if col_type != object:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n\n    end_mem = df.memory_usage().sum() \/ 1024**2\n    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) \/ start_mem))\n\n    return df","487c6d56":"training_df = reduce_mem_usage(training_df)\nprint('_'*40)\ntesting_df = reduce_mem_usage(testing_df)","a12bb81f":"X_train = np.asarray(training_df.drop(['label'], axis = 1))\nX_train = preprocessing.StandardScaler().fit(X_train).transform(X_train)\n\ny_train = np.asarray(training_df[['label']])\n\nX_test = np.asarray(testing_df)\nX_test = preprocessing.StandardScaler().fit(X_test).transform(X_test)\n\nprint ('Train set:', X_train.shape,y_train.shape)\nprint ('Test set:', X_test.shape)","1d5101ca":"models = [\n    #RandomForestClassifier(n_estimators=1, random_state = 1),\n    #RandomForestClassifier(n_estimators=10, random_state = 1),\n    #RandomForestClassifier(n_estimators=20, random_state = 1),\n    #RandomForestClassifier(n_estimators=30, random_state = 1),\n    #RandomForestClassifier(n_estimators=50, random_state = 1),\n    #RandomForestClassifier(n_estimators=100, random_state = 1),\n    #RandomForestClassifier(n_estimators=200, random_state = 1)\n]\n\nmodel_results = pd.DataFrame(data = {'test_score_mean': [], 'fit_time_mean': [], 'n_estimators': []})\n\n# Spliting the model\ncross_validation_split = model_selection.ShuffleSplit(n_splits = 5, test_size = .3, train_size = .6, random_state = 0 )\n# Performing shufflesplit cross validation, with the whole training set (the cross_validate function coupled with ShuffleSplit take care of spliting the training set) \nfor model in models:\n    cross_validation_results = model_selection.cross_validate(model, X_train, y_train, cv= cross_validation_split, return_train_score=True)    \n    # Checking the mean of test scores for each iteration of the validation    \n    model_results = model_results.append({'test_score_mean' : cross_validation_results['test_score'].mean(), \n                                          'fit_time_mean' : cross_validation_results['fit_time'].mean(), \n                                          'n_estimators' : model.n_estimators}, ignore_index=True) \n \nmodel_results\n#sns.pointplot(x = model_results.n_estimators, y = model_results.test_score_mean)","dd8fa919":"RFC = RandomForestClassifier(n_estimators = 10, oob_score = True, random_state = 1)\nparam_grid = {'min_samples_leaf' : [1, 2, 5], \n              'min_samples_split' : [2 ,5, 10, 20, 40, 60], \n              'max_depth': [20, 30, 40, 50, None]}\n\n#RS = RandomizedSearchCV(estimator = RFC, \n                        param_distributions = param_grid, \n                        n_iter = 50, \n                        cv = cross_validation_split, verbose = 10, random_state = 0, n_jobs = -1)\n\n#RS = RS.fit(X_train, y_train)\n\nprint(RS.best_score_)\nprint(RS.best_params_)","eeb9872d":"param_grid = {'min_samples_leaf' : [1, 2, 3, 5], \n              'min_samples_split' : [3, 5, 7, 10, 12], \n              'max_depth': [35, 40, 45, 50, None]}\n\n#GS = GridSearchCV(estimator = RFC, param_grid = param_grid, cv = cross_validation_split, verbose = 10, n_jobs = -1)\n#GS = GS.fit(X_train, y_train)\n\nprint(GS.best_score_)\nprint(GS.best_params_)","ab6a3637":"best_model = RandomForestClassifier(n_estimators = 200, min_samples_leaf = 1, min_samples_split = 7, \n                                    max_depth = 35, oob_score = True, random_state = 1)","c960f034":"# Predicting the results of the tessting set with the model\nyhat_test = best_model.fit(X_train, y_train).predict(X_test)\n# Submitting\nsubmission = sample_submission.copy()\nsubmission['label'] = yhat_test\nsubmission.to_csv('submission.csv', columns=['ImageId', 'label'], index=False)\n\nsubmission[['ImageId', 'label']].head(15)","c7b65ff0":"# Table of content\n\n## **<div id=\"0\">0. Disclaimer<\/div>**\n\n## **<div id=\"I\">I. Define the problem<\/div>**\n\n### **<div id=\"I1\">1. Problem description<\/div>**\n\nMNIST (\"Modified National Institute of Standards and Technology\") is the de facto \u201chello world\u201d dataset of computer vision. Since its release in 1999, this classic dataset of handwritten images has served as the basis for benchmarking classification algorithms. As new machine learning techniques emerge, MNIST remains a reliable resource for researchers and learners alike.\n\nIn this competition, your goal is to correctly identify digits from a dataset of tens of thousands of handwritten images. We\u2019ve curated a set of tutorial-style kernels which cover everything from regression to neural networks. We encourage you to experiment with different algorithms to learn first-hand what works well and how techniques compare.\n\nThe data files train.csv and test.csv contain gray-scale images of hand-drawn digits, from zero through nine.\n\nEach image is 28 pixels in height and 28 pixels in width, for a total of 784 pixels in total. Each pixel has a single pixel-value associated with it, indicating the lightness or darkness of that pixel, with higher numbers meaning darker. This pixel-value is an integer between 0 and 255, inclusive.\n\nThe training data set, (train.csv), has 785 columns. The first column, called \"label\", is the digit that was drawn by the user. The rest of the columns contain the pixel-values of the associated image.\n\nEach pixel column in the training set has a name like pixelx, where x is an integer between 0 and 783, inclusive. To locate this pixel on the image, suppose that we have decomposed x as x = i * 28 + j, where i and j are integers between 0 and 27, inclusive. Then pixelx is located on row i and column j of a 28 x 28 matrix, (indexing by zero).\n\nFor example, pixel31 indicates the pixel that is in the fourth column from the left, and the second row from the top, as in the ascii-diagram below.\n\nThe test data set, (test.csv), is the same as the training set, except that it does not contain the \"label\" column.\n\nThe evaluation metric for this contest is the categorization accuracy, or the proportion of test images that are correctly classified. For example, a categorization accuracy of 0.97 indicates that you have correctly classified all but 3% of the images.\n\n### **<div id=\"I2\">2. Methodology<\/div>**\n\nThe methodology involved in this machine learning problem go through multiple stages :\n\n* **1. Define the Problem**: If data science, big data, machine learning, predictive analytics, business intelligence, or any other buzzword is the solution, then what is the problem? As the saying goes, don't put the cart before the horse. Problems before requirements, requirements before solutions, solutions before design, and design before technology. Too often we are quick to jump on the new shiny technology, tool, or algorithm before determining the actual problem we are trying to solve.\n* **2. Gather the Data**: John Naisbitt wrote in his 1984 (yes, 1984) book Megatrends, we are \u201cdrowning in data, yet staving for knowledge.\" So, chances are, the dataset(s) already exist somewhere, in some format. It may be external or internal, structured or unstructured, static or streamed, objective or subjective, etc. As the saying goes, you don't have to reinvent the wheel, you just have to know where to find it. In the next step, we will worry about transforming \"dirty data\" to \"clean data.\"\n* **3. Prepare Data for Consumption**: This step is often referred to as data wrangling, a required process to turn \u201cwild\u201d data into \u201cmanageable\u201d data. Data wrangling includes implementing data architectures for storage and processing, developing data governance standards for quality and control, data extraction (i.e. ETL and web scraping), and data cleaning to identify aberrant, missing, or outlier data points.\n* **4. Perform Exploratory Analysis**: Anybody who has ever worked with data knows, garbage-in, garbage-out (GIGO). Therefore, it is important to deploy descriptive and graphical statistics to look for potential problems, patterns, classifications, correlations and comparisons in the dataset. In addition, data categorization (i.e. qualitative vs quantitative) is also important to understand and select the correct hypothesis test or data model.\n* **5. Model Data**: Like descriptive and inferential statistics, data modeling can either summarize the data or predict future outcomes. Your dataset and expected results, will determine the algorithms available for use. It's important to remember, algorithms are tools and not magical wands or silver bullets. You must still be the master craft (wo)man that knows how-to select the right tool for the job. An analogy would be asking someone to hand you a Philip screwdriver, and they hand you a flathead screwdriver or worst a hammer. At best, it shows a complete lack of understanding. At worst, it makes completing the project impossible. The same is true in data modelling. The wrong model can lead to poor performance at best and the wrong conclusion (that\u2019s used as actionable intelligence) at worst.\n* **6. Validate and Implement Data Model**: After you've trained your model based on a subset of your data, it's time to test your model. This helps ensure you haven't overfit your model or made it so specific to the selected subset, that it does not accurately fit another subset from the same dataset. In this step we determine if our model overfit, generalize, or underfit our dataset.\n* **7. Optimize and Strategize**: This is the \"bionic man\" step, where you iterate back through the process to make it better...stronger...faster than it was before. As a data scientist, your strategy should be to outsource developer operations and application plumbing, so you have more time to focus on recommendations and design. Once you're able to package your ideas, this becomes your \u201ccurrency exchange\" rate.\n\n### **<div id=\"I3\">3. Tools importing<\/div>**\n\nHere we are importing every useful tool needed during our research process.","41ee0d75":"## **V. Model data**\n\n### **1. Initial toughts**","f6835169":"# **Digit recognition**","8f48419d":"## **<div id=\"IV\">IV. Wrangle, cleanse and Prepare Data for Consumption<\/div>**\n\n### **1. Saving time and memory with big datasets**\n\nThe size of the dataset is pretty big. Implementing a script to make the dataset smaller without losing information can save us a lot of time. I did not create this script, the credit goes to https:\/\/www.kaggle.com\/gemartin\/load-data-reduce-memory-usage\n\nThis script uses the following approach:\n* Iterate over every column\n* Determine if the column is numeric\n* Determine if the column can be represented by an integer\n* Find the min and the max value\n* Determine and apply the smallest datatype that can fit the range of values\n","c808b49c":"* 0.9362698412698413\n* {'min_samples_split': 5, 'min_samples_leaf': 2, 'max_depth': 40}","065a72bc":"## **<div id=\"III\">III. Perform Exploratory Analysis and visualize the data<\/div>**\n\n### **<div id=\"III1\">1. Descriptive analysis of the data<\/div>**\n\nWe can first display the first rows of the training dataset, in order to have an overview of the parameters :","395e6bfa":"## **<div id=\"II\">II. Gather the data<\/div>**\n\nWe start by acquiring the training and testing datasets into Pandas DataFrames. We also combine these datasets to run certain operations on both datasets together.","11360843":"### **<div id=\"V2\">3. Modeling<\/div>**","f3f0c92c":"* 0.9367063492063492\n* {'max_depth': 35, 'min_samples_leaf': 1, 'min_samples_split': 7}","501de34e":"### **2. Creating matrices for our model**"}}