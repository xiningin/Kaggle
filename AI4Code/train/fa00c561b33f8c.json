{"cell_type":{"b27a5ddb":"code","ee6c1cce":"code","b7f5abb0":"code","a0563fc8":"code","cfac122f":"code","6b8eae29":"code","42bda62a":"code","7404a092":"code","6e319d2e":"code","9e075493":"code","671261ec":"code","4b3ec24f":"code","d46ee15b":"code","93620b95":"code","8bd1fcff":"code","4b1ba536":"code","22c95bb4":"code","2671af0e":"code","3db63455":"code","b1ee5c64":"code","40773632":"code","460e4692":"code","2b01a776":"code","5845384d":"code","d73e6022":"code","a755f614":"code","aeb0788f":"code","b44e3670":"code","81c28fcd":"code","d219f877":"code","71fda186":"code","20aaab00":"code","74a33ab4":"code","1f50448c":"markdown","84b5ff46":"markdown","b672bf22":"markdown","538c37a6":"markdown","926cb165":"markdown","fa59f91d":"markdown","902fec59":"markdown","21209bae":"markdown","af8a3f27":"markdown","e4c983e5":"markdown"},"source":{"b27a5ddb":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport matplotlib.ticker as ticker","ee6c1cce":"file_path = '..\/input\/reddit-wallstreetsbets-posts\/reddit_wsb.csv'","b7f5abb0":"df = pd.read_csv(file_path)\n\ndf.head()","a0563fc8":"df.shape","cfac122f":"body_df = df.dropna(subset=['body'])\nbody_df.reset_index(inplace=True)\nbody_df.drop(['index'], axis=1, inplace=True)\n\nbody_df.head()","6b8eae29":"body_df.shape","42bda62a":"from torchtext.data.utils import get_tokenizer\n\ntokenizer = get_tokenizer('basic_english')\n\ncount_words = lambda x : len(tokenizer(x))\n\nbody_df['body_lenght'] = body_df['body'].map(str).apply(count_words)","7404a092":"plt.hist(body_df.body_lenght, bins=25)\nplt.show()","6e319d2e":"body_df.describe()","9e075493":"import re\n\ndef remove_urls(text):\n    url_pattern = re.compile(r'https?:\/\/\\S+|www\\.\\S+')\n    return url_pattern.sub(r'', text)","671261ec":"remove_spaces = lambda x : re.sub('\\\\s+', ' ', x)","4b3ec24f":"# Reference : https:\/\/gist.github.com\/slowkow\/7a7f61f495e3dbb7e3d767f97bd7304b\ndef remove_emoji(string):\n    emoji_pattern = re.compile(\"[\"\n                               u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                               u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                               u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                               u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                               u\"\\U00002500-\\U00002BEF\"  # chinese char\n                               u\"\\U00002702-\\U000027B0\"\n                               u\"\\U00002702-\\U000027B0\"\n                               u\"\\U000024C2-\\U0001F251\"\n                               u\"\\U0001f926-\\U0001f937\"\n                               u\"\\U00010000-\\U0010ffff\"\n                               u\"\\u2640-\\u2642\"\n                               u\"\\u2600-\\u2B55\"\n                               u\"\\u200d\"\n                               u\"\\u23cf\"\n                               u\"\\u23e9\"\n                               u\"\\u231a\"\n                               u\"\\ufe0f\"  # dingbats\n                               u\"\\u3030\"\n                               \"]+\", flags=re.UNICODE)\n    return emoji_pattern.sub(r'', string)","d46ee15b":"remove_double_quotes = lambda x : x.replace('\"', '')\nremove_single_quotes = lambda x : x.replace('\\'', '')\ntrim = lambda x : x.strip()","93620b95":"other_chars = ['*', '#', '&x200B', '[', ']', '; ',' ;' \"&nbsp\", \"\\\u201c\"]\ndef remove_other_chars(x: str):\n    for char in other_chars:\n        x = x.replace(char, '')\n    \n    return x","8bd1fcff":"funcs = [remove_urls, remove_spaces, remove_emoji, remove_double_quotes, remove_single_quotes, remove_other_chars, trim]\n\nfor fun in funcs:\n    body_df['body'] = body_df['body'].apply(fun)","4b1ba536":"index_names = body_df[body_df['body_lenght'] <= 20].index\n  \nbody_df.drop(index_names, inplace = True)","22c95bb4":"index_names = body_df[body_df['body_lenght'] >= 300].index\n\nbody_df.drop(index_names, inplace = True)","2671af0e":"# reset indexes (again)\nbody_df.reset_index(inplace=True)\nbody_df.drop(['index'], axis=1, inplace=True)\n\nbody_df","3db63455":"body_df.describe()","b1ee5c64":"plt.hist(body_df.body_lenght, bins=25)\nplt.show()","40773632":"body_data = body_df.body.tolist()","460e4692":"SOS_token = \"<sos>\"\nEOS_token = \"<eos>\"","2b01a776":"body_data = [SOS_token + \" \" + body + \" \" + EOS_token for body in body_data]","5845384d":"from collections import Counter\nimport torch\nfrom torchtext.vocab import Vocab","d73e6022":"from collections import Counter\nfrom torchtext.vocab import Vocab\n\ncounter = Counter()\n\nfor body in body_data:\n    counter.update(tokenizer(body))\n\nvocab = Vocab(counter, specials=['<unk>', '<pad>', SOS_token, EOS_token])","a755f614":"from torch.utils.data import Dataset\nimport itertools\n\ndef load_data(data: list, vocab) -> list:\n    return list(itertools.chain(*[[vocab[token] for token in tokenizer(item)] for item in data]))\n\nclass WSBDataset(Dataset):\n\n    def __init__(self, vocab, data, sequence_length):\n        self.vocab = vocab\n        self.sequence_length = sequence_length\n        self.words = load_data(data, vocab)\n  \n    def __len__(self):\n        return len(self.words) - self.sequence_length\n\n    def __getitem__(self, idx):\n        return (\n          torch.tensor(self.words[idx:idx+self.sequence_length]),\n          torch.tensor(self.words[idx+1:idx+self.sequence_length+1]),\n        )","aeb0788f":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")","b44e3670":"import torch.nn as nn\n\nclass Model(nn.Module):\n    \n    def __init__(self):\n        super(Model, self).__init__()\n        self.lstm_size = 256\n        self.embedding_dim = 256\n        self.num_layers = 3\n        n_vocab = len(vocab)\n      \n        self.embedding = nn.Embedding(\n            num_embeddings=n_vocab,\n            embedding_dim=self.embedding_dim,\n        )\n      \n        self.lstm = nn.LSTM(\n            input_size=self.lstm_size,\n            hidden_size=self.lstm_size,\n            num_layers=self.num_layers,\n            dropout=0.2,\n        )\n\n        self.fc = nn.Linear(self.lstm_size, n_vocab)\n    \n    def forward(self, x, prev_state):\n        embed = self.embedding(x)\n        output, state = self.lstm(embed, prev_state)\n        logits = self.fc(output)\n        return logits, state\n\n    def init_state(self, sequence_length):\n        return (\n          torch.zeros(self.num_layers, sequence_length, self.lstm_size, device=device),\n          torch.zeros(self.num_layers, sequence_length, self.lstm_size, device=device),\n          )","81c28fcd":"from torch.utils.data import DataLoader\nfrom torch import optim\n\nlr = 0.001 # learning rate\nmax_epochs = 10\nprint_every = 200\nbatch_size = 256\nsequence_length = 7\nsave_model_path = '.\/wsb_lstm.chkpt'\nall_losses = []","d219f877":"def train(dataset: torch.utils.data.Dataset, model):\n    model.train()\n\n    dataloader = DataLoader(dataset, batch_size, shuffle=True)\n    criterion = nn.CrossEntropyLoss()\n    optimizer = optim.Adam(model.parameters(), lr=lr)\n\n    for epoch in range(max_epochs):\n        state_h, state_c = model.init_state(sequence_length)\n    \n        for batch, (x, y) in enumerate(dataloader):\n            optimizer.zero_grad()\n            x = x.to(device)\n            y = y.to(device)\n\n            y_pred, (state_h, state_c) = model(x, (state_h, state_c))\n            loss = criterion(y_pred.transpose(1, 2), y)\n\n            state_h = state_h.detach()\n            state_c = state_c.detach()\n\n            loss.backward()\n            optimizer.step()\n\n            if batch != 0 and batch % 1000 == 0:\n                torch.save(model.state_dict(), save_model_path)\n\n            all_losses.append(loss.item())\n            \n            if batch % print_every == 0 and batch != 0:    \n                print({ 'epoch': epoch, 'batch': batch, 'loss': sum(all_losses[batch-200:batch]) \/ 200 })\n    \n    torch.save(model.state_dict(), save_model_path)\n    # show losses\n    plt.plot(all_losses)\n    plt.show()","71fda186":"def predict(dataset, model, text, next_words=100):\n    model.eval()\n  \n    words = text.split(' ')\n    state_h, state_c = model.init_state(len(words))\n  \n    for i in range(0, next_words):\n        x = torch.tensor([[dataset.vocab[w] for w in words[i:]]]).to(device)\n        y_pred, (state_h, state_c) = model(x, (state_h, state_c))\n    \n        last_word_logits = y_pred[0][-1]\n        p = torch.nn.functional.softmax(last_word_logits, dim=0).detach().cpu().numpy()\n        word_index = np.random.choice(len(last_word_logits), p=p)\n        words.append(dataset.vocab.itos[word_index])\n  \n    return words","20aaab00":"dataset = WSBDataset(vocab, body_data, sequence_length)\nmodel = Model().to(device)\n\ntrain(dataset, model)","74a33ab4":"print(predict(dataset, model, text='The ceo of nasdaq pushed to halt trading'))","1f50448c":"# Loading the data","84b5ff46":"# Model\n\nSince the text that we are to deal with can be seen as a sequence of tokens, the prefect fit for it are reccurent neural nets. Nevertheless, to avoid the vanishing\/expoloding gradient problem I've choose to use a more complex (and better) version of a vanilla RNN: a [LSTM](https:\/\/en.wikipedia.org\/wiki\/Long_short-term_memory) (Long short term memory).","b672bf22":"# Dataset Creation \n\nTo make a phrase recognizable from the model I'm going to add at the begining of a new post the *\\<sos>* (start of sentence) special token and at the end of a post the *\\<eos>* special token (end of sentence)","538c37a6":"To make our data samplable using the `dataloader` we must create our own Dataset class by extending the `torch.utils.data.Dataset` class and overring the `__len__` method and the `__getitem__` method.","926cb165":"First of all let's drop the rows that have an empty body.","fa59f91d":"Now that we've clean the body of the posts, let's remove all the post that are not contained in the interquartile range (between the 1st quartile and the 3rd quartile).","902fec59":"If you've done something related to NLP, you might know that to feed our model with words we might turn them into numbers, and in order to do that we need a table of vocabulary to map the split tokens to numerical indices. Instead of coding my own PyTorch provides us with a useful library **torchtext** that is equipped with a Vocab class that is going to build a vocab for us!","21209bae":"# Contents\n\n- [Loading the data](#loading-the-data)\n- [Cleaning and preprocessing](#cleaning-&-preprocessing)\n- [Dataset creation](#dataset-creation)\n- [Model](#model)\n- [Training the model](#train-the-model)","af8a3f27":"# Cleaning & Preprocessing\n\nSince in any machine learning task, cleaning or preprocessing the data is as important as model building, in this section I'm going apply basic text preprocessing to the body of the wsb posts such as removing urls, lowercasing the text, remove emoji and emoticons and so on..","e4c983e5":"# Train the model"}}