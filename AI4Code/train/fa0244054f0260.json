{"cell_type":{"aa0aa5f4":"code","f16acd51":"code","b25e051e":"code","552cd91a":"code","a55113a5":"code","ceecdb2a":"code","15e3e7be":"code","86625f63":"code","342f48da":"code","9719621c":"code","161f2e7f":"code","f8b6cc3e":"code","63e1fac0":"code","d30ac520":"code","f1d8b59e":"code","a179ed09":"code","a60dc5c4":"code","29644edd":"code","da001c8a":"code","cc0af92b":"code","b831bb34":"code","7212a669":"code","8167defa":"code","8d9fe0ed":"code","be7672dc":"code","149a0f54":"code","88f0bf93":"code","1d8d0616":"code","02b94eb2":"code","214c2f7e":"code","2271972e":"code","5cbd3236":"code","1c621ba9":"code","9a1de8e9":"code","9738019e":"code","14a0a6a0":"code","9ddca4d4":"code","609b4dcc":"code","fab07e5b":"code","39b0af72":"code","c0a9e551":"code","9accfcf5":"code","8785dde4":"markdown","cda34e4a":"markdown","99162216":"markdown","7d463f44":"markdown","0d557787":"markdown","0684a60a":"markdown","702e14df":"markdown","38daf871":"markdown","398d470f":"markdown","50d94653":"markdown","d2c01712":"markdown","2d8e3cf8":"markdown","e39fefbc":"markdown","149a49bc":"markdown","d19b4db9":"markdown","d6ab8f3a":"markdown","f43fd4c1":"markdown","539771d6":"markdown","7546d21e":"markdown","f83f86ec":"markdown","da702776":"markdown","77709671":"markdown","e1816973":"markdown","951ca251":"markdown","46b34aee":"markdown","760fd0c9":"markdown","10cd4707":"markdown","07b7bf8b":"markdown","f17acbe4":"markdown","a51c2992":"markdown","4badea21":"markdown","d70bb203":"markdown","e3b6b2ce":"markdown","f8befdcc":"markdown","071ed8a2":"markdown"},"source":{"aa0aa5f4":"import pandas as pd\nimport numpy as np\nimport re\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer\nfrom wordcloud import WordCloud\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\n\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler, MinMaxScaler\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.feature_extraction.text import TfidfTransformer, CountVectorizer, TfidfVectorizer\nfrom sklearn.preprocessing import FunctionTransformer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.ensemble import RandomForestClassifier,BaggingClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.calibration import CalibratedClassifierCV\nfrom sklearn.metrics import classification_report, confusion_matrix\nfrom sklearn.pipeline import FeatureUnion, Pipeline\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn import svm\nfrom sklearn.multiclass import OneVsRestClassifier\nfrom sklearn.svm import LinearSVC\n\nfrom imblearn.under_sampling import RandomUnderSampler\nimport eli5\n\nimport IPython\nfrom IPython.display import display\nimport graphviz\nfrom sklearn.tree import export_graphviz\nimport re\n\n\nwarnings.filterwarnings('ignore')\npd.set_option('display.max_rows', 100)","f16acd51":"lines_df = pd.read_csv('..\/input\/movie_lines.tsv', sep='\\t', error_bad_lines=False,\n                       warn_bad_lines=False, header=None)\ncharacters_df = pd.read_csv('..\/input\/movie_characters_metadata.tsv', sep='\\t', warn_bad_lines=False,\n                            error_bad_lines=False, header=None)\n\ncharacters_df.head()","b25e051e":"characters_df.columns=['chId','chName','mId','mName','gender','posCredits']\ncharacters_df.head()","552cd91a":"characters_df.shape","a55113a5":"characters_df.gender.value_counts()","ceecdb2a":"characters_df = characters_df[characters_df.gender != '?']\ncharacters_df.gender = characters_df.gender.apply(lambda g: 0 if g in ['m', 'M'] else 1)  ## Label encoding\n\ncharacters_df.shape","15e3e7be":"characters_df.gender.value_counts()","86625f63":"characters_df.posCredits.value_counts()","342f48da":"characters_df.posCredits = characters_df.posCredits.apply(lambda p: '10+' if not p in ['1', '2', '3', '4', '5', '6', '7', '8', '9'] else p)  ## Label encoding\ncharacters_df.posCredits.value_counts()","9719621c":"lines_df.columns = ['lineId','chId','mId','chName','dialogue']\nlines_df.head()","161f2e7f":"df = pd.merge(lines_df, characters_df, how='inner', on=['chId','mId', 'chName'],\n         left_index=False, right_index=False, sort=True,\n         copy=False, indicator=False)\ndf.head()","f8b6cc3e":"df.shape","63e1fac0":"df = df[df['dialogue'].notnull()]\ndf.shape","d30ac520":"movies = pd.read_csv(\"..\/input\/movie_titles_metadata.tsv\", sep='\\t', error_bad_lines=False,\n                       warn_bad_lines=False, header=None)\nmovies.columns = ['mId','mName','releaseYear','rating','votes','genres']\n\nmovies.head()","f1d8b59e":"movie_yr = movies[['mId', 'releaseYear']]\nmovie_yr.releaseYear = pd.to_numeric(movie_yr.releaseYear.apply(lambda y: str(y)[0:4]), errors='coerce')\nmovie_yr = movie_yr.dropna()\nmovie_yr.releaseYear.value_counts()","a179ed09":"df = pd.merge(df, movie_yr, how='inner', on=['mId'],\n         left_index=False, right_index=False, sort=True,\n         copy=False, indicator=False)\ndf.head()","a60dc5c4":"df['lineLength'] = df.dialogue.str.len()             ## Length of each line by characters\ndf['wordCountLine'] = df.dialogue.str.count(' ') + 1 ## Length of each line by words\ndf.head()","29644edd":"wordnet_lemmatizer = WordNetLemmatizer()\ndef clean_dialogue( dialogue ):\n    # Function to convert a raw review to a string of words\n    # The input is a single string (a raw movie review), and \n    # the output is a single string (a preprocessed movie review)\n    # Source : https:\/\/www.kaggle.com\/akerlyn\/wordcloud-based-on-character\n    #\n    # 1. Remove HTML\n    #\n    # 2. Remove non-letters        \n    letters_only = re.sub(\"[^a-zA-Z]\", \" \", dialogue) \n    #\n    # 3. Convert to lower case, split into individual words\n    words = letters_only.lower().split()                             \n    #\n    # 4. In Python, searching a set is much faster than searching\n    #   a list, so convert the stop words to a set\n    stops = set(stopwords.words(\"english\"))   \n    \n    # 5. Use lemmatization and remove stop words\n    meaningful_words = [wordnet_lemmatizer.lemmatize(w) for w in words if not w in stops]   \n    #\n    # 6. Join the words back into one string separated by space, \n    # and return the result.\n    return( \" \".join( meaningful_words ))\n\ndf['cleaned_dialogue'] = df['dialogue'].apply(clean_dialogue)\ndf[['dialogue','cleaned_dialogue']].sample(5)","da001c8a":"train = df.groupby(['chId', 'mId', 'chName', 'gender', 'posCredits','releaseYear']). \\\n            agg({'lineLength' : ['median'], \n                 'wordCountLine' : ['median'],\n                 'chId' : ['count'],\n                 'cleaned_dialogue' : [lambda x : ' '.join(x)]\n                })\n\n## Renaming columns by aggregate functions\ntrain.columns = [\"_\".join(x) for x in train.columns.ravel()]\n\ntrain.reset_index(inplace=True)\ntrain","cc0af92b":"sns.boxplot(data = train, x = 'gender', y = 'chId_count', hue = 'gender')","b831bb34":"sns.boxplot(data = train, x = 'gender', y = 'wordCountLine_median', hue = 'gender')","7212a669":"sns.boxplot(data = train, x = 'gender', y = 'lineLength_median', hue = 'gender')","8167defa":"sns.scatterplot(data = train, x = 'wordCountLine_median', y = 'chId_count', hue = 'gender', alpha = 0.5) ","8d9fe0ed":"## Separating labels from features\ny = train['gender']\nX = train.copy()\nX.drop('gender', axis=1, inplace=True)\n\n## Removing unnecessary columns\nX.drop('chId', axis=1, inplace=True)\nX.drop('mId', axis=1, inplace=True)\nX.drop('chName', axis=1, inplace=True)\nX.head()","be7672dc":"undersample = RandomUnderSampler(sampling_strategy='majority')\nX_under, y_under = undersample.fit_resample(X, y)\ny_under.value_counts()","149a0f54":"X_train, X_val, y_train, y_val = train_test_split(X_under, y_under, test_size=0.2, random_state = 10, stratify=y_under)\n\ny_val.value_counts()","88f0bf93":"X_val.head()","1d8d0616":"class Converter(BaseEstimator, TransformerMixin):\n    ## Source : https:\/\/www.kaggle.com\/tylersullivan\/classifying-phishing-urls-three-models\n    def fit(self, x, y=None):\n        return self\n\n    def transform(self, data_frame):\n        return data_frame.values.ravel()","02b94eb2":"numeric_features = ['lineLength_median', 'wordCountLine_median', 'chId_count', 'releaseYear']\n\nnumeric_transformer = Pipeline(steps=[('scaler', MinMaxScaler())])","214c2f7e":"categorical_features = ['posCredits']\ncategorical_transformer = Pipeline(steps=[\n    ('onehot', OneHotEncoder(handle_unknown='ignore'))])","2271972e":"vectorizer_features = ['cleaned_dialogue_<lambda>']\nvectorizer_transformer = Pipeline(steps=[\n    ('con', Converter()),\n    ('tf', TfidfVectorizer())])","5cbd3236":"preprocessor = ColumnTransformer(\n    transformers=[\n        ('num', numeric_transformer, numeric_features),\n        ('cat', categorical_transformer, categorical_features),\n        ('vec', vectorizer_transformer, vectorizer_features)\n    ])\n\nsvc_clf = Pipeline(steps=[('preprocessor', preprocessor),\n                      ('classifier', CalibratedClassifierCV(LinearSVC()))])  ## LinearSVC has no predict_proba method\n\nlog_clf = Pipeline(steps=[('preprocessor', preprocessor),\n                      ('classifier', LogisticRegression())])\n\nnb_clf = Pipeline(steps=[('preprocessor', preprocessor),\n                      ('classifier', MultinomialNB())])\n\nrf_clf = Pipeline(steps=[('preprocessor', preprocessor),\n                      ('classifier', RandomForestClassifier(n_estimators=120, min_samples_leaf=10, \n                                                            max_features=0.7, n_jobs=-1, oob_score=True))])","1c621ba9":"svc_clf.fit(X_train, y_train)\nlog_clf.fit(X_train, y_train)\nnb_clf.fit(X_train, y_train)\nrf_clf.fit(X_train, y_train)","9a1de8e9":"def results(name: str, model: BaseEstimator) -> None:\n    '''\n    Custom function to check model performance on validation set\n    '''\n    preds = model.predict(X_val)\n\n    print(name + \" score: %.3f\" % model.score(X_val, y_val))\n    print(classification_report(y_val, preds))\n    labels = ['Male', 'Female']\n\n    conf_matrix = confusion_matrix(y_val, preds)\n    plt.figure(figsize= (10,6))\n    sns.heatmap(conf_matrix, xticklabels=labels, yticklabels=labels, annot=True, fmt=\"d\", cmap='Blues')\n    plt.title(\"Confusion Matrix for \" + name)\n    plt.ylabel('True Class')\n    plt.xlabel('Predicted Class')","9738019e":"results(\"SVC\" , svc_clf)\nresults(\"Logistic Regression\" , log_clf)\nresults(\"Naive Bayes\" , nb_clf)\nresults(\"Random Forest\" , rf_clf)","14a0a6a0":"vect_columns = list(svc_clf.named_steps['preprocessor'].named_transformers_['vec'].named_steps['tf'].get_feature_names())\nonehot_columns = list(svc_clf.named_steps['preprocessor'].named_transformers_['cat'].named_steps['onehot'].get_feature_names(input_features=categorical_features))\nnumeric_features_list = list(numeric_features)\nnumeric_features_list.extend(onehot_columns)\nnumeric_features_list.extend(vect_columns)","9ddca4d4":"lr_weights = eli5.explain_weights_df(log_clf.named_steps['classifier'], top=30, feature_names=numeric_features_list)\nlr_weights.head(15)","609b4dcc":"lr_weights","fab07e5b":"lr_weights.tail(14)","39b0af72":"m = RandomForestClassifier(n_estimators=1, min_samples_leaf=5, max_depth = 3, \n                           oob_score=True, random_state = np.random.seed(123))\ndt_clf = Pipeline(steps=[('preprocessor', preprocessor),\n                         ('classifier', m)])\n\ndt_clf.fit(X_train, y_train)\nresults(\"Decision Tree Classifier\", dt_clf)","c0a9e551":"def draw_tree(t, df, size=10, ratio=0.6, precision=0):\n    \"\"\" \n    Draws a representation of a decition tree in IPython from fastai v0.7\n    \"\"\"\n    s=export_graphviz(t, out_file=None, feature_names=numeric_features_list, filled=True,\n                      special_characters=True, rotate=True, precision=precision, \n                      proportion=True, class_names = [\"male\", \"female\"], impurity = False)\n    IPython.display.display(graphviz.Source(re.sub('Tree {',\n       f'Tree {{ size={size}; ratio={ratio}', s)))\n\ndraw_tree(m.estimators_[0], X_train, precision=2)","9accfcf5":"eli5.explain_weights_df(rf_clf.named_steps['classifier'], top=30, feature_names=numeric_features_list)","8785dde4":"Pipeline for numeric features","cda34e4a":"Pipeline for tokens dereived from dialogues","99162216":"The position of characters in the credits section seems to be a useful feature for classification. We can try to use it as a categorical variable later. But let's combine the low frequency ones together first.","7d463f44":"Let's join lines_df and characters_df together.","0d557787":"While a single decision is a poor classifier with accuracy barely more than 50%, we see that bagging enough of such weak classifiers to form a Random Forest model helps us improve the model performance drastically! Let's look at how the splits are made for a single decision tree.","0684a60a":"### Create training dataset\nNow, we can aggregate all data for a particular movie character into 1 record. We will combine their dialogue tokens from the entire movie, calculate their median dialogue length by characters & words, and count their total no of lines in the movie.","702e14df":"We need to clean this column. Let's also remove the characters were gender information is not available.\nWe'll assign a label of 0 to male characters & 1 to female characters.","38daf871":"### Reading the dataset","398d470f":"### Check results on the validation set","50d94653":"Checking the distribution of gender in the characters dataset","d2c01712":"#### Feature importance for the Random Forest model","2d8e3cf8":"Now, we can combine preprocessing pipelines with the classifers. We will try 4 basic models:\n- Linear Support Vector Classifier\n- Logistic Regression Classifier\n- Naive Bayes Classifier\n- Random Forest Clasifier","e39fefbc":"Remove empty dialogues from the dataset","149a49bc":"We are getting accuracy in the range of 70-75% for most of the models, which is pretty good. \nSome possible ways to improve this performance could be:\n- using bi-grams or tri-grams for dialogue tokens\n- Trying out feed forward neural networks, LSTM or CNN\n- additional feature engineering related to context of dialogues in the movie\n\nStill, our current classifier performance is good enough to understand that there is indeed a gender bias in the characters of Hollywood movies which our models are capturing fairly well.\n\nLet's explore what features contribute the most to our classifiers through some model explainability techniques.","d19b4db9":"We'll also try to keep equal no of male & female records in the train & validation datasets","d6ab8f3a":"### Pipeline for classifiers\n\nSince our dataset includes both numerical features & NLP tokens, we'll use a special converter class in our pipeline.","f43fd4c1":"## Feature importance","539771d6":"Next, let's convert the dialogues into clean tokens \n<ol>\n<li>Remove Stopwords : because they occur very often, but serve no meaning. e.g. : is,am,are,the.<\/li>\n<li>Turn all word to smaller cases : I, i -> i<\/li>\n<li>walk,walks -> walk or geographical,geographic -> geographic<\/li>  #Lemmatization\n<\/ol>","7546d21e":"We see that dialogue keywords like \"oh\", \"love\", \"like\", \"darling\", \"want\", \"honey\" are strong indicators that the character is a female, while keywords like \"sir\", \"man\", \"hell\", \"fuckin\", \"gotta\" & \"yeah\" are usually found in the dialogues of male characters!","f83f86ec":"Fitting the preprocessing & classifier pipelines on training data","da702776":"### Import necessary libraries","77709671":"Adding movie metadata like year of release to the dataset","e1816973":"### Let's check some feature distributions by gender","951ca251":"Adding column names to characters dataframe","46b34aee":"In this notebook, we will try to predict the gender of a Hollywood movie character based in his \/ her dialogues in the movie. The [dataset](https:\/\/www.kaggle.com\/Cornell-University\/movie-dialog-corpus) was released by [Cornell University](http:\/\/www.cs.cornell.edu\/~cristian\/Cornell_Movie-Dialogs_Corpus.html). This classifier will help us understand if there is some kind of gender bias between male and female characters in Hollywood movies.","760fd0c9":"Let's also take a look at the position of the character in the post credits of the movie","10cd4707":"## Feature Engineering\n- Length of lines\n- Count of lines\n- One hot encodings for tokens","07b7bf8b":"Here the blue colored nodes indicate their majority class is \"female\" while the orange colored nodes have a majority of \"male\" labels. The decision tree starts with a mixed sample, but the leaves of the tree are biased towards one class or the other. Most splits seem to be happening using dialogue tokens.","f17acbe4":"Creating a list of all features including numeric & vectorised features","a51c2992":"### Train test split\n\nNow, we can split our data into a training set & a validation set.","4badea21":"We will pick equal no of records for both male & female characters to avoid any kind of bias due to no of records.","d70bb203":"#### Let's also try to visualize a single decision tree\n\nWe can training a single decision tree using the Random Forest Classifier.","e3b6b2ce":"#### Feature importance for Logistic Regression","f8befdcc":"We see that the median length of a dialogue, total no of lines (`chId_count`) & position in the post credits in a movie are important features along with the tokens extracted from the character's dialogues for the Random Forest model!","071ed8a2":"Let's clean the lines dataframe now!"}}