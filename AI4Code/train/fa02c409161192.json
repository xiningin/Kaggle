{"cell_type":{"086f7211":"code","b9cf99b2":"code","fd23285a":"code","fc6b54de":"code","44704b29":"code","a8e8fe98":"code","9ca98b11":"code","422165f2":"code","733139ac":"code","30308a29":"code","c4f03f06":"code","1be6c85f":"code","669f6602":"code","c292bbfa":"code","b552308e":"code","7259c7af":"code","3ed2d88b":"code","abe15116":"code","cfffad26":"code","50dda785":"code","c86a63c3":"code","fc82a879":"code","678818ae":"code","3df1bcaa":"code","601b20d6":"code","e4b3f289":"markdown","ff9ce027":"markdown","477eeb56":"markdown","7ad5e651":"markdown","bedf0196":"markdown","84ae9af5":"markdown","3aba7caf":"markdown","76275b2d":"markdown","7e021b95":"markdown","64d03b14":"markdown","d05b43e6":"markdown","8d36c548":"markdown","9fc1366b":"markdown","05bb4667":"markdown","e4f8d067":"markdown","4d0353b1":"markdown","af14befd":"markdown","cf2d585a":"markdown","151d1d60":"markdown","aecffa46":"markdown","3e97b839":"markdown","7f5a723d":"markdown","bd1e049b":"markdown","6ec24911":"markdown","16491da5":"markdown","66fae7f2":"markdown"},"source":{"086f7211":"import numpy as np\nfrom scipy.special import expit","b9cf99b2":"# 1. Create the NN\nclass Neural_Network:\n    \n    def __init__(self, inputnodes, hiddennodes, outputnodes, learning_rate):\n        \n        self.inodes = inputnodes\n        self.hnodes = hiddennodes\n        self.onodes = outputnodes\n        \n        # Takes random weight between -0.5 and 0.5\n        # Normally in NN one choses uniformly random initial weights\n        #self.weight_input_hidden = ( np.random.rand(self.hnodes, self.inodes) - 0.5 )\n        #self.weight_hidden_output = ( np.random.rand(self.onodes, self.hnodes) - 0.5 )\n        \n        # Research has shown that the NN converges to the \"correct\" weights considerablly faster if one \n        # draws random weigths from a normal (Gaussian) distribution. Google it. Maybe compare. \n        # Sigma is related to the size of the inputs, maybe something to do with Poisson Statistics, find\n        # a paper about it.\n        self.wih = np.random.normal( 0.0, pow(self.inodes, -0.5), (self.hnodes, self.inodes) )\n        self.who = np.random.normal( 0.0, pow(self.inodes, -0.5), (self.onodes, self.hnodes) )\n        \n        # For testing\n        #print( \"Matrix 1: \\n\", self.wih)\n        #print( \"Matrix 2: \\n\", self.who)\n             \n        # Setting the learning rate\n        self.lr = learning_rate\n        \n        # Activation function, we will use the logistic sigmoid function: Give it a Google\n        self.activation_function = lambda x: expit(x)\n        pass\n    \n        print(\"Input Nodes:\", self.inodes, \"Hidden Nodes:\", self.hnodes, \"Output Nodes:\", self.onodes, \"Learning rate:\", self.lr )\n        \n        \n\n# 2. Trains the NN\n    def train(self, inputs_list, targets_list):\n        \n        inputs = np.array( inputs_list, ndmin = 2 ).T\n        targets = np.array( targets_list, ndmin = 2 ).T\n        \n        # Calculate hidden signals\/inputs        \n        # Forward Pass\n        \n        \n        hidden_inputs = np.dot( self.wih, inputs)\n        hidden_outputs = self.activation_function( hidden_inputs )\n        \n        final_inputs = np.dot( self.who, hidden_outputs )\n        final_outputs = self.activation_function( final_inputs )\n        \n        # Calculate errors\n        \n        output_errors = targets - final_outputs\n        \n        # Backward Pass (see Intro.ipynb)\n        \n        hidden_errors = np.dot( self.who.T, output_errors)\n        \n        self.who += self.lr * np.dot( output_errors * final_outputs * (1.0 - final_outputs), np.transpose(hidden_outputs) )\n        \n        self.wih += self.lr * np.dot( hidden_errors * hidden_outputs * (1.0 - hidden_outputs), np.transpose(inputs) )\n        \n        pass\n\n# 3. Tests the NN\n    def test(self, inputs_list):\n        \n        inputs = np.array( inputs_list, ndmin=2 ).T\n        \n        hidden_inputs = np.dot( self.wih, inputs)\n        hidden_outputs = self.activation_function( hidden_inputs )\n        \n        final_inputs = np.dot( self.who, hidden_outputs )\n        final_outputs = self.activation_function( final_inputs )\n        \n        return final_outputs\n    ","fd23285a":"n = Neural_Network(inputnodes=3, hiddennodes=5, outputnodes=2, learning_rate=0.2)","fc6b54de":"n.test( [0.1,0.2,0.5] )","44704b29":"def convert(imgf, labelf, outf, n):\n    f = open(imgf, \"rb\")\n    o = open(outf, \"w\")\n    l = open(labelf, \"rb\")\n\n    f.read(16)\n    l.read(8)\n    images = []\n\n    for i in range(n):\n        image = [ord(l.read(1))]\n        for j in range(28*28):\n            image.append(ord(f.read(1)))\n        images.append(image)\n\n    for image in images:\n        o.write(\",\".join(str(pix) for pix in image)+\"\\n\")\n    f.close()\n    o.close()\n    l.close()\n\nconvert(\"train-images-idx3-ubyte\", \"train-labels-idx1-ubyte\",\n        \"mnist_train.csv\", 60000)\nconvert(\"t10k-images-idx3-ubyte\", \"t10k-labels-idx1-ubyte\",\n        \"mnist_test.csv\", 10000)","a8e8fe98":"import pandas as pd\nimport matplotlib.pyplot as plt\n\ndf_orig_train = pd.read_csv('mnist_train.csv')\ndf_orig_test = pd.read_csv('mnist_test.csv')\n\n\ndf_orig_train.rename(columns={'5':'label'}, inplace=True)\ndf_orig_test.rename(columns={'7':'label'}, inplace=True)\n\ndf_orig_train.to_csv('mnist_train_final.csv', index=False)\ndf_orig_test.to_csv('mnist_test_final.csv', index=False)","9ca98b11":"train_df = pd.read_csv('mnist_train_final.csv')\ntest_df = pd.read_csv('mnist_test_final.csv')\n\n# Converts to NumPy array\ntrain_labels = train_df.iloc[:,0].to_numpy()\ntest_labels = test_df.iloc[:,0].to_numpy()\n\ntrain_digits = train_df.drop('label',axis=1)\ntrain_data = train_digits.to_numpy()\n\ntest_digits = test_df.drop('label',axis=1)\ntest_data = test_digits.to_numpy()","422165f2":"test_labels","733139ac":"test_data","30308a29":"pic = train_data[0]\nlabel = train_labels[0]\nprint( \"Label:\", label, \"Image size:\", pic.shape )\n\n\n%matplotlib inline\nplt.imshow( pic.reshape(28, 28), cmap = 'Greys', interpolation = 'None')","c4f03f06":"n = Neural_Network(inputnodes=784, hiddennodes=200, outputnodes=10, learning_rate=0.1)\nn.who","1be6c85f":"# The epoch is how many times one wants to train the NN with the whole data set.\nepoch = 5\noutput_nodes=10\n\n# Loop through each epoch\nfor e in range(epoch):\n    print('Epoch:', e + 1)\n    \n    for digit, label in zip( train_data, train_labels ):\n        \n        # Normalise the image, avoid big numbers\n        inputs = digit\/255.0 * 0.99 + 0.01\n        \n        # Setting the target\n        targets = np.zeros( output_nodes ) + 0.01\n        targets[int( label )] = 0.99\n        \n        n.train( inputs, targets )\n    ","669f6602":"scorecard = []\n\nfor digit, correct_label in zip( test_data, test_labels):\n    \n    # Normalise the image, avoid big numbers\n    inputs = digit\/255.0 * 0.99 + 0.01\n    \n    # Calculate outputs and the predicted label\n    outputs = n.test( inputs )\n    label = np.argmax( outputs )\n    \n    if ( label == correct_label ):\n        scorecard.append(1)\n    else:\n        scorecard.append(0)\n    \nscorecard_res = np.asarray( scorecard )\nprint( 'Preformance:', round(scorecard_res.sum() \/ scorecard_res.size * 100, 2), '%' )\n    ","c292bbfa":"train_sizes = np.linspace(1, 60000, 20 ).astype(int)\n\nperformance_list = []\n\nfor size in train_sizes:\n    \n    #Initializing a new NN with random weights\n    n = Neural_Network(inputnodes=784, hiddennodes=200, outputnodes=10, learning_rate=0.1)\n    \n    # Trains on a data set of size\n    for digit, label in zip( train_data[:size], train_labels ):\n        \n        \n        # Normalise the image, avoid big numbers\n        inputs = digit\/255.0 * 0.99 + 0.01\n        \n        # Setting the target\n        targets = np.zeros( output_nodes ) + 0.01\n        targets[int( label )] = 0.99\n        \n        n.train( inputs, targets )\n    \n    # Calculate the performance using the testing data set\n    performance = []\n        \n    for digit, correct_label in zip( test_data, test_labels):\n    \n        # Normalise the image, avoid big numbers\n        inputs = digit\/255.0 * 0.99 + 0.01\n    \n        # Calculate outputs and the predicted label\n        outputs = n.test( inputs )\n        label = np.argmax( outputs )\n    \n        if ( label == correct_label ):\n            performance.append(1)\n        else:\n            performance.append(0)\n            \n    # Calculates the performance and adds to the list perform  \n    performance_res = np.asarray( performance )\n    performance_list.append( round(performance_res.sum() \/ performance_res.size * 100, 10) )\n        \n","b552308e":"plt.plot( train_sizes, performance_list)\nplt.grid()\nplt.xlim(0, 60000)\nplt.ylim(0, 100)\nplt.xlabel('Traing set size')\nplt.ylabel('Performance (%)')\nplt.title('The relationship between the size of the traing set against performance.')\nplt.show()","7259c7af":"hidden_nodes = np.linspace(1, 250, 25 ).astype(int)\n\nperformance_list = []\n\nfor nodes in hidden_nodes:\n    \n    #Initializing a new NN with random weights\n    n = Neural_Network(inputnodes=784, hiddennodes=nodes, outputnodes=10, learning_rate=0.1)\n    \n    # Trains on a data set of size\n    for digit, label in zip( train_data[:10000], train_labels ):\n        \n        \n        # Normalise the image, avoid big numbers\n        inputs = digit\/255.0 * 0.99 + 0.01\n        \n        # Setting the target\n        targets = np.zeros( output_nodes ) + 0.01\n        targets[int( label )] = 0.99\n        \n        n.train( inputs, targets )\n    \n    # Calculate the performance using the testing data set\n    performance = []\n        \n    for digit, correct_label in zip( test_data, test_labels):\n    \n        # Normalise the image, avoid big numbers\n        inputs = digit\/255.0 * 0.99 + 0.01\n    \n        # Calculate outputs and the predicted label\n        outputs = n.test( inputs )\n        label = np.argmax( outputs )\n    \n        if ( label == correct_label ):\n            performance.append(1)\n        else:\n            performance.append(0)\n            \n    # Calculates the performance and adds to the list perform  \n    performance_res = np.asarray( performance )\n    performance_list.append( round(performance_res.sum() \/ performance_res.size * 100, 10) )\n        \n\n","3ed2d88b":"plt.plot( hidden_nodes, performance_list)\nplt.grid()\nplt.xlabel('Number of hidden nodes')\nplt.ylabel('Performance (%)')\nplt.title('The relationship between the number of hidden nodes against performance.')\nplt.show()","abe15116":"learning_rates = np.linspace(0.01, 1, 20 )\nperformance_list = []\n\nfor rates in learning_rates:\n    \n    #Initializing a new NN with random weights\n    n = Neural_Network(inputnodes=784, hiddennodes=200, outputnodes=10, learning_rate=rates)\n    \n    # Trains on a data set of size\n    for digit, label in zip( train_data[:10000], train_labels ):\n        \n        \n        # Normalise the image, avoid big numbers\n        inputs = digit\/255.0 * 0.99 + 0.01\n        \n        # Setting the target\n        targets = np.zeros( output_nodes ) + 0.01\n        targets[int( label )] = 0.99\n        \n        n.train( inputs, targets )\n    \n    # Calculate the performance using the testing data set\n    performance = []\n        \n    for digit, correct_label in zip( test_data, test_labels):\n    \n        # Normalise the image, avoid big numbers\n        inputs = digit\/255.0 * 0.99 + 0.01\n    \n        # Calculate outputs and the predicted label\n        outputs = n.test( inputs )\n        label = np.argmax( outputs )\n    \n        if ( label == correct_label ):\n            performance.append(1)\n        else:\n            performance.append(0)\n            \n    # Calculates the performance and adds to the list perform  \n    performance_res = np.asarray( performance )\n    performance_list.append( round(performance_res.sum() \/ performance_res.size * 100, 10) )\n        \n\n\n","cfffad26":"plt.plot( learning_rates, performance_list)\nplt.grid()\nplt.xlabel('Learning rate')\nplt.ylabel('Performance (%)')\nplt.title('The relationship between the learning rate against performance.')\nplt.show()","50dda785":"n = Neural_Network(inputnodes=784, hiddennodes=200, outputnodes=10, learning_rate=0.25)\n\n# The epoch is how many times one wants to train the NN with the whole data set.\nepoch = 10\noutput_nodes=10\n\n# Loop through each epoch\nfor e in range(epoch):\n    print('Epoch:', e + 1)\n    \n    for digit, label in zip( train_data, train_labels ):\n        \n        # Normalise the image, avoid big numbers\n        inputs = digit\/255.0 * 0.99 + 0.01\n        \n        # Setting the target\n        targets = np.zeros( output_nodes ) + 0.01\n        targets[int( label )] = 0.99\n        \n        n.train( inputs, targets )","c86a63c3":"scorecard = []\n\nfor digit, correct_label in zip( test_data, test_labels):\n    \n    # Normalise the image, avoid big numbers\n    inputs = digit\/255.0 * 0.99 + 0.01\n    \n    # Calculate outputs and the predicted label\n    outputs = n.test( inputs )\n    label = np.argmax( outputs )\n    \n    if ( label == correct_label ):\n        scorecard.append(1)\n    else:\n        scorecard.append(0)\n    \nscorecard_res = np.asarray( scorecard )\nprint( 'Preformance:', round(scorecard_res.sum() \/ scorecard_res.size * 100, 2), '%' )","fc82a879":"Kaggle_test = pd.read_csv('test.csv')\ntest_set = Kaggle_test.to_numpy()\n\nKaggle_predictions = []\n\nfor digit in test_set:\n    \n    # Normalise the image, avoid big numbers\n    inputs = digit\/255.0 * 0.99 + 0.01\n    \n    # Calculate outputs and the predicted label\n    outputs = n.test( inputs )\n    Kaggle_predictions.append(np.argmax( outputs ))\n ","678818ae":"result = pd.DataFrame()\nimageId = np.arange(1,28001,1)\nresult[\"ImageId\"] = imageId\nresult[\"Label\"] = Kaggle_predictions","3df1bcaa":"result.to_csv(\"submission.csv\", index = False)","601b20d6":"pd.read_csv(\"submission.csv\")","e4b3f289":"### 1.4.2 How the number of hidden node effects the performance","ff9ce027":"Below we will plot the relationship between number of hidden nodes against performance. From the plot we can see that the NN does extremely badly for smal numbers of hidden nodes but plateaus at around a $100$ nodes where the performance levels out. Note the convergence of the performance to $100\\%$ and that the number of hidden nodes has to be exponetially larger to improve the preformance at the high end.","477eeb56":"### 1.4.3 How the learning rate effects the performance ","7ad5e651":"The code converts the lecun files into csv then Read new csv files. These files are then processed by renaming the columns and then saves them again. This is then the final processed data.","bedf0196":"## 1.2 Dealing with the data","84ae9af5":"This is my first go at creating a neural network (NN) to create a classifier for the MNIST dataset. The MNIST database (Modified National Institute of Standards and Technology database) is a large database of handwritten digits that is commonly used for training various image processing systems. The database is also widely used for training and testing in the field of machine learning.\n\nThe MNIST database contains 60,000 training images and 10,000 testing images.","3aba7caf":"## 1.3 Training using the training data","76275b2d":"In this section, we will see how the learning rate the performance of the NN. The NN will be trained of the same data set for each iteration and the NN will have the same structure (equal number of hidden layers, same activation functions, etc). Then we will see how the learning rate effects the performance. The NN will ne train on the entire MNIST data set.\n\nWe will plot the results, I am not sure what the outcome will be. I predict that for a small learning rate NN will converge slowly and the convergence will improve (convergnce to $100\\%$ performance), there may be a decrease in performance for larger learning rates due to the fit being unstable. The shape of this relationship is not as obvious but this analysis will reveal it. The trade off is that more hidden nodes takes longer to train, I'm guessing here.","7e021b95":"### Convert original lecun files into csv","64d03b14":"Below we will plot the relationship between the size of the traing set against performance. From the plot we can see that the NN does extremely badly for small traing sets but plateaus at around a $10 000$ where the performance levels out. Note the convergence of the performance to $100\\%$ and that the data sets need to be exponetially larger to improve the preformance at the high end.","d05b43e6":"In this section, we will see how the number of hidden nodes effects the performance of the NN. The NN will be trained of the same data set for each iteration and we will vary the number of hidden layers and see how this effects the performance. The NN will ne train on the entire MNIST data set.\n\nWe will plot the results, I am not sure what the outcome will be. I predict that for a small number of hidden nodes the NN will perform badly and will increase until the time complexity is just to large, there may be a decrease due to **over fitting**. The shape of this relationship is not as obvious but this analysis will reveal it. The trade off is that more hidden nodes takes longer to train, I'm guessing here.","8d36c548":"### Importing libraries","9fc1366b":"# The MNIST dataset","05bb4667":"Below we will plot the relationship between number of hidden nodes against performance. From the plot we can see that the NN does extremely badly for smal numbers of hidden nodes but plateaus at around a $100$ nodes where the performance levels out. Note the convergence of the performance to $100\\%$ and that the number of hidden nodes has to be exponetially larger to improve the preformance at the high end.\n\nSeems that my guess was somewhat correct. The performance is extremely bad for a small learning rate and increases sharply to a maximum at around $1.0$ and drops off for larger learning rates.\n\nIf I am not mistaken then this is what is known as *hyperparameter tuning*.","e4f8d067":"### Basic test to see what percentage of the trained data is predicted correctly. ","4d0353b1":"Extracts the labels and the images","af14befd":"Plotting the numbers and their label to make sure that they look correct","cf2d585a":"## 1.4 Testing the Model","151d1d60":"In this section, we will see how the size of the training set effects the performance of the NN. The NN will have a constant structure (equal number of hidden layers, same activation functions, etc) and we will vary the size of the training set and see how this effects the performance. \n\nWe will plot the results, I expect that a larger training set will have a positive effect on the performance. The shape of this relationship is not as obvious but this analysis will reveal it. The trade off is that a larger data set takes longer to train. ","aecffa46":"As seen above, the input has size $28\\cdot28\\cdot1 = 784$ and the output is has size $10$. Therefore we know what the external structure of our NN should be. Let's define out NN.","3e97b839":"# Predictions for Kaggle","7f5a723d":"## 1. First attempt (no libraries) ","bd1e049b":"Here we will test the NN to see if how well it makes predictions. Here we begin with a simple analysis, we will compare the outputs of the trained NN with their 'true values' given by the label. We will calculate what percentage of the test data produces the correct output, we will define this as the performance. I will potentially see if I can calculate the confusion matrix.\n\nAdditionally I see three obvious stratagies which will allow for me to gain useful insight on NN. They are as follows;\n\n1. Test to how the size of the training set effects performance\n2. Test to how the number of hidden nodes effects performance\n3. Test to how the learning rate effects performance (~ Hyperparameter tuning)\n\nThe analysis is below but first we need to define a metric that we can use to quantify the 'performance' of the NN. We will just say that the NN performed well if it was able to predict a large percentage of the test data's labels. The percentage of correct predictions will be define as the performance. ","6ec24911":"### 1.4.1 How the training data's size effects preformance.  ","16491da5":"### Reading in the processed .csv files ","66fae7f2":"## 1.1 Defining the NN"}}