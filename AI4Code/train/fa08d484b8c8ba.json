{"cell_type":{"536d9b7c":"code","3a4d3eaf":"code","008d39b1":"code","ccb36646":"code","9dcd069c":"code","8612a9af":"code","a0385133":"code","1de28351":"code","7d8bfd2b":"code","155665b1":"code","909cf8b1":"code","42dd7b11":"code","81aa0618":"code","56266b68":"code","dcc789c6":"code","bd3dc52e":"code","e9a00bf0":"code","67414715":"code","757e695c":"code","9c1a5e4d":"code","6f8ac867":"code","45277043":"code","46be7343":"code","69f12dea":"code","27c9be91":"code","edcacf5f":"code","20112f6f":"code","5f26909a":"code","66012e08":"code","7f970b9a":"code","036fb50d":"code","26849b1b":"code","ffa49cf8":"code","478aab2f":"code","f74f797a":"code","f8aea313":"code","5100de0b":"code","eb578c3c":"code","774e91fa":"code","f06f07cf":"code","719f13eb":"code","b8fffd4c":"code","45c9e67a":"code","edebec66":"code","ccc77766":"code","3c409f80":"code","853bf9d9":"code","66131578":"code","9843441c":"code","fe974c77":"code","0a3bcf6a":"code","0c3d454a":"code","0b537476":"code","45f2b723":"code","c3cf2667":"code","a4a00ae7":"code","9f69033f":"code","f125e5ee":"code","b2227a7b":"code","44d2a741":"code","513c468d":"code","8192c7e3":"code","ed03fa6f":"code","9f0c4b9e":"markdown","0617930f":"markdown","18e14f72":"markdown","f23f85ec":"markdown","5a7a0af7":"markdown","75e4a94e":"markdown","05b78c65":"markdown","f247dba2":"markdown","cf7cb1ec":"markdown","069da961":"markdown","496c60aa":"markdown","8878534f":"markdown","d8945e2b":"markdown","bdb2d742":"markdown","3d2c19fc":"markdown","20a9e07d":"markdown","e7fd3a56":"markdown"},"source":{"536d9b7c":"#library imports\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport tensorflow as tf\nfrom tensorflow import keras\nimport math\n%matplotlib inline","3a4d3eaf":"#loading data\n(X_train, y_train), (X_test, y_test) = keras.datasets.fashion_mnist.load_data()","008d39b1":"X_train.shape, y_train.shape, X_test.shape, y_test.shape","ccb36646":"X_train.dtype, y_train.dtype, X_test.dtype, y_test.dtype","9dcd069c":"#creating validation data and scaling data to range 0-1\nX_valid, X_train = X_train[:5000] \/ 255., X_train[5000:] \/ 255.\ny_valid, y_train = y_train[:5000], y_train[5000:]\nX_test = X_test \/ 255.","8612a9af":"plt.figure(figsize=(15, 6))\nn_rows = 5\nn_cols = 10\n\nfor i in range(n_rows * n_cols):\n    ax = plt.subplot(n_rows, n_cols, i+1)\n    plt.imshow(X_train[i], cmap='binary')\n    plt.axis('off')\n    \nplt.tight_layout()\nplt.show()","a0385133":"y_train[:50]","1de28351":"class_names = [\"T-shirt\/top\", \"Trouser\", \"Pullover\", \"Dress\", \"Coat\", \n               \"Sandal\", \"Shirt\", \"Sneaker\", \"Bag\", \"Ankle boot\"]","7d8bfd2b":"class_names[y_train[0]]","155665b1":"plt.figure(figsize=(15, 6))\nn_rows = 5\nn_cols = 10\n\nfor i in range(n_rows * n_cols):\n    ax = plt.subplot(n_rows, n_cols, i+1)\n    plt.imshow(X_train[i], cmap='binary')\n    plt.axis('off')\n    plt.title(class_names[y_train[i]], fontdict={'fontsize': 12, 'color': 'black'})\n    \nplt.tight_layout()\nplt.show()","909cf8b1":"#scale inputs to mean 0 and standard deviation 1\npixel_means = X_train.mean(axis=0, keepdims=True)\npixel_stds = X_train.std(axis=0, keepdims=True)\nX_train_scaled = (X_train - pixel_means) \/ pixel_stds\nX_valid_scaled = (X_valid - pixel_means) \/ pixel_stds\nX_test_scaled = (X_test - pixel_means) \/ pixel_stds","42dd7b11":"keras.backend.clear_session()\ntf.random.set_seed(42)\nnp.random.seed(42)","81aa0618":"model = keras.models.Sequential()\nmodel.add(keras.layers.Flatten(input_shape=[28, 28]))\nfor _ in range(20):\n    model.add(keras.layers.Dense(100, activation='elu', \n                                 kernel_initializer='he_normal'))\nmodel.add(keras.layers.Dense(10, activation='softmax'))","56266b68":"model.summary()","dcc789c6":"optimizer = keras.optimizers.Nadam(learning_rate=1e-3)\nmodel.compile(loss='sparse_categorical_crossentropy', optimizer=optimizer, \n              metrics=['accuracy'])","bd3dc52e":"K = keras.backend\n\nclass ExponentialLearningRate(keras.callbacks.Callback):\n    def __init__(self, factor):\n        self.factor = factor\n        self.rates = []\n        self.losses = []\n    def on_batch_end(self, batch, logs):\n        self.rates.append(K.get_value(self.model.optimizer.lr))\n        self.losses.append(logs['loss'])\n        K.set_value(self.model.optimizer.lr, self.model.optimizer.lr * self.factor)\n\ndef find_learning_rate(model, X, y, epochs=1, batch_size=32, min_rate=10**-5, max_rate=10):\n    init_weights = model.get_weights()\n    iterations = math.ceil(len(X) \/ batch_size) * epochs\n    factor = np.exp(np.log(max_rate \/ min_rate) \/ iterations)\n    init_lr = K.get_value(model.optimizer.lr)\n    K.set_value(model.optimizer.lr, min_rate)\n    exp_lr = ExponentialLearningRate(factor)\n    history = model.fit(X, y, epochs=epochs, batch_size=batch_size, callbacks=[exp_lr])\n    K.set_value(model.optimizer.lr, init_lr)\n    model.set_weights(init_weights)\n    return exp_lr.rates, exp_lr.losses\n\ndef plot_lr_vs_loss(rates, losses):\n    plt.plot(rates, losses)\n    plt.gca().set_xscale('log')\n    plt.hlines(min(losses), min(rates), max(rates))\n    plt.axis([min(rates), max(rates), min(losses), (losses[0] + min(losses)) \/ 2])\n    plt.xlabel('Learning rate')\n    plt.ylabel('Loss')","e9a00bf0":"batch_size = 32\nrates, losses = find_learning_rate(model, X_train, y_train, epochs=1, batch_size=batch_size)\nplot_lr_vs_loss(rates, losses)","67414715":"rates[np.argmin(losses)]","757e695c":"optimizer = keras.optimizers.Nadam(learning_rate=8e-4)\nmodel.compile(loss='sparse_categorical_crossentropy', optimizer=optimizer, \n              metrics=['accuracy'])","9c1a5e4d":"early_stopping_cb = keras.callbacks.EarlyStopping(patience=20)\nmodel_checkpoint_cb = keras.callbacks.ModelCheckpoint('my_fashion_mnist_model_v1.h5', \n                                                      save_best_only=True)\ncallbacks = [early_stopping_cb, model_checkpoint_cb]","6f8ac867":"model.fit(X_train, y_train, epochs=100, validation_data=(X_valid, y_valid), \n          callbacks=callbacks)","45277043":"model = keras.models.load_model('my_fashion_mnist_model_v1.h5')\nmodel.evaluate(X_valid, y_valid)","46be7343":"keras.backend.clear_session()\ntf.random.set_seed(42)\nnp.random.seed(42)","69f12dea":"model = keras.models.Sequential()\nmodel.add(keras.layers.Flatten(input_shape=[28, 28]))\nmodel.add(keras.layers.BatchNormalization())\nfor _ in range(20):\n    model.add(keras.layers.Dense(100, kernel_initializer='he_normal'))\n    model.add(keras.layers.BatchNormalization())\n    model.add(keras.layers.Activation('elu'))\nmodel.add(keras.layers.Dense(10, activation='softmax'))","27c9be91":"model.summary()","edcacf5f":"optimizer = keras.optimizers.Nadam(learning_rate=1e-3)\nmodel.compile(loss='sparse_categorical_crossentropy', optimizer=optimizer, \n              metrics=['accuracy'])","20112f6f":"batch_size = 32\nrates, losses = find_learning_rate(model, X_train, y_train, epochs=1, batch_size=batch_size)\nplot_lr_vs_loss(rates, losses)","5f26909a":"rates[np.argmin(losses)]","66012e08":"optimizer = keras.optimizers.Nadam(learning_rate=0.03)\nmodel.compile(loss='sparse_categorical_crossentropy', optimizer=optimizer, \n              metrics=['accuracy'])","7f970b9a":"early_stopping_cb = keras.callbacks.EarlyStopping(patience=20)\nmodel_checkpoint_cb = keras.callbacks.ModelCheckpoint('my_fashion_mnist_model_v2.h5', \n                                                      save_best_only=True)\ncallbacks = [early_stopping_cb, model_checkpoint_cb]","036fb50d":"model.fit(X_train, y_train, epochs=100, validation_data=(X_valid, y_valid), \n          callbacks=callbacks)","26849b1b":"model = keras.models.load_model('my_fashion_mnist_model_v2.h5')\nmodel.evaluate(X_valid, y_valid)","ffa49cf8":"keras.backend.clear_session()\ntf.random.set_seed(42)\nnp.random.seed(42)","478aab2f":"model = keras.models.Sequential()\nmodel.add(keras.layers.Flatten(input_shape=[28, 28]))\nfor _ in range(20):\n    model.add(keras.layers.Dense(100, activation='selu', \n                                 kernel_initializer='lecun_normal'))\nmodel.add(keras.layers.Dense(10, activation='softmax'))","f74f797a":"model.summary()","f8aea313":"optimizer = keras.optimizers.Nadam(learning_rate=1e-3)\nmodel.compile(loss='sparse_categorical_crossentropy', optimizer=optimizer, \n              metrics=['accuracy'])","5100de0b":"batch_size = 32\nrates, losses = find_learning_rate(model, X_train_scaled, y_train, epochs=1, batch_size=batch_size)\nplot_lr_vs_loss(rates, losses)","eb578c3c":"rates[np.argmin(losses)]","774e91fa":"early_stopping_cb = keras.callbacks.EarlyStopping(patience=20)\nmodel_checkpoint_cb = keras.callbacks.ModelCheckpoint('my_fashion_mnist_model_v3.h5', \n                                                      save_best_only=True)\ncallbacks = [early_stopping_cb, model_checkpoint_cb]","f06f07cf":"model.fit(X_train_scaled, y_train, epochs=100, \n          validation_data=(X_valid_scaled, y_valid), \n          callbacks=callbacks)","719f13eb":"model = keras.models.load_model('my_fashion_mnist_model_v3.h5')\nmodel.evaluate(X_valid_scaled, y_valid)","b8fffd4c":"keras.backend.clear_session()\ntf.random.set_seed(42)\nnp.random.seed(42)","45c9e67a":"model = keras.models.Sequential()\nmodel.add(keras.layers.Flatten(input_shape=[28, 28]))\nfor _ in range(20):\n    model.add(keras.layers.Dense(100, activation='selu', \n                                 kernel_initializer='lecun_normal'))\nmodel.add(keras.layers.AlphaDropout(rate=0.1))\nmodel.add(keras.layers.Dense(10, activation='softmax'))","edebec66":"model.summary()","ccc77766":"optimizer = keras.optimizers.Nadam(learning_rate=1e-3)\nmodel.compile(loss='sparse_categorical_crossentropy', optimizer=optimizer, \n              metrics=['accuracy'])","3c409f80":"batch_size = 32\nrates, losses = find_learning_rate(model, X_train_scaled, y_train, epochs=1, batch_size=batch_size)\nplot_lr_vs_loss(rates, losses)","853bf9d9":"rates[np.argmin(losses)]","66131578":"optimizer = keras.optimizers.Nadam(learning_rate=0.002)\nmodel.compile(loss='sparse_categorical_crossentropy', optimizer=optimizer, \n              metrics=['accuracy'])","9843441c":"early_stopping_cb = keras.callbacks.EarlyStopping(patience=20)\nmodel_checkpoint_cb = keras.callbacks.ModelCheckpoint('my_fashion_mnist_model_v4.h5', \n                                                      save_best_only=True)\ncallbacks = [early_stopping_cb, model_checkpoint_cb]","fe974c77":"model.fit(X_train_scaled, y_train, epochs=100, \n          validation_data=(X_valid_scaled, y_valid), \n          callbacks=callbacks)","0a3bcf6a":"model = keras.models.load_model('my_fashion_mnist_model_v4.h5')\nmodel.evaluate(X_valid_scaled, y_valid)","0c3d454a":"keras.backend.clear_session()\ntf.random.set_seed(42)\nnp.random.seed(42)","0b537476":"model = keras.models.Sequential()\nmodel.add(keras.layers.Flatten(input_shape=[28, 28]))\nfor _ in range(20):\n    model.add(keras.layers.Dense(100, activation='selu', \n                                 kernel_initializer='lecun_normal'))\nmodel.add(keras.layers.Dense(10, activation='softmax'))","45f2b723":"model.summary()","c3cf2667":"optimizer = keras.optimizers.Nadam(learning_rate=1e-3)\nmodel.compile(loss='sparse_categorical_crossentropy', optimizer=optimizer, \n              metrics=['accuracy'])","a4a00ae7":"batch_size = 128\nrates, losses = find_learning_rate(model, X_train_scaled, y_train, epochs=1, batch_size=batch_size)\nplot_lr_vs_loss(rates, losses)","9f69033f":"rates[np.argmin(losses)]","f125e5ee":"class OneCycleScheduler(keras.callbacks.Callback):\n    def __init__(self, iterations, max_rate, start_rate=None, \n                 last_iterations=None, last_rate=None):\n        self.iterations = iterations\n        self.max_rate = max_rate\n        self.start_rate = start_rate or max_rate \/ 10\n        self.last_iterations = last_iterations or iterations \/\/ 10 + 1\n        self.half_iteration = (iterations - self.last_iterations) \/\/ 2\n        self.last_rate = last_rate or self.start_rate \/ 1000\n        self.iteration = 0\n    def _interpolate(self, iter1, iter2, rate1, rate2):\n        return ((rate2 - rate1) * (self.iteration - iter1) \/ (iter2 - iter1) + rate1)\n    def on_batch_begin(self, batch, logs):\n        if self.iteration < self.half_iteration:\n            rate = self._interpolate(0, self.half_iteration, self.start_rate, self.max_rate)\n        elif self.iteration < 2 * self.half_iteration:\n            rate = self._interpolate(self.half_iteration, 2 * self.half_iteration, self.max_rate, self.start_rate)\n        else:\n            rate = self._interpolate(2 * self.half_iteration, self.iterations, self.start_rate, self.last_rate)\n        \n        self.iteration += 1\n        K.set_value(self.model.optimizer.lr, rate)","b2227a7b":"n_epochs = 50\nonecycle = OneCycleScheduler(math.ceil(len(X_train) \/ batch_size) * n_epochs, max_rate=0.001)\nhistory = model.fit(X_train_scaled, y_train, epochs=n_epochs, batch_size=batch_size, \n                    validation_data=(X_valid_scaled, y_valid), callbacks=[onecycle])","44d2a741":"model.evaluate(X_train_scaled, y_train)","513c468d":"model.evaluate(X_valid_scaled, y_valid)","8192c7e3":"model = keras.models.load_model('my_fashion_mnist_model_v2.h5')\nmodel.evaluate(X_valid, y_valid)","ed03fa6f":"model.evaluate(X_test, y_test)","9f0c4b9e":"We will try adding Batch Normalization and changing the learning rate:","0617930f":"We will set the learning rate slightly lower (learning_rate=0.002), create the callbacks we need and train the model:","18e14f72":"The accuracy did not improve, the best model so far is still the one with Batch Normalization.","f23f85ec":"The accuracy has slightly improved from 88.88% to 88.94% using this model.","5a7a0af7":"We will use the following code to determine the optimum learning rate:","75e4a94e":"The model overfits the training data because the accuracy for the training and validation data differs quite a lot. Hence, we will use the model with Batch Normalization for predictions:","05b78c65":"Let's try to build an ANN with 20 hidden layers of 100 neurons each:","f247dba2":"We will set the learning rate slightly lower (learning_rate=0.001), create the callbacks we need and train the model:","cf7cb1ec":"We will use Nadam optimization and early stopping:","069da961":"We will set the learning rate slightly lower (learning_rate=0.03), create the callbacks we need and train the model:","496c60aa":"We will retrain the model using 1cycle scheduling and see if it improves the model accuracy:","8878534f":"## Model Building","d8945e2b":"We will try regularizing the model with alpha dropout:","bdb2d742":"We will set the learning rate slightly lower (learning_rate=0.0008), create the callbacks we need and train the model:","3d2c19fc":"We will set the max learning rate slightly lower (learning_rate=0.001) and train the model. The code below implements 1cycle scheduling:","20a9e07d":"We will try replacing Batch Normalizaion with SELU, and making necessary adjustments to ensure the network self-normalizes:","e7fd3a56":"The accuracy is actually worse using alpha dropout. We could try different thresholds of alpha dropout but we will move on for now."}}