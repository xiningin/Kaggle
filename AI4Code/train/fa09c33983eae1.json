{"cell_type":{"0f1cc65e":"code","6aa435fa":"code","e2687ed4":"code","7c6daaa8":"code","1196ddcb":"code","98472ca8":"code","a4e8c53c":"code","c5c5ab4d":"code","32736de7":"code","50cfc04c":"code","711b648d":"code","458fa857":"code","4ff0d02c":"markdown","c09d34f8":"markdown","ee624051":"markdown","15bd5bc5":"markdown","3dbf0869":"markdown","87f16107":"markdown"},"source":{"0f1cc65e":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","6aa435fa":"%%time\nimport lightgbm as lgb\nfrom sklearn.model_selection import KFold\nfrom sklearn import model_selection, preprocessing, metrics\n\nfrom sklearn import preprocessing\nimport gc\n\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nwarnings.simplefilter(action='ignore', category=FutureWarning)\n\n\n# Load data\n\n\ntest = pd.read_csv('..\/input\/best-features-only\/X_test.csv')\nfeatures = test.columns\ntrain = pd.read_csv('..\/input\/best-features-only\/X_train.csv', usecols=features)\n\ntrs = train.shape[0]\ntes = test.shape[0]\n\nprint(features)\nprint(train.shape)\nprint(test.shape)","e2687ed4":"train = pd.concat([train, test], axis =0)\ndel test\ngc.collect()","7c6daaa8":"target = np.hstack([np.zeros(trs,), np.ones(tes,)])\n","1196ddcb":"train, test, y_train, y_test = model_selection.train_test_split(train, target, test_size=0.33, random_state=42, shuffle=True)\ndel target\ngc.collect()","98472ca8":"train = lgb.Dataset(train, label=y_train)\ntest = lgb.Dataset(test, label=y_test)","a4e8c53c":"param = {'num_leaves': 50,\n         'min_data_in_leaf': 30, \n         'objective':'binary',\n         'max_depth': 5,\n         'learning_rate': 0.01,\n         \"min_child_samples\": 20,\n         \"boosting\": \"gbdt\",\n         \"feature_fraction\": 0.9,\n         \"bagging_freq\": 1,\n         \"bagging_fraction\": 0.9 ,\n         \"bagging_seed\": 56,\n         \"metric\": 'auc',\n         \"verbosity\": -1}","c5c5ab4d":"num_round = 50\nclf = lgb.train(param, train, num_round, valid_sets = [train, test], verbose_eval=50, early_stopping_rounds = 50)","32736de7":"feature_imp = pd.DataFrame(sorted(zip(clf.feature_importance(),features)), columns=['Value','Feature'])\n\nplt.figure(figsize=(20, 20))\nsns.barplot(x=\"Value\", y=\"Feature\", data=feature_imp.sort_values(by=\"Value\", ascending=False).head(100))\nplt.title('LightGBM Features')\nplt.tight_layout()\nplt.show()\nplt.savefig('lgbm_importances-01.png')","50cfc04c":"%%time\ndel train, test, clf\n\ngc.collect()\ngc.collect()\n\nfeatures = ['item_id', 'dept_id', 'store_id', 'cat_id', 'state_id', 'wday', 'month',\n       'year', 'event_name_1', 'event_type_1', 'event_name_2', 'event_type_2',\n       'snap_CA', 'snap_TX', 'snap_WI', 'sell_price', 'lag_7', 'lag_28',\n       'rmean_7_7', 'rmean_28_7', 'rmean_7_28', 'rmean_28_28',\n       'quarter', 'mday']\n\ntest = pd.read_csv('..\/input\/best-features-only\/X_test.csv', usecols=features)\ntrain = pd.read_csv('..\/input\/best-features-only\/X_train.csv', usecols=features)\n\ntrain = pd.concat([train, test], axis =0)\ndel test\ngc.collect()\n\ntarget = np.hstack([np.zeros(trs,), np.ones(tes,)])\n\ntrain, test, y_train, y_test = model_selection.train_test_split(train, target, test_size=0.33, random_state=42, shuffle=True)\ndel target\ngc.collect()\n\ntrain = lgb.Dataset(train, label=y_train)\ntest = lgb.Dataset(test, label=y_test)\n\nclf = lgb.train(param, train, num_round, valid_sets = [train, test], verbose_eval=50, early_stopping_rounds = 50)\n\nfeature_imp = pd.DataFrame(sorted(zip(clf.feature_importance(),features)), columns=['Value','Feature'])\n\nplt.figure(figsize=(20, 20))\nsns.barplot(x=\"Value\", y=\"Feature\", data=feature_imp.sort_values(by=\"Value\", ascending=False).head(100))\nplt.title('LightGBM Features')\nplt.tight_layout()\nplt.show()\nplt.savefig('lgbm_importances-02.png')","711b648d":"%%time\ndel train, test, clf\n\ngc.collect()\ngc.collect()\n\nfeatures = ['item_id', 'dept_id', 'store_id', 'cat_id', 'state_id', 'wday',\n       'year', 'event_name_1', 'event_type_1', 'event_name_2', 'event_type_2',\n       'snap_CA', 'snap_TX', 'snap_WI', 'sell_price', 'lag_7', 'lag_28',\n       'rmean_7_7', 'rmean_28_7', 'rmean_7_28', 'rmean_28_28',\n       'quarter', 'mday']\n\ntest = pd.read_csv('..\/input\/best-features-only\/X_test.csv', usecols=features)\ntrain = pd.read_csv('..\/input\/best-features-only\/X_train.csv', usecols=features)\n\ntrain = pd.concat([train, test], axis =0)\ndel test\ngc.collect()\n\ntarget = np.hstack([np.zeros(trs,), np.ones(tes,)])\n\ntrain, test, y_train, y_test = model_selection.train_test_split(train, target, test_size=0.33, random_state=42, shuffle=True)\ndel target\ngc.collect()\n\ntrain = lgb.Dataset(train, label=y_train)\ntest = lgb.Dataset(test, label=y_test)\n\nclf = lgb.train(param, train, num_round, valid_sets = [train, test], verbose_eval=50, early_stopping_rounds = 50)\n\nfeature_imp = pd.DataFrame(sorted(zip(clf.feature_importance(),features)), columns=['Value','Feature'])\n\nplt.figure(figsize=(20, 20))\nsns.barplot(x=\"Value\", y=\"Feature\", data=feature_imp.sort_values(by=\"Value\", ascending=False).head(100))\nplt.title('LightGBM Features')\nplt.tight_layout()\nplt.show()\nplt.savefig('lgbm_importances-03.png')","458fa857":"%%time\ndel train, test, clf\n\ngc.collect()\ngc.collect()\n\nfeatures = ['dept_id', 'store_id', 'cat_id', 'state_id', 'wday',\n       'year', 'event_name_1', 'event_type_1', 'event_name_2', 'event_type_2',\n       'snap_CA', 'snap_TX', 'snap_WI', 'sell_price', 'lag_7', 'lag_28',\n       'rmean_7_7', 'rmean_28_7', 'rmean_7_28', 'rmean_28_28',\n       'quarter', 'mday']\n\ntest = pd.read_csv('..\/input\/best-features-only\/X_test.csv', usecols=features)\ntrain = pd.read_csv('..\/input\/best-features-only\/X_train.csv', usecols=features)\n\ntrain = pd.concat([train, test], axis =0)\ndel test\ngc.collect()\n\ntarget = np.hstack([np.zeros(trs,), np.ones(tes,)])\n\ntrain, test, y_train, y_test = model_selection.train_test_split(train, target, test_size=0.33, random_state=42, shuffle=True)\ndel target\ngc.collect()\n\ntrain = lgb.Dataset(train, label=y_train)\ntest = lgb.Dataset(test, label=y_test)\n\nclf = lgb.train(param, train, num_round, valid_sets = [train, test], verbose_eval=50, early_stopping_rounds = 50)\n\nfeature_imp = pd.DataFrame(sorted(zip(clf.feature_importance(),features)), columns=['Value','Feature'])\n\nplt.figure(figsize=(20, 20))\nsns.barplot(x=\"Value\", y=\"Feature\", data=feature_imp.sort_values(by=\"Value\", ascending=False).head(100))\nplt.title('LightGBM Features')\nplt.tight_layout()\nplt.show()\nplt.savefig('lgbm_importances-04.png')","4ff0d02c":"Well, the AUC hardly changed. Let's see what happens if we leave out 'month' as well.","c09d34f8":"Now we will repeat the same procedure, but now we'll drop 'week' from the features.","ee624051":"Getting a reliable validation strategies has been one of the biggest issues in this compettition. In this kernel we'll take a look at the adverserial validation, and what it may imply about our data. \n\nFor features we have used feateus from thsi kernel: https:\/\/www.kaggle.com\/kneroma\/m5-first-public-notebook-under-0-50, which have been recomputed here: https:\/\/www.kaggle.com\/tunguz\/best-features-only\/\n\nUnfortunately, it is impossible to create and save the full dataset in Kaggle kernels, so we had to resort to subsampling. Due to this, it is very likely that there are some sampling isssues that skew the conclusions in some way. Results in this kernel are meant to be strictly preliminary.","15bd5bc5":"Still no substantial change. Let's now remove 'item_id'.","3dbf0869":"Wow, AUC of 0.9999 is as large as it gets! Let's see which columns are the most responsible for this discrepancy.","87f16107":"Seems that the temporal features are the most disperate between the two models, which may not be surprising for a time-series problem. "}}