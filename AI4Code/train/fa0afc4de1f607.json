{"cell_type":{"0222983b":"code","7ed0fa7c":"code","4cfbde93":"code","6db4410a":"code","6cdd88fa":"code","2c955b1b":"code","8950f9b2":"code","15c6ba28":"code","2d2ea99b":"code","8736074d":"code","ea24a5da":"code","5f9c8279":"code","32a100c1":"code","f6ae58d4":"code","947d757e":"code","6b23f24e":"code","8e51a496":"code","8c54a371":"code","239d029e":"code","b9ea3dec":"code","4ecd0520":"code","a06099ef":"code","ba6cfce2":"code","6ee59e02":"code","82ff7f99":"code","2933114f":"code","dc087b8e":"code","0d8d6f04":"code","92293ee3":"code","b740e3e5":"code","3e3eeb68":"code","0451d317":"code","e7fe5672":"code","d411991e":"code","28bdbd17":"code","f6467cbc":"code","28df92dd":"code","e30fcb69":"code","3fb11642":"code","2283a0cc":"code","b683a346":"code","4b0c53c0":"code","09e65e91":"code","a4eaeccc":"code","5e7c5eb6":"code","ff3994b0":"code","1b643aa1":"code","c2f7cacf":"code","a4a867b3":"code","90408907":"code","4b74a8a9":"code","bbea7d99":"code","af175531":"code","273b2d2d":"code","5ae8a75f":"code","181cdceb":"code","bce504db":"code","024a5a82":"code","350da13d":"code","72b03aa7":"code","7c551a5b":"code","a8752b1b":"markdown","43917d9a":"markdown","f439b3bf":"markdown","8ea4645f":"markdown","be232cff":"markdown","5fcc241f":"markdown","31dd681d":"markdown","fe271d6c":"markdown","4f620c09":"markdown","2e35c7d7":"markdown","ef121e15":"markdown","22786129":"markdown","59760298":"markdown","b5e6c25c":"markdown","68dd3d2d":"markdown","b3fe4b38":"markdown","22adc66b":"markdown","1fe83ce5":"markdown","0ffa6d12":"markdown","0e784a6d":"markdown","1a17a756":"markdown","b24f956e":"markdown","774a7740":"markdown","7f5d82da":"markdown","1bb0b4a5":"markdown","664d6367":"markdown"},"source":{"0222983b":"#importing the required libraries & packages\nimport pandas as pd\nimport numpy as np\nimport time\nimport argparse\nimport string\nfrom sklearn.model_selection import train_test_split\nfrom nltk.tokenize import regexp_tokenize\nfrom datetime import datetime\nimport pytz","7ed0fa7c":"#loading the US Airline Sentiment data\ndata_frame = pd.read_csv('..\/input\/Tweets.csv')\ndata_frame.head()","4cfbde93":"data_frame.shape","6db4410a":"#repalcing the categorical values of 'airline_sentiment' to numeric values\ndata_frame['airline_sentiment'].replace(('neutral', 'positive', 'negative'), (0, 1, 2), inplace=True)\ndata_frame['airline_sentiment'].value_counts()","6cdd88fa":"#forming the feature & label variables\ndata = data_frame['text'].values.tolist()\nlabels = data_frame['airline_sentiment'].values.tolist()","2c955b1b":"#First five samples text\ndata[:5]","8950f9b2":"#first 5 samples label\nlabels[:5]","15c6ba28":"#splitting the data into 80 and 20 split\ntrain_X, test_X, y_train, y_test = train_test_split(data, labels, test_size=0.2, \n                                                    random_state=42, shuffle=True)\n\nprint(f'Number of training examples: {len(train_X)}')\nprint(f'Number of testing examples: {len(test_X)}')","2d2ea99b":"# Here is a default pattern for tokenization\ndefault_pattern =  r\"\"\"(?x)                  \n                        (?:[A-Z]\\.)+          \n                        |\\$?\\d+(?:\\.\\d+)?%?    \n                        |\\w+(?:[-']\\w+)*      \n                        |\\.\\.\\.               \n                        |(?:[.,;\"'?():-_`])    \n                    \"\"\"","8736074d":"#funtion for tokenizing the data\n\"\"\" Tokenize sentence with specific pattern\nArguments: text {str} -- sentence to be tokenized, such as \"I love NLP\"\nKeyword Arguments: pattern {str} -- reg-expression pattern for tokenizer (default: {default_pattern})\nReturns: list -- list of tokenized words, such as ['I', 'love', 'nlp'] \"\"\"\n\ndef tokenize(text, pattern = default_pattern):\n    text = text.lower()\n    return regexp_tokenize(text, pattern)","ea24a5da":"# Tokenize training text into tokens\ntokenized_text = []\nfor i in range(0, len(train_X)):\n    tokenized_text.append(tokenize(train_X[i]))\n\nX_train = tokenized_text\n\n# Tokenize testing text into tokens\ntokenized_text = []\nfor i in range(0, len(test_X)):\n    tokenized_text.append(tokenize(test_X[i]))\n\nX_test = tokenized_text","5f9c8279":"#tokenized train & test data\nprint(X_train[0], X_train[1])\nprint(X_test[0])","32a100c1":"#building dictionary\n\"\"\" Function: To create a dictionary of tokens from the data\nArguments: data in the type - list\nReturns: Sorted dictionary of the tokens and their count in the data \"\"\"\n\ndef createDictionary(data):\n    dictionary = dict()\n    for sample in  data:\n        for token in sample:\n            dictionary[token] = dictionary.get(token, 0) + 1\n    \n    #sorting the dictionary based on the values\n    sorted_dict = sorted(dictionary.items(), key=lambda x: x[1], reverse=True)\n    return dict(sorted_dict)","f6ae58d4":"bog = createDictionary(X_train)\n#top 10 items in the dictionary\nprint(\"Top 10 tokens in the training dictionary:\\n\")\nlist(bog.items())[:10]","947d757e":"#Navie Bayes Classifier \nclass NBClassifier:\n\n    def __init__(self, X_train, y_train, size):\n        tz_NY = pytz.timezone('America\/New_York') \n        print(\"Model Start Time:\", datetime.now(tz_NY).strftime(\"%H:%M:%S\"))\n        self.X_train = X_train\n        self.y_train = y_train\n        self.size = size\n\n    def createDictionary(self):\n        dictionary = dict()\n        for sample in  X_train:\n            for token in sample:\n                dictionary[token] = dictionary.get(token, 0) + 1\n        #sorting the dictionary based on the values\n        sorted_dict = sorted(dictionary.items(), key=lambda x: x[1], reverse=True)\n        return dict(sorted_dict)\n    \n    def fit(self):\n        \"\"\" Function: To compute the count of words in training data dictionary\n        Arguments: Trianing data & Size of dictionary\n        Returns: dictionary of tokens with their class wise probabilities \"\"\"\n      \n        X_train_dict = self.createDictionary()\n        if self.size == 'full':\n            self.words_list = list(X_train_dict.keys())\n            self.words_count = dict.fromkeys(self.words_list, None)\n        else:\n            self.words_list = list(X_train_dict.keys())[:int(self.size)]\n            self.words_count = dict.fromkeys(self.words_list, None)\n            \n        #DataFrame of training data\n        train = pd.DataFrame(columns = ['X_train', 'y_train'])\n        train['X_train'] = X_train\n        train['y_train'] = y_train\n\n        train_0 = train.copy()[train['y_train'] == 0]\n        train_1 = train.copy()[train['y_train'] == 1]\n        train_2 = train.copy()[train['y_train'] == 2]\n\n        #computing the prior of each class\n        Pr0 = train_0.shape[0]\/train.shape[0]\n        Pr1 = train_1.shape[0]\/train.shape[0]\n        Pr2 = train_2.shape[0]\/train.shape[0]\n\n        self.Prior = np.array([Pr0, Pr1, Pr2])\n\n        #converting list of lists into a list\n        def flatList(listOfList):\n            flatten = []\n            for elem in listOfList:\n                flatten.extend(elem)\n            return flatten\n  \n        #Creating the data list for each class - tokens of each class\n        X_train_0 = flatList(train[train['y_train'] == 0]['X_train'].tolist())\n        X_train_1 = flatList(train[train['y_train'] == 1]['X_train'].tolist())\n        X_train_2 = flatList(train[train['y_train'] == 2]['X_train'].tolist())\n    \n        self.X_train_len = np.array([len(X_train_0), len(X_train_1), len(X_train_2)])\n\n        for token in self.words_list:\n            #list to store three word counts of a token\n            res = []\n\n            #inserting count of token in class 0: Neutral\n            res.insert(0, X_train_0.count(token))\n\n            #inserting count of token in class 1: Positive\n            res.insert(1, X_train_1.count(token))\n\n              #inserting count of token in class 2: Negative\n            res.insert(2, X_train_2.count(token))\n\n            #assigning the count list to its token in the dictionary \n            self.words_count[token] = res\n        return self\n\n    def predict(self, X_test):\n        \"\"\" Function: Predicts the label of the data\n        Arguments: self and the test data\n        Returns: List of predicted labels for the test data \"\"\"     \n        pred = []\n        for sample in X_test:\n            mul = np.array([1,1,1])\n            for tokens in sample:\n                vocab_count = len(self.words_list)\n                if tokens in self.words_list:\n                    prob = ((np.array(self.words_count[tokens])+1) \/ (self.X_train_len + vocab_count))\n                mul = mul * prob\n            val = mul * self.Prior\n            pred.append(np.argmax(val))\n        tz_NY = pytz.timezone('America\/New_York') \n        print(\"Model End Time:\", datetime.now(tz_NY).strftime(\"%H:%M:%S\"))\n        return pred\n    \n    def score(self, pred, labels):\n        \"\"\" Function: To compute the perfoemance of the model\n        Arguments: self, predicted labels and actual labels of the test data\n        Returns: Number of lables correctly predicted and the accuracy of the model \"\"\"\n        correct = (np.array(pred) == np.array(labels)).sum()\n        accuracy = correct\/len(pred)\n        return correct, accuracy","6b23f24e":"# Creating holders to store the model performance results\nattributes = []\ncorr = []\nacc = []\n\n#function to call for storing the results\ndef storeResults(attr, cor,ac):\n    attributes.append(attr)\n    corr.append(round(cor, 3))\n    acc.append(round(ac, 3))","8e51a496":"#training the classifier     \nnb = NBClassifier(X_train, y_train, 'full')  \nnb.fit()\n\n#predicting the labels for test samples\ny_pred = nb.predict(X_test)\n\n#Checking\nprint(\"NBClassifier Model miss any prediction???\", len(X_test) != len(y_pred))","8c54a371":"#Performance of the classifier\ncor1, acc1 = nb.score(y_pred, y_test)\nprint(\"Count of Correct Predictions:\", cor1)\nprint(\"Accuracy of the model: %i \/ %i = %.4f \" %(cor1, len(y_pred), acc1))","239d029e":"#storing the results. The below mentioned order of parameter passing is important.\n#Caution: Execute only once to avoid duplications.\nstoreResults('Unprocessed Data', cor1, acc1)","b9ea3dec":"#string of punctiations\nstring.punctuation","4ecd0520":"#Removing the punctuation\n'''Function: Removes the punctuation from the tokens\n   Arguments: list of text data samples\n   Returns: list of tokens of each sample without punctuation '''\ndef removePunctuation(data):\n    update = []\n    for sample in data:\n        #removing punctuation from the tokens\n        re_punct = [''.join(char for char in word if char not in string.punctuation) for word in sample]\n        #removes the empty strings\n        re_punct = [word for word in re_punct if word]\n       \n        update.append(re_punct)\n    return update","a06099ef":"#Removing punctuation from training data text tokens  \nX_train_P = removePunctuation(X_train)\n\n#Removing punctuation from testing data text tokens\nX_test_P = removePunctuation(X_test)\n\n#train & test data after removing punctuation\nprint(X_train_P[0])\nprint(X_test_P[0])","ba6cfce2":"#training the classifier     \nnb_punct = NBClassifier(X_train_P, y_train, 'full')\nnb_punct.fit()\n\n#predicting the labels for test samples\ny_pred_P = nb_punct.predict(X_test_P)\n\n#Checking\nprint(\"NBClassifier Model miss any prediction???\", len(X_test) != len(y_pred_P))","6ee59e02":"#Performance of the classifier\ncor2, acc2 = nb_punct.score(y_pred_P, y_test)\nprint(\"Count of Correct Predictions:\", cor2)\nprint(\"Accuracy of the model: %i \/ %i = %.4f \" %(cor2, len(y_pred_P), acc2))","82ff7f99":"#storing the results. The below mentioned order of parameter passing is important.\n#Caution: Execute only once to avoid duplications.\nstoreResults('No Punctuation Data', cor2, acc2)","2933114f":"'''Function: Removes the stopwords from the tokens\n   Arguments: list of text data samples\n   Returns: list of tokens of each sample without punctuation '''\ndef removeStopWords(data):\n    update = []\n    stopwords = ['the', 'at','i', 'of', 'us', 'have', 'a', 'you','ours', 'themselves', \n                 'that', 'this', 'be', 'is', 'for']\n    for sample in data:\n        #removing stopwords from tokenized data\n        re_stop = [word for word in sample if word not in stopwords]\n        \n        update.append(re_stop)\n    return update","dc087b8e":"#Removing stopwords from training data text tokens  \nX_train_S = removeStopWords(X_train)\n\n#Removing stopwords from testing data text tokens\nX_test_S = removeStopWords(X_test)\n\n#train & test data after removing stopwords\nprint(X_train_S[0])\nprint(X_test_S[0])","0d8d6f04":"#training the classifier     \nnb_stop = NBClassifier(X_train_S, y_train, 'full')\nnb_stop.fit()\n\n#predicting the labels for test samples\ny_pred_S = nb_stop.predict(X_test_S)\n\n#Checking\nprint(\"NBClassifier Model miss any prediction???\", len(X_test) != len(y_pred_S))","92293ee3":"#Performance of the classifier\ncor3, acc3 = nb_stop.score(y_pred_S, y_test)\nprint(\"Count of Correct Predictions:\", cor3)\nprint(\"Accuracy of the model: %i \/ %i = %.4f \" %(cor3, len(y_pred_S), acc3))","b740e3e5":"#storing the results. The below mentioned order of parameter passing is important.\n#Caution: Execute only once to avoid duplications.\nstoreResults('Removed few Stopwords', cor3, acc3)","3e3eeb68":"#Removing stopwords from training data text tokens  \nX_train_PS = removeStopWords(X_train_P)\n\n#Removing stopwords from testing data text tokens\nX_test_PS = removeStopWords(X_test_P)\n\n#train & test data after removing stopwords\nprint(X_train_PS[0])\nprint(X_test_PS[0])","0451d317":"#training the classifier     \nnb_PS = NBClassifier(X_train_PS, y_train, 'full')\nnb_PS.fit()\n\n#predicting the labels for test samples\ny_pred_PS = nb_PS.predict(X_test_PS)\n\n#Checking\nprint(\"NBClassifier Model miss any prediction???\", len(X_test) != len(y_pred_PS))","e7fe5672":"#Performance of the classifier\ncor4, acc4 = nb_PS.score(y_pred_PS, y_test)\nprint(\"Count of Correct Predictions:\", cor4)\nprint(\"Accuracy of the model: %i \/ %i = %.4f \" %(cor4, len(y_pred_PS), acc4))","d411991e":"#storing the results. The below mentioned order of parameter passing is important.\n#Caution: Execute only once to avoid duplications.\nstoreResults('Removed both Punctuation & Few Stopwords', cor4, acc4)","28bdbd17":"#total tokens in training dictionary\nprint('Total tokens in the dictionary:', len(bog))","f6467cbc":"#training the classifier - 5000 tokens \nnb_5k = NBClassifier(X_train, y_train, '5000')\nnb_5k.fit()\n\n#predicting the labels for test samples\ny_pred_5k = nb_5k.predict(X_test)\n\n#Checking\nprint(\"NBClassifier Model miss any prediction???\", len(X_test) != len(y_pred_5k))","28df92dd":"#Performance of the classifier\ncor5, acc5 = nb_5k.score(y_pred_5k, y_test)\nprint(\"Count of Correct Predictions:\", cor5)\nprint(\"Accuracy of the model: %i \/ %i = %.4f \" %(cor5, len(y_pred), acc5))","e30fcb69":"#storing the results. The below mentioned order of parameter passing is important.\n#Caution: Execute only once to avoid duplications.\nstoreResults('5k Tokens of Voab - Unprocessed Data', cor5, acc5)","3fb11642":"#training the classifier - 5000 tokens \nnb_5k_P = NBClassifier(X_train_P, y_train, '5000')\nnb_5k_P.fit()\n\n#predicting the labels for test samples\ny_pred_5k_P = nb_5k_P.predict(X_test_P)\n\n#Checking\nprint(\"NBClassifier Model miss any prediction???\", len(X_test) != len(y_pred_5k_P))","2283a0cc":"#Performance of the classifier\ncor6, acc6 = nb_5k.score(y_pred_5k_P, y_test)\nprint(\"Count of Correct Predictions:\", cor6)\nprint(\"Accuracy of the model: %i \/ %i = %.4f \" %(cor6, len(y_pred), acc6))","b683a346":"#storing the results. The below mentioned order of parameter passing is important.\n#Caution: Execute only once to avoid duplications.\nstoreResults('5k Tokens of Voab - No Punctuation Data', cor6, acc6)","4b0c53c0":"#training the classifier - 5000 tokens \nnb_5k_S = NBClassifier(X_train_S, y_train, '5000')\nnb_5k_S.fit()\n\n#predicting the labels for test samples\ny_pred_5k_S = nb_5k_S.predict(X_test_S)\n\n#Checking\nprint(\"NBClassifier Model miss any prediction???\", len(X_test) != len(y_pred_5k_S))","09e65e91":"#Performance of the classifier\ncor7, acc7 = nb_5k_S.score(y_pred_5k_S, y_test)\nprint(\"Count of Correct Predictions:\", cor7)\nprint(\"Accuracy of the model: %i \/ %i = %.4f \" %(cor7, len(y_pred), acc7))","a4eaeccc":"#storing the results. The below mentioned order of parameter passing is important.\n#Caution: Execute only once to avoid duplications.\nstoreResults('5k Tokens of Voab - Removed few Stopwords', cor7, acc7)","5e7c5eb6":"#training the classifier - 5000 tokens \nnb_5k_PS = NBClassifier(X_train_PS, y_train, '5000')\nnb_5k_PS.fit()\n\n#predicting the labels for test samples\ny_pred_5k_PS = nb_5k_PS.predict(X_test_PS)\n\n#Checking\nprint(\"NBClassifier Model miss any prediction???\", len(X_test) != len(y_pred_5k_PS))","ff3994b0":"#Performance of the classifier\ncor8, acc8 = nb_5k_PS.score(y_pred_5k_PS, y_test)\nprint(\"Count of Correct Predictions:\", cor8)\nprint(\"Accuracy of the model: %i \/ %i = %.4f \" %(cor8, len(y_pred), acc8))","1b643aa1":"#storing the results. The below mentioned order of parameter passing is important.\n#Caution: Execute only once to avoid duplications.\nstoreResults('5k Tokens of Voab - Removed both Punctuation & Few Stopwords', cor8, acc8)","c2f7cacf":"#training the classifier - 5000 tokens \nnb_10k = NBClassifier(X_train, y_train, '5000')\nnb_10k.fit()\n\n#predicting the labels for test samples\ny_pred_10k = nb_10k.predict(X_test)\n\n#Checking\nprint(\"NBClassifier Model miss any prediction???\", len(X_test) != len(y_pred_10k))","a4a867b3":"#Performance of the classifier\ncor9, acc9 = nb_10k.score(y_pred_10k, y_test)\nprint(\"Count of Correct Predictions:\", cor9)\nprint(\"Accuracy of the model: %i \/ %i = %.4f \" %(cor9, len(y_pred), acc9))","90408907":"#storing the results. The below mentioned order of parameter passing is important.\n#Caution: Execute only once to avoid duplications.\nstoreResults('10k Tokens of Voab - Unprocessed Data', cor9, acc9)","4b74a8a9":"#training the classifier - 10000 tokens \nnb_10k_P = NBClassifier(X_train_P, y_train, '10000')\nnb_10k_P.fit()\n\n#predicting the labels for test samples\ny_pred_10k_P = nb_10k_P.predict(X_test_P)\n  \n#Checking\nprint(\"NBClassifier Model miss any prediction???\", len(X_test) != len(y_pred_10k_P))","bbea7d99":"#Performance of the classifier\ncor10, acc10 = nb_10k_P.score(y_pred_10k_P, y_test)\nprint(\"Count of Correct Predictions:\", cor10)\nprint(\"Accuracy of the model: %i \/ %i = %.4f \" %(cor10, len(y_pred), acc10))","af175531":"#storing the results. The below mentioned order of parameter passing is important.\n#Caution: Execute only once to avoid duplications.\nstoreResults('10k Tokens of Voab - No Punctuation Data', cor10, acc10)","273b2d2d":"#training the classifier - 10000 tokens \nnb_10k_S = NBClassifier(X_train_S, y_train, '10000')\nnb_10k_S.fit()\n\n#Sredicting the labels for test samSles\ny_pred_10k_S = nb_10k_S.predict(X_test_S)\n  \n#Checking\nprint(\"NBClassifier Model miss any Srediction???\", len(X_test) != len(y_pred_10k_S))","5ae8a75f":"#Performance of the classifier\ncor11, acc11 = nb_10k_S.score(y_pred_10k_S, y_test)\nprint(\"Count of Correct Predictions:\", cor11)\nprint(\"Accuracy of the model: %i \/ %i = %.4f \" %(cor11, len(y_pred), acc11))","181cdceb":"#storing the results. The below mentioned order of parameter passing is important.\n#Caution: Execute only once to avoid duplications.\nstoreResults('10k Tokens of Voab - Removed few Stopwords', cor11, acc11)","bce504db":"#training the claPSPSifier - 10000 tokenPS \nnb_10k_PS = NBClassifier(X_train_PS, y_train, '10000')\nnb_10k_PS.fit()\n\n#PSredicting the labelPS for tePSt PSamPSlePS\ny_pred_10k_PS = nb_10k_PS.predict(X_test_PS)\n  \n#Checking\nprint(\"NBClaPSPSifier Model miSS any PSrediction???\", len(X_test) != len(y_pred_10k_PS))","024a5a82":"#Performance of the classifier\ncor12, acc12 = nb_10k_PS.score(y_pred_10k_PS, y_test)\nprint(\"Count of Correct Predictions:\", cor12)\nprint(\"Accuracy of the model: %i \/ %i = %.4f \" %(cor12, len(y_pred), acc12))","350da13d":"#storing the results. The below mentioned order of parameter passing is important.\n#Caution: Execute only once to avoid duplications.\nstoreResults('10k Tokens of Voab - Removed both Punctuation & Few Stopwords', cor12, acc12)","72b03aa7":"#creating dataframe\nresults = pd.DataFrame({ 'Data Modification': attributes,    \n    'Correct Predictions': corr,\n    'Model Accuracy': acc})","7c551a5b":"results.sort_values(by=['Model Accuracy', 'Correct Predictions'], ascending=False)","a8752b1b":"# **2. Importing Libraries & Loading Data**","43917d9a":"**NOTE: Detailed description & analysis of each step are mentioned in the report @ https:\/\/github.com\/shreyagopal\/US-Airlines-Sentiment-Classification\/blob\/main\/Report_US%20Airlines%20Sentiment%20Classification.pdf.**","f439b3bf":"### **1. Problem Statement**\n\n<div style=\"text-align: justify\">Sentiment analysis is a text analysis technique that detects polarity (e.g. a positive or negative opinion) within text, whether a whole document, paragraph,sentence, or clause. Sentimentanalysis is also known as opinion mining. Understanding people\u2019s emotions is essential for businesses since customers express their thoughts and feelings more openly than ever before. The commonly used social media platform to express one\u2019s opinions or emotions is Twitter. <\/div> <br>\n\n<div style=\"text-align: justify\">Performing analysis on customer feedback, such as opinions in survey responses and social media conversations, allows brands to listen attentively to their customers, and tailor products and services to meet their needs. However, all the opiniated data from the Twitter is in the form of text which is unstructured. . Sentiment analysis, however, helps businesses make sense of all this unstructured text by automatically understanding, processing, and tagging it.<\/div> <br>\n\n<div style=\"text-align: justify\">Objective of this project is to perform sentiment analysis on the tweets of six US Airlines. The scrapped tweets contain positive, negative, or neutral sentiments about the airline from their respective customers. The task is to analyze how travelers in February 2015 expressed their feelings on Twitter about six major US airlines. Few of the algorithms used for sentiment analysis are Naive Bayes, SVM, Logistic Regression and LSTM. Out of them, in this project Na\u00efve Bayes classifier is used to build the sentiment analysis model for the US Airline Tweets. The classifier is hard coded in Python without using any libraries with inbuilt classifiers. <\/div> <br>\n\nThe tasks invloved in this project are as follows:\n\n* Build a dictionary based on your training corpus. Calculate conditional probability of each token for each class (this is also called unigram probability). Then evaluate on test data and report accuracy.\n* Try to improve your algorithm. Some suggestions: <br>\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;i. Remove STOP words from the vocabulary that appear vary frequently but not related to the attitude or opinion of the writer. <br>\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;ii. Reduce the size of your vocabulary further by taking only top-k frequent word types that appear in the training dataset. Vary k and compare performance.","8ea4645f":"# **8. Navie Bayes Classifier Training and Evaluation**\n\nThe Navie Bayes Classifier, NBClassifier takes three arguments:\n* X_train: Features of training dataset\n* y_train: Labels of training dataset\n* size: Size of vacabulary to be used in the model\n\nAll three arguments are needed for the model to work.","be232cff":"### **9.1.3. Removing both Punctuation & Few Stopwords**","5fcc241f":"The Navie Bayers Classifier that we trainined on the data predicts 78.24% of samples correctly. Now, to improve this number few more text processing methods are appilied on the training data and then the classifier is trained on this  modified data to predict the sentiment of the test samples.\n\n","31dd681d":"## **9.2. Reducing the Dictionary Size**\n\nTo improve the model performance, we reduce the size of training dictionary further by taking only top-k frequent word types that appear in it. Here, we vary the value of k and compare the model performance.\n\n","fe271d6c":"**5k Tokens of Vocabulary - Removed both Punctuation & Few Stopwords**\n\n\n","4f620c09":"# **3. Data Preprocessing**\n\nThe US Airline Sentiment dataset has so many features. Among them, in our project we will be working with the 'text' & 'airline_segment' features. Here, the 'text' is considered as a feature (X) and the 'airline_segment' as a target label (y) for the classifier.\n\nThe values in the target data are categorical and are 'neutral', 'positive' & 'negative'. For the easy computation, we are replacing the 'neutral', 'positive' & 'negative' with 0, 1 & 2 values respectively.","2e35c7d7":"**10k Tokens of Vocabulary - Removed few Stopwords**\n\n\n","ef121e15":"# **7. Building the Navie Bayes Classifier**\n\nNow, we define text classifier class called NBClassifier, which comprises of three functions:\n* createDictionary()\n* fit()\n* predict()\n* score()\n\n**createDictionary():** This function takes in the tokenized text data and gives out the dictionary or the bag of words of the data.\n\n**fit():** This function has all the word counts required to calculate the Navie Bayes Classifier probabilities and then fits the classifier on our training data.\n\n**predict():** The test data is inputed to this function which determines the sentiment label based of each tweet by using the word counts computed during the training process (from fit function). In this step, Laplace smoothing is applied while computing Na\u00efve Bayes probabilities for the test data.\n\n**score():** Determine how many tweets are classified correctly and measures the performance of the model in terms of accuracy.","22786129":"### **9.1.2. Remove Stopwords**","59760298":"**5k Tokens of Vocabulary - Removed few Stopwords**\n\n\n","b5e6c25c":"**10k Tokens of Vocabulary - No Punctuation Data**","68dd3d2d":"### **9.1.1. Remove Puntuation**","b3fe4b38":"# **6. Building Dictionary**\n\nBuilding dictionary of the training data.","22adc66b":"### **9.2.2. Considering Top 10k Tokens**\n**10k Tokens of Vocabulary - Unprocessed data**","1fe83ce5":"# **4. Splitting the data for Classification**\n\nThe data splitting is done in 80-20 split using trian_test_split method of sklearn.","0ffa6d12":"# **5. Text Preprocessing**\n\nA process of transforming text into something an algorithm can digest is text processing. This includes:\n* &nbsp;tokenizing the data\n* &nbsp;removing the punctuation\n* &nbsp;removing the stopwords\n* &nbsp;stemming \n* &nbsp;lemmatization\n\nAs of now, we are only going to tokenize the data and work with it without removing the punctuation or stop words and apply any other text processing methods.","0e784a6d":"# **US Airlines Tweets Sentiment Classification**","1a17a756":"### **9.2.1. Considering Top 5k Tokens**\n\n**5k Tokens of Vocabulary - Unprocessed data**","b24f956e":"**10k Tokens of Vocabulary - Removed both Punctuation & Few Stopwords**\n\n\n","774a7740":"# **10. Comparing the Results**","7f5d82da":"**5k Tokens of Vocabulary - No Punctuation Data**","1bb0b4a5":"## **9.1. Further Processing Text Data**\n\nIn this step, we are going to apply two text processing methods on the previously tokenized data:\n* remove the punctuation \n* remove stop words","664d6367":"# **9. Trying to improve the NBClassifier**\nTo improve the performance of the NBClassifier, \n* apply other text processing methods\n* reduce the size of dictonary"}}