{"cell_type":{"33eeff20":"code","be0db2dd":"code","1968fdca":"code","63a0ffae":"code","3909660c":"code","db249f62":"code","48145803":"code","550f64c8":"code","ce2ec271":"code","0b1f1d49":"code","94e56147":"code","925124df":"code","dfc646fc":"code","a9e2bdbf":"code","367b56cf":"code","e738c06a":"code","ded15836":"code","0e3920e1":"code","464ea5bb":"code","e95471fb":"code","21e53423":"code","101ce3e4":"code","73ba7137":"code","e83687b6":"code","a757f4a3":"code","10ae5221":"code","4c68ff5e":"code","4646e008":"code","5f159cce":"code","006e99b4":"code","c88dc17b":"code","95f9a3dd":"code","bc27e03a":"code","a8bb65ae":"code","380311ca":"code","ada4e979":"markdown","94298b02":"markdown","5f00d498":"markdown","42c599d9":"markdown","3328ae0c":"markdown","8a980e69":"markdown","e4550bd5":"markdown","c64eddd8":"markdown","4ed7fd85":"markdown","dbde027e":"markdown","e8d3f7d0":"markdown","786cb977":"markdown","1553101d":"markdown","ffe3649f":"markdown","f11d9975":"markdown","34b452b2":"markdown"},"source":{"33eeff20":"# import required libraries\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport datetime\nfrom math import sqrt\nimport scipy.stats  as stats\nimport seaborn as sns\n\n#regression library\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.linear_model import LinearRegression, LogisticRegression\nfrom sklearn import svm, preprocessing\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.svm import SVR\nimport lightgbm as lgb\nimport gc","be0db2dd":"# define the function to change the category column from string to int16\ndef chg_cat_int(df, cat_col):\n    \n    for col, col_dtype in cat_col.items():\n        if col_dtype == 'category':\n            df[col] = df[col].cat.codes.astype('int16')\n            df[col] -= df[col].min()\n            \n    return df","1968fdca":"# loaded the dataset of the sales training\nsales_catcol = ['id', 'item_id', 'dept_id','store_id', 'cat_id', 'state_id']\nsales_numcol = [f\"d_{day}\" for day in range(1,1914)]\nsales_cat_dtype = {col:'category' for col in sales_catcol if col != 'id'}\n\n# load the data\ndf_sales = pd.read_csv(\"..\/input\/m5-forecasting-accuracy\/sales_train_validation.csv\", dtype=sales_cat_dtype)\n\ndf_sales.head()","63a0ffae":"# check if null values in the dataframe\ndf_sales.columns[df_sales.isnull().any()]","3909660c":"# loaded the data of the calendar\ncalendar_catcol = ['weekday', 'event_name_1', 'event_type_1', 'event_name_2', 'event_type_2', 'snap_CA', 'snap_TX', 'snap_WI']\ncalendar_dtype = {col:'category' for col in calendar_catcol}\n\n# load the data\ndf_calendar = pd.read_csv(\"..\/input\/m5-forecasting-accuracy\/calendar.csv\", dtype=calendar_dtype)\n#df_calendar = chg_cat_int(df_calendar, calendar_dtype)\n        \ndf_calendar.head()","db249f62":"# check if null values in the dataframe\ndf_calendar.columns[df_calendar.isnull().any()]","48145803":"# loaded the dataset with information of price for each sku and each week\nprice_catcol = ['store_id', 'item_id']\nprice_dtype = {col:'category' for col in price_catcol}\n\ndf_price = pd.read_csv(\"..\/input\/m5-forecasting-accuracy\/sell_prices.csv\", dtype=price_dtype)\n\ndf_price.head()","550f64c8":"# check if null values in the dataframe\ndf_price.columns[df_price.isnull().any()]","ce2ec271":"# merge all the dataset as a final one for analysis\nsales_catcol = ['id', 'item_id', 'dept_id','store_id', 'cat_id', 'state_id']\nsales_numcol = [f\"d_{day}\" for day in range(1,1914)]\nsales_cat_dtype = {col:'category' for col in sales_catcol if col != 'id'}\n\nfinal_data = df_sales[sales_catcol+sales_numcol]\n\nfinal_data = final_data.melt(id_vars=sales_catcol, value_vars=sales_numcol, var_name='d', value_name='sales')\n\nfinal_data = final_data.merge(df_calendar, on='d')\nfinal_data = final_data.merge(df_price, on=['store_id', 'item_id', 'wm_yr_wk'])\n\n# change category from string to int16\nfinal_data = chg_cat_int(final_data, sales_cat_dtype)\n\nfinal_data.head()","0b1f1d49":"week_day_sales = final_data.groupby(['wday'])['sales'].sum()\nweeklabel = [\"Saturday\", \"Sunday\", \"Monday\", \"Tuesday\", \"Wednesday\", \"Thursday\", \"Friday\"]\n\nmonth_sales = final_data.groupby(['month'])['sales'].sum()\nmonthlabel = ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec']\n\nplt.figure(figsize=(16,4))\nplt.subplot(1, 2, 1)\nplt.bar(week_day_sales.index, week_day_sales.values, width=0.5)\nplt.xticks(week_day_sales.index, weeklabel, rotation=30)\nplt.title('Sales distribution in week days')\n\nplt.subplot(1, 2, 2)\nplt.bar(month_sales.index, month_sales.values, width=0.5)\nplt.xticks(month_sales.index, monthlabel, rotation=30)\nplt.title('Sales distribution in months')","94e56147":"f, axes = plt.subplots(1, 2, figsize=(20, 7))\nsns.countplot(x='event_type_2', hue='event_name_2', data=final_data, ax=axes[0])\n\nsns.countplot(y='event_name_1', data=final_data, ax=axes[1])","925124df":"# delete the dataframe for visualization to save the memory\ndel final_data\ndel week_day_sales\ndel month_sales\ngc.collect()","dfc646fc":"# change the null value as 0 and also update the string category to int\ndf_calendar = chg_cat_int(df_calendar, calendar_dtype)\ndf_calendar.columns[df_calendar.isnull().any()]","a9e2bdbf":"# define the function to shift the column values with lags parameter\ndef shift_days(df, lags=[28, 35, 42, 49]):\n    \n    lag_cols = [f'lag_{lag}' for lag in lags]\n\n    for lag, lag_col in zip(lags, lag_cols):\n        df[lag_col] = df[['id', 'sales']].groupby('id')['sales'].shift(lag)\n\n    df.dropna(inplace=True)\n    \n    return df","367b56cf":"# function to create the 3 dataset with the source data. It can proudced the data by giving the batch size and the start day\n\ndef create_dataset(df_sales, df_calendar, df_price, id_list, batch_size=1000, batch_start=0, start_day=500):\n       \n    total_size = len(id_list)\n    # only select data with the selected batch sku item id \n    if batch_start+batch_size < total_size:\n        batch_id_list = id_list[batch_start:batch_start + batch_size]\n    else:\n        batch_id_list = id_list[batch_start:]\n    \n    #----------------------------------------------\n    # produce training data set\n    sales_catcol = ['id', 'item_id', 'dept_id','store_id', 'cat_id', 'state_id']\n    sales_numcol = [f\"d_{day}\" for day in range(start_day,1914)]\n    sales_cat_dtype = {col:'category' for col in sales_catcol if col != 'id'}\n    \n    batch_train_data = df_sales[sales_catcol+sales_numcol][df_sales['id'].isin(batch_id_list)]\n    batch_train_data = batch_train_data.melt(id_vars=sales_catcol, value_vars=sales_numcol, var_name='d', value_name='sales')\n    \n    batch_train_data = batch_train_data.merge(df_calendar, on='d')\n    batch_train_data = batch_train_data.merge(df_price, on=['store_id', 'item_id', 'wm_yr_wk'])\n    \n    # change category from string to int16\n    batch_train_data = chg_cat_int(batch_train_data, sales_cat_dtype)\n    \n    # give lag parameter. Put the day sales of 1 month ago as the supportive features, and predict today's sales number. \n    # the reason plus 1 month \uff0828 days) is that when preparing the validation dataset, you need to predict the next 28 days, \n    lags = [28, 35, 42, 49]   \n    batch_train_data = shift_days(batch_train_data, lags)\n    #----------------------------------------------\n    # end of produce training data set\n    \n    \n    #----------------------------------------------\n    # produce validate data set\n    lag_numcol = [f\"d_{day}\" for day in range(1914-49,1914)]\n    predict_numcol = [f\"d_{day}\" for day in range(1914,1914+28)]\n  \n    batch_predict_data = df_sales[df_sales['id'].isin(batch_id_list)]\n    batch_predict_data = batch_predict_data[sales_catcol+lag_numcol]\n\n    for num_col in predict_numcol:\n        batch_predict_data[num_col] = 0     #prefill with 0 values\n    \n    batch_predict_data = batch_predict_data.melt(id_vars=sales_catcol, value_vars=lag_numcol+predict_numcol, var_name='d', value_name='sales')\n    batch_predict_data = batch_predict_data.merge(df_calendar, on='d')\n    batch_predict_data = batch_predict_data.merge(df_price, on=['store_id', 'item_id', 'wm_yr_wk'])\n    \n    # change category from string to int16\n    batch_predict_data = chg_cat_int(batch_predict_data, sales_cat_dtype)\n    \n    # give lag parameter. Put the day sales 1 month ago as the training data, and predict today's sales number\n    lags = [28, 35, 42, 49]\n    batch_predict_data = shift_days(batch_predict_data, lags)\n    # end of produce validate data set\n    #----------------------------------------------\n    \n    #----------------------------------------------\n    # produce evaluation data set\n    lag_numcol = [f\"d_{day}\" for day in range(1914-49,1914)]\n    evaluate_numcol = [f\"d_{day}\" for day in range(1914+28,1914+56)]\n  \n    batch_evaluate_data = df_sales[df_sales['id'].isin(batch_id_list)]\n    batch_evaluate_data = batch_evaluate_data[sales_catcol+lag_numcol]\n\n    for num_col in evaluate_numcol:\n        batch_evaluate_data[num_col] = 0\n    \n    batch_evaluate_data = batch_evaluate_data.melt(id_vars=sales_catcol, value_vars=lag_numcol+evaluate_numcol, var_name='d', value_name='sales')\n    batch_evaluate_data = batch_evaluate_data.merge(df_calendar, on='d')\n    batch_evaluate_data = batch_evaluate_data.merge(df_price, on=['store_id', 'item_id', 'wm_yr_wk'])\n    batch_evaluate_data = chg_cat_int(batch_evaluate_data, sales_cat_dtype)\n    \n    # give lag parameter. Put the day sales 1 month ago as the training data, and predict today's sales number\n    lags = [28, 35, 42, 49]\n    batch_evaluate_data = shift_days(batch_evaluate_data, lags)\n    \n    batch_evaluate_data[\"id\"] = batch_evaluate_data[\"id\"].str.replace(\"validation\", \"evaluation\")\n    #----------------------------------------------\n    # end of produce evaluation data set\n    \n    \n    return batch_train_data, batch_predict_data, batch_evaluate_data","e738c06a":"# Define the category features and the numeric features\n\n#cat_features = ['item_id', 'dept_id', 'store_id', 'cat_id', 'state_id', 'event_name_1', 'event_type_1', 'event_name_2', 'event_type_2', 'snap_CA', 'snap_TX', 'snap_WI', 'weekday', #'wday', 'month', 'year']\n#cat_features = ['item_id', 'store_id', 'event_name_1', 'event_type_1', 'event_name_2', 'event_type_2', 'snap_CA', 'snap_TX', 'snap_WI', 'wday', 'month', 'year']\ncat_features = ['item_id', 'store_id', 'event_name_1', 'event_name_2', 'snap_CA', 'snap_TX', 'snap_WI', 'wday', 'month', 'year']\nnum_features = ['sell_price', 'lag_28', 'lag_35', 'lag_42', 'lag_49']","ded15836":"id_list = df_sales['id'].unique()\nid_list = id_list[0:50]  #limit the total id list for KNN due to computational issue\n\nbatch_size = 60             # The size for the number of item id list for each batch\nbatch_start = 0             # The starting place for the item id list batch\n\ntotal_size = len(id_list)   # Total length of the item id list\n\n# The paramter for the data start day, range from 0 to 1913. It means select the data from first_day to the 1913th day for training. \n# Note: small start day would cost large memory and long time\nstart_day = 1000     ","0e3920e1":"%%time\n# create the training dataset and validation dataset\ntrain_data, validate_data, evaluate_data = create_dataset(df_sales, df_calendar, df_price, id_list, batch_size, batch_start, start_day)    \n\nX = train_data[cat_features + num_features]\ny = train_data['sales']\n#X_validate = validate_data[cat_features + num_features]\n#X_evaluate = evaluate_data[cat_features + num_features]\n\n# feed the training data (X, y) and train the model\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n\n# create dummy value for the category features via OneHotEncoder function\ncat_transformer = Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='constant', fill_value=0)),\n    ('onehot', preprocessing.OneHotEncoder(handle_unknown='ignore'))])\n\npreprocessor = ColumnTransformer(\n    transformers=[('cat', cat_transformer, cat_features)])\n\nclf = Pipeline(steps=[('preprocessor', preprocessor),\n                  ('KNN', KNeighborsRegressor())])\n\nk_range = [5, 50, 100, 200]\nweight_options = ['uniform', 'distance']\nmetric_options = ['euclidean', 'minkowski', 'mahalanobis']\n\nparam_grid = {'KNN__n_neighbors':k_range, 'KNN__weights':weight_options, 'KNN__metric':metric_options}\ngrid = GridSearchCV(clf, param_grid, n_jobs=-1)\n\ngrid.fit(X_train, y_train)\n\nprint(\"Best Parameter: \", grid.best_params_)\nprint(\"KNN Model with best parameter training RMSE: %.4f\" %(sqrt(mean_squared_error(y_train, grid.best_estimator_.predict(X_train)))))\nprint(\"KNN Model with best parameter testing RMSE: %.4f\" %(sqrt(mean_squared_error(y_test, grid.best_estimator_.predict(X_test)))))\n","464ea5bb":"# define the function to create the model. The category features would be preprocessed before the training step. \ndef create_model(name, model, X, y):\n    # split the training and testing dataset with testing size is 20%\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n    \n    # create dummy value for the category features via OneHotEncoder function\n    cat_transformer = Pipeline(steps=[\n        ('imputer', SimpleImputer(strategy='constant', fill_value=0)),\n        ('onehot', preprocessing.OneHotEncoder(handle_unknown='ignore'))])\n\n    preprocessor = ColumnTransformer(\n        transformers=[('cat', cat_transformer, cat_features)])\n    \n    clf = Pipeline(steps=[('preprocessor', preprocessor),\n                      ('classifier', model)])\n    \n    clf.fit(X_train, y_train)\n    \n    tr_RMSE = {'model':name, 'RMSE':sqrt(mean_squared_error(y_train, clf.predict(X_train)))}\n    te_RMSE = {'model':name, 'RMSE':sqrt(mean_squared_error(y_test, clf.predict(X_test)))}\n    \n    print(\"model: %s, training RMSE: %.4f\" % (name, tr_RMSE['RMSE']))\n    print(\"model: %s, test RMSE: %.4f\" % (name, te_RMSE['RMSE']))\n    print(\"\")\n    \n    return clf, tr_RMSE, te_RMSE","e95471fb":"%%time\n# dataframe to store the training and test RMSE for each model\ntrain_RMSE = pd.DataFrame()\ntest_RMSE = pd.DataFrame()\n\n# Fit estimators\nESTIMATORS = {\n    \"Random Forest\": RandomForestRegressor(n_estimators=100, max_features=32, random_state=0),\n    \"Linear regression\": LinearRegression(),\n    \"Logistics regression\": LogisticRegression(solver='saga'),\n    \"SVM\": SVR(),}\n\nfor name, estimator in ESTIMATORS.items():\n    \n    clf, tr_RMSE, te_RMSE = create_model(name, estimator, X, y)\n    train_RMSE = train_RMSE.append(tr_RMSE, ignore_index=True)\n    test_RMSE = test_RMSE.append(te_RMSE, ignore_index=True)\n    \ntest_RMSE","21e53423":"id_list = df_sales['id'].unique()\nid_list = id_list[0:1000]  #give larger number of id list for lightGBM model\n\nbatch_size = 1100             # > 1000 to let the batch to run once only\nbatch_start = 0             # The starting place for the item id list batch\n\nstart_day = 600    # give larger start date as lightGBM support large dataset","101ce3e4":"# use the lightGBM model, the category features is not required to performe the OneHotEnconder as the lightGBM have the parameter to indicate the category features. \ndef create_GBMmodel(X, y):\n    # split the training and testing dataset with testing size is 20%\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n    \n    train_data = lgb.Dataset(X_train, label=y_train, categorical_feature=cat_features)\n    validate_data = lgb.Dataset(X_test, label=y_test, categorical_feature=cat_features)\n    \n    params = {\n        \"objective\" : \"poisson\",\n        \"metric\" :\"rmse\",\n        \"force_row_wise\" : True,\n        'verbosity': 1,\n    }\n    \n    del X_train, y_train; gc.collect()\n    \n    num_round = 1500\n    m_lgb = lgb.train(params, train_data, num_round, valid_sets = [validate_data], early_stopping_rounds=5, verbose_eval=200) \n        \n    return m_lgb","73ba7137":"%%time\ntrain_data, validate_data, evaluate_data = create_dataset(df_sales, df_calendar, df_price, id_list, batch_size, batch_start, start_day)    \n    \nX = train_data[cat_features + num_features]\ny = train_data['sales']\n\n# feed the training data (X, y) and train the model\nbst = create_GBMmodel(X, y)  ","e83687b6":"id_list = df_sales['id'].unique()\n\nbatch_size = 20000             # The size for the number of item id list for each batch\nbatch_start = 0             # The starting place for the item id list batch\n\ntotal_size = len(id_list)   # Total length of the item id list\n\n# The paramter for the data start day, range from 0 to 1912. It means select the data from first_day to the 1913th day for training. \n# Note: small start day would cost large memory and long time\nstart_day = 500     ","a757f4a3":"batch_num = 1   # batch number indicator\nvalidate_df = pd.DataFrame() # dataframe used to calculate the uncertainty\nevaluate_df = pd.DataFrame() # dataframe used to calculate the uncertainty\n\nfor val in range(batch_start, total_size, batch_size):\n    \n    # print the batch start time\n    #print('batch %d started from ' % batch_num, datetime.datetime.now())\n    \n    # create the training dataset and validation dataset\n    train_data, validate_data, evaluate_data = create_dataset(df_sales, df_calendar, df_price, id_list, batch_size, val, start_day)    \n    \n    X = train_data[cat_features + num_features]\n    y = train_data['sales']\n    X_validate = validate_data[cat_features + num_features]\n    X_evaluate = evaluate_data[cat_features + num_features]\n    \n    # feed the training data (X, y) and train the model\n    bst = create_GBMmodel(X, y)\n    #bst.save_model('model.txt', num_iteration=bst.best_iteration)\n            \n    # use the trained model to predict the target submission\n    y_validate = bst.predict(X_validate, num_iteration=bst.best_iteration)\n\n    validate_data['sales'] = y_validate\n    \n    result = validate_data[['id', 'd', 'sales']]\n    result = result.pivot(index='id', columns='d', values='sales')\n    result = result.reset_index()\n     \n    out_col = [f'F{d}' for d in range(1, 29)]\n    result.rename(columns=dict(zip(result.columns[1:], out_col)), inplace=True)\n    \n    validate_df = validate_df.append(result, ignore_index=True)\n   \n    \n    # use the trained model to predict the evaluate data\n    y_evaluate = bst.predict(X_evaluate, num_iteration=bst.best_iteration)\n    \n    evaluate_data['sales'] = y_evaluate\n    \n    result2 = evaluate_data[['id', 'd', 'sales']]\n    result2 = result2.pivot(index='id', columns='d', values='sales')\n    result2 = result2.reset_index()\n    \n    result2.rename(columns=dict(zip(result2.columns[1:], out_col)), inplace=True)\n    \n    evaluate_df = evaluate_df.append(result2, ignore_index=True)\n    \n    result = pd.concat([result, result2], axis=0, sort=False)\n    \n    if val == 0:\n        result.to_csv('accuracy-submission.csv', index=False, mode='w')\n    else:\n        result.to_csv('accuracy-submission.csv', index=False, header=False, mode='a')\n        \n    batch_num += 1\n","10ae5221":"validate_df = validate_df.merge(df_sales[[\"id\", \"item_id\", \"dept_id\", \"cat_id\", \"store_id\", \"state_id\"]], on = \"id\")\nvalidate_df['Total'] = 'Total'\nvalidate_df.head()","4c68ff5e":"evaluate_df[\"id\"] = evaluate_df[\"id\"].str.replace(\"evaluation\", \"validation\")\nevaluate_df = evaluate_df.merge(df_sales[[\"id\", \"item_id\", \"dept_id\", \"cat_id\", \"store_id\", \"state_id\"]], on = \"id\")\nevaluate_df['Total'] = 'Total'\nevaluate_df.head()","4646e008":"qs = np.array([0.005,0.025,0.165,0.25, 0.5, 0.75, 0.835, 0.975, 0.995])\n\n# the ratios calculation is learned from the kaggle notebook \"From point to uncertainty prediction\"\nqs2 = np.log(qs\/(1-qs))*.065\n\nratios = stats.norm.cdf(qs2)\nratios \/= ratios[4]\nratios = pd.Series(ratios, index=qs)\nratios","5f159cce":"def quantile_coefs(q):\n    return ratios.loc[q].values","006e99b4":"# define the function to come out the uncertainty distribution with one level grouping\ndef get_group_preds(pred, level, cols):\n    df = pred.groupby(level)[cols].sum()\n    q = np.repeat(qs, len(df))\n    df = pd.concat([df]*9, axis=0, sort=False)\n    df.reset_index(inplace = True)\n    df[cols] *= quantile_coefs(q).reshape(-1, 1)\n    if level != \"id\":\n        df[\"id\"] = [f\"{lev}_X_{q:.3f}_validation\" for lev, q in zip(df[level].values, q)]\n    else:\n        df[\"id\"] = [f\"{lev.replace('_validation', '')}_{q:.3f}_validation\" for lev, q in zip(df[level].values, q)]\n    df = df[[\"id\"]+list(cols)]\n    return df","c88dc17b":"# define the function to come out the uncertainty distribution with two level grouping\ndef get_couple_group_preds(pred, level1, level2, cols):\n    df = pred.groupby([level1, level2])[cols].sum()\n    q = np.repeat(qs, len(df))\n    df = pd.concat([df]*9, axis=0, sort=False)\n    df.reset_index(inplace = True)\n    df[cols] *= quantile_coefs(q).reshape(-1, 1)\n    df[\"id\"] = [f\"{lev1}_{lev2}_{q:.3f}_validation\" for lev1,lev2, q in \n                zip(df[level1].values,df[level2].values, q)]\n    df = df[[\"id\"]+list(cols)]\n    return df","95f9a3dd":"# the levels parameter prepared for grouping calculation\nlevels = [\"id\", \"item_id\", \"dept_id\", \"cat_id\", \"store_id\", \"state_id\", \"Total\"]\ncouples = [(\"state_id\", \"item_id\"),  (\"state_id\", \"dept_id\"),(\"store_id\",\"dept_id\"),\n                            (\"state_id\", \"cat_id\"),(\"store_id\",\"cat_id\")]\ncols = [f\"F{i}\" for i in range(1, 29)]","bc27e03a":"# produce the output file for uncertainty\nuncertainty_df = pd.DataFrame(columns=['id']+list(cols))\nuncertainty_df2 = pd.DataFrame(columns=['id']+list(cols))\n\nfor level in levels:\n    uncertainty_df = uncertainty_df.append(get_group_preds(validate_df, level, cols))\n    uncertainty_df2 = uncertainty_df2.append(get_group_preds(evaluate_df, level, cols))\n\nfor level1,level2 in couples:\n    uncertainty_df = uncertainty_df.append(get_couple_group_preds(validate_df, level1, level2, cols))\n    uncertainty_df2 = uncertainty_df2.append(get_couple_group_preds(evaluate_df, level1, level2, cols))\n    \nuncertainty_df2['id'] = uncertainty_df2[\"id\"].str.replace(\"validation\", \"evaluation\")\n\noutput = pd.concat([uncertainty_df, uncertainty_df2], axis=0, ignore_index=True)\noutput.to_csv(\"uncertainty_submission.csv\", index = False)\noutput.head()","a8bb65ae":"plt.figure(figsize=(15,4))\nplt.subplot(1, 2, 1)\naccuracy_score = [4.21835, 2.61307, 0.77669, 0.73193, 0.70767, 0.68186,0.68060]\nplt.plot(accuracy_score)\nplt.xlabel('number of times')\nplt.ylabel('Weighted Root Mean Squared Scaled Error')\nplt.title('Accuracy score trend')\n\nplt.subplot(1, 2, 2)\nuncertainty_score = [0.24640, 0.22897, 0.21921, 0.21045, 0.20218]\nplt.plot(uncertainty_score)\nplt.xlabel('number of times')\nplt.ylabel('Weighted Scaled Pinball Loss')\nplt.title('Uncertainty score trend')","380311ca":"import matplotlib.image as mpimg\nplt.figure(figsize = (20,2))\nimg=mpimg.imread('..\/input\/score-image\/accuracy-score.png')\nimgplot = plt.imshow(img)\nplt.show()\n\nplt.figure(figsize = (20,2))\nimg=mpimg.imread('..\/input\/score-image\/uncertainty-score.png')\nimgplot = plt.imshow(img)\nplt.show()","ada4e979":"From above result, the Random Forest have the best training result but it's abviously have overfitting issue to come out a worse testing result. Within the 4 models, the Linear Regression model have the better testing score in RMSE metrics. But the RMSE is still not so good from above result. \n\nAnother model lightGBM was used below. Light GBM is a gradient boosting framework that uses tree based learning algorithm. Light GBM is prefixed as \u2018Light\u2019 because of its high speed. Light GBM can handle the large size of data and takes lower memory to run. And this model focuses on accuracy of results. But Light GBM is sensitive to overfitting and can easily overfit small data. Some experience from other people suggest to use it only for data with 10,000+ rows. Since the sales dataset size is huge, it would be good to use lightGBM to predict the result. ","94298b02":"### KNN Regression\n\nFirst, I would use the KNN Regression and give different parameters to find out the best parameters. And there are several category columns, those category features require to conduct the preprocessing of creating dummy values before fit into the model training. The sk-learn OneHotEncoder library was used. ","5f00d498":"## Data Prepration\n\nConsidering the datasize is huge, it would be better to split the dataset to train the model and predict the data by batch. It would reduce the memory consuming and speed up some model's training time with feeding small dataset every time. \n\nFurthermore, apart from those category features, what's the numeric feature we should put in the prediction model? The \"sell_price\" should be required which will affect the sales of particular sku item. And the sales data of last week or last month in the same week day would be also important for the prediction. In below data prepration, it shifts the sales data of last month as the suportive numeric features for the model training. Then it would be sales data of 28, 35, 42, 49 days ago. ","42c599d9":"## Summary\n\nApart from transformation from accuracy to uncertainty shown above, there are other methods to come out the uncertainty data. For example, using the K-fold to perform the model training and prediction, there will be different sets of prediction results. And then use the numpy function to come out distribution of different quantiles. While the dataset size is huge, it will take very long time to come out the final result. \n\nAnd it generated 2 submissions separately for M5-Forecasting Accuracy and M5-Forecasting Uncertainty. Enclosed attached the improvement trend of the score for each submission in both accuracy and uncertainty competition. ","3328ae0c":"The event columns represent the holiday information for the calendar. And different holidays belongs to different holiday type. So for the category features, we don't need the event type but the holiday itself is enough to represent sales during the holiday. ","8a980e69":"This is how to select the features for the model:\n1. Combine the item_id and the store_id to get the unique ID for a SKU. After remove dept_id, cat_id, state_id, the RMSE improve a bit. \n2. Only include event_name as the holiday information, remove event_type. It also improve some on RMSE\n3. Whether have promotion (snap_CA, snap_TX, snap_WI)\n4. Date information (week day, month, year)\n5. Sales price. ","e4550bd5":"It's seen that the Saturday and Sunday have more sales than other days. And also the monthly distribution is different for each month. It's identified that the week days and the month information would be important category features to help the prediction. ","c64eddd8":"Use the best parameter evaluated by GridSearchCV, the testing RMSE is 3.8787. Then in the next stage, I would used other model to see whether there are better model than KNN. ","4ed7fd85":"## Dataset Analysis and Visualization\n\nMerge the dataset into one and analysis\/Visualize the dataset to find out some useful features","dbde027e":"Below is the function to create 3 datasets for 3 different purposes: Training, Validation and Evaluation. It have the parameters to split the total data by batch with item id list. Besides, it also have the start_day parameter to control how many days (start_day->day 1913) data would be selected, large start day will have less data size. With the control of batch size and the start day, it will avoid out of memory error since the original dataset size is huge. \n\nOriginally, I put lags=[7, 14, 21, 28] as supportive features. The evaluation score was 2.61307 in M5 Forecasting-Accuracy score board. While I found out it made the validation and evaluation with a lot of zero values in the numeric features. Then I realized it's required to shift at least 28 days ago. So I put [35, 42, 49, 56] as supportive features, which looking back 28 days to the first setting, then the score had great improvement to 0.77669. Finally I found it should be [28, 35, 42, 49] for last month's sales data. And it come out the score of 0.68186. ","e8d3f7d0":"## Abstract\n\nThe goal is to predict sales data provided by the retail giant Walmart 28 days into the future. This competition will run in 2 tracks: In addition to forecasting the values themselves in the Forecasting competition, we are simultaneously tasked to estimate the uncertainty of our predictions in the Uncertainty Distribution competition. Both competitions will have the same 28 day forecast horizon.\n\nThe data: We are working with 42,840 hierarchical time series. The data were obtained in the 3 US states of California (CA), Texas (TX), and Wisconsin (WI). \u201cHierarchical\u201d here means that data can be aggregated on different levels: item level, department level, product category level, and state level. The sales information reaches back from Jan 2011 to June 2016. In addition to the sales numbers, we are also given corresponding data on prices, promotions, and holidays. Note, that we have been warned that most of the time series contain zero values.\n\nThe data comprises 3049 individual products from 3 categories and 7 departments, sold in 10 stores in 3 states. The hierachical aggregation captures the combinations of these factors. For instance, we can create 1 time series for all sales, 3 time series for all sales per state, and so on. The largest category is sales of all individual 3049 products per 10 stores for 30490 time series.","786cb977":"# M5 Forecasting - Uncertainty","1553101d":"From the result, we can see the model achieve a good RMSE result of 1.997. And it took very short time to complete the training of 1000 items' data. So here I adopted the lightGBM to come out the final submissioin for all the sku items. ","ffe3649f":"## Loading data\n\nThere are 3 spreadsheets storing the data while sales_train_validation.csv is the main dataset that stores all the sales data. Load all the data into the dataframe for further usage. And make sure the category columns are loaded as category type. ","f11d9975":"After produce the accuracy submission, below would be to generate the uncertainty with 9 quartiles. It's going to calculate the distribution from the accuracy submission. The ratios from accuracy to uncertainty distribution was learnt from the Kaggle Notebook \"From point to uncertainty prediction\". ","34b452b2":"Null value found in 4 category columns, will handle the null values after visualization. "}}