{"cell_type":{"b8f0aeec":"code","455da077":"code","c2d2dc4b":"code","2139f524":"code","44e1d34b":"code","a466ea44":"code","03664ff2":"code","588fccf0":"code","23f50b64":"code","db77f84b":"code","1988b06e":"code","eba7bf54":"code","7a6d0190":"code","36134160":"code","d14e5b67":"code","3163882a":"code","2b24c088":"code","4fcf1580":"code","37a7ef14":"code","df697176":"code","0fc6cfe1":"code","fd3c2ed2":"code","8145c186":"code","bfbba884":"code","b0cd0624":"code","63a614d0":"markdown"},"source":{"b8f0aeec":"import pandas as pd\nd = pd.read_csv('..\/input\/nfl-impact-detection\/test_player_tracking.csv')\nIS_PRIVATE = d.shape != (19269, 12)\nprint(IS_PRIVATE)\n\nIS_PRIVATE = True","455da077":"if IS_PRIVATE:\n    !pip install ..\/input\/nfl-lib\/timm-0.1.26-py3-none-any.whl\n    !tar xfz ..\/input\/nfl-lib\/pkgs.tgz\n    # for pytorch1.6\n    cmd = \"sed -i -e 's\/ \\\/ \/ \\\/\\\/ \/' timm-efficientdet-pytorch\/effdet\/bench.py\"\n    !$cmd","c2d2dc4b":"import sys\nsys.path.insert(0, \"timm-efficientdet-pytorch\")\nsys.path.insert(0, \"omegaconf\")\n\nimport torch\nimport os\nfrom datetime import datetime\nimport time\nimport random\nimport cv2\nimport pandas as pd\nimport numpy as np\nimport albumentations as A\nimport matplotlib.pyplot as plt\nfrom albumentations.pytorch.transforms import ToTensorV2\nfrom sklearn.model_selection import StratifiedKFold\nfrom torch.utils.data import Dataset,DataLoader\nfrom torch.utils.data.sampler import SequentialSampler, RandomSampler\nfrom glob import glob\nimport pandas as pd\nimport gc\nfrom effdet import get_efficientdet_config, EfficientDet, DetBenchTrain, DetBenchEval\nfrom effdet.efficientdet import HeadNet\nimport warnings\nfrom tqdm import tqdm\n\nimport seaborn as sns\n\nimport numpy as np\nfrom scipy.optimize import linear_sum_assignment\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as patches\nfrom PIL import Image\n\nwarnings.filterwarnings(\"ignore\")\n\nDATA_ROOT_PATH = 'test_images'\nSEED = 42\n\ndef seed_everything(seed):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\nseed_everything(SEED)","2139f524":"#################\n# SET CONSTANTS\n#################\n\nDETECTION_THRESHOLD = 0.4\nDETECTOR_FILTERING_THRESHOLD = 0.3","44e1d34b":"def iou(bbox1, bbox2):\n    bbox1 = [float(x) for x in bbox1]\n    bbox2 = [float(x) for x in bbox2]\n\n    (x0_1, y0_1, x1_1, y1_1) = bbox1\n    (x0_2, y0_2, x1_2, y1_2) = bbox2\n\n    # get the overlap rectangle\n    overlap_x0 = max(x0_1, x0_2)\n    overlap_y0 = max(y0_1, y0_2)\n    overlap_x1 = min(x1_1, x1_2)\n    overlap_y1 = min(y1_1, y1_2)\n\n    # check if there is an overlap\n    if overlap_x1 - overlap_x0 <= 0 or overlap_y1 - overlap_y0 <= 0:\n            return 0\n\n    # if yes, calculate the ratio of the overlap to each ROI size and the unified size\n    size_1 = (x1_1 - x0_1) * (y1_1 - y0_1)\n    size_2 = (x1_2 - x0_2) * (y1_2 - y0_2)\n    size_intersection = (overlap_x1 - overlap_x0) * (overlap_y1 - overlap_y0)\n    size_union = size_1 + size_2 - size_intersection\n\n    return size_intersection \/ size_union","a466ea44":"def precision_calc(gt_boxes, pred_boxes):\n    cost_matix = np.ones((len(gt_boxes), len(pred_boxes)))\n    for i, box1 in enumerate(gt_boxes):\n        for j, box2 in enumerate(pred_boxes):\n            dist = abs(box1[0]-box2[0])\n            if dist > 4:\n                continue\n            iou_score = iou(box1[1:], box2[1:])\n\n            if iou_score < 0.35:\n                continue\n            else:\n                cost_matix[i,j]=0\n\n    row_ind, col_ind = linear_sum_assignment(cost_matix)\n    fn = len(gt_boxes) - row_ind.shape[0]\n    fp = len(pred_boxes) - col_ind.shape[0]\n    tp=0\n    for i, j in zip(row_ind, col_ind):\n        if cost_matix[i,j]==0:\n            tp+=1\n        else:\n            fp+=1\n            fn+=1\n    return tp, fp, fn","03664ff2":"video_labels = pd.read_pickle(\"..\/input\/nfl-pkl\/video_label_just.pkl\")\nvideo_labels['image_name'] = video_labels['video'].str.replace('.mp4', '') + '_' + video_labels['frame'].astype(str).str.zfill(3) + '.png'\nvideo_labels","588fccf0":"np.random.seed(0)\nvideo_names = np.random.permutation(video_labels.video.str[:12].unique())\nvalid_video_len = int(len(video_names)*0.2)\nvideo_valid = video_names[:valid_video_len]\nvideo_train = video_names[valid_video_len:]\nimages_valid = video_labels[ video_labels.video.str[:12].isin(video_valid)].image_name.unique()\nimages_train = video_labels[~video_labels.video.str[:12].isin(video_valid)].image_name.unique()","23f50b64":"def get_valid_transforms():\n    return A.Compose([\n            A.Resize(height=512, width=512, p=1.0),\n            ToTensorV2(p=1.0),\n        ], p=1.0)","db77f84b":"TRAIN_ROOT_PATH = '..\/input\/nfl-impact-detection-train-frames'\n\nclass DatasetRetriever(Dataset):\n\n    def __init__(self, marking, image_ids, transforms=None, test=False):\n        super().__init__()\n\n        self.image_ids = image_ids\n        self.marking = marking\n        self.transforms = transforms\n        self.test = test\n\n    def __getitem__(self, index: int):\n        image_id = self.image_ids[index]\n        \n        image, boxes, labels = self.load_image_and_boxes(index)\n        \n        target = {}\n        target['boxes'] = boxes\n        target['labels'] = torch.tensor(labels)\n        target['image_id'] = torch.tensor([index])\n        \n        if self.transforms:\n            for i in range(10):\n                sample = self.transforms(**{\n                    'image': image,\n                    'bboxes': target['boxes'],\n                    'labels': labels\n                })\n                if len(sample['bboxes']) > 0:\n                    image = sample['image']\n                    target['boxes'] = torch.stack(tuple(map(torch.tensor, zip(*sample['bboxes'])))).permute(1, 0)\n                    target['boxes'][:,[0,1,2,3]] = target['boxes'][:,[1,0,3,2]]  #yxyx: be warning\n                    break\n        return image, image_id, target\n        \n    def __len__(self) -> int:\n        return self.image_ids.shape[0]\n\n    def load_image_and_boxes(self, index):\n        image_id = self.image_ids[index]\n#         print(f'{TRAIN_ROOT_PATH}\/{image_id}')\n        image = cv2.imread(f'{TRAIN_ROOT_PATH}\/{image_id[:-8]}\/{image_id}', cv2.IMREAD_COLOR).copy().astype(np.float32)\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB).astype(np.float32)\n        image \/= 255.0\n        records = self.marking[self.marking['image_name'] == image_id]\n        boxes = records[['x', 'y', 'w', 'h']].values\n        boxes[:, 2] = boxes[:, 0] + boxes[:, 2]\n        boxes[:, 3] = boxes[:, 1] + boxes[:, 3]\n        labels = records['impact'].values\n        return image, boxes, labels","1988b06e":"def load_net(checkpoint_path):\n    config = get_efficientdet_config('tf_efficientdet_d5')\n    net = EfficientDet(config, pretrained_backbone=False)\n    config.num_classes = 2\n    config.image_size=512\n    net.class_net = HeadNet(config, num_outputs=config.num_classes, norm_kwargs=dict(eps=.001, momentum=.01))\n    checkpoint = torch.load(checkpoint_path)\n    net.load_state_dict(checkpoint['model_state_dict'])\n    net = DetBenchEval(net, config)\n    net.eval();\n    return net.cuda()\nif IS_PRIVATE:\n    net = load_net('..\/input\/nfl-models\/\/best-checkpoint-002epoch.bin')","eba7bf54":"validation_dataset = DatasetRetriever(\n    image_ids=images_valid,\n    marking=video_labels,\n    transforms=get_valid_transforms(),\n    test=True,\n)\n\ndef collate_fn(batch):\n    return tuple(zip(*batch))\n\ndata_loader = DataLoader(\n    validation_dataset,\n    batch_size=16,\n    shuffle=False,\n    num_workers=4,\n    drop_last=False,\n    collate_fn=collate_fn,\n    pin_memory=True\n)","7a6d0190":"def make_predictions(images, score_threshold=0.5):\n    images = torch.stack(images).cuda().float()\n    box_list = []\n    score_list = []\n    with torch.no_grad():\n        det = net(images, torch.tensor([1]*images.shape[0]).float().cuda())\n        for i in range(images.shape[0]):\n            boxes = det[i].detach().cpu().numpy()[:,:4]    \n            scores = det[i].detach().cpu().numpy()[:,4]   \n            label = det[i].detach().cpu().numpy()[:,5]\n            # useing only label = 2\n            indexes = np.where((scores > score_threshold) & (label == 2))[0]\n            boxes[:, 2] = boxes[:, 2] + boxes[:, 0]\n            boxes[:, 3] = boxes[:, 3] + boxes[:, 1]\n            box_list.append(boxes[indexes])\n            score_list.append(scores[indexes])\n    return box_list, score_list\nimport matplotlib.pyplot as plt","36134160":"#check prediction\n\ncnt = 0\nfor images, image_ids, labels in data_loader:\n    box_list, score_list = make_predictions(images, score_threshold=DETECTION_THRESHOLD)\n    for i in range(len(images)):\n        sample = images[i].permute(1,2,0).cpu().numpy()\n        boxes = box_list[i].astype(np.int32).clip(min=0, max=511)\n        scores = score_list[i]\n        if len(scores) >= 1:\n            fig, ax = plt.subplots(1, 1, figsize=(16, 8))\n            sample = cv2.resize(sample , (int(1280), int(720)))\n            for box,score in zip(boxes,scores):\n                box[0] = box[0] * 1280 \/ 512\n                box[1] = box[1] * 720 \/ 512\n                box[2] = box[2] * 1280 \/ 512\n                box[3] = box[3] * 720 \/ 512\n                cv2.rectangle(sample, (box[0], box[1]), (box[2], box[3]), (1, 0, 0), 3)\n            for box in labels[i]['boxes'][labels[i]['labels']==2]:\n                cv2.rectangle(sample, (box[1], box[0]), (box[3], box[2]), (0, 1, 0), 2)\n            ax.set_axis_off()\n            ax.imshow(sample);\n            cnt += 1\n    if cnt >= 10:\n        break","d14e5b67":"result_image_ids = []\nresults_boxes = []\nresults_scores = []\ngt_image_ids = []\ngt_boxes = []\nfor images, image_ids, labels in tqdm(data_loader):\n    box_list, score_list = make_predictions(images, score_threshold=DETECTION_THRESHOLD)\n    for i, image in enumerate(images):\n        boxes = box_list[i]\n        scores = score_list[i]\n        image_id = image_ids[i]\n        boxes[:, 0] = (boxes[:, 0] * 1280 \/ 512)\n        boxes[:, 1] = (boxes[:, 1] * 720 \/ 512)\n        boxes[:, 2] = (boxes[:, 2] * 1280 \/ 512)\n        boxes[:, 3] = (boxes[:, 3] * 720 \/ 512)\n#         boxes[:, 2] = boxes[:, 2] - boxes[:, 0]\n#         boxes[:, 3] = boxes[:, 3] - boxes[:, 1]\n        boxes = boxes.astype(np.int32)\n        boxes[:, 0] = boxes[:, 0].clip(min=0, max=1280-1)\n        boxes[:, 2] = boxes[:, 2].clip(min=0, max=1280-1)\n        boxes[:, 1] = boxes[:, 1].clip(min=0, max=720-1)\n        boxes[:, 3] = boxes[:, 3].clip(min=0, max=720-1)\n        result_image_ids += [image_id]*len(boxes)\n        results_boxes.append(boxes)\n        results_scores.append(scores)\n        gt = labels[i]['boxes'][labels[i]['labels']==2]\n        gt_image_ids += [image_id]*len(gt)\n        gt_boxes.append(gt)\n    for i, image in enumerate(labels):\n        image_id = image_ids[i]","3163882a":"box_df = pd.DataFrame(np.concatenate(gt_boxes), columns=['bot','left','top', 'right' ])\ngt_df = pd.DataFrame({'image_name':gt_image_ids})\ngt_df = pd.concat([gt_df, box_df], axis=1)","2b24c088":"gt_df['gameKey'] = gt_df.image_name.str.split('_').str[0].astype(int)\ngt_df['playID'] = gt_df.image_name.str.split('_').str[1].astype(int)\ngt_df['view'] = gt_df.image_name.str.split('_').str[2]\ngt_df['frame'] = gt_df.image_name.str.split('_').str[3].str.replace('.png','').astype(int)\ngt_df['video'] = gt_df.image_name.str.rsplit('_',1).str[0] + '.mp4'\ngt_df = gt_df[[\"gameKey\",\"playID\",\"view\",\"video\",\"frame\",'left', 'bot', 'right', 'top']]\ngt_df","4fcf1580":"DETECTOR_FILTERING_THRESHOLD = 0.3","37a7ef14":"box_df = pd.DataFrame(np.concatenate(results_boxes), columns=['left', 'bot', 'right', 'top'])\ntest_df = pd.DataFrame({'scores':np.concatenate(results_scores), 'image_name':result_image_ids})\ntest_df = pd.concat([test_df, box_df], axis=1)\ntest_df = test_df[test_df.scores > DETECTOR_FILTERING_THRESHOLD]\ntest_df['gameKey'] = test_df.image_name.str.split('_').str[0].astype(int)\ntest_df['playID'] = test_df.image_name.str.split('_').str[1].astype(int)\ntest_df['view'] = test_df.image_name.str.split('_').str[2]\ntest_df['frame'] = test_df.image_name.str.split('_').str[3].str.replace('.png','').astype(int)\ntest_df['video'] = test_df.image_name.str.rsplit('_',1).str[0] + '.mp4'\ntest_df = test_df[[\"scores\",\"gameKey\",\"playID\",\"view\",\"video\",\"frame\",'left', 'bot', 'right', 'top']]\n# test_df","df697176":"test_df_end = test_df[test_df['view']=='Endzone']\ntest_df_side = test_df[test_df['view']=='Sideline']","0fc6cfe1":"ftp, ffp, ffn = [], [], []\nfor count, video in enumerate(set(gt_df['video'])):\n    pred_boxes = test_df_side[test_df_side['video']==video][[\"frame\",'left', 'bot', 'right', 'top']].to_numpy()\n    gt_boxes = gt_df[gt_df['video']==video][[\"frame\",'left', 'bot', 'right', 'top']].to_numpy()\n    tp, fp, fn = precision_calc(gt_boxes, pred_boxes)\n    ftp.append(tp)\n    ffp.append(fp)\n    ffn.append(fn)\n\ntp = np.sum(ftp)\nfp = np.sum(ffp)\nfn = np.sum(ffn)\nprecision = tp \/ (tp + fp + 1e-6)\nrecall =  tp \/ (tp + fn +1e-6)\nf1_score = 2*(precision*recall)\/(precision+recall+1e-6)\nprint(f'TP: {tp}, FP: {fp}, FN: {fn}, PRECISION: {precision:.4f}, RECALL: {recall:.4f}, F1 SCORE: {f1_score:.4f}')","fd3c2ed2":"#################\n# FILTER\n#################\n\n\ndropIDX = []\nfor keys in test_df.groupby(['gameKey', 'playID']).size().to_dict().keys():\n    tmp_df = test_df.query('gameKey == @keys[0] and playID == @keys[1]')\n    for index, row in tmp_df.iterrows():\n        if row['view'] == 'Endzone':\n            check_df = tmp_df.query('view == \"Sideline\"')\n            if check_df['frame'].apply(lambda x: np.abs(x - row['frame']) <= 4).sum() == 0:\n                dropIDX.append(index)\n        \n        if row['view'] == 'Sideline':\n            check_df = tmp_df.query('view == \"Endzone\"')\n            if check_df['frame'].apply(lambda x: np.abs(x - row['frame']) <= 4).sum() == 0:\n                dropIDX.append(index)","8145c186":"# #################\n# # FILTER\n# #################\n\n\n# dropIDX = []\n# for keys in test_df.groupby(['gameKey', 'playID']).size().to_dict().keys():\n#     tmp_df = test_df.query('gameKey == @keys[0] and playID == @keys[1]')\n    \n#     for index, row in tmp_df.iterrows():\n            \n#         currentFrame = row['frame']\n\n#         bboxCount1 = tmp_df.query('view == \"Sideline\" and abs(frame - @currentFrame) <= 0').shape[0]\n#         bboxCount2 = tmp_df.query('view == \"Endzone\" and abs(frame - @currentFrame) <= 0').shape[0]\n#         if bboxCount1 != bboxCount2:\n#             dropIDX.append(index)","bfbba884":"test_df = test_df.drop(index = dropIDX).reset_index(drop = True)\ntest_df","b0cd0624":"ftp, ffp, ffn = [], [], []\nfor count, video in enumerate(set(gt_df['video'])):\n    pred_boxes = test_df[test_df['video']==video][[\"frame\",'left', 'bot', 'right', 'top']].to_numpy()\n    gt_boxes = gt_df[gt_df['video']==video][[\"frame\",'left', 'bot', 'right', 'top']].to_numpy()\n    tp, fp, fn = precision_calc(gt_boxes, pred_boxes)\n    ftp.append(tp)\n    ffp.append(fp)\n    ffn.append(fn)\n\ntp = np.sum(ftp)\nfp = np.sum(ffp)\nfn = np.sum(ffn)\nprecision = tp \/ (tp + fp + 1e-6)\nrecall =  tp \/ (tp + fn +1e-6)\nf1_score = 2*(precision*recall)\/(precision+recall+1e-6)\nprint(f'TP: {tp}, FP: {fp}, FN: {fn}, PRECISION: {precision:.4f}, RECALL: {recall:.4f}, F1 SCORE: {f1_score:.4f}')","63a614d0":"This notebook is based on https:\/\/www.kaggle.com\/artkulak\/2class-object-detection-inference-with-filtering <br> which is elicited from https:\/\/www.kaggle.com\/its7171\/2class-object-detection-inference\nI added a metric from https:\/\/www.kaggle.com\/nvnnghia\/evaluation-metrics \n\nThe model in this notebook is trained from the dataset which is shuffled regardless of Endzone & Sideline. so it is quite difficult to implement this metric to the model.\n\nIf this was helpful to you, Please Give me upvote, that will be of help to me."}}