{"cell_type":{"23a7cbb1":"code","c2b03e75":"code","5b78e4fe":"code","67f88d05":"code","a6ca0dcd":"code","d73dadf3":"code","7570f476":"code","1b61fada":"code","9b7c6070":"code","93b2c1d5":"code","2ad62536":"code","df88c307":"code","16250aee":"code","cafe6ea0":"code","a8c83a4a":"code","3d96eaf3":"code","4502bde7":"code","0840bceb":"code","a03dbebb":"markdown"},"source":{"23a7cbb1":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","c2b03e75":"train = pd.read_csv('..\/input\/digit-recognizer\/train.csv')\ntest = pd.read_csv('..\/input\/digit-recognizer\/test.csv').values\n","5b78e4fe":"test[:5]","67f88d05":"print(\"shape of the train dataset:\", train.shape)\nprint(\"Shape of the test dataset:\", test.shape)","a6ca0dcd":"X = train.drop(['label'], axis='columns').values\ny = train['label'].values\n\n#Split data into train and validation\nfrom sklearn.model_selection import train_test_split\nX_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2)\nX_train.shape, y_train.shape, X_valid.shape, y_valid.shape","d73dadf3":"X=X.reshape(len(X),28,28,-1)\n\nX_train, X_valid,y_train,y_valid = train_test_split(X,y,test_size=0.2)","7570f476":"from keras.preprocessing.image import ImageDataGenerator\ndata_generation = ImageDataGenerator(\n                featurewise_center=False,\n                samplewise_center=False,\n                featurewise_std_normalization=False,\n                samplewise_std_normalization=False,\n                zca_whitening=False,\n                rotation_range=10,\n                zoom_range=0.1,\n                width_shift_range=0.1,\n                height_shift_range=0.1,\n                horizontal_flip=False,\n                vertical_flip=False)\n\ndata_generation.fit(X_train)","1b61fada":"reduce_lr = tf.keras.callbacks.LearningRateScheduler(lambda x: 1e-3 * 0.5 ** x)","9b7c6070":"model = tf.keras.models.Sequential([\n    tf.keras.layers.Conv2D(64,(3,3), padding='same', activation='relu', input_shape=(28, 28, 1)),\n    tf.keras.layers.BatchNormalization(),\n    tf.keras.layers.MaxPooling2D(),\n    tf.keras.layers.Dropout(0.2),\n    tf.keras.layers.Conv2D(32,(3,3), padding='same', activation='relu'),\n    tf.keras.layers.BatchNormalization(),\n    tf.keras.layers.MaxPooling2D(),\n    tf.keras.layers.Dropout(0.2),\n    tf.keras.layers.Conv2D(64,(3, 3), padding='same', activation='relu'),\n    tf.keras.layers.BatchNormalization(),\n    tf.keras.layers.MaxPooling2D(),#######\n    tf.keras.layers.Dropout(0.2),\n    tf.keras.layers.Conv2D(64,(3,3),padding='same', activation='relu'),\n    tf.keras.layers.BatchNormalization(),\n    tf.keras.layers.Flatten(),\n    tf.keras.layers.Dense(1024, activation='relu'),\n    tf.keras.layers.BatchNormalization(),\n    tf.keras.layers.Dropout(0.2),\n    tf.keras.layers.Dense(512, activation='relu'),\n    tf.keras.layers.BatchNormalization(),\n    tf.keras.layers.Dense(10, activation='softmax')\n    ])\n\noptimizer = tf.keras.optimizers.Adam(lr=0.001)\nmodel.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n\n","93b2c1d5":"%%time\nbatch_size = 64\nepochs = 5\nhistory = model.fit_generator(data_generation.flow(X_train, y_train, batch_size = batch_size), epochs = epochs, \n                              validation_data = (X_valid, y_valid), verbose=2, \n                              steps_per_epoch=X_train.shape[0] \/\/ batch_size,\n                              callbacks = [reduce_lr])","2ad62536":"plt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('Model Loss')\nplt.xlabel('Epochs')\nplt.ylabel('loss')\nplt.legend(['Train', 'Valid'])\nplt.show();","df88c307":"plt.plot(history.history['accuracy'])\nplt.plot(history.history['val_accuracy'])\nplt.title('Model Accuracy')\nplt.xlabel('Epochs')\nplt.ylabel('Accuracy')\nplt.legend(['Train','Valid'])\nplt.show()","16250aee":"model.evaluate(X_valid, y_valid)","cafe6ea0":"from sklearn.metrics import accuracy_score\ny_preds = model.predict(X_valid)\ny_preds=[np.argmax(pred) for pred in y_preds]\naccuracy_score(y_preds, y_valid)","a8c83a4a":"test_df = test.reshape(len(test),28, 28, -1)\ny_test = model.predict((test_df)\/255.0)\n#y_test = model.predict(test_df)\ny_test = [np.argmax(pred) for pred in y_test]","3d96eaf3":"    \nsubmission=pd.read_csv(\"..\/input\/digit-recognizer\/sample_submission.csv\")\nsubmission[\"Label\"]=y_test\nsubmission.head()","4502bde7":"submission.head()","0840bceb":"submission.to_csv(\"final22.csv\",index=False,float_format='%.4f')","a03dbebb":"This model is overfited becouse Val accuracy is higher than train accuracy."}}