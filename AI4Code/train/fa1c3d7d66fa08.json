{"cell_type":{"014ef619":"code","67775407":"code","e14d5a1c":"code","8ece9004":"code","1548f29a":"code","32d34f70":"code","cb1800f8":"code","f5b99b07":"code","a7e91fb3":"code","47f5cb1d":"code","9ce870f1":"code","b28ca481":"code","0f107727":"code","c69bd183":"code","450a8cc6":"code","c22b8d9e":"code","f070d9ec":"code","1b650b21":"code","c55d1397":"code","c5e75a80":"code","fcf862f7":"code","ebd7aedc":"code","ef1690c2":"code","5ed6a414":"code","5ddb18ce":"code","bee7799d":"code","031b731f":"code","095ad28d":"code","72dc9c96":"markdown","5ffa91d1":"markdown","08ce812b":"markdown","671e7350":"markdown","23ed1479":"markdown"},"source":{"014ef619":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.color_palette(\"Set2\")\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","67775407":"data = pd.read_csv('..\/input\/heart-attack-analysis-prediction-dataset\/heart.csv')\ndata.head()","e14d5a1c":"data.info()","8ece9004":"data.isnull().sum()","1548f29a":"data_columns = data.columns.tolist()\ndata_columns","32d34f70":"data.nunique()","cb1800f8":"plt.figure(figsize=(15,10))\ncorr = data.corr()\nsns.heatmap(corr,annot=True)","f5b99b07":"catgory_columns = [cat for cat in data_columns if len(data[cat].unique()) < 6]\nnum_columns = [num for num in data_columns if len(data[num].unique()) > 6]\nprint(catgory_columns)\nprint(num_columns)","a7e91fb3":"plt.figure(figsize=(16,10))\nfor i in range(len(catgory_columns[:-1])):\n    plt.subplot(2,4,i+1)\n    sns.countplot(data=data,x=data[catgory_columns[i]],hue='output')","47f5cb1d":"sns.countplot(x=data['output'],data=data)","9ce870f1":"plt.figure(figsize=(16,10))\nfor i in range(len(num_columns)):\n    plt.subplot(2,3,i+1)\n    sns.kdeplot(data=data,x=data[num_columns[i]],hue='output')","b28ca481":"plt.figure(figsize=(16,10))\nfor i in range(len(num_columns)):\n    plt.subplot(2,3,i+1)\n    sns.boxplot(data=data,x=data['sex'],y=data[num_columns[i]])","0f107727":"data = data.drop(data[data['trtbps'] > 180].index)\ndata = data.drop(data[data['chol'] > 500].index)\nprint(len(data[data['trtbps'] > 180]))\nprint(len(data[data['chol'] > 500]))","c69bd183":"plt.figure(figsize=(18,15))\nfor i in range(len(num_columns)):\n    plt.subplot(2,3,i+1)\n    sns.lineplot(y=data['output'],x=data[num_columns[i]],hue=data['sex'])","450a8cc6":"\npre_data = data.copy()\ndata = pd.get_dummies(data=data,columns=catgory_columns[:-1])\ndata.head()","c22b8d9e":"pre_data","f070d9ec":"from sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nfit_data_num_columns = scaler.fit(data[num_columns])\nfit_pre_data_num_columns = scaler.fit(pre_data[num_columns])\ndata[num_columns] = fit_num_columns.transform(data[num_columns])\npre_data[num_columns] = fit_pre_data_num_columns.transform(pre_data[num_columns])\ndata.head()","1b650b21":"pre_data.head()","c55d1397":"pre_x = pre_data.drop(['output'],axis=1)\npre_y = pre_data['output']\nx = data.drop(['output'],axis=1)\ny = data['output']\npre_x","c5e75a80":"x","fcf862f7":"from sklearn.model_selection import cross_val_score\nfrom sklearn.linear_model import LogisticRegression","ebd7aedc":"np.random.seed(10)\npre_LR = LogisticRegression()\npre_LR_cross = cross_val_score(pre_LR,pre_x,pre_y, cv = 3,scoring = \"recall\")\nprint(pre_LR_cross)\nprint('recall_mean:')\nprint(np.mean(pre_LR_cross)*100)\npre_LR_cross = np.mean(pre_LR_cross)*100","ef1690c2":"LR = LogisticRegression()\nLR_cross = cross_val_score(LR,x,y, cv = 3,scoring = \"recall\")\nprint(LR_cross)\nprint('recall_mean:')\nprint(np.mean(LR_cross) * 100)\nLR_cross = np.mean(LR_cross) * 100","5ed6a414":"from sklearn.svm import SVC\npre_svc = SVC()\npre_svc_cross = cross_val_score(pre_svc,pre_x,pre_y, cv = 3,scoring = \"recall\")\nprint(pre_svc_cross)\nprint('recall_mean:')\nprint(np.mean(pre_svc_cross)*100)\npre_svc_cross = np.mean(pre_svc_cross)*100","5ddb18ce":"svc = SVC()\nsvc_cross = cross_val_score(svc,x,y, cv = 3,scoring = \"recall\")\nprint(svc_cross)\nprint('recall_mean:')\nprint(np.mean(svc_cross)*100)\nsvc_cross = np.mean(svc_cross)*100","bee7799d":"from xgboost import XGBClassifier\nimport warnings\nwarnings.filterwarnings(\"ignore\")\npre_xgb = XGBClassifier()\npre_xgb_cross = cross_val_score(pre_xgb,pre_x,pre_y, cv = 3,scoring = \"recall\")\nprint(pre_xgb_cross)\nprint('recall_mean:')\nprint(np.mean(pre_xgb_cross)*100)\npre_xgb_cross = np.mean(pre_xgb_cross)*100","031b731f":"xgb = XGBClassifier()\nxgb_cross = cross_val_score(xgb,x,y, cv = 3,scoring = \"recall\")\nprint(xgb_cross)\nprint('recall_mean:')\nprint(np.mean(xgb_cross)*100)\nxgb_cross = np.mean(xgb_cross)*100\n","095ad28d":"model_data_reall = {\"data_no_one_hot_recall\":[pre_LR_cross,pre_svc_cross,pre_xgb_cross],\"data_one_hot_recall\":[LR_cross,svc_cross,xgb_cross]}\nmodel_score = pd.DataFrame(model_data_reall, index=['LR','SVC','XGB'])\nmodel_score","72dc9c96":"# XGboost","5ffa91d1":"# Divide the data into two parts\n# one is one-hot encoding\n# the other is one-hot encoding not\n# check the effect of the model","08ce812b":"1. The highest score for the model is svc \uff1arecall 92.6%\n2. The bottom score of the model is svc\uff1a recall 84.1%\n3. There is not much difference in LR model scores\n4. I hope you all experts can advise me a lot","671e7350":"# SVM","23ed1479":"#     LR"}}