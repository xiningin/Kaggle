{"cell_type":{"5a5b81e0":"code","ba5f6ea5":"code","35c37b7f":"code","0bfc5891":"code","e87ca580":"code","3244f320":"code","b8d04169":"code","5081606a":"code","95ac4eb2":"code","35ca3c30":"code","a4c0a6b3":"code","2a03d527":"code","d2e16df6":"code","2e0f5ee0":"code","a49f8021":"code","3f2bbc7d":"code","d250d1ea":"code","d3a6f519":"code","b3d503b3":"code","4388eea8":"code","302e38c2":"code","4742de02":"code","01d9a476":"markdown","d4a924c1":"markdown","1d73ad51":"markdown","7a947052":"markdown","9e933234":"markdown","ea7eb86b":"markdown","f6b5d7f3":"markdown","25000c3a":"markdown"},"source":{"5a5b81e0":"import pandas as pd\nimport numpy as np\nimport matplotlib.pylab as plt\n# For reading files.\nfrom glob import glob\nimport os\nplt.style.use('ggplot')\nimport pathlib\nfrom tqdm.notebook import tqdm","ba5f6ea5":"train = pd.read_parquet('..\/input\/kaggle-pog-series-s01e01\/\/train.parquet')\ntest = pd.read_parquet('..\/input\/kaggle-pog-series-s01e01\/\/test.parquet')\nss = pd.read_csv('..\/input\/kaggle-pog-series-s01e01\/sample_submission.csv')","35c37b7f":"# Category Mapping Names\ncategory_id_map = {\n    1: \"Film & Animation\",\n    2: \"Autos & Vehicles\",\n    10: \"Music\",\n    15: \"Pets & Animals\",\n    17: \"Sports\",\n    19: \"Travel & Events\",\n    20: \"Gaming\",\n    22: \"People & Blogs\",\n    23: \"Comedy\",\n    24: \"Entertainment\",\n    25: \"News & Politics\",\n    26: \"Howto & Style\",\n    27: \"Education\",\n    28: \"Science & Technology\",\n    29: \"Nonprofits & Activism\",\n}\n\ntrain['category_name'] = train['categoryId'].map(category_id_map)\ntest['category_name'] = test['categoryId'].map(category_id_map)","0bfc5891":"# Identifies the location of the thumbnail image\nimgs_fns = glob('..\/input\/kaggle-pog-series-s01e01\/thumbnails\/*.jpg')\nvideo_ids = [v.split('\/')[-1].strip('.jpg') for v in imgs_fns]\n\nthumb_map = {}\nfor i in range(len(video_ids)):\n    thumb_map[video_ids[i]] = imgs_fns[i]\n    \ntrain['thumbnail_jpg'] = train['video_id'].map(thumb_map)\ntest['thumbnail_jpg'] = test['video_id'].map(thumb_map)","e87ca580":"examples = train.query('has_thumbnail').sample(9, random_state=529)\n\nfig, axs = plt.subplots(3, 3, figsize=(20, 20))\naxs = axs.flatten()\nax_idx = 0\nfor i, e in examples.iterrows():\n    img = plt.imread(e['thumbnail_jpg'])\n    axs[ax_idx].imshow(img)\n    etitle = e['title']\n    etarget = e['target']\n    ecatname = e['category_name']\n    axs[ax_idx].set_title(f'{ecatname} \\n Target: {etarget:0.6f}')\n    axs[ax_idx].grid(False)\n    axs[ax_idx].set_xticks([])\n    axs[ax_idx].set_yticks([])\n    ax_idx += 1","3244f320":"examples = train.query('category_name == \"Entertainment\"') \\\n    .query('has_thumbnail').sample(9, random_state=529)\n\nfig, axs = plt.subplots(3, 3, figsize=(20, 15))\naxs = axs.flatten()\nax_idx = 0\nfor i, e in examples.iterrows():\n    img = plt.imread(e['thumbnail_jpg'])\n    axs[ax_idx].imshow(img)\n    etitle = e['title']\n    etarget = e['target']\n    ecatname = e['category_name']\n    axs[ax_idx].set_title(f'{ecatname} \\n Target: {etarget:0.6f}')\n    axs[ax_idx].grid(False)\n    axs[ax_idx].set_xticks([])\n    axs[ax_idx].set_yticks([])\n    ax_idx += 1","b8d04169":"def create_features(df, train=True):\n    \"\"\"\n    Adds features to training or test set.\n    \"\"\"\n    df['publishedAt'] = pd.to_datetime(df['publishedAt'])\n    df['trending_date'] = pd.to_datetime(df['trending_date'], utc=True)\n    \n    # Feature 1 - Age of video\n    df['video_age_seconds'] = (df['trending_date'] - df['publishedAt']) \\\n        .dt.total_seconds().astype('int')\n    \n    # Trending day of week As a category\n    df['trending_dow'] = df['trending_date'].dt.day_name()\n    df['trending_dow']= df['trending_dow'].astype('category')\n    \n    df['published_dow'] = df['publishedAt'].dt.day_name()\n    df['published_dow']= df['published_dow'].astype('category')\n    \n    df['categoryId'] = df['categoryId'].astype('category')\n    \n    df['channel_occurance'] = df['channelId'].map(\n        df['channelId'].value_counts().to_dict())\n\n    df['channel_unique_video_count'] = df['channelId'].map(\n        df.groupby('channelId')['video_id'].nunique().to_dict())\n    \n    df['video_occurance_count'] = df.groupby('video_id')['trending_date'] \\\n        .rank().astype('int')\n    \n    return df","5081606a":"# Make Train\/Test DataFrame\ntrain['isTest'] = False\ntest['isTest'] = True\ntt = pd.concat([train, test]).reset_index(drop=True).copy()\n\ntrain['duration_seconds'] = train['duration_seconds'] \\\n    .fillna(tt['duration_seconds'].median()).astype('int')\ntest['duration_seconds'] = test['duration_seconds'] \\\n    .fillna(tt['duration_seconds'].median()).astype('int')\ntt = pd.concat([train, test]).reset_index(drop=True).copy()\ntt = create_features(tt)\ntt['description'] = tt['description'].fillna(' ')\ntrain_ = tt.query('isTest == False').reset_index(drop=True).copy()\ntest_ = tt.query('isTest == True').reset_index(drop=True).copy()\n\ntrain_['view_count'] = train_['view_count'].astype('int')\ntrain_['likes'] = train_['likes'].astype('int')\ntrain_['dislikes'] = train_['dislikes'].astype('int')\ntrain_['comment_count'] = train_['comment_count'].astype('int')","95ac4eb2":"import tensorflow as tf\nimport cv2\n\ndef _str_to_bytes_feature(value):\n    value = str.encode(value)\n    \"\"\"Returns a bytes_list from a string \/ byte.\"\"\"\n    if isinstance(value, type(tf.constant(0))):\n        value = value.numpy() # BytesList won't unpack a string from an EagerTensor.\n    return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\n\ndef _bytes_feature(value):\n    \"\"\"Returns a bytes_list from a string \/ byte.\"\"\"\n    if isinstance(value, type(tf.constant(0))):\n        value = value.numpy() # BytesList won't unpack a string from an EagerTensor.\n    return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\n\ndef _float_feature(value):\n    \"\"\"Returns a float_list from a float \/ double.\"\"\"\n    return tf.train.Feature(float_list=tf.train.FloatList(value=[value]))\n\ndef _int64_feature(value):\n    \"\"\"Returns an int64_list from a bool \/ enum \/ int \/ uint.\"\"\"\n    return tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))\n\n\n\ndef serialize_example(img, **kwargs):\n    feature = {\n      'image': _bytes_feature(img),\n      'video_id': _str_to_bytes_feature(kwargs['video_id']),\n      'title': _str_to_bytes_feature(kwargs['title']),\n#         'publishedAt' : _str_to_bytes_feature(kwargs['publishedAt']),\n        'channelId' : _str_to_bytes_feature(kwargs['channelId']),\n        'channelTitle' : _str_to_bytes_feature(kwargs['channelTitle']),\n        'categoryId' : _int64_feature(kwargs['categoryId']),\n#         'trending_date' : _str_to_bytes_feature(kwargs['trending_date']),\n        'tags' : _str_to_bytes_feature(kwargs['tags']),\n\n        'description' : _str_to_bytes_feature(kwargs['description']),\n        'id' : _str_to_bytes_feature(kwargs['id']),\n        'duration_seconds' : _int64_feature(kwargs['duration_seconds']),\n        'category_name' : _str_to_bytes_feature(kwargs['category_name']),\n        'video_age_seconds' : _int64_feature(kwargs['video_age_seconds']),\n        'trending_dow' : _str_to_bytes_feature(kwargs['trending_dow']),\n        'published_dow' : _str_to_bytes_feature(kwargs['published_dow']),\n        'channel_occurance' : _int64_feature(kwargs['channel_occurance']),\n        'channel_unique_video_count' : _int64_feature(kwargs['channel_unique_video_count']),\n        'video_occurance_count' : _int64_feature(kwargs['video_occurance_count'])\n    }\n    if kwargs['target'] is not None:\n        feature['target'] = _float_feature(kwargs['target'])\n        \n        feature['view_count'] = _int64_feature(kwargs['view_count']),\n        feature['likes'] = _int64_feature(kwargs['likes']),\n        feature['dislikes'] = _int64_feature(kwargs['dislikes']),\n        feature['comment_count'] = _int64_feature(kwargs['comment_count']),\n        \n    example_proto = tf.train.Example(features=tf.train.Features(feature=feature))\n    return example_proto.SerializeToString()","35ca3c30":"# Make Train and test directory\nos.makedirs('train', exist_ok=True)\nos.makedirs('test', exist_ok=True)","a4c0a6b3":"for i, d in tqdm(train_.query('has_thumbnail == True').iterrows(),\n                 total=len(train_.query('has_thumbnail == True'))):\n    example_id = d['id']\n    with tf.io.TFRecordWriter(f'train\/{example_id}.tfrec') as writer:\n        img = cv2.imread(d['thumbnail_jpg'])\n        img = cv2.resize(img,(360,480))\n        img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)\n        img = cv2.imencode('.jpg', img,\n                           (cv2.IMWRITE_JPEG_QUALITY, 94))[1].tobytes()\n        example = serialize_example(img,\n            **d.to_dict(),\n        )\n        writer.write(example)","2a03d527":"counter = 0\nfor i, d in tqdm(test_.query('has_thumbnail == True').iterrows(),\n                 total=len(test_.query('has_thumbnail == True'))):\n    example_id = d['id']\n    with tf.io.TFRecordWriter(f'test\/{example_id}.tfrec') as writer:\n        img = cv2.imread(d['thumbnail_jpg'])\n        img = cv2.resize(img,(360,480))\n        img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)\n        img = cv2.imencode('.jpg', img,\n                           (cv2.IMWRITE_JPEG_QUALITY, 94))[1].tobytes()\n        d['target'] = None\n        example = serialize_example(img,\n            **d.to_dict(),\n        )\n        writer.write(example)","d2e16df6":"# !ls train\/ -l","2e0f5ee0":"filenames = glob('train\/*.tfrec')\nraw_dataset = tf.data.TFRecordDataset(filenames)\nraw_dataset","a49f8021":"# import tensorflow as tf\n\n# from tensorflow import keras\n# from tensorflow.keras import layers\n\n# print(tf.__version__)","3f2bbc7d":"# data_dir = '..\/input\/kaggle-pog-series-s01e01\/thumbnails\/'\n# data_dir = pathlib.Path(data_dir)\n# img_height = img.shape[0]\n# img_width = img.shape[1]\n# batch_size = 4\n\n# train_fns = train['thumbnail_jpg'].dropna().unique().tolist()\n# list_ds = tf.data.Dataset.list_files(train_fns,\n#                                      shuffle=False)\n\n# image_count = len(train_fns)\n# list_ds = list_ds.shuffle(image_count, reshuffle_each_iteration=False)\n\n# val_size = int(image_count * 0.2)\n# train_ds = list_ds.skip(val_size)\n# val_ds = list_ds.take(val_size)\n\n# len(train_ds), len(val_ds)","d250d1ea":"# video_target_map = train.groupby('video_id')['target'].mean().to_dict()\n\n# def get_label(file_path):\n# #     video_id = file_path.split('\/')[-1].strip('.jpg')\n# #     video_id = tf.strings.split(file_path, os.path.sep)[-1].numpy().decode(\"utf-8\").strip('.jpg')\n#     print(file_path)\n#     fn = tf.strings.split(file_path, os.path.sep)[-1]\n#     print(fn)\n#     video_id = tf.strings.split(fn, '.')[0]\n#     print(video_id.ref())\n#     target = video_target_map[video_id.ref()]\n#     return tf.convert_to_tensor(target)\n\n# def decode_img(img):\n#     # Convert the compressed string to a 3D uint8 tensor\n#     img = tf.io.decode_jpeg(img, channels=3)\n#     # Resize the image to the desired size\n#     return tf.image.resize(img, [img_height, img_width])\n\n# def process_path(file_path):\n#     label = get_label(file_path)\n#     # Load the raw data from the file as a string\n#     img = tf.io.read_file(file_path)\n#     img = decode_img(img)\n#     print(img.shape, label.shape)\n#     return img, label","d3a6f519":"# # Set `num_parallel_calls` so multiple images are loaded\/processed in parallel.\n# train_ds_ = train_ds.map(process_path, num_parallel_calls=AUTOTUNE)\n# val_ds_ = val_ds.map(process_path, num_parallel_calls=AUTOTUNE)","b3d503b3":"# num_classes = 5\n\n# model = tf.keras.Sequential([\n#   tf.keras.layers.Rescaling(1.\/255, input_shape=(img_width, img_height, 3)),\n#   tf.keras.layers.Conv2D(32, 3, activation='relu'),\n#   tf.keras.layers.MaxPooling2D(),\n#   tf.keras.layers.Conv2D(32, 3, activation='relu'),\n#   tf.keras.layers.MaxPooling2D(),\n#   tf.keras.layers.Conv2D(32, 3, activation='relu'),\n#   tf.keras.layers.MaxPooling2D(),\n#   tf.keras.layers.Flatten(),\n#   tf.keras.layers.Dense(128, activation='relu'),\n#   tf.keras.layers.Dense(1)\n# ])\n\n# model.compile(\n#   optimizer='adam',\n#   loss=tf.losses.MeanAbsoluteError(),\n#   metrics=['mae'])","4388eea8":"# model.summary()","302e38c2":"# tf.keras.utils.plot_model(model, show_shapes=True)\n","4742de02":"# model.fit(\n#   train_ds,\n#   validation_data=val_ds,\n#   epochs=3\n# )","01d9a476":"## TFRecords for Test","d4a924c1":"## Check out the images.","1d73ad51":"# Checkout some Random Training Images.","7a947052":"# Just A Single Category","9e933234":"# Make TFRecords\nTFRecords make training a tensorflow model much easier. It combines the data in a way that tensorflow can pull easily.","ea7eb86b":"# Youtube Thumbnail EDA & Model","f6b5d7f3":"# Modeling using Tensorflow to Predict our Target","25000c3a":"## TFrecords for train"}}