{"cell_type":{"5b66edae":"code","cc63a114":"code","51ec6fff":"code","dd6b9ce7":"code","6e4eaa74":"code","f8fb9e89":"code","7da7175c":"code","b54838fd":"code","6e9b432f":"code","e5a3f043":"code","ef9127cd":"code","44e35e5e":"code","c08d0ee9":"code","bc9dd343":"code","aa376d89":"code","45847a60":"code","3ed6c705":"code","3273bd4c":"code","44cd62ee":"code","871f9be9":"code","2690bbe9":"code","6d67cf2a":"code","08ce8a0b":"code","54496ac4":"code","e002023a":"code","047ea90f":"code","b0784301":"code","427e1727":"code","1c733d70":"code","110fcc54":"code","554dcfff":"code","95e27d21":"code","54f2b5e3":"code","6216f75e":"code","81882bee":"code","4d2a77f3":"markdown","647966b0":"markdown","ee5902ae":"markdown","a2d7a3b7":"markdown","3abc4b7e":"markdown","06009ec5":"markdown","f45774ff":"markdown","4923b20b":"markdown","4de7a0b3":"markdown","ec6a9057":"markdown","8eaab6e3":"markdown","89172954":"markdown","358c8a58":"markdown","4538082c":"markdown","1efa6425":"markdown"},"source":{"5b66edae":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","cc63a114":"#import additional libraries\nimport seaborn as sns\nimport matplotlib.pyplot as plt","51ec6fff":"#read the dataset\ndf = pd.read_csv(\"\/kaggle\/input\/online-shoppers-intention\/online_shoppers_intention.csv\")","dd6b9ce7":"#first few rows of the dataframe\ndf.head()","6e4eaa74":"df.isnull().sum()","f8fb9e89":"#drop the null values\ndf.dropna(axis = 0, how = \"any\", inplace = True)","7da7175c":"#check for null values again\ndf.isnull().sum()","b54838fd":"#describe the dataframe\ndf.describe().T","6e9b432f":"#Count of Revenue\nsns.countplot(df.Revenue, palette = 'seismic_r')","e5a3f043":"#pie chart for revenue\nlabels = ['False', 'True']\nplt.title(\"Revenue\")\nplt.pie(df.Revenue.value_counts(), labels = labels, autopct = '%.4f%%')\nplt.legend()","ef9127cd":"#different users\nsns.countplot(x = df.VisitorType)","44e35e5e":"#different regions\nsns.countplot(df.Region)","c08d0ee9":"#Operating system wrt Revenue\nsns.countplot(df.OperatingSystems, hue = df.Revenue)","bc9dd343":"#Revenue for each region\nsns.countplot(df.Region, hue = df.Revenue)","aa376d89":"#Revenue with respect to Weekend\nsns.countplot(df.Weekend, hue = df.Revenue)","45847a60":"plt.figure(figsize = (10,10))\nsns.heatmap(df.corr())","3ed6c705":"#check datatypes\ndf.dtypes","3273bd4c":"#split into X and y\nX = df.iloc[:,6:-1].values\ny = df.iloc[:,-1].values","44cd62ee":"#label encode the objects\nfrom sklearn.preprocessing import LabelEncoder, OneHotEncoder\nle = LabelEncoder()\ny = le.fit_transform(y)\nX[:,4] = le.fit_transform(X[:,4])\nX[:,9] = le.fit_transform(X[:,9])\nX[:,10] = le.fit_transform(X[:,10])","871f9be9":"#split into train and test set\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)","2690bbe9":"from sklearn.linear_model import LogisticRegression\nclassifier = LogisticRegression(solver = 'lbfgs')\nclassifier.fit(X_train, y_train)","6d67cf2a":"#predict the values\ny_pred = classifier.predict(X_test)","08ce8a0b":"from sklearn.metrics import classification_report\nprint(classification_report(y_test, y_pred))","54496ac4":"from sklearn.neighbors import KNeighborsClassifier\nclassifier = KNeighborsClassifier(metric = 'minkowski', p = 2, n_neighbors = 5)\nclassifier.fit(X_train, y_train)","e002023a":"#predict the values\ny_pred = classifier.predict(X_test)","047ea90f":"#print report\nprint(classification_report(y_test, y_pred))","b0784301":"from sklearn.svm import SVC\nclassifier = SVC(kernel = 'rbf', random_state = 0)\nclassifier.fit(X_train, y_train)","427e1727":"#predict the values\ny_pred = classifier.predict(X_test)","1c733d70":"#print report\nprint(classification_report(y_test, y_pred))","110fcc54":"from sklearn.naive_bayes import GaussianNB\nclassifier = GaussianNB()\nclassifier.fit(X_train, y_train)","554dcfff":"#predict the values\ny_pred = classifier.predict(X_test)","95e27d21":"#print report\nprint(classification_report(y_test, y_pred))","54f2b5e3":"from sklearn.ensemble import RandomForestClassifier\nclassifier = RandomForestClassifier(n_estimators = 100, criterion = 'entropy', random_state = 0)\nclassifier.fit(X_train, y_train)","6216f75e":"#predict the values\ny_pred = classifier.predict(X_test)","81882bee":"#print report\nprint(classification_report(y_test, y_pred))","4d2a77f3":"The accuracy of the Random Forest Classifier is also 89%. \n\nThus by comparing the Precision and the f1 - score we can chose the Random Forest Classifier \n\n(To be honest, both are equally good for the given dataset.)","647966b0":"4. **Naive Bayes**","ee5902ae":"Now, lets visualize the data.","a2d7a3b7":"The accuracy is 89%.\n\nJust 1 more than the Logistic and KNN Classifiers!","3abc4b7e":"3. **Support Vector Classifier**","06009ec5":"2.** KNN Classifier**","f45774ff":"![breathtaking-online-shopping-statistics-you-never-knew-1250x600.png](attachment:breathtaking-online-shopping-statistics-you-never-knew-1250x600.png)\n\nThis notebook looks at the features that comprise the dataset and define the revenue of the online store based on various features.\n\nWe shall see how the data is spread in the dataset with visualizations and then procedd to determine the features that most affect the Revenue.\n\nOnce the features are determined we shall use these features to train the following models and determine the best model for classification.\n\n1. Logistical Regression Classifier\n2. KNN Classifier\n3. SVC\n4. Naive Bayes\n5. Random Forest Classifier\n\n*Now, lets begin!*","4923b20b":"Lets begin the Classification","4de7a0b3":"Naive Bayes surprisingly has a lower accuracy of 78%","ec6a9057":"5. **Random Forest Classifier**","8eaab6e3":"Accuracy of Logistic Classifier is 88%","89172954":"From the figure it can be determined that the features are all the columns from the BounceRates column.","358c8a58":"1. **LOGISTIC REGRESSION**","4538082c":"The accuracy of the KNN model is 88% as well.","1efa6425":"Now lets check the correlation of the features with respect to Revenue to know the best features to select for modeling."}}