{"cell_type":{"5fcd13fb":"code","fde880cb":"code","49c0642f":"code","ba7eb18d":"code","4104a295":"code","486b16c5":"code","1a591818":"code","dfbb5384":"code","887aa233":"code","c259d2ad":"code","3ec50e63":"code","c6b5a179":"code","a19304c0":"code","bd9315d8":"code","781b1d02":"code","68da3400":"code","0f5c979a":"code","30d710b6":"code","a7700233":"code","004929a5":"code","75d290de":"code","679d9edd":"code","f0b2d308":"code","07c9abfb":"code","e4f73cc5":"code","b926f7a4":"code","8718637a":"code","564ea9b6":"code","5a25534b":"code","dac811c1":"code","3ca865b4":"code","f6cff929":"code","3d246fbc":"code","8eab6fae":"code","46db59e3":"markdown","b8bfe1c4":"markdown","c7d4a6db":"markdown","aaefdcb9":"markdown","57ac9216":"markdown","2d1bf1b4":"markdown","88f21039":"markdown","63df3e84":"markdown","29ea7da3":"markdown","767253ba":"markdown","c86f2c7f":"markdown","896fe20f":"markdown","9b5f2e5a":"markdown","1d50b00e":"markdown","2ded46ef":"markdown","0239b008":"markdown","392ccf82":"markdown","a36218f1":"markdown","78706f6c":"markdown","494deed7":"markdown","89fab578":"markdown","c8f47b8b":"markdown","df5b9949":"markdown","1a0e67e1":"markdown","5efc40f5":"markdown","6a413659":"markdown","13607d4b":"markdown","85e740b2":"markdown","11220e73":"markdown","31e96991":"markdown","5155c969":"markdown","4f11b3b6":"markdown","dbf8bd00":"markdown","c96e7fd6":"markdown","d99b7be8":"markdown","176c37ea":"markdown","4171471d":"markdown","f2902105":"markdown","42bb451a":"markdown","3a1883d5":"markdown","1d21b724":"markdown","363a15ea":"markdown","f97103fd":"markdown","0d400799":"markdown"},"source":{"5fcd13fb":"!pip install yfinance","fde880cb":"import matplotlib.pyplot as plt\nimport pandas as pd\nimport yfinance as yf\n\n# explicitly require this experimental feature\nfrom sklearn.experimental import enable_iterative_imputer  # noqa\n# now you can import normally from sklearn.impute\nfrom sklearn.impute import IterativeImputer","49c0642f":"tick = yf.Ticker(\"GOOGL\")\ngoogl = tick.history(period = \"60d\", interval = \"30m\")","ba7eb18d":"googl","4104a295":"def splitDataByDates(df):\n    \"\"\"\n    Description\n    -----------\n    This function will split the dataframe returned by `yfinance` by days. \n    For example, if your `period` was set to \"60d\", then you'll get 60\n    individual dataframes which will be stored in `the_data`.\n    \n    NOTE: \n    Use this function only when you have set an `interval` to an \n    intraday value. \n    Valid intraday intervals are: 1m, 2m, 5m, 15m, 30m, 60m,and 90m.\n    \n    Parameters\n    ----------\n    df (dataframe):\n        The dataframe returned from yfinance.\n    \n    Returns\n    -------\n    the_dates (list):\n        A list of all the dates in the dataframe returned from yfinance.\n    \n    the_data (list of dataframes):\n        A list of dataframes. Each date will have it's own dataframe.\n        \n    \"\"\"\n    # The returned lists\n    the_dates = []\n    the_data = []\n    \n    # Splits the data retrieved from yfinance by date\n    for group in df.groupby(df.index.date):\n        the_dates.append(group[0])\n        the_data.append(group[1])\n    return the_dates, the_data","486b16c5":"dates, data = splitDataByDates(googl)","1a591818":"dates[:10]","dfbb5384":"data[0]","887aa233":"def makeTimesDict(the_data):\n    \"\"\"\n    Description\n    -----------\n    This will create an empty dictionary and set the keys equal to\n    all the time interval in the `Datetime` index of `the_data`,\n    which was retrieved after running `splitDataByDates()`.\n    This will also give the created dictionary a 'Date' key to hold\n    all of the dates. This dictionary is required if you've retrieved\n    data from yfinance with an intraday interval.\n    \n    Parameters\n    ----------\n    the_data (list of dataframes):\n        The list of dataframes which was retrieved after running the\n        function `splitDataByDates()`.\n    \n    Returns\n    -------\n    adict (dict):\n        A dictionary with the appropriate keys.\n        \n    \"\"\"\n    # Create an empty dictionary\n    adict = {}\n    # Give dictionary a 'Date' key\n    adict['Date'] = []\n    \n    # For every time listed in the index of each dataframes in`the_data`,\n    # make that time a key for the dictionary.\n    for i in the_data[0].index:\n        adict[str(i.time())] = []\n    return adict","c259d2ad":"dic = makeTimesDict(data)\ndic","3ec50e63":"print(*data[0].index.time, sep='\\n') # All times in first dataframe","c6b5a179":"def appendValues(adict, the_dates, the_data, use='Close'):\n    \"\"\"\n    Description\n    -----------\n    This will append the appropriate values from each dataframe\n    in `the_data` to the dictionary `adict` based on the `use`\n    case. The default use case is 'Close'. This will also append \n    the dates from `the_dates` to the dictionary.\n    \n    Parameters\n    ----------\n    adict (dict):\n        The dictionary with set keys retrieved after running\n        the function `makeTimesDict()`.\n    \n    the_dates (list):\n        The list of dates retrieved after running the function\n        `splitDataByDates()`.\n        \n    the_data (list of dataframes):\n        The list of dataframes which was retrieved after running\n        the function `splitDataByDates()`.\n    \n    use (str):\n        The column from the data returned from yFinance of which \n        you want to retrieve data from. The valid `use` cases\n        are: 'Open', 'High', 'Low', or 'Close'.\n        The default `use` case is set to 'Close'.\n        \n    Returns\n    -------\n    Nothing.\n    \n    Examples\n    --------\n    `appendValues(dic, dates, data, use='Open')` will append all\n    \"Open\" values from `data` to `dic`, and will append all dates\n    from the list `dates` to `dic`.\n    \n    \"\"\"\n    # If invalid `use` case is given, a print statement will be returned instead\n    if use not in ['Open', 'High', 'Low', 'Close']:\n        print(f'{use} was not a valid use case, a valid use case would be one of the four:')\n        print('\\t- \"Open\"\\n\\t- \"High\"\\n\\t- \"Low\"\\n\\t- \"Close\"')\n        print('Note: The default use case is \"Close\", if you don\\'t specify a use case, the default will be used.')\n    # If valid `use` case is given, function will carry on\n    else:\n        # Append all dates from `the_dates` to `adict`\n        for date in range(len(the_dates)):\n            adict['Date'].append(the_dates[date])\n            # Append all appropriate values from `the_data` to the corresponding `adict` keys\n            for i, time in enumerate(list(adict.keys())[1:]):\n                adict[time].append(round(the_data[date][use][i], 2))","a19304c0":"appendValues(dic, dates, data, use='Open')","bd9315d8":"set([len(dic[i]) for i in dic])","781b1d02":"adf = pd.DataFrame.from_dict(dic)\nadf.set_index('Date', inplace = True)","68da3400":"adf.head()","0f5c979a":"print(f'The size of our new dataframe is {len(adf)}, which means that our dataframe is using {len(adf)} days worth of data.')\nprint(f'There are {adf.count().sum()} values in our data in total because we\\'re grabbing the price of $GOOGL at {len(adf.columns)} different times every day for {len(adf)} different days.')","30d710b6":"adf.info()","a7700233":"def replaceWithNan(df, noOfCols, percOfNulls):\n    \"\"\"\n    Description\n    -----------\n    This will randomly choose `noOfCols` columns and will randomly\n    choose `percOfNulls` percent of the rows in each chosen column\n    to replace the value of with NaN for a given dataframe `df`.\n    \n    Parameters\n    ----------\n    df (dataframe):\n        The dataframe after converting the dictionary retrieved\n        after calling function `appendValues()`.\n    \n    noOfCols (int):\n        The number of columns you wish to randomly choose for\n        NaN replacement.\n    \n    percOfNulls (int or float):\n        The percentage of values in each chosen column that you\n        want to replace with NaN.\n        \n    Returns\n    -------\n    ndf (dataframe):\n        The dataframe with NaN replacements.\n        \n    Examples\n    --------\n    `cdf = replaceWithNan(adf, 4, 25)` choose 4 columns at random\n        and will choose 25% of the values to replace with NaN.\n    `cdf = replaceWithNan(adf, 7, 101)` will cause the machine to\n        be confused af, and `cdf` will be `NoneType`.\n    `cdf = replaceWithNan(adf, 10000, .15)` will display a message\n        saying that there aren't enough columns.\n    \"\"\"\n    # If percentage is greater than 100%, then this won't work\n    if percOfNulls > 100:\n        print(\"You're probably asking to make more than the whole column null.\")\n        print(\"Why so confuse me? I am only machine! :(\")\n    # If the number of columns specified is more than the number of columns available,\n    # then this print statement will be displayed\n    elif noOfCols > len(df.columns):\n        print(f\"There are only {len(df.columns)} columns in your dataframe, but you're telling me to choose {noOfCols} columns.\")\n        print(f\"If you want me to choose ALL columns, please set 'noOfCols' equal to {len(df.columns)}.\")\n    else:\n        # If `percOfNulls` >=1, it will be assumed that `percOfNulls` is out of 100\n        if percOfNulls >= 1:\n            percOfNulls = percOfNulls\/100\n        # Make deep copy of original dataframe\n        ndf = df.copy(deep = True)\n        # For every randomly chosen column, replace `percOfNulls` percent of the values with NaN\n        for column in ndf.sample(noOfCols, axis = 1):\n            ndf[column] = ndf[column].sample(frac = 1 - percOfNulls)\n        return ndf","004929a5":"# Attempting to replace 101% of data in 7 different rows of dataframe `adf`\nreplaceWithNan(adf, 7, 101)","75d290de":"# Attempting to replace 15% of the data in 10000 columns\nreplaceWithNan(adf, 10000, .15)","679d9edd":"cdf = replaceWithNan(adf, 7, 0.15)","f0b2d308":"cdf.head()","07c9abfb":"cdf.info()","e4f73cc5":"print(f\"Our dataframe containing null values has {cdf.count().sum()} values (not considering NaNs).\")\nprint(f\"Our original dataframe without null values has {adf.count().sum()} values.\")\nprint(f\"This means that {adf.count().sum() - cdf.count().sum()} values are NaN, which equates to approximately {((adf.count().sum() - cdf.count().sum()) \/ (adf.count().sum())*100):.04f}% of the dataset.\")","b926f7a4":"cdf.columns[cdf.isna().any()]","8718637a":"def imputeMissingVals(ndf, imputer = IterativeImputer()):\n    \"\"\"\n    Description\n    -----------\n    This will impute the NaNs in dataframe `ndf` with \n    `IterativeImputer()`. If `imputer` is invalid, an error will\n    be displayed along with the error description. If no imputer\n    is specified in the function, the function will use the\n    default imputer, `IterativeImputer()` with no specific\n    parameters. The default estimator used for the imputer is\n    `BayesianRidge()`.\n    \n    Parameters\n    ----------\n    ndf (dataframe):\n        The dataframe containing NaNs, which was retrieved from\n        function `replaceWithNan()`.\n    \n    imputer (IterativeImputer()):\n        `IterativeImputer()` with any parameters of your choice.\n    \n    Returns\n    -------\n    ndft (dataframe):\n        A dataframe with the imputed values.\n        \n    \"\"\"\n    # If `imputer` is invalid, an error will be thrown\n    try:\n        imputer.fit(ndf)\n    except Exception as e:\n        print(\"============================================================================\")\n        print(f\"I wasn't able to iteratively impute with the given imputer:\\n{str(imputer)}.\")\n        print(f\"This was the error I've received from my master:\\n\\n{e}.\")\n        print(\"\\nNOTE:\\n\\tAn imputer is already given by default for this function.\")\n        print(\"\\tIf you don't set `imputer` to anything, then the default\")\n        print(\"\\t`IterativeImputer()` with no other parameters will be used.\")\n        print(\"\\tThe default estimator for `IterativeImputer()` is `BayesianRidge()`.\")\n        print(\"============================================================================\")\n    # If `imputer` is valid, then the imputer will fit to the dataframe containing NaNs and transform\n    # A dataframe `ndft` will then be returned which contains the imputed values\n    else:\n        imputer.fit(ndf)\n        ndft = pd.DataFrame(imputer.transform(ndf), columns = ndf.columns, index = ndf.index)\n        \n        return ndft","564ea9b6":"cdft = imputeMissingVals(cdf)","5a25534b":"cdft.isna().any().any()","dac811c1":"cdft.head()","3ca865b4":"def displayResults(original_df, null_df, imputed_df, showDF = True, showGraph = True):\n    \"\"\"\n    Description\n    -----------\n    This will display how far\/close the imputed values were to\n    the original values. Each column that was randomly chosen\n    after running the function `replaceWithNan()` was a \n    certain \"time\". For however many columns chosen, there will\n    be that many dataframes created for comparison purposes.\n    So basically, every column (\"time\") that has a NaN value in\n    `null_df` will have it's own set of results which will be\n    separated by a border: \"===========\".\n    \n    In each separation, there will be a dataframe of 4 rows.\n    Each separation is it's own \"time\", and each separation\n    will have different dates that were replaced by NaNs for\n    the specific \"time\".\n    So this means, a specific (\"time\", \"date\") pair was NaN.\n    The first row is a row containing only NaNs from the\n    dataframe `null_df`. \n    The second row contains the original value for that given\n    (\"time\", \"date\") pair.\n    The third row contains the imputed value for that given\n    pair.\n    The fourth row contains the difference between the imputed\n    and the original value.\n    If the difference is positive, then it was an overestimation\n    by the amount.\n    If the difference is negative, the imputer underestimated\n    the original value by the amount.The absolute value of the\n    \"differences\" will be added up for each separation and will\n    be displayed in each separation. The average difference\n    will also be displayed in each separation. The average\n    difference basically states that \"for this current time, on\n    an average, this was how far off the imputed value were from the \n    original values.\" The overall average in difference will also be \n    tracked to be displayed in the end along with the overall difference.\n    A graph will also be displayed showing the difference between\n    the original value and the imputed value for only the dates\n    which were NaN for the current time in `null_df`.\n    \n    Parameters\n    ----------\n    original_df (dataframe):\n        The original dataframe without any NaN replacements.\n        This is the dataframe after converting the dictionary\n        retrieved after calling the function `appendValues()`.\n        \n    null_df (dataframe):\n        The dataframe after NaN replacements.\n        This is the dataframe retrieved after calling the\n        function `replaceWithNan()`.\n        \n    imputed_df (dataframe):\n        The dataframe after imputations.\n        This is the dataframe retrieved after calling the\n        function `imputeMissingVals()`.\n        \n    showDF (bool):\n        If True, then the dataframe containing the 4 rows\n        will be displayed along with the other results.\n        This is set to True by default.\n    \n    showGraph (bool):\n        If True, then a graph will be displayed showing the\n        original vs. imputed value.\n        This is set to True by default.\n    \n    Returns\n    -------\n    Nothing.\n    \n    \"\"\"\n    # Keep track of the overall difference\n    overallDifference = 0\n    # Keep track of the overall average in difference\n    overallAvg = 0\n    \n    cols = null_df.columns[null_df.isna().any()].tolist()\n    for time in cols:\n        \n        # `currentDifference` will be used to display the overall difference for the current dataframe (\"time\")\n        currentDifference = 0\n        # `currentAvg` will be used to display the overall average in difference for the current dataframe (\"time\")\n        currentAvg = 0\n\n        # Print which \"time\" column we're dealing with\n        print(f'Time:   {time}')\n\n        # Create dataframe `somedf`, which contains 4 rows:\n           # cdf[time]                         --> first row will have all values in our dataframe replaced with NaNs for the current \"time\" (index = 'the_nans')\n           # adf[time]                         --> second row will have all values in our original dataframe for the current \"time\" (index = 'original_value')\n           # round(cdft[time], 2)              --> third row will have all values (rounded) in our imputed dataframe for the current \"time\" (index = 'imputed_value')\n           # round(cdft[time], 2) - adf[time]] --> fourth row will display the difference in imputed value to original value for the current \"time\" (index = 'difference')\n             # Note: If difference is negative, that means the imputer underestimated the original value, if difference is positive, the imputer overestimated the original\n        somedf = pd.DataFrame([null_df[time], original_df[time], round(imputed_df[time], 2), round(imputed_df[time], 2) - original_df[time]], index = ['the_nans', 'original_value', 'imputed_value', 'difference'])\n\n        \n        # Make the differences absolute value, and sum up the differences to get the total difference for the current \"time\"\n        currentDifference = round(somedf.loc['difference'].abs().sum(), 2)\n        print(f'Total difference for current time: ${currentDifference}')\n\n        # It's like an OnlyFans but for `onlyNans` \n        onlyNans = somedf.loc[:, somedf.isna().any()]\n        # Set `currentAvg` to the average of the absolute values of the differences\n        currentAvg = round(onlyNans.loc['difference'].abs().mean(), 2)\n        # Update `overallAvg`\n        overallAvg += currentAvg\n        print(f\"Avg. difference for current time:  ${currentAvg}\")\n        \n        if (showDF):\n            # Display dataframe of ONLY the dates containing NaNs for easier comparison\n            display(onlyNans)\n            \n        if (showGraph):\n            # Display a chart showing the difference between the original values\n            # and the imputed values of ONLY the values which were NaNs \n            plt.figure(figsize=(15.5, 4), dpi=80)\n            plt.plot(onlyNans.loc['original_value'], color = 'green', marker = 'o', label=\"Original Values\")\n            plt.plot(onlyNans.loc['imputed_value'], color = 'red', marker = 'x', label=\"Imputed Values\")\n            plt.xticks(onlyNans.columns,rotation = 'vertical')\n            plt.xlabel('NaN Dates', fontsize=12)\n            plt.ylabel('Price', fontsize=12)\n            plt.title('Original vs Imputed', fontsize=14)\n            plt.legend(loc=\"upper left\")\n            plt.show()\n\n        # Update the overall difference\n        overallDifference += currentDifference\n        print('==============================================================================================================================================')\n\n    # Print the overall difference, which is a measurement to determine how \"off\" the imputer was\n    print(f'The overall difference is: ${round(overallDifference, 2)}')\n    print(f'On an average, the imputer was ${round(overallAvg\/len(cols), 2)} away from the actual value.')","f6cff929":"displayResults(adf, cdf, cdft)","3d246fbc":"print(f\"Of the {adf.count().sum()} prices in the original dataframe, `adf`, the average price is approximately ${adf.to_numpy().mean():.2f}.\")","8eab6fae":"# First\ndef splitDataByDates(df):\n    \"\"\"\n    Description\n    -----------\n    This function will split the dataframe returned by `yfinance` by days. \n    For example, if your `period` was set to \"60d\", then you'll get 60\n    individual dataframes which will be stored in `the_data`.\n    \n    NOTE: \n    Use this function only when you have set an `interval` to an \n    intraday value. \n    Valid intraday intervals are: 1m, 2m, 5m, 15m, 30m, 60m,and 90m.\n    \n    Parameters\n    ----------\n    df (dataframe):\n        The dataframe returned from yfinance.\n    \n    Returns\n    -------\n    the_dates (list):\n        A list of all the dates in the dataframe returned from yfinance.\n    \n    the_data (list of dataframes):\n        A list of dataframes. Each date will have it's own dataframe.\n        \n    \"\"\"\n    # The returned lists\n    the_dates = []\n    the_data = []\n    \n    # Splits the data retrieved from yfinance by date\n    for group in df.groupby(df.index.date):\n        the_dates.append(group[0])\n        the_data.append(group[1])\n    return the_dates, the_data\n\n\n\n# Second\ndef makeTimesDict(the_data):\n    \"\"\"\n    Description\n    -----------\n    This will create an empty dictionary and set the keys equal to\n    all the time interval in the `Datetime` index of `the_data`,\n    which was retrieved after running `splitDataByDates()`.\n    This will also give the created dictionary a 'Date' key to hold\n    all of the dates. This dictionary is required if you've retrieved\n    data from yfinance with an intraday interval.\n    \n    Parameters\n    ----------\n    the_data (list of dataframes):\n        The list of dataframes which was retrieved after running the\n        function `splitDataByDates()`.\n    \n    Returns\n    -------\n    adict (dict):\n        A dictionary with the appropriate keys.\n        \n    \"\"\"\n    # Create an empty dictionary\n    adict = {}\n    # Give dictionary a 'Date' key\n    adict['Date'] = []\n    \n    # For every time listed in the index of each dataframes in`the_data`,\n    # make that time a key for the dictionary.\n    for i in the_data[0].index:\n        adict[str(i.time())] = []\n    return adict\n\n\n\n# Third\ndef appendValues(adict, the_dates, the_data, use='Close'):\n    \"\"\"\n    Description\n    -----------\n    This will append the appropriate values from each dataframe\n    in `the_data` to the dictionary `adict` based on the `use`\n    case. The default use case is 'Close'. This will also append \n    the dates from `the_dates` to the dictionary.\n    \n    Parameters\n    ----------\n    adict (dict):\n        The dictionary with set keys retrieved after running\n        the function `makeTimesDict()`.\n    \n    the_dates (list):\n        The list of dates retrieved after running the function\n        `splitDataByDates()`.\n        \n    the_data (list of dataframes):\n        The list of dataframes which was retrieved after running\n        the function `splitDataByDates()`.\n    \n    use (str):\n        The column from the data returned from yFinance of which \n        you want to retrieve data from. The valid `use` cases\n        are: 'Open', 'High', 'Low', or 'Close'.\n        The default `use` case is set to 'Close'.\n        \n    Returns\n    -------\n    Nothing.\n    \n    Examples\n    --------\n    `appendValues(dic, dates, data, use='Open')` will append all\n    \"Open\" values from `data` to `dic`, and will append all dates\n    from the list `dates` to `dic`.\n    \n    \"\"\"\n    # If invalid `use` case is given, a print statement will be returned instead\n    if use not in ['Open', 'High', 'Low', 'Close']:\n        print(f'{use} was not a valid use case, a valid use case would be one of the four:')\n        print('\\t- \"Open\"\\n\\t- \"High\"\\n\\t- \"Low\"\\n\\t- \"Close\"')\n        print('Note: The default use case is \"Close\", if you don\\'t specify a use case, the default will be used.')\n    # If valid `use` case is given, function will carry on\n    else:\n        # Append all dates from `the_dates` to `adict`\n        for date in range(len(the_dates)):\n            adict['Date'].append(the_dates[date])\n            # Append all appropriate values from `the_data` to the corresponding `adict` keys\n            for i, time in enumerate(list(adict.keys())[1:]):\n                adict[time].append(round(the_data[date][use][i], 2))\n                \n                \n                \n# Fourth\ndef replaceWithNan(df, noOfCols, percOfNulls):\n    \"\"\"\n    Description\n    -----------\n    This will randomly choose `noOfCols` columns and will randomly\n    choose `percOfNulls` percent of the rows in each chosen column\n    to replace the value of with NaN for a given dataframe `df`.\n    \n    Parameters\n    ----------\n    df (dataframe):\n        The dataframe after converting the dictionary retrieved\n        after calling function `appendValues()`.\n    \n    noOfCols (int):\n        The number of columns you wish to randomly choose for\n        NaN replacement.\n    \n    percOfNulls (int or float):\n        The percentage of values in each chosen column that you\n        want to replace with NaN.\n        \n    Returns\n    -------\n    ndf (dataframe):\n        The dataframe with NaN replacements.\n        \n    Examples\n    --------\n    `cdf = replaceWithNan(adf, 4, 25)` choose 4 columns at random\n        and will choose 25% of the values to replace with NaN.\n    `cdf = replaceWithNan(adf, 7, 101)` will cause the machine to\n        be confused af, and `cdf` will be `NoneType`.\n    `cdf = replaceWithNan(adf, 10000, .15)` will display a message\n        saying that there aren't enough columns.\n    \"\"\"\n    # If percentage is greater than 100%, then this won't work\n    if percOfNulls > 100:\n        print(\"You're probably asking to make more than the whole column null.\")\n        print(\"Why so confuse me? I am only machine! :(\")\n    # If the number of columns specified is more than the number of columns available,\n    # then this print statement will be displayed\n    elif noOfCols > len(df.columns):\n        print(f\"There are only {len(df.columns)} columns in your dataframe, but you're telling me to choose {noOfCols} columns.\")\n        print(f\"If you want me to choose ALL columns, please set 'noOfCols' equal to {len(df.columns)}.\")\n    else:\n        # If `percOfNulls` >=1, it will be assumed that `percOfNulls` is out of 100\n        if percOfNulls >= 1:\n            percOfNulls = percOfNulls\/100\n        # Make deep copy of original dataframe\n        ndf = df.copy(deep = True)\n        # For every randomly chosen column, replace `percOfNulls` percent of the values with NaN\n        for column in ndf.sample(noOfCols, axis = 1):\n            ndf[column] = ndf[column].sample(frac = 1 - percOfNulls)\n        return ndf\n    \n    \n    \n# Fifth\ndef imputeMissingVals(ndf, imputer = IterativeImputer()):\n    \"\"\"\n    Description\n    -----------\n    This will impute the NaNs in dataframe `ndf` with \n    `IterativeImputer()`. If `imputer` is invalid, an error will\n    be displayed along with the error description. If no imputer\n    is specified in the function, the function will use the\n    default imputer, `IterativeImputer()` with no specific\n    parameters. The default estimator used for the imputer is\n    `BayesianRidge()`.\n    \n    Parameters\n    ----------\n    ndf (dataframe):\n        The dataframe containing NaNs, which was retrieved from\n        function `replaceWithNan()`.\n    \n    imputer (IterativeImputer()):\n        `IterativeImputer()` with any parameters of your choice.\n    \n    Returns\n    -------\n    ndft (dataframe):\n        A dataframe with the imputed values.\n        \n    \"\"\"\n    # If `imputer` is invalid, an error will be thrown\n    try:\n        imputer.fit(ndf)\n    except Exception as e:\n        print(\"============================================================================\")\n        print(f\"I wasn't able to iteratively impute with the given imputer:\\n{str(imputer)}.\")\n        print(f\"This was the error I've received from my master:\\n\\n{e}.\")\n        print(\"\\nNOTE:\\n\\tAn imputer is already given by default for this function.\")\n        print(\"\\tIf you don't set `imputer` to anything, then the default\")\n        print(\"\\t`IterativeImputer()` with no other parameters will be used.\")\n        print(\"\\tThe default estimator for `IterativeImputer()` is `BayesianRidge()`.\")\n        print(\"============================================================================\")\n    # If `imputer` is valid, then the imputer will fit to the dataframe containing NaNs and transform\n    # A dataframe `ndft` will then be returned which contains the imputed values\n    else:\n        imputer.fit(ndf)\n        ndft = pd.DataFrame(imputer.transform(ndf), columns = ndf.columns, index = ndf.index)\n        \n        return ndft\n    \n    \n    \n# Sixth\ndef displayResults(original_df, null_df, imputed_df, showDF = True, showGraph = True):\n    \"\"\"\n    Description\n    -----------\n    This will display how far\/close the imputed values were to\n    the original values. Each column that was randomly chosen\n    after running the function `replaceWithNan()` was a \n    certain \"time\". For however many columns chosen, there will\n    be that many dataframes created for comparison purposes.\n    So basically, every column (\"time\") that has a NaN value in\n    `null_df` will have it's own set of results which will be\n    separated by a border: \"===========\".\n    \n    In each separation, there will be a dataframe of 4 rows.\n    Each separation is it's own \"time\", and each separation\n    will have different dates that were replaced by NaNs for\n    the specific \"time\".\n    So this means, a specific (\"time\", \"date\") pair was NaN.\n    The first row is a row containing only NaNs from the\n    dataframe `null_df`. \n    The second row contains the original value for that given\n    (\"time\", \"date\") pair.\n    The third row contains the imputed value for that given\n    pair.\n    The fourth row contains the difference between the imputed\n    and the original value.\n    If the difference is positive, then it was an overestimation\n    by the amount.\n    If the difference is negative, the imputer underestimated\n    the original value by the amount.The absolute value of the\n    \"differences\" will be added up for each separation and will\n    be displayed in each separation. The average difference\n    will also be displayed in each separation. The average\n    difference basically states that \"for this current time, on\n    an average, this was how far off the imputed value were from the \n    original values.\" The overall average in difference will also be \n    tracked to be displayed in the end along with the overall difference.\n    A graph will also be displayed showing the difference between\n    the original value and the imputed value for only the dates\n    which were NaN for the current time in `null_df`.\n    \n    Parameters\n    ----------\n    original_df (dataframe):\n        The original dataframe without any NaN replacements.\n        This is the dataframe after converting the dictionary\n        retrieved after calling the function `appendValues()`.\n        \n    null_df (dataframe):\n        The dataframe after NaN replacements.\n        This is the dataframe retrieved after calling the\n        function `replaceWithNan()`.\n        \n    imputed_df (dataframe):\n        The dataframe after imputations.\n        This is the dataframe retrieved after calling the\n        function `imputeMissingVals()`.\n        \n    showDF (bool):\n        If True, then the dataframe containing the 4 rows\n        will be displayed along with the other results.\n        This is set to True by default.\n    \n    showGraph (bool):\n        If True, then a graph will be displayed showing the\n        original vs. imputed value.\n        This is set to True by default.\n    \n    Returns\n    -------\n    Nothing.\n    \n    \"\"\"\n    # Keep track of the overall difference\n    overallDifference = 0\n    # Keep track of the overall average in difference\n    overallAvg = 0\n    \n    cols = null_df.columns[null_df.isna().any()].tolist()\n    for time in cols:\n        \n        # `currentDifference` will be used to display the overall difference for the current dataframe (\"time\")\n        currentDifference = 0\n        # `currentAvg` will be used to display the overall average in difference for the current dataframe (\"time\")\n        currentAvg = 0\n\n        # Print which \"time\" column we're dealing with\n        print(f'Time:   {time}')\n\n        # Create dataframe `somedf`, which contains 4 rows:\n           # cdf[time]                         --> first row will have all values in our dataframe replaced with NaNs for the current \"time\" (index = 'the_nans')\n           # adf[time]                         --> second row will have all values in our original dataframe for the current \"time\" (index = 'original_value')\n           # round(cdft[time], 2)              --> third row will have all values (rounded) in our imputed dataframe for the current \"time\" (index = 'imputed_value')\n           # round(cdft[time], 2) - adf[time]] --> fourth row will display the difference in imputed value to original value for the current \"time\" (index = 'difference')\n             # Note: If difference is negative, that means the imputer underestimated the original value, if difference is positive, the imputer overestimated the original\n        somedf = pd.DataFrame([null_df[time], original_df[time], round(imputed_df[time], 2), round(imputed_df[time], 2) - original_df[time]], index = ['the_nans', 'original_value', 'imputed_value', 'difference'])\n\n        \n        # Make the differences absolute value, and sum up the differences to get the total difference for the current \"time\"\n        currentDifference = round(somedf.loc['difference'].abs().sum(), 2)\n        print(f'Total difference for current time: ${currentDifference}')\n\n        # It's like an OnlyFans but for `onlyNans` \n        onlyNans = somedf.loc[:, somedf.isna().any()]\n        # Set `currentAvg` to the average of the absolute values of the differences\n        currentAvg = round(onlyNans.loc['difference'].abs().mean(), 2)\n        # Update `overallAvg`\n        overallAvg += currentAvg\n        print(f\"Avg. difference for current time:  ${currentAvg}\")\n        \n        if (showDF):\n            # Display dataframe of ONLY the dates containing NaNs for easier comparison\n            display(onlyNans)\n            \n        if (showGraph):\n            # Display a chart showing the difference between the original values\n            # and the imputed values of ONLY the values which were NaNs \n            plt.figure(figsize=(15.5, 4), dpi=80)\n            plt.plot(onlyNans.loc['original_value'], color = 'green', marker = 'o', label=\"Original Values\")\n            plt.plot(onlyNans.loc['imputed_value'], color = 'red', marker = 'x', label=\"Imputed Values\")\n            plt.xticks(onlyNans.columns,rotation = 'vertical')\n            plt.xlabel('NaN Dates', fontsize=12)\n            plt.ylabel('Price', fontsize=12)\n            plt.title('Original vs Imputed', fontsize=14)\n            plt.legend(loc=\"upper left\")\n            plt.show()\n\n        # Update the overall difference\n        overallDifference += currentDifference\n        print('==============================================================================================================================================')\n\n    # Print the overall difference, which is a measurement to determine how \"off\" the imputer was\n    print(f'The overall difference is: ${round(overallDifference, 2)}')\n    print(f'On an average, the imputer was ${round(overallAvg\/len(cols), 2)} away from the actual value.')","46db59e3":"#### 2.6.2 The Results are in<a id=\"2.6.2\"><\/a>\n\nNow let's display our results below. Remember that `adf` was the original dataframe, `cdf` was the dataframe after null replacements and `cdft` is the dataframe after imputations.","b8bfe1c4":"### 2.3 Converting the Layout<a id=\"2.3\"><\/a>\n\nFor our example, I'll only be using the \"Open\" price for every row in our data. This means, I only want to work with the \"Open\" price of every <i>half hour<\/i>.<br>\nI want to be able to group the data by dates, however, because the date and time is all clumped together into one dataframe, I decided to this data into multiple individual dataframes (based on the date) to be able to make our data more useful for this notebook.\n\nLet's now split the data by dates using this function that I've created:","c7d4a6db":"# 1. Introduction<a id=\"1\"><\/a>\nChances are, you've read my Medium article and that's probably why you're here. If you don't know what I'm talking about, then please go ahead and check out my [Medium article by clicking here](https:\/\/gifari.medium.com\/a-better-way-to-handle-missing-values-in-your-dataset-using-iterativeimputer-9e6e84857d98), where I go more in depth on what I'll be doing in this notebook! If you don't have access to Medium, that's fine as well, I'll still go over in detail on what I will be doing and I'll have some parts of my explanation from my article pasted in here as well.\n\nThis notebook is pretty much exactly what I've demonstrated in my Medium article, except here, I've created functions to make your life easier. ","aaefdcb9":"Although probably not necessary, but just to verify, we don't have any null values in our dataframe.\n\nNow we're ready to manipulate our dataframe. Let's replace some values with `NaN` using my function in the following section.","57ac9216":"### 2.5 Imputing Missing Values with IterativeImputer<a id=\"2.5\"><\/a>\n\nNow let's use Iterative Imputer to replace the NaNs using my function below! I'll be using `IterativeImputer()` without any parameter changes for this example. Feel free to try out your own modifications on `IterativeImputer()`.\n\n#### 2.5.1 FUNCTION: imputeMissingVals<a id=\"2.5.1\"><\/a>\n\nThis will impute the NaNs in a given dataframe with `IterativeImputer()`. If the given `imputer` is invalid, an error will be displayed along with the error description. If no imputer is specified in the function, the function will use the default imputer, `IterativeImputer()` with no specific parameters. The default estimator used is `BayesianRidge()`. This will also return the whole dataset with the imputed values as a dataframe.\n\n\n<b>Expand the cell below to view the function.<\/b>","2d1bf1b4":"#### 2.3.2 Having 60 Dates<a id=\"2.3.2\"><\/a>\nMore dates more data.<br>\nSince the function above returns two lists, let's give some meaning to two.","88f21039":"Now, let's create a dictionary for our column names, append the values (rounded) and make a dataframe.\n\n> Note: Python's built-in function `round()` can behave inappropriately.<br>\n> For instance, `round(2.675, 2)` gives `2.67`.<br>\n> This isn't a bug or an error, to learn more, [click here](https:\/\/docs.python.org\/3\/library\/functions.html#round).<br>\n> For my example, [it doesn't matter](https:\/\/memegenerator.net\/img\/instances\/67036946\/it-doesnt-matter-what-your-opinion-is.jpg) how `round()` treats the values.","63df3e84":"<b>2.3.3.6 Our New Layout<\/b><a id=\"2.3.3.6\"><\/a>\n","29ea7da3":"If we divide the overall average difference by the average price, we get `3.48\/2789.78 = 0.0012474101900508284`, which is no where near 1%. Investors wouldn't even mind risking 1% of their portfolio.","767253ba":"#### 2.5.2 After the Imputation<a id=\"2.5.2\"><\/a>\n","c86f2c7f":"The `dates` will display a list of all the dates from `googl` and `data` will display a list of dataframes which shows the data for each date. Let's look at the first ten dates and the first dataframe:","896fe20f":"<b>2.3.3.2 Adding Keys to our Dictionary<\/b><a id=\"2.3.3.2\"><\/a>\n\nLet's run this function and see our results.","9b5f2e5a":"#### 2.2.2 Understanding The Data So Far<a id=\"2.2.2\"><\/a>\n\nWithout getting too much into detail, I'll break this down. \n\nThe `Datetime` index contains days from the past 60 market days, and goes by intervals of 30 minutes. I'm currently writing this on 10\/25\/2021. 60 market days ago from when I'm writing this was August 02, 2021. The market opens at 09:30 AM EST and closes at 04:00 PM EST. If you don't yet know your military time, then pick it up soldier.\n\n> <i>NOTE: The stock market occasionally closes before 4pm EST.<\/i>\n\n<b>2.2.2.1 Learning Your Military Time<\/b><a id=\"2.2.2.1\"><\/a>\n\nMidnight (12:00:00 AM) is 00:00:00.<br>\nAs soon as you hit 12:59:59 PM, instead of going down to 01:00:00 PM, you go to 13:00:00.<br>\nKeep going until it's 23:59:59 which is 11:59:59 PM, and you're back to midnight, at 00:00:00.<br>\nThat's right, military time is telling time without the use of AM or PM. I had to learn this the hard way <b>\ud83e\udd15<\/b>.<br><br>\nIf you're still confused, here's a table that I've made for you:<br>\n```\n...................................................................\n\u2af6     MORNING    \u2af6    AFTERNOON   \u2af6     MORNING    \u2af6    AFTERNOON   \u2af6\n===================================================================\n|    Regular    |    Military    |    Regular    |    Military    |\n|---------------|----------------|---------------|----------------|\n|  12:00:00 AM  |    00:00:00    |  12:00:00 PM  |    12:00:00    |\n|   (MIDNIGHT)  |   (24:00:00)   |     (NOON)    |                |\n|---------------|----------------|---------------|----------------|\n|  01:00:00 AM  |    01:00:00    |  01:00:00 PM  |    13:00:00    |\n|---------------|----------------|---------------|----------------|\n|  02:00:00 AM  |    02:00:00    |  02:00:00 PM  |    14:00:00    |\n|---------------|----------------|---------------|----------------|\n|  03:00:00 AM  |    03:00:00    |  03:00:00 PM  |    15:00:00    |\n|---------------|----------------|---------------|----------------|\n|  04:00:00 AM  |    04:00:00    |  04:00:00 PM  |    16:00:00    |\n|---------------|----------------|---------------|----------------|\n|  05:00:00 AM  |    05:00:00    |  05:00:00 PM  |    17:00:00    |\n|---------------|----------------|---------------|----------------|\n|  06:00:00 AM  |    06:00:00    |  06:00:00 PM  |    18:00:00    |\n|---------------|----------------|---------------|----------------|\n|  07:00:00 AM  |    07:00:00    |  07:00:00 PM  |    19:00:00    |\n|---------------|----------------|---------------|----------------|\n|  08:00:00 AM  |    08:00:00    |  08:00:00 PM  |    20:00:00    |\n|---------------|----------------|---------------|----------------|\n|  09:00:00 AM  |    09:00:00    |  09:00:00 PM  |    21:00:00    |\n|---------------|----------------|---------------|----------------|\n|  10:00:00 AM  |    10:00:00    |  10:00:00 PM  |    22:00:00    |\n|---------------|----------------|---------------|----------------|\n|  11:00:00 AM  |    11:00:00    |  11:00:00 PM  |    23:00:00    |\n===================================================================\n\n```\n\nOkay, now going back to the dataframe, you can see on the table that the first row's datetime is 09:30:30\u201304:00 and the very last row is 16:00:00\u201304:00. Market opens at 09:30 AM, closes at 04:00 PM. The \"-04:00\" part of the datetime is saying that it's based on New York's time. The time zone in New York is \"GMT-4\" as of the day I'm writing this.\n\n<b>2.2.2.2 But what do the Columns Stand for?<\/b><a id=\"2.2.2.2\"><\/a>\n\nThe \"Open\" column is what price the stock was at exactly the corresponding time of the time specified in the row. The \"High\" and \"Low\" columns are the most highest and most lowest price that the stock reached within that 30 minute interval. The \"Close\" column is approximately what the price of the stock was at the very last second of the 30 minute interval. The \"Volume\" is pretty much the size of the trades within the interval.<br><br>\nFor example, looking at the third row\u200a(08\/02\/2021 at 10:30:00):<br>\n- At 10:30:00, the price per share of \\\\$GOOGL was approximately \\\\$2695.39 (\"Open\").<br>\n- From 10:30:00 to 10:59:59, the lowest price each share became was \\\\$2688.12 (\"Low\").<br>\n- The highest price each share became within that half hour was approximately \\\\$2695.39 (\"High\"), which is coincidentally the same price as the \"Open\" price.<br>\n- At 10:59:59, the price per share of \\\\$GOOGL was approximately \\\\$2693.70 (\"Close\").","1d50b00e":"Let's look at out new dataframe containing NaNs. You'll realize that the rows chosen in one column is not the same as the rows chosen in another column.","2ded46ef":"### 3.3 References<a id=\"3.3\"><\/a>\n\n[It doesn't matter image](https:\/\/memegenerator.net\/img\/instances\/67036946\/it-doesnt-matter-what-your-opinion-is.jpg)<br>\n[IterativeImputer](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.impute.IterativeImputer.html)<br>\n[My Medium article]()<br>\n[Python round() function](https:\/\/docs.python.org\/3\/library\/functions.html#round)<br>\n[yfinance documentation](https:\/\/aroussi.com\/post\/python-yahoo-finance)","0239b008":"### 2.2 Downloading the Historical Data for Google Using yfinance<a id=\"2.2\"><\/a>\n\nThe stock I've chosen for this example is Google. The ticker symbol which I'll be using is \\\\$GOOGL (Alphabet Inc Class A). I'll be using data from the past 60 \"stock market days\" (period). I'm going to be using the closing price of every 30 minute <i>interval<\/i>.\n\nSome valid periods are: 1d,5d,1mo,3mo,6mo,1y,2y,5y,10y,ytd,max<br>\nSome valid intervals are: 1m,2m,5m,15m,30m,60m,90m,1h,1d,5d,1wk,1mo,3mo<br>\n    \nFor this example, I advise you to only stick with intraday periods (1m,2m,5m,15m,30m,60m,90m, or 1h).\nTo learn more, please [click here](https:\/\/aroussi.com\/post\/python-yahoo-finance) to read the yfinance documentation.\n\n\nTo download data for \\\\$GOOGL for the past 60 days of 30 minute intervals, run this code below:","392ccf82":"Our new layout now has all of the \"times\" listed our column, and all of the dates as our index. From here, let's interpret this as a (date, time) pair.<br>\nFor example, `(2021-08-04, 15:30:00) = 2704.91` means that on date \"2021-08-04\" <b>at<\/b> time \"15:30:00\", the price of \\\\$GOOGL was \\\\$2704.91.","a36218f1":"<h1>Predicting Missing Stock Prices with IterativeImputer<\/h1>\n<h3 style=\"color:gray\">Replacing past history stock prices with NaN and then using sklearn's IterativeImputer to guess what value it was<\/h3>\n<h4>An example, and yet another tutorial on another use case for IterativeImputer, in addition to my <a href=\"https:\/\/gifari.medium.com\/a-better-way-to-handle-missing-values-in-your-dataset-using-iterativeimputer-9e6e84857d98\">Medium article<\/a><\/h4>\n\n[Gifari Hoque](https:\/\/gifari.github.io) - October 2021","78706f6c":"#### 2.3.1 FUNCTION: splitDataByDates<a id=\"2.3.1\"><\/a>\n\nThis function will split the dataframe returned by `yfinance` by days.<br>\nFor example, if your `period` was set to \"60d\", then you'll get 60 individual dataframes which will be stored in `the_data`.\n\n<b>Expand the cell below to view the function.<\/b>","494deed7":"Let's check if there are any null values in our new imputed dataframe:","89fab578":"#### 2.6.1 FUNCTION: displayResults<a id=\"2.6.1\"><\/a>\n\nThis function will show the comparison between the imputed and the original values. Only the (date, time) pair which were NaNs will be shown in the results (including the graph).<br>\nEach \"chosen time\" will have a set of it's own results: some measurement varaibles, a dataframe containing 4 rows, and a graph displaying the original vs. imputed values (of only the values which were replaced with `NaN`).<br>\nThe dataframe containing 4 rows will have a row for:\n- Dataframe `cdf` (the_nans) --> The (date, time) pair which was replaced with `NaN`. This is literally a row of NaNs.\n- Dataframe `adf` (original_value) --> The original value of the (date, time) pair.\n- Dataframe `cdft` (imputed_value) --> The imputed value of the (date, time) pair.\n- (difference) --> The difference between the original value and the imputed value for that (date, time) pair.\n    - If the difference is negative, that means the imputer underestimated the original value by the displayed amount.\n    - If the difference is positive, that means the imputer overestimated the original value by the displayed amount.\n<br><br>\n    \n`overallDifference`, `overallAvg`, `currentDifference`, and `currentAvg` are all used for measurement purposes:\n- `currentDifference`: is the absolute value total of the `difference` row in the dataframe. Each \"time\" separation will have it's own `currentDifference`.\n- `overallDifference`: is all of the `currentDifference` from each \"time\" separation, summed up. There is only one `overallDifference` which will be displayed in the end.\n- `currentAvg`: is the average of the differences for the current \"time\". This basically states that \"<i><b>for this current time<\/b>, on an average, this is how far off the imputed values were from the original values.<\/i>\" Each \"time\" separation will have it's own `currentAvg`.\n- `overallAvg`: is the overall average of the differences. This basically states that \"<i>on an average, this is how far off the imputed values were from the original values.<\/i>\" There is only one `overallAvg` which will be displayed in the end.\n\n\n> NOTE: You have the choice. You can view the dataframe or graph, or both, or none, but by default, the results will display both, the dataframe and the graph.\n\n\n<b>Expand the cell below to view the function.<\/b>","c8f47b8b":"<b>2.3.3.4 Making our Dictionary Valuable<\/b><a id=\"2.3.3.4\"><\/a>\n\nFor my example, I'll use the prices from the \"Open\" column.","df5b9949":"<b>2.3.3.5 Dataframe It!<\/b><a id=\"2.3.3.5\"><\/a>\n\nLooks good on my end, let's now make a dataframe of our data and look at the head of our new dataframe.","1a0e67e1":"# Table of Contents<a id=\"toc\"><\/a>\n#### 1. [An Introduction](#1)\n   - 1.1 [What to Expect in this Notebook](#1.1)\n   - 1.2 [A Brief Introduction to yfinance](#1.2)\n   - 1.3 [A Brief Introduction to IterativeImputer](#1.3)\n   - 1.4 [How to be the \"Chosen\u00a0One\"](#1.4)\n   - 1.5 [The Process Behind the Iterations](#1.5)\n   \n#### 2. [Here is Where the Action Starts](#2)\n   - 2.1 [Importing Packages](#2.1)\n   - 2.2 [Downloading the Historical Data for Google Using yfinance](#2.2)\n       - 2.2.1 [The Data So Far](#2.2.1)\n       - 2.2.2 [Understanding The Data So Far](#2.2.2)\n           - 2.2.2.1 [Learning Your Military Time](#2.2.2.1)\n           - 2.2.2.2 [But what do the Columns Stand for?](#2.2.2.2)\n   - 2.3 [Converting the Layout](#2.3)\n       - 2.3.1 [<b>FUNCTION:<\/b> splitDataByDates](#2.3.1)\n       - 2.3.2 [Having 60 Dates](#2.3.2)\n       - 2.3.3 [Making a Dictionary](#2.3.3)\n           - 2.3.3.1 [<b>FUNCTION:<\/b> makeTimesDict](#2.3.3.1)\n           - 2.3.3.2 [Adding Keys to our Dictionary](#2.3.3.2)\n           - 2.3.3.3 [<b>FUNCTION:<\/b> appendValues](#2.3.3.3)\n           - 2.3.3.4 [Making our Dictionary Valuable](#2.3.3.4)\n           - 2.3.3.5 [Dataframe It!](#2.3.3.5)\n           - 2.3.3.6 [Our New Layout](#2.3.3.6)\n   - 2.4 [Replacing Random Values with `NaN`](#2.4)\n       - 2.4.1 [<b>FUNCTION:<\/b> replaceWithNan](#2.4.1)\n       - 2.4.2 [Machines Can Get Confused Too](#2.4.2)\n       - 2.4.3 [Results After Replacement](#2.4.3)\n   - 2.5 [Imputing Missing Values with IterativeImputer](#2.5)\n       - 2.5.1 [<b>FUNCTION:<\/b> imputeMissingVals](#2.5.1)\n       - 2.5.2 [After the Imputation](#2.5.2)\n   - 2.6 [The Results](#2.6)\n       - 2.6.1 [<b>FUNCTION:<\/b> displayResults](#2.6.1)\n       - 2.6.2 [The Results are in](#2.6.2)\n       - 2.6.3 [Understanding The Results](#2.6.3)\n\n#### 3. [A Thank You](#3)\n   - 3.1 [Final Words](#3.1)\n   - 3.2 [All Functions](#3.2)\n   - 3.3 [References](#3.3)","5efc40f5":"The overall average difference was \\\\$3.48. This may seem like a lot if we're viewing this result as: \"On an average, the imputations were \\\\$3.48 away from the actual value,\" but we must not forget to consider the average price of \\\\$GOOGL from our values in our original dataframe `adf`.","6a413659":"We can see from the output above that 9 values were replaced with `NaN` in 7 different columns.","13607d4b":"# 2. Action Starts Here<a id=\"2\"><\/a>\nHere is where the action starts.\n\n### 2.1 Importing Packages<a id=\"2.1\"><\/a>\nAfter installing yfinance, these are the necessary packages that you'll need in order to run this program.\n\n<b>Expand the cell below to see all imports.<\/b>","85e740b2":"# 3. Thank You<a id=\"3\"><\/a>\n### 3.1 Final Words<a id=\"3.1\"><\/a>\nThank you all for taking the time out of your day to read this notebook. Please don't forget to upvote if you think I deserve it.<br>\nFeel free to play around with this notebook, I've pasted all the functions used in this notebook into one cell in the next section.\n\nIf you notice an error in my notebook, please reach out to me or leave a comment and I'll fix it so I don't mislead anyone.<br>\nTo me, any criticism is positive criticism, that's one of the best ways I learn.\n\n<i>If you think I deserve a coffee<\/i>, please do buy me one by [clicking here](https:\/\/www.buymeacoffee.com\/Gifari)! Any support helps me keep going.<br>\nBe sure to follow me here on [GitHub](https:\/\/github.com\/Gifari) and also on [Kaggle](https:\/\/www.kaggle.com\/gifarihoque) where I'll be posting more notebooks and tutorials from time to time.<br>\nHere is the link to my [Medium](https:\/\/gifari.medium.com\/) account, where I'll also be posting tutorials and guides, please follow me there too.\nAlso, feel free to connect with me on [LinkedIn](https:\/\/www.linkedin.com\/in\/gifari\/).\n\n[Click here to check out my online portfolio!](https:\/\/gifari.github.io\/)<br>\n\nMy other related works on Kaggle:<br>\n[PIMA Indians Diabetes Database: Missing Data!? Where? A Machine Learning & IterativeImputer Tutorial (~86% Accuracy)](https:\/\/www.kaggle.com\/gifarihoque\/pidd-missing-data-ml-iterimputer-tut-86)<br><br>\n\n> Although this notebook ends here, I hope you've got something out of this. Be sure to play around with the yfinance package as well, it's quite interesting what you can do with stock market data. I've been playing around with the data for some time now, and I'm always finding new things to do with the data. Many days of writing and research went into this notebook and my [Medium article](https:\/\/gifari.medium.com\/a-better-way-to-handle-missing-values-in-your-dataset-using-iterativeimputer-9e6e84857d98). Hopefully this notebook along with my article inspired at least one of you with an idea or something to do greater things in your life.\n> \n> <i>Let this ending, be a new beginning for you.<\/i>\n> \n> As always, thanks again for stopping by.","11220e73":"### 2.4 Replacing Random Values with `NaN`<a id=\"2.4\"><\/a>\nFor this demonstration, I'll be using\u00a0`.sample()` for random selection.\n\n#### 2.4.1 FUNCTION: replaceWithNan<a id=\"2.4.1\"><\/a>\n\nThis will randomly choose a certain number of columns based on user input and will randomly choose a certain percent of data,also based on the user input to replace in each <i>chosen<\/i> column with `NaN`.\n\n<b>Expand the cell below to view the function.<\/b>","31e96991":"### 1.1 What to Expect in this Notebook<a id=\"1.1\"><\/a>\n\nHere is a brief summary of what I will be doing:<br>\n1. First I'm going to grab the historical market data of a stock information using the package <i>yfinance<\/i>.\n2. I will then convert the original layout that is retrieved from yfinance into a layout of my own.\n3. Next, I'm going to randomly replace a certain percentage of the data with `NaN`.\n4. After that, I'm going to use `IterativeImputer()` to impute the missing values.\n5. Finally, I'll display the results to compare the original price vs. the imputed price of the stock.\n\nThat's literally all! It's going to be a short notebook, but hopefully you guys learn something out of it and find it useful in your lives!\n\nWithout further ado, let's get started.","5155c969":"These are the columns which have `NaN` replacements:","4f11b3b6":"<b>2.3.3.3 FUNCTION: appendValues<\/b><a id=\"2.3.3.3\"><\/a>\n\nThis will append the appropriate values from each dataframe `data` to the dictionary `dic` based on the `use` case.<br>\nThe default use case is \"Close\". This means that if no `use` case is specified, then the function will use the \"Close\" column prices from `googl`. If an invalid `use` case is specified, an error will be returned.This will also append the dates from `dates` to the dictionary.\n\n> NOTE: The valid `use` cases are one of the four: 'Open', 'High', 'Low', or 'Close'.\n\n<b>Expand the cell below to view the function.<\/b>","dbf8bd00":"#### 2.3.3 Making a Dictionary<a id=\"2.3.3\"><\/a>\n\n<b>2.3.3.1 FUNCTION: makeTimesDict<\/b><a id=\"2.3.3.1\"><\/a>\n\n\nThis will create an empty dictionary and set the keys equal to all the time interval in the `Datetime` index of `the_data`, which was retrieved after running `splitDataByDates()`.<br>\nThis will also gives the created dictionary a 'Date' key to hold all of the dates.<br>\nThis dictionary is required if you've retrieved data from yfinance with an intraday interval.\n\n<b>Expand the cell below to view the function.<\/b>","c96e7fd6":"### 3.2 All Functions<a id=\"3.2\"><\/a>\n<b>Expand the cell below to see all functions.<\/b>\n\n","d99b7be8":"### 1.2 A Brief introduction to yfinance<a id=\"1.2\"><\/a>\n\nyfinance is a package used to download historical market data of stocks from [Yahoo! Finance](https:\/\/finance.yahoo.com\/). It was developed by [Ran Aroussi](https:\/\/aroussi.com\/) and is maintained by himself along with [other contributors](https:\/\/github.com\/ranaroussi\/yfinance\/graphs\/contributors). To learn more about yfinance, [click here](https:\/\/pypi.org\/project\/yfinance\/) to view the documentation on PyPi and [click here](https:\/\/aroussi.com\/post\/python-yahoo-finance) to view the documentation on Ran Aroussi's page. You can also see the source code on GitHub by [clicking here](https:\/\/github.com\/ranaroussi\/yfinance).\n\nYou can install yfinance by running `pip install yfinance` or by downloading from [PyPi](https:\/\/pypi.org\/project\/yfinance\/#files), or by cloning\/downloading from [GitHub](https:\/\/github.com\/ranaroussi\/yfinance).\n\nI have to say, even after using yfinance for this notebook, definitely play around with it and see what else you can do with it. It's definitely a convenient way to obtain the stock market data.\n\n\nFor this example, you'll need to have yfinance installed. To install yfinance on Kaggle, you can simply run the code below. It takes like a few seconds to install it.<br>\nIf you already have yfinance installed, you can skip this step.","176c37ea":"#### 2.6.3 Understanding The Results<a id=\"2.6.3\"><\/a>\n","4171471d":"#### 2.4.3 Results After Replacement<a id=\"2.4.3\"><\/a>\n\nHere is a proper way to do it. For my example, I'm going to replace 15% of the data in 7 different rows (all at random).","f2902105":"### 1.3 A Brief Introduction to IterativeImputer<a id=\"1.3\"><\/a>\n\nIterative Imputer is a multivariate imputing strategy which models a column with the missing values (target variable) as a function of other features (predictor variables) in a [round-robin fashion](https:\/\/scikit-learn.org\/stable\/modules\/impute.html#:~:text=It%20does%20so%20in%20an%20iterated%20round-robin%20fashion), and uses that estimate for imputation. The source code can be found on GitHub by [clicking here](https:\/\/github.com\/scikit-learn\/scikit-learn\/blob\/844b4be24\/sklearn\/impute\/_iterative.py).\n\nThere are a number of parameters that you can see in [IterativeImputer's documentations](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.impute.IterativeImputer.html#:~:text=impute%20import%20IterativeImputer-,Parameters,-estimatorestimator%20object%2C%20default). It requires no parameter input as the default parameters are already set for you, meaning that, after you finish importing necessary packages, you can simply introduce an Iterative Imputer by running: `imp = IterativeImputer()`.\n\nThe default estimator used when doing <i>scikit-learn's<\/i> `IterativeImputer` is the <i>`BayesianRidge()`<\/i> estimator.\n\nI'll be using the default estimator for this example. Feel free to try out your own!<br><br>\n><b>Note:<\/b> This estimator is still <b>experimental<\/b> for now: the predictions and the API might change without any deprecation >cycle. To use it, you need to explicitly import `enable_iterative_imputer`.<br>\n>To learn more about `IterativeImputer`, click [here](https:\/\/scikit->learn.org\/stable\/modules\/generated\/sklearn.impute.IterativeImputer.html).\n\n<br><br>Because Iterative Imputer is a multivariate imputing method, it's dealing with multiple features. So how exactly does `IterativeImputer` know which feature to choose first to impute missing values for? That is a good question, and I'll address that in the following section.\n\n### 1.4 How to be the \"Chosen\u00a0One\"<a id=\"1.4\"><\/a>\n\n\nThe \"current feature\" is chosen based on the `imputation_order` given where the default order is \"ascending,.\" This means that the feature <i>chosen<\/i> first will be the feature containing the least missing values. After imputing all missing values in the chosen feature, the feature no longer contains missing values, so it is no longer the feature with the <i>least<\/i> missing values, hence the imputer will move onto the next feature with the least missing values.\n\nThe other orders are:\n- descending: features with most missing values to fewest\n- roman: left to right\n- arabic: right to left\n- random: a random order for each round\n\n\n### 1.5 The Process Behind the Iterations<a id=\"1.5\"><\/a>\n\nNow that you know how to be the Chosen One, let's understand the process behind the iterations.<br>\nIterative Imputer initially initializes the missing values with the value passed for `initial_strategy`, where the initial strategy is the \"mean\" for each feature.<br>\nThe imputer then uses an estimator (where the default estimator used is [Bayesian Ridge](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.linear_model.BayesianRidge.html)) at each step of the round-robin imputation. At each step, a feature column is chosen as the target variable `y` and the other feature columns are treated as predictor variables `X`. The missing values are then predicted for each features. This process is repeated for at most `max_iter` number of times, where the default is 10 times (rounds). The reason why I say \"at most\" and not \"exactly\" is because early stopping is enabled due to the default parameterization of `sample_posterior=False`.\u00a0<br>\n\n> The stopping criterion is met once<br>\n> `max(abs(X_t - X_{t-1}))\/max(abs(X[known_vals])) < tol`,\u00a0<br>\n> where `X_t` is `X` <i>at iteration<\/i> `t`.\n\nThis essentially means that for a single missing value, there are at most `max_iter` number of predictions and the iterations stop once the difference between the previous iteration prediction and current iteration prediction for a value is smaller than the given `tol` value (default = 1e-3).<br>\n\nThere are several other parameters that you can mess around with, be sure to try them out on your own dataset! Now enough of this nerdy stuff, let's see some action.\n<br><br><br>\n[back to table of contents](#toc)","42bb451a":"Let's now verify that we have equal dimensions before attempting make a dataframe out of our dictionary.","3a1883d5":"### 2.6 The Results<a id=\"2.6\"><\/a>\nGreat! Now that we've got our dataframe with imputed values, let's finally compare the imputated values with the original values.","1d21b724":"#### 2.2.1 The Data So Far<a id=\"2.2.1\"><\/a>\n\nLet's see what we've got from yfinance.\nIf you run `googl`, you'll get a table looking similar to this:","363a15ea":"Perfect.\n\nNow all we have left is to fill up our dictionary with the proper values.<br>\nLet's do this by using my function below:","f97103fd":"#### 2.4.2 Machines Can Get Confused Too<a id=\"2.4.2\"><\/a>\nHere are two examples which will result in an error:","0d400799":"Let's look at the head."}}