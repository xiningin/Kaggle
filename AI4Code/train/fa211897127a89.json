{"cell_type":{"703878f4":"code","9a774748":"code","49ed8772":"code","e7ba4130":"code","051abe9c":"code","6fae8b3d":"code","3c6064df":"code","f05b1250":"code","d86c94b1":"code","fb8ea37a":"code","b3e311bf":"code","39380d34":"code","65dcf596":"code","e5ef1ff3":"code","b52c7f16":"code","b2cf32c4":"code","a600d110":"code","8cb98726":"code","037fd1e8":"code","191264f0":"code","d5d2febd":"code","c51d7b9d":"code","7d3a9e94":"code","98f6c0ed":"code","14810266":"code","fbafca8e":"code","2168ec04":"code","e9409def":"code","c1cbec0f":"code","92dc06cf":"code","44cf9a82":"code","0265f03d":"code","0f05c2d4":"code","f84c0fcf":"code","cfd363f4":"code","6f31fc29":"code","5caaaae9":"code","1dac9f62":"code","18059ee9":"code","c9834a4b":"code","e682e175":"code","247474f7":"code","99e7f125":"code","b9cb0e4b":"code","f51b241b":"code","d3f7f479":"code","1e560077":"code","5c31e330":"code","38dfcdc2":"code","44133259":"code","77fe84a5":"markdown","185e741c":"markdown","7fd3fe4f":"markdown","7424a0cb":"markdown","db7434ec":"markdown","39ced9f7":"markdown","52c35013":"markdown"},"source":{"703878f4":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt","9a774748":"data = pd.read_csv(\"..\/input\/housing-prices\/housing2 (1) (1).csv\", sep = \",\" , encoding=\"utf-8\")","49ed8772":"data","e7ba4130":"data_copy = data.copy()","051abe9c":"plt.figure(figsize=(12,8))\nsns.heatmap(data_copy.corr(), annot = True)","6fae8b3d":"sns.catplot(y=\"longitude\", x=\"latitude\",data=data_copy,palette=\"twilight\",alpha=0.2)\n#Using a scatter plot we can see clearly that the high-density areas in california","3c6064df":"data_copy.hist(bins=75,figsize=(16,14))","f05b1250":"data.info()","d86c94b1":"data.isnull().sum()\n# Sum of null values in each column","fb8ea37a":"data['households'].replace(\"no\", np.nan , inplace=True)\n# in the 'households' column the value 'no' indicates null value so, we replace it with null","b3e311bf":"data.isnull().sum()\n# Sum of null values after the replacement in the 'household' column","39380d34":"len(data)-len(data.drop_duplicates())\n# No duplicates found in the data set","65dcf596":"data['housing_median_age'] = data['housing_median_age'].replace(np.nan , data['housing_median_age'].mean())","e5ef1ff3":"data.dropna(subset=[\"population\"] , inplace = True)","b52c7f16":"gender_count = data['gender'].value_counts().sum()\nmale_count = data['gender'].value_counts()[1]\nfemale_count = data['gender'].value_counts()[0]\ngender_count","b2cf32c4":"male_ratio = male_count \/ gender_count\nfemale_ratio = female_count \/ gender_count\ngender_list = ['female' , \"male\"]\ndata['gender'] = data['gender'].fillna(pd.Series(np.random.choice(gender_list,  p=[female_ratio,male_ratio], size=len(data))))\nsns.set_theme(style=\"whitegrid\")        # To add lines to the background adding more details\nx = round(data['gender'].value_counts()\/data.shape[0]*100,2) # Rounds the value of the percentage of males and females to nearest 2 decimals\nx.plot.bar(color ='maroon')  # plotting the ratio","a600d110":"data['gender'].value_counts()","8cb98726":"plt.scatter(data['total_bedrooms'], data['total_rooms'])","037fd1e8":"ratio = data['total_bedrooms']\/data['total_rooms']\nratio","191264f0":"# Calculate the mean of each value of the ratio\nmean_total_bedrooms = ratio.mean()\n# Filling Nan values with the mean value of the ratio multiplied by the corresponding value\ndata['total_bedrooms'].fillna(mean_total_bedrooms * data['total_rooms'] , inplace=True)","d5d2febd":"data['total_bedrooms'].hist()","c51d7b9d":"data['total_rooms'].hist()","7d3a9e94":"#fill_list = data['median_income']\n#data['median_income'] = data['median_income'].fillna(pd.Series(np.random.choice(fill_list , size = len(data.index))))\n# Used the values in ' median_income ' to randomly fill the missing values\nratio_median= data['median_income']\/data['median_house_value']\nmeanratio_median = ratio_median.mean()\ndata['median_income'].fillna(meanratio_median * data['median_house_value'] , inplace=True)","98f6c0ed":"#data.dropna(subset=[\"median_income\"] , inplace = True)\ndata.isnull().sum() ","14810266":"data['median_income'].hist()","fbafca8e":"data['population']=data['population'].astype('int64')\ndata['households'].fillna(0,inplace= True)\ndata['households']=data['households'].astype('int64')","2168ec04":"ratio_popul_house = data['households']\/data['population']\nmeanratio_popul_house = ratio_popul_house.mean()\ndata['households'].fillna(meanratio_popul_house * data['population'] , inplace=True)\ndata.isnull().sum()","e9409def":"data.info()","c1cbec0f":"data['total_bedrooms']=data['total_bedrooms'].astype('int64')","92dc06cf":"data.plot(kind = \"box\" , subplots = True , figsize = (18,15) ,  layout = (3,3) )\nplt.show()","44cf9a82":"data['ocean_proximity'].unique()","0265f03d":"data.replace(\"female\", 1 , inplace = True)\ndata.replace(\"male\", 0 , inplace = True) \nplt.figure(figsize=(12,8))\nsns.heatmap(data.corr(), annot = True)","0f05c2d4":"data.drop([\"ocean_proximity\"] , axis = 1 , inplace = True)\ndata.drop([\"gender\"] , axis = 1 , inplace = True)\n# Dropping columns because: Ocean proximity values are stored in strings and if we encode it to int, we are going to have\n# random data which have no meaning\n# Same for Gender but, in my opinion we do not even need a Gender column anyways","f84c0fcf":"X = data.drop(\"median_house_value\", axis = 1).values\ny = data['median_house_value'].values","cfd363f4":"from sklearn.model_selection import train_test_split\nX_train,X_test,y_train,y_test = train_test_split(X,y, test_size=0.15 , random_state=10)","6f31fc29":"from sklearn.preprocessing import StandardScaler\nscale = StandardScaler ()\nX_train = scale.fit_transform(X_train)\nX_test = scale.fit_transform(X_test)","5caaaae9":"from sklearn.linear_model import Ridge\nrid = Ridge()\n# fit model\nrid.fit(X_train,y_train)","1dac9f62":"rid.score(X_train,y_train)","18059ee9":"rid.score(X_test,y_test)","c9834a4b":"y_predict = rid.predict(X_test)\ny_predict","e682e175":"from sklearn.metrics import r2_score\nr2 = r2_score(y_test , y_predict)\nr2","247474f7":"X2 = data.drop(\"population\", axis = 1).values\ny2 = data['population'].values","99e7f125":"from sklearn.model_selection import train_test_split\nX2_train,X2_test,y2_train,y2_test = train_test_split(X2,y2, test_size=0.15 , random_state=10)","b9cb0e4b":"from sklearn.preprocessing import StandardScaler\nscale = StandardScaler ()\nX2_train = scale.fit_transform(X2_train)\nX2_test = scale.fit_transform(X2_test)","f51b241b":"from sklearn.linear_model import LinearRegression\nrid = LinearRegression()\nrid.fit(X2_train,y2_train)","d3f7f479":"rid.score(X2_train,y2_train)","1e560077":"rid.score(X2_test,y2_test)","5c31e330":"y2_predict = rid.predict(X_test)","38dfcdc2":"from sklearn.metrics import r2_score\nr2_s = r2_score(y2_test , y2_predict)\nr2_s","44133259":"import statsmodels.api as sm\nfrom scipy import stats\nX3 = sm.add_constant(X)\nest = sm.OLS(y,X3)\nest2 = est.fit()\nprint(est2.summary())","77fe84a5":"## Data cleaning \/ preprocessing","185e741c":"## Creating the model and applying Regression","7fd3fe4f":"### That is a low successful r squared score (tried several methods like linear reg. , robust reg. and checked on other notebooks) so, we conclude that we have an inconsistent data set as, columns like 'population' and 'total_bedrooms' can easily be predicted but, House_median_value is hard to predict with the set of data we have (only have a strong relation between median_income)","7424a0cb":"### Removing the null values in the 'total_bedrooms' column\n#### First we want to see the relationship between 'total_bedrooms' and 'total_rooms'","db7434ec":"### A relitavely good prediction percentage when it comes to 'population'","39ced9f7":"## EDA ","52c35013":"### Feature Selection"}}