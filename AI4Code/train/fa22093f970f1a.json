{"cell_type":{"a8392a8e":"code","7ee1a7c4":"code","b1d9c1b3":"code","f70c3b98":"code","456685bb":"code","9d72eefe":"code","cdfff982":"code","fb502558":"code","cf782484":"code","6f415424":"code","abe53f6e":"code","fee7c4fe":"code","022cfa2a":"code","6fc8430d":"markdown","fa1918d3":"markdown","d8c67e5a":"markdown","43081402":"markdown","569d33c0":"markdown","27e9f918":"markdown","de2f9800":"markdown","96862f00":"markdown","e5dc6a19":"markdown","6b3dae71":"markdown","4b3f5f43":"markdown","dd66290b":"markdown","58755847":"markdown","00889c29":"markdown","d11a95b8":"markdown","1f7c5abd":"markdown","756d7261":"markdown","102ac2ec":"markdown","2a055f64":"markdown","767070fb":"markdown"},"source":{"a8392a8e":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom glob import glob","7ee1a7c4":"name_train = sorted(glob(\"\/kaggle\/input\/enseeiht\/cerfacs\/TRAIN\/*\"))\nname_test = sorted(glob(\"\/kaggle\/input\/enseeiht\/cerfacs\/TEST\/*\"))\n\ny_train = np.load(\"\/kaggle\/input\/enseeiht\/cerfacs\/y_train.npy\")\n\nprint (len(name_train), len(name_test))","b1d9c1b3":"import matplotlib.pyplot as plt\nfrom PIL import Image\n\nnum = np.random.randint(len(name_train))\nplt.figure(figsize=(6, 6))\nplt.title(\"Image {} : {}\".format(num, y_train[num]))\nplt.imshow(Image.open(name_train[num]));","f70c3b98":"figure = plt.figure(figsize=(12, 12))\nsize = 5\ngrid = plt.GridSpec(size, size, hspace=0.05, wspace=0.0)\n\nfor line in range(size):\n    for col in range(size):\n        figure.add_subplot(grid[line, col])\n        num = np.random.randint(len(name_train))\n        plt.imshow(Image.open(name_train[num]))\n        plt.axis('off')  ","456685bb":"X_train = np.array([np.array(Image.open(jpg)) for jpg in name_train])\nX_test = np.array([np.array(Image.open(jpg)) for jpg in name_test])\ny_train = np.load(\"\/kaggle\/input\/enseeiht\/cerfacs\/y_train.npy\")\n\nprint (X_train.shape, X_test.shape)\nprint (y_train.shape)","9d72eefe":"print (y_train[0])","cdfff982":"from sklearn.preprocessing import OneHotEncoder\n\nprint (f\"Shape label raw : {y_train.shape}\")\n\nencoder = OneHotEncoder()\ny_train = encoder.fit_transform(y_train.reshape(-1, 1)).toarray()\n\nprint (f\"Shape label One Hot Encoded : {y_train.shape}\")\nprint (f\"Label for y_train[0] : {y_train[0]}\")","fb502558":"X_train, X_valid = X_train[:15000], X_train[15000:]\ny_train, y_valid = y_train[:15000], y_train[15000:]","cf782484":"X_train, X_valid, X_test = X_train\/255, X_valid\/255, X_test\/255","6f415424":"import keras \nfrom keras.models import Sequential\nfrom keras.layers import Dense, Conv2D, Flatten, MaxPooling2D\n\nmodel = Sequential()\nmodel.add(Conv2D(32, kernel_size = (3, 3), activation='relu', input_shape=(64, 64, 3)))\nmodel.add(Conv2D(32, kernel_size = (3, 3), activation='relu'))\nmodel.add(Conv2D(32, kernel_size = (3, 3), activation='relu'))\nmodel.add(MaxPooling2D(pool_size=(2,2)))\nmodel.add(Conv2D(32, kernel_size = (3, 3), activation='relu'))\nmodel.add(Conv2D(32, kernel_size = (3, 3), activation='relu'))\nmodel.add(Conv2D(32, kernel_size = (3, 3), activation='relu'))\nmodel.add(Flatten())\nmodel.add(Dense(128, activation='relu'))\nmodel.add(Dense(10, activation = 'softmax'))","abe53f6e":"model.compile(loss=keras.losses.categorical_crossentropy,\n              optimizer=keras.optimizers.SGD(),\n              metrics=['accuracy'])","fee7c4fe":"history = model.fit(X_train, y_train, batch_size = 32, \n                   validation_data=(X_valid, y_valid), epochs=30)","022cfa2a":"loss, metrics = model.evaluate(X_valid, y_valid)\n\nprint (metrics)","6fc8430d":"If we look at the X_train shape for example, we can see that it contains 20 000 images of size (64*64) with 3 channels (RGB).  \ny_train is a list containing the land cover for each of the X_train samples.","fa1918d3":"## 3) Training a model on this dataset","d8c67e5a":"You will need GPU for this course, make sure the GPU option on the right is 'On'.","43081402":"## 1) Loading data","569d33c0":"However despite the decent accuracy obtained on the validation set, the accuracy on the training set is almost perfect with about 99.9%.\nThis huge gap between the accuracy on the training an validation set is a clear sign of [overfitting](here) : when a model starts memorizing specific pattern of the training set and do not generalize well to new data.\n\nTherefore, our main concern now will be to reduce this overfitting.","27e9f918":"And we scale our inputs data so that each pixel value has a value between 0 and 1 :","de2f9800":"And we start the training phase :","96862f00":"ML algorithm cannot work with label directly so we need to use a one hot encoding for our data :  \nEvery label will be represented as a binary vector with 0 everywhere and 1 for the index of its class.","e5dc6a19":"# IAP202-Land_Cover_Classification","6b3dae71":"We first define the architecture of our network, a basic CNN with few convolutional layers and a dense layers with 10 neurons : one for every possible classes.","4b3f5f43":"We obtained an accuracy of about 75%, that's a good start !","dd66290b":"## 2) Pre processing","58755847":"Our dataset is divided in a training and a test set containing respectively 20 000 and 7000 images.  \nLet's visualise some of them with [matplotlib](https:\/\/matplotlib.org\/3.1.1\/api\/_as_gen\/matplotlib.pyplot.plot.html):","00889c29":"We can also convert this list into a numpy array, which is more suitable for common mainpulations :","d11a95b8":"We then create a validation set to evaluate our models during the training phase :","1f7c5abd":"Once our model is trained, we can check its accuracy on the validation set :","756d7261":"Our database consists of 27 000 RGB images of size 64*64.  \nWe will use the [numpy](https:\/\/www.numpy.org\/) library to process these data.","102ac2ec":"We then compile this model with an [SGD](https:\/\/keras.io\/optimizers\/) optimizer and a [categorical crossentropy](https:\/\/keras.io\/metrics\/) as loss :","2a055f64":"## 4) The overfitting","767070fb":"We would like to automatically classify the land cover of different landscapes.  \n![mozaic](https:\/\/docs.google.com\/uc?export=download&id=1HypyX6kYGjEdt8M7E_J2JDy29BppoGkr)"}}