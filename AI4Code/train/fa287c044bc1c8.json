{"cell_type":{"60611398":"code","afa777ea":"code","4641f28c":"code","03ff376e":"code","a59bf0ba":"code","caaf254a":"code","24d7b58e":"code","f5b5621a":"code","4b1a7461":"code","021e9a32":"code","714ee98b":"code","5fb35927":"code","a108c0a6":"code","17c5e1c4":"code","ec76cc0f":"code","c95fb98d":"code","e8c1ff06":"code","c50c25d8":"code","9ee1d093":"code","d98e4e50":"code","2735bf10":"code","73445956":"code","1b7fac8e":"code","55ffc701":"code","041bd1d7":"code","5190c21d":"code","17e78d49":"code","dac3efd7":"code","d635f773":"code","6ff0eeb4":"code","f9293d59":"code","7e98fa34":"code","e2fe1cbc":"code","d823a182":"code","5724387e":"code","17d440f2":"code","de818e67":"code","76150c6e":"code","4b72acae":"code","ea85b7ee":"code","04f1b67e":"markdown","617e6810":"markdown","99373c15":"markdown","5d9f074e":"markdown","109b5f68":"markdown","d625b57c":"markdown","1a2c6b7a":"markdown","88bd795c":"markdown","e2c12770":"markdown","4a556859":"markdown","0de53f34":"markdown","87bda7f7":"markdown","8077870a":"markdown","e0d9a77a":"markdown","23d80ee4":"markdown","ed8701da":"markdown","a15f8def":"markdown","b23bb791":"markdown","6321942b":"markdown","443953b3":"markdown","582d3008":"markdown","9e3d397f":"markdown","a5ed4fdd":"markdown","9deb1665":"markdown","f5c0f79d":"markdown","ab3231e4":"markdown","1ea78611":"markdown","e31eaf53":"markdown","7ae52c44":"markdown","fef7da3d":"markdown","805c77d6":"markdown","93be8416":"markdown","a1ad62eb":"markdown","16944d48":"markdown","c9097779":"markdown","2f18dc89":"markdown","8acefd83":"markdown","77fdfeb4":"markdown","ca4323db":"markdown","9cf13a94":"markdown","34b96cc1":"markdown","9a2d0f3c":"markdown","8075054a":"markdown"},"source":{"60611398":"import pandas as pd\nimport numpy as np\n\n# sklearn preprocessing for dealing with categorical variables and PCA\nfrom sklearn.preprocessing import LabelEncoder, Imputer\nfrom sklearn.decomposition import PCA\nfrom sklearn.pipeline import Pipeline\n\n# File system manangement\nimport os\n\n# For string manipulation\nimport re\n\n# Suppress warnings \nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Missing value analysis\nimport missingno as msno\n\n# modeling \nimport lightgbm as lgb\n\n# utilities\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.preprocessing import LabelEncoder\n\n# # Plotly Library\n# from plotly.offline import init_notebook_mode, iplot\n# import plotly.graph_objs as go\n# import plotly.plotly as py\n# from plotly import tools\n# import plotly.figure_factory as ff\n# init_notebook_mode(connected=True)","afa777ea":"df_train = pd.read_csv('..\/input\/train.csv')\ndf_test = pd.read_csv('..\/input\/test.csv')\ndf_all = pd.concat([df_train, df_test], sort=False)\ndf = df_train.copy()","4641f28c":"data = []\nfor col in df.columns:\n    if col == 'Target':\n        role = 'target'\n    elif col == 'Id':\n        role = 'id'\n    else:\n        role = 'input'\n    \n    col_dict = {\n        'varname': col,\n        'role': role,\n        'dtype': df[col].dtype,\n        'nunique': df[col].nunique(),\n        'response_rate': 100 * df[col].notnull().sum() \/ df.shape[0]\n    }\n    data.append(col_dict)\n\nmeta = pd.DataFrame(data, columns=['varname', 'role', 'dtype', 'nunique', 'response_rate'])","03ff376e":"foo = [\n(\"v2a1\",\" Monthly rent payment\"),\n(\"hacdor\",\" =1 Overcrowding by bedrooms\"),\n(\"rooms\",\"  number of all rooms in the house\"),\n(\"hacapo\",\" =1 Overcrowding by rooms\"),\n(\"v14a\",\" =1 has toilet in the household\"),\n(\"refrig\",\" =1 if the household has refrigerator\"),\n(\"v18q\",\" owns a tablet\"),\n(\"v18q1\",\" number of tablets household owns\"),\n(\"r4h1\",\" Males younger than 12 years of age\"),\n(\"r4h2\",\" Males 12 years of age and older\"),\n(\"r4h3\",\" Total males in the household\"),\n(\"r4m1\",\" Females younger than 12 years of age\"),\n(\"r4m2\",\" Females 12 years of age and older\"),\n(\"r4m3\",\" Total females in the household\"),\n(\"r4t1\",\" persons younger than 12 years of age\"),\n(\"r4t2\",\" persons 12 years of age and older\"),\n(\"r4t3\",\" Total persons in the household\"),\n(\"tamhog\",\" size of the household\"),\n(\"tamviv\",\" no. of persons living in the household\"),\n(\"escolari\",\" years of schooling\"),\n(\"rez_esc\",\" Years behind in school\"),\n(\"hhsize\",\" household size\"),\n(\"paredblolad\",\" =1 if predominant material on the outside wall is block or brick\"),\n(\"paredzocalo\",\" =1 if predominant material on the outside wall is socket (wood, zinc or absbesto\"),\n(\"paredpreb\",\" =1 if predominant material on the outside wall is prefabricated or cement\"),\n(\"pareddes\",\" =1 if predominant material on the outside wall is waste material\"),\n(\"paredmad\",\" =1 if predominant material on the outside wall is wood\"),\n(\"paredzinc\",\" =1 if predominant material on the outside wall is zink\"),\n(\"paredfibras\",\" =1 if predominant material on the outside wall is natural fibers\"),\n(\"paredother\",\" =1 if predominant material on the outside wall is other\"),\n(\"pisomoscer\",\" =1 if predominant material on the floor is mosaic ceramic   terrazo\"),\n(\"pisocemento\",\" =1 if predominant material on the floor is cement\"),\n(\"pisoother\",\" =1 if predominant material on the floor is other\"),\n(\"pisonatur\",\" =1 if predominant material on the floor is  natural material\"),\n(\"pisonotiene\",\" =1 if no floor at the household\"),\n(\"pisomadera\",\" =1 if predominant material on the floor is wood\"),\n(\"techozinc\",\" =1 if predominant material on the roof is metal foil or zink\"),\n(\"techoentrepiso\",\" =1 if predominant material on the roof is fiber cement,   mezzanine \"),\n(\"techocane\",\" =1 if predominant material on the roof is natural fibers\"),\n(\"techootro\",\" =1 if predominant material on the roof is other\"),\n(\"cielorazo\",\" =1 if the house has ceiling\"),\n(\"abastaguadentro\",\" =1 if water provision inside the dwelling\"),\n(\"abastaguafuera\",\" =1 if water provision outside the dwelling\"),\n(\"abastaguano\",\" =1 if no water provision\"),\n(\"public\",\" =1 electricity from CNFL,  ICE, ESPH\/JASEC\"),\n(\"planpri\",\" =1 electricity from private plant\"),\n(\"noelec\",\" =1 no electricity in the dwelling\"),\n(\"coopele\",\" =1 electricity from cooperative\"),\n(\"sanitario1\",\" =1 no toilet in the dwelling\"),\n(\"sanitario2\",\" =1 toilet connected to sewer or cesspool\"),\n(\"sanitario3\",\" =1 toilet connected to  septic tank\"),\n(\"sanitario5\",\" =1 toilet connected to black hole or letrine\"),\n(\"sanitario6\",\" =1 toilet connected to other system\"),\n(\"energcocinar1\",\" =1 no main source of energy used for cooking (no kitchen)\"),\n(\"energcocinar2\",\" =1 main source of energy used for cooking electricity\"),\n(\"energcocinar3\",\" =1 main source of energy used for cooking gas\"),\n(\"energcocinar4\",\" =1 main source of energy used for cooking wood charcoal\"),\n(\"elimbasu1\",\" =1 if rubbish disposal mainly by tanker truck\"),\n(\"elimbasu2\",\" =1 if rubbish disposal mainly by botan hollow or buried\"),\n(\"elimbasu3\",\" =1 if rubbish disposal mainly by burning\"),\n(\"elimbasu4\",\" =1 if rubbish disposal mainly by throwing in an unoccupied space\"),\n(\"elimbasu5\",\" =1 if rubbish disposal mainly by throwing in river,   creek or sea\"),\n(\"elimbasu6\",\" =1 if rubbish disposal mainly other\"),\n(\"epared1\",\" =1 if walls are bad\"),\n(\"epared2\",\" =1 if walls are regular\"),\n(\"epared3\",\" =1 if walls are good\"),\n(\"etecho1\",\" =1 if roof are bad\"),\n(\"etecho2\",\" =1 if roof are regular\"),\n(\"etecho3\",\" =1 if roof are good\"),\n(\"eviv1\",\" =1 if floor are bad\"),\n(\"eviv2\",\" =1 if floor are regular\"),\n(\"eviv3\",\" =1 if floor are good\"),\n(\"dis\",\" =1 if disable person\"),\n(\"male\",\" =1 if male\"),\n(\"female\",\" =1 if female\"),\n(\"estadocivil1\",\" =1 if less than 10 years old\"),\n(\"estadocivil2\",\" =1 if free or coupled uunion\"),\n(\"estadocivil3\",\" =1 if married\"),\n(\"estadocivil4\",\" =1 if divorced\"),\n(\"estadocivil5\",\" =1 if separated\"),\n(\"estadocivil6\",\" =1 if widow\/er\"),\n(\"estadocivil7\",\" =1 if single\"),\n(\"parentesco1\",\" =1 if household head\"),\n(\"parentesco2\",\" =1 if spouse\/partner\"),\n(\"parentesco3\",\" =1 if son\/doughter\"),\n(\"parentesco4\",\" =1 if stepson\/doughter\"),\n(\"parentesco5\",\" =1 if son\/doughter in law\"),\n(\"parentesco6\",\" =1 if grandson\/doughter\"),\n(\"parentesco7\",\" =1 if mother\/father\"),\n(\"parentesco8\",\" =1 if father\/mother in law\"),\n(\"parentesco9\",\" =1 if brother\/sister\"),\n(\"parentesco10\",\" =1 if brother\/sister in law\"),\n(\"parentesco11\",\" =1 if other family member\"),\n(\"parentesco12\",\" =1 if other non family member\"),\n(\"idhogar\",\" Household level identifier\"),\n(\"hogar_nin\",\" Number of children 0 to 19 in household\"),\n(\"hogar_adul\",\" Number of adults in household\"),\n(\"hogar_mayor\",\" # of individuals 65+ in the household\"),\n(\"hogar_total\",\" # of total individuals in the household\"),\n(\"dependency\",\" Dependency rate\"),\n(\"edjefe\",\" years of education of male head of household\"),\n(\"edjefa\",\" years of education of female head of household\"),\n(\"meaneduc\",\"average years of education for adults (18+)\"),\n(\"instlevel1\",\" =1 no level of education\"),\n(\"instlevel2\",\" =1 incomplete primary\"),\n(\"instlevel3\",\" =1 complete primary\"),\n(\"instlevel4\",\" =1 incomplete academic secondary level\"),\n(\"instlevel5\",\" =1 complete academic secondary level\"),\n(\"instlevel6\",\" =1 incomplete technical secondary level\"),\n(\"instlevel7\",\" =1 complete technical secondary level\"),\n(\"instlevel8\",\" =1 undergraduate and higher education\"),\n(\"instlevel9\",\" =1 postgraduate higher education\"),\n(\"bedrooms\",\" number of bedrooms\"),\n(\"overcrowding\",\" # persons per room\"),\n(\"tipovivi1\",\" =1 own and fully paid house\"),\n(\"tipovivi2\",\" =1 own,   paying in installments\"),\n(\"tipovivi3\",\" =1 rented\"),\n(\"tipovivi4\",\" =1 precarious\"),\n(\"tipovivi5\",\" =1 other(assigned\"),\n(\"computer\",\" =1 if the household has notebook or desktop computer,   borrowed)\"),\n(\"television\",\" =1 if the household has TV\"),\n(\"mobilephone\",\" =1 if mobile phone\"),\n(\"qmobilephone\",\" # of mobile phones\"),\n(\"lugar1\",\" =1 region Central\"),\n(\"lugar2\",\" =1 region Chorotega\"),\n(\"lugar3\",\" =1 region Pac\u00c3\u0192\u00c2\u00adfico central\"),\n(\"lugar4\",\" =1 region Brunca\"),\n(\"lugar5\",\" =1 region Huetar Atl\u00c3\u0192\u00c2\u00a1ntica\"),\n(\"lugar6\",\" =1 region Huetar Norte\"),\n(\"area1\",\" =1 zona urbana\"),\n(\"area2\",\" =2 zona rural\"),\n(\"age\",\" Age in years\"),\n(\"SQBescolari\",\" escolari squared\"),\n(\"SQBage\",\" age squared\"),\n(\"SQBhogar_total\",\" hogar_total squared\"),\n(\"SQBedjefe\",\" edjefe squared\"),\n(\"SQBhogar_nin\",\" hogar_nin squared\"),\n(\"SQBovercrowding\",\" overcrowding squared\"),\n(\"SQBdependency\",\" dependency squared\"),\n(\"SQBmeaned\",\" meaned squared\"),\n(\"agesq\",\" Age squared\"),]\n\ndescription = pd.DataFrame(foo, columns=['varname', 'description'])","a59bf0ba":"meta = meta.merge(description, on='varname')","caaf254a":"meta.sort_values(by='response_rate').head(10)","24d7b58e":"df.info()","f5b5621a":"df.head()","4b1a7461":"meta.iloc[:20,:]","021e9a32":"df.loc[df['v2a1'].isnull(),['Id','Target']].groupby(by='Target').count().plot()","714ee98b":"print(df['v18q1'].unique())\ndf.loc[df['v18q1'].isnull(),['Id','Target']].groupby(by='Target').count().plot()","5fb35927":"meta.iloc[20:80,:]","a108c0a6":"meta.iloc[80:141,:]","17c5e1c4":"((df['Target'].value_counts())*100\/len(df)).plot(kind='bar')\nplt.ylabel(\"Percent\")","ec76cc0f":"#Correlation heatmap with target variable for 20 most correlated variables (Credit - https:\/\/www.kaggle.com\/ishaan45)\ncorrmat = df.corr().abs()['Target'].sort_values(ascending=False).drop('Target')\ncorr_df = corrmat.to_frame(name='values')\nplt.figure(figsize=(12,8))\nsns.heatmap(corr_df[:20])","c95fb98d":"meta.loc[meta['varname'].isin(corr_df[:20].index)]","e8c1ff06":"def missing_values(df):\n    total = df.isnull().sum().sort_values(ascending=False)\n    percent = (df.isnull().sum()*100\/df.isnull().count()).sort_values(ascending=False)\n    missing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\n    return missing_data","c50c25d8":"# Scikit-learn Imputer for missing-value imputation\ndf_numerical = df.select_dtypes(exclude=['object'])\nimputed_df = pd.DataFrame(Imputer(missing_values ='NaN', strategy='mean', axis=0).fit_transform(df_numerical.values), columns=df_numerical.columns)","9ee1d093":"# Missing value analysis using 'missingno' package by Aleksey Bilogur\nprint(msno.matrix(df_train.sample(100)))\n\n# Zooming-in on first 25 features\nprint(msno.matrix(df_train.iloc[0:100, :25]))","d98e4e50":"missing_df = missing_values(df).head(10).reset_index()\nmissing_df","2735bf10":"meta.loc[meta['varname'].isin(missing_df['index'].head(5).values)]","73445956":"df['v2a1'].fillna(0.0, inplace = True)\ndf['v18q1'].fillna(0.0, inplace = True)\ndf['rez_esc'].fillna(0.0, inplace = True)\n\ndf['meaneduc'].fillna(df['meaneduc'].mean(), inplace = True)\ndf['SQBmeaned'].fillna(df['SQBmeaned'].mean(), inplace = True)","1b7fac8e":"missing_values(df).head(5)","55ffc701":"# No. of unique values in categorical columns\ndf.select_dtypes('object').apply(pd.Series.nunique, axis = 0)","041bd1d7":"#Identifying categorical\/binary values \nmeta.sort_values(by='nunique')","5190c21d":"#get binary variable dataframe\nbinary = list(meta.loc[meta['nunique'].values == 2, 'varname']) + [\"Target\"]\n\nfig = plt.figure(figsize=(25,120))\nfig.subplots_adjust(hspace=0.4)\n\nfor i,col in enumerate(binary[:len(binary)-1]):\n    ax = fig.add_subplot(26,4,(i+1))\n    sns.violinplot(x='Target', y=col, data=df, hue='Target', ax=ax)\n    title = meta.loc[meta['varname'] == col, 'description'].iloc[0]\n    title_clean = re.sub(\"[^a-zA-Z ]\",\"\", title)\n    plt.title(title_clean)\n    ax.legend_.remove()","17e78d49":"cols_assets = [\"v18q\", \"Refrig\", \"computer\", \"television\", \"mobilephone\"]\ntitles_assets = [\"Tablet\", \"Refrigirator\", \"Computer\", \"Television\", \"MobilePhone\"]","dac3efd7":"dict_outside = {'paredblolad' : \"Block \/ Brick\", \"paredpreb\" : \"Cement\", \"paredmad\" : \"Wood\",\n      \"paredzocalo\" : \"Socket\", \"pareddes\" : \"Waste Material\", \"paredfibras\" : \"Fibres\",\n      \"paredother\" : \"Other\", \"paredzinc\": \"Zink\"}\n\ndict_floor = {'pisomoscer' : \"Mosaic \/ Ceramic\", \"pisocemento\" : \"Cement\", \"pisonatur\" : \"Natural Material\",\n      \"pisonotiene\" : \"No Floor\", \"pisomadera\" : \"Wood\", \"pisoother\" : \"Other\"}\n\ndict_roof = {'techozinc' : \"Zinc\", \"techoentrepiso\" : \"Fibre \/ Cement\", \"techocane\" : \"Natural Fibre\", \"techootro\" : \"Other\"}\n\ndict_sanitary = {'sanitario1' : \"No Toilet\", \"sanitario2\" : \"Sewer \/ Cesspool\", \"sanitario3\" : \"Septic Tank\",\n       \"sanitario5\" : \"Black Hole\", \"sanitario6\" : \"Other System\"}\n\ndict_energy = {'energcocinar1' : \"No Kitchen\", \"energcocinar2\" : \"Electricity\", \"energcocinar3\" : \"Cooking Gas\",\n       \"energcocinar4\" : \"Wood Charcoal\"}\n\ndict_disposal = {\"elimbasu1\":\"Tanker truck\", \"elimbasu2\": \"Buried\", \"elimbasu3\": \"Burning\", \"elimbasu4\": \"Unoccupied space\", \n       \"elimbasu5\": \"River\", \"elimbasu6\": \"Other\"}\n\ntitles_residence = [\"Outside Wall Material\", \"Floor Material\", \"Roof Material\", \"Sanitary Conditions\", \"Cooking Energy Sources\", \"Disposal Methods\"]","d635f773":"dict_edu = {\"instlevel1\": \"No Education\", \"instlevel2\": \"Incomplete Primary\", \"instlevel3\": \"Complete Primary\", \n       \"instlevel4\": \"Incomplete Sc.\", \"instlevel5\": \"Complete Sc.\", \"instlevel6\": \"Incomplete Tech Sc.\",\n       \"instlevel7\": \"Complete Tech Sc.\", \"instlevel8\": \"Undergraduation\", \"instlevel9\": \"Postgraduation\"}\n\ndict_marital = {\"estadocivil1\": \"< 10 years\", \"estadocivil2\": \"Free \/ Coupled union\", \"estadocivil3\": \"Married\", \n       \"estadocivil4\": \"Divorced\", \"estadocivil5\": \"Separated\", \"estadocivil6\": \"Widow\", \"estadocivil7\": \"Single\"}\n\ndict_member = {\"parentesco1\": \"Household Head\", \"parentesco2\": \"Spouse\/Partner\", \"parentesco3\": \"Son\/Daughter\", \n       \"parentesco4\": \"Stepson\/Daughter\", \"parentesco5\" : \"Son\/Daughter in Law\" , \"parentesco6\": \"Grandson\/Daughter\", \n       \"parentesco7\": \"Mother\/Father\", \"parentesco8\": \"Mother\/Father in Law\", \"parentesco9\" : \"Brother\/Sister\" , \n       \"parentesco10\" : \"Brother\/Sister in law\", \"parentesco11\" : \"Other Family Member\", \"parentesco12\" : \"Other Non Family Member\"}","6ff0eeb4":"# Note: We have to observe manually for the limit of unique values limit, here it is 22\nmeta[meta['nunique'].values > 2].sort_values(by='nunique')","f9293d59":"categorical = list(meta.loc[(meta['nunique'].between(3, 22, inclusive=True)) & (meta['dtype']!='object'), 'varname'])\n\nfig = plt.figure(figsize=(20,30))\nfig.subplots_adjust(hspace=0.4)\n\nfor i,col in enumerate(categorical[:len(categorical)-1]):\n    ax = fig.add_subplot(7,4,(i+1))\n    sns.barplot(x='Target', y=col, data=df, ax=ax)\n    title = meta.loc[meta['varname'] == col, 'description'].iloc[0]\n    title_clean = re.sub(\"[^a-zA-Z ]\",\"\", title)\n    plt.title(title_clean)","7e98fa34":"meta[meta['nunique'].values >20]","e2fe1cbc":"cont_collist = list(meta.loc[(meta['nunique'] > 20) & (meta['dtype']!='object'), 'varname'])\n\nfig = plt.figure(figsize=(15,15))\nfig.subplots_adjust(hspace=0.4)\n\nfor i,col in enumerate(cont_collist[:len(cont_collist)]):\n    ax = fig.add_subplot(4,3,(i+1))\n    sns.kdeplot(df[col], legend=False, ax=ax)\n    title = meta.loc[meta['varname'] == col, 'description'].iloc[0]\n    title_clean = re.sub(\"[^a-zA-Z ]\",\"\", title)\n    plt.title(title_clean)","d823a182":"# Threshold for removing correlated variables\nthreshold = 0.9\n\n# Absolute value correlation matrix\ncorr_matrix = df.corr().abs()\ncorr_matrix.head()\n\n# Upper triangle of correlations\nupper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))\n\n# Select columns with correlations above threshold\nto_drop = [column for column in upper.columns if any(upper[column] > threshold)]\nprint('There are %d columns to remove.' % (len(to_drop)))\n\n# Drop correlated variables\ntrain_df = df.drop(columns = to_drop)\nprint('Training shape: ', df.shape)","5724387e":"train_df = train_df.drop(df.select_dtypes('object').columns, axis=1).reset_index()\n\n# Initialize an empty array to hold feature importances\nfeature_importances = np.zeros(train_df.shape[1])\n\n# Create the model with several hyperparameters\nmodel = lgb.LGBMClassifier(objective='binary', boosting_type = 'goss', n_estimators = 10000, \n                           class_weight = 'balanced')","17d440f2":"labels = train_df['Target']\nids = train_df['index']\n\n# Fit the model twice to avoid overfitting\nfor i in range(2):\n    \n    # Split into training and validation set\n    train_features, valid_features, train_y, valid_y = train_test_split(train_df, labels, test_size = 0.25, \n                                                                        random_state = i)\n    \n    # Train using early stopping\n    model.fit(train_features, train_y, early_stopping_rounds=100, eval_set = [(valid_features, valid_y)], \n              eval_metric = 'auc', verbose = 200)\n    \n    # Record the feature importances\n    feature_importances += model.feature_importances_","de818e67":"# Make sure to average feature importances! \nfeature_importances = feature_importances \/ 2\nfeature_importances = pd.DataFrame({'feature': list(train_df.columns), 'importance': \n                                    feature_importances}).sort_values('importance', ascending = False)\n\nfeature_importances.head()","76150c6e":"# Make sure to drop the ids and target\ndf_pca = df.copy().drop(df.select_dtypes('object').columns, axis=1).reset_index()\nlabels = df_pca['Target']\nids = df_pca['index']\ndf_pca = df_pca.drop(columns = ['index', 'Target'])\n\n# Make a pipeline with imputation and pca\npipeline = Pipeline(steps = [('imputer', Imputer(strategy = 'median')),\n             ('pca', PCA())])\n\n# Fit and transform on the training data\ndf_pca = pipeline.fit_transform(df_pca)","4b72acae":"# Extract the pca object\npca = pipeline.named_steps['pca']\n\n# Plot the cumulative variance explained\nplt.figure(figsize = (5, 4))\nplt.plot(list(range(df_pca.shape[1])), np.cumsum(pca.explained_variance_ratio_), 'r-')\nplt.xlabel('Number of PC'); plt.ylabel('Cumulative Variance Explained');\nplt.title('Cumulative Variance Explained with PCA');","ea85b7ee":"# Dataframe of pca results\ndf_pca_final = pd.DataFrame({'pc_1': df_pca[:, 0], 'pc_2': df_pca[:, 1], 'target': labels})\n\n# Plot pc2 vs pc1 colored by target\nsns.lmplot('pc_1', 'pc_2', data = df_pca_final, hue = 'target', fit_reg=False, size = 5)\nplt.title('PC2 vs PC1 by Target')\n\nprint('2 principal components account for {:.4f}% of the variance.'.format\n      (100 * np.sum(pca.explained_variance_ratio_[:2])))","04f1b67e":"## Data Cleaning\n1. Missing value treatment\n2. Outliers treatment\n3. Handling categorical variables","617e6810":"### Build metaframe","99373c15":"### Households Assets","5d9f074e":" ## Feature Engineering\n Credit - https:\/\/www.kaggle.com\/willkoehrsen\n \nWhile choosing the right model and optimal settings are important, the model can only learn from the data it is given. Making sure this data is as relevant to the task as possible is the job of the data scientist (and maybe some automated tools to help us out).\n\n> Feature engineering refers to a geneal process and can involve both **feature construction**: adding new features from the existing data, and **feature selection**: choosing only the most important features or other methods of dimensionality reduction. There are many techniques we can use to both create features and select features.","109b5f68":"> Again, the plot is similar to the above one. I assume the missing value is equivalent  to 0. The most probable reason might be that the survey form didn't have '0' as an option.","d625b57c":"**Categorical Variable Visualization Strategy:** We will plot Violin Plot Grid using Seaborn, comparing binary variables against all 4 type of target household categories.","1a2c6b7a":"> Data is already 1-hot encoded. So we don't need to handle categorical variables here","88bd795c":"### Remove correlated variables","e2c12770":"## Visual Analysis (with target variable)\n1.   Binary Variables \n2.  Categorical Variables\n3. Continuous Variables","4a556859":"> Target Variable 4 (non vulnerable households) has the most missing values of 'Monthly Rent Payment'. The most obvious reason is that the person owns a house, so don't have to pay the rent. So we can safely deduce that missing entry is equivalent to 0. ","0de53f34":"## Problem Understanding","87bda7f7":"* As already discussed, missing values in 'v2a1', 'v18q1' are equivalent to 0. Same logic is applicable on 'rez_esc'.\n* We can either drop or better impute missing values in 'meaneduc' and 'SQBmeaned' variables.","8077870a":"In short, We are considering,\n1. **About Individuals**: Education levels, marital status, Individual's role in the family (father, mother etc.)\n2. **About Family**: Family size, Assets owned (tv, refrigerator, tablet), Family structure (i.e. younger\/older males,females)\n3. **About House\/Residence**: House facilities (e.g. toilet facility) , House build parameters (e.g. wall material), Sanity factors (e.g. Rubbish disposal), household geographical region\n* And some transformed variabels (e.g. Age squared)","e0d9a77a":"Let's see the description of these 5 variables with missing values (using meta dataframe)","23d80ee4":"## Few words on Exploratory Data Analysis","ed8701da":"> Note: Id variable seems to be a unique identifier. Data is partially (or fully, may be) encoded (with one-hot encoding, mostly). 'Target' is our target variable (datatype - 'ordered')","a15f8def":"## A compiled list of various EDA steps (suggestions welcomed!)\n\n### Data Statistics\n* Shape\n* Peek\n* Description\n* Skew\n\n\n### Data Cleaning & Preparation\n* Missing Value Treatment\n* Outliers Handling\n* Categorical Variable Encoding (one-hot encoding in most cases)\n* Data Tidying (Melting, Pivoting & Concatenating)\n\n\n### Visual Exploration\n* Correlation and Heatmaps\n* Scatter Plot\n* Box and Density Plots\n* Grouping of one-hot encoded variables\n\n> A short note on visualization fundamentals (when to use which kind of plot):\n1. Single Categorical Variable (Count-Plot)\n2. Continuous Variables XY (Scatter-Plot and\/or Pair-Plot)\n3. Single Continuous Variable Y (Box-Plot  and\/or Violin-Plot and\/or KDE-Plot)\n\n### Transformation\n* Correction of Skew\n\n\n### Feature Engineering\n* Creating new features (Log, Squared, Cubed, Sq. Root etc.)\n* Selecting important features (Manual, XGBoost)\n\n### Additional concepts\/libraries (to understand data more clearly, especially for datasets with large features)\n* Creating Meta Dataframe\n* Automated Feature engineering\n* Missing values analysis (using 'missingno' package by Aleksey Bilogur)\n* 3-Stage process for identiffying important-features\n    * Stage 1: Correlation Heatmap\n    * Stage 2: Zoomed Heatmap on Target Variable Correlations\n    * Stage 3: Scatter Plot between Target Variable and Correlated Variables","b23bb791":"### 3. Handling categorical variables\n* Identify categorical variables and unique values in each column (meta dataframe comes handy here!)","6321942b":"## The 4 C's of Data Cleaning: Correcting, Completing, Creating, and Converting\nIn this stage, we will clean our data by 1) correcting aberrant values and outliers, 2) completing missing information, 3) creating new features for analysis, and 4) converting fields to the correct format for calculations and presentation.\n\n**Correcting**: Reviewing the data, there does not appear to be any aberrant or non-acceptable data inputs. In addition, we see we may have potential outliers in age and fare. However, since they are reasonable values, we will wait until after we complete our exploratory analysis to determine if we should include or exclude from the dataset. It should be noted, that if they were unreasonable values, for example age = 800 instead of 80, then it's probably a safe decision to fix now. However, we want to use caution when we modify data from its original value, because it may be necessary to create an accurate model.\n\n**Completing**: There are null values or missing data in the age, cabin, and embarked field. Missing values can be bad, because some algorithms don't know how-to handle null values and will fail. While others, like decision trees, can handle null values. Thus, it's important to fix before we start modeling, because we will compare and contrast several models. There are two common methods, either delete the record or populate the missing value using a reasonable input. It is not recommended to delete the record, especially a large percentage of records, unless it truly represents an incomplete record. Instead, it's best to impute missing values. A basic methodology for qualitative data is impute using mode. A basic methodology for quantitative data is impute using mean, median, or mean + randomized standard deviation. An intermediate methodology is to use the basic methodology based on specific criteria; like the average age by class or embark port by fare and SES. There are more complex methodologies, however before deploying, it should be compared to the base model to determine if complexity truly adds value. For this dataset, age will be imputed with the median, the cabin attribute will be dropped, and embark will be imputed with mode. Subsequent model iterations may modify this decision to determine if it improves the model\u2019s accuracy.\n\n**Creating**: Feature engineering is when we use existing features to create new features to determine if they provide new signals to predict our outcome. For this dataset, we will create a title feature to determine if it played a role in survival.\n\n**Converting**: Last, but certainly not least, we'll deal with formatting. There are no date or currency formats, but datatype formats. Our categorical data imported as objects, which makes it difficult for mathematical calculations. For this dataset, we will convert object datatypes to categorical dummy variables.","443953b3":"We are also considering **House build parameters (e.g. wall material), Sanity factors (e.g. Rubbish disposal), marriage status**.\n\nMoreover, most of the <int64> datatype variables are binary in nature. Also, elimbasu5 <index 61> has only 1 type of value.","582d3008":"** 1. The objective of EDA is to understand the problem in order to generate testable hypotheses**\n\nIn addition to summaries, also look at transforms of the data and re-scalings of the data. Flush out interesting structures that you can describe.\n\n\n\n** 2. Take notes. Take lots of notes. Ask lots of questions of the data**\n\nThe outcomes like the graphs and summary statistics are only for you to improve your understanding, not to demonstrate a relationship in the data to a general audience.\n\n\n\n** 3. Focus on Understanding**\n\nThe results are ultimately throw-away, and all that you should be left with is a greater understanding and intuition for the data and a long list of hypotheses to explore when modeling.\n\n\n>**In short,**\nExploratory Data Analysis (EDA) is an open-ended process where we calculate statistics and make figures to find trends, anomalies, patterns, or relationships within the data. The goal of EDA is to learn what our data can tell us. It generally starts out with a high level overview, then narrows in to specific areas as we find intriguing areas of the data. The findings may be interesting in their own right, or they can be used to inform our modeling choices, such as by helping us decide which features to use.","9e3d397f":"Let's know more about these highly correlated features","a5ed4fdd":"### Missing Data  using meta dataframe","9deb1665":"### House build features","f5c0f79d":"**v18q1** has only around 23% data available. I think missing","ab3231e4":"1. As we can see, the transformed variables has high correlation with our Target variable.\n2. Owning a Tablet is quite a differentiator.\n3. No. of younger persons (less than 12 years) also highly correlated with Target variable. (another interesting but predictable relation)\n4. House build quality has already been identified by IDB as a high correlated variable. (people invest in houses only after fulfillment of basic needs)\n5. No. of children and education level.....(again a predictable relation, there are many socio-economic theories on this relation)","1ea78611":"## Concept : Meta DataFrame\nCredit: https:\/\/www.kaggle.com\/youhanlee","e31eaf53":"### Build feature description frame","7ae52c44":"### Dimensionality Reduction using PCA (Principal Component Analysis)","fef7da3d":"Let's verify that missing values has been successfully treated","805c77d6":"### Analysing Target variable","93be8416":"### 'Education level', 'Marital status' and 'Family members' for household individuals","a1ad62eb":"Since there are 141 variables, lets use our meta dataframe to philosophically understand one by one. It is very important for us to understand variables (or I must say features).","16944d48":"### Categorical Bar plot visualization","c9097779":"### Feature selection using LightGBM","2f18dc89":"> Note: Large no. of columns with approximately 9500 observations. 5 object, 8 float and 130 integer variables.","8acefd83":"### Continuous variable visualization","77fdfeb4":"### Correlation Heatmap","ca4323db":"### 1. Missing value treatment","9cf13a94":"Now, we will see this target variable relationship with various categorical and numerical variables.","34b96cc1":"So, we are considering **Family size, House facilities (toilets), Assets (tv, refrigerator, tablet), Schooling, Family structure (i.e. younger\/older males,females).**\n\nMoreover,  **Monthly Rent Payment** variable has only around 28% entries and 157 unique values. Why such low response rate?","9a2d0f3c":"### Merge and build meta dataframe","8075054a":"> Almost 60% individuals belong to category 4 ('non vulnerable households') in out Training dataset. "}}