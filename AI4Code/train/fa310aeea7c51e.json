{"cell_type":{"0f25462d":"code","def19c39":"code","d681d34a":"code","d8500d95":"code","350db478":"code","c567b0c6":"code","e933eaa2":"code","404cf01a":"code","ea295ab8":"markdown","ceab4fb8":"markdown","08c88e97":"markdown","a4bfb6c5":"markdown","cfc43048":"markdown","f4de4780":"markdown","70f80c4a":"markdown","e15b1f82":"markdown","39662c78":"markdown","1a25f407":"markdown"},"source":{"0f25462d":"    import numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn import datasets\nfrom sklearn.gaussian_process import GaussianProcessClassifier\nfrom sklearn.gaussian_process.kernels import RBF\nfrom sklearn.model_selection import GridSearchCV,RandomizedSearchCV\nfrom sklearn.metrics import classification_report\nfrom sklearn.gaussian_process import GaussianProcessRegressor\nfrom sklearn.gaussian_process.kernels import DotProduct\nfrom sklearn.gaussian_process.kernels import Matern","def19c39":"# import the dataset and form the subsets\ndigits = datasets.load_digits()\n\nX = digits.data\ny = np.array(digits.target, dtype = int)\n\nN,d = X.shape\n\nN = np.int(1797)\nNtrain = np.int(1697)\nNtest = np.int(100)\n\n\nXtrain = X[0:Ntrain-1,:]\nytrain = y[0:Ntrain-1]\nXtest = X[N-100:N,:]\nytest = y[N-100:N]\n","d681d34a":"#Data Visualization\n\ndef plotPictures(data, nrow=0, ncol =0):\n  if nrow==0 or ncol == 0:\n    rows = int(data.shape[0]**0.5)\n    cols = int(data.shape[0]**0.5-np.finfo(float).eps)+1\n  else:\n    rows = nrow\n    cols = ncol\n    \n  fig, ax = plt.subplots(nrows=rows,ncols=cols,figsize=(15,15))\n  ax = ax.flatten()\n  for a,im,index in zip(ax,data, range(rows*cols)):\n      if len(im.shape) == 1:\n          im = np.reshape(im,[int(im.shape[0]**0.5),-1])\n      a.imshow(im, cmap='gray')\n      #a.set_title(\"Image {}\".format(index+1))\n  for i in range(rows*cols):\n    ax[i].axis('off')\n  plt.show() \n  fig.savefig(\"numbers.pdf\")\n  \nplotPictures(Xtest,10,10)","d8500d95":"#Training phase\n\ngpc_rbf = GaussianProcessClassifier().fit(Xtrain, ytrain)\n\nyp_test = gpc_rbf.predict(Xtest)\nprint('Test error rate')\nprint(np.mean(np.not_equal(yp_test,ytest)))","350db478":"model = GaussianProcessClassifier(kernel=DotProduct(1.0))\n\nparam_grid = {'kernel':[DotProduct(i) for i in [0.2, 0.5, 1,2,3,5]] + [Matern(i) for i in [0.2, 0.5, 1,2,3,5]]  + [RBF(i) for i in [0.2, 0.5, 1,2,3,5]]}\nclf = RandomizedSearchCV(model, param_grid,n_jobs=4,n_iter=20,random_state=0,verbose=3)\nclf.fit(Xtrain, ytrain)\n\nparam = clf.best_params_    \nprint('Best params : ',param)\nbest = clf.best_estimator_ \n\nyp_test = best.predict(Xtest)\ntest_error_rate = (np.mean(np.not_equal(yp_test,ytest)))\nprint('Test error rate')\nprint(test_error_rate)\n","c567b0c6":"#create one-hot vector for the targets. i.e. the labels we want to correctly classify\nyhotvec = [ ]\nfor n in range(0, len(y)):\n\n    zeros = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n    yvalue = y[n]\n    zeros[yvalue] += 1\n    yhotvec.append(zeros)\n\nyhotvectrain = yhotvec[0:Ntrain-1]\nyhotvectest = yhotvec[N-100:N]","e933eaa2":"gpc_rbr = GaussianProcessRegressor().fit(Xtrain, yhotvectrain)\nyp_test = gpc_rbr.predict(Xtest)\nyp_test = np.argmax(yp_test, axis = 1)\nyhotvectest = np.argmax(yhotvectest, axis = 1)\ntest_error_rate = np.mean(np.not_equal(yp_test, yhotvectest))\n\nprint('Test error rate')\nprint(test_error_rate)","404cf01a":"model = GaussianProcessRegressor(kernel = 1.0 * RBF(1.0), normalize_y = False)\n\nparam_grid = {'kernel':[Matern(i) for i in [0.2, 0.5, 1,2,3,5]]  + [RBF(i) for i in [0.2, 0.5, 1,2,3,5]]}\nclf = RandomizedSearchCV(model, param_grid,n_jobs=4,n_iter=20,random_state=0,verbose=3)\nclf.fit(Xtrain, yhotvectrain)\n\nparam = clf.best_params_    \nprint('Best params : ',param)\nbest = clf.best_estimator_ \n\nyp_test = best.predict(Xtest)\nyp_test = np.argmax(yp_test, axis = 1)\ntest_error_rate = np.mean(np.not_equal(yp_test, yhotvectest))\nprint('Test error rate')\nprint(test_error_rate)","ea295ab8":"We can reach a decent enough accuracy with this model, but the training phase is already quite long (roughly 2 hours). This nice thing though is that our model is still really simple (only one hyperparameter).\n\nLet's keep trying the Gaussian Processes, but using this time a regression approch.","ceab4fb8":"Simplest approch : Try to classify the test set using the basic sklearn GP regressor : ","08c88e97":"This was way faster than the previous optimization, and the results are far better. Even if this work on the kernel was not mandatory for this exemple (the previous results are good enough), it can surely be more important for some other classification problems.","a4bfb6c5":"It is really bad. We will see how we can improve this accuracy by adding only only parameter : a specified kernel (among DotProduct, Matern and RBF), with a specified length scale.","cfc43048":"## Introduction\n\nI will try to classify pictures of digits thanks to Gaussian Processes. I will only use ressources available on scikit-learn to do so. First, one need to import the required modules, download the data and separate it into a training set and a test set. Thus, one can visualize the test set, to get some insights about the difficulty of the problem. ","f4de4780":"## Conclusion\n\nThis classification problem was actually an assigment in a Machine Learning course. The other methods that we had to try were SVM and CNN. I already did some scholar and professional Machine Learning projects, and I instantaneously thought that CNN was the best approach for this task. I was surprised by the results of my classmates with GPs, so I did this little project for my personal insight.\n\nThis little test of the performance of GPs went above and beyond what I expected. The results for the the GP Regressor WITH NO PROVIDED HYPERPARAMETERS are astounding ! I will definitively remember it for my future classification problems. ","70f80c4a":"## GP for classification (Classifiers)\n\nSimplest approch : Try to classify the test set using the basic sklearn GP classifier : ","e15b1f82":"The results are outstanding !! Let's see, mainly for fun, if one can improve the results by optimizing the kernel : ","39662c78":"## GP for Regression\n\nTo use regression models for classification problems, one need to transform its features. A simple approach is to create one-hot vector for the target. We can select the corresponding class afterwards using an argmax function. ","1a25f407":"One can see that some pictures are hard to classify. In the mean, a human would probably have around 95% accuracy while classifying those kind of picture. Therefore, our goal will be to create the simplest model to classify those with a similar accuracy. To do so, we will be using Gaussian Processes."}}