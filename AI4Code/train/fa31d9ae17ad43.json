{"cell_type":{"9dd489df":"code","aa79106e":"code","cb3272ad":"code","c622a6d5":"code","bde66b56":"code","a0690240":"code","f680de00":"code","131bd500":"code","e78ff478":"code","7e797e2d":"code","b2d3b007":"code","5e14223d":"code","44f86ad1":"code","290e513b":"code","6ffe8e3e":"code","e2c66933":"code","fee2989a":"code","1c0698db":"code","667259db":"markdown","019aa5a0":"markdown","8cb2cd24":"markdown","72b0c407":"markdown","7511769b":"markdown","ffce13a3":"markdown","79a073ec":"markdown","0d87508c":"markdown","65fa185b":"markdown","dfd117df":"markdown","d0da8646":"markdown","90113cdc":"markdown","d93353ec":"markdown","7d9e48e1":"markdown","19c912dd":"markdown","85446f2f":"markdown","557d7a26":"markdown","698ef8d0":"markdown","c38bc6f5":"markdown","18776b1d":"markdown","a1a64ff5":"markdown"},"source":{"9dd489df":"import pip._internal as pip\npip.main(['install', '--upgrade', 'numpy==1.17.3'])\nimport numpy as np\nimport pandas as pd\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nfrom lightgbm import LGBMClassifier\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.ensemble import BaggingClassifier\nfrom sklearn.ensemble import ExtraTreesClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.experimental import enable_hist_gradient_boosting\nfrom sklearn.ensemble import HistGradientBoostingClassifier\nfrom sklearn.ensemble import IsolationForest\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import RandomTreesEmbedding\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score, confusion_matrix\nfrom sklearn.model_selection import cross_val_predict, cross_val_score\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.svm import SVC\nfrom sklearn.utils.multiclass import unique_labels\nfrom xgboost import XGBClassifier\n\nimport time\n\nfrom lwoku import get_prediction","aa79106e":"N_ESTIMATORS = 2000\nMIN_SAMPLE_LEAFS = 100\nRANDOM_STATE = 42\nN_JOBS = -1\nVERBOSE = 0","cb3272ad":"# Read training and test files\nX_train = pd.read_csv('..\/input\/learn-together\/train.csv', index_col='Id', engine='python')\nX_test = pd.read_csv('..\/input\/learn-together\/test.csv', index_col='Id', engine='python')\n\n# Define the dependent variable\ny_train = X_train['Cover_Type'].copy()\n\n# Define a training set\nX_train = X_train.drop(['Cover_Type'], axis='columns')","c622a6d5":"lr_clf = LogisticRegression(verbose=VERBOSE,\n                            random_state=RANDOM_STATE,\n                            n_jobs=1)","bde66b56":"lda_clf = LinearDiscriminantAnalysis()","a0690240":"knn_clf = KNeighborsClassifier(n_jobs=N_JOBS)","f680de00":"gnb_clf = GaussianNB()","131bd500":"svc_clf = SVC(random_state=RANDOM_STATE,\n              verbose=True)","e78ff478":"bg_clf = BaggingClassifier(n_estimators=N_ESTIMATORS,\n                           verbose=VERBOSE,\n                           random_state=RANDOM_STATE)","7e797e2d":"xt_clf = ExtraTreesClassifier(n_estimators=N_ESTIMATORS,\n                              min_samples_leaf=MIN_SAMPLE_LEAFS,\n                              verbose=VERBOSE,\n                              random_state=RANDOM_STATE,\n                              n_jobs=N_JOBS)","b2d3b007":"rf_clf = RandomForestClassifier(n_estimators=N_ESTIMATORS,\n                                min_samples_leaf=MIN_SAMPLE_LEAFS,\n                                verbose=VERBOSE,\n                                random_state=RANDOM_STATE,\n                                n_jobs=N_JOBS)","5e14223d":"ab_clf = AdaBoostClassifier(n_estimators=N_ESTIMATORS,\n                            random_state=RANDOM_STATE)","44f86ad1":"gb_clf = GradientBoostingClassifier(n_estimators=N_ESTIMATORS,\n                              min_samples_leaf=MIN_SAMPLE_LEAFS,\n                              verbose=VERBOSE,\n                              random_state=RANDOM_STATE)","290e513b":"lg_clf = LGBMClassifier(n_estimators=N_ESTIMATORS,\n                        num_leaves=MIN_SAMPLE_LEAFS,\n                        verbosity=VERBOSE,\n                        random_state=RANDOM_STATE,\n                        n_jobs=N_JOBS)","6ffe8e3e":"xg_clf = XGBClassifier(random_state=RANDOM_STATE,\n                       n_jobs=-N_JOBS,\n                       learning_rate=0.1,\n                       n_estimators=100,\n                       max_depth=3)","e2c66933":"models = [\n          ('lr', lr_clf),\n          ('lda', lda_clf),\n          ('knn', knn_clf),\n          ('gnb', gnb_clf),\n          ('svc', svc_clf),\n          ('bg', bg_clf),\n          ('xt', xt_clf),\n          ('rf', rf_clf),\n          ('ab', ab_clf),\n          ('gb', gb_clf),\n          ('lg', lg_clf),\n          ('xg', xg_clf)\n]","fee2989a":"results = pd.DataFrame(columns = ['Model',\n                                  'Accuracy',\n                                  'Fit time',\n                                  'Predict test set time',\n                                  'Predict train set time'])\n\nfor name, model in models:\n\n    # Fit\n    t0 = time.time()\n    model.fit(X_train, y_train)\n    t1 = time.time()\n    t_fit = (t1 - t0)\n    \n    # Predict test set\n    t0 = time.time()\n    y_test_pred = pd.Series(model.predict(X_test), index=X_test.index)\n    t1 = time.time()\n    t_test_pred = (t1 - t0)\n\n    # Predict train set\n    t0 = time.time()\n    y_train_pred = pd.Series(get_prediction(model, X_train, y_train), index=X_train.index)\n    accuracy = accuracy_score(y_train, y_train_pred)\n    t1 = time.time()\n    t_train_pred = (t1 - t0)\n\n    # Submit\n    y_train_pred.to_csv('train_' + name + '.csv', header=['Cover_Type'], index=True, index_label='Id')\n    y_test_pred.to_csv('submission_' + name + '.csv', header=['Cover_Type'], index=True, index_label='Id')\n    print('\\n')\n    \n    results = results.append({\n        'Model': name,\n        'Accuracy': accuracy,\n        'Fit time': t_fit,\n        'Predict test set time': t_test_pred,\n        'Predict train set time': t_train_pred\n    }, ignore_index = True)","1c0698db":"results = results.sort_values('Accuracy', ascending=False).reset_index(drop=True)\nresults.to_csv('results.csv', index=True, index_label='Id')\nresults","667259db":"#### [Bagging classifier](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.ensemble.BaggingClassifier.html#sklearn.ensemble.BaggingClassifier)","019aa5a0":"#### [LightGBM](https:\/\/lightgbm.readthedocs.io\/en\/latest\/pythonapi\/lightgbm.LGBMClassifier.html#lightgbm.LGBMClassifier)","8cb2cd24":"### Boosting","72b0c407":"### [Discriminant Analysis](https:\/\/scikit-learn.org\/stable\/modules\/classes.html#module-sklearn.discriminant_analysis)\n\n#### [Linear Discriminant Analysis](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.discriminant_analysis.LinearDiscriminantAnalysis.html#sklearn.discriminant_analysis.LinearDiscriminantAnalysis)","7511769b":"# Models\n## Define","ffce13a3":"#### [Extra-trees classifier](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.ensemble.ExtraTreesClassifier.html#sklearn.ensemble.ExtraTreesClassifier)","79a073ec":"### [Naive Bayes](https:\/\/scikit-learn.org\/stable\/modules\/classes.html#module-sklearn.naive_bayes)\n\n#### [Gaussian Naive Bayes](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.naive_bayes.GaussianNB.html#sklearn.naive_bayes.GaussianNB)","0d87508c":"# Do you know some other model that is not in this list?\n\nPlease, help me to find all the possible models (good or not) to analyse.","65fa185b":"# Define constants\n## Set model parameters\n\n- n_estimators = 2000. A big value to get a tight fit model.\n- min_sample_leafs = 100. To equalize all models.\n- random_state = 42. To get always the same results. And get always the same random split. 42 is the answer to the ultimate question of life, the universe, and everything.\n- n_jobs = -1. Use all processors.\n- verbose = 0. Per default, not nag this notebook. It could be change for testing.","dfd117df":"### [Nearest Neighbors](https:\/\/scikit-learn.org\/stable\/modules\/classes.html#module-sklearn.neighbors)\n\n#### [k-nearest neighbors classifier](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.neighbors.KNeighborsClassifier.html#sklearn.neighbors.KNeighborsClassifier)","d0da8646":"#### [Gradient Boosting for classification](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.ensemble.GradientBoostingClassifier.html#sklearn.ensemble.GradientBoostingClassifier)","90113cdc":"# Conclusions\nThe best model with the default parameters,\nand without doing feature selection and feature engineering,\nis the Bagging classifier.\n\nAfter checking all the models and choosing the best one (Bagging classifier).\nThis one offers an improvement of about a 12 % in the accuracy respect the random forest classifier.\n\n## Adequacy of the models\n\nSome models have taken into account, but finally, not added to this list. As not all the models are adequate for the data, or for the nature of the model.\n\n### Isolation Forest Algorithm\n\nReturns the anomaly score of each sample, not the prediction.\n\nIt's useful for detecting outliers.\n\n### LightGBM\n\nIt's working extremely slowly (150 times slower) with numper 1.16.4 version.\nWith 2000 estimators the notebook reach the 9 hours limit.\n\n### Ensemble of totally random trees\n\nThis model has no predict method.\n\n### Histogram-based Gradient Boosting Classification\n\nThis model is experimental, and it takes too long.","d93353ec":"#### [Random forest classifier](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.ensemble.RandomForestClassifier.html#sklearn.ensemble.RandomForestClassifier)","7d9e48e1":"# Prepare data","19c912dd":"# Introduction\n\nThis notebook checks all known classifiers with almost all their default parameters to see how good are they in this data set.\n\nSome parameters have been configured to compare the models on equal terms:\n\nThe results are collected in [Tactic 99. Summary](https:\/\/www.kaggle.com\/juanmah\/tactic-99-summary).\n\nThe models in this notebook are, on purpose, not optimized.\nThey will be optimized in successive notebooks in this tactic series,\nwhere some tactics will be tested and the results analysed.","85446f2f":"### Model list","557d7a26":"#### [XGBoost](https:\/\/xgboost.readthedocs.io\/en\/latest\/python\/python_api.html?#xgboost.XGBClassifier)","698ef8d0":"### [Generalized Linear Models](https:\/\/scikit-learn.org\/stable\/modules\/classes.html#module-sklearn.linear_model)\n#### [Logistic Regression classifier](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.linear_model.LogisticRegression.html)","c38bc6f5":"### [Ensemble Methods](https:\/\/scikit-learn.org\/stable\/modules\/classes.html#module-sklearn.ensemble)\n\n### Bagging\n","18776b1d":"#### [AdaBoost classifier](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.ensemble.AdaBoostClassifier.html#sklearn.ensemble.AdaBoostClassifier)","a1a64ff5":"### [Support Vector Machines](https:\/\/scikit-learn.org\/stable\/modules\/classes.html#module-sklearn.svm)\n\n#### [C-Support Vector Classification](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.svm.SVC.html#sklearn.svm.SVC)"}}