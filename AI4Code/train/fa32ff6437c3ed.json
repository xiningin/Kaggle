{"cell_type":{"6e8c233b":"code","2f09bfa9":"code","0535da13":"code","6929ac13":"code","3c208598":"code","f93179e5":"code","28c1afb6":"code","62636970":"code","1dd20484":"code","c822b0e8":"code","2c71d090":"code","655ede72":"code","1ccc50b0":"code","3a65673d":"code","b6bdaa0d":"code","c4b671b6":"code","4872d52d":"code","022ba80d":"code","53a1ff65":"code","5e3f21f5":"code","a97b8e22":"code","a7096307":"code","e82eda5e":"code","62fe8af3":"code","30d6f5ba":"code","7f874d13":"code","5441fa52":"code","4bb7d95d":"code","c6e67a6b":"code","db23ee9d":"code","160b8563":"code","7f22895a":"code","ef0c91e9":"code","1864c842":"code","96c05e99":"code","477c8aff":"code","cf0635c4":"code","f8169368":"code","e36575b1":"code","6cf1da08":"code","396f3878":"code","25f8a665":"code","e2b2d9cc":"code","be3beb3b":"code","84e985e9":"code","de59c899":"code","db643968":"code","8339b56d":"code","c6f2210f":"code","c317a465":"code","1d664e61":"code","75cc655e":"code","2b7f6ddb":"code","bdf805d5":"code","e77ca6be":"code","7f77d0c9":"code","05d41d86":"code","e336be05":"code","cdc40f38":"code","846e83cc":"code","8e4ecc45":"code","ed5a9856":"code","ddb5d6aa":"code","e522111f":"code","28682dc9":"code","18c713ef":"code","68910260":"code","2c43e2a4":"code","4fc2af61":"code","794d14d6":"code","1a852f50":"code","f444775d":"code","7372f6cd":"code","f76b96e6":"code","9cf75266":"code","d3643555":"markdown","2e2f4c42":"markdown","841fb7ae":"markdown","2ddbed6b":"markdown","7d525447":"markdown","7d878c25":"markdown","6d5785a4":"markdown","48dd9c96":"markdown","1f72729e":"markdown","3146357f":"markdown","6854ad97":"markdown","ef01c038":"markdown","2928135d":"markdown","183ab4de":"markdown","bbc7f4f7":"markdown","c0411448":"markdown","603e53a2":"markdown","7f05faef":"markdown","a6d96f7d":"markdown","6422a5d9":"markdown","d8ea7c5f":"markdown","20318633":"markdown","a4dea889":"markdown","4bf76f6a":"markdown","d818cfc1":"markdown","02067019":"markdown","193bfc1e":"markdown","7c9dfc44":"markdown","f1c139ff":"markdown","54582559":"markdown","bd32127f":"markdown","70a722a0":"markdown","7d81ff54":"markdown","ccc8fe22":"markdown","e1b704d3":"markdown","0eeb9dfa":"markdown","8d61146b":"markdown","2e44fcfd":"markdown","f556b114":"markdown","6764137d":"markdown","7b7af630":"markdown","b32294f6":"markdown","b865f561":"markdown","a6e28760":"markdown","a196b155":"markdown","cf16a652":"markdown","f16f520b":"markdown","5a0fadb2":"markdown","4fc60bbb":"markdown","6845ec61":"markdown","ed039d0a":"markdown","ab194707":"markdown","2b22746b":"markdown","0c12ed57":"markdown","3329a599":"markdown","5b8cb5a3":"markdown","3187982b":"markdown","2406695c":"markdown","660c423b":"markdown","595dbccc":"markdown","a02612e3":"markdown","aeb0b58b":"markdown","ef2476d4":"markdown","4af37ff2":"markdown","060150f3":"markdown","1c375a78":"markdown","082f2290":"markdown","c20be742":"markdown","9b589350":"markdown","607ca8f9":"markdown","41f3428f":"markdown"},"source":{"6e8c233b":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport os\nfrom tqdm import tqdm_notebook\n\nfrom sklearn.metrics import accuracy_score, confusion_matrix\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.naive_bayes import BernoulliNB\nimport eli5\nfrom scipy.sparse import hstack, vstack\n\nfrom wordcloud import WordCloud, STOPWORDS\nfrom collections import defaultdict\nimport plotly.graph_objs as go\nfrom plotly import tools\nimport plotly.offline as py\npy.init_notebook_mode(connected=True)\nimport textblob\nimport string","2f09bfa9":"print(os.listdir('..\/input\/'))","0535da13":"PATH = '..\/input\/aclimdb\/aclImdb\/'","6929ac13":"train_text = []\ntest_text = []\ntrain_label = []\ntest_label = []\n\nfor train_test in ['train','test']:\n    for neg_pos in ['neg','pos']:\n        file_path = PATH + train_test + '\/' + neg_pos + '\/'\n        for file in tqdm_notebook(os.listdir(file_path)):\n            with open(file_path + file, 'r') as f:\n                if train_test == 'train':\n                    train_text.append(f.read())\n                    if neg_pos == 'neg':\n                        train_label.append(0)\n                    else:\n                        train_label.append(1)\n                else:\n                    test_text.append(f.read())\n                    if neg_pos == 'neg':\n                        test_label.append(0)\n                    else:\n                        test_label.append(1)","3c208598":"X_train = pd.DataFrame()\nX_train['review'] = train_text\nX_train['label'] = train_label\n\nX_test = pd.DataFrame()\nX_test['review'] = test_text\nX_test['label'] = test_label","f93179e5":"X_train.head()","28c1afb6":"# with open(PATH + 'README') as f:\n#     readme = f.read()\n# readme","62636970":"sns.countplot(X_train['label']);","1dd20484":"X_train[X_train['label']==0]['review'].apply(lambda x: np.log1p(len(x))).hist(alpha=0.6, color='red', label='Negative')\nX_train[X_train['label']==1]['review'].apply(lambda x: np.log1p(len(x))).hist(alpha=0.6, color='green', label='Positive')\nplt.title('Character count')\nplt.legend();","c822b0e8":"X_train[X_train['label']==0]['review'].apply(lambda x: np.log1p(len(x.split()))).hist(alpha=0.6, color='red', label='Negative')\nX_train[X_train['label']==1]['review'].apply(lambda x: np.log1p(len(x.split()))).hist(alpha=0.6, color='green', label='Positive')\nplt.title('Word count')\nplt.legend();","2c71d090":"X_train[X_train['label']==0]['review'].apply(lambda x: np.log1p(len([word for word in x.split() if word.istitle()]))).hist(alpha=0.6, color='red', label='Negative')\nX_train[X_train['label']==1]['review'].apply(lambda x: np.log1p(len([word for word in x.split() if word.istitle()]))).hist(alpha=0.6, color='green', label='Positive')\nplt.title('Title word count')\nplt.legend();","655ede72":"X_train[X_train['label']==0]['review'].apply(lambda x: np.log1p(len([word for word in x.split() if word.isupper()]))).hist(alpha=0.6, color='red', label='Negative')\nX_train[X_train['label']==1]['review'].apply(lambda x: np.log1p(len([word for word in x.split() if word.isupper()]))).hist(alpha=0.6, color='green', label='Positive')\nplt.title('Upper Case Word count')\nplt.legend();","1ccc50b0":"X_train[X_train['label']==0]['review'].apply(lambda x: np.log1p(len(''.join(_ for _ in x if _ in string.punctuation)))).hist(alpha=0.6, color='red', label='Negative')\nX_train[X_train['label']==1]['review'].apply(lambda x: np.log1p(len(''.join(_ for _ in x if _ in string.punctuation)))).hist(alpha=0.6, color='green', label='Positive')\nplt.title('Punctuation Character count')\nplt.legend();","3a65673d":"def plot_wordcloud(data, title):\n    wordcloud = WordCloud(background_color='black',\n                          stopwords=set(STOPWORDS),\n                          max_words=200,\n                          max_font_size=100,\n                          random_state=17,\n                          width=800,\n                          height=400,\n                          mask=None)\n    wordcloud.generate(str(data))\n    plt.figure(figsize=(15.0,10.0))\n    plt.axis('off')\n    plt.title(title)\n    plt.imshow(wordcloud);","b6bdaa0d":"plot_wordcloud(X_train[X_train['label']==0]['review'], 'Negative IMDB reviews')","c4b671b6":"plot_wordcloud(X_train[X_train['label']==1]['review'], 'Positive IMDB reviews')","4872d52d":"X_train_neg = X_train[X_train['label']==0]\nX_train_pos = X_train[X_train['label']==1]\nadditional_stopwords = ['<br', '-', '\/><br', '\/>the', '\/>this']\n\ndef generate_ngrams(text, n_gram=1):\n    token = [token for token in text.lower().split(' ') if token != '' if token not in STOPWORDS if token not in additional_stopwords]\n    ngrams = zip(*[token[i:] for i in range(n_gram)])\n    return [' '.join(ngram) for ngram in ngrams]\n\ndef horizontal_bar_chart(df, color):\n    trace = go.Bar(\n        y=df['word'].values[::-1],\n        x=df['wordcount'].values[::-1],\n        showlegend=False,\n        orientation = 'h',\n        marker=dict(\n            color=color,\n        ),\n    )\n    return trace","022ba80d":"freq_dict = defaultdict(int)\nfor sent in X_train_neg['review']:\n    for word in generate_ngrams(sent,1):\n        freq_dict[word] += 1\nfd_sorted = pd.DataFrame(sorted(freq_dict.items(), key=lambda x: x[1])[::-1])\nfd_sorted.columns = ['word', 'wordcount']\ntrace0 = horizontal_bar_chart(fd_sorted.head(20), 'red')\n\nfreq_dict = defaultdict(int)\nfor sent in X_train_pos['review']:\n    for word in generate_ngrams(sent,1):\n        freq_dict[word] += 1\nfd_sorted = pd.DataFrame(sorted(freq_dict.items(), key=lambda x: x[1])[::-1])\nfd_sorted.columns = ['word', 'wordcount']\ntrace1 = horizontal_bar_chart(fd_sorted.head(20), 'green')\n\n# Creating two subplots\nfig = tools.make_subplots(rows=1, cols=2, vertical_spacing=0.04,horizontal_spacing=0.15,\n                          subplot_titles=['Frequent unigrams within negative reviews', \n                                          'Frequent unigrams within positive reviews'])\nfig.append_trace(trace0, 1, 1)\nfig.append_trace(trace1, 1, 2)\nfig['layout'].update(height=600, width=900, paper_bgcolor='rgb(233,233,233)', title='Unigram Count Plots')\npy.iplot(fig, filename='word-plots')","53a1ff65":"freq_dict = defaultdict(int)\nfor sent in X_train_neg['review']:\n    for word in generate_ngrams(sent,2):\n        freq_dict[word] += 1\nfd_sorted = pd.DataFrame(sorted(freq_dict.items(), key=lambda x: x[1])[::-1])\nfd_sorted.columns = ['word', 'wordcount']\ntrace0 = horizontal_bar_chart(fd_sorted.head(20), 'red')\n\nfreq_dict = defaultdict(int)\nfor sent in X_train_pos['review']:\n    for word in generate_ngrams(sent,2):\n        freq_dict[word] += 1\nfd_sorted = pd.DataFrame(sorted(freq_dict.items(), key=lambda x: x[1])[::-1])\nfd_sorted.columns = ['word', 'wordcount']\ntrace1 = horizontal_bar_chart(fd_sorted.head(20), 'green')\n\n# Creating two subplots\nfig = tools.make_subplots(rows=1, cols=2, vertical_spacing=0.04,horizontal_spacing=0.15,\n                          subplot_titles=['Frequent bigrams within negative reviews', \n                                          'Frequent bigrams within positive reviews'])\nfig.append_trace(trace0, 1, 1)\nfig.append_trace(trace1, 1, 2)\nfig['layout'].update(height=600, width=900, paper_bgcolor='rgb(233,233,233)', title='Bigram Count Plots')\npy.iplot(fig, filename='word-plots')","5e3f21f5":"X_train_train, X_train_valid, y_train_train, y_train_valid = train_test_split(X_train['review'].values, X_train['label'].values, test_size=0.3, random_state=19)","a97b8e22":"vect = TfidfVectorizer(ngram_range=(1,1), max_features=50000)\nlogit = LogisticRegression(C=1, random_state=19)\n\npipe = Pipeline([\n    ('vect', vect),\n    ('logit', logit)\n])","a7096307":"%%time\npipe.fit(X_train_train, y_train_train)","e82eda5e":"%%time\npreds_valid = pipe.predict(X_train_valid)","62fe8af3":"print('Accuracy score on the validation dataset: ',accuracy_score(y_train_valid, preds_valid))","30d6f5ba":"_,axes = plt.subplots(figsize=(6,5))\nsns.heatmap(confusion_matrix(y_train_valid, preds_valid), annot=True, ax=axes, fmt='g', cmap='Blues', square=True)\naxes.set_xlabel('Predicted labels');\naxes.set_ylabel('True labels'); \naxes.set_title('Confusion Matrix'); \naxes.xaxis.set_ticklabels(['Negative', 'Positive']);\naxes.yaxis.set_ticklabels(['Negative', 'Positive']);","7f874d13":"eli5.show_weights(vec=pipe.named_steps['vect'],\n                  estimator=pipe.named_steps['logit'],\n                  top=10)","5441fa52":"vect = TfidfVectorizer(ngram_range=(1,3), max_features=100000)\nlogit = LogisticRegression(C=1, random_state=19)\n\npipe = Pipeline([\n    ('vect', vect),\n    ('logit', logit)\n])","4bb7d95d":"%%time\npipe.fit(X_train_train, y_train_train)","c6e67a6b":"%%time\npreds_valid = pipe.predict(X_train_valid)","db23ee9d":"print('Accuracy score on the validation dataset: ',accuracy_score(y_train_valid, preds_valid))","160b8563":"eli5.show_weights(vec=pipe.named_steps['vect'],\n                  estimator=pipe.named_steps['logit'],\n                  top=10)","7f22895a":"vect = CountVectorizer(ngram_range=(1,3), max_features=100000)\nlogit = LogisticRegression(C=1, random_state=19)\n\npipe = Pipeline([\n    ('vect', vect),\n    ('logit', logit)\n])","ef0c91e9":"%%time\npipe.fit(X_train_train, y_train_train)","1864c842":"%%time\npreds_valid = pipe.predict(X_train_valid)","96c05e99":"print('Accuracy score on the validation dataset: ',accuracy_score(y_train_valid, preds_valid))","477c8aff":"eli5.show_weights(vec=pipe.named_steps['vect'],\n                  estimator=pipe.named_steps['logit'],\n                  top=10)","cf0635c4":"%%time\nvect = CountVectorizer(ngram_range=(1,3), max_features=100000)\nX_train_train_vect = vect.fit_transform(X_train_train)\nX_train_valid_vect = vect.transform(X_train_valid)","f8169368":"X_train_train_vect, X_train_valid_vect","e36575b1":"fun1 = lambda x: len(x)\nfeat1_train = [fun1(review) for review in X_train_train]\nfeat1_valid = [fun1(review) for review in X_train_valid]\n\nfun2 = lambda x: len(x.split())\nfeat2_train = [fun2(review) for review in X_train_train]\nfeat2_valid = [fun2(review) for review in X_train_valid]\n\nfun3 = lambda x: len([word for word in x.split() if word.istitle()])\nfeat3_train = [fun3(review) for review in X_train_train]\nfeat3_valid = [fun3(review) for review in X_train_valid]\n\nfun4 = lambda x: len([word for word in x.split() if word.isupper()])\nfeat4_train = [fun4(review) for review in X_train_train]\nfeat4_valid = [fun4(review) for review in X_train_valid]\n\nfun5 = lambda x: len(''.join(_ for _ in x if _ in string.punctuation))\nfeat5_train = [fun5(review) for review in X_train_train]\nfeat5_valid = [fun5(review) for review in X_train_valid]","6cf1da08":"X_train_train_custom = hstack([\n    X_train_train_vect,\n    pd.DataFrame(feat1_train),\n    pd.DataFrame(feat2_train),\n    pd.DataFrame(feat3_train),\n    pd.DataFrame(feat4_train),\n    pd.DataFrame(feat5_train)\n])\nX_train_valid_custom = hstack([\n    X_train_valid_vect,\n    pd.DataFrame(feat1_valid),\n    pd.DataFrame(feat2_valid),\n    pd.DataFrame(feat3_valid),\n    pd.DataFrame(feat4_valid),\n    pd.DataFrame(feat5_valid)\n])","396f3878":"X_train_train_custom, X_train_valid_custom","25f8a665":"logit = LogisticRegression(C=1, random_state=19)","e2b2d9cc":"%%time\nlogit.fit(X_train_train_custom, y_train_train)","be3beb3b":"%%time\npreds_valid = logit.predict(X_train_valid_custom)","84e985e9":"print('Accuracy score on the validation dataset: ',accuracy_score(y_train_valid, preds_valid))","de59c899":"import nltk.stem\nfrom nltk import word_tokenize          \nfrom nltk.stem import WordNetLemmatizer","db643968":"class LemmaTokenizer(object):\n    def __init__(self):\n        self.wnl = WordNetLemmatizer()\n    def __call__(self, articles):\n        return [self.wnl.lemmatize(t) for t in word_tokenize(articles)]\n\nlem_vect = CountVectorizer(tokenizer=LemmaTokenizer(),\n                           max_features=100000,\n                           ngram_range=(1,3),\n#                            stop_words='english'\n                          )","8339b56d":"%%time\nX_train_train_vect = lem_vect.fit_transform(X_train_train)\nX_train_valid_vect = lem_vect.transform(X_train_valid)","c6f2210f":"X_train_train_custom = hstack([\n    X_train_train_vect,\n    pd.DataFrame(feat1_train),\n    pd.DataFrame(feat2_train),\n    pd.DataFrame(feat3_train),\n    pd.DataFrame(feat4_train),\n    pd.DataFrame(feat5_train)\n])\nX_train_valid_custom = hstack([\n    X_train_valid_vect,\n    pd.DataFrame(feat1_valid),\n    pd.DataFrame(feat2_valid),\n    pd.DataFrame(feat3_valid),\n    pd.DataFrame(feat4_valid),\n    pd.DataFrame(feat5_valid)\n])","c317a465":"logit = LogisticRegression(C=1, random_state=19)","1d664e61":"%%time\nlogit.fit(X_train_train_custom, y_train_train)","75cc655e":"%%time\npreds_valid = logit.predict(X_train_valid_custom)","2b7f6ddb":"print('Accuracy score on the validation dataset: ',accuracy_score(y_train_valid, preds_valid))","bdf805d5":"%%time\nvect = CountVectorizer(ngram_range=(1,3), max_features=100000)\nX_train_train_vect = vect.fit_transform(X_train_train)\nX_train_valid_vect = vect.transform(X_train_valid)","e77ca6be":"X_train_train_custom = hstack([\n    X_train_train_vect,\n    pd.DataFrame(feat1_train),\n    pd.DataFrame(feat2_train),\n    pd.DataFrame(feat3_train),\n    pd.DataFrame(feat4_train),\n    pd.DataFrame(feat5_train)\n])\nX_train_valid_custom = hstack([\n    X_train_valid_vect,\n    pd.DataFrame(feat1_valid),\n    pd.DataFrame(feat2_valid),\n    pd.DataFrame(feat3_valid),\n    pd.DataFrame(feat4_valid),\n    pd.DataFrame(feat5_valid)\n])","7f77d0c9":"# Cs = np.logspace(-3,2,10)\n# params = {\n#     'C':Cs\n# }\n# logit = LogisticRegression(random_state=19)\n# grid_logit = GridSearchCV(logit, params, cv=5)","05d41d86":"# %%time\n# grid_logit.fit(X_train_train_custom, y_train_train)","e336be05":"# grid_logit.best_params_","cdc40f38":"# Cs = np.linspace(0.01,0.16,16)\n# params = {\n#     'C':Cs\n# }\n# logit = LogisticRegression(random_state=19)\n# grid_logit = GridSearchCV(logit, params, cv=5)","846e83cc":"# %%time\n# grid_logit.fit(X_train_train_custom, y_train_train)","8e4ecc45":"# grid_logit.best_params_","ed5a9856":"logit = LogisticRegression(C=0.07, random_state=19)\nlogit.fit(X_train_train_custom, y_train_train)\npreds_valid = logit.predict(X_train_valid_custom)\nprint('Accuracy score on the validation dataset: ',accuracy_score(y_train_valid, preds_valid))","ddb5d6aa":"bern = BernoulliNB()","e522111f":"%%time\nbern.fit(X_train_train_custom, y_train_train)","28682dc9":"preds_valid = bern.predict(X_train_valid_custom)\nprint('Accuracy score on the validation dataset: ',accuracy_score(y_train_valid, preds_valid))","18c713ef":"alp = np.linspace(0.1,1,10)\nparams = {\n    'alpha':alp\n}\nbern = BernoulliNB()\ngrid_bern = GridSearchCV(bern, params, cv=5)\ngrid_bern.fit(X_train_train_custom, y_train_train)","68910260":"grid_bern.best_params_","2c43e2a4":"bern = BernoulliNB(alpha=0.1)\nbern.fit(X_train_train_custom, y_train_train)\npreds_valid = bern.predict(X_train_valid_custom)\nprint('Accuracy score on the validation dataset: ',accuracy_score(y_train_valid, preds_valid))","4fc2af61":"%%time\nvect = CountVectorizer(ngram_range=(1,3), max_features=100000)\nX_train_vect = vect.fit_transform(X_train['review'].values)\nX_test_vect = vect.transform(X_test['review'].values)","794d14d6":"feat1_train = [fun1(review) for review in X_train['review'].values]\nfeat1_test = [fun1(review) for review in X_test['review'].values]\n\nfeat2_train = [fun2(review) for review in X_train['review'].values]\nfeat2_test = [fun2(review) for review in X_test['review'].values]\n\nfeat3_train = [fun3(review) for review in X_train['review'].values]\nfeat3_test = [fun3(review) for review in X_test['review'].values]\n\nfeat4_train = [fun4(review) for review in X_train['review'].values]\nfeat4_test = [fun4(review) for review in X_test['review'].values]\n\nfeat5_train = [fun5(review) for review in X_train['review'].values]\nfeat5_test = [fun5(review) for review in X_test['review'].values]","1a852f50":"X_train_custom = hstack([\n    X_train_vect,\n    pd.DataFrame(feat1_train),\n    pd.DataFrame(feat2_train),\n    pd.DataFrame(feat3_train),\n    pd.DataFrame(feat4_train),\n    pd.DataFrame(feat5_train)\n])\nX_test_custom = hstack([\n    X_test_vect,\n    pd.DataFrame(feat1_test),\n    pd.DataFrame(feat2_test),\n    pd.DataFrame(feat3_test),\n    pd.DataFrame(feat4_test),\n    pd.DataFrame(feat5_test)\n])","f444775d":"X_train_custom, X_test_custom","7372f6cd":"logit = LogisticRegression(C=0.07, random_state=19)","f76b96e6":"%%time\nlogit.fit(X_train_custom, X_train['label'].values)","9cf75266":"preds_test = logit.predict(X_test_custom)\nprint('Accuracy score on the validation dataset: ',accuracy_score(X_test['label'].values, preds_test))","d3643555":"#### Custom Features Analysis","2e2f4c42":"## Baseline model","841fb7ae":"The goal of binary logistic regression is to train a classifier that can make a binary decision about the class of a new input observation. Logistic regression solves this task by learning, from a training set, a vector of weights and a bias term. Each weight $w$ is a real number, and is associated with one of the input features $x$.","2ddbed6b":"2. EDA on NLP within QIQC competition: https:\/\/www.kaggle.com\/sudalairajkumar\/simple-exploration-notebook-qiqc","7d525447":"#### N-Grams","7d878c25":"One of the main parts of IMDB content is database of movie reviews. In our analysis we will use the dataset, which contains 50,000 movie reviews (both negative and positive).","6d5785a4":"Despite our hopes lemmatization didn't manage to increase model's performance, it even decreased accuracy.","48dd9c96":"To make a decision on a test instance\u2014 after we\u2019ve learned the weights in training\u2014 the classifier first multiplies each $x$ by its weight $w$, sums up the weighted features, and adds the bias term $b$. The resulting single number $z$ expresses the weighted sum of the evidence for the class, $z$ ranges from \u2212$\\infty$ to $\\infty$.","1f72729e":"Both types of reviews have the same number of words, which start with a capital letter.","3146357f":"$p(y|x) = \\hat{y}^y (1 - \\hat{y})^{1-y}$","6854ad97":"To train logistic regression we need to define a loss function. We will use a loss function that prefers the correct class labels of the training example to be more likely. This is called **conditional maximum likelihood estimation**: we choose the parameters $w$, $b$ that maximize the log probability of the true $y$ labels in the training data given the observations $x$. The resulting loss function is the negative log likelihood loss, generally called the **cross entropy loss.**","ef01c038":"#### GridSearch","2928135d":"Now we will try find the best parameters for our logistic regression model.","183ab4de":"In order to make kernel run faster I will comment all lines connected with GridSearch, leaving just resulting parameter.","bbc7f4f7":"Next we will try to understand, which Unigrams (in our baseline model we used only Unigrams), have the biggest impact on our model.","c0411448":"On the Test dataset our model managed to achived almost 90% accuracy. More important aspect is that accuracy on the Test dataset is even higher than on a Validation dataset. This means there is no overfitting, thus our model is expected to show approximately the same accuracy on new movie reviews.","603e53a2":"#### Adding custom features","7f05faef":"## Naive Bayes","a6d96f7d":"We slightly increased accuracy of our model. Let us check what N-Grams are the most valuable.","6422a5d9":"$Loss = -log p(y|x) = -[y log\\hat{y}+(1 - y)log(1 - \\hat{y})]$","d8ea7c5f":"Using CountVectorizer instead of TfidfVectorizer gave us small increase in accuracy.","20318633":"With the next step we will try to improve model's performance.","a4dea889":"P.S. Also there are other approaches like Word2Vec worth to try.","4bf76f6a":"Let us inspect, what are the most frequently occuring words in imdb reviews. For this purpose we will use \"Word Clouds\". This instrument can show us the most frequently occuring words in a friendly and fancy maner. ","d818cfc1":"#### N-Gram Countplots","02067019":"Cost function for the whole dataset with $m$ observations we'll define this way:","193bfc1e":"## Useful links","7c9dfc44":"$Cost(w,b) = -\\frac{1}{m}\\sum_{i=1}^{m}y^{(i)}log\u03c3(z) + (1-y^{(i)})log(1-\u03c3(z))$","f1c139ff":"## Exploratory Data Analysis ","54582559":"## Logistic Regression","bd32127f":"#### Word Clouds","70a722a0":"Sometimes good interpretation of model's result is vital for business. Here we will try to understand, which features have the biggest impact.","7d81ff54":"**IMDb (Internet Movie Database)** is an online database of information related to films, television programs, home videos and video games, and internet streams, including cast, production crew and personnel biographies, plot summaries, trivia, and fan reviews and ratings. An additional fan feature, message boards, was abandoned in February 2017. Originally a fan-operated website, the database is owned and operated by IMDb.com, Inc., a subsidiary of Amazon. *(c) Wikipedia*","ccc8fe22":"## Loading Data","e1b704d3":"Although \"World Cloud\" is a very impressive way to show some insights, still usually for better understanding we'd better use more common methods.","0eeb9dfa":"## Improving Baseline Model","8d61146b":"#### CountVectorizer","2e44fcfd":"As we decided before, we will use Logistic Regression to predict our classes. For a baseline model we will not tune any parameters, but use their default values. Data preprocessing step is going to be completed with a help of TfidfVectorizer. Data will be split to train and test sets in a ratio of 7 to 3, in other words we will train our model with 70% of all training data, and then we will validate it on 30% of our training dataset.","f556b114":"<img src=\"https:\/\/upload.wikimedia.org\/wikipedia\/commons\/thumb\/6\/69\/IMDB_Logo_2016.svg\/250px-IMDB_Logo_2016.svg.png\" width=\"250px\" align=\"left\"\/>","6764137d":"From the graph above we can see that positive and negative reviews have approximately the same length.","7b7af630":"1. NLP basics: http:\/\/web.stanford.edu\/~jurafsky\/slp3\/ed3book.pdf","b32294f6":"And we need to maximize this probability. Maximizing this probability is the same as maximizing $log$ of this probability, and also the same as minimizing $- log$ of the probability. We will skip some math steps and rewrite our loss function for a single observation this way:","b865f561":"$y = \u03c3(z) = \\frac{1}{1 + e^{-z}}$","a6e28760":"Lemmatization is a part of task of text normalization. Some words may differ, but they may have the same root. During lemmatization, we will alter different words with the same root to their original form.","a196b155":"Next step is minimizing cost function with gradient descent.","cf16a652":"The goal of this analysis is to create a strong analytical report in order to find valuable insights in the given dataset, and build a classification model, which will be able to classify movie review as positive or negative.","f16f520b":"**Positive** reviews contain such words as: \"Emotional\", \"Brilliant\", \"Amusing\", \"Amazing\", \"Best\", \"Passionate\" (which are usually used to express someone's satisfaction).","5a0fadb2":"Let\u2019s derive this loss function, applied to a single observation $x$. We\u2019d like to learn weights that maximize the probability of the correct label $p(y|x)$. Since there are only two discrete outcomes (1 or 0), we can express the probability $p(y|x)$ that our classifier produces for one observation as the following:","4fc60bbb":"As we can see \"Unigrams\" do not undercover any interesting insights comparing to Word Cloud. But if we have look at \"Bigrams\" plot we will see some phrases, which describe people's unequivocal attitude to a film.","6845ec61":"Also we see that positive and negative reviews contain approximately the same number of words.","ed039d0a":"We managed to increase accuracy on validatoin set, still we didn't manage to breake 90% accuracy border.","ab194707":"We will create some additional features and check their distribution among two classes.","2b22746b":"4. Diverse NLP techniques: https:\/\/www.kaggle.com\/abhishek\/approaching-almost-any-nlp-problem-on-kaggle","0c12ed57":"Countplot shows, that we deal with perfectly balanced dataset. This is a good news, because we can use in further analysis the easiest for understanding and enterpretability metric - accuracy. Accuracy gives us the percentage of correctly classified observations. Accuracy lies in the range from 0 to 1.","3329a599":"3. NLP Kernel within ods.ai course: https:\/\/www.kaggle.com\/kashnitsky\/a4-demo-sarcasm-detection-with-logit-solution","5b8cb5a3":"# IMDB Movie Review","3187982b":"#### Lemmatization","2406695c":"Unfortunately Naive Bayes method performed not well enough to beat Logistic Regression result.","660c423b":"To create a probability, we\u2019ll pass $z$ through the sigmoid function, $\u03c3(z)$. The sigmoid function has the following equation:","595dbccc":"## Loading Libraries","a02612e3":"As we can see both categories contain such popular words like \"Movie\" and \"Film\", and it is not a surprise, because we deal with **movie** reviews. Still there are some interesting moments.","aeb0b58b":"We will try one more classic NLP approach for text classification, which is called Naive Bayes.","ef2476d4":"**Negative** reviews contain such words as: \"Really\" (may be used in sarcastic way or just to amplify person's opinion), \"Dreadful\", \"Trashy\" (nothing good about a movie).","4af37ff2":"$z = (\\sum_{i=1}^{n} w_i x_i) + b$","060150f3":"Quite obvious results. \"eli5\" shows us which features (Unigrams in this case) inclines our model to label a review as Positive review. Top 10 list includes one of the most common adjectives (and 1 noun), which are used by people to express their emotions, when they are satisfied or unsatisfied.","1c375a78":"## Content\n* Loading Libraries\n* Loading Data\n* Exploratory Data Analysis\n    * Brief Exploration of the dataset\n    * Custom Features Analysis\n    * Word Cloud\n    * N-Gram countplots\n* Logistic Regression\n* Baseline Model\n* Improving Baseline Model","082f2290":"#### Brief Exploration of the dataset","c20be742":"## Prediction on a Test dataset","9b589350":"It looks like there is no significant difference in top10 most valuable features, still we see one bigram in this list and one new unigram.","607ca8f9":"Accuarcy score is **0.886**, in other words our model has managed to correctly label **88.6%** of reviews in the validation dataset. Pretty good for a baseline model.","41f3428f":"We see that our model managed to classify negative and positive reviews with approximately the same success. Still our model tends to classify Negative reviews as Positive more often than Positive as Negative."}}