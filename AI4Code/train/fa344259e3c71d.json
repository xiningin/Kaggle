{"cell_type":{"09d5fce6":"code","620388d6":"code","89c7108e":"code","08e78a2b":"code","0b545e96":"code","d4e77060":"code","1d8bd0c4":"code","b45d753f":"code","bb061fe7":"code","7c4441bc":"code","8abc400a":"code","136dd79f":"code","f0b3543c":"code","c521b908":"code","8b930447":"code","1590c6b5":"code","3d737d42":"code","40866395":"code","310d30c7":"code","82015cc0":"code","3ff5aa53":"code","10edb805":"code","acd2482a":"code","ecb7c5f0":"code","d7c8f4a1":"code","4b65affb":"code","3234c4e9":"code","fe1b5356":"code","0f2c4cbb":"code","1c6d4515":"code","9206728d":"code","21194491":"code","06e2dbd3":"code","1900c348":"code","5cc27f48":"code","6114e385":"code","b065c5a4":"code","f2179ac3":"code","9e9303a2":"code","5e27c5ef":"code","b62ad0fa":"code","94243efa":"code","94f88627":"code","b6dfc4cd":"code","1f4bfda7":"code","4091566b":"code","375f7fb5":"code","95c24b66":"code","190acb74":"code","be9237be":"code","0e810510":"code","c7231f9b":"code","94e90199":"code","ba843756":"code","ea47a731":"code","eb9b1dd5":"code","9426c275":"code","71105b19":"code","f82866ba":"code","2b331525":"code","24e10407":"code","c7d5c0c4":"code","5a15cea3":"code","5c1588fa":"code","9a801163":"code","f20309b8":"code","c03065d9":"code","cdb77d75":"code","99b412ab":"code","2010b753":"code","8f42dc6e":"code","eb638c99":"code","b4512c30":"code","94b66109":"code","3afe3922":"code","e1171faf":"code","94150b64":"code","183e3507":"code","545c2353":"code","f021abc7":"markdown","54f13967":"markdown","51376166":"markdown","88fd1b94":"markdown","5d97a443":"markdown","ec1c5a29":"markdown","94e0d7ac":"markdown","cbc8eb23":"markdown","c426b611":"markdown","c194fb3e":"markdown","a9edc494":"markdown","cead0b15":"markdown","9bb347d8":"markdown","2f7598f1":"markdown","bba8eac2":"markdown","4e08cc8a":"markdown","9b7be17d":"markdown","d4b65610":"markdown","b1216a3b":"markdown","9a075a25":"markdown","dd8ac01f":"markdown","93525577":"markdown","b8b9637d":"markdown","b2ecd4d7":"markdown","978caeba":"markdown","a71c5366":"markdown","346ddd65":"markdown","0ac98dc3":"markdown","0ab957b7":"markdown","2a36b3e1":"markdown","ad604327":"markdown","a33e9288":"markdown","f4a4f01d":"markdown","0b743757":"markdown","f22c22f2":"markdown","42b5d8a8":"markdown","ff534b84":"markdown","8599ba75":"markdown","b71af168":"markdown","fda17c55":"markdown","37d97202":"markdown","73958644":"markdown","ad76edeb":"markdown","d0329e9d":"markdown","0afcbabe":"markdown","05f1bdc1":"markdown","99f81b54":"markdown","78f5905b":"markdown","63bc38a8":"markdown","4565423a":"markdown","555eedcc":"markdown","23b2dabd":"markdown","bba5fa08":"markdown","ccf4c19b":"markdown","3765219b":"markdown","bb50dbb8":"markdown","5e8b7db1":"markdown","fb427b76":"markdown","00429624":"markdown","8cf0f175":"markdown","5983da33":"markdown","169002dd":"markdown","0e0c7aa3":"markdown","2f5e0e6b":"markdown","d9e83850":"markdown","d1c31ec4":"markdown","5440a3dd":"markdown","9b0bac7c":"markdown","269edfec":"markdown","24a4c8f8":"markdown","3d4cff85":"markdown"},"source":{"09d5fce6":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","620388d6":"# Importing necessary libraries\nimport numpy as np\nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport math \nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn import metrics\n\n%matplotlib inline\ndf_train_data_out = pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/train.csv')\ndf_test_data = pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/test.csv')\n\n# Reading Data from CSV to DF - Jupyter\n# df_train_data_out = pd.read_csv('data\/train.csv')\n# df_test_data = pd.read_csv('data\/test.csv')","89c7108e":"submission = pd.DataFrame(df_test_data['Id'], columns={'Id'})","08e78a2b":"# A function that prints the top columns with missing NaN count and percent\ndef print_nan_percentage(df_to_print):\n    # Getting the sum of missing values from DF Sorted in descending order\n    nan_count = df_to_print.isnull().sum().sort_values(ascending=False)\n    # Dividing the NaN sum with coulmn length to get a percentage \n    nan_percentage = nan_count \/ len(df_to_print)\n    # Returning the top 20 columns with missing NaNs\n    return pd.DataFrame(data=[nan_count, nan_percentage],index=['nan_count', 'nan_percentage']).T.head(20)","0b545e96":"print_nan_percentage(df_train_data_out)","d4e77060":"print_nan_percentage(df_test_data)","1d8bd0c4":"df_train_data_out.describe()","b45d753f":"df_test_data.describe()","bb061fe7":"# Set index in both DFs to be the Id column since the Id between the two DFs are connected \n# Train end at 1459 and test start from 1460\ndf_test_data.set_index('Id')\ndf_train_data_out.set_index('Id')","7c4441bc":"#Set the default subplots number of rows,number of columns and figure size,:\nfig, ax = plt.subplots(nrows = 2, ncols = 1, figsize = (10, 10),  sharex=True)\n\n# train data \nsns.heatmap(df_train_data_out.isnull(), yticklabels=False, ax = ax[0],cbar=False, cmap='YlGnBu')\n# Set the heatmap title\nax[0].set_title('Missing data in the train dataframe')\n\n# test data\nsns.heatmap(df_test_data.isnull(), yticklabels=False, ax = ax[1], cbar=False, cmap='YlGnBu')\n# Set the heatmap title\nax[1].set_title('Missing data in the test dataframe');\nplt.tight_layout()","8abc400a":"print('MSZoning: ', df_train_data_out['MSZoning'].unique())\nprint('\\nMSSubClass: ', df_train_data_out['MSSubClass'].unique())\nprint('\\nPoolQC: ', df_train_data_out['PoolQC'].unique())\nprint('\\nUtilities: ', df_train_data_out['Utilities'].unique())","136dd79f":"print('MSZoning: ', df_test_data['MSZoning'].unique())\nprint('\\nMSSubClass: ', df_test_data['MSSubClass'].unique())\nprint('\\nPoolQC: ', df_test_data['PoolQC'].unique())\nprint('\\nUtilities: ', df_test_data['Utilities'].unique())","f0b3543c":"#Correlation with output variable\ndf_corr = df_train_data_out.corr()\ncor_target = abs(df_corr[\"SalePrice\"])\n#Selecting highly correlated features\nrelevant_features = cor_target[cor_target>0.4]\nrelevant_features","c521b908":"# Plotting the best correlated features with SalePrice using pair plot\nsns.pairplot(df_train_data_out,height=2, x_vars=['GrLivArea',\n                                             'OverallQual',\n                                             'YearBuilt',\n                                             'YearRemodAdd',\n                                             'TotalBsmtSF',\n                                             '1stFlrSF',\n                                             'FullBath',\n                                             'TotRmsAbvGrd',\n                                             'GarageCars',\n                                             'GarageArea'], y_vars='SalePrice', markers=['+'])","8b930447":"#box plot overallqual\/saleprice\ndata = pd.concat([df_train_data_out['SalePrice'], df_train_data_out['OverallQual']], axis=1)\nf, ax = plt.subplots(figsize=(8, 6))\nfig = sns.boxplot(x='OverallQual', y='SalePrice', data=data)\nfig.axis(ymin=0, ymax=850000);","1590c6b5":"#box plot YearBuilt\/saleprice\ndata = pd.concat([df_train_data_out['SalePrice'], df_train_data_out['YearBuilt']], axis=1)\nfig, ax = plt.subplots(figsize=(20, 10))\nfig = sns.boxplot(x='YearBuilt', y='SalePrice', data=data, linewidth=1)\nplt.xticks(rotation=90);\nfig.axis(ymin=0, ymax=800000);","3d737d42":"# Dropping outliers from train DF for column GrLivArea\ndf_train_data = df_train_data_out.drop(df_train_data_out[(df_train_data_out['GrLivArea']>4000) & \n                                        (df_train_data_out['SalePrice']<300000)].index)\n\ndf_train_data","40866395":"gr=sns.boxplot(x=df_train_data['GrLivArea'])","310d30c7":"# Getting the SalePrice in a target column\ny = df_train_data['SalePrice']","82015cc0":"# Dropping SalePrice from train DF(\ndf_train_data.drop('SalePrice',axis=1 ,inplace= True)","3ff5aa53":"# Merging both train and test DF and preparing for data cleaning \ndata_merge = pd.merge(df_train_data ,df_test_data,how='outer',left_index=False, right_index=False)","10edb805":"# Checking that the merge done appropriately \ndata_merge.head()","acd2482a":"# Replace NaNs with 0 for garage year built\ndata_merge['GarageYrBlt'].fillna(0, inplace=True)\n\n# For garages with no built year, assume that it was built in the same year as the house \nfixed_garage = [j if i == 0 else i for i,j in zip(data_merge['GarageYrBlt'], data_merge['YearBuilt'])]\n    \n# Assigne new replaced zero values to the garage year built\ndata_merge['GarageYrBlt'] = fixed_garage","ecb7c5f0":"# Getting the columns that should be filled using the most common category in the column\ncol_mode_fill = ['MSZoning', 'Utilities', 'Exterior1st', 'MasVnrType', 'BsmtQual', 'KitchenQual', \n                 'Functional', 'Electrical']\n# Filling the category columns with the mode of the column\ndata_merge.update(data_merge[col_mode_fill].fillna(df_train_data[col_mode_fill].mode(), inplace=True))\ndata_merge[col_mode_fill].head(100)","d7c8f4a1":"# Fill NaN values with NA for object columns that were not filled with mode,\n# and fill floats with medain\ndata_merge = data_merge.apply(lambda x: x.fillna('NA') if x.dtype.kind in 'O' else x.fillna(x[:1457].median()) \n                               if x.dtype.kind in 'f' else x)\ndata_merge.head()","4b65affb":"# Display train info to check that data does not have any more NaN values\nprint_nan_percentage(data_merge)","3234c4e9":"# Creating a heatmap for Merged Data \nsns.heatmap(data_merge.isnull(), yticklabels=False,cbar=False, cmap='YlGnBu')\n# Set the heatmap title\nplt.title('Missing data in merged dataframe')\n","fe1b5356":"data_merge.set_index(\"Id\", inplace=True)","0f2c4cbb":"# Import skew to calculate the skew of the functions and BoxCox for transformation\nfrom scipy.stats import skew\nfrom scipy.special import boxcox1p\n\n# Get numerical columns from DF\nnumeric_feats = data_merge.dtypes[data_merge.dtypes != \"object\"].index\n\n# Check the skew of all numerical features\nskewed_cols = data_merge[numeric_feats].apply(lambda x: skew(x.dropna())).sort_values(ascending=False)\nskewness = pd.DataFrame({'Skew' :skewed_cols})\nskewness.head(10)","1c6d4515":"# Check how many columns are skewed based on previous scores \nskewness = skewness[abs(skewness) > 0.75]\nprint(\"There are \", skewness.shape[0], \" skewed numerical features to Box Cox transform\")","9206728d":"# Get the index of skewed features\nskewed_features = skewness.index\n\nlam = 0.15\nfor feat in skewed_features:\n    data_merge[feat] = boxcox1p(data_merge[feat], lam)","21194491":"# Convert categorical variable into dummy\/indicator variables.\ndt_me_dumy = pd.get_dummies(data_merge, drop_first=True)","06e2dbd3":"dt_me_dumy.info()","1900c348":"train_cleaned = dt_me_dumy[:1458]","5cc27f48":"train_cleaned.tail()","6114e385":"test_cleand = dt_me_dumy[1458:]","b065c5a4":"test_cleand.head()","f2179ac3":"# initialize the Scaler\nss = StandardScaler()\n\n# Fit train data using the scaler (scale the data)\ntrain_cleaned_s = ss.fit_transform(train_cleaned)\n\ntest_cleand_s = ss.transform(test_cleand)","9e9303a2":"# Display standardized data (train)\ntrain_cleaned_s","5e27c5ef":"# Display standardized data (test)\ntest_cleand_s","b62ad0fa":"from sklearn.model_selection import cross_val_score\nfrom sklearn.linear_model import Ridge, Lasso, ElasticNet, LinearRegression, RidgeCV, LassoCV, ElasticNetCV","94243efa":"sns.distplot(y, norm_hist=True, bins=146)","94f88627":"ylog = y.copy()\nylog = np.log(ylog)\nsns.distplot(ylog, norm_hist=True, bins=146)","b6dfc4cd":"# A function that takes a model name, its scores, the train score and prints them\ndef print_model_mean_and_train(model, scores, train_scores):\n    print(model, \" mean score is:\", scores.mean())\n    print('Train score for ', model,' is: ', train_scores)","1f4bfda7":"# a function that takes the predictions of the model, the model name and save ot to a CSV\n# The function will return the first 10 predictions\ndef save_csv(predictions, model_name):\n    # Reverse log operation on predictions using exp\n    submission['SalePrice'] = np.exp(predictions)\n    # Save CSV to path\n    submission.to_csv('submission_'+model_name+'.csv', index=False)\n    # Jupyter save to csv\n    #submission.to_csv('data\/submission_'+model_name+'.csv', index=False)\n    return submission.head(10)","4091566b":"# A function that takes the model, X & y train, cv number, and performs cross validation\n# The function will return the model and its scores\ndef performe_CV(used_model, X, y_data, cv):\n    used_model.fit(X, y_data)\n    scores = cross_val_score(used_model, X, y_data, cv=cv)\n    return used_model, scores","375f7fb5":"from sklearn.feature_selection import SelectFromModel","95c24b66":"# Build LassoCV regression model \n\n# Fit on standarized data and performe 8-fold CV\nlassoCV, lassoCV_scores = performe_CV(LassoCV(), train_cleaned_s, ylog, 8)\n\n# Print Model Mean Score\nprint_model_mean_and_train('LassoCV', lassoCV_scores, lassoCV.score(train_cleaned_s, ylog))\n\n# Get best alpha\nbestAlphaLasso = lassoCV.alpha_\nprint(\"Best Alpha for LassoCV: \", bestAlphaLasso)","190acb74":"# Build Lasoo regression model using best alpha\nlasso = Lasso(alpha=bestAlphaLasso, copy_X=True,\n                             fit_intercept=True, max_iter=1000, normalize=False,\n                             positive=False, precompute=False,\n                             random_state=None, selection='cyclic', tol=0.0001,\n                             warm_start=False)\n   \n# Fit on standarized data and performe 8-fold CV\nlasso, lasso_scores = performe_CV(lasso, train_cleaned_s, ylog, 8)\n# Print Model Mean Score\nprint_model_mean_and_train('Lasso', lasso_scores, lasso.score(train_cleaned_s, ylog))","be9237be":"# Give the model data that it have not seen yet and get the predictions\npred_lasso = lasso.predict(test_cleand_s)\npred_lasso","0e810510":"save_csv(pred_lasso, 'Lasso')","c7231f9b":"sfm = SelectFromModel(lasso, threshold=0.1)\nsfm.fit(train_cleaned_s, ylog)\nfeatures = sfm.transform(train_cleaned_s)\nfeature_idx = sfm.get_support()\nfeature_name = train_cleaned.columns\n#feature_name = feature_name.drop('Id')\nfeature_name","94e90199":"# Get a copy of the best features and standardize it \nbest_train = train_cleaned\n\n# Dropping multicollinearity columns\nbest_train.drop('1stFlrSF', axis=1, inplace=True)\nbest_train.drop('TotRmsAbvGrd', axis=1, inplace=True)                                             \nbest_train.drop('GarageArea', axis=1, inplace=True)                                             \nbest_train","ba843756":"# Get a copy of the best features and standardize it \nbest_test = test_cleand\n\n# Dropping multicollinearity columns\nbest_test.drop('1stFlrSF', axis=1, inplace=True)\nbest_test.drop('TotRmsAbvGrd', axis=1, inplace=True)                                             \nbest_test.drop('GarageArea', axis=1, inplace=True)   ","ea47a731":"# Fit train data using the scaler (scale the data)\nbest_train_s = ss.fit_transform(best_train)\nbest_test_s = ss.transform(best_test)","eb9b1dd5":"from sklearn.model_selection import cross_val_score","9426c275":"# Create model instance\nlm = LinearRegression()\n     \n# Perform 10-fold cross validation and fit on model\nlm, lm_scores = performe_CV(lm,  best_train, ylog, 10)\nprint_model_mean_and_train('Linear Regression', lm_scores, lm.score(best_train, ylog))\n","71105b19":"# Give the model data that it have not seen yet and get the predictions\npred_lm = lm.predict(best_test)\npred_lm","f82866ba":"save_csv(pred_lm, 'LinearReg')","2b331525":"# np.logspace gives us points between specified orders of magnitude on a logarithmic scale. It is base 10.\nr_alphas = np.logspace(0, 5, 500)","24e10407":"# Build RidgeCV regression model \n\n# Perform 8-fold cross validation and fit on model\nridgeCV, ridgeCV_scores = performe_CV(RidgeCV(alphas=r_alphas), best_train, ylog, 8)\nprint_model_mean_and_train('RidgeCV', ridgeCV_scores, ridgeCV.score(best_train, ylog))        \n\n# Get best alpha \nbestAlpha = ridgeCV.alpha_\nprint(\"Best Alpha for RidgeCV: \", bestAlpha)","c7d5c0c4":"# Build Ridge regression model using best alpha\nridge = Ridge(alpha=bestAlpha, copy_X=True, fit_intercept=True, max_iter=None,\n      normalize=False, random_state=1, solver='auto', tol=0.001)\n\n# Perform 8-fold cross validation and fit on model\nridge_model, ridge_scores = performe_CV(ridge, best_train, ylog, 8)\nprint_model_mean_and_train('Ridge', ridge_scores, ridge_model.score(best_train, ylog))","5a15cea3":"# Give the model data that it have not seen yet and get the predictions\npred_ridge = ridge_model.predict(best_test)\npred_ridge","5c1588fa":"save_csv(pred_ridge, 'Ridge')","9a801163":"# Creating multiple distplots to compare the predictions with the target data\nplt.figure(figsize=(20,10))\nsns.distplot(pred_ridge, label='Ridge', hist=False)\nsns.distplot(pred_lasso, label='Lasso', hist=False)\nsns.distplot(pred_lm, label='LinearReg', hist=False)\nsns.distplot(ylog, label='Log Y', hist=False)\nplt.legend(fontsize='22')","f20309b8":"# Creating l1_ratio points to try and get optimal ratio \nl1_ratios = np.linspace(0.01, 1.0, 5)\n\n# Build ElasticNetCV regression model\nelasticCV = ElasticNetCV(l1_ratio=[.1, .5, .7,\n    .9, .95, .99, 1], n_alphas=600)\n\n# Perform 8-fold cross validation and fit on model\nelasticCV, elasticCV_scores = performe_CV(elasticCV, best_train_s, ylog, 8)\nprint_model_mean_and_train('ElasticNetCV', elasticCV_scores, elasticCV.score(best_train_s, ylog))\n\n\n# Getting best alpha and optimal ratio\nbestAlphaElastic = elasticCV.alpha_\noptimall1 = elasticCV.l1_ratio_\n\nprint(\"Best Alpha for ElasticNetCV: \", bestAlphaElastic)\nprint(\"Optimal l1_ratio for ElasticNetCV: \", optimall1)","c03065d9":"# Build ElasticNet regression model\nelasticNet = ElasticNet(alpha=bestAlphaElastic, l1_ratio=optimall1, random_state=1)\n\n# Perform 8-fold cross validation and fit on model\nelastic, elastic_scores = performe_CV(elasticNet, best_train_s, ylog, 8)\nprint_model_mean_and_train('ElasticNet', elastic_scores, elastic.score(best_train_s, ylog))","cdb77d75":"# Give the model data that it have not seen yet and get the predictions\npred_elastic = elastic.predict(best_test_s)\npred_elastic","99b412ab":"save_csv(pred_elastic, 'elastic')","2010b753":"from sklearn.model_selection import GridSearchCV","8f42dc6e":"# Build a GS regression model \n\n# Creating a list of alphas for Ridge\nalphas = np.logspace(-4, -0.5, 30)\n\n# Setting the parameters for Ridge Grid Search\ntuned_parameters = {'alpha': alphas,\n                    'fit_intercept': [True,False], \n                    'normalize' :[False, True]}\n\n# Performing GS CV \ngs = GridSearchCV(ridge, param_grid=tuned_parameters, cv=5)\n\n# Fitting the model \ngs.fit(best_train_s, ylog)\n\n# Printing the score \nprint(\"Grid Search Train Score is: \",gs.score(best_train_s, ylog))","eb638c99":"# Give the model data that it have not seen yet and get the predictions\ngs_pred = gs.predict(best_test_s)\ngs_pred","b4512c30":"# Getting the best estimator from the model \nbest = gs.best_estimator_\nbest","94b66109":"# Getting the best Parameters from the model \ngs.best_params_","3afe3922":"save_csv(gs_pred, 'grid')","e1171faf":"from sklearn.ensemble import RandomForestRegressor","94150b64":"# Building RF model\nrf = RandomForestRegressor()\n\n# Perform 7-fold cross validation and fit on model\nrf, rf_score = performe_CV(rf, best_train_s, ylog, 7)\nprint_model_mean_and_train('RF', rf_score, rf.score(best_train_s, ylog))","183e3507":"# Give the model data that it have not seen yet and get the predictions\npred_rf = rf.predict(best_test_s)\npred_rf","545c2353":"save_csv(pred_rf, 'RF')","f021abc7":"We will start by looking at the distribution of the y values (target values)","54f13967":"# Creating a submission template","51376166":"# Y-target analysis","88fd1b94":"We will start by performing Lasso regression and after that we will select the best features based on what Lasso decides","5d97a443":"- We will split the DF based on ID for Train ","ec1c5a29":"- Log transformation will help shape our target to become normally distributed and fit our data better","94e0d7ac":"- We noticed that the data contains a lot of missing NaN values.\n- We will have to take a closer look at some of the columns and perform some data cleaning.","cbc8eb23":"- We can see that the train DF starts from ID 1 and ends at ID 1460.\n- While the test DF start from 1461 and end at 2919.","c426b611":"# We are almost there, now we will fill our NaN values accordingly ","c194fb3e":"We will performe Ridge and RidgeCV for comparison","a9edc494":"We can see that the distribution is right skewed, so let's try and fix it by performing log on the values of y","cead0b15":"# We noticed a connection between the ID columns from the train and test DFs","9bb347d8":"- Save predictions to CSV file","2f7598f1":"- As we clearly see from the heatmap there are many missing values of features in both train dataframe and test dataframe.\n- We can see that that the train & test DFs share a lot o missing values in the PoolQC, Alley, MSZoning and Fireplaces columns\n- While the rest of the columns with missing values sugest that there might be a difference in the categories in which the columns are filled with","bba8eac2":"- Finally we are doing one last test on Random Forest\n- The scores for this were the worst between them all","4e08cc8a":"# Creating Functions to help with modeling and reducing the number of lines being added to the file","9b7be17d":"- Print predictions based on the model with the highest mean it this case it was Lasso","d4b65610":"- Train DataFrame","b1216a3b":"# Ridge vs. RidgeCV","9a075a25":"# Grid Search","dd8ac01f":"- We can see the the target variable is not normally distributed","93525577":"# HeatMap of missing null values ","b8b9637d":"- Check our data after filling and make sure there are now missing data","b2ecd4d7":"# Skewness of features","978caeba":"- The Overall quality for the house positively correlates with the house price, as the quality increase the price of the house increases","a71c5366":"- Using the same heatmap above for NaNs, we can see that all NaN values are now gone","346ddd65":"- By looking at the GRLiveArea, we can see that noticable outliers are now gone","0ac98dc3":"- Now we want to get the best features from Lasso or LassoCV","0ab957b7":"# Random Forest","2a36b3e1":"- Seting index to be ID so it does not get changed in the process of Box Cox","ad604327":"# Linear Regression ","a33e9288":"- Ridge","f4a4f01d":"- Print predictions based on the model with the highest mean","0b743757":"- We will split the DF based on ID for Test ","f22c22f2":"# Before we start filling columns, we would like to look at the data for outliers","42b5d8a8":"#### Lets see the results","ff534b84":"- This step is done to convert categorical columns to numerical ones, which will be needed in performing training on our models and preducing predictions for the test data in the last step  \n- We will do this step with the merged data becasue some categories in the test DF are not in the train DF and vice versa\n- As a result, we might have a mismatch in shapes if this step was done seperatly for the two DFs\\","8599ba75":"- In this step we will standardize the data for use in Lasso and Ridge regression","b71af168":"- RidgeCV","fda17c55":"- After looking at the data descriptions, we noticed the following:\n    - Some columns in the Train DF contain Certain categories, while the Test DF contain a different set of categories \n    - We also noticed that NaN values in some columns represent NA category, while in other it was simply missing data\n    - As a result, these columns will have to be filled appropriately based on what is provided in the data description","37d97202":"# Now we will split our train data to X & y (target)","73958644":"# ElasticNet","ad76edeb":"# In this step we will convert our data to dummy variables","d0329e9d":"- From the pairplots we noticed that there are outlier points for the features (GrLivArea, TotalBsmtSF and 1stFlrSF)","0afcbabe":"# Now we will merger the train & test DFs","05f1bdc1":"# Relation exploration for some features","99f81b54":"### First we want to inspect the data and see  what is the Percentage of None values of ever column\n#### below is a function that takes a dataframe and rutern the Percentage  ","78f5905b":"# Let's investigate some of the categorical columns","63bc38a8":"- Save predictions to CSV file","4565423a":"# Conclusion","555eedcc":"- Test DataFrmae","23b2dabd":"- Lasso","bba5fa08":"- We will plot using SNS pairplot to check for any noticable outliers","ccf4c19b":"# HeatMap After Cleaning Data","3765219b":"# Modeling Section","bb50dbb8":"### so we try to see the price and the year to check the outlier and how the prise go with the year ","5e8b7db1":"- LassoCV","fb427b76":"# Lasso vs. LassoCV","00429624":"We will performe Lasso and LassoCV for comparison","8cf0f175":"- Save predictions to CSV file","5983da33":"# Let's start by taking taking a quick look at the statistics of each column","169002dd":"# Finally, the last step before modeling and getting predictions","0e0c7aa3":"- This model did not perform as good as the rest of the models and we are leaving it here as a point of reference","2f5e0e6b":"# DSI7 - Group2\n- Team Members: Rawan AlMalki, Rayan Raad, Ghalib Tawfiq","d9e83850":"- Use distplot to compare all the outputs of the predictions shown by the models to find the best prediction comparing with ylog.  \n- We notice that all three models approximately produce similiar predictions","d1c31ec4":"- Our model took into consideration the data, the statistics and the regression models.\n- We made sure when cleaning the data that they are categorized poperly and that missing values where filled acoordingly. \n- After we took a look from a statistical point of view where we identified outliers, looked at column skewness and performed necessary data transformation to get a better fit.\n- Next we took the data and transofrmed it to dummy data to make sure we have all the categorical columns into consideration.\n- Then we split the data again before standardizing and fit\/transformed our train data while only transforming the test data.\n- Before we start performing regression, we looked in the target variable and noticed that the data were also not normally distributed, we decided to perform log transformation that helps with transforming data from skewed to look more like a normal distribution.\n- We attempted to select best features using the selectFromModel function but we kept getting all the columns.\n- After that we performed multiple regression models on the data and figured that Ridge\/RidgeCV were the best models when producing results.\n- Finally, our Kaggle score was 0.11872","5440a3dd":"- Save predictions to CSV file","9b0bac7c":"# From now on we will only use the best features that lasso or lassoCV decided on ","269edfec":"- Categorical columns with actual missing values will be filled with the mode of the column\n- Numerical columns with numbers that are not; for example, years, will ne filled with the median of the column\n- Lastly, The reset of the columns with NaN and are legitimate NA categories will be filled with NA","24a4c8f8":"# Let's drop the outliers for GRLivArea based on SalePrice","3d4cff85":"- We will start by looking at the columns with the highest correlation with our Traget, in this case the SalePrice"}}