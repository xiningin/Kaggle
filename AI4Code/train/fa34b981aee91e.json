{"cell_type":{"d3d4b87b":"code","838eed40":"code","fec1e7e7":"code","99cf320b":"code","ef0cc916":"code","2ab3f0ff":"code","66e02134":"code","dc55ef02":"code","2237649f":"code","de563595":"code","3290721c":"code","ae1beb21":"code","1f689187":"code","60e92177":"code","f8b7cbec":"code","a5f9cb5a":"code","6b5a972d":"code","49b700ff":"code","13c43067":"code","893cb215":"code","ac2d8c9a":"code","8fbcf3aa":"code","f43d7234":"code","c1841bf5":"code","aaf567a0":"code","05d57cf2":"code","05f01196":"code","9cae699f":"code","ebaa63f5":"code","b47448a4":"code","d9898e2c":"markdown","723ead97":"markdown","74431bff":"markdown","bb9d9517":"markdown","490d6e5f":"markdown","bfd84572":"markdown","49fa4256":"markdown","47a9a08e":"markdown","89dafc76":"markdown","383cf687":"markdown","5929243e":"markdown","976e7156":"markdown","0129aaeb":"markdown","be99e600":"markdown","cc50d1c9":"markdown","ad228317":"markdown","3ffad45a":"markdown","bc943ad1":"markdown","368afaff":"markdown","64123e57":"markdown"},"source":{"d3d4b87b":"''' Global Configuration Settings '''\nclass CFG:\n    \n    def __init__(self):\n        self.labels = 4\n        self.sshape = (100,100,3) # Increasable to 224\n        self.n_epochs = 50\n        self.seed = 221\n\ncfg = CFG()","838eed40":"import os\nimport numpy as np\nimport random as rn\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.axes_grid1 import ImageGrid\nfrom plotly.subplots import make_subplots\nimport plotly.graph_objs as go\nfrom PIL import Image\n%matplotlib inline\nimport time\n\nfrom keras import models\nimport tensorflow as tf\nfrom tensorflow.keras import applications as app\nfrom tensorflow.keras.models import Sequential # Sequential model initialization\nfrom tensorflow.keras.layers import Dense,BatchNormalization,Dropout \nfrom tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten,AveragePooling2D\nfrom tensorflow.keras.applications import EfficientNetB4\nfrom tensorflow.keras.applications.inception_v3 import InceptionV3\nfrom tensorflow.keras.callbacks import EarlyStopping,ReduceLROnPlateau,ModelCheckpoint\nfrom tqdm.keras import TqdmCallback\nfrom tensorflow.keras.callbacks import ReduceLROnPlateau, ModelCheckpoint\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator # For data pipeline building\nfrom functools import partial\nimport keras\nfrom keras import backend as K\n\n# various library seeds\nos.environ['PYTHONHASHSEED'] = '0'                      \nnp.random.seed(cfg.seed)\nrn.seed(cfg.seed)\ntf.random.set_seed(cfg.seed)\n\n# Palettes\nlst_color = ['#B1D784','#2E8486','#004379','#032B52','#EAEA8A']\n\n''' Folder Pathways'''\nmain_folder = '\/kaggle\/input\/hummingbirds-at-my-feeders\/'\ntrain_folder = '\/kaggle\/input\/hummingbirds-at-my-feeders\/hummingbirds\/train\/'\nval_folder = '\/kaggle\/input\/hummingbirds-at-my-feeders\/hummingbirds\/valid\/'\ntest_folder = '\/kaggle\/input\/hummingbirds-at-my-feeders\/hummingbirds\/test\/'\nvideo_folder = '\/kaggle\/input\/hummingbirds-at-my-feeders\/video_test\/'","fec1e7e7":"os.listdir(main_folder)","99cf320b":"os.listdir(train_folder)","ef0cc916":"''' Visualise Image Data '''\ndef show_grid(image_list,nrows,ncols,label_list=None,\n              show_labels=False,savename=None,\n              figsize=(20,10),showaxis='off'):\n    \n    if type(image_list) is not list:\n        if(image_list.shape[-1]==1):\n            image_list = [image_list[i,:,:,0] for i in range(image_list.shape[0])]\n        elif(image_list.shape[-1]==3):\n            image_list = [image_list[i,:,:,:] for i in range(image_list.shape[0])]\n    fig = plt.figure(None, figsize,frameon=False)\n    grid = ImageGrid(fig, 111,  # similar to subplot(111)\n                     nrows_ncols=(nrows, ncols),  # creates 2x2 grid of axes\n                     axes_pad=0.3,  # pad between axes in inch.\n                     share_all=True)\n    \n    for i in range(nrows*ncols):\n        ax = grid[i]\n        img = Image.open(image_list[i])\n        ax.imshow(img,cmap='Greys_r')  # The AxesGrid object work as a list of axes.\n        ax.axis(showaxis)\n        if show_labels:\n            ax.set_title(class_mapping[y_int[i]])\n    if savename != None:\n        plt.savefig(savename,bbox_inches='tight')","2ab3f0ff":"lst_temp = os.listdir(train_folder+'\/Rufous_female')\nlst_rufous_female = []\nfor i in lst_temp:\n    lst_rufous_female.append(train_folder+'\/Rufous_female\/'+i)\n    \nlst_temp = os.listdir(train_folder+'\/Broadtailed_female')\nlst_broad_female = []\nfor i in lst_temp:\n    lst_broad_female.append(train_folder+'\/Broadtailed_female\/'+i)\n    \nlst_temp = os.listdir(train_folder+'\/Broadtailed_male')\nlst_broad_male = []\nfor i in lst_temp:\n    lst_broad_male.append(train_folder+'\/Broadtailed_male\/'+i)\n    \nlst_temp = os.listdir(train_folder+'\/No_bird')\nlst_none = []\nfor i in lst_temp:\n    lst_none.append(train_folder+'\/No_bird\/'+i)","66e02134":"show_grid(lst_rufous_female,5,10,figsize=(20,10))","dc55ef02":"show_grid(lst_broad_female,5,10,figsize=(20,10))","2237649f":"show_grid(lst_broad_male,5,10,figsize=(20,10))","de563595":"show_grid(lst_none,5,10,figsize=(20,10))","3290721c":"class_types = len(os.listdir(train_folder))\nprint('Number of classes for Classification: ',class_types)\nclass_names = os.listdir(train_folder)\nprint(f'The class names are {class_names}\\n')\n\nprint('Training dataset:')\nfor i in class_names:\n    print(i + ':' + str(len(os.listdir(train_folder+i))))\n\nprint('\\nValidation dataset:')\nfor i in class_names:\n    print(i + ':' + str(len(os.listdir(val_folder+i))))\n    \nprint('\\nTest dataset:')\nfor i in class_names:\n    print(i + ':' + str(len(os.listdir(test_folder+i))))","ae1beb21":"from tensorflow.keras.preprocessing.image import ImageDataGenerator \n\n# Define DataGenerators\ntrain_datagen = ImageDataGenerator(rescale=1.0\/255)\ngen_datagen = ImageDataGenerator(rescale=1.0\/255)","1f689187":"# DataGenerators via Folder Directory\ngen_train = train_datagen.flow_from_directory(train_folder, \n                        target_size=(cfg.sshape[0],cfg.sshape[1]),  # target size\n                        batch_size=32,                              # batch size\n                        class_mode='categorical')    \n\ngen_valid = gen_datagen.flow_from_directory(val_folder,\n                        target_size=(cfg.sshape[0],cfg.sshape[1]),  # target size\n                        batch_size=32,                              # batch size\n                        class_mode='categorical')\n\ngen_test = gen_datagen.flow_from_directory(test_folder,\n                        target_size=(cfg.sshape[0],cfg.sshape[1]),  # target size\n                        batch_size=32,                              # batch size\n                        class_mode='categorical')","60e92177":"# Evaluation Metrics for Callback\ndef get_recall(y_true, y_pred):\n    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n    recall = true_positives \/ (possible_positives + K.epsilon())\n    return recall\n\ndef get_precision(y_true, y_pred):\n    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n    precision = true_positives \/ (predicted_positives + K.epsilon())\n    return precision\n\ndef get_f1(y_true, y_pred):\n    precision = get_precision(y_true, y_pred)\n    recall = get_recall(y_true, y_pred)\n    return 2*((precision*recall)\/(precision+recall+K.epsilon()))","f8b7cbec":"# Two Convolution Layer CNN\nmodel = keras.models.Sequential([\n    keras.layers.Conv2D(32, kernel_size=3,\n                            padding=\"same\", \n                            activation=\"relu\", \n                            input_shape=cfg.sshape),    \n    keras.layers.Conv2D(64, kernel_size=3, \n                            padding=\"same\", \n                            activation=\"relu\"),\n    keras.layers.MaxPool2D(),\n    keras.layers.Flatten(),\n    keras.layers.Dropout(0.25),\n    keras.layers.Dense(128, activation=\"relu\"),\n    keras.layers.Dropout(0.5),\n    keras.layers.Dense(cfg.labels, activation=\"softmax\")\n])\n\n# Show the Model Architecture\nmodel.summary()\n\n''' Model Compilation '''\nmodel.compile(optimizer='Adam', \n              loss='categorical_crossentropy',\n              metrics=['acc',get_f1,get_precision,get_recall])","a5f9cb5a":"''' Callback Options During Training '''\ncallbacks = [ReduceLROnPlateau(monitor='val_accuracy',patience=2,verbose=0, \n                               factor=0.5,mode='max',min_lr=0.001),\n             ModelCheckpoint(filepath=f'model_cnn.h5',monitor='val_accuracy',\n                             mode = 'max',verbose=0,save_best_only=True),\n             TqdmCallback(verbose=0)] \n\n''' Start Training '''\nstart = time.time()\nhistory = model.fit(gen_train,\n                    validation_data = gen_valid,\n                    callbacks=callbacks,\n                    verbose=0,\n                    epochs=cfg.n_epochs  # Training for n_epoch interations\n                   )\nend = time.time()\nprint(f'The time taken to execute is {round(end-start,2)} seconds.')\nprint(f'Maximum Train\/Val {max(history.history[\"acc\"]):.4f}\/{max(history.history[\"val_acc\"]):.4f}')","6b5a972d":"''' FUNCTIONS FOR PLOTTING KERAS HISTORY RESULTS '''\n\n# Function to plot all metrics side by side (defined above)\ndef plot_keras_metric(history):\n\n    # Palettes\n    lst_color = ['#B1D784','#2E8486','#004379','#032B52','#EAEA8A']\n    metric_id = ['loss','get_f1','acc','get_precision','get_recall']\n    fig = make_subplots(rows=1, cols=len(metric_id),subplot_titles=metric_id)\n\n    jj=0;\n    for metric in metric_id:     \n\n        jj+=1\n\n        # Main Trace\n        fig.add_trace(go.Scatter(x=[i for i in range(1,cfg.n_epochs+1)],\n                                 y=history.history[metric],\n                                 name=f'train_{metric}',\n                                 line=dict(color=lst_color[0]),mode='lines'),\n                      row=1,col=jj)\n        fig.add_trace(go.Scatter(x=[i for i in range(1,cfg.n_epochs+1)],\n                                 y=history.history['val_'+metric],\n                                 name=f'valid_{metric}',\n                                 line=dict(color=lst_color[3]),mode='lines'),\n                      row=1,col=jj)\n\n        # difference between training\/validation metrics\n        if(metric is not 'loss'):\n            diff = abs(np.array(history.history[metric]) - np.array(history.history['val_'+metric]))\n            fig.add_trace(go.Bar(x=[i for i in range(1,cfg.n_epochs+1)],y=diff,name='metric diff.',\n                                 text=diff.round(3),marker_color=lst_color[3],opacity=0.25,showlegend=False)\n                          ,row=1,col=jj)\n\n    fig.update_layout(yaxis=dict(range=[0,1]),yaxis_range=[0,1],\n                      margin=dict(t=60,b=10),\n                      height=300,showlegend=False,template='plotly_white',\n                      hovermode=\"x\",title=f'<b>CNN TRAINING<\/b>')\n    fig['layout']['yaxis'].update(title='', range=[0,5], autorange=True,type='log')\n    fig['layout']['yaxis2'].update(title='', range=[0, 1.1], autorange=False)\n    fig['layout']['yaxis3'].update(title='', range=[0, 1.1], autorange=False)\n    fig['layout']['yaxis4'].update(title='', range=[0, 1.1], autorange=False)\n    fig['layout']['yaxis5'].update(title='', range=[0,1.1],autorange=False)\n    fig.show()","49b700ff":"plot_keras_metric(history)","13c43067":"def plot_keras_metric2(lst_history,names):\n\n    # Palettes\n    lst_color = ['#B1D784','#2E8486','#004379','#032B52','#EAEA8A']\n    metric_id = ['get_f1','acc','get_precision','get_recall']\n\n    ii=-1\n    for i in range(len(lst_history)):\n\n        ii+=1; history = lst_history[ii]\n        fig = make_subplots(rows=1, cols=4,subplot_titles=metric_id); jj=0\n        for metric in metric_id:     \n\n            jj+=1\n\n            # Main Trace\n            fig.add_trace(go.Scatter(x=[i for i in range(1,cfg.n_epochs+1)],y=history.history[metric],\n                                     name=f'train_{metric}_{ii}',line=dict(color=lst_color[0]),mode='lines'),\n                          row=1,col=jj)\n            fig.add_trace(go.Scatter(x=[i for i in range(1,cfg.n_epochs+1)],y=history.history['val_'+metric],\n                                     name=f'valid_{metric}_{ii}',line=dict(color=lst_color[3]),mode='lines'),\n                          row=1,col=jj)\n\n            # difference between training\/validation metrics\n            diff = abs(np.array(history.history[metric]) - np.array(history.history['val_'+metric]))\n            fig.add_trace(go.Bar(x=[i for i in range(1,cfg.n_epochs+1)],y=diff,name='metric diff.',\n                                 text=diff.round(3),marker_color=lst_color[2],opacity=0.25,showlegend=False)\n                          ,row=1,col=jj)\n\n            # Rest are for reference only\n            for j in range(len(names)):\n                if(j != ii):\n                    lhistory = lst_history[j]\n                    fig.add_trace(go.Scatter(x=[i for i in range(1,cfg.n_epochs+1)],y=lhistory.history[metric],\n                                             name=f'train_{metric}_{j}',opacity=0.3,showlegend=False,\n                                             line=dict(color='#D3D3D3'),mode='lines'),row=1, col=jj)\n                    fig.add_trace(go.Scatter(x=[i for i in range(1,cfg.n_epochs+1)],y=lhistory.history['val_'+metric],\n                                             name=f'val_{metric}_{j}',opacity=0.3,showlegend=False,\n                                             line=dict(color='#D3D3D3'),mode='lines'),row=1, col=jj)\n\n        if(len(names)>1):\n            title=f\"{' | '.join(str(x) for x in names[ii])}\"\n            if(type(names[0]) is not list):\n                title= names[ii]\n        else:\n            title= f\"{''.join(str(x) for x in names[ii])}\"                    \n                    \n        fig.update_yaxes(tickvals=[i for i in np.arange(0.0,1.0,0.1)])\n        fig.update_layout(yaxis=dict(range=[0,1]),yaxis_range=[0,1],\n                          template='plotly_white',font=dict(family='sans-serif',size=14),\n                          title=title,height=300,showlegend=False,\n                          margin=dict(t=60,b=10)\n                         )\n            \n        fig['layout']['yaxis1'].update(title='', range=[0,1], autorange=False, tick0=0, dtick=5)\n        fig['layout']['yaxis2'].update(title='', range=[0,1], autorange=False, tick0=0, dtick=5)\n        fig['layout']['yaxis3'].update(title='', range=[0,1], autorange=False, tick0=0, dtick=5)\n        fig['layout']['yaxis4'].update(title='', range=[0,1], autorange=False, tick0=0, dtick=5)\n\n        fig.add_hline(y=1.0,line_width=5)\n        fig.show()","893cb215":"''' Evaluate CNN model w\/ imported list of augmentation options '''\n# augment_model inputs nested lists of augmentation options & evaluates \n\ndef augment_model(lst_aug):\n\n    # Define DataGenerator, load image data from directory\n    gen_train = lst_aug[0].flow_from_directory(train_folder, \n                            target_size=(cfg.sshape[0],cfg.sshape[1]),  # target size\n                            batch_size=32,          # batch size\n                            class_mode='categorical')    # batch size\n\n    gen_valid = lst_aug[1].flow_from_directory(val_folder,\n                            target_size=(cfg.sshape[0],cfg.sshape[1]),\n                            batch_size=32,\n                            class_mode='categorical')\n\n    gen_test = lst_aug[1].flow_from_directory(test_folder,\n                            target_size=(cfg.sshape[0],cfg.sshape[1]),\n                            batch_size=32,\n                            class_mode='categorical')\n\n    # Define a CNN Model\n    model = keras.models.Sequential([\n        \n        keras.layers.Conv2D(32, kernel_size=3, \n                                padding=\"same\", \n                                activation=\"relu\", \n                                input_shape=cfg.sshape),    \n        keras.layers.Conv2D(64, \n                            kernel_size=3, \n                            padding=\"same\", \n                            activation=\"relu\"),\n        \n        keras.layers.MaxPool2D(),\n        keras.layers.Flatten(),\n        keras.layers.Dropout(0.25),\n        keras.layers.Dense(128, activation=\"relu\"),\n        keras.layers.Dropout(0.5),\n        keras.layers.Dense(cfg.labels, activation=\"softmax\")\n    ])\n    \n    # Compile Model\n    model.compile(optimizer='Adam',\n                  loss='categorical_crossentropy',\n                  metrics=['acc',get_f1,get_precision,get_recall])\n    \n    # Callback Options During Training \n    callbacks = [ReduceLROnPlateau(monitor='val_accuracy',patience=2,verbose=0, \n                                   factor=0.5,mode='max',min_lr=0.001),\n                 ModelCheckpoint(filepath=f'model_out.h5',monitor='val_accuracy',\n                                 mode = 'max',verbose=0,save_best_only=True),\n                 TqdmCallback(verbose=0)] \n    \n    # Evaluate Model\n    history = model.fit(gen_train,\n                        validation_data = gen_valid,\n                        callbacks=callbacks,\n                        verbose=0,epochs=cfg.n_epochs)\n    \n    # Return Result History\n    return history ","ac2d8c9a":"# lst of augmentation options\nlst_augopt = ['rescale','horizontal_flip','vertical_flip',\n              'brightness_range','rotation_range','shear_range',\n              'zoom_range','width_shift_range','height_shift_range',\n              'channel_shift_range','zca_whitening','featurewise_center',\n              'samplewise_center','featurewise_std_normalization','samplewise_std_normalization']\n\n# lst of default setting corresponding to lst_augopt\nlst_augval = [1.0\/255,True,True,  \n              [1.1,1.5],0.2,0.2,\n              0.2,0,0,\n              0,True,False,\n             False,False,False]\n\n# Get Augmentation Names from lst_select options\ndef get_aug_name(lst_select):\n    lst_selectn = [];\n    for i in lst_select:\n        tlst_all = []\n        for j in i:\n            tlist_selectn = tlst_all.append(lst_augopt[j])\n        lst_selectn.append(tlst_all)\n    return lst_selectn\n\n# Model Evaluation w\/ Augmentation\ndef aug_eval(lst_select=None):\n\n    ii=-1; lst_history = []\n    for augs in lst_select:\n\n        print('Augmentation Combination')\n        # get dictionary of augmentation options\n        ii+=1; dic_select = dict(zip([lst_augopt[i] for i in lst_select[ii]],[lst_augval[i] for i in lst_select[ii]]))\n        print(dic_select)\n\n        # define augmentation options\n        train_datagen = ImageDataGenerator(**dic_select) # pass arguments\n        gen_datagen = ImageDataGenerator(rescale=1.0\/255)\n\n        # evaluate model & return history metric\n        history = augment_model([train_datagen,gen_datagen])\n\n        # store results\n        lst_history.append(history)\n        \n    return lst_history","8fbcf3aa":"# Select Augmentation\nlst_select = [[0,1],[0,2],[0,3],[0,1,5,6]] # list of augmentations\nlst_selectn = get_aug_name(lst_select)     # get list of augmentation names\nprint(lst_selectn)","f43d7234":"# Evaluation Augmentation Models\nhistory = aug_eval(lst_select) # get list of results","c1841bf5":"plot_keras_metric2(history,names=lst_selectn)","aaf567a0":"# Using the best augmentation combination\ntrain_datagen = ImageDataGenerator(rescale=1.0\/255,\n                                   shear_range=0.2,\n                                   zoom_range=0.2,\n                                   horizontal_flip=True,\n                                  )\ngen_datagen = ImageDataGenerator(rescale=1.0\/255)\n\ngen_train = train_datagen.flow_from_directory(train_folder, \n                        target_size=(cfg.sshape[0],cfg.sshape[1]),  # target size\n                        batch_size=32,          # batch size\n                        class_mode='categorical')    # batch size\n\ngen_valid = gen_datagen.flow_from_directory(val_folder,\n                        target_size=(cfg.sshape[0],cfg.sshape[1]),\n                        batch_size=32,\n                        class_mode='categorical')\n\ngen_test = gen_datagen.flow_from_directory(test_folder,\n                        target_size=(cfg.sshape[0],cfg.sshape[1]),\n                        batch_size=32,\n                        class_mode='categorical')","05d57cf2":"# from tensorflow.keras import applications as app\ndef pretrained_model(head_id):\n\n    # Define model with different applications\n    model = Sequential()\n\n    ''' Define Head Pretrained Models '''\n\n    if(head_id is 'vgg'):\n        model.add(app.VGG16(input_shape=cfg.sshape,\n                            pooling='avg',\n                            classes=1000,\n                            include_top=False,\n                            weights='imagenet'))\n\n    elif(head_id is 'resnet'):\n        model.add(app.ResNet101(include_top=False,\n                               input_tensor=None,\n                               input_shape=cfg.sshape,\n                               pooling='avg',\n                               classes=100,\n                               weights='imagenet'))\n\n    elif(head_id is 'mobilenet'):\n        model.add(app.MobileNet(alpha=1.0,\n                               depth_multiplier=1,\n                               dropout=0.001,\n                               include_top=False,\n                               weights=\"imagenet\",\n                               input_tensor=None,\n                               input_shape = cfg.sshape,\n                               pooling=None,\n                               classes=1000))\n\n    elif(head_id is 'inception'):\n        model.add(InceptionV3(input_shape = cfg.sshape, \n                                 include_top = False, \n                                 weights = 'imagenet'))\n\n    elif(head_id is 'efficientnet'):\n        model.add(EfficientNetB4(input_shape = cfg.sshape, \n                                    include_top = False, \n                                    weights = 'imagenet'))\n\n    ''' Tail Model Part '''\n    model.add(Flatten())\n    model.add(Dense(1024,activation='relu'))\n    model.add(Dropout(0.1))\n    model.add(Dense(cfg.labels,activation='softmax'))\n\n    # # freeze main model coefficients\n    model.layers[0].trainable = False\n    model.compile(optimizer='Adam', \n                  loss='categorical_crossentropy',\n                  metrics=['acc',get_f1,get_precision,get_recall])\n\n    return model # return compiled model","05f01196":"# Pretraine Loaded Model \ndef pretrain_eval(lst_heads,verbose=False):\n\n    lst_history = []\n    for head_id in lst_heads:\n\n        # define CNN head model\n        model = pretrained_model(head_id)\n\n        ''' Callback Options During Training '''\n        callbacks = [ReduceLROnPlateau(monitor='val_acc',patience=2,verbose=0, \n                                       factor=0.5,mode='max',min_lr=0.001),\n                     ModelCheckpoint(filepath=f'model_{head_id}.h5',monitor='val_acc',\n                                     mode = 'max',verbose=0,save_best_only=True),\n                     TqdmCallback(verbose=0)] \n\n        ''' Start Training '''\n        start = time.time()\n        history = model.fit(gen_train,\n                            validation_data = gen_valid,\n                            callbacks=callbacks,\n                            verbose=0,\n                            epochs=cfg.n_epochs)\n        end = time.time()\n        if(verbose):\n            print(f'Head Model: {head_id}')\n            print(f'The time taken to execute is {round(end-start,2)} seconds.')\n            print(f'Maximum Train\/Val {max(history.history[\"acc\"]):.4f}\/{max(history.history[\"val_acc\"]):.4f}')\n\n        lst_history.append(history)\n        \n    return lst_history","9cae699f":"''' Define Model Architectre '''\nlst_heads = ['vgg','resnet','mobilenet','inception','efficientnet']\nhistory = pretrain_eval(lst_heads)","ebaa63f5":"plot_keras_metric2(history,names=lst_heads)","b47448a4":"for head_id in lst_heads:\n    print(f'Head Model: {head_id}')\n    \n    # Load uncompiled model\n    load_model = models.load_model(f\"model_{head_id}.h5\",compile=False)\n    \n    # Compile model\n    load_model.compile(optimizer='Adam',\n                       loss='categorical_crossentropy',\n                       metrics=['acc',get_f1,get_precision,get_recall])\n    \n    # Evaluate on test dataset\n    scores = load_model.evaluate(gen_test, verbose=1)","d9898e2c":"#### **<span style='color:#B6DA32'>BROADTAIL MALE<\/span>**\n- Like the adult female, the males also have green and buffy flanks.\n- What separates the male broadtail from the female and even from the Rufous female is the **distinctive rose\/magenta throats**.\n- It's quite likely the model would be easily able to classify any image containing the male from the rest, if the feeder or flag wasn't of similar color.\n- We can clearly observe that the feeder, has both darker spots and lighter spots, lighter spots have values very similar values to the throat.\n- We can also note some images don't visibly have this distinctive red colour throat (at least to the naked eye), one possible reason being that the bird is <b>in the shade at the time of capture<\/b>. It's also possible that the images are not correctly labeled, which cannot also be ruled out. However we can clearly note that the model will need to adapt to images <b>taken under direct sunlight<\/b> and <b>different shade variations<\/b>, which create some problematic scenarios.\n- And <b>last but not least<\/b>, we can't rule out <b>immature males<\/b> from the pack as well. They are extremly similar to female as well.","723ead97":"# <b>3 <span style='color:#B6DA32'>|<\/span> BASELINE CNN MODEL<\/b>\n\n- Firstly, we will create a <b>baseline model<\/b>, which we will use as a reference.\n\n<div style=\"color:white;display:fill;border-radius:5px;background-color:#B6DA32;\n       font-size:220%;font-family:Nexa;letter-spacing:0.5px\">\n    <p style=\"padding: 10px;color:white;\"><b>3.2 | IMAGE AUGMENTATION & DATASET GENERATION<\/b><\/p>\n<\/div>\n\n- One of the important factors affecting the accuracy of the CNN; <b>image augmentation<\/b>. We will focus our attention to the influence of augmentation quite a bit in this notebook.\n- <b>Let's start off with a baseline<\/b>; With a preset <b>rescale<\/b> image augmentation, applied to both training data, validation & test data. ie. Making sure the values lie between [0,1]; which has proven to be quite important for neural netoworks to perform well.\n- Upon defining the desired augmentation modification we will apply to the dataset, we can use the <b>flow_from_directory<\/b> data generator function to generate the dataset, as we already have our dataset nicely assembled into folders <b>train<\/b>,<b>valid<\/b> & <b>test<\/b>.","74431bff":"#### **<span style='color:#B6DA32'>REVIEWING RESULTS<\/span>**\n- What we can see is a neural network steadily learning & reaching a <b>validation accuracy<\/b> peak of about 0.85, which isn't too bad, both precision and recall are quite high.\n- We are <b>not applying any augmentations other than scaling<\/b>, so it is unlikely the model will improve past this point, we defitely can see the validation loss stagnating a little.\n- Next, we ought to test other augmentation additions, applied to the training of the same network.","bb9d9517":"#### **<span style='color:#B6DA32'>REVIEWING RESULTS<\/span>**\n- Pretrained models are built in a way that allows us to load and modify the tail end of the model to fit our own number of label case.\n- A simplistic tail end dense layer was added, however it most likely is not optimal, and neither are the predefined settings for each class, nevertheless it was interesting to compare the different models to one another and compare their performance.\n- <b>VGG<\/b>, <b>InceptionV3<\/b> and most notably <b>MobileNet<\/b> were among the best performing models. In this case, we can see that model performance for both training and test showed a high nature of correlation, <b>MobileNet<\/b> was able to reach a validation accuracy of 0.975. Due to the large nature of customisability in such problems, it's quite likely that further improvement is possible given the right combination of augmentations are applied.","490d6e5f":"#### **<span style='color:#B6DA32'>NO BIRD<\/span>**\n- Given the amount of background noise\/clutter (non bird pixels) we have in our images, <b>no_bird images seem like significant additions<\/b>.\n- Especially important are the <b>flag<\/b> & <b>feeder<\/b> images which we saw in the background of some of the hummingbird images.\n- The subset is quite self explanatory, no birds are present in any of the images, showing the environment around the feeder.","bfd84572":"# <b>5 <span style='color:#B6DA32'>|<\/span> TRANSFER LEARNING CNN MODELS<\/b>\n\n- Having a wide range of networks available to use, let's investigate various networks and their effects on the accuracy of the overall model.\n- Aside from our own simple convolution network, let's look into established networks; <b>VGG<\/b>,<b>ResNet<\/b>,<b>MobileNet<\/b>,<b>Inception<\/b> & <b>EfficientNet<\/b>\n- If we use established networks, we need to adjust the network, ie. adjust the tailend of the model, so it can be used in this problem.\n- Head weights will be frozen so that the network doesn't overfit the training data.\n- Having looked at different augmentations, let's use one of the better combinations & define the dataloaders again. It's not a given that augmentations that led to a better result using the simpler model will translate to a more complex, so let's investigate.","49fa4256":"#### **<span style='color:#B6DA32'>DEFINING THE CALLBACKS<\/span>**\n**<span style='color:#B6DA32'>Callbacks<\/span>** will be used to monitor the progress of our model:\n- We'll be using the <code>ReduceLROnPlateau<\/code> callback & stop model training once the model performance starts to stall as we don't want the model to overfit.\n- We'll be saving our best performing model based on the <b>accuracy<\/b> metric using <code>ModelCheckpoint<\/code>.\n- <code>TqdmCallback(verbose=0)<\/code> is very useful for reducing the keras iterative outputs to a few lines in each cell.\n\n#### **<span style='color:#B6DA32'>TRAINING OUR MODEL<\/span>**\n- Having defined the <b>architecture<\/b> & <b>compiled<\/b> our model, we can start to train the model.","47a9a08e":"# <b>4 <span style='color:#B6DA32'>|<\/span> AUGMENTATION INFLUENCE CNN MODELS<\/b>\n\n#### **<span style='color:#B6DA32'>DATASET VARIATION DURING TRAINING<\/span>**\n\n- There various combinations of **<span style='color:#B6DA32'>image augmentations<\/span>** that we can try in order to attempt find a <b>balance between overfitting and generalisation<\/b>. \n- **<span style='color:#B6DA32'>Image augmentations<\/span>** also allow us to replicate the effect of adding slightly modified images during training, thus resulting in osscilations during training.\n- The full collection of augmentations can be found in the [Keras manual](https:\/\/keras.io\/api\/preprocessing\/image\/).\n\n#### **<span style='color:#B6DA32'>AUGMENTATION OPTIONS<\/span>**\n\n- The selection of augmentation is like a hyperparameter, which will affect our accuracy. We need to investigate the effects of image augmentation in order to understand how they influence the training of the model accuracy, some combinations will result in an improvement & others won't, so the evaluation metric is expected to oscillate quite a bit compared to the base model that we have already evaluated.\n- In the next section, we will use a generalised function, <code>augment_model<\/code> and **cycle through various combinations of image augmentation** to be applied during training.\n- The <b>augment_model<\/b> is simply a function with all steps required to train a model, with the exception of the augmentation settings, which need to be imported via a list (<b>lst_aug<\/b>). ","89dafc76":"#### **<span style='color:#B6DA32'>VARIATION DURING TRAINING<\/span>**\n\n- During each epoch, **a randomised combination of augmentation applied to each image** may lead to a spontaneous improvement in model accuracy. \n- Aside from the perhaps adjustment of <b>brightness_range<\/b>, its not entirely clear what can potentially improve the model, so let's look at a few examples.\n- What we are interesting in determining: <b>which particular combination of augmentation gives us the most promising improvement in metric performance.<\/b>\n- Once we have this information, we can go ahead and try this augmentation on more sophisticated models, so we can potentially obtain a more accurate model.\n- We'll loop through different augmentation combinations and save results in a list format.","383cf687":"#### **<span style='color:#B6DA32'>RUFOUS FEMALE<\/span>**\n- The addition of the <b>female as opposed to the male<\/b> is an interesting choice for the dataset, making it indeed very challenging for the model, due to the high similarity of different species' image values. The female, unlike the male is very similar to the <b>broadtail female<\/b>, <b>especially when in the shade<\/b>, we actually have quite a few such cases, as seen in the images below.\n- However we can notice that in all images, colours of the back\/rump are <b>quite dull, more saturated, but with hints of green<\/b>, just enough to be able to make out the green colour.\n- We can also note that the images have quite a <b>bit of \"noise\"<\/b>, that could potentally affect the accuracy of the model; <b>the feeder & the flag introduce red values into the images<\/b>, which can affect the model accuracy.","5929243e":"#### **<span style='color:#B6DA32'>REVIEWING THE DATA<\/span>**\n- In the context of bird monitoring, what I think this dataset outlines more than anything else is that you don't need to place cameras right next to the feeder, which for some species can be offputting & the images don't need to be of perfect quality in order to create a classifier that can identify hummingbird species accurately.\n- Most hummingbirds are very similar in shape and are <b>mostly differentiable by their colours<\/b>, so one channel CNN input network would be less effective than a three channel network, and we have to rely on all colour channels to distinguish the species.\n- Having gone through the images, we can see that the current dataset is quite a challenging one. A lot of other hummingbirds, especially male have very <b>identifiable feather colours<\/b>, however in this dataset, aside from the <b>broadtail male<\/b>, most hummingbirds <b>seem amost identical to the naked eye<\/b>, especially in images which are slightly underexposed. As a result, it would be of great benefit to create a model that can identify the specie as accurately as possible at any given feeder.\n- The <b>model heavily relies on accurate initial training label data<\/b> in order to be able to accurately classify the current set of hummingbirds on unseed data, so it was assumed that the dataset labels are as accurate as possible.\n\n<div style=\"color:white;display:fill;border-radius:5px;background-color:#B6DA32;\n       font-size:220%;font-family:Nexa;letter-spacing:0.5px\">\n    <p style=\"padding: 10px;color:white;\"><b>2.2 | MODEL TRAINING DATA<\/b><\/p>\n<\/div>\n\n- The modelling dataset is not very big; 100 images per class for <b>training<\/b> & 20 images per class for <b>validation<\/b> and <b>test<\/b> evaluation, used during training and subsequently for inference.\n- The <b>classes are also well balanced<\/b>, containing the same number of images per class, thus we don't need to alter the balance of samples of each class.","976e7156":"#### **<span style='color:#B6DA32'>DEFINING THE CNN MODEL<\/span>**\n- We need to defined the **<span style='color:#B6DA32'>CNN architecture<\/span>** using <code>keras.models.Sequential<\/code>.\n- We'll need **<span style='color:#B6DA32'>an optimiser<\/span>**, let's use the [Adam optimiser](https:\/\/keras.io\/api\/optimizers\/)\n- We need to define a **<span style='color:#B6DA32'>loss function<\/span>**; we'll use [categorical_crossentropy](https:\/\/keras.io\/api\/losses\/probabilistic_losses\/#categoricalcrossentropy-function)","0129aaeb":"![](https:\/\/images-wixmp-ed30a86b8c4ca887773594c2.wixmp.com\/f\/8cc1eeaa-4046-4c4a-ae93-93d656f68688\/desecm2-bab1ae06-e8fa-4e8b-971b-ac8c0f10391e.jpg?token=eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJzdWIiOiJ1cm46YXBwOjdlMGQxODg5ODIyNjQzNzNhNWYwZDQxNWVhMGQyNmUwIiwiaXNzIjoidXJuOmFwcDo3ZTBkMTg4OTgyMjY0MzczYTVmMGQ0MTVlYTBkMjZlMCIsIm9iaiI6W1t7InBhdGgiOiJcL2ZcLzhjYzFlZWFhLTQwNDYtNGM0YS1hZTkzLTkzZDY1NmY2ODY4OFwvZGVzZWNtMi1iYWIxYWUwNi1lOGZhLTRlOGItOTcxYi1hYzhjMGYxMDM5MWUuanBnIn1dXSwiYXVkIjpbInVybjpzZXJ2aWNlOmZpbGUuZG93bmxvYWQiXX0.Oh-7_wDEl-0xLF-6M9CsxIUdJmWS6HXrd_Mdaq43HOA)\nFor [Hummingbirds at my feeders](https:\/\/www.kaggle.com\/akimball002\/hummingbirds-at-my-feeders) dataset, Wonderful Photo by [Zden\u011bk Mach\u00e1\u010dek](https:\/\/unsplash.com\/@zmachacek)\n\n# <b>1 <span style='color:#B6DA32'>|<\/span> INTRODUCTION<\/b>\n\n<div style=\"color:white;display:fill;border-radius:5px;background-color:#B6DA32;\n       font-size:220%;font-family:Nexa;letter-spacing:0.5px\">\n    <p style=\"padding: 10px;color:white;\"><b>1.1 | PROBLEM STATEMENT<\/b><\/p>\n<\/div>\n\n#### **<span style='color:#B6DA32'>TYPES OF BIRD MONITORING<\/span>**\n\n- There are two main types of possible bird monitoring approaches that exist to this day:\n> We can either see a bird (**<span style='color:#B6DA32'>visual monitoring<\/span>**) or we can hear it (**<span style='color:#B6DA32'>sound based monitoring<\/span>**).\n- Both approaches have specific challenges, & in both cases, **<span style='color:#B6DA32'>expert identification<\/span>** tends to be required to identify a specie correctly.\n- To this day, a sizable portion of the **<span style='color:#B6DA32'>monitoring<\/span>** & **<span style='color:#B6DA32'>observation<\/span>** surrounding birds is mostly **<span style='color:#B6DA32'>done manually<\/span>** by researchers & bird monitoring enthusiasts.\n\nJust to note a few collaboration efforts:\n\n> I. For example; you can submit your own sightings to [Hummingbirdcentral](https:\/\/www.hummingbirdcentral.com\/) <br>\n> II. Another example; for bird sound based sighting; [Xeno-Canto](https:\/\/www.xeno-canto.org\/).\n\n#### **<span style='color:#B6DA32'>ANYONE CAN CONTRIBUTE<\/span>**\n\n- Various people from different backgrounds and level of bird knowledge & bird monitoring can contribute to such a wonderful cause:\n\n> I. Conservation organizations would also have better information on migratory and breeding patterns<\/b> <br>\n> II. This knowledge can be used to determine if specific changes in the environment or ecology has positively or negatively impacted bird life. <br>\n\n#### **<span style='color:#B6DA32'>SOME OF THE ISSUES<\/span>**\n\n- Such contributions often requiring only field notes and reasoning, with optional photographic evidence, which is an indication that sampled data could potentially be incorrect due to the inexperienced contributors. \n- Visual monitoring requires **<span style='color:#B6DA32'>constant expert confirmation<\/span>** in order for the data to be of significant use. \n- <b>Visual monitoring<\/b> is definitely by far quite the more popular approach for monitoring birds.\n- People who are interested in birds often end up deciding to help, turning to quite affordable photography\/video gear, even smartphones, which these day have the capability to help bird monitoring.\n\n\n<div style=\"color:white;display:fill;border-radius:5px;background-color:#B6DA32;\n       font-size:220%;font-family:Nexa;letter-spacing:0.5px\">\n    <p style=\"padding: 10px;color:white;\"><b>1.2 | SOME OF THE CHALLENGES<\/b><\/p>\n<\/div>\n\n- With the increased affordability of visual monitoring equipment, it has become practical for anyone to contribute to such a wonderful cause & help make each sighting more valid. \n\n#### **<span style='color:#B6DA32'>ARISING INCONSISTENCIES<\/span>**\n\n- Often, due to <b>limitations of gear<\/b>, <b>poor photography\/videography technique<\/b>, or **<span style='color:#B6DA32'>simply poor lighting conditions<\/span>**, it can be difficult for users, <b>let alone experts<\/b> to distinguish what specific bird was seen at the time of monitoring.\n\n#### **<span style='color:#B6DA32'>DATASET WITH VISUALLY SIMILAR SPECIES<\/span>**\n\n- In this study, we will focus our attention to a bird called the <b>hummingbird<\/b>. What is useful about this specie is that, despite being quite distinguishible in shape, <b>they have a variery of unique colouring<\/b>, which means that if images are of poor quality, it may be hard for humans or models to distinguish them.\n\n#### **<span style='color:#B6DA32'>USING MODELS TO HELP IDENTIFY SPECIES<\/span>**\n\n- In the entire process of <b>expert identification<\/b> & dealing with various image related inconsistencies outlied previously, manual identification for monitoring can be quite labourous, so an automated system of identification can go a long way.\n- In our quest to create an automated approach, we can be left with a collection or under or over exposed images that will create difficulties for the model to distinguish between different classes correctly. \n\n<div style=\"color:white;display:fill;border-radius:5px;background-color:#B6DA32;\n       font-size:220%;font-family:Nexa;letter-spacing:0.5px\">\n    <p style=\"padding: 10px;color:white;\"><b>1.3 | NOTEBOOK AIM<\/b><\/p>\n<\/div>\n\nThe ultimate goal is to have a **<span style='color:#B6DA32'>classification system<\/span>** that can address such the above stated varieties in an image & **<span style='color:#B6DA32'>correctly distinguish very similar bird species<\/span>**.\n\n\n<div style=\"color:white;display:fill;border-radius:5px;background-color:#B6DA32;\n       font-size:220%;font-family:Nexa;letter-spacing:0.5px\">\n    <p style=\"padding: 10px;color:white;\"><b>1.4 | USEFUL LINKS<\/b><\/p>\n<\/div>\n\n- [Hummingbirds 101](https:\/\/www.perkypet.com\/advice\/hummingbirds-101)\n- [Hummingbird Nectar Recipes](https:\/\/nationalzoo.si.edu\/migratory-birds\/hummingbird-nectar-recipe)\n- [Keras Image Augmentation](https:\/\/www.tensorflow.org\/tutorials\/images\/data_augmentation) ","be99e600":"#### **<span style='color:#B6DA32'>TESTED AUGMENTATION COMBINATIONS<\/span>**\n\n- <b>Combination 1<\/b> : <code>rescale (1\/255)<\/code>, <code>horizontal_flip<\/code>\n- <b>Combination 2<\/b> : <code>rescale (1\/255)<\/code>, <code>vertical_flip<\/code>\n- <b>Combination 3<\/b> : <code>rescale (1\/255)<\/code>, <code>brightness_range (+1.1,+1.5)<\/code>\n- <b>Combination 4<\/b> : <code>rescale (1\/255)<\/code>, <code>horizontal_flip<\/code>, <code>shear_range (0.2)<\/code>, <code>zoom_range (0.2)<\/code>","cc50d1c9":"#### **<span style='color:#B6DA32'>BROADTAIL FEMALE<\/span>**\n- To the naked eye, there is <b>a lot of similarities<\/b> between the <b>Rufous<\/b> & <b>Broadtail Females<\/b>.\n- Without adequate lighting and refence to multiple frames (such as from a video), one could easily mislabel the species.\n- The boadtail female colour definitely stand out more, the two species so far have very similar bagrounds & the ocassional feeder.","ad228317":"<div style=\"color:white;display:fill;border-radius:5px;background-color:#B6DA32;\n       font-size:220%;font-family:Nexa;letter-spacing:0.5px\">\n    <p style=\"padding: 10px;color:white;\"><b>3.2 | TRAINING A CONVOLUTION NEURAL NETWORK<\/b><\/p>\n<\/div>\n\n- Let's be sure to try out a less complicated neural network consisting of <b>two convolution layers<\/b>, <b>a pooling layer<\/b> and <b>dense block of layers<\/b> at first, and work out way upto more complex networks.\n- The Neural network will have to be trained from scratch, with <b>randomly generated weights for initialisation<\/b>.\n- To train a model, we need to first define a <b>model architecture<\/b>, then <b>compile<\/b> (req. <b>an optimiser<\/b>, <b>a loss function<\/b> & <b>evaluation metric<\/b>) it & subsequently <b>fit<\/b>, to train the model.\n- <b>callbacks<\/b> are useful as well to control various aspect of the training procedure; lets use two commonly used ones <b>ReduceLROnPlateau<\/b> (for learning rate adjustment) & <b>ModelCheckpoint<\/b> (for model saves).\n- Let's also plot common metrics used in classification: accuracy, f1, precision & recall.\n\n#### **<span style='color:#B6DA32'>DEFINING THE EVALUATION METRICS<\/span>**\n- By default, keras doesn't offer monitoring of <b>F1<\/b>,<b>precision<\/b> & <b>recall<\/b> during callbacks, but we can define our own, adding them to <code>model.compile<\/code>.","3ffad45a":"# <b>6 <span style='color:#B6DA32'>|<\/span> INFERENCE<\/b>\n\n<div style=\"color:white;display:fill;border-radius:5px;background-color:#B6DA32;\n       font-size:220%;font-family:Nexa;letter-spacing:0.5px\">\n    <p style=\"padding: 10px;color:white;\"><b>6.1 | UNSEEN TEST IMAGES<\/b><\/p>\n<\/div>\n\n- We want to test our models on unseed data, so we'll be using the previously defined <code>gen_test<\/code> data.\n- We'll be using the transfer learning models that that were defined in the previous section as they were the higher performing models.\n- Having created a model & verified that our model has a <b>good balance between training & validation accuracy<\/b>, let's see how well the model performs on unseed data.","bc943ad1":"#### **<span style='color:#B6DA32'>REVIEWING RESULTS<\/span>**\n\n#### **POSITIVE INFLUENCE OF BRIGHTNESS AUGMENTAION**\n\n- It was thought that <b>due to the low brightness nature of a lot of images<\/b>, an increase in brightness would allow the model to more easily distinguish between different classes. \n- We can see that when just by the applying the increased brightness augmentation (<b>Combination 3<\/b>); [0,3]  set to (+1.1,+1.5), the model outperforms all other variations within the first 5 iterations, both on <b>training<\/b> & <b>validation<\/b> dataasets, after which the validation accuracy starts to stagnate, and the model starts to show signs of overfitting.\n\n#### **OTHER OBSERVATIONS**\n\n- What was interesting to observe was the <b>balance between training\/validation accuracies<\/b>. \n- Models with lots of augmentation combinations (<b>Combination 4<\/b>) tended to learned slower, ended up with lower training accuracies but generalised better on unseen data.\n- Simple Horizontal flipping, [0,1] (<b>Combination 1<\/b>) and the combination of four augmentations (shearing,zooming,flipping) [0,1,5,6], both were more effective than simply applying a brightness augmentation adjustments [0,3].\n\n#### **USING FUNCTIONS**\n\n- Due to the large number of possible augmentation combinations, it was quite convenient to create and test a <b>list based augmentation selection approach<\/b>. \n- This allowed us to simple loop through the various set options. \n- More testing would be quite interesting to try, however we should next <b>shift our attention to more sophisticated neural networks<\/b>, since they are more than likely going to allow us to edge the current validaiton accuracy score of our simple network.","368afaff":"<div style=\"color:white;display:fill;border-radius:5px;background-color:#B6DA32;\n       font-size:220%;font-family:Nexa;letter-spacing:0.5px\">\n    <p style=\"padding: 10px;color:white;\"><b>2.1 | TRAINING IMAGE EXPLORATION<\/b><\/p>\n<\/div>\n\nLet's view what kind of images we are dealing with in this dataset.","64123e57":"# <b>2 <span style='color:#B6DA32'>|<\/span> THE DATASET<\/b>\n\n- We will go through the images images used in training and attempt to outline some key things we can observe.\n- The primary birds at the feeder are [broad-tailed (Selasphorous platycerus)](https:\/\/www.allaboutbirds.org\/guide\/Broad-tailed_Hummingbird\/id ) and [rufous (Selasphorous rufus)](https:\/\/www.allaboutbirds.org\/guide\/Rufous_Hummingbird\/) hummingbirds.\n\n#### **<span style='color:#B6DA32'>FOLDER LAYOUT<\/span>**\n- We have a specific folder already assembled specifically for modelling; <b>hummingbirds<\/b>, which contains subfolders for each separate class. When it comes time to create a dataset, this will be very convenient since we can just call the <b>.flow_from_directory<\/b> function.\n- There also is a folder containing a 'no bird' classification.\n- As well as two other folder associated with a video recording (collection of still images); <b>video_test<\/b> & All images in the dataset; <b>All_images<\/b>"}}