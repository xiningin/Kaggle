{"cell_type":{"50fec21d":"code","71aef707":"code","571627c0":"code","236d1098":"code","f573d6c0":"code","1c412193":"code","3b026557":"code","0aaf3fa9":"code","58c62635":"code","51674b70":"code","fa6c7158":"code","68508571":"code","1efc70b6":"code","364a5f05":"code","77fa9309":"markdown","b2c7ca6b":"markdown","0945be68":"markdown","a6ac1e74":"markdown","74af8bfd":"markdown","a054adf9":"markdown","3fbdb69f":"markdown","95eed450":"markdown","6443636a":"markdown","d7649d55":"markdown","565da130":"markdown","6b4ed09a":"markdown","dde4cba2":"markdown","5135ad9c":"markdown","72750cbb":"markdown","b332636d":"markdown","18b2ffb0":"markdown","755a258d":"markdown","7e698175":"markdown","4301a2f4":"markdown","e10a22cb":"markdown"},"source":{"50fec21d":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn import metrics\nfrom sklearn.model_selection import cross_validate\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import make_scorer\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","71aef707":"df = pd.read_csv('\/kaggle\/input\/credit-card-customers\/BankChurners.csv')\ncolumn_list = df.columns\ndf = df.loc[:,column_list[:21]]\ndf.head()","571627c0":"flag =  df['Attrition_Flag'].unique()\nflag_count = []\nfor f in flag:\n    filt = (df['Attrition_Flag'] == f)\n    flag_count.append(df.loc[filt,'Attrition_Flag'].count())    \nfig,ax = plt.subplots(2,figsize=(10,10))\nax[0].pie(flag_count,labels=flag,autopct='%1.1f%%',shadow=True)\nax[0].set_title('Current Situation')\nsns.countplot(df['Attrition_Flag'],ax=ax[1])\nplt.show()","236d1098":"df['Customer_Age'].max()\nage_bin = list(range(20,100,5))\nfig,ax = plt.subplots(2,figsize=(10,10))\nsns.countplot(df['Customer_Age'],ax=ax[0])\nax[0].set_title('Actual count of age groups')\nsns.distplot(df['Customer_Age'],bins=age_bin,ax=ax[1])\nax[1].set_title('Variation of age group')\nplt.show()\n","f573d6c0":"fig,ax = plt.subplots(5,figsize=(20,20))\nsns.countplot(df['Gender'],ax=ax[0])\nax[0].set_title('Gender count plot')\nsns.countplot(df['Dependent_count'],ax=ax[1])\nax[1].set_title('Dependent count distribution')\nsns.countplot(df['Education_Level'],ax=ax[2])\nax[2].set_title('Education_Level level')\nsns.countplot(df['Income_Category'],ax=ax[3])\nax[3].set_title('Income_Category plot')\ncard_list = df['Card_Category'].unique()\ncard_count = []\nfor card in card_list:\n    filt = (df['Card_Category'] == card)\n    card_count.append(df.loc[filt,'Card_Category'].count())\nax[4].pie(card_count,labels = card_list,autopct='%1.1f%%',shadow=True)    \nax[4].set_title('Percentge of various cards')\nplt.show()\n","1c412193":"#['Existing Customer', 'Attrited Customer']\nfig, ax = plt.subplots(4,figsize=(30,30))\ndf_EC = df[df['Attrition_Flag'] == 'Existing Customer']\ndf_AC = df[df['Attrition_Flag'] == 'Attrited Customer']\nfor i,card in enumerate(card_list):\n    filt_EC = (df_EC['Card_Category'] == card)\n    filt_AC = (df_AC['Card_Category'] == card)\n    ax[i].scatter(df_EC.loc[filt_EC,'Months_on_book'],df_EC.loc[filt_EC,'Credit_Limit'],color='r',marker='.',label='Existing Customer')\n    ax[i].scatter(df_AC.loc[filt_AC,'Months_on_book'],df_AC.loc[filt_AC,'Credit_Limit'],color='g',marker='x',label='Churned Customer')\n    ax[i].set_title('for {} card'.format(card))\n    ax[i].set_xlabel('Duration in months with cutomer')\n    ax[i].set_ylabel('Credit Limit')\nplt.legend()\nplt.show()    \n#df.head()\n#sns.countplot('Card_Category',hue='Customer_Age',data=df)","3b026557":"sns.countplot(df['Contacts_Count_12_mon'],hue=df['Attrition_Flag'])\nplt.show()","0aaf3fa9":"df.drop(columns=['CLIENTNUM'],inplace=True)\ndf.isnull().sum()","58c62635":"#['Attrition_Flag', 'Gender', 'Education_Level', 'Marital_Status','Income_Category', 'Card_Category']\nincome_map = {'Less than $40K':20,\n              '$40K - $60K':50,\n              '$60K - $80K':70,\n              '$80K - $120K':100,\n              '$120K +':140,\n              'Unknown':0}\ndf['Income_Category'] = df['Income_Category'].map(income_map)\ndf.head()\nfilt = (df['Income_Category'] != 0)\nnum_cols = df.select_dtypes(include=['int32','int64','float64']).columns\ndf_num = df.loc[filt,num_cols]\ncorr = df_num.corr()\nsns.heatmap(corr)\n#income_array = np.array(df.loc[filt,'Income_Category'])\n#limit_array = np.array(df.loc[filt,'Avg_Utilization_Ratio'])\n#plt.plot(limit_array,income_array)","51674b70":"from sklearn.linear_model import LinearRegression\nnum_feat = ['Customer_Age', 'Dependent_count', 'Months_on_book','Total_Relationship_Count', 'Months_Inactive_12_mon','Contacts_Count_12_mon', 'Credit_Limit', 'Total_Revolving_Bal','Avg_Open_To_Buy', 'Total_Amt_Chng_Q4_Q1', 'Total_Trans_Amt','Total_Trans_Ct', 'Total_Ct_Chng_Q4_Q1', 'Avg_Utilization_Ratio']\ny = df_num['Income_Category']\nX = df_num.drop(columns = 'Income_Category')\nlr = LinearRegression()\nlr.fit(X,y)\nfilt2 = (df['Income_Category'] == 0)\nfor i in range(len(df)):\n    if df.loc[i,'Income_Category'] == 0:\n               feature = np.array(df.loc[i,num_feat]).reshape(1,-1)\n               temp_y = lr.predict(feature)\n               df.loc[i,'Income_Category'] = temp_y ","fa6c7158":"df['Attrition_Flag'] = df['Attrition_Flag'].map({'Existing Customer':0,'Attrited Customer':1})\ny = df['Attrition_Flag']\nX = df.drop(columns='Attrition_Flag')\ncat_cols = X.select_dtypes(include=['object']).columns\nX_cat = pd.get_dummies(X[cat_cols],drop_first=True)\nX.drop(columns=cat_cols,inplace=True)\nX = pd.concat([X,X_cat],axis=1)\nX.head()","68508571":"y.value_counts()","1efc70b6":"from imblearn.over_sampling import SMOTE\nfig,ax = plt.subplots(2,figsize=(15,15))\nprint('class counts before balancing dataset')\nprint(y.value_counts())\nsns.countplot(y,ax=ax[0])\nax[0].set_title('Before balancing')\n\nsmote = SMOTE(sampling_strategy='minority')\nX_bal, y_bal = smote.fit_sample(X,y)\nprint('class count after balancing the dataset')\nprint(y_bal.value_counts())\nsns.countplot(y_bal,ax=ax[1])\nax[1].set_title('After balancing')\nplt.show()\n","364a5f05":"# KNN model\nclf = KNeighborsClassifier(n_neighbors = 9)\nscoring = {'accuracy': 'accuracy',\n           'precision': 'precision',\n           'recall': 'recall',\n           'f1': 'f1'\n           }\nModel = ''\ntest_accuracy = 0\ntest_precision = 0\ntest_recall = 0\ntest_f1 = 0\nperformance_metrics = pd.DataFrame(columns=['Model','test_accuracy','test_precision','test_recall','test_f1'])\nmetrics_dict = {'Model':Model,'test_accuracy':test_accuracy,'test_precision':test_precision,'test_recall':test_recall,'test_f1':test_f1}\nscores_dict = cross_validate(clf, X_bal, y_bal, scoring=scoring, n_jobs=-1)\nfit_time = scores_dict['fit_time'].mean()\nscore_time = scores_dict['score_time'].mean()\nmetrics_dict['Model'] = 'KNN'\nmetrics_dict['test_accuracy'] = scores_dict['test_accuracy'].mean()\nmetrics_dict['test_precision'] = scores_dict['test_precision'].mean()\nmetrics_dict['test_recall'] = scores_dict['test_recall'].mean()\nmetrics_dict['test_f1'] = scores_dict['test_f1'].mean()\nperformance_metrics = performance_metrics.append(metrics_dict, ignore_index=True)\n\n#Logistic regression\nclf = LogisticRegression()\nscores_dict = cross_validate(clf, X_bal, y_bal, scoring=scoring, n_jobs=-1)\nfit_time = scores_dict['fit_time'].mean()\nscore_time = scores_dict['score_time'].mean()\nmetrics_dict['Model'] = 'LogisticRegression'\nmetrics_dict['test_accuracy'] = scores_dict['test_accuracy'].mean()\nmetrics_dict['test_precision'] = scores_dict['test_precision'].mean()\nmetrics_dict['test_recall'] = scores_dict['test_recall'].mean()\nmetrics_dict['test_f1'] = scores_dict['test_f1'].mean()\nperformance_metrics = performance_metrics.append(metrics_dict, ignore_index=True)\n\n#Support Vector\nclf = SVC()\nscores_dict = cross_validate(clf, X_bal, y_bal, scoring=scoring, n_jobs=-1)\nfit_time = scores_dict['fit_time'].mean()\nscore_time = scores_dict['score_time'].mean()\nmetrics_dict['Model'] = 'SVM'\nmetrics_dict['test_accuracy'] = scores_dict['test_accuracy'].mean()\nmetrics_dict['test_precision'] = scores_dict['test_precision'].mean()\nmetrics_dict['test_recall'] = scores_dict['test_recall'].mean()\nmetrics_dict['test_f1'] = scores_dict['test_f1'].mean()\nperformance_metrics = performance_metrics.append(metrics_dict, ignore_index=True)\n\n#Naive bayes\n\nclf = GaussianNB()\nscores_dict = cross_validate(clf, X_bal, y_bal, scoring=scoring, n_jobs=-1)\nfit_time = scores_dict['fit_time'].mean()\nscore_time = scores_dict['score_time'].mean()\nmetrics_dict['Model'] = 'Naive bayes'\nmetrics_dict['test_accuracy'] = scores_dict['test_accuracy'].mean()\nmetrics_dict['test_precision'] = scores_dict['test_precision'].mean()\nmetrics_dict['test_recall'] = scores_dict['test_recall'].mean()\nmetrics_dict['test_f1'] = scores_dict['test_f1'].mean()\nperformance_metrics = performance_metrics.append(metrics_dict, ignore_index=True)\n\n#Random forest \nclf = RandomForestClassifier(n_estimators = 100)\nscores_dict = cross_validate(clf, X_bal, y_bal, scoring=scoring, n_jobs=-1)\nfit_time = scores_dict['fit_time'].mean()\nscore_time = scores_dict['score_time'].mean()\nmetrics_dict['Model'] = 'Randon Forest'\nmetrics_dict['test_accuracy'] = scores_dict['test_accuracy'].mean()\nmetrics_dict['test_precision'] = scores_dict['test_precision'].mean()\nmetrics_dict['test_recall'] = scores_dict['test_recall'].mean()\nmetrics_dict['test_f1'] = scores_dict['test_f1'].mean()\nperformance_metrics = performance_metrics.append(metrics_dict, ignore_index=True)\n\n#\n\n\nperformance_metrics\n","77fa9309":"**As we know majority of cutomers hold Blue card and looking at the plot for blue card it seems that people with medium to low credit limit are most likely to churn, alleast for the blue card, not much of a noticeble pattern in other products**","b2c7ca6b":"**customers contacted arund 2-4 times churned most, may be more follow ups were required**","0945be68":"**Plotting the target variable**","a6ac1e74":"**Encoding the Catagorical Variables**\n**We need to encode our target column 'Attrition_Flag' as well into numeric(0,1)**","74af8bfd":"1- Gender plot.\n2- Dependent Distribution\n3- Education Level Plot\n4- Income Category Plot\n5- Card Category","a054adf9":"This is a highly unbalanced dataset.","3fbdb69f":"**Let's see if contacting customer from the bank has any pattern with churning**","95eed450":"Above result shows the high degree of imbalance, next we balance it","6443636a":"# Loading dataframe","d7649d55":"Variation of age groups in dataset.","565da130":"# Machine learning","6b4ed09a":"**Let's visualize trend among people churned**","dde4cba2":"# Feature engineering.","5135ad9c":"Finally we have populated the Unknown values in 'Income_Category' column with the help of linear regression.","72750cbb":"# Basic data analysis","b332636d":"Dropping 'CLIENTNUM'column, Checking for Null values","18b2ffb0":"Lets see if any column correlates with income catagory, so that we can use that to replace unknown.\nWe average out the range for Income Category and draw heatmap, from heatmap we can see that income catagory is highly negative correlated to Avg_Utilization_Ratio, we'll have to create a linear regression model to predict unknown values.","755a258d":"**Building linear regression model to predict unknown values in income category**","7e698175":"Age looks to be uniformly distributed","4301a2f4":"# # Balancing our dataset\n**As we know there is an imbalance of classes in our dataset, we need to balance it before moving to build our model**","e10a22cb":"**Now we need to OneHotEncode our categorical features except 'Income_Category' as this is a ordinal feature and we need to map it to ordinal values.**\n\n'Income_Category' column has some unknown values, we need to figure out a strategy to fill in suitable value there.\n"}}