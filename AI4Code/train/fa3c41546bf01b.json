{"cell_type":{"9d57bcbd":"code","64c36304":"code","a4af13b3":"code","9b9670a5":"code","17e6d705":"code","6243dac9":"code","88c76bec":"code","d78146de":"code","36928b48":"code","a76d2c54":"code","075d7c42":"code","0dfee696":"code","230c618a":"code","5fa43e67":"code","b5f7aa4e":"code","bbb7e727":"code","b51232dc":"code","b8e918bd":"code","c874097f":"code","e5040061":"code","3cb29070":"code","1073aee2":"code","24c564d2":"code","18f0ac23":"code","52510100":"code","f2890408":"code","807adc9f":"code","969e045f":"code","b921c900":"code","5d253c21":"code","b30505e1":"code","d40be13e":"code","7af6d237":"code","c180c0ef":"code","3c39c01d":"code","a8a6d1cb":"code","6011d8bb":"code","f7b88cc6":"code","f2867cfb":"code","eb05d77d":"code","375f6821":"code","454724b4":"markdown","a5b49dd7":"markdown","3f213f32":"markdown","962cdcb0":"markdown","10d35036":"markdown","47cfdfcb":"markdown","de0cc7cb":"markdown","2442e181":"markdown","40f30631":"markdown","9b7f29c4":"markdown","0f7b4990":"markdown","5cf96ce4":"markdown","e9e5c201":"markdown","d538e82f":"markdown","5cf7af99":"markdown","c3b24b4c":"markdown","4ed1b3d8":"markdown","796b66b6":"markdown"},"source":{"9d57bcbd":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns","64c36304":"df = pd.read_csv(r\"\/kaggle\/input\/factors-affecting-campus-placement\/Placement_Data_Full_Class.csv\")\ndf.head()","a4af13b3":"df.drop(['sl_no'], axis = 1, inplace = True) #dropping insignificant values\ndf.head()","9b9670a5":"df.info()\n","17e6d705":"\ndf['salary'].fillna(int(df['salary'].mean()), inplace=True)\ndf.head()","6243dac9":"df.info()\ndf['degree_t'].value_counts()","88c76bec":"df[\"Stat\"] = df[\"status\"]\ndf.head()\ndf.drop(['status'], axis = 1, inplace = True)","d78146de":"df.head()","36928b48":"# heat map correlation\n#HEAT MAP CORRELATION\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\ncorrmat = df.corr()\nf, ax = plt.subplots(figsize=(20, 15))\nsns.heatmap(corrmat, vmax=.8, square=True, annot=True,cmap=\"YlGnBu\")","a76d2c54":"#count plot genders\nf = plt.subplots(figsize = (9 , 7))\nsns.countplot(data = df, x = \"gender\", palette = \"flare\")\nplt.show()\n","075d7c42":"#COUNT PLOT DEGREE\nf = plt.subplots(figsize = (9 , 7))\nsns.countplot(data = df, x = \"degree_t\", palette = \"husl\")\nplt.show()\n","0dfee696":"#COUNT PLOT WORK EXPERIENCE\nf = plt.subplots(figsize = (9 , 7))\nsns.countplot(data = df, x = \"workex\", palette = \"ch:s=.25,rot=-.25\")\nplt.show()\n","230c618a":"#COUNT PLOT SPECIALISATION\nf = plt.subplots(figsize = (9 , 7))\nsns.countplot(data = df, x = \"specialisation\", palette = \"Set2\")\nplt.show()\n","5fa43e67":"#SPECIALISATION V\/S STATUS\n#COUNT PLOT SPECIALISATION\nf = plt.subplots(figsize = (9 , 7))\nsns.countplot(data = df, x = \"specialisation\",hue = 'Stat', palette = \"Set2\")\nplt.title(\"specialisation v\/s Status\")\nplt.show()\n","b5f7aa4e":"#COUNT PLOT \nf = plt.subplots(figsize = (7, 7))\nsns.countplot(data = df, x = \"degree_t\", hue = 'Stat', palette = \"flare\")\nplt.title(\"DEGREE V\/S STATUS\")\nplt.show()\n","bbb7e727":"#WORK EXP V\/S STATUS\nf = plt.subplots(figsize = (7, 7))\nsns.countplot(data = df, x = \"workex\", hue = 'Stat', palette = \"pastel\")\nplt.title(\"WORK EXP V\/S STATUS\")\nplt.show()\n","b51232dc":"#WORK EXP V\/S STATUS\nf = plt.subplots(figsize = (12 , 12))\nsns.histplot(data = df, x = \"salary\", hue = 'specialisation', palette = \"pastel\", bins = 30)\nplt.title(\"SALARY V\/S SPECIALISATION\")\nplt.show()\n","b8e918bd":"plt.figure(figsize=(12,10))\nsns.histplot(data=df, x=\"degree_p\", hue=\"gender\", kde = True, palette = \"flare\")\nplt.title('gender v\/s DEGREE percentage' , fontsize=15)\nplt.xlim([18,100])\nplt.show()","c874097f":"#placed with percentage\nplt.figure(figsize=(12,10))\nsns.histplot(data=df, x=\"degree_p\", hue=\"Stat\", kde = True, element= 'poly',palette = \"pastel\")\nplt.title('DEGREE percentage V\/S STATUS' , fontsize=15)\nplt.xlim([18,100])\nplt.show()","e5040061":"plt.figure(figsize=(12,10))\nsns.histplot(data=df, x=\"mba_p\", hue=\"gender\", kde = True)\nplt.title('gender v\/s MBA percentage' , fontsize=15)\nplt.xlim([18,100])\nplt.show()","3cb29070":"#placed with percentage\nplt.figure(figsize=(12,10))\nsns.histplot(data=df, x=\"mba_p\", hue=\"Stat\", kde = True, element= 'poly',palette = \"flare\")\nplt.title(' MBA percentage V\/S STATUS' , fontsize=15)\nplt.xlim([18,100])\nplt.show()","1073aee2":"x = df.iloc[:, :13]\ny = df.iloc[:, 13]\nx","24c564d2":"y","18f0ac23":"#ONE HOT ENCODING\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.compose import ColumnTransformer\n\nct = ColumnTransformer(transformers = [('encoder', OneHotEncoder(), [5, 7])], remainder = 'passthrough')\nx = np.array(ct.fit_transform(x))\nprint(x.shape)","52510100":"#label encoding\nle = LabelEncoder()\nx[:, 6] = le.fit_transform(x[:, 6]) #gender\nx[:, 8] = le.fit_transform(x[:, 8]) #ssc_b\nx[:, 10] = le.fit_transform(x[:, 10]) #hsc_b\nx[:, 12] = le.fit_transform(x[:, 12]) #workexp\nx[:, 14] = le.fit_transform(x[:, 14]) #specialisation\nx[:, 6] = le.fit_transform(x[:, 6])","f2890408":"y = le.fit_transform(y)\ny","807adc9f":"print(x.shape)\nprint(y.shape)","969e045f":"#splitting\nfrom sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.2, random_state = 1)","b921c900":"#scaling\nfrom sklearn.preprocessing import StandardScaler\nstd = StandardScaler()\nx_train = std.fit_transform(x_train)\nx_test = std.fit_transform(x_test)","5d253c21":"#importing metrics\nfrom sklearn.metrics import accuracy_score, confusion_matrix, roc_auc_score, ConfusionMatrixDisplay, precision_score, recall_score, f1_score, classification_report, roc_curve, plot_roc_curve, auc, precision_recall_curve, plot_precision_recall_curve, average_precision_score\nfrom sklearn.model_selection import cross_val_score","b30505e1":"#logistic regression\nfrom sklearn.linear_model import LogisticRegression\nm1 = LogisticRegression()\nm1.fit(x_train, y_train)\n\nm1_pred = m1.predict(x_test)\nm1_pred\nprint(\"logistic regression\", \"\\n\")\nprint(\"accuracy score :\", accuracy_score(m1_pred, y_test))\nprint(\"precision score: \", precision_score(m1_pred, y_test))\nprint(\"f1 score: \", f1_score(m1_pred, y_test))\nprint(\"auc score: \", roc_auc_score(m1_pred, y_test))\nprint(\"recall score : \", recall_score(m1_pred, y_test))\nprint(\"confusion matrix\", confusion_matrix(m1_pred, y_test))\nprint(\"\\n\", \"*\" * 30)\n\n#svm\nfrom sklearn.svm import SVC\nm2 = SVC()\nm2.fit(x_train, y_train)\n\nm2_pred = m2.predict(x_test)\nm2_pred\nprint(\"SVC\", \"\\n\")\nprint(\"accuracy score :\", accuracy_score(m2_pred, y_test))\nprint(\"precision score: \", precision_score(m2_pred, y_test))\nprint(\"f1 score: \", f1_score(m2_pred, y_test))\nprint(\"auc score: \", roc_auc_score(m2_pred, y_test))\nprint(\"recall score : \", recall_score(m2_pred, y_test))\nprint(\"confusion matrix\", confusion_matrix(m2_pred, y_test))\nprint(\"\\n\", \"*\" * 30)\n\n\n#random forest\nfrom sklearn.ensemble import RandomForestClassifier\nm3 = RandomForestClassifier()\nm3.fit(x_train, y_train)\n\nm3_pred = m3.predict(x_test)\nm3_pred\nprint(\"Random Forest\", \"\\n\")\nprint(\"accuracy score :\", accuracy_score(m3_pred, y_test))\nprint(\"precision score: \", precision_score(m3_pred, y_test))\nprint(\"f1 score: \", f1_score(m3_pred, y_test))\nprint(\"auc score: \", roc_auc_score(m3_pred, y_test))\nprint(\"recall score : \", recall_score(m3_pred, y_test))\nprint(\"confusion matrix\", confusion_matrix(m3_pred, y_test))\nprint(\"\\n\", \"*\" * 30)\n\n\n#knn\nfrom sklearn.neighbors import KNeighborsClassifier\nm4 = KNeighborsClassifier()\nm4.fit(x_train, y_train)\n\nm4_pred = m4.predict(x_test)\nm4_pred\nprint(\"Knn\", \"\\n\")\nprint(\"accuracy score :\", accuracy_score(m4_pred, y_test))\nprint(\"precision score: \", precision_score(m4_pred, y_test))\nprint(\"f1 score: \", f1_score(m4_pred, y_test))\nprint(\"auc score: \", roc_auc_score(m4_pred, y_test))\nprint(\"recall score : \", recall_score(m4_pred, y_test))\nprint(\"confusion matrix\", confusion_matrix(m4_pred, y_test))\nprint(\"\\n\", \"*\" * 30)\n\n\n#decision tree\nfrom sklearn.tree import DecisionTreeClassifier\nm5 = DecisionTreeClassifier()\nm5.fit(x_train, y_train)\n\nm5_pred = m5.predict(x_test)\nm5_pred\nprint(\"decision Tree\", \"\\n\")\nprint(\"accuracy score :\", accuracy_score(m5_pred, y_test))\nprint(\"precision score: \", precision_score(m5_pred, y_test))\nprint(\"f1 score: \", f1_score(m5_pred, y_test))\nprint(\"auc score: \", roc_auc_score(m5_pred, y_test))\nprint(\"recall score : \", recall_score(m5_pred, y_test))\nprint(\"confusion matrix\", confusion_matrix(m5_pred, y_test))\nprint(\"\\n\", \"*\" * 30)\n\n#naive bayes\nfrom sklearn.naive_bayes import BernoulliNB\nm6 = BernoulliNB()\nm6.fit(x_train, y_train)\n\nm6_pred = m6.predict(x_test)\nm6_pred\nprint(\"naive bayes\", \"\\n\")\nprint(\"accuracy score :\", accuracy_score(m6_pred, y_test))\nprint(\"precision score: \", precision_score(m6_pred, y_test))\nprint(\"f1 score: \", f1_score(m6_pred, y_test))\nprint(\"auc score: \", roc_auc_score(m6_pred, y_test))\nprint(\"recall score : \", recall_score(m6_pred, y_test))\nprint(\"confusion matrix\", confusion_matrix(m6_pred, y_test))\nprint(\"\\n\", \"*\" * 30)","d40be13e":"#parameter tuning\n#logistic regression\nfrom sklearn.model_selection import GridSearchCV \n\nparams = {'C':[5, 10, 15, 20],'random_state':[0]}\ngrid1 = GridSearchCV(estimator = m1, param_grid = params, scoring = 'accuracy', cv = 10)\ngrid1.fit(x_train, y_train)\nbest_acc = grid1.best_score_\nbest_param = grid1.best_params_\nprint(\"best parameters: \", best_param)\n\nprint('best accuracy:', best_acc*100)\n","7af6d237":"#svc\nparams ={'C':[10, ],'kernel':['linear', 'rbf'],'random_state':[0]}\ngrid2 = GridSearchCV(estimator = m2, param_grid = params , scoring = \"accuracy\", cv = 10)\ngrid2.fit(x_train, y_train)\nbest_acc = grid2.best_score_\nparam = grid2.best_params_\nprint(\"best accuracy :\", best_acc*100)\nprint(\"best parameters :\", param )","c180c0ef":"#random forest\nparams = {\"n_estimators\": [100, 200, 300], \"criterion\": [\"gini\", \"entropy\"],\"random_state\":[42] }\ngrid3= GridSearchCV(estimator = m3, param_grid = params , scoring = \"accuracy\", cv = 10)\ngrid3.fit(x_train, y_train)\nbest_Acc = grid3.best_score_\nbest_param = grid3.best_params_\nprint(\"best accuracy :\", best_Acc*100)\nprint(\"best parameters : \", best_param)","3c39c01d":"#knn \nparams = {\n    'n_neighbors' : [5, 25],\n    'weights': ['uniform', 'distance'],\n    'algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute']}\ngrid4 = GridSearchCV(estimator = m4 , param_grid = params , scoring = \"accuracy\", cv = 10)\ngrid4.fit(x_train, y_train)\nbest_Acc = grid4.best_score_\nbest_param = grid4.best_params_\nprint(\"best parameters: \", best_param)\nprint(\"best accuracy :\", best_Acc*100)   ","a8a6d1cb":"#decison\nparams = {\"criterion\": [\"gini\", \"entropy\"], \"random_state\": [0] } \ngrid5 = GridSearchCV(estimator = m5, param_grid = params, scoring = \"accuracy\", cv = 10)\ngrid5.fit(x_train, y_train)\nbest_acc = grid5.best_score_\nbest_param = grid5.best_params_\nprint(\"best acuracy: \", best_acc*100)\nprint(\"best parameters : \", best_param )\n","6011d8bb":"#bernoulli naive bayes\nparams = {'alpha': [0.25, 0.5, 1]}\ngrid6 = GridSearchCV(estimator = m6, param_grid = params , scoring = \"accuracy\", cv = 10)\ngrid6.fit(x_train, y_train)\nbest_acc = grid6.best_score_\nparam = grid6.best_params_\nprint(\"best accuracy :\", best_acc*100)\nprint(\"best parameters :\", param )","f7b88cc6":"#selecting random forest and decision tree as it shows the highest accuracies\n#fiiting random forest model\nmodel1 =  RandomForestClassifier(criterion = 'entropy', n_estimators = 300, random_state = 42)\nmodel1.fit(x_train, y_train)\n\nmodel_pred1 = model1.predict(x_test)\nmodel_pred1\nprint(\"accuracy score : \", accuracy_score(model_pred1, y_test))\nprint(\"precision score:\", precision_score(model_pred1, y_test))\nprint(\"recall score: \", recall_score(model_pred1, y_test))\nprint(\"f1_score :\", f1_score(model_pred1, y_test))\nprint(\"auc score : \", roc_auc_score(model_pred1, y_test))\nprint(\"confusion matrix\", confusion_matrix(model_pred1, y_test))","f2867cfb":"#fitting decision tree\nmodel2 =  DecisionTreeClassifier(criterion = 'gini', random_state = 0)\nmodel2.fit(x_train, y_train)\n\nmodel_pred2 = model2.predict(x_test)\nmodel_pred2\nprint(\"accuracy score : \", accuracy_score(model_pred2, y_test))\nprint(\"precision score:\", precision_score(model_pred2, y_test))\nprint(\"recall score: \", recall_score(model_pred2, y_test))\nprint(\"f1_score :\", f1_score(model_pred2, y_test))\nprint(\"auc score : \",  roc_auc_score(model_pred2, y_test))\nprint(\"confusion matrix\", confusion_matrix(model_pred2, y_test))","eb05d77d":"\n#Visualizing Confusion Matrix\ncm = confusion_matrix(model_pred1, y_test)\nplt.figure(figsize = (8, 5))\nsns.heatmap(cm, cmap = 'Blues',annot = True, annot_kws= {'Fontsize': 15},cbar = False,  yticklabels = [\"NOT PLACED \", \"PLACED\"], xticklabels = ['predicted NOT PLACED', 'predicted PLACED'])\nplt.yticks(rotation= 0)\nplt.show()","375f6821":"#SHOWING ROU_AUC_CURVE\n# Roc AUC Curve\nfalse_positive_rate, true_positive_rate, thresholds = roc_curve(y_test, model_pred1)\nroc_auc = auc(false_positive_rate, true_positive_rate)\n\nsns.set_theme(style = 'white')\nplt.figure(figsize = (8, 8))\nplt.plot(false_positive_rate,true_positive_rate, color = '#b01717', label = 'AUC = %0.3f' % roc_auc)\nplt.legend(loc = 'lower right')\nplt.plot([0, 1], [0, 1], linestyle = '--', color = '#174ab0')\nplt.axis('tight')\nplt.ylabel('True Positive Rate')\nplt.xlabel('False Positive Rate')\nplt.legend()\nplt.show()","454724b4":"IN THE ABOVE GRAPH, WE CAN SEE NUMBER OF MALEs ARE MORE THAN NUMBER OF FEMALES","a5b49dd7":"# MODEL SELECTION","3f213f32":"AS DEMONSTRATED, A DECENT AMOUNT OF INDIVIUALS ARE SPECIALIZED IN MARKETING & FINANCE","962cdcb0":"IN THIS, MBA % IN RANGE 52% - 75% ARE HIGLY LIKED TO BE PLACED","10d35036":"HERE, THE INDIVIUALS WHO HOLDS COMMERCE & MANAGEMENT DEGREE ARE LIKELY TO PLACED.","47cfdfcb":"# EXPLORATORY DATA ANALYSIS","de0cc7cb":"HERE, MARKETING & HR ARE LIKELY TO PLACED WITH SALARY 3L PER ANNUM , IT IS FOLLOWED BY MARKETING & FINANCE","2442e181":"after tuning the hyperparameters we can see that, random forest shows the 86% accuracy. \n**Random Forest** model fits the best for this dataset","40f30631":"# TUNING PARAMETER","9b7f29c4":"MARKETING & FINANCE ARE MORE LIKELY TO BE PLACED.","0f7b4990":"\n# IMPORTING LIBRARIES","5cf96ce4":"AS WE CAN SEE, THE SIGNIFICANT NUMBER OF PEOPLE HAS BEEN GRADUATED THROUGH COMMERCE & MANAGEMENT FIELD, FOLLOWED BY SCIENCE AND TECHNOLOGY AND OTHER STREAMS","e9e5c201":"HERE,THE INDIVIUALS ARE NOT EXPERIENCED THOUGH , THEY ARE LIKELY TO BE PLACED.","d538e82f":"IN THIS GRAPH , MAXIMUM NUMBER OF PEOPLE HAS SCORED IN RANGE 62%- 65% IN THEIR DEGREE RESPECTIVELY.","5cf7af99":"OVER HERE, WE CAN SEE PERCENTAGE IN RANGE 62% - 90% ARE LIKELY TO BE PLACED.","c3b24b4c":"# DATA PREPROCESSING","4ed1b3d8":"HERE, MAXIMUM NUMBER OF PEOPLE HAS SCORED IN RANGE 58% - 65% IN THEIR MBA RESPECTIVELY.","796b66b6":"AS DEMONSTRATED, LARGE NUMBER OF PEOPLE DO NOT HAVE WORK EXPERIENCE"}}