{"cell_type":{"d69739a3":"code","5dfcf16e":"code","803d39fc":"code","8af7ca62":"code","f49a4a4a":"code","598030f1":"code","388cda4a":"code","47a7de99":"code","000a0af8":"code","7420b68f":"code","21d2439f":"code","ec70b290":"code","c13d08d0":"code","f07eee1f":"code","11e287a1":"code","799528ef":"code","13e21179":"code","5a271185":"code","0b43e135":"code","d61f26a9":"code","d4804ee4":"code","e8939d9b":"code","3a50ce7a":"code","26686f22":"code","498a5679":"code","a5b55602":"code","efdff2a2":"code","85e04827":"code","120e42f2":"code","d90e275e":"code","e20dab0b":"code","3e492d21":"code","7eb3ee5b":"code","3f5c7fd6":"code","97121658":"code","6b46581c":"code","c4cef18d":"code","4eb169f6":"markdown","ab036d2b":"markdown","608b79d0":"markdown","0a5c92e9":"markdown","bef66bd6":"markdown","98b241e6":"markdown","be9fb144":"markdown","6c6efebb":"markdown","ed24dca6":"markdown","cef1f55b":"markdown","838d06cd":"markdown","b9ed2e59":"markdown","f2ef5a75":"markdown","ece5fd27":"markdown","37af3c8f":"markdown","53c1c7ba":"markdown","b654f7ae":"markdown"},"source":{"d69739a3":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","5dfcf16e":"import warnings\nwarnings.filterwarnings('ignore')\n%config Completer.use_jedi = False # if autocompletion doesnot work in kaggle notebook | hit tab","803d39fc":"# importing the dataset \ndf_train = pd.read_csv('..\/input\/emotions-dataset-for-nlp\/train.txt', header =None, sep =';', names = ['Input','Sentiment'], encoding='utf-8')\ndf_test = pd.read_csv('..\/input\/emotions-dataset-for-nlp\/test.txt', header = None, sep =';', names = ['Input','Sentiment'],encoding='utf-8')\ndf_val=pd.read_csv('..\/input\/emotions-dataset-for-nlp\/val.txt',header=None,sep=';',names=['Input','Sentiment'],encoding='utf-8')","8af7ca62":"df_train.head()","f49a4a4a":"print(df_train.shape, df_test.shape, df_val.shape)","598030f1":"import matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set()\n%config InlineBackend.figure_format = 'retina'\n","388cda4a":"sns.countplot(df_train.Sentiment)\nplt.show()","47a7de99":"df_train['Length'] = df_train.Input.apply(lambda x:len(x))","000a0af8":"plt.plot(df_train.Length)\nplt.show()","7420b68f":"df_train.Length.max() # max length of our text body ","21d2439f":"# i'm using a text preprocessing library for this \n!pip install text_hammer \nimport text_hammer as th","ec70b290":"%%time\n\nfrom tqdm._tqdm_notebook import tqdm_notebook\ntqdm_notebook.pandas()\n\ndef text_preprocessing(df,col_name):\n    column = col_name\n    df[column] = df[column].progress_apply(lambda x:str(x).lower())\n    df[column] = df[column].progress_apply(lambda x: th.cont_exp(x)) #you're -> you are; i'm -> i am\n    df[column] = df[column].progress_apply(lambda x: th.remove_emails(x))\n    df[column] = df[column].progress_apply(lambda x: th.remove_html_tags(x))\n#     df[column] = df[column].progress_apply(lambda x: ps.remove_stopwords(x))  \n# here we can remove stop-words but in this case removing not, and ,can change the meaning of context \n\n    df[column] = df[column].progress_apply(lambda x: th.remove_special_chars(x))\n    df[column] = df[column].progress_apply(lambda x: th.remove_accented_chars(x))\n    df[column] = df[column].progress_apply(lambda x: th.make_base(x)) #ran -> run,\n    return(df)","c13d08d0":"df_cleaned_train = text_preprocessing(df_train, 'Input')","f07eee1f":"df_cleaned_train.head()","11e287a1":"df_cleaned_train['Sentiment']=df_cleaned_train.Sentiment.replace({'joy':0,'anger':1,'love':2,'sadness':3,'fear':4,'surprise':5})\ndf_test['Sentiment']=df_test.Sentiment.replace({'joy':0,'anger':1,'love':2,'sadness':3,'fear':4,'surprise':5})\ndf_val['Sentiment']=df_val.Sentiment.replace({'joy':0,'anger':1,'love':2,'sadness':3,'fear':4,'surprise':5})","799528ef":"from keras.preprocessing.text import Tokenizer","13e21179":"num_words = 10000 # this means 10000 unique words can be taken \ntokenizer=Tokenizer(num_words,lower=True)\ndf_total = pd.concat([df_cleaned_train['Input'], df_test.Input], axis = 0)\ntokenizer.fit_on_texts(df_total)\n","5a271185":"len(tokenizer.word_index) # this is whole unique words in our corpus\n# but we are taking only 10000 words in our model","0b43e135":"from keras.preprocessing.sequence import pad_sequences\n\nX_train=tokenizer.texts_to_sequences(df_cleaned_train['Input']) # this converts texts into some numeric sequences \nX_train_pad=pad_sequences(X_train,maxlen=300,padding='post') # this makes the length of all numeric sequences equal \nX_test = tokenizer.texts_to_sequences(df_test.Input)\nX_test_pad = pad_sequences(X_test, maxlen = 300, padding = 'post')\nX_val = tokenizer.texts_to_sequences(df_val.Input)\nX_val_pad = pad_sequences(X_val, maxlen = 300, padding = 'post')","d61f26a9":"from keras.utils import to_categorical\ny_train = to_categorical(df_cleaned_train.Sentiment.values)\ny_test = to_categorical(df_test.Sentiment.values)\ny_val = to_categorical(df_val.Sentiment.values)\n","d4804ee4":"print(X_train_pad.shape, X_val_pad.shape)","e8939d9b":"import gensim.downloader as api\nglove_gensim  = api.load('glove-wiki-gigaword-100') #100 dimension\n# more dimension means more deep meaning of words but it may take longer time to download ","3a50ce7a":"glove_gensim['cat'].shape[0]","26686f22":"vector_size = 100\ngensim_weight_matrix = np.zeros((num_words ,vector_size))\ngensim_weight_matrix.shape\n\nfor word, index in tokenizer.word_index.items():\n    if index < num_words: # since index starts with zero \n        if word in glove_gensim.wv.vocab:\n            gensim_weight_matrix[index] = glove_gensim[word]\n        else:\n            gensim_weight_matrix[index] = np.zeros(100)","498a5679":"gensim_weight_matrix.shape","a5b55602":"from tensorflow.keras.models import Sequential \nfrom tensorflow.keras.layers import Dense, LSTM, Embedding,Bidirectional\nimport tensorflow\n# tf.compat.v1.keras.layers.CuDNNLSTM\nfrom tensorflow.compat.v1.keras.layers import CuDNNLSTM,CuDNNGRU\nfrom tensorflow.keras.layers import Dropout\n\n","efdff2a2":"EMBEDDING_DIM = 100 # this means the embedding layer will create  a vector in 100 dimension\nmodel = Sequential()\nmodel.add(Embedding(input_dim = num_words,# the whole vocabulary size \n                          output_dim = EMBEDDING_DIM, # vector space dimension\n                          input_length= X_train_pad.shape[1], # max_len of text sequence\n                          weights = [gensim_weight_matrix],trainable = False))\nmodel.add(Dropout(0.2))\nmodel.add(Bidirectional(CuDNNLSTM(100,return_sequences=True)))\nmodel.add(Dropout(0.2))\nmodel.add(Bidirectional(CuDNNLSTM(200,return_sequences=True)))\nmodel.add(Dropout(0.2))\nmodel.add(Bidirectional(CuDNNLSTM(100,return_sequences=False)))\nmodel.add(Dense(6, activation = 'softmax'))\nmodel.compile(loss = 'categorical_crossentropy', optimizer = 'adam',metrics = 'accuracy')","85e04827":"#EarlyStopping and ModelCheckpoint\n\nfrom keras.callbacks import EarlyStopping, ModelCheckpoint\n\n\nes = EarlyStopping(monitor = 'val_loss', mode = 'min', verbose = 1, patience = 5)\nmc = ModelCheckpoint('.\/model.h5', monitor = 'val_accuracy', mode = 'max', verbose = 1, save_best_only = True)","120e42f2":"history_embedding = model.fit(X_train_pad,y_train, epochs = 25, batch_size = 120, validation_data=(X_val_pad, y_val),verbose = 1, callbacks= [es, mc]  )\n","d90e275e":"plt.plot(history_embedding.history['accuracy'],c='b',label='train accuracy')\nplt.plot(history_embedding.history['val_accuracy'],c='r',label='validation accuracy')\nplt.legend(loc='lower right')\nplt.show()","e20dab0b":"model.evaluate(X_test_pad, y_test) ","3e492d21":"y_pred =   np.argmax(model.predict(X_test_pad), axis  =  1)","7eb3ee5b":"y_true = np.argmax(y_test, axis = 1)\n","3f5c7fd6":"from sklearn import metrics\nprint(metrics.classification_report(y_pred, y_true))","97121658":"def get_key(value):\n    dictionary={'joy':0,'anger':1,'love':2,'sadness':3,'fear':4,'surprise':5}\n    for key,val in dictionary.items():\n          if (val==value):\n            return key","6b46581c":"def predict(sentence):\n    sentence_lst=[]\n    sentence_lst.append(sentence)\n    sentence_seq=tokenizer.texts_to_sequences(sentence_lst)\n    sentence_padded=pad_sequences(sentence_seq,maxlen=300,padding='post')\n    ans=get_key(model.predict_classes(sentence_padded))\n    print(\"The emotion predicted is\",ans)","c4cef18d":"predict(str(input('Enter a sentence : ')))\n","4eb169f6":"### now lets do some EDA","ab036d2b":"### Now lets do some text preprocessing ","608b79d0":"Encoding the output values ","0a5c92e9":"### now we have our X_data and y_data is ready now lets create a weight matrix\n### For creating weight matrix we are using gensim word2vec \n#### here we are going to use glove-word2vec since we dont wanna loose the context meaning\n#### lets say ( 'love', 'affection','like') these  words will have almost same meaning if we will use glove-word2vec ","bef66bd6":"### here i am using progress_apply inorder to plot a nice looking bar_plot","98b241e6":"### We are going to learn Bidirectional(LSTM) with GPU Support (Cudnn) and its implementation on Emotion Detection\n* #### These followings things we are going to cover in this notebook\n* Text clearning for NLP tasks\n* feature selection and preprocessing in Text data\n* We will see How to make various plots in case of Text Data\n* we will prepare our data In order to train LSTM\n* we will learn how to use word2vec\n* we will learn how to setup various LSTM layers and their methods \n* we will train and validation our LSTM model\n\n### Pls Follow me and UPvote this notebook to help me to get my First Bronze \ud83d\ude4f\n\n@author -Abhishek Jaiswal | kaggle.com\/preatcher\/emotion-detection-lstm","be9fb144":"### I have created my own library for doing text cleaning and its preprocessinng so i am using that \n","6c6efebb":"### takeout !\n* #### i have improved this accuracy by using BERT State of the Art model in my notebook as well  i encourage you to check my BERT notebook \n* #### This accuracy can be improved by using more data \n* #### Further this is the best accuracy we can get out from using Bidirectional LSTM so next task to checkout my BERT Notebook and implement in the same task\n### thanks for reading if you did like my notebook Pls upvote ","ed24dca6":"#### Stacked Bidirectional LSTM with GPU(CUDNN)","cef1f55b":"Designing a LSTM model with GPU support ","838d06cd":"#### Importing inorder to ignore all warnings","b9ed2e59":"#### now lets test on some real data","f2ef5a75":"#### Converting the text output  into integer (label encoding )","ece5fd27":"Creating a word2Vec weight matrix ","37af3c8f":"# checking our own Sentences ","53c1c7ba":"#### Using The tokenizer Class to convert the sentences into word vectors\n","b654f7ae":"#### loading the dataset "}}