{"cell_type":{"acd3d1e2":"code","cad89a5c":"code","f8f958f3":"code","61519a46":"code","0e68f896":"code","87518bec":"code","0a23c175":"code","fb6f22e5":"code","344f28c4":"code","e2a557f6":"code","a89867ab":"code","33cca42e":"code","dab82d95":"code","cc82a293":"code","58aacaa7":"code","4713bd16":"code","1edfb8b7":"code","af34b1d9":"code","06732a8d":"code","103a5be5":"code","9b52ef90":"code","ba62b718":"code","68abdce9":"code","696f287f":"code","6cd3cdeb":"code","cc36784b":"code","a3a0c1d5":"code","78f17053":"code","93779634":"code","664b6802":"code","4d1ed2e6":"code","04183da5":"markdown","4e9902ec":"markdown","fdbab883":"markdown","73e1fca0":"markdown","7fca6dd3":"markdown","929d2032":"markdown"},"source":{"acd3d1e2":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n!pip install bs4 # for handling html and tags\n!pip install contractions # for handling contractions\n!pip install textsearch\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","cad89a5c":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nimport tensorflow as tf\nfrom tensorflow import keras\n\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MinMaxScaler\n\nfrom tensorflow.keras import layers, Model\nfrom tensorflow.keras.layers import Input, Embedding, Bidirectional, LSTM, GlobalAveragePooling1D, Dense, Dropout \nfrom tensorflow.keras.optimizers import RMSprop, Adam\nfrom tensorflow.keras.callbacks import ReduceLROnPlateau, ModelCheckpoint, EarlyStopping\nimport tensorflow.keras.backend as k\nfrom keras import models","f8f958f3":"print(\"Tensorflow version \" + tf.__version__)","61519a46":"try:\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n    print('Device:', tpu.master())\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\nexcept:\n    strategy = tf.distribute.get_strategy()\nprint('Number of replicas:', strategy.num_replicas_in_sync)","0e68f896":"train_data = pd.read_csv('..\/input\/commonlitreadabilityprize\/train.csv')\ntest_data = pd.read_csv('..\/input\/commonlitreadabilityprize\/test.csv')\nsample_sub = pd.read_csv('..\/input\/commonlitreadabilityprize\/sample_submission.csv')","87518bec":"train_data.head()","0a23c175":"train_data.shape","fb6f22e5":"from bs4 import BeautifulSoup\ndef strip_html_tags(text):\n    soup = BeautifulSoup(text, \"html.parser\")\n    [s.extract() for s in soup(['iframe', 'script'])]\n    \n    stripped_text = soup.get_text()\n    stripped_text = re.sub(r'[\\r|\\n|\\r\\n]+', '\\n', stripped_text)\n    return stripped_text\n\n\nimport unicodedata\ndef remove_accented_chars(text):\n    text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n    return text\n\n\ndef remove_special_chars(text, remove_digits=True):\n    pattern = r'[^a-zA-Z0-9\\s]' if not remove_digits else r'[^a-zA-Z\\s]'\n    text = re.sub(pattern, '', text)\n    return text\n\nfrom nltk.corpus import stopwords\n', '.join(stopwords.words('english'))\ndef remove_stopwords(text):\n    return ' '.join([word for word in str(text).split() if word not in STOPWORDS])","344f28c4":"import contractions\ntrain_data['excerpt'] = train_data['excerpt'].apply(contractions.fix)\ntest_data['excerpt'] = test_data['excerpt'].apply(contractions.fix)","e2a557f6":"from nltk.stem import PorterStemmer\ntrain_data['excerpt'] = train_data['excerpt'].apply(PorterStemmer().stem)\ntest_data['excerpt'] = test_data['excerpt'].apply(PorterStemmer().stem)\n\n\nfrom nltk.stem import WordNetLemmatizer\ntrain_data['excerpt'] = train_data['excerpt'].apply(WordNetLemmatizer().lemmatize)\ntest_data['excerpt'] = test_data['excerpt'].apply(WordNetLemmatizer().lemmatize)","a89867ab":"train_data.head()","33cca42e":"maxlen_ = 200\nmax_words = 20000\n\ntokenizer = Tokenizer(num_words = max_words)\ntokenizer.fit_on_texts(train_data['excerpt'])\nsequences = tokenizer.texts_to_sequences(train_data['excerpt'])\ntrain_data_preped = pad_sequences(sequences, maxlen=maxlen_, padding='post')\nword_index = tokenizer.word_index","dab82d95":"train_data_preped","cc82a293":"train_data_preped.shape","58aacaa7":"train_data['target'].shape","4713bd16":"X_train, X_val, y_train, y_val = train_test_split(train_data_preped,\n                                                  train_data['target'],\n                                                  test_size=0.15)\nprint('Size of Train: ',X_train.shape)\nprint('Size of Validation: ',X_val.shape)","1edfb8b7":"mm = MinMaxScaler()\nX_train = mm.fit_transform(X_train)\nX_val = mm.transform(X_val)","af34b1d9":"BATCH_SIZE = 32 * strategy.num_replicas_in_sync\n\ndef rmse(y_true, y_pred):\n        return k.sqrt(k.mean(k.square(y_pred - y_true)))\n    \n# Callbacks\nrop = ReduceLROnPlateau(min_lr=0.00000001, patience=5)\nmc = ModelCheckpoint('model1.h5', save_freq='epoch')\n\n# Earlystopping\nearly_stopping = EarlyStopping(patience=10, monitor='val_loss')","06732a8d":"with strategy.scope(): \n    inp = Input(maxlen_)\n    x = Embedding(max_words, 300)(inp)\n    x.trainable = True\n    x = Dropout(0.4)(x)\n    x = Bidirectional(LSTM(512,return_sequences=True))(x)\n    x = Bidirectional(LSTM(1024,return_sequences=True))(x)\n    x = GlobalAveragePooling1D()(x)\n    x = Dense(1024,activation='relu')(x)\n    x = Dropout(0.4)(x)\n    x = Dense(512,activation='relu')(x)\n    out = Dense(1, activation='linear')(x)\n    \n    model = Model(inp,out)\n    \n    model.compile(loss=rmse, optimizer=RMSprop(0.01))","103a5be5":"model.summary()","9b52ef90":"history = model.fit(X_train, y_train,\n                    batch_size = BATCH_SIZE, \n                    epochs=100,\n                    validation_data = (X_val, y_val), \n                    callbacks=[rop, mc, early_stopping])","ba62b718":"# loss = history.history['loss']\n# val_loss = history.history['val_loss']\n# epochs = range(1, len(loss) + 1)\n\n# plt.figure(figsize=(10,5))\n# plt.plot(epochs, loss, 'bo', label='Training loss')\n# plt.plot(epochs, val_loss, 'b', label='Validation loss')\n# plt.title('Training and validation loss')\n# plt.xlabel('Epochs')\n# plt.ylabel('Loss')\n# plt.legend()\n# plt.grid()\n# plt.show()","68abdce9":"test_data = pd.read_csv('..\/input\/commonlitreadabilityprize\/test.csv')\nsample_sub = pd.read_csv('..\/input\/commonlitreadabilityprize\/sample_submission.csv')","696f287f":"test_data.head()","6cd3cdeb":"test_data.shape","cc36784b":"maxlen_ = 200\nmax_words = 20000\n\ntokenizer = Tokenizer(num_words = max_words)\ntokenizer.fit_on_texts(test_data['excerpt'])\nsequences = tokenizer.texts_to_sequences(test_data['excerpt'])\ntest_data_preped = pad_sequences(sequences, maxlen=maxlen_, padding='post')\nword_index = tokenizer.word_index","a3a0c1d5":"test_data.head()","78f17053":"test_datax = mm.transform(test_data_preped)","93779634":"model = models.load_model('model1.h5', custom_objects={'rmse': rmse})\npreds = model.predict(test_datax)","664b6802":"predictions = pd.DataFrame()\npredictions['id'] = test_data['id']\npredictions['target'] = preds","4d1ed2e6":"predictions.to_csv(\"\/kaggle\/working\/submission.csv\", index=False)\npredictions","04183da5":"# Model Building","4e9902ec":"# Loading the data","fdbab883":"# Inference","73e1fca0":"# Preprocessing","7fca6dd3":"# Import Dependencies","929d2032":"# Submission"}}