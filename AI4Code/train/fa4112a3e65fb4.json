{"cell_type":{"e4e63390":"code","644ba83c":"code","1f52d8e0":"code","9232f532":"code","7266a142":"code","3b943993":"code","40df551f":"code","14af3924":"code","31db44b1":"code","b8086a7f":"markdown","c06fdb75":"markdown"},"source":{"e4e63390":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom sklearn import model_selection\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn import metrics\nfrom sklearn.naive_bayes import GaussianNB\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","644ba83c":"#reading the dataset\npima_indians_df = pd.read_csv(\"\/kaggle\/input\/pima-indians-diabetes-database\/diabetes.csv\")","1f52d8e0":"pima_indians_df.describe().T","9232f532":"pima_indians_df['Outcome'].value_counts()","7266a142":"pima_indians_df['SkinThickness'].value_counts(0)","3b943993":"pima_indians_df['Insulin'].value_counts()","40df551f":"# Data Cleaning\nimport random\n\npima_indians_df['SkinThickness']=pima_indians_df['SkinThickness'].replace(0,random.randrange(30, 40))\n\npima_indians_df['Insulin']=pima_indians_df['Insulin'].replace(0,random.randrange(30, 140))\n\npima_indians_df.head(10)\n\n\n\n","14af3924":"array = pima_indians_df.values\nX = array[:,0:8] # select all rows and first 7 columns which are the attributes\nY = array[:,8]   # select all rows and the 8th column which is the classification \"Yes\", \"No\" for diabeties\ntest_size = 0.15 # taking 70:30 training and test set\nseed = 7  # Random numbmer seeding for reapeatability of the code\nX_train, X_test, Y_train, Y_test = model_selection.train_test_split(X, Y, test_size=test_size, random_state=seed)","31db44b1":"\nmodel = GaussianNB()\nmodel.fit(X_train, Y_train)\nprint(model)\n# make predictions\nexpected = Y_test\npredicted = model.predict(X_test)\n# summarize the fit of the model\nprint(metrics.classification_report(expected, predicted))\nprint(metrics.confusion_matrix(expected, predicted))","b8086a7f":"Precision: Within a given set of positively-labeled results, the fraction that were true positives = tp\/(tp + fp)\nRecall: Given a set of positively-labeled results, the fraction of all positives that were retrieved = tp\/(tp + fn)\nAccuracy: tp + tn \/ (tp + tn + fp +fn) But this measure can be dominated by larger class. Suppose 10, 90 and 80 of 90 is correctly predicted while only 2 of 0 is predicted correctly. Accuracy is 80+2 \/ 100 i.e. 82%\n\nTO over come the dominance of the majority class, use weighted measure (not shown)\n\nF is harmonic mean of precision and recal given by ((B^2 +1) PR) \/ (B^2P +R)\nWhen B is set to 1 we get F1 = 2PR \/ (P+R)","c06fdb75":"## Please upvote if you like this kernel for further encoragement"}}