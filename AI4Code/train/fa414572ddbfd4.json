{"cell_type":{"1c605b91":"code","a051f339":"code","3a7027e2":"code","1ef2d2ae":"code","4fdd3b3e":"code","40a1bd7e":"code","236b3bbf":"code","62d51b2c":"code","39a18e28":"code","53c786df":"code","b2ce5298":"code","f4d583cd":"code","33abfd74":"code","4b58753a":"code","8bc21a7a":"code","41acd336":"code","c975904a":"code","fde6af5f":"markdown","9fb5569b":"markdown","15ce4dbc":"markdown","fbb1465e":"markdown","2e56eff7":"markdown","d34d5c89":"markdown","3e5183ca":"markdown","e7e444d1":"markdown","fcbb68cd":"markdown","e598e10d":"markdown","4ad4e234":"markdown","15060d67":"markdown"},"source":{"1c605b91":"!pip install transformers\n\nimport torch\nfrom transformers import BertForQuestionAnswering\nfrom transformers import BertTokenizer\n\nbert_model = BertForQuestionAnswering.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')\n#bert_model.save_weights(\"bert_model.h5\")\nimport pickle\npkl_filename = \"bert_model.pkl\"\nwith open(pkl_filename, 'wb') as file:\n    pickle.dump(bert_model, file)\n\nbert_tokenizer = BertTokenizer.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')\n#tokenizer.save_weights(\"tokenizer.h5\")\nimport pickle\npkl_filename = \"bert_tokenizer.pkl\"\nwith open(pkl_filename, 'wb') as file:\n    pickle.dump(bert_tokenizer, file)\n","a051f339":"\nimport os\n#%%capture\n!curl -O https:\/\/download.java.net\/java\/GA\/jdk11\/9\/GPL\/openjdk-11.0.2_linux-x64_bin.tar.gz\n!mv openjdk-11.0.2_linux-x64_bin.tar.gz \/usr\/lib\/jvm\/; cd \/usr\/lib\/jvm\/; tar -zxvf openjdk-11.0.2_linux-x64_bin.tar.gz\n!update-alternatives --install \/usr\/bin\/java java \/usr\/lib\/jvm\/jdk-11.0.2\/bin\/java 1\n!update-alternatives --set java \/usr\/lib\/jvm\/jdk-11.0.2\/bin\/java\nos.environ[\"JAVA_HOME\"] = \"\/usr\/lib\/jvm\/jdk-11.0.2\"","3a7027e2":"#%%capture\n!pip install pyserini==0.8.1.0\nfrom pyserini.search import pysearch","1ef2d2ae":"#%%capture\n!wget -O lucene.tar.gz https:\/\/www.dropbox.com\/s\/j55t617yhvmegy8\/lucene-index-covid-2020-04-10.tar.gz?dl=0\n!tar xvfz lucene.tar.gz\nminDate = '2020\/04\/10'  \nluceneDir = '\/kaggle\/working\/lucene-index-covid-2020-04-10\/'","4fdd3b3e":"from IPython.core.display import display, HTML\nimport json\ndef show_query(query):\n    \"\"\"HTML print format for the searched query\"\"\"\n    return HTML('<br\/><div style=\"font-family: Times New Roman; font-size: 20px;'\n                'padding-bottom:12px\"><b>Query<\/b>: '+query+'<\/div>')\n\ndef show_document(idx, doc):\n    \"\"\"HTML print format for document fields\"\"\"\n    have_body_text = 'body_text' in json.loads(doc.raw)\n    body_text = ' Full text available.' if have_body_text else ''\n    return HTML('<div style=\"font-family: Times New Roman; font-size: 18px; padding-bottom:10px\">' + \n               f'<b>Document {idx}:<\/b> {doc.docid} ({doc.score:1.2f}) -- ' +\n               f'{doc.lucene_document.get(\"authors\")} et al. ' +\n              f'{doc.lucene_document.get(\"journal\")}. ' +\n              f'{doc.lucene_document.get(\"publish_time\")}. ' +\n               f'{doc.lucene_document.get(\"title\")}. ' +\n               f'<a href=\"https:\/\/doi.org\/{doc.lucene_document.get(\"doi\")}\">{doc.lucene_document.get(\"doi\")}<\/a>.'\n               + f'{body_text}<\/div>')\n\ndef show_query_results(query, searcher, top_k=10):\n    \"\"\"HTML print format for the searched query\"\"\"\n    output_query = searcher.search(query)\n#    print(\"output_query\",output_query)\n    display(show_query(query))\n    for i, k in enumerate(output_query[:top_k]):\n        display(show_document(i+1, k))\n    return output_query[:top_k]   ","40a1bd7e":"import json\ndef query_id(query_result):\n    #print(len(query_result))\n    #print(\"query_result\",query_result)\n    files_list=[]\n    for i in range(len(query_result)):\n      doc_json = json.loads(query_result[i].raw)\n      #print(\"doc_json\",doc_json)\n      paper_id = 'paper_id' in doc_json    \n      if paper_id :\n        #print(doc_json['paper_id'])\n        files_list.append(doc_json)\n        #print(doc_json)\n      #else:\n        #print(doc_json['sha'])\n    #print(\"files_list\",files_list)\n    return files_list","236b3bbf":"import glob\nimport json\nimport pandas as pd\nfrom tqdm import tqdm\nimport re\n#all_json=files_list\n#print(\"all_json type\", type(all_json))\n#print(\"length of json:\",len(all_json))\n\nclass FileReader:\n    def __init__(self, file):        \n #         print(\"file is=\",file)\n          content = file\n #         print(\"content is\", type(content))\n          self.paper_id = content['paper_id']\n          self.abstract = []\n          self.body_text = []\n          self.abstract_section=[]\n          self.body_section=[]\n          # Abstract\n          try:\n              for entry in content['abstract']:\n                  self.abstract.append(entry['text'])\n                  self.abstract_section.append(entry['section'])\n          except KeyError:pass    \n          # Body text\n          for entry in content['body_text']:\n              self.body_text.append(entry['text'])\n              self.body_section.append(entry['section'])\n          self.abstract = '\\n'.join(self.abstract)\n          self.body_text = '\\n'.join(self.body_text)\n          self.abstract_section = '\\n'.join(self.abstract_section)\n          self.body_section = '\\n'.join(self.body_section)\n            \n    def __repr__(self):\n        return f'{self.paper_id}: {self.abstract[:200]}++++++ {self.body_text[:200]}+++++'\n        \n#first_row = FileReader(all_json[0])\n#print(\"first_row===\",first_row)\n\ndef get_breaks(content, length):\n    data = \"\"\n    words = content.split(' ')\n    total_chars = 0\n    # add break every length characters\n    for i in range(len(words)):\n        total_chars += len(words[i])\n        if total_chars > length:\n            data = data + \"<br>\" + words[i]\n            total_chars = 0\n        else:\n            data = data + \" \" + words[i]\n    return data\n\ndef generate_clean_df(files_list):\n    all_json=files_list\n    dict_ = {'paper_id': [], 'abstract': [], 'body_text': [],'body_section':[],'abstract_section':[], 'authors': [], 'title': [], 'journal': [], 'abstract_summary': [],'source_x':[],'publish_time':[]}\n\n    for idx, entry in enumerate(all_json):\n        content = FileReader(entry) \n        dict_['paper_id'].append(content.paper_id)\n        dict_['abstract'].append(content.abstract)\n        dict_['body_text'].append(content.body_text)\n        dict_['body_section'].append(content.body_section)\n        dict_['abstract_section'].append(content.abstract_section)\n    df_covid = pd.DataFrame(dict_, columns=['paper_id','abstract','abstract_section','body_section','body_text'])\n    #print(df_covid.head())\n    text_dict = df_covid.to_dict()\n    len_text = len(text_dict[\"paper_id\"])\n    paper_id_list  = []\n    body_text_list = []\n    body_section_list =[]\n    section_list =[]\n    for i in tqdm(range(0,len_text)):\n      paper_id = df_covid['paper_id'][i]\n      body_text = df_covid['body_text'][i].split(\"\\n\")\n      body_section = df_covid['body_section'][i].split('\\n')\n      for i in tqdm(range(0,len(body_text))):\n        paper_id_list.append(paper_id)\n        body_text_list.append(body_text[i])\n        body_section_list.append(body_section[i])\n        section_list.append(\"BODY\")\n\n    df_paragraph = pd.DataFrame({\"paper_id\":paper_id_list,\"section\":section_list,\"paragraph\":body_text_list,\"subsection\":body_section_list})\n    df_paragraph.to_csv(\"paragraph.csv\")\n    return df_paragraph\n","62d51b2c":"#files_list=query_id(query_result)\n#cl_df=generate_clean_df(files_list)","39a18e28":"!wget https:\/\/github.com\/facebookresearch\/fastText\/archive\/v0.9.1.zip\n!unzip v0.9.1.zip\n!cd fastText-0.9.1\n","53c786df":"! ls -l\n! pip install \/kaggle\/working\/fastText-0.9.1\/.","b2ce5298":"! git clone https:\/\/github.com\/epfml\/sent2vec.git\n#! cd \/content\/sent2vec\/\n! pip install \/kaggle\/working\/sent2vec\/.","f4d583cd":"pkl_filename = \"\/kaggle\/working\/bert_model.pkl\"\nwith open(pkl_filename, 'rb') as file:\n    bert_model = pickle.load(file)\n    \npkl_filename = \"\/kaggle\/working\/bert_tokenizer.pkl\"\nwith open(pkl_filename, 'rb') as file:\n    tokenizer = pickle.load(file)\n","33abfd74":"import numpy as np\ndef bertsquadpred(bert_model, text, query):\n    input_ids = tokenizer.encode(query, text)\n    tokens = tokenizer.convert_ids_to_tokens(input_ids)\n    sep_index = input_ids.index(tokenizer.sep_token_id)\n    num_seg_a = sep_index + 1\n    num_seg_b = len(input_ids) - num_seg_a\n    segment_ids = [0]*num_seg_a + [1]*num_seg_b\n    assert len(segment_ids) == len(input_ids)\n    n_ids = len(segment_ids)\n    #print(\"n_ids\",n_ids)\n    #print(n_ids)\n    if n_ids < 512:\n        start_scores, end_scores = bert_model(torch.tensor([input_ids]), \n                                 token_type_ids=torch.tensor([segment_ids]))\n    else:        \n        start_scores, end_scores = bert_model(torch.tensor([input_ids[:512]]), \n                                 token_type_ids=torch.tensor([segment_ids[:512]]))\n    #print(\"start_scores\",start_scores)\n    #print(\"end_scores\",end_scores)\n    start_scores = start_scores[:,1:-1]\n    end_scores = end_scores[:,1:-1]\n    answer_start = torch.argmax(start_scores)\n    answer_end = torch.argmax(end_scores)\n    #print(\"answer_start, answer_end\",answer_start, answer_end)\n    answer = ''\n    \n    t_count = 0    \n    for i in range(answer_start, answer_end + 2):\n        if tokens[i] == '[SEP]' or tokens[i] == '[CLS]':\n            continue\n        if tokens[i][0:2] == '##':\n            answer += tokens[i][2:]\n        else:\n            if t_count == 0:\n                answer +=  tokens[i]\n            else:\n                answer += ' ' + tokens[i]\n        t_count+=1\n            \n    full_txt = ''\n    for t in tokens:\n        if t[0:2] == '##':\n            full_txt += t[2:]\n        else:\n            full_txt += ' ' + t\n            \n    abs_returned = full_txt.split('[SEP]')[1]\n            \n    #print(abs_returned)\n    ans={}\n    ans['answer'] = answer\n    #print(answer)\n    if answer.startswith('[CLS]') or answer_end.item() < sep_index or answer.endswith('[SEP]'):\n        ans['confidence'] = -1.0\n    else:\n        confidence = torch.max(start_scores) + torch.max(end_scores)\n        confidence = np.log(confidence.item())\n        ans['confidence'] = confidence\/(1.0+confidence)\n    ans['start'] = answer_start\n    ans['end'] = answer_end\n    ans['paragraph_bert'] = abs_returned\n    return ans","4b58753a":"import sent2vec\n#from nltk import word_tokenize\n#from nltk.corpus import stopwords\nfrom string import punctuation\nfrom scipy.spatial import distance\n\n#model_path = \"\/content\/BioSentVec_PubMed_MIMICIII-bigram_d700.bin\"\n#model_path = \"\/kaggle\/input\/biosentvec\/BioSentVec_CORD19-bigram_d700.bin\"\nmodel_path = \"\/kaggle\/input\/covid-sent2vec-ver2\/BioSentVec_CORD19-bigram_d700_v2.bin\"\n#model_path = \"\/content\/BioSentVec_CORD19-bigram_d700.bin\"\nmodel = sent2vec.Sent2vecModel()\ntry:\n    model.load_model(model_path)\nexcept Exception as e:\n    print(e)\nprint('model successfully loaded')","8bc21a7a":"import warnings\nwarnings.filterwarnings(\"ignore\")\n#from IPython.display import Markdown, display\nfrom IPython.display import Markdown\ndef test_query(cl_df):\n    para_vector_dict = {}\n    subsection_vector_dict={}\n    para_list = []\n    #data=[]\n    f_data=[]\n    count = 0\n    n_return = 2\n    for id, group in cl_df.groupby(['paper_id']):\n        para = group[\"paragraph\"].values\n        paper_id1=group[\"paper_id\"].values\n        paper_id=group[\"paper_id\"].values\n        paper_id=paper_id.tolist()       \n        #print(\"\\n paper_id\",set(paper_id)) \n        #display(Markdown(\"\"\"#Paper_id\"\"\"#Paper_id:\",set(paper_id)))\n        subsection = group[\"subsection\"].values\n        paras = [p for p in para if isinstance(p,str)]\n        subsections = [s for s in subsection if isinstance (s, str)]\n        \n        paras_count = len(paras)\n        #print(paras_count)    \n        if paras_count==0:\n            continue\n        #paras_text = \" \".join(paras)\n        paragraph_dict={}\n        paragraph_list=[]\n        k=0\n        for sub_para in paras:\n          paragraph_dict[k]=model.embed_sentence(sub_para)\n          paragraph_list.append(sub_para)\n          k+=1    \n        \n        keys = list(paragraph_dict.keys())\n        \n        vectors = np.array(list(paragraph_dict.values()))\n        #print(\"vectors are\",vectors)\n\n        nsamples, nx, ny = vectors.shape\n        para_vectors = vectors.reshape((nsamples,nx*ny))\n\n        from scipy.spatial import distance\n        from sklearn.metrics.pairwise import cosine_similarity\n\n        para_matrix_query = cosine_similarity(para_vectors, query_vector.reshape(1,-1))\n        para_indexes = np.argsort(para_matrix_query.reshape(1,-1)[0])[::-1][:5]        \n        short_listed_paras = [paragraph_list[i] for i in para_indexes]        \n        subsection_head=[subsections[i] for i in para_indexes]\n        \n        #display(Markdown(\"query:\",query))\n        \n        from itertools import chain \n        answer_squad_list=[]\n        answer_squad_dict={}\n        for sub_head,sub_para in zip(subsection_head,short_listed_paras):         \n            answer_vector = model.embed_sentence(sub_para)\n            cosine_sim = 1 - distance.cosine(query_vector, answer_vector)            \n            answer_squad = bertsquadpred(bert_model, sub_para, query)\n            answer_squad_list.append(answer_squad['answer'])\n        \n        display(Markdown(\"\"\"Paper_id:\"\"\"+str(set(paper_id))+\" \"))\n        #display(Markdown(\"\"\"Paper_id:\"\"\"+group['paper_id'].unique()+\" \"))\n        display(Markdown(\"\"\"Query   :\"\"\"+query))\n        display(Markdown(\"\"\"Answer  :\"\"\"+\" \".join(answer_squad_list)))\n        #print(\"-\"*90)\n        #data={'paper_id':group['paper_id'].unique(),'answer':\" \".join(answer_squad_list)}\n        #data={'paper_id':(group['paper_id'].nunique()),'subsection_consider':\",\".join(subsection_head),'answer':\" \".join(answer_squad_list)}\n        #data={'paper_id':set(paper_id),'subsection_consider':\",\".join(subsection_head),'answer':\" \".join(answer_squad_list)}\n        #f_data.append(data)dispgroup['paper_id'].unique()lay(Markdown(\"\"\"Paper_id:\"\"\"+str(set(paper_id))+\" \"))\n    #df = pd.DataFrame(f_data) \n    #print(df)\n    #display(Markdown(\"---Answer---\"))\n    #display(df[['paper_id', 'answer']])\n    #df.to_csv(\"\/kaggle\/working\/t.csv\", index=False)\n    #print(df)                                                                                                              \n                                                                                                                  \n        \n    ","41acd336":"query_list=[\n\"What do we know about diagnostics and surveillance\",\n\"What has been published concerning systematic, holistic approach to diagnostics (from the public health surveillance perspective to being able to predict clinical outcomes)?\"\n\"How widespread current exposure is to be able to make immediate policy recommendations on mitigation measures\",\n\"Denominators for testing and a mechanism for rapidly sharing that information, including demographics, to the extent possible.\",\n\"Efforts to increase capacity on existing diagnostic platforms and tap into existing surveillance platforms.\"\n\"Recruitment, support, and coordination of local expertise and capacity (public, private\u2014commercial, and non-profit, including academic), including legal, ethical, communications, and operational issues.\",\n\"National guidance and guidelines about best practices to states (e.g., how states might leverage universities and private laboratories for testing purposes, communications to public health officials and the public)\",\n\"Development of a point-of-care test (like a rapid influenza test) and rapid bed-side tests, recognizing the tradeoffs between speed, accessibility, and accuracy.\",\n\"Rapid design and execution of targeted surveillance experiments calling for all potential testers using PCR in a defined area to start testing and report to a specific entity.\",\n\"Separation of assay development issues from instruments, and the role of the private sector to help quickly migrate assays onto those devices.\",\n\"Efforts to track the evolution of the virus (i.e., genetic drift or mutations) and avoid locking into specific reagents and surveillance\/detection schemes\",\n\"Latency issues and when there is sufficient viral load to detect the pathogen, and understanding of what is needed in terms of biological and environmental sampling.\",\n\"Policies and protocols for screening and testing\",\n\"Policies to mitigate the effects on supplies associated with mass testing, including swabs and reagents.\",\n\"Technology roadmap for diagnostics\",\n\"New platforms and technology (e.g., CRISPR) to improve response times and employ more holistic approaches to COVID-19 and future diseases.\",\n\"Coupling genomics and diagnostic testing on a large scale.\"\n\"Enhance capabilities for rapid sequencing and bioinformatics to target regions of the genome that will allow specificity for a particular variant.\",\n\"Enhance capacity (people, technology, data) for sequencing with advanced analytics for unknown pathogens, and explore capabilities for distinguishing naturally-occurring pathogens from intentional.\",\n\"One Health surveillance of humans and potential sources of future spillover or ongoing exposure for this organism and future pathogens, including both evolutionary hosts (e.g., bats) and transmission hosts (e.g., heavily trafficked and farmed wildlife and domestic food and companion species), inclusive of environmental, demographic, and occupational risk factors.\"]","c975904a":"for query in query_list:\n    from pyserini.search import pysearch\n    searcher = pysearch.SimpleSearcher(luceneDir)\n    query_result = show_query_results(query, searcher, top_k=10)\n    files_list=query_id(query_result)\n    cl_df=generate_clean_df(files_list)\n    query_vector = model.embed_sentence(query)\n    test_query(cl_df)","fde6af5f":"Extracting the Paper_id of the information of the top searched results","9fb5569b":"Loaded Bert For Question and Answer pre-trained model and fucntion to extract answer","15ce4dbc":"Downloading pyserini for searching in Lucene database\n","fbb1465e":"Setting up Java to use Lucene database","2e56eff7":"Extracting relevant pargraphs from the selected documents which are relevant to the question and extracting appropriate answer from the paragraphs. ","d34d5c89":"Downloading latest Lucene index on Covid database","3e5183ca":"Importing Sent2vec pre-trained model on Cord 19","e7e444d1":"Extracting the data from json files for appropriate Top 10 documents from Lucene database","fcbb68cd":"Downloading Pre-trained model of Bert for Question Answering ","e598e10d":"Note: This work is highly inspired from few other kaggle kernels , github sources and other data science resources. Any traces of replications, which may appear , is purely co-incidental. Due respect & credit to all my fellow kagglers. Thanks !!\n\nAnd this notebook is WIP","4ad4e234":"# **About Us:**\nWe are a group of AI and NLP scientists with experience across NLP, image processing and computer vision. Covid-19 Kaggle challenge has provided us with a unique opportunity to help humanity fight the corona virus pandemic collectively by utilizing benefits of AI and NLP. We have focused on creating NLP solution to enable users to ask questions and get the most accurate results from the vast corpus of medical journals.\n","15060d67":"![Lucene.png](attachment:Lucene.png)"}}