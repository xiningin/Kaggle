{"cell_type":{"568b60bf":"code","b0133c0e":"code","128e0445":"code","1ca795e8":"code","72e8c9b0":"code","03bd27ea":"code","30603ae0":"code","9bd89751":"code","198d3830":"code","26314f22":"code","2c132a96":"code","2947e162":"code","c4f5cd93":"code","c07b697b":"code","ad8d799a":"code","a4ac1472":"code","7814663a":"code","e5ac541e":"code","3c61017a":"code","3cc8faf2":"markdown","876f8433":"markdown","64975ab7":"markdown","0c5ab365":"markdown","01cf465c":"markdown","210c2889":"markdown","d0ef2987":"markdown"},"source":{"568b60bf":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","b0133c0e":"dataset = pd.read_csv('\/kaggle\/input\/spam-mails-dataset\/spam_ham_dataset.csv')","128e0445":"dataset.columns","1ca795e8":"dataset.head(5)","72e8c9b0":"dataset.shape","03bd27ea":"dataset.value_counts('label_num')","30603ae0":"dataset.value_counts('Unnamed: 0')","9bd89751":"dataset.drop('Unnamed: 0',axis=1,inplace=True)","198d3830":"dataset.drop('label',axis=1,inplace=True)","26314f22":"dataset.head(5)","2c132a96":"import re\ndef preprocess(text):\n    text = text.replace('\\r',' ')\n    text = text.replace('\\n',' ')\n    text = text.replace('#',' ')\n    text = text.replace(\"we ' re\",\"we are\")\n    text = text.replace(\"they ' re\",\"they are\")\n    text = text.replace(\"you ' re\",\"you are\")\n    text = text.replace(\"Subject:\",\" \")\n\n    return text","2947e162":"dataset['text_clean']=dataset['text'].map(preprocess)","c4f5cd93":"dataset.head(5)","c07b697b":"from sklearn.feature_extraction.text import CountVectorizer\n\n\ncount_vectorizer = CountVectorizer(ngram_range=(1,1))\nX = count_vectorizer.fit_transform(dataset['text_clean'])\nprint(X.shape)","ad8d799a":"from sklearn.model_selection import train_test_split\n\nx,x_test,y,y_test = train_test_split(X,dataset['label_num'],test_size=0.2,random_state=42)\nx_train,x_cv,y_train,y_cv = train_test_split(x,y,test_size=0.2,random_state=42)","a4ac1472":"print((x_train).shape)","7814663a":"import numpy as np\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import accuracy_score\nerror=[]\nfrom sklearn.neighbors import KNeighborsClassifier\nfor i in range(1,20):\n    neigh = KNeighborsClassifier(n_neighbors=i)\n    neigh.fit(x_train,y_train)\n    y_pred = neigh.predict(x_cv)\n    print(classification_report(y_cv, y_pred))\n    print(confusion_matrix(y_cv, y_pred))\n    print()\n    print()\n    print(accuracy_score(y_cv, y_pred))\n    print(\"*\"*100)\n    \n    ","e5ac541e":"neigh = KNeighborsClassifier(n_neighbors=1)\nneigh.fit(x_train,y_train)\ny_pred = neigh.predict(x_test)\nprint(classification_report(y_test, y_pred))\nprint(confusion_matrix(y_test, y_pred))\nprint(accuracy_score(y_test, y_pred))","3c61017a":"from sklearn.naive_bayes import GaussianNB\n\ngnb = GaussianNB()\ny_pred = gnb.fit(x.toarray(), y).predict(x_test.toarray())\nprint(classification_report(y_test, y_pred))\nprint(confusion_matrix(y_test, y_pred))\nprint(accuracy_score(y_test, y_pred))\n","3cc8faf2":"88 percent accuracy when we use k=1","876f8433":"Imbalanced data","64975ab7":"Converting text to","0c5ab365":"Since we cannot use Unnamed for classification we are dropping it","01cf465c":" When using naive bayes we get an accuracy of 95.4% f1 score =0.97","210c2889":"We can have either label or label_num \n\nBy observing the top 5 datapoints in dataset\n\nif label is ham; label_num is 0 \n\nif label is spam; label_num is 1","d0ef2987":"Converting text_clean to vector"}}