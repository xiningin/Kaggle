{"cell_type":{"19b92984":"code","aa4abf8e":"code","3e3e8036":"code","2f16f83f":"code","bd83bec9":"code","64deef86":"code","8d29686b":"code","856ae7cc":"code","7e9b17af":"code","d48870e8":"code","1f621e14":"code","1ff53656":"code","39f36e9e":"code","f7c668c4":"code","9088557d":"code","20098e67":"markdown","a9c34530":"markdown","5a84d704":"markdown","cd83b820":"markdown","7f6bc5c7":"markdown","2a262028":"markdown","21bc9485":"markdown","89c6ecdc":"markdown","6cfb0e44":"markdown","d0f49ec9":"markdown"},"source":{"19b92984":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n## Adding in some more useful packages here\nimport matplotlib.pyplot as plt\n\nfrom wordcloud import WordCloud\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))","aa4abf8e":"train = pd.read_csv('..\/input\/train.csv')","3e3e8036":"train.head()","2f16f83f":"fig,ax = plt.subplots(1,1)\ntrain.hist(column = 'target', ax = ax)\nax.set_title('Number of entries classified as sincere vs insincere')\nax.set_xticks([0,1])\nprint('Percent of insincere entries %.3f %%'%(100*(sum(train['target'])\/len(train))))\n\ntrain[train['target']==1].head()","bd83bec9":"n_posts = 1000\nq_S = ' '.join(train[train['target'] == 0]['question_text'].str.lower().values[:n_posts])\nq_I = ' '.join(train[train['target'] == 1]['question_text'].str.lower().values[:n_posts])\n\nwordcloud_S = WordCloud(max_font_size=None, stopwords=stop,scale = 2,colormap = 'Dark2').generate(q_S)\nwordcloud_I = WordCloud(max_font_size=None, stopwords=stop,scale = 2,colormap = 'Dark2').generate(q_I)\n\nfig, ax = plt.subplots(1,2, figsize=(20, 5))\nax[0].imshow(wordcloud_S)\nax[0].set_title('Top words sincere posts',fontsize = 20)\nax[0].axis(\"off\")\n\nax[1].imshow(wordcloud_I)\nax[1].set_title('Top words INsincere posts',fontsize = 20)\nax[1].axis(\"off\")\n\nplt.show()","64deef86":"embeddings_index = {}\nf = open('\/kaggle\/input\/embeddings\/glove.840B.300d\/glove.840B.300d.txt')\nfor line in f:\n    values = line.split(' ')\n    word = values[0] ## The first entry is the word\n    coefs = np.asarray(values[1:], dtype='float32') ## These are the vecotrs representing the embedding for the word\n    embeddings_index[word] = coefs\nf.close()\n\nprint('GloVe data loaded')","8d29686b":"import re\n\n## Iterate over the data to preprocess by removing stopwords\nlines_without_stopwords=[] \nfor line in train['question_text'].values: \n    line = line.lower()\n    line_by_words = re.findall(r'(?:\\w+)', line, flags = re.UNICODE) # remove punctuation ans split\n    new_line=[]\n    for word in line_by_words:\n        if word not in stop:\n            new_line.append(word)\n    lines_without_stopwords.append(new_line)\ntexts = lines_without_stopwords\n\nprint(texts[0:5])","856ae7cc":"## Code adapted from (https:\/\/github.com\/keras-team\/keras\/blob\/master\/examples\/pretrained_word_embeddings.py)\n# Vectorize the text samples\n\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.utils import to_categorical\n\nMAX_NUM_WORDS = 1000\nMAX_SEQUENCE_LENGTH = 100\ntokenizer = Tokenizer(num_words=MAX_NUM_WORDS)\ntokenizer.fit_on_texts(texts)\nsequences = tokenizer.texts_to_sequences(texts)\n\nword_index = tokenizer.word_index\nprint('Found %s unique tokens.' % len(word_index))\n\ndata = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n\nlabels = to_categorical(np.asarray(train['target']))\nprint(data.shape)\nprint(labels.shape)","7e9b17af":"## More code adapted from the keras reference (https:\/\/github.com\/keras-team\/keras\/blob\/master\/examples\/pretrained_word_embeddings.py)\n# prepare embedding matrix \nfrom keras.layers import Embedding\nfrom keras.initializers import Constant\n\n## EMBEDDING_DIM =  ## seems to need to match the embeddings_index dimension\nEMBEDDING_DIM = embeddings_index.get('a').shape[0]\nnum_words = min(MAX_NUM_WORDS, len(word_index)) + 1\nembedding_matrix = np.zeros((num_words, EMBEDDING_DIM))\nfor word, i in word_index.items():\n    if i > MAX_NUM_WORDS:\n        continue\n    embedding_vector = embeddings_index.get(word) ## This references the loaded embeddings dictionary\n    if embedding_vector is not None:\n        # words not found in embedding index will be all-zeros.\n        embedding_matrix[i] = embedding_vector\n\n# load pre-trained word embeddings into an Embedding layer\n# note that we set trainable = False so as to keep the embeddings fixed\nembedding_layer = Embedding(num_words,\n                            EMBEDDING_DIM,\n                            embeddings_initializer=Constant(embedding_matrix),\n                            input_length=MAX_SEQUENCE_LENGTH,\n                            trainable=False)\n","d48870e8":"## Peeking at the embedding matrix values\nprint(embedding_matrix.shape)\nplt.plot(embedding_matrix[16])\nplt.plot(embedding_matrix[37])\nplt.plot(embedding_matrix[18])\nplt.title('example vectors')","1f621e14":"## Code from: https:\/\/medium.com\/@sabber\/classifying-yelp-review-comments-using-cnn-lstm-and-pre-trained-glove-word-embeddings-part-3-53fcea9a17fa\n## To create and visualize a model\n\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Flatten, LSTM, Conv1D, MaxPooling1D, Dropout, Activation\n\nmodel = Sequential()\nmodel.add(Embedding(num_words, 300, input_length=100, weights= [embedding_matrix], trainable=False))\n\nmodel.add(Dropout(0.2))\nmodel.add(Conv1D(64, 5, activation='relu'))\nmodel.add(MaxPooling1D(pool_size=4))\nmodel.add(LSTM(100))\nmodel.add(Dense(2, activation='sigmoid'))\n\nmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])","1ff53656":"## Fit train data\nprint(labels.shape)\nmodel.fit(data, np.array(labels), validation_split=0.1, epochs = 1)","39f36e9e":"## Model visualization code adapted from: https:\/\/medium.com\/@sabber\/classifying-yelp-review-comments-using-cnn-lstm-and-pre-trained-glove-word-embeddings-part-3-53fcea9a17fa\n\nfrom sklearn.manifold import TSNE\n## Get weights\nembds = model.layers[0].get_weights()[0]\n## Plotting function\n## Visualize words in two dimensions \ntsne_embds = TSNE(n_components=2).fit_transform(embds)\n\nplt.plot(tsne_embds[:,0],tsne_embds[:,1],'.')","f7c668c4":"#test = pd.read_csv('..\/input\/test.csv')\n#test.head()\n#pred = model.predict()\n#pred = np.round(pred)","9088557d":"#df = pd.DataFrame({\"qid\": test_df[\"qid\"], \"prediction\": pred})\n#df.to_csv(\"submission.csv\", index=False)","20098e67":"**Description**\n\nNotebook to load the quora sincere\/insincere questions data set and figure out some initial impressions.\n\nThe dataset contains the questions posted but no information about time of post or possibility to extract other posts made by the same user. So predictions will be made just based on the text content of each single question.\n\nLooking through quora is a guilty pleasure of mine, and I definitely notice the inflamatory fake looking posts. Usually I'm impressed with the time and thought the responders put in to responding about the question, and it's a huge waste of their time. I have to admit, it can be hard to tell some of the sincere but perhaps ill-informed questions from fake ones, so I think this will be a fun thing to look at!\n\nThis is my first quora kernel and a work in progress! I'll go through some simple analysis try to choose one of the suggested word embedding tools and work on incorporating that into the kernel.","a9c34530":"I'll start by loading the training data and taking a look at the first few entries","5a84d704":"There certainly looks to be a difference in the words used in 'insincere' posts, but again many of these words can appear in legitimate questions as well.\n\n____________________________________________________________________________________________________________________________________________________________\n\n**Embeddings** I'll move on to loading and using the embeddings tools.\n\nThere are 4 options with links provided in the dataset description.\nI chose to start with 'glove' This is new stuff for me, so I used https:\/\/blog.keras.io\/using-pre-trained-word-embeddings-in-a-keras-model.html and https:\/\/medium.com\/@japneet121\/word-vectorization-using-glove-76919685ee0b as references and modified their code to work with this dataset (other referenced to be added).\n\nGloVe is feature description dataset built on a large corpus of words that represent words based on their co-occcurence with other words. In the file provided, each line lists one word that is followed by a vector of numbers that represents the word.\n\nFirst we read in the embeddings file into a dictionary - each entry is a word, followed by the vector of numbers to represent its values\n\n\n\n","cd83b820":"* results visualization to be improved..","7f6bc5c7":"Now preprocess the question text\n\nAs with the work cloud, I initially start by working on a subset of the posts.\n6% of 10,000 posts will be 600, so I won't go lower than total of 10k posts.\nCurrent code includes all posts in analysis","2a262028":"Only a little over 6% of the questions are insincere.\n\nAt first glance, in the 5 displayed insincere titles, there some words related to race and politics. Some sincere questions could also have these words but maybe some top words could be used as initial flags for insincere posts that warrant further examination.\n_______________________________________________________________________________________________________________________________\n* **Next step** - start parsing through the word data and find the initial trends in word usage amongst the insincere posts.\nI'll use a word cloud to visualize top words used in the questions. To speed up processing I'll choose just 1000 posts of each category\nI'm using the standard stop words from nltk.corpus (loaded above).","21bc9485":"Now try applying a keras sequential model - this isn't a great model yet - just copied over from the source\n\nI'll update it as I make it better","89c6ecdc":"This dataset contains just 3 values for each of the categories: \n\n    'qid' \n    'question text' \n    'target' value of 0 or 1\n\nThe first thing I see is that all of the entries we see so fat have a target value of 0 - so they are categorized as sincere. \n\nLets see what fraction of the data is labeled insincere (has target value 1) and take a look at some example data that was categorized as insincere.","6cfb0e44":"Apply the model to predict values","d0f49ec9":"The following code uses some tools from keras to 'Tokenize' the questions text - ie assign numbers to every word\n\nlabels get converted to a 2-column array (1,0) for 0 and (0,1) for 1 in the target column.\n\nThis is the first preprocessing step to be able to use the embedding."}}