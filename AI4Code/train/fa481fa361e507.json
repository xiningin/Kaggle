{"cell_type":{"c55d5b74":"code","af69b4e7":"code","30c6fc5b":"code","f52b3342":"code","67350ade":"code","be80e054":"code","03665265":"code","798d5d05":"code","57ffe0db":"code","740fc730":"code","3ffbed50":"code","f9978034":"code","9782c69e":"code","9b6a706f":"code","90625814":"code","66d76288":"code","32e71c47":"code","c2f2d6ae":"code","689142bc":"code","4c048636":"code","3ec52abc":"code","5ade672a":"code","5aa21922":"code","3fab8013":"markdown","2952d94e":"markdown","9f1e8f65":"markdown","a359df8f":"markdown","b69d587c":"markdown","804e5c6c":"markdown"},"source":{"c55d5b74":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","af69b4e7":"from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MinMaxScaler, StandardScaler\nfrom sklearn.metrics import accuracy_score, log_loss\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom itertools import combinations","30c6fc5b":"#read the dataset\ndf = pd.read_csv('..\/input\/pima-indians-diabetes-database\/diabetes.csv')\ndf.head()","f52b3342":"df.info()","67350ade":"df.describe()","be80e054":"# check for na values\ndf[df.isna()].count()","03665265":"# check for zero values\ndf[df == 0].count().sort_values(ascending=False)","798d5d05":"#fill Insulin's zero values\nInsulin_by_outcome = df.groupby(['Outcome']).mean()['Insulin']\n\ndef fixInsulin(insulin, outcome):\n    if insulin == 0:\n        return Insulin_by_outcome[outcome]\n    else:\n        return insulin\n    \ndf['Insulin'] = df.apply(lambda row : fixInsulin(row['Insulin'], row['Outcome']), axis=1)","57ffe0db":"#fill SkinThickness's zero values\nskinThickness_by_outcome = df.groupby(['Outcome']).mean()['SkinThickness']\n\ndef fixSkinThickness(skinThickness, outcome):\n    if skinThickness == 0:\n        return skinThickness_by_outcome[outcome]\n    else:\n        return skinThickness\n    \ndf['SkinThickness'] = df.apply(lambda row : fixSkinThickness(row['SkinThickness'], row['Outcome']), axis=1)","740fc730":"#fill BloodPressure's zero values\n\nbloodPressure_by_outcome = df.groupby(['Outcome']).mean()['BloodPressure']\n\ndef fixBloodPressure(bloodPressure, outcome):\n    if bloodPressure == 0:\n        return bloodPressure_by_outcome[outcome]\n    else:\n        return bloodPressure\n    \ndf['BloodPressure'] = df.apply(lambda row : fixBloodPressure(row['BloodPressure'], row['Outcome']), axis=1)","3ffbed50":"#fill Glucose's zero values\n\nglucose_by_outcome = df.groupby(['Outcome']).mean()['Glucose']\n\ndef fixGlucose(glucose, outcome):\n    if glucose == 0:\n        return glucose_by_outcome[outcome]\n    else:\n        return glucose\n    \ndf['Glucose'] = df.apply(lambda row : fixGlucose(row['Glucose'], row['Outcome']), axis=1)","f9978034":"#fill BMI's zero values\n\nbim_by_outcome = df.groupby(['Outcome']).mean()['BMI']\n\ndef fixBMI(bim, outcome):\n    if bim == 0:\n        return bim_by_outcome[outcome]\n    else:\n        return bim\n    \ndf['BMI'] = df.apply(lambda row : fixBMI(row['BMI'], row['Outcome']), axis=1)","9782c69e":"# try to compute a 'Sex' columns based on pregnancy (if more than zero for sure is a female)\n\ndef getSex(pregnancy):\n    if pregnancy > 0:\n        return 1 #'Female'\n    else:\n        return 0 #'Unknown'\n    \ndf['Sex'] = df.apply(lambda row : getSex(row['Pregnancies']), axis=1)","9b6a706f":"#drop some outliers based on scatterplots (not shown here)\n\ndf = df[df.Glucose > 50]\nprint(len(df))\ndf = df[(df.BloodPressure > 42) | (df.BloodPressure < 116)]\nprint(len(df))\ndf = df[(df.SkinThickness > 5) | (df.SkinThickness < 58)]\nprint(len(df))\ndf = df[df.Insulin < 625]\nprint(len(df))\ndf = df[(df.BMI > 15) | (df.BMI < 55)]\nprint(len(df))\ndf = df[df.DiabetesPedigreeFunction < 55]\nprint(len(df))\ndf = df[df.Age < 70]\nprint(len(df))","90625814":"# add BMI classes as per World Health Organizations\n\nBMI_OMSNutritional_map = {\n-1:(0,18.5), #'Underweight'\n0:(18.5,24.9), #'Normal weight'\n1: (24.9,29.9), #'Pre-obesity'\n2: (29.9,34.9), #'Obesity class I'\n3: (34.9,39.9), #'Obesity class II'\n4: (39.9, 1000) #'Obesity class III'\n}\n\ndef getBMIClass(bmi):\n    bmi_class = -100\n    for limit_index, limit in enumerate(BMI_OMSNutritional_map.values()):\n        if int(bmi) >= limit[0] and int(bmi) < limit[1]: # >= for lower limit tends to assign higher category rather than lower\n            bmi_class = list(BMI_OMSNutritional_map.keys())[limit_index]\n            break\n    if bmi_class == -100:\n        print('Assined -100 class for: %d' %(bmi))\n    return bmi_class\n\ndf['BMI_class'] = df.apply(lambda row : getBMIClass(row['BMI']), axis=1)\nsns.barplot(x=df.BMI_class, y=df.Outcome,data=df);","66d76288":"# There is no documentation about how the 'Glucose' measure is taken in the dataset, anyway I try to consider the following:\n# A blood sugar level less than 140 mg\/dL (7.8 mmol\/L) is normal.\n# A reading between 140 and 199 mg\/dL (7.8 mmol\/L and 11.0 mmol\/L) indicates prediabetes\n# A reading of more than 200 mg\/dL (11.1 mmol\/L) after two hours indicates diabetes.\n\n\nOMS_Glucose_map = {\n0 :(0,140), #'normal'\n1:(140,200), #'prediabetes'\n2: (200,1000) #'diabetes'\n}\n\n\n\ndef getGlucoseClass(glucose):\n    glucose_class = 'None'\n    for limit_index, limit in enumerate(OMS_Glucose_map.values()):\n        if glucose >= limit[0] and glucose < limit[1]: # >= for lower limit tends to assign higher category rather than lower\n            glucose_class = list(OMS_Glucose_map.keys())[limit_index]\n            break\n    return glucose_class\n\ndf['Glucose_Class'] = df.apply(lambda row : getGlucoseClass(row['Glucose']), axis=1)\nsns.barplot(x=df.Glucose_Class, y=df.Outcome,data=df);","32e71c47":"#ideal blood pressure is considered to be between 90\/60mmHg and 120\/80mmHg.\n#high blood pressure is considered to be 140\/90mmHg or higher.\n#low blood pressure is considered to be 90\/60mmHg or lower.\n\nPressure_map = {\n-1:(60,90), #'low'\n0: (90,140), #'ideal'\n1 : (140,1000) #'high'\n}\n\ndef getPressureClass(pressure):\n    pressure_class = -2\n    for limit_index, limit in enumerate(Pressure_map.values()):\n        if pressure >= limit[0] and pressure < limit[1]: # >= for lower limit tends to assign higher category rather than lower\n            pressure_class = list(Pressure_map.keys())[limit_index]\n            break\n    return pressure_class\n\ndf['BloodPressure_Class'] = df.apply(lambda row : getPressureClass(row['BloodPressure']), axis=1)\nsns.barplot(x=df.BloodPressure_Class, y=df.Outcome,data=df);","c2f2d6ae":"# add Insulin classes: 100 and 126 limits are values found on several articles on Internet\n\ndef getInsulinClass(insulin): \n    if insulin >= 100 and insulin <= 126:\n        return 0 #'Normal'\n    else:\n        return 1 #'Abnormal'\n\ndf['Insulin_Class'] = df.apply(lambda row : getInsulinClass(row['Insulin']), axis=1)\nsns.barplot(x=df.Insulin_Class, y=df.Outcome,data=df);","689142bc":"# see which are the most correlated features\ndf.corr()['Outcome'].sort_values(ascending=False)","4c048636":"# prepare the dataset for training and predictions\n\ndf = pd.get_dummies(df)\nX = df.drop(['Outcome'], axis=1)\nY = df.Outcome.values\n\nX_train, X_test, Y_train, Y_test = train_test_split(X, Y, stratify = df.Outcome, random_state=0)\n\nscaler = StandardScaler()\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)","3ec52abc":"# quickly try with RandomForest and all collected features\nregressor = RandomForestClassifier(n_estimators=20)\nregressor.fit(X_train,Y_train)\n\nY_train_pred = regressor.predict(X_train)\nY_test_pred = regressor.predict(X_test)\n\naccuracy_train = accuracy_score(Y_train,Y_train_pred)\naccuracy_test = accuracy_score(Y_test,Y_test_pred)\n\n\nprint('RandomForest Accuracy\\ttrain: %.4f , test: %.4f' %(accuracy_train,accuracy_test))\n\n#try with GradientBoosting and all collected features\n\nregressor = GradientBoostingClassifier(n_estimators=20)\nregressor.fit(X_train,Y_train)\n\nY_train_pred = regressor.predict(X_train)\nY_test_pred = regressor.predict(X_test)\n\naccuracy_train = accuracy_score(Y_train,Y_train_pred)\naccuracy_test = accuracy_score(Y_test,Y_test_pred)\n\n\nprint('GradientBoost Accuracy\\ttrain: %.4f , test: %.4f' %(accuracy_train,accuracy_test))\n\n# See how accuracy already improved","5ade672a":"# The number of feature is not very big: try all possible feature's combinations to get the best performing list of features\n\ncolumns = df.columns\ncolumns = columns.drop('Outcome')\n\nY = df.Outcome.values\n\nbest_test_accuracy = 0\nbest_feature_list = []\n\nfor n_features in range(1,len(columns)):\n    for comb in combinations(columns,n_features):\n        feature_list = [e for e in comb]\n        #print(feature_list)\n        X = df[feature_list]\n        X_train, X_test, Y_train, Y_test = train_test_split(X, Y, stratify = df.Outcome, random_state=0)\n        regressor = GradientBoostingClassifier(n_estimators=60)\n        regressor.fit(X_train,Y_train)\n\n        Y_train_pred = regressor.predict(X_train)\n        Y_test_pred = regressor.predict(X_test)\n\n        accuracy_train = accuracy_score(Y_train,Y_train_pred)\n        accuracy_test = accuracy_score(Y_test,Y_test_pred)\n        if accuracy_test > best_test_accuracy:\n            best_test_accuracy = accuracy_test\n            best_feature_list = feature_list\n            print('GradientBoost Accuracy\\ttrain: %.4f , test: %.4f' %(accuracy_train,accuracy_test))\n            \nprint(\"best feature list: \" + str(feature_list))\n\n\n# It looks like the best feature list for GradientBoosting contains: ['Glucose', 'BloodPressure', 'SkinThickness', 'Insulin',\n# 'BMI', 'DiabetesPedigreeFunction', 'Age', 'Sex', 'BMI_class', 'Glucose_Class', 'BloodPressure_Class', 'Insulin_Class']\n# the accuracy on test set is around 0.91\n\n\n#GradientBoost Accuracy\ttrain: 0.9474 , test: 0.9058\n#GradientBoost Accuracy\ttrain: 0.9474 , test: 0.9110","5aa21922":"# Do the same with RandomForest classifier\ncolumns = df.columns\ncolumns = columns.drop('Outcome')\n\nY = df.Outcome.values\n\nbest_test_accuracy = 0\nbest_feature_list = []\n\nfor n_features in range(1,len(columns)):\n    for comb in combinations(columns,n_features):\n        feature_list = [e for e in comb]\n        #print(feature_list)\n        X = df[feature_list]\n        X_train, X_test, Y_train, Y_test = train_test_split(X, Y, stratify = df.Outcome, random_state=0)\n        regressor = RandomForestClassifier(n_estimators=60)\n        regressor.fit(X_train,Y_train)\n\n        Y_train_pred = regressor.predict(X_train)\n        Y_test_pred = regressor.predict(X_test)\n\n        accuracy_train = accuracy_score(Y_train,Y_train_pred)\n        accuracy_test = accuracy_score(Y_test,Y_test_pred)\n        if accuracy_test > best_test_accuracy:\n            best_test_accuracy = accuracy_test\n            best_feature_list = feature_list\n            print('RandomForest Accuracy\\ttrain: %.4f , test: %.4f' %(accuracy_train,accuracy_test))\n            \nprint(\"best feature list: \" + str(feature_list))\n\n# It looks like the best feature list for RandomForest contains: ['Glucose', 'BloodPressure', 'SkinThickness', 'Insulin',\n# 'BMI', 'DiabetesPedigreeFunction', 'Age', 'Sex', 'BMI_class', 'Glucose_Class', 'BloodPressure_Class', 'Insulin_Class']\n# which is the same set found for GradientBoosting\n# but the accuracy on test set is around 0.90 (almost the same anyway)","3fab8013":"**Conclusions**\n\nI got accuracy values around 0.9 for both RandomForest and GradientBoosting classifiers by adding some categorical features to the dataset and finding the best feature combination.\n\nI did not tried to tune the hyperparameters of the classifiers: to do that, I could save the combinations map\nto a pickle file, so that I can at least save the time to generate the combinations, and for each combination of feature perform a grid-search. I guess getting the results would take a lot of time anyway...\n\nAnother further improvement could come from the engineering of 'SkinThickness' and 'DiabetesPedigreeFunction' features, which I did not find any documentation easy to understand for me on Internet, so I left them as they were\n\nHope this notebook helps! Please upvote it in case you find it useful.","2952d94e":"# Follow-up of article at https:\/\/towardsdatascience.com\/machine-learning-workflow-on-diabetes-data-part-01-573864fcc6b8 about Diabetes dataset\n\nThis article suggests to add some categorical features to the dataset and see whether the accuracy can increase to around 90%. The following notebook shows what I did to achieve that","9f1e8f65":"# The number of feature is not very big: try all possible feature's combinations to get the best performing list of features","a359df8f":"There are many values set to zero in several columns, so I try to replace with mean values computed on 'Outcome' target column","b69d587c":"Following the suggestions provided by the article's author, try to add some categorical features to improve the overall accuracy which is rated around 0.75 for the dataset without feature engineering","804e5c6c":"There is no indication of sex, but based on 'Pregnancy' values I can at least assume who is female"}}