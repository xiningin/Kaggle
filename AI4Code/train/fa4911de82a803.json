{"cell_type":{"ae75b403":"code","3eb23f70":"code","7b5456f5":"code","f614d344":"code","90fd53ff":"code","95dc5c35":"code","ee88a30b":"code","e317cc01":"code","146e6a99":"code","f36eaa86":"code","6d6c2090":"code","298e914b":"code","2d69b8f7":"code","faa7055a":"code","d16e576a":"code","ed5d3b96":"code","9b3b1c92":"code","6264d00e":"code","95d11ed2":"code","ed4a7b6d":"code","22e2e05a":"code","687bd52e":"code","16a0b4a3":"code","e0317204":"code","e0ee2804":"code","30d7136d":"code","7be1e419":"code","c72f2141":"code","49e10f1f":"code","fe4d3796":"code","09badaf8":"code","14f5e649":"code","c590731a":"code","63ba5768":"code","56530eb2":"code","838a5bb1":"code","23db9708":"markdown","a15d7967":"markdown","77389490":"markdown","195ddd06":"markdown","ba8d2926":"markdown","43f24b42":"markdown","f4749940":"markdown","5bad8901":"markdown","c32eacc6":"markdown","b7361c25":"markdown","1419720f":"markdown","c6126af9":"markdown","d4ca202d":"markdown","0e876e9c":"markdown","749f110d":"markdown","5dc5f88c":"markdown","a68d829f":"markdown","60a5f0e1":"markdown","1768c792":"markdown","a99c47c8":"markdown","25b445bb":"markdown","48c6d76c":"markdown","148dd89e":"markdown","9c707bb7":"markdown","4507251a":"markdown","51813aaa":"markdown","3885def0":"markdown","f518c443":"markdown","a6b9ea68":"markdown","f97ec4fe":"markdown","7c60dfb2":"markdown","df9b5d93":"markdown","9d87dfad":"markdown","25749f6e":"markdown","2eaf3a09":"markdown","e77e9d8c":"markdown","292794c8":"markdown","ee8a78d7":"markdown","f0c627ba":"markdown","16e35d50":"markdown","4d1a335c":"markdown"},"source":{"ae75b403":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.utils import resample\nfrom xgboost import XGBClassifier\nfrom sklearn.metrics import roc_auc_score,precision_recall_curve,roc_curve\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.manifold import TSNE\nfrom sklearn.decomposition import PCA, TruncatedSVD\nimport time\nimport matplotlib.patches as mpatches\nfrom sklearn.metrics import confusion_matrix","3eb23f70":"%%time \ntrain_transactions=pd.read_csv('..\/input\/train_transaction.csv')\ntrain_identity=pd.read_csv('..\/input\/train_identity.csv')\nprint('Train data set is loaded !')","7b5456f5":"train_transactions.head()","f614d344":"train_transactions.info()","90fd53ff":"train_identity.info()","95dc5c35":"x=train_transactions['isFraud'].value_counts().values\nsns.barplot([0,1],x)\nplt.title('Target variable count')\n","ee88a30b":"train=train_transactions.merge(train_identity,how='left',left_index=True,right_index=True)\ny_train=train['isFraud'].astype('uint8')\nprint('Train shape',train.shape)\n\n\n\ndel train_transactions,train_identity\n\nprint(\"Data set merged \")\n\n","e317cc01":"\n\n%%time\n# From kernel https:\/\/www.kaggle.com\/gemartin\/load-data-reduce-memory-usage\n# WARNING! THIS CAN DAMAGE THE DATA \ndef reduce_mem_usage2(df):\n    \"\"\" iterate through all the columns of a dataframe and modify the data type\n        to reduce memory usage.        \n    \"\"\"\n    start_mem = df.memory_usage().sum() \/ 1024**2\n    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n    \n    for col in df.columns:\n        col_type = df[col].dtype\n        \n        if col_type != object:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n        else:\n            df[col] = df[col].astype('category')\n\n    end_mem = df.memory_usage().sum() \/ 1024**2\n    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) \/ start_mem))\n    \n    return df\n\n","146e6a99":"\n\n%%time\ntrain = reduce_mem_usage2(train)\n\n\n","f36eaa86":"X_train,X_test,y_train,y_test=train_test_split(train.drop('isFraud',axis=1),y_train,test_size=.2,random_state=1)","6d6c2090":"X=pd.concat([X_train,y_train],axis=1)\n\n\nnot_fraud=X[X.isFraud==0]\nfraud=X[X.isFraud==1]\n\n# upsample minority\nfraud_upsampled = resample(fraud,\n                          replace=True, # sample with replacement\n                          n_samples=len(not_fraud), # match number in majority class\n                          random_state=27) # reproducible results\n\n# combine majority and upsampled minority\nupsampled = pd.concat([not_fraud, fraud_upsampled])\n\n# check new class counts\nupsampled.isFraud.value_counts()\n\n","298e914b":"y=upsampled.isFraud.value_counts()\nsns.barplot(y=y,x=[0,1])\nplt.title('upsampled data class count')\nplt.ylabel('count')","2d69b8f7":"not_fraud_downsampled = resample(not_fraud,\n                                replace = False, # sample without replacement\n                                n_samples = len(fraud), # match minority n\n                                random_state = 27) # reproducible results\n\n# combine minority and downsampled majority\ndownsampled = pd.concat([not_fraud_downsampled, fraud])\n\n# checking counts\ndownsampled.isFraud.value_counts()","faa7055a":"y=downsampled.isFraud.value_counts()\nsns.barplot(y=y,x=[0,1])\nplt.title('downsampled data class count')\nplt.ylabel('count')","d16e576a":"from sklearn.datasets import make_classification\n\nX, y = make_classification(\n    n_classes=2, class_sep=1.5, weights=[0.9, 0.1],\n    n_informative=3, n_redundant=1, flip_y=0,\n    n_features=20, n_clusters_per_class=1,\n    n_samples=1000, random_state=10\n)\n\ndf = pd.DataFrame(X)\ndf['target'] = y\ndf.target.value_counts().plot(kind='bar', title='Count (target)')","ed5d3b96":"def logistic(X,y):\n    X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=.2,random_state=1)\n    lr=LogisticRegression()\n    lr.fit(X_train,y_train)\n    prob=lr.predict_proba(X_test)\n    return (prob[:,1],y_test)","9b3b1c92":"probs,y_test=logistic(X,y)","6264d00e":"def plot_pre_curve(y_test,probs):\n    precision, recall, thresholds = precision_recall_curve(y_test, probs)\n    plt.plot([0, 1], [0.5, 0.5], linestyle='--')\n    # plot the precision-recall curve for the model\n    plt.plot(recall, precision, marker='.')\n    plt.title(\"precision recall curve\")\n    plt.xlabel('Recall')\n    plt.ylabel('Precision')\n    # show the plot\n    plt.show()\n    \ndef plot_roc(y_test,prob):\n    fpr, tpr, thresholds = roc_curve(y_test, probs)\n    # plot no skill\n    plt.plot([0, 1], [0, 1], linestyle='--')\n    # plot the roc curve for the model\n    plt.plot(fpr, tpr, marker='.')\n    plt.title(\"ROC curve\")\n    plt.xlabel('false positive rate')\n    plt.ylabel('true positive rate')\n    # show the plot\n    plt.show()","95d11ed2":"plot_pre_curve(y_test,probs)","ed4a7b6d":"plot_roc(y_test,probs)","22e2e05a":"def plot_2d_space(X_train, y_train,X=X,y=y ,label='Classes'):   \n    colors = ['#1F77B4', '#FF7F0E']\n    markers = ['o', 's']\n    \n    fig,(ax1,ax2)=plt.subplots(1,2, figsize=(8,4))\n   \n    for l, c, m in zip(np.unique(y), colors, markers):\n        ax1.scatter(\n            X_train[y_train==l, 0],\n            X_train[y_train==l, 1],\n            c=c, label=l, marker=m\n        )\n    for l, c, m in zip(np.unique(y), colors, markers):\n        ax2.scatter(\n            X[y==l, 0],\n            X[y==l, 1],\n            c=c, label=l, marker=m\n        )\n   \n    ax1.set_title(label)\n    ax2.set_title('original data')\n    plt.legend(loc='upper right')\n    plt.show()\n","687bd52e":"\n# T-SNE Implementation\nt0 = time.time()\nX_reduced_tsne = TSNE(n_components=2, random_state=42).fit_transform(X)\nt1 = time.time()\nprint(\"T-SNE took {:.2} s\".format(t1 - t0))\n\n# PCA Implementation\nt0 = time.time()\nX_reduced_pca = PCA(n_components=2, random_state=42).fit_transform(X)\nt1 = time.time()\nprint(\"PCA took {:.2} s\".format(t1 - t0))\n\n# TruncatedSVD\nt0 = time.time()\nX_reduced_svd = TruncatedSVD(n_components=2, algorithm='randomized', random_state=42).fit_transform(X)\nt1 = time.time()\nprint(\"Truncated SVD took {:.2} s\".format(t1 - t0))","16a0b4a3":"f, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(24,6))\n# labels = ['No Fraud', 'Fraud']\nf.suptitle('Clusters using Dimensionality Reduction', fontsize=14)\n\n\nblue_patch = mpatches.Patch(color='#0A0AFF', label='No Fraud')\nred_patch = mpatches.Patch(color='#AF0000', label='Fraud')\n\n\n# t-SNE scatter plot\nax1.scatter(X_reduced_tsne[:,0], X_reduced_tsne[:,1], c=(y == 0), cmap='coolwarm', label='No Fraud', linewidths=2)\nax1.scatter(X_reduced_tsne[:,0], X_reduced_tsne[:,1], c=(y == 1), cmap='coolwarm', label='Fraud', linewidths=2)\nax1.set_title('t-SNE', fontsize=14)\n\nax1.grid(True)\n\nax1.legend(handles=[blue_patch, red_patch])\n\n# PCA scatter plot\nax2.scatter(X_reduced_pca[:,0], X_reduced_pca[:,1], c=(y == 0), cmap='coolwarm', label='No Fraud', linewidths=2)\nax2.scatter(X_reduced_pca[:,0], X_reduced_pca[:,1], c=(y == 1), cmap='coolwarm', label='Fraud', linewidths=2)\nax2.set_title('PCA', fontsize=14)\n\nax2.grid(True)\n\nax2.legend(handles=[blue_patch, red_patch])\n\n# TruncatedSVD scatter plot\nax3.scatter(X_reduced_svd[:,0], X_reduced_svd[:,1], c=(y == 0), cmap='coolwarm', label='No Fraud', linewidths=2)\nax3.scatter(X_reduced_svd[:,0], X_reduced_svd[:,1], c=(y == 1), cmap='coolwarm', label='Fraud', linewidths=2)\nax3.set_title('Truncated SVD', fontsize=14)\n\nax3.grid(True)\n\nax3.legend(handles=[blue_patch, red_patch])\n\nplt.show()\n","e0317204":"import imblearn","e0ee2804":"from imblearn.under_sampling import RandomUnderSampler\n\nran=RandomUnderSampler(return_indices=True) ##intialize to return indices of dropped rows\nX_rs,y_rs,dropped = ran.fit_sample(X,y)\n\nprint(\"The number of removed indices are \",len(dropped))\nplot_2d_space(X_rs,y_rs,X,y,'Random under sampling')\n","30d7136d":"probs,y_test=logistic(X_rs,y_rs)\nplot_pre_curve(y_test,probs)","7be1e419":"plot_roc(y_test,probs)","c72f2141":"from imblearn.over_sampling import RandomOverSampler\n\nran=RandomOverSampler()\nX_ran,y_ran= ran.fit_resample(X,y)\n\nprint('The new data contains {} rows '.format(X_ran.shape[0]))\n\nplot_2d_space(X_ran,y_ran,X,y,'over-sampled')\n","49e10f1f":"probs,y_test=logistic(X_ran,y_ran)\nplot_pre_curve(y_test,probs)","fe4d3796":"plot_roc(y_test,probs)","09badaf8":"from imblearn.under_sampling import TomekLinks\n\ntl = TomekLinks(return_indices=True, ratio='majority')\nX_tl, y_tl, id_tl = tl.fit_sample(X, y)\n\n#print('Removed indexes:', id_tl)\n\nplot_2d_space(X_tl, y_tl,X,y, 'Tomek links under-sampling')","14f5e649":"probs,y_test=logistic(X_tl,y_tl)\nplot_pre_curve(y_test,probs)","c590731a":"plot_roc(y_test,probs)","63ba5768":"\n\nfrom imblearn.over_sampling import SMOTE\n\nsmote = SMOTE(ratio='minority')\nX_sm, y_sm = smote.fit_sample(X, y)\n\nplot_2d_space(X_sm, y_sm,X,y, 'SMOTE over-sampling')\n\n","56530eb2":"probs,y_test=logistic(X_sm,y_sm)\nplot_pre_curve(y_test,probs)","838a5bb1":"plot_roc(y_test,probs)","23db9708":"\n## [Python imbalanced-learn module](#8)<a id='8'><\/a><\/br>\n\nA number of more sophisticated resapling techniques have been proposed in the scientific literature.\n\nFor example, we can cluster the records of the majority class, and do the under-sampling by removing records from each cluster, thus seeking to preserve information. In over-sampling, instead of creating exact copies of the minority class records, we can introduce small variations into those copies, creating more diverse synthetic samples.\n\nLet's apply some of these resampling techniques, using the Python library imbalanced-learn. It is compatible with scikit-learn and is part of scikit-learn-contrib projects.\n","a15d7967":"<div align='left'><font size='4' color='#229954'>Reducing memory usage<\/font><\/div>\n\n","77389490":"<div align='left'><font size='4' color=' #6c3483'>  Random over-sampling  with imbalanced-learn <\/font><\/div>\n\n","195ddd06":"I don't understand why the competition hosts selected ROC_AUC as evaluation metric,I think\n- ROC curves should be used when there are roughly equal numbers of observations for each class.\n-  Precision-Recall curves should be used when there is a moderate to large class imbalance.\n","ba8d2926":"<div align='left'><font size='4' color='#229954'>Splitting to train and validation<\/font><\/div>\n","43f24b42":"<div align='left'><font size='5' color='#5b2c6f '> Purpose of this notebook<\/font><\/div>","f4749940":"## [The metric trap](#3)<a id=\"3\"><\/a> <br>\n\nOne of the major issues that novice users fall into when dealing with unbalanced datasets relates to the metrics used to evaluate their model. Using simpler metrics like accuracy_score can be misleading. In a dataset with highly unbalanced classes, if the classifier always \"predicts\" the most common class without performing any analysis of the features, it will still have a high accuracy rate, obviously illusory.\n\n\n    False Positive. Predict an event when there was no event.\n    False Negative. Predict no event when in fact there was an event.\n\n   In the overview of the problem statement the organizers has described a situation where you stand at the queue for a long time and when your chance arrives,the transaction gets denied because it was interpreted as a Fraudulent transaction which many of us have faced.\n This is classical example of **False Negative** prediction.\n \n\n\n**Change the performance metric**\n\nAs we saw above, accuracy is not the best metric to use when evaluating imbalanced datasets as it can be very misleading. Metrics that can provide better insight include:\n\n**Confusion Matrix**: a table showing correct predictions and types of incorrect predictions.\n    \n**Precision**: the number of true positives divided by all positive predictions. Precision is also called Positive Predictive Value. It is a measure of a classifier\u2019s exactness. Low precision indicates a high number of false positives.\n    \n**Recall**: the number of true positives divided by the number of positive values in the test data. Recall is also called Sensitivity or the True Positive Rate. It is a measure of a classifier\u2019s completeness. Low recall indicates a high number of false negatives.\n    \n**F1 Score**: the weighted average of precision and recall.\n    ","5bad8901":"Let's try fit and predict on this data and observe the outcome.","c32eacc6":"<div align='left'><font size='4' color='#229954'>Getting basic Idea<\/font><\/div>","b7361c25":"Let's try fit and predict on this data and observe the outcome.","1419720f":"\n\nFor ease of visualization, let's create a small unbalanced sample dataset using the make_classification method:\n","c6126af9":"<div align='left'><font size='4' color=' #6c3483'>  Random under-sampling  with imbalanced-learn <\/font><\/div>\n\n","d4ca202d":"- We will now split the train dataset into train and validation set.\n- We will keeep 20% of data for validation.","0e876e9c":"\n## [Under-sampling: Tomek links](#9)\n\nTomek links are pairs of very close instances, but of opposite classes. Removing the instances of the majority class of each pair increases the space between the two classes, facilitating the classification process.\n\n![](https:\/\/raw.githubusercontent.com\/rafjaa\/machine_learning_fecib\/master\/src\/static\/img\/tomek.png?v=2)","749f110d":"- We will do an experiment with this data without any resampling technique.\n- We will fit and predict the data on a Logistic regression model and observe the output scores.","5dc5f88c":"<div align='left'><font size='4' color='#229954'>Target variable<\/font><\/div>\n","a68d829f":"\n\nWe will also create a 2-dimensional plot function, plot_2d_space, to see the data distribution:\n","60a5f0e1":"Undersampling can be defined as removing some observations of the majority class. Undersampling can be a good choice when you have a ton of data -think millions of rows. But a drawback is that we are removing information that may be valuable. This could lead to underfitting and poor generalization to the test set.\n\nWe will again use the resampling module from Scikit-Learn to randomly remove samples from the majority class.","1768c792":"## [Merging transaction and identity dataset](#4)<a id=\"4\"><\/a> <br>\n\nWe will firt merge our **transactions** and **identity** datasets.","a99c47c8":"- Let's try fit and predict on this data and observe the outcome.","25b445bb":"Now we will visualize the output of the above three algorithms in a 2D space.","48c6d76c":"- In this notebook we will discuss about class imbalance problem which is occus often more in problems like fraudulent transaction identification and\n  spam  identification .\n- Discuss and implement methods to solve this issue to an extend.\n- [Loading Libraries](#1)\n- [Loading Data ](#2)\n- [The metric trap](#3)\n- [Data preparating](#4)\n- [Resampling](#5)\n- [Resampling using sklearn](#6)\n- [Dimensionality Reduction and Clustering](#7)\n- [Python imbalanced-learn module](#8)\n- [Algorithmic Ensemble Techniques](#9)","148dd89e":"<div align='left'><font size='4' color=' #6c3483'>  2. Undersample majority class <\/font><\/div>\n","9c707bb7":"## [Loading Data](#2)<a id=\"2\"><\/a> <br>","4507251a":"<div align='left'><font size='4' color=' #6c3483'>  XGBoost <\/font><\/div>\n\n\n\n![](https:\/\/miro.medium.com\/max\/1400\/1*FLshv-wVDfu-i54OqvZdHg.png)\n\nXGBoost (Extreme Gradient Boosting) is an advanced and more efficient implementation of Gradient Boosting Algorithm discussed in the previous section.\n\nAdvantages over Other Boosting Techniques\n\nIt is 10 times faster than the normal Gradient Boosting as it implements parallel processing. It is highly flexible as users can define custom optimization objectives and evaluation criteria, has an inbuilt mechanism to handle missing values.\nUnlike gradient boosting which stops splitting a node as soon as it encounters a negative loss, XG Boost splits up to the maximum depth specified and prunes the tree backward and removes splits beyond which there is an only negative loss.\n\nExtreme gradient boosting can be done using the XGBoost package in R and Python","51813aaa":"![](https:\/\/cdn-images-1.medium.com\/max\/853\/1*DgFPLm5TKXuKnNUlYCE2DQ.jpeg)","3885def0":"## [Algorithmic Ensemble Techniques](#9)<a id=\"1\"><\/a> <br>\nThe above section, deals with handling imbalanced data by resampling original data to provide balanced classes. In this section, we are going to look at an alternate approach i.e.  Modifying existing classification algorithms to make them appropriate for imbalanced data sets.\n\nThe main objective of ensemble methodology is to improve the performance of single classifiers. The approach involves constructing several two stage classifiers from the original data and then aggregate their prediction\n\n![ Approach to Ensemble based Methodologies](https:\/\/s3-ap-south-1.amazonaws.com\/av-blog-media\/wp-content\/uploads\/2017\/03\/16142904\/ICP4.png)","f518c443":"\n## [Resampling](#5)<a id=\"5\"><\/a> <br>\n\nA widely adopted technique for dealing with highly unbalanced datasets is called resampling. It consists of removing samples from the majority class (under-sampling) and \/ or adding more examples from the minority class (over-sampling).\n\n![](https:\/\/raw.githubusercontent.com\/rafjaa\/machine_learning_fecib\/master\/src\/static\/img\/resampling.png)","a6b9ea68":"We will review other resampling techniques.","f97ec4fe":"\n\n\n\nSMOTE (Synthetic Minority Oversampling TEchnique) consists of synthesizing elements for the minority class, based on those that already exist. It works randomly picingk a point from the minority class and computing the k-nearest neighbors for this point. The synthetic points are added between the chosen point and its neighbors.\n\n![](https:\/\/raw.githubusercontent.com\/rafjaa\/machine_learning_fecib\/master\/src\/static\/img\/smote.png)\n","7c60dfb2":"- There is clearly a class imbalace problem.\n- We will look into methods of solving this issue later in this notebook.","df9b5d93":"## [Resampling Techniques using sklearn](#6)<a id=\"6\"><\/a> <br>","9d87dfad":"<div align='left'><font size='4' color=' #6c3483'>  Over-sampling: SMOTE <\/font><\/div>\n\n","25749f6e":"<div align='left'><font size='4' color=' #6c3483'> 1.Oversample minority class <\/font><\/div>\n","2eaf3a09":"### WORK IN PROGRESS\n<div align='left'><font size='5' color=' #a93226 '>  If you like my work,please do upvote ^ <\/font><\/div>\n\n","e77e9d8c":"### [Loading Required libraries](#1)<a id=\"1\"><\/a> <br>","292794c8":"In the below section we will implement three major dimensionality reduction algorithms\n- **T-sne**\n- **PCA**\n- **Truncated SVD**","ee8a78d7":"\n\n\n\n## [Dimensionality Reduction and Clustering](#7)<a id=\"7\"><\/a> <br>\n\nUnderstanding t-SNE:\nIn order to understand this algorithm you have to understand the following terms:\n\n    Euclidean Distance\n    Conditional Probability\n    Normal and T-Distribution Plots\n\n","f0c627ba":"Oversampling can be defined as adding more copies of the minority class. Oversampling can be a good choice when you don\u2019t have a ton of data to work with.\n\nWe will use the resampling module from Scikit-Learn to randomly replicate samples from the minority class.","16e35d50":"- We will define two functions to plot precision_recall curve and roc curve","4d1a335c":"<div align='left'><font size='4' color=' #6c3483'> References <\/font><\/div>\n\n\n- [Dealing with Imbalanced Data](https:\/\/towardsdatascience.com\/methods-for-dealing-with-imbalanced-data-5b761be45a18)\n- [Resampling strategies for imbalanced datasets](https:\/\/www.kaggle.com\/rafjaa\/resampling-strategies-for-imbalanced-datasets)"}}