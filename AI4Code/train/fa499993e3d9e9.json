{"cell_type":{"900d644c":"code","579906eb":"code","d969c853":"code","74eb3146":"code","4196844f":"code","bb94a589":"code","54595042":"code","deb18161":"code","115dff93":"code","87a524b5":"code","b0dc151d":"code","f9a08224":"code","bebb4ee9":"code","33c95b31":"code","0b863b39":"code","e8d46273":"code","88450fd1":"code","6c0e08b0":"code","6eca83d5":"code","af0f190a":"code","6e0f6f9a":"code","3509a38b":"code","89643538":"code","d29e0c7b":"code","2fec033d":"code","b9719a2d":"code","b2f58de9":"code","da19a7ea":"code","2357b642":"code","9252748c":"code","d205c675":"code","368b88b3":"code","f24ee242":"code","13448e64":"code","c86c2b31":"code","80fc5376":"code","7962ec0d":"code","de5b72ee":"code","27effef4":"code","be32ab0f":"code","8b898fe3":"code","c7af9628":"code","00728953":"code","bf3b1563":"code","c72708aa":"code","5bbf8fec":"code","47284f27":"code","26a365f9":"code","81146020":"code","7c5ec116":"code","d1fd9814":"code","6f01f589":"code","10cb394d":"code","67ffba3c":"code","8fdb586e":"code","b78cdd26":"code","97e8493e":"code","4cae884f":"code","9c3a2c43":"code","b22bb5b7":"code","7541c908":"code","ffbbf3ea":"code","86056dac":"code","84494d97":"code","feddc60e":"code","f004a877":"code","27117139":"code","77e6d2c9":"code","0b4ab970":"code","5ef57ee8":"code","11facc2a":"code","e201f84c":"code","528ee8fd":"markdown","8bcd16c8":"markdown","cfd7c70a":"markdown","8a226527":"markdown","fc78316c":"markdown","f6b5af76":"markdown","f915afed":"markdown","2baa814d":"markdown","c9b66405":"markdown","f5ba4d74":"markdown","a8d90dd0":"markdown","bf141972":"markdown","b01e0b89":"markdown","7bb33ae8":"markdown","7c4fb9e2":"markdown","b260bea8":"markdown","3062e9a0":"markdown","17c4fe37":"markdown","bb7910a8":"markdown","03e2bfe2":"markdown","681f7d4d":"markdown","8c6ee4b4":"markdown","e6c7a68d":"markdown"},"source":{"900d644c":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","579906eb":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sas\n%matplotlib inline ","d969c853":"test = pd.read_csv(\"\/kaggle\/input\/hackerearth-ml-challenge-pet-adoption\/test.csv\")\ntrain = pd.read_csv(\"\/kaggle\/input\/hackerearth-ml-challenge-pet-adoption\/train.csv\")","74eb3146":"#storing id's in some variables so that it can be used in future for prediction\ntrain_id = train[\"pet_id\"]\ntest_id = test[\"pet_id\"]","4196844f":"train[\"days\"] = (pd.to_datetime(train[\"listing_date\"]) - pd.to_datetime(train[\"issue_date\"])).dt.days\ntest[\"days\"] = (pd.to_datetime(test[\"listing_date\"]) - pd.to_datetime(test[\"issue_date\"])).dt.days\n## here i calculated the difference in listing nd issue date store it in a column named days.","bb94a589":"train.head()","54595042":"train.info()","deb18161":"train.describe()","115dff93":"test.shape","87a524b5":"test.info()","b0dc151d":"test.describe()","f9a08224":"train.isnull().sum().sort_values(ascending=False)[0:2]","bebb4ee9":"test.isnull().sum().sort_values(ascending=False)[0:2]","33c95b31":"list_drop=[\"pet_id\",\"issue_date\",\"listing_date\"]\n\nfor col in list_drop:\n    del train[col]\n    del test[col]\n    ","0b863b39":"train.condition.value_counts(dropna=False)","e8d46273":"test.condition.value_counts(dropna=False)","88450fd1":"train.condition.fillna(-1,inplace=True)","6c0e08b0":"test.condition.fillna(-1,inplace=True)","6eca83d5":"train.head()","af0f190a":"mat=train.corr()\nfig,ax = plt.subplots(figsize = (10,10))\nsas.heatmap(mat,annot = True, annot_kws={'size': 12})","6e0f6f9a":"test.shape","3509a38b":"test.head()","89643538":"Y1=train[\"breed_category\"]\nY2=train[\"pet_category\"]\nprint(Y1.shape)","d29e0c7b":"train.drop(\"pet_category\",axis=1,inplace=True)\n\ntrain.drop(\"breed_category\",axis=1,inplace=True)","2fec033d":"final=pd.concat([train,test],axis=0)","b9719a2d":"## Appyling one_hot_enconding to convert categorical data into numerical data\ndef One_hot_encoding(columns):\n    final_df=final\n    i=0\n    for fields in columns:\n        df1=pd.get_dummies(final[fields],drop_first=True)\n        \n        final.drop([fields],axis=1,inplace=True)\n        if i==0:\n            final_df=df1.copy()\n        else:           \n            final_df=pd.concat([final_df,df1],axis=1)\n        i=i+1\n       \n        \n    final_df=pd.concat([final,final_df],axis=1)\n        \n    return final_df","b2f58de9":"columns=[\"condition\",\"color_type\",\"X1\",\"X2\"]\n# i applied it to all the columns which have categories in it\n## in place of one_hot_encoding you can also use label encoder for encoding but your accuracy should be compromised","da19a7ea":"df_final=One_hot_encoding(columns)","2357b642":"df_final.head()","9252748c":"df_final.shape","d205c675":"df_final.shape","368b88b3":"from sklearn import preprocessing\n# Get column names first\nnames = df_final.columns\n# Create the Scaler object\nscaler = preprocessing.StandardScaler()\n# Fit your data on the scaler object\nscaled_df = scaler.fit_transform(df_final)\ndf_final = pd.DataFrame(scaled_df, columns=names)","f24ee242":"## As color_type is present in both dataset have maximum no. of classes present in it\n## try to analyze it that is there any class diffrence in both train and test data set","13448e64":"color_type_train=pd.get_dummies(train[\"color_type\"])\ncolor_type_test=pd.get_dummies(test[\"color_type\"])","c86c2b31":"co1=[]\nfor i in color_type_train:\n    if  i not in color_type_test :\n        co1.append(i)\nco1","80fc5376":"## as we can see that train dataset of two extra classes which are not present in test dataset \n## so these are not of our use \n## we can safely drop these columns which we added during applying one_hot_encoding","7962ec0d":"df_final.drop('Black Tiger',axis=1,inplace=True)\ndf_final.drop(\"Brown Tiger\",axis=1,inplace=True)","de5b72ee":"df_final.columns","27effef4":"cols = []\ncount = 1\nfor column in df_final.columns:\n    cols.append(count)\n    count+=1\n    continue\n    \ndf_final.columns = cols","be32ab0f":"df_final.columns","8b898fe3":"df_train=df_final.iloc[:18834,:]\ndf_test=df_final.iloc[18834:,:]","c7af9628":"X=df_train","00728953":"df_test.shape","bf3b1563":"from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\nfrom sklearn.model_selection import train_test_split","c72708aa":"x1_train,x1_test,y1_train,y1_test=train_test_split(X,Y2)","5bbf8fec":"booster=['gbtree','gblinear']\nbase_score=[0.25,0.5,0.75,1]","47284f27":"## Hyper Parameter Optimization\n\n\nn_estimators = [100, 500, 900, 1100, 1500]\nmax_depth = [2, 3, 5, 10, 15]\nbooster=['gbtree','gblinear']\nlearning_rate=[0.05,0.1,0.15,0.20]\nmin_child_weight=[1,2,3,4]\n\n# Define the grid of hyperparameters to search\nhyperparameter_grid = {\n    'n_estimators': n_estimators,\n    'max_depth':max_depth,\n    'learning_rate':learning_rate,\n    'min_child_weight':min_child_weight,\n    'booster':booster,\n    'base_score':base_score\n    }","26a365f9":"from xgboost import XGBClassifier","81146020":"xgb1 = XGBClassifier()","7c5ec116":"random_cv_01 = RandomizedSearchCV(estimator=xgb1,\n            param_distributions=hyperparameter_grid,\n            cv=5, n_iter=50,\n            scoring = 'neg_mean_absolute_error',n_jobs = 4,\n            verbose = 5, \n            return_train_score = True,\n            random_state=42)","d1fd9814":"random_cv_01.fit(x1_train,y1_train)","6f01f589":"random_cv_01.best_estimator_","10cb394d":"xgb1=XGBClassifier(base_score=0.25, booster='gbtree', colsample_bylevel=1,\n       colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,\n       importance_type='gain', interaction_constraints='',\n       learning_rate=0.1, max_delta_step=0, max_depth=2,\n       min_child_weight=1, monotone_constraints='()',\n       n_estimators=900, n_jobs=0, num_parallel_tree=1,\n       objective='multi:softprob', random_state=0, reg_alpha=0,\n       reg_lambda=1, scale_pos_weight=None, subsample=1,\n       tree_method='exact', validate_parameters=1, verbosity=None)","67ffba3c":"xgb1.fit(x1_train,y1_train)","8fdb586e":"new_feat=xgb1.predict(X)\nout_01=xgb1.predict(df_test)\nvald_01=xgb1.predict(x1_test)","b78cdd26":"X2=X","97e8493e":"X2[\"output1\"]=new_feat","4cae884f":"df_test_2=df_test","9c3a2c43":"df_test_2[\"output1\"]=out_01","b22bb5b7":"x2_train,x2_test,y2_train,y2_test=train_test_split(X2,Y1)","7541c908":"xgb2=XGBClassifier()","ffbbf3ea":"booster=['gbtree','gblinear']\nbase_score=[0.25,0.5,0.75,1]","86056dac":"## Hyper Parameter Optimization\n\n\nn_estimators = [100, 500, 900, 1100, 1500]\nmax_depth = [2, 3, 5, 10, 15]\nbooster=['gbtree','gblinear']\nlearning_rate=[0.05,0.1,0.15,0.20]\nmin_child_weight=[1,2,3,4]\n\n# Define the grid of hyperparameters to search\nhyperparameter_grid = {\n    'n_estimators': n_estimators,\n    'max_depth':max_depth,\n    'learning_rate':learning_rate,\n    'min_child_weight':min_child_weight,\n    'booster':booster,\n    'base_score':base_score\n    }","84494d97":"random_cv_02 = RandomizedSearchCV(estimator=xgb2,\n            param_distributions=hyperparameter_grid,\n            cv=5, n_iter=50,\n            scoring = 'neg_mean_absolute_error',n_jobs = 4,\n            verbose = 5, \n            return_train_score = True,\n            random_state=42)","feddc60e":"random_cv_02.fit(x2_train,y2_train)","f004a877":"random_cv_02.best_estimator_","27117139":"xgb2=XGBClassifier(base_score=0.25, booster='gblinear', colsample_bylevel=None,\n       colsample_bynode=None, colsample_bytree=None, gamma=None, gpu_id=-1,\n       importance_type='gain', interaction_constraints=None,\n       learning_rate=0.1, max_delta_step=None, max_depth=3,\n       min_child_weight=4, monotone_constraints=None,\n       n_estimators=1100, n_jobs=0, num_parallel_tree=None,\n       objective='multi:softprob', random_state=0, reg_alpha=0,\n       reg_lambda=0, scale_pos_weight=None, subsample=None,\n       tree_method=None, validate_parameters=1, verbosity=None)","77e6d2c9":"xgb2.fit(x2_train,y2_train)","0b4ab970":"out_02=xgb2.predict(df_test)\nvald_02=xgb2.predict(x2_test)","5ef57ee8":"from sklearn.metrics  import f1_score","11facc2a":"s1=f1_score(y1_test,vald_01,average='weighted')\ns2=f1_score(y2_test,vald_02,average='weighted')\naccuracy=100*((s1+s2)\/2)\naccuracy","e201f84c":"sub_new=pd.DataFrame({\n    \"pet_id\":test_id,\n    \"breed_category\":out_02,\n    \"pet_category\":out_01\n})\nsub_new.to_csv(\"sub_new_13.csv\",index=False)","528ee8fd":"## Normalizing Data","8bcd16c8":"## Reading datasets","cfd7c70a":"# Model 2\n- for breed_category","8a226527":"## Hypertuning for model 1","fc78316c":"## Changing columns names into some unique value ","f6b5af76":"## Hypertuning of Parameter of Model 2","f915afed":"## Upvote if you like this notebook and feel free to ask your doubts in comment section.\n## your suggestions are welcome\n\n\n# Thank You :)\n\n","2baa814d":"## Training model 2 using Best estimators predicted by hypertuning","c9b66405":"## Importing required Libraries","f5ba4d74":"# Checking Accuracy","a8d90dd0":"- In the above co-relation heatmap we have noticed that breed_ category and pet_category is interdependent so we can use that in modeling","bf141972":"## Training model 1 using Best estimators predicted by hypertuning","b01e0b89":"Problem type: Multitarget Multiclass Classification\nThis is an ongoing ML competition on Hackerearth (Jul 30, 2020 - Aug 23, 2020). We are required to build an model to determine type and breed of the animal based on its physical attributes and other factors. The evaluation metric being used is (the average of both f1_scores * 100).","7bb33ae8":" ## Marging test and train dataset","7c4fb9e2":"# Applying XgBoost\n- we are going to apply Xgboost in parts \n- first we predict pet_category using model 1\n- then we use that outcome to predict model 2 \n- as we have seen pet category and breed category are interdependent","b260bea8":"# Submission","3062e9a0":"- This was my first HackerEarth challenge.\n- By the above steps we can achieve Accureacy of 90.47876 by which i got 100th rank in the competition.\n","17c4fe37":"## HackerEarth ML Challenge 2020 - Adopt a buddy","bb7910a8":"# Spliting Dataset again into Train and Test","03e2bfe2":"## Feature engineering","681f7d4d":" # Model 1\n - for pet_category","8c6ee4b4":"#### Deleting all the columns which you feel like they are not contributing in predictions.","e6c7a68d":"**Kindly upvote if you find it interesting\/helpful and comment your suggestions or any queries**"}}