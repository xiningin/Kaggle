{"cell_type":{"9ae2a5c8":"code","dc4daff0":"code","d7efe81c":"code","7bf0f307":"code","bd4a1642":"code","36d913cc":"code","39978aee":"code","49801a60":"code","3b46093c":"code","678ae5d9":"code","38ff6e59":"code","b4b03f95":"code","1532e15f":"code","7f580917":"code","64cff3a6":"code","14b4725c":"code","67dc3adf":"code","af659182":"code","751adee0":"code","480260ec":"code","404b8630":"code","0e5e46ae":"code","27bf8c8a":"code","20a7658a":"code","edb3a6de":"code","dcf66db0":"code","34a9761a":"code","484faeff":"code","fe3ddb04":"code","4f938351":"code","6e7055f3":"markdown","00609e89":"markdown","bd0a9cdf":"markdown","08bf2214":"markdown","bb029959":"markdown","1b967f55":"markdown","97eb9236":"markdown","e2a49e57":"markdown","2e7ca5f6":"markdown","aed090ba":"markdown","c3fd2441":"markdown","d3ffd614":"markdown","4963b2d0":"markdown","cfb3d14e":"markdown","4a641024":"markdown"},"source":{"9ae2a5c8":"import pandas as pd\nimport numpy as np\n# Dataset\ndataset_filename = '..\/input\/processed-dataframe-v2'","dc4daff0":"data = pd.read_csv(dataset_filename, index_col=0)\nfor x in [\"doi\",\"title\"]:\n    data = data[data[x].notnull()]\ndata = data[[\"doi\",\"title\"]]","d7efe81c":"from nltk.tokenize import RegexpTokenizer\nfrom nltk.corpus import stopwords\nimport nltk\nfrom nltk.corpus import wordnet\nfrom nltk.stem import WordNetLemmatizer\nnltk.download('wordnet')","7bf0f307":"wordnet_lemmatizer = WordNetLemmatizer()\ntokenizer = RegexpTokenizer(r'[a-z]+')\nstop_words = set(stopwords.words('english'))\n\ndef preprocess(document):\n    # Text to lowercase\n    document = document.lower() \n    # Tokenize\n    words = tokenizer.tokenize(document) \n    # Removing stopwords\n    words = [w for w in words if not w in stop_words] \n    # Lemmatizing\n    for pos in [wordnet.NOUN, wordnet.VERB, wordnet.ADJ, wordnet.ADV]:\n        words = [wordnet_lemmatizer.lemmatize(x, pos) for x in words]\n    return \" \".join(words)\n\n","bd4a1642":"# Processed title\ndata['Processed Title'] = data['title'].apply(preprocess)\ndata.head()","36d913cc":"from sklearn.feature_extraction.text import TfidfVectorizer\n\nvectorizer = TfidfVectorizer()\nTF_IDF_matrix = vectorizer.fit_transform(data['Processed Title'])\n# T is for transpose\nTF_IDF_matrix = TF_IDF_matrix.T\n\nprint('Vocabulary Size : ', len(vectorizer.get_feature_names()))\nprint('N*K Matrix : ', TF_IDF_matrix.shape)","39978aee":" K = 2 # number of components\nword = 'response' #use suitable keyword for which you want to look into the topics","49801a60":"import numpy as np\n\n# Applying SVD\nU, s, VT = np.linalg.svd(TF_IDF_matrix.toarray())\n\nTF_IDF_matrix_reduced = np.dot(U[:,:K], np.dot(np.diag(s[:K]), VT[:K, :]))\n\n# Document and term representation (N here is no.of documents)\nt_rep = np.dot(U[:,:K], np.diag(s[:K]))\nd_rep = np.dot(np.diag(s[:K]), VT[:K, :]).T ","3b46093c":"from matplotlib import pyplot as plt\nimport seaborn as sns\n\n# sns settings\nsns.set(rc={'figure.figsize':(15,15)})\n\n# colors\npalette = sns.color_palette(\"bright\", 1)\n\n# plot\nsns.scatterplot(d_rep[:,0], d_rep[:,1], palette=palette)\nplt.title('Document Representation')\nplt.savefig(\"covid19.png\")\nplt.show()","678ae5d9":"plt.scatter(t_rep[:,0], t_rep[:,1])\nplt.title(\"Term Representation\")\nplt.show()","38ff6e59":"# Function to generate query representation\ndef query_rep_lsa(word):\n    q_rep = [vectorizer.vocabulary_[x] for x in preprocess(word).split()]\n    q_rep = np.mean(t_rep[q_rep],axis=0)\n    return q_rep","b4b03f95":"# Information retrieval \nfrom scipy.spatial.distance import cosine\n\nq_rep = query_rep_lsa(word)\n\nqdoc_cosine_dist = [cosine(q_rep, doc_rep) for doc_rep in d_rep]\nqdoc_sort_index = np.argsort(np.array(qdoc_cosine_dist))\n\ncount = 0\nfor rank, sort_index in enumerate(qdoc_sort_index):\n    print ('Rank : ', rank, ' Consine : ', 1 - qdoc_cosine_dist[sort_index],' Title : ', data['title'][sort_index])\n    if count == 2 :\n        break\n    else:\n        count += 1","1532e15f":"from sklearn.feature_extraction.text import CountVectorizer\n\ncount_vectorizer = CountVectorizer(stop_words='english', max_features=40000)\ntext_sample = data['title'].values\n\nprint('Topic pre vectorization: {}'.format(text_sample[123]))\n\ndoc_term_matrix = count_vectorizer.fit_transform(text_sample)\n\nprint('Topic post vectorization: \\n{}'.format(doc_term_matrix[123]))","7f580917":"# Parameter for number of topics\ntopics = 8","64cff3a6":"from sklearn.decomposition import TruncatedSVD","14b4725c":"# Truncated SVD of document term matrix\n\nlsa_model = TruncatedSVD(n_components=topics)\nlsa_topic_matrix = lsa_model.fit_transform(doc_term_matrix)","67dc3adf":"# Fetch topic categories for topic matrix\ndef get_keys(topic_matrix):\n    keys = topic_matrix.argmax(axis=1).tolist()\n    return keys","af659182":"# Fetching topic categories and their corresponding values\nfrom collections import Counter\ndef keys_to_counts(all_keys):\n\n    count_pairs = Counter(all_keys).items()\n    categories_list = [pair[0] for pair in count_pairs]\n    counts = [pair[1] for pair in count_pairs]\n    return (categories_list, counts)","751adee0":"lsa_key_items = get_keys(lsa_topic_matrix)\ntopic_categories, counts = keys_to_counts(lsa_key_items)","480260ec":"def frequent_words(n, key_items, doc_term_matrix, count_vectorizer):\n    frequent_word_list = []\n    \n    for topic in range(topics):\n        # Initialise temporary vector sum to 0\n        vectar_sum = 0\n        for i in range(len(key_items)):\n            #Check for match\n            if key_items[i] == topic:\n                vectar_sum = vectar_sum + doc_term_matrix[i]\n        vectar_sum = vectar_sum.toarray()\n        # Fetch index\n        n_frequent_word_index = np.flip(np.argsort(vectar_sum)[0][-n:],0)\n        # Append to list\n        frequent_word_list.append(n_frequent_word_index)   \n    most_frequent_words = []\n    \n    for topic in frequent_word_list:\n        occurences_list = []\n        for index in topic:\n            temporary_vector = np.zeros((1,doc_term_matrix.shape[1]))\n            temporary_vector[:,index] = 1\n            # Get word\n            occurence = count_vectorizer.inverse_transform(temporary_vector)[0][0]\n            occurences_list.append(occurence.encode('ascii').decode('utf-8'))\n        most_frequent_words.append(\"\\t\".join(occurences_list))         \n    return most_frequent_words","404b8630":"frequent_words_lsa = frequent_words(10, lsa_key_items, doc_term_matrix, count_vectorizer)\n# Print 8 top topics\nfor i in range(len(frequent_words_lsa)):\n    print(\"Topic: \",frequent_words_lsa[i])","0e5e46ae":"frequent_3_words = frequent_words(3, lsa_key_items, doc_term_matrix, count_vectorizer)\n# Set labels\nlabels = [format(i) + frequent_3_words[i] for i in topic_categories]\n# Plotting Results\nfig, ax = plt.subplots(figsize=(16,8))\nax.set_title('Topic wise LSA counts representation');\nax.bar(topic_categories, counts);\nax.set_xticklabels(labels);\nax.set_xticks(topic_categories);\nplt.show()","27bf8c8a":"from sklearn.manifold import TSNE\ntsne_lsa_model = TSNE(n_components=2, perplexity=50, learning_rate=100, \n                        n_iter=2000, verbose=1, random_state=0, angle=0.75)\ntsne_lsa_vectors = tsne_lsa_model.fit_transform(lsa_topic_matrix)","20a7658a":"colormap = np.array([\n    \"#ceae74\", \"#081b5b\", \"#ed40f4\", \"#4ca766\", \"#d1aad3\",\n    \"#63b181\", \"#543346\", \"#bed3ce\", \"#bfe14c\", \"#9c7072\",\n    \"#cd9e53\", \"#7c1adf\", \"#bb1a28\", \"#dfccee\", \"#617dcc\",\n    \"#e59da4\", \"#fde983\", \"#ee9909\", \"#c483a1\", \"#37cfcc\" ])\ncolormap = colormap[:topics]","edb3a6de":"def mean_topics(keys, two_dim_vectors):\n    vector_topis = []\n    for t in range(topics):\n        articles = []\n        for i in range(len(keys)):\n            if keys[i] == t:\n                articles.append(two_dim_vectors[i])    \n        \n        articles = np.vstack(articles)\n        mean_articles = np.mean(articles, axis=0)\n        vector_topis.append(mean_articles)\n    return vector_topis","dcf66db0":"from bokeh.plotting import figure, show\nfrom bokeh.io import output_notebook\nfrom bokeh.models import Label\nfrequent_3_words = frequent_words(3, lsa_key_items, doc_term_matrix, count_vectorizer)\nlsa_vectar_topics = mean_topics(lsa_key_items, tsne_lsa_vectors)\n\nplot = figure(title=\"LSA t-SNE Clustering of {} topics\".format(topics), plot_width=650, plot_height=650)\nplot.scatter(x=tsne_lsa_vectors[:,0], y=tsne_lsa_vectors[:,1], color=colormap[lsa_key_items])\nlabel_list = []\nfor t in range(topics):\n    label = Label(x=lsa_vectar_topics[t][0], y=lsa_vectar_topics[t][1], \n                  text=frequent_3_words[t], text_color=colormap[t])\n    label_list.append(label)\n    plot.add_layout(label)\noutput_notebook()\n# Display plot\nshow(plot)","34a9761a":"label_list","484faeff":"from sklearn.metrics import silhouette_score, calinski_harabasz_score","fe3ddb04":"silhouette_score(tsne_lsa_vectors, tsne_lsa_vectors[:,1] )","4f938351":"calinski_harabasz_score(tsne_lsa_vectors, tsne_lsa_vectors[:,1] )","6e7055f3":"### CORRESPOND CATEGORIES TO MOST FREQUENT WORDS","00609e89":"### Silhouette Score","bd0a9cdf":"### PLOT CLUSTERING RESULT","08bf2214":"### PROCESSING DATA FOR MODELLING ","bb029959":"### Calinski Harabasz Score\n","1b967f55":"### DATA PREPROCESSING","97eb9236":"### GET CENTROIDS FOR TOPICS","e2a49e57":"### REPRESENTATIONS","2e7ca5f6":"### CREATE TF-IDF MATRIX","aed090ba":"### APPLY DIMENSIONALITY-REDUCTION t -SNE","c3fd2441":"### MATCH WORD TO TOPICS USING LSA","d3ffd614":"### LOAD DATA AND BASIC IMPORTS","4963b2d0":"### APPLY SVD TO TF-IDF MATRIX","cfb3d14e":"### FETCH TOPIC CATEGORIES","4a641024":"### APPLY SVD TO DOCUMENT TERM MATRIX"}}