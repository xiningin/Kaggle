{"cell_type":{"262c8a1b":"code","9015a794":"code","ccdab6a8":"code","7c470ffc":"code","82cdd4fb":"code","642b5cf3":"code","6cd501a1":"code","71aeb75d":"code","4b29f837":"code","739b356b":"code","36ef3898":"code","e7ff32f3":"markdown","4247f1f1":"markdown","f06e4e98":"markdown","6aaf532c":"markdown","6ae2e227":"markdown","ddf0d6d2":"markdown","d122ac5e":"markdown","1bf615bd":"markdown","9634e290":"markdown","a8c508f5":"markdown","c4046fe2":"markdown","96b2e6b2":"markdown","0a3c1a9e":"markdown"},"source":{"262c8a1b":"from IPython.display import clear_output\nfrom transformers import AutoTokenizer\n\ncheckpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\ntokenizer = AutoTokenizer.from_pretrained(checkpoint); clear_output()","9015a794":"raw_inputs = [\n    \"I've been waiting for a HuggingFace course my whole life.\", \n    \"I hate this so much!\",\n]\ninputs = tokenizer(raw_inputs, padding=True, truncation=True, return_tensors=\"tf\")\nprint(inputs)","ccdab6a8":"from transformers import TFAutoModel\n\n# !!use the same checkpoint!!\nmodel = TFAutoModel.from_pretrained(checkpoint); clear_output()","7c470ffc":"outputs = model(inputs)\nprint(outputs.last_hidden_state.shape)","82cdd4fb":"outputs.keys()","642b5cf3":"from transformers import TFAutoModelForSequenceClassification\n\nmodel = TFAutoModelForSequenceClassification.from_pretrained(checkpoint); clear_output()\noutputs = model(inputs)","6cd501a1":"outputs","71aeb75d":"outputs['logits']","4b29f837":"import tensorflow as tf\n\npredictions = tf.math.softmax(outputs.logits, axis=1)\nprint(predictions)","739b356b":"model.config.id2label","36ef3898":"[model.config.id2label[i] for i in tf.math.argmax(predictions, axis=1).numpy()]","e7ff32f3":"# 3. Postprocessing the output\n---\nThe values we get as output from our model don\u2019t necessarily make sense by themselves. Let\u2019s take a look:","4247f1f1":"## 2.1) Model Head\nThe model heads take the high-dimensional vector of hidden states as input and project them onto a different dimension. They are usually composed of one or a few linear layers:\n![](https:\/\/huggingface.co\/course\/static\/chapter2\/transformer_and_head.png)\n1. The embeddings layer converts each input ID in the tokenized input into a vector that represents the associated token. \n2. The subsequent layers manipulate those vectors using the attention mechanism to produce the final representation of the sentences.(contextual understanding thing mentioned before)\n\nThere are many different *Head* architectures in \ud83e\udd17, each one designed to tackle a specific task. Here is a non-exhaustive list:\n- `*Model` (retrieve the hidden states)\n- `*ForCausalLM`\n- `*ForMaskedLM`\n- `*ForMultipleChoice`\n- `*ForQuestionAnswering`\n- `*ForSequenceClassification`\n- `*ForTokenClassification`","f06e4e98":"In example, we will need a model with a sequence classification head (to be able to classify the sentences as positive or negative). So, we won\u2019t actually use the `TFAutoModel` class, but `TFAutoModelForSequenceClassification`:","6aaf532c":"# 2. Going through the model\n---\nWe can get our pretrained model the same way we did with our tokenizer. \ud83e\udd17 Transformers provides an `TFAutoModel` class which also has a `from_pretrained` method:","6ae2e227":"Once we have the tokenizer, we can directly pass our sentences to it and we\u2019ll get back a dictionary that\u2019s ready to feed to our model. <br>\nTo specify the type of tensors we want to get back from tokenizer(PyTorch, TensorFlow, or plain NumPy), we use the `return_tensors` argument (if no type is passed, you will get a list of lists as a result):","ddf0d6d2":"Now we can see that the model predicted the probability scores [0.0402, 0.9598] for the first sentence and [0.9995, 0.0005] for the second one.\n\nTo get the labels corresponding to each position,","d122ac5e":"# Introduction\n---\nFull original tutorial at https:\/\/huggingface.co\/course\/chapter2\n\nIn Chapter 1, we used Transformer models for different tasks using the high-level pipeline API. Although this API is powerful and convenient, it\u2019s important to understand how it works under the hood \n\nIn this chapter, you will learn:\n\n- How to use tokenizers and models to replicate the pipeline API\u2019s behavior\n- How to load and save models and tokenizers\n- Different tokenization approaches, such as word-based, character-based, and subword-based\n- How to handle multiple sentences of varying lengths\n","1bf615bd":"We have successfully reproduced the three steps of the pipeline: \n1. preprocessing with tokenizers\n2. passing the inputs through the model\n3. postprocessing\n\nNow let\u2019s take some time to dive deeper into each of those steps.","9634e290":"The model predicted [-1.5607, 1.6123] for the first sentence and [ 4.1692, -3.3464] for the second one. \n\nThose are not probabilities but logits, the raw, unnormalized scores outputted by the last layer of the model. To be converted to probabilities, they need to go through a SoftMax layer **(all \ud83e\udd17 Transformers models output the logits, as the loss function for training will generally fuse the last activation function, such as SoftMax, with the actual loss function, such as cross entropy):**","a8c508f5":"This `TFAutoModel` contains only the base Transformer. For each model input, we\u2019ll retrieve a high-dimensional vector representing the <u><strong>contextual understanding of that input by the Transformer model<\/strong><\/u>.\n\nThese hidden states are usually inputs to another part of the model (*Transfer learning*). In Chapter 1, different tasks could have been performed with the same base Transformer, but each task will have a different head associated with it.\n\nThe vector output by the Transformer module generally has three dimensions:\n- **Batch size**: The number of sequences processed at a time (2 in our example).\n- **Sequence length**: The length of the numerical representation of the sequence (16 in our example).\n- **Hidden size**: Embedding dimension of each token (768 is common for smaller models, and in larger models this can reach 3072 or more).","c4046fe2":"# Behind the pipeline\n---\nPipeline groups together three steps: preprocessing, passing the inputs through the model, and postprocessing:\n![](https:\/\/huggingface.co\/course\/static\/chapter2\/full_nlp_pipeline.png)\n# 1. Preprocessing with a tokenizer\n---\nThe first step of our pipeline is to convert the text inputs into numbers that the model can make sense of. To do this we use a *tokenizer*, which will be responsible for:\n- Splitting the input into *tokens* (how the token is defined depends on the training objective)\n- Mapping each token to an integer\n- Adding additional inputs that may be useful to the model\n\n$$\\text{All this preprocessing needs to be done in exactly the same way as when the model was pretrained.}$$ <br>\nTo do this, we use the `AutoTokenizer` class and its `from_pretrained` method. Using the checkpoint name of our model, it will automatically fetch the information associated with the model\u2019s tokenizer","96b2e6b2":"The output itself is a dictionary containing two keys,\n- `input_ids` contains two rows of integers (one for each sentence) that are the unique identifiers of the tokens in each sentence. \n- `attention_mask ` is a mask telling the model to ignore padding tokens (or other unnecessary tokens)\n","0a3c1a9e":"Note that the outputs of \ud83e\udd17 Transformers models behave like *namedtuples*(dictionaries-like). You can access the elements by attributes or by key (`outputs[\"last_hidden_state\"]`), or even by index if you know exactly where the thing you are looking for is (`outputs[0]`)."}}