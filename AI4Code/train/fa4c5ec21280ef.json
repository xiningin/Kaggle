{"cell_type":{"ef64764d":"code","6810ecf3":"code","6a0ba21f":"code","30d7ceda":"code","97810d69":"code","bf886b1e":"code","7fd26879":"code","0b90e48a":"code","b33f74c1":"code","384f079b":"code","0e4fdce7":"code","f838d4db":"code","f63c3700":"markdown","0c14cd8c":"markdown"},"source":{"ef64764d":"import numpy as np\nimport pandas as pd\nimport os","6810ecf3":"data = pd.read_csv('\/kaggle\/input\/imdb-dataset-of-50k-movie-reviews\/IMDB Dataset.csv')\nprint(data.head())\nsize = data.shape[0]\ndata = data[:int(size\/10)]","6a0ba21f":"# Tokenise - find a more efficient way (spacy, textblob)\n# Does stop word removal increase the accuracy?\nimport nltk\nfrom nltk.stem import PorterStemmer\nfrom nltk.tokenize import sent_tokenize, word_tokenize\nimport re\nimport string\ndef preprocessor(text):\n    text = text.lower()\n    text = re.sub('\\[.*?\\]', '', text)\n    text = re.sub(\"\\\\W\",\" \",text) # remove special chars\n    text = re.sub('https?:\/\/\\S+|www\\.\\S+', '', text)\n    text = re.sub('<.*?>+', '', text)\n    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n    text = re.sub('\\n', '', text)\n    text = re.sub('\\w*\\d\\w*', '', text)\n    return text\ndata['review']=data['review'].apply(preprocessor)\ndata.head()","30d7ceda":"print(\"Shape: \", data.shape)\nprint()\nprint(\"Info: \",data.info())\nprint()\nprint(\"Counts: \", data.sentiment.value_counts())","97810d69":"# Label encoding just changes the classification to a numerical value that is easier to work with as compared to a word\/phrase\nfrom sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\ndata[\"sentiment\"] = le.fit_transform(data[\"sentiment\"])\ndata.head()\n\n# Some of the data will be used to train and some of the data will be used to test how well the model is doing\nfrom sklearn.model_selection import train_test_split\ny=data['sentiment']\nx=data['review']\nx_train,x_test,y_train,y_test=train_test_split(x,y,random_state=0,test_size=0.2)\nprint(\"Train: \", x_train.shape)\nprint(\"Test: \", x_test.shape)","bf886b1e":"# Bag of words with count vectoriser\nfrom sklearn.feature_extraction.text import CountVectorizer\nvectCount=CountVectorizer(ngram_range=(1,2))\nx_train_trans_Count=vectCount.fit_transform(x_train)","7fd26879":"#CLASSIFIER PIPLINES - logistic regression and decision tree for a good variability in outcome\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import accuracy_score, classification_report\nfrom sklearn.pipeline import Pipeline\n\nlr = Pipeline([('lrmodel', LogisticRegression())])\nlr.fit(x_train_trans_Count,y_train)\npred_y=lr.predict(vectCount.transform(x_test))\nscore_lr=accuracy_score(y_test,pred_y)\nreport_lr = classification_report(y_test,pred_y)\nprint(\"Logistic regression\")\nprint(\"Accuracy: \", score_lr)\nprint(report_lr)\n\ndt = Pipeline([('dtmodel', DecisionTreeClassifier())])\ndt.fit(x_train_trans_Count,y_train)\npred_y=dt.predict(vectCount.transform(x_test))\nscore_dt=accuracy_score(y_test,pred_y)\nreport_dt = classification_report(y_test,pred_y)\nprint(\"Decision tree\")\nprint(\"Accuracy: \", score_dt)\nprint(report_dt)","0b90e48a":"#VOTING CLASSIFIER - quick, simple way to produce increased relaibility.\n\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.ensemble import VotingClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import accuracy_score, classification_report\n\npipe1 = make_pipeline(CountVectorizer(ngram_range=(1,2)), LogisticRegression(C=1000))\n\npipe2 = make_pipeline(CountVectorizer(ngram_range=(1,2)), DecisionTreeClassifier(max_depth=6))\n              \nvotingClassifier = VotingClassifier(estimators=[('p1', pipe1), ('p2', pipe2)])\nvotingClassifier.fit(x_train, y_train)\ny_predicted = votingClassifier.predict(x_test)\nprint(\"Hard voting:\")\nprint(\"Accuracy: \", accuracy_score(y_test, y_predicted))\nprint(classification_report(y_test, y_predicted))\n\nvotingClassifier = VotingClassifier(estimators=[('p1', pipe1), ('p2', pipe2)], voting ='soft')\nvotingClassifier.fit(x_train,y_train)\ny_predicted =votingClassifier.predict(x_test)\nprint(\"Soft voting:\")\nprint(\"Accuracy: \", accuracy_score(y_test, y_predicted))\nprint(classification_report(y_test, y_predicted))","b33f74c1":"# STATCKING\n\nfrom sklearn.ensemble import StackingClassifier\n\nbase_learners = [\n    ('p1', pipe1), \n    ('p2', pipe2)\n]\n\nstack = StackingClassifier(estimators=base_learners, final_estimator=LogisticRegression())\nstack.fit(x_train,y_train)\ny_predicted = stack.predict(x_test)\nprint(\"Stacking:\")\nprint(\"Accuracy: \", accuracy_score(y_test, y_predicted))\nprint(classification_report(y_test, y_predicted))","384f079b":"# BAGGING\n\nfrom sklearn.ensemble import BaggingClassifier\n\nbag = BaggingClassifier(LogisticRegression(C=1000))\n\nbag.fit(vectCount.transform(x_train),y_train)\ny_predicted = bag.predict(vectCount.transform(x_test))\nprint(\"Bagging:\")\nprint(\"Accuracy: \", accuracy_score(y_test, y_predicted))\nprint(classification_report(y_test, y_predicted))","0e4fdce7":"# Boosting\n\nfrom sklearn.ensemble import AdaBoostClassifier\n\nboosting = AdaBoostClassifier(\n    LogisticRegression(C=1000),\n    n_estimators=200\n)\n\nboosting.fit(vectCount.transform(x_train),y_train)\ny_predicted = boosting.predict(vectCount.transform(x_test))\nprint(\"Boosting:\")\nprint(\"Accuracy: \", accuracy_score(y_test, y_predicted))\nprint(classification_report(y_test, y_predicted))","f838d4db":"# IPYNB in vscode and push repo to azure for voting classifier progress \n# Data visualisation\n# Reduce dataset size for more variance and redo gridsearch - function ","f63c3700":"# Ensemble methods\n\n### *Parallel ( Models are independent of one another ) :*\n\n### Simple voting classifier:\nHard voting - Each model's prediction is weighted equally, the most common predicted output is used.\nSoft voting - The probability of each prediction being correct is assigned as the weight and these are averaged to find the most relaible predcition.\nOverall provides impoves stability and predictive performance by combiing multiple different learning algorithms and aggregating the results.\n\n### Stacking:\nSimilar to voting as different models are used however, each model isnt neccesiarly weighted equally. All outputs are put through a final algorithm to then learn when to trust certain outputs or models more or less.\nWhen multiple base learner models are useful for a problem but in different ways stacking allows for improved performance with a meta learner. \n\n### Bagging:\nGenerate bootstrap samples, build and fit weak learners on each sample and take the average of all the predictions to produce a final output.\nThis method reduces the variance and produces a more generalise and consistent output.\n\n### Random forest:\nUses a diverse range of decision trees looking for different sets of features then votes on all the outputs to provide one.\nThis technique is effective in overcoming the issue decion trees can have of being overfitted to the training data and creates a more generalised algorithm.\n\n### *Sequential ( Models are dependent upon one another ) :*\n\n### Boosting:\nIn boosting the training set aims on focusing on previosuly misclassified data from the last model. Typically uses very\/fairly weak learners.\nGradient boosting (and extreme gradient boosting) - focuses on reducing the loss function overall.\nAdaptive boosting (AdaBoost) - Considers how miscalculated a model is and then assigns a weight and attempts to optimise the weighted sum of errors (developed from gradient boosting and tends to work very well).\nBoosting is aimed at reducing the overall bias a dataset may have by focusing on the areas that are often misclassified, if precision is a priority it is a good option.","0c14cd8c":"# Classifiers\n\n***Aim:*** Check the perfomance of the models individually for reference to a baseline accuracy that can be acheived and to compare whether the ensemble method makes an imporvement.\n\n### 1) Logistic regression: \nSuitable for a binary classifier (reviews are either positive or negative).\n\n### 2) Decision Tree:\nClassified more intractely at each stange analysing the data for a feature then sending it down to the relevant sub-category.\n"}}