{"cell_type":{"94220561":"code","23df7c43":"code","0a8c6a91":"code","c6d1a860":"code","f54bed0c":"code","ad0e8ad0":"code","51ba1e05":"code","d63d1730":"code","9478fcf7":"code","4245f105":"code","e3ccdfd3":"code","9ec48196":"code","85f1b634":"code","d4b9ebf5":"code","d3ec02eb":"code","ce04b52d":"code","4618f5a0":"code","810e48ec":"code","3d774351":"code","3a0c53f8":"code","7dd9e8cd":"code","568a37d2":"code","9b490c45":"code","a2caa04b":"code","a4a7c360":"code","2fb2decf":"code","874ce6d9":"code","caf4cf77":"code","13311536":"code","103ef6f8":"code","19fdb408":"code","10ee275b":"code","5c60d9d7":"code","ed73c9e6":"code","af99f08d":"code","2446ddb9":"code","b6f9e183":"code","676ea221":"code","52740fbe":"code","9d51d40c":"code","373fe983":"code","d2369787":"markdown","f835c044":"markdown","2bd33ad6":"markdown","fa269019":"markdown","844ced03":"markdown","e50f35e9":"markdown","e1abb705":"markdown","bb4d1dc0":"markdown","9d283d11":"markdown","f64fbdb5":"markdown","aa92e0fc":"markdown","faa8e796":"markdown","2ffde1f6":"markdown","0b3055e2":"markdown","52e7e825":"markdown","cc6f3354":"markdown","e829a8f4":"markdown","519b95a9":"markdown","aa324221":"markdown","07f0cdcb":"markdown","9d619c44":"markdown","d7c869ea":"markdown","7c063ea7":"markdown","423b68a0":"markdown","10d7b158":"markdown","a711b4b4":"markdown","d019d6f7":"markdown","f625c04b":"markdown","fc930068":"markdown","852818ef":"markdown","a78342af":"markdown","8181783c":"markdown","f2e3a82a":"markdown","94fed71f":"markdown","95df2df7":"markdown","9899bcdd":"markdown","8f0d346c":"markdown","0c3050d7":"markdown"},"source":{"94220561":"import os, sys\n\nimport numpy as np\nfrom scipy.stats import chi2_contingency\nimport pandas as pd\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline","23df7c43":"df = pd.read_csv('\/kaggle\/input\/heart-disease-uci\/heart.csv')\ndf.head(2)","0a8c6a91":"df.info()","c6d1a860":"def summarize_categoricals(df, show_levels=False):\n    \"\"\"\n        Display uniqueness in each column\n    \"\"\"\n    data = [[df[c].unique(), len(df[c].unique()), df[c].isnull().sum()] for c in df.columns]\n    df_temp = pd.DataFrame(data, index=df.columns,\n                           columns=['Levels', 'No. of Levels', 'No. of Missing Values'])\n    return df_temp.iloc[:, 0 if show_levels else 1:]\n\n\ndef find_categorical(df, cutoff=10):\n    \"\"\"\n        Function to find categorical columns in the dataframe.\n    \"\"\"\n    cat_cols = []\n    for col in df.columns:\n        if len(df[col].unique()) <= cutoff:\n            cat_cols.append(col)\n    return cat_cols\n\n\ndef to_categorical(columns, df):\n    \"\"\"\n        Converts the columns passed in `columns` to categorical datatype\n    \"\"\"\n    for col in columns:\n        df[col] = df[col].astype('category')\n    return df","f54bed0c":"summarize_categoricals(df, show_levels=True)","ad0e8ad0":"df = to_categorical(find_categorical(df), df)\ndf.info()","51ba1e05":"df.describe()","d63d1730":"numeric_fts = list(df.select_dtypes(exclude='category').columns)+['target']\n_, ax = plt.subplots(nrows=1, ncols=1, figsize=(10, 8))\ncorr_matrix = np.corrcoef(df[numeric_fts].astype('float64').values,\n                          rowvar=False)\nsns.heatmap(data=corr_matrix, annot=True, cmap='coolwarm', ax=ax);\nax.set_xticklabels(labels=numeric_fts)\nax.set_yticklabels(labels=numeric_fts);","9478fcf7":"def cramers_corrected_stat(contingency_table):\n    \"\"\"\n        Computes corrected Cramer's V statistic for categorial-categorial association\n    \"\"\"\n    chi2 = chi2_contingency(contingency_table)[0]\n    n = contingency_table.sum().sum()\n    phi2 = chi2\/n\n    \n    r, k = contingency_table.shape\n    r_corrected = r - (((r-1)**2)\/(n-1))\n    k_corrected = k - (((k-1)**2)\/(n-1))\n    phi2_corrected = max(0, phi2 - ((k-1)*(r-1))\/(n-1))\n    \n    return (phi2_corrected \/ min( (k_corrected-1), (r_corrected-1)))**0.5\n\n\ndef categorical_corr_matrix(df):\n    \"\"\"\n        Computes corrected Cramer's V statistic between\n        all the categorical variables in the dataframe\n    \"\"\"\n    df = df.select_dtypes(include='category')\n    cols = df.columns\n    n = len(cols)\n    corr_matrix = pd.DataFrame(np.zeros(shape=(n, n)), index=cols, columns=cols)\n    \n    for col1 in cols:\n        for col2 in cols:\n            if col1 == col2:\n                corr_matrix.loc[col1, col2] = 1\n                break\n            df_crosstab = pd.crosstab(df[col1], df[col2], dropna=False)\n            corr_matrix.loc[col1, col2] = cramers_corrected_stat(df_crosstab)\n    \n    # Flip and add to get full correlation matrix\n    corr_matrix += np.tril(corr_matrix, k=-1).T\n    return corr_matrix","4245f105":"fig, ax = plt.subplots(figsize=(15, 10))\nsns.heatmap(categorical_corr_matrix(df), annot=True, cmap='coolwarm', \n            cbar_kws={'aspect': 50}, square=True, ax=ax)\nplt.xticks(rotation=30, ha='right');\nplt.tight_layout()","e3ccdfd3":"_, axs = plt.subplots(nrows=3, ncols=3, figsize=(10, 10))\ncategorical_cols = list(df.select_dtypes(include='category').columns)\n\nax_title_pairs = zip(axs.flat, categorical_cols)\n\nfor ax, title in ax_title_pairs:\n    sns.countplot(x=''.join(title.lower().split()),\n                  data=df, palette='Pastel2', ax=ax)\n    ax.set_title(title.title())\n    ax.set_xlabel('')\n\nplt.tight_layout()","9ec48196":"df_grouped = df.groupby(by='target')\n_, axs = plt.subplots(nrows=2, ncols=3, figsize=(10, 5))\nnumeric_cols = list(df.select_dtypes(exclude='category').columns)\n\nax_title_pairs = zip(axs.flat, numeric_cols)\n\nfor ax, title in ax_title_pairs:\n    sns.distplot(df_grouped.get_group(0)[title],\n                 bins=10, ax=ax, label='Absent')\n    sns.distplot(df_grouped.get_group(1)[title],\n                 bins=10, ax=ax, label='Present')\n    ax.set_title(title)\n    ax.set_xlabel('')\n    ax.legend(title='target')\n\naxs[-1, -1].remove()\nplt.tight_layout()","85f1b634":"_, axs = plt.subplots(nrows=2, ncols=3, figsize=(15, 8))\nax_title_pairs = zip(axs.flat, numeric_cols)\n\nfor ax, col in ax_title_pairs:\n    sns.boxplot(x='target', y=col, data=df, ax=ax)\n\naxs[-1, -1].remove()\nplt.tight_layout()","d4b9ebf5":"x = df.iloc[:, :-1]\ny = df['target']\n\ncategorical_columns = list(x.select_dtypes(include='category').columns)\nnumeric_columns = list(x.select_dtypes(exclude='category').columns)","d3ec02eb":"from sklearn.model_selection import train_test_split\n\ndata_splits = train_test_split(x, y, test_size=0.20, random_state=0,\n                               shuffle=True, stratify=y)\nx_train, x_test, y_train, y_test = data_splits\n\n\n# For CatBoost and Naive Bayes\ndata_splits = train_test_split(x, y, test_size=0.20, random_state=0,\n                               shuffle=True, stratify=y)\nx_train_cat, x_test_cat, y_train_cat, y_test_cat = data_splits\n\n\nlist(map(lambda x: x.shape, [x, y, x_train, x_test, y_train, y_test]))","ce04b52d":"pd.Series(y_test).value_counts()","4618f5a0":"sns.countplot(x=y_test);","810e48ec":"from sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler, LabelEncoder\n\n\n## Column Transformer\ntransformers = [('one_hot_encoder',\n                  OneHotEncoder(drop='first',dtype='int'),\n                  categorical_columns),\n                ('standard_scaler', StandardScaler(), numeric_columns)]\nx_trans = ColumnTransformer(transformers, remainder='passthrough')\n\n## Applying Column Transformer\nx_train = x_trans.fit_transform(x_train)\nx_test = x_trans.transform(x_test)\n\n## Label encoding\ny_trans = LabelEncoder()\ny_train = y_trans.fit_transform(y_train)\ny_test = y_trans.transform(y_test)\n\n\n## Save feature names after one-hot encoding for feature importances plots\nfeature_names = list(x_trans.named_transformers_['one_hot_encoder'] \\\n                            .get_feature_names(input_features=categorical_columns))\nfeature_names = feature_names + numeric_columns","3d774351":"import timeit\nimport pickle\nimport sys\nfrom sklearn.metrics import confusion_matrix, classification_report, roc_auc_score, \\\n                            precision_recall_curve, roc_curve, accuracy_score\nfrom sklearn.exceptions import NotFittedError","3a0c53f8":"def confusion_plot(matrix, labels=None):\n    \"\"\" Display binary confusion matrix as a Seaborn heatmap \"\"\"\n    \n    labels = labels if labels else ['Negative (0)', 'Positive (1)']\n    \n    fig, ax = plt.subplots(nrows=1, ncols=1)\n    sns.heatmap(data=matrix, cmap='Blues', annot=True, fmt='d',\n                xticklabels=labels, yticklabels=labels, ax=ax)\n    ax.set_xlabel('PREDICTED')\n    ax.set_ylabel('ACTUAL')\n    ax.set_title('Confusion Matrix')\n    plt.close()\n    \n    return fig","7dd9e8cd":"def roc_plot(y_true, y_probs, label, compare=False, ax=None):\n    \"\"\" Plot Receiver Operating Characteristic (ROC) curve \n        Set `compare=True` to use this function to compare classifiers. \"\"\"\n    \n    fpr, tpr, thresh = roc_curve(y_true, y_probs, drop_intermediate=False)\n    auc = round(roc_auc_score(y_true, y_probs), 2)\n    \n    fig, axis = (None, ax) if ax else plt.subplots(nrows=1, ncols=1)\n    label = ' '.join([label, f'({auc})']) if compare else None\n    sns.lineplot(x=fpr, y=tpr, ax=axis,\n                 estimator=None, label=label)\n    \n    if compare:\n        axis.legend(title='Classifier (AUC)', loc='lower right')\n    else:\n        axis.text(0.72, 0.05, f'AUC = { auc }', fontsize=12,\n                  bbox=dict(facecolor='green', alpha=0.4, pad=5))\n            \n        # Plot No-Info classifier\n        axis.fill_between(fpr, fpr, tpr, alpha=0.3, edgecolor='g',\n                          linestyle='--', linewidth=2)\n        \n    axis.set_xlim(0, 1)\n    axis.set_ylim(0, 1)\n    axis.set_title('ROC Curve')\n    axis.set_xlabel('False Positive Rate [FPR]\\n(1 - Specificity)')\n    axis.set_ylabel('True Positive Rate [TPR]\\n(Sensitivity or Recall)')\n    \n    plt.close()\n    \n    return axis if ax else fig","568a37d2":"def precision_recall_plot(y_true, y_probs, label, compare=False, ax=None):\n    \"\"\" Plot Precision-Recall curve.\n        Set `compare=True` to use this function to compare classifiers. \"\"\"\n    \n    p, r, thresh = precision_recall_curve(y_true, y_probs)\n    p, r, thresh = list(p), list(r), list(thresh)\n    p.pop()\n    r.pop()\n    \n    fig, axis = (None, ax) if ax else plt.subplots(nrows=1, ncols=1)\n    \n    if compare:\n        sns.lineplot(r, p, estimator=None,\n                     ax=axis, label=label)\n        axis.set_xlabel('Recall')\n        axis.set_ylabel('Precision')\n        axis.legend(loc='lower left')\n    else:\n        sns.lineplot(thresh, p, estimator=None,\n                     label='Precision', ax=axis)\n        axis.set_xlabel('Threshold')\n        axis.set_ylabel('Precision')\n        axis.legend(loc='lower left')\n\n        axis_twin = axis.twinx()\n        sns.lineplot(thresh, r, estimator=None,\n                     color='limegreen', label='Recall', ax=axis_twin)\n        axis_twin.set_ylabel('Recall')\n        axis_twin.set_ylim(0, 1)\n        axis_twin.legend(bbox_to_anchor=(0.24, 0.18))\n    \n    axis.set_xlim(0, 1)\n    axis.set_ylim(0, 1)\n    axis.set_title('Precision Vs Recall')\n    \n    plt.close()\n    \n    return axis if ax else fig","9b490c45":"def feature_importance_plot(importances, feature_labels, ax=None):\n    fig, axis = (None, ax) if ax else plt.subplots(nrows=1, ncols=1, figsize=(5, 10))\n    sns.barplot(x=importances, y=feature_labels, ax=axis)\n    axis.set_title('Feature Importance Measures')\n    \n    plt.close()\n    \n    return axis if ax else fig","a2caa04b":"def train_clf(clf, x_train, y_train, sample_weight=None, refit=False):\n    train_time = 0\n    \n    try:\n        if refit:\n            raise NotFittedError\n        y_pred_train = clf.predict(x_train)\n    except NotFittedError:\n        start = timeit.default_timer()\n        \n        if sample_weight is not None:\n            clf.fit(x_train, y_train, sample_weight=sample_weight)\n        else:\n            clf.fit(x_train, y_train)\n        \n        end = timeit.default_timer()\n        train_time = end - start\n        \n        y_pred_train = clf.predict(x_train)\n    \n    train_acc = accuracy_score(y_train, y_pred_train)\n    return clf, y_pred_train, train_acc, train_time","a4a7c360":"def model_memory_size(clf):\n    return sys.getsizeof(pickle.dumps(clf))","2fb2decf":"def report(clf, x_train, y_train, x_test, y_test, sample_weight=None,\n           refit=False, importance_plot=False, confusion_labels=None,\n           feature_labels=None, verbose=True):\n    \"\"\" Trains the passed classifier if not already trained and reports\n        various metrics of the trained classifier \"\"\"\n    \n    dump = dict()\n    \n    ## Train if not already trained\n    clf, train_predictions, \\\n    train_acc, train_time = train_clf(clf, x_train, y_train,\n                                                     sample_weight=sample_weight,\n                                                     refit=refit)\n    ## Testing\n    start = timeit.default_timer()\n    test_predictions = clf.predict(x_test)\n    end = timeit.default_timer()\n    test_time = end - start\n    \n    test_acc = accuracy_score(y_test, test_predictions)\n    y_probs = clf.predict_proba(x_test)[:, 1]\n    \n    roc_auc = roc_auc_score(y_test, y_probs)\n    \n    \n    ## Model Memory\n    model_mem = round(model_memory_size(clf) \/ 1024, 2)\n    \n    print(clf)\n    print(\"\\n=============================> TRAIN-TEST DETAILS <======================================\")\n    \n    ## Metrics\n    print(f\"Train Size: {x_train.shape[0]} samples\")\n    print(f\" Test Size: {x_test.shape[0]} samples\")\n    print(\"------------------------------------------\")\n    print(f\"Training Time: {round(train_time, 3)} seconds\")\n    print(f\" Testing Time: {round(test_time, 3)} seconds\")\n    print(\"------------------------------------------\")\n    print(\"Train Accuracy: \", train_acc)\n    print(\" Test Accuracy: \", test_acc)\n    print(\"------------------------------------------\")\n    print(\" Area Under ROC: \", roc_auc)\n    print(\"------------------------------------------\")\n    print(f\"Model Memory Size: {model_mem} kB\")\n    print(\"\\n=============================> CLASSIFICATION REPORT <===================================\")\n    \n    ## Classification Report\n    clf_rep = classification_report(y_test, test_predictions, output_dict=True)\n    \n    print(classification_report(y_test, test_predictions,\n                                target_names=confusion_labels))\n    \n    \n    if verbose:\n        print(\"\\n================================> CONFUSION MATRIX <=====================================\")\n    \n        ## Confusion Matrix HeatMap\n        display(confusion_plot(confusion_matrix(y_test, test_predictions),\n                               labels=confusion_labels))\n        print(\"\\n=======================================> PLOTS <=========================================\")\n\n\n        ## Variable importance plot\n        fig, axes = plt.subplots(nrows=2, ncols=2, figsize=(14, 10))\n        roc_axes = axes[0, 0]\n        pr_axes = axes[0, 1]\n        importances = None\n\n        if importance_plot:\n            if not feature_labels:\n                raise RuntimeError(\"'feature_labels' argument not passed \"\n                                   \"when 'importance_plot' is True\")\n\n            try:\n                importances = pd.Series(clf.feature_importances_,\n                                        index=feature_labels) \\\n                                .sort_values(ascending=False)\n            except AttributeError:\n                try:\n                    importances = pd.Series(clf.coef_.ravel(),\n                                            index=feature_labels) \\\n                                    .sort_values(ascending=False)\n                except AttributeError:\n                    pass\n\n            if importances is not None:\n                # Modifying grid\n                grid_spec = axes[0, 0].get_gridspec()\n                for ax in axes[:, 0]:\n                    ax.remove()   # remove first column axes\n                large_axs = fig.add_subplot(grid_spec[0:, 0])\n\n                # Plot importance curve\n                feature_importance_plot(importances=importances.values,\n                                        feature_labels=importances.index,\n                                        ax=large_axs)\n                large_axs.axvline(x=0)\n\n                # Axis for ROC and PR curve\n                roc_axes = axes[0, 1]\n                pr_axes = axes[1, 1]\n            else:\n                # remove second row axes\n                for ax in axes[1, :]:\n                    ax.remove()\n        else:\n            # remove second row axes\n            for ax in axes[1, :]:\n                ax.remove()\n\n\n        ## ROC and Precision-Recall curves\n        clf_name = clf.__class__.__name__\n        roc_plot(y_test, y_probs, clf_name, ax=roc_axes)\n        precision_recall_plot(y_test, y_probs, clf_name, ax=pr_axes)\n\n        fig.subplots_adjust(wspace=5)\n        fig.tight_layout()\n        display(fig)\n    \n    ## Dump to report_dict\n    dump = dict(clf=clf, train_acc=train_acc, train_time=train_time,\n                train_predictions=train_predictions, test_acc=test_acc,\n                test_time=test_time, test_predictions=test_predictions,\n                test_probs=y_probs, report=clf_rep, roc_auc=roc_auc,\n                model_memory=model_mem)\n    \n    return clf, dump","874ce6d9":"def compare_models(y_test=None, clf_reports=[], labels=[]):\n    \"\"\" Compare evaluation metrics for the True Positive class [1] of \n        binary classifiers passed in the argument and plot ROC and PR curves.\n        \n        Arguments:\n        ---------\n        y_test: to plot ROC and Precision-Recall curves\n        \n        Returns:\n        -------\n        compare_table: pandas DataFrame containing evaluated metrics\n                  fig: `matplotlib` figure object with ROC and PR curves \"\"\"\n\n    \n    ## Classifier Labels\n    default_names = [rep['clf'].__class__.__name__ for rep in clf_reports]\n    clf_names =  labels if len(labels) == len(clf_reports) else default_names\n    \n    \n    ## Compare Table\n    table = dict()\n    index = ['Train Accuracy', 'Test Accuracy', 'Overfitting', 'ROC Area',\n             'Precision', 'Recall', 'F1-score', 'Support']\n    for i in range(len(clf_reports)):\n        train_acc = round(clf_reports[i]['train_acc'], 3)\n        test_acc = round(clf_reports[i]['test_acc'], 3)\n        clf_probs = clf_reports[i]['test_probs']\n        roc_auc = clf_reports[i]['roc_auc']\n        \n        # Get metrics of True Positive class from sklearn classification_report\n        true_positive_metrics = list(clf_reports[i]['report'][\"1\"].values())\n        \n        table[clf_names[i]] = [train_acc, test_acc,\n                               test_acc < train_acc, roc_auc] + true_positive_metrics\n    \n    table = pd.DataFrame(data=table, index=index)\n    \n    \n    ## Compare Plots\n    fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(15, 5))\n    \n    # ROC and Precision-Recall\n    for i in range(len(clf_reports)):\n        clf_probs = clf_reports[i]['test_probs']\n        roc_plot(y_test, clf_probs, label=clf_names[i],\n                 compare=True, ax=axes[0])\n        precision_recall_plot(y_test, clf_probs, label=clf_names[i],\n                              compare=True, ax=axes[1])\n    # Plot No-Info classifier\n    axes[0].plot([0,1], [0,1], linestyle='--', color='green')\n        \n    fig.tight_layout()\n    plt.close()\n    \n    return table.T, fig","caf4cf77":"from sklearn.naive_bayes import CategoricalNB, GaussianNB \nfrom sklearn.preprocessing import KBinsDiscretizer, OrdinalEncoder\n\nconfusion_lbs = ['Absent', 'Present']\n\n## Discretize 'monthlycharges' and 'totalcharges' into 3bins\nkbn = KBinsDiscretizer(n_bins=3, encode='ordinal')\node = OrdinalEncoder(dtype=np.int64)\nnb_trans = [('ordinal', ode, categorical_columns),\n            ('kbn', kbn, numeric_columns[1:])]\nnb_col_trans = ColumnTransformer(nb_trans, remainder='passthrough')\n\n## Applying Column Transformer\nx_train_nb = nb_col_trans.fit_transform(x_train_cat)\nx_test_nb = nb_col_trans.transform(x_test_cat)\n\nnb_clf = CategoricalNB()\n\nnb_clf, nb_report = report(nb_clf, x_train_nb, y_train,\n                           x_test_nb, y_test, refit=True,\n                           confusion_labels=confusion_lbs)","13311536":"from sklearn.linear_model import LogisticRegressionCV\n\nlogit_cv = LogisticRegressionCV(class_weight='balanced', cv=5, max_iter=500,\n                                scoring='f1', penalty='l2', solver='liblinear',\n                                n_jobs=-1, random_state=0, refit=True, verbose=0)\n\nlogit_cv, logit_report = report(logit_cv, x_train, y_train,\n                                x_test, y_test, refit=True,\n                                importance_plot=True,\n                                feature_labels=feature_names,\n                                confusion_labels=confusion_lbs)","103ef6f8":"from sklearn.neighbors import KNeighborsClassifier\n\nknn = KNeighborsClassifier(n_neighbors=12, p=2,\n                           weights='uniform', n_jobs=-1)\n\nknn, knn_report = report(knn, x_train, y_train,\n                         x_test, y_test,\n                         importance_plot=True,\n                         feature_labels=feature_names,\n                         confusion_labels=confusion_lbs)","19fdb408":"from sklearn.tree import DecisionTreeClassifier\n\ndecision_tree = DecisionTreeClassifier(class_weight='balanced',\n                                       criterion='entropy',\n                                       max_depth=2,\n                                       random_state=0)\n\ndecision_tree, decision_tree_report = report(decision_tree, x_train, y_train,\n                                             x_test, y_test,\n                                             importance_plot=True,\n                                             feature_labels=feature_names,\n                                             confusion_labels=confusion_lbs)","10ee275b":"from sklearn.ensemble import BaggingClassifier\n\nbagging_dtree = DecisionTreeClassifier(max_depth=2, class_weight='balanced',\n                                       criterion='entropy', random_state=0)\n\nbagging_clf = BaggingClassifier(base_estimator=bagging_dtree,\n                                max_samples=0.158, n_estimators=120,\n                                n_jobs=-1, random_state=0)\n\nbagging_clf, bagging_clf_report = report(bagging_clf, x_train, y_train,\n                                         x_test, y_test,\n                                         feature_labels=feature_names,\n                                         confusion_labels=confusion_lbs)","5c60d9d7":"from sklearn.ensemble import RandomForestClassifier\n\nrandom_forest = RandomForestClassifier(class_weight='balanced', criterion='entropy',\n                                       max_depth=1, n_estimators=100,\n                                       n_jobs=-1, random_state=0)\n\nrandom_forest, random_forest_report = report(random_forest, x_train, y_train,\n                                             x_test, y_test,\n                                             importance_plot=True,\n                                             feature_labels=feature_names,\n                                             confusion_labels=confusion_lbs)","ed73c9e6":"from sklearn.ensemble import AdaBoostClassifier\n\nboosting_dtree = DecisionTreeClassifier(class_weight='balanced',\n                                        criterion='entropy',\n                                        max_depth=1, random_state=0)\nadaboot = AdaBoostClassifier(base_estimator=boosting_dtree,\n                             n_estimators=100, learning_rate=0.063,\n                             random_state=0)\n\nadaboot, adaboot_report = report(adaboot, x_train, y_train,\n                                 x_test, y_test,\n                                 importance_plot=True,\n                                 feature_labels=feature_names,\n                                 confusion_labels=confusion_lbs)","af99f08d":"from sklearn.svm import SVC\n\nlinear_svc = SVC(C=4.4, kernel='linear', probability=True,\n                 class_weight='balanced', random_state=0)\n\nlinear_svc, linear_svc_report = report(linear_svc, x_train, y_train,\n                                       x_test, y_test,\n                                       importance_plot=True,\n                                       feature_labels=feature_names,\n                                       confusion_labels=confusion_lbs)","2446ddb9":"rbf_svc = SVC(C=0.3, kernel='rbf', probability=True,\n              class_weight='balanced', random_state=0)\n\nrbf_svc, rbf_svc_report = report(rbf_svc, x_train, y_train,\n                                 x_test, y_test,\n                                 importance_plot=True,\n                                 feature_labels=feature_names,\n                                 confusion_labels=confusion_lbs)","b6f9e183":"from xgboost import XGBClassifier\nfrom sklearn.utils import class_weight\n\n## Compute `class_weights` using sklearn\ncls_weight = (y_train.shape[0] - np.sum(y_train)) \/ np.sum(y_train)\n\nxgb_clf = XGBClassifier(learning_rate=0.014,\n                        scale_pos_weight=cls_weight,\n                        random_state=0, n_jobs=-1)\nxgb_clf.fit(x_train, y_train);\n\nxgb_clf, xgb_report = report(xgb_clf, x_train, y_train,\n                             x_test, y_test,\n                             importance_plot=True,\n                             feature_labels=feature_names,\n                             confusion_labels=confusion_lbs)","676ea221":"from catboost import CatBoostClassifier\n\n# Basic working\n\ncatboost_clf = CatBoostClassifier(cat_features=categorical_columns,\n                                  l2_leaf_reg=120, depth=6,\n                                  auto_class_weights='Balanced',\n                                  iterations=200, learning_rate=0.16,\n                                  use_best_model=True,\n                                  early_stopping_rounds=150,\n                                  eval_metric='F1', random_state=0)\n\ncatboost_clf.fit(x_train_cat, y_train, \n                 eval_set=(x_train_cat, y_train),\n                 verbose=False)\n\n\nf_labels = categorical_columns+numeric_columns\ncatboost_clf, catboost_report = report(catboost_clf, x_train_cat, y_train,\n                                       x_test_cat, y_test,\n                                       importance_plot=True,\n                                       feature_labels=f_labels,\n                                       confusion_labels=confusion_lbs)","52740fbe":"report_list = [nb_report, logit_report, knn_report, decision_tree_report,               \n               bagging_clf_report, random_forest_report, adaboot_report,\n               xgb_report, linear_svc_report, rbf_svc_report, catboost_report]\nclf_labels = [rep['clf'].__class__.__name__ for rep in report_list]\nclf_labels[-3], clf_labels[-2] = 'Linear SVC', 'RBF SVC'","9d51d40c":"compare_table, compare_plot = compare_models(y_test, clf_reports=report_list, labels=clf_labels)\n\ncompare_table.sort_values(by=['Overfitting', 'F1-score'],\n                          ascending=[True, False])","373fe983":"compare_plot","d2369787":"<a id=\"utility-functions\"><\/a>\n## 5.1.   Utility Functions","f835c044":"<a id=\"checking-for-null-values-in-the-dataset\"><\/a>\n## 1.2. Checking for null values in the dataset","2bd33ad6":"<a id=\"frequency-distribution%3A-continuous-variables\"><\/a>\n## 3.2. Frequency Distribution: Continuous Variables","fa269019":"<a id=\"import-and-clean-data\"><\/a>\n# 1. Import and Clean data","844ced03":"<a id=\"data-visualization\"><\/a>\n# 3. Data Visualization","e50f35e9":"There are no null values.","e1abb705":"<a id=\"dataset-details\"><\/a>\n## 1.1. Dataset details\nsex - (1 = male; 0 = female)  \nfbs - (1 = true; 0 = false)  \nexang - (1 = yes; 0 = no)  \nthal - (3 = normal; 6 = fixed defect; 7 = reversable defect)","bb4d1dc0":"<a id=\"random-forests\"><\/a>\n## 5.7.   Random Forests","9d283d11":"**Highlights:**\n> 1. Correlation analysis using Pearson coefficient for continuous features and Cramer's V for categorical features\n>\n> 2. Classification of the imbalanced dataset using `class weighted` or `cost sensitive` learning\n>\n> 3. Tried CatBoost classifier\n>\n> 4. Results for all ML algorithms are presented after performing 5-fold cross validation based on F1-score","f64fbdb5":"<a id=\"data-preprocessing\"><\/a>\n# 4.   Data Preprocessing\nData needs to be one-hot-encoded before applying machine learning models.","aa92e0fc":"<a id=\"decision-tree\"><\/a>\n## 5.5.   Decision Tree","faa8e796":"<a id=\"correlation-between-qualitative\/-categorical-variables\"><\/a>\n## 2.2. Correlation between Qualitative\/ Categorical variables\n`Cramer's V` is more appropriate than Pearson correlation to find correlation between two nominal variables. Here, the `Cramer's V` metric is implemented.","2ffde1f6":"***Inference:*** The dataset is imbalanced as can be seen from the Target plot.","0b3055e2":"<a id=\"catboost\"><\/a>\n## 5.12.   CatBoost\nCat boost performs better without One-hot encoding because it performs an internal categorical encoding that is similar to Leave One Out Encoding (LOOE). So, we can give the dataframe as input to the catboost classifier.","52e7e825":"# Heart Disease Prediction using Cost Sensitive Learning","cc6f3354":"<a id=\"k-nearest-neighbors\"><\/a>\n## 5.4.   K-Nearest Neighbors\nKNN estimator in Scikit-learn does not provide a way to pass class-weights to enable cost-sensitive\/ class-weighted learning.","e829a8f4":"<a id=\"correlation-between-quantitative-variables\"><\/a>\n## 2.1. Correlation between Quantitative variables","519b95a9":"<a id=\"data-modeling\"><\/a>\n# 5.   Data Modeling\nSince the dataset is imbalanced we will be using class-weighted\/ cost-sensitive learning. In cost-sensitive learning, a weighted cost function is used. Therefore, misclassifying a sample from the minority class will cost the classifiers more than misclassifying a sample from the majority class. In most of the Sklearn classifiers, cost-sensitive learning can be enabled by setting `class_weight='balanced'`.","aa324221":"<a id=\"train-test-split\"><\/a>\n## 4.1.   Train-Test split\nCatBoost classifier does not require any knd of preprocessing while Naive bayes requires a different kind of preprocesing. Therefore, we will use raw\/ unmodified data (`x_train_cat, x_test_cat, y_train_cat, y_test_cat`) for CatBoost and preprocessed data (`x_train, x_test, y_train, y_test`) for all other classifiers. For Naive Bayes, we will use the raw data (`x_train_cat, x_test_cat, y_train_cat, y_test_cat`) and preprocess it as required in the Naive Bayes section.","07f0cdcb":"<a id=\"search-for-categorical-columns-and-cast-them-to-%60pd.categorical%60\"><\/a>\n## 1.3. Search for categorical columns and cast them to `pd.Categorical`\nWe need to manually identify categorical columns in the data before casting them to `pd.Categorical`. Casting categorical columns from the detected *object* type to *categorical* will ease visualization.","9d619c44":"<a id=\"model-comparison\"><\/a>\n# 6.   Model Comparison\nSince input data format for Naive Bayes and CatBoost are different, we will add them to the comparison manually.","d7c869ea":"<a id=\"decision-trees-with-bagging\"><\/a>\n## 5.6.   Decision Trees with Bagging","7c063ea7":"***Inference:*** We can see that among the classifiers that do not overfit, Linear SVC has the highest F1-score.\n\n***Note:-*** Please note that the overfitting is in terms of accuracy and NOT in terms of F1 score since 5-fold CV in done based on F1 score. Therefore, although most classifiers are overfitting in terms of accuracy they are not overfitting in terms of F1 score.","423b68a0":"<a id=\"linear-svc\"><\/a>\n## 5.9.   Linear SVC","10d7b158":"<a id=\"logistic-regression\"><\/a>\n## 5.3.   Logistic Regression","a711b4b4":"<a id=\"svm-with-rbf-kernel\"><\/a>\n## 5.10.   SVM with RBF kernel","d019d6f7":"<a id=\"box-plots\"><\/a>\n## 3.3.   Box Plots","f625c04b":"<a id=\"correlations-in-the-data\"><\/a>\n# 2. Correlations in the data","fc930068":"<a id=\"frequency-distribution%3A-categorical-variables\"><\/a>\n## 3.1. Frequency Distribution: Categorical Variables","852818ef":"***Inference:*** ***sex, cp, exang, , slope, ca, and thal*** are highly correlated with the target.","a78342af":"* [1. Import and Clean data](#import-and-clean-data)\n    * [1.1. Dataset details](#dataset-details)\n    * [1.2. Checking for null values in the dataset](#checking-for-null-values-in-the-dataset)\n    * [1.3. Search for categorical columns and cast them to `pd.Categorical`](#search-for-categorical-columns-and-cast-them-to-%60pd.categorical%60)\n* [2. Correlations in the data](#correlations-in-the-data)\n    * [2.1. Correlation between Quantitative variables](#correlation-between-quantitative-variables)\n    * [2.2. Correlation between Qualitative\/ Categorical variables](#correlation-between-qualitative\/-categorical-variables)\n* [3. Data Visualization](#data-visualization)\n    * [3.1. Frequency Distribution: Categorical Variables](#frequency-distribution%3A-categorical-variables)\n    * [3.2. Frequency Distribution: Continuous Variables](#frequency-distribution%3A-continuous-variables)\n    * [3.3.   Box Plots](#box-plots)\n* [4.   Data Preprocessing](#data-preprocessing)\n    * [4.1.   Train-Test split](#train-test-split)\n    * [4.2.   One-hot Encoding and Standardization](#one-hot-encoding-and-standardization)\n* [5.   Data Modeling](#data-modeling)\n    * [5.1.   Utility Functions](#utility-functions)\n    * [5.2.   Naive Bayes](#naive-bayes)\n    * [5.3.   Logistic Regression](#logistic-regression)\n    * [5.4.   K-Nearest Neighbors](#k-nearest-neighbors)\n    * [5.5.   Decision Tree](#decision-tree)\n    * [5.6.   Decision Trees with Bagging](#decision-trees-with-bagging)\n    * [5.7.   Random Forests](#random-forests)\n    * [5.8.   Decision Trees with AdaBoost](#decision-trees-with-adaboost)\n    * [5.9.   Linear SVC](#linear-svc)\n    * [5.10.   SVM with RBF kernel](#svm-with-rbf-kernel)\n    * [5.11.   XGBoost](#xgboost)\n    * [5.12.   CatBoost](#catboost)\n* [6.   Model Comparison](#model-comparison)\n    * [6.1.   Evaluation Metrics](#evaluation-metrics)\n    * [6.2.  ROC and PR Curves](#roc-and-pr-curves)","8181783c":"<a id=\"naive-bayes\"><\/a>\n## 5.2.   Naive Bayes\nThe fundamental assumption made by Naive Bayes regarding the data is ***class conditional independence of features***. Sklearn provides different variants of Naive Bayes depending on whether the features follow a categorical distribution (CategoricalNB), normal distribution (GaussianNB), bernoulli distribution (BernoulliNB), multinomial distribution (MultinomialNB).\n\nSince majority of the features are categorical and follow a categorical distribution, we will use CategoricalNB. Continuous features will be discretized.","f2e3a82a":"<a id=\"xgboost\"><\/a>\n## 5.11.   XGBoost","94fed71f":"<a id=\"one-hot-encoding-and-standardization\"><\/a>\n## 4.2.   One-hot Encoding and Standardization\nWe need to standardize the continuous or quantitative variables\/ features before applying Machine Learning models. This is important because if we don't standardize the features, features with high variance that are orders of magnitude larger that others might dominate the model fitting process and causing the model unable to learn from other features (with lower variance) correctly as expected. <br\/>\nThere is no need to standardize categorical variables.\n\n***Also we need to standardize the data only after performing train-test split because if we standardize before splitting then there is a chance for some information leak from the test set into the train set. We always want the test set to be completely new to the ML models. [Read more](https:\/\/scikit-learn.org\/stable\/modules\/compose.html#columntransformer-for-heterogeneous-data)***","95df2df7":"<a id=\"roc-and-pr-curves\"><\/a>\n## 6.2.  ROC and PR Curves","9899bcdd":"<a id=\"decision-trees-with-adaboost\"><\/a>\n## 5.8.   Decision Trees with AdaBoost\nThe default base estimator for `AdaBoostClassifier` is `DecisionTreeClassifier(max_depth=1)`","8f0d346c":"<a id=\"evaluation-metrics\"><\/a>\n## 6.1.   Evaluation Metrics","0c3050d7":"**Thank You!!**"}}