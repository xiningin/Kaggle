{"cell_type":{"64732ebe":"code","674f3715":"code","5a09dd52":"code","2c895438":"code","b5e33f1c":"code","0eb7dc90":"code","3c5843f9":"code","320c29ef":"code","9fddeb7a":"code","58d91d8d":"code","fd912ad0":"code","10ba816b":"code","f0a42163":"code","d62e8a54":"code","2d64f3b1":"code","37ced194":"code","8e2b7722":"code","ebd05b8a":"code","7f54a756":"code","0a974921":"code","99e1ccf0":"code","2a1f0012":"code","310224a3":"code","a9fede62":"code","ea128064":"code","6cb4658a":"code","2f926918":"code","63561ee6":"code","5247220e":"code","bc1a17a8":"code","b5d00453":"code","1f93a5a1":"code","b7fd9027":"code","745098f7":"code","d25fe99e":"code","1fafab4f":"code","af7c7a49":"code","094aee28":"code","80f971e4":"code","22ecd0e5":"code","ea214a14":"code","254d25de":"code","a534c43a":"code","8bf957b6":"code","f0178e50":"code","2dbb111a":"code","16c30151":"code","5de63833":"code","5f13514c":"code","5aab89d9":"code","b635beca":"code","bf302d1d":"code","a5fbe98d":"code","63cade58":"code","21373f13":"code","4719f9d4":"code","771f2c24":"code","86d93dd2":"code","48951936":"code","b572aaa4":"code","c4307511":"code","4411c5fc":"code","a3c83744":"code","c13e1538":"code","ac22078b":"code","d9dd2ea7":"code","a09ca6f0":"code","7d665626":"markdown","97d7a76a":"markdown","a6a5ed12":"markdown","d5296279":"markdown","358f8c25":"markdown","8234a7e1":"markdown","6c3381a3":"markdown","8e1ee4c1":"markdown","d79ec665":"markdown"},"source":{"64732ebe":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport torch.nn.functional as F\nfrom sklearn import preprocessing\nfrom sklearn.model_selection import train_test_split, cross_val_score, LeaveOneOut\nfrom sklearn.preprocessing import OneHotEncoder, LabelEncoder\nfrom sklearn.metrics import f1_score, accuracy_score, confusion_matrix, classification_report","674f3715":"impt_data = pd.read_csv(\"train.csv\")\ntest_data = pd.read_csv(\"test.csv\")","5a09dd52":"impt_data","2c895438":"impt_data.drop(5742, inplace = True)","b5e33f1c":"expt = impt_data[impt_data[\"ServiceSpan\"] <= 0]","0eb7dc90":"(1) * expt[\"QuarterlyPayment\"]","3c5843f9":"expt[\"GrandPayment\"]","320c29ef":"# impt_data[\"ServiceSpan\"].replace([0, -1], 1, inplace = True)","9fddeb7a":"(impt_data[\"ServiceSpan\"]) * impt_data[\"QuarterlyPayment\"]","58d91d8d":"impt_data[\"GrandPayment\"]","fd912ad0":"def_data = impt_data[impt_data[\"CustomerAttrition\"] == \"No\"]\ngood_data = impt_data[impt_data[\"CustomerAttrition\"] == \"Yes\"]","10ba816b":"impt_data.describe()","f0a42163":"impt_data.info()","d62e8a54":"def percentage_hue(attribute):\n    fig, ax = plt.subplots(figsize=(20, 6))\n\n    attribute_counts = (impt_data.groupby([attribute])['CustomerAttrition']\n                         .value_counts(normalize=True)\n                         .rename('percentage')\n                         .mul(100)\n                         .reset_index()\n                         .sort_values('percentage'))\n    p = sns.barplot(x=attribute, y=\"percentage\", hue=\"CustomerAttrition\", data=attribute_counts)\n    _ = plt.setp(p.get_xticklabels(), rotation=90)  # Rotate labels\n\n    for p in ax.patches:\n        ax.annotate('{:.1f}'.format(p.get_height()), (p.get_x()+0.1, p.get_height()))","2d64f3b1":"def density_plot(attribute):\n    fig, ax = plt.subplots(2, 1, figsize=(12, 12))\n\n    sns.kdeplot(data=good_data, x=attribute, ax=ax[0], shade=True, hue_order=[1, 0], alpha=1)\n    sns.kdeplot(data=def_data,  x=attribute, ax=ax[1], shade=True, hue_order=[1, 0], alpha=1)\n    \n    ax[0].set_title(\"Yes\")\n    ax[1].set_title(\"No\")\n    \n    plt.show()","37ced194":"impt_data.columns","8e2b7722":"percentage_hue(\"sex\")","ebd05b8a":"percentage_hue(\"Aged\")","7f54a756":"percentage_hue(\"Married\")","0a974921":"percentage_hue(\"TotalDependents\")","99e1ccf0":"density_plot(\"ServiceSpan\")","2a1f0012":"percentage_hue(\"MobileService\")","310224a3":"percentage_hue(\"4GService\")","a9fede62":"percentage_hue(\"CyberProtection\")","ea128064":"percentage_hue(\"HardwareSupport\")","6cb4658a":"percentage_hue(\"TechnicalAssistance\")","2f926918":"percentage_hue(\"FilmSubscription\")","63561ee6":"percentage_hue(\"SettlementProcess\")","5247220e":"density_plot(\"QuarterlyPayment\")","bc1a17a8":"density_plot(\"GrandPayment\")","b5d00453":"sns.heatmap(impt_data.corr(), annot=True)","1f93a5a1":"impt_data[\"CustomerAttrition\"].value_counts()","b7fd9027":"include_columns = ['Aged', 'Married', 'TotalDependents', 'ServiceSpan', 'CyberProtection', 'HardwareSupport',\n                   'TechnicalAssistance', 'SettlementProcess',\n                   'QuarterlyPayment', 'GrandPayment']\n\ndata_pd = impt_data[include_columns]\ndata_test_pd = test_data[include_columns]\n\ndata_pd.replace([np.inf, -np.inf], np.nan, inplace=True)\ndata_pd.fillna(999, inplace=True)\ndata_test_pd.replace([np.inf, -np.inf], np.nan, inplace=True)\ndata_test_pd.fillna(999, inplace=True)","745098f7":"def OH_enc(attribute):\n    global data_pd, data_test_pd\n    \n    attribute_series = data_pd.pop(attribute)\n    attribute_test_series = data_test_pd.pop(attribute)\n\n    y = pd.get_dummies(attribute_series)\n    data_pd = pd.concat([data_pd, y], axis=1, join=\"inner\")\n\n    y = pd.get_dummies(attribute_test_series)\n    data_test_pd = pd.concat([data_test_pd, y], axis=1, join=\"inner\")","d25fe99e":"def lab_enc(attribute):\n    global data_pd, data_test_pd\n    \n    le = LabelEncoder()\n    att = pd.concat([data_pd[attribute], data_test_pd[attribute]], axis=0)\n    le.fit(att)\n\n    data_pd[attribute] = le.transform(data_pd[attribute])\n    data_test_pd[attribute] = le.transform(data_test_pd[attribute])","1fafab4f":"bc_labels_pd = impt_data[\"CustomerAttrition\"]","af7c7a49":"impt_data.columns","094aee28":"# Categorical boolean mask\ncategorical_feature_mask = data_pd.dtypes==object\n\n# filter categorical columns using mask and turn it into a list\ncategorical_cols = data_pd.columns[categorical_feature_mask].tolist()","80f971e4":"for col in categorical_cols:\n    lab_enc(col)\n#     OH_enc(col)","22ecd0e5":"data = data_pd.to_numpy()\nX_test = data_test_pd.to_numpy()\ndata = preprocessing.scale(data)\nX_test = preprocessing.scale(X_test)\nlabels = bc_labels_pd.to_numpy()\nprint(data_pd.shape)\nprint(X_test.shape)\nprint(labels.shape)","ea214a14":"sns.heatmap(data_pd.corr(), annot=True)","254d25de":"from sklearn.decomposition import PCA\nfrom sklearn.preprocessing import StandardScaler\n\npca = PCA(n_components = 10)\ndata = StandardScaler().fit_transform(data)\n# data = pca.fit_transform(data)\n# principalDf = pd.DataFrame(data = principalComponents\n#              , columns = ['principal component 1', 'principal component 2'])","a534c43a":"# finalDf = pd.concat([principalDf, df[['target']]], axis = 1)","8bf957b6":"def accuracy_ml(Y_hat, Y):\n    return(classification_report(Y_hat, Y))","f0178e50":"def visualize(model):    \n    print(\"Train: \", accuracy_ml(Y_train, model.predict(X_train)))\n    cm = confusion_matrix(Y_train, model.predict(X_train))\n    print(cm)\n    plt.imshow(cm, cmap='binary')\n    \n    print(\"Val: \", accuracy_ml(Y_val, model.predict(X_val)))\n    cm = confusion_matrix(Y_val, model.predict(X_val))\n    print(cm)\n    plt.imshow(cm, cmap='binary')\n    \n    print(\"Test: \", accuracy_ml(Y_Test, model.predict(X_Test)))\n    cm = confusion_matrix(Y_Test, model.predict(X_Test))\n    print(cm)\n    plt.imshow(cm, cmap='binary')","2dbb111a":"data.shape","16c30151":"X_temp, X_Test, Y_temp, Y_Test = train_test_split(data, labels, random_state = 10, test_size = 0.2, shuffle=True, stratify=labels)","5de63833":"ar_nan = np.where(np.isnan(X_temp))\nprint (ar_nan)","5f13514c":"from imblearn.pipeline import Pipeline\nfrom imblearn.over_sampling import SMOTE\nfrom imblearn.under_sampling import RandomUnderSampler\n\nover = SMOTE(sampling_strategy = 0.3)\nunder = RandomUnderSampler(sampling_strategy = 0.5)\n\nsteps = [('o', over), ('u', under)]\n# steps = [('o', over)]\npipeline = Pipeline(steps=steps)\n\nX_temp, Y_temp = pipeline.fit_resample(X_temp, Y_temp)","5aab89d9":"X_train, X_val, Y_train, Y_val = train_test_split(X_temp, Y_temp, random_state = 100, test_size = 0.2, shuffle=True, stratify=Y_temp)","b635beca":"X_train.shape","bf302d1d":"np.count_nonzero(Y_train == \"No\")","a5fbe98d":"from sklearn.ensemble import RandomForestClassifier\nrfc = RandomForestClassifier()\nrfc.fit(X_train,Y_train)","63cade58":"visualize(rfc)","21373f13":"# from sklearn.model_selection import GridSearchCV\n# # Create the parameter grid based on the results of random search \n# param_grid = {\n#     'bootstrap': [True],\n#     'max_depth': [80, 90, 100, 110],\n#     'max_features': [2, 3],\n#     'min_samples_leaf': [3, 4, 5],\n#     'min_samples_split': [8, 10, 12],\n#     'n_estimators': [100, 200, 300, 1000]\n# }\n# # Create a based model\n# rf = RandomForestClassifier()\n# # Instantiate the grid search model\n# grid_search = GridSearchCV(estimator = rf, param_grid = param_grid, \n#                           cv = 3, n_jobs = -1, verbose = 2)","4719f9d4":"# # Fit the grid search to the data\n# grid_search.fit(X_train, Y_train)\n# grid_search.best_params_","771f2c24":"# best_grid = grid_search.best_estimator_\n# grid_accuracy = visualize(best_grid)","86d93dd2":"from sklearn.svm import SVC, LinearSVC, NuSVC\n\nsvc = SVC(kernel=\"linear\", C=0.05, probability=True)\nsvc.fit(X_train, Y_train)","48951936":"visualize(svc)","b572aaa4":"from sklearn.ensemble import AdaBoostClassifier\n\nabc = AdaBoostClassifier(n_estimators=100)\nabc.fit(X_train, Y_train)","c4307511":"visualize(abc)","4411c5fc":"from sklearn.ensemble import GradientBoostingClassifier\n\ngbc = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0,\n     max_depth=1, random_state=0)\ngbc.fit(X_train, Y_train)","a3c83744":"visualize(gbc)","c13e1538":"from sklearn.linear_model import LogisticRegression\n\nlr = LogisticRegression(random_state=0)\nlr.fit(X_train, Y_train)","ac22078b":"visualize(lr)","d9dd2ea7":"from sklearn.metrics import accuracy_score, log_loss\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC, LinearSVC, NuSVC\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n\nclassifiers = [\n    KNeighborsClassifier(30),\n    SVC(kernel=\"rbf\", C=0.025, probability=True),\n    NuSVC(probability=True),\n    DecisionTreeClassifier(),\n    RandomForestClassifier(),\n    AdaBoostClassifier(),\n    GradientBoostingClassifier(),\n    GaussianNB(),\n    LinearDiscriminantAnalysis(),\n    QuadraticDiscriminantAnalysis()]\n\n# Logging for Visual Comparison\nlog_cols=[\"Classifier\", \"Accuracy\", \"Log Loss\"]\nlog = pd.DataFrame(columns=log_cols)\n\nfor clf in classifiers:\n    clf.fit(X_train, Y_train)\n    name = clf.__class__.__name__\n    \n    print(\"=\"*30)\n    print(name)\n    \n    print('****Results****')\n    train_predictions = clf.predict(X_Test)\n    acc = accuracy_score(Y_Test, train_predictions)\n    print(\"Accuracy: {:.4%}\".format(acc))\n    \n    train_predictions = clf.predict_proba(X_Test)\n    ll = log_loss(Y_Test, train_predictions)\n    print(\"Log Loss: {}\".format(ll))\n    \n    log_entry = pd.DataFrame([[name, acc*100, ll]], columns=log_cols)\n    log = log.append(log_entry)\n    \nprint(\"=\"*30)","a09ca6f0":"import csv\nY_test_pred = clf1.predict(X_test)\n\nwith open('submission.csv', 'w', newline='') as file:\n    writer = csv.writer(file)\n    writer.writerow(['ID', 'CustomerAttrition'])\n    for i, row in enumerate(Y_test_pred):\n        writer.writerow([test_data[\"ID\"][i], row])\n\nfile.close()","7d665626":" # Encoding","97d7a76a":"# Import Data","a6a5ed12":"# Train Test Split","d5296279":"# Setup","358f8c25":"# Accuracy Score","8234a7e1":"# Exp Data Analysis","6c3381a3":"# Pre Processing","8e1ee4c1":"# Saving Model","d79ec665":"# PCA"}}