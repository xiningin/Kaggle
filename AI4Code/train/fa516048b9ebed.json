{"cell_type":{"e6ca6aa0":"code","a0f4620d":"code","8b3dc113":"code","f11f587a":"code","eb3c3222":"code","a38158d7":"code","5219ccd1":"code","eecf35ca":"code","642ea1b6":"code","bded94c5":"code","77d4bc43":"code","c8485b77":"code","6dad80b4":"code","772340e8":"code","c9c825b8":"code","74a62d11":"code","1e407569":"code","f16f8833":"code","f96d8007":"code","7bf6a41e":"code","9b8aabf9":"code","c8b87905":"code","54d79dfa":"code","47e88aa2":"code","42e2b91a":"code","7ac45767":"code","d7496a36":"code","d6b24e87":"code","8ff21bde":"code","8f0bb204":"code","7810e66a":"code","72830100":"code","2aef9d8a":"code","84968d3c":"code","b20c31f7":"code","8f9200ef":"code","75e6dc56":"code","14f62a7f":"code","958aea6b":"code","74e972a7":"code","ee9a6c8d":"code","0e99d352":"code","639f5247":"code","c571618f":"code","7dfcbbbe":"code","fde45b0e":"code","8b97f6aa":"code","96bb894b":"code","74cfa7ea":"code","f56bf303":"code","7d348d91":"code","a7022fbe":"code","64fa244d":"code","24e30b5a":"code","00f62273":"code","dd161992":"code","b304649b":"markdown","bda30a76":"markdown","4347c334":"markdown","d55749ea":"markdown","c0532c8a":"markdown","d20ce88b":"markdown"},"source":{"e6ca6aa0":"import os, gc, pickle, copy, datetime, warnings, time, tqdm\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport lightgbm as lgb\nfrom sklearn import metrics\npd.set_option('display.max_columns', 100)\nwarnings.filterwarnings('ignore')\n\nFULL_RUN = True\n\n# Listing files\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","a0f4620d":"if FULL_RUN:\n    df_train = pd.read_csv(\"..\/input\/covid19-global-forecasting-week-4\/train.csv\")\n    df_test = pd.read_csv(\"..\/input\/covid19-global-forecasting-week-4\/test.csv\")\n    df_traintest = pd.concat([df_train, df_test])\n    \n    df_latlong = pd.read_csv(\"..\/input\/smokingstats\/df_Latlong.csv\")\n    \n    df_smoking = pd.read_csv(\"..\/input\/smokingstats\/share-of-adults-who-smoke.csv\")\n    \n    df_weo = pd.read_csv(\"..\/input\/smokingstats\/WEO.csv\")\n    \n    df_life = pd.read_csv(\"..\/input\/smokingstats\/Life expectancy at birth.csv\")\n    \n    df_country = pd.read_csv(\"..\/input\/countryinfo\/covid19countryinfo.csv\")\n    \nelse:\n    df_train = pd.read_csv('\/kaggle\/working\/df_train.csv')\n    df_test = pd.read_csv('\/kaggle\/working\/df_test.csv')","8b3dc113":"if FULL_RUN:\n    start_time = time.time()\n    \n    df_traintest['Date'] = pd.to_datetime(df_traintest['Date'])\n    df_traintest['day'] = df_traintest['Date'].apply(lambda x: x.dayofyear).astype(np.int16)\n    df_traintest.head()\n    \n    # ---\n\n    day_before_valid = 93-7 # 3\/26, the day before of validation\n    day_before_public = 93# 4\/2, the last day of train\n    day_before_private = df_traintest['day'][pd.isna(df_traintest['ForecastId'])].max() # the last day of train\n    print(df_traintest['Date'][df_traintest['day']==day_before_valid].values[0])\n    print(df_traintest['Date'][df_traintest['day']==day_before_public].values[0])\n    print(df_traintest['Date'][df_traintest['day']==day_before_private].values[0])\n    \n    # ---\n\n    def func(x):\n        try:\n            x_new = x['Country_Region'] + \"\/\" + x['Province_State']\n        except:\n            x_new = x['Country_Region']\n        return x_new\n\n    df_traintest['place_id'] = df_traintest.apply(lambda x: func(x), axis=1)\n    df_traintest.head()\n    \n    # ---\n    \n    df_traintest[(df_traintest['day']>=day_before_public-3) & (df_traintest['place_id']=='China\/Hubei')].head()\n    \n    # ---\n    \n    def func(x):\n        try:\n            x_new = x['Country\/Region'] + \"\/\" + x['Province\/State']\n        except:\n            x_new = x['Country\/Region']\n        return x_new\n        \n    df_latlong['place_id'] = df_latlong.apply(lambda x: func(x), axis=1)\n    df_latlong = df_latlong[df_latlong['place_id'].duplicated()==False]\n    df_latlong.head()\n    \n    print(f\"consumption time:{time.time()-start_time}[s]\")","f11f587a":"df_happiness = pd.read_csv(\"..\/input\/world-happiness\/2019.csv\")\ndf_happiness = df_happiness.rename(columns={\"Country or region\":\"Country_Region\", \"Score\":\"happiness_Score\"\n                                            , \"GDP per capita\":\"happiness_GDP\", \"Social support\":\"happiness_Social_support\"\n                                            , \"Healthy life expectancy\":\"happiness_Healthy_lifeEx\"\n                                            , \"Freedom to make life choices\":\"happiness_Freedom\"\n                                            , \"Generosity\":\"happiness_Generosity\"\n                                            , \"Perceptions of corruption\":\"happiness_Corruption\"})","eb3c3222":"df_traintest.head()\ndf_happiness.head()","a38158d7":"df_traintest_happiness = pd.merge(df_traintest, df_happiness[[\n                                                    \"Country_Region\", \"happiness_Score\", \"happiness_GDP\"\n                                                    , \"happiness_Social_support\", \"happiness_Healthy_lifeEx\", \"happiness_Freedom\"\n                                                    , \"happiness_Generosity\", \"happiness_Corruption\"]], on='Country_Region', how='left')\n\ndf_traintest_happiness.head()","5219ccd1":"#df_traintest_happiness[df_traintest_happiness['happiness_Score'].isnull()]\n\n# fill nan by mean\ndf_traintest_happiness['happiness_Score'].fillna(df_traintest_happiness['happiness_Score'].mean())\ndf_traintest_happiness['happiness_GDP'].fillna(df_traintest_happiness['happiness_GDP'].min())\ndf_traintest_happiness['happiness_Social_support'].fillna(df_traintest_happiness['happiness_Social_support'].min())\ndf_traintest_happiness['happiness_Healthy_lifeEx'].fillna(df_traintest_happiness['happiness_Healthy_lifeEx'].min())\ndf_traintest_happiness['happiness_Freedom'].fillna(df_traintest_happiness['happiness_Freedom'].mean())\ndf_traintest_happiness['happiness_Generosity'].fillna(df_traintest_happiness['happiness_Generosity'].mean())\ndf_traintest_happiness['happiness_Corruption'].fillna(df_traintest_happiness['happiness_Corruption'].max())\n\ndf_traintest = df_traintest_happiness","eecf35ca":"\nif FULL_RUN:\n    start_time = time.time()\n    \n    df_tmp = df_traintest[['place_id']][df_traintest['place_id'].duplicated()==False]\n    df_tmp = pd.merge(df_tmp, df_latlong[['place_id', 'Lat', 'Long']], on='place_id', how='left')\n    tmp = df_latlong[['Lat', 'Long']].values[df_latlong['place_id']=='United Kingdom\/United Kingdom']\n    df_tmp.loc[df_tmp['place_id']=='United Kingdom', ['Lat', 'Long']] = tmp\n    tmp = df_latlong[['Lat', 'Long']].values[df_latlong['place_id']=='Cruise Ship\/Diamond Princess']\n    df_tmp.loc[df_tmp['place_id']=='Diamond Princess', ['Lat', 'Long']] = tmp\n    tmp = df_latlong[['Lat', 'Long']].values[df_latlong['place_id']=='Denmark\/Denmark']\n    df_tmp.loc[df_tmp['place_id']=='Denmark', ['Lat', 'Long']] = tmp\n    tmp = df_latlong[['Lat', 'Long']].values[df_latlong['place_id']=='France\/France']\n    df_tmp.loc[df_tmp['place_id']=='France', ['Lat', 'Long']] = tmp\n    tmp = df_latlong[['Lat', 'Long']].values[df_latlong['place_id']=='Gambia, The']\n    df_tmp.loc[df_tmp['place_id']=='Gambia', ['Lat', 'Long']] = tmp\n    tmp = df_latlong[['Lat', 'Long']].values[df_latlong['place_id']=='Netherlands\/Netherlands']\n    df_tmp.loc[df_tmp['place_id']=='Netherlands', ['Lat', 'Long']] = tmp\n    df_tmp.loc[df_tmp['place_id']=='Dominica', ['Lat', 'Long']] = (15.3, -61.383333)\n    df_tmp.loc[df_tmp['place_id']=='Angola', ['Lat', 'Long']] = (-8.830833, 13.245)\n    df_tmp.loc[df_tmp['place_id']=='Bahamas', ['Lat', 'Long']] = (25.066667, -77.333333)\n    df_tmp.loc[df_tmp['place_id']=='Belize', ['Lat', 'Long']] = (17.498611, -88.188611)\n    df_tmp.loc[df_tmp['place_id']=='Cabo Verde', ['Lat', 'Long']] = (14.916667, -23.516667)\n    df_tmp.loc[df_tmp['place_id']=='Chad', ['Lat', 'Long']] = (12.134722, 15.055833)\n    df_tmp.loc[df_tmp['place_id']=='Denmark\/Greenland', ['Lat', 'Long']] = (64.181389, -51.694167)\n    df_tmp.loc[df_tmp['place_id']=='El Salvador', ['Lat', 'Long']] = (13.698889, -89.191389)\n    df_tmp.loc[df_tmp['place_id']=='Eritrea', ['Lat', 'Long']] = (15.322778, 38.925)\n    df_tmp.loc[df_tmp['place_id']=='Fiji', ['Lat', 'Long']] = (-18.166667, 178.45)\n    df_tmp.loc[df_tmp['place_id']=='France\/Martinique', ['Lat', 'Long']] = (14.666667, -61)\n    df_tmp.loc[df_tmp['place_id']=='France\/New Caledonia', ['Lat', 'Long']] = (-22.2758, 166.458)\n    df_tmp.loc[df_tmp['place_id']=='Grenada', ['Lat', 'Long']] = (12.05, -61.75)\n    df_tmp.loc[df_tmp['place_id']=='Guinea-Bissau', ['Lat', 'Long']] = (11.85, -15.566667)\n    df_tmp.loc[df_tmp['place_id']=='Haiti', ['Lat', 'Long']] = (18.533333, -72.333333)\n    df_tmp.loc[df_tmp['place_id']=='Laos', ['Lat', 'Long']] = (17.966667, 102.6)\n    df_tmp.loc[df_tmp['place_id']=='Libya', ['Lat', 'Long']] = (32.887222, 13.191389)\n    df_tmp.loc[df_tmp['place_id']=='Madagascar', ['Lat', 'Long']] = (-18.933333, 47.516667)\n    df_tmp.loc[df_tmp['place_id']=='Mali', ['Lat', 'Long']] = (12.639167, -8.002778)\n    df_tmp.loc[df_tmp['place_id']=='Mozambique', ['Lat', 'Long']] = (-25.966667, 32.583333)\n    df_tmp.loc[df_tmp['place_id']=='Netherlands\/Sint Maarten', ['Lat', 'Long']] = (18.052778, -63.0425)\n    df_tmp.loc[df_tmp['place_id']=='Nicaragua', ['Lat', 'Long']] = (12.136389, -86.251389)\n    df_tmp.loc[df_tmp['place_id']=='Niger', ['Lat', 'Long']] = (13.511667, 2.125278)\n    df_tmp.loc[df_tmp['place_id']=='Papua New Guinea', ['Lat', 'Long']] = (-9.478889, 147.149444)\n    df_tmp.loc[df_tmp['place_id']=='Saint Kitts and Nevis', ['Lat', 'Long']] = (17.3, -62.733333)\n    df_tmp.loc[df_tmp['place_id']=='Syria', ['Lat', 'Long']] = (33.513056, 36.291944)\n    df_tmp.loc[df_tmp['place_id']=='Timor-Leste', ['Lat', 'Long']] = (-8.566667, 125.566667)\n    df_tmp.loc[df_tmp['place_id']=='Uganda', ['Lat', 'Long']] = (0.313611, 32.581111)\n    df_tmp.loc[df_tmp['place_id']=='Zimbabwe', ['Lat', 'Long']] = (-17.829167, 31.052222)\n    df_tmp.loc[df_tmp['place_id']=='United Kingdom\/Bermuda', ['Lat', 'Long']] = (32.293, -64.782)\n    df_tmp.loc[df_tmp['place_id']=='United Kingdom\/Isle of Man', ['Lat', 'Long']] = (54.145, -4.482)\n\n    df_tmp.loc[df_tmp['place_id']=='Botswana', ['Lat', 'Long']] = (-24.658056, 25.912222)\n    df_tmp.loc[df_tmp['place_id']=='Burma', ['Lat', 'Long']] = (16.85, 96.183333)\n    df_tmp.loc[df_tmp['place_id']=='Burundi', ['Lat', 'Long']] = (-3.383333, 29.366667)\n    df_tmp.loc[df_tmp['place_id']=='Canada\/Northwest Territories', ['Lat', 'Long']] = (62.442222, -114.394722)\n    df_tmp.loc[df_tmp['place_id']=='Canada\/Yukon', ['Lat', 'Long']] = (60.716667, -135.05)\n    df_tmp.loc[df_tmp['place_id']=='Kosovo', ['Lat', 'Long']] = (42.666667, 21.166667)\n    df_tmp.loc[df_tmp['place_id']=='MS Zaandam', ['Lat', 'Long']] = (26.086111, -80.115278) # Uncertain\n    df_tmp.loc[df_tmp['place_id']=='Sierra Leone', ['Lat', 'Long']] = (8.484444, -13.234444)\n    df_tmp.loc[df_tmp['place_id']=='United Kingdom\/Anguilla', ['Lat', 'Long']] = (18.220833, -63.051667)\n    df_tmp.loc[df_tmp['place_id']=='United Kingdom\/British Virgin Islands', ['Lat', 'Long']] = (18.431389, -64.623056)\n    df_tmp.loc[df_tmp['place_id']=='United Kingdom\/Turks and Caicos Islands', ['Lat', 'Long']] = (21.783333, -72.283333)\n    df_tmp.loc[df_tmp['place_id']=='West Bank and Gaza', ['Lat', 'Long']] = (31.703056, 35.195556)\n    df_tmp.loc[df_tmp['place_id']=='France\/Saint Pierre and Miquelon', ['Lat', 'Long']] = (46.7778, -56.1778)\n    df_tmp.loc[df_tmp['place_id']=='Malawi', ['Lat', 'Long']] = (-13.983333, 33.783333)\n    df_tmp.loc[df_tmp['place_id']=='Netherlands\/Bonaire, Sint Eustatius and Saba', ['Lat', 'Long']] = (12.144444, -68.265556)\n    df_tmp.loc[df_tmp['place_id']=='Sao Tome and Principe', ['Lat', 'Long']] = (0.336111, 6.730556)\n    df_tmp.loc[df_tmp['place_id']=='South Sudan', ['Lat', 'Long']] = (4.85, 31.6)\n    df_tmp.loc[df_tmp['place_id']=='United Kingdom\/Falkland Islands (Malvinas)', ['Lat', 'Long']] = (-51.694444, -57.852778)\n    df_tmp.loc[df_tmp['place_id']=='Western Sahara', ['Lat', 'Long']] = (27.153611, -13.203333)\n    \n    print(f\"consumption time:{time.time()-start_time}[s]\")","642ea1b6":"if FULL_RUN:\n    start_time = time.time()\n    \n    df_traintest = pd.merge(df_traintest, df_tmp[['place_id', 'Lat', 'Long']], on='place_id', how='left')\n    \n    # count the places with no Lat and Long.\n    tmp = np.sort(df_traintest['place_id'][pd.isna(df_traintest['Lat'])].unique())\n    print(len(tmp)) # count Nan\n    print(tmp)\n    \n    # get place list\n    places = np.sort(df_traintest['place_id'].unique())\n    print(len(places))\n    \n    print(pd.isna(df_traintest['Lat']).sum()) # count Nan\n    df_traintest[pd.isna(df_traintest['Lat'])].head()\n\n    print(f\"consumption time:{time.time()-start_time}[s]\")","bded94c5":"if FULL_RUN:\n    start_time = time.time()\n    \n    # calc cases, fatalities per day\n    df_traintest2 = copy.deepcopy(df_traintest)\n    df_traintest2['cases\/day'] = 0\n    df_traintest2['fatal\/day'] = 0\n    tmp_list = np.zeros(len(df_traintest2))\n    for place in places:\n        tmp = df_traintest2['ConfirmedCases'][df_traintest2['place_id']==place].values\n        tmp[1:] -= tmp[:-1]\n        df_traintest2['cases\/day'][df_traintest2['place_id']==place] = tmp\n        tmp = df_traintest2['Fatalities'][df_traintest2['place_id']==place].values\n        tmp[1:] -= tmp[:-1]\n        df_traintest2['fatal\/day'][df_traintest2['place_id']==place] = tmp\n    print(df_traintest2.shape)\n    df_traintest2[df_traintest2['place_id']=='China\/Hubei'].head()\n    \n    print(f\"consumption time:{time.time()-start_time}[s]\")","77d4bc43":"if FULL_RUN:\n    start_time = time.time()\n\n    # aggregate cases and fatalities\n    def do_aggregation(df, col, mean_range):\n        df_new = copy.deepcopy(df)\n        col_new = '{}_({}-{})'.format(col, mean_range[0], mean_range[1])\n        df_new[col_new] = 0\n        tmp = df_new[col].rolling(mean_range[1]-mean_range[0]+1).mean()\n        df_new[col_new][mean_range[0]:] = tmp[:-(mean_range[0])]\n        df_new[col_new][pd.isna(df_new[col_new])] = 0\n        return df_new[[col_new]].reset_index(drop=True)\n\n    def do_aggregations(df):\n        df = pd.concat([df, do_aggregation(df, 'cases\/day', [1,1]).reset_index(drop=True)], axis=1)\n        df = pd.concat([df, do_aggregation(df, 'cases\/day', [1,7]).reset_index(drop=True)], axis=1)\n        df = pd.concat([df, do_aggregation(df, 'cases\/day', [8,14]).reset_index(drop=True)], axis=1)\n        df = pd.concat([df, do_aggregation(df, 'cases\/day', [15,21]).reset_index(drop=True)], axis=1)\n        df = pd.concat([df, do_aggregation(df, 'fatal\/day', [1,1]).reset_index(drop=True)], axis=1)\n        df = pd.concat([df, do_aggregation(df, 'fatal\/day', [1,7]).reset_index(drop=True)], axis=1)\n        df = pd.concat([df, do_aggregation(df, 'fatal\/day', [8,14]).reset_index(drop=True)], axis=1)\n        df = pd.concat([df, do_aggregation(df, 'fatal\/day', [15,21]).reset_index(drop=True)], axis=1)\n        for threshold in [1, 10, 100]:\n            days_under_threshold = (df['ConfirmedCases']<threshold).sum()\n            tmp = df['day'].values - 22 - days_under_threshold\n            tmp[tmp<=0] = 0\n            df['days_since_{}cases'.format(threshold)] = tmp\n\n        for threshold in [1, 10, 100]:\n            days_under_threshold = (df['Fatalities']<threshold).sum()\n            tmp = df['day'].values - 22 - days_under_threshold\n            tmp[tmp<=0] = 0\n            df['days_since_{}fatal'.format(threshold)] = tmp\n\n        # process China\/Hubei\n        if df['place_id'][0]=='China\/Hubei':\n            df['days_since_1cases'] += 35 # 2019\/12\/8\n            df['days_since_10cases'] += 35-13 # 2019\/12\/8-2020\/1\/2 assume 2019\/12\/8+13\n            df['days_since_100cases'] += 4 # 2020\/1\/18\n            df['days_since_1fatal'] += 13 # 2020\/1\/9\n        return df\n    \n    print(f\"consumption time:{time.time()-start_time}[s]\")","c8485b77":"if FULL_RUN:\n    start_time = time.time()\n    \n    df_traintest3 = []\n    for place in places[:]:\n        df_tmp = df_traintest2[df_traintest2['place_id']==place].reset_index(drop=True)\n        df_tmp = do_aggregations(df_tmp)\n        df_traintest3.append(df_tmp)\n    df_traintest3 = pd.concat(df_traintest3).reset_index(drop=True)\n    df_traintest3[df_traintest3['place_id']=='China\/Hubei'].head()\n\n    print(f\"consumption time:{time.time()-start_time}[s]\")","6dad80b4":"if FULL_RUN:\n    start_time = time.time()\n    \n    # extract newest data\n    df_smoking_recent = df_smoking.sort_values('Year', ascending=False).reset_index(drop=True)\n    df_smoking_recent = df_smoking_recent[df_smoking_recent['Entity'].duplicated()==False]\n    df_smoking_recent['Country_Region'] = df_smoking_recent['Entity']\n    df_smoking_recent['SmokingRate'] = df_smoking_recent['Smoking prevalence, total (ages 15+) (% of adults)']\n    df_smoking_recent.head()\n    \n    print(f\"consumption time:{time.time()-start_time}[s]\")","772340e8":"if FULL_RUN:\n    start_time = time.time()\n    \n    # merge\n    df_traintest4 = pd.merge(df_traintest3, df_smoking_recent[['Country_Region', 'SmokingRate']], on='Country_Region', how='left')\n    print(df_traintest4.shape)\n    df_traintest4.head()\n    \n    print(f\"consumption time:{time.time()-start_time}[s]\")","c9c825b8":"if FULL_RUN:\n    start_time = time.time()\n    \n    # fill na with world smoking rate\n    SmokingRate = df_smoking_recent['SmokingRate'][df_smoking_recent['Entity']=='World'].values[0]\n    print(\"Smoking rate of the world: {:.6f}\".format(SmokingRate))\n    df_traintest4['SmokingRate'][pd.isna(df_traintest4['SmokingRate'])] = SmokingRate\n    df_traintest4.head()\n    \n    print(f\"consumption time:{time.time()-start_time}[s]\")","74a62d11":"if FULL_RUN:\n    start_time = time.time()\n    \n    subs  = df_weo['Subject Descriptor'].unique()[:-1]\n    df_weo_agg = df_weo[['Country']][df_weo['Country'].duplicated()==False].reset_index(drop=True)\n    for sub in subs[:]:\n        df_tmp = df_weo[['Country', '2019']][df_weo['Subject Descriptor']==sub].reset_index(drop=True)\n        df_tmp = df_tmp[df_tmp['Country'].duplicated()==False].reset_index(drop=True)\n        df_tmp.columns = ['Country', sub]\n        df_weo_agg = df_weo_agg.merge(df_tmp, on='Country', how='left')\n    df_weo_agg.columns = [\"\".join (c if c.isalnum() else \"_\" for c in str(x)) for x in df_weo_agg.columns]\n    df_weo_agg.columns\n    df_weo_agg['Country_Region'] = df_weo_agg['Country']\n    df_weo_agg.head()\n    \n    print(f\"consumption time:{time.time()-start_time}[s]\")","1e407569":"if FULL_RUN:\n    start_time = time.time()\n    \n    df_traintest5 = pd.merge(df_traintest4, df_weo_agg, on='Country_Region', how='left')\n    \n    print(f\"consumption time:{time.time()-start_time}[s]\")","f16f8833":"if FULL_RUN:\n    start_time = time.time()\n    \n    tmp = df_life.iloc[:,1].values.tolist()\n    df_life = df_life[['Country', '2018']]\n    def func(x):\n        x_new = 0\n        try:\n            x_new = float(x.replace(\",\", \"\"))\n        except:\n    #         print(x)\n            x_new = np.nan\n        return x_new\n\n    df_life['2018'] = df_life['2018'].apply(lambda x: func(x))\n    df_life.head()\n    \n    print(f\"consumption time:{time.time()-start_time}[s]\")","f96d8007":"if FULL_RUN:\n    start_time = time.time()\n    \n    df_life = df_life[['Country', '2018']]\n    df_life.columns = ['Country_Region', 'LifeExpectancy']\n    \n    print(f\"consumption time:{time.time()-start_time}[s]\")","7bf6a41e":"if FULL_RUN:\n    start_time = time.time()\n    \n    # merge\n    df_traintest6 = pd.merge(df_traintest5, df_life, on='Country_Region', how='left')\n    print(len(df_traintest6))\n    df_traintest6.head()\n    \n    print(f\"consumption time:{time.time()-start_time}[s]\")","9b8aabf9":"if FULL_RUN:\n    start_time = time.time()\n    \n    df_country['Country_Region'] = df_country['country']\n    df_country = df_country[df_country['country'].duplicated()==False]\n    \n    print(f\"consumption time:{time.time()-start_time}[s]\")","c8b87905":"if FULL_RUN:\n    start_time = time.time()\n    \n    df_traintest7 = pd.merge(df_traintest6, \n                             df_country.drop(['tests', 'testpop', 'country'], axis=1), \n                             on=['Country_Region',], how='left')\n    print(df_traintest7.shape)\n    df_traintest7.head()\n    \n    print(f\"consumption time:{time.time()-start_time}[s]\")","54d79dfa":"if FULL_RUN:\n    start_time = time.time()\n    \n    def encode_label(df, col, freq_limit=0):\n        df[col][pd.isna(df[col])] = 'nan'\n        tmp = df[col].value_counts()\n        cols = tmp.index.values\n        freq = tmp.values\n        num_cols = (freq>=freq_limit).sum()\n        print(\"col: {}, num_cat: {}, num_reduced: {}\".format(col, len(cols), num_cols))\n\n        col_new = '{}_le'.format(col)\n        df_new = pd.DataFrame(np.ones(len(df), np.int16)*(num_cols-1), columns=[col_new])\n        for i, item in enumerate(cols[:num_cols]):\n            df_new[col_new][df[col]==item] = i\n\n        return df_new\n\n    def get_df_le(df, col_index, col_cat):\n        df_new = df[[col_index]]\n        for col in col_cat:\n            df_tmp = encode_label(df, col)\n            df_new = pd.concat([df_new, df_tmp], axis=1)\n        return df_new\n\n    df_traintest7['id'] = np.arange(len(df_traintest7))\n    df_le = get_df_le(df_traintest7, 'id', ['Country_Region', 'Province_State'])\n    df_traintest8 = pd.merge(df_traintest7, df_le, on='id', how='left')\n    \n    df_traintest8['cases\/day'] = df_traintest8['cases\/day'].astype(np.float)\n    df_traintest8['fatal\/day'] = df_traintest8['fatal\/day'].astype(np.float)\n    \n    print(f\"consumption time:{time.time()-start_time}[s]\")","47e88aa2":"if FULL_RUN:\n    start_time = time.time()\n    \n    # covert object type to float\n    def func(x):\n        x_new = 0\n        try:\n            x_new = float(x.replace(\",\", \"\"))\n        except:\n    #         print(x)\n            x_new = np.nan\n        return x_new\n    cols = [\n        'Gross_domestic_product__constant_prices', \n        'Gross_domestic_product__current_prices', \n        'Gross_domestic_product__deflator', \n        'Gross_domestic_product_per_capita__constant_prices', \n        'Gross_domestic_product_per_capita__current_prices', \n        'Output_gap_in_percent_of_potential_GDP', \n        'Gross_domestic_product_based_on_purchasing_power_parity__PPP__valuation_of_country_GDP', \n        'Gross_domestic_product_based_on_purchasing_power_parity__PPP__per_capita_GDP', \n        'Gross_domestic_product_based_on_purchasing_power_parity__PPP__share_of_world_total', \n        'Implied_PPP_conversion_rate', 'Total_investment', \n        'Gross_national_savings', 'Inflation__average_consumer_prices', \n        'Inflation__end_of_period_consumer_prices', \n        'Six_month_London_interbank_offered_rate__LIBOR_', \n        'Volume_of_imports_of_goods_and_services', \n        'Volume_of_Imports_of_goods', \n        'Volume_of_exports_of_goods_and_services', \n        'Volume_of_exports_of_goods', 'Unemployment_rate', 'Employment', 'Population', \n        'General_government_revenue', 'General_government_total_expenditure', \n        'General_government_net_lending_borrowing', 'General_government_structural_balance', \n        'General_government_primary_net_lending_borrowing', 'General_government_net_debt', \n        'General_government_gross_debt', 'Gross_domestic_product_corresponding_to_fiscal_year__current_prices', \n        'Current_account_balance', 'pop'\n    ]\n    for col in cols:\n        df_traintest8[col] = df_traintest8[col].apply(lambda x: func(x))  \n    print(df_traintest8['pop'].dtype)\n    \n    print(f\"consumption time:{time.time()-start_time}[s]\")\n    \n    \ndf_traintest8.head()","42e2b91a":"if FULL_RUN:\n    start_time = time.time()\n    # Saving DFs\n    \n    df_traintest8.to_csv('\/kaggle\/working\/df_train.csv')\n    df_traintest8.to_csv('\/kaggle\/working\/df_test.csv')\n    \n    print(f\"consumption time:{time.time()-start_time}[s]\")","7ac45767":"def calc_score(y_true, y_pred):\n    y_true[y_true<0] = 0\n    score = metrics.mean_squared_error(np.log(y_true.clip(0, 1e10)+1), np.log(y_pred[:]+1))**0.5\n    return score","d7496a36":"# train model to predict fatalities\/day\n# params\nSEED = 42\nparams = {'num_leaves': 8,\n          'min_data_in_leaf': 5,  # 42,\n          'objective': 'regression',\n          'max_depth': 8,\n          'learning_rate': 0.02,\n          'boosting': 'gbdt',\n          'bagging_freq': 5,  # 5\n          'bagging_fraction': 0.8,  # 0.5,\n          'feature_fraction': 0.8201,\n          'bagging_seed': SEED,\n          'reg_alpha': 1,  # 1.728910519108444,\n          'reg_lambda': 4.9847051755586085,\n          'random_state': SEED,\n          'metric': 'mse',\n          'verbosity': 100,\n          'min_gain_to_split': 0.02,  # 0.01077313523861969,\n          'min_child_weight': 5,  # 19.428902804238373,\n          'num_threads': 6,\n          }","d6b24e87":"start_time = time.time()\n\n# train model to predict fatalities\/day\n# features are selected manually based on valid score\ncol_target = 'fatal\/day'\ncol_var = [\n    'Lat', 'Long',\n#     'days_since_1cases', \n#     'days_since_10cases', \n#     'days_since_100cases',\n#     'days_since_1fatal', \n#     'days_since_10fatal', 'days_since_100fatal',\n#     'days_since_1recov',\n#     'days_since_10recov', 'days_since_100recov', \n    'cases\/day_(1-1)', \n    'cases\/day_(1-7)', \n#     'cases\/day_(8-14)',  \n#     'cases\/day_(15-21)', \n    \n#     'fatal\/day_(1-1)', \n    'fatal\/day_(1-7)', \n    'fatal\/day_(8-14)', \n    'fatal\/day_(15-21)', \n    'SmokingRate',\n#     'Gross_domestic_product__constant_prices',\n#     'Gross_domestic_product__current_prices',\n#     'Gross_domestic_product__deflator',\n#     'Gross_domestic_product_per_capita__constant_prices',\n#     'Gross_domestic_product_per_capita__current_prices',\n#     'Output_gap_in_percent_of_potential_GDP',\n#     'Gross_domestic_product_based_on_purchasing_power_parity__PPP__valuation_of_country_GDP',\n#     'Gross_domestic_product_based_on_purchasing_power_parity__PPP__per_capita_GDP',\n#     'Gross_domestic_product_based_on_purchasing_power_parity__PPP__share_of_world_total',\n#     'Implied_PPP_conversion_rate', 'Total_investment',\n#     'Gross_national_savings', 'Inflation__average_consumer_prices',\n#     'Inflation__end_of_period_consumer_prices',\n#     'Six_month_London_interbank_offered_rate__LIBOR_',\n#     'Volume_of_imports_of_goods_and_services', 'Volume_of_Imports_of_goods',\n#     'Volume_of_exports_of_goods_and_services', 'Volume_of_exports_of_goods',\n#     'Unemployment_rate', \n#     'Employment', 'Population',\n#     'General_government_revenue', 'General_government_total_expenditure',\n#     'General_government_net_lending_borrowing',\n#     'General_government_structural_balance',\n#     'General_government_primary_net_lending_borrowing',\n#     'General_government_net_debt', 'General_government_gross_debt',\n#     'Gross_domestic_product_corresponding_to_fiscal_year__current_prices',\n#     'Current_account_balance', \n#     'LifeExpectancy',\n#     'pop',\n    'density', \n#     'medianage', \n#     'urbanpop', \n#     'hospibed', 'smokers', \n\n    # happiness\n    #\"happiness_Score\"\n    #, \"happiness_GDP\"\n    #, \"happiness_Social_support\"\n    \"happiness_Healthy_lifeEx\"\n    , \"happiness_Freedom\"\n    , \"happiness_Generosity\"\n    #, \"happiness_Corruption\"\n]\ncol_cat = []\ndf_train = df_traintest8[(pd.isna(df_traintest8['ForecastId'])) & (df_traintest8['day']<=day_before_valid)]\ndf_valid = df_traintest8[(pd.isna(df_traintest8['ForecastId'])) & (day_before_valid<df_traintest8['day']) & (df_traintest8['day']<=day_before_public)]\ndf_test = df_traintest8[pd.isna(df_traintest8['ForecastId'])==False]\nX_train = df_train[col_var]\nX_valid = df_valid[col_var]\ny_train = np.log(df_train[col_target].values.clip(0, 1e10)+1)\ny_valid = np.log(df_valid[col_target].values.clip(0, 1e10)+1)\ntrain_data = lgb.Dataset(X_train, label=y_train, categorical_feature=col_cat)\nvalid_data = lgb.Dataset(X_valid, label=y_valid, categorical_feature=col_cat)\nnum_round = 15000\nmodel = lgb.train(params, train_data, num_round, valid_sets=[train_data, valid_data],\n                  verbose_eval=100,\n                  early_stopping_rounds=150,)\n\nbest_itr = model.best_iteration\n\nprint(f\"consumption time:{time.time()-start_time}[s]\")","8ff21bde":"start_time = time.time()\n\ny_true = df_valid['fatal\/day'].values\ny_pred = np.exp(model.predict(X_valid))-1\nscore = calc_score(y_true, y_pred)\nprint(\"{:.6f}\".format(score))\n\nprint(f\"consumption time:{time.time()-start_time}[s]\")","8f0bb204":"start_time = time.time()\n\n# display feature importance\ntmp = pd.DataFrame()\ntmp[\"feature\"] = col_var\ntmp[\"importance\"] = model.feature_importance()\ntmp = tmp.sort_values('importance', ascending=False)\nprint(tmp)\n\nprint(f\"consumption time:{time.time()-start_time}[s]\")","7810e66a":"start_time = time.time()\n\n# train with all data before public\ndf_train = df_traintest8[(pd.isna(df_traintest8['ForecastId'])) & (df_traintest8['day']<=day_before_public)]\ndf_valid = df_traintest8[(pd.isna(df_traintest8['ForecastId'])) & (df_traintest8['day']<=day_before_public)]\ndf_test = df_traintest8[pd.isna(df_traintest8['ForecastId'])==False]\nX_train = df_train[col_var]\nX_valid = df_valid[col_var]\ny_train = np.log(df_train[col_target].values.clip(0, 1e10)+1)\ny_valid = np.log(df_valid[col_target].values.clip(0, 1e10)+1)\ntrain_data = lgb.Dataset(X_train, label=y_train, categorical_feature=col_cat)\nvalid_data = lgb.Dataset(X_valid, label=y_valid, categorical_feature=col_cat)\nmodel_pub = lgb.train(params, train_data, best_itr, valid_sets=[train_data, valid_data],\n                  verbose_eval=100,\n                  early_stopping_rounds=150,)\n\nprint(f\"consumption time:{time.time()-start_time}[s]\")","72830100":"start_time = time.time()\n\n# train model to predict fatalities\/day\ncol_target2 = 'cases\/day'\ncol_var2 = [\n    'Lat', 'Long',\n#     'days_since_1cases', \n#     'days_since_10cases', #selected\n#     'days_since_100cases',\n#     'days_since_1fatal', \n#     'days_since_10fatal',\n#     'days_since_100fatal',\n#     'days_since_1recov',\n#     'days_since_10recov', 'days_since_100recov', \n    'cases\/day_(1-1)', \n    'cases\/day_(1-7)', \n    'cases\/day_(8-14)',  \n    'cases\/day_(15-21)', \n    \n#     'fatal\/day_(1-1)', \n#     'fatal\/day_(1-7)', \n#     'fatal\/day_(8-14)', \n#     'fatal\/day_(15-21)', \n#     'recov\/day_(1-1)', 'recov\/day_(1-7)', \n#     'recov\/day_(8-14)',  'recov\/day_(15-21)',\n#     'active_(1-1)', \n#     'active_(1-7)', \n#     'active_(8-14)',  'active_(15-21)', \n#     'SmokingRate',\n#     'Gross_domestic_product__constant_prices',\n#     'Gross_domestic_product__current_prices',\n#     'Gross_domestic_product__deflator',\n#     'Gross_domestic_product_per_capita__constant_prices',\n#     'Gross_domestic_product_per_capita__current_prices',\n#     'Output_gap_in_percent_of_potential_GDP',\n#     'Gross_domestic_product_based_on_purchasing_power_parity__PPP__valuation_of_country_GDP',\n#     'Gross_domestic_product_based_on_purchasing_power_parity__PPP__per_capita_GDP',\n#     'Gross_domestic_product_based_on_purchasing_power_parity__PPP__share_of_world_total',\n#     'Implied_PPP_conversion_rate', 'Total_investment',\n#     'Gross_national_savings', 'Inflation__average_consumer_prices',\n#     'Inflation__end_of_period_consumer_prices',\n#     'Six_month_London_interbank_offered_rate__LIBOR_',\n#     'Volume_of_imports_of_goods_and_services', 'Volume_of_Imports_of_goods',\n#     'Volume_of_exports_of_goods_and_services', 'Volume_of_exports_of_goods',\n#     'Unemployment_rate', \n#     'Employment', \n#     'Population',\n#     'General_government_revenue', 'General_government_total_expenditure',\n#     'General_government_net_lending_borrowing',\n#     'General_government_structural_balance',\n#     'General_government_primary_net_lending_borrowing',\n#     'General_government_net_debt', 'General_government_gross_debt',\n#     'Gross_domestic_product_corresponding_to_fiscal_year__current_prices',\n#     'Current_account_balance', \n#     'LifeExpectancy',\n#     'pop',\n#     'density', \n#     'medianage', \n#     'urbanpop', \n#     'hospibed', 'smokers', \n    \n#happiness\n    #\"happiness_Score\"\n    #, \"happiness_GDP\"\n    #, \"happiness_Social_support\"\n    \"happiness_Healthy_lifeEx\"\n    , \"happiness_Freedom\"\n    , \"happiness_Generosity\"\n    #, \"happiness_Corruption\"\n]\ncol_cat = []\ndf_train = df_traintest8[(pd.isna(df_traintest8['ForecastId'])) & (df_traintest8['day']<=day_before_valid)]\ndf_valid = df_traintest8[(pd.isna(df_traintest8['ForecastId'])) & (day_before_valid<df_traintest8['day']) & (df_traintest8['day']<=day_before_public)]\nX_train = df_train[col_var2]\nX_valid = df_valid[col_var2]\ny_train = np.log(df_train[col_target2].values.clip(0, 1e10)+1)\ny_valid = np.log(df_valid[col_target2].values.clip(0, 1e10)+1)\ntrain_data = lgb.Dataset(X_train, label=y_train, categorical_feature=col_cat)\nvalid_data = lgb.Dataset(X_valid, label=y_valid, categorical_feature=col_cat)\nmodel2 = lgb.train(params, train_data, num_round, valid_sets=[train_data, valid_data],\n                  verbose_eval=100,\n                  early_stopping_rounds=150,)\nbest_itr2 = model2.best_iteration\n\nprint(f\"consumption time:{time.time()-start_time}[s]\")","2aef9d8a":"start_time = time.time()\n\ny_true = df_valid['cases\/day'].values\ny_pred = np.exp(model2.predict(X_valid))-1\nscore = calc_score(y_true, y_pred)\nprint(\"{:.6f}\".format(score))\n\nprint(f\"consumption time:{time.time()-start_time}[s]\")","84968d3c":"start_time = time.time()\n\n# display feature importance\ntmp = pd.DataFrame()\ntmp[\"feature\"] = col_var2\ntmp[\"importance\"] = model2.feature_importance()\ntmp = tmp.sort_values('importance', ascending=False)\nprint(tmp)\n\nprint(f\"consumption time:{time.time()-start_time}[s]\")","b20c31f7":"start_time = time.time()\n\ndf_train = df_traintest8[(pd.isna(df_traintest8['ForecastId'])) & (df_traintest8['day']<=day_before_public)]\ndf_valid = df_traintest8[(pd.isna(df_traintest8['ForecastId'])) & (df_traintest8['day']<=day_before_public)]\nX_train = df_train[col_var2]\nX_valid = df_valid[col_var2]\ny_train = np.log(df_train[col_target2].values.clip(0, 1e10)+1)\ny_valid = np.log(df_valid[col_target2].values.clip(0, 1e10)+1)\ntrain_data = lgb.Dataset(X_train, label=y_train, categorical_feature=col_cat)\nvalid_data = lgb.Dataset(X_valid, label=y_valid, categorical_feature=col_cat)\nmodel2_pub = lgb.train(params, train_data, best_itr2, valid_sets=[train_data, valid_data],\n                  verbose_eval=100,\n                  early_stopping_rounds=150,)\n\nprint(f\"consumption time:{time.time()-start_time}[s]\")","8f9200ef":"start_time = time.time()\n\n# train model to predict fatalities\/day\ndf_train = df_traintest8[(pd.isna(df_traintest8['ForecastId'])) & (df_traintest8['day']<=day_before_public)]\ndf_valid = df_traintest8[(pd.isna(df_traintest8['ForecastId'])) & (day_before_public<df_traintest8['day'])]\ndf_test = df_traintest8[pd.isna(df_traintest8['ForecastId'])==False]\nX_train = df_train[col_var]\nX_valid = df_valid[col_var]\ny_train = np.log(df_train[col_target].values.clip(0, 1e10)+1)\ny_valid = np.log(df_valid[col_target].values.clip(0, 1e10)+1)\ntrain_data = lgb.Dataset(X_train, label=y_train, categorical_feature=col_cat)\nvalid_data = lgb.Dataset(X_valid, label=y_valid, categorical_feature=col_cat)\nnum_round = 15000\nmodel = lgb.train(params, train_data, num_round, valid_sets=[train_data, valid_data],\n                  verbose_eval=100,\n                  early_stopping_rounds=150,)\n\nbest_itr = model.best_iteration\n\nprint(f\"consumption time:{time.time()-start_time}[s]\")","75e6dc56":"start_time = time.time()\n\n# train with all data\ndf_train = df_traintest8[(pd.isna(df_traintest8['ForecastId']))]\ndf_valid = df_traintest8[(pd.isna(df_traintest8['ForecastId']))]\nX_train = df_train[col_var]\nX_valid = df_valid[col_var]\ny_train = np.log(df_train[col_target].values.clip(0, 1e10)+1)\ny_valid = np.log(df_valid[col_target].values.clip(0, 1e10)+1)\ntrain_data = lgb.Dataset(X_train, label=y_train, categorical_feature=col_cat)\nvalid_data = lgb.Dataset(X_valid, label=y_valid, categorical_feature=col_cat)\nmodel_pri = lgb.train(params, train_data, best_itr, valid_sets=[train_data, valid_data],\n                  verbose_eval=100,\n                  early_stopping_rounds=150,)\n\nprint(f\"consumption time:{time.time()-start_time}[s]\")","14f62a7f":"start_time = time.time()\n\n# train model to predict cases\/day\ndf_train = df_traintest8[(pd.isna(df_traintest8['ForecastId'])) & (df_traintest8['day']<=day_before_public)]\ndf_valid = df_traintest8[(pd.isna(df_traintest8['ForecastId'])) & (day_before_public<df_traintest8['day'])]\nX_train = df_train[col_var2]\nX_valid = df_valid[col_var2]\ny_train = np.log(df_train[col_target2].values.clip(0, 1e10)+1)\ny_valid = np.log(df_valid[col_target2].values.clip(0, 1e10)+1)\ntrain_data = lgb.Dataset(X_train, label=y_train, categorical_feature=col_cat)\nvalid_data = lgb.Dataset(X_valid, label=y_valid, categorical_feature=col_cat)\nmodel2 = lgb.train(params, train_data, num_round, valid_sets=[train_data, valid_data],\n                  verbose_eval=100,\n                  early_stopping_rounds=150,)\nbest_itr2 = model2.best_iteration\n\nprint(f\"consumption time:{time.time()-start_time}[s]\")","958aea6b":"start_time = time.time()\n\n# train with all data\ndf_train = df_traintest8[(pd.isna(df_traintest8['ForecastId']))]\ndf_valid = df_traintest8[(pd.isna(df_traintest8['ForecastId']))]\nX_train = df_train[col_var2]\nX_valid = df_valid[col_var2]\ny_train = np.log(df_train[col_target2].values.clip(0, 1e10)+1)\ny_valid = np.log(df_valid[col_target2].values.clip(0, 1e10)+1)\ntrain_data = lgb.Dataset(X_train, label=y_train, categorical_feature=col_cat)\nvalid_data = lgb.Dataset(X_valid, label=y_valid, categorical_feature=col_cat)\nmodel2_pri = lgb.train(params, train_data, best_itr2, valid_sets=[train_data, valid_data],\n                  verbose_eval=100,\n                  early_stopping_rounds=150,)\n\nprint(f\"consumption time:{time.time()-start_time}[s]\")","74e972a7":"start_time = time.time()\n\n# remove overlap for public LB prediction\nprint(day_before_public)\ndf_tmp = df_traintest8[\n    ((df_traintest8['day']<=day_before_public)  & (pd.isna(df_traintest8['ForecastId'])))\n    | ((day_before_public<df_traintest8['day']) & (pd.isna(df_traintest8['ForecastId'])==False))].reset_index(drop=True)\ndf_tmp = df_tmp.drop([\n    'cases\/day_(1-1)', 'cases\/day_(1-7)', 'cases\/day_(8-14)', 'cases\/day_(15-21)', \n    'fatal\/day_(1-1)', 'fatal\/day_(1-7)', 'fatal\/day_(8-14)', 'fatal\/day_(15-21)',\n    'days_since_1cases', 'days_since_10cases', 'days_since_100cases',\n    'days_since_1fatal', 'days_since_10fatal', 'days_since_100fatal',\n                               ],  axis=1)\ndf_traintest9 = []\nfor i, place in enumerate(places[:]):\n    df_tmp2 = df_tmp[df_tmp['place_id']==place].reset_index(drop=True)\n    df_tmp2 = do_aggregations(df_tmp2)\n    df_traintest9.append(df_tmp2)\ndf_traintest9 = pd.concat(df_traintest9).reset_index(drop=True)\nprint(df_traintest9.shape)\ndf_traintest9[df_traintest9['day']>day_before_public-2].head()\n\nprint(f\"consumption time:{time.time()-start_time}[s]\")","ee9a6c8d":"start_time = time.time()\n\n# remove overlap for private LB prediction\ndf_tmp = df_traintest8[\n    ((df_traintest8['day']<=day_before_private)  & (pd.isna(df_traintest8['ForecastId'])))\n    | ((day_before_private<df_traintest8['day']) & (pd.isna(df_traintest8['ForecastId'])==False))].reset_index(drop=True)\ndf_tmp = df_tmp.drop([\n    'cases\/day_(1-1)', 'cases\/day_(1-7)', 'cases\/day_(8-14)', 'cases\/day_(15-21)', \n    'fatal\/day_(1-1)', 'fatal\/day_(1-7)', 'fatal\/day_(8-14)', 'fatal\/day_(15-21)',\n    'days_since_1cases', 'days_since_10cases', 'days_since_100cases',\n    'days_since_1fatal', 'days_since_10fatal', 'days_since_100fatal',\n                               ],  axis=1)\ndf_traintest10 = []\nfor i, place in enumerate(places[:]):\n    df_tmp2 = df_tmp[df_tmp['place_id']==place].reset_index(drop=True)\n    df_tmp2 = do_aggregations(df_tmp2)\n    df_traintest10.append(df_tmp2)\ndf_traintest10 = pd.concat(df_traintest10).reset_index(drop=True)\nprint(df_traintest10.shape)\ndf_traintest10[df_traintest10['day']>day_before_private-2].head()\n\nprint(f\"consumption time:{time.time()-start_time}[s]\")","0e99d352":"start_time = time.time()\n\n# predict test data in public\n# predict the cases and fatatilites one day at a time and use the predicts as next day's feature recursively.\ndf_preds = []\nfor i, place in tqdm.tqdm(enumerate(places[:]), total=len(places[:]), ncols=80, position=0):\n    #for i, place in tqdm.tqdm(enumerate(places[:])):\n    df_interest = copy.deepcopy(df_traintest9[df_traintest9['place_id']==place].reset_index(drop=True))\n    df_interest['cases\/day'][(pd.isna(df_interest['ForecastId']))==False] = -1\n    df_interest['fatal\/day'][(pd.isna(df_interest['ForecastId']))==False] = -1\n    len_known = (df_interest['day']<=day_before_public).sum()\n    len_unknown = (day_before_public<df_interest['day']).sum()\n    for j in range(len_unknown): # use predicted cases and fatal for next days' prediction\n        X_valid = df_interest[col_var].iloc[j+len_known]\n        X_valid2 = df_interest[col_var2].iloc[j+len_known]\n        pred_f = model_pub.predict(X_valid)\n        pred_c = model2_pub.predict(X_valid2)\n        pred_c = (np.exp(pred_c)-1).clip(0, 1e10)\n        pred_f = (np.exp(pred_f)-1).clip(0, 1e10)\n        df_interest['fatal\/day'][j+len_known] = pred_f\n        df_interest['cases\/day'][j+len_known] = pred_c\n        df_interest['Fatalities'][j+len_known] = df_interest['Fatalities'][j+len_known-1] + pred_f\n        df_interest['ConfirmedCases'][j+len_known] = df_interest['ConfirmedCases'][j+len_known-1] + pred_c\n#         print(df_interest['ConfirmedCases'][j+len_known-1], df_interest['ConfirmedCases'][j+len_known], pred_c)\n        df_interest = df_interest.drop([\n            'cases\/day_(1-1)', 'cases\/day_(1-7)', 'cases\/day_(8-14)', 'cases\/day_(15-21)', \n            'fatal\/day_(1-1)', 'fatal\/day_(1-7)', 'fatal\/day_(8-14)', 'fatal\/day_(15-21)',\n            'days_since_1cases', 'days_since_10cases', 'days_since_100cases',\n            'days_since_1fatal', 'days_since_10fatal', 'days_since_100fatal',\n\n                                       ],  axis=1)\n        df_interest = do_aggregations(df_interest)\n    if (i+1)%10==0:\n        print(\"{:3d}\/{}  {}, len known: {}, len unknown: {}\".format(i+1, len(places), place, len_known, len_unknown), df_interest.shape)\n    df_interest['fatal_pred'] = np.cumsum(df_interest['fatal\/day'].values)\n    df_interest['cases_pred'] = np.cumsum(df_interest['cases\/day'].values)\n    df_preds.append(df_interest)\ndf_preds = pd.concat(df_preds).reset_index(drop=True)\n\nprint(f\"consumption time:{time.time()-start_time}[s]\")","639f5247":"start_time = time.time()\n\n# predict test data in public\ndf_preds_pri = []\nprint(\"num_iter: \", len(places[:]))\nfor i, place in tqdm.tqdm(enumerate(places[:]), total=len(places[:]), ncols=80, position=0):\n    df_interest = copy.deepcopy(df_traintest10[df_traintest10['place_id']==place].reset_index(drop=True))\n    df_interest['cases\/day'][(pd.isna(df_interest['ForecastId']))==False] = -1\n    df_interest['fatal\/day'][(pd.isna(df_interest['ForecastId']))==False] = -1\n    len_known = (df_interest['day']<=day_before_private).sum()\n    len_unknown = (day_before_private<df_interest['day']).sum()\n    for j in range(len_unknown): # use predicted cases and fatal for next days' prediction\n        X_valid = df_interest[col_var].iloc[j+len_known]\n        X_valid2 = df_interest[col_var2].iloc[j+len_known]\n        pred_f = model_pri.predict(X_valid)\n        pred_c = model2_pri.predict(X_valid2)\n        pred_c = (np.exp(pred_c)-1).clip(0, 1e10)\n        pred_f = (np.exp(pred_f)-1).clip(0, 1e10)\n        df_interest['fatal\/day'][j+len_known] = pred_f\n        df_interest['cases\/day'][j+len_known] = pred_c\n        df_interest['Fatalities'][j+len_known] = df_interest['Fatalities'][j+len_known-1] + pred_f\n        df_interest['ConfirmedCases'][j+len_known] = df_interest['ConfirmedCases'][j+len_known-1] + pred_c\n#         print(df_interest['ConfirmedCases'][j+len_known-1], df_interest['ConfirmedCases'][j+len_known], pred_c)\n        df_interest = df_interest.drop([\n            'cases\/day_(1-1)', 'cases\/day_(1-7)', 'cases\/day_(8-14)', 'cases\/day_(15-21)', \n            'fatal\/day_(1-1)', 'fatal\/day_(1-7)', 'fatal\/day_(8-14)', 'fatal\/day_(15-21)',\n            'days_since_1cases', 'days_since_10cases', 'days_since_100cases',\n            'days_since_1fatal', 'days_since_10fatal', 'days_since_100fatal',\n\n                                       ],  axis=1)\n        df_interest = do_aggregations(df_interest)\n    if (i+1)%10==0:\n        print(\"{:3d}\/{}  {}, len known: {}, len unknown: {}\".format(i+1, len(places), place, len_known, len_unknown), df_interest.shape)\n    df_interest['fatal_pred'] = np.cumsum(df_interest['fatal\/day'].values)\n    df_interest['cases_pred'] = np.cumsum(df_interest['cases\/day'].values)\n    df_preds_pri.append(df_interest)\ndf_preds_pri = pd.concat(df_preds_pri).reset_index(drop=True)\n\nprint(f\"consumption time:{time.time()-start_time}[s]\")","c571618f":"start_time = time.time()\n\nplaces_sort = df_traintest10[['place_id', 'ConfirmedCases']][df_traintest10['day']==day_before_private]\nplaces_sort = places_sort.sort_values('ConfirmedCases', ascending=False).reset_index(drop=True)['place_id'].values\nprint(len(places_sort))\nplaces_sort[:5]\n\nprint(f\"consumption time:{time.time()-start_time}[s]\")","7dfcbbbe":"start_time = time.time()\n\nplaces_sort = df_traintest10[['place_id', 'ConfirmedCases']][df_traintest10['day']==day_before_private]\nplaces_sort = places_sort.sort_values('ConfirmedCases', ascending=False).reset_index(drop=True)['place_id'].values\nprint(len(places_sort))\nplaces_sort[:5]\n\nprint(f\"consumption time:{time.time()-start_time}[s]\")","fde45b0e":"start_time = time.time()\n\nprint(\"Fatalities \/ Public\")\nplt.figure(figsize=(30,30))\nfor i in range(30):\n    plt.subplot(5,6,i+1)\n    idx = i * 10\n    df_interest = df_preds[df_preds['place_id']==places_sort[idx]].reset_index(drop=True)\n    tmp = df_interest['fatal\/day'].values\n    tmp = np.cumsum(tmp)\n    sns.lineplot(x=df_interest['day'], y=tmp, label='pred')\n    df_interest2 = df_traintest10[(df_traintest10['place_id']==places_sort[idx]) & (df_traintest10['day']<=day_before_private)].reset_index(drop=True)\n    sns.lineplot(x=df_interest2['day'].values, y=df_interest2['Fatalities'].values, label='true')\n    plt.title(places_sort[idx])\nplt.show()\n\nprint(f\"consumption time:{time.time()-start_time}[s]\")","8b97f6aa":"start_time = time.time()\n\nprint(\"Confirmed Cases \/ Public\")\nplt.figure(figsize=(30,30))\nfor i in range(30):\n    plt.subplot(5,6,i+1)\n    idx = i * 10\n    df_interest = df_preds[df_preds['place_id']==places_sort[idx]].reset_index(drop=True)\n    tmp = df_interest['cases\/day'].values\n    tmp = np.cumsum(tmp)\n    sns.lineplot(x=df_interest['day'], y=tmp, label='pred')\n    df_interest2 = df_traintest10[(df_traintest10['place_id']==places_sort[idx]) & (df_traintest10['day']<=day_before_private)].reset_index(drop=True)\n    sns.lineplot(x=df_interest2['day'].values, y=df_interest2['ConfirmedCases'].values, label='true')\n    plt.title(places_sort[idx])\nplt.show()\n\nprint(f\"consumption time:{time.time()-start_time}[s]\")","96bb894b":"start_time = time.time()\n\nprint(\"Fatalities \/ Private\")\nplt.figure(figsize=(30,30))\nfor i in range(30):\n    plt.subplot(5,6,i+1)\n    idx = i * 10\n    df_interest = df_preds_pri[df_preds_pri['place_id']==places_sort[idx]].reset_index(drop=True)\n    tmp = df_interest['fatal\/day'].values\n    tmp = np.cumsum(tmp)\n    sns.lineplot(x=df_interest['day'], y=tmp, label='pred')\n    df_interest2 = df_traintest10[(df_traintest10['place_id']==places_sort[idx]) & (df_traintest10['day']<=day_before_private)].reset_index(drop=True)\n    sns.lineplot(x=df_interest2['day'].values, y=df_interest2['Fatalities'].values, label='true')\n    plt.title(places_sort[idx])\nplt.show()\n\nprint(f\"consumption time:{time.time()-start_time}[s]\")","74cfa7ea":"start_time = time.time()\n\nprint(\"ConfirmedCases \/ Private\")\nplt.figure(figsize=(30,30))\nfor i in range(30):\n    plt.subplot(5,6,i+1)\n    idx = i * 10\n    df_interest = df_preds_pri[df_preds_pri['place_id']==places_sort[idx]].reset_index(drop=True)\n    tmp = df_interest['cases\/day'].values\n    tmp = np.cumsum(tmp)\n    sns.lineplot(x=df_interest['day'], y=tmp, label='pred')\n    df_interest2 = df_traintest10[(df_traintest10['place_id']==places_sort[idx]) & (df_traintest10['day']<=day_before_private)].reset_index(drop=True)\n    sns.lineplot(x=df_interest2['day'].values, y=df_interest2['ConfirmedCases'].values, label='true')\n    plt.title(places_sort[idx])\nplt.show()\n\nprint(f\"consumption time:{time.time()-start_time}[s]\")","f56bf303":"start_time = time.time()\n\nprint(f\"consumption time:{time.time()-start_time}[s]\")","7d348d91":"start_time = time.time()\n\nprint(f\"consumption time:{time.time()-start_time}[s]\")","a7022fbe":"start_time = time.time()\n\n# merge 2 preds\ndf_preds[df_preds['day']>day_before_private] = df_preds_pri[df_preds['day']>day_before_private]\n\nprint(f\"consumption time:{time.time()-start_time}[s]\")","64fa244d":"start_time = time.time()\n\ndf_preds.to_csv(\"df_preds.csv\", index=None)\n\nprint(f\"consumption time:{time.time()-start_time}[s]\")","24e30b5a":"# load sample submission\ndf_sub = pd.read_csv(\"..\/input\/covid19-global-forecasting-week-4\/submission.csv\")\nprint(len(df_sub))\ndf_sub.head()","00f62273":"# merge prediction with sub\ndf_sub = pd.merge(df_sub, df_traintest3[['ForecastId', 'place_id', 'day']])\ndf_sub = pd.merge(df_sub, df_preds[['place_id', 'day', 'cases_pred', 'fatal_pred']], on=['place_id', 'day',], how='left')\ndf_sub.head(10)","dd161992":"# save\ndf_sub['ConfirmedCases'] = df_sub['cases_pred']\ndf_sub['Fatalities'] = df_sub['fatal_pred']\ndf_sub = df_sub[['ForecastId', 'ConfirmedCases', 'Fatalities']]\ndf_sub.to_csv(\"submission.csv\", index=None)\ndf_sub.head(10)","b304649b":"# Model Training","bda30a76":"# happiness","4347c334":"# Make Submission","d55749ea":"# Prediction","c0532c8a":"# Visualize Prediction","d20ce88b":"# --------"}}