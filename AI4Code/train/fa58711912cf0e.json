{"cell_type":{"a5a18a8c":"code","ed1ccb48":"code","26e259a4":"code","a80f9581":"code","eea50f7a":"markdown","fa0abe17":"markdown","194b44e3":"markdown","6b526fdd":"markdown","ba2a0e55":"markdown"},"source":{"a5a18a8c":"import torch \ndevice = 'cuda' if torch.cuda.is_available() else 'cpu' # GPU recommended\n\n# Loading custom pre-trained ALBERT model already fine-tuned to SQuAD 2.0\nimport transformers\nfrom transformers import AlbertTokenizer, AlbertForQuestionAnswering\ntokenizer = AlbertTokenizer.from_pretrained('albert-base-v2')\nmodel = AlbertForQuestionAnswering.from_pretrained(\n    '\/kaggle\/input' \\\n    '\/nlp-albert-models-fine-tuned-for-squad-20'\\\n    '\/albert-base-v2-tuned-for-squad-2.0').to(device)\n\n# Loading the CORD-19 dataset and pre-processing\nimport pandas as pd\ndata = pd.read_csv('\/kaggle\/input\/CORD-19-research-challenge\/metadata.csv',\n                   keep_default_na=False)\ndata = data[data['abstract']!=''] \\\n       .reset_index(drop=True) # Remove rows with no abstracts","ed1ccb48":"from tqdm.notebook import tqdm\nimport numpy as np\n\ndef inference_ALBERT(question):\n    \n    spans, scores, token_ids = [], [], []\n    \n    # Iterating over all CORD-19 articles and perform model inference\n    for i in tqdm(range(len(data))): # tqdm is for showing the process bar\n        text = data['abstract'][i]\n        input_ids = tokenizer.encode(question, text)\n        \n        # We have token limit of 512, so truncate if needed\n        if len(input_ids) > 512:\n            input_ids, token_type_ids = \\\n                input_ids[:511] + [3], token_type_ids[:512]\n                # [3] is the SEP token\n        \n        token_type_ids = [0 if i <= input_ids.index(3) \n                          else 1 for i in range(len(input_ids))]\n\n        # Preparing the tensors for feeding into model\n        input_ids_tensor = torch.tensor([input_ids]).to(device)\n        token_type_ids_tensor = torch.tensor([token_type_ids]).to(device)\n        \n        # Performing model inference\n        start_scores, end_scores = \\\n            model(input_ids_tensor, \n                  token_type_ids=token_type_ids_tensor)\n        \n        # Releasing GPU memory by moving each tensor back to CPU\n        # If GPU is not used, this step is uncessary but won't give error\n        input_ids_tensor, token_type_ids_tensor, start_scores, end_scores = \\\n            tuple(map(lambda x: x.to('cpu').detach().numpy(), \n                     (input_ids_tensor, token_type_ids_tensor, \\\n                      start_scores, end_scores)))\n        # Let me know if there's an easier way to do this, as I mostly work\n        # with tensorflow and I'm not very familiar with Keras\n\n        # Appending results to the corresponding lists\n        # Spans are the indices of the start and end of the answer\n        spans.append( [start_scores.argmax(), end_scores.argmax()+1] )\n        # Scores are the \"confidence\" level in the start and end\n        scores.append( [start_scores.max(), end_scores.max()] )\n        token_ids.append( input_ids )\n\n    spans = np.array(spans, dtype='int')\n    scores = np.array(scores)\n    \n    return spans, scores, token_ids","26e259a4":"# Define a helper function to directly convert token IDs to string\nconvert_to_str = lambda token_ids: \\\n    tokenizer.convert_tokens_to_string(\n    tokenizer.convert_ids_to_tokens(token_ids))\n\nfrom IPython.display import display, HTML\n\ndef display_results(spans, scores, token_ids, first_n_entries=15,\n                    max_disp_len=100):\n    \n    display(HTML(\n        'Model output (<text style=color:red>red font<\/text> '\\\n        'highlights the answer predicted by ALBERT NLP model)'\\\n        ))\n    \n    # We first sort the results based on the confidence in either the \n    # start or end index of the answer, whichever is smaller\n    min_scores = scores.min(axis=1) \n    sorted_idx = (-min_scores).argsort() # Descending order\n    \n    counter = 0    \n    for idx in sorted_idx:\n        \n        # Stop if first_n_entries papers have been displayed\n        if counter >= first_n_entries:\n            break\n        \n        # If the span is empty, the model prdicts no answer exists \n        # from the article. In rare cases, the end is smaller than\n        # the start. Both will be skipped\n        if spans[idx,0] == 0 or spans[idx,1] == 0 or \\\n            spans[idx,1]<=spans[idx,0]:\n            continue\n\n        # Obtaining the start and end token indices of answer\n        start, end = spans[idx, :]\n\n        abstract = data['abstract'][idx]\n        abstract_highlight = convert_to_str(token_ids[idx][start:end])\n        \n        # If we cannot fully convert tokens to original text,\n        # we then use the detokenized text (lower cased)\n        # Otherwise it would be best to have the original text,\n        # because there's lots of formatting especially in bio articles\n        start = abstract.lower().find(abstract_highlight)\n        if start == -1:\n            abstract = convert_to_str(token_ids[idx]\n                                      [token_ids[idx].index(3)+1:])\n            start = abstract.find(abstract_highlight)\n            end = start + len(abstract_highlight)\n            abstract = abstract[:-5] # to remove the [SEP] token in the end\n        else:\n            end = start + len(abstract_highlight)\n            abstract_highlight = abstract[start:end]\n        abstract_before_highlight, abstract_after_highlight = \\\n            abstract[: start], \\\n            abstract[end : ]\n    \n        # Putting information in HTML format\n        html_str = f'<b>({counter+1}) {data[\"title\"][idx]}<\/b><br>' + \\\n                   f'Confidence: {scores[idx].min():.2f} | ' + \\\n                   f'<i>{data[\"journal\"][idx]}<\/i> | ' + \\\n                   f'{data[\"publish_time\"][idx]} | ' + \\\n                   f'<a href={data[\"url\"][idx]}>{data[\"doi\"][idx]}<\/a>' + \\\n                   '<p style=line-height:1.1><font size=2>' + \\\n                   abstract_before_highlight + \\\n                   '<text style=color:red>%s<\/text>'%abstract_highlight + \\\n                   abstract_after_highlight + '<\/font><\/p>'\n        \n        display(HTML(html_str))\n        \n        counter += 1","a80f9581":"# Combining the inference function and the display function into one\ndef inference_ALBERT_and_display_results(question, \n                                         first_n_entries=15,\n                                         max_disp_len=100):\n    \n    spans, scores, token_ids = inference_ALBERT(question)\n    display_results(spans, scores, token_ids, \n                    first_n_entries, max_disp_len)\n\n# Testing the functions\ninference_ALBERT_and_display_results(\n    'What is the host range for the coronavirus pathogen?')\nprint('Done!')","eea50f7a":"This work uses the [**ALBERT**](https:\/\/arxiv.org\/abs\/1909.11942) model (**a light [BERT](https:\/\/en.wikipedia.org\/wiki\/BERT_(language_model) model**) to perform **question answering** tasks on CORD-19 dataset.\n\nCurrently, ALBERT outperforms most other BERT variants, including BERT itself, on the popular Q&A benchmark [SQuAD 2.0](https:\/\/rajpurkar.github.io\/SQuAD-explorer\/). As of April 9, 2020, it is ranked 9th in all Q&A models and its accuracy is only marginally lower than the top ones. Therefore, it is definitely worth the time to apply ALBERT to this urgent task.\n\nALBERT is relatively new (developed around Sep 2019), so it is difficult to find pretrained model specifically fine-tuned to Q&A, unlike BERT model. So I have went ahead and fine-tuned it with SQuAD 2.0 myself, and I simply attached the pretrained model as a Kaggle dataset. This notebook will use the pretrained model. Both pretraining and inference are done thanks to Huggingface's [Transformers](https:\/\/github.com\/huggingface\/transformers) module.","fa0abe17":"Next, we will define the function where we inference the pre-trained ALBERT model over all CORD-19 papers.","194b44e3":"Next, we will define the function where we display the results in an organized way.","6b526fdd":"ALBERT is a bit computationally heavy. We could use some methods to reduce ALBERT calculations, e.g. do simple search then apply ALBERT only to the search results. But I found ALBERT could capture interesting relationships that traditional search engines couldn't. So I decided to apply ALBERT to all ~40k articles in the dataset. Overall, it takes around 15 minutes for a full iteration using the \"base\" ALBERT model, which is still managable. We could also use other methods, perhaps caching, to reduce ALBERT run time. But for simplicity and time, I sticked to the pure ALBERT approach.\n\nThis notebook is for walking through the code. I will submit individual notebooks to the 9 tasks very soon.\n\nIt is highly recommended to use GPU to run this notebook.","ba2a0e55":"Finally, we test the display results function. \n\nThis notebook is for walking through the code. I will submit individual notebooks to the 9 tasks very soon."}}