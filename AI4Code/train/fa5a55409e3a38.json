{"cell_type":{"c24c20d4":"code","cf5e317f":"code","0be41aca":"code","d4848ab4":"code","76da7e2b":"code","c5ee9908":"code","ffa56c6f":"code","9d8459a8":"code","22f5191c":"code","314c9e82":"code","94bea747":"code","9bdb187d":"code","7a2fcab8":"code","6a6c5b28":"code","2c683063":"markdown","be2074e3":"markdown","c4dcdb00":"markdown","1cbb7a88":"markdown","5bad9190":"markdown","bad0a72e":"markdown","2bd857f2":"markdown","66d875a3":"markdown","21dbeff7":"markdown","4f81e545":"markdown","910a11f7":"markdown","80372bbb":"markdown","ad803c82":"markdown","ac206d37":"markdown","c3cd8d5e":"markdown","ad42e83d":"markdown","7d3b3166":"markdown"},"source":{"c24c20d4":"# importing libraries\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier,GradientBoostingClassifier ,VotingClassifier\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n# roc curve and auc score\nfrom sklearn.metrics import roc_curve\nfrom sklearn.metrics import roc_auc_score","cf5e317f":"# loading dataset \ntraining_v2 = pd.read_csv(\"..\/input\/widsdatathon2020\/training_v2.csv\")\ntest = pd.read_csv(\"..\/input\/widsdatathon2020\/unlabeled.csv\")","0be41aca":"# creating independent features X and dependant feature Y\ny = training_v2['hospital_death']\nX = training_v2\nX = training_v2.drop('hospital_death',axis = 1)\ntest = test.drop('hospital_death',axis = 1)","d4848ab4":"# Remove Features with more than 75 percent missing values\ntrain_missing = (X.isnull().sum() \/ len(X)).sort_values(ascending = False)\ntrain_missing = train_missing.index[train_missing > 0.75]\nX = X.drop(columns = train_missing)\ntest = test.drop(columns = train_missing)","76da7e2b":"categoricals_features = ['hospital_id','ethnicity','gender','hospital_admit_source','icu_admit_source','icu_stay_type','icu_type','apache_3j_bodysystem','apache_2_bodysystem']\nX = X.drop(columns = categoricals_features)\ntest = test.drop(columns = categoricals_features)","c5ee9908":"# Imputation transformer for completing missing values.\nmy_imputer = SimpleImputer()\nnew_data = pd.DataFrame(my_imputer.fit_transform(X))\ntest_data = pd.DataFrame(my_imputer.fit_transform(test))\nnew_data.columns = X.columns\ntest_data.columns = test.columns\nX= new_data\ntest = test_data","ffa56c6f":"# Split into training and validation set\nX_train, valid_features, Y_train, valid_y = train_test_split(X, y, test_size = 0.25, random_state = 1)","9d8459a8":"# Gradient Boosting Classifier\nGBC = GradientBoostingClassifier(random_state=1)\n","22f5191c":"# Random Forest Classifier\nRFC = RandomForestClassifier(n_estimators=100)\n","314c9e82":"# Voting Classifier with soft voting \nvotingC = VotingClassifier(estimators=[('rfc', RFC),('gbc',GBC)], voting='soft')\nvotingC = votingC.fit(X_train, Y_train)","94bea747":"predict_y = votingC.predict(valid_features)","9bdb187d":"def plot_roc_curve(fpr, tpr):\n    plt.plot(fpr, tpr, color='orange', label='ROC')\n    plt.plot([0, 1], [0, 1], color='darkblue', linestyle='--')\n    plt.xlabel('False Positive Rate')\n    plt.ylabel('True Positive Rate')\n    plt.title('Receiver Operating Characteristic (ROC) Curve')\n    plt.legend()\n    plt.show()","7a2fcab8":"probs = votingC.predict_proba(valid_features)\nprobs = probs[:, 1]\nauc = roc_auc_score(valid_y, probs)\nfpr, tpr, thresholds = roc_curve(valid_y, probs)\nplot_roc_curve(fpr, tpr)\nprint(\"AUC-ROC :\",auc)\n","6a6c5b28":"test1 = test.copy()\ntest1[\"hospital_death\"] = votingC.predict(test)\ntest1[[\"encounter_id\",\"hospital_death\"]].to_csv(\"submission5.csv\",index=False)\ntest1[[\"encounter_id\",\"hospital_death\"]].head()","2c683063":"## Data Description \n\nMIT's GOSSIS community initiative, with privacy certification from the Harvard Privacy Lab, has provided a dataset of more than 130,000 hospital Intensive Care Unit (ICU) visits from patients, spanning a one-year timeframe. This data is part of a growing global effort and consortium spanning Argentina, Australia, New Zealand, Sri Lanka, Brazil, and more than 200 hospitals in the United States.\n\nThe data includes:\n\n**Training data** for 91,713 encounters.  \n**Unlabeled test data** for 39,308 encounters, which includes all the information in the training data except for the values for hospital_death.  \n**WiDS Datathon 2020 Dictionary** with supplemental information about the data, including the category (e.g., identifier, demographic, vitals), unit of measure, data type (e.g., numeric, binary), description, and examples.  \n**Sample submission files**","be2074e3":"## When to use Voting Classifier ?\n\nVoting classifier is a powerful method and can be a very good option when a single method shows bias towards a particular factor. This method can be used to derive a generalized fit of all the individual models.","c4dcdb00":"## Import the relevant libraries","1cbb7a88":"## AUC - ROC Curve :\n\nAUC - ROC curve is a performance measurement for classification problem at various thresholds settings. ROC is a probability curve and AUC represents degree or measure of separability. It tells how much model is capable of distinguishing between classes. Higher the AUC, better the model is at predicting 0s as 0s and 1s as 1s. Higher the AUC, better the model is at distinguishing between patients with disease and no disease.\nThe ROC curve is plotted with TPR against the FPR where TPR is on y-axis and FPR is on the x-axis.","5bad9190":"## Random Forest Classifier\n\nRandom forest consists of a large number of individual decision trees that operate as an ensemble. Each individual tree in the random forest spits out a class prediction and the class with the most votes becomes our model\u2019s prediction . The fudamental concept of Random Forest is that large number of relatively uncorrelated modelsoperating as a committee will outperform any of the individual constituent models","bad0a72e":"<img src = 'https:\/\/drive.google.com\/uc?id=1xy2wdrEKk0Y78N_WPni-rieltXXeYQlF'>\n\nPic Credit: medium.com","2bd857f2":"<img src = \"https:\/\/drive.google.com\/uc?id=1QFpY4TjfXYfKrCiRl1BcePuFEXOQ8-pX\">","66d875a3":"## Train\/Test Split :\n\nThe data is split into training data and test data. The training set contains a known output and the model learns on this data in order to be generalized to other data later on. We have the test dataset (or subset) in order to test our model\u2019s prediction on this subset.The above is achieved in Scikit-Learn library using the train_test_split method.","21dbeff7":"## Submissions :\n\nSubmissions will be evaluated on the Area under the Receiver Operating Characteristic (ROC) curve between the predicted mortality and the observed target (hospital_death)","4f81e545":"## Voting Classifier : \n\n**A Voting Classifier** is a machine learning model that trains on an ensemble of numerous models and predicts an output (class) based on their highest probability of chosen class as the output.It simply aggregates the findings of each classifier passed into Voting Classifier and predicts the output class based on the highest majority of voting. \n\nVoting Classifier supports two types of votings.\n\n**Hard Voting:** In hard voting, the predicted output class is a class with the highest majority of votes i.e the class which had the highest probability of being predicted by each of the classifiers. \n\n**Soft Voting:** In soft voting, the output class is the prediction based on the average of probability given to that class. ","910a11f7":"## Ensemble Learning : \n\nA collection of several models working together on a single set is called an Ensemble and the method is called **Ensemble Learning.**\n\n**Ensemble methods combine several trees base algorithms to construct better predictive performance than a single tree base algorithm. The main principle behind the ensemble model is that a group of weak learners come together to form a strong learner, thus increasing the accuracy of the model. When we try to predict the target variable using any machine learning technique, the main causes of difference in actual and predicted values are noise, variance, and bias.**","80372bbb":"## Gradient boosting Classifier :\n\nGradient boosting is a machine learning technique for regression and classification problems, which produces a prediction model in the form of an ensemble of weak prediction models, typically decision trees. The intuition behind gradient boosting algorithm is to repetitively leverage the patterns in residuals and strengthen a model with weak predictions and make it better. ","ad803c82":"<img src = 'https:\/\/drive.google.com\/uc?id=1Ghsr8dhLX7pbnQRYA8-YZPnz2-9V050f' >\n\nPic Credit : iq.opengenus.org","ac206d37":"## Read the dataset","c3cd8d5e":"## Scikit-learn :\n\nScikit-learn is the most useful library for machine learning in Python and the library contains a lot of effiecient tools for machine learning and statistical modeling including classification, regression, clustering and dimensionality reduction.**","ad42e83d":"References :\n\nLee, M., Raffa, J., Ghassemi, M., Pollard, T., Kalanidhi, S., Badawi, O., Matthys, K., Celi, L. A. (2020). WiDS (Women in Data Science) Datathon 2020: ICU Mortality Prediction. PhysioNet. doi:10.13026\/vc0e-th79\n\nGoldberger AL, Amaral LAN, Glass L, Hausdorff JM, Ivanov PCh, Mark RG, Mietus JE, Moody GB, Peng C-K, Stanley HE. PhysioBank, PhysioToolkit, and PhysioNet: Components of a New Research Resource for Complex Physiologic Signals (2003). Circulation. 101(23):e215-e220.\n\nhttps:\/\/iq.opengenus.org\/voting-classifier\/\n\nhttps:\/\/medium.com\/ml-research-lab\/ensemble-learning-the-heart-of-machine-learning-b4f59a5f9777\n\nhttps:\/\/towardsdatascience.com\/understanding-random-forest-58381e0602d2\n\nhttps:\/\/medium.com\/mlreview\/gradient-boosting-from-scratch-1e317ae4587d\n\nhttps:\/\/stackabuse.com\/understanding-roc-curves-with-python\/","7d3b3166":"<img src=\"https:\/\/drive.google.com\/uc?id=1fqvUVxD8GcnwREynD5HQYndqP68SnIBd\">\n\n## Objective\n\n**The challenge is to create a model that uses data from the first 24 hours of intensive care to predict patient survival. MIT's GOSSIS community initiative, with privacy certification from the Harvard Privacy Lab, has provided a dataset of more than 130,000 hospital Intensive Care Unit (ICU) visits from patients, spanning a one-year timeframe. This data is part of a growing global effort and consortium spanning Argentina, Australia, New Zealand, Sri Lanka, Brazil, and more than 200 hospitals in the United States.**"}}