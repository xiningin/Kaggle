{"cell_type":{"5e51acc1":"code","b3dcb8d1":"code","08ddbc54":"code","85165b89":"code","d525f0af":"code","c2ab42f7":"code","7091cf3e":"code","d9cc510a":"code","a1898c00":"code","7e637a5f":"code","f80c6182":"code","a0dc32bd":"code","96590eeb":"markdown","59c5408d":"markdown","a897bf45":"markdown","d142956f":"markdown","88430ead":"markdown","88d847ec":"markdown","def097a6":"markdown","47fe962f":"markdown","a25d5304":"markdown","aeaf6148":"markdown","e03a3a83":"markdown"},"source":{"5e51acc1":"## Import necessary packages\nimport pandas as pd\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.preprocessing.imputation import Imputer","b3dcb8d1":"iowa_file_path = '..\/input\/train.csv'\nhome_data = pd.read_csv(iowa_file_path)\ntest_data_path = '..\/input\/test.csv'\ntest_data = pd.read_csv(test_data_path)","08ddbc54":"new_home_data = pd.get_dummies(home_data)\nnew_test_data = pd.get_dummies(test_data)","85165b89":"missing_val_count_by_column = (new_home_data.isnull().sum())\nprint(missing_val_count_by_column[missing_val_count_by_column > 0])","d525f0af":"# Imputation\nmy_imputer = Imputer()\nimputed_home_data = my_imputer.fit_transform(new_home_data)\nimputed_test_data = my_imputer.fit_transform(new_test_data)","c2ab42f7":"train_df = pd.DataFrame(data=imputed_home_data, columns=new_home_data.columns) \ntest_df = pd.DataFrame(data=imputed_test_data, columns=new_test_data.columns) \nfinal_train, final_test = train_df.align(test_df, join='inner', axis=1)","7091cf3e":"# Create target object and call it y\ny = home_data.SalePrice\nX = final_train.drop(['Id'], axis=1)\n\n# Split into validation and training data\ntrain_X, val_X, train_y, val_y = train_test_split(X, y, random_state=1)\n","d9cc510a":"# Model1: Decision Tree Regressor\niowa_model = DecisionTreeRegressor(random_state=1)\n# Fit Model\niowa_model.fit(train_X, train_y)\n\n# Make validation predictions and calculate mean absolute error\nval_predictions = iowa_model.predict(val_X)\nval_mae = mean_absolute_error(val_predictions, val_y)\nprint(\"Validation MAE when not specifying max_leaf_nodes: {:,.0f}\".format(val_mae))\n\n# Using best value for max_leaf_nodes\niowa_model = DecisionTreeRegressor(max_leaf_nodes=100, random_state=1)\niowa_model.fit(train_X, train_y)\nval_predictions = iowa_model.predict(val_X)\nval_mae = mean_absolute_error(val_predictions, val_y)\nprint(\"Validation MAE for best value of max_leaf_nodes: {:,.0f}\".format(val_mae))","a1898c00":"# Model2: Random Forest Regressor\nrf_model = RandomForestRegressor(random_state=1)\nrf_model.fit(train_X, train_y)\nrf_val_predictions = rf_model.predict(val_X)\nrf_val_mae = mean_absolute_error(rf_val_predictions, val_y)\n\nprint(\"Validation MAE for Random Forest Model: {:,.0f}\".format(rf_val_mae))","7e637a5f":"# Model3: XGB Regressor\nfrom xgboost import XGBRegressor\n\nxgb_model = XGBRegressor()\n# Add silent=True to avoid printing out updates with each cycle\nxgb_model.fit(train_X, train_y, verbose=False)\nxgb_val_predictions = xgb_model.predict(val_X)\nxgb_val_mae = mean_absolute_error(xgb_val_predictions, val_y)\n\nprint(\"Validation MAE for XGBoost Model before tuning: {:,.0f}\".format(xgb_val_mae))\n\n# Add tuning parameters: n_estimators, learning_rate and early_stopping_rounds\nxgb_model = XGBRegressor(n_estimators=300, learning_rate=0.05)\nxgb_model.fit(train_X, train_y, early_stopping_rounds=5, \n             eval_set=[(val_X, val_y)], verbose=False)\nxgb_val_predictions = xgb_model.predict(val_X)\nxgb_val_mae = mean_absolute_error(xgb_val_predictions, val_y)\n\nprint(\"Validation MAE for XGBoost Model after tuning: {:,.0f}\".format(xgb_val_mae))","f80c6182":"# To improve accuracy, create a new Random Forest model which you will train on all training data\nrf_model_on_full_data = XGBRegressor(n_estimators=300, learning_rate=0.05)\n\n# fit rf_model_on_full_data on all data from the \nrf_model_on_full_data.fit(X, y, verbose=False)\n","a0dc32bd":"# make predictions which we will submit. \ntest_X = final_test.drop(['Id'], axis=1)\ntest_preds = rf_model_on_full_data.predict(test_X)\n\n# The lines below shows you how to save your data in the format needed to score it in the competition\noutput = pd.DataFrame({'Id': test_data.Id,\n                       'SalePrice': test_preds})\n\noutput.to_csv('submission.csv', index=False)","96590eeb":"## The remaining things to do in pre-processing:\n1. The type of data is numpy array. To change it into a pandas dataframe, use DataFrame constructor as belows.\n2. Ensure the test data is encoded in the same manner as the training data with the align command.","59c5408d":"## Use imputation to handle missing values","a897bf45":"**Result for XGBoost Regressor**\n* Validation MAE for XGBoost Model before tuning: 14,967\n* Validation MAE for XGBoost Model after tuning: 14,746","d142956f":"**Result for Decision Tree Regressor:**\n* Validation MAE when not specifying max_leaf_nodes: 27,477\n* Validation MAE for best value of max_leaf_nodes: 24,373","88430ead":"# Pre-processing\n## Read the training data and test data from the input","88d847ec":"**Result for Randam Forest Regressor**\n* Validation MAE for Random Forest Model: 18,197","def097a6":"# Creating a Model For the Competition\n\nBuild a Random Forest model and train it on all of **X** and **y**.  ","47fe962f":"# Make Predictions","a25d5304":"## Use one-hot-encoding to get dummy variables","aeaf6148":"# Model Training","e03a3a83":"# Steps in this notebook:\n1. Pre-processing:\n  Read the data from the input. Transform object types to categorical data. Handle missing values.\n2. Try Decision Tree, Random Forest and XGBoost model with the training data.\n3. Choose the model with best performance. In this case it's XGBoost Model. Predict home values in the test data with that model."}}