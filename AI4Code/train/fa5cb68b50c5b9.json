{"cell_type":{"77c7f465":"code","7b82b605":"code","ef8fda62":"code","e5ee1453":"code","3b538ded":"code","259bb9ef":"code","b821a4a9":"code","9055aac8":"code","a1013de6":"code","56877ce2":"code","61386110":"code","40592b37":"code","7f122ccf":"code","4eff858a":"code","2cae4b7d":"code","f2cb4c00":"code","658d9d85":"code","ea712cbb":"code","0fda9b80":"code","6f363ffa":"markdown","94ca4a74":"markdown","c354846f":"markdown","76cd78a9":"markdown","d7d609df":"markdown"},"source":{"77c7f465":"# Ahora haremos una tareas de modeling sobre los comentarios que van a entrenar un modelo LDA\n# Primero hay que limpiarlos y preprocesarlos, con la finalidad de bajar el ruido, se usar\u00e1 la librer\u00eda NLTK\n\nimport json, re\nimport pandas as pd \nfrom nltk.corpus import stopwords\nfrom nltk.stem import SnowballStemmer\nfrom nltk.tokenize import ToktokTokenizer\n\n# Ahora se se carga el archivo de noticias y se lo convierte en un objeto DataFrame\n\ndf = pd.read_csv('..\/input\/corpuscomenta\/corpuscomentarios.csv') \ndf.columns = ['Noticia']\nprint(df)","7b82b605":"# Operaci\u00f3n de limpieza de los textos de cada noticia, se crea una funci\u00f3n\n# Se eliminan los caracteres especiales, las palabras con un solo car\u00e1cter y se pasa todo a min\u00fasculas\n \ndef limpiar_texto(texto): # Funci\u00f3n para realizar la limpieza de un texto dado.\n    # Eliminamos los caracteres especiales\n    texto = re.sub(r'\\W', ' ', str(texto))\n    # Eliminado las palabras que tengo un solo caracter\n    texto = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', texto)\n    # Sustituir los espacios en blanco en uno solo\n    texto = re.sub(r'\\s+', ' ', texto, flags=re.I)\n    # Convertimos textos a minusculas\n    texto = texto.lower()\n    return texto\n\n# se crea un atributo nuevo que va a ser el resultado de la limpieza\n\ndf[\"Tokens\"] = df.Noticia.apply(limpiar_texto)\n\ndf.head()","ef8fda62":"# Se  tokeniza el texto procesad. Consiste en dividir los textos en tokens o terminos individuales. \n\ntokenizer = ToktokTokenizer() \ndf[\"Tokens\"] = df.Tokens.apply(tokenizer.tokenize)\n\ndf.head()\n","e5ee1453":"# Se eliminan las stopwords (palabras vac\u00edas o comunes) \n \nSTOPWORDS = set(stopwords.words(\"spanish\"))\n\ndef filtrar_stopword_digitos(tokens): # Filtra stopwords y digitos de una lista de tokens.\n    return [token for token in tokens if token not in STOPWORDS \n            and not token.isdigit()]\n\ndf[\"Tokens\"] = df.Tokens.apply(filtrar_stopword_digitos)\n\ndf.head()","3b538ded":"# Se lleva a la forma de ra\u00edz a cada palabra usando el proceso de stemming\n''' \nstemmer = SnowballStemmer(\"spanish\")\n\ndef stem_palabras(tokens): #     Reduce cada palabra de una lista dada a su ra\u00edz.\n    return [stemmer.stem(token) for token in tokens]\n\ndf[\"Tokens\"] = df.Tokens.apply(stem_palabras)\n\ndf.head()\n'''","259bb9ef":"# Fin de la primera parte del proceso\n\n# A continuaci\u00f3n mostramos las primeras diez palabras procesadas de una noticia, la primera \n\nprint(df.Tokens[0][0:10])\n","b821a4a9":"# Se debe construir un diccionario con todas las palabras y un corpus con la frecuencia de las palabras\n# en cada documento. Estas ser\u00e1n las principales entradas de nuestro modelo LDA. \n\n# Importamos las librer\u00edas que vamos a necesitar para este tutorial pr\u00e1ctico:\n\nfrom gensim.corpora import Dictionary\nfrom gensim.models import LdaModel\nimport random\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom wordcloud import WordCloud\n%matplotlib inline\n","9055aac8":"# Los diccionarios son objetos que asignan un identificador num\u00e9rico a cada palabra \u00fanica y \n# se pueden usar para obtener el identificador a partir de la palabra y viceversa.\n\n# La generaci\u00f3n de estos diccionarios con la librer\u00eda Gensim es tan f\u00e1cil como crear un objeto de tipo Dictionary \n# pas\u00e1ndole como argumento las listas de palabras que est\u00e1 en el atributo del dataframe llamado \"tokens\"\n\n\ndiccionario = Dictionary(df.Tokens)\n\nprint(f'N\u00famero de tokens: {len(diccionario)}')","a1013de6":"# Se limpian t\u00e9rminos del diccionario.Por ejemplo las palabras m\u00e1s raras o demasiado frecuentes. \n# Se aplica la funci\u00f3n filter_extremes, que nos dejar\u00e1 \u00fanicamente aquellos tokens que se encuentran \n# en al menos 2 documentos (no_below) y los que est\u00e1n contenidos en no m\u00e1s del 80% de documentos (no_above). \n\ndiccionario.filter_extremes(no_below=2, no_above = 0.8)\n\nprint(f'N\u00famero de tokens: {len(diccionario)}') # cantidad de tokens restantes luego del filtro\n","56877ce2":"#  Se inicializa el corpus en base al diccionario creado. \n# Cada documento se transformar\u00e1 en una bolsa de palabras (BOW) con las frecuencias de aparici\u00f3n.\n\n# Al aplicar esta t\u00e9cnica veremos que cada documento est\u00e1 representado como una lista de tuplas \n# donde el primer elemento es el identificador num\u00e9rico de la palabra y el segundo es el n\u00famero \n# de veces que esa palabra aparece en el documento.\n\n# Creamos el corpus \ncorpus = [diccionario.doc2bow(noticia) for noticia in df.Tokens]\n\n# Mostramos el BOW de una noticia cualquiera, la seis en el ejemplo\nprint(corpus[6]) \n\n# Por ej., una  tupla x si tuviese (3, 1) indica que, para el documento x, la palabra con el identificador \n# 3 aparece una vez. # Mediante el comando (diccionario[3]) se puede ver que el identificador 3 \n# corresponde al token \"???\".\n\nprint(diccionario[3]) ","61386110":"lda = LdaModel(corpus=corpus, id2word=diccionario, \n               num_topics=10, random_state=42, \n               chunksize=1000, passes=10, alpha='auto')\n\n# Para visualizar los t\u00f3picos extra\u00eddos se usa print_topics\n# Se indic\u00e1 el n\u00famero de t\u00f3picos y el n\u00famero de palabras por t\u00f3pico que queremos que se muestre:\n\ntopicos = lda.print_topics(num_words=5, num_topics=20)\n\nfor topico in topicos:\n    print(topico)\n","40592b37":"for i in range(0, 10):\n    plt.figure()\n    plt.imshow(WordCloud(background_color='white', prefer_horizontal=1.0)\n               .fit_words(dict(lda.show_topic(i, 20))))\n    plt.axis(\"off\")\n    plt.title(\"T\u00f3pico \" + str(i))\n    plt.show() # Muestra nube de palabras de cada t\u00f3pico","7f122ccf":"# Se empieza escogiendo un comentario del corpus al azar y mostramos su contenido \n\nindice_noticia = random.randint(0,len(df))\nnoticia = df.iloc[indice_noticia]\nprint(noticia.Noticia)\n\n# Primero debemos obtener la representaci\u00f3n BOW del documento y la distribuci\u00f3n de los t\u00f3picos:\n\nbow_noticia = corpus[indice_noticia]\ndistribucion_noticia = lda[bow_noticia]\n\n# Luego, sacamos los \u00edndices y la contribuci\u00f3n (proporci\u00f3n) de los t\u00f3picos m\u00e1s significativos para \n# nuestra noticia y los mostramos en un gr\u00e1fico de barras para un mejor entendimiento.\n\n# Indices de los topicos mas significativos\ndist_indices = [topico[0] for topico in lda[bow_noticia]]\n\n# Contribuci\u00f3n de los topicos mas significativos\ndist_contrib = [topico[1] for topico in lda[bow_noticia]]\ndistribucion_topicos = pd.DataFrame({'Topico':dist_indices,\n                                     'Contribucion':dist_contrib })\ndistribucion_topicos.sort_values('Contribucion', \n                                 ascending=False, inplace=True)\nax = distribucion_topicos.plot.bar(y='Contribucion',x='Topico', \n                                   rot=0, color=\"orange\",\n                                   title = 'T\u00f3picos mas importantes'\n                                   'de noticia ' + str(indice_noticia))\n","4eff858a":"# Se observan los t\u00f3picos m\u00e1s predominante en la noticia\n# Ahora imprimimos las palabras m\u00e1s significativas de estos.\n\nfor ind, topico in distribucion_topicos.iterrows():\n    print(\"*** T\u00f3pico: \" + str(int(topico.Topico)) + \" ***\")\n    palabras = [palabra[0] for palabra in lda.show_topic(\n        topicid=int(topico.Topico))]\n    palabras = ', '.join(palabras)\n    print(palabras, \"\\n\")\n","2cae4b7d":"\n# En una segunda prueba, asignaremos los t\u00f3picos a un nuevo documento no utilizado \n# en el entrenamiento del modelo LDA.\n\narticulo_nuevo = \"Julio ayuda, por favor, ma\u00f1ana tengo examen de problemas con ecuaciones. Necesito tu ayuda, eres un crack, me has salvado, dios, bendiciones para ti desde colombia. Nos seguimos viendo, este semestro voy a meterle muchas horas crack\"\n\narticulo_nuevo = limpiar_texto(articulo_nuevo)\narticulo_nuevo = tokenizer.tokenize(articulo_nuevo)\narticulo_nuevo = filtrar_stopword_digitos(articulo_nuevo)\n#articulo_nuevo = stem_palabras(articulo_nuevo)\n\n#articulo_nuevo\n","f2cb4c00":"# Se convierte a bow el art\u00edculo nuevo a testear\n\nbow_articulo_nuevo = diccionario.doc2bow(articulo_nuevo)\n\n# Se muestran los resultados:\n\ndist_indices = [topico[0] for topico in lda[bow_articulo_nuevo]] # Indices de los topicos mas significativos\n\n# Contribucion de los topicos mas significativos\ndist_contrib = [topico[1] for topico in lda[bow_articulo_nuevo]]\n\ndistribucion_topicos = pd.DataFrame({'Topico':dist_indices,\n                                     'Contribucion':dist_contrib })\n\ndistribucion_topicos.sort_values('Contribucion', \n                                 ascending=False, inplace=True)\n\nax = distribucion_topicos.plot.bar(y='Contribucion',x='Topico', \n                                   rot=0, color=\"green\",\n                                   title = 'T\u00f3picos m\u00e1s importantes' \n                                   'para documento nuevo')\n","658d9d85":"# El t\u00f3pico 6 es el m\u00e1s significante con diferencia en la noticia. Imprimimos de nuevo las palabras m\u00e1s significativas:\n\nfor ind, topico in distribucion_topicos.iterrows():\n    print(\"*** T\u00f3pico: \" + str(int(topico.Topico)) + \" ***\")\n    palabras = [palabra[0] for palabra in lda.show_topic(\n        topicid=int(topico.Topico))]\n    palabras = ', '.join(palabras)\n    print(palabras, \"\\n\")\n","ea712cbb":"# Se guarda el modelo y el diccionario para utilizarlo m\u00e1s adelante.  \n\n# lda.save(\"..\/kaggle\/working\/articulos.model\")\n# diccionario.save(\"..\/kaggle\/working\/articulos.dictionary\")\n \n# Fin. En este tutorial hemos visto que el modelo LDA es una buena herramienta para clasificar textos.\n# Conviene utilizar muchas m\u00e1s noticias para entrenar el modelo y de esta forma obtener un mejor rendimiento.\n","0fda9b80":"# Visualizaci\u00f3n de los datos, con reducci\u00f3n de dimensionalidad, por m\u00e9todo TSNE \n\n# The t-distributed stochastic neighbor embedding (t-SNE) algorithm projects high-dimensional vectors to 2-D space. \n\nfrom gensim.test.utils import common_texts\nfrom gensim.models import Word2Vec\nfrom sklearn.manifold import TSNE\nimport matplotlib.pyplot as plt\n\n# Anda pero demora much\u00edsmo, hay que tunearlo con los t\u00e9rminos m\u00e1s importantes\n\nmodel = Word2Vec(sentences=df[\"Tokens\"], window=5, min_count=200)\n\nlabels = [i for i in model.wv.key_to_index.keys()]\ntokens = model.wv[labels]\n\ntsne_model = TSNE(init='pca' )\n\nnew_values = tsne_model.fit_transform(tokens)\n\nx = []\ny = []\nfor value in new_values:\n    x.append(value[0])\n    y.append(value[1])\n    \nplt.figure(figsize=(20, 35)) \nfor i in range(new_values.shape[0]):\n    plt.scatter(x[i],y[i])\n    plt.annotate(labels[i],\n                 xy=(x[i], y[i]),\n                 xytext=(5, 2),\n                 textcoords='offset points',\n                 ha='right',\n                 va='bottom')\nplt.plot()\n\n","6f363ffa":"# Evaluaci\u00f3n del modelo\n\nPara evaluar el rendimiento del modelo, estimaremos los t\u00f3picos en dos documentos diferentes. En el primer caso, escogeremos un documento de entre las noticias utilizadas en el corpus para entrenar el modelo y para la segunda prueba utilizaremos una nueva noticia.","94ca4a74":"# Construcci\u00f3n del modelo LDA\n\nSe crea un objeto **LdaModel** de la librer\u00eda Gensim pas\u00e1ndole como argumento el corpus y el diccionario y le indicamos adicionalmente los siguientes par\u00e1metros:\n\n* num_topics: n\u00famero de t\u00f3picos. Para este ejemplo se extraen 10 t\u00f3picos.\n* random_state: par\u00e1metro para controlar la aleatoriedad del proceso de entrenamiento y que nos devuelva siempre los mismos resultados.\n* chunksize: n\u00famero de documentos que ser\u00e1 utilizado en cada pasada de entrenamiento.\n* passes: n\u00famero de pasadas por el corpus durante el entrenamiento.\n* alpha: representa la densidad de t\u00f3picos por documento. Un mayor valor de este par\u00e1metro implica que los documentos est\u00e9n compuestos de m\u00e1s t\u00f3picos. En este caso, fijamos el valor en auto.\n\n","c354846f":"En cada t\u00f3pico se observan  las cinco palabras que m\u00e1s contribuyen a ese t\u00f3pico y sus pesos en el mismo. El peso nos indica como de importante es la palabra en ese t\u00f3pico. \n\nEsto significa que, por ejemplo, las cinco principales palabras en el t\u00f3pico 32 son: azul, eolic, sue\u00f1, arroy y luz. Por lo tanto, esta claro que este t\u00f3pico se centra en el medio ambiente y la energ\u00eda renovable.\n\nEs f\u00e1cil tambi\u00e9n interpretar que el t\u00f3pico 26 tiene relaci\u00f3n con la econom\u00eda al contener palabras como mill\u00f3n, eur y banc y el t\u00f3pico 38 se centra en elecciones, ya que sus palabras m\u00e1s significativas son encuest, votant y dat.\n\nTambi\u00e9n podemos visualizar las palabras m\u00e1s importantes de cada t\u00f3pico mediante nube de palabras, donde el tama\u00f1o de cada palabra corresponde con su contribuci\u00f3n en el t\u00f3pico. ","76cd78a9":"# Segunda parte: construcci\u00f3n de un dicccionario y entrenamiento del modelo LDA.","d7d609df":"# An\u00e1lisis por LDA aplicado a un corpus de comentarios de edutubers\nOriginal en https:\/\/elmundodelosdatos.com\/topic-modeling-gensim-fundamentos-preprocesamiento-textos\/\n\nEl topic modeling es una t\u00e9cnica no supervisada de NLP, capaz de detectar y extraer de manera autom\u00e1tica relaciones sem\u00e1nticas latentes de grandes vol\u00famenes de informaci\u00f3n.\n\nEstas relaciones son los llamados t\u00f3picos, que son un conjunto de palabras que suelen aparecer juntas en los mismos contextos y nos permiten observar relaciones que ser\u00edamos incapaces de observar a simple vista.\n\nExisten diversas t\u00e9cnicas que pueden ser usadas para obtener estos t\u00f3picos. El principal algoritmo y que adem\u00e1s ser\u00e1 el que utilizaremos en esta publicaci\u00f3n, es el modelo latent dirichlet allocation (LDA), propuesto por David Blei, que nos devuelve por un lado los diferentes t\u00f3picos que componen la colecci\u00f3n de documentos y por otro lado cu\u00e1nto de cada t\u00f3pico est\u00e1 presente en cada documento. Los t\u00f3picos consisten en una distribuci\u00f3n de probabilidades de aparici\u00f3n de las distintas palabras del vocabulario.\n\nPaper base: https:\/\/www.jmlr.org\/papers\/volume3\/blei03a\/blei03a.pdf\n"}}