{"cell_type":{"5b169af3":"code","17d29aec":"code","1aa35a7d":"code","db1692e1":"code","56bcd757":"code","f1eae388":"code","27803870":"code","cf76f5d0":"code","b074407a":"code","cf0a0f7d":"code","5a84b525":"code","e129b678":"code","80897329":"code","8295ea2e":"code","f10dcfdf":"code","fb537b18":"code","7a55a06d":"code","7b8b1968":"code","3446c649":"code","85289f2f":"code","5c40f5f7":"code","8b01c30f":"code","2b2f8ce3":"code","9c28df46":"code","50421def":"code","3a5f6732":"code","ca5f3e7d":"code","4f8c19f5":"code","14aca4bc":"code","fdeb98a2":"code","dc60259c":"code","b6b0c62f":"code","dde4b94a":"code","44e67b63":"code","4f9efe4e":"code","81290cd0":"code","36eb9670":"code","8a0a4f55":"code","ab00370b":"code","7e4bdedb":"code","13e76c63":"code","01f17cfe":"code","3c6660ce":"code","0774d7bf":"code","7e59c7ca":"code","5dd9c61d":"markdown","322fd8ab":"markdown","a48a59be":"markdown","90b35631":"markdown","cd5e37ce":"markdown","1d3c57e0":"markdown","41c73028":"markdown","3a9cfb32":"markdown","b9d3989c":"markdown","a4677f2b":"markdown","ff7f7e23":"markdown","d0d69ba5":"markdown","ea05f3d2":"markdown","34ee5641":"markdown","402598f0":"markdown","69573bb6":"markdown","b6310eef":"markdown","cb6a20ae":"markdown","919b6bbe":"markdown","1fc4b2a0":"markdown","4f3bad51":"markdown","8253d0ce":"markdown","c9d2ff73":"markdown","b9d86985":"markdown","05220a31":"markdown","2250690b":"markdown","76fd9389":"markdown","3ebceeee":"markdown","34ccff38":"markdown","72fb8be7":"markdown","1c1afbfb":"markdown","afa07f9e":"markdown","0da573cc":"markdown","84c30031":"markdown","6decf397":"markdown","0f80aeec":"markdown","bbf20ae5":"markdown","a2cf3ad0":"markdown","dab0087d":"markdown","123c79dc":"markdown","0008efef":"markdown","3f08d730":"markdown","8d029114":"markdown","b6deebee":"markdown","966ba05e":"markdown","2328f8a1":"markdown","4b37cbc3":"markdown","7dbdc4bc":"markdown","343b2610":"markdown","385b5491":"markdown","df407bda":"markdown","91722a09":"markdown","44dadd0e":"markdown","51eda3df":"markdown","4b7b8c18":"markdown","fe38551b":"markdown","3f95b7a8":"markdown","f844a87c":"markdown","094fac27":"markdown","206ea44c":"markdown","b5b48cc9":"markdown","86be9d43":"markdown","1b42ba46":"markdown","7099d0bb":"markdown","6b058dec":"markdown","46bf3bc8":"markdown","615b540b":"markdown","ff3cd60d":"markdown","ea975482":"markdown"},"source":{"5b169af3":"# comandos m\u00e1gicos que n\u00e3o se comunicam com a linguagem Python e sim diretamente com o kernel do Jupyter\n# come\u00e7am com %\n\n%load_ext autoreload\n%autoreload 2\n\n%matplotlib inline","17d29aec":"# importando os principais m\u00f3dulos que usaremos ao longo da aula\n\n# a vers\u00e3o 0.3.4, lan\u00e7ada em 14\/09\/2020, n\u00e3o funcionou neste kernel \n!pip install pyreadr==v0.3.3\n\nimport pyreadr\n\nimport numpy as np\nimport pandas as pd\n\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport sklearn.decomposition","1aa35a7d":"print(np.__version__)\nprint(pd.__version__)\nprint(matplotlib.__version__)\nprint(sklearn.__version__)\nprint(sns.__version__)","db1692e1":"from IPython.display import YouTubeVideo\nYouTubeVideo(\"RmO9-Tx31x8\")","56bcd757":"YouTubeVideo(\"rg6-k91hvy0\")","f1eae388":"%%time\n\n# troque aqui pela localiza\u00e7\u00e3o do dataset na sua m\u00e1quina\nPATH = '\/kaggle\/input\/tennessee-eastman-process-simulation-dataset\/'\n\ntrain_normal_path = PATH+'TEP_FaultFree_Training.RData'\ntrain_faulty_path = PATH+'TEP_Faulty_Training.RData'\n\ntest_normal_path = PATH+'TEP_FaultFree_Testing.RData'\ntest_faulty_path = PATH+'TEP_Faulty_Testing.RData'\n\ntrain_normal_complete = pyreadr.read_r(train_normal_path)['fault_free_training']\n#train_faulty_complete = pyreadr.read_r(train_fault_path)['faulty_training']\n\n#test_normal_complete = pyreadr.read_r(test_normal_path)['fault_free_testing']\ntest_faulty_complete = pyreadr.read_r(test_faulty_path)['faulty_testing']","27803870":"train_normal_complete","cf76f5d0":"test_faulty_complete","b074407a":"df_train = train_normal_complete[train_normal_complete.simulationRun==1].iloc[:,3:]\n\ndf_test = test_faulty_complete[(test_faulty_complete.simulationRun==1)&\n                               (test_faulty_complete.faultNumber==1)].iloc[:,3:]","cf0a0f7d":"fig, ax = plt.subplots(13,4,figsize=(30,90))\n\nfor i in range(df_train.shape[1]):\n    \n    x = df_train.iloc[:,i]\n    \n    mean  = x.mean()\n    std = x.std(ddof=1)\n    \n    LCL = mean-3*std\n    UCL = mean+3*std\n    \n    x.plot(ax=ax.ravel()[i]) \n\n    ax.ravel()[i].legend();\n    \n    ax.ravel()[i].axhline(mean,c='k')\n    ax.ravel()[i].axhline(LCL,ls='--',c='r')\n    ax.ravel()[i].axhline(UCL,ls='--',c='r')","5a84b525":"fig, ax = plt.subplots(13,4,figsize=(30,70))\n\nfor i in range(df_train.shape[1]):\n    \n    x = df_train.iloc[:,i]\n    x_ts = df_test.iloc[:,i]\n\n    mean  = x.mean()\n    std = x.std(ddof=1)\n    \n    LCL = mean-3*std\n    UCL = mean+3*std\n    \n    x_ts.plot(ax=ax.ravel()[i]) \n\n    ax.ravel()[i].legend();\n    \n    ax.ravel()[i].axhline(mean,c='k')\n    ax.ravel()[i].axhline(LCL,ls='--',c='r')\n    ax.ravel()[i].axhline(UCL,ls='--',c='r')\n    \n    ax.ravel()[i].axvline(160,c='g')","e129b678":"# adaptado de https:\/\/stackoverflow.com\/a\/38705297\/11439214\n\nfrom scipy.stats import multivariate_normal\nfrom mpl_toolkits.mplot3d import Axes3D\n\n#Parameters to set\nmu_x = 0\nvariance_x = 1\n\nmu_y = 0\nvariance_y = 5\n\n#Create grid and multivariate normal\nx = np.linspace(-3,3,500)\ny = np.linspace(-6,6,500)\nX, Y = np.meshgrid(x,y)\npos = np.empty(X.shape + (2,))\npos[:, :, 0] = X; pos[:, :, 1] = Y\nrv = multivariate_normal([mu_x, mu_y], [[variance_x, 0], [0, variance_y]])\n\nfig = plt.figure(figsize=(22,6))\n\n#Plotting 3d\nax = fig.add_subplot(121, projection='3d')\nax.plot_surface(X, Y, rv.pdf(pos), cmap='viridis', linewidth=0)\nax.set_xlabel('$x_1$')\nax.set_ylabel('$x_2$')\nax.set_zlabel('$p(\\mathbf{x})$')\n\n# Plotting contour\nax = fig.add_subplot(122)\nax.contourf(X, Y, rv.pdf(pos) ,cmap='viridis')\nax.set_xlabel('$x_1$')\nax.set_ylabel('$x_2$');","80897329":"# exemplo adaptado de https:\/\/jakevdp.github.io\/PythonDataScienceHandbook\/05.09-principal-component-analysis.html\n\nrng = np.random.RandomState(1)\nX = np.dot(rng.rand(2, 2), rng.randn(2, 200)).T\n\nplt.scatter(X[:, 0], X[:, 1])\nplt.axis('equal')\nplt.xlabel('x1')\nplt.ylabel('x2');","8295ea2e":"pca = sklearn.decomposition.PCA(n_components=2)\npca.fit(X)","f10dcfdf":"print('componentes:')\nprint(pca.components_)\n\nprint('\\nvari\u00e2ncias explicadas:')\nprint(pca.explained_variance_)","fb537b18":"def draw_vector(v0, v1, ax=None):\n    ax = ax or plt.gca()\n    arrowprops=dict(arrowstyle='->',\n                    linewidth=2,\n                    shrinkA=0, shrinkB=0)\n    ax.annotate('', v1, v0, arrowprops=arrowprops)","7a55a06d":"plt.scatter(X[:, 0], X[:, 1], alpha=0.3)\n\nfor length, vector in zip(pca.explained_variance_, pca.components_):\n    v = vector * 3 * np.sqrt(length)\n    draw_vector(pca.mean_, pca.mean_ + v)\n    \nplt.axis('equal')\nplt.xlabel('x1')\nplt.ylabel('x2');","7b8b1968":"T = pca.transform(X)","3446c649":"plt.scatter(T[:, 0], T[:, 1], alpha=0.3)\nplt.axis('equal')\nplt.xlabel('t1')\nplt.ylabel('t2');","85289f2f":"pca.components_.round(3)","5c40f5f7":"pca = sklearn.decomposition.PCA(n_components=1)\npca.fit(X)\n\nT = pca.transform(X)","8b01c30f":"display(X.shape)\ndisplay(T.shape)","2b2f8ce3":"plt.plot(T, np.zeros(len(T)),'.')\nplt.gca().get_yaxis().set_visible(False)\nplt.xlabel('t1');","9c28df46":"X_reconstruido = pca.inverse_transform(T)","50421def":"plt.scatter(X[:, 0], X[:, 1], alpha=0.2, label = 'original')\nplt.scatter(X_reconstruido[:, 0], X_reconstruido[:, 1], alpha=0.8, label='reconstru\u00e7\u00e3o')\nplt.legend()\nplt.axis('equal')\nplt.xlabel('x1')\nplt.ylabel('x2');","3a5f6732":"display(X.shape)\ndisplay(T.shape)\ndisplay(X_reconstruido.shape)","ca5f3e7d":"class PCA():\n   \n    ###############\n\n    def __init__ (self, a = 0.9):\n        '''\n        Construtor: fun\u00e7\u00e3o chamada toda vez que um objeto PCA \u00e9 inicializado\n        '''\n        # se 0<=a<1,  'a' indica a fra\u00e7ao de variancia explicada desejada\n        # se a>=1,    'a' indica o numero de componentes desejado\n        self.a = a\n   \n    ###############\n\n    def fit(self, X, conf_Q = 0.99, conf_T2 = 0.99, plot = True):\n        '''\n        Fun\u00e7\u00e3o para treino do modelo\n        '''       \n        # guardando m\u00e9dias e desvios-padr\u00e3o do treino\n        self.mu_train = X.mean(axis=0)\n        self.std_train = X.std(axis=0)        \n       \n        # normalizando dados de treino\n        X = np.array(((X - self.mu_train)\/self.std_train))\n       \n        # calculando a matriz de covari\u00e2ncias dos dados\n        Cx = np.cov(X, rowvar=False)\n        \n        # aplicando decomposi\u00e7\u00e3o em autovalores e autovetores\n        self.L, self.P = np.linalg.eig(Cx)\n        \n        # fra\u00e7\u00f5es da vari\u00e2ncia explicada\n        fv = self.L\/np.sum(self.L)\n        \n        # fra\u00e7\u00f5es da vari\u00e2ncia explicada acumuladas\n        fva = np.cumsum(self.L)\/sum(self.L)\n       \n        # definindo n\u00famero de componentes\n        if self.a>0 and self.a<1:\n            self.a = np.where(fva>self.a)[0][0]+1 \n            \n        # calculando limites de detec\u00e7\u00e3o\n\n        # limite da estat\u00edstica T^2\n        from scipy.stats import f\n        F = f.ppf(conf_T2, self.a, X.shape[0]-self.a)\n        self.T2_lim = ((self.a*(X.shape[0]**2-1))\/(X.shape[0]*(X.shape[0]-self.a)))*F\n        \n        # limite da estat\u00edstica Q\n        theta = [np.sum(self.L[self.a:]**(i)) for i in (1,2,3)]\n        ho = 1-((2*theta[0]*theta[2])\/(3*(theta[1]**2)))\n        from scipy.stats import norm\n        nalpha = norm.ppf(conf_Q)\n        self.Q_lim = (theta[0]*(((nalpha*np.sqrt(2*theta[1]*ho**2))\/theta[0])+1+\n                                ((theta[1]*ho*(ho-1))\/theta[0]**2))**(1\/ho))\n        \n        # plotando vari\u00e2ncias explicadas\n        if plot:\n            fig, ax = plt.subplots()\n            ax.bar(np.arange(len(fv)),fv)\n            ax.plot(np.arange(len(fv)),fva)\n            ax.set_xlabel('N\u00famero de componentes')\n            ax.set_ylabel('Vari\u00e2ncia dos dados')\n            ax.set_title('PCA - Vari\u00e2ncia Explicada');\n\n    ###############\n            \n    def predict(self, X):\n        '''\n        Fun\u00e7\u00e3o para teste do modelo\n        '''\n            \n        # normalizando dados de teste (usando os par\u00e2metros do treino!)\n        X = np.array((X - self.mu_train)\/self.std_train)\n\n        # calculando estat\u00edstica T^2\n        T = X@self.P[:,:self.a]\n        self.T2 = np.array([T[i,:]@np.linalg.inv(np.diag(self.L[:self.a]))@T[i,:].T for i in range(X.shape[0])])\n\n        # calculando estat\u00edstica Q\n        e = X - X@self.P[:,:self.a]@self.P[:,:self.a].T\n        self.Q  = np.array([e[i,:]@e[i,:].T for i in range(X.shape[0])])\n        \n        # calculando contribui\u00e7\u00f5es para Q\n        self.c = np.absolute(X*e) \n                \n    ###############\n\n    def plot_control_charts(self, fault = None):\n        '''\n        Fun\u00e7\u00e3o para plotar cartas de controle\n        '''        \n        fig, ax = plt.subplots(1,2, figsize=(15,3))\n\n        ax[0].semilogy(self.T2,'.')\n        ax[0].axhline(self.T2_lim,ls='--',c='r');\n        ax[0].set_title('Carta de Controle $T^2$')\n        \n        ax[1].semilogy(self.Q,'.')\n        ax[1].axhline(self.Q_lim,ls='--',c='r')\n        ax[1].set_title('Carta de Controle Q')\n \n        if fault is not None:\n            ax[0].axvline(fault, c='k')\n            ax[1].axvline(fault, c='k')\n\n    ###############\n            \n    def plot_contributions(self, fault = None, \n                           index = None, \n                           columns = None):\n        '''\n        Fun\u00e7\u00e3o para plotar mapas de contribui\u00e7\u00e3o\n        '''\n        fig, ax = plt.subplots(figsize=(20, 6))\n        \n        c = pd.DataFrame(self.c, \n                         index = index,\n                         columns = columns)\n    \n        sns.heatmap(c, ax = ax, \n                    yticklabels=int(self.c.shape[0]\/10),\n                    cmap = plt.cm.Blues);\n        \n        ax.set_title('Contribui\u00e7\u00f5es parciais para Q')\n        \n        if fault is not None:\n            ax.axhline(y=c.index[fault],\n                       ls='--', c='k')","4f8c19f5":"df_train = train_normal_complete[(train_normal_complete.simulationRun>=1)&\n                                 (train_normal_complete.simulationRun<5)].iloc[:,3:]\n\ndf_test = train_normal_complete[(train_normal_complete.simulationRun>5)&\n                                (train_normal_complete.simulationRun<10)].iloc[:,3:]","14aca4bc":"pca = PCA(a = 0.9)\npca.fit(df_train)","fdeb98a2":"pca.a","dc60259c":"pca.predict(df_test)","b6b0c62f":"pca.plot_control_charts()\nplt.suptitle('IDV(0)');","dde4b94a":"print('Taxa de falsos alarmes\\n--------------')\n\nprint(f'T2: {(pca.T2>pca.T2_lim).sum()\/pca.T2.shape[0]}')\nprint(f'Q: {(pca.Q>pca.Q_lim).sum()\/pca.Q.shape[0]}')","44e67b63":"IDV = 1\n\ndf_test = test_faulty_complete[(test_faulty_complete.faultNumber==IDV) & \n                               (test_faulty_complete.simulationRun==1)].iloc[:,3:]\n\npca.predict(df_test)\n\npca.plot_control_charts(fault=160)\nplt.suptitle(f'IDV({IDV})');\n\npca.plot_contributions(fault=160, columns = df_test.columns)","4f9efe4e":"print(f'Taxas de detec\u00e7\u00e3o de falhas - IDV({IDV})\\n--------------')\n\nprint(f'T2: {(pca.T2[160:]>pca.T2_lim).sum()\/pca.T2[160:].shape[0]}')\nprint(f'Q: {(pca.Q[160:]>pca.Q_lim).sum()\/pca.Q[160:].shape[0]}')","81290cd0":"IDV = 4\n\ndf_test = test_faulty_complete[(test_faulty_complete.faultNumber==IDV) & \n                               (test_faulty_complete.simulationRun==1)].iloc[:,3:]\n\npca.predict(df_test)\n\npca.plot_control_charts(fault=160)\nplt.suptitle(f'IDV({IDV})');\n\npca.plot_contributions(fault=160, columns = df_test.columns)\n\nprint(f'Taxas de detec\u00e7\u00e3o de falhas - IDV({IDV})\\n--------------')\n\nprint(f'T2: {(pca.T2[160:]>pca.T2_lim).sum()\/pca.T2[160:].shape[0]}')\nprint(f'Q: {(pca.Q[160:]>pca.Q_lim).sum()\/pca.Q[160:].shape[0]}')","36eb9670":"IDV = 11\n\ndf_test = test_faulty_complete[(test_faulty_complete.faultNumber==IDV) & \n                               (test_faulty_complete.simulationRun==1)].iloc[:,3:]\n\npca.predict(df_test)\n\npca.plot_control_charts(fault=160)\nplt.suptitle(f'IDV({IDV})');\n\npca.plot_contributions(fault=160, columns = df_test.columns)\n\nprint(f'Taxas de detec\u00e7\u00e3o de falhas - IDV({IDV})\\n--------------')\n\nprint(f'T2: {(pca.T2[160:]>pca.T2_lim).sum()\/pca.T2[160:].shape[0]}')\nprint(f'Q: {(pca.Q[160:]>pca.Q_lim).sum()\/pca.Q[160:].shape[0]}')","8a0a4f55":"def apply_lag (df, lag = 1):\n       \n    from statsmodels.tsa.tsatools import lagmat\n    array_lagged = lagmat(df, maxlag=lag,\n                          trim=\"forward\", original='in')[lag:,:]  \n    new_columns = []\n    for l in range(lag):\n        new_columns.append(df.columns+'_lag'+str(l+1))\n    columns_lagged = df.columns.append(new_columns)\n    index_lagged = df.index[lag:]\n    df_lagged = pd.DataFrame(array_lagged, index=index_lagged,\n                             columns=columns_lagged)\n       \n    return df_lagged ","ab00370b":"exemplo = pd.DataFrame([[1.,10.],[2.,20.],[3.,30.],[4.,40.]],columns=['A','B'])\nexemplo","7e4bdedb":"apply_lag(exemplo)","13e76c63":"apply_lag(exemplo, lag=2)","01f17cfe":"lag = 1\n\npca.fit(apply_lag(df_train, lag=lag))\n\nIDV = 11\n\ndf_test = test_faulty_complete[(test_faulty_complete.faultNumber==IDV) & \n                               (test_faulty_complete.simulationRun==1)].iloc[:,3:]\n\npca.predict(apply_lag(df_test, lag=lag))\n\npca.plot_control_charts(fault=160-lag)\nplt.suptitle(f'IDV({IDV})');\n\npca.plot_contributions(fault=160-lag, columns = apply_lag(df_test, lag=lag).columns)\n\nprint(f'Taxas de detec\u00e7\u00e3o de falhas - IDV({IDV})\\n--------------')\n\nprint(f'T2: {(pca.T2[160-lag:]>pca.T2_lim).sum()\/pca.T2[160-lag:].shape[0]}')\nprint(f'Q: {(pca.Q[160-lag:]>pca.Q_lim).sum()\/pca.Q[160-lag:].shape[0]}')","3c6660ce":"def filter_noise_ma (df, W = 5):\n\n    import copy\n    \n    new_df = copy.deepcopy(df)\n\n    for column in df:\n        new_df[column] = new_df[column].rolling(W).mean()\n        \n    return new_df.drop(df.index[:W])","0774d7bf":"fig, ax = plt.subplots(1,5, figsize=(20,3))\n\ndf_test['xmv_10'].plot(ax=ax[0])\nax[0].set_title('Sem filtro')\n\ni = 1\n\nfor W in [5,10,15,20]:\n    filter_noise_ma(pd.DataFrame(df_test['xmv_10']), W=W).plot(ax=ax[i], legend=False)\n    ax[i].set_title(f'W={W}')\n    i+=1","7e59c7ca":"W = 5\n\npca.fit(filter_noise_ma(df_train, W=W))\n\nIDV = 11\n\ndf_test = test_faulty_complete[(test_faulty_complete.faultNumber==IDV) & \n                               (test_faulty_complete.simulationRun==1)].iloc[:,3:]\n\npca.predict(filter_noise_ma(df_test, W=W))\n\npca.plot_control_charts(fault=160-W)\nplt.suptitle(f'IDV({IDV})');\n\npca.plot_contributions(fault=160-W, columns = filter_noise_ma(df_test, W=W).columns)\n\nprint(f'Taxas de detec\u00e7\u00e3o de falhas - IDV({IDV})\\n--------------')\n\nprint(f'T2: {(pca.T2[160-W:]>pca.T2_lim).sum()\/pca.T2[160-W:].shape[0]}')\nprint(f'Q: {(pca.Q[160-W:]>pca.Q_lim).sum()\/pca.Q[160-W:].shape[0]}')","5dd9c61d":"Para entender como a t\u00e9cnica funciona, usemos um DataFrame simples como exemplo:","322fd8ab":"# Videoaulas\n\nEste notebook \u00e9 explicado em detalhes ao longo das seguintes videoaulas:","a48a59be":"# MSPC com PCA - Aplica\u00e7\u00e3o na Tennesse Eastman\n\nEnfim aplicaremos nosso modelo aos dados da Tennessee!\n\nPara come\u00e7ar, vamos selecionar dados de opera\u00e7\u00e3o normal para efetuar treino e teste:","90b35631":"Analisando a [taxa de falsos alarmes](https:\/\/en.wikipedia.org\/wiki\/False_positive_rate), ou seja, a fra\u00e7\u00e3o dos pontos que caiu acima do limite estat\u00edstico mesmo com a planta em opera\u00e7\u00e3o normal:","cd5e37ce":"O DataFrame de treino foi gerado com 500 simula\u00e7\u00f5es, cada uma delas contendo 500 pontos de amostragem (totalizando 500\\*500 = 250000 pontos no conjunto de dados). As tr\u00eas primeiras colunas identificam os ID's da falha, da simula\u00e7\u00e3o e da amostra, respectivamente. As demais colunas cont\u00eam as medi\u00e7\u00f5es de processo.","1d3c57e0":"Para a playlist do curso completo, clique [aqui](https:\/\/www.youtube.com\/playlist?list=PLvr45Arc0UpzsRhzq3q4_KmZcm0utwvvB).","41c73028":"Os reagentes gasosos $A$, $C$, $D$ e $E$ e o inerte $B$ s\u00e3o alimentados ao reator, onde se formam os produtos $G$ e $H$, no estado l\u00edquido, e o subproduto $F$, por meio de quatro rea\u00e7\u00f5es simult\u00e2neas irrevers\u00edveis e exot\u00e9rmicas:\n\n$A$<sub>(g)<\/sub> + $C$<sub>(g)<\/sub> + $D$<sub>(g)<\/sub> &rarr; $G$<sub>(liq)<\/sub>\n\n$A$<sub>(g)<\/sub> + $C$<sub>(g)<\/sub> + $E$<sub>(g)<\/sub> &rarr; $H$<sub>(liq)<\/sub>\n\n$A$<sub>(g)<\/sub> + $E$<sub>(g)<\/sub> &rarr; $F$<sub>(liq)<\/sub>\n\n$3D$<sub>(g)<\/sub> &rarr; $2F$<sub>(liq)<\/sub>\n\nA corrente de produto passa por um condensador e um separador l\u00edquido-vapor, respectivamente. O vapor resultante \u00e9 reciclado por meio de um compressor, enquanto o condensado passa por um esgotador, da base do qual saem os produtos.\n\nO processo apresenta 41 vari\u00e1veis medidas e 12 vari\u00e1veis manipuladas. Os tempos de amostragem variam entre 3, 6 e 15 minutos. S\u00e3o 20 perturba\u00e7\u00f5es na planta (falhas) pr\u00e9-programadas na simula\u00e7\u00e3o, que variam entre aumento de variabilidade de algumas vari\u00e1veis, agarramento de v\u00e1lvulas e tend\u00eancia na cin\u00e9tica do processo, entre outras.\n\nSegue uma descri\u00e7\u00e3o das vari\u00e1veis e falhas (retirada [daqui](https:\/\/github.com\/camaramm\/tennessee-eastman-profBraatz)):","3a9cfb32":"Inspecionando os DataFrames:","b9d3989c":"## Falha IDV(11)\n\nA falha `IDV(11)` \u00e9 similar \u00e0 `IDV(4) `, pois tamb\u00e9m envolve a temperatura da \u00e1gua de resfriamento do reator. No entanto, a perturba\u00e7\u00e3o aqui n\u00e3o \u00e9 um degrau, como antes, e sim uma varia\u00e7\u00e3o aleat\u00f3ria. Os efeitos no processo s\u00e3o:\n\n* grandes oscila\u00e7\u00f5es na vaz\u00e3o de \u00e1gua de resfriamento do reator;\n* flutua\u00e7\u00e3o da temperatura do reator.\n\nTodas as demais vari\u00e1veis mant\u00eam-se aproximadamente nos valores de opera\u00e7\u00e3o normal do processo.","a4677f2b":"O desempenho da estat\u00edstica $Q$ aumenta ainda mais, superando a taxa dos 90%. Al\u00e9m do mais, a estat\u00edstica $T^2$ tamb\u00e9m melhora bastante :)","ff7f7e23":"Basicamente, a transforma\u00e7\u00e3o do PCA rotacionou o conjunto de dados, alinhando a dire\u00e7\u00e3o de maior variabilidade dos dados com o eixo $x$ do espa\u00e7o cartesiano.\n\nUma grande vantagem de expressar os dados em termos de `T` \u00e9 que, ao contr\u00e1rio das colunas da matriz original `X`, as colunas de `T` s\u00e3o *linearmente independentes*. Ou seja, n\u00e3o h\u00e1 correla\u00e7\u00e3o entre as colunas, o que em tese elimina a redund\u00e2ncia da informa\u00e7\u00e3o: cada coluna cont\u00e9m sua pr\u00f3pria informa\u00e7\u00e3o, independente das demais.\n\nAcabamos de visualizar que, do ponto de vista geom\u00e9trico, o PCA gera os dados transformados `T` projetando os dados originais `X` nas dire\u00e7\u00f5es definidas pelos componentes principais. \u00c9 \u00fatil tamb\u00e9m interpretar essa opera\u00e7\u00e3o do ponto de vista alg\u00e9brico: o PCA gera os dados transformados `T` combinando linearmente as colunas dos dados originais de `X`. Os coeficientes das combina\u00e7\u00f5es lineares s\u00e3o dados pelos componentes principais. No nosso exemplo, os componentes s\u00e3o:","d0d69ba5":"# T\u00e9cnicas de pr\u00e9-processamento\n\nOs dados podem ser pr\u00e9-processados antes de serem entregues ao PCA, o que em alguns casos melhora o desempenho de detec\u00e7\u00e3o do modelo. Nessa se\u00e7\u00e3o, aprenderemos sobre duas t\u00e9cnicas de pr\u00e9-processamento que melhoram bastante o desempenho na falha `IDV(11)`: \n\n* adi\u00e7\u00e3o de vari\u00e1veis atrasadas no tempo (que modifica as colunas);\n* filtro de ru\u00eddo com m\u00e9dia m\u00f3vel (que modifica as linhas).","ea05f3d2":"### PCA - Dedu\u00e7\u00e3o\n\nNessa se\u00e7\u00e3o estudaremos em detalhes a dedu\u00e7\u00e3o matem\u00e1tica do modelo. Muito do material aqui apresentado \u00e9 inspirado no excelente tutorial de [SHLENS (2014)](https:\/\/arxiv.org\/abs\/1404.1100).\n\nO desenvolvimento se dar\u00e1 por meio da enuncia\u00e7\u00e3o de todas as hip\u00f3teses referentes ao modelo. A primeira \u00e9:\n\n* **Hip\u00f3tese 1 (linearidade)**: As vari\u00e1veis latentes podem ser obtidas por meio da aplica\u00e7\u00e3o de uma transforma\u00e7\u00e3o linear no conjunto de dados. \n\nSeja um conjunto de dados $\\mathbf{X} \\in \\mathbb{R}^{n\\times m}$, em que cada linha corresponde a uma observa\u00e7\u00e3o e cada coluna corresponde a uma vari\u00e1vel medida. A transforma\u00e7\u00e3o linear efetuada pelo PCA pode ser representada por meio de uma mudan\u00e7a de base:\n\n$$\n\\mathbf{T} = \\mathbf{XP},\n\\tag{1} \\label{eq:pca_proj}\n$$\n\nem que as colunas de $\\mathbf{P} \\in \\mathbb{R}^{m\\times m}$, \\{$\\mathbf{p_1}$, ..., $\\mathbf{p_m}$\\}, denominadas de *componentes principais*, representam uma nova base para expressar cada observa\u00e7\u00e3o (linha) da matriz $\\mathbf{X}$. As colunas de $\\mathbf{T} \\in \\mathbb{R}^{n\\times m}$ s\u00e3o as *vari\u00e1veis latentes* e constituem proje\u00e7\u00f5es das colunas de $\\mathbf{X}$ na base \\{$\\mathbf{p_1}$, ..., $\\mathbf{p_m}$\\}.\n\nPara escolher a nova base $\\mathbf{P}$, \u00e9 preciso avaliar quais caracter\u00edsticas da matriz transformada $\\mathbf{T}$ levam \u00e0 melhor representa\u00e7\u00e3o dos dados. Essa avalia\u00e7\u00e3o s\u00f3 \u00e9 poss\u00edvel se s\u00e3o enunciadas mais hip\u00f3teses acerca da natureza dos dados:\n\n* **Hip\u00f3tese 2 (redund\u00e2ncia)**: H\u00e1 um grau de depend\u00eancia linear entre as vari\u00e1veis, o que atrapalha a descri\u00e7\u00e3o da ess\u00eancia dos dados por conta da presen\u00e7a de informa\u00e7\u00e3o redundante e desnecess\u00e1ria.\n\n* **Hip\u00f3tese 3 (alta raz\u00e3o sinal-ru\u00eddo)**: Os dados possuem alta raz\u00e3o sinal\/ru\u00eddo, o que significa que as vari\u00e2ncias maiores apresentam maior import\u00e2ncia relativa da informa\u00e7\u00e3o.\n\nA Hip\u00f3tese 2 indica que \u00e9 desej\u00e1vel que as colunas de $\\mathbf{T}$ sejam linearmente independentes. Em outras palavras, a matriz de covari\u00e2ncias $\\mathbf{C_T}$:\n\n$$\n\\mathbf{C_T} =\\frac{ (\\mathbf{T}- \\mathbf{\\overline{T}})^{'} (\\mathbf{T}- \\mathbf{\\overline{T}})}{n-1},\n\\tag{2} \\label{eq:cov_comp}\n$$\n\ndeve conter apenas termos diagonais. Na Equa\u00e7\u00e3o \\ref{eq:cov_comp}, $\\mathbf{\\overline{T}}$ \u00e9 o vetor de m\u00e9dias das colunas de $\\mathbf{T}$. Antes de prosseguir, \u00e9 necess\u00e1rio para o desenvolvimento matem\u00e1tico enunciar mais uma hip\u00f3tese:\n\n* **Hip\u00f3tese 4 (conjunto de dados centralizado)**: Cada vari\u00e1vel da matriz de dados apresenta m\u00e9dia nula: $\\mathbf{\\overline{X}} = \\mathbf{\\overline{T}} = 0$.\n\nNa pr\u00e1tica, for\u00e7a-se a validade da hip\u00f3tese 4 por meio de um processo conhecido como *centraliza\u00e7\u00e3o*, em que se subtrai a m\u00e9dia de cada vari\u00e1vel.\n\nNesse caso, a matriz de covari\u00e2ncias de $\\mathbf{T}$ toma a forma:\n\n$$ \n\\mathbf{C_T} = \\frac{\\mathbf{T}^{'}\\mathbf{T}}{n-1}.\n\\label{eq:cov_simp} \\tag{3}\n$$\n\nFazendo uso da Equa\u00e7\u00e3o \\ref{eq:pca_proj}, a Equa\u00e7\u00e3o \\ref{eq:cov_simp} \u00e9 reescrita em termos da matriz de componentes principais $\\mathbf{P}$:\n\n$$\n\\mathbf{C_T} = \\mathbf{P}^{'} \\mathbf{C_X}\\mathbf{P}.\n\\label{eq:cov_comp2}\\tag{4}\n$$\n\nDe acordo com o [Teorema Espectral](https:\/\/en.wikipedia.org\/wiki\/Spectral_theorem), a matriz $\\mathbf{C_X}$, por ser sim\u00e9trica, pode ser [decomposta em autovalores e autovetores](https:\/\/en.wikipedia.org\/wiki\/Eigendecomposition_of_a_matrix), na forma:\n\n$$\n\\mathbf{C_X} = \\mathbf{E}\\mathbf{\\Lambda} \\mathbf{E}^{'},\n\\label{eq:cx_auto}\\tag{5}\n$$\n\nem que $\\mathbf{E}$ \u00e9 a matriz que cont\u00eam nas colunas os autovetores de $\\mathbf{C_X}$ e $\\mathbf{\\Lambda}$ \u00e9 uma matriz diagonal que cont\u00e9m os autovalores de $\\mathbf{C_X}$ em ordem decrescente de magnitude.\n\nPara tornar a matriz $\\mathbf{C_T}$ diagonal, uma escolha conveniente para a nova base \u00e9 fazer $\\mathbf{P} = \\mathbf{E}$, j\u00e1 que, nesse caso, substituindo a Equa\u00e7\u00e3o \\ref{eq:cx_auto} na Equa\u00e7\u00e3o \\ref{eq:cov_comp2}, chega-se a:\n\n$$\n\\mathbf{C_T} = \\mathbf{\\Lambda}.\n\\label{eq:final_pca}\\tag{6}\n$$\n\nSe v\u00e1lidas as Hip\u00f3teses 2 e 3, essa escolha faz com que a matriz transformada $\\mathbf{T}$ esteja em uma forma muito conveniente do ponto de vista da an\u00e1lise de dados, j\u00e1 que:\n\n* a redund\u00e2ncia causada pela depend\u00eancia linear entre as vari\u00e1veis \u00e9 eliminada;\n* as vari\u00e1veis est\u00e3o organizadas em ordem decrescente de import\u00e2ncia, quantificada pelos valores de vari\u00e2ncia presentes na matriz diagonal $\\mathbf{C_T}$.\n\nPara fins de redu\u00e7\u00e3o de dimensionalidade, \u00e9 selecionada uma quantidade $a$ de componentes principais, dando origem \u00e0 matriz reduzida $\\mathbf{P_a} \\in \\mathbb{R}^{m\\times a}$. A matriz de vari\u00e1veis latentes reduzida $\\mathbf{T_a} \\in \\mathbb{R}^{n\\times a}$ \u00e9 obtida por:\n\n$$\n\\mathbf{T_a} = \\mathbf{XP_a}.\n\\tag{7}\n$$\n\nDe acordo com a Hip\u00f3tese 3, as vari\u00e1veis mais relevantes s\u00e3o aquelas que mais variam. Portanto, a matriz $\\mathbf{T_a}$ seria uma representa\u00e7\u00e3o mais eficiente e adequada dos dados, j\u00e1 que nela n\u00e3o est\u00e3o presentes as vari\u00e1veis associadas \u00e0s menores vari\u00e2ncias (correspondentes, segundo a hip\u00f3tese, aos ru\u00eddos de medi\u00e7\u00e3o). Essa matriz pode ser projetada nas vari\u00e1veis originais, gerando a matriz reconstru\u00edda $\\mathbf{\\hat{X}}$:\n\n$$\n\\mathbf{\\hat{X}} = \\mathbf{T_aP_a^{'}} =  \\mathbf{X}(\\mathbf{P_aP_a^{'}}).\n\\tag{8}\n$$\n\nDessa maneira, recupera-se na matriz $\\mathbf{\\hat{X}}$ as vari\u00e1veis originais, mas contendo apenas a varia\u00e7\u00e3o dita principal do conjunto de dados, correspondente aos $a$ primeiros componentes principais. Nesse sentido, pode-se encarar o PCA como um filtro, j\u00e1 que a transforma\u00e7\u00e3o de $\\mathbf{X}$ para $\\mathbf{\\hat{X}}$, em tese, reduz o n\u00edvel de ru\u00eddo do conjunto de dados. \n\nA diferen\u00e7a entre $\\mathbf{X}$ e $\\mathbf{\\hat{X}}$ \u00e9 a *matriz residual* $\\mathbf{E}$:\n\n$$\n\\mathbf{E} = \\mathbf{X} - \\mathbf{\\hat{X}} = \\mathbf{X}(\\mathbf{I} - \\mathbf{P_aP_a^{'}}).\n\\tag{9}\n$$\n\nEnquanto a  matriz $\\mathbf{\\hat{X}}$ cont\u00e9m a variabilidade capturada pelo modelo PCA, a matriz $\\mathbf{E}$ cont\u00e9m a variabilidade *n\u00e3o* capturada pelo modelo. Os subespa\u00e7os gerados por $\\mathbf{\\hat{X}}$ e $\\mathbf{E}$ s\u00e3o conhecidos por *espa\u00e7o principal* e *espa\u00e7o residual*, respectivamente. \n\nEm resumo:\n\n* o PCA aplica uma transforma\u00e7\u00e3o linear no conjunto de dados $\\mathbf{X}$, gerando o conjunto de dados nas vari\u00e1veis latentes $\\mathbf{T}$;\n* essa transforma\u00e7\u00e3o linear \u00e9 uma proje\u00e7\u00e3o dada pela equa\u00e7\u00e3o $\\mathbf{T} = \\mathbf{XP}$;\n* as colunas de $\\mathbf{P}$, ou seja, as dire\u00e7\u00f5es em que os dados s\u00e3o projetados, s\u00e3o chamadas de componentes principais;\n* os componentes principais s\u00e3o os autovetores da matriz de covari\u00e2ncias dos dados, $\\mathbf{C_X}$;\n* as vari\u00e2ncias explicadas por cada componente principal s\u00e3o os autovalores de $\\mathbf{C_X}$;\n* as vantagens de se usar a matriz $\\mathbf{T}$ para representar os dados (ao inv\u00e9s da matriz original $\\mathbf{X}$) \u00e9 que as colunas de $\\mathbf{T}$ s\u00e3o ortogonais (ou seja, n\u00e3o h\u00e1 redund\u00e2ncia entre as vari\u00e1veis) e organizadas em ordem decrescente de vari\u00e2ncia (e em ordem decrescente de import\u00e2ncia, se \u00e9 v\u00e1lida a hip\u00f3tese de que vari\u00e2ncias maiores implicam em import\u00e2ncias maiores);\n* pode-se usar apenas uma por\u00e7\u00e3o $a$ de componentes na matriz $\\mathbf{T}$, resultando na matriz $\\mathbf{T_a}$ e  reduzindo-se a dimensionalidade dos dados;\n* pode-se projetar a matriz $\\mathbf{T_a}$ de volta no espa\u00e7o original, resultando na matriz reconstru\u00edda $\\mathbf{\\hat{X}}$  e retornando-se \u00e0 dimensionalidade original;\n* do ponto de vista da \u00c1lgebra Linear, a matriz $\\mathbf{\\hat{X}}$ gera o subespa\u00e7o principal, em que se encontram as varia\u00e7\u00f5es capturadas pelo modelo, enquanto a matriz $\\mathbf{E} =  \\mathbf{X} - \\mathbf{\\hat{X}}$ gera o subespa\u00e7o residual, em que se encontram as varia\u00e7\u00f5es n\u00e3o capturadas pelo modelo.\n\nNa pr\u00f3xima se\u00e7\u00e3o, entenderemos como esses procedimentos podem ser usados para construir metodologias de MSPC.","34ee5641":"A t\u00edtulo de ilustra\u00e7\u00e3o, analisemos o efeito de filtros com diferentes tamanhos de janela na vari\u00e1vel `XMV(10)` - a vaz\u00e3o da \u00e1gua de resfriamento do reator - do conjunto de testes do cen\u00e1rio `IDV(11)`:","402598f0":"***M\u00e3o na massa 2!***\n\n* Use o m\u00f3dulo [sklearn.datasets](https:\/\/scikit-learn.org\/stable\/modules\/classes.html#module-sklearn.datasets) para carregar o [conjunto de dados Iris de Fisher](https:\/\/pt.wikipedia.org\/wiki\/Conjunto_de_dados_flor_Iris). Pesquise sobre o conjunto (sua origem, o problema que se prop\u00f5e a ilustrar, etc.). Qual sua dimensionalidade original? Aplique o modelo PCA aos dados. Verifique a vari\u00e2ncia explicada por cada componente principal. Plote em gr\u00e1fico cartesiano as duas primeiras dimens\u00f5es dos dados transformados (ou seja, as proje\u00e7\u00f5es dos dados originais nos dois primeiros componentes principais). \n\nQue conclus\u00f5es esse procedimento pode sugerir sobre a natureza dos dados?","69573bb6":"Perceba como o filtro \u00e9 efetivo para real\u00e7ar a din\u00e2mica das oscila\u00e7\u00f5es!\n\nAplicando ao processo e gerando as cartas de controle:","b6310eef":"A opera\u00e7\u00e3o de reprojetar os dados no espa\u00e7o original \u00e9 conhecida como *reconstru\u00e7\u00e3o*.\n\nPlotando, para compara\u00e7\u00e3o, o conjunto original e o reconstru\u00eddo:","cb6a20ae":"### Importando dados\n\nBoa parte dos estudos com a Tennessee Eastman utilizam um [pequeno conjunto de dados](https:\/\/github.com\/camaramm\/tennessee-eastman-profBraatz) j\u00e1 bem consolidado na literatura.\n\nOs dados utilizados nesta aula, no entanto, referem-se \u00e0 uma extens\u00e3o publicada por [RIETH *et al*. (2017)](https:\/\/dataverse.harvard.edu\/dataset.xhtml?persistentId=doi:10.7910\/DVN\/6C3JR1), que rodaram v\u00e1rias simula\u00e7\u00f5es e aumentaram consideravelmente o tamanho do conjunto, tornando-o mais adequado para aplica\u00e7\u00f5es *big data*. \n\nOs dados encontram-se no formato RData, da [linguagem R](https:\/\/pt.wikipedia.org\/wiki\/R_(linguagem_de_programa%C3%A7%C3%A3o)). Para l\u00ea-los na linguagem Python, \u00e9 necess\u00e1rio utilizar um m\u00f3dulo especial, como o [pyreadr](https:\/\/github.com\/ofajardo\/pyreadr):","919b6bbe":"No gr\u00e1fico acima, as barras s\u00e3o as vari\u00e2ncias explicadas por cada componente principal e a linha cont\u00ednua \u00e9 a vari\u00e2ncia acumulada. \n\nNote como o resultado \u00e9 interessante! Ele evidencia que apenas algumas combina\u00e7\u00f5es lineares das vari\u00e1veis originais (ou seja, apenas alguns componentes principais) s\u00e3o necess\u00e1rias para descrever a maior parte da variabilidade dos dados. Na verdade, isso \u00e9 muito comum em dados provenientes de plantas de processo, que em geral resultam de medi\u00e7\u00f5es altamente correlacionadas, apresentando muitas vari\u00e1veis colineares (muitas vezes por conta da a\u00e7\u00e3o de controladores). Por isso o modelo PCA \u00e9 t\u00e3o \u00fatil no nosso dom\u00ednio.\n\nEm particular, para capturar 90% da variabilidade dos dados, o n\u00famero $a$ de componentes \u00e9:","1fc4b2a0":"### Perturba\u00e7\u00f5es de processo\n\nVariable | Description\n-------- | -----------\n`IDV(1)`  | A\/C Feed Ratio, B Composition Constant (Stream 4)          Step\n`IDV(2)`  | B Composition, A\/C Ratio Constant (Stream 4)               Step\n`IDV(3)`  | D Feed Temperature (Stream 2)                              Step\n`IDV(4)`  | Reactor Cooling Water Inlet Temperature                    Step\n`IDV(5)`  | Condenser Cooling Water Inlet Temperature                  Step\n`IDV(6)`  | A Feed Loss (Stream 1)                                     Step\n`IDV(7)`  | C Header Pressure Loss - Reduced Availability (Stream 4)   Step\n`IDV(8)`  | A, B, C Feed Composition (Stream 4)            Random Variation\n`IDV(9)`  | D Feed Temperature (Stream 2)                  Random Variation\n`IDV(10)` | C Feed Temperature (Stream 4)                  Random Variation\n`IDV(11)` | Reactor Cooling Water Inlet Temperature        Random Variation\n`IDV(12)` | Condenser Cooling Water Inlet Temperature      Random Variation\n`IDV(13)` | Reaction Kinetics                                    Slow Drift\n`IDV(14)` | Reactor Cooling Water Valve                            Sticking\n`IDV(15)` | Condenser Cooling Water Valve                          Sticking\n`IDV(16)` | Unknown\n`IDV(17)` | Unknown\n`IDV(18)` | Unknown\n`IDV(19)` | Unknown\n`IDV(20)` | Unknown\n\n\u00c9 importante ressaltar que o processo opera sob malha fechada, ou seja, com a atua\u00e7\u00e3o de controladores autom\u00e1ticos. Nesse sentido, uma *falha* pode ser definida como uma perturba\u00e7\u00e3o no processo que a malha de controle n\u00e3o consegue gerenciar completamente.","4f3bad51":"Na figura acima, \u00e9 poss\u00edvel visualizar o que t\u00ednhamos observado numericamente com o atributo `explained_variance_`: o primeiro componente principal explica uma parcela muito maior da vari\u00e2ncia que o segundo.\n\nNa verdade, o primeiro componente principal identifica a *dire\u00e7\u00e3o de maior variabilidade dos dados*. O segundo componente principal, sendo ortogonal ao primeiro, abrange uma parcela muito menor da variabilidade.\n\nAt\u00e9 agora utilizamos o m\u00e9todo `fit`, que  apenas calcula os componentes principais. Para projetar os dados nos componentes principais e obter a matriz de dados transformada `T`, deve-se usar a fun\u00e7\u00e3o `transform`:","8253d0ce":"Plotando o conjunto `T`:","c9d2ff73":"Ambas as estat\u00edsticas mostram-se sens\u00edveis \u00e0 falha, por\u00e9m com relativa baixa taxa de detec\u00e7\u00e3o.\n\nNa pr\u00f3xima se\u00e7\u00e3o, estudaremos como t\u00e9cnicas de pr\u00e9-processamento dos dados podem ajudar a melhorar o desempenho nessa falha em espec\u00edfico.","b9d86985":"# An\u00e1lise de Componentes Principais (PCA)\n\nO modelo [PCA](https:\/\/en.wikipedia.org\/wiki\/Principal_component_analysis) faz parte de um grupo de t\u00e9cnicas baseadas em [modelagem em vari\u00e1veis latentes](https:\/\/en.wikipedia.org\/wiki\/Latent_variable), ou seja, que efetuam transforma\u00e7\u00f5es nos dados de modo a express\u00e1-los como um novo conjunto de vari\u00e1veis que explicitam caracter\u00edsticas ocultas (latentes) dos dados originais. A depender de que caracter\u00edsticas se deseja explicitar, v\u00e1rias t\u00e9cnicas podem ser propostas. \n\nEm particular, o modelo PCA tem como objetivo *gerar um conjunto ortogonal de combina\u00e7\u00f5es lineares das vari\u00e1veis originais, com o objetivo de selecionar um subconjunto dessas combina\u00e7\u00f5es que sumariza apropriadamente a variabilidade dos dados*.\n\nVamos definir um conjunto de dados bidimensional simples para ver o modelo em a\u00e7\u00e3o:","05220a31":"A falha que usaremos abaixo, a `IDV(1)`, resulta de uma perturba\u00e7\u00e3o degrau na raz\u00e3o de alimenta\u00e7\u00e3o $A\/C$ na corrente 4. Isso faz com que a alimenta\u00e7\u00e3o de $A$ na corrente de reciclo 5 caia. A malha de controle autom\u00e1tico reage, de modo a aumentar a alimenta\u00e7\u00e3o de $A$ na corrente 1. Esses dois efeitos se contrabalan\u00e7am e, com o tempo, a composi\u00e7\u00e3o de $A$ na corrente 6 se estabiliza.\n\nUsando a refer\u00eancia e os limites apresentados acima para plotar as cartas relativas ao teste (a barra vertical marca o instante a partir do qual o processo opera em falha):","2250690b":"# Um segundo prel\u00fadio: MSPC com $T^2$ de Hotelling\n\nA maneira mais direta de formular uma t\u00e9cnica de MSPC \u00e9 assumir que os dados seguem uma [distribui\u00e7\u00e3o gaussiana](https:\/\/en.wikipedia.org\/wiki\/Multivariate_normal_distribution) (ou normal):\n\n$$ p(\\mathbf{x}) = \\frac{\\exp\\left(-\\frac 1 2 ({\\mathbf x}-{\\boldsymbol\\mu})^\\mathrm{T}{\\boldsymbol\\Sigma}^{-1}({\\mathbf x}-{\\boldsymbol\\mu})\\right)}{\\sqrt{(2\\pi)^m|\\boldsymbol\\Sigma|}}.$$\n\nNa equa\u00e7\u00e3o acima:\n\n* $\\mathbf{x} \\in \\mathbb{R}^m$ \u00e9 uma observa\u00e7\u00e3o qualquer (uma linha do conjunto de dados);\n* $\\boldsymbol\\mu$ \u00e9 o vetor de m\u00e9dias de cada uma das $m$ vari\u00e1veis;\n* $\\boldsymbol\\Sigma$ \u00e9 a [matriz de covari\u00e2ncias](https:\/\/en.wikipedia.org\/wiki\/Covariance_matrix) dos dados;\n* $p(\\mathbf{x})$ \u00e9 a probabilidade de ocorr\u00eancia da observa\u00e7\u00e3o $\\mathbf{x}$. \n\nAssumir que os dados seguem uma distribui\u00e7\u00e3o gaussiana \u00e9 afirmar que a probabilidade de cada observa\u00e7\u00e3o ocorrer \u00e9 conhecida e dada pela equa\u00e7\u00e3o acima. \u00c9 uma hip\u00f3tese forte e que muitas vezes n\u00e3o se verifica na pr\u00e1tica.\n\nA t\u00edtulo de ilustra\u00e7\u00e3o, a pr\u00f3xima c\u00e9lula plota um exemplo de distribui\u00e7\u00e3o gaussiana bivariada:","76fd9389":"Plotando as cartas de controle:","3ebceeee":"O PCA gera os dados transformados projetando os dados originais em dire\u00e7\u00f5es especiais chamadas de *componentes principais*. Cada uma dessas dire\u00e7\u00f5es, que s\u00e3o ortogonais entre si, explica uma parcela da vari\u00e2ncia dos dados.\n\nNo `scikit-learn`, os componentes principais ficam armazenados no atributo `components_` e as vari\u00e2ncias explicadas, no atributo `explained_variance_`:","34ccff38":"### PCA - Redu\u00e7\u00e3o de dimensionalidade\n\nNo exemplo acima, aplicamos o PCA para gerar dois componentes principais, especificando o hiperpar\u00e2metro `n_components = 2` na etapa de inicializa\u00e7\u00e3o do modelo. \u00c9 o n\u00famero m\u00e1ximo de componentes que podem ser gerados nesse caso, pois o n\u00famero de vari\u00e1veis no conjunto original \u00e9 2.\n\nMas, j\u00e1 que o primeiro componente captura quase toda a variabilidade dos dados, que tal usar apenas ele e descartar o segundo? Esse procedimento, em que os dados transformados apresentam dimens\u00e3o menor que a dos dados originais, \u00e9 chamado de [redu\u00e7\u00e3o de dimensionalidade](https:\/\/en.wikipedia.org\/wiki\/Dimensionality_reduction).\n\nPor exemplo, aplicando o PCA para gerar apenas 1 componente principal:","72fb8be7":"Nota-se que o primeiro componente principal explica uma parcela muito maior da vari\u00e2ncia que o segundo.\n\nPara visualizar os componentes principais, vamos definir a seguinte fun\u00e7\u00e3o, que plota vetores em um gr\u00e1fico:","1c1afbfb":"***M\u00e3o na massa 1!***\n\n* Plote cartas de controle univariadas para mais alguns cen\u00e1rios de falha do benchmark Tennessee Eastman. Observe os comportamentos das vari\u00e1veis em cada caso, consultando o fluxograma e as informa\u00e7\u00f5es do processo para melhor compreens\u00e3o.","afa07f9e":"Visualizando:","0da573cc":"Plotando os dados transformados (vari\u00e1veis latentes):","84c30031":"Uau! A taxa de detec\u00e7\u00e3o da estat\u00edstica $Q$ aumentou consideravelmente. Isso significa que, durante a falha `IDV(11)`, *novos padr\u00f5es de correla\u00e7\u00e3o din\u00e2mica foram introduzidos no processo*. O que faz todo sentido, j\u00e1 que a falha `IDV(11)`, em particular, resulta em oscila\u00e7\u00f5es n\u00e3o presentes no per\u00edodo de opera\u00e7\u00e3o normal.\n\nObs: o uso do PCA com vari\u00e1veis atrasadas no tempo \u00e9 conhecido como DPCA (Dynamic PCA) e foi proposto por [KU *et al.* (1995)](https:\/\/www.sciencedirect.com\/science\/article\/abs\/pii\/0169743995000763).","6decf397":"A falha `IDV(1)` \u00e9 bem clara, sendo capturada por ambas as estat\u00edsticas com taxas de detec\u00e7\u00e3o pr\u00f3ximas a 100%.","0f80aeec":"Ent\u00e3o, se $\\mathbf{x}_1$ e $\\mathbf{x}_2$ s\u00e3o as colunas da matriz $\\mathbf{X} = [\\mathbf{x}_1, \\mathbf{x}_2]$, as colunas da matriz transformada $\\mathbf{T} = [\\mathbf{t}_1, \\mathbf{t}_2]$ s\u00e3o:\n\n$$\n\\mathbf{t}_1 = -0.944\\mathbf{x}_1 - 0.329\\mathbf{x}_2\n$$\n$$\n\\mathbf{t}_2 = -0.329\\mathbf{x}_1 + 0.944\\mathbf{x}_2\n$$\n\nEssa \u00e9 uma das belezas da \u00c1lgebra Linear: os conceitos podem ser interpretados tanto do ponto de vista geom\u00e9trico quanto do ponto de vista alg\u00e9brico.","bbf20ae5":"Inicializando o modelo e treinando:","a2cf3ad0":"Perceba a vantagem dessa abordagem em rela\u00e7\u00e3o \u00e0 estrat\u00e9gia de monitoramento univariada: temos agora apenas duas cartas de controle capazes de sumarizar os comportamentos de todas as 52 vari\u00e1veis!! Al\u00e9m do mais, o mapa de contribui\u00e7\u00f5es nos fornece a informa\u00e7\u00e3o de quais vari\u00e1veis est\u00e3o associadas ao comportamento anormal, e em que propor\u00e7\u00f5es (de acordo com o \u00edndice $Q$, neste exemplo).\n\nAnalisando a taxa de detec\u00e7\u00e3o de falhas, ou seja, a fra\u00e7\u00e3o dos pontos que caiu acima do limite estat\u00edstico no per\u00edodo de opera\u00e7\u00e3o problem\u00e1tica da planta (a partir do ponto 160):","dab0087d":"### PCA - Reconstru\u00e7\u00e3o\n\nOs dados transformados `T` podem ser reprojetados no espa\u00e7o original com o uso da fun\u00e7\u00e3o `inverse_transform`:","123c79dc":"Agora os dados est\u00e3o representados em apenas uma dimens\u00e3o! Toda a variabilidade da outra dimens\u00e3o foi perdida, mas como ela era uma parcela pequena da variabilidade total, a perda n\u00e3o foi muito significativa. \n\nAl\u00e9m do mais, em muitas aplica\u00e7\u00f5es, *nosso objetivo \u00e9 justamente descartar essa parte da informa\u00e7\u00e3o*. Por exemplo, quando ela representa ru\u00eddo de medi\u00e7\u00e3o! Nesse sentido, o PCA pode ser encarado como uma t\u00e9cnica para *filtrar ru\u00eddo*.\n\nA redu\u00e7\u00e3o de dimensionalidade tamb\u00e9m \u00e9 \u00fatil para fins de visualiza\u00e7\u00e3o. Por exemplo, se um conjunto de dados possui mais que tr\u00eas dimens\u00f5es, n\u00e3o podemos visualiz\u00e1-los no plano ou no espa\u00e7o cartesiano; por\u00e9m, podemos plotar as duas ou tr\u00eas primeiras vari\u00e1veis latentes e visualizar a maior parte da variabilidade dos dados (isso vai ficar claro no M\u00e3o na Massa 2, proposto mais adiante).","0008efef":"# MSPC com PCA\n\nNo MSPC com PCA, a matriz de dados de treino $\\mathbf{X}$ a ser modelada \u00e9 proveniente de uma [s\u00e9rie temporal](https:\/\/en.wikipedia.org\/wiki\/Time_series). Para a aplica\u00e7\u00e3o do modelo PCA em s\u00e9ries temporais, \u00e9 preciso enunciar mais uma hip\u00f3tese:\n\n* **Hip\u00f3tese 5 (estacionariedade)**: A matriz de dados $\\mathbf{X}$ \u00e9 obtida a partir de um processo em estado estacion\u00e1rio.\n\nA atividade de monitoramento de processos com PCA consiste de duas etapas:\n\n1. Na etapa de treino, um modelo PCA \u00e9 gerado para dados de opera\u00e7\u00e3o normal $\\mathbf{X}$. Vimos que, matematicamente, isso significa calcular as matrizes de proje\u00e7\u00e3o $ \\mathbf{P_a}$ e de vari\u00e2ncias explicadas $\\boldsymbol\\Lambda_\\mathbf{a}$. A hip\u00f3tese \u00e9 de que o modelo \u00e9 capaz de explicar a estrutura de variabilidade dos dados no per\u00edodo de opera\u00e7\u00e3o normal.\n\n1. Na etapa de teste, novas observa\u00e7\u00f5es s\u00e3o testadas para verificar-se a conformidade com o modelo. A hip\u00f3tese \u00e9 de que, quando as observa\u00e7\u00f5es n\u00e3o se enquadram com o previsto pelo modelo, o processo saiu do comportamento normal e pode ser classificado como em falha.\n\nComo o PCA separa o espa\u00e7o original em dois subespa\u00e7os, o principal (gerado por $\\mathbf{\\hat{X}}$) e o residual (gerado por $\\mathbf{E}$), muitas vezes \u00e9 \u00fatil monitorar mudan\u00e7as em ambos os espa\u00e7os.\n\nDe modo a monitorar as mudan\u00e7as, para cada observa\u00e7\u00e3o s\u00e3o calculadas [estat\u00edsticas de monitoramento](http:\/\/wiki.eigenvector.com\/index.php?title=T-Squared_Q_residuals_and_Contributions). Quando alguma das estat\u00edsticas ultrapassa um certo *limite de detec\u00e7\u00e3o*, alarmes s\u00e3o acionados. As mais utilizadas s\u00e3o as estat\u00edsticas $T^2$ e $Q$.\n\n### Estat\u00edstica $T^2$\n\nA estat\u00edstica $T^2$ monitora mudan\u00e7as no espa\u00e7o principal e \u00e9 a mesma descrita no come\u00e7o desse notebook, mas, no contexto do PCA, \u00e9 aplicada \u00e0 vari\u00e1vel latente $\\mathbf{t_{a}} = \\mathbf{P^{'}_a} \\mathbf{x} $ (a proje\u00e7\u00e3o da observa\u00e7\u00e3o $\\mathbf{x}$ nos primeiros $a$ componentes principais):\n\n$$ \\label{eq:t2_pca}\nT^2 = \\mathbf{t_{a}'} {\\boldsymbol{\\Lambda_\\mathbf{a}}}^{-1}\\mathbf{t_{a}} = {\\mathbf {x}'}\\mathbf{P_a}{\\boldsymbol{\\Lambda_\\mathbf{a}}}^{-1}\\mathbf{P_a'}. {\\mathbf {x}}\n\\tag{11}\n$$\n\nA estat\u00edstica $T^2$ indica uma medida global da variabilidade capturada pelo modelo PCA nos primeiros $a$ componentes principais, associada \u00e0s varia\u00e7\u00f5es sistem\u00e1ticas do processo ([CHIANG *et al.*, 2001](https:\/\/www.springer.com\/gp\/book\/9781852333270)). Portanto, no monitoramento, o aumento no valor de $T^2$ em novas observa\u00e7\u00f5es sugere a ocorr\u00eancia de dist\u00farbios de mesma natureza que as varia\u00e7\u00f5es identificadas pelo modelo PCA, ou seja, dist\u00farbios nas dire\u00e7\u00f5es do plano descrito pelo modelo ([KUNDU *et al.*, 2017](https:\/\/www.taylorfrancis.com\/books\/9781315155135)). Estabelecer um limite de detec\u00e7\u00e3o utilizando a estat\u00edstica $T^2$ equivale a delimitar uma hiper-elipse como regi\u00e3o de confian\u00e7a no espa\u00e7o de vari\u00e1veis latentes.\n\nUm limite de controle muito usado para a estat\u00edstica $T^2$ segue diretamente da Estat\u00edstica Cl\u00e1ssica:\n\n$$\\label{eq:t2_lim_pca}\nT_{lim}^2 = \\frac{a(n^2-1)}{n(n-a)} F_{\\alpha(a,n-a)},\n$$\n\nsendo $F_{\\alpha(a,n-a)}$ o valor cr\u00edtico do percentil $\\alpha$ da distribui\u00e7\u00e3o [F de Fisher-Snedecor](https:\/\/en.wikipedia.org\/wiki\/F-distribution) com graus de liberdade $a$ e $n-a$.\n\nComo a estat\u00edstica $T^2$ est\u00e1 associada \u00e0 distribui\u00e7\u00e3o gaussiana, mais uma hip\u00f3tese acerca da natureza dos dados \u00e9 necess\u00e1ria:\n\n* **Hip\u00f3tese 6 (normalidade)**: O conjunto de dados $\\mathbf{X}$ segue uma distribui\u00e7\u00e3o normal.\n\nEm alguns casos, a estat\u00edstica $T^2$ pode ser usada mesmo em situa\u00e7\u00f5es que violem a hip\u00f3tese 6, por conta de as vari\u00e1veis latentes serem combina\u00e7\u00f5es lineares das vari\u00e1veis originais; a hip\u00f3tese, no caso, \u00e9 de que sigam aproximadamente uma distribui\u00e7\u00e3o normal devido ao [Teorema do Limite Central](https:\/\/en.wikipedia.org\/wiki\/Central_limit_theorem), que estabelece que a soma de vari\u00e1veis aleat\u00f3rias se aproxima assintoticamente da representa\u00e7\u00e3o normal, \u00e0 medida que o n\u00famero de vari\u00e1veis aumenta.\n\n### Estat\u00edstica $Q$\n\nA estat\u00edstica $Q$ monitora mudan\u00e7as no espa\u00e7o residual e \u00e9 definida como a dist\u00e2ncia euclidiana da observa\u00e7\u00e3o $\\mathbf{x}$ \u00e0 respectiva proje\u00e7\u00e3o no espa\u00e7o principal, $\\mathbf{\\hat{x}} = (\\mathbf{P_aP_a'}) \\mathbf{x}$:\n\n$$ \\label{eq:q_pca}\nQ = ||\\mathbf{x} -\\mathbf{\\hat{x}}||^2 =  ||(\\mathbf{I} - \\mathbf{P_aP_a'})\\mathbf{x}||^2 = (\\mathbf{x} -\\mathbf{\\hat{x}})'(\\mathbf{x} -\\mathbf{\\hat{x}}) = \\mathbf{x'}  (\\mathbf{I} - \\mathbf{P_aP_a'})\\mathbf{x}.\n\\tag{10}\n$$\n\nA estat\u00edstica $Q$ consiste do quadrado dos m\u00f3dulos dos erros de predi\u00e7\u00e3o (res\u00edduos) do modelo PCA, sendo por isso tamb\u00e9m conhecida como SPE (*Square Prediction Error*). Essa estat\u00edstica indica uma medida global da variabilidade *n\u00e3o* capturada pelo modelo PCA (variabilidade presente no espa\u00e7o residual), associada \u00e0s varia\u00e7\u00f5es n\u00e3o usuais ou n\u00e3o explicadas do processo ([CHIANG *et al.*, 2001](https:\/\/www.springer.com\/gp\/book\/9781852333270)). Portanto, no monitoramento, o aumento no valor de $Q$ em novas observa\u00e7\u00f5es sugere uma mudan\u00e7a nas rela\u00e7\u00f5es entre as vari\u00e1veis (por exemplo, viola\u00e7\u00e3o nas correla\u00e7\u00f5es), o que resulta na quebra da estrutura capturada pelo modelo durante o treino. Em outras palavras, o plano descrito pelo modelo PCA n\u00e3o inclui a dire\u00e7\u00e3o do dist\u00farbio ([KUNDU *et al.*, 2017](https:\/\/www.taylorfrancis.com\/books\/9781315155135)). Por conta da forma da Equa\u00e7\u00e3o \\ref{eq:q_pca}, estabelecer um limite de detec\u00e7\u00e3o utilizando a estat\u00edstica $Q$ equivale a delimitar uma hiper-esfera como regi\u00e3o de confian\u00e7a no espa\u00e7o residual ([LI e QIN, 2016](https:\/\/www.sciencedirect.com\/science\/article\/pii\/S0959152416301081)).\n\nUm limite de controle muito usado para a estat\u00edstica $Q$ \u00e9 baseado na aproxima\u00e7\u00e3o da distribui\u00e7\u00e3o de formas quadr\u00e1ticas ([JACKSON e MUDHOLKAR, 1979](https:\/\/www.jstor.org\/stable\/1267757)):\n\n$$ \\label{eq:q_lim_pca}\nQ_{lim} = \\theta_1 \\left[\\frac{h_0c_{\\alpha}\\sqrt{2\\theta_2}}{\\theta_1}+1+\\frac{\\theta_2 h_0 (h_0-1)}{\\theta_1^2}\\right]^{1\/h_0},\n\\tag{11}\n$$\n\nsendo:\n\n* $\\theta_i = \\sum_{j=a+1}^m \\lambda_j^i$; \n* $h_0 = 1-\\left(\\displaystyle\\frac{2\\theta_1\\theta_3}{3\\theta_2^2}\\right)$;\n* $c_{\\alpha}$ \u00e9 o valor cr\u00edtico do percentil $1-\\alpha$ da distribui\u00e7\u00e3o gaussiana.\n\n### Contribui\u00e7\u00f5es das vari\u00e1veis para as falhas\n\nComo vimos, os \u00edndices de detec\u00e7\u00e3o $Q$ e $T^2$ s\u00e3o indicativos da sa\u00fade do processo; enquanto eles permanecem em um patamar baixo (abaixo dos limites de controle correspondentes), provavelmente o processo se encontra saud\u00e1vel (ou seja, em opera\u00e7\u00e3o normal, sem falhas). Quando os \u00edndices aumentam, ultrapassando o limite de controle (crescendo indefinidamente, oscilando ou atingindo oto patamar), provavelmente o processo se encontra em falha.\n\n\u00c9 poss\u00edvel decompor os \u00edndices de detec\u00e7\u00e3o, de modo a verificar as contribui\u00e7\u00f5es de cada vari\u00e1vel para o aumento dos \u00edndices. Em muitas refer\u00eancias, esse procedimento \u00e9 referido como um m\u00e9todo para [diagn\u00f3stico de falhas](https:\/\/www.sciencedirect.com\/topics\/computer-science\/fault-diagnosis), mas tome cuidado: ele s\u00f3 consegue indicar quais vari\u00e1veis s\u00e3o afetadas por uma falha, n\u00e3o a sua causa. Um verdadeiro m\u00e9todo de diagn\u00f3stico, na minha opini\u00e3o, deve ser capaz de indicar a causa raiz da falha e n\u00e3o apenas seus efeitos.\n\nAqui, a t\u00edtulo de ilustra\u00e7\u00e3o, usaremos a decomposi\u00e7\u00e3o parcial do \u00edndice $Q$:\n\n$$\nc_{j} = ((\\mathbf{x} -\\mathbf{\\hat{x}}) ' \\mathbf{I}_i)(\\mathbf{x} ' \\mathbf{I}_i) = (x_j-\\hat{x}_j)x_j,\n$$\n\nem que:\n\n* $c_{j}$ \u00e9 a contribui\u00e7\u00e3o da vari\u00e1vel $j$ para o c\u00e1lculo do \u00edndice $Q$ referente \u00e0 observa\u00e7\u00e3o $\\mathbf{x}$;\n* $\\mathbf{I}_i$ \u00e9 a $i$-\u00e9sima coluna da matriz identidade.\n\nA base te\u00f3rica para essa equa\u00e7\u00e3o (e v\u00e1rias outras equa\u00e7\u00f5es de contribui\u00e7\u00f5es) pode ser conferida no excelente trabalho de [ALCALA e QIN (2011)](https:\/\/www.sciencedirect.com\/science\/article\/pii\/S0959152410001915?via%3Dihub).\n\n### Implementa\u00e7\u00e3o\n\nA classe PCA dispon\u00edvel no `scikit-learn` n\u00e3o \u00e9 projetada para aplica\u00e7\u00f5es de monitoramento (n\u00e3o h\u00e1 o c\u00e1lculo das estat\u00edsticas, limites de detec\u00e7\u00e3o, contribui\u00e7\u00f5es, etc). Por conta disso, \u00e9 \u00fatil definirmos a nossa pr\u00f3pria classe de PCA:","3f08d730":"No `scikit-learn `, o PCA est\u00e1 dispon\u00edvel no m\u00f3dulo [sklearn.decomposition](https:\/\/scikit-learn.org\/stable\/modules\/classes.html#module-sklearn.decomposition). \n\nInicializando o modelo e aplicando-o ao conjunto `X` mostrado acima:","8d029114":"# Considera\u00e7\u00f5es finais\n\nNesta aula, aprendemos como funciona o modelo de aprendizado n\u00e3o-supervisionado PCA e o aplicamos ao relevante problema de detec\u00e7\u00e3o de falhas. Aprendemos a gerar dois tipos de cartas de controle multivariadas, $Q$ e $T^2$, a plotar mapas de contribui\u00e7\u00f5es que indicam as vari\u00e1veis mais relacionadas \u00e0s falhas e a aplicar algumas t\u00e9cnicas de pr\u00e9-processamento que podem melhorar os resultados em alguns casos.\n\nO PCA \u00e9 o modelo de aprendizado mais utilizado para detec\u00e7\u00e3o de falhas em processos qu\u00edmicos. Como h\u00e1 v\u00e1rias hip\u00f3teses restritivas em sua formula\u00e7\u00e3o, que impedem a aplica\u00e7\u00e3o do modelo na forma padr\u00e3o em muitos casos, v\u00e1rias extens\u00f5es e melhorias v\u00eam sendo desenvolvidas desde a d\u00e9cada de 1990. Vale a pena destacar alguns cen\u00e1rios para os quais adapta\u00e7\u00f5es foram propostas:\n\n* processos em batelada ([RENDALL *et al.*, 2019](https:\/\/www.sciencedirect.com\/science\/article\/abs\/pii\/S0098135418311104?via%3Dihub));\n* processos din\u00e2micos ([DONG e QIN, 2018](https:\/\/linkinghub.elsevier.com\/retrieve\/pii\/S0098135417303848));\n* processos fortemente n\u00e3o-lineares ([CHOI *et al., 2005*](https:\/\/www.sciencedirect.com\/science\/article\/abs\/pii\/S0169743904001224?via%3Dihub)): \n* processos multimodais ([FEITAL *et al.*, 2013](https:\/\/aiche.onlinelibrary.wiley.com\/doi\/abs\/10.1002\/aic.13953));\n* processos operando em m\u00faltiplas escalas de tempo ([BAKSHI, 1998](https:\/\/aiche.onlinelibrary.wiley.com\/doi\/abs\/10.1002\/aic.690440712));\n* processos em m\u00faltiplas unidades e\/ou est\u00e1gios([JIANG *et al.*, 2019](https:\/\/pubs.acs.org\/doi\/10.1021\/acs.iecr.9b02391)):  \n* modelagem rigorosa dos erros de medi\u00e7\u00e3o ([FEITAL *et al.*, 2010](http:\/\/www.sciencedirect.com\/science\/article\/pii\/S0169743910001607));\n* robustez quanto \u00e0 qualidade dos dados ([JINLIN *et al.*, 2018](https:\/\/linkinghub.elsevier.com\/retrieve\/pii\/S1367578818301056)).\n\nA literatura da \u00e1rea \u00e9 riqu\u00edssima e at\u00e9 hoje muitos trabalhos s\u00e3o publicados sobre o assunto.\n\nEnt\u00e3o \u00e9 isso, galera. Espero que a aula tenha sido proveitosa. At\u00e9 mais!!!!","b6deebee":"### Vari\u00e1veis manipuladas\n\nVariable | Description\n-------- | -----------\n`XMV(1)`  | D Feed Flow (stream 2)            (Corrected Order)\n`XMV(2)`  | E Feed Flow (stream 3)            (Corrected Order)\n`XMV(3)`  | A Feed Flow (stream 1)            (Corrected Order)\n`XMV(4)`  | A and C Feed Flow (stream 4)\n`XMV(5)`  | Compressor Recycle Valve\n`XMV(6)`  | Purge Valve (stream 9)\n`XMV(7)`  | Separator Pot Liquid Flow (stream 10)\n`XMV(8)`  | Stripper Liquid Product Flow (stream 11)\n`XMV(9)`  | Stripper Steam Valve\n`XMV(10)` | Reactor Cooling Water Flow\n`XMV(11)` | Condenser Cooling Water Flow\n`XMV(12)` | Agitator Speed","966ba05e":"# Um prel\u00fadio - Controle Estat\u00edstico de Processos (SPC)\n\nO objetivo principal desta aula \u00e9 demonstrar a aplica\u00e7\u00e3o de t\u00e9cnicas de aprendizado n\u00e3o-supervisionado, em particular o modelo [PCA (Principal Component Analysis)](https:\/\/en.wikipedia.org\/wiki\/Principal_component_analysis), no contexto do [Controle Estat\u00edstico Multivariado de Processos](https:\/\/www.amazon.com\/Multivariate-Statistical-Process-Control-Applications\/dp\/1447145127) (MSPC, da sigla em ingl\u00eas) para detec\u00e7\u00e3o e diagn\u00f3stico de falhas em processos industriais.\n\nPor\u00e9m, \u00e9 \u00fatil analisarmos preliminarmente um m\u00e9todo mais simples para detec\u00e7\u00e3o de falhas, o [Controle Estat\u00edstico de Processos](https:\/\/en.wikipedia.org\/wiki\/Statistical_process_control) (SPC, da sigla em ingl\u00eas), por duas raz\u00f5es:\n\n* o SPC \u00e9 usado em muitas ind\u00fastrias;\n* entender as limita\u00e7\u00f5es do SPC \u00e9 essencial para entender as motiva\u00e7\u00f5es para o desenvolvimento de aplica\u00e7\u00f5es de MSPC com aprendizado de m\u00e1quina.\n\nA diferen\u00e7a entre SPC e MSPC \u00e9 que o SPC analisa cada vari\u00e1vel separadamente, enquanto o MSPC analisa todas de uma s\u00f3 vez, levando em conta as rela\u00e7\u00f5es entre elas.","2328f8a1":"O conjunto original `X` tem duas vari\u00e1veis, enquanto o conjunto transformado `T` tem apenas uma:","4b37cbc3":"Perceba que, apesar do conjunto transformado `T` ter dimens\u00e3o 1, a reconstru\u00e7\u00e3o tem dimens\u00e3o 2, assim como o conjunto original:","7dbdc4bc":"## Falha IDV(4)\n\nA falha `IDV(4)` \u00e9 mais dif\u00edcil de detectar do que a `IDV(1)`. Ela consiste de uma perturba\u00e7\u00e3o degrau positiva na temperatura da \u00e1gua de resfriamento do reator, o que causa dois efeitos nas vari\u00e1veis de processo:\n\n* um salto degrau na vaz\u00e3o de \u00e1gua de resfriamento do reator;\n* um aumento s\u00fabito na temperatura do reator, que rapidamente volta ao set-point por conta da a\u00e7\u00e3o da malha de controle.\n\nApenas essas duas vari\u00e1veis s\u00e3o afetadas de modo significativo; todas as outras mant\u00eam-se aproximadamente nos valores de opera\u00e7\u00e3o normal do processo.","343b2610":"***M\u00e3o na massa 3!***\n\n* Modifique a classe `PCA` de modo a fazer com que ela calcule as estat\u00edsticas $T^2$ e $Q$ tamb\u00e9m correspondentes \u00e0 etapa de treino. Plote as cartas de controle correspondentes. Voc\u00ea consegue pensar em alguma utilidade para esse c\u00e1lculo\/visualiza\u00e7\u00e3o?","385b5491":"Ou seja, dos 52 componentes, 30 s\u00e3o usados para modelar o espa\u00e7o principal e 22 sobram no espa\u00e7o residual.\n\nTestando o modelo:","df407bda":"# Tennessee Eastman Process\n\nA *Tennessee Eastman Process* \u00e9 uma simula\u00e7\u00e3o realista de um processo qu\u00edmico da [Eastman Chemical Company](https:\/\/www.eastman.com\/Pages\/Home.aspx) e foi introduzida por [DOWNS e VOGEL (1993)](http:\/\/users.abo.fi\/khaggblo\/RS\/Downs.pdf). \u00c9 o benchmark mais utilizado na \u00e1rea de controle e monitoramento de processos, por fornecer um ambiente controlado para gera\u00e7\u00e3o de dados, mas ainda assim realista.\n\nAbaixo tem-se o fluxograma do processo:","91722a09":"Ou seja, o conjunto de dados `X_reconstruido` est\u00e1 na dimensionalidade original do problema, mas com uma grande diferen\u00e7a: sem a informa\u00e7\u00e3o inicialmente presente ao longo do segundo componente principal.","44dadd0e":"O DataFrame de teste foi gerado com 500 simula\u00e7\u00f5es para cada um dos 20 cen\u00e1rio de falha, cada uma delas contendo 960 pontos de amostragem (totalizando 500\\*20\\*960 = 9600000 pontos no conjunto de dados). Como no conjunto de treino, as tr\u00eas primeiras colunas identificam os ID's da falha, da simula\u00e7\u00e3o e da amostra, respectivamente, e as demais colunas cont\u00eam as medi\u00e7\u00f5es de processo.","51eda3df":"Para cada coluna, uma nova coluna foi criada com o ponto da linha imediatamente anterior. Ou seja, cada linha  agora cont\u00e9m o passado das vari\u00e1veis! Essa representa\u00e7\u00e3o pode ajudar a modelar din\u00e2mica, ou seja, quando o passado das vari\u00e1veis \u00e9 importante para entender o presente.\n\nPerceba que a primeira linha foi eliminada, j\u00e1 que ela, obviamente, n\u00e3o tem passado (ao menos n\u00e3o registrado no DataFrame).\n\nPodemos adicionar mais pontos do passado, al\u00e9m do imediatamente anterior:","4b7b8c18":"### SPC - Cartas de controle\n\n[Cartas de controle](https:\/\/en.wikipedia.org\/wiki\/Control_chart) s\u00e3o gr\u00e1ficos que exibem a evolu\u00e7\u00e3o de um processo ao longo do tempo, junto com [limites de controle](https:\/\/en.wikipedia.org\/wiki\/Control_limits) que indicam a faixa de variabilidade natural ou aceit\u00e1vel do processo. Quando o processo se encontra dentro dos limites, diz-se que ele est\u00e1 \"sob controle\".\n\nObs: essa \u00e9 uma defini\u00e7\u00e3o de controle diferente da que aprendemos em cursos de engenharia de processos, em que [controle](https:\/\/en.wikipedia.org\/wiki\/Process_control) em geral refere-se \u00e0 t\u00e9cnicas de atua\u00e7\u00e3o autom\u00e1tica em processos de modo a mant\u00ea-los em estados especificados. Nesse sentido, um nome mais adequado para as cartas de controle seria \"cartas de monitoramento\".\n\nUtilizaremos os dados da Tennessee Eastman para demonstrar a constru\u00e7\u00e3o de cartas de controle univariadas. Definindo um conjunto de treino com dados de opera\u00e7\u00e3o normal e um conjunto de testes com dados do cen\u00e1rio `IDV(1)`:","fe38551b":"### Medi\u00e7\u00f5es de processo amostradas\n\n- Reactor Feed Analysis (Stream 6)\n  > - Sampling Frequency = 0.1 hr\n  > - Dead Time = 0.1 hr\n  > - Mole %\n  \nVariable | Description\n-------- | -----------\n`XMEAS(23)` | Component A\n`XMEAS(24)` | Component B\n`XMEAS(25)` | Component C\n`XMEAS(26)` | Component D\n`XMEAS(27)` | Component E\n`XMEAS(28)` | Component F\n\n- Purge Gas Analysis (Stream 9)\n  > - Sampling Frequency = 0.1 hr\n  > - Dead Time = 0.1 hr\n  > - Mole %\n\nVariable | Description\n-------- | -----------\n`XMEAS(29)` | Component A\n`XMEAS(30)` | Component B\n`XMEAS(31)` | Component C\n`XMEAS(32)` | Component D\n`XMEAS(33)` | Component E\n`XMEAS(34)` | Component F\n`XMEAS(35)` | Component G\n`XMEAS(36)` | Component H\n\n- Product Analysis (Stream 11)\n  > - Sampling Frequency = 0.25 hr\n  > - Dead Time = 0.25 hr\n  > - Mole %\n\nVariable | Description\n-------- | -----------\n`XMEAS(37)` | Component D\n`XMEAS(38)` | Component E\n`XMEAS(39)` | Component F\n`XMEAS(40)` | Component G\n`XMEAS(41)` | Component H","3f95b7a8":"No gr\u00e1fico da esquerda, a distribui\u00e7\u00e3o $p(\\mathbf{x})$ \u00e9 representada no eixo $z$. No gr\u00e1fico da direita, s\u00e3o mostrados os contornos de $p(\\mathbf{x})$.\n\nOs contornos da distribui\u00e7\u00e3o s\u00e3o elipses centradas em $\\boldsymbol \\mu$, o vetor de m\u00e9dias dos dados. Cada uma das elipses corresponde a um valor de probabilidade em particular. Em outras palavras, *cada uma das elipses define uma regi\u00e3o de confian\u00e7a associada a uma probabilidade em particular*. Por exemplo, a regi\u00e3o delimitada pela elipse de $p(\\mathbf{x}) = $ 95% cont\u00e9m 95% das observa\u00e7\u00f5es poss\u00edveis de serem medidas; ou seja, apenas 5% das observa\u00e7\u00f5es poss\u00edveis caem fora da dita regi\u00e3o.\n\nMatematicamente, as elipses s\u00e3o denotadas por $T^2$ e correspondem \u00e0 express\u00e3o:\n\n$$ T^2 = ({\\mathbf x}-{\\boldsymbol\\mu})^\\mathrm{T}{\\boldsymbol\\Sigma}^{-1}({\\mathbf x}-{\\boldsymbol\\mu}).$$\n\nNote que a express\u00e3o acima \u00e9 um fator do expoente presente na equa\u00e7\u00e3o da distribui\u00e7\u00e3o $p(\\mathbf{x})$.\n\nA interpreta\u00e7\u00e3o apresentada encara as elipses como os lugares geom\u00e9tricos dos pontos *estatisticamente equidistantes* da m\u00e9dia (desse ponto de vista, introduzido por [MAHALANOBIS (1936)](http:\/\/bayes.acs.unt.edu:8083\/BayesContent\/class\/Jon\/MiscDocs\/1936_Mahalanobis.pdf), a elipse \u00e9 chamada de [dist\u00e2ncia de Mahalanobis](https:\/\/en.wikipedia.org\/wiki\/Mahalanobis_distance)). A nota\u00e7\u00e3o $T^2$, introduzida por HOTELLING (1947), resulta de outra abordagem, que interpreta as elipses como uma esp\u00e9cie de generaliza\u00e7\u00e3o da estat\u00edstica [$t$ de Student](https:\/\/pt.wikipedia.org\/wiki\/Distribui%C3%A7%C3%A3o_t_de_Student).\n\nO monitoramento multivariado utilizando $T^2$ segue a seguinte l\u00f3gica:\n\n* define-se um limite de confian\u00e7a (digamos, 95%);\n* dados que caem dentro dos limites da elipse de 95% s\u00e3o considerados resultados da varia\u00e7\u00e3o natural do processo; caso caiam sistematicamente al\u00e9m dos limites, s\u00e3o considerados varia\u00e7\u00e3o anormal e um alarme \u00e9 acionado.\n\nTudo isso \u00e9 muito bonito, mas h\u00e1 dois grandes entraves que impedem a aplica\u00e7\u00e3o direta do $T^2$ para monitoramento de processos qu\u00edmicos:\n\n* nem sempre os dados seguem uma distribui\u00e7\u00e3o gaussiana;\n\n* para o c\u00e1lculo de $T^2$ \u00e9 necess\u00e1ria a invers\u00e3o da matriz de covari\u00e2ncias $\\boldsymbol\\Sigma$, o que \u00e9 dif\u00edcil quando h\u00e1 alta correla\u00e7\u00e3o entre as vari\u00e1veis (como \u00e9 comum na \u00e1rea de processos), tornando-se imposs\u00edvel no limite em que a correla\u00e7\u00e3o \u00e9 igual (ou pr\u00f3xima) a 1, por conta da consequente singularidade de $\\boldsymbol\\Sigma$.\n\nPor conta desses problemas \u00e9 que precisamos usar o PCA.","f844a87c":"\n## Escola Piloto Virtual - PEQ\/COPPE\/UFRJ\n## Data Science e Machine Learning na Pr\u00e1tica - Introdu\u00e7\u00e3o e Aplica\u00e7\u00f5es na Ind\u00fastria de Processos\n\nEste notebook \u00e9 referente \u00e0 Aula 2 do curso, que trata de t\u00e9cnicas de detec\u00e7\u00e3o de falhas utilizando PCA.\n","094fac27":"Para esse caso em espec\u00edfico, v\u00e1rias das cartas gerariam alarmes para a falha de maneira satisfat\u00f3ria. Apesar disso, h\u00e1 quest\u00f5es que nos motivam a ir al\u00e9m e utilizar uma estrat\u00e9gia multivariada:\n\n* H\u00e1 muitas cartas de controle! Em processos com mais vari\u00e1veis, isso seria ainda mais cr\u00edtico. \u00c9 dif\u00edcil gerir os v\u00e1rios alarmes associados \u00e0s v\u00e1rias cartas.\n\n* A metodologia exposta funciona para comportamentos anormais associados \u00e0 cada vari\u00e1vel individual, e n\u00e3o \u00e0s rela\u00e7\u00f5es entre vari\u00e1veis.\n\nPara mais detalhes quanto \u00e0s metodologias de SPC univariadas, pode-se consultar, por exemplo, [MONTGOMERY (2012)](https:\/\/www.amazon.com\/Statistical-Quality-Control-Douglas-Montgomery\/dp\/1118146816) ou o [pacote PySpc](https:\/\/github.com\/carlosqsilva\/pyspc).","206ea44c":"Abaixo s\u00e3o constru\u00eddas cartas de controle com os dados de treino. Cada carta cont\u00e9m:\n\n* as medi\u00e7\u00f5es da vari\u00e1vel, em azul;\n* a refer\u00eancia adotada (no caso, a m\u00e9dia da vari\u00e1vel), como uma reta horizontal preta;\n* os limites de controle (no caso, definidos como tr\u00eas desvios-padr\u00e3o amostrais abaixo e acima da m\u00e9dia), como retas horizontais tracejadas vermelhas.","b5b48cc9":"No caso acima, como geramos vari\u00e1veis com atraso 1 e 2, as duas primeiras linhas tiveram de ser eliminadas.\n\nSem mais delongas, vamos aplicar a t\u00e9cnica aos dados da falha `IDV(11)`!","86be9d43":"Lembre que definimos o valor do limite de confian\u00e7a, em ambos os casos, como 99%; ou seja, a taxa de falsos alarmes esperada pela especifica\u00e7\u00e3o nos limites \u00e9 de 1%. Nota-se que, na pr\u00e1tica, esse valor \u00e9 ligeiramente menor para o $T^2$ e maior para o $Q$.","1b42ba46":"Apenas a estat\u00edstica $Q$ foi capaz de detectar satisfatoriamente a falha! Isso significa que a falha quebrou rela\u00e7\u00f5es entre vari\u00e1veis de processo, gerando novas dire\u00e7\u00f5es n\u00e3o identificadas pelo modelo PCA. As dire\u00e7\u00f5es antes identificadas pelo PCA foram pouco afetadas, como nota-se pela carta de controle $T^2$.","7099d0bb":"## Filtro de ru\u00eddo com m\u00e9dia m\u00f3vel\n\nO filtro de [m\u00e9dia m\u00f3vel](https:\/\/en.wikipedia.org\/wiki\/Moving_average) \u00e9 uma das maneiras mais simples para diminuir ru\u00eddo (variabilidade indesejada) em um conjunto de dados. Ele funciona substituindo pontos individuais por m\u00e9dias calculadas em janelas de dados. \u00c9 uma estrat\u00e9gia que est\u00e1 sendo usada, por exemplo, por [ve\u00edculos de comunica\u00e7\u00e3o durante o per\u00edodo da pandemia de COVID-19](https:\/\/g1.globo.com\/bemestar\/coronavirus\/noticia\/2020\/07\/27\/entenda-como-e-calculada-a-media-movel-e-a-variacao-dos-casos-e-mortes-por-covid-19.ghtml), para atenuar o efeito de flutua\u00e7\u00f5es di\u00e1rias nos n\u00fameros de casos confirmados e mortes pela doen\u00e7a (por exemplo, os n\u00fameros flutuam negativamente todo fim de semana, mas por quest\u00f5es burocr\u00e1ticas e n\u00e3o epidemiol\u00f3gicas, j\u00e1 que nesses dias h\u00e1 menos registros).\n\nExistem v\u00e1rias estrat\u00e9gias para aplicar filtros de m\u00e9dia m\u00f3vel. Aqui, substituiremos cada ponto pela m\u00e9dia de uma janela de tamanho $W$ que cont\u00e9m o ponto em quest\u00e3o e $W-1$ pontos anteriores. A seguinte fun\u00e7\u00e3o implementa essa funcionalidade:","6b058dec":"## Adi\u00e7\u00e3o de vari\u00e1veis atrasadas no tempo\n\nVari\u00e1veis atrasadas no tempo (conhecidas tamb\u00e9m como *lags*) s\u00e3o colunas adicionadas ao conjunto de dados que incluem pontos do passado das vari\u00e1veis. \n\nA fun\u00e7\u00e3o abaixo aceita um DataFrame e retorna outro contendo vari\u00e1veis atrasadas:","46bf3bc8":"<img src=\"https:\/\/raw.githubusercontent.com\/gmxavier\/TEP-meets-LSTM\/master\/tep_flowsheet.png\" width=\"800\" height=\"800\"\/>","615b540b":"***M\u00e3o na massa 4!***\n\n* Analise todos os demais cen\u00e1rios de falha, gerando as respectivas cartas de controle e taxas de detec\u00e7\u00e3o para ambas as estat\u00edsticas e o mapa de contribui\u00e7\u00e3o para a estat\u00edstica $Q$. Identifique quais falhas s\u00e3o mais dif\u00edceis de detectar e em quais delas o desempenho melhora com o uso das t\u00e9cnicas de pr\u00e9-processamento. \n\nDica: como s\u00e3o muitos cen\u00e1rios de falha e os procedimentos s\u00e3o os mesmos para todos eles, recomendo que a an\u00e1lise seja automatizada com [la\u00e7os *for*](https:\/\/www.w3schools.com\/python\/python_for_loops.asp), por exemplo.","ff3cd60d":"### Medi\u00e7\u00f5es de processo cont\u00ednuas\n\nVariable | Description | unit\n-------- | ----------- | ----\n`XMEAS(1)`  | A Feed  (stream 1)                  | kscmh\n`XMEAS(2)`  | D Feed  (stream 2)                  | kg\/hr\n`XMEAS(3)`  | E Feed  (stream 3)                  | kg\/hr\n`XMEAS(4)`  | A and C Feed  (stream 4)            | kscmh\n`XMEAS(5)`  | Recycle Flow  (stream 8)            | kscmh\n`XMEAS(6)`  | Reactor Feed Rate  (stream 6)       | kscmh\n`XMEAS(7)`  | Reactor Pressure                    | kPa gauge\n`XMEAS(8)`  | Reactor Level                       | %\n`XMEAS(9)`  | Reactor Temperature                 | Deg C\n`XMEAS(10)` | Purge Rate (stream 9)               | kscmh\n`XMEAS(11)` | Product Sep Temp                    | Deg C\n`XMEAS(12)` | Product Sep Level                   | %\n`XMEAS(13)` | Prod Sep Pressure                   | kPa gauge\n`XMEAS(14)` | Prod Sep Underflow (stream 10)      | m3\/hr\n`XMEAS(15)` | Stripper Level                      | %\n`XMEAS(16)` | Stripper Pressure                   | kPa gauge\n`XMEAS(17)` | Stripper Underflow (stream 11)      | m3\/hr\n`XMEAS(18)` | Stripper Temperature                | Deg C\n`XMEAS(19)` | Stripper Steam Flow                 | kg\/hr\n`XMEAS(20)` | Compressor Work                     | kW\n`XMEAS(21)` | Reactor Cooling Water Outlet Temp   | Deg C\n`XMEAS(22)` | Separator Cooling Water Outlet Temp | Deg C","ea975482":"## Falha IDV(1)\n\nVamos gerar cartas de controle para o cen\u00e1rio `IDV(1)`, que analisamos no come\u00e7o do notebook:"}}