{"cell_type":{"2f45f178":"code","e96c8c84":"code","0c585c09":"code","0464c294":"code","d7cad148":"code","3b998ed7":"code","996a2fce":"code","1561bb02":"markdown"},"source":{"2f45f178":"import os\nimport torch\nimport random\nimport pandas as pd\nimport numpy as np\nfrom operator import lt, gt\nfrom sklearn.metrics import mean_squared_error\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.preprocessing import LabelEncoder\n\n\ndef seed_everything(seed=1127):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n\n\nseed_everything()\n\n\nclass TrainDataset(Dataset):\n    def __init__(self, x, y): \n        self.x, self.y = x.astype('float32'), y.astype('float32')\n\n    def __len__(self): \n        return self.x.shape[0]\n\n    def __getitem__(self, index): \n        return self.x[index], self.y[index]\n\n\nclass PredictDataset(Dataset):\n    def __init__(self, x): \n        self.x = x.astype('float32')\n\n    def __len__(self): \n        return self.x.shape[0]\n\n    def __getitem__(self, index): \n        return self.x[index]\n\n\nclass AverageMeter(object):\n    def __init__(self): self.reset()\n\n    def reset(self): self.val, self.avg, self.sum, self.count = 0, 0, 0, 0\n\n    def update(self, val, n=1):\n        self.val = val\n        self.sum += val * n\n        self.count += n\n        self.avg = self.sum \/ self.count\n\n\nclass EarlyStopping(object):\n    def __init__(self, mode='min', min_delta=0, percentage=False, patience=10, initial_bad=0, initial_best=np.nan, verbose=0):\n        self.mode = mode\n        self.patience = patience\n        self.best = float('inf') if mode == 'min' else float('-inf')\n        self.num_bad_epochs = initial_bad\n        self.is_better = self._init_is_better(mode, min_delta, percentage)\n        self.verbose = verbose\n        self._stop = False\n\n    def step(self, metric):\n        if self.is_better(metric, self.best):\n            self.num_bad_epochs = 0\n            self.best = metric\n        else:\n            self.num_bad_epochs += 1\n\n        if np.isnan(self.best) and (not np.isnan(metric)):\n            self.num_bad_epochs = 0\n            self.best = metric\n\n        self._stop = self.num_bad_epochs >= self.patience\n        if self.verbose and self._stop: print('Early Stopping Triggered, best score is: ', self.best)\n        return self._stop\n\n    def _init_is_better(self, mode, min_delta, percentage):\n        comparator = lt if mode == 'min' else gt\n        if not percentage:\n            def _is_better(new, best):\n                target = best - min_delta if mode == 'min' else best + min_delta\n                return comparator(new, target)\n        else:\n            def _is_better(new, best):\n                target = best * (1 - (min_delta \/ 100)) if mode == 'min' else best * (1 + (min_delta \/ 100))\n                return comparator(new, target)\n        return _is_better\n\n    \ndef add_swap_noise_torch(X, ratio=.15):\n    obfuscation_mask = torch.bernoulli(ratio * torch.ones(X.shape)).to(X.device)\n    obfuscated_X = torch.where(obfuscation_mask == 1, X[torch.randperm(X.shape[0])], X)\n    return obfuscated_X, obfuscation_mask\n\n\nclass MLPModel(torch.nn.Module):\n    def __init__(self, num_inputs, hidden_size=512, input_dropout=.1, dropout_rate=.25, lower_bound=0, upper_bound=10.5):\n        super().__init__()\n        self.use_input_dropout = input_dropout > 0\n        if input_dropout: self.input_dropout = torch.nn.Dropout(input_dropout)\n        self.linear_1 = torch.nn.Linear(num_inputs, hidden_size)\n        self.dropout_1 = torch.nn.Dropout(dropout_rate)\n        self.linear_2 = torch.nn.Linear(hidden_size, hidden_size)\n        self.dropout_2 = torch.nn.Dropout(dropout_rate)\n        self.last_linear = torch.nn.Linear(hidden_size, 1)\n        self.lower_bound = lower_bound\n        self.upper_bound = upper_bound\n\n    def net(self, x):\n        if self.use_input_dropout: x = self.input_dropout(x)\n        act1 = torch.nn.functional.relu(self.linear_1(x))\n        x = self.dropout_1(act1)\n        act2 = torch.nn.functional.relu(self.linear_2(x))\n        x = self.dropout_2(act2)\n        return self.last_linear(x)\n\n    def forward(self, x):\n        return torch.sigmoid(self.net(x)) * (self.upper_bound - self.lower_bound) + self.lower_bound\n\n\nclass DeepStackDAE(torch.nn.Module):\n    def __init__(self, cards, hidden_size=1500, num_cats=10, num_conts=14, emphasis=1, lower_bound=0, upper_bound=10.5):\n        super().__init__()\n        self.cards = cards\n        self.num_cats = num_cats\n        self.num_conts = num_conts\n\n        post_encoding_input_size = num_conts + sum(cards)\n\n        self.linear_1 = torch.nn.Linear(in_features=post_encoding_input_size, out_features=hidden_size)\n        self.linear_2 = torch.nn.Linear(in_features=hidden_size, out_features=hidden_size)\n        self.linear_3 = torch.nn.Linear(in_features=hidden_size, out_features=hidden_size)\n        self.linear_4 = torch.nn.Linear(in_features=hidden_size, out_features=post_encoding_input_size)\n\n        self.emphasis = emphasis\n        self.upper_bound = upper_bound\n        self.lower_bound = lower_bound\n\n    def one_hot_encoding(self, x):\n        encoded = torch.cat([\n            torch.nn.functional.one_hot(x[:, i].long(), num_classes=self.cards[i])\n            for i in range(self.num_cats)\n        ], dim=1)\n        return encoded\n\n    def forward(self, x):\n        x_post_encoded = torch.cat([self.one_hot_encoding(x), x[:, self.num_cats:]], dim=1)\n        act_1 = torch.nn.functional.relu(self.linear_1(x_post_encoded))\n        act_2 = torch.nn.functional.relu(self.linear_2(act_1))\n        act_3 = torch.nn.functional.relu(self.linear_3(act_2))\n        out = self.linear_4(act_3)\n        return act_1, act_2, act_3, out\n\n    def feature(self, x):\n        return torch.cat(self.forward(x)[:-1], dim=1)\n\n    def split(self, t):\n        return torch.split(t, self.cards + [self.num_conts], dim=1)\n\n    def loss(self, x, y, mask=None, weights=[10, 14]):\n        if mask is None: mask = torch.ones(x.shape).to(x.device)\n        loss_weights = mask * self.emphasis + (1 - mask) * (1 - self.emphasis)\n\n        out = self.split(self.forward(x)[-1])\n        cat_losses = torch.cat([\n            torch.nn.functional.cross_entropy(out[i], y[:, i].long(), reduction='none').reshape(-1, 1)\n            for i in range(self.num_cats)\n        ], dim=1)\n        cont_losses = torch.nn.functional.mse_loss(out[-1], y[:, -self.num_conts:], reduction='none')\n        unweighted_loss = torch.cat([cat_losses, cont_losses], dim=1)\n        weighted_loss = loss_weights * unweighted_loss\n        return weighted_loss.mean()","e96c8c84":"# prepare data\ntrain_data = pd.read_csv('..\/input\/tabular-playground-series-feb-2021\/train.csv')\ntest_data = pd.read_csv('..\/input\/tabular-playground-series-feb-2021\/test.csv')\n\ncards = []\nX = np.vstack([train_data.iloc[:, 1:-1].to_numpy(), test_data.iloc[:, 1:].to_numpy()])\nfor i in range(10):\n    encoder = LabelEncoder().fit(X[:, i])\n    X[:, i] = encoder.transform(X[:, i])\n    cards.append(len(encoder.classes_))\nembeded_dims = [int(c ** (2\/3)) for c in cards]\n\ny = train_data['target'].to_numpy().reshape(-1, 1)","0c585c09":"# hyperparams\ndae_hidden_size = 1500\ndae_noise_ratio = .3\ndae_batch_size = 512 \ndae_init_lr = 3e-4\ndae_lr_gamma = .995\ndae_denoise_emphesis = .8\n\nmlp_hidden_size = 512\nmlp_batch_size = 512\nmlp_init_lr = 5e-5\nmlp_input_dropout = 0\nmlp_dropout = .4\nmlp_l2_reg = 2e-3","0464c294":"dae_dl = DataLoader(dataset=PredictDataset(X), batch_size=dae_batch_size, shuffle=True, pin_memory=True, drop_last=True)\ndae = DeepStackDAE(\n    cards, \n    hidden_size=dae_hidden_size, \n    emphasis=dae_denoise_emphesis\n).cuda()\noptimizer = torch.optim.Adam(\n    dae.parameters(), \n    lr=dae_init_lr\n)\nscheduler = torch.optim.lr_scheduler.ExponentialLR(\n    optimizer, \n    gamma=dae_lr_gamma\n)\nearlystopper = EarlyStopping(mode='min', min_delta=1e-7, patience=200, percentage=False, verbose=0)\n\nfor epoch in range(3000):\n    dae.train()    \n    meter = AverageMeter()\n    for i, x in enumerate(dae_dl):\n        x = x.cuda()\n        noisy_x, mask = add_swap_noise_torch(x, dae_noise_ratio)\n        optimizer.zero_grad()\n        loss = dae.loss(noisy_x, x, mask)\n        loss.backward()\n        optimizer.step()\n        meter.update(loss.detach().cpu().numpy())\n    scheduler.step()\n    if epoch % 100 == 0: print(epoch, meter.avg)    \n    if earlystopper.step(meter.avg): break    \n    ","d7cad148":"n_total = len(train_data)\ncut_off = int(n_total * .9)\ntrain_dl = DataLoader(dataset=TrainDataset(X[:cut_off], y[:cut_off]), batch_size=mlp_batch_size, shuffle=True, pin_memory=True, drop_last=True)\nvalid_dl = DataLoader(dataset=TrainDataset(X[cut_off:n_total], y[cut_off:]), batch_size=mlp_batch_size, shuffle=False, pin_memory=True, drop_last=False)\nmodel = MLPModel(\n    3 * dae_hidden_size, \n    hidden_size=mlp_hidden_size, \n    input_dropout=mlp_input_dropout, \n    dropout_rate=mlp_dropout\n).cuda()\noptimizer = torch.optim.Adam(\n    model.parameters(), \n    lr=mlp_init_lr, \n    weight_decay=mlp_l2_reg\n)\nscheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=1\/3, patience=10, verbose=0, cooldown=2, min_lr=1e-7)\nearlystopper = EarlyStopping(mode='min', min_delta=1e-7, patience=20, percentage=False, verbose=0)\n\nfor epoch in range(777):\n    model.train()\n    for i, (x, target) in enumerate(train_dl):\n        x, target = x.cuda(), target.cuda()\n        with torch.no_grad(): x = dae.feature(x)\n        optimizer.zero_grad()\n        loss = torch.nn.functional.mse_loss(model.forward(x), target)\n        loss.backward()\n        optimizer.step()\n\n    model.eval()\n    predictions = []\n    with torch.no_grad():\n        for _, (x, _) in enumerate(valid_dl):\n            x = dae.feature(x.cuda())\n            prediction = model.forward(x)\n            predictions.append(prediction.detach().cpu().numpy())\n    predictions = np.concatenate(predictions)\n    valid_rmse = mean_squared_error(valid_dl.dataset.y, predictions, squared=False)\n    scheduler.step(valid_rmse)\n    if epoch % 20 == 0: print(epoch, valid_rmse)    \n    if earlystopper.step(valid_rmse): break","3b998ed7":"predictions = []\ntest_dl = DataLoader(dataset=PredictDataset(X[n_total:]), batch_size=mlp_batch_size, shuffle=False, pin_memory=True, drop_last=False)\nwith torch.no_grad():\n    for _, x in enumerate(test_dl):\n        x = dae.feature(x.cuda())\n        prediction = model.forward(x)\n        predictions.append(prediction.detach().cpu().numpy())\npredictions = np.concatenate(predictions)\n\nsub = pd.read_csv('..\/input\/tabular-playground-series-feb-2021\/sample_submission.csv')\nsub['target'] = predictions\nsub.to_csv('submission.csv', index=False)","996a2fce":"sub.head()","1561bb02":"This is my DAE experimentation code from last month. Did not have as much time as I wished to play around with it myself. Feel free to take it from here and see if you can get it working. "}}