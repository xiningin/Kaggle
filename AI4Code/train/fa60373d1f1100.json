{"cell_type":{"4ba181dc":"code","ef4abd86":"code","c25c12db":"code","79755051":"code","956ad64d":"code","d466ba16":"code","05f3fdcc":"code","ead7d419":"code","418722ee":"code","224962c0":"code","e97bf120":"code","c723a413":"code","5bee5a59":"code","3decd993":"code","0fa34254":"code","47470837":"code","c4923b4b":"code","1ff5ae20":"code","75afea3f":"code","364da87d":"code","8230530a":"code","86c9f04c":"code","e12de4ca":"code","e55f724a":"code","0b7c4a19":"code","4d04ddd0":"markdown","6ff71b86":"markdown","469438ca":"markdown","33f5303d":"markdown","223d02f2":"markdown","286094c2":"markdown","8c82bbee":"markdown","638f6f87":"markdown","22dc972c":"markdown","0b409911":"markdown","6eb6d083":"markdown","1bb57199":"markdown","53b29e9d":"markdown","ba1a0229":"markdown","3506a1b8":"markdown","630cbdda":"markdown","2940236b":"markdown","a76056cd":"markdown","d1858a8f":"markdown"},"source":{"4ba181dc":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn import model_selection\n\nfrom sklearn import metrics\nfrom sklearn.metrics import accuracy_score\n\nsns.set(style='ticks')\n%matplotlib inline","ef4abd86":"dataset = pd.read_csv('..\/input\/parkinsons-data\/Data - Parkinsons')\ndataset.head()","c25c12db":"dataset.tail()","79755051":"dataset.dtypes","956ad64d":"dataset.shape","d466ba16":"# Dispalying the descriptive statistics describe each attribute\ndataset.describe()","05f3fdcc":"plt.figure(figsize=(18,5))\nsns.set_color_codes()\nsns.distplot(dataset[\"MDVP:Fo(Hz)\"])","ead7d419":"dataset.boxplot(figsize=(24,8))","418722ee":"sns.pairplot(dataset,diag_kind='kde')","224962c0":"# Heatmap visulisation for each attribute coefficient correlation.\nimport seaborn as sns\ncorr_map=dataset.corr()\nsns.heatmap(corr_map,square=True)","e97bf120":"import matplotlib.pyplot as plt\nimport numpy as np\n\n# K value means how many features required to see in heat map\nk=10\n\n# finding the columns which related to output attribute and we are arranging from top coefficient correlation value to downwards.\ncols=corr_map.nlargest(k,'status')['status'].index\n\n# correlation coefficient values\ncoff_values=np.corrcoef(dataset[cols].values.T)\nsns.set(font_scale=1.25)\nsns.heatmap(coff_values,cbar=True,annot=True,square=True,fmt='.2f',\n           annot_kws={'size': 10},yticklabels=cols.values,xticklabels=cols.values)\nplt.show()","c723a413":"# correlation coefficient values in each attributes.\ncorrelation_values=dataset.corr()['status']\ncorrelation_values.abs().sort_values(ascending=False)","5bee5a59":"# Checking null values\ndataset.info()","3decd993":"# Checking null value sum\ndataset.isna().sum()","0fa34254":"# split the dataset into input and output attribute.\n\ny=dataset['status']\ncols=['MDVP:RAP','Jitter:DDP','DFA','NHR','MDVP:Fhi(Hz)','name','status']\nx=dataset.drop(cols,axis=1)","47470837":"# Splitting the dataset into trianing and test set\n\ntrain_size=0.70\ntest_size=0.30\nseed=5\n\nfrom sklearn.model_selection import train_test_split\nx_train,x_test,y_train,y_test=train_test_split(x,y,train_size=train_size,test_size=test_size,random_state=seed)","c4923b4b":"x_train.shape","1ff5ae20":"x_train.head()","75afea3f":"\nn_neighbors=5\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.naive_bayes import GaussianNB\n\n# keeping all models in one list\nmodels=[]\nmodels.append(('LogisticRegression',LogisticRegression()))\nmodels.append(('knn',KNeighborsClassifier(n_neighbors=n_neighbors)))\nmodels.append(('SVC',SVC()))\nmodels.append((\"decision_tree\",DecisionTreeClassifier()))\nmodels.append(('Naive Bayes',GaussianNB()))\n\n# Evaluating Each model\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score\nnames=[]\npredictions=[]\nerror='accuracy'\nfor name,model in models:\n    fold=KFold(n_splits=10,random_state=0)\n    result=cross_val_score(model,x_train,y_train,cv=fold,scoring=error)\n    predictions.append(result)\n    names.append(name)\n    msg=\"%s : %f (%f)\"%(name,result.mean(),result.std())\n    print(msg)\n    \n\n# Visualizing the Model accuracy\nfig=plt.figure()\nfig.suptitle(\"Comparing Algorithms\")\nplt.boxplot(predictions)\nplt.show()","364da87d":"# Spot Checking and Comparing Algorithms With StandardScaler Scaler\nfrom sklearn.pipeline import Pipeline\nfrom sklearn. preprocessing import StandardScaler\npipelines=[]\npipelines.append(('scaled Logisitic Regression',Pipeline([('scaler',StandardScaler()),('LogisticRegression',LogisticRegression())])))\npipelines.append(('scaled KNN',Pipeline([('scaler',StandardScaler()),('KNN',KNeighborsClassifier(n_neighbors=n_neighbors))])))\npipelines.append(('scaled SVC',Pipeline([('scaler',StandardScaler()),('SVC',SVC())])))\npipelines.append(('scaled DecisionTree',Pipeline([('scaler',StandardScaler()),('decision',DecisionTreeClassifier())])))\npipelines.append(('scaled naive bayes',Pipeline([('scaler',StandardScaler()),('scaled Naive Bayes',GaussianNB())])))\n\n# Evaluating Each model\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score\nnames=[]\npredictions=[]\nfor name,model in models:\n    fold=KFold(n_splits=10,random_state=0)\n    result=cross_val_score(model,x_train,y_train,cv=fold,scoring=error)\n    predictions.append(result)\n    names.append(name)\n    msg=\"%s : %f (%f)\"%(name,result.mean(),result.std())\n    print(msg)\n    \n# Visualizing the Model accuracy\nfig=plt.figure()\nfig.suptitle(\"Comparing Algorithms\")\nplt.boxplot(predictions)\nplt.show()","8230530a":"# Decision Tree Tunning Algorithms\nimport numpy as np\nfrom sklearn.model_selection import GridSearchCV\nscaler=StandardScaler().fit(x_train)\nrescaledx=scaler.transform(x_train)\nparam_grid=dict()\nmodel=DecisionTreeClassifier()\nfold=KFold(n_splits=10,random_state=5)\ngrid=GridSearchCV(estimator=model,param_grid=param_grid,scoring=error,cv=fold)\ngrid_result=grid.fit(rescaledx,y_train)\n\nprint(\"Best: %f using %s \"%(grid_result.best_score_,grid_result.best_params_))","86c9f04c":"# Logistic Regression Tuning Algorithm\nimport numpy as np\nfrom sklearn.model_selection import GridSearchCV\nscaler=StandardScaler().fit(x_train)\nrescaledx=scaler.transform(x_train)\nc=[0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1.0]\nparam_grid=dict(C=c)\nmodel=LogisticRegression()\nfold=KFold(n_splits=10,random_state=5)\ngrid=GridSearchCV(estimator=model,param_grid=param_grid,scoring=error,cv=fold)\ngrid_result=grid.fit(rescaledx,y_train)\n\nprint(\"Best: %f using %s \"%(grid_result.best_score_,grid_result.best_params_))","e12de4ca":"#Ensemble\n# Boosting methods\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\n# Bagging methods\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import ExtraTreesClassifier\nensembles=[]\nensembles.append(('scaledAB',Pipeline([('scale',StandardScaler()),('AB',AdaBoostClassifier())])))\nensembles.append(('scaledGBC',Pipeline([('scale',StandardScaler()),('GBc',GradientBoostingClassifier())])))\nensembles.append(('scaledRFC',Pipeline([('scale',StandardScaler()),('rf',RandomForestClassifier(n_estimators=10))])))\nensembles.append(('scaledETC',Pipeline([('scale',StandardScaler()),('ETC',ExtraTreesClassifier(n_estimators=10))])))\n\n# Evaluate each Ensemble Techinique\nresults=[]\nnames=[]\nfor name,model in ensembles:\n    fold=KFold(n_splits=10,random_state=5)\n    result=cross_val_score(model,x_train,y_train,cv=fold,scoring=error)\n    results.append(result)\n    names.append(name)\n    msg=\"%s : %f (%f)\"%(name,result.mean(),result.std())\n    print(msg)\n    \n# Visualizing the compared Ensemble Algorithms\nfig=plt.figure()\nfig.suptitle('Ensemble Compared Algorithms')\nplt.boxplot(results)\nplt.show()","e55f724a":"# GradientBoosting ClassifierTuning\nimport numpy as np\nfrom sklearn.model_selection import GridSearchCV\nscaler=StandardScaler().fit(x_train)\nrescaledx=scaler.transform(x_train)\nn_estimators=[10,20,30,40,50,100,150,200,250,300]\nlearning_rate=[0.001,0.01,0.1,0.3,0.5,0.7,1.0]\nparam_grid=dict(n_estimators=n_estimators,learning_rate=learning_rate)\nmodel=GradientBoostingClassifier()\nfold=KFold(n_splits=10,random_state=5)\ngrid=GridSearchCV(estimator=model,param_grid=param_grid,scoring=error,cv=fold)\ngrid_result=grid.fit(rescaledx,y_train)\n\nprint(\"Best: %f using %s \"%(grid_result.best_score_,grid_result.best_params_))","0b7c4a19":"# Extra Trees Classifier Classifier Tuning\nimport numpy as np\nfrom sklearn.model_selection import GridSearchCV\nscaler=StandardScaler().fit(x_train)\nrescaledx=scaler.transform(x_train)\nn_estimators=[10,20,30,40,50,100,150,200]\nparam_grid=dict(n_estimators=n_estimators)\nmodel=ExtraTreesClassifier()\nfold=KFold(n_splits=10,random_state=5)\ngrid=GridSearchCV(estimator=model,param_grid=param_grid,scoring=error,cv=fold)\ngrid_result=grid.fit(rescaledx,y_train)\n\nprint(\"Best: %f using %s \"%(grid_result.best_score_,grid_result.best_params_))","4d04ddd0":"# Loading Dataset","6ff71b86":"# Classification Models With Feature Scale.","469438ca":"\n**Data Description & Context:**\nParkinson\u2019s Disease (PD) is a degenerative neurological disorder marked by decreased dopamine levels in the brain. It manifests itself through a deterioration of movement, including the presence of tremors and stiffness. There is commonly a marked effect on speech, including dysarthria (difficulty articulating sounds), hypophonia (lowered volume), and monotone (reduced pitch range). Additionally, cognitive impairments and changes in mood can occur, and risk of dementia is increased.\nTraditional diagnosis of Parkinson\u2019s Disease involves a clinician taking a neurological history of the patient and observing motor skills in various situations. Since there is no definitive laboratory test to diagnose PD, diagnosis is often difficult, particularly in the early stages when motor effects are not yet severe. Monitoring progression of the disease over time requires repeated clinic visits by the patient. An effective screening process, particularly one that doesn\u2019t require a clinic visit, would be beneficial. Since PD patients exhibit characteristic vocal features, voice recordings are a useful and non-invasive tool for diagnosis. If machine learning algorithms could be applied to a voice recording dataset to accurately diagnosis PD, this would be an effective screening step prior to an appointment with a clinician\n\n**Domain:**\nMedicine\n\n**Attribute Information:**\nname - ASCII subject name and recording number\nMDVP:Fo(Hz) - Average vocal fundamental frequency\nMDVP:Fhi(Hz) - Maximum vocal fundamental frequency\nMDVP:Flo(Hz) - Minimum vocal fundamental frequency\nMDVP:Jitter(%),MDVP:Jitter(Abs),MDVP:RAP,MDVP:PPQ,Jitter:DDP - Several\nmeasures of variation in fundamental frequency\nMDVP:Shimmer,MDVP:Shimmer(dB),Shimmer:APQ3,Shimmer:APQ5,MDVP:APQ,Shimmer:DDA - Several measures of variation in amplitude\nNHR,HNR - Two measures of ratio of noise to tonal components in the voice\nstatus - Health status of the subject (one) - Parkinson's, (zero) - healthy\nRPDE,D2 - Two nonlinear dynamical complexity measures\nDFA - Signal fractal scaling exponent\nspread1,spread2,PPE - Three nonlinear measures of fundamental frequency variation 9. car name: string (unique for each instance)\n\n**Objective:**\nGoal is to classify the patients into the respective labels using the attributes from their voice recordings","33f5303d":"As per above accuracy we are going to pickup top 2 best performance algorithms.\n\nDecision Tree Classification Algorithm\n\nLogistic Regression Classification Algorithm","223d02f2":"# Classification Models Without Feature Scale.","286094c2":"# # Studying the data distribution in each attribute","8c82bbee":"#  Splitting Data into Train Set and Test Set","638f6f87":"# Extra Trees Classification Algoriothm 0.904412 using {'n_estimators': 30}giving the best accuracy performance so we are going to use this ensemble algorithm to fit and predict our dataset","22dc972c":"# Comparing all 4 algorithms top 2 algorithm and top 2 ensemble algorithms.","0b409911":"# Importing Libraries","6eb6d083":"From the above box plots, we observe that there are less outliers. So, the model will not be affected by the outliers.","1bb57199":"#Prediction we got without applying feature scaling\n\n1.Logistic Regression Classification Algorithm : 0.859583 (0.114429)\n\n2.K-Nearest Neighbors classification Algorithm : 0.834167 (0.118714)\n\n3.Support Vector Machine classification Algorithm : 0.821667 (0.117951)\n\n4.Decision Tree Classification Algorithm : 0.840000 (0.106771)\n\n5.Naive bayes Classification Algorithm : 0.735833 (0.071715)\n\n\n#Prediction we got with applying feature scaling\n\n1.Logistic Regression Classification Algorithm : 0.859583 (0.114429)\n\n2.K-Nearest Neighbors classification Algorithm : 0.834167 (0.118714)\n\n3.Support Vector Machine classification Algorithm : 0.821667 (0.117951)\n\n4.Decision Tree Classification Algorithm : 0.865833 (0.076508)\n\n5.Naive bayes Classification Algorithm : 0.735833 (0.071715)","53b29e9d":"We got accuracy for ensemble algorithms likewise...\n\nAda Boost Classification Algorithm : 0.839560 (0.081896)\n    \nGradient Boosting Classification Algorithm : 0.883516 (0.064067)\n    \nRandom Forest Classification Algorithm : 0.882418 (0.047913)\n    \nExtra Trees Classification Algoriothm :  0.897802 (0.033259)\n    ","ba1a0229":"# Univariate and BiVariate Analysis of Different Attributes","3506a1b8":"After Applying Tuning to top 2 algorithms.\n\nDecision Tree Classification Algorithm Best: 0.838235 using {}\n\nLogistic Regression Classification Algorithm Best: 0.838235 using {'C': 0.6}","630cbdda":"## EDA","2940236b":"# Checking Null Values","a76056cd":"\nDecision Tree Classification Algorithm Best: 0.838235 using {}\n    \nLogistic Regression Classification Algorithm Best: 0.838235 using {'C': 0.6}\n    \nGradient Boosting Classification Algorithm 0.897059 using {'learning_rate': 0.1, 'n_estimators': 40} \n\nExtra Trees Classification Algoriothm 0.904412 using {'n_estimators': 30}","d1858a8f":"After applying tuning to top 2 ensemble algorithms we got accuracy like\n\nGradient Boosting Classification Algorithm 0.897059 using {'learning_rate': 0.1, 'n_estimators': 40} \n\nExtra Trees Classification Algoriothm 0.904412 using {'n_estimators': 30} "}}