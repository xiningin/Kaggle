{"cell_type":{"6947d62c":"code","a8c4fb4d":"code","3e9e3cc1":"code","6c8c43ce":"code","1bceb23d":"code","b0272afb":"code","7de0b4dd":"code","d85bc306":"code","67796d69":"code","1ad5ba15":"code","229c0a75":"code","ca9332fe":"code","57c47bc8":"code","ef32d647":"code","46197e14":"code","12594709":"code","23caafc3":"code","57a5997d":"code","284f507a":"code","b15486fb":"code","84c4ee8a":"code","545a70df":"code","a71f7f56":"code","283d38ac":"code","c0055103":"code","4de74fc8":"code","8c68310f":"code","2591d441":"code","a73002b0":"code","fd6142eb":"code","b7248034":"code","00bb3b28":"code","05983bf4":"code","047a0a99":"code","ac340c70":"code","87535b0e":"code","724bec4a":"code","bea54f0e":"code","0fc18846":"code","a6bddc91":"code","e7840b6c":"code","027e3065":"code","077d20df":"code","b8bce840":"code","5fda7a2a":"code","0afc8ab9":"code","15d2ce72":"code","f6041d2c":"code","509bb420":"code","a338304e":"code","df9c5964":"code","fb2c332f":"code","4781ed2e":"code","87c3890c":"code","cb0891c2":"code","7cdc38c7":"code","4569a9bf":"code","e5d53eec":"code","6543e8ea":"code","2ef52770":"code","2bc72159":"code","a198a6d4":"code","5b486914":"code","77416d4b":"code","514b8442":"code","8ddad254":"code","85fe473a":"code","839f5cdf":"code","dbd139a8":"code","812d73b8":"code","24baceaa":"code","7928450a":"code","72553a51":"code","ae1a3c61":"code","fa472196":"code","385f813f":"code","2290e7f2":"code","bbfee98e":"code","c17868b4":"code","b4051343":"code","3f8319a0":"markdown","33e61600":"markdown","e80f2257":"markdown","b3b6738b":"markdown","7c5774fb":"markdown","fb342abe":"markdown","fdcc08d7":"markdown","3bb3cec0":"markdown","921419db":"markdown","c6beccee":"markdown","8aad6b15":"markdown","158a5102":"markdown","dd11dc7c":"markdown","08b0fbce":"markdown","f9f14c77":"markdown","b1d8c0ce":"markdown","14ba942b":"markdown"},"source":{"6947d62c":"import numpy as np \nimport pandas as pd\nimport matplotlib.pylab as plt\nimport os\nfrom os import listdir\nfrom os.path import isfile, join","a8c4fb4d":"# Resized images directory\ndir_2019_images = \"\/kaggle\/input\/resizedsiimisic\/train_resized\/\"\nimages_2019 = [f for f in listdir(dir_2019_images) if isfile(join(dir_2019_images, f))]\n\n# CSV file\ntrain_df = pd.read_csv('\/kaggle\/input\/resizedsiimisic\/train.csv')","3e9e3cc1":"train_df.head()","6c8c43ce":"print(\"Train shape:\", train_df.shape)","1bceb23d":"train_df.target.value_counts().rename_axis('Tipo').reset_index(name='Total de muestras')","b0272afb":"# Number of samples of majority class to delete\nN = train_df.target.value_counts()[0] - train_df.target.value_counts()[1]\n\ntrain_df = train_df.drop(train_df[train_df['target'].eq(0)].sample(N).index)","7de0b4dd":"train_df.target.value_counts().rename_axis('Tipo').reset_index(name='Total de muestras')","d85bc306":"train_df.head()","67796d69":"from collections import Counter\nfrom sklearn.model_selection import train_test_split\n\nX = train_df\ny = train_df['target']","1ad5ba15":"# Split into train, validation test and calibration sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, \n                                                    test_size=0.2, \n                                                    stratify=y,\n                                                    random_state=42)\n\nX_train, X_val, y_train, y_val = train_test_split(X_train, y_train, \n                                                  test_size=0.2,\n                                                  stratify=y_train,\n                                                  random_state=42)\n\nX_test, X_calib, y_test, y_calib = train_test_split(X_test, y_test, \n                                                    test_size=0.4,\n                                                    stratify=y_test,\n                                                    random_state=42)\n\n\nprint(\"Conjunto de train:\", X_train.shape)\nprint(\"Conjunto de validacion:\", X_val.shape)\nprint(\"Conjunto de prueba:\", X_test.shape)\nprint(\"Conjunto de calibracion:\", X_calib.shape)\nprint(\"-----------------------\")\nprint('Distribucion de train ->', Counter(y_train))\nprint('Distribucion de validacion ->', Counter(y_val))\nprint(\"Distribucion de prueba:\", Counter(y_test))\nprint(\"Distribucion de calibracion:\", Counter(y_calib))","229c0a75":"X_train.to_csv('\/kaggle\/working\/train.csv', index=False)\nX_val.to_csv('\/kaggle\/working\/val.csv', index=False)\nX_calib.to_csv('\/kaggle\/working\/calib.csv', index=False)\nX_test.to_csv('\/kaggle\/working\/test.csv', index=False)","ca9332fe":"X_train[\"target\"] = X_train['target'].astype(str)\nX_val[\"target\"] = X_val['target'].astype(str)\nX_calib[\"target\"] = X_calib['target'].astype(str)\nX_test[\"target\"] = X_test['target'].astype(str)","57c47bc8":"import tensorflow as tf\nfrom tensorflow.keras.layers.experimental import preprocessing\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.applications import DenseNet121\nfrom tensorflow.keras.layers import Input, Dense, GlobalAveragePooling2D\nfrom tensorflow.keras.metrics import TruePositives, FalsePositives, TrueNegatives, FalseNegatives, AUC\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.losses import BinaryCrossentropy\nfrom tensorflow.keras.callbacks import ModelCheckpoint\nfrom tensorflow.keras.models import Model","ef32d647":"datagen = ImageDataGenerator(rescale=1.\/255.)\n\ntrain_generator = datagen.flow_from_dataframe(\n    dataframe=X_train,\n    directory=dir_2019_images,\n    x_col=\"image_name\",\n    y_col=\"target\",\n    batch_size=32,\n    seed=42,\n    shuffle=True,\n    class_mode=\"binary\"\n)\n\nvalid_generator = datagen.flow_from_dataframe(\n    dataframe=X_val,\n    directory=dir_2019_images,\n    x_col=\"image_name\",\n    y_col=\"target\",\n    batch_size=32,\n    seed=42,\n    shuffle=True,\n    class_mode=\"binary\"\n)\n\ncalib_generator=datagen.flow_from_dataframe(\n    dataframe=X_calib,\n    directory=dir_2019_images,\n    x_col=\"image_name\",\n    y_col=\"target\",\n    batch_size=32,\n    seed=42,\n    shuffle=True,\n    class_mode=\"binary\"\n)\n\ntest_generator = datagen.flow_from_dataframe(\n    dataframe=X_test,\n    directory=dir_2019_images,\n    x_col=\"image_name\",\n    y_col=\"target\",\n    batch_size=32,\n    seed=42,\n    shuffle=False,\n    class_mode=\"binary\"\n)\n\nSTEP_SIZE_TRAIN = train_generator.n\/\/train_generator.batch_size\nSTEP_SIZE_VALID = valid_generator.n\/\/valid_generator.batch_size\nSTEP_SIZE_CALIB=calib_generator.n\/\/calib_generator.batch_size\nSTEP_SIZE_TEST = test_generator.n\/\/test_generator.batch_size","46197e14":"encoder = DenseNet121(input_shape=(None,None,3), \n                      include_top=False, \n                      weights='imagenet')","12594709":"inputs = Input(shape=(None, None, 3))\nx = encoder(inputs, training=False)\nx = GlobalAveragePooling2D()(x)\npredictions = Dense(1, activation='sigmoid')(x)\nmodel_undersampling = Model(inputs=inputs, outputs=predictions)","23caafc3":"model_undersampling.summary()","57a5997d":"METRICS = [\n      TruePositives(name='tp'),\n      FalsePositives(name='fp'),\n      TrueNegatives(name='tn'),\n      FalseNegatives(name='fn'),\n      AUC(name='auc')\n]","284f507a":"checkpoint_filepath = '\/kaggle\/working\/undersampling_model.h5'\nmodel_checkpoint_callback = ModelCheckpoint(filepath=checkpoint_filepath,\n                                            monitor='val_auc',\n                                            mode='max',\n                                            verbose=1,\n                                            save_best_only=True)","b15486fb":"model_undersampling.compile(\n    optimizer=Adam(),\n    loss=BinaryCrossentropy(),\n    metrics=METRICS\n)","84c4ee8a":"history = model_undersampling.fit(train_generator,  \n                                  validation_data =valid_generator,\n                                  steps_per_epoch=STEP_SIZE_TRAIN, \n                                  validation_steps=STEP_SIZE_VALID,\n                                  callbacks=[model_checkpoint_callback],\n                                  epochs = 100)","545a70df":"plt.plot(history.history['auc'], \n         label='Training AUC (area = {:.3f})'.format(history.history['auc'][-1]))\nplt.plot(history.history['val_auc'], \n         label='Validation AUC (area = {:.3f})'.format(history.history['val_auc'][-1]))\nplt.title('Model undersampling')\nplt.ylabel('AUC')\nplt.xlabel('epoch')\nplt.grid()\nplt.legend(loc='best')\n\nplt.show()","a71f7f56":"plt.plot(history.history['loss'], label='Training loss (loss = {:.3f})'.format(history.history['loss'][-1]))\nplt.plot(history.history['val_loss'], label='Validation loss (loss = {:.3f})'.format(history.history['val_loss'][-1]))\nplt.title('Model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.grid()\nplt.legend(loc='best')\n\nplt.show()","283d38ac":"eval_metrics = model_undersampling.evaluate(test_generator,\n                                            steps=STEP_SIZE_TEST,\n                                            return_dict=True,\n                                            use_multiprocessing=False,\n                                            verbose=1)","c0055103":"true_labels = test_generator.classes\npredict = model_undersampling.predict(test_generator, \n                                      verbose=1)","4de74fc8":"from sklearn import metrics\nimport scikitplot as skplt\n\nfpr, tpr, tr = metrics.roc_curve(true_labels,predict)\nauc = metrics.roc_auc_score(true_labels, predict)\nplt.plot(fpr,tpr,'b',label=\"AUC=\"+str(auc))\nplt.plot([0,1],[0,1],'k--')\nplt.title('Test evaluation')\nplt.grid()\nplt.legend(loc='best')\nplt.show()","8c68310f":"import seaborn as sns\n\ncm = [eval_metrics['tn'],eval_metrics['fp'],eval_metrics['fn'],eval_metrics['tp']]\n\ngroup_names = ['True Neg','False Pos','False Neg','True Pos']\ngroup_counts = [\"{0:0.0f}\".format(value) for value in cm]\ngroup_percentages = [\"{0:.2%}\".format(value) for value in cm\/np.sum(cm)]\nlabels = [f\"{v1}\\n{v2}\\n{v3}\" for v1, v2, v3 in zip(group_names,group_counts,group_percentages)]\nlabels = np.asarray(labels).reshape(2,2)\n\nsns.heatmap([[cm[0], cm[1]], [cm[2], cm[3]]], annot=labels, fmt='', cmap='Blues')","2591d441":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.isotonic import IsotonicRegression\nfrom sklearn.metrics import brier_score_loss, log_loss\n!pip install ml_insights\nimport ml_insights as mli\n!pip install betacal\nfrom betacal import BetaCalibration","a73002b0":"true_labels_test = y_test\npredict_undersampling_test = model_undersampling.predict(test_generator, verbose=1)\n\ntrue_labels_calib = y_calib\npredict_undersampling_calib = model_undersampling.predict(calib_generator, verbose=1)","fd6142eb":"print('Calibracion Brier Score:', brier_score_loss(true_labels_calib, predict_undersampling_calib))\nprint(\"-----------------------\")\nprint('Test Brier Score:', brier_score_loss(true_labels_test, predict_undersampling_test))","b7248034":"def plot_reliability_diagram(true_labels, predict_labels):\n    plt.figure(figsize=(15,5))\n    rd = mli.plot_reliability_diagram(true_labels, predict_labels, show_histogram=True)\n    return rd","00bb3b28":"rd = plot_reliability_diagram(np.array(true_labels_calib), predict_undersampling_calib.ravel())\nplt.title('Reliability Diagram on Calibration Data')","05983bf4":"rd = plot_reliability_diagram(np.array(true_labels_test),predict_undersampling_test.ravel())\nplt.title('Reliability Diagram on Test Data')","047a0a99":"# Fit Platt scaling (logistic calibration)\nlr = LogisticRegression(C=99999999999, solver='lbfgs')\nlr.fit(predict_undersampling_calib.reshape(-1,1), np.array(true_labels_calib))","ac340c70":"calibset_platt_probs = lr.predict_proba(predict_undersampling_calib.reshape(-1,1))[:,1]\ntestset_platt_probs = lr.predict_proba(predict_undersampling_test.reshape(-1,1))[:,1]","87535b0e":"iso = IsotonicRegression(out_of_bounds = 'clip')\niso.fit(predict_undersampling_calib.ravel(), np.array(true_labels_calib))","724bec4a":"calibset_iso_probs = iso.predict(predict_undersampling_calib.ravel())\ntestset_iso_probs = iso.predict(predict_undersampling_test.ravel())","bea54f0e":"# Fit three-parameter beta calibration\nbc = BetaCalibration()\nbc.fit(predict_undersampling_calib.ravel(), np.array(true_labels_calib))","0fc18846":"calibset_bc_probs = bc.predict(predict_undersampling_calib.ravel())\ntestset_bc_probs = bc.predict(predict_undersampling_test.ravel())","a6bddc91":"# Define SplineCalib object\nsplinecalib = mli.SplineCalib()\nsplinecalib.fit(predict_undersampling_calib.ravel(), np.array(true_labels_calib))","e7840b6c":"calibset_splinecalib_probs = splinecalib.predict(predict_undersampling_calib.ravel())\ntestset_splinecalib_probs = splinecalib.predict(predict_undersampling_test.ravel())","027e3065":"mli.plot_reliability_diagram(np.array(true_labels_test), predict_undersampling_test.ravel())\nplt.title('Reliability Diagram on Test Data\\n before Platt Calibration')","077d20df":"mli.plot_reliability_diagram(np.array(true_labels_test), testset_platt_probs)\nplt.title('Reliability Diagram on Test Data\\n after Platt Calibration')","b8bce840":"mli.plot_reliability_diagram(np.array(true_labels_calib), predict_undersampling_calib.ravel())\n#tvec = np.linspace(.01, .99, 99)\n#plt.plot(tvec, iso.predict(tvec), label='Isotonic')\nplt.title('Isotonic Calibration Curve on Calibration Data')","5fda7a2a":"mli.plot_reliability_diagram(np.array(true_labels_test), predict_undersampling_test.ravel())\n#tvec = np.linspace(.01, .99, 99)\n#plt.plot(tvec, iso.predict(tvec), label='Isotonic')\nplt.title('Isotonic Calibration Curve on Test Data')","0afc8ab9":"mli.plot_reliability_diagram(np.array(true_labels_test), testset_iso_probs)\nplt.title('Reliability Diagram on Test Data\\n after Isotonic Calibration')","15d2ce72":"mli.plot_reliability_diagram(np.array(true_labels_calib), predict_undersampling_calib.ravel())\n#tvec = np.linspace(.01, .99, 99)\n#plt.plot(tvec, bc.predict(tvec))\nplt.title('Beta Calibration Curve on Calibration Set')","f6041d2c":"mli.plot_reliability_diagram(np.array(true_labels_test), predict_undersampling_test.ravel())\n#tvec = np.linspace(.01, .99, 99)\n#plt.plot(tvec, bc.predict(tvec))\nplt.title('Beta Calibration Curve on Test Set')","509bb420":"mli.plot_reliability_diagram(np.array(true_labels_test), testset_bc_probs)\nplt.title('Reliability Diagram on Test Data\\n after Beta Calibration')","a338304e":"mli.plot_reliability_diagram(np.array(true_labels_calib), predict_undersampling_calib.ravel())\n#tvec = np.linspace(.01, .99, 99)\n#plt.plot(tvec, splinecalib.predict(tvec))\nplt.title('SplineCalib Calibration Curve on Calibration Set')","df9c5964":"mli.plot_reliability_diagram(np.array(true_labels_test), predict_undersampling_test.ravel())\n#tvec = np.linspace(.01, .99, 99)\n#plt.plot(tvec, splinecalib.predict(tvec))\nplt.title('SplineCalib Calibration Curve on Test Set')","fb2c332f":"mli.plot_reliability_diagram(np.array(true_labels_test), testset_splinecalib_probs)\nplt.title('Reliability Diagram on Test Data\\n after SplineCalib Calibration')","4781ed2e":"print('Uncalibrated log_loss = {}'.format(log_loss(true_labels_test, predict_undersampling_test.ravel())))\nprint('Platt calibrated log_loss = {}'.format(log_loss(true_labels_test, testset_platt_probs)))\nprint('Isotonic calibrated log_loss = {}'.format(log_loss(true_labels_test, testset_iso_probs)))\nprint('Beta calibrated log_loss = {}'.format(log_loss(true_labels_test, testset_bc_probs)))\nprint('Spline calibrated log_loss = {}'.format(log_loss(true_labels_test, testset_splinecalib_probs)))","87c3890c":"print('Uncalibrated Brier Score = {}'.format(brier_score_loss(true_labels_test, predict_undersampling_test.ravel())))\nprint('Platt calibrated Brier Score = {}'.format(brier_score_loss(true_labels_test, testset_platt_probs)))\nprint('Isotonic calibrated Brier Score = {}'.format(brier_score_loss(true_labels_test, testset_iso_probs)))\nprint('Beta calibrated Brier Score = {}'.format(brier_score_loss(true_labels_test, testset_bc_probs)))\nprint('Spline calibrated Brier Score = {}'.format(brier_score_loss(true_labels_test, testset_splinecalib_probs)))","cb0891c2":"train_df = pd.read_csv('\/kaggle\/input\/resizedsiimisic\/train.csv')\n\n# Unique values in each features\nfor k in train_df.keys():\n    print('{0}: {1}'.format(k, len(train_df[k].unique())))","7cdc38c7":"def for_age(row):\n    try:\n        age = float(row['age_approx'])\n        if age < 18:\n            return 'teen'\n        elif age < 30:\n            return 'twenties'\n        elif age < 40:\n            return 'thirties'\n        elif age < 50:\n            return 'forties'\n        elif age < 60:\n            return 'fifties'\n        elif age < 70:\n            return 'sixties'\n        else:\n            return 'old'\n    except ValueError:\n        return np.nan\n    \ndef for_boolean(row, col):\n    try:\n        val = int(row[col])\n        if row[col] >= 1:\n            return \"True\"\n        else:\n            return \"False\"\n    except ValueError:\n        return \"False\"","4569a9bf":"def pretreat(df):\n    if 'target' in df.columns:\n        df['target'] = df.apply(lambda row: for_boolean(row, 'target'), axis=1)\n    df['age_approx'] = df.apply(for_age, axis=1)\n    return df\n\ntrain_df = pretreat(train_df)","e5d53eec":"train_df.head()","6543e8ea":"!pip install hnet","2ef52770":"from hnet import hnet\n\n# Initialize with default settings\nhn = hnet()\n\n# Initialize and set user specific parameters\nhn = hnet(alpha=0.05, y_min=10, perc_min_num=0.8, multtest='holm', dtypes='pandas')\ndf = train_df","2bc72159":"# Example to Black list the underneath variables\nhn = hnet(black_list=['image_name'])","a198a6d4":"from scipy.stats import hypergeom\n\n# Compute Association between survived and Female\nN = df.shape[0] # Total number of samples\nK = sum(df['target']==1) # Number of success in the population\nn = sum(df['sex']=='female') # Sample size\/number of draws\nx = sum((df['sex']=='female') & (df['target']==1))\n\nprint(x-1, N, n, K)\n# 232 891 314 342\n\n# Compute P\nP = hypergeom.sf(x-1, N, n, K)\n\nprint(P)\n# 3.5925132664684234e-60","5b486914":"# Learn the relationships in the dataframe\nresults = hn.association_learning(df)","77416d4b":"print(results)","514b8442":"# Make static network plot\nG1 = hn.plot()\n\n# Make static heatmap\nG2 = hn.heatmap()\n\n# Make interactive network plot\nG3 = hn.d3graph()\n\n# Make interactive heatmap\nG4 = hn.d3heatmap()","8ddad254":"# Plot summarized heatmap\nhn.heatmap(summarize=True, cluster=True)\nhn.d3heatmap(summarize=True)\n\n# Plot summarized static graph\nhn.plot(summarize=True)\nhn.d3graph(summarize=True, charge=1000)","85fe473a":"# Plot feature importance\nhn.plot_feat_importance(marker_size=50)","839f5cdf":"!pip install pyAgrum\n!pip install pydotplus","dbd139a8":"train_df = train_df.rename(columns={\"anatom_site_general_challenge\": \"location\"})","812d73b8":"import pyAgrum as gum\nimport pyAgrum.lib.notebook as gnb\nfrom pyAgrum.lib.bn2roc import showROC_PR\n%matplotlib inline \n\nbn = gum.BayesNet(\"Melanoma\")\nbn =gum.fastBN(\"sex{female|male};age{teen|twenties|thirties|forties|fifties|sixties|old}<-target{False|True}->location{anterior torso|head\/neck|lateral torso|lower extremity|oral\/genital|palms\/soles|posterior torso|upper extremity}->sex\")\nprint(bn.variable(\"target\"))\nprint(bn.variable(\"age\"))\nprint(bn.variable(\"sex\"))\nprint(bn.variable(\"location\"))\n\nbn","24baceaa":"train_df.groupby(['target']).size()\/len(train_df)","7928450a":"# Benign malignant prevalence\nbn.cpt('target')[:] = [0.806673, 0.193327]\nbn.cpt('target')","72553a51":"train_df.groupby(['age_approx', 'target']).size()\/len(train_df)","ae1a3c61":"bn.cpt('age')[{'target':0}] = [0.020685, 0.041326, 0.101957, 0.159742, 0.147153, 0.129226, 0.206584]\nbn.cpt('age')[{'target':1}] = [0.000311, 0.002802, 0.012055, 0.027847, 0.037144, 0.038835, 0.074333]\nbn.cpt('age')","fa472196":"train_df.groupby(['sex', 'target']).size()\/len(train_df)","385f813f":"bn.cpt('sex')[{'target':0}] = [0.381406, 0.425267]\nbn.cpt('sex')[{'target':1}] = [0.087011, 0.106317]\nbn.cpt('sex')","2290e7f2":"train_df.groupby(['location', 'target']).size()\/len(train_df)","bbfee98e":"bn.cpt('location')[{'target':0}] = [0.245285, 0.164591, 0.001379, 0.185543, 0.001779, 0.008763, 0.102936, 0.096397]\nbn.cpt('location')[{'target':1}] = [0.058808, 0.038968, 0.000623, 0.035009, 0.000845, 0.008719, 0.018772,0.031584]\nbn.cpt('location')","c17868b4":"gnb.showInference(bn, size=\"7\")","b4051343":"# Features of a row\ngnb.showPosterior(bn, evs={'age':'old', 'sex':'male', 'location':'anterior torso'}, target='target')","3f8319a0":"#### Se obtiene las predicciones y las etiquetas del conjunto de prueba.","33e61600":"## <font color=red>1. <\/font>Cargar las im\u00e1genes y los datos tabulares","e80f2257":"## <font color=red>6. <\/font>Red bayesiana","b3b6738b":"#### M\u00e9todo 2: Isotonic Regression","7c5774fb":"## <font color=red>3. <\/font>Crear y entrenar el modelo","fb342abe":"#### M\u00e9todo 4: SplineCalib","fdcc08d7":"## <font color=red>2. <\/font>Submuestreo de las im\u00e1genes mayoritarias","3bb3cec0":"## <font color=red>5. <\/font>Calibraci\u00f3n del modelo","921419db":"#### M\u00e9todo 1: Platt Scaling","c6beccee":"#### M\u00e9todo 3: Beta Calibration","8aad6b15":"# Modelo de submuestreo","158a5102":"#### Comparaci\u00f3n de los m\u00e9todos de calibraci\u00f3n","dd11dc7c":"basado en:\nhttps:\/\/towardsdatascience.com\/explore-and-understand-your-data-with-a-network-of-significant-associations-9a03cf79d254","08b0fbce":"#### A continuaci\u00f3n, se crea la red bayesiana","f9f14c77":"#### Finalmente, se observa la curva ROC-AUC y la matriz de confusi\u00f3n.","b1d8c0ce":"## <font color=red>4. <\/font>Evaluar el modelo","14ba942b":"#### M\u00e9tricas"}}