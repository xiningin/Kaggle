{"cell_type":{"42d840c4":"code","0215ecb1":"code","b519e5ea":"code","9079c60f":"code","0465acd6":"code","47928924":"code","57227ab9":"code","c76ae549":"code","b61d4829":"code","3179890a":"code","5a0e6dad":"code","3bda66c5":"code","ed896ccc":"code","dc810d00":"code","90e403f4":"code","2c995baa":"code","51df2f33":"code","53e34fc4":"code","0b9a1584":"code","4160f6e1":"code","e51aa77e":"code","b4e07783":"code","d84d4ae5":"code","d834f791":"code","8295a0da":"code","d7bea8cf":"code","da78575f":"code","5eba9495":"code","f304dcd2":"code","249be5e7":"code","46296f92":"code","b757c51d":"code","e0cb961b":"code","120b044d":"code","80077093":"code","342d6ea7":"code","b3050ec0":"code","5282efab":"code","19166e37":"code","1f1f0b90":"code","2bf0b17a":"code","09ffbc94":"code","aa74a40c":"code","6fb09317":"code","6930aa50":"code","113315e0":"code","e2e8fadc":"code","fbc5c0a8":"code","1f126068":"code","2e21be42":"code","c7a56eeb":"code","f5c7c8cb":"code","c4649fbf":"code","e030a617":"code","2e7d9aa0":"code","78237c42":"code","7467c348":"code","9361a8b4":"code","136365f2":"code","8900f258":"code","c4c906d3":"code","6c47ac92":"code","4d45a631":"code","e950f80d":"code","05702edf":"code","787f5fe2":"code","7198b0af":"code","cc9a0da8":"code","2159fecf":"code","fd28e83f":"code","50b9d1aa":"code","a401b1d7":"code","31dc7fef":"code","10c1424c":"code","ee04564d":"code","c43d03d2":"code","e328c32c":"code","8020d24f":"code","3c96276f":"code","9b873467":"code","8a013228":"code","72389db3":"code","0516074b":"code","1600512a":"code","cc67ada2":"code","5598be47":"code","a20fa2b4":"code","ae38b84b":"code","2b90e97a":"code","9634c551":"code","842ad6eb":"code","f3b506f3":"code","68d0b62a":"code","ffdf6274":"code","04b5fcf2":"code","424b2984":"code","5c0ffb05":"code","548a8866":"code","14c58ecb":"markdown","13162127":"markdown","2f0130bc":"markdown","d1e707e0":"markdown","0bbb5eea":"markdown","ce5dca48":"markdown","c26e06a2":"markdown","a172b8b6":"markdown","9f80ec97":"markdown","4bc4bf78":"markdown","9672a8a7":"markdown","59017c77":"markdown","2e12c4dd":"markdown","606f8a5c":"markdown","2dd611ec":"markdown","834cd2bd":"markdown"},"source":{"42d840c4":"%reset -sf","0215ecb1":"# notebook hyperparameters\nTEST_SET_SIZE = 1000\nRANKED_LIST_SIZE = 100\nRANDOM_STATE = 42\nEVALUATING = True  # make False if you want to run query quickly","b519e5ea":"import os, collections, random, itertools, functools, time, json\n\nfrom collections import defaultdict, Counter\nfrom math import log\n\nimport tqdm.notebook as tqdm\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nrandom.seed(RANDOM_STATE)\nnp.random.seed(RANDOM_STATE)","9079c60f":"# load data\ndf = pd.read_csv(\"\/kaggle\/input\/quora-question-pairs\/train.csv.zip\")\ndf[\"question1\"] = df[\"question1\"].astype(str)  # resolve nan\ndf[\"question2\"] = df[\"question2\"].astype(str)\ndf[\"qid1\"] -= 1  #  start index from zero\ndf[\"qid2\"] -= 1","0465acd6":"df.sample(10)","47928924":"# all questions are identified with its qid\nqid_to_question = {}\nfor qid1, qid2, question1, question2 in zip(df[\"qid1\"], df[\"qid2\"], df[\"question1\"], df[\"question2\"]):\n    qid_to_question[qid1] = question1\n    qid_to_question[qid2] = question2","57227ab9":"# extract 1000 questions for testing\ntest_query_qids = set()\n\ndf_duplicate = df[df[\"is_duplicate\"] == 1].sample(frac=1, random_state=RANDOM_STATE)\nfor qid1, qid2, is_duplicate in zip(df_duplicate[\"qid1\"], df_duplicate[\"qid2\"], df_duplicate[\"is_duplicate\"]):\n    if is_duplicate and qid1 not in test_query_qids and len(test_query_qids) < TEST_SET_SIZE:\n        test_query_qids.add(qid2)\n    if qid1 in test_query_qids and qid2 in test_query_qids:\n        # to guarantee that there is a duplicate question in the training set\n        test_query_qids.remove(qid1)\n        test_query_qids.remove(qid2)\nassert len(test_query_qids) == TEST_SET_SIZE  # if fail, change random_state\n\ntest_query_qids_list = sorted(test_query_qids)\ntrain_query_qids_list = sorted(set(qid_to_question.keys()) - test_query_qids)\nassert test_query_qids_list[:3] == [331, 489, 501]   # to check random state fixed","c76ae549":"# # uncomment this to test only limited queries\nif not EVALUATING:\n    test_query_qids_list = test_query_qids_list[:10]\n    TEST_SET_SIZE = 10","b61d4829":"# extract duplicate relationship of training set\n\nqid_to_duplicate_qids = defaultdict(set)\nqid_to_nonduplicate_qids = defaultdict(set)\n\nfor qid1, qid2, is_duplicate in zip(df[\"qid1\"], df[\"qid2\"], df[\"is_duplicate\"]):\n    if not (qid1 in test_query_qids or qid2 in test_query_qids):\n        if is_duplicate:\n            qid_to_duplicate_qids[qid1].add(qid2)\n            qid_to_duplicate_qids[qid2].add(qid1)\n        else:\n            qid_to_nonduplicate_qids[qid1].add(qid2)\n            qid_to_nonduplicate_qids[qid2].add(qid1)","3179890a":"# complete graph of duplicate relationships\n\nqid_to_duplicate_qids_complete = defaultdict(set)\nqid_to_qid_group_leader = {}\nqid_group_leader_to_duplicate_qid_group = defaultdict(set)\n\nvisited_qids = set()\nfor train_qid in train_query_qids_list:\n    if train_qid in visited_qids:\n        continue\n    current_qids_group = set([train_qid])\n    qid_to_qid_group_leader[train_qid] = train_qid\n    stack = [train_qid]\n    \n    while stack:\n        cur_qid = stack.pop()\n        for nex_qid in qid_to_duplicate_qids[cur_qid]:\n            if nex_qid in current_qids_group:\n                continue\n            qid_to_qid_group_leader[nex_qid] = train_qid\n            stack.append(nex_qid)\n            current_qids_group.add(nex_qid)\n\n    # complete the graph\n    for qid1, qid2 in itertools.combinations(current_qids_group, r=2):\n        qid_to_duplicate_qids_complete[qid1].add(qid2)\n        qid_to_duplicate_qids_complete[qid2].add(qid1)\n    qid_group_leader_to_duplicate_qid_group[train_qid] = current_qids_group\n    visited_qids.update(current_qids_group)","5a0e6dad":"# extract duplicate relationship of the test set\n\ntest_qid_to_duplicate_qids = defaultdict(set)\ntest_qid_to_duplicate_qids_complete = defaultdict(set)\n\nfor qid1, qid2, is_duplicate in zip(df_duplicate[\"qid1\"], df_duplicate[\"qid2\"], df_duplicate[\"is_duplicate\"]):\n    if qid2 in test_query_qids:\n        qid1, qid2 = qid2, qid1\n    if qid1 in test_query_qids:\n        if qid2 in test_query_qids:\n            continue\n        test_qid_to_duplicate_qids[qid1].add(qid2)\n        test_qid_to_duplicate_qids_complete[qid1].add(qid2)\n        for train_qid in qid_group_leader_to_duplicate_qid_group[qid_to_qid_group_leader[qid2]]:\n            test_qid_to_duplicate_qids_complete[qid1].add(train_qid)","3bda66c5":"# count inconsistencies in dataset\n\ncnt = 0\nfor qid1, qid2, is_duplicate in zip(df[\"qid1\"], df[\"qid2\"], df[\"is_duplicate\"]):\n    if not is_duplicate and qid1 not in test_query_qids and qid2 not in test_query_qids:\n        if qid_to_qid_group_leader[qid1] == qid_to_qid_group_leader[qid2]:\n            cnt += 1\nprint(\"Number of inconsistencies: \", cnt)  # slightly smaller than 96 because some edges are associated with the test set","ed896ccc":"test_mask = (df[\"qid1\"].isin(test_query_qids)) | (df[\"qid2\"].isin(test_query_qids))\ntrain_df = df[~test_mask].copy()\ntest_df = df[test_mask].copy()","dc810d00":"# clean up\ndel qid_to_qid_group_leader, qid_group_leader_to_duplicate_qid_group\ndel cnt\ndel test_query_qids   # not sorted, use test_query_qids_list\ndel df                # all data you can train on is in train_df\n\n# enable use of complete graphs\ntest_qid_to_duplicate_qids = test_qid_to_duplicate_qids_complete\nqid_to_duplicate_qids = qid_to_duplicate_qids_complete","90e403f4":"def method_random_guess(test_qid):\n    # returns ranklist and scores of each size RANKED_LIST_SIZE\n    return random.choices(train_query_qids_list, k=RANKED_LIST_SIZE), [0]*RANKED_LIST_SIZE\n\n# 1000 x 100 (the ranked list of similar qn for each of the 1000 test qns)\nranklists_method_random_guess = [method_random_guess(test_qid)[0] for test_qid in test_query_qids_list]","2c995baa":"def show_sample_query_results(test_qid, method_ranklist, method_scores=[0]*RANKED_LIST_SIZE, num_to_show=10):\n    # not a metric, just print a few examples and its scores\n    print(\"Query: {}\".format(qid_to_question[test_qid]))\n    for rank, (score, result_qid) in enumerate(zip(method_scores, method_ranklist[:num_to_show]), start=1):\n        relevance = \"Registered\" if result_qid in test_qid_to_duplicate_qids[test_qid] else \"Unregistered\"\n        print(\"Rank {} - Score {:.4f} - {}:  \\t{}\".format(rank, score, relevance, qid_to_question[result_qid]))","51df2f33":"show_sample_query_results(test_query_qids_list[0], *method_random_guess(test_query_qids_list[0]))","53e34fc4":"def evaluation_with_first_relevant_rank(method_ranklists, considered=1, eps=10**-6, debug=True, **kwargs):\n    # calculation of the statistics of the rank of the first c=considered duplicates\n    # if the duplicate does not appear in the ranklist, it has a default rank of RANKED_LIST_SIZE\n    assert np.array(method_ranklists).shape == (TEST_SET_SIZE, RANKED_LIST_SIZE)\n    reciprocal_ranks = []\n    ranks = []\n    for test_qid, ranklist in zip(test_query_qids_list, method_ranklists):\n        test_qid_to_rank = {result_qid:rank for rank, result_qid in enumerate(ranklist, start=1)}\n        rank = []  # may be shorter than `considered` because of lack of duplicates\n        for expected_qid in test_qid_to_duplicate_qids[test_qid]:\n            if expected_qid in test_qid_to_rank:\n                rank.append(test_qid_to_rank[expected_qid])\n            else:\n                rank.append(RANKED_LIST_SIZE+1)\n        rank.sort()\n        ranks.extend(rank[:considered])\n        if rank[0] > RANKED_LIST_SIZE:\n            reciprocal_ranks.append(0)\n        else:\n            reciprocal_ranks.append(1\/rank[0])\n    \n    plt.figure(figsize=(14,4))\n    plt.title(\"Highest rank of duplicate question\")\n    plt.hist(ranks, bins=np.arange(RANKED_LIST_SIZE+2))\n    plt.xlabel(\"Rank\")\n    plt.ylabel(\"Frequency\")\n    plt.show()\n    \n    mrr = sum(reciprocal_ranks)\/len(reciprocal_ranks)\n    har = 1\/(mrr+eps)\n    print(f\"Mean Reciprocal Rank (MRR) is {mrr:.2f}\")\n    print(f\"Harmonic Average Rank (HAR) is {har:.2f}\")    \n    \n    p50 = np.median(ranks)\n    proportion_out_of_result = ranks.count(RANKED_LIST_SIZE+1)\/len(ranks)\n    if debug:\n        print(\"Median rank: {:.2f}\".format(p50))\n        print(\"Proportion out of result: {:.3f}\".format(proportion_out_of_result))\n    \n    return mrr, har, p50, proportion_out_of_result","0b9a1584":"_ = evaluation_with_first_relevant_rank(ranklists_method_random_guess)","4160f6e1":"def evaluation_with_auc(method_ranklists, k=10, weights=None, debug=True, **kwargs):\n    assert np.array(method_ranklists).shape == (TEST_SET_SIZE, RANKED_LIST_SIZE)\n    \n    counts = np.array([0.]*k)\n    ## Identify duplicates among top K ranks for each test\n    for i, (test_qid, ranklist) in enumerate(zip(test_query_qids_list, method_ranklists)):\n        topk = ranklist[:k]\n        is_duplicate = np.array([1 if (result_qid in test_qid_to_duplicate_qids[test_qid]) else 0 for result_qid in topk])\n        counts += is_duplicate \n    \n    ## Calculate AUC\n    if weights:\n        counts *= np.array(weights)\/sum(weights)\n    else:\n        counts \/= k\n    \n    auc = sum(counts)\/(TEST_SET_SIZE)\n    \n    if debug:\n        print(f\"{auc:.2%} of top {k} results are duplicates\")\n\n    return auc # between [0,1], 1 is perfect","e51aa77e":"_ = evaluation_with_auc(ranklists_method_random_guess)\n_ = evaluation_with_auc(ranklists_method_random_guess, weights = [10,9,8,7,6,5,4,3,2,1])","b4e07783":"def single_r_precision(test_qid, ranklist):\n    # use this to check a single test query\n    num_duplicate = len(test_qid_to_duplicate_qids[test_qid]) # this dict needs to be updated when train:test set separation is updated\n    if num_duplicate == 0:\n        return 0, 0, 0\n    top_r = ranklist[:num_duplicate]\n    num_duplicates_in_top_r = sum([1 if (result_qid in test_qid_to_duplicate_qids[test_qid]) else 0 for result_qid in top_r])\n    r_precision = num_duplicates_in_top_r\/num_duplicate\n    return num_duplicate, num_duplicates_in_top_r, r_precision\n\n\ndef evaluation_with_r_precision(method_ranklists, k=10, report_k=0, debug=True, **kwargs):\n    print(np.array(method_ranklists).shape)\n    assert np.array(method_ranklists).shape == (TEST_SET_SIZE, RANKED_LIST_SIZE) # method_ranklists size is (1000,100)\n    \n    total_num_duplicates = np.array([0 for i in range(TEST_SET_SIZE)])\n    r_precision = np.array([0 for i in range(TEST_SET_SIZE)])\n    \n    ## Iter over 1->1000 tests\n    for i, (test_qid, ranklist) in enumerate(zip(test_query_qids_list, method_ranklists)): # iter over 1->1000 tests\n        total_num_duplicates[i], num_duplicates_in_top_r, r_precision[i] = single_r_precision(test_qid, ranklist)\n    \n    # note: if want do error analysis, intervene here to find test cases with low r precision\n    if report_k > 0:\n        k_lowest_r_precision_idx = np.argpartition(r_precision, k)[:k]\n        k_lowest_r_precision_test_qids = np.array(test_query_qids_list)[k_lowest_r_precision_idx]\n\n    ## Calculate metrics\n    avg_r_precision = r_precision.mean()\n    weighted_avg_r_precision = np.multiply(r_precision, total_num_duplicates).sum() \/ total_num_duplicates.sum()\n    \n    if debug:\n        print(f\"Average R-Precision = {avg_r_precision:.2%}\")\n        print(f\"Weighted Average R-Precision by proportion of duplicates = {weighted_avg_r_precision:.2%}\") \n        if avg_r_precision > weighted_avg_r_precision:\n            print(\"A higher average R-Precisions suggests that there are many test queries with high R-Precision but there are some test queries with high number of duplicates that model is not effective with.\")\n    \n    if not report_k: return avg_r_precision, weighted_avg_r_precision\n    else:\n        return avg_r_precision, weighted_avg_r_precision, k_lowest_r_precision_test_qids","d84d4ae5":"_ = evaluation_with_r_precision(ranklists_method_random_guess, k=10)","d834f791":"def evaluation_with_precision_recall_at_k(method_ranklists, k=10, exclude_precision=False, exclude_recall=False, debug=True, **kwargs):\n    assert np.array(method_ranklists).shape == (TEST_SET_SIZE, RANKED_LIST_SIZE)\n    ## Evaluation returns the macro average P@K and R@Kfor test set\n    ## Interpretation P@K: what % of top k retrieved is relevant?\n    ## Interpretation R@K: what % of all duplicates for query is retrieved within top k?\n    \n    ## Iter thru each test\n    precisions_at_k = []\n    recalls_at_k = []\n    for i, (test_qid, ranklist) in enumerate(zip(test_query_qids_list, method_ranklists)):\n        ## 1. Set rank threshold K, ignore all docs after K\n        ## 2. Count num_relevant in top-K\n        ## 3. Count total_num_duplicates_for_query\n        ## 4. P@K = num_relevant\/k\n        ## 5. R@K = num_relevant\/total_num_duplicates_for_query\n        topk = ranklist[:k]\n        num_relevant = sum([1 if (result_qid in test_qid_to_duplicate_qids[test_qid]) else 0 for result_qid in topk])\n        \n        precision_at_k = num_relevant\/k\n        precisions_at_k.append(precision_at_k)\n        \n        total_num_duplicates_for_query = len(test_qid_to_duplicate_qids[test_qid])\n        recall_at_k = num_relevant\/total_num_duplicates_for_query\n        recalls_at_k.append(recall_at_k)\n    \n    mean_precision_at_k = sum(precisions_at_k)\/len(precisions_at_k) # macro average\n    mean_recall_at_k = sum(recalls_at_k)\/len(recalls_at_k) # macro average\n    print(f\"Macro Average Precision@k={k} is {mean_precision_at_k:.2%}\")\n    print(f\"Macro Average Recall@k={k} is {mean_recall_at_k:.2%}\")\n    return (mean_precision_at_k, mean_recall_at_k)\n\n_ = evaluation_with_precision_recall_at_k(ranklists_method_random_guess, k=10)","8295a0da":"def evaluation_with_map(method_ranklists, debug=True, **kwargs):\n    assert np.array(method_ranklists).shape == (TEST_SET_SIZE, RANKED_LIST_SIZE)\n    ## Interpretation: what is the average precision for all relevant docs across all queries?\n\n    ## Iter thru each test\n    average_precisions = []\n    for i, (test_qid, ranklist) in enumerate(zip(test_query_qids_list, method_ranklists)):\n        ## 1. Find the rank positions of each of the R relevant docs: K1, K2, ... KR and sort \n        ## 2. Compute P@K for each K1, K2, ... If K >=RANKED_LIST_SIZE, assume never retrieved\n        ## 3. AP = average of P@K for query\n        ## 4. MAP = macro average of AP across queries\n\n        ## 1. Find the rank positions of each of the R relevant docs: K1, K2, ... and sort \n        dup_qids_in_train_set = [dup_qid for dup_qid in test_qid_to_duplicate_qids[test_qid] if dup_qid in train_query_qids_list] # find all the dup_qid that can be found in the train set so you know total dup qn that could be found\n        total_num_dup_qid = len(dup_qids_in_train_set) # how many dup qn to expect\n\n        dup_ranks = []\n        for dup_qid in dup_qids_in_train_set:\n            if dup_qid not in ranklist: # not found\n                dup_ranks.append(RANKED_LIST_SIZE) # give \"out of range\" rank which would be checked later during calculation\n                continue\n            dup_ranks.append(list(ranklist).index(dup_qid)+1) # append the rank of the retrieved dup qn\n        \n        dup_ranks, dup_qids_in_train_set = (list(t) for t in zip(*sorted(zip(dup_ranks, dup_qids_in_train_set)))) # sort by rank\n        ## 2. Compute P@K for each K1, K2, ... If K >=RANKED_LIST_SIZE, assume never retrieved\n        precisions_at_k = []\n        for j, rank in enumerate(dup_ranks, start=1): # dup_ranks is sorted\n            if rank >= RANKED_LIST_SIZE: # handle \"unretrieved\" duplicates\n                precisions_at_k.append(0)\n            else: \n                precision_at_k = j \/ rank # = num_dup_so_far \/ rank_of_latest_dup_found\n                precisions_at_k.append(precision_at_k)\n        \n        ## 3. AP = average of P@K for query\n        average_precisions.append(sum(precisions_at_k)\/len(precisions_at_k))\n    \n    ## Out of test query loop\n    ## 4. MAP = macro average of AP across queries\n    MAP = sum(average_precisions)\/len(average_precisions)\n    print(f\"Mean Average Precision (MAP) is {MAP:.2%}\")\n    return MAP\n\n_ = evaluation_with_map(ranklists_method_random_guess)","d7bea8cf":"def evaluation_process(method, test_query_qids_list=test_query_qids_list, \n                       calculate_metrics=True, use_tqdm=True, **kwargs):\n    # executes the method and runs the evaluation functions \n    ranklists, scorelists = [], []\n    \n    iterator = tqdm.tqdm if use_tqdm else iter\n        \n    for test_qid in iterator(test_query_qids_list):\n        ranklist, scores = method(test_qid)\n        ranklists.append(ranklist)\n        scorelists.append(scores)\n    \n    if calculate_metrics:\n        evaluation_with_first_relevant_rank(ranklists, **kwargs)\n        # evaluation_with_auc(ranklists, **kwargs)\n        evaluation_with_r_precision(ranklists, **kwargs)\n\n        evaluation_with_precision_recall_at_k(ranklists, k=10, **kwargs)\n        evaluation_with_map(ranklists, **kwargs)\n\n    return ranklists, scorelists","da78575f":"results_random_guess = evaluation_process(method_random_guess)","5eba9495":"## This entire cell is important to enable tokeniser pipeline \n## Use this to replace tokenise function if using Tokenise then Spellcheck (TSC) pipeline\n\n######### spacy basic tokenizer\nimport spacy\nprint(\"Spacy version: \", spacy.__version__)\nfrom spacy.tokenizer import Tokenizer  # https:\/\/spacy.io\/api\/tokenizer\n\n# !python3 -m spacy download en_core_web_sm\nprint(\"Loading Spacy en_core_web_sm loaded\")\nnlp = spacy.load(\"en_core_web_sm\")\ntokenizer = Tokenizer(nlp.vocab)\ntokenizer.add_special_case(\"[math]\", [{\"ORTH\": \"[math]\"}]) # see qid=7: '[math]23^{24}[\/math]' becomes one token\n# add more special cases here if found","f304dcd2":"def spacy_tokenise(text, lower=False, split_last_punc=True):\n    \"\"\"\n    returns a list of tokens given a question text\n    note: each punctuation is also considered a token\n    note: \"\\n\" is a token\n    note: \"'s\" is a token\n    note: '(Koh-i-Noor)' is a token\n    see tokenizer instantiation code for special cases or to add\n    lowercase text only after spell check\n    \"\"\"\n    if lower: text = text.lower()\n    tokens = tokenizer(text)\n    token_list = [token.text for token in tokens]\n\n    # further split tokens that end with certain punct e.g. \"me?\" => \"me\", \"?\"\n    if split_last_punc: \n        split_lists = [[token[:-1], token[-1]] if (token[-1] in [\"!\",\"?\",\",\",\":\"]) else [token] for token in token_list]\n        token_list = [token for sublist in split_lists for token in sublist]\n    return token_list\n\n######### symspell spellchecker\nprint(\"Loading symspell\")\n!pip install symspellpy\nfrom symspellpy.symspellpy import SymSpell, Verbosity  # https:\/\/github.com\/mammothb\/symspellpy\nimport pkg_resources\n\n# instantiate spellchecker\nsym = SymSpell(max_dictionary_edit_distance=2, prefix_length=7, count_threshold=1)\n# https:\/\/symspellpy.readthedocs.io\/en\/latest\/api\/symspellpy.html\ndictionary_path = pkg_resources.resource_filename(\"symspellpy\", \"frequency_dictionary_en_82_765.txt\")\nsym.load_dictionary(dictionary_path, 0, 1) # might take a short while\n\ndef spellcheck_single(word):\n    # returns top correct spelling or the same word if no correction found within max_edit_distance\n    if not word.isascii(): return word # do not spellcheck non ascii words e.g. \u30b7\n\n    # obtain list of suggestions\n    suggestions = sym.lookup(word, Verbosity.CLOSEST, max_edit_distance=2,\n        include_unknown=True, # a mispelled word with no found corrections is returned as is\n        ignore_token=r\"[:,.!?\\\\-]\" # use if want to avoid correcting certain phrases\n        )\n    # get the term from the suggestItem object\n    suggested_words = [suggestion._term for suggestion in suggestions]\n    \n    # check if the input word is legit and return if so else return corrected word\n    word_lower = word.lower()\n    if word_lower in suggested_words: return word_lower # do not correct if input is a legit word\n    else: return suggested_words[0] # top suggestion\n\ndef spellcheck_compound(sent):\n    # spellchecks a sentence\n    suggestions = sym.lookup_compound(sent, max_edit_distance=2)\n    return suggestions[0]._term # returns the top suggestion\n\n######### tokenise pipeline\ndef tokenise_then_spellcheck(sent):\n    # 8 times faster than spellcheck_then_tokenise\n    tokens = spacy_tokenise(sent) # NOTE: replace tokenise with spacy_tokenise\n    checked_tokens = [spellcheck_single(token).lower() for token in tokens] # lower after spell check\n    return checked_tokens\n\ndef spellcheck_then_tokenise(sent):\n    checked_sent = spellcheck_compound(sent)\n    tokens = spacy_tokenise(checked_sent, lower=True) # lower after spell check\n    return tokens","249be5e7":"# define tokenisation process\n\nimport pickle\nqid_to_tokens_preprocessed_filename = \"..\/input\/quora-question-pairs-tokenise-pipeline\/qid_to_processed_token_list_tokenise_then_spellcheck.pkl\"\nwith open(qid_to_tokens_preprocessed_filename, \"rb\") as f:\n    qid_to_tokens_preprocessed = pickle.load(f)\n\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\n\nstopword_set = set(stopwords.words())\nstopword_set.update([\"?\", \",\"])\n\ndef nltk_tokenize(sentence):\n    return word_tokenize(sentence.lower())\n\ndef tokenise_qid(qid, qid_to_tokens_preprocessed=qid_to_tokens_preprocessed, \n                 tokenise_method=tokenise_then_spellcheck):\n    # return a list of tokens, does not remove stopwords or duplicates\n    if qid_to_tokens_preprocessed and qid in qid_to_tokens_preprocessed:\n        return qid_to_tokens_preprocessed[qid]\n    return tokenise_method(qid_to_question[qid])","46296f92":"def preprocess_vsm(train_query_qids_list=train_query_qids_list, stopword_set=stopword_set, exclude_stopwords=True):\n    '''\n    Input:\n        qid_to_question = {qid: question string}\n            Note: only use the test subset of qids\n    \n    Outputs:\n        qid_to_tokens = {qid: set(tokens)}\n        token_to_qids = {token: set(qids)}\n        tf = {token: {qid: TF as int}}\n        df = {token: DF as int}\n        L = {qid: question length as int}\n    '''\n    qid_to_tokens = defaultdict(set)\n    token_to_qids = defaultdict(set)\n    tf = defaultdict(Counter)\n    df = defaultdict(int)\n    L = defaultdict(int)\n\n    qid_processed = set()\n    for qid in tqdm.tqdm(train_query_qids_list):\n        qid_tokenised = tokenise_qid(qid)\n\n        for token in set(qid_tokenised):\n            if token not in stopword_set or not exclude_stopwords:\n                # store qid-to-token mapping\n                qid_to_tokens[qid].add(token)\n                token_to_qids[token].add(qid)\n\n                # compute and store term frequency\n                tf[token][qid] += 1 \n\n                # store doc frequency in df\n                df[token] += 1\n\n        # store doc length in L (double-count repeated tokens)\n        L[qid] = len(qid_tokenised)\n        \n    # output\n    return qid_to_tokens, token_to_qids, tf, df, L","b757c51d":"qid_to_tokens, token_to_qids, tf, df, L = preprocess_vsm()\n\n# save a copy of the original to allow reset later\nqid_to_tokens_original, token_to_qids_original = qid_to_tokens.copy(), token_to_qids.copy()\ntf_original, df_original, L_original = tf.copy(), df.copy(), L.copy()","e0cb961b":"def method_overlapping_root_word_count(query_qid, ignore_stopwords=True):\n    query_tokens = set(tokenise_qid(query_qid))\n    if ignore_stopwords:\n        query_tokens = [token for token in query_tokens if token not in stopword_set]\n    counter = collections.Counter()\n    \n    for dummy_qid in random.choices(train_query_qids_list, k=RANKED_LIST_SIZE):\n        # prefill with random results to address the possibility of no matches\n        counter[dummy_qid] = 0.01\n    \n    for query_token in query_tokens:\n        counter += collections.Counter(token_to_qids[query_token])\n    \n    query_results = list(counter.items())\n    random.shuffle(query_results)  # so that qids are not ordered\n    query_results = sorted(query_results, key=lambda x:x[1], reverse=True)[:RANKED_LIST_SIZE]\n\n    return [x[0] for x in query_results], [x[1] for x in query_results]","120b044d":"show_sample_query_results(test_query_qids_list[0], *method_overlapping_root_word_count(test_query_qids_list[0]))","80077093":"results_overlapping_root_word_count = evaluation_process(method_overlapping_root_word_count)","342d6ea7":"def compute_idf(doc_freq, N):\n    '''\n    Inputs:\n        doc_freq = document frequency of some token\n        N = corpus size including query\n    \n    Output:\n        idf = IDF as float\n    '''\n    return log(N\/doc_freq)","b3050ec0":"from functools import reduce\nimport operator\n\ndef prod(iterable):\n    return reduce(operator.mul, iterable, 1)\n\n\ndef use_vsm(qid_query, \\\n    # qid_to_tokens=qid_to_tokens, tf=tf, df=df, L=L,\n    method='tf-idf', compute_idf=compute_idf,\n    k1=1.5, k3=1.5, b=0.75,\n    smoothing='add-one', alpha=0.75, eps=10**(-6),\n    exclude_stopwords=True,\n    return_top=RANKED_LIST_SIZE):\n    \n    '''\n    Inputs:\n        qid_query = qid of question match   # this comes from \"test\" set\n        qid_to_tokens = {qid: set(tokens)}  # this is the \"training\" corpus\n        tf = {token: term freq}             # required for all methods\n        df = {token: doc freq}              # required for method='tf-idf','bm25'\n        L = {qid: doc length}               # required for method='bm25','unigram'\n\n        method = model to apply\n        k1, k3, b = tuning params           # required for method='bm25'\n        smoothing = type of smoothing       # required for method='unigram'\n        return_top = num of docs to return\n    \n    Procedure:\n        0. Corpus is already tokenised, tf, df, L already computed\n        1. Tokenise query, expand tf, df, L with query information\n        \n        if method='boolean':\n            Remove idf calculation, then use method='tf-idf'\n\n        if method='tf-idf':\n            2. Compute tf-idf weights only for relevant (t,d) pairs\n            3. Compute cosine similarity only for docs containing query terms\n        \n        if method='bm25':\n            2. Compute RSV summation terms only for relevant (t,d) pairs\n            3. Compute RSV only for docs containing query terms\n        \n        if method='unigram':\n            2. Compute probabilities only for relevant (t,d) pairs\n            3. Compute query probability only for docs containing query terms\n        \n        4. Return docs in ranked order\n\n    Output:\n        ranking = [qids in decreasing order of match]\n        scoring = [corresponding scores]\n    '''\n    \n    assert method in ['boolean','tf-idf','bm25','unigram'], \"Supported methods: 'boolean', 'tf-idf', 'bm25', 'unigram'\"\n    assert len(L.keys()) > 0 if method=='bm25' else True, \"Please include L for bm25\"\n    assert len(L.keys()) > 0 if method=='unigram' else True, \"Please include L for unigram\"\n    assert smoothing in ['add-one','linear-interpolation'] if method=='unigram' else True\n    assert alpha >= 0 and alpha <= 1 if smoothing=='linear-interpolation' else True\n\n    qid_tmp = time.time()\n\n    ''' STEP 1: PROCESS QUERY '''\n    query_tokenised = tokenise_qid(qid_query)\n    \n    for token in set(query_tokenised):\n        if token not in stopword_set or not exclude_stopwords:\n            # store qid-to-token mapping\n            # store query as qid=0 (corpus starts from qid=1)\n            qid_to_tokens[qid_tmp].add(token)\n\n            # compute and store term frequency\n            tf[token][qid_tmp] = sum([1 if t==token else 0 for t in query_tokenised])\n            \n            # update doc frequency in df\n            df[token] += 1\n    \n    # store query length\n    L[qid_tmp] = len(query_tokenised)\n\n    if method=='boolean':\n        def compute_idf(doc_freq, N):\n            return 1\n        method = 'tf-idf'\n        \n    if method=='tf-idf':\n        \n        ''' STEP 2: COMPUTE TF-IDF WEIGHTS '''\n        weights = defaultdict(lambda: defaultdict(float))\n        N = len(qid_to_tokens) # original corpus + query\n\n        # only bother computing for tokens in the query\n        for token in set(query_tokenised):\n            if token not in stopword_set or exclude_stopwords==False:\n                weights[qid_tmp][token] = tf[token][qid_tmp] * compute_idf(df[token], N)\n                \n                for qid in tf[token].keys():\n                    weights[qid][token] = tf[token][qid] * compute_idf(df[token], N)\n                    \n                    # also compute weight for other tokens contained by these qids\n                    # needed for computing qid vector length\n                    for other_token in qid_to_tokens[qid]:\n                        weights[qid][other_token] = tf[other_token][qid] * compute_idf(df[other_token], N)\n\n                        \n        ''' STEP 3: COMPUTE COSINE SIMILARITY TO QUERY '''\n        cosine_similarities = defaultdict(float)\n        # compute denominator (part 1), i.e., |q| * |d|\n        query_vector_length = (sum([w**2 for w in weights[qid_tmp].values()]))**0.5\n        if query_vector_length == 0:\n            print(f\"query={qid_query}\\nweights[qid_tmp].items()={weights[qid_tmp].items()}\")\n        \n        for qid in weights.keys():\n            \n            # compute numerator, i.e., dot product of q and d\n            cosine_numerator = 0\n            \n            for token in weights[qid].keys():\n                if token in weights[qid_tmp]:\n                    cosine_numerator += weights[qid][token] * weights[qid_tmp][token]\n            \n            # compute denominator (part 2), i.e., |q| * |d|\n            qid_vector_length = (sum([w**2 for w in weights[qid].values()]))**0.5\n            if qid_vector_length == 0: # example: qid=25026 => question='?'\n                qid_vector_length = 1e-8\n\n            # compute and store cosine similarity between q and d\n            cosine_similarities[qid] = cosine_numerator \/ (query_vector_length+eps) \/ (qid_vector_length+eps)\n        \n        scores = cosine_similarities\n\n    if method=='bm25':\n\n        ''' STEP 2: COMPUTE RSV TERMS '''\n        rsv_terms = defaultdict(lambda: defaultdict(float))\n        N = len(qid_to_tokens) # original corpus + query\n        L_avg = sum(L.values())\/len(L.values())\n\n        # only bother computing for tokens in the query\n        for token in set(query_tokenised):\n            for qid in tf[token].keys():\n                rsv_terms[qid][token] = compute_idf(df[token], N) \\\n                    * (k1+1)*tf[token][qid] \/ (k1*((1-b)+b*L[qid]*L_avg) + tf[token][qid]) \\\n                        * (k3+1)*tf[token][qid_tmp] \/ (k3 + tf[token][qid_tmp])\n\n        ''' STEP 3: COMPUTE RSV '''\n        rsv = {qid: sum(rsv_terms[qid].values()) for qid in rsv_terms.keys()}\n        scores = rsv\n    \n    if method=='unigram':\n        \n        ''' STEP 2: COMPUTE PROBABILITIES '''\n        probabilities = defaultdict(lambda: defaultdict(float))\n        corpus_model = defaultdict(float)\n        \n        # only bother computing for tokens in the query\n        for token in set(query_tokenised):\n            for qid in tf[token].keys():\n\n                if smoothing=='add-one':\n                    probabilities[qid][token] = (tf[token][qid]+1) \/ (L[qid]+len(query_tokenised))\n                else:\n                    probabilities[qid][token] = (tf[token][qid]) \/ (L[qid])\n\n                # for linear-interpolation smoothing, build corpus language model\n                if smoothing=='linear-interpolation':\n                    corpus_model[token] += tf[token][qid]\n\n        # remaining operations for linear-interpolation smoothing        \n        if smoothing=='linear-interpolation':\n            # finish building corpus language model by dividing corpus tf by corpus L\n            total_corpus_length = sum(L.values())\n            for token in corpus_model.keys():\n                corpus_model[token] = corpus_model[token] \/ total_corpus_length\n            \n            # then update the probabilities\n            for qid in probabilities.keys():\n                for token in probabilities[qid].keys():\n                    probabilities[qid][token] = alpha*probabilities[qid][token] + (1-alpha)*corpus_model[token]\n\n        ''' STEP 3: COMPUTE QUERY PROBABILITY '''\n        query_prob = {qid: -log(prod(probabilities[qid].values())) for qid in probabilities.keys()}\n        scores = query_prob\n\n    ''' STEP 4: RANK DOCUMENTS AND RETURN RESULT '''\n    # cleanup\n    if qid_tmp in qid_to_tokens:\n        del qid_to_tokens[qid_tmp]\n    for token in set(query_tokenised):\n        if token not in stopword_set or not exclude_stopwords:\n            del tf[token][qid_tmp]\n            df[token] -= 1\n    \n    if qid_tmp in scores:\n        del scores[qid_tmp] # remove query from result\n    ranking = sorted(scores, key=scores.get, reverse=True)\n    scoring = sorted(scores.values(), reverse=True)\n\n    # if too few documents match the query, add dummy documents\n    if len(ranking) < return_top:\n        ranking.extend([0]*(return_top-len(ranking)))\n        scoring.extend([0]*(return_top-len(ranking)))\n\n    # return top k results\n    return ranking[:return_top], scoring[:return_top]","5282efab":"def method_boolean(qid):\n    return use_vsm(qid, method='boolean')","19166e37":"show_sample_query_results(test_query_qids_list[0], *method_boolean(test_query_qids_list[0]))","1f1f0b90":"results_boolean = evaluation_process(method_boolean)","2bf0b17a":"def method_tf_idf(qid):\n    return use_vsm(qid, method='tf-idf')","09ffbc94":"show_sample_query_results(test_query_qids_list[0], *method_tf_idf(test_query_qids_list[0]))","aa74a40c":"results_tf_idf = evaluation_process(method_tf_idf)","6fb09317":"def method_bm25(qid):\n    return use_vsm(qid, method='bm25')","6930aa50":"show_sample_query_results(test_query_qids_list[0], *method_bm25(test_query_qids_list[0]))","113315e0":"results_bm25 = evaluation_process(method_bm25)","e2e8fadc":"def method_unigram(qid):\n    return use_vsm(qid, method='unigram', smoothing='add-one')","fbc5c0a8":"show_sample_query_results(test_query_qids_list[0], *method_unigram(test_query_qids_list[0]))","1f126068":"results_unigram = evaluation_process(method_unigram)","2e21be42":"for alpha in [0, 0.25, 0.5, 0.75, 1.0]:\n    def method_unigram(qid):\n        return use_vsm(qid, method='unigram', smoothing='linear-interpolation', alpha=alpha)\n    _ = evaluation_process(method_unigram)","c7a56eeb":"def to_vec(token_or_list):\n    # converts a token string or a list of tokens into a word or doc vec respectively\n    if type(token_or_list) == list:\n        # token list needs to be joined into a sentence first\n        token_or_list = ' '.join(token_or_list)\n    return nlp(token_or_list).vector\n\nnlp2 = spacy.load(\"en_core_web_lg\")\ndef to_vec2(token_or_list):\n    # converts a token string or a list of tokens into a word or doc vec respectively\n    if type(token_or_list) == list:\n        # token list needs to be joined into a sentence first\n        token_or_list = ' '.join(token_or_list)\n    return nlp2(token_or_list).vector","f5c7c8cb":"# Load pre-processed dict\nwith open(\"..\/input\/quora-question-pairs-tokenise-pipeline\/qid_to_vec.pkl\", \"rb\") as f:\n    qid_to_vec = pickle.load(f)\n\nprint(\"Pre-processed question vector is of shape {}\".format(qid_to_vec[0].shape))","c4649fbf":"from numpy import dot\nfrom numpy.linalg import norm\n\ndef method_spacy_embedding_similarity(test_qid):\n    tokens = tokenise_then_spellcheck(qid_to_question[test_qid])\n    test_vec = to_vec(tokens)\n    \n    ## Run baseline model as a filter\n    qid_list, scores = method_overlapping_root_word_count(test_qid)\n    \n    cos_sims = [] # bigger better\n    for train_qid in qid_list:# train_query_qids_list:\n        train_vec = qid_to_vec[train_qid]\n        cos_sim = dot(test_vec, train_vec)\/(norm(test_vec)*norm(train_vec))\n        cos_sims.append(cos_sim)\n\n    cos_sims = np.array(cos_sims)\n    qid_list = np.array(qid_list) # train_query_qids_list\n    inds = cos_sims.argsort()[::-1] # reverse so biggest come first\n    cos_sims = cos_sims[inds]\n    ranklist = qid_list[inds] \n\n    return ranklist[:RANKED_LIST_SIZE], cos_sims[:RANKED_LIST_SIZE]","e030a617":"show_sample_query_results(test_query_qids_list[0], *method_spacy_embedding_similarity(test_query_qids_list[0]))","2e7d9aa0":"results_spacy_embedding_similarity = evaluation_process(method_spacy_embedding_similarity)","78237c42":"with open(\"..\/input\/quora-question-pairs-tokenise-pipeline\/qid_to_vec_trf.pkl\", \"rb\") as f: # note, actually lg not trf\n    qid_to_vec2 = pickle.load(f)\n\nprint(\"Pre-processed question vector is of shape {}\".format(qid_to_vec2[0].shape)) # 300 dim vec","7467c348":"def method_spacy_embedding_similarity_lg(test_qid):\n    tokens = tokenise_then_spellcheck(qid_to_question[test_qid])\n    test_vec = to_vec2(tokens)\n    \n    ## Run baseline model as a filter\n    qid_list, scores = method_overlapping_root_word_count(test_qid)\n    \n    cos_sims = [] # bigger better\n    for train_qid in qid_list:# train_query_qids_list:\n        train_vec = qid_to_vec2[train_qid]\n        cos_sim = dot(test_vec, train_vec)\/(norm(test_vec)*norm(train_vec))\n        cos_sims.append(cos_sim)\n\n    cos_sims = np.array(cos_sims)\n    qid_list = np.array(qid_list) # train_query_qids_list)\n    inds = cos_sims.argsort()[::-1] # reverse so biggest come first\n    cos_sims = cos_sims[inds]\n    ranklist = qid_list[inds] \n\n    return ranklist[:RANKED_LIST_SIZE], cos_sims[:RANKED_LIST_SIZE]","9361a8b4":"show_sample_query_results(test_query_qids_list[0], *method_spacy_embedding_similarity_lg(test_query_qids_list[0]))","136365f2":"results_spacy_embedding_similarity_lg = evaluation_process(method_spacy_embedding_similarity_lg)","8900f258":"import gensim\nimport gensim.downloader\n# gensim.downloader.info() # find more models to download\n\nfrom gensim.models import KeyedVectors\n\ntry: model = KeyedVectors.load(\"..\/input\/ir-project-download-keyed-vectors\/glove-wiki-gigaword-50.keyedvectors\")\nexcept: # gs_model not downloaded\n    model = gensim.downloader.load('glove-wiki-gigaword-50')\n    # model.save(\"\/kaggle\/working\/glove-wiki-gigaword-50.keyedvectors\") # if not already saved","c4c906d3":"def method_wordmover_distance(test_qid, model):\n    # out of box duplicate finder does not work!\n    # returns ranklist and scores of each size RANKED_LIST_SIZE\n    \n    ## Run baseline model as a filter\n    ranklist, scores = method_overlapping_root_word_count(test_qid)\n    \n    ## Process test question\n    test_qn = tokenise_qid(test_qid)\n    \n    ## Get wordmover distance from every candidate\n    distances = []\n    qid_list = ranklist\n    for candidate_qid in qid_list:\n        candidate_qn = tokenise_qid(candidate_qid)\n        distances.append(1-model.wmdistance(test_qn, candidate_qn))\n    \n    ## Sort by distance\n    sorted_dist_and_candidate_qid = sorted(zip(distances,qid_list))[::-1]\n    sorted_candidate_qid = [qid for _,qid in sorted_dist_and_candidate_qid]\n    sorted_dist = [dist for dist,_ in sorted_dist_and_candidate_qid]\n    return sorted_candidate_qid[:RANKED_LIST_SIZE], sorted_dist[:RANKED_LIST_SIZE]","6c47ac92":"def method_wordmover_distance_glovewiki50(test_qid):\n    return method_wordmover_distance(test_qid, model)","4d45a631":"show_sample_query_results(test_query_qids_list[0], *method_wordmover_distance_glovewiki50(test_query_qids_list[0]))","e950f80d":"results_wordmover_distance_glovewiki50 = evaluation_process(method_wordmover_distance_glovewiki50)","05702edf":"models_to_try = ['glove-wiki-gigaword-300', 'glove-twitter-50','word2vec-google-news-300','fasttext-wiki-news-subwords-300']\n\nif not EVALUATING:\n    models_to_try = []\n\nfor m in models_to_try:\n    print(\"Model: \",m)\n    try:\n        model = KeyedVectors.load(f\"..\/input\/ir-project-download-keyed-vectors\/{m}.keyedvectors\")\n    except:\n        model = gensim.downloader.load(m)\n\n    def method_wordmover_distance_new_model(test_qid):\n        return method_wordmover_distance(test_qid, model)\n\n    show_sample_query_results(test_query_qids_list[0], *method_wordmover_distance_new_model(test_query_qids_list[0]))\n\n    _ = evaluation_process(method_wordmover_distance_new_model)","787f5fe2":"!pip install sentence-transformers > \/dev\/null","7198b0af":"from sentence_transformers import SentenceTransformer\nmodel_name = 'bert-base-nli-stsb-mean-tokens'\nmodel_tf = SentenceTransformer(model_name)","cc9a0da8":"model_name = \"bert-base-nli-stsb-mean-tokens\"\nsentence_vectors = np.load(f\"..\/input\/quora-question-pairs-bert-sentence-vectors\/sentence_vectors_{model_name}.npy\")\nsentence_vectors = {i:vec for i,vec in enumerate(sentence_vectors)}","2159fecf":"from scipy.spatial.distance import cosine\n\ndef method_sentence_vector(query_qid, method_preliminary=method_overlapping_root_word_count, preliminary_factor=1):\n    # method_preliminary can be either of the previous methods\n    # recommended method_overlapping_root_word_count, method_boolean, method_tf_idf\n\n    qid_list, preliminary_scores = method_preliminary(query_qid)\n    \n    # sort by cosine similarity\n    query_sentence_vector = sentence_vectors[query_qid]\n    query_results = [(qid, preliminary_factor*preliminary_score+1-abs(cosine(query_sentence_vector, sentence_vectors[qid])))\n                     for qid,preliminary_score in zip(qid_list,preliminary_scores)]\n    query_results = sorted(query_results, key=lambda x:x[1], reverse=True)[:RANKED_LIST_SIZE]\n    \n    return [x[0] for x in query_results], [x[1] for x in query_results]","fd28e83f":"show_sample_query_results(test_query_qids_list[0], *method_sentence_vector(test_query_qids_list[0], preliminary_factor=0))","50b9d1aa":"show_sample_query_results(test_query_qids_list[0], *method_sentence_vector(test_query_qids_list[0], preliminary_factor=1))","a401b1d7":"results_sentence_vector = evaluation_process(method_sentence_vector)","31dc7fef":"SUPERVISED_MODEL_TRAINING_SET_SIZE = 10000\nLOAD_DATA_FOR_SUPERVISED = False\nDIR_DATA_FOR_SUPERVISED = \".\/\"\n\nsupervised_query_qids = random.sample(set(qid_to_duplicate_qids.keys()) - set(test_query_qids_list), \n                                      SUPERVISED_MODEL_TRAINING_SET_SIZE)\n\ndef create_supervised_features(qids, testing=True):\n    \n    kwargs = {\"test_query_qids_list\": qids, \"calculate_metrics\": False, \"use_tqdm\": False}\n    method_to_ranklists_scorelists_supervised = {\n        \"overlapping_root_word_count\": evaluation_process(method_overlapping_root_word_count, **kwargs),\n        \"boolean\": evaluation_process(method_boolean, **kwargs),\n        \"tf_idf\": evaluation_process(method_tf_idf, **kwargs),\n        \"bm25\": evaluation_process(method_bm25, **kwargs),\n        \"unigram\": evaluation_process(method_unigram, **kwargs),\n        \"spacy_embedding_similarity\": evaluation_process(method_spacy_embedding_similarity, **kwargs),\n        \"spacy_embedding_similarity_lg\": evaluation_process(method_spacy_embedding_similarity_lg, **kwargs),\n        \"wordmover_distance_glovewiki50\": evaluation_process(method_wordmover_distance_glovewiki50, **kwargs),\n        \"sentence_vector\": evaluation_process(method_sentence_vector, **kwargs),\n    }\n    return method_to_ranklists_scorelists_supervised\n\ndef parse_ndarray(obj):  # https:\/\/stackoverflow.com\/a\/52604722\/5894029\n    if isinstance(obj, np.ndarray):\n        return obj.tolist()\n\nif not LOAD_DATA_FOR_SUPERVISED:\n    method_to_ranklists_scorelists_supervised = create_supervised_features(supervised_query_qids)\n\n    with open(DIR_DATA_FOR_SUPERVISED+'supervised_query_qids.json', 'w') as f:\n        json.dump(supervised_query_qids, f, indent=4, default=parse_ndarray)    \n\n    with open(DIR_DATA_FOR_SUPERVISED+'method_to_ranklists_scorelists_supervised.json', 'w') as f:\n        json.dump(method_to_ranklists_scorelists_supervised, f, indent=4, default=parse_ndarray)\n\nwith open(DIR_DATA_FOR_SUPERVISED+'supervised_query_qids.json') as f:\n    supervised_query_qids = json.load(f)\n\nwith open(DIR_DATA_FOR_SUPERVISED+'method_to_ranklists_scorelists_supervised.json') as f:\n    method_to_ranklists_scorelists_supervised = json.load(f)","10c1424c":"def parse_supervised_features_into_df(method_to_ranklists_scorelists_supervised, training=False, \n                                      supervised_query_qids_set=set(supervised_query_qids)):\n    supervised_scores = defaultdict(dict)\n    for method, (ranklists, scorelists) in method_to_ranklists_scorelists_supervised.items():\n        for supervised_query_qid, ranklist, scorelist in zip(supervised_query_qids, ranklists, scorelists):\n            for candidate_qid, score in zip(ranklist, scorelist):\n                if training and candidate_qid in supervised_query_qids_set:\n                    continue\n                supervised_scores[supervised_query_qid, candidate_qid][method] = score\n                \n    df_supervised = pd.DataFrame.from_dict(supervised_scores, orient='index')\n    return df_supervised\n\ndef extract_supervised_labels_from_df(df_supervised):\n    supervised_labels = [int(candidate_qid in qid_to_duplicate_qids[supervised_query_qid]) \n                         for supervised_query_qid, candidate_qid in df_supervised.index]\n    return supervised_labels\n\ndf_supervised = parse_supervised_features_into_df(method_to_ranklists_scorelists_supervised, training=True)\nsupervised_labels = extract_supervised_labels_from_df(df_supervised)\n\n# extracted and total number of positive labels\nsum(supervised_labels), sum(len(qid_to_duplicate_qids[supervised_query_qid]) for supervised_query_qid in supervised_query_qids)","ee04564d":"from sklearn.linear_model import LogisticRegression\nclf = LogisticRegression(random_state=0, class_weight='balanced').fit(np.nan_to_num(df_supervised.values), supervised_labels)\nfor coef, feature in zip(clf.coef_[0], df_supervised.columns):\n    print(\"{:.4f}\".format(coef), feature)","c43d03d2":"def method_supervised_model_logr(query_qid):\n    df_predict = parse_supervised_features_into_df(create_supervised_features([query_qid]))\n    scores = clf.predict_proba(np.nan_to_num(df_predict.values))[:,1]\n    candidate_qids = df_predict.reset_index()[\"level_1\"]  # resolve dataframe multi-index\n    results = sorted(list(zip(scores, candidate_qids)))[::-1]\n    return [x[1] for x in results][:RANKED_LIST_SIZE], [x[0] for x in results][:RANKED_LIST_SIZE]  # qid, scores","e328c32c":"show_sample_query_results(test_query_qids_list[0], *method_supervised_model_logr(test_query_qids_list[0]))","8020d24f":"results_supervised_model_logr = evaluation_process(method_supervised_model_logr)","3c96276f":"import lightgbm as lgb\n\ndf_train = df_supervised.copy()\ntarget_train = np.array(supervised_labels)\neval_set = np.array([True if i < len(df_train)*0.2 else False for i in range(len(df_train))])\nlgb_train = lgb.Dataset(df_train[~eval_set], target_train[~eval_set])\nlgb_eval = lgb.Dataset(df_train[eval_set], target_train[eval_set], reference=lgb_train)\nlgb_all = lgb.Dataset(df_train, target_train)","9b873467":"params = {\n#     'boosting_type': 'gbdt',\n    'objective': 'binary',\n    'monotone_constraints': [1]*len(df_supervised.columns),\n#     'scale_pos_weight': 0.360,\n#     'metric': {'auc'},\n#     'num_leaves': 15,\n#     'learning_rate': 0.05,\n#     'feature_fraction': 0.9,\n#     'bagging_fraction': 0.8,\n#     'bagging_freq': 5,\n    'verbose': -1,\n}\n\ngbm = lgb.train(params,\n                lgb_train,\n                num_boost_round=1000,\n                valid_sets=lgb_eval,\n                verbose_eval=-1,\n                early_stopping_rounds=10)\n\npd.DataFrame({\"feature\": df_train.columns, \"importance\": gbm.feature_importance(importance_type=\"gain\")})[:20]","8a013228":"def method_supervised_model_lgbm(query_qid):\n    df_predict = parse_supervised_features_into_df(create_supervised_features([query_qid]))\n    scores = gbm.predict(df_predict)\n    candidate_qids = df_predict.reset_index()[\"level_1\"]\n    results = sorted(list(zip(scores, candidate_qids)))[::-1]\n    return [x[1] for x in results][:RANKED_LIST_SIZE], [x[0] for x in results][:RANKED_LIST_SIZE]  # qid, scores","72389db3":"show_sample_query_results(test_query_qids_list[0], *method_supervised_model_lgbm(test_query_qids_list[0]))","0516074b":"results_supervised_model_lgbm = evaluation_process(method_supervised_model_lgbm)","1600512a":"method_to_ranklists_scorelists = {\n#     \"random_guess\": results_random_guess,\n    \"overlapping_root_word_count\": results_overlapping_root_word_count,\n    \"boolean\": results_boolean,\n    \"tf_idf\": results_tf_idf,\n    \"bm25\": results_bm25,\n    \"unigram\": results_unigram,\n    \"spacy_embedding_similarity\": results_spacy_embedding_similarity,\n    \"spacy_embedding_similarity_lg\": results_spacy_embedding_similarity_lg,\n    \"wordmover_distance_glovewiki50\": results_wordmover_distance_glovewiki50,\n    \"sentence_vector\": results_sentence_vector,\n    \"supervised_model_logr\": results_supervised_model_logr,\n    \"supervised_model_lgbm\": results_supervised_model_lgbm\n}\n\nimport json\n\ndef parse_ndarray(obj):  # https:\/\/stackoverflow.com\/a\/52604722\/5894029\n    if isinstance(obj, np.ndarray):\n        return obj.tolist()\n\nwith open('method_to_ranklists_scorelists.json', 'w') as f:\n    json.dump(method_to_ranklists_scorelists, f, indent=4, default=parse_ndarray)","cc67ada2":"QUESTIONS_TO_HANDEVAL = set(x-1 for x in [\n    332, 490, 1955, 6319, 9690, 17279, 19619, 20557, 26378, 33734, 38984, \n    49864, 57291, 89903, 116882, 126992, 131214, 144297, 159628, 201409, \n    273666, 284107, 286721, 312887, 318523, 378759, 384832, 405081, \n    405877, 423313, 464279, 480116, 533401])\nHANDEVAL_RANK_THRESHOLD = 10\n\nmap_qid_to_handeval = defaultdict(set)\n\nfor ranklists, scorelists in method_to_ranklists_scorelists.values():\n    for test_qid, ranklist in zip(test_query_qids_list, ranklists):\n        if test_qid in QUESTIONS_TO_HANDEVAL:\n            for candidate_qid in ranklist[:HANDEVAL_RANK_THRESHOLD]:\n                map_qid_to_handeval[test_qid].add(candidate_qid)\n            \nfor qid in map_qid_to_handeval:\n    map_qid_to_handeval[qid] = sorted(map_qid_to_handeval[qid])","5598be47":"dataframe_columns = [\"test_qid\", \"test_question\", \"candidate_qid\", \"candidate_question\"]\ndataframe_entries = []\nfor qid in sorted(map_qid_to_handeval.keys()):\n    for candidate_qid in map_qid_to_handeval[qid]:\n        line_entry = [qid, qid_to_question[qid], candidate_qid, qid_to_question[candidate_qid]]\n        dataframe_entries.append(line_entry)\n        \nrandom.shuffle(dataframe_entries)\ndataframe_entries = sorted(dataframe_entries, key = lambda x: x[0])","a20fa2b4":"df_handeval = pd.DataFrame(dataframe_entries, columns=dataframe_columns)\n# labeller columns\ndf_handeval[\"jh\"] = np.nan\ndf_handeval[\"hk\"] = np.nan\ndf_handeval[\"wt\"] = np.nan\n\ndf_handeval.to_csv(\"df_handeval.csv\", index=None)","ae38b84b":"df_handeval = pd.read_csv(\"..\/input\/quoraquestionpairhandannotateddataset\/df_handeval.csv\")\nwith open('..\/input\/quoraquestionpairhandannotateddataset\/method_to_ranklists_scorelists.json') as f:\n    method_to_ranklists_scorelists = json.load(f)","2b90e97a":"import math\n\ndef calculate_dcg_at_k(r, k, method=0):\n    if method == 0:\n        logn = [1.] + [1\/math.log(i,2) for i in range(2, k+1)]\n    else:\n        logn = [1\/math.log(i,2) for i in range(2, k+2)]\n    \n    dcg = 0.\n    for gain,disc in zip(r[:k], logn):\n        dcg += gain*disc\n    return dcg\n\ndef calculate_ndcg_at_k(scores, ref, k=10, method=0):\n    denom = calculate_dcg_at_k(ref, k, method=method)\n    numer = calculate_dcg_at_k(scores, k, method=method)\n    if denom == 0:\n        return 0.\n    return numer\/denom","9634c551":"test_qid_to_candidate_qid_to_scores = collections.defaultdict(dict)\n\nfor _,row in df_handeval.iterrows():\n    test_qid = row[\"test_qid\"]\n    candidate_qid = row[\"candidate_qid\"]\n    score = row[\"average\"]\n    test_qid_to_candidate_qid_to_scores[test_qid][candidate_qid] = score\n    \ntest_qid_to_ideal_scores = collections.defaultdict(list)\nfor test_qid, candidate_qid_to_scores in test_qid_to_candidate_qid_to_scores.items():\n    ideal_scores = sorted(candidate_qid_to_scores.values())[::-1]\n    test_qid_to_ideal_scores[test_qid] = ideal_scores\n\nmethod_to_ndcg_score = collections.defaultdict(list)\ncount_out_of_eval = 0\n\nfor method_name, (ranklists, _) in method_to_ranklists_scorelists.items():\n    for test_qid, ranklist in zip(test_query_qids_list, ranklists):\n        if test_qid in QUESTIONS_TO_HANDEVAL:\n            scores = []\n            for candidate_qid in ranklist[:HANDEVAL_RANK_THRESHOLD]:\n                if candidate_qid not in test_qid_to_candidate_qid_to_scores[test_qid]:\n                    scores.append(1)\n                    print(method_name, len(scores))\n                    count_out_of_eval += 1\n                else:\n                    scores.append(test_qid_to_candidate_qid_to_scores[test_qid][candidate_qid])\n            ref = test_qid_to_ideal_scores[test_qid]\n            ndcg_at_k = calculate_ndcg_at_k(scores, ref)\n            method_to_ndcg_score[method_name].append(ndcg_at_k)\n\ncount_out_of_eval","842ad6eb":"for method_name, scores in method_to_ndcg_score.items():\n    print(method_name)\n    print(sum(scores)\/len(scores))\n    print(\" \".join(f\"{x:.2f}\" for x in scores))\n    print()","f3b506f3":"def index_unseen_question(unseen_question_text_list):\n    unseen_sentence_vectors = model_tf.encode(unseen_question_text_list, show_progress_bar=True)\n    qids_new = [time.time() for _ in unseen_question_text_list]\n\n    for qid_new, unseen_sentence_vector, unseen_question_text in zip(qids_new, unseen_sentence_vectors, unseen_question_text_list):\n        qid_to_question[qid_new] = unseen_question_text\n        \n        # compute and update word embedding\n        token_list = tokenise_then_spellcheck(unseen_question_text)\n        qid_to_vec[qid_new] = to_vec(token_list)\n        qid_to_vec2[qid_new] = to_vec2(token_list)\n\n        # update sentence embedding\n        sentence_vectors[qid_new] = unseen_sentence_vector    \n\n    # update tf-idf\n    qid_to_tokens_, token_to_qids_, tf_, df_, L_  = preprocess_vsm(qids_new)\n    for qid in qid_to_tokens_:\n        qid_to_tokens[qid] = qid_to_tokens_[qid]\n    for token in token_to_qids_:\n        token_to_qids[token].update(token_to_qids_[token])\n    for token in tf_:\n        for qid in tf_[token]:\n            tf[token][qid] += tf_[token][qid]\n    for token in df_:\n        df[token] += df_[token]\n    for qid in L_:\n        L[qid] = L_[qid]","68d0b62a":"def query_unseen_question(unseen_question_text, method):\n    qid_new = time.time()\n    qid_to_question[qid_new] = unseen_question_text\n    \n    # update word embedding\n    token_list = tokenise_then_spellcheck(unseen_question_text)\n    qid_to_vec[qid_new] = to_vec(token_list)\n    qid_to_vec2[qid_new] = to_vec2(token_list)\n    \n    # update sentence embedding\n    sentence_vectors[qid_new] = model_tf.encode(unseen_question_text, show_progress_bar=False)\n    \n    show_sample_query_results(qid_new, *method(qid_new))","ffdf6274":"# list of methods, uncomment to select\nmethod = method_random_guess\nmethod = method_overlapping_root_word_count\nmethod = method_boolean\nmethod = method_tf_idf\nmethod = method_bm25\nmethod = method_unigram\nmethod = method_spacy_embedding_similarity\nmethod = method_spacy_embedding_similarity_lg\nmethod = method_wordmover_distance_glovewiki50\nmethod = method_sentence_vector\nmethod = method_supervised_model_logr\n# method = method_supervised_model_lgbm","04b5fcf2":"query_unseen_question(\"Why are computer screens dark in color?\", method=method)","424b2984":"index_unseen_question([\n    \"Why are computer screens black when unpowered?\",\n    \"Why are computer screens manufactured black?\"])","5c0ffb05":"query_unseen_question(\"Why are computer screens dark in color?\", method=method)","548a8866":"## uncomment if you want to reset the indexing\n# qid_to_tokens, token_to_qids = qid_to_tokens_original.copy(), token_to_qids_original.copy()\n# tf, df, L = tf_original.copy(), df_original.copy(), L_original.copy()","14c58ecb":"# Preprocessing Dataset","13162127":"# Word Embeddings - SpaCy","2f0130bc":"# Supervised Model","d1e707e0":"# Indexing and Querying of Unseen Questions\n\nThis is probably the Graphical User Interface that we will present","0bbb5eea":"# Preprocessing the Text","ce5dca48":"# Gensim WordMover Distance on Boolean Retrieval\n* Applies further sorting by wordmover distance on the output ranklist of Boolean Retrieval  \n* Current pre-trained model: `glove-wiki-gigaword-50`\n","c26e06a2":"# Preparation for Hand Evaluation Dataset","a172b8b6":"# Sentence Embeddings\n\nEach sentence can be embedded as a vector with SentenceTransformer","9f80ec97":"# Calculate NDCG with Hand Evaluation Dataset\n\nThis calculates NDCG from a snapshot version of `method_to_ranklists_scorelists`, and a hand annotated `df_handeval`\n\nDue to randomness, the `method_to_ranklists_scorelists` may not be reproduced exactly.","4bc4bf78":"# TFIDF","9672a8a7":"This notebook covers\n- Dataset Preparation (train-test split)\n- TF-IDF indexes \n- Evaluation algorithm\n- Evaluation procedure with the test set\n- Update indexes with unseen questions\n- Query with unseen questions\n\nThe following process is done on another notebook\n- Spellcheck and SpaCy tokenisation for the training set \n- SentenceTransformer computation of vectors\n- Downloading of the SpaCy and GenSim models","59017c77":"# Evaluation Metrics","2e12c4dd":"# Baseline - Overlapping Root Word Count (Working Title)\nOrder by the number of overlapping non-stopword words. Random if tie.","606f8a5c":"# BM25","2dd611ec":"#### LightGBM classification","834cd2bd":"#### Logistic Regression"}}