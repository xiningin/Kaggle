{"cell_type":{"a93434f9":"code","1eeccc00":"code","8a64dfa7":"code","43babb63":"code","bfdc1ea2":"code","9623025d":"code","d155b91d":"code","acbfb7d1":"code","089843b2":"code","ebedcb83":"code","c70ca5b4":"code","6f7ba9ae":"code","d50d6522":"code","d7f05d47":"code","6acac005":"code","a29b1c90":"code","b3477d6d":"code","198e5d16":"code","e02d9605":"code","5314e15b":"code","5f1fcbc0":"code","33844abd":"code","37c461f7":"code","9a8b82dd":"code","4ece56e3":"code","0f130605":"code","5d36b412":"code","da71c71c":"code","a28a3d0f":"code","fa46591d":"code","c897489d":"code","432a3ee5":"code","22af01c5":"code","8cd496ec":"code","af13ef15":"code","a9bd5881":"code","54b657ac":"code","53f1a777":"code","14d33766":"code","90d39d62":"code","ea40d2ff":"code","c6b971d1":"code","fba88e23":"code","05ca3439":"code","f4a3f58b":"code","d4b63d03":"code","1d23c484":"code","286f645d":"code","42b0d487":"code","2d320ad1":"code","eb5aab8f":"code","60ca4ffe":"code","c2716ca8":"code","1c9e14f4":"code","5d914811":"code","5da26dd8":"markdown","99e87e1e":"markdown","7b34c222":"markdown","31e86823":"markdown","b2f67716":"markdown","e8b7b2d8":"markdown","17eb3e64":"markdown","3538ba55":"markdown","3f4322d6":"markdown","8486abfa":"markdown","e00bec42":"markdown","c1f30acf":"markdown","c9986194":"markdown","adb5e33f":"markdown","16d7edda":"markdown","51359932":"markdown","3f380e75":"markdown","30fad5d5":"markdown","160edf9c":"markdown","922ac138":"markdown","66268e3e":"markdown","03321389":"markdown","03518d30":"markdown","bb8a9dd4":"markdown","93e8bb86":"markdown","f1cbd418":"markdown","5e715f96":"markdown","a8924f19":"markdown","6d7baf6e":"markdown","afe38461":"markdown","8c015deb":"markdown","f36a135c":"markdown","7914a1d8":"markdown","13d36ac9":"markdown","4c6ff68f":"markdown","29ea810d":"markdown","623a4244":"markdown","0bbaf267":"markdown"},"source":{"a93434f9":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nsns.set_style('whitegrid')\nimport matplotlib.pyplot as plt\nimport missingno as msno\n%matplotlib inline  \n\nfrom scipy import stats\nfrom sklearn import preprocessing\nfrom sklearn import feature_selection\nimport warnings\nwarnings.filterwarnings('ignore')\nSEED = 42","1eeccc00":"def concat_df(train_data, test_data):\n    # Returns a concatenated df of training and test set\n    return pd.concat([train_data, test_data], sort=True).reset_index(drop=True)\n\ndef divide_df(all_data):\n    # Returns divided dfs of training and test set\n    return all_data.loc[:1459], all_data.loc[1460:]\n\ndf_train = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/train.csv')\ny_train = df_train.SalePrice\nid_val = df_train.Id\ndf_test = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/test.csv')\ndf_all = concat_df(df_train, df_test).drop(['SalePrice', 'Id'], axis=1)\n\ndf_train.name = 'Training Set'\ndf_test.name = 'Test Set'\ndf_all.name = 'All Set' \n\ndfs = [df_train, df_test]\n\nprint(f'Number of Training Examples = {df_train.shape[0]}')\nprint(f'Number of Test Examples = {df_test.shape[0]}\\n')\nprint(f'Training X Shape = {df_train.shape}')\nprint(f'Training y Shape = {df_train[\"SalePrice\"].shape[0]}\\n')\nprint(f'Test X Shape = {df_test.shape}')\nprint(f'Test y Shape = {df_test.shape[0]}\\n')\nprint(df_train.columns)","8a64dfa7":"df_train.head()","43babb63":"# Visualize the general missing values of data\nmsno.matrix(df_all)\nplt.show()","bfdc1ea2":"for df in dfs:\n    print(f'Only features contained missing value in {df.name}')\n    temp = df.isnull().sum()\n    print(temp.loc[temp!=0], '\\n')","9623025d":"null_features = df_all.isnull().sum()\n\n# For features having smaller than 100 missing values\nnull_100 = df_all.columns[list((null_features < 100) & (null_features != 0))]\nnum = df_all[null_100].select_dtypes(include=np.number).columns\nnon_num = df_all[null_100].select_dtypes(include='object').columns\n# Numerous features\ndf_all[num] = df_all[num].apply(lambda x: x.fillna(x.median()))\n# Object features\ndf_all[non_num] = df_all[non_num].apply(lambda x: x.fillna(x.value_counts().index[0]))\n\n# For features having larger than 1000 missing values\nnull_1000 = df_all.columns[list(null_features > 1000)]\ndf_all.drop(null_1000, axis=1, inplace=True)\ndf_all.drop(['GarageYrBlt', 'LotFrontage'], axis=1, inplace=True)","d155b91d":"# For other features having missing values\n# GarageCond\ndf_all['GarageCond'] = df_all['GarageCond'].fillna('Null')\n# GarageFinish\ndf_all['GarageFinish'] = df_all['GarageFinish'].fillna('Null')\n# GarageQual\ndf_all['GarageQual'] = df_all['GarageQual'].fillna('Null')\n# GarageType\ndf_all['GarageType'] = df_all['GarageType'].fillna('Null')","acbfb7d1":"\n# Update training & testing data\ndf_train, df_test = divide_df(df_all)\ndf_train = pd.concat([df_train, y_train], axis=1)\n\n# Checking existing missing value or not\nprint('If the result is zero means not exist any missing values in dataset')\nprint(df_all.isnull().any().sum())","089843b2":"# Seeing the correlation between features and target\ndf_train_corr = df_train.corr()['SalePrice'].sort_values(ascending=False).drop(['SalePrice'])\ndf_train_corr.head(10)","ebedcb83":"fig, axs = plt.subplots(4, 4, figsize=(18, 18))\nplt.subplots_adjust(right=1.3, top=1.3)\naxs = axs.flatten()\nfor i, col in enumerate(list(df_train_corr.index[:16])):\n    sns.scatterplot(y='SalePrice', x=col, ax=axs[i], data=df_train)\n    axs[i].set_xlabel('SalePrice')\n    axs[i].set_ylabel(col)\nplt.show()","c70ca5b4":"# Corr of \"OverallQual\": 0.7909\nfig = plt.figure(figsize=(8, 8))\nsns.boxplot(df_train['OverallQual'], df_train['SalePrice'])\nplt.show()","6f7ba9ae":"# Corr of \"GrLivArea\": 0.708\nfig = plt.figure(figsize=(8, 8))\nsns.scatterplot(df_train['GrLivArea'], df_train['SalePrice'])\nplt.show()","d50d6522":"df_train_cate = df_train.select_dtypes(include=['object', 'category'])\ndf_train_cate.head()","d7f05d47":"data = pd.melt(pd.concat([df_train_cate, y_train], axis=1),\n               id_vars=['SalePrice'], value_vars=df_train_cate.columns, var_name='features')\ng = sns.FacetGrid(data, col='features', col_wrap=2, sharex=False, sharey=False, size=5)\ng.map(sns.violinplot, 'value', 'SalePrice')","6acac005":"# There're some features having the mean smaller than 1 -> Problematic :<\ndf_train.describe()","a29b1c90":"fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(10, 10))\ny_train.hist(bins=100, ax=ax1)\nax1.set_ylabel('Occurences')\nax1.set_xlabel('SalePrice')\n\nstats.probplot(y_train, dist=stats.norm, plot=ax2)\nax2.set_ylabel('SalePrice')\nplt.show()","b3477d6d":"# Using Box-Cot transformation on target feature\norg_y_train = y_train\ny_train = pd.Series(stats.boxcox(y_train, lmbda=0), name='SalePrice')\n\n# Visualize target after box-cox transformation\nfig, (ax1, ax2) = plt.subplots(2, 1, figsize=(10, 10))\ny_train.hist(bins=100, ax=ax1)\nax1.set_ylabel('Occurences')\nax1.set_xlabel('SalePrice')\n\nstats.probplot(y_train, dist=stats.norm, plot=ax2)\nax2.set_ylabel('SalePrice')\nplt.show()","198e5d16":"# Compute Skewness & Kurtosis\nprint(f'Skewness before transformation: {stats.skew(org_y_train)}')\nprint(f'Kurtosis before transformation: {stats.kurtosis(org_y_train)}\\n')\n\nprint(f'Skewness after transformation: {stats.skew(y_train)}')\nprint(f'Kurtosis after transformation: {stats.kurtosis(y_train)}')","e02d9605":"# Using binned technique for \"YearBuilt\", \"YearRemodAdd\" & \"YrSold\"\ndf_all['YearBuilt'] = pd.qcut(df_all['YearBuilt'], 10, duplicates='drop')\ndf_all['YearRemodAdd'] = pd.qcut(df_all['YearRemodAdd'], 10, duplicates='drop')\ndf_all['YrSold'] = pd.qcut(df_all['YrSold'], 10, duplicates='drop')","5314e15b":"# Encode categorical features to numeric feature\nfor cate_col in ['YearBuilt', 'YearRemodAdd', 'YrSold']:\n    df_all[cate_col] = preprocessing.LabelEncoder().fit_transform(df_all[cate_col].values)","5f1fcbc0":"fig, axs = plt.subplots(2, 1, figsize=(15, 10))\nsns.countplot(df_all['YearBuilt'], ax=axs[0])\nsns.countplot(df_all['YearRemodAdd'], ax=axs[1])\nplt.show()","33844abd":"# Transform numeric features that are really the categorical features\ndf_all['MSSubClass'] = df_all['MSSubClass'].astype(str)\ndf_all['OverallCond'] = df_all['OverallCond'].astype(str)\ndf_all['MoSold'] = df_all['MoSold'].astype(str)","37c461f7":"# Generating new features\n# Total square foot\ndf_all['TotalSF'] = df_all['BsmtFinSF1'] + df_all['BsmtFinSF2'] + df_all['1stFlrSF'] + df_all['2ndFlrSF']\n\n# Total number of bathroom\ndf_all['TotalBath'] = (df_all['FullBath'] + (0.5 * df_all['HalfBath']) +\n                               df_all['BsmtFullBath'] + (0.5 * df_all['BsmtHalfBath']))\ndf_all['TotalBsmtbath'] = df_all['BsmtFullBath'] + (0.5 * df_all['BsmtHalfBath'])\n\n# Total square feet of porch in a house\ndf_all['TotalPorchSF'] = (df_all['OpenPorchSF'] + df_all['3SsnPorch'] +\n                            df_all['EnclosedPorch'] + df_all['ScreenPorch'] + df_all['WoodDeckSF'])\n\n# Check the exist of each infrastructure (Ex: basement, bath,...) in a house\ndf_all['IsRemodel'] = df_all[['YearBuilt', 'YearRemodAdd']].apply(lambda x: 1 if x[0] != x[1] else 0, axis=1)\ndf_all['HasPool'] = df_all['PoolArea'].apply(lambda x: 1 if x > 0 else 0)\ndf_all['Has2ndFloor'] = df_all['2ndFlrSF'].apply(lambda x: 1 if x > 0 else 0)\ndf_all['HasGarage'] = df_all['GarageArea'].apply(lambda x: 1 if x > 0 else 0)\ndf_all['HasBsmt'] = df_all['TotalBsmtSF'].apply(lambda x: 1 if x > 0 else 0)\ndf_all['HasFireplace'] = df_all['Fireplaces'].apply(lambda x: 1 if x > 0 else 0)","9a8b82dd":"# Drop all the recipe features\nremove_cols = ['BsmtFinSF1', 'BsmtFinSF2', 'FullBath', 'HalfBath', 'BsmtFullBath',\n              'BsmtHalfBath', 'OpenPorchSF', '3SsnPorch', 'EnclosedPorch', 'ScreenPorch', 'WoodDeckSF']\ndf_all.drop(remove_cols, axis=1, inplace=True)","4ece56e3":"# List of categorical features\ncate_features = list(df_all.select_dtypes(include=['object', 'category']).columns)\n\n# List of numeric features\nnumeric_dtypes = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\nnum_features = list(df_all.select_dtypes(include=numeric_dtypes).columns)","0f130605":"# Update training and testing dataset\ndf_train, df_test = divide_df(df_all)","5d36b412":"from sklearn.ensemble import RandomForestRegressor\nfrom mlxtend.feature_selection import SequentialFeatureSelector as SFS","da71c71c":"def forward_feature_selection(df_train, cate_features):\n    '''Activate the \"forward feature selection\" technique to select the most appropriate features\n    Arg:\n        cate_features: list of string names of all categorical features in dataset\n    Return:\n        SFS object\n        '''\n    # df prepared for inputing into the technique\n    pre_ffs = pd.DataFrame(columns=cate_features)\n\n    # Encode categorical features to numeric feature for utilize \"forward selection feature\"\n    for cate_col in cate_features:\n        pre_ffs[cate_col] = preprocessing.LabelEncoder().fit_transform(df_train[cate_col].values)\n        \n    # Step forward feature selection\n    sfs1 = SFS(RandomForestRegressor(),\n               k_features=2,\n               forward=True,\n               floating=False,\n               verbose=2,\n               scoring='r2',\n               cv=3)\n    \n    sfs1 = sfs1.fit(np.array(pre_ffs[cate_features]), np.array(y_train))\n    return sfs1\n\n# (UNCOMMENT HERE TO TRY)\n# Choose categorical features using SFS technique\n# sfs1 = forward_feature_selection(df_train, cate_features)\n\n# Print out the chosen categorical feature\n# cate_features = list(df_all[cate_features].columns[list(sfs1.k_feature_idx_)])\n# cate_features","a28a3d0f":"cate_features =  ['BldgType', 'BsmtExposure', 'BsmtFinType1', 'BsmtQual', 'CentralAir', 'Condition1', 'Condition2', 'Electrical',\n 'ExterCond', 'ExterQual', 'Exterior2nd', 'Functional', 'GarageCond', 'GarageType', 'Heating', 'HouseStyle', 'KitchenQual',\n 'LandContour', 'LandSlope', 'LotShape', 'MSSubClass', 'Neighborhood', 'PavedDrive', 'RoofMatl', 'RoofStyle',\n 'SaleCondition', 'SaleType', 'Street', 'Utilities']","fa46591d":"# \"Electrical\" is an example of mismatched feature values\nprint('In training dataset\\n', df_train['Electrical'].value_counts(), '\\n')\nprint('In testing dataset\\n', df_test['Electrical'].value_counts())","c897489d":"cate_mismatch = list()\n\n# Determine features contained mismatched values\nfor cate_col in cate_features:\n    train_cate = df_train[cate_col].value_counts().index\n    test_cate = df_test[cate_col].value_counts().index\n    check_len = len(np.setdiff1d(train_cate, test_cate)) + len(np.setdiff1d(test_cate, train_cate))\n    if check_len != 0:\n        cate_mismatch.append(cate_col)\n        \nprint('List of mismatched value features: \\n', cate_mismatch)","432a3ee5":"# \"Electrical\" features\ndf_train['Electrical'].loc[df_train['Electrical']=='Mix'] = 'SBrkr'\n\n# \"Exterior2nd\" features\ndf_train['Exterior2nd'].loc[df_train['Exterior2nd']=='Other'] = 'VinylSd'\n\n# \"Heating\" features\ndf_train['Heating'].loc[df_train['Heating']=='OthW'] = 'GasA'\ndf_train['Heating'].loc[df_train['Heating']=='Floor'] = 'GasA'\n\n# \"HouseStyle\" features\ndf_train['HouseStyle'].loc[df_train['HouseStyle']=='2.5Fin'] = '1.5Fin'\n\n# \"MSSubClass\" features\ndf_test['MSSubClass'].loc[df_test['MSSubClass']=='150'] = '160'\n\n# \"Condition2\" feature\ntemp = [True if ((val=='RRNn') | (val=='RRAn') | (val=='RRAe')) else False\n        for val in df_train['Condition2']]\ndf_train['Condition2'].loc[temp] = 'Norm'\n\n# \"Utilities\" is a constant-value feature --> Delete it\n# \"RoofMatl\" has high number of different values --> Delete it\ncate_drop = ['Utilities', 'RoofMatl']\ndf_train.drop(cate_drop, axis=1, inplace=True)\ndf_test.drop(cate_drop, axis=1, inplace=True)\n\n# Update the cate_features list also\ncate_features = [col for col in cate_features if col not in cate_drop]","22af01c5":"# Check \"Condition2\" feature\nprint('In training dataset\\n', df_train['Electrical'].value_counts(), '\\n')\nprint('In testing dataset\\n', df_test['Electrical'].value_counts())","8cd496ec":"# Find feature correlation with target using pearson's coeficient\npearson = dict()\nfor col in num_features:\n    pear_val = stats.pearsonr(np.array(df_train[col]), np.array(y_train))[0]\n    pearson[col] = pear_val\n    \npearson = pd.Series(pearson).abs().sort_values(ascending=False)\n# Choose only feature having correlation larger than 0.2\nnum_features = list(pearson.loc[pearson > 0.2].index)\nnum_features","af13ef15":"chosen_cols = num_features + cate_features\n\n# Visualizing the correlation table\nfig = plt.figure(figsize=(10, 10))\nsns.heatmap(pd.concat([df_train[chosen_cols], y_train], axis=1).corr(), square=True,\n            cmap='mako', annot_kws={'size': 14})","a9bd5881":"df_train = df_train[chosen_cols]\ndf_test = df_test[chosen_cols]\ndf_all = concat_df(df_train, df_test)","54b657ac":"# Normalize skewness feature using Log function\nskew_features = df_all[num_features].apply(lambda x: stats.skew(x)).sort_values(ascending=False)\nskew_features = skew_features[abs(skew_features) > 0.75]\nprint(skew_features)           \n\n# Apply Box cox for skewness > 0.75\nfor feat in skew_features.index:\n    df_all[feat] = np.log1p(df_all[feat])","53f1a777":"df_train, df_test = divide_df(df_all)","14d33766":"print(df_train.shape, df_test.shape)","90d39d62":"# Transform categorical feature to dummies features\nencoded_features = list()\n\nfor df in [df_train, df_test]:\n    for feature in cate_features:\n        # Change to array after encoding b.c want to add columns when change back to df\n        encoded_feat = preprocessing.OneHotEncoder().fit_transform(df[feature].values.reshape(-1, 1)).toarray()\n        # \"n\": Number of unique value in each feature\n        n = df[feature].nunique()\n        # \"feature_uniqueVal\" are the col's names in df after One-hot encoding\n        cols = ['{}_{}'.format(feature, n) for n in range(1, n + 1)]\n        \n        encoded_df = pd.DataFrame(encoded_feat, columns=cols)\n        encoded_df.index = df.index\n        encoded_features.append(encoded_df)\n        \ndf_train = pd.concat([df_train, *encoded_features[:len(cate_features)]], axis=1)\ndf_test = pd.concat([df_test, *encoded_features[len(cate_features):]], axis=1)","ea40d2ff":"# Drop original category features\ndf_train.drop(cate_features, axis=1, inplace=True)\ndf_test.drop(cate_features, axis=1, inplace=True)\n\ndf_all = concat_df(df_train, df_test)","c6b971d1":"print(df_train.shape, df_test.shape)","fba88e23":"df_train.head()","05ca3439":"from sklearn.model_selection import KFold # for repeated K-fold cross validation\nfrom sklearn.model_selection import cross_val_score # score evaluation\nfrom sklearn.model_selection import cross_val_predict # prediction\nfrom sklearn.linear_model import ElasticNetCV, LassoCV, RidgeCV\nfrom sklearn.svm import SVR\nfrom sklearn.metrics import mean_squared_error\n\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom mlxtend.regressor import StackingCVRegressor\nfrom sklearn.ensemble import StackingRegressor\nfrom xgboost import XGBRegressor\nfrom lightgbm import LGBMRegressor\nimport time\nSEED = 42","f4a3f58b":"# Repeated K-fold cross validation\nkfolds = KFold(n_splits=10, shuffle=True, random_state=SEED)\n\n# Return root mean square error of model prediction (Used for test prediction)\ndef rmse(y, y_pred):\n    return np.sqrt(mean_squared_error(y, y_pred))\n\n# Return root mean square error applied cross validation (Used for training prediction)\ndef evaluate_model_cv(model, X, y):\n    rmse = np.sqrt(-cross_val_score(model, X, y, scoring=\"neg_mean_squared_error\", cv=kfolds))\n    return (rmse)","d4b63d03":"def construct_models():\n    # Initialize parameters for models\n    alphas_ridge = [0.005, 0.01, 0.1, 1, 5, 10, 15]\n    alphas_lasso = [5e-05, 0.0001, 0.0002, 0.0003, 0.0004, 0.0005, 0.0006, 0.0007, 0.0008]\n    e_alphas_elas = [0.0001, 0.0002, 0.0003, 0.0004, 0.0005, 0.0006, 0.0007]\n    e_l1ratio_elas = [0.8, 0.85, 0.9, 0.95, 0.99, 1]\n    \n    # Constructing the models\n    models = dict()\n    \n    models['ridge'] = RidgeCV(alphas=alphas_ridge, cv=kfolds)\n    models['lasso'] = LassoCV(alphas=alphas_lasso, random_state=SEED, cv=kfolds)\n    models['elasticnet'] = ElasticNetCV(alphas=e_alphas_elas, cv=kfolds, l1_ratio=e_l1ratio_elas)\n    models['svr'] = SVR(C = 20, epsilon = 0.008, gamma =0.0003)\n    models['gbr'] = GradientBoostingRegressor(n_estimators=3000, learning_rate=0.05, \n                                              max_depth=4, max_features='sqrt',\n                                              min_samples_leaf=15, min_samples_split=10, \n                                              loss='huber',random_state =SEED) \n    models['lgbm'] = LGBMRegressor(objective='regression', num_leaves=4,\n                                   learning_rate=0.01, n_estimators=5000,\n                                   max_bin=200, bagging_fraction=0.75,\n                                   bagging_freq=5, bagging_seed=7,\n                                   feature_fraction=0.2,\n                                   feature_fraction_seed=7, verbose=-1,\n                                  colsample_bytree=None, subsample=None, subsample_freq=None)\n    models['xgboost'] = XGBRegressor(learning_rate=0.01, n_estimators=3460,\n                                     max_depth=3, min_child_weight=0,\n                                     gamma=0, subsample=0.7,\n                                     colsample_bytree=0.7, verbosity = 0,\n                                     objective='reg:squarederror', nthread=-1,\n                                     scale_pos_weight=1, seed=SEED, reg_alpha=0.00006)\n    return models\n\n# Construct the set of model\nmodels = construct_models()","1d23c484":"for name, model in models.items():\n    # Start counting time\n    start = time.perf_counter()\n    \n    model = model.fit(np.array(df_train), np.array(y_train))\n    rmse_result = rmse(y_train, model.predict(np.array(df_train)))\n    print(f'{name}\\'s rmse after training: {rmse_result}')\n    \n    # Compute time for executing each algo\n    run = time.perf_counter() - start\n    print(f'Computational runtime of this algo: {round(run, 2)} seconds\\n')","286f645d":"cv_rmse_result = dict()\ncv_rmse_mean = dict()\ncv_rmse_std = dict()\n\nfor name, model in models.items():\n    # Start counting time\n    start = time.perf_counter()\n    \n    cv_rmse_result[name] = evaluate_model_cv(model, np.array(df_train), np.array(y_train))\n    cv_rmse_mean[name] = cv_rmse_result[name].mean()\n    cv_rmse_std[name] = cv_rmse_result[name].std()\n    print(f'Finish {name}\\'s model')\n    \n    # Compute time for executing each algo\n    run = time.perf_counter() - start\n    print(f'Computational runtime of this algo: {round(run, 2)} seconds\\n')","42b0d487":"ML_cv = pd.DataFrame({'cv_rsme_mean' : cv_rmse_mean, 'cv_rmse_std' : cv_rmse_std})\nML_cv","2d320ad1":"# Type 1 stacking model\nstack_model = StackingCVRegressor(regressors=(models['ridge'], models['lasso'], models['xgboost'],\n                                              models['elasticnet'], models['gbr'], models['lgbm']),\n                                  meta_regressor=models['xgboost'], use_features_in_secondary=True)","eb5aab8f":"# Time performance counter\nstart = time.perf_counter()\n\nstack_model = stack_model.fit(np.array(df_train), np.array(y_train))\nprint('Finish training')\n\n# Compute rmse with cross-validation technique\n# rmse_stack_cv = evaluate_model_cv(stack_model, np.array(df_train), y_train)\n# print(f'stack_model\\'s rmse (using cv) after training: {rmse_stack_cv.mean()}')\n\n# Compute rmse without cross-validation technique\nrmse_stack = rmse(y_train, stack_model.predict(np.array(df_train)))\nprint(f'stack_model\\'s rmse (using cv) after training: {rmse_stack}')\n\n# Compute time for executing each algo\nrun = time.perf_counter() - start\nprint(f'Computational runtime of this algo: {round(run, 2)} seconds\\n')","60ca4ffe":"def blend_models_predict(X):\n    return ((0.05 * models['ridge'].predict(np.array(X))) + \\\n            (0.05 * models['lasso'].predict(np.array(X))) + \\\n            (0.05 * models['elasticnet'].predict(np.array(X))) + \\\n            (0.15 * models['gbr'].predict(np.array(X))) + \\\n            (0.15 * models['lgbm'].predict(np.array(X))) + \\\n            (0.25 * models['xgboost'].predict(np.array(X))) + \\\n            (0.3 * stack_model.predict(np.array(X))))","c2716ca8":"print('RMSLE score on train data:')\nprint(rmse(y_train, blend_models_predict(np.array(df_train))))","1c9e14f4":"# Get the id feature from testing dataset\ntest = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/test.csv')\ntest_id = test['Id']","5d914811":"# Submission set\nsubmit = pd.concat((test_id, pd.Series(np.exp(blend_models_predict(df_test)), \n                                       name='SalePrice')), axis=1)\nsubmit.to_csv('Submission.csv', index=False)","5da26dd8":"# House price: feature select, stacking, blending (top 3%)\n(Hoang Pham)\n\n* [**1. Getting familiar with data**](#1)\n    * [**1.1 Missing values**](#1.1)\n    * [**1.2 Numeric features**](#1.2)\n    * [**1.3 Categorical features**](#1.3)\n    * [**1.4 Target column distribution**](#1.4)\n* [**2. Feature engineering**](#2)\n    * [**2.1 Binning continuous features**](#2.1)\n    * [**2.2 Construct new useful features**](#2.2)\n    * [**2.3 Feature selection**](#2.3)\n        * [**2.3.1 Select categorical features**](#2.3.1)\n        * [**2.3.2 Mismatched value between train & test set in categorical features**](#2.3.2)\n        * [**2.3.3 Select contunious features**](#2.3.3)\n    * [**2.4 Features transformation**](#2.4)\n        * [**2.4.1 Highly skewed numeric features**](#2.4.1)\n        * [**2.4.2 One-hot encoding categorical features**](#2.4.2)\n* [**3. Modeling**](#3)\n    * [**3.1 Base models**](#3.1)\n    * [**3.2 Stacking model**](#3.2)\n    * [**3.3 Blending model**](#3.3)\n* [**4 Submision**](#4)","99e87e1e":"## Introduction\nKaggle describes this competition as follows:\n\nAsk a home buyer to describe their dream house, and they probably won\u2019t begin with the height of the basement ceiling or the proximity to an east-west railroad. But this playground competition\u2019s dataset proves that much more influences price negotiations than the number of bedrooms or a white-picket fence.\n\nWith 79 explanatory variables describing (almost) every aspect of residential homes in Ames, Iowa, this competition challenges you to predict the final price of each home.","7b34c222":"- Now we'll visualize the relationship between categorical features and target feature (SalePrice) using violin plot\n- Violin plot is effective to categorical features, it shows some important statistic terms so that we can compare between each other\n\n<img src=\"https:\/\/miro.medium.com\/max\/650\/1*TTMOaNG1o4PgQd-e8LurMg.png\" style=\"width:400px;height:400px;\">","31e86823":"<a name='2.2'><\/a>\n## 2.2 Construct new useful features\n- There are some features that we can concatunate them together to get more useful features\n- After constructing new features, all the recipe features might be removed b.c these features and the new one both represent the same type of infomation. Therefore, they would not be more effective to be together than to be alone","b2f67716":"- For dealing with mismatched values, I'll replace them by the values having the highest frequency in each feature. \n- With some features having high number of different values, I think removing them might be a good choice","e8b7b2d8":"<a name='2.3.3'><\/a>\n### 2.3.3 Select contunious features\n- The Pearson correlation coeficient is a statistical measure of the strength of a linear association between 2 continuous features. This technique is suitable for linear correlation, or rank-based methods for a non linear correlation\n- Therefore, Pearson could be a compeling choice for choosing the features having high correlation to the SalePrice target\n\nReference link: [here](https:\/\/machinelearningmastery.com\/feature-selection-with-real-and-categorical-data\/)","17eb3e64":"- Target feature is a heavy-tailed distribution --> So Box-Cox transformation should be useful to bring the target from heavy-tailed to normal distribution","3538ba55":"<a name='1.3'><\/a>\n## 1.3 Categorical features\n- There are totally 38 categorical features in the dataset\n- Be careful that there are some categorical features in training dataset containing some values which do not exist in  the same features of testing dataset (E.g \"Condition2\" feature). We'll detect them and fix them later","3f4322d6":"After design and construct the based model, we need to fit the training data to the model and compute the root mean square error (rmse) result to validate models after the training\n- **Note that:** numpy array is recommended as an input to the training model instead of Dataframe. Because numpy has a hugh benefit relating to the time consuming than pandas. Link for more infomation [Here](https:\/\/towardsdatascience.com\/speed-testing-pandas-vs-numpy-ffbf80070ee7)","8486abfa":"- \"Overfitting\" problem happens when the model overlearns the detail of training dataset so that it'll negatively impact the performance of model on the testing dataset. In short, when the performance on training dataset is much more higher than performance on testing dataset\n- Computing rmse applying cross validation technique is effective to prevent the \"Overfitting\" problem","e00bec42":"OverallQual is a categorical feature. Therefore, the box plot should be suitable in this case to clearly show the high correlation characteristic between Overall quality (\"OverallQual\") of the house with the its price (\"SalePrice\").","c1f30acf":"<a name='2.3.2'><\/a>\n### 2.3.2 Mismatched value between train & test set in categorical features\n- In some case, some columns in train dataset contained values which do not exist in testing dataset, we called them **missmatched data** between train and test\n- This would be an serious problem if we plan to perform one-hot encoding for categorical features in the future because of the different number of features between train and test data\n- Below are an example of missmatched data in \"Electrical\" feature. The value \"mix\" exist in training set but not in testing set","c9986194":"<a name='3'><\/a>\n# 3. Modeling","adb5e33f":"Next we'll visualize the year features after applying the binning technique","16d7edda":"Now the number of features in training and testing dataset are the same & all the preprocessing steps are finished. The data is ready for training!!","51359932":"<a name='2.1'><\/a>\n## 2.1 Binning continuous features\n- Using \"Bin\" technique to all features representing \"year\" value (Ex: 2000, 1999,...)\n- And after that encoding them into continuous features","3f380e75":"<a name='3.2'><\/a>\n## 3.2 Stacking model\n- In statistics and machine learning, ensemble methods use multiple learning algorithms to obtain better predictive performance than the learning algorithm alone. **Stacking model** is an ensemble one\n- It uses a meta-learning algorithm to learn how to best combine the predictions from two or more base machine learning algorithms.\n\nReference link: [Stacking model](https:\/\/machinelearningmastery.com\/stacking-ensemble-machine-learning-with-python\/)","30fad5d5":"- For features having missing value smaller than 100 -> I'll fill numeric features with the corresponding median & categorical features with the corresponding most frequent values\n- For features having missing value larger than 1000 -> Removing these features might be a good choice\n- For other features having missing value -> I'll fill them with \"Null\" value (b.c all the other features are object features)","160edf9c":"<a name='2'><\/a>\n# 2. Feature engineering","922ac138":"<a name='1'><\/a>\n# 1. Getting familiar with data\n- Training set has 1460 rows, testing set has 1459 rows\n- There are 81 features in training set and 80 features in testing set\n- One extra feature in training set is \"SalePrice\" which is the target feature\n- \"df_all\" the is concatenated dataframe between training and testing data for more convenience preprocessing. And we also should be careful about the \"data leak\" problem","66268e3e":"<a name='2.4'><\/a>\n## 2.4 Features transformation","03321389":"<a name='2.4.2'><\/a>\n### 2.4.2 One-hot encoding categorical features\nNow after finishing preprocessing all the categorical features, we should encode them to numeric features to be successfully inputted into the model by \"one-hot encoding\" technique. ","03518d30":"Visualize the 16 features which has the highest correlation with the target","bb8a9dd4":"<a name='2.3'><\/a>\n## 2.3 Feature selection","93e8bb86":"<a name='4'><\/a>\n# 4 Submision","f1cbd418":"After encoding all the categorical features, we need to remove the original ones to prevent training the same type of information. Because the orginal categorical features and their one-hot encoding version represent the same infomation","5e715f96":"<a name='3.3'><\/a>\n## 3.3 Blending model\n- **Blending** is an ensemble machine learning technique that uses a machine learning model to learn how to best combine the predictions from multiple contributing ensemble member models\n- For more understanding, the link [Blending model](https:\/\/mlwave.com\/kaggle-ensembling-guide\/) might be useful!","a8924f19":"## Excutive summary:\nI started this competition with the purpose of obtaining an intuitive knowledge about feature selection, stacking model and blending model to solve the regression problem. After completing the competition, there are a lot of interesting and useful things that I'd like to share with all of Kagglers.\n- For features selection in this kernel, I use Pearson's correlation values to choose the numeric features based on their linear association with the target. And \"Forward feature selection\" technique is used for choosing suitable categorical features\n- While using based models only, the average values of rmse is nearly 0.13 on LB. But when I applied stacking model, the rmse value moved down directly to only 0.124 on LB. It worths trying!\n- I've also implement the short version of this notebook for someone who want to go through a quick start of this competion. Link: [Quick start house price competition](https:\/\/www.kaggle.com\/hoangphamviet\/beginner-quick-start-house-price-competition)","6d7baf6e":"**Skewness**\n- Is the degree of distortion from the symmetrical normal curve --> Skewness of normal distribution is \"0\"\n- **Positive skewness** means the tail on the right side of the distribution is longer and fatter\n- **Negative skewness** means the tail on the left side of the ditribution is longer and fatter\n\n**Kurtosis**\n- In probability theory and statistics, **Kurtosis** is the measure of extreme values (outliers) presented in the distribution","afe38461":"<a name='1.1'><\/a>\n## 1.1 Missing values","8c015deb":"## Credit & resource\n- Credit should be extended for [Serigne](https:\/\/www.kaggle.com\/serigne) and [Prashant Banerjee](https:\/\/www.kaggle.com\/prashant111\/comprehensive-guide-on-feature-selection) notebooks to help me to gain knowledge about features selection, blending model and give me some hint to finish this notebook. Great thanks for them\n- There are 2 supper informative and comprehensive sources to understand the concept [ensemble model](https:\/\/mlwave.com\/kaggle-ensembling-guide\/) and learn how to code them [ensemble model code](https:\/\/machinelearningmastery.com\/stacking-ensemble-machine-learning-with-python\/)\n- Great notebook about features selection [Feature selections](https:\/\/www.kaggle.com\/prashant111\/comprehensive-guide-on-feature-selection). I've learned a lot from it!\n\n### Please give it an upvote, if you found this notebook helpful, thank for reading!","f36a135c":"<a name='1.4'><\/a>\n## 1.4 Target column distribution\n- Target feature is a heavy-tailed distribution. It'd be problematic to input the small-range features to predict the large-range feature (SalePrice). Therefore, \"Box-cox transformation\" technique to transform target feature to normal distribution might be appropriate in this occasion\n- There're also some large-scale input features & we'll normalize them later \ud83d\udc4d\ud83d\udc4d","7914a1d8":"<a name='3.1'><\/a>\n## 3.1 Base models\nWe'll plan to construct these below based model\n- **Ridge** is regression model applying l2 regularization technique. ([Link here](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.linear_model.Ridge.html))\n- **Lasso** stands for Least Absolute Shrinkage and Selection Operator that is a linear regression model applied l1 regularization technique ([Link here](https:\/\/www.statisticshowto.com\/lasso-regression\/))\n- **elasticnet** is a penalized linear regression model that includes both the L1 and L2 penalties during training ([Link here](https:\/\/machinelearningmastery.com\/elastic-net-regression-in-python\/))\n- **svr** stands for Support Vector Regression is a type of \"SVM\" model using for regression problem ([Link here](https:\/\/towardsdatascience.com\/an-introduction-to-support-vector-regression-svr-a3ebc1672c2))\n- **gbr** is gradient boosting model for regression problem ([Link here](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.ensemble.GradientBoostingRegressor.html))\n- **lgbm** is a fast, distributed, high-performance gradient boosting framework that uses a tree-based learning algorithm ([Link here](https:\/\/machinelearningmastery.com\/light-gradient-boosted-machine-lightgbm-ensemble\/))\n- **xgboost** is a decision-tree-based ensemble Machine Learning algorithm that uses a gradient boosting framework ([Link here](https:\/\/towardsdatascience.com\/https-medium-com-vishalmorde-xgboost-algorithm-long-she-may-rein-edd9f99be63d#:~:text=What%20is%20XGBoost%3F,all%20other%20algorithms%20or%20frameworks.))","13d36ac9":"Great!! All the mismatched values are fixed. Now let's move to selecting the continuous features","4c6ff68f":"<a name='2.4.1'><\/a>\n### 2.4.1 Highly skewed numeric features\n- Highly skewed numeric features are the heavy-tail features like our target features\n- We decide whether a feature is skewness or not based on the value of \"skewness\" statistics measurement\n- All skewed features will be normalize by Box-cox normalization technique","29ea810d":"And then transform some numeric features that are actually the categorical feature","623a4244":"<a name='2.3.1'><\/a>\n### 2.3.1 Select categorical features\n- Using **forward feature selection** to select the categorical features\n- **Forward selection** is an iterative method in which we start with having no feature in the model. In each iteration, we keep adding the feature which best improves our model until the technique finishs choosing k features as we set\n- RandomForest Regression can be used as a model to filter the features\n- There are totally 38 numerical features, I'll use forward feature selection technique to select the most 30 correlated categorical features with the target\n- Because this forward selection technique trains on categorical data to filter features. Therefore, I need to encode the categorical features using \"label encoding\" technique first, but only for this step not for actual training\n\n**(NOTE)** Because of the iterative training process, so I commented the training code and print out the result below for avoiding time processing. If you're curious, you can uncomment it and try by yourself ","0bbaf267":"<a name='1.2'><\/a>\n## 1.2 Numeric features\n- There are totally 34 numeric features in the dataset\n- We'll visualize the features having the high level of correlation with target feature (SalePrice)"}}