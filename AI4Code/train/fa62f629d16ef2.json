{"cell_type":{"ec9857de":"code","5cddff75":"code","1ed15436":"code","bf450a68":"code","40f064b5":"code","d292348e":"code","5d408bf2":"code","b6fb5330":"code","bac5240f":"code","e99da0be":"code","f1a7e23d":"code","bfb985ee":"code","88caa2eb":"code","1e1a16d7":"code","933f4359":"code","0522871d":"code","7614f7a1":"code","a1ac3f3b":"code","e980d190":"code","9955e7df":"code","aecb18e0":"code","750fd69f":"code","a962a140":"code","1466ef6e":"markdown","d1c0f72e":"markdown","10f6d01e":"markdown","010c88e9":"markdown","39bbe7cc":"markdown","9ba2d950":"markdown","54a25a35":"markdown","e5a5db6c":"markdown","3a0e4d1a":"markdown","03d9c8b6":"markdown","a910e86a":"markdown","11526eb3":"markdown"},"source":{"ec9857de":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set(style = \"darkgrid\")\n%matplotlib inline\nimport gc","5cddff75":"def reduce_mem_usage(train_data):\n    \"\"\" iterate through all the columns of a dataframe and modify the data type\n    to reduce memory usage.\n    \"\"\"\n    start_mem = train_data.memory_usage().sum() \/ 1024**2\n    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n\n    for col in train_data.columns:\n        col_type = train_data[col].dtype\n\n        if col_type != object:\n            c_min = train_data[col].min()\n            c_max = train_data[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    train_data[col] = train_data[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    train_data[col] = train_data[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    train_data[col] = train_data[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    train_data[col] = train_data[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    train_data[col] = train_data[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    train_data[col] = train_data[col].astype(np.float32)\n                else:\n                    train_data[col] = train_data[col].astype(np.float64)\n        else:\n            train_data[col] = train_data[col].astype('category')\n\n    end_mem = train_data.memory_usage().sum() \/ 1024**2\n    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) \/ start_mem))\n\n    return train_data","1ed15436":"sample = pd.read_csv(\"..\/input\/tabular-playground-series-oct-2021\/sample_submission.csv\")\ntrain  = pd.read_csv(\"..\/input\/tabular-playground-series-oct-2021\/train.csv\")\ntest = pd.read_csv(\"..\/input\/tabular-playground-series-oct-2021\/test.csv\")\n\nsample = reduce_mem_usage(sample)\ntrain = reduce_mem_usage(train)\ntest = reduce_mem_usage(test)\n\ngc.collect()","bf450a68":"train.head(5)","40f064b5":"test.head(5)","d292348e":"sample.head(5)","5d408bf2":"train.info()","b6fb5330":"train.describe()","bac5240f":"print(train.shape, test.shape, sample.shape)","e99da0be":"list(zip(train.columns, train.dtypes, train.nunique()))","f1a7e23d":"train.isnull().sum().sum(), test.isnull().sum().sum()  # No null in train and test datasets","bfb985ee":"# 'f22','f43','f242'-'f284'  each have only two value so we will use it as categorical features\ncat_feat = ['f22','f43']\nfor i in range(242,285):\n    cat_feat.append(f\"f{i}\")\nprint(cat_feat)","88caa2eb":"train[cat_feat].describe()","1e1a16d7":"# for col in cat_feat:\n#     print(col)\n#     print(train[col].value_counts(normalize=True))\n#     print(\"_\"*40)","933f4359":"sample_train = train.sample(10000)\nsample_test = test.sample(10000)\ndel train\ndel test\ndel sample\ngc.collect()","0522871d":"sample_train[cat_feat].hist(figsize=(20,20))\nplt.show()\nplt.tight_layout()","7614f7a1":"sample_train.drop(cat_feat, axis=1).hist(figsize=(50,50))\nplt.show()\nplt.tight_layout()","a1ac3f3b":"sns.countplot(x=sample_train.target)\nprint(sample_train.target.value_counts())","e980d190":"num_corr = sample_train.drop(cat_feat+['id'], axis=1)\nmask = np.triu(np.ones_like(num_corr, dtype = bool))\nplt.figure(figsize=(20,16))\nsns.heatmap(num_corr, mask = mask, cmap='magma')","9955e7df":"len(cat_feat)","aecb18e0":"fig, axes = plt.subplots(15,3, figsize=(30,90))\naxes = axes.flatten()\nfor idx, ax in enumerate(axes):\n        sns.kdeplot(sample_train[cat_feat[idx]], color=\"red\", label=\"train\", ax=ax)\n        sns.kdeplot(sample_test[cat_feat[idx]],  color=\"green\", label=\"test\", ax=ax)\n        ax.get_yaxis().set_visible(False)\n        ax.legend()\nfig.suptitle(\"distribution of train-test cat_feat\")\nfig.tight_layout()\nplt.show()","750fd69f":"num_feat = list(set(sample_train.columns)- set(cat_feat) - set(['id','target']))\nprint(num_feat,len(num_feat))","a962a140":"fig, axes = plt.subplots(80,3, figsize=(30,500))\naxes = axes.flatten()\nfor idx, ax in enumerate(axes):\n        sns.kdeplot(sample_train[num_feat[idx]], color=\"red\", label=\"train\", ax=ax)\n        sns.kdeplot(sample_test[num_feat[idx]],  color=\"green\", label=\"test\", ax=ax)\n        ax.get_yaxis().set_visible(False)\n        ax.set_title(f'f{num_feat[idx]}', loc = 'right', fontsize = 12)\n        ax.legend()\nfig.suptitle(\"distribution of train-test num_feat\")\nfig.tight_layout()\nplt.show()","1466ef6e":"## Thanks for sticking with me till end, I hope it helped you get some more insight of the data.","d1c0f72e":"Now we look at distribution of numerical features.","10f6d01e":"We look at the distribution of categorical features in our training dataset","010c88e9":"### reduce_mem_usage function has been taken from https:\/\/www.kaggle.com\/questions-and-answers\/148011","39bbe7cc":"#### below function makes our dataframe memory efficient.","9ba2d950":"## Now we compare the distribution of train and test set to see if they have same distribution or not.","54a25a35":"### For numerical features also distribution is almost same for both train and test set.","e5a5db6c":"Here we see that our target class is balanced.","3a0e4d1a":"### For categorical features train and test set have almost same distribution.","03d9c8b6":"### We check for if there is any null value in train or test set.","a910e86a":"### Here we look at no of unique values in each column and it's data type.","11526eb3":"Since we have have huge dataset so we take only sample of 10000 and since the sample is random so it represents our original dataset."}}