{"cell_type":{"b30dad72":"code","9777896a":"code","5e487663":"code","b89b2a26":"code","9ad2f4a1":"code","2011163f":"code","32c8b780":"code","37a62bcc":"code","46f7b072":"code","e04f3ddb":"code","cb224ccf":"code","048f704f":"code","c4610f8e":"code","ec7cddb3":"code","e7b4e4e1":"code","9e55395a":"code","add628cd":"code","26214b61":"code","ec30903e":"code","0b153b8f":"code","0bad0a2c":"markdown","209a87da":"markdown","e00ca022":"markdown","8022eba0":"markdown","9a1e337f":"markdown","a18bebe6":"markdown","0b81ccbd":"markdown","08b92765":"markdown","10ec8180":"markdown","c5a84d44":"markdown","715158f4":"markdown","07b3043a":"markdown","a9bd4a89":"markdown","3725755a":"markdown","17e19b1c":"markdown","e06ed222":"markdown"},"source":{"b30dad72":"# include all outputs\nfrom IPython.core.interactiveshell import InteractiveShell\nInteractiveShell.ast_node_interactivity = \"all\"","9777896a":"# import packages\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport nltk # For NLP\nimport re # For Regex \nimport string # For punctuation\nimport warnings # For warnings\n  \nwarnings.filterwarnings(action = 'ignore') \npd.option_context('display.max_colwidth', 1000)","5e487663":"# read data\ntrain_data = pd.read_csv(\"..\/input\/sentiment-analysis-of-tweets\/train.txt\",  sep = \",\")\nprint(\"Train Data has been read\")\ntest_data = pd.read_csv(\"..\/input\/sentiment-analysis-of-tweets\/test_samples.txt\",  sep = \",\")\nprint(\"Test Data has been read\")","b89b2a26":"# EDA\n## 1) type of data\nprint(\"Type of data : \", type(train_data))  ## dataframe","9ad2f4a1":"## 2) viewing top and bottom rows\nprint(\"Top 5 rows : \\n\")\ntrain_data.head()","2011163f":"## 3) getting row names and colnames\nprint(\"Row names : \\n\", train_data.index, \"\\n\") # row names\nprint(\"Column names : \\n\", train_data.columns, \"\\n\") # column names","32c8b780":"## 4) getting feature name and their types - to check for missing values\nprint(\"Feature names and their types : \\n\",train_data.info(), \"\\n\")","37a62bcc":"## 5) NULL values in the data\nprint (\"Null Value Statistics:\", '\\n',train_data.isnull().sum()) ## Sum will tell the total number of NULL values inside the dataset","46f7b072":"## 6) shape of data\nprint(\"Shape of Data : \\n\", train_data.shape, \"\\n\")\nprint(\"No. of rows in the data = \", train_data.shape[0])\nprint(\"No. of columns in the data = \", train_data.shape[1])","e04f3ddb":"## 7) see statistics for non- numerical features\nprint(\"See statistics for non- numerical columns : \")\ntrain_data.describe(include=['object'])","cb224ccf":"## 8) see statistics for categorical features\nprint(\"See counts for categorical columns : \")\ntrain_data['sentiment'].value_counts()","048f704f":"print(\"See percenatge for categorical columns : \")\nd = train_data['sentiment'].value_counts(normalize = True) * 100 # series\nprint(d)\n# d.index # col 1\n# d.values # col 2\n## plot for analysis\nplt.bar(x = d.index, height = d.values, label = d.index, color= ['green','blue','red'])\nplt.xlabel(\"Sentiments\")\nplt.ylabel(\"Percentage\")\nplt.title(\"Percentage of each type of tweet\")\nplt.show()","c4610f8e":"del train_data['tweet_id'] \nif (train_data.columns.tolist() == ['sentiment', 'tweet_text']):\n  print(\"Column 'tweet_id' has been removed from the data.\")","ec7cddb3":"from string import punctuation\nprint(\"DATA CLEANING -- \\n\")\n# emojis defined\nemoji_pattern = re.compile(\"[\"\n         u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n         u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n         u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n         u\"\\U00002702-\\U000027B0\"\n         u\"\\U000024C2-\\U0001F251\"\n         \"]+\", flags=re.UNICODE)\n\ndef replace_emojis(t):\n  '''\n  This function replaces happy unicode emojis with \"happy\" and sad unicode emojis with \"sad.\n  '''\n  emoji_happy = [\"\\U0001F600\", \"\\U0001F601\", \"\\U0001F602\",\"\\U0001F603\",\"\\U0001F604\",\"\\U0001F605\", \"\\U0001F606\", \"\\U0001F607\", \"\\U0001F609\", \n                \"\\U0001F60A\", \"\\U0001F642\",\"\\U0001F643\",\"\\U0001F923\",r\"\\U0001F970\",\"\\U0001F60D\", r\"\\U0001F929\",\"\\U0001F618\",\"\\U0001F617\",\n                r\"\\U000263A\", \"\\U0001F61A\", \"\\U0001F619\", r\"\\U0001F972\", \"\\U0001F60B\", \"\\U0001F61B\", \"\\U0001F61C\", r\"\\U0001F92A\",\n                \"\\U0001F61D\", \"\\U0001F911\", \"\\U0001F917\", r\"\\U0001F92D\", r\"\\U0001F92B\",\"\\U0001F914\",\"\\U0001F910\", r\"\\U0001F928\", \"\\U0001F610\", \"\\U0001F611\",\n                \"\\U0001F636\", \"\\U0001F60F\",\"\\U0001F612\", \"\\U0001F644\",\"\\U0001F62C\",\"\\U0001F925\",\"\\U0001F60C\",\"\\U0001F614\",\"\\U0001F62A\",\n                \"\\U0001F924\",\"\\U0001F634\", \"\\U0001F920\", r\"\\U0001F973\", r\"\\U0001F978\",\"\\U0001F60E\",\"\\U0001F913\", r\"\\U0001F9D0\"]\n\n  emoji_sad = [\"\\U0001F637\",\"\\U0001F912\",\"\\U0001F915\",\"\\U0001F922\", r\"\\U0001F92E\",\"\\U0001F927\", r\"\\U0001F975\", r\"\\U0001F976\", r\"\\U0001F974\",\n                       \"\\U0001F635\", r\"\\U0001F92F\", \"\\U0001F615\",\"\\U0001F61F\",\"\\U0001F641\", r\"\\U0002639\",\"\\U0001F62E\",\"\\U0001F62F\",\"\\U0001F632\",\n                       \"\\U0001F633\", r\"\\U0001F97A\",\"\\U0001F626\",\"\\U0001F627\",\"\\U0001F628\",\"\\U0001F630\",\"\\U0001F625\",\"\\U0001F622\",\"\\U0001F62D\",\n                       \"\\U0001F631\",\"\\U0001F616\",\"\\U0001F623\"\t,\"\\U0001F61E\",\"\\U0001F613\",\"\\U0001F629\",\"\\U0001F62B\", r\"\\U0001F971\",\n                       \"\\U0001F624\",\"\\U0001F621\",\"\\U0001F620\", r\"\\U0001F92C\",\"\\U0001F608\",\"\\U0001F47F\",\"\\U0001F480\", r\"\\U0002620\"]\n\n  words = t.split()\n  reformed = []\n  for w in words:\n    if w in emoji_happy:\n      reformed.append(\"happy\")\n    elif w in emoji_sad:\n      reformed.append(\"sad\") \n    else:\n      reformed.append(w)\n  t = \" \".join(reformed)\n  return t\n\n\ndef replace_smileys(t):\n  '''\n  This function replaces happy smileys with \"happy\" and sad smileys with \"sad.\n  '''\n  emoticons_happy = set([':-)', ':)', ';)', ':o)', ':]', ':3', ':c)', ':>', '=]', '8)', '=)', ':}', ':D',\n    ':^)', ':-D', ':D', '8-D', '8D', 'x-D', 'xD', 'X-D', 'XD', '=-D', '=D',\n    '=-3', '=3', ':-))', \":'-)\", \":')\", ':*', ':^*', '>:P', ':-P', ':P', 'X-P',\n    'x-p', 'xp', 'XP', ':-p', ':p', '=p', ':-b', ':b', '>:)', '>;)', '>:-)', '<3'])\n\n  emoticons_sad = set([':L', ':-\/', '>:\/', ':S', '>:[', ':@', ':-(', ':[', ':-||', '=L', ':<',\n    ':-[', ':-<', '=\\\\', '=\/', '>:(', ':(', '>.<', \":'-(\", \":'(\", ':\\\\', ':-c',\n    ':c', ':{', '>:\\\\', ';('])  \n\n  words = t.split()\n  reformed = []\n  for w in words:\n    if w in emoticons_happy:\n      reformed.append(\"happy\")\n    elif w in emoticons_sad:\n      reformed.append(\"sad\") \n    else:\n      reformed.append(w)\n  t = \" \".join(reformed)\n  return t\n\ndef replace_contractions(t):\n  '''\n  This function replaces english lanuage contractions like \"shouldn't\" with \"should not\"\n  '''\n  cont = {\"aren't\" : 'are not', \"can't\" : 'cannot', \"couln't\": 'could not', \"didn't\": 'did not', \"doesn't\" : 'does not',\n  \"hadn't\": 'had not', \"haven't\": 'have not', \"he's\" : 'he is', \"she's\" : 'she is', \"he'll\" : \"he will\", \n  \"she'll\" : 'she will',\"he'd\": \"he would\", \"she'd\":\"she would\", \"here's\" : \"here is\", \n   \"i'm\" : 'i am', \"i've\"\t: \"i have\", \"i'll\" : \"i will\", \"i'd\" : \"i would\", \"isn't\": \"is not\", \n   \"it's\" : \"it is\", \"it'll\": \"it will\", \"mustn't\" : \"must not\", \"shouldn't\" : \"should not\", \"that's\" : \"that is\", \n   \"there's\" : \"there is\", \"they're\" : \"they are\", \"they've\" : \"they have\", \"they'll\" : \"they will\",\n   \"they'd\" : \"they would\", \"wasn't\" : \"was not\", \"we're\": \"we are\", \"we've\":\"we have\", \"we'll\": \"we will\", \n   \"we'd\" : \"we would\", \"weren't\" : \"were not\", \"what's\" : \"what is\", \"where's\" : \"where is\", \"who's\": \"who is\",\n   \"who'll\" :\"who will\", \"won't\":\"will not\", \"wouldn't\" : \"would not\", \"you're\": \"you are\", \"you've\":\"you have\",\n   \"you'll\" : \"you will\", \"you'd\" : \"you would\", \"mayn't\" : \"may not\"}\n  words = t.split()\n  reformed = []\n  for w in words:\n    if w in cont:\n      reformed.append(cont[w])\n    else:\n      reformed.append(w)\n  t = \" \".join(reformed)\n  return t  \n\ndef remove_single_letter_words(t):\n  '''\n  This function removes words that are single characters\n  '''\n  words = t.split()\n  reformed = []\n  for w in words:\n    if len(w) > 1:\n      reformed.append(w)\n  t = \" \".join(reformed)\n  return t  \n\nprint(\"Cleaning the tweets from the data.\\n\")\nprint(\"Replacing handwritten emojis with their feeling associated.\")\nprint(\"Convert to lowercase.\")\nprint(\"Replace contractions.\")\nprint(\"Replace unicode emojis with their feeling associated.\")\nprint(\"Remove all other unicoded emojis.\")\nprint(\"Remove NON- ASCII characters.\")\nprint(\"Remove numbers.\")\nprint(\"Remove \\\"#\\\". \")\nprint(\"Remove \\\"@\\\". \")\nprint(\"Remove usernames.\")\nprint(\"Remove \\'RT\\'. \")\nprint(\"Replace all URLs and Links with word \\'URL\\'.\")\nprint(\"Remove all punctuations.\")\nprint(\"Removes single letter words.\\n\")\n\ndef dataclean(t):\n  '''\n  This function cleans the tweets.\n  '''\n  t = replace_smileys(t) # replace handwritten emojis with their feeling associated\n  t = t.lower() # convert to lowercase\n  t = replace_contractions(t) # replace short forms used in english  with their actual words\n  t = replace_emojis(t) # replace unicode emojis with their feeling associated\n  t = emoji_pattern.sub(r'', t) # remove emojis other than smiley emojis\n  t = re.sub('\\\\\\\\u[0-9A-Fa-f]{4}','', t) # remove NON- ASCII characters\n  t = re.sub(\"[0-9]\", \"\", t) # remove numbers # re.sub(\"\\d+\", \"\", t)\n  t = re.sub('#', '', t) # remove '#'\n  t = re.sub('@[A-Za-z0\u20139]+', '', t) # remove '@'\n  t = re.sub('@[^\\s]+', '', t) # remove usernames\n  t = re.sub('RT[\\s]+', '', t) # remove retweet 'RT'\n  t = re.sub('((www\\.[^\\s]+)|(https?:\/\/[^\\s]+))', '', t) # remove links (URLs\/ links)\n  t = re.sub('[!\"$%&\\'()*+,-.\/:@;<=>?[\\\\]^_`{|}~]', '', t) # remove punctuations\n  t = t.replace('\\\\\\\\', '')\n  t = t.replace('\\\\', '')\n  t = remove_single_letter_words(t) # removes single letter words\n  \n  return t\n\ntrain_data['tweet_text'] = train_data['tweet_text'].apply(dataclean)\nprint(\"Tweets have been cleaned.\")","e7b4e4e1":"def freq_words(tokens):\n  '''See frequency distribution of words'''\n  from nltk.probability import FreqDist\n  \n  fdist = FreqDist(tokens)\n  print(fdist) # total 508935 values and 56653 unique values\n    \n  # top 20 most common words\n  print(\"Top 10 most common words in the data. \\n\")\n  df = pd.DataFrame(fdist.most_common(20), columns=['Tokens', 'Frequency'])\n  print(df)\n\n  # plot of top 20 most common words\n  fdist.plot(20, cumulative = False)\n  plt.show()\n\ndef word_cloud(tokens):\n  '''A word cloud (also known as text clouds or tag clouds) is a visualization, \n  the more a specific word appears in the text, the bigger and bolder it appears in the word cloud.\n  '''\n  from wordcloud import WordCloud\n  w = ' '.join([i for i in tokens])\n  wc = WordCloud(width = 1000, height = 700, random_state=21, max_font_size=110).generate(w)\n  plt.imshow(wc, interpolation = \"bilinear\")\n  plt.axis('off')\n  plt.show()\n\ndef preprocess(t):\n  # downloading necessary packages\n  from nltk.tokenize import word_tokenize\n  from nltk.corpus import stopwords\n  stop_words = set(stopwords.words('english'))\n   \n  from nltk.stem.wordnet import WordNetLemmatizer\n  lem = WordNetLemmatizer()\n  \n  # tokenization\n  tw = []  \n  tw = word_tokenize(t)  \n  \n  # remove the stopwords\n  fs = [] # filtered list after removing stop words\n  fs = ([i for i in tw if i not in stop_words])\n  \n  # lemmatization  \n  ns = []\n  for i in fs:\n    ns.append(lem.lemmatize(i, 'v'))\n  ns = ' '.join(ns)\n  return ns  \n\nprint(\"Pre - processing the tweets.\\n\")\nprint(\"Downloading necessary packages for preprocessing.\")\n## for tokenization\nif (nltk.download(\"punkt\")):\n  print(\"\\\"punkt\\\" is downloaded.\")\n## for stopwords\nif (nltk.download('stopwords')):\n  print(\"\\\"Stopwords\\\" are downloaded.\")\n## for lemmatization\nif (nltk.download('wordnet')):\n  print(\"Algorithm for lemmatization is downloaded.\")  \nprint(\"-\" * 50)\nprint(\"\\nSteps involved are : \")\nprint(\"\\nStep 1 - \\\"Tokenization\\\". \")\nprint(\"Importing the necessary functions required for tokenization.\")\n\nprint(\"Necessary functions are downloaded.\")\nprint(\"\\nStep 2 - \\\"Removing the stopwprds\\\". \")\nprint(\"Importing the necessary functions required for removing stopwords.\")\nprint(\"Necessary functions are downloaded.\")\nprint(\"Stop words in ENGLISH language are : \")\n\nprint(\"\\nStep 3 - \\\"Lemmatization\\\". \")\nprint(\"Importing the necessary functions required for lemmatization.\")\nprint(\"Necessary functions are downloaded.\")\nprint(\"Loading the algorithm for Lemmatization.\")\nprint(\"Algorithm for lemmatization is loaded.\\n\")\n  \npp_list = []\nfor tweet in train_data['tweet_text']:\n  pp_list.append(preprocess(tweet))\nprint(\"After preprocessing.\", pp_list[:5])\n\n# for graphical display of preprocessed data\nfrom nltk.tokenize import word_tokenize\ntokens = []\nfor i in pp_list:\n  tokens.extend(word_tokenize(i))\nprint(\"Top 20 words used in the tweets are : \\n\", tokens[:20])\nprint(\"\\nFrequency distribution of top 20 words in the tweets.\\n\")\nfreq_words(tokens)\nprint(\"\\nWord Cloud for the pre- processed data.\")\nword_cloud(tokens)","9e55395a":"# Dividing Data to Training and Validation Sets\nfrom sklearn.model_selection import train_test_split  \nx = pp_list\ny = train_data['sentiment'].values\nx_train, x_val, y_train, y_val = train_test_split(x, y, test_size=0.2, random_state = 1)\nprint(\"Number of data points in training data :\\n\")\nprint(\"X = \", len(x_train))\nprint(\"Y = \", len(y_train))\n\nprint(\"\\nNumber of data points in validation data :\\n\")\nprint(\"X = \", len(x_val))\nprint(\"Y = \", len(y_val))","add628cd":"print(\"Now convert these pre processed text to numeric format to send it to the model.\")\nprint(\"Using TF-IDF approach to covert text to numbers.\\n\")\n\nfrom sklearn.feature_extraction.text import TfidfTransformer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\n#The attribute max_features  specifies the number of most occurring words for which you want to create feature vectors. \n#Less frequently occurring words do not play a major role in classification. \n#Therefore we only retain 2000 most frequently occurring words in the dataset. \n#> min_df  value of 5 specifies that the word must occur in at least 5 documents. \n#> max_df  value of 0.7 percent specifies that the word must not occur in more than 70 percent of the documents. \n#rationale behind choosing 70 percent as the threshold is that words occurring in more than 70 percent of the documents are too common and are less likely to play any role in the classification of sentiment.\n\n#To convert your dataset into corresponding TF-IDF feature vectors, you need to call the fit_transform  method on TfidfVectorizer  class and pass it our preprocessed dataset.\n\nvectorizer = TfidfVectorizer()\ntrain_vectors = vectorizer.fit_transform(x_train)\n\nval_vectors = vectorizer.transform(x_val)\nprint(\"Shape of training data : \",train_vectors.shape, \"\\nShape of validation data : \",val_vectors.shape)","26214b61":"# building a LR Model using tf- idf approach\nprint(\"Building a Logistic Regression Model using tf- idf approach\")\nfrom sklearn.linear_model import LogisticRegression\nmodel_LR = LogisticRegression(solver='lbfgs')\nmodel_LR.fit(train_vectors, y_train)\nprint(\"\\n\\nOur Sentiment Analyzer from Logistic Regression is ready and trained.\")","ec30903e":"def performance(model, y_true, vectors):\n  from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n  print(\"Predicting the sentiments...\")\n  y_pred = model.predict(vectors)   #y_pred_RF = model_RF.predict(val_vectors)\n  df = pd.DataFrame({'actual' : y_true, 'predicted' : y_pred})\n  print(\"\\nAnalysis after prediction : \\n\")\n  d = df['predicted'].value_counts(normalize = True) * 100 # series\n  print(d)\n  ## plot for analysis\n  plt.bar(x = d.index, height = d.values, label = d.index, color= ['green','blue','red'])\n  plt.xlabel(\"Sentiments\")\n  plt.ylabel(\"Percentage\")\n  plt.title('Sentiment Analysis')\n  plt.show()\n  \n  cm = confusion_matrix(y_true,y_pred)\n  crp = classification_report(y_true,y_pred)\n  acc = accuracy_score(y_true,y_pred)\n  return (cm, crp, acc)\n\nprint(\"Let us test the performance of our model on the validation set to predict the sentiment labels using the different ML models trained.\")\nprint(\"PERFORMANCE OF LOGISTIC REGRESSION MODEL : \\n\")\nperform = performance(model_LR, y_val, val_vectors)\nprint(\"Confusion Matrix :\\n\", perform[0])  \nprint(\"classification report: \\n\", perform[1])  \nprint(\"Accuracy score  = \", perform[2] *100)\nprint(\"-\" * 100)","0b153b8f":"print(\"-\" *20, \"Testing the model of Test dataset\", \"-\" * 20)\ntest_data.head()\nprint(\"\\nShape of test data = \", test_data.shape)\ntest_data['tweet_text'] = test_data['tweet_text'].apply(dataclean)\n\nprint(\"-\" * 50)\nprint(\"\\nBuilding Features for the test data set.\")\ntest_list = []\nfor tweet in test_data['tweet_text']:\n  test_list.append(preprocess(tweet))\nprint(\"After preprocessing test tweets look like : \\n\", test_list[:5], \"\\n\")\nprint(\"-\" * 50)\n\ntest_vectors = vectorizer.transform(test_list)\nprint(\"Shape of testing features : \",test_vectors.shape)\nprint(\"-\" * 50)\n\nprint(\"\\nLet us test the performance of our model on the testing set to predict the sentiment labels.\")\nfrom sklearn.metrics import classification_report, confusion_matrix, accuracy_score\npredicted = model_LR.predict(test_vectors)\n\nprint(\"Results : \\n\")\nresults = pd.DataFrame({'tweet_id' : test_data['tweet_id'].astype(str), 'sentiment' : predicted}, \n                       columns = ['tweet_id', 'sentiment'])\nprint(results)\nresults.to_csv(\"results_LR.csv\", sep= \",\", index = False)\nprint(\"-\" * 50)\n\nprint(\"Analysis on the test dataset.. \\n\")\nd = results['sentiment'].value_counts(normalize = True) * 100\nif (d.index[np.argmax(d.values)] == 'positive'): \n  print(\"\\nOverall Positive Sentiment.\")\n  print(\"Positive Sentiment Percentage = \", d.values[np.argmax(d.values)])\nelif (d.index[np.argmax(d.values)] == 'negative'): \n  print(\"\\nOverall Negative Sentiment.\")\n  print(\"Negative Sentiment Percentage = \", d.values[np.argmax(d.values)])\nelse:\n  print(\"\\nOverall Neutral Sentiment.\")\n  print(\"Neutral Sentiment Percentage = \", d.values[np.argmax(d.values)])  ","0bad0a2c":"### DATA CLEANING\n1.  Replacing handwritten emojis with their feeling associated.\n2.  Convert to lowercase.\n3.  Replace contractions.\n4.  Replace unicode emojis with their feeling associated.\n5.  Remove all other unicoded emojis.\n6.  Remove NON- ASCII characters.\n7.  Remove numbers.\n8.  Remove \"#\". \n9.  Remove \"@\". \n10. Remove usernames.\n11. Remove 'RT'. \n12. Replace all URLs and Links with word 'URL'.\n13. Remove all punctuations.\n14. Removes single letter words.","209a87da":"#### UNBALANCED DATA \n##### **Positive** tweets occupy **42.27%** of the total tweets.\n##### **Neutral** tweets occupy **41.99%** of the total tweets.\n##### **Negative** tweets occupy **15.78%** of the total tweets.","e00ca022":"### Performing EXPLORATORY DATA ANALYSIS..","8022eba0":"#### No column has MISSING VALUES.","9a1e337f":"### MODEL BUILDING - LOGISTIC REGRESSION","a18bebe6":"### Loading the datasets..","0b81ccbd":"### LOGISTIC REGRESSION Model gives an accuracy of 64.29%","08b92765":"### DATA PRE-PROCESSING\n1. Tokenization\n2. Removing Stopwords\n3. Lemmatization\n\n### Display FREQUENCY DISTRIBUTION OF TOP 20 WORDS in the corpus\n### Display of Word Cloud","10ec8180":"### From above analysis, we have found that 'Tweet_id' is NOT required in ouur analysis. Therefore, we remove unncessary columns from our training data","c5a84d44":"### TESTING THE PERFORMANCE OF SUPPORT VECTOR MACHINE MODEL ON VALIDATION DATASET.\n1. Confusion Matrix\n2. Precision, Recall, F1 Score\n3. Accuracy","715158f4":"### TESTING the LOGISTIC REGRESSION model on test dataset","07b3043a":"### Importing the necsessary packages","a9bd4a89":"### TRAIN AND VALIDATION Split - 80% and 20% of the training data","3725755a":"#### sentiment column is a ordinal data column that contains 3 labels - **positive, neutral and negative**","17e19b1c":"### FEATURE GENERATION","e06ed222":"### LOGISTIC REGRESSION Model predicts major sentiments as Neutral sentiments with a percentage of 54.16%"}}