{"cell_type":{"473da081":"code","decd5399":"code","62c62cd8":"code","69ae0c71":"code","20619728":"code","8f111756":"code","69428515":"code","d6549b1c":"code","02293563":"code","c9dd57be":"code","25d4bcf2":"code","c43f70bf":"code","081cebcb":"code","6b40050f":"code","7510d2af":"code","dcf2651f":"code","320b1725":"code","624357f0":"code","e856fc84":"code","c504b545":"code","a1165f9d":"code","948f0d00":"code","7e852341":"code","8591b463":"code","75ed769e":"code","8ca47980":"code","99340abf":"code","901a66a6":"code","8389195f":"code","ad4ed7bd":"markdown","d4f19882":"markdown","b147d875":"markdown","deea74a6":"markdown","197ab85a":"markdown","421b19ee":"markdown","7f1f6f28":"markdown","8a3102cb":"markdown","c722c18b":"markdown","299c8f44":"markdown","232cebfe":"markdown","00624332":"markdown","64f24220":"markdown","96d1b2ce":"markdown","60eff830":"markdown","cd799353":"markdown","8a724a81":"markdown","1046570b":"markdown","2369de95":"markdown","49c6c36f":"markdown","284046fa":"markdown","6d6c2408":"markdown","5e9a4a0d":"markdown","bafdb1cb":"markdown","6820d095":"markdown","d8c6d4d7":"markdown","f60324c3":"markdown","6ebb9667":"markdown","55484197":"markdown","2659da6b":"markdown","29f853ac":"markdown","c5744432":"markdown","5e41b62f":"markdown","5cf2785f":"markdown","09dc9f89":"markdown"},"source":{"473da081":"import numpy as np\nimport pandas as pd\n\ntrain = pd.read_csv('..\/input\/train.csv')\ntest = pd.read_csv('..\/input\/test.csv')\nfull_data = train.drop(columns = 'SalePrice').merge(test, how = 'outer').drop(columns = 'Id')","decd5399":"# missing values\nfull_data.isnull().sum()[full_data.isnull().sum() > 0]","62c62cd8":"# fill NA and 0\nto_na = ['Alley', \n         'MasVnrType', \n         'BsmtQual', \n         'BsmtCond', \n         'BsmtExposure', \n         'BsmtFinType1', \n         'BsmtFinType2', \n         'FireplaceQu', \n         'GarageType', \n         'GarageFinish', \n         'GarageQual', \n         'GarageCond', \n         'PoolQC', \n         'Fence',\n         'MiscFeature']\nto_zero = ['MasVnrArea',\n           'BsmtFinSF1',\n           'BsmtFinSF2',\n           'BsmtUnfSF',\n           'TotalBsmtSF',\n           'BsmtFullBath',\n           'BsmtHalfBath'\n           'GarageYrBlt']\nto_zero = dict.fromkeys(to_zero, 0)\nto_fill = dict.fromkeys(to_na, 'NA')\nto_fill.update(to_zero)\nfull_data = full_data.fillna(value = to_fill)","69ae0c71":"# fill means\nfull_data.LotFrontage = full_data.groupby(['Neighborhood'])['LotFrontage'].transform(lambda x: x.fillna(x.mean()))\nfull_data.MSZoning = full_data.groupby(['MSSubClass'])['MSZoning'].transform(lambda x: x.fillna(x.value_counts()[0]))\n\n# fill modes\nlist_nulls = list(full_data.isnull().sum()[full_data.isnull().sum() > 0].index)\nfull_data[list_nulls] = full_data.groupby(['Neighborhood'])[list_nulls].transform(lambda x: x.fillna(x.value_counts().index[0]))\n","20619728":"to_months = dict(zip([x for x in range(1,13)],\n                      ['January', 'February', \n                      'March', 'April', 'May', \n                      'June', 'July', 'August', \n                      'September', 'October', \n                      'November', 'December'],))\nfull_data['MoSold'] = full_data['MoSold'].map(to_months)","8f111756":"# add new variables\nfull_data['TotalSF'] = full_data['TotalBsmtSF'] + full_data['1stFlrSF']+ full_data['2ndFlrSF']\nfull_data['HouseAge'] = full_data['YrSold'] - full_data['YearBuilt']\nfull_data['RemodAge'] = full_data['YrSold'] - full_data['YearRemodAdd']\n\n# drop old and GarageYrBlt\nfull_data = full_data.drop(columns = ['YrSold', 'YearRemodAdd', \n                                      'YearBuilt', 'GarageYrBlt'])","69428515":"# correlation heatmap\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nsns.set_style('darkgrid')\nplt.figure(figsize = (20, 18))\n\ntrain_corr = full_data[full_data.dtypes[full_data.dtypes != 'object'].index]\ntrain_corr = train_corr[:train.shape[0]]\nfor_pairplot = pd.concat([pd.DataFrame(train.SalePrice), train_corr], axis = 1)\ntrain_corr = for_pairplot.corr()\n\nsns.heatmap(train_corr, cmap = 'YlGnBu_r', annot = True).set_title('Figure 1: Correlation Heatmap for Ames Data')","d6549b1c":"# most correlated by absolute value\nmost_corr = train_corr['SalePrice'].map(lambda x: abs(x)).sort_values(ascending = False)[:11].index\nprint(most_corr)","02293563":"print(full_data['GarageCars'].describe())\nprint(full_data['FullBath'].describe())\nprint(full_data['TotRmsAbvGrd'].describe())","c9dd57be":"fig, axs = plt.subplots(nrows = 3, ncols = 2, figsize = (20, 27))\nsns.scatterplot('TotalSF', 'SalePrice', data = for_pairplot, ax = axs[0][0])\nsns.scatterplot('GrLivArea', 'SalePrice', data = for_pairplot, ax = axs[0][1])\nsns.scatterplot('GarageArea', 'SalePrice', data = for_pairplot, ax = axs[1][0])\nsns.scatterplot('TotalBsmtSF', 'SalePrice', data = for_pairplot, ax = axs[1][1])\nsns.scatterplot('1stFlrSF', 'SalePrice', data = for_pairplot, ax = axs[2][0])\nsns.scatterplot('HouseAge', 'SalePrice', data = for_pairplot, ax = axs[2][1])\nplt.suptitle('Figure 2: Plots for SalePrice and Most Correlated Variables')","25d4bcf2":"to_drop = [523, 1298, 581, 1190, 1061, 691, 1182, 185]\nfull_data = full_data.drop(to_drop)","c43f70bf":"fd_skew = full_data.skew(axis = 0).sort_values(ascending = False)\nfd_skew = fd_skew.drop(['MSSubClass', 'OverallQual', 'OverallCond',\n                        'HouseAge', 'RemodAge'])\nprint(fd_skew)","081cebcb":"from scipy import stats\nplt.figure(figsize = (12, 7))\nsns.distplot(train.SalePrice, fit = stats.f).set_title('Figure 3: Distribution of SalePrice')","6b40050f":"fig, axs = plt.subplots(nrows = 3, ncols = 2, figsize = (20, 27))\nsns.residplot('TotalSF', 'SalePrice', data = for_pairplot, ax = axs[0][0])\nsns.residplot('GrLivArea', 'SalePrice', data = for_pairplot, ax = axs[0][1])\nsns.residplot('GarageArea', 'SalePrice', data = for_pairplot, ax = axs[1][0])\nsns.residplot('TotalBsmtSF', 'SalePrice', data = for_pairplot, ax = axs[1][1])\nsns.residplot('1stFlrSF', 'SalePrice', data = for_pairplot, ax = axs[2][0])\nsns.residplot('HouseAge', 'SalePrice', data = for_pairplot, ax = axs[2][1])\nplt.suptitle('Figure 4: Residual Plots of Most Correlated Continuous Variables')","7510d2af":"# adjust skewness with log(x+1)\nskew_pos = fd_skew[fd_skew > 0.5].index\nfull_data[skew_pos] = full_data[skew_pos].transform(lambda x: np.log(x+1))","dcf2651f":"# create standard normal variables\nto_z_score = ['HouseAge', 'RemodAge',\n              'BsmtFullBath', 'BsmtHalfBath', 'FullBath',\n              'HalfBath', 'BedroomAbvGr', 'KitchenAbvGr',\n              'TotRmsAbvGrd', 'Fireplaces', 'GarageCars']\nfull_data[to_z_score] = full_data[to_z_score].transform(lambda x: (x - np.mean(x)) \/ np.std(x))","320b1725":"# Street, Alley\ngrav_map = {'NA' : 0, 'Grvl' : 1, 'Pave' : 2}\nfull_data[['Street', 'Alley']] = full_data[['Street', 'Alley']].replace(grav_map)\n\n# LotShape \nshape_map = {'IR3' : 0, 'IR2' : 1, 'IR1' : 2, 'Reg' : 3}\nfull_data[['LotShape']] = full_data[['LotShape']].replace(shape_map)\n\n# Utilities\nutil_map = {'ELO' : 0 , 'NoSeWa' : 1, 'NoSewr' : 2, 'AllPub' : 3}\nfull_data[['Utilities']] = full_data[['Utilities']].replace(util_map)\n\n# LandSlope\nslope_map = {'Sev' : 0, 'Mod' : 1, 'Gtl' : 2}\nfull_data[['LandSlope']] = full_data[['LandSlope']].replace(slope_map)\n\n# HouseStyle\nhouse_map = {'1Story' : 0, '1.5Unf' : 1, '1.5Fin' : 2, 'SFoyer' : 3, 'SLvl' : 4,\n             '2Story' : 5, '2.5Unf' : 6, '2.5Fin' : 7} \nfull_data[['HouseStyle']] = full_data[['HouseStyle']].replace(house_map)\n\n# Quality\nqual_map = {'NA' : 0, 'Po' : 1, 'Fa' : 2, 'TA' : 3, 'Gd' : 4, 'Ex' : 5}\nby_qual = ['ExterQual', 'ExterCond', 'BsmtQual', 'BsmtCond',\n           'HeatingQC', 'KitchenQual', 'FireplaceQu', 'GarageQual',\n           'GarageCond', 'PoolQC']\nfull_data[by_qual] = full_data[by_qual].replace(qual_map)\n\n# BsmtExposure\nexpo_map = {'NA' : 0, 'No' : 1, 'Mn' : 2, 'Av' : 3, 'Gd' : 4} \nfull_data['BsmtExposure'] = full_data['BsmtExposure'].replace(expo_map)\n\n#BsmtFinType1, 2\nfin_map = {'NA' : 0, 'Unf' : 1, 'LwQ' : 2, 'Rec' : 3, 'BLQ' : 4, 'ALQ' : 5,\n           'GLQ' : 6}\nfull_data[['BsmtFinType1', 'BsmtFinType2']] = full_data[['BsmtFinType1', 'BsmtFinType2']].replace(fin_map)\n\n# Electrical\nelec_map = {'FuseP' : 0, 'FuseF' : 1, 'Mix' : 2, 'FuseA' : 3, 'SBkr' : 4, 'SBrkr' : 4}\nfull_data['Electrical'] = full_data['Electrical'].replace(elec_map)\n\n# CentralAir\nbin_map = {'N' : 0, 'No' : 0, 'Y' : 1, 'Yes' : 1}\nfull_data['CentralAir'] = full_data['CentralAir'].replace(bin_map)\n\n# Functional\nfunc_map = {'Sal' : 0, 'Sev' : 1, 'Maj2' : 2, 'Maj1' : 3, 'Mod' : 4,\n            'Min2' : 5, 'Min1' : 6, 'Typ' : 7}\nfull_data['Functional'] = full_data['Functional'].replace(func_map)\n\n# GarageFinish\nfing_map = {'NA' : 0, 'Unf' : 1, 'RFn' : 2, 'Fin' : 3} \nfull_data['GarageFinish'] = full_data['GarageFinish'].replace(fing_map)\n\n# PavedDrive\ndrive_map = {'N' : 0, 'P' : 1, 'Y' : 2} \nfull_data['PavedDrive'] = full_data['PavedDrive'].replace(drive_map)\n\n# Fence\nfence_map = {'NA' : 0, 'MnWw' : 1, 'GdWo' : 2, 'MnPrv' : 3, 'GdPrv' : 4}\nfull_data['Fence'] = full_data['Fence'].replace(fence_map)","624357f0":"# shift overallqual and cond\nfull_data[['OverallQual', 'OverallCond']] = full_data[['OverallQual', 'OverallCond']] - 1","e856fc84":"to_min_max = ['Street', 'Alley', 'LotShape', 'HouseStyle', \n              'Utilities', 'ExterQual', 'ExterCond', 'BsmtQual', \n              'BsmtCond', 'HeatingQC', 'KitchenQual', 'FireplaceQu', \n              'GarageQual', 'GarageCond', 'PoolQC', 'BsmtExposure', \n              'BsmtFinType1', 'BsmtFinType2', 'Electrical', 'CentralAir', \n              'Functional', 'GarageFinish', 'PavedDrive', 'Fence', \n              'OverallQual', 'OverallCond', 'LandSlope']\n\nfull_data[to_min_max] = full_data[to_min_max].apply(lambda x: (x - np.min(x)) \/ (np.max(x) - np.min(x)))\n","c504b545":"# for some reason did not convert type to int earlier\nfull_data.CentralAir = full_data.CentralAir.astype(int)\n\n# manual dummies\ndummies_mssub = pd.get_dummies(full_data.MSSubClass).add_prefix('MSSubClass_')\ndummies_mszoning = pd.get_dummies(full_data.MSZoning).add_prefix('MSZoning_')\ndummies_lcontour = pd.get_dummies(full_data.LandContour).add_prefix('LandContour_')\ndummies_lconfig = pd.get_dummies(full_data.LotConfig).add_prefix('LotConfig_')\ndummies_month = full_data.MoSold.str.get_dummies().add_prefix('MoSold_')\ndummies_bldg = full_data.BldgType.str.get_dummies().add_prefix('BldgType_')\ndummies_nbr = full_data.Neighborhood.str.get_dummies().add_prefix('Neighborhood_')\ndummies_cond1 = full_data.Condition1.str.get_dummies().add_prefix('Cond1_')\ndummies_cond2 = full_data.Condition2.str.get_dummies().add_prefix('Cond2_')\ndummies_roofs = full_data.RoofStyle.str.get_dummies().add_prefix('RoofStyle_')\ndummies_roofm = full_data.RoofMatl.str.get_dummies().add_prefix('RoofMatl_')\ndummies_ext1 = full_data.Exterior1st.str.get_dummies().add_prefix('Ext1_')\ndummies_ext2 = full_data.Exterior2nd.str.get_dummies().add_prefix('Ext2_')\ndummies_mvt = full_data.MasVnrType.str.get_dummies().add_prefix('MasVnrType_')\ndummies_found = full_data.Foundation.str.get_dummies().add_prefix('Foundaton_')\ndummies_heat = full_data.Heating.str.get_dummies().add_prefix('Heating_')\ndummies_gart = full_data.GarageType.str.get_dummies().add_prefix('GarageType_')\ndummies_misc = full_data.MiscFeature.str.get_dummies().add_prefix('Misc_')\ndummies_salet = full_data.SaleType.str.get_dummies().add_prefix('SaleType_')\ndummies_salec = full_data.SaleCondition.str.get_dummies().add_prefix('SaleCond_')\n\n# out with the old\ndrop_vars = ['MSSubClass', 'MSZoning', 'LandContour', 'LotConfig', 'MoSold',\n             'BldgType', 'Neighborhood', 'Condition1', 'Condition2', 'RoofStyle',\n             'RoofMatl', 'Exterior1st', 'Exterior2nd', 'MasVnrType', 'Foundation',\n             'Heating', 'GarageType', 'MiscFeature', 'SaleType', 'SaleCondition']\n\nfull_data_dropped = full_data.drop(columns = drop_vars)\n\n# in with the new\nfull_data_dummies = pd.concat([full_data_dropped,\n                               dummies_mssub,\n                               dummies_mszoning,\n                               dummies_lcontour,\n                               dummies_lconfig,\n                               dummies_month,\n                               dummies_bldg,\n                               dummies_nbr,\n                               dummies_cond1,\n                               dummies_cond2,\n                               dummies_roofs,\n                               dummies_roofm,\n                               dummies_ext1,\n                               dummies_ext2,\n                               dummies_mvt,\n                               dummies_found,\n                               dummies_heat,\n                               dummies_gart,\n                               dummies_misc,\n                               dummies_salet,\n                               dummies_salec], axis = 1)\n\nprint(full_data_dummies.shape)","a1165f9d":"train_clean = full_data_dummies[:(train.shape[0]-len(to_drop))]\ntest_clean = full_data_dummies[train_clean.shape[0]:]\n\nprint([train_clean.shape, test_clean.shape])\n","948f0d00":"from sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import cross_val_predict\nfrom collections import namedtuple\n\ndef get_results(model, regr):\n    # for error and prediction from model cross-validation\n    ErrReturn = namedtuple('Output', 'Error Predictions')\n    model.fit(regr, response)\n    pred = cross_val_predict(model, regr, response, cv = 10)\n    err = np.sqrt(mean_squared_error(pred, response))\n    return ErrReturn(err, pred)\n\npredictors = train_clean\nresponse = np.log(train.SalePrice.drop(to_drop)) # was not transformed earlier, outliers dropped","7e852341":"## LASSO","8591b463":"from sklearn.linear_model import LassoCV\nfrom sklearn.linear_model import Lasso\nmodel_lasso_cv = LassoCV(alphas = [0.0001, 0.0005, 0.001, 0.005, 0.01, 0.05,\n                               0.1, 0.5, 1], max_iter = 50000, cv = 10)\nmodel_lasso_cv.fit(predictors, response)\nmodel_lasso = Lasso(alpha = model_lasso_cv.alpha_, max_iter = 50000)\nresults_lasso = get_results(model_lasso, predictors)\n\nprint(f\"The RMSE for the LASSO model was {round(results_lasso.Error, 5)}\")","75ed769e":"# Plot important coefficients\ncoefs = pd.Series(model_lasso.coef_, index = predictors.columns)\nprint(f\"LASSO picked {sum(coefs != 0)} features and eliminated the other {sum(coefs == 0)} features\")\nimp_coefs = pd.concat([coefs.sort_values().head(10),\n                     coefs.sort_values().tail(10)])\nsns.barplot(x = imp_coefs.index, y = imp_coefs)\nplt.title(\"Figure 5: Coefficients in the LASSO Model\")\nplt.xticks(rotation = 45,\n           horizontalalignment='right')","8ca47980":"from sklearn.linear_model import RidgeCV\nfrom sklearn.linear_model import Ridge\nmodel_ridge_cv = RidgeCV(alphas = [0.0001, 0.001, 0.1, 1, 5, 10, 15, 20, 25, 30])\nmodel_ridge_cv.fit(predictors, response)\nmodel_ridge = Ridge(alpha = model_ridge_cv.alpha_)\nresults_ridge = get_results(model_ridge, predictors)\n\nprint(f\"The RMSE for the Ridge model was {round(results_ridge.Error, 5)}\")","99340abf":"# Plot important coefficients\ncoefs = pd.Series(model_ridge.coef_, index = predictors.columns)\nprint(f\"Ridge picked {sum(coefs != 0)} features and eliminated the other {sum(coefs == 0)} features\")\nimp_coefs = pd.concat([coefs.sort_values().head(10),\n                     coefs.sort_values().tail(10)])\nsns.barplot(x = imp_coefs.index, y = imp_coefs)\nplt.title(\"Figure 6: Coefficients in the Ridge Model\")\nplt.xticks(rotation = 45,\n           horizontalalignment='right')","901a66a6":"from sklearn.linear_model import ElasticNetCV\nfrom sklearn.linear_model import ElasticNet\nmodel_en_cv = ElasticNetCV(l1_ratio = [.1, .3, .6, .9], max_iter = 50000,\n                           alphas = [0.0001, 0.001, 0.1, 1, 5, 10, 15, 20, 25, 30],\n                           fit_intercept = True, cv = 10)\nmodel_en_cv.fit(predictors, response)\nmodel_en = ElasticNet(l1_ratio = model_en_cv.l1_ratio_,\n                           alpha = model_en_cv.alpha_,\n                           max_iter = 50000,\n                           fit_intercept = True,\n                           random_state = 1)\nresults_en = get_results(model_en, predictors)\nprint(f\"The RMSE for the ElasticNet model was {round(results_en.Error, 5)}\")","8389195f":"# Plot important coefficients\ncoefs = pd.Series(model_en.coef_, index = predictors.columns)\nprint(f\"ElasticNet picked {sum(coefs != 0)} features and eliminated the other {sum(coefs == 0)} features\")\nimp_coefs = pd.concat([coefs.sort_values().head(10),\n                     coefs.sort_values().tail(10)])\nsns.barplot(x = imp_coefs.index, y = imp_coefs)\nplt.title(\"Figure 7: Coefficients in the ElasticNet Model\")\nplt.xticks(rotation = 45,\n           horizontalalignment='right')","ad4ed7bd":"# Overview\nThe purpose of this notebook is to ultimately develop models to predict housing data from the Ames, Iowa housing data set. On the way, we will explore the data as-is, clean and correct the data, and visualize some relationships. For the sake of my own learning, I will be strictly employing linear regression models.","d4f19882":"A-ha! Some clear outliers. Luckily for us, the two furthest out in `GrLivArea` are also the outliers in `1stFlrSF`, `TotalBsmtSF`, and `TotalSF`. There are five remaining in `GarageArea`: the three in the bottom right and the two at the very top. There are also three in `HouseAge`: one where a >100 year old house was sold for over $400,000, as well as the two most expensive overall (note that these don't look so far out-of-line in the other plots; that's the power of perspective). These last two overlap with the problem points in `GarageArea`.\n\nEight samples in more than fourteen-hundred is a loss of less than one percent, so we're dropping them. The rest of the data points on the edges are not decisive enough in my eyes, so they're staying.","b147d875":"# Correlation and outliers\nWe're at a point where we should easily be able to explore correlations between `SalePrice` and the rest of our numerical variables. We want to do this now as there is some data that appears clearly ordinal that we will convert and treat a little differently. We have some data like this now, e.g. `OverallCond`, but we'll be ignoring these in our search. This is all in an effort to deal with outliers.\n\nLooking through the data, there are plenty of them, but we're really only concerned with the ones as they relate to `SalePrice` the most. Linear models tend to be very sensitive to outliers. There are methods for making more robust models, but if there aren't too many \"bad\" data points, it's easier to just get rid of them. To find these points, we'll look at a correlation heatmap.","deea74a6":"I like `LassoCV` because it is much faster than `GridSearchCV` for parameter optimization. I kept my search relatively simple as you can seemingly spend your whole life optimizing an algorithm.","197ab85a":"# Ordinal Data\nThere is debate on how to handle ordinal data. It seems that the main contention is \"How far apart are these values, really?\" This is an excellent question. How far away, exactly, is a 9-quality house from a 10-quality? While we can say 9 is exactly 1 away from 10 on the number line, there is no answer to this question of quality: it is completely subjective.\n\n[Ordinal Independent Variables](https:\/\/www3.nd.edu\/~rwilliam\/stats3\/OrdinalIndependent.pdf) (Williams) makes the case that in practice is it generally fine to treat ordinal data as continuous. I'm going to heed this advice, as trying to handle it any other way in the context of this project may be impossible (for me, at the very least). What follows is the result of poring over the data and discerning what are likely ordinal variables as well as their classifications.","421b19ee":"There does seem to be some measure of heteroscedasticity based on the slight \"spray\" patterns. The log transformation for skewness will also help correct this. Autocorrelation seems absent, which is what we want.","7f1f6f28":"## Ridge Regression","8a3102cb":"I borrowed most of the following code from Julien Cohen-Solal's [A study on Regression applied to the Ames dataset](https:\/\/www.kaggle.com\/juliencs\/a-study-on-regression-applied-to-the-ames-dataset) as it is very useful and pretty.","c722c18b":"We see that ridge regression was the most conservative in terms of feature selection and it performed the worst, but only by a little. Really, they're all comparable.","299c8f44":"# Checking Modeling Assumptions\nAs we will be using linear models, it is important we check a few [assumptions](https:\/\/www.statisticssolutions.com\/assumptions-of-linear-regression\/) of our data.\n\n### 1. Linear Relationships\nThis is what we're trying to establish: if linear models perform well on this data or not. We assume enough of a relationship for this whole project to not be a total waste of time.\n\n### 2. Multivariate Normality\nOne of the assumptions of linear regression analysis is that of multivariate normality. That is, each variable must be normally distributed. We will be using some [normalization techniques](https:\/\/en.wikipedia.org\/wiki\/Normalization_(statistics) to transform what variables we can to ensure the best outcomes. For continuous numerical data this means log(x) or log(1+x) transformations (to account for 0s) ; for ordinal data we will be using min-max normalization; and for discrete numerical data we'll be standard score normalizing them. This last choice was somewhat arbitrary: as there is technically no upper limit to these variables, it seemed to make more sense than constraining them with min-max normalization.\n\n#### Skewness\n[Skewness](https:\/\/en.wikipedia.org\/wiki\/Skewness) is a measure of assymetry in a distribution, with normal distributions having 0 skew. An absolute skewness of more than 0.5 can be said to be significantly skewed.","232cebfe":"We see there is only one ordinal variable, `OverallQual`, so let's drop it. Let's have a look at the variables representing small, discrete quantities (e.g. rooms, cars).","00624332":"# Housing Prices and Regression Modeling: An Exercise in Machine Learning with Python\n**Michael Sieviec**\n\n**August 6, 2019**\n\nIn this notebook I will clean, analyze, and build predictive regression models on the Ames housing dataset. Some notebooks I found helpful were:\n\n* [Stacked Regressions to predict House Prices](https:\/\/www.kaggle.com\/serigne\/stacked-regressions-top-4-on-leaderboard) by **Serigne**\n* [COMPREHENSIVE DATA EXPLORATION WITH PYTHON](https:\/\/www.kaggle.com\/pmarcelino\/comprehensive-data-exploration-with-python) by **Pedro Marcelino**\n* [A study on Regression applied to the Ames dataset](https:\/\/www.kaggle.com\/juliencs\/a-study-on-regression-applied-to-the-ames-dataset) by **Julien Cohen-Solal**","64f24220":"I'm also going to shift `OverallQual` and `OverallCond` left by one for homogeneity (because I like it).","96d1b2ce":"These seem like reasonable enough statistics, so we'll ignore them, too.","60eff830":"We see there are quite a few missing values. According to the documentation, this actually isn't something to be too worried about as NA is a valid entry for many of the variables. Additionally, many of them seem to be related (e.g. `GarageQual` and `GarageCond`) and as such probably indicate missing values in a single row.\n\nHowever, we do want to get this data reasonably clean before we start visualizing relationships as there are so many missing points. Luckily, some of the missing values will be fairly easy to deal with. First, we will treat empty values in optional features (those non-essential to a house like a pool) as simply being absent. This constitutes assigning 'NA' to ordinal or categorical data and '0' to continuous or discrete.","cd799353":"We're almost in business.","8a724a81":"That's pretty good!","1046570b":"# Conclusion\nLinear methods are a good choice for the Ames housing dataset if you take the time and adjust the data to something more usable. Anecdotally, after all the preprocessing, I found that other methods like XGB and GBR performed slightly worse than the techniques presented here. Before preprocessing they performed better, but still not as good as the models we have here. That goes to show the power of engineering your data toward your purposes.","2369de95":"# Missing Data\nFor our purposes, we want to the clean all the data together. This ensures no process is applied assymetrically to either the training or test sets.","49c6c36f":"Next, missing essential features (e.g. `Utilities`) will be filled, first with the means for each class as grouped by their neighborhood for continuous data, and then the same for modes for discrete and ordinal data as the counts are so small.","284046fa":"## ElasticNet","6d6c2408":"### Z-Score Normalization","5e9a4a0d":"# Reconstructing Train and Test Sets","bafdb1cb":"Here we see our most skewed continuous numerical features, with no significant negative skew. This is nice, as negative skew must be [transformed differently](http:\/\/core.ecu.edu\/psyc\/wuenschk\/StatHelp\/NegSkew.pdf). Those with skewness > 0.5 will be log transformed.","6820d095":"# Creating and Evaluating Models\nWe're going to explore three kinds of linear regression models in the scikit-learn library: LASSO, Ridge, and ElasticNet. One advantage of each of these is that they employ a type of feature selection through weighting the variables so we don't have to incorporate dimensionality reduction into our preprocessing. Another is that linear models are more intrepretable than more complex methods like ensembles or neural networks, so they are easier to use for inference.\n\nFirst, we need to create a function to score our models. We're going to manually compute it using the RMSE between the cross-validation predictions and the actual values. This will give us a good idea of the out-of-sample accuracy.","d8c6d4d7":"This is a lot to digest, so let's reduce our list to the ten most correlated variables (by absolute value) and have a look.","f60324c3":"# New Variables\nWe will add new variables: `TotalSF`, `HouseAge`, and `RemodAge`. `TotalSF` indicates the sum of each of the basement, first floor, and second floor square-footage of the homes as price per square-foot is a common metric in home shopping. `HouseAge` and `RemodAge` are both attempts at making the year data more useful. I considered the question, \"Will it be more important that a house was built in X year or that it was Y years old at the time it was sold?\" I figured the latter to be the way to go. These variables led to some negative values, but people do buy land and pay to have their houses built, so I wasn't concerned.\n\nHere, we also drop `GarageYrBlt` as it proved more difficult to work with in a similar manner (because of the missing values) and I find it easy to imagine that whatever way it could have affected `SalePrice` was either insubstantial or not already explained by some other variables.","6ebb9667":"# Transforming Numerical Data\n### Log Transformation","55484197":"That's also really good!","2659da6b":"# Transforming Ordinal Data\n### Min-Max Normalization","29f853ac":"Next, we convert months from numerical to categorical.","c5744432":"We see also that `SalePrice` is nearly F-distributed, so we will log transform it as well.","5e41b62f":"Again, pretty good for comparatively simple methods.","5cf2785f":"# Categorical Variables and Dummies\nScikit-learn models unfortunately do not take factors as variables like R models do. To deal with this, we will create dummy variables. Also unfortunately, I found that `pandas.get_dummies` does *not* always operate as expected. As such, I had to construct a dummy dataframe from individually created dummy dataframes for each of the affected variables.","09dc9f89":"### 3. Multicollinearity\nOur heatmap in figure 1 shows that there are no variables that are perfectly multicollinear (correlation of 1). There are some highly correlated variables, but feature selection in our regression models should be able to deal with this.\n\n### 4. Autocorrelation and Homoscedasticity\nWe can check for autocorrelation (predicted observations not being independent from previous ones) and homoscedasticity (constant variance) with a graph of the residuals. For our purposes, we're only going to look at  our most highly correlated variables from before."}}