{"cell_type":{"12cc0a70":"code","c4eb5bcc":"code","ee818e0f":"code","5d318eab":"code","be4a3726":"code","c4391d1f":"code","39ee611c":"code","642bd6f9":"code","bf4fd333":"code","2fc6960a":"code","39dca81d":"code","30b40c26":"code","15e841c7":"code","d8d9274a":"code","c116ba46":"code","591f46d2":"code","7ce2ffab":"code","d35f34f5":"code","642ddcfe":"code","c288f3bb":"code","a732eb3a":"code","e37b689c":"code","538ff53d":"code","ea9dfe7b":"code","cf42b9d2":"code","19d70795":"code","6f5c7957":"code","641a0e35":"code","0223a483":"code","4219575d":"code","8c5b4f34":"code","15be0367":"code","0ab71293":"code","48f4339c":"code","4fa2d6ce":"code","7dab897b":"code","96df4416":"code","f17bcb88":"code","e76bf811":"code","98f38645":"code","07cac9f6":"code","418555ba":"code","86522008":"code","a7f5bacd":"code","dc8617c1":"code","3ab6544f":"code","178e3fb7":"code","40ec5c3f":"code","d32112a3":"code","51cbc076":"code","036a83ef":"markdown","3cdb18ea":"markdown","ea4e1cbe":"markdown","136dc7a1":"markdown","6ef7d1e9":"markdown","609b0fa5":"markdown","6f0c05dc":"markdown","b9c94529":"markdown","2a985d0d":"markdown","46e4683b":"markdown","f2549297":"markdown","4302e1f4":"markdown","7cb52965":"markdown","1561bb8c":"markdown","eb8465c6":"markdown","ea4bff9a":"markdown","7bb5957f":"markdown","03d4e724":"markdown","58dba449":"markdown","53d3d78d":"markdown","d375c755":"markdown","d0256dec":"markdown","15d736cc":"markdown","a7f77eda":"markdown","08ee3771":"markdown","89984faa":"markdown","09ef03b7":"markdown","bc56990f":"markdown","96c9d757":"markdown","d2b50870":"markdown","b881ec09":"markdown","b0890f01":"markdown","fe29a8d8":"markdown","37a4cb69":"markdown","9d9f15a9":"markdown"},"source":{"12cc0a70":"import datetime\nimport imageio\nimport numpy as np\nimport pandas as pd\nimport pickle\n\nfrom sklearn.model_selection import train_test_split\nfrom fastai.vision import *\nfrom fastai.metrics import accuracy, error_rate\nfrom fastai.widgets import DatasetFormatter, PredictionsCorrector","c4eb5bcc":"import fastai\nprint(fastai.__version__)","ee818e0f":"np.random.seed(42)","5d318eab":"train = pd.read_csv(\"\/kaggle\/input\/digit-recognizer\/train.csv\")\ntest = pd.read_csv(\"\/kaggle\/input\/digit-recognizer\/test.csv\")","be4a3726":"train.head()","c4391d1f":"y = train[\"label\"].values\nX = train.iloc[:, 1:].values\n\nX.shape, y.shape","39ee611c":"# Pull out one image\n\nimg = X.reshape(-1, 28 ,28)[10,:]\n\n# Now lets just repeat the same matrix three times\n\nstacked_img = np.stack((img,)*3, axis=-1)\nimg.shape, stacked_img.shape","642bd6f9":"fig, ax = plt.subplots(1, 2, figsize=(15,10))\n\nax[0].matshow(img, cmap='gray', alpha=1.0)\nax[0].set_title('Single Channel 28x28 Image Values')\nax[0].set_xticks(())\nax[0].set_yticks(())\n\nfor i in range(28):\n    for j in range(28):\n        c = img[j,i]\n        ax[0].text(i, j, str(c), va='center', ha='center', fontsize=6, color='red')\n\nax[1].matshow(img, cmap='gray')\nax[1].set_title('Single Channel 28x28 Image');\nax[1].set_xticks(())\nax[1].set_yticks(());","bf4fd333":"channels = ['Red', 'Green', 'Blue']\ncmaps = [plt.cm.Reds_r, plt.cm.Greens_r, plt.cm.Blues_r]\n\nfig, ax = plt.subplots(1, 4, figsize=(15, 10))\n\nfor i, axs in enumerate(fig.axes[:3]):\n    axs.imshow(stacked_img[:,:,i], cmap=cmaps[i])\n    axs.set_title(f'{channels[i]} Channel')\n    axs.set_xticks([])\n    axs.set_yticks([])\n\nax[3].imshow(stacked_img, cmap='gray')\nax[3].set_title('Three Channel Image')\nax[3].set_xticks([])\nax[3].set_yticks([]);","2fc6960a":"X_train, X_valid, y_train, y_valid = \\\n    train_test_split(X, y, test_size=0.05, random_state=42, stratify=y)","39dca81d":"def to_img_shape(X, y=[]):\n    \"Format matrix of rows, Nx784 to Nx28x28x3\"\n    X = np.array(X).reshape(-1,28,28)\n    X = np.stack((X,)*3, axis=-1)\n    y = np.array(y)\n    return X, y\n\ndef save_imgs(path:Path, data, labels=[]):\n    \"\"\"Save numpy formated images to files. We have to be careful with the filenames.\n    If you just name the files like 0.jpg, 1.jpg, 2.jpg, etc. then the sort order will\n    be off. The sort order for strings is like strings sort like 0, 1, 10, 2... Therefore\n    we pad the integer with leading zeros like \"00001.jpg\".\n    \"\"\"\n    \n    path.mkdir(parents=True, exist_ok=True)\n    \n    for label in np.unique(labels):\n        (path \/ str(label)).mkdir(parents=True, exist_ok=True)\n\n    for i in range(len(data)):\n        if(len(labels) != 0):\n            imageio.imsave(str(path \/ str(labels[i]) \/ (str(i) + '.jpg')), data[i])\n        else:  # this must be test data; order matters!!\n            imageio.imsave(str(path \/ f'testimg_{str(i).zfill(5)}.jpg'), data[i])","30b40c26":"X_train, y_train = to_img_shape(X_train, y_train)\nX_valid, y_valid = to_img_shape(X_valid, y_valid)\nX_test, _ = to_img_shape(test)","15e841c7":"X_train.shape, X_valid.shape, X_test.shape","d8d9274a":"%%time\nsave_imgs(Path('\/kaggle\/working\/data\/train'), X_train, y_train)","c116ba46":"%%time\nsave_imgs(Path('\/kaggle\/working\/data\/valid'), X_valid, y_valid)","591f46d2":"%%time\nsave_imgs(Path('\/kaggle\/working\/data\/test'), X_test)","7ce2ffab":"path = Path('\/kaggle\/working\/data')\nimage_list = (ImageList.from_folder(path)\n                       .split_by_folder()\n                       .label_from_folder())\n\ndata = (image_list.databunch(bs=1)\n                  .normalize(imagenet_stats))","d35f34f5":"tfms = get_transforms()\ntfms[0]  # The zero lists the transforms applied during training","642ddcfe":"tfms[1]  # These are the transforms which are applied during validation","c288f3bb":"# The images are not guaranteed to be in a particular order!\nfor idx in range(len(data.train_ds.x)):\n    label = int(str(data.train_ds.y[idx])) == 5\n    if label:\n        break\n        \ndef get_ex(i):\n    return data.train_ds.x[i]\n\ndef plots_f(tfms, rows, cols, width, height, **kwargs):\n    np.random.seed(17)\n    j = idx\n    # Code from the fastai docs https:\/\/docs.fast.ai\/vision.transform.html\n    [get_ex(j).apply_tfms(tfms[0], **kwargs).show(ax=ax)\n        for i, ax in enumerate(plt.subplots(rows, cols, figsize=(width,height))[1].flatten())]","a732eb3a":"plots_f(tfms, 2, 4, 12, 6)","e37b689c":"tfms = get_transforms(do_flip=False)\ntfms[0]","538ff53d":"plots_f(tfms, 2, 4, 12, 6, padding_mode='zeros') ","ea9dfe7b":"image_list = (image_list.transform(get_transforms(do_flip=False), size=28)\n                        .add_test(ItemList.from_folder(path=path\/\"test\"), label=None))\n\ndata = (image_list.databunch(bs=256)\n                  .normalize(imagenet_stats))","cf42b9d2":"data","19d70795":"data.show_batch(3, figsize=(7,7))","6f5c7957":"from graphviz import Source\nsrc = Source('''\n    digraph {\n        rankdir=LR\n        node [shape=box style=rounded]\n        \"make ImageDataBunch\" -> \"make pretrained cnn_learner\"->\"lr_find\"->\"fit head for a few cycles\"->\"unfreeze\"->\"lr_find\"->\"fit full model\"->\"get Kaggle \ud83e\udd47\"\n    }\n''')\nsrc","641a0e35":"learn = cnn_learner(data, models.resnet18, metrics=accuracy)","0223a483":"learn.model","4219575d":"learn.lr_find()","8c5b4f34":"learn.recorder.plot()\nplt.axvline(0.020, c='r', linestyle='--');","15be0367":"learn.fit_one_cycle(5, 0.02)","0ab71293":"learn.save('stage-1')","48f4339c":"learn.load('stage-1');","4fa2d6ce":"learn.unfreeze()\nlearn.lr_find()","7dab897b":"learn.recorder.plot()","96df4416":"%%time\nlearn.fit_one_cycle(35, max_lr=slice(1e-5, 0.02\/10))","f17bcb88":"learn.save('stage-2')","e76bf811":"learn.recorder.plot_losses()","98f38645":"interp = ClassificationInterpretation.from_learner(learn)","07cac9f6":"interp.plot_confusion_matrix()","418555ba":"interp.plot_top_losses(9, figsize=(7,7))","86522008":"def make_submission_file(\n    learner,\n    filename=f'submission_{datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")}.csv',\n    preds=None\n):\n    \"\"\"Takes a trained fastai learner (and optional pre-computed list of predicted\n    labels) and makes a Kaggle submisison file. This is tricky because the learner\n    returns the order of the test set as `os.listdir` does. This is *not* in the\n    order they were created!\n    \"\"\"\n    \n    if preds is None:\n        preds, _ = learner.get_preds(ds_type=DatasetType.Test)\n        preds = np.argmax(preds, 1)\n\n    test_index = []\n    num = len(learn.data.test_ds)\n    for i in range(num):\n        test_index.append(str(learner.data.test_ds.items[i]).split('\/')[-1])\n    \n    df = (pd.DataFrame(data={\"Label\": preds, \"Filename\": test_index})\n            .sort_values(by='Filename')\n            .drop('Filename', axis=1)\n            .assign(ImageId = range(1, len(preds) + 1))\n            .reset_index(drop=True)\n            .reindex(columns=['ImageId', 'Label']))\n\n    df.to_csv(filename, index=False)\n    print(f'Saved predictions as {filename}.')","a7f5bacd":"make_submission_file(learn, filename=\"resnet18-fine-tuned.csv\")","dc8617c1":"most_unsure = DatasetFormatter.from_most_unsure(learn)","3ab6544f":"# wgt = PredictionsCorrector(*most_unsure)","178e3fb7":"# wgt.show_corrections(ncols=6, figsize=(12, 7))","40ec5c3f":"err1 = 1 - 0.99442\nerr2 = 1 - 0.99571\nprint(f'Human in the loop improvement: {100*(err1-err2)\/err1}%')","d32112a3":"%%capture\n# I have to fetch the hand corrected labels\n!wget https:\/\/kaggle-himl-mnist.s3.amazonaws.com\/resnet18-hitlml-corrected.csv","51cbc076":"!rm -rf \/kaggle\/working\/data\/train\n!rm -rf \/kaggle\/working\/data\/test\n!rm -rf \/kaggle\/working\/data\/valid","036a83ef":"For reference, the complete code is\n\n```python\ndata = (ImageList.from_folder(path)\n                 .split_by_folder()\n                 .label_from_folder()\n                 .transform(get_transforms(do_flip=False), size=28)\n                 .add_test(ItemList.from_folder(path=path\/\"test\"), label=None)\n                 .databunch(bs=256)\n                 .normalize(imagenet_stats))\n```","3cdb18ea":"Now we **unfreeze** the pretrained layers and look for a learning rate slice to apply. In Lesson 3 49:20: \"Call \\[lr_find\\] again...Look to **just before it shoots up**1e-4) and go **back 10x**, which is 1e-5, and that's what I do for the first half of my slice; and for the second half of my slice, I normally **use whatever learning rate I used for the first part divided by 5 or 10.**\"","ea4e1cbe":"A low difference means the classifier is unsure. There is a Jupyter widget, **[`PredictionsCorrector`](https:\/\/docs.fast.ai\/widgets.image_cleaner.html#PredictionsCorrector)** which then serves up **batches of the most unsure predictions for the human analyst (you!) to confirm**. It looks like this:\n\n<img src=\"https:\/\/docs.fast.ai\/imgs\/PredictionsCorrector.gif\">\n\n*Image source: fastai docs, https:\/\/docs.fast.ai\/widgets.image_cleaner.html*\n\nThis kind of \"post prediction human supervision\" is of course not appropriate in all cases. Sometimes classifiers operate at super-human level. For example. one student project highlighted in the 2018 course was classifying satellite images to one of over 100 countries. In this case, it's unlikely that a human could be of assistance. However, **MNIST** is a perfect use case. We already have 99.5% accuracy on 28,000 labels. We are wrong on only approximately 140(!) in a domain where humans excel.\n\nYou are (likely) viewing the commited and published version of this kernel. In that case, you cannot interact with the `PredictionsCorrector`. That class runs on top of `ipywidgets` which needs a live Python kernel behind it (note when you see other Kaggle kernels with interactive plots, you are interacting with Javascript, not Python).\n\nI ran the `PredictionsCorrector` with\n\n```python\nmost_unsure = DatasetFormatter.from_most_unsure(learn)\nwgt = PredictionsCorrector(*most_unsure)\n```\n\nand made manual corrections on X labels. The manual corrections can be seen with\n\n```python\nwgt.show_corrections(ncols=6, figsize=(9, 7))\n```\n\n**If you want to do corrections yourself in this kernel, you have to fork it and run it from the top to this point.**","136dc7a1":"And now we fit for a long time. Why? I tried a few values for the number of epochs. Per Jeremy, training loss should be **less than validation loss** or else you are **underfitting**. Lesson 3: 1:25:12: \"when you do plot_losses and see you the training loss go down, then go up a little for a bit, then go down a lot you've found a really good maximum learning rate.\" Generally want to keep fitting as long as training loss is going down **and** validation loss is not going up. \n\n![](https:\/\/hackernoon.com\/hn-images\/1*vuZxFMi5fODz2OEcpG-S1g.png)","6ef7d1e9":"## ...Sidebar on Data Augmentation...\n`fastai` also ships with out-of-the box data augmentation. The idea is that there are many kinds of slight variations in images which you would expect to see, but which don't actually exist in the data. We can therefore create synthetic data by making slight modifications of training samples in very specific ways. It's best to show this by example. `fastai` has a function `get_transforms()` which returns a **preset list of common transforms that work well on many image types**:","609b0fa5":"# Clean Up\n\nThe commited kernel has a maximum output file count of 500, so we need to delete all the images we cached to the file system.","6f0c05dc":"# Human in the Loop AI\n\nThe idea of uncertainty and \"trustworthiness\" in AI is a big deal in industry. In order to combat **[algorithm aversion](https:\/\/hbr.org\/2015\/02\/heres-why-people-trust-human-judgment-over-algorithms)** and deploy production systems in high risk domains like medicine, researchers often insert a \"human in the loop\". This is best explained in the following diagram.\n\n![image.png](attachment:image.png)\n\n*Image source: https:\/\/www.figure-eight.com\/resources\/human-in-the-loop\/*\n\nWe trust our classifier...unless it tells us that it is not very confident! In that case, we raise the case to a human arbiter. It turns out that **`fastai` supports human in the loop AI out-of-the box!** Amazing! There are lots of complicated ways to get a classifier to tell you if it is confident or not. For example, one way is to employ **\"monte carlo dropout\" in the inference stage**. In this way, we make many predictions all the while randomly turning off some neurons. We get many predictions and then we cacluate the entropy of the prediction set. If the entropy is high (meaning the classifier gave us lots of different predictions while we were flipping neurons on and off), then we know that it isn't very confident as the classifer is not robust to that example. Another way is to make a **fully Bayesian neural network**: we model the weights as random variables and thus we get a posterior distribution for the weights **and** the predicted classes. Both of those are very very complex. In usual `fastai` brilliance, the library does something simple, intuitive, and elegant: for each prediction, it looks at the gap between the actual preduction (`np.argmax(pred[i, :], 1))` as usual) and the **second best prediction**. ","b9c94529":"I pick `0.02` as my maximum learning rate as it's \"in the middle of the steepest slope\", and then I fit several cycles.","2a985d0d":"## Convert single channel grayscale images to 3-channel RGB\nThe fastai library seems to be set up to handle **RGB images**. This means that every image is actually **three matrices**: one each for red, green, and blue values. One of the reasons, I think, this is the case is that fastai excels at **transfer learning** -- meaning the libary uses pre-trained models as a starting point. Pre-trained models, like **`resnet18`** (described below) have been trained on three channel images. A grayscale image is a single channel though. However a grayscale image can be represented as an RGB image where all three matrices have the same value. Let's confirm.","46e4683b":"Here we run the **learning rate finder**. Per Jeremy (\"Jeremy says...\"), lesson 3: 49:00, \"do lr_find, lr_plot and find the learning rate with the steepest slope, **not the bottom!**\"","f2549297":"In order to get the corrected labels to persist with this commited kernel I cached it in an external file store (I used an Amazon S3 bucket), and here I reload it. This allows the kernel to be able to use the submission file with the corrected labels after hitting \"Commit\".","4302e1f4":"Now we are ready to submit to Kaggle. Here is a helper function that makes the prediction on the test set and saves the submission file.","7cb52965":"Note that the data is in a strange format. Even though these are images, they have been **flatted out into rows**. Each row is an image and each column is a pixel value of the image. An image is **28x28 pixel gray-scale image flattend out to a single row of the pixel values (from 0 to 255)**.","1561bb8c":"<tr>\n<td> <img src=\"https:\/\/upload.wikimedia.org\/wikipedia\/commons\/thumb\/2\/27\/MnistExamples.png\/220px-MnistExamples.png\" alt=\"Drawing\" style=\"width: 150px;\"\/> <\/td>\n<td> <img src=\"https:\/\/miro.medium.com\/max\/3220\/1*EoktyGnUpOv9Zq-85AIDZw.png\" alt=\"Drawing\" style=\"width: 150px;\"\/> <\/td>\n<td> <img src=\"https:\/\/upload.wikimedia.org\/wikipedia\/commons\/thumb\/9\/96\/Pytorch_logo.png\/800px-Pytorch_logo.png\" alt=\"Drawing\" style=\"width: 150px;\"\/> <\/td>\n<td> <img src=\"https:\/\/neurohive.io\/wp-content\/uploads\/2019\/01\/resnet-e1548261477164.png\" alt=\"Drawing\" style=\"width: 150px;\"\/> <\/td>\n<\/tr>","eb8465c6":"## Make the train\/validation split\n\nAs usual, we split out a (single) validation set. I chose to stratify so that we have the same proportion of classes in the validation set. Note I set the random seed above. ","ea4bff9a":"The are 7 transforms. **Let's try them out on a single image.**","7bb5957f":"# MNIST 99.5% From Scratch (fastai\/PyTorch\/ResNet)\nby [@marketneutral](https:\/\/www.kaggle.com\/marketneutral)\n\n**YAMN**? Yet another MNIST notebook? There are probably a gazillion notebooks that go through MNIST and CNN fitting. Why another one? I have several reasons for creating this notebook including:\n\n- lots of kernels show code but **don't explain** what's going on or how to set certain hyperparameters. This kernel does nothing without fully explaining what's going on. Where possible, I correlate choices I make back the **fastai lectures with timestamps**.\n- I've never seen an application of **\"human in the loop\" machine learning** on Kaggle. Spoiler alert: this notebook is an example. See down below.\n- Lastly, these are just **my personal notes** as I learn computer vision applications.\n","03d4e724":"The errors that we do make seem like errors even a human would make.","58dba449":"We can see from the confusion matrix that we make very very few errors on the validation set.","53d3d78d":"We create the learner. This will download a pre-trained `PyTorch` model from it's hub. We pass in our `ImageDataBunch` and register accuracy as the metric we care about. Note that the accuracy is just a printed metric; **it does not affect fitting**. The default loss function for the a `cnn_learner` is cross entropy -- the loss function for a multi-class classification problem (which this is).","d375c755":"This submission gets **0.99442** on the public leaderboard.","d0256dec":"and also plot some of the training images","15d736cc":"We can inspect the `ImageDataBunch` as","a7f77eda":"# Create the fastai `ImageDataBunch`","08ee3771":"Our plot of train and validation loss looks good.","89984faa":"# Load the Data","09ef03b7":"The images above are **randomly generated from a single real training image**. There are slight changes in rotation, zoom, contrast, etc. There's also a problem...we've randomly **flipped images**. That might make sense in some circumstances, but doesn't make sense for numbers or letters. As such, let's remove the flipping behavior; also we increase the rotation to accentuate that.","bc56990f":"### Where is the addition step?\n\nThe famous \"ResNet block\" above is the `BasicBlock`. We can simply see the names of the components (Conv2d, BatchNorm2d, ReLU) and the order and that they match the [architecture diagram](http:\/\/torch.ch\/blog\/2016\/02\/04\/resnets.html)\n\n![image.png](attachment:image.png)\n\nNotice that the stride is `1`, kernel size is `3x3`, and (zero)padding is `1`. As discussed in [this great post](https:\/\/adeshpande3.github.io\/A-Beginner%27s-Guide-To-Understanding-Convolutional-Neural-Networks-Part-2\/), if you have a stride of 1 and set padding equal to (K-1)\/2, **then the output size will be the same as the input size**. So, again, we are robust to whatever image size you want (image size, not number of channels; we must have 3 and only 3 channels) -- the network just continues to pass your size through. **But where is the addition step?** If you look at the [`pytorch` source code for `BasicBlock()`](https:\/\/github.com\/pytorch\/vision\/blob\/master\/torchvision\/models\/resnet.py#L35) you will see it! It's very simple... the forward pass through the block is\n```python\n    def forward(self, x):\n        identity = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n\n        if self.downsample is not None:\n            identity = self.downsample(x)\n\n        out += identity\n        out = self.relu(out)\n\n        return out\n```        \n\nAt start start of the `BasicBlock` we store the input as `identity` and at the end we add it back with `out += identity`.","96c9d757":"We make a `cnn_learner` which is based on a **pre-trained PyTorch** model. We then use the **learning rate finder** to find the optimal maximum learning rate to apply. `fastai` takes the pretrained model and adds a head (described below). Initially we **fit just the head**, leaving the pre-trained model fixed (or \"frozen\"). We can get pretty good results already, but to get even better we **unfreeze the pre-trained model** and fit it with a **much smaller learning rate**.","d2b50870":"## Write out data to the file system\n\nWe will need some helper functions to process all the data. We ultimately need to get our data into a `fastai.vision.ImageDataBunch`. It turns out that the easiest way to do that is to write out all the images to files and then point fastai to those files. This seems a little silly since the data are small and fit in memory, but it makes things a lot simpler. The first helper function, `to_img_shape`, takes the data matrix provided (which is of dimensions N images by 784 pixels and converts it to three channel **(N, x pixel, y pixel, channel) shape**. The second function, `save_imgs`, writes out the data to files. The folder stucture follows the \"ImageNet folder structure\" like\n\n```\ntrain\/\n   0\/\n   1\/\n   2\/\n   : \n   : \n   9\/\nvalid\/\n   0\/\n   1\/\n   2\/\n   : \n   : \n   9\/\ntest\/\n   testimg_00000.jpg\n   testimg_00001.jpg\n     : \n     : \n   testimg_27999.jpg\n```","b881ec09":"I \"corrected\" 19 images. The actual changes were\n\n![image.png](attachment:image.png)\n\nThis corrected solution improves the final leaderboard score to **0.99571** (47 places improved as of the publication of this kernel). **The error rate is improved by 23% from the human-in-the-loop step.**","b0890f01":"# How Does the Pre-Trained Model Work???\n\nHow does the pre-trained model use **our image size**? The PyTorch `resnet18` model was trained on ImageNet images. These images are 3-channel 224x224 pixels. Our images are 3-channel 28x28. How can we use pre-trained weights for a model trained on different images sizes? This took me a while to grok, so I am making a special call-out box on it. Let's look at the first layers of the model instantiated as `learn`:\n\n```\n  Path: \/kaggle\/working\/data, model=Sequential(\n  (0): Sequential(\n    (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (2): ReLU(inplace=True)\n    (3): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n    (4): Sequential(\n      (0): BasicBlock(\n        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n      :\n      :\n      :\n```\n\nThe first layer is a 2d convolution. What is a convolution doing? The convolution slides a 7x7 matrix across each input channel of your image and does element-by-element multiplication summed against what it sees (this is sometimes described as matrix multiplication--wrong--or a dot prodcut. The dot product of the stretched out kernel (7x7=49 in this case) and the image segment is the same as element-by-element multuplcation summed). In this model, there are 64 kernels that slide across each channel. A key idea is that the elements of these 7x7 matrices are **learned weights**. The **sizes of the kernels** are fixed even though the image sizes are not. This means that the count of weights does not vary at all as a function of image size. This also makes clear why we needed to convert our one channel images to three channels: pre-trained `resnet18` requires 3 input channels; these **kernels are the weights**. In addition to convolutional layers, the model has `ReLU` layers too. However there are no weights here; this layer is just making some non-linearity of what the prior convolutional layer outputs. So how many weights are there in the first convolutional layer? 7 x 7 x 3 channels x 64 = 9408. At some point though, this logic breaks. At the end of the model, we take the last convolutional layer, connect it to a \"fully-connected\" layer and output to a certain number of classes. There are two things to understand about this. First let's look at the **actually pre-trained model from the PyTorch website** (**not** our `learn` instance):\n\n```\n    :\n    :\n    :\n    (1): BasicBlock(\n      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n  )\n  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n  (fc): Linear(in_features=512, out_features=1000, bias=True)\n)\n```\n\nThe last convolutional block feeds into an `AdaptiveAvgPool2d(...)` layer which feeds into a fully-connected layer with 1000 outputs. These 1000 outputs are the ImageNet classes. Without going into [details](https:\/\/pytorch.org\/docs\/stable\/nn.html#torch.nn.AdaptiveAvgPool2d), the key idea of the `AdaptiveAvgPool2d` layer is that it takes **any input size** and always outputs the same size. This is how we can use any input image size passing through the convolutional layers and make it fit a pre-defined fully-connected layer. However this last layer -- this is where our idea breaks down: we don't have 1000 classes, we have 10. To see how fastai deals with this, let's look not at the model from the PyTorch website, but the actual model instantiated as `learn`. When we look at the last layers of this model we see:\n\n```\n     :\n     :\n     :\n     (1): BasicBlock(\n        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n  )\n  (1): Sequential(\n    (0): AdaptiveConcatPool2d(\n      (ap): AdaptiveAvgPool2d(output_size=1)\n      (mp): AdaptiveMaxPool2d(output_size=1)\n    )\n    (1): Flatten()\n    (2): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (3): Dropout(p=0.25, inplace=False)\n    (4): Linear(in_features=1024, out_features=512, bias=True)\n    (5): ReLU(inplace=True)\n    (6): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (7): Dropout(p=0.5, inplace=False)\n    (8): Linear(in_features=512, out_features=10, bias=True)\n  )\n  ```\n  \n  Complex! fastai cut off the `avgpool` and the `fc` layers from the pre-trained model and then added a bunch of it's own layers. **These added layers are the only ones trained when you first call `fit_one_cycle`**. Notice that `out_features=10`; fastai made this layer for us based on the number of classes in our data. And that's it. ","fe29a8d8":"We take our existing `image_list` and append the transforms. Note this does **not** apply the transforms; it merely registers them as valid transforms to apply when a learner asks for a batch. We recreate the data bynch object again with the transforms attached.","37a4cb69":"<div class=\"alert alert-info\">\nIn summary: 1) convolutional layers are defined by their kernel sizes (weights) and can handle variable images sized; 2) ReLu doesn't care about input\/output size; 3) adaptive pooling takes any input size and, with a constant kernel size, always outputs the same size; this is the key to connect variable image sizes to a pre-defined fully-connected layer size; 4) fastai cuts off the last layer of the pre-trained model, and adds a layer that matches the number of classes in our data.\n<\/div>","9d9f15a9":"# The fast.ai way\n\nHere it is!"}}