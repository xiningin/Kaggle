{"cell_type":{"159c9ee3":"code","a19a502a":"code","b5420df3":"code","af3c4a23":"code","dab5945f":"code","e80f2143":"code","7d2e15a6":"code","c7e1bef6":"code","28d5f8ea":"code","82b11348":"code","ba54e8eb":"code","e4594824":"code","7dff1445":"code","824e865b":"code","ecf4a38a":"code","1a592df8":"code","8f56ca36":"code","806d37a7":"code","41daf700":"code","b17a186f":"code","b1e19886":"code","4c7896e1":"code","63dc5b4d":"code","5ee5c05d":"code","484eada1":"code","c4a854e9":"code","9d319eba":"code","2d83bb7a":"code","3dd6d479":"code","5bad3fca":"code","7a984351":"code","577b82c1":"code","86061009":"code","cea83da5":"code","3483501c":"code","743adbe1":"code","2c345bf8":"code","ee566e45":"code","8a6ba63c":"code","d09b7c24":"code","867bd5ae":"code","09835bd8":"code","aba8935c":"code","e7e491d1":"code","eab8d75d":"code","c8d77128":"code","c467cef0":"code","f8a810c8":"code","3d16b65e":"code","a94dc4ab":"markdown","952cccd7":"markdown","50867a92":"markdown","cdde1cd3":"markdown","d064658a":"markdown","55397dd0":"markdown","8bd13b45":"markdown"},"source":{"159c9ee3":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\nfrom matplotlib import pyplot as plt\nimport seaborn as sns \n%matplotlib inline \nimport os\nimport matplotlib as mpl\nfrom pandas import Series\nimport re\n#Pas de message d'alertes\nimport warnings\nwarnings.filterwarnings('ignore')\n# Any results you write to the current directory are saved as output.","a19a502a":"df = pd.read_csv(\"..\/input\/train.csv\")  \nxtest=pd.read_csv('..\/input\/xtest.csv')","b5420df3":"#Lire 5 ligne de mani\u00e8re al\u00e9a \ndf.sample(5)","af3c4a23":"#Regardons la taille  de notre dataset\ndf.dtypes","dab5945f":"#convertir le type de author en object \ndf['author']=df['author'].astype('object')","e80f2143":"df.columns","7d2e15a6":"#Nous allons supprimer la variables entities \ndf.drop(columns=['entities', 'Unnamed: 0'], inplace=True)","c7e1bef6":"#Regardons les vleurs manquantes \ndf.isnull().sum()","28d5f8ea":"#Statistiques descriptives des variables quantitatives \ndf_qual=df.select_dtypes(exclude=['object'])\ndf_qual.describe().plot(kind = \"area\",fontsize=22, figsize = (18,8), table = True,colormap=\"rainbow\")\nplt.xlabel('',)\nplt.ylabel('Value')\nplt.title(\"Statistiques g\u00e9n\u00e9rales des variables \")","82b11348":"df1=df\ndf1['author']=df1['author'].astype('object')\ndf1['author'][df1['author'] == '0' ] = 'Trump'\ndf1['author'][df1['author'] == '1' ] = 'Clinton'\nf,ax=plt.subplots(1,2,figsize=(16,7))\ndf['author'].value_counts().plot.pie(explode=[0.1,0.1],autopct='%1.1f%%',ax=ax[0],shadow=True)\nax[0].set_title('df1 author Count')\nax[0].set_ylabel('Count')\nsns.countplot('author',data=df,ax=ax[1])\nax[1].set_title('df1 author Count')\nplt.show()","ba54e8eb":"#Description des variables quantitatives \ndf.select_dtypes(exclude='object').describe()","e4594824":"#description des variables quantitatives \ndf.select_dtypes(include='object').describe()","7dff1445":"df2=df[['retweet_count', 'favorite_count', 'author']]\ndf2['author'] = df2['author'].astype('object')\ndf2['author'][df2['author'] == 1 ] = 'Trump'\ndf2['author'][df2['author'] == 0 ] = 'Clinton'\nfig=plt.gcf()\nfig.set_size_inches(10,7)\nfig=sns.stripplot(x='author',\n                  y='favorite_count',data=df2,jitter=True,\n                  edgecolor='gray',size=8,palette='winter',orient='v')","824e865b":"df2=df[['retweet_count', 'favorite_count', 'author']]\ndf2['author'] = df2['author'].astype('object')\ndf2['author'][df2['author'] == 1 ] = 'Trump'\ndf2['author'][df2['author'] == 0 ] = 'Clinton'\nfig=plt.gcf()\nfig.set_size_inches(10,7)\nfig=sns.stripplot(x='author',\n                  y='retweet_count',data=df2,jitter=True,\n                  edgecolor='gray',size=8,palette='winter',orient='v')","ecf4a38a":"#Allos nous allons d\u00e9finir un new data appel\u00e9 data qui contient uniquement lentgh, text et author \ndata=df[['author', 'text', 'favorite_count']] \ndata['author'][data['author'] == 1 ] = 'Trump'\ndata['author'][data['author'] == 0 ] = 'Clinton'","1a592df8":"data.sample(5)","8f56ca36":"#Regardons est ce que la taille des twett peut nous indiquer sa  provenence \nsns.factorplot('author','favorite_count',data=df)\nplt.ioff()\nplt.show()","806d37a7":"#Regardone la distribution des length en fonction des candidants \nmpl.rcParams['patch.force_edgecolor'] = True\nplt.style.use('seaborn-bright')\ndf.hist(column='favorite_count', by='author', bins=50,figsize=(11,5))","41daf700":"import nltk\nfrom wordcloud import WordCloud,STOPWORDS\nimport re\nfrom nltk.corpus import stopwords\n#nltk.download('stopwords')","b17a186f":"def cleanedWords(raw_sentence):\n    letters_only = re.sub(\"[^a-zA-Z]\", \" \", raw_sentence)\n    words = letters_only.lower().split()                            \n    stops = set(stopwords.words(\"english\"))                  \n    meaningful_words = [w for w in words\n                       if w not in stops]\n    return meaningful_words","b1e19886":"#sms=tweet\n#cr\u00e9ation de la base tweet\ntweet=data[['text','author']]\n#fonction de code en binaire 0\/1\ndef transformSpamColumn(x):\n    if x=='Clinton':\n        return 0\n    return 1\n#appication de la fonction sur tweet['author']\ntweet['author']=tweet['author'].apply(transformSpamColumn)","4c7896e1":"def getWordCloud(data, author):\n    data=data[data['author'] == author]\n    words = ' '.join(df['text'])\n    cleaned_word = \" \".join(cleanedWords(words))\n    wordcloud = WordCloud(stopwords=STOPWORDS,\n                      background_color='black',\n                      width=3000,\n                      height=2500\n                     )\n    wordcloud.generate(cleaned_word)\n    plt.figure(1,figsize=(12, 12))\n    plt.imshow(wordcloud)\n    plt.axis('off')\n    plt.show()","63dc5b4d":"#les mots les plus fr\u00e9quent de Trump\ngetWordCloud(tweet,1)","5ee5c05d":"#Les mots plus fr\u00e9quents de Clinton\ngetWordCloud(tweet,0)","484eada1":"from textblob import TextBlob\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.svm import SVC, LinearSVC\nfrom sklearn.metrics import classification_report, f1_score, accuracy_score, confusion_matrix\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import learning_curve, GridSearchCV\nfrom sklearn.model_selection import StratifiedKFold, cross_val_score, train_test_split \n#from sklearn.cross_validation import StratifiedKFold\nfrom sklearn.tree import DecisionTreeClassifier \n\nimport calendar\nimport datetime\nimport re","c4a854e9":"messages = df[['author','text']]\ntest = xtest[['text']] \nmessages.columns = ['label', 'message']\ntest.columns = ['message']","9d319eba":"print(messages[:5])","2d83bb7a":"def split_into_tokens(message):\n    message = message  # convert bytes into proper unicode\n    return TextBlob(message).words","3dd6d479":"messages.message.head()","5bad3fca":"messages.message.apply(split_into_tokens).head()","7a984351":"# nltk.download('wordnet')\ndef split_into_lemmas(message):\n    message = message.lower()\n    words = TextBlob(message).words\n    # for each word, take its \"base form\" = lemma \n    return [word.lemma for word in words]\n\nmessages.message.apply(split_into_lemmas).head()","577b82c1":"bow_transformer = CountVectorizer(analyzer=split_into_lemmas).fit(messages['message'])\nprint(len(bow_transformer.vocabulary_))\nprint(bow_transformer.get_feature_names()[:5])","86061009":"messages_bow = bow_transformer.transform(messages['message'])\nprint('sparse matrix shape:', messages_bow.shape)\nprint('number of non-zeros:', messages_bow.nnz)\nprint('sparsity: %.2f%%' % (100.0 * messages_bow.nnz \/ (messages_bow.shape[0] * messages_bow.shape[1])))","cea83da5":"tfidf_transformer = TfidfTransformer().fit(messages_bow)","3483501c":"print (tfidf_transformer.idf_[bow_transformer.vocabulary_['the']])\nprint (tfidf_transformer.idf_[bow_transformer.vocabulary_['hannity']])","743adbe1":"messages_tfidf = tfidf_transformer.transform(messages_bow)\nprint(messages_tfidf.shape)","2c345bf8":" messages['label'] = messages['label']\n messages['label'] =  messages['label'].astype('int64')","ee566e45":"spam_detector = SVC(kernel='sigmoid', gamma=1.0).fit(messages_tfidf,  messages['label'])","8a6ba63c":"all_predictions = spam_detector.predict(messages_tfidf)","d09b7c24":"tr_acc = accuracy_score(messages['message'], all_predictions)\n#print(\"Accuracy on training set:  %.2f%%\" % (100 * tr_acc))","867bd5ae":"pipeline = Pipeline([\n    ('bow', CountVectorizer(analyzer=split_into_lemmas)),  # strings to token integer counts\n    ('tfidf', TfidfTransformer()),  # integer counts to weighted TF-IDF scores\n    ('classifier', SVC(kernel='sigmoid', gamma=1.0)),  # train on TF-IDF vectors w\/ Naive Bayes classifier\n])        ","09835bd8":"scores = cross_val_score(pipeline,  # steps to convert raw messages into models\n                         messages['message'], # training data\n                         messages['label'],  # training labels\n                         cv=10,  # split data randomly into 10 parts: 9 for training, 1 for scoring\n                         scoring='accuracy',  # which scoring metric?\n                         n_jobs=-1,  # -1 = use all cores = faster\n                         )\nprint(scores)","aba8935c":"print(classification_report(messages['label'], all_predictions))","e7e491d1":"print('Mean score:', scores.mean(), '\\n')\nprint('Stdev:', scores.std())","eab8d75d":"skf = StratifiedKFold(n_splits=5)\nparams = {\n    'tfidf__use_idf': (True, False),\n    'bow__analyzer': (split_into_lemmas, split_into_tokens),\n}\n\ngrid = GridSearchCV(\n    pipeline,  # pipeline from above\n    params,  # parameters to tune via cross validation\n    refit=True,  # fit using all available data at the end, on the best found param combination\n    n_jobs=-1,  # number of cores to use for parallelization; -1 for \"all cores\"\n    scoring='accuracy',  # what score are we optimizing?\n    cv=StratifiedKFold( n_splits=5),  # what type of cross validation to use\n)\n\n%time nb_detector = grid.fit(messages['message'], messages['label'])\nprint(nb_detector.cv_results_)","c8d77128":"from sklearn.model_selection import GridSearchCV\nnb_detector.cv_results_","c467cef0":"predictions = nb_detector.predict(test['message'])","f8a810c8":"print(predictions.shape[0])","3d16b65e":"predictions = pd.Series(predictions)\npredictions.index = range(predictions.shape[0])\npredictions.to_csv(\"predictions.csv\", index_label =\"index\", header = [\"prediction\"])","a94dc4ab":"1. D'apr\u00e8s ce graphique les twet de Clinton sont plus longue en moyenne que ceux de Trump ","952cccd7":"# Textmining Tweets Trump vs Clinton","50867a92":"Dans la  base de donn\u00e9e, nous avons 50% de tweet fait par Clinton et 50% fait par Trump","cdde1cd3":"# Partie Mod\u00e9lisation ","d064658a":"Soumission ","55397dd0":"En moyenne  Trump \u00e0 plus de favorite_count que Clinton ","8bd13b45":" Dans notre base donn\u00e9es df, il n'y a aucune valeurs manquantes "}}