{"cell_type":{"d334c48c":"code","80aa488b":"code","32adea00":"code","7b78ebde":"code","8958ee7a":"code","3111d29a":"code","0bbea582":"code","ef958a66":"code","01b5443a":"code","b3bc4b42":"code","b068d85a":"code","8a3380e9":"code","c954bed4":"code","5d458179":"code","680cce1e":"code","f220bb61":"code","4ee587a9":"code","9001b470":"code","9226695e":"code","b7aac5f6":"code","9bdb98cd":"code","237ec00a":"code","ea1f30d3":"code","ee88970a":"code","7d8271f8":"code","66380ecc":"code","cb54fa36":"code","8248cbd7":"code","d44041f2":"code","a6d5bd67":"code","cd918a7d":"code","f3a5ad65":"code","e8faa756":"code","3ef15752":"code","35ede794":"code","df992290":"code","213513fe":"code","da1411a2":"code","20d7e5a1":"code","6853f88f":"code","8c154869":"code","ce7c5ddd":"code","40b7966c":"code","050e4ad6":"code","37cdcbd7":"code","0ea37de0":"code","07953166":"code","03cdb5e3":"code","9b3ecae4":"code","1e3aff72":"code","3982a0e7":"code","0292d320":"code","9f371a4d":"code","8caf12e4":"code","fdc54826":"code","cc1e7ffc":"code","2c070a47":"code","b9e064a9":"markdown","6b1ef39f":"markdown","536fa8f0":"markdown","bcb8b22f":"markdown","99860c18":"markdown","78184b90":"markdown","94a2eccf":"markdown","b0650641":"markdown","abdde240":"markdown","4cc64eae":"markdown","a701955f":"markdown","ddf85d98":"markdown","1e28e68f":"markdown","5313399c":"markdown","33ce64db":"markdown","d179d40e":"markdown","48f25b0d":"markdown","a7d4c896":"markdown","58ee133f":"markdown","2cf40259":"markdown","937dc9c5":"markdown"},"source":{"d334c48c":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","80aa488b":"import numpy as np\nimport pandas as pd \nimport seaborn as sns","32adea00":"df=pd.read_csv(\"\/kaggle\/input\/kickstarter-projects\/ks-projects-201801.csv\", parse_dates=['deadline', 'launched'])","7b78ebde":"df.head()","8958ee7a":"df.groupby('state')['ID'].count()","3111d29a":"df = df.drop(df[df.state == \"live\"].index)\ndf.groupby('state')['ID'].count()","0bbea582":"mapping = {\"canceled\": 0 , \"failed\" : 0, \"suspended\" : 0, \"undefined\": 0, \"successful\": 1}\ndf[\"outcome\"] = df[\"state\"].map(mapping)","ef958a66":"(df.groupby('outcome')['ID'].count())","01b5443a":"df[\"outcome\"].hist()","b3bc4b42":"df.head()","b068d85a":"from sklearn.preprocessing import LabelEncoder\ncat_features = ['category', 'currency', 'country']\nencoder = LabelEncoder()\nencoded = df[cat_features].apply(encoder.fit_transform)\nencoded.head()","8a3380e9":"df = df.drop(df[cat_features], axis = 1)\ndf.head()","c954bed4":"df = pd.concat([df, encoded], axis = 1)","5d458179":"\ndf = df.assign(hour=df.launched.dt.hour,\n               day=df.launched.dt.day,\n               month=df.launched.dt.month,\n               year=df.launched.dt.year)\ndf.head()","680cce1e":"df = df[['goal', 'hour', 'day', 'month', 'year', 'category', 'currency', 'country', 'outcome']]\ndf.head()","f220bb61":"X = df[['goal', 'hour', 'day', 'month', 'year', 'category', 'currency', 'country']]\ny = df[\"outcome\"]\n\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y,\n                                                    stratify=y, \n                                                    test_size=0.25)\n","4ee587a9":"import lightgbm as lgb\n\ndtrain = lgb.Dataset(X_train, label=y_train)\ndvalid = lgb.Dataset(X_test, label=y_test)\n\nparam = {'num_leaves': 64, 'objective': 'binary'}\nparam['metric'] = 'auc'\nnum_round = 100\nbst = lgb.train(param, dtrain, num_round, valid_sets=[dvalid], early_stopping_rounds=10, verbose_eval=False)","9001b470":"from sklearn import metrics\nypred = bst.predict(X_test)\nscore = metrics.roc_auc_score(y_test, ypred)\n\nprint(f\"Test AUC score: {score}\")","9226695e":"ypred.shape","b7aac5f6":"from sklearn.metrics import roc_curve, auc , roc_auc_score\nfrom matplotlib import pyplot\n\nlr_fpr, lr_tpr, _ = roc_curve(y_test, ypred)","9bdb98cd":"ns_probs = [0 for _ in range(len(y_test))]\nns_auc = roc_auc_score(y_test, ns_probs)\nns_fpr, ns_tpr, _ = roc_curve(y_test, ns_probs)\n\npyplot.plot(ns_fpr, ns_tpr, marker='.', label='None')\npyplot.plot(lr_fpr, lr_tpr, marker='.', label='model')\n# axis labels\npyplot.xlabel('False Positive Rate')\npyplot.ylabel('True Positive Rate')\n# show the legend\npyplot.legend()\n# show the plot\npyplot.show()","237ec00a":"import category_encoders as ce","ea1f30d3":"cat_features = ['category', 'currency', 'country']\ncount_enc = ce.CountEncoder(cols=cat_features)\ncount_enc.fit(X_train[cat_features])","ee88970a":"train_encoded = X_train.join(count_enc.transform(X_train[cat_features]).add_suffix('_count'))\nvalid_encoded = X_test.join(count_enc.transform(X_test[cat_features]).add_suffix('_count'))","7d8271f8":"valid_encoded.head()","66380ecc":"import lightgbm as lgb\n\ndtrain = lgb.Dataset(train_encoded, label=y_train)\ndvalid = lgb.Dataset(valid_encoded, label=y_test)\n\nparam = {'num_leaves': 64, 'objective': 'binary'}\nparam['metric'] = 'auc'\nnum_round = 100\nbst = lgb.train(param, dtrain, num_round, valid_sets=[dvalid], early_stopping_rounds=10, verbose_eval=False)","cb54fa36":"ypred = bst.predict(valid_encoded)\nscore = metrics.roc_auc_score(y_test, ypred)\nprint (score)","8248cbd7":"import category_encoders as ce\ncat_features = ['category', 'currency', 'country']\n# Create the encoder itself\ntarget_enc = ce.TargetEncoder(cols=cat_features)\n# Fit the encoder using the categorical features and target\ntarget_enc.fit(X_train[cat_features], y_train)\n\ntrain_encoded = X_train.join(target_enc.transform(X_train[cat_features]).add_suffix('_target'))\nvalid_encoded = X_test.join(target_enc.transform(X_test[cat_features]).add_suffix('_target'))\n\ntrain_encoded.head()","d44041f2":"import lightgbm as lgb\n\ndtrain = lgb.Dataset(train_encoded, label=y_train)\ndvalid = lgb.Dataset(valid_encoded, label=y_test)\n\nparam = {'num_leaves': 64, 'objective': 'binary'}\nparam['metric'] = 'auc'\nnum_round = 100\nbst = lgb.train(param, dtrain, num_round, valid_sets=[dvalid], early_stopping_rounds=10, verbose_eval=False)\nypred = bst.predict(valid_encoded)\nscore = metrics.roc_auc_score(y_test, ypred)\nprint (score)","a6d5bd67":"import category_encoders as ce\ncat_features = ['category', 'currency', 'country']\n# Create the encoder itself\ntarget_enc = ce.CatBoostEncoder(cols=cat_features)\n# Fit the encoder using the categorical features and target\ntarget_enc.fit(X_train[cat_features], y_train)\n\ntrain_encoded = X_train.join(target_enc.transform(X_train[cat_features]).add_suffix('_cb'))\nvalid_encoded = X_test.join(target_enc.transform(X_test[cat_features]).add_suffix('_cb'))\n\ntrain_encoded.head()","cd918a7d":"import lightgbm as lgb\n\ndtrain = lgb.Dataset(train_encoded, label=y_train)\ndvalid = lgb.Dataset(valid_encoded, label=y_test)\n\nparam = {'num_leaves': 64, 'objective': 'binary'}\nparam['metric'] = 'auc'\nnum_round = 100\nbst = lgb.train(param, dtrain, num_round, valid_sets=[dvalid], early_stopping_rounds=10, verbose_eval=False)\nypred = bst.predict(valid_encoded)\nscore = metrics.roc_auc_score(y_test, ypred)\nprint (score)","f3a5ad65":"df=pd.read_csv(\"\/kaggle\/input\/kickstarter-projects\/ks-projects-201801.csv\", parse_dates=['deadline', 'launched'])","e8faa756":"interactions = df['category'] + \"_\" + df['country']\nprint(interactions.head(10))","3ef15752":"from sklearn.preprocessing import LabelEncoder\nlabel_enc = LabelEncoder()\ndf = df.assign(category_country=label_enc.fit_transform(interactions))\ndf.head()","35ede794":"launched = pd.Series(df.index, index=df.launched, name=\"count_7_days\").sort_index()\nlaunched.head(20)","df992290":"import matplotlib.pyplot as plt\ncount_7_days = launched.rolling('7d').count() - 1\nprint(count_7_days.head(20))\n\n# Ignore records with broken launch dates the first 7 numbers..\nplt.plot(count_7_days[7:]);\nplt.title(\"Competitions in the last 7 days\");","213513fe":"count_7_days.index = launched.values  # launched.values  are the index\ncount_7_days = count_7_days.reindex(df.index)\ncount_7_days.head(10)","da1411a2":"df = df.join(count_7_days)\ndf.head()","20d7e5a1":"def time_since_last_project(series):\n    # Return the time in hours\n    return series.diff().dt.total_seconds() \/ 3600.\n\ndf_temp = df[['category', 'launched']].sort_values('launched')\ndf_temp.head()","6853f88f":"\ndf_temp.groupby('category').count().head(10)","8c154869":"timedeltas = df_temp.groupby('category').transform(time_since_last_project)\ntimedeltas.head(20)","ce7c5ddd":"timedeltas = timedeltas.fillna(timedeltas.median()).reindex(df.index)\ntimedeltas.head(20)","40b7966c":"df[\"competitionValue\"] = timedeltas","050e4ad6":"df.head()","37cdcbd7":"df = df.drop(df[df.state == \"live\"].index)\nmapping = {\"canceled\": 0 , \"failed\" : 0, \"suspended\" : 0, \"undefined\": 0, \"successful\": 1}\ndf[\"outcome\"] = df[\"state\"].map(mapping)\nfrom sklearn.preprocessing import LabelEncoder\ncat_features = ['category', 'currency', 'country']\nencoder = LabelEncoder()\nencoded = df[cat_features].apply(encoder.fit_transform)\ndf = df.drop(df[cat_features], axis = 1)\ndf = pd.concat([df, encoded], axis = 1)\ndf = df.assign(hour=df.launched.dt.hour,\n               day=df.launched.dt.day,\n               month=df.launched.dt.month,\n               year=df.launched.dt.year)\ndf.head()","0ea37de0":"X = df[['goal', 'hour', 'day', 'month', 'year', 'category', 'currency', 'country', 'category_country',\n       'count_7_days', 'competitionValue']]\ny = df[\"outcome\"]\n\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y,\n                                                    stratify=y, \n                                                    test_size=0.25)","07953166":"import category_encoders as ce\ncat_features = ['category', 'currency', 'country']\n# Create the encoder itself\ntarget_enc = ce.CatBoostEncoder(cols=cat_features)\n# Fit the encoder using the categorical features and target\ntarget_enc.fit(X_train[cat_features], y_train)\n\ntrain_encoded = X_train.join(target_enc.transform(X_train[cat_features]).add_suffix('_cb'))\nvalid_encoded = X_test.join(target_enc.transform(X_test[cat_features]).add_suffix('_cb'))\n\ntrain_encoded.head()","03cdb5e3":"import lightgbm as lgb\n\ndtrain = lgb.Dataset(train_encoded, label=y_train)\ndvalid = lgb.Dataset(valid_encoded, label=y_test)\n\nparam = {'num_leaves': 64, 'objective': 'binary'}\nparam['metric'] = 'auc'\nnum_round = 100\nbst = lgb.train(param, dtrain, num_round, valid_sets=[dvalid], early_stopping_rounds=10, verbose_eval=False)\nypred = bst.predict(valid_encoded)\nscore = metrics.roc_auc_score(y_test, ypred)\nprint (score)","9b3ecae4":"from sklearn.feature_selection import SelectKBest, f_classif\n# Keep 5 features\nselector = SelectKBest(f_classif, k=5)\nX_new = selector.fit_transform(train_encoded, y_train)\nX_new","1e3aff72":"# Get back the features we've kept, zero out all other features\nselected_features = pd.DataFrame(selector.inverse_transform(X_new), \n                                 index=train_encoded.index, \n                                 columns=train_encoded.columns)\nselected_features.head()","3982a0e7":"selected_columns = selected_features.columns[selected_features.var() != 0]\ntrain_encoded[selected_columns].head()","0292d320":"import lightgbm as lgb\n\ndtrain = lgb.Dataset(train_encoded[selected_columns], label=y_train)\ndvalid = lgb.Dataset(valid_encoded[selected_columns], label=y_test)\n\nparam = {'num_leaves': 64, 'objective': 'binary'}\nparam['metric'] = 'auc'\nnum_round = 100\nbst = lgb.train(param, dtrain, num_round, valid_sets=[dvalid], early_stopping_rounds=10, verbose_eval=False)\nypred = bst.predict(valid_encoded[selected_columns])\nscore = metrics.roc_auc_score(y_test, ypred)\nprint (score)","9f371a4d":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.feature_selection import SelectFromModel\n\n","8caf12e4":"# Set the regularization parameter C=1\nlogistic = LogisticRegression(C=1, penalty=\"l1\", random_state=7).fit(train_encoded, y_train)\nmodel = SelectFromModel(logistic, prefit=True)\n\nX_new = model.transform(train_encoded)","fdc54826":"# Get back the features we've kept, zero out all other features\nselected_features_2 = pd.DataFrame(model.inverse_transform(X_new), \n                                 index=train_encoded.index, \n                                 columns=train_encoded.columns)\nselected_features_2.head()","cc1e7ffc":"selected_columns = selected_features_2.columns[selected_features_2.var() != 0] # var() --> variance of columns\ntrain_encoded[selected_columns].head()","2c070a47":"import lightgbm as lgb\n\ndtrain = lgb.Dataset(train_encoded[selected_columns], label=y_train)\ndvalid = lgb.Dataset(valid_encoded[selected_columns], label=y_test)\n\nparam = {'num_leaves': 64, 'objective': 'binary'}\nparam['metric'] = 'auc'\nnum_round = 100\nbst = lgb.train(param, dtrain, num_round, valid_sets=[dvalid], early_stopping_rounds=10, verbose_eval=False)\nypred = bst.predict(valid_encoded[selected_columns])\nscore = metrics.roc_auc_score(y_test, ypred)\nprint (score)","b9e064a9":"# Train","6b1ef39f":"Do projects in the same category compete for donors? If you're trying to fund a video game and another game project was just launched, you might not get as much money. We can capture this by calculating the time since the last launch project in the same category.\n\n* .groupby then .transform. The .transform method takes a function then passes a series or dataframe to that function for each group. This returns a dataframe with the same indices as the original dataframe. In our case, we'll perform a groupby on \"category\" and use transform to calculate the time differences for each category.","536fa8f0":"* Set the type of 'deadline', 'launched' as datetime type in pandas","bcb8b22f":"# extra \n\n* np.log transform the goal column","99860c18":"# Creating new features from the raw data ","78184b90":"# feature selections","94a2eccf":"### 2) Count encodings\n\nHere, encode the categorical features `['category', 'currency', 'country']` using the count of each value in the data set. Using `CountEncoder` from the `category_encoders` library, fit the encoding using the categorical feature columns defined in `cat_features`. Then apply the encodings to the train and validation sets, adding them as new columns with names suffixed `\"_count\"`.","b0650641":"# Training test split","abdde240":"### Exercise from the Kaggle Course\n\n* https:\/\/www.kaggle.com\/learn\/feature-engineering","4cc64eae":"### Categorical data\n* https:\/\/stackoverflow.com\/questions\/24458645\/label-encoding-across-multiple-columns-in-scikit-learn[](http:\/\/)","a701955f":"* start over..","ddf85d98":"# L1 regularization\nUnivariate methods consider only one feature at a time when making a selection decision. Instead, we can make our selection using all of the features by including them in a linear model with L1 regularization. This type of regularization (sometimes called Lasso) penalizes the absolute magnitude of the coefficients, as compared to L2 (Ridge) regression which penalizes the square of the coefficients.","1e28e68f":"## Training\n","5313399c":"### Target\n* drop live\n* set successful as 1\n* set the rest as 0","33ce64db":"We get NaNs here for projects that are the first in their category. We'll need to fill those in with something like the mean or median. We'll also need to reset the index so we can join it with the other data","d179d40e":"### Evaluation\n","48f25b0d":"* Count encoding is a good idea, how does it improve the model score?\n\n* Rare values tend to have similar counts (with values like 1 or 2), so you can classify rare values together at prediction time. Common values with large counts are unlikely to have the same exact count as other values. So, the common\/important values get their own grouping.","a7d4c896":"### CatBoost Encoding\n* This is similar to target encoding in that it's based on the target probablity for a given value. However with CatBoost, for each row, the target probability is calculated only from the rows before it.","58ee133f":"# train test split again","2cf40259":"# Train the model on the encoded dataset","937dc9c5":"### 4) Target encoding\n\n* supervised encodings that use the labels (the targets) to transform categorical features. \n* Target encoding replaces a categorical value with the average value of the target for that value of the feature. "}}