{"cell_type":{"72859cf1":"code","0739545b":"code","e679d9da":"code","72006862":"code","114b2ded":"code","243a00ba":"code","6e34b567":"code","b0ba4bbb":"code","8c1a8291":"code","03579693":"code","b61e9618":"code","6293c58c":"code","8f506d84":"code","aed3de3f":"code","bc16eae0":"code","ac6a8414":"code","8efdcc99":"code","cd4217a7":"code","a81b2b45":"code","76bede11":"code","f3b15194":"code","dea6ebcf":"code","80108d36":"code","4023d36a":"code","72ce8683":"code","6aeff561":"code","7da0d69e":"code","b4a20843":"code","3b08171e":"code","6efa4467":"code","3291f6fa":"code","714adba1":"code","b037cdee":"code","391501ed":"code","23e9ad94":"code","19f2339e":"code","dbc66d3c":"code","3fcc0966":"code","4ee70d2e":"code","3a44a854":"code","619aa9fc":"code","b400a672":"code","98944cfe":"code","33ab4edd":"code","e44cd0c4":"code","8fc648e5":"code","8108c588":"code","99d8a11b":"code","58dd5812":"markdown","dc59c7fb":"markdown","d669cc65":"markdown","ba556bbe":"markdown","de33e2b2":"markdown","3194a532":"markdown","363b2f39":"markdown","933d8f6e":"markdown","21b8284e":"markdown","7f01f3dd":"markdown","81599b18":"markdown","63159654":"markdown"},"source":{"72859cf1":"# install World Bank's API librari\n# if you get an error, try enabling the \"Internet\" setting in your notebook\n\n!pip install wbgapi","0739545b":"# importing libraries:\n\nimport wbgapi as wb\n\nimport pandas as pd\nimport numpy as np\nimport re\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport itertools\nfrom statsmodels.tsa.seasonal import seasonal_decompose\nfrom statsmodels.tsa.stattools import adfuller\nfrom sklearn.model_selection import TimeSeriesSplit\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import OrdinalEncoder\nfrom catboost import CatBoostRegressor","e679d9da":"# disabling warnings:\n\npd.options.mode.chained_assignment = None","72006862":"# loading datasets:\n\ntrain_orig = pd.read_csv('\/kaggle\/input\/tabular-playground-series-jan-2022\/train.csv')\ntest_orig = pd.read_csv('\/kaggle\/input\/tabular-playground-series-jan-2022\/test.csv')","114b2ded":"# date range in datasets:\n\nmin_date = min(train_orig['date'])\nmax_date = max(test_orig['date'])\n\nprint(f'min date in datasets:{min_date}\\nmax date in datasets:{max_date}')","243a00ba":"# extra data from worldbank API:\n# I couldn't figure out how to get the data link from the API, so I opened the website and copied the link from the URL.\n# example link \"NY.GDP.MKTP.CD\"\n\nGDP_wb_df = wb.data.DataFrame('NY.GDP.MKTP.CD', ['FIN', 'SWE', 'NOR'] , range(2015, 2020, 1))\npopulation_wb_df = wb.data.DataFrame('SP.POP.TOTL', ['FIN', 'SWE', 'NOR'] , range(2015, 2020, 1))\nperc_urban_population_wb_df = wb.data.DataFrame('SP.URB.TOTL.IN.ZS', ['FIN', 'SWE', 'NOR'] , range(2015, 2020, 1))\npopulation_density_wb_df = wb.data.DataFrame('EN.POP.DNST', ['FIN', 'SWE', 'NOR'] , range(2015, 2020, 1))","6e34b567":"print('GDP 2015 - 2019 subdataset')\ndisplay(GDP_wb_df)\nprint()\nprint('Population 2015 - 2019 subdataset')\ndisplay(population_wb_df)\nprint()\nprint('Urban population (% of total population) 2015 - 2019 subdataset')\ndisplay(perc_urban_population_wb_df)\nprint()\nprint('Population density (people per sq. km of land area) 2015 - 2019 subdataset')\ndisplay(population_density_wb_df)","b0ba4bbb":"print(f'train_orig df size - {train_orig.shape}')\nprint(f'test_orig df size - {test_orig.shape}')","8c1a8291":"# checking for missing values:\n\ndef na_values(data):\n    report = data.isna().sum().to_frame()\n    report = report.rename(columns = {0: 'missing_values'})\n    report = report.loc[report['missing_values'] != 0]\n    report['% of total'] = (report['missing_values'] \/ data.shape[0]).round(2)\n    return report.sort_values(by = 'missing_values', ascending = False)","03579693":"display(na_values(train_orig))\ndisplay(na_values(test_orig))","b61e9618":"train_orig.info()","6293c58c":"train_orig.head()","8f506d84":"cat_features_list = ['country', 'store', 'product']\ntarget_feature = 'num_sold'\ndata_feature = 'date'\nrow_index = 'row_id'\nvalidation_size = 0.2\n\ncountrys_list = train_orig['country'].unique()\nstores_list = train_orig['store'].unique()\nproducts_list = train_orig['product'].unique()","aed3de3f":"# all combinations:\n\nfeatures_combinations = [list(item) for item in itertools.product(countrys_list, stores_list, products_list)]","bc16eae0":"# save train to dictionary:\n\nfull_data_dict = {}\n\nfor combination in features_combinations:\n    full_data_dict[(' & '.join (combination))] = train_orig.loc[(train_orig['country'] == combination[0]) & \n                                                (train_orig['store'] == combination[1]) & \n                                                (train_orig['product'] == combination[2])]","ac6a8414":"# cast to datetime64 type\n\nfor key in full_data_dict:\n    full_data_dict[key].loc[:,data_feature] = pd.to_datetime(\n        full_data_dict[key].loc[:,data_feature].copy() , format='%Y-%m-%d')","8efdcc99":"# convert the \"date\" feature to an index.\n\nfor key in full_data_dict:\n    full_data_dict[key] = full_data_dict[key].set_index(data_feature)","cd4217a7":"# sort the df in ascending index order\n\nfor key in full_data_dict:\n    full_data_dict[key] = full_data_dict[key].sort_index()","a81b2b45":"# creating a subdataset and drop unnecessary features for analysis:\n\ndata_dict = full_data_dict.copy()\ndrop_columns = ['row_id','country','store','product']\n\nfor key in data_dict:\n    data_dict[key] = data_dict[key].drop(drop_columns, axis=1)","76bede11":"# check the continuity of the time series using the \"is_monotonic\" function.\n\nfor key in data_dict:\n    print(f'Time series \"{key}\" is monotonic: {data_dict[key].index.is_monotonic}')","f3b15194":"# checking the time series for stationarity\n# use Dickey-Fuller test https:\/\/www.statsmodels.org\/dev\/generated\/statsmodels.tsa.stattools.adfuller.html\n# H0 - The time series is not stationary\n# H1 - The time series is stationary\n\nalpha = 0.05\nadfuller_dict = {'stationary': [],'stationary_p-value': [],'non-stationary': [],'non-stationary_p-value': []}\n\nfor key in data_dict:\n    values_seria = data_dict[key].values\n    result = adfuller(values_seria)\n    if result[1] < alpha:\n        adfuller_dict['stationary'].append(key)\n        adfuller_dict['stationary_p-value'].append(f'{result[1]:.3f}')\n    else:\n        adfuller_dict['non-stationary'].append(key)\n        adfuller_dict['non-stationary_p-value'].append(f'{result[1]:.3f}')\n        \ndisplay(pd.DataFrame.from_dict(adfuller_dict))","dea6ebcf":"# if you need to visually analyze trends and seasonality:\n\nplot_graphs = False","80108d36":"#Trend analysis\n\nif plot_graphs:\n    for key in data_dict:\n        decomposed = seasonal_decompose(data_dict[key]) \n        dec_trend = decomposed.trend\n        dec_trend.plot(figsize=(12,7))\n        plt.xlabel('Date', fontsize=15)\n        plt.ylabel('Number of sales ',fontsize=15)\n        plt.title(f'Trend {key}', fontsize=15)\n        plt.grid()\n        plt.show()\n","4023d36a":"# Seasonal analysis\n\nif plot_graphs:\n    for key in data_dict:\n        decomposed = seasonal_decompose(data_dict[key])\n        dec_seasonal = decomposed.seasonal    \n        dec_seasonal['2018-03-01':'2018-03-31'].plot(figsize=(12,7))\n        plt.xlabel('Date', fontsize=15)\n        plt.ylabel('Number of sales ',fontsize=15)\n        plt.title(f'Trend {key}', fontsize=15)\n        plt.grid()\n        plt.show()","72ce8683":"# train and validation split\n\ntrain, valid = train_test_split(train_orig, shuffle=False, test_size = validation_size)\ntest = test_orig.copy()","6aeff561":"# cast to datetime64 type\n# convert the \"date\" feature to an index.\n\ndef date_conversion_to_index(data,data_feature):\n    data_copy = data.copy()\n    data_copy.loc[:,data_feature] = pd.to_datetime(data.loc[:,data_feature], format='%Y-%m-%d')\n    data_copy = data_copy.set_index(data_feature)\n    return data_copy","7da0d69e":"train = date_conversion_to_index(train,data_feature)\nvalid = date_conversion_to_index(valid,data_feature)\ntest = date_conversion_to_index(test,data_feature)","b4a20843":"def make_date_features(data):\n    data_copy = data.copy()\n    data_copy['year'] = data_copy.index.year\n    data_copy['quarter'] = data_copy.index.quarter\n    data_copy['month'] = data_copy.index.month\n    data_copy['week'] = data_copy.index.isocalendar().week\n    data_copy['day_of_year'] = data_copy.index.dayofyear\n    data_copy['day_of_week'] = data_copy.index.dayofweek\n    data_copy['weekend'] = data_copy['day_of_week'].isin([5,6])*1   \n    return data_copy","3b08171e":"train = make_date_features(train)\nvalid = make_date_features(valid)\ntest = make_date_features(test)","6efa4467":"# using extra data from www.worldbank.org\/\n\ndef make_features_from_wbdf(data,\n                            data_group_column,\n                            wbdf,\n                            match_dict,\n                            left_on_feature,\n                            right_on_feature):\n    \n    wb_df = wbdf.T\n    wb_df_columns = list(wb_df.columns)\n    \n    # dictionary substitution\n    for item in wb_df_columns:\n        if item in match_dict:\n            wb_df_columns[wb_df_columns.index(item)] = match_dict[item]\n    wb_df.columns = wb_df_columns\n    \n    # merge\n    wb_df = wb_df.reset_index()\n    wb_df['index'] = wb_df['index'].str.replace(r\"[^\\d]\", \"\", regex=True)\n    wb_df['index'] = wb_df['index'].astype('int')\n    merge_data = data.merge(wb_df,\n                             how = 'left',\n                             left_on = left_on_feature,\n                             right_on = right_on_feature)\n    merge_data = merge_data.set_index(data.index)\n    \n    # getting a single feature\n    for column in wb_df_columns:\n        merge_data[column].loc[merge_data[data_group_column] != column] = np.nan\n    merge_data['new_feature_name'] = merge_data[wb_df_columns].sum(axis=1)\n    \n    return merge_data['new_feature_name']\n    ","3291f6fa":"# dictionary for matching country names\n\nmatch_dict = {'FIN' : 'Finland', \n              'SWE' : 'Sweden', \n              'NOR': 'Norway'}","714adba1":"train['population'] = make_features_from_wbdf(train,\n                                              'country',\n                                              population_wb_df,\n                                              match_dict,\n                                              'year',\n                                              'index')\n\ntrain['gdp'] = make_features_from_wbdf(train,\n                                       'country',\n                                       GDP_wb_df,\n                                       match_dict,\n                                       'year',\n                                       'index')\n\ntrain['perc_urban_population'] = make_features_from_wbdf(train,\n                                                         'country',\n                                                         perc_urban_population_wb_df,\n                                                         match_dict,\n                                                         'year',\n                                                         'index')\n\ntrain['population_density'] = make_features_from_wbdf(train,\n                                                      'country',\n                                                      population_density_wb_df,\n                                                      match_dict,\n                                                      'year',\n                                                      'index')","b037cdee":"valid['population'] = make_features_from_wbdf(valid,\n                                              'country',\n                                              population_wb_df,\n                                              match_dict,\n                                              'year',\n                                              'index')\n\nvalid['gdp'] = make_features_from_wbdf(valid,\n                                       'country',\n                                       GDP_wb_df,\n                                       match_dict,\n                                       'year',\n                                       'index')\n\nvalid['perc_urban_population'] = make_features_from_wbdf(valid,\n                                                         'country',\n                                                         perc_urban_population_wb_df,\n                                                         match_dict,\n                                                         'year',\n                                                         'index')\n\nvalid['population_density'] = make_features_from_wbdf(valid,\n                                                      'country',\n                                                      population_density_wb_df,\n                                                      match_dict,\n                                                      'year',\n                                                      'index')","391501ed":"test['population'] = make_features_from_wbdf(test,\n                                              'country',\n                                              population_wb_df,\n                                              match_dict,\n                                              'year',\n                                              'index')\n\ntest['gdp'] = make_features_from_wbdf(test,\n                                       'country',\n                                       GDP_wb_df,\n                                       match_dict,\n                                       'year',\n                                       'index')\n\ntest['perc_urban_population'] = make_features_from_wbdf(test,\n                                                         'country',\n                                                         perc_urban_population_wb_df,\n                                                         match_dict,\n                                                         'year',\n                                                         'index')\n\ntest['population_density'] = make_features_from_wbdf(test,\n                                                      'country',\n                                                      population_density_wb_df,\n                                                      match_dict,\n                                                      'year',\n                                                      'index')","23e9ad94":"# make features subsets\n\nX_train = train.drop([row_index,target_feature], axis=1)\ny_train = train[target_feature]\n\nX_valid = valid.drop([row_index,target_feature], axis=1)\ny_valid = valid[target_feature]\n\nX_test = test.drop([row_index], axis=1)","19f2339e":"# encoding categorical features\n\nX_train[cat_features_list] = OrdinalEncoder().fit_transform(X_train[cat_features_list])\nX_valid[cat_features_list] = OrdinalEncoder().fit_transform(X_valid[cat_features_list])\nX_test[cat_features_list] = OrdinalEncoder().fit_transform(X_test[cat_features_list])","dbc66d3c":"# scaling\n\nscaler = StandardScaler()\n\ndef scaling_data(data,numerical_features):\n    scaler.fit(data[numerical_features])\n    data[numerical_features] = scaler.transform(data[numerical_features])","3fcc0966":"# in this case catboost works better without scaling\n\n#numerical_features = ['population', 'gdp']\n#scaling_data(X_train,numerical_features)\n#scaling_data(X_valid,numerical_features)\n#scaling_data(X_test,numerical_features)","4ee70d2e":"X_train['week'] = X_train['week'].astype('float64')\nX_valid['week'] = X_valid['week'].astype('float64')\nX_test['week'] = X_test['week'].astype('float64')","3a44a854":"# simple CatBoostRegressor model\n\nmodel = CatBoostRegressor(eval_metric='SMAPE',\n                          use_best_model=True,\n                          random_seed=123,\n                          verbose = 200)","619aa9fc":"model.fit(X_train,y_train,eval_set=[(X_valid, y_valid)])","b400a672":"def plot_feature_importances(model,features_train):\n    feature_names = list(features_train)\n    importances = model.feature_importances_\n    model_importances = pd.Series(importances, index=feature_names)\n    model_importances = model_importances.sort_values(ascending=False).head(10)\n    model_importances.plot.bar(figsize=(10,5))\n    plt.xlabel('Features', fontsize=15)\n    plt.ylabel('Importances',fontsize=15)\n    plt.title(f'Top-10 features', fontsize=15)\n    plt.grid()\n    plt.show()","98944cfe":"best_model = model","33ab4edd":"best_model.get_best_score()","e44cd0c4":"# model parameters\n\nbest_model.get_all_params()","8fc648e5":"# feature importances\n\nplot_feature_importances(best_model,X_valid)","8108c588":"model = best_model\npredictions_array = model.predict(X_test)\nsubmission = pd.DataFrame(data = zip(test_orig['row_id'], predictions_array), columns = ['row_id', 'num_sold'])","99d8a11b":"submission.to_csv('.\/submission.csv', index = False)","58dd5812":"* [1. Importing libraries and loading datasets](#1.0)\n* [2. Primary analysis](#2.0)\n* [3. Pre-processing](#3.0)\n* [4. Time series analysis](#4.0)\n* [5. Feature Engineering](#5.0)\n* [6. Predicting](#6.0)","dc59c7fb":"<a id = '4.0'><\/a>\n# 4. Time series analysis","d669cc65":"Originally wanted to build a SARIMA model, but it turned out to be too heavy. I found a great notebook on this from the grandmaster, but too late. https:\/\/www.kaggle.com\/kailex\/tabular-playground-22ts","ba556bbe":"<a id = '2.0'><\/a>\n# 2. Primary analysis","de33e2b2":"<a id = '1.0'><\/a>\n# 1. Importing libraries and loading datasets","3194a532":"Peaks of sales on certain days and weeks were noticed.\nWeekly seasonality. A special feature of the weekend and the day of the week is required.","363b2f39":"<a id = '5.0'><\/a>\n# 5. Feature Engineering","933d8f6e":"<a id = '6.0'><\/a>\n# 6. Predicting","21b8284e":"# Contents","7f01f3dd":"# Problem statement","81599b18":"Goal:\nBuild a model for forecasting the number of sales in stores based on historical data.\nTarget feature - \"num_sold\"\n\nMetric:\nSubmissions are evaluated on SMAPE between forecasts and actual values.","63159654":"<a id = '3.0'><\/a>\n# 3. Pre-processing"}}