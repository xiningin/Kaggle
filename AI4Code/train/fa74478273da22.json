{"cell_type":{"6d359c1e":"code","88b95281":"code","8284bf39":"code","ea6052cc":"code","bdd3d140":"code","d0391e9a":"code","5c261b1c":"code","7db8ed02":"code","f00bd981":"code","f0455bf0":"code","08870fca":"code","7fd886fe":"code","7c7f120d":"code","5bab3e90":"code","b77c6ad8":"code","d2e8ef8c":"code","bf4a78e2":"code","9cafbfba":"code","4465fed1":"code","fb797120":"code","6127d2d0":"code","232030d0":"code","9319697b":"code","0efb8c6a":"code","4ef45dd0":"code","34d911cb":"code","250fae20":"code","63ec57b7":"code","0891fd14":"code","b9d9e7d0":"code","b9bc7ba7":"code","e9d23c78":"code","5636aba0":"code","4a95ceac":"code","12bbaf9c":"code","92eb6ab5":"code","a26a7039":"code","dbf544ea":"code","88acbb18":"code","46e74f6b":"code","7c93c82e":"code","3fa4c5f1":"code","10236d7b":"code","f4a9083a":"code","f6795358":"code","331f1cca":"code","95b6314b":"code","649c9637":"code","71feb803":"code","aab5f850":"code","7bb700de":"code","be9e398a":"code","53755106":"code","562cbe21":"code","f13fc40b":"code","f4027f04":"code","38763203":"code","30a79577":"code","51f6f52a":"code","3713ddbe":"code","eb5688f7":"code","872a6dd1":"code","569031be":"code","96136ac9":"code","9a1bf9d7":"code","7f6a8105":"code","17b3648e":"code","4df0f476":"code","ff98553a":"code","8774fd38":"code","a9d7099d":"code","8801778a":"code","9730b099":"code","389b8e49":"markdown","93bda4b5":"markdown","8882832c":"markdown","1d14993d":"markdown","a0e9489f":"markdown","cfa22231":"markdown","edd60fab":"markdown","28588eb2":"markdown","0f90ad13":"markdown","57d2d312":"markdown","16f9e2d6":"markdown","e115b97b":"markdown","ef988e90":"markdown","fb77b07e":"markdown","855ee179":"markdown","9d98a990":"markdown","cc272f7d":"markdown","a4c4347d":"markdown","595ce886":"markdown","6cc5580d":"markdown","1f9c15e2":"markdown","446c5ec2":"markdown","39a3b2ca":"markdown","a499b570":"markdown","f500706d":"markdown"},"source":{"6d359c1e":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom pandas.io.json import json_normalize\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom wordcloud import WordCloud, STOPWORDS\nimport string\nimport json\n\n\nimport numpy as np \nimport pandas as pd \nfrom time import time\nimport re\nimport string\nimport os\nimport emoji\nimport collections\n\n\nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n#from sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import train_test_split\n\nfrom sklearn.metrics import mean_squared_error, r2_score\nfrom math import sqrt\n\n\n\nfrom sklearn.linear_model import LinearRegression\nfrom nltk.corpus import stopwords\nfrom nltk.stem import PorterStemmer\nfrom nltk.tokenize import word_tokenize\n\n#import requests\n#from mpl_toolkits.basemap import Basemap\n#from geopy.geocoders import Nominatim\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","88b95281":"df = pd.read_excel('..\/input\/twitterData.xlsx')","8284bf39":"df.head(5)","ea6052cc":"df.info()","bdd3d140":"#barplot function\n\ndef drawbarplot(x,y,xlabel,title,figsize=(10,10)):\n    plt.figure(figsize=figsize)    \n    sns.barplot(x=x,y=y,palette = 'terrain',orient='h',order=y)\n    for i,v in enumerate(x):\n        plt.text(0.8,i,v,color='k',fontsize=10)\n    \n    plt.title(title,fontsize=20)\n    plt.xlabel(xlabel,fontsize =14)\n    plt.show()\n\n","d0391e9a":"import datetime","5c261b1c":"#Converting datetime string to datetime\ndf['TweetPostedTime'] = [datetime.datetime.strptime(d, '%a %b %d %H:%M:%S %z %Y') for d in df['TweetPostedTime']]\ndf['UserSignupDate'] = [datetime.datetime.strptime(d, '%a %b %d %H:%M:%S %z %Y') for d in df['UserSignupDate']]","7db8ed02":"df['TweetPostedTime_hour'] = [d.hour for d in df['TweetPostedTime']]","f00bd981":"count =  df['TweetPostedTime_hour'].value_counts()\ndrawbarplot(x=count.values,y=count.index,xlabel='count',title='Time of the day distribution',figsize=(10,10))\n","f0455bf0":"#Which user is most active on twitter\ncount=df['UserName'].value_counts()\ndf_count=pd.DataFrame()\ndf_count['Username'] = count.index\ndf_count['activeCount'] = count.values\ndf_count = df_count.iloc[:50,:]\ndrawbarplot(x=df_count.activeCount,y=df_count.Username,xlabel='count',title='Top 50 active user in given time span',figsize=(16,16))","08870fca":"df_tweetcount = df.loc[:,['UserName','UserTweetCount']]\ndf_tweetcount.sort_values(by='UserTweetCount',ascending=False,inplace=True)\ndf_tweetcount.drop_duplicates(subset='UserName',keep='first',inplace=True)\ndf_count=pd.DataFrame()\ndf_count = df_tweetcount.iloc[:50,:]\ndrawbarplot(x=df_count.UserTweetCount,y=df_count.UserName,xlabel='count',title='All time Top 50 active user',figsize=(16,16))","7fd886fe":"#Which user has most number of follower on twitter\ndf_userfollower = df.loc[:,['UserName','UserFollowersCount']]\ndf_userfollower.sort_values(by='UserFollowersCount',ascending=False,inplace=True)\ndf_userfollower.drop_duplicates(subset='UserName',keep='first',inplace=True)\ndf_count =df_userfollower.iloc[:50,:]\ndrawbarplot(x=df_count.UserFollowersCount,y=df_count.UserName,xlabel='count',title='Top 50 active user',figsize=(16,16))\n","7c7f120d":"#Which user has most number of Friend on twitter\ndf_userFriend = df.loc[:,['UserName','UserFriendsCount']]\ndf_userFriend.sort_values(by='UserFriendsCount',ascending=False,inplace=True)\ndf_userFriend.drop_duplicates(subset='UserName',keep='first',inplace=True)\ndf_count = df_userFriend.iloc[:50,:]\ndrawbarplot(x=df_count.UserFriendsCount,y=df_count.UserName,xlabel='count',title='Top 50 Friendly user',figsize=(16,16))\n","5bab3e90":"#function to extract @mentions and #tags\ndef extracter():\n    mentions={}\n    tags={}\n    for i in df_trend_user.index:\n        tokens = df_trend_user['TweetBody'][i].split()    \n        for token in tokens:\n            if('@' in token[0] and len(token) > 1):\n                if token.strip('@') in mentions:\n                    mentions[token.strip('@')] += 1\n                else:\n                    mentions[token.strip('@')] = 1\n        \n        \n            if('#' in token[0] and len(token) > 1):\n                if token.strip('#') in tags:\n                    tags[token.strip('#')] += 1\n                else:\n                    tags[token.strip('#')] = 1    \n                    \n    return mentions,tags    ","b77c6ad8":"df_trend_user = df.loc[:,['UserName','TweetBody','TweetRetweetFlag','TweetRetweetCount','TweetFavoritesCount','TweetHashtags']]\n#df_trend_user.shape","d2e8ef8c":"mentions ,tags = extracter()","bf4a78e2":"mentions_keys = list(mentions.keys())\nmentions_values = list(mentions.values())\ntags_keys = list(tags.keys())\ntags_values = list(tags.values())","9cafbfba":"df_mention = pd.DataFrame(columns=['mentions','m_count'])\ndf_mention['mentions'] = mentions_keys\ndf_mention['m_count'] = mentions_values\ndf_mention.sort_values(ascending=False,by='m_count',inplace=True)\ndf_count = df_mention.iloc[:50,:]\ndrawbarplot(x=df_count.m_count,y=df_count.mentions,xlabel='Count of mentions',title='Top 50 mentions',figsize=(16,16))","4465fed1":"df_tags =pd.DataFrame(columns=['tags','t_count'])\ndf_tags['tags'] = tags_keys\ndf_tags['t_count'] = tags_values\ndf_tags.sort_values(ascending=False,by='t_count',inplace=True)\ndf_count = df_tags.iloc[:50,:]\ndrawbarplot(x=df_count.t_count,y=df_count.tags,xlabel='Count of tags',title='Top 50 Tags',figsize=(16,16))","fb797120":"df_trend_tweets = df.loc[:,['UserName','TweetBody','TweetRetweetFlag','TweetRetweetCount','TweetFavoritesCount','TweetHashtags']]\ndf_trend_tweets = df_trend_tweets.loc[(df_trend_tweets['TweetRetweetFlag'] == 1) & (df_trend_tweets['TweetRetweetCount'] > 1)]\ndf_trend_tweets.drop(columns='TweetRetweetFlag',axis=1,inplace=True)\n#Removing duplicate tweets\ndf_trend_tweets.drop_duplicates(keep='first',subset='TweetBody',inplace=True)\ndf_trend_tweets['TweetBody']= df_trend_tweets['TweetBody'].str.lower()\n#df_trend_user.shape","6127d2d0":"df_trend_tweets.head()","232030d0":"st_words = set(STOPWORDS)\n#enhancing stopword by removing @mentions and shorthands\nst_words.update(['https','CO','RT','Please','via','amp','place','new','ttot','best','great','top','ht','ysecrettravel','ysecrettravel_'])\nst_words.update([s.lower() for s in mentions_keys])","9319697b":"wc = WordCloud(height=600,repeat=False,width=1400,max_words=1000,stopwords=st_words,colormap='terrain',background_color='Cyan',mode='RGBA').generate(' '.join(df_trend_tweets['TweetBody'].dropna().astype(str)))\nplt.figure(figsize = (16,16))\nplt.imshow(wc)\nplt.title('Tweets Wordcloud')\nplt.axis('off')\nplt.show()","0efb8c6a":"df_userfollower= df_userfollower.merge(df[['UserTweetCount']],left_index=True,right_index=True,how='left',sort=False)\ndf_count = df_userfollower.iloc[:50,:]\ndrawbarplot(x=df_count.UserTweetCount,y=df_count.UserName,xlabel='Total Tweet counts',title='Total Tweet counts by top 50 leaders in Twitter',figsize=(16,16))","4ef45dd0":"df_userfollower.sort_values(by='UserTweetCount',ascending=False,inplace=True)\ndf_count = df_userfollower.iloc[:50,:]\ndrawbarplot(x=df_count.UserTweetCount,y=df_count.UserName,xlabel='Total Tweet counts',title='Top 50 Tweeters among leaders in Twitter',figsize=(16,16))","34d911cb":"#df_user_signup = df['UserrName','UserScreenName','UserLocation','UserDesciption','UserWarning','UserSignupDate']\n#df.columns[df.columns.str.startswith('User')]\ndf_user_signup=df.loc[:,df.columns[df.columns.str.startswith('User')]]\ndf_user_signup.sort_values(by='UserSignupDate',ascending=True,inplace=True)\ndf_user_signup.drop_duplicates(inplace=True,keep='first',subset='UserName')","250fae20":"df_count = df_user_signup.iloc[:50,:]\ndrawbarplot(x=df_count.UserFollowersCount,y=df_count.UserName,title='Number of Followers of top 50 Earliest Users',xlabel='No. of Users',figsize=(16,16))","63ec57b7":"#df_count = df_user_signup.iloc[:50,:]\ndrawbarplot(x=df_count.UserFriendsCount,y=df_count.UserName,title='Number of Friends of top 50 Earliest Users',xlabel='No. of Users',figsize=(16,16))","0891fd14":"df_user_signup['year_of_signup']=[d.year for d in df_user_signup['UserSignupDate']]\ndf_user_signup['month_of_signup']=[d.month for d in df_user_signup['UserSignupDate']]","b9d9e7d0":"count = df_user_signup['year_of_signup'].value_counts()\ndrawbarplot(x=count.values,y=count.index,xlabel='count',title='Year with Maximum User SignUp',figsize=(10,10))","b9bc7ba7":"#df_user_signup.groupby('year_of_signup')['month_of_signup'].transform('count')\nmonths = {1:'Jan',\n 2:'Feb',\n 3:'March',\n 4:'April',\n 5:'May',\n 6:'June',\n 7:'July',\n 8:'Aug',\n 9:'Sept',\n 10:'Oct',\n 11:'Nov',\n 12:'Dec'\n}\ndf_user_signup['month_of_signup'] =df_user_signup['month_of_signup'].map(months)\ndf_count = df_user_signup.loc[df_user_signup['year_of_signup']==2016,['UserName','month_of_signup']]","e9d23c78":"count = df_count['month_of_signup'].value_counts()\ndrawbarplot(x=count.values,y=count.index,xlabel='count',title='In Which month with Maximum User SignedUp',figsize=(10,10))","5636aba0":"country = {'united states':'united states of america','new york city':'new york'}\nreplc_list = ['us','ny','india','uk','california','chicago','los angeles','london']\nval_list=['united states of america','new york','india','united kingdom','california','chicago','los angeles','london']\n#us_list =['united states','us','usa','new york','chicago','los angeles','california','seattle','las vegas','new york city']\n#uk_list = ['uk','london','england']\n#india_list = ['mumbai','new delhi']","4a95ceac":"# Removing reduntant cities.\ndf_user_signup['UserLocation'] =df['UserLocation'].str.lower()\ndf_user_signup['UserLocation'].replace(country,inplace=True)\nfor i in range(0,len(val_list)):    \n    df_user_signup['UserLocation'].replace(to_replace=r'^.*'+replc_list[i]+'.*$',value=val_list[i],regex=True,inplace=True)","12bbaf9c":"count=df_user_signup['UserLocation'].value_counts()\ndf_count = pd.DataFrame()\ndf_count['UserLocation']=count.index\ndf_count['loc_count']=count.values\ndf_count=df_count.iloc[:25,:]","92eb6ab5":"drawbarplot(x=df_count.loc_count,y=df_count.UserLocation,xlabel='Location Count',title='Top 25 place with maximum Twitter Users',figsize=(16,16))","a26a7039":"df_place = df.loc[:,['tweet.place']]\ndf_place.dropna(axis=0,how='any',inplace=True)\ndf_place.reset_index(drop=True,inplace=True)\ndf_place.shape","dbf544ea":"df_place['tweet.place'] = df_place['tweet.place'].apply(json.loads)\ndf_place= pd.DataFrame(df_place['tweet.place'].tolist())","88acbb18":"df_count = pd.DataFrame()\ndf_count['country_code'] = df_place['country_code'].value_counts().index\ndf_count['tw_count'] = df_place['country_code'].value_counts().values\ndf_count = df_count.merge(df_place[['country_code','country']],how='inner',on='country_code',left_index=False,right_index=False)\ndf_count.drop_duplicates(inplace=True,subset='country_code')\ndf_count.reset_index(drop=True,inplace=True)","46e74f6b":"df_count.head()","7c93c82e":"drawbarplot(x=df_count.tw_count,y=df_count.country,figsize=(16,16),xlabel='tweet Count',title='Location with max tweets')","3fa4c5f1":"df.head()","10236d7b":"#Drop irrelevant columns\ncolsToDrop=['TweetID','TweetSource','TweetPostedTime', 'TweetPlaceID','TweetPlaceAttributes',\n       'TweetPlaceContainedWithin', 'UserID','UserDescription','UserLink', 'UserExpandedLink',\n        'UserListedCount','tweet.place']\ndf_final=df.drop(colsToDrop,axis=1)","f4a9083a":"df_final.shape","f6795358":"df_final.isnull().mean()","331f1cca":"##Dropping columns which are having more than 95%Null Values\nnullCols = ['TweetInReplyToStatusID','TweetInReplyToUserID','TweetInReplyToScreenName','TweetPlaceName',\n           'TweetPlaceFullName','TweetCountry','TweetPlaceBoundingBox']\ndf_final.drop(nullCols,axis=1,inplace=True)","95b6314b":"#Baseestimator class to extract fetures from tweetbody\n\nclass TextCounts(BaseEstimator, TransformerMixin):\n    \n    def count_regex(self, pattern, tweet):\n        return len(re.findall(pattern, tweet))\n    \n    def fit(self, X, y=None, **fit_params):\n        # fit method is used when specific operations need to be done on the train data, but not on the test data\n        return self\n    \n    def transform(self, X, **transform_params):\n        count_words = X.apply(lambda x: self.count_regex(r'\\w+', x)) \n        count_mentions = X.apply(lambda x: self.count_regex(r'@\\w+', x))\n        count_hashtags = X.apply(lambda x: self.count_regex(r'#\\w+', x))\n        #count_capital_words = X.apply(lambda x: self.count_regex(r'\\b[A-Z]{2,}\\b', x))\n        #count_excl_quest_marks = X.apply(lambda x: self.count_regex(r'!|\\?', x))\n        count_urls = X.apply(lambda x: self.count_regex(r'http.?:\/\/[^\\s]+[\\s]?', x))\n        # We will replace the emoji symbols with a description, which makes using a regex for counting easier\n        # Moreover, it will result in having more words in the tweet\n        count_emojis = X.apply(lambda x: emoji.demojize(x)).apply(lambda x: self.count_regex(r':[a-z_&]+:', x))\n        \n        df = pd.DataFrame({'count_words': count_words\n                           , 'count_mentions': count_mentions\n                           , 'count_hashtags': count_hashtags                           \n                           , 'count_urls': count_urls\n                           , 'count_emojis': count_emojis\n                          })\n        \n        return df","649c9637":"tc = TextCounts()\ndf_feature =  tc.fit_transform(df_final['TweetBody'])\ndf_feature.head(10)","71feb803":"df_feature.shape","aab5f850":"#Converting TweetRetweetFlag to integer\ndf_final['TweetRetweetFlag'] = df_final['TweetRetweetFlag'].map({True:1,False:0})","7bb700de":"#Extracting features from date\ndf_final['year_of_signup']=[d.year for d in df['UserSignupDate']]\ndf_final['month_of_signup']=[d.month for d in df['UserSignupDate']]\ndf_final.drop(columns='UserSignupDate',inplace=True,axis=1)","be9e398a":"#Cleaning Country attribute\ncountry = {'united states':'united states of america','new york city':'new york'}\nreplc_list = ['us','ny','india','uk','california','chicago','los angeles','london']\nval_list=['united states of america','new york','india','united kingdom','california','chicago','los angeles','london']\n\ndf_final['UserLocation'] =df_final['UserLocation'].str.lower()\ndf_final['UserLocation'].replace(country,inplace=True)\nfor i in range(0,len(val_list)):    \n    df_final['UserLocation'].replace(to_replace=r'^.*'+replc_list[i]+'.*$',value=val_list[i],regex=True,inplace=True)","53755106":"#df_final.head()\n#df_final.shape\ndf_feature.shape","562cbe21":"## Cleaning Tweet Body\nclass CleanText(BaseEstimator, TransformerMixin):\n    def remove_mentions(self, input_text):\n        return re.sub(r'@\\w+', '', input_text)\n    \n    def remove_urls(self, input_text):\n        return re.sub(r'http.?:\/\/[^\\s]+[\\s]?', '', input_text)\n    \n    def emoji_oneword(self, input_text):\n        # By compressing the underscore, the emoji is kept as one word\n        return input_text.replace('_','')\n    \n    def remove_punctuation(self, input_text):\n        # Make translation table\n        punct = string.punctuation\n        trantab = str.maketrans(punct, len(punct)*' ')  # Every punctuation symbol will be replaced by a space\n        return input_text.translate(trantab)\n\n    def remove_digits(self, input_text):\n        return re.sub('\\d+', '', input_text)\n    \n    def to_lower(self, input_text):\n        return input_text.lower()\n    \n    def remove_stopwords(self, input_text):\n        #stopwords_list = st_words\n        stopwords_list=STOPWORDS\n        # Some words which might indicate a certain sentiment are kept via a whitelist        \n        words = input_text.split() \n        clean_words = [word for word in words if (word not in stopwords_list) and len(word) > 1] \n        return \" \".join(clean_words) \n    \n    def stemming(self, input_text):\n        porter = PorterStemmer()\n        words = input_text.split() \n        stemmed_words = [porter.stem(word) for word in words]\n        return \" \".join(stemmed_words)\n    \n    def fit(self, X, y=None, **fit_params):\n        return self\n    \n    def transform(self, X, **transform_params):\n        clean_X = X.apply(self.remove_mentions).apply(self.remove_urls).apply(self.emoji_oneword).apply(self.remove_punctuation).apply(self.remove_digits).apply(self.to_lower).apply(self.remove_stopwords).apply(self.stemming)        \n        return clean_X","f13fc40b":"ct = CleanText()\ndf_final['TweetBody'] = ct.fit_transform(df_final.TweetBody)\n#Imputing '[no text]' value where there is no text\ndf_final.loc[df_final['TweetBody'] == '','TweetBody'] = '[no text]'","f4027f04":"df_final.drop(columns='TweetHashtags',axis=1,inplace=True)","38763203":"cv = CountVectorizer()\nbow = cv.fit_transform(df_final['TweetBody'])\nword_freq = dict(zip(cv.get_feature_names(), np.asarray(bow.sum(axis=0)).ravel()))\nword_counter = collections.Counter(word_freq)\nword_counter_df = pd.DataFrame(word_counter.most_common(20), columns = ['word', 'freq'])\n\nfig, ax = plt.subplots(figsize=(12, 10))\nsns.barplot(y=\"word\", x=\"freq\", data=word_counter_df, palette=\"PuBuGn_d\", ax=ax,orient='h')\nplt.show();","30a79577":"df_final = pd.concat([df_final,df_feature],ignore_index=False,axis=1,)","51f6f52a":"#Encoding Text columns into numeric\ndf_final['UserName'] = pd.factorize(df_final['UserName'])[0]\ndf_final['UserScreenName'] = pd.factorize(df_final['UserScreenName'])[0]\ndf_final['UserLocation'] = pd.factorize(df_final['UserLocation'])[0]","3713ddbe":"df_final.skew()","eb5688f7":"from sklearn.model_selection import train_test_split","872a6dd1":"X=df_final.drop(['TweetRetweetCount'],axis=1)\ny=df_final['TweetRetweetCount']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=22,shuffle=True)\nprint('X_train.shape %s, X_test.shape %s\\ny_train.shape %s, y_test.shape %s'%(X_train.shape,X_test.shape,y_train.shape,y_test.shape))","569031be":"#Tokenizing text with scikit learn countVectorizer\ncountvec= CountVectorizer()\nX_train_count = countvec.fit_transform(X_train.TweetBody)\nX_test_count = countvec.transform(X_test.TweetBody)\nprint('X_train_count.shape %s\\nX_test_count.shape %s'%(X_train_count.shape,X_test_count.shape))","96136ac9":"#Moving from Occurance to frequency using Term Frequency and \n#downscale weights for words that occur in most tweet as tthey may carry less info than those which \n#occur more frequently usinf Tf-idf\n\nfrom sklearn.feature_extraction.text import TfidfTransformer\ntfid = TfidfTransformer(use_idf=True)\nX_train_tfidf = tfid.fit_transform(X_train_count)\nX_test_tfidf = tfid.transform(X_test_count)\n\nprint('X_train_tfidf.shape %s\\nX_test_tfidf.shape %s'%(X_train_tfidf.shape,X_test_tfidf.shape))","9a1bf9d7":"### Combining all the features\nfrom scipy import sparse\nnumCols = X_train.columns\nnumCols =numCols.drop('TweetBody')\n\nX_train_num_feature = X_train[numCols].values\nX_test_num_feature = X_test[numCols].values\n\nX_traindata = sparse.hstack((X_train_tfidf,X_train_num_feature))\nX_testdata = sparse.hstack((X_test_tfidf,X_test_num_feature))\n\nprint('X_traindata.shape %s\\nX_testdata.shape %s'%(X_traindata.shape,X_testdata.shape))","7f6a8105":"from sklearn.decomposition import TruncatedSVD\n\nn_components = 1000\npca = TruncatedSVD(n_components)\nX_traindata = pca.fit_transform(X_traindata)\nX_testdata = pca.transform(X_testdata)","17b3648e":"type(X_traindata)\n### Normalise feature\n#from sklearn.preprocessing import StandardScaler","4df0f476":"#scaler = StandardScaler(with_mean=False)\n#scaled_X_train = scaler.fit_transform(X_traindata)\n#scaled_X_test = scaler.transform(X_testdata)","ff98553a":"lr = LinearRegression().fit(X_traindata,y_train)","8774fd38":"y_pred = lr.predict(X_testdata)","a9d7099d":"#y_pred_train = lr.predict(X_traindata)\n# r2_score score: 1 is perfect prediction\n#lr_score=r2_score(y_pred=y_pred_train,y_true=y_train)\n#print(\"Variance score (r2_score): %f\"%lr_score)\n#print('Model accuracy:%.2f '%(lr_score*100))\n#print(\"Root mean squared error of test:%f\"%sqrt(mean_squared_error(y_train,y_pred_train)))","8801778a":"#Applying LR on CV set and checking it's accuracy.\n# r2_score score: 1 is perfect prediction\nlr_score=r2_score(y_pred=y_pred,y_true=y_test)\nprint(\"Variance score (r2_score): %f\"%lr_score)\nprint('Model accuracy:%.2f '%(lr_score*100))\nprint(\"Root mean squared error of test:%f\"%sqrt(mean_squared_error(y_test,y_pred)))","9730b099":"plt.spy(X_test_tfidf)\nplt.show()","389b8e49":"## Let's do some EDA first!","93bda4b5":"### Tracking behaviour of most famous user","8882832c":"### Divide Data into test and train","1d14993d":"### Extracting at what time of the day most tweet takes place","a0e9489f":"### Twitter Leader with maximum followers","cfa22231":"#### Let's dig deeper in 2016!","edd60fab":"### Perform dimension reduction on data","28588eb2":" ### Analysing user signUp!","0f90ad13":"### Appying Linear Regresser","57d2d312":"#### Which Famous user tweets the most","16f9e2d6":"### Where are maximum Twitter Users located?","e115b97b":"### Most Mentions and tags","ef988e90":"### Most trending Tweets","fb77b07e":"### Analysis of Tweet posted time!","855ee179":"- #### United States is the country with maximum Tweets\n- #### While Austria,Bulgaria,Bahrain with lowest Tweets","9d98a990":"### Vectorizing text to number ","cc272f7d":"### Most Friendly having maximum friends","a4c4347d":"#### How many Tweets they do?","595ce886":"## Building Data for model","6cc5580d":"-  #### With the mark of new year 2016, many user took resolution to go social on twitter ;p","1f9c15e2":"#### At What year maximum Users Joined Twitter","446c5ec2":"### Exploring tweet.place attribute","39a3b2ca":"### Most active Twitter users in given timeperiod","a499b570":"### Users with maximum tweets","f500706d":" #### Who is the Veteran here?"}}