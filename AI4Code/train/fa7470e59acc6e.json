{"cell_type":{"5a08e1b8":"code","4baf7f54":"code","aa26974f":"code","bb17faa7":"code","52db802c":"code","bb52874c":"code","9ac4fc85":"code","337f586f":"code","84e26ff3":"code","012d1678":"code","9dfd5c86":"code","ea26278d":"code","4a6c39e4":"code","de67e7b5":"code","6b6c18b2":"code","deb6d470":"code","0a2ea203":"code","64a40402":"code","3a322f16":"code","200c5641":"code","aa8d999f":"code","8a130936":"code","49f62a5d":"code","1fb791de":"code","75999bb1":"code","83560c62":"code","f8cb9ef2":"code","e5a2626d":"code","c6e7c746":"code","9665443b":"code","7f06a700":"code","88ffee94":"code","40e519d1":"code","431d40ac":"code","21d1fd37":"code","0e00c86f":"code","db89f8d1":"code","6ba0dd76":"code","62f82624":"code","3b50f76a":"code","1f5be4ee":"code","40c2a73b":"code","055f3937":"code","e69e1e4c":"code","cdd0dafa":"code","5022b643":"code","15740507":"code","31b549d4":"code","32d881b1":"code","b0adbb7e":"code","c8fd754f":"code","3315fa6f":"code","b062fe3e":"code","6c7359a3":"code","f9667f0c":"code","34c1d3ad":"code","e6cf185a":"code","8f0c9995":"code","48e0a2fb":"code","d7a01bb3":"code","b7137a55":"code","aa337e07":"code","914c9bdc":"code","cbc64bc6":"code","23752ade":"markdown","18b35a5e":"markdown","bf75a5d3":"markdown","d28b1ec4":"markdown","36e42744":"markdown","bea56c9e":"markdown","a64e3d26":"markdown","44ac2488":"markdown","e81e9515":"markdown","1f39d812":"markdown","bad3f1c6":"markdown","9693d4a8":"markdown","a5023488":"markdown","e07bcb5d":"markdown","b2a226bf":"markdown","a041fb15":"markdown","7cd1dba1":"markdown","a4a6d0a9":"markdown","2ea0e0e3":"markdown","7a061154":"markdown","364ac6ea":"markdown","8b73d2b1":"markdown","89ceaf18":"markdown","e4b81da3":"markdown","5ec6cddf":"markdown","a50aeae5":"markdown","8e63ffca":"markdown","81a92060":"markdown","a5a9005d":"markdown","9edf60e6":"markdown","8e5322c7":"markdown","090ea2f8":"markdown","b9c20dbb":"markdown","8b0eaedf":"markdown","c30cb2f0":"markdown","7896a91b":"markdown","b3c6d817":"markdown","7f7f9132":"markdown","3ffb795b":"markdown","b73ac199":"markdown","3b7d554e":"markdown","bd398298":"markdown","38306009":"markdown","d1f35bc9":"markdown","566d212a":"markdown","2bcc2af1":"markdown","34516f82":"markdown","96dfcde2":"markdown","76384ec1":"markdown","1cf27e6b":"markdown","682048fa":"markdown","57756fa2":"markdown","4993f4d9":"markdown","d89a734a":"markdown","cc3df816":"markdown","9cb81d6c":"markdown","f19f1d7b":"markdown","990d4492":"markdown","9012a5b9":"markdown"},"source":{"5a08e1b8":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn import preprocessing\nfrom sklearn.model_selection import GridSearchCV,StratifiedKFold,cross_val_score\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier,IsolationForest, RandomForestClassifier, VotingClassifier","4baf7f54":"sns.set(rc={'figure.figsize':(8,6)})\nsns.set_style(\"darkgrid\")","aa26974f":"train = pd.read_csv(\"..\/input\/titanic\/train.csv\")\ntest = pd.read_csv(\"..\/input\/titanic\/test.csv\")","bb17faa7":"test_ids = test['PassengerId']","52db802c":"train","bb52874c":"test","9ac4fc85":"rng = np.random.RandomState(42)\n\nclf = IsolationForest(max_samples=100, random_state=rng,contamination = 0.05)\nclf.fit(train[[\"Age\",\"SibSp\",\"Parch\",\"Fare\"]].dropna())\nprediction = clf.predict(train[[\"Age\",\"SibSp\",\"Parch\",\"Fare\"]].dropna())\n\ndf_outliers = train[[\"Age\",\"SibSp\",\"Parch\",\"Fare\"]].dropna()\ndf_outliers['Outlier'] = prediction\nindex_list = df_outliers[df_outliers['Outlier'] == -1].index\n\ntrain.drop(index_list, inplace=True)","337f586f":"df =  pd.concat(objs=[train, test], axis=0).reset_index(drop=True)","84e26ff3":"df","012d1678":"df.isnull().sum()","9dfd5c86":"train.info()\ntrain.isnull().sum()","ea26278d":"train.dtypes","4a6c39e4":"train.head()","de67e7b5":"## MAIN STATISTICS \n\ntrain.describe()","6b6c18b2":"g = sns.heatmap(train[[\"Survived\",\"SibSp\",\"Parch\",\"Age\",\"Fare\"]].corr(),annot=True, fmt = \".2f\", cmap = \"magma\")\n","deb6d470":"g  = sns.factorplot(x=\"Parch\",y=\"Survived\",data=train,kind=\"bar\",height = 6 ,palette = \"magma\")\ng.despine(left=True)\ng = g.set_ylabels(\"Survival Probability\")","0a2ea203":"ns = sns.distplot(train[train['Survived'] == 0].Age)\nns.set_title(\"Not Survived Age Density\")\nns.set_ylabel(\"Density\")\n\nplt.show()\n\ns = sns.distplot(train[train['Survived'] == 1].Age)\ns.set_title(\"Survived Age Density\")\ns.set_ylabel(\"Density\")\n\nplt.show()","64a40402":"ns = sns.kdeplot(train[train['Survived'] == 0].Age,color=\"red\",label=\"Not Survived\",shade=True)\nns.set_title(\"Not Survived Age Density\")\nns.set_ylabel(\"Density\")\n\ns = sns.kdeplot(train[train['Survived'] == 1].Age,color=\"blue\",label=\"Survived\",shade=True)\ns.set_title(\"Survived Age Density\")\ns.set_ylabel(\"Density\")\ns.set_xlabel(\"Age\")\n\nplt.show()","3a322f16":"g = sns.factorplot(x=\"SibSp\",y=\"Survived\",data=train,kind=\"bar\", height = 6 , \npalette = \"magma\")\ng.despine(left=True)\ng = g.set_ylabels(\"survival probability\")","200c5641":"df[\"Fare\"].isnull().sum()","aa8d999f":"df[\"Fare\"].fillna(df[\"Fare\"].median(),inplace=True)","8a130936":"ns = sns.distplot(df['Fare'])\nns.set_title(\"Fare Density\")\nns.set_ylabel(\"Density\")\n\nplt.show()","49f62a5d":"df[\"Fare\"].skew()","1fb791de":"df[\"Fare\"] = np.log(df[\"Fare\"]).replace(-np.inf, 0)","75999bb1":"ns = sns.distplot(df['Fare'])\nns.set_title(\"Fare Density\")\nns.set_ylabel(\"Density\")\nprint(\"Skew:\",df[\"Fare\"].skew())\nplt.show()","83560c62":"g = sns.barplot(x=\"Sex\",y=\"Survived\",data=train)\ng.set_ylabel(\"Survival Probability\")\ng.set_title(\"Sex Survival Prob.\")","f8cb9ef2":"g = sns.factorplot(x=\"Pclass\",y=\"Survived\",data=train,kind=\"bar\", height = 6 , palette = \"magma\")\nplt.title(\"Pclass Survival Prob.\")\nplt.show()","e5a2626d":"df[\"Embarked\"].isnull().sum()","c6e7c746":"df[\"Embarked\"] = df[\"Embarked\"].fillna(\"S\") # Most frequent value","9665443b":"g = sns.factorplot(x=\"Embarked\", y=\"Survived\",  data=train, size=6, kind=\"bar\", palette=\"magma\")","7f06a700":"g = sns.factorplot(\"Pclass\", col=\"Embarked\",  data=train,\n                   size=6, kind=\"count\", palette=\"magma\")\ng.despine(left=True)\ng = g.set_ylabels(\"Count\")","88ffee94":"df.isna().sum()","40e519d1":"# Transorm to numerical data for the corr matrix\n\ndf[\"Sex\"] = df[\"Sex\"].map({\"male\": 0, \"female\":1})","431d40ac":"g = sns.heatmap(df[[\"Age\",\"Sex\",\"SibSp\",\"Parch\",\"Pclass\"]].corr(),cmap=\"magma\",annot=True)","21d1fd37":"g = sns.catplot(y=\"Age\",x=\"Pclass\",data=df,kind=\"box\")\n","0e00c86f":"ages=[]\nfor i in range(1,4):\n    ages.append(((df[df['Pclass'] == i].Age).median()))\nfor index, row in df.iterrows():\n    if(np.isnan(row['Age'])):\n        df.loc[index,'Age'] = ages[row['Pclass'] - 1]","db89f8d1":"df.isna().sum()","6ba0dd76":"df[\"Cabin\"].describe()","62f82624":"df[\"Cabin\"].isnull().sum()","3b50f76a":"for index,row in df.iterrows():\n    if \"nan\" != str(df.loc[index,\"Cabin\"]):\n        df.loc[index,\"Cabin\"] = df.loc[index,\"Cabin\"][0]\n    else:\n        df.loc[index,\"Cabin\"] = \"X\"","1f5be4ee":"g = sns.countplot(df[\"Cabin\"],order=['A','B','C','D','E','F','G','T','X'])","40c2a73b":"df = pd.get_dummies(df, columns = [\"Cabin\"],prefix=\"Cabin\")","055f3937":"df[\"Name\"].head()","e69e1e4c":"for index, row in df.iterrows():\n    df.loc[index,'Title'] = row['Name'].split(\".\")[0].split(\", \")[1]\n","cdd0dafa":"g = sns.countplot(x=\"Title\",data=df)\ng = plt.setp(g.get_xticklabels(), rotation=\"vertical\") ","5022b643":"frequent_titles = ['Mr','Mrs','Miss','Master']\nfor index, row in df.iterrows():\n    if df.loc[index,'Title'] not in frequent_titles:\n        df.loc[index,'Title'] = \"Rare\"","15740507":"g = sns.countplot(x=\"Title\",data=df)\ng = plt.setp(g.get_xticklabels(), rotation=\"vertical\") ","31b549d4":"df.drop(labels = [\"Name\"], axis = 1, inplace = True)","32d881b1":"df[\"Ticket\"].head()","b0adbb7e":"for index,row in df.iterrows():\n    if df.loc[index,\"Ticket\"].isdigit():\n        df.loc[index,\"Ticket\"] = \"X\"\n    else:\n        df.loc[index,\"Ticket\"] = df.loc[index,\"Ticket\"].replace(\".\",\"\").replace(\"\/\",\"\").strip().split(' ')[0]\ndf = pd.get_dummies(df, columns = [\"Ticket\"], prefix=\"T\")","c8fd754f":"df","3315fa6f":"df = pd.get_dummies(df, columns = [\"Pclass\"],prefix=\"Pc\")\ndf = pd.get_dummies(df, columns = [\"Embarked\"],prefix=\"Embarked\")\ndf = pd.get_dummies(df, columns = [\"Title\"],prefix=\"Title\")\n\ndf.drop(labels = [\"PassengerId\"], axis = 1, inplace = True)","b062fe3e":"df.head()","6c7359a3":"train = df[:train.shape[0]]\ntest = df[train.shape[0]:]\ntest.drop(labels=[\"Survived\"],axis = 1,inplace=True)\n\nY_train = train[\"Survived\"]\nX_train = train.drop(labels = [\"Survived\"],axis = 1)","f9667f0c":"random_state = 2\n\nclf = ['SVC','RandomForest','GradientBoosting','KNeighbors','LogisticRegression']\nclassifiers = []\n\nclassifiers.append(SVC(random_state=random_state))\nclassifiers.append(RandomForestClassifier(random_state=random_state))\nclassifiers.append(GradientBoostingClassifier(random_state=random_state))\nclassifiers.append(KNeighborsClassifier())\nclassifiers.append(LogisticRegression(random_state = random_state))\n","34c1d3ad":"clf_scores = []\nfor i in range(len(classifiers)):\n    scores = cross_val_score(classifiers[i], X_train, y = Y_train, scoring = \"accuracy\", n_jobs=-1)\n    clf_scores.append([clf[i],scores.mean(),scores.std()])","e6cf185a":"df_scores = pd.DataFrame(clf_scores)\ndf_scores.columns = ['Model','Acc_Mean','Acc_Std']\ndf_scores","8f0c9995":"SVMC = SVC(probability=True)\nsvc_param_grid = {'kernel': ['rbf'], \n                  'gamma': [ 0.001, 0.01, 0.1, 1],\n                  'C': [1, 10, 50, 100,200,300, 1000]}\n\ngsSVMC = GridSearchCV(SVMC,param_grid = svc_param_grid,scoring=\"accuracy\", n_jobs= -1, verbose = 1)\n\ngsSVMC.fit(X_train,Y_train)\n\nSVMC_best = gsSVMC.best_estimator_\n\n# Best score\n\ngsSVMC.best_score_","48e0a2fb":"GBC = GradientBoostingClassifier()\ngb_param_grid = {'loss' : [\"deviance\"],\n              'n_estimators' : [100,200,300],\n              'learning_rate': [0.1, 0.05, 0.01],\n              'max_depth': [4, 8],\n              'min_samples_leaf': [100,150],\n              'max_features': [0.3, 0.1] \n              }\n\ngsGBC = GridSearchCV(GBC,param_grid = gb_param_grid, scoring=\"accuracy\", n_jobs= -1, verbose = 1)\n\ngsGBC.fit(X_train,Y_train)\n\nGBC_best = gsGBC.best_estimator_\n\n\nprint(\"Best Score: \",gsGBC.best_score_)","d7a01bb3":"RFC = RandomForestClassifier()\n\nrf_param_grid = {\"max_depth\": [None],\n              \"max_features\": [1, 3, 10],\n              \"min_samples_split\": [2, 3, 10],\n              \"min_samples_leaf\": [1, 3, 10],\n              \"bootstrap\": [False],\n              \"n_estimators\" :[100,300],\n              \"criterion\": [\"gini\"]}\n\ngsRFC = GridSearchCV(RFC,param_grid = rf_param_grid,scoring=\"accuracy\", n_jobs= -1, verbose = 1)\n\ngsRFC.fit(X_train,Y_train)\n\nRFC_best = gsRFC.best_estimator_\n\nprint(\"Best Score: \",gsRFC.best_score_)","b7137a55":"votingC = VotingClassifier(estimators=[('RandomForest', RFC_best), ('SVC', SVMC_best), ('GradientBoosting',GBC_best)], voting='soft', n_jobs=-1)\n\nvotingC = votingC.fit(X_train, Y_train)","aa337e07":"predictions = pd.Series(votingC.predict(test),name=\"Survived\")\nresults = pd.concat([test_ids,predictions],axis=1)\nresults = results.astype(int)","914c9bdc":"results.head()","cbc64bc6":"results.to_csv(\"submission.csv\",index=False)","23752ade":"The strategy here is to take just the prefix of the cabin, as its enough to indicate in which zone of the Titanic was more probable to find a passenger. We will also set to 'X' the missing values representing passengers that didn't had cabin.","18b35a5e":"I will explore 5 models:\n\n    1. SVC\n    2. Random Forest\n    3. Gradient Boosting\n    4. KNeightbors\n    5. Logistic Regression\n \n Then, I will combine 3 to obain our predictions. \n ","bf75a5d3":"We can see that passengers having less siblings\/spouses had more chance to survive. Passengers travelling with 3 or 4 more had less chance of survival than passengers travelling alone (SibSP = 0) or with one or two more. ","d28b1ec4":"To manage outliers I decided to use the [Isolation Forest](https:\/\/en.wikipedia.org\/wiki\/Isolation_forest) algorithm. An Isolation Tree is basically based on the fact that, if you have your data ploted, and you start drawing random lines, it will be easier to isolate an outlier, i.e. less lines will be needed. However, it's easy to deduce that an Isolation Forest will be a set of Isolation Trees. At the end, this algorithm will return us the (contamination * 100%) of the most probable outlier points. We will drop them.","36e42744":"### 3.4 SibSp","bea56c9e":"Here we see a very interesting feature analyisis. We appreciate that clearly, higher class passengers had much more probability of survival. In particular, first class passengers had more than two times the probability of a third class passenger.","a64e3d26":"Let's group train and test datasets in order to apply same corrections and analyisis on both.","44ac2488":"We just have 1 missing value so we can fill it with the median.","e81e9515":"Here we note that, although we see missing values in three features, we just care about Age and Cabin missings. This is because, as we have seen, Survived NaN's correspond to the test dataset.","1f39d812":"### 4.1 Age","bad3f1c6":"We serch for features that may be correlated with Survival, Fare and Parch are the most correlated features. However, we should explore all features to see whetere they ar significative or not to determine Survival.","9693d4a8":"### 2.2 Handle outliers","a5023488":"We see that there is region where it was more probable to survive. This corresponds to the youngest passengers. It was more porbable to survive as a child, probably because of the well-known phrase \"Women and children first\".We also see that there is a small peak for very old passengers that survived. Here we see that, even Age was not correlated with Survival, there are some regions where it was more probable to survive.","e07bcb5d":"### 3.5 Fare","b2a226bf":"## 6. Predict","a041fb15":"## 1. Introduction","7cd1dba1":"## 2. Read Data & Check for Outliers","a4a6d0a9":"First of all, we need to split our dataset into a training and a testing part. Obiously, we will use the rows corresponding to the train df to train our model and the test for submiting our predictions. This can be confusing. **We are not validating our results with the test dataset, we are training and cross-validating our model with the train dataset in order to predict the test passengers survival** ","2ea0e0e3":"### 5.2 Ticket","7a061154":"We see that there is a clear difference between median Ages based on the travelling class. We will use it to compute the missing values.","364ac6ea":"To address this problem I will use the assumption that, not every passenger had a cabin based on [this](https:\/\/titanic.fandom.com\/wiki\/First_Class_Staterooms#:~:text=The%20Titanic%20had%20a%20total,used%20as%202nd%20class%20staterooms.) . Then, if we have more than 1200 passengers and less than 400 cabins, it's not crazy to think that the 997 missing values where people without Cabin (taking into account that we have deleted some outliers).","8b73d2b1":"We have our data splited into two diferent files. Train file contains 418 rows of passengers of which we know whether they survived or not. Test file contains 891 rows of passengers of which we don't know whether they survived or not. We will now save our test passengers ids as we will use them later.","89ceaf18":"Age distribution seems to be Gaussian or at least tailed. The important fact here is that, the distribution is not the same for those who survived and for those who not.  Let's try to overlap them and explore the diferences.","e4b81da3":"We see a considerable improvement of accuracy in every model. I will combine those three using a Voting Classifier with soft mode. This is, I will make predictions with the thre models and obtain as a result the more calibrated combination.","5ec6cddf":"This distribution seems to be very skewed so, as you might now, it could affect to our classifing predictions (See this [link](https:\/\/stats.stackexchange.com\/questions\/267078\/why-is-skewed-data-not-preferred-for-modelling)). We will apply a natrual logarithm to our data in order to modify skewness. Here it's important to check for zero divisions as it could crash later.","a50aeae5":"Now we have a more usefull distribution after the transformation.","8e63ffca":"## 5. Feature engineering","81a92060":"### 5.1 Name","a5a9005d":"We don't care about the name of the passenger as it tells nothing about survival. However we want to preserve the tilte of every passenger as it indicates relevant things such as sex, age and social status.","9edf60e6":"## 4. Fill missings","8e5322c7":"### 5.3 Final Preprocessing","090ea2f8":"We see that people having a small family also had more chance to survive. The ones that were travelling alone ( Parch = 0 ) or with large families had less survival probability. We can see that, for the case of Parch = 3, we have a very large standard deviation. This could mean that another feature is taking place here.","b9c20dbb":"## 3. Feature Analysis","8b0eaedf":"I have decided to use SVC, RF and GB as I've tried many combinations and those are the better ones. However, I will try to tune the hyperparameters in order to obtain better accuracy. In other words, I will search for the best combination of parameters for each model that increases accuracy. I will use [GridSerchCV](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.model_selection.GridSearchCV.html) from sklearn.","c30cb2f0":"**V\u00edctor Aguado, Mathematical Engineering in Data Science Student**\n\n\n02\/01\/2021","7896a91b":"# Titanic EDA + Ensemble  Modeling (as a Data Science Student)","b3c6d817":"This is my first notebook on Kaggle, my goal is to become more active in this plattofrm as I really think it's a very good way to learn and improve. I'am not trying to get the best final score, I 'am trying to maximize the knowledge I get in every notebook. \n\n\nPlease feel free to comment whatever you want and share if it was useful !\n\nThank you for reading.\n\n\n---------------------------------------------------------\n\nV\u00edctor Aguado,\nMathematical Engineering in Data Science Student.\n\n\n2021","7f7f9132":"I use cross validation with Kfolds k=5. This is, I compute accuracy 5 times for every model with a random resample of the train dataset. Then we keep, for every classifier, the average accuracy and the standard deviation.","3ffb795b":"This is what we obtain after preprocessing and analyzing the data. The next step is to get numerical variables from Pclass, Embarked and Title as we can't process words in our algorithms. Then, we will drop the Passenger IDs as they are not relevant for our model. Having that done, it's time to build our ensemble model. ","b73ac199":" ### 2.4 Check missings & nulls ","3b7d554e":"Finally we generate the submission file.","bd398298":"Again, \"Women and children first\". We see that womens had arround a **0.75** probability of surviving in the Titanic while men had less than **0.2**.","38306009":"### 3.3 Age","d1f35bc9":"Here we have 2 missing values, I've decided to fill them with the most frequent value, S. We see that passengers who embarked in Cherbourg (C) had more chance to survive. As I'm not familiarized with this places I've been searching about the social status of people from this regions. I've found some some signifiicative differences so let's check whether Pclass is playing this role here.","566d212a":"### 3.6 Sex","2bcc2af1":"### 3.8 Embarked","34516f82":"### 2.3 Concat both df","96dfcde2":"We see that we have **263** null values for Age, **1000** null values for Cabin, **2** for Embarked and **385** for Survived. There is a great ammount of nulls for Age and Cabin that we will manage later, for the case of Survived, those NaN values are part of the test dataset.","76384ec1":"We see that we have 4 common titles and many others that are less frequent. At the end, we don't want our model to overfit to our data inserting 18 columns corresponding to every title we have here, also, we would like the make predictions for titles that may not be considered here (expanding our model). For that reason, we will keep the mos frequent ones and set to 'Rare' the others.","1cf27e6b":"We see that Pclass is very correlated with Pclass in a negative way. This is, older passengers are more probable to be on a higher class. Although we had more correlations (SibSp, Parch), I will impute the value of Age based on the Pclass feature. In other words, similar passengers will have the same age in terms of PClass. This, of course, could be more accurated using all the features, however, I'am no taking this detailism level on this notebook.","682048fa":"This notebook is my first upload on Kaggle. I will firstly explore the data, extract some key ideas or insights and, at the end, try to make decent predictions using Ensemble Modeling. I've been reeding notebooks from other Kagglers and I've seen that, without cheating, most scored notebooks using just ML techinques are arround the 0.75 - 0.85 final score. I will do my best to get into that threshold. For doing this notebook I will use as reference a few notbooks and try to get the best of them combined with my knowledge. Let's start !","57756fa2":"### 3.7 Pclass","4993f4d9":"Here we could explore whether paassengers having cabin belong to higher class or not.","d89a734a":"### 3.1 Correlations","cc3df816":"We see that as I thought it exists a clear difference between social status of people embarking in different places. For the case of Southampton (S), the less probable survival group, we see that it was formed mainly by third class passengers. Analogously we can deduce the behavior of the other embarked places.","9cb81d6c":"### 2.1 Read the Data","f19f1d7b":"### 4.2 Cabin","990d4492":"We see that ticket is formed by a prefix and a number. We have many different tickets but common prefixes. My idea is that same prefixes match to similar or same cabins. Then similar tickets may match to similar passengers in terms of cabin, fare or famili size. \n\nHere we don't want to keep the number so we will extract the prefix.","9012a5b9":"### 3.2 Parch"}}