{"cell_type":{"04080edd":"code","eda98567":"code","d0e2609c":"code","587803e4":"code","9acb0368":"code","9f20296c":"code","7c4ea614":"code","dcca9c5f":"code","81a262e0":"code","533275f2":"code","a84b2810":"code","cbfc5303":"code","672fb675":"code","502d629a":"code","a820a704":"code","14daf8f8":"code","04fc2a0c":"code","e94128a6":"code","68e0486c":"code","6f863093":"code","5e1e07fa":"code","6f636fa0":"code","9f9a1f23":"code","97f208d7":"code","d8c5706e":"code","b285cccc":"code","cbea28b0":"code","54c4582a":"code","7e314b3f":"markdown","bf2169d9":"markdown","c8d1f914":"markdown","8a167448":"markdown","867e2a58":"markdown","7c45d0ab":"markdown","63a096c4":"markdown","874be589":"markdown","b2318a49":"markdown","c7361df0":"markdown","1cfed9ca":"markdown","a6ff3a1c":"markdown","59102251":"markdown","8c92d38d":"markdown","c42d5e01":"markdown","17bdcd72":"markdown","2cce8ebf":"markdown","d79410cc":"markdown","7d0bfe8e":"markdown","880fa4cb":"markdown","d7dc8058":"markdown","bd524302":"markdown","cfaaad4c":"markdown","bb56f706":"markdown","398cb5ee":"markdown","fdd67002":"markdown","bd1e0036":"markdown","9df9e4f1":"markdown","e59e3410":"markdown","329d06d0":"markdown","9419fbaa":"markdown"},"source":{"04080edd":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport h5py\nimport scipy\nfrom PIL import Image","eda98567":"df = pd.read_csv(\"..\/input\/weather-dataset-rattle-package\/weatherAUS.csv\")","d0e2609c":"df.head()","587803e4":"df.info()","9acb0368":"df.drop(['RISK_MM'], axis=1, inplace=True)","9f20296c":"df.drop(['Date'], axis=1, inplace=True)","7c4ea614":"categorical = [var for var in df.columns if df[var].dtype=='O']\nprint(\"The categorical features are : \",categorical)","dcca9c5f":"df = pd.concat(      [df,\n                     pd.get_dummies(df.RainToday, drop_first=True, dummy_na=True),\n                     pd.get_dummies(df.Location), \n                     pd.get_dummies(df.WindGustDir),\n                     pd.get_dummies(df.WindDir9am),\n                     pd.get_dummies(df.WindDir3pm)], axis=1)","81a262e0":"df.head()","533275f2":"df.drop(['Location'], axis=1, inplace=True)\ndf.drop(['WindGustDir'], axis=1, inplace=True)\ndf.drop(['WindDir9am'], axis=1, inplace=True)\ndf.drop(['WindDir3pm'], axis=1, inplace=True)\ndf.drop(['RainToday'], axis=1, inplace=True)","a84b2810":"df.head()","cbfc5303":"y = df[\"RainTomorrow\"]\ndf.drop(['RainTomorrow'], axis=1, inplace=True)\n\ny=pd.Series(map(lambda x: dict(Yes=1, No=0)[x],\n              y.values.tolist()))","672fb675":"y.isnull().any()","502d629a":"df.isna().any()","a820a704":"df=df.apply(lambda x: x.fillna(x.mean()),axis=0)","14daf8f8":"from sklearn.model_selection import train_test_split\n\ntrain_set_x,  test_set_x, train_set_y,test_set_y = train_test_split(df, y, test_size = 0.2, random_state = 0)","04fc2a0c":"from sklearn.preprocessing import StandardScaler  \nscaler = StandardScaler()  \nscaler.fit(train_set_x)  \ntrain_set_x = scaler.transform(train_set_x)  \ntest_set_x = scaler.transform(test_set_x)","e94128a6":"print(\"train_set_x\",train_set_x.shape)\nprint(\"train_set_y\",train_set_y.shape)\nprint(\"test_set_x\",test_set_x.shape)\nprint(\"test_set_y\",test_set_y.shape)","68e0486c":"train_set_y=train_set_y.values.reshape(113754,1)\ntest_set_y=test_set_y.values.reshape(28439,1)\nprint(\"train_set_x\",train_set_x.shape)\nprint(\"train_set_y\",train_set_y.shape)\nprint(\"test_set_x\",test_set_x.shape)\nprint(\"test_set_y\",test_set_y.shape)","6f863093":"def sigmoid(z):\n    s = 1\/(1+np.exp(-z))\n    return s","5e1e07fa":"def initialize(dim):\n    w = np.zeros((dim,1))\n    b = 0  \n    return w, b","6f636fa0":"def propagate(w, b, X, Y):\n    m = X.shape[1]\n    # FORWARD \n    A = sigmoid(np.dot(w.T,X)+b)   \n    cost = (-1\/m)*np.sum(np.multiply(Y,np.log(A))+np.multiply((1-Y),np.log(1-A)))                               \n    # BACKWARD \n    dz = A-Y\n    dw = (1\/m)*np.dot(X,dz.T)\n    db = (1\/m)*np.sum(dz)\n    \n    grads = {\"dw\": dw,\n             \"db\": db}\n\n    return grads, cost","9f9a1f23":"def optimize(w, b, X, Y,print_cost, num_iterations, learning_rate):\n    costs = []\n    for i in range(num_iterations):\n        grads, cost = propagate(w, b, X, Y)\n        \n        dw = grads[\"dw\"]\n        db = grads[\"db\"]\n\n        w = w-learning_rate*dw\n        b = b-learning_rate*db\n\n        if i % 100 == 0:\n            costs.append(cost)\n        \n        if print_cost and i % 100 == 0:\n            print (\"Cost after iteration %i: %f\" %(i, cost))\n    \n    params = {\"w\": w,\n              \"b\": b}\n    grads = {\"dw\": dw,\n             \"db\": db}\n    \n    return params, grads, costs","97f208d7":"def predict(w, b, X):\n    m = X.shape[1]\n    Y_prediction = np.zeros((1,m))\n    w = w.reshape(X.shape[0], 1)\n    \n    A = sigmoid(np.dot(w.T,X)+b)\n\n    for i in range(A.shape[1]):\n        if A[0,i]>0.5:\n            Y_prediction[0,i]=1\n        else:\n            Y_prediction[0,i]=0    \n    return Y_prediction","d8c5706e":"def model(X_train, Y_train, X_test, Y_test,print_cost, num_iterations = 2000, learning_rate = 0.5 ):\n    w, b = initialize(X_train.shape[0])\n\n    parameters, grads, costs = optimize(w, b, X_train, Y_train,print_cost, num_iterations, learning_rate)\n\n    w = parameters[\"w\"]\n    b = parameters[\"b\"]\n\n    Y_prediction_test = predict(w, b, X_test)\n    Y_prediction_train = predict(w, b, X_train)\n\n    print(\"train accuracy: {} %\".format(100 - np.mean(np.abs(Y_prediction_train - Y_train)) * 100))\n    print(\"test accuracy: {} %\".format(100 - np.mean(np.abs(Y_prediction_test - Y_test)) * 100))\n\n    \n    d = {\"costs\": costs,\n         \"Y_prediction_test\": Y_prediction_test, \n         \"Y_prediction_train\" : Y_prediction_train, \n         \"w\" : w, \n         \"b\" : b,\n         \"learning_rate\" : learning_rate,\n         \"num_iterations\": num_iterations}\n    \n    return d","b285cccc":"d = model(train_set_x.T, train_set_y.T, test_set_x.T, test_set_y.T,print_cost = True, num_iterations = 3000, learning_rate = 0.15)","cbea28b0":"learning_rates = [0.1, 0.01, 0.001, 0.0001]\nmodels = {}\nfor i in learning_rates:\n    print (\"learning rate is: \" + str(i))\n    models[str(i)] = model(train_set_x.T, train_set_y.T, test_set_x.T, test_set_y.T,print_cost = False, num_iterations = 1500, learning_rate = i)\n    print ('\\n' + \"-------------------------------------------------------\" + '\\n')\n\nfor i in learning_rates:\n    plt.plot(np.squeeze(models[str(i)][\"costs\"]), label= str(models[str(i)][\"learning_rate\"]))\n\nplt.ylabel('cost')\nplt.xlabel('iterations (hundreds)')\n\nlegend = plt.legend(loc='upper center', shadow=True)\nframe = legend.get_frame()\nframe.set_facecolor('0.90')\nplt.show()","54c4582a":"from sklearn import linear_model #import the model library\nlogreg = linear_model.LogisticRegression(random_state = 0,max_iter= 3000,solver='liblinear') # sitting model parameters\nfitting = logreg.fit(train_set_x, train_set_y)\nprint(\"test accuracy: {} \".format(fitting.score(test_set_x, test_set_y))) # printing the results of fitting the model over the testing set\nprint(\"train accuracy: {} \".format(fitting.score(train_set_x, train_set_y))) # printing the results of fitting the model over the training set","7e314b3f":"Our results and scikit learn results are almost the same\n","bf2169d9":"fill NaN with the mean","c8d1f914":"define the sigmoid function","8a167448":"view data shape","867e2a58":"remove the old categorical data columns","7c45d0ab":"show data frame","63a096c4":"split data into training and testing","874be589":"**Import all needed libraries**","b2318a49":"reshape the labels to elemenate rank one arraies","c7361df0":"Check the results with multible Learning Rate","1cfed9ca":"scalling the data","a6ff3a1c":"show the target data","59102251":"Date represent sequence data and as LR is not that good in capturing sequence model I will remove the date info","8c92d38d":"view data information","c42d5e01":"Initialize weights and bias with zeros","17bdcd72":"**My own LR implementation start here:**\n\n","2cce8ebf":"show all categorical data that need to be handeled","d79410cc":"Read the csv features file","7d0bfe8e":"use the final weights and bias to predict the results for new unseen testing data","880fa4cb":"view data","d7dc8058":"remove the RISK_MM as mentioned on data scource","bd524302":"use RainTomorrow as target (labels) data and remove it from data frame\nconvert the Yes\/No results to 0\/1","cfaaad4c":"big thanks for coursera and deeplearning.ai for the knowledge ","bb56f706":"optimizing the weights and bias using the gradieent at each iteration","398cb5ee":"In this kernal I will solve the \"*Will it rain tomorrow*\" using Logistic Regression.\n\nI will use my own LR implementation vs Scikit learn one\n\n**FOR ANY QUESTIONS OR NEEDED EXPLANATIONS JUST COMMENT AND I WILL REPLY ASAP**\n\n","fdd67002":"use get dummies function to fo the hot encoding to convert categorical data to numirical one in a way that is not give extra weight for any above others ","bd1e0036":"the main model Implementation","9df9e4f1":"scikit learn LR implementation","e59e3410":"Calling the model","329d06d0":"show data frame","9419fbaa":"start the propagate, forward and backward to compute the activations\/Cost and gradients respictivly"}}