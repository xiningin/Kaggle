{"cell_type":{"be171e18":"code","5a58c399":"code","b773c360":"code","0cafe21c":"code","2d286b7d":"code","a1476430":"code","38f1b764":"code","82f80c92":"code","96b89369":"code","ba69c262":"code","499abc1a":"code","3debf1cf":"code","1518100b":"code","f5ca2c6c":"code","77c40846":"code","171c128e":"code","cfe0ad4a":"code","08c6fe25":"code","8a26a7af":"code","33d17d0b":"code","b65e58c8":"code","73fe4743":"code","c41d5e3d":"code","7208ee74":"code","009c4c4b":"code","b1899f11":"markdown","99d0e886":"markdown","7c3d7fe5":"markdown","2afabc59":"markdown","5a681f9b":"markdown","0e731ab2":"markdown","b3c1f220":"markdown","69ea6e21":"markdown","fb996426":"markdown"},"source":{"be171e18":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)","5a58c399":"train_data = pd.read_csv('..\/input\/train_data_full.csv')\ntrain_data.head()","b773c360":"top_10_videos = train_data.loc[train_data.session_start_datetime >= '2018-09-01 00:00:00', \n                               'primary_video_id'].value_counts()[:10].index.tolist()","0cafe21c":"sample_submission = pd.read_csv('..\/input\/sample_submission_full.csv')\nsample_submission.primary_video_id = ' '.join([str(v) for v in top_10_videos])\ntest_users = sample_submission.user_id.unique()\nsample_submission.head()","2d286b7d":"# dropping samples with kind of 'negative implicit feedback'\ntrain_data = train_data[train_data.watching_percentage >= 0.5]","a1476430":"train_data.primary_video_id = train_data.primary_video_id.astype('category')\ntrain_data['categ_id'] = train_data.primary_video_id.cat.codes + 1","38f1b764":"# Let`s define inverse transform dictionary\ncat_to_element_uid = dict(zip(\n    range(1, len(train_data.primary_video_id.cat.categories) + 1),\n    train_data.primary_video_id.cat.categories\n))\n\n# Assigning most popular film index to inverse transform of zero padding value\ncat_to_element_uid[0] = 29114276","82f80c92":"%%time\nimport tqdm\ntqdm.tqdm.pandas()\nsequences = train_data.groupby('user_id')['categ_id'].progress_apply(list)","96b89369":"sequences.head()","ba69c262":"# Some statistics\nprint('Median length: {}\\nMean length: {}\\nMax length: {}'.format(\n    sequences.apply(len).median(), sequences.apply(len).mean(), sequences.apply(len).max()))","499abc1a":"# We will use users with 5 and more wathced films\nsequences2use = sequences[sequences.apply(len) >= 5]","3debf1cf":"maxlen = 18 # Length of sequences in X\nX = []\ny = []\n\ndef slice_sequence(seq, num_slices):\n    for i in range(1, num_slices):\n        X.append(seq[-(i+maxlen): -i])\n        y.append(seq[-i])\n        \nfor seq in tqdm.tqdm(sequences2use):\n    if len(seq) <= 5:\n        slice_sequence(seq, 2)\n    elif len(seq) <= 6:\n        slice_sequence(seq, 3)\n    elif len(seq) <= 8:\n        slice_sequence(seq, 4)\n    elif len(seq) <= 12:\n        slice_sequence(seq, 6)\n    elif len(seq) <= 16:\n        slice_sequence(seq, 8)\n    elif len(seq) <= 20:\n        slice_sequence(seq, 11)\n    elif len(seq) <= 26:\n        slice_sequence(seq, 16)\n    else:\n        slice_sequence(seq, 23)","1518100b":"len(X), len(y)","f5ca2c6c":"lens = [len(x) for x in X]\nmax(lens), min(lens), np.mean(lens), np.median(lens)","77c40846":"from keras.preprocessing.sequence import pad_sequences\n\n# We should pad our sequences with 0 values, so they all will have the same length\nX = pad_sequences(X, maxlen=maxlen)\ny = np.array(y)\nX.shape, y.shape","171c128e":"from keras.layers import Input, Embedding, SpatialDropout1D, CuDNNLSTM, Dropout, Dense\nfrom keras.models import Model\n\n# Let's set random seed\nimport tensorflow as tf\ntf.set_random_seed(42)\nnp.random.seed(42)","cfe0ad4a":"train_data.categ_id.unique().size + 1","08c6fe25":"max_features = train_data.categ_id.unique().size + 1\nembed_size = 64\n\ndef lstm128():\n    inp = Input(shape=(maxlen,))\n    x = Embedding(max_features, embed_size)(inp)\n    x = SpatialDropout1D(0.05)(x)\n    x = CuDNNLSTM(128, return_sequences=False)(x)\n    x = Dropout(0.02)(x)\n    outp = Dense(max_features, activation=\"softmax\")(x)\n    model = Model(inputs=inp, outputs=outp)\n    model.compile(loss='sparse_categorical_crossentropy', optimizer='rmsprop',\n                  metrics=['sparse_categorical_accuracy'])\n    return model","8a26a7af":"# Let's train our film recommender system\nmodel = lstm128()\nmodel.fit(X, y, batch_size=2048*4, epochs=25, verbose=True, validation_split=0.01, shuffle=True)","33d17d0b":"model_json = model.to_json()\nwith open('lstm128.json', 'w') as json_file:\n    json_file.write(model_json)\n# serialize weights to HDF5\nmodel.save_weights('lstm128.h5')\nprint(\"Saved model to disk\")","b65e58c8":"sequences_test = sequences.apply(lambda x: x[-maxlen:])\nsequences_test = sequences_test.apply(lambda x: [0 for i in range(maxlen - len(x))] + x)","73fe4743":"test_users_in_sequences = sorted(set(sequences_test.index) & set(sample_submission.user_id))","c41d5e3d":"X_test = np.array(sequences_test[test_users_in_sequences].tolist())","7208ee74":"%%time\nfrom itertools import chain\nbatch_size = 2048*8\nn_batches = int(X_test.shape[0]\/batch_size) + 1\npreds = []\n\nfor batch_ind in tqdm.tqdm(range(n_batches)):\n    batch = X_test[batch_ind*batch_size: (batch_ind + 1)*batch_size]\n    curr_preds = model.predict(batch)\n    curr_preds = np.argsort(-curr_preds)[:, :10]\n    curr_preds = [[cat_to_element_uid[x] for x in row] for row in curr_preds]\n    preds.append([' '.join(map(lambda x: str(x), row)) for row in curr_preds])\n    \npreds = list(chain(*preds))","009c4c4b":"sample_submission.index = sample_submission.user_id\nsample_submission.primary_video_id[test_users_in_sequences] = preds\nsample_submission.to_csv('submission_lstm.csv', header=True, index=False)","b1899f11":"## Let's define the model architecture","99d0e886":"Let's find top 10 most popular films. This will be useful later to make recommendations for users, about whom we don't have historical data.","7c3d7fe5":"## Data Preprocessing","2afabc59":"In the next cell we define sequences of films for each user <br>\nExample of transformation:\n<table style=\"width:50%\">\n  <tr>\n    <th align=\"left\">Before<\/th>\n    <th align=\"left\">After<\/th> \n  <\/tr>\n  <tr>\n    <td><table style=\"width:100%\">\n  <tr>\n    <th>user_id<\/th>\n    <th>categ_id<\/th> \n  <\/tr>\n  <tr>\n    <td>12<\/td>\n    <td>2<\/td> \n  <\/tr>\n  <tr>\n    <td>13<\/td>\n    <td>1<\/td> \n  <\/tr>  \n  <tr>\n    <td>12<\/td>\n    <td>1<\/td> \n  <\/tr>    \n   <tr>\n    <td>13<\/td>\n    <td>2<\/td> \n  <\/tr>  \n  <tr>\n    <td>12<\/td>\n    <td>1<\/td> \n  <\/tr>    \n  <tr>\n    <td>13<\/td>\n    <td>3<\/td> \n  <\/tr>   \n<\/table><\/td>\n    <td><table style=\"width:50%\">\n  <tr>\n    <th>user_id<\/th>\n    <th>sequence<\/th> \n  <\/tr>\n  <tr>\n    <td>12<\/td>\n    <td>[2, 1, 1]<\/td> \n  <\/tr>\n  <tr>\n    <td>13<\/td>\n    <td>[1, 2, 3]<\/td> \n<\/table><\/td> \n  <\/tr>\n<\/table>","5a681f9b":"## Data Loading","0e731ab2":"## Prediction","b3c1f220":"# Recurrent Neural Network based Recommender System\nIn this kernel I'm going to show you how to implement a pretty well-performing recommender system using Keras.<br>\nDataset was provided by [MEGOGO](https:\/\/megogo.net\/) in [Megogo Challenge](https:\/\/www.kaggle.com\/c\/megogochallenge)<br>\nTarget metric - [MAP@10](https:\/\/habr.com\/ru\/company\/econtenta\/blog\/303458\/) <br>\nThe main idea of this approach is to predict the next film, that user will watch, knowing the sequence of films, that user has watched earlier.<br>\nIn the terms of ML-engineering we can define this problem as multiclass (with very large amount of classes) sequences classification.","69ea6e21":"Transforming primary_video_id column in a more suitable representation <br>\nFor example, if we have primary_video_id column like [1435, 56453, 1245, 76544], we want to transform it in [2, 3, 1, 4]. Zero value will be used later to pad user-video interactions sequences.","fb996426":"One of the most important part of this solution is to make X and y for our RNN model <br>\nFor example, if we define maxlen = 3, we transform sequence [2, 3, 3, 1, 5, 9] to \n<table style=\"width:50%\">\n  <tr>\n    <th>X<\/th>\n    <th>y<\/th> \n  <\/tr>\n  <tr>\n    <td>[3, 1, 5]<\/td>\n    <td>9<\/td>\n  <\/tr>\n  <tr>\n    <td>[3, 3, 1]<\/td>\n    <td>5<\/td> \n  <\/tr>\n  <tr>\n    <td>[2, 3, 3]<\/td>\n    <td>1<\/td> \n  <\/tr>\n<\/table><br>\nSo, user who watched a lot of films, will be represented by many sequences, and thus, the size of our training dataset will increase significantly."}}