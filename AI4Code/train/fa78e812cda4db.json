{"cell_type":{"ccf389a5":"code","1f10a7b8":"code","6b02065a":"code","bd6d4ffb":"code","1b994fd4":"code","e97ea0fd":"code","2749e91f":"code","844bb8be":"code","039cfbc2":"code","8214ef92":"code","9879ff0f":"code","d2251041":"code","67f4d1a5":"code","952682b4":"code","534298de":"code","c2f1b709":"code","277bcd89":"code","0a764184":"code","92f47bf0":"code","fb8fbecc":"code","e4a17182":"code","2b9201c0":"code","7323fe28":"code","dd40a244":"code","f3f81896":"code","356e65c9":"code","b92cd1f0":"code","347d299f":"code","e12baad2":"code","211eeb29":"code","ed0f1d0c":"code","58ef092e":"code","caa4a53e":"code","3480464c":"code","8dc42e44":"code","ccdf7b5b":"code","aab98d75":"code","2d5af11d":"code","c2738a8b":"code","ac028daf":"code","fa4f46d5":"code","a9402671":"code","d1e8f851":"code","35c4f71b":"code","fd763396":"code","458d2efc":"code","42ea08f5":"code","a0d2d939":"code","80a9af35":"code","5bf206cb":"code","7d916a3e":"code","7c681967":"code","ade9d7cc":"code","fc6c404c":"code","d95a2093":"markdown","aea2d414":"markdown","ca92d974":"markdown","27ef6af3":"markdown","9bf2553c":"markdown","451c88de":"markdown","2567a988":"markdown","7799a86f":"markdown","b86cb8d6":"markdown","66b99143":"markdown","7cb5e91e":"markdown","211f5713":"markdown","77bfd9c8":"markdown","f5a435a4":"markdown","ae9f423e":"markdown","bb960df6":"markdown","4176d94e":"markdown","bedfdee0":"markdown","cfe1ae0e":"markdown","bf3184cd":"markdown","96fd60e6":"markdown","4652d5ea":"markdown","da9a3bcf":"markdown","d5591eee":"markdown","366aa3e9":"markdown","a3fdf20d":"markdown","9b1116fb":"markdown","8f928e11":"markdown","49b9aa6f":"markdown","f784c684":"markdown","038d42a1":"markdown","3ddfe84a":"markdown","4b4102ba":"markdown","933accc9":"markdown","251debff":"markdown","04fd3d32":"markdown"},"source":{"ccf389a5":"!pip install sklearn_evaluation\n","1f10a7b8":"!pip install xgboost","6b02065a":"!pip install lightgbm","bd6d4ffb":"from mpl_toolkits.mplot3d import Axes3D\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.feature_selection import SelectFromModel\n\nimport matplotlib.pyplot as plt # plotting\nfrom sklearn_evaluation import plot\nimport numpy as np # linear algebra\nimport os # accessing directory structure\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom sklearn.model_selection import train_test_split,GridSearchCV\n\nfrom sklearn import preprocessing\nfrom sklearn.externals import joblib\n\n\n%matplotlib inline\n","1b994fd4":"print(os.listdir('..\/input\/kdd-cup-1999-data'))","e97ea0fd":"# Distribution graphs (histogram\/bar graph) of column data\ndef plotPerColumnDistribution(df, nGraphShown, nGraphPerRow):\n    nunique = df.nunique()\n    df = df[[col for col in df if nunique[col] > 1 and nunique[col] < 70]] # For displaying purposes, pick columns that have between 1 and 50 unique values\n    nRow, nCol = df.shape\n    columnNames = list(df)\n    nGraphRow = (nCol + nGraphPerRow - 1) \/ nGraphPerRow\n    plt.figure(num = None, figsize = (8 * nGraphPerRow, 10 * nGraphRow), dpi = 80, facecolor = 'w', edgecolor = 'k')\n    for i in range(min(nCol, nGraphShown)):\n        plt.subplot(nGraphRow, nGraphPerRow, i + 1)\n        columnDf = df.iloc[:, i]\n        if (not np.issubdtype(type(columnDf.iloc[0]), np.number)):\n            valueCounts = columnDf.value_counts()\n            valueCounts.plot.bar()\n        else:\n            columnDf.hist()\n        plt.ylabel('counts')\n        plt.xticks(rotation = 90)\n        plt.title(f'{columnNames[i]} (column {i})')\n    plt.tight_layout(pad = 1.0, w_pad = 1.0, h_pad = 1.0)\n    plt.show()\n","2749e91f":"# Correlation matrix\ndef plotCorrelationMatrix(df, graphWidth, dataframeName):\n    filename = dataframeName#df.dataframeName\n    df = df.dropna('columns') # drop columns with NaN\n    df = df[[col for col in df if df[col].nunique() > 1]] # keep columns where there are more than 1 unique values\n    if df.shape[1] < 2:\n        print(f'No correlation plots shown: The number of non-NaN or constant columns ({df.shape[1]}) is less than 2')\n        return\n    corr = df.corr()\n    plt.figure(num=None, figsize=(graphWidth, graphWidth), dpi=80, facecolor='w', edgecolor='k')\n    corrMat = plt.matshow(corr, fignum = 1)\n    plt.xticks(range(len(corr.columns)), corr.columns, rotation=90)\n    plt.yticks(range(len(corr.columns)), corr.columns)\n    plt.gca().xaxis.tick_bottom()\n    plt.colorbar(corrMat)\n    plt.title(f'Correlation Matrix for {filename}', fontsize=15)\n    plt.show()\n","844bb8be":"# Scatter and density plots\ndef plotScatterMatrix(df, plotSize, textSize):\n    df = df.select_dtypes(include =[np.number]) # keep only numerical columns\n    # Remove rows and columns that would lead to df being singular\n    df = df.dropna('columns')\n    df = df[[col for col in df if df[col].nunique() > 1]] # keep columns where there are more than 1 unique values\n    columnNames = list(df)\n    if len(columnNames) > 10: # reduce the number of columns for matrix inversion of kernel density plots\n        columnNames = columnNames[:10]\n    df = df[columnNames]\n    ax = pd.plotting.scatter_matrix(df, alpha=0.75, figsize=[plotSize, plotSize], diagonal='kde')\n    corrs = df.corr().values\n    for i, j in zip(*plt.np.triu_indices_from(ax, k = 1)):\n        ax[i, j].annotate('Corr. coef = %.3f' % corrs[i, j], (0.8, 0.2), xycoords='axes fraction', ha='center', va='center', size=textSize)\n    plt.suptitle('Scatter and Density Plot')\n    plt.show()\n","039cfbc2":"def list_files(startpath):\n    for root, dirs, files in os.walk(startpath):\n        level = root.replace(startpath, '').count(os.sep)\n        indent = ' ' * 4 * (level)\n        print('{}{}\/'.format(indent, os.path.basename(root)))\n        subindent = ' ' * 4 * (level + 1)\n        for f in files:\n            print('{}{}'.format(subindent, f))\nlist_files('..\/input\/')\n    ","8214ef92":"# with open(\"..\/input\/kddcup.names\", 'r') as f:\n#     print(f.read())\ncols = \"\"\"\n    duration,\nprotocol_type,\nservice,\nflag,\nsrc_bytes,\ndst_bytes,\nland,\nwrong_fragment,\nurgent,\nhot,\nnum_failed_logins,\nlogged_in,\nnum_compromised,\nroot_shell,\nsu_attempted,\nnum_root,\nnum_file_creations,\nnum_shells,\nnum_access_files,\nnum_outbound_cmds,\nis_host_login,\nis_guest_login,\ncount,\nsrv_count,\nserror_rate,\nsrv_serror_rate,\nrerror_rate,\nsrv_rerror_rate,\nsame_srv_rate,\ndiff_srv_rate,\nsrv_diff_host_rate,\ndst_host_count,\ndst_host_srv_count,\ndst_host_same_srv_rate,\ndst_host_diff_srv_rate,\ndst_host_same_src_port_rate,\ndst_host_srv_diff_host_rate,\ndst_host_serror_rate,\ndst_host_srv_serror_rate,\ndst_host_rerror_rate,\ndst_host_srv_rerror_rate\"\"\"\ncols = [c.strip() for c in cols.split(\",\") if c.strip()]\ncols.append('target')\nprint(len(cols))","9879ff0f":"attacks_type = {\n'normal': 'normal',\n'back': 'dos',\n'buffer_overflow': 'u2r',\n'ftp_write': 'r2l',\n'guess_passwd': 'r2l',\n'imap': 'r2l',\n'ipsweep': 'probe',\n'land': 'dos',\n'loadmodule': 'u2r',\n'multihop': 'r2l',\n'neptune': 'dos',\n'nmap': 'probe',\n'perl': 'u2r',\n'phf': 'r2l',\n'pod': 'dos',\n'portsweep': 'probe',\n'rootkit': 'u2r',\n'satan': 'probe',\n'smurf': 'dos',\n'spy': 'r2l',\n'teardrop': 'dos',\n'warezclient': 'r2l',\n'warezmaster': 'r2l',\n    }","d2251041":"df = pd.read_csv(\"..\/input\/kdd-cup-1999-data\/kddcup.data_10_percent\/kddcup.data_10_percent\", names=cols)\ndf['Attack'] = df.target.apply(lambda r: attacks_type[r[:-1]])\nprint(\"The data shape is (lines, columns):\",df.shape)\ndf.head(5)","67f4d1a5":"hajar_to_cup = {\n    'is_hot_login' : 'is_host_login',\n'urg' : 'urgent',\n'protocol' : 'protocol_type',\n'count_sec' : 'count',\n'srv_count_sec' : 'srv_count',\n'serror_rate_sec' : 'serror_rate',\n'srv_serror_rate_sec' : 'srv_serror_rate',\n'rerror_rate_sec' : 'rerror_rate',\n'srv_error_rate_sec' : 'srv_rerror_rate',\n'same_srv_rate_sec' : 'same_srv_rate',\n'diff_srv_rate_sec' : 'diff_srv_rate',\n'srv_diff_host_rate_sec' : 'srv_diff_host_rate',\n'count_100' : 'dst_host_count',\n'srv_count_100' : 'dst_host_srv_count',\n'same_srv_rate_100' : 'dst_host_same_srv_rate',\n'diff_srv_rate_100' : 'dst_host_diff_srv_rate',\n'same_src_port_rate_100' : 'dst_host_same_src_port_rate',\n'srv_diff_host_rate_100' : 'dst_host_srv_diff_host_rate',\n'serror_rate_100' : 'dst_host_serror_rate',\n'srv_serror_rate_100' : 'dst_host_srv_serror_rate',\n'rerror_rate_100' : 'dst_host_rerror_rate',\n'srv_rerror_rate_100' : 'dst_host_srv_rerror_rate',\n}\nfor k,v in hajar_to_cup.items():\n    print(k,v)","952682b4":"df.Attack.value_counts()","534298de":"df.target.unique(), df.Attack.unique()","c2f1b709":"print(df.shape)\ndf.Attack.value_counts()","277bcd89":"\nplotPerColumnDistribution(df[[\n    'protocol_type',\n    'service',\n    'flag',\n    'logged_in',\n    'srv_serror_rate',\n    'srv_diff_host_rate',\n]], nGraphShown=30, nGraphPerRow=2)","0a764184":"plotPerColumnDistribution(df[['target']], nGraphShown=20, nGraphPerRow=4)","92f47bf0":"plotPerColumnDistribution(df[['Attack']], nGraphShown=20, nGraphPerRow=4)","fb8fbecc":"plotCorrelationMatrix(df, graphWidth=20, dataframeName=\"Packets\")","e4a17182":"#plotScatterMatrix(df, plotSize=10, textSize=2)","2b9201c0":"for c in df.columns:\n    print(\"%30s : %d\"%(c, sum(pd.isnull(df[c]))))","7323fe28":"\n# for c in X_train.columns:\n#     if str(X_train[c].dtype) == 'object':\n#         print(c, \"::\", X_train[c].dtype, X_train[c].value_counts())\n#         print(c, \"::\", X_test[c].dtype, X_test[c].value_counts())\n#         print(\"=======\")\n\n# le_X = preprocessing.LabelEncoder()\n# le_y = preprocessing.LabelEncoder()\n\n# for c in X_train.columns:\n#     if str(X_train[c].dtype) == 'object': \n#         X_train[c] = le_X.fit_transform(X_train[c])\n#         X_test[c] = le_X.transform(X_test[c])\n    \n\n# y_train = le_y.fit_transform(y_train.values)\n# y_test = le_y.fit_transform(y_test.values)","dd40a244":"df_std = df.std() # STD of all features\ndf_std = df_std.sort_values(ascending=True) # Std sorted for all features\ndf_std","f3f81896":"plt.figure(figsize=(15,10))\nplt.plot(list(df_std.index) ,list(df_std.values), 'go')\n\nplt.show()","356e65c9":"# To do well we can plot the STD without the greath values (without the feature have std > 3.0)\nplt.figure(figsize=(15,10))\nplt.plot(list(df_std.index)[:-7] ,list(df_std.values)[:-7] , 'go')\n\nplt.show()","b92cd1f0":"#============== MAPPING THE COLUMNS \ndef standardize_columns(df, cols_map=hajar_to_cup):\n    \"\"\"\n    1- Delete the 'service' column.\n    2- Verify if TCPDUMP columns exists, then they will renamed\n    \"\"\"\n    df = df.drop(['service'], axis = 1)\n    df.rename(columns = cols_map)\n    return df\n\ndf = standardize_columns(df, cols_map=hajar_to_cup)","347d299f":"df = df.drop(['target',], axis=1)\nprint(df.shape)\n# Target variable and train set\ny = df.Attack\nX = df.drop(['Attack',], axis=1)\n# Split test and train data \nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\nprint(X_train.shape, X_test.shape)\nprint(y_train.shape, y_test.shape)","e12baad2":"# for c in X_train.columns:\n#     if str(X_train[c].dtype) == 'object':\n#         print(c, \"::\", X_train[c].dtype, X_train[c].value_counts())\n#         print(c, \"::\", X_test[c].dtype, X_test[c].value_counts())\n#         print(\"=======\")\n\n        ","211eeb29":"le_X_cols = {}\nle_y = preprocessing.LabelEncoder()\n\nfor c in X_train.columns:\n    if str(X_train[c].dtype) == 'object': \n        le_X = preprocessing.LabelEncoder()\n        X_train[c] = le_X.fit_transform(X_train[c])\n        X_test[c] = le_X.transform(X_test[c])\n        le_X_cols[c] = le_X\n#------\ny_train = le_y.fit_transform(y_train.values)\ny_test = le_y.transform(y_test.values)\n\n# save the labelers for depploy\njoblib.dump(le_X_cols, 'le_X_cols.pkl') \njoblib.dump(le_y, 'le_y.pkl') ","ed0f1d0c":"class_names, class_index = le_y.classes_, np.unique(y_train)\nclass_names, class_index","58ef092e":"# Feature Scaling\nscaler = StandardScaler(copy=True, with_mean=True, with_std=True)\nX_train[['dst_bytes','src_bytes']] = scaler.fit_transform(X_train[['dst_bytes','src_bytes']])\nX_test[['dst_bytes','src_bytes']] = scaler.transform(X_test[['dst_bytes','src_bytes']])\n#== save the scaler for deploy it\njoblib.dump(scaler, 'scaler_1.pkl') \n","caa4a53e":"X_train[['dst_bytes','src_bytes']].head(5)","3480464c":"from sklearn.ensemble import RandomForestClassifier\n\nclassifier = RandomForestClassifier(n_estimators = 20, criterion = 'entropy', random_state = 42)\nclassifier.fit(X_train, y_train)\nprint(\"Train score is:\", classifier.score(X_train, y_train))\nprint(\"Test score id:\",classifier.score(X_test,y_test))# New data, not included in Training data\ndiff_base = abs(classifier.score(X_train, y_train) - classifier.score(X_test,y_test))\nprint(\"We see that same over\/under fitting for this model, \", diff_base)","8dc42e44":"# Predicting the Test set results\ny_pred = classifier.predict(X_test)\n#\nreversefactor = dict(zip(class_index,class_names))\ny_test_rev = np.vectorize(reversefactor.get)(y_test)\ny_pred_rev = np.vectorize(reversefactor.get)(y_pred)\n# Making the Confusion Matrix\nprint(pd.crosstab(y_test_rev, y_pred_rev, rownames=['Actual packets attacks'], colnames=['Predicted packets attcks']))\n\n\n\nfig, ax = plt.subplots(figsize=(15, 10))\nplot.confusion_matrix(y_test_rev, y_pred_rev, ax=ax)\nplt.show()","ccdf7b5b":"clf = RandomForestClassifier(n_estimators=30)\nclf = clf.fit(X_train, y_train)\nfti = clf.feature_importances_\nmodel = SelectFromModel(clf, prefit=True, threshold= 0.005)\nX_train_new = model.transform(X_train)\nX_test_new = model.transform(X_test)\nselcted_features = X_train.columns[model.get_support()]\nprint(X_train_new.shape)","aab98d75":"selcted_features","2d5af11d":"parameters = {\n    'n_estimators'      : [20,40,128,130],\n    'max_depth'         : [None,14, 15, 17],\n    'criterion' :['gini','entropy'],\n    'random_state'      : [42],\n    #'max_features': ['auto'],\n    \n}\nclf = GridSearchCV(RandomForestClassifier(), parameters, cv=2, n_jobs=-1, verbose=5)\nclf.fit(X_train_new, y_train)","c2738a8b":"print(\"clf.best_estimator_:\",clf.best_estimator_)\nprint(\"clf.best_params_\",clf.best_params_)\nprint(\"results:\")\n#print(clf.cv_results_)","ac028daf":"print(\"CV Train score,\",clf.best_score_)\nprint(\"CV Test score,\",clf.score(X_test_new,y_test))\ndiff_fst = abs(clf.best_score_ - clf.score(X_test_new,y_test))\nprint(\"Diff\", diff_fst)\nprint(\"Diff feature selection is best than base model ? \", diff_base > diff_fst)\n","fa4f46d5":"# Predicting the Test set results\ny_pred = clf.predict(X_test_new)\n#\nreversefactor = dict(zip(class_index,class_names))\ny_test_rev = np.vectorize(reversefactor.get)(y_test)\ny_pred_rev = np.vectorize(reversefactor.get)(y_pred)\n# Making the Confusion Matrix\nprint(pd.crosstab(y_test_rev, y_pred_rev, rownames=['Actual packets attacks'], colnames=['Predicted packets attcks']))\n\n\n\nfig, ax = plt.subplots(figsize=(15, 10))\nplot.confusion_matrix(y_test_rev, y_pred_rev, ax=ax)\nplt.show()","a9402671":"fig, ax = plt.subplots(figsize=(15, 10))\nplot.feature_importances(clf.best_estimator_, top_n=5, ax=ax, feature_names=list(selcted_features))\nplt.show()","d1e8f851":"joblib.dump(clf, 'random_forest_classifier.pkl') \n#To load it: clf_load = joblib.load('saved_model.pkl') ","35c4f71b":"from sklearn.multiclass import OneVsRestClassifier\nfrom xgboost import XGBClassifier\nfrom sklearn.preprocessing import MultiLabelBinarizer","fd763396":"clf = OneVsRestClassifier(XGBClassifier(n_jobs=-1, max_depth=4, n_estimators=70, random_state=42,verbosity=1))\n\n# You may need to use MultiLabelBinarizer to encode your variables from arrays [[x, y, z]] to a multilabel \n# format before training.\nlb = preprocessing.LabelBinarizer(neg_label=0, pos_label=1, sparse_output=False)\nlb.fit(y_train)\ny_train_xgb = lb.transform(y_train)\ny_test_xgb = lb.transform(y_test)\n\nclf.fit(X_train[selcted_features], y_train_xgb)\n","458d2efc":"y_pred_xgb = clf.predict(X_test[selcted_features])\n\nprint(\"Train score is:\", clf.score(X_train[selcted_features], y_train_xgb))\nprint(\"Test score id:\",clf.score(X_test[selcted_features],y_test_xgb))# New data, not included in Training data\ndiff_xgb = abs(clf.score(X_train[selcted_features], y_train_xgb) - clf.score(X_test[selcted_features],y_test_xgb))\nprint(\"The diff, \", diff_xgb)\n","42ea08f5":"y_pred_xgb = np.argmax(y_pred_xgb, axis=1)","a0d2d939":"\n#\nreversefactor = dict(zip(class_index,class_names))\ny_test_rev = np.vectorize(reversefactor.get)(y_test)\ny_pred_rev = np.vectorize(reversefactor.get)(y_pred_xgb)\n# Making the Confusion Matrix\nprint(pd.crosstab(y_test_rev, y_pred_rev, rownames=['Actual packets attacks'], colnames=['Predicted packets attcks']))\n\n\n\nfig, ax = plt.subplots(figsize=(15, 10))\nplot.confusion_matrix(y_test_rev, y_pred_rev, ax=ax)\nplt.show()","80a9af35":"import xgboost as xgb\nprint(X_train.shape)\n\nxgb_model = OneVsRestClassifier(XGBClassifier(n_jobs=-1, max_depth=4, n_estimators=70, random_state=42,verbosity=1))\n\nparameters = {'estimator__nthread':[4,], #when use hyperthread, xgboost may become slower\n              'estimator__objective':['binary:logistic',],\n              'estimator__learning_rate': [0.1,0.08], #so called `eta` value\n              'estimator__max_depth': [4,6],\n              'estimator__min_child_weight': [1,],\n              'estimator__silent': [1,],\n              'estimator__subsample': [1,],\n              'estimator__colsample_bytree': [1,],\n              'estimator__n_estimators': [70,100], #number of trees, change it to 1000 for better results\n              'estimator__random_state':[42],\n              }\n\n\nclf = GridSearchCV(xgb_model, parameters, \n                   cv=2, n_jobs=-1, verbose=5, refit=True)\n\nclf.fit(X_train[selcted_features], y_train_xgb)","5bf206cb":"print(\"CV Train score,\",clf.best_score_)\nprint(\"Params\", clf.best_params_)\nprint(\"CV Test score,\",clf.score(X_test[selcted_features],y_test_xgb))\ndiff_fst = abs(clf.best_score_ - clf.score(X_test[selcted_features],y_test_xgb))\nprint(\"Diff\", diff_fst)\n\nprint(\"Diff feature selection is best than xgb base model ? \", diff_xgb > diff_fst)\nprint(\"Diff feature selection is best than RDF best model ? \", diff_base > diff_fst)\n","7d916a3e":"#\ny_pred_xgb = clf.predict(X_test[selcted_features])\ny_pred_xgb = np.argmax(y_pred_xgb, axis=1)\nreversefactor = dict(zip(class_index,class_names))\ny_test_rev = np.vectorize(reversefactor.get)(y_test)\ny_pred_rev = np.vectorize(reversefactor.get)(y_pred_xgb)\n# Making the Confusion Matrix\nprint(pd.crosstab(y_test_rev, y_pred_rev, rownames=['Actual packets attacks'], colnames=['Predicted packets attcks']))\n\n\n\nfig, ax = plt.subplots(figsize=(15, 10))\nplot.confusion_matrix(y_test_rev, y_pred_rev, ax=ax)\nplt.show()","7c681967":"joblib.dump(clf, 'xgboost_classifier.pkl') \n#To load it: clf_load = joblib.load('saved_model.pkl') ","ade9d7cc":"scaler_1 = joblib.load('scaler_1.pkl') \nle_X_cols = joblib.load('le_X_cols.pkl') \nle_y = joblib.load('le_y.pkl') \nxgb_clf = joblib.load('xgboost_classifier.pkl') \nrdf_clf = joblib.load('random_forest_classifier.pkl') ","fc6c404c":"def do_what_we_want(X, \n                    scaler_1, \n                    le_X_cols, \n                    selcted_features, \n                    map_cols,\n                    rdf_clf,\n                    xgb_clf):\n    X[['dst_bytes','src_bytes']] = scalscaler_1er.fit_transform(X[['dst_bytes','src_bytes']])\n    for c in X.columns:\n        if str(X[c].dtype) == 'object': \n            le_X = le_X_cols[c]\n            X[c] = le_X.transform(X[c])\n            \n    X = standardize_columns(X, cols_map=map_cols) # Rename the columns, and delete the \n    \n    X = X[selcted_features]\n    #====\n    rd_prediction = rdf_clf.predict(X)\n    xgb_prediction = xgb_clf.predict(X)\n    \n    return rd_prediction, xgb_prediction\n    \n\n    ","d95a2093":"### Save the XGBoost Model","aea2d414":"* Attack (The attack types grouped by attack, it's what we will predict)","ca92d974":"## Exploratory Analysis","27ef6af3":"### Split Train and Test data","9bf2553c":"### 2-1: base model","451c88de":"###### Matrix confusion for this model","2567a988":"* ** <strong style=\"color:green\">Data distribution: Target feature distribution <\/strong>**","7799a86f":"#### 1-1: Features Selection\nWe search the must importants foeatures for RandomForestClassier","b86cb8d6":"### 1-3: Feature importances (RandomForestClassifier)\nThe importance of a feature is the increase in the prediction error of the model after we permuted the feature\u2019s values, which breaks the relationship between the feature and the true outcome.\n","66b99143":"There is 0 csv file in the current version of the dataset:\n","7cb5e91e":"* Attack types","211f5713":"#### 1-0: base model, without parameters tuning and features selection","77bfd9c8":"The next hidden code cells define functions for plotting data. Click on the \"Code\" button in the published kernel to reveal the hidden code.","f5a435a4":"Oh, no! There are no automatic insights available for the file types used in this dataset. As your Kaggle kerneler bot, I'll keep working to fine-tune my hyper-parameters. In the meantime, please feel free to try a different dataset.","ae9f423e":"### Data Labeling\/Encoding","bb960df6":"#### Best XGB model, confusion matrix[](http:\/\/)","4176d94e":"** <strong style=\"color:green\">Data correlation: Numeric Features correlation <\/strong>**","bedfdee0":"### 1-2: Confusion Matrix \nA confusion matrix is a summary of prediction results on a classification problem.\nThe number of correct and incorrect predictions are summarized with count values and broken down by each class. This is the key to the confusion matrix.\nThe confusion matrix shows the ways in which your classification model is confused when it makes predictions.\nIt gives us insight not only into the errors being made by a classifier but more importantly the types of errors that are being made","cfe1ae0e":"### Data Descovring","bf3184cd":"# Data Preprocessing","96fd60e6":"### 1- RandomForestClassifier Model","4652d5ea":"** <strong style=\"color:green\">Data distribution: Categorical Features distribution <\/strong>**","da9a3bcf":"#### 2-1: Params Tuning (Find best prams => best model)","d5591eee":"## Ensembling (Belnding): \n#### Construction a Model ensembling from the models we realised\n","366aa3e9":"### 2- XGBoost Model","a3fdf20d":"### Featrures Elimination, (Variance, Correclation ....)\n* STD (variance):\n\nWe will remove the features with a null std (num_outbound_cmds, is_host_login )","9b1116fb":"## Introduction\n","8f928e11":"### XGB based model, confusion matrix","49b9aa6f":"* STD equal 0 : <font style=\"color:blue;\">is_host_login<\/font> and <font style=\"color:blue;\">num_outbound_cmds<\/font> ==> It's mean than the two features are constant.\n* A greath variance: <font style=\"color:blue;\">src_bytes<\/font> and <font style=\"color:blue;\">dst_bytes<\/font>, that is mean either are identifier variables like IDs. We can also eleminate this kind of variable            ","f784c684":"** <strong style=\"color:green\">Data correlation: Numeric Features Scatter matrix <\/strong>**","038d42a1":"#### 1-2 Parameters Tunning\nSearch the best parameters for RandomForestClassifier","3ddfe84a":"### Save RandomForestModel","4b4102ba":"* Protocol type: \nWe notice that ICMP is the most present in the used data, then TCP and almost 20000 packets of UDP type\n* logged_in (1 if successfully logged in; 0 otherwise):\nWe notice that just 70000 packets are successfully logged in.\n","933accc9":"* Correlation, (We already plot it below \"Correlation matrix for packets\")\n\nWe will eleminate some features whose fully correlated, Example: (srv_serror_rate, serror_rate) correlated with (dst_host_srv_count, dst_host_count), we can in this case eleminate srv_rate and dst_host_count.","251debff":"We will use:\n* **OneVsRestClassifier**:\n    One-vs-the-rest (OvR) multiclass\/multilabel strategy\n    Also known as one-vs-all, this strategy consists in fitting one classifier per class. For each classifier, the class is fitted    against all the other classes.\n* **LabelBinarizer**:  At learning time, this simply consists in learning one regressor or binary classifier per class. In doing so, one needs to convert multi-class labels to binary labels (belong or does not belong to the class). LabelBinarizer makes this process easy with the transform method.","04fd3d32":"### Missing Values"}}