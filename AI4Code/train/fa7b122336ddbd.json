{"cell_type":{"837f9d22":"code","30e659cc":"code","b683faa1":"code","b5a10f58":"code","5e0b8ae2":"code","227d5a5a":"code","14ff100d":"code","eeb3fe5e":"code","73d98c0f":"code","f2f7cc0a":"code","95600d5e":"code","f4fee44b":"code","bb5d6ed2":"code","db6d1928":"code","bd7588af":"code","308a4d49":"code","d48cb98b":"code","7335422d":"code","90f29ae8":"code","b9cebad8":"code","9228742c":"code","5ce41420":"code","24b03d51":"code","579de406":"code","d3e25e3b":"code","4e37cb3f":"code","27263fc5":"code","a3b15af1":"code","b84f0b4d":"code","e3b41afe":"code","820064b7":"code","8ef52b96":"code","29dedc18":"code","09402af2":"code","82341b44":"code","09049040":"code","a46ab005":"code","9086a32b":"code","d00754d4":"code","5f9e32b2":"code","d8db96ab":"code","8a836e18":"code","adb47ea2":"code","3670f159":"code","6016320b":"code","a0503642":"code","189948d8":"code","28f4e12c":"code","fb89d0cb":"code","9463afe9":"code","395f6ac2":"markdown","4aadb271":"markdown","823ad06c":"markdown","5d6357e9":"markdown","e21e020b":"markdown","75ad4bb8":"markdown","f9aea65f":"markdown","4fc3e876":"markdown","901f3aa0":"markdown","46f7d368":"markdown","9e427629":"markdown","f5a63bef":"markdown","ea25b537":"markdown","84dfe4bd":"markdown","447e623b":"markdown","273b5467":"markdown","30e2d66a":"markdown","2f2288d5":"markdown","4d972761":"markdown","360ec506":"markdown","65035cf6":"markdown"},"source":{"837f9d22":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","30e659cc":"df_fraud = pd.read_csv('..\/input\/creditcard.csv')","b683faa1":"def fraud_per(data):\n    fraud = data[data == 1].count()\/data.shape[0] \n    return fraud *100\nfraud = fraud_per(df_fraud['Class'])\nprint('Percentage of fraud observations = ' + str(fraud)+'%')","b5a10f58":"df_fraud.describe()","5e0b8ae2":"df_fraud.Time.plot()","227d5a5a":"df_fraud['sin_time'] = np.sin(2*np.pi*df_fraud.Time\/(24*60*60))\ndf_fraud['cos_time'] = np.cos(2*np.pi*df_fraud.Time\/(24*60*60))","14ff100d":"df_fraud.sample(100).plot.scatter('sin_time','cos_time').set_aspect('equal')","eeb3fe5e":"df_fraud.head()","73d98c0f":"X = df_fraud.drop(['Time', 'Class'], axis = 1)\ny = df_fraud['Class']","f2f7cc0a":"from sklearn.preprocessing import MinMaxScaler\ndef scaling_data(X):\n    x_col = X.columns\n    scaler = MinMaxScaler()\n    scaler.fit(X)\n    #Xtr_norm = pd.DataFrame(scaler.transform(Xtr), columns = x_col)\n    #Xte_norm = pd.DataFrame(scaler.transform(Xte), columns = x_col)\n    X_norm = scaler.transform(X)\n    return X_norm","95600d5e":"X_scaled= scaling_data(X)","f4fee44b":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X_scaled, y, random_state=0, test_size=0.2)\nX_train1, X_valid, y_train1, y_valid = train_test_split(X_train, y_train, random_state=0, test_size=0.2)","bb5d6ed2":"print('Percentage of fraud observations on Train Data', fraud_per(y_train))\nprint('Percentage of fraud observations on Validation Data', fraud_per(y_valid))\nprint('Percentage of fraud observations on Test Data', fraud_per(y_test))","db6d1928":"from sklearn.linear_model import LogisticRegression\ndef LogisticClassifier(Xtr, ytr, c):\n    lr = LogisticRegression(C = c, solver = 'lbfgs', max_iter=2000)\n    model = lr.fit(Xtr, ytr)\n    return model","bd7588af":"from sklearn.metrics import recall_score, precision_score\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import classification_report\ndef recall_pre_accuracy(Xv, yv, model, target_value):\n    yp = model.predict(Xv)\n    accuracy = model.score(Xv, yv)\n    recall= recall_score(yv, yp)\n    precision = precision_score(yv, yp)\n    report = classification_report(yv, yp, target_names = target_value)\n    return accuracy, recall, precision, report","308a4d49":"from sklearn.metrics import confusion_matrix\ndef conf_matrix(Xv, yv, model):\n    # Negative class (0) is most frequent\n    y_predicted = model.predict(Xv)\n    confusion = confusion_matrix(yv, y_predicted)\n    return confusion","d48cb98b":"from sklearn.metrics import roc_curve, auc\nfrom sklearn.metrics import precision_recall_curve\n\ndef precision_recall_curves(y_test, y_scores):\n    # precision recall curve\n    precisionc, recallc, thresholds = precision_recall_curve(y_test, y_scores)\n    pr_rel_df = pd.DataFrame([precisionc, recallc], index = ['precision', 'recall']).T\n    # find threshold closest to zero:\n    close_zero = np.argmin(np.abs(thresholds))\n    plt.plot(precisionc[close_zero], recallc[close_zero], 'o', markersize=10, label=\"threshold zero\", \n             fillstyle=\"none\", c='k', mew=2)\n    plt.plot(precisionc, recallc, label=\"precision recall curve\")\n    plt.xlabel(\"precision\")\n    plt.ylabel(\"recall\")\n    return precisionc[close_zero], recallc[close_zero]","7335422d":"def auc_roc_curves(y_test, y_scores, title): \n    #roc_curve, auc\n    fpr, tpr, thresholds = roc_curve(y_test, y_scores)\n    roc_auc = auc(fpr, tpr)\n    roc_df = pd.DataFrame([fpr, tpr], index = ['fpr', 'tpr']).T\n    plt.plot(fpr, tpr, label=\"ROC Curve\")\n    plt.xlabel(\"FPR\")\n    plt.ylabel(\"TPR (recall)\")\n    plt.title(title)\n    # find threshold closest to zero:\n    close_zero = np.argmin(np.abs(thresholds))\n    plt.plot(fpr[close_zero], tpr[close_zero], 'o', markersize=10, label=\"threshold zero\", \n             fillstyle=\"none\", c='k', mew=2)\n    plt.legend(loc=4)\n    return roc_auc","90f29ae8":"def best_C(Xt, yt, xv, yv, c_list):\n    best_C = 0\n    min_mae = 0\n    for c in C_list:\n        model = LogisticClassifier(X_train1, y_train1, c)\n        # calculate mean absolute error\n        #preds_val = model.predict(X_valid)\n        mae = 1- model.score(X_valid, y_valid)\n        if min_mae == 0 or mae < min_mae:\n            min_mae = mae\n            best_C = c\n    return best_C","b9cebad8":"C_list = list(1\/np.logspace(-4, 2, num=10))","9228742c":"C = best_C(X_train1, y_train1, X_valid, y_valid, C_list)","5ce41420":"print('Best C:\\n', C)","24b03d51":"lr_modelC = LogisticClassifier(X_train1, y_train1, C)\ny_scores_lrC = lr_modelC.decision_function(X_test)","579de406":"target_value = [ 'not fraud', 'fraud']\naccuracy1, recall1, precision1, report1 = recall_pre_accuracy(X_test, y_test, lr_modelC, target_value)\nprint('Logistic Regression on Original Data:')\nprint ('Accuracy Score = ', accuracy1)\nprint ('Recall Score = ', recall1)\nprint ('Precision Score1 = ', precision1)\nprint('Report Score:\\n', report1)","d3e25e3b":"print('Confusion Matrix: \\n', conf_matrix(X_test, y_test, lr_modelC))","4e37cb3f":"precision0, recall0  = precision_recall_curves(y_test, y_scores_lrC)\nprint(precision0, recall0)","27263fc5":"roc_auc = auc_roc_curves(y_test, y_scores_lrC, 'ROC Curve on Original Data')\nprint('AUC = ', roc_auc)","a3b15af1":"from imblearn.over_sampling import SMOTE, ADASYN\nX_resampled, y_resampled = SMOTE(kind='borderline2').fit_sample(X_train1, y_train1)","b84f0b4d":"x_col = X.columns\ndf_fraud_over = pd.DataFrame(X_resampled, columns = x_col)\ndf_fraud_over['Class'] = y_resampled\ndf_fraud_over.shape","e3b41afe":"print('Percentage of fraud observations on Upsamled Data', fraud_per(df_fraud_over['Class']))","820064b7":"X_train_up, X_valid_up, y_train_up, y_valid_up = train_test_split(X_resampled, y_resampled,\n                                                                random_state=0, test_size=0.2)","8ef52b96":"C_up = best_C(X_train_up, y_train_up, X_valid_up, y_valid_up, C_list)","29dedc18":"print('Best C: ', C_up)","09402af2":"lr_model_up = LogisticClassifier(X_resampled, y_resampled, C_up)","82341b44":"y_scores_lr_up = lr_model_up.decision_function(X_test)","09049040":"target_value = [ 'not fraud', 'fraud']\naccuracyup, recallup, precisionup, reportup = recall_pre_accuracy(X_test, y_test, lr_model_up, target_value)\nprint('Logistic Regression on Over_Sampled Data Validation Set:')\nprint ('Accuracy Score = ', accuracyup)\nprint ('Recall Score = ', recallup)\nprint ('Precision Score1 = ', precisionup)\nprint('Report Score:\\n', reportup)","a46ab005":"print('Confusion Matrix: \\n', conf_matrix(X_test, y_test, lr_model_up))","9086a32b":"precision_up, recall_up = precision_recall_curves(y_test, y_scores_lr_up)\nprecision_up, recall_up","d00754d4":"roc_auc_up = auc_roc_curves(y_test, y_scores_lr_up, 'ROC Curve on Over-sample Minority Class')\nprint('AUC on Over-sample = ', roc_auc_up)","5f9e32b2":"from imblearn.under_sampling import NearMiss\nnm1 = NearMiss(random_state=0, version=3)\nX_resampledu, y_resampledu = nm1.fit_sample(X_train, y_train)","d8db96ab":"x_col = X.columns\ndf_fraud_Under = pd.DataFrame(X_resampledu, columns = x_col)\ndf_fraud_Under['Class'] = y_resampledu\ndf_fraud_Under.head()\ndf_fraud_Under.shape","8a836e18":"print('Percentage of fraud observations on Under-samled Data', fraud_per(df_fraud_Under['Class']))","adb47ea2":"from sklearn.linear_model import LogisticRegressionCV\ndef lrCV(X, y):\n    lr = LogisticRegressionCV(cv = 5, penalty= 'l2', scoring='recall',  max_iter=2000)\n    model = lr.fit(X,y)\n    return model ","3670f159":"lrCV_un = lrCV(X_resampledu, y_resampledu)","6016320b":"C_un = lrCV_un.C_[0]\nprint('Best C: ', lrCV_un.C_[0])","a0503642":"lr_model_under = LogisticClassifier(X_resampledu, y_resampledu, C_un)\ny_scores_lr_under = lr_model_under.decision_function(X_test)","189948d8":"target_value = [ 'not fraud', 'fraud']\naccuracy1, recall1, precision1, report1 = recall_pre_accuracy(X_test, y_test, lr_model_under, target_value)\nprint('Logistic Regression on Under_Sampled Data TEst Set:')\nprint ('Accuracy Score = ', accuracy1)\nprint ('Recall Score = ', recall1)\nprint ('Precision Score1 = ', precision1)\nprint('Report Score:\\n', report1)","28f4e12c":"print('Confusion Matrix: \\n', conf_matrix(X_test, y_test, lr_model_under))","fb89d0cb":"recallund, precisionund = precision_recall_curves(y_test, y_scores_lr_under)","9463afe9":"roc_auc_under = auc_roc_curves(y_test, y_scores_lr_under, 'ROC Curve on Under-sample Majority Class')\nprint('AUC on Under-sample = ', roc_auc_under)","395f6ac2":"#### Because the sample sizes are small we will use cross validation set","4aadb271":"#### Time data feature engineering\nin below plot we can see that time is increasing, and this not true, the time has a cyclical nature of 24-hour time, so we need to change the time feature","823ad06c":"### Metrics and model Evaluation","5d6357e9":"### Calculate the percentage of the fraud observations in the dataset\nthe percentage shown that the data is unbalanced","e21e020b":"#### Fit the over-sample minority classes into logistic regression classifier and then use the original test set without sampling","75ad4bb8":"#### ROC Curve, AUC, and Precision Recall Curve Functions","f9aea65f":"### 2. Under-sample Majority Class","4fc3e876":"### Conclusion\nthe over-sample Minority Class is the best model for fraud prediction, because the recall score increased.","901f3aa0":"### Feature Normalize and data training","46f7d368":"> #### The Best C for Best Regulaization Function","9e427629":"## Balancing the Data","f5a63bef":"### Choosing a classifiers\nHere i chose logistic regression, because the data is large and all the other classifier take a long time in training","ea25b537":"We will use type two NearMiss adds some heuristic rules to select samples. NearMiss implements 3 different types of heuristic which can be selected with the parameter version: 1, 2, 3. We will use version 3","84dfe4bd":"Above metric shows that the recall value is very low, that reflects the non fraud predcition, this becauxe the data are imbalanced.","447e623b":"#### Train Upsamled Data","273b5467":"### Now lets train the original data","30e2d66a":"### 1. Over-sample Minority Class[](http:\/\/)\nHere we will use SMOTE to over-sample minority classes\n****We will train the X_train and y_train to check later the test set (original data) how well it will evaluate","2f2288d5":"* The most logical way to transform second is into two variables that swing back and forth out of sink. Imagine the position of the end of the second hand of a 24-hour clock. The x position swings back and forth out of sink with the y position, for a 24-hour clock, you can accomplish this with x=sin(2pi*second\/(24x60x60), y=cos(2pi*hour\/(24x60x60)).","4d972761":"#### Calculate the percentage of Not Fraud in data","360ec506":"> #### Cconfusion matrix Function","65035cf6":"#### Recall, precision and accuracy Function"}}