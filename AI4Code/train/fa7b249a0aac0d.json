{"cell_type":{"9b60f667":"code","adef36bf":"code","3702cd63":"code","8e4dad8c":"code","cc4ac4ac":"code","565f2dd6":"code","18e80bf4":"code","2052f7be":"markdown","1ccc1c3e":"markdown","5e032027":"markdown"},"source":{"9b60f667":"from kaggle_environments import make\n\nenv = make(\"rps\")","adef36bf":"%%writefile decision_tree.py\n\nimport numpy as np\nimport collections\nfrom sklearn.tree import DecisionTreeClassifier\n\ndef construct_local_features(rollouts):\n    step_mode_features = np.array([[step % k for step in rollouts['steps']] for k in (2, 3, 5)])\n    step_div_features = np.array([[step \/\/ k for step in rollouts['steps']] for k in (100, 150, 250)])\n    features = np.concatenate([step_mode_features, step_div_features])\n    features = np.append(features, rollouts['actions'])\n    features = np.append(features, rollouts['opp-actions'])\n    return features\n\ndef construct_global_features(rollouts):\n    features = []\n    for key in ['actions', 'opp-actions']:\n        for i in range(3):\n            actions_count = np.mean([r == i for r in rollouts[key]])\n            features.append(actions_count)\n    return np.array(features)\n\ndef construct_features(short_stat_rollouts, long_stat_rollouts):\n    lf = construct_local_features(short_stat_rollouts)\n    gf = construct_global_features(long_stat_rollouts)\n    features = np.concatenate([lf, gf])\n    return features\n\ndef predict_opponent_move(train_data, test_sample):\n    classifier = DecisionTreeClassifier(random_state=42)\n    classifier.fit(train_data['x'], train_data['y'])\n    return classifier.predict(test_sample)\n\ndef update_rollouts_hist(rollouts_hist, last_move, opp_last_action):\n    rollouts_hist['steps'].append(last_move['step'])\n    rollouts_hist['actions'].append(last_move['action'])\n    rollouts_hist['opp-actions'].append(opp_last_action)\n    return rollouts_hist\n\ndef warmup_strategy(observation, configuration):\n    global rollouts_hist, last_move\n    action = int(np.random.randint(3))\n    if observation.step == 0:\n        last_move = {'step': 0, 'action': action}\n        rollouts_hist = {'steps': [], 'actions': [], 'opp-actions': []}\n    else:\n        rollouts_hist = update_rollouts_hist(rollouts_hist, last_move, observation.lastOpponentAction)\n        last_move = {'step': observation.step, 'action': action}\n    return int(action)\n\ndef init_training_data(rollouts_hist, k):\n    for i in range(len(rollouts_hist['steps']) - k + 1):\n        short_stat_rollouts = {key: rollouts_hist[key][i:i+k] for key in rollouts_hist}\n        long_stat_rollouts = {key: rollouts_hist[key][:i+k] for key in rollouts_hist}\n        features = construct_features(short_stat_rollouts, long_stat_rollouts)        \n        data['x'].append(features)\n    test_sample = data['x'][-1].reshape(1, -1)\n    data['x'] = data['x'][:-1]\n    data['y'] = rollouts_hist['opp-actions'][k:]\n    return data, test_sample\n\ndef agent(observation, configuration):\n    # hyperparameters\n    k = 5\n    min_samples = 25\n    global rollouts_hist, last_move, data, test_sample\n    if observation.step == 0:\n        data = {'x': [], 'y': []}\n    # if not enough data -> randomize\n    if observation.step <= min_samples + k:\n        return warmup_strategy(observation, configuration)\n    # update statistics\n    rollouts_hist = update_rollouts_hist(rollouts_hist, last_move, observation.lastOpponentAction)\n    # update training data\n    if len(data['x']) == 0:\n        data, test_sample = init_training_data(rollouts_hist, k)\n    else:        \n        short_stat_rollouts = {key: rollouts_hist[key][-k:] for key in rollouts_hist}\n        features = construct_features(short_stat_rollouts, rollouts_hist)\n        data['x'].append(test_sample[0])\n        data['y'] = rollouts_hist['opp-actions'][k:]\n        test_sample = features.reshape(1, -1)\n        \n    # predict opponents move and choose an action\n    next_opp_action_pred = predict_opponent_move(data, test_sample)\n    action = int((next_opp_action_pred + 1) % 3)\n    last_move = {'step': observation.step, 'action': action}\n    return action","3702cd63":"import numpy as np\n\ndef copy_opponent_agent(observation, configuration):\n    if observation.step > 0:\n        return observation.lastOpponentAction\n    else:\n        return 0\n\ndef random_agent(observation, configuration):\n    return int(np.random.randint(3))","8e4dad8c":"env.reset()\nenv.run(['decision_tree.py', copy_opponent_agent])\nenv.render(mode=\"ipython\", width=500, height=450)","cc4ac4ac":"env.reset()\nenv.run(['decision_tree.py', random_agent])\nenv.render(mode=\"ipython\", width=500, height=450)","565f2dd6":"%%writefile markov.py\n\nimport numpy as np\nimport collections\n\ndef markov_agent(observation, configuration):\n    k = 2\n    global table, action_seq\n    if observation.step % 250 == 0: # refresh table every 250 steps\n        action_seq, table = [], collections.defaultdict(lambda: [1, 1, 1])    \n    if len(action_seq) <= 2 * k + 1:\n        action = int(np.random.randint(3))\n        if observation.step > 0:\n            action_seq.extend([observation.lastOpponentAction, action])\n        else:\n            action_seq.append(action)\n        return action\n    # update table\n    key = ''.join([str(a) for a in action_seq[:-1]])\n    table[key][observation.lastOpponentAction] += 1\n    # update action seq\n    action_seq[:-2] = action_seq[2:]\n    action_seq[-2] = observation.lastOpponentAction\n    # predict opponent next move\n    key = ''.join([str(a) for a in action_seq[:-1]])\n    if observation.step < 500:\n        next_opponent_action_pred = np.argmax(table[key])\n    else:\n        scores = np.array(table[key])\n        next_opponent_action_pred = np.random.choice(3, p=scores\/scores.sum()) # add stochasticity for second part of the game\n    # make an action\n    action = (next_opponent_action_pred + 1) % 3\n    # if high probability to lose -> let's surprise our opponent with sudden change of our strategy\n    if observation.step > 900:\n        action = next_opponent_action_pred\n    action_seq[-1] = action\n    return int(action)","18e80bf4":"env.reset()\nenv.run(['decision_tree.py', 'markov.py'])\nenv.render(mode=\"ipython\", width=500, height=450)","2052f7be":"# Define Decision-Tree agent","1ccc1c3e":"# Compare to simple agents","5e032027":"## ... and to [(not-so) Markov agent](https:\/\/www.kaggle.com\/alexandersamarin\/not-so-markov)"}}