{"cell_type":{"3ee9d0f2":"code","0272523b":"code","01071ade":"code","577a2346":"code","71e090ad":"code","beccaf42":"code","aeaa0f67":"code","f2828ba2":"code","d5177ede":"code","7cfadc08":"code","e48ad8e9":"code","274c7f3d":"code","8d866bfc":"code","92add2cf":"code","0555b528":"code","ccd436c7":"code","e74f78cf":"code","a336c9a2":"code","53f26180":"code","81c209a7":"code","cd8e508d":"code","d8ebd01b":"code","8ea65ee0":"code","464425c9":"code","8d2d3445":"code","11e648fe":"code","c856d412":"code","bd1ce3d8":"code","7f9e7788":"code","7a53857a":"code","59ea6111":"code","8c5c94cf":"code","14219ad8":"code","76b38e02":"markdown","2056e202":"markdown","3d6dc9fa":"markdown","c60ae26a":"markdown","513a421a":"markdown","e081f47d":"markdown","212cc4c6":"markdown","91bb33fe":"markdown","3e9b9af3":"markdown","926b3b27":"markdown","af5e5b45":"markdown","4d71f583":"markdown","61c4769d":"markdown","da4e3fb5":"markdown","3d5a8f29":"markdown","8bb72f59":"markdown","49ff327f":"markdown","da584a32":"markdown","492dbead":"markdown","1150dd89":"markdown","505464d8":"markdown","e3fd0baf":"markdown","5e4478e3":"markdown","97a115a6":"markdown","67825cba":"markdown","c200a994":"markdown","ad65a2db":"markdown","4437859c":"markdown","22ecc506":"markdown","bff2c8fe":"markdown","43f147f4":"markdown","82679709":"markdown","62f0dd8d":"markdown","32311370":"markdown","b6290038":"markdown","8aff66e5":"markdown","6320ea8e":"markdown","18381b3b":"markdown","42a80da2":"markdown","4ac0c9df":"markdown","6efba678":"markdown","04ae0cad":"markdown","557b4afb":"markdown","6736a0bc":"markdown","7dbcd16d":"markdown","454a586d":"markdown","e3f885c4":"markdown","a2ee84e3":"markdown","194a9c83":"markdown","f74351b9":"markdown","e4e6be10":"markdown","619e4814":"markdown","59bc5922":"markdown","6e5eda77":"markdown","9fe7716f":"markdown","b091cead":"markdown"},"source":{"3ee9d0f2":"# import libraries for data manipulation\nimport pandas as pd\nimport numpy as np\n\n# import libraries for data visualization\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom statsmodels.graphics.gofplots import ProbPlot\n\n# import libraries for building linear regression model\nfrom statsmodels.formula.api import ols\nimport statsmodels.api as sm\nfrom sklearn.linear_model import LinearRegression\n\n# import library for preparing data\nfrom sklearn.model_selection import train_test_split\n\n# import library for data preprocessing\nfrom sklearn.preprocessing import MinMaxScaler\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","0272523b":"df = pd.read_csv('..\/input\/the-boston-houseprice-data\/boston.csv')\ndf.head()","01071ade":"df.info()","577a2346":"df.describe()","71e090ad":"# let's plot all the columns to look at their distributions\nfor i in df.columns:\n    plt.figure(figsize=(7, 4))\n    sns.histplot(data=df, x=i, kde = True)\n    plt.show()","beccaf42":"df['MEDV_log'] = np.log(df['MEDV'])","aeaa0f67":"sns.histplot(data=df,x='MEDV_log',kde=True)","f2828ba2":"plt.figure(figsize = (12,8))\ncmap = sns.diverging_palette(230,20,as_cmap=True)\nsns.heatmap(df.corr(),annot=True,fmt='.2f',cmap=cmap)\nplt.show()","d5177ede":"# scatterplot to visualize the relationship between NOX and INDUS\nplt.figure(figsize=(6, 6))\nsns.scatterplot(x = 'INDUS', y = 'NOX', data = df) #write you code here\nplt.show()","7cfadc08":"# scatterplot to visualize the relationship between AGE and NOX\nplt.figure(figsize=(6, 6))\nsns.scatterplot(x='AGE', y = 'NOX', data = df)   #Write your code here\nplt.show()","e48ad8e9":"# scatterplot to visualize the relationship between DIS and NOX\nplt.figure(figsize=(6, 6))\nsns.scatterplot(x = 'DIS', y = 'NOX', data = df) #Write your code here\nplt.show()","274c7f3d":"# scatterplot to visualize the relationship between AGE and DIS\nplt.figure(figsize=(6, 6))\nsns.scatterplot(x = 'AGE', y = 'DIS', data = df)\nplt.show()","8d866bfc":"# scatterplot to visualize the relationship between AGE and INDUS\nplt.figure(figsize=(6, 6))\nsns.scatterplot(x = 'AGE', y = 'INDUS', data = df)\nplt.show()","92add2cf":"# scatterplot to visulaize the relationship between RAD and TAX\nplt.figure(figsize=(6, 6))\nsns.scatterplot(x = 'RAD', y = 'TAX', data = df)\nplt.show()","0555b528":"# removing the data corresponding to high tax rate\ndf1 = df[df['TAX'] < 600]\n# importing the required function\nfrom scipy.stats import pearsonr\n# calculating the correlation\nprint('The correlation between TAX and RAD is', pearsonr(df1['TAX'], df1['RAD'])[0])","ccd436c7":"# scatterplot to visualize the relationship between INDUS and TAX\nplt.figure(figsize=(6, 6))\nsns.scatterplot(x = 'INDUS', y = 'TAX', data = df)\nplt.show()","e74f78cf":"# scatterplot to visulaize the relationship between RM and MEDV\nplt.figure(figsize=(6, 6))\nsns.scatterplot(x = 'RM', y = 'MEDV', data = df)\nplt.show()","a336c9a2":"# scatterplot to visulaize the relationship between LSTAT and MEDV\nplt.figure(figsize=(6, 6))\nsns.scatterplot(x = 'LSTAT', y = 'MEDV', data = df)\nplt.show()","53f26180":"# separate the dependent and independent variable\nY = df['MEDV_log']\nX = df.drop(columns = {'MEDV', 'MEDV_log'})\n\n# add the intercept term\nX = sm.add_constant(X)","81c209a7":"# splitting the data in 70:30 ratio of train to test data\nX_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.30 , random_state=1)","cd8e508d":"from statsmodels.stats.outliers_influence import variance_inflation_factor\n\n# function to check VIF\ndef checking_vif(train):\n    vif = pd.DataFrame()\n    vif[\"feature\"] = train.columns\n\n    # calculating VIF for each feature\n    vif[\"VIF\"] = [\n        variance_inflation_factor(train.values, i) for i in range(len(train.columns))\n    ]\n    return vif\n\n\nprint(checking_vif(X_train))","d8ebd01b":"# creating the model after dropping TAX\nX_train = X_train.drop(columns = {'TAX'})\n\n# checking for VIF\nprint(checking_vif(X_train))","8ea65ee0":"# create the model\nmodel1 = sm.OLS(y_train, X_train).fit()\n\n\n# get the model summary\nmodel1.summary()","464425c9":"# creating the model after dropping columns 'MEDV', 'MEDV_log', 'TAX', 'ZN', 'AGE', 'B' 'INDUS' from df dataframe\nY = df['MEDV_log']\nX = df.drop(columns = {'MEDV', 'MEDV_log', 'TAX', 'ZN', 'AGE', 'B', 'INDUS'}) \nX = sm.add_constant(X)\n\n#splitting the data in 70:30 ratio of train to test data\nX_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.30 , random_state=1)\n\n# create the model\nmodel2 = sm.OLS(y_train, X_train).fit() #write your code here\n# get the model summary\nmodel2.summary()","8d2d3445":"residuals = model2.resid\n\nresiduals.mean()","11e648fe":"from statsmodels.stats.diagnostic import het_white\nfrom statsmodels.compat import lzip\nimport statsmodels.stats.api as sms","c856d412":"name = [\"F statistic\", \"p-value\"]\ntest = sms.het_goldfeldquandt(y_train, X_train)\nlzip(name, test)","bd1ce3d8":"# predicted values\nfitted = model2.fittedvalues\n\n# sns.set_style(\"whitegrid\")\nsns.residplot(x = fitted, y = residuals, color=\"lightblue\", lowess=True) #write your code here\nplt.xlabel(\"Fitted Values\")\nplt.ylabel(\"Residual\")\nplt.title(\"Residual PLOT\")\nplt.show()","7f9e7788":"# Plot histogram of residuals\nsns.histplot(residuals, kde = True)","7a53857a":"# Plot q-q plot of residuals\nimport pylab\nimport scipy.stats as stats\n\nstats.probplot(residuals, dist=\"norm\", plot=pylab)\nplt.show()","59ea6111":"# RMSE\ndef rmse(predictions, targets):\n    return np.sqrt(((targets - predictions) ** 2).mean())\n\n\n# MAPE\ndef mape(predictions, targets):\n    return np.mean(np.abs((targets - predictions)) \/ targets) * 100\n\n\n# MAE\ndef mae(predictions, targets):\n    return np.mean(np.abs((targets - predictions)))\n\n\n# Model Performance on test and train data\ndef model_pref(olsmodel, x_train, x_test):\n\n    # In-sample Prediction\n    y_pred_train = olsmodel.predict(x_train)\n    y_observed_train = y_train\n\n    # Prediction on test data\n    y_pred_test = olsmodel.predict(x_test)\n    y_observed_test = y_test\n\n    print(\n        pd.DataFrame(\n            {\n                \"Data\": [\"Train\", \"Test\"],\n                \"RMSE\": [\n                    rmse(y_pred_train, y_observed_train),\n                    rmse(y_pred_test, y_observed_test),\n                ],\n                \"MAE\": [\n                    mae(y_pred_train, y_observed_train),\n                    mae(y_pred_test, y_observed_test),\n                ],\n                \"MAPE\": [\n                    mape(y_pred_train, y_observed_train),\n                    mape(y_pred_test, y_observed_test),\n                ],\n            }\n        )\n    )\n\n\n# Checking model performance\nmodel_pref(model2, X_train, X_test)  ","8c5c94cf":"# import the required function\n\nfrom sklearn.model_selection import cross_val_score\n\n# build the regression model and cross-validate\nlinearregression = LinearRegression()                                    \n\ncv_Score11 = cross_val_score(linearregression, X_train, y_train, cv = 10)\ncv_Score12 = cross_val_score(linearregression, X_train, y_train, cv = 10, \n                             scoring = 'neg_mean_squared_error')                              \n\n\nprint(\"RSquared: %0.3f (+\/- %0.3f)\" % (cv_Score11.mean(), cv_Score11.std() * 2))\nprint(\"Mean Squared Error: %0.3f (+\/- %0.3f)\" % (-1*cv_Score12.mean(), cv_Score12.std() * 2))","14219ad8":"coef1 = pd.DataFrame({\"Features\":model2.params, \"Coefs\":np.transpose(model2.params.values)})\n\nEquation = \"log (Price) =\"\nprint(Equation, end='\\t')\nfor i in range(len(coef1)):\n    print('(', coef1['Features'][i], ') * ', coef1.index[i], '+', end = ' ')","76b38e02":"Next, we will check the multicollinearity in the train dataset.","2056e202":"**Observations:**\n\n* The price of the house indicated by the variable MEDV is the target variable and the rest are the independent variables based on which we will predict house price.","3d6dc9fa":"## Bivariate Analysis","c60ae26a":"**Observations:**\n\nThe above two graphs show that the residuals are normally distributed, therefore the assumption for normality is satisfied. ","513a421a":"Now, we will create the linear regression model as the VIF is less than 5 for all the independent variables, and we can assume that multicollinearity has been removed between the variables.","e081f47d":"#### Normality of error terms\nThe residuals should be normally distributed.","212cc4c6":"Let's check the correlation using a heatmap","91bb33fe":"**Observations:**\n* No trend between the two variables is visible in the above plot.","3e9b9af3":"**Observations:**\n\n* There is an average of around 6.2 rooms per dwelling across all the towns. \n\n* There is a very large variance for crime rate, as we can see that the lowest crime rate is 0.006 whereas the highest one is 88.97. It could be an outlier since it is very different from the median and even 75% quantile value. ","926b3b27":"**Observations:**\n* The distance of the houses to the Boston employment centers appears to decrease moderately as the the proportion of the old houses increase in the town. It is possible that the Boston employment centers are located in the established towns where proportion of owner-occupied units built prior to 1940 is comparatively high.","af5e5b45":"### Examining the significance of the model\n\nIt is not enough to fit a multiple regression model to the data, it is necessary to check whether all the regression coefficients are significant or not. Significance here means whether the population regression parameters are significantly different from zero. \n\nFrom the above it may be noted that the regression coefficients corresponding to ZN, AGE, B, and INDUS are not statistically significant at level \u03b1 = 0.05. In other words, the regression coefficients corresponding to these three are not significantly different from 0 in the population. Hence, we will eliminate the three features and create a new model.","4d71f583":"**Observations:**\n* There are two variables with a high VIF - RAD and TAX. Let's remove TAX as it has the highest VIF values and check the multicollinearity again.","61c4769d":"**Observations:**\n* The price of the house seems to increase as the value of RM increases. This is expected as the price is generally higher for more rooms.\n\n* There are a few outliers in a horizontal line as the MEDV value seems to be capped at 50.","da4e3fb5":"### Checking the below linear regression assumptions\n\n1. **Mean of residuals should be 0**\n2. **No Heteroscedasticity**\n3. **Linearity of variables**\n4. **Normality of error terms**","3d5a8f29":"**Conclusions** \n\n* First we did EDA on the data set and did initial visual analysis on the variables including both univariate and bivariate analysis. \n\n* We then checked for multicollinearity, and dropped the TAX column as a result. \n\n* Then we ran our linear regression model on the data and observed the results. Following that, we dropped variables that were not significant and examined the changes in the R-Squared variable. \n\n* Checked to make sure our model satisfies the assumptions of linear regression. \n\n* We then checked the performance of our model using various evaluation metrics including cross validation. \n\n* We then finalized our regression equation","8bb72f59":"We may want to reiterate the model building process again with new features or better feature engineering to increase the R-squared and decrease the MSE on cross validation.","49ff327f":"Now, we will check the linear regression assumptions.","da584a32":"**Observations**\n* The log-transformed variable (**MEDV_log**) appears to have a **nearly normal distribution without skew**, and hence we can proceed.","492dbead":"**Observations:**\n* We can see that the **R-squared value has decreased by 0.004**, since we have removed variables from the model, whereas the **adjusted R-squared value has decreased by 0.001.**","1150dd89":"**Observations:**\n\n* We see a significantly correlated relationship between NOX and INDUS. \n\n* NOX also has a high positive correlation value with Age, as well as a significantly negatively correlated value with DIS.\n\n* MEDV has a high correlation value with RM, and also significantly negatively correlated with LSTAT.\n\n* INDUS has a high correlation value with TAX","505464d8":"### Checking for Multicollinearity\n\nWe will use the Variance Inflation Factor (VIF), to check if there is multicollinearity in the data.\n\nFeatures having a VIF score > 5 will be dropped\/treated till all the features have a VIF score < 5","e3fd0baf":"## Importing the required libraries","5e4478e3":"### Splitting the Dataset\n\nLet's split the dataset into the dependent and independent variables and further split it into train and test set in a ratio of 70:30 for train and test set.","97a115a6":"----------------------------\n## Data Information\n---------------------------\n\nEach record in the database describes a Boston suburb or town. The data was drawn from the Boston Standard Metropolitan Statistical Area (SMSA) in 1970. Detailed attribute information can be found below-\n\nAttribute Information (in order):\n- **CRIM:**     per capita crime rate by town\n- **ZN:**       proportion of residential land zoned for lots over 25,000 sq.ft.\n- **INDUS:**    proportion of non-retail business acres per town\n- **CHAS:**     Charles River dummy variable (= 1 if tract bounds river; 0 otherwise)\n- **NOX:**      nitric oxides concentration (parts per 10 million)\n- **RM:**       average number of rooms per dwelling\n- **AGE:**     proportion of owner-occupied units built before 1940\n- **DIS:**      weighted distances to five Boston employment centers\n- **RAD:**      index of accessibility to radial highways\n- **TAX:**      full-value property-tax rate per 10,000 dollars\n- **PTRATIO:**  pupil-teacher ratio by town\n- **B - 1000(Bk - 0.63)^2** where Bk is the proportion of blacks by town\n- **LSTAT:**    %lower status of the population\n- **MEDV:**     Median value of owner-occupied homes in 1000 dollars","67825cba":"**Observations**\n- The R-squared on the cross validation is 0.729, whereas on the training dataset it was 0.769\n- And the MSE on cross validation is 0.041, whereas on the training dataset it was 0.038","c200a994":"### Checking the performance of the model on the train and test data set","ad65a2db":"### Let's get model coefficients in a pandas dataframe with column 'Feature' having all the features and column 'Coefs' with all the corresponding Coefs. We will then write the regression equation.","4437859c":"## Model Building","22ecc506":"Let's check the correlation after removing the outliers.","bff2c8fe":"#### Linearity of variables\n\nIt states that the predictor variables must have a linear relation with the dependent variable.\n\nTo test the assumption, we'll plot residuals and fitted values on a plot and ensure that residuals do not form a strong pattern. They should be randomly and uniformly scattered on the x-axis.","43f147f4":"### Applying cross validation to improve the model and evaluating it using different evaluation metrics","82679709":"**Observations:**\n* The price of the house tends to decrease with an increase in LSTAT. This is also possible as the house price is lower in areas where lower status people live.\n* There are few outliers and the data seems to be capped at 50.\n\nWe have seen that the variables LSTAT and RM have a linear relationship with the dependent variable MEDV. Also, there are significant relationships among a few independent variables, which is not desirable for a linear regression model. Let's first split the dataset.","62f0dd8d":"**Observations:**\n\n* We can see a negative, and somewhat curved relationship between nitric oxide concentration levels and weighted distances to five Boston employment centers. \n\n* Lower weighted distances seem to be associated with higher nitric oxide concentration levels, and higher weighted distances seem to be associated with lower nitric oxide concentration levels. ","32311370":"**Observations:**\n\n* The mean absolute percentage error is 4.98 on the training data and 5.26 on the test data\n\n* The mean absolute error is 0.14 on the training data and 0.15 on the test data. \n\n* The Root Mean Squared Error is 0.196 on the training data and 0.198 on the test data. We use this value to assess how accurately the model predicts the response.  ","b6290038":"Before creating the linear regression model, it is important to check the bivariate relationship between the variables. Let's check the same using the heatmap and scatterplot.","8aff66e5":"**Observations**\n* **The variables CRIM and ZN are positively skewed.** This suggests that most of the areas have lower crime rates and most residential plots are under the area of 25,000 sq. ft.\n* **The variable CHAS, with only 2 possible values 0 and 1, follows a binomial distribution**, and the majority of the houses are away from Charles river (CHAS = 0).\n* The distribution of the variable AGE suggests that many of the owner-occupied houses were built before 1940. \n* **The variable DIS** (average distances to five Boston employment centers) **has a nearly exponential distribution**, which indicates that most of the houses are closer to these employment centers.\n* **The variables TAX and RAD have a bimodal distribution.**, indicating that the tax rate is possibly higher for some properties which have a high index of accessibility to radial highways.  \n* The dependent variable MEDV seems to be slightly right skewed.","6320ea8e":"**Observations:**\n* The tax rate appears to increase with an increase in the proportion of non-retail business acres per town. This might be due to the reason that the variables TAX and INDUS are related with a third variable.","18381b3b":"**Observations:**\n\n* The R-Squared valued value is 0.771, and the adjusted R_Squared value is 0.763\n\n* The P-Values for CRIM, CHAS, NOX, RM, DIS, RAD, PTRATIO, and LSTAT all look statistically significant as they are either zero or close to it. ","42a80da2":"**Observations**\n* There are a total of 506 non-null observations in each of the columns. This indicates that there are no missing values in the data.\n\n* Every column in this dataset is numeric in nature.","4ac0c9df":"Now, we will visualize the relationship between the pairs of features having significant correlations.","6efba678":"**Checking for mean residuals**","04ae0cad":"**Observations:**\n* The correlation between RAD and TAX is very high. But, no trend is visible between the two variables. \nThis might be due to outliers. ","557b4afb":"**Observations:**\n\n* The above plot shows that there is no clear pattern, and that the points are randomly and uniformly scattered on the x axist, and therefore the linearity assumption is satisfied. ","6736a0bc":"#### Checking the distribution of the variables","7dbcd16d":"### Univariate Analysis","454a586d":"#### Checking for homoscedasticity\n\n* Homoscedasticity - If the residuals are symmetrically distributed across the regression line, then the data is said to homoscedastic.\n\n* Heteroscedasticity - If the residuals are not symmetrically distributed across the regression line, then the data is said to be heteroscedastic. In this case, the residuals can form a funnel shape or any other non-symmetrical shape.\n\n* We'll use `Goldfeldquandt Test` to test the following hypothesis with alpha = 0.05:\n\n    - Null hypothesis: Residuals are homoscedastic\n    - Alternate hypothesis: Residuals have heteroscedastic","e3f885c4":"### Reading the dataset","a2ee84e3":"**Observations:**\n\n* There seems to be a somewhat positive relationship between the nitric oxides concentration and the proportion of non-retail business acres per town. When the proportion gets higher, there also seems to be a higher concentration of nitric oxides. \n\n* There are a number of data points that are on a vertical line at some point between the 15 and 20 values for INDUS.  ","194a9c83":"**Observations:**\n\n* The above test shows that the p-value is greater than 0.05 which is not stastitically significant and therefore means that we fail to reject the null-hypothesis which says that residuals are homoscedastic, therefore- the assumption for no heteroscedasticity is satisfied. ","f74351b9":"**Observations:**\n\n* When the proportion of owner occupied units built before 1940 gets higher, there also seems to be a higher concentration of Nitric oxides, thus, we can see a positive relationship. It is not the strongest relationship however, as we can see a good number of higher end AGE data points that are associated with low nitric oxides concentrations. ","e4e6be10":"As the dependent variable is sightly skewed, we will apply a **log transformation on the 'MEDV' column** and check the distribution of the transformed column.","619e4814":"**Observations:**\n\n* The mean of the residuals is very close to 0, therefore, the assumption for residuals is satisfied","59bc5922":"---\n## Objective\n---\n\nThe problem on hand is to predict the housing prices of a town or a suburb based on the features of the locality provided to us. In the process, we need to identify the most important features in the dataset. We need to employ techniques of data preprocessing and build a linear regression model that predicts the prices for us. ","6e5eda77":"So the high correlation between TAX and RAD is due to the outliers. The tax rate for some properties might be higher due to some other reason.","9fe7716f":"Before performing the modeling, it is important to check the univariate distribution of the variables.","b091cead":"---\n# Boston Housing Prices Prediction: Linear Regression\n---"}}