{"cell_type":{"7656d37e":"code","6f5a8ad2":"code","4868904c":"code","1444a145":"code","ed63bbd4":"code","c084a115":"code","2a8ca26f":"code","1278c9dd":"code","073cab58":"code","ea19d734":"code","11d03694":"code","37e2e417":"code","46fda58f":"code","1e5b8253":"code","ee095b4c":"code","04df6ecc":"code","8183167b":"code","792e1cc4":"code","112e7044":"code","285ad906":"code","407c6d47":"code","1ccc9a07":"code","67950126":"code","3f697f33":"code","0301bf62":"code","4d5ecf52":"code","2dc41748":"code","9d62226c":"code","8c3ab1eb":"code","4c17e1a3":"code","8ae9900a":"code","c81c81ec":"code","d6ffaf33":"code","22e7121e":"code","5c05f8da":"code","8c5d234d":"code","1fae4c20":"markdown","aee59083":"markdown","e69755a3":"markdown","290fa0ac":"markdown","2acae85f":"markdown","f0c0e7c9":"markdown","c9d32e48":"markdown","40040100":"markdown","f5b5b783":"markdown","b5c3b156":"markdown","048c06fa":"markdown","e1a5b8bb":"markdown","1b511b09":"markdown","50f1bc3e":"markdown","e6929064":"markdown","013c75da":"markdown","68e71241":"markdown","7073e5f2":"markdown","43906e11":"markdown","f16f0f25":"markdown","4b174372":"markdown","2daa5d49":"markdown","178d3e81":"markdown","f0f0fdf0":"markdown","8cd5a96d":"markdown","59958c8d":"markdown","3a85288d":"markdown","0e977e17":"markdown","d3da1a47":"markdown","a194cd22":"markdown","1b0d2a60":"markdown"},"source":{"7656d37e":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session\n\nsns.set(palette='RdYlGn')","6f5a8ad2":"df = pd.read_csv('\/kaggle\/input\/kc-house-data\/kc_house_data.csv')\ndf.head()","4868904c":"df.info()","1444a145":"df.drop(columns=['id', 'date'], inplace=True)","ed63bbd4":"df.describe().T.drop(columns=['count'])","c084a115":"df.sort_values(by='price').tail(1)","2a8ca26f":"df.sort_values(by='yr_built').head(1)","1278c9dd":"num_vars = ['price', 'long', 'lat', 'sqft_living', 'sqft_lot', 'sqft_above', 'sqft_basement', 'sqft_living15', 'sqft_lot15', 'yr_built', 'yr_renovated', 'zipcode']\ncat_vars = ['bedrooms', 'bathrooms', 'floors', 'waterfront', 'view', 'condition', 'grade']","073cab58":"def plot_univariate_panel(vars_name, data, func_plot, n_cols=2):\n    \n    from math import ceil\n    \n    n_rows = ceil(len(vars_name) \/ n_cols)\n    \n    plt.figure(figsize=(7 * n_cols, 4 * n_rows))\n    for idx, var in enumerate(vars_name, 1):\n        plt.subplot(n_rows, n_cols, idx)\n        func_plot(data[var])","ea19d734":"plot_univariate_panel(num_vars, df, sns.boxplot, 3)","11d03694":"plot_univariate_panel(cat_vars, df, sns.countplot, 3)","37e2e417":"def plot_bivariate_panel(vars_name, var_ref, data, n_cols=3):\n\n    from math import ceil\n    \n    n_rows = ceil(len(vars_name) \/ n_cols)\n    \n    plt.figure(figsize=(7 * n_cols, 4 * n_rows))\n    for idx, var in enumerate(vars_name, 1):\n        \n        mean = df.groupby(by=var).mean()[[var_ref]].reset_index()\n        std = df.groupby(by=var).std()[var_ref].fillna(0)\n\n        plt.subplot(n_rows, n_cols, idx)        \n        sns.scatterplot(x=var, y=var_ref, data=mean)\n        plt.ylabel(f'Mean {var_ref}')               ","46fda58f":"plot_bivariate_panel(df.drop(columns=['price']).columns, 'price', df)","1e5b8253":"bbox = (\n    (df['long'].min(), df['long'].max(),\n    df['lat'].min(), df['lat'].max())\n)\nbbox","ee095b4c":"plt.figure(figsize=(15, 8))\nhouse_map = plt.imread('\/kaggle\/input\/mapcity\/map.png')\n\nplt.imshow(house_map, zorder=0, extent=bbox, aspect='equal')\nsns.scatterplot(\n    x='long', \n    y='lat', \n    data=df[df['price'] < 2e6], \n    hue='price',    \n    zorder=1, \n    edgecolor=None, \n    alpha=0.2,    \n    palette='hot'\n)","04df6ecc":"plt.figure(figsize=(15, 8))\nhouse_map = plt.imread('\/kaggle\/input\/mapcity\/map.png')\n\nplt.imshow(house_map, zorder=0, extent=bbox, aspect='equal')\nsns.scatterplot(\n    x='long', \n    y='lat', \n    data=df, \n    hue='yr_built',    \n    zorder=1, \n    edgecolor=None, \n    alpha=0.2,    \n    palette='hot'\n)","8183167b":"new_df = df.copy()\n\nnew_df['price'] = np.log(new_df['price'])\nnew_df['sqft_living'] = np.log(new_df['sqft_living'])\nnew_df['sqft_above'] = np.log(new_df['sqft_above'])\nnew_df['sqft_living15'] = np.log(new_df['sqft_living15'])","792e1cc4":"print(f'Missing values: {new_df.isnull().sum().sum()}')","112e7044":"def remove_outliers(data, var_names):\n    ans = data.copy()\n    for var in var_names:\n        var_info = ans[var].describe()        \n        iq_range = var_info['75%'] - var_info['25%']\n        intv_range = (var_info['25%'] - 1.5 * iq_range, var_info['75%'] + 1.5 * iq_range)\n        ans = ans[ans[var].between(*intv_range)]\n    return ans\n\nnew_df = remove_outliers(new_df, num_vars)","285ad906":"plot_univariate_panel(num_vars, new_df, sns.boxplot, 3)","407c6d47":"new_df.drop('zipcode', axis=1, inplace=True)","1ccc9a07":"new_df['yr_renovated'] = new_df[['yr_renovated', 'yr_built']].apply(lambda pair: pair[0] if pair[0] != 0 else pair[1], axis=1)","67950126":"def square_features(features, X, degree=2):\n    \n    X_cp = X.copy()\n    X_features = X.columns\n    \n    for feature in features:\n        if feature in X_features:\n            X_cp[feature + f'^{degree}'] = np.power(X_cp[feature], degree)\n    \n    return X_cp\n\nnew_df = square_features(num_vars[1:] + cat_vars, new_df)","3f697f33":"cat = new_df.groupby('grade')['yr_built']\nnew_df['yr_built_dev_grade'] = cat.transform(lambda x: (x - x.mean()) \/ x.std())","0301bf62":"from sklearn.ensemble import RandomForestRegressor\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MinMaxScaler\nfrom tensorflow import keras\nfrom sklearn.metrics import mean_absolute_error, r2_score, explained_variance_score\n\n\nX = new_df.drop(columns=['price'])\ny = new_df['price']\ny = np.power(np.exp(1), y)\n\n# Splitting the dataset into train and test data\nX_train, X_test, y_train, y_test = train_test_split(X, y)","4d5ecf52":"scaler = MinMaxScaler().fit(X_train)\n\nX_train = scaler.transform(X_train)\nX_test = scaler.transform(X_test)","2dc41748":"model = keras.Sequential()\n\nn_units = X.shape[1]\nn_layers = 8\n\nfor _ in range(n_layers):\n    model.add(keras.layers.Dense(units=n_units, activation='relu'))\n\nmodel.add(keras.layers.Dense(units=1))\n\nmodel.compile(optimizer='adam', loss='mse')","9d62226c":"hist = model.fit(\n    x=X_train, \n    y=y_train,    \n    verbose=False,\n    validation_data=(X_test, y_test),\n    batch_size=128,\n    epochs=500\n)","8c3ab1eb":"y_train_pred = model.predict(x=X_train)\ny_test_pred = model.predict(x=X_test)","4c17e1a3":"print(f'MAE train set: {mean_absolute_error(y_train_pred, y_train):.2}')\nprint(f'MAE test set: {mean_absolute_error(y_test_pred, y_test):.2}')","8ae9900a":"loss_df = pd.DataFrame(hist.history)","c81c81ec":"plt.figure(figsize=(15, 6))\nx, y = range(1, len(loss_df['loss']) + 1), loss_df['loss']\nstart = 2\nplt.plot(x[start:], y[start:], label='Training Loss')\nx, y = range(1, len(loss_df['val_loss']) + 1), loss_df['val_loss']\nplt.plot(x[start:], y[start:], label='Validation Loss')\nplt.xlabel('Epochs')\nplt.ylabel('MSE')\nplt.legend()","d6ffaf33":"y_test_pred = model.predict(X_test)\n\ncomp_df = pd.DataFrame(\n    data={\n        'Test True Y': y_test,\n        'Test Pred Y': y_test_pred.reshape((-1,))\n    }\n)\n\ncomp_df.describe().T","22e7121e":"plt.figure(figsize=(15, 6))\n\nplt.subplot(1,2,1)\nplt.scatter(\n    x=y_train, \n    y=y_train_pred, \n    alpha=0.3, \n    edgecolor=None\n)\nplt.plot(\n    [y_train.min(), y_train.max()],\n    [y_train.min(), y_train.max()],\n    color='black', \n    linestyle='dashed', \n    label=f'Desired\\nR\u00b2 train set: {r2_score(y_train, y_train_pred):.2}'\n)\nplt.legend()\nplt.xlabel('True value')\nplt.ylabel('Predicted value')\n\n\nplt.subplot(1,2,2)\nplt.scatter(\n    x=y_test, \n    y=y_test_pred, \n    alpha=0.3, \n    edgecolor=None\n)\nplt.plot(\n    [y_test.min(), y_test.max()],\n    [y_test.min(), y_test.max()],\n    color='black', \n    linestyle='dashed', \n    label=f'Desired\\nR\u00b2 test set: {r2_score(y_test, y_test_pred):.2}'\n)\nplt.legend()\nplt.xlabel('True value')\nplt.ylabel('Predicted value')","5c05f8da":"def feature_importance(model, metric, X, y, features_name, shuffles_per_column=10):\n    \n    n_rows, n_cols = X.shape\n    ref_score = metric(y, model.predict(X)) # Reference score    \n    mean_list, std_list = [], []\n    X_cp = X.copy()\n    \n    for c in range(n_cols):\n        metric_list = []\n        for _ in range(shuffles_per_column):\n            np.random.shuffle(X_cp[:, c])\n            y_pred = model.predict(X_cp)\n            score = metric(y, y_pred)\n            metric_list.append(ref_score - score)\n            X_cp[:, c] = X[:, c]        \n        mean_list.append(np.mean(metric_list))\n        std_list.append(np.std(metric_list))\n    \n    importance_rel = [np.round(100*np.mean(mean)\/np.sum(mean_list), 2) for mean in mean_list]\n            \n    return pd.DataFrame({\n        'Feature': features_name,\n        'Importance (%)': importance_rel,\n        'Mean': mean_list,\n        'Std': std_list\n    }).sort_values('Importance (%)', ascending=False).reset_index(drop=True)\n\nans = feature_importance(model, r2_score, X_test, y_test, X.columns)","8c5d234d":"plt.figure(figsize=(15, 8))\nsns.barplot(x='Importance (%)', y='Feature', data=ans)","1fae4c20":"A house built in 1900, which has never been renovated. It must probably be falling apart.","aee59083":"<h3><a target=\"_self\" href=\"#index-model\" id=\"model\" style=\"color: black;\">Model - Deep Neural Network<\/a><\/h3>\n\nTo model this problem, I'll use a deep neural network with eight layers and 36 units per layer.","e69755a3":"<h3><a target=\"_self\" href=\"#index-nan-value\" id=\"nan-value\" style=\"color: black;\">Missing value treatment<\/a><\/h3>\n\nFortunately, there are no missing values.","290fa0ac":"<h3><a target=\"_self\" href=\"#index-uni-analysis\" id=\"uni-analysis\" style=\"color: black;\">Non-Graphical Univariate Analysis<\/a><\/h3>","2acae85f":"The second thing I'm going to do is remap the year renovated variable, because some houses weren't renovated since it was built. My idea is to replace the zero values by the year that the house was built.","f0c0e7c9":"<h3><a target=\"_self\" href=\"#index-var-trans\" id=\"var-trans\" style=\"color: black;\">Variable transformations<\/a><\/h3>\n\nSome variables are almost normal distributed, right-skewed, and we can fix it taking the logarithm of each value.","c9d32e48":"In the boxplots above, we can see that there are still outliers, but I will not remove them because we may lose information.","40040100":"As we would expect, the localization of the house is very important to predict the price, that's why latitude and longitude is on the top of importance. Second, the area available. Third, the year that the house was built and so on.","f5b5b783":"Again, the oldest houses are near Lake Washington, on the side of Seattle city.","b5c3b156":"As we can see in the learning curves above, the model is not over-dimensioning or under-adjusting the data. It has achived a good balance. Let's compare the some statistical information about the real and predicted price.","048c06fa":"<h3><a target=\"_self\" href=\"#index-obj\" id=\"obj\" style=\"color: black;\">Objective<\/a><\/h3>","e1a5b8bb":"<h3><a target=\"_self\" href=\"#index-uni-analysis2\" id=\"uni-analysis2\" style=\"color: black;\">Graphical Univariate Analysis<\/a><\/h3>\n\nLet's check the behaviour of our features.","1b511b09":"<h3><a target=\"_self\" href=\"#index-feat-eng\" id=\"feat-eng\" style=\"color: black;\">Feature Engineering<\/a><\/h3>\n\nAt that point, we could go ahead and build our model, but we will try to build more resources to capture more information.\n\nThe first thing I wanto to do is exclude the zipcode variable, because it will give me an extra work figuring out how to use this feature properly, and since we have the geographic coordinates, propably those will give us a more accurate information.","50f1bc3e":"I'm going to delete the id and date columns, since they will not help us too much in our model.","e6929064":"In the boxplots above, we can see many outliers and the data distributions are distorted. We will deal with these problems later. Next, let's take a look on the discrete features.","013c75da":"<h3><a target=\"_self\" href=\"#index-feat-imp\" id=\"feat-imp\" style=\"color: black;\">Feature Importance<\/a><\/h3>\n\nNow let's get an idea which features are more important to our model.","68e71241":"What are the characteristics of the oldest house?","7073e5f2":"What are the characteristics of the most expensive house?","43906e11":"<h3><a target=\"_self\" href=\"#index-outlier\" id=\"outlier\" style=\"color: black;\">Outlier treatment<\/a><\/h3>\n\nAs we saw on the past graphs, many variables have outliers. I'm going to use Tukey's fences method to remove them.","f16f0f25":"The third thing to do is to create square features.","4b174372":"From the graphs above, the model is doing a pretty decent job. The R\u00b2 coefficient in the training set is about 0.9 and 0.88 in the test set.","2daa5d49":"# King County House Data\n\n\n\n\nThe dataset consists of house prices from King County an area in the US State of Washington, this data also covers Seattle. The <a target=\"_new\" href=\"https:\/\/www.kaggle.com\/shivachandel\/kc-house-data\">dataset<\/a> was obtained from <a target=\"_new\" href=\"https:\/\/www.kaggle.com\/shivachandel\/kc-house-data\">Kaggle<\/a>.\n\n<ol>\n    <li><a target=\"_self\" href=\"#obj\" id=\"index-obj\" style=\"color: black;\">Objective<\/a><\/li>\n    <li><a target=\"_self\" href=\"#load-data\" id=\"index-load-data\" style=\"color: black;\">Load Data<\/a><\/li>\n    <li><a target=\"_self\" href=\"#var-types\" id=\"index-var-types\" style=\"color: black;\">Identification of variables and data types<\/a><\/li>\n    <li><a target=\"_self\" href=\"#metrics\" id=\"index-metrics\" style=\"color: black;\">Analyzing the basic metrics<\/a><\/li>\n    <li><a target=\"_self\" href=\"#uni-analysis\" id=\"index-uni-analysis\" style=\"color: black;\">Non-Graphical Univariate Analysis<\/a><\/li>\n    <li><a target=\"_self\" href=\"#uni-analysis2\" id=\"index-uni-analysis2\" style=\"color: black;\">Graphical Univariate Analysis<\/a><\/li> \n    <li><a target=\"_self\" href=\"#bi-analysis\" id=\"index-bi-analysis\" style=\"color: black;\">Graphical Bivariate Analysis<\/a><\/li>\n    <li><a target=\"_self\" href=\"#var-trans\" id=\"index-var-trans\" style=\"color: black;\">Variable transformations<\/a><\/li>\n    <li><a target=\"_self\" href=\"#nan-value\" id=\"index-nan-value\" style=\"color: black;\">Missing value treatment<\/a><\/li>\n    <li>\n        <a target=\"_self\" href=\"#outlier\" id=\"index-outlier\" style=\"color: black;\">Outlier treatment<\/a>\n    <\/li>\n    <li>\n        <a target=\"_self\" href=\"#feat-eng\" id=\"index-feat-eng\" style=\"color: black;\">Feature Engineering<\/a>\n    <\/li>\n    <li>\n        <a target=\"_self\" href=\"#model\" id=\"index-model\" style=\"color: black;\">Model - Deep Neural Network<\/a>\n    <\/li>     \n    <li>\n        <a target=\"_self\" href=\"#feat-imp\" id=\"index-feat-imp\" style=\"color: black;\">Feature Importance<\/a>\n    <\/li>    \n<\/ol>\n","178d3e81":"<h3><a target=\"_self\" href=\"#index-bi-analysis\" id=\"bi-analysis\" style=\"color: black;\">Graphical Bivariate Analysis<\/a><\/h3>\n\nNow let's take a look at how each feature behaves compared to the price.","f0f0fdf0":"<h3><a target=\"_self\" href=\"#index-var-types\" id=\"var-types\" style=\"color: black;\">Identification of variables and data types<\/a><\/h3>","8cd5a96d":"According to the map, the most expensive houses are near Lake Washington, and it is very concentrated on Mercer Island. The next map shows the distribution of houses according to their age.","59958c8d":"Apparently, all features seem to have some relationship with the price. Next, let's see how price is distributed along geographical coordinates.","3a85288d":"Wooow! A house of 7.7 millions dollars?! I should have made an offer! :)","0e977e17":"The objective here is to build a model to predict the price of the houses, and check which features play an important role to the model.","d3da1a47":"<h3><a target=\"_self\" href=\"#index-metrics\" id=\"metrics\" style=\"color: black;\">Analyzing the basic metrics<\/a><\/h3>","a194cd22":"<h3><a target=\"_self\" href=\"#index-load-data\" id=\"load-data\" style=\"color: black;\">Load Data<\/a><\/h3>","1b0d2a60":"The fourth and final thing to do is to create a deviation variable to capture how the year the house was built fits into your average grade."}}