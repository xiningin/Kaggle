{"cell_type":{"e50d81ba":"code","61f7bb13":"code","08a607e1":"code","c370f6fe":"code","ce08bcc2":"code","7f99600c":"code","61992b27":"code","dd8b31ee":"code","1ccea2b2":"code","1a9e7528":"code","0bc40b79":"code","a9fd9973":"code","1edec983":"code","6a2e0f2c":"code","f39108ca":"code","dae99c45":"code","fe156999":"code","b190e172":"code","de76572d":"code","01f3aed9":"code","3327ce90":"code","65713a09":"code","8e7e15f7":"code","7e8dcb36":"code","1b49e75d":"code","c4496c0d":"code","2a876e5f":"code","d766e4ed":"code","cdd6d2cb":"code","861c6cce":"code","31163cfe":"code","fd3b77a6":"code","e1574cb8":"markdown","dc61e388":"markdown","21bf13d3":"markdown","ec8a494f":"markdown","b7dd3811":"markdown","a8dd2ce2":"markdown","433570f3":"markdown","0d7916ec":"markdown","c27fcea4":"markdown","68eacc68":"markdown","30446d75":"markdown","191bb5eb":"markdown","d387640d":"markdown","48257130":"markdown","6468e856":"markdown","9d17d454":"markdown","f99747fc":"markdown","b4bf7641":"markdown","a59dcf57":"markdown","4a1f41bc":"markdown"},"source":{"e50d81ba":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom scipy import stats\nfrom scipy.stats import chisquare\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import accuracy_score,precision_score,recall_score,f1_score,roc_auc_score\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.linear_model import LogisticRegression\nfrom xgboost import XGBClassifier\nfrom sklearn.neural_network import MLPClassifier\n\n\nimport imblearn\nfrom imblearn.over_sampling import RandomOverSampler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\nfrom imblearn.over_sampling import SMOTE\nfrom imblearn.pipeline import Pipeline\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nfrom sklearn.feature_selection import chi2\nfrom sklearn.metrics.cluster import normalized_mutual_info_score\nfrom sklearn.ensemble import RandomForestClassifier\nimport os\n","61f7bb13":"#Reading teh data from kaggle local storage\ndf = pd.read_csv(\"\/kaggle\/input\/diabetes-health-indicators-dataset\/diabetes_binary_health_indicators_BRFSS2015.csv\")\nprint(df.columns)\nprint(df.info())","08a607e1":"#Predicot Columns and teh Target columns bifurcation\ntarget = 'Diabetes_binary'\nfeats = [col for col in df.columns if col!=target]\n\nX = df[feats]\n\ny=df[target]","c370f6fe":"#Number of unique values in each column! All the columns expect BMI are discrete whereas BMI is an approximation\ncol_types = {}\nfor col in df.columns:\n    if(col != target):\n        col_types[col] = df.groupby([col]).count()[target].shape[0]\nprint(col_types)","ce08bcc2":"fig,ax=plt.subplots(11,2,figsize = (30,50))\ni=0\nrow = 0\ncol = 0\nfor col in df.columns:\n    if(col!=target):\n        \n        r = int(i\/2)\n        c = i % 2\n        if(col_types[col]>2):\n            sns.scatterplot(df[target],df[col],ax=ax[r][c])\n        if(col_types[col]<=2):\n            sns.barplot(df[target],df[col],ax=ax[r][c])\n        i=i+1\n            \nplt.show()\n            \nprint('The disparity in the heights of bars indicate the dependence between of the target and predictor variables.Higher the disparity higher the importance of the feature') ","7f99600c":"# HighCol, Stroke,HeartDiseaseorAttack seem to be dependent intuitively\n#Chi-Square test\ndef chi2_independence(col1,col2):\n    vals = X.groupby([col1,col2]).count()['BMI'].values\n    X.groupby([col1,col2]).count()['BMI']\n    col1_p0 = X[X[col1]==0].shape[0]\/X.shape[0]\n    col2_p0 = X[X[col2]==0].shape[0]\/X.shape[0]\n    exp =  [X.shape[0] *col1_p0*col2_p0,X.shape[0] *col1_p0*(1-col2_p0),X.shape[0] *(1-col1_p0)*col2_p0,X.shape[0] *(1-col1_p0)*(1-col2_p0)]\n    return stats.chisquare(vals,exp).pvalue\n\np1 = chi2_independence('Stroke','HeartDiseaseorAttack')\nprint(p1)\np1 = chi2_independence('Stroke','HighChol')\nprint(p1)\np1 = chi2_independence('HighBP','HighChol')\nprint(p1)\n","61992b27":"#Mutual Information Score\nprint(normalized_mutual_info_score(X['Stroke'],X['HeartDiseaseorAttack'] ))\nprint(normalized_mutual_info_score(X['Stroke'],X['HighChol'] ))\nprint(normalized_mutual_info_score(X['HighBP'],X['HighChol'] ))\nprint(normalized_mutual_info_score(X['PhysActivity'],X['HighChol'] ))\nprint(normalized_mutual_info_score(X['PhysActivity'],X['HighChol'] ))","dd8b31ee":"\nprint(X.corr().unstack().sort_values().drop_duplicates().head(5))\nprint(X.corr().unstack().sort_values(ascending=False).drop_duplicates().head(5))\nprint('Colums are not heavily linearly corelated. So cant use this to filter out any!')","1ccea2b2":"#Prepping the data for hypothesis testing - Making them all booleans\nht_columns = ['Age','GenHlth','PhysActivity','BMI','Smoker']\nht = X[ht_columns]\n\nht['Age'] = (ht['Age']>4).astype('int')\nht['GenHlth'] = (ht['GenHlth']<3).astype('int')\nht['BMI'] = ((ht['BMI']<=25) & (ht['BMI']>=18)).astype('int')\nht['Sum']=ht.sum(axis=1)\nht['target']= y\nht.head()","1a9e7528":"\n#\"Sum\" column being 0 represents that we are in the \"safe-zone\" with respect to all the features picked by CDC\nht['safe_feat'] = (ht['Sum']<1).astype('int')\nfreq_obs = [ht[(ht['safe_feat']==1) & (ht['target']==1)].shape[0],ht[(ht['safe_feat']==1) & (ht['target']==0)].shape[0]]\n#Because the expected is that \"safe-zone\" is free of diabetes\n#Instead of using \"0\" in the expected as formulas might have issues we take it as 10\nfreq_exp = [10,sum(freq_obs)-10]\n\nchisquare(freq_obs).pvalue","0bc40b79":"print(freq_obs)","a9fd9973":"\nclf_young = DecisionTreeClassifier(random_state=0)\nclf_old = DecisionTreeClassifier(random_state=0)\ncol_name = 'Age'\ncur_feats = [col for col in feats if col!=col_name]\n#Data split to build two different models\nX_young = X[X[col_name]<=4][cur_feats]\ny_young = y[X[col_name]<=4]\nX_old = X[X[col_name]>4][cur_feats]\ny_old = y[X[col_name]>4]\n#Data Frame with feature importance results\nmodel_diff=pd.DataFrame()\n\nfig,ax=plt.subplots(1,2,figsize=(30,10))\n#Model for younger\nclf_young.fit(X_young,y_young)\nfeat_imp_young=pd.Series(clf_young.feature_importances_)\nmodel_diff['young'] = feat_imp_young\nfeat_imp_young.index=cur_feats\nfeat_imp_young = feat_imp_young.sort_values(ascending=False)\n#Plotting feature importances\nsns.barplot(y=feat_imp_young.index,x=feat_imp_young,ax=ax[0])\n\n#Model for older\nclf_old.fit(X_old,y_old)\nfeat_imp_old=pd.Series(clf_old.feature_importances_)\nmodel_diff['old'] = feat_imp_old\nfeat_imp_old.index=cur_feats\nfeat_imp_old = feat_imp_old.sort_values(ascending=False)\n#plotting feat importances\nsns.barplot(y=feat_imp_old.index,x=feat_imp_old,ax=ax[1])\n\nmodel_diff.index=cur_feats","1edec983":"\nstats.ttest_rel(model_diff['young'], model_diff['old'])","6a2e0f2c":"\n\nclf_male = DecisionTreeClassifier(random_state=0)\nclf_female = DecisionTreeClassifier(random_state=0)\n\n#Data split to build two different models\ncol_name = 'Sex'\ncur_feats = [col for col in feats if col!=col_name]\nX_female = X[X[col_name]==0][cur_feats]\ny_female = y[X[col_name]==0]\nX_male = X[X[col_name]==1][cur_feats]\ny_male = y[X[col_name]==1]\n#Data Frame with feature importance results\nmodel_diff=pd.DataFrame()\n\nfig,ax=plt.subplots(1,2,figsize=(30,10))\n#Model for female\nclf_female.fit(X_female,y_female)\nfeat_imp_female=pd.Series(clf_female.feature_importances_)\nmodel_diff['female'] = feat_imp_female\nfeat_imp_female.index=cur_feats\nfeat_imp_female = feat_imp_female.sort_values(ascending=False)\n#Plotting feature importances\nsns.barplot(y=feat_imp_female.index,x=feat_imp_female,ax=ax[0])\n\n#Model for male\nclf_male.fit(X_male,y_male)\nfeat_imp_male=pd.Series(clf_male.feature_importances_)\nmodel_diff['male'] = feat_imp_male\nfeat_imp_male.index=cur_feats\nfeat_imp_male = feat_imp_male.sort_values(ascending=False)\n#plotting feat importances\nsns.barplot(y=feat_imp_male.index,x=feat_imp_male,ax=ax[1])\n\nmodel_diff.index=cur_feats","f39108ca":"\nstats.ttest_rel(model_diff['female'], model_diff['male'])","dae99c45":"\n\nclf_male = DecisionTreeClassifier(random_state=0)\nclf_female = DecisionTreeClassifier(random_state=0)\n\n#Data split to build two different models\ncol_name = 'Smoker'\ncur_feats = [col for col in feats if col!=col_name]\nX_female = X[X[col_name]==0][cur_feats]\ny_female = y[X[col_name]==0]\nX_male = X[X[col_name]==1][cur_feats]\ny_male = y[X[col_name]==1]\n#Data Frame with feature importance results\nmodel_diff=pd.DataFrame()\n\nfig,ax=plt.subplots(1,2,figsize=(30,10))\n#Model for female\nclf_female.fit(X_female,y_female)\nfeat_imp_female=pd.Series(clf_female.feature_importances_)\nmodel_diff['non_smoker'] = feat_imp_female\nfeat_imp_female.index=cur_feats\nfeat_imp_female = feat_imp_female.sort_values(ascending=False)\n#Plotting feature importances\nsns.barplot(y=feat_imp_female.index,x=feat_imp_female,ax=ax[0])\n\n#Model for male\nclf_male.fit(X_male,y_male)\nfeat_imp_male=pd.Series(clf_male.feature_importances_)\nmodel_diff['smoker'] = feat_imp_male\nfeat_imp_male.index=cur_feats\nfeat_imp_male = feat_imp_male.sort_values(ascending=False)\n#plotting feat importances\nsns.barplot(y=feat_imp_male.index,x=feat_imp_male,ax=ax[1])\n\nmodel_diff.index=cur_feats\n\nstats.ttest_rel(model_diff['non_smoker'], model_diff['smoker'])","fe156999":"X.groupby(['NoDocbcCost']).count()","b190e172":"\n\nclf_male = DecisionTreeClassifier(random_state=0)\nclf_female = DecisionTreeClassifier(random_state=0)\n\n#Data split to build two different models\ncol_name = 'NoDocbcCost'\ncur_feats = [col for col in feats if col!=col_name]\nX_female = X[X[col_name]==0][cur_feats]\ny_female = y[X[col_name]==0]\nX_male = X[X[col_name]==1][cur_feats]\ny_male = y[X[col_name]==1]\n#Data Frame with feature importance results\nmodel_diff=pd.DataFrame()\n\nfig,ax=plt.subplots(1,2,figsize=(30,10))\n#Model for female\nclf_female.fit(X_female,y_female)\nfeat_imp_female=pd.Series(clf_female.feature_importances_)\nmodel_diff['costProb'] = feat_imp_female\nfeat_imp_female.index=cur_feats\nfeat_imp_female = feat_imp_female.sort_values(ascending=False)\n#Plotting feature importances\nsns.barplot(y=feat_imp_female.index,x=feat_imp_female,ax=ax[0])\n\n#Model for male\nclf_male.fit(X_male,y_male)\nfeat_imp_male=pd.Series(clf_male.feature_importances_)\nmodel_diff['noCostProb'] = feat_imp_male\nfeat_imp_male.index=cur_feats\nfeat_imp_male = feat_imp_male.sort_values(ascending=False)\n#plotting feat importances\nsns.barplot(y=feat_imp_male.index,x=feat_imp_male,ax=ax[1])\n\nmodel_diff.index=cur_feats\n\nstats.ttest_rel(model_diff['costProb'], model_diff['noCostProb'])","de76572d":"#Split into Train and Test\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n# Split train into train and CV\nX_train, X_cv, y_train, y_cv = train_test_split(X_train, y_train, test_size=0.33, random_state=42)\n#Oversample data\noversample = RandomOverSampler(sampling_strategy=0.5)\nX_over, y_over = oversample.fit_resample(X_train, y_train)","01f3aed9":"#Scaling Data - Standard Sclaer before PCA\nscaler = StandardScaler()\nscaler.fit(X_train)\nX_train_std = scaler.transform(X_train)\nX_test_std = scaler.transform(X_test)\n\n#Performing PCA where we just pick the features that cover 85% of the variance\npca = PCA(n_components=0.85)\npca.fit(X_train_std)\nlen(pca.components_)\nX_train_std_pca = pca.transform(X_train_std)\nX_test_std_pca = pca.transform(X_test_std)\n\nX_over_pca, y_over_pca = oversample.fit_resample(X_train_std_pca, y_train)","3327ce90":"feat_imp=np.zeros(len(feats))\n\ncnt=0\nfor n in range(50,200,50):\n#     for d in range(5,20,5):\n            for m in range(40,60,10):\n                    for m2 in range(10,30,10):\n                        clf = RandomForestClassifier(n_estimators = n,min_samples_split=m,min_samples_leaf=m2)\n                        clf.fit(X_over,y_over)\n                        print(f'Esimators : {n}, min_samples_split : {m},min_samples_leaf: {m2}')\n                        print(f'CV Score - {f1_score(clf.predict(X_cv),y_cv,average=\"weighted\")}')\n                        print(f'Train Score - {f1_score(clf.predict(X_train),y_train,average=\"weighted\")}')\n                        feat_imp = feat_imp+clf.feature_importances_\n                        cnt=cnt+1\nfeat_imp = feat_imp\/cnt\nfeat_imp=pd.Series(feat_imp)\n\n#plotting feat importances\nfeat_imp.index=feats\nfeat_imp = feat_imp.sort_values(ascending=False)\nsns.barplot(y=feat_imp.index,x=feat_imp)\n\n","65713a09":"A = X.copy()\nA['Food'] = A['Fruits']+A['Veggies']\nA.drop(columns=['Fruits','Veggies'],inplace=True)\nA['lifeStyle'] = A['Smoker']+A['HvyAlcoholConsump']\nA.drop(columns=['Smoker','HvyAlcoholConsump'],inplace=True)\nA.drop(columns=['CholCheck','NoDocbcCost','AnyHealthcare'],inplace=True)\n\n\nA_train,A_test,Ay_train,Ay_test = train_test_split(A,y,test_size=0.33,random_state=0)\nA_over, Ay_over = oversample.fit_resample(A_train, Ay_train)","8e7e15f7":"from sklearn.dummy import DummyClassifier\ndummy_clf = DummyClassifier(strategy=\"most_frequent\")\ndummy_clf.fit(X_train, y_train)\n\nprint(f'Test Score - {f1_score(dummy_clf.predict(X_test),y_test)}')\nprint(f'Train Score - {f1_score(dummy_clf.predict(X_train),y_train)}')\n\nprint(f'Test Score - {accuracy_score(dummy_clf.predict(X_test),y_test)}')\nprint(f'Train Score - {accuracy_score(dummy_clf.predict(X_train),y_train)}')\n\nprint(f'Test Score - {precision_score(dummy_clf.predict(X_test),y_test)}')\nprint(f'Train Score - {precision_score(dummy_clf.predict(X_train),y_train)}')\nprint(f'Test Score - {recall_score(dummy_clf.predict(X_test),y_test)}')\nprint(f'Train Score - {recall_score(dummy_clf.predict(X_train),y_train)}')","7e8dcb36":"print('Logistic Regression Classification')\nprint('--------------------------------------------------------------')\n#Decision Tree as a model slightly better than baseline\nprint('Imbalanced')\nclf = LogisticRegression(solver='liblinear')\nclf.fit(X_train,y_train)\nprint(f'Test Score - {f1_score(clf.predict(X_test),y_test)}')\nprint(f'Train Score - {f1_score(clf.predict(X_train),y_train)}')\n\n\n\n#Comparing Decision Tree performance on imbalanced data vs Oversampled data\nprint('Oversampled')\nclf = LogisticRegression(solver='liblinear')\nclf.fit(X_over,y_over)\nprint(f'Test Score - {f1_score(clf.predict(X_test),y_test)}')\nprint(f'Train Score - {f1_score(clf.predict(X_train),y_train)}')\n\nprint('PCA and oversampled')\nclf = LogisticRegression(solver='liblinear')\nclf.fit(X_over_pca,y_over_pca)\nprint(f'Test Score - {f1_score(clf.predict(X_test_std_pca),y_test)}')\nprint(f'Train Score - {f1_score(clf.predict(X_train_std_pca),y_train)}')\n\nprint('Feature Selection')\nclf.fit(A_over,Ay_over)\nprint(f'Test Score - {f1_score(clf.predict(A_test),Ay_test)}')\nprint(f'Train Score - {f1_score(clf.predict(A_over),Ay_over)}')","1b49e75d":"print('DecisionTree Classification')\nprint('--------------------------------------------------------------')\n\n#Decision Tree as a model slightly better than baseline\nprint('Imbalanced')\nclf = DecisionTreeClassifier()\nclf.fit(X_train,y_train)\nprint(f'Test Score - {f1_score(clf.predict(X_test),y_test)}')\nprint(f'Train Score - {f1_score(clf.predict(X_train),y_train)}')\n\n#Comparing Decision Tree performance on imbalanced data vs Oversampled data\nprint('Oversampled')\nclf = DecisionTreeClassifier()\nclf.fit(X_over,y_over)\nprint(f'Test Score - {f1_score(clf.predict(X_test),y_test)}')\nprint(f'Train Score - {f1_score(clf.predict(X_train),y_train)}')\n\nprint('PCA and oversampled')\nclf.fit(X_over_pca,y_over_pca)\nprint(f'Test Score - {f1_score(clf.predict(X_test_std_pca),y_test)}')\nprint(f'Train Score - {f1_score(clf.predict(X_train_std_pca),y_train)}')\n\nprint('Feature Selection')\nclf.fit(A_over,Ay_over)\nprint(f'Test Score - {f1_score(clf.predict(A_test),Ay_test)}')\nprint(f'Train Score - {f1_score(clf.predict(A_over),Ay_over)}')","c4496c0d":"print('RandomForestClassification')\nprint('--------------------------------------------------------------')\n\n#Is Random Forest in-general performing better\nprint('Imbalanced')\nclf = RandomForestClassifier()\nclf.fit(X_train,y_train)\nprint(f'Test Score - {f1_score(clf.predict(X_test),y_test)}')\nprint(f'Train Score - {f1_score(clf.predict(X_train),y_train)}')\n\n#Is Random Forest in-general performing better\nprint('Oversampled')\nclf = RandomForestClassifier()\nclf.fit(X_over,y_over)\nprint(f'Test Score - {f1_score(clf.predict(X_test),y_test)}')\nprint(f'Train Score - {f1_score(clf.predict(X_train),y_train)}')\n\nprint('PCA and oversampled')\nclf.fit(X_over_pca,y_over_pca)\nprint(f'Test Score - {f1_score(clf.predict(X_test_std_pca),y_test)}')\nprint(f'Train Score - {f1_score(clf.predict(X_train_std_pca),y_train)}')\n\nprint('Feature Selection')\nclf.fit(A_over,Ay_over)\nprint(f'Test Score - {f1_score(clf.predict(A_test),Ay_test)}')\nprint(f'Train Score - {f1_score(clf.predict(A_over),Ay_over)}')","2a876e5f":"print('XGBoost Classifier')\nprint('--------------------------------------------------------------')\n\n#Is Random Forest in-general performing better\nprint('Imbalanced')\nmodel = XGBClassifier(eval_metric='auc',scale_pos_weight = (y_over.shape[0] \/ y.sum())-1)\nmodel.fit(X_over, y_over)\nprint(f'Test Score - {f1_score(model.predict(X_test),y_test)}')\nprint(f'Train Score - {f1_score(model.predict(X_train),y_train)}')\n\nprint('Oversampled')\nmodel = XGBClassifier(eval_metric='auc',scale_pos_weight = (y_over.shape[0] \/ y.sum())-1)\nmodel.fit(X_train, y_train)\nprint(f'Test Score - {f1_score(model.predict(X_test),y_test)}')\nprint(f'Train Score - {f1_score(model.predict(X_train),y_train)}')\n\nprint('PCA and oversampled')\nmodel.fit(X_over_pca,y_over_pca)\nprint(f'Test Score - {f1_score(clf.predict(X_test_std_pca),y_test)}')\nprint(f'Train Score - {f1_score(clf.predict(X_train_std_pca),y_train)}')\n\nprint('Feature Selection')\nmodel.fit(A_over,Ay_over)\nprint(f'Test Score - {f1_score(model.predict(A_test),Ay_test)}')\nprint(f'Train Score - {f1_score(model.predict(A_over),Ay_over)}')","d766e4ed":"#Cross Validation of Random Forest\nfor n in range(50,200,50):\n            for m in range(20,60,10):\n                    for m2 in range(10,50,10):\n                        clf = RandomForestClassifier(n_estimators = n,min_samples_split=m,min_samples_leaf=m2)\n                        clf.fit(X_over,y_over)\n                        print(f'Esimators : {n}, min_samples_split : {m},min_samples_leaf: {m2}')\n                        print(f'CV Score - {f1_score(clf.predict(X_cv),y_cv,average=\"weighted\")}')\n                        print(f'Train Score - {f1_score(clf.predict(X_train),y_train,average=\"weighted\")}')\n                        ","cdd6d2cb":"#XGBoost Hyper Parameter tuning\nfor ss in range(5,9):\n        for cs in range(5,9):\n\n            model = XGBClassifier(learning_rate =0.1,n_estimators=50,gamma=0,colsample_bytree=(cs\/10),subsample=(ss\/10),objective= 'binary:logistic',seed=27)\n            model.fit(X_over,y_over)\n            print(f'{ss},{cs}')\n            print(f'Test Score - {f1_score(model.predict(X_test),y_test,average=\"weighted\")}')\n            print(f'Train Score - {f1_score(model.predict(X_over),y_over,average=\"weighted\")}')\n        ","861c6cce":"#Neural Network Hyper Parametr Tuning\nfor h1 in np.arange(1, 6):\n     for h2 in np.arange(1, 5):\n\n        clf = MLPClassifier(solver='sgd',alpha=1e-05,hidden_layer_sizes=(h1,h2), random_state=1,max_iter=100)\n        clf.fit(X_train_std, y_train)\n        print(f'{h1},{h2}')\n        print(f1_score(clf.predict(X_test_std),y_test,average='weighted'))\n        print(f1_score(clf.predict(X_train_std),y_train,average='weighted'))\n                ","31163cfe":"# for n in range(50,200,50):\n#             for m in range(20,60,10):\n#                     for m2 in range(10,50,10):\n#                         clf = RandomForestClassifier(n_estimators = n,min_samples_split=m,min_samples_leaf=m2)\n#                         clf.fit(A_over,Ay_over)\n#                         print(f'Esimators : {n}, min_samples_split : {m},min_samples_leaf: {m2}')\n#                         print(f'CV Score - {f1_score(clf.predict(A_test),Ay_test,average=\"weighted\")}')\n#                         print(f'Train Score - {f1_score(clf.predict(A_over),Ay_over,average=\"weighted\")}')\n                        ","fd3b77a6":"# #XGBoost Hyper Parameter tuning\n# for ss in range(5,9):\n#         for cs in range(5,9):\n\n#             model = XGBClassifier(learning_rate =0.1,n_estimators=50,gamma=0,colsample_bytree=(cs\/10),subsample=(ss\/10),objective= 'binary:logistic',seed=27)\n#             model.fit(A_over,Ay_over)\n#             print(f'{ss},{cs}')\n#             print(f'Test Score - {f1_score(model.predict(A_test),Ay_test,average=\"weighted\")}')\n#             print(f'Train Score - {f1_score(model.predict(A_over),Ay_over,average=\"weighted\")}')\n        ","e1574cb8":"# Modeling","dc61e388":"# EDA","21bf13d3":"# Feature Importance","ec8a494f":"##### Conclusion:\np Value is very high and hence we know that the models are not very different and hence we do not need to split and build different for younger and older individuals","b7dd3811":"# Data Prep\nUsing Random Forest\n1. Perform classification with PCA\n2. Perform calssification without PCA","a8dd2ce2":"### Hyper Parameter Tuning using Cross Validation data with MLP","433570f3":"### **Q1** - CDC has a simple [form](https:\/\/www.cdc.gov\/prediabetes\/takethetest\/) to help a candidate understand whether he\/she is diabetic. If the candidate is \"safe\" with respect to the select features the result is that the candidate is not diabetic. We want to validate this hypothesis using the data we have\n#### **Assumptions**: Obviously this wouldnt be 100% accurate but with an alpha of 1-2% would this be accurate?\n\n#### **Steps**\n1. Choose the hypothesis test\n2. Data Prep.\n3. Perform the test\n4. Conclude\n5. Explanation","0d7916ec":"## Hyper Parameter Tuning with 80% of the Features","c27fcea4":"# Possible Feature Elmination","68eacc68":"### Baseline Model","30446d75":"### Hyper Parameter Tuning using Cross Validation data with XGBoost","191bb5eb":"### Q2 - Does it make sense to have different models for younger individuals vs older individuals?\n1. Splitting around the age of 40 years let us check this.\n2. Build different models for the datasets\n3. Compare feature importances in the resultant models using paired t-test\n(If the feature importances differ for younger and older individuals we need different models)","d387640d":"### Q3 - Does it make sense to have different models for NoDoctorBcos Of Cost individuals vs No Cost Problem individuals?\n1. Splitting the data for male and female.\n2. Build different models for the datasets\n3. Compare feature importances in the resultant models using paired t-test\n(If the feature importances differ for female and male individuals we need different models)","48257130":"### Q3 - Does it make sense to have different models for female individuals vs male individuals?\n1. Splitting the data for male and female.\n2. Build different models for the datasets\n3. Compare feature importances in the resultant models using paired t-test\n(If the feature importances differ for female and male individuals we need different models)","6468e856":"### Q4 - Does it make sense to have different models for Smoker individuals vs Non-Smoker individuals?\n1. Splitting the data for male and female.\n2. Build different models for the datasets\n3. Compare feature importances in the resultant models using paired t-test\n(If the feature importances differ for female and male individuals we need different models)","9d17d454":"### Hyper Parameter Tuning using Cross Validation data with Random Forest","f99747fc":"## Impact of OverSampling the data for imbalanced classification problems\n## Impact of PCA on the data like this which is mostly categorical","b4bf7641":"# Feature Dependence\nSince most of the columns are categorical it is not advisable to use corelation to eliminate any features and hence we are resorting to Chi-Square test ; Mutual Information Score","a59dcf57":"# Hypothesis Testing\n\n\n","4a1f41bc":"##### Conclusion\npValue is quite low. So we can reject the null hypothesis and conclude that there is a significant impact "}}