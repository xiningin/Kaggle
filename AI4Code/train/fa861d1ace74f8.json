{"cell_type":{"a5b6edd2":"code","7e3b8526":"code","e2cc17eb":"code","a9c0a1e1":"code","417fe30f":"code","7f5bbca2":"code","0cdc72b9":"code","c135417f":"code","574d2dd0":"code","05995af1":"code","c3c14bf8":"code","6179d165":"code","1be951ae":"code","8760edbc":"code","91ca9659":"markdown","79f3ce71":"markdown","092635c3":"markdown","264a6ba5":"markdown","d07957fd":"markdown","e8f75da5":"markdown","dc9d5003":"markdown","66609d7d":"markdown"},"source":{"a5b6edd2":"import re\nimport string\nfrom functools import reduce\n\nimport numpy as np\nimport pandas as pd\nimport transformers\nimport torch\nimport plotly.graph_objects as go\nfrom tqdm.notebook import tqdm\n\n\npd.set_option(\"display.max_colwidth\", 300)","7e3b8526":"INPUT_DIR = \"..\/input\/tweet-sentiment-extraction\"\ntrain = pd.read_csv(f\"{INPUT_DIR}\/train.csv\")\ntrain.head()","e2cc17eb":"train.isnull().sum()","a9c0a1e1":"train = train.dropna()\n\nassert train.isnull().sum().eq(0).all()","417fe30f":"# Ref: https:\/\/www.kaggle.com\/parulpandey\/basic-preprocessing-and-eda\n\ndef clean_text(text):\n    \"\"\"\n    Does the following:\n    - Make text lowercase\n    - Remove text in square brackets\n    - Remove links\n    - Remove punctuation\n    - Remove words containing numbers\n    \"\"\"\n    text = text.lower()\n    text = re.sub('\\[.*?\\]', '', text)\n    text = re.sub('https?:\/\/\\S+|www\\.\\S+', '', text)\n    text = re.sub('<.*?>+', '', text)\n    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n    text = re.sub('\\n', '', text)\n    text = re.sub('\\w*\\d\\w*', '', text)\n    return text","7f5bbca2":"train[\"clean\"] = train[\"text\"].map(clean_text)\ntrain[[\"text\", \"clean\"]].head()","0cdc72b9":"train[\"num_words\"] = train[\"clean\"].str.split(\" \").map(len)\ntrain[[\"clean\", \"num_words\"]].head()","c135417f":"not_too_short = train[\"num_words\"] >= 10\nnot_too_short.sum()","574d2dd0":"raw_texts = train[not_too_short][\"text\"].sample(n=5000, random_state=42)\nclean_texts = train.loc[raw_texts.index][\"clean\"]\nselected_text = train.loc[raw_texts.index][\"selected_text\"]\nsentiment = train.loc[raw_texts.index][\"sentiment\"]\n\n\ntrain.loc[raw_texts.index][[\"text\", \"clean\", \"sentiment\"]]","05995af1":"# Ref.: https:\/\/www.kaggle.com\/abhishek\/distilbert-use-features-oof\/notebook\n\ndef chunks(l, n):\n    \"\"\"\n    Yield successive n-sized chunks from l.\n    \n    Example\n    -------\n    >>> l = list(range(10))\n    >>> for c in chunks(l, 3):\n    ...     print(c)\n    [0, 1, 2]\n    [3, 4, 5]\n    [6, 7, 8]\n    [9]\n\n    \"\"\"\n    for i in range(0, len(l), n):\n        yield l[i:i + n]\n\n\ndef fetch_vectors(string_list, batch_size=64):\n    # inspired by https:\/\/jalammar.github.io\/a-visual-guide-to-using-bert-for-the-first-time\/\n    DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    tokenizer = transformers.DistilBertTokenizer.from_pretrained(\"..\/input\/distilbertbaseuncased\/\")\n    model = transformers.DistilBertModel.from_pretrained(\"..\/input\/distilbertbaseuncased\/\")\n    model.to(DEVICE)\n\n    fin_features = []\n    total = len(string_list) \/\/ batch_size + 1\n    for data in tqdm(chunks(string_list, batch_size), total=total):\n        tokenized = []\n        for x in data:\n            x = \" \".join(x.strip().split()[:300])\n            tok = tokenizer.encode(x, add_special_tokens=True)\n            tokenized.append(tok[:512])\n\n        max_len = 512\n        padded = np.array([i + [0] * (max_len - len(i)) for i in tokenized])\n        attention_mask = np.where(padded != 0, 1, 0)\n        input_ids = torch.tensor(padded).to(DEVICE)\n        attention_mask = torch.tensor(attention_mask).to(DEVICE)\n        \n        with torch.no_grad():\n            last_hidden_states = model(input_ids, attention_mask=attention_mask)\n\n        features = last_hidden_states[0][:, 0, :].cpu().numpy()\n        fin_features.append(features)\n\n    fin_features = np.vstack(fin_features)\n    return fin_features","c3c14bf8":"vectors = fetch_vectors(clean_texts)\nvectors.shape","6179d165":"from sklearn.manifold import TSNE\n\nreduced = TSNE(n_components=2).fit_transform(vectors)\nreduced.shape","1be951ae":"def wrap_text(text):\n    \"\"\"\n    Insert <br> to wrap long text on a Plotly chart.\n\n    Example\n    -------\n    >>> import string\n    >>> text = \"a b c d e f g h i\"\n    >>> wrap_text(text, 3)\n    \"a b c<br>d e f<br>g h i\"\n\n    \"\"\"\n    rows = [\" \".join(c) for c in chunks(text.split(), 10)]\n    return \"<br>\".join(rows)","8760edbc":"hovertext = reduce(lambda a, b: a + \"<br>\" + b, [\n    \"# Raw text\",\n    raw_texts.map(wrap_text),\n    \"\",\n    \"# Clean text\",\n    clean_texts.map(wrap_text),\n    \"\",\n    \"# Selected text\",\n    selected_text.map(wrap_text),\n    \"\",\n    \"# Sentiment\",\n    sentiment,\n])\n\ncolor = sentiment.map({\n    \"positive\": \"green\",\n    \"neutral\": \"#bbbbbb\",\n    \"negative\": \"red\",\n})\n\ndata = go.Scatter(\n    x=reduced[:, 0],\n    y=reduced[:, 1],\n    mode=\"markers\",\n    hoverinfo=\"text\",\n    hovertext=hovertext,\n    marker=dict(color=color),\n)\n\ngo.Figure(data=data)","91ca9659":"# Import libraries","79f3ce71":"# Dimensionality reduction with t-SNE","092635c3":"# Sampling","264a6ba5":"# Visualize vectors ","d07957fd":"# Convert text to vectors ","e8f75da5":"# Drop rows containing NaN","dc9d5003":"# Load data","66609d7d":"# Clean text"}}