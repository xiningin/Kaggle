{"cell_type":{"53b81778":"code","963288a1":"code","2df398b8":"code","55919aec":"code","6d38990b":"code","e9d416bc":"code","8bb8bc37":"code","438e8e65":"code","bca449ff":"code","9575efc1":"code","8604fed2":"code","3b10f00f":"code","a25c4b0a":"markdown","05eeccd5":"markdown","2a704211":"markdown","ee46f0e8":"markdown","917be870":"markdown","c699e36b":"markdown","f1539a2c":"markdown"},"source":{"53b81778":"# Air Passengers\n# 1949\ub144 ~ 1960\ub144 \ub9e4\ub2ec \ube44\ud589\uae30 \ud0d1\uc2b9\uac1d \uc218\uc5d0 \ub300\ud55c \uc2dc\uacc4\uc5f4 \ub370\uc774\ud130\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\ndf = pd.read_csv(r'..\/input\/ysp-koreauniv-tutorial\/AirPassengers.csv',index_col=0)\ndf","963288a1":"df.index = pd.to_datetime(df.index) # index\ub97c \uc2dc\uac04\ud615\uc73c\ub85c \ubcc0\ud658","2df398b8":"df","55919aec":"df.plot()\nplt.show()","6d38990b":"from statsmodels.tsa.seasonal import seasonal_decompose # \uc704 markdown \uc218\uc2dd\uc5d0 \ub530\ub77c \uc6d0\ubcf8 \ub370\uc774\ud130\ub97c \ucd94\uc138, \uacc4\uc808\uc131, \uc794\ucc28\ub85c \ubd84\ud574\nresult = seasonal_decompose(df, model='multiplicative',period=12)\nplt.rcParams['figure.figsize'] = [12, 8]\nresult.plot(observed=True, seasonal=True, trend=True, resid=True)\nplt.show()","e9d416bc":"from statsmodels.tsa.seasonal import seasonal_decompose\nresult = seasonal_decompose(df, model='additive',period=12)\nplt.rcParams['figure.figsize'] = [12, 8]\nresult.plot(observed=True, seasonal=True, trend=True, resid=True)\nplt.show()","8bb8bc37":"from sklearn.model_selection import train_test_split\ntrain_data, test_data = train_test_split(df, test_size = 0.2, shuffle = False) # \uc911\uc694 shuffle = False","438e8e65":"from statsmodels.tsa.holtwinters import ExponentialSmoothing\n\nfit1 = ExponentialSmoothing(train_data, trend=None, seasonal= None, freq = 'MS')\nfit1 = fit1.fit()\n\nfit2 = ExponentialSmoothing(train_data, trend='add', seasonal= None, freq = 'MS')\nfit2 = fit2.fit()\n\nfit3 = ExponentialSmoothing(train_data, trend='mul', seasonal= None, freq = 'MS')\nfit3 = fit3.fit()\n\nfit4 = ExponentialSmoothing(train_data, trend=None, seasonal= 'add', freq = 'MS')\nfit4 = fit4.fit()\n\nfit5 = ExponentialSmoothing(train_data, trend='add', seasonal= 'add', freq = 'MS')\nfit5 = fit5.fit()\n\nfit6 = ExponentialSmoothing(train_data, trend='mul', seasonal= 'add', freq = 'MS')\nfit6 = fit6.fit()\n\nfit7 = ExponentialSmoothing(train_data, trend=None, seasonal= 'multiplicative', seasonal_periods=12,freq = 'MS')\nfit7 = fit7.fit()\n\nfit8 = ExponentialSmoothing(train_data, trend='add', seasonal= 'multiplicative', seasonal_periods=6, freq = 'MS')\nfit8 = fit8.fit()\n\nfit9 = ExponentialSmoothing(train_data.values, trend='add', seasonal= 'mul', seasonal_periods=6, \n                            damped=True, initialization_method=\"estimated\", missing='none')\nfit9 = fit9.fit()","bca449ff":"from statsmodels.tsa.holtwinters import ExponentialSmoothing\ntrends = [None, 'add', 'mul']\nseasonals = [None, 'add', 'mul']\nESmodels = []\ntitles=[]\nfor i in trends:\n    for j in seasonals:\n        print(i,j)\n        titles.append((i,j)) # \uadf8\ub798\ud504\ub97c \uadf8\ub9ac\uae30 \uc704\ud574 trend\uc640 seasonal\uc744 \uc5b4\ub5a4 \ubcc0\uc218\ub85c \ub123\uc5c8\ub294\uc9c0 \ud655\uc778\ud558\uae30 \uc704\ud574\n        ESmodels.append(ExponentialSmoothing(train_data, trend=i, seasonal= j, freq = 'MS').fit())","9575efc1":"from sklearn.metrics import r2_score\npredicted_values = []\nr2_scores = []\nfor i in range(len(ESmodels)):\n    predicted_values.append(ESmodels[i].predict(start = pd.to_datetime('1958-08-01'), end = pd.to_datetime('1960-12-01'))) # start \ub0a0\uc9dc\ubd80\ud130 end \ub0a0\uc9dc\uae4c\uc9c0 \uae30\uac04\uc744 \uc608\uce21\n    try:\n        r2_scores.append(r2_score(test_data, predicted_values[-1]))\n    except:\n        r2_scores.append(-5)\n        continue","8604fed2":"r2_scores","3b10f00f":"predict_index = list(test_data.index)\nfig, ax = plt.subplots(figsize=(18,6))\n# df.plot(ax=ax);\nax.plot(df.index, df) # train, test\uac00 \ud568\uaed8 \uc788\ub294 \uc6d0\ubcf8 \ub370\uc774\ud130\nax.vlines(pd.to_datetime('1958-08-01'), 100, 650, linestyle='--', color='r', label = 'Start of Forecast') # train \ub370\uc774\ud130\uc640 test \ub370\uc774\ud130\uc758 \uacbd\uacc4\ubd80\ubd84 \ud45c\uc2dc\nfor i in range(len(predicted_values)):\n    ax.plot(predict_index, predicted_values[i], label = 'Prediction') # test\ubd80\ubd84\uc5d0 \ub300\ud55c \uc608\uce21\uacb0\uacfc \ud45c\uc2dc\n# ax.fill_between(predict_index, predicted_lb, predicted_ub, color='k', alpha=0.1, label='0.95 Prediction Interval')\nplt.show()\n\nfor i in range(len(predicted_values)):\n    fig, ax = plt.subplots(figsize=(18,6))\n    # df.plot(ax=ax);\n    ax.plot(df.index, df)\n    ax.vlines(pd.to_datetime('1958-08-01'), 100, 650, linestyle='--', color='r', label = 'Start of Forecast')\n\n    ax.plot(predict_index, predicted_values[i], label = 'Prediction')\n    plt.title('{}, r2_score: {}'.format(titles[i], r2_scores[i]))\n    # ax.fill_between(predict_index, predicted_lb, predicted_ub, color='k', alpha=0.1, label='0.95 Prediction Interval')\n    plt.show()","a25c4b0a":"## additive \ud640\ud2b8\uc708\ud130","05eeccd5":"![image.png](attachment:b6d2dab9-51b1-4eed-92aa-fd2194ed380e.png)","2a704211":"## multiplicative \ud640\ud2b8\uc708\ud130\n![image.png](attachment:211303c7-e764-47cf-9dc6-034358cad1ba.png)","ee46f0e8":"![image.png](attachment:5cbcc475-c3e8-416c-a473-b8f8817ad31c.png)","917be870":"![image.png](attachment:9286c1b5-92f6-4f18-b0d4-092008bd2255.png)","c699e36b":"![image.png](attachment:069a2918-7f5d-4977-9c03-2204e9e8efa0.png)","f1539a2c":"![image.png](attachment:2d58467e-780f-4c2d-887b-bedd4b53f4e5.png)"}}