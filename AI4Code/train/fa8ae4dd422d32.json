{"cell_type":{"c12f4dcc":"code","d565dc19":"code","5b83205b":"code","99b16044":"code","06f041c9":"code","66d88eb0":"code","bfa11926":"code","4a953a25":"code","09bdef75":"code","1c37e7c8":"code","59e24b5b":"code","ab5d39a0":"code","2c5a60a6":"code","e8a3936c":"code","0bc11a74":"code","aecb197d":"code","93956cca":"code","f0aedd02":"code","0f7d4d69":"code","3ea83877":"code","f16bd861":"code","6b998a63":"code","205fd331":"code","35884803":"code","cab6d502":"code","fa6f746e":"code","85812ba4":"code","124f461b":"code","7723e2c4":"code","b11ffd4b":"code","92ee98a7":"code","54e52551":"code","048870bc":"code","e87b6d8d":"code","830fefe0":"code","86b6b2b1":"code","6de05711":"code","ca76b36d":"code","3c9dad62":"code","3661d4e9":"code","f3f3b037":"code","20f44163":"code","e7c428b8":"code","2caacb71":"code","49edc526":"code","99548e91":"code","06c0a716":"code","a4a5b55c":"code","b18a5cc1":"code","b8afb297":"code","275a0860":"code","4eeaa8ee":"code","1f4288b7":"code","8c211f30":"code","7648b4e6":"code","cb927053":"code","8f7090df":"code","e031056c":"code","32ce1ede":"code","b3e076fb":"code","7203b866":"code","6c331dc2":"code","e11fdda4":"code","f7eaf6c1":"code","c44d6d8f":"code","ee200965":"code","5c83c508":"code","f3408d70":"code","02990d29":"code","a818ff22":"code","f1d57303":"code","caccc730":"code","b1c53766":"code","555727b1":"markdown","5f30a89c":"markdown","13435f92":"markdown","3dc548de":"markdown","f65d9730":"markdown","7ac6a79a":"markdown","821f0531":"markdown","b756077d":"markdown","9cbbda06":"markdown","0bf42566":"markdown","1510c963":"markdown","ae02aa87":"markdown","70f656ca":"markdown","536ae064":"markdown","03c799e2":"markdown","5e31438c":"markdown","9567c016":"markdown","f784bff5":"markdown","74a1951a":"markdown","56ae74fb":"markdown","8c85f220":"markdown","82494a9c":"markdown","66c5e5b0":"markdown","f9d02cc4":"markdown","a0d1f3e7":"markdown","82207cda":"markdown","92cb58c5":"markdown","93a5a933":"markdown","9263182a":"markdown","eeea9014":"markdown","ae19a030":"markdown","67ae8ca1":"markdown","4a248fef":"markdown","da94afb1":"markdown","83d45d79":"markdown","6c6ebe83":"markdown","f0cb78b6":"markdown","582f418e":"markdown","1841c16d":"markdown","91bde8df":"markdown","3db21352":"markdown","3dc41c49":"markdown","e514c0da":"markdown","7876c22c":"markdown","e94b2227":"markdown","de1c4e4c":"markdown","2a907e44":"markdown","8ee58578":"markdown","f6c601d7":"markdown","997f93c1":"markdown","8d050a4d":"markdown"},"source":{"c12f4dcc":"from matplotlib import pyplot \nimport matplotlib as mpl\n\ndef show(image): \n    fig = pyplot.figure()\n    ax = fig.add_subplot(1,1,1)\n    imgplot = ax.imshow(image, cmap = ml.cm.Greys)\n    ax.xaxis.set_ticks_position('neariest')\n    pyplot.show()\n","d565dc19":"import os\nimport pandas as pd\nimport numpy as np\nimport random\nimport sys\n\nimport scipy\nimport sklearn\nimport sklearn.metrics as skm\n\nfrom matplotlib import pyplot as plt\nfrom sklearn import svm\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import confusion_matrix as skm_conf_mat\n\nimport datetime as DT","5b83205b":"def confusionMatrixInfo(p,a, labels = None):\n    #\n    from sklearn.metrics import confusion_matrix as skm_conf_mat\n#    p = pd.Series([1,1,1,0,0,0,0,0,0,0])\n#    a = pd.Series([1,0,0,1,1,1,0,0,0,0])\n#    labels = [1,0]\n#\n#    x = skm.confusion_matrix(a,p,labels=labels)\n    if 'sklearn' not in sys.modules:\n        import sklearn\n    x = skm_conf_mat(a,p, labels = labels)\n    tp = x[0,0]\n    tn = x[1,1]\n    fp = x[1,0]\n    fn = x[0,1]\n    # tp, fp, fn, tn # test order\n    \n    tsensitivity = tp\/(tp+fn)\n    tspecificity = tn\/(tn + fp)\n    # no information rate?\n    tnir = (tp + fn)\/x.sum()\n    tnir = max(tnir, 1-tnir)\n    # accuracy\n    taccuracy = (tp + tn)\/x.sum()\n    \n    res = {'confusionMatrix':x,\n           'accuracy': taccuracy,\n           'no information rate': tnir,\n           'sensitivity': tsensitivity,\n           'specificity': tspecificity\n           }\n    return(res)","99b16044":"from sklearn.model_selection import train_test_split as skl_traintest_split\nfrom sklearn.utils import shuffle\n#print(check_output([\"ls\", \"..\/input\"]).decode(\"utf8\"))\nkdigits = pd.read_csv(\"..\/input\/train.csv\")","06f041c9":"kdigits.shape","66d88eb0":"kdigits.head() #784 pixels in 28x28 frame","bfa11926":"kdigits.max() #Pixel values cover full range 0-255","4a953a25":"train, test = skl_traintest_split(kdigits.copy(), test_size = 0.20, random_state = 2019)\ntrain.shape","09bdef75":"test.shape","1c37e7c8":"def scale(data):\n    X = data.drop(columns = {'label'})\n    #Scale variance\n    stdev = np.std(X, axis = 1)\n    X_scaled = X.mul(1\/stdev, axis = 0)\n    #np.std(X_scaled, axis = 1) #Consistent variance across digits\n    \n    #Scale magnitude\n    max_scale = X_scaled.max(axis=1) #Generate a vector of max values from each row\n    X_scaled = X_scaled.mul(1\/max_scale, axis=0) #Scale from 0-1 for each digit\n    \n    #np.std(X_train_scaled, axis = 1) # No longer consistet variance b\/c we scaled the magnitude\n    #This shows scaling variance does nothing b\/c we also scaled magnitude. No difference in variance after magnitudes scaled\n    #np.mean(np.std(X_train.mul(1\/X_train.max(axis=1),axis =0),axis=1) - np.std(X_train_scaled, axis = 1))\n    \n    #Add labels back\n    data = pd.concat([data['label'],X_scaled], axis = 1)\n    return data\n\ntrain = scale(train)\ntest = scale(test)","59e24b5b":"#Save Scaled Data to Folder\nscaledTrain = os.path.join(\"scaledTrain\")\nscaledTest = os.path.join(\"scaledTest\")\ntrain.to_pickle(scaledTrain + \".pkl\")\ntest.to_pickle(scaledTest + \".pkl\")","ab5d39a0":"def bifurcateDigits(whichSeries, value, train = True):\n    temp = whichSeries.copy()\n    if(value >= 10):\n        print(\"Value's only range from 0-9\")\n        return\n    #Boolean for label = value\n    temp['label'] = (temp['label'] == value).astype(int)\n    #Split T\/F\n    x1 = temp[temp['label'] == 1]\n    x2 = temp[temp['label'] == 0]\n    #Identify proportion of value to not value\n    prop = len(x1)\/len(x2)\n    #Randomly select a subset of size prop from x2 (value = False)\n    random.seed(2019)\n    rndindex = [random.uniform(0,1) for x in range(x2.shape[0])]\n    rndindex = [True if x < prop else False for x in rndindex]\n    x2 = x2.loc[rndindex]\n    #Shuffled subset of original with ~50\/50 value to not value\n    temp = shuffle(x1.append(x2))\n    #Isolate dependent var from predictors for either train\/test data\n    if(train == False):\n        global y_test, X_test\n        y_test = temp['label']\n        X_test = temp.drop(columns = {'label'})\n    else:\n        global y_train, X_train\n        y_train = temp['label']\n        X_train= temp.drop(columns = {'label'})\n\nbifurcateDigits(train, 0, train = True)\n\n#Returns y\/n for 'value' without adjusting for 90\/10 split\ndef biasedBifurcatedDigits(whichSeries, value, train = True):\n    temp = whichSeries.copy()\n    if(value >= 10):\n        print(\"Value's only range from 0-9\")\n        return\n    #Boolean for label = value\n    temp['label'] = (temp['label'] == value).astype(int)\n    #Split T\/F\n    if(train == False):\n        global y_test, X_test\n        y_test = temp['label']\n        X_test = temp.drop(columns = {'label'})\n    else:\n        global y_train, X_train\n        y_train = temp['label']\n        X_train= temp.drop(columns = {'label'})","2c5a60a6":"def viewDigits(whichSeries, threshold=None):\n    whichSeries = np.array(whichSeries, dtype='float')\n    if threshold is not None:\n        whichSeries = (whichSeries > threshold).astype(int)\n    plt.imshow(whichSeries.reshape((28,28)), cmap='gray')\n\nviewDigits(X_train.iloc[100])","e8a3936c":"from sklearn.model_selection import train_test_split as skl_traintest_split\nfrom sklearn.ensemble import GradientBoostingClassifier as GBClass\nfrom sklearn.metrics import confusion_matrix","0bc11a74":"params = {'n_estimators':100, 'subsample':1.0, 'learning_rate':0.1}\nparams = dict(params)\nmodel_gbc = GBClass(**params)","aecb197d":"bifurcateDigits(train, 0, train = True) \nbifurcateDigits(test, 0, train = False)\nmodel_gbc.fit(X_train, y_train)\npred_Y = model_gbc.predict(X_test)\nz_gbc = model_gbc.predict_proba(X_test)[:,1]","93956cca":"#confusion Matrix\nconfusionMatrixInfo(pred_Y, y_test, labels = None)","f0aedd02":"# RUC\nfpr_gbc, tpr_gbc, thresh_gbc = skm.roc_curve(y_test, z_gbc)\nplt.figure(1)\nplt.plot(fpr_gbc, tpr_gbc, 'r-')","0f7d4d69":"# AUC \nskm.auc(fpr_gbc,tpr_gbc)  ","3ea83877":"bifurcateDigits(train, 1, train = True)\nbifurcateDigits(test, 1, train = False)\nmodel_gbc.fit(X_train, y_train)\npred_Y = model_gbc.predict(X_test)\nz_gbc = model_gbc.predict_proba(X_test)[:,1]","f16bd861":"confusionMatrixInfo(pred_Y, y_test, labels = None)","6b998a63":"fpr_gbc, tpr_gbc, thresh_gbc = skm.roc_curve(y_test, z_gbc)","205fd331":"skm.auc(fpr_gbc,tpr_gbc)","35884803":"bifurcateDigits(train, 2, train = True)  \nbifurcateDigits(test, 2, train = False)\nmodel_gbc.fit(X_train, y_train)\npred_Y = model_gbc.predict(X_test)\nz_gbc = model_gbc.predict_proba(X_test)[:,1]","cab6d502":"confusionMatrixInfo(pred_Y, y_test, labels = None)","fa6f746e":"fpr_gbc, tpr_gbc, thresh_gbc = skm.roc_curve(y_test, z_gbc)\nplt.figure(2)\nplt.plot(fpr_gbc, tpr_gbc, 'r-')","85812ba4":"skm.auc(fpr_gbc,tpr_gbc)","124f461b":"bifurcateDigits(train, 3, train = True)   # when the digit is 3 \nbifurcateDigits(test, 3, train = False)\nmodel_gbc.fit(X_train, y_train)\npred_Y = model_gbc.predict(X_test)\nz_gbc = model_gbc.predict_proba(X_test)[:,1]","7723e2c4":"confusionMatrixInfo(pred_Y, y_test, labels = None)","b11ffd4b":"fpr_gbc, tpr_gbc, thresh_gbc = skm.roc_curve(y_test, z_gbc)","92ee98a7":"skm.auc(fpr_gbc,tpr_gbc)","54e52551":"bifurcateDigits(train, 4, train = True)\nbifurcateDigits(test, 4, train = False)\nmodel_gbc.fit(X_train, y_train)\npred_Y = model_gbc.predict(X_test)\nz_gbc = model_gbc.predict_proba(X_test)[:,1]","048870bc":"confusionMatrixInfo(pred_Y, y_test, labels = None)","e87b6d8d":"fpr_gbc, tpr_gbc, thresh_gbc = skm.roc_curve(y_test, z_gbc)","830fefe0":"skm.auc(fpr_gbc,tpr_gbc)","86b6b2b1":"bifurcateDigits(train, 5, train = True) \nbifurcateDigits(test, 5, train = False)\nmodel_gbc.fit(X_train, y_train)\npred_Y = model_gbc.predict(X_test)\nz_gbc = model_gbc.predict_proba(X_test)[:,1]","6de05711":"confusionMatrixInfo(pred_Y, y_test, labels = None)","ca76b36d":"fpr_gbc, tpr_gbc, thresh_gbc = skm.roc_curve(y_test, z_gbc)","3c9dad62":"skm.auc(fpr_gbc,tpr_gbc)","3661d4e9":"bifurcateDigits(train, 6, train = True)  \nbifurcateDigits(test, 6, train = False)\nmodel_gbc.fit(X_train, y_train)\npred_Y = model_gbc.predict(X_test)\nz_gbc = model_gbc.predict_proba(X_test)[:,1]","f3f3b037":"confusionMatrixInfo(pred_Y, y_test, labels = None)","20f44163":"fpr_gbc, tpr_gbc, thresh_gbc = skm.roc_curve(y_test, z_gbc)","e7c428b8":"skm.auc(fpr_gbc,tpr_gbc)","2caacb71":"bifurcateDigits(train, 7, train = True)   \nbifurcateDigits(test, 7, train = False)\nmodel_gbc.fit(X_train, y_train)\npred_Y = model_gbc.predict(X_test)\nz_gbc = model_gbc.predict_proba(X_test)[:,1]","49edc526":"confusionMatrixInfo(pred_Y, y_test, labels = None)","99548e91":"fpr_gbc, tpr_gbc, thresh_gbc = skm.roc_curve(y_test, z_gbc)","06c0a716":"skm.auc(fpr_gbc,tpr_gbc)","a4a5b55c":"bifurcateDigits(train, 8, train = True)   \nbifurcateDigits(test, 8, train = False)\nmodel_gbc.fit(X_train, y_train)\npred_Y = model_gbc.predict(X_test)\nz_gbc = model_gbc.predict_proba(X_test)[:,1]","b18a5cc1":"confusionMatrixInfo(pred_Y, y_test, labels = None)","b8afb297":"fpr_gbc, tpr_gbc, thresh_gbc = skm.roc_curve(y_test, z_gbc)","275a0860":"skm.auc(fpr_gbc,tpr_gbc)","4eeaa8ee":"bifurcateDigits(train, 9, train = True)   # when the digit is 9 \nbifurcateDigits(test, 9, train = False)\nmodel_gbc.fit(X_train, y_train)\npred_Y = model_gbc.predict(X_test)\nz_gbc = model_gbc.predict_proba(X_test)[:,1]","1f4288b7":"confusionMatrixInfo(pred_Y, y_test, labels = None)","8c211f30":"fpr_gbc, tpr_gbc, thresh_gbc = skm.roc_curve(y_test, z_gbc)","7648b4e6":"skm.auc(fpr_gbc,tpr_gbc)","cb927053":"# set up parameters \nparameters = {'kernel':('linear', 'rbf'), 'C':list(np.logspace(-10,3,14))}\nsvc = svm.SVC(gamma=\"scale\")\n#svc = GridSearchCV(svc, parameters, cv=5, n_jobs=2)\n\nfor n in range(0, 10):\n    bifurcateDigits(train, n, train = True)\n    bifurcateDigits(test, n, train = False)\n    svc.fit(X_train, y_train)    # have problem running \n    y_cvsvm = svc.predict(X_test)\n    \n    # Prediction results\/ Confusion Matrix \n    print(confusionMatrixInfo(y_cvsvm,y_test, labels = None))","8f7090df":"import sklearn.metrics as skm\nfrom sklearn.ensemble import RandomForestClassifier as RFClass\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix","e031056c":"model_rf = RFClass()\nprint(model_rf.get_params())","32ce1ede":"#1 Number of trees in random forest\nn_estimators = [int(x) for x in np.linspace(start = 50, stop = 300, num =6)]\n#2 Number of features to consider at every split\nmax_features = ['auto']\n#3 Maximum number of levels in tree\nmax_depth = [int(x) for x in np.linspace(3, 7, num = 5)]\n#4 Minimum number of samples required to split a node\nmin_samples_split = [2, 5, 10]\n#5 Minimum number of samples required at each leaf node\nmin_samples_leaf = [1, 2, 4]\n#6 Create the random grid\nrandom_grid = {'n_estimators': n_estimators, 'max_features': max_features, 'max_depth': max_depth, \n               'min_samples_split': min_samples_split, 'min_samples_leaf': min_samples_leaf}\n","b3e076fb":"#Model Selection\nt0=DT.datetime.now()\ndigit = [0,1,2,3,4,5,6,7,8,9]\nmodellist = []\n\nfor value in digit:\n    t0=DT.datetime.now()\n    #Split train data 50\/50\n    bifurcateDigits(train, value, train = True)\n    #Use the random grid to search for best parameters\n    # Search of parameters, using 3 fold cross validation with 50 random combinations\n    rf_random = RandomizedSearchCV(estimator = model_rf, param_distributions = random_grid, n_iter = 50, cv = 3, verbose=2, random_state=2019)\n    # Fit the random search model\n    rf_random.fit(X_train, y_train)\n    #Identify strongest parameters by accuracy\n    rf_random.best_params_\n    #Apply strongest parameters to the rf model\n    model_rf = RFClass(n_estimators = rf_random.best_params_['n_estimators'], \n                       min_samples_split =  rf_random.best_params_['min_samples_split'], \n                       min_samples_leaf = rf_random.best_params_['min_samples_leaf'], \n                       max_features = 'auto', \n                       max_depth = rf_random.best_params_['max_depth'])\n    #Append best model for each digit (0-9) to list\n    modellist.append(model_rf)\n\nt1=DT.datetime.now()\nprint('Randomized Search of Random Forest for all digits took ' + str(t1-t0))","7203b866":"AUC = []\naccuracy = []\n#Specify a cost for false positive and false negatives. Choose 50\/50 here. No obvious reason to lean more or less 'conservative'\nfpc = 100\nfnc = 100\n\nt0=DT.datetime.now()\nfor value in digit:\n    #Call train and test sets\n    bifurcateDigits(train, value, train = True)\n    bifurcateDigits(test, value, train = False)\n    #Fit the model selected with the locally optimal parameters\n    modellist[value].fit(X_train, y_train)\n    #Use the RF to predict on the test data\n    predict = modellist[value].predict_proba(X_test)[:,1]\n    #Compute false positive rate, true positive rate, and the subsequent threshold\n    fpr_rf, tpr_rf, thresh_rf = skm.roc_curve(y_test, predict)\n    #Append AUC to list\n    AUC.append(skm.auc(fpr_rf,tpr_rf))\n    #Compute costs for false positives and negatives for shifting thresholds\n    totalcost = fpc*fpr_rf + [fnc*(1-x) for x in tpr_rf]\n    #Locate minimum total cost \n    minpos = np.argmin(totalcost)\n    #Locate the cutoff value\n    cutoff = thresh_rf[minpos]\n    #Apply the cutoff to determine T\/F\n    predict = [True if x > cutoff else False for x in predict]\n    #Isolate 'accuracy' and append to the accuracy measure list for comparison to other prediction methods.\n    accuracy.append(confusionMatrixInfo(predict, y_test)['accuracy'])\n    \nt1 = DT.datetime.now() \nprint('Computing performance measures took '+ str(t1-t0))\n#Computing performance measures took 0:00:41.871032","6c331dc2":"#Look at feature importances. Must specify the digit in modellist[i]\nfeature_importances = pd.DataFrame(modellist[9].feature_importances_,\n                                   index = X_train.columns,\n                                    columns=['importance'])\\\n                                   .sort_values('importance',\n                                                ascending=False)\n\n#Plot top twenty important features\nplt.plot(feature_importances[:20])","e11fdda4":"#Plot one tree\nfrom graphviz import Source\nfrom sklearn import tree as treemodule\nSource(treemodule.export_graphviz(\n        modellist[1].estimators_[1]\n        , out_file=None\n        , feature_names=X_train.columns\n        , filled = True\n        , proportion = True\n        )\n)\n","f7eaf6c1":"testX = test.drop(columns = {'label'})\ntestY = test['label']  ","c44d6d8f":"globalpred = {}\nfor i in digit:          \n    globalpred[i] = modellist[i].predict_proba(testX)[:,1]","ee200965":"predValue = pd.DataFrame(globalpred).idxmax(axis = 1)","5c83c508":"cfmatrix = confusion_matrix(testY, predValue)","f3408d70":"tptn = 0\nfor i in digit:\n    tptn += cfmatrix[i,i]\naccuracy = tptn\/len(testY)","02990d29":"target_names = ['0','1','2','3','4','5','6','7','8','9']\nreport = classification_report(testY, predValue, target_names = target_names)","a818ff22":"#Expansion of Results for (3)\nprint('Global Confusion Matrix: ' + '\\n'  + str(cfmatrix) + '\\n')\n#Global Confusion Matrix: \n#   0    1   2   3   4   5   6   7   8   9\n#[[794   0   0   0   4   0   3   0   6   0]\n# [  0 862   5   4   3   2   2   3   2   0]\n# [  6   2 785   8  11   0  16  14   7   9]\n# [  1   6  22 795   2  16   4   8  19  11]\n# [  3   2   1   0 782   1   4   1   4  34]\n# [ 15   5   2  14   3 711  16   1   6   8]\n# [ 13   3   1   0   3   3 772   0  12   0]\n# [  6   6   9   1   7   0   1 794   5  31]\n# [  0  11   4  18   3   8   7   3 775  18]\n# [  3   2   2  18  33   0   0  17  13 753]]","f1d57303":"print('Global Accuracy: ' + str(accuracy) + '\\n')","caccc730":"print('Classification Report: ' + '\\n' + str(report))\n#Classification Report: \n#              precision    recall  f1-score   support\n#\n#           0       0.94      0.98      0.96       807\n#           1       0.96      0.98      0.97       883\n#           2       0.94      0.91      0.93       858\n#           3       0.93      0.90      0.91       884\n#           4       0.92      0.94      0.93       832\n#           5       0.96      0.91      0.93       781\n#           6       0.94      0.96      0.95       807\n#           7       0.94      0.92      0.93       860\n#           8       0.91      0.91      0.91       847\n#           9       0.87      0.90      0.88       841\n#\n#   micro avg       0.93      0.93      0.93      8400\n#   macro avg       0.93      0.93      0.93      8400\n#weighted avg       0.93      0.93      0.93      8400","b1c53766":"rawTest = pd.read_csv('..\/input\/test.csv')\nsub = pd.DataFrame()\n\nfor i in digits:\n    bifurcateDigits(rawTest, i, train = False)\n    predictions = svc.predict(rawTest)\n    sub = pd.concat([sub, pd.DataFrame(predictions)])\n","555727b1":"### 3.3.2 Specify potential parameters to apply ","5f30a89c":"### 3.2 Supported Vector Machine (SVM)","13435f92":"#### when the digit is 3 ","3dc548de":"#### when the digit is 4","f65d9730":"### 3.3.4 Performance","7ac6a79a":" AUC(digits 0-9) \n>  0:  0.9984546050087215,\n>  1:  0.9990136690458005\n>  2:  0.9930195552950043\n>  3:  0.9921168806626702\n>  4:  0.9935744481811293\n>  5:  0.9888587281263337\n>  6:  0.998162574290247\n>  7:  0.9974924286627211\n>  8:  0.9937558339264936\n>  9:  0.9902653826553945\n\nAccuracy(digits 0-9)\n>  0:  0.9830402010050251\n>  1:  0.9868043602983362\n>  2:  0.9616066154754873\n>  3:  0.9610538373424972\n>  4:  0.9628953771289538\n>  5:  0.9477465708687133\n>  6:  0.9773869346733668\n>  7:  0.9781968179139658\n>  8:  0.9630512514898689\n>  9:  0.9493975903614458\n> \nAverage Accuracy: 0.9671","821f0531":"* Purpose: View 28x28 image of pixels for specified digit.\n* Input: Dataframe of pixel values, some threshold\n* Output: Image of pixels with varying magnitude for handwritten digit with the optional adjustment by threshold","b756077d":"#### when the digit is 9 ","9cbbda06":"### Generate dictionary of final probabalistic predictions","0bf42566":"Randomized Search of Random Forest for all digits took '0:08:01.855425'\nHowever, this is wrong for unknown reasons. It took closer to 2.5 hours\nRan w\/0 (n_jobs = -1)","1510c963":"### 2.2 Bifurcation of data","ae02aa87":"### 3.3.1 View info on parameters ","70f656ca":"### Calculate the global accuracy","536ae064":"### 3.3.3 Model Training","03c799e2":"#### When the digit is 0","5e31438c":"### 1.1 Define ConfusionMatrix Function","9567c016":"* Purpose: Create train\/test dataframe of ~50\/50 sample for the value to be predicted.\n* Philosophy: This will lower the no information rate which might allow us to select better model parameters.\n* Inputs: (1a) training series or (1b) test series, (2) the value identifyer, (3) T\/F is this a training series?\n* Outputs: (1) Array of T\/F for the specified value, (2) Dataframe of predictors X associated with (1)","f784bff5":"### 3.1.3 Overview of results","74a1951a":"#### when the digit is 6 ","56ae74fb":"### 3.3.6 Plotting a tree","8c85f220":"* Purpose: Increase comparability b\/w digit data\n* Philosophy: Digits with homogenized pixel magnitude\/variance will improve model performance.\n* Input: train\/test dataframe\n* Output: Scaled dataframe","82494a9c":"* Write predictions to output folder\n\n### Show the confusion matrix","66c5e5b0":"## 3. Training Models \n### 3.1 Gradient Boosting","f9d02cc4":"#### when the digit is 5 ","a0d1f3e7":"# Hand-written Digit Recognition \n\n## 1. [Importing and Processing](https:\/\/www.kaggle.com\/sherrytp\/digit-recognition) \n### 1.0 import needed packages \n### 1.1 define ConfusionMatrix Function\n### 1.2 Import Data \n### 1.3 Split train and test data \n\n## 2. [Getting Start](https:\/\/www.kaggle.com\/sherrytp\/digit-recognition\/)\n### 2.1 Scaling the data \n### 2.2 Bifurication of data \n### 2.3 View digits \n\n## 3. [Training Models](https:\/\/www.kaggle.com\/sherrytp\/digit-recognition\/) \n### 3.1 Boosting \n### 3.2 SVM\n### 3.3 Random Forest \n\n## 4. [Making Predictions using Random Forest](https:\/\/www.kaggle.com\/sherrytp\/digit-recognition\/) ","82207cda":"* Purpose: Find scoring values to compare with GradientBoosting and SVM model.\n* Output: A list of AUC and Accuracy scores for each digit.","92cb58c5":"### 3.3 Random Forest","93a5a933":"> Global Accuracy: 0.9313","9263182a":"## 1. Importing and Preprocessing \n\n### 1.0 Import needed packages ","eeea9014":"Purpose: Identifies 'best' parameters for each RF to predict digits through a randomized search via cross validation\nOutput: A list of the best local models given the specified search space. \nNote: This trains CV = 3 * n_iter = 50 * digits = 10 = 1,500 random forests. ","ae19a030":"> Average Accuracy: 0.9677","67ae8ca1":"### 2.3 View digits ","4a248fef":"### 1.3 Split train and test data ","da94afb1":"> Accuracy measure","83d45d79":"### 3.3.5 Results Per Model","6c6ebe83":"#### when the digit is 8 ","f0cb78b6":"### 1.2 Import Data","582f418e":"## 4. Making Predictions using Random Forest","1841c16d":"### Make Global Predictions","91bde8df":"### Set to dataframe\n* Select value with highest predictive confidence and store in predValue","3db21352":"#### when the digit is 2 ","3dc41c49":"### Confusion Matrix for 3","e514c0da":"### 3.1.1 Split up the files","7876c22c":"> This model performs slightly better than the gradient boosting model and the SVM","e94b2227":"#### when the digit is 7 ","de1c4e4c":"### Summary of Results:\n(1) Training RF on biased (90\/10) and unscaled data with non-optimized parameters: Accuracy = 0.893\n\n(2) Training RF on unbiased (50\/50) and scaled data with non-optimized parameters: Accuracy = 0.907\n\n(3) Training RF on unbiased (50\/50) and scaled data with locally optimized parameters: Accuracy = 0.931","2a907e44":">  AUC(digits 0-9)             \n>  0: 0.9986819154058043\n\n>  1: 0.9988793489425584       \n\n>  2: 0.9938654160211046      \n\n>  3: 0.991160197793199        \n\n>  4: 0.9930134520651762       \n\n>  5: 0.9924250960307299       \n\n>  6: 0.9978484439498339       \n\n>  7: 0.9972701508710511\n\n>  8: 0.9918733492740712\n\n>  9: 0.9853639556374396\n---------------------\n>  Accuracy(Digits 0-9)\n\n>  0: 0.9855527638190955\n\n>  1: 0.9873780837636259\n\n>  2: 0.9686946249261665\n\n>  3: 0.9541809851088202\n\n>  4: 0.9635036496350365\n\n>  5: 0.9601567602873938\n\n>  6: 0.9798994974874372\n\n>  7: 0.9817324690630524\n\n>  8: 0.9564958283671037\n\n>  9: 0.9397590361445783","8ee58578":"## 2. Getting Started\n### 2.1 Scaling the data","f6c601d7":"### 3.1.2 Setting paramaters","997f93c1":"### Call classification report","8d050a4d":"#### When the digit is 1"}}