{"cell_type":{"eb4d3daa":"code","65b0d87e":"code","ea177eab":"code","f14aa52b":"code","5954eb16":"code","2c22069a":"code","59a18df3":"code","673b21ae":"code","f84a2813":"markdown","d0571c67":"markdown","30771892":"markdown","4b7b4e61":"markdown","227664d2":"markdown","77bac351":"markdown","00c0c8eb":"markdown","ba11ec7d":"markdown"},"source":{"eb4d3daa":"# Helper packages\nimport pandas as pd\nimport json\nimport glob\nfrom tqdm import tqdm\nimport pickle\n\n# NLP related packages\nimport re\nimport string\nfrom nltk.tokenize import word_tokenize,sent_tokenize\nfrom nltk.corpus import stopwords\nimport nltk","65b0d87e":"# Basic text cleaning\ndef text_cleaning(text):\n    text = re.sub(\"\\s+\",\" \", text) # remove extra spaces\n    text = ''.join([k for k in text if k not in string.punctuation]) # remove punctuation\n    text = re.sub('[^A-Za-z0-9]+', ' ', str(text).lower()).strip() # remove unnecessary literals and Perform Case Normalization\n    return text","ea177eab":"##### STEP 1: Make a list of the known labels provided to us\ndf_train = pd.read_csv(\"..\/input\/coleridgeinitiative-show-us-the-data\/train.csv\")\n\ntemp_1 = [text_cleaning(x) for x in df_train['dataset_label']]\ntemp_2 = [text_cleaning(x) for x in df_train['dataset_title']]\ntemp_3 = [text_cleaning(x) for x in df_train['cleaned_label']]\n\nexisting_labels = set(temp_1 + temp_2 + temp_3)","f14aa52b":"count_BIO = 0\ncount_O = 0\n\n# Get all the files containing text\ntrain_files = glob.glob(\"..\/input\/coleridgeinitiative-show-us-the-data\/train\/*\")\n\nX = []\ny = []\n\nfor file in tqdm(train_files):\n    text = \"\"\n    \n    # For each file, merge the text of all sections\n    with open(file,\"r\") as f:\n        json_file = json.load(f)\n        for section in json_file:\n            text += \" \" + section[\"text\"]\n            \n    # Tokenize sentencewise\n    sentences = sent_tokenize(text)\n    \n    ##### STEP 2: For each sentence in our dataset\n    for sentence in sentences:\n        lab_len = 0\n        max_lab = \"\"\n        clean_sent = text_cleaning(sentence)\n        tokenized_sent = word_tokenize(clean_sent)\n        bio_labels = []\n        \n        ##### STEP 2 i): Out of all matching labels in the sentence, find the label with maximum length\n        for clean_lab in existing_labels:\n            if clean_lab in clean_sent and lab_len < len(clean_lab):\n                lab_len = len(clean_lab)\n                max_lab = clean_lab\n        \n        ##### STEP 2 ii): Use the selected label of maximum length to annotate the sentence using BIO tokens\n        if lab_len > 0:\n            tokenized_lab = word_tokenize(max_lab)\n            en = 0\n            for word in tokenized_sent:\n                if en < len(tokenized_lab) and word == tokenized_lab[en]:\n                    if en == 0:\n                        bio_labels.append('B')\n                    else:\n                        bio_labels.append('I')\n                    en += 1\n                else:\n                    bio_labels.append('O')\n\n            count_BIO += 1\n        ##### STEP 2 iii): If no label was found then annotate the sentence using only O token\n        else:\n            for word in tokenized_sent:\n                bio_labels.append('O')\n            count_O += 1\n            \n        y.append(bio_labels)\n        X.append(tokenized_sent)","5954eb16":"print(f\"There are {count_O} sentences with only O labels and {count_BIO} sentences with BIO labels\")","2c22069a":"##### STEP 3: Pickle the resulting dataset\nwith open('.\/bio_labels.pkl', 'wb') as f:\n    pickle.dump(y, f)\n\nwith open('.\/sentences.pkl', 'wb') as f:\n    pickle.dump(X, f)","59a18df3":"X = []\ny = []\n\nwith open('.\/sentences.pkl', 'rb') as f:\n    X = pickle.load(f)\n    \nwith open('.\/bio_labels.pkl', 'rb') as f:\n    y = pickle.load(f)","673b21ae":"print(X[0])\nprint(y[0])","f84a2813":"<div style=\"color:#FFFFFF; background:#34b1eb; border-radius:5px;\"> \n    <center><br> <h1>BIO Labeling for Coleridge Dataset<\/h1> <br><\/center>\n<\/div>","d0571c67":"I found a very nice <a href=\"https:\/\/link.springer.com\/content\/pdf\/10.1007%2F978-3-030-50417-5_23.pdf\">paper<\/a> where the authors are doing something very similar to what this competition is asking for. It seemed a very good starting point to me.\n\nTo summarize their approach :\n\n1. **Create BIO (B:Begin, I:Inside, and O:Outside) labeling for your dataset**, for eg- label this sentence [\"We\", \"are\", \"using\", \"COVID\", \"19\", \"Dataset\", \"in\", \"our\", \"paper\"] as [\"O\", \"O\", \"O\", \"B\", \"I\", \"I\", \"O\", \"O\", \"O\"]. Here, B is first word of a dataset, I is word that is a part of a dataset and O is any other word in our dataset.\n\n2. **Experiment with various word to vector conversion algorithms (word embedding algorithms)**, for eg - Word2Vec, Bag of Words etc.\n\n3. **Train Bi-directional LSTM to predict BIO labels for a given sentence**.\n\n4. It may output sequences (e.g., \u201cOIO\u201d) that are invalid under the BIO labeling scheme. **To penalize such invalid label sequences, add a Conditional Random Field (CRF) layer** on top of the Bi-LSTM network.\n\nIn this notebook I have tried to label coleridge dataset using BIO labeling scheme.","30771892":"We can use Named Entity Recognition based approach as mentioned in the figure above. For this approach 1st step is to label our dataset using BIO scheme. You can read the <a href=\"https:\/\/link.springer.com\/content\/pdf\/10.1007%2F978-3-030-50417-5_23.pdf\">paper<\/a> to explore on the next steps.\n\nAlso, thanks to @nbroad for pointing out bugs in the code","4b7b4e61":"### Fig: SciNER Paper\n\n![Screenshot from 2021-05-17 23-57-54.png](attachment:16d854e7-1c13-45b2-ab01-130b5f262b45.png)","227664d2":"<center>\n<h2>Named Entity recognition using B I O Labeling in NLP<\/h2>\n<img src=\"https:\/\/miro.medium.com\/max\/700\/0*NoDvGqvXTfaDoGmg\"\/>\n<br>\n<\/center>","77bac351":"<div style=\"color:#FFFFFF; background:#34b1eb; border-radius:5px;\"> \n    <center><br> <h1>Introduction<\/h1> <br><\/center>\n<\/div>","00c0c8eb":"### So how can we label our dataset under BIO scheme? Following is an approach I came up with. \n\n**Old approach -**\n\n1. Make a list of the known labels provided to us\n2. For each sentence in our dataset<br>\n    i. Check if any known label is present in the sentence<br>\n    ii. If present then label it using B I O for each word accordingly<br>\n    iii. If not present then label all words as O\n3. Pickle the resulting dataset\n\nThe problem with the above algorithm is that there might be many labels present in a sentence. We should always select the largest label present in the sentence.\n\n**For eg -** \n<br>We have two labels 1) Some Covid Related Dataset (SCRD) 2) (SCRD)\n<br>And our sentence is - \n<br> ['We', 'have', 'used', 'Some', 'Covid', 'Related', 'Dataset', '(SCRD)', 'in', 'our', 'reseach', 'paper'].\n<br>Ideally this sentence should be annotated as \n<br>[O, O, O, B, I, I, I, I, I, O, O, O, O]\n<br>But there is a chance that our algorithm would annotate it as \n<br>[O, O, O, O, O, O, O, O, B, O, O, O, O], because (SCRD) is a valid label.\n<br> To solve this issue we should tweak our algorithm a little, as follows\n\n**New approach -**\n\n1. Make a list of the known labels provided to us\n2. For each sentence in our dataset<br>\n    i. Out of all matching labels in the sentence, find the label with maximum length<br>\n    ii. Use the selected label of maximum length to annotate the sentence using BIO tokens<br>\n    iii. If no label was found then annotate the sentence using only O token\n3. Pickle the resulting dataset\n\n**You can find the resulting dataset in the output of this notebook.**","ba11ec7d":"<div style=\"color:#FFFFFF; background:#34b1eb; border-radius:5px;\"> \n    <center><br> <h1>Conclusion<\/h1> <br><\/center>\n<\/div>\n<\/br>"}}