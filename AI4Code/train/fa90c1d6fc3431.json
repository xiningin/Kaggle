{"cell_type":{"7cb93d6b":"code","ff3124bc":"code","cb31b633":"code","91cd8433":"code","05db769f":"code","0d1a875d":"code","f7cec075":"code","a0e97723":"code","6230e75e":"code","61d71007":"code","4f09b48d":"code","b1cc7d83":"code","562fdb0e":"markdown"},"source":{"7cb93d6b":"%%time\nimport sys\n!cp -f ..\/input\/rapids\/rapids.21.06 \/opt\/conda\/envs\/rapids.tar.gz\n!cd -f \/opt\/conda\/envs\/ && tar -xzvf rapids.tar.gz\nsys.path = [\"\/opt\/conda\/envs\/rapids\/lib\"] + [\"\/opt\/conda\/envs\/rapids\/lib\/python3.7\"] + [\"\/opt\/conda\/envs\/rapids\/lib\/python3.7\/site-packages\"] + sys.path\n!cp -f \/opt\/conda\/envs\/rapids\/lib\/libxgboost.so \/opt\/conda\/lib\/","ff3124bc":"# Importing core libraries\nimport numpy as np\nimport pandas as pd\nfrom time import time\nimport os\nimport pprint\nimport joblib\nfrom functools import partial\n\n# Suppressing warnings because of skopt verbosity\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n# Regressors\nimport lightgbm as lgb\n\n# Model selection\nfrom sklearn.model_selection import KFold, StratifiedKFold\n\n# Metrics\nfrom sklearn.metrics import mean_squared_error\n\n# Data processing\nfrom sklearn.preprocessing import OrdinalEncoder\nfrom sklearn.preprocessing import OneHotEncoder\n\n# \nimport cudf, cuml\nimport cupy as cp\nfrom cuml.manifold import TSNE, UMAP\nimport matplotlib.pyplot as plt\nfrom matplotlib.pyplot import ylim, xlim\n%matplotlib inline","cb31b633":"# Loading data \nX = pd.read_csv(\"..\/input\/30-days-of-ml\/train.csv\")\nX_test = pd.read_csv(\"..\/input\/30-days-of-ml\/test.csv\")","91cd8433":"# Preparing data as a tabular matrix\ny = X.target\nX = X.set_index('id').drop('target', axis='columns')\nX_test = X_test.set_index('id')","05db769f":"# Stratifying the target\ny_stratified = pd.cut(y, bins=10, labels=False)","0d1a875d":"# Dealing with categorical data\ncategoricals = [item for item in X.columns if 'cat' in item]\nnumeric = [item for item in X.columns if 'cat' not in item]\nordinal_encoder = OrdinalEncoder()\nX[categoricals] = ordinal_encoder.fit_transform(X[categoricals]).astype(int)\nX_test[categoricals] = ordinal_encoder.transform(X_test[categoricals]).astype(int)","f7cec075":"ohe = OneHotEncoder(sparse=False)\nH = np.hstack((ohe.fit_transform(X[categoricals]), X[numeric].values))","a0e97723":"tsne = TSNE(n_components=2, perplexity=10, n_neighbors=100)\nprojection_2D = tsne.fit_transform(H)","6230e75e":"plt.figure(figsize=(15, 15))\nplt.scatter(projection_2D[:,0], projection_2D[:,1],\n            c=y_stratified.values, \n            edgecolor='none', \n            alpha=0.80, \n            s=10)\nplt.axis('off')\nplt.show();","61d71007":"umap = UMAP(n_components=2, n_neighbors=50)\nprojection_2D = umap.fit_transform(H)","4f09b48d":"plt.figure(figsize=(15, 15))\nplt.scatter(projection_2D[:,0], projection_2D[:,1],\n            c=y_stratified.values, \n            edgecolor='none', \n            alpha=0.80, \n            s=10)\nplt.axis('off')\nplt.show();","b1cc7d83":"Ht = np.hstack((ohe.transform(X_test[categoricals]), X_test[numeric].values))\n_ = pd.DataFrame(umap.transform(H), columns = ['umap_0', 'umap_1'], index=X.index).reset_index().to_csv('umap_train.csv', index=False)\n_ = pd.DataFrame(umap.transform(Ht), columns = ['umap_0', 'umap_1'], index=X_test.index).reset_index().to_csv('umap_test.csv', index=False)","562fdb0e":"# T-SNE & UMAP\nT-SNE (https:\/\/lvdmaaten.github.io\/tsne\/) and UMAP (https:\/\/github.com\/lmcinnes\/umap) are two technicalities, often used by data scientists, that allow to project multivariate data into lower dimensions. They are often used to find clusters in data. I used the fast t-SNE and UMAP implementations offered by Rapids (they require GPU access). \n\nAs stated by the article \"How to t-SNE Effectively\" (https:\/\/distill.pub\/2016\/misread-tsne\/), it is easy to see clusters where there are not, and in our data there are no clear macro clusters, probably. What can be inferedded by t-SNE and UMAP projects, though, is that there are many local clusters, just like in mixture of (this is what can be read from t-SNE). Also there are probably a few outliers, given the noise injected into the artificial data (and this can be read from UMAP, where a few central clusters are surronded by very small scattered ones).\n\nI wonder if, given such evidence, gaussian mixtures may prove a good feature engineering approach."}}