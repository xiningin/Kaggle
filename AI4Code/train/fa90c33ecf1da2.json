{"cell_type":{"9781f82f":"code","62b4d3c5":"code","d19f6434":"code","6afd9d87":"code","8648b2e2":"code","b49d6086":"code","7a816cc0":"code","0d7546cc":"code","802303f8":"code","b01aaff1":"code","4334ad00":"code","b3de614d":"code","5e230d29":"code","8825aa32":"code","fed54ad1":"code","d5300eff":"code","fca0f30e":"code","f6291a0c":"markdown","4d55c1b4":"markdown","8868c227":"markdown","ad823d95":"markdown"},"source":{"9781f82f":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","62b4d3c5":"import matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split","d19f6434":"# Data loading \n\ntrain=pd.read_csv('\/kaggle\/input\/digit-recognizer\/train.csv')\ntest_images=pd.read_csv('\/kaggle\/input\/digit-recognizer\/test.csv')\n\n#Splitting the Training part into X_Train and Y_train\n\n#X_Train Here we are dropping the label column, axis=1 will perform the action along column \ntrain_images=train.drop('label',axis=1)\n\n#Y_Train\ntrain_labels=train['label']\n\ntrain_images.head()","6afd9d87":"# Checking for the missing values \n\ntrain_images.isnull().any().describe()","8648b2e2":"# Data Normalization by performing a grayscale normalization. Moreover the CNNs converge faster on [0..1] data\n\ntrain_images=train_images\/255.0\ntest_images=test_images\/255.0","b49d6086":"# We know that train images have been stocked as a 1D vector of 784 values. We have to reshape all the data to 28x28x1, i.e., 3D matrices \n# Reshape image in 3 dimensions (height = 28px, width = 28px , canal = 1 (as it is gray scale))\n\ntrain_images=train_images.values.reshape(len(train_images),28,28,1)\ntest_images=test_images.values.reshape(len(test_images),28,28,1)\n\nX_train, X_val, Y_train, Y_val = train_test_split(train_images, train_labels, test_size = 0.1, random_state=2)","7a816cc0":"# Using the matplot library to view the dataset\n\nplt.imshow(train_images[78][:,:,0])","0d7546cc":"import tensorflow as tf\nfrom tensorflow import keras","802303f8":"def build_model(hp):\n    model=keras.Sequential([\n        keras.layers.Conv2D(\n            filters=hp.Int('conv_1_filter', min_value=32, max_value=128, step=16),   # hp.Int will choose the value in range[min_value,max_value]\n            kernel_size=hp.Choice('conv_1_kernel', values = [3,5,7]),                # hp.Choice will choose the best values from values\n            activation='relu',\n            input_shape=(28,28,1)                                                    # This is the shape of input image\n        ),\n         keras.layers.Conv2D(\n            filters=hp.Int('conv_2_filter', min_value=32, max_value=128, step=16),\n            kernel_size=hp.Choice('conv_2_kernel', values = [3,5,7]),\n            activation='relu'\n         ),\n         keras.layers.Flatten(),\n         keras.layers.Dense(\n             units=hp.Int('dense_1_units', min_value=32, max_value=128, step=16),\n             activation='relu'\n             ),\n        keras.layers.Dense(10, activation='softmax')     \n        ])\n    model.compile(optimizer=keras.optimizers.Adam(hp.Choice('learning_rate', values=[1e-2, 1e-3])),\n              loss='sparse_categorical_crossentropy',\n              metrics=['accuracy'])\n  \n    return model","b01aaff1":"from kerastuner import RandomSearch\nfrom kerastuner.engine.hyperparameters import HyperParameters","4334ad00":"tuner_search=RandomSearch(build_model,\n                          max_trials=5,\n                          objective='val_accuracy')","b3de614d":"# This step is really amazing it will display the accuracy for number of trials and the hyperparameters used for each trials.\n\ntuner_search.search(train_images,train_labels,epochs=3,validation_split=0.1,verbose=2)","5e230d29":"# When search is over, you can retrieve the best model(s):\n\nmodel=tuner_search.get_best_models(num_models=1)[0]\nmodel.summary()","8825aa32":"from keras.preprocessing.image import ImageDataGenerator\nfrom keras.callbacks import ReduceLROnPlateau\n\ndatagen = ImageDataGenerator(\n        featurewise_center=False,  # set input mean to 0 over the dataset\n        samplewise_center=False,  # set each sample mean to 0\n        featurewise_std_normalization=False,  # divide inputs by std of the dataset\n        samplewise_std_normalization=False,  # divide each input by its std\n        zca_whitening=False,  # apply ZCA whitening\n        rotation_range=10,  # randomly rotate images in the range (degrees, 0 to 180)\n        zoom_range = 0.1, # Randomly zoom image \n        width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)\n        height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)\n        horizontal_flip=False,  # randomly flip images\n        vertical_flip=False)  # randomly flip images\n\n\ndatagen.fit(train_images)","fed54ad1":"# Now our best model is prepared and we will fit our Augmented training Data\n\nmodel.fit_generator(datagen.flow(train_images,train_labels, batch_size=80),\n                              epochs = 5, validation_data = (X_val,Y_val),\n                              verbose = 2\n                              )","d5300eff":"# Predidtions for the test_ images\ntest_pred = pd.DataFrame(model.predict(test_images, batch_size=200))\ntest_pred = pd.DataFrame(test_pred.idxmax(axis = 1))\ntest_pred.index.name = 'ImageId'\ntest_pred = test_pred.rename(columns = {0: 'Label'}).reset_index()\ntest_pred['ImageId'] = test_pred['ImageId'] + 1\n\ntest_pred.head()\n","fca0f30e":"#Converting to csv format\ntest_pred.to_csv('\/kaggle\/working\/DR_submission.csv', index = False)","f6291a0c":"Building Model\n\nI will be using Keras for building CNN model and using hyperparameters with the help of Keras Tuner for optimization.\nHere is the Link to documentation of Keras Tuner on how to use Hyperparameters https:\/\/keras-team.github.io\/keras-tuner\/","4d55c1b4":"Start the search for the best hyperparameter configuration. The call to search has the same signature as model.fit()\n\nHere's what happens in search: models are built iteratively by calling the model-building function, which populates the hyperparameter space (search space) tracked by the hp object. The tuner progressively explores the space, recording metrics for each configuration.","8868c227":"# Data Augmentation \n\nData Augmentation is a method of artificially creating a new dataset for training from the existing training dataset to improve the performance of deep learning neural networks with the amount of data available. It is a form of regularization which makes our model generalize better than before.\nHere we have used a Keras ImageDataGenerator object to apply data augmentation for randomly translating, resizing, rotating, etc the images. Each new batch of our data is randomly adjusting according to the parameters supplied to ImageDataGenerator.\n\nFor the data augmentation, i choosed to :\n1. Randomly rotate some training images by 10 degrees\n2. Randomly Zoom by 10% some training images.\n3. Randomly shift images horizontally by 10% of the width.\n4. Randomly shift images vertically by 10% of the height\n\nI did not apply a vertical_flip nor horizontal_flip since it could have lead to misclassify symetrical numbers such as 6 and 9.\n\n* Before Data Augmentation- Accuracy 98.7\n* After Data Augmentation - Axxuracy 99.8","ad823d95":"Next, we will instantiate a tuner. You should specify the model-building function, the name of the objective to optimize (whether to minimize or maximize is automatically inferred for built-in metrics), the total number of trials (max_trials) to test, and the number of models that should be built and fit for each trial (executions_per_trial).\n\nAvailable tuners are RandomSearch and Hyperband.  We will be using RandomSearch\n\nNote: the purpose of having multiple executions per trial is to reduce results variance and therefore be able to more accurately assess the performance of a model. If you want to get results faster, you could set executions_per_trial=1 (single round of training for each model configuration)."}}