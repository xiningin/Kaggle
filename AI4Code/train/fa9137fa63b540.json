{"cell_type":{"7ebf6cc4":"code","34a6e412":"code","16cee52b":"code","70bbaa54":"code","843911b4":"code","75b86fa8":"code","3d8547a1":"code","df4cb3d6":"code","0bac37f3":"code","36ff2de2":"code","30a5aa45":"code","4a9803d8":"code","78ece821":"code","f4f2afdf":"code","94e0106c":"code","4942c0ef":"code","a1c712eb":"code","4f43eac2":"code","31e2361c":"code","1eac8656":"code","b7512733":"code","d9861e16":"markdown","5daad98d":"markdown","d41b9bc7":"markdown","125677f7":"markdown","c1961a31":"markdown","e7b69106":"markdown","7ad253ae":"markdown","14d64b34":"markdown","adbc545b":"markdown"},"source":{"7ebf6cc4":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","34a6e412":"debug_mode = False","16cee52b":"!rm -rf \/kaggle\/working\/new_train && mkdir -p \/kaggle\/working\/new_train && mkdir -p \/kaggle\/working\/new_train\/images","70bbaa54":"def remove_horizontal_lines(image):\n  gray = cv2.cvtColor(image,cv2.COLOR_BGR2GRAY)\n  thresh = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)[1]\n\n  # Remove horizontal\n  horizontal_kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (15,1))\n  detected_lines = cv2.morphologyEx(thresh, cv2.MORPH_OPEN, horizontal_kernel, iterations=2)\n  cnts = cv2.findContours(detected_lines, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n  cnts = cnts[0] if len(cnts) == 2 else cnts[1]\n  for c in cnts:\n      cv2.drawContours(image, [c], -1, (255,255,255), 2)\n\n  # Repair image\n  repair_kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (1,3))\n  result = 255 - cv2.morphologyEx(255 - image, cv2.MORPH_CLOSE, repair_kernel, iterations=1)\n\n  return result\n\n  \ndef remove_vertical_lines(image):\n  gray = cv2.cvtColor(image,cv2.COLOR_BGR2GRAY)\n  thresh = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)[1]\n\n  # Remove horizontal\n  vertical_kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (1,15))\n  detected_lines = cv2.morphologyEx(thresh, cv2.MORPH_OPEN, vertical_kernel, iterations=2)\n  cnts = cv2.findContours(detected_lines, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n  cnts = cnts[0] if len(cnts) == 2 else cnts[1]\n  \n  for c in cnts:\n      cv2.drawContours(image, [c], -1, (255,255,255), 2)\n\n  # Repair image\n  repair_kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (3,1))\n  result = 255 - cv2.morphologyEx(255 - image, cv2.MORPH_CLOSE, repair_kernel, iterations=1)\n\n  return result\n\ndef remove_lines(image):\n  return remove_vertical_lines(remove_horizontal_lines(image))","843911b4":"!pip install imutils","75b86fa8":"# load the input image from disk, convert it to grayscale, and blur\n# it to reduce noise\nfrom imutils.contours import sort_contours\nimport numpy as np\nimport argparse\nimport imutils\nimport cv2\nimport pandas as pd\n\ndef join_contours(thresh_gray, contours):\n  #Join near morphology\n  thresh_gray = cv2.morphologyEx(thresh_gray, cv2.MORPH_CLOSE, cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (13, 13)));\n  \n  contours = cv2.findContours(thresh_gray, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n  contours = contours[0] if len(contours) == 2 else contours[1]\n\n  sorted_cnts = sorted(contours, key=lambda x: cv2.contourArea(x), reverse=True)\n\n  \n  new_cnts = []\n\n  count = 0\n  for i in range(len(sorted_cnts)):\n    new_cnts.append(sorted_cnts[i])\n    if count >= 3:\n      break\n    count += 1\n    \n  if debug_mode:\n    print(\"LEN NEW_CNTS:\", len(new_cnts))\n\n  if debug_mode:\n    for i in range(len(new_cnts)):\n      (x, y, w, h) = cv2.boundingRect(new_cnts[i])\n      roi = thresh_gray[y:y + h, x:x + w]\n      print(\"BOX, \", x, y, w, h, \" AREA \", cv2.contourArea(new_cnts[i]))\n      cv2_imshow(roi)\n\n  for idx, c in enumerate(sorted_cnts):\n    if idx >= 4:\n      cv2.fillPoly(thresh_gray, pts=[c], color=0) \n  if debug_mode:\n    print(\"THRESH_GRAT AT THE END OF JOIN#############\")\n    cv2.imshow(thresh_gray)\n\n  thresh_gray = cv2.morphologyEx(thresh_gray, cv2.MORPH_CLOSE, cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (9, 9)));\n\n  return thresh_gray\n\ndef get_digits_from_image(image):\n  if debug_mode:\n    cv2.imshow(image)\n  image = remove_lines(image)\n  # Crop a bit\n  if debug_mode:\n    print(image.shape)\n  image = image[int(0.05*image.shape[0]):int(0.95*image.shape[0]), int(0.05*image.shape[1]):int(0.95*image.shape[1])]\n\n  gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n  blurred = cv2.GaussianBlur(gray, (5, 5), 0)\n\n  # perform edge detection, find contours in the edge map, and sort the\n  # resulting contours from left-to-right\n  edged = cv2.Canny(blurred, 30, 150)\n  cnts = cv2.findContours(edged.copy(), cv2.RETR_EXTERNAL,\n    cv2.CHAIN_APPROX_SIMPLE)\n  cnts = imutils.grab_contours(cnts)\n  try:\n    old_cnts = []\n    if len(cnts) == 0:\n      return None\n    cnts = sort_contours(cnts, method=\"left-to-right\")[0]\n    old_cnts = cnts[:]\n    # initialize the list of contour bounding boxes and associated\n    # characters that we'll be OCR'ing\n\n    #Join close enough contours\n    debug_edged = edged.copy()\n    if len(cnts) > 1:\n      joined = join_contours(edged, cnts)\n      cnts = cv2.findContours(joined, cv2.RETR_EXTERNAL,\n        cv2.CHAIN_APPROX_SIMPLE)\n      cnts = cnts[0] if len(cnts) == 2 else cnts[1]\n      if len(cnts) == 0:\n        return None\n      cnts = sort_contours(cnts, method=\"left-to-right\")[0]\n  except Exception as e:\n    print(\"Exception: \", e, \" cnts len: \", len(cnts), \"olds \", len(old_cnts))\n    cv2.imshow(image)\n    cv2.imshow(joined)\n    cv2.imshow(edged)\n    cv2.imshow(debug_edged)\n    return 0\n\n  chars = []\n\n  if debug_mode:\n    print(\"IMAGE\")\n    cv2.imshow(image)\n    print(\"EDGED\")\n    cv2.imshow(edged)\n    print(\"DEBUG_EDGED\")\n    cv2.imshow(debug_edged)\n    print(\"JOINED\")\n    cv2.imshow(joined)\n\n  for c in cnts:\n    # compute the bounding box of the contour\n    (x, y, w, h) = cv2.boundingRect(c)\n    # filter out bounding boxes, ensuring they are neither too small\n    # nor too large\n    if debug_mode:\n      print(\"Box: \", x, y, w, h)\n    if (w >= 8 and w <= 250) and (h >= 17 and h <= 220):\n      # extract the character and threshold it to make the character\n      # appear as *white* (foreground) on a *black* background, then\n      # grab the width and height of the thresholded image\n      roi = gray[y:y + h, x:x + w]\n      thresh = cv2.threshold(roi, 0, 255,\n        cv2.THRESH_BINARY_INV | cv2.THRESH_OTSU)[1]\n      (tH, tW) = thresh.shape\n      # if the width is greater than the height, resize along the\n      # width dimension\n      if tW > tH:\n        thresh = imutils.resize(thresh, width=32)\n      # otherwise, resize along the height\n      else:\n        thresh = imutils.resize(thresh, height=32)\n\n      # re-grab the image dimensions (now that its been resized)\n      # and then determine how much we need to pad the width and\n      # height such that our image will be 32x32\n      (tH, tW) = thresh.shape\n      dX = int(max(0, 32 - tW) \/ 2.0)\n      dY = int(max(0, 32 - tH) \/ 2.0)\n      # pad the image and force 32x32 dimensions\n      padded = cv2.copyMakeBorder(thresh, top=dY, bottom=dY,\n        left=dX, right=dX, borderType=cv2.BORDER_CONSTANT,\n        value=(0, 0, 0))\n    \n      padded = cv2.resize(padded, (32, 32))\n      # prepare the padded image for classification via our\n      # handwriting OCR model\n      padded = padded.astype(\"float32\") \/ 255.0\n      padded = np.expand_dims(padded, axis=-1)\n\n      #cv2_imshow(padded)\n      # update our list of characters that will be OCR'd\n      chars.append((padded, (x, y, w, h)))\n\n  if debug_mode:\n    for c in chars:\n      cv2.imshow(c[0] * 255)\n      print(\"APPROVED BOX: \", c[1][0], c[1][1], c[1][2], c[1][3])\n\n  if len(chars) > 4:\n    return None\n  return chars","3d8547a1":"def create_dataset(df_path, from_path, to_path):\n  df = pd.read_csv(df_path)\n\n  new_df = pd.DataFrame(columns = ['Id', 'label'])\n  new_image_id = 0\n\n  for index, row in df.iterrows():\n    img_path = from_path + row['Id']\n    image = cv2.imread(img_path)\n\n    digits = get_digits_from_image(image)\n    label = row['label']\n\n    if isinstance(digits, list) and len(digits) < 5:\n      digit = 10**(len(digits) - 1)\n      for i in range(len(digits)):\n        #Save image\n        cv2.imwrite(to_path + str(new_image_id) + '.jpg', digits[i][0] * 255)\n        mini_label = (label % (digit * 10)) \/\/ digit\n\n        digit \/\/= 10\n        new_df.loc[new_image_id] = (str(new_image_id) + '.jpg', mini_label)\n        new_image_id += 1\n    \n  new_df.to_csv('\/kaggle\/working\/new_train\/new_data.csv')\n\n\ncreate_dataset('\/kaggle\/input\/recunoasterea-scris-de-mana\/train.csv', '\/kaggle\/input\/recunoasterea-scris-de-mana\/train\/train\/', '\/kaggle\/working\/new_train\/images\/')","df4cb3d6":"!pip install git+https:\/\/github.com\/fastai\/fastai.git","0bac37f3":"from fastai.vision.all import *","36ff2de2":"df = pd.read_csv('\/kaggle\/working\/new_train\/new_data.csv')\n\ndls = ImageDataLoaders.from_df(df, path='\/kaggle\/working\/new_train\/images\/', valid_pct=0.2, fn_col='Id', label_col='label',\n                               item_tfms=Resize(224, method=ResizeMethod.Squish),\n                               batch_tfms=[Normalize.from_stats(*imagenet_stats)], bs=32)","30a5aa45":"dls.show_batch()","4a9803d8":"from torchvision import models\nfrom torchvision.models import densenet121\n\nlearner = cnn_learner(dls, densenet121, metrics=error_rate, config=cnn_config(ps=0.7))","78ece821":"#Load pretrained model\nlearner.load('\/kaggle\/input\/mymodelweights\/modelddensenet12120epochs.pkl')","f4f2afdf":"learner.show_results(max_n=2)","94e0106c":"interp = Interpretation.from_learner(learner)\ninterp.plot_top_losses(2)","4942c0ef":"def num_from_digits(digits):\n  reversed = digits[::-1]\n  sum = 0\n  for i in range(len(reversed)):\n    sum += int(reversed[i]) * 10**i\n  return sum\n\ndef test_inference(image, learner):\n  digits = get_digits_from_image(image)\n  if digits is None:\n    return 0\n  y = []\n  for d in digits:\n    arr = torch.tensor(d[0] * 255).squeeze(2)\n    res = learner.predict(arr)\n\n    if debug_mode:\n      cv2_imshow(d[0]*255)\n      print(res)\n    if res[0] == '0':\n      if torch.max(res[2]) > 0.7:\n        y.append(res[0])\n      else:\n        y.append(torch.topk(res[2], 2)[1][1])\n    else:\n      y.append(res[0])\n\n  return num_from_digits(y)","a1c712eb":"import os\nnew_df = pd.DataFrame(columns=['Id', 'Expected'])\n\nwith learner.no_bar():\n    idx = 0\n    for filename in os.listdir('\/kaggle\/input\/recunoasterea-scris-de-mana\/test\/test'):\n        image_id = int(filename.split('.')[0])\n        if image_id >= 22000 and image_id <= 24998:\n            new_df.loc[idx] = [filename, test_inference(cv2.imread('\/kaggle\/input\/recunoasterea-scris-de-mana\/test\/test\/' + filename), learner)]\n            idx += 1\n            if (idx + 1) % 500 == 0:\n                print(\"idx \", idx)","4f43eac2":"new_df = new_df.sort_values(by='Id')","31e2361c":"# Remove images from the working directory\n!rm -rf \/kaggle\/working\/new_train","1eac8656":"new_df.set_index('Id').to_csv('submission.csv')","b7512733":"new_df.head()","d9861e16":"# Model results","5daad98d":"# Inference","d41b9bc7":"# Create submission for test files","125677f7":"# Training","c1961a31":"# Create dataset","e7b69106":"# Get digits from image","7ad253ae":"# Prepare data for model","14d64b34":"# Line removal","adbc545b":"# Install fastai"}}