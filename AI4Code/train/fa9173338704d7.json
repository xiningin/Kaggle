{"cell_type":{"6ece969e":"code","c17b01a7":"code","51602e48":"code","fcc15d41":"code","12f482b7":"code","6102640c":"code","dc40a7e9":"code","56327f8f":"code","75a1c06c":"code","bddc1a46":"code","f9d58084":"code","900f2d97":"code","3cfd3acd":"code","e6e4dbdf":"code","0a6d9bd7":"code","ba3ba677":"code","02f1f000":"code","e5461f1d":"code","d446bd67":"code","5903dc0f":"code","0bfc28ab":"code","cb42fb49":"code","4aad01d2":"code","60c8d71b":"markdown","8f75de84":"markdown","a3d32a9a":"markdown","26879a81":"markdown","d46d0125":"markdown","da0480f3":"markdown","2755ac51":"markdown","65c338e0":"markdown","727a2556":"markdown","0624c0f0":"markdown"},"source":{"6ece969e":"\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom collections import Counter\n\n\n#preprocessing\nfrom sklearn.feature_extraction.text import HashingVectorizer\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder, PolynomialFeatures\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom math import floor\n\n#visualization\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport pickle\nfrom sklearn.linear_model import LinearRegression\nimport xgboost as xgb\nfrom xgboost import XGBClassifier, XGBRegressor\nimport lightgbm as lgb\nfrom lightgbm import LGBMRegressor\n\nfrom xgboost import plot_importance\nimport optuna\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import metrics\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n        \n        \n%matplotlib inline","c17b01a7":"train_df = pd.read_csv('..\/input\/hse-aml-2020\/books_train.csv')\ntest_df = pd.read_csv('..\/input\/hse-aml-2020\/books_test.csv')\nsample_df = pd.read_csv('..\/input\/hse-aml-2020\/books_sample_submission.csv')","51602e48":"def export_res(X, filename='X'):\n    try:\n        with open('{}.pickle'.format(filename), 'wb') as fout:\n            pickle.dump(X, fout)\n        print(f'Preprocessed {filename} exported')\n    except FileNotFoundError:\n        print('File not found')\n\n\ndef load_saved_parameters(filename):\n    try:\n        with open('..\/input\/hse-aml-params\/{}.pickle'.format(filename), 'rb') as fin:\n            X = pickle.load(fin)\n        print('Parameters loaded')\n    except FileNotFoundError:\n        print('File with saved parameters not found')\n    return X","fcc15d41":"train_df.info()","12f482b7":"train_df.head()","6102640c":"test_df.info()","dc40a7e9":"all_df = pd.concat([train_df, test_df])\nall_df.info()","56327f8f":"print('Numbers of isbn for bookID: ', all_df.groupby('bookID')['isbn'].count().unique())\nprint('Numbers of isbn13 for bookID: ',all_df.groupby('bookID')['isbn13'].count().unique())\nprint('Numbers of language_code for bookID: ',all_df.groupby('bookID')['language_code'].count().unique())\nprint('Numbers of publisher for bookID: ',all_df.groupby('bookID')['publisher'].count().unique())","75a1c06c":"def count_authors(authors_str):\n    authors_str = str(authors_str)\n    return authors_str.count('\/') + 1\n\nauthors = np.array(train_df['authors'])\ncounted_auth = np.array(list(map(count_authors, authors)))\ntrain_df['n_authors'] = counted_auth","bddc1a46":"most_rated = all_df.sort_values(by=\"ratings_count\", ascending = False).head(10)\n\nmost_rated_titles = pd.DataFrame(most_rated.title).join(pd.DataFrame(most_rated.ratings_count))\nmost_rated_titles","f9d58084":"def scatter_plot(x, y, title, x_label, y_label):\n    plt.subplots(figsize=(8, 8))\n    plt.scatter(x,\n                y)\n    plt.title(title)\n    plt.xlabel(x_label)\n    plt.ylabel(y_label)\n    \npages_count_and_average_rating_title = \"Relation between pages count and average rating\"\naverage_rating_label = \"Average rating\"\npages_count_label = \"Pages count\"\nscatter_plot(all_df.average_rating,\n             all_df['  num_pages'],\n            pages_count_and_average_rating_title, average_rating_label, pages_count_label)","900f2d97":"books_data = all_df.drop(all_df.index[all_df[\"  num_pages\"] >= 2000], inplace=True)\npages_count_and_average_rating_title = \"Relation between pages count and average rating\"\naverage_rating_label = \"Average rating\"\npages_count_label = \"Pages count\"\nscatter_plot(all_df.average_rating,\n             all_df['  num_pages'],\n            pages_count_and_average_rating_title, average_rating_label, pages_count_label)","3cfd3acd":"all_df[all_df[\"  num_pages\"] == 0]","e6e4dbdf":"all_df[\"  num_pages\"] = all_df[\"  num_pages\"].replace(0, np.nan)\nall_df[\"  num_pages\"].fillna(float(floor(all_df[\"  num_pages\"].mean())), inplace=True)\n\nprint(\"Number 0s in num_pages:\", len(all_df[all_df[\"  num_pages\"] == 0]))\nprint(\"Is there any NaN in num_pages:\" , all_df[\"  num_pages\"].isna().any().any())","0a6d9bd7":"reviews_count_and_average_rating_title = \"Relation between text_reviews_count and average rating\"\naverage_rating_label = \"Average rating\"\nreviews_count_label = \"text_reviews_count\"\nscatter_plot(all_df.average_rating,\n             all_df['text_reviews_count'],\n            reviews_count_and_average_rating_title, average_rating_label, reviews_count_label)","ba3ba677":"# find text_reviews_count count outliers\nsns.boxplot(x=all_df['text_reviews_count'])","02f1f000":"all_df.drop(all_df.index[all_df[\"text_reviews_count\"] >= 40000], inplace=True)","e5461f1d":"# find ratings count outliers\nsns.boxplot(x=all_df['ratings_count'])","d446bd67":"all_df.drop(all_df.index[all_df[\"ratings_count\"] >= 1000000], inplace=True)","5903dc0f":"def preprocess_df(train_df):\n    # removing num_pages outliers, replacing 0s with mean value\n    #train_df.drop(train_df.index[train_df[\"  num_pages\"] >= 2000], inplace=True)\n    train_df[\"  num_pages\"] = train_df[\"  num_pages\"].replace(0, np.nan)\n    train_df[\"  num_pages\"].fillna(float(floor(train_df[\"  num_pages\"].mean())), inplace=True)\n\n    # removing text_reviews_count outliers\n    #train_df.drop(train_df.index[train_df[\"text_reviews_count\"] >= 40000], inplace=True)\n\n\n    # removing ratings count outliers\n    #train_df.drop(train_df.index[train_df[\"ratings_count\"] >= 1000000], inplace=True)\n    \n    # Handling text data: titles, authors, publisher\n    # encoding title column\n    lb_encoder = LabelEncoder()\n    train_df['title'] = lb_encoder.fit_transform(train_df['title'])\n    \n    # encoding authors column\n    train_df['authors'] = lb_encoder.fit_transform(train_df['authors'])\n    \n    # encoding publisher column\n    train_df['publisher'] = lb_encoder.fit_transform(train_df['publisher'])\n    \n    # encoding language column\n    lang_enc = pd.get_dummies(train_df['language_code'])\n    train_df = pd.concat([train_df, lang_enc], axis = 1)\n    \n    #Adding frequency rate for authors, publisher\n    auth_freq = train_df['authors'].value_counts(normalize=True)\n    train_df['authors_freq'] = train_df['authors'].map(lambda x: auth_freq[x])\n\n    publisher_freq = train_df['publisher'].value_counts(normalize=True)\n    train_df['publishers_freq'] = train_df['publisher'].map(lambda x: publisher_freq[x])\n    \n    \n    \n    return train_df","0bfc28ab":"train_df['label'] = 'train'\ntest_df['label'] = 'test'\n\nall_df_pre = pd.concat([train_df, test_df])\n\nall_df_sub =  preprocess_df(all_df_pre)\n\ntrain_df_sub = all_df_sub[all_df_sub['label']=='train']\ntest_df_sub = all_df_sub[all_df_sub['label']=='test']\n\n# divide the data into attributes and labels\nX_train_v = train_df_sub.drop(['average_rating', 'language_code', 'isbn', 'isbn13', 'publication_date','publisher','label'], axis = 1)\ny_train_v = train_df_sub['average_rating']\n\ntest_df_sub = test_df_sub.drop(['language_code', 'isbn', 'isbn13', 'publication_date','publisher','label','average_rating'], axis = 1)\nX_test_v = test_df_sub\n\nresults = load_saved_parameters('lgb_fin_1031')\nlgb_params = results['LGB']['params']\n\n\nmodel = LGBMRegressor(**lgb_params)\nlgbm = model.fit(X_train_v, y_train_v,  verbose=False)\n    \nprediction = model.predict(X_test_v)","cb42fb49":"#Create a  DataFrame with the passengers ids and our prediction regarding whether they survived or not\nsubmission = pd.DataFrame({'bookID':test_df_sub['bookID'],'average_rating':prediction})\n\n#Visualize the first 5 rows\nsubmission.head()","4aad01d2":"submission.to_csv('submission_lgbm.csv', index=False)","60c8d71b":"### There are also some books with number of pages = 0. It's better replace them with mean value.","8f75de84":"#### In the plot above we can see that point above 40.000 are outliers.","a3d32a9a":"### There is no significant relation between average rating and the books' number of pages. Though, there are outliers that should be removed.\n### The books with number of pages > 2000 will be dropped.","26879a81":"#### As for the most reviewed books:","d46d0125":"### Exploring most rated and most reviewed books (num_pages,text_reviews_count, ratings_count)","da0480f3":"### Thus, there are 4 columns with 1-to-1 relationship to bookID\n### Though, there is a column authors that may include several authors per book. Let's count number of authors and create new feature based on that.","2755ac51":"#### The boxplot shows that points between >= 1,000,000 are outliers.","65c338e0":"#### As for ratings_count:","727a2556":"## Data exploration","0624c0f0":"### Checking relationship between bookID and columns"}}