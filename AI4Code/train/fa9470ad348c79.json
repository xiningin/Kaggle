{"cell_type":{"84e5e440":"code","37f26da3":"code","2164af8b":"code","2aee6e07":"code","8962976c":"code","20921dc6":"markdown","e4d4fd10":"markdown"},"source":{"84e5e440":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","37f26da3":"from sklearn.preprocessing import OrdinalEncoder\nimport gc\n\nTRAIN_FILEPATH = '\/kaggle\/input\/30-days-of-ml\/train.csv'\nTEST_FILEPATH = '\/kaggle\/input\/30-days-of-ml\/test.csv'\n\ntrain = pd.read_csv(TRAIN_FILEPATH)\ntest = pd.read_csv(TEST_FILEPATH)\n\ncat_features = [f'cat{i}' for i in range(10)]\nord_encoder = OrdinalEncoder()\nord_encoder.fit(train[cat_features])\ntrain[cat_features] = ord_encoder.transform(train[cat_features])\ntest[cat_features] = ord_encoder.transform(test[cat_features])\ndel train, ord_encoder\ngc.collect()","2164af8b":"import json\nimport numpy as np\nimport pandas as pd\nimport time\nfrom sklearn.pipeline import Pipeline\nfrom sklearn import preprocessing\nfrom sklearn import linear_model\nfrom sklearn import kernel_ridge\nfrom sklearn import svm\nfrom sklearn import neighbors\nfrom sklearn import gaussian_process\nfrom sklearn import cross_decomposition\nfrom sklearn import tree \nfrom sklearn.experimental import enable_hist_gradient_boosting \nfrom sklearn import ensemble\nfrom sklearn import neural_network\nfrom sklearn import naive_bayes\nfrom sklearn.metrics import mean_squared_error\n\nregressors = {\n    'ARDRegression':linear_model.ARDRegression(),\n    'AdaBoostRegressor':ensemble.AdaBoostRegressor(),\n    'BaggingRegressor':ensemble.BaggingRegressor(n_jobs=-1),\n    'BayesianRidge':linear_model.BayesianRidge(),\n    'DecisionTreeRegressor':tree.DecisionTreeRegressor(),\n    'ElasticNet':linear_model.ElasticNet(),\n    'ExtraTreesRegressor':ensemble.ExtraTreesRegressor(n_jobs=-1),\n    'GaussianProcessRegressor':gaussian_process.GaussianProcessRegressor(),\n    'GradientBoostingRegressor':ensemble.GradientBoostingRegressor(),\n    'HistGradientBoostingRegressor':ensemble.HistGradientBoostingRegressor(),\n    'HuberRegressor':linear_model.HuberRegressor(),\n    'KNeighborsRegressor':neighbors.KNeighborsRegressor(n_jobs=-1),\n    'KernelRidge':kernel_ridge.KernelRidge(),\n    'Lars':linear_model.Lars(),\n    'Lasso':linear_model.Lasso(),\n    'LassoLars':linear_model.LassoLars(),\n    'LinearRegression':linear_model.LinearRegression(n_jobs=-1),\n    'LinearSVR':svm.LinearSVR(),\n    'MLPRegressor':neural_network.MLPRegressor(),\n    'NuSVR-poly':svm.NuSVR(kernel='poly'),\n    'NuSVR-rbf':svm.NuSVR(kernel='rbf'),\n    'NuSVR-sigmoid':svm.NuSVR(kernel='sigmoid'),\n    'OrthogonalMatchingPursuit':linear_model.OrthogonalMatchingPursuit(),\n    'PLSCanonical':cross_decomposition.PLSCanonical(),\n    'PLSRegression':cross_decomposition.PLSRegression(),\n    'PassiveAggressiveRegressor':linear_model.PassiveAggressiveRegressor(),\n    'Polynomial-2':Pipeline([('poly', preprocessing.PolynomialFeatures(degree=2)), ('linear', linear_model.LinearRegression(n_jobs=-1)), ]),\n    'Polynomial-3':Pipeline([('poly', preprocessing.PolynomialFeatures(degree=3)), ('linear', linear_model.LinearRegression(n_jobs=-1)), ]),\n    'RANSACRegressor':linear_model.RANSACRegressor(),\n    'RadiusNeighborsRegressor':neighbors.RadiusNeighborsRegressor(n_jobs=-1),\n    'RandomForestRegressor':ensemble.RandomForestRegressor(n_jobs=-1),\n    'Ridge':linear_model.Ridge(),\n    'SGDRegressor':linear_model.SGDRegressor(),\n    'SVR-poly':svm.SVR(kernel='poly'),\n    'SVR-rbf':svm.SVR(kernel='rbf'),\n    'SVR-sigmoid':svm.SVR(kernel='sigmoid'),\n    'StackingRegressor':ensemble.StackingRegressor([('lr',linear_model.Ridge()), ('svr',svm.LinearSVR()), ('rf',ensemble.RandomForestRegressor(n_jobs=-1)), ], n_jobs=-1), # too slow.\n    'TheilSenRegressor':linear_model.TheilSenRegressor(n_jobs=-1),\n    'TweedieRegressor':linear_model.TweedieRegressor(),\n    'VotingRegressor':ensemble.VotingRegressor([('lr',linear_model.Ridge()), ('rf',ensemble.RandomForestRegressor(n_jobs=-1)), ], n_jobs=-1),\n}\n\nK_TRAIN_FILEPATH = '\/kaggle\/input\/d\/hiroshikameya\/30daysofml\/train_smallest.csv'\n\ndf = pd.read_csv(K_TRAIN_FILEPATH)\ndf_test = test\nn_splits = len(df['kfold'].unique())\nuseful_features = [\n    colname for colname in df.columns if not colname in ['id', 'target', 'kfold', 'group']\n]\nprint(\n    f'{K_TRAIN_FILEPATH}\\n'\n    f'n_splits=kfold={n_splits}, df={df.shape}, useful_features={useful_features}\\n\\n'\n)\n\ntrials = []\nfor reg_name, model in regressors.items():\n    colname = reg_name\n    train_filepath = f'.\/train_{colname}.csv'\n    test_filepath = f'.\/test_{colname}.csv'\n    \n    final_valid_rmses = []\n\n    start_time = time.time()\n    test_X = df_test[['id']+useful_features]\n    exec_times = []\n    for kfold in range(n_splits):\n        begin_time = time.time()\n        train_X = df[df.kfold!=kfold].reset_index(drop=True)\n        valid_X = df[df.kfold==kfold].reset_index(drop=True)\n        train_X = train_X[['id']+useful_features+['target']]\n        valid_X = valid_X[['id']+useful_features+['target']]        \n        train_y, valid_y = train_X.target, valid_X.target\n        train_X, valid_X = train_X.drop('target', axis=1), valid_X.drop('target', axis=1)\n\n        valid_ids, test_ids = valid_X.id.values.tolist(), test_X.id.values.tolist()\n\n        try:\n            model.fit(train_X, train_y, ) \n            valid_preds, test_preds = model.predict(valid_X), model.predict(test_X)\n        except:\n            continue\n        n_nan = np.isnan(valid_preds).sum()\n        if n_nan > 0:\n            print(f'{n_nan} nan found.')\n            valid_preds= np.nan_to_num(valid_preds, 0)\n        valid_rmse = mean_squared_error(valid_y, valid_preds, squared=False)\n        final_valid_rmses.append(valid_rmse)\n        end_time = time.time()\n        exec_times.append(end_time-begin_time)\n        print(f'{reg_name}, kfold={kfold}, valid-rmse={valid_rmse:.6f}, '\n              f'{end_time-begin_time:.1f}s')\n    \n    del model\n    gc.collect()\n    final_valid_rmse = np.mean(np.column_stack(final_valid_rmses))\n    print(f'{reg_name},    final-valid-rmse={final_valid_rmse:.6f}, {time.time()-start_time:.1f}s\\n')\n","2aee6e07":"import os\nimport json\n%matplotlib inline\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport matplotlib.cm as cm\nfrom matplotlib.colors import Normalize\n\ndf_times = pd.read_csv('\/kaggle\/input\/d\/hiroshikameya\/30daysofml\/df_times.csv')\ndf_rmses = pd.read_csv('\/kaggle\/input\/d\/hiroshikameya\/30daysofml\/df_rmses.csv')\n\nsns.set_theme(style='whitegrid')\n\nfig = plt.figure(figsize=(14,15))\nax = sns.boxplot(x=df_times['Execution Time'], y=df_times['Estimator'], orient='h', palette='Set3')\nplt.xscale('log')\nplt.xlabel('Execution Time [s]')\nplt.ylabel('Estimator')\nplt.show()","8962976c":"fig = plt.figure(figsize=(14,15))\nsns.set_theme(style='whitegrid')\nbox = sns.boxplot(x=df_rmses['RMSE'], y=df_rmses['Estimator'], orient='h', palette='Set3')\nplt.xlim(0.7, 1)\nplt.xlabel('RMSE')\nplt.ylabel('Estimator')\nplt.show()","20921dc6":"There are various Regressors in sklearn. I didn't have time to try them out, so I only used some of the regressors that I have always used, but for the sake of study, I measured the execution time and accuracy of various regressors implemented in sklearn.","e4d4fd10":"| Module              | Class                         | Short Description                       | \n| ------------------- | ----------------------------- | --------------------------------------- | \n| cross_decomposition | PLS regression                | Cross decomposition                     | \n| ensemble            | AdaBoostRegressor             | Ensemble                                | \n| ensemble            | BaggingRegressor              | Ensemble                                | \n| ensemble            | ExtraTreesRegressor           | Ensemble                                | \n| ensemble            | GradientBoostingRegressor     | Ensemble                                | \n| ensemble            | RandomForestRegressor         | Ensemble                                | \n| ensemble            | HistGradientBoostingRegressor | Ensemble                                | \n| gaussian_process    | GaussianProcessRegressor      | Gaussian processes                      | \n| isotonic            | IsotonicRegression            | Isotonic ridge regressor                | \n| kernel_ridge        | KernelRidge                   | Kernel ridge regressor                  | \n| linear_model        | LinearRegression              | Classical linear regressor              | \n| linear_model        | RidgeSGDRegressor             | Classical linear regressor              | \n| linear_model        | ElasticNet                    | Regressor with variable selection       | \n| linear_model        | Lars                          | Regressor with variable selection       | \n| linear_model        | Lasso                         | Regressor with variable selection       | \n| linear_model        | LassoLars                     | Regressor with variable selection       | \n| linear_model        | OrthogonalMatchingPursuit     | Regressor with variable selection       | \n| linear_model        | ARDRegression                 | Bayesian regressor                      | \n| linear_model        | BayesianRidge                 | Bayesian regressor                      | \n| linear_model        | HuberRegressor                | Outlier-robust regressor                | \n| linear_model        | RANSACRegressor               | Outlier-robust regressor                | \n| linear_model        | TheilSenRegressor             | Outlier-robust regressor                | \n| linear_model        | PoissonRegressor              | Generalized linear model for regression | \n| linear_model        | TweedieRegressor              | Generalized linear model for regression | \n| linear_model        | GammaRegressor                | Generalized linear model for regression | \n| linear_model        | PassiveAggressiveRegressor    | Miscellaneous                           | \n| neighbors           | KNeighborsRegressor           | Nearest neighbors                       | \n| neighbors           | RadiusNeighborsRegressor      | Nearest neighbors                       | \n| neural_network      | MLPRegressor                  | Neural network model                    | \n| svm                 | LinearSVR                     | Support vector machine                  | \n| svm                 | NuSVR                         | Support vector machine                  | \n| svm                 | SVR                           | Support vector machine                  | \n| tree                | DecisionTreeRegressor         | Decision trees                          | \n| tree                | ExtraTreeRegressor            | Decision trees                          | "}}