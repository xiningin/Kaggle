{"cell_type":{"47e3f70a":"code","26df4911":"code","526f22db":"code","fe6aa27f":"code","b945591e":"code","661b2c8d":"code","5ec803b9":"code","2df09d9e":"code","e1b20232":"code","c0d3d7f1":"code","70fc80cc":"code","10d7f10c":"code","7aa1bb87":"code","1fcb610f":"code","1be1fded":"code","0151562c":"code","ca3a4b8d":"code","a39e8659":"code","019531c7":"code","cfdfd7c5":"code","af5ec37b":"code","8f933154":"code","da3b65b7":"code","00204936":"markdown"},"source":{"47e3f70a":"import pandas\nimport functools\nimport math\nimport re\npandas.set_option('display.max_rows', 500)\npandas.set_option('display.max_columns', 500)\npandas.set_option('display.width', 1000)\npandas.set_option(\"max_column\", None)","26df4911":"corpus = \"I am a little confused on all of the models of the 88-89 bonnevilles. I have heard of the LE SE LSE SSE SSEI. Could someone tell me the differences are far as features or performance. I am also curious toknow what the book value is for prefereably the 89 model. And how much less than book value can you usually get them for. In other words how much are they in demand this time of year. I have heard that the mid-spring early summer is the best time to buy.\"\n# corpus = str(input(\"Enter Your Coupus: \"))\n# print(\"\\n\\n\")\n# while not re.search(r'([A-Z .-\/-\\&\/\\,])\\w*', corpus):\n#     corpus = str(input(\"Enter Your Coupus: \"))\nprint(corpus)","526f22db":"corpus_list = corpus.split(\".\")\nfor item in corpus_list:\n    index = corpus_list.index(item)\n    corpus_list[index] = corpus_list[index].strip()\n    if item == \"\":\n        corpus_list.remove(\"\")","fe6aa27f":"print(corpus_list)","b945591e":"# Making a dictionary where the key will be a variable named bag_Of_words_{n}\n# And the value will be BAG OF WORDS made from each item in corpus_list\n\ndictionary_of_bag_of_words = {}  # bowA\n\nfor item in corpus_list:\n    dictionary_of_bag_of_words[str(corpus_list.index(item))] = item.split(\" \")","661b2c8d":"print(dictionary_of_bag_of_words)\nfor key in dictionary_of_bag_of_words:\n    print(f\"Key:: {key}\\t Value:  {dictionary_of_bag_of_words[key]}\")","5ec803b9":"# Function for union set\n# def make_UNION(set1, set2):\n#     set1 = set(set1)\n#     set2 = set(set2)\n#     return set1.union(set2)\n\n# Later replaced by lambda xpression\n# lambda set1, set2: set(set1).union(set(set2))\n\nall_word_set = functools.reduce(lambda set1, set2: set(set1).union(set(set2)),\n                                list(dictionary_of_bag_of_words.values()))\nprint(all_word_set, len(all_word_set))\n\nprint(list(dictionary_of_bag_of_words.values()))","2df09d9e":"dictionary_of_word_set = {}  # wordDictA\nfor item in corpus_list:\n    dictionary_of_word_set[str(corpus_list.index(item))] = dict.fromkeys(\n        all_word_set, 0)","e1b20232":"for key in dictionary_of_word_set:\n    print(f\"Key:: {key}\\t Value:  {dictionary_of_word_set[key]}\")","c0d3d7f1":"for bow_as_key in dictionary_of_bag_of_words:\n    iterable = dictionary_of_bag_of_words[bow_as_key]\n    for item in iterable:\n        x = dictionary_of_word_set[str(bow_as_key)]\n        x[str(item)] += 1","70fc80cc":"for key in dictionary_of_word_set:\n    print(f\"Key:: {key}\\t\\nValue:  {dictionary_of_word_set[key]}\\n\\n\")","10d7f10c":"list_of_dictionary_of_word_set = list(dictionary_of_word_set.values())\n# print(list_of_dictionary_of_word_set)\ndata_drame = pandas.DataFrame(list_of_dictionary_of_word_set)","7aa1bb87":"data_drame","1fcb610f":"def computeTF(dictionary_of_word, bag_Of_words):\n    tf_dictionary_to_return = {}\n    length_of_doc = len(bag_Of_words)\n    for word, count in dictionary_of_word.items():\n        tf_dictionary_to_return[word] = count \/ float(length_of_doc)\n\n    return tf_dictionary_to_return\n\n\ndef computerIDF(_corpus_list):\n    idf_dictionary_to_return = dict.fromkeys(_corpus_list[0].keys(), 0)\n    N = len(_corpus_list)\n    for document_dict in _corpus_list:\n        for word, val in document_dict.items():\n            if val > 0:\n                idf_dictionary_to_return[word] += 1\n\n    # Final processing of idf_dictionary_to_return\n    for word, val in idf_dictionary_to_return.items():\n        idf_dictionary_to_return[word] = math.log(N \/ float(val))\n\n    return idf_dictionary_to_return\n\n\ndef calculateTF_IDF(tf_dict, idf_dict):\n    tfidf_to_return = {}\n    for word, val in tf_dict.items():\n        tfidf_to_return[word] = val * idf_dict[word]\n\n    return tfidf_to_return","1be1fded":"dictionary_of_word_set_for_tf = dictionary_of_word_set.copy()","0151562c":"for key, val in dictionary_of_word_set.items():\n    x = computeTF(dictionary_of_word_set[key], dictionary_of_bag_of_words[key])\n    dictionary_of_word_set_for_tf[key] = x","ca3a4b8d":"for key in dictionary_of_word_set_for_tf:\n    print(f\"Key:: {key}\\t\\n Value:  {dictionary_of_word_set_for_tf[key]}\\n\\n\")","a39e8659":"dictionary_of_word_set_list = []\nfor key, val in dictionary_of_word_set.items():\n    dictionary_of_word_set_list.append(dictionary_of_word_set[key])","019531c7":"for key in dictionary_of_word_set_list:\n    print(key)","cfdfd7c5":"idfs = computerIDF(dictionary_of_word_set_list)\ntf_idf_dictionary = dict.fromkeys(dictionary_of_word_set_for_tf.keys(), 0)\nfor key, val in dictionary_of_word_set_for_tf.items():\n    tf_idf_dictionary[key] = calculateTF_IDF(\n        dictionary_of_word_set_for_tf[key], idfs)","af5ec37b":"for key in tf_idf_dictionary:\n    print(f\"Key:: {key}\\t\\n Value:  {tf_idf_dictionary[key]}\\n\\n\")","8f933154":"tf_idf_list = []\nfor key, val in tf_idf_dictionary.items():\n    tf_idf_list.append(tf_idf_dictionary[key])","da3b65b7":"data_drame_tf_idf = pandas.DataFrame(tf_idf_list)\nprint(\"TF-IDF Data Frame\\n==================\\n\")\ndata_drame_tf_idf","00204936":"<h1 style='color:blue'>TF-IDF<\/h1><h3 style='color:green'>(Term Frequency-Inverse Document Frequency)<\/h3>\n<br>\n<p>In <a href=\"https:\/\/en.wikipedia.org\/wiki\/Information_retrieval\" title=\"Information retrieval\">information retrieval<\/a>, <b>tf\u2013idf<\/b> or <b>TFIDF<\/b>, short for <b>term frequency\u2013inverse document frequency<\/b>, is a numerical statistic that is intended to reflect how important a word is to a <a href=\"https:\/\/en.wikipedia.org\/wiki\/Document\" title=\"Document\">document<\/a> in a collection or <a href=\"https:\/\/en.wikipedia.org\/wiki\/Text_corpus\" title=\"Text corpus\">corpus<\/a>.<sup id=\"cite_ref-1\" class=\"reference\"><\/sup> It is often used as a <a href=\"https:\/\/en.wikipedia.org\/wiki\/Weighting_factor\" class=\"mw-redirect\" title=\"Weighting factor\">weighting factor<\/a> in searches of information retrieval, <a href=\"https:\/\/en.wikipedia.org\/wiki\/Text_mining\" title=\"Text mining\">text mining<\/a>, and <a href=\"https:\/\/en.wikipedia.org\/wiki\/User_modeling\" title=\"User modeling\">user modeling<\/a>.\n<\/p>\n<br>\n<h3><span class=\"mw-headline\" id=\"Term_frequency\">Term frequency<\/span><\/h3>\n<p>Suppose we have a set of English text documents and wish to rank which document is most relevant to the query, \"the\n    brown cow\". A simple way to start out is by eliminating documents that do not contain all three words \"the\",\n    \"brown\", and \"cow\", but this still leaves many documents. To further distinguish them, we might count the number of\n    times each term occurs in each document; the number of times a term occurs in a document is called its <i>term\n        frequency<\/i>. However, in the case where the length of documents varies greatly, adjustments are often made\n    (see definition below). The first form of term weighting is due to <a href=\"https:\/\/en.wikipedia.org\/wiki\/Hans_Peter_Luhn\" title=\"Hans Peter Luhn\">Hans\n        Peter Luhn<\/a> (1957) which may be summarized as:\n<\/p>\n<ul><li>The weight of a term that occurs in a document is simply proportional to the term frequency.<\/li><\/ul>\n<br>\n<h3><span class=\"mw-headline\" id=\"Inverse_document_frequency\">Inverse document frequency<\/span><\/h3>\n<p>Because the term \"the\" is so common, term frequency will tend to incorrectly emphasize documents which happen to use the word \"the\" more frequently, without giving enough weight to the more meaningful terms \"brown\" and \"cow\". The term \"the\" is not a good keyword to distinguish relevant and non-relevant documents and terms, unlike the less-common words \"brown\" and \"cow\". Hence an <i>inverse document frequency<\/i> factor is incorporated which diminishes the weight of terms that occur very frequently in the document set and increases the weight of terms that occur rarely.\n<\/p>\n<p><a href=\"https:\/\/en.wikipedia.org\/wiki\/Karen_Sp%C3%A4rck_Jones\" title=\"Karen Sp\u00e4rck Jones\">Karen Sp\u00e4rck Jones<\/a> (1972) conceived a statistical interpretation of term specificity called Inverse Document Frequency (idf), which became a cornerstone of term weighting:\n<\/p>\n<ul><li>The specificity of a term can be quantified as an inverse function of the number of documents in which it occurs.<\/li><div><\/div><\/ul>\n\n<p style='float:right; font-style:italic'>Source: Wikipedia<\/p>"}}