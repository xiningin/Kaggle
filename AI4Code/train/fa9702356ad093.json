{"cell_type":{"704e0926":"code","04b5fc9d":"code","60c3d35a":"code","b6cb315b":"code","f104bbdc":"code","b8c86490":"code","e3d55305":"code","972be13d":"code","32db97b0":"code","7625d238":"code","caee35b5":"code","0e94987f":"code","30c22985":"code","6562a94d":"code","0fa7f507":"code","5f77364e":"code","4a9e21ec":"code","f73bc556":"code","5623c1e3":"code","4df0decd":"code","38a93b26":"code","0cdf8afe":"code","89defa64":"code","08a125c1":"code","3f8835b2":"code","b9a2addb":"code","19e92271":"code","28a753f5":"code","e55d125a":"code","2039e13c":"code","64ad9a05":"code","3affe745":"code","cebec9da":"code","ec92b4d5":"code","f7b5b8a1":"code","a08a0721":"code","8adf2a80":"code","7eb4f06f":"code","5ece934e":"code","f8306536":"code","7cddd3d4":"code","2d791a94":"code","53a248c1":"code","f5b1108c":"code","89eb81f9":"code","58ea77cc":"code","4b527971":"code","6dbbf90b":"code","1c09ce96":"code","b9de1be8":"code","79fe76bb":"code","2bcb5fe1":"code","f32c393e":"code","3846340c":"code","5cfd65e3":"code","2fdf1dd8":"code","f53057d9":"code","ff80c3b0":"code","080a4d7a":"code","162c10d6":"code","2c01b4bb":"code","22ae54c2":"code","656bf1c3":"code","b7cbbcc3":"code","03e7ec30":"code","793008d1":"code","d0871de4":"code","ff0e0ab3":"code","e8497b09":"code","2d876671":"code","0fe14e8d":"code","e2d5a0f4":"code","ee12e689":"code","f9d899ba":"code","b5ab2737":"code","71a5654c":"code","0bc19f9e":"code","f51197ea":"code","79220b2d":"code","a6e58059":"code","3b47f1b5":"code","9d9c8c20":"code","a256b70d":"code","57c09ac8":"code","9420eb88":"code","55042af6":"code","87b52a44":"code","32d0f227":"code","65f05e25":"code","61c8cae9":"code","5a3d6b18":"code","a3dee7ae":"code","50b97e7f":"code","651ff84a":"code","2ddb2a16":"code","1ef68dfa":"code","3a61d508":"code","266be355":"markdown","bdb11865":"markdown","3bee57b8":"markdown","1626444d":"markdown","c99ef242":"markdown","5db62110":"markdown","69e42b7f":"markdown","868d8eca":"markdown","3671de0d":"markdown","754d8919":"markdown","38ef7cec":"markdown","697f8268":"markdown","68969839":"markdown","99cc451b":"markdown","0a7590cc":"markdown","9da9bc3d":"markdown","d192f377":"markdown","487571e7":"markdown","d0b0e086":"markdown","03ecc818":"markdown","c518a1fc":"markdown","0e00ae1d":"markdown","40878445":"markdown","d6272822":"markdown","6b0d2a7a":"markdown","3046be5b":"markdown","cb01b2e2":"markdown","24920b29":"markdown","383a46c6":"markdown","f9df96b2":"markdown","ab4d3b63":"markdown","b84853eb":"markdown","1d536529":"markdown","1d94800e":"markdown","c38a11ae":"markdown","3b2bd355":"markdown","de1c7e78":"markdown","ff186b4f":"markdown","845bd0a7":"markdown","2bf58013":"markdown","5ddce451":"markdown","4da74dcc":"markdown","0938e474":"markdown","d55d45c0":"markdown","b5183ad2":"markdown","f4583094":"markdown","8792141a":"markdown","f570f2c4":"markdown","bc0aec8b":"markdown","6b5c49df":"markdown","125e7af1":"markdown","a9341fab":"markdown"},"source":{"704e0926":"import matplotlib.pyplot as plt\nimport numpy as np\nimport os\nimport pandas as pd\nimport seaborn as sns\nimport warnings\nimport xgboost as xgb\n\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.ensemble import ExtraTreesClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import VotingClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.multiclass import OneVsOneClassifier\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier","04b5fc9d":"# This magic method displays matplotlib plots inside of Jupyter notebooks instead of in a separate window\n%matplotlib inline\n\n# Set plot style using the Seaborn library\nsns.set_style(\"whitegrid\")\n\n# Supress the oh so common pandas warnings; use at own risk\nwarnings.filterwarnings(\"ignore\")","60c3d35a":"train = pd.read_csv(\"..\/input\/train.csv\")","b6cb315b":"# The head() method shows the first rows of a DataFrame; tail() shows the last rows\ntrain.head()","f104bbdc":"test = pd.read_csv(\"..\/input\/test.csv\")","b8c86490":"test.head()","e3d55305":"passenger_id = test[\"PassengerId\"]","972be13d":"all_data = pd.concat([train, test], keys=[\"Train\", \"Test\"], names=[\"Dataset\", \"Id\"], sort=False)","32db97b0":"all_data.head()","7625d238":"all_data.loc[\"Train\"].head()","caee35b5":"all_data.loc[\"Test\"].head()","0e94987f":"all_data.loc[\"Train\"].info()","30c22985":"all_data.loc[\"Test\"].info()","6562a94d":"all_data.loc[\"Train\"].describe()","0fa7f507":"# The 'bins' parameter sets the number of vertical bars\n# The 'figsize' parameter sets the size of the chart\nall_data.loc[\"Train\"].hist(bins=25, figsize=(20, 15))","5f77364e":"# The dropna() method excludes missing values, so if the most common value in the column is null, we don't return that as the mode\nembarked_mode = all_data.loc[\"Train\"][\"Embarked\"].dropna().mode()\nembarked_mode","4a9e21ec":"# The mode() method returns a pandas Series, but we only want the value, so we look at index 0\nembarked_mode = embarked_mode[0]\nembarked_mode","f73bc556":"# The fillna() method fills in missing values with the value indicated in the first parameter position\nall_data[\"Embarked\"].fillna(embarked_mode, inplace=True)","5623c1e3":"# The isnull() method returns a pandas Series with the row index and a Boolean (True or False) indicating if the value is null\n# The sum() method sums up the Boolean values, with True = 1 and False = 0\nall_data[\"Embarked\"].isnull().sum()","4df0decd":"# Perform the same process as with 'Embarked' only combined into one line of code\nall_data[\"Fare\"].fillna(all_data.loc[\"Train\"][\"Fare\"].dropna().mode()[0], inplace=True)","38a93b26":"all_data[\"Fare\"].isnull().sum()","0cdf8afe":"# The corr() method returns a correlation matrix for all numerical features\ncorrelation_matrix = all_data.loc[\"Train\"].corr()\ncorrelation_matrix[\"Age\"].sort_values(ascending=False)","89defa64":"# We can return multiple columns by passing a list to the second indexer\n# The groupby() method groups the results by the values in the 'Sex' column, so male and female\n# The mean() method returns the average\n# The sort_values() method sorts the resulting DataFrame\nall_data.loc[\"Train\"][[\"Age\", \"Sex\"]].groupby([\"Sex\"]).mean().sort_values(by=\"Age\", ascending=False)","08a125c1":"all_data.loc[\"Train\"][[\"Age\", \"Embarked\"]].groupby([\"Embarked\"]).mean().sort_values(by=\"Age\", ascending=False)","3f8835b2":"for value in [\"male\", \"female\"]:\n    for i in range(0, 3):\n        median_age = all_data.loc[\"Train\"][(all_data.loc[\"Train\"][\"Sex\"] == value) & (all_data.loc[\"Train\"][\"Pclass\"] == i+1)][\"Age\"].dropna().median()\n        all_data.loc[(all_data[\"Age\"].isnull()) & (all_data[\"Sex\"] == value) & (all_data[\"Pclass\"] == i+1), \"Age\"] = median_age","b9a2addb":"all_data[\"Age\"].isnull().sum()","19e92271":"#missing_age_index = list(all_data.loc[\"train\"][all_data.loc[\"train\"][\"Age\"].isnull()].index)\n#missing_age_index","28a753f5":"# Get the index of any row missing an Age in the dataset\n#missing_age_index = list(all_data.loc[\"train\"][dataset[\"Age\"].isnull()].index)\n#for i in missing_age_index:\n#    age_average = all_data.loc[\"train\"][\"Age\"].median()\n#    age_predict = all_data.loc[\"train\"][(all_data.loc[\"train\"][\"SibSp\"] == all_data.iloc[i][\"SibSp\"]) & (all_data.loc[\"train\"][\"Parch\"] == all_data.iloc[i][\"Parch\"]) & (all_data.loc[\"train\"][\"Pclass\"] == all_data.iloc[i][\"Pclass\"])][\"Age\"].median()\n#    if np.isnan(age_predict):\n#        all_data[\"Age\"].iloc[i] = age_average\n#    else:\n#        all_data[\"Age\"].iloc[i] = age_predict","e55d125a":"all_data[\"Cabin\"].fillna(\"None\", inplace=True)","2039e13c":"all_data[\"Cabin\"].isnull().sum()","64ad9a05":"# The extract() method uses a regular expression to extract the title from the 'Name' column\n# The expand=False parameter returns a pandas Series\n# Assigning values to an index that does not yet exist ('Title') will create it\nall_data[\"Title\"] = all_data[\"Name\"].str.extract(\"([A-Za-z]+)\\.\", expand=False)","3affe745":"# The unique() method returns all the unique values in the Series\nall_data[\"Title\"].sort_values().unique()","cebec9da":"pd.crosstab(all_data.loc[\"Train\"][\"Title\"], all_data.loc[\"Train\"][\"Sex\"])","ec92b4d5":"all_data[\"Title\"].replace([\"Capt\", \"Col\", \"Countess\", \"Don\", \"Dona\", \"Dr\", \"Jonkheer\", \"Lady\", \"Major\", \"Rev\", \"Sir\"], \"Rare\", inplace=True)\nall_data[\"Title\"].replace(\"Mlle\", \"Miss\", inplace=True)\nall_data[\"Title\"].replace(\"Ms\", \"Miss\", inplace=True)\nall_data[\"Title\"].replace(\"Mme\", \"Mrs\", inplace=True)","f7b5b8a1":"all_data[\"Title\"].sort_values().unique()","a08a0721":"all_data.loc[\"Train\"][[\"Title\", \"Survived\"]].groupby([\"Title\"]).mean().sort_values(by=\"Survived\", ascending=False)","8adf2a80":"# The pandas cut function splits the data into equal sized value ranges\npd.cut(all_data.loc[\"Train\"][\"Age\"], bins=5).dtype","7eb4f06f":"all_data.loc[all_data[\"Age\"] <= 16, \"Age\"] = 0\nall_data.loc[(all_data[\"Age\"] > 16) & (all_data[\"Age\"] <= 32), \"Age\"] = 1\nall_data.loc[(all_data[\"Age\"] > 32) & (all_data[\"Age\"] <= 48), \"Age\"] = 2\nall_data.loc[(all_data[\"Age\"] > 48) & (all_data[\"Age\"] <= 64), \"Age\"] = 3\nall_data.loc[all_data[\"Age\"] > 64, \"Age\"] = 4\n# Since the category values are integers, we set the column type to int\n# Notice that reassignment is used here as the astype() method does not have an inplace parameter\nall_data[\"Age\"] = all_data[\"Age\"].astype(int)","5ece934e":"all_data[\"Age\"].sort_values().unique()","f8306536":"all_data.loc[\"Train\"][[\"Age\", \"Survived\"]].groupby([\"Age\"]).mean().sort_values(by=\"Survived\", ascending=False)","7cddd3d4":"all_data[\"FamilySize\"] = all_data[\"SibSp\"] + all_data[\"Parch\"] + 1","2d791a94":"all_data[\"FamilySize\"].sort_values().unique()","53a248c1":"all_data.loc[\"Train\"][[\"FamilySize\", \"Survived\"]].groupby([\"FamilySize\"]).mean().sort_values(by=\"Survived\", ascending=False)","f5b1108c":"all_data[\"IsAlone\"] = 0\nall_data.loc[all_data[\"FamilySize\"] == 1, \"IsAlone\"] = 1","89eb81f9":"all_data[\"IsAlone\"].sort_values().unique()","58ea77cc":"all_data.loc[\"Train\"][[\"IsAlone\", \"Survived\"]].groupby([\"IsAlone\"]).mean()","4b527971":"# The qcut() method splits the data into equal sized bins where each bin has the same number of records\npd.qcut(train[\"Fare\"], 4).dtype","6dbbf90b":"all_data.loc[all_data[\"Fare\"] <= 7.91, \"Fare\"] = 0\nall_data.loc[(all_data[\"Fare\"] > 7.91) & (all_data[\"Fare\"] <= 14.45), \"Fare\"] = 1\nall_data.loc[(all_data[\"Fare\"] > 14.45) & (all_data[\"Fare\"] <= 31.00), \"Fare\"] = 2\nall_data.loc[all_data[\"Fare\"] > 31.00, \"Fare\"] = 3\nall_data[\"Fare\"] = all_data[\"Fare\"].astype(int)","1c09ce96":"all_data[\"Fare\"].sort_values().unique()","b9de1be8":"all_data.loc[\"Train\"][[\"Fare\", \"Survived\"]].groupby([\"Fare\"], as_index=False).mean().sort_values(by=\"Survived\", ascending=False)","79fe76bb":"all_data[\"Cabin\"] = all_data[\"Cabin\"].loc[all_data[\"Cabin\"].isnull() == False].str.extract(\"([A-Za-z]+)\", expand=False)","2bcb5fe1":"all_data[\"Cabin\"].sort_values().unique()","f32c393e":"all_data.loc[\"Train\"][[\"Cabin\", \"Survived\"]].groupby([\"Cabin\"], as_index=False).mean().sort_values(by=\"Survived\", ascending=False)","3846340c":"all_data.drop([\"PassengerId\", \"Name\", \"Ticket\"], axis=1, inplace=True)","5cfd65e3":"all_data.head()","2fdf1dd8":"# Instantiate a LabelEncoder object\nlabel_encoder = LabelEncoder()\n# The apply() method applies a function across a column or row\n# Here we apply the LabelEncoder across the entire DataFrame, affecting only categorical (text) features\nall_data_encoded = all_data.apply(label_encoder.fit_transform)\nall_data_encoded.head()","f53057d9":"correlation_matrix = all_data_encoded.loc[\"Train\"].corr()\ncorrelation_matrix[\"Survived\"].sort_values(ascending=False)","ff80c3b0":"plt.figure(figsize=(10, 8))\nplt.title(\"Pearson Correlation of Features\", y=1.05, size=15)\nsns.heatmap(\n    correlation_matrix,\n    linewidths=0.1,\n    vmax=1.0,\n    square=True,\n    cmap=plt.cm.jet,\n    linecolor=\"white\",\n    annot=True\n)","080a4d7a":"y_train = all_data_encoded.loc[\"Train\"][\"Survived\"].astype(int)\nX_train = all_data_encoded.loc[\"Train\"].drop([\"Survived\"], axis=1)","162c10d6":"# Instantiate a Random Forest Classifier object\nrandom_forest_classifier = RandomForestClassifier()\n# Train (fit) the Random Forest Classifier on the training data\nrandom_forest_classifier.fit(X_train, y_train)","2c01b4bb":"# The zip() function takes two iterables and joins them together into an iterable of tuples, in this case with the column name matched up to its feature importance\nfeature_importances = zip(list(X_train.columns.values), random_forest_classifier.feature_importances_)\n# Sort the list by feature importance using a lambda function\nfeature_importances = sorted(feature_importances, key=lambda feature: feature[1], reverse=True)\n# Iterate over the feature_importances list\nfor name, score in feature_importances:\n    # The format() method replaces any set of curly brackets in a string with the specified arguments\n    # The :<12 inside the first set of curly brackets aligns the text to the left and sets the character length to 12 characters, making everything print neatly\n    print(\"{:<12} | {}\".format(name, score))","22ae54c2":"all_data.drop([\"FamilySize\", \"Cabin\"], axis=1, inplace=True)","656bf1c3":"all_data.head()","b7cbbcc3":"all_data = pd.get_dummies(all_data)","03e7ec30":"all_data.loc[\"Train\"].head()","793008d1":"y_train = all_data.loc[\"Train\"][\"Survived\"].astype(int)\nX_train = all_data.loc[\"Train\"].drop([\"Survived\"], axis=1)\nX_test = all_data.loc[\"Test\"].drop([\"Survived\"], axis=1)","d0871de4":"y_train.head()","ff0e0ab3":"X_train.head()","e8497b09":"X_test.head()","2d876671":"#standard_scaler = StandardScaler()\n#X_train = standard_scaler.fit_transform(X_train)\n#X_test = standard_scaler.transform(X_test)\n\nrobust_scaler = RobustScaler()\nX_train = robust_scaler.fit_transform(X_train)\nX_test = robust_scaler.fit_transform(X_test)","0fe14e8d":"X_train[:3]","e2d5a0f4":"X_test[:3]","ee12e689":"sgd_classifier = SGDClassifier()\ncross_val_score(sgd_classifier, X_train, y_train, cv=10, scoring=\"accuracy\").mean()","f9d899ba":"one_vs_one_classifier = OneVsOneClassifier(SGDClassifier())\ncross_val_score(one_vs_one_classifier, X_train, y_train, cv=10, scoring=\"accuracy\").mean()","b5ab2737":"random_forest_classifier = RandomForestClassifier(min_samples_split=10, min_samples_leaf=2)\ncross_val_score(random_forest_classifier, X_train, y_train, cv=10, scoring=\"accuracy\").mean()","71a5654c":"extra_trees_classifier = ExtraTreesClassifier(min_samples_split=7, min_samples_leaf=5)\ncross_val_score(extra_trees_classifier, X_train, y_train, cv=10, scoring=\"accuracy\").mean()","0bc19f9e":"svm_classifier = SVC(probability=True, C=4.5)\ncross_val_score(svm_classifier, X_train, y_train, cv=10, scoring=\"accuracy\").mean()","f51197ea":"adaboost_classifier = AdaBoostClassifier(n_estimators=200, learning_rate=0.1)\ncross_val_score(adaboost_classifier, X_train, y_train, cv=10, scoring=\"accuracy\").mean()","79220b2d":"gradient_boost_classifier = GradientBoostingClassifier(learning_rate=0.03)\ncross_val_score(gradient_boost_classifier, X_train, y_train, cv=10, scoring=\"accuracy\").mean()","a6e58059":"logistic_regression_classifier = LogisticRegression()\ncross_val_score(logistic_regression_classifier, X_train, y_train, cv=5, scoring=\"accuracy\").mean()","3b47f1b5":"lda_classifier = LinearDiscriminantAnalysis()\ncross_val_score(lda_classifier, X_train, y_train, cv=5, scoring=\"accuracy\").mean()","9d9c8c20":"xgboost_classifier = xgb.XGBClassifier(gamma=0.7)\ncross_val_score(xgboost_classifier, X_train, y_train, cv=5, scoring=\"accuracy\").mean()","a256b70d":"parameter_grid = [\n    {\n        \"kernel\": [\"rbf\"],\n        \"C\": [4, 4.5, 5],\n        \"shrinking\": [True, False],\n        \"tol\": [0.00001, 0.00003, 0.00005, 0.00008],\n        \"class_weight\": [\"balanced\", None],\n        \"gamma\": [\"auto_deprecated\", \"scale\"],\n        \"probability\": [True]\n    },\n    {\n        \"kernel\": [\"poly\"],\n        \"degree\": [1, 3, 5],\n        \"gamma\": [\"auto_deprecated\", \"scale\"]\n    }\n]","57c09ac8":"grid_search = GridSearchCV(\n    svm_classifier,\n    parameter_grid,\n    cv=5,\n    scoring=\"accuracy\",\n    n_jobs=2,\n    verbose=1,\n    return_train_score=True\n)","9420eb88":"grid_search.fit(X_train, y_train)","55042af6":"grid_search.best_params_","87b52a44":"grid_search.best_estimator_","32d0f227":"svm_classifier = grid_search.best_estimator_","65f05e25":"cross_val_score(svm_classifier, X_train, y_train, cv=5, scoring=\"accuracy\").mean()","61c8cae9":"svm_classifier.fit(X_train, y_train)\ngradient_boost_classifier.fit(X_train, y_train)\nlogistic_regression_classifier.fit(X_train, y_train)\nlda_classifier.fit(X_train, y_train)\nxgboost_classifier.fit(X_train, y_train)","5a3d6b18":"voting_classifier = VotingClassifier(\n    estimators=[\n        (\"svc\", svm_classifier),\n        (\"gradient_boost\", gradient_boost_classifier),\n        (\"logistic_regression\", logistic_regression_classifier),\n        (\"lda\", lda_classifier),\n        (\"xgboost\", xgboost_classifier)\n    ],\n    voting=\"soft\"\n)","a3dee7ae":"voting_classifier.fit(X_train, y_train)","50b97e7f":"cross_val_score(voting_classifier, X_train, y_train, cv=10, scoring=\"accuracy\").mean()","651ff84a":"predictions = voting_classifier.predict(X_test)","2ddb2a16":"submission = pd.DataFrame(\n    {\n        \"PassengerId\": passenger_id,\n        \"Survived\": predictions\n    }\n)","1ef68dfa":"submission.head(10)","3a61d508":"# Write the submission DataFrame to a CSV file using the constructed filename\nsubmission.to_csv(\"submission.csv\", index=False)","266be355":"## _XGBoost Classifier_","bdb11865":"# Explore the Data\n\n## _Correlation Matrix_\n\nThe correlation matrix only works on numerical features, so we're going to convert the labels of the categorical features using scikit-learn's `LabelEncoder`.\n\nThe `LabelEncoder` must first be _fit_ to the data using the `fit()` method, which determines how many labels are in a category and what numeric values to assign to those categories. The `transform()` method then replaces the categorical values with the newly determined numeric values. The `fit_transform` method performs both operations.\n\nWe are going to store the label encoded data in a new variable, _all_data_encoded_, because we'll only use it to explore correlations and feature importances. We're going to use a different kind of encoding for categorical features that we'll feed into our machine learning models.","3bee57b8":"The `RobustScaler` returns a numpy array, so at this point our data is no longer in the form of a pandas DataFrame.","1626444d":"**Observations**\n- Again we see the importance of 'Sex', 'Pclass', and 'Fare'\n- The importance of 'Title' is not surprising given its relation to 'Sex'\n\n# Final Data Preparation\n\n## _Drop Unimportant and Correlated Features_\n\nWe don't want multiple features that measure the same thing, as this skews the importance of the underlying data. We have two decisions to make:\n- **Drop 'FamilySize'**\n    - While 'FamilySize' showed a slightly higher importance, 'Parch' was noticeably more correlated with survival, so we'll keep the set of 'Parch' & 'SibSp' \n- **Drop 'Cabin'**\n    - 'Pclass' shows a higher correlation to surival and a slightly higher feature importance, so we'll keep it and drop 'Cabin'\n    \n**Note**: While we're dropping features that we created earlier, it's still worth going through the process of creating them. You won't know if it is useful until you measure its importance later on. As mentioned earlier, machine learning involves a lot of trial and error, so throw everything against the wall and see what sticks.","c99ef242":"### Fare\n\n'Fare' is only missing 1 value, so we'll again replace it with the mode:","5db62110":"There is a pretty clear correlation between survival and the fare amount paid. Higher fare groups, where a higher fare was paid, have increased rates of survival. This could be related to socioeconomic standing or simple logistics. Passengers who paid lower fares were likely housed in the bowels of the ship where it would be much more difficult to escape to the deck and available lifeboats.\n\n### Create Cabin Groups\n\nPlaying off of the above point, cabin numbers may indication positions on the boat where escape was easier than others. We'll create it and see.\n\nFirst, extract the letter from the cabin number:","69e42b7f":"## _Linear Discriminant Analysis Classifier_","868d8eca":"After all of that, I'm just going to cheat and fit the best classifiers to the data using the parameters use in training :)","3671de0d":"# TITANIC - INTRODUCTORY KAGGLE WORKFLOW\n\n# Intro\n\nThis notebook serves as both my first Kaggle submission(s) as well as an attempt to establish a personal baseline Kaggle workflow. It incorporates the techniques from some of the popular Kaggle kernels available for the Titanic competition, as well as influences from outside of Kaggle. I want this to serve as a decent reference, both for myself and others, so I have explained things to a fair level of detail. I don't explain every method or function call, or go into detail about how things like the `pandas` library works, but even a beginner should be able to follow along.\n\n## _References\/Inspiration_\n\n[Introduction to Ensembling\/Stacking in Python](https:\/\/www.kaggle.com\/arthurtok\/introduction-to-ensembling-stacking-in-python)\n\n[Titanic Data Science Solutions](https:\/\/www.kaggle.com\/startupsci\/titanic-data-science-solutions)\n\n[Titanic Top 4% with ensemble modeling](https:\/\/www.kaggle.com\/yassineghouzam\/titanic-top-4-with-ensemble-modeling)\n\n# Dataset\n\n## _Source_\n\nhttps:\/\/www.kaggle.com\/c\/titanic\/data\n\n## _Description_\n\nDataset containing 891 samples of passenger data labeled for survival, split into `train` and `test` sets. The `train` set contains 12 features while the `test` set contains 11 features.\n\n## _Goal_\n\nPredict the survival of a passenger based on the available features.\n\n## _Data Dictionary_\n\n|Variable|Definition|Key|\n|:--|:--|:--|\n|**survival**|Survival|No = 0, Yes = 1|\n|**pclass**|Ticket class|First Class = 1, Second Class = 2, Third Class = 3|\n|**name**|Passenger name||\n|**sex**|Gender||\n|**age**|Age in years||\n|**sibsp**|# of siblings \/ spouses aboard the ship||\n|**parch**|# of parents \/ children aboard the ship||\n|**ticket**|Ticket number||\n|**fare**|Passenger fare||\n|**cabin**|Cabin number||\n|**embarked**|Port of Embarkation|Cherbourg = C, Queenstown = Q, Southampton = S|\n\n## _Data Notes_\n\n**pclass**: Approximate representation of socio-economic status\n- 1st = Upper Class\n- 2nd = Middle Class\n- 3rd = Lower Class\n\n**age**: Is fractional if less than 1. If age is estimated, will take the form x.5\n\n**sibsp**:\n- _Sibling_ = brother, sister, stepbrother, stepsister\n- _Spouse_ = husband, wife (mistresses and fiances are ignored)\n\n**parch**:\n- _Parent_ = mother, father\n- _Child_ = daughter, son, stepdaughter, stepson\n- Some children travelled only with a nanny, and thus parch = 0 in those instances\n\n# Setup\n\n## _Import Libraries_","754d8919":"**Observations:**\n- 'Age' is not normally distributed &mdash; may be worth normalizing\n- 'Fare', 'Parch', and 'SibSp' are all very positively skewed (data concentrated on the left indicates a positive skew; data concentrated on the right would indicate a negative skew)","38ef7cec":"# Submission","697f8268":"## _Extra Trees Classifier_","68969839":"## _Configuration_","99cc451b":"We can see a clear impact on survival based on age.\n\n### Create Family Size\n\nCombine 'SibSp' and 'Parch' into one 'FamilySize' feature. Add +1 to include the passenger in the size of the family:","0a7590cc":"## _Gradient Boosting Classifier_","9da9bc3d":"## _Alternative method for calculating missing age_\n\nThis method not only shows a different programmatic approach, but derives age from 'SibSp' and 'Parch'. I did not see any improvement in predictions using this approach, so I kept the original method in place. Including the below code for reference.","d192f377":"### Age\n\n'Age' is a continuous value, and there are a lot missing. We don't want to just fill it in with the mode. Instead, we'll look at features correlated to 'Age', find the median 'Age' value for each combination, and then use those average values to fill in what is missing.","487571e7":"We'll reclassify the uncommon titles to consolidate things:","d0b0e086":"In the testing dataset, 'Age' and 'Cabin' contain a substantial number of null values. 'Fare' also contains 1 null value. Since this is the `test` set, there are no target values (the 'Survived' column.","03ecc818":"## _Evaluate Feature Importance_\n\nScikit-learn's `RandomForestClassifier()` includes a `feature_importances_` property which can be used to measure how important different features are. We'll train a model on the training dataset and then examine the feature importances. In order to do this, we must first split the data into the `X_train` (training data) and `y_train` (training targets) datasets. We'll do this on the `all_data_encoded` DataFrame:","c518a1fc":"## _Create New Features_\n\n### Extract Title from Name","0e00ae1d":"## _Support Vector Machine Tuning_","40878445":"# Evaluate Models\n\nWe use scikit-learn's `cross_val_score()` function to test each model. I do a little bit of brute force[](http:\/\/) parameter tuning by setting some basic parameters and testing out different values.\n\n## _SGD Classifier_","d6272822":"# Fine-Tune the Best Models\n\nWhile it is time consuming and does not always result in drastic improved predictions, parameter tuning is still an important part of the machine learning workflow. While it is possible to guess which parameters to use, this is time consuming and inefficient. A better method is to use a grid or random search. Scikit-learn convienently provides functions to do both. The below code shows how to tune the Support Vector Machine classifier using a grid search.","6b0d2a7a":"## _Feature Scaling_\n\nBefore feeding the data into our models, we're going to scale the data. At first I used scikit-learn's `StandardScaler()`. But after seeing the `RobustScaler()` used in a couple other kernels, I tried it. The `RobustScaler()` provides resistance to outliers and results in better predictions, so we'll we use that one.","3046be5b":"Using the groups returned by the `cut()` function, we replace 'Age' with numerical categories. Since most 'Age' values are rounded, we use rounded values when defining the groups.","cb01b2e2":"## _Convert Categorical Data_\n\nMost machine learning algorithms only accept numeric data, so we need to convert text categories to numeric values. We could just use the label encoded dataset we created for looking at feature importance, but the machine learning algorithms may interpret meaning with the distances between numeric values.  \n\nFor example, the distance between 'FamilySize' values of 2 and 7 has meaning. One is a small family while the other is a large family. The same principle applies to 'Age'. Our encoded value of 0 means someone is very young while an encoded value of 4 means someone is much older. With similar numerical values applied to 'Title', an algorithm may interpret some meaning if Master has a value of 1 and Mrs has a value of 4. We know however that these numerical values are arbitrary.\n\nTo get around this issue, we use One-Hot Encoding. With One-Hot Encoding, a column is created for each possible feature value. So for 'Sex', there is a column for female and a column for male. If the passenger is female, the 'Sex_female' column = 1 and the 'Sex_male' column = 0. The reverse would be true if the passenger is male.\n\nWe easily apply One-Hot Encoding using pandas `get_dummies()` function below:","24920b29":"Here we see that male passengers are on average almost 3 years older than female passengers.","383a46c6":"# Emsembling via a Voting Classifier\n\nBy using a voting classifier, we use the predictions of several different models and hopefully make better predictions as a result.","f9df96b2":"There seems to be a trend toward larger families having a lower rate of survival.\n\n### Label if Passenger is Alone\n\nIf 'FamilySize' = 1, then the passenger is alone aboard the ship.","ab4d3b63":"# Inspect the Data","b84853eb":"**Observations**:\n- There are significant correlations between 'FamilySize', 'Parch', and 'SibSp', which is unsurprising since 'FamilySize' was derived from the other two features. We should consider keeping either the set of 'Parch' & 'SibSp' or 'FamilySize'.\n- 'Pclass' is strongly correlated with 'Cabin'. We should consider dropping one or the other.","1d536529":"It's not surprising that people who were not in cabins had a lower survival rate. But it would be better than having a cabin number starting with T...\n\n## _Drop Irrelevant Features_\n\nThere are some features that we can confidently assume have no bearing on survival ('PassengerId', 'Ticket') and others we no longer need ('Name'). We'll drop these features.","1d94800e":"## _One vs One Classifier_","c38a11ae":"# Load Data\n\nWe're going to load both train.csv and test.csv into pandas DataFrames. A DataFrame is sort of like an in-memory spreadsheet and an essential tool when working with datasets in Python.\n\nWhy two datasets? The `train` set is used to train the machine learning model. The `test` set is used to test how well the machine learning model works.","3b2bd355":"## _Random Forest Classifier_","de1c7e78":"# Feature Engineering\n\nFeature engineering is the most time consuming and (often) most influential part of the machine learning workflow. How well feature engineering is executed will have a significant impact on how well the machine learning models perform. Take your time and do this right.\n\n## _Handle Missing Values_\n\n**NOTE:** Missing values can be referred to as null, NA, or NaN (Not a Number). They all mean the same thing &mdash; the absence of a value.\n\nThere are two strategies for altering data in DataFrames: editing `inplace` or reassigning values. Some methods, such as `drop()` have a parameter to edit data `inplace`, which directly alters the underlying DataFrame. Alternatively, you can perform a manipulation that returns a _copy_ of a column and then reassign that column with the new values. This can also be done with an entire DataFrame. I use both inplace editing and value reassignment in this notebook.\n\n**NOTE:** Sometimes pandas returns a _view_ (which edits the underlying DataFrame) and sometimes it returns a _copy_ (which does not edit the underlying DataFrame). It can be confusing when one is returned instead of the other. If you are trying to edit data in a DataFrame and no changes are being made, it's probably an issue of editing a copy instead of a view. This is why I have validation code after each transformation that validates the changes being made. More information on views versus copies can be found [here](http:\/\/pandas.pydata.org\/pandas-docs\/stable\/user_guide\/indexing.html#indexing-view-versus-copy). \n\n### Embarked\n\n'Embarked' is only missing 2 values, so we'll simply fill in the missing values with the most frequently occurring value (mode) in the training dataset. Remember, _evalute_ the training set and _transform_ both datasets:","ff186b4f":"## _AdaBoost Classifier_","845bd0a7":"**Observations:**\n- 38% of the passengers in the training set survived\n- The majority of 'Parch' values are 0, as even the 75 percentile is 0\n- Similarly, the majority of 'SibSp' values are also 0\n\n## _Create a Histogram_\n\nViewing a histogram of the numerical features gives us an idea of the distribution of values.","2bf58013":"## _Split into X and Y Datasets_\n\nWe split the data again, this time using the One-Hot Encoded data:","5ddce451":"Using the `crosstab()` function, we look at how each title lines up with gender:","4da74dcc":"**Observations**:\n- The positive correlation of 'Fare' and the negative correlation of 'Pclass' with survival may be related &mdash; does higher socioeconomic standing increase your chance for survival or is this related to a person's location on the ship?\n- The most significant correlation with survival is the negative correlation for 'Sex'\n- 'IsAlone' has a negative correlation with survival &mdash; maybe due to mothers with children escaping before single men?\n\n## _Pearson Correlation Heatmap_\n\nWe'll use a heatmap to see how features are correlated.","0938e474":"It looks like passengers who were alone had a lower rate of survival.\n\n### Create Fare Groups\n\nJust like with age groups, fare groups did not influence my predictions. But we don't know how a new feature will influence predictions until we test it, so don't be afraid to create new features. Machine learning involves a lot of trial and error.","d55d45c0":"In the training dataset, both 'Age' and 'Cabin' contain a substantial number of null values. 'Embarked' also contains 2 null values.\n\nThe 5 features with a data type of `object` are strings.","b5183ad2":"Save the 'PassengerId' from the `test` data because we'll need it when we submit our predictions:","f4583094":"## _SVM Classifier_","8792141a":"Now our entire DataFrame contains only numerical features.","f570f2c4":"## _Logistic Regression Classifier_","bc0aec8b":"Unsurprisingly, we see that female titles (Mrs & Miss) have a much higher survival rate. The title of Mr significantly reduces the rate of survival.\n\n### Create Age Groups\n\nWe'll group ages into several age groups, because that's what all the cool kernels are doing. First determine the groups based on the `train` set, then apply the groups to both `train` and `test` sets. \n\nI tested predictions with age groups and without them, and there was no impact on my final score. As an instructive exercise, they are included in the workflow:","6b5c49df":"### Cabin\n\n'Cabin' contains a lot of missing values, but in this case a null value does not indicate unknown data. Instead, a null indicates that the passenger did not have a cabin room. To handle this, we'll fill in nulls with \"None\" and handle the cabin numbers later.","125e7af1":"The features with the strongest correlation to 'Age' are 'SibSp' and 'Pclass'. However, there are some combinations of 'SibSp' and 'Pclass' which do not have any age values, resulting in a mean of `NaN`. The same thing occurs with the combination of 'Pclass' and 'Parch'.  Since there is no point to filling in null values with more nulls, we're going to use the combination of 'Pclass' and 'Sex' instead. \n\n'Sex' values are either male or female. 'Pclass' values range from 1 to 3.  We iterate over every combination of 'Sex' and 'Pclass' and find the average 'Age', then fill in the missing 'Age' values:","a9341fab":"## _Join Train and Test into One DataFrame_\n\nA big part of the machine learning workflow is cleaning and transforming data. An important point is that we want to _evaluate_ the `train` dataset but _transform_ data in both datasets. The `test` dataset represents new data that we don't know about. In the 'real world', we would train a model and then use that model to make predictions on new data. The training data _influences_ the model while new data (`test`) does not. Thus we make decisions based on the `train` dataset but not `test`.\n\nNow before making predictions, even new data needs to be changed. Machine learning algorithms almost universally will not work with text data, so we either need to remove that text data or transform it into numerical data. _This has to happen before making predictions_. So if we drop a column or transform the values in `train`, we should do the same thing in `test`.\n\nSo that data transformations can be done with less code, I am going to create a [hierarchical DataFrame](http:\/\/pandas.pydata.org\/pandas-docs\/stable\/user_guide\/advanced.html) by concatenating the `train` and `test` DataFrames together. The drawback is that there is now an extra level of indexing, which makes selecting data a little more complicated. There are alternative approaches, such as looping through a list of DataFrames, handling each DataFrame individually, or using scikit-learn pipelines."}}