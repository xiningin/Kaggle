{"cell_type":{"c82d078d":"code","29cf8cbd":"code","71a2aafe":"code","6a5eba6e":"code","2ba55a88":"code","96da1d79":"code","9f8cc2fc":"code","8606f0b7":"code","dda046c0":"code","f4507ccb":"code","b115c4d9":"code","341d4d9e":"code","704b3516":"code","b49a333a":"code","9f02a6c8":"code","2221b266":"code","56d17bed":"code","e066ee02":"code","091c2fce":"code","afa7d89a":"code","706f4440":"code","ee59e6d1":"code","b002a635":"code","3bec0b49":"code","32bf4275":"code","b0355d1d":"code","2bac89a5":"code","6a1fc3cf":"code","89d5cbcf":"code","bfca19a5":"code","44bd26a1":"code","8adca938":"code","6abafb4a":"code","4f6be6d9":"code","cacc3287":"code","c2b3456a":"code","43ee1869":"code","b426c544":"code","f8180038":"code","7c50ea9b":"code","051d9a82":"code","46ac6dbe":"code","bea41c15":"code","b18f59a5":"code","2ef575ea":"code","6c52f561":"code","8af1643e":"markdown","5a0a4360":"markdown","657ac9f5":"markdown","ddbb9f09":"markdown","5b11905f":"markdown","176fa8fe":"markdown","672eafea":"markdown","7dddd42f":"markdown","c6088749":"markdown"},"source":{"c82d078d":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport cv2\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\nimport time\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","29cf8cbd":"!pip install timm","71a2aafe":"img_dir = '..\/input\/museum-my\/data\/img\/'\nphoto_dir = '..\/input\/museum-my\/data\/photo_examples\/'\npath = '..\/input\/museum-my\/data\/'\n\nphoto_imgs = os.listdir(photo_dir)\nphoto_imgs","6a5eba6e":"os.listdir(path)","2ba55a88":"df = pd.read_csv(path + 'train.csv')\ndf","96da1d79":"def prepare_img(img_path, color_low = np.array([60 , 60, 60]), color_high = np.array([255, 255, 255])):\n    img = cv2.imread(img_path, 0)\n\n    hsv_img = cv2.cvtColor(cv2.imread(img_path), cv2.COLOR_BGR2HSV)\n\n    curr_mask = cv2.inRange(hsv_img, color_low, color_high)\n    hsv_img[curr_mask > 0] = ([200,200,200])\n    RGB_again = cv2.cvtColor(hsv_img, cv2.COLOR_HSV2RGB)\n    gray = cv2.cvtColor(RGB_again, cv2.COLOR_RGB2GRAY)\n    ret, threshold = cv2.threshold(gray, 90, 255, 0)\n    threshold = threshold[100:-100, 100:-100]\n    x, y = np.where(threshold == 0)\n    img = img[100:-100, 100:-100]\n    return img[min(x):max(x), min(y):max(y)]","9f8cc2fc":"plt.imshow(cv2.imread(photo_dir + photo_imgs[1]))\nplt.show()\nplt.imshow(prepare_img(photo_dir + photo_imgs[1]))\nplt.show()","8606f0b7":"def get_templates(img):\n    input_shape_x = img.shape[0]\n    input_shape_y = img.shape[1]\n    template_images = []\n    shift = 15\n    size_x = int(0.65*input_shape_x)\n    size_y = int(0.65*input_shape_y)\n    template_images.append(img[shift:size_x, -size_y : -shift])\n    template_images.append(img[shift:size_x, shift:size_y])\n    template_images.append(img[-size_x : -shift, -size_y : -shift])\n    template_images.append(img[-size_x : -shift, shift:size_y])\n\n    quarter_x = input_shape_x\/\/4\n    quarter_y = input_shape_y\/\/4\n    template_images.append(img[quarter_x : 3*quarter_x, quarter_y:3*quarter_y])\n\n    template_shape = []\n    for i in range(5):\n        template_shape.append(template_images[i].shape[::-1])\n    return template_images\n\n\ndef prepare_img(img_path, color_low = np.array([60 , 60, 60]), color_high = np.array([255, 255, 255])):\n    img = cv2.imread(img_path, 0)\n\n    hsv_img = cv2.cvtColor(cv2.imread(img_path), cv2.COLOR_BGR2HSV)\n\n    curr_mask = cv2.inRange(hsv_img, color_low, color_high)\n    hsv_img[curr_mask > 0] = ([200,200,200])\n    RGB_again = cv2.cvtColor(hsv_img, cv2.COLOR_HSV2RGB)\n    gray = cv2.cvtColor(RGB_again, cv2.COLOR_RGB2GRAY)\n    ret, threshold = cv2.threshold(gray, 90, 255, 0)\n    threshold = threshold[100:-100, 100:-100]\n    x, y = np.where(threshold == 0)\n    img = img[100:-100, 100:-100]\n    return img[min(x):max(x), min(y):max(y)]\n\ndef get_templates(img):\n    input_shape_x = img.shape[0]\n    input_shape_y = img.shape[1]\n    template_images = []\n    shift = 15\n    size_x = int(0.65*input_shape_x)\n    size_y = int(0.65*input_shape_y)\n    template_images.append(img[shift:size_x, -size_y : -shift])\n    template_images.append(img[shift:size_x, shift:size_y])\n    template_images.append(img[-size_x : -shift, -size_y : -shift])\n    template_images.append(img[-size_x : -shift, shift:size_y])\n\n    quarter_x = input_shape_x\/\/4\n    quarter_y = input_shape_y\/\/4\n    template_images.append(img[quarter_x : 3*quarter_x, quarter_y:3*quarter_y])\n\n    template_shape = []\n    for i in range(5):\n        template_shape.append(template_images[i].shape[::-1])\n    return template_images\n\ndef finde_img(df, photo_img_path, img_dir):\n    photo_img = prepare_img(photo_img_path)\n#     templates = get_templates(photo_img)\n    threshold = 0.6\n    counts = []\n    for i in range(len(df)):\n        img = cv2.imread(img_dir + df.id[i], 0)\n#         img = cv2.resize(cv2.imread(img_dir + df.id[i], 0), (photo_img.shape[1], photo_img.shape[0]))\n        method = eval('cv2.TM_CCOEFF_NORMED')\n        count = 0\n        templates = get_templates(cv2.resize(photo_img, (img.shape[1], img.shape[0])))\n        for template in templates:\n            res = cv2.matchTemplate(img, template, method)\n            loc = np.where(res >= threshold)\n            if len(loc[0])!=0:\n                count += 1\n        counts.append(count)\n        \n    return img_dir + df.id[np.argmax(counts)]","dda046c0":"num = 5\nstart_time = time.time()\nprint('\u0418\u0441\u0445\u043e\u0434\u043d\u043e\u0435 \u0438\u0437\u043e\u0431\u0440\u0430\u0436\u0435\u043d\u0438\u0435 (\u0444\u043e\u0442\u043e)')\nplt.imshow(prepare_img(photo_dir + photo_imgs[num]))\nplt.show()\nfinded_img = finde_img(df, photo_dir + photo_imgs[num], img_dir)\nprint('\u041e\u0442\u0432\u0435\u0442 \u043f\u043e\u0438\u0441\u043a\u0430')\nplt.imshow(cv2.imread(finded_img))\nplt.show()\nprint(\"\u0412\u0440\u0435\u043c\u044f \u0437\u0430\u0442\u0440\u0430\u0447\u0435\u043d\u043d\u043e\u0435 \u043d\u0430 \u0432\u044b\u0447\u0438\u0441\u043b\u0435\u043d\u0438\u0435 \" + str(time.time() - start_time) + str(' \u0441\u0435\u043a\u0443\u043d\u0434\u044b'))","f4507ccb":"def finde_img_by_one(df, photo_img_path, img_dir):\n    photo_img = prepare_img(photo_img_path)\n#     templates = get_templates(photo_img)\n    threshold = 0.6\n    counts = []\n    for i in range(len(df)):\n        img = cv2.imread(img_dir + df.id[i], 0)\n#         img = cv2.resize(cv2.imread(img_dir + df.id[i], 0), (photo_img.shape[1], photo_img.shape[0]))\n        resized_photo = cv2.resize(photo_img, (img.shape[1], img.shape[0]))\n        method = eval('cv2.TM_CCOEFF_NORMED')\n        \n        templates = get_templates(resized_photo)\n        res = cv2.matchTemplate(img, templates[-1], method)\n#         res = cv2.matchTemplate(img, photo_img, method)\n        loc = np.where(res >= threshold)\n#         if len(loc[0]) > 500:\n#             return img_dir + df.id[i]\n        counts.append(len(loc[0]))\n        \n    return img_dir + df.id[np.argmax(counts)]","b115c4d9":"num = 0\nstart_time = time.time()\nplt.imshow(prepare_img(photo_dir + photo_imgs[num]))\nplt.show()\n\nfinded_img = finde_img_by_one(df, photo_dir + photo_imgs[num], img_dir)\nprint(\"\u0412\u0440\u0435\u043c\u044f \u0437\u0430\u0442\u0440\u0430\u0447\u0435\u043d\u043d\u043e\u0435 \u043d\u0430 \u0432\u044b\u0447\u0438\u0441\u043b\u0435\u043d\u0438\u0435 \" + str(time.time() - start_time))\nplt.imshow(cv2.imread(finded_img))\nplt.show()","341d4d9e":"num = 3\n#\u0442\u0443\u0442 \u043f\u0440\u043e\u043c\u0430\u0437\u0430\u043b\u0438\nstart_time = time.time()\nplt.imshow(prepare_img(photo_dir + photo_imgs[num]))\nplt.show()\n\nfinded_img = finde_img_by_one(df, photo_dir + photo_imgs[num], img_dir)\nprint(\"\u0412\u0440\u0435\u043c\u044f \u0437\u0430\u0442\u0440\u0430\u0447\u0435\u043d\u043d\u043e\u0435 \u043d\u0430 \u0432\u044b\u0447\u0438\u0441\u043b\u0435\u043d\u0438\u0435 \" + str(time.time() - start_time))\nplt.imshow(cv2.imread(finded_img))\nplt.show()","704b3516":"num = 4\nstart_time = time.time()\nplt.imshow(prepare_img(photo_dir + photo_imgs[num]))\nplt.show()\n\nfinded_img = finde_img_by_one(df, photo_dir + photo_imgs[num], img_dir)\nprint(\"\u0412\u0440\u0435\u043c\u044f \u0437\u0430\u0442\u0440\u0430\u0447\u0435\u043d\u043d\u043e\u0435 \u043d\u0430 \u0432\u044b\u0447\u0438\u0441\u043b\u0435\u043d\u0438\u0435 \" + str(time.time() - start_time))\nplt.imshow(cv2.imread(finded_img))\nplt.show()\nprint(finded_img)","b49a333a":"num = 5\nstart_time = time.time()\nplt.imshow(prepare_img(photo_dir + photo_imgs[num]))\nplt.show()\n\nfinded_img = finde_img_by_one(df, photo_dir + photo_imgs[num], img_dir)\nprint(\"\u0412\u0440\u0435\u043c\u044f \u0437\u0430\u0442\u0440\u0430\u0447\u0435\u043d\u043d\u043e\u0435 \u043d\u0430 \u0432\u044b\u0447\u0438\u0441\u043b\u0435\u043d\u0438\u0435 \" + str(time.time() - start_time))\nplt.imshow(cv2.imread(finded_img))\nplt.show()","9f02a6c8":"num = 6\nstart_time = time.time()\nplt.imshow(prepare_img(photo_dir + photo_imgs[num]))\nplt.show()\n\nfinded_img = finde_img_by_one(df, photo_dir + photo_imgs[num], img_dir)\nprint(\"\u0412\u0440\u0435\u043c\u044f \u0437\u0430\u0442\u0440\u0430\u0447\u0435\u043d\u043d\u043e\u0435 \u043d\u0430 \u0432\u044b\u0447\u0438\u0441\u043b\u0435\u043d\u0438\u0435 \" + str(time.time() - start_time))\nplt.imshow(cv2.imread(finded_img))\nplt.show()","2221b266":"import timm \nfrom pprint import pprint\nimport torch\nimport torch.nn as nn\nfrom torchvision import transforms\n\nmodel_names = timm.list_models(pretrained=True)\npprint(model_names)","56d17bed":"class Identity(nn.Module):\n    def __init__(self):\n        super(Identity, self).__init__()\n        \n    def forward(self, x):\n        return x\n\nmodel = timm.create_model('tf_efficientnet_b0_ns', pretrained=True)\nmodel.classifier = Identity()\n\n# model = timm.create_model('efficientnet_b0', pretrained=True)\n# model.classifier = Identity()\nmodel.eval()","e066ee02":"img = cv2.resize(cv2.imread(img_dir + df.id[3]), (224, 224))\nplt.imshow(img)","091c2fce":"img = cv2.resize(cv2.imread(img_dir + df.id[3]), (224, 224))\/255.0\nstart_time = time.time()\nimg = np.expand_dims(img, 0).transpose((0, 3, 1, 2))\n\nansw = model(torch.from_numpy(img).float())\nprint('\u0412\u0440\u0435\u043c\u044f \u043d\u0430 \u043e\u0434\u043d\u043e \u0438\u0437\u043e\u0431\u0440\u0430\u0436\u0435\u043d\u0438\u0435 ' + str(time.time() - start_time))","afa7d89a":"\ntr = transforms.Compose([transforms.ToTensor(),\n                              transforms.Normalize(mean=[0.485, 0.456, 0.406],\n                                 std=[0.229, 0.224, 0.225]),\n                             ])\n\ndata_base = []\nfor i in range(len(df)):\n    img = cv2.resize(cv2.imread(img_dir + df.id[i]), (224, 224))\n    img = tr(img)\n    img = torch.unsqueeze(img, 0)\n    answ = model(img.float())\n    data_base.append(answ[0])","706f4440":"def finde_with_nn(model, data_base, photo_img_path, df, img_dir):\n    photo_img = cv2.resize(cv2.cvtColor(prepare_img(photo_img_path), cv2.COLOR_BGR2RGB), (224, 224))\n#     img = cv2.resize(cv2.imread(img_dir + df.id[3]), (224, 224))\/255.0\n    photo_img = tr(photo_img)\n    photo_img = torch.unsqueeze(photo_img, 0)\n    features_photo = model(photo_img.float())[0]\n    loss = nn.MSELoss()\n\n    losses = []\n    for img_feature in data_base:\n        losses.append(float(loss(img_feature, features_photo)))\n    return img_dir + df.id[np.argmin(losses)]\n    ","ee59e6d1":"num = 0\nphoto_img_path = photo_dir + photo_imgs[num]\n\nstart_time = time.time()\nfinded_img_path = finde_with_nn(model, data_base, photo_img_path, df, img_dir)\nprint('\u0412\u0440\u0435\u043c\u044f \u043d\u0430\u0445\u043e\u0436\u0434\u0435\u043d\u0438\u044f \u0438\u0437\u043e\u0431\u0440\u0430\u0436\u0435\u043d\u0438\u044f \u0441 \u043f\u043e\u043c\u043e\u0449\u044c\u044e \u043d\u0435\u0439\u0440\u043e\u0441\u0435\u0442\u0438 ' + str(time.time() - start_time))\nplt.imshow(cv2.imread(photo_img_path))\nplt.show()\n\nplt.imshow(cv2.imread(finded_img_path))\nplt.show()","b002a635":"num = 1\nphoto_img_path = photo_dir + photo_imgs[num]\n\nstart_time = time.time()\nfinded_img_path = finde_with_nn(model, data_base, photo_img_path, df, img_dir)\nprint('\u0412\u0440\u0435\u043c\u044f \u043d\u0430\u0445\u043e\u0436\u0434\u0435\u043d\u0438\u044f \u0438\u0437\u043e\u0431\u0440\u0430\u0436\u0435\u043d\u0438\u044f \u0441 \u043f\u043e\u043c\u043e\u0449\u044c\u044e \u043d\u0435\u0439\u0440\u043e\u0441\u0435\u0442\u0438 ' + str(time.time() - start_time))\nplt.imshow(cv2.imread(photo_img_path))\nplt.show()\n\nplt.imshow(cv2.imread(finded_img_path))\nplt.show()","3bec0b49":"num = 2\nphoto_img_path = photo_dir + photo_imgs[num]\n\nstart_time = time.time()\nfinded_img_path = finde_with_nn(model, data_base, photo_img_path, df, img_dir)\nprint('\u0412\u0440\u0435\u043c\u044f \u043d\u0430\u0445\u043e\u0436\u0434\u0435\u043d\u0438\u044f \u0438\u0437\u043e\u0431\u0440\u0430\u0436\u0435\u043d\u0438\u044f \u0441 \u043f\u043e\u043c\u043e\u0449\u044c\u044e \u043d\u0435\u0439\u0440\u043e\u0441\u0435\u0442\u0438 ' + str(time.time() - start_time))\nplt.imshow(cv2.imread(photo_img_path))\nplt.show()\n\nplt.imshow(cv2.imread(finded_img_path))\nplt.show()","32bf4275":"num = 3\nphoto_img_path = photo_dir + photo_imgs[num]\n\nstart_time = time.time()\nfinded_img_path = finde_with_nn(model, data_base, photo_img_path, df, img_dir)\nprint('\u0412\u0440\u0435\u043c\u044f \u043d\u0430\u0445\u043e\u0436\u0434\u0435\u043d\u0438\u044f \u0438\u0437\u043e\u0431\u0440\u0430\u0436\u0435\u043d\u0438\u044f \u0441 \u043f\u043e\u043c\u043e\u0449\u044c\u044e \u043d\u0435\u0439\u0440\u043e\u0441\u0435\u0442\u0438 ' + str(time.time() - start_time))\nplt.imshow(cv2.imread(photo_img_path))\nplt.show()\n\nplt.imshow(cv2.imread(finded_img_path))\nplt.show()","b0355d1d":"num = 4\nphoto_img_path = photo_dir + photo_imgs[num]\n\nstart_time = time.time()\nfinded_img_path = finde_with_nn(model, data_base, photo_img_path, df, img_dir)\nprint('\u0412\u0440\u0435\u043c\u044f \u043d\u0430\u0445\u043e\u0436\u0434\u0435\u043d\u0438\u044f \u0438\u0437\u043e\u0431\u0440\u0430\u0436\u0435\u043d\u0438\u044f \u0441 \u043f\u043e\u043c\u043e\u0449\u044c\u044e \u043d\u0435\u0439\u0440\u043e\u0441\u0435\u0442\u0438 ' + str(time.time() - start_time))\nplt.imshow(cv2.imread(photo_img_path))\nplt.show()\n\nplt.imshow(cv2.imread(finded_img_path))\nplt.show()","2bac89a5":"num = 5\nphoto_img_path = photo_dir + photo_imgs[num]\n\nstart_time = time.time()\nfinded_img_path = finde_with_nn(model, data_base, photo_img_path, df, img_dir)\nprint('\u0412\u0440\u0435\u043c\u044f \u043d\u0430\u0445\u043e\u0436\u0434\u0435\u043d\u0438\u044f \u0438\u0437\u043e\u0431\u0440\u0430\u0436\u0435\u043d\u0438\u044f \u0441 \u043f\u043e\u043c\u043e\u0449\u044c\u044e \u043d\u0435\u0439\u0440\u043e\u0441\u0435\u0442\u0438 ' + str(time.time() - start_time))\nplt.imshow(cv2.imread(photo_img_path))\nplt.show()\n\nplt.imshow(cv2.imread(finded_img_path))\nplt.show()","6a1fc3cf":"num = 6\nphoto_img_path = photo_dir + photo_imgs[num]\n\nstart_time = time.time()\nfinded_img_path = finde_with_nn(model, data_base, photo_img_path, df, img_dir)\nprint('\u0412\u0440\u0435\u043c\u044f \u043d\u0430\u0445\u043e\u0436\u0434\u0435\u043d\u0438\u044f \u0438\u0437\u043e\u0431\u0440\u0430\u0436\u0435\u043d\u0438\u044f \u0441 \u043f\u043e\u043c\u043e\u0449\u044c\u044e \u043d\u0435\u0439\u0440\u043e\u0441\u0435\u0442\u0438 ' + str(time.time() - start_time))\nplt.imshow(cv2.imread(photo_img_path))\nplt.show()\n\nplt.imshow(cv2.imread(finded_img_path))\nplt.show()","89d5cbcf":"num = 7\nphoto_img_path = photo_dir + photo_imgs[num]\n\nstart_time = time.time()\nfinded_img_path = finde_with_nn(model, data_base, photo_img_path, df, img_dir)\nprint('\u0412\u0440\u0435\u043c\u044f \u043d\u0430\u0445\u043e\u0436\u0434\u0435\u043d\u0438\u044f \u0438\u0437\u043e\u0431\u0440\u0430\u0436\u0435\u043d\u0438\u044f \u0441 \u043f\u043e\u043c\u043e\u0449\u044c\u044e \u043d\u0435\u0439\u0440\u043e\u0441\u0435\u0442\u0438 ' + str(time.time() - start_time))\nplt.imshow(cv2.imread(photo_img_path))\nplt.show()\n\nplt.imshow(cv2.imread(finded_img_path))\nplt.show()","bfca19a5":"df","44bd26a1":"model = timm.create_model('mobilenetv2_100', pretrained=True)\n# model.conv_stem = nn.Conv2d(1, 32, (3,3), (2,2), padding = (1,1), bias = False)\nmodel.classifier = nn.Linear(1280, len(df))\nprint(model)","8adca938":"torch.save(model.state_dict(), 'museum_model.pth')","6abafb4a":"from torch.utils.data import DataLoader, Dataset\nfrom torchvision import transforms\n\n\nclass MuseumDataset(nn.Module):\n    def __init__(self, root_dir, df, transforms=None, output_label=True):\n        self.root_dir = root_dir\n        self.df = df\n        self.transforms = transforms\n        self.output_label = output_label\n    def __len__(self):\n        return len(self.df)\n    \n    def __getitem__(self, index):\n        img_path = os.path.join(self.root_dir, self.df.iloc[index, 0])\n        img = cv2.imread(img_path, 0)\n        img = cv2.resize(img, (224, 224))\/255.0\n#         img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\/255.0\n        label = self.df.iloc[index, 1]\n        \n        if self.transforms:\n            img = self.transforms(img)\n            \n        # do label smoothing\n        if self.output_label == True:\n            return img, label\n        else:\n            return img","4f6be6d9":"tr = transforms.Compose([transforms.ToTensor()])\n\nds = MuseumDataset(img_dir, df, tr)","cacc3287":"trainloader = torch.utils.data.DataLoader(ds, batch_size=4, shuffle=True)","c2b3456a":"import torch.optim as optim\n\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=0.001)","43ee1869":"for epoch in range(10):  # loop over the dataset multiple times\n\n    running_loss = 0.0\n    accuracy = 0\n    for i, data in enumerate(trainloader, 0):\n        # get the inputs; data is a list of [inputs, labels]\n        inputs, labels = data\n    \n        # zero the parameter gradients\n        optimizer.zero_grad()\n\n        # forward + backward + optimize\n        outputs = model(inputs.float())\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n        \n        # print statistics\n        accuracy += (torch.argmax(outputs, axis = -1) == labels).sum()\n\n    print('accuracy = ' + str(accuracy\/len(df)))\n\n        \n#         print(outputs)\n#         running_loss += loss.item()\n#         if i % 2000 == 1999:    # print every 2000 mini-batches\n#             print('[%d, %5d] loss: %.3f' %\n#                   (epoch + 1, i + 1, running_loss \/ 2000))\n#             running_loss = 0.0\n\nprint('Finished Training')","b426c544":"num = 4\nphoto_img_path = photo_dir + photo_imgs[num]\n\nphoto_img = cv2.resize(prepare_img(photo_img_path), (224, 224))\/255.0\nphoto_img = tr(photo_img)\nphoto_img = torch.unsqueeze(photo_img, 0)\nfeatures_photo = model(photo_img.float())\n\nfinded_img = img_dir + df.iloc[int(torch.argmax(features_photo)), 0]\nplt.imshow(cv2.imread(photo_img_path))\nplt.show()\n\nplt.imshow(cv2.imread(finded_img))\nplt.show()","f8180038":"num = 2\nphoto_img_path = photo_dir + photo_imgs[num]\nimg = cv2.resize(prepare_img(photo_img_path), (224, 224))\n\nret,thresh1 = cv2.threshold(img,100,255,cv2.THRESH_BINARY)\n\nplt.imshow(thresh1)","7c50ea9b":"num = 100\nimg_path = img_dir + df.iloc[num, 0]\nreal_img = cv2.resize(cv2.imread(img_path,0), (224, 224))\n\nret,thresh = cv2.threshold(real_img,140,255,cv2.THRESH_BINARY)\nplt.imshow(thresh)","051d9a82":"plt.imshow(cv2.imread(img_path))","46ac6dbe":"img_path = img_dir + df.iloc[num, 0]\nreal_img = cv2.resize(cv2.imread('..\/input\/museum-my\/data\/img\/images (1).jpeg',0), (224, 224))\n\nret,thresh = cv2.threshold(real_img,100,255,cv2.THRESH_BINARY)\nplt.imshow(thresh)","bea41c15":"cv2.resize(prepare_img(photo_img_path), (224, 224))","b18f59a5":"torch.argmax(features_photo)","2ef575ea":"img = cv2.resize(cv2.cvtColor(cv2.imread(finded_img), cv2.COLOR_BGR2RGB), (224, 224))\/255.0\nimg = tr(img)\nimg = torch.unsqueeze(img, 0)\nansw = model(img.float())\ntorch.argmax(answ)","6c52f561":"img = cv2.resize(cv2.imread(finded_img, 0), (224, 224))\/255.0\nimg = tr(img)\nimg = torch.unsqueeze(img, 0)\nmodel(img.float())\n","8af1643e":"**\u041f\u0440\u0435\u0434\u044b\u0434\u0443\u0449\u0438\u0439 \u0432\u0430\u0440\u0438\u0430\u043d\u0442 \u0441\u0447\u0438\u0442\u0430\u043b \u043f\u043e 5 \u0438\u0437\u043e\u0431\u0440\u0430\u0436\u0435\u043d\u0438\u044f\u043c \u043f\u043e\u043f\u0440\u043e\u0443\u0435\u043c \u043f\u043e \u043e\u0434\u043d\u043e\u043c\u0443 \u0440\u0430\u0434\u0438 \u0443\u0441\u043a\u043e\u0440\u0435\u043d\u0438\u044f**","5a0a4360":"**\u041f\u0440\u043e\u0432\u0435\u0440\u044f\u0435\u043c**","657ac9f5":"**\u0421\u043e\u0437\u0434\u0430\u0435\u043c \u0431\u0430\u0437\u0443 \u0434\u0430\u043d\u043d\u044b\u0445 \u0438\u0437 \u0443\u0436\u0435 \u0433\u043e\u0442\u043e\u0432\u044b\u0445 \u0432\u0435\u043a\u0442\u043e\u0440\u043e\u0432-\u0444\u0438\u0447\u0435\u0439 \u0438\u0437\u043e\u0431\u0440\u0430\u0436\u0435\u043d\u0438\u044f \u0437\u0430\u0440\u0430\u043d\u0435\u0435 \u043f\u043e \u043a\u043e\u0442\u043e\u0440\u044b\u043c \u0431\u0443\u0434\u0435\u043c \u0441\u0440\u0430\u0432\u043d\u0438\u0432\u0430\u0442\u044c \u0438\u0441\u0445\u043e\u0434\u043d\u043e\u0435**","ddbb9f09":"**Remove Fone**","5b11905f":"# TEMPLATE MATCHING -- 1-st approach","176fa8fe":"**\u0422\u0443\u0442 \u0442\u043e\u0436\u0435 \u0441\u0432\u043e\u0438 \u043d\u0435\u0442\u043e\u0447\u043d\u043e\u0441\u0442\u0438 \u043e\u043d\u0438 \u0443\u0434\u0430\u043b\u044f\u044e\u0442\u0441\u044f \u0441 \u043f\u043e\u043c\u043e\u0449\u044c\u044e \u043e\u0431\u0443\u0447\u0435\u043d\u0438\u044f \u043d\u0430\u0434\u043e \u043f\u0440\u043e\u0433\u043d\u0430\u0442\u044c \u043f\u0430\u0440\u0443 \u0440\u0430\u0437 \u0434\u043b\u044f \u043b\u0443\u0447\u0448\u0435\u0433\u043e \u0440\u0430\u0437\u043b\u0438\u0447\u0438\u044f. \u0417\u0430\u0442\u043e \u0442\u0443\u0442 \u043c\u043e\u0436\u043d\u043e 4 \u0444\u043f\u0441 \u0434\u0430\u0442\u044c=) (\u0434\u0430\u043b\u044c\u0448\u0435 \u043c\u043e\u0436\u043d\u043e \u0435\u0449\u0435 \u0443\u0441\u043a\u043e\u0440\u0438\u0442\u044c \u0437\u0430\u043c\u0435\u043d\u043e\u0439 \u0444\u043b\u043e\u0442 \u043d\u0430 \u0438\u043d\u0442 \u0438 \u0442.\u0434)**","672eafea":"# \u041d\u0435\u0439\u0440\u043e\u0441\u0435\u0442\u0438","7dddd42f":"**\u0418\u0441\u0445\u043e\u0434\u043d\u043e\u0435 \u0438\u0437\u043e\u0431\u0440\u0430\u0436\u0435\u043d\u0438\u0435**","c6088749":"# Train (\u0421\u043e\u0437\u0434\u0430\u043d\u0438\u0435 \u043c\u043e\u0434\u0435\u043b\u0438)"}}