{"cell_type":{"6d8f4415":"code","ec824546":"code","d5180abc":"code","c293c096":"code","93b58be1":"code","6bb5b457":"code","14972ead":"code","dee93434":"code","14b4b52f":"code","0cd105c7":"code","0758a67e":"code","8d4c3902":"code","193e9651":"code","d8efe1c9":"code","0fbf50c0":"code","6e44ddd0":"code","3a134e31":"code","be1e541a":"code","c341ecca":"code","67a9f3b5":"code","35f4efb6":"code","33f9d007":"code","e8d3b1f9":"code","8e251c91":"code","054bffe3":"code","a9c11aa4":"code","d7b93a4a":"markdown","155b4d6e":"markdown","6c7c07f0":"markdown","b79ab490":"markdown","1e6ee06b":"markdown","7e276b9e":"markdown","91173bf7":"markdown","5d282949":"markdown","f7ae7f1e":"markdown","66a458b9":"markdown","fe996189":"markdown","54a60351":"markdown","1efba3cb":"markdown","da7ce6ba":"markdown"},"source":{"6d8f4415":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","ec824546":"import matplotlib.pyplot as plt\nimport seaborn as sns\nimport tensorflow as tf\nfrom tensorflow.keras.utils import plot_model\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping, ModelCheckpoint\nfrom sklearn.model_selection import KFold, train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics import confusion_matrix, accuracy_score\nle = LabelEncoder()","d5180abc":"data = pd.read_csv('\/kaggle\/input\/gender-classification\/Transformed Data Set - Sheet1.csv')\ndata.head()","c293c096":"# Get the data summary\ndata.describe()","93b58be1":"# List all the anomaly (same feature values but got different labels)\ngrouping = data.groupby(list(data.columns)[:-1]).apply(lambda x: x.Gender.nunique())\ngrouping[grouping.eq(2)]","6bb5b457":"data[(data['Favorite Color']=='Cool') &\n     (data['Favorite Music Genre'] == 'Rock') &\n     (data['Favorite Beverage']==\"Vodka\") &\n     (data['Favorite Soft Drink']=='Coca Cola\/Pepsi')]","14972ead":"# Split the features and labels\nX = data.iloc[:,:-1]\ny = data.iloc[:,-1]\n\n# Change the data into one-hot encoding (for features) and change label to 0-1\nX = pd.get_dummies(X)\ny = le.fit_transform(y)","dee93434":"X.head()","14b4b52f":"print(\"Shape of new data: \", X.shape)","0cd105c7":"def train_model(X_train, X_test, y_train, y_test):\n  model = tf.keras.models.Sequential([\n        tf.keras.Input(shape=(20), dtype='float32'),\n        tf.keras.layers.Dense(units=1024, activation='relu'),\n        tf.keras.layers.Dropout(0.4),\n        tf.keras.layers.Dense(units=1, activation='sigmoid')\n  ])\n\n  model.compile(optimizer=Adam(lr=0.0001),\n                loss='binary_crossentropy',\n                metrics=['accuracy'])\n  \n  # Callback to reduce learning rate if no improvement in validation loss for certain number of epochs\n  reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=10, min_lr=1e-8, verbose=0)\n  # Callback to stop training if no improvement in validation loss for certain number of epochs\n  early_stop = EarlyStopping(monitor='val_loss', patience=20, verbose=0)\n\n  history = model.fit(\n    X_train, y_train,\n    epochs=1000,\n    validation_data=(X_test, y_test),\n    callbacks=[reduce_lr, early_stop],\n    verbose=0\n  )\n\n  tr_loss, tr_acc = model.evaluate(X_train, y_train)\n  loss, accuracy = model.evaluate(X_test, y_test)\n\n  return model, history, tr_loss, tr_acc, loss, accuracy","0758a67e":"kfold = KFold(n_splits=5, random_state=42, shuffle=True)\n\nloss_arr = []\nacc_arr = []\ntrloss_arr = []\ntracc_arr = []\n\ntemp_acc = 0\n\nfor train, test in kfold.split(data):\n  model, history, trloss_val, tracc_val, loss_val, acc_val = train_model(X.iloc[train], X.iloc[test], y[train], y[test])\n  if acc_val > temp_acc:\n    print(\"Model changed\")\n    temp_acc = acc_val\n    model.save('best_model.h5')\n    train_index = train\n    test_index = test\n    best_history = history\n  trloss_arr.append(trloss_val)\n  tracc_arr.append(tracc_val)\n  loss_arr.append(loss_val)\n  acc_arr.append(acc_val)","8d4c3902":"# Compile the Train and Test Accuracy from KFold\npd.DataFrame({\n    'Train Accuracy': tracc_arr,\n    'Test Accuracy': acc_arr},\n    index=range(1,6))","193e9651":"print(\"Train Index (Best Split): \", train_index)\nprint(\"Test Index (Best Split): \", test_index)","d8efe1c9":"# Import model\nmodeltf = tf.keras.models.load_model('best_model.h5')","0fbf50c0":"# Plot the model architecture\nplot_model(model, show_shapes=True)","6e44ddd0":"modeltf.summary()","3a134e31":"# Plot the accuracy of the best model for each epoch of training\nplt.plot(best_history.history['accuracy'])\nplt.plot(best_history.history['val_accuracy'])\nplt.title('model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()\n\n# Plot the accuracy of the best model for each epoch of loss\nplt.plot(best_history.history['loss'])\nplt.plot(best_history.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()","be1e541a":"# Using best split from KFold\ntrain_loss, train_acc = modeltf.evaluate(X.iloc[train_index], y[train_index])\ntest_loss, test_acc = modeltf.evaluate(X.iloc[test_index], y[test_index])\n\nprint(\"\\n==============================\")\nprint(\"Train Accuracy: \", train_acc)\nprint(\"Train Loss: \", train_loss)\nprint(\"==============================\")\nprint(\"Test Accuracy: \", test_acc)\nprint(\"Test Loss: \", test_loss)","c341ecca":"y_pred = modeltf.predict(X.iloc[test_index])\ny_pred = (y_pred > 0.5)","67a9f3b5":"compare_res = pd.DataFrame({\n    'Y test': y[test_index],\n    'Y pred': y_pred.astype(int).reshape(len(y[test_index]))\n}, index=test_index)\ncompare_res","35f4efb6":"wrong_res_index = compare_res[compare_res['Y test'] != compare_res['Y pred']].index.values\n\nwrong_res = data.iloc[wrong_res_index,:]\ngender_pred = np.where(wrong_res['Gender']=='F', 'M', 'F')\n# wrong_res.loc[:,'Gender_Pred'] = gender_pred\nwrong_res = wrong_res.assign(Gender_Pred = gender_pred)\nwrong_res","33f9d007":"data[(data.index.isin(train_index)) &\n     (data['Favorite Color'] == 'Cool') &\n     (data['Favorite Music Genre'] == 'Rock') &\n     (data['Favorite Beverage'] == \"Vodka\") &\n     (data['Favorite Soft Drink'] == 'Coca Cola\/Pepsi')]","e8d3b1f9":"data[(data.index.isin(train_index)) &\n     (data['Favorite Color']=='Cool') &\n     (data['Favorite Music Genre'] == 'Pop') &\n     (data['Favorite Beverage'] == \"Whiskey\") &\n     (data['Favorite Soft Drink'] == 'Fanta')]","8e251c91":"cm = confusion_matrix(y[test_index], y_pred)\nsns.heatmap(cm.T, square=True, annot=True, fmt='d', cbar=False, cmap=\"YlGnBu\")\nplt.xlabel('Actual Label')\nplt.ylabel('Predicted Label')","054bffe3":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)","a9c11aa4":"# Using split scenario from train_test_split\ntrain_loss, train_acc = modeltf.evaluate(X_train, y_train)\ntest_loss, test_acc = modeltf.evaluate(X_test, y_test)\n\nprint(\"\\n==============================\")\nprint(\"Train Accuracy: \", train_acc)\nprint(\"Train Loss: \", train_loss)\nprint(\"==============================\")\nprint(\"Test Accuracy: \", test_acc)\nprint(\"Test Loss: \", test_loss)","d7b93a4a":"Let's take a look at the data that is wrongly classified.","155b4d6e":"If we see, there is 2 condition of feature values that have different labels. One of them is shown below.","6c7c07f0":"If we see the train data that have the same feature values with index 38 (the index that is wrongly classified), we see nothing.","b79ab490":"## Import the Best Data from K-Fold Iteration","1e6ee06b":"Since we have an anomaly in our data, then I prefer to use KFold and try to find best train-test split scenario that gives best accuracy in both train and test set.","7e276b9e":"## Train the Model using KFold (Split = 5)","91173bf7":"## **OMG! WE GOT 1.0 ACCURACY SCORE IN THE TEST SET XD**","5d282949":"In this case, nothing we can do now. If it is because of the mistake happened during the data entry (human error), then we can drop those values. However, we can't just do that since it will make bias in our model (because in reality, it is reasonable if both genders have same interests. In other cases, we can also add more features (ask more question in questionnaire).","f7ae7f1e":"From index 22, we can see above that it is wrongly classified (so the model predict in opposite of the actual value). If we see the data in our train (that have the same value with index 22), we can see that it is from our data anomaly (which have 2 different labels from the same feature values). Because M is trained more than F, then the model will predict the value in M (as shown above).","66a458b9":"It's for fun only. Let's try to evaluate the model performance using different train_test_split! :D","fe996189":"## Preprocess the Data","54a60351":"From the table above, we know that there is no missing value. However, since the data has only a few features and only 66 rows of data, I suspect that there is anomaly in the data. That is, there is maybe some rows that have same feature values but different labels.","1efba3cb":"Because the data is still in text and categorical, then we can change the data into numeric using one-hot encoding. Hence, it can be used as an input value to our Neural Network.","da7ce6ba":"We have saved our best model and also its train-test split index, so let's try to import them and use it in different scenario."}}