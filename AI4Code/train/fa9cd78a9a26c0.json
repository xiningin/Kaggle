{"cell_type":{"dba97bd4":"code","02c8f45c":"code","0a9c1905":"code","0a194b4d":"code","8519ba2b":"code","3622e7b9":"code","276b7c07":"code","d5372440":"code","7766517d":"code","5687a8b0":"code","3fd029de":"code","0f22864c":"code","f8275afc":"markdown","3cd0bfce":"markdown","de11597d":"markdown","791a9360":"markdown","7730a084":"markdown","34874f3d":"markdown","e5b69b55":"markdown","972e15d7":"markdown","f14584bd":"markdown","75fdd5d2":"markdown","37428737":"markdown","791008b3":"markdown","8002d9bd":"markdown","6b8e9e1f":"markdown","e41e92db":"markdown"},"source":{"dba97bd4":"#!pip install imutils\nimport keras\nimport tensorflow\nimport os\nfrom keras.layers import Input\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom keras.layers import Flatten, Dense, Conv2D, MaxPooling2D, Dropout\nfrom tensorflow.keras.layers import Dropout\nfrom tensorflow.keras.layers import Flatten\nfrom tensorflow.keras.layers import Dense\nfrom tensorflow.keras.layers import Input\nfrom keras.layers import BatchNormalization\nfrom tensorflow.keras import Sequential\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.preprocessing.image import img_to_array\nfrom tensorflow.keras.preprocessing.image import load_img\nfrom tensorflow.keras.utils import to_categorical\nfrom sklearn.preprocessing import LabelBinarizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report\nfrom imutils import paths\nimport matplotlib.pyplot as plt\nfrom keras.models import Model\nfrom sklearn.utils import shuffle\nfrom cv2 import imread\nimport numpy as np\nimport pandas as pd","02c8f45c":"data = []\nlabels = []\nwidth,height=150,150\n\nimagePaths = list(paths.list_images('..\/input\/diabetic-retinopathy-224x224-gaussian-filtered\/gaussian_filtered_images\/gaussian_filtered_images'))\ndata = []\nlabels = []\n\nfor imagePath in imagePaths:\n    label = imagePath.split(os.path.sep)[-2]\n    #print(imagePath)\n    image = load_img(imagePath, target_size=(width, height))\n    image = img_to_array(image)\n    data.append(image)\n    labels.append(label)\n\ndata = np.array(data, dtype=\"float32\")\nlabels = np.array(labels)\n\nlb = LabelBinarizer()\nlabels = lb.fit_transform(labels)\n#labels = to_categorical(labels)\n\ndata, labels = shuffle(data, labels)\n\nprint(data.shape)\nprint(labels.shape)","0a9c1905":"test_ratio = 0.25\n\n# train is now 75% of the entire data set\nx_train, x_test, y_train, y_test = train_test_split(data, labels, test_size=test_ratio)\n\nprint(\"Train images:\",x_train.shape)\nprint(\"Test images:\",x_test.shape)\nprint(\"Train label:\",y_train.shape)\nprint(\"Test label:\",y_test.shape)","0a194b4d":"from keras.models import Model\nfrom keras.layers import Input \nfrom keras.applications.vgg16 import VGG16\nfrom keras import layers\n\nINIT_LR = 1e-4\nEPOCHS = 25\nBS = 32\n\nvgg = VGG16(weights= \"imagenet\", include_top=False, input_shape= (150,150,3)) \nvgg.trainable = False\n\nmodel = Sequential()\nmodel.add(vgg)\nmodel.add(layers.Flatten())\nmodel.add(layers.Dense(100, activation = 'relu'))\nmodel.add(layers.Dense(5, activation = 'sigmoid'))\n\n\n\nopt = Adam(learning_rate=INIT_LR)\nmodel.compile(optimizer=opt, loss='binary_crossentropy', metrics=['accuracy'])\nmodel.summary()\n","8519ba2b":"# train the head of the network\nprint(\"[INFO] training head..\")\nh = model.fit(x_train,y_train,epochs=EPOCHS)\nprint(\"Done !!\")","3622e7b9":"#!pip install scikit-plot\nfrom sklearn.metrics import classification_report\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport scikitplot as skplt\nfrom sklearn.metrics import accuracy_score\nimport numpy as np\nfrom sklearn import metrics\n\nprint(\"[INFO] evaluating network...\")\npredIdxs = model.predict(x_test, batch_size=BS)\npredIdxs = np.argmax(predIdxs, axis=1)\n\ntrainpredIdxs = model.predict(x_train, batch_size=BS)\ntrainpredIdxs = np.argmax(trainpredIdxs, axis=1)\n\ntrainScore=accuracy_score(trainpredIdxs,y_train.argmax(axis=1))*100\nScore=accuracy_score(predIdxs,y_test.argmax(axis=1))*100\n\nprint(\"\\nTrainig Accuracy Score:-\",trainScore)\nprint(\"\\nTesting Accuracy Score:-\",Score)\nprint(\"\\nTraning Graph:- \\n \")\n\n# plot the training loss and accuracy\nN = EPOCHS\nplt.style.use(\"ggplot\")\nplt.figure()\nplt.plot(np.arange(0, N), h.history[\"loss\"], label=\"train_loss\")\nplt.plot(np.arange(0, N), h.history[\"accuracy\"], label=\"train_acc\")\nplt.title(\"Training Loss and Accuracy\")\nplt.xlabel(\"Epoch #\")\nplt.ylabel(\"Loss\/Accuracy\")\nplt.legend(loc=\"lower left\",)\nplt.show()","276b7c07":"extract = Model(model.inputs, model.layers[-3].output)\n\n#del(data)\n#del(labels)\nfeat_train  = extract.predict(x_train)  \nfeat_test = extract.predict(x_test)      \n\nprint(feat_train.shape)","d5372440":"from sklearn.svm import SVC\n\nsvm = SVC(kernel='linear')\nsvm.fit(feat_train,np.argmax(y_train,axis=1))\n\nTrainSVMScore=svm.score(feat_train,np.argmax(y_train,axis=1))*100\nprint(\"SVM Training Accuracy Score:-\",TrainSVMScore)\n\nTestSVMScore=svm.score(feat_test,np.argmax(y_test,axis=1))*100\nprint(\"\\nSVM Testing Accuracy Score:-\",TestSVMScore)","7766517d":"from sklearn.tree import DecisionTreeClassifier\n\nclf = DecisionTreeClassifier(random_state=0)\nclf = clf.fit(feat_train,np.argmax(y_train,axis=1))\n\nTrainDecisionScore=clf.score(feat_train,np.argmax(y_train,axis=1))*100\nprint(\"Decision Tree Training Accuracy Score:-\",TrainDecisionScore)\n\n\nTestDecisionScore=clf.score(feat_test,np.argmax(y_test,axis=1))*100\nprint(\"\\nDecision Tree Testing Accuracy Score:-\",TestDecisionScore)","5687a8b0":"from sklearn.neighbors import KNeighborsClassifier\n\nknn = KNeighborsClassifier(n_neighbors=5)\nknn.fit(feat_train,np.argmax(y_train,axis=1))\n\nTrainKNNScore=knn.score(feat_train,np.argmax(y_train,axis=1))*100\nprint(\"KNN Training Accuracy Score:-\",TrainKNNScore)\n\nTestKNNScore=knn.score(feat_test,np.argmax(y_test,axis=1))*100\nprint(\"\\nKNN Testing Accuracy Score:-\",TestKNNScore)","3fd029de":"from sklearn.naive_bayes import GaussianNB\n\ngnb = GaussianNB()\ngnb.fit(feat_train,np.argmax(y_train,axis=1))\n\nTrainNBScore=gnb.score(feat_train,np.argmax(y_train,axis=1))*100\nprint(\"\\nGaussianNaive Bayes Training Accuracy Score:-\",TrainNBScore)\n\nTestNBScore=gnb.score(feat_test,np.argmax(y_test,axis=1))*100\nprint(\"\\nGaussianNaive Bayes Testing Accuracy Score:-\",TestNBScore)","0f22864c":"print(\"--Training Accuracy..\")\nprint(\"VGG Accuracy:- {:.2f} %\".format(trainScore))\nprint(\"VGG-SVM Accuracy:- {:.2f} %\".format(TrainSVMScore))\nprint(\"VGG-DT Accuracy:- {:.2f} %\".format(TrainDecisionScore))\nprint(\"VGG-KNN Accuracy:- {:.2f} %\".format(TrainKNNScore))\nprint(\"VGG-NB Accuracy:- {:.2f} %\".format(TrainNBScore))\n\nprint(\"\\n--Testing Accuracy..\")\nprint(\"VGG Accuracy:- {:.2f} %\".format(Score))\nprint(\"VGG-SVM Accuracy:- {:.2f} %\".format(TestSVMScore))\nprint(\"VGG-DT Accuracy:- {:.2f} %\".format(TestDecisionScore))\nprint(\"VGG-KNN Accuracy:- {:.2f} %\".format(TestKNNScore))\nprint(\"VGG-NB Accuracy:- {:.2f} %\".format(TestNBScore))","f8275afc":"**Split the data for training and testing**","3cd0bfce":"# <p style=\"background-color:#000;font-family:newtimeroman;color:#fff;font-size:120%;text-align:center;border-radius:20px 80px;\">Training VGG<\/p>","de11597d":"# <p style=\"background-color:#000;font-family:newtimeroman;color:#fff;font-size:120%;text-align:center;border-radius:20px 80px;\">Extracting Features<\/p>","791a9360":"<a id=\"1\"><\/a>\n# <p style=\"background-color:#000000;font-family:newtimeroman;color:#fff;font-size:120%;text-align:center;border-radius:20px 80px;\">Import libraries <\/p>","7730a084":"**Decision Tree**","34874f3d":"# <p style=\"background-color:#000;font-family:newtimeroman;color:#fff;font-size:120%;text-align:center;border-radius:20px 80px;\">Summary<\/p>","e5b69b55":"<a id=\"1\"><\/a>\n# <p style=\"background-color:#000;font-family:newtimeroman;color:#fff;font-size:120%;text-align:center;border-radius:20px 80px;\">VGG16-ML Model<\/p>\n![CNNSVM architecture (1).jpg](attachment:7b37f6aa-88b3-4527-8b7a-34a3a8271441.jpg)\n\n\n> *   VGG16 layers extract the features and Pass 3D features to Flatten Layer.\n> *   Flatten Layer Convert 3D features to 1D features.\n> *   Pass those 1D features to Machine Learning Classifier.","972e15d7":"# <p style=\"text-align:center\"> If you find this notebook helpful, please do upvote :) <\/p>\n# <p style=\"text-align:center\"> ![giphy.gif](attachment:62a64e42-59b3-42ce-a9bd-7496ed743330.gif) <\/p>\n\n\n","f14584bd":"# <p style=\"background-color:#000;font-family:newtimeroman;color:#fff;font-size:120%;text-align:center;border-radius:20px 80px;\">Training Machine Learning Classifier<\/p>","75fdd5d2":"**Gaussian Naive Bayes**","37428737":"**K-Nearest Neighbor(KNN)**","791008b3":"<a id=\"2\"><\/a>\n# <p style=\"background-color:#000;font-family:newtimeroman;color:#fff;font-size:120%;text-align:center;border-radius:20px 80px;\">Load Data<\/p>\n","8002d9bd":"> Aim of this work is to demonstrate how we can combine VGG and ML using detiatic retinopathy dataset.\n\n> We are going to extract the features using VGG and pass those features to some ML classifier.\n\n> Each model will be evaluate based on Testing Accuracy.","6b8e9e1f":"<a id=\"1\"><\/a>\n# <p style=\"background-color:#000;font-family:newtimeroman;color:#fff;font-size:120%;text-align:center;border-radius:20px 80px;\">Introduction<\/p>","e41e92db":"**Support Vector Machine (SVM)**"}}