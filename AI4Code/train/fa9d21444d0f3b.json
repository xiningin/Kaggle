{"cell_type":{"b07c4593":"code","f55ecaab":"code","070ed1fc":"code","56ecfa10":"code","37de710c":"code","3f6219e2":"code","ca93f349":"code","a2edb8c8":"code","e28e794b":"code","e2a21083":"code","4aa74169":"code","165f8691":"code","8cdf21f0":"code","f50d2055":"code","e7d4a470":"code","081b1582":"code","540d7ddc":"code","13444971":"code","5672f40c":"code","4520241a":"code","239f26f2":"code","ec967fd5":"code","8d41c8e8":"code","84979a9f":"code","e5219371":"code","49e40355":"code","0cd9caa7":"code","efa11d2c":"code","dac3cddf":"code","3444fa8e":"code","e2672a55":"code","6d6c1d42":"code","77b48c72":"code","1c728e82":"code","a5ee265b":"code","ab7d73c7":"code","64d5da65":"code","977baceb":"code","55d2b1b6":"markdown","3d3eb94f":"markdown","a799fef4":"markdown","9d224933":"markdown","7ddb3b89":"markdown","8850627a":"markdown","ddfffb35":"markdown","6acbf593":"markdown","9b3e1a26":"markdown","a9cbfb52":"markdown"},"source":{"b07c4593":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","f55ecaab":"import matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\nimport warnings\nwarnings.filterwarnings(\"ignore\")","070ed1fc":"# Load Data\ndf = pd.read_csv('..\/input\/bitcointweets.csv', header = None)\npd.set_option('display.max_colwidth', -1)\ndf.head()","56ecfa10":"df = df[[1,7]]\ndf.columns = ['tweet','label']\ndf.head()","37de710c":"df.tail()","3f6219e2":"df.info()","ca93f349":"df.shape","a2edb8c8":"df.describe()","e28e794b":"sns.countplot(df[\"label\"])","e2a21083":"df[\"text_length\"] = df[\"tweet\"].apply(len)","4aa74169":"df[[\"label\",\"text_length\",\"tweet\"]].head()","165f8691":"df[\"text_length\"].describe()","8cdf21f0":"df[\"text_length\"].hist(bins=20)","f50d2055":"g = sns.FacetGrid(df,col = \"label\")\ng.map(plt.hist,\"text_length\")","e7d4a470":"sns.boxenplot(x=\"label\",y=\"text_length\",data=df, palette=\"rainbow\")","081b1582":"#word cloud\nfrom nltk.corpus import stopwords\nfrom wordcloud import WordCloud","540d7ddc":"import re\n\ndef clean_text(s):\n    s = re.sub(r'http\\S+', '', s)\n    s = re.sub('(RT|via)((?:\\\\b\\\\W*@\\\\w+)+)', ' ', s)\n    s = re.sub(r'@\\S+', '', s)\n    s = re.sub('&amp', ' ', s)\n    return s\n\ndf['clean_tweet'] = df['tweet'].apply(clean_text)","13444971":"df.head()","5672f40c":"text = df[\"clean_tweet\"].to_string()\nwordcloud = WordCloud(relative_scaling=0.5 , background_color='white',stopwords=set(stopwords.words('english'))).generate(text)\nplt.figure(figsize=(12,12))\nplt.imshow(wordcloud)\nplt.axis(\"off\")\nplt.show()","4520241a":"X = df[\"clean_tweet\"]\ny = pd.get_dummies(df[\"label\"].values)\ny.head()","239f26f2":"y = np.array(y)\nnum_classes = df[\"label\"].nunique()\nprint(y)\nprint(np.argmax(y[0:5],axis=1))\n","ec967fd5":"num_classes","8d41c8e8":"# setting seed to have identical result in future run for comparisons\nseed = 42\nnp.random.seed(seed)\n","84979a9f":"from sklearn.model_selection import train_test_split\nX_train , X_test , y_train , y_test = train_test_split(X , y , test_size = 0.3 , random_state = seed)\nprint(X_train.shape)\nprint(X_test.shape)\nprint(y_train.shape)\nprint(y_test.shape)","e5219371":"from keras.preprocessing.text import Tokenizer\nmax_features = 20000\ntokenizer = Tokenizer(num_words=max_features)\ntokenizer.fit_on_texts(list(X_train))\nX_train = tokenizer.texts_to_sequences(X_train)\nX_test = tokenizer.texts_to_sequences(X_test)","49e40355":"totalNumWords = [len(one_cleaned_tweet) for one_cleaned_tweet in X_train]\nplt.hist(totalNumWords,bins=30)\nplt.show()","0cd9caa7":"X_train[7]","efa11d2c":"max(totalNumWords)","dac3cddf":"from keras.preprocessing import sequence\nmax_words = max(totalNumWords)\nX_train = sequence.pad_sequences(X_train , maxlen = max_words)\nX_test = sequence.pad_sequences(X_test , maxlen = max_words)\nprint(X_train.shape,X_test.shape)","3444fa8e":"X_train[7]","e2672a55":"import keras.backend as K\nfrom keras.models import Sequential\nfrom keras.layers import Dense,Embedding,LSTM\nfrom sklearn.metrics import accuracy_score,confusion_matrix,classification_report","6d6c1d42":"batch_size = 128\nepochs = 2","77b48c72":"def get_model(max_features , embed_dim):\n    np.random.seed(seed)\n    K.clear_session()\n    model = Sequential()\n    model.add(Embedding(max_features , embed_dim , input_length=X_train.shape[1]))\n    model.add(LSTM(100 , dropout=0.2 , recurrent_dropout=0.2))\n    model.add(Dense(num_classes , activation='softmax'))\n    model.compile(loss = 'categorical_crossentropy' , optimizer = 'adam' , metrics = ['accuracy'])\n    print(model.summary())\n    return model\n    ","1c728e82":"def model_train(model):\n    #training the model\n    model_history = model.fit(X_train , y_train , validation_data = (X_test , y_test), \n                              epochs = epochs ,batch_size= batch_size,verbose = 2)\n    #plotting train history\n    plot_model_history(model_history)\n","a5ee265b":"def plot_model_history(model_history):\n    fig , axs = plt.subplots( 1 , 2 , figsize=(15,5))\n    \n    #Summarize history for accuracy\n    \n    axs[0].plot(range(1,len(model_history.history['acc'])+1),model_history.history['acc'])\n    axs[0].plot(range(1,len(model_history.history['val_acc'])+1),model_history.history['val_acc'])\n    \n    axs[0].set_title(\"Model Accuracy\")\n    axs[0].set_ylabel(\"Accuracy\")\n    axs[0].set_xlabel(\"Epoch\")\n    \n    axs[0].set_xticks(np.arange(1,len(model_history.history['acc'])+1),len(model_history.history['acc'])\/10)\n    \n    axs[0].legend(['train', 'val'], loc='best')\n    \n    #Summarize history for loss\n    \n    axs[1].plot(range(1,len(model_history.history['loss'])+1),model_history.history['loss'])\n    axs[1].plot(range(1,len(model_history.history['val_loss'])+1),model_history.history['val_loss'])\n    \n    axs[1].set_title(\"Model Loss\")\n    axs[1].set_ylabel(\"Loss\")\n    axs[1].set_xlabel(\"Epoch\")\n    \n    axs[1].set_xticks(np.arange(1,len(model_history.history['loss'])+1),len(model_history.history['loss'])\/10)\n    \n    axs[1].legend(['train', 'val'], loc='best')\n    \n    plt.show()\n    ","ab7d73c7":"def model_evaluate():\n    #predict classes with test set\n    y_pred_test = model.predict_classes(X_test , batch_size = batch_size, verbose =0)\n    print(\"Predicted \", y_pred_test[:50])\n    print(\"True \" , np.argmax(y_test[:50],axis = 1))\n    print('Accuracy:\\t{:0.1f}%'.format(accuracy_score(np.argmax(y_test,axis = 1),y_pred_test)*100))\n    \n    #Classification Report\n    print(\"\\n\")\n    print(classification_report(np.argmax(y_test, axis =1),y_pred_test))\n    \n    #Confusion Matrix\n    confmat = confusion_matrix(np.argmax(y_test , axis = 1), y_pred_test)\n    fig , ax = plt.subplots(figsize=(4,4))\n    ax.matshow(confmat , cmap =plt.cm.Blues , alpha = 0.3)\n    \n    for i in range(confmat.shape[0]):\n        for j in range(confmat.shape[1]):\n            ax.text( x = j , y = i , s =confmat[i,j] , va = 'center' , ha = 'center')\n    \n    plt.xlabel(\"Predicted Label\")\n    plt.ylabel(\"True Label\")\n    plt.tight_layout()","64d5da65":"max_features = 20000\nembed_dim =100\nmodel = get_model(max_features,embed_dim)\nmodel_train(model)","977baceb":"model_evaluate()","55d2b1b6":"# LSTM","3d3eb94f":"# Train Test Split\n* test_size is how much do you subset the training data into a validation set","a799fef4":"# Exploratory Data Analysis","9d224933":"# Encode Categorical Variable","7ddb3b89":"# Importing Required Libraries","8850627a":"# Evaluate Model With Test Set\u00b6","ddfffb35":"# Tokenize Text","6acbf593":"# Loading Data","9b3e1a26":"### Importing Libraries","a9cbfb52":"# Train the model"}}