{"cell_type":{"08ad5c70":"code","e1a063d0":"code","70c6cd67":"code","1dc20c01":"code","541dc10b":"code","2fa9e5dc":"code","bfeb0745":"code","2020c03d":"code","38f12b56":"code","b47640ae":"code","810d9ad2":"code","e9a721ce":"code","08b14c5a":"code","30ff1da0":"code","4a682de7":"code","c9dae4e7":"code","b0a222b8":"code","9a3d74e7":"code","e2fe3ec2":"code","544670ba":"code","50af0e31":"code","ad69e261":"code","5d55b546":"code","d2ce79aa":"code","b31e40ee":"code","670496f2":"code","8fc232fb":"code","574069d8":"code","feb42d4c":"code","495ec030":"code","1c02be6d":"code","71c0742a":"code","f4709e27":"code","9d856d2e":"code","1389dd8a":"code","477c837e":"code","9f4e80b3":"code","e0c82bd6":"code","22534ab3":"code","c24ec17e":"code","898fb1d7":"code","2e1e7c19":"code","84ec8b56":"code","9e834380":"code","dd99fdd6":"code","704cb0f6":"code","a5b8ba53":"code","b0baab5c":"code","6b7666ff":"code","6e71096a":"code","113f07ab":"code","6c524989":"code","1b9b47e8":"code","2234708f":"code","2ba9c061":"code","78dc15e7":"code","13ef4344":"code","b53e6774":"code","fc2b41e8":"code","e1480267":"code","dbbb717e":"code","fd2a269b":"code","05cf1537":"code","dc8803d7":"code","6e4d872c":"code","ac88c76f":"code","f8c72c58":"code","cda7ebfc":"code","e6aee5b0":"code","2046be7c":"code","32145763":"code","e9d96863":"code","ad94ffb6":"code","068feb0d":"code","78a76041":"code","d597d220":"code","cd7dbf18":"code","8935d98a":"code","43167ee1":"code","0c79b28d":"code","c2821e5c":"code","df52d33d":"code","940b4ad6":"code","320285e1":"code","b8e51a99":"code","3f3f244f":"code","2f6a7858":"code","45e9bd26":"code","3dfc3341":"code","936c4730":"code","11f69e98":"code","550eedd7":"code","7c785bc1":"code","a268a6a3":"code","9a5ace67":"code","5803bad7":"code","16af111b":"code","4ecd3300":"code","8aeccdb0":"code","92218567":"code","a1de97c4":"code","15619d2f":"code","bcb6d623":"code","1d6a12c1":"markdown","8c826fbf":"markdown","8a166399":"markdown","8c03d867":"markdown","be521e74":"markdown","f2d4405b":"markdown","04f79701":"markdown","3150bd6c":"markdown","9cb5cac5":"markdown","969773a4":"markdown","683dd882":"markdown","b4f5a974":"markdown","7a4b0849":"markdown","f2238c35":"markdown","026247be":"markdown","051afb4e":"markdown","677948db":"markdown","6e34809b":"markdown","3eb67f32":"markdown","454fdd97":"markdown","a906f3c8":"markdown","187b3ceb":"markdown","a97adb3b":"markdown","49c0c650":"markdown","a3cbe622":"markdown","b54f4c3a":"markdown","fadf1778":"markdown","34d4e844":"markdown","84e6d31c":"markdown","0071fcf3":"markdown","da77bd16":"markdown","d91a1603":"markdown","f066752a":"markdown","0e906d6f":"markdown","2ba432dc":"markdown","6ce2e514":"markdown","a50d8b86":"markdown","2bfa95ed":"markdown","9d73e708":"markdown","d233927a":"markdown","5d878edb":"markdown","92c60ca6":"markdown","6c8a6af1":"markdown","b5658bcf":"markdown","8a30ca24":"markdown","80af97c4":"markdown","c58ff8de":"markdown","78bb4c3d":"markdown","eb516912":"markdown","fad29436":"markdown","6e7c7e03":"markdown","4d7b4c90":"markdown","70318063":"markdown","35e0c455":"markdown","c1ceaeed":"markdown","4283d028":"markdown","861efadc":"markdown","1e33e39b":"markdown","826cbfeb":"markdown","1d91d834":"markdown","b188d81d":"markdown","23a2b854":"markdown","7c71b2d8":"markdown","ddf18442":"markdown","3534d45b":"markdown","3b7f1de9":"markdown","84b24b0e":"markdown","32e9f44d":"markdown","9b2fd1ee":"markdown","207682b5":"markdown","b9b0b686":"markdown","99b2f90e":"markdown"},"source":{"08ad5c70":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nfrom scipy.stats import zscore\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.model_selection import KFold\nfrom sklearn.linear_model import Ridge\nfrom sklearn.linear_model import Lasso\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.preprocessing import StandardScaler \nfrom sklearn.decomposition import PCA\nfrom sklearn.svm import SVR\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.pipeline import Pipeline\nfrom sklearn import metrics\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.model_selection import train_test_split","e1a063d0":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","70c6cd67":"data=pd.read_csv(\"\/kaggle\/input\/concrete-compressive-strength\/concrete.csv\")","1dc20c01":"data.head(10)","541dc10b":"data.dtypes","2fa9e5dc":"data.info()","bfeb0745":"data.shape","2020c03d":"na_values=data.isna().sum()\nprint(na_values)","38f12b56":"null_values=data.isnull().sum()\nprint(null_values)","b47640ae":"data.describe().T","810d9ad2":"#Distribution of continous data\n\nplt.figure(figsize=(30,6))\n\n#Subplot 1\nplt.subplot(1,3,1)\nplt.title('cement')\nsns.distplot(data['cement'],color='red')\n\n#Subplot 2\nplt.subplot(1,3,2)\nplt.title('slag')\nsns.distplot(data['slag'],color='blue')\n\n#Subplot 3\nplt.subplot(1,3,3)\nplt.title('ash')\nsns.distplot(data['ash'],color='green')\n\n\n\nplt.figure(figsize=(30,6))\n\n#Subplot 1- Boxplot\nplt.subplot(1,3,1)\nplt.title('cement')\nsns.boxplot(data['cement'],orient='horizondal',color='red')\n\n#Subplot 2\nplt.subplot(1,3,2)\nplt.title('slag')\nsns.boxplot(data['slag'],orient='horizondal',color='blue')\n\n#Subplot 3\nplt.subplot(1,3,3)\nplt.title('ash')\nsns.boxplot(data['ash'],orient='horizondal',color='green')\n","e9a721ce":"#Distribution of continous data\n\nplt.figure(figsize=(30,6))\n\n#Subplot 1\nplt.subplot(1,3,1)\nplt.title('water')\nsns.distplot(data['water'],color='orange')\n\n#Subplot 2\nplt.subplot(1,3,2)\nplt.title('superplastic')\nsns.distplot(data['superplastic'],color='violet')\n\n#Subplot 3\nplt.subplot(1,3,3)\nplt.title('coarseagg')\nsns.distplot(data['coarseagg'],color='cyan')\n\n\n\nplt.figure(figsize=(30,6))\n\n#Subplot 1- Boxplot\nplt.subplot(1,3,1)\nplt.title('water')\nsns.boxplot(data['water'],orient='horizondal',color='orange')\n\n#Subplot 2\nplt.subplot(1,3,2)\nplt.title('superplastic')\nsns.boxplot(data['superplastic'],orient='horizondal',color='violet')\n\n#Subplot 3\nplt.subplot(1,3,3)\nplt.title('coarseagg')\nsns.boxplot(data['coarseagg'],orient='horizondal',color='cyan')\n","08b14c5a":"#Distribution of continous data\n\nplt.figure(figsize=(30,6))\n\n#Subplot 1\nplt.subplot(1,3,1)\nplt.title('fineagg')\nsns.distplot(data['fineagg'],color='grey')\n\n#Subplot 2\nplt.subplot(1,3,2)\nplt.title('age')\nsns.distplot(data['age'],color='purple')\n\n#Subplot 3\nplt.subplot(1,3,3)\nplt.title('strength')\nsns.distplot(data['strength'],color='aqua')\n\n\n\nplt.figure(figsize=(30,6))\n\n#Subplot 1- Boxplot\nplt.subplot(1,3,1)\nplt.title('fineagg')\nsns.boxplot(data['fineagg'],orient='horizondal',color='grey')\n\n#Subplot 2\nplt.subplot(1,3,2)\nplt.title('age')\nsns.boxplot(data['age'],orient='horizondal',color='purple')\n\n#Subplot 3\nplt.subplot(1,3,3)\nplt.title('strength')\nsns.boxplot(data['strength'],orient='horizondal',color='aqua')\n","30ff1da0":"sns.pairplot(data,palette=\"Set2\", diag_kind=\"kde\", height=2.5)","4a682de7":"correlation=data.corr()\ncorrelation.style.background_gradient(cmap='coolwarm')","c9dae4e7":"data.corr()>0.5","b0a222b8":"data.corr()<-0.5","9a3d74e7":"df=data.drop(['strength'],axis=1)\ndef get_redundant_pairs(df):\n    '''Get diagonal and lower triangular pairs of correlation matrix'''\n    pairs_to_drop = set()\n    cols = df.columns\n    for i in range(0, df.shape[1]):\n        for j in range(0, i+1):\n            pairs_to_drop.add((cols[i], cols[j]))\n    return pairs_to_drop\n\ndef get_top_abs_correlations(df, n=20):\n    au_corr = df.corr().abs().unstack()\n    labels_to_drop = get_redundant_pairs(df)\n    au_corr = au_corr.drop(labels=labels_to_drop).sort_values(ascending=False)\n    return au_corr[0:n]\n\nprint(\"Top Absolute Correlations\")\nprint(get_top_abs_correlations(df, 20))","e2fe3ec2":"data_scaled=data","544670ba":"data_scaled=data_scaled.apply(zscore)","50af0e31":"data_scaled.head(10)","ad69e261":"data_scaled.info()","5d55b546":"floats = data_scaled.columns[data_scaled.dtypes == 'float64']\nfor x in floats:\n    indexNames_larger = data_scaled[data_scaled[x]>3].index\n    indexNames_lesser = data_scaled[data_scaled[x]<-3].index\n    # Delete these row indexes from dataFrame\n    data_scaled.drop(indexNames_larger , inplace=True)\n    data_scaled.drop(indexNames_lesser , inplace=True)\n    data.drop(indexNames_larger , inplace=True)\n    data.drop(indexNames_lesser , inplace=True)\n\ndata.head()","d2ce79aa":"data.info()","b31e40ee":"data.shape","670496f2":"data_features=data.copy()","8fc232fb":"data_features.insert(data_features.shape[-1]-1,'water-cement-ratio', data_features['water']\/data_features['cement'])","574069d8":"data_features.head()","feb42d4c":"data_features.shape","495ec030":"data_features.insert(data_features.shape[-1]-1,'water-ratio_with_cement_slag_ash',data_features['water']\/(data_features['cement'] + data_features['ash'] + data_features['slag']))\ndata_features.drop([ 'ash', 'slag','water','cement'], axis=1, inplace=True)","1c02be6d":"data_features.head()","71c0742a":"data_features.shape","f4709e27":"data_features=data_features.apply(zscore)\ndata_features.head()","9d856d2e":"X=data_features.drop(['strength'],axis=1)","1389dd8a":"Y=data_features['strength']","477c837e":"Xtrain_val,X_test,ytrain_val,Y_test=train_test_split(X,Y,test_size=0.2,random_state=22)","9f4e80b3":"pca_model= PCA(n_components=6)\npca_model.fit(X)\nplt.step(list(range(1,7)),np.cumsum(pca_model.explained_variance_ratio_), where='mid')\nplt.ylabel('Cum of variation explained')\nplt.xlabel('eigen Value')\nplt.show()","e0c82bd6":"    pca_df= PCA(n_components=5)\n    # Fitting the data\n    pca_df.fit(X)\n    # Transforming the data\n    pca_df=pca_df.transform(X)\n    PCA_data=pd.DataFrame(pca_df)\n    #Creatinf PCA_dataframe\n    PCA_data.head()","22534ab3":"kf = KFold(n_splits=10,random_state=2,shuffle=True)\nkf.get_n_splits(Xtrain_val)\nprint(kf)\n\n\nfor train_index, val_index in kf.split(Xtrain_val):\n    print(\"TRAIN:\", train_index, \"VALIDATION:\", val_index)\n    X_train, X_val = Xtrain_val.iloc[train_index], Xtrain_val.iloc[val_index]\n    y_train, y_val = ytrain_val.iloc[train_index], ytrain_val.iloc[val_index]","c24ec17e":"#Pipeline\npipe_svr = Pipeline([('scl', StandardScaler()), ('svr', SVR())]) \n\n#Parameter-grid\nparam_grid = {'svr__C': [0.01, 0.1, 1], 'svr__gamma': [0.1, 1],'svr__kernel':['rbf','poly']} \n\ngrid_svr = GridSearchCV( pipe_svr , param_grid = param_grid, cv = 5) \n\n#Fitting the data in the model\ngrid_svr.fit( X_train, y_train) \n\nprint(\" Best cross-validation score obtained is: {:.2f}\". format( grid_svr.best_score_)) \nprint(\" Best parameters as part of Gridsearch is: \", grid_svr.best_params_) \nprint(\" Train set score obtained is: {:.2f}\". format( grid_svr.score( X_train, y_train)))\nprint(\" Validation set score obtained is: {:.2f}\". format( grid_svr.score( X_val, y_val)))\nprint(\" Test set score obtained is: {:.2f}\". format( grid_svr.score( X_test, Y_test)))","898fb1d7":"y_pred=grid_svr.predict(X_test)","2e1e7c19":"SVR_MAE=metrics.mean_absolute_error(Y_test,y_pred)\nSVR_MSE=metrics.mean_squared_error(Y_test,y_pred)\nSVR_R2=metrics.r2_score(Y_test,y_pred)\nprint(\"The mean_absolute_error is:{:.2f}\".format(SVR_MAE))\nprint(\"The mean_squared_error is:{:.2f}\".format(SVR_MSE))\nprint(\"The R2 score is:{:.2f}\".format(SVR_R2))","84ec8b56":"scores = cross_val_score(grid_svr, X, Y, cv=5)\nscores","9e834380":"model= SVR(kernel='linear',C=1,gamma=0.1)","dd99fdd6":"model.fit(X_train,y_train)","704cb0f6":"model.coef_","a5b8ba53":"coef = pd.DataFrame(model.coef_.ravel())\ncoef[\"feat\"] = X_train.columns\nplt.figure(figsize=(20,10))\nax1 = sns.barplot(coef[\"feat\"],coef[0],palette=\"vlag\",linewidth=2,edgecolor=\"k\"*coef[\"feat\"].nunique())\nax1.set_facecolor(\"white\")\nax1.axhline(0,color=\"k\",linewidth=1)     \nplt.ylabel(\"coefficients\")\nplt.xlabel(\"features\")\nplt.title('FEATURE IMPORTANCES')","b0baab5c":"log_cols = [\"Classifier\", \"Mean Absolute Error\",\"Mean Squared Error\",\"R2 Score\"]\nlog = pd.DataFrame(columns=log_cols)","6b7666ff":"log_entry = pd.DataFrame([[\"Support Vector Regressor\",SVR_MAE,SVR_MSE,SVR_R2]], columns=log_cols)\nlog = log.append(log_entry)\nlog","6e71096a":"poly = PolynomialFeatures(degree=2, interaction_only=True)","113f07ab":"X_train2 = poly.fit_transform(X_train)\nX_val2 = poly.fit_transform(X_val)\nX_test2 = poly.fit_transform(X_test)","6c524989":"#Pipeline\npipe_ridge = Pipeline([('scl', StandardScaler()), ('Ridge', Ridge())]) \n\n#Parameter-grid\nparam_grid = {'Ridge__alpha': [0.1, 1,0.01]} \n\ngrid_ridge = GridSearchCV( pipe_ridge , param_grid = param_grid, cv = 5) \n\n#Fitting the data in the model\ngrid_ridge.fit( X_train2, y_train) \n\nprint(\" Best cross-validation score obtained is: {:.2f}\". format( grid_ridge.best_score_)) \nprint(\" Best parameters as part of Gridsearch is: \", grid_ridge.best_params_) \nprint(\" Train set score obtained is: {:.2f}\". format( grid_ridge.score( X_train2, y_train)))\nprint(\" Validation set score obtained is: {:.2f}\". format( grid_ridge.score( X_val2, y_val)))\nprint(\" Test set score obtained is: {:.2f}\". format( grid_ridge.score( X_test2, Y_test)))","1b9b47e8":"y_pred=grid_ridge.predict(X_test2)","2234708f":"Ridge_MAE=metrics.mean_absolute_error(Y_test,y_pred)\nRidge_MSE=metrics.mean_squared_error(Y_test,y_pred)\nRidge_R2=metrics.r2_score(Y_test,y_pred)\nprint(\"The mean_absolute_error is:{:.2f}\".format(Ridge_MAE))\nprint(\"The mean_squared_error is:{:.2f}\".format(Ridge_MSE))\nprint(\"The R2 score is:{:.2f}\".format(Ridge_R2))","2ba9c061":"scores = cross_val_score(grid_ridge, X, Y, cv=5)\nscores","78dc15e7":"log_entry = pd.DataFrame([[\"Ridge Regressor\",Ridge_MAE,Ridge_MSE,Ridge_R2]], columns=log_cols)\nlog = log.append(log_entry)\nlog","13ef4344":"#Pipeline\npipe_lasso = Pipeline([('scl', MinMaxScaler()), ('Lasso', Lasso())]) \n\n#Parameter-grid\nparam_grid = {'Lasso__alpha': [0.1, 1,0.01]} \n\n#Using RandomSearchCV\nRandom_lasso = RandomizedSearchCV( pipe_lasso , param_distributions=param_grid, cv = 5, n_iter=3) \n\n#Fitting the data in the model\nRandom_lasso.fit( X_train2, y_train) \n\nprint(\" Best cross-validation score obtained is: {:.2f}\". format( Random_lasso.best_score_)) \nprint(\" Best parameters as part of Gridsearch is: \", Random_lasso.best_params_) \nprint(\" Train set score obtained is: {:.2f}\". format( Random_lasso.score( X_train2, y_train)))\nprint(\" Validation set score obtained is: {:.2f}\". format( Random_lasso.score( X_val2, y_val)))\nprint(\" Test set score obtained is: {:.2f}\". format( Random_lasso.score( X_test2, Y_test)))","b53e6774":"y_pred=Random_lasso.predict(X_test2)","fc2b41e8":"Lasso_MAE=metrics.mean_absolute_error(Y_test,y_pred)\nLasso_MSE=metrics.mean_squared_error(Y_test,y_pred)\nLasso_R2=metrics.r2_score(Y_test,y_pred)\nprint(\"The mean_absolute_error is:{:.2f}\".format(Lasso_MAE))\nprint(\"The mean_squared_error is:{:.2f}\".format(Lasso_MSE))\nprint(\"The R2 score is:{:.2f}\".format(Lasso_R2))","e1480267":"scores = cross_val_score(Random_lasso, X, Y, cv=5)\nscores","dbbb717e":"log_entry = pd.DataFrame([[\"Lasso Regressor\",Lasso_MAE,Lasso_MSE,Lasso_R2]], columns=log_cols)\nlog = log.append(log_entry)\nlog","fd2a269b":"#Pipeline with pruning the data for regularization to avoid overfit\npipe_DTR = Pipeline([('scl', MinMaxScaler()), ('DTR', DecisionTreeRegressor(max_depth=4))]) \n\n#Parameter-grid\nparam_grid = {'DTR__criterion': [\"mse\", \"mae\"],'DTR__min_samples_split':[2,4,6],'DTR__min_samples_leaf':[1,2,3]} \n\n#Using RandomSearchCV\nRandom_DTR = RandomizedSearchCV( pipe_DTR , param_distributions=param_grid, cv = 5, n_iter=5) \n\n#Fitting the data in the model\nRandom_DTR.fit( X_train, y_train) \n\nprint(\" Best cross-validation score obtained is: {:.2f}\". format( Random_DTR.best_score_)) \nprint(\" Best parameters as part of Gridsearch is: \", Random_DTR.best_params_) \nprint(\" Train set score obtained is: {:.2f}\". format( Random_DTR.score( X_train, y_train)))\nprint(\" Validation set score obtained is: {:.2f}\". format( Random_DTR.score( X_val, y_val)))\nprint(\" Test set score obtained is: {:.2f}\". format( Random_DTR.score( X_test, Y_test)))","05cf1537":"y_pred=Random_DTR.predict(X_test)","dc8803d7":"DTR_MAE=metrics.mean_absolute_error(Y_test,y_pred)\nDTR_MSE=metrics.mean_squared_error(Y_test,y_pred)\nDTR_R2=metrics.r2_score(Y_test,y_pred)\nprint(\"The mean_absolute_error is:{:.2f}\".format(DTR_MAE))\nprint(\"The mean_squared_error is:{:.2f}\".format(DTR_MSE))\nprint(\"The R2 score is:{:.2f}\".format(DTR_R2))","6e4d872c":"scores = cross_val_score(Random_DTR, X, Y, cv=5)\nscores","ac88c76f":"model=DecisionTreeRegressor(min_samples_split= 6, min_samples_leaf=3, criterion= 'mse', max_depth=4)","f8c72c58":"model.fit(X_train,y_train)","cda7ebfc":"model.feature_importances_","e6aee5b0":"## Calculating feature importance\nfeat_imp_dict = dict(zip(X.columns, model.feature_importances_))\nfeat_imp = pd.DataFrame.from_dict(feat_imp_dict, orient='index')\nfeat_imp.sort_values(by=0, ascending=False)","2046be7c":"log_entry = pd.DataFrame([[\"Decision Tree Regressor\",DTR_MAE,DTR_MSE,DTR_R2]], columns=log_cols)\nlog = log.append(log_entry)\nlog","32145763":"#Pipeline\npipe_GBR = Pipeline([('scl', MinMaxScaler()), ('GBR', GradientBoostingRegressor())]) \n\n#Parameter-grid\nparam_grid = {'GBR__n_estimators': [50,100,150],'GBR__learning_rate':[0.1,0.2,0.5]} \n \n#Using RandomSearchCV\nRandom_GBR = RandomizedSearchCV( pipe_GBR , param_distributions=param_grid, cv= 5, n_iter=3) \n\n#Fitting the data in the model\nRandom_GBR.fit( X_train, y_train) \n\nprint(\" Best cross-validation score obtained is: {:.2f}\". format( Random_GBR.best_score_)) \nprint(\" Best parameters as part of Gridsearch is: \", Random_GBR.best_params_) \nprint(\" Train set score obtained is: {:.2f}\". format( Random_GBR.score( X_train, y_train)))\nprint(\" Validation set score obtained is: {:.2f}\". format( Random_GBR.score( X_val, y_val)))\nprint(\" Test set score obtained is: {:.2f}\". format( Random_GBR.score( X_test, Y_test)))","e9d96863":"y_pred=Random_GBR.predict(X_test)","ad94ffb6":"GBR_MAE=metrics.mean_absolute_error(Y_test,y_pred)\nGBR_MSE=metrics.mean_squared_error(Y_test,y_pred)\nGBR_R2=metrics.r2_score(Y_test,y_pred)\nprint(\"The mean_absolute_error is:{:.2f}\".format(GBR_MAE))\nprint(\"The mean_squared_error is:{:.2f}\".format(GBR_MSE))\nprint(\"The R2 score is:{:.2f}\".format(GBR_R2))","068feb0d":"scores = cross_val_score(Random_GBR, X, Y, cv=5)\nscores","78a76041":"model= GradientBoostingRegressor(n_estimators=150,learning_rate=0.2)","d597d220":"model.fit(X_train,y_train)","cd7dbf18":"model.feature_importances_","8935d98a":"## Calculating feature importance\nfeat_imp_dict = dict(zip(X.columns, model.feature_importances_))\nfeat_imp = pd.DataFrame.from_dict(feat_imp_dict, orient='index')\nfeat_imp.sort_values(by=0, ascending=False)","43167ee1":"log_entry = pd.DataFrame([[\"Gradient Boosting Regressor\",GBR_MAE,GBR_MSE,GBR_R2]], columns=log_cols)\nlog = log.append(log_entry)\nlog","0c79b28d":"GBR_mean_score=scores.mean()","c2821e5c":"GBR_score_SD=scores.std()","df52d33d":"confidence_95_positive=GBR_mean_score+(1.96*GBR_score_SD)","940b4ad6":"confidence_95_negative=GBR_mean_score-(1.96*GBR_score_SD)","320285e1":"print(confidence_95_positive,confidence_95_negative)","b8e51a99":"sns.pairplot(data_features,palette=\"Set2\", diag_kind=\"kde\", height=2.5)","3f3f244f":"sns.distplot(data_features['superplastic'])","2f6a7858":"data_guassian1=data_features[data_features['superplastic']>=0]","45e9bd26":"data_guassian1.head()","3dfc3341":"data_guassian2=data_features[data_features['superplastic']<0]","936c4730":"data_guassian1.head()","11f69e98":"X1=data_guassian1.drop(['strength'],axis=1)","550eedd7":"Y1=data_guassian1['strength']","7c785bc1":"X2=data_guassian2.drop(['strength'],axis=1)","a268a6a3":"Y2=data_guassian2['strength']","9a5ace67":"X1train,X1test,Y1train,Y1test=train_test_split(X1,Y1,test_size=0.3,random_state=2)","5803bad7":"X2train,X2test,Y2train,Y2test=train_test_split(X2,Y2,test_size=0.3,random_state=2)","16af111b":"model1= GradientBoostingRegressor(n_estimators=150,learning_rate=0.2)","4ecd3300":"model1.fit(X1train,Y1train)","8aeccdb0":"print(\"Guassian 1 Train score is :{:.2f}\".format(model1.score(X1train,Y1train)))","92218567":"print(\"Guassian 1 Test score is :{:.2f}\".format(model1.score(X1test,Y1test)))","a1de97c4":"model1.fit(X2train,Y2train)","15619d2f":"print(\"Guassian 2 Train score is :{:.2f}\".format(model1.score(X2train,Y2train)))","bcb6d623":"print(\"Guassian 2 Test score is :{:.2f}\".format(model1.score(X2test,Y2test)))","1d6a12c1":"### Lasso Linear Regression","8c826fbf":"#### Evaluating the performance","8a166399":"#### Predicting the test values","8c03d867":"### Gradient Boosting Regressor","be521e74":"### Explore for Guassians","f2d4405b":"##### Age, Water ratio with cement, slagand and ash  and water cement ratio are features of high importance","04f79701":"##### Water and super-plastic provides some negative correlation","3150bd6c":"##### The above are the cross validation score and our score should be in the above range\n","9cb5cac5":"#### Evaluating the performance","969773a4":"#### Principal Component Analysis","683dd882":"##### There are 1030 records with 8 independent variables and 1 target variable","b4f5a974":"##### 49 records were removed as they were considered as outliers","7a4b0849":"#### Checking the information of the data","f2238c35":"#### Super-plastic and age has multiple indiviual guassians which are very well seperated\n#### coraseagg and fineagg has multiple guassians but are not super seperated\n#### Considering super-plastic and creating 2 models for 2 guassians and comparing the results}","026247be":"##### There are no null values present","051afb4e":"##### None of the independent variables show positive high correlation(0.5)\/linear relationship between earch other or the target variable","677948db":"#### Checking the head of the data-set","6e34809b":"#### To describe the data- Five point summary","3eb67f32":"### Splitting the data with k-fold cross-validation- 10 folds- Training and validation","454fdd97":"### Support Vector Regressor","a906f3c8":"##### Model performance range with Gradient Boosting at 95% confidence is between 88-95%","187b3ceb":"#### Evaluating the performance","a97adb3b":"#### Evaluating the performance","49c0c650":"#### Predicting the test values","a3cbe622":"#### Removing all columns with z-score greater and lesser than 3 and -3 respectivley as the values are outliers","b54f4c3a":"##### Average strength ranges from 20 to 50 with normal distribution, outliers are present\n##### Average age values ranges from 0 to 50 with multiple guassians and right skew, outliers are present\n##### Average fine aggregate  values ranges from 725 to 825 and there are presence of outliers, follows a normal distribution","fadf1778":"#### Predicting the test values","34d4e844":"#### Splitting the data-set based on the 2 guassians","84e6d31c":"##### We now only have 6 independent features as opposed to 8","0071fcf3":"#### Comparing the train and test scores of both the guassians, they perform almost alike. Hence splitting and creating multiple models is not very fruitful","da77bd16":"#### Checking the data-types of the data","d91a1603":"#### Scaling the featured data","f066752a":"### Ridge Linear Regression","0e906d6f":"### Final Insights\n##### 1. The aim of the data-set is to predict strength of concrete for a specific mixture for a specific age(in days)\n##### 2. Data had 8 independent variable and 1 target variable and are continous. So regression model is to be developed\n##### 3. Data was checked for info and five point summary to better understand the features and for missing values\n##### 4. Exploratory Data Analytics was performed to visualize the data and its distribution, skewness and outliers\n##### 5. Outliers were handled by converting all numerical variables to z-score and removing rows greater than +\/-3, there are zero values but they are not missing values\n##### 6. Feature Engineering was performed to convert features like Creating water cement ratio by dividing the values of water with cement and Creating water cement ratio with slag ash by dividing the values of water with cement, slag and ash and independent features were reduced from 8 to 6\n##### 7. PCA was performed but 5 dimensions were still required to explain 95% of the variance which is just reducing 1 dimension and is not fruitful\n##### 8. K-Fold cross validation was performed and data was split the data to train, validation and test\n##### 9. 5 models were chosen(Support Vector Regressor, Ridge Regressor,Lasso Regressor, Decision Tree Regressor, Gradient Boosting Regressor)- Ridge and Lasso were trained with polynoimial features\n##### 10. Regrularization,Pipeline, GridSearchCV\/RandomSearchCV were done on each module to tune the model and extract the best parameters\n##### 11. Cross validation scores were calculated and Gradient Boosting performed well with 93% score and at 95% confidence score is between 88-95%","2ba432dc":"#### Check for any null values in the data","6ce2e514":"##### The above are the cross validation score and our score should be in the above range\n","a50d8b86":"### Decision Tree Regression","2bfa95ed":"##### Average cement values ranges from 200 to 350 and there are no presence of outliers, follows a normal distribution\n##### Average slag values ranges from 0 to 150 and is highly right skewed with presence of outliers\n##### Average ash values ranges from 0 to 125 and has multiple guassians","9d73e708":"### Reading and understanding the data-set","d233927a":"#### Creating water cement ratio by dividing the values of water with cement","5d878edb":"#### Creating water cement ratio with slag ash by dividing the values of water with cement, slag and ash","92c60ca6":"##### Independednt variables like slag, ash, super-plastic and coarseagg have multiple guassians","6c8a6af1":"##### These pairs of independednt attibutes have average correlation\n","b5658bcf":"#### Creating composite Features","8a30ca24":"##### Mean of the following attributes are not in sync with the median which implies the presence of outliers\n###### Cement, Slag, ash, age","80af97c4":"#### Applying z-score to scale the data and standardize the data","c58ff8de":"#### Dimensionality Reduction\n\n##### Now 5 dimensions seems good. With 5 variables we can explain over 95% of the variation in the original data, but PCA to reduce just 1 dimension is not very fruitful","78bb4c3d":"#### Correlation Matrix","eb516912":"#### Uni-Variate Analysis","fad29436":"##### The above are the cross validation score and our score should be in the above range\n","6e7c7e03":"#####  Water ratio with cement, slag and ash ,age and water cement ratio are features of high importance","4d7b4c90":"#### Strategies to remove Outliers","70318063":"#### Creating Polynomial Features","35e0c455":"#### Predicting the test values","c1ceaeed":"### Exploratory Data Analytics","4283d028":"##### Average values for water ranges from 160 to 190 with multiple guassians and skewness, outliers are present\n##### Average Superplastic values ranges from 0 to 10 with multiple guassians and right skew, outliers are present\n##### Average corase aggregate  values ranges from 925 to 1025 and there are no presence of outliers, follows a normal distribution","861efadc":"#### Gradient Boosting Regressor provides us with the best test score of 93%","1e33e39b":"##### There are no null or nan values present","826cbfeb":"#### Multi-variate Analysis","1d91d834":"### Feature Engineering","b188d81d":"#### Creating a Pipeline to scale and then fit the model","23a2b854":"###### \u25cf Cement : measured in kg in a m3 mixture\n###### \u25cf Blast : measured in kg in a m3 mixture\n###### \u25cf Fly ash : measured in kg in a m3 mixture\n###### \u25cf Water : measured in kg in a m3 mixture\n###### \u25cf Superplasticizer : measured in kg in a m3 mixture\n###### \u25cf Coarse Aggregate : measured in kg in a m3 mixture\n###### \u25cf Fine Aggregate : measured in kg in a m3 mixture\n###### \u25cf Age : day (1~365)\n###### \u25cf Concrete compressive strength measured in MPa\n","7c71b2d8":"##### Age, Water ratio with cement, slagand and ash  and water cement ratio are features of high importance","ddf18442":"##### The above are the cross validation score and our score should be in the above range\n","3534d45b":"#### Creating a pair plot for the feature Engineered data-set to infer the occurance of multiple guasians","3b7f1de9":"#### Splitting X-independent attributes and Y-dependent attributes and keeping the test set seperate","84b24b0e":"#### Splitting the data set into test and train","32e9f44d":"#### Evaluating the performance","9b2fd1ee":"#### Predicting the test values","207682b5":"##### The above are the cross validation score and our score should be in the above range\n","b9b0b686":"##### All the individual and dependednt variables are continous numerical data","99b2f90e":"### Importing Necessary Libraries"}}