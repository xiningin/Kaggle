{"cell_type":{"fc8a5e61":"code","0376aa0d":"code","657d238d":"code","cab29cd5":"code","0dc8fa19":"code","4bec04f8":"code","1c5fba48":"code","dc894acd":"code","f3fbdf9d":"code","7ab2be58":"code","1ea0bbcf":"code","cb47ec81":"code","51d6518d":"code","c3a5d4f6":"code","eaf9296b":"code","a7dea806":"code","3bcc783f":"code","705e1ec4":"code","a2d6d7ba":"code","a7e23a74":"code","6244279a":"code","a39888bf":"code","4dbcb4f2":"code","f9982960":"code","8a49d44d":"code","ecee12a6":"code","6578f1f5":"code","78ed58ea":"code","772e4555":"code","6f5ddf47":"code","705ab58f":"code","7683f918":"code","77d76ee5":"code","386a08f2":"code","dc788b34":"code","2145edf6":"code","ecf158d3":"code","ddc91be7":"code","6951647c":"code","5d5c8177":"code","7379e5a3":"code","65231b25":"code","c20486ca":"code","6dc0e96c":"code","faa2006a":"code","15260b35":"code","bc76dfc2":"markdown","838c6c45":"markdown","19440420":"markdown","72261358":"markdown","ceb4be8f":"markdown","a953e3c4":"markdown","618e4375":"markdown","ccc58e37":"markdown","b68a4633":"markdown","b7f48e8e":"markdown","a298369f":"markdown","2cd2d79d":"markdown","a304af93":"markdown","8917083c":"markdown","7adce926":"markdown","8b0d425a":"markdown","d1e56490":"markdown","5b642669":"markdown","42db46ee":"markdown","3c3c7903":"markdown","c6685864":"markdown","ede1fca1":"markdown","4898fa00":"markdown","371bf8a9":"markdown"},"source":{"fc8a5e61":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns","0376aa0d":"train = pd.read_csv('..\/input\/titanic\/train.csv')\ntrain.head()","657d238d":"categorical = ['Pclass', 'Sex', 'Embarked']\nnumerical = ['Age', 'Fare', 'SibSp', 'Parch']\ntarget = 'Survived'","cab29cd5":"train[numerical].describe()","0dc8fa19":"from scipy.stats import kurtosis, skew\n\nfor num in numerical:\n    print('{} has kurtosis: {} and skew: {}'\n          .format(num, kurtosis(train[num], nan_policy = 'omit'), skew(train[num], nan_policy = 'omit')))","4bec04f8":"from scipy.stats import probplot\n\nfor num in numerical:\n    f, axs = plt.subplots(1, 3, figsize = (12, 4))\n    sns.distplot(train[num], kde = False, \n                 ax = axs[0])\n    probplot(train[num], \n             plot = axs[1])\n    probplot(np.log1p(train[num]), \n             plot = axs[2])\n    axs[1].set_title('No transformation')\n    axs[2].set_title('Log transformation')\n    plt.tight_layout()\n    plt.show()","1c5fba48":"train['Fare'] = np.log1p(train['Fare'])\ntrain['Age'] = np.log1p(train['Age'])","dc894acd":"g = sns.heatmap(train[numerical].corr(method = 'spearman'),\n                vmax = 0.6, vmin = -0.6,\n                annot = True,\n                fmt = '.2f',\n                square = True)","f3fbdf9d":"from scipy.stats import ttest_ind\n\nmask = (train[target] == 1)\nt_result = []\nfor num in numerical:\n    t_stat, p_val = ttest_ind((train[mask])[num],\n                              (train[~mask])[num],\n                              equal_var = True,\n                              nan_policy = 'omit')\n    t_result.append({\n        'group_1' : 'Survived',\n        'group_2' : 'Died',\n        'variable' : num,\n        't_stat' : t_stat,\n        'p_value' : p_val\n    })\nt_result = pd.DataFrame(t_result)","7ab2be58":"t_result","1ea0bbcf":"import re\n\ndef get_titles(data):\n    title_re = re.compile(r'(?:^.+), (\\w+)')\n    titles = []\n    for name in data['Name']:\n        titles.append(title_re.findall(name)[0])\n\n    data['Title'] = titles\n    #data['Title'].value_counts()\n\n    for i, title in enumerate(data['Title']):\n        if title in ['Miss', 'Ms', 'Mlle', 'Mrs', 'Mme']:\n            cat = 'Mrs_Ms'\n        elif title in ['Mr', 'Don']:\n            cat = 'Mr'\n        elif title in ['Master']:\n            cat = 'Master'\n        else:\n            cat = 'Honorable'\n        data.loc[i, 'Title'] = cat","cb47ec81":"categorical = ['Pclass', 'Sex', 'Embarked', 'Title']\nget_titles(train)","51d6518d":"for cat in categorical:\n    g = sns.countplot(x = cat, hue = target, data = train)\n    plt.show()","c3a5d4f6":"g = sns.catplot(x = 'Pclass', hue = target, col = 'Sex', data = train,\n                kind = 'count')","eaf9296b":"g = sns.catplot(x = 'Title', hue = target, col = 'Pclass', data = train,\n                kind = 'count')","a7dea806":"from scipy.stats import chi2_contingency\n\nchi2_result = []\nfor cat in categorical:\n    crosstab = pd.crosstab(train[cat], train[target])\n    chi2_stat, p_val, dof, ex = chi2_contingency(crosstab)\n    chi2_result.append({\n        'var_1' : cat,\n        'var_2' : target,\n        'chi2' : chi2_stat,\n        'dof' : dof,\n        'p_value' : p_val\n    })\nchi2_result = pd.DataFrame(chi2_result)","3bcc783f":"chi2_result","705e1ec4":"train.isnull().sum()","a2d6d7ba":"X = train.copy()\ny = X.pop(target)","a7e23a74":"from sklearn.compose import ColumnTransformer\nfrom sklearn.impute import SimpleImputer\n\nmedian_imputer = SimpleImputer(strategy = 'median')\nmode_imputer = SimpleImputer(strategy = 'most_frequent')\nmissing_transformer = ColumnTransformer([('num', median_imputer, numerical), \n                                         ('cat', mode_imputer, categorical)])\nmissing_transformer.fit(X)\nX_imp = pd.DataFrame(missing_transformer.transform(X))\nX_imp.columns = numerical + categorical\nX.drop(numerical + categorical, axis = 1, inplace = True)\nX = pd.concat([X, X_imp], axis = 1)","6244279a":"from sklearn.preprocessing import OrdinalEncoder\n\nordinal_encoder = OrdinalEncoder()\nX_enc = pd.DataFrame(ordinal_encoder.fit_transform(X[categorical]))\nX_enc.columns = categorical\nX.drop(categorical, axis = 1, inplace = True)\nX = pd.concat([X, X_enc], axis = 1)\nX = pd.get_dummies(X, columns = categorical, dtype = np.int64, drop_first = True)","a39888bf":"X.head()","4dbcb4f2":"from sklearn.feature_selection import mutual_info_classif\nmi = mutual_info_classif(X[features], y)\nmi_idx = mi.argsort()\nfor i in mi_idx:\n    print('{} has MI score: {}'.format(X[features].columns[i], mi[i]))","f9982960":"features = ['Age', 'Fare', 'SibSp', 'Parch', 'Sex_1.0',\n            'Embarked_1.0', 'Embarked_2.0', 'Pclass_1.0',\n            'Pclass_2.0', 'Title_1.0', 'Title_2.0', 'Title_3.0']\ng = sns.clustermap(X.corr(method = 'spearman'))","8a49d44d":"from sklearn.model_selection import train_test_split\n\nX_train, X_val, y_train, y_val = train_test_split(X, y)","ecee12a6":"features = ['Title_2.0', 'Sex_1.0', 'Fare']","6578f1f5":"from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, GradientBoostingClassifier\nfrom sklearn.tree import DecisionTreeClassifier\n\nclfs = {\n    'tree' : DecisionTreeClassifier(),\n    'rf' : RandomForestClassifier(),\n    'extra' : ExtraTreesClassifier(),\n    'grad' : GradientBoostingClassifier(),\n}","78ed58ea":"from sklearn.metrics import f1_score\n\nclf_result = []\nfor model_name, model in clfs.items():\n    model.fit(X_train[features], y_train)\n    preds = model.predict(X_test[features])\n    cv_score = cross_val_score(model, X_train[features], y_train, scoring = 'accuracy', cv = 5)\n    clf_result.append({\n        'model' : model_name,\n        'mean acc' : cv_score.mean(),\n        'std acc' : cv_score.std()\n    })\n    \nclf_result = pd.DataFrame(clf_result)","772e4555":"clf_result","6f5ddf47":"from sklearn.model_selection import RandomizedSearchCV\n\nparams = {\n    'n_estimators' : [50, 100, 150, 200],\n    'learning_rate' : [0.01, 0.03, 0.05, 0.07, 0.1, 0.3],\n    'subsample' : [0.4, 0.45, 0.5, 0.55, 0.7],\n    'max_features' : ['auto', 'sqrt'],\n    'n_iter_no_change' : [0, 5, 10],\n    'max_depth' : [2, 3, 4],\n    \n}\nbase_clf = GradientBoostingClassifier(criterion = 'mse', validation_fraction = 0.25, random_state = 0)\n\ngrid_search = RandomizedSearchCV(base_clf, params, scoring = 'accuracy', random_state = 0, n_iter = 40)\ngrid_search.fit(X_train[features], y_train)\nprint('40 random searches complete')","705ab58f":"grid_search_result = pd.DataFrame(grid_search.cv_results_)\ngrid_search_result.loc[grid_search_result['rank_test_score'] < 5].sort_values('rank_test_score')","7683f918":"clf_1 = GradientBoostingClassifier(n_estimators = 200,\n                                 subsample = 0.45,\n                                 max_features = 'auto',\n                                 criterion = 'mse',\n                                 n_iter_no_change = 10,\n                                 learning_rate = 0.03,\n                                 validation_fraction = 0.25,\n                                 max_depth = 3, \n                                 random_state = 0)\n\nclf_2 = GradientBoostingClassifier(n_estimators = 50,\n                                 subsample = 0.7,\n                                 max_features = 'sqrt',\n                                 criterion = 'mse',\n                                 n_iter_no_change = 10,\n                                 learning_rate = 0.03,\n                                 validation_fraction = 0.25,\n                                 max_depth = 4, \n                                 random_state = 0)\n\nclf_3 = GradientBoostingClassifier(n_estimators = 150,\n                                 subsample = 0.7,\n                                 max_features = 'auto',\n                                 criterion = 'mse',\n                                 n_iter_no_change = 5,\n                                 learning_rate = 0.1,\n                                 validation_fraction = 0.25,\n                                 max_depth = 2, \n                                 random_state = 0)","77d76ee5":"from sklearn.model_selection import learning_curve\n\ndef plot_learning_curve(my_clf):\n    n_samples, train_score, test_score = learning_curve(my_clf, X_train[features], y_train,\n                                                        cv = 5, scoring = 'neg_mean_squared_error',\n                                                        train_sizes = np.linspace(0.05, 1, 8),\n                                                        random_state = 1)\n    n_samples_mean = np.mean(n_samples)\n    train_score_mean = -1 * np.mean(train_score, axis = 1)\n    train_score_std = -1 * np.std(train_score, axis = 1)\n    test_score_mean = -1 * np.mean(test_score, axis = 1)\n    test_score_std =  -1 * np.std(test_score, axis = 1)\n    \n    plt.plot(n_samples, train_score_mean, '-o', color = 'darkorange', label = 'train')\n    plt.fill_between(n_samples, \n                     train_score_mean + train_score_std, \n                     train_score_mean - train_score_std,\n                     color = 'darkorange', alpha = 0.1)\n    plt.plot(n_samples, test_score_mean,'-o', color = 'navy', label = 'test')\n    plt.fill_between(n_samples, \n                     test_score_mean + test_score_std, \n                     test_score_mean - test_score_std,\n                     color = 'navy', alpha = 0.1)\n    plt.title('learning curve for gradient boosting model')\n    plt.xlabel('train size')\n    plt.ylabel('mse')\n    plt.legend()\n    plt.show()","386a08f2":"plot_learning_curve(clf_1)","dc788b34":"plot_learning_curve(clf_2)","2145edf6":"plot_learning_curve(clf_3)","ecf158d3":"final_clf = GradientBoostingClassifier(n_estimators = 150,\n                                 subsample = 0.5,\n                                 max_features = 'auto',\n                                 criterion = 'mse',\n                                 n_iter_no_change = 5,\n                                 learning_rate = 0.07,\n                                 validation_fraction = 0.25,\n                                 max_depth = 3, \n                                 random_state = 0)\nplot_learning_curve(final_clf)","ddc91be7":"from sklearn.metrics import f1_score, accuracy_score\n\nfinal_clf.fit(X_train[features], y_train)\npreds = final_clf.predict(X_val[features])\n\nprint('Number of estimators after early stopping: {}'.format(final_clf.n_estimators_))\nprint('Accuracy: {}'.format(accuracy_score(preds, y_val)))\nprint('F1: {}'.format(f1_score(preds, y_val)))","6951647c":"from sklearn.metrics import confusion_matrix\ncm = confusion_matrix(preds, y_val)\ng = sns.heatmap(cm, annot = True, fmt = 'd')","5d5c8177":"from sklearn.metrics import plot_roc_curve\n\ng = plot_roc_curve(final_clf, X_val[features], y_val)","7379e5a3":"from sklearn.metrics import plot_precision_recall_curve\n\ng = plot_precision_recall_curve(final_clf, X_val[features], y_val)","65231b25":"test = pd.read_csv('..\/input\/titanic\/test.csv')\nX_test = test.copy()\nX_test['Fare'] = np.log1p(X_test.Fare)\nget_titles(X_test)","c20486ca":"X_test_imp = pd.DataFrame(missing_transformer.transform(X_test))\nX_test_imp.columns = numerical + categorical\nX_test.drop(numerical + categorical, axis = 1, inplace = True)\nX_test = pd.concat([X_test, X_test_imp], axis = 1)","6dc0e96c":"X_test_enc = pd.DataFrame(ordinal_encoder.fit_transform(X_test[categorical]))\nX_test_enc.columns = categorical\nX_test.drop(categorical, axis = 1, inplace = True)\nX_test = pd.concat([X_test, X_test_enc], axis = 1)\nX_test = pd.get_dummies(X_test, columns = categorical, dtype = np.int64, drop_first = True)","faa2006a":"X_test.head()","15260b35":"final_clf.fit(X[features], y)\nfinal_preds = final_clf.predict(X_test[features])\nfinal_submission = pd.DataFrame({'PassengerId' : X_test.PassengerId,\n                               'Survived' : final_preds})\nfinal_submission.to_csv('may12_finalpreds.csv', index = False)","bc76dfc2":"I plotted the distribution for the numerical variables and visually inspected their normality with the QQ plot. I tried log, sqrt, power transformations to make the distributions more normal, and the best transformation was log for Fare and Age. It is not necessary to transform the data before using a tree-based model, but transformations could help prevent overfitting.","838c6c45":"We are going to fit different types of tree classifiers to see which one is the optimal model to predict Survived.","19440420":"Chi2 contigency test notes:\n* There is a significant effect of Pclass, Sex, Embarked, and Title on survival.","72261358":"Categorical plot notes:\n* Passengers from Pclass 1 had a higher probability of survival than passengers from Pclass 2 or 3\n* Females are more likely to survive than males\n* Passengers who had the title \"Master\" were all males but had a higher probability of survival than passengers who had the title \"Mr\"\n* There are more male than female passengers\n* There are more passengers in Pclass 3 than Pclass 1 or 2\n* The majority of passengers embarked from origin S\n* Passengers who embarked from origin C had a higher probability of survival","ceb4be8f":"Preparing submission on test data","a953e3c4":"# Preprocessing\nCheck for and fill missing variables","618e4375":"Age is roughly a normal distribution, but Fare is not normal due to high skew and peakedness (kurtosis). Since SibSp and Parch are not continuous variables, I am not too concerned about the normality of their distribution.","ccc58e37":"To confirm that the final model is accurate, I looked at the confusion matrix, ROC curve, and PR curve.","b68a4633":"I only transform the continuous variables.","b7f48e8e":"# EDA\nI classify the variables based on the data type that they hold. Looking back on the notebook, I would probably split the numerical variables into discrete and continuous categories in the future. (However, this will likely not affect the EDA.)","a298369f":"Clustermap notes:\n* There is one cluster with major collinearity between Title_3.0 (Master), Sex_1.0 (Female), and Title_2.0 (Mr). For now, we will not do anything about this because they all have high mutual information score.","2cd2d79d":"Learning curve notes:\n* The MSE is roughly 0.15 - 0.20 for all the curves\n* The second and third curve have larger splitting than the first curve, which suggests there is more overfitting in those models.","a304af93":"Descriptive statistics notes:\n* Most passengers are middle-aged adults\n* Age has some missing values\n* Fare is very right-skewed\n* Most passengers have no siblings or spouses on board","8917083c":"I decided to create a new feature after the first model performed poorly. Feature engineering helped me greatly improve the accuracy of the model. Not shown is a contingency table between Sex and Title, which helped me decide how to group the titles into the four bins: Mr, Mrs_Ms, Master, and Honorable. One important observation from this table is that all the passengers with title \"Master\" are males, and the majority of these passengers survived even though the overall porbability of survival for males is low.","7adce926":"One-hot encode categorical variables with low cardinality. We can drop the first variable to avoid any collinearity.","8b0d425a":"I evaluated the top three combinations from the randomized grid search using a learning curve.","d1e56490":"I used a Randomized Grid Search to evaluate different combinations of hyperparameters.","5b642669":"# Titanic Tree Model Comparison\nIn this notebook, I build a gradient boosting classifier to determine whether someone aboard the Titanic survives or dies. ","42db46ee":"T test between two means notes:\n* There is a significant difference in Age, Fare, and Parch between passengers that survived or died.","3c3c7903":"# Modeling\nThis was the hardest part of the notebook, as I was originally scoring roughly 0.69 with the model that I created because the model was overfitting the training data. I learned a lot about different diagnostic plots and visualizations to assess how well the model is performing in the next section.","c6685864":"Correlation plot notes:\n* Moderate positive correlation between Fare and Parch and SibSp, which are like measures of family size\n* Weak correlation between Age and Parch and SibSp","ede1fca1":"Mutual information score notes:\n* Survival is mainly predicted by Title_3.0, Sex_1.0, Fare, and Title_2.0","4898fa00":"Model selection notes:\n* There does not seem to be much difference between baseline Random Forests or Gradient Boosting Classifiers, but since Gradient Boosting usually performs well for hyperparameter tuning, I will choose this model.","371bf8a9":"I did some manual, guess-and-check work to further optimize the hyperparameters and reduce overfitting by reducing the gap between the train and test curves."}}