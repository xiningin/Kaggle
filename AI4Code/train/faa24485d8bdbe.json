{"cell_type":{"05bbae12":"code","620fa2d6":"code","7bd3f327":"code","7ee3f410":"code","fb08b8eb":"code","33b08807":"code","2f348955":"code","c2a508c0":"code","2bd60b0a":"code","f4950b3c":"code","04ee8718":"code","a48dd224":"code","a19ead41":"code","f8d39c32":"code","e40caebf":"code","4a16eed6":"code","a0ab2523":"code","2a5c03ad":"code","bb0f8fa6":"code","51956fd7":"code","46539e21":"code","caa366f7":"code","9105348b":"code","2921af48":"code","e2c163a5":"code","634c2ebe":"code","20d9779d":"code","6a917722":"code","040cc9ba":"code","c8957a00":"code","b205c37b":"code","e9412d7e":"code","375f67a0":"code","50dce7e2":"code","c56e254c":"code","8b9f4214":"code","80768f3a":"code","a3897bc9":"code","679bac6e":"code","38351ac9":"code","0ea5b613":"code","86de3388":"code","0f115bef":"code","66cf9a6c":"code","fe0836f7":"code","b928ec92":"code","c73f1a95":"code","4c219a4d":"code","291d0f76":"code","eccf2bbe":"code","aab2d399":"code","fb81c856":"code","861ae290":"code","77ed8a1d":"code","1bc4709f":"code","76405cf2":"code","35888a5c":"code","2636300c":"code","e6829986":"code","34033620":"code","37a92cde":"code","7231442b":"code","ffe55ed6":"code","0df0a55f":"code","5d593527":"code","d52aec78":"code","8d651629":"code","5595afef":"code","3b539062":"code","8c6186ce":"code","afd0f1b0":"code","0c31a820":"code","a58309f2":"code","0ac8ba49":"code","9f30d64e":"code","8d6018eb":"code","aa9dea01":"code","b4cd80b5":"code","b8d326de":"code","714f36e5":"code","614ca1b8":"code","92edba5e":"code","a9ac545c":"code","dbff82e1":"code","10a1b353":"code","2d85f293":"code","4b6de87b":"code","dd9a9412":"code","e9c844e5":"markdown","f820dc7b":"markdown","00a0b0df":"markdown","fa76bec9":"markdown","62e7a772":"markdown","95c0ab9b":"markdown","91b1dffe":"markdown","757d1648":"markdown","74e9ed6c":"markdown","2bf2797a":"markdown","d6135aa5":"markdown","e8d5c1e5":"markdown","d7d29746":"markdown","39b6c380":"markdown","5977630c":"markdown","92e90c11":"markdown","5e9a3e66":"markdown","debc5657":"markdown","efd49994":"markdown","e7036761":"markdown","2b9c0714":"markdown","a768e463":"markdown","852e71cb":"markdown","edd3cc07":"markdown","f67409c2":"markdown","24aca612":"markdown","2df19d44":"markdown","97b27bc6":"markdown","552a6a77":"markdown","996f0e6c":"markdown","b94681aa":"markdown","137850d0":"markdown","db588499":"markdown","570eac12":"markdown","8e807bbe":"markdown","05214e7b":"markdown","e25d2168":"markdown","4374d2af":"markdown","7dd2cca2":"markdown","2ef7b176":"markdown","ca908a68":"markdown","6656c5e6":"markdown","981c77f6":"markdown","ae81521c":"markdown","ad6015b8":"markdown","59eb7a65":"markdown","7c5edc0e":"markdown","252454e7":"markdown","abbe77c8":"markdown","0eb072a9":"markdown","2e945b56":"markdown","d04a1b5b":"markdown","f8041211":"markdown","b65f14f7":"markdown","331cd285":"markdown","bd819e6c":"markdown","117b13e9":"markdown","424765a3":"markdown","1b3d1231":"markdown","dac3ed5b":"markdown","a7589150":"markdown","67139906":"markdown"},"source":{"05bbae12":"from IPython.display import HTML\nHTML(\"\"\"<style>h1,h2,h3 {margin: 1em 0 0.5em 0;font-weight: 600;font-family:'Titillium Web', sans-serif;position: relative;  font-size: 36px;line-height: 40px;padding: 15px 15px 15px 2.5%;color: #00018D;box-shadow: inset 0 0 0 1px rgba(97,0,45, 1), inset 0 0 5px rgba(53,86,129, 1),inset -285px 0 35px #F2D8FF;border-radius: 0 10px 0 15px;background: #FFD8B2}<\/style>\"\"\")","620fa2d6":"from sklearn import datasets\nfrom sklearn import metrics\nfrom sklearn import preprocessing\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import train_test_split\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns\nsns.set()\n\n# for 3d interactive plots\nfrom ipywidgets import interact, fixed\nfrom mpl_toolkits import mplot3d\n\nfrom IPython.core.interactiveshell import InteractiveShell\nInteractiveShell.ast_node_interactivity = \"all\"","7bd3f327":"def generate_linear_regression_data(n=100, d=1, coef=[5], intercept=1, sigma=0):\n  x = np.random.randn(n,d)\n  y = (np.dot(x, coef) + intercept).squeeze() + sigma * np.random.randn(n)\n  return x, y","7ee3f410":"n_samples = 100\ncoef = [5]\nintercept = 1","fb08b8eb":"x_train, y_train = generate_linear_regression_data(n=n_samples, d=1, coef=coef, intercept=intercept)\nx_test,  y_test  = generate_linear_regression_data(n=50, d=1, coef=coef, intercept=intercept)","33b08807":"sns.scatterplot(x=x_train.squeeze(), y=y_train, s=50);\nplt.xlabel('x');\nplt.ylabel('y');","2f348955":"reg_simple = LinearRegression().fit(x_train, y_train)\nprint(\"Intercept: \" , reg_simple.intercept_)\nprint(\"Coefficient list: \", reg_simple.coef_)","c2a508c0":"x_line = [np.min(x_train), np.max(x_train)]\ny_line = x_line*reg_simple.coef_ + reg_simple.intercept_\nsns.scatterplot(x=x_test.squeeze(), y=y_test, s=50, color='green');\nsns.scatterplot(x=x_train.squeeze(), y=y_train, s=50);\nsns.lineplot(x=x_line, y=y_line, color='red');\nplt.xlabel('x');\nplt.ylabel('y');","2bd60b0a":"# Note: other ways to do the same thing...\n# first, add a ones column to design matrix\nx_tilde = np.hstack((np.ones((n_samples, 1)), x_train))\n\n# using matrix operations to find w = (X^T X)^{-1} X^T y\nprint( (np.linalg.inv((x_tilde.T.dot(x_tilde))).dot(x_tilde.T)).dot(y_train) )\n\n# using solve on normal equations: X^T X w = X^T y\n# solve only works on matrix that is square and of full-rank\n# see https:\/\/numpy.org\/doc\/stable\/reference\/generated\/numpy.linalg.solve.html\nprint( np.linalg.solve(x_tilde.T.dot(x_tilde), x_tilde.T.dot(y_train)) )\n\n# using the lstsq solver \n# problem may be under-, well-, or over-determined\n# see https:\/\/numpy.org\/doc\/stable\/reference\/generated\/numpy.linalg.lstsq.html\nprint( np.linalg.lstsq(x_tilde,y_train,rcond=0)[0] ) ","f4950b3c":"x_train_mr = x_train - np.mean(x_train)\ny_train_mr = y_train - np.mean(y_train)\nsns.scatterplot(x=x_train_mr.squeeze(), y=y_train_mr, s=50);\nplt.xlabel('x');\nplt.ylabel('y');","04ee8718":"reg_mr = LinearRegression().fit(x_train_mr, y_train_mr)\nprint(\"Intercept: \" , reg_mr.intercept_)\nprint(\"Coefficient list: \", reg_mr.coef_)","a48dd224":"y_test_hat = reg_simple.intercept_ + np.dot(x_test,reg_simple.coef_)","a19ead41":"x_line = [np.min(x_test), np.max(x_test)]\ny_line = x_line*reg_simple.coef_ + reg_simple.intercept_","f8d39c32":"sns.lineplot(x=x_line, y=y_line, color='red');\nsns.scatterplot(x=x_test.squeeze(), y=y_test_hat, s=50, color='purple');\nplt.xlabel('x');\nplt.ylabel('y');","e40caebf":"y_test_hat = reg_simple.intercept_ + np.dot(x_test,reg_simple.coef_)\nmse_simple = 1.0\/(len(y_test)) * np.sum((y_test - y_test_hat)**2)\nmse_simple","4a16eed6":"# another way to do the same thing using sklearn\ny_test_hat = reg_simple.predict(x_test)\nmetrics.mean_squared_error(y_test, y_test_hat)","a0ab2523":"p = sns.scatterplot(x=x_test.squeeze(), y=y_test_hat, s=50);\np = plt.xlabel('x')\np = plt.ylabel('y')\n\ncoefs = np.arange(2, 8, 0.5)\nmses = np.zeros(len(coefs))\n\nfor idx, c in enumerate(coefs):\n  y_test_hat_c = (reg_simple.intercept_ + np.dot(x_test,c)).squeeze()\n  mses[idx] =  1.0\/(len(y_test_hat_c)) * np.sum((y_test - y_test_hat_c)**2)\n  x_line = [np.min(x_train), np.max(x_train)]\n  y_line = [x_line[0]*c + reg_simple.intercept_, x_line[1]*c + intercept]\n  p = sns.lineplot(x=x_line, y=y_line, color='red', alpha=0.2);","2a5c03ad":"sns.lineplot(x=coefs, y=mses);\nsns.scatterplot(x=coefs, y=mses, s=50);\nsns.scatterplot(x=reg_simple.coef_, y=mse_simple, color='red', s=100);\np = plt.xlabel('w1');\np = plt.ylabel('Test MSE');","bb0f8fa6":"var_y = 1.0\/len(y_test) * np.sum((y_test - np.mean(y_test))**2)\nvar_y","51956fd7":"mean_y = np.mean(y_test)\nmean_y","46539e21":"plt.hlines(y=mean_y, xmin=np.min(x_test), xmax=np.max(x_test));\nplt.vlines(x_test, ymin=mean_y, ymax=y_test, alpha=0.5, color='magenta');\nsns.scatterplot(x=x_test.squeeze(), y=y_test, color='purple', s=50);\nplt.xlabel('x');\nplt.ylabel('y');","caa366f7":"plt.plot(x_test, y_test_hat);\nplt.vlines(x_test, ymin=y_test, ymax=y_test_hat, color='magenta', alpha=0.5);\nsns.scatterplot(x=x_test.squeeze(), y=y_test, color='purple', s=50);\nx_line = [np.min(x_test), np.max(x_test)]\ny_line = x_line*reg_simple.coef_ + reg_simple.intercept_\nsns.lineplot(x=x_line, y=y_line, color='red');\nplt.xlabel('x');\nplt.ylabel('y');","9105348b":"x_train, y_train = generate_linear_regression_data(n=n_samples, d=1, coef=coef, intercept=intercept, sigma=2)\nx_test,  y_test =  generate_linear_regression_data(n=50, d=1, coef=coef, intercept=intercept, sigma=2)","2921af48":"sns.scatterplot(x=x_train.squeeze(), y=y_train, s=50);\nplt.xlabel('x');\nplt.ylabel('y');","e2c163a5":"reg_noisy = LinearRegression().fit(x_train, y_train)\nprint(\"Coefficient list: \", reg_noisy.coef_)\nprint(\"Intercept: \" , reg_noisy.intercept_)","634c2ebe":"x_line = [np.min(x_train), np.max(x_train)]\ny_line = x_line*reg_noisy.coef_ + reg_noisy.intercept_\n\nsns.scatterplot(x=x_train.squeeze(), y=y_train, s=50);\nsns.lineplot(x=x_line, y=y_line, color='red');\nplt.xlabel('x');\nplt.ylabel('y');\n","20d9779d":"y_test_hat = reg_noisy.intercept_ + np.dot(x_test,reg_noisy.coef_)","6a917722":"x_line = [np.min(x_test), np.max(x_test)]\ny_line = x_line*reg_noisy.coef_ + reg_noisy.intercept_","040cc9ba":"sns.lineplot(x=x_line, y=y_line, color='red');\nsns.scatterplot(x=x_test.squeeze(), y=y_test_hat, color='red', s=50);\nsns.scatterplot(x=x_test.squeeze(), y=y_test, color='purple', s=50);\nplt.xlabel('x');\nplt.ylabel('y');\n","c8957a00":"y_test_hat = reg_noisy.intercept_ + np.dot(x_test,reg_noisy.coef_)\nmse_noisy = 1.0\/(len(y_test)) * np.sum((y_test - y_test_hat)**2)\nmse_noisy","b205c37b":"y_test_perfect_coef = intercept + np.dot(x_test,coef)\n\nmse_perfect_coef = 1.0\/(len(y_test_perfect_coef)) * np.sum((y_test_perfect_coef - y_test)**2)\nmse_perfect_coef","e9412d7e":"y_train_hat = reg_noisy.intercept_ + np.dot(x_train,reg_noisy.coef_)\nmse_train_est = 1.0\/(len(y_train)) * np.sum((y_train - y_train_hat)**2)\nmse_train_est","375f67a0":"y_train_perfect_coef = intercept + np.dot(x_train,coef)\nmse_train_perfect = 1.0\/(len(y_train_perfect_coef)) * np.sum((y_train_perfect_coef - y_train)**2)\nmse_train_perfect","50dce7e2":"coefs = np.arange(4.5, 5.5, 0.1)\nmses_test = np.zeros(len(coefs))\nmses_train = np.zeros(len(coefs))\n\nfor idx, c in enumerate(coefs):\n  y_test_hat_c = (reg_noisy.intercept_ + np.dot(x_test,c)).squeeze()\n  mses_test[idx] =  1.0\/(len(y_test_hat_c)) * np.sum((y_test - y_test_hat_c)**2)\n  y_train_hat_c = (reg_noisy.intercept_ + np.dot(x_train,c)).squeeze()\n  mses_train[idx] =  1.0\/(len(y_train_hat_c)) * np.sum((y_train - y_train_hat_c)**2)","c56e254c":"plt.figure(figsize=(10,5))\n\nplt.subplot(1,2,1)\nsns.lineplot(x=coefs, y=mses_train)\nsns.scatterplot(x=coefs, y=mses_train, s=50);\nsns.scatterplot(x=reg_noisy.coef_, y=mse_train_est, color='red', s=100);\nplt.title(\"Training MSE vs. coefficient\");\nplt.xlabel('w1');\nplt.ylabel('MSE');\n\nplt.subplot(1,2,2)\nsns.lineplot(x=coefs, y=mses_test)\nsns.scatterplot(x=coefs, y=mses_test, s=50);\nsns.scatterplot(x=reg_noisy.coef_, y=mse_noisy, color='red', s=100);\nplt.title(\"Test MSE vs. coefficient\");\nplt.xlabel('w1');\nplt.ylabel('MSE');","8b9f4214":"var_y = 1.0\/len(y_test) * np.sum((y_test - np.mean(y_test))**2)\nvar_y","80768f3a":"mean_y = np.mean(y_test)\nmean_y","a3897bc9":"x_line = [np.min(x_test), np.max(x_test)]\ny_line = x_line*reg_noisy.coef_ + reg_noisy.intercept_\nplt.hlines(mean_y, xmin=np.min(x_test), xmax=np.max(x_test));\nplt.vlines(x_test, ymin=mean_y, ymax=y_test, color='magenta', alpha=0.5);\nsns.scatterplot(x=x_test.squeeze(), y=y_test, color='purple', s=50);\nplt.xlabel('x');\nplt.ylabel('y');","679bac6e":"plt.vlines(x_test, ymin=y_test, ymax=y_test_hat, color='red', alpha=0.5);\nsns.scatterplot(x=x_test.squeeze(), y=y_test, color='purple', s=50);\nx_line = [np.min(x_test), np.max(x_test)]\ny_line = x_line*reg_noisy.coef_ + reg_noisy.intercept_\nsns.lineplot(x=x_line, y=y_line, color='red');\nplt.xlabel('x');\nplt.ylabel('y');","38351ac9":"\nx_line = [np.min(x_test), np.max(x_test)]\ny_line = x_line*reg_noisy.coef_ + reg_noisy.intercept_\n\nplt.hlines(mean_y, xmin=np.min(x_test), xmax=np.max(x_test));\nplt.vlines(x_test, ymin=mean_y, ymax=y_test, color='red');\nplt.vlines(x_test, ymin=y_test, ymax=y_test_hat, color='maroon');\nsns.scatterplot(x=x_test.squeeze(), y=y_test, color='purple', s=50);\nsns.lineplot(x=x_line, y=y_line, color='red');\nplt.xlabel('x');\nplt.ylabel('y');","0ea5b613":"fvu = mse_noisy\/var_y\nfvu","86de3388":"r2 = 1 - fvu\nr2","0f115bef":"# another way to do the same thing...\nmetrics.r2_score(y_test, y_test_hat)","66cf9a6c":"df = sns.load_dataset(\"anscombe\")\ndf.groupby('dataset').agg({'x': ['count','mean', 'std'], 'y': ['count','mean', 'std']})","fe0836f7":"data_i   = df[df['dataset'].eq('I')]\ndata_ii  = df[df['dataset'].eq('II')]\ndata_iii = df[df['dataset'].eq('III')]\ndata_iv  = df[df['dataset'].eq('IV')]","b928ec92":"reg_i   = LinearRegression().fit(data_i[['x']],   data_i['y'])\nreg_ii  = LinearRegression().fit(data_ii[['x']],  data_ii['y'])\nreg_iii = LinearRegression().fit(data_iii[['x']], data_iii['y'])\nreg_iv  = LinearRegression().fit(data_iv[['x']],  data_iv['y'])","c73f1a95":"print(\"Dataset I:   \",   reg_i.coef_,   reg_i.intercept_)\nprint(\"Dataset II:  \",  reg_ii.coef_,  reg_ii.intercept_)\nprint(\"Dataset III: \", reg_iii.coef_, reg_iii.intercept_)\nprint(\"Dataset IV:  \",  reg_iv.coef_,  reg_iv.intercept_)","4c219a4d":"print(\"Dataset I:   \", metrics.r2_score(data_i['y'],  reg_i.predict(data_i[['x']])))\nprint(\"Dataset II:  \", metrics.r2_score(data_ii['y'], reg_ii.predict(data_ii[['x']])))\nprint(\"Dataset III: \", metrics.r2_score(data_iii['y'],reg_iii.predict(data_iii[['x']])))\nprint(\"Dataset IV:  \", metrics.r2_score(data_iv['y'], reg_iv.predict(data_iv[['x']])))","291d0f76":"sns.lmplot(x=\"x\", y=\"y\", col=\"dataset\", hue=\"dataset\", \n           data=df, col_wrap=2, ci=None, palette=\"muted\", height=4, \n           scatter_kws={\"s\": 50, \"alpha\": 1});","eccf2bbe":"data_i   = data_i.assign(   yhat = reg_i.predict(  data_i[['x']]) )\ndata_ii  = data_ii.assign(  yhat = reg_ii.predict( data_ii[['x']]) )\ndata_iii = data_iii.assign( yhat = reg_iii.predict( data_iii[['x']]) )\ndata_iv  = data_iv.assign(  yhat = reg_iv.predict(  data_iv[['x']]) )\n\ndata_i   = data_i.assign(   residual = data_i['y'] - data_i['yhat'] )\ndata_ii  = data_ii.assign(  residual = data_ii['y'] - data_ii['yhat'] )\ndata_iii = data_iii.assign( residual = data_iii['y'] - data_iii['yhat'] )\ndata_iv  = data_iv.assign(  residual = data_iv['y'] - data_iv['yhat'] )\n\ndata_all = pd.concat([data_i, data_ii, data_iii, data_iv])\ndata_all.head()","aab2d399":"sns.lmplot(x=\"x\", y=\"residual\", col=\"dataset\", hue=\"dataset\", \n           data=data_all, col_wrap=2, ci=None, palette=\"muted\", height=4, \n           scatter_kws={\"s\": 50, \"alpha\": 1}, fit_reg=False);","fb81c856":"x_train, y_train = generate_linear_regression_data(n=n_samples, d=2, coef=[5,5], intercept=intercept)\nx_test,  y_test  = generate_linear_regression_data(n=50, d=2, coef=[5,5], intercept=intercept)\n","861ae290":"x_train.shape","77ed8a1d":"y_train.shape","1bc4709f":"plt.figure(figsize=(10,5));\nplt.subplot(1,2,1);\nplt.scatter(x_train[:,0],  y_train);\nplt.xlabel(\"x1\");\nplt.ylabel(\"y\");\nplt.subplot(1,2,2);\nplt.scatter(x_train[:,1],  y_train);\nplt.xlabel(\"x2\");\nplt.ylabel(\"y\");","76405cf2":"reg_multi = LinearRegression().fit(x_train, y_train)\nprint(\"Coefficient list: \", reg_multi.coef_)\nprint(\"Intercept: \" , reg_multi.intercept_)","35888a5c":"def plot_3D(elev=20, azim=-20, X=x_train, y=y_train):\n    plt.figure(figsize=(10,10))\n    ax = plt.subplot(projection='3d')\n\n\n    X1 = np.arange(-4, 4, 0.2)\n    X2 = np.arange(-4, 4, 0.2)\n    X1, X2 = np.meshgrid(X1, X2)\n    Z = X1*reg_multi.coef_[0] + X2*reg_multi.coef_[1]\n\n    # Plot the surface.\n    ax.plot_surface(X1, X2, Z, alpha=0.1, color='gray',\n                          linewidth=0, antialiased=False)\n    ax.scatter3D(X[:, 0], X[:, 1], y, s=50)\n\n    ax.view_init(elev=elev, azim=azim)\n    ax.set_xlabel('x1')\n    ax.set_ylabel('x2')\n    ax.set_zlabel('y')\n\ninteract(plot_3D, elev=np.arange(-90,90,10), azim=np.arange(-90,90,10),\n         X=fixed(x_train), y=fixed(y_train));","2636300c":"coefs = np.arange(3.0, 7.0, 0.05)\nmses_train = np.zeros((len(coefs), len(coefs)))\n\nfor idx_1, c_1 in enumerate(coefs):\n  for idx_2, c_2 in enumerate(coefs):\n    y_train_hat_c = (reg_multi.intercept_ + np.dot(x_train,[c_1, c_2])).squeeze()\n    mses_train[idx_1,idx_2] =  1.0\/(len(y_train_hat_c)) * np.sum((y_train - y_train_hat_c)**2)","e6829986":"plt.figure(figsize=(5,5));\nX1, X2 = np.meshgrid(coefs, coefs)\np = plt.scatter(x=reg_multi.coef_[1], y=reg_multi.coef_[0], c='red')\np = plt.contour(X1, X2, mses_train, levels=5);\nplt.clabel(p, inline=1, fontsize=10);\nplt.xlabel('w2');\nplt.ylabel('w1');","34033620":"coefs = np.arange(3.0, 7.0, 0.05)\nmses_test = np.zeros((len(coefs), len(coefs)))\n\nfor idx_1, c_1 in enumerate(coefs):\n  for idx_2, c_2 in enumerate(coefs):\n    y_test_hat_c = (reg_multi.intercept_ + np.dot(x_test,[c_1, c_2])).squeeze()\n    mses_test[idx_1,idx_2] =  1.0\/(len(y_test_hat_c)) * np.sum((y_test - y_test_hat_c)**2)","37a92cde":"plt.figure(figsize=(5,5));\nX1, X2 = np.meshgrid(coefs, coefs)\np = plt.scatter(x=reg_multi.coef_[1], y=reg_multi.coef_[0], c='red')\np = plt.contour(X1, X2, mses_test, levels=5);\nplt.clabel(p, inline=1, fontsize=10);\nplt.xlabel('w2');\nplt.ylabel('w1');","7231442b":"x_train, y_train = generate_linear_regression_data(n=n_samples, d=2, coef=[5,5], intercept=intercept, sigma=5)\nx_test,  y_test  = generate_linear_regression_data(n=50, d=2, coef=[5,5], intercept=intercept, sigma=5)","ffe55ed6":"plt.figure(figsize=(10,5));\nplt.subplot(1,2,1);\nplt.scatter(x_train[:,0],  y_train);\nplt.xlabel(\"x1\");\nplt.ylabel(\"y\");\nplt.subplot(1,2,2);\nplt.scatter(x_train[:,1],  y_train);\nplt.xlabel(\"x2\");\nplt.ylabel(\"y\");","0df0a55f":"reg_multi_noisy = LinearRegression().fit(x_train, y_train)\nprint(\"Coefficient list: \", reg_multi_noisy.coef_)\nprint(\"Intercept: \" , reg_multi_noisy.intercept_)","5d593527":"def plot_3D(elev=20, azim=-20, X=x_train, y=y_train):\n    plt.figure(figsize=(10,10))\n    ax = plt.subplot(projection='3d')\n\n\n    X1 = np.arange(-4, 4, 0.2)\n    X2 = np.arange(-4, 4, 0.2)\n    X1, X2 = np.meshgrid(X1, X2)\n    Z = X1*reg_multi_noisy.coef_[0] + X2*reg_multi_noisy.coef_[1]\n\n    # Plot the surface.\n    ax.plot_surface(X1, X2, Z, alpha=0.1, color='gray',\n                          linewidth=0, antialiased=False)\n    ax.scatter3D(X[:, 0], X[:, 1], y, s=50)\n\n    ax.view_init(elev=elev, azim=azim)\n    ax.set_xlabel('x1')\n    ax.set_ylabel('x2')\n    ax.set_zlabel('y')\n\ninteract(plot_3D, elev=np.arange(-90,90,10), azim=np.arange(-90,90,10),\n         X=fixed(x_train), y=fixed(y_train));","d52aec78":"coefs = np.arange(3.0, 7.0, 0.05)\nmses_train = np.zeros((len(coefs), len(coefs)))\n\nfor idx_1, c_1 in enumerate(coefs):\n  for idx_2, c_2 in enumerate(coefs):\n    y_train_hat_c = (reg_multi_noisy.intercept_ + np.dot(x_train,[c_1, c_2])).squeeze()\n    mses_train[idx_1,idx_2] =  1.0\/(len(y_train_hat_c)) * np.sum((y_train - y_train_hat_c)**2)","8d651629":"plt.figure(figsize=(5,5));\nX1, X2 = np.meshgrid(coefs, coefs)\np = plt.scatter(x=reg_multi_noisy.coef_[1], y=reg_multi_noisy.coef_[0], c='red')\np = plt.contour(X1, X2, mses_train, levels=5);\nplt.clabel(p, inline=1, fontsize=10);\nplt.xlabel('w2');\nplt.ylabel('w1');","5595afef":"coefs = np.arange(3.0, 7.0, 0.05)\nmses_test = np.zeros((len(coefs), len(coefs)))\n\nfor idx_1, c_1 in enumerate(coefs):\n  for idx_2, c_2 in enumerate(coefs):\n    y_test_hat_c = (reg_multi_noisy.intercept_ + np.dot(x_test,[c_1, c_2])).squeeze()\n    mses_test[idx_1,idx_2] =  1.0\/(len(y_test_hat_c)) * np.sum((y_test - y_test_hat_c)**2)","3b539062":"plt.figure(figsize=(5,5));\nX1, X2 = np.meshgrid(coefs, coefs)\np = plt.scatter(x=reg_multi_noisy.coef_[1], y=reg_multi_noisy.coef_[0], c='red')\np = plt.contour(X1, X2, mses_test, levels=5);\nplt.clabel(p, inline=1, fontsize=10);\nplt.xlabel('w2');\nplt.ylabel('w1');","8c6186ce":"import itertools\n\ndef generate_linear_basis_data(n=100, d=2, coef=[1,1,0.5,0.5,1], intercept=1, sigma=0):\n  x = np.random.randn(n,d)\n  x = np.column_stack((x, x**2 ))\n  for pair in list(itertools.combinations(range(d), 2)):\n    x = np.column_stack((x, x[:,pair[0]]*x[:,pair[1]]))\n  y = (np.dot(x, coef) + intercept).squeeze() + sigma * np.random.randn(n)\n  return x[:,:d], y","afd0f1b0":"x_train, y_train = generate_linear_basis_data(sigma=0.2)\nx_test,  y_test  = generate_linear_basis_data(n=50, sigma=0.2)","0c31a820":"print(x_train.shape)\nprint(y_train.shape)","a58309f2":"plt.figure(figsize=(10,5));\nplt.subplot(1,2,1);\nplt.scatter(x_train[:,0],  y_train);\nplt.xlabel(\"x1\");\nplt.ylabel(\"y\");\nplt.subplot(1,2,2);\nplt.scatter(x_train[:,1],  y_train);\nplt.xlabel(\"x2\");\nplt.ylabel(\"y\");","0ac8ba49":"reg_lbf = LinearRegression().fit(x_train, y_train)\nprint(\"Intercept: \" , reg_lbf.intercept_)\nprint(\"Coefficient list: \", reg_lbf.coef_)","9f30d64e":"y_train_hat = reg_lbf.predict(x_train)\nprint(\"Training MSE: \", metrics.mean_squared_error(y_train, y_train_hat))\nprint(\"Training R2:  \", metrics.r2_score(y_train, y_train_hat))","8d6018eb":"residual_train = y_train - y_train_hat","aa9dea01":"_ = sns.scatterplot(x=y_train, y=y_train_hat)\n_ = plt.xlabel('y')\n_ = plt.ylabel('y_hat')","b4cd80b5":"_ = sns.scatterplot(x=y_train_hat, y=residual_train)\n_ = plt.xlabel('y_hat')\n_ = plt.ylabel('Residual')","b8d326de":"x_train_trans = np.column_stack((x_train, x_train**2))\n\nreg_lbf_trans = LinearRegression().fit(x_train_trans, y_train)\nprint(\"Intercept: \" , reg_lbf_trans.intercept_)\nprint(\"Coefficient list: \", reg_lbf_trans.coef_)\n\ny_train_trans_hat = reg_lbf_trans.predict(x_train_trans)\nprint(\"Training MSE: \", metrics.mean_squared_error(y_train, y_train_trans_hat))\nprint(\"Training R2:  \", metrics.r2_score(y_train, y_train_trans_hat))\n\nresidual_train_trans = y_train - y_train_trans_hat","714f36e5":"_ = sns.scatterplot(x=y_train, y=y_train_trans_hat)\n_ = plt.xlabel('y')\n_ = plt.ylabel('y_hat')","614ca1b8":"_ = sns.scatterplot(x=y_train, y=residual_train_trans)\n_ = plt.xlabel('y')\n_ = plt.ylabel('Residual')","92edba5e":"plt.figure(figsize=(10,5));\nplt.subplot(1,2,1);\nplt.scatter(x_train[:,0],  residual_train_trans);\nplt.xlabel(\"x1\");\nplt.ylabel(\"Residual\");\nplt.subplot(1,2,2);\nplt.scatter(x_train[:,1],  residual_train_trans);\nplt.xlabel(\"x2\");\nplt.ylabel(\"Residual\");","a9ac545c":"x_train_inter = np.column_stack((x_train_trans, x_train[:,0]*x_train[:,1]))\n\nreg_lbf_inter = LinearRegression().fit(x_train_inter, y_train)\nprint(\"Intercept: \" , reg_lbf_inter.intercept_)\nprint(\"Coefficient list: \", reg_lbf_inter.coef_)\n\ny_train_inter_hat = reg_lbf_inter.predict(x_train_inter)\nprint(\"Training MSE: \", metrics.mean_squared_error(y_train, y_train_inter_hat))\nprint(\"Training R2:  \", metrics.r2_score(y_train, y_train_inter_hat))\n\nresidual_train_inter = y_train - y_train_inter_hat","dbff82e1":"_ = sns.scatterplot(x=y_train, y=y_train_inter_hat)\n_ = plt.xlabel('y')\n_ = plt.ylabel('y_hat')","10a1b353":"_ = sns.scatterplot(x=y_train, y=residual_train_inter)\n_ = plt.xlabel('y')\n_ = plt.ylabel('Residual')","2d85f293":"plt.figure(figsize=(10,5));\nplt.subplot(1,2,1);\nplt.scatter(x_train[:,0],  residual_train_inter);\nplt.xlabel(\"x1\");\nplt.ylabel(\"Residual\");\nplt.subplot(1,2,2);\nplt.scatter(x_train[:,1],  residual_train_inter);\nplt.xlabel(\"x2\");\nplt.ylabel(\"Residual\");","4b6de87b":"y_train_hat = reg_lbf_inter.predict(x_train_inter)\nprint(\"Training MSE: \", metrics.mean_squared_error(y_train, y_train_hat))\nprint(\"Training R2:  \", metrics.r2_score(y_train, y_train_hat))","dd9a9412":"x_test_inter = np.column_stack((x_test, x_test**2))\nx_test_inter = np.column_stack((x_test_inter, x_test[:,0]*x_test[:,1]))\n\ny_test_hat = reg_lbf_inter.predict(x_test_inter)\nprint(\"Test MSE: \", metrics.mean_squared_error(y_test, y_test_hat))\nprint(\"Test R2:  \", metrics.r2_score(y_test, y_test_hat))","e9c844e5":"The \u201ccorrect\u201d coefficients had slightly higher MSE on the training set\nthan the fitted coefficients. We fit parameters so that they are optimal\non the *training* set, then we use the test set to understand how the\nmodel will generalize to new, unseen data.","f820dc7b":"### Variance, explained variance, R2","00a0b0df":"**Fraction of variance unexplained** is the ratio of the sum of squared\ndistances from data to the regression line (sum of squared vertical\ndistances in second plot), to the sum of squared distanced from data to\nthe mean (sum of squared vertical distances in first plot):\n\n$$\\frac{MSE}{Var(y)} = \\frac{Var(y-\\hat{y})}{Var(y)} = \\frac{\\sum_{i=1}^n(y_i-\\hat y_i)^2}{\\sum_{i=1}^n(y_i - \\bar{y})^2}$$","fa76bec9":"The variance of $y$ is the mean sum of the squares of the distances from\neach $y_i$ to $\\bar{y}$. These distances are illustrated here:\n\n-   the horizontal line shows $\\bar{y}$\n-   each vertical line is a distance from a $y_i$ to $\\bar{y}$","62e7a772":"### Compute MSE","95c0ab9b":"Quick reminder:\n\nMean of $x$ and $y$:\n\n$$\\bar{x} = \\frac{1}{n} \\sum_{i=1}^n x_i, \\quad \\bar{y} = \\frac{1}{n} \\sum_{i=1}^n y_i$$\n\nSample variance of $x$ and $y$:\n\n$$\\sigma_x^2 = \\frac{1}{n} \\sum_{i=1}^n (x_i - \\bar{x}) ^2, \\quad \\sigma_y^2 = \\frac{1}{n} \\sum_{i=1}^n (y_i - \\bar{y}) ^2$$\n\nSample covariance of $x$ and $y$:\n\n$$\\sigma_{xy} = \\frac{1}{n} \\sum_{i=1}^n (x_i - \\bar{x})(y_i - \\bar{y})$$","91b1dffe":"The assumptions of the linear model (that the target variable can be\npredicted as a linear combination of the features) can be restrictive.\nWe can capture more complicated relationships using linear basis\nfunction regression.","757d1648":"Simple linear regression with noise\n-----------------------------------","74e9ed6c":"### Generate some data","2bf2797a":"These two plots together show how well the variance of $y$ is\n\u201cexplained\u201d by the linear regression model:\n\n-   The total variance of $y$ is shown in the first plot, where each\n    vertical line is $y_i - \\bar{y}$\n-   The *unexplained* variance of $y$ is shown in the second plot, where\n    each vertical line is the error of the model, $y_i - \\hat{y}_i$\n\nIn this example, *all* of the variance of $y$ is \u201cexplained\u201d by the\nlinear regression.","d6135aa5":"### Fit a linear regression","e8d5c1e5":"Multiple linear regression\n--------------------------","d7d29746":"### Generate some data","39b6c380":"### Generate some data","5977630c":"We saw that part of the MSE is due to noise in the data, and part is due\nto error in the parameter estimates.\n\nSoon - we will formalize this discussion of different sources of error:\n\n-   Error in parameter estimates\n-   \u201cNoise\u201d - any variation in data that is not a function of the $X$\n    that we use as input to the model\n-   Other error - model (hypothesis class) not a good choice for the\n    data, for example","92e90c11":"Recall that there is no stochastic noise in this data - so it fits a\nlinear model perfectly. But it\u2019s more difficult to see that linear\nrelationship in higher dimensions.","5e9a3e66":"### Variance, explained variance, R2","debc5657":"In the plot on the left (for training MSE), the red dot (our coefficient\nestimate) should always have minimum MSE, because we select parameters\nto minimize MSE on the training set.\n\nIn the plot on the right (for test MSE), the red dot might not have the\nminimum MSE, because the best coefficient on the training set might not\nbe the best coefficient on the test set. This gives us some idea of how\nour model will generalize to new, unseen data. We may suspect that if\nthe coefficient estimate is not perfect for *this* test data, it might\nhave some error on other new, unseen data, too.\n\nIf you re-run this notebook many times, you\u2019ll get a new random sample\nof training and test data each time. Sometimes, the \u201ctrue\u201d coefficients\nmay have smaller MSE on the test set than the estimated coefficients. On\nother runs, the estimated coefficients might have smaller MSE on the\ntest set.","efd49994":"### Visualize MSE for different coefficients","e7036761":"### MSE contour","2b9c0714":"### Generate some data","a768e463":"Note: in this example, we use a \u201cstochastic error\u201d term. This is not to\nbe confused with a residual term which can include systematic,\nnon-random error.\n\n-   stochastic error: difference between observed value and \u201ctrue\u201d\n    value. These random errors are independent, not systematic, and\n    cannot be \u201clearned\u201d by any machine learning model.\n-   residual: difference between observed value and estimated value.\n    These errors are typical *not* independent, and they can be\n    systematic.","852e71cb":"### Plot hyperplane","edd3cc07":"Is the error random? Or does it look systematic?","f67409c2":"Simple linear regression\n------------------------","24aca612":"### Compute MSE","2df19d44":"### Fit a linear regression","97b27bc6":"Does the linear model fit well?\n\n-   the linear model is a good fit for Dataset I\n-   Dataset II is clearly non-linear\n-   Dataset III has an outlier\n-   Dataset IV has a high leverage point","552a6a77":"Now let\u2019s look at a similar kind of plot, but with distances to the\nregression line instead of the to mean line:\n\n-   In the previous plot, each vertical line was a $y_i - \\bar{y}$\n-   In the following plot, each vertical line is a $y_i - \\hat{y}_i$\n\n(where $\\hat{y}_i$ is the prediction of the linear regression for a\ngiven sample $i$)","996f0e6c":"### Residual analysis","b94681aa":"Since there is clearly some non-linearity, we can try to fit a model to\na non-linear transformation of the features.","137850d0":"MSE for this example is 0, R2 is 1.","db588499":"Note that now the data is mean removed - zero mean in every dimension.\n(Removing the mean is also called *centering* the data.)\n\nThis time, the fitted linear regression has 0 intercept:","570eac12":"What does a negative R2 mean, in terms of a comparison to \u201cprediction by\nmean\u201d?","8e807bbe":"### Fit a linear regression","05214e7b":"### Fit a linear regression","e25d2168":"and some default values we\u2019ll use:","4374d2af":"<div class=\"alert alert-block alert-info\">  \n    <h1><strong>\ud83d\udc68\u200d\ud83d\udcbb Getting Started with Linear regression in depth<\/strong><\/h1>\n    <i><\/i>\n<\/div>","7dd2cca2":"To evaluate the model, we will compute the MSE on the test data (not the\ndata used to find the parameters).\n\n$$MSE = \\frac{1}{n} \\sum_{i=1}^n (y_i - (w_0 + w_1 x_i)) ^2$$\n\nUse $\\hat{y}_i = w_0 + w_1 x_i$, then\n\n$$MSE = \\frac{1}{n} \\sum_{i=1}^n (y_i - \\hat{y}_i) ^2$$","2ef7b176":"### Predict some new points","ca908a68":"# <center><img src=\"https:\/\/www.gatevidyalay.com\/wp-content\/uploads\/2020\/01\/Representing-Linear-Regression-Model.png\"><\/center>","6656c5e6":"### Evaluate on test set","981c77f6":"Here\u2019s a function to generate this kind of data","ae81521c":"### MSE contour","ad6015b8":"Easy to identify problems in 1D - what about in higher D?\n\n-   Plot $\\hat{y}$ against $y$\n-   Plot residuals against $\\hat{y}$\n-   Plot residuals against each $x$ (including any $x$ not in the model)\n-   Plot residuals against time (for time series data)\n\nWhat should each of these plots look like if the regression is \u201cgood\u201d?","59eb7a65":"### Visualize MSE for different coefficients","7c5edc0e":"Alternative interpretation: imagine we would develop a very simple ML\nmodel, in which we always predict $\\hat{y}_i = \\bar{y}_i$. Then, we use\nthis model as a basis for comparison for other, more sophisticated\nmodels. The ratio above is the ratio of error of the regression model,\nto the error of a \u201cprediction by mean\u201d model.\n\n-   If this quantity is less than 1, our model is better than\n    \u201cprediction by mean\u201d\n-   If this quantity is greater than 1, our model is worse than\n    \u201cprediction by mean\u201d","252454e7":"The MSE is higher than before!\n\nDoes this mean our estimate of $w_0$ and $w_1$ is not optimal?\n\nSince we generated the data, we know the \u201ctrue\u201d coefficient value and we\ncan see how much the MSE would be with the true coefficient values.","abbe77c8":"Suppose we have a process that generates data as\n\n$$y_i = w_0 + w_1 x_{i,1} + \\ldots + w_d x_{i,d} + \\epsilon_i $$\n\nwhere $\\epsilon_i \\sim N(0, \\sigma^2)$.","0eb072a9":"Linear basis function regression\n--------------------------------","2e945b56":"### The mean-removed equivalent\n\nQuick digression - what if we don\u2019t want to bother with intercept?","d04a1b5b":"### Predict some new points\n\nOK, now we can predict some new points:","f8041211":"Data generated by a linear function\n-----------------------------------","b65f14f7":"Multiple linear regression with noise\n-------------------------------------","331cd285":"### Fit a linear regression","bd819e6c":"Residual analysis\n-----------------","117b13e9":"Important: I thought we selected the coefficients that minimize MSE! But\nsometimes our linear regression doesn\u2019t select the \u201ctrue\u201d coefficients?","424765a3":"### Plot hyperplane","1b3d1231":"### Generate some data","dac3ed5b":"\n# <center> I hope you Like my work and Efforts \ud83d\ude0d <\/center>","a7589150":"### Compute MSE and R2","67139906":"Remember:\n\n-   The total variance of $y$ is shown in the first plot, where each\n    vertical line is $y_i - \\bar{y}$\n-   The *unexplained* variance of $y$ is shown in the second plot, where\n    each vertical line is the error of the model, $y_i - \\hat{y}_i$\n\nIn the next plot, we\u2019ll combine them to get some intuition regarding the\n*fraction of unexplained variance*. The dark maroon part of each\nvertical bar is the *unexplained* part, while the red part is\n*explained* by the linear regression."}}