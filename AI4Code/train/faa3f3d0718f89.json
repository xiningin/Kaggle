{"cell_type":{"0b09957f":"code","22f7e795":"code","ff7818b5":"code","ea8dff62":"code","8dc795e5":"code","f46d9a3f":"code","bc45894a":"code","ab145e32":"code","9e06e83a":"code","c50a7587":"code","e27e6810":"code","c9bd7c34":"code","f6146da3":"code","46dc2ed1":"code","275c2e95":"code","006db601":"code","f6ffe652":"code","ca31a61a":"code","aa559b57":"code","994c032c":"code","9d67a19d":"code","37fad8c6":"code","011d8313":"code","50217a23":"code","ca8bf792":"markdown","6fb13c14":"markdown","313a2beb":"markdown","c78c0112":"markdown","92baad78":"markdown","4fd86311":"markdown","5c4505ce":"markdown","d8f599d2":"markdown","d9b07ce7":"markdown","3c87e626":"markdown","d374bf81":"markdown","81e4d6c1":"markdown","b5d09e21":"markdown","1c849958":"markdown","b85dbf6b":"markdown","0548420e":"markdown","e86a97aa":"markdown","af13467f":"markdown","e5d6c9d6":"markdown","f341f6f2":"markdown","1cc87416":"markdown","037778f9":"markdown","c2a816f3":"markdown","8866e393":"markdown","549ce1fc":"markdown","a09d20fc":"markdown"},"source":{"0b09957f":"from tensorflow import keras","22f7e795":"mnist = keras.datasets.mnist\n(X_train_full, y_train_full), (X_test, y_test) = mnist.load_data()","ff7818b5":"X_train_full.shape","ea8dff62":"X_test.shape","8dc795e5":"X_valid, X_train = X_train_full[:5000] \/ 255.0, X_train_full[5000:] \/ 255.0\ny_valid, y_train = y_train_full[:5000], y_train_full[5000:]","f46d9a3f":"model = keras.models.Sequential()\nmodel.add(keras.layers.Flatten(input_shape=[28, 28]))\nmodel.add(keras.layers.Dense(500, activation=\"relu\"))\nmodel.add(keras.layers.Dense(10, activation=\"softmax\"))","bc45894a":"model.summary()","ab145e32":"model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"sgd\", metrics=[\"accuracy\"])","9e06e83a":"history = model.fit(X_train, y_train, epochs=10, validation_data=(X_valid, y_valid))","c50a7587":"model.evaluate(X_test, y_test)","e27e6810":"mnist = keras.datasets.mnist\n(X_train_full, y_train_full), (X_test, y_test) = mnist.load_data()","c9bd7c34":"X_valid, X_train = X_train_full[:5000] \/ 255.0, X_train_full[5000:] \/ 255.0\ny_valid, y_train = y_train_full[:5000], y_train_full[5000:]","f6146da3":"model = keras.models.Sequential()\nmodel.add(keras.layers.Flatten(input_shape=[28, 28]))\nmodel.add(keras.layers.Dense(200, activation=\"relu\"))\nmodel.add(keras.layers.Dense(200, activation=\"relu\"))\nmodel.add(keras.layers.Dense(200, activation=\"relu\"))\nmodel.add(keras.layers.Dense(10, activation=\"softmax\"))","46dc2ed1":"model.summary()","275c2e95":"model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"sgd\", metrics=[\"accuracy\"])","006db601":"history = model.fit(X_train, y_train, epochs=10, validation_data=(X_valid, y_valid))","f6ffe652":"model.evaluate(X_test, y_test)","ca31a61a":"mnist = keras.datasets.mnist\n(X_train_full, y_train_full), (X_test, y_test) = mnist.load_data()","aa559b57":"X_train_full = X_train_full.reshape(60000, 784) \/ 255.0\nX_test = X_test.reshape(10000, 784) \/ 255.0","994c032c":"input_ = keras.layers.Input(shape=(784,))\nhidden1 = keras.layers.Dense(200, activation=\"relu\")(input_)\nhidden2 = keras.layers.Dense(200, activation=\"relu\")(hidden1)\nhidden3 = keras.layers.Dense(200, activation=\"relu\")(hidden2)\nconcat = keras.layers.Concatenate()([input_, hidden3])\noutput = keras.layers.Dense(10, activation=\"softmax\")(concat)\nmodel = keras.Model(inputs=[input_], outputs=[output])","9d67a19d":"model.summary()","37fad8c6":"model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"sgd\", metrics=[\"accuracy\"])","011d8313":"history = model.fit(X_train_full, y_train_full, epochs=10, validation_split=0.1)","50217a23":"model.evaluate(X_test, y_test)","ca8bf792":"Now we will compile the model and train it on the training set. We will use only 10 epochs here due to limitations but feel free to increase it.","6fb13c14":"Reference - Hands-On Machine Learning with Scikit-Learn, Keras and Tensorflow <br>By Aur\u00e9lien G\u00e9ron","313a2beb":"We got to 95.2% test accuracy with our wide model!","c78c0112":"Now we will create a model with 3 hidden layers with 200 neurons per layer.","92baad78":"### 4.2. Implementation","4fd86311":"Breaking training set into validation sets and normalizing all features from 0 to 255 range into 0 to 1","5c4505ce":"## 1. Overview\nWhen beginning with neural networks and deep learning, few concepts can get tricky. Neural networks comes with alot of parameters to choose from which includes choosing the number of hidden layers and neurons per layer. These parameters is what differentiates the architecture of the network and also contributes to how your model will perform. Here, we will look at basic wide, deep and later combination of both called wide & deep neural networks using Tensorflow Keras.","d8f599d2":"# Wide vs Deep vs Wide & Deep Neural Networks","d9b07ce7":"## 3. Deep Neural Networks\n\n### 3.1. Introduction\nDeep neural networks represent a network with more number of hidden layers(more than 1-2).\n\n<img src=\"http:\/\/neuralnetworksanddeeplearning.com\/images\/tikz36.png\" \/>\n\nImage source - (<a href=\"http:\/\/neuralnetworksanddeeplearning.com\/chap5.html\">here<\/a>)\n\nThese types of networks can be useful in complex datasets and problems where finding high accuracy with small models is difficult. These are typically used in computer vision and NLP problems. Let's see how to make these networks using Tensorflow Keras Sequential API.","3c87e626":"input_shape is [28,28] to tell Keras in advance what to expect as input. Activation function used for hideen layer is RelU and for output layer, it is Softmax.\n\nLet's look at the model summary.","d374bf81":"## 5. Conclusion\nDifferent structures can output different results and it depends on the dataset and problem at hand while making the decision. Also, its useful to keep experimenting as to see for yourself what works best.","81e4d6c1":"Here comes the main difference, the building of model structure using Functional API.","b5d09e21":"Here, the input is passed to hidden layer 1, which is passed to hidden layer 2. Now, the output of hidden layer 2 is passed to hidden layer 3. Now, the 3rd hidden layer is passed to a special concatination layer which combines hidden layer 3 with the initial input layer. Finally, this is processed on to output layer to make predictions.\n\nLet's look at the model summary.","1c849958":"Now we will compile the model and train it on the training set. We will use only 10 epochs here due to limitations but feel free to increase it.","b85dbf6b":"### 2.2. Implementation\nLet's start by importing Tensorflow Keras. We will be using the MNIST dataset throughout this notebook. MNIST is a very famous dataset of handwritten digit images which contains 70,000 samples. Each image has 28x28 pixels.","0548420e":"input_shape is [28,28] to tell Keras in advance what to expect as input. Activation function used for hideen layer is RelU and for output layer, it is Softmax.\n\nLet's look at the model summary.","e86a97aa":"Let's evaluate the model.","af13467f":"<b>Please upvote if you found this notebook useful!<\/b>","e5d6c9d6":"We got to 96.7% test accuracy with our wide & deep model!","f341f6f2":"### 2.2. Implementation\nThe code used will be similar to the previous section except model creation, where we will use more hidden layers.","1cc87416":"## 2. Wide Neural Networks\n\n### 2.1. Introduction\nWide neural networks represent a network with less number of hidden layers(usually 1-2) but more number of neurons per layer. \n\n<img src=\"http:\/\/neuralnetworksanddeeplearning.com\/images\/tikz35.png\" \/>\n\nImage source - (<a href=\"http:\/\/neuralnetworksanddeeplearning.com\/chap5.html\">here<\/a>)\n\nThese types of networks can be useful when you have less data and your problem isn't too complex. Single hidden layer with a lot of neurons can detect simple patterns(simple classification and regression problems) in the dataset but will fail when you start expecting it to detect complex relations (Image detection, Speech recognition, etc.). Let's see how to make these networks using Tensorflow Keras Sequential API.","037778f9":"## 4. Wide & Deep Neural Networks\n\n### 4.1. Introduction\nKeras allows the developers to experiment with model structures. It can be done using the Functional APIs provided by Keras. Wide & Deep neural networks are a combination of both types of networks. This makes it possible for the model to learn from deep as well as wide patterns.\n\n<img src=\"https:\/\/www.researchgate.net\/profile\/Kaveh_Bastani\/publication\/328161216\/figure\/fig3\/AS:679665219928064@1539056224036\/Illustration-of-the-wide-and-deep-model-which-is-an-integration-of-wide-component-and.ppm\" \/>\n\nImage source - (<a href=\"https:\/\/www.researchgate.net\/figure\/Illustration-of-the-wide-and-deep-model-which-is-an-integration-of-wide-component-and_fig3_328161216\">here<\/a>)\n\nThe wide part of the network directly connects to the output layer while the deep path runs as usual. This makes the network learn a variety of patterns which are not possible in simple structures. Let's see how to make these networks using Tensorflow Keras Functional API.","c2a816f3":"We got to 96.7% test accuracy with our wide model which is an improvement to the wide model.","8866e393":"Let's evaluate the model.","549ce1fc":"Let's evaluate the model.","a09d20fc":"Now we will create a model with one hidden layer with alot of neurons(let's take 500). Our input layer has 784 neurons so its fair to take 500 neurons in our hidden layer."}}