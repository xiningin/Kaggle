{"cell_type":{"80b33c22":"code","a0567b9f":"code","66154085":"code","8a661198":"code","f813f973":"code","a17079fe":"code","361a59ee":"code","6e832c2d":"code","8e04e78f":"code","e58bab3e":"code","fad2da60":"code","b07c469f":"code","8d88972b":"code","98b89b1c":"code","436ea332":"code","5ba12913":"code","383c2a9b":"code","6f799cea":"code","80c3e89f":"code","e0378020":"code","9b45f086":"code","f8db4581":"code","6eb469a6":"code","5535e3c8":"code","b20ca0ea":"code","3b0929b5":"code","1c15904a":"code","468feabd":"code","655c6fdf":"code","f94e11f7":"code","a048d019":"code","5a99a26b":"code","d2377fe6":"code","f894b9a3":"code","5c7edad6":"code","461c7698":"markdown","b7c23293":"markdown","79905346":"markdown","d6eab09f":"markdown","5bc1cbea":"markdown","a2c5fc93":"markdown","f4a8b129":"markdown","d9868c92":"markdown","6fcc5dad":"markdown","555ac706":"markdown","aa24bd4c":"markdown","a30b241a":"markdown","98bd9c8a":"markdown","fb2b9637":"markdown","c8d87110":"markdown","8eede514":"markdown","726af559":"markdown","a48189ac":"markdown","c5edb025":"markdown","8222fd7a":"markdown","91651673":"markdown","1da68ffe":"markdown","ee2ce961":"markdown","744874b8":"markdown","ded44202":"markdown","f99c1f72":"markdown"},"source":{"80b33c22":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n\n","a0567b9f":"\ntrain = pd.read_csv(\"..\/input\/train.csv\")\ntest = pd.read_csv(\"..\/input\/test.csv\")","66154085":"print(train.shape)\nprint(test.shape)","8a661198":"train.head(5)","f813f973":"test.head(5)","a17079fe":"X_train=train.drop(labels = [\"label\"],axis = 1) \nY_train=train['label']\nprint(X_train.shape)\nprint(Y_train.shape)","361a59ee":"Y_train.value_counts()","6e832c2d":"import seaborn as sns\nplt.figure(figsize=(8,4))\nsns.countplot(x='label', data=train);","8e04e78f":"X_train=X_train.astype('float32')\/255\ntest=test.astype('float32')\/255","e58bab3e":"X_train = X_train.values.reshape(-1,28,28,1)\ntest = test.values.reshape(-1,28,28,1)","fad2da60":"X_train.shape","b07c469f":"test.shape","8d88972b":"from keras.utils.np_utils import to_categorical\nY_train = to_categorical(Y_train, num_classes = 10)","98b89b1c":"Y_train.shape","436ea332":"print(Y_train[:5])","5ba12913":"from sklearn.model_selection import train_test_split\nX_train, X_valid, Y_train, Y_valid = train_test_split(X_train, Y_train, test_size = 0.1, random_state=42)","383c2a9b":"plt.figure(figsize=(6,6))\nplt.imshow(X_train[1][:,:,0])\nplt.title(Y_train[1].argmax());","6f799cea":"from keras.layers import Input,InputLayer, Dense, Activation, ZeroPadding2D, BatchNormalization, Flatten, Conv2D\nfrom keras.layers import AveragePooling2D, MaxPooling2D, Dropout\nfrom keras.models import Sequential,Model\nfrom keras.optimizers import SGD\nfrom keras.callbacks import ModelCheckpoint,LearningRateScheduler\nimport keras\nfrom keras import backend as K","80c3e89f":"inputShape=(28,28,1)\ninput = Input(inputShape)\n\nx = Conv2D(64,(3,3),strides = (1,1),name='layer_conv1',padding='same')(input)\nx = BatchNormalization()(x)\nx = Activation('relu')(x)\nx = MaxPooling2D((2,2),name='maxPool1')(x)\n\n\n\nx = Conv2D(64,(3,3),strides = (1,1),name='layer_conv2',padding='same')(x)\nx = BatchNormalization()(x)\nx = Activation('relu')(x)\nx = MaxPooling2D((2,2),name='maxPool2')(x)\n\nx = Conv2D(32,(3,3),strides = (1,1),name='conv3',padding='same')(x)\nx = BatchNormalization()(x)\nx = Activation('relu')(x)\nx = MaxPooling2D((2,2),name='maxPool3')(x)\n\n\nx = Flatten()(x)\nx = Dense(64,activation = 'relu',name='fc0')(x)\nx = Dropout(0.25)(x)\nx = Dense(32,activation = 'relu',name='fc1')(x)\nx = Dropout(0.25)(x)\nx = Dense(10,activation = 'softmax',name='fc2')(x)\n\nmodel = Model(inputs = input,outputs = x,name='Predict')\n","e0378020":"model.summary()","9b45f086":"# define SGD optimizer\nmomentum = 0.5\nsgd = SGD(lr=0.01, momentum=momentum, decay=0.0, nesterov=False) \n\n# compile the model\nmodel.compile(loss='categorical_crossentropy',optimizer=sgd, metrics=['accuracy'])","f8db4581":"import math\ndef step_decay(epoch):\n    \n    \n    initial_lrate=0.1\n    drop=0.6\n    epochs_drop = 3.0\n    lrate= initial_lrate * math.pow(drop,  \n           math.floor((1+epoch)\/epochs_drop))\n    return lrate\n   \n\nlrate = LearningRateScheduler(step_decay)\ncallbacks_list = [ lrate]\n","6eb469a6":"history=model.fit(X_train, Y_train, validation_data=(X_valid, Y_valid),\n                          epochs=35,callbacks=callbacks_list,verbose=1)","5535e3c8":"import matplotlib.pyplot as plt\nloss = history.history['loss']\nval_loss = history.history['val_loss']\nepochs = range(1, len(loss) + 1)\nplt.plot(epochs, loss, color='red', label='Training loss')\nplt.plot(epochs, val_loss, color='green', label='Validation loss')\nplt.title('Training and validation loss')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\nplt.show()","b20ca0ea":"acc = history.history['acc']\nval_acc = history.history['val_acc']\nplt.plot(epochs, acc, color='red', label='Training acc')\nplt.plot(epochs, val_acc, color='green', label='Validation acc')\nplt.title('Training and validation accuracy')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\nplt.show()","3b0929b5":"print(\"on valid data\")\npred1=model.evaluate(X_valid,Y_valid)\nprint(\"accuaracy\", str(pred1[1]*100))\nprint(\"Total loss\",str(pred1[0]*100))","1c15904a":"from keras.models import Model\nlayer_outputs = [layer.output for layer in model.layers]\nactivation_model = Model(inputs=model.input, outputs=layer_outputs)\nactivations = activation_model.predict(X_train[10].reshape(1,28,28,1))\n \ndef display_activation(activations, col_size, row_size, act_index): \n    activation = activations[act_index]\n    activation_index=0\n    fig, ax = plt.subplots(row_size, col_size, figsize=(row_size*2.5,col_size*1.5))\n    for row in range(0,row_size):\n        for col in range(0,col_size):\n            ax[row][col].imshow(activation[0, :, :, activation_index], cmap='gray')\n            activation_index += 1\n        \n        \n      ","468feabd":"plt.imshow(X_train[10][:,:,0]);","655c6fdf":"display_activation(activations, 8, 8, 1)","f94e11f7":"display_activation(activations, 8, 8, 3)","a048d019":"display_activation(activations, 8, 8, 7)","5a99a26b":"from sklearn.metrics import confusion_matrix\nY_prediction = model.predict(X_valid)\n# Convert predictions classes to one hot vectors \nY_pred_classes = np.argmax(Y_prediction,axis = 1) \n# Convert validation observations to one hot vectors\nY_true = np.argmax(Y_valid,axis = 1) \n# compute the confusion matrix\nconfusion_mtx = confusion_matrix(Y_true, Y_pred_classes) ","d2377fe6":"plt.figure(figsize=(10,8))\nsns.heatmap(confusion_mtx, annot=True, fmt=\"d\");","f894b9a3":"# predict results\nresults = model.predict(test)\n\n# select the indix with the maximum probability\nresults = np.argmax(results,axis = 1)\n","5c7edad6":"submissions=pd.DataFrame({\"ImageId\": list(range(1,len(results)+1)),\n                         \"Label\": results})\nsubmissions.to_csv(\"re2-submission.csv\", index=False, header=True)","461c7698":"## 3.  Building CNN architecture using keras","b7c23293":"###  above we have  not run our model with augmented data\n** we can run our model model with augmentated data like below **\n```\nmodel.fit_generator(datagen_train.flow(X_train, Y_train, batch_size=16), validation_data=(X_valid, Y_valid),\n                          epochs=10,steps_per_epoch=X_train.shape[0],callbacks=[checkpointer,lrate], verbose=1)\n                         \n ```                        ","79905346":"##  2.Data preprocess\n","d6eab09f":"## 3.3 optimizer ","5bc1cbea":"### 3.1 Defining cnn model","a2c5fc93":"###  2.4 Label Encoding","f4a8b129":"### Desplaying above image after layer 2 .\n** layer 1 is input layer **.","d9868c92":"## model Evaluation","6fcc5dad":"## Learning Rate Schedules\n\nLearning rate schedules seek to adjust the learning rate during training by reducing the learning rate according to a pre-defined schedule. Common learning rate schedules include time-based decay, step decay and exponential decay\n\n** Here we will implement Step Decay **\n\nStep decay schedule drops the learning rate by a factor every few epochs. The mathematical form of step decay is :\n```\nlr = lr0 * drop^floor(epoch \/ epochs_drop)\n```\n** we will drop learning rate after every 3 epochs **","555ac706":"### 2.1  loading data","aa24bd4c":"### 2.3  Reshape \n\nReshaping image into 3D matrix","a30b241a":"Refrences \n1.[visualize-convolutional-neural-network ](http:\/\/www.codeastar.com\/visualize-convolutional-neural-network\/)\n<br>\n2.[learning-rate-schedules-and-adaptive-learning-rate-methods-for-deep-learning ](https:\/\/towardsdatascience.com\/learning-rate-schedules-and-adaptive-learning-rate-methods-for-deep-learning-2c8f433990d1)\n ","98bd9c8a":"## Displaying output of layer 8","fb2b9637":"### visualizing the number of different labels in traing data","c8d87110":"### 4.3 confusion matrix ","8eede514":"###  2.2 Normalizing  data","726af559":"## 4.2 plotting training and validation accuracy","a48189ac":"## Visualize CNN Layers","c5edb025":"### Displaying original Image","8222fd7a":"<center> <h1> 1.Introduction to Convolution Neural network <h1><\/center>\n\n![](https:\/\/adeshpande3.github.io\/assets\/Cover.png)\n<br>\n<br>\n<p style=\"font-size:120%;\"> A CNN is a neural network that typically contains several types of layers, one of which is a convolutional layer, as well as pooling, and activation layers. <\/p>\n\n<h2>  convolutional layer <\/h2>\n<\/br>\n<p style=\"font-size:120%;\">The Conv layer is the core building block of a Convolutional Network that does most of the computational heavy lifting.<\/p>\n\n\n<img src=\"https:\/\/cdn-images-1.medium.com\/max\/1600\/0*1PSMTM8Brk0hsJuF.\">\n\n<p style=\"font-size:120%;\"> Imagine you have an image represented as a 5x5 matrix of values, and you take a 3x3 matrix and slide that 3x3 window around the image. At each position the 3x3 visits, you matrix multiply element wise the values of your 3x3 window by the values in the image that are currently being covered by the window and it also passes through RELU Activation.. This results in a single number the represents all the values in that window of the image. <\/p>\n\n\n\n<img src=\"https:\/\/cdn-images-1.medium.com\/max\/1600\/1*ZCjPUFrB6eHPRi4eyP6aaA.gif\" \/>\n\n\n<p style=\"font-size:120%;\"> The \u201cwindow\u201d that moves over the image is called a <b>kernel. <\/b>. The weigts of Kernel are randomly initialize and later it learn them.<br> The distance the window moves each time is called the <b>stride. <\/b>\n\n<h2> pooling layer<\/h2>\n\n<p style=\"font-size:120%;\"> Convolutional networks may include local or global pooling layers, which combine the outputs of neuron clusters at one layer into a single neuron in the next layer. For example, <b>max pooling<\/b> uses the maximum value from each of a cluster of neurons at the prior layer.Another is <b>average pooling<\/b>, which uses the average value from each of a cluster of neurons at the prior layer.<\/p>\n\n![](https:\/\/www.embedded-vision.com\/sites\/default\/files\/technical-articles\/CadenceCNN\/Figure7.jpg)\n\nFor more detail [Convolutional Neural Networks](http:\/\/cs231n.github.io\/convolutional-networks\/) <br>\n\n<br>\n\n<h2> Activation layer <\/h2>\n\n<p style=\"font-size:120%;\"> Activation functions are important for a Artificial Neural Network to learn and understand the complex patterns. The main function of it is to introduce non-linear properties into the network. What it does is, it calculates the \u2018weighted sum\u2019 and adds direction and decides whether to \u2018fire\u2019 a particular neuron or not.  There are  several kinds of non-linear activation functions, like Sigmoid, Tanh, ReLU and leaky ReLU. The non linear activation function will help the model to understand  the complexity and give accurate results.<\/p>\n![](https:\/\/i.stack.imgur.com\/iIcbq.gif)\n\nFor more detail. [Types Of Activation Functions In Neural Networks And Rationale Behind It](https:\/\/i.stack.imgur.com\/iIcbq.gif)\n\n\n","91651673":"## 3.2 Data Augmentation","1da68ffe":"# Introduction to CNN with keras\n\n*  ** 1. introduction ** \n*  ** 2. Data preparation **\n *               2.1  loading data \n\n *               2.2 Normalization\n *               2.3 Reshape\n *               2.4 Label Encoding\n *              2.5 splitting into training and validation data\n \n \n ** 3.  CNN Architecure **\n*          3.1. Define Model\n*          3.2 Data Augmentation\n*           3.3 optimizer and Learning Rate scheduler\n\n** 4. Model Evaluation **\n*        4.1 plotting training and validation loss\n*        4.2 plotting training and validation loss\n*         4.3 confusion matrix\n\n\n","ee2ce961":"**  2.5 Now we will split training data into training data and validation data **","744874b8":"### Displaying output of layer 4","ded44202":"## 4.1 plotting training and validation loss","f99c1f72":"```\ndatagen_train = ImageDataGenerator(\n    width_shift_range=0.2,  # randomly shift images horizontally \n    height_shift_range=0.2,# randomly shift images vertically \n    \n    horizontal_flip=True) # randomly flip images horizontally\n\n# fit augmented image generator on data\ndatagen_train.fit(X_train)\n```"}}