{"cell_type":{"2ac3b563":"code","0bb25595":"code","cfd782df":"code","d6ff7827":"code","c39e93fc":"code","526673ea":"code","d4756203":"code","08c61f16":"code","f5af76ee":"code","5b90ecd4":"code","2d814ead":"code","f132b4c5":"code","fdcb6f7f":"code","30756809":"code","0def1d0b":"markdown","ab021c23":"markdown","83a8108e":"markdown","df3d2fa8":"markdown","228b8e8b":"markdown","56d303f0":"markdown","2bd7b1a6":"markdown"},"source":{"2ac3b563":"import itertools\nimport collections\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.decomposition import PCA\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n# Importing wordcloud for plotting word clouds and textwrap for wrapping longer text\nfrom wordcloud import WordCloud\n# Importing spacy\nimport spacy\n# Loading model\nnlp = spacy.load('en_core_web_sm',disable=['parser', 'ner'])","0bb25595":"train_file = \"..\/input\/commonlitreadabilityprize\/train.csv\"","cfd782df":"data = pd.read_csv(train_file)\ndata.head()","d6ff7827":"data['binned_target'] = pd.cut(data['target'], bins=10)\ndata['binned_target'].value_counts()","c39e93fc":"data['target'].plot.hist(bins=10, alpha=0.5)","526673ea":"# Lemmatization with stopwords removal using spacy\ndata['lemmatized']=data['excerpt'].apply(lambda x: ' '.join([token.lemma_ for token in list(nlp(x)) if ((not token.is_punct) and (token.is_stop==False))]))","d4756203":"# Creating Bag of words vectors\ncv=CountVectorizer(analyzer='word')\nbow_vectors=cv.fit_transform(data['lemmatized'])","08c61f16":"# Visualizing our vectors\n# For bag of words\npca = PCA(n_components=2)\nx_pca = pca.fit_transform(bow_vectors.todense())\nplt.figure(figsize=(8,6))\nplt.scatter(x_pca[:,0],x_pca[:,1],c=data['target'],cmap='rainbow')\nplt.xlabel('First principal component')\nplt.ylabel('Second Principal Component')","f5af76ee":"# Creating vectors using TF-IDF \nTFIDF_vectorizer = TfidfVectorizer(min_df=5)\ntfidf_vectors = TFIDF_vectorizer.fit_transform(data['lemmatized'])","5b90ecd4":"# Visualizing our vectors\n# For TF-IDF\npca = PCA(n_components=2)\nx_pca = pca.fit_transform(tfidf_vectors.todense())\nplt.figure(figsize=(8,6))\nplt.scatter(x_pca[:,0],x_pca[:,1],c=data['target'],cmap='rainbow')\nplt.xlabel('First principal component')\nplt.ylabel('Second Principal Component')","2d814ead":"# WordCloud\nwordcloud = WordCloud(width = 3000, \n                      height = 2000, \n                      random_state=1, \n                      background_color='black', \n                      colormap='Set2', \n                      collocations=False).generate(\" \".join(list(data['lemmatized'])))\n\n# Save image\nwordcloud.to_file(\"wordcloud.png\")\n\n# plot the WordCloud image                       \nplt.figure(figsize = (8, 8), facecolor = None)\nplt.imshow(wordcloud)\nplt.axis(\"off\")\nplt.tight_layout(pad = 0)\n  \nplt.show()","f132b4c5":"# List of all words across texts\nall_words = \" \".join(data['lemmatized']).split()\n\n# Create counter\ncounts = collections.Counter(all_words)\n\ncounts.most_common(15)","fdcb6f7f":"clean_texts = pd.DataFrame(counts.most_common(50),\n                             columns=['words', 'count'])\n\nclean_texts.head()","30756809":"fig, ax = plt.subplots(figsize=(12, 12))\n\n# Plot horizontal bar graph\nclean_texts.sort_values(by='count').plot.barh(x='words',\n                      y='count',\n                      ax=ax,\n                      color=\"purple\")\n\nax.set_title(\"Common Words Found in Excerpts after cleaning\")\n\nplt.show()","0def1d0b":"# Import necessary libraries","ab021c23":"# Examine and clean the Textual Data","83a8108e":"# Calculate and Plot Word Frequency","df3d2fa8":"# Vectorization","228b8e8b":"Exploratory data analysis (EDA) is used by data scientists to analyze and investigate data sets and summarize their main characteristics, often employing data visualization methods. It helps determine how best to manipulate data sources to get the answers you need, making it easier for data scientists to discover patterns, spot anomalies, test a hypothesis, or check assumptions.\n\nEDA is primarily used to see what data can reveal beyond the formal modeling or hypothesis testing task and provides a provides a better understanding of data set variables and the relationships between them. It can also help determine if the statistical techniques you are considering for data analysis are appropriate. Originally developed by American mathematician John Tukey in the 1970s, EDA techniques continue to be a widely used method in the data discovery process today.\n\nDue to its importance let's explore our dataset and our texts","56d303f0":"If you find the notebook interesting please upvote !!","2bd7b1a6":"# WordCloud"}}