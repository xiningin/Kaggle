{"cell_type":{"6478d1f9":"code","a7a03f62":"code","6cb72da3":"code","286a4f57":"code","ad2afd77":"code","506fbf62":"code","f3215b63":"code","006460f8":"code","d74a65b5":"code","d27c1594":"code","60df0078":"code","a2d2a5de":"code","b9f7da20":"code","d25b1866":"code","fd11c3f5":"code","76868817":"markdown","6cf6d6b9":"markdown","49f18db2":"markdown","42280540":"markdown","d625f90c":"markdown","05bf7a24":"markdown","e81d618a":"markdown","9284a1f8":"markdown","64b46fe6":"markdown","f1fcf87c":"markdown","c2cb2aea":"markdown","930e1d68":"markdown"},"source":{"6478d1f9":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","a7a03f62":"import numpy as np\nimport pandas as pd\nimport tensorflow.keras.layers as Layer\nimport tensorflow.keras.backend as K\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.utils import plot_model\nfrom tensorflow.keras.layers import *\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras import optimizers\n\nimport tensorflow.keras.layers as layer\n%matplotlib inline","6cb72da3":"data = pd.read_csv(\"..\/input\/jane-street-market-prediction\/train.csv\")\ndata = data.astype({c: np.float32 for c in data.select_dtypes(include='float64').columns})  # limit memory use\ndata['action'] = (data['resp'] > 0).astype('int')\nfeatures = [c for c in data.columns if 'feature' in c]\nsfeature = features[0]\ndfeature = features[1:]\ndata[features].head()","286a4f57":"bindict = {}\ndef binning(df,col,method,bins,bindict):\n    uniqs=df[col].nunique()\n    if uniqs<=bins:\n        raise KeyError('nunique is smaller than bins: '+col)\n        return \n    def ff(x,fre_list):\n        if pd.isnull(x):\n            return -1\n        if x<=fre_list[0]:\n            return 0\n        elif x>fre_list[-1]:\n            return len(fre_list)-1\n        else :\n            for i in range(len(fre_list)-1):\n                if x>fre_list[i] and x<=fre_list[i+1]:\n                    return i\n    # \u7b49\u8ddd\u5206\u7bb1Isometric bin\n    if method=='distance':\n        umax=np.percentile(df[col],99.99)\n        umin=np.percentile(df[col],0.01)\n        step=(umax-umin)\/bins\n        fre_list=[umin+i*step for i in range(bins+1)]\n        return df[col].map(lambda x:ff(x,fre_list))\n    # \u7b49\u9891\u5206\u7bb1Equal frequency bin\n    elif method=='frequency' :\n        fre_list=[np.nanpercentile(df[col],100\/bins*i) for i in range(bins+1)]\n        fre_list=sorted(list(set(fre_list)))\n        bindict[col] = fre_list\n        return df[col].map(lambda x:ff(x,fre_list))","ad2afd77":"#Binning the original features at equal frequency \uff08\u5c06\u539f\u59cb\u7279\u5f81\u7b49\u9891\u5206\u7bb1\uff09\nfor fea in dfeature:\n    data[fea] = binning(df = data,col = fea, method='frequency',bins=10, bindict=bindict)\ndata.to_csv(\"spasetrain.csv\")\ndata.head()","506fbf62":"from sklearn.preprocessing import LabelEncoder\nfor feat in features:\n    lbe = LabelEncoder()\n    data[feat] = lbe.fit_transform(data[feat])\ndata.head()","f3215b63":"sparse_inputs = []\nfor f in features:\n    _input = Input([1], name=f)\n    sparse_inputs.append(_input)\n    \nsparse_1d_embed = []\nfor i, _input in enumerate(sparse_inputs):\n    f = features[i]\n    voc_size = data[f].nunique()\n    # Use l2 regularization to prevent overfitting \u4f7f\u7528 l2 \u6b63\u5219\u5316\u9632\u6b62\u8fc7\u62df\u5408\n    reg = tf.keras.regularizers.l2(0.5)\n    _embed = Embedding(voc_size, 1, embeddings_regularizer=reg)(_input)\n    # Since the result of Embedding is two-dimensional, \u7531\u4e8e Embedding \u7684\u7ed3\u679c\u662f\u4e8c\u7ef4\u7684\uff0c\n    # Therefore, if you need to add the Dense layer after Embedding, you need to connect to the Flatten layer first\n    # \uff08\u56e0\u6b64\u5982\u679c\u9700\u8981\u5728 Embedding \u4e4b\u540e\u52a0\u5165 Dense \u5c42\uff0c\u5219\u9700\u8981\u5148\u8fde\u63a5\u4e0a Flatten \u5c42\uff09\n    _embed = Flatten()(_embed)\n    sparse_1d_embed.append(_embed)\nfst_order_sparse_layer = Add()(sparse_1d_embed)","006460f8":"#\u4e8c\u9636sparse\u7279\u5f81\u4ea4\u53c9\n# embedding size\nk = 8\n\n# \u53ea\u8003\u8651sparse\u7684\u4e8c\u9636\u4ea4\u53c9\nsparse_kd_embed = []\nfor i, _input in enumerate(sparse_inputs):\n    f = features[i]\n    voc_size = data[f].nunique()\n    reg = tf.keras.regularizers.l2(0.7)\n    _embed = Embedding(voc_size, k, embeddings_regularizer=reg)(_input)\n    sparse_kd_embed.append(_embed)","d74a65b5":"# 1.\u5c06\u6240\u6709sparse\u7684embedding\u62fc\u63a5\u8d77\u6765\uff0c\u5f97\u5230 (n, k)\u7684\u77e9\u9635\uff0c\u5176\u4e2dn\u4e3a\u7279\u5f81\u6570\uff0ck\u4e3aembedding\u5927\u5c0f\nconcat_sparse_kd_embed = Concatenate(axis=1)(sparse_kd_embed)  # ?, n, k\n# 2.\u5148\u6c42\u548c\u518d\u5e73\u65b9\nsum_kd_embed = Lambda(lambda x: K.sum(x, axis=1))(concat_sparse_kd_embed)  # ?, k\nsquare_sum_kd_embed = Multiply()([sum_kd_embed, sum_kd_embed])  # ?, k\n# 3.\u5148\u5e73\u65b9\u518d\u6c42\u548c\nsquare_kd_embed = Multiply()([concat_sparse_kd_embed, concat_sparse_kd_embed]) # ?, n, k\nsum_square_kd_embed = Lambda(lambda x: K.sum(x, axis=1))(square_kd_embed)  # ?, k\n# 4.\u76f8\u51cf\u9664\u4ee52\nsub = Subtract()([square_sum_kd_embed, sum_square_kd_embed])  # ?, k\nsub = Lambda(lambda x: x*0.5)(sub)  # ?, k\nsnd_order_sparse_layer = Lambda(lambda x: K.sum(x, axis=1, keepdims=True))(sub)  # ?, 1\n","d27c1594":"##dnn\u90e8\u5206\nflatten_sparse_embed = Flatten()(concat_sparse_kd_embed)  # ?, n*k\nfc_layer = Dropout(0.5)(Dense(256, activation='relu')(flatten_sparse_embed))  # ?, 256\nfc_layer = Dropout(0.3)(Dense(256, activation='relu')(fc_layer))  # ?, 256\nfc_layer = Dropout(0.1)(Dense(256, activation='relu')(fc_layer))  # ?, 256\nfc_layer_output = Dense(1)(fc_layer)  # ?, 1","60df0078":"#\u8f93\u51fa\u7ed3\u679c\noutput_layer = Add()([fst_order_sparse_layer, snd_order_sparse_layer, fc_layer_output])\noutput_layer = Activation(\"sigmoid\")(output_layer)","a2d2a5de":"#\u7f16\u8bd1\u6a21\u578b\nmodel = Model(sparse_inputs, output_layer)\nmodel.summary()","b9f7da20":"model.compile(optimizer=\"adam\", \n              loss=\"binary_crossentropy\", \n              metrics=[\"binary_crossentropy\", tf.keras.metrics.AUC(name='auc')])","d25b1866":"#\u8bad\u7ec3\ndata = data.query('date > 85').reset_index(drop = True) \ndata = data.query('weight > 0').reset_index(drop = True)\nvalid = data.loc[(data.date >= 451) & (data.date < 500)].reset_index(drop=True)\ntrain = data.loc[data.date < 499].reset_index(drop=True)\ntrain_data = train.loc[:, features]\nvalid_data = valid.loc[:, features]\ntrain_label = [train.loc[:, \"action\"].values]\nval_label = [valid.loc[:, \"action\"].values]\ndel data","fd11c3f5":"train_sparse_x = [train_data[f].values for f in features]\nval_sparse_x = [valid_data[f].values for f in features]\nmodel.fit(train_sparse_x, \n          train_label, epochs=50, batch_size=5000,\n          validation_data=(val_sparse_x, val_label),\n         )\nmodel.save_weights(\"deepfmweights.hdf5\")","76868817":"* dnn","6cf6d6b9":"* the output layer","49f18db2":"![image.png](attachment:image.png)","42280540":"* In order to make better use of deepfm for feature crossing, we need to divide the numerical features into buckets\uff08\u4e3a\u4e86\u66f4\u597d\u7684\u5e94\u7528deepfm\u6765\u8fdb\u884c\u7279\u5f81\u4ea4\u53c9\uff0c\u6211\u4eec\u9700\u8981\u5bf9\u6570\u503c\u7279\u5f81\u8fdb\u884c\u5206\u6876\uff09\n* In this block, a universal bin function is defined, which can realize equal frequency bin and equal distance bin, and can help to realize feature bin easily\uff08\u5728\u8fd9\u91cc\u5b9a\u4e49\u4e86\u4e00\u4e2a\u5177\u6709\u666e\u9002\u6027\u7684\u5206\u7bb1\u51fd\u6570\uff0c\u53ef\u4ee5\u5b9e\u73b0\u7b49\u9891\u5206\u7bb1\u548c\u7b49\u8ddd\u5206\u7bb1\uff0c\u53ef\u4ee5\u5e2e\u52a9\u8f7b\u677e\u5b9e\u73b0\u7279\u5f81\u5206\u7bb1\uff09","d625f90c":"* First-order sparse features\n* Here is a separate input for each sparse feature,\n* The purpose is to facilitate the construction of second-order combination features later\n* \u4e00\u9636sparse\u7279\u5f81\n* \u8fd9\u91cc\u5355\u72ec\u5bf9\u6bcf\u4e00\u4e2a sparse \u7279\u5f81\u6784\u9020\u8f93\u5165\uff0c\n* \u76ee\u7684\u662f\u65b9\u4fbf\u540e\u9762\u6784\u9020\u4e8c\u9636\u7ec4\u5408\u7279\u5f81","05bf7a24":"* Second-order sparse feature crossover","e81d618a":"* Compire the model","9284a1f8":"**When mlp sweeps the rankings, I think we need to try more models. Although mlp can achieve high scores, it lacks cross-learning of features. So here I open a notebook based on deepfm, hoping to discuss and learn together.**\n\n**\uff08\u5728mlp\u6a2a\u626b\u6392\u884c\u699c\u7684\u65f6\u5019\uff0c\u6211\u8ba4\u4e3a\u6211\u4eec\u9700\u8981\u53bb\u5c1d\u8bd5\u66f4\u591a\u7684\u6a21\u578b\u3002mlp\u867d\u7136\u53ef\u4ee5\u53d6\u5f97\u5f88\u9ad8\u7684\u5206\u6570\uff0c\u4f46\u662f\u7f3a\u5c11\u5bf9\u4e8e\u7279\u5f81\u7684\u4ea4\u53c9\u5b66\u4e60\u3002\u6240\u4ee5\u5728\u8fd9\u91cc\u6211\u516c\u5f00\u57fa\u4e8edeepfm\u7684notebook,\u5e0c\u671b\u4e00\u8d77\u8ba8\u8bba\u5b66\u4e60\u3002\uff09**","64b46fe6":"The architecture of deepfm","f1fcf87c":"****Training****","c2cb2aea":"1. Join all the embeddings of sparse to get a matrix of (n, k), where n is the feature number and k is the embedding size\n2. Sum first and then square\n3. Square first and then sum\n4. Subtract and divide by 2","930e1d68":" **LOAD DATA**"}}