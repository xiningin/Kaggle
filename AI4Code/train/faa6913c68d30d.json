{"cell_type":{"f595b5a4":"code","e0585054":"code","8fe688f1":"code","62953d81":"code","e8a1c217":"code","6450b963":"code","32c78ce0":"code","606b6c2e":"code","2f3201ec":"code","8ca3c821":"code","40ef7fa0":"code","4b985c4b":"code","359f0fbe":"code","160d6012":"code","63d2e4c1":"code","7c5d2651":"code","c11570a6":"code","1bea6e3f":"code","87796fad":"code","8aa27d81":"code","862ef190":"markdown","9882b2ed":"markdown","0446a9b4":"markdown","cf58d71e":"markdown","9546d043":"markdown","a98de4b1":"markdown","f99c0088":"markdown","722e91c3":"markdown","903eb2d4":"markdown","e7cddf35":"markdown","629aad58":"markdown","fc3ae11e":"markdown","ceb6ccab":"markdown","9dd70db5":"markdown","97a8a707":"markdown","d3dd9d9b":"markdown","922e2397":"markdown","2498f5f1":"markdown"},"source":{"f595b5a4":"import numpy as np\nimport pandas as pd\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\nseed = 51\n\nBATCH_SIZE=32","e0585054":"data = pd.read_csv('\/kaggle\/input\/heart-disease-uci\/heart.csv')\ndata.sample(5)","8fe688f1":"data.info()","62953d81":"data['chol_age'] = data['chol']\/data['age']\ndata.sample(5)","e8a1c217":"from sklearn.preprocessing import RobustScaler\n\ndata['age'] = RobustScaler().fit_transform(data['age'].values.reshape(-1, 1))\ndata['chol_age'] = RobustScaler().fit_transform(data['chol_age'].values.reshape(-1, 1))\ndata['trestbps'] = RobustScaler().fit_transform(data['trestbps'].values.reshape(-1, 1))\ndata['chol'] = RobustScaler().fit_transform(data['chol'].values.reshape(-1, 1))\ndata['thalach'] = RobustScaler().fit_transform(data['thalach'].values.reshape(-1, 1))\ndata['oldpeak'] = RobustScaler().fit_transform(data['oldpeak'].values.reshape(-1, 1))\n\ndata.sample(10)","6450b963":"data['cp'][data['cp'] == 0] = 'asymptomatic'\ndata['cp'][data['cp'] == 1] = 'atypical angina'\ndata['cp'][data['cp'] == 2] = 'non-anginal pain'\ndata['cp'][data['cp'] == 3] = 'typical angina'\n\ndata['restecg'][data['restecg'] == 0] = 'left ventricular hypertrophy'\ndata['restecg'][data['restecg'] == 1] = 'normal'\ndata['restecg'][data['restecg'] == 2] = 'ST-T wave abnormality '\n\ndata['slope'][data['slope'] == 0] = 'down'\ndata['slope'][data['slope'] == 1] = 'flat'\ndata['slope'][data['slope'] == 2] = 'up'","32c78ce0":"corr = data.corr()\ncorr.sort_values([\"target\"], ascending = False, inplace = True)\ncorr.target","606b6c2e":"from sklearn.preprocessing import OneHotEncoder\n\nOH_cols = ['cp', 'slope', 'restecg','thal']\n\n# Apply one-hot encoder to each column with categorical data\nOH_encoder = OneHotEncoder(handle_unknown='ignore', sparse=False)\nOH_cols_data = pd.DataFrame(OH_encoder.fit_transform(data[OH_cols]))\n\n# One-hot encoding put in generic column names, use feature names instead\nOH_cols_data.columns = OH_encoder.get_feature_names(OH_cols)\n\n# # remove the original columns\n# for c in OH_cols:\n#     cols_to_use.remove(c)\n    \n# # Add one-hot columns to cols_to_use\n# for c in OH_cols_data.columns:\n#     cols_to_use.append(c)\n\n# # print(cols_to_use)\n\n# One-hot encoding removed index; put it back\nOH_cols_data.index = data.index\n\n# Remove categorical columns (will replace with one-hot encoding)\nnum_data = data.drop(OH_cols, axis=1)\n\n# Add one-hot encoded columns to numerical features\nOH_data = pd.concat([num_data, OH_cols_data], axis=1)\n\ndata = OH_data","2f3201ec":"corr = data.corr()\ncorr.sort_values([\"target\"], ascending = False, inplace = True)\ncorr.target","8ca3c821":"from sklearn.model_selection import train_test_split\n\nX = data.drop(['target'], axis=1)\ny = data['target']\n\ndef setup_data(X_in, y_in):\n    return train_test_split(X_in, y_in, test_size=0.2, random_state=seed)","40ef7fa0":"import tensorflow\ntensorflow.random.set_seed(seed) \nfrom tensorflow.keras.layers import Input, Dense, ELU, Dropout\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.regularizers import l2\n\ninput = Input(shape=X.shape[1])\n\nm = Dense(1024)(input)\nm = ELU()(m)\nm = Dropout(0.33)(m)\n\nm = Dense(1024)(m)\nm = ELU()(m)\nm = Dropout(0.33)(m)\n\nm = Dense(1024)(m)\nm = ELU()(m)\nm = Dropout(0.33)(m)\n\nm = Dense(1024)(m)\nm = ELU()(m)\nm = Dropout(0.33)(m)\n\nm = Dense(1024)(m)\nm = ELU()(m)\nm = Dropout(0.33)(m)\n\nm = Dense(1024)(m)\nm = ELU()(m)\nm = Dropout(0.33)(m)\n\nm = Dense(1024)(m)\nm = ELU()(m)\nm = Dropout(0.33)(m)\n\nm = Dense(1024)(m)\nm = ELU()(m)\nm = Dropout(0.33)(m)\n\nm = Dense(1024)(m)\nm = ELU()(m)\nm = Dropout(0.33)(m)\n\nm = Dense(1024)(m)\nm = ELU()(m)\nm = Dropout(0.33)(m)\n\nm = Dense(1024)(m)\nm = ELU()(m)\nm = Dropout(0.33)(m)\n\nm = Dense(1024)(m)\nm = ELU()(m)\nm = Dropout(0.33)(m)\n\n#####\n\nm = Dense(1024)(m)\nm = ELU()(m)\nm = Dropout(0.33)(m)\n\nm = Dense(1024)(m)\nm = ELU()(m)\nm = Dropout(0.33)(m)\n\nm = Dense(1024)(m)\nm = ELU()(m)\nm = Dropout(0.33)(m)\n\nm = Dense(1024)(m)\nm = ELU()(m)\nm = Dropout(0.33)(m)\n\nm = Dense(1024)(m)\nm = ELU()(m)\nm = Dropout(0.33)(m)\n\nm = Dense(1024)(m)\nm = ELU()(m)\nm = Dropout(0.33)(m)\n\nm = Dense(1024)(m)\nm = ELU()(m)\nm = Dropout(0.33)(m)\n\nm = Dense(1024)(m)\nm = ELU()(m)\nm = Dropout(0.33)(m)\n\nm = Dense(1024)(m)\nm = ELU()(m)\nm = Dropout(0.33)(m)\n\nm = Dense(1024)(m)\nm = ELU()(m)\nm = Dropout(0.33)(m)\n\nm = Dense(1024)(m)\nm = ELU()(m)\nm = Dropout(0.33)(m)\n\noutput = Dense(1, activation='sigmoid')(m)\n\nmodel = Model(inputs=[input], outputs=[output])\n\nmodel.summary()","4b985c4b":"from tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n\nmodel.compile(optimizer=Adam(lr=0.001), loss='binary_crossentropy', metrics=['acc'])\n\nes = EarlyStopping(monitor='val_loss', patience=100, verbose=1, restore_best_weights=True)\n\nrlp = ReduceLROnPlateau(monitor='val_loss', patience=9, verbose=1, factor=0.5, cooldown=5, min_lr=1e-10)","359f0fbe":"X_remainder, X_test, y_remainder, y_test = setup_data(X,y)","160d6012":"X_train, X_validation, y_train, y_validation = setup_data(X_remainder, y_remainder)\n\nhistory = model.fit(X_train,\n    y_train,\n    batch_size=BATCH_SIZE,\n    epochs=200,\n    verbose=2,\n    callbacks=[es, rlp],\n    validation_data=(X_validation, y_validation),\n    shuffle=True\n         ).history","63d2e4c1":"import matplotlib.pyplot as plt\nimport seaborn as sns\n\nfig, (ax1, ax2) = plt.subplots(2, 1, sharex='col', figsize=(20, 14))\n\nax1.plot(history['loss'], label='Train loss')\nax1.plot(history['val_loss'], label='Validation loss')\nax1.legend(loc='best')\nax1.set_title('Loss')\n\nax2.plot(history['acc'], label='Train accuracy')\nax2.plot(history['val_acc'], label='Validation accuracy')\nax2.legend(loc='best')\nax2.set_title('Accuracy')\n\nplt.xlabel('Epochs')\nsns.despine()\nplt.show()","7c5d2651":"model.evaluate(X, y, batch_size=BATCH_SIZE, verbose=1)","c11570a6":"history = model.fit(X_remainder,\n    y_remainder,\n    batch_size=BATCH_SIZE,\n    epochs=200,\n    verbose=2,\n    callbacks=[es, rlp],\n    shuffle=True\n         ).history","1bea6e3f":"model.evaluate(X_test, y_test, verbose=0)","87796fad":"from sklearn.metrics import confusion_matrix\n\ny_prob = model.predict(X_test)\ny_pred = np.around(y_prob)\nconfusion_matrix = confusion_matrix(y_test, y_pred)\nconfusion_matrix","8aa27d81":"total=sum(sum(confusion_matrix))\n\nsensitivity = confusion_matrix[0,0]\/(confusion_matrix[0,0]+confusion_matrix[1,0])\nprint('Sensitivity : ', sensitivity )\n\nspecificity = confusion_matrix[1,1]\/(confusion_matrix[1,1]+confusion_matrix[0,1])\nprint('Specificity : ', specificity)","862ef190":"As expected chol has very little correlation with the target. **Note** that a more negative number correlates with having the disease (target = 0). A more positive number correlates with not having the disease (target = 1). A small number means there is little correlation with the target.\n\nHighest correlations are with asymptomatic chest pain, thal_3(probably reversible defect), and exang(angina=yes).","9882b2ed":"Let's set aside 20% of the data as a test set and use the rest for training.","0446a9b4":"Create model","cf58d71e":"Now, lets check the correlation again:","9546d043":"Create some training and test data to use","a98de4b1":"Let's fix the scaling.","f99c0088":"Check correlation of each attribute to the target","722e91c3":"For curiosity's sake, let's evaluate the model on the full data set","903eb2d4":"> > So we achieved more than 80% accuracy. Pretty reasonable for such a small dataset.","e7cddf35":"Let's make some of the values clearer:","629aad58":"Visualize the training","fc3ae11e":"**Feature engineering:** There is a hypothesis that high cholesterol actually protects elderly patients. So lets create a chol_age feature that reduces the value of chol as we age:","ceb6ccab":"Thanks to [IntiPic](https:\/\/www.kaggle.com\/intipic) for explaining the dataset in [this](https:\/\/www.kaggle.com\/ronitf\/heart-disease-uci\/discussion\/105877) post. To me this looks more like a derivation of the [Statlog Heart dataset](https:\/\/archive.ics.uci.edu\/ml\/datasets\/Statlog+%28Heart%29). Especially since the attributes (features) are an exact match.","9dd70db5":"Train our model","97a8a707":"Let's see how it does for the test data we set aside earlier.","d3dd9d9b":"It is possible that our engineered feature, chol_age, is proving the hypothesis by showing a slight correlation with the abscense of heart disease. In the future it might be interesting to replace the chol feature with our new chol_age feature.\n\nLet's take a closer look at cp, slope, restcg, and thal correlation. Here is what they mean:\n\ncp: chest pain type\n* -- Value 0: asymptomatic\n* -- Value 1: atypical angina\n* -- Value 2: non-anginal pain\n* -- Value 3: typical angina\n\nslope: the slope of the peak exercise ST segment\n* 0: downsloping; \n* 1: flat; \n* 2: upsloping\n\nrestecg: resting electrocardiographic results\n* -- Value 0: showing probable or definite left ventricular hypertrophy by Estes' criteria\n* -- Value 1: normal\n* -- Value 2: having ST-T wave abnormality (T wave inversions and\/or ST elevation or depression of > 0.05 mV)\n\nthal: \n* 1 = fixed defect; \n* 2 = normal; \n* 7 = reversable defect\n\nSo, let's one hot encode them","922e2397":"Sensitivity = true positive rate, Specificity = true negative rate","2498f5f1":"It appears our model can predict the target accurately. But of course this would be a lot more interesting with a much larger training dataset and a separate test dataset.\n\nOK. Let's do a final training on the full training set without validation."}}