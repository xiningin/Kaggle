{"cell_type":{"6c4b7c18":"code","077d955c":"code","f169e8b7":"code","eb0e5f4c":"code","127060a4":"code","8f29ac54":"code","73558fb1":"code","379120ff":"code","12579d9f":"code","14840b4e":"code","ddf6ed9b":"code","2663235f":"code","3c0ff660":"code","f72be41f":"code","05164a92":"code","d6f53731":"markdown","54d56593":"markdown","5e3ee4be":"markdown","e78272b9":"markdown","a66b5644":"markdown","7ffd51f4":"markdown","b8f9771e":"markdown","389035ba":"markdown","59940d11":"markdown","8f59acc6":"markdown","b3f4e306":"markdown","d677fa1f":"markdown","d35fafde":"markdown","59fc7117":"markdown","f08c809b":"markdown"},"source":{"6c4b7c18":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport xgboost as xgb\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn import pipeline\nfrom sklearn import preprocessing\nfrom sklearn import impute\nfrom sklearn import metrics\nfrom sklearn import compose\nfrom sklearn import model_selection\nimport optuna\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session\n\nGPU_ENABLED = True","077d955c":"train = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/train.csv')\ntest = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/test.csv')","f169e8b7":"print('Train.shape: ', train.shape)\nprint('Test.shape: ', test.shape)","eb0e5f4c":"train.head()","127060a4":"test.head()","8f29ac54":"train.info()","73558fb1":"numerical_columns = [c for c in train.select_dtypes('number').columns.tolist() if c not in ['Id', 'SalePrice']]\ncategorical_columns = train.select_dtypes('object').columns.tolist()\n\ndummies = pd.get_dummies(train.append(test)[categorical_columns])\ntrain[dummies.columns] = dummies.iloc[:len(train), :]\ntest[dummies.columns] = dummies.iloc[len(train):, :]\n\nprint(\"train.shape:\", train.shape, \", test.shape: \", test.shape)","379120ff":"missing = train.isnull().sum()\nmissing = missing[missing > 0]\nmissing.sort_values(inplace=True)\nmissing.plot.bar()","12579d9f":"sns.displot(train['SalePrice'])","14840b4e":"train[\"SalePrice\"] = np.log1p(train[\"SalePrice\"])\nsns.displot(train['SalePrice'])","ddf6ed9b":"numerical_preprocessor = pipeline.Pipeline(steps=[\n    ('imputer', impute.SimpleImputer(strategy='mean')),\n    ('scaler', preprocessing.StandardScaler())\n])\n\ncategorical_preprocessor = pipeline.Pipeline(steps=[\n    ('imputer', impute.SimpleImputer(strategy='most_frequent')),\n    ('onehot', preprocessing.OneHotEncoder(handle_unknown='ignore'))\n])\n\npreprocessor = compose.ColumnTransformer(transformers=[\n    ('numerical_preprocessor', numerical_preprocessor, numerical_columns),\n    ('categorical_preprocessor', categorical_preprocessor, categorical_columns)\n])\n\nmodel_pipeline = pipeline.Pipeline(steps=[\n    ('preprocessing', preprocessor),\n    ('model', xgb.XGBRegressor(learning_rate=0.03, subsample=0.8, n_estimators=1000))\n])","2663235f":"X = train.drop([\n    'Id',\n    'SalePrice'\n], axis=1)\ny = train[['SalePrice']]\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\nmodel = model_pipeline.fit(X_train, y_train)\nprediction = model.predict(X_test)\nprint(\"XGBRegressor RMSE score:\", metrics.mean_squared_error(y_test, prediction, squared=False))","3c0ff660":"encoder = preprocessor.fit(X_train)\nX_train = encoder.transform(X_train)\nX_test = encoder.transform(X_test)\n\ndef train_model_for_study(X, y, model):\n    X_train, X_valid, y_train, y_valid = model_selection.train_test_split(\n        X, \n        y, \n        test_size=0.20, \n        random_state=42\n    )\n\n    X_train = preprocessor.fit_transform(X_train, y_train)\n    X_valid = preprocessor.transform(X_valid)\n\n    model.fit(\n        X_train, \n        y_train,\n        early_stopping_rounds=300,\n        eval_set=[(X_valid, y_valid)], \n        verbose=False\n    )\n\n    yhat = model.predict(X_valid)\n    return metrics.mean_squared_error(y_valid, yhat, squared=False)\n\ndef objective_xgb(trial):\n    \"\"\"\n    Objective function to tune an `XGBRegressor` model.\n    \"\"\"\n\n    params = {\n        'n_estimators': trial.suggest_int(\"n_estimators\", 1000, 10000),\n        'reg_alpha': trial.suggest_loguniform(\"reg_alpha\", 1e-8, 100.0),\n        'reg_lambda': trial.suggest_loguniform(\"reg_lambda\", 1e-8, 100.0),\n        \"subsample\": trial.suggest_float(\"subsample\", 0.5, 1.0, step=0.1),\n        \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.01, 1.0, log=True),\n        'max_depth': trial.suggest_int(\"max_depth\", 2, 9),\n        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.1, 1.0),\n    }\n\n    if GPU_ENABLED:\n        params[\"tree_method\"] = \"gpu_hist\"\n        params[\"predictor\"] = \"gpu_predictor\"\n\n    model = xgb.XGBRegressor(\n        booster=\"gbtree\",\n        objective=\"reg:squarederror\",\n        random_state=42,\n        **params\n    )\n\n    return train_model_for_study(X, y, model)\n\n# study_xgb = optuna.create_study(direction=\"minimize\")\n# study_xgb.optimize(objective_xgb, n_trials=5)\n# study_xgb.best_params","f72be41f":"xgbregressor = xgb.XGBRegressor(\n    n_estimators=6268,\n    reg_alpha=0.0001471214273251562,\n    reg_lambda=4.765085783965461e-06,\n    subsample=0.5,\n    learning_rate=0.0307485169431525,\n    max_depth=2,\n    colsample_bytree=0.10058061595167168\n)\nmodel = xgbregressor.fit(X_train, np.ravel(y_train))\nprediction = model.predict(X_test)\nprint(\"XGBRegressor RMSE score:\", metrics.mean_squared_error(y_test, prediction, squared=False))","05164a92":"X_sub = test.drop(['Id'], axis=1)\n\nX_sub = encoder.transform(X_sub)\n\nprediction = model.predict(X_sub)\nprediction = np.expm1(prediction)\ntest['SalePrice'] = prediction\nsubmission = test[['Id', 'SalePrice']]\nsubmission.to_csv(\"submission.csv\", index=False)","d6f53731":"## Exploratory Data Analysis","54d56593":"## Submission","5e3ee4be":"### Load data","e78272b9":"TODO: describe steps in pipeline.","a66b5644":"I'm going to use Optuna for tuning hyperparameters. This was inspired by [Santiago Valdarrama](https:\/\/www.kaggle.com\/santiagovaldarrama) in [this notebook](https:\/\/deepnote.com\/@svpino\/Tuning-Hyperparameters-with-Optuna-6hoSPY0vTiCPIpXwdHDVVw).","7ffd51f4":"## Model","b8f9771e":"### Optimizing the model for better score","389035ba":"Predicting with optimized parameters:","59940d11":"### Training","8f59acc6":"### Data preprocessing pipeline","b3f4e306":"As we can see in the plot above, SalePrice is skewed to the right. We should transform SalePrice to a normal distribution because skewed data degrade the regression based model's ability to describe typical cases as it has to deal with rare cases on extreme values.","d677fa1f":"```json\nlast_result: {\n    'n_estimators': 6268,\n    'reg_alpha': 0.0001471214273251562,\n    'reg_lambda': 4.765085783965461e-06,\n    'subsample': 0.5,\n    'learning_rate': 0.0307485169431525,\n    'max_depth': 2,\n    'colsample_bytree': 0.10058061595167168\n }\n```","d35fafde":"### Missing data","59fc7117":"In the 1460 rows of training data and 1459 rows of test data, there are a total of 81 attributes. From those 81 attributes, excluding Id and SalePrice, 36 of them are numerical and 43 are categorical.\n\n**Numerical: ** MSSubClass, LotFrontage, LotArea, OverallQual, OverallCond, YearBuilt, YearRemodAdd, MasVnrArea, BsmtFinSF1, BsmtFinSF2, BsmtUnfSF, TotalBsmtSF, 1stFlrSF, 2ndFlrSF, LowQualFinSF, GrLivArea, BsmtFullBath, BsmtHalfBath, FullBath, HalfBath, BedroomAbvGr, KitchenAbvGr, TotRmsAbvGrd, Fireplaces, GarageYrBlt, GarageCars, GarageArea, WoodDeckSF, OpenPorchSF, EnclosedPorch, 3SsnPorch, ScreenPorch, PoolArea, MiscVal, MoSold, YrSold.\n\n**Categorical: ** MSZoning, Street, Alley, LotShape, LandContour, Utilities, LotConfig, LandSlope, Neighborhood, Condition1, Condition2, BldgType, HouseStyle, RoofStyle, RoofMatl, Exterior1st, Exterior2nd, MasVnrType, ExterQual, ExterCond, Foundation, BsmtQual, BsmtCond, BsmtExposure, BsmtFinType1, BsmtFinType2, Heating, HeatingQC, CentralAir, Electrical, KitchenQual, Functional, FireplaceQu, GarageType, GarageFinish, GarageQual, GarageCond, PavedDrive, PoolQC, Fence, MiscFeature, SaleType, SaleCondition.\n\nWe should one-hot encode these categorical features. We can do that by generating dummies for all categorical features. This was inspired by [Santiago Valdarrama](https:\/\/www.kaggle.com\/santiagovaldarrama) in [this notebook](https:\/\/www.kaggle.com\/santiagovaldarrama\/30-days-of-ml-stacked-ensembles).\n\nAt the end of this process, we should end up with a dataset with 332 columns.","f08c809b":"### Distribution"}}