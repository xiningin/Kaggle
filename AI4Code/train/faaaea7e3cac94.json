{"cell_type":{"6e9a576c":"code","a2c0cdfe":"code","c00c1f36":"code","8fb0ba5b":"code","d39ca925":"code","b2258e86":"code","d29d7bc0":"code","e900ca47":"code","5c5b3c90":"code","3e1906cd":"code","7eeba6bc":"code","b9cb5639":"code","4428eed6":"code","8d36fa28":"code","210978d1":"code","d1872e60":"code","33e56386":"code","b6de3eb0":"code","07ebf82f":"code","02d9c28c":"code","fc694594":"code","c18afed0":"code","fd6e3385":"code","8a3d3273":"code","f4bb4957":"code","84e967b5":"code","a0455235":"code","84e3afcd":"code","e7e1b30f":"code","e90e7683":"code","580da0cb":"code","9bcf05f4":"code","f33528e7":"code","59563cd9":"code","df0a43c2":"code","320e77f4":"code","c258d7ee":"code","3e67465e":"code","4b411aa1":"code","7f845f42":"code","5d91c50f":"code","63b4cb85":"code","eb8c9fe0":"code","d2d7f4dc":"markdown","53f3802c":"markdown","9f0e22ad":"markdown","bb520d00":"markdown","15ba6f70":"markdown","1f9f53e3":"markdown","a0a88e83":"markdown","d2890777":"markdown","b03f2760":"markdown","d3366130":"markdown","9cbd1a37":"markdown","400a92a0":"markdown","66a02a40":"markdown","70fca76e":"markdown","21f8711e":"markdown","7232874e":"markdown","2bcba222":"markdown"},"source":{"6e9a576c":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport os\nfrom wordcloud import WordCloud, STOPWORDS\nfrom textblob import TextBlob\nimport scipy.stats as stats\nimport spacy \n\nfrom tqdm import tqdm\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\nfrom sklearn.decomposition import LatentDirichletAllocation,  PCA, NMF\nimport random \n\nimport gensim\nfrom gensim import corpora, models, similarities\n\nimport logging\nimport tempfile\nfrom nltk.corpus import stopwords\nfrom string import punctuation\nfrom collections import OrderedDict\nimport seaborn as sns\nimport pyLDAvis.gensim\nimport pyLDAvis.sklearn\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n# Suppress warnings \nimport warnings\nwarnings.filterwarnings(\"ignore\", category=DeprecationWarning)\nwarnings.filterwarnings(\"ignore\", category=UserWarning)\nwarnings.filterwarnings(\"ignore\", category=FutureWarning)\nfrom IPython.display import HTML\n\n","a2c0cdfe":"# Dimension reduction\nfrom sklearn.decomposition import PCA\nfrom sklearn.manifold import TSNE\nfrom sklearn.manifold import Isomap\nfrom sklearn.decomposition import FactorAnalysis","c00c1f36":"df = pd.read_csv('\/kaggle\/input\/covid19-public-media-dataset\/covid19_articles_20200512.csv',index_col='Unnamed: 0')\ndf2 = pd.read_csv('\/kaggle\/input\/covid19-public-media-dataset\/covid19_articles_20200526.csv',index_col='Unnamed: 0')\ndf3 = pd.read_csv('\/kaggle\/input\/covid19-public-media-dataset\/covid19_articles_20200504.csv',index_col='Unnamed: 0')\n# concatenating sources \ndf = pd.concat([df,df2,df3],ignore_index=True)","8fb0ba5b":"# test for 1000 rows\ndf = df.head(500)\ndf.head()","d39ca925":"\ntext_example = ['novel coronavirus strain infect least people kill patient disease emerge china december last year coronavirus initial symptom include dry cough fever pneumonia kidney failure death leave untreated chinese health official confirm monday january virus jump people fuel fear global epidemic brewing big threat hand call asymptomatic carrier people infect coronavirus show symptom ill dr jeremy farrar director non profit charity wellcome warn true number infect different official count major concern range severity symptom virus cause clear people affect infectious experience mild symptom experience symptom asymptomatic read more warn coronavirus expect spread china may mask true number infected extent person person transmission matter urgency work novel coronavirus 2019-ncov belong family pathogens responsible sars pandemic sars severe acute respiratory syndrome kill least people infect more human contract sars cov virus civet cat asia coronavirus family zoonotic mean spread animal human world health organization believe many undiscovered strain wild time coronavirus outbreak trace seafood market wuhan city hubei province prevent virus spread authority stop travel strong city chinese official suspend bus subway system airport train link wuhan dr farrar outbreak concern person person transmission confirm expect increase case number china more country health care worker infect may mask true number infect dr jeremy farrar wellcome world health organization role ensure global public health response outbreak rapid robust comprehensive geographic spread case call emergency committee consider declare international public health emergency part process coronavirus break china infection confirm japan south korea thailand us accord who virus contract traveller wuhan don\u2019t misscoronavirus symptom coronavirus chinese coronavirus strike map]how dangerous china coronavirus analysis concern mount increase travel public celebration mark chinese year saturday january dr farrar speed virus identify testament change public health china sars strong global coordination who know more outbreak travel huge part approach chinese year right concern level high level scientist able devise vaccine virus understand coronavirus strain preventative medication official urge traveller maintain good standard hygiene avoid contact raw food infected people dr farrar world prepare identify patient take necessary public health clinical measure sars decade understand virus public health clinical impact urgent focus evidence base intervention prove treatment vaccine cepi coalition epidemic preparedness innovations wellcome support work global partner accelerate vaccine research virus today front back page download newspaper order issue historic daily express newspaper archive']\ntext_example2 = [\"coronavirus sweep globe chaotic epidemic virus continue spread epicentre china country world flu like illness think begin wuhan transfer country include france germany us death toll skyrocket death remain china present bid reduce further spread disease protect traveller uk airline british airways virgin atlantic amend booking policy flight mainland china weekend british airways introduce flexible booking policy ticket destine affected region january february airline allow passenger opportunity rebook alternative flight later date cancel booking penalty term condition lay initial booking read more coronavirus cruise line cancel sailing death toll rise update ba spokesperson express.co.uk understand customer may want change travel plan time order flexible possible await further advice government health organisation offer customer travel china february ability receive refund rebook flight country continue monitor situation virgin atlantic offer same flexible policy passenger book hong kong shanghai codeshare destination china passenger rebook cancel receive full refund book term condition don't missflight attendant reveal sad thing passenger insider]best bad cruise line reveal analysis]plane passenger ban bring onboard insight virgin atlantic spokesperson be monitor situation regard coronavirus follow guidance set relevant authority urge customer visit foreign commonwealth office travel web page more information travel affect area customer book travel china include hong kong like discuss travel plan invite contact customer care team sms messaging system +44 team happy assist enquiry time write foreign commonwealth office advise travel hubei province statement fco website urge be area able leave january wuhan authority close transport hub include airport railway bus station shop amenity closed public event cancel public health uk enforce monitoring flight wuhan uk statement public health england state enhanced monitoring package include number measure help provide advice traveller feel unwell travel wuhan include port health team meet direct flight aircraft provide advice support feel unwell team include principal port medical inspector port health doctor administrative support team leader check symptom coronavirus provide information passenger symptom become ill fco update travel information visit country surround china include thailand mongolia sri lanka marshall islands multiple major cruise line globe cancel journey mainland port royal caribbean msc passenger expect sail affected cruise line offer full refund port change royal caribbean cancel january sailing cruise ship spectrum seas schedule depart shanghai decision coordinate disease prevention ensure health safety passenger crew royal caribbean statement msc cancel january departure splendida shanghai schedule sail night guest book cruise option receive full refund cruise ticket port charge book alternative sailing equivalent price receive additional onboard credit embarkation date end year spokesperson msc time writing msc splendida plan remain port duration cruise january february today front back page download newspaper order issue historic daily express newspaper archive\"]\n\n# Load the small English model\nnlp = spacy.load('en_core_web_sm')\n# Process a text\ndoc = nlp(text_example[0])\n# Iterate over the tokens\nfor token in doc:\n    # Print the text and the predicted part-of-speech tag\n    print(token.text, token.pos_)","b2258e86":"## Predicting Named Entities\n# Iterate over the predicted entities\nfor ent in doc.ents:\n    # Print the entity text and its label\n    print(ent.text, ent.label_)","d29d7bc0":"## Predicting Syntactic Dependencies\nfor token in doc:\n    print(token.text, token.pos_, token.dep_, token.head.text)","e900ca47":"## Document Similarity\n\n# Compare two documents\ndoc1 = nlp(text_example[0])\ndoc2 = nlp(text_example2[0])\nprint(doc1.similarity(doc2))","5c5b3c90":"# Data Preprocessing \ndef preprocess(txt):\n  '''\n  Take text pass through spacy's pipeline \n  Normalize text using remove stopwords from CUSTOM_STOPWORDS, take words which are RELEVENT_POS_TAGS and\n  take lemma and use that in smaller version of alphabet\n  '''\n  doc = nlp(txt)\n  rel_tokens = \" \".join([tok.lemma_.lower() for tok in doc if tok.pos_ in RELEVANT_POS_TAGS and tok.lemma_.lower() not in CUSTOM_STOPWORDS])\n  return rel_tokens","3e1906cd":"nlp = spacy.load('en_core_web_sm',disable=['parser','ner','tokenizer'])\n\n# from https:\/\/www.kaggle.com\/jannalipenkova\/covid-19-media-overview\nRELEVANT_POS_TAGS = [\"PROPN\", \"VERB\", \"NOUN\", \"ADJ\"]\n\nCUSTOM_STOPWORDS = [\"say\", \"%\", \"will\", \"new\", \"would\", \"could\", \"other\", \n                    \"tell\", \"see\", \"make\", \"-\", \"go\", \"come\", \"can\", \"do\", \n                    \"such\", \"give\", \"should\", \"must\", \"use\"]\n\ntqdm.pandas()\nprocessed_content = df[\"content\"].progress_apply(preprocess)\ndf[\"processed_content\"] = processed_content\n","7eeba6bc":"df.to_csv(\"processed_csv\", index=False)  # execute the 1\u00b0 time and save the file.  ","b9cb5639":"# df = pd.read_csv('.\/processed_csv.csv',index_col='Unnamed: 0')  # more fast, just read the processed file \n# df.head()","4428eed6":"reindexed_data = df['processed_content']","8d36fa28":"# Define helper functions\ndef get_top_n_words(n_top_words, count_vectorizer, text_data):\n    '''\n    returns a tuple of the top n words in a sample and their \n    accompanying counts, given a CountVectorizer object and text sample\n    '''\n    vectorized_headlines = count_vectorizer.fit_transform(text_data.values)\n    vectorized_total = np.sum(vectorized_headlines, axis=0)\n    word_indices = np.flip(np.argsort(vectorized_total)[0,:], 1)\n    word_values = np.flip(np.sort(vectorized_total)[0,:],1)\n    \n    word_vectors = np.zeros((n_top_words, vectorized_headlines.shape[1]))\n    for i in range(n_top_words):\n        word_vectors[i,word_indices[0,i]] = 1\n\n    words = [word[0].encode('ascii').decode('utf-8') for \n             word in count_vectorizer.inverse_transform(word_vectors)]\n\n    return (words, word_values[0,:n_top_words].tolist()[0])","210978d1":"count_vectorizer = CountVectorizer(stop_words='english')\nwords, word_values = get_top_n_words(n_top_words=15,\n                                     count_vectorizer=count_vectorizer, \n                                     text_data=reindexed_data)\n\nfig, ax = plt.subplots(figsize=(16,8))\nax.bar(range(len(words)), word_values);\nax.set_xticks(range(len(words)));\nax.set_xticklabels(words, rotation='vertical');\nax.set_title('Top words in the first 15 content articles (excluding stop words)');\nax.set_xlabel('Word');\nax.set_ylabel('Number of occurences');\nplt.show()","d1872e60":"order_by = df['topic_area'].value_counts().index\nsns.catplot(kind='count',x='topic_area',aspect=2,data=df,order=order_by)\nplt.show()\n","33e56386":"# Preparing a corpus for analysis and checking the first 2 entries\ncorpus=[]\ncorpus = df['processed_content'].to_list()","b6de3eb0":"corpus = list(set(corpus))\ncorpus[:2]","07ebf82f":"print('Corpus lenght, ',len(df['processed_content'].to_list()),' There is '+ str(len(corpus))+' unique content')","02d9c28c":"# Generating the wordcloud with the values under the category dataframe\ncorpus_graph_1 = list(set( df['processed_content'].to_list() ))\ncorpus_graph_2 = list(set( df['processed_content'].to_list() ))\ncorpus_graph_3 = list(set( df['processed_content'].to_list() ))\ncorpus_graph_4 = list(set( df['processed_content'].to_list() ))\ncorpus_graph_5 = list(set( df['processed_content'].to_list() ))","fc694594":"firstcloud = WordCloud(\n                          stopwords=STOPWORDS,\n                          background_color='black',\n                          width=4500,\n                          height=2800\n                         ).generate(\" \".join(corpus_graph_1))\nplt.imshow(firstcloud)\nplt.axis('off')\nplt.show()","c18afed0":"seccloud = WordCloud(\n                          stopwords=STOPWORDS,\n                          background_color='black',\n                          width=4500,\n                          height=2800\n                         ).generate(\" \".join(corpus_graph_2))\nplt.imshow(seccloud)\nplt.axis('off')\nplt.show()","fd6e3385":"tcloud = WordCloud(\n                          stopwords=STOPWORDS,\n                          background_color='black',\n                          width=4500,\n                          height=2800\n                         ).generate(\" \".join(corpus_graph_3))\nplt.imshow(tcloud)\nplt.axis('off')\nplt.show()","8a3d3273":"fourcloud = WordCloud(\n                          stopwords=STOPWORDS,\n                          background_color='black',\n                          width=4500,\n                          height=2800\n                         ).generate(\" \".join(corpus_graph_4))\nplt.imshow(fourcloud)\nplt.axis('off')\nplt.show()","f4bb4957":"fivecloud = WordCloud(\n                          stopwords=STOPWORDS,\n                          background_color='black',\n                          width=4500,\n                          height=2800\n                         ).generate(\" \".join(corpus_graph_5))\nplt.imshow(fivecloud)\nplt.axis('off')\nplt.show()","84e967b5":"TEMP_FOLDER = tempfile.gettempdir()\nprint('Folder \"{}\" will be used to save temporary dictionary and corpus.'.format(TEMP_FOLDER))\nlogging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)","a0455235":"# removing common words and tokenizing\nstoplist = stopwords.words('english') + list(punctuation) + list(\"([)]?\") + [\")?\"]\ntexts = [[word for word in str(document).lower().split() if word not in stoplist] for document in corpus]\ndictionary = corpora.Dictionary(texts)\ndictionary.save(os.path.join(TEMP_FOLDER, 'content_file.dict'))  # store the dictionary","84e3afcd":"corpus = [dictionary.doc2bow(text) for text in texts]\ncorpora.MmCorpus.serialize(os.path.join(TEMP_FOLDER, 'content_file.mm'), corpus) ","e7e1b30f":"tfidf = models.TfidfModel(corpus) # step 1 -- initialize a model","e90e7683":"corpus_tfidf = tfidf[corpus]  # step 2 -- use the model to transform vectors","580da0cb":"#I will try 15 topics\ntotal_topics = 15\n\nlda = models.LdaModel(corpus, id2word=dictionary, num_topics=total_topics)\ncorpus_lda = lda[corpus_tfidf] # create a double wrapper over the original corpus: bow->tf","9bcf05f4":"data_lda = {i: OrderedDict(lda.show_topic(i,25)) for i in range(total_topics)}","f33528e7":"df_lda = pd.DataFrame(data_lda)\ndf_lda = df_lda.fillna(0).T\nprint(df_lda.shape)","59563cd9":"HTML('<iframe width=\"900\" height=\"687\" src=\"https:\/\/www.youtube.com\/embed\/SF50IK5XgKA\" frameborder=\"0\" allow=\"accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen><\/iframe>')","df0a43c2":"pyLDAvis.enable_notebook()\npanel = pyLDAvis.gensim.prepare(lda, corpus_lda, dictionary, mds='mmds')\npanel","320e77f4":"pyLDAvis.enable_notebook()\npanel = pyLDAvis.gensim.prepare(lda, corpus_lda, dictionary, mds='tsne')\npanel","c258d7ee":"corpus_topic_modeling_2example = df['processed_content'].to_list()\ncorpus_topic_modeling_2example = list(set(corpus_topic_modeling_2example))","3e67465e":"## YOUR CODE HERE\n\nmax_features = 500\n# Create a CountVectorizer object\ntf_vectorizer = CountVectorizer(max_df=0.95, min_df=2,\n                                max_features=max_features,\n                                stop_words='english')\n# Fit and transform this object to the processed reviews\ntf = tf_vectorizer.fit_transform(corpus_topic_modeling_2example)\nprint(\"ready\")\nn_topics = 15\nlda_model = LatentDirichletAllocation(n_components=n_topics, max_iter=5,\n                                      learning_method='online',\n                                      learning_offset=50.,\n                                      random_state=0)\nlda_model.fit(tf)\nlda_transformed = lda_model.transform(tf)","4b411aa1":"def make_plot(lda_mat, n_components, alg):\n    \n    fig = plt.figure(figsize=(10,10), facecolor='white')\n    ax = fig.add_subplot(111)\n    if alg== 'TSNE':\n        tsne = TSNE(n_components=n_components, perplexity=10, init='pca')\n        projected = tsne.fit_transform(lda_mat)\n    if alg== 'Isomap':    \n        # Create instance\n        iso = Isomap(n_components=n_components)\n        # Fitting\n        iso.fit(lda_mat)\n        projected = iso.transform(lda_mat)        \n    if alg== 'PCA':    \n        pca = PCA(n_components=n_components)\n        projected = pca.fit_transform(lda_mat)\n    if alg== 'FactorAnalysis':    \n        projected = FactorAnalysis(n_components = n_components).fit_transform(lda_mat)\n    for class_num in np.arange(n_topics):\n        topic_inds = np.where(lda_mat[:, class_num] > 0.5)[0]\n        ax.scatter(projected[topic_inds, 0],\n                   projected[topic_inds, 1], \n                   edgecolor='none', marker='.', alpha=0.7, label=str(class_num))\n    plt.title('{} Components'.format(alg))\n    ax.set_xlabel('component 1')\n    ax.set_ylabel('component 2')\n    ax.legend()\n","7f845f42":"make_plot(lda_transformed, 2,'FactorAnalysis')","5d91c50f":"make_plot(lda_transformed, 2,'PCA')","63b4cb85":"make_plot(lda_transformed, 2, 'Isomap')","eb8c9fe0":"make_plot(lda_transformed, 2, 'TSNE')","d2d7f4dc":"### Word cloud ","53f3802c":"# End notebook","9f0e22ad":"### Creating a transformation","bb520d00":">> removing common words and tokenizing","15ba6f70":"# Read the Data","1f9f53e3":"# Import libs","a0a88e83":"#### Show first n important word in the topics","d2890777":"If you used `transform` on your original tokens you should have a `2000 x k` array where `k` is the number of topics you choose.  \nCreate a PCA, FA, Isomap or tSNE visualization that projects this matrix into lower dimensional space then uses colors to indicate which documents belong to a topic (e.g. probability > 0.5).","b03f2760":"Some notes:\n\n1. **[Multidimensional scaling (MDS)](https:\/\/en.wikipedia.org\/wiki\/Multidimensional_scaling)** is a means of visualizing the level of similarity of individual cases of a dataset. MDS is used to translate \"information about the pairwise 'distances' among a set of n objects or individuals\" into a configuration of n points mapped into an abstract [Cartesian space](https:\/\/en.wikipedia.org\/wiki\/Cartesian_coordinate_system)\n2. \n\n","d3366130":"## Using different MDS functions\nWith sklearn installed, other MDS functions, such as MMDS and TSNE can be used for plotting if the default PCA is not satisfactory.\n\n","9cbd1a37":"See this link: https:\/\/anacode.de\/wordpress\/wp-content\/uploads\/2020\/04\/covid19media_clusters5.html","400a92a0":"## Understanding spacy\n","66a02a40":"# LDA\nIn natural language processing, the latent Dirichlet allocation ([LDA](https:\/\/en.wikipedia.org\/wiki\/Latent_Dirichlet_allocation)) is a generative statistical model that allows sets of observations to be explained by unobserved groups that explain why some parts of the data are similar ","70fca76e":"Some references about topic modeling, usefull links:\n- [introduction factor analysis](https:\/\/www.datacamp.com\/community\/tutorials\/introduction-factor-analysis)\n- [dimensionality reduction techniques python](https:\/\/www.analyticsvidhya.com\/blog\/2018\/08\/dimensionality-reduction-techniques-python\/)\n- [pca practical guide principal component analysis python](https:\/\/www.analyticsvidhya.com\/blog\/2016\/03\/pca-practical-guide-principal-component-analysis-python\/)\n-  [Comparison of Latent Dirichlet Modeling and Factor Analysis for Topic Extraction: A Lesson of History](https:\/\/core.ac.uk\/download\/pdf\/143480918.pdf)\n- [case: covid19 starter media topic modeling](https:\/\/www.kaggle.com\/caesarlupum\/covid-19-starter-media-topic-modeling)\n- [case: covid19 media_clusters5](https:\/\/anacode.de\/wordpress\/wp-content\/uploads\/2020\/04\/covid19media_clusters5.html)\n        ","21f8711e":"Video about topic modeling:\n","7232874e":"## Topic Modelling\n\nIn machine learning and natural language processing, a [topic model](https:\/\/en.wikipedia.org\/wiki\/Topic_model) is a type of statistical model for discovering the abstract \"topics\" that occur in a collection of documents. Topic modeling is a frequently used text-mining tool for discovery of hidden semantic structures in a text body\n","2bcba222":"### Data Processing "}}