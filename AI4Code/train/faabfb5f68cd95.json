{"cell_type":{"6740f19e":"code","bbfdee50":"code","4fabbd41":"code","9683e82f":"code","4e373dc5":"code","65827760":"code","02397a8d":"code","cc82a64a":"code","8ecc1931":"code","f5bead5d":"code","ea18a23d":"code","f8f4f20e":"code","c8fadfbd":"code","e1bcc152":"code","bd984bef":"code","73bf65ed":"code","7f91899e":"code","530401d0":"code","7bd1db4f":"code","abfdfc73":"code","fcb7a5c2":"code","164c138f":"code","c4ab7174":"code","56bcb8c5":"code","7af79902":"code","c86bb7a6":"code","43e0dd27":"code","c352296d":"code","3ca6be2a":"code","a6a62a0d":"code","11c97a86":"code","5f022401":"code","e7036769":"code","d8637dac":"code","dde365a6":"code","39755902":"code","846942fa":"code","f1f1dc78":"code","d7ebd00d":"code","dd553301":"code","64ffa824":"code","ddd504eb":"code","00d3ba7a":"code","9e6dcff6":"code","a14ae253":"code","827f1324":"code","9bc36521":"code","bba712c6":"code","462b60b5":"code","4a6bcbc9":"code","298781f3":"code","d894bf4d":"code","cfb2680b":"code","c6013142":"code","125c11a6":"code","a783cd9c":"code","ea7c5f7b":"code","db9a8bf9":"code","39130b29":"code","e889f7fc":"code","f3eae984":"code","a9543a26":"code","56be1dcd":"code","c6d50e52":"code","986d3ca1":"code","b6ceb693":"code","708a6b3f":"code","011a4844":"code","ca5014f2":"code","32da73ed":"code","db7caede":"code","73068b95":"code","36ed806a":"code","af2893ce":"code","6100489a":"code","d0ff7a61":"code","1e5a6ee5":"code","8a069632":"code","3822cc77":"code","2539d9f4":"code","5757cb73":"code","40e4a039":"code","7ce56cea":"code","f562369c":"code","7902d499":"code","0657a1b2":"code","e30d174a":"code","0668c573":"code","125b76c5":"code","9db1a8c5":"code","f41c182c":"code","584cb59a":"code","63a73b0b":"code","74bc20b5":"code","03139427":"code","760d9360":"code","10f3b35e":"code","a315628e":"code","8f465405":"code","0291f211":"code","811d9b50":"code","39f865fe":"code","21704e97":"code","05b1d67c":"code","e1d74950":"code","4d0141d3":"code","f7b77d57":"code","a1162856":"code","df108d33":"code","6c0450b1":"code","98cae56a":"code","b28e23d7":"code","48e06ce3":"code","35a4bdf9":"code","d1375593":"code","8c6b6609":"markdown","caac4277":"markdown","bb92c882":"markdown","6fd5eed9":"markdown","2e93fa10":"markdown","20b9e248":"markdown","7f1fb7bf":"markdown","1451efa1":"markdown","76ed3b53":"markdown","c655fdc1":"markdown","cac6252f":"markdown","fb602bb0":"markdown","0a5a33ef":"markdown","12c47636":"markdown","55bc7d36":"markdown","eb975ada":"markdown","de80c751":"markdown","94f5860c":"markdown","df536f42":"markdown","902280ee":"markdown","f6a5ee92":"markdown","055b6e50":"markdown","bc06407d":"markdown","03b5eb50":"markdown","650876e5":"markdown","eb7d6ff6":"markdown","068b9978":"markdown","169d0f09":"markdown","b958d180":"markdown","83deb1bf":"markdown","8bee79e0":"markdown","c0a6fc37":"markdown","65d8068c":"markdown","5c1f4d72":"markdown","77819e75":"markdown","79d2241c":"markdown","52d39f96":"markdown","71c994cd":"markdown","3afc5199":"markdown","7867c590":"markdown","3255738a":"markdown","15952426":"markdown"},"source":{"6740f19e":"# Import all libraries for EDA & data reading\nimport warnings\nwarnings.filterwarnings('ignore')\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom matplotlib.pyplot import xticks\n%matplotlib inline","bbfdee50":"# Import all machine learning libraries\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MinMaxScaler\nimport statsmodels.api as sm\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\nfrom sklearn.metrics import r2_score\n\nfrom sklearn import metrics","4fabbd41":"# Reading the lead file\nleads=pd.DataFrame(pd.read_csv(\"..\/input\/leadsdataset1\/Leads.csv\"))\nleads.head()","9683e82f":"# Get the info of the dataset\nleads.info()","4e373dc5":"leads.shape","65827760":"leads.describe()","02397a8d":"# In data set there are many Select values given which should be treated as Null. It means user didn't share anything.\nleads=leads.replace('Select',np.nan)","cc82a64a":"# Analyze categorical columns which have same values in more than 90% rows (Skewed columns)\ncols=leads.columns[leads.eq('No').mean()>0.9]\ncols","8ecc1931":"leads.drop(cols, axis = 1, inplace = True)\nleads.info()","f5bead5d":"leads.drop(leads.columns[[0,1,23]], axis = 1, inplace = True)\nleads.head()","ea18a23d":"leads.info()","f8f4f20e":"# Get the % of missing or null data in the leads dataset\nmissing_data = round(100*(leads.isnull().sum()\/len(leads.index)),2)\nmissing_data","c8fadfbd":"# We can drop the columns which have more than 40% missing values (Lead profile, How did you hear about X eductaion, etc)\n# Before dropping, reviewed these columns & they are not critical for model building\nleads=leads.drop(leads.loc[:,list(missing_data >= 40)].columns,1)","e1bcc152":"leads.info()","bd984bef":"# Check missing data% again\nround(100*(leads.isnull().sum()\/len(leads.index)),2)","73bf65ed":"plt.figure(figsize=(20,10))\nplt.xticks(rotation=90,fontsize=20)\nsns.countplot(x=\"Country\",hue='Converted',data=leads)\nplt.xlabel('Country',fontsize = 30) \nplt.show()","7f91899e":"plt.figure(figsize=(20,10))\nplt.xticks(rotation=90,fontsize=20)\nsns.countplot(x=\"Specialization\",hue='Converted',data=leads)\nplt.xlabel('Specialization',fontsize = 30) \nplt.show()","530401d0":"plt.figure(figsize=(15,5))\nplt.xticks(rotation=90,fontsize=12)\nsns.countplot(x=\"City\",hue='Converted',data=leads)\nplt.xlabel('City',fontsize = 10) \nplt.show()","7bd1db4f":"plt.figure(figsize=(15,5))\nplt.xticks(rotation=90,fontsize=12)\nsns.countplot(x=\"Tags\",hue='Converted',data=leads)\nplt.xlabel('Tags',fontsize = 10) \nplt.show()","abfdfc73":"plt.figure(figsize=(10,5))\nplt.xticks(rotation=90,fontsize=10)\nsns.countplot(x=\"What is your current occupation\",hue='Converted',data=leads)\nplt.xlabel('Occupation',fontsize = 10) \nplt.show()","fcb7a5c2":"plt.figure(figsize=(10,5))\nplt.xticks(fontsize=10)\nsns.countplot(x=\"What matters most to you in choosing a course\",hue='Converted',data=leads)\nplt.xlabel('Reason',fontsize = 11) \nplt.show()","164c138f":"# Lets drop the columns which are not required as per above analysis\nleads=leads.drop(leads.loc[:,('Country','City','What is your current occupation','Specialization','Tags',\n                 'What matters most to you in choosing a course')].columns,1)\nleads.head()","c4ab7174":"leads.shape","56bcb8c5":"# Check missing data again\nleads.isnull().sum()","7af79902":"leads = leads.dropna(subset=['TotalVisits'])","c86bb7a6":"leads.head()","43e0dd27":"leads[\"Lead Source\"].fillna(\"Google\", inplace = True)","c352296d":"# Check missing data again\nleads.isnull().sum()","3ca6be2a":"leads.info()","a6a62a0d":"# Lets check the outliers\n# Checking for outliers in the continuous variables\nleads_num = leads[['TotalVisits','Total Time Spent on Website','Page Views Per Visit']]\nleads_num.describe(percentiles=[.25, .5, .75, .90, .95, .99])","11c97a86":"# Reviewing the outliers using box plot\nplt.figure(figsize=[15,10])\nplt.subplot(2,2,1)\nplt.boxplot(\"TotalVisits\",data=leads)\nplt.subplot(2,2,2)\nplt.boxplot(\"Total Time Spent on Website\",data=leads)\nplt.subplot(2,2,3)\nplt.boxplot(\"Page Views Per Visit\",data=leads)\nplt.show()","5f022401":"sns.pairplot(leads, vars=leads.columns[3:6])\nplt.show()","e7036769":"leads.info()","d8637dac":"# Same thing lets visualize using box plot\nplt.figure(figsize=[45,60])\nplt.subplot(2,2,1)\nplt.xticks(fontsize=20)\nsns.countplot(x=\"Lead Origin\",hue='Converted',data=leads)\nplt.xlabel('Lead Origin',fontsize = 30) \nplt.subplot(2,2,2)\nplt.xticks(rotation=90,fontsize=30)\nsns.countplot(x=\"Lead Source\",hue='Converted',data=leads)\nplt.xlabel('Lead Source',fontsize = 30) \nplt.subplot(2,2,3)\nplt.xticks(rotation=90,fontsize=30)\nsns.countplot(x=\"Last Activity\",hue='Converted',data=leads)\nplt.xlabel('Last Activity',fontsize = 30) \nplt.subplot(2,2,4)\nplt.xticks(fontsize=20)\nsns.countplot(x=\"A free copy of Mastering The Interview\",hue='Converted',data=leads)\nplt.xlabel('Interview Free copy',fontsize = 30) \nplt.subplots_adjust(hspace=0.3)\nplt.show()","dde365a6":"leads[\"Lead Origin\"] = leads[\"Lead Origin\"].replace([\"Lead Import\", \"Quick Add Form\"],'Lead Add Form')\nleads[\"Lead Origin\"].value_counts()","39755902":"leads[\"Lead Source\"] = leads[\"Lead Source\"].replace([\"Reference\",\"Welingak Website\",\"Referral Sites\",\"Facebook\",\"bing\",\"Click2call\",\"Live Chat\",\"Press_Release\",\"Social Media\",\"NC_EDM\",\"blog\",\"testone\",\"welearnblog_Home\",\"Pay per Click Ads\",\"youtubechannel\",\"WeLearn\"],\"Source_others\")\nleads[\"Lead Source\"] = leads[\"Lead Source\"].replace(\"google\",\"Google\")\nleads[\"Lead Source\"].value_counts()","846942fa":"leads['Last Activity']=leads['Last Activity'].replace([\"Converted to Lead\",\"Email Bounced\",\"Email Link Clicked\",\"Form Submitted on Website\",\"Unreachable\",\"Unsubscribed\",\"Had a Phone Conversation\",\"View in browser link Clicked\",\"Approached upfront\",\"Email Marked Spam\",\"Email Received\",\"Visited Booth in Tradeshow\",\"Resubscribed to emails\"],\"LA_Others\")\nleads['Last Activity'].value_counts()","f1f1dc78":"leads.info()","d7ebd00d":"dummy1 = pd.get_dummies(leads[[\"Lead Origin\",\"Lead Source\",\"Last Activity\"]], drop_first=True)","dd553301":"dummy1.head()","64ffa824":"leads = pd.concat([leads, dummy1], axis=1)\nleads.head()","ddd504eb":"leads.drop([\"Lead Origin\",\"Lead Source\",\"Last Activity\"], axis=1 , inplace=True)","00d3ba7a":"leads.head()","9e6dcff6":"leads[\"A free copy of Mastering The Interview\"]=leads[\"A free copy of Mastering The Interview\"].apply(lambda x: 1 if x== \"Yes\" else (0 if x == \"No\" else \"NA\"))\nleads[\"A free copy of Mastering The Interview\"].value_counts()","a14ae253":"X = leads.drop([\"Converted\"], axis=1)","827f1324":"X.head()","9bc36521":"Y = leads[\"Converted\"]","bba712c6":"Y.head()","462b60b5":"# Splitting the data into train and test\nX_train, X_test, y_train, y_test = train_test_split(X, Y, train_size=0.7, test_size=0.3, random_state=100)","4a6bcbc9":"X_train.info()","298781f3":"scaler = MinMaxScaler()\nnum_vars = [\"TotalVisits\",\"Total Time Spent on Website\",\"Page Views Per Visit\"]\n\nX_train[num_vars] = scaler.fit_transform(X_train[num_vars])\nX_train.head()","d894bf4d":"#Checking conversion rate\nconv= (sum(leads[\"Converted\"])\/len(leads[\"Converted\"].index))*100\nconv","cfb2680b":"# Let's see the correlation matrix \nplt.figure(figsize = (20,10))        # Size of the figure\nsns.heatmap(leads.corr(),annot = True)\nplt.show()","c6013142":"X_test = X_test.drop(['Lead Source_Source_others'], 1)\nX_train = X_train.drop(['Lead Source_Source_others'], 1)","125c11a6":"plt.figure(figsize = (20,10))        # Size of the figure\nsns.heatmap(X_train.corr(),annot = True)\nplt.show()","a783cd9c":"# Logistic regression model\nlogm1 = sm.GLM(y_train,(sm.add_constant(X_train)), family = sm.families.Binomial())\nlogm1.fit().summary()","ea7c5f7b":"from sklearn.linear_model import LogisticRegression\nlogreg = LogisticRegression()","db9a8bf9":"from sklearn.feature_selection import RFE\nrfe = RFE(logreg, 10)             # running RFE with 10 variables as output\nrfe = rfe.fit(X_train, y_train)","39130b29":"rfe.support_","e889f7fc":"list(zip(X_train.columns, rfe.support_, rfe.ranking_))","f3eae984":"col = X_train.columns[rfe.support_]","a9543a26":"X_train.columns[~rfe.support_]","56be1dcd":"X_train_sm = sm.add_constant(X_train[col])\nlogm2 = sm.GLM(y_train,X_train_sm, family = sm.families.Binomial())\nres = logm2.fit()\nres.summary()","c6d50e52":"y_train_pred = res.predict(X_train_sm)\ny_train_pred[:10]","986d3ca1":"y_train_pred = y_train_pred.values.reshape(-1)\ny_train_pred[:10]","b6ceb693":"y_train_pred_final = pd.DataFrame({'Converted':y_train.values, 'Converted_Prob':y_train_pred})\ny_train_pred_final['LeadID'] = y_train.index\ny_train_pred_final.head()","708a6b3f":"y_train_pred_final['predicted'] = y_train_pred_final.Converted_Prob.map(lambda x: 1 if x > 0.5 else 0)\n\n# Let's see the head\ny_train_pred_final.head()","011a4844":"# Confusion matrix \nconfusion = metrics.confusion_matrix(y_train_pred_final.Converted, y_train_pred_final.predicted )\nprint(confusion)","ca5014f2":"# Print accuracy (Correctly predicted variables \/ Total no of labels )\nprint(metrics.accuracy_score(y_train_pred_final.Converted, y_train_pred_final.predicted))","32da73ed":"vif = pd.DataFrame()\nvif['Features'] = X_train[col].columns\nvif['VIF'] = [variance_inflation_factor(X_train[col].values, i) for i in range(X_train[col].shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","db7caede":"TP = confusion[1,1] # true positive \nTN = confusion[0,0] # true negatives\nFP = confusion[0,1] # false positives\nFN = confusion[1,0] # false negatives","73068b95":"# Let's see the sensitivity of our logistic regression model \nTP \/ float(TP+FN)","36ed806a":"# Let us calculate specificity\nTN \/ float(TN+FP)","af2893ce":"# Calculate false postive rate - predicting churn when customer does not have churned\nprint(FP\/ float(TN+FP))","6100489a":"# positive predictive value \nprint (TP \/ float(TP+FP))","d0ff7a61":"# Negative predictive value\nprint (TN \/ float(TN+ FN))","1e5a6ee5":"def draw_roc( actual, probs ):\n    fpr, tpr, thresholds = metrics.roc_curve( actual, probs,\n                                              drop_intermediate = False )\n    auc_score = metrics.roc_auc_score( actual, probs )\n    plt.figure(figsize=(5, 5))\n    plt.plot( fpr, tpr, label='ROC curve (area = %0.2f)' % auc_score )\n    plt.plot([0, 1], [0, 1], 'k--')\n    plt.xlim([0.0, 1.0])\n    plt.ylim([0.0, 1.05])\n    plt.xlabel('False Positive Rate or [1 - True Negative Rate]')\n    plt.ylabel('True Positive Rate')\n    plt.title('Receiver operating characteristic example')\n    plt.legend(loc=\"lower right\")\n    plt.show()\n\n    return None","8a069632":"fpr, tpr, thresholds = metrics.roc_curve( y_train_pred_final.Converted, y_train_pred_final.Converted_Prob, drop_intermediate = False )","3822cc77":"draw_roc(y_train_pred_final.Converted, y_train_pred_final.Converted_Prob)","2539d9f4":"numbers = [float(x)\/10 for x in range(10)]\nfor i in numbers:\n    y_train_pred_final[i]= y_train_pred_final.Converted_Prob.map(lambda x: 1 if x > i else 0)\ny_train_pred_final.head()","5757cb73":"# Calculating accuracy sensitivity and specificity for various probability cutoffs.\ncutoff_df = pd.DataFrame( columns = ['prob','accuracy','sensi','speci'])\nfrom sklearn.metrics import confusion_matrix\n\n# TP = confusion[1,1] # true positive \n# TN = confusion[0,0] # true negatives\n# FP = confusion[0,1] # false positives\n# FN = confusion[1,0] # false negatives\n\nnum = [0.0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9]\nfor i in num:\n    cm1 = metrics.confusion_matrix(y_train_pred_final.Converted, y_train_pred_final[i] )\n    total1=sum(sum(cm1))\n    accuracy = (cm1[0,0]+cm1[1,1])\/total1\n    \n    speci = cm1[0,0]\/(cm1[0,0]+cm1[0,1])\n    sensi = cm1[1,1]\/(cm1[1,0]+cm1[1,1])\n    cutoff_df.loc[i] =[ i ,accuracy,sensi,speci]\nprint(cutoff_df)","40e4a039":"cutoff_df.plot.line(x='prob', y=['accuracy','sensi','speci'])\nplt.grid()\nplt.show()","7ce56cea":"y_train_pred_final['final_predicted'] = y_train_pred_final.Converted_Prob.map( lambda x: 1 if x > 0.33 else 0)\n\ny_train_pred_final.head()","f562369c":"#Calculating final accuracy\nmetrics.accuracy_score(y_train_pred_final.Converted, y_train_pred_final.final_predicted)","7902d499":"conf2 = metrics.confusion_matrix(y_train_pred_final.Converted, y_train_pred_final.final_predicted )\nconf2","0657a1b2":"TP = conf2[1,1] # true positive \nTN = conf2[0,0] # true negatives\nFP = conf2[0,1] # false positives\nFN = conf2[1,0] # false negatives","e30d174a":"# Calculating sensitivity\nTP \/ float(TP+FN)","0668c573":"# Calculating specificity\nTN \/ float(TN+FP)","125b76c5":"# Calculating false postive rate - predicting churn when customer does not have churned\nprint(FP\/ float(TN+FP))","9db1a8c5":"# Positive predictive value \nprint (TP \/ float(TP+FP))","f41c182c":"# Negative predictive value\nprint (TN \/ float(TN+ FN))","584cb59a":"y_train_pred_final.head() ","63a73b0b":"y_train_pred_final[\"Lead_score\"] = round((y_train_pred_final.Converted_Prob * 100),2)\ny_train_pred_final.head()","74bc20b5":"X_test[num_vars] = scaler.transform(X_test[num_vars])\nX_test.head()","03139427":"X_test = X_test[col]\nX_test.head()","760d9360":"X_test_sm = sm.add_constant(X_test)","10f3b35e":"y_test_pred = res.predict(X_test_sm)","a315628e":"y_pred_1 = pd.DataFrame(y_test_pred)","8f465405":"y_pred_1.head()","0291f211":"y_test_df = pd.DataFrame(y_test)","811d9b50":"y_test_df['CustID'] = y_test_df.index","39f865fe":"\ny_pred_1.reset_index(drop=True, inplace=True)\ny_test_df.reset_index(drop=True, inplace=True)","21704e97":"y_pred_final = pd.concat([y_test_df, y_pred_1],axis=1)","05b1d67c":"y_pred_final.head()","e1d74950":"y_pred_final= y_pred_final.rename(columns={ 0 : 'Converted_Prob'})","4d0141d3":"y_pred_final.head()","f7b77d57":"y_pred_final = y_pred_final.rename(columns={\"CustID\" : \"LeadID\"})","a1162856":"y_pred_final.head()","df108d33":"y_pred_final['final_predicted'] = y_pred_final.Converted_Prob.map(lambda x: 1 if x > 0.33 else 0)","6c0450b1":"y_pred_final.head()","98cae56a":"#Calculating accuracy\nmetrics.accuracy_score(y_pred_final.Converted, y_pred_final.final_predicted)","b28e23d7":"confusion2 = metrics.confusion_matrix(y_pred_final.Converted, y_pred_final.final_predicted )\nconfusion2","48e06ce3":"TP = confusion2[1,1] # true positive \nTN = confusion2[0,0] # true negatives\nFP = confusion2[0,1] # false positives\nFN = confusion2[1,0] # false negatives","35a4bdf9":"#Calculating sensitivity\nTP \/ float(TP+FN)","d1375593":"# Calculating specificity\nTN \/ float(TN+FP)","8c6b6609":"# Delete the unwanted columns ","caac4277":"### Checking VIFs","bb92c882":"#### Create confusion matrix to view the difference between actual leads & predicted leads","6fd5eed9":"We have 38% lead conversion rate","2e93fa10":"#### From the above curve the optimum point is at 0.33.","20b9e248":"Most of the leads are from Maharashtra & especially from Mumbai so we can drop this column  \nOther values are very generic. This column doesn't look very significant","7f1fb7bf":"#### Dropping the original columns","1451efa1":"From above confusion matrix we can see that   \n  1: 800 people have actually converted into leads but model has predicted them No  \n  2: 1619 people have actually converted & model also predicted them as Yes     \n  3: 501 have actually not converted but model have predicted them as converted    \n  4: 3452 have not converted & model has also predicted them as No   ","76ed3b53":"We will start with 0.5 probability to convert it into good leads.","c655fdc1":"1: Drop all the above cols as 90% rows have value \"No\".   \n2: In addition drop these columns also  \n     Prospect ID  \n     Lead number (Not relevant)             \n3: Last Notable Activity - Last activity & last notable activity have same information. One of them could be dropped.","cac6252f":"### Model Building ","fb602bb0":"### Feature selection using RFE","0a5a33ef":"Delete this column also as data is skewed.   \nRevert after reading email & ringing are 2 prominent values","12c47636":"# Lets review the columns which have more than 25% missing values & drop or imput them if required","55bc7d36":"##### All the VIF vales are under threshold, so there is no need to drop any more variables.","eb975ada":"## Model Evaluation","de80c751":"#### Checking the Correlation Matrix","94f5860c":"## Sensitivity is 67% which is low. As per requirement it should be around 80%.  \n Model predicted 67% of the overall leads. ","df536f42":"Drop the highly corelated variables ","902280ee":"## Above model has no high p values ","f6a5ee92":"### Plotting the ROC curve","055b6e50":"### Making predictions on test data set","bc06407d":"### Accuracy is 79.6% ","03b5eb50":"Specialization column is not giving clear information to impute & 37% of records are blank. We can drop this column as well.","650876e5":"After dropping highly correlated variables now let's check the correlation matrix again.","eb7d6ff6":"# Let have a look at Correlations","068b9978":"### Finding Optimal Cutoff Point","169d0f09":"## Sensitivity 80.5%\n## Specifivity 76.2%","b958d180":"# Step 1: Reading & cleaning Data","83deb1bf":"#### Converting Yes and No to 0 and 1","8bee79e0":"Mostly leads are from India so company should focus on Indian customers. We can drop this column.","c0a6fc37":"There are outliers in 2 columns columns.   \n1: Total visits have mx value as 251 whereas 99% data has 17  \n2: Pages views per visit is 55 whereas 75% of records have 5","65d8068c":"## Replace the uncommon values with common values","5c1f4d72":"We can drop records having Total Visits blank. This will remove missing values from Last Activity & pages views per visit","77819e75":"### Feature Scaling","79d2241c":"### Preparing data for modelling ( creating dummy variables )","52d39f96":"### Test train split","71c994cd":"### Calculating Lead score","3afc5199":"Clearly leads have value \"Better Career Prospects\" which is too evident. This column could be dropped","7867c590":"### Creating a dataframe with actual converted flag and predicted probabilities","3255738a":"Now we don't have any missing values.","15952426":"We can drop this column as most data is unemployed. Information maybe not correctly updated at source."}}