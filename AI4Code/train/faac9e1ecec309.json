{"cell_type":{"2489aae8":"code","bfd0d194":"code","949b4334":"code","4205fc5b":"code","4ef17d7c":"code","d8e25768":"code","3cbe6fdd":"code","a98b6b43":"code","9c4e86a5":"code","c68ffb1c":"code","1dc69b02":"code","48abb543":"code","67f5f419":"code","b7e56e25":"code","4b547892":"code","29b81ca8":"code","ed0838d6":"code","bd16c0a5":"code","ebf624f7":"code","35691d9b":"code","08b789b4":"code","76a72d41":"code","9cbf021f":"code","0ea61186":"markdown","654f40d3":"markdown","6f956da2":"markdown","44dcd94c":"markdown","8a50cf80":"markdown","f0de28bb":"markdown","64052db0":"markdown","174cba63":"markdown","4750ea75":"markdown","08eac7c4":"markdown","d47f6ab3":"markdown","4a065add":"markdown"},"source":{"2489aae8":"from IPython.display import clear_output\n!pip install livelossplot\nclear_output()","bfd0d194":"# Data Handling tools\nimport os\nimport cv2\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n# Tools to analyze the model\nfrom sklearn.metrics import confusion_matrix\n\n# import tensorflow\nimport tensorflow as tf\n\n# TensorFLow Tools\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras import Sequential\nfrom tensorflow.keras.layers import Dense, Conv2D, Flatten, Dropout, MaxPool2D\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.callbacks import ModelCheckpoint\nfrom livelossplot.tf_keras import PlotLossesCallback","949b4334":"print(\"We are using the TensorFlow version {}\".format(tf.__version__))","4205fc5b":"# get all the directories of the data\ntrain_dir = \"..\/input\/seti-data\/primary_small\/train\/\"\nvalid_dir = \"..\/input\/seti-data\/primary_small\/valid\/\"\ntest_dir = \"..\/input\/seti-data\/primary_small\/test\/\"\nclasses = [\"brightpixel\", \"narrowband\",\"narrowbanddrd\",\"noise\",\"squarepulsednarrowband\",\"squiggle\",\"squigglesquarepulsednarrowband\"]","4ef17d7c":"# Get the train Data and corresponding labels\nX_train = list()\ny_train = list()\nfor class_ in classes:\n    path = os.path.join(train_dir, class_)\n    label = classes.index(class_)\n    for img_name in os.listdir(path):\n        \n        # 3 channel image\n        img_3c = cv2.imread(os.path.join(path,img_name))\n        \n        # 1 channel image i.e grayscale\n        img = np.array(cv2.cvtColor(img_3c, cv2.COLOR_BGR2GRAY))\n        \n        # Shaped Image\n        img = cv2.resize(img,(256,256))\n        y_train.append(label)\n        X_train.append(img)   ","d8e25768":"# Convert to numpy arrays\nX_train = np.array(X_train)\ny_train = np.array(y_train)","3cbe6fdd":"# Shuffling the data\nnp.random.seed(0)\nindex_train = np.arange(len(y_train))\nnp.random.shuffle(index_train)\nX_train = X_train[index_train]\ny_train = y_train[index_train]","a98b6b43":"# Let's have a look at the first image from the data and the corresponding label\nplt.imshow(X_train[0])\nplt.title(classes[int(y_train[0])])","9c4e86a5":"# Now, let's have a look at each of the type of radio signal in our data\n# Let's get the first indices for the first occurences of each of the label\n\nclasses_index = np.arange(len(classes)) # contains number of each of the class\nindex = list()                          # will contain the index for first occurence of each class\nfor class_ in classes_index:\n    for position, label in enumerate(y_train):\n        if label == class_:\n            index.append(position)\n            break\n            ","c68ffb1c":"# Let's plot the images in a subplot to get some idea about each of the class\nplt.figure(figsize=(12,12))\nfor i, j in enumerate(index):\n    plt.subplot(3,3,i+1)\n    img = X_train[int(j)]\n    plt.xticks([])\n    plt.yticks([])\n    plt.title(classes[int(y_train[int(j)])])\n    plt.imshow(img)","1dc69b02":"print(\"The shape of the X_train: {}\".format(X_train.shape))\nprint(\"The shape of the y_train: {}\".format(y_train.shape))","48abb543":"# Now, we reshape our images to make it compatible with TensorFlow\n\nX_train = X_train.reshape(X_train.shape[0],*(256,256,1))","67f5f419":"print(\"The shape of the X_train: {}\".format(X_train.shape))","b7e56e25":"# Get the validation Data and corresponding labels\nX_valid = list()\ny_valid = list()\nfor class_ in classes:\n    path = os.path.join(valid_dir, class_)\n    label = classes.index(class_)\n    for img_name in os.listdir(path):\n        \n        # 3 channel image\n        img_3c = cv2.imread(os.path.join(path,img_name))\n        \n        # 1 channel image i.e grayscale\n        img = np.array(cv2.cvtColor(img_3c, cv2.COLOR_BGR2GRAY))\n        \n        # Shaped Image\n        img = cv2.resize(img,(256,256))\n        \n        y_valid.append(label)\n        X_valid.append(img) \n        \n# Convert to numpy arrays\nX_valid = np.array(X_valid)\ny_valid = np.array(y_valid)\n\n# Shuffling the data\nnp.random.seed(0)\nindex_val = np.arange(len(y_valid))\nnp.random.shuffle(index_val)\nX_valid = X_valid[index_val]\ny_valid = y_valid[index_val]\n\n# Reshaping images to make it compatible for TensorFlow\nX_valid = X_valid.reshape(X_valid.shape[0],*(256,256,1))","4b547892":"# Get the validation Data and corresponding labels\nX_test = list()\ny_test = list()\nfor class_ in classes:\n    path = os.path.join(test_dir, class_)\n    label = classes.index(class_)\n    for img_name in os.listdir(path):\n        \n        # 3 channel image\n        img_3c = cv2.imread(os.path.join(path,img_name))\n        \n        # 1 channel image i.e grayscale\n        img = np.array(cv2.cvtColor(img_3c, cv2.COLOR_BGR2GRAY))\n        \n        # Shaped Image\n        img = cv2.resize(img,(256,256))\n        \n        y_test.append(label)\n        X_test.append(img) \n        \n# Convert to numpy arrays\nX_test = np.array(X_test)\ny_test = np.array(y_test)\n\n# Shuffling the data\nnp.random.seed(0)\nindex_test = np.arange(len(y_test))\nnp.random.shuffle(index_test)\nX_test = X_test[index_test]\ny_test = y_test[index_test]\n\n# Reshaping images to make it compatible for TensorFlow\nX_test = X_test.reshape(X_test.shape[0],*(256,256,1))","29b81ca8":"# Normalizing the images \nX_train = X_train\/255.0\nX_valid = X_valid\/255.0\nX_test = X_test\/255.0","ed0838d6":"# Get the Train Data\n\ntrain_data = tf.keras.preprocessing.image_dataset_from_directory(\n    train_dir,\n    seed=0,\n    image_size= (256, 256),\n    batch_size=64\n\n)","bd16c0a5":"# Get the Validation Data\nvalidation_data = tf.keras.preprocessing.image_dataset_from_directory(\n    valid_dir,\n    seed=0,\n    image_size= (256, 256),\n    batch_size=64\n)","ebf624f7":"test_data = tf.keras.preprocessing.image_dataset_from_directory(\n    test_dir,\n    seed=0,\n    image_size= (256, 256),\n    batch_size=64\n)","35691d9b":"# Optimizing\nAUTOTUNE = tf.data.experimental.AUTOTUNE\ntrain_data = train_data.cache().prefetch(buffer_size=AUTOTUNE)\nvalidation_data = validation_data.cache().prefetch(buffer_size=AUTOTUNE)\ntest_data = test_data.cache().prefetch(buffer_size=AUTOTUNE)","08b789b4":"CNN_model = Sequential([\n    \n                        #Normalizing\n                        tf.keras.layers.experimental.preprocessing.Rescaling(1.\/255),\n    \n                        #First Convolution Layer\n                        Conv2D(filters=32,kernel_size=(3,3),\n                               activation=tf.keras.layers.LeakyReLU(alpha=0.01),\n                               kernel_initializer='he_normal',\n                               input_shape=(256,256,1)),\n                        \n                        MaxPool2D(pool_size=(2,2)),\n                        Dropout(0.5),\n\n                        #Second Convolution Layer\n                        Conv2D(filters=64,kernel_size=(3,3),\n                               kernel_initializer='he_normal',\n                               activation='relu'),\n                        \n                        MaxPool2D(pool_size=(2,2)),\n                        Dropout(0.2),\n\n                        #Third Convolution Layer\n                        Conv2D(filters=128,kernel_size=(3,3),\n                               kernel_initializer='he_normal',\n                               activation='relu'),\n                        \n                        MaxPool2D(pool_size=(2,2)),\n                        Dropout(0.2),\n\n                        #Flattening the output from last conv layer\n                        Flatten(),\n\n                        #Using Feed Forward NN as final layers for Classification\n                        #Feed Forward Layer 1\n                        Dense(128, activation='relu'),\n                        Dropout(0.2),\n\n                        #Feed Forward Layer 2\n                        Dense(64, activation='relu'),\n\n                        #Feed Forward Layer 3\n                        Dense(32, activation='relu'),\n\n                        #Output Layer\n                        Dense(7, 'softmax')\n])","76a72d41":"# Define Loss Function and Batch Size\nloss_function = 'sparse_categorical_crossentropy'\nbatch_size = 32\n\n# Decaying Learning Rate\nlearning_rate = 3e-4\n\n# Compile the Model\nCNN_model.compile(\n    loss = loss_function,\n    optimizer = Adam(lr=learning_rate),\n    metrics = [\"accuracy\"]\n)","9cbf021f":"history = CNN_model.fit(train_data,\n                        validation_data=validation_data,\n                        epochs=50)","0ea61186":"### Prepare the Train Data","654f40d3":"## Train the Model","6f956da2":"## Import Libraries ","44dcd94c":"## Prepare the Train, Validation, Test Data Manually","8a50cf80":"## Get the data with tf.keras","f0de28bb":"## Develop the CNN Model with Tensorflow 2","64052db0":"### Prepare Validation and Test Data","174cba63":"Therefore, we can clearly see that the difference between Training Accuracy and Validation Accuracy is less which suggests that a deeper CNN Network can be used to perform the task classification.","4750ea75":"# Radio Signal Classification with SETI & Tensorflow","08eac7c4":"### Define the model","d47f6ab3":"### Compile the Model","4a065add":"## Evaluation"}}