{"cell_type":{"c296d74c":"code","156adcf1":"code","5b8fdc31":"code","58a817e1":"code","3655f514":"code","49295de0":"code","1a5bab06":"code","deee5e56":"code","c1f3685c":"code","8f130df7":"code","d82b4e0b":"code","4e53a2ee":"code","d64245df":"code","03d3fc08":"code","da668c26":"code","58eaad3c":"code","d3f05991":"code","b7905e20":"code","b7bcbce8":"code","b73ecc63":"code","787fed5f":"code","0ef0f355":"code","015e1588":"code","a9cfb30e":"code","dc9948ff":"code","9365a754":"code","b1e76a20":"code","c62cddde":"code","8da70a18":"code","cba678d3":"code","d1ab03ce":"code","ef4c9cba":"code","83ae7d8b":"code","37dba870":"code","28e15be5":"code","a08128ce":"code","fa2a2a0e":"code","687f1bd1":"code","b3fe9e4c":"code","51ae28d8":"code","f2b9726b":"code","3e4524b8":"code","0bdba25d":"code","b6d12106":"code","6d991cf0":"code","e8043ed7":"code","74a6388c":"code","5a850464":"code","83c44c37":"code","f20b2df5":"code","4079deb2":"code","ac22efe8":"code","dae5caa2":"code","55ed2d4b":"code","e9eef9e6":"code","f6227c3b":"code","e317bf5d":"markdown","eb4b70b2":"markdown","0918b948":"markdown","e2682cf7":"markdown","c52681a1":"markdown","b5aa95ff":"markdown","6ec76ffc":"markdown","b7b19e11":"markdown","140dcbf1":"markdown","e5b9f6c3":"markdown","3f09607b":"markdown","6a41e34a":"markdown","d88ab883":"markdown","ebc5e981":"markdown","56d62cc8":"markdown","d295688e":"markdown","09a5a9fe":"markdown","474e264c":"markdown","f85a8c65":"markdown","03c4ee4e":"markdown","f98b1cb6":"markdown","338bc1e1":"markdown","3556bc10":"markdown","af0745ca":"markdown","49a27560":"markdown","02d5c40f":"markdown","35bfd35e":"markdown","11ca901a":"markdown","1307dc06":"markdown","30cec1ef":"markdown","be718448":"markdown"},"source":{"c296d74c":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport pandas_profiling\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.metrics import roc_curve, auc\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.metrics import confusion_matrix\nimport sklearn.metrics as mt\n%matplotlib inline\nimport warnings\nwarnings.filterwarnings('ignore')\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","156adcf1":"df = pd.read_csv('\/kaggle\/input\/womens-ecommerce-clothing-reviews\/Womens Clothing E-Commerce Reviews.csv')","5b8fdc31":"df.head()","58a817e1":"df.shape","3655f514":"df.info()","49295de0":"df.describe()","1a5bab06":"df.isnull().sum()","deee5e56":"df.groupby(['Rating', 'Recommended IND'])['Recommended IND'].count()","c1f3685c":"df1=df.drop(['Unnamed: 0','Division Name','Department Name',],axis=1)\ndf1.columns","8f130df7":"df1.head()","d82b4e0b":"df1[df1['Review Text'].isnull()]","4e53a2ee":"df1 = df1[~df1['Review Text'].isnull()]","d64245df":"df1.shape","03d3fc08":"df1.head(77)","da668c26":"import plotly.offline as py\nimport plotly.graph_objs as go\nx=df1['Recommended IND'].value_counts()\ncolors = ['#FEBFB3', '#E1396C']\n\ntrace=go.Pie(labels=x.index,values=x,textinfo=\"value\",\n            marker=dict(colors=colors, \n                           line=dict(color='#000000', width=2)))\nlayout=go.Layout(title=\"Cloths are Recommended or not\",width=500,height=500)\nfig=go.Figure(data=[trace],layout=layout)\npy.iplot(fig, filename='pie_chart_subplots')","58eaad3c":"import plotly.express as px\nfig = px.histogram(df1, x=df1['Rating'], nbins=10)\nfig.show()","d3f05991":"\nfig = px.histogram(df1, x = df1['Class Name'])\nfig.show()","b7905e20":"df1['review_len'] = df1['Review Text'].astype(str).apply(len)","b7bcbce8":"px.histogram(df1, x = 'review_len')","b73ecc63":"df1['token_count'] = df1['Review Text'].apply(lambda x: len(str(x).split()))","787fed5f":"px.histogram(df1, x = 'token_count')","0ef0f355":"!pip install TextBlob\nfrom textblob import *","015e1588":"df1['polarity'] = df1['Review Text'].map(lambda text: TextBlob(text).sentiment.polarity)\ndf1['polarity']","a9cfb30e":"fig = px.histogram(df1, x = df1['polarity'])\nfig.show()","dc9948ff":"pop = df1.loc[df1.polarity == 1,['Review Text']].sample(3).values\nfor i in pop:\n    print(i[0])","9365a754":"pop = df1.loc[df1.polarity == 0.5,['Review Text']].sample(3).values\nfor i in pop:\n    print(i[0])","b1e76a20":"pop = df1.loc[df1.polarity < 0,['Review Text']].sample(3).values\nfor i in pop:\n    print(i[0])","c62cddde":"negative = (len(df1.loc[df1.polarity <0,['Review Text']].values)\/len(df1))*100\npositive = (len(df1.loc[df1.polarity >0.5,['Review Text']].values)\/len(df1))*100\nneutral  = len(df1.loc[df1.polarity >0 ,['Review Text']].values) - len(df1.loc[df1.polarity >0.5 ,['Review Text']].values)\nneutral = neutral\/len(df1)*100 \nplt.figure(figsize =(10, 7)) \nplt.pie([positive,negative,neutral], labels = ['Positive','Negative','Neutral'],colors = [ 'blue','#E1396C','#FEBFB3'])","8da70a18":"plt.figure(figsize=(8,8))\nAge = df1['Age']\nfx=sns.boxplot(x='Rating',y='Age',data=df1)\nplt.title(\"Distribution of age with respect to rating\")\nplt.xlabel(\"Rating\")\nplt.ylabel(\"Age\")","cba678d3":"y = df1['Recommended IND']\nX = df1.drop(columns = 'Recommended IND')","d1ab03ce":"plt.figure(figsize=(14,7))\nsns.heatmap(df1.corr(method='kendall'), annot=True )","ef4c9cba":"set1 =set()\ncor = df1.corr()\nfor i in cor.columns:\n    for j in cor.columns:\n        if cor[i][j]>0.8 and i!=j:\n            set1.add(i)\nprint(set1)","83ae7d8b":"X = X.drop(labels = ['token_count'],axis = 1)","37dba870":"X.corr()","28e15be5":"import re\nimport nltk\nnltk.download('stopwords')\nfrom nltk.corpus import stopwords\nfrom nltk.stem.porter import PorterStemmer\nfrom nltk.corpus import stopwords\nstop=stopwords.words('english')","a08128ce":"from wordcloud import WordCloud\npositivedata = df1[ df1['Recommended IND'] == 1]\npositivedata =positivedata['Review Text']\nnegdata = df1[df1['Recommended IND'] == 0]\nnegdata= negdata['Review Text']\n\ndef wordcloud_draw(df1, color = 'white'):\n    words = ' '.join(df1)\n    cleaned_word = \" \".join([word for word in words.split()\n                              if(word!='clothes' and word!='shop')\n                            ])\n    wordcloud = WordCloud(stopwords=stop,\n                      background_color=color,\n                      width=2500,\n                      height=2000\n                     ).generate(cleaned_word)\n    plt.figure(1,figsize=(10, 7))\n    plt.imshow(wordcloud)\n    plt.axis('off')\n    plt.show()\n    \nprint(\"Positive words are as follows\")\nwordcloud_draw(positivedata,'white')\nprint(\"Negative words are as follows\")\nwordcloud_draw(negdata)","fa2a2a0e":"corpus =[]\nX.index = np.arange(len(X))","687f1bd1":"for i in range(len(X)):\n    review = re.sub('[^a-zA-z]',' ',X['Review Text'][i])\n    review = review.lower()\n    review = review.split()\n    ps = PorterStemmer()\n    review =[ps.stem(i) for i in review if not i in set(stopwords.words('english'))]\n    review =' '.join(review)\n    corpus.append(review)","b3fe9e4c":"from sklearn.feature_extraction.text import CountVectorizer as CV\ncv  = CV(max_features = 3000,ngram_range=(1,1))\nX_cv = cv.fit_transform(corpus).toarray()\ny = y.values","51ae28d8":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X_cv, y, test_size = 0.20, random_state = 0)","f2b9726b":"from sklearn.naive_bayes import BernoulliNB\nclassifier = BernoulliNB()\nclassifier.fit(X_train, y_train)","3e4524b8":"y_pred = classifier.predict(X_test)\nfrom sklearn.metrics import accuracy_score\nacc = accuracy_score(y_test, y_pred)\nprint('accuracy:',acc)","0bdba25d":"from sklearn.feature_extraction.text import TfidfVectorizer as TV\ntv  = TV(ngram_range =(1,1),max_features = 3000)\nX_tv = tv.fit_transform(corpus).toarray()","b6d12106":"X_train, X_test, y_train, y_test = train_test_split(X_tv, y, test_size = 0.20, random_state = 0)\nfrom sklearn.naive_bayes import MultinomialNB\nclassifier = MultinomialNB()\nclassifier.fit(X_train, y_train)","6d991cf0":"y_pred = classifier.predict(X_test)\nacc = accuracy_score(y_test, y_pred)\nprint(\"accuracy:\" , acc)","e8043ed7":"from sklearn.ensemble import RandomForestClassifier\nclassifier = RandomForestClassifier()\nclassifier.fit(X_train,y_train)\npreds=classifier.predict(X_test)\nrf_accuracy=accuracy_score(preds,y_test)\nprint(\"Random Forest Model accuracy\",rf_accuracy)","74a6388c":"import xgboost as xgb\nxgb=xgb.XGBClassifier()\nxgb.fit(X_train,y_train)\npreds2=xgb.predict(X_test)\nxgb_accuracy=accuracy_score(preds2,y_test)\nprint(\"XGBoost Model accuracy\",xgb_accuracy)","5a850464":"from sklearn.linear_model import LogisticRegressionCV\nclassifier=LogisticRegressionCV(cv=6,scoring='accuracy',random_state=0,n_jobs=-1,verbose=3,max_iter=500).fit(X_train,y_train)\ny_pred1 = classifier.predict(X_test)","83c44c37":"from sklearn import metrics\nprint(\"Logistic Regression Accuracy:\",metrics.accuracy_score(y_test, y_pred1))","f20b2df5":"import tensorflow as tf\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\ntokenizer = Tokenizer(num_words = 3000)\ntokenizer.fit_on_texts(corpus)\nsequences = tokenizer.texts_to_sequences(corpus)\npadded = pad_sequences(sequences, padding='post')\nword_index = tokenizer.word_index\ncount = 0\nfor i,j in word_index.items():\n    if count == 11:\n        break\n    print(i,j)\n    count = count+1","4079deb2":"embedding_dim = 64\nmodel = tf.keras.Sequential([\n    tf.keras.layers.Embedding(3000, embedding_dim),\n    tf.keras.layers.GlobalAveragePooling1D(),\n    tf.keras.layers.Dense(6, activation='relu'),\n    tf.keras.layers.Dense(1, activation='sigmoid')\n])\n\nmodel.summary()","ac22efe8":"num_epochs = 10\n\nmodel.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\nmodel.fit(padded,y,epochs= num_epochs,validation_split= 0.39)","dae5caa2":"loss = model.history.history\nloss = pd.DataFrame(loss)","55ed2d4b":"f, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\nt = f.suptitle('Basic ANN Performance', fontsize=12)\nf.subplots_adjust(top=0.85, wspace=0.3)\n\nepoch_list = range(1,11)\nax1.plot(epoch_list, loss['accuracy'], label='Train Accuracy')\nax1.plot(epoch_list, loss['val_accuracy'], label='Validation Accuracy')\nax1.set_xticks(np.arange(0, 11, 1))\nax1.set_ylabel('Accuracy Value')\nax1.set_xlabel('Epoch')\nax1.set_title('Accuracy')\nl1 = ax1.legend(loc=\"best\")\n\nax2.plot(epoch_list, loss['loss'], label='Train Loss')\nax2.plot(epoch_list, loss['val_loss'], label='Validation Loss')\nax2.set_xticks(np.arange(0, 11, 1))\nax2.set_ylabel('Loss Value')\nax2.set_xlabel('Epoch')\nax2.set_title('Loss')\nl2 = ax2.legend(loc=\"best\")","e9eef9e6":"sample_string = \"I hate this dress\"\nsample = tokenizer.texts_to_sequences(sample_string)\npadded_sample = pad_sequences(sample, padding='post')\nprint(\"Padded sample\", padded_sample.T)\nprint(\"Probabilty of a person recommending :\",model.predict(padded_sample.T)[0][0]*100,\"%\")","f6227c3b":"sample_string = \"i love the fabric\"\nsample = tokenizer.texts_to_sequences(sample_string)\npadded_sample = pad_sequences(sample, padding='post')\nprint(\"Padded sample\", padded_sample.T)\nprint(\"Probabilty of a person recommending :\",model.predict(padded_sample.T)[0][0]*100,\"%\")","e317bf5d":"# Reviews with Positive Polarity","eb4b70b2":"# Handling Multi-Colinearity","0918b948":"Let's find the how much clothes are recommendable in our data ","e2682cf7":"# Model Building","c52681a1":"# Visualising Model history","b5aa95ff":"# Reviews with Negative Polarity","6ec76ffc":"# CountVectorizer ","b7b19e11":"check the ratio of ratings in the data","140dcbf1":"# Tf-Idf Vectorizer","e5b9f6c3":"# group by the ratings as per the recommended id.","3f09607b":"# Random Forrest Model","6a41e34a":"# Deep Learning Model","d88ab883":"check the null values in the reviews columns and drop them from the column.","ebc5e981":"# Creating a ANN structure","56d62cc8":"import the libraries","d295688e":"here the maximum null values are in the title and second most in the review text.","09a5a9fe":"# Let's see positive and negative words by using WordCloud.","474e264c":"# Correlation","f85a8c65":"# Reviews with Neutral Polarity","03c4ee4e":"# Data Visualization","f98b1cb6":"# Checking the model on a random example","338bc1e1":"dropping the null values for review text from the data","3556bc10":"ratings with respect to the age of a customer","af0745ca":"# Mulinomial Naive Bayes","49a27560":"we can drop some colums as they are not meaningfull for our data.","02d5c40f":"check the count of the class name of clothes in the data","35bfd35e":"# Read dataset","11ca901a":"# Bernoulli Naive Bayes","1307dc06":"# Logistic Regression Model","30cec1ef":"import some more libraries","be718448":"# XGBoost Model"}}