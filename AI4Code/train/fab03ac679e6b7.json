{"cell_type":{"66bf5ec6":"code","745b8dcd":"code","1155ff65":"code","234a65d8":"code","c06152ed":"code","05804899":"code","8b4d0fd6":"code","89ddb4e9":"code","175c1c42":"code","0649d7ac":"code","25542b93":"code","2f5d44e2":"code","eb6d55b1":"code","b67f6df2":"code","336635a4":"code","d3dc0809":"code","ece02326":"code","9fd7b15d":"code","5f4644fe":"code","1ef3f56c":"code","f4a18a4e":"code","9430f90a":"code","07a70ad6":"code","431140c4":"code","55865897":"code","f1ece826":"code","ba678a3c":"code","3f7210f4":"code","90773ad8":"markdown","912d4a33":"markdown","6e24a9a4":"markdown","0155a028":"markdown","9512b160":"markdown","b52ceb4b":"markdown","3145dd5f":"markdown","46403bd0":"markdown","b10daee4":"markdown","e3899caa":"markdown","bf137a50":"markdown","15df6c19":"markdown","739d4a58":"markdown","6a69d7c8":"markdown"},"source":{"66bf5ec6":"import numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n%matplotlib inline","745b8dcd":"gene_df=pd.read_csv('..\/input\/gene-expression\/actual.csv')\ngene_df_independent=pd.read_csv('..\/input\/gene-expression\/data_set_ALL_AML_independent.csv')\ngene_df_train=pd.read_csv('..\/input\/gene-expression\/data_set_ALL_AML_train.csv')","1155ff65":"gene_df.head()","234a65d8":"gene_df_independent.head()","c06152ed":"gene_df_train.head()","05804899":"print(gene_df.info())\nprint()\nprint(gene_df_independent.info())\nprint()\nprint(gene_df_train.info())","8b4d0fd6":"print(gene_df.shape)\nprint()\nprint(gene_df_independent.shape)\nprint()\nprint(gene_df_train.shape)","89ddb4e9":"gene_df_train.head()","175c1c42":"gene_df_train.drop([col for col in gene_df_train.columns if 'call' in col],1, inplace=True)\ngene_df_train.drop(['Gene Description','Gene Accession Number'],axis=1,inplace=True)\ngene_df_train.head()","0649d7ac":"gene_df_train.shape","25542b93":"sns.heatmap(gene_df_train.corr() , annot=False,cmap='Blues')","2f5d44e2":"from sklearn.preprocessing import StandardScaler\n","eb6d55b1":"scaler = StandardScaler()\n","b67f6df2":"scaled_X = scaler.fit_transform(gene_df_train)\n","336635a4":"scaled_X\n","d3dc0809":"from sklearn.decomposition import PCA\n","ece02326":"model = PCA(n_components=2)\n","9fd7b15d":"principal_components = model.fit_transform(scaled_X)\n","5f4644fe":"plt.figure(figsize=(8,6))\nplt.scatter(principal_components[:,0],principal_components[:,1])\nplt.xlabel('First principal component')\nplt.ylabel('Second Principal Component')","1ef3f56c":"model.n_components\n","f4a18a4e":"model.components_\n","9430f90a":"df_comp = pd.DataFrame(model.components_,index=['PC1','PC2'],columns=gene_df_train.columns)","07a70ad6":"df_comp\n","431140c4":"plt.figure(figsize=(20,3),dpi=150)\nsns.heatmap(df_comp,annot=True,cmap='Blues')","55865897":"model.explained_variance_ratio_\n","f1ece826":"np.sum(model.explained_variance_ratio_)\n","ba678a3c":"explained_variance = []\n\nfor n in range(1,38):\n    pca = PCA(n_components=n)\n    pca.fit(scaled_X)\n    \n    explained_variance.append(np.sum(pca.explained_variance_ratio_))","3f7210f4":"plt.plot(range(1,38),explained_variance)\nplt.xlabel(\"Number of Components\")\nplt.ylabel(\"Variance Explained\");","90773ad8":"## Creating and Fitting (PCA)","912d4a33":"## What Is Principal Component Analysis?\nPrincipal Component Analysis, or PCA, is a dimensionality-reduction method that is often used to reduce the dimensionality of large data sets, by transforming a large set of variables into a smaller one that still contains most of the information in the large set.\n\nReducing the number of variables of a data set naturally comes at the expense of accuracy, but the trick in dimensionality reduction is to trade a little accuracy for simplicity. Because smaller data sets are easier to explore and visualize and make analyzing data much easier and faster for machine learning algorithms without extraneous variables to process.\n\nSo to sum up, the idea of PCA is simple \u2014 reduce the number of variables of a data set, while preserving as much information as possible.","6e24a9a4":"## Choose n-components","0155a028":"![](https:\/\/miro.medium.com\/max\/2158\/0*5Iaw94wlYCTp0GuK.png)","9512b160":"### Scaling the features","b52ceb4b":"## Calculating the Explained Variance by the Component","3145dd5f":"## Component","46403bd0":"## Read Datasets and Check out the Data","b10daee4":"**n_component = 10 gives us more that 96% of information.**\n\n","e3899caa":"## HOW DO YOU DO A PCA?\n**1.** Standardize the range of continuous initial variables.\n\n**2.** Compute the covariance matrix to identify correlations.\n\n**3.** Compute the eigenvectors and eigenvalues of the covariance matrix to identify the principal components.\n\n**4.** Create a feature vector to decide which principal components to keep.\n\n**5.** Recast the data along the principal components axes.\n","bf137a50":"## Import Libraries","15df6c19":"## Data preparation","739d4a58":"#### Gene Expression Monitoring Analysis\n**Introduction**\n\nIn this notebook we shall examine a small gene expression dataset, attempting to classify leukemia patients into one of two classes. This dataset was the focus of a Kaggle Days meetup in London that I attended in March 2019 and the original data can be found here. It comes with the following explanatory notes:\n\n**Context**\n\nThis dataset comes from a proof-of-concept study published in 1999 by Golub et al. It showed how new cases of cancer could be classified by gene expression monitoring (via DNA microarray) and thereby provided a general approach for identifying new cancer classes and assigning tumors to known classes. These data were used to classify patients with acute myeloid leukemia (AML) and acute lymphoblastic leukemia (ALL).\n\n**Content**\n\nGolub et al \"Molecular Classification of Cancer: Class Discovery and Class Prediction by Gene Expression Monitoring\"\n\nThere are two datasets containing the initial (training, 38 samples) and independent (test, 34 samples) datasets used in the paper. These datasets contain measurements corresponding to ALL and AML samples from Bone Marrow and Peripheral Blood. Intensity values have been re-scaled such that overall intensities for each chip are equivalent.\n\n**Acknowledgements**\n\nMolecular Classification of Cancer: Class Discovery and Class Prediction by Gene Expression\n\n**Inspiration**\n\nThese datasets are great for classification problems. The original authors used the data to classify the type of cancer in each patient by their gene expressions.","6a69d7c8":"**In this numpy matrix array, each row represents a principal component, Principal axes in feature space, representing the directions of maximum variance in the data. The components are sorted by explainedvariance. We can visualize this relationship with a heatmap:**"}}