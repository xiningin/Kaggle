{"cell_type":{"6756ccbb":"code","a83bb818":"code","8383a9d1":"code","f9773a21":"code","2c97d140":"code","ba3c003b":"code","129616ce":"code","23158045":"code","086010b8":"code","c983b524":"code","69f4ac73":"markdown"},"source":{"6756ccbb":"import os\nimport torch\nimport pandas as pd\nimport numpy as np\nimport joblib\nimport logging\nimport transformers\nimport sys\nimport torch.nn as nn\nimport gc;\nimport h5py\nfrom scipy import stats\nfrom collections import OrderedDict, namedtuple\nfrom torch.optim import lr_scheduler\nfrom transformers import (\n    AdamW, get_linear_schedule_with_warmup, get_constant_schedule, \n    XLMRobertaTokenizer, XLMRobertaModel, XLMRobertaConfig,\n)\nfrom sklearn import metrics, model_selection\nfrom tqdm.autonotebook import tqdm","a83bb818":"%%time\n# load the data\n\ntrain1 = pd.read_csv(\"\/kaggle\/input\/jigsaw-multilingual-toxic-comment-classification\/jigsaw-toxic-comment-train.csv\", usecols=[\"comment_text\", \"toxic\"])\ntrain2 = pd.read_csv(\"\/kaggle\/input\/jigsaw-multilingual-toxic-comment-classification\/jigsaw-unintended-bias-train.csv\", usecols=[\"comment_text\", \"toxic\"])\ntrain2.toxic = train2.toxic.round().astype(int)\n\ndf_valid = pd.read_csv('\/kaggle\/input\/jigsaw-multilingual-toxic-comment-classification\/validation.csv')\ntest = pd.read_csv('\/kaggle\/input\/jigsaw-multilingual-toxic-comment-classification\/test.csv')\nsub = pd.read_csv('\/kaggle\/input\/jigsaw-multilingual-toxic-comment-classification\/sample_submission.csv')\n\ndf_train = pd.concat([\n    train1[['comment_text', 'toxic']],\n    train2[['comment_text', 'toxic']].query('toxic==1'),\n    train2[['comment_text', 'toxic']].query('toxic==0').sample(n=99937, random_state=0), # hacked to make train_data size divisible by bs;\n])\n\ndel train1, train2\ngc.collect(); gc.collect();\nprint(df_train.shape, df_valid.shape)","8383a9d1":"%%time\ntqdm.pandas()\ndf_train[\"comment_text\"] = df_train[\"comment_text\"].progress_apply(lambda x: \" \" + \" \".join(str(x).split()))\ndf_valid[\"comment_text\"] = df_valid[\"comment_text\"].progress_apply(lambda x: \" \" + \" \".join(str(x).split()))","f9773a21":"%%time\n\nfrom joblib import Parallel, delayed\ntokenizer = transformers.XLMRobertaTokenizer.from_pretrained('xlm-roberta-large')\n\ndef regular_encode(texts, tokenizer=tokenizer, maxlen=128):\n    enc_di = tokenizer.encode_plus(\n        str(texts[0]),\n        return_attention_masks=False, \n        return_token_type_ids=False,\n        pad_to_max_length=True,\n        max_length=maxlen\n    )\n    \n    return np.array(enc_di['input_ids']), np.array(enc_di[\"attention_mask\"]), texts[1]\n\nrows = zip(df_train['comment_text'].values.tolist(), df_train.toxic.values.tolist())\nx_train = Parallel(n_jobs=4, backend='multiprocessing')(delayed(regular_encode)(row) for row in tqdm(rows))\n\nrows = zip(df_valid['comment_text'].values.tolist(), df_valid.toxic.values.tolist())\nx_valid = Parallel(n_jobs=4, backend='multiprocessing')(delayed(regular_encode)(row) for row in tqdm(rows))","2c97d140":"np.save(\"x_train_tokenized\", x_train)\nnp.save(\"x_valid_tokenized\", x_valid);","ba3c003b":"np.array(x_train).shape, np.array(x_valid).shape","129616ce":"import numpy\n\na = numpy.memmap('train.mymemmap', dtype='int32', mode='w+', shape=(2, np.array(x_train).shape[0], 128))\nfor idx in tqdm(range(np.array(x_train).shape[0])):\n    a[0][idx] = np.array(x_train[idx][0], dtype=np.int32)\n    a[1][idx] = np.array(x_train[idx][1], dtype=np.int32)\ndel a;\n\na = numpy.memmap('train_targets.mymemmap', dtype='int32', mode='w+', shape=(np.array(x_train).shape[0],))\nfor idx in tqdm(range(np.array(x_train).shape[0])):\n    a[idx] = np.array(x_train[idx][2], dtype=np.int32)\ndel a;\n\na = numpy.memmap('valid.mymemmap', dtype='int32', mode='w+', shape=(2, np.array(x_valid).shape[0], 128))\nfor idx in tqdm(range(np.array(x_valid).shape[0])):\n    a[0][idx] = np.array(x_valid[idx][0], dtype=np.int32)\n    a[1][idx] = np.array(x_valid[idx][1], dtype=np.int32)\ndel a\n\na = numpy.memmap('valid_targets.mymemmap', dtype='int32', mode='w+', shape=(np.array(x_valid).shape[0],))\nfor idx in tqdm(range(np.array(x_valid).shape[0])):\n    a[idx] = np.array(x_valid[idx][2], dtype=np.int32)\ndel a;","23158045":"class MyIterableDataset_v1(torch.utils.data.IterableDataset):\n    \n    def __init__(self):\n        \n        self.data = np.memmap(\"valid.mymemmap\", shape=(2, 8000, 128), mode=\"r\", dtype=\"int32\")\n        self.target = np.memmap(\"valid_targets.mymemmap\", shape=(8000,), mode=\"r\", dtype=\"int32\")\n    \n    def __iter__(self):\n        # memmap contains input_ids, masks, targets\n        return iter(zip(np.array(self.data[0]), np.array(self.data[1]), np.array(self.target)))","086010b8":"iterable_dataset = MyIterableDataset_v1()\nloader = torch.utils.data.DataLoader(iterable_dataset, batch_size=32)","c983b524":"for batch in tqdm(loader):\n    print(batch)\n    break","69f4ac73":"# imports"}}