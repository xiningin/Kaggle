{"cell_type":{"72075cd9":"code","ea770b76":"code","74cde32f":"code","b8d3a8a2":"code","4b1b3a74":"code","748ac723":"code","931c9f8f":"code","111e63e7":"code","c9ffc1fc":"code","0d6cff37":"code","c0504ee8":"code","1b36965a":"code","4ca63b1e":"code","6705952a":"code","7a523467":"code","2553f85f":"code","4ab0a78d":"code","0dae53e8":"code","f693ff06":"code","50d25f25":"code","0eb78c0c":"code","b0597fc3":"code","d1f55e71":"code","d4e54f9b":"code","66ada401":"code","6a259b03":"code","c26e7000":"code","186e0b79":"code","40039783":"code","1dcd2c05":"code","e9e073c8":"code","f7293a52":"code","1d198f5b":"code","a4cc61ce":"code","3779868d":"code","510558b1":"code","130522e8":"code","9206a7df":"code","c3468dce":"code","1c1e1909":"code","c641ee31":"code","86ce8b62":"code","03cfef55":"code","35fffe5f":"code","358a8e45":"code","c811cda4":"code","82c36e1b":"code","314e31e9":"code","5585f577":"code","8cb46b01":"code","f0fef6da":"code","1961125f":"code","c036a986":"code","c96588c9":"code","1e0c2f44":"code","2441814e":"code","4f697b26":"code","6e37d1a1":"code","e1dc6f12":"code","1db518a4":"code","56a52f83":"code","6b6f9d0a":"code","62fdd54b":"code","1f3eba1c":"code","98c7ca6a":"code","dc326fa5":"code","4fd705e9":"code","f0bca2e1":"code","a7cce2ab":"code","ae0a34bd":"code","4f618c27":"code","3e6825df":"code","90a165a0":"code","07cb43b0":"markdown","d0ee8dbb":"markdown","2637c089":"markdown","401286c9":"markdown","7e48cbfa":"markdown","0d1dd546":"markdown","68e8c056":"markdown","3d42f174":"markdown","7346bd71":"markdown","a16dc0b2":"markdown","4f7254f6":"markdown","8bc57eb3":"markdown","8325e243":"markdown","2cc1120d":"markdown","e4c8b556":"markdown","ee16b13b":"markdown","a1392c7b":"markdown","bd05739f":"markdown","f3eb0834":"markdown","0f74b0ab":"markdown","322cb454":"markdown","890bd68a":"markdown","7cfe0a47":"markdown","2813e0c8":"markdown","18682c3e":"markdown","c219abc2":"markdown","5d7faedb":"markdown","05d4ff90":"markdown","ce905e4e":"markdown","48dd081a":"markdown","fcc63146":"markdown","c341b76d":"markdown","b75923f0":"markdown","b45a5232":"markdown","13a77fe8":"markdown","da1c6f15":"markdown","8f732a20":"markdown","8a4ce060":"markdown","bf4939ae":"markdown","03e0452b":"markdown","87e77d83":"markdown","0ff45cd7":"markdown","68b5883b":"markdown","2e4ac154":"markdown","1117f71f":"markdown","3c1be527":"markdown","f6a9d540":"markdown","9ef5b338":"markdown","076d4031":"markdown","9716e5c7":"markdown","72d69089":"markdown","fabb6baf":"markdown","658fb928":"markdown","3ab13a65":"markdown","619ca02b":"markdown","8c7dd39e":"markdown","7658eef8":"markdown","317c51b6":"markdown","f57ebea6":"markdown","a33af660":"markdown","d8a02309":"markdown","827d3da1":"markdown"},"source":{"72075cd9":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","ea770b76":"# import numpy as np     numpy is already imported\n# import pandas as pd    pandas is already imported\n\n# import seaborn to visualize data that built on top of matplotlib\nimport seaborn as sns\n\n# import pipeline to make machine learning pipeline to overcome data leakage problem\nfrom sklearn.pipeline import Pipeline\n\n# import StandardScaler to Column Standardize the data\n# many algorithm assumes data to be Standardized\nfrom sklearn.preprocessing import StandardScaler\n\n# train_test_split is used to split the data into train and test set of given data\nfrom sklearn.model_selection import train_test_split\n\n# KFold is used for defining the no.of folds for Cross Validation\nfrom sklearn.model_selection import KFold\n\n# cross_val_score is used to find the score on given model and KFlod\nfrom sklearn.model_selection import cross_val_score\n\n# used for Hyper-parameter\nfrom sklearn.model_selection import GridSearchCV\n\n# classification report show the classification report\n# precision, recall, f1-score\nfrom sklearn.metrics import classification_report\n\n# accuracy score is also a metrics to judge the model\n# mostly used for Balanced dataset\n# not better for Imabalanced dataset\nfrom sklearn.metrics import accuracy_score\n\n# confusion matrix show the comparision between actual label and predicted label of data\n# mostly used for Binary classification\nfrom sklearn.metrics import confusion_matrix\n\n# importing different algorithms to train our data and find better model among all algorithms\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\n\n# KNN is slow algorithm for runtime b'z it doesn't learn anything at the time of fitting the model\n# KNN just stores every datapoint and find K Nearest Neighbor and\n# among all nearest neighbor whichever has high no.of points that is the label of that query point\n# Because it store every datapoint in memory to predict the label in runtime, It is not better for large data\n# KNN is less used in industry because it is not good for Low \"latency\"(time to predict the label of given query point) systems like SearchEngines\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.svm import SVC\n\n# Ensembles generally group more than one model to give better model\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import ExtraTreesClassifier\n\n# import matplotlib\nimport matplotlib.pyplot as plt\n%matplotlib inline","74cde32f":"train = pd.read_csv('\/kaggle\/input\/titanic\/train.csv')\ntest = pd.read_csv('\/kaggle\/input\/titanic\/test.csv')","b8d3a8a2":"train.shape","4b1b3a74":"train.head()","748ac723":"train.tail()","931c9f8f":"train.describe().transpose()","111e63e7":"train.groupby('Survived').size()","c9ffc1fc":"train.info()","0d6cff37":"train.isnull().sum()","c0504ee8":"sns.heatmap(train.isnull(),yticklabels=False,cmap='viridis')","1b36965a":"# setting the style of axes(plotting area) as 'whitegrid'.\nsns.set_style('whitegrid')\n\n# let's count the #person survived\nsns.countplot(x='Survived',data= train, palette='RdBu_r')","4ca63b1e":"# count # survived person catergorised by 'Sex'\nsns.countplot(x='Survived', hue='Sex',data= train, palette='RdBu_r')","6705952a":"sns.countplot(x='Survived', hue='Pclass',data= train, palette='rainbow')","7a523467":"sns.distplot(train['Age'].dropna(),kde=False,bins=30,color='darkred')","2553f85f":"train['Age'].hist(bins=30,color='darkred',alpha=0.7)","4ab0a78d":"sns.countplot(x='SibSp',data=train)","0dae53e8":"sns.countplot(x='Survived',hue='SibSp',data=train)","f693ff06":"train['Fare'].hist(color='green',bins=35)","50d25f25":"sns.boxplot(x='Pclass', y='Age',data=train,palette='winter')","0eb78c0c":"def impute_age(cols):\n    Age = cols[0]\n    Pclass = cols[1]\n    \n    if pd.isnull(Age):\n        if Pclass == 1:\n            return 37\n        elif Pclass == 2:\n            return 29\n        else:\n            return 24\n    else:\n        return Age","b0597fc3":"train['Age'] = train[['Age','Pclass']].apply(impute_age,axis=1)\n\n# we have also to clean the test data\ntest['Age'] = test[['Age','Pclass']].apply(impute_age,axis=1)","d1f55e71":"sns.heatmap(train.isnull(),yticklabels=False,cbar=False,cmap='viridis')","d4e54f9b":"train.info()","66ada401":"train.drop('Cabin',axis=1,inplace=True)\n","6a259b03":"# drop Cabin from test data also\ntest.drop('Cabin',axis=1,inplace=True)","c26e7000":"train.head()","186e0b79":"train.isnull().sum()","40039783":"train.dropna(inplace=True)\n","1dcd2c05":"test.isnull().sum()","e9e073c8":"test[test['Fare'].isnull()]","f7293a52":"# set value of 'Fare' at index Location 152 as 50\n# we can't delete the row because it we have to submit my prediction values and that should be equal to rows that is being given\ntest.set_value(152,'Fare',50)","1d198f5b":"train.isnull().sum()\n","a4cc61ce":"test.isnull().sum()","3779868d":"train.info()","510558b1":"sex = pd.get_dummies(train['Sex'],drop_first=True) # getting dummy of 'Sex' column\nembark = pd.get_dummies(train['Embarked'],drop_first=True) # getting dummy of 'Embarked'","130522e8":"# for test data\nsex_test = pd.get_dummies(test['Sex'],drop_first=True) # getting dummy of 'Sex' column\nembark_test = pd.get_dummies(test['Embarked'],drop_first=True) # getting dummy of 'Embarked'","9206a7df":"# drop columns: 'Sex', 'Embarked', 'Name','Ticket','PassengerId'\ntrain.drop(['Sex','Embarked','Name','Ticket','PassengerId'],axis=1,inplace=True)\n\n# for test\ntest.drop(['Sex','Embarked','Name','Ticket','PassengerId'],axis=1,inplace=True)\n","c3468dce":"# for train\ntrain = pd.concat([train,sex,embark],axis=1)\n# for test\ntest = pd.concat([test,sex_test,embark_test],axis=1)","1c1e1909":"train.head()","c641ee31":"# let's also see test data header part\ntest.head()","86ce8b62":"predictors = train.drop(['Survived'],axis=1)","03cfef55":"predictors.head() # Now It has no label, It is pure training data without labels","35fffe5f":"target = train['Survived']","358a8e45":"target.head()","c811cda4":"# Create a Validation dataset\n# .values returns numpy.ndarray\n# we are using array instead of dataframe to train model because\n# array is faster to compute instead of dataframe\n\nX = predictors.values\nY = target.values\nvalidation_size = 0.20\nseed = 42 \nX_train,X_test,y_train,y_test = train_test_split(X,Y,test_size=validation_size,random_state=seed)","82c36e1b":"type(X)","314e31e9":"# code source: machinelearningmastery.com\n# Spot-check Algorithms\nmodels = []\n\n# In LogisticRegression set: solver='lbfgs',multi_class ='auto', max_iter=10000 to overcome warning\nmodels.append(('LR',LogisticRegression(solver='lbfgs',multi_class='auto',max_iter=10000)))\nmodels.append(('LDA',LinearDiscriminantAnalysis()))\nmodels.append(('KNN',KNeighborsClassifier()))\nmodels.append(('CART',DecisionTreeClassifier()))\nmodels.append(('NB',GaussianNB()))\nmodels.append(('SVM',SVC(gamma='scale')))\n\n# evaluate each model in turn\nresults = []\nnames = []\n\nfor name, model in models:\n    # initializing kfold by n_splits=10(no.of K)\n    kfold = KFold(n_splits = 10, random_state=seed)\n    \n    # cross validation score of given model using cross-validation=kfold\n    cv_results = cross_val_score(model,X_train,y_train,cv=kfold, scoring=\"accuracy\")\n    \n    # appending cross validation result to results list\n    results.append(cv_results)\n    \n    # appending name of algorithm to names list\n    names.append(name)\n    \n    # printing cross_validation_result's mean and standard_deviation\n    print(name, cv_results.mean()*100.0, \"(\",cv_results.std()*100.0,\")\")","5585f577":"#Let's Compare by plotting it\nfigure = plt.figure()\nfigure.suptitle('Algorithm Comparison')\nax = figure.add_subplot(111)\nplt.boxplot(results)\nax.set_xticklabels(names)","8cb46b01":"# test options and evaluation matrix\nnum_folds=10\nseed=42\nscoring='accuracy'","f0fef6da":"# import warnings filter\nfrom warnings import simplefilter\n# ignore all future warnings\nsimplefilter(action='ignore', category=FutureWarning)","1961125f":"# source of code: machinelearningmastery.com\n# Standardize the dataset\npipelines = []\npipelines.append(('ScaledLR', Pipeline([('Scaler', StandardScaler()),('LR',LogisticRegression(solver='lbfgs',multi_class='auto',max_iter=10000))])))\npipelines.append(('ScaledLDA', Pipeline([('Scaler', StandardScaler()),('LDA',LinearDiscriminantAnalysis())])))\npipelines.append(('ScaledKNN', Pipeline([('Scaler', StandardScaler()),('KNN',KNeighborsClassifier())])))\npipelines.append(('ScaledCART', Pipeline([('Scaler', StandardScaler()),('CART',DecisionTreeClassifier())])))\npipelines.append(('ScaledNB', Pipeline([('Scaler', StandardScaler()),('NB',GaussianNB())])))\npipelines.append(('ScaledSVM', Pipeline([('Scaler', StandardScaler()),('SVM', SVC(gamma='scale'))])))\nresults = []\nnames = []\nfor name, model in pipelines:\n    kfold = KFold(n_splits=num_folds, random_state=seed)\n    cv_results = cross_val_score(model, X_train, y_train, cv=kfold, scoring=scoring)\n    results.append(cv_results)\n    names.append(name)\n    msg = \"%s: %f (%f)\" % (name, cv_results.mean()*100, cv_results.std())\n    print(msg)","c036a986":"# Compare Algorithms\nfig = plt.figure()\nfig.suptitle('Algorithm Comparison')\nax = fig.add_subplot(111)\nplt.boxplot(results)\nax.set_xticklabels(names)\nplt.show()","c96588c9":"# Tune scaled SVM\nscaler = StandardScaler().fit(X_train)\nrescaledX = scaler.transform(X_train)\nc_values = [0.1, 0.3, 0.5, 0.7, 0.9, 1.0, 1.3, 1.5, 1.7, 2.0]\nkernel_values = ['linear', 'poly', 'rbf', 'sigmoid']\nparam_grid = dict(C=c_values, kernel=kernel_values)\nmodel = SVC()\nkfold = KFold(n_splits=num_folds, random_state=seed)\ngrid = GridSearchCV(estimator= model, param_grid=param_grid, scoring=scoring,cv=kfold)\ngrid_result = grid.fit(rescaledX,y_train)\nprint(\"Best: %f using %s\"% (grid_result.best_score_, grid_result.best_params_))\nmeans = grid_result.cv_results_['mean_test_score']\nstds= grid_result.cv_results_['std_test_score']\nparams = grid_result.cv_results_['params']\n\nfor mean, stdev, param in zip(means, stds, params):\n    print(\"%f (%f) with: %r\"%(mean,stdev,param))","1e0c2f44":"# code source: https:\/\/www.machinelearningmastery.com by jason Brownlee\n# ensembles\nensembles = []\nensembles.append(('AB', AdaBoostClassifier()))\nensembles.append(('GBM', GradientBoostingClassifier()))\nensembles.append(('RF', RandomForestClassifier()))\nensembles.append(('ET', ExtraTreesClassifier()))\nresults = []\nnames = []\nfor name, model in ensembles:\n    kfold = KFold(n_splits=num_folds, random_state=seed)\n    cv_results = cross_val_score(model, X_train, y_train, cv=kfold, scoring=scoring)\n    results.append(cv_results)\n    names.append(name)\n    msg = \"%s: %f (%f)\" % (name, cv_results.mean()*100, cv_results.std()*100)\n    print(msg)","2441814e":"# Compare Algorithms\nfig = plt.figure()\nfig.suptitle('Ensemble Algorithm Comparison')\nax = fig.add_subplot(111)\nplt.boxplot(results)\nax.set_xticklabels(names)\nplt.show()","4f697b26":"# this accuracy score shows the accuracy of what we have splitted earlier\n# this accuracy is not using test dataset, right.\n\n# fitting the StandardScaler() to calculate mean and standard deviation\nscaler = StandardScaler().fit(X_train)\n\n# transform X_train according to calculated mean and standard deviation\nrescaledX = scaler.transform(X_train)\n\n# making model SVC(Support Vector Classifier)\nmodel = SVC(C=2.0,kernel='rbf')\n\n# fitting the model\nmodel.fit(rescaledX,y_train)\n\n\n\n# estimated accuracy on validation dataset\nrescaledValidationX = scaler.transform(X_test)\npredictions = model.predict(rescaledValidationX)\nprint(accuracy_score(y_test,predictions)*100)","6e37d1a1":"# let's transform the test data\n# let's again fit the model using complete train data.\n\nscaler = StandardScaler().fit(X)\n\nrescaledX = scaler.transform(X)\n\n# create model SVC(Support Vector Classifier)\nmodel = SVC(C=2.0,kernel='rbf')\n\n# fit the model\nmodel.fit(rescaledX,Y)\n\ntransformed_test = scaler.transform(test)","e1dc6f12":"predictions = model.predict(transformed_test)","1db518a4":"predictions","56a52f83":"# importing XGBClassifier\nfrom xgboost import XGBClassifier","6b6f9d0a":"new_model = XGBClassifier(n_estimators = 1000, learning_rate=0.05)","62fdd54b":"new_model.fit(X_train,y_train,\n             early_stopping_rounds = 5,\n             eval_set = [(X_test,y_test)],\n             verbose = False)","1f3eba1c":"xgb_predictions = new_model.predict(X_test)","98c7ca6a":"xgb_predictions","dc326fa5":"accuracy_score(xgb_predictions,y_test)","4fd705e9":"# passing the array(using test.values, .values returns an array) not dataframe\ntest_predictions = new_model.predict(test.values)","f0bca2e1":"test_predictions","a7cce2ab":"xgb_submission= pd.read_csv('\/kaggle\/input\/titanic\/gender_submission.csv')","ae0a34bd":"xgb_submission.head(15)","4f618c27":"xgb_submission['Survived'] = test_predictions","3e6825df":"xgb_submission.head(15)","90a165a0":"xgb_submission.to_csv('gender_submission.csv',index=False)","07cb43b0":"# Prepare Data","d0ee8dbb":"above distribution plot shows that most of the person belongs to age 15-50","2637c089":"> <p style=\"font-family:'Segoe UI';font-size:28px;color:green\">Exploratory Data Analysis<\/p>","401286c9":"## Converting Categorical Features\nWe'll need to convert categorical features to dummy variables using pandas! Otherwise our machine learning algorithm won't be able to directly take in those features as inputs.","7e48cbfa":"<p style=\"font-family:'Segoe UI';font-size:22px;color:green\">(a)Descriptive statistics<\/p>\n<p style=\"font-family:'Segoe UI';font-size:18px;color:purple\">description<\/p>","0d1dd546":"## Making the model using whole training (train) data now","68e8c056":"<p style=\"font-family:'Segoe UI';font-size:20px;color:purple\">0 shows not survived<br>\n1 shows survived","3d42f174":"We can see the most accurate configuration was SVM with an RBF kernel and a C value of 2.0. The accuracy(83.26) is seemingly better.","7346bd71":"<p style=\"font-family:'Segoe UI';font-size:20px;color:purple\">let's begin some exploratory data analysis! We'll start by checking out missing data!<\/p>","a16dc0b2":"## Evaluate Algorithms: Baseline","4f7254f6":"another way to show above plot","8bc57eb3":"There is only two missing values in Embarked column. so, it is better to drop that row","8325e243":"<p style=\"font-family:'Segoe UI';font-size:18px;color:green\">(b) load dataset <br>\nLet's start reading train and test dataset into a pandas DataFrame","2cc1120d":"<p style=\"font-family:'Segoe UI';font-size:22px;color:green\">Summarize Data<\/p>\n<p style=\"font-family:'Segoe UI';font-size:18px;color:purple\">Dimension of the dataset<\/p>","e4c8b556":"GBM has highest prediction score. But, still it is not better than SVM.\nWe have got an accuracy of 83% using SVM. let's check it on unseen data","ee16b13b":"Now apply that function to fill the age's missing values","a1392c7b":"## WOW!, we got an accuracy of 82.58%.\n","bd05739f":"let's see distribution plot to show which age of people is more in the Titanic.","f3eb0834":"It is showing that Linear Discriminant Analysis is better choice","0f74b0ab":"Now let's create our predicted values dataframe for submission to the Titanic Survival Prediction Challenge.","322cb454":"<p style=\"font-family:'Segoe UI';font-size:28px;color:green\">Missing Data<\/p>\n<p style=\"font-family:'Segoe UI';font-size:20px;color:purple\">check is there any null value in the dataset<\/p>\n","890bd68a":"# Finalize Model\n","7cfe0a47":"<p style=\"font-family:'Segoe UI';font-size:20px;color:#222\">Dataset is not highly imbalanced but little bit.<br>\nYou can do upsampling of data(if you want) and compare the model accuracy.","2813e0c8":"let's see the fare using histogram to understand the distribution of fare","18682c3e":"# Tuning SVM\nWe can tune two key parameters of the SVM algorithm, the value of C (how much to relax the\nmargin) and the type of kernel. The default for SVM (the SVC class) is to use the Radial\nBasis Function (RBF) kernel with a C value set to 1.0. Like with KNN, we will perform a grid\nsearch using 10-fold cross validation with a standardized copy of the training dataset. We will\ntry a number of simpler kernel types and C values with less bias and more bias (less than and\nmore than 1.0 respectively)","c219abc2":"Great! Our data is ready for our model!","5d7faedb":"Here, GBM is not better than SVM","05d4ff90":"<p style=\"font-family:'Segoe UI';font-size:20px;color:purple\">class distribution","ce905e4e":"# WOW! we got an accuracy of 82.5%","48dd081a":"Now let's check that heat map again","fcc63146":"# Evaluate Algorithms: Standardize Data\nWe suspect that the differing distributions of the raw data may be negatively impacting the skill\nof some of the algorithms. Let\u2019s evaluate the same algorithms with a standardized copy of the\ndataset. This is where the data is transformed such that each attribute has a mean value of zero\nand a standard deviation of one. We also need to avoid data leakage when we transform the\ndata. A good way to avoid leakage is to use pipelines that standardize the data and build the\nmodel for each fold in the cross validation test harness. That way we can get a fair estimation\nof how each model with standardized data might perform on unseen data.","c341b76d":"<p style=\"font-family:'Segoe UI';font-size:20px;color:purple\">Let's Visualize it using heatmap","b75923f0":"Great! Let's go ahead and drop the Cabin column and the row in Embarked that is NaN.","b45a5232":"<p style=\"font-family:'Segoe UI';font-size:20px;color:green\">Prepare Problem<\/p>\n<p style=\"font-family:'Segoe UI';font-size:16px;color:green\">(a) Load libraries<\/p>","13a77fe8":"# Ensemble Methods\nAnother way that we can improve the performance of algorithms on this problem is by using\nensemble methods. In this section we will evaluate four different ensemble machine learning\nalgorithms, two boosting and two bagging methods: <br>\n1. Boosting Methods: AdaBoost (AB) and Gradient Boosting (GBM).\n2. Bagging Methods: Random Forests (RF) and Extra Trees (ET).<br><br>\nWe will use the same test harness as before, 10-fold cross validation. No data standardization\nis used in this case because all four ensemble algorithms are based on decision trees that are\nless sensitive to data distributions","da1c6f15":"### let's find the predictions on complete test data\n","8f732a20":"#### Let's see impact of Sibligs on survival","8a4ce060":" # Python Project Template <br><br>\n \n<p style=\"font-family:'Segoe UI';font-size:16px;color:blue\"> <b>1. Prepare Problem<\/b> <br> <br>\na) Load libraries <br>\nb) Load dataset <br> <br>\n    <b>2. Summarize Data <\/b><br> <br>\na) Descriptive statistics <br>\nb) Data visualizations <br><br>\n    <b>3. Prepare Data <\/b><br> <br>\n a) Data Cleaning <br>\n b) Feature Selection <br>\n c) Data Transforms <br><br>\n    <b>4. Evaluate Algorithms<\/b> <br><br>\n a) Split-out validation dataset <br>\n b) Test options and evaluation metric <br>\n c) Spot Check Algorithms <br>\n d) Compare Algorithms <br><br>\n    <b>5. Improve Accuracy <\/b><br> <br>\n a) Algorithm Tuning <br>\n b) Ensembles <br><br>\n    <b>6. Finalize Model<\/b> <br> <br>\n a) Predictions on validation dataset <br>\n b) Create standalone model on entire training dataset <br>\n c) Save model for later use <br><br><br><\/p>","bf4939ae":"# Evaluate Some Algorithms\nNow it is time to create some models of the data and estimate their accuracy on unseen data.\nHere is what we are going to cover in this step:\n1. Separate out a validation dataset.\n2. Setup the test harness to use 10-fold cross validation.\n3. Build 6 different models to predict survival\n4. Select the best model.","03e0452b":"Now, Our dataset is clean, right. (train and test both)","87e77d83":"<p style=\"font-family:'Segoe UI';font-size:20px;color:purple\">Conclusion:<br> There is some column(Age,Cabin,Embarked) which has missing data.<br>\nSurvived datatype is int64 and we know that survived has just label as output, so we can convert it into categorical values<\/p>","0ff45cd7":"to understand XGBoost goto: <a href=\"https:\/\/www.kaggle.com\/alexisbcook\/xgboost\">click here<\/a>[](http:\/\/)","68b5883b":"There are few columns Which doesn't seems to be useful like: PassengerId(absolutely not useful at all), Name.\n<br>\nLet's change Categorical feature into numerical features.","2e4ac154":"# Even, after using XGBoost, I got final score on kaggle is 79.425%","1117f71f":"# Data Cleaning\nWe want to fill-in missing age data instead of just dropping the missing age data rows. One way to do this is by filling in the mean age of all the passengers(imputation). However we can be smarter about this and check the average age by passenger class. For example:","3c1be527":"1. Below code is for supress <a href=\"https:\/\/machinelearningmastery.com\/how-to-fix-futurewarning-messages-in-scikit-learn\/\">FutureWarning<\/a>","f6a9d540":"the above countplot shows that no.of people belongs to lower class is more in not survived people group.<br>\nSo, PassengerClass(PClass) seems to be good feature to predict Survival.","9ef5b338":"let's see the no.of siblings of every passenger of ship(Titanic) using countplot","076d4031":"<p style=\"font-family:'Segoe UI';font-size:20px;color:#222\">In Survived category: Female survived more than male<br>\nIn not-survived: no.of not-survived Male is very high than Female.","9716e5c7":"in above data <br><br>\nin male columns: <br>\n    1 : male <br>\n    0 : female<br>\n    \nQ & S is in the place of Embarked<br>\nthere are two columns Q and S is added to represent Embark because in embark there was three group. So, in order to represent three groups we need two bits in binary(same story is here mean two columns and binary representation is used here).\n    ","72d69089":"<p style=\"font-family:'Segoe UI';font-size:20px;color:#111111\">Roughly 20 percent of the Age data is missing.The proportion of Age missing is likely small enough for reasonable replacement with some form of imputation. Looking at the cabin column, it looks like we are just missing too much of that data to do something useful with at a basic level. We'll probably drop this later, or change it to another feature like \"Cabin Known: 1 or 0\" <br><br>\nLet's continue on by visualizing some more of the data!","fabb6baf":"<p style=\"font-family:'Segoe UI';font-size:20px;color:purple\">let's see basic information about the train data <\/p>","658fb928":"We can see the wealthier passengers in the higher classes tend to be older, which make sense. We'll use these age values to impute based on Pclass for Age.","3ab13a65":"<p style=\"font-family:'Segoe UI';font-size:18px;color:green\">Peek at the Data<\/p>","619ca02b":"See above, there is an impact of Standardization on SVM and KNN both(We'll try for KNN later, right).\nKNN is not good for low latency systems.","8c7dd39e":"## let's Understand the problem\n\nThis is a very famous problem specially for beginners. sometimes it is also called as \"Hello World\" of Machine Learning. It is also a first step to understand Machine Learning problem end-to-end.","7658eef8":"<p style=\"font-family:'Segoe UI';font-size:18px;color:purple\">Bottom of the Data<\/p>","317c51b6":"<p style=\"font-family:'Segoe UI';font-size:24px;color:blue\"> Now, let's use XGBoost to improve model performance.","f57ebea6":"# If you liked it.\n# *Please upvote.*\n# Thanks.","a33af660":"<p style=\"font-family:'Segoe UI';font-size:20px;color:purple\">There are three columns which has null values<br>\n1. Age has 177 null value <br>\n2. Cabin has 687 null value <br>\n3. Embarked has 2 null value <br>(in total)","d8a02309":"Most of the person has no Siblings","827d3da1":"## Create a Validation dataset"}}