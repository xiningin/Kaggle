{"cell_type":{"f05ab331":"code","10cc447f":"code","6f7dcd00":"code","a06d1397":"code","c67aa01d":"code","e3caf3e9":"code","2a043781":"code","6b57b663":"code","50c02b5c":"code","c45fe43e":"code","bed100ba":"code","4f16cb94":"code","ca4fba4e":"code","d27c1299":"code","2133719a":"code","69c9d88b":"code","cc1ef7bc":"code","d7e758f0":"code","bccb5c17":"code","a1001e3c":"code","9a9fe070":"code","3b167457":"code","e2e0cb63":"code","e7102116":"code","9b5bc534":"code","d1031d5d":"code","80c9c681":"code","ef9515d1":"code","3de8e488":"code","b9d8f107":"code","d1fab18a":"code","91e12886":"code","6e002edb":"code","1ba642fb":"code","a1868637":"code","5886fee2":"markdown","8278d83b":"markdown","5f634768":"markdown","48e33a10":"markdown","b2cae167":"markdown","7e3b45ed":"markdown","c80f96f1":"markdown","d778a43e":"markdown","220b2c96":"markdown","e9ed818a":"markdown","d937df84":"markdown","af96547b":"markdown","86c217ee":"markdown","3f0693de":"markdown","b21e9bc4":"markdown","e0e8992c":"markdown","0fef73ca":"markdown","93684510":"markdown","286f9970":"markdown","8fe20452":"markdown","24f8dcad":"markdown","f6b8f942":"markdown","1b20c5d5":"markdown","9b37bd3e":"markdown","63c64d3b":"markdown","f4031aed":"markdown","d13ffbf9":"markdown"},"source":{"f05ab331":"print('mon premier programme python')\nfor i in [0, 1, 2]:\n    print(\"valeur :\", i)\nprint(\"Fin\")","10cc447f":"for i in range(3):\n    print(\"valeur :\", i)\nprint(\"Fin\")","6f7dcd00":"# addition\na = 4 + 5\na","a06d1397":"#multiplication\n2*5","c67aa01d":"#puissance\n2**5","e3caf3e9":"import numpy as np\n\nx = np.arange(-6, 6, 0.3)\nx","2a043781":"import matplotlib.pyplot as plt # pour afficher la figure\n\ndef linear(x):\n    res = []\n    for item in x:\n        res.append(2.0*item)\n    return res\n\ny = linear(x)\n\nplt.plot(x,y)\nplt.grid()\nplt.show()","6b57b663":"# importation des lib \n\nimport pandas as pd","50c02b5c":"import pandas as pd\n\ndf = pd.read_csv(\"..\/input\/iriscsv\/Iris.csv\")\n\ndf.head()","c45fe43e":"# Il y a plein de mani\u00e8re d'\u00e9crire cette commande\nprint(df[df[\"Species\"]=='Iris-setosa'].PetalLengthCm.mean())\nprint(df[df.Species=='Iris-setosa'].PetalLengthCm.mean())\nprint(df[df[\"Species\"]=='Iris-setosa'][\"PetalLengthCm\"].mean())","bed100ba":"rep = df[df[\"Species\"]=='Iris-setosa'].SepalLengthCm.max()\n\nrep","4f16cb94":"df.info() #donner les infos de notre data frame","ca4fba4e":"df.drop('Id',axis=1,inplace=True) \n\n#dropping the Id column as it is unnecessary,\n#  axis=1 specifies that it should be column wise, \n#  inplace =1 means the changes should be reflected into the dataframe","d27c1299":"# d\u00e9finir les attributs qui nous int\u00e9ressent\ndf_features = df[['SepalLengthCm', 'SepalWidthCm', 'PetalLengthCm', 'PetalWidthCm' ]]","2133719a":"# d\u00e9finir l'attribut classe\ndf_labels = df[['Species']]\ndf_labels","69c9d88b":"import seaborn as sns\n# sch\u00e9matiser la distribution des classes\nsns.countplot(df['Species'])","cc1ef7bc":"from sklearn.preprocessing import LabelEncoder\n\nle = LabelEncoder()\ndf[['Species']] = le.fit_transform(df[['Species']])\ndf_labels = df[['Species']]\ndf_labels","d7e758f0":"from sklearn.model_selection import train_test_split\n# d\u00e9couper le data set en 30% pour test et 70% pour l'entrainement\nX_train, X_test, y_train, y_test = train_test_split(df_features, df_labels, test_size=0.4, random_state=42)","bccb5c17":"print('x_train shape:', X_train.shape)\nprint('x_test shape:', X_test.shape)\nprint('y_train shape:', y_train.shape)\nprint('y_test shape:', y_test.shape)","a1001e3c":"X_train.shape[0]","9a9fe070":"from sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import confusion_matrix,accuracy_score","3b167457":"# Chargement d'un classifieur K-ppv\nmon_knn = KNeighborsClassifier(n_neighbors=2)\n\n# Apprentissage\nmon_knn.fit(X_train, y_train)#.values.ravel())\n\n# Pr\u00e9diction\nypred = mon_knn.predict(X_test)\n\nprint ('KNN accuracy score')\n\nprint (accuracy_score(y_test, ypred))","e2e0cb63":"# Function to plot confusion matrix\nimport matplotlib.pyplot as plt\nimport itertools\nimport numpy as np\n\ndef plot_confusion_matrix(cm, classes, normalize=False,  title=' confusion matrix ', cmap=plt.cm.Blues):\n    \"\"\"\n    This function prints and plots the confusion matrix.\n    \"\"\"\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=45)\n    plt.yticks(tick_marks, classes)\n\n    if normalize:\n        cm = cm.astype('float') \/ cm.sum(axis=1)[:, np.newaxis]\n\n    thresh = cm.max() \/ 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, cm[i, j],\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    plt.tight_layout()\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n\n# Predict the values from the validation dataset\nY_pred = mon_knn.predict(X_test)\n# Convert predictions classes to one hot vectors \n#Y_pred_classes = np.argmax(Y_pred , axis = 1) \n# Convert validation observations to one hot vectors\nY_true = y_test#np.argmax(y_test,axis = 1) \n# compute the confusion matrix\nconfusion_mtx = confusion_matrix(Y_true, Y_pred)\n\n \n\n# plot the confusion matrix\nplot_confusion_matrix(confusion_mtx, classes = range(3)) ","e7102116":"# SVC est le Support vecteur machine classifieur\nfrom sklearn.svm import SVC \nclf = SVC()\nclf.fit(X_train, y_train)# values.ravel()\n\n#ypred\nypred = mon_knn.predict(X_test)\n\nprint ('SVM accuracy score')\n\nprint (accuracy_score(y_test, ypred))","9b5bc534":"from sklearn.ensemble import RandomForestClassifier\n\nclf = RandomForestClassifier(max_depth=2, random_state=0)\nclf.fit(X_train, y_train)# values.ravel()\n\n#ypred\nypred = mon_knn.predict(X_test)\n\nprint ('Random Forest accuracy score')\n\nprint (accuracy_score(y_test, ypred))","d1031d5d":"from sklearn.datasets import load_iris\nfrom sklearn import tree\niris = load_iris()\nclf = tree.DecisionTreeClassifier()\nclf = clf.fit(iris.data, iris.target)","80c9c681":"iris.data.shape","ef9515d1":" tree.plot_tree(clf.fit(iris.data, iris.target)) ","3de8e488":"from sklearn.datasets import load_iris\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.tree.export import export_text\niris = load_iris()\nX = iris['data']\ny = iris['target']\ndecision_tree = DecisionTreeClassifier(random_state=0, max_depth=3)\ndecision_tree = decision_tree.fit(X, y)\nr = export_text(decision_tree, feature_names=iris['feature_names'])\nprint(r)","b9d8f107":"clf.predict(iris.data[:1, :])","d1fab18a":"clf.predict_proba(iris.data[:1, :])","91e12886":"\nclf = tree.DecisionTreeClassifier(max_depth=1,min_samples_leaf=6)\nclf = clf.fit(X_train, y_train)","6e002edb":"ypred = clf.predict(X_test)\n\nprint ('Tree accuracy score')\n\nprint (accuracy_score(y_test, ypred))","1ba642fb":"confusion_mtx = confusion_matrix(Y_true, ypred)\n\n \n\n# plot the confusion matrix\nplot_confusion_matrix(confusion_mtx, classes = range(3)) ","a1868637":"confusion_mtx","5886fee2":"### M\u00e9thode des machines \u00e0 support de vecteurs (SVM)\n","8278d83b":"### B. Importation des donn\u00e9es\nAvec la fonction *read_csv* de Pandas: on peut mettre dans notre dataframe le contenu du fichier csv, en indiquant comme param\u00e8tre (1: le chemin du fichier csv, 2: les s\u00e9parateurs entre les valeurs -dans notre cas des virgules-, 3: un param\u00e8tre facultatif pour sp\u00e9cifier le type d'encodage de notre fichier -exemple encoding =\"UTF8\"-).","5f634768":"### Exercice 1 :\nEn se basant sur ce notebook :\n- Ajouter un code qui cherche les meilleurs param\u00e8tres pour chaque m\u00e9thode. ( vous pouvez utiliser gridsearch)\n- Ajouter d'autres m\u00e9thodes de classification \u00e0 ce notebook\n- Evaluer toutes vos m\u00e9thodes par validation crois\u00e9e (nbr de paquets = 5).\n- Afficher un tableau comparatif des m\u00e9thodes utilis\u00e9es avec les r\u00e9sultats obtenus.","48e33a10":"# DIU-M5 : IA : TD1 : Introduction au Machine Learning\nEric Desjardin, Amine Chemchem","b2cae167":"### M\u00e9thode des arbres de d\u00e9cision","7e3b45ed":"### 1. La classification supervis\u00e9e : \nC\u2019est l\u2019op\u00e9ration qui permet de placer chaque individu de la population dans une classe parmi l\u2019ensemble des classes pr\u00e9\u00e9tablies, en suivant un processus d\u2019apprentissage supervis\u00e9. \nLe choix de la classe d\u2019un individu d\u00e9pend de ses caract\u00e9ristiques.","c80f96f1":"### M\u00e9thode des Random Forests","d778a43e":"## TP1 : Introduction \u00e0 la fouille de donn\u00e9es (Data mining)","220b2c96":"## F. Diviser le dataset en donn\u00e9es d'entrainement et donn\u00e9es de test\nCeci est r\u00e9alisable avec *sklearn* qui permet de prendre al\u00e9atoirement des donn\u00e9es de test \u00e0 partir du benchmark et laisser le reste pour l'apprentissage.\nLa fonction *train_test_split(param1,param2,param3,param4)* prend 4 param\u00e9tres:\nle premier d\u00e9di\u00e9 \u00e0 l'ensemble d'entrainement, le deuxi\u00e8me \u00e0 l'ensemble de test, le troisi\u00e8me est le param\u00e8tre du % de l'ensemble de test (g\u00e9n\u00e9ralement entre 15 et 40%), et le 4 \u00e8me param\u00e8tre (facultatif) pour sp\u00e9cifier quel type de fonction random utiliser:\nsi vous utilisez random_state = some_number, vous pouvez garantir que la sortie de Run 1 sera \u00e9gale \u00e0 la sortie de Run 2, c'est-\u00e0-dire que votre split sera toujours le m\u00eame. Peu importe ce que le nombre r\u00e9el random_state est 42, 0, 21, ... L'important est que chaque fois que vous utilisez 42, vous obtiendrez toujours la m\u00eame sortie la premi\u00e8re fois que vous faites la division. Ceci est utile si vous voulez des r\u00e9sultats reproductibles, par exemple dans la documentation, afin que tout le monde puisse toujours voir les m\u00eames nombres lors de l'ex\u00e9cution des exemples.\n\nCette fonction retourne 4 sorties: \nLa 1\u00e8re est le sous-ensemble al\u00e9atoire d'entrainement \nLa 2\u00e9me est le vecteur de leurs labels (leurs classes).\nLa 3\u00e8me est le sous-ensemble al\u00e9atoire pour le test.\nLa 4\u00e8me est le vecteur de leurs labels (leurs classes).\n\n","e9ed818a":"- Algorithme KPPV (K Nearest Neighbors)\n- R\u00e9seau de neurones\n- Arbre de d\u00e9cision\n- Forets al\u00e9atoires\n- Machine \u00e0 vecteurs de support (SVM)\n- ...","d937df84":"### C. Statistiques descriptives \u00e9l\u00e9mentaires\nLire les informations sur nos donn\u00e9es (Types d'attributs, valeurs manquantes...).\nPandas nous permet de voir les informations sur notre benchmark exemple: avec dataframe.info() il nous affiche tous les attributs de notre fichier avec le type de donn\u00e9e et le nombre de valeurs de chaque colonne\ndataframe.columns permet de citer les noms de toutes les colonnes","af96547b":"## QUESTION 2\nQuelle est la longueur maximale des s\u00e9pales de la s\u00e9tosa ?","86c217ee":"#### D\u00e9finir une fonction :  la fonction lin\u00e9aire par exemple\n\nC'est une fonction simple de la forme: f(x) = a x","3f0693de":"### QUESTION 1\nQuelle est la moyenne de la longueur des p\u00e9tales de la setosa ?","b21e9bc4":"### D. pr\u00e9paration des donn\u00e9es\nDans cette \u00e9tape nous d\u00e9terminons les attributs choisis pour l'entrainement et nous d\u00e9finissons l'attribut \"classe\" de notre benchmark","e0e8992c":"Si on veut sch\u00e9matiser la distribution de nos classes, il suffit de faire appel \u00e0 la libraire seaborn, ensuite d\u00e9finir l'attribut concern\u00e9","0fef73ca":"### Jeu de donn\u00e9es :\nLe jeu de donn\u00e9es *Iris*, utilis\u00e9 dans l''article de Fisher, publi\u00e9 en 1936, intitul\u00e9 \"L'utilisation de plusieurs mesures dans des probl\u00e8mes taxonomiques\".\n\nIl comprend trois esp\u00e8ces d\u2019iris de 50 \u00e9chantillons chacune, ainsi que des propri\u00e9t\u00e9s propres \u00e0 chaque fleur. Une esp\u00e8ce de fleur est s\u00e9parable lin\u00e9airement des deux autres, mais les deux autres ne sont pas s\u00e9parables lin\u00e9airement l'une de l'autre.\n\nLes colonnes de cet ensemble de donn\u00e9es sont:\n\n     Id\n     Longueur du S\u00e9pale Cm\n     Largeur du S\u00e9pale Cm\n     Longueur du P\u00e9tale cm\n     Largeur du P\u00e9tale Cm\n     Esp\u00e8ce : classe : Iris Setosa, Iris Versicolor ou Iris Virginica.\n\nUn \u00e9chantillon : (4.9,3.6,1.4,0.1, \u201cIris-setosa\u201d)","93684510":"### Reponse 1","286f9970":"### A. Importation des librairies\nAvec Pandas on peut manipuler lire (et\/ou \u00e9crire) nos jeux de donn\u00e9es, g\u00e9n\u00e9ralement avec une extension .csv","8fe20452":"Cet ensemble de donn\u00e9es provient d\u2019une extraction fournie par un Institut du diab\u00e8te. Vous pouvez le t\u00e9l\u00e9charger \u00e0 partir [d'ici](https:\/\/drive.google.com\/file\/d\/1lrROnXEB5b55IznkdDKKCK9rZgOCuETK\/view). \nL'objectif de cet ensemble est de construire un outil permettant de r\u00e9aliser un diagnostic positif ou n\u00e9gatif de la pr\u00e9sence d'un diab\u00e8te chez un patient. Plusieurs contraintes ont \u00e9t\u00e9 plac\u00e9es sur la s\u00e9lection de ces instances dans la base de donn\u00e9es d'origine (bien plus volumineuse). En particulier, tous les patients ici sont des femmes \u00e2g\u00e9es d'au moins 21 ans.\n\nLes ensembles de donn\u00e9es comprennent plusieurs variables pr\u00e9dictives m\u00e9dicales et la variable cible \u00ab Outcome \u00bb dont la valeur 1 signifie que la patiente est diab\u00e9tique et la valeur 0 qu'elle ne l'est pas.\nLes variables pr\u00e9dictives comprennent le nombre de grossesses que le patient a eues, son IMC, son taux d'insuline, son \u00e2ge, etc.\n\nChaque ligne repr\u00e9sente un patient et les colonnes sont :\n\n    - Grossesses : nombre de fois o\u00f9 la patiente a d\u00e9j\u00e0 \u00e9t\u00e9 enceinte\n    - Glucose : concentration en glucose plasmatique 2 heures dans un test de tol\u00e9rance au glucose par voie orale\n    - BloodPressure : pression art\u00e9rielle diastolique (mm Hg)\n    - SkinThickness : \u00e9paisseur du pli cutan\u00e9 des triceps (mm)\n    - Insuline : insuline s\u00e9rique de 2 heures (mu U \/ ml)\n    - IMC : indice de masse corporelle (poids en kg \/ (taille en m) ^ 2)\n    - DiabetesPedigreeFunction : Fonction p\u00e9digr\u00e9e du diab\u00e8te\n    - Age : age (ans)\n    - Outcome : Variable cible dont les \u00e9tats sont soit 1 (diab\u00e8te), soit 0 (non diab\u00e8te).\n","24f8dcad":"L\u2019objectif ici est de montrer l\u2019utilisation des m\u00e9thodes de fouille de donn\u00e9es pour les probl\u00e8mes de classification et de r\u00e9gression en python. \n\nCe document reprend librement certains exemples montr\u00e9s dans l\u2019excellente documentation de Scikit-Learn.","f6b8f942":".shape permet de savoir la dimension d'un ensemble.","1b20c5d5":"\nOn peut supprimer la colonne ID :\n\n\n\n","9b37bd3e":"### E. Transformer la colonne des classes en labels num\u00e9riques","63c64d3b":"### Probl\u00e9matique:\n#### Donn\u00e9es :\n- Une liste d\u2019individus X {1..n} caract\u00e9ris\u00e9s par un ensemble d\u2019attributs *P*. \n- Un ensemble C de classes pr\u00e9\u00e9tablies.   \n- Les caract\u00e9ristiques du nouvel individu \u00abindiv\u00bb.\n#### Question\u00a0:\n-  Quelle est la classe appropri\u00e9e \u00e0 \u00abindiv\u00bb ?\n","f4031aed":"### Exercice 2 : Challenge 1 : PR\u00c9DICTION DE DIAB\u00c8TE  \n\nPour r\u00e9aliser ce programme, vous pouvez utiliser en local sur vos machines l'environnement _Jupyter Notebook_ distribu\u00e9 par _anaconda_ https:\/\/www.anaconda.com\/distribution\/\n\nL\u2019objectif fondamental est de pr\u00e9dire si oui ou non un patient est atteint du diab\u00e8te \u00e0 partir de certains attributs : \u00e2ge, nombre de grossesses, taux d\u2019insuline, etc.  \n\n_NB : la comp\u00e9tence \u00e0 utiliser le langage Python n'est pas l'objectif de ce travail, m\u00eame s'il va certainement vous permettre d'\u00e9voluer dans ce domaine. Nous vous conseillons la documentation officielle https:\/\/www.python.org\/ et en particulier la description de sa syntaxe. Vous pouvez ais\u00e9ment conna\u00eetre la version ex\u00e9cut\u00e9e par votre plateforme gr\u00e2ce \u00e0 la commande `print (sys.version)`_\n\nEn se basant sur ce notebook :\n- Refaire les m\u00eames \u00e9tapes sur le dataset : pr\u00e9diction du diab\u00e8te (dont la description est ci-dessous)\n- Ajouter un code qui cherche les meilleurs param\u00e8tres pour chaque m\u00e9thode. ( vous pouvez utiliser gridsearch)\n- Ajouter d'autres m\u00e9thodes de classification \u00e0 ce notebook ( exmple: Naive Bayes, SVM, Random Forest, ... etc)\n- Evaluer toutes vos m\u00e9thodes par validation crois\u00e9e (nbr de paquet = 5).\n- Afficher un tableau comparatif des m\u00e9thodes utilis\u00e9es avec les r\u00e9sultats obtenus. ","d13ffbf9":"### M\u00e9thode des K Plus Proches Voisins ( K Nearest Neighbors)"}}