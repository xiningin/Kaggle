{"cell_type":{"e011e8c5":"code","15aeecb0":"code","b05c4e46":"code","856fc84e":"code","c313a533":"code","51162fe7":"code","a7ca44c5":"code","ad19dc65":"code","3f5aac66":"code","583b7861":"code","daedb2e7":"code","a57feecd":"code","ea105843":"code","56d19c9e":"code","99b33ea4":"code","04bb26c8":"code","b996e587":"code","a6835d6b":"code","735e383d":"code","167f4f0c":"code","a1b0d175":"code","fcddd608":"code","899cc57b":"code","32eba034":"code","c7a0e654":"code","4793402f":"code","84d0e491":"code","1c45ea8d":"code","902a8e22":"code","e20b5b51":"markdown","ccdb8fa3":"markdown","85a59bd2":"markdown","240fd160":"markdown","c0f6db30":"markdown","657fc10e":"markdown","d4b508f2":"markdown","b055eabe":"markdown","697464f5":"markdown","d83fcb00":"markdown","47dc14aa":"markdown","eb34521f":"markdown","b19408d0":"markdown"},"source":{"e011e8c5":"import gc\nimport os\nimport re\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport scipy.sparse\nimport seaborn as sns\nfrom scipy import stats\nfrom sklearn.impute import SimpleImputer  # for categorical variables\nfrom sklearn.preprocessing import LabelEncoder, OneHotEncoder\n\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n\n\n%matplotlib inline\nsns.set_style(\"darkgrid\", {\"axes.facecolor\": \".9\"})","15aeecb0":"#Helper Functions\n\n# I have given thanks to reduce_mem in one of my previous kernels\ndef reduce_mem_usage(df, verbose=True):\n    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    start_mem = df.memory_usage(deep=True).sum() \/ 1024**2\n    for col in df.columns:\n        col_type = df[col].dtypes\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)\n            else:\n                c_prec = df[col].apply(lambda x: np.finfo(x).precision).max()\n                if c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max and c_prec == np.finfo(np.float32).precision:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n    end_mem = df.memory_usage().sum() \/ 1024**2\n    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) \/ start_mem))\n    return df\n\ndef boxcox_Numfeats(frame: pd.DataFrame, columns_to_boxcox: list):\n    \"\"\"\n    Smoothing the numerical-dataframe with box-cox\n    \"\"\"\n    for feature in columns_to_boxcox:\n        print(f\"Processing Feature: {feature}\")\n        \n        # boxcox only takes positive values\n        \n        if frame[feature].min() < 0:\n            \n            rc_bc, bc_params = stats.boxcox(frame[feature]+np.abs(frame[feature].min())+0.0001) \n            frame[feature] = rc_bc\n            \n        else:\n            \n            rc_bc, bc_params  = stats.boxcox(frame[feature]+0.0001) \n            frame[feature] = rc_bc\n            \n        gc.collect()\n    \n    return frame\n\n\n## Thanks to https:\/\/www.amazon.com\/Feature-Engineering-Machine-Learning-Principles\/dp\/1491953241 for help on this. I engineered it a bit more ;)\n\ndef plot_boxcoxes(df : pd.DataFrame,feature_names ,number_of_feats_to_plot = 6):\n    \"\"\"\n    Plots a random sample of the transformed features as a histrograme\n    \"\"\"\n    fig, axes = plt.subplots(number_of_feats_to_plot,1, figsize= (10,15))\n    choices = np.random.choice(feature_names, number_of_feats_to_plot, replace= False)\n    \n    for i, feat in enumerate(choices):\n\n        df[feat].hist(ax=axes[i], bins=100, color = \"sandybrown\")\n        axes[i].set_yscale('log')\n        axes[i].tick_params(labelsize=14)\n        axes[i].set_title(feat+ ' Histogram after boxcox', fontsize=14)\n        axes[i].set_xlabel('')\n        axes[i].set_ylabel('Occurrence', fontsize=14)\n        fig.tight_layout()\n        fig.show()","b05c4e46":"train_df_identity = pd.read_csv(\"\/kaggle\/input\/ieee-fraud-detection\/train_identity.csv\")\ntrain_df_transaction = pd.read_csv(\"\/kaggle\/input\/ieee-fraud-detection\/train_transaction.csv\")\ntest_df_identity = pd.read_csv(\"\/kaggle\/input\/ieee-fraud-detection\/test_identity.csv\")\ntest_df_transaction = pd.read_csv(\"\/kaggle\/input\/ieee-fraud-detection\/test_transaction.csv\")\n\nsub_df = pd.read_csv(\"\/kaggle\/input\/ieee-fraud-detection\/sample_submission.csv\")\n\ntrain_df = pd.merge(left=train_df_identity, right=train_df_transaction, how='right', on='TransactionID', validate ='one_to_many')\ntest_df = pd.merge(left=test_df_identity, right=test_df_transaction, how='right', on='TransactionID', validate ='one_to_many')\n\ntrain_df.set_index(\"TransactionID\", inplace=True)\n# train_df.drop([\"TransactionID\"], axis=1,inplace=True) #This wont be needed in training set its already in ID\n\ntrain_df.drop(['isFraud'],axis=1, inplace=True) # we don't want to leak this into our predictor features\n\n## The following code aids us in submission later on\nsub_df.set_index(\"TransactionID\", inplace=True)\ntest_df.set_index(\"TransactionID\", inplace=True)\n\n\ntest_df = test_df.reindex(sub_df.index) #So order is similar to submission file\nassert all(test_df.index.values == sub_df.index.values) #Test if this worked\n\ndel train_df_identity,train_df_transaction,test_df_identity,test_df_transaction\n\ngc.collect()","856fc84e":"#according to https:\/\/www.kaggle.com\/c\/ieee-fraud-detection\/discussion\/101203#latest-607486\n\nCatfeats = ['ProductCD'] + \\\n           [\"card\"+f\"{i+1}\" for i in range(6)] + \\\n           [\"addr\"+f\"{i+1}\" for i in range(2)] + \\\n           [\"P_emaildomain\", \"R_emaildomain\"] + \\\n           [\"M\"+f\"{i+1}\" for i in range(9)] + \\\n           [\"DeviceType\", \"DeviceInfo\"] + \\\n           [\"id_\"+f\"{i}\" for i in range(12, 39)]\n\nNumfeats = list(set(train_df.columns)-set(Catfeats))\n\nassert set(Catfeats+Numfeats) == set(train_df.columns.values)","c313a533":"for col in Catfeats:\n    train_df[col] = train_df[col].astype('str').astype('category')\n    test_df[col] = test_df[col].astype('str').astype('category')\n    \ntrain_df[Catfeats] = train_df[Catfeats].replace('nan', np.nan) # to create a sparse matrix    \ntest_df[Catfeats] = test_df[Catfeats].replace('nan', np.nan) # to create a sparse matrix","51162fe7":"# test_df['isFraud'] = np.NaN \n\n# train and test have same columns so we append\n\ntest_indeces = test_df.index\ntrain_indeces = train_df.index\n\nimputed_df = train_df.append(test_df, ignore_index=False, verify_integrity = True, sort=True)\n\ndel train_df,test_df\n\nimputed_df = reduce_mem_usage(imputed_df)\n\ngc.collect()","a7ca44c5":"#######\n# I still need to figure out how to save memory. If we take all data the kernel crashes\n\nimputed_df = imputed_df.sample(frac=0.15)\ngc.collect()","ad19dc65":"imputed_df.head()","3f5aac66":"imp = SimpleImputer(strategy=\"most_frequent\", missing_values= np.nan)\n\nfor col in Catfeats:\n    imputed_df[col] = imp.fit_transform(imputed_df[col].as_matrix().reshape(-1,1))","583b7861":"imputed_df[Catfeats].head()","daedb2e7":"# missing_na_perc = (imputed_df[Numfeats].isna().sum()\/len(imputed_df[Numfeats])).sort_values(ascending=False)\n\n# print(f\"Missing percentage of values per feature:\\n\\n{missing_na_perc}\")","a57feecd":"# sixty_perc_or_less = list(missing_na_perc.loc[missing_na_perc <= .6].index)\n# sixty_perc_or_more = list(missing_na_perc.loc[missing_na_perc > .6].index)","ea105843":"# Simple imputation for al features. Iterative umputer is too computationally expensive\nfrom sklearn.impute import SimpleImputer \n\n# to introduce as less bias as we can, we iterate between mean and median for imputer\n\nimpute_method = 'mean'\n\nfor col in Numfeats:\n    if impute_method == 'mean':\n        impute_method = 'median'\n        imp_mean = SimpleImputer(missing_values=np.nan, strategy=impute_method)\n        imputed_df[col] = imp_mean.fit_transform(imputed_df[col].ravel().reshape(-1,1))\n    elif impute_method == 'median':     \n        impute_method = 'mean'\n        imp_mean = SimpleImputer(missing_values=np.nan, strategy=impute_method)\n        imp_mean.fit_transform(imputed_df[col].ravel().reshape(-1,1))\n        imputed_df[col] = imp_mean.fit_transform(imputed_df[col].ravel().reshape(-1,1))\n        \n\ngc.collect()","56d19c9e":"assert imputed_df[Numfeats].isna().sum().sum() == 0, \"We aren't done\"","99b33ea4":"# # Complex imputation of more than 60% of missing values\n# from sklearn.experimental import enable_iterative_imputer  # noqa\n# # now you can import normally from sklearn.impute\n# from sklearn.impute import IterativeImputer # for our numerical outpurs\n\n# finished_cols = []\n\n# for col in sixty_perc_or_more:\n    \n#     imp_iterative = IterativeImputer(max_iter=50, min_value = train_df[col].min(), max_value = train_df[col].max())\n#     train_df[col] = imp_iterative.fit_transform(pd.concat([train_df[sixty_perc_or_less + finished_cols], train_df[col]], axis=1))[:,-1] # we take the last column as that is the \n#                                                                                                                         # one we are imputing using the rest\n        \n#     finished_cols.append(col)\n    \n#     gc.collect()","04bb26c8":"# assert train_df.isna().sum().sum() == 0, \"We aren't done\"","b996e587":"print(\"before boxcox:\")\nplot_boxcoxes(imputed_df[Numfeats],feature_names = Numfeats)","a6835d6b":"imputed_df = boxcox_Numfeats(imputed_df, Numfeats)\n\nimputed_df = reduce_mem_usage(imputed_df)\n\ngc.collect()","735e383d":"print(\"after boxcox:\")\nplot_boxcoxes(imputed_df[Numfeats],feature_names = Numfeats)","167f4f0c":"less_than_4_levels = imputed_df[Catfeats].columns[np.where(imputed_df[Catfeats].nunique() <= 4)]\nmore_than_4_levels = imputed_df[Catfeats].columns[np.where(imputed_df[Catfeats].nunique() > 4)]","a1b0d175":"## Less or equal than 4 levels we do label encoder:\n\nle = LabelEncoder()\nfor col in less_than_4_levels:\n    imputed_df[col] = le.fit_transform(imputed_df[col].astype(str))\n    gc.collect()","fcddd608":"## More than 4 levels we do one-hot encoder:\n\n# enc = OneHotEncoder(handle_unknown='ignore')\n# OH_encoded_sparse = enc.fit_transform(imputed_df[more_than_4_levels])","899cc57b":"object_feats = (imputed_df.dtypes == 'O').index\n\nstrings_feats = []\n\nfor obj_f in object_feats:\n    try:\n         imputed_df[obj_f] = imputed_df[obj_f].astype('float')\n    except:\n        strings_feats.append(obj_f)","32eba034":"imputed_df[strings_feats]","c7a0e654":"for col in strings_feats:\n    imputed_df[col] = le.fit_transform(imputed_df[col].astype(str))\n    gc.collect()","4793402f":"# we will need our indeces to restore the sparse matrix in our next kernel\n# imputed_df.reset_index(inplace=True)\n# imputed_df.head()\n\n# test_csr_indeces = np.where(imputed_df.TransactionID.isin(test_indeces))","84d0e491":"# # imputed_df.drop(more_than_4_levels,axis=1, inplace=True)\n# imputed_df = scipy.sparse.csr_matrix(imputed_df)\n\n# #adding one hot encoded features\n# imputed_df = scipy.sparse.hstack((imputed_df, OH_encoded_sparse))\n\n# gc.collect()","1c45ea8d":"# # Now we get our original training and test sets back\n# test_sparse = imputed_df.tocsr()[np.where(imputed_df.TransactionID.isin(test_indeces))]\n# train_sparse_no_target_var = imputed_df.tocsr()\n\n# del imputed_df;gc.collect()","902a8e22":"# train_sparse_no_target_var.to_pickle(\"train_sparse_no_target_var.pkl\") # You can load it in your kernel using df = pd.read_pickle(\"Imputed_Train.pkl\") \n\n# scipy.sparse.save_npz('\/tmp\/imputed_df.npz', imputed_df)\n\n# scipy.sparse.save_npz('\/tmp\/train_sparse_no_target_var.npz', train_sparse_no_target_var)\n\n# You can load it in your kernel using sparse_matrix = scipy.sparse.load_npz('\/tmp\/sparse_matrix.npz')\n\nimputed_df.to_csv(\"Imputed_df.csv\",index=True)","e20b5b51":"This is a helper kernel for anyone struggling to deal with all the missing values and unsmooth features. Im sure it does add quite a lot of bias, but for linear models the NaN values are a pain so I hope this helps you.\n# \n# We also encode labels for ease of use\n# \n# If you want to see my further analysis where I build on this data using catboost and Logistic Regression  on this Please see https:\/\/www.kaggle.com\/pipboyguy\/simple-models-smoothing-of-features","ccdb8fa3":"Lets transform the categorical features that present as floats and text to pandas categorical","85a59bd2":"## Feature smoothing","240fd160":"## Finally we are done!!\n# \n# I save the finished product. I hope it serves you well ;) Just remember that we introduced a lot of bias by imputing in this fasion## Finally we are done!!\n# \n# I save the finished product. I hope it serves you well ;) Just remember that we introduced a lot of bias by imputing in this fasion\n","c0f6db30":"#### before boxcox","657fc10e":"Plotting a random sample of the box coxed features:","d4b508f2":"![](https:\/\/cdn.cnn.com\/cnnnext\/dam\/assets\/140820084625-peanut-butter-stock-super-tease.jpg)","b055eabe":"We combine the sparse matrix to the original (sparsed) imputed_df","697464f5":"#### after boxcox","d83fcb00":"We combine train and test to impute values","47dc14aa":"Let's get rid of these pesky critters","eb34521f":"Categorical features look good. We will now focus on numerical features  Simple imputation for all features. Iterative imputer is too computationally expensive","b19408d0":"### Encoding of Categorical Features"}}