{"cell_type":{"12a8597f":"code","8ec241af":"code","1aae4c94":"code","aa3f5f4f":"code","d6011895":"code","849e5f0c":"code","a00f02d4":"code","4abf18e6":"code","145a41c7":"markdown","7ae912c0":"markdown","31f63dd1":"markdown","554e85e6":"markdown","e53bd8c9":"markdown","cd3a12a0":"markdown","0ff20b15":"markdown","e7081a3a":"markdown","a6723324":"markdown"},"source":{"12a8597f":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","8ec241af":"#install packages\n!pip install -r https:\/\/github.com\/casperbh96\/Web-Scraping-Reddit\/raw\/master\/requirements.txt","1aae4c94":"#import libs\nimport numpy as np\nimport pandas as pd\n\nfrom urllib.request import urlopen\nfrom bs4 import BeautifulSoup","aa3f5f4f":"def getHTMLContent(link):\n    html = urlopen(link)\n    soup = BeautifulSoup(html, 'html.parser')\n    return soup","d6011895":"content = getHTMLContent('https:\/\/en.wikipedia.org\/wiki\/List_of_countries_and_dependencies_by_population')\ntables = content.find_all('table')\nfor table in tables:\n    print(table.prettify())","849e5f0c":"table = content.find('table', {'class': 'wikitable sortable'})\nrows = table.find_all('tr')\n\n# List of all links\nfor row in rows:\n    cells = row.find_all('td')\n    if len(cells) > 1:\n        country_link = cells[1].find('a')\n       # print(country_link.get('href'))","a00f02d4":"def getAdditionalDetails(url):\n    try:\n        country_page = getHTMLContent('https:\/\/en.wikipedia.org' + url)\n        table = country_page.find('table', {'class': 'infobox geography vcard'})\n        additional_details = []\n        read_content = False\n        for tr in table.find_all('tr'):\n            if (tr.get('class') == ['mergedtoprow'] and not read_content):\n                link = tr.find('a')\n                if (link and (link.get_text().strip() == 'Area' or\n                   (link.get_text().strip() == 'GDP' and tr.find('span').get_text().strip() == '(nominal)'))):\n                    read_content = True\n                if (link and (link.get_text().strip() == 'Population')):\n                    read_content = False\n            elif ((tr.get('class') == ['mergedrow'] or tr.get('class') == ['mergedbottomrow']) and read_content):\n                additional_details.append(tr.find('td').get_text().strip('\\n')) \n                if (tr.find('div').get_text().strip() != '\u2022\\xa0Total area' and\n                   tr.find('div').get_text().strip() != '\u2022\\xa0Total'):\n                    read_content = False\n        return additional_details\n    except Exception as error:\n        print('Error occured: {}'.format(error))\n        return []","4abf18e6":"data_content = []\nfor row in rows:\n    cells = row.find_all('td')\n    if len(cells) > 1:\n        print(cells[1].get_text())\n        country_link = cells[1].find('a')\n        country_info = [cell.text.strip('\\n') for cell in cells]\n        additional_details = getAdditionalDetails(country_link.get('href'))\n        if (len(additional_details) == 4):\n            country_info += additional_details\n            data_content.append(country_info)\n\ndataset = pd.DataFrame(data_content)\n\n# Define column headings\nheaders = rows[0].find_all('th')\nheaders = [header.get_text().strip('\\n') for header in headers]\nheaders += ['Total Area', 'Percentage Water', 'Total Nominal GDP', 'Per Capita GDP']\ndataset.columns = headers\n\ndrop_columns = ['Rank', 'Date', 'Source']\ndataset.drop(drop_columns, axis = 1, inplace = True)\ndataset.sample(3)\n\ndataset.to_csv(\"Dataset.csv\", index = False)","145a41c7":"Generally, web scraping involves accessing numerous websites and collecting data from them. However, we can limit ourselves to collect large amount of information from a single source and use it as a dataset.","7ae912c0":"Initially, we define just the basic function of reading the url and then extracting the HTML from the same.","31f63dd1":"now lets clean the data.","554e85e6":"Create the dataset","e53bd8c9":"We use try and except to ensure that even if we miss certain values, our whole process does not end. ","cd3a12a0":"We\u2019ll access the List of countries and dependencies by population Wikipedia webpage. The webpage includes a table with the names of countries, their population, date of data collection, percentage of world population and source. And if we go to any country\u2019s page, all information about it is written on the page with a standard box on the right. This box includes a lot of information such as total area, water percentage, GDP etc.\nHere, we will combine the data from these two webpages into one dataset.\nList of Countries: On accessing the first page, we\u2019ll extract the list of countries, their population and percentage of world population.\nCountry: We\u2019ll then access each country\u2019s page, and get information including total area, percentage water, and GDP (nominal).\nThus, our final dataset will include information about each country.","0ff20b15":"we use the find_all() method to find all tables on the page. The parameter that we supply inside this function determines the element that it returns. As we require tables, we pass the argument as table and then iterate over all tables to identify the one we need.","e7081a3a":"Now, we need to analyse the output and see which table has the data we are searching for. After much inspection, we can see that the table with the class, wikitable sortable, has the data we need. ","a6723324":"Iterating over all rows in the table, the iterator checks if the present row is a heading matching Area or GDP (nominal) and starts reading. In reading mode, it checks if the new element is Total area or Total and if yes, it reads it and keeps reading to read the Water (%) or per capita GDP respectively in the next run."}}