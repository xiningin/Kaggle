{"cell_type":{"ac5cb6e1":"code","1370a895":"code","de743e80":"code","7cf09741":"code","16b8f643":"code","36a1fcf2":"code","97caf7f9":"code","70e1bd1b":"code","6ac002d5":"code","aa31253b":"code","00e8b0d3":"code","b7684d92":"code","92cd870c":"markdown","bdc59fea":"markdown","13b9a5e8":"markdown","421cc5bc":"markdown","c9543025":"markdown","c76300d1":"markdown","ae4a1568":"markdown"},"source":{"ac5cb6e1":"import pandas as pd\nimport numpy as np\nfrom sklearn.metrics import mean_squared_error","1370a895":"train_data = pd.read_csv(\"..\/input\/30-days-of-ml\/train.csv\")\ntrain_data.head()","de743e80":"from pandas_profiling import ProfileReport\nreport = ProfileReport(train_data, title=\"Data Quality Report\")\nreport.to_widgets()","7cf09741":"from sklearn.model_selection import train_test_split\nX = train_data.drop(\"target\", axis=1)\ny = train_data.target\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n","16b8f643":"object_col = [col for col in X_train.columns if X_train[col].dtype == object]\n\n# Columns that will be one-hot encoded\n\nhigh_cardinality_cols = [col for col in object_col if X_train[col].nunique() > 3]\n\n# Columns that will be dropped from the dataset\n\nlow_cardinality_cols = list(set(object_col) - set(high_cardinality_cols))","36a1fcf2":"print('Categorical columns that will be one-hot encoded:', high_cardinality_cols)\nprint('\\nCategorical columns that will be dropped from the dataset:', low_cardinality_cols)","97caf7f9":"from sklearn.preprocessing import OneHotEncoder\n\n\n# Apply one-hot encoder to each column with categorical data\nOH_encoder = OneHotEncoder(handle_unknown='ignore', sparse=False)\nOH_cols_train = pd.DataFrame(OH_encoder.fit_transform(X_train[high_cardinality_cols]))\nOH_cols_test = pd.DataFrame(OH_encoder.transform(X_test[high_cardinality_cols]))\n\n# One-hot encoding removed index; put it back\nOH_cols_train.index = X_train.index\nOH_cols_test.index = X_test.index\n\n# Remove categorical columns (will replace with one-hot encoding)\nnum_X_train = X_train.drop(object_col, axis=1)\nnum_X_test = X_test.drop(object_col, axis=1)\n\n# Add one-hot encoded columns to numerical features\nOH_X_train = pd.concat([num_X_train, OH_cols_train], axis=1)\nOH_X_test = pd.concat([num_X_test, OH_cols_test], axis=1)\n\nOH_X_train.head()","70e1bd1b":"from xgboost import XGBRegressor\n\nmodel = XGBRegressor(n_estimators= 2700,\n learning_rate = 0.1,\n colsample_bytree= 0.85,\n gamma= 0.85,\n eval_metric= 'rmse',\n max_depth= 12,\n min_child_weight= 14.0,\n subsample= 0.81,\n tree_method= 'hist')\n\n# Train fit the model on training dataset once again\n\nmodel.fit(OH_X_train, y_train)","6ac002d5":"preds_test = model.predict(OH_X_test)\n\nscore = mean_squared_error(preds_test,y_test,squared=False)\n\nprint('RMSE:', score * 100)","aa31253b":"submission_sample_data = pd.read_csv('..\/input\/30-days-of-ml\/sample_submission.csv')\nsubmission_sample_data.head()","00e8b0d3":"# First we read the test data into a variable\ntest = pd.read_csv('..\/input\/30-days-of-ml\/test.csv')\n\nOH_cols_test = pd.DataFrame(OH_encoder.fit_transform(test[high_cardinality_cols]))\n\n# One-hot encoding removed index; put it back\nOH_cols_test.index = test.index\n\n\n# Remove categorical columns (will replace with one-hot encoding)\nnum_test = test.drop(object_col, axis=1)\n\n# Add one-hot encoded columns to numerical features\nOH_test = pd.concat([num_test, OH_cols_test], axis=1)\n\nOH_test.head()","b7684d92":"# Fill in the line below: preprocess test data\nfinal_X_test = pd.DataFrame(OH_test)\n\n\n# Fill in the line below: get test predictions\npreds_test = model.predict(final_X_test)\n\n\n# Save test predictions to file with only two columns, id and the prediction values under the target column\noutput = pd.DataFrame({'id': final_X_test.id,\n                       'target': preds_test})\noutput.to_csv('submission.csv', index=False)\n\nprint(output.head())","92cd870c":"# **Test Data**\n\nThe test data needs the same treatment as the training data before it can be used for prediction using XGBoost. So we need to encode the categorical columns to numerical values.","bdc59fea":"# **Data Profile**","13b9a5e8":"# **Train Data**","421cc5bc":"# **Final Submission**","c9543025":"# **Importing Libaries**","c76300d1":"# **XGBoost Algorithm**\n\nXGBoost is a decision-tree-based ensemble Machine Learning algorithm that uses a gradient boosting framework. In prediction problems involving unstructured data (images, text, etc.) artificial neural networks tend to outperform all other algorithms or frameworks. However, when it comes to small-to-medium structured\/tabular data, decision tree based algorithms are considered best-in-class right now.","ae4a1568":"# **Spliting & Preprocessing of data**\n1. Lets first split the training data into training and validation data. We will use the technique explained on [30 Days of ML] Day 9 Lesson 4 to achieve a 80:20 split of data. That is 80% of the data will be used for training and 20% will be used to validate our model.\n \n1. NOTE: The target column here is \"target\" and hence it should be dropped from the training input X."}}