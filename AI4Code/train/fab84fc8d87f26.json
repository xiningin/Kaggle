{"cell_type":{"36dbb6c2":"code","1b4496dc":"code","9cb1cc96":"code","27754a7e":"code","ea870093":"code","9a60527f":"code","ed6fd579":"code","4e8c85dc":"code","3805be47":"code","88566185":"code","ea631802":"code","e9453f3d":"code","1f7aa68c":"code","b4f61f92":"code","4f887f00":"code","71a5b42d":"code","93ab848d":"code","1aee75e9":"code","05e68f14":"markdown","3bcf45a6":"markdown","f50fb8b7":"markdown","8785fe44":"markdown","a4a19282":"markdown","a50eb5bf":"markdown","e761d354":"markdown","1bd46819":"markdown","a10187ad":"markdown","5ed973df":"markdown","496ca2c9":"markdown","f659e960":"markdown","5feab901":"markdown","b0ff5d15":"markdown","709a7be0":"markdown","6e921632":"markdown","44a37b59":"markdown","0f2711cc":"markdown","de2f9c14":"markdown","e86d4f9e":"markdown","61f56633":"markdown","04c84e0a":"markdown"},"source":{"36dbb6c2":"import os\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport catboost\nimport tensorflow as tf  # Just for checking if GPU is available :)\n\nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.metrics import make_scorer, cohen_kappa_score, accuracy_score\nfrom sklearn.model_selection import RandomizedSearchCV, cross_val_score, StratifiedKFold, RepeatedStratifiedKFold\nfrom sklearn.ensemble import VotingClassifier\nimport scipy\n\n%matplotlib inline\nplt.rc('figure', figsize=(20.0, 10.0))\n\nGPU_AVAILABLE = tf.test.is_gpu_available()\nQUADRATIC_WEIGHT_SCORER = make_scorer(cohen_kappa_score, weights='quadratic')\nprint(\"GPU available:\", GPU_AVAILABLE)","1b4496dc":"INPUT_DIR = \"..\/input\"\nprint(os.listdir(INPUT_DIR))","9cb1cc96":"import json\n\ndef read_sentiment_if_exists(directory: str, pet_id: str):\n    path = os.path.join(directory, \"{}.json\".format(pet_id))\n    if not os.path.exists(path):\n        return None\n    with open(path, 'r') as f:\n        return json.load(f)\n        \ndef parse_sentiment(sentiment):\n    documentSentiment = sentiment['documentSentiment']\n    return { 'SentimentMagnitude': documentSentiment['magnitude'],\n             'SentimentScore': documentSentiment['score'],\n             'SentimentLanguage': sentiment['language'] }\n\ndef read_csv_to_pandas(is_train=True):\n    path = os.path.join(INPUT_DIR, 'train', 'train.csv') if is_train else os.path.join(INPUT_DIR, 'test', 'test.csv')\n    df = pd.read_csv(path)\n    \n    def add_sentiment_if_exists(row):\n        pet_id = row.get('PetID')\n        directory = os.path.join(INPUT_DIR, 'train_sentiment') if is_train else os.path.join(INPUT_DIR, 'test_sentiment')\n        sentiment_or_none = read_sentiment_if_exists(directory, pet_id)\n        if sentiment_or_none is None:\n            return row.append(pd.Series({ 'SentimentMagnitude': np.nan, 'SentimentScore': np.nan, 'SentimentLanguage': \"unknown\" }))\n        sentiment_json = sentiment_or_none\n        sentiment = parse_sentiment(sentiment_json)\n        sentiment_series = pd.Series(sentiment)\n        return row.append(sentiment_series)\n\n    df = df.apply(add_sentiment_if_exists, axis=1, result_type='expand')\n    df.Name = df.Name.fillna(value=\"Unknown\")\n    return df.astype({\n        'Type': 'category',\n        'Name': 'category',\n        'Breed1': 'category',\n        'Breed2': 'category',\n        'Gender': 'category',\n        'Color1': 'category',\n        'Color2': 'category',\n        'Color3': 'category',\n        'MaturitySize': 'category',\n        'FurLength': 'category',\n        'Vaccinated': 'category',\n        'Dewormed': 'category',\n        'Sterilized': 'category',\n        'Health': 'category',\n        'State': 'category',\n        'RescuerID': 'category',\n        'SentimentLanguage': 'category'\n    })\n\ntrain_df = read_csv_to_pandas(is_train=True)\n\nX_test = read_csv_to_pandas(is_train=False)\ntrain_df.info()","27754a7e":"from sklearn.model_selection import train_test_split\n\ndef to_features_and_labels(df):\n    y = df['AdoptionSpeed'].values\n    X = df.drop('AdoptionSpeed', axis=1)\n    return X, y\n\nX_train_val, y_train_val = to_features_and_labels(train_df)\n\nX_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=0.20, random_state=42,\n                                                  stratify=y_train_val)\n\nprint(\"Shape of X_train:\", X_train.shape)\nprint(\"Shape of X_val:\", X_val.shape)\nprint(\"Shape of y_train:\", y_train.shape)\nprint(\"Shape of y_val:\", y_val.shape)","ea870093":"class DataFrameColumnDropper(BaseEstimator, TransformerMixin):\n    \"\"\"\n    Drop given columns.\n    \n    Attributes:\n        column_names (List[Str]): List of columns to drop\n    \"\"\"\n    def __init__(self, column_names):\n        self.column_names = column_names\n    def fit(self, X, y=None):\n        return self\n    def transform(self, X):\n        return X.copy().drop(self.column_names, axis=1)\n    \nclass ColumnImputer(BaseEstimator, TransformerMixin):\n    def __init__(self, column_name, imputer):\n        self.column_name = column_name\n        self.imputer = imputer\n    def fit(self, X, y=None):\n        values = X[[self.column_name]].values\n        self.imputer.fit(values)\n        return self\n    def transform(self, X):\n        Y = X.copy()\n        Y.loc[:, self.column_name] = self.imputer.transform(Y[[self.column_name]].values)\n        return Y","9a60527f":"from sklearn.impute import SimpleImputer\n\ndef build_preprocessing_pipeline() -> Pipeline:\n     return Pipeline([\n        ('impute_sentiment_magnitude', ColumnImputer(column_name='SentimentMagnitude',\n                                                     imputer=SimpleImputer(strategy='constant', fill_value=0.0))),\n        ('impute_sentiment_score', ColumnImputer(column_name='SentimentScore',\n                                                     imputer=SimpleImputer(strategy='constant', fill_value=0.0))),\n        ('drop_unused_columns', DataFrameColumnDropper(\n            column_names=['PetID', 'Description', 'RescuerID'])\n        )\n     ])\n\npreprocessing_pipeline = build_preprocessing_pipeline()\nX_train_preprocessed = preprocessing_pipeline.fit_transform(X_train, y_train)\n\nX_train_preprocessed.head(20)","ed6fd579":"print(\"Number of features:\", len(list(X_train_preprocessed)))\nprint(\"\")\n\nprint(\"Numerical columns:\", list(X_train_preprocessed.select_dtypes(include=\"number\")))\nprint(\"\")\n\nprint(\"Categorical columns:\", list(X_train_preprocessed.select_dtypes(include=\"category\")))","4e8c85dc":"from catboost import CatBoostClassifier\n\nclass CatBoostPandasClassifier:\n    \"\"\"\n    Helper class for training `CatBoostClassifier` with Pandas dataframes. The class\n    passes the columns of type \"category\" as `cat_features` to `CatBoostClassifier`'s fit method and ensures\n    that the order of the columns in the fitting and prediction phases matches.\n    \n    Author: Kimmo S\u00e4\u00e4skilahti\n    \"\"\"\n    def __init__(self, *args, **kwargs):\n        self.catboost_classifier = CatBoostClassifier(*args, **kwargs)\n        self.columns = None\n        \n    def fit(self, X, y, *args, **kwargs):\n        self.columns = list(X)\n        cat_columns = list(X.select_dtypes(include='category'))\n        cat_features = [X.columns.get_loc(name) for name in cat_columns]  # Indices of categorical features\n        return self.catboost_classifier.fit(X.values, y, *args, cat_features=cat_features, **kwargs)\n        \n    def copy(self, *args, **kwargs):\n        returned_classifier = CatBoostPandasClassifier()\n        returned_classifier.catboost_classifier = self.catboost_classifier.copy()\n        returned_classifier.columns = self.columns\n        return returned_classifier\n        \n    def predict(self, X, *args, **kwargs):\n        X_copy = X.loc[:, self.columns].copy()\n        return self.catboost_classifier.predict(X_copy.values, *args, **kwargs)\n    \n    def predict_proba(self, X, *args, **kwargs):\n        X_copy = X.loc[:, self.columns].copy()\n        return self.catboost_classifier.predict_proba(X_copy.values, *args, **kwargs)\n        \n    def __getattr__(self, attr):\n        \"\"\"\n        Pass all other method calls to self.catboost_classifier.\n        \"\"\"\n        return getattr(self.catboost_classifier, attr)\n\n# Example usage\ncatboost_pandas_clf = CatBoostPandasClassifier(iterations=10,\n                                               learning_rate=0.1,\n                                               loss_function='MultiClass',\n                                               allow_writing_files=False)\n# catboost_pandas_clf.fit(X_train_preprocessed, y_train)\ncross_val_score(catboost_pandas_clf, X_train_preprocessed, y_train, cv=5, scoring=QUADRATIC_WEIGHT_SCORER)","3805be47":"def build_search(pipeline, param_distributions, cv=5, n_iter=10):\n    \"\"\"\n    Builder function for RandomizedSearch.\n    \"\"\"\n    return RandomizedSearchCV(pipeline,\n                              param_distributions=param_distributions, \n                              cv=cv,\n                              return_train_score=True,\n                              refit='cohen_kappa_quadratic',\n                              n_iter=n_iter,\n                              n_jobs=None,\n                              scoring={\n                                    'accuracy': make_scorer(accuracy_score),\n                                    'cohen_kappa_quadratic': QUADRATIC_WEIGHT_SCORER\n                              },\n                              verbose=1,\n                              random_state=42)\n\ndef pretty_cv_results(cv_results, \n                      sort_by='rank_test_cohen_kappa_quadratic',\n                      sort_ascending=True,\n                      n_rows=30):\n    \"\"\"\n    Return pretty Pandas dataframe from the `cv_results_` attribute of finished parameter search,\n    ranking by test performance and only keeping the columns of interest.\n    \"\"\"\n    df = pd.DataFrame(cv_results)\n    cols_of_interest = [key for key in df.keys() \n                            if key.startswith('param_') \n                                or key.startswith(\"mean_train\")\n                                or key.startswith(\"std_train\")\n                                or key.startswith(\"mean_test\")\n                                or key.startswith(\"std_test\")\n                                or key.startswith('mean_fit_time')\n                                or key.startswith('rank')]\n    return df.loc[:, cols_of_interest].sort_values(by=sort_by, ascending=sort_ascending).head(n_rows)\n\ndef run_search(search):\n    search.fit(X_train, y_train)\n    print('Best score is:', search.best_score_)\n    return pretty_cv_results(search.cv_results_)","88566185":"task_type = \"GPU\" if GPU_AVAILABLE else \"CPU\"\n\ndef build_catboost_pipeline():\n    return Pipeline([\n        ('preprocessing', preprocessing_pipeline),\n        ('classifier', CatBoostPandasClassifier(verbose=0,\n                                                loss_function='MultiClass',\n                                                allow_writing_files=False,\n                                                task_type=task_type))\n    ])\n\nparam_distributions = {\n    'classifier__iterations': [500, 1000],  # Sets the value of `iterations` to the pipeline step `classifier` in parameter search\n    'classifier__learning_rate': scipy.stats.uniform(0.01, 0.3),\n    'classifier__max_depth': scipy.stats.randint(3, 10),\n    'classifier__one_hot_max_size': [30],\n    'classifier__l2_leaf_reg': scipy.stats.reciprocal(a=1e-2, b=1e1)  # Samples *exponents* uniformly between a and b\n}\n\ncatboost_search = build_search(build_catboost_pipeline(),\n                               param_distributions=param_distributions,\n                               n_iter=50,\n                               cv=RepeatedStratifiedKFold(n_splits=10, n_repeats=1, random_state=42))\ncatboost_cv_results = run_search(search=catboost_search)\ncatboost_cv_results","ea631802":"best_estimator = catboost_search.best_estimator_\nprint(best_estimator.named_steps['classifier'].get_params())","e9453f3d":"all_estimators = [build_catboost_pipeline().set_params(**params) for params in catboost_search.cv_results_['params']]\n\nscores_and_estimators = list(zip(catboost_search.cv_results_['mean_test_cohen_kappa_quadratic'], all_estimators))\n\nscores_and_estimators.sort(key=lambda x: x[0], reverse=True)\n\nscores_and_estimators[:3]","1f7aa68c":"N_TOP_ESTIMATORS = 5\n\nbest_estimators = scores_and_estimators[:N_TOP_ESTIMATORS]\nvoting_classifier = VotingClassifier([(str(score), estimator) for score, estimator in best_estimators], voting='soft')","b4f61f92":"best_estimator = catboost_search.best_estimator_\ny_val_pred_best_estimator = best_estimator.predict(X_val)\n\nvoting_classifier.fit(X_train, y_train)\ny_val_pred_voting_classifier = voting_classifier.predict(X_val)\n\nprint(\"Performance of best estimator on the hold-out set:\",\n      cohen_kappa_score(y_val, y_val_pred_best_estimator, weights='quadratic'))\nprint(\"Performance of voting classifier on the hold-out set:\",\n      cohen_kappa_score(y_val, y_val_pred_voting_classifier, weights='quadratic'))","4f887f00":"column_names = best_estimator.named_steps['classifier'].columns\nfeature_importances = best_estimator.named_steps['classifier'].feature_importances_\nprint(\"{} columns, {} feature importances.\".format(len(column_names), len(feature_importances)))\n\nfeatures_and_importances = list(zip(column_names, feature_importances))\n\nfeatures_and_importances.sort(key=lambda x: x[1], reverse=True)\n\nfor name, feature_importance in features_and_importances:\n    print(\"{} -> {}\".format(name, feature_importance))","71a5b42d":"voting_classifier.fit(X_train_val, y_train_val)","93ab848d":"def get_predictions(estimator, X):\n    predictions = estimator.predict(X).astype(np.int32)\n    predictions = np.squeeze(predictions)  # Estimators may return arrays for each prediction\n    indices = X.loc[:, 'PetID']\n    as_dict = [{'PetID': index, 'AdoptionSpeed': prediction} for index, prediction in zip(indices, predictions)]\n    df = pd.DataFrame.from_dict(as_dict)\n    df = df.reindex(['PetID', 'AdoptionSpeed'], axis=1)\n    return df\n\npredictions = get_predictions(best_estimator, X=X_test)","1aee75e9":"def write_submission(predictions):\n    submission_folder = '.'\n    dest_file = os.path.join(submission_folder, 'submission.csv')\n    predictions.to_csv(dest_file, index=False)\n    print(\"Wrote to {}\".format(dest_file))\n    \nwrite_submission(predictions)","05e68f14":"### Print the columns:","3bcf45a6":"### Define imports and load data","f50fb8b7":"## Parameter tuning","8785fe44":"Check the parameters of the best estimator:","a4a19282":"### Define transformers\nWe'll use [scikit-learn pipelines](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.pipeline.Pipeline.html) to define our data preprocessing transforms. Because we're using CatBoost that can handle categorical features quite well, we don't do any other preprocessing than drop some of the columns and impute missing values for sentiments. For that, we'll define custom transformers `DataFrameColumnDropper` and `ColumnImputer`:","a50eb5bf":"### Print feature importances for the best estimator","e761d354":"Write `submission.csv`:","1bd46819":"Build the pipeline:","a10187ad":"### Split training data into training and validation set\nWe split `train_df` into two sets. `X_train` is used for the cross-validation, `X_val` is used at the end of the notebook to estimate the generalization error.","5ed973df":"## Preprocessing","496ca2c9":"# Pets: Definitive CatBoost parameter tuning\nWe'll do parameter tuning for [CatBoost](https:\/\/catboost.ai\/) algorithm using [Pandas](https:\/\/pandas.pydata.org\/) and [scikit-learn pipelines](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.pipeline.Pipeline.html). I'm a huge fan of pipelines as they ensure that (1) one **never** needs to modify the training data in-place and (2) all preprocessing steps can be parametrized and therefore tuned with, e.g., [RandomizedSearchCV](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.model_selection.RandomizedSearchCV.html) (see the [example](https:\/\/scikit-learn.org\/stable\/auto_examples\/compose\/plot_compare_reduction.html)). \n\nWe'll be using randomized search instead of [GridSearchCV](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.model_selection.GridSearchCV.html) as randomly chosen trials are more efficient for hyper-parameter optimization than trials on a grid ([Random Search for Hyper-Parameter Optimization](http:\/\/www.jmlr.org\/papers\/volume13\/bergstra12a\/bergstra12a.pdf) by J. Bergstra and Y. Bengio, 2012).\n\nComments on this notebook are most welcome!\n\n### TODO\n- [ ] More tuning","f659e960":"## Data description (copied from [competition description](https:\/\/www.kaggle.com\/c\/petfinder-adoption-prediction\/data))","5feab901":"Create a voting classifier out of the best estimators:","b0ff5d15":"## CatBoost classifier\nWe define a small helper class called `CatBoostPandasClassifier` for training `CatBoostClassifier` with Pandas dataframes. The helper class correctly passes the categorical columns as `cat_features` to CatBoostClassifier's `fit` method and ensures that the order of the columns in fitting and prediction phases matches.","709a7be0":"Evaluate predictions on the test set:","6e921632":"Clearly there's a big discrepancy in `mean_test_cohen_kappa_quadratic` between cross-validation and the hold-out set. Any advice on how to bridge the gap (reduce the overfitting to training set) is welcome :)","44a37b59":"Let us first define a few helper functions to help in parameter tuning:","0f2711cc":"### Tune CatBoostPandasClassifier","de2f9c14":"Read training data CSV to pandas dataframe, marking categorical columns as category type.","e86d4f9e":"<i>\nIn this competition you will predict the speed at which a pet is adopted, based on the pet\u2019s listing on PetFinder. Sometimes a profile represents a group of pets. In this case, the speed of adoption is determined by the speed at which all of the pets are adopted. The data included text, tabular, and image data. See below for details. \nThis is a Kernels-only competition. At the end of the competition, test data will be replaced in their entirety with new data of approximately the same size, and your kernels will be rerun on the new data.\n\n### File descriptions\n- train.csv - Tabular\/text data for the training set\n- test.csv - Tabular\/text data for the test set\n- sample_submission.csv - A sample submission file in the correct format\n- breed_labels.csv - Contains Type, and BreedName for each BreedID. Type 1 is dog, 2 is cat.\n- color_labels.csv - Contains ColorName for each ColorID\n- state_labels.csv - Contains StateName for each StateID\n\n### Data Fields\n- PetID - Unique hash ID of pet profile\n- AdoptionSpeed - Categorical speed of adoption. Lower is faster. This is the value to predict. See below section for more info.\n- Type - Type of animal (1 = Dog, 2 = Cat)\n- Name - Name of pet (Empty if not named)\n- Age - Age of pet when listed, in months\n- Breed1 - Primary breed of pet (Refer to BreedLabels dictionary)\n- Breed2 - Secondary breed of pet, if pet is of mixed breed (Refer to BreedLabels dictionary)\n- Gender - Gender of pet (1 = Male, 2 = Female, 3 = Mixed, if profile represents group of pets)\n- Color1 - Color 1 of pet (Refer to ColorLabels dictionary)\n- Color2 - Color 2 of pet (Refer to ColorLabels dictionary)\n- Color3 - Color 3 of pet (Refer to ColorLabels dictionary)\n- MaturitySize - Size at maturity (1 = Small, 2 = Medium, 3 = Large, 4 = Extra Large, 0 = Not Specified)\n- FurLength - Fur length (1 = Short, 2 = Medium, 3 = Long, 0 = Not Specified)\n- Vaccinated - Pet has been vaccinated (1 = Yes, 2 = No, 3 = Not Sure)\n- Dewormed - Pet has been dewormed (1 = Yes, 2 = No, 3 = Not Sure)\n- Sterilized - Pet has been spayed \/ neutered (1 = Yes, 2 = No, 3 = Not Sure)\n- Health - Health Condition (1 = Healthy, 2 = Minor Injury, 3 = Serious Injury, 0 = Not Specified)\n- Quantity - Number of pets represented in profile\n- Fee - Adoption fee (0 = Free)\n- State - State location in Malaysia (Refer to StateLabels dictionary)\n- RescuerID - Unique hash ID of rescuer\n- VideoAmt - Total uploaded videos for this pet\n- PhotoAmt - Total uploaded photos for this pet\n- Description - Profile write-up for this pet. The primary language used is English, with some in Malay or Chinese.\n- AdoptionSpeed Contestants are required to predict this value. The value is determined by how quickly, if at all, a pet is adopted. The values are determined in the following way: \n    0 - Pet was adopted on the same day as it was listed. \n    1 - Pet was adopted between 1 and 7 days (1st week) after being listed. \n    2 - Pet was adopted between 8 and 30 days (1st month) after being listed. \n    3 - Pet was adopted between 31 and 90 days (2nd & 3rd month) after being listed. \n    4 - No adoption after 100 days of being listed. (There are no pets in this dataset that waited between 90 and 100 days).\n\n### Images\n\nFor pets that have photos, they will be named in the format of PetID-ImageNumber.jpg. Image 1 is the profile (default) photo set for the pet. For privacy purposes, faces, phone numbers and emails have been masked.\n\n### Image Metadata\nWe have run the images through Google's Vision API, providing analysis on Face Annotation, Label Annotation, Text Annotation and Image Properties. You may optionally utilize this supplementary information for your image analysis.\n\nFile name format is PetID-ImageNumber.json.\n\nSome properties will not exist in JSON file if not present, i.e. Face Annotation. Text Annotation has been simplified to just 1 entry of the entire text description (instead of the detailed JSON result broken down by individual characters and words). Phone numbers and emails are already anonymized in Text Annotation.\n\nGoogle Vision API reference: https:\/\/cloud.google.com\/vision\/docs\/reference\/rest\/v1\/images\/annotate\n\n### Sentiment Data\nWe have run each pet profile's description through Google's Natural Language API, providing analysis on sentiment and key entities. You may optionally utilize this supplementary information for your pet description analysis. There are some descriptions that the API could not analyze. As such, there are fewer sentiment files than there are rows in the dataset.\n\nFile name format is PetID.json.\n\nGoogle Natural Language API reference: https:\/\/cloud.google.com\/natural-language\/docs\/basics\n\nWhat will change in the 2nd stage of the competition?\nIn the second stage of the competition, we will re-run your selected Kernels. The following files will be swapped with new data:\n\ntest.zip including test.csv and sample_submission.csv\ntest_images.zip\ntest_metadata.zip\ntest_sentiment.zip\n\nIn stage 2, all data will be replaced with approximately the same amount of different data. The stage 1 test data will not be available when kernels are rerun in stage 2.\n<\/i>","61f56633":"Create a list of the estimators along their scores:","04c84e0a":"### Train the final estimator with all data available\nFinally we'll the train the voting classifier with all the data available and write our `submission.csv`. Of course, much better performance could be achieved by ensembling and\/or blending more versatile model families."}}