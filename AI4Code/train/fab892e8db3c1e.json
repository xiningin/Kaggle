{"cell_type":{"3961566b":"code","2243bc11":"code","c680e409":"code","893638ee":"code","ec4902c2":"code","8a820ba3":"code","1870b053":"code","794bf438":"code","6e5865fc":"code","199f3837":"code","d3052d53":"code","28fc80d0":"code","6600ccbc":"code","8e3faf1f":"code","a42501bd":"code","f689448e":"markdown","f962bc06":"markdown","f997459a":"markdown","138712ab":"markdown","1e52458b":"markdown","2967a196":"markdown","0f47ad17":"markdown","0a41f647":"markdown","1473cd20":"markdown","136b570d":"markdown"},"source":{"3961566b":"import re\nimport nltk\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom wordcloud import WordCloud\n\nnltk.download(\"stopwords\")","2243bc11":"# for other theme, please run: mpl.pyplot.style.available\nPLOT_PALETTE = 'tableau-colorblind10'\n# for other color map, please run: mpl.pyplot.colormaps()\nWORDCLOUD_COLOR_MAP = 'tab10_r'","c680e409":"# set palette color\nplt.style.use(PLOT_PALETTE)\n%matplotlib inline","893638ee":"df = pd.read_csv('..\/input\/resume-dataset\/Resume\/Resume.csv')\ndf.head()","ec4902c2":"# drop unused columns\ndf.pop('ID')\ndf.pop('Resume_html')\ndf","8a820ba3":"# for other stemmer, please refer to: https:\/\/www.nltk.org\/api\/nltk.stem.html\nSTEMMER = nltk.stem.porter.PorterStemmer()","1870b053":"def preprocess(txt):\n    # convert all characters in the string to lower case\n    txt = txt.lower()\n    # remove non-english characters, punctuation and numbers\n    txt = re.sub('[^a-zA-Z]', ' ', txt)\n    # tokenize word\n    txt = nltk.tokenize.word_tokenize(txt)\n    # remove stop words\n    txt = [w for w in txt if not w in nltk.corpus.stopwords.words('english')]\n    # stemming\n    txt = [STEMMER.stem(w) for w in txt]\n\n    return ' '.join(txt)","794bf438":"# preprocessing text\ndf['Resume'] = df['Resume_str'].apply(lambda w: preprocess(w))\n# drop original text column\ndf.pop('Resume_str')\ndf","6e5865fc":"# create list of all categories\ncategories = np.sort(df['Category'].unique())\ncategories","199f3837":"# create new df for corpus and category\ndf_categories = [df[df['Category'] == category].loc[:, ['Resume', 'Category']] for category in categories]\ndf_categories[10]","d3052d53":"df['Category'].value_counts().sort_index().plot(kind='bar', figsize=(12, 6))\nplt.show()","28fc80d0":"def wordcloud(df):\n    txt = ' '.join(txt for txt in df['Resume'])\n    wordcloud = WordCloud(\n        height=2000,\n        width=4000,\n        colormap=WORDCLOUD_COLOR_MAP\n    ).generate(txt)\n\n    return wordcloud","6600ccbc":"plt.figure(figsize=(32, 28))\n\nfor i, category in enumerate(categories):\n    wc = wordcloud(df_categories[i])\n\n    plt.subplot(6, 4, i + 1).set_title(category)\n    plt.imshow(wc)\n    plt.axis('off')\n    plt.plot()\n\nplt.show()\nplt.close()","8e3faf1f":"def wordfreq(df):\n    count = df['Resume'].str.split(expand=True).stack().value_counts().reset_index()\n    count.columns = ['Word', 'Frequency']\n\n    return count.head(10)","a42501bd":"fig = plt.figure(figsize=(32, 64))\n\nfor i, category in enumerate(categories):\n    wf = wordfreq(df_categories[i])\n\n    fig.add_subplot(12, 2, i + 1).set_title(category)\n    plt.bar(wf['Word'], wf['Frequency'])\n    plt.ylim(0, 3500)\n\nplt.show()\nplt.close()","f689448e":"<a id=\"wordcloud\"><\/a>\n## Word Cloud\n\nAfter word clouds are created, the word \"manag\" (inflected for \"manage\") prominently visible on many categories. Moreover, words like \"citi\", \"state\" and \"compani\" are noticeable on different categories as well. These common words are likely to contain low weight for computation. On the other hand, words like \"account\", \"develop\" and \"design\" probably has higher weight for calculation because it only appears on specific domains.","f962bc06":"<a id=\"wordfreq\"><\/a>\n## Word Frequency Table\n\nWord frequency is plotted to visualize how often popular words are used. Most categories have similar distribution on top 10 frequently used words. However, ACCOUNTANT is highly skewed, where the word \"account\" is used for 3,000 times from 120 samples, appears on one resume for 25 times on an average, which looks fine as it related to the category. Yet, there is another job category that usually contains the word \"account\" like FINANCE. Due to imbalance of the word used, it is verly likely that an ill-trained algorithm will be bias towards ACCOUNTANT.","f997459a":"<a id=\"imports\"><\/a>\n# Imports\n\nfollowing libraries are used for:\n1. re - remove unwanted charater from string using regulare expression\n2. nltk - remove stop words and conjunctions\n3. numpy - transform data into respective shape\n4. pandas - import data from file into dataframe\n5. matplotlib - create visualization\n6. wordcloud - create word cloud","138712ab":"<a id=\"read-csv\"><\/a>\n# Read CSV\n\nID and Resume_html columns does not contain useful information and are not aligend with my interest. Therefore, both columns are removed.","1e52458b":"<a id=\"barchart\"><\/a>\n## Bar Chart\n\nThe bar chart shows the number of records for each category, where class imbalanced is spotted easily. BPO, AUTOMOBILE and AGRICULTURE can suffer from limited number of  samples, especially for BPO which has only 22 samples, while the majority of classes have 100 samples approximately.","2967a196":"<a id=\"key-takeaway\"><\/a>\n# Key Takeaways\n\nThere are few takeaways to consider when building a model, such as:\n1. data is imbalanced - sampling techniques can be helpful\n2. dominant words are shared amongst few categories - parameters and regularization should be carefully tuned","0f47ad17":"# Table of Contents\n\n- [Imports](#imports)\n- [Theme Configuration](#theme-configuration)\n- [Read CSV](#read-csv)\n- [Preprocessing](#preprocessing)\n- [Exploratory Data Analysis](#exploratory-data-analysis)\n    - [Bar Chart](#barchart)\n    - [Word Cloud](#wordcloud)\n    - [Word Frequency Table](#wordfreq)\n- [Key Takeaways](#key-takeaway)","0a41f647":"<a id=\"exploratory-data-analysis\"><\/a>\n# Exploratory Data Analysis\n\nEDA is excercised to inspect class imbalance, word similarity and word frequency.","1473cd20":"<a id=\"preprocessing\"><\/a>\n# Preprocessing\n\nIn this stage, I followed basic cleaning processes for text analysis which includes:\n1. converting characters to lowercases.\n2. remove punctuations, numbers and foreign languages.\n3. tokenize word. (spliting sentence into unigram)\n4. stemming word. (convert to inflected form of word which is usually a root form)","136b570d":"<a id=\"theme-configuration\"><\/a>\n# Theme Configuration\n\nchoosing color template for graph and word cloud"}}