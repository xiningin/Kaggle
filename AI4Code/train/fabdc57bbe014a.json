{"cell_type":{"40ae0d3d":"code","98cec51c":"code","55b173bf":"code","ad0f09e4":"code","74ca58b0":"code","fa108d44":"code","6c11c5bb":"code","264f645c":"code","eb95a40f":"code","acf0d907":"code","38ccf36a":"code","47e4308b":"code","99a744c2":"code","d898148e":"code","cb0e264a":"code","2931e08f":"code","96b051f9":"code","7e297776":"code","d638bfb5":"code","80e57e54":"code","222fb972":"code","5e865282":"code","44404a69":"code","f33ce3d7":"code","aef63fa0":"code","b67bc24d":"code","760a2c58":"code","0d4aa731":"code","cdd80136":"code","0aa718b0":"code","8bb7a22b":"code","d60e1813":"code","20e69bb4":"code","ad400cd7":"code","d3ea75f2":"code","2c1f82a7":"code","4dbffe47":"code","24b046d3":"code","38d3c5d0":"code","2005263a":"code","5ee363c5":"code","64ada8d3":"code","d8ef6814":"code","ada05c5f":"code","93418741":"code","864c5ce5":"code","1ad365fe":"code","9c844222":"code","d42aa94b":"code","09fc0480":"code","4306b26d":"code","bad7f47e":"code","9ca89a66":"code","edd3ac32":"code","2fb24b81":"code","5f722d90":"code","d61d577c":"code","d05c3f61":"code","624545aa":"code","020c9b6e":"code","8d6d28b6":"code","75cd9e79":"code","d0516ed3":"code","c20d1b60":"code","ecd87808":"code","c272cf9c":"code","7691aad6":"code","55dde214":"code","b815e741":"code","04b629f3":"code","1ad2b2ec":"code","d9851298":"code","9b8942dc":"code","588b87a5":"code","e7e7ef0e":"code","314c943f":"code","584696b4":"code","1c910b94":"code","e67ea22e":"code","c8232432":"code","7f95ac1e":"code","1b97f8d0":"code","9c562c90":"code","3e7d04b5":"code","1df57c7b":"code","2dfcbdd6":"code","541bb1b7":"code","ced3f7a3":"code","4ae9f417":"code","d55cb17f":"code","9a1fefaf":"markdown","05ecae65":"markdown","007d898b":"markdown","a1800f3c":"markdown","ae164634":"markdown","3137389e":"markdown","3f5c4dfa":"markdown","26620710":"markdown","b1eb2693":"markdown","a1e8181b":"markdown","3b61dff5":"markdown","4bc5e647":"markdown","e09453df":"markdown","795d5302":"markdown","e88e56d4":"markdown","92b3c5c1":"markdown","d3e4b9f4":"markdown","7b691aa6":"markdown","8921fcb0":"markdown","5b99110a":"markdown","bab6273f":"markdown","45fe9a32":"markdown","05e9876e":"markdown","329844bf":"markdown","d558bcf9":"markdown","9414be68":"markdown","57887b72":"markdown","a4a26978":"markdown","c7616eae":"markdown","9f518e80":"markdown","44ba3506":"markdown","82475c7e":"markdown","d324bb7f":"markdown","24c78d45":"markdown","73b6019c":"markdown","16f227de":"markdown","795f1df1":"markdown","2c5a09b4":"markdown","1fde7162":"markdown","a9a296bd":"markdown","3d3feef8":"markdown","af47d286":"markdown","42b64a85":"markdown","e625f85f":"markdown","34689094":"markdown","331bda23":"markdown"},"source":{"40ae0d3d":"import numpy as np\nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport matplotlib.patches as mpatches\nimport os\nimport json\nfrom tqdm.auto import tqdm\nimport sys\nimport random\n\nimport re\nfrom sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\nfrom nltk.corpus import stopwords\nfrom nltk import word_tokenize\nfrom nltk.stem import WordNetLemmatizer\nfrom string import punctuation, digits\nfrom collections import Counter\nfrom wordcloud import WordCloud\nfrom pprint import pprint \nfrom sklearn.feature_extraction.text import TfidfVectorizer\nimport gensim\nimport gensim.corpora as corpora\nfrom gensim.utils import simple_preprocess\nfrom gensim.models import CoherenceModel\n\nfrom PIL import Image\nimport requests\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics.pairwise import linear_kernel, cosine_similarity","98cec51c":"reviews = pd.read_parquet('..\/input\/google-play-games-reviews\/reviews_minimum.parquet.gzip')\nreviews['at'] = pd.to_datetime(reviews['at'])\nreviews.head()","55b173bf":"reviews.dtypes","ad0f09e4":"len(reviews)","74ca58b0":"# This is for when using the dataset with review content\n# create iterator for going through users to see if they are fake reviewers\n\n# number of reviews per user\n# counts = reviews['userID'].value_counts(sort=True)\n# counts = counts[counts >= 5]\n# counts\n# iterID = iter(counts.index)","fa108d44":"# keep running this cell to go through each user\n# reviews[reviews['userID'] == next(iterID)]     ","6c11c5bb":"# # remove fake users\n# fake_users = [3969953, 4861687, 4804703]\n# reviews = reviews[~reviews['userID'].isin(fake_users)]\n\n# # remove users with less than 10 reviews\n# tmp_reviews = reviews[reviews['userID'].isin(counts.index)]\n\n# counts = reviews['userID'].value_counts(sort=True)\n# counts = counts[counts >= 5]\n# counts","264f645c":"# get sd and count of each game\nsd_count = reviews.groupby('appId')['score'].agg(['std','count'])\n# find games with very low sd ratings and many num ratings\nattacked_games = sd_count[((sd_count['std'] < 0.1) &\n                           (sd_count['count'] > 10)) | \n                          ((sd_count['std'] < 0.25) & \n                           (sd_count['count'] > 50))] \nattacked_games.head(10)","eb95a40f":"# remove those attacked games\nreviews = reviews[~reviews['appId'].isin(attacked_games.index)]\nreviews.groupby('appId')['score'].agg(['mean','std','count']).sort_values('mean',ascending=False)[:30]","acf0d907":"# sd_count = reviews.groupby('userID')['score'].agg(['std','count'])\n# sd_count = sd_count[(sd_count['std'] < 0.25) & (sd_count['count'] > 50)]\n# sd_count\n# iterID = iter(sd_count.index)","38ccf36a":"# reset userID\nuserID = reviews[\"userID\"].astype('category').cat.codes\nreviews[\"userID\"] = userID\n# reviews[\"userID\"] = reviews[\"userID\"].astype('category',copy=False)\n# reviews[\"userID\"] = reviews[\"userID\"].cat.codes\n# reviews.sort_values('userID', inplace=True)\n# reviews.reset_index(drop=True, inplace=True)\nreviews","47e4308b":"# number of reviews per user\ncounts = reviews['userID'].value_counts(sort=True)\ncounts = counts[counts >= 5]\ncounts\n\ncounts.index = counts.index.map(str)\np = counts.plot.line(color='black')\np.axes.get_xaxis().set_ticks([])\np.set_xlabel('User ID')\np.set_ylabel('Number of Reviews')\nplt.fill_between(counts.index, counts, where=counts >= 15, alpha=0.4)\nplt.fill_between(counts.index, counts, where=counts < 15, alpha=0.4)\nplt.axvline(x=sum(counts >= 15), linestyle='--', color='red')\nplt.legend(handles=[mpatches.Patch(color='#1f77b4', alpha=0.4, label='>= 15 reviews'),\n                    mpatches.Patch(color='#ff7f0e', alpha=0.4, label='< 15 reviews')],\n           loc=\"upper right\")\nplt.title('Distribution of Reviews Per User', size=20)\nplt.show()","99a744c2":"# distribution of mean user rating\ntmp = reviews.groupby(['userID'], as_index=False).agg({'score':'mean'})\nplt.hist(tmp['score'],bins=4, edgecolor='black')\nplt.title('Distribution of User Ratings', size=20)\nplt.show()","d898148e":"gamereview_counts = reviews['appId'].value_counts()\ngamereview_counts[gamereview_counts < 250]","cb0e264a":"gamereview_counts[gamereview_counts > 250]","2931e08f":"import matplotlib.patches as mpatches\np = gamereview_counts.plot.line(color='black')\np.axes.get_xaxis().set_ticks([])\np.set_xlabel('Game ID')\np.set_ylabel('Number of Reviews')\nplt.fill_between(gamereview_counts.index, gamereview_counts, where=gamereview_counts >= 1990, alpha=0.4)\nplt.fill_between(gamereview_counts.index, gamereview_counts, \n                 where=(gamereview_counts < 1990) & (gamereview_counts > 250), alpha=0.4)\nplt.fill_between(gamereview_counts.index, gamereview_counts, where=gamereview_counts <= 250,alpha=0.4)\nplt.axvline(x=sum(gamereview_counts >= 1990), linestyle='--', color='red')\nplt.axvline(x=sum(gamereview_counts >= 250), linestyle='--', color='red')\nplt.legend(handles=[mpatches.Patch(color='#1f77b4', alpha=0.4, label='>= 1990 reviews'),\n                    #mpatches.Patch(color='#ff7f0e', alpha=0.4, label='< 1990 reviews'),\n                    mpatches.Patch(color='#2ca02c', alpha=0.4, label='<= 250 reviews')],\n           loc=\"upper right\")\nplt.title('Distribution of Reviews Per Game', size=20)\nplt.show()","96b051f9":"# mean ratings for each game\ntmp = reviews.groupby(['appId'], as_index=False).agg({'score':'mean'}).sort_values('score', ascending=False)\ntmp","7e297776":"# distribution of mean game rating\nplt.hist(tmp['score'], edgecolor='black')\nplt.title('Distribution of Mean Game Rating', size=20)\nplt.show()","d638bfb5":"# filter out users with less that 5 reviews\nreviews = reviews[reviews['userID'].isin(counts.index)]\n# reset userID\nreviews.sort_values('userID', inplace=True)\nreviews = reviews.reset_index(drop=True)\nreviews[\"userID\"] = reviews[\"userID\"].astype('category')\nreviews[\"userID\"] = reviews[\"userID\"].cat.codes\n# reviews.to_csv('game_reviews_cleaned.csv', index=False)\nreviews","80e57e54":"print('Number of unique users with >= 5 ratings: {num}'.format(num=len(reviews['userID'].unique())))\nprint('')\nprint('Number of games: {num}'.format(num=len(reviews['appId'].unique())))","222fb972":"# remove duplicate games\ngameinfo = pd.read_csv('..\/input\/google-games-info\/google_games_info_detailed_wDescription.csv')\ngameinfo = gameinfo.drop_duplicates(['appId'], keep='first')\ngameinfo.head()","5e865282":"gameinfo.columns","44404a69":"# display sample game icons\nfig, ax = plt.subplots(nrows=1, ncols=5, figsize=(25,4))\nfor i in range(5):\n    r = requests.get(gameinfo['icon'][i], stream=True)\n    img = Image.open(r.raw)\n    ax[i].imshow(img)\n    ax[i].set_title(gameinfo['title'][i], fontsize=20)\n    ax[i].axis('off')\nplt.show()","f33ce3d7":"gameinfo['minInstalls'].value_counts().plot.barh()\nplt.title('Distribution of Game Minimum Installs', size=20)\nplt.show()","aef63fa0":"gameinfo['genreId'].value_counts().plot.barh()\nplt.title('Distribution of Game Genres', size=20)\nplt.show()","b67bc24d":"print(gameinfo['contentRatingDescription'].unique().tolist()[:10])\nprint('\\nNumber of unique content rating description: {length}'.format(length=len(gameinfo['contentRatingDescription'].unique().tolist())))","760a2c58":"gameinfo['price'].plot.hist()\nplt.title('Distribution of Game Prices', size=20)\nplt.show()\nprint('Max game price: ${price}'.format(price=gameinfo['price'].max()))","0d4aa731":"pd.to_datetime(gameinfo['released']).dt.year.plot.hist(edgecolor='black',bins=11)\nplt.title('Distribution of Game Release Year', size=20)\nplt.show()","cdd80136":"# game_features will be used for creating features\n# gameinfo will be used for referring to other game details like URL, icons etc\ngame_features = gameinfo.copy()\ngame_features = game_features[['appId', 'description', 'summary', 'minInstalls', 'price',\n                               'genreId', 'contentRatingDescription', 'containsAds', 'released', \n                               'editorsChoice']]\n# keep only games that are in user reviews\n#tmp = pd.read_csv('..\/input\/google-game-reviews-7675\/game_reviews_cleaned.csv')\n#game_features = game_features.merge(pd.DataFrame(tmp['appId'].unique(), columns=['appId']), how='inner').reset_index(drop=True) \ngame_features = game_features[~game_features['summary'].isnull()] # remove games with no summaries\ngame_features.head()","0aa718b0":"# get year from released date\ngame_features['released'] = pd.to_datetime(game_features['released'], format='%b %d, %Y').dt.year.astype('Int16')\n\n# remove games with little minInstalls and released before 2019 \ngames_to_remove = game_features[(game_features['minInstalls'] <= 100) & (game_features['released'] < 2019)].appId.values.tolist()\ngames_to_remove.append('com.booboo.pagain') # has very high price\ngame_features = game_features[~game_features['appId'].isin(games_to_remove)]\n\n# convert to boolean cols to int\n# game_features[['containsAds', 'editorsChoice']] = game_features[['containsAds', 'editorsChoice']].astype('int8')\n# convert containsAds to boolean and reduce weight\ngame_features['containsAds'] = game_features['containsAds']*0.015\n\n# convert editorsChoice to boolean and reduce weight\ngame_features['editorsChoice'] = game_features['editorsChoice']*0.015\n# add editorsChoice to summary for computing word features\n#game_features['summary'] = game_features.apply(lambda x: x['summary']+' ad' if x['containsAds'] else x['summary'], axis=1)\n# game_features['summary'] = game_features.apply(lambda x: x['summary']+' editor' if x['editorsChoice'] else x['summary'], axis=1)\n# game_features.drop(['editorsChoice'], axis=1, inplace=True) # remove","8bb7a22b":"# function for normalizing column to 01 range and returning min_max as well\ndef normalize(df_col):\n    col_min = df_col.min()\n    col_max = df_col.max()\n    col_norm = (df_col-col_min) \/ (col_max-col_min)\n    return col_norm, col_min, col_max\n\n# normalize price \ntmp, price_min, price_max = normalize(game_features['price'])\ngame_features['price'] = tmp*0.1 # reduce weight\n\n# normalize minInstalls \ntmp, install_min, install_max = normalize(game_features['minInstalls'])\ngame_features['minInstalls'] = tmp*0.1 # reduce weight\n\n# normalize released year \ntmp, released_min, released_max = normalize(game_features['released'])\ngame_features['released'] = tmp*0.1 # reduce weight\n\ngame_features.head()","d60e1813":"# unique content ratings\nimport itertools\ncontent_unique = list(set(list(itertools.chain.from_iterable([content.split(', ') if isinstance(content, str) else [content] for content in game_features['contentRatingDescription'].unique().tolist()]))))\ncontent_unique = content_unique[1:] # remove nan\n#content_unique.remove('Violence')\n\n# split content rating column into list so that for eg., 'sex' is not in ['sexual innuendo', 'fear'] but 'sexual innuendo' is\ngame_features['contentRatingDescription'] = game_features['contentRatingDescription'].str.split(',')\n\n# get content ratings as features\nfeature_list = []\nfor content in content_unique:\n    feature_name = content.lower().replace(' ', '_') # lowercase and join with underscore\n    feature_list.append(game_features['contentRatingDescription'].apply(lambda x: content in x if isinstance(x,list) else x).astype('boolean').astype('Int8').rename(feature_name))\n\nfeature_list = pd.concat(feature_list, axis=1).fillna(0) # concat all features and replace NAs with 0s\ngame_features = pd.concat([game_features, feature_list], axis=1) # concat to game_features\ngame_features.drop(['contentRatingDescription'], axis=1, inplace=True)\ngame_features.head()","20e69bb4":"# dummy encode genre as features\ngame_features = pd.get_dummies(game_features, columns=['genreId'], prefix='', prefix_sep='')\n\n# apply smaller weight to contentRating and genre features\ncols = ['strong_violence',\n       'extreme_violence', 'nudity', 'parental_guidance_recommended', 'horror',\n       'moderate_violence', 'gambling', 'sexual_innuendo', 'implied_violence',\n       'simulated_gambling', 'strong_language', 'mild_violence', 'drugs',\n       'mild_swearing', 'real_gambling\/paid_contests',\n       'use_of_alcohol\/tobacco', 'sex', 'violence', 'fear']\ngame_features[cols] = game_features[cols]*0.15\n\ncols = ['GAME_ACTION',\n       'GAME_ADVENTURE', 'GAME_ARCADE', 'GAME_BOARD', 'GAME_CARD',\n       'GAME_CASINO', 'GAME_CASUAL', 'GAME_EDUCATIONAL', 'GAME_MUSIC',\n       'GAME_PUZZLE', 'GAME_RACING', 'GAME_ROLE_PLAYING', 'GAME_SIMULATION',\n       'GAME_SPORTS', 'GAME_STRATEGY', 'GAME_TRIVIA', 'GAME_WORD']\n\ngame_features[cols] = game_features[cols]*0.2\n\ngame_features.head()","ad400cd7":"## Preprocess description text to remove all forms of urls and links, emails, escape sequences (\/n\/t\/r),\n## punctuations, stopwords, gamewords, and lemmatize words\n\n# add summary to game description\ngame_features['description'] = game_features['description'] + ' ' + game_features['summary']\ndescription = game_features['description'].values.tolist()\n\nlemmatizer = WordNetLemmatizer()\nstop = set(stopwords.words('english'))\nstop = ENGLISH_STOP_WORDS.union(stop) # combine stop words from different sources\nstop = stop.union(['game','play','free','best','fun','new','enjoy','1',\n                   'real','epic','challenge','app','make','mobile','win', \n                   'android','100','2020','2021','1000','version', \n                   'youtube','facebook','instagram','twitter']) # add common game words\n\n# function for removing url from text\ndef remove_url(text):\n    text = re.sub(r'''(?i)\\b((?:https?:\/\/|www\\d{0,3}[.]|[a-z0-9.\\-]+[.][a-z]{2,4}\/)(?:[^\\s()<>]+|\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\))+(?:\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\)|[^\\s`!()\\[\\]{};:'\".,<>?\u00ab\u00bb\u201c\u201d\u2018\u2019]))''', \n                  '', text)\n    return \" \".join(re.sub(\"([^0-9A-Za-z \\t])|(\\w+:\\\/\\\/\\S+)\", \"\", text).split())\n\ndef remove_email(text):\n    return re.sub('\\S*@\\S*\\s?', '', text)\n\ndef remove_escape_sequences(text):\n    return text.translate(str.maketrans(\"\\n\\t\\r\", \"   \"))\n    #return text.translate(str.maketrans(' ',' ',\"\\n\\t\\r\"))\n\n# Function for expanding most common contractions https:\/\/stackoverflow.com\/questions\/19790188\/expanding-english-language-contractions-in-python\ndef decontraction(phrase):\n    # specific\n    phrase = re.sub(r\"won\\'t\", \"will not\", phrase)\n    phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n    # general\n    phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n    phrase = re.sub(r\"\\'re\", \" are\", phrase)\n    phrase = re.sub(r\"\\'s\", \" is\", phrase)\n    phrase = re.sub(r\"\\'d\", \" would\", phrase)\n    phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n    phrase = re.sub(r\"\\'t\", \" not\", phrase)\n    phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n    phrase = re.sub(r\"\\'m\", \" am\", phrase)\n    return phrase\n    \n# remove punctuations\ndef remove_punct(text):\n    table=str.maketrans('','',punctuation)\n    return text.translate(table)\n\n# remove stopwords\ndef remove_stopwords(text):\n    return [word for word in word_tokenize(text) if word.lower() not in stop]\n\ndef lemmatization(text):\n    return [lemmatizer.lemmatize(word.lower(), pos='n') for word in text]\n\ndef remove_gamewords(text):\n    return [word for word in text if word not in stop]\n\n# remove only digits that are not joined to char (eg. 3d is not removed)\ndef remove_digits(text):\n    return [word for word in text if not word.isdigit()]\n\n# - remove url and punct,stopwords and lemmatize \ndescription_words = [remove_digits(remove_gamewords(lemmatization(remove_stopwords(remove_punct(decontraction(remove_url(remove_email(remove_escape_sequences(d).strip())))))))) for d in description]\n#description_words[0]\n\n# join words together for tf-idf instead of using individual word tokens\ndescription_text = [' '.join(d) for d in description_words]","d3ea75f2":"## Get most common words in game description\n\ncorpus = []\n[corpus.append(word) for d in description_words for word in d]\ntmp = Counter(corpus).most_common()\ncommonwords = pd.DataFrame(tmp, columns=['Word','Count'])\n\ncommonwords = commonwords[~ commonwords['Word'].isin(['game','play','free','best','fun','new','win',\n                                                         'enjoy','1','real','epic','challenge','app',\n                                                         'make','mobile','feature','like','use','mode',\n                                                         'different','help','unique','way'])].reset_index(drop=True)\ncommonwords.head(25).T","2c1f82a7":"# word cloud\nfrom PIL import Image\nfrom wordcloud import ImageColorGenerator\n#mask = np.array(Image.open('..\/input\/google-games-info\/googleplay_white.jpeg'))\n#image_colors = ImageColorGenerator(mask)\n\n# my_cloud = WordCloud(background_color='white', stopwords=stop,\n#                      mask=mask, max_font_size=40).generate_from_frequencies(commonwords.set_index('Word').T.to_dict('records')[0]).recolor(color_func=image_colors)\n\nmy_cloud = WordCloud(background_color='white', stopwords=stop).generate_from_frequencies(commonwords.set_index('Word').T.to_dict('records')[0])\n\n# fig, ax = plt.subplots(figsize=(18, 10))\n# ax.imshow(my_cloud, interpolation='bilinear')\n# ax.title('Words in Game Descriptions', size=15)\n# ax.axis('off')\n# ax.show() \nplt.imshow(my_cloud, interpolation='bilinear')\nplt.title('Words in Game Descriptions', size=15)\nplt.axis('off')\nplt.show() ","4dbffe47":"# Ngrams\nfrom nltk.util import ngrams\n\ndef generate_ngrams(text, n):\n    words = word_tokenize(text)\n    return [' '.join(ngram) for ngram in list(get_data(ngrams(words, n))) if not all(w in stop for w in ngram)] # exclude if all are stopwords\n\n\n# in newer versions of python, raising StopIteration exception to end a generator, which is used in ngram, is deprecated\ndef get_data(gen):\n    try:\n        for elem in gen:\n            yield elem\n    except (RuntimeError, StopIteration):\n        return","24b046d3":"# Bigrams\nbigrams = [generate_ngrams(text, 2) for text in description_text]\nbigrams_dict = {}\nfor bgs in bigrams:\n    for bg in bgs:\n        if bg in bigrams_dict:\n            bigrams_dict[bg] += 1\n        else:\n            bigrams_dict[bg] = 1\n\n# word cloud\nmy_cloud = WordCloud(background_color='white', stopwords=stop).generate_from_frequencies(bigrams_dict)\nplt.imshow(my_cloud, interpolation='bilinear')\nplt.title('Bigrams in Game Descriptions', size=15)\nplt.axis('off')\nplt.show() ","38d3c5d0":"# Trigrams\ntrigrams = [generate_ngrams(text, 3) for text in description_text]\ntrigrams_dict = {}\nfor tgs in trigrams:\n    for tg in tgs:\n        if tg in trigrams_dict:\n            trigrams_dict[tg] += 1\n        else:\n            trigrams_dict[tg] = 1\n\n# word cloud\nmy_cloud = WordCloud(background_color='white', stopwords=stop).generate_from_frequencies(trigrams_dict)\nplt.imshow(my_cloud, interpolation='bilinear')\nplt.title('Trigrams in Game Descriptions', size=15)\nplt.axis('off')\nplt.show() ","2005263a":"# TF-IDF \ntvec = TfidfVectorizer(min_df=.03, max_df=.1, stop_words='english', ngram_range=(1,1))\ntvec_weights = tvec.fit_transform(description_text)\nweights = np.asarray(tvec_weights.mean(axis=0)).ravel().tolist()\nweights_df = pd.DataFrame({'term': tvec.get_feature_names(), 'weight': weights})\nselected_weights = weights_df.sort_values(by='weight', ascending=False) \nselected_weights = selected_weights[selected_weights['weight'] >= 0.012] # try out differnt number of features\ndisplay(selected_weights.T)\n\n# word cloud\nmy_cloud = WordCloud(background_color='white', stopwords=stop).generate_from_frequencies(selected_weights.set_index('term').T.to_dict('records')[0])\nplt.imshow(my_cloud, interpolation='bilinear')\nplt.title('Most Important Words in Game Descriptions', size=15)\nplt.axis('off')\nplt.show()","5ee363c5":"selected_matrix = tvec_weights.toarray()[:, selected_weights.index.tolist()] # select matrix for top words\nselected_matrix.shape","64ada8d3":"tfidf_df = pd.DataFrame(selected_matrix, columns=selected_weights.term.tolist())\ntfidf_df.head()","d8ef6814":"# apply fitted tf-idf on new synthetic example\npd.DataFrame(tvec.transform(['join a quest with your favourite war animal to fight opponent at home']).toarray()[:, selected_weights.index.tolist()], columns=selected_weights.term.tolist())","ada05c5f":"# add tf-idf features to game_features\ngame_features = game_features.reset_index(drop=True)\ngame_features_tfidf = pd.concat([game_features, tfidf_df], axis=1)\ngame_features_tfidf.drop(['description','summary'], axis=1, inplace=True)\ngame_features_tfidf.head()","93418741":"## Create the Dictionary and Corpus needed for Topic Modeling\n\n# Create Dictionary\nid2word = corpora.Dictionary(description_words)\n# Create Corpus\ntexts = description_words\n# Term Document Frequency\ncorpus = [id2word.doc2bow(text) for text in texts]\n# View\nprint(corpus[0])","864c5ce5":"id2word[0]","1ad365fe":"# Human readable format of corpus (term-frequency)\n[[(id2word[id], freq) for id, freq in cp] for cp in corpus[:1]]","9c844222":"# Compute coherence values for n range of topics\n\ndef compute_coherence_values(dictionary, corpus, texts, limit, start=2, step=3):\n    \"\"\"\n    Compute c_v coherence for various number of topics\n\n    Parameters:\n    ----------\n    dictionary : Gensim dictionary\n    corpus : Gensim corpus\n    texts : List of input texts\n    limit : Max num of topics\n\n    Returns:\n    -------\n    model_list : List of LDA topic models\n    coherence_values : Coherence values corresponding to the LDA model with respective number of topics\n    \"\"\" \n    coherence_values = []\n    model_list = []\n    for num_topics in range(start, limit, step):\n        model = gensim.models.ldamodel.LdaModel(corpus=corpus, num_topics=num_topics, id2word=id2word, random_state=100, \n                                                update_every=1, chunksize=100, passes=10, alpha='auto', per_word_topics=True)                                        \n        model_list.append(model)\n        coherencemodel = CoherenceModel(model=model, texts=texts, dictionary=dictionary, coherence='c_v')\n        coherence_values.append(coherencemodel.get_coherence())\n\n    return model_list, coherence_values\n\nmodel_list, coherence_values = compute_coherence_values(dictionary=id2word, corpus=corpus, texts=description_words, start=2, limit=40, step=6)","d42aa94b":"import matplotlib.pyplot as plt\nlimit=40; start=2; step=6;\nx = range(start, limit, step)\nplt.plot(x, coherence_values)\nplt.xlabel(\"Num Topics\")\nplt.ylabel(\"Coherence score\")\nplt.legend(([\"coherence_values\"]), loc='best')\nplt.show()","09fc0480":"# Build LDA model\nnum_topics = 15\nlda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n                                           id2word=id2word,\n                                           num_topics=num_topics, \n                                           random_state=100,\n                                           update_every=1,\n                                           chunksize=100,\n                                           passes=10,\n                                           alpha='auto',\n                                           per_word_topics=True)\n\n# Print the Keyword in the topics\npprint(lda_model.print_topics())\ndoc_lda = lda_model[corpus]","4306b26d":"# Compute Coherence Score\ncoherence_model_lda = CoherenceModel(model=lda_model, texts=description_words, dictionary=id2word, coherence='c_v')\ncoherence_lda = coherence_model_lda.get_coherence()\nprint('\\nCoherence Score: ', coherence_lda)","bad7f47e":"import pyLDAvis\nimport pyLDAvis.gensim\n\n# pyLDAvis.enable_notebook()\n# vis = pyLDAvis.gensim.prepare(lda_model, corpus, id2word)\n# vis","9ca89a66":"# get topic weights for each document\/game description\n\ntopic_features = np.zeros((len(corpus),num_topics)) # init array\nfor row, d in enumerate(corpus):\n    topic_weights = lda_model.get_document_topics(d, minimum_probability=None) # get topic weights for doc\n    for t in topic_weights:\n        topic_features[row, t[0]] = t[1] # input weight into array\n\n# change to dataframe\ntopic_features = pd.DataFrame(topic_features, columns=['topic'+str(i) for i in range(num_topics)])\ntopic_features.head()","edd3ac32":"# add LDA matrix to game_features\ngame_features = game_features.reset_index(drop=True)\ngame_features_lda = pd.concat([game_features, topic_features], axis=1)\ngame_features_lda.drop(['description','summary'], axis=1, inplace=True)\ngame_features_lda.head()","2fb24b81":"# see if any NaNs left\nprint(game_features_tfidf.isnull().sum(axis=0))","5f722d90":"# replace NaNs in price and released with 0\n# replace with 0 will not influence the similarity much as there are many columns\n# I think it is better than replacing with mean, which gives a false idea of the released date\n\n# CONSIDER REMOVING THOSE WITH NAN RELEASED YEAR AND ALSO LOW MININSTALLS!!!\n\ngame_features_tfidf = game_features_tfidf.fillna(0)\ngame_features_lda = game_features_lda.fillna(0)","d61d577c":"# computing the similarity matrix for all games, 1 for tf-idf, 1 for LDA\ncosine_similarities_tfidf = cosine_similarity(game_features_tfidf.iloc[:,1:], game_features_tfidf.iloc[:,1:]) \ncosine_similarities_lda = cosine_similarity(game_features_lda.iloc[:,1:], game_features_lda.iloc[:,1:]) \n\nresults_tfidf = {}\nfor idx, row in game_features_tfidf.iterrows():\n   similar_indices = cosine_similarities_tfidf[idx].argsort()[:-100:-1] # find 100 most similar game vectors\n   similar_items = [(cosine_similarities_tfidf[idx][i], game_features_tfidf['appId'][i]) for i in similar_indices] # get cosine similarity value and app name\n   results_tfidf[row['appId']] = similar_items[1:] # omit the first one as it will be completely similar\n\nresults_lda = {}\nfor idx, row in game_features_lda.iterrows():\n   similar_indices = cosine_similarities_lda[idx].argsort()[:-100:-1] # find 100 most similar game vectors\n   similar_items = [(cosine_similarities_lda[idx][i], game_features_lda['appId'][i]) for i in similar_indices] # get cosine similarity value and app name\n   results_lda[row['appId']] = similar_items[1:] # omit the first one as it will be completely similar","d05c3f61":"# Recommend 5 games similar to input game\n\ndef item(appId, game_features):  \n  return game_features.loc[game_features['appId'] == appId]['appId'].tolist()[0].split(' - ')[0] \n\ndef recommend(item_id, num, game_features, results, title, show=True, plot=True): \n    if show:\n        print(\"Recommending \" + str(num) + \" games similar to \" + item(item_id, game_features) + \"...\")  \n        print(\"-------\")    \n    recs = results[item_id][:num]   \n    for rec in recs: \n        if show:\n           print(\"Recommended: \" + item(rec[1], game_features) + \" (score:\" +      str(rec[0]) + \")\")\n        \n    if plot:\n        # display sample game icons\n        fig, ax = plt.subplots(nrows=1, ncols=5, figsize=(12,4), constrained_layout=True)\n        fig.suptitle('Top 5 Similar Games Using '+title, fontsize=40)\n\n        for i in range(5):\n            url,title = gameinfo[gameinfo['appId'] == recs[i][1]][['icon','title']].values[0]\n            if len(title) >26:\n                title = title[:23] + '...'\n            r = requests.get(url, stream=True)\n            img = Image.open(r.raw)\n            ax[i].imshow(img)\n            ax[i].set_title(title, fontsize=12, wrap=True)\n            ax[i].axis('off')\n\n        plt.show()\n        \n    return recs\n    \nrecs = recommend('com.king.candycrushsaga', 15, game_features_tfidf, results_tfidf, 'TF-IDF')\nrecs = recommend('com.king.candycrushsaga', 15, game_features_lda, results_lda, 'LDA', show=False)","624545aa":"recs = recommend('com.moonactive.coinmaster', 15, game_features_tfidf, results_tfidf, 'TF-IDF', show=False)\nrecs = recommend('com.moonactive.coinmaster', 15, game_features_lda, results_lda, 'LDA', show=False)","020c9b6e":"recs = recommend('com.igg.android.lordsmobile', 15, game_features_tfidf, results_tfidf, 'TF-IDF', show=False)\nrecs = recommend('com.igg.android.lordsmobile', 15, game_features_lda, results_lda, 'LDA', show=False)","8d6d28b6":"# clear memory\nimport gc\ndel()\ngc.collect()","75cd9e79":"# one more round of filtering\nreviews = reviews[reviews['appId'].isin(game_features['appId'])]\n# update num reviews per user after gamefeatures was cleaned and merged to reviews\ncounts = reviews['userID'].value_counts() \ncounts = counts[counts >= 5]\nreviews = reviews[reviews['userID'].isin(counts.index)].reset_index(drop=True)\nreviews['userID'] = reviews[\"userID\"].astype('category')\nreviews[\"userID\"] = reviews[\"userID\"].cat.codes\nreviews","d0516ed3":"# label encode appId\nfrom sklearn import preprocessing\nle = preprocessing.LabelEncoder()\nle.fit(game_features_tfidf['appId'])\nreviews['appId'] = le.transform(reviews['appId']) \n#le.inverse_transform(reviews['appId'])\ngame_features_tfidf['appId'] = le.transform(game_features_tfidf['appId'])\ngame_features_lda['appId'] = le.transform(game_features_lda['appId'])","c20d1b60":"reviews_content_all_train, reviews_content_test = train_test_split(reviews,\n                                                               stratify=reviews['userID'], \n                                                               test_size=0.20, # 1 test review for users with 5 reviews\n                                                               random_state=0)\nreviews_content_all_train.drop('at', axis=1, inplace=True) # no need timestamp for this method\nreviews_content_test.drop('at', axis=1, inplace=True)\n\nreviews_content_train, reviews_content_val = train_test_split(reviews_content_all_train,\n                                                             stratify=reviews_content_all_train['userID'],\n                                                             test_size=0.25, # 1 val review for users with 5 reviews\n                                                             random_state=0)\nprint('reviews_content_train shape: {shape}'.format(shape=reviews_content_train.shape))\nprint('reviews_content_val shape: {shape}'.format(shape=reviews_content_val.shape))\nprint('reviews_content_test shape: {shape}'.format(shape=reviews_content_test.shape))","ecd87808":"reviews_content_train.sort_values('userID').head(10)","c272cf9c":"# build user profile from games they rated\ndef build_user_profile(appIds, scores, game_features, method='nonweighted'):\n    methods = ['weighted', 'nonweighted']\n    if method not in methods:\n        raise ValueError(\"Invalid method. Expected one of: %s\" % methods)\n    # get user-reviewed game vector\/features\n    user_games = game_features[game_features['appId'].isin(appIds)].iloc[:,1:] \n    \n    if method == 'nonweighted':\n        return user_games.sum().to_frame().T  \n        \n    # weighted average by review scores    \n    \n    \n    return weighted_avg_reviews\n    \n# build and combine all user profiles       \ndef build_user_profiles(train_data):\n    userprofiles = []\n    for u in train_data['userID'].unique():\n        user_reviews = train_data[train_data['userID'] == u]\n        userprofile = build_user_profile(user_reviews['appId'].values, \n                                         user_reviews['score'].values)\n        userprofile = pd.concat([pd.Series(u,['userID'],dtype='Int8'), userprofile], axis=0) # add userID\n        userprofiles.append(userprofile)\n    # concat all user profiles\n    return pd.concat(userprofiles,axis=1).T\n    ","7691aad6":"# function for computing cosine similarity for large arrays\n# by breaking into chunks and taking only the top 100 recommendations\n# reduces the size of cos_sim matrix\ndef cos_sim_DAC(x,y,sim_thres=0.8,num_rec=100,part_size=100):\n    results=np.array([], dtype='int16')\n    slice_start = 0\n    slice_end = slice_start + part_size\n    \n    xshape = x.shape[0]\n    while slice_end <= xshape+part_size: # account for last partition\n        cos_sim=cosine_similarity(x[slice_start:slice_end],y)\n        # replace values below threshold with 0 to make them sorted 1st\n        clipped = np.where(cos_sim < sim_thres, 0, cos_sim) \n        s = clipped.copy() # make a copy to sort\n        s.sort() # sort\n        mask = s == 0 # create mask to identify 0's sorted positions\n        similar_indices = clipped.argsort()\n        similar_indices = np.where(mask, -1, similar_indices)[:,:-(num_rec+1):-1]\n        results = np.append(results, [similar_indices])\n        # update next partition indices\n        slice_start += part_size\n        slice_end = slice_start + part_size\n        \n    return np.reshape(results, (-1,100))","55dde214":"class ContentBasedRecommender:\n    \n    MODEL_NAME = 'Content-Based'\n    \n    def __init__(self, gamefeatures, gameinfo):\n        self.gfeat = gamefeatures\n        self.ginfo = gameinfo\n        \n    def model_name(self):\n        return self.MODEL_NAME\n    \n    # get top 100 games similar to each user profile\/vector\n    def similar_games_to_user_profiles(self, profiles, num_games):\n        # build cosine similarities matrix (users X all games)\n        cosine_similarities = cosine_similarity(profiles.iloc[:,1:], self.gfeat.iloc[:,1:])\n        profiles_similar_games = {}\n        for idx, row in profiles.iterrows():\n            # find index of top (num_games) most similar game vectors\n            similar_indices = cosine_similarities[idx].argsort()[:-(num_games+1):-1] \n            # get cosine similarity value and app name\n            similar_items = [(cosine_similarities[idx][i], \n                              self.gfeat['appId'][i]) for i in similar_indices]\n            profiles_similar_games[row['userID']] = similar_items \n        return profiles_similar_games\n    \n    # get games similar to each user's reviewed games\n    def similar_games_to_user_games(self, usergames, num_games):\n        pass\n    \n    def recommend_all(self, usergames, profiles=None, num_games=100, method='userprofile'):\n        methods = ['userprofile', 'usergames']\n        if method not in methods:\n            raise ValueError(\"Invalid method. Expected one of: %s\" % methods)\n        if method == 'userprofile':\n            if profiles is None:\n                raise ValueError(\"Method 'userprofile' requires profiles argument\")\n            print('Recommending based on user profiles...')\n            profile_sim_games = self.similar_games_to_user_profiles(profiles, num_games)\n            return profile_sim_games\n            \n            \n        elif method == 'usergames':\n            print('Recommending based on user reviewed games...')\n            pass\n        \n    def recommend_new_user(self,profile,sim_thres=None,num_rec=None,plot=True):\n        if not sim_thres: sim_thres = self.sim_thres\n        if not num_rec: num_rec = self.num_rec    \n        # game features\/vectors\n        gf = self.gfeat.set_index('appId').sort_index()\n        cos_sim = cosine_similarity(profile, gf)\n        # replace values below threshold with 0 to make them sorted 1st\n        clipped = np.where(cos_sim < sim_thres, 0, cos_sim) \n        s = clipped.copy() # make a copy to sort\n        s.sort() # sort\n        mask = s == 0 # create mask to identify 0's sorted positions\n        similar_indices = clipped.argsort()\n        similar_indices = np.where(mask, -1, similar_indices)[:,:-(num_rec+1):-1][0]\n        similar_games = le.inverse_transform(gf.index[similar_indices[similar_indices != -1]]) # remove -1 and get game id\n        similar_games = similar_games[:5] # get max 5\n        # PUT LE INTO CLASS\n\n        # display sample game icons\n        fig, ax = plt.subplots(nrows=1, ncols=len(similar_games), figsize=(12,4), constrained_layout=True)\n        fig.suptitle('Top {N} Recommended Games'.format(N=np.clip(len(similar_games),0,5)), fontsize=40)\n        for i in range(min(len(similar_games),len(similar_games))):\n            url,title = gameinfo[gameinfo['appId'] == similar_games[i]][['icon','title']].values[0]\n            if len(title) >26:\n                title = title[:23] + '...'\n            r = requests.get(url, stream=True)\n            img = Image.open(r.raw)\n            if len(similar_games) != 1:\n                ax[i].imshow(img)\n                ax[i].set_title(title, fontsize=12, wrap=True)\n                ax[i].axis('off')\n            else:\n                ax.imshow(img)\n                ax.set_title(title, fontsize=12, wrap=True)\n                ax.axis('off')        \n        l = len(similar_games); w=0.15 ; g= 0.048 \n        plt.draw()\n        if len(similar_games) != 1:\n            for c,eax in enumerate(ax):\n                # this id good for figure with 5 or less subplots\n                p11 = eax.get_position().bounds\n                x0 = (0.5-((0.5*l*w)+(((l\/2)-0.5)*g)) + (c*(w+g)))\n                p11_new = [x0, p11[1], w, p11[3]]\n                eax.set_position(p11_new)  \n        else:\n            p11 = eax.get_position().bounds\n            p11_new = [0.5-(0.5*w), p11[1], w, p11[3]]\n            ax.set_position(p11_new)  \n        return similar_games\n    \n    def evaluate_CB_recommender_kfoldcv(self, usergames, num_rec=100):\n        kf = KFold(n_splits=3, shuffle=True, random_state=0)\n        hit=0 # init hit; hit = val game in recommendations\n\n        ids = usergames['userID'].unique()\n        for u in tqdm(ids, bar_format='{l_bar}{bar:30}{r_bar}{bar:-10b}'):\n            u_revs = usergames[usergames['userID']==u]\n\n            for train_index, val_index in kf.split(u_revs):\n                train = u_revs.iloc[train_index]\n                val = u_revs.iloc[val_index]\n\n                scores = np.array(train['score']\/train['score'].sum()) # weights for each game\n                gtrain = self.gfeat.iloc[train['appId']] # get game vectors\n                profile = np.array(gtrain.mul(scores, axis='rows').sum())\n\n                cosine_similarities = cosine_similarity([profile], self.gfeat)[0]\n                similar_indices = cosine_similarities.argsort()[:-(num_rec+1):-1] \n                # get val appids, for each game, see if its inside similar indices\n                val_in_rec = val['appId'].isin(similar_indices)\n                hit += sum(val_in_rec)\n\n        return hit           \n    \n    def evaluate_CB_recommender_val(self,train,val,num_rec=100,sim_thres=0.8):\n        self.num_rec=num_rec; self.sim_thres=sim_thres\n        \n        # game features\/vectors\n        gf = self.gfeat.set_index('appId').sort_index()\n\n        # get user game features\/vectors\n        ugf = gf.iloc[train['appId'].values]\n        ugf['userID'] = train['userID'].values\n        ugf = ugf.groupby('userID').sum()\n\n        similar_indices = cos_sim_DAC(ugf.to_numpy(dtype='float32'),\n                                      gf.to_numpy(dtype='float32'),\n                                      sim_thres,num_rec,1000)\n        \n        total_rec = sum(sum(similar_indices != -1))\n        total_relevant = sum(val['score'] > 2)\n        \n        hit=0\n        ids = val['userID'].unique()\n        for u in tqdm(ids, bar_format='{l_bar}{bar:30}{r_bar}{bar:-10b}'):\n            user_val = val[(val['userID']==u) & (val['score'] > 2)] # get relevant game ratings\n            # no recommendation equals no hits\n            if sum(similar_indices[u] != -1) != 0: # if there's recommendations\n                hit += np.isin(user_val['appId'].values, similar_indices[u]).sum()\n                \n        print('Precision@{k}: {p}'.format(k=num_rec,p=hit\/total_rec))\n        print('Recall@{k}: {r}'.format(k=num_rec,r=hit\/total_relevant))\n        \n        return {'hits':hit,'total_rec':total_rec,\n                'total_relevant':total_relevant}\n    \n    def evaluate_CB_recommender_val_weighted(self,train,val,num_rec=100,sim_thres=0.8):\n        self.num_rec=num_rec; self.sim_thres=sim_thres\n        \n        # game features\/vectors\n        gf = self.gfeat.set_index('appId').sort_index()\n\n        # get user game features\/vectors & apply weightings based on scores\n        scores = ((train['score']-1) \/ (4)).values # normalize scores for weightings\n        ugf = gf.iloc[train['appId'].values]\n        ugf = ugf.mul(scores, axis=0)  # apply weighting to game vectors \n        # get user avg profile, ignoring those that have score of 1 (normalized to 0)\n        count= pd.concat([ugf.sum(axis=1).reset_index(drop=True),\n                          train['userID'].reset_index(drop=True)],\n                         axis=1).rename(columns={0:'summ'}) # find vectors with 0 scores (sum of vector is 0)\n        count = count.groupby('userID')['summ'].apply(lambda x: x.ne(float(0)).sum()).values # count number of non-0 reviews\n        ugf['userID'] = train['userID'].values\n        profile = ugf.groupby('userID').sum().div(count, axis=0)\n        #profile = ugf.groupby('userID').sum()\n        profile = profile.fillna(0) # set profile as 0s if all reviewed games are 1 score\n\n        ## compute cosine similarity matrix and get top N similar games for each profile\n        similar_indices = cos_sim_DAC(profile.to_numpy(dtype='float32'),\n                                      gf.to_numpy(dtype='float32'),\n                                      sim_thres,num_rec,1000)\n        \n        total_rec = sum(sum(similar_indices != -1))\n        total_relevant = sum(val['score'] > 2)\n\n        hit=0\n        ids = val['userID'].unique()\n        for u in tqdm(ids, bar_format='{l_bar}{bar:30}{r_bar}{bar:-10b}'):\n            user_val = val[(val['userID']==u) & (val['score'] > 2)] # get relevant game ratings\n            # no recommendation equals no hits\n            if sum(similar_indices[u] != -1) != 0: # if there's recommendations\n                hit += np.isin(user_val['appId'].values, similar_indices[u]).sum()\n                \n        print('Precision@{k}: {p}'.format(k=num_rec,p=hit\/total_rec))\n        print('Recall@{k}: {r}'.format(k=num_rec,r=hit\/total_relevant))\n        \n        return {'hits':hit,'total_rec':total_rec,\n                'total_relevant':total_relevant}\n    \n    def predict(self,x,y,method='weighted'):\n        methods = ['weighted', 'nonweighted']\n        if method not in methods:\n            raise ValueError(\"Invalid method. Expected one of: %s\" % methods)\n        if method == 'weighted':\n            return self.evaluate_CB_recommender_val_weighted(x,y,self.num_rec,self.sim_thres)\n        elif method == 'nonweighted':\n            return self.evaluate_CB_recommender_val(x,y,self.num_rec,self.sim_thres)","b815e741":"%%time\ncbr = ContentBasedRecommender(game_features_tfidf, gameinfo)\n#cbr.evaluate_CB_recommender_kfoldcv(X_train)\ncbr.evaluate_CB_recommender_val(reviews_content_train,reviews_content_val,sim_thres=0.85)\n#cbr.evaluate_CB_recommender_val(X_train,X_val)\n#cbr.evaluate_CB_recommender_val_weighted(X_train, X_val)","04b629f3":"%%time\ncbr.evaluate_CB_recommender_val_weighted(reviews_content_train, reviews_content_val,sim_thres=0.85)","1ad2b2ec":"%%time\ncbr = ContentBasedRecommender(game_features_lda, gameinfo)\ncbr.evaluate_CB_recommender_val(reviews_content_train,reviews_content_val,sim_thres=0.85)","d9851298":"%%time\ncbr.evaluate_CB_recommender_val_weighted(reviews_content_train,reviews_content_val,sim_thres=0.85)","9b8942dc":"# plot evaluation metrics for range of cos_sim threshold\nresults=[]\nfor i in [0.3,0.4,0.5,0.6,0.7,0.8,0.9]:\n    result=cbr.evaluate_CB_recommender_val_weighted(reviews_content_train,\n                                                    reviews_content_val,\n                                                    sim_thres=i)\n    results.append(result)","588b87a5":"from matplotlib.lines import Line2D\nprecision = np.array([]); recall = np.array([])\nfor i in results:\n    precision=np.append(precision,i['hits']\/i['total_rec'])\n    recall=np.append(recall,i['hits']\/i['total_relevant'])\n#f1 = 2 * (precision*recall) \/ (precision+recall)\nx = np.array([0.3,0.4,0.5,0.6,0.7,0.8,0.9])\nfig, ax1 = plt.subplots()\nax1.set_xlabel('Cosine Similarity Threshold')\nax1.set_ylabel('Precision')\nplt.plot(x, precision, color='#1f77b4')\n#plt.plot(x, f1, color='#2ca02c')\nax2 = ax1.twinx()\nax2.set_ylabel('Recall') \nplt.plot(x, recall, color='#ff7f0e')\nlegend_elements = [Line2D([0], [0], color='#1f77b4', lw=2, label='Precision'),\n                   Line2D([0], [0], color='#ff7f0e', lw=2, label='Recall')]\nplt.legend(handles=legend_elements, loc='center left')\nplt.show()","e7e7ef0e":"%%time\ncbr.predict(reviews_content_all_train,reviews_content_test,method='nonweighted')","314c943f":"newuser = reviews_content_all_train[reviews_content_all_train['userID']==35014]\nnewuser = build_user_profile(newuser.appId, newuser.score, \n                             game_features_lda, method='nonweighted')\ncbr.recommend_new_user(newuser,sim_thres=0.1)","584696b4":"from surprise import SVD, SVDpp, NMF\nfrom surprise import Dataset, Reader, accuracy\nfrom surprise.model_selection import cross_validate\n\nfrom sklearn.model_selection import train_test_split, TimeSeriesSplit","1c910b94":"my_seed = 0\nrandom.seed(my_seed)\nnp.random.seed(my_seed)","e67ea22e":"# filter so that reviews contains items and users with min 5 reviews\n# this is done so that train_test_split will give same number of \n# users and items for reviews and train\n# otherwise, SVD will return matrix will less games or users\n\n# numuser = reviews.value_counts('userID')\n# numgame = reviews.value_counts('appId')\n# while numuser[numuser < 5].any() or numgame[numgame < 5].any():\n#     reviews = reviews[~reviews.appId.isin(numgame[numgame < 5].index)]\n#     reviews = reviews[~reviews.userID.isin(numuser[numuser < 5].index)]\n#     numuser = reviews.value_counts('userID')\n#     numgame = reviews.value_counts('appId')\n    \n# # reset userID\n# reviews['userID'] = reviews['userID'].astype('category')\n# reviews[\"userID\"] = reviews[\"userID\"].cat.codes","c8232432":"# train test split by time\ntrain = reviews[reviews['at'] <= reviews['at'].quantile(0.9)]\ntest = reviews[reviews['at'] > reviews['at'].quantile(0.9)]\nprint('Number of Games in Reviews: %.2d\\nNumber of Games in Train: %.2d'\n      %(len(reviews.appId.unique()), len(train.appId.unique())))\nprint('Number of Users in Reviews: %.2d\\nNumber of Users in Train: %.2d'\n      %(len(reviews.userID.unique()), len(train.userID.unique())))","7f95ac1e":"print('Train Date Range: {s} to {e}'.format(s=train['at'].dt.date.min(), \n                                            e=train['at'].dt.date.max()))\nprint('Test Date Range: {s} to {e}'.format(s=test['at'].dt.date.min(), \n                                           e=test['at'].dt.date.max()))","1b97f8d0":"# # train test split within users\n# train, test = train_test_split(reviews, \n#                                test_size=0.2,\n#                                shuffle=True, \n#                                stratify=reviews['userID'],\n#                                random_state=0)\n# print('Number of Games in Reviews: %.2d\\nNumber of Games in Train: %.2d'\n#       %(len(reviews.appId.unique()), len(train.appId.unique())))\n# print('Number of Users in Reviews: %.2d\\nNumber of Users in Train: %.2d'\n#       %(len(reviews.userID.unique()), len(train.userID.unique())))","9c562c90":"# A reader is still needed but only the rating_scale param is required.\nreader = Reader(rating_scale=(1, 5))\n\n# The columns must correspond to user id, item id and ratings (in that order).\ntraindata = Dataset.load_from_df(train[['userID', 'appId', 'score']], reader)\ntestdata = Dataset.load_from_df(test[['userID', 'appId', 'score']], reader)\n\nprint(traindata)\nprint(testdata)","3e7d04b5":"traindata.raw_ratings[:5] # show 5 samples","1df57c7b":"from surprise.model_selection import GridSearchCV\nparam_grid = {'lr_all':[0.005, 0.01], 'n_epochs':[50],\n              'n_factors':[100],\n              'reg_bu': [0.2], 'reg_bi': [0.2],\n              'reg_pu': [0.5], 'reg_qi': [0.5],\n              'random_state': [0]}\n\ngs = GridSearchCV(SVD, param_grid, measures=['rmse', 'mae'], \n                   cv=3, n_jobs=-1)\n\ngs.fit(traindata)\nprint(gs.cv_results['mean_test_rmse'])\nprint(gs.best_params)","2dfcbdd6":"# after tuning CV, retrain on the whole trainset\n# it's called trainset here as it is not for folding, like traindata\n# traindata is a DatasetAutoFolds class \n# while trainset is a Trainset class \nsvd = gs.best_estimator['rmse']\ntrainset = traindata.build_full_trainset()\nsvd.fit(trainset)\n\n# Compute biased accuracy on whole trainset\n# testset contains only observed ratings\npredictions = svd.test(trainset.build_testset()) \nprint('Biased RMSE on trainset:   %.4f' %(accuracy.rmse(predictions, \n                                                         verbose=False)),end=' ')","541bb1b7":"print(svd.pu.shape, svd.qi.shape, svd.bu.shape, svd.bi.shape)","ced3f7a3":"# Compute unbiased accuracy on testset \/ unseen data\ntestset = testdata.construct_testset(testdata.raw_ratings) # get test raw data\npredictions = svd.test(testset)\nprint('Unbiased RMSE on testset:   %.4f' %(accuracy.rmse(predictions, \n                                                         verbose=False)),end=' ')","4ae9f417":"unique_games = set(reviews.appId)\n\ndef recommend_games_to_user(userid, num_rec, unique_games=unique_games):\n    rec = {}\n    low_r = 6.0\n    low_g = ''\n    # get games not rated by user\n    unrated_g = list(unique_games ^ set(reviews[reviews.userID==userid].appId))\n    random.shuffle(unrated_g) # shuffle the list\n\n    for g in unrated_g:\n        #print(svd.predict(userID, g))\n        rhat = svd.predict(userid, g).est\n        if rhat < low_r and len(rec) < num_rec:\n            rec[g] = rhat # append game and rating\n            low_r = rhat # update lowest rating\n            low_g = g # update game with lowest rating\n        elif rhat > low_r and len(rec) < num_rec:\n            rec[g] = rhat # append game and rating\n        elif rhat > low_r and len(rec) == num_rec:\n            del rec[low_g] # delete lowest\n            rec[g] = rhat # append game and rating\n            low_g = min(rec, key=rec.get) # update game with lowest rating\n            low_r = min(rec.values()) # update lowest rating\n    return rec\n\nunique_users = list(set(reviews.userID))\nall_recs = []\nfor u in unique_users[:100]:\n    rec = recommend_games_to_user(u, 50)\n    all_recs.append(pd.DataFrame.from_dict(rec, 'index'))\n    \nall_recs = pd.concat(all_recs)\nall_recs","d55cb17f":"all_recs.reset_index(inplace=True)\nall_recs.columns = ['app','rating']\nall_recs.value_counts('app')","9a1fefaf":"<a id='CB_Filter'><\/a>\n# 4. Content-Based Filtering\n\nContent-based filtering uses item features (game features in this case) to recommend other games which are similar to what the user likes. This can be based on their previous actions, such as playing time, or ratings. Therefore, they only need information from the user and information from other users are not utilized at all. This can be both a con and a pro. To do content-based filtering, we need to have a feature matrix (`game_features`), where each row represents a game and each column represents a feature, like above. As for the user, we will need a way to represent the user in the same feature space, meaning that for each user, we will create a `user profile` for them, and this `user profile` should have the same features as the `game_features` matrix. This can be done in a few ways, which will be described later on down below. The `user profle` will then be compared to the `game_features`, and games that are more \"similar\" will be recommended to the user.\n\n<u> **Advantages** <\/u>\n* The model does not depend on data from other users, since the recommendations are specific to the current user, making it more scalable\n* It can capture the specific interests of a user, and can recommend niche games that very few other users are interested in or have played before\n* It can avoid the cold-start problem, which arises when users are new and have not rated many games\n\n<u> **Disadvantages** <\/u>\n* The model requires hand-engineered features, meaning that a certain level of domain knowledge is needed. The better the engineered features, the better the model\n* The model can only make recommendations based on the users' current interests, meaning it has limits on expanding on the users' interests\n* It does not treat users as a network of connected users. Each user is considered independent of the others and thus the phrase and logic, 'Birds of the same feather flock together' cannot be applied to recommend products to users\n\nFor more information about content-based filtering, refer to this [post](https:\/\/developers.google.com\/machine-learning\/recommendation\/content-based\/basics).\n\n<br>\n\n<a id='Extract'><\/a>\n## 4.1. Extract content ratings & genre as features\n\nContent ratings and genres are categorical features and they will need to be dummy encoded (one-hot encoded and leave 1 out). Their weights are then reduced.","05ecae65":"### LightFM\n\n[Movielens simple tutorial](https:\/\/github.com\/lyst\/lightfm\/blob\/master\/examples\/movielens\/example.ipynb) <br>\n[Including meta information with LightFM](\nhttps:\/\/www.ethanrosenthal.com\/2016\/11\/07\/implicit-mf-part-2\/)\n\n### Using Tensorflow\n[Tensorflow WALS (Weighted Alternating Least Squares)](https:\/\/cloud.google.com\/solutions\/machine-learning\/recommendation-system-tensorflow-overview)\n\n\n### Overview of many libraries\n[Here](https:\/\/techairesearch.com\/overview-of-matrix-factorization-techniques-using-python\/)","007d898b":"Here, we evaluate 4 different models, weighted and non-weighted for each of TF-IDF and LDA game features.","a1800f3c":"The word cloud shows that \"world\" and 'puzzle\" are the most frequently occuring word in the entire game description corpus.","ae164634":"<a id='SVD'><\/a>\n## 5.1. Singular Value Decomposition (SVD)\n\n### Surprise (Simple Python Recommendation System Engine)\n[Surprise](http:\/\/surpriselib.com\/); [Docs](https:\/\/surprise.readthedocs.io\/en\/stable\/)\n\n[Guide](https:\/\/towardsdatascience.com\/how-you-can-build-simple-recommender-systems-with-surprise-b0d32a8e4802)","3137389e":"<a id='BOW'><\/a>\n## 4.2. Bag-of-Words\n\nBag-of-Words is a  simple model used in Natural Language Modelling to simplify the contents of a document. It simply counts the frequency of words in a document. The higher the count, the more important they are. We will not be using them to create features but just as a way to understand more about the game `descriptions`.\n\n### Preprocess and clean game description\n\nBefore doing that, the game `descriptions` need to be cleaned and preprocessed first. The following steps were performed:\n\n1. Remove escape sequences. An example is `\\n` which is used to break sentences\n2. Remove emails such as gamename@gmail.com, usually developer email address\n3. Remove URLs, which were included in game descriptions to link to other pages\n4. Perform decontraction, which expands words like \"won't\" to \"will not\"\n5. Remove punctuations\n6. Remove stopwords (eg. to, with, and) and general game and app words (eg. game, play, fun)\n7. Perform lemmatization, which reduces words to their base form, such as \"playing\" to \"play\", or \"adventurous\" to \"adventure\"\n8. Remove digits such as \"2021\", \"1 v 1\", \"100%\" but not digits in words like '3d'\n\n","3f5c4dfa":"Here we create a class for the content-based filtering model containing the different methods for evaluation and prediction. ","26620710":"For the distribution of reviews per game, the maximum is 10k reviews, which was selected during scraping and reduced after filtering and cleaning. There are some games with very few reviews as well. The green portion is usually referred to as the long-tail because most games are not rated by a lot of users.","b1eb2693":"<a id='TF_IDF'><\/a>\n## 4.3. Term Frequency-Inverse Document Frequency (TF-IDF)\n\n\nTerm Frequency - Inverse Document Frequency (TF-IDF) is a bag of words (BoW) technique used in many areas of NLP such as text mining and information retrieval. Here, our purpose is to use it to extract the most ***important*** words from the game summaries, which can then be used as features of games for content-based filter recommender systems. \n\n**TF**: Term Frequency refers to the frequency of a word in a document (a single game description in this case)\n\n**IDF**: Inverse Document Frequency refers to the inverse of the document frequency among the whole corpus of documents, or how significant the word is in the whole corpus\n\nTF-IDF is used over Word Counts for this reason: Suppose a game has the following summary \"`Join us in this amazing shooter game`\". It is certain that words like `join` and `game` will occur more frequently than `shooter` in all the game summaries but the relative importance of `shooter` as a feature of the game is higher than the other words. TF-IDF weighting negates the effect of high frequency words in determining the importance of an item.\n\n\n\n$$W_{i,j} = tf_{i,j} \\cdot log(\\frac{N}{df_{i}})$$\n\n$_{i}$ = term <br>\n$_{j}$ = document <br>\n$tf_{i,j}$ = frequency of $_{i}$ in $_{j}$ <br>\n$df_{i}$ = number of documents containing $_{i}$ <br>\n$N$ = total number of documents <br>\n","a1e8181b":"### Sparse Representation of the Rating Matrix\n\nThe rating matrix could be very large and, in general, most of the entries are unobserved, since a given user will only rate a small subset of movies. For effcient representation, we will use a [tf.SparseTensor](https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/SparseTensor). A `SparseTensor` uses three tensors to represent the matrix: `tf.SparseTensor(indices, values, dense_shape)` represents a tensor, where a value $A_{ij} = a$ is encoded by setting `indices[k] = [i, j]` and `values[k] = a`. The last tensor `dense_shape` is used to specify the shape of the full underlying matrix.\n\n#### Toy example\nAssume we have $2$ users and $4$ games. Our game ratings dataframe has three ratings,\n\nuser\\_id | game\\_id | rating\n--:|--:|--:\n0 | 0 | 5.0\n0 | 1 | 3.0\n1 | 3 | 1.0\n\nThe corresponding rating matrix is\n\n\n$$\nA =\n\\begin{bmatrix}\n5.0 & 3.0 & 0 & 0 \\\\\n0   &   0 & 0 & 1.0\n\\end{bmatrix}\n$$\n\n\nAnd the SparseTensor representation is:\n```python\nSparseTensor(\n  indices=[[0, 0], [0, 1], [1,3]],\n  values=[5.0, 3.0, 1.0],\n  dense_shape=[2, 4])\n```\nThe COO (coordinate list) matrix in SciPy will be:\n```python\nscipy.sparse.coo.coo_matrix\n(0, 0)     5\n(0, 1)     3\n(1, 3)     1\n```","3b61dff5":"Another very good LDA modelling is Mallet's version, which unfortunately, I was not able to implement on Kaggle.","4bc5e647":"Should we change train test split to split by time instead of within user?","e09453df":"<a id='LDA'><\/a>\n## 4.4. Latent Dirichlet Allocation (LDA) Model for Topic Modelling to Create Features\n\nAn alternative to TF-IDF is Latent Dirichlet Allocation (LDA), which is a topic model used to classify texts in documents to a user pre-specified number of topics, based on the co-occurence of words collected from the documents. Each document (game description in this case) is generated from a mixture of topics and each of these topics is a mixture of keywords. A good quality of topics depend on how clear, segregated and meaningful the topics are.\n\n**Latent**: Topic structures in a document are latent meaning they are hidden structures in the text.\n\n**Dirichlet**: The Dirichlet distribution determines the mixture proportions of the topics in the documents and the words in each topic.\n\n**Allocation**: Allocation of words to a given topic.\n\n\n\nRefer to this [post](https:\/\/humboldt-wi.github.io\/blog\/research\/information_systems_1819\/is_lda_final\/) for an in-depth statistical introduction to LDA and [this](https:\/\/www.machinelearningplus.com\/nlp\/topic-modeling-gensim-python\/#:~:text=Topic%20Modeling%20with%20Gensim%20(Python)&text=Topic%20Modeling%20is%20a%20technique,in%20the%20Python's%20Gensim%20package.) for a detailed guide to using it in Python.\n\nWe will be using the [Gensim](https:\/\/pypi.org\/project\/gensim\/) library for LDA topic modelling. \n","795d5302":"## Game Information","e88e56d4":"Next, we do the one more filtering after removing some games, to remove users with less than 5 reviews and give each a unique `userID`. We have about **436,000** unique users and ~**3.3 million** reviews from **6924** games.","92b3c5c1":"Going through some of the topics, we can get an idea of what they are by looking at their keywords.\n\n<img src=\"https:\/\/drive.google.com\/uc?export=view&id=1lhC8iNtKkOqyzjnnZ40--q5mQ5jAMuBi\" width=800>","d3e4b9f4":"We can also use the pyLDAvis library to visualize the LDA model. As the library only allows viewing on 1080 width size, it gets cut off on Kaggle's notebook and messes with it. Thus, I only included a screenshot of it below but the interactive chart can be plotted by uncommenting the code chunk below.\n\nEach bubble on the left plot represents a topic and the larger it is, the more prevalent the topic is. Good topics will have big, non-overlapping bubbles scattered throughout the 2-dimensional space. When there are too many topics selected than necessary, the bubbles will become small and many will overlap with each other.","7b691aa6":"<a id='Ngrams'><\/a>\n### 4.2.1. Ngrams \n\nWe can also look at **Ngrams**, which shows N words that are adjacent to each other.\nJudging from the word cloud, it does not seem that any bigrams or trigrams provide additional information over that of unigrams. Therefore, they are not likely to be useful features. For example, `slot machine` as a feature can be inferred from just the single word `slot`. Same goes for `casino slot` from `casino` or `slot`.","8921fcb0":"<a id='EDA'><\/a>\n# 2. Exploratory Data Analysis\n\nHere we explore the reviews dataset and its distribution. But before we do that, we are going to account for attacks on game reviews.","5b99110a":"~15 topics appears to have the highest coherence score. Next, we view the topics and the keywords that made them up. Each keyword contributes a certain weightage to the topic, which is indicated by the \"weightage * keyword\".","bab6273f":"# Google Games Recommendation System<br>\n\n\nTeYang Lau<br>\nCreated: 14\/1\/2021<br>\nLast update: 19\/2\/2021<br>\n\n\n<img src=\"https:\/\/drive.google.com\/uc?export=view&id=1Rf5YcdRcndMqANzXwQQ2c1v0ePndjG6P\" width=900>\n<br><br>\n\n## Github Repo\n\nThe repo for the entire project can be found [**here**](https:\/\/github.com\/teyang-lau\/GoogleGame_RecommenderSystem). Please consider starring it if you enjoy this notebook! \n\n\n\n## Project Goals\n\n1. *Build* a recommendation system for **suggesting** games to people based on their reviews\/ratings of games\n\n2. *Scrape* google game **reviews** and game **features** from google play store\n\n3. *Preprocess\/Clean* game **description** to **generate\/engineer** more features about the games\n\n4. Use **TF-IDF** to extract most important words and **Latent Dirichlet Allocation (LDA)** for topic modelling \n\n5. *Employ* **content-based** and **collaborative filtering** techniques to recommend games to each user based on their game ratings\n\n6. *Evaluate* recommendation models using appropriate offline testing metrics such as **prediction accuracy, usage performance, novelty, and serendipity**\n\n7. *Deploy* the model onto a **web app**, for users to obtain game recommendations \n\n   \n\n## About this Dataset\n\nThere are thousand of games on the Google Play Store. Using the Node.js [`google-play-scraper`](https:\/\/github.com\/facundoolano\/google-play-scraper) package, 200 games' `app IDs` from each category and collection in the store were obtained. Detailed game information and game reviews were scraped using Python [`google-play-scraper`](https:\/\/github.com\/JoMingyu\/google-play-scraper) package. A maximum of 2000 reviews were collected from each game, and stored in JSON format. Detailed game information contains game features like the long game description, its summary, price, released year, genre, content rating, url etc. while each review contains the username, review text, and review score on a scale of 1-5. In total, there were **~7600 games** and **~8 million game reviews** before cleaning and filtering.\n\n\n\n## What's in this notebook:\n1. [Data Loading](#Data_loading)<br>\n\n2. [Exploratory Data Analysis](#EDA)<br>\n\n3. [Data Cleaning](#Clean)<br>\n\n4. [Content-Based Filtering](#CB_Filter)<br>\n    4.1. [Extract content ratings & genre as features](#Extract)<br>\n    4.2. [Bag-of-Words](#BOW)<br>\n    4.3. [Term Frequency-Inverse Document Frequency (TF-IDF)](#TF_IDF)<br>\n    4.4. [Latent Dirichlet Allocation (LDA) Model for Topic Modelling to Create Features](#LDA)<br>\n    4.5. [Recommending Based on Game Content Similarity](#Recommend)<br>\n    4.6. [Evaluation](#Eval_CB)<br>\n    \n5. [Collaborative Filtering (ongoing)](#C_Filter)<br>\n    5.1. [Singular Value Decomposition (SVD)](#SVD)<br>","45fe9a32":"Below approach by user is not useful as many attackers might be single reviewers due to the extraction approach.","05e9876e":"<a id='Eval_CB'><\/a>\n## 4.6. Evaluation\n\n\n\n\nFor evaluating the content-based model, the user reviews were split into a train, validation and test set (40,20,40, so that for users with only 5 ratings, 2 will be used to train and evaluate on 1 rating, and finally predicted and compared to 2 test ratings). \n\nCurrently, only 2 methods of recommendations were implemented for each of TF-IDF and LDA game features.\n\n1. **Non-weighted**: User profiles were created by getting the mean of the vectors of games that were rated (train set). This `user profile` was then compared to all the game vectors to find the top ***100 most similar games*** using **cosine similarity**. For each of the game rated in the validation set that received a rating of >=3 (relevant games), a **hit** is given if it is inside the 100 recommendations. **Precision@N** and **Recall@N** metrics were then computed\n2. **Weighted**: User profiles were created by computing the weighted average of the games they rated based on the ratings. A game with a rating of 2 will get a smaller weighting compared to a game with a rating of 5. The following steps are the same as the non-weighted method\n\n$$precision@k = \\frac{number\\ of\\ hits\\ (correct\\ recommendations)}{number\\ of\\ predictions\\ made}$$\n\n$$recall@k = \\frac{number\\ of\\ hits\\ (correct\\ recommendations)}{number\\ of\\ true\\ labels}$$\n\n<br> \n\nRefer to these posts [[1, ](https:\/\/medium.com\/@m_n_malaeb\/recall-and-precision-at-k-for-recommender-systems-618483226c54)[2, ](https:\/\/towardsdatascience.com\/evaluation-metrics-for-recommender-systems-df56c6611093)[3](https:\/\/towardsdatascience.com\/recommendation-systems-models-and-evaluation-84944a84fb8e)] for a great intro about evaluation metrics for recommender systems. This [paper](https:\/\/link.springer.com\/chapter\/10.1007\/978-0-387-85820-3_8) by Shani and Gunawardana is also an excellent resource beyond the normal evaluation metrics.\n\nhttps:\/\/www.kaggle.com\/gspmoreira\/recommender-systems-in-python-101","329844bf":"The above step takes a long time depending on the range of topics chosen and steps taken.","d558bcf9":"<a id='C_Filter'><\/a>\n# 5. Collaborative Filtering (ongoing)\n\n\nThere are 2 types of methods commonly used in collaborative filtering: **Neighborhood-based filtering** (aka Memory-based filtering) and **Model-based filtering**.\n\nNeighbourhood-based filtering can be handled in 2 ways: **User-based filtering** or **Item-based filtering**\n\n<img src=\"https:\/\/drive.google.com\/uc?export=view&id=10tmhU_sX123U20Fk7sNUwgJaZm1Vw7Ds\" width=400>\n<br>\n\n**User-based filtering**: Find similar users to active user and recommend games they liked that the active user has not played before <br>\n**Item-based filtering**: Find games that active user likes and come up with similar items to recommend\n\nWhich one to use? It's unwise to do pre-computation of user similarities as users can grow quickly and are more susceptible to change. Items are considered more stable and the same types of people usually like the same types of items, leading to higher accuracy compared to user-based models. If there are many more users than items, going for item-based filtering is better. Otherwise, user-based filtering is more economical. However, user-based filtering is often a \"*better*\" way to give recommendations. If item-based filtering is used, the model will find items similar to what the user has already rated, but similar items won't provide the diversity and serendipity that similar users can provide. With user similarity, the hope is that the data will connect a user with other users with different pecularities in their tastes and provide surprising and novel, but yet good recommendations. Item-based filtering can also provide reasons for the recommendations, such as the case when, \"Because you played Candy Crush, you might also like Balloon Buster\".\n\n**Neighbourhood** methods can be viewed as generalizations of k-nearest neighbour classifiers. They are different from model-based methods in that they do not train a model beforehand for prediction but rather, the prediction is specific to the instance being predicted by looking for similar users or items. Therefore, they belong to *instance-based learning methods* or *lazy learnng methods*. They are simple and intuitive, and thus easy to implement and debug. It is easy to justify why items are recommended and the interpretability of item-based filtering is noteworthy. The **main disadvantage** of these methods is the **complexity of its offline implementation**, especially in large-scale settings. The offline phase of the user-based method requires at least *O*(m<sup>2<\/sup>) time and space, as the similarities for every user with all users need to be obtained. However, the online phase is usually very efficient, as the similarity matrix has already been pre-computed offline. Also, these methods usually have limited coverage because of sparsity. For example, if none of user1's neareat neighbours have rated game1, then it is not possible to compute a rating prediction of game1 for user1. Sparsity also create challenges for robust similarity computation.\n\nSource: [Recommender Systems: The Textbook](https:\/\/www.springer.com\/gp\/book\/9783319296579) by C.C.Aggarwal and [Practical Recommender Systems](https:\/\/www.manning.com\/books\/practical-recommender-systems) by K.Falk.","9414be68":"The final prediction on the test set yielded a ***precision@100*** of **0.006** and a ***recall@100*** of **0.12**. The low precision is most probably due to the sparse data that we have. To reduce this issue, we can use a hybrid model consisting of both content-based and collaborative filtering methods.\n\nNext, we create a sample user profile and use it to give 5 recommendations to the user.","57887b72":"<a id='Clean'><\/a>\n# 3. Data Cleaning\n","a4a26978":"<a id='Data_loading'><\/a>\n# 1. Data Loading","c7616eae":"## Other Evaluation Metrics\n\n### Coverage\nEven if a recommendation system is accurate, it may not be able to recommend a certain proportion of items or to a certain proportion of users. **User-space** coverage measures the fraction of users for which at least k-ratings can be predicted while **item-space** coverage refers to the fraction of items for which the ratings of k-users can be predicted. **Catalog coverage** on the other hand, is specific to list recommendations rather than rating prediction. It represents the proportion of items that are recommended to at least one user. This is closely linked to diversity. If every user is recommended the most popular 100 games out of 1000 games, then the actual coverage is very poor (10%) even though the accuracy might be high.\n\n### Confidence and Trust\n**Confidence** refers to the confidence in which the recoomendation models give useful recommendations to the user. This is often measured by computing the confidence interval of the predicted ratings. In general, recommendation systems that can recommend smaller confidence intervals are more desirable. \n\nOn the other hand, **trust** measures the amount of faith a user has on the recommendations given to him or her. This is difficult to quantify and is mostly measured by conducting user surveys or by asking users how useful or how much they trust each recommendation during online testing. Examples of improving trust include justifying why a recommendation is given (you are recommended game x because you played y) and recommending products already used\/liked or known by the user. This is in direct contrast to the notion of novelty where recommendations that are already liked or known by the user are undesirable, and which provides little utility. Many evaluation metrics of recommendation systems have trade-offs with each other and balancing them requires adequate domain knowledge.\n\n### Novelty\n**Novelty** refers to the ability of a recommendation system to provide suggestions which are *not known* by the user. This helps users to widen their circles of interest and often helps them to understand their likes and dislikes. Content-based systems tend to perform poorly on this because items recommended are often in the same category or contain the same content. Collaborative methods, however, are able to make recommendations by leveraging similar users with peculiar tastes. The most natural way to measure novelty is during online testing to explicitly ask users whether the recommendations provided were known by them previously. For offline testing, data with timestamps can be used by splitting them at a certain timepoint *t*<sub>0<\/sub>. Items correctly recommended before *t*<sub>0<\/sub> (past) will be penalized while items correctly recommended after *t*<sub>0<\/sub> (future) will be rewarded. Popular items that are recommended can also be penalized as they are less likely to be novel.\n\n### Serendipity\nThis is a measure of how \"unexpected\" or \"surprising\" a recommendation is. It includes the notion of novelty but also of \"non-obviousness\". For example, if a user has always used an Apple mobile phone, recommending a Samsung phone might be novel but not \"non-obvious\" because both are popular high-end phones by the top mobile phone companies. Instead, recommending a budget phone such as a Redmi phone (budget phone by Xiaomi) is more serendipitious as it is a departure from \"obviousness\". Serendipity can be measured during online testing through user feedback on how useful and non-obvious recommendations. During offline testing, a primitive model can be used, such as a content-based model, which usually suggests obvious items. The fraction of recommendations made that are correct, but not recommended by the content-based model, is used as a measure of seredipity. However, usefulness of a recommendation also has to be incorporated into serendipity because a recommendation might be surprising yet totally unrelated (recommending a TV instead of a mobile phone). Serendipity has long-term effects for improving the conversion rate of the recommender systems although it might not boost accuracy in the short-term. \n\n### Diversity\nThis is closely related to novelty, serendipity and coverage. It measures how diverse a set of recommendations given to a user is. For example, if 3 movies were recommended to a user and all 3 are from the same genre\/same production companies\/same actors, the user might dislike all 3 movies if he or she dislikes the first one. Presenting different items often increases the chances that the user might choose one of them and often also improves measures of novelty, serendipity and coverage. Sales diversity across different products can also improve (such as recommending a tablet instead of a mobile phone). Diversity can be measured by computing the average similarity using the feature vectors of the items from the set of recommendation. The smaller the average similarity, the more diverse the set of recommendation is. \n\n### Robustness and Stability\nThis refers to how \"unchanged\" recommendations systems are when they are under the presence of attacks such as fake review and ratings. \n\n### Scalability\nDue to the increase in big data, there is a need for measuring how a recommendation system can be effectively and efficiently implemented under the constraints of both **time and space complexity**. The **amount of training time** can be a useful measure. As most recommendations are trained offline, in the case of neighbourhood methods where peer groups of users are pre-computed or matrix factorization systems where latent factors are predetermined, the amount of training time is usually acceptable if they can be completed within a day (this also depends on availability and capacity of organizational resources). Prediction is usually done online to generate recommendations to specific active users. Thus, the **prediction time** needs to be low, as it determines the latency in which customers receive suggestions and influences the overall user experience. While the previous 2 measurements deal with time complexity, space complexity is important as well. When rating matrices are large, or when the pre-computed similarity matrices are large, it becomes a challenge to hold them in memory and thus efficient algorithms need to be used to reduce the **memory requirements**. When this becomes too high, it becomes mroe difficult to implement the systems in large-scale and practical settings.\n\n\nSource: [Recommender Systems: The Textbook](https:\/\/www.springer.com\/gp\/book\/9783319296579) by C.C.Aggarwal","9f518e80":"`gameinfo` contains many features and information about each game, including game `description`, `price`, `minInstalls`, url link to the `icon`, `genre` and many more.","44ba3506":"There are many unique `content ratings`, because some games actually include several of them.","82475c7e":"To get an idea of how good the topics are, the topics coherence score can be computed. Also, this can be plotted for a range of number of topics, to find the optimal number of topics.","d324bb7f":"**If you like this notebook, please give me an upvote and\/or check out my other work!**\n<br><br>\n\n\n<font size=\"+3\" color=\"steelblue\"><b>My other works<\/b><\/font><br>\n\n<div class=\"row\">\n    \n  <div class=\"col-sm-4\">\n    <div class=\"card\">\n      <div class=\"card-body\" style=\"width: 20rem;\">\n         <h5 class=\"card-title\"><u>Pneumonia Detection with PyTorch<\/u><\/h5>\n         <img style='height:170' src=\"https:\/\/raw.githubusercontent.com\/teyang-lau\/Pneumonia_Detection\/master\/Pictures\/train_grid.png\" class=\"card-img-top\" alt=\"...\"><br>\n         <p class=\"card-text\">Pneumonia Detection using Transfer Learning via ResNet in PyTorch<\/p>\n         <a href=\"https:\/\/www.kaggle.com\/teyang\/pneumonia-detection-resnets-pytorch\" class=\"btn btn-primary\" style=\"color:white;\">Go to Post<\/a>\n      <\/div>\n    <\/div>\n  <\/div>   \n    \n  <div class=\"col-sm-4\">\n    <div class=\"card\">\n      <div class=\"card-body\" style=\"width: 20rem;\" style='background:red'>\n        <h5 class=\"card-title\"><u>Transformer is All You Need for Disaster Tweets? <\/u><\/h5>\n        <img style='width:250px' src=\"https:\/\/bn1301files.storage.live.com\/y4miItelaS-PGrdZrtHBBmQsPvQ8kv1csnkuYMYpwTlo5r4ayFzZ-LngiqzvwfZhNFMkPPH1NCliUr6r6SEe0cJgqA3a1xHkDVlyOQD5amu1z_tKzTte8I9C7tJYjHj_ETqZXLh7ZVrW4AD_S3shi0tn9FmnJuEd9Ej2nRXYb1FS72mGoGszHN6wrenHBtUygDk?width=521&height=329&cropmode=none\" class=\"card-img-top\" alt=\"...\"><br>\n        <p class=\"card-text\">Classify real disaster tweets using LSTM and BERT.<\/p>\n        <a href=\"https:\/\/www.kaggle.com\/teyang\/transformer-is-all-you-need-for-disaster-tweets\" class=\"btn btn-primary\" style=\"color:white;\">Go to Post<\/a>\n      <\/div>\n    <\/div>\n  <\/div>\n    \n  <div class=\"col-sm-4\">\n    <div class=\"card\">\n      <div class=\"card-body\" style=\"width: 20rem;\">\n         <h5 class=\"card-title\"><u>Covid-19 & Google Trends<\/u><\/h5>\n         <img style='height:135px' src=\"https:\/\/miro.medium.com\/max\/821\/1*Fi6masemXJT3Q8YWekQCDQ.png\" class=\"card-img-top\" alt=\"...\"><br><br>\n         <p class=\"card-text\">Covid-19-Google Trend Analysis and Data Vizualization<\/p>\n         <a href=\"https:\/\/www.kaggle.com\/teyang\/covid-19-google-trends-auto-arima-forecasting\" class=\"btn btn-primary\" style=\"color:white;\">Go to Post<\/a>\n      <\/div>\n    <\/div>\n  <\/div>  ","24c78d45":"### Matrix Factorization in Latent Factor Models\n\nMatrix factorization is a general way of approximating a matrix by reducing its dimensions. A rating matrix *R* with shape *m* x *n* of rank *k* can be *approximately* expressed as the product of rank-*k* factors:\n\n$$R \\approx UV{^T}$$\n\nwhere<br>\n$U = m \\times k$ matrix<br>\n$V = n \\times k$ matrix<br>\n$m =$ number of users\/rows<br>\n$n =$ number of items\/games\/columns<br>\n$k =$ rank of matrix\/number of dimensions left\n\nEach column of *U* can be viewed as one of the k basis vectors of the *k*-dimensional column space of *R*, while each row is a latent factor. The same applies to the *V* matrix. The error of the approximation above is equal to $||R-UV{^T}||{^2}$, which is the sum of squares of the entries in the *residual matrix* $R-UV{^2}$. This is also known as the squared *Frobenius norm* of the residual matrix. The residual matrix represents the noise in the underlying ratings matrix, which cannot be modeled by the low-rank factors.\n\nIn the figure below, a *R* matrix with 7 users and 6 games is illustrated. The games belong to the puzzle or the adventure genre, but one game - `Minesweeper: Puzzle Adventure`, belongs to both. Users also show their preferences to game genres, with Users 1 to 3 liking puzzle games and neutral to adventure games, Users 5 to 7 liking adventure games but disliking puzzle games. User 4 likes both genres. Because of this high correlation, the matrix can be approximately factored into rank-2 factors, shown by matrix *U* and *V*. *U* provides the basis for the columns space. For example, User 5 prefers adventure and dislikes puzzle games. *V* provides the basis for the row space. For example, Sudoku belongs to the puzzle genre. \n\nMatrix *R* does not need to be fully specified. It can contain missing data. Once the latent factors *U* and *V* are estimated, the entire *R* matrix can be estimated as $UV{^T}$ in one shot. In a real scenario, values are also likely to be floats rather than integers\/whole numbers.<br>\n\n<img src=\"https:\/\/drive.google.com\/uc?export=view&id=1llCjSXDPgnp1Xmnmup6Sjtccskc3oT7V\" width=700>\n<br><br>\n\nEach column of *U* (or *V*) is referred to as a **latent *vector*** or **latent *component***, while each row of *U* (or *V*) is referred to as a **latent *factor***. The *i*th row of *U* is referred to as a ***user factor***, and it contains *k* entries corresponding to the affinity of user *i* towards the *k* concepts in the ratings matrix. Each row of *V* if referred to as an ***item factor***, which represents the affinity of item *i* towards the *k* concepts. From the figure above, one can compute the rating of user *i* for item *j* using the dot product *i*th user factor and *j*th item factor:\n\n$$r_{ij} \\approx \\overline{u{_i}} \\cdot \\overline{v{_j}}$$\n$$\\approx \\sum_{s=1}^k \\text{(Affinity of user i to concept s) x (Affinity of item j to concept s)} $$ \n\nIn the example above with 2 game genres, it will be:\n\n$$r{_i}{_j} \\approx \\text{(Affinity of user i to puzzle) x (Affinity of item j to puzzle)}\n+ \\text{(Affinity of user i to adventure) x (Affinity of item j to adventure)}$$ \n\nThe key differences between various types of matrix factorization methods arise from their **constraints imposed** on *U* and *V* (e.g., orthogonality or non-negativity of the latent vectors) and the **nature of the objective function** (e.g., minimizing the Frobenius norm or maximizing the likelikhood estimation)\n\nOne **objective\/cost function** is to minimize the Frobenius norm (sum of the squares of the matrix entries) of the residual matrix $(R - UV{^T})$, which is quite similar to the concept of **Root Mean Squared Error**:\n\n$$\\text{Minimize} \\: J = \\frac{1}{2}||R-UV{^T}||{^2}$$\n\n\nSince the predicted (*i*, *j*)th entry of the matrix *R* is given by:\n\n$$\\hat r{_i}{_j} = \\sum_{s=1}^k u_{is} \\cdot v_{is}$$\n\nThe modified cost function is given by:\n\n$$\\text{Minimize} \\: J = \\frac{1}{2} \\sum_{(i,j)\\,\\in S} e^{2}_{ij}=\\frac{1}{2} \\sum_{(i,j)\\,\\in S}\\left(r_{ij}-\\sum_{s=1}^k u_{is} \\cdot v_{is}\\right)^{2}$$\n\nwhere <br>\n$S = \\{(i,j):r_{ij} \\:\\text{is observed}\\}$ \u2014 only the observed ratings in *R* are used for minimizing the cost function<br>\n$e^{2}_{ij} = $ squared error for *i*th row and *j*th column of residual matrix  \n\n<br><br>\nSource: [Recommender Systems: The Textbook](https:\/\/www.springer.com\/gp\/book\/9783319296579) by C.C.Aggarwal","73b6019c":"Notice that the games with the most reviews after cleaning are not the most popular. This is because many of the popular games were only voted by people who rated <5 reviews. After removing these reviews, these popular games no longer have very high number of reviews. This also reflects a flaw in the scraping process, as only a max of 10,000 reviews were obtained for each game. This means that some of the reviewers who were removed might actually have voted for > 5 reviews but were not downloaded.","16f227de":"<img src=\"https:\/\/drive.google.com\/uc?export=view&id=1IFNpuu-x29wXDnz9vxovdnaEREUJfVPW\" width=900>","795f1df1":"### Remove non-useful games, clean & normalize\n\nHere we do some cleaning, and edit some of the features for the games such as the `release year`, and convert some features like `containsAds` and `editorsChoice` to boolean and reduced their weights. This was done because if they were set to [1,0], the 1s will actually account for higher weight and will actually drive the similarity values. We do not want this as they are unlikely to be major features that contribute to successful recommendations. We also normalize and reduce the weights for some of the numeric features, such as `release year`, `price' and 'minInstalls`.","2c5a09b4":"Becuase we have ~ 435k unique users and ~6.8k games, creating a cosine similarity matrix of every user with every game will yield one with a size of ~435,000 * 6,800, which takes up a lot of space. To get around this issue, there are a few things we can do to improve the space complexity issue:\n\n1. Changing the game vectors\/features from `float64` to `float32`. This will reduce the accuracy but this will be very small, compared to the reduction in twice the memory space\n2. Partition the 435k users into chunks (100 rows per chunk), compute the matrix with the 6.8k game_features, and extract the needed results (index of games), without storing the entire 435,000 * 6,800 matrix. So each time, only 100 * 6,800 matrix is stored in memory\n3. Because only the top K recommendations are needed, we just need to extract the indices of the top K games. The cosine similarity matrix is actually not needed\n\nBelow is a function for doing this 3 steps.","1fde7162":"Gensim creates a unique id for each word in the document. The produced corpus shown above is a mapping of (word_id, word_frequency). This is used as the input by the LDA model. To see the corresponding word of a given ID, pass the id as a key into the dictionary: `id2word[id]`.","a9a296bd":"In **model-based filtering**, a model is trained in advance and used for prediction later. They are very similar to traditional machine learning classification and regression models such as decision trees, support vector machines, regression and neural networks. They key difference is that instead of having a target\/dependent variable in a specific column in the case of classification and regression, they are spread out over all columns in the matrix completion\/collaborative filtering problem. <br>\nThe state-of-the art models in recommender systems are known as **latent factor** models. They leverage dimensionality reduction techniques to estimate the predicted rating matrix. In most rating matrices, the rows and columns have significant correlation, and these redundancies can be removed, such that they can often be approximated by a low-rank matrix, which provides a robust estimation of missing entries.\n\nTo get an intuition of latent factor models, imagine we have 3 movies (*Nero, Gladiator, Spartacus*), whose ratings are very positively correlated to each other. Because of the high correlation, their 3-dimensional scatterplot can be arranged along a 1-dimensional line (Figure below). This means that the matrix has a rank of approximately 1 after removing noisy variations, and the rank-1 approximation will the the 1-dimensional line (*latent vector*) that passes through the center of the scatterplot data. When a matrix has a rank of *p* (1 in this case), it means that the data can be approximately represented on a *p*-dimensional hyperplane. Thus, missing values can be robustly estimated with as few as *p* values. In the example, one can see that with a value of 0.5 for Spartacus, one can approximate the rating values of Gladiator and Nero using the intersection of the 0.5 hyperplane for Spartacus and the latent vector.\n\n<img src=\"https:\/\/drive.google.com\/uc?export=view&id=1vFr0l0RWTdr9_s6vWTMe9BDMlC3VjOa6\" width=400>\n<br><br>\n\nSource: [Recommender Systems: The Textbook](https:\/\/www.springer.com\/gp\/book\/9783319296579) by C.C.Aggarwal","3d3feef8":"After filtering out users with < 5 reviews, the long tail plot shows that the majority of the users only voted for a few games, which means the user ratings matrix will be very sparse.","af47d286":"Write here why RMSE or MAE","42b64a85":"## Resources\n\nhttps:\/\/www.kaggle.com\/ibtesama\/getting-started-with-a-movie-recommendation-system\n\nhttps:\/\/towardsdatascience.com\/an-overview-of-several-recommendation-systems-f9f8afbf00ea#:~:text=Content%2Dbased%20filtering%20approaches%20leverage,interacted%20to%20recommend%20similar%20items.&text=For%20textual%20items%2C%20like%20articles,item%20profiles%20and%20user%20profiles.","e625f85f":"Some users have fake reviews that are made up of a similar sentence structure and syntax. Most likely to be bots or automated reviews. They are removed.","34689094":"https:\/\/towardsdatascience.com\/if-you-cant-measure-it-you-can-t-improve-it-5c059014faad","331bda23":"<a id='Recommend'><\/a>\n## 4.5. Recommending Based on Game Content Similarity\n \nTo recommend games that are similar to a particular game, we can use cosine similarity. It is a metric used to measure how similar 2 items are. Mathematically, it measures the cosine of the angle between 2 non-zero vectors of an inner product space. It is concerned with orientation, rather than magnitude. The output ranges from 0-1, with 0 representing no similarity and 1 having perfect identical vectors. \n \n$$Cosine\\:Similarity = cos(\\theta) = \\frac{A \\cdot B}{||A||\\,||B||}$$\n\n<img src=\"https:\/\/drive.google.com\/uc?export=view&id=1kzyseWVEA1Qj_ZSxsYlvJNZvN4yi6RdN\" width=700>\n<br>\n\nBy calculating each pairwise cosine similarity for all the game vectors, a cosine similarity matrix is obtained. Therefore, getting the top N similar games for a particular game just requires sorting the cosine similarity values for that particular game and slicing the first N values. \n\n<br><br><br>\n \n\n### Check for missing values"}}