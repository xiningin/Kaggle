{"cell_type":{"e0ca7055":"code","b230f22e":"code","690c98d8":"code","3f4e2b00":"code","51d2e876":"code","84bcd26d":"code","d158c479":"code","f562a1c2":"code","81336d7c":"code","21f9f094":"code","57f3f1ae":"code","7304ff86":"code","a70aed16":"code","f214f510":"code","f5fcde2d":"code","aee29590":"code","74794530":"code","d81b5595":"code","ff8d4d6f":"code","406e6708":"code","30800a0e":"code","32e8467f":"code","56e0cac0":"code","c59580b4":"code","b18c21e8":"code","721ac44e":"code","389f259a":"code","203fa483":"code","1e61fe9c":"code","27412c32":"code","51a70120":"code","37a60dfd":"code","5f04f48b":"code","38021ee9":"code","ce012816":"code","0fb8af61":"markdown","79771707":"markdown","a226c5c5":"markdown","f6b2a2e6":"markdown","1f3bb924":"markdown","7de428e1":"markdown","6116c9ff":"markdown","3b7c464c":"markdown","0c6a3e74":"markdown","0363b955":"markdown","404aa4a6":"markdown","09dea335":"markdown","c3fbd90a":"markdown","2193b164":"markdown","3711e989":"markdown","52269ef0":"markdown","40f2afcd":"markdown","e85a6ad7":"markdown","97e581ec":"markdown","31f504a7":"markdown","4ed7502f":"markdown","6a17898b":"markdown","1235860d":"markdown","8babcd38":"markdown"},"source":{"e0ca7055":"#Import Packages\nimport time\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom scipy.sparse import csr_matrix, hstack\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\nfrom sklearn.preprocessing import LabelBinarizer\n\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.model_selection import KFold, cross_val_score, train_test_split\n\nfrom sklearn.linear_model import Ridge\nimport os\nprint(os.listdir(\"..\/input\"))\n\n","b230f22e":"# Observe the training set\ntrain = pd.read_table('..\/input\/train.tsv')\ntrain.head()\n","690c98d8":"print(\"The size of the training data is: \" + str(train.shape))\nprint(train.dtypes)","3f4e2b00":"train.astype('object').describe().transpose()","51d2e876":"# Observe test set\ntest = pd.read_table('..\/input\/test.tsv')\ntest.head()\n","84bcd26d":"test.shape","d158c479":"# Get 10% of the Training Data\nreduced_X_train = train.sample(frac=0.1).reset_index(drop=True)\nreduced_y_train = np.log1p(reduced_X_train['price'])","f562a1c2":"# Fast Cleaning of Data\nreduced_X_train['category_name'] = reduced_X_train['category_name'].fillna('Other').astype(str)\nreduced_X_train['brand_name'] = reduced_X_train['brand_name'].fillna('missing').astype(str)\nreduced_X_train['shipping'] = reduced_X_train['shipping'].astype(str)\nreduced_X_train['item_condition_id'] = reduced_X_train['item_condition_id'].astype(str)\nreduced_X_train['item_description'] = reduced_X_train['item_description'].fillna('None')\n","81336d7c":"%%time\nfrom sklearn.decomposition import LatentDirichletAllocation\n\n# Initialize CountVectorizer\ncvectorizer = CountVectorizer(max_features=20000,\n                              stop_words='english', \n                              lowercase=True)\n\n# Fit it to our dataset\ncvz = cvectorizer.fit_transform(reduced_X_train['item_description'])\n\n# Initialize LDA Model with 10 Topics\nlda_model = LatentDirichletAllocation(n_topics=10,\n                                      random_state=42)\n\n# Fit it to our CountVectorizer Transformation\nX_topics = lda_model.fit_transform(cvz)\n\n# Define variables\nn_top_words = 10\ntopic_summaries = []\n\n# Get the topic words\ntopic_word = lda_model.components_\n# Get the vocabulary from the text features\nvocab = cvectorizer.get_feature_names()\n\n# Display the Topic Models\nfor i, topic_dist in enumerate(topic_word):\n    topic_words = np.array(vocab)[np.argsort(topic_dist)][:-(n_top_words+1):-1]\n    topic_summaries.append(' '.join(topic_words))\n    print('Topic {}: {}'.format(i, ' | '.join(topic_words)))","21f9f094":"# Definte RMSLE Cross Validation Function\ndef rmsle_cv(model):\n    kf = KFold(shuffle=True, random_state=42).get_n_splits(reduced_X_train['item_description'])\n    rmse= np.sqrt(-cross_val_score(model, reduced_X_train['item_description'], reduced_y_train, scoring=\"neg_mean_squared_error\", cv = kf))\n    return(rmse.mean())","57f3f1ae":"from sklearn.linear_model import Ridge\n\nvec = CountVectorizer()\nclf = Ridge(random_state=42)\npipe = make_pipeline(vec, clf)\npipe.fit(reduced_X_train['item_description'], reduced_y_train)\n\ncv_rmsle = rmsle_cv(pipe)\n\nprint(\"The Validation Score is: \" + str(cv_rmsle))","7304ff86":"import eli5\neli5.show_weights(pipe, vec=vec, top=100, feature_filter=lambda x: x != '<BIAS>')","a70aed16":"eli5.show_prediction(clf, doc=reduced_X_train['item_description'][1297], vec=vec)","f214f510":"vec = CountVectorizer(stop_words='english')\nclf = Ridge(random_state=42)\npipe = make_pipeline(vec, clf)\npipe.fit(reduced_X_train['item_description'], reduced_y_train)\n\ncv_sw_rmsle = rmsle_cv(pipe)\n\nprint(\"The Validation Score is: \" + str(cv_sw_rmsle))","f5fcde2d":"eli5.show_prediction(clf, doc=reduced_X_train['item_description'][1297], vec=vec)","aee29590":"vec = TfidfVectorizer()\nclf = Ridge(random_state=42)\npipe = make_pipeline(vec, clf)\npipe.fit(reduced_X_train['item_description'], reduced_y_train)\n\ntfidf_rmsle = rmsle_cv(pipe)\n\nprint(\"The Validation Score is: \" + str(tfidf_rmsle))","74794530":"eli5.show_prediction(clf, doc=reduced_X_train['item_description'][1297], vec=vec)","d81b5595":"vec = TfidfVectorizer(stop_words='english')\nclf = Ridge(random_state=42)\npipe = make_pipeline(vec, clf)\npipe.fit(reduced_X_train['item_description'], reduced_y_train)\n\ntfidf_sw_rmsle = rmsle_cv(pipe)\n\nprint(\"The Validation Score is: \" + str(tfidf_sw_rmsle))","ff8d4d6f":"eli5.show_prediction(clf, doc=reduced_X_train['item_description'][1297], vec=vec)","406e6708":"vec = TfidfVectorizer(stop_words='english', ngram_range=(1,2))\nclf = Ridge(random_state=42)\npipe = make_pipeline(vec, clf)\npipe.fit(reduced_X_train['item_description'], reduced_y_train)\n\ntfidf_sw_ng_rmsle = rmsle_cv(pipe)\n\nprint(\"The Validation Score is: \" + str(tfidf_sw_ng_rmsle))","30800a0e":"eli5.show_prediction(clf, doc=reduced_X_train['item_description'][1297], vec=vec)","32e8467f":"print (\"RMSLE Score: \" + str(cv_rmsle) + \" | CountVectorizer\")\nprint (\"RMSLE Score: \" + str(cv_sw_rmsle) + \" | CountVectorizer | Stop Words\")\nprint (\"RMSLE Score: \" + str(tfidf_rmsle) + \" | TF-IDF\")\nprint (\"RMSLE Score: \" + str(tfidf_sw_rmsle) + \" | TF-IDF | Stop Words\")\nprint (\"RMSLE Score: \" + str(tfidf_sw_ng_rmsle) + \" | TF-IDF | Stop Words | N-Grams\")","56e0cac0":"from sklearn.pipeline import FeatureUnion\n\ndefault_preprocessor = CountVectorizer().build_preprocessor()\n\ndef build_preprocessor(field):\n    field_idx = list(reduced_X_train.columns).index(field)\n    return lambda x: default_preprocessor(x[field_idx])\n\nvectorizer = FeatureUnion([\n    ('name', CountVectorizer(\n        ngram_range=(1, 2),\n        max_features=50000,\n        preprocessor=build_preprocessor('name'))),\n    ('category_name', CountVectorizer(\n        token_pattern='.+',\n        preprocessor=build_preprocessor('category_name'))),\n    ('brand_name', CountVectorizer(\n        token_pattern='.+',\n        preprocessor=build_preprocessor('brand_name'))),\n    ('shipping', CountVectorizer(\n        token_pattern='\\d+',\n        preprocessor=build_preprocessor('shipping'))),\n    ('item_condition_id', CountVectorizer(\n        token_pattern='\\d+',\n        preprocessor=build_preprocessor('item_condition_id'))),\n    ('item_description', TfidfVectorizer(\n        ngram_range=(1, 2),\n        max_features=55000,\n        stop_words='english',\n        preprocessor=build_preprocessor('item_description'))),\n])","c59580b4":"# Create Transformed Train Set\nreduced_Xt_train = vectorizer.fit_transform(reduced_X_train.values)\n","b18c21e8":"def get_rmsle(y, pred): return np.sqrt(mean_squared_error(y, pred))","721ac44e":"%%time\n\n# Create 3-Fold CV\ncv = KFold(n_splits=3, shuffle=True, random_state=42)\nfor train_ids, valid_ids in cv.split(reduced_Xt_train):\n    # Define LGBM Model\n    model_ridge = Ridge(solver = \"lsqr\", fit_intercept=True, random_state=42)\n    \n    # Fit LGBM Model\n    model_ridge.fit(reduced_Xt_train[train_ids], reduced_y_train[train_ids])\n    \n    # Predict & Evaluate Training Score\n    y_pred_train = model_ridge.predict(reduced_Xt_train[train_ids])\n    rmsle_train = get_rmsle(y_pred_train, reduced_y_train[train_ids])\n    \n    # Predict & Evaluate Validation Score\n    y_pred_valid = model_ridge.predict(reduced_Xt_train[valid_ids])\n    rmsle_valid = get_rmsle(y_pred_valid, reduced_y_train[valid_ids])\n    \n    print(f'LGBM Training RMSLE: {rmsle_train:.5f}')\n    print(f'LGBM Validation RMSLE: {rmsle_valid:.5f}')","389f259a":"%%time\nfrom sklearn.linear_model import Lasso\n\n# Create 3-Fold CV\ncv = KFold(n_splits=3, shuffle=True, random_state=42)\nfor train_ids, valid_ids in cv.split(reduced_Xt_train):\n    # Define LGBM Model\n    model_LASSO = Lasso(fit_intercept=True, random_state=42)\n    \n    # Fit LGBM Model\n    model_LASSO.fit(reduced_Xt_train[train_ids], reduced_y_train[train_ids])\n    \n    # Predict & Evaluate Training Score\n    y_pred_train = model_LASSO.predict(reduced_Xt_train[train_ids])\n    rmsle_train = get_rmsle(y_pred_train, reduced_y_train[train_ids])\n    \n    # Predict & Evaluate Validation Score\n    y_pred_valid = model_LASSO.predict(reduced_Xt_train[valid_ids])\n    rmsle_valid = get_rmsle(y_pred_valid, reduced_y_train[valid_ids])\n    \n    print(f'LASSO Training RMSLE: {rmsle_train:.5f}')\n    print(f'LASSO Validation RMSLE: {rmsle_valid:.5f}')","203fa483":"%%time\nimport lightgbm as lgb\n\n# Create 3-Fold CV\ncv = KFold(n_splits=3, shuffle=True, random_state=42)\nfor train_ids, valid_ids in cv.split(reduced_Xt_train):\n    # Define LGBM Model\n    model_lgb = lgb.LGBMRegressor(num_leaves=31, n_jobs=-1, learning_rate=0.1, n_estimators=500, random_state=42)\n    \n    # Fit LGBM Model\n    model_lgb.fit(reduced_Xt_train[train_ids], reduced_y_train[train_ids])\n    \n    # Predict & Evaluate Training Score\n    y_pred_train = model_lgb.predict(reduced_Xt_train[train_ids])\n    rmsle_train = get_rmsle(y_pred_train, reduced_y_train[train_ids])\n    \n    # Predict & Evaluate Validation Score\n    y_pred_valid = model_lgb.predict(reduced_Xt_train[valid_ids])\n    rmsle_valid = get_rmsle(y_pred_valid, reduced_y_train[valid_ids])\n    \n    print(f'LGBM Training RMSLE: {rmsle_train:.5f}')\n    print(f'LGBM Validation RMSLE: {rmsle_valid:.5f}')","1e61fe9c":"#Create Train\/Test Split\ntrain_X, test_X, train_y, test_y = train_test_split(reduced_Xt_train, reduced_y_train, test_size=0.2, random_state=211)","27412c32":"# Define LGBM Model\nmodel_lgb = lgb.LGBMRegressor(num_leaves=31, n_jobs=-1, learning_rate=0.1, n_estimators=500, random_state=40)\n\n# Fit LGBM Model\nmodel_lgb.fit(train_X, train_y)\n\n# Predict with LGBM Model\nlgbm_y_pred = model_lgb.predict(test_X)","51a70120":"# Define Ridge Model\nmodel_ridge = Ridge(solver = \"lsqr\", fit_intercept=True, random_state=42)\n    \n# Fit Ridge Model\nmodel_ridge.fit(train_X, train_y)\n    \n# Evaluate Training Score\nridge_y_pred = model_ridge.predict(test_X)","37a60dfd":"ensemble_y_pred = (lgbm_y_pred+ridge_y_pred)\/2\n\nensemble_rmsle = get_rmsle(ensemble_y_pred, test_y)\n\nprint(f'Ensemble RMSLE: {ensemble_rmsle:.5f}')","5f04f48b":"#Ensemble Predictions without Inverse Log Transformation\nensemble_y_pred[0:20]","38021ee9":"# Ensemble Predictions (Inverse Log - Exponential)\nensemble_y = (np.expm1(lgbm_y_pred)+np.expm1(ridge_y_pred))\/2\nensemble_y[200:220]","ce012816":"# Test Predictions \nnp.expm1(test_y[200:220])","0fb8af61":"**Baseline Model with TF-IDF**","79771707":"**Baseline Model with TF-IDF and Stop Words**","a226c5c5":"**Create Transformed Training Set**","f6b2a2e6":"**Eli5**\n\nLooking at features helps to understand how classifier works. Maybe even more importantly, it helps to notice preprocessing bugs, data leaks, issues with task specification - all these nasty problems you get in a real world.\n\n**How does Eli5 Work?**\nIt shows you the correlation of each feature\/text with the target variable. We can inspect features and weights because we\u2019re using a bag-of-words vectorizer and a linear classifier (so there is a direct mapping between individual words and classifier coefficients).","1f3bb924":"LASSO Cross Validation\n\n\nOne reason why LASSO Perform way worse than Ridge could be because since LASSO performs automatic feature selection. So keep in mind majority of our features are just words. It'll remove some of our text features. And this may not generalize well with new data. Because our dataset is suppose to capture and use all our words as features.","7de428e1":"**Baseline Model with CountVectorizer**","6116c9ff":"**LGBM Cross Validation**","3b7c464c":"**Analyzing Item Description with Eli5**","0c6a3e74":"**RMSLE Summary**","0363b955":"**Feature Pre-Processing \/ Transformation**","404aa4a6":"**Ensemble (Ridge + LGBM)**","09dea335":"**Modeling**\n\n* Ridge Regression\n* LASSO Regression\n* Light GBM","c3fbd90a":"**LGBM Model**","2193b164":"**Mercari Price Suggestion**\n\nThe objective is to come up with the right pricing algorithm that can we can use as a pricing recommendation to the users.\nSince its a regression problem, the evaluation metric that should be used is RMSE (Root Mean Squared Error). But in this case for the competition, we'll be using the RMSLE; which puts less penalty on large errors and focuses more on the smaller errors (since our main distribution in price is centered at around $10)","3711e989":"**Ridge Cross Validation**","52269ef0":"**Data Cleaning**","40f2afcd":"**Predictions**","e85a6ad7":"**Baseline Model with CountVectorizer and Stop Words**","97e581ec":"**Ensemble Model**","31f504a7":"**Ridge Model**","4ed7502f":"Topic Modeling\n\nIts foundations are Probabilistic Graphical Models\n\nHow does LDA Work?\n\nIt is an iterative algorithm\n* In the initialization stage, each word is assigned to a random topic.\n* Iteratively, the algorithm goes through each word and reassigns the word to a topic taking into consideration: What\u2019s the probability of the word belonging to a topic and What\u2019s the probability of the document to be generated by a topic\n    ","6a17898b":"**Define RMSLE Function**","1235860d":"**Summary Statistics**","8babcd38":"**Baseline Model with TF-IDF, Stop Words, and N-Grams**"}}