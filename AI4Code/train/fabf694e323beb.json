{"cell_type":{"ec9f960d":"code","f8c23530":"code","4bf54ac9":"code","fd05d3b3":"code","49d5c505":"code","75f17822":"code","33774917":"code","5f3cd17f":"code","189a05d2":"code","598a235c":"code","a7803147":"code","fdfad6f4":"code","92f1ad48":"code","0bc5e870":"code","0c29606c":"code","e138e3b7":"code","ca0b6e97":"code","2b085783":"code","2bdaae3e":"code","67e9c5f2":"code","b6d15f3c":"code","121e20ca":"code","8e5b4144":"code","3527ab89":"markdown","b44dec1b":"markdown"},"source":{"ec9f960d":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n%matplotlib inline\nimport matplotlib.pyplot as plt  # Matlab-style plotting\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\ncolor = sns.color_palette()\nsns.set_style('darkgrid')\nimport warnings\ndef ignore_warn(*args, **kwargs):\n    pass\nwarnings.warn = ignore_warn #ignore annoying warning (from sklearn and seaborn)\n\n\nfrom scipy import stats\nfrom scipy.stats import norm, skew #for some statistics\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","f8c23530":"train=pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/train.csv',index_col='Id')\ntest=pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/test.csv',index_col='Id')","4bf54ac9":"train.head()","fd05d3b3":"test.head()","49d5c505":"train.columns","75f17822":"#histogram\nsns.distplot(train['SalePrice']);","33774917":"#skewness and kurtosis\nprint(\"Skewness: %f\" % train['SalePrice'].skew())\nprint(\"Kurtosis: %f\" % train['SalePrice'].kurt())\n","5f3cd17f":"fig, ax = plt.subplots()\nax.scatter(x = train['GrLivArea'], y = train['SalePrice'])\nplt.ylabel('SalePrice', fontsize=13)\nplt.xlabel('GrLivArea', fontsize=13)\nplt.show()\n","189a05d2":"#Deleting outliers\ntrain = train.drop(train[(train['GrLivArea']>4000) & (train['SalePrice']<300000)].index)\n\n#Check the graphic again\nfig, ax = plt.subplots()\nax.scatter(train['GrLivArea'], train['SalePrice'])\nplt.ylabel('SalePrice', fontsize=13)\nplt.xlabel('GrLivArea', fontsize=13)\nplt.show()","598a235c":"#correlation matrix\ncorrmat = train.corr()\nf, ax = plt.subplots(figsize=(12, 9))\nsns.heatmap(corrmat, vmax=.8, square=True);","a7803147":"#saleprice correlation matrix\nk = 10 #number of variables for heatmap\ncols = corrmat.nlargest(k, 'SalePrice')['SalePrice'].index\ncm = np.corrcoef(train[cols].values.T)\nsns.set(font_scale=1.25)\nhm = sns.heatmap(cm, cbar=True, annot=True, square=True, fmt='.2f', annot_kws={'size': 10}, yticklabels=cols.values, xticklabels=cols.values)\nplt.show()","fdfad6f4":"#scatterplot\nsns.set()\ncols = ['SalePrice', 'OverallQual', 'GrLivArea', 'GarageCars', 'TotalBsmtSF', 'FullBath', 'YearBuilt']\nsns.pairplot(train[cols], size = 2.5)\nplt.show()","92f1ad48":"#missing data\ntotal = train.isnull().sum().sort_values(ascending=False)\npercent = (train.isnull().sum()\/train.isnull().count()).sort_values(ascending=False)\nmissing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\nmissing_data.head(20)","0bc5e870":"#dealing with missing data\ntrain = train.drop((missing_data[missing_data['Total'] > 1]).index,1)\ntrain = train.drop(train.loc[train['Electrical'].isnull()].index)\ntrain.isnull().sum().max() #just chcking that there's no missing data missing...","0c29606c":"#histogram and normal probability plot\nsns.distplot(train['SalePrice'], fit=norm);\nfig = plt.figure()\nres = stats.probplot(train['SalePrice'], plot=plt)","e138e3b7":"#applying log transformation\ntrain['SalePrice'] = np.log(train['SalePrice'])\n#transformed histogram and normal probability plot\nsns.distplot(train['SalePrice'], fit=norm);\nfig = plt.figure()\nres = stats.probplot(train['SalePrice'], plot=plt)","ca0b6e97":"train.dropna(axis=0, subset=['SalePrice'], inplace=True)\ny = train.SalePrice\ntrain.drop(['SalePrice'], axis=1, inplace=True)\n\n# Break off validation set from training data\nX_train, X_valid, y_train, y_valid = train_test_split(train, y, train_size=0.8, test_size=0.2,\n                                                      random_state=0)\n\n# \"Cardinality\" means the number of unique values in a column\n# Select categorical columns with relatively low cardinality (convenient but arbitrary)\ncategorical_cols = [cname for cname in X_train.columns if\n                    X_train[cname].nunique() < 10 and \n                    X_train[cname].dtype == \"object\"]\n\n# Select numerical columns\nnumerical_cols = [cname for cname in X_train.columns if \n                X_train[cname].dtype in ['int64', 'float64']]\n","2b085783":"#Keep selected columns only\nmy_cols = categorical_cols + numerical_cols\nX_train = X_train[my_cols].copy()\nX_valid = X_valid[my_cols].copy()\nX_test = test[my_cols].copy()\nX_test.head()","2bdaae3e":"def mean_absolute_percentage_error(y_true, y_pred): \n    y_true, y_pred = np.array(y_true), np.array(y_pred)\n    return np.mean(np.abs((y_true - y_pred) \/ y_true)) * 100","67e9c5f2":"from sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_absolute_error\n\n# Preprocessing for numerical data\nnumerical_transformer = SimpleImputer(strategy='constant')\n\n# Preprocessing for categorical data\ncategorical_transformer = Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='most_frequent')),\n    ('onehot', OneHotEncoder(handle_unknown='ignore'))])\n\n# Bundle preprocessing for numerical and categorical data\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('num', numerical_transformer, numerical_cols),\n        ('cat', categorical_transformer, categorical_cols)\n    ])\n# Define model\nmodel = RandomForestRegressor(n_estimators=100, random_state=0)\n\n# Bundle preprocessing and modeling code in a pipeline\nclf = Pipeline(steps=[('preprocessor', preprocessor),\n                      ('model', model)\n                     ])\n\n# Preprocessing of training data, fit model \nclf.fit(X_train, y_train)\n\n# Preprocessing of validation data, get predictions\npreds = clf.predict(X_valid)\n\nprint('MAPE:', mean_absolute_percentage_error(y_valid, preds)) ","b6d15f3c":"from xgboost import XGBRegressor\n\nmy_model = XGBRegressor(n_estimators=1500 ,learning_rate=0.01)\n\nclf1 = Pipeline(steps=[('preprocessor', preprocessor),\n                      ('model', my_model)\n                     ])\nclf1.fit(X_train, y_train)\n# Preprocessing of validation data, get predictions\npreds1 = clf1.predict(X_valid)\n\nprint('MAPE:', mean_absolute_percentage_error(y_valid, preds1)) ","121e20ca":"# Preprocessing of test data, fit model\npreds_test = clf1.predict(X_test)","8e5b4144":"# Save test predictions to file\noutput = pd.DataFrame({'Id': X_test.index,\n                       'SalePrice': np.exp(preds_test)})\noutput.to_csv('submission.csv', index=False)","3527ab89":"# Break off validation set from training data\n\n","b44dec1b":"# Dealing with Missing and Categorial Data\n"}}