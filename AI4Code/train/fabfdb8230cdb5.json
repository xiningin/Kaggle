{"cell_type":{"0ed9d152":"code","8f36a88c":"code","b2711261":"code","e9f21bc2":"code","3bb20687":"code","86d96fbf":"code","d00f0b14":"code","c55903bc":"code","efac3df8":"code","ec768b3e":"code","29172c79":"code","21d619b2":"code","6914bd97":"code","79e68033":"code","57f6e71b":"code","d1812888":"code","8513d7c1":"code","20a71d86":"code","91e94dad":"code","9e483b0f":"code","10245a12":"code","c5852ba3":"code","36081a13":"code","116fda26":"code","0da146e2":"markdown","24e22e47":"markdown","2ff1523e":"markdown","d0438445":"markdown","68fa84db":"markdown","0b7080d0":"markdown","517103e3":"markdown","d779dd3b":"markdown","54b92e05":"markdown","e6decf2d":"markdown","ca39c2ba":"markdown","a40372e2":"markdown","20a22d05":"markdown","fd428fb3":"markdown","2befc6bd":"markdown","869c5f0e":"markdown","304b1414":"markdown"},"source":{"0ed9d152":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport cv2\nimport tensorflow as tf\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\n#for dirname, _, filenames in os.walk('\/kaggle\/input'):\n#    for filename in filenames:\n#        print(os.path.join(dirname, filename))\n\ndata_dir= \"..\/input\/notebookb-skin-mnist-data-preparation\/skin\"\nlist_dir = os.listdir(data_dir)\n","8f36a88c":"batch_size = 32\nimg_height = 224\nimg_width = 224\n","b2711261":"train_ds = tf.keras.preprocessing.image_dataset_from_directory(\n  data_dir,\n  validation_split=0.2,\n  subset=\"training\",\n  seed=123,\n  image_size=(img_height, img_width),\n  batch_size=batch_size)","e9f21bc2":"val_ds = tf.keras.preprocessing.image_dataset_from_directory(\n  data_dir,\n  validation_split=0.2,\n  subset=\"validation\",\n  seed=123,\n  image_size=(img_height, img_width),\n  batch_size=batch_size)","3bb20687":"class_names = train_ds.class_names\nprint(class_names)\n\n","86d96fbf":"import matplotlib.pyplot as plt\n\nplt.figure(figsize=(10, 10))\nfor images, labels in train_ds.take(1):\n    for i in range(9):\n        ax = plt.subplot(3, 3, i + 1)\n        plt.imshow(images[i].numpy().astype(\"uint8\"))\n        plt.title(class_names[labels[i]])\n        plt.axis(\"off\")","d00f0b14":"AUTOTUNE = tf.data.AUTOTUNE\n\ntrain_ds = train_ds.cache().shuffle(1000).prefetch(buffer_size=AUTOTUNE)\nval_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)","c55903bc":"#  This model expects pixel vaues in [-1,1], but at this point, \n# the pixel values in your images are in [0-255]. \n# To rescale them, use the preprocessing method included with the model\npreprocess_input = tf.keras.applications.mobilenet_v2.preprocess_input","efac3df8":"IMG_SIZE = (224, 224)\nIMG_SHAPE = IMG_SIZE + (3,)\n\nbase_model = tf.keras.applications.MobileNetV2(input_shape=IMG_SHAPE,\n                                               include_top=False,\n                                               weights='imagenet')\n\n\n\n# Let's take a look at the base model architecture\n# base_model.summary()\n","ec768b3e":"base_model.trainable = False","29172c79":"image_batch, label_batch = next(iter(train_ds))\nfeature_batch = base_model(image_batch)\nprint(feature_batch.shape)","21d619b2":"flatten_layer = tf.keras.layers.Flatten()\nflatten_batch = flatten_layer(feature_batch)\n\ndense_layer = tf.keras.layers.Dense(512, activation='relu')\ndense_batch = dense_layer(flatten_batch)\n\nprediction_layer = tf.keras.layers.Dense(7, activation='softmax')\nprediction_batch = prediction_layer(dense_batch)\n","6914bd97":"# Create a model that includes the augmentation stage\ninput_shape = (224, 224, 3)\nclasses = 7\ninputs = tf.keras.Input(shape=input_shape)\nx = preprocess_input(inputs)\nx = base_model(x, training=False)\nx = flatten_layer(x)\nx = dense_layer(x)\n#x = tf.keras.layers.Dropout(0.2)(x)\noutputs = prediction_layer(x)\nmodel = tf.keras.Model(inputs, outputs)","79e68033":"base_learning_rate = 0.0001\nmodel.compile(optimizer=tf.keras.optimizers.Adam(lr=base_learning_rate),\n              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n              metrics=['accuracy'])","57f6e71b":"model.summary()","d1812888":"initial_epochs = 10\n\nloss0, accuracy0 = model.evaluate(val_ds)","8513d7c1":"history = model.fit(train_ds,\n                    epochs=initial_epochs,\n                    validation_data=val_ds)","20a71d86":"loss0, accuracy0 = model.evaluate(val_ds)","91e94dad":"acc = history.history['accuracy']\nval_acc = history.history['val_accuracy']\n\nloss = history.history['loss']\nval_loss = history.history['val_loss']\n\nepochs_range = range(10)\n\nplt.figure(figsize=(18, 8))\nplt.subplot(1, 2, 1)\nplt.plot(epochs_range, acc, label='Training Accuracy')\nplt.plot(epochs_range, val_acc, label='Validation Accuracy')\nplt.legend(loc='lower right')\nplt.title('Training and Validation Accuracy')\n\nplt.subplot(1, 2, 2)\nplt.plot(epochs_range, loss, label='Training Loss')\nplt.plot(epochs_range, val_loss, label='Validation Loss')\nplt.legend(loc='upper right')\nplt.title('Training and Validation Loss')\nplt.show()\n","9e483b0f":"base_model.trainable = True","10245a12":"model.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n              optimizer = tf.keras.optimizers.RMSprop(lr=base_learning_rate\/10),\n              metrics=['accuracy'])\n\nmodel.summary()","c5852ba3":"fine_tune_epochs = 10\ntotal_epochs =  initial_epochs + fine_tune_epochs\n\nhistory_fine = model.fit(train_ds,\n                         epochs=total_epochs,\n                         initial_epoch=history.epoch[-1],\n                         validation_data=val_ds)","36081a13":"acc += history_fine.history['accuracy']\nval_acc += history_fine.history['val_accuracy']\n\nloss += history_fine.history['loss']\nval_loss += history_fine.history['val_loss']\n\nplt.figure(figsize=(8, 8))\nplt.subplot(2, 1, 1)\nplt.plot(acc, label='Training Accuracy')\nplt.plot(val_acc, label='Validation Accuracy')\nplt.ylim([0.8, 1])\nplt.plot([initial_epochs-1,initial_epochs-1],\n          plt.ylim(), label='Start Fine Tuning')\nplt.legend(loc='lower right')\nplt.title('Training and Validation Accuracy')\n\nplt.subplot(2, 1, 2)\nplt.plot(loss, label='Training Loss')\nplt.plot(val_loss, label='Validation Loss')\nplt.ylim([0, 1.0])\nplt.plot([initial_epochs-1,initial_epochs-1],\n         plt.ylim(), label='Start Fine Tuning')\nplt.legend(loc='upper right')\nplt.title('Training and Validation Loss')\nplt.xlabel('epoch')\nplt.show()","116fda26":"loss, accuracy = model.evaluate(val_ds)\nprint('Val accuracy :', accuracy)","0da146e2":"Ensure we have the 7 class names loaded","24e22e47":"This feature extractor converts each 224x224x3 image into a 7x7x1280 block of features. Let's see what it does to an example batch of images:","2ff1523e":"## Compile the model\n\nAs you are training a much larger model and want to readapt the pretrained weights, it is important to use a lower learning rate at this stage. Otherwise, your model could overfit very quickly.","d0438445":"# Training\n\nStart to get an evaluation of model using validation and get the acc of a non trained model. After train the model and get new model.","68fa84db":"Load the model from data dir folder and use the 80% for training and 20% for validation","0b7080d0":"Plot a sample of data with corresponding labels","517103e3":"# Configure the dataset for performance\n\nLet's make sure to use buffered prefetching so you can yield data from disk without having I\/O become blocking. These are two important methods you should use when loading data.\n\n[Dataset.cache()](https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/data\/Dataset#cache) keeps the images in memory after they're loaded off disk during the first epoch. This will ensure the dataset does not become a bottleneck while training your model. If your dataset is too large to fit into memory, you can also use this method to create a performant on-disk cache.\n\n[Dataset.prefetch()](https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/data\/Dataset#prefetch) overlaps data preprocessing and model execution while training.\n\nInterested readers can learn more about both methods, as well as how to cache data to disk in the [data performance guide](https:\/\/www.tensorflow.org\/guide\/data_performance#prefetching).","d779dd3b":"## Fine tunning conclusion\n\nIn this case fine tunning is not working as expected, we require a more biggest dataset that not must be unbalanced and not so duplicated\/augmented images to unbalance the DS.","54b92e05":"# Create the head layers and prediction dense layer","e6decf2d":"## Continue training the model\n\nIf you trained to convergence earlier, this step will improve your accuracy by a few percentage points.","ca39c2ba":"# Prepare model","a40372e2":"Define the batch and image size we are working in our model","20a22d05":"# Freeze the convolutional base\n\nIt is important to freeze the convolutional base before you compile and train the model. Freezing (by setting layer.trainable = False) prevents the weights in a given layer from being updated during training. MobileNet V2 has many layers, so setting the entire model's trainable flag to False will freeze all of them.","fd428fb3":"# Build the model\n\nappend the preprocessing and add flatten, dense and prediction layers to base model","2befc6bd":"# Create the base model from the pre-trained convnets\n\nYou will create the base model from the MobileNet V2 model developed at Google. This is pre-trained on the ImageNet dataset, a large dataset consisting of 1.4M images and 1000 classes. ImageNet is a research training dataset with a wide variety of categories like jackfruit and syringe. This base of knowledge will help us classify cats and dogs from our specific dataset.\n\nFirst, you need to pick which layer of MobileNet V2 you will use for feature extraction. The very last classification layer (on \"top\", as most diagrams of machine learning models go from bottom to top) is not very useful. Instead, you will follow the common practice to depend on the very last layer before the flatten operation. This layer is called the \"bottleneck layer\". The bottleneck layer features retain more generality as compared to the final\/top layer.\n\nFirst, instantiate a MobileNet V2 model pre-loaded with weights trained on ImageNet. By specifying the include_top=False argument, you load a network that doesn't include the classification layers at the top, which is ideal for feature extraction.","869c5f0e":"# Fine tuning\n\nIn the feature extraction experiment, you were only training a few layers on top of an MobileNet V2 base model. The weights of the pre-trained network were not updated during training.\n\nOne way to increase performance even further is to train (or \"fine-tune\") the weights of the top layers of the pre-trained model alongside the training of the classifier you added. The training process will force the weights to be tuned from generic feature maps to features associated specifically with the dataset.","304b1414":"# Rescale pixel values\n\nIn a moment, you will download tf.keras.applications.MobileNetV2 for use as your base model. This model expects pixel values in [-1,1], but at this point, the pixel values in your images are in [0-255]. To rescale them, use the preprocessing method included with the model."}}