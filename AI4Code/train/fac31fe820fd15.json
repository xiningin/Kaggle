{"cell_type":{"78655f44":"code","a6dfad68":"code","e0fe17c4":"code","70d4288d":"code","ec7013fc":"code","d95f9101":"code","b21e338c":"code","91e2690c":"code","a23c4139":"code","c15ea381":"code","591e0e1c":"code","8a3a6c57":"code","2dc86cee":"code","d41a69de":"code","752d99cf":"code","ce91302f":"code","806c4586":"code","b08c5aff":"code","e6501e61":"code","34c72759":"code","79c2171c":"code","07662984":"code","4b64adf9":"code","8cfcde4d":"code","1a5884ce":"code","ddd1e8d6":"code","f4bc423f":"code","28deed58":"code","548ea23d":"code","d7a29d8c":"code","13fe3718":"code","a88ba2ae":"code","73d4b4ab":"code","41b45219":"code","84483290":"code","74b2821a":"code","6560987f":"code","994ae9aa":"code","f6720034":"code","decb6608":"code","9f5a8ece":"code","16592a3e":"code","d177e154":"code","ad650e1d":"code","aa97cf7f":"code","b34cd470":"code","79a66b96":"code","96521e89":"code","eefb33cf":"code","633f71ab":"code","a799acdc":"code","4ef24b59":"code","ee1760da":"code","3fa76023":"code","75782a45":"markdown","96393233":"markdown","3d24b942":"markdown","241d3d6f":"markdown","5c94c377":"markdown","db5482ac":"markdown","89d401da":"markdown","65c7b706":"markdown","33d22d8b":"markdown","c4eec0d8":"markdown","43419cbc":"markdown","360bff60":"markdown","f002fc5c":"markdown","a8fcb018":"markdown","68819407":"markdown","e4f9a664":"markdown","0a821422":"markdown","6bef9d33":"markdown","058ea739":"markdown","6c95944a":"markdown"},"source":{"78655f44":"# Importing libraries for data analysis and wrangling\nimport pandas as pd\nimport numpy as np\nimport random as rnd\n\n# Importing libraries for data visualization\nimport seaborn as sns\n%matplotlib inline\nimport matplotlib.pyplot as plt\n\n# Importing libraries for machine learning algorithms\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC, LinearSVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.linear_model import Perceptron\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.tree import DecisionTreeClassifier","a6dfad68":"train_df = pd.read_csv('..\/input\/titanic\/train.csv')\ntest_df = pd.read_csv('..\/input\/titanic\/test.csv')\ncombine = [train_df, test_df]","e0fe17c4":"# See what are the name of columns in the train dataset\nprint(train_df.columns.values)","70d4288d":"# Preview the data from top\ntrain_df.head(10)","ec7013fc":"# Preview the data from bottom\ntrain_df.tail(10)","d95f9101":"# Get a information about the dataset which we are going to use \ntrain_df.info()\nprint('_'*40)\ntest_df.info()","b21e338c":"train_df.describe()\n\"\"\" \n Review survived rate using `percentiles=[.61, .62]` knowing our problem description mentions 38% survival rate.\n Review Parch distribution using `percentiles=[.75, .8]`\n SibSp distribution `[.68, .69]`\n Age and Fare `[.1, .2, .3, .4, .5, .6, .7, .8, .9, .99]`\n \"\"\"","91e2690c":"train_df.describe(include=['O'])","a23c4139":"train_df[['Pclass', 'Survived']].groupby(['Pclass'], as_index=False).mean().sort_values(by='Survived', ascending=False)","c15ea381":"train_df[[\"Sex\", \"Survived\"]].groupby(['Sex'], as_index=False).mean().sort_values(by='Survived', ascending=False)","591e0e1c":"train_df[[\"Sex\", \"Survived\"]].groupby(['Sex'], as_index=False).mean().sort_values(by='Survived', ascending=False)","8a3a6c57":"train_df[[\"SibSp\", \"Survived\"]].groupby(['SibSp'], as_index=False).mean().sort_values(by='Survived', ascending=False)","2dc86cee":"train_df[[\"Parch\", \"Survived\"]].groupby(['Parch'], as_index=False).mean().sort_values(by='Survived', ascending=False)","d41a69de":"g = sns.FacetGrid(train_df, col='Survived')\ng.map(plt.hist, 'Age', bins=20)","752d99cf":"# grid = sns.FacetGrid(train_df, col='Pclass', hue='Survived')\ngrid = sns.FacetGrid(train_df, col='Survived', row='Pclass', size=2.2, aspect=1.6)\ngrid.map(plt.hist, 'Age', alpha=.5, bins=20)\ngrid.add_legend();","ce91302f":"# grid = sns.FacetGrid(train_df, col='Embarked')\ngrid = sns.FacetGrid(train_df, row='Embarked', size=2.2, aspect=1.6)\ngrid.map(sns.pointplot, 'Pclass', 'Survived', 'Sex', palette='deep')\ngrid.add_legend()","806c4586":"# grid = sns.FacetGrid(train_df, col='Embarked', hue='Survived', palette={0: 'k', 1: 'w'})\ngrid = sns.FacetGrid(train_df, row='Embarked', col='Survived', size=2.2, aspect=1.6)\ngrid.map(sns.barplot, 'Sex', 'Fare', alpha=.5, ci=None)\ngrid.add_legend()","b08c5aff":"print(\"Before\", train_df.shape, test_df.shape, combine[0].shape, combine[1].shape)\n\ntrain_df = train_df.drop(['Ticket', 'Cabin'], axis=1)\ntest_df = test_df.drop(['Ticket', 'Cabin'], axis=1)\ncombine = [train_df, test_df]\n\n\"After\", train_df.shape, test_df.shape, combine[0].shape, combine[1].shape","e6501e61":"for dataset in combine:\n    dataset['Title'] = dataset.Name.str.extract(' ([A-Za-z]+)\\.', expand=False)\n\npd.crosstab(train_df['Title'], train_df['Sex'])","34c72759":"for dataset in combine:\n    dataset['Title'] = dataset['Title'].replace(['Lady', 'Countess','Capt', 'Col',\\\n \t'Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare')\n\n    dataset['Title'] = dataset['Title'].replace('Mlle', 'Miss')\n    dataset['Title'] = dataset['Title'].replace('Ms', 'Miss')\n    dataset['Title'] = dataset['Title'].replace('Mme', 'Mrs')\n    \ntrain_df[['Title', 'Survived']].groupby(['Title'], as_index=False).mean()","79c2171c":"title_mapping = {\"Mr\": 1, \"Miss\": 2, \"Mrs\": 3, \"Master\": 4, \"Rare\": 5}\nfor dataset in combine:\n    dataset['Title'] = dataset['Title'].map(title_mapping)\n    dataset['Title'] = dataset['Title'].fillna(0)\n\ntrain_df.head()","07662984":"train_df = train_df.drop(['Name', 'PassengerId'], axis=1)\ntest_df = test_df.drop(['Name'], axis=1)\ncombine = [train_df, test_df]\ntrain_df.shape, test_df.shape","4b64adf9":"for dataset in combine:\n    dataset['Sex'] = dataset['Sex'].map( {'female': 1, 'male': 0} ).astype(int)\n\ntrain_df.head()","8cfcde4d":"# grid = sns.FacetGrid(train_df, col='Pclass', hue='Gender')\ngrid = sns.FacetGrid(train_df, row='Pclass', col='Sex', size=2.2, aspect=1.6)\ngrid.map(plt.hist, 'Age', alpha=.5, bins=20)\ngrid.add_legend()","1a5884ce":"guess_ages = np.zeros((2,3))\nguess_ages","ddd1e8d6":"for dataset in combine:\n    for i in range(0, 2):\n        for j in range(0, 3):\n            guess_df = dataset[(dataset['Sex'] == i) & \\\n                                  (dataset['Pclass'] == j+1)]['Age'].dropna()\n            # age_mean = guess_df.mean()\n            # age_std = guess_df.std()\n            # age_guess = rnd.uniform(age_mean - age_std, age_mean + age_std)\n\n            age_guess = guess_df.median()\n\n            # Convert random age float to nearest .5 age\n            guess_ages[i,j] = int( age_guess\/0.5 + 0.5 ) * 0.5\n            \n    for i in range(0, 2):\n        for j in range(0, 3):\n            dataset.loc[ (dataset.Age.isnull()) & (dataset.Sex == i) & (dataset.Pclass == j+1),\\\n                    'Age'] = guess_ages[i,j]\n    dataset['Age'] = dataset['Age'].astype(int)\n\ntrain_df.head()","f4bc423f":"train_df['AgeBand'] = pd.cut(train_df['Age'], 5)\ntrain_df[['AgeBand', 'Survived']].groupby(['AgeBand'], as_index=False).mean().sort_values(by='AgeBand', ascending=True)","28deed58":"for dataset in combine:    \n    dataset.loc[ dataset['Age'] <= 16, 'Age'] = 0\n    dataset.loc[(dataset['Age'] > 16) & (dataset['Age'] <= 32), 'Age'] = 1\n    dataset.loc[(dataset['Age'] > 32) & (dataset['Age'] <= 48), 'Age'] = 2\n    dataset.loc[(dataset['Age'] > 48) & (dataset['Age'] <= 64), 'Age'] = 3\n    dataset.loc[ dataset['Age'] > 64, 'Age']\ntrain_df.head()","548ea23d":"train_df = train_df.drop(['AgeBand'], axis=1)\ncombine = [train_df, test_df]\ntrain_df.head()","d7a29d8c":"for dataset in combine:\n    dataset['FamilySize'] = dataset['SibSp'] + dataset['Parch'] + 1\n\ntrain_df[['FamilySize', 'Survived']].groupby(['FamilySize'], as_index=False).mean().sort_values(by='Survived', ascending=False)","13fe3718":"for dataset in combine:\n    dataset['IsAlone'] = 0\n    dataset.loc[dataset['FamilySize'] == 1, 'IsAlone'] = 1\n\ntrain_df[['IsAlone', 'Survived']].groupby(['IsAlone'], as_index=False).mean()","a88ba2ae":"train_df = train_df.drop(['Parch', 'SibSp', 'FamilySize'], axis=1)\ntest_df = test_df.drop(['Parch', 'SibSp', 'FamilySize'], axis=1)\ncombine = [train_df, test_df]\n\ntrain_df.head()","73d4b4ab":"for dataset in combine:\n    dataset['Age*Class'] = dataset.Age * dataset.Pclass\n\ntrain_df.loc[:, ['Age*Class', 'Age', 'Pclass']].head(10)","41b45219":"freq_port = train_df.Embarked.dropna().mode()[0]\nfreq_port","84483290":"for dataset in combine:\n    dataset['Embarked'] = dataset['Embarked'].fillna(freq_port)\n    \ntrain_df[['Embarked', 'Survived']].groupby(['Embarked'], as_index=False).mean().sort_values(by='Survived', ascending=False)","74b2821a":"for dataset in combine:\n    dataset['Embarked'] = dataset['Embarked'].map( {'S': 0, 'C': 1, 'Q': 2} ).astype(int)\n\ntrain_df.head()","6560987f":"test_df['Fare'].fillna(test_df['Fare'].dropna().median(), inplace=True)\ntest_df.head()","994ae9aa":"train_df['FareBand'] = pd.qcut(train_df['Fare'], 4)\ntrain_df[['FareBand', 'Survived']].groupby(['FareBand'], as_index=False).mean().sort_values(by='FareBand', ascending=True)","f6720034":"for dataset in combine:\n    dataset.loc[ dataset['Fare'] <= 7.91, 'Fare'] = 0\n    dataset.loc[(dataset['Fare'] > 7.91) & (dataset['Fare'] <= 14.454), 'Fare'] = 1\n    dataset.loc[(dataset['Fare'] > 14.454) & (dataset['Fare'] <= 31), 'Fare']   = 2\n    dataset.loc[ dataset['Fare'] > 31, 'Fare'] = 3\n    dataset['Fare'] = dataset['Fare'].astype(int)\n\ntrain_df = train_df.drop(['FareBand'], axis=1)\ncombine = [train_df, test_df]\n    \ntrain_df.head(10)","decb6608":"test_df.head(10)","9f5a8ece":"X_train = train_df.drop(\"Survived\", axis=1)\nY_train = train_df[\"Survived\"]\nX_test  = test_df.drop(\"PassengerId\", axis=1).copy()\nX_train.shape, Y_train.shape, X_test.shape","16592a3e":"# Logistic Regression Algorithm\n\nlogreg = LogisticRegression()\nlogreg.fit(X_train, Y_train)\nY_pred = logreg.predict(X_test)\nacc_log = round(logreg.score(X_train, Y_train) * 100, 2)\nacc_log","d177e154":"coeff_df = pd.DataFrame(train_df.columns.delete(0))\ncoeff_df.columns = ['Feature']\ncoeff_df[\"Correlation\"] = pd.Series(logreg.coef_[0])\n\ncoeff_df.sort_values(by='Correlation', ascending=False)","ad650e1d":"# Support Vector Machines Algorithm\n\nsvc = SVC()\nsvc.fit(X_train, Y_train)\nY_pred = svc.predict(X_test)\nacc_svc = round(svc.score(X_train, Y_train) * 100, 2)\nacc_svc","aa97cf7f":"# KNN (K-nearest neighbours) Algorithm\nknn = KNeighborsClassifier(n_neighbors = 3)\nknn.fit(X_train, Y_train)\nY_pred = knn.predict(X_test)\nacc_knn = round(knn.score(X_train, Y_train) * 100, 2)\nacc_knn","b34cd470":"# Gaussian Naive Bayes Algorithm\n\ngaussian = GaussianNB()\ngaussian.fit(X_train, Y_train)\nY_pred = gaussian.predict(X_test)\nacc_gaussian = round(gaussian.score(X_train, Y_train) * 100, 2)\nacc_gaussian","79a66b96":"# Perceptron Algorithm\nperceptron = Perceptron()\nperceptron.fit(X_train, Y_train)\nY_pred = perceptron.predict(X_test)\nacc_perceptron = round(perceptron.score(X_train, Y_train) * 100, 2)\nacc_perceptron","96521e89":"# Linear SVC Algorithm\n\nlinear_svc = LinearSVC()\nlinear_svc.fit(X_train, Y_train)\nY_pred = linear_svc.predict(X_test)\nacc_linear_svc = round(linear_svc.score(X_train, Y_train) * 100, 2)\nacc_linear_svc","eefb33cf":"# Stochastic Gradient Descent Algorithm\n\nsgd = SGDClassifier()\nsgd.fit(X_train, Y_train)\nY_pred= sgd.predict(X_test)\nacc_sgd = round(sgd.score(X_train, Y_train) * 100, 2)\nacc_sgd","633f71ab":"# Decision Tree Algorithm\n\ndecision_tree = DecisionTreeClassifier()\ndecision_tree.fit(X_train, Y_train)\nY_pred = decision_tree.predict(X_test)\nacc_decision_tree = round(decision_tree.score(X_train, Y_train) * 100, 2)\nacc_decision_tree","a799acdc":"# Random Forest Algorithm\n\nrandom_forest = RandomForestClassifier(n_estimators=100)\nrandom_forest.fit(X_train, Y_train)\nY_pred = random_forest.predict(X_test)\nacc_random_forest = round(random_forest.score(X_train, Y_train) * 100, 2)\nacc_random_forest","4ef24b59":"models = pd.DataFrame({\n    'Model': ['Support Vector Machines', 'KNN', 'Logistic Regression', \n              'Random Forest', 'Naive Bayes', 'Perceptron', \n              'Stochastic Gradient Decent', 'Linear SVC', \n              'Decision Tree'],\n    'Score': [acc_svc, acc_knn, acc_log, \n              acc_random_forest, acc_gaussian, acc_perceptron, \n              acc_sgd, acc_linear_svc, acc_decision_tree]})\nmodels.sort_values(by='Score', ascending=False)","ee1760da":"print(Y_pred)","3fa76023":"submission = pd.DataFrame({\n        \"PassengerId\": test_df[\"PassengerId\"],\n        \"Survived\": Y_pred\n    })\nsubmission.to_csv('.\/submission.csv', index=False)","75782a45":"<h1><center>Exploring Machine Algorithms on Titanic Dataset:<\/center><\/h1>","96393233":"**Get the data:**<br>\nWith the help of pandas, we read datasets named  `train.csv` and `test.csv`. After that, we combine the datasets so we can perform certain information on both the datasets together.","3d24b942":"**K-nearest neighbours:**<br>\nK-nearest neighbors (KNN) algorithm is a type of supervised learning which can be used for both classification as well as regression predictive problems. But, it is mainly used for classification predictive problems in industry.","241d3d6f":"In this kernel I'll talk about Titanic dataset and few basic machine learning algorithms which I used while solving the Titanic: Machine Learning from Disaster problem(P.S: This is first problem which I solved on Kaggle) <br>\n<br>\n**List of the machine learning alogrithms included in the notebook:**\n- \tRandom Forest\n-   Decision Tree\n-   KNN\t\n-   Logistic Regression\n-   Linear SVC\n-   Perceptron\n-\tSupport Vector Machine\n-\tStochastic Gradient Decent\n-\tNaive Bayes\t","5c94c377":"**Decision Tree Algorithm:**<br>\nThe goal of using a Decision Tree is to create a training model that can use to predict the class or value of the target variable by learning simple decision rules inferred from prior data(training data).","db5482ac":"**Perceptron:**<br>\nA Perceptron is an algorithm used for supervised learning of binary classifiers. Binary classifiers decide whether an input, usually represented by a series of vectors, belongs to a specific class. In short, a perceptron is a single-layer neural network. They consist of four main parts including input values, weights and bias, net sum, and an activation function.","89d401da":"![](https:\/\/storage.googleapis.com\/kaggle-competitions\/kaggle\/3136\/logos\/header.png)","65c7b706":"**Model Evalution in descending order:**\n","33d22d8b":"### Distinguish the data into types:\n\nHere we distinguish the data into mainly two types `categorical` and `numerical`. Among other things this helps us select the appropriate plots for visualization.:<br>\n**Categorical:** Survived, Sex,Pclass and Embarked.<br>\n**Numerical:** Age, Fare,SibSp, and Parch.","c4eec0d8":"**Approach to solve the Problem:**\n- Get the training and testing data.\n- Wrangle, prepare, cleanse the data.\n- Analyze, identify patterns, and explore the data.\n- Model, predict and solve the problem.\n- Visualize, report, and present the problem solving steps and final solution.","43419cbc":"### Implementation of machine learning algorithms:\n","360bff60":"**Gaussian Naive Bayes Algorithm:**<br>\nIn Gaussian Naive Bayes, continuous values associated with each feature are assumed to be distributed according to a Gaussian distribution.\n","f002fc5c":"**Logistic Regression Algorithm:**<br>\nLogistic regression is used to estimate the probability that an instance belonogs to a particular class. If the estimated is probablility than 50%,then model predicts that the instance belong to that class, and otherwise it predicts that it does not. This is the reason behind it is also known as *binary classifier.*","a8fcb018":"**Stochastic Gradient Descent Algorithm:**<br>\nStochastic gradient descent is an iterative method for optimizing an objective function with suitable smoothness properties. It can be regarded as a stochastic approximation of gradient descent optimization, since it replaces the actual gradient by an estimate thereof.","68819407":"**Support Vector Machines:**<br>\nSupport vector machines (SVMs) are a set of supervised learning methods used for classification, regression and outliers detection. It is effective in high dimensional spaces.","e4f9a664":"## Model, Predict and Solve:\n","0a821422":"**Linear SVC Algorithm:**<br>\nThe objective of a Linear SVC (Support Vector Classifier) is to fit to the data you provide, returning a \"best fit\" hyperplane that divides, or categorizes, the data. ","6bef9d33":"**Random Forest Algorithm:**<br>\nThe random forest is a classification algorithm consisting of many decisions trees. It uses bagging and feature randomness when building each individual tree to try to create an uncorrelated forest of trees whose prediction by committee is more accurate than that of any individual tree.","058ea739":"### Any type of the suggestion or feedback will be great. So, I can improve myself from the next time onwards.\n","6c95944a":"### Analyze the data:\nPandas is also great for describing the data"}}