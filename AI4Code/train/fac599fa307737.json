{"cell_type":{"3cc1f705":"code","20f88bbd":"code","1cee1b46":"code","48c8c2e9":"code","4055f823":"code","da7e2ee8":"code","c54451dc":"code","aab6a58d":"code","cebe9b81":"code","e44829b6":"code","6e4abc78":"code","98d89c4d":"code","fa54ea7f":"code","ec8786f8":"code","61b31a8e":"code","6288f503":"code","c911add2":"code","fbedb43c":"code","15398ae2":"code","1790155c":"code","2c1f58a1":"code","b38c244d":"code","8a20954e":"code","901f7178":"code","78561501":"code","fb78e745":"code","b04e4287":"code","411de11e":"code","722d0287":"code","91d737a4":"code","494beade":"markdown","dde75e44":"markdown","645bfecb":"markdown","e31dea93":"markdown","60bebf09":"markdown","0168ca09":"markdown","9491d300":"markdown","f704efff":"markdown","8064fcfa":"markdown","8c04f50c":"markdown"},"source":{"3cc1f705":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","20f88bbd":"import seaborn as sns\nimport matplotlib.pyplot as plt","1cee1b46":"train = pd.read_csv(\"\/kaggle\/input\/allstate-claims-severity\/train.csv\")\ntest = pd.read_csv(\"\/kaggle\/input\/allstate-claims-severity\/test.csv\")\ntrain.shape, test.shape","48c8c2e9":"train.head()","4055f823":"train_cat = train.iloc[:, 1:117]\ntrain_cont = train.iloc[:, 117:-1]","da7e2ee8":"plt.figure(figsize=(16, 150))\nfor i, col in enumerate(train_cat.columns):\n    plt.subplot(30, 4, i+1)\n    sns.countplot(train_cat[col], order=train_cat[col].value_counts().sort_index().index)\nplt.tight_layout()","c54451dc":"plt.figure(figsize=(16, 12))\nfor i, col in enumerate(train_cont.columns):\n    plt.subplot(4, 4, i+1)\n    sns.distplot(train_cont[col])\nplt.tight_layout()","aab6a58d":"sns.distplot(np.log1p(train['loss']))","cebe9b81":"corr = train.drop(columns='id').corr()\nplt.figure(figsize=(10, 8))\nsns.heatmap(corr, annot=True, fmt='.2f', linewidths=0.5)","e44829b6":"high_corr= []\nthreshold = 0.8\nfor i in range(len(corr)):\n    for j in range(i+1, len(corr)):\n        if corr.iloc[i,j] >= threshold or (corr.iloc[i, j]<=-threshold and corr.iloc[i, j] < 0):\n            high_corr.append([corr.iloc[i,j], i, j])","6e4abc78":"for v, i, j in high_corr:\n    sns.pairplot(train_cont, x_vars=train_cont.columns[i], y_vars=train_cont.columns[j], size= 6)","98d89c4d":"# In order to make sure train & test sets would have same amount of cols(except loss) after modification\n\ndataset = pd.concat([train, test])\ndataset = dataset.drop(columns = ['cont1', 'cont6', 'cont11'])","fa54ea7f":"dataset = pd.get_dummies(dataset)\ndf_train = dataset[:len(train)]\ndf_test = dataset[len(train):]\ndf_test = df_test.drop(columns='loss')","ec8786f8":"y = np.log1p(df_train['loss'])\ndf_train = df_train.drop(columns='loss')","61b31a8e":"from sklearn.linear_model import LinearRegression,Ridge, Lasso\nfrom sklearn.model_selection import cross_val_score, train_test_split\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error\nfrom xgboost import XGBRegressor\nfrom lightgbm import LGBMRegressor","6288f503":"x_train, x_test, y_train, y_test = train_test_split(df_train, y, test_size=0.3, random_state=0)","c911add2":"# xgb = XGBRegressor(learning_rate=0.3, n_estimators=500)\n# xgb.fit(x_train, y_train)\n# mean_absolute_error(np.expm1(y_test), np.expm1(xgb.predict(x_test)))","fbedb43c":"# xgb=XGBRegressor(seed=18, objective='reg:linear', n_jobs=-1, verbosity=0,\n#                        colsample_bylevel=0.764115402027029, colsample_bynode=0.29243734009596956, \n#                        colsample_bytree= 0.7095719673041723, gamma= 4.127534050725986, learning_rate= 0.02387231810322894, \n#                        max_depth=14, min_child_weight=135, n_estimators=828,reg_alpha=0.3170105723222332, \n#                        reg_lambda= 0.3660379465131937, subsample=0.611471430211575)\n# xgb.fit(x_train, y_train)\n# mean_absolute_error(np.expm1(y_test), np.expm1(xgb.predict(x_test)))","15398ae2":"lgb = LGBMRegressor(objective='regression_l1', random_state=18, subsample_freq=1,\n                        colsample_bytree=0.3261853512759363, min_child_samples=221, n_estimators=2151, num_leaves= 45, \n                        reg_alpha=0.9113713668943361, reg_lambda=0.8220990333713991, subsample=0.49969995651550947, \n                        max_bin=202, learning_rate=0.02959820893211799)","1790155c":"lgb.fit(x_train, y_train)\nmean_absolute_error(np.expm1(y_test), (np.expm1(lgb.predict(x_test))))\n# mean_absolute_error(np.expm1(y_test), (np.expm1(lgb.predict(x_test))+np.expm1(xgb.predict(x_test)))\/2)","2c1f58a1":"sub = pd.DataFrame({'id': df_test['id'], 'loss': np.expm1(lgb.predict(df_test))})","b38c244d":"sub.to_csv('sub.csv', index=False)","8a20954e":"from sklearn.model_selection import learning_curve","901f7178":"from hyperopt import hp, fmin, Trials, tpe, pyll","78561501":"def f(params):\n    lgb = LGBMRegressor(**params)\n    lgb.fit(x_train, y_train)\n    return mean_absolute_error(np.expm1(y_test), (np.expm1(lgb.predict(x_test))))\n#     return -cross_val_score(LGBMRegressor(**params), df_train, y, cv=10).mean()\n\nspace = {\n        'subsample_freq':hp.choice('subsample_freq', range(1, 5)),\n        'colsample_bytree':hp.uniform('colsample_bytree', 0.2, 0.5), \n        'min_child_samples':hp.choice('min_child_samples', range(200, 250, 5)), \n        'n_estimators': hp.choice('n_estimators', range(1000, 3000, 100)), \n        'num_leaves': hp.choice('num_leaves', range(20, 50, 5)), \n        'reg_alpha': hp.uniform('reg_alpha', 0.70, 1), \n        'reg_lambda': hp.uniform('reg_lambda', 0.70, 1), \n        'subsample': hp.uniform('subsample', 0.3, 0.6), \n        'max_bin':hp.choice('max_bin', range(150, 250, 5)), \n        'learning_rate': hp.loguniform('learning_rate', np.log(0.005), np.log(0.2))\n}","fb78e745":"trial = Trials()\nbest = fmin(f, space, algo=tpe.suggest, max_evals=20, trials=trial)","b04e4287":"# only idx of best parameters could be achieved from best, so according to space, the values of best parameters could be found\n\nparams = {'colsample_bytree':0.2, 'learning_rate': 0.013636902671116896, 'max_bin': 85, 'min_child_samples': 205, \n          'n_estimators': 2000,'num_leaves': 35,'reg_alpha': 0.9579863172141052,'reg_lambda': 0.8783040346489164,\n          'subsample': 0.5899650955658289,'subsample_freq': 2}","411de11e":"lgb = LGBMRegressor(**params)\nlgb.fit(df_train, y)\nsub = pd.DataFrame({'id': df_test['id'], 'loss': np.expm1(lgb.predict(df_test))})\nsub.to_csv('sub.csv', index=False)","722d0287":"train_size, train_score, test_score = learning_curve(LGBMRegressor(**params), df_train, y, n_jobs=-1)","91d737a4":"train_mean = train_score.mean(axis=1)\ntrain_std = train_score.std(axis=1)\ntest_mean = test_score.mean(axis=1)\ntest_std = test_score.std(axis=1)\n\nplt.figure(figsize=(10, 8))\nplt.plot(train_size, train_mean, 'o-', linewidth=3)\nplt.fill_between(train_size, train_mean+train_std, train_mean-train_std, alpha=0.1)\nplt.plot(train_size, test_mean, 'o-', linewidth=3)\nplt.fill_between(train_size, test_mean+test_std, test_mean-test_std, alpha=0.1)\nplt.title('Learning Curve', size=20)\nplt.xlabel('Training Examples')\nplt.ylabel('Score')","494beade":"The curves look pretty reasonable","dde75e44":"#  Correlation","645bfecb":"# Try hyperopt","e31dea93":"# Modeling ","60bebf09":"Learning Curve--to see whether it's ideal or not","0168ca09":"# Data Visualization","9491d300":"# Let's begin\n**From the introduction, we know that the goal is to estimate the loss which is continuous. So this project is about REGRESSION**\n\n**Steps:**\n1. Data Visualization\n    * Import Data\n    * Object Data Barplot\n    * Numerical Data Distplot\n    * Goal Data Normal Distribution\n2. Correlation\n    * Find the highly correlated columns\n3. Data Preparation\n    * Delete highly correlated columns\n    * Ajust Goal Data\n4. Modeling\n    * LGBMCLASSIFIER\n    * Export Result\n5. Hyperopt\n    * Find the optimized params\n    * Learning Curve","f704efff":"# Data Preparation","8064fcfa":"* For categorical cols, there are some cols which are totally unbalanced so that they might not be useful for the data analysis\n* For numerical cols, normal distrition is always regarded as the best situtaion, obviously some cols is either skewed or korted or just wierd\n* For numerical cols, further process to check each col is necessary -- correlation study","8c04f50c":"* The five pairs are (1,9), (1,10), (6, 10), (6, 13), (11, 12)\n* From the graphics above, the five pairs are all with high correlation\n* It's necessary to remove some of them and 1, 6 are paired with two another col respectively, removing 1, 6 and 11 or 12 is my choice"}}