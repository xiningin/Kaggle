{"cell_type":{"a4ed04de":"code","f6d4306a":"code","ef7f80ec":"code","59c07afe":"code","428d5123":"code","0bbcf34c":"code","851a6b89":"code","52be0618":"code","557d2c8e":"code","d8080650":"code","2340916f":"code","388aa74e":"code","1cf05147":"code","5c4a51de":"code","cb8cbe2a":"code","bb72d535":"code","5355ffbf":"code","e4d02889":"code","aed8221c":"code","705a1307":"code","056c518b":"code","3267e6f9":"code","37387d6d":"code","2d4dbd81":"code","947fc0af":"code","81a4de5e":"code","45a24640":"code","fa6d2e02":"code","0b5c54f0":"code","61b4d9ec":"code","550a69b2":"code","627502ce":"code","d2e5eefe":"code","bbd68641":"code","a40b383b":"code","09e68ccb":"code","497ad79c":"code","e92cd6d3":"code","4dc1617e":"code","7c532a61":"code","039c6f9a":"code","aea34faf":"code","d5d84f29":"code","07e873b4":"code","25e55844":"code","d24c638f":"code","46c6067b":"code","94dd2523":"code","ffce5569":"code","16a1bcfa":"code","621d7514":"code","1014324f":"code","442d543f":"code","e701eee8":"code","ec27695a":"code","2e958c8f":"code","9decaada":"code","3f8c54d8":"code","f1884fb9":"markdown","0bd3eaab":"markdown","f160c374":"markdown","284fc4db":"markdown","cd64fb2c":"markdown","9d5804b3":"markdown","727ab48c":"markdown","8ceafbef":"markdown","1eda6e5c":"markdown","a2b94830":"markdown","5cf0a405":"markdown","e0e869c4":"markdown","5a3f298d":"markdown","332278d1":"markdown","5177fa74":"markdown","7bb81974":"markdown","aeacfdc0":"markdown","50761f2c":"markdown","c0a2afd7":"markdown","e284362b":"markdown","1d367fda":"markdown","be78c6b0":"markdown","9fb3a53c":"markdown"},"source":{"a4ed04de":"import numpy as np # linear algebra\r\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\r\nimport matplotlib.pyplot as plt\r\nimport seaborn as sn\r\n\r\nfrom random import seed\r\nseed(1)\r\n\r\n\r\nkaggle = 1 # Kaggle active 1\r\n\r\nif kaggle == 1 :\r\n    MNIST_PATH= '..\/input\/digit-recognizer'\r\nelse:\r\n    MNIST_PATH= '..\/Another_MNIST_try\/data\/input\/digit-recognizer'\r\n\r\n\r\n\r\n%matplotlib inline\r\n\r\nimport os\r\nfor dirname, _, filenames in os.walk(MNIST_PATH): \r\n    for filename in filenames:\r\n        print(os.path.join(dirname, filename))","f6d4306a":"# Data path and file\r\n#MNIST_PATH= '..\/input\/digit-recognizer'\r\n#MNIST_PATH= '..\/Another_MNIST_try\/data\/input\/digit-recognizer'\r\nCSV_FILE_TRAIN='train.csv'\r\nCSV_FILE_TEST='test.csv'\r\n\r\ndef load_mnist_data(minist_path, csv_file):\r\n    csv_path = os.path.join(minist_path, csv_file)\r\n    return pd.read_csv(csv_path)\r\n\r\ndef load_mnist_data_manuel(minist_path, csv_file):\r\n    csv_path = os.path.join(minist_path, csv_file)\r\n    csv_file = open(csv_path, 'r')\r\n    csv_data = csv_file.readlines()\r\n    csv_file.close()\r\n    return csv_data\r\n\r\ndef split_train_val(data, val_ratio):\r\n    return \r\n    \r\n\r\ntrain = load_mnist_data(MNIST_PATH,CSV_FILE_TRAIN)\r\ntest = load_mnist_data(MNIST_PATH,CSV_FILE_TEST)\r\n\r\ntrain_2 = load_mnist_data_manuel(MNIST_PATH,CSV_FILE_TRAIN)","ef7f80ec":"train.describe()","59c07afe":"train.info()","428d5123":"train","0bbcf34c":"train_copy = train.copy()","851a6b89":"# separating labels from features\nmnist_features = train_copy.drop('label', axis=1)\nmnist_labels = train_copy['label']","52be0618":"plt.imshow(np.asfarray(mnist_features[4:5]).reshape(28,28), cmap='binary')\nplt.axis(\"off\")\nplt.show()","557d2c8e":"def print_digits(digit_dataframe):\n    figsize = (8,6)\n    cols = 4\n    rows = 6 \/\/ cols +1 \n\n    def trim_axs(axs, N):\n        \"\"\"\n        Reduce *axs* to *N* Axes. All further Axes are removed from the figure.\n        \"\"\"\n        axs = axs.flat\n        for ax in axs[N:]:\n            ax.remove()\n        return axs[:N]\n\n\n    axs = plt.figure(figsize=figsize).subplots(rows, cols)\n    axs = trim_axs(axs, len(digit_dataframe))\n\n    i = 0\n\n    for ax in axs:\n        ax.imshow((np.asfarray(digit_dataframe.iloc[i]).reshape(28,28)),cmap='binary')\n        ax.set_xticklabels([])\n        ax.set_yticklabels([])\n        i = i + 1\n","d8080650":"print_digits(mnist_features)","2340916f":"corr_matrix = train_copy.corr()","388aa74e":"corr_matrix['label'].sort_values(ascending=False)","1cf05147":"attributes = ['pixel381','pixel409','pixel436','pixel408']\n\npd.plotting.scatter_matrix(train_copy[attributes], figsize=(8,6))","5c4a51de":"from sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.cluster import KMeans\n\n# Pipeline for transforming \/ scaling the data in various ways\nnum_pipeline = Pipeline([\n    ('std_scaler', StandardScaler())\n])","cb8cbe2a":"mnist_features_prepared = num_pipeline.fit_transform(mnist_features)\r\nmnist_features_prepared","bb72d535":"from sklearn.neighbors import KNeighborsClassifier\n\n# Train classifier with prepared train set\nkneighbors = KNeighborsClassifier()\nkneighbors.fit(mnist_features_prepared, mnist_labels)","5355ffbf":"some_digits = mnist_features[:8]\r\nsome_digits_labels = mnist_labels[:8]\r\n\r\nsome_digits_prepared = num_pipeline.transform(some_digits)","e4d02889":"# Test the new classifier\nprint(\"Following were predicted: \", kneighbors.predict(some_digits_prepared))\nprint(\"The labels: \", list(some_digits_labels))","aed8221c":"print_digits(some_digits)","705a1307":"from sklearn.model_selection import cross_val_predict\n\nmnist_label_preds = cross_val_predict(KNeighborsClassifier(), mnist_features_prepared, mnist_labels, cv=3, n_jobs=-1)","056c518b":"print(\"Cross Val Predict results: \" , mnist_label_preds)","3267e6f9":"from sklearn.metrics import confusion_matrix\n\ncm = confusion_matrix(mnist_label_preds, mnist_labels)","37387d6d":"#Confusion matrix \nplt.figure(figsize = (10,8))\nsn.heatmap(cm, annot=True, vmin=0, vmax=2000)","2d4dbd81":"row_sums = cm.sum(axis=1, keepdims=True) \nnorm_conf_mx = cm \/ row_sums # normalized confusion matrix\n\nnp.fill_diagonal(norm_conf_mx, 0)\n\nplt.figure(figsize = (10,8))\nsn.heatmap(norm_conf_mx, annot=True, vmin=0, vmax=0.05)","947fc0af":"from sklearn.metrics import precision_score, recall_score, f1_score\n\nprint(\"Precision: \",precision_score(mnist_labels, mnist_label_preds, average='weighted'))\nprint(\"Recall: \",recall_score(mnist_labels, mnist_label_preds, average='weighted'))\nprint(\"F1-Score (weighted): \", f1_score(mnist_labels, mnist_label_preds, average='weighted'))","81a4de5e":"# show digits for 9 and 4 \nminst_9_labels_idx = mnist_labels[mnist_labels == 9].index\nmnist_9_features = mnist_features.iloc[minst_9_labels_idx]\n\nminst_4_labels_idx = mnist_labels[mnist_labels == 4].index\nmnist_4_features = mnist_features.iloc[minst_4_labels_idx]\n\nminst_5_labels_idx = mnist_labels[mnist_labels == 5].index\nmnist_5_features = mnist_features.iloc[minst_5_labels_idx]\n\nprint_digits(mnist_9_features)\nprint_digits(mnist_4_features)\nprint_digits(mnist_5_features)","45a24640":"from sklearn.neighbors import KNeighborsClassifier\n\nknn_cl = KNeighborsClassifier()\n#knn_cl_grid = KNeighborsClassifier()\nknn_cl_grid_rd = KNeighborsClassifier()\n\nknn_cl.get_params().keys()     # what params to modify","fa6d2e02":"# RandomizedSearch try\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.utils.fixes import loguniform\nfrom scipy.stats import expon\nfrom scipy.stats import randint\n\n# 'algorithm', 'leaf_size', 'metric', 'metric_params', 'n_jobs', 'n_neighbors', 'p', 'weights'\n\ndistributions = dict(\n        leaf_size =randint(1,20)\n        ,n_neighbors = randint(1,20)\n        ,weights=['uniform','distance']\n        ,p=[1,2]\n        )\n\ndistributions_set2 = dict(\n        n_neighbors = randint(1,10)\n        ,weights=['uniform','distance']\n        ,p=[1,2]\n        )\n\ndistributions_set3 = dict(\n        #leaf_size =randint(1,30)\n        n_neighbors = randint(1,40) #[3,6,9,12,15,18,21] #\n        ,weights = ['distance']\n        #,p=[1]\n        )\n\ndistributions_set4 = dict(\n        #leaf_size =randint(1,30)\n        n_neighbors = randint(1,40) #[3,6,9,12,15,18,21] #\n        ,weights = ['uniform','distance']\n        #,p=[1]\n        )\n\ndistributions_set5 = dict(\n        leaf_size =range(1,100)\n        ,n_neighbors = range(1,100) #[3,6,9,12,15,18,21] #\n        ,weights = ['uniform','distance']\n        #,p=[1]\n        )\n\ndistributions_set6 = dict(\n        leaf_size =range(1,50)\n        ,n_neighbors = range(1,50) #[3,6,9,12,15,18,21] #\n        ,weights = ['uniform','distance']\n        ,p=[1]\n        )\n\ndistributions_set7 = dict(\n        leaf_size =range(1,10)\n        ,n_neighbors = range(1,10) #[3,6,9,12,15,18,21] #\n        ,weights = ['distance']\n       ## ,p=[2]\n        )\n\ndistributions_set8 = dict(\n        #leaf_size =range(1,10)\n        n_neighbors = range(1,100) #[3,6,9,12,15,18,21] #\n        ,weights = ['distance']\n        ,p=[1]\n        )\n\ndistributions_set9 = dict(n_neighbors = range(1,8)\n                          ,weights = ['distance']\n                          ,p=[1] \n                         )\n\n\nknn_cl_ransearch = RandomizedSearchCV(knn_cl_grid_rd, distributions_set9, n_iter=3, cv=3, random_state=0, return_train_score=True, n_jobs=-1)\nknn_cl_ransearch.fit(mnist_features_prepared, mnist_labels)","0b5c54f0":"knn_cl_ransearch.best_params_","61b4d9ec":"# Model load and save structure.\nimport pickle\n\nknnPkl_rnn_filename = 'knnPickle_dist9_3_rans'\n\nknnPkl_dirname = '..\\Another_MNIST_try'    # local Path\nknnPkl_dirname_kgg = '..\/'                 # Kaggle path\n\nknnPkl_rnn_fildir = os.path.join(knnPkl_dirname_kgg, knnPkl_rnn_filename)","550a69b2":"# Save model\r#\npickle.dump(knn_cl_ransearch, open(knnPkl_rnn_fildir, 'wb'))","627502ce":"# Load model\r\nknn_cl_ransearch = pickle.load(open(knnPkl_rnn_fildir, 'rb'))","d2e5eefe":"cv_results_knn_ransearch = knn_cl_ransearch.cv_results_\r\n\r\nfor mean_score, params in zip(cv_results_knn_ransearch[\"mean_test_score\"],cv_results_knn_ransearch[\"params\"]):\r\n    print(mean_score, params)","bbd68641":"fig = plt.figure(figsize=(8,12))\r\n\r\nax = fig.add_subplot(2,1,1)\r\n\r\nax.plot(list(range(1,len(cv_results_knn_ransearch['mean_train_score']) + 1)), cv_results_knn_ransearch['mean_train_score'])\r\nax.plot(list(range(1,len(cv_results_knn_ransearch['mean_test_score']) + 1)), cv_results_knn_ransearch['mean_test_score'])","a40b383b":"from sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.cluster import KMeans\n\n# Pipeline for transforming \/ scaling the data in various ways\nprep_pipeline = Pipeline([\n    ('kmeans', KMeans()),\n    ('kneighbors', KNeighborsClassifier())\n])","09e68ccb":"KMeans().get_params().keys()","497ad79c":"k_range = range(20,500,20) # defining the range of clusters looking into\nk_list = list()\n\n\nfor k in k_range:\n    kmeans = KMeans(n_clusters=k, random_state=42)\n    kmeans = kmeans.fit(mnist_features_prepared)\n    \n    k_list.append(kmeans)","e92cd6d3":"inertias_list = []\nfor model in k_list:\n    inertias_list.append([model.inertia_])\n    ","4dc1617e":"plt.figure(figsize=(8, 5))\nplt.plot(k_range, inertias_list, \"bo-\")\nplt.title(\"Inertia for the Amount of Clusters\")\nplt.xlabel(\"$k$\", fontsize=14)\nplt.ylabel(\"Inertia in M.\", fontsize=14)\nplt.show()","7c532a61":"from sklearn.metrics import silhouette_score\r\n\r\nsil_list = list()\r\n\r\nfor model in k_list:\r\n    sil_score = silhouette_score(mnist_features_prepared,model.labels_)\r\n    sil_list.append(sil_score)","039c6f9a":"best_index = np.argmax(sil_list)\r\nbest_k = k_list[best_index]\r\nbest_score = sil_list[best_index]","aea34faf":"print(best_score)\r\nprint(best_k)","d5d84f29":"plt.figure(figsize=(8,3))\r\nplt.plot(k_range,sil_list)","07e873b4":"##%%time\r\nfrom sklearn.model_selection import GridSearchCV\r\n\r\nparam_grid_try_1 =[\r\n  { \r\n    'n_neighbors':[1,2,3,4],\r\n    'weights':['distance','uniform'],\r\n    'p':[1]\r\n  },\r\n  { \r\n    'n_neighbors':[4,5,6],\r\n    'weights':['distance'],\r\n    'p':[1]\r\n  },\r\n   {\r\n    'n_neighbors':[4],\r\n    'leaf_size': [200,300,400],\r\n    'weights':['distance'],\r\n    'p':[1]\r\n  }\r\n]\r\n\r\n\r\nparam_grid_try_2 = dict(\r\n   kmeans__n_clusters= [400,500],\r\n   kneighbors__n_neighbors=[4,5,6,10,15,20],\r\n   kneighbors__weights=['distance'],\r\n   kneighbors__p=[1]\r\n)\r\n\r\nparam_grid_try_3 = dict(\r\n   kmeans__n_clusters=[400,500,600],\r\n   kneighbors__n_neighbors=[2,3,4,5],\r\n   kneighbors__weights=['distance'],\r\n   kneighbors__p=[1]\r\n)\r\n\r\nparam_grid_try_4 = dict(\r\n   kmeans__n_clusters= [200,300,350,400,500],\r\n   kneighbors__n_neighbors=[4],\r\n   kneighbors__weights=['distance'],\r\n   kneighbors__p=[2]  \r\n)\r\n\r\nparam_grid_try_5 = dict(\r\n   kmeans__n_clusters= [5000],\r\n   kneighbors__n_neighbors=[4],\r\n   kneighbors__weights=['distance']\r\n)\r\n\r\n## one with kmeans__n_clusters param for showcase\r\nparam_grid = dict(\r\n   kmeans__n_clusters=[50],\r\n   kneighbors__n_neighbors=[4],\r\n   kneighbors__weights=['distance'],\r\n   kneighbors__p=[1]  \r\n)\r\n\r\ngrid_search = GridSearchCV(prep_pipeline, param_grid, cv=3, n_jobs=1, return_train_score=True, verbose=2)\r\ngrid_search.fit(mnist_features_prepared, mnist_labels)\r\n","25e55844":"# Model load and save structure.\nknnPkl_gs_filename = 'knnPickle_gs_3'\n\nknnPkl_gs_fildir = os.path.join(knnPkl_dirname_kgg, knnPkl_gs_filename)","d24c638f":"# Save model\n#pickle.dump(grid_search, open(knnPkl_gs_fildir, 'wb'))","46c6067b":"# Load model\n#grid_search = pickle.load(open(knnPkl_gs_fildir, 'rb'))","94dd2523":"print(\"Best params: \", grid_search.best_params_)\nprint(\"Best score: \", grid_search.best_score_)","ffce5569":"cvres = grid_search.cv_results_\n\nfor mean_score, params in zip(cvres[\"mean_test_score\"],cvres[\"params\"]):\n    print(mean_score, params)","16a1bcfa":"knn_final = KNeighborsClassifier()\r\nknn_final = KNeighborsClassifier(n_neighbors=4,weights='distance',p=1, n_jobs=-1)\r\nknn_final.fit(mnist_features_prepared, mnist_labels)","621d7514":"test_copy = test.copy()\r\n\r\ntest_prep = num_pipeline.transform(test_copy) # Transform data for kneighbors","1014324f":"print(\"Prediction: \" , knn_cl_ransearch.predict(test_prep[5].reshape(1,-1)))\r\nprint(\"Propability: \", knn_cl_ransearch.predict_proba(test_prep[5].reshape(1,-1)))\r\n\r\nprint(\"Prediction: \" , grid_search.predict(test_prep[5].reshape(1,-1)))\r\nprint(\"Propability: \", grid_search.predict_proba(test_prep[5].reshape(1,-1)))\r\n\r\nprint(\"Prediction: \" , knn_final.predict(test_prep[5].reshape(1,-1)))\r\nprint(\"Propability: \", knn_final.predict_proba(test_prep[5].reshape(1,-1)))","442d543f":"plt.imshow(np.asfarray(test_prep[6]).reshape(28,28), cmap='binary')\nplt.axis(\"off\")\nplt.show()","e701eee8":"mnist_submission = pd.DataFrame(columns=['ImageId','Label'])\r\ntest_prep_df = pd.DataFrame(data=test_prep)","ec27695a":"# fill submission file\r\ni=0\r\nfor row in test_prep:\r\n    i = i+1\r\n    index = i\r\n    label = knn_final.predict(row.reshape(1,-1))\r\n    \r\n    mnist_submission = mnist_submission.append({'ImageId' : index , 'Label': label} , ignore_index=True)\r\n    pass","2e958c8f":"# Preparing the submission file\nmnist_submission.ImageId = mnist_submission.ImageId.astype(int)\nmnist_submission.Label = mnist_submission.Label.astype(int)","9decaada":"mnist_submission.to_csv(\"mnist_submission.csv\",index=False)","3f8c54d8":"mnist_submission","f1884fb9":"### Pipelines\r\nFit the pipeline on training data only and then transform it based on the fitted pipeline.","0bd3eaab":"### Precision, Recall and F1-Score","f160c374":"# Discover and Visualize the Data","284fc4db":"fig = plt.figure(figsize=(8,12))\n\nax = fig.add_subplot(2,1,1)\n\nax.plot(list(range(1,len(cvres['mean_train_score']) + 1)), cvres['mean_train_score'])\nax.plot(list(range(1,len(cvres['mean_test_score']) + 1)), cvres['mean_test_score'])","cd64fb2c":"## Correlation Matrix","9d5804b3":"The following Confusion Matrix prints the amount of predicted labels corresponding to their x and y coordinates. That means the 0 (at X = 0) has been labeld 4104 times with a prediction label 0 (at Y = 0). This corresponds to a correct prediction of 4103 times. The otherway around can be seen at the coordinates x=3 y=8. The digit 3 has been predicted 78 times as the digit 8 instead of the digit 3. The color highlighting of the Confusion Matrix does show this very good.","727ab48c":"The following Confusion Matrix shows the relative error corresponding to its category. This is necessary in case of not balanced category values.","8ceafbef":"## Preparing Submission","1eda6e5c":"# Prepare Data for Algorithm\n- Write functions to build transformation pipelines\n- Scaling features (normalization)","a2b94830":"A closer look into the different cluster runs lead to the realization, that KMeans will not support me with this dataset (Neverthelese I will leave it in the configuration as described in the chapter headline)","5cf0a405":"# The Big Picture\nLink to the topic: https:\/\/www.kaggle.com\/c\/digit-recognizer\/data","e0e869c4":"### KMeans as Data Preparation Step\r\nIn this part I will try to find out whether the KMeans is a good data preparation step based on the given train data or not. This goes hand in hand with the determination of finding the correct amount of clusters, if they exist. A very good indicator for the correct amount of clusters is the silhouette coefficient.","5a3f298d":"# Final\r\nRetraining the model with the best hyperparam set found out","332278d1":"### Save and Load","5177fa74":"Getting the inertia of the Clusters for a plot","7bb81974":"# Get Data","aeacfdc0":"# Choose a Model","50761f2c":"This Confusion Matrix shows us clearly that there is a small amout of errors in predicting the digit 4 respectively in predicting the digit 9 for the digit 4 with an error rate of 2.3%. But its only in this case, the otherway around works better. In case of the digit 9, the predictor has only an error rate of 1.1% when mistakenly predicting a 4. <br>\n<br>\nThe digit 8 seems to be more often mispredicted then other digits, its visualized by the more brighter rowbased results.","c0a2afd7":"## Accuracy with Cross Validation and Confusion Matrix (Very important within Classification Problems)","e284362b":"# Refine the model\nIn this part I am trying to find the best hyperparameters for the model. For this I will use the randomizedsearch and the gridsearch method. ","1d367fda":"### First Test  \r\nUsing some (original) train data for tests. I need to transform them again (with the pipeline) because I selected them from the original unmodified train set. This is the best way to do it instead of using the same data object which has already been trained with the model.","be78c6b0":"## RandomizedSearchCV\r\nIn the following dictionary set you will find several configurations I tried out with the randomized search to find the best parameter area for my model. It ended up in the distribution set number 9 witch gave me a smaller parameter area to take a look into. In later process of this notebook I will use this information in the GridSearch function to try out with some specific parameters (not randomized) that could probably improve my accuracy.","9fb3a53c":"## GridSearch\r\nWith this GridSearch I want to look closer into specific parameters. I have already tried some different values around the n_neighbors parameter (see the other dictionary params grids (\"param_grid_try_#\") ) and took a closer look into the leaf_size which in the end turns out as a parameter that does not have any effects on the accuracy of the model with this dataset.\r\n\r\nIn further processing, it turns out that there is more need to prepare the data for the KNearestNeighbor algorithm to get better accuracy. During the research, I found out that there is a way to preprocess data due to clustering with the KMeans algorithm. So I am going to use a pipeline with the KMeans as the first step for preprocessing and train the KNeighbors model with the results from the processing. It is possible to use the Gridsearch in combination with a pipeline. The parameters have to be named specifically so the Gridsearch is able to match each parameter to its corresponding function\/algorithm.\r\n\r\nEDIT: While I am writing this here I had already tried several parameters with the Gridsearch approach, especially with the cluster parameter. It turns out that there is no good cluster amount to prepare the data in that way it would support the KNeighbors algorithm to increase its accuracy. I also added a calculation for the silhouette coefficient to determine the best cluster amount by using the KMeans by himself, no success! KMeans clustering or preprocessing does not support the KNeighbors algorithm with the given dataset. \r\n\r\nHowever, I let the code as it is to show the usage of pipelines, KMeans, and silhouette coefficient calculation for learning purposes."}}