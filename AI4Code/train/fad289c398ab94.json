{"cell_type":{"db49cdd0":"code","caaf16a0":"code","277858f2":"code","4b804b71":"code","5344d94c":"code","ff8e9e42":"code","c726e25f":"code","0c1092fd":"code","2de3bbd3":"code","6b8115fe":"code","d2ee1585":"code","874e2162":"code","62277af7":"code","575bfe44":"code","66237a69":"code","9cbdc4a4":"code","8e36c73d":"code","be64ff46":"code","28366e84":"code","82eb2a6d":"code","5dae5d6c":"code","2db209aa":"code","06ecbd1b":"code","2061e2da":"code","bf8788d4":"code","b9c677f6":"code","ea929f38":"code","c3aa8cc6":"code","2f0982d7":"code","dda1dc1b":"code","e8fd799d":"code","12bccca6":"code","b9dd131d":"code","8f81a916":"code","d9cf6a02":"code","54413f4b":"code","4a9627b5":"code","ffc3a8fe":"code","f5096ad2":"code","f03ec411":"code","189f28e5":"code","88f8c2cc":"code","63c64243":"code","86835991":"code","2d1fa6a9":"code","8b7a74a0":"code","ec83eaa8":"code","360757a7":"code","fd7f481c":"code","560a2a44":"code","30d331cc":"code","3922e060":"code","bba3f461":"code","75620cc8":"code","53aa9df5":"code","8c2090de":"code","6b6114be":"code","0df7434a":"code","d85b21d2":"code","2c4d69b6":"code","67fd1b76":"code","91cb0b7d":"code","c5a3dd15":"code","d02fdafd":"code","3ede5e3c":"code","f50a7aba":"code","54f02cc4":"code","fab4dab6":"code","0137708a":"code","f6a368ce":"code","a003e106":"code","5414452b":"code","ebf7e166":"code","ac43be67":"code","7efe4d28":"code","9fc1c2e6":"code","ec95a286":"code","d8cf3cbd":"markdown","c4c17f47":"markdown","14755d43":"markdown","7ca1b47e":"markdown","50652a95":"markdown","d7f28b3c":"markdown","7fb45d9b":"markdown","c0ee64db":"markdown","62e3328c":"markdown","95e02dc2":"markdown","db8fa2bc":"markdown","74784257":"markdown","987d86d7":"markdown","8c701642":"markdown"},"source":{"db49cdd0":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","caaf16a0":"# Reading training data\ntraining = pd.read_csv('..\/input\/titanic\/train.csv')\ntesting = pd.read_csv('..\/input\/titanic\/test.csv')\n\n# Adding indicator for Training and Testing observations\ntraining['train_test'] = 1\ntesting['train_test'] = 0\n\n# Adding Survived column to testing dataframe\ntesting['Survived'] = np.NaN\nall_data = pd.concat([training, testing])\n\nall_data.columns","277858f2":"all_data.info()","4b804b71":"all_data.describe().transpose()","5344d94c":"all_data.describe(include='O').transpose()","ff8e9e42":"all_data.describe().columns","c726e25f":"training.describe(include='O').columns","0c1092fd":"# Create df with numerical and categorical columns to perform EDA\ndf_num = training[[ 'Age', 'SibSp', 'Parch', 'Fare']]\ndf_cat = training[['Survived', 'Pclass', 'Name', 'Sex', 'Ticket', 'Cabin', 'Embarked']]","2de3bbd3":"import matplotlib.pyplot as plt\n\n# create histogram for each numerical column\nfor i in df_num.columns:\n    plt.hist(df_num[i], bins=70)\n    plt.title(i)\n    plt.show()","6b8115fe":"# Checking corrilation\nimport seaborn as sns\nprint(df_num.corr())\nsns.heatmap(df_num.corr())","d2ee1585":"# Compare survival rate across Age, SibSp, Parch and Fare\npd.pivot_table(data=training, index='Survived', values=['Age', 'SibSp', 'Parch', 'Fare'])","874e2162":"# Create barcharts for each if categorical columns\nfor i in df_cat.columns:\n    sns.barplot(df_cat[i].value_counts().index, df_cat[i].value_counts())\n    plt.title(i)\n    plt.show()","62277af7":"# Comparing survival and each of categorical variables\nprint(pd.pivot_table(data=training, index='Survived', columns='Pclass', values='Ticket', aggfunc='count'))\nprint('\\n')\nprint(pd.pivot_table(data=training, index='Survived', columns='Sex', values='Ticket', aggfunc='count'))\nprint('\\n')\nprint(pd.pivot_table(data=training, index='Survived', columns='Embarked', values='Ticket', aggfunc='count'))\nprint('\\n')","575bfe44":"# Checking if Cabin letter or purchase of multiple cabins impacted survival rate\n\ntraining['cabin_multiple'] = training.Cabin.apply(lambda x: 0 if pd.isna(x) else len(x.split(' ')))\ntraining['cabin_multiple'].value_counts()","66237a69":"pd.pivot_table(data=training, index='Survived', columns='cabin_multiple', values='Ticket' , aggfunc='count')","9cbdc4a4":"training['cabin_adv'] = training['Cabin'].apply(lambda x: str(x)[0])","8e36c73d":"pd.pivot_table(data=training, index='Survived', columns='cabin_adv', values='Name' , aggfunc='count')","be64ff46":"training.Ticket.value_counts(sort=True, ascending=False)","28366e84":"training['numeric_ticket'] = training.Ticket.apply(lambda x: 1 if x.isnumeric() else 0)\ntraining['ticket_letters'] = training.Ticket.apply(lambda x: ''.join(x.split(' ')[:-1]).replace('.','').replace('\/','').lower() \n                                                   if len(x.split(' ')[:-1]) > 0 else 0)","82eb2a6d":"training['numeric_ticket'].value_counts()","5dae5d6c":"training['ticket_letters'].value_counts(sort=True, ascending=False)","2db209aa":"plt.figure(figsize=(10,6))\nsns.barplot(data=training, x=training['ticket_letters'].value_counts(sort=True, ascending=False).index[1:], \n            y=training['ticket_letters'].value_counts(sort=True, ascending=False).values[1:])\nplt.show()","06ecbd1b":"# checking if numeric ticket number has any impact on survival rate\n\npd.pivot_table(data=training, index='Survived', columns=['numeric_ticket'], values='Ticket', aggfunc='count')","2061e2da":"# checking if Ticket Letters have any impact on survival rate\n\npd.pivot_table(data=training, index='Survived', columns='ticket_letters', values='Ticket', aggfunc='count')","bf8788d4":"training.Name.head()","b9c677f6":"# checking survival rate based on person's Title\ntraining['name_title'] = training.Name.apply(lambda x: x.split(',')[1].split('.')[0].strip())","ea929f38":"pd.pivot_table(data=training, index='Survived', columns='name_title', values='Ticket', aggfunc='count')","c3aa8cc6":"all_data['cabin_multiple'] = all_data.Cabin.apply(lambda x: 0 if pd.isna(x) else len(x.split(' ')))\nall_data['cabin_adv'] = all_data['Cabin'].apply(lambda x: str(x)[0])\nall_data['numeric_ticket'] = all_data.Ticket.apply(lambda x: 1 if x.isnumeric() else 0)\nall_data['ticket_letters'] = all_data.Ticket.apply(lambda x: ''.join(x.split(' ')[:-1]).replace('.','').replace('\/','').lower() \n                                                   if len(x.split(' ')[:-1]) > 0 else 0)\nall_data['name_title'] = all_data.Name.apply(lambda x: x.split(',')[1].split('.')[0].strip())\n","2f0982d7":"# assign values to NaN in Age, Cabin, Embarked\ntraining.isnull().sum()","dda1dc1b":"training.Age.mean()","e8fd799d":"all_data.Age = all_data.Age.fillna(value=training.Age.mean())\nall_data.Fare = all_data.Fare.fillna(training.Fare.mean())","12bccca6":"# Drop all rows with Embarked as null\nall_data.dropna(subset=['Embarked'], inplace=True)","b9dd131d":"# Normalize libling count\nall_data['norm_sibsp'] = np.log(all_data.SibSp+1)\nall_data['norm_sibsp'].hist()","8f81a916":"# Normalize fare price\nall_data['norm_fare'] = np.log(all_data.Fare+1)\nall_data['norm_fare'].hist()","d9cf6a02":"# Convert passenger class to string\nall_data.Pclass = all_data.Pclass.astype(str)","54413f4b":"all_data.columns","4a9627b5":"all_data.isnull().sum()","ffc3a8fe":"# create dummy variables\nall_dummies = pd.get_dummies(all_data[['Pclass', 'Sex', 'Age', 'SibSp','Parch', 'Embarked', 'train_test',\n       'cabin_multiple', 'cabin_adv', 'numeric_ticket','name_title', 'norm_fare']])","f5096ad2":"all_dummies.dropna(how='any', axis=0, inplace=True)","f03ec411":"all_dummies.isnull().sum()","189f28e5":"X_train = all_dummies[all_dummies['train_test']==1].drop('train_test', axis=1)\nX_test = all_dummies[all_dummies['train_test']==0].drop('train_test', axis=1)","88f8c2cc":"y_train = all_data[all_data.train_test == 1].Survived\ny_train.shape","63c64243":"# scale all numeric columns\nfrom sklearn.preprocessing import StandardScaler\nscale = StandardScaler()\nall_dummies_scaled = all_dummies.copy()\nall_dummies_scaled[['Age','SibSp','Parch','norm_fare']] = scale.fit_transform(all_dummies_scaled[['Age','SibSp','Parch','norm_fare']])\nall_dummies_scaled.head()","86835991":"X_train_scaled = all_dummies_scaled[all_dummies_scaled.train_test == 1].drop(['train_test'], axis=1)\nX_test_scaled = all_dummies_scaled[all_dummies_scaled.train_test == 0].drop(['train_test'], axis=1)","2d1fa6a9":"X_train_scaled.shape, X_test_scaled.shape","8b7a74a0":"from sklearn.model_selection import cross_val_score\nfrom sklearn.naive_bayes import GaussianNB, MultinomialNB\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn import tree\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC\nfrom xgboost import XGBClassifier","ec83eaa8":"# using Naive Bayes as the baseline for classification tasks\ngnb = GaussianNB()\ncv = cross_val_score(estimator=gnb,X=X_train_scaled,y=y_train,cv=5 )\nprint(cv)\nprint(cv.mean())","360757a7":"# Logictic Regression with scaled data\nlr = LogisticRegression(max_iter=2000)\ncv = cross_val_score(estimator=lr,X=X_train_scaled,y=y_train,cv=5)\nprint(cv)\nprint(cv.mean())","fd7f481c":"# Logistic Regression with non scaled data \nlr = LogisticRegression(max_iter=2000)\ncv = cross_val_score(estimator=lr,X=X_train,y=y_train,cv=5)\nprint(cv)\nprint(cv.mean())","560a2a44":"# Decision Tree without scaled data\ndt = tree.DecisionTreeClassifier(random_state=1)\ncv = cross_val_score(estimator=dt,X=X_train,y=y_train,cv=5)\nprint(cv)\nprint(cv.mean())","30d331cc":"# Decision Tree with scaled data\ndt = tree.DecisionTreeClassifier(random_state=1)\ncv = cross_val_score(estimator=dt,X=X_train_scaled,y=y_train,cv=5)\nprint(cv)\nprint(cv.mean())","3922e060":"# KNN without scaled data\nknn = KNeighborsClassifier()\ncv = cross_val_score(estimator=knn,X=X_train,y=y_train,cv=5)\nprint(cv)\nprint(cv.mean())","bba3f461":"# KNN with scaled data\nknn = KNeighborsClassifier()\ncv = cross_val_score(estimator=knn,X=X_train_scaled,y=y_train,cv=5)\nprint(cv)\nprint(cv.mean())","75620cc8":"# Random Forest without scaled data\nrf = RandomForestClassifier(random_state=1)\ncv = cross_val_score(estimator=rf,X=X_train,y=y_train,cv=5)\nprint(cv)\nprint(cv.mean())","53aa9df5":"# Random Forest with scaled data\nrf = RandomForestClassifier(random_state=1)\ncv = cross_val_score(estimator=rf,X=X_train_scaled,y=y_train,cv=5)\nprint(cv)\nprint(cv.mean())","8c2090de":"# Support Vector Classifier without scaled data\nsvc = SVC(probability=True)\ncv = cross_val_score(estimator=svc,X=X_train,y=y_train,cv=5)\nprint(cv)\nprint(cv.mean())","6b6114be":"# Support Vector Classifier with scaled data\nsvc = SVC(probability=True)\ncv = cross_val_score(estimator=svc,X=X_train_scaled,y=y_train,cv=5)\nprint(cv)\nprint(cv.mean())","0df7434a":"from xgboost import XGBClassifier\n# Support Vector Classifier without scaled data\nxgb = XGBClassifier(random_state=1)\ncv = cross_val_score(estimator=xgb,X=X_train,y=y_train,cv=5)\nprint(cv)\nprint(cv.mean())","d85b21d2":"# Support Vector Classifier with scaled data\nxgb = XGBClassifier(random_state=1)\ncv = cross_val_score(estimator=xgb,X=X_train_scaled,y=y_train,cv=5)\nprint(cv)\nprint(cv.mean())","2c4d69b6":"### Voting classifier takes the output from several classifiers and combines them to provide output. \n# HARD voting - each classifier gets one vote and the most popular outcome is selected. slect odd number of classifiers\n# SOFT voting - averages the confidence is each model. If the average is >50%, output is 1.\nfrom sklearn.ensemble import VotingClassifier\nvc = VotingClassifier(estimators=[('lr',lr),('knn',knn),('rf',rf),('gnb',gnb),('svc',svc),('xgb',xgb)], voting='soft')\ncv = cross_val_score(estimator=vc,X=X_train_scaled,y=y_train,cv=5)\nprint(cv)\nprint(cv.mean())","67fd1b76":"vc.fit(X=X_train_scaled, y=y_train)\ny_hat_base_vc = vc.predict(X=X_test_scaled)\nbasic_submission = {'PassengerID':testing.PassengerId,'Survived':y_hat_base_vc}\nbase_submission = pd.DataFrame(data=basic_submission)\n# base_submission.to_csv('base_sumission.csv', index=False)","91cb0b7d":"from sklearn.model_selection import GridSearchCV, RandomizedSearchCV","c5a3dd15":"# simplify performance reporting of classifier\ndef clf_performance(classifier, model_name):\n    print(model_name)\n    print(f\"Best Score: {str(classifier.best_score_)}\")\n    print(f\"Best Parameters: {str(classifier.best_params_)}\")","d02fdafd":"lr = LogisticRegression()\nparam_grid = {'max_iter': [2000],\n             'penalty': ['l1','l2'],\n             'C': np.logspace(-4,4,20),\n             'solver': ['liblinear']}\n\nclf_lr = GridSearchCV(estimator=lr, param_grid=param_grid, cv=5, verbose=True, n_jobs=-1)\nbest_clf_lr = clf_lr.fit(X_train_scaled, y_train)\nclf_performance(classifier=best_clf_lr, model_name='Logistic Regression')","3ede5e3c":"knn = KNeighborsClassifier()\nparam_grid = {'n_neighbors': [3,5,7,9],\n             'weights': ['uniform','distance'],\n             'algorithm': ['auto','ball_tree','kd_tree'],\n             'p': [1,2]}\n\nclf_knn = GridSearchCV(estimator=knn, param_grid=param_grid, cv=5, verbose=True, n_jobs=-1)\nbest_clf_knn = clf_knn.fit(X_train_scaled, y_train)\nclf_performance(classifier=best_clf_knn, model_name='KNN')","f50a7aba":"svc = SVC(probability=True)\nparam_grid = [{'kernel': ['rbf'],'gamma': [.1,.5,1,2,5,10],'C': [.1,1,10,100,1000]},\n              {'kernel': ['linear'],'C': [.1,1,10,100,1000]},\n              {'kernel': ['poly'],'degree':[2,3,4,5],'C':[.1,1,10,100,1000]}]\n\nclf_svc = GridSearchCV(estimator=svc, param_grid=param_grid, cv=5, verbose=True, n_jobs=-1)\nbest_clf_svc = clf_svc.fit(X_train_scaled, y_train)\nclf_performance(classifier=best_clf_svc, model_name='SVC')","54f02cc4":"# using Randon search CV because the feature space is large\n\nrf = RandomForestClassifier()\nparam_grid = {'n_estimators': [400,450,500,550],\n              'criterion': ['gini','entropy'],\n             'bootstrap': [True],\n             'max_depth': [15,20,25],\n             'max_features': ['auto','sqrt',10],\n             'min_samples_leaf': [2,3],\n             'min_samples_split': [2,3]}\n\nclf_rf_rnd = RandomizedSearchCV(estimator=rf, param_distributions=param_grid, n_iter=100, cv=5, verbose=True, n_jobs=-1)\nbest_clf_rf_rnd = clf_rf_rnd.fit(X_train_scaled, y_train)\nclf_performance(classifier=best_clf_rf_rnd, model_name='Random Forest')","fab4dab6":"# Plotting feature importance\nbest_rf = best_clf_rf_rnd.best_estimator_.fit(X_train_scaled,y_train)\nfeat_importance = pd.Series(best_rf.feature_importances_,index=X_train_scaled.columns)\nfeat_importance.nlargest(15).plot(kind='barh')","0137708a":"xgb = XGBClassifier(random_state = 1)\nparam_grid = {'n_estimators': [20,50,100,250,500,1000],\n              'colsample_bytree': [0.2,0.5,0.7,0.8,1],\n             'max_depth': [2,5,10,15,20,25,None],\n             'reg_alpha': [0,0.5,1],\n              'reg_lambda': [1,1.5,2],\n              'subsample': [0.5,0.6,0.7,0.8,0.9],\n              'learning_rate': [.01,0.1,0.2,0.3,0.5,0.7,0.9],\n             'gamma': [0,.01,.1,1,10,100],\n             'min_child_weight': [0,.01,0.1,1,10,100],\n             'sampling_method': ['uniform','gradiant_based']}\n\n# clf_xgb = GridSearchCV(estimator=xgb, param_grid=param_grid, cv=5, verbose=True, n_jobs=-1)\nclf_xgb = RandomizedSearchCV(estimator=xgb, param_distributions=param_grid,n_iter=1000, cv=5, verbose=True, n_jobs=-1)\nbest_clf_xgb = clf_xgb.fit(X_train_scaled, y_train)\nclf_performance(classifier=best_clf_xgb, model_name='XGBoost')","f6a368ce":"y_hat_xgb = best_clf_xgb.best_estimator_.predict(X_test_scaled)\nxgb_submissions = {'PassengerID':testing.PassengerId,'Survived':y_hat_xgb}\nxgb_submissions = pd.DataFrame(data=xgb_submissions)\n# xgb_submissions.to_csv('xgb_sumission.csv', index=False)","a003e106":"# using voting classifier with tuned models\nbest_lr = best_clf_lr.best_estimator_\nbest_knn = best_clf_knn.best_estimator_\nbest_xgb = best_clf_xgb.best_estimator_\nbest_svc = best_clf_svc.best_estimator_\nbest_rf = best_clf_rf_rnd.best_estimator_","5414452b":"# Hard voting classifier with 3 estimaors - KNN, RF, SVC\nvoting_clf_hard = VotingClassifier(estimators=[('knn',best_knn),('rf',best_rf),('svc',best_svc)], voting='hard')\nprint(f'Voting clf hard: {cross_val_score(estimator=voting_clf_hard, X=X_train_scaled,y=y_train,cv=5).mean()}')\n\n# Soft voting classifier with 3 estimaors - KNN, RF, SVC\nvoting_clf_soft = VotingClassifier(estimators=[('knn',best_knn),('rf',best_rf),('svc',best_svc)], voting='soft')\nprint(f'Voting clf soft: {cross_val_score(estimator=voting_clf_soft, X=X_train_scaled,y=y_train,cv=5).mean()}')\n\n# Soft voting classifier with all estimaors performing better than 80% - KNN, RF, SVC, LR\nvoting_clf_80up = VotingClassifier(estimators=[('lr',best_lr),('knn',best_knn),('rf',best_rf),('svc',best_svc)], \n                                   voting='soft')\nprint(f'Voting clf 80%: {cross_val_score(estimator=voting_clf_80up, X=X_train_scaled,y=y_train,cv=5).mean()}')\n\n# Soft voting classifier with all estimaors performing better than 80% - KNN, RF, SVC, LR\nvoting_clf_all = VotingClassifier(estimators=[('lr',best_lr),('knn',best_knn),('rf',best_rf),('svc',best_svc),\n                                               ('xgb',best_xgb)], voting='soft')\nprint(f'Voting clf ALL: {cross_val_score(estimator=voting_clf_all, X=X_train_scaled,y=y_train,cv=5).mean()}')","ebf7e166":"# fine tuning voting classifier with weights for each estimator\n\nparams = {'weights':[[1,1,1],[1,2,1],[1,1,2],[2,1,1],[2,2,1],[1,2,2],[1,1,2]]}\n\nvote_weight = GridSearchCV(estimator=voting_clf_soft, param_grid=params, cv=5, verbose=True, n_jobs=-1)\nbest_clf_weights = vote_weight.fit(X_train_scaled,y_train)\nclf_performance(classifier=best_clf_weights, model_name='VC Weights')\n","ac43be67":"# Making predictions with voting classifiers\nvoting_clf_hard.fit(X_train_scaled,y_train)\nvoting_clf_soft.fit(X_train_scaled,y_train)\nvoting_clf_80up.fit(X_train_scaled,y_train)\nvoting_clf_all.fit(X_train_scaled,y_train)\nbest_rf.fit(X_train_scaled,y_train)\n\ny_hat_rf = best_rf.predict(X_test_scaled)\ny_hat_vc_hard = voting_clf_hard.predict(X_test_scaled)\ny_hat_vc_soft = voting_clf_soft.predict(X_test_scaled)\ny_hat_vc_80up = voting_clf_80up.predict(X_test_scaled)\ny_hat_vc_all = voting_clf_all.predict(X_test_scaled)","7efe4d28":"# convert output to DF\nfinal_data = {'PassengerID':testing.PassengerId,'Survived':y_hat_rf}\nsubmissions_1 = pd.DataFrame(data=final_data)\n\nfinal_data_2 = {'PassengerID':testing.PassengerId,'Survived':y_hat_vc_hard}\nsubmissions_2 = pd.DataFrame(data=final_data_2)\n\nfinal_data_3 = {'PassengerID':testing.PassengerId,'Survived':y_hat_vc_soft}\nsubmissions_3 = pd.DataFrame(data=final_data_3)\n\nfinal_data_4 = {'PassengerID':testing.PassengerId,'Survived':y_hat_vc_80up}\nsubmissions_4 = pd.DataFrame(data=final_data_4)\n\nfinal_data_5 = {'PassengerID':testing.PassengerId,'Survived':y_hat_vc_all}\nsubmissions_5 = pd.DataFrame(data=final_data_5)\n\nfinal_data_comp = {'PassengerID':testing.PassengerId,'Survived_vc_hard':y_hat_vc_hard, 'Survived_vc_soft':y_hat_vc_soft,\n                  'Survived_vc_80up':y_hat_vc_80up, 'Survived_vc_all':y_hat_vc_all, 'Survived_rf':y_hat_rf}\ncomparison = pd.DataFrame(data=final_data_comp)","9fc1c2e6":"comparison['rf_vs_vchard'] = comparison.apply(lambda x: 1 if x.Survived_rf != x.Survived_vc_hard else 0, axis=1 )\ncomparison['rf_vs_vcsoft'] = comparison.apply(lambda x: 1 if x.Survived_rf != x.Survived_vc_soft else 0, axis=1 )\ncomparison['vchard_vs_vcsoft'] = comparison.apply(lambda x: 1 if x.Survived_vc_hard != x.Survived_vc_soft else 0, axis=1 )\ncomparison['vchard_vs_vcall'] = comparison.apply(lambda x: 1 if x.Survived_vc_hard != x.Survived_vc_all else 0, axis=1 )","ec95a286":"# Prepare submission files\nsubmissions_1.to_csv('submission_rf.csv', index=False)\nsubmissions_2.to_csv('submission_vc_hard.csv', index=False)\nsubmissions_3.to_csv('submission_vc_soft.csv', index=False)\nsubmissions_4.to_csv('submission_vc_80up.csv', index=False)\nsubmissions_5.to_csv('submission_vc_all.csv', index=False)","d8cf3cbd":"# Exploratory Data Analysis","c4c17f47":"Hyperparameter tuning of XGBoost","14755d43":"# Perform Train Test Split","7ca1b47e":"Hyperparameter tuning of Logistic Regression","50652a95":"# Model Building","d7f28b3c":"# Feature Engineering","7fb45d9b":"# Titanic - Data exploration and Prediction \n\nThis is my first attempt at model creation and prediction. Any feedback is appreciated. I would like to credit Ken Jee's project for the inspiration.  (https:\/\/www.kaggle.com\/kenjee\/titanic-project-example).\n","c0ee64db":"Hyperparameter tuning of Support Vector Classifier","62e3328c":"# Model Tuning","95e02dc2":"Hyperparameter tuning of Random Forest","db8fa2bc":"# Additional Ensamble Approach","74784257":"# Reading dataset","987d86d7":"# Preprocessing Data\n\n1. Drop Null\n2. Include only relevent data\n3. Do categorical transformation (One Hot)\n4. Update missing data with mean for fare and age\n5. Normalize Fare using logarithm to give more semblance\n6. scale Data between 0-1 with standard scalar\n","8c701642":"Hyperparameter tuning of KNN"}}