{"cell_type":{"51a178de":"code","30112093":"code","80e86a2e":"code","bbc9ff5a":"code","ca18a0d9":"code","2b2a2f18":"code","64d987cf":"code","f5204102":"code","8655ead4":"code","096117fa":"code","89d9c7bd":"code","1693231b":"code","de21188a":"code","11122755":"code","c77e70ac":"code","df160110":"code","519b7f0d":"code","a4dd4c02":"code","f2ef1117":"code","76f31c4f":"code","020ec2da":"code","99a76704":"code","434e5c49":"code","da091669":"code","47b4262e":"code","4929293a":"code","10033a18":"code","306dd4cf":"code","7daa70a0":"code","3d95d81e":"code","702bb015":"code","c8d73afa":"code","204ce728":"code","8a849509":"code","6698b7e1":"code","ce497d6e":"code","1189be01":"code","7816b826":"code","344cf02e":"code","b40cd005":"code","e6ce406c":"code","633bced2":"code","7d9c4fd8":"code","ffd1cb8b":"code","7cbfb348":"code","8ca4c0a1":"code","882e9973":"code","3fbe26fe":"code","4b108c50":"code","20d9be17":"code","665f6cdb":"code","94d24264":"code","9d9711f7":"code","c1b01b2d":"code","12dbdbb8":"code","823d0979":"code","18b5b2eb":"code","fdceb882":"code","31d0a2ee":"code","a48ca140":"code","68a2ecbd":"code","768edad5":"code","b59c8afd":"code","4ff81a30":"code","3ba3a2fa":"code","d91541cd":"code","eeb6fb73":"code","eb0e5dc8":"code","0266ab70":"code","50f98eed":"code","675124d2":"markdown","e2adcd66":"markdown","cc7eec35":"markdown","47ea85ae":"markdown","ef022352":"markdown","c6c00a20":"markdown","87c8cf1d":"markdown","8aff834b":"markdown","80541d53":"markdown","b0c1e392":"markdown","78a642b6":"markdown","6466065b":"markdown","4ee69ccf":"markdown","3b43d88a":"markdown","0e669f5a":"markdown","bcb9f0b7":"markdown","367f34ac":"markdown","603e8073":"markdown","cda52eb1":"markdown","080d513d":"markdown","bf14a2ae":"markdown","4a457f82":"markdown","73dae303":"markdown","d8871a56":"markdown","542bf40a":"markdown","1cc8af97":"markdown","60e2aabd":"markdown","7344bb23":"markdown","155a300c":"markdown","f52d0b5c":"markdown","d6d8a4a6":"markdown","90faab35":"markdown","76b8abcf":"markdown","669c0f77":"markdown","9d484450":"markdown","404c4e07":"markdown","57314ae6":"markdown","90cf80ef":"markdown","a8edb429":"markdown","25c8f798":"markdown","6d39d2e4":"markdown","6d041e1b":"markdown","ed5b8cfe":"markdown","c2e843a3":"markdown","558f317e":"markdown","036ab4d4":"markdown","30b1b3b6":"markdown","d3b5358a":"markdown","5d006723":"markdown","280234c2":"markdown","b7a2c691":"markdown","f95ee64e":"markdown","03aa1169":"markdown","1ad688e2":"markdown"},"source":{"51a178de":"import os\nimport pandas as pd\nimport numpy as np\nimport json\nimport matplotlib.pyplot as plt\nimport datetime as datetime\nfrom datetime import timedelta, date\nimport seaborn as sns\nimport matplotlib.cm as CM\nimport lightgbm as lgb\nfrom sklearn import preprocessing\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import GridSearchCV, train_test_split\n%matplotlib inline","30112093":"train_data = pd.read_csv(\"..\/input\/train_v2.csv\",nrows=700000)\ntrain_data.head()","80e86a2e":"train_data.describe()","bbc9ff5a":"list(train_data.columns.values)","ca18a0d9":"train_data.channelGrouping.value_counts().plot(kind=\"bar\",title=\"channelGrouping distro\",figsize=(8,8),rot=25,colormap='Paired')","2b2a2f18":"\"date :{}, visitStartTime:{}\".format(train_data.head(1).date[0],train_data.head(1).visitStartTime[0])","64d987cf":"train_data[\"date\"] = pd.to_datetime(train_data[\"date\"],format=\"%Y%m%d\")\ntrain_data[\"visitStartTime\"] = pd.to_datetime(train_data[\"visitStartTime\"],unit='s')","f5204102":"train_data.head(1)[[\"date\",\"visitStartTime\"]]","8655ead4":"list_of_devices = train_data.device.apply(json.loads).tolist()\nkeys = []\nfor devices_iter in list_of_devices:\n    for list_element in list(devices_iter.keys()):\n        if list_element not in keys:\n            keys.append(list_element)","096117fa":"\"keys existed in device attribute are:{}\".format(keys)","89d9c7bd":"tmp_device_df = pd.DataFrame(train_data.device.apply(json.loads).tolist())[[\"browser\",\"operatingSystem\",\"deviceCategory\",\"isMobile\"]]","1693231b":"tmp_device_df.head()","de21188a":"tmp_device_df.describe()","11122755":"fig, axes = plt.subplots(2,2,figsize=(15,15))\ntmp_device_df[\"isMobile\"].value_counts().plot(kind=\"bar\",ax=axes[0][0],rot=25,legend=\"isMobile\",color='tan')\ntmp_device_df[\"browser\"].value_counts().head(10).plot(kind=\"bar\",ax=axes[0][1],rot=40,legend=\"browser\",color='teal')\ntmp_device_df[\"deviceCategory\"].value_counts().head(10).plot(kind=\"bar\",ax=axes[1][0],rot=25,legend=\"deviceCategory\",color='lime')\ntmp_device_df[\"operatingSystem\"].value_counts().head(10).plot(kind=\"bar\",ax=axes[1][1],rot=80,legend=\"operatingSystem\",color='c')","c77e70ac":"tmp_geo_df = pd.DataFrame(train_data.geoNetwork.apply(json.loads).tolist())[[\"continent\",\"subContinent\",\"country\",\"city\"]]","df160110":"tmp_geo_df.head()","519b7f0d":"tmp_geo_df.describe()","a4dd4c02":"fig, axes = plt.subplots(3,2, figsize=(15,15))\ntmp_geo_df[\"continent\"].value_counts().plot(kind=\"bar\",ax=axes[0][0],title=\"Global Distributions\",rot=0,color=\"c\")\ntmp_geo_df[tmp_geo_df[\"continent\"] == \"Americas\"][\"subContinent\"].value_counts().plot(kind=\"bar\",ax=axes[1][0], title=\"America Distro\",rot=0,color=\"tan\")\ntmp_geo_df[tmp_geo_df[\"continent\"] == \"Asia\"][\"subContinent\"].value_counts().plot(kind=\"bar\",ax=axes[0][1], title=\"Asia Distro\",rot=0,color=\"r\")\ntmp_geo_df[tmp_geo_df[\"continent\"] == \"Europe\"][\"subContinent\"].value_counts().plot(kind=\"bar\",ax=axes[1][1],  title=\"Europe Distro\",rot=0,color=\"lime\")\ntmp_geo_df[tmp_geo_df[\"continent\"] == \"Oceania\"][\"subContinent\"].value_counts().plot(kind=\"bar\",ax = axes[2][0], title=\"Oceania Distro\",rot=0,color=\"teal\")\ntmp_geo_df[tmp_geo_df[\"continent\"] == \"Africa\"][\"subContinent\"].value_counts().plot(kind=\"bar\" , ax=axes[2][1], title=\"Africa Distro\",rot=0,color=\"silver\")","f2ef1117":"train_data[\"socialEngagementType\"].describe()","76f31c4f":"train_data.head()\ntrain_data[\"revenue\"] = pd.DataFrame(train_data.totals.apply(json.loads).tolist())[[\"transactionRevenue\"]]\n","020ec2da":"revenue_datetime_df = train_data[[\"revenue\" , \"date\"]].dropna()\nrevenue_datetime_df[\"revenue\"] = revenue_datetime_df.revenue.astype(np.int64)\nrevenue_datetime_df.head()","99a76704":"daily_revenue_df = revenue_datetime_df.groupby(by=[\"date\"],axis = 0 ).sum()\nimport matplotlib.pyplot as plt\nfig, axes = plt.subplots(figsize=(20,10))\naxes.set_title(\"Daily Revenue\")\naxes.set_ylabel(\"Revenue\")\naxes.set_xlabel(\"date\")\naxes.plot(daily_revenue_df[\"revenue\"])\n","434e5c49":"fig, axes = plt.subplots(nrows=1, ncols=1, figsize=(9, 9))\naxes.set_title(\"Daily revenue Violin\")\naxes.set_ylabel(\"revenue\")\naxes.violinplot(list(daily_revenue_df[\"revenue\"].values),showmeans=False,showmedians=True)","da091669":"visit_datetime_df = train_data[[\"date\",\"visitNumber\"]]\nvisit_datetime_df[\"visitNumber\"] = visit_datetime_df.visitNumber.astype(np.int64)","47b4262e":"daily_visit_df = visit_datetime_df.groupby(by=[\"date\"], axis = 0).sum()\n\nfig, axes = plt.subplots(1,1,figsize=(20,10))\naxes.set_ylabel(\"# of visits\")\naxes.set_xlabel(\"date\")\naxes.set_title(\"Daily Visits\")\naxes.plot(daily_visit_df[\"visitNumber\"])","4929293a":"fig, axes = plt.subplots(nrows=1, ncols=1, figsize=(9, 9))\naxes.set_title(\"Daily visits Violin\")\naxes.set_ylabel(\"# of visitors\")\naxes.violinplot(list(daily_visit_df[\"visitNumber\"].values),showmeans=False,showmedians=True)","10033a18":"train_data.visitNumber.describe()","306dd4cf":"\"90 percent of sessions have visitNumber lower than {} times.\".format(np.percentile(list(train_data.visitNumber),90))","7daa70a0":"import collections\n\ntmp_least_10_visitNumbers_list = collections.Counter(list(train_data.visitNumber)).most_common()[:-10-1:-1]\ntmp_most_10_visitNumbers_list = collections.Counter(list(train_data.visitNumber)).most_common(10)\nleast_visitNumbers = []\nmost_visitNumbers = []\nfor i in tmp_least_10_visitNumbers_list:\n    least_visitNumbers.append(i[0])\nfor i in tmp_most_10_visitNumbers_list:\n    most_visitNumbers.append(i[0])\n\"10 most_common visitNumbers are {} times and 10 least_common visitNumbers are {} times\".format(most_visitNumbers,least_visitNumbers)","3d95d81e":"fig,ax = plt.subplots(1,1,figsize=(9,5))\nax.set_title(\"Histogram of log(visitNumbers) \\n don't forget it is per session\")\nax.set_ylabel(\"Repetition\")\nax.set_xlabel(\"Log(visitNumber)\")\nax.grid(color='b', linestyle='-', linewidth=0.1)\nax.hist(np.log(train_data.visitNumber))","702bb015":"traffic_source_df = pd.DataFrame(train_data.trafficSource.apply(json.loads).tolist())[[\"keyword\",\"medium\" , \"source\"]]","c8d73afa":"fig,axes = plt.subplots(1,2,figsize=(15,10))\ntraffic_source_df[\"medium\"].value_counts().plot(kind=\"bar\",ax = axes[0],title=\"Medium\",rot=0,color=\"tan\")\ntraffic_source_df[\"source\"].value_counts().head(10).plot(kind=\"bar\",ax=axes[1],title=\"source\",rot=75,color=\"teal\")","204ce728":"traffic_source_df.loc[traffic_source_df[\"source\"].str.contains(\"google\") ,\"source\"] = \"google\"\nfig,axes = plt.subplots(1,1,figsize=(8,8))\ntraffic_source_df[\"source\"].value_counts().head(15).plot(kind=\"bar\",ax=axes,title=\"source\",rot=75,color=\"teal\")","8a849509":"fig,axes = plt.subplots(1,2,figsize=(15,10))\ntraffic_source_df[\"keyword\"].value_counts().head(10).plot(kind=\"bar\",ax=axes[0], title=\"keywords (total)\",color=\"orange\")\ntraffic_source_df[traffic_source_df[\"keyword\"] != \"(not provided)\"][\"keyword\"].value_counts().head(15).plot(kind=\"bar\",ax=axes[1],title=\"keywords (dropping NA)\",color=\"c\")","6698b7e1":"repetitive_users = list(np.sort(list(collections.Counter(list(train_data[\"fullVisitorId\"])).values())))\n\"25% percentile: {}, 50% percentile: {}, 75% percentile: {}, 88% percentile: {}, 88% percentile: {}\".format(\nnp.percentile(repetitive_users,q=25),np.percentile(repetitive_users,q=50),\nnp.percentile(repetitive_users,q=75),np.percentile(repetitive_users,q=88), np.percentile(repetitive_users,q=89))","ce497d6e":"date_list = np.sort(list(set(list(train_data[\"date\"]))))\n\"first_day:'{}' and last_day:'{}' and toal number of data we have is: '{}' days.\".format(date_list[0], date_list[-1],len(set(list(train_data[\"date\"]))))","1189be01":"month = 8\nstart_date = datetime.date(2016, month, 1)\nend_date = datetime.date(2017, month, 1)\ndef daterange(start_date, end_date):\n    for n in range(int((end_date - start_date).days)):\n        yield start_date + timedelta(n)\ndates_month = []\nfor single_date in daterange(start_date, end_date):\n    dates_month.append(single_date.strftime(\"%Y-%m\"))\ndates_month = list(set(dates_month))\ndates_month","7816b826":"tmp_churn_df = pd.DataFrame()\ntmp_churn_df[\"date\"] = train_data[\"date\"]\ntmp_churn_df[\"yaer\"] = pd.DatetimeIndex(tmp_churn_df[\"date\"]).year\ntmp_churn_df[\"month\"] =pd.DatetimeIndex(tmp_churn_df[\"date\"]).month\ntmp_churn_df[\"fullVisitoId\"] = train_data[\"fullVisitorId\"]\ntmp_churn_df.head()","344cf02e":"\"distinct users who visited the website on 2016-08 are:'{}'persons\".format(len(set(tmp_churn_df[(tmp_churn_df.yaer == 2016) & (tmp_churn_df.month == 8) ][\"fullVisitoId\"])))","b40cd005":"target_intervals_list = [(2016,8),(2016,9),(2016,10),(2016,11),(2016,12),(2017,1),(2017,2),(2017,3),(2017,4),(2017,5),(2017,6),(2017,7)]\nintervals_visitors = []\nfor tmp_tuple in target_intervals_list:\n    intervals_visitors.append(tmp_churn_df[(tmp_churn_df.yaer == tmp_tuple[0]) & (tmp_churn_df.month == tmp_tuple[1]) ][\"fullVisitoId\"])\n\"Size of intervals_visitors:{} \".format(len(intervals_visitors))","e6ce406c":"tmp_matrix = np.zeros((11,11))\n\nfor i in range(0,11):\n    k = False\n    tmp_set = []\n    for j in range(i,11): \n        if k:\n            tmp_set = tmp_set & set(intervals_visitors[j])\n        else:\n            tmp_set = set(intervals_visitors[i]) & set(intervals_visitors[j])\n        tmp_matrix[i][j] = len(list(tmp_set))\n        k = True","633bced2":"xticklabels = [\"interval 1\",\"interval 2\",\"interval 3\",\"interval 4\",\"interval 5\",\"interval 6\",\"interval 7\",\"interval 8\",\n              \"interval 9\",\"interval 10\",\"interval 11\"]\nyticklabels = [(2016,8),(2016,9),(2016,10),(2016,11),(2016,12),(2017,1),(2017,2),(2017,3),(2017,4),(2017,5),(2017,6),(2017,7)]\nfig, ax = plt.subplots(figsize=(11,11))\nax = sns.heatmap(np.array(tmp_matrix,dtype=int), annot=True, cmap=\"RdBu_r\",xticklabels=xticklabels,fmt=\"d\",yticklabels=yticklabels)\nax.set_title(\"Churn-rate heatmap\")\nax.set_xlabel(\"intervals\")\nax.set_ylabel(\"months\")\n","7d9c4fd8":"A = tmp_matrix\nmask =  np.tri(A.shape[0], k=-1)\nA = np.ma.array(A, mask=mask) # mask out the lower triangle\nfig = plt.figure(figsize=(9,9))\nax = fig.add_subplot(111)\nax.set_xlabel(\"interval\")\nax.set_ylabel(\"period\")\ncmap = CM.get_cmap('RdBu_r', 50000) \ncmap.set_bad('w') # default value is 'k'\nax.imshow(A, interpolation=\"nearest\", cmap=cmap)","ffd1cb8b":"revenue_datetime_df = train_data[[\"revenue\" , \"date\"]].dropna()\nrevenue_datetime_df[\"revenue\"] = revenue_datetime_df.revenue.astype(np.int64)\nrevenue_datetime_df.head()","7cbfb348":"total_revenue_daily_df = revenue_datetime_df.groupby(by=[\"date\"],axis=0).sum()\ntotal_revenue_daily_df.head()","8ca4c0a1":"total_visitNumber_daily_df = train_data[[\"date\",\"visitNumber\"]].groupby(by=[\"date\"],axis=0).sum()\ntotal_visitNumber_daily_df.head()","882e9973":"datetime_revenue_visits_df = pd.concat([total_revenue_daily_df,total_visitNumber_daily_df],axis=1)\n\nfig, ax1 = plt.subplots(figsize=(20,10))\nt = datetime_revenue_visits_df.index\ns1 = datetime_revenue_visits_df[\"visitNumber\"]\nax1.plot(t, s1, 'b-')\nax1.set_xlabel('day')\n# Make the y-axis label, ticks and tick labels match the line color.\nax1.set_ylabel('visitNumber', color='b')\nax1.tick_params('y', colors='b')\n\nax2 = ax1.twinx()\ns2 = datetime_revenue_visits_df[\"revenue\"]\nax2.plot(t, s2, 'r--')\nax2.set_ylabel('revenue', color='r')\nax2.tick_params('y', colors='r')\nfig.tight_layout()","3fbe26fe":"revenue_df = train_data.dropna(subset=[\"revenue\"])\nrevenue_os_df = pd.DataFrame(revenue_df.device.apply(json.loads).tolist())[[\"browser\",\"operatingSystem\",\"deviceCategory\",\"isMobile\"]]\n\nbuys_is_mobile_dict = dict(collections.Counter(list(revenue_os_df.isMobile)))\npercent_buys_is_mobile_dict = {k: v \/ total for total in (sum(buys_is_mobile_dict.values()),) for k, v in buys_is_mobile_dict.items()}\nsizes = list(percent_buys_is_mobile_dict.values())\nexplode=(0,0.1)\nlabels = 'isNotMobile', 'isMobile'\nfig, ax = plt.subplots(1,1, figsize=(8,8))\nax.set_title(\"buys mobile distro\")\nax.pie(sizes, explode=explode, labels=labels, autopct='%1.1f%%',shadow=True, startangle=90)","4b108c50":"mobiles_browsers = dict(collections.Counter(revenue_os_df[revenue_os_df[\"isMobile\"] == True][\"browser\"]))\nnot_mobiles_browsers = dict(collections.Counter(revenue_os_df[revenue_os_df[\"isMobile\"] == False][\"browser\"]))\nprint(\"for mobile users:\")\nfor i,v in mobiles_browsers.items():\n    print(\"{}:{}\".format(i,v))\nprint(\"\\nfor not mobile users:\")\nfor i,v in not_mobiles_browsers.items():\n    print(\"{}:{}\".format(i,v))\nvals = np.array([[552.,6.,2.,12.,16.,431.], [9801.,189.,58.,93.,5.,349.]])\n\nfig, ax = plt.subplots(subplot_kw=dict(polar=True),figsize=(9,9))\nsize = 0.3\nvalsnorm = vals \/ np.sum(vals) * 2 * np.pi\n\n# obtain the ordinates of the bar edges\nvalsleft = np.cumsum(np.append(0, valsnorm.flatten()[:-1])).reshape(vals.shape)\n\ncmap = plt.get_cmap(\"tab20c\")\nouter_colors = cmap(np.arange(3) * 4)\ninner_colors = cmap(np.array([1, 2, 5, 6, 9, 10]))\n\nax.bar(x=valsleft[:, 0],\n       width=valsnorm.sum(axis=1), bottom=1 - size, height=size,\n       color=outer_colors, edgecolor='w', linewidth=1, align=\"edge\")\n\nax.bar(x=valsleft.flatten(),\n       width=valsnorm.flatten(), bottom=1 - 2 * size, height=size,\n       color=inner_colors, edgecolor='w', linewidth=1, align=\"edge\")\n# ax.set_axis_off()\n\nax.set(title=\"Nested pi-plot for buyers devices.\")","20d9be17":"df_train = train_data.drop([\"date\", \"socialEngagementType\", \"visitStartTime\", \"visitId\", \"fullVisitorId\" , \"revenue\",\"customDimensions\"], axis=1)\n\ndevices_df = pd.DataFrame(df_train.device.apply(json.loads).tolist())[[\"browser\", \"operatingSystem\", \"deviceCategory\", \"isMobile\"]]\ngeo_df = pd.DataFrame(df_train.geoNetwork.apply(json.loads).tolist())[[\"continent\", \"subContinent\", \"country\", \"city\"]]\ntraffic_source_df = pd.DataFrame(df_train.trafficSource.apply(json.loads).tolist())[[\"keyword\", \"medium\", \"source\"]]\ntotals_df = pd.DataFrame(df_train.totals.apply(json.loads).tolist())[[\"transactionRevenue\", \"newVisits\", \"bounces\", \"pageviews\", \"hits\"]]\n\n\ndf_train = pd.concat([df_train.drop([\"hits\"],axis=1), devices_df, geo_df, traffic_source_df, totals_df], axis=1)\ndf_train = df_train.drop([\"device\", \"geoNetwork\", \"trafficSource\", \"totals\"], axis=1)\n","665f6cdb":"df_train.head(1)","94d24264":"df_train[\"transactionRevenue\"] = df_train[\"transactionRevenue\"].fillna(0)\ndf_train[\"bounces\"] = df_train[\"bounces\"].fillna(0)\ndf_train[\"pageviews\"] = df_train[\"pageviews\"].fillna(0)\ndf_train[\"hits\"] = df_train[\"hits\"].fillna(0)\ndf_train[\"newVisits\"] = df_train[\"newVisits\"].fillna(0)","9d9711f7":"df_train, df_test = train_test_split(df_train, test_size=0.2, random_state=42)\n\ndf_train[\"transactionRevenue\"] = df_train[\"transactionRevenue\"].astype(np.float)\ndf_test[\"transactionRevenue\"] = df_test[\"transactionRevenue\"].astype(np.float)\n\"Finaly, we have these columns for our regression problems: {}\".format(df_train.columns)","c1b01b2d":"df_train.head(1)","12dbdbb8":"categorical_features = ['channelGrouping', 'browser', 'operatingSystem', 'deviceCategory', 'isMobile',\n                        'continent', 'subContinent', 'country', 'city', 'keyword', 'medium', 'source']\n\nnumerical_features = ['visitNumber', 'newVisits', 'bounces', 'pageviews', 'hits']\n\nfor column_iter in categorical_features:\n    lbl = preprocessing.LabelEncoder()\n    lbl.fit(list(df_train[column_iter].values.astype('str')) + list(df_test[column_iter].values.astype('str')))\n    df_train[column_iter] = lbl.transform(list(df_train[column_iter].values.astype('str')))\n    df_test[column_iter] = lbl.transform(list(df_test[column_iter].values.astype('str')))\n\nfor column_iter in numerical_features:\n    df_train[column_iter] = df_train[column_iter].astype(np.float)\n    df_test[column_iter] = df_test[column_iter].astype(np.float)","823d0979":"params = {\n    \"objective\": \"regression\",\n    \"metric\": \"rmse\",\n    \"num_leaves\": 30,\n    \"min_child_samples\": 100,\n    \"learning_rate\": 0.1,\n    \"bagging_fraction\": 0.7,\n    \"feature_fraction\": 0.5,\n    \"bagging_frequency\": 5,\n    \"bagging_seed\": 2018,\n    \"verbosity\": -1\n}\nlgb_train = lgb.Dataset(df_train.loc[:,df_train.columns != \"transactionRevenue\"], np.log1p(df_train.loc[:,\"transactionRevenue\"]))\nlgb_eval = lgb.Dataset(df_test.loc[:,df_test.columns != \"transactionRevenue\"], np.log1p(df_test.loc[:,\"transactionRevenue\"]), reference=lgb_train)\ngbm = lgb.train(params, lgb_train, num_boost_round=2000, valid_sets=[lgb_eval], early_stopping_rounds=100,verbose_eval=100)","18b5b2eb":"predicted_revenue = gbm.predict(df_test.loc[:,df_test.columns != \"transactionRevenue\"], num_iteration=gbm.best_iteration)\npredicted_revenue[predicted_revenue < 0] = 0 \ndf_test[\"predicted\"] = np.expm1(predicted_revenue)\ndf_test[[\"transactionRevenue\",\"predicted\"]].head(10)","fdceb882":"import gc; gc.collect()\nimport time; time.sleep(5)\n\ndf_train = pd.read_csv(filepath_or_buffer=\"..\/input\/train_v2.csv\",nrows=50000)\ndf_actual_test = pd.read_csv(filepath_or_buffer=\"..\/input\/test_v2.csv\",nrows=25000)\n\n# drop useless features => date, fullVisitorId, sessionId, socialEngagement, visitStartTime\ndf_train = df_train.drop([\"date\", \"socialEngagementType\", \"visitStartTime\", \"visitId\", \"fullVisitorId\"], axis=1)\ndf_actual_test = df_actual_test.drop([\"date\", \"socialEngagementType\", \"visitStartTime\", \"visitId\"], axis=1)\n\n\n#preprocessing for trains\ndevices_df = pd.DataFrame(df_train.device.apply(json.loads).tolist())[[\"browser\", \"operatingSystem\", \"deviceCategory\", \"isMobile\"]]\ngeo_df = pd.DataFrame(df_train.geoNetwork.apply(json.loads).tolist())[[\"continent\", \"subContinent\", \"country\", \"city\"]]\ntraffic_source_df = pd.DataFrame(df_train.trafficSource.apply(json.loads).tolist())[[\"keyword\", \"medium\", \"source\"]]\ntotals_df = pd.DataFrame(df_train.totals.apply(json.loads).tolist())[[\"transactionRevenue\", \"newVisits\", \"bounces\", \"pageviews\", \"hits\"]]\ndf_train = pd.concat([df_train.drop([\"hits\"],axis=1), devices_df, geo_df, traffic_source_df, totals_df], axis=1)\ndf_train = df_train.drop([\"device\", \"geoNetwork\", \"trafficSource\", \"totals\"], axis=1)\ndf_train[\"transactionRevenue\"] = df_train[\"transactionRevenue\"].fillna(0)\ndf_train[\"bounces\"] = df_train[\"bounces\"].fillna(0)\ndf_train[\"pageviews\"] = df_train[\"pageviews\"].fillna(0)\ndf_train[\"hits\"] = df_train[\"hits\"].fillna(0)\ndf_train[\"newVisits\"] = df_train[\"newVisits\"].fillna(0)\n\n#preprocessing for tests\ndevices_df = pd.DataFrame(df_actual_test.device.apply(json.loads).tolist())[[\"browser\", \"operatingSystem\", \"deviceCategory\", \"isMobile\"]]\ngeo_df = pd.DataFrame(df_actual_test.geoNetwork.apply(json.loads).tolist())[[\"continent\", \"subContinent\", \"country\", \"city\"]]\ntraffic_source_df = pd.DataFrame(df_actual_test.trafficSource.apply(json.loads).tolist())[[\"keyword\", \"medium\", \"source\"]]\ntotals_df = pd.DataFrame(df_actual_test.totals.apply(json.loads).tolist())[[\"newVisits\", \"bounces\", \"pageviews\", \"hits\"]]\ndf_actual_test = pd.concat([df_actual_test.drop([\"hits\"],axis=1), devices_df, geo_df, traffic_source_df, totals_df], axis=1)\ndf_actual_test = df_actual_test.drop([\"device\", \"geoNetwork\", \"trafficSource\", \"totals\"], axis=1)\n# df_actual_test[\"transactionRevenue\"] = df_train[\"transactionRevenue\"].fillna(0)\ndf_actual_test[\"bounces\"] = df_train[\"bounces\"].fillna(0)\ndf_actual_test[\"pageviews\"] = df_train[\"pageviews\"].fillna(0)\ndf_actual_test[\"hits\"] = df_train[\"hits\"].fillna(0)\ndf_actual_test[\"newVisits\"] = df_train[\"newVisits\"].fillna(0)\n\n#garbage collector ';-)'\ndel devices_df,geo_df,traffic_source_df,totals_df\n\n\n#evaluation \ndf_train, df_eval = train_test_split(df_train, test_size=0.2, random_state=42)\n\n# lgb_train = lgb.Dataset(df_train.loc[:, df_train.columns != \"revenue\"], df_train[\"revenue\"])\n# lgb_eval = lgb.Dataset(df_test.loc[:, df_test.columns != \"revenue\"], df_test[\"revenue\"], reference=lgb_train)\n\ndf_train[\"transactionRevenue\"] = df_train[\"transactionRevenue\"].astype(np.float)\ndf_eval[\"transactionRevenue\"] = df_eval[\"transactionRevenue\"].astype(np.float)\n\nprint(df_train.columns)","31d0a2ee":"params = {\n    \"objective\": \"regression\",\n    \"metric\": \"rmse\",\n    \"num_leaves\": 30,\n    \"min_child_samples\": 100,\n    \"learning_rate\": 0.1,\n    \"bagging_fraction\": 0.7,\n    \"feature_fraction\": 0.5,\n    \"bagging_frequency\": 5,\n    \"bagging_seed\": 2018,\n    \"verbosity\": -1\n}\n\nprint('Start training...')","a48ca140":"df_actual_test = df_actual_test.drop([\"customDimensions\"],axis=1)\ndf_train = df_train.drop([\"customDimensions\"],axis=1)\ndf_eval = df_eval.drop([\"customDimensions\"],axis=1)","68a2ecbd":"categorical_features = ['channelGrouping', 'browser', 'operatingSystem', 'deviceCategory', 'isMobile',\n                        'continent', 'subContinent', 'country', 'city', 'keyword', 'medium', 'source']\n\nnumerical_features = ['visitNumber', 'newVisits', 'bounces', 'pageviews', 'hits']\n\nfor column_iter in categorical_features:\n    lbl = preprocessing.LabelEncoder()\n    lbl.fit(list(df_train[column_iter].values.astype('str')) + list(df_eval[column_iter].values.astype('str')) + list(df_actual_test[column_iter].values.astype('str')))\n    \n    df_train[column_iter] = lbl.transform(list(df_train[column_iter].values.astype('str')))\n    df_eval[column_iter] = lbl.transform(list(df_eval[column_iter].values.astype('str')))\n    df_actual_test[column_iter] = lbl.transform(list(df_actual_test[column_iter].values.astype('str')))\n\nfor column_iter in numerical_features:\n    df_train[column_iter] = df_train[column_iter].astype(np.float)\n    df_eval[column_iter] = df_eval[column_iter].astype(np.float)\n    df_actual_test[column_iter] = df_actual_test[column_iter].astype(np.float)","768edad5":"lgb_train = lgb.Dataset(df_train.loc[:,df_train.columns != \"transactionRevenue\"], np.log1p(df_train.loc[:,\"transactionRevenue\"]))\nlgb_eval = lgb.Dataset(df_eval.loc[:,df_eval.columns != \"transactionRevenue\"], np.log1p(df_eval.loc[:,\"transactionRevenue\"]), reference=lgb_train)","b59c8afd":"gbm = lgb.train(params, lgb_train, num_boost_round=2000, valid_sets=[lgb_eval], early_stopping_rounds=100,verbose_eval=100)","4ff81a30":"eval_predicted_revenue = gbm.predict(df_eval.loc[:,df_eval.columns != \"transactionRevenue\"], num_iteration=gbm.best_iteration)\neval_predicted_revenue[eval_predicted_revenue < 0] = 0 \ndf_eval[\"predicted\"] = np.expm1(eval_predicted_revenue)\ndf_eval[[\"transactionRevenue\",\"predicted\"]].head()","3ba3a2fa":"actual_predicted_revenue = gbm.predict(df_actual_test.loc[:,df_actual_test.columns != \"fullVisitorId\"], num_iteration=gbm.best_iteration)\nactual_predicted_revenue[actual_predicted_revenue < 0] = 0 \n# df_actual_test[\"predicted\"] = np.expm1(actual_predicted_revenue)\ndf_actual_test[\"predicted\"] = actual_predicted_revenue\ndf_actual_test.head()\n\ndf_actual_test = df_actual_test[[\"fullVisitorId\" , \"predicted\"]]\ndf_actual_test[\"fullVisitorId\"] = df_actual_test.fullVisitorId.astype('str')\ndf_actual_test[\"predicted\"] = df_actual_test.predicted.astype(np.float)\ndf_actual_test.index = df_actual_test.fullVisitorId\ndf_actual_test = df_actual_test.drop(\"fullVisitorId\",axis=1)","d91541cd":"df_actual_test.head()","eeb6fb73":"df_submission_test = pd.read_csv(filepath_or_buffer=\"..\/input\/sample_submission_v2.csv\",index_col=\"fullVisitorId\")\ndf_submission_test.shape","eb0e5dc8":"\"test shape is :{} and submission shape is : {}\".format(df_actual_test.shape , df_submission_test.shape)\nfinal_df = df_actual_test.loc[df_submission_test.index,:]","0266ab70":"final_df = final_df[~final_df.index.duplicated(keep='first')]\nfinal_df = final_df.rename(index=str, columns={\"predicted\": \"PredictedLogRevenue\"})\nfinal_df.PredictedLogRevenue.fillna(0).head()\n# final_df.head()","50f98eed":"fig, ax = plt.subplots(figsize=(10,16))\nlgb.plot_importance(gbm, max_num_features=30, height=0.8, ax=ax)\nplt.title(\"Feature Importance\", fontsize=15)\nplt.show()","675124d2":"<a id=\"20\"><\/a> <br>\n* **A. INVESTIGATION OF FEATURE IMPORTANCE**\n\nLightGBM have a method for representation of feature importance.\n","e2adcd66":"Replacing NaN variables with 0 (It may have positive\/negative effect. We will check it later).\nAnother point we must touch on is we need to convert","cc7eec35":"<a id=\"9\"><\/a> <br>\n* **G. GEO_NETWORK**\n\nIt is json and the similar manner to previous feature (device) should be done.\n","47ea85ae":"<a id=\"1\"><\/a> <br>\n#  2-DATA ANALYSIS\n\n\n<a id=\"3\"><\/a> <br>\n* **A. IMPORTS**\n\nImporting packages and libraries.","ef022352":"<a id=\"0\"><\/a> <br>\n## Kernel Headlines\n1. [Introduction and Crisp Methodology](#1)\n2. [Data Analysis](#2)\n    1.  [imports](#3)\n\t1.  [Reading Data](#4)\n\t1.  [Features Descriptions](#5)\n\t1.  [ChannelGrouping_barchart](#6)\n\t1.  [date and visitStartTime_describe](#7)\n\t1.  [device_barchart](#8)\n\t1.  [geoNetwork_barchart](#9)\n\t1.  [socialEngagement_describe](#10)\n\t1.  [totals_line_violin](#11)\n\t1.  [visitNumber_line_violin_hist](#12)\n\t1.  [trafficSource_barchart](#13)\n\t1.  [fullVisitorId_qpercentile](#14)\n\t\n3. [Compound Features](#15)\n     1.  [Churn Rate and Conversion Rate](#16)\n\t 1.  [revenue_datetime](#17)\n\t 1.  [device_revenue](#18)\n4. [Basic Regression](#19)\n5. [Preparing for More Evaluations and Tests](#20)\n\t 1.  [Investigation of Feature Importance](#21)","c6c00a20":"<a id=\"8\"><\/a> <br>\n* **F. DEVICE**\n\ndevice is stored in json format. There is a need to extract its fields and analyze them. Using json library to deserializing json values.","87c8cf1d":"Google dependent redirects are more than twice the youtube sources. Combination of this feature with revenue and visits may have important result. We will do it in next step (when we are analyzing feature correlations).\nNow let's move on keywords feature.\nA glance to keyword featre represnets lot of missing values '(not provided)'. Drawing a bar chart for both of them...\n","8aff834b":"<a id=\"10\"><\/a> <br>\n* **H.SOCIAL_ENGANEMENT_TYPE **\n\nDescribing this feature confirms its uniqueness. It should be dropped. Because its entropy is 0. ","80541d53":"Whole the period exist in data. So, Lets define new empty dataframe and do this calculations on it and copy the our requirements ot it.\nfor churn rate calculations, we need to check which users visited have visited the website monthly. this information is located in fullVisitorId. we will copy it to new df.","b0c1e392":"#  3-COMPOUND FEATURES\n\n<a id=\"16\"><\/a> <br>\n* **A. CHURN&CONVERSION_VISUALIZATION**\n\nThe main definition of ChurnRate is The percentage rate at which customers stop subscribing to a service or employees leave a job. Churn rate period can be various from day to a year correspoding to business type. In this section, we will compute and visualize the monthly churn rate. \nLets do more investigation on features date and fullVisitorId for more detail mining.\n","78a642b6":"<a id=\"18\"><\/a> <br>\n#  4-BASIC REGRESSION\n\nNow, based on the analysis we have done, it's time to start the regression progress.\nLets do a simple regression for testing our regression method.\nCalling our main_df and dropping the features that may have not any positive effect on the regression results ( HINT: consider it is starting point. We want to learn how we can use the regression for combination of categorical features and continus features. We will change our features in next steps for getting better results).\nWe use the tricks we used during this tutorial for dealing with features.","6466065b":"By generalizing the above solution we have:","4ee69ccf":"Aggregation on days and plotting daily revenue.","3b43d88a":"The 75% of sessions have visitNumber lower than one time. You can get more information about percentiles by calling np.percentile method.","0e669f5a":"<a id=\"4\"><\/a> <br>\n* **B. READING DATA**\n\nReading data and caughting a glimpse of what data it is.","bcb9f0b7":"Using train_test_split for splitting data to train and evaluate sets. Converting revenue (our target variable) to float for performing regression.","367f34ac":"Congrate ! \nYou get reasonable rmse in first step. Now, lets predict the revenues for our evaluation dataset to be familar with transformation needed.\n\n(Point that the revenue of user cant be negative ;-) so, remove them).","603e8073":"Doing similar process on visitNumber and getting total visitNumber per day.","cda52eb1":" It is clear that the dispersion of the 'visitNumber' per session is huge. for this sort of features, we can use Log and map the feature space to\nnew lower space. As a result of this mapping, visualization the data will be easier.","080d513d":"OK. Now we have all requirements which are needed for first regression test ;-).\n\nCheck [this link](http:\/\/github.com\/Microsoft\/LightGBM\/tree\/master\/examples\/python-guide) out for get more information about Regression algorithm we are using.\n\nIt is mentioned in competition description that we need to use ln(x+1) for evaluation of results. So, check [this links](https:\/\/docs.scipy.org\/doc\/numpy-1.15.1\/reference\/generated\/numpy.log1p.html) and [this link](https:\/\/docs.scipy.org\/doc\/numpy-1.13.0\/reference\/generated\/numpy.expm1.html) about these scipy built-in methods.\nWe can control prints with 'verbose_eval' parameter.","bf14a2ae":"Now we have 2D matrix containig the continus visited users.","4a457f82":"Checking the transformed features.","73dae303":"For calculation of churn rate we need to count the users appeared in two, three, four, etc. continus months. \nWe will using the following format for collocation of users. \nFor example assume we want to extract the number of distinct users who visited the website on 2016-08.","d8871a56":"<a id=\"7\"><\/a> <br>\n* **E. DATE&VISIT_START_TIME**\n\nThere are two varialbe related to time and can be used in time dependent analyzes specially TimeSeries.","542bf40a":"Another point to touch on is we need to convert our categorical data to (int) values. As a result regression algorithm and classifiers can deal with these sort of features ( String features are not supported).\n(Search about LabelEncoder and its use case. This tool helps us to convert the categorical features to Integer ones).","1cc8af97":"analysing the distribution of users in 5 continents.","60e2aabd":"<a id=\"19\"><\/a> <br>\n#  5-PREPARING FOR MORE EVALUATIONS AND TESTS\n\nNow, we have all requirements for doing more tests. Lets summarize all the prcess we have done earlier in one script. You can change parameters regarding your understanding about feature engineering process. If you cant understand any section of it, please read the details which are described below.","7344bb23":"keys existed in device attribute are listed below.\nNow we should ignore the features which are not usefull in rest of the process. If feature is misrelated, or it contains lot of \"NaN\" values it should be discarded.\nWe select the [\"browser\",\"operatingSystem\",\"deviceCategory\",\"isMobile\"] for doing the analyzing. The rest of the device features are ignored and will be removed.","155a300c":"So, we have 366 days (12 month = 1 year) from August 2016  to August 2017  data for churn rate calculations. The bes period for churn maybe is the monthly churn rate.\nNow, lets list all the months existed in library for checking having no missing months.","f52d0b5c":"Extracting all the revenues can bring us an overview about the total revenue.","d6d8a4a6":"Doing groupby on date and getting the total revenue per day:","90faab35":"Now, lets check another side of 'visitNumber' feature. As it is mentioned in data description, visitNumber is the number of sessions for each user. It can also be the factor of users interest. lets 'describe' and  'visualize' them.\nusing 'collections' package, we can count repetition of each element.","76b8abcf":"<a id=\"6\"><\/a> <br>\n* **D. CHANNEL_GROUPING**","669c0f77":"As it is completely obvious in source diagram, google is the most repetitive source. It would be interesting if we replace all google subdomains with exact 'google' and do the same analyze again. let's do it.","9d484450":"<a id=\"14\"><\/a> <br>\n* **L. FULL_VISITOR_ID**\n\nNow, lets see how many of users are repetitive ?! This feature can represent important information answering this question ? (Is more repeation proportional to more buy ?! ) \nThe response will be discussed in next section (Where we are analyzing compound features) but now, lets move on calculation of repetitive visits percentiles.","404c4e07":"<a id=\"5\"><\/a> <br>\n* **C. FEATURES DESCRIPTION**\n\nReturning back to Data description for understanding features.\n\n*     channelGrouping - The channel via which the user came to the Store.\n*     date - The date on which the user visited the Store.\n*     device - The specifications for the device used to access the Store.\n*     fullVisitorId- A unique identifier for each user of the Google Merchandise Store.\n*     geoNetwork - This section contains information about the geography of the user.\n*     sessionId - A unique identifier for this visit to the store.\n*     socialEngagementType - Engagement type, either \"Socially Engaged\" or \"Not Socially Engaged\".\n*     totals - This section contains aggregate values across the session.\n*     trafficSource - This section contains information about the Traffic Source from which the session originated.\n*     visitId - An identifier for this session. This is part of the value usually stored as the _utmb cookie. This is only unique to the user. For a completely unique ID, you should use a combination of fullVisitorId and visitId.\n*     visitNumber - The session number for this user. If this is the first session, then this is set to 1.\n*     visitStartTime - The timestamp (expressed as POSIX time).\n","57314ae6":"Now you have a information about which features are more important that others.","90cf80ef":"<a id=\"11\"><\/a> <br>\n* **I. TOTALS**\n","a8edb429":"<a id=\"1\"><\/a> <br>\n#  1-INTRODUCTION AND CRISP METHODOLOGY\n\n![](https:\/\/www.kdnuggets.com\/wp-content\/uploads\/crisp-dm-4-problems-fig1.png)\n    Crisp methodology is on the acceptable manners for data mining tasks. As it is belowed in the following figure, it contains three main parts should be passed to deliver a product to business\n*     Data cleaning\n        1. Understanding the business and data.\n        2. Try to comprehent the business and extract the data which is needed\n        3. Understand the dependencies between attributes. Analyzing the target variables. Handling missing values. Transforming data formats to standard data format.\n*     Data Modeling\n        1. Understanding the business and data.\n        2. Selecting more accurate classfier or regression engine based on the charactristic any of them have.\n        3. Train a model \n*     Evaluation and Deployment.\n        1. Evalute created model using evaluation methods (test-data, cross-validation, etc)\n        2. Catrefully Evaluate model with real data (i.e AB testing) (As it is shown in crisp diagram, there is a link between business undestanding and evaluation part). \n        3. Migrate to new model and replace the old one with new version.\n","25c8f798":"\n<a id=\"17\"><\/a> <br>\n* **B. REVENUE&DATETIME**\n\n\nNow, it is time to move on to analysing compound features. The main target of this section is undestanding the features correlation. \nAt the first point, lets analyze this probable assumption :\n\n\"Is more visitNumber proportional to more Revenue ?!\"\n","6d39d2e4":"Concatenate these two dataframe and compound visualization.","6d041e1b":"We will try to do more supervisored regressions in next steps...","ed5b8cfe":"As it is shown, only 12 percent of users are repetitive and visited the website more than once. \n(Search about churn rate and conversion rate if you want to know why we have analyzed this feature ;-) )\n","c2e843a3":"<a id=\"12\"><\/a> <br>\n* **J. VISIT_NUMBER**\n\nNumber of visits have profound potential to be an important factor in regression progress. ","558f317e":"A churn-rate heat map is the one the important keys of business. The more repetitive users in continues time periods, the more success in user loyalty.\n\nGeneraly it is better to drop the zeors below the main diagonal for better visualization and more clearer representaion. \n(I couldn't find the sns pleasant visualization for half churn rate matrix. If you find it, it will be appreciated to ping me. Ill replace the below diagram with your recommendation as soon as possible).","036ab4d4":"Going deeper to subclasses ...","30b1b3b6":"By comparing the revenue and visitNumbers we can confirm our consumption. Where there is more visit the revenue is also more than neighbour days.\nThe behaviour of line charts is completely similar.\n\nAnother point to touch on is the rate of visitNumber and revenue  which have a peak on December. Before christmas people visit and buy more than other days. The same behaviour is represented in the days after christmas where people have bought their requirements and the level of visit and buy goes down (They are in vacation and have less time to check the website ;-) )\n\nThe above diagram can be represent more detail if you visualize period of it. Do it yourself with by quering on daterange and check our confirmed assumtion :-D.","d3b5358a":"Lets find most_common and least_common visitNumbers for being familiar with collections module and its powrefull tools ;-) ","5d006723":"So, we have 12 list and each elemets contains the users who visited the website on the correspondence period.\nNow its time to do some matrix calculation for filling the churn-rate matrix.\nIt is very probable that you calculate the matrix with more efficient ways. I used this manner for more simplicity.","280234c2":"Lets focus on user who have addressed our challenge (users who have revenue) and checking some assumptions","b7a2c691":"Now, you can generalize this procedure and use do your submission.\n\nWe will try to do a more supervised regression in next steps. We will utilize our preprocess facts for getting regression results with less error.","f95ee64e":"<a id=\"13\"><\/a> <br>\n* **K. TRAFFIC_SOURCE**\n\nWhat is the most conventional manner for visitor who visit to the website and do their shopping ? trafficSource attribute can resolve this qurestion.\nLike a previous Json elements existed in the dataset, this attribute is also Json file. so, we use the similar way to deserialize it. We have select keyword, source and the medium as a features which can bring more useful infromation.\n","03aa1169":"For buyers who use mobile for purchasing, Safari and chrome approximately have equal number of users. But for users who do not use mobile for purchasing, Safari have a few percent of total users. ","1ad688e2":"date is stored in String and should be converted to pandas datetime format.\nvisitStartTime is stored in epoch unix format and should be converted to pandas datetime format.\ndoing the correspondence transforms and storing on the same attribute."}}