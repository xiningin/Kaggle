{"cell_type":{"27a8d608":"code","0bcd9b74":"code","c42720a0":"code","d78dff43":"code","aee8d11f":"code","b93baeb7":"code","bcf50f3c":"code","b94930f4":"code","f7aa28c8":"code","49129918":"code","42adb0aa":"code","7073a00b":"code","4dbef07c":"code","6f9885d6":"code","8cde75a2":"code","b1788116":"code","86df62c1":"code","5b2bb7a8":"code","b566ca51":"code","65107873":"code","fa1a644c":"code","343b510b":"code","560bcb42":"code","1fbcde73":"code","f31e9ad7":"code","f203854d":"code","e3ef4597":"code","6d56a24c":"code","0e32838c":"code","bf788f57":"code","8d28db12":"code","4cfca428":"code","fdc0b8f0":"code","672455cb":"code","92c1b0fc":"markdown","1994e412":"markdown","2cccb0ff":"markdown","78ab9c9e":"markdown","5e360e67":"markdown","f48c11a9":"markdown","70e6ae81":"markdown","9019ebbc":"markdown","83bd7b8f":"markdown","3b254230":"markdown","6c12302c":"markdown","788d3a27":"markdown","20f987d3":"markdown","ba0b0ac3":"markdown","1bfb6555":"markdown","28d394f1":"markdown","2b0e2cd6":"markdown","6bf35d48":"markdown"},"source":{"27a8d608":"import warnings\nimport inspect\nimport datetime\nfrom collections.abc import Iterable\nwarnings.filterwarnings('ignore')\n\n# Imports\nimport plotly.offline as py\nimport plotly.graph_objs as go\nfrom functools import reduce\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n\n\nimport seaborn as sns\nimport missingno\n\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom keras.layers import Dense, BatchNormalization, LeakyReLU\nfrom keras.models import Sequential\nfrom keras.wrappers.scikit_learn import KerasRegressor\nfrom keras.callbacks import EarlyStopping, ModelCheckpoint, Callback\nfrom keras_tuner import RandomSearch, Objective\nfrom mlxtend.regressor import StackingCVRegressor\nfrom sklearn.linear_model import Ridge, Lasso\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error","0bcd9b74":"LOG_DIR = 'nov-9-2021--1'","c42720a0":"# Utils\n\n''' Ensures element(s) is\/are a specific data type \n    Returns a collection of the element(s)\n    \n    I.e. if given a single `el`, returns a list containing only `el`.\n'''\ndef check_type_or_list_of_type(el, dtype):\n    if type(el) is dtype:\n        return [el]\n    \n    elif not all(type(x) is dtype for x in el):\n        raise ValueError(f\"Expected single or collection of {dtype}\")\n    \n    return el\n    \n''' Drops column(s) from a\/multiple DataFrame(s) '''\ndef drop_cols(dfs, cols, inplace=True, verbose=False):\n    \n    # Ensure dtypes and that arguments are in list\/iterable form\n    dfs = check_type_or_list_of_type(dfs, type(pd.DataFrame()))\n    cols = check_type_or_list_of_type(cols, str)\n    \n    res = []\n    \n    for df in dfs:\n        res.append(df.drop(list(set(df.columns.values) & set(cols)), axis=1, inplace=inplace))\n    \n    if not inplace:\n        if len(res) is 1:\n            return res[0]\n        return tuple(res)","d78dff43":"class ThresholdStopping(Callback):\n    def __init__(self, monitor='val_loss', num_epochs=5, thresh=0.00001, verbose=0, mode='min'):\n        super(Callback, self).__init__()\n        assert mode in ['min', 'max']\n        self.mode = mode\n        self.monitor = monitor\n        self.num_epochs = num_epochs\n        self.thresh = thresh\n        self.verbose = verbose\n        self.thresh_passed = False\n\n    def on_epoch_end(self, epoch, logs={}):\n        if self.thresh_passed:\n            return\n        \n        # Try to pass threshold\n        if epoch + 1 < self.num_epochs:\n            current = logs.get(self.monitor)\n            if current is None:\n                warnings.warn(\"Threshold stopping requires %s available!\" % self.monitor, RuntimeWarning)\n\n            if self.mode is 'min':\n                if current < self.thresh:\n                    self.thresh_passed = True\n            elif self.mode is 'max':\n                if current > self.thresh:\n                    self.thresh_passed = True\n                \n        # Halt if threshold isn't achieved\n        else:\n            if self.verbose > 0:\n                print(\"Epoch %05d: threshold stopping\" % epoch)\n            self.model.stop_training = True\n","aee8d11f":"train_df = pd.read_csv('..\/input\/cap-4611-2021-fall-assignment-3\/train.csv')\ntest_df = pd.read_csv('..\/input\/cap-4611-2021-fall-assignment-3\/eval.csv')","b93baeb7":"train_df.head()","bcf50f3c":"train_df.describe()","b94930f4":"# Check for null values in train dataset\n(train_df.isna().sum() > 0).sum()","f7aa28c8":"# Check for null values in eval dataset\n(test_df.isna().sum() > 0).sum()","49129918":"# Make sure identifiers are unique and are not classifications\n(train_df['pubchem_id'].value_counts() > 1).sum()","42adb0aa":"# Make sure identifiers are unique and are not classifications\n(test_df['pubchem_id'].value_counts() > 1).sum()","7073a00b":"# Use util method to drop column from both dataframes\ndrop_cols([train_df, test_df], 'pubchem_id')","4dbef07c":"train_df.head()","6f9885d6":"X_train = drop_cols(train_df, ['id', 'Eat'], inplace=False)\ny_train = train_df['Eat']","8cde75a2":"default_model_hidden_layers = [\n    {\n        'n_neurons': 100,\n        'activation': 'relu'\n    },\n    {\n        'n_neurons': 250,\n        'activation': 'relu'\n    },\n    {\n        'n_neurons': 100,\n        'activation': 'relu'\n    },\n]\ndef create_model(hidden_layers=default_model_hidden_layers, optimizer='adam', input_shape=X_train.columns.shape):\n    model = Sequential()\n\n    model.add(Dense(\n        hidden_layers[0]['n_neurons'],\n        activation=hidden_layers[0]['activation'],\n        input_shape=input_shape\n    ))\n    if len(hidden_layers) is not 1:\n        for idx, layer in enumerate(hidden_layers[1:]):\n            if 'normalize' in layer and layer['normalize'] is True:\n                model.add(BatchNormalization())\n            else:\n                model.add(\n                    Dense(\n                        layer['n_neurons'],\n                        activation=layer['activation'],\n                    )\n                )\n    model.add(Dense(1))\n    \n    model.compile(optimizer=optimizer, loss=\"mse\", metrics=[tf.keras.metrics.RootMeanSquaredError(name='rmse')])\n    return model\n\n''' Find the best performing models from a set of models\/architectures '''\ndef eval_models(architectures, X, y, num_best_models=1, val_size=.3, test_size=.3, random_state=None, callbacks=[], epochs=250, verbose=1, keras_verbose=1, save_best=True, use_best=True, batch_size=None, shuffle=False, optimizer='adam'):\n    \n    if type(verbose) is not int:\n        verbose = -1\n    \n    if type(num_best_models) is not int or num_best_models > len(architectures) or num_best_models < 1:\n        num_best_models = -1\n        \n    if type(optimizer) is list:\n        assert len(optimizer) is len(architectures)\n    \n    _best_model_checkpoint_prefix = 'model-fit--'\n    _callbacks = callbacks\n    evaluations = [] # Loss (MSE), Metric (RMSE)\n    best_models_dict = {}\n    train_test_split_args = []\n    fit_args = []\n    eval_args = []\n    if type(random_state) is int:\n        train_test_split_args.append(dict(random_state=random_state))\n    \n#     if type(batch_size) is int and batch_size > 0:\n#             eval_args.append(dict(batch_size=batch_size))\n#     if type(shuffle) is bool:\n#             fit_args.append(dict(shuffle=shuffle))\n    \n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, *train_test_split_args)\n   \n    if verbose > 2:\n        print(f'[eval_models()] Created hold out set of ({int(test_size * 100)}% of dataset)\\n')\n\n    for idx, architechture in enumerate(architectures):\n        \n        arch_name = f'{_best_model_checkpoint_prefix}{idx+1}'\n\n        # Checkpoint for best model\n        save_best_model_name = f'{arch_name}.hdf5'\n        checkpoint = ModelCheckpoint(save_best_model_name, save_best_only=True, monitor='val_rmse', mode='min')\n        \n        if verbose > 1:\n            _newline = \"\" if idx is 0 else \"\\n\\n\"\n            print(f'{_newline}[eval_models()] Train\\Eval of model {idx+1}\/{len(architectures)} ({int((idx+1)\/len(architectures) * 100)}%)')\n\n        # Create Model from Architecture\n        if type(optimizer) is list:\n            model = create_model(architechture, optimizer=optimizer[idx])\n        else: model = create_model(architechture, optimizer=optimizer)\n\n        if verbose > 2:\n            print(model.summary())\n            print(*architechture, sep=',\\n')\n        \n        # Checkpoint\n        if save_best:\n            _callbacks = [ModelCheckpoint(save_best_model_name, save_best_only=True, monitor='val_rmse', mode='min'), *callbacks]\n            \n        history = model.fit(X_train, y_train, validation_split=val_size, epochs=epochs, callbacks=[checkpoint], verbose=keras_verbose, batch_size=batch_size)\n                \n        if use_best:\n            model = keras.models.load_model(save_best_model_name)\n\n        evaluations.append(model.evaluate(X_test, y_test, batch_size=batch_size))\n        if verbose > 1:\n            _newline = \"\" if idx is 0 else \"\\n\\n\"\n            print(f'[eval_models()] Model Eval MSE {evaluations[-1][0]}, RMSE {evaluations[-1][1]}')\n                \n        ###################################################\n        # TODO : Compare eval of best model vs last model\n        ###################################################\n\n        if verbose > 2:\n            # Plot history val vs train loss\n            pd.DataFrame(history.history).plot(figsize=(8,5))\n            plt.grid(True)\n            plt.gca().set_ylim(0, 0.6)\n            plt.show()\n\n    \n    evaluations = np.array(evaluations)\n    rmse_evals = evaluations[:,1]\n    \n    if num_best_models is not -1:\n        best_models_idx = np.argpartition(rmse_evals, num_best_models)[:num_best_models]\n    else: best_models_idx = range(len(architectures))\n\n    # Select k best models\n    for idx in best_models_idx:\n        best_models_dict[idx] = dict(\n            architecture=architectures[idx],\n            mse=evaluations[idx, 0],\n            rmse=evaluations[idx, 1]            \n        )\n    \n    # Select k best models\n#     best_models = np.array(architectures)[best_models_idx].tolist()\n    \n    ###################################\n    # TODO : graph all model's eval\n    ###################################\n    \n    ###################################\n    # TODO : graph best model's eval\n    ###################################\n    \n    return best_models_dict, evaluations\n\n''' Evaluates a model on multiple train-val splits, lastly evaluates the model on the holdout test set '''\n''' Specify n_splits=0 for just comparing model against a single test set '''\ndef make_eval(arch, X, y, n_splits=50, test_size=.3, val_size=.3, fit_val_size=.3, use_holdout=True, random_state=None, graph_report=False, graph_bins=5, callbacks=[], epochs=250, verbose=None, keras_verbose=1, batch_size=64, optimizer='adam'):\n    \n    __rmse = []\n    __mse = []\n    \n    assert type(n_splits) is int\n    assert n_splits >= 0\n    \n    if type(verbose) is not int:\n        verbose = -1\n    \n#     if type(val_size) is str:\n#         val_size = test_size\n    \n    evaluations = [] # Loss (MSE), Metric (RMSE)\n    train_test_split_args = []\n    if type(random_state) is int:\n        train_test_split_args.append(dict(random_state=random_state))\n    \n    # Use Split X and y to train and test holdout\n    if use_holdout:\n        X, X_test, y, y_test = train_test_split(X, y, test_size=test_size, *train_test_split_args)\n        \n        if verbose > 2:\n            print(f'[make_eval()] Created hold out set of ({int(test_size * 100)}% of dataset)\\n\\n')\n        \n    \n    ''' Validation split loop '''\n    for idx, _ in enumerate(range(n_splits)):\n        # Create the model\n        model = create_model(arch, optimizer=optimizer)\n        \n        # Split Data\n        X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=val_size, *train_test_split_args)\n        if verbose > 2:\n            _newline = \"\" if idx is 0 else \"\\n\\n\"\n            print(f'{_newline}[make_eval()] Train\\Eval of sample {idx+1}\/{n_splits} ({int((idx+1)\/n_splits * 100)}%)')\n        \n        # Fit the model\n        model.fit(X_train, y_train, validation_split=fit_val_size, batch_size=batch_size, epochs=epochs, callbacks=callbacks, verbose=keras_verbose)\n        \n        # Evaluate the model\n        _eval = model.evaluate(X_val, y_val, batch_size=batch_size)\n        \n        # Save results\n        __mse.append(_eval[0])\n        __rmse.append(_eval[1])\n        evaluations.append(_eval)\n    \n    # Eval DataFrame\n    metrics_df = pd.DataFrame({'mse': __mse, 'rmse': __rmse})\n          \n    ''' Graph histogram of evaluation spread '''\n    if graph_report:\n        if type(graph_bins) is not int or graph_bins < 2:\n            graph_bins = min(5, n_splits)\n    \n        if verbose > 4:\n            print(f'[make_eval()] Preparing evaluation histogram')\n        evaluations_df = pd.DataFrame(np.array(evaluations)[:, 1]) # Just RMSE\n        plt.hist(evaluations_df, bins=graph_bins)\n        plt.title('RMSE Distribution')\n        plt.show()\n    \n    if verbose > 1:\n        print(metrics_df)\n        print(metrics_df.describe())\n    \n    ''' Evaluate holdout test set as well '''\n    if use_holdout:\n        \n        model = create_model(arch, optimizer=optimizer)\n        \n        if verbose > 2:\n            print(f'[make_eval()] Evaluating holdoutset ({int(test_size * 100)}% of dataset)')\n        history = model.fit(X, y, validation_split=fit_val_size, epochs=epochs, batch_size=batch_size, callbacks=callbacks, verbose=keras_verbose)\n        _holdout_eval = model.evaluate(X_test, y_test, batch_size=batch_size)\n        \n        if verbose > 1:\n            print(f'[make_eval()] Holdout RMSE {_holdout_eval[-1]}')\n        \n        return evaluations, _holdout_eval, history\n    \n    # No holdout, just different samples\n    return evaluations\n","b1788116":"class ComplexModel:\n    \n    ___DEFAULT_INPUT_MODELS___ = [        \n        [\n            # 0.12751464545726776\n            {'n_neurons': 200, 'activation':'relu'},\n            {'n_neurons': 1500, 'activation':'tanh'},\n            {'n_neurons': 1000, 'activation':'relu'},\n            {'n_neurons': 1400, 'activation':'relu'},\n            {'n_neurons': 1300, 'activation':'relu'},\n            {'n_neurons': 400, 'activation':'relu'},\n            {'n_neurons': 300, 'activation':'relu'},\n        ],\n        [\n            # RMSE 0.11791416257619858\n            {'n_neurons': 200, 'activation': 'relu'},\n            {'n_neurons': 1500, 'activation': 'tanh'},\n            {'n_neurons': 1000, 'activation': 'relu'},\n            {'n_neurons': 1400, 'activation': 'tanh'},\n            {'n_neurons': 1300, 'activation': 'tanh'},\n            {'n_neurons': 400, 'activation': 'tanh'},\n            {'n_neurons': 300, 'activation': 'tanh'}\n        ],\n        [\n            # 0.12822529673576355\n            {'n_neurons': 700, 'activation':'relu'},\n            {'n_neurons': 1600, 'activation':'tanh'},\n            {'n_neurons': 700, 'activation':'tanh'},\n            {'n_neurons': 900, 'activation':'tanh'},\n        ],\n        [\n            # 0.12867093086242676\n            {'n_neurons': 1000, 'activation':'relu'},\n            {'n_neurons': 1100, 'activation':'tanh'},\n            {'n_neurons': 900, 'activation':'relu'},\n            {'n_neurons': 900, 'activation':'relu'},\n        ],\n        [\n            {'n_neurons': 1000, 'activation': 'relu'},\n            {'n_neurons': 1200, 'activation': 'tanh'},\n            {'n_neurons': 900, 'activation': 'tanh'}\n        ],\n        [\n            {'n_neurons': 1000, 'activation': 'tanh'},\n            {'n_neurons': 700, 'activation': 'tanh'},\n            {'n_neurons': 1100, 'activation': 'relu'},\n            {'n_neurons': 400, 'activation': 'relu'}\n        ],\n    ]\n    \n    ___DEFAULT_ENSEMBLE_MODEL___ = [\n        {'n_neurons': 120, 'activation': 'relu'},\n        {'n_neurons': 400, 'activation': 'tanh'},\n        {'n_neurons': 200, 'activation': 'relu'},\n        {'n_neurons': 100, 'activation': 'relu'}\n    ]\n    \n    ''' Initialize the ensemble model '''\n    def __init__(self, input_models_architectures=___DEFAULT_INPUT_MODELS___, ensemble_model_architecture=___DEFAULT_ENSEMBLE_MODEL___, input_optimizer='adam', ensemble_optimizer=None):\n        \n        \n        # Intialize Model Stuff (generated from training)\n        self.input_rmse=None\n        self.input_models_=None\n        self.ensemble_model_=None\n        self.trained_=False\n        \n        # Base Parameters Stuff for Model\n        self.input_archs=input_models_architectures\n        self.ensemble_arch=ensemble_model_architecture\n        self.input_optimizer=input_optimizer\n        \n        if ensemble_optimizer is None:\n            if isinstance(input_optimizer, Iterable) and type(input_optimizer) is not str:\n                raise ValueError('Ensemble optimizer must be specified when providing an input optimizers list')\n            self.ensemble_optimizer=self.input_optimizer\n        else: self.ensemble_optimizer=ensemble_optimizer\n    \n    ''' Save Ensemble Model and its component Input Models to disk ''' \n    def save_to_disk(self, prefix='ensemble_model', sep='__', num_sep='-', directory=None):\n        # Prep file names\n        directory_path = ''\n        if type(directory) is str:\n            directory_path = f'.\/{directory}\/' # TODO figure this out..\n        base_file_name=f'{directory}{prefix}{sep}'\n        \n        # Save models\n        for i, model in enumerate(self.input_models_):\n            model.save(f'{base_file_name}input{num_sep}{i}')\n        self.ensemble_model_.save(f'{base_file_name}output')\n        \n    ''' Assembles an Ensemble Model using its Input and Output Model Components ''' \n    ''' *Note: Order is VERY important! '''\n    def load_model(self, input_models, output_model):\n        tmp_input = []\n        \n        # Load Each Input Model\n        for model in input_models:\n            # TODO : use method to save architechture as well\n            tmp_input.append(model)\n        self.input_models_ = tmp_input\n        self.ensemble_model_ = output_model\n        \n    ''' Assembles an Ensemble Model that was previosuly aved to disk ''' \n    ''' *Note: Order is VERY important! '''\n    def load_model_from_disk(self, input_model_paths, output_model_path):\n        tmp_input = []\n        \n        # Load Each Input Model\n        for path in input_model_paths:\n            # TODO : use method to save architechture as well\n            tmp_input.append(keras.models.load_model(path))\n            \n        self.ensemble_model_ = keras.models.load_model(output_model_path)\n        self.input_models_ = tmp_input\n        \n        # TODO : use method to save architechture as well\n\n        \n    ''' Fit the ensemble model '''\n    def fit(self, X, y, batch_size=64, epochs=600, val_size=.2, holdout_size=.35, test_size=None, random_state=None, use_best=True, shuffle=False, verbose=None, keras_verbose=1, input_callbacks=[], ensemble_callbacks=[], ensembel_epochs=None, use_tmp_name=False):\n        \n        __using_test = type(test_size) is float and test_size > 0 and test_size < 1\n        self.input_models_=[]\n        input_predictions = None\n        self.input_rmse = []\n        \n        # Handle kwargs\n        # (Ensemble Epochs)\n        if type(ensembel_epochs) is not int or ensembel_epochs < 1:\n            ensembel_epochs=epochs # assume epochs is valid\n        \n        # (Holdout Random State)\n        train_test_split_args = []\n        if type(random_state) is int:\n            train_test_split_args.append(dict(random_state=random_state))\n\n        # Generate a test set for testing ensemble model as a whole\n        if __using_test:\n            X, X_test, y, y_test = train_test_split(X, y, test_size=test_size, *train_test_split_args)\n            \n        # Generate holdout for training the ensemble model (prevent overfitting)\n        X_input, X_ensemble, y_input, y_ensemble = train_test_split(X, y, test_size=holdout_size, *train_test_split_args)\n        \n        \n        ###\n        ### Input Models\n        ############################\n        \n        # Create, fit and generate 'features' from Input Models for the Ensemble Model \n        for i, arch in enumerate(self.input_archs):\n            arch_name = f'ensemble_input-{i+1}'\n            if use_tmp_name:\n                arch_name = 'tmp_model'\n\n            # Checkpoint for best model\n            save_best_model_name = f'{arch_name}.hdf5'\n            checkpoint = ModelCheckpoint(save_best_model_name, save_best_only=True, monitor='val_rmse', mode='min')\n        \n            if verbose > 1:\n                _newline = \"\" if i is 0 else \"\\n\\n\"\n                print(f'{_newline}[ComplexModel] Training input model {i+1}\/{len(self.input_archs)} ({int((i+1)\/len(self.input_archs) * 100)}%)')\n\n            # Create Model from Architecture\n            if isinstance(self.input_optimizer, Iterable):\n                model = create_model(arch, optimizer=self.input_optimizer[i])\n            else: model = create_model(arch, optimizer=self.input_optimizer)\n\n            if verbose > 2:\n                print(model.summary())\n                print(*arch, sep=',\\n')\n\n            # Checkpoint\n            if use_best:\n                _callbacks = [ModelCheckpoint(save_best_model_name, save_best_only=True, monitor='val_rmse', mode='min'), *input_callbacks]\n            \n            # Train model\n            history = model.fit(X_input, y_input, validation_split=val_size, epochs=epochs, callbacks=[checkpoint], verbose=keras_verbose, batch_size=batch_size)\n\n            if use_best:\n                model = keras.models.load_model(save_best_model_name)\n                \n            ##### TODO\n            ##### I might need to use a hold out set for predictions,\n            ##### Otherwise, there will possibly be terrible overfitting.\n            #############################################################\n            self.input_rmse.append(model.evaluate(X_ensemble, y_ensemble, batch_size=batch_size)[-1])\n            if verbose > 1:\n                _newline = \"\" if i is 0 else \"\\n\\n\"\n                print(f'[ComplexModel] input model {i+1} holdout RMSE {self.input_rmse[-1]}')\n\n            if verbose > 2:\n                # Plot history val vs train loss\n                pd.DataFrame(history.history).plot(figsize=(8,5))\n                plt.title(f'(NN Ensemble) Input Model {i+1} Train\/Val')\n                plt.grid(True)\n                plt.gca().set_ylim(0, 0.6)\n                plt.show()\n            \n            # Get predictions to train ensemble model\n            pred = model.predict(X_ensemble, batch_size=batch_size)\n            \n            if input_predictions is None:\n                input_predictions = pd.DataFrame(pred, columns=[f'model{i+1}'])\n            else:\n                input_predictions[f'model{i+1}'] = pred\n\n            # Save input model\n            self.input_models_.append(model)\n        \n        ###\n        ###   Ensemble Model   (Output aggregator)\n        ##########################################\n        arch_name = f'ensemble_model'\n        if use_tmp_name:\n            arch_name = 'tmp_model'\n\n        # Checkpoint for best model\n        save_best_model_name = f'{arch_name}.hdf5'\n        checkpoint = ModelCheckpoint(save_best_model_name, save_best_only=True, monitor='val_rmse', mode='min')\n\n        if verbose > 1:\n            _newline = \"\" if i is 0 else \"\\n\\n\"\n            print(f'{_newline}[ComplexModel] Training Ensemble Model (output aggregator)')\n        \n        # Create Ensemble Model\n        ensemble_model = create_model(self.ensemble_arch, input_shape=(input_predictions.columns.shape), optimizer=self.ensemble_optimizer)\n\n        # Checkpoint Ensemble Model\n        if use_best:\n            _callbacks = [ModelCheckpoint(save_best_model_name, save_best_only=True, monitor='val_rmse', mode='min'), *ensemble_callbacks]\n\n        # Fit Ensemble Model\n        history = ensemble_model.fit(input_predictions, y_ensemble, validation_split=val_size, epochs=ensembel_epochs, callbacks=[checkpoint], verbose=keras_verbose, batch_size=batch_size)\n\n        if use_best:\n            ensemble_model = keras.models.load_model(save_best_model_name)\n\n        if verbose > 1 and __using_test:\n            _rmse_df = pd.DataFrame(self.input_rmse)\n            print('Input Models RMSE')\n            print(_rmse_df)\n            print(_rmse_df.describe())\n            __eval_input_preds = None\n            for i, model in enumerate(self.input_models_):\n                pred = model.predict(X_test, batch_size=batch_size)\n                if __eval_input_preds is None:\n                    __eval_input_preds = pd.DataFrame(pred, columns=[f'model{i+1}'])\n                else:\n                    __eval_input_preds[f'model{i+1}'] = pred\n                \n            _newline = \"\\n\\n\"\n            print(f'[ComplexModel] Ensemble Model (output) RMSE {ensemble_model.evaluate(__eval_input_preds, y_test, batch_size=batch_size)[-1]}')\n            print(f'[ComplexModel] Aggregate Average RMSE {mean_squared_error(__eval_input_preds.mean(axis=1), y_test, squared=False)}')\n            \n        if verbose > 2:\n            # Plot history val vs train loss\n            pd.DataFrame(history.history).plot(figsize=(8,5))\n            plt.title(f'(NN Ensemble) Ensemble Model (output) Train\/Val')\n            plt.grid(True)\n            plt.gca().set_ylim(0, 0.6)\n            plt.show()\n\n        # Save ensemble model\n        self.ensemble_model_ = ensemble_model\n\n\n\n    ''' Fit the ensemble model for a simple mean aggregate '''\n    def fit_for_mean(self, X, y, batch_size=64, epochs=600, val_size=.2, holdout_size=.35, test_size=None, random_state=None, use_best=True, shuffle=False, verbose=None, keras_verbose=1, input_callbacks=[], use_tmp_name=False):\n        \n        __using_test = type(test_size) is float and test_size > 0 and test_size < 1\n        self.input_models_=[]\n        self.input_rmse = []\n        \n        # (Holdout Random State)\n        train_test_split_args = []\n        if type(random_state) is int:\n            train_test_split_args.append(dict(random_state=random_state))\n\n        # Generate a test set for testing ensemble model as a whole\n        if __using_test:\n            X, X_test, y, y_test = train_test_split(X, y, test_size=test_size, *train_test_split_args)\n            \n        # Generate holdout for training the ensemble model (prevent overfitting)\n        X_input, X_ensemble, y_input, y_ensemble = train_test_split(X, y, test_size=holdout_size, *train_test_split_args)\n        \n        \n        ###\n        ### Input Models\n        ############################\n        \n        # Create, fit and generate 'features' from Input Models for the Ensemble Model \n        for i, arch in enumerate(self.input_archs):\n            arch_name = f'ensemble_input-{i+1}'\n            if use_tmp_name:\n                arch_name = 'tmp_model'\n\n            # Checkpoint for best model\n            save_best_model_name = f'{arch_name}.hdf5'\n            checkpoint = ModelCheckpoint(save_best_model_name, save_best_only=True, monitor='val_rmse', mode='min')\n        \n            if verbose > 1:\n                _newline = \"\" if i is 0 else \"\\n\\n\"\n                print(f'{_newline}[ComplexModel] Training input model {i+1}\/{len(self.input_archs)} ({int((i+1)\/len(self.input_archs) * 100)}%)')\n\n            # Create Model from Architecture\n            if isinstance(self.input_optimizer, Iterable):\n                model = create_model(arch, optimizer=self.input_optimizer[i])\n            else: model = create_model(arch, optimizer=self.input_optimizer)\n\n            if verbose > 2:\n                print(model.summary())\n                print(*arch, sep=',\\n')\n\n            # Checkpoint\n            if use_best:\n                _callbacks = [ModelCheckpoint(save_best_model_name, save_best_only=True, monitor='val_rmse', mode='min'), *input_callbacks]\n            \n            # Train model\n            history = model.fit(X, y, validation_split=val_size, epochs=epochs, callbacks=[checkpoint], verbose=keras_verbose, batch_size=batch_size)\n\n            if use_best:\n                model = keras.models.load_model(save_best_model_name)\n                \n            ##### TODO\n            ##### I might need to use a hold out set for predictions,\n            ##### Otherwise, there will possibly be terrible overfitting.\n            #############################################################\n            if __using_test:\n                self.input_rmse.append(model.evaluate(X_test, y_test, batch_size=batch_size)[-1])\n                if verbose > 1:\n                    _newline = \"\" if i is 0 else \"\\n\\n\"\n                    print(f'[ComplexModel] input model {i+1} holdout RMSE {self.input_rmse[-1]}')\n\n            if verbose > 2:\n                # Plot history val vs train loss\n                pd.DataFrame(history.history).plot(figsize=(8,5))\n                plt.title(f'(NN Ensemble) Input Model {i+1} Train\/Val')\n                plt.grid(True)\n                plt.gca().set_ylim(0, 0.6)\n                plt.show()\n\n            # Save input model\n            self.input_models_.append(model)\n\n        \n        ###\n        ###   Ensemble Model   (Mean aggregator)\n        ##########################################\n        \n        if verbose > 1 and __using_test:\n            _rmse_df = pd.DataFrame(self.input_rmse)\n            print('Input Models RMSE')\n            print(_rmse_df)\n            print(_rmse_df.describe())\n            __eval_input_preds = None\n            for i, model in enumerate(self.input_models_):\n                pred = model.predict(X_test, batch_size=batch_size)\n                if __eval_input_preds is None:\n                    __eval_input_preds = pd.DataFrame(pred, columns=[f'model{i+1}'])\n                else:\n                    __eval_input_preds[f'model{i+1}'] = pred\n                \n            _newline = \"\\n\\n\"\n            print(f'[ComplexModel] Aggregate Average RMSE {mean_squared_error(__eval_input_preds.mean(axis=1), y_test, squared=False)}')\n\n\n\n\n    ''' Generates a prediction from the Ensemble Model '''\n    def predict(self, X, batch_size=64):\n        \n        # Initialize Input Models Predictions DataFrame\n        input_predictions = None\n        \n        # Get Predictions from each Input Model\n        for i, model in enumerate(self.input_models_):\n            \n            # Get predictions to train ensemble model\n            \n            pred = model.predict(X, batch_size=batch_size)\n            if input_predictions is None:\n                input_predictions = pd.DataFrame(pred, columns=[f'model{i+1}'])\n            else:\n                input_predictions[f'model{i+1}'] = pred\n\n            \n            \n        return self.ensemble_model_.predict(input_predictions)\n    \n    def predict_with_mean(self, X, batch_size=64):\n\n        # Initialize Input Models Predictions DataFrame\n        input_predictions = None\n\n        # Get Predictions from each Input Model\n        for i, model in enumerate(self.input_models_):\n\n            # Get predictions to train ensemble model\n\n            pred = model.predict(X, batch_size=batch_size)\n            if input_predictions is None:\n                input_predictions = pd.DataFrame(pred, columns=[f'model{i+1}'])\n            else:\n                input_predictions[f'model{i+1}'] = pred\n\n        return input_predictions.mean(axis=1)","86df62c1":"stackd_archs = [\n        [\n            # 0.12867093086242676\n            {'n_neurons': 1000, 'activation':'relu'},\n            {'n_neurons': 1100, 'activation':'tanh'},\n            {'n_neurons': 900, 'activation':'relu'},\n            {'n_neurons': 900, 'activation':'relu'},\n        ],\n        [\n            {'n_neurons': 1000, 'activation': 'relu'},\n            {'n_neurons': 1200, 'activation': 'tanh'},\n            {'n_neurons': 900, 'activation': 'tanh'}\n        ],\n        [\n            {'n_neurons': 1000, 'activation': 'tanh'},\n            {'n_neurons': 700, 'activation': 'tanh'},\n            {'n_neurons': 1100, 'activation': 'relu'},\n            {'n_neurons': 400, 'activation': 'relu'}\n        ],\n]\nstacked_model = StackingCVRegressor(\n    regressors=[KerasRegressor(build_fn=lambda: create_model(model_arch, optimizer=keras.optimizers.Adam(0.0001)), epochs=500, batch_size=128, verbose=0) for model_arch in stackd_archs],\n    meta_regressor=Ridge(),\n    verbose=1\n)","5b2bb7a8":"stacked_model.fit(X_train, y_train)","b566ca51":"X_test = drop_cols(test_df, ['id', 'Eat'], inplace=False)\nsubmission_df = test_df[['id']]\nsubmission_df['Eat'] = stacked_model.predict(X_test)\nsubmission_df.to_csv(f'submission-stacked--3.csv',index=False)\nsubmission_df","65107873":"print(submission_df.to_string(index=False))","fa1a644c":"# Keras Tuner Build Model Method\ndef build_model(hp):\n    model = Sequential()\n    \n    # Input Layer\n    model.add(Dense(\n        hp.Int('input__units', min_value=100, max_value=1800, step=100),\n        activation=hp.Choice(\"input__activation\", ['relu', 'tanh',]),\n        input_shape=X_train.columns.shape\n    ))\n    \n    # First Hidden Layer\n    model.add(Dense(\n        hp.Int('hidden_1__units', min_value=400, max_value=1800, step=100),\n        activation='tanh'\n    ))\n    \n    # Other Hidden Layers\n    for i in range(hp.Int(\"num_hidden\", 1, 5)):\n        model.add(Dense(\n            hp.Int(f\"hidden_{i+2}__units\", min_value=100, max_value=1800, step=100),\n            activation=hp.Choice(f\"hidden_{i+2}__activation\", ['relu', 'tanh']),\n        ))\n\n    # Output layer\n    model.add(Dense(1))\n    \n    # Compile Model\n    model.compile(\n        optimizer=keras.optimizers.Adam(hp.Fixed(\"learning_rate\", 1e-4)),\n        loss=\"mse\",\n        metrics=[tf.keras.metrics.RootMeanSquaredError(name='rmse')]\n    )\n    return model","343b510b":"tuner = RandomSearch(\n    build_model,\n    objective=Objective(\"val_rmse\", direction=\"min\"),\n    max_trials=165,\n    directory=LOG_DIR,\n    overwrite=True\n)","560bcb42":"thresh_1__cb = ThresholdStopping(monitor=\"val_rmse\", thresh=0.4, num_epochs=5, mode='min', verbose=55)\nthresh_2__cb = ThresholdStopping(monitor=\"val_rmse\", thresh=1, num_epochs=2, mode='min', verbose=55)\nthresh_3__cb = ThresholdStopping(monitor=\"val_rmse\", thresh=0.3, num_epochs=15, mode='min', verbose=55)\nthresh_4__cb = ThresholdStopping(monitor=\"val_rmse\", thresh=0.23, num_epochs=30, mode='min', verbose=55)\nthresh_5__cb = ThresholdStopping(monitor=\"val_rmse\", thresh=0.16, num_epochs=80, mode='min', verbose=55)\nthresh_6__cb = ThresholdStopping(monitor=\"val_rmse\", thresh=0.15, num_epochs=100, mode='min', verbose=55)\ntuner.search(x=X_train, y=y_train, batch_size=64, epochs=200, validation_split=.3, callbacks=[thresh_1__cb, thresh_2__cb, thresh_3__cb, thresh_4__cb, thresh_5__cb, thresh_6__cb], verbose=1)","1fbcde73":"print(tuner.results_summary(num_trials=30))","f31e9ad7":"n=25\nbest = tuner.get_best_models(num_models=n)\nparams = tuner.get_best_hyperparameters(num_trials=n)\narchs = []\nlrs = []\nfor idx, model in enumerate(best):\n    arch = []\n    for layer in model.layers[:-1]:\n        try:\n            arch.append(dict(n_neurons=layer.units, activation=type(layer.activation).__name__ if type(layer.activation).__name__ is 'LeakyReLU' else layer.activation.__name__))\n        except:\n\n            pass\n\n    archs.append(arch)\n    lrs.append(params[idx].get('learning_rate'))\n        \n    print(*arch, sep=',\\n')\n    print(\"Learning Rate:\", params[idx].get('learning_rate'))\n#     print(model.summary())\n    print('\\n\\n')","f203854d":"archs = best_25_random_search = [\n    [\n        {'n_neurons': 1300, 'activation': 'relu'},\n        {'n_neurons': 900, 'activation': 'tanh'},\n        {'n_neurons': 500, 'activation': 'relu'},\n        {'n_neurons': 1700, 'activation': 'relu'}\n    ],\n\n    [\n        {'n_neurons': 100, 'activation': 'relu'},\n        {'n_neurons': 1500, 'activation': 'tanh'},\n        {'n_neurons': 500, 'activation': 'relu'},\n        {'n_neurons': 900, 'activation': 'tanh'}\n    ],\n\n    [\n        {'n_neurons': 1000, 'activation': 'relu'},\n        {'n_neurons': 1700, 'activation': 'tanh'},\n        {'n_neurons': 1700, 'activation': 'tanh'},\n        {'n_neurons': 1300, 'activation': 'relu'},\n        {'n_neurons': 1600, 'activation': 'tanh'},\n        {'n_neurons': 1200, 'activation': 'relu'}\n    ],\n\n    [\n        {'n_neurons': 1300, 'activation': 'relu'},\n        {'n_neurons': 700, 'activation': 'tanh'},\n        {'n_neurons': 1100, 'activation': 'relu'},\n        {'n_neurons': 1500, 'activation': 'relu'}\n    ],\n\n    [\n        {'n_neurons': 700, 'activation': 'relu'},\n        {'n_neurons': 1400, 'activation': 'tanh'},\n        {'n_neurons': 400, 'activation': 'tanh'}\n    ],\n\n    [\n        {'n_neurons': 1300, 'activation': 'relu'},\n        {'n_neurons': 1200, 'activation': 'tanh'},\n        {'n_neurons': 500, 'activation': 'relu'},\n        {'n_neurons': 1500, 'activation': 'relu'},\n        {'n_neurons': 800, 'activation': 'tanh'},\n        {'n_neurons': 900, 'activation': 'tanh'}\n    ],\n\n    [\n        {'n_neurons': 1500, 'activation': 'relu'},\n        {'n_neurons': 1200, 'activation': 'tanh'},\n        {'n_neurons': 1400, 'activation': 'relu'},\n        {'n_neurons': 1700, 'activation': 'relu'},\n        {'n_neurons': 700, 'activation': 'relu'},\n        {'n_neurons': 800, 'activation': 'relu'}\n    ],\n\n    [\n        {'n_neurons': 1800, 'activation': 'relu'},\n        {'n_neurons': 1700, 'activation': 'tanh'},\n        {'n_neurons': 800, 'activation': 'tanh'}\n    ],\n\n    [\n        {'n_neurons': 800, 'activation': 'relu'},\n        {'n_neurons': 1600, 'activation': 'tanh'},\n        {'n_neurons': 800, 'activation': 'tanh'}\n    ],\n\n    [\n        {'n_neurons': 1300, 'activation': 'relu'},\n        {'n_neurons': 1100, 'activation': 'tanh'},\n        {'n_neurons': 1100, 'activation': 'relu'},\n        {'n_neurons': 700, 'activation': 'relu'}\n    ],\n\n    [\n        {'n_neurons': 200, 'activation': 'relu'},\n        {'n_neurons': 1800, 'activation': 'tanh'},\n        {'n_neurons': 800, 'activation': 'relu'},\n        {'n_neurons': 1400, 'activation': 'tanh'}\n    ],\n\n    [\n        {'n_neurons': 1200, 'activation': 'relu'},\n        {'n_neurons': 900, 'activation': 'tanh'},\n        {'n_neurons': 400, 'activation': 'relu'},\n        {'n_neurons': 1800, 'activation': 'tanh'}\n    ],\n\n\n    [\n        {'n_neurons': 700, 'activation': 'relu'},\n        {'n_neurons': 500, 'activation': 'tanh'},\n        {'n_neurons': 1500, 'activation': 'relu'},\n        {'n_neurons': 400, 'activation': 'relu'},\n        {'n_neurons': 1100, 'activation': 'relu'},\n        {'n_neurons': 1400, 'activation': 'tanh'}\n    ],\n\n    [\n        {'n_neurons': 200, 'activation': 'relu'},\n        {'n_neurons': 1000, 'activation': 'tanh'},\n        {'n_neurons': 1400, 'activation': 'tanh'}\n    ],\n\n    [\n        {'n_neurons': 1000, 'activation': 'relu'},\n        {'n_neurons': 1700, 'activation': 'tanh'},\n        {'n_neurons': 1400, 'activation': 'relu'},\n        {'n_neurons': 500, 'activation': 'relu'}\n    ],\n\n    [\n        {'n_neurons': 1800, 'activation': 'relu'},\n        {'n_neurons': 1200, 'activation': 'tanh'},\n        {'n_neurons': 200, 'activation': 'relu'}\n    ],\n\n    [\n        {'n_neurons': 1700, 'activation': 'relu'},\n        {'n_neurons': 800, 'activation': 'tanh'},\n        {'n_neurons': 500, 'activation': 'tanh'},\n        {'n_neurons': 300, 'activation': 'relu'},\n        {'n_neurons': 1300, 'activation': 'relu'},\n        {'n_neurons': 300, 'activation': 'relu'}\n    ],\n\n    [\n        {'n_neurons': 1300, 'activation': 'relu'},\n        {'n_neurons': 1600, 'activation': 'tanh'},\n        {'n_neurons': 1300, 'activation': 'relu'},\n        {'n_neurons': 1800, 'activation': 'relu'}\n    ],\n\n    [\n        {'n_neurons': 400, 'activation': 'relu'},\n        {'n_neurons': 1100, 'activation': 'tanh'},\n        {'n_neurons': 1200, 'activation': 'tanh'},\n        {'n_neurons': 400, 'activation': 'tanh'},\n        {'n_neurons': 1300, 'activation': 'relu'},\n        {'n_neurons': 600, 'activation': 'relu'},\n        {'n_neurons': 1800, 'activation': 'tanh'}\n    ],\n\n    [\n        {'n_neurons': 400, 'activation': 'relu'},\n        {'n_neurons': 1200, 'activation': 'tanh'},\n        {'n_neurons': 700, 'activation': 'tanh'},\n        {'n_neurons': 800, 'activation': 'relu'},\n        {'n_neurons': 1200, 'activation': 'relu'}\n    ],\n\n    [\n        {'n_neurons': 1300, 'activation': 'relu'},\n        {'n_neurons': 800, 'activation': 'tanh'},\n        {'n_neurons': 1000, 'activation': 'relu'}\n    ],\n\n    [\n        {'n_neurons': 1500, 'activation': 'relu'},\n        {'n_neurons': 1700, 'activation': 'tanh'},\n        {'n_neurons': 1100, 'activation': 'relu'},\n        {'n_neurons': 200, 'activation': 'tanh'}\n    ],\n\n\n    [\n        {'n_neurons': 1300, 'activation': 'relu'},\n        {'n_neurons': 1700, 'activation': 'tanh'},\n        {'n_neurons': 1400, 'activation': 'tanh'},\n        {'n_neurons': 1300, 'activation': 'relu'}\n    ],\n\n    [\n        {'n_neurons': 700, 'activation': 'relu'},\n        {'n_neurons': 1800, 'activation': 'tanh'},\n        {'n_neurons': 900, 'activation': 'tanh'},\n        {'n_neurons': 600, 'activation': 'relu'},\n        {'n_neurons': 100, 'activation': 'relu'}\n    ],\n\n    [\n        {'n_neurons': 200, 'activation': 'relu'},\n        {'n_neurons': 1700, 'activation': 'tanh'},\n        {'n_neurons': 300, 'activation': 'relu'},\n        {'n_neurons': 1100, 'activation': 'relu'}\n    ],\n]","e3ef4597":"ensemble = ComplexModel(input_models_architectures=[*ComplexModel.___DEFAULT_INPUT_MODELS___, *archs], input_optimizer=keras.optimizers.Adam(0.0001))\nensemble.fit(X_train, y_train, batch_size=64, epochs=650, verbose=55, ensembel_epochs=100, keras_verbose=0, use_tmp_name=True)","6d56a24c":"ensemble_backup = ensemble","0e32838c":"\nimport datetime\nX_test = drop_cols(test_df, ['id', 'Eat'], inplace=False)\nsubmission_df = test_df[['id']]\nsubmission_df['Eat'] = ensemble.predict(X_test)\n\n# Save Submission to CSV\nsubmission_df.to_csv(f'000_ensemble-submission-3-mean-{datetime.datetime.now()}.csv',index=False)\nsubmission_df.head()","bf788f57":"import datetime\nX_test = drop_cols(test_df, ['id', 'Eat'], inplace=False)\nsubmission_df = test_df[['id']]\nsubmission_df['Eat'] = ensemble.predict_with_mean(X_test)\n\n# Save Submission to CSV\nsubmission_df.to_csv(f'000_ensemble-submission-3-mean-{datetime.datetime.now()}.csv',index=False)\nsubmission_df.head()","8d28db12":"# Define Architectures to Evaluate\neval_archs = ComplexModel.___DEFAULT_INPUT_MODELS___[-3:]\n\n# Evaluate Each Architechture\nfor i, arch in enumerate(eval_archs):\n    _model = create_model(arch, optimizer=keras.optimizers.Adam(0.0001))\n    \n    # Display Model Architecture\n    print(_model.summary())\n    print(*arch, sep='\\n')\n    \n    # Make evaluation for model\n    ret_val = make_eval(arch, X_train, y_train, n_splits=50, graph_report=True, graph_bins=10, epochs=350, verbose=4, keras_verbose=0, batch_size=64, optimizer=keras.optimizers.Adam(0.0001))\n    if i + 1 is not len(eval_archs): print('\\n\\n')","4cfca428":"import datetime\nepochs=700\n\n# Architectures for Submissions\narchs = [\n    [\n        # 0.12751464545726776\n        {'n_neurons': 200, 'activation':'relu'},\n        {'n_neurons': 1500, 'activation':'tanh'},\n        {'n_neurons': 1000, 'activation':'relu'},\n        {'n_neurons': 1400, 'activation':'relu'},\n        {'n_neurons': 1300, 'activation':'relu'},\n        {'n_neurons': 400, 'activation':'relu'},\n        {'n_neurons': 300, 'activation':'relu'},\n    ],\n    *ComplexModel.___DEFAULT_INPUT_MODELS___[-3:]\n]\n\n# Fit Each Model\nfor i, arch in enumerate(archs):\n    arch_name = f'model-fit--{i+1}'\n    \n    # Create Model from Architecture\n    model = create_model(arch, optimizer=keras.optimizers.Adam(0.0001))\n\n    # Checkpoint for best model\n    save_best_model_name = f'{arch_name}.hdf5'\n    checkpoint = ModelCheckpoint(save_best_model_name, save_best_only=True, monitor='val_rmse', mode='min')\n    \n    # Display Model Architecture\n    print(model.summary())\n    print(*arch, sep='\\n')\n\n    history = model.fit(X_train, y_train, validation_split=0.3, epochs=epochs, callbacks=[checkpoint], batch_size=64, verbose=0)\n\n    # Plot History\n    pd.DataFrame(history.history).plot(figsize=(8,5))\n    plt.grid(True)\n    plt.gca().set_ylim(0, 0.6)\n    plt.show()\n\n    # Load Best Model\n    submission_model = keras.models.load_model(save_best_model_name)\n\n    # Prepare Submission\n    X_test = drop_cols(test_df, ['id', 'Eat'], inplace=False)\n    submission_df = test_df[['id']]\n    submission_df['Eat'] = submission_model.predict(X_test)\n\n    # Save Submission to CSV\n    submission_df.to_csv(f'000_weirdo-{i+1}-{datetime.datetime.now()}.csv',index=False)\n    print(submission_df.head())\n    print('\\n\\n')","fdc0b8f0":"# Scored 0.15534 on the public leaderboard\n# Number 1 BAEYBEEEEEE\nmodel10 = Sequential()\nmodel10.add(Dense(200, activation='relu', input_shape=X_train.columns.shape))\nmodel10.add(Dense(400, activation='tanh'))\nmodel10.add(Dense(200, activation='relu'))\nmodel10.add(Dense(1))\n\nmodel10.compile(optimizer=keras.optimizers.Adam(0.0001), loss=\"mse\", metrics=[tf.keras.metrics.RootMeanSquaredError(name='rmse')])\n\n# Checkpoint\ncheckpoint = ModelCheckpoint('model_10.hdf5', save_best_only=True, monitor='val_rmse', mode='min')\n\nhistory = model10.fit(X_train, y_train, validation_split=.3, epochs=250, callbacks=[checkpoint])\n\n# ret_val = make_eval(model10, X_train, y_train, n_splits=50, graph_report=True, graph_bins=15, epochs=300, verbose=4, keras_verbose=0)\n\npd.DataFrame(history.history).plot(figsize=(8,5))\nplt.grid(True)\nplt.gca().set_ylim(0, 0.6)\nplt.show()","672455cb":"submission_model = keras.models.load_model('model_10.hdf5')\nprint(submission_model.summary())\nX_test = drop_cols(test_df, ['id', 'Eat'], inplace=False)\nsubmission_df = test_df[['id']]\nsubmission_df['Eat'] = submission_model.predict(X_test)\nsubmission_df.to_csv(f'my_weird_model_8.csv',index=False)\nsubmission_df","92c1b0fc":"### Ensemble Submission (Avg)","1994e412":"# Building a Model\nFirst we will make utility methods to create and evaluate our models.\\\nThen we will make a complex model class that is a composite ensemble of a few models.\\\nLastly we will define a random search using `keras-tuner` to help us find better architechtures and hyper parameters.","2cccb0ff":"#### Running the random Search","78ab9c9e":"# Ensemble Submission (Complex NN)","5e360e67":"# Split Data","f48c11a9":"# Making Submissions","70e6ae81":"### Drop `pubchem_id`\nIt's not a feature so it won't help with anything","9019ebbc":"## Exploring Data\nCheck for missing data in both `train` and `eval` (test) datasets\\\nAnd also make sure that the `pubchem_id`'s don't actually mean anything beyond being identifiers","83bd7b8f":"# The 'Comlpex' Model\nAn agregate of models that are being used to generate predictions which are fed to a final 'meta' model as features.\\\nIt can also be simplified and used to perform a simple average aggregation or use any other ML model as the final meta model.","3b254230":"# Stacking Submission\nApparently, what I implemented above is called stacking and there are few APIs for it.\\\nUsing only 3 of my top NN Architectures with a Ridge meta model results in a *~0.1* RMSE on leaderboard","6c12302c":"#### Extracting the Best Models\nCould run evaluations on them or use them for ensemble or submissions.\\\n*I'm sorry, it's long again.*","788d3a27":"### Submission CSV\nRMSE ~0.101","20f987d3":"# Load Dataset\nThis dataset contains ground state energies of 16,242 molecules calculated by quantum mechanical simulations.\n\n## Content\nThe data contains 1277 columns. The first 1275 columns are entries in the Coulomb matrix that act as molecular features. The 1276th column is the Pubchem Id where the molecular structures are obtained. The 1277th column is the atomization energy calculated by simulations using the Quantum Espresso package.\n\n## Inspiration\nSimulations of molecular properties are computationally expensive. The purpose of this project is to use machine learning methods to come up with a model that can predict molecular properties from a database of simulations. If this can be done with high accuracy, properties of new molecules can be calculated using the trained model. This could open up many possibilities in computational design and discovery of molecules, compounds and new drugs.\n\nThe purpose is to use the 1275 molecular features to predict the atomization energy. This is a regression problem so mean squared error is minimized during training.","ba0b0ac3":"# Using the Ensemble Model","1bfb6555":"# Random Search\nConfiguring a search space for `keras-tuner` and saving the best models from the random search.","28d394f1":"# Evaluating Models","2b0e2cd6":"### My first Submission\nFor giggles etc.","6bf35d48":"#### Random Search Summary\n*Sorry, it's long - the output is hidden.*"}}