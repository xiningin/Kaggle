{"cell_type":{"6f4989a5":"code","aeb13efa":"code","04030afc":"code","deb3ab16":"code","6feb8a2e":"code","8c615749":"code","1ba1d2d2":"code","adbb5065":"code","ed25893c":"code","b664b7e3":"code","61af6c3f":"code","83db04bf":"markdown","c4358c6e":"markdown","d097c853":"markdown","1942a9f2":"markdown","1707b32e":"markdown","23967d9c":"markdown"},"source":{"6f4989a5":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set_style('darkgrid')\nimport plotly.express as ex\nimport plotly.graph_objs as go\nimport plotly.figure_factory as ff\nfrom wordcloud import WordCloud,STOPWORDS\nimport nltk as nlp\nimport string \nimport re","aeb13efa":"p_data = pd.read_csv('\/kaggle\/input\/poe-short-stories-corpuscsv\/preprocessed_data.csv')\np_data.head(3)","04030afc":"title_language = []\ntext_language  = []\n\ntitle_bow = {}\ntext_bow = {} \n\nfor index,row in p_data.iterrows():\n    title_language += row['title'].lower().split(' ')\n    text_language += row['text'].lower().split(' ')\n\nfor index,row in p_data.iterrows():\n    title =  row['title'].lower().split(' ')\n    text  = row['text'].lower().split(' ')\n    for te in text:\n        text_bow[te] = text_bow.get(te,0) +1\n    for ti in title:\n        title_bow[ti] = title_bow.get(ti,0) +1\n          \n    \ntitle_language = list(set(title_language))\ntext_language = list(set(text_language))\n\n    ","deb3ab16":"all_texts = ' '.join(p_data.text.values)","6feb8a2e":"def generate_ngram(n,text):\n    s = text.lower()\n    s = re.sub(r'[^a-zA-Z0-9\\s]', ' ', s)\n    tokens = s.split(' ')\n    ngrams = zip(*[tokens[i:] for i in range(n)])\n    return [\" \".join(ngram) for ngram in ngrams]\n\nbigram = generate_ngram(2,all_texts)\ndef get_nword_probs(word):\n    contains = [w.split(' ')[1] for w in bigram if w.split(' ')[0] == word and w.split(' ')[1] != '']\n    cont_dic = {}\n    for word in contains:\n        cont_dic[word] = cont_dic.get(word,0)+1\n    occ = len(contains)\n    cont_dic = {word:cont_dic[word]\/occ for word in cont_dic.keys()}\n    return cont_dic    \n\ndef get_next_word(cur_word,alpha):\n    prob_dic = get_nword_probs(cur_word)\n    prob_dic_top_5 = sorted(prob_dic, key=prob_dic.get, reverse=True)[:5]\n    if np.random.normal(0,1,1) > alpha and len(prob_dic_top_5)>4:\n        return prob_dic_top_5[int(np.round(np.random.uniform(1,4,1)))]\n    elif len(prob_dic_top_5) == 0:\n        return list(STOPWORDS)[int(np.round(np.random.uniform(0,len(STOPWORDS)-1,1)))]\n    else:\n        return prob_dic_top_5[0]\n\ndef get_random_words(n_words):\n    tsample = p_data.text.sample(int(np.sqrt(n_words)))\n    words = []\n    for i in tsample:\n        words += i.split(' ')\n    choice = np.round(np.random.uniform(0,len(words),n_words))\n    return [words[int(i)] for i in choice]","8c615749":"words = get_random_words(1)\npoem_length = 80\npoem = ''\ncur_word = words[0]\nfor i in range(0,poem_length):\n    poem+= (' '+(get_next_word(cur_word,0.5)))\n    if np.random.normal(0,1,1) >0.8:\n        poem+='\\n'\n    elif np.random.normal(0,1,1) >0.7:\n        poem+=','\n    elif np.random.normal(0,1,1) >0.9:\n        words = get_random_words(5)\n        words = [word for word in words if word not in STOPWORDS]\n        if len(words) == 0:\n            cur_word = get_next_word(cur_word,0.5)\n        else:\n            cur_word = words[0]\n    else:\n        cur_word = get_next_word(cur_word,0.5)\n","1ba1d2d2":"print(poem)","adbb5065":"words = get_random_words(1)\npoem_length = 80\npoem = ''\ncur_word = words[0]\nfor i in range(0,poem_length):\n    poem+= (' '+(get_next_word(cur_word,0.8)))\n    if np.random.normal(0,1,1) >0.8:\n        poem+='\\n'\n    elif np.random.normal(0,1,1) >0.7:\n        poem+=','\n    elif np.random.normal(0,1,1) >0.9:\n        words = get_random_words(5)\n        words = [word for word in words if word not in STOPWORDS]\n        if len(words) == 0:\n            cur_word = get_next_word(cur_word,0.8)\n        else:\n            cur_word = words[0]\n    else:\n        cur_word = get_next_word(cur_word,0.8)\n","ed25893c":"print(poem)","b664b7e3":"words = get_random_words(1)\npoem_length = 120\npoem = ''\ncur_word = words[0]\nfor i in range(0,poem_length):\n    poem+= (' '+(get_next_word(cur_word,0.62)))\n    if np.random.normal(0,1,1) >0.8:\n        poem+='\\n'\n    elif np.random.normal(0,1,1) >0.7:\n        poem+=','\n    elif np.random.normal(0,1,1) >0.9:\n        words = get_random_words(5)\n        words = [word for word in words if word not in STOPWORDS]\n        if len(words) == 0:\n            cur_word = get_next_word(cur_word,0.62)\n        else:\n            cur_word = words[0]\n    else:\n        cur_word = get_next_word(cur_word,0.62)\n","61af6c3f":"print(poem)","83db04bf":"# $\\text{Introduction}$\n\n![](https:\/\/storage.googleapis.com\/kaggle-datasets-images\/34436\/46048\/ef20fcd937e23fe3062cd2b7ab48f212\/data-original.jpg?t=2018-06-30-08-42-20)\n\nIn the following Kernel we will try and see can we get some reasonable poems using a simple bigram model and word conditional probabilites.\nwe all know that making a bombastic nlp model on such a small dataset will give us poems that resemble Poes poems and stories. The question I will investigate in this Kernel is how good of a Poem can a bigram model with some randomness produce.\n\nThe logic behind the following code will be as follows:\n1. Create a bigram model of all Poem \/ Story text's \n2. Create helper functions to extract the probability and decide on the next word using the following probability:\n\nLet $w1$ be the current word we will find all the bigrams that start with $w1$ and calculate $P(w2 | w1 )$ $=>$ $count(w1w2) \/ count(w1)$ \n\nWe will fill a list with 5 words with the highest probabilities and use another variable $'alpha'$ to decide which word should we take.\nWe will choose a random number from a normal distribution and if this number is larger than $alpha$ we will randomly select a word from the less probable words meaning words 1-4 (without the most probable word at index 0 ).\nelse we will return the most probable word at index 0.\n\n","c4358c6e":"# Preprocessing And Constraction Of Our Text's Language ","d097c853":"## Test Poem 2","1942a9f2":"# Creating Helper Functions For N-gram Model Construction And Probability Calculation","1707b32e":"## Test Poem 3","23967d9c":"## Test Poem 1"}}