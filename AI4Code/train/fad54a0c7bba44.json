{"cell_type":{"6975149c":"code","b6ff7849":"code","1f12be05":"code","9dd25750":"code","37c3a229":"code","9c62e48f":"code","ce5b2172":"code","92f49d3d":"code","fc87c159":"code","76e9fa0b":"code","d2ddbcde":"code","a3bb83b1":"code","3cb02997":"code","008fb8ee":"code","557ac540":"code","d672ea77":"code","fee70e28":"code","3a08742f":"code","281b681a":"code","ba54a74f":"markdown","24e187cd":"markdown","8034a374":"markdown","a5f71bc6":"markdown","1164c9ec":"markdown","f7207776":"markdown","5eefa620":"markdown","7d7f76ac":"markdown"},"source":{"6975149c":"import re\nfrom bs4 import BeautifulSoup\nimport os\nimport random\nimport joblib\nimport numpy as np\nimport pandas as pd\n\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.model_selection import KFold, cross_val_score\n\nfrom sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, VotingRegressor\nfrom sklearn.kernel_ridge import KernelRidge\n\nimport matplotlib.pyplot as plt","b6ff7849":"TEST_DATA_PATH = '..\/input\/jigsaw-toxic-severity-rating\/comments_to_score.csv'\nVALID_DATA_PATH = '..\/input\/jigsaw-toxic-severity-rating\/validation_data.csv'\nTRAIN_DATA_PATH = '..\/input\/d\/julian3833\/jigsaw-toxic-comment-classification-challenge\/train.csv'","1f12be05":"VAL_SIZE = 0.15\nSEED = 10\nN_FOLDS = 5","9dd25750":"def set_seed(seed=42):\n    \"\"\"Utility function to use for reproducibility.\n    :param seed: Random seed\n    :return: None\n    \"\"\"\n    np.random.seed(seed)\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n\n\ndef set_display():\n    \"\"\"Function sets display options for charts and pd.DataFrames.\n    \"\"\"\n    # Plots display settings\n    plt.style.use('fivethirtyeight')\n    plt.rcParams['figure.figsize'] = 12, 8\n    plt.rcParams.update({'font.size': 14})\n    # DataFrame display settings\n    pd.set_option('display.max_columns', None)\n    pd.set_option('display.max_rows', None)\n    pd.options.display.float_format = '{:.4f}'.format\n    \ndef text_cleaning(text: str) -> str:\n    \"\"\"Function cleans text removing special characters,\n    extra spaces, embedded URL links, HTML tags and emojis.\n    Code source: https:\/\/www.kaggle.com\/manabendrarout\/pytorch-roberta-ranking-baseline-jrstc-infer\n    :param text: Original text\n    :return: Preprocessed text\n    \"\"\"\n    template = re.compile(r'https?:\/\/\\S+|www\\.\\S+')  # website links\n    text = template.sub(r'', text)\n\n    soup = BeautifulSoup(text, 'lxml')  # HTML tags\n    only_text = soup.get_text()\n    text = only_text\n\n    emoji_pattern = re.compile(\"[\"\n                               u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                               u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                               u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                               u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                               u\"\\U00002702-\\U000027B0\"\n                               u\"\\U000024C2-\\U0001F251\"\n                               \"]+\", flags=re.UNICODE)\n    text = emoji_pattern.sub(r'', text)\n\n    text = re.sub(r\"[^a-zA-Z\\d]\", \" \", text)  # special characters\n    text = re.sub(' +', ' ', text)  # extra spaces\n    text = text.strip()  # spaces at the beginning and at the end of string\n\n    return text","37c3a229":"set_seed(SEED)\nset_display()","9c62e48f":"# Extract classified text samples.\ndata_train = pd.read_csv(TRAIN_DATA_PATH)\ndata_train.head()","ce5b2172":"data_train.iloc[6]","92f49d3d":"data_train.describe()","fc87c159":"categories = data_train.loc[:, 'toxic':'identity_hate'].sum()\nplt.title('Category Frequency')\nplt.bar(categories.index, categories.values)\nplt.show()","76e9fa0b":"scores = data_train.loc[:, 'toxic':'identity_hate'].sum(axis=1).value_counts()\nplt.bar(scores.index, scores.values)\nplt.title('Scores Distribution: Simple Sum')\nplt.show()","d2ddbcde":"# Multiplication factors for categories.\ncat_mtpl = {'toxic': 1, 'severe_toxic': 1.75, 'obscene': 0.95,\n            'threat': 2, 'insult': 1.6, 'identity_hate': 1.95}\n\nfor category in cat_mtpl:\n    data_train[category] = data_train[category] * cat_mtpl[category]\n\ndata_train['score'] = data_train.loc[:, 'toxic':'identity_hate'].sum(axis=1)","a3bb83b1":"plt.hist(data_train['score'])\nplt.title('Scores Distribution: Adjusted Sum')\nplt.show()","3cb02997":"n_samples_toxic = len(data_train[data_train['score'] != 0])\nn_samples_normal = len(data_train) - n_samples_toxic\n\nidx_to_drop = data_train[data_train['score'] == 0].index[n_samples_toxic\/\/5:]\ndata_train = data_train.drop(idx_to_drop)\n\nprint(f'Reduced number of neutral text samples from {n_samples_normal} to {n_samples_toxic\/\/5}.')\nprint(f'Total number of training samples: {len(data_train)}')","008fb8ee":"print(f'Mean toxicity score: {data_train[\"score\"].mean()}\\n'\n      f'Standard deviation: {data_train[\"score\"].std()}')","557ac540":"data_train.iloc[6]","d672ea77":"X = data_train[\"comment_text\"]\nY = data_train['score']","fee70e28":"X = X.apply(text_cleaning)","3a08742f":"X.head()","281b681a":"# New data for validation: text pairs.\ndata_valid = pd.read_csv(VALID_DATA_PATH)\n\n# Clean the texts\ndata_valid['less_toxic'] = data_valid['less_toxic'].apply(text_cleaning)\ndata_valid['more_toxic'] = data_valid['more_toxic'].apply(text_cleaning)\n\ndata_valid.head()","ba54a74f":"In the previous competition the task was to perform multi-class classification. Text sample could be labeled with one or several categories or not labeled with any. Non-toxic comments represent the majority of text samples, while toxic comments are a minority class and extremely toxic comments are more rare than plain toxic.\n\nIn this competition we have to score texts based on the level of toxicity. To get a toxicity score from the previous data we can use two approaches:\n- Simply sum up all values in each row of the DataFrame. The toxicity score will vary between 0 and 6. However some unequally toxic samples could have the same score.\n- Adjust the values in the DataFrame according to extremety of the category (for example, \"toxic\" and \"severe toxic\" should have different score) and then sum up per row values.","24e187cd":"**Now You have value of X and Y. Now you can train your model whatever type you want. If It helps you so please give upvaote.**","8034a374":"## Functions","a5f71bc6":"Data set is imbalanced: number of neutral text samples is much larger than the number of toxic samples. We limit number of samples with 0 toxicity score to 1\/5 of the total number of texts labeled as toxic.","1164c9ec":"By the rules of this competition we can use public data sets on Kaggle. However for some reason we cannot submit notebooks that directly use the data from the previous competition of 4 years ago where text samples were labeled with 6 classes. In this version we use an exact copy of this competition data set created and published by [Julian Peller](https:\/\/www.kaggle.com\/julian3833) [here](https:\/\/www.kaggle.com\/julian3833\/jigsaw-toxic-comment-classification-challenge).","f7207776":"**This is the starter code of the Compitition [jigsaw-toxic-severity-rating](https:\/\/www.kaggle.com\/c\/jigsaw-toxic-severity-rating)**","5eefa620":"## Data processing","7d7f76ac":"In the original DataFrame each category contains binary values (0 or 1). We will change the original values using multiplicative factors observing the following (disputable) common sense rules:\n- Normal non-offensive text samples would have a score of 0.\n- \"toxic\" category with a score of 1 would be used as a benchmark to score other offensive categories.\n- Samples marked as \"severe_toxic\" would have higher toxicity score than those marked as \"toxic\".\n- Obscene language along would have slightly lower score than \"toxic\" samples.\n- Insults would be scored in between \"toxic\" and \"severe_toxic\" closer to the upper bound.\n- Samples containing threats would have the highest toxicity score.\n- Identity hate would be scored marginally lower than threats.\n- If a sample is marked as belonging to several offensive classes total score would be calculated as a sum of values in all individual categories."}}