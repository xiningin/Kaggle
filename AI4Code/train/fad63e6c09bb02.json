{"cell_type":{"ead1cc79":"code","87df9456":"code","ead5382c":"code","23529d67":"code","cd08493b":"code","ed5dcb21":"code","9d877fc9":"code","e5c2326b":"code","a27b95db":"code","b7baf594":"code","373c6b49":"code","ef25e85f":"code","342e436a":"code","c29a7fc1":"code","2b2a3e23":"code","04c2e07c":"code","b29602b3":"code","e858c88b":"code","87c5b85b":"code","20ce2057":"code","a117e670":"code","aa3f0fb5":"code","8568211b":"code","7e49361a":"code","7f784a04":"code","527fd711":"code","389a65b4":"code","b791afd3":"code","93352d8e":"code","321f521f":"code","e534dcc2":"markdown","b45436ea":"markdown","9e5b05d7":"markdown","c6fe4a60":"markdown","d3c76ca3":"markdown","0d78cb50":"markdown","5878402c":"markdown","c8f0315e":"markdown"},"source":{"ead1cc79":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport re\n\nimport nltk as nlp # Main Library for NLP\nfrom nltk.corpus import stopwords # stopwords\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.naive_bayes import MultinomialNB #MultinomialNB # for the model - also try Guassian\n\nimport nltk, re, string, collections\nfrom nltk.util import ngrams\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","87df9456":"fake = pd.read_csv(\"..\/input\/fake-and-real-news-dataset\/Fake.csv\")\nreal = pd.read_csv(\"..\/input\/fake-and-real-news-dataset\/True.csv\")","ead5382c":"fake.head()","23529d67":"real.head()","cd08493b":"fake.shape,real.shape\n#if i conacatenate these you want the rows to be (23481+21417=44898)","ed5dcb21":"fake.info()\nreal.info()","9d877fc9":"#adding classifier as a number: 1 if the article is fake, 0 otherwise\nfake['class'] = 1\nreal['class'] = 0\n\nall_data = pd.concat([fake,real])\nall_data.head()","e5c2326b":"all_data.shape","a27b95db":"fig, ax = plt.subplots(figsize=(10,10))\nplt.xticks(rotation=45)\nax = sns.countplot(x = all_data.subject)","b7baf594":"all_data = all_data.drop(['title','date','subject'], axis=1)\nall_data.head()","373c6b49":"all_data = all_data.sample(frac = 1) #Shuffling our data\nall_data.head()","ef25e85f":"nlp.download(\"stopwords\") \nlemma=nlp.WordNetLemmatizer()","342e436a":" def text_process(data):\n    text_list=[]\n    for text in data.text:\n        text=re.sub(\"[^a-zA-Z]\",\" \",text) # extracting unnecesary characters\n        text=text.lower() #makes characters lowercase\n        text=nlp.word_tokenize(text) # splits all the words\n        text=[word for word in text if not word in set(stopwords.words(\"english\"))] # extract stopwords\n        text=[lemma.lemmatize(word) for word in text] # Lemmatisation\n        text=\" \".join(text) \n        text_list.append(text)\n        \n    return text_list","c29a7fc1":"fake_sample = fake.sample(n=1000) #10,000\nfake_text = text_process(fake_sample)","2b2a3e23":"text_all = ''\nfor text in fake_text:    \n    text_all = text_all + \" \" + text\n\n    \nbigrm = list(nltk.bigrams(text_all.split()))\nBigramFreq = collections.Counter(bigrm)\nBigramFreq = BigramFreq.most_common(10)\nprint(BigramFreq)","04c2e07c":"bigrams = [x[0] for x in BigramFreq]\ncount = [x[1] for x in BigramFreq]\n\nfor i in range(len(bigrams)):\n    bigrams[i] = bigrams[i][0] + \" \" + bigrams[i][1]\n\n    \nf, ax = plt.subplots(figsize=(10,10))\nplt.xticks(rotation=45)\nsns.barplot(x=bigrams,y=count)","b29602b3":"real_sample = real.sample(n=1000) #10,000\nreal_text = text_process(real_sample)","e858c88b":"text_all_2 = ''\nfor text in real_text:    \n    text_all_2 = text_all_2 + \" \" + text\n\n    \nbigrm2 = list(nltk.bigrams(text_all_2.split()))\nBigramFreq2 = collections.Counter(bigrm2)\nBigramFreq2 = BigramFreq2.most_common(10)\nprint(BigramFreq2)","87c5b85b":"bigrams2 = [x[0] for x in BigramFreq2]\ncount2 = [x[1] for x in BigramFreq2]\n\nfor i in range(len(bigrams2)):\n    bigrams2[i] = bigrams2[i][0] + \" \" + bigrams2[i][1]\n\n    \nf, ax = plt.subplots(figsize=(10,10))\nplt.xticks(rotation=45)\nsns.barplot(x=bigrams2,y=count2)","20ce2057":"all_data_sample = all_data.sample(n=1000) #10,000\nall_data_processed = text_process(all_data_sample)","a117e670":"max_features=1000\ncount_vectorizer=CountVectorizer(max_features=max_features,stop_words=\"english\")\nsparce_matrix=count_vectorizer.fit_transform(all_data_processed).toarray()","aa3f0fb5":"print(sparce_matrix)","8568211b":"y=all_data_sample.iloc[: , -1]\nx=sparce_matrix","7e49361a":"print(y)","7f784a04":"x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.1,random_state=5)","527fd711":"nb=MultinomialNB()\nnb.fit(x_train,y_train)\ny_pred= nb.predict(x_test)","389a65b4":"y_test = y_test.to_numpy()","b791afd3":"print(type(y_test))\nprint(type(y_pred))","93352d8e":"print(y_test)\nprint(y_pred)","321f521f":"count = 0\nfor i in range(len(y_pred)):\n    if y_test[i] == y_pred[i]:\n        count+=1\n    \nprint(\"Accuracy: \" + str((count\/len(y_pred))* 100))","e534dcc2":"Lets process our text!","b45436ea":"**Accuracy is 94% for Multinomial Bayes - this accuracy could be increased if we choose a larger sample size.**","9e5b05d7":"# 2. Text Preprocessing","c6fe4a60":"# 3. Modelling","d3c76ca3":"Now lets do this for the real articles","0d78cb50":"# 1. EDA and Data Preprocessing","5878402c":"**I am going to be using a multinomial naive bayes classifier. You could do this with a Gaussian naive bayes classifier as well - however multinomial is better for classifying between two distinct classes - in our case this is Real and Fake news.**","c8f0315e":"**You will see me take sample sizes for my datasets - this is because my computer is not powerful enough to do this for the full datasets**"}}