{"cell_type":{"93773041":"code","8e761896":"code","6e99fee5":"code","91878b26":"code","b106cbf3":"code","d287058a":"code","3b30c717":"code","bff64b25":"code","3f968f51":"code","16211099":"code","e366170f":"code","84593b10":"code","da8a7aec":"code","e7275373":"code","5e0f64d8":"code","9fde75c9":"code","c44cb7f3":"code","720de867":"code","875d9518":"code","e93d9141":"code","1a9193a0":"code","99740e44":"code","6c14c4eb":"code","6577b101":"code","770dda69":"code","8d4345b9":"code","f7f977d4":"code","3788c971":"code","29baea1c":"code","cb6bb88b":"code","9db8ad17":"code","9159310c":"code","7c799b55":"code","495a0f8c":"code","bbf9ab5a":"code","bf1c31d4":"code","bb9dadc5":"code","20c8da29":"code","fc2dd2ff":"code","6d1a6147":"code","c96b0c5b":"code","b2b5256d":"code","5fe47e28":"code","3c67d995":"code","e47f3cc8":"code","c85143a2":"code","371ca63d":"code","f6ce96c3":"code","a812c684":"code","932a9dc4":"code","c704d405":"code","d83cea1d":"code","711d0f0f":"code","b47cd4dd":"code","3be66d7c":"code","05665c4e":"code","abce0f6b":"code","b78e25b9":"code","f1d1d39c":"code","9b1701cf":"markdown","4b4c6fa3":"markdown","7b690276":"markdown","6d94ba61":"markdown","2b357653":"markdown","68d9afcc":"markdown","0b3721e6":"markdown","0caad98d":"markdown","1d047759":"markdown","54de3565":"markdown","6e369ea7":"markdown","eb5a7f2e":"markdown","7b26a040":"markdown","6aebbbb5":"markdown","3c17f225":"markdown","7b5c3ab4":"markdown","75ae1337":"markdown","9d9e0095":"markdown","3bf2dd65":"markdown","e27ddb1a":"markdown","e08bdca3":"markdown","ab0c4642":"markdown","37f58354":"markdown","fe296227":"markdown","8c275b15":"markdown","dacd519e":"markdown","9ee5d1d6":"markdown","ed24b18d":"markdown","13c84eef":"markdown","6188ea03":"markdown","cad39f08":"markdown","939f3af0":"markdown","bb4598c3":"markdown","cfa28568":"markdown","cf4e7ff8":"markdown","c6573329":"markdown","d5b3d0df":"markdown"},"source":{"93773041":"import numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.preprocessing import StandardScaler\nfrom imblearn.over_sampling import SMOTE\nfrom imblearn.under_sampling import RandomUnderSampler\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom imblearn.pipeline import Pipeline\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\nfrom sklearn.tree import ExtraTreeClassifier\nfrom xgboost import XGBClassifier\n\nfrom sklearn.metrics import confusion_matrix, roc_auc_score, roc_curve\nfrom sklearn.inspection import permutation_importance\n\nimport pickle\nfrom time import time\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nsns.set_style('darkgrid')        ","8e761896":"train_df = pd.read_csv(\"..\/input\/health-insurance-cross-sell-prediction\/train.csv\")\ntrain_df.head()","6e99fee5":"print(\"There are total {} samples present in the dataset each with {} features.\"\n      .format(train_df.shape[0], train_df.shape[1]))","91878b26":"train_df.describe()","b106cbf3":"train_df.select_dtypes(include='object').columns","d287058a":"train_df.select_dtypes(include='object').nunique()","3b30c717":"fig, ax = plt.subplots(1,2, figsize=(14,8))\nsns.countplot(x='Gender', data=train_df, ax=ax[0])\nsns.countplot(x='Gender', hue='Response', data=train_df, ax=ax[1])","bff64b25":"print(\"response:no response ratio for male customers : {}\"\n.format(len(train_df[(train_df['Gender']=='Male') & (train_df['Response']==1)])\/len(train_df[(train_df['Gender']=='Male') & (train_df['Response']==0)])))","3f968f51":"print(\"response:no response ratio for female customers : {}\"\n.format(len(train_df[(train_df['Gender']=='Female') & (train_df['Response']==1)])\/len(train_df[(train_df['Gender']=='Female') & (train_df['Response']==0)])))","16211099":"plt.figure(figsize=(14,8))\nsns.barplot(x='Gender', y='Age', hue='Response', data=train_df)\nplt.legend(loc='upper right', bbox_to_anchor=(1.1,1.0))","e366170f":"fig,ax = plt.subplots(1,3,figsize=(14,6))\nsns.countplot('Driving_License', data=train_df, ax=ax[0])\nsns.countplot('Driving_License', hue='Response', data=train_df, ax=ax[1])\nsns.countplot('Driving_License', hue='Previously_Insured', data=train_df, ax=ax[2])","84593b10":"fig,ax = plt.subplots(1,2,figsize=(14,8))\nsns.countplot('Previously_Insured', data=train_df, ax=ax[0])\nsns.countplot('Previously_Insured', hue='Response', data=train_df, ax=ax[1])","da8a7aec":"fig, ax = plt.subplots(1,3,figsize=(14,6))\nsns.countplot('Vehicle_Damage', data=train_df, ax=ax[0])\nsns.countplot('Vehicle_Damage', hue='Response', data=train_df, ax=ax[1])\nsns.countplot('Vehicle_Damage', hue='Previously_Insured', data=train_df, ax=ax[2])\nplt.tight_layout()","e7275373":"plt.figure(figsize=(12,6))\nsns.distplot(train_df['Age'])","5e0f64d8":"fig, ax = plt.subplots(1, 3, figsize=(14,8))\nsns.countplot('Vehicle_Age', data=train_df, ax=ax[0])\nsns.countplot('Vehicle_Age', hue='Response', data=train_df, ax=ax[1])\nsns.countplot('Vehicle_Age', hue='Previously_Insured', data=train_df, ax=ax[2])\nplt.tight_layout()","9fde75c9":"fig, ax = plt.subplots(2,2,figsize=(14,6))\nsns.distplot(train_df['Annual_Premium'], ax=ax[0,0])\nsns.barplot(x='Response', y='Annual_Premium', data=train_df, ax=ax[0,1])\nsns.distplot(train_df[train_df['Response']==0]['Annual_Premium'], ax=ax[1,0])\nsns.distplot(train_df[train_df['Response']==1]['Annual_Premium'], ax=ax[1,1])\nax[0,0].set_xlim([0,100000])\nax[1,0].set_xlim([0,100000])\nax[1,1].set_xlim([0,100000])","c44cb7f3":"train_df['Annual_Premium'].mean()","720de867":"sns.distplot(train_df['Policy_Sales_Channel'])","875d9518":"# top 10 marketting channels used by the company\ntrain_df['Policy_Sales_Channel'].value_counts().head(10).plot(kind='bar', figsize=(14,8))","e93d9141":"# top 10 marketting channels for the customers who didn't respond\ntrain_df[train_df['Response']==0]['Policy_Sales_Channel'].value_counts(normalize=True).head(10).plot(kind='bar')","1a9193a0":"# top 10 marketting channels for the customers who responded\ntrain_df[train_df['Response']==1]['Policy_Sales_Channel'].value_counts(normalize=True).head(10).plot(kind='bar')","99740e44":"train_df.head()","6c14c4eb":"train_df.isnull().sum()","6577b101":"# drop the 'id' column since it won't be used during model traiing\nfinal_train_df = train_df.drop('id', axis=1)","770dda69":"final_train_df[final_train_df.duplicated()]","8d4345b9":"final_train_df.drop_duplicates(inplace=True)","f7f977d4":"print(\"The shape of the dataframe after dropping duplicate rows is : {}\".format(final_train_df.shape))","3788c971":"for col in final_train_df.select_dtypes(include='object').columns:\n    print(col, \":\", final_train_df[col].unique())\n    print()","29baea1c":"# encoding binary categorical features\nfinal_train_df.replace({'Male':0, 'Female':1, 'No':0, 'Yes':1}, inplace=True)\nfinal_train_df.head()","cb6bb88b":"# create dummy variables for the categorical feature with more than two classes\nfinal_train_df = pd.get_dummies(final_train_df, drop_first=True)\nfinal_train_df.head()","9db8ad17":"corr_df = final_train_df.corr()\nplt.figure(figsize=(12,8))\nsns.heatmap(corr_df, annot=True, cmap='coolwarm')","9159310c":"# the feature matrix\nX = final_train_df.drop('Response', axis=1)\n\n# the target vector\ny = final_train_df['Response']","7c799b55":"X.head()","495a0f8c":"# distribution of classes in the target variable\ny.value_counts(normalize=True)","bbf9ab5a":"y.value_counts().plot(kind='bar')","bf1c31d4":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\nprint(\"The shape of the training set is {}\".format(X_train.shape))\nprint(\"The shape of the training set is {}\".format(X_test.shape))","bb9dadc5":"y_train.value_counts(normalize=True)","20c8da29":"y_test.value_counts(normalize=True)","fc2dd2ff":"def build_pipeline(clf):\n    \n    \"\"\"\n    Function to build a data pipeline consisting of the following steps :\n    \n    1. over : oversampling the minority class (response=1) class using SMOTE technique.\n    2. under : undersampling the majority class (response=0).\n    3. scaler : standardizing the dataset.\n    4. clf : the classification algorithm.\n    \n    Parameter\n    ----------\n    clf : object of a class\n       the classification class object\n       \n    Returns   \n    ---------\n    pipeline : object of a class\n       the data pipeline object \n    \n    \"\"\"\n    \n    over = SMOTE(sampling_strategy=0.2)\n    under = RandomUnderSampler(sampling_strategy=0.5)\n    scaler = StandardScaler()\n    \n    pipeline = Pipeline([\n                        ('over', over),\n                        ('under', under),\n                        ('scaler', scaler),\n                        ('clf', clf)\n                       ])\n    return pipeline","6d1a6147":"# list of classifiers to be analyzed\nclf_list = [LogisticRegression(), KNeighborsClassifier(), RandomForestClassifier(), \n            AdaBoostClassifier(), XGBClassifier()]\n\n# list of dictionaries of parameters and corresponding values associated with each of the classifiers in the above \n# list that will be used during grid search\n\nparam_list = [\n             {\n              'clf__C':[0.01, 0.05, 0.1, 0.3],\n              'clf__class_weight':[None, 'balanced']                \n             },\n             {\n              'clf__n_neighbors':[5, 10, 15, 20],\n              'clf__weights':['uniform', 'distance']   \n             },\n             {\n              'clf__n_estimators':[80, 100, 150],\n              'clf__criterion':['gini', 'entropy'],\n              'clf__class_weight':[None, 'balanced']   \n             },\n             {\n              'clf__n_estimators':[80, 100, 150],\n              'clf__learning_rate':[0.001, 0.01, 0.1]   \n             },\n             {\n              'clf__n_estimators':[80, 100, 150],\n              'clf__learning_rate':[0.001, 0.01, 0.1]                    \n             }\n             ]","c96b0c5b":"#roc_score_max = 0.\n\n# loop over classifiers and corresponding dictionary of parameters\n#for param_dict, clf in zip(param_list, clf_list):\n    \n    # build the pipeline\n#    pipeline = build_pipeline(clf)\n#    print(\"Now running the model : {}\".format(pipeline.steps[3][1]))\n    \n    # build the GridSearchCV object\n#    grid_cv = GridSearchCV(pipeline, param_grid=param_dict, cv=3, scoring='roc_auc', verbose=2)\n#    print()\n#    initial_time = time()\n    \n    # fit this object to the training set\n#    grid_cv.fit(X_train, y_train)\n#    train_time = time() - initial_time\n#    print(\"total time taken for fitting the model : {}\".format(train_time))\n#    print()\n    \n    # make predictions on the test set\n#    pred = grid_cv.predict_proba(X_test)\n    \n    # compute the roc_auc score for the test set\n#    roc_score = roc_auc_score(y_test,pred[:, 1:])\n    \n    # store the best roc_score and the corresponding classifier\n#    if roc_score > roc_score_max:\n#        roc_score_max = roc_score\n#        opt_model = pipeline.steps[3][1]\n#        opt_param = grid_cv.best_params_\n#        opt_val_score, opt_test_score = grid_cv.best_score_, roc_score_max\n        \n        \n#print(\"the best model is {} for the parameter set {} that produces an roc_auc_score {} when evaluated on the train set and roc_auc_score {} when evaluated on the test set.\"\n#      .format(opt_model, opt_param, opt_val_score, opt_test_score))        ","b2b5256d":"# further optimization of the best model found during grid search\nxgb = XGBClassifier()\n\nparam_dict = {'clf__n_estimators':list(np.arange(250,400,50)),\n              'clf__learning_rate':list(np.round(np.arange(0.1,0.4,0.15),2)),\n              'clf__scale_pos_weight':[0.3,0.4,0.5] }\n\n\npipeline = build_pipeline(xgb)\ngrid_cv = GridSearchCV(pipeline, param_grid=param_dict, cv=5, scoring='roc_auc', verbose=2)\n\ninitial_time = time()\ngrid_cv.fit(X_train, y_train)\ntrain_time = time() - initial_time\nprint(\"total time taken for fitting the model : {}\".format(train_time))\nprint()\npred = grid_cv.predict_proba(X_test)\nroc_score = roc_auc_score(y_test,pred[:, 1:])","5fe47e28":"print(\"the best parametet set for the optimized xgb classifier is : {}\".format(grid_cv.best_params_))","3c67d995":"print(\"ROC AUC score for the optimized model is {}\".format(roc_score))","e47f3cc8":"# save the model\npickle.dump(grid_cv, open(\"model_xgb.pickle\", \"wb\"))","c85143a2":"#grd_boost = GradientBoostingClassifier()\n\n#param_dict = {'clf__n_estimators':list(np.arange(280,350,20)),\n#              'clf__learning_rate':list(np.round(np.arange(0.2,0.4,0.05),2))}\n\n\n#pipeline = build_pipeline(grd_boost)\n#grid_cv_grd = GridSearchCV(pipeline, param_grid=param_dict, cv=5, scoring='roc_auc', verbose=2)\n\n#initial_time = time()\n#grid_cv_grd.fit(X_train, y_train)\n#train_time = time() - initial_time\n#print(\"total time taken for fitting the model : {}\".format(train_time))\n#print()\n#pred_gb = grid_cv_grd.predict_proba(X_test)\n#roc_score = roc_auc_score(y_test,pred_gb[:, 1:])","371ca63d":"#param_dict = {'clf__n_estimators':list(np.arange(20,100,10)),\n#              'clf__learning_rate':list(np.round(np.arange(0.1,0.8,0.1),2)),\n#              'clf__base_estimator__min_samples_split':[40, 60, 80, 100],\n#              'clf__base_estimator__min_samples_leaf':[40, 60, 80, 100]\n#             }\n\n#clf = AdaBoostClassifier(base_estimator=ExtraTreeClassifier())\n\n#pipeline = build_pipeline(clf=clf)\n#grid_cv_ada = GridSearchCV(pipeline, param_grid=param_dict, cv=3, scoring='roc_auc', verbose=2)\n\n#initial_time = time()\n#grid_cv_ada.fit(X_train, y_train)\n#train_time = time() - initial_time\n#print(\"total time taken for fitting the model : {}\".format(train_time))\n\n#pred_ada = grid_cv_ada.predict_proba(X_test)\n#roc_score = roc_auc_score(y_test,pred_ada[:, 1:])","f6ce96c3":"# fpr and tpr for our optimized model\nfpr, tpr, thresholds = roc_curve(y_test,pred[:, 1:])","a812c684":"# plot the ROC curve\nplt.figure(figsize=(12,6))\nident = [0.0, 1.0]\nplt.plot(fpr, tpr)\nplt.plot(ident, ident, linestyle='--')\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.legend(['xgbbost', 'noskill model'])\nplt.title(\"ROC Curve\")","932a9dc4":"# permutation importance for feature evaluation on the training set\nfeat_imp = permutation_importance(grid_cv, X_train, y_train, scoring='roc_auc', n_repeats=10)","c704d405":"pd.DataFrame(feat_imp['importances_mean'], columns=['feature_imp'], index=X_train.columns).sort_values(by='feature_imp').plot(kind='barh', figsize=(14,8))\nplt.legend(loc='lower right')\nplt.title('Feature importance for XGboost (Training set)')","d83cea1d":"test_df = pd.read_csv(\"..\/input\/health-insurance-cross-sell-prediction\/test.csv\")\ntest_df.head()","711d0f0f":"print(\"the shape of the test dataset is : {}\".format(test_df.shape))","b47cd4dd":"# encoding binary categorical features\ntest_df.replace({'Male':0, 'Female':1, 'No':0, 'Yes':1}, inplace=True)\n\n# create dummy variables for the categorical feature with more than two classes\nfinal_test_df = pd.get_dummies(test_df, drop_first=True)\nfinal_test_df.head()","3be66d7c":"# load the trained model\nclf_model = pickle.load(open(\"model_xgb.pickle\", \"rb\")) ","05665c4e":"# predict the probability of the minority class (positive response) on the test set\ntest_pred = clf_model.predict_proba(final_test_df.iloc[:, 1:])[:, 1:]\ntest_pred","abce0f6b":"submission_df = pd.read_csv(\"..\/input\/health-insurance-cross-sell-prediction\/sample_submission.csv\")\nsubmission_df.head()","b78e25b9":"submission_df['Response'] = test_pred\nsubmission_df.head(10)","f1d1d39c":"submission_df.to_csv('submission.csv')","9b1701cf":"**Observations:**\n- male customers are greater in number as compared to female customers.\n- response:no response ratio for the vehicle insurance is also greater for male customers.\n- interestingly customers (both male and female) with comparatively high values of average age are more likely to \n  respond positively for the vehicle insurance.","4b4c6fa3":"**Observation:**\n- A large chunk of customers have age in the range 20 to 40 years.","7b690276":"### 4. What is the age distribution of the customers?","6d94ba61":"Below we try two more classification algorithms (GradientBoosting and AdaBoosting) and optimize them in order to see whether they can perform better than our present model.","2b357653":"## Use the optimized model to make predictions on the test set ","68d9afcc":"How many categories are present in each of these above features?","0b3721e6":"### 2. Check for duplicate values","0caad98d":"## Exploratory data analysis ","1d047759":"Therefore, further tuning improves the performance of the model by a very tiny amount.","54de3565":"The most important feature in case of xgboost turns out to be 'Previuosly_Insured' whereas 'Driving_License' has practically no affect on the classification result.","6e369ea7":"### 7. How do different marketting schemes affect customer's responses?","eb5a7f2e":"### 5. Do customers with comparatively new vehicles have higher chance to insure it through the company? or they have already done it?","7b26a040":"### 2. Do customers with driving license have higher chance to go for vehicle insurance? What about customers who already have vehicle insurance? ","6aebbbb5":"**Observations:**\n- all of the customers of the company have driving licenses although most of them have not shown interest in the  vehicle insurance scheme of the company.\n- interestingly, the number of customers with vehicle insurance is lesser than those with no vehicle insurance (where both of them have driving licenses).\n- the number of customers already having vehicle insurance is slightly lesser than those with no vehicle insurance.\n- most of the customers with vehicle insurances want to stick to their old scheme. Also, even people with no previous vehicle insurance have shown less interest to invest in this company.","3c17f225":"**Observations:**\n- number of customers with previous vehicle accident history is almost comparable to those with no such history.\n- almost none of the customers with no previous vehicle accident history has responded to the vehicle insurance scheme of the company (middle plot). This is because most of them already have their vehicles insured (right-most plot). ","7b5c3ab4":"How many 'object' type features do we have in the dataset?","75ae1337":"ROC AUC score on the validation set (best score found through grid search) for GradientBoostingClassifier is 0.853778 and for AdaBoostClassifier is 0.8504725. Therefore, in the present analysisthe best performing algorithm on the validation set is extreme gradient boosting *i.e.* XGBoosting and we choose it as our final optimized model.","9d9e0095":"## Load the dataset ","3bf2dd65":"## Import necessary libraries ","e27ddb1a":"## Data processing ","e08bdca3":"### 1. male:female ratio among the customers of the company and how likely they are to respond positively for the vehicle insurance ","ab0c4642":"### 3. Encode categorical features ","37f58354":"**Observations:**\n- most of the schemes for the annual premium of vehicle insurance lies in the range 20k - 40k with an average of 30.5k (upper left plot).\n- interestingly, the average value of the annual premium is comparable for customers who have responded and who haven't (upper right and bottom plots).","fe296227":"**Observations:**\n- most used policy sale channels by the company are - 152, 26, 124 (first two plots).\n- two most promising channels to attract customers are - 26  and 124 (last plot).\n- although channel 152 is one of the most used channels by the company, it has failed to increse the number of customers.","8c275b15":"# Customer prediction for vehicle insurance  \n\nIn this notebook, we analyze dataset of a health insurance company. The company is planning to launch vehicle insurance trecently and wants to understand the main customer base who are likely to be interested in the vehicle insurance.\n\nLet's start by importing necessary libraries.","dacd519e":"### 3. How many customers have vehicle accident history in the past? Does it have any effect on their response? ","9ee5d1d6":"### 6. Split the dataset ","ed24b18d":"### 1. Check for null values ","13c84eef":"###  5. Create the feature matrix and target vector","6188ea03":"### 6. What role does the total amount of premium play?","cad39f08":"**Observations:**\n- most of the customers have comparatively new vehicles (left-most plot) and are less likely to insure the vehicles through this company (middle plot).\n- this is because a large number among them have already insured their vehicles (especiaally those with most recent vehicles).\n- interestingly, people with quite old vehicles have not made any insurance.","939f3af0":"So, our target variable is highly imbalanced.","bb4598c3":"### 4. Check for correlated fetaures ","cfa28568":"It is clear from the above heatmap that highest correlation (negative correlation) exists between the feature pair (Vehicle_damage, Previously_Insured). Most of the feature pairs donot have strong correlation. Hence, we decide not to drop any fetaure based on correlation.","cf4e7ff8":"## Model building","c6573329":"After performing the above grid search for all the classifiers considered, XGBClassifier turns out to be the best performer for the following configuration:\n\nthe best model is XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,\n              colsample_bynode=None, colsample_bytree=None, gamma=None,\n              gpu_id=None, importance_type='gain', interaction_constraints=None,\n              learning_rate=None, max_delta_step=None, max_depth=None,\n              min_child_weight=None, missing=nan, monotone_constraints=None,\n              n_estimators=100, n_jobs=None, num_parallel_tree=None,\n              random_state=None, reg_alpha=None, reg_lambda=None,\n              scale_pos_weight=None, subsample=None, tree_method=None,\n              validate_parameters=None, verbosity=None) for the parameter set {'clf__learning_rate': 0.1, 'clf__n_estimators': 150} that produces an roc_auc_score 0.8530182934346179 when evaluated on the train set and roc_auc_score 0.8530633898633144 when evaluated on the test set.","d5b3d0df":"So, there are total 269 duplicate rows. Let's drop them."}}