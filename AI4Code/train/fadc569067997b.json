{"cell_type":{"935ea97c":"code","6f2430ec":"code","f2910e03":"code","af4b94b3":"code","cee505aa":"code","a6ab0c0a":"code","049d4cce":"code","434a5dc7":"code","3795c8d1":"code","2c013b26":"code","10766c0d":"code","7aa5bacd":"code","1d8a3995":"code","209c895a":"code","51a616b1":"code","899c7538":"code","a7ffb00a":"code","a450ceab":"code","bd2f92d0":"code","e8af4aaa":"code","9810f4bc":"code","e4d64d39":"code","19adeb95":"code","09506d2a":"code","e701909d":"code","66c8619c":"code","4d25d9a0":"code","9a14eac5":"code","88340e1d":"code","b7233c1a":"code","fb45d6ba":"code","d5f80d5f":"code","80dbd642":"markdown","7e0ed6c2":"markdown","873d5245":"markdown","2fcec23a":"markdown","9cbb3c9d":"markdown","3e5d3a5f":"markdown","6ec79e32":"markdown","f28a8e81":"markdown","3093a1db":"markdown","27063add":"markdown","77919672":"markdown","a3b835a6":"markdown","b5aa66f6":"markdown","a337bcc3":"markdown","a9fdefda":"markdown","1961bed8":"markdown","70f8644e":"markdown","e1fc0961":"markdown","371ab512":"markdown","ad38a2cd":"markdown","3ee24728":"markdown","2c4b7e88":"markdown"},"source":{"935ea97c":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n\n\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","6f2430ec":"train_X = pd.read_csv('..\/input\/career-con-2019\/X_train.csv')\ntest_X  = pd.read_csv('..\/input\/cconlink\/X_test_trial.csv' )\ntot = pd.concat([train_X, test_X])\ntrain_X = train_X.iloc[:,3:].values.reshape(-1,128,10)\ntest_X = test_X.iloc[:,3:].values.reshape(-1,128,10)\nprint('train_X shape:', train_X.shape, ', test_X shape:', test_X.shape)","f2910e03":"df_train_y = pd.read_csv('..\/input\/career-con-2019\/y_train.csv')\n\n# build a dict to convert surface names into numbers\nsurface_names = df_train_y['surface'].unique()\nnum_surfaces = len(surface_names)\nsurface_to_numeric = dict(zip(surface_names, range(num_surfaces)))\nprint('Convert to numbers: ', surface_to_numeric)\n\n# y and group data as numeric values:\ntrain_y = df_train_y['surface'].replace(surface_to_numeric).values\ntrain_group = df_train_y['group_id'].values","af4b94b3":"fig, axes = plt.subplots(1,4)\nfig.set_size_inches(20,3)\n\nfor i in range(4):\n    axes[i].plot(train_X[train_group == 17][:,:,i].reshape(-1))\n    axes[i].grid(True)","cee505aa":"def sq_dist(a,b):\n    ''' the squared euclidean distance between two samples '''\n    \n    return np.sum((a-b)**2, axis=1)\n\n\ndef find_run_edges(data, edge):\n    ''' examine links between samples. left\/right run edges are those samples which do not have a link on that side. '''\n\n    if edge == 'left':\n        border1 = 0\n        border2 = -1\n    elif edge == 'right':\n        border1 = -1\n        border2 = 0\n    else:\n        return False\n    \n    edge_list = []\n    linked_list = []\n    \n    for i in range(len(data)):\n        dist_list = sq_dist(data[i, border1, :4], data[:, border2, :4]) # distances to rest of samples\n        min_dist = np.min(dist_list)\n        closest_i   = np.argmin(dist_list) # this is i's closest neighbor\n        if closest_i == i: # this might happen and it's definitely wrong\n            print('Sample', i, 'linked with itself. Next closest sample used instead.')\n            closest_i = np.argsort(dist_list)[1]\n        dist_list = sq_dist(data[closest_i, border2, :4], data[:, border1, :4]) # now find closest_i's closest neighbor\n        rev_dist = np.min(dist_list)\n        closest_rev = np.argmin(dist_list) # here it is\n        if closest_rev == closest_i: # again a check\n            print('Sample', i, '(back-)linked with itself. Next closest sample used instead.')\n            closest_rev = np.argsort(dist_list)[1]\n        if (i != closest_rev): # we found an edge\n            edge_list.append(i)\n        else:\n            linked_list.append([i, closest_i, min_dist])\n            \n    return edge_list, linked_list\n\n\ndef find_runs(data, left_edges, right_edges):\n    ''' go through the list of samples & link the closest neighbors into a single run '''\n    \n    data_runs = []\n\n    for start_point in left_edges:\n        i = start_point\n        run_list = [i]\n        while i not in right_edges:\n            tmp = np.argmin(sq_dist(data[i, -1, :4], data[:, 0, :4]))\n            if tmp == i: # self-linked sample\n                tmp = np.argsort(sq_dist(data[i, -1, :4], data[:, 0, :4]))[1]\n            i = tmp\n            run_list.append(i)\n        data_runs.append(np.array(run_list))\n    \n    return data_runs","a6ab0c0a":"train_left_edges, train_left_linked  = find_run_edges(train_X, edge='left')\ntrain_right_edges, train_right_linked = find_run_edges(train_X, edge='right')\nprint('Found', len(train_left_edges), 'left edges and', len(train_right_edges), 'right edges.')","049d4cce":"train_runs = find_runs(train_X, train_left_edges, train_right_edges)","434a5dc7":"flat_list = [series_id for run in train_runs for series_id in run]\nprint(len(flat_list), len(np.unique(flat_list)))","3795c8d1":"print([ len(np.unique(train_y[run])) for run in train_runs ])","2c013b26":"print([ (np.unique(train_y[run])) for run in train_runs ])","10766c0d":"print([ len(np.unique(train_group[run])) for run in train_runs ])","7aa5bacd":"print([ (np.unique(train_group[run])) for run in train_runs ])","1d8a3995":"fig, axes = plt.subplots(10,1, sharex=True)\nfig.set_size_inches(20,15)\nfig.subplots_adjust(hspace=0)\n\nfor i in range(10):\n    axes[i].plot(train_X[train_runs[1]][:,:,i].reshape(-1))\n    axes[i].grid(True)","209c895a":"train_runs[1]","51a616b1":"df_train_y['run_id'] = 0\ndf_train_y['run_pos'] = 0\n\nfor run_id in range(len(train_runs)):\n    for run_pos in range(len(train_runs[run_id])):\n        series_id = train_runs[run_id][run_pos]\n        df_train_y.at[ series_id, 'run_id'  ] = run_id\n        df_train_y.at[ series_id, 'run_pos' ] = run_pos\n\ndf_train_y.to_csv('y_train_with_runs.csv', index=False)\ndf_train_y.tail()","899c7538":"df_train_y.head()","a7ffb00a":"tot[487675:487700]","a450ceab":"tot = tot.iloc[:,3:].values.reshape(-1,128,10)","bd2f92d0":"train_left_edges, train_left_linked  = find_run_edges(tot, edge='left')\ntrain_right_edges, train_right_linked = find_run_edges(tot, edge='right')\nprint('Found', len(train_left_edges), 'left edges and', len(train_right_edges), 'right edges.')","e8af4aaa":"train_runs = find_runs(tot, train_left_edges, train_right_edges)","9810f4bc":"len(train_runs)","e4d64d39":"flat_list = [series_id for run in train_runs for series_id in run]\nprint(len(flat_list), len(np.unique(flat_list)))","19adeb95":"x = train_runs[1]\ny=np.sort(x)\ny\n#print([int(s) for s in x.split() if s.isdigit()])\n#y = x.sort()\n#print(y)","09506d2a":"ss = pd.read_csv('..\/input\/career-con-2019\/sample_submission.csv')","e701909d":"ss['surface'] = ''","66c8619c":"ss[:5]","4d25d9a0":"l=[]\nsurf = ''\nfor i in range(151):\n    x = train_runs[i]\n    x = np.sort(x)\n    if x[0]<3810:\n        l.append((i,df_train_y['surface'][x[0]]))\n        surf = df_train_y['surface'][x[0]]\n        for j in range(len(train_runs[i])):\n            if train_runs[i][j]-3810>-1:\n                ss['surface'][train_runs[i][j]-3810] = surf","9a14eac5":"l[:5]","88340e1d":"ss[:5]","b7233c1a":"ss.to_csv('x.csv',index = False)","fb45d6ba":"sub96 =  pd.read_csv('..\/input\/096acc\/woodconc.csv')","d5f80d5f":"sub96.to_csv('sub96.csv',index = False)","80dbd642":"After adding RF models' predicted surface, we have sub96 file :)","7e0ed6c2":"Filling in relevant correlated surface information.","873d5245":"Markus published a kernel named Missing links. This kernel is highly inspired from that. If you like this, please go Markus kernel and upvote that as well :)","2fcec23a":"Have we found all samples? Have we used any sample twice? The answer is yes, and no. Perfect.","9cbb3c9d":"Base linked file is ready !!","3e5d3a5f":"# Best Public LB Kernel","6ec79e32":"You can fill in your predicted model values in empty slots of ss Dataframe or can use it from x.csv file in Output. ","f28a8e81":"Interesting. Some runs contain  2, 3 and even 4 groups. So several groups were cut from one run:","3093a1db":"Let's plot the 4 orientation channels of a random group in series:","27063add":"Let's plot all 10 channels for one run.  Beautiful.","77919672":"# Change begins here","a3b835a6":"Let's go:","b5aa66f6":"Well, that certainly looks like a jigsaw puzzle to me. And that leads to an idea: the euclidean distance in the 4-dimensional \"orientation space\" between, for example, the right edge of one sample and the left edge of its true neighbor should be a minimum, *ideally even among all samples*, not only the samples in its group. Same for left\/right. This should enable us to stitch the runs together again. All we have to do is link samples together which are *each other's* closest neighbors. Let's code:","a337bcc3":"### You can all have fun filling empty 'x.csv' rows with your ML model predicted values and can check the insane accuracy boost it provides.","a9fdefda":"Clearing out all the values.","1961bed8":"Now for the real test. How many different surfaces are in each run? *Only 4 runs have more than one surface* (and if you look at them, you can easily split them by hand). This actually works!","70f8644e":"Well, that certainly looks promising. Found 76 runs, similar number than the number of groups. Build the runs:","e1fc0961":"### An upvote is always appreciated it helps a lot!! Thank you :) Enjoy :)","371ab512":"### You can all have fun filling empty 'x.csv' rows with your ML model predicted values and can check the insane accuracy boost it provides.","ad38a2cd":"This standalone kernel without any ML model predicts approx 2650 rows on private dataset and still has an accuracy of 0.74 on PublicLB. When empty slots are filled with some predicted values, accuracy rises to **0.96 on PublicLB. **","3ee24728":"I am not downplaying anyone's effort but by tweaking some code, this kernel easily ranks in **top 20 on PublicLB & top 50 on PrivateLB**","2c4b7e88":"Let's add our new knowledge to train_y. Now you can use this info to train your models to even greater perfection. Enjoy!"}}