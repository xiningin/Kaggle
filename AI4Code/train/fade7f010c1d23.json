{"cell_type":{"64430bdd":"code","70214808":"code","6cb1c0ca":"code","fdb92acb":"code","6aeb5c3a":"code","ab8f282f":"code","dda6c70d":"code","b000888d":"code","5a37be4c":"code","3d5c2de3":"code","4e43170a":"code","dc855314":"code","e5054c4c":"code","c3d925be":"markdown","e5e54fee":"markdown","f6b452db":"markdown","e681c081":"markdown","2f7490d5":"markdown","88cc7d4f":"markdown","9608aaa9":"markdown","3fe962b4":"markdown","512773bd":"markdown","d87251d2":"markdown","eb17f547":"markdown","b5d022b0":"markdown","217b9ba7":"markdown","4c5440a4":"markdown","22a292f5":"markdown","7c475861":"markdown","1fca821f":"markdown","b400db98":"markdown","51e9c2d5":"markdown","112991dc":"markdown","fb5c1479":"markdown","63276471":"markdown","c780f71d":"markdown","a35d1e5e":"markdown","3bcf3c13":"markdown","d7da539b":"markdown","cc6edc1f":"markdown","8c011f11":"markdown","3f68f4b7":"markdown","7ad28f99":"markdown","3ebab2e8":"markdown","ad8d94e5":"markdown","17ca6a00":"markdown","23de6b40":"markdown","35c6faac":"markdown","6bcabf77":"markdown","b8d44c73":"markdown"},"source":{"64430bdd":"import pandas as pd \ndf = pd.read_csv(\"..\/input\/lecture7risk2\/CreditRisk2.csv\")  # read data\ndf","70214808":"df = pd.read_csv(\"..\/input\/lecture7risk2\/CreditRisk2.csv\")  # read data\ndf.iloc[[7,11]] #use list to select multiple rows","6cb1c0ca":"from sklearn.tree import DecisionTreeClassifier # Import Decision Tree Classifier","fdb92acb":"from sklearn.preprocessing import OrdinalEncoder\nencoder = OrdinalEncoder(categories = [['Low', 'High']]) # create an encoder with order\ndf['Savings'] = encoder.fit_transform(df['Savings'].values.reshape(-1, 1)) # fit encoder  and transfer data with 'Savings'\ndf","6aeb5c3a":"encoder = OrdinalEncoder(categories = [['Low', 'High']]) # create an encoder with order\ndf['Income'] = encoder.fit_transform(df['Income'].values.reshape(-1, 1)) # fit encoder  and transfer data with 'Savings'\ndf","ab8f282f":"encoder = OrdinalEncoder(categories = [['No', 'Yes']]) # create an encoder with order\ndf['Marriage'] = encoder.fit_transform(df['Marriage'].values.reshape(-1, 1)) # fit encoder  and transfer data with 'Savings'\ndf","dda6c70d":"encoder = OrdinalEncoder(categories = [['Bad', 'Good']]) # create an encoder with order\ndf['Credit Risk'] = encoder.fit_transform(df['Credit Risk'].values.reshape(-1, 1)) # fit encoder  and transfer data with 'Savings'\ndf","b000888d":"#split features and target variable\nX = df.iloc[:,0:3]\ny = df.iloc[:,3]","5a37be4c":"# Split dataset into training set and test set\nX_train = X[0:10] # stop is excluded in slicing\nX_test = X[10:]\ny_train = y[0:10]\ny_test = y[10:]","3d5c2de3":"import matplotlib.pyplot as plt \nfrom sklearn import tree \nclf = DecisionTreeClassifier(criterion='gini',max_depth=1)\nclf = clf.fit(X_train,y_train)\nfig = plt.figure(figsize=(4,5))\n_ = tree.plot_tree(clf, feature_names=df.columns,class_names=['Bad Loss','Good Risk'],filled=True)","4e43170a":"# Create Decision Tree classifer object\nclf = DecisionTreeClassifier(criterion='gini',max_depth=2)\nclf = clf.fit(X_train,y_train)\nfig = plt.figure(figsize=(5,6))\n_ = tree.plot_tree(clf, feature_names=df.columns,class_names=['Bad','Good'],filled=True)","dc855314":"# Create Decision Tree classifer object\nclf = DecisionTreeClassifier(criterion='gini',max_depth=3)\nclf = clf.fit(X_train,y_train)\nfig = plt.figure(figsize=(5,7))\n_ = tree.plot_tree(clf, feature_names=df.columns,class_names=['Bad','Good'],filled=True)","e5054c4c":"from sklearn import metrics #Import scikit-learn metrics module for accuracy calculation\n\ny_pred = clf.predict(X_test) \nprint(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))","c3d925be":"### Step 3: Repeat to decide child node (1)\n* Calculate Gini index of \u201cSaving\u201d. Crosstab between \"Saving\" and \"Risk\"\n \n|               |             |     Credit Risk    |            |     Number of instances    |\n|---------------|-------------|--------------------|------------|----------------------------|\n|               |             |     Good           |     Bad    |     6                      |\n|     Saving    |     Low     |     0              |     3      |     3                      |\n|               |     High    |     2              |     1      |     3                      |\n\n\n$$Gini (Saving = Low) =1-(\\frac{0}{3})^2-(\\frac{3}{3})^2=0$$\n$$Gini (Saving = High) =1-(\\frac{2}{3})^2-(\\frac{1}{3})^2= 0.444$$\n$$Gini (Saving) = \\frac{3}{6} \\times 0+ \\frac{3}{6} \\times 0.444=0.222  $$\n","e5e54fee":"### Step 3: Repeat to decide child node (3)\n$$ Gini (Saving) = 0.222 $$\n$$ Gini (Marriage)= 0.333 $$ \nSince Gini (Saving)  =0.222 is smallest, choose Saving as child node  \n","f6b452db":"# Decision Trees\nCOMP20121 Machine Learning for Data Analytics\n\nAuthor: [Jun He](https:\/\/www.ntu.ac.uk\/staff-profiles\/science-technology\/jun-he) ","e681c081":"### Decision tree design \n1. Splitting criteria\n    * criterion='gini' or 'entropy'\n2. Stopping criteria\n    * When to stop building the tree such as `max_depth=3`","2f7490d5":"### Data preparation\n* Decision tree splitting is based on catogerical values of features \n    * such as whether `Savings` is `Low, Medium, High`\n    * Numerical features are split into bins  `Savings` `low: <20K, Medium: [20K,40K),High: >=40K`\n* But Sklearn does NOT support categorical variables\n* Categorical features must be converted into numerical ones with Sklearn\n* Ordinal features are encoded by `OrdinalEncoder`\n    * `Savings, Marriage, Income, Credit Risk` have some order\n    * Encode them one by one by `OrdinalEncoder`\n","88cc7d4f":"### Step 1: Determine Best Root Node (3)\n* Calculate Gini index of \u201cIncome\u201d\n\n|               |             |     Credit Risk    |            |     Number of instances    |\n|---------------|-------------|--------------------|------------|----------------------------|\n|               |             |     Good           |     Bad    |     10                     |\n|     Income    |     High    |     4              |     0      |     4                      |\n|               |     Low     |     2              |     4      |     6                      |\n\n$$Gini(Income=High)=1-(\\frac{4}{4})^2-(\\frac{0}{4})^2=0.0$$\n$$Gini(Income=Low)=1-(\\frac{2}{6})^2-(\\frac{2}{6})^2= 0.44$$\n$$Gini(Income)=\\frac{4}{10} \\times 0+\\frac{6}{10} \\times 0.44= 0.27\t$$\n","9608aaa9":"### Build a decision tree model\n* Create `DecisionTreeClassifer` object\n* Train `DecisionTreeClassifer`\n  ","3fe962b4":"### Decision trees \nThere are different algorithms to grow a tree using different splitting criteria\n1. Gini index determines the purity of a specific class as a result of a decision to branch along a particular attribute\/value\n    * Used in CART (classification and regression trees)\n2. Information gain uses entropy to measure the extent of uncertainty or randomness of a particular attribute\/value split\n    * Used in ID3, C4.5, C5\n3. Chi-square statistics\n    * used in CHAID","512773bd":"### Pure leaf node  and diverse leaf node \n* A **pure leaf node** has records with same target class value\n    * either 100% \u201dGood Risk\u201d for records in the leaf node\n    * Or 100% \u201cBad Risk\u201d for records in the leaf node\n    * A pure node is in the form  like `[21,0]` representing the number of records `Bad`  =21 and the number of records `Good` =0\n\n* A **diverse leaf node** has records with different target class values \n    * including both \u201cGood Risk\u201d and \u201cBad Risk\u201d records in the leaf node\n    * a divers node  is in the form `[1,2]` representing the number of records `Bad`  =1 and the number of records `Good` =2","d87251d2":"depth = 2: `Saving` is chosen to split the data. Why Saving? This will be explained in Part 3","eb17f547":"### Parameters in decision tree\n`sklearn.tree.DecisionTreeClassifier` has several parameters\n\n* `criterion` :  (default=`\u201dgini`\u201d). measure the quality of a split. \n    1. \u201cgini\u201d index\n    2. \u201centropy\u201d for information gain \n* `splitter` :  (default=`\u201dbest\u201d`). Strategy used to choose the split at each node. \n    1. `\u201cbest\u201d` to choose the best split\n    2. `\u201crandom\u201d` to choose the best random split.\n* `max_depth` : (default=None): the maximum depth of the tree, e.g. =3\n\nNote: for large data, the decision tree could be very large. Need to tune `max_depth`","b5d022b0":"### Step 1: Determine Best Root Node (4)\n* Gini index values for attributes saving, marriage, and income\n    1. Gini(Saving )=   0.4\n    2. Gini(Marriage)= 0.46\t \n    3. Gini(Income)= 0.27 \n\n* Since Gini(Income)=0.27 is smallest, Income is taken as the root node. \n","217b9ba7":"### Step 6: Draw Decision Tree\n* The tree is the same as that with Sklearn\n![image.png](attachment:image.png)","4c5440a4":"\n### Decision Tree Algorithm\n1. Create a root node and assign all of the training data to it. \n2. Select the best splitting attribute.\n3. Add a branch to the root node for each value of the split. Split the data into mutually exclusive subsets along the lines of the specific split.\n4. Repeat the steps 2 and 3 for each and every leaf node until the stopping criteria is reached.\n","22a292f5":"### Example: data set\n\n|     ID    |     Savings    |     Marriage    |     Income    |     Credit   Risk    |\n|-----------|----------------|-----------------|---------------|----------------------|\n|     1     |     Low        |     No          |     Low       |     Bad              |\n|     2     |     High       |     Yes         |     Low       |     Good             |\n|     3     |     Low        |     No          |     High      |     Good             |\n|     4     |     Low        |     Yes         |     Low       |     Bad              |\n|     5     |     Low        |     Yes         |     High      |     Good             |\n|     6     |     High       |     No          |     Low       |     Bad              |\n|     7     |     High       |     No          |     High      |     Good             |\n|     8     |     High       |     Yes         |     Low       |     Good             |\n|     9     |     Low        |     Yes         |     Low       |     Bad              |\n|     10    |     High       |     Yes         |     High      |     Good             |\n","7c475861":"### Step 5: Split Tree\n* Only \u201cMarriage\u201d is the remained attribute, so it will be taken as the child node without calculating Gini index.\n\n|     ID    |     Savings    |     Marriage    |     Income    |     Credit   Risk    |\n|-----------|----------------|-----------------|---------------|----------------------|\n|     2     |     High       |     Yes         |     Low       |     Good             |\n|     8     |     High       |     Yes         |     Low       |     Good             | \n\n* Branch Marriage =Yes $\\to$ Credit Risk = Good\n\n|     ID    |     Savings    |     Marriage    |     Income    |     Credit   Risk    |\n|-----------|----------------|-----------------|---------------|----------------------|\n|     6     |     High       |     No          |     Low       |     Bad              |\n\n* Right branch Marriage = No $\\to $ credit Risk = Bad ","1fca821f":"### Step 1: Determine Best Root Node (1)\n* Start from `Savings` and crosstab betweeen `Savings` and `Risk`\n* Calculate Gini index of \u201cSaving\u201d\n\n|               |             |     Credit Risk    |            |     Number of instances    |\n|---------------|-------------|--------------------|------------|----------------------------|\n|               |             |     Good           |     Bad    |     10                     |\n|     Saving    |     Low     |     2              |     3      |     5                      |\n|               |     High    |     4              |     1      |     5                      |\n\n$$Gini(Saving=low)=1\u2212(\\frac{2}{5})^2\u2212(\\frac{3}{5})^2 =0.48$$\n$$Gini(Saving=high)=1\u2212(\\frac{4}{5})^2\u2212(\\frac{1}{5})^2=0.32$$\n$$Gini(Saving)=(\\frac{5}{10}) \\times 0.48+\\frac{5}{10} \\times 0.32=0.40$$\n","b400db98":"## Part 2 Case Study: Decision tree for credit risk classificatin with Python\n### The decision tree created by Sklearn \n![image.png](attachment:image.png)\n* Highest-level decision node is root node and tests whether record has `income = Low 0.5?`  \n    * `<=0.5` because `low = 0` and `high=1` \n    * `samples =10`: the number of records\n    * `value=[4,6]`: 4 `bad` records and 6 `good` records\n    *  `Class = good`:  label from the label of majority of records  \n* Records are split according to value of `income`. Records with `True` go to left branch, and with `False` go to right branch \n* Right branch reaches leaf node `Good` risk with Bad = 0 and  Good = 4\n* Right branch reaches another decision node `Savings =Low 0.5?`\n* Repeat the splitting. The tree grows until all nodes are leaf nodes or a predefined maximal depth is reaced","51e9c2d5":"## Summary\n* Decision tree is a supervised learning method for classification\n* Sklearn provide high-level library for decision tree. Users don\u2019t need to consider low-level implementation of decision tree algorithms\n* CART is a popular decision tree algorithm using gini index for splitting tree. Sklearn implement CART\n* Decision Trees produce interpretable output in IF THEN rules\n\n","112991dc":"## Part 3 Manually build a decision tree step by step\n### Main questions in building a decision tree\n* How to select a feature as the root node?\n* How to select a features as a decision node? \n    * Each branch should contain records with outcome value as pure  (same value) as possible.  \n    * Need a measure of \u201cpure\u201d degree\n* When to stop tree growing?\n    * Set a maximal depth","fb5c1479":"## Learning objectives\n* What is a decision tree?\n* How to implement a decision tree for classification with Python? \n\n|<img src =\"https:\/\/upload.wikimedia.org\/wikipedia\/commons\/e\/eb\/Ash_Tree_-_geograph.org.uk_-_590710.jpg\"   width =200>|\n|:--:|\n|[Tree with root, branches and leaves](https:\/\/upload.wikimedia.org\/wikipedia\/commons\/e\/eb\/Ash_Tree_-_geograph.org.uk_-_590710.jpg)||","63276471":"### Evaluate model\n* Compare predicted label `y_pred` and actutal label `y_test` on test data\n* Only two test data. Pediction is wrong on Customer 11 \n* 50% accuracy","c780f71d":"### Decision Trees Requirements\n* Decision Tree is **supervised learning** and training data must be labelled  \n* Target variable must be **categorical**, such as `Good, Bad`, not **numeric**\n* Decision trees learn by instances, so training set should cover rich and varied instances\n* If training set lacks data on some subset (for example, no data on `low income`), then classification on that subset (`low income`) will become problematic  \n","a35d1e5e":"depth = 3: all leaf nodes are pure","3bcf3c13":"## Part 1 What is a decision tree?\n### Example: Credit Risk\n* Our task is to classify customers into either `Good` or `Bad` risk \n* `Credit Risk` is the target variable\n* Predictor variables are `Savings`, `Marriage` and `Income`","d7da539b":"### Classification and regression trees (CART)\n\n* **Classification and regression trees** (CART or CRT) is implemented in Sklearn\n* CRT adopts binary node splitting (two branches at each node)\n* CRT uses Gini index for splitting\n* **Gini index** is a metric, which is a concept from economics\n\n|<img width  =800 src = \"https:\/\/i.redd.it\/3yzywl2pytc21.png\">|\n|:--:|\n|[Gini Index World Map](https:\/\/i.redd.it\/3yzywl2pytc21.png)|\n\n* Measure the inequality of the income or wealth distribution in a nation's residents: the lower (greener), the more equal\n* Measure the purity of a specific class: the lower, the purer, the better for splitting","cc6edc1f":"### Step 2: Split Tree\n\n|     ID    |     Savings    |     Marriage    |     Income    |     Credit   Risk    |\n|-----------|----------------|-----------------|---------------|----------------------|\n|     3     |     Low        |     No          |     High      |     Good             |\n|     5     |     Low        |     Yes         |     High      |     Good             |\n|     7     |     High       |     No          |     High      |     Good             |\n|     10    |     High       |     Yes         |     High      |     Good             ||\n\n1. Branch for Income =High. \n    * No need for further splitting. Leaf node (Decision) is Good (pure)\n\n|     ID    |     Savings    |     Marriage    |     Income    |     Credit   Risk    |\n|-----------|----------------|-----------------|---------------|----------------------|\n|     1     |     Low        |     No          |     Low       |     Bad              |\n|     2     |     High       |     Yes         |     Low       |     Good             |\n|     4     |     Low        |     Yes         |     Low       |     Bad              |\n|     6     |     High       |     No          |     Low       |     Bad              |\n|     8     |     High       |     Yes         |     Low       |     Good             |\n|     9     |     Low        |     Yes         |     Low       |     Bad              |\n  \n2. Branch for Income = Low\n    * Need for further splitting","8c011f11":"Split dataset in features (predictors) `X` and target variable `y` ","3f68f4b7":"### Import library\n* `Sklearn.Tree.DecisionTreeClassifier`: decision tree module for classification","7ad28f99":"Split dataset into training set and test set\n* first 10 records for training \n* last 2 records for test","3ebab2e8":"We show the procedure of growing the tree step by step: `max_depth=1`\n\n* Income is chosen as root node. Why Income? This will be explained in Part 3\n* Left leaf node is not pure `value =[4,2]`. Need to be split","ad8d94e5":"### Step 1: Determine Best Root Node (2)\n* Calculate Gini index of \u201cMarriage\u201d\n\n|                 |            |     Credit Risk    |            |     Number of instances    |\n|-----------------|------------|--------------------|------------|----------------------------|\n|                 |            |     Good           |     Bad    |     10                     |\n|     Marriage    |     Yes    |     4              |     2      |     6                      |\n|                 |     No     |     2              |     2      |     4                      |\n\n\n$$Gini(Marriage=Yes )=1-(\\frac{4}{6})^2-(\\frac{2}{6})^2=0.44$$\n$$Gini(Marriage=No )=1-(\\frac{2}{4})^2-(\\frac{2}{4})^2= 0.5$$\n$$Gini(Marriage)=\\frac{6}{10} \\times 0.44+\\frac{4}{10} \\times 0.5= 0.46$$\n","17ca6a00":"### Gini Index\n\nAssume that a data set $S$ contains records from $n$ classes, the **Gini index** is defined as\n$$\nGini(S) =1-  p^2_1 -\\cdots - p^2_n\n$$\nwhere $p_j$ is a relative frequency of class $j$ in $S$\n\n\nIf a data set $S$ is split into two subsets, $S_1$ and $S_2$, with sizes $N_1$ and $N_2$, respectively, the **Gini index** of the split set $S$ is defined as the weighted sum of Gini index oof the two subsets\n$$\nGini_{split} =\\frac{N_1}{N} Gini(S_1) +\\frac{N_2}{N} Gini(S_2)\n$$\nwhere $N=N_1+N_2$ is the total numbers of records","23de6b40":"### Step 3: Repeat to decide child node (2)\n* Calculate Gini index of \u201cMarriage\u201d\n\n|                 |            |     Credit Risk    |            |     Number of instances    |\n|-----------------|------------|--------------------|------------|----------------------------|\n|                 |            |     Good           |     Bad    |     6                      |\n|     Marriage    |     Yes    |     2              |     2      |     4                      |\n|                 |     No     |     0              |     2      |     2                      ||\n\n\n$$ Gini (Marriage=Yes )= 1-(\\frac{2}{4})^2-(\\frac{2}{4})^2=0.5$$\n$$Gini (Marriage=No)= 1-(\\frac{0}{2})^2-(\\frac{2}{2})^2= 0$$\n$$Gini (Marriage)= \\frac{4}{6} \\times 0.5+ \\frac{2}{6} \\times 0=0.333   $$","35c6faac":"### Decision Trees\n* Decision tree is a popular classification model in machine learning \n* **Decision Tree** is collection of **decision nodes**, connected by **branches**, extending downward from **root node** to terminating **leaf nodes**\n* Beginning with root node, attributes tested at decision nodes, and each possible outcome results in branch\n* Each branch leads to decision node or leaf node \n![image.png](attachment:image.png)\n\n* The tree can be interpreted as `IF THEN` rules, for example\n    * IF income = low is false, THEN credit risk is good","6bcabf77":"### Leaf node may not always be pure\n* Customers 7 and 11 have the same predictor values, but they have different risk outcomes   \n    * \u201cbad\u201d with 1\/2 confidence; \u201cGood\u201d with 1\/2 confidence\n* In this case, a leaf node cannot be pure","b8d44c73":"### Step 4. Repeat to Split Tree\n|     ID    |     Savings    |     Marriage    |     Income    |     Credit   Risk    |\n|-----------|----------------|-----------------|---------------|----------------------|\n|     1     |     Low        |     No          |     Low       |     Bad              |\n|     4     |     Low        |     Yes         |     Low       |     Bad              |\n|     9     |     Low        |     Yes         |     Low       |     Bad              |\n\n1. Branch for Income = Low and Saving= Low\n    * Reach a pure leaf node: Decision = Bad for credit risk\n\n|     ID    |     Savings    |     Marriage    |     Income    |     Credit   Risk    |\n|-----------|----------------|-----------------|---------------|----------------------|\n|     2     |     High       |     Yes         |     Low       |     Good             |\n|     6     |     High       |     No          |     Low       |     Bad              |\n|     8     |     High       |     Yes         |     Low       |     Good             |\n\n2. Branch for Income = Low and Saving= High\n    * Need further splitting\n"}}