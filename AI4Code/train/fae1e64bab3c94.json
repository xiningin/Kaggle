{"cell_type":{"e196d5e6":"code","6313539d":"code","3cc37a60":"code","89d8463e":"code","11ebc95c":"code","31048378":"code","217c7b69":"code","040d8ea1":"code","a7a1f4f7":"code","af771f93":"code","d10fb7e1":"code","8fc138bf":"code","2c0687c7":"code","7c3fb881":"code","9fdf1474":"code","bf711c2e":"code","d596993e":"code","c53a9e00":"code","e0db9d32":"code","39e99b1e":"code","c592a367":"code","a0a2a8ed":"code","01cc444a":"code","883a8c73":"code","ff301a48":"code","7212a891":"code","41879344":"code","1b31dfcd":"code","4e232d8a":"code","3202f0c9":"code","6e443efb":"code","de6017cb":"code","f839d115":"code","a7b9a470":"code","9783e900":"code","76dac97c":"code","93844ac7":"code","43e08a77":"code","1f598f33":"code","5af94b88":"code","7df2a9c6":"code","1bbacdac":"code","baed5344":"code","74278736":"code","cdc16d24":"code","7e2ac3e2":"code","51f22f08":"code","32f2b838":"code","55d11865":"markdown","8883afc9":"markdown","34f02d96":"markdown","1f9b3c40":"markdown","6401ad9d":"markdown","6f3809b9":"markdown","d173e539":"markdown","5be8ece5":"markdown","27daa388":"markdown","4bfa2b16":"markdown","ab43ef9b":"markdown","7fb5d545":"markdown","b5384959":"markdown","34fa6e48":"markdown","9a3453ab":"markdown","ed89498c":"markdown","5fb2fefb":"markdown","ecaf35fc":"markdown","e9a7b4b3":"markdown","21fa9e07":"markdown","bf8a1859":"markdown","3c938a08":"markdown","0d423606":"markdown","f8b2aad3":"markdown"},"source":{"e196d5e6":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.model_selection import train_test_split, cross_val_score ,StratifiedKFold, GridSearchCV\nfrom sklearn.preprocessing import MinMaxScaler,StandardScaler #To scale the Dataset\nfrom sklearn.pipeline import Pipeline #to assemble several steps that can be cross-validated together while setting different parameters.\nfrom sklearn.metrics import confusion_matrix,roc_curve,accuracy_score, classification_report, roc_auc_score  #to evaluate best model\nfrom sklearn.decomposition import TruncatedSVD,PCA\n#Algorithms \nfrom sklearn.neighbors import KNeighborsClassifier #to get the KNN classifier \nfrom sklearn.naive_bayes import GaussianNB #to get the Gaussian Naive Bayes Classifier \nfrom sklearn.linear_model import LogisticRegression #to get Logistic Regression\nfrom sklearn.svm import SVC #to get support vector machine","6313539d":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","3cc37a60":"df_raw = pd.read_csv(\"..\/input\/water-potability\/water_potability.csv\")\ndf_raw","89d8463e":"#Generates the descriptive statistics which includes - mean,  standard deviation, etc.\ndf_raw.describe()","11ebc95c":"df_raw.info()","31048378":"#figuring out how many examples are available per class\nsns.countplot(x=df_raw[\"Potability\"])\nprint(f'{df_raw.Potability[df_raw.Potability==1].count()\/df_raw.Potability.count()*100:.2f} % of samples are potable (1)')","217c7b69":"# Correlation matrix for dataset\nplt.figure(figsize=(15,10))\nsns.heatmap(df_raw.corr(), annot=True, cmap=\"inferno\")","040d8ea1":"df_raw.isna().sum()","a7a1f4f7":"def fill_nan(df):\n    for index, column in enumerate(df.columns[:9]):\n        # print(index, column)\n        df[column] = df[column].fillna(df.groupby('Potability')[column].transform('mean'))\n    return df\n        \ndf = fill_nan(df_raw)\ndf.isna().sum()                                                       ","af771f93":"df.describe()","d10fb7e1":"scaler = MinMaxScaler()\narray = scaler.fit_transform(df.to_numpy())\ndf2 = pd.DataFrame(array, columns = ['ph','Hardness','Solids','Chloramines', 'Sulphate', 'Conductivity', 'Organic_carbon', 'Trihalomethanes','Turbidity', 'Potability'])\ndf2.describe()","8fc138bf":"#prepare the data for PCA \nX = df2.drop(['Potability'], axis = 1)\ny = df2['Potability']\n\n# Splitting\nX_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.3, random_state=1111, stratify=y) #stratify=y","2c0687c7":"#Applying it to PCA function\npca = PCA(n_components=5)\npca.fit(X)\nmat_reduced = pca.transform(X)\n \n#Creating a Pandas DataFrame of reduced Dataset\ndf_pca = pd.DataFrame(mat_reduced , columns = ['PC1','PC2','PC3','PC4','PC5'])\n \n#Concat it with target variable to create a complete Dataset\ndf_pca = pd.concat([df_pca , pd.DataFrame(y)] , axis = 1)\n\ndf_pca.head(5)","7c3fb881":"import seaborn as sb\nimport matplotlib.pyplot as plt\n \nplt.figure(figsize = (6,6))\nsb.scatterplot(data = df_pca , x = 'PC1',y = 'PC2' , hue = 'Potability' , s = 60)\n","9fdf1474":"import statsmodels.api as sm\nimport plotly.express as ex\nimport plotly.graph_objs as go\nex.scatter_3d(df_pca,x='PC1',y='PC2',z='PC3',color='Potability',color_discrete_sequence=['salmon','green'],title=r'$\\textit{Data in Reduced Dimension } R^9 \\rightarrow R^3$')","bf711c2e":"\nevr = pca.explained_variance_ratio_\ntotal_var = evr.sum() * 100\ncumsum_evr = np.cumsum(evr)\n\ntrace1 = {\n    \"name\": \"individual explained variance\", \n    \"type\": \"bar\",\n    'y':evr}\ntrace2 = {\n    \"name\": \"cumulative explained variance\", \n    \"type\": \"scatter\", \n     'y':cumsum_evr}\ndata = [trace1, trace2]\nlayout = {\n    \"xaxis\": {\"title\": \"Principal components\"}, \n    \"yaxis\": {\"title\": \"Explained variance ratio\"},\n  }\nfig = go.Figure(data=data, layout=layout)\nfig.update_layout(title='{:.2f}% of the Original Feature Variance Can Be Explained Using {} Dimensions'.format(np.sum(evr)*100,5))\nfig.show()","d596993e":"#Dropping last column (output)\nX_pca = df_pca.drop(['Potability'], axis = 1)\ny_pca = df_pca['Potability']\n\n# Splitting\nX_train_pca, X_test_pca, y_train_pca, y_test_pca = train_test_split(X_pca,y_pca, test_size=0.3, random_state=1111, stratify=y_pca) #stratify=y","c53a9e00":"pp = sns.pairplot(data=df2,\n                  y_vars=['Potability'],\n                  x_vars=['ph', 'Hardness', 'Solids','Chloramines', 'Sulphate'])\n\npp = sns.pairplot(data=df2,\n                  y_vars=['Potability'],\n                  x_vars=['Conductivity','Organic_carbon', 'Trihalomethanes', 'Turbidity'])","e0db9d32":"from sklearn.neighbors import KNeighborsClassifier \n\nk = range(1,20,1)\n\ntesting_accuracy = []\ntraining_accuracy = []\n\ntesting_accuracy_PCA = []\ntraining_accuracy_PCA = []\n\nscore = 0\n\n\n\n#Without PCA \nfor i in k:\n    knn= KNeighborsClassifier(n_neighbors=i)  \n    knn.fit(X_train, y_train)  \n    y_pred_train = knn.predict(X_train)\n    training_accuracy.append(accuracy_score(y_train, y_pred_train))\n    \n    y_pred_test = knn.predict(X_test)\n    acc_score = accuracy_score(y_test,y_pred_test)\n    testing_accuracy.append(acc_score)\n    \n    if score < acc_score:\n        score = acc_score\n        best_k = i\n        \nprint('Best Accuracy Score for KNN', score, 'Best K-Score', best_k)\nKNN_score = score\n\n#With PCA\nfor i in k:\n    knn_pca= KNeighborsClassifier(n_neighbors=i)  \n    knn_pca.fit(X_train_pca, y_train_pca)  \n    y_pred_train = knn_pca.predict(X_train_pca)\n    training_accuracy_PCA.append(accuracy_score(y_train_pca, y_pred_train))\n    \n    y_pred_test = knn_pca.predict(X_test_pca)\n    acc_score = accuracy_score(y_test_pca,y_pred_test)\n    testing_accuracy_PCA.append(acc_score)\n    if score < acc_score:\n        score = acc_score\n        best_k = i\n\nprint('Best Accuracy Score for PCA from built -in', score, 'Best K-Score', best_k)\nKNN_score_PCA = score","39e99b1e":"sns.lineplot(k, testing_accuracy)\nsns.scatterplot(k, testing_accuracy)\n\nsns.lineplot(k, training_accuracy)\nsns.scatterplot(k, training_accuracy)\nplt.legend(['testing accuracy', 'training accuracy'])","c592a367":"sns.lineplot(k, testing_accuracy_PCA)\nsns.scatterplot(k, testing_accuracy_PCA)\n\nsns.lineplot(k, training_accuracy_PCA)\nsns.scatterplot(k, training_accuracy_PCA)\nplt.legend(['testing accuracy with pca', 'training accuracy with pca'])","a0a2a8ed":"# Train the model again for K = 6 to plot ROC curves \ndef model_evaluation(model, metric):\n    skfold = StratifiedKFold(n_splits = 5)\n    model_cv = cross_val_score(model, X_train, y_train, cv = skfold, scoring = metric)\n    return model_cv\n\nknn = KNeighborsClassifier(n_neighbors = 6)\npipe_knn = Pipeline([('scale', MinMaxScaler()), ('knn', knn)])\npipe_knn.fit(X_train, y_train)\npipe_knn_cv = model_evaluation(pipe_knn, 'roc_auc')\nKNN_roc_auc= roc_auc_score(y,pipe_knn.predict(X))\nfpr,tpr,thresholds = roc_curve(y,pipe_knn.predict_proba(X)[:,1])\nplt.figure()\nplt.plot(fpr,tpr,label=\"AUC (area = %0.2f)\" % KNN_roc_auc)\nplt.plot([0,1],[0,1],\"r--\")\nplt.xlim([0.0,1.0])\nplt.ylim([0.0,1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title(\"ROC\")\nplt.show()\n\ny_pred_test = pipe_knn.predict(X_test)\ntest_accuracy = (accuracy_score(y_test, y_pred_test))\nprint(\"test accuracy for KNN is \", test_accuracy)","01cc444a":"# confusion Maxtrix\ncm5 = confusion_matrix(y_test, pipe_knn.predict(X_test))\nsns.heatmap(cm5\/np.sum(cm5), annot = True, fmt=  '0.2%', cmap = 'Reds')","883a8c73":"lr_model = LogisticRegression(max_iter=120,random_state=0, n_jobs=20)\nlr_model_pca = LogisticRegression(max_iter=120,random_state=0, n_jobs=20)\n\nlr_model.fit(X_train, y_train)\nlr_pred = lr_model.predict(X_test)\n\nlr_model_pca.fit(X_train_pca, y_train_pca)\nlr_pred_pca = lr_model_pca.predict(X_test_pca)","ff301a48":"lr = accuracy_score(y_test, lr_pred)\nprint(\"accuracy without pca - \", lr)\n\nlr_pca = accuracy_score(y_test_pca, lr_pred_pca)\nprint(\"accuracy with pca - \" ,lr_pca)","7212a891":"print(\"WITHOUT PCA\")\nprint(classification_report(y_test,lr_pred))\n\nprint(\"WITH PCA\")\nprint(classification_report(y_test_pca,lr_pred_pca))","41879344":"logit_roc_auc= roc_auc_score(y,lr_model.predict(X))\nfpr,tpr,thresholds = roc_curve(y,lr_model.predict_proba(X)[:,1])\nplt.figure()\nplt.plot(fpr,tpr,label=\"AUC (area = %0.2f)\" % logit_roc_auc)\nplt.plot([0,1],[0,1],\"r--\")\nplt.xlim([0.0,1.0])\nplt.ylim([0.0,1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title(\"ROC\")\nplt.show()","1b31dfcd":"# confusion Maxtrix\ncm1 = confusion_matrix(y_test, lr_pred)\nsns.heatmap(cm1\/np.sum(cm1), annot = True, fmt=  '0.2%', cmap = 'Reds')","4e232d8a":"nb=GaussianNB()\nnb_model=nb.fit(X_train,y_train)\ny_pred=nb_model.predict(X_test)\nacc = accuracy_score(y_test,y_pred)\nprint(acc)\n\nnb_model_pca=nb.fit(X_train_pca,y_train_pca)\ny_pred_pca=nb_model_pca.predict(X_test_pca)\nacc_pca = accuracy_score(y_test_pca,y_pred_pca)\nprint(acc_pca)","3202f0c9":"nb_params={'var_smoothing': np.logspace(0,-9, num=100)}\nnb_cv=GridSearchCV(estimator=nb, \n                 param_grid=nb_params, \n                 cv=10, \n                 verbose=1, \n                 scoring='accuracy') \nnb_cv.fit(X_train,y_train)\nnb_cv.best_params_","6e443efb":"nb=GaussianNB(var_smoothing=0.0533669)\nnb_tuned=nb.fit(X_train,y_train)","de6017cb":"y_pred=nb_tuned.predict(X_test)\naccuracy_score(y_test,y_pred)","f839d115":"nb_params={'var_smoothing': np.logspace(0,-9, num=100)}\nnb_cv=GridSearchCV(estimator=nb, \n                 param_grid=nb_params, \n                 cv=10, \n                 verbose=1, \n                 scoring='accuracy') \nnb_cv.fit(X_train_pca,y_train_pca)\nnb_cv.best_params_","a7b9a470":"nb_tuned_pca=GaussianNB(var_smoothing=0.15199110)\nnb_tuned_pca=nb_tuned_pca.fit(X_train_pca,y_train_pca)","9783e900":"y_pred=nb_tuned_pca.predict(X_test_pca)\naccuracy_score(y_test_pca,y_pred_pca)","76dac97c":"bayes_roc_auc= roc_auc_score(y,nb_tuned.predict(X))\nfpr,tpr,thresholds = roc_curve(y,nb_tuned.predict_proba(X)[:,1])\nplt.figure()\nplt.plot(fpr,tpr,label=\"AUC (area = %0.2f)\" % bayes_roc_auc)\nplt.plot([0,1],[0,1],\"r--\")\nplt.xlim([0.0,1.0])\nplt.ylim([0.0,1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title(\"ROC\")\nplt.show()","93844ac7":"# confusion Maxtrix\ncm1 = confusion_matrix(y_test, y_pred)\nsns.heatmap(cm1\/np.sum(cm1), annot = True, fmt=  '0.2%', cmap = 'Reds')","43e08a77":"# Initialize SVM classifier\nclf_rbf = SVC(kernel='rbf')\nclf_poly = SVC(kernel='poly')\n\n# Fit data\nclf_rbf = clf_rbf.fit(X_train_pca, y_train_pca)\nclf_poly = clf_poly.fit(X_train_pca, y_train_pca)\n\ny_pred_pca=clf_rbf.predict(X_test_pca)\nrbf_acc = accuracy_score(y_test_pca,y_pred_pca)\nprint(rbf_acc)\n\ny_pred_pca=clf_poly.predict(X_test_pca)\npoly_acc= accuracy_score(y_test_pca,y_pred_pca)\nprint(poly_acc)","1f598f33":"clf_rbf.get_params()","5af94b88":"# Get support vectors themselves\nfrom mpl_toolkits.mplot3d import Axes3D\nsupport_vectors = clf_rbf.support_vectors_\n# Visualize support vectors\nfig = plt.figure()\nax = fig.add_subplot(111, projection='3d')\nax.scatter(X_train_pca.iloc[0:10,0], X_train_pca.iloc[0:10,1],X_train_pca.iloc[0:10,2], s=60, c ='skyblue')\nax.scatter(support_vectors[0:10,0], support_vectors[0:10,1],support_vectors[0:10,2],s=60, c='red')\nplt.show()","7df2a9c6":"support_vectors = clf_poly.support_vectors_\n# Visualize support vectors\nfig = plt.figure()\nax = fig.add_subplot(111, projection='3d')\nax.scatter(X_train_pca.iloc[0:10,0], X_train_pca.iloc[0:10,1],X_train_pca.iloc[0:10,2], s=60, c ='skyblue')\nax.scatter(support_vectors[0:10,0], support_vectors[0:10,1],support_vectors[0:10,2],s=60, c='red')\nplt.show()","1bbacdac":"svc_params={\"kernel\" : [\"rbf\"],\n                 \"gamma\": [0.001, 0.01, 0.1, 1,'scale'],\n                 \"C\": [1,10,50,100]}\n\nsvc=SVC()\nsvc_cv_model=GridSearchCV(svc,svc_params,cv=10,n_jobs=-1,verbose=1).fit(X_train_pca,y_train_pca)\nsvc_cv_model.best_params_","baed5344":"kernel = \"rbf\"\nC = 1\ngamma = 0.001","74278736":"svc_tuned_pca=SVC(kernel=kernel,C=C,gamma=gamma).fit(X_train_pca,y_train_pca)\ny_pred_pca=svc_tuned_pca.predict(X_test_pca)\naccuracy_score(y_test_pca,y_pred_pca)","cdc16d24":"svc_tuned=SVC(kernel=kernel,C=C,gamma=gamma).fit(X_train,y_train)\ny_pred=svc_tuned.predict(X_test)\naccuracy_score(y_test,y_pred)","7e2ac3e2":"# confusion Maxtrix\ncm1 = confusion_matrix(y_test, y_pred)\nsns.heatmap(cm1\/np.sum(cm1), annot = True, fmt=  '0.2%', cmap = 'Reds')","51f22f08":"models_pca=[knn_pca,\n         lr_model_pca,\n         svc_tuned_pca,\n         nb_tuned_pca]\n\nmodels=[knn,\n         lr_model,\n         svc_tuned,\n         nb_tuned]\n\n\nresult=[]\nresult_pca=[]\n\nresults=pd.DataFrame(columns=[\"Models\",\"Accuracy\"])\nresults_pca=pd.DataFrame(columns=[\"Models\",\"Accuracy\"])\nnames = ['K Nearest Neighbours','Logistic Regression','Support Vector Machines','Gaussian Naive Bayes']\nprint(\"for PCA\")\ni = 0\nfor model in models_pca:\n    name = names[i]\n    y_pred_pca=model.predict(X_test_pca)\n    accuracy=accuracy_score(y_test_pca,y_pred_pca)\n    result_pca=pd.DataFrame([[name,accuracy*100]],columns=[\"Models\",\"Accuracy\"])\n    results_pca=results_pca.append(result_pca)\n    i = i+ 1\nprint(results_pca)\nsns.barplot(x=\"Accuracy\",y=\"Models\",data=results_pca,color=\"b\")\n","32f2b838":"print(\"With PCA\")\ni = 0\nfor model in models:\n    name = names[i]\n    y_pred=model.predict(X_test)\n    accuracy=accuracy_score(y_test,y_pred) \n    result=pd.DataFrame([[name,accuracy*100]],columns=[\"Models\",\"Accuracy\"])\n    results=results.append(result)\n    i =i + 1\nprint(results)\nsns.barplot(x=\"Accuracy\",y=\"Models\",data=results,color=\"b\")","55d11865":"### Calculating Accuracy Score","8883afc9":"## performing PCA","34f02d96":"Therefore, the variables are barely linearly seperable, therefore models like SVM that depend of linear seperability will perform poorly! ","1f9b3c40":"## INPUTS for splitting the dataset into test and train\n### Test_size\nTest size = 0.3 i.e, 70% of the data is used for training and 30% for testing\n\n### random_state\ncontrols the shuffling applied to the data before applying the split. Pass an int for reproducible output across multiple function calls.\n\n### Stratify \nensure that both the train and test sets have the proportion of examples in each class that is present in the provided \u201cy\u201d array","6401ad9d":"# <center> Naive Bayes<\/center> ","6f3809b9":"## Splitting Data before PCA","d173e539":"# <center> KNN<\/center> ","5be8ece5":"## Checking Missing Data","27daa388":"# Dataset \nhttps:\/\/www.kaggle.com\/sharomeethan\/water-potability-classifier\/\n\nThe dataset contains of 9 attributes\/features which are;\npH.\nHardness.\nSolids.\nChloramines.\nSulfates.\nConductivity.\nOrganic Carbon.\nTrihalomethanes.\nTurbidity.\n\nAll these attributes are continous\n\nThe variable to be predicted is Potability (Discrete)  - that defines the consumption state of that particular sample of water.\n\n0 -> not suitable for consumption.\n\n1 -> suitable for consumption.","4bfa2b16":"Using five components (out of initially 9), we can see that we can only preserve 60 percent of the original variance; we can learn from this fact that our features are indeed uncorrelated between them and there is no linear combination that can tell us a better story regarding the target label after looking at the different permutations of principal components","ab43ef9b":"Since most values have low correlation, there's no collinearity, \n\n### All the features are required for determining the potability!! (Cannot drop any)\n","7fb5d545":"## Import Libraries","b5384959":"# <center> Water Portability Prediction<\/center> ","34fa6e48":"We have incomplete data for pH, sulfate, and trihalomethanes, We can fill all NA values with the mean of each column ","9a3453ab":"# <center> SVM <\/center> ","ed89498c":"Here, you can see that the dataset is imbalanced, i.e only about 39% of the data is portable\n\nTherefore we need to balance the dataset to get more accurate results. ","5fb2fefb":"# Assignment -2 \n# Meghana Rao \n# BL.EN.U4CSE18071","ecaf35fc":"## Understanding PCA","e9a7b4b3":"### Therefore, 6 neighbours is the best choice ","21fa9e07":"## Splitting PCA dataset","bf8a1859":"## Import data","3c938a08":"## Scaling the dataset \nTransform features by scaling each feature to a given range.\nThis estimator scales and translates each feature individually such that it is in the given range on the training set, e.g. between zero and one.\n\n\nBelow, we can see that the dataset is also scaled using Min-Max scaler \n\nThe formulae to calculate how to scale is \n### X_standard_deviation = (X - X_min) \/ (X_max - X_min)\n\nthe scaled dataset is given by \n\n### X_scaled = X_std * (X_max - X_min ) + X_min","0d423606":"## Filling missing values by mean of each column. \nWe need to make sure that the dataset is grouped by various portability values, i.e, only mean values of each column for portability 0 must be seperated from mean values for portability 1 to ensure that the distinction between the 2 still exists and the model can perform better. \n\n### Therefore we use group by to find the 2 different means(for potability - 0 and 1) for each column.","f8b2aad3":"# <center> Logistic Regression<\/center> "}}