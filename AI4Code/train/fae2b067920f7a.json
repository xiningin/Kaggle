{"cell_type":{"8bd15701":"code","1ce3c190":"code","28908615":"code","159887c1":"code","9e40720e":"code","e18921e2":"code","dc926739":"code","846e8549":"code","6a61feaa":"code","f805c3db":"code","e3fe66a9":"code","dba00c72":"code","a8b056f1":"code","094282e4":"code","d2ece865":"markdown","838ddb3a":"markdown","a897395c":"markdown","47ea7c5d":"markdown","02e7ca36":"markdown","32053b03":"markdown","99cf7535":"markdown","3874f671":"markdown","9b046e93":"markdown","8651d630":"markdown","eedde799":"markdown","96e307c4":"markdown","5d23b614":"markdown","ad44fb34":"markdown","7a167c23":"markdown","6bcaf29d":"markdown"},"source":{"8bd15701":"import os\nfrom glob import glob\nimport numpy as np\nimport math\nimport pandas as pd\nimport datatable as dt\nfrom tqdm.notebook import tqdm\nimport copy\nimport pickle as pkl\nimport multiprocessing\n\nfrom sklearn.model_selection import KFold\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\n\nfrom scipy.ndimage import gaussian_filter1d, uniform_filter1d\nfrom scipy.interpolate import interp1d\nimport scipy\nfrom scipy.spatial.distance import cdist\n\nimport torch\nfrom torch import nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.optim import AdamW, lr_scheduler\n\nimport gc\nimport warnings\nwarnings.filterwarnings('ignore')","1ce3c190":"data_dir = '..\/input\/04-waypoints-calculated'\nfiles = glob(f'{data_dir}\/*.txt')","28908615":"# Raw IMU\nacce_ = ['acc_x', 'acc_y', 'acc_z']\ngyro_ = ['gyro_x', 'gyro_y', 'gyro_z']\nmagn_ = ['magn_x', 'magn_y', 'magn_z']\nahrs_ = ['ahrs_x', 'ahrs_y', 'ahrs_z']\nimu_ = acce_ + gyro_ + magn_ + ahrs_\n\n# Rotation\nrotation_ = ['quaternion_cos', 'r_x_x', 'r_x_y', 'r_x_z', 'r_y_x', 'r_y_y', 'r_y_z', \n             'r_x_x_2', 'r_x_y_2', 'r_x_z_2', 'r_y_x_2', 'r_y_y_2', 'r_y_z_2']\n\n# Waypoint\nwaypoint_ = ['x', 'y']\nsmoothed_waypoint_ = ['x_smoothed_model_2', 'y_smoothed_model_2']\ndel_waypoint_ = ['del_x', 'del_y']\nrel_waypoint_ = ['rel_x', 'rel_y']\nstep_mask_ = ['step_mask']\n\n# Timestamp\ntimestamp_ = ['timestamp']\nrel_timestamp_ = ['rel_timestamp']","159887c1":"import json\n# Building map\nwith open('..\/input\/iln-wifi-and-building-mapping\/building_map.json', 'r') as f:\n    building_map = json.load(f)\n    \nwith open('..\/input\/iln-wifi-and-building-mapping\/path_map.json', 'r') as f:\n    path_map = json.load(f)\n    \ninv_path_map = {v: k for k, v in path_map.items()}\n    \n# Floor map\ndef get_floor_target(floor):\n    floor = floor.lower()\n    if floor in ['bf','bm']:\n        return None\n    elif floor == 'b':\n        return -1\n    if floor.startswith('f'):\n        return int(floor[1])-1\n    elif floor.endswith('f'):\n        return int(floor[0])-1\n    elif floor.startswith('b'):\n        return -int(floor[1])\n    elif floor.endswith('b'):\n        return -int(floor[0])\n    else:\n        return None\n\ndef feature_extraction(df, framming = False):\n    # Extract\n    imu = df[imu_].values.astype(float)[1:]\n    rotation = df[rotation_].values.astype(float)[1:]\n    waypoint = np.diff(df[smoothed_waypoint_].values.astype(float), axis = 0)\n    \n    # Framing\n    if framming:\n        imu = framing_time_series(imu)\n        rotation = framing_time_series(rotation)\n        waypoint = framing_time_series(waypoint, islabel = True)\n    \n    return imu, rotation, waypoint\n\ndef framing_time_series(X, islabel = False, window = 50, flatten = True):\n    T = X.shape[0]\n        \n    frame_X = []\n        \n    if islabel:\n        for i in range(T - window + 1):\n            sub_X = X[(i + window - 1),:] - X[i,:]\n            frame_X.append(sub_X)\n    else:\n        for i in range(T - window + 1):\n            sub_X = X[i:(i + window),:]\n            frame_X.append(sub_X)\n            \n    if flatten:\n        return np.array(frame_X).reshape(len(frame_X), -1)\n    else:\n        return np.array(frame_X)\n\ndef get_building_floor_path(fname):\n    # Extract information from file name\n    building, floor, path = fname.split('\/')[-1].split('.')[0].split('_')\n    \n    # Encode\n    building = np.array([building_map[building]])\n    floor = np.array([get_floor_target(floor)])\n    \n    return building, floor, path","9e40720e":"class ILN_Dataset(Dataset):\n    def __init__(self, files, max_len = 12_500, framming = False):\n        self.files = files\n        self.max_len = max_len\n        self.framming = framming\n        \n    def __len__(self):\n        return len(self.files)\n    \n    def __getitem__(self, idx):\n        # File name\n        file = self.files[idx]\n        \n        # Extract building, floor, path\n        building, floor, path = get_building_floor_path(file)\n        \n        # Read data\n        df = pd.read_csv(file, index_col = 0)\n        \n        # Extract features\n        imu, rotation, waypoint = feature_extraction(df, framming = self.framming)\n        seq_len = imu.shape[0]\n        \n        padding_mask = np.ones(seq_len)\n        \n        # Padding and cutting\n        padding_mask_ = np.zeros(self.max_len)\n        imu_feature_ = np.zeros((self.max_len, imu.shape[1]))\n        rotation_feature_ = np.zeros((self.max_len, rotation.shape[1]))\n        waypoint_feature_ = np.zeros((self.max_len, 2))\n        \n        if seq_len <= self.max_len:   # Pad\n            padding_mask_[-seq_len:] = padding_mask\n            imu_feature_[-seq_len:,:] = imu\n            rotation_feature_[-seq_len:,:] = rotation\n            waypoint_feature_[-seq_len:,:] = waypoint\n        else:    # Cut\n            padding_mask_ = padding_mask[-self.max_len:]\n            imu_feature_ = imu[-self.max_len:,:]\n            rotation_feature_ = rotation[-self.max_len:,:]\n            waypoint_feature_ = waypoint[-self.max_len:,:]\n            \n        return {\n            'padding_mask': torch.tensor(padding_mask_, dtype = torch.bool),\n            'building': torch.tensor(building, dtype = torch.long),\n            'imu': torch.tensor(imu_feature_, dtype = torch.float),\n            'rotation': torch.tensor(rotation_feature_, dtype = torch.float),\n            'waypoint': torch.tensor(waypoint_feature_, dtype = torch.float)\n        }","e18921e2":"def clones(module, N):\n    \"Produce N identical layers.\"\n    return nn.ModuleList([copy.deepcopy(module) for _ in range(N)])\n\ndef attention(query, key, value, mask=None, dropout=None):\n    \"Compute 'Scaled Dot Product Attention'\"\n    d_k = query.size(-1)\n    scores = torch.matmul(query, key.transpose(-2, -1)) \\\n             \/ math.sqrt(d_k)\n    if mask is not None:\n        scores = scores.masked_fill(mask == 0, -1e9)\n    p_attn = F.softmax(scores, dim=-1)\n    if dropout is not None:\n        p_attn = dropout(p_attn)\n    return torch.matmul(p_attn, value), p_attn\n\n\nclass MultiHeadedAttention(nn.Module):\n    def __init__(self, d_model, nhead, dropout=0.1, max_len = 512):\n        \"Take in model size and number of heads.\"\n        super(MultiHeadedAttention, self).__init__()\n        assert d_model % nhead == 0\n        # We assume d_v always equals d_k\n        self.d_k = d_model \/\/ nhead\n        self.nhead = nhead\n        self.linears = clones(nn.Linear(d_model, d_model, bias=False), 4) # Q, K, V, last\n        self.linear_key_value = clones(nn.Linear(max_len, 256, bias= False), 3)\n        self.attn = None\n        self.dropout = nn.Dropout(p=dropout)\n\n    def forward(self, query, key, value, mask=None):\n        \"Implements Figure 2\"\n        if mask is not None:\n            # Same mask applied to all h heads.\n            mask = mask.unsqueeze(-1)\n            # Mask query, key and value\n            query = query * mask\n            key = key * mask\n            mask = value * mask\n        nbatches = query.size(0)\n\n        # 1) Linear projection\n        key, value = \\\n            [l(x).transpose(-1, -2)\n             for l, x in zip(self.linear_key_value, (key.transpose(-1,-2), value.transpose(-1,-2)))]\n        \n        # 2) Do all the linear projections in batch from d_model => h x d_k\n        query, key, value = \\\n            [l(x).view(nbatches, -1, self.nhead, self.d_k).transpose(1, 2)\n             for l, x in zip(self.linears, (query, key, value))]\n        \n\n        # 2) Apply attention on all the projected vectors in batch.\n        x, self.attn = attention(query, key, value, mask=None,\n                                 dropout=self.dropout)\n\n        # 3) \"Concat\" using a view and apply a final linear.\n        x = x.transpose(1, 2).contiguous() \\\n            .view(nbatches, -1, self.nhead * self.d_k)\n        return self.linears[-1](x)\n\n\nclass PositionwiseFeedForward(nn.Module):\n    \"Implements FFN equation.\"\n    def __init__(self, d_model, d_ff, dropout=0.1):\n        super(PositionwiseFeedForward, self).__init__()\n        self.w_1 = nn.Linear(d_model, d_ff)\n        self.w_2 = nn.Linear(d_ff, d_model)\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x):\n        return self.w_2(self.dropout(F.relu(self.w_1(x))))\n    \nclass EncoderLayer(nn.Module):\n    \"\"\"\n    Single Encoder block of SAINT\n    \"\"\"\n    def __init__(self, d_model, nhead, dim_feedforward = 1024, dropout = 0.1, max_len = 512):\n        super().__init__()\n        self._self_attn = MultiHeadedAttention(d_model, nhead, dropout = dropout, max_len = max_len)\n        self._ffn = PositionwiseFeedForward(d_model, dim_feedforward, dropout)\n        self._layernorms = clones(nn.LayerNorm(d_model, eps=1e-6), 2)\n        self._dropout = nn.Dropout(dropout)\n\n    def forward(self, src, mask = None):\n        \"\"\"\n        query: question embeddings\n        key: interaction embeddings\n        \"\"\"\n        # self-attention block\n        src2 = self._self_attn(query=src, key=src, value=src, mask=mask)\n        src = src + self._dropout(src2)\n        src = self._layernorms[0](src)\n        src2 = self._ffn(src)\n        src = src + self._dropout(src2)\n        src = self._layernorms[1](src)\n        return src\n    \nclass DecoderLayer(nn.Module):\n    \"\"\"\n    Single Encoder block of SAINT\n    \"\"\"\n    def __init__(self, d_model, nhead, dim_feedforward = 1024, dropout = 0.1, max_len = 512):\n        super().__init__()\n        self._self_attn_decoder = MultiHeadedAttention(d_model, nhead, dropout = dropout, max_len = max_len)\n        self._self_attn_encoder_decoder = MultiHeadedAttention(d_model, nhead, dropout = dropout, max_len = max_len)\n        self._ffn = PositionwiseFeedForward(d_model, dim_feedforward, dropout)\n        self._layernorms = clones(nn.LayerNorm(d_model, eps=1e-6), 3)\n        self._dropout = nn.Dropout(dropout)\n\n    def forward(self, tgt, memory, encoder_decoder_mask = None, decoder_mask = None):\n        \"\"\"\n        query: question embeddings\n        key: interaction embeddings\n        \"\"\"\n        # self-attention block\n        tgt2 = self._self_attn_decoder(query=tgt, key=tgt, value=tgt, mask=decoder_mask)\n        tgt = tgt + self._dropout(tgt2)\n        tgt = self._layernorms[0](tgt)\n        tgt2 = self._self_attn_encoder_decoder(query=tgt, key=memory, value=memory, mask=encoder_decoder_mask)\n        tgt = tgt + self._dropout(tgt2)\n        tgt = self._layernorms[1](tgt)\n        tgt2 = self._ffn(tgt)\n        tgt = tgt + self._dropout(tgt2)\n        tgt = self._layernorms[2](tgt)\n        return tgt","dc926739":"class PositionalEncoding(nn.Module):\n\n    def __init__(self, d_model, dropout = 0.1, max_len = 15_000):\n        super(PositionalEncoding, self).__init__()\n        self.max_len = max_len\n        self.dropout = nn.Dropout(p=dropout)\n        self.div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) \/ d_model))\n\n    def forward(self, x, mask = None):\n        if mask is not None:\n            position = torch.cumsum(mask.unsqueeze(-1), dim = 1)\n        else:\n            position = torch.repeat_interleave(torch.arange(0, self.max_len).unsqueeze(-1).unsqueeze(0), x.shape[0], dim = 0).to(x.device)\n        \n        pe = torch.zeros(x.shape).to(x.device)\n        div_term = self.div_term.to(x.device)\n        pe[:,:,0::2] = torch.sin(position * div_term)\n        pe[:,:,1::2] = torch.cos(position * div_term)\n        \n        x = x + pe\n        \n        return self.dropout(x)","846e8549":"class ILN_Model(nn.Module):\n    def __init__(self, num_feature_imu = 12, num_feature_rotation = 13, num_building = 24, \n                 d_model = 128, nhead = 4, max_len = 12_500, window = 50, droprate = 0.1):\n        super(ILN_Model, self).__init__()\n        \n        # Embedding\n        self.building_embedding = nn.Embedding(num_embeddings = num_building, embedding_dim = d_model)\n        \n        # Linear projection\n        self.linear_imu_rotation = nn.Sequential(\n            nn.Linear((num_feature_imu + num_feature_rotation) * window, d_model),\n            nn.LayerNorm(d_model),\n            nn.ReLU(),\n            nn.Dropout(droprate)\n        )\n        \n        # Positional encoding\n        self.position = PositionalEncoding(d_model, dropout = droprate, max_len = max_len)\n        \n        # Encoder\n        self.encoder_layers = clones(EncoderLayer(d_model, nhead, dim_feedforward = 512, dropout = droprate, max_len = max_len), 1)\n        \n        # Output layers\n        self.output = nn.Sequential(\n            nn.Linear(d_model, 128),\n            nn.LayerNorm(128),\n            nn.ReLU(),\n            nn.Dropout(0.1),\n            nn.Linear(128, 2)\n        )\n        \n        \n    def forward(self, padding_mask, building, imu, rotation):\n        # Embed building\n        building = self.building_embedding(building)\n        \n        # Project IMU and rotation\n        x = torch.cat((imu, rotation), dim = -1)\n        x_input = self.linear_imu_rotation(x) + building\n        \n        # Positional embedding\n        x = self.position(x_input, mask = padding_mask)\n        \n        # Attention\n        for layer in self.encoder_layers:\n            x = layer(x, mask = padding_mask)\n        \n        x = self.output(x)\n        \n        return x","6a61feaa":"class MeanPositionLoss(nn.Module):\n    def __init__(self):\n        super(MeanPositionLoss, self).__init__()\n        \n    def forward(self, output, true, mask):\n        diff = output - true\n        diff = diff[mask,:]\n        \n        loss = torch.mean(torch.sum(torch.abs(diff), dim = -1))\n        return loss\n    \ndef loss_fn(output, true, mask):\n    return MeanPositionLoss()(output, true, mask)","f805c3db":"def train_fn(model, train_dataloader, optimizer, scheduler = None, device = 'cpu'):\n    model.train()\n    \n    way_pred = []\n    way_true = []\n\n    loss = 0\n    \n    tbar = tqdm(train_dataloader)\n    for item in tbar:\n        padding_mask = item['padding_mask'].to(device)\n        building = item['building'].to(device)\n        imu = item['imu'].to(device)\n        rotation = item['rotation'].to(device)\n        waypoint = item['waypoint'].to(device)\n\n        # Set zero gradient\n        optimizer.zero_grad()\n        \n        # Feed input to the model\n        output = model(padding_mask, building, imu, rotation)\n        \n        # Compute losses\n        loss_batch = loss_fn(output, waypoint, padding_mask)\n        \n        tbar.set_description(f'Loss: {round(loss_batch.item(), 3)}')\n        \n        # Backpropagation\n        loss_batch.backward()\n        \n        # Update parameters\n        optimizer.step()\n        \n        if scheduler is not None:\n            scheduler.step()\n            \n        # Update loss\n        loss += loss_batch.item() \/ len(train_dataloader)\n    \n    return loss","e3fe66a9":"def valid_fn(model, valid_dataloader, device = 'cpu'):\n    model.eval()\n    \n    way_pred = []\n    way_true = []\n    \n    loss = 0\n    \n    tbar = tqdm(valid_dataloader)\n    for item in tbar:\n        padding_mask = item['padding_mask'].to(device)\n        building = item['building'].to(device)\n        imu = item['imu'].to(device)\n        rotation = item['rotation'].to(device)\n        waypoint = item['waypoint'].to(device)\n        \n        # Feed input to the model\n        with torch.no_grad():\n            output = model(padding_mask, building, imu, rotation)\n        \n            # Compute losses\n            loss_batch = loss_fn(output, waypoint, padding_mask)\n            \n            tbar.set_description(f'Loss: {round(loss_batch.item(), 3)}')\n\n            # Update loss\n            loss += loss_batch.item() \/ len(valid_dataloader)\n    \n    return loss","dba00c72":"class config():\n    # For training\n    nepochs = 12\n    lr = 1e-3\n    weight_decay = 1e-5\n    split = KFold(n_splits = 5)\n    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n    resume = False\n    verbose = 1\n    framming = False\n    if framming:\n        window = 50\n    else:\n        window = 1\n    # For dataloader\n    batch_size = 4\n    num_workers = 4\n    # For model\n    d_model = 128\n    nhead = 2\n    max_len = 25_000\n    droprate = 0.3\n    \ncfg = config()","a8b056f1":"X = np.array(files)\ny = np.array([building_map[i.split('\/')[-1].split('_')[0]] for i in files])","094282e4":"for i, (trn_idx, val_idx) in enumerate(cfg.split.split(X, y)):\n    if i in [0]:\n        print('*' * 50)\n        print(f'Fold {i}-th')\n        \n        # Dataset\n        train_dataset = ILN_Dataset(X[trn_idx].tolist(), max_len = cfg.max_len, framming = cfg.framming)\n        valid_dataset = ILN_Dataset(X[val_idx].tolist(), max_len = cfg.max_len, framming = cfg.framming)\n        \n\n        # Dataloader\n        train_dataloader = DataLoader(train_dataset, batch_size = cfg.batch_size, num_workers = cfg.num_workers, shuffle = True)\n        valid_dataloader = DataLoader(valid_dataset, batch_size = cfg.batch_size, num_workers = cfg.num_workers, shuffle = False)\n\n        # Model\n        model = ILN_Model(d_model = cfg.d_model, nhead = cfg.nhead, window = cfg.window,\n                          max_len = cfg.max_len, droprate = cfg.droprate).to(cfg.device)\n        optimizer = AdamW(model.parameters(), lr = cfg.lr, weight_decay = cfg.weight_decay)\n        scheduler = lr_scheduler.OneCycleLR(optimizer, max_lr = 5 * cfg.lr, steps_per_epoch = len(train_dataloader), epochs = cfg.nepochs)\n\n        # Early stopping\n        es = 0\n        best_mpe = np.inf\n\n        # Train and validate\n        for epoch in range(cfg.nepochs):\n            loss_train = train_fn(model, train_dataloader, optimizer, scheduler = scheduler, device = cfg.device)\n            torch.cuda.empty_cache()\n            loss_valid  = valid_fn(model, valid_dataloader, device = cfg.device)\n\n            if (epoch + 1) % cfg.verbose == 0:\n                print('-' * 50)\n                print(f'Epoch: {epoch + 1}  -  Training  -  Loss: {loss_train}')\n                print(f'Epoch: {epoch + 1}  -  Validating  -  Loss: {loss_valid}')\n\n            if loss_valid < best_mpe:\n                best_mpe = loss_valid\n                model_dict = {\n                    'epoch': epoch,\n                    'model_state_dict': model.state_dict(),\n                    'optimizer_state_dict': optimizer.state_dict(),\n                    'scheduler_state_dict': scheduler.state_dict(),\n                    'best_mpe': loss_valid,\n                }\n                torch.save(model_dict, f'model_best_fold_{i}_delta.pt')\n                es = 0\n            else:\n                es += 1\n                if (epoch + 1) % cfg.verbose == 0:\n                    print(f'Epoch: {epoch + 1}  -  There is no improvement in this epoch...')\n\n            if (epoch + 1) % cfg.verbose == 0:\n                print(f'Epoch: {epoch + 1}  -  Best MPE: {best_mpe}')\n\n            if es == 5:\n                break","d2ece865":"# Necessary packages","838ddb3a":"# Dataset","a897395c":"* Main model","47ea7c5d":"* Loss functions","02e7ca36":"# Model","32053b03":"* Training function","99cf7535":"* Run","3874f671":"# Configuration and util functions","9b046e93":"* Prepare for KFold","8651d630":"* Configuration","eedde799":"* Positional encoding","96e307c4":"* Validating function","5d23b614":"* Utils","ad44fb34":"# Import data","7a167c23":"# Feature name","6bcaf29d":"* Feature extraction functions"}}