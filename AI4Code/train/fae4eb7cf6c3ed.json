{"cell_type":{"c4670ac5":"code","b6b7ed69":"code","fa904f18":"code","c7cdca32":"code","13c2a4ca":"code","a2c85181":"code","ea39ecbe":"code","f2fc9b76":"code","ac440dd2":"code","c6d5878b":"code","34212a2d":"code","30cc4c86":"code","9471b86e":"markdown","813f0e2e":"markdown","ab5f26da":"markdown"},"source":{"c4670ac5":"import numpy as np\nfrom sklearn.metrics import accuracy_score\nimport pandas as pd\nimport re\nimport sklearn\nimport xgboost as xgb\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline","b6b7ed69":"# Load in the train and test datasets\ntrain = pd.read_csv('..\/input\/train.csv')\ntest = pd.read_csv('..\/input\/test.csv')\n\nall = pd.concat([ train, test ],sort=False)","fa904f18":"all.head()","c7cdca32":"from sklearn.preprocessing import OneHotEncoder\nfrom sklearn.preprocessing import LabelEncoder\n\nle = LabelEncoder()\nall[\"Embarked\"] = le.fit_transform(all[\"Embarked\"].fillna('0'))\nall[\"Sex\"] = le.fit_transform(all[\"Sex\"].fillna('3'))\nall.head()","13c2a4ca":"# split the data back into train and test\ndf_train = all.loc[all['Survived'].isin([np.nan]) == False]\ndf_test  = all.loc[all['Survived'].isin([np.nan]) == True]","a2c85181":"print(df_train.shape)\nprint(df_test.shape)\ndf_train.head()","ea39ecbe":"df_test.head()","f2fc9b76":"'''\n# XGBoost model + parameter tuning with GridSearchCV\nfrom xgboost import XGBRegressor\nfrom sklearn.model_selection import GridSearchCV, train_test_split\n\nfeature_names = ['Sex','Embarked','Pclass','Survived','Age','SibSp','Parch','Fare']\n\nxgb = XGBRegressor()\nparams={\n    'max_depth': [2,3,4,5], \n    'subsample': [0.4,0.5,0.6,0.7,0.8,0.9,1.0],\n    'colsample_bytree': [0.5,0.6,0.7,0.8],\n    'n_estimators': [1000,2000,3000],\n    'reg_alpha': [0.01, 0.02, 0.03, 0.04]\n}\n\ngrs = GridSearchCV(xgb, param_grid=params, cv = 10, n_jobs=4, verbose=2)\ngrs.fit(np.array(df_train[feature_names]), np.array(df_train['Survived']))\n\nprint(\"Best parameters \" + str(grs.best_params_))\ngpd = pd.DataFrame(grs.cv_results_)\nprint(\"Estimated accuracy of this model for unseen data: {0:1.4f}\".format(gpd['mean_test_score'][grs.best_index_]))\n# TODO: why is this so bad?\n'''","ac440dd2":"train_y = df_train['Survived']; train_x = df_train.drop('Survived',axis=1)\nexcluded_feats = ['PassengerId','Ticket','Cabin','Name']\nfeatures = [f_ for f_ in train_x.columns if f_ not in excluded_feats]\nfeatures","c6d5878b":"from sklearn.model_selection import KFold\nfolds = KFold(n_splits=4, shuffle=True, random_state=546789)","34212a2d":"oof_preds = np.zeros(train_y.shape[0])\nsub_preds = np.zeros(df_test.shape[0])","30cc4c86":"import gc\nfrom xgboost import XGBClassifier\nfrom sklearn.metrics import accuracy_score, roc_auc_score, confusion_matrix\n\n\nfor n_fold, (trn_idx, val_idx) in enumerate(folds.split(train_x)):\n\n    trn_x, trn_y = train_x[features].iloc[trn_idx], train_y.iloc[trn_idx]\n\n    val_x, val_y = train_x[features].iloc[val_idx], train_y.iloc[val_idx]\n\n    \n\n    clf = XGBClassifier(\n\n        objective = 'binary:logistic', \n\n        booster = \"gbtree\",\n\n        eval_metric = 'auc', \n\n        nthread = 4,\n\n        eta = 0.05,\n\n        gamma = 0,\n\n        max_depth = 2, \n\n        subsample = 0.6, \n\n        colsample_bytree = 0.8, \n\n        colsample_bylevel = 0.675,\n\n        min_child_weight = 22,\n\n        alpha = 0,\n\n        random_state = 42, \n\n        nrounds = 2000,\n        \n        n_estimators=3000\n\n    )\n\n\n\n    clf.fit(trn_x, trn_y, eval_set= [(trn_x, trn_y), (val_x, val_y)], verbose=10, early_stopping_rounds=100)\n\n    \n\n    oof_preds[val_idx] = clf.predict_proba(val_x)[:, 1]\n\n    sub_preds += clf.predict_proba(df_test[features])[:, 1] \/ folds.n_splits\n\n    \n\n    print('Fold %2d AUC : %.6f' % (n_fold + 1, roc_auc_score(val_y, oof_preds[val_idx])))\n\n    del clf, trn_x, trn_y, val_x, val_y\n\n    gc.collect()\n\n    \n\nprint('Full AUC score %.6f' % roc_auc_score(train_y, oof_preds))   \n\n\n\ntest['Survived'] = sub_preds\n\n\n\ntest[['PassengerId', 'Survived']].to_csv('xgb_submission_esi.csv', index=False, float_format='%.8f')","9471b86e":"XGBOOST \uacf5\uc2dd \ubb38\uc11c : https:\/\/xgboost.readthedocs.io\/en\/latest\/","813f0e2e":"\uc704\uc5d0\ub294 \uadf8\ub9ac\ub4dc\uc11c\uce58\ub77c\ub294 \uae30\ubc95\uc744 \uc774\uc6a9\ud574\uc11c \uc608\uce21\ud55c\uac74\ub370 \uc5ec\uae30\uc11c \uc120\ud0dd\uc9c0\uac00 3\uac1c\ub85c \uac08\ub9bc.\n1. \uadf8\ub9ac\ub4dc \uc11c\uce58   2. \ub79c\ub364 \uc11c\uce58    3.  \ubca0\uc774\uc9c0\uc548 \ucd5c\uc801\ud654 \n\n\uadf8\ub9ac\ub4dc < \ub79c\ub364 < \ubca0\uc774\uc9c0\uc548 \uc73c\ub85c \uad1c\ucc2e\uc740 \ubc29\ubc95\uc785\ub2c8\ub2e4.\n\nBest parameters {'colsample_bytree': 0.8, 'max_depth': 2, 'n_estimators': 3000, 'reg_alpha': 0.01, 'subsample': 0.6}","ab5f26da":"\uc544\ub798\ub294 XGBOOST \ubaa8\ub378 \ub9cc\ub4dc\ub294 \ubd80\ubd84\uc778\ub370 \uc815\ubcf4\uc190\uc2e4 \ucd5c\uc18c\ud654 \ud558\uae30 \uc704\ud574\uc11c kfold\ubc29\uc2dd \uc774\uc6a9\ud588\uc2b5\ub2c8\ub2e4. \n\n\uc21c\uc11c\ub294 \n\n1. X\ubcc0\uc218\uc640 Y\ubcc0\uc218\ub97c \ubd84\ub9ac\ud574\uc900\ub2e4.\n\n```\n    trn_x, trn_y = train_x[features].iloc[trn_idx], train_y.iloc[trn_idx]\n\n    val_x, val_y = train_x[features].iloc[val_idx], train_y.iloc[val_idx]\n```\n\n2.  \ubaa8\ub378\uc758 \ud558\uc774\ud37c\ud30c\ub77c\ubbf8\ud130\ub97c \uc870\uc815\ud574\uc900\ub2e4. \uc5ec\uae30\uc5d0\uc11c \uc704\uc5d0\uc11c \ub9d0\ud55c \uadf8\ub9ac\ub4dc\uc11c\uce58, \ub79c\ub364\uc11c\uce58, \ubca0\uc774\uc9c0\uc548 \ucd5c\uc801\ud654\uac00 \uc4f0\uc785\ub2c8\ub2e4.\n\n\ubca0\uc774\uc9c0\uc548 \ucd5c\uc801\ud654 in XGBOOST : http:\/\/www.kwangsiklee.com\/2018\/06\/bayesianoptimization%EC%9D%84-%EC%9D%B4%EC%9A%A9%ED%95%98%EC%97%AC-xgboost-%ED%95%98%EC%9D%B4%ED%8D%BC-%ED%8C%8C%EB%9D%BC%EB%AF%B8%ED%84%B0%EB%A5%BC-%EC%B5%9C%EC%A0%81%ED%99%94\/\n\n```\n    clf = XGBClassifier(\n\n        objective = 'binary:logistic', \n\n        booster = \"gbtree\",\n\n        eval_metric = 'auc', \n\n        nthread = 4,\n\n        eta = 0.05,\n\n        gamma = 0,\n\n        max_depth = 2, \n\n        subsample = 0.6, \n\n        colsample_bytree = 0.8, \n\n        colsample_bylevel = 0.675,\n\n        min_child_weight = 22,\n\n        alpha = 0,\n\n        random_state = 42, \n\n        nrounds = 2000,\n        \n        n_estimators=3000\n\n    )\n```   \n 3.   \ubaa8\ub378\uc744 \uc801\uc6a9\uc2dc\ud0b5\ub2c8\ub2e4.\n \n```\n    clf.fit(trn_x, trn_y, eval_set= [(trn_x, trn_y), (val_x, val_y)], verbose=10, early_stopping_rounds=100)\n```\n\n2 \ubc88\uc758 \ucd5c\uc801\ud654 \uad00\ub828\ud574\uc11c\ub294 \uc800\uae30\uc5d0 \uc801\ud78c\uac83\ubcf4\ub2e4 \ub9ce\uc740 \ud558\uc774\ud37c \ud30c\ub77c\ubbf8\ud130\uac00 \uc788\ub294\ub370 \nhttps:\/\/www.analyticsvidhya.com\/blog\/2016\/03\/complete-guide-parameter-tuning-xgboost-with-codes-python\/ \uc774 \ub9c1\ud06c\uc5d0 \uc124\uba85\uc774 \uc798 \ub098\uc640\uc788\uc2b5\ub2c8\ub2e4. \n\uadf8\ub9ac\uace0 \ubaa8\ub4e0 \ubcc0\uc218\uc5d0 \ub300\ud55c \uc124\uba85\uc740 https:\/\/xgboost.readthedocs.io\/en\/latest\/parameter.html \ub9c1\ud06c\uc5d0 \ub098\uc640\uc788\uc2b5\ub2c8\ub2e4.\n\n\uac1c\uc778\uc801\uc73c\ub85c \uc0ac\ub78c\ub4e4\uc774 \ub9ce\uc774 \ub2e4\ub8e8\ub294\uac74 **`booster`** \uc640 `eta` , `gmmama`,  **`max_depth`**,  `min_child_weight`, `subsample`, `colsample_bytree`, `scale_pos_weigh`, `max_leaves`\ub97c \ub9ce\uc774 \uc124\uc815\ud558\ub294\uac83 \uac19\uc558\uc2b5\ub2c8\ub2e4.\n\n\n"}}