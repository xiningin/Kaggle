{"cell_type":{"92c5fd78":"code","57233e3c":"code","7362050a":"code","318f447e":"code","e1abdeda":"code","f2162925":"code","5ccfa419":"code","720285bd":"code","2348c0d2":"code","aab147f0":"code","1cf7905d":"code","d570b6bd":"code","b8aad027":"code","b8731d13":"code","c3820b82":"code","a495434c":"code","7f894dff":"markdown","37e05f20":"markdown","0f48b3c5":"markdown","ffed752f":"markdown","0d5722fc":"markdown","345916f5":"markdown","0f5f349a":"markdown","17f33673":"markdown","6c4dca3b":"markdown","c447a93e":"markdown","d18d7435":"markdown","710f4acc":"markdown","629924ab":"markdown","6ad56044":"markdown","fedc2959":"markdown","aebe1358":"markdown","7a01d4e0":"markdown","7f0a4079":"markdown","f1e38ec2":"markdown","ee6b8865":"markdown","81826fc9":"markdown","5f0465a2":"markdown","53b0fd6f":"markdown"},"source":{"92c5fd78":"import seaborn as sns\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport scipy.stats as stat\nimport pylab ","57233e3c":"df=sns.load_dataset('titanic')\ndf=df[['age','fare','survived']]","7362050a":"df.drop(df[df['fare']==0.0].index,inplace=True)","318f447e":"from sklearn.preprocessing import StandardScaler","e1abdeda":"scaler=StandardScaler()\ndf_scaled=scaler.fit_transform(df)","f2162925":"df_scaled","5ccfa419":"from sklearn.preprocessing import MinMaxScaler\nmin_max=MinMaxScaler()\ndf_minmax=min_max.fit_transform(df)\ndf_minmax","720285bd":"from sklearn.preprocessing import RobustScaler\nscaler=RobustScaler()\ndf_robust_scaler=scaler.fit_transform(df)\ndf_robust_scaler","2348c0d2":"def plot_data(df,feature):\n    plt.figure(figsize=(10,6))\n    plt.subplot(1,2,1)\n    df[feature].hist()\n    plt.subplot(1,2,2)\n    stat.probplot(df[feature],dist='norm',plot=pylab)\n    plt.show","aab147f0":"plot_data(df,'fare')","1cf7905d":"# This is how we can perform Log Transformation on a feaeture.\ndf['fare_log']=np.log(df['fare'])\n\n# Now making the Q-Q plot of the transformed feature.\nplot_data(df,'fare_log')","d570b6bd":"df['fare_reciprocal']=1\/df.fare\nplot_data(df,'fare_reciprocal')","b8aad027":"df['fare_square']=df.fare**(1\/2)\nplot_data(df,'fare_square')","b8731d13":"df['fare_exponential']=df.fare**(1\/1.2)\nplot_data(df,'fare_exponential')","c3820b82":"df['fare_boxcox'],parameters=stat.boxcox(df['fare'])","a495434c":"plot_data(df,'fare_boxcox')","7f894dff":"***","37e05f20":"## D. Exponential Transformation","0f48b3c5":"Here's an example of how the Q-Q plot would look like.<br>\nOne thing we have to notice is that in the probability plot on the right, the points which aligns the best with the red line (which is the line of regression) is the most suitable transformation for the feature.","ffed752f":"***","0d5722fc":"## E. Box-Cox Transformation\n<br>\nThe Box-Cox transformation is defined as: \n\n> $\\displaystyle y(\u03bb) = \\frac{(y^{\u03bb}\u22121)} {\u03bb}\\qquad  if\\quad \u03bb\\ne0 $\n<br><br>\n> $\\displaystyle y(\u03bb) = log(y)\\qquad  if\\quad \u03bb\\: =0 $\n\n<br>\nwhere y is the response variable and \u03bb is the transformation parameter. \u03bb varies from -5 to 5. In the transformation, all values of \u03bb  are considered and the optimal value for a given variable is selected.","345916f5":"## B. Reciprocal Transformation","0f5f349a":"# Importing Libraries and Dataset","17f33673":"# Conclusion\nThis is how we can perform different types of Feature Transformations.","6c4dca3b":"***","c447a93e":"***","d18d7435":"We can use Q-Q plots to check whether or not a feaure is normally distributed.","710f4acc":"***","629924ab":"## A. Logarithmic Transformation","6ad56044":"## C. Square Root Transformation","fedc2959":"# 2. Min-Max Scaling\nMin Max Scaling scales the values between 0 to 1.\nWe can also set the range the range according to our need. \n>$ \\displaystyle X_{scaled} = \\frac{(X - X_{min})} {(X_{max} - X_{min})}$","aebe1358":"#### Types Of Transformation:\n\n1. Standardization - Using StandardScaler\n\n2. Scaling to Minimum And Maximum values - Using MinMaxScaler\n\n3. Scaling To Median And Quantiles - Using RobustScaler\n\n4. Guassian Transformation:\n    1. Logarithmic Transformation\n    2. Reciprocal Trnasformation\n    3. Square Root Transformation\n    4. Exponential Trnasformation\n    5. Box Cox Transformation","7a01d4e0":"0,1,2,3,4,5,6,7,8,9,10\n\n9-90 percentile -> 90% of all values in this group is less than 9<br>\n1-10 precentile -> 10% of all values in this group is less than 1<br>","7f0a4079":"# 3. Robust Scaler\nRobust Scaler are robust to outliers. It is used to scale the feature to median and quantiles.<br>\nScaling using median and quantiles consists of substracting the median to all the observations, and then dividing by the interquantile difference.<br> The interquantile difference is the difference between the 75th and 25th quantile:\n<br><br>\n>$IQR = 75^{th}quantile - 25^{th} quantile$\n<br><br>\n>$\\displaystyle X_{scaled} = \\frac{(X - X_{median})} {IQR}$","f1e38ec2":"# <center>Thank You!","ee6b8865":"***","81826fc9":"# 4. Guassian Transformation\n\nSome machine learning algorithms like linear and logistic assume that the features are normally distributed which is usally not the case.<br>\nTherefore we can convert them into normally distributed features which will in turn help the model to learn in a better way.\n<br>Now, there are a few ways we can do this:\n- logarithmic transformation\n- reciprocal transformation\n- square root transformation\n- exponential transformation\n- box cox transformation\n","5f0465a2":"# 1. Standardization\nStandardization comes into picture when features of input data set have large differences between their ranges, or simply when they are measured in different measurement units (e.g., Pounds, Meters, Miles \u2026 etc).\n\nWe try to bring all the variables or features to a similar scale. Standarisation means centering the variable at zero.\n>$ \\displaystyle z = \\frac{(x-x_{mean})}{std}$","53b0fd6f":"# Transformation of Features\n\nWhy is Transformation of Features required?\n\n\n1. Linear Models : Linear models work on the concept of gradient descent which is used to find the Minima of our cost function. Since outliers can heavily affect the model, we try to transform the data.\n2. Algorithms like KNN,K Means,Hierarichal Clustering uses Eucledian Distance which is also heavily affected by outliers so to avoid this, we transform the data.\n\n\nIn Deep Learning, transormations are necessary when we're using ANNs, CNNs, or RNNs."}}