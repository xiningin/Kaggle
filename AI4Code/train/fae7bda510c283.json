{"cell_type":{"456e2083":"code","c2a77842":"code","fba6afcd":"code","231b9384":"code","af144b09":"code","3c448a02":"code","9733cb1a":"code","47ded2d2":"code","5af4e84b":"code","59f5ae15":"code","de6f28cd":"code","ce1a375a":"code","24e77cf5":"code","8666f2a0":"code","270bd3d4":"code","99803a87":"code","b5fc4a01":"code","5f649d51":"code","50e85480":"code","2f288c34":"code","2c251880":"code","769888c8":"code","a9bbd368":"code","a863acdd":"code","cc05419a":"code","b31b0615":"code","67f78d28":"code","cb15ff98":"code","678ab4e3":"code","da560030":"code","dcac2e17":"code","5bb30bc9":"code","521f07f7":"code","0e3c1bf3":"code","8bd66dd2":"code","fd215b17":"code","ad4e9aa5":"code","95e05148":"code","041c1c8e":"code","188facbe":"code","61aa82f7":"code","737549ec":"markdown","2da6157d":"markdown","b98ac889":"markdown","4ac01a8e":"markdown","6ed7e143":"markdown","37aec960":"markdown","98d1812c":"markdown","c4f37d57":"markdown","0110e9ab":"markdown","a205e010":"markdown","859db75b":"markdown"},"source":{"456e2083":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Visualization\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\n%matplotlib inline\nimport warnings\nwarnings.filterwarnings('ignore')\nsns.set_style(\"darkgrid\")","c2a77842":"train_data = pd.read_csv('..\/input\/train.csv')\ntest_data = pd.read_csv('..\/input\/test.csv')","fba6afcd":"train_data.head()","231b9384":"print(train_data.target.value_counts())","af144b09":"print(train_data.shape, test_data.shape)","3c448a02":"train_data.describe()","9733cb1a":"# Lets see the std distribution of the data\nsns.distplot(train_data[train_data.columns[2:]].std(), bins=30)\nplt.title('Stds distribution of all columns');","47ded2d2":"# Lets see the mean distribution of the data\nsns.distplot(train_data[train_data.columns[2:]].mean(), bins=30)\nplt.title('Mean distribution of all columns');","5af4e84b":"# Check wether we have missing values\ntrain_data.isnull().any().any()","59f5ae15":"test_data.isnull().any().any()","de6f28cd":"sns.set(rc={'figure.figsize':(10,7)})\ncolours = [\"goldenrod\",\"purple\",\"darkgreen\",\"maroon\",\"aqua\",\"olive\",\"coral\",\"darkorchid\",\"darkviolet\",\"saddlebrown\"]\nindex = -1\nfor i in train_data.columns[2:12]:\n    index = index + 1\n    fig = sns.kdeplot(train_data[i] , shade=True, color=colours[index])\nplt.xlabel(\"Features\")\nplt.ylabel(\"Density\")\nplt.title(\"Feature Distribution\")\nplt.grid(True)\nplt.show(fig)","ce1a375a":"sns.set(rc={'figure.figsize':(10,7)})\ncolours = [\"goldenrod\",\"purple\",\"darkgreen\",\"maroon\",\"aqua\",\"olive\",\"coral\",\"darkorchid\",\"darkviolet\",\"saddlebrown\"]\nindex = -1\nfor i in train_data.columns[12:22]:\n    index = index + 1\n    fig = sns.kdeplot(train_data[i] , shade=True, color=colours[index])\nplt.xlabel(\"Features\")\nplt.ylabel(\"Density\")\nplt.title(\"Feature Distribution\")\nplt.grid(True)\nplt.show(fig)","24e77cf5":"sns.set(rc={'figure.figsize':(10,7)})\ncolours = [\"goldenrod\",\"purple\",\"darkgreen\",\"maroon\",\"aqua\",\"olive\",\"coral\",\"darkorchid\",\"darkviolet\",\"saddlebrown\"]\nindex = -1\nfor i in train_data.columns[22:32]:\n    index = index + 1\n    fig = sns.kdeplot(train_data[i] , shade=True, color=colours[index])\nplt.xlabel(\"Features\")\nplt.ylabel(\"Density\")\nplt.title(\"Feature Distribution\")\nplt.grid(True)\nplt.show(fig)","8666f2a0":"sns.set(rc={'figure.figsize':(10,7)})\ncolours = [\"goldenrod\",\"purple\",\"darkgreen\",\"maroon\",\"aqua\",\"olive\",\"coral\",\"darkorchid\",\"darkviolet\",\"saddlebrown\"]\nindex = -1\nfor i in train_data.columns[32:42]:\n    index = index + 1\n    fig = sns.kdeplot(train_data[i] , shade=True, color=colours[index])\nplt.xlabel(\"Features\")\nplt.ylabel(\"Density\")\nplt.title(\"Feature Distribution\")\nplt.grid(True)\nplt.show(fig)","270bd3d4":"sns.set(rc={'figure.figsize':(10,7)})\ncolours = [\"goldenrod\",\"purple\",\"darkgreen\",\"maroon\",\"aqua\",\"olive\",\"coral\",\"darkorchid\",\"darkviolet\",\"saddlebrown\"]\nindex = -1\nfor i in train_data.columns[42:52]:\n    index = index + 1\n    fig = sns.kdeplot(train_data[i] , shade=True, color=colours[index])\nplt.xlabel(\"Features\")\nplt.ylabel(\"Density\")\nplt.title(\"Feature Distribution\")\nplt.grid(True)\nplt.show(fig)","99803a87":"sns.jointplot(data=train_data, x='var_0', y='var_1', kind='hex')","b5fc4a01":"print('Distributions of second 20 columns after the first 50')\nplt.figure(figsize=(28, 26))\nfor i, col in enumerate(list(train_data.columns)[52:72]):\n    plt.subplot(5, 4, i + 1)\n    sns.distplot(train_data[col])\n    plt.title(col)","5f649d51":"# Since is a binary classifacation, lets check for balance in the train dataset\ntrain_data['target'].value_counts(normalize=True)","50e85480":"# Check for correlations\ndata_cor = train_data.corr().abs().unstack().sort_values(kind=\"quicksort\").reset_index()\ndata_cor = data_cor[data_cor['level_0'] != data_cor['level_1']]","2f288c34":"data_cor.head(10)","2c251880":"data_cor.tail(10)","769888c8":"trian_X = train_data.drop(['ID_code', 'target'], axis = 1)\ntrain_y = train_data['target']","a9bbd368":"print (trian_X.shape, train_y.shape)","a863acdd":"test_X = test_data.drop(['ID_code'], axis = 1)\nid_test = test_data['ID_code']","cc05419a":"print (test_X.shape, id_test.shape)","b31b0615":"# Feature Scaling\nfrom sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\ntrian_X = sc.fit_transform(trian_X)\ntest_X = sc.fit_transform(test_X)","67f78d28":"# Splitting the data into training set and test set\nfrom sklearn.model_selection import train_test_split\nX_train, dev_X, Y_train, dev_Y = train_test_split(trian_X, train_y, test_size=0.30, random_state=101)\n","cb15ff98":"# import the Keras libraries and packages\nimport tensorflow as tf\nfrom keras.models import Sequential\nfrom keras.layers import Dense, BatchNormalization\nfrom keras.optimizers import Adam\nfrom keras import regularizers\nfrom keras.callbacks import EarlyStopping, ReduceLROnPlateau\nfrom sklearn.metrics import roc_auc_score","678ab4e3":"# Initialize the ANN\nmodel = Sequential()","da560030":"model.add(Dense(64, input_dim=X_train.shape[1] , activation='relu',kernel_regularizer=regularizers.l1_l2(0.001)))\nmodel.add(Dense(64, activation='relu'))\nmodel.add(BatchNormalization())\nmodel.add(Dense(128, activation='relu'))\nmodel.add(BatchNormalization())\nmodel.add(Dense(196, activation='relu'))\nmodel.add(BatchNormalization())\nmodel.add(Dense(32, activation='relu'))\nmodel.add(BatchNormalization())\nmodel.add(Dense(16, activation='relu'))\nmodel.add(Dense(1, activation='sigmoid'))","dcac2e17":"def auroc(dev_Y, y_score):\n    return tf.py_func(roc_auc_score, (dev_Y, y_score), tf.double)","5bb30bc9":"metrics_list = ['accuracy', auroc]","521f07f7":"model.compile(loss='binary_crossentropy', optimizer=Adam(lr = 0.01, decay=0.01\/50), metrics=metrics_list)","0e3c1bf3":"model.summary()","8bd66dd2":"# define learning rate schedule\nrlrp = ReduceLROnPlateau(monitor='val_acc', factor=0.1, epsilon=0.0001, patience=5, verbose=1)\n\n# patient early stopping\n# stop when the validation loss has not improved for 10 training epochs.\nes = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=10)\n\ncallbacks_list = [rlrp,es]","fd215b17":"# fit model\nhistory = model.fit(X_train, Y_train, batch_size = 25800, epochs=50, validation_data=(dev_X, dev_Y), callbacks=callbacks_list)","ad4e9aa5":"# Visualise report to check for overfitting\nfrom pylab import rcParams\nrcParams['figure.figsize'] = 10, 4\nplt.plot(history.history['acc'])\nplt.plot(history.history['val_acc'])\nplt.title('model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()\n# summarize history for loss\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()","95e05148":"# Predicting the test set results\ny_score = model.predict_proba(dev_X)\ny_pred = model.predict(test_X)\n# y_pred = np.argmax(y_pred, axis = 1) ","041c1c8e":"threshold = 0.5\n# y_pred_ = (y_pred > threshold)\ny_pred = (y_pred > threshold).astype(int)","188facbe":"# calculate AUC\nauc = roc_auc_score(dev_Y, y_score)\nprint('AUC: %.2f' % auc)","61aa82f7":"pd.DataFrame({\"ID_code\":id_test,\"target\":y_pred[:,0]}).to_csv('Customer_Transaction.csv',\n                                                                                     index=False,header=True)","737549ec":"#### Plot the first 50 visualizations for feature distribution in space.","2da6157d":"#### Pre-processing and data preparation to feed Network.","b98ac889":"# Part 3. Building machine learning model","4ac01a8e":"###The  function auroc prints the roc auc score  as part of the metric to judge the performance of your model.","6ed7e143":"# Understanding the dataset","37aec960":"# Submission","98d1812c":"# Part 2. Data Preprocessing","c4f37d57":"Our solution is successfully submitted.","0110e9ab":"### Joint distrubutions of some variable","a205e010":"### How training and validation tries to mimic each other on Training loss and accuracy  pass epoch 7 and stagnation of the model past epoch 10 shows that the model didnt suffer overfitting. \n","859db75b":"### All features have a low correlation with target, hence no dealing with highly correlated features."}}