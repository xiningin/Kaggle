{"cell_type":{"82725662":"code","52fbf195":"code","61400f0b":"code","1550ea81":"code","970dc887":"code","bd3eb47e":"code","cf4171cb":"code","ad654cca":"code","ab89fcaa":"code","2261ba89":"code","ed01120e":"code","38fdd2a9":"code","db903e3d":"markdown","e904fa46":"markdown","6c0eeb70":"markdown","324eb386":"markdown"},"source":{"82725662":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt","52fbf195":"student_data = pd.read_csv(\"..\/input\/students-performance-in-exams\/StudentsPerformance.csv\")","61400f0b":"# Preparing the dataset, including:\n## 1. averaging the three scores since they are highly correlated\n## 2. convert parent education to numerical (low to high)\nstudent_data['avg_score'] = (student_data['math score']+student_data['reading score']+student_data['writing score'])\/3\nmapping_edu = {\"some high school\":0, \"high school\":1, \"some college\":2, \"associate's degree\":3, \\\n               \"bachelor's degree\":4, \"master's degree\":5} \nstudent_data['edu_parent'] = student_data['parental level of education'].map(lambda x: mapping_edu[x])\nstudent_data.head()","1550ea81":"ds = pd.get_dummies(student_data,\n                    columns=['gender','race\/ethnicity','test preparation course','lunch'], drop_first=True)\ny = ds['avg_score']\nds.drop(columns=[\"parental level of education\", 'math score','reading score','writing score','avg_score'],inplace=True)\nn_cols = len(ds.columns)\nn_cols","970dc887":"y.hist()","bd3eb47e":"from sklearn import model_selection, linear_model, metrics","cf4171cb":"n_it = 5\n# similar with n_it=10\/20, use 5 to save time\nkf = model_selection.KFold(n_splits=n_it)\nclf = linear_model.Ridge()\n#Note: results are similar with parameter selection (RidgeCV). \nprint(clf)","ad654cca":"accs = np.zeros(n_it)\nlabels_pred = np.zeros(len(y))\ncoefs = []\nfor it, (tr,te) in enumerate(kf.split(ds, y )):\n    clf.fit(ds.iloc[tr], y.iloc[tr])\n    y_true = y.iloc[te]\n    y_pred = clf.predict(ds.iloc[te])\n    accs[it] = metrics.r2_score(y_true=y_true, y_pred=y_pred)\n    print('Iter#%d acc=%f, alpha=%f' % (it,accs[it],clf.alpha))\n    coefs.append(clf.coef_)\n    labels_pred[te] = y_pred\nprint('Acc: %f+\/-%f' % (np.mean(accs),np.std(accs)))\ncoefs = np.asarray(coefs)","ab89fcaa":"## 10k permutatiaon test to obtain the significancy of the accuracy (r square) and the coefficients.\nn_perm = 10000\nacc_rand = np.zeros(n_perm)\ncoefs_rand = np.zeros((n_perm,kf.n_splits,n_cols))\nfor iperm in range(n_perm):\n    y_rand = np.random.permutation(y)\n    accs_tmp = np.zeros(n_it)\n    for it, (tr,te) in enumerate(kf.split(ds, y)):\n        clf.fit(ds.iloc[tr], y_rand[tr])\n        y_true = y_rand[te]\n        y_pred = clf.predict(ds.iloc[te])\n        accs_tmp[it] = metrics.r2_score(y_true=y_true, y_pred=y_pred)\n        coefs_rand[iperm, it] = clf.coef_\n    acc_rand[iperm] = np.mean(accs_tmp)\n    if np.mod(iperm,1000)==0:\n        print('#%d Acc: %f+\/-%f' % (iperm, np.mean(accs_tmp),np.std(accs_tmp)))\ncoefs_rand = np.nanmean(coefs_rand,axis=1)\ncoefs_rand.shape","2261ba89":"realAcc = np.mean(accs)\nplt.hist(acc_rand,50)\nplt.plot(realAcc,10,'rd')\nplt.show()\nprint('actual acc=%f, p=%f',realAcc, np.sum(acc_rand>realAcc)\/(n_perm*1.))","ed01120e":"print('r2=%f' % metrics.r2_score(y, labels_pred))\nplt.plot(y, labels_pred, 'ro')\n#plt.xlim([0,101])plt.ylim([0,101])\nplt.ylabel('predicted')\nplt.xlabel('true')\nplt.show()","38fdd2a9":"# avg coef values across cv:\ncoefs_avg = np.nanmean(coefs,axis=0)\nplt.barh(range(n_cols),coefs_avg,color='r')\nplt.barh(range(n_cols),np.mean(coefs_rand,axis=0),xerr=np.var(coefs_rand,axis=0),color='k',capsize=6)\nplt.yticks(range(n_cols),ds.columns)\nplt.axvline(x=0,color='k')\nplt.show()\nfor ic,col in enumerate(ds.columns):\n    print('#%d %s coef=%f, p=%f' % (ic,col,coefs_avg[ic], np.sum(abs(coefs_avg[ic]) < coefs_rand[:,ic]) \/(n_perm*1.)))","db903e3d":"## Aim: Predicting student performance based on demographic\/socioeconomic information\n\n## Methods: Ridge regression with cross-validation\n* Predicted variable: average score consisted of math, reading, and writing.\n* Predictors: gender, parent education level (low to high), lunch type (standard or not), race\/ethnicity (5 groups), whether having taken preparetion course\n*Given the demographic\/socioeconomic variables are likely correlated with each other, we used regresion with regularization.\n\n## Results: \n* Prediction accuracy, measured by r-squared score, is signficantly better than chance (p<10e-4)\n* Several predictors are signifcant (see Result summary)\n\n## Conclusion:\n* Better performance is related to (order by effect size): having standard lunch (instead of free\/reduced), having taken the preparation course, being in the race\/ethnicity group D and E, being female, and parents with higher education level.\n** The amount of explained variance is relatively low desite bing significant. The biggerst limitation is the small amount of info provided by the dataset, nonetheless the model prediction is remarkably signficant. More variables (other information) will likely improve the amount the explained variance.","e904fa46":"# Result summary\n## Significant positive variables (being in the category (or higher value), HIGHER test score)\n1. parent education level\n2. being race\/ethinicity D and E\n3. standard lunch\n\n## Significant negative variables (****being in the category, LOWER test score)\n1. being male \n2. not taken test preparation course","6c0eeb70":"# Run regression analysis with cross-validataion\n* Using regularization given colinearity in the dataset (the variables are correlated)\n* Ridge regression is implemented here. \n    * Lasso and elastic net gave similar results\n* Estimating significance of the prediciton result and regression coefficient with 10K permutation","324eb386":"# Construt dataset:\n## 1. Define predicted variables: average score of the three subjects\n## 2. Defind predictors:\n* Convert \"parental level of education\" to continuous\n* Dummy coding categorical variables\n## 3. Drop unnecessary columns"}}