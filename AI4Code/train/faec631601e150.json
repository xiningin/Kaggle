{"cell_type":{"0b7575c6":"code","ac12dd6d":"code","c4bc8efb":"code","171b0256":"code","fad89a9d":"code","9a18b9a7":"code","be586170":"code","4683c3e1":"code","f5fdfd6e":"code","c8db863d":"code","0d486b67":"code","d5d3a278":"code","720a0e3a":"code","2d6a6dcb":"code","23066649":"code","c768bc63":"code","95899122":"markdown","aa42bedc":"markdown","ff320d1b":"markdown","6faa73ee":"markdown","7925e586":"markdown","ef613f9d":"markdown","bb07e65e":"markdown","99ef90db":"markdown","764fdec3":"markdown","a6dcdbd3":"markdown","d1bf08fd":"markdown","64323086":"markdown","71dd9178":"markdown","85e407c8":"markdown","d942e27c":"markdown","370c632d":"markdown","75fdfb38":"markdown","a3b63017":"markdown","881782bd":"markdown","deaf39a2":"markdown","046e87fa":"markdown","7add874e":"markdown","9a0f82d8":"markdown"},"source":{"0b7575c6":"\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom IPython.display import clear_output\nfrom six.moves import urllib\nimport tensorflow.compat.v2.feature_column as fc\nimport tensorflow as tf\nimport os\nimport sys\npd.options.mode.chained_assignment = None\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\nfor dirname, _, filenames in os.walk(\"\/content\"):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\ntrain_data = pd.read_csv(\"\/kaggle\/input\/titanic\/train.csv\").copy()\n\ntest_data = pd.read_csv(\"\/kaggle\/input\/titanic\/test.csv\").copy()\ndisplay(train_data,test_data)","ac12dd6d":"y = train_data[\"Survived\"]\nVariable_set=train_data[[\"Pclass\",\"Sex\",\"Age\",\"SibSp\",\"Parch\",\"Fare\",\"Embarked\",\"Name\"]]\ntest_data=test_data[[\"Pclass\",\"Sex\",\"Age\",\"SibSp\",\"Parch\",\"Fare\",\"Embarked\",\"Name\"]]\n\n\navarageA=np.mean(Variable_set[\"Age\"])\nVariable_set[\"Age\"][np.isnan(Variable_set[\"Age\"])]=avarageA\navarageB=np.mean(test_data[\"Age\"])\ntest_data[\"Age\"][np.isnan(test_data[\"Age\"])]=avarageB\n\nVariable_set[\"Embarked\"]=Variable_set[\"Embarked\"].replace(np.nan, 'N', regex=True)\ntest_data[\"Embarked\"]=test_data[\"Embarked\"].replace(np.nan, 'N', regex=True)","c4bc8efb":"Train=Variable_set[0:771]\ny_train=y[0:771]\n\nCross_validation=Variable_set[771:891]\ny_croosV=y[771:891]","171b0256":"all_datasets=[Train,Cross_validation,test_data,Variable_set]\n\nfor dataset in all_datasets:\n      dataset[\"FamilySize\"]=dataset[\"Parch\"]+dataset[\"SibSp\"]  \nfor dataset in all_datasets:\n          dataset['IsAlone'] = 0\n          dataset.loc[dataset['FamilySize'] == 1, 'IsAlone'] = 1\n#Labeling people\nfor dataset in all_datasets:\n    dataset['Title'] = dataset.Name.str.extract(' ([A-Za-z]+)\\.', expand=False)","fad89a9d":"CATEGORICAL_COLUMNS = ['Sex',\"Embarked\",\"Title\"]\nNUMERIC_COLUMNS = [\"Fare\",\"Parch\",\"SibSp\",\"Pclass\",'Age',\"FamilySize\",\"IsAlone\"]\n\nfeature_columns = []\nfor feature_name in CATEGORICAL_COLUMNS:\n  vocabulary = Train[feature_name].unique()\n  feature_columns.append(tf.feature_column.categorical_column_with_vocabulary_list(feature_name, vocabulary))\n\nfor feature_name in NUMERIC_COLUMNS:\n  feature_columns.append(tf.feature_column.numeric_column(feature_name, dtype=tf.float32))","9a18b9a7":"def make_input_fn(data_df, label_df, num_epochs=300, shuffle=True, batch_size=32):\n  def input_function():\n    ds = tf.data.Dataset.from_tensor_slices((dict(data_df), label_df))\n    if shuffle:\n        ds = ds.shuffle(1000)\n    ds = ds.batch(batch_size).repeat(num_epochs)\n    return ds\n  return input_function\ntrain_input_fn = make_input_fn(Train,y_train)\neval_input_fn = make_input_fn(Cross_validation,y_croosV, num_epochs=1, shuffle=False)\n","be586170":"def predict_fn(data_df,num_epochs=300, shuffle=False, batch_size=32):\n  def input_function():\n    ds = tf.data.Dataset.from_tensor_slices((dict(data_df)))\n    if shuffle:\n      ds = ds.shuffle(0)\n    ds = ds.batch(batch_size).repeat(num_epochs)\n    return ds\n  return input_function\n\ntest_inpt_fn=predict_fn(test_data,num_epochs=1, shuffle=False)","4683c3e1":"AxP=tf.feature_column.crossed_column(['Age', 'Pclass',], hash_bucket_size=100)\nPxS=tf.feature_column.crossed_column(['Pclass', 'Sex',], hash_bucket_size=100)\nage_x_gender = tf.feature_column.crossed_column(['Age', 'Sex',], hash_bucket_size=100)\n\nderived_feature_columns = [age_x_gender,PxS,AxP]","f5fdfd6e":"linear_est = tf.estimator.LinearClassifier(feature_columns=feature_columns+derived_feature_columns, optimizer=tf.keras.optimizers.Ftrl(learning_rate=0.1,l1_regularization_strength=0))\n\nlinear_est.train(train_input_fn)\nresult = linear_est.evaluate(eval_input_fn)\nprint(result)\n","c8db863d":"regularization_paramaters=[0,1,10,15,20,25,50,75,100,125,150]\nmN=[]\n\nfor a in regularization_paramaters:\n  linear_est = tf.estimator.LinearClassifier(feature_columns=feature_columns+derived_feature_columns, optimizer=tf.keras.optimizers.Ftrl(learning_rate=0.1,l1_regularization_strength=a))\n\n  linear_est.train(train_input_fn)\n  result = linear_est.evaluate(eval_input_fn)\n  mN.append(result[\"accuracy\"])\n\nfor a in range(0,len(regularization_paramaters)): \n  print(\"accuracy:{} ,lambda:{}\".format(mN[a],regularization_paramaters[a]))\nplt.plot(regularization_paramaters,mN)\nplt.xlabel('regularization_paramater(\u03bb)')\nplt.ylabel('accuracy')\nplt.title('accuracy-\u03bb graph')\nplt.show()\n\n","0d486b67":"alphas=[0.01,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1,1.1,1.2]\nmN=[]\n\nfor a in alphas:\n  linear_est = tf.estimator.LinearClassifier(feature_columns=feature_columns+derived_feature_columns, optimizer=tf.keras.optimizers.Ftrl(learning_rate=a,l1_regularization_strength=10))\n\n  linear_est.train(train_input_fn)\n  result = linear_est.evaluate(eval_input_fn)\n  mN.append(result[\"accuracy\"])\n\nfor a in range(0,len(alphas)): \n  print(\"accuracy:{} ,alpha:{}\".format(mN[a],alphas[a]))\nplt.plot(alphas,mN)\nplt.xlabel('learning rate\/alpha(\u03b1)')\nplt.ylabel('accuracy')\nplt.title('accuracy- learning rate(\u03b1) graph')\nplt.show()","d5d3a278":"regularization_paramaters=[0,1,10,15,20,25,50,75]\nalphas=[0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1,1.1,1.2]\nmN=[]\n\nfor a in alphas:\n    for b in regularization_paramaters:\n    linear_est = tf.estimator.LinearClassifier(feature_columns=feature_columns+derived_feature_columns, optimizer=tf.keras.optimizers.Ftrl(learning_rate=a,l1_regularization_strength=b))\n\n    linear_est.train(train_input_fn)\n    result = linear_est.evaluate(eval_input_fn)\n    mN.append([a,b,result[\"accuracy\"]])\n\nfor a in range(0,len(mN)) :\n    print(\"alpha:{} ,reg-paramater:{},accuracy:{}\".format(mN[a][0],mN[a][1],mN[a][2]))","720a0e3a":"epochs=[50,100,200,250,275,300,350,400,500,600,700,1000]\nresult_list=[]\nfor epoch in epochs:\n    train_input_fn = make_input_fn(Train,y_train,num_epochs=epoch)\n    eval_input_fn = make_input_fn(Cross_validation,y_croosV,num_epochs=epoch,shuffle=False)\n    linear_est = tf.estimator.LinearClassifier(feature_columns=feature_columns+derived_feature_columns, optimizer=tf.keras.optimizers.Ftrl(learning_rate=0.3,l1_regularization_strength=20))\n    linear_est.train(train_input_fn)\n    result = linear_est.evaluate(eval_input_fn)\n    result_list.append(result[\"accuracy\"])\n\nfor a in range(0,len(epochs)):\n      print(\"epoch:{},accuracy:{}\".format(epochs[a],result_list[a]))\n\n\nplt.plot(epochs,result_list)\nplt.xlabel('epoch')\nplt.ylabel('accuracy')\nplt.title('accuracy-epoch graph')\nplt.show()\n","2d6a6dcb":"linear_est = tf.estimator.LinearClassifier(feature_columns=feature_columns+derived_feature_columns, optimizer=tf.keras.optimizers.Ftrl(learning_rate=0.3,l1_regularization_strength=20))\n\n\ntrain_input_fn = make_input_fn(Train,y_train,num_epochs=250)\neval_input_fn = make_input_fn(Cross_validation,y_croosV, num_epochs=250, shuffle=False)\n\n\n\nlinear_est.train(train_input_fn)\nresult1 = linear_est.evaluate(eval_input_fn)\nprint(\"Result-With just Test set:{}\".format(result1[\"accuracy\"]))\n\n\nlinear_est = tf.estimator.LinearClassifier(feature_columns=feature_columns+derived_feature_columns, optimizer=tf.keras.optimizers.Ftrl(learning_rate=0.3,l1_regularization_strength=20))\n\n\ntrain_input_fn2 = make_input_fn(Variable_set,y,num_epochs=250)\neval_input_fn2 = make_input_fn(Cross_validation,y_croosV, num_epochs=250, shuffle=False)\n\n\n\nlinear_est.train(train_input_fn2)\nresult2 = linear_est.evaluate(eval_input_fn2)\nprint(\"Result-With all give variable set:{}\".format(result2[\"accuracy\"]))","23066649":"linear_est = tf.estimator.LinearClassifier(feature_columns=feature_columns+derived_feature_columns, optimizer=tf.keras.optimizers.Ftrl(learning_rate=0.3,l1_regularization_strength=20))\n\ntrain_input_fn = make_input_fn(Train,y_train,num_epochs=250)\neval_input_fn = make_input_fn(Cross_validation,y_croosV, num_epochs=250, shuffle=False)\n\nlinear_est.train(train_input_fn)\nprint(linear_est.evaluate(eval_input_fn))\n\n\npredlist=[]\n\npred_dicts = list(linear_est.predict(test_inpt_fn))\nprobs = pd.Series([pred['probabilities'][1] for pred in pred_dicts])\n\nfor a in probs:\n \n    if a>0.5:\n        predlist.append(1)\n    else:\n        predlist.append(0)\n\noutput = pd.DataFrame({'PassengerId':(pd.read_csv(\"\/kaggle\/input\/titanic\/test.csv\").PassengerId), 'Survived': predlist})\noutput.to_csv('Predictions.csv',index=False) \nprint(output)\n","c768bc63":"train_input_fn = make_input_fn(Train,y_train)\n\n\nlinear_est = tf.estimator.LinearClassifier(feature_columns=feature_columns+derived_feature_columns, optimizer=tf.keras.optimizers.Ftrl(learning_rate=0.25,l1_regularization_strength=15))\n\nlinear_est.train(train_input_fn)\n\n\neval_input_fn = make_input_fn(Cross_validation,y_croosV, num_epochs=1, shuffle=False)\npred_dicts = list(linear_est.predict(eval_input_fn))\nprobs = pd.Series([pred['probabilities'][1] for pred in pred_dicts])\nparameter=[0.01,0.1,0.2,0.22,0.24,0.26,0.28,0.29,0.3,0.32,0.4,0.5,0.6,0.62,0.63,0.64,0.65,0.67,0.685,0.7,0.8,0.9]\nfor m in parameter:\n    predlist1=[]\n    for a in probs:\n  \n      if a>m:\n        predlist1.append(1)\n      \n      else:\n        predlist1.append(0)\n\n    Correct=0\n    for a in range(0,len(predlist1)):\n        if predlist1[a]==y_croosV[a+771]:\n            Correct+=1\n    accuracy=Correct\/len(y_croosV)\n    print(\"paramater={},accuracy={}\".format(m,accuracy))\n\n\n","95899122":"# Arranging epoch value\n**\"Epoch\" is  the number of times that we give our data to our Classifier to train.Similarly,if we give our data so many times it can create a model that overfits,if we give our data less than necessary it creates a model that underfits.(we are not going to train for epoch,learning rate,reg-parameter together because it would be  computationally very expensive)**","aa42bedc":"# Configuration of Data","ff320d1b":"# Creating derived feature columns\n**Our LinearClassifier doesn't look for combinations of columns. It just looks for correlations between each single feature and surviving results.**\n**With a mathematical analogy, it can be more clear**\n\n\n**Before we create derived_feature_columns our function is:**\n\n**(x1)*Q+(x2)W...x(n)*Z**\n\n**After we create derived_feature_columns our function becomes:**\n\n**(x1^b)*Q+(x2^v)*W...(xn^l)*Z+(x1^b)*(x2^v)*H**\n\n**(However this analogy doesn't precisely show the direct result of the operation, it has shown just to give the intuition about what derived features stands for.)**\n\n\n**After a couple of tries, i have found these combinations are the most useful ones.**\n\n**--Keep in mind adding a new derived_feature may not always improve your accuracy, it may even reduce it.--**","6faa73ee":"# Dividing the data","7925e586":"**As we see, the values in [200,300] are our optimum ones. Let's take 250 as our epoch**\n\n**So our best parameters  alpha:0.3, regparamater:20, epoch:250=**\n**We have set our train_input_fn as Train without using Cross_validation to find best reg-parameter, learning rate and epoch. However, after finding these we can train with all Variable set and just with Train set separately to see the difference.**\n\n**(Remember we have divided our data into 2 separate sets as:**\n\n**Train=Variable_set[0:771]**\n\n**y_train=y[0:771]**\n\n**Cross_validation=Variable_set[771:891]**\n\n**y_croosV=y[771:891]**","ef613f9d":"# Preparing required structure for LinearClassifier\n\n\n**In order to train our LinearClassifier,we need to introduce our columns to our LinearClassifier according to their datatype.The columns that have string data type will be assigned as CATEGORICAL,and the columns that have float data type will be assigned as NUMERIC  (One of the advantages of tensorflow is this.We dont have to explicitly hard-code our string values to convert them into numeric ones)**","bb07e65e":"# -Extras","99ef90db":"**We also will create an another input function for predicting our test set.This function is almost same with our make_input_fn,the only difference is  predict_fn does not take labels(since our test set doesn't have y-labels)**","764fdec3":"# Data Load","a6dcdbd3":"# Trying different thresholds\n\n**Optionally we can try different treshold on eval_input_fn(cross_validation)**","d1bf08fd":"# Feature engineering\n**In order to have better results we need to derive new features from the ones that already exist.** \n\n**1.We will create FamilySize by using Parch and SibSp**\n**2.We will create \"IsAlone\" column**\n**3.We will create \"Title\" column by using Name column.Title column is basically is a column which includes titles like \"Mr.\",\"Mrs.\"...**\n\n**Most of these features are very well explained by following notebook(which i have learned lot):**\n**https:\/\/www.kaggle.com\/startupsci\/titanic-data-science-solutions**","64323086":"**As we see we achieved 'accuracy':0.8666667 rate on our cross-validation set. Now let's think about how we can improve it.**","71dd9178":"**But we should also check results for different learning-rates(alpha).Because with big alphas our classifier might overshoot the local minima**","85e407c8":"\n**As we see there are 4 combinations which correspond to maximum accuracy**\n\n**We can take one of the combinations and plug this combination into our classifier.I have chosen this as \n---------->alpha:0.3 regparamater:20,accuracy:0.8999999761581421**","d942e27c":"**As we have seen when we increase the number of data on training our accuracy decreases. This means when we give more data into our set it starts to overfit. In other words, when we give more data our model is becoming  less generalized so it is becoming bad at making predictions(with a slight difference) **","370c632d":"**However,looking separately to the learning rates(alpha) and regularization parameters(lambda) can be deceptive.It's better to look for combinations of these**\n\n\n**Although these graphs are still useful to see the correlations**","75fdfb38":"**As we see our best threshold is same with our conventional threshold**","a3b63017":"**In order to make evaluations we will divide our data into 2 separate data.These are \"Train\" set and \"Cross_validation\" set.(Normally convention is dividing the data into 3 separate sets as train,cross validation,test.But \u0131 think given data is so small to do this.Therefore,to avoid from loss of data we have divided into 2)**","881782bd":"# Overfitting-Underfitting\n\n**If our model is so precise\/accurate on our train set it might be not so generalized therefore it might be bad at making predictions.This means our hypothesis is suffering from overfitting.**\n**If our model is not so precise\/accurate on our train set it might be a bad hypothesis.This means our hypothesis is suffering from underfitting.**\n\n\n**In order to modify and find the best one for our regularization  paramater(lambda),we can try \ndifferent lambdas on our classifier.(Regularization _parameter(lambda) is a parameter to avoid  overfitting.**","deaf39a2":"**Tensorflow LinearClassifiers need  input functions to work,and this is the structure of an input function.After defining our input function we need to create  our train_input_fn(by deriving from train set) and eval_input_fn(by deriving from Cross_validation set).Because our LinearClassifier is not able to take raw data as Train and Cross_validation.They must be converted into tensors.**","046e87fa":"**We need to convert our NaN(not a number) values into more plausible ones,because NaN values cause errors in our LinearClassifier**\n**Thereby,we are converting:**\n**1.-->Age NaN values into  mean of Age column** \n**2.-->Embarked NaN values into \"N\"letter.(This letter choice is totally arbitrary)**","7add874e":"**As we see lambdas(reg-parameters)=15 or 20  results correspond to best accuracy results.**","9a0f82d8":"# Making predictions\n\n**Now we can start making predictions.Convention for making predictions  is setting the treshold as 0.5.In other words;we are predicting 1 if  it has more than %50 probability,and 0 if it has less than %50 probability."}}