{"cell_type":{"4e888543":"code","b30caffb":"code","6d969caa":"code","ca5e1a3a":"code","327ae80f":"code","36b048a7":"code","93a40ca8":"code","531ecddb":"code","61bab740":"code","7b2b55da":"code","246b5ec2":"code","472d05d8":"code","339a8ec4":"code","2620892a":"code","51a09eaa":"code","fdd867c7":"code","8806c7eb":"code","3fa0f32d":"code","333b622a":"code","df398346":"code","fc53cb17":"code","1e9302db":"code","bbb6a16a":"code","d56c33df":"code","417ca8ab":"code","0b1c8798":"code","d5397c2a":"code","15b6aa9b":"code","8d114d01":"code","7165cb4f":"code","8f13f566":"code","afcb1131":"code","29f8fe80":"code","030cf77c":"code","edf8411f":"code","9e43fe5b":"code","f87579f4":"code","7d8d7a14":"code","617837d9":"code","b90de964":"code","bfa58738":"code","dd2c60fe":"code","f5bd3045":"code","ee31be5a":"code","b7b61973":"code","90f6dc17":"code","286f1d72":"code","be82c523":"code","8166e906":"code","32836016":"code","624f5eca":"code","350b64a9":"code","570b3ea3":"code","64342bb3":"code","1250eb07":"code","0b52e784":"code","66602b69":"code","a9be0fec":"code","801a4587":"code","b509ea91":"code","d0701dce":"code","adf6ac01":"code","3e4bbff4":"code","d75cc8dd":"code","6fcd6087":"markdown","58b136c9":"markdown","8f411d07":"markdown","35b1c158":"markdown","9efdbc61":"markdown","13310f2e":"markdown"},"source":{"4e888543":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","b30caffb":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\nsns.set_style('whitegrid')","6d969caa":"train = pd.read_csv('\/kaggle\/input\/titanic\/train.csv')","ca5e1a3a":"train.head()","327ae80f":"## Checking the dataset for completeness\n\ntrain.info()\n\n## Remove Cabin\n## Substitute missing Age\n## Remove column with missing Embarked","36b048a7":"## Creating the first two characters of ticket as a variable\n\ntrain['Ticket_First'] = train['Ticket'].apply(lambda x:x.replace('.','').replace('\/','').split()[0][:2])","93a40ca8":"## Checking the survival frequency across Ticket_First variable\n\nticket_freq = train[['Ticket_First','Survived']].groupby(['Ticket_First']).agg([('Nos people', 'count'), ('Nos survived', 'sum')])\nticket_freq.columns = ticket_freq.columns.get_level_values(1)\nticket_freq = ticket_freq.reset_index(level = [0])\nticket_freq['Survival %'] = round(ticket_freq['Nos survived']*100\/ticket_freq['Nos people'])\nticket_freq.sort_values(by = ['Nos people'], ascending = False)\n\n## It does seem like there are too many variables with too little observations to reliably decide survival. So, grouping the \n## Ticket_First where # observations =< 10","531ecddb":"## Grouping the minor Ticket_First as others\n\ndef Ticket_Grp(col):\n    \n    if col[0] in ticket_freq[ticket_freq['Nos people'] > 10]['Ticket_First'].to_list():\n        return col[0]\n    else:\n        return 'Others'","61bab740":"train['Ticket_Grp'] = train[['Ticket_First']].apply(Ticket_Grp, axis =1)\ntrain['Ticket_Grp'].value_counts()","7b2b55da":"## Extracting the salutation from name as a variable\n\ntrain['Salute'] = train['Name'].apply(lambda x:x.split()[1])","246b5ec2":"pd.value_counts(train['Salute']).head()","472d05d8":"## Grouping the minor salutations as others\n\ndef Salute_group(col):\n    \n    if col[0] in ['Mr.', 'Miss.', 'Mrs.', 'Master.']:\n        return col[0]\n    else:\n        return 'Others'","339a8ec4":"train['Salute_Grp'] = train[['Salute']].apply(Salute_group, axis =1)","2620892a":"sns.set_style('whitegrid')\nsns.countplot(x='Salute_Grp', data = train, hue = 'Survived')","51a09eaa":"##Missing Values\n\nsns.heatmap(train.isnull())","fdd867c7":"# Treat Age\n","8806c7eb":"\n# Option4\n#train[pd.isnull(train['Age'])]\n\nsns.boxplot (x='Sex', y='Age', data = train, hue = 'Pclass')\n\n# Age is not a huge differentiator here but going ahead to see difference in the model results\n","3fa0f32d":"\n# Calculating medians\n\nPclassXSex_med = train[['Sex','Age','Pclass']].groupby(['Sex','Pclass']).median()","333b622a":"# Defining a function to impute median (using median since the data is skewed) for each PclassXSex.\n\n## MUCH MORE EFFICIENT WAY TO WRITING FUNCTION THAN BEFORE ##\n\ndef age_PclassSex(cols):\n    age = cols[0]\n    Pclass = cols[1]\n    Sex = cols[2]\n    \n    if pd.isnull(age) == True:\n        return PclassXSex_med.loc[Sex].loc[Pclass][0]\n    else:\n        return age","df398346":"train['Age_PclXSex'] = train[['Age', 'Pclass', 'Sex']].apply(age_PclassSex, axis = 1)","fc53cb17":"# Removing the unneeded and NA-dominated columns\n\ntrain.drop(['Age', 'Cabin'], axis =1 , inplace = True)","1e9302db":"# Drop the na rows\n\ntrain.dropna(inplace = True)","bbb6a16a":"# Check if all the null values are gone\n\nsns.heatmap(pd.isnull(train))","d56c33df":"\n## Now creating dummy variables for Sex and Embarked\n\n\nSex_Dumm = pd.get_dummies(train['Sex'], drop_first = True)\nEmbarked_Dumm = pd.get_dummies(train['Embarked'], drop_first = True)\nTicket_Grp = pd.get_dummies(train['Ticket_Grp'], drop_first = True, prefix = 'Ticket')\nSalute_Group = pd.get_dummies(train['Salute_Grp'], drop_first = True)","417ca8ab":"train = pd.concat([train, Sex_Dumm, Embarked_Dumm, Ticket_Grp, Salute_Group], axis = 1)\ntrain.head()","0b1c8798":"train.info()","d5397c2a":"train.columns","15b6aa9b":"## Creating test train dataset from 'train' dataframe only as we don't have the 'y' for test.\n\nfrom sklearn.model_selection import train_test_split\n\ny = train['Survived']\n\nX_train, X_test, y_train, y_test = train_test_split(train[['Pclass', 'SibSp', 'Parch', 'Fare',\n       'Age_PclXSex', 'male', 'Q', 'S', 'Ticket_13', 'Ticket_17',\n       'Ticket_19', 'Ticket_23', 'Ticket_24', 'Ticket_25', 'Ticket_26',\n       'Ticket_28', 'Ticket_29', 'Ticket_31', 'Ticket_33', 'Ticket_34',\n       'Ticket_35', 'Ticket_36', 'Ticket_37', 'Ticket_A5', 'Ticket_CA',\n       'Ticket_Others', 'Ticket_PC', 'Ticket_SC', 'Ticket_SO', 'Ticket_ST',\n        'Miss.', 'Mr.', 'Mrs.', 'Others']], y, test_size = 0.3, random_state = 143)\n\n\n","8d114d01":"## Fitting into the base RF model\n\nfrom sklearn.ensemble import RandomForestClassifier\n\nrf = RandomForestClassifier()\n\nrf.fit(X_train,y_train)","7165cb4f":"pred = rf.predict(X_test)","8f13f566":"from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n\nprint(accuracy_score(y_test, pred))","afcb1131":"## Trying to tune the hyperparameters (the parameters which can be decided by us and not by the model)\n\n## Using Cross Validation for the same\n\nfrom sklearn.model_selection import RandomizedSearchCV\n\n\n# Define the parameter sets\n# int(x) for x in ... converts the array into list. \n\nn_estimators = [int(x) for x in np.arange(200, 2200, 200)]\nmax_features = ['auto','sqrt']\nmax_depth = [int(x) for x in np.arange(10,110,10)]\nmax_depth.append(None)\nmin_samples_leaf = [1,2,3,4,5]\nmin_samples_split = [2,4,6,8,10]\nbootstrap = [True,False]\n\nparam_grid = {'n_estimators' : n_estimators,\n              'max_features' : max_features,\n              'max_depth' : max_depth,\n              'min_samples_leaf' : min_samples_leaf,\n              'min_samples_split' : min_samples_split,\n              'bootstrap' : bootstrap}\n\n\nrf1 = RandomForestClassifier()\nrf1_random = RandomizedSearchCV(rf1, param_grid, n_iter = 100, cv = 3, verbose =2 , random_state = 143, n_jobs = -1)\n","29f8fe80":"rf1_random.fit(X_train, y_train)","030cf77c":"rf1_random.best_params_","edf8411f":"rf1_random.best_estimator_","9e43fe5b":"pred2 = rf1_random.best_estimator_.predict(X_test)","f87579f4":"print(accuracy_score(y_test, pred2))","7d8d7a14":"### No change in accuracy after trying to tune hyperparamters (-_-). ","617837d9":"## Prepare the test dataset in the same way\n\ntest = pd.read_csv('\/kaggle\/input\/titanic\/test.csv')","b90de964":"test.info()","bfa58738":"## Creating the first two characters of ticket as a variable\n\ntest['Ticket_First'] = test['Ticket'].apply(lambda x:x.replace('.','').replace('\/','').split()[0][:2])","dd2c60fe":"## Grouping the minor Ticket_First as others\n\ndef Ticket_Grp2(col):\n    \n    if col[0] in ['A5', 'PC', 'ST', '11', '37', '33', '17', '34', '23',\n       '35', '24', '26', '19', 'CA', 'SC', '31', '29', '36', 'SO', '25',\n       '28', '13']:\n        return col[0]\n    else:\n        return 'Others'","f5bd3045":"test['Ticket_Grp'] = test[['Ticket_First']].apply(Ticket_Grp2, axis =1)","ee31be5a":"test['Salute'] = test['Name'].apply(lambda x:x.split()[1])","b7b61973":"test['Salute'] = test['Name'].apply(lambda x:x.split()[1])\ndef Salute_group(col):\n    \n    if col[0] in ['Mr.', 'Miss.', 'Mrs.', 'Master.']:\n        return col[0]\n    else:\n        return 'Others'","90f6dc17":"test['Salute_Grp'] = test[['Salute']].apply(Salute_group, axis =1)","286f1d72":"\n# Calculating medians\n\nPclassXSex_med = test[['Sex','Age','Pclass']].groupby(['Sex','Pclass']).median()\nPclassXSex_med","be82c523":"test['Age_PclXSex'] = test[['Age', 'Pclass', 'Sex']].apply(age_PclassSex, axis = 1)","8166e906":"# Removing the unneeded and NA-dominated columns\n\ntest.drop(['Cabin', 'Age'], axis =1 , inplace = True)","32836016":"# Substituting the missing value of fare using mean fare of that \"Passenger_class X Sex X Embarked\" group\n\ntest[pd.isnull(test['Fare'])]","624f5eca":"Fare_med = test[['Pclass','Fare','Sex', 'Embarked']].groupby(['Pclass','Sex', 'Embarked']).agg(['count', 'mean'])\n\nFare_med","350b64a9":"test['Fare'].fillna(12.718, inplace = True)","570b3ea3":"test.info()","64342bb3":"## Now creating dummy variables for Sex and Embarked\n\n\nSex_Dumm = pd.get_dummies(test['Sex'], drop_first = True)\nEmbarked_Dumm = pd.get_dummies(test['Embarked'], drop_first = True)\nTicket_Grp = pd.get_dummies(test['Ticket_Grp'], drop_first = True, prefix = 'Ticket')\nSalute_Group = pd.get_dummies(test['Salute_Grp'], drop_first = True)","1250eb07":"test = pd.concat([test, Sex_Dumm, Embarked_Dumm, Ticket_Grp, Salute_Group], axis = 1)\ntest.head()","0b52e784":"# Now using all the train dataset to fit the model and then predicting the test data\n\nX = train[['Pclass', 'SibSp', 'Parch', 'Fare',\n       'Age_PclXSex', 'male', 'Q', 'S',  'Ticket_13', 'Ticket_17',\n       'Ticket_19', 'Ticket_23', 'Ticket_24', 'Ticket_25', 'Ticket_26',\n       'Ticket_28', 'Ticket_29', 'Ticket_31', 'Ticket_33', 'Ticket_34',\n       'Ticket_35', 'Ticket_36', 'Ticket_37', 'Ticket_A5', 'Ticket_CA',\n       'Ticket_Others', 'Ticket_PC', 'Ticket_SC', 'Ticket_SO', 'Ticket_ST',\n        'Miss.', 'Mr.', 'Mrs.', 'Others']]\ny = train['Survived']","66602b69":"## Fit the model with the best set of parameters\n\nrf_fin = rf1_random.best_estimator_\n\nrf_fin.fit(X,y)","a9be0fec":"test.set_index('PassengerId', inplace = True)","801a4587":"test_fin =test[['Pclass', 'SibSp', 'Parch', 'Fare',\n       'Age_PclXSex', 'male', 'Q', 'S',  'Ticket_13', 'Ticket_17',\n       'Ticket_19', 'Ticket_23', 'Ticket_24', 'Ticket_25', 'Ticket_26',\n       'Ticket_28', 'Ticket_29', 'Ticket_31', 'Ticket_33', 'Ticket_34',\n       'Ticket_35', 'Ticket_36', 'Ticket_37', 'Ticket_A5', 'Ticket_CA',\n       'Ticket_Others', 'Ticket_PC', 'Ticket_SC', 'Ticket_SO', 'Ticket_ST',\n        'Miss.', 'Mr.', 'Mrs.', 'Others']]\n\ntest_fin","b509ea91":"pred_fin = rf_fin.predict(test_fin)\n\n\npred_df = pd.DataFrame(pred_fin, columns = ['Survived'],index = test_fin.index)\npred_df","d0701dce":"# Output Result\npred_df['Survived'].to_csv('My_Titanic_Predictions.csv', index = True, header = True)","adf6ac01":"importance = rf_fin.feature_importances_","3e4bbff4":"feat_imp = pd.DataFrame(importance, index = X.columns, columns = ['Importance'])\n\nfeat_imp.sort_values(['Importance'], inplace = True)\n","d75cc8dd":"plt.figure(figsize = (10,10))\nsns.barplot(x=feat_imp['Importance'], y = feat_imp.index, data = feat_imp, palette = 'coolwarm')","6fcd6087":"# EDA and Data Preparation","58b136c9":"# Building final model","8f411d07":"# Checking feature importance","35b1c158":"# Preparing test data ","9efdbc61":"# Importing required Libraries and datasets","13310f2e":"# Creating base random forest model"}}