{"cell_type":{"699b63d1":"code","54f9f9c5":"code","0d6109e9":"code","91dc93e8":"code","e6550bdf":"code","b9bf0810":"code","b28324bd":"code","9a9f8197":"code","bef71895":"code","a9a6a633":"code","6231bd2b":"code","5da49f41":"code","dac8b2c2":"code","e06ec6b0":"code","38948aed":"code","d20c6a61":"code","8d69027c":"code","4c77cf8e":"code","7c751a0b":"code","3668c81f":"code","c5f73b26":"code","f97e4a9c":"code","a6052c05":"code","de7da443":"code","46cca96a":"code","7f00a2b0":"code","3969f70c":"code","9bf7ff0d":"code","be6107d4":"code","0c7ab5d9":"markdown","c39630d6":"markdown","95008a4a":"markdown","0393de39":"markdown"},"source":{"699b63d1":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","54f9f9c5":"import numpy as np\nimport pandas as pd\nfrom bs4 import BeautifulSoup\nimport matplotlib.pyplot as plt","0d6109e9":"from keras.models import Sequential\nfrom keras.preprocessing import sequence, text\nfrom keras.layers import Embedding, Dense, LSTM, GRU, Conv1D, MaxPooling1D, Flatten","91dc93e8":"\n\ndata = pd.read_csv('..\/input\/imdb-dataset-of-50k-movie-reviews\/IMDB Dataset.csv')\ndata.head()\n\n","e6550bdf":"\n\ndata['sentiment'].value_counts()\n\n","b9bf0810":"def remove_html(text):\n    soup = BeautifulSoup(text, 'html.parser')\n    return soup.get_text()\n\ndef clean_text(raw_text):\n    text = remove_html(raw_text)\n    return text","b28324bd":"data['review'] = data['review'].apply(clean_text)\ndata.head()","9a9f8197":"data['sentiment'].value_counts()","bef71895":"# maximum number of words to keep, based on word frequency\nvocab_size = 10000\n\ntokenizer = text.Tokenizer(num_words=vocab_size)\ntokenizer.fit_on_texts(data['review'])\nsequences = tokenizer.texts_to_sequences(data['review'])\nword_index = tokenizer.word_index","a9a6a633":"# maximum length of all sequences\nmax_len = 100\n\nx = sequence.pad_sequences(sequences, maxlen=max_len)","6231bd2b":"sentiments = {\n    'positive': 1,\n    'negative': 0\n}\n\ny = np.asarray(data['sentiment'].map(sentiments))","5da49f41":"\n\ntrain_samples = 40000\n\nx_train = x[:train_samples]\ny_train = y[:train_samples]\n\nx_test = x[train_samples:]\ny_test = y[train_samples:]\n\n","dac8b2c2":"def load_glove(path):\n    \n    embedding_index = {}\n    for line in open(path):\n        values = line.split()\n        word = values[0]\n        coeff = np.asarray(values[1:], dtype='float32')\n        embedding_index[word] = coeff\n    \n    return embedding_index","e06ec6b0":"\n\nembedding_index = load_glove('..\/input\/datasettxt\/glove.6B.100d.txt')\n\n","38948aed":"embedding_dim = 100\nembedding_matrix = np.zeros((vocab_size, embedding_dim))\n\nfor word, index in word_index.items():\n    if index < vocab_size:\n        vector = embedding_index.get(word);\n        if vector is not None:\n            embedding_matrix[index] = embedding_index.get(word)\n\nprint('Shape of embedding matrix:', embedding_matrix.shape)\n","d20c6a61":"def get_LSTM_model(units = 32, dropout = 0):\n    \n    model = Sequential([\n        Embedding(vocab_size, embedding_dim, input_length=max_len, weights=[embedding_matrix], trainable=False),\n        LSTM(units, dropout=dropout),\n        Dense(1, activation='sigmoid')\n    ])\n    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n    \n    return model\n\n\ndef get_GRU_model(units = 32, dropout = 0):\n    \n    model = Sequential([\n        Embedding(vocab_size, embedding_dim, input_length=max_len, weights=[embedding_matrix], trainable=False),\n        GRU(units, dropout=dropout),\n        Dense(1, activation='sigmoid')\n    ])\n    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n    \n    return model\n\n\ndef get_CNN_model(filters = 32, filter_size = 7, pool_size = 5):\n    \n    model = Sequential([\n        Embedding(vocab_size, embedding_dim, input_length=max_len, weights=[embedding_matrix], trainable=False),\n        Conv1D(filters, filter_size, activation='relu'),\n        MaxPooling1D(pool_size),\n        Flatten(),\n        Dense(1, activation='sigmoid')\n    ])\n    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n    \n    return model","8d69027c":"LSTM_model = get_LSTM_model()\nLSTM_model.summary()","4c77cf8e":"GRU_model = get_GRU_model()\nGRU_model.summary()","7c751a0b":"\n\nCNN_model = get_CNN_model()\nCNN_model.summary()\n\n","3668c81f":"epochs = 5\nbatch_size = 32\nval_split = 0.2","c5f73b26":"LSTM_history = LSTM_model.fit(x_train, y_train, epochs=epochs, batch_size=batch_size, validation_split=val_split)\nLSTM_model.save('LSTM_imdb_sentiment_analysis.h5')","f97e4a9c":"GRU_history = GRU_model.fit(x_train, y_train, epochs=epochs, batch_size=batch_size, validation_split=val_split)\nGRU_model.save('GRU_imdb_sentiment_analysis.h5')","a6052c05":"CNN_history = CNN_model.fit(x_train, y_train, epochs=epochs, batch_size=batch_size, validation_split=val_split)\nCNN_model.save('CNN_imdb_sentiment_analysis.h5')","de7da443":"def plot_graph(history, title = 'accuracy and loss graphs'):\n    \n    acc_values = history.history['acc']\n    val_acc_values = history.history['val_acc']\n\n    loss_values = history.history['loss']\n    val_loss_values = history.history['val_loss']\n\n    epochs_range = range(1, epochs + 1)\n\n    fig, ax = plt.subplots(1, 2, figsize=(15, 5))\n    fig.suptitle(title)\n    \n    ax[0].plot(epochs_range, acc_values, label='Training accuracy')\n    ax[0].plot(epochs_range, val_acc_values, label='Validation accuracy')\n    ax[0].set(xlabel='Epochs', ylabel='Accuracy')\n    ax[0].legend()\n    ax[0].set_title('Accuracy')\n\n    ax[1].plot(epochs_range, loss_values, label='Training loss')\n    ax[1].plot(epochs_range, val_loss_values, label='Validation loss')\n    ax[1].set(xlabel='Epochs', ylabel='Loss')\n    ax[1].legend()\n    ax[1].set_title('Loss')\n","46cca96a":"\n\nplot_graph(LSTM_history, 'LSTM Model')\nplot_graph(GRU_history, 'GRU Model')\nplot_graph(CNN_history, 'CNN Model')\n\n","7f00a2b0":"def test_model(model):\n    scores = model.evaluate(x_test, y_test)\n    print('Loss: {}'.format(scores[0]))\n    print('Accuracy: {}'.format(scores[1]))","3969f70c":"print('LSTM Model')\ntest_model(LSTM_model)","9bf7ff0d":"print('GRU Model')\ntest_model(GRU_model)","be6107d4":"print('CNN Model')\ntest_model(CNN_model)","0c7ab5d9":"# Tokenize the words in reviews","c39630d6":"# # Prepare embedding matrix\n\n\n","95008a4a":"****Split data****\n","0393de39":"Load Glove embedding vector"}}