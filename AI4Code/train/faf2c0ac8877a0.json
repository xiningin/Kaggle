{"cell_type":{"c7fbb903":"code","12be570b":"code","e631beac":"code","ab026b32":"code","8671a1c3":"code","7a882da7":"code","8be36875":"code","08987a49":"code","cc310662":"code","400a5dbd":"code","85a95155":"code","1163b51d":"code","783b6702":"code","3fbf210c":"code","463a892d":"code","dcba1596":"code","77c2e2b4":"code","5a16e152":"code","9f0b959c":"code","496984bf":"code","55e38921":"code","370341fd":"code","13bc668f":"code","5c74212c":"code","1b4d4569":"code","8c225d0b":"code","12e66f16":"code","fc3a5365":"code","6c2065e5":"code","565a6cc5":"code","a64877f6":"code","7a010d44":"code","03c60b80":"code","dda343e8":"markdown","0d724235":"markdown","4e426044":"markdown","1858e1ed":"markdown","3edd8284":"markdown","37cc49c1":"markdown","e422ceda":"markdown","eddcc0f2":"markdown","46246ccf":"markdown","328ef96b":"markdown","19f16039":"markdown","1495866d":"markdown","2cec9d5f":"markdown","67fd4139":"markdown","7e820f4c":"markdown","049e4777":"markdown","1cecfc53":"markdown","7f28582b":"markdown","ed17d5eb":"markdown","c508ede2":"markdown","f42c6bbe":"markdown","ad44f215":"markdown","f2030989":"markdown","cb272273":"markdown","56898264":"markdown","91a44e24":"markdown","ced44b6c":"markdown","f9b785d2":"markdown","da5a5e58":"markdown","84c55ea3":"markdown","7cdf07e7":"markdown","77aad60b":"markdown","6310bab7":"markdown","ab3acd12":"markdown","4b6cc372":"markdown","021576ec":"markdown","29efd6b7":"markdown","982573d7":"markdown","d5c7599f":"markdown","2c72f2db":"markdown","5d5c3385":"markdown","40bb532e":"markdown","1c7dc5d9":"markdown","ac24bb0c":"markdown","c986b92f":"markdown","5f71b874":"markdown","1c3f9339":"markdown","88574dd0":"markdown","6a1200bb":"markdown"},"source":{"c7fbb903":"# Import all the libraries that we shall be using\nimport numpy as np\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom mpl_toolkits.mplot3d import Axes3D\n%matplotlib inline\n\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\nfrom sklearn.model_selection import GridSearchCV, cross_val_score\nfrom sklearn.metrics import accuracy_score, confusion_matrix\nfrom sklearn.cluster import KMeans\nfrom sklearn.svm import SVC\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.ensemble import RandomForestClassifier\n\nimport xgboost as xgb\n\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.callbacks import EarlyStopping","12be570b":"# Import labels (for the whole dataset, both training and testing)\ny = pd.read_csv('..\/input\/actual.csv')\nprint(y.shape)\ny.head()","e631beac":"y['cancer'].value_counts()","ab026b32":"# Recode label to numeric\ny = y.replace({'ALL':0,'AML':1})\nlabels = ['ALL', 'AML'] # for plotting convenience later on","8671a1c3":"# Import training data\ndf_train = pd.read_csv('..\/input\/data_set_ALL_AML_train.csv')\nprint(df_train.shape)\n\n# Import testing data\ndf_test = pd.read_csv('..\/input\/data_set_ALL_AML_independent.csv')\nprint(df_test.shape)","7a882da7":"df_train.head()","8be36875":"df_test.head()","08987a49":"# Transform all the call values to numbers (not used in this version)\n# df_train.replace(['A','P','M'],['1','2','3'], inplace=True)\n# df_test.replace(['A','P','M'],['1','2','3'], inplace=True)\n\n# Remove \"call\" columns from training and testing data\ntrain_to_keep = [col for col in df_train.columns if \"call\" not in col]\ntest_to_keep = [col for col in df_test.columns if \"call\" not in col]\n\nX_train_tr = df_train[train_to_keep]\nX_test_tr = df_test[test_to_keep]","cc310662":"train_columns_titles = ['Gene Description', 'Gene Accession Number', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10',\n       '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', \n       '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38']\n\nX_train_tr = X_train_tr.reindex(columns=train_columns_titles)","400a5dbd":"test_columns_titles = ['Gene Description', 'Gene Accession Number','39', '40', '41', '42', '43', '44', '45', '46',\n       '47', '48', '49', '50', '51', '52', '53',  '54', '55', '56', '57', '58', '59',\n       '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72']\n\nX_test_tr = X_test_tr.reindex(columns=test_columns_titles)","85a95155":"X_train = X_train_tr.T\nX_test = X_test_tr.T\n\nprint(X_train.shape) \nX_train.head()","1163b51d":"# Clean up the column names for training and testing data\nX_train.columns = X_train.iloc[1]\nX_train = X_train.drop([\"Gene Description\", \"Gene Accession Number\"]).apply(pd.to_numeric)\n\n# Clean up the column names for Testing data\nX_test.columns = X_test.iloc[1]\nX_test = X_test.drop([\"Gene Description\", \"Gene Accession Number\"]).apply(pd.to_numeric)\n\nprint(X_train.shape)\nprint(X_test.shape)\nX_train.head()","783b6702":"# Split into train and test (we first need to reset the index as the indexes of two dataframes need to be the same before you combine them).\n\n# Subset the first 38 patient's cancer types\nX_train = X_train.reset_index(drop=True)\ny_train = y[y.patient <= 38].reset_index(drop=True)\n\n# Subset the rest for testing\nX_test = X_test.reset_index(drop=True)\ny_test = y[y.patient > 38].reset_index(drop=True)","3fbf210c":"X_train.describe()","463a892d":"# Convert from integer to float\nX_train_fl = X_train.astype(float, 64)\nX_test_fl = X_test.astype(float, 64)\n\n# Apply the same scaling to both datasets\nscaler = StandardScaler()\nX_train_scl = scaler.fit_transform(X_train_fl)\nX_test_scl = scaler.transform(X_test_fl) # note that we transform rather than fit_transform","dcba1596":"pca = PCA()\npca.fit_transform(X_train)","77c2e2b4":"total = sum(pca.explained_variance_)\nk = 0\ncurrent_variance = 0\nwhile current_variance\/total < 0.90:\n    current_variance += pca.explained_variance_[k]\n    k = k + 1\n    \nprint(k, \" features explain around 90% of the variance. From 7129 features to \", k, \", not too bad.\", sep='')\n\npca = PCA(n_components=k)\nX_train.pca = pca.fit(X_train)\nX_train_pca = pca.transform(X_train)\nX_test_pca = pca.transform(X_test)\n\nvar_exp = pca.explained_variance_ratio_.cumsum()\nvar_exp = var_exp*100\nplt.bar(range(k), var_exp);","5a16e152":"pca3 = PCA(n_components=3).fit(X_train)\nX_train_reduced = pca3.transform(X_train)\n\nplt.clf()\nfig = plt.figure(1, figsize=(10,6 ))\nax = Axes3D(fig, elev=-150, azim=110,)\nax.scatter(X_train_reduced[:, 0], X_train_reduced[:, 1], X_train_reduced[:, 2], c = y_train.iloc[:,1], cmap = plt.cm.Paired, linewidths=10)\nax.set_title(\"First three PCA directions\")\nax.set_xlabel(\"1st eigenvector\")\nax.w_xaxis.set_ticklabels([])\nax.set_ylabel(\"2nd eigenvector\")\nax.w_yaxis.set_ticklabels([])\nax.set_zlabel(\"3rd eigenvector\")\nax.w_zaxis.set_ticklabels([])","9f0b959c":"fig = plt.figure(1, figsize = (10, 6))\nplt.scatter(X_train_reduced[:, 0],  X_train_reduced[:, 1], c = y_train.iloc[:,1], cmap = plt.cm.Paired, linewidths=10)\nplt.annotate('Note the Brown Cluster', xy = (30000,-2000))\nplt.title(\"2D Transformation of the Above Graph \")","496984bf":"print(\"Simply predicting everything as acute lymphoblastic leukemia (ALL) results in an accuracy of \", round(1 - np.mean(y_test.iloc[:,1]), 3), \".\", sep = '')","55e38921":"kmeans = KMeans(n_clusters=2, random_state=0).fit(X_train_scl)\nkm_pred = kmeans.predict(X_test_scl)\n\nprint('K-means accuracy:', round(accuracy_score(y_test.iloc[:,1], km_pred), 3))\n\ncm_km = confusion_matrix(y_test.iloc[:,1], km_pred)\n\nax = plt.subplot()\nsns.heatmap(cm_km, annot=True, ax = ax, fmt='g', cmap='Greens') \n\n# labels, title and ticks\nax.set_xlabel('Predicted labels')\nax.set_ylabel('True labels') \nax.set_title('K-means Confusion Matrix') \nax.xaxis.set_ticklabels(labels) \nax.yaxis.set_ticklabels(labels, rotation=360);","370341fd":"# Create a Gaussian classifier\nnb_model = GaussianNB()\n\nnb_model.fit(X_train, y_train.iloc[:,1])\n\nnb_pred = nb_model.predict(X_test)\n\nprint('Naive Bayes accuracy:', round(accuracy_score(y_test.iloc[:,1], nb_pred), 3))\n\ncm_nb =  confusion_matrix(y_test.iloc[:,1], nb_pred)\n\nax = plt.subplot()\nsns.heatmap(cm_nb, annot=True, ax = ax, fmt='g', cmap='Greens') \n\n# labels, title and ticks\nax.set_xlabel('Predicted labels')\nax.set_ylabel('True labels') \nax.set_title('Naive Bayes Confusion Matrix') \nax.xaxis.set_ticklabels(labels) \nax.yaxis.set_ticklabels(labels, rotation=360);","13bc668f":"log_grid = {'C': [1e-03, 1e-2, 1e-1, 1, 10], \n                 'penalty': ['l1', 'l2']}\n\nlog_estimator = LogisticRegression(solver='liblinear')\n\nlog_model = GridSearchCV(estimator=log_estimator, \n                  param_grid=log_grid, \n                  cv=3,\n                  scoring='accuracy')\n\nlog_model.fit(X_train, y_train.iloc[:,1])\n\nprint(\"Best Parameters:\\n\", log_model.best_params_)\n\n# Select best log model\nbest_log = log_model.best_estimator_\n\n# Make predictions using the optimised parameters\nlog_pred = best_log.predict(X_test)\n\nprint('Logistic Regression accuracy:', round(accuracy_score(y_test.iloc[:,1], log_pred), 3))\n\ncm_log =  confusion_matrix(y_test.iloc[:,1], log_pred)\n\nax = plt.subplot()\nsns.heatmap(cm_log, annot=True, ax = ax, fmt='g', cmap='Greens') \n\n# labels, title and ticks\nax.set_xlabel('Predicted labels')\nax.set_ylabel('True labels') \nax.set_title('Logistic Regression Confusion Matrix') \nax.xaxis.set_ticklabels(labels) \nax.yaxis.set_ticklabels(labels, rotation=360);","5c74212c":"# Parameter grid\nsvm_param_grid = {'C': [0.1, 1, 10, 100], 'gamma': [1, 0.1, 0.01, 0.001, 0.00001, 10], \"kernel\": [\"linear\", \"rbf\", \"poly\"], \"decision_function_shape\" : [\"ovo\", \"ovr\"]} \n\n# Create SVM grid search classifier\nsvm_grid = GridSearchCV(SVC(), svm_param_grid, cv=3)\n\n# Train the classifier\nsvm_grid.fit(X_train_pca, y_train.iloc[:,1])\n\nprint(\"Best Parameters:\\n\", svm_grid.best_params_)\n\n# Select best svc\nbest_svc = svm_grid.best_estimator_\n\n# Make predictions using the optimised parameters\nsvm_pred = best_svc.predict(X_test_pca)\n\nprint('SVM accuracy:', round(accuracy_score(y_test.iloc[:,1], svm_pred), 3))\n\ncm_svm =  confusion_matrix(y_test.iloc[:,1], svm_pred)\n\nax = plt.subplot()\nsns.heatmap(cm_svm, annot=True, ax = ax, fmt='g', cmap='Greens') \n\n# Labels, title and ticks\nax.set_xlabel('Predicted labels')\nax.set_ylabel('True labels') \nax.set_title('SVM Confusion Matrix') \nax.xaxis.set_ticklabels(labels) \nax.yaxis.set_ticklabels(labels, rotation=360);","1b4d4569":"# Hyperparameters search grid \nrf_param_grid = {'bootstrap': [False, True],\n         'n_estimators': [60, 70, 80, 90, 100],\n         'max_features': [0.6, 0.65, 0.7, 0.75, 0.8],\n         'min_samples_leaf': [8, 10, 12, 14],\n         'min_samples_split': [3, 5, 7]\n        }\n\n# Instantiate random forest classifier\nrf_estimator = RandomForestClassifier(random_state=0)\n\n# Create the GridSearchCV object\nrf_model = GridSearchCV(estimator=rf_estimator, param_grid=rf_param_grid, cv=3, scoring='accuracy')\n\n# Fine-tune the hyperparameters\nrf_model.fit(X_train, y_train.iloc[:,1])\n\nprint(\"Best Parameters:\\n\", rf_model.best_params_)\n\n# Get the best model\nrf_model_best = rf_model.best_estimator_\n\n# Make predictions using the optimised parameters\nrf_pred = rf_model_best.predict(X_test)\n\nprint('Random Forest accuracy:', round(accuracy_score(y_test.iloc[:,1], rf_pred), 3))\n\ncm_rf = confusion_matrix(y_test.iloc[:,1], rf_pred)\n\nax = plt.subplot()\nsns.heatmap(cm_rf, annot=True, ax = ax, fmt='g', cmap='Greens') \n\n# labels, title and ticks\nax.set_xlabel('Predicted labels')\nax.set_ylabel('True labels') \nax.set_title('Random Forest Confusion Matrix') \nax.xaxis.set_ticklabels(labels) \nax.yaxis.set_ticklabels(labels, rotation=360);","8c225d0b":"xgb_grid_params = {'max_depth': [3, 4, 5, 6, 7, 8, 10, 12],\n               'min_child_weight': [1, 2, 4, 6, 8, 10, 12, 15],\n               'n_estimators': [40, 50, 60, 70, 80, 90, 100, 110, 120, 130],\n               'learning_rate': [0.001, 0.01, 0.05, 0.1, 0.2, 0.3]}\n\nfixed_params = {'random_state': 0,\n                'n_jobs': -1}\n\nxgb_model = GridSearchCV(xgb.XGBClassifier(**fixed_params), \n                       param_grid = xgb_grid_params, \n                       scoring = 'accuracy',\n                       cv = 3)\n\nxgb_model.fit(X_train_pca, y_train.iloc[:,1])\n\nprint(\"Best Parameters:\\n\", xgb_model.best_params_)\n\n# Get the best model\nxgb_model_best = xgb_model.best_estimator_\n\n# Make predictions using the optimised parameters\nxgb_pred = xgb_model_best.predict(X_test_pca)\n\nprint('XGB (PCA with Grid Search) accuracy:', round(accuracy_score(y_test.iloc[:,1], xgb_pred), 3))\n\ncm_xgb = confusion_matrix(y_test.iloc[:,1], xgb_pred)\n\nax = plt.subplot()\nsns.heatmap(cm_xgb, annot=True, ax = ax, fmt='g', cmap='Greens') \n\n# labels, title and ticks\nax.set_xlabel('Predicted labels')\nax.set_ylabel('True labels') \nax.set_title('XGB (PCA with Grid Search) Confusion Matrix') \nax.xaxis.set_ticklabels(labels) \nax.yaxis.set_ticklabels(labels, rotation=360);","12e66f16":"xgb2_model = xgb.XGBClassifier()\nxgb2_model.fit(X_train_pca, y_train.iloc[:,1])\n\nxgb2_pred = xgb2_model.predict(X_test_pca)\n\nprint('Accuracy: ', round(accuracy_score(y_test.iloc[:,1], xgb2_pred), 3))\n\ncm_xgb2 = confusion_matrix(y_test.iloc[:,1], xgb2_pred)\n\nax = plt.subplot()\nsns.heatmap(cm_xgb2, annot=True, ax = ax, fmt='g', cmap='Greens') \n\n# Labels, title and ticks\nax.set_xlabel('Predicted labels')\nax.set_ylabel('True labels') \nax.set_title('XGB (PCA without Grid Search) Confusion Matrix') \nax.xaxis.set_ticklabels(labels) \nax.yaxis.set_ticklabels(labels, rotation=360);","fc3a5365":"xgb3_model = xgb.XGBClassifier()\nxgb3_model.fit(X_train, y_train.iloc[:,1])\n\nxgb3_pred = xgb3_model.predict(X_test)\n\nprint('XGB (no PCA or Grid Search) accuracy:', round(accuracy_score(y_test.iloc[:,1], xgb3_pred), 3))\n\ncm_xgb3 = confusion_matrix(y_test.iloc[:,1], xgb3_pred)\n\nax = plt.subplot()\nsns.heatmap(cm_xgb3, annot=True, ax = ax, fmt='g', cmap='Greens') \n\n# Labels, title and ticks\nax.set_xlabel('Predicted labels')\nax.set_ylabel('True labels') \nax.set_title('XGB (no PCA or Grid Search) Confusion Matrix') \nax.xaxis.set_ticklabels(labels) \nax.yaxis.set_ticklabels(labels, rotation=360);","6c2065e5":"# Create model architecture\nmodel = Sequential()\nmodel.add(Dense(16, activation='relu', input_shape=(7129,)))\nmodel.add(Dense(1, activation='sigmoid'))","565a6cc5":"# Compile model\nmodel.compile(optimizer='adam',\nloss='binary_crossentropy',\nmetrics=['accuracy'])","a64877f6":"# Create training\/validation sets\npartial_X_train = X_train_scl[:30]\nX_val = X_train_scl[30:]\n\ny_train_label = y_train.iloc[:,1]\npartial_y_train = y_train_label[:30]\ny_val = y_train_label[30:]","7a010d44":"# Set up early stopping\nes = EarlyStopping(monitor='val_loss', verbose=1, patience=3)\n\n# Fit model\nhistory = model.fit(partial_X_train,\n            partial_y_train,\n            epochs=50,\n            batch_size=4,\n            validation_data=(X_val, y_val),\n            callbacks=[es])","03c60b80":"# Make predictions\nnn_pred = model.predict_classes(X_test_scl)\n\nprint('Neural Network accuracy: ', round(accuracy_score(y_test.iloc[:,1], nn_pred), 3))\n\ncm_nn = confusion_matrix(y_test.iloc[:,1], nn_pred)\n\nax = plt.subplot()\nsns.heatmap(cm_nn, annot=True, ax = ax, fmt='g', cmap='Greens') \n\n# Labels, title and ticks\nax.set_xlabel('Predicted labels')\nax.set_ylabel('True labels') \nax.set_title('Neural Network Confusion Matrix') \nax.xaxis.set_ticklabels(labels) \nax.yaxis.set_ticklabels(labels, rotation=360);","dda343e8":"Finally we shall build a neural network using Keras (with TensorFlow as a backend). This only a \"shallow\" learning model with one hidden layer \u2014 adding several extra layers with so few training datapoints would just lead to overfitting.  ","0d724235":"### XG Boost","4e426044":"OK, that's more like it. It seems that the PCA was actually the wrong approach for the XGB model.","1858e1ed":"The neural network isn't as good as some of the other models.","3edd8284":"Let's start by establishing a naive baseline. This doesn't require a model, we are just taking the proportion of tests that belong to the majority class as a baseline. In other words, let's see what happens if we were to predict that every patient belongs to the \"ALL\" class.","37cc49c1":"Having prepared the dataset, it's now finally time to try out some models. ","e422ceda":"Clearly there is some variation in the scales across the different features. Many machine learning models work much better with data that's on the same scale, so let's create a scaled version of the dataset.","eddcc0f2":"Let's now take a look at some summary statistics:","46246ccf":"### Support Vector Machine\n\nHere we will try another traditional approach, a support vector machine (SVM) classifier. For the SVM, so we using the PCA version of the dataset. Again we use grid search cross-validation to tune the model.","328ef96b":"### Random Forest","19f16039":"With 7129 features, it's also worth considering whether we might be able to reduce the dimensionality of the dataset. Once very common approach to this is principal components analysis (PCA). Let's start by leaving the number of desired components as an open question: ","1495866d":"We actually need our labels to be numeric, so let's just do that now.","2cec9d5f":"Let's start by taking a look at our target, the ALL\/AML label.","67fd4139":"The 7129 gene descriptions are provided as the rows and the values for each patient as the columns. This will clearly require some tidying up.\n\nOur first decision is: What should we do about all the \"call\" columns, one for each patient. No explanation for these is provided, so it's difficult to know whether they might be useful or not. We have taken the decision to simply remove them, but this may possibly not be the best approach. ","7e820f4c":"## Data Preparation","049e4777":"In the combined training and testing sets there are 72 patients, each of whom are labelled either \"ALL\" or \"AML\" depending on the type of leukemia they have. Here's the breakdown:","1cecfc53":"### Logistic Regression","7f28582b":"### Baseline","ed17d5eb":"## Model Building","c508ede2":"### Naive Bayes","f42c6bbe":"We now move on to tree-base approaches, starting with the very popular random forest. We don't need scaled data for this, so again we wont use the scaled version of the dataset, just a grid search for tuning the hyperparameters.","ad44f215":"Let's set a threshold for explained variance of 90% and see how many features are required to meet that threshold. (Here we are using the code from [this kernel](https:\/\/www.kaggle.com\/rstogi896\/geneclassification-using-gridsearchcv-and-svm).) ","f2030989":"This K-means approach is better than the baseline, but we should be able to do better with some kind of supervised learning model.","cb272273":"#### XGB \u2014 PCA with Grid Search","56898264":"In conclusion, it was the logistic regression model that provided the best performance on this dataset.","91a44e24":"## Introduction","ced44b6c":"That looks much better. We have the 38 patients as rows in the training set, and the other 34 as rows in the testing set. Each of those datasets has 7129 gene expression features.\n\nBut we haven't yet associated the target labels with the right patients. You will recall that all the labels are all stored in a single dataframe. Let's split the data so that the patients and labels match up across the training and testing dataframes.","f9b785d2":"First we shall try an unsupervised clustering approach using the scaled data.","da5a5e58":"This is still messy as the first two rows are more or less duplicates of one another and we haven't yet created the column names. Let's simply turn the second row into the column names and delete the first row.","84c55ea3":"In this notebook we shall examine a small gene expression dataset, attempting to classify leukemia patients into one of two classes. This dataset was the focus of a [Kaggle Days meetup](https:\/\/www.meetup.com\/Kaggle-Days-Meetup-London\/events\/258570474\/) in London that I attended in March 2019 and the original data can be found [here](https:\/\/www.kaggle.com\/crawford\/gene-expression). It comes with the following explanatory notes:\n\n### Context\nThis dataset comes from a proof-of-concept study published in 1999 by Golub et al. It showed how new cases of cancer could be classified by gene expression monitoring (via DNA microarray) and thereby provided a general approach for identifying new cancer classes and assigning tumors to known classes. These data were used to classify patients with acute myeloid leukemia (AML) and acute lymphoblastic leukemia (ALL).\n\n### Content\nGolub et al \"Molecular Classification of Cancer: Class Discovery and Class Prediction by Gene Expression Monitoring\"\n\nThere are two datasets containing the initial (training, 38 samples) and independent (test, 34 samples) datasets used in the paper. These datasets contain measurements corresponding to ALL and AML samples from Bone Marrow and Peripheral Blood. Intensity values have been re-scaled such that overall intensities for each chip are equivalent.\n\n### Acknowledgements\nMolecular Classification of Cancer: Class Discovery and Class Prediction by Gene Expression\n\nScience 286:531-537. (1999). Published: 1999.10.14\n\nT.R. Golub, D.K. Slonim, P. Tamayo, C. Huard, M. Gaasenbeek, J.P. Mesirov, H. Coller, M. Loh, J.R. Downing, M.A. Caligiuri, C.D. Bloomfield, and E.S. Lander\n\nThese datasets have been converted to a comma separated value files (CSV).\n\n### Inspiration\nThese datasets are great for classification problems. The original authors used the data to classify the type of cancer in each patient by their gene expressions.","7cdf07e7":"Random forest almost matches the SVM performance.","77aad60b":"This SVM model is making just a couple of classification errors.","6310bab7":"### K-Means Clustering","ab3acd12":"Now we can simply transpose the columns and rows so that genes become features and each patient's observations occupies a single row.","4b6cc372":"Nowadays, gradient boosting models such as XG Boost(XGB) are extremely popular. Here we shall experiment with three alternative versions, PCA with grid search, PCA without grid search and also the orginal data without either PCA or grid search.","021576ec":"Another very standard approach is logistic regression. Here we will be using grid search cross-validation tuning to try and determine the best hyperparameters. We don't need to scale the data for logistic regression, nor are we using the PCA version of the dataset. ","29efd6b7":"XGB with PCA and grid search isn't particularly good.","982573d7":"This logistic regression model manages perfect classification.","d5c7599f":"Without the grid search, this is barely any better. However, it seems that the grid search may possibly be resulting in some overfitting.","2c72f2db":"### Neural Network","5d5c3385":"#### XGB \u2014 PCA with no Grid Search","40bb532e":"# Gene Expression Monitoring Analysis","1c7dc5d9":"For our first supervised model, we shall use a very straightforward naive bayes approach.","ac24bb0c":"### I hope that you have enjoyed this kernel and found it useful. If so I would be grateful if you please upvote it, so that others will become aware of it. Please feel free to add any comments, suggestions or corrections below.","c986b92f":"We can't plot something in 22 dimensions, so let's just see what the PCA looks like when we just pick the top three compoments. (Here we are using code taken from [this kernel](https:\/\/www.kaggle.com\/kanav0183\/pca-analysis-for-geneclassification).)  ","5f71b874":"#### XGB \u2014 no PCA or Grid Search","1c3f9339":"Neither the training and testing column names are not in numeric order, so it's important that we reorder these at some point, so that the labels will line up with the corresponding data.  ","88574dd0":"Now we move on to the features, which are provided for the training and testing datasets separately.","6a1200bb":"The naive bayes model is pretty good, just three incorrect classifications."}}