{"cell_type":{"956f81a9":"code","5f7cfb79":"code","574fbdfb":"code","906211ed":"code","2965c63f":"code","337dbad4":"code","efe6b47d":"code","61980391":"code","8fb96c2d":"code","0f2dfe87":"code","ac9c569a":"markdown","78168c24":"markdown","b8ae910d":"markdown","090b0412":"markdown","5ac9a29b":"markdown","45943873":"markdown"},"source":{"956f81a9":"import os\nimport pandas as pd\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nimport numpy as np\nimport zipfile\nimport shutil\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' # Supress tensorflow log outputs","5f7cfb79":"with zipfile.ZipFile(\"..\/input\/dogs-vs-cats\/train.zip\", 'r') as zip_ref:\n    zip_ref.extractall(\".\/\")","574fbdfb":"train_path = \".\/train\/\"\nprint(len(os.listdir(\".\/train\")))\nfor file_name in os.listdir(train_path)[:5]:\n    print(file_name)","906211ed":"\"\"\"\nCreate following directory structure:\ndataset\/\n...dog\/\n......dog_image_1.jpg\n......dog_image_2.jpg\n...cat\/\n......cat_image_1.jpg\n......cat_image_2.jpg\n\"\"\"\n\nos.makedirs(\".\/dataset\/dog\")\nos.makedirs(\".\/dataset\/cat\")\n\nfor file_name in os.listdir(train_path):\n    if \"cat\" in file_name:\n        shutil.move(os.path.join(train_path, file_name), os.path.join(\".\/dataset\/cat\/\", file_name))\n    elif \"dog\" in file_name:\n        shutil.move(os.path.join(train_path, file_name), os.path.join(\".\/dataset\/dog\/\", file_name))","2965c63f":"train_dataset = tf.keras.utils.image_dataset_from_directory(\n    \".\/dataset\", # root directory given as image_dataset_from_directory requires directory \n    image_size=(150,150),\n    label_mode=\"binary\",\n    validation_split=0.2,\n    subset=\"training\",\n    batch_size=32,\n    seed=1\n)\n\nval_dataset = tf.keras.utils.image_dataset_from_directory(\n    \".\/dataset\", # root directory given as image_dataset_from_directory requires directory \n    image_size=(150,150),\n    label_mode=\"binary\",\n    validation_split=0.2,\n    subset=\"validation\",\n    batch_size=32,\n    seed=1\n)\n\n\n# Improve performance of data loading; see: https:\/\/www.tensorflow.org\/guide\/data_performance\nAUTOTUNE = tf.data.AUTOTUNE\ntrain_dataset = train_dataset.cache().prefetch(AUTOTUNE)\nval_dataset = val_dataset.cache().prefetch(AUTOTUNE)","337dbad4":"# Data augmentation layers\ndata_augmentation = keras.Sequential(\n    [\n        layers.RandomFlip(\"horizontal\"),\n        layers.RandomRotation(0.2),\n        layers.RandomZoom(0.2),\n        layers.RandomContrast(0.2)\n    ]\n)","efe6b47d":"model = keras.Sequential([\n    layers.Input((150, 150, 3)),\n    data_augmentation,\n    layers.Rescaling(scale=1.\/255), # rescale between 0 and 1\n    \n    layers.Conv2D(32, 5, activation=\"relu\"),\n    layers.MaxPooling2D(),\n    layers.BatchNormalization(),  \n    \n    layers.Conv2D(64, 3, activation=\"relu\"),\n    layers.MaxPooling2D(),\n    layers.BatchNormalization(),  \n    \n    layers.Conv2D(128, 3, activation=\"relu\"),\n    layers.MaxPooling2D(),\n    layers.BatchNormalization(),  \n    \n    layers.Conv2D(256, 3, activation=\"relu\"),\n    layers.MaxPooling2D(),\n    layers.BatchNormalization(),  \n    \n    layers.Flatten(),\n    \n    layers.Dense(256, activation=\"relu\"),\n    layers.BatchNormalization(), \n    layers.Dropout(0.4),\n    \n    layers.Dense(128, activation=\"relu\"),\n    layers.BatchNormalization(), \n    layers.Dropout(0.3),\n    \n    layers.Dense(64, activation=\"relu\"),\n    layers.BatchNormalization(), \n  \n    layers.Dense(1, activation=\"sigmoid\") # sigmoid for binary classification   \n])","61980391":"model.compile(\n    optimizer = keras.optimizers.Adam(learning_rate=0.01),\n    loss = keras.losses.BinaryCrossentropy(),\n    metrics=[\"accuracy\"]\n)","8fb96c2d":"# Add callbacks to minimize validation loss and maximize validation accuracy\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom tensorflow.keras.callbacks import ReduceLROnPlateau\nfrom tensorflow.keras.callbacks import ModelCheckpoint\n\ncallbacks = [\n    EarlyStopping(patience=15, monitor='val_loss', restore_best_weights=True),\n\n\tReduceLROnPlateau(monitor='val_loss', min_lr=1e-7, patience=5, mode='min', verbose=1, factor=0.5),\n\n\tModelCheckpoint(monitor='val_loss', filepath='.\/best_model.h5', save_best_only=True)\n]","0f2dfe87":"model.fit(train_dataset, epochs=100, callbacks=callbacks, validation_data=val_dataset)","ac9c569a":"The required directory structure for [image_dataset_from_directory](https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/keras\/utils\/image_dataset_from_directory:\/\/) requires a main directory, followed by subfolders ","78168c24":"### Unzip train.zip to working directory","b8ae910d":"### Create dataset","090b0412":"### List the files in the train directory\nAs we can see, the files are labelled by the file name. ","5ac9a29b":"### CNN Model with Regularization","45943873":"### Callbacks "}}