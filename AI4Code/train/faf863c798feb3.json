{"cell_type":{"5d586e71":"code","555b3335":"code","4bd916bc":"code","01ef1caf":"code","f7c1d890":"code","bc3916c0":"code","c9513485":"code","d8b7cff9":"code","bcab3303":"code","104863fb":"code","8a1b8820":"code","f36f3107":"code","8c6a7741":"code","0708c921":"code","e5bd1eb8":"code","7dc9fd3a":"code","1d8bb52a":"code","84d94555":"code","244db08a":"code","c71b4cab":"code","5839b013":"code","4ad51023":"code","5b626feb":"code","d8db3887":"code","67836f4a":"code","0d550dbb":"code","19e3ae69":"code","762f5f53":"code","6cdda32e":"code","530cfde1":"code","79762c12":"code","de2efeee":"code","d1c26efa":"code","4cd2c523":"code","5ca17d20":"code","de2739f7":"code","20c00da4":"code","f214bf5b":"code","63e83163":"code","787cee2a":"code","85fe983b":"code","c4e8c1f2":"markdown","cd4add01":"markdown","e02822b1":"markdown","1a29625a":"markdown","3f1b1c6e":"markdown","4a10cfa2":"markdown","305c9c29":"markdown","ca53a63e":"markdown","b8fed95c":"markdown","d9a75f4a":"markdown","5e170cb0":"markdown","111f4e3b":"markdown","61ad140f":"markdown","c77906bd":"markdown","7975bbf3":"markdown","ef99c051":"markdown","b48e3122":"markdown","ed486c58":"markdown","d54f4a9c":"markdown","e29220f2":"markdown","9d8d800c":"markdown","985a1919":"markdown","62b18214":"markdown","56cd5c7c":"markdown","ad838af6":"markdown","6835d9fc":"markdown","82ccb56b":"markdown","b6ee1795":"markdown","5e0e46c9":"markdown","955bf840":"markdown","f039e177":"markdown","518c6a79":"markdown","1c7ba5a4":"markdown","4640db08":"markdown"},"source":{"5d586e71":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# import data viz libraires\nimport seaborn as sns\nfrom matplotlib import pyplot as plt\nfrom matplotlib import style\n\n# Ignore warnings\nimport warnings\nwarnings.filterwarnings('ignore')\n\n#import algorithms\nfrom sklearn import linear_model\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import Perceptron\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC, LinearSVC\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.preprocessing import StandardScaler\n\nfrom IPython.display import display\n\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","555b3335":"train_raw = pd.read_csv(\"\/kaggle\/input\/titanic\/train.csv\")\ntest_raw = pd.read_csv(\"\/kaggle\/input\/titanic\/test.csv\")\n\ntrain_raw.head()","4bd916bc":"train= train_raw\ntest= test_raw\ntest.head()","01ef1caf":"train.info()","f7c1d890":"#find percent of missing data\npercent_missing = train.isnull().sum() *100 \/len(train)\npercent_missing = percent_missing.sort_values(ascending=False)\npercent_missing","bc3916c0":"train.describe()","c9513485":"train.describe().columns","d8b7cff9":"#plot histograms for all variables\n#first we need to drop passenger id\ntrain.drop(['PassengerId'], 1).hist(bins=50, figsize=(20,15))\nplt.show()","bcab3303":"train.corr()\nsns.heatmap(train.corr(), cmap='Spectral')","104863fb":"#see effect of class on survival rate\nsns.barplot(x='Pclass', y='Survived', data=train)\n","8a1b8820":"sns.barplot(x='Sex', y='Survived', data=train)","f36f3107":"pd.pivot_table(train, index = 'Survived', values = ['Age','SibSp','Parch','Fare', 'Pclass'])","8c6a7741":"# Comparing survival and each of these categorical variables \nprint(pd.pivot_table(train, index = 'Survived', columns = 'Pclass', values = 'Ticket' ,aggfunc ='count'))\nprint()\nprint(pd.pivot_table(train, index = 'Survived', columns = 'Sex', values = 'Ticket' ,aggfunc ='count'))\nprint()\nprint(pd.pivot_table(train, index = 'Survived', columns = 'Embarked', values = 'Ticket' ,aggfunc ='count'))","0708c921":"train= train.drop(['PassengerId'], axis=1)","e5bd1eb8":"figure, axes = plt.subplots(1,1,figsize=(20, 6))\nplot = sns.barplot(x=\"Embarked\", y=\"Fare\", hue=\"Sex\", data=train, ax=axes)\nplt.show()\ndisplay(train[train['Embarked'].isnull()])","7dc9fd3a":"def data_preprocessing(df):\n# use this function to process both training and testing data\n\n    processed_df= df\n    #replace emabrked with C\n    processed_df['Embarked'].fillna('C', inplace=True)\n\n\n    # replace missing ages by the mean age of passengers who belong to the same group of class\/sex\/family\n    processed_df['Age'] = processed_df.groupby(['Pclass','Sex','Parch','SibSp'])['Age'].transform(lambda x: x.fillna(x.mean()))   \n    processed_df['Age'] = processed_df.groupby(['Pclass','Sex','Parch'])['Age'].transform(lambda x: x.fillna(x.mean()))\n    processed_df['Age'] = processed_df.groupby(['Pclass','Sex'])['Age'].transform(lambda x: x.fillna(x.mean()))\n    \n    # replace the only missing fare value for test dataset \n    processed_df['Fare'] = processed_df['Fare'].interpolate()    \n    \n    return processed_df","1d8bb52a":"train = data_preprocessing(train)\ntest = data_preprocessing(test)","84d94555":"import re\n\ndeck = {\"A\": 1, \"B\": 2, \"C\": 3, \"D\": 4, \"E\": 5, \"F\": 6, \"G\": 7, \"U\": 8}\ndata = [train, test]\n\nfor dataset in data:\n    # Modification of cabin column to keep only the letter contained corresponding to the deck of the boat\n\n    dataset['Cabin'] = dataset['Cabin'].fillna(\"U0\")\n    dataset['Deck'] = dataset['Cabin'].map(lambda x: re.compile(\"([a-zA-Z]+)\").search(x).group())\n    dataset['Deck'] = dataset['Deck'].map(deck)\n    dataset['Deck'] = dataset['Deck'].fillna(0)\n    dataset['Deck'] = dataset['Deck'].astype(int)\n    \n    #calculate feature that shows number of relatives on the ship \n\n    dataset['relatives'] = dataset['SibSp'] + dataset['Parch']\n    dataset.loc[dataset['relatives'] > 0, 'not_alone'] = 0\n    dataset.loc[dataset['relatives'] == 0, 'not_alone'] = 1\n    dataset['not_alone'] = dataset['not_alone'].astype(int)\n\n    # Create a Title column from name column\n    dataset['Title'] = pd.Series((name.split('.')[0].split(',')[1].strip() for name in dataset['Name']), index=dataset.index)\n    dataset['Title'] = dataset['Title'].replace(['Lady', 'the Countess','Countess','Capt', 'Col','Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare')\n    dataset['Title'] = dataset['Title'].replace(['Mlle', 'Ms'], 'Miss')\n    dataset['Title'] = dataset['Title'].replace('Mme', 'Mrs')\n    dataset['Title'] = dataset['Title'].map({\"Mr\": 1, \"Miss\": 2, \"Mrs\": 3, \"Master\": 4, \"Rare\": 5})\n","244db08a":"train.info()","c71b4cab":"# Transform categorical variables to numeric variables\ntrain['Sex'] = train['Sex'].map({'male': 0, 'female': 1})\ntrain['Embarked'] = train['Embarked'].map({'S': 0, 'C': 1, 'Q': 2})\n\ntest['Sex'] = test['Sex'].map({'male': 0, 'female': 1})\ntest['Embarked'] = test['Embarked'].map({'S': 0, 'C': 1, 'Q': 2})\n\n#convert Fare and Age from flaot to integer type\ncols = ['Fare', 'Age']\ntrain[cols] = train[cols].fillna(0)\ntest[cols] = test[cols].fillna(0)\n\ntrain[cols] = train[cols].applymap(np.int64)\ntest[cols] = test[cols].applymap(np.int64)\n\n#drop name, ticket  and cabin from the dataset\ntrain = train.drop(['Name', 'Ticket', 'Cabin'], axis=1)\ntest = test.drop(['Name', 'Ticket', 'Cabin'], axis=1)","5839b013":"train.info()","4ad51023":"from sklearn.model_selection import train_test_split\n\nX = train.drop(['Survived'], 1)\nY = train['Survived']\n\n#scale data because we need all variables to be on the same scale when paing to the model\nsc = StandardScaler()\nX = pd.DataFrame(sc.fit_transform(X.values), index=X.index, columns=X.columns)\n    \n# Split dataset for model testing\nX_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n\nX_train.head()","5b626feb":"logreg = LogisticRegression()\nlogreg.fit(X_train, Y_train)\n\nY_pred = logreg.predict(X_test)\n\nacc_log = round(logreg.score(X_train, Y_train) * 100, 2)\nprint(acc_log)","d8db3887":"# KNN \nknn = KNeighborsClassifier(n_neighbors = 3) \nknn.fit(X_train, Y_train)  \nY_pred = knn.predict(X_test)  \nacc_knn = round(knn.score(X_train, Y_train) * 100, 2)\nprint(acc_knn)","67836f4a":"from sklearn.svm import SVC\n\nSVMC = SVC(probability=True, kernel='rbf')\nSVMC.fit(X_train, Y_train)\n\nY_pred = SVMC.predict(X_test)\n\nacc_svc = round(SVMC.score(X_train, Y_train) * 100, 2)\nprint(acc_svc)","0d550dbb":"decision_tree = DecisionTreeClassifier() \ndecision_tree.fit(X_train, Y_train)  \nY_pred = decision_tree.predict(X_test)  \nacc_decision_tree = round(decision_tree.score(X_train, Y_train) * 100, 2)\nprint(acc_decision_tree)","19e3ae69":"\nrandom_forest = RandomForestClassifier(n_estimators=250, min_samples_leaf=3, max_features=0.5, n_jobs=-1)\nrandom_forest.fit(X_train, Y_train)\n\nrandom_forest.score(X_train, Y_train)","762f5f53":"from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\nY_prediction = random_forest.predict(X_test)\n\n#acc_random_forest = round(random_forest.score(X_train, Y_train) * 100, 2)\n\nacc_random_forest=accuracy_score(Y_test, Y_prediction)\nprint(acc_random_forest)","6cdda32e":"print(classification_report(Y_test, Y_prediction))","530cfde1":"from xgboost import XGBClassifier\nxgb = XGBClassifier(random_state =1)\n\nxgb.fit(X_train, Y_train)\n\ny_pred = xgb.predict(X_test)\nacc_xgb = round(xgb.score(X_train, Y_train) * 100, 2)\nprint(acc_xgb)","79762c12":"results = pd.DataFrame({\n    'Model': ['Support Vector Machines', 'KNN', 'Logistic Regression', \n              'Random Forest', 'XGBoost', \n              'Decision Tree'],\n    'Score': [acc_svc, acc_knn, acc_log, acc_random_forest, acc_xgb,acc_decision_tree]})\n\nresult_df = results.sort_values(by='Score', ascending=False)\nresult_df = result_df.set_index('Score')\nresult_df.head(7)","de2efeee":"# scale test dataset\ntest.drop(labels = [\"PassengerId\"], axis = 1, inplace = True)\n\ntest = pd.DataFrame(sc.fit_transform(test.values), index=test.index, columns=test.columns)","d1c26efa":"test.head(10)","4cd2c523":"from sklearn.model_selection import cross_val_predict, cross_val_score, cross_validate\nfrom sklearn.model_selection import StratifiedKFold, StratifiedShuffleSplit\nfrom sklearn.metrics import confusion_matrix, roc_curve\nfrom sklearn.metrics import precision_score, recall_score, f1_score\n\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras.wrappers.scikit_learn import KerasClassifier\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.utils import plot_model\nfrom tensorflow.keras.layers import Input, Dense, Dropout, AlphaDropout, BatchNormalization,Concatenate, concatenate\nfrom tensorflow.keras.optimizers import SGD, RMSprop, Adamax, Adagrad, Adam, Nadam, SGD\nfrom tensorflow.keras.regularizers import l2\nfrom tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping\nfrom tensorflow.keras.metrics import *","5ca17d20":"metrics = ['accuracy' , Precision(),Recall()]","de2739f7":"def create_model():\n    model = Sequential()\n    model.add(Input(shape=X_train.shape[1], name='Input_'))\n    model.add(Dense(8, activation='relu', kernel_initializer='glorot_normal', kernel_regularizer=l2(0.001)))\n    model.add(Dense(16, activation='relu', kernel_initializer='glorot_normal', kernel_regularizer=l2(0.1)))\n    model.add(Dropout(0.5))\n    model.add(Dense(16, activation='relu', kernel_initializer='glorot_normal', kernel_regularizer=l2(0.1)))\n    model.add(Dropout(0.5))\n    model.add(Dense(1, activation='sigmoid', kernel_initializer='glorot_normal'))\n\n    model.summary()\n    optimize = Adam(lr = 0.0001)\n    model.compile(optimizer = optimize, \n                       loss = 'binary_crossentropy', \n                       metrics = metrics)\n    return model","20c00da4":"estimator = KerasClassifier(build_fn = create_model, epochs = 600, batch_size = 32, verbose = 0)\nkfold = StratifiedKFold(n_splits = 3)\nresults = cross_val_score(estimator, X_train, Y_train, cv = kfold)","f214bf5b":"train_history = estimator.fit(X_train, Y_train, epochs = 600, batch_size = 32)","63e83163":"print(train_history.history.keys())","787cee2a":"import matplotlib.ticker as ticker\n\n\nfig = plt.figure(figsize=(22,8))\nhist = sns.lineplot(data=train_history.history['accuracy'], color=\"darkturquoise\", label='Accuracy')\nhist = sns.lineplot(data=train_history.history['loss'], color=\"chocolate\", label='Loss')\nhist = sns.lineplot(data=train_history.history['recall'], color=\"indianred\", label='Recall')\n\ntitle = fig.suptitle(\"ACCURACY VS LOSS VS RECALL CUREVES\", x=0.125, y=1.01, ha='left',\n             fontweight=100, fontfamily='Lato', size=37)\n\nhist.xaxis.set_major_locator(ticker.MultipleLocator(20))\nhist.xaxis.set_major_formatter(ticker.ScalarFormatter())\n\nplt.legend()\nplt.show()\n","85fe983b":"y_preds = estimator.predict(test)\nsubm = pd.read_csv(\"..\/input\/titanic\/gender_submission.csv\", index_col='PassengerId')\nsubm['Survived'] = y_preds.astype(int)\nsubm.to_csv('submission_ann.csv')","c4e8c1f2":"Both passengers are female who paid 80 dollars as fare for their tickets. They have the same ticket and cabin, so they probably had to board at the same place. According to the distribution above, the more probable embarked value for them is Cherbourg (C). Let's replace it with C. ","cd4add01":"With this first exploration, we can see that :\n* Only aproximately 35% of passengers survived \n* More than the half of passengers are in the lowest class (pclass = 3)\n* Most of the fare tickets are below 50\n* Majority of passengers are alone (sibsp and parch)","e02822b1":"# Developing a Feed forward NN","1a29625a":"# Linear Support Vector Machine\n","3f1b1c6e":"I have explained the Exploratory Data Analysis and the Feature Engineering that was carried out to best understand the data and also to better train the Neural Network. \n\nI started modeling the data with logistic regression to see how it performs and then gradually moved upwards from there and tried different algorithms KNN, SVM, Decsion trees, Random forest and xGBoost. I submitted th results of the Random forest and also an ensemble techique where I combined te results from all the models. The best score I got was 0.75 which is not good so I tried an ANN afer that. \n\n**Best results : 79.186% accuracy (Neural Network)\n**\n\nPlease upvote and comment if you like my work :)","4a10cfa2":"Next drop null values from Embarked (only 2 values). \nBelow is the distribution of Embarked according to Fare and sex, and the two observations with missing \"Embarked\" value. Let's look at there two observations and choose the best matching embarked value according to their fare value and sex\n","305c9c29":"Here we see clearly, that Pclass is contributing to a persons chance of survival, especially if this person is in class 1.","ca53a63e":"# Scaling Dataset","b8fed95c":"Welcome to this Titanic project. This is my first kernel at Kaggle. In this notebook, I'm going to predict wether a passenger of the famous boat will survive or not. Firstly, I will display some feature analyses then ill focus on the feature engineering. Last part concerns modeling and predicting the survival on the Titanic using an voting procedure. If you find a way to improve it I encourage you to fork this notebook and contribute by adding a better solution. ","d9a75f4a":"# Modeling \n","5e170cb0":"As expected, random forest outperforms all of the other algorithms while naive baes is at bottom of the list. ","111f4e3b":"The Embarked feature has only 2 missing values, which can easily be filled. It will be much more tricky, to deal with the \u2018Age\u2019 and 'Cabin' features, which have 19% and 77% missing values. We wil deal with them later on in feature engineering section.","61ad140f":"# Understanding Data with plots","c77906bd":"# K Nearest Neighbor","7975bbf3":"# Finding the best model \n","ef99c051":"# Logistic Regression\nWe'll start with logistic regression because it is one of the simplest and most basic model used for testing. ","b48e3122":" # Data Pre processing","ed486c58":"0 is for male, 1 for female. It is clear that female survival rate is more than male's. ","d54f4a9c":"We have four features that are categorical in nature (Dtype object) and fare is float type. We need to transform our variables. We will drop features that we do not need anymore.","e29220f2":"# Data Exploration","9d8d800c":"# Feature Engineering","985a1919":"# Missing Data ","62b18214":"If you found this notebook helpful or you just liked it , some upvotes would be very much appreciated.","56cd5c7c":"# Decision Tree","ad838af6":"# XGBoost","6835d9fc":"We can see that some of the features have missing values. We need to examine the features that have missing values and the percnetage of the missing values so we can deal with those. Let's take a look.  ","82ccb56b":"# Random Forest","b6ee1795":"Thank you for your reading, feel free to fork this kernel and improve it. There is still room for improvement, like doing a more extensive feature engineering, by comparing and plotting the features against each other and identifying and removing the noisy features. Another thing that can improve the overall result on the kaggle leaderboard would be a more extensive hyperparameter tuning on several machine learning models.","5e0e46c9":"Trying several models","955bf840":"Here I import the data. For this analysis, I will be exclusively working with the Training set. We will be validating based on data from the training set as well. For our final submissions, I will make predictions based on the test set.\n\n","f039e177":"A cabin number looks like \u2018C123\u2019 and the letter refers to the deck. Therefore we\u2019re going to extract these and create a new feature, that contains a persons deck. We will then convert the feature into a numeric variable. The missing values will be converted to zero. \nUsing SibSp and Parch, new features 'number of relatives' and 'not_alone' are created. \nWe will use the Name feature to extract the Titles from the Name, so that we can build a new feature out of that.","518c6a79":"# Import libraries","1c7ba5a4":"First, let's drop PassengerId from the train set, because it does not contribute to a persons survival probability. I'll keep it in the test set, since it is required for submission.","4640db08":"# Titanic Project Example Walk Through\n"}}