{"cell_type":{"e50f08ae":"code","e715fe65":"code","8a5097e7":"code","763a41a8":"code","c621e24b":"code","e536eed7":"code","617b91cb":"code","8214d3b3":"code","cb98fa3b":"code","abe876db":"code","1281ac58":"code","6b8e7e9b":"code","4871ac96":"code","46bd404d":"code","ffa3fff6":"code","b2e7e9b0":"code","2f3939c9":"code","69d6aa3a":"code","e5165742":"code","48d35f7a":"code","d756b632":"code","5e9b079f":"code","98da7dab":"code","be4b1c0c":"markdown","7e84b7d5":"markdown","5d6b0353":"markdown","a9de5926":"markdown","ad8bd1e3":"markdown","5a1cd322":"markdown","9920b049":"markdown","fb098898":"markdown","9a86e7a1":"markdown","aa157997":"markdown","c147b62f":"markdown","41a022bf":"markdown","7b39ae97":"markdown","205946c1":"markdown","2f193b9f":"markdown","938bc7f9":"markdown","79d042ef":"markdown","12c592f5":"markdown","5541a3ea":"markdown","04e41692":"markdown","e81f32f6":"markdown","8ac6c779":"markdown","4c40fb40":"markdown","d76e064d":"markdown","7d9cc00e":"markdown"},"source":{"e50f08ae":"import math\nimport numpy as np\nimport pandas as pd\nimport geopandas as gpd\nfrom shapely.geometry import Point\nfrom shapely.ops import nearest_points\nimport networkx as nx\nfrom tqdm import tqdm_notebook as tqdm\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set()\nsns.set_style(\"whitegrid\")\n\n\nimport folium\nfrom folium import Choropleth, Circle, Marker\nfrom folium.plugins import HeatMap, MarkerCluster\n\nfrom itertools import tee\n\ndef pairwise(iterable):\n    \"s -> (s0,s1), (s1,s2), (s2, s3), ...\"\n    a, b = tee(iterable)\n    next(b, None)\n    return zip(a, b)","e715fe65":"# just downloading the dataset, which has all the necessary data already\ndf = pd.read_csv(\"..\/input\/coronavirus-latlon-dataset\/coronavirus_cleaned_21Jan2Feb.csv\", index_col=0)\n# WARNING: set the coordinates for the province of Hubei to its capital Wuhan, instead of the geo-pyshical center.\n# It helps to match the routes more precisely, at least from the epicenter to the rest of the world.\ndf.loc[df[\"Province\/State\"]==\"Hubei\", [\"lon\", \"lat\"]] = [114.283333, 30.583332]\n# Cast into GeoDataFrame\ndf = gpd.GeoDataFrame(df, geometry=[Point(xy) for xy in zip(df['lon'], df['lat'])])\n# Set coordinate reference system\ndf.crs = {'init' :'epsg:4326'}  \ndf.head(3)","8a5097e7":"columns_and_dtypes = {\n    'Airline': \"category\", # 2-letter (IATA) or 3-letter (ICAO) code of the airline.\n    'Airline ID': \"category\", # Unique OpenFlights identifier for airline.\n    'Source Airport': \"category\", # 3-letter (IATA) or 4-letter (ICAO) code of the source airport.\n    'Source Airport ID': \"category\", # Unique OpenFlights identifier for source airport.\n    'Destination Airport': \"category\", # 3-letter (IATA) or 4-letter (ICAO) code of the destination airport.\n    'Destination Airport ID': \"category\", # Unique OpenFlights identifier for destination airport.\n    'Codeshare': \"category\", # \"Y\" if this flight is a codeshare (that is, not operated by Airline, but another carrier), empty otherwise.\n    'Stops': int, # Number of stops on this flight (\"0\" for direct)\n    'Equipment': \"category\" # 3-letter codes for plane type(s) generally used on this flight, separated by spaces\n}\nroutes = pd.read_csv('..\/input\/openflights-20200201-dump\/routes.dat', \n                     names=columns_and_dtypes.keys(),\n                     dtype=columns_and_dtypes)\ndisplay(routes.head(3))","763a41a8":"columns_and_dtypes = {\n    'Airport ID': \"category\", # Unique OpenFlights identifier for this airport.\n    'Airport': \"category\", # Name of airport. May or may not contain the City name.\n    'City': \"category\", # Main city served by airport. May be spelled differently from Name.\n    'Country': \"category\", # or territory where airport is located. See Countries to cross-reference to ISO 3166-1 codes.\n    'IATA': \"category\", # 3-letter IATA code. Null if not assigned\/unknown.\n    'ICAO': \"category\", # 4-letter ICAO code. Null if not assigned.\n    'Latitude': float, # Decimal degrees, usually to six significant digits. Negative is South, positive is North.\n    'Longitude': float, # Decimal degrees, usually to six significant digits. Negative is West, positive is East.\n    'Altitude': float, # In feet.\n    'Timezone': \"category\", # Hours offset from UTC. Fractional hours are expressed as decimals, eg. India is 5.5.\n    'DST': \"category\", # Daylight savings time. One of E (Europe), A (US\/Canada), S (South America), O (Australia), Z (New Zealand), N (None) or U (Unknown). See also: Help: Time\n    'Tz database time zone': \"category\", # Timezone in \"tz\" (Olson) format, eg. \"America\/Los_Angeles\".\n    'Type': \"category\", # Type of the airport. Value \"airport\" for air terminals, \"station\" for train stations, \"port\" for ferry terminals and \"unknown\" if not known. In airports.csv, only type=airport is included.\n    'Source': \"category\", # Source of this data.\n}\nairports = pd.read_csv('..\/input\/openflights-20200201-dump\/airports.dat', \n                     names=columns_and_dtypes.keys(),\n                     dtype=columns_and_dtypes)\n# Cast into GeoDataFrame\nairports = gpd.GeoDataFrame(airports, geometry=[Point(xy) for xy in zip(airports['Longitude'], airports['Latitude'])])\n# Set coordinate reference system\nairports.crs = {'init' :'epsg:4326'}  \nairports.head(3)","c621e24b":"airports[\"Type\"].value_counts()","e536eed7":"# Number of airports by country. \nairports[\"Country\"].value_counts().head(10)","617b91cb":"# Number of airports by chinese city\nairports.loc[airports[\"Country\"]==\"China\", \"City\"].value_counts().head(10)","8214d3b3":"# Add the prefix source to the column names\nsrc_airports = airports[['Airport ID', 'City', 'Country', 'Latitude', 'Longitude', 'Altitude']]\nsrc_airports.columns = 'Source ' + src_airports.columns\n# Add the prefix destination to the column names\ndst_airports = airports[['Airport ID', 'City', 'Country', 'Latitude', 'Longitude', 'Altitude']]\ndst_airports.columns = 'Destination ' + dst_airports.columns\n# Add source coordinates to routes dataset\nflights = pd.merge(\n    routes[['Airline', 'Source Airport ID', 'Destination Airport ID', 'Stops']], \n    src_airports, \n    on=\"Source Airport ID\", \n    how=\"left\",\n)\n# Add destination coordinates to routes dataset\nflights = pd.merge(\n    flights, \n    dst_airports, \n    on=\"Destination Airport ID\", \n    how=\"left\",\n)\n# There are some unknown destination or source airports... I will drop them\nflights = flights[~flights.isna().any(axis=1)].reset_index(drop=True).copy()\nflights[\"Route ID\"] = flights.index.copy()\n\ndef haversine(lat1, lon1, lat2, lon2, earth_radius=6378*1e3):\n    \"\"\"\n    Need to compute distance from lon and lat to deal with mercator discontinuities (-180 -> 180, -90 - >90).\n    \"\"\"\n    lat1, lon1, lat2, lon2 = np.radians([lat1, lon1, lat2, lon2])\n\n    a = np.sin((lat2-lat1)\/2.0)**2 + \\\n        np.cos(lat1) * np.cos(lat2) * np.sin((lon2-lon1)\/2.0)**2\n\n    return earth_radius * 2 * np.arcsin(np.sqrt(a))\n\nflights[\"Distance\"] = haversine(flights[\"Source Latitude\"], flights[\"Source Longitude\"], flights[\"Destination Latitude\"], flights[\"Destination Longitude\"])","cb98fa3b":"flights.sample(3)","abe876db":"fig, ax = plt.subplots(1, 1, figsize=(10, 3))\nax = sns.distplot(flights[\"Distance\"]\/1000, norm_hist=True, ax=ax)\nax.set_xlabel(\"Route length (km)\", fontsize=16);\nstart, end = ax.get_xlim();\nax.xaxis.set_ticks(np.arange(0, end, 1500));","1281ac58":"# Save complete routes dataset with coordinates, it may be useful for other kernels\nflights.to_csv(\"openflights_routes.csv\", index=False)","6b8e7e9b":"wuhan = flights[flights[\"Source City\"]==\"Wuhan\"].copy()\nprint(f\"{len(wuhan)} routes departing from Wuhan.\")\ndisplay(wuhan.head())\nprint(\"Top ten destination countries:\")\ndisplay(wuhan[\"Destination Country\"].value_counts()[:10])\nprint(\"Top ten destination cities:\")\ndisplay(wuhan[\"Destination City\"].value_counts()[:10])","4871ac96":"display(flights[\"Stops\"].value_counts())\ndisplay(flights[flights[\"Stops\"]>0])","46bd404d":"# Project airports dataset to world mercator. The bounds of this projection are from -180.0 -80.0 to 180.0 84.0 degrees.\n# Therefore, we have to filter the airport data accordingly, by rejecting 4 airports (from Antarctica, New Zealand, Russia and Canda).\nEPSG = \"epsg:3832\" # epsg:3395\ncolumns = ['Airport ID', 'City', 'Country', 'geometry']\nkeep_latitude = (airports[\"Latitude\"] > -80) & (airports[\"Latitude\"] < 80)\n# Display rejected airports.\nprint(f\"Rejecting airports that cannot be reprojected into {EPSG}\")\ndisplay(airports.loc[~keep_latitude, columns])\n# Filter airports and project into mercator.\nairports_mercator = airports.loc[keep_latitude, columns].to_crs({'init': EPSG}).copy()\n# Reproject also corona dataset\ncorona_with_airports = df.to_crs({'init': EPSG}).copy()\n# Keep only last update per province\ncorona_with_airports = corona_with_airports.sort_values(\"Last Update\", ascending=False).drop_duplicates([\"lat\", \"lon\"], keep=\"first\")\n# Find the nearest airport for the coronavirus dataset (I know, I know, it is zero-optimized, TOO SLOW...)\npts = airports_mercator.geometry.unary_union\ndef near(point):\n    \"\"\"\n    Credit from:\n    https:\/\/gis.stackexchange.com\/questions\/222315\/geopandas-find-nearest-point-in-other-dataframe\n    \"\"\"\n    # find the nearest point and return the corresponding 'Airport ID'\n    geom_1, geom_2 = nearest_points(point, pts)\n    nearest = airports_mercator.geometry == geom_2\n    nearest_feats = airports_mercator.loc[nearest, ['City', 'Airport ID']].iloc[0].tolist() + [geom_1.distance(geom_2)]\n    return nearest_feats\n(corona_with_airports['Airport City'], \n corona_with_airports['Nearest Airport'], \n corona_with_airports[\"Distance to Airport\"]) = zip(*corona_with_airports[\"geometry\"].apply(near))","ffa3fff6":"fig, ax = plt.subplots(1, 1, figsize=(10, 3))\nax = sns.distplot(corona_with_airports[\"Distance to Airport\"]\/1000, norm_hist=True, ax=ax)\nax.set_xlabel(\"Distance to airport (km)\", fontsize=16);","b2e7e9b0":"corona_with_airports[corona_with_airports[\"Distance to Airport\"]>200*1e3]","2f3939c9":"airport_counts = corona_with_airports['Nearest Airport'].value_counts()\ntarget_airports = airports_mercator[airports_mercator[\"Airport ID\"].isin(airport_counts.index)]\ntarget_airports = pd.merge(target_airports, \n                           airport_counts.to_frame(\"Freq\").reset_index().rename(columns={\"index\":\"Airport ID\"}),\n                           on=\"Airport ID\", \n                           how=\"left\")\ntarget_airports = target_airports.sort_values(\"Freq\", ascending=False).reset_index(drop=True).copy()","69d6aa3a":"# Simple debug plot\nrename_countries ={\n    \"Mainland China\": \"China\",\n    \"Hong Kong\": \"China\",\n    \"Macau\": \"China\",\n    \"United States\": \"United States of America\",\n    \"US\": \"United States of America\",\n    \"UK\": \"United Kingdom\",\n    \"Singapore\": \"Malaysia\",\n    \"Ivory Coast\": \"C\u00f4te d'Ivoire\"\n}\nfig, ax = plt.subplots(1, 1, figsize=(20, 20))\nworld = gpd.read_file(gpd.datasets.get_path('naturalearth_lowres'))\nworld = world[(world.name != \"Antarctica\") & (world.name != \"Fr. S. Antarctic Lands\")].to_crs({'init': EPSG})\nworld[\"active\"] = world[\"name\"].isin(df[\"Country\/Region\"].apply(lambda x: rename_countries.get(x, x)).unique())\nax = world.plot(ax=ax, column=\"active\", edgecolor='black')\nax = corona_with_airports.plot(ax=ax, marker='o', color='red', markersize=20, label=\"Provinces with coronavirus\")\nax = target_airports.plot(ax=ax, marker='x', color='green', markersize=20, label=\"Nearest Airports\")\nax.set_ylim(-1.e7, 1.05e7);\nax.set_xlim(-2.e7, 2.e7);\nax.legend(fontsize=16);","e5165742":"airports[airports[\"City\"]==\"Wuhan\"]","48d35f7a":"def get_routes_and_distance_for_path(net, path):\n    \"\"\"Compute the distance in km for a particular path using flights dataframe.\"\"\"\n    routes = [net.get_edge_data(src, dst)[\"Route ID\"] for src, dst in pairwise(path)]\n    distance = flights.loc[flights[\"Route ID\"].isin(routes), \"Distance\"].sum()\/1e3\n    return routes, distance     \n    \nMAX_ABSOLUTE_NUM_STOPS = 3\nHEURISTIC_ROUTE_TH = 0.2\n\n# Creates graph from edges -> flights dataset\nG = nx.from_pandas_edgelist(flights, \"Source Airport ID\", \"Destination Airport ID\", edge_attr=[\"Route ID\", \"Distance\"])\n# Add nodes information -> airports dataset\nairports_mercator[\"pos\"] = airports_mercator[\"geometry\"].apply(lambda x: list(list(x.coords)[0]))\nairports_mercator[\"stops_to_wuhan\"] = np.iinfo(np.int).max\nairports_mercator[\"related_to_wuhan\"] = False\nnode_attr = airports_mercator.set_index('Airport ID').to_dict('index')\nnx.set_node_attributes(G, node_attr)\n# Compute number of stops to arrive at Wuhan\nwuhan_airport_id = corona_with_airports.loc[corona_with_airports[\"Province\/State\"]==\"Hubei\", \"Nearest Airport\"].iloc[0]\nnodes_related_to_wuhan_by_stops = {num_stops:[] for num_stops in range(0, MAX_ABSOLUTE_NUM_STOPS + 1)}\nedges_related_to_wuhan_by_stops = {num_stops:[] for num_stops in range(0, MAX_ABSOLUTE_NUM_STOPS + 1)}\ndiff_rates = []\ncount_win_distance = 0\ncount_win_stops = 0\nfor node in tqdm(G.nodes()):\n    try:\n        # Compute two kinds of shortest paths and choose one of them.        \n        shortest_stops_path = nx.shortest_path(G, source=wuhan_airport_id, target=node)\n        # km from A to B using min number of stops path\n        stops_routes, shortest_stops_length = get_routes_and_distance_for_path(G, shortest_stops_path) \n        shortest_distance_path = nx.shortest_path(G, source=wuhan_airport_id, target=node, weight=\"Distance\")\n        # km from A to B using min length path\n        length_routes, shortest_distance_length = get_routes_and_distance_for_path(G, shortest_distance_path) \n        # Compute percentual difference between routes\n        diff_rate = (shortest_stops_length - shortest_distance_length)\/shortest_distance_length\n        diff_rates.append(diff_rate)\n        if  diff_rate < HEURISTIC_ROUTE_TH:\n            path = shortest_stops_path\n            distance = shortest_stops_length\n            count_win_stops += 1\n        else:\n            path = shortest_distance_path\n            distance = shortest_distance_length\n            count_win_distance += 1\n        num_stops = len(path) - 2 # Wuhan -> Paris = 0 Stops\n        G.nodes[node]['stops_to_wuhan'] = num_stops\n        G.nodes[node]['distance_to_wuhan'] = distance\n        if num_stops <= MAX_ABSOLUTE_NUM_STOPS:\n            if num_stops >= 0:\n                # Avoid self loop: Wuhan -> Wuhan                \n                nodes_related_to_wuhan_by_stops[num_stops] += path\n                edges_related_to_wuhan_by_stops[num_stops] += pairwise(path)\n            for n in path:\n                G.nodes[n]['related_to_wuhan'] = True\n    except nx.NetworkXNoPath:\n        pass","d756b632":"# uniquify\nnodes_related_to_wuhan_by_stops = {stops:set(nodes) for stops, nodes in nodes_related_to_wuhan_by_stops.items()}\n# Create subgraphs\nsubgraphs_by_stops = {}\nfor num_stops in range(1, MAX_ABSOLUTE_NUM_STOPS + 1):\n    subg = nx.Graph()\n    subg.add_nodes_from(nodes_related_to_wuhan_by_stops[num_stops])\n    subg.add_edges_from(edges_related_to_wuhan_by_stops[num_stops])\n    subgraphs_by_stops[num_stops] = subg.copy()\n# Create dictionary of subgraphs at a certain number of stops\nprint(\"Routes network:\")\nprint(nx.info(G))","5e9b079f":"diffs = pd.Series(diff_rates, dtype=np.float) # there's one nan\ndiffs = diffs[~diffs.isna()] * 100\n\nfig, ax = plt.subplots(1, 3, figsize=(16, 2))\nsns.barplot([\"Less distance\", \"Fewer stops\"], [count_win_distance, count_win_stops], ax=ax[0]);\nax[0].set_ylabel(\"Number of wins [#]\", fontsize=14);\nsns.distplot(diffs, norm_hist=True, ax=ax[1])\nax[1].set_xlabel(\"Distance rate\\nbetween routes [%]\", fontsize=14);\nsns.distplot(diffs[diffs<200], norm_hist=True, ax=ax[2])\nax[2].set_xlabel(\"Distance rate\\nbetween routes zom < 200% [%]\", fontsize=14);\nstart, end = ax[2].get_xlim();\nax[2].xaxis.set_ticks(np.arange(0, end, 50));","98da7dab":"# Plot Wuhan network\nnode_to_pos = {node:attrs[\"pos\"] for node, attrs in node_attr.items()}\n\nfor num_stops in range(0, MAX_ABSOLUTE_NUM_STOPS + 1):\n    fig, ax = plt.subplots(1, 1, figsize=(20, 20))\n    ax = world.plot(ax=ax, column=\"active\", edgecolor='black')\n    ax = corona_with_airports.plot(ax=ax, marker='o', color='red', markersize=26, label=\"Provinces with coronavirus\")\n    # Keep only edges connected into Wuhan    \n    nx.draw(\n        G,\n        nodelist=nodes_related_to_wuhan_by_stops[num_stops], \n        edgelist=edges_related_to_wuhan_by_stops[num_stops], \n        pos=node_to_pos,\n        ax=ax,\n        node_size=2,\n        arrowsize=2,\n        edge_color=\"blue\",\n        style=\"dashed\"\n    )\n    ax.set_ylim(-1.e7, 1.05e7);\n    ax.set_xlim(-2.e7, 2.e7);\n    ax.set_title(f\"Wuhan connections at {num_stops} stop\/s\", fontsize=16)\n    plt.show()","be4b1c0c":"## Plot network of routes related to Wuhan\n___\n\nPlot network of routes related to Wuhan at different number of stops (node distances).\n\n**Disclaimer**: routes from North America to West Europe are not properly plotted because of the geospatial projection. They cross the whole map... but I would rather prefer a pacman effect.","7e84b7d5":"## Create network of routes\n___\n\nCreate a graph of routes in order to find the shortest path between Wuhan and the rest of provinces.","5d6b0353":"250km still makes sense to me.... But honestly, I am not sure about that these russian guys arriving at Yerbogachen. We know that the coordinates of \"Province\/State\" are coarse grained. In general the only information that we really have is not the city, and rather the country, so the coordinates of both, the provinces and airports, are not precise. \n\nFor instance, the coronavirus in Spain appeared at La Gomera (Canary Islands), even though our dataset locates it in Madrid.","a9de5926":"## Reading OpenAirflights data\n___\n\nThe OpenFlights dataset is already available at [kaggle](https:\/\/www.kaggle.com\/open-flights\/airline-database), but I have created a new kaggle dataset using updated data from the [original sources](https:\/\/github.com\/jpatokal\/openflights).","ad8bd1e3":"## Routes EDA\n___\nAnalisys of routes departing from Wuhan.","5a1cd322":"Following up on the great [Mykola's kernel](https:\/\/www.kaggle.com\/grebublin\/coronavirus-propagation-visualization-forecast), I have matched the coronavirus propagation with the information of the [OpenFlights dataset](https:\/\/openflights.org\/data.html). This is the most complete open airflights routes dataset I have found, even though the last updated date back to 2014. In fact, the dataset webpage warnings that \"the third-party that OpenFlights uses for route data ceased providing updates in June 2014. The current data is of historical value only\". \n\nTherefore, we cannot expect a precise analysis here, since the airflights routes may have dramatically change. I am not an expert on this field, and maybe some of you may shed light on this matter. \n\n![](http:\/\/)Moreover, this dataset does not provide information of the actual traffic of these routes. It describes the routes (airline X operating from airport A to airport B), but it does not account for the passenger flux of these routes, nor is the date time information available. Thus, most of the kernel prepares the OpenFlights dataset for the visualization.","9920b049":"Top ten is sorted according to the number of combinations of airlines and destination airports of a particular place. For instance, there are 9 different combinations of airline + destination airport that departure from Wuhan and arrive to Shanghai.\n\nIt does not mean number of flights nor amount of passengers, though it may be related to.","fb098898":"## Proof of concept\n___\n\nBefore writting a single line of code, I want to check whether there is some kind of correlation between the propagation of the virus (during 2019 and 2020) and the airflights routes (till 2014). To do so, I just use the [OpenFlights interface](https:\/\/openflights.org\/) and I search the routes for the Wuhan Tianhe International Airport.\n\nComparing coronavirus propagation and the flights departing from Wuhan,  we can observe that the two figures roughly match one another:\n\n- Most regions far from the epicenter are connected through a direct flight route from Wuhan.\n\n- We do not see any connection from Wuhan to North America, neither to Oceania nor Africa. We guess that there is an indirect connection between these places. \n\n<img src=\"https:\/\/drive.google.com\/uc?id=1piaVoYDrLLle4ROvjELGNJ-l4XP9Qex3\" width=\"1000px\">\n<img src=\"https:\/\/drive.google.com\/uc?id=1ONHcu2f5pIa5A_SRcZojN8npi0vepT7D\" width=\"1000px\">","9a86e7a1":"#### Number of stops on this flight (\"0\" for direct)\n\nLet's analyze the variable stops for the whole routes dataset","aa157997":"<img src=\"https:\/\/openflights.org\/demo\/openflights-routedb-2048.png\" width=\"1000px\">","c147b62f":"# Coronavirus propagation and airflights routes","41a022bf":"## TODO\n\nThis is an incomplete kernel. There is still a lot of work to be done... starting by combining mykola's visualization with this airfligts data.","7b39ae97":"## TODO: Mykola's Visualization\n___\n","205946c1":"#### Airports\n\nI use the dataset airports.csv, therefore, I only consider \"airport\" terminals. I discard on the other train stations, ferry terminals and unknown terminals.","2f193b9f":"#### Routes from A to B","938bc7f9":"Plot winning criteria stats for debugging purposes","79d042ef":"Unfortunately, the \"stops\" variable does not provide further information...","12c592f5":"## Reading coronavirus data\n___\nBig thanks to [Brenda So](https:\/\/www.kaggle.com\/brendaso) for her original coronavirus [dataset](https:\/\/www.kaggle.com\/brendaso\/2019-coronavirus-dataset-01212020-01262020) and to [Mykola](https:\/\/www.kaggle.com\/grebublin) for his geocoded [dataset](https:\/\/www.kaggle.com\/grebublin\/coronavirus-latlon-dataset). \n\nI will use here the geocoded dataset.","5541a3ea":"### Debug mapping of province into airport","04e41692":"## Conclusions\n___\n\nRecently, the Institute for Theoretical Biology from the University of Berlin has pusblished a incredible work for the [Coronavirus Global Risk Assessment](http:\/\/rocs.hu-berlin.de\/corona\/) . They estimate the likelihood of importing a case from an affected location to an airport or country distant from the outbreak location. I strongly recommend going throught the description of the dataset and the statistical model. They provide wonderful visualizations and insights as well.\n\nFor this work, they have used a \"closed\" dataset I think, collected from https:\/\/www.oag.com\/. I hope we all could access these data :(\n\n","e81f32f6":"First, create the actual network of routes. Then, find a route to Wuhan from every other node:\n\n1. Find the shortest route in number of stops.\n2. Find the shortest route in distance length (km).\n3. Choose one of the two using the following heuristic: if the difference in distance (km) of the shortest stops route and the shortest distance route is bellow a threshold select the one with the minimum number of stops.","8ac6c779":"As you can see airports are close to the provinces.","4c40fb40":"## Join coronavirus & airports datasets\n___\n\nIt is hard to join both datasets, since the coordinates of the coronavirus dataset are very coarse grained. I mean, for so many cases, we only know the country or state. On the other hand, the coordinates of the airports are really fined grained.","d76e064d":"#### Distance thresholding\n\n\nI check the distances from the \"Province\" to the \"Nearest airport\", maybe I need to filter out airports that are too far away from the coronavirus province... Please, keep in mind that distances are inexact because of epsg:3395 trasform.\n","7d9cc00e":"### Imports"}}