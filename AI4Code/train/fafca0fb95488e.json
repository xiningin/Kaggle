{"cell_type":{"b3473934":"code","8dd31a6d":"code","0a56dd7a":"code","42ae1ba0":"code","3b8a26c3":"code","4714fc1c":"code","82f4bc78":"code","3b148429":"code","d0ec72a7":"code","52239e19":"code","7d41aa91":"code","2d4ac70d":"code","dd23653d":"code","4368eb7b":"code","f27dbd97":"code","45f4b482":"code","eb42630c":"markdown","983c9609":"markdown","f0233df3":"markdown","4674215a":"markdown","4a7cfdf0":"markdown","f231f7a6":"markdown","8f6daee9":"markdown","71fabb41":"markdown","0571037a":"markdown","e912cc92":"markdown","720527db":"markdown","03d84dbb":"markdown","d30e95a0":"markdown","5ed18503":"markdown","b033e5ac":"markdown","0b952bf0":"markdown"},"source":{"b3473934":"import tensorflow as tf\n\nimport numpy as np\nimport os\nimport time\n\ndata = open('..\/input\/597-poems-by-emily-dickinson\/final-emily.csv','rb')\ncorpus = data.read().decode(encoding='utf-8').strip()\nvocab = sorted(set(corpus))","8dd31a6d":"print ('Total characters:', len(corpus))\nprint ('Unique characters', len(vocab))","0a56dd7a":"character_to_index = {u:i for i, u in enumerate(vocab)}\nindex_to_character = np.array(vocab)","42ae1ba0":"corpus_int = np.array([character_to_index[c] for c in corpus])","3b8a26c3":"seq_length = 100\nexamples_per_epoch = len(corpus)\/\/(seq_length+1)\n\nchar_dataset = tf.data.Dataset.from_tensor_slices(corpus_int)\nsequences = char_dataset.batch(seq_length+1, drop_remainder=True)","4714fc1c":"def split_input_target(chunk):\n  input_text = chunk[:-1]\n  target_text = chunk[1:]\n  return input_text, target_text\n\ndataset = sequences.map(split_input_target)","82f4bc78":"BATCH_SIZE = 64\nBUFFER_SIZE = 10000\nvocab_size = len(vocab)\nembedding_dim = 256\nrnn_units = 1024","3b148429":"dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)","d0ec72a7":"model = tf.keras.Sequential([\n    tf.keras.layers.Embedding(vocab_size, embedding_dim,\n                              batch_input_shape=[BATCH_SIZE, None]),\n    tf.keras.layers.GRU(rnn_units,\n                        return_sequences=True,\n                        stateful=True,\n                        recurrent_initializer='glorot_uniform'),\n    tf.keras.layers.Dense(len(vocab))\n])\n    \nmodel.summary()","52239e19":"checkpoint_dir = '.\/tmp'\n\ncheckpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n\ncheckpoint_callback=tf.keras.callbacks.ModelCheckpoint(\n    filepath=checkpoint_prefix,\n    save_weights_only=True)","7d41aa91":"def loss_fn(labels, logits):\n    return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)","2d4ac70d":"model.compile(optimizer='adam', loss=loss_fn)\nhistory = model.fit(dataset, epochs=100, callbacks=[checkpoint_callback])","dd23653d":"def generate_text(model, start_string):\n  num_generate = 15\n\n  input_eval = [character_to_index[s] for s in start_string]\n  input_eval = tf.expand_dims(input_eval, 0)\n\n  text_generated = []\n\n  temperature = 1.0\n\n  model.reset_states()\n    \n  while(num_generate > 0):\n    predictions = model(input_eval)\n    predictions = tf.squeeze(predictions, 0)\n\n    predictions = predictions \/ temperature\n    predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n\n    input_eval = tf.expand_dims([predicted_id], 0)\n    text_generated.append(index_to_character[predicted_id])\n    if index_to_character[predicted_id]=='\\n':\n        num_generate -= 1\n\n  return (start_string + ''.join(text_generated)).strip()","4368eb7b":"model = tf.keras.Sequential([\n    tf.keras.layers.Embedding(len(vocab), embedding_dim,\n                              batch_input_shape=[1, None]),\n    tf.keras.layers.GRU(rnn_units,\n                        return_sequences=True,\n                        stateful=True,\n                        recurrent_initializer='glorot_uniform'),\n    tf.keras.layers.Dense(len(vocab))\n])\nmodel.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\nmodel.build(tf.TensorShape([1, None]))","f27dbd97":"print(generate_text(model, start_string=u\"Love \"))","45f4b482":"print(generate_text(model, start_string=u\"Flower \"))","eb42630c":"---\n\n### Generate poem starting with string 'Love'","983c9609":"### Convert text corpus to dataset","f0233df3":"### Procedure to generate text given starting string","4674215a":"### Set checkpoint directory and filename","4a7cfdf0":"### Load model and change input shape","f231f7a6":"### Create loss function","8f6daee9":"### Create dictionary mapping characters to integers, and vice versa","71fabb41":"### Compile model and train on dataset","0571037a":"### Generate poem starting with string 'Flower'","e912cc92":"### Convert text corpus to integer representation","720527db":"### Shuffle the dataset","03d84dbb":"### Total and unique character counts","d30e95a0":"### Convert dataset to input->output pairs","5ed18503":"### Create GRU model","b033e5ac":"# GRU-based RNN gets inspired by Emily Dickinson and writes poems\n\nWe will use a Recurrent Neural Network (RNN) made with Gated Recurrent Unit (GRU) to generate poetry after learning using the **597 poems by Emily Dickinson** dataset.\n\nWe use character-based prediction, wherein given a string, the model would output the next character that should follow the given string. We use this strategy to iteratively predict characters until 15 lines of poetry is generated, hoping that neural network outputs resemble a poem by Emily Dickinson.\n\nNotice that because of character-based prediction sometimes meaningless words are also generated while generating the poetry. The generation of meaningless words can be removed by using a word-based prediction scheme (which we will undertake in another notebook).\n\nThis notebook uses the concepts described and demonstrated in the Tensorflow documentation ([Text generation with an RNN](https:\/\/www.tensorflow.org\/tutorials\/text\/text_generation)). Please refer to the tutorial for better understanding of the notebook.\n\n\n### Import required modules and load dataset","0b952bf0":"### Set hyperparameters for GRU model"}}