{"cell_type":{"5b16745d":"code","9f9dfbcf":"code","aa9abaf0":"code","f0fd6005":"code","3b40a351":"code","29ab4ddd":"code","fd8d4858":"code","e7933328":"code","c13ac9d2":"code","0036447a":"code","2f6965d9":"code","323bbc51":"code","5ab1546b":"code","7a155ef5":"code","714e4756":"markdown","05404099":"markdown","929fab18":"markdown","ca35cc5f":"markdown","30448039":"markdown","05f76c27":"markdown","38701086":"markdown","ea67cc63":"markdown","7899ed11":"markdown","d0221343":"markdown","e33205d0":"markdown","514c001f":"markdown","b62e52c9":"markdown","c687f5bc":"markdown","ddfcc013":"markdown","ae8600dd":"markdown"},"source":{"5b16745d":"import seaborn as sns\nimport scipy as sp\nimport pandas as pd\nimport numpy as np\nimport matplotlib\nimport matplotlib.pyplot as plt\nfrom sklearn.pipeline import Pipeline\nwines = pd.read_csv('..\/input\/red-wine-quality-cortez-et-al-2009\/winequality-red.csv')\nwines.head()","9f9dfbcf":"wines.describe()","aa9abaf0":"wines.info()","f0fd6005":"colors = [\"red\",\"blue\",\"green\",\"grey\",\"orange\",\"purple\"]\nseries = wines[\"quality\"].value_counts()\nsq = series.sort_index()\nfig = plt.figure()\nax = fig.add_axes([0,0,1,1])\nax.bar(sq.index,sq.values, color = colors)\nax.set_title(\"Wine quality\")\nplt.show()","3b40a351":"wines_corr = wines.corr()\nsns.heatmap(wines_corr, center = 0)\nplt.show()","29ab4ddd":"sns.pairplot(wines[[\"alcohol\",\"sulphates\",\"quality\"]], hue = \"quality\", palette = colors, height = 4.5, aspect = 1.3)\nplt.show()","fd8d4858":"wines_copy = wines.copy()\nconditions = [\n    wines_copy[\"quality\"]>=7,\n    (wines_copy[\"quality\"]<7) & (wines_copy[\"quality\"]>=5),\n    (wines_copy[\"quality\"]<5)\n]\nvalues = [2,1,0]\nwines_copy[\"simple_quality\"] = np.select(conditions, values)\n#wines_copy = wines_copy.astype({\"simple_quality\":int})\nwines_copy.drop(\"quality\", axis = 1, inplace = True)\n\ncolors1 = [\"red\",\"green\",\"blue\"]\nseries = wines_copy[\"simple_quality\"].value_counts()\nsq = series.sort_index()\nfig = plt.figure()\nqua = [\"0\",\"1\",\"2\"]\nax = fig.add_axes([0,0,1,1])\nax.bar(qua,sq.values, color = colors1)\nax.set_title(\"Simplified wine quality\")\nplt.show()","e7933328":"sns.pairplot(wines_copy[[\"alcohol\",\"sulphates\",\"simple_quality\"]], hue = \"simple_quality\", palette = colors1, height = 4.5, aspect = 1.3, markers = [\"o\",\"s\",\"D\"])\nplt.show()","c13ac9d2":"from collections import Counter\nfrom imblearn.over_sampling import RandomOverSampler\n\nX = wines_copy.drop(\"simple_quality\", axis = 1)\ny = wines_copy[\"simple_quality\"]\nprint('Original dataset shape %s' % Counter(y))\nros = RandomOverSampler(random_state= 42)\nX_res, y_res = ros.fit_resample(X, y)\nX_new = pd.concat([X_res,y_res],axis = 1)\nprint('Resampled dataset shape %s' % Counter(y_res))\n","0036447a":"series = X_new[\"simple_quality\"].value_counts()\nsq = series.sort_index()\nfig = plt.figure()\nqua = [\"0\",\"1\",\"2\"]\nax = fig.add_axes([0,0,1,1])\nax.bar(qua,sq.values, color = colors1)\nax.set_title(\"Simplified wine quality with balanced classes\")\nplt.show()","2f6965d9":"sns.pairplot(X_new[[\"alcohol\",\"sulphates\",\"simple_quality\"]], hue = \"simple_quality\", palette = colors1, height = 4.5, aspect = 1.3, markers = [\"o\",\"s\",\"D\"])\nplt.show()","323bbc51":"from sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score\n\nX_train, X_test, y_train, y_test = train_test_split(X_res, y_res, test_size=0.33, random_state=42)\npipe = Pipeline([('scaler',StandardScaler()),('RF',RandomForestClassifier(n_estimators=10))])\npipe.fit(X_train, y_train)\npipe.predict(X_test)\nprint(accuracy_score(y_test,pipe.predict(X_test)))","5ab1546b":"feature_names = [f\"{i}\" for i in wines_copy.columns[:-1]]\nforest_importances = pd.Series(pipe[1].feature_importances_, index=feature_names)\nfeatureImp= []\nfor feat, importance in zip(X_train.columns, pipe[1].feature_importances_):  \n    temp = [feat, importance*100]\n    featureImp.append(temp)\n\nfT_df = pd.DataFrame(featureImp, columns = ['Feature', 'Importance'])\nprint (fT_df.sort_values('Importance', ascending = False))","7a155ef5":"std = np.std([tree.feature_importances_ for tree in pipe[1].estimators_], axis=0)\nforest_importances.plot.bar(yerr=std,figsize = (10,4))\nplt.title(\"Feature importances\")\nplt.ylabel(\"Mean decrease in impurity\")\nplt.show()","714e4756":"The previous barplot conveys a lot of important info on the data:\n* classes 0,1 and 2 are empty, that is, there are no wines which have been labeled 0 or 1 or 2. As consumers we rejoice of that, as data scientists we decide to ignore these classes in the following analysis;\n* classes 5 and 6 are much more numerous than the others. This comes as no surprise as 5 and 6 are labels for average quality wines such as the ones that we can find at the supermarket. However, very unbalanced classes are challenging for classification algorithms and we will have to take this into account in the following.\n\nGiven what we have just learnt, let us study the correlations between columns. This will give us a first idea of which variables could have the largest impact on the classification process.","05404099":"Now, after postponing it for so long, we finally have to address the imbalancement of the classes. To do this we use the random over sampler provided by the package imblearn. Fundamentally the random over sampler implements a resampling with replacement for the minority classes, so that the number of samples from the majority class is the same as the minority classes. We really need such a replacement as, as you can see in the output of the next block, the majority class is more than 20 times larger than the smallest class.","929fab18":"# Summary\n\n**A simple random forest can classify the quality of wines as bad, average or good with a high accuracy, around 96%. We are confident in saying that the volatile acidity and the sulphates are of major importance to distinguish a good wine from a bad one. The alcohol content of a wine is also an important factor in the choice of a good wine. To reach this result we restated the problem in simpler terms (reducing the number of classes from 6 to 3) and we applied a resampling technique to address the imbalancement of the quality classes.**","ca35cc5f":"The heatmap gives a visual representation of the correlations between variables. We can immediately spot highly correlated variables by the \nred\/orange color and highly negatively correlated columns by the blue\/ purple color. Let us summarise the most important info we can extract by this heatmap:\n* there is a strong positive correlation between \"free sulfur dioxide\" and \"total sulfur dioxide\". According to [this](https:\/\/www.extension.iastate.edu\/wine\/total-sulfur-dioxide-why-it-matters-too\/) blogpost the 2 variables are indeed interelated. If you are interested in the chemical intricacies of winemaking and how free and total sulfur dioxide play a role in that, we suggest the linked article;\n* the variable \"citric acid\" is strogly positively correlated with \"fixed acidity\" and strongly negatively correlated with \"volatile acidity\". This is consistent with what we have found in [this](https:\/\/waterhouse.ucdavis.edu\/whats-in-wine\/fixed-acidity) blogpost, where the author explains that citric acid is one of the main factors contributing to the fixed acidity, whereas it does not matter as far as the volatile acidity is concerned;\n* of major interest are the variables correlated to \"quality\": we can see that \"volatile acidity\" is negatively correlated with \"quality\", whereas \"alcohol\" is strongly positively correlated with \"quality\". In [this](https:\/\/waterhouse.ucdavis.edu\/whats-in-wine\/volatile-acidity) blogpost the author explains that the amount of volatile acidity in sound grapes should be negligibile and that gives a chemical explanation to our finding. We expect \"volatile acidity\" and \"alcohol\" to play an important role in the classification process, but let us not jump ahead of ourselves. ","30448039":"Hello everyone! In this notebook we are going to analyse and predict the quality of wines based on data about their chemical composition. The goal is twofold:\n\n* Understand which are the most important chemical measuraments to predict the quality of wines;\n* Build a machine learning model that predicts the quality of wines with a high degree of certainty.\n\nLet us start by looking at the data at our disposal.","05f76c27":"The previous barplot shows that now the classes are perfectly balanced, as expected. Let us now present yet another scatterplot, similar to the previous ones, to show that now the classes are much more clear and distinguishable. Of special interest are the univariate distribution plots on the diagonal, which clearly show how the variables separate the 3 quality classes.","38701086":"From the 2 previous blocks we see there is quite a big difference in mean and standard deviation among variables. The columns \"free sulfur dioxide\" and \"total sulfur dioxide\" have especially large mean and standard deviation. Moreover, there are no null values in the data, which is quite convenient, and the type of the variables confirm that only \"quality\" is not a real number. We can now start with some simple descriptive analysis, just to understand how the data look like. Since \"quality\" is our main variable of interest, let us start by studying it.","ea67cc63":"From the first 5 rows of the dataset we see that all the variables but \"quality\" are numeric. Quality is a discrete variable that takes values from 0 to 8, 0 being a very bad wine and 8 being an extremely good wine. Let us see some basic statistic about the variables and if there are null values in the data.","7899ed11":"The previous barplot shows the new distribution of \"quality\" with the new classes. Note that the classes remain unbalanced, we still have to address this issue. In the next block we present the scatter plots of the variables \"alcohol\" and \"sulphates\" in a similar vein to what we have seen before. Once more, the 2 variables have been chosen only for illustrative purposes. It is easy to see that now the separations among classes is a bit more clear.","d0221343":"On the diagonal we can see the univariate distribution of the variable under consideration divided by quality: for example, in the top left corner we can see the distribution of \"alcohol\" for different qualities. Note that all the plots give the same info, that is, there is no easy way to distinguish between classes just by using these two variables (at least on a graphical level).","e33205d0":"We have sufficiently pre-processed the data to try to predict the quality of wines. We are going to use a random forest, as implemented in sklearn. Note that our goal is not only to achieve an accuracy on the test set of at least 90%, but also of identifying which chemical measuraments are considered relevant by the random forest. Let us start by scaling the data and applying the random forest.","514c001f":"We can see that a basic random forest does a great job of predicting the quality of wines, reaching over 96% accuracy on the test set. We now want to understand which variables the random forest used to get such a good result. To do that recall that the feature importances for a random forest are computed as the mean and standard deviation of accumulation of the impurity decrease within each tree.","b62e52c9":"Now that we have an idea of the relations between variables, we would like to understand if some of them can be used to separate wines in different quality classes. To do this on a graphical level we can use a scatterplot. In the following we have decided to plot just a few examples of the most informative bidimensional scatterplots rather than showing all of them. Of course, one could decide to plot all of them, but we fond such a graphical representation hard to interpret and somewhat cumbersome.\nThe first pairplot uses the variables \"alcohol\" and \"sulphates\". These were chosen only as they provide a certain degree of separation among quality classes. Even so,we can see that it is impossible to distinguish significant patterns in the data.","c687f5bc":"In the above barplot the blue bars are the feature importances of the forest, whereas the black error bars represent their inter-trees variability, that is how much the importance of a feature varies among trees.\n\nEven though the error bars of volatile acidity and sulphates are fairly large they are clearly the most important variables. Note that alcohol presents a fairly large standard deviation, therefore we suspect that it is important in the classification task but we cannot be that confident about its relevance. A feature importance with large standard deviation means that the importance given to the feature varies quite a bit from tree to tree, so that we may find trees where alcohol plays a large role and trees where it does not. We may want to also consider \"total sulfur dioxide\" that, depending on the tree, may assume a large importance, though not comparable to volatile acidity, sulphates and alcohol.","ddfcc013":"We see that the 3 most important features are \"volatile acidity\", \"alcohol\" and \"sulphates\". Note that we already expected volatile acidity and alcohol to play important roles at the beginning of the analysis. Let us plot the importances, so to provide a more clear representation of this result.","ae8600dd":"A first possible approach to make the classification feasible is to reduce the number of quality classes. Note that in doing so we are not changing the problem in a significant way: consider a wine quality composed of 3 classes, let us call them \"bad wines\", \"mediocre wines\" and \"good wines\". The first category will consist of wines with label 3 or 4: though for some particular applications it may matter if a wine is labelled 3 or 4, in general a consumer would like to just avoid such wines, independently on the specific label. Use them to make Calimocho, if you like, otherwise you may want to avoid them. Similar arguments hold for the other 2 categories."}}