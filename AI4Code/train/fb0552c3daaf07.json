{"cell_type":{"42d7e68e":"code","0df018b6":"code","bca5ebf1":"code","45c5cf21":"code","4c68f802":"code","86ff18b4":"code","bf4db590":"code","8c5c607a":"code","e24719a7":"code","7a549439":"code","ffc67d65":"code","6a5b0e4f":"code","78e7cb40":"code","b583ac7e":"code","af53da79":"code","b97e6b44":"code","d6f555f8":"code","3e04bd6d":"code","2b18d4d0":"code","9675d80b":"code","585c6785":"code","49cc5797":"code","9698dc10":"code","df3cf665":"code","eab3754a":"code","d75710c7":"code","976ca99e":"code","239f85de":"code","800640f6":"code","71fc18ca":"code","60a3d784":"code","2c87af61":"code","b79f0454":"code","a625153c":"code","fe48f353":"code","bb88422f":"code","0ea8a5a3":"code","96263d03":"code","4cf5dbb0":"code","94635299":"code","26cb91d7":"code","7ccd2007":"code","0b4b1015":"code","38589d34":"code","ffdb3c1d":"code","5dc70ae3":"code","a0dbf381":"code","27dfdded":"code","3c0257d4":"code","397371a3":"code","7e06d414":"code","0ddcf9f9":"code","438abc26":"code","96812de3":"markdown","ea391261":"markdown","780ba003":"markdown","033f89fd":"markdown","c21c3c1c":"markdown","35997ce2":"markdown","888071ab":"markdown","9847f447":"markdown","161e381b":"markdown","57a8c2ae":"markdown","b0d26a6f":"markdown","430a2c4f":"markdown","bad94c65":"markdown","cc3aad6a":"markdown","0aea6a0e":"markdown","01437c5f":"markdown","9b354358":"markdown","91f8aafe":"markdown","c74e8799":"markdown","52a503ad":"markdown","de8602dc":"markdown","72912403":"markdown","d312d474":"markdown","f1fa5282":"markdown","1ed853fa":"markdown","60425c28":"markdown","fff66b0a":"markdown","3a0fcd58":"markdown","fcd232a7":"markdown","fe116fab":"markdown","c1ec6ccf":"markdown"},"source":{"42d7e68e":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nimport tarfile\nimport urllib\n\n\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","0df018b6":"import matplotlib as mpl\nimport matplotlib.pyplot as plt\n!pip install seaborn --upgrade\nimport seaborn as sns\nimport plotly.graph_objects as go\nimport plotly.express as px\nfrom plotly.subplots import make_subplots\n!pip install swiftviz\nimport swiftviz as sv\n!pip install pandas_profiling\nimport pandas_profiling\n\n# Feature Selection\n!pip install ppscore\nimport ppscore as pps","bca5ebf1":"sns.__version__","45c5cf21":"# Library for pre-processing:\nfrom sklearn.preprocessing import StandardScaler\n\n# Library for Dimensionality-Reduction:\nfrom sklearn.decomposition import PCA\n\n# Libraries for modelling\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, VotingClassifier\nfrom xgboost import XGBClassifier\nfrom lightgbm import LGBMClassifier\n!pip install catboost\nfrom catboost import CatBoostClassifier\n\n\n# Model Selection:\nfrom sklearn.pipeline import Pipeline as Pipeline\nfrom sklearn.model_selection import GridSearchCV, train_test_split\nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.base import clone\n\n# Libraries for model evaluaton \nfrom sklearn.model_selection import GridSearchCV, RandomizedSearchCV, cross_val_predict\n\n# --CLASSIFICATION:\nfrom sklearn import metrics\n\n# Library for plotting confusion matrix\nfrom mlxtend.plotting import plot_confusion_matrix\n\n# Miscellanous libraries\nfrom IPython.display import display","4c68f802":"bc_df = pd.read_csv('..\/input\/breast-cancer-wisconsin-data\/data.csv')\nbc_df.head()","86ff18b4":"bc_df.columns","bf4db590":"bc_df.drop(columns=['id', 'Unnamed: 32'], inplace=True)","8c5c607a":"bc_df.info()","e24719a7":"report = bc_df.describe().T\nreport","7a549439":"features = np.array(report.index)\ntarget = ['diagnosis']\nfeatures","ffc67d65":"cust_palette = sv.CustomPalette()\ncust_palette.display_palette('Dark2')\ncust_palette.set_default_custom_palette('Dark2')","6a5b0e4f":"print(bc_df[\"diagnosis\"].value_counts())\nsns.countplot(x=bc_df[\"diagnosis\"]);","78e7cb40":"axis = sv.Plotter.row_col_merge(rows=2, cols=5)","b583ac7e":"cust_palette.set_default_custom_palette('dark')\naxis_var_list = list(zip(axis, features[:10]))\n\nfig, axs = plt.subplots(figsize=(30, 10), nrows=2, ncols=5)\n\nfor axis_tup, var in axis_var_list:\n    row = axis_tup[0]\n    col = axis_tup[1]\n    sns.histplot(data=bc_df, x=var, ax=axs[row, col], hue=\"diagnosis\")","af53da79":"cust_palette.set_default_custom_palette('Set1')\naxis_var_list = list(zip(axis, features[10:20]))\n\nfig, axs = plt.subplots(figsize=(30, 10), nrows=2, ncols=5)\n\nfor axis_tup, var in axis_var_list:\n    row = axis_tup[0]\n    col = axis_tup[1]\n    sns.histplot(data=bc_df, x=var, ax=axs[row, col], hue=\"diagnosis\")","b97e6b44":"cust_palette.set_default_custom_palette('Set2')\naxis_var_list = list(zip(axis, features[20:]))\n\nfig, axs = plt.subplots(figsize=(30, 10), nrows=2, ncols=5)\n\nfor axis_tup, var in axis_var_list:\n    row = axis_tup[0]\n    col = axis_tup[1]\n    sns.histplot(data=bc_df, x=var, ax=axs[row, col], hue=\"diagnosis\")","d6f555f8":"outlier_handler = sv.OutlierAnalysis(bc_df)\nscaled_bc_df = outlier_handler.data_scaler(list(features))","3e04bd6d":"cust_palette.set_default_custom_palette('ocean_r')\ndata_mean = pd.melt(pd.concat([scaled_bc_df[features[:10]], bc_df[target]], axis=1), \n                    id_vars=\"diagnosis\",\n                    var_name=\"features\",\n                    value_name='value')\n\nfig, axs = plt.subplots(figsize=(30, 10))\n\ndisplay(sns.violinplot(data=data_mean, x=\"features\", y=\"value\", hue=\"diagnosis\", inner=\"quart\",\n                       split=True));\n\nmean_outlier_report = outlier_handler.OutlierReport(list(features[:10]))\nmean_outlier_report","2b18d4d0":"data_se = pd.melt(pd.concat([scaled_bc_df[features[10:20]], bc_df[target]], axis=1), \n                    id_vars=\"diagnosis\",\n                    var_name=\"features\",\n                    value_name='value')\n\nfig, axs = plt.subplots(figsize=(30, 10))\n\ndisplay(sns.violinplot(data=data_se, x=\"features\", y=\"value\", hue=\"diagnosis\", inner=\"quart\",\n                       split=True, palette=sns.color_palette('Dark2')[3:6:2]));\n\nse_outlier_report = outlier_handler.OutlierReport(list(features[10:20]))\nse_outlier_report","9675d80b":"data_worst = pd.melt(pd.concat([scaled_bc_df[features[20:30]], bc_df[target]], axis=1), \n                    id_vars=\"diagnosis\",\n                    var_name=\"features\",\n                    value_name='value')\n\nfig, axs = plt.subplots(figsize=(30, 10))\n\ndisplay(sns.violinplot(data=data_worst, x=\"features\", y=\"value\", hue=\"diagnosis\", inner=\"quart\",\n                       split=True, palette=sns.color_palette('viridis')[3:6:2]));\n\nworst_outlier_report = outlier_handler.OutlierReport(list(features[20:30]))\nworst_outlier_report","585c6785":"cust_palette.set_default_custom_palette([\"#f7a400\", \"#3a9efd\", \"#3e4491\", \"#292a73\", \"#1a1b4b\"])\nplot_maker = sv.Plotter()\nplot_maker.plotter(bc_df, 'radius_mean', ['texture_mean', 'perimeter_mean', 'smoothness_mean', 'concavity_mean', 'fractal_dimension_mean', 'area_mean'],\n                   ['diagnosis'], 30, 7)","49cc5797":"plot_maker.plotter(bc_df, 'radius_se', ['texture_se', 'perimeter_se', 'smoothness_se', 'concavity_se', 'fractal_dimension_se', 'area_se'], ['diagnosis'], 30, 5)","9698dc10":"plot_maker.plotter(bc_df, 'radius_worst', ['texture_worst', 'perimeter_worst', 'area_worst', 'smoothness_worst', 'compactness_worst', 'fractal_dimension_worst'], ['diagnosis'], 30, 5)","df3cf665":"plot_maker.plotter(bc_df, 'radius_mean', ['radius_se', 'radius_worst', 'area_se', 'area_worst', 'perimeter_se', 'perimeter_worst'], ['diagnosis'], 30, 5)","eab3754a":"corr_matrix = bc_df.corr()\n\nfig, axs = plt.subplots(figsize=(20, 15))\nsns.heatmap(corr_matrix, annot=True)","d75710c7":"threshold = 0.8\ncond_corr_matrix = corr_matrix[(corr_matrix > threshold) | (corr_matrix < -threshold)]\n\nfig, axs = plt.subplots(figsize=(20, 10))\nsns.heatmap(cond_corr_matrix, annot=True)","976ca99e":"drop_features = ['radius_mean', 'perimeter_mean', 'radius_worst', 'perimeter_worst',\n                 'area_worst', 'radius_se', 'perimeter_se', 'compactness_mean', 'concave points_worst', 'smoothness_worst',\n                 'concavity_mean', 'compactness_worst', 'texture_worst']\n\nmodified_bc_df = bc_df.drop(columns=drop_features)\nmodified_corr_matrix = modified_bc_df.corr()\n\nfig, axs = plt.subplots(figsize=(20, 10))\nsns.heatmap(modified_corr_matrix, annot=True)","239f85de":"modified_bc_df.columns","800640f6":"X = modified_bc_df.drop(columns='diagnosis')\ny = modified_bc_df.diagnosis\n\nX_train, X_test_cv, y_train, y_test_cv = train_test_split(X, y, test_size=0.4)\nX_val, X_test, y_val, y_test = train_test_split(X_test_cv, y_test_cv, test_size=0.5)","71fc18ca":"for name, x, y in (('Train', X_train, y_train), ('Validation', X_val, y_val), ('Test', X_test, y_test)):\n    print(f'{name}:')\n    print(f'Data: {x.shape}')\n    print(f'Target: {y.shape}')\n    print('')","60a3d784":"X_train_scaled = StandardScaler().fit_transform(X_train)\npca = PCA()\npca.fit(X_train_scaled)\ncumsum = np.cumsum(pca.explained_variance_ratio_)\nprint(cumsum)\n\nplt.figure(figsize=(10,7))\nplt.plot(cumsum, linewidth=3)\nplt.axis([0, 30, 0, 1])\nplt.xlabel(\"n_components\")\nplt.ylabel(\"Explained Variance\")\nplt.grid(True)","2c87af61":"main_pipeline = Pipeline([\n    ('scaler', StandardScaler()),\n    ('pca', PCA()),\n    ('model', None),\n])","b79f0454":"rnd_state = 4\n\nparams_grid = [\n    {\n        \"pca__n_components\": [8, 9, 10, 11, 12],\n        \"model\": [XGBClassifier()],\n        \"model__n_estimators\": [100, 500, 1000],\n        \"model__learning_rate\": [0.005, 0.01, 0.05, 0.1],\n        \"model__max_depth\": [3, 4, 5, 6, 10],\n        \"model__gamma\": [0, 1, 5],\n        \"model__random_state\": [rnd_state],\n    },\n    {\n        \"pca__n_components\": [8, 9, 10, 11, 12],\n        \"model\": [GradientBoostingClassifier()],\n        \"model__n_estimators\": [100, 1000],\n        \"model__learning_rate\": [0.001, 0.05, 0.1, 0.5],\n        \"model__random_state\": [rnd_state],\n    },\n    {\n        \"pca__n_components\": [8, 9, 10, 11, 12],\n        \"model\": [RandomForestClassifier()],\n        \"model__n_estimators\": [100, 1000],\n        \"model__max_depth\": [3, 4, 5, 6, 10, 15, 20],\n        \"model__random_state\": [rnd_state],\n    },\n    {\n        \"pca__n_components\": [8, 9, 10, 11, 12],\n        \"model\": [LGBMClassifier()],\n        \"model__n_estimators\": [100, 500, 1000],\n        \"model__learning_rate\": [0.005, 0.01, 0.05, 0.1],\n        \"model__max_depth\": [3, 4, 5, 6, 10, -1],\n        \"model__random_state\": [rnd_state],\n    },\n    {\n        \"pca__n_components\": [8, 9, 10, 11, 12],\n        \"model\": [CatBoostClassifier()],\n        \"model__n_estimators\": [100, 500, 1000],\n        \"model__learning_rate\": [0.005, 0.01, 0.05, 0.1],\n        \"model__max_depth\": [3, 4, 5, 6, 10],\n        \"model__random_state\": [rnd_state],\n    },\n]","a625153c":"main_grid = GridSearchCV(main_pipeline, params_grid, cv=3, verbose=2, scoring=\"accuracy\")","fe48f353":"main_grid.fit(X_train, y_train)","bb88422f":"main_grid.best_estimator_","0ea8a5a3":"y_pred = main_grid.best_estimator_.predict(X_val)\nprint(metrics.classification_report(y_val, y_pred))\nmetrics.plot_roc_curve(main_grid.best_estimator_, X_val, y_val);","96263d03":"conf_matrix = metrics.confusion_matrix(y_val, y_pred)\nplot_confusion_matrix(conf_mat=conf_matrix);","4cf5dbb0":"best_params = main_grid.best_params_\nbest_params","94635299":"best_pipeline = clone(main_grid.best_estimator_)\n\nbest_pipeline.fit(X_train, y_train)","26cb91d7":"y_pred = best_pipeline.predict(X_val)\nprint(metrics.classification_report(y_val, y_pred))\nmetrics.plot_roc_curve(best_pipeline, X_val, y_val);","7ccd2007":"conf_matrix = metrics.confusion_matrix(y_val, y_pred)\nplot_confusion_matrix(conf_mat=conf_matrix);","0b4b1015":"second_pipeline = Pipeline([\n                (\"pca\", PCA()),\n                (\"cat\", CatBoostClassifier()),\n])","38589d34":"second_param_grid = [\n        {\n            \"pca__n_components\": [8, 9, 10, 11, 12],\n            \"cat__n_estimators\": [100, 500, 1000],\n            \"cat__learning_rate\": [0.005, 0.01, 0.05, 0.1],\n            \"cat__max_depth\": [3, 4, 5, 6, 10],\n            \"cat__random_state\": [rnd_state],\n        }\n]","ffdb3c1d":"main_grid_2 = GridSearchCV(second_pipeline, second_param_grid, cv=3, verbose=2, scoring='accuracy')\nmain_grid_2.fit(X_train, y_train)","5dc70ae3":"main_grid_2.best_estimator_","a0dbf381":"best_params_2 = main_grid_2.best_params_\nbest_params_2","27dfdded":"y_pred = main_grid_2.best_estimator_.predict(X_val)\nprint(metrics.classification_report(y_val, y_pred))\nmetrics.plot_roc_curve(main_grid_2.best_estimator_, X_val, y_val);","3c0257d4":"best_pipeline_2 = clone(main_grid_2.best_estimator_)","397371a3":"softVoter = VotingClassifier(estimators=[\n        ('cat_1', best_pipeline), ('cat_2', best_pipeline_2)], voting=\"soft\")","7e06d414":"softVoter.fit(X_train, y_train)","0ddcf9f9":"y_pred = softVoter.predict(X_val)\nprint(metrics.classification_report(y_val, y_pred))\nmetrics.plot_roc_curve(softVoter, X_val, y_val);","438abc26":"y_pred_test = softVoter.predict(X_test)\nprint(metrics.classification_report(y_test, y_pred_test))\nmetrics.plot_roc_curve(softVoter, X_test, y_test);","96812de3":"### Outlier Report For Means:","ea391261":"## Heatmap: (Using Pearson's Corr)","780ba003":"### Outlier Report for Worst:","033f89fd":"We will use this visualizations for determining whether to use pearson's correlation or spearman's correlation","c21c3c1c":"Let's filter out the values using a threshold, it will be a lot easier for us to find strong correlations:","35997ce2":"Creating second CatBoostClassifier Pipeline for VotingEnsemble:","888071ab":"# Data Exploration:","9847f447":"### Thank You!!","161e381b":"### Distributions of Worst:","57a8c2ae":"### No Null-values & every column has the correct data-type. All good!","b0d26a6f":"## Libraries for Preprocessing & Model-Selection:","430a2c4f":"### Remember: \nLarger the distance between the two medians('M' & 'B'), more useful the feature.<br>\nThis is because larger the distance between the medians<br>\nmeans larger the distance between clusters, this will make the clusters to be more distinguishable.","bad94c65":"## Random Visualizations for Feature Selections:","cc3aad6a":"## PCA:","0aea6a0e":"## Main Pipeline:","01437c5f":"## Libraries for EDA & Feature-Selection:","9b354358":"## Trying to increase model accuracy using Soft Vote Ensembling:","91f8aafe":"The data we have consists of mean, standard deviation, and worst measure (average of top 3 maximum values). From this we can already judge that, features will have very strong correlation between them. But, Let's check for null values.","c74e8799":"# Model-Selection:","52a503ad":"## Outlier Analysis:","de8602dc":"We will only keep 4 features from the above 4 points (1 for each) randomly. We will keep:\n* area_mean (It has got more correlations than radius_mean)\n* area_se\n* concave points_mean\n* texture_mean","72912403":"### Distribution of Standard Errors:","d312d474":"The table shown is for the entire dataset, and the plots are shown to visualize w.r.t \"diagnosis\".","f1fa5282":"### Distributions of Means:","1ed853fa":"#### Setting default custom palette:","60425c28":"We can see many strong correlations from the matrix, they are:\n\n* radius_mean, perimeter_mean, area_mean, radius_worst, perimeter_worst, area_worst\n* radius_se, perimeter_se, area_se\n* compactness_mean, concavity_mean, concave points_mean\n* texture_mean, texture_worst\n<br>\n\n& many more, but, majority of the features from above are going to be removed as they have strong correlation with each other.<br>\nHence, we can derive one feature from another, so, it is of no use to keep all the strongly correlated features.","fff66b0a":"## Loading Dataset:","3a0fcd58":"We can see, that for to get explained variance >= 95%, we need n_componenets >= 8","fcd232a7":"# Importing Libraries:","fe116fab":"Let's combine the two estimators using Soft Voting Ensemble:","c1ec6ccf":"### Outlier Report for Standard Errors:"}}