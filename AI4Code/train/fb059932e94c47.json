{"cell_type":{"67fd4f06":"code","d05e21c4":"code","f8130cac":"code","73a37967":"code","bba6c112":"code","5714d3f0":"code","9f917df2":"code","682423e9":"code","16633057":"code","0664d7f8":"code","05be5541":"code","4007fa8a":"code","9389369f":"code","f4c69041":"code","c629a065":"code","2c2a0709":"code","bbbb6027":"code","96457fda":"code","130382e3":"code","2574e493":"code","0b9f8e41":"code","716cfc13":"code","436f01ea":"code","c7dbbe1d":"code","c5953bea":"code","a282a71a":"code","5bad59ac":"code","a91cd25d":"code","8ee50f76":"code","71f5210b":"code","fc4fa221":"code","fcb90974":"code","33f357bd":"code","1435686e":"markdown"},"source":{"67fd4f06":"import numpy as np\nimport pandas as pd\nimport pickle\nimport tensorflow as tf\nfrom wordcloud import WordCloud, STOPWORDS\nimport matplotlib.pyplot as plt\nfrom tensorflow.keras.preprocessing import text, sequence\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Dropout, Activation\nfrom tensorflow.keras.layers import Embedding\nfrom tensorflow.keras.layers import Conv1D, GlobalMaxPooling1D, MaxPooling1D\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, f1_score, confusion_matrix\nimport seaborn as sns\nplt.style.use('fivethirtyeight')\nprint(tf.__version__)\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(filename)","d05e21c4":"df = pd.read_csv('..\/input\/hate-speech-and-offensive-language-dataset\/labeled_data.csv')\ndf.head()","f8130cac":"nRow, nCol = df.shape\nprint('There are {} rows and {} columns'.format(nRow, nCol))","73a37967":"c=df['class']\ndf.rename(columns={'tweet' : 'text',\n                   'class' : 'category'}, \n                    inplace=True)\na=df['text']\nb=df['category'].map({0: 'hate_speech', 1: 'offensive_language',2: 'neither'})\n\ndf= pd.concat([a,b,c], axis=1)\ndf.rename(columns={'class' : 'label'}, \n                    inplace=True)\ndf","bba6c112":"# Grouping data by label\ndf['category'].value_counts()","5714d3f0":"hate, ofensive, neither = np.bincount(df['label'])\ntotal = hate + ofensive + neither\nprint('Examples:\\n    Total: {}\\n    hate: {} ({:.2f}% of total)\\n'.format(\n    total, hate, 100 * hate \/ total))\nprint('Examples:\\n    Total: {}\\n    Ofensive: {} ({:.2f}% of total)\\n'.format(\n    total, ofensive, 100 * ofensive \/ total))\nprint('Examples:\\n    Total: {}\\n    Neither: {} ({:.2f}% of total)\\n'.format(\n    total, neither, 100 * neither \/ total))","9f917df2":"X_train_, X_test, y_train_, y_test = train_test_split(\n    df.index.values,\n    df.label.values,\n    test_size=0.10,\n    random_state=42,\n    stratify=df.label.values,    \n)","682423e9":"X_train, X_val, y_train, y_val = train_test_split(\n    df.loc[X_train_].index.values,\n    df.loc[X_train_].label.values,\n    test_size=0.10,\n    random_state=42,\n    stratify=df.loc[X_train_].label.values,  \n)","16633057":"df['data_type'] = ['not_set']*df.shape[0]\ndf.loc[X_train, 'data_type'] = 'train'\ndf.loc[X_val, 'data_type'] = 'val'\ndf.loc[X_test, 'data_type'] = 'test'","0664d7f8":"df.groupby(['category', 'label', 'data_type']).count()","05be5541":"df","4007fa8a":"df_train = df.loc[df[\"data_type\"]==\"train\"]\ndf_val = df.loc[df[\"data_type\"]==\"val\"]\ndf_test = df.loc[df[\"data_type\"]==\"test\"]\n\ndf_train.head()","9389369f":"df_train_plus_val = pd.concat([df_train,df_val], axis=0)\ndf_train_plus_val.head()","f4c69041":"x = df_train_plus_val.text.values\ny = df_train_plus_val.label.values","c629a065":"max_features = 20000\nmax_text_length = 512","2c2a0709":"x_tokenizer = text.Tokenizer(max_features)\nx_tokenizer.fit_on_texts(list(x))","bbbb6027":"x_tokenized = x_tokenizer.texts_to_sequences(x)\nx_train_val= sequence.pad_sequences(x_tokenized, maxlen=max_text_length)","96457fda":"x_test_tokenized = x_tokenizer.texts_to_sequences(df_test.text.values)\nx_test = sequence.pad_sequences(x_test_tokenized,maxlen=max_text_length)","130382e3":"embedding_dim =100\nembeddings_index = dict()\nf = open('\/kaggle\/input\/glove6b100dtxt\/glove.6B.100d.txt')\nfor line in f:\n    values = line.split()\n    word = values[0]\n    coefs = np.asarray(values[1:],dtype='float32')\n    embeddings_index[word]= coefs\nf.close()\nprint(f'Found {len(embeddings_index)} word vectors')","2574e493":"embedding_matrix= np.zeros((max_features,embedding_dim))\nfor word, index in x_tokenizer.word_index.items():\n    if index>max_features-1:\n        break\n    else:\n        embedding_vector = embeddings_index.get(word)\n        if embedding_vector is not None:\n            embedding_matrix[index]= embedding_vector","0b9f8e41":"y_train_plus_val =  tf.keras.utils.to_categorical(y, num_classes=3)\ny_test =  tf.keras.utils.to_categorical(df_test.label, num_classes=3)\ny_train_plus_val","716cfc13":"model = Sequential()\nmodel.add(Embedding(max_features,\n                    embedding_dim,\n                    embeddings_initializer=tf.keras.initializers.Constant(embedding_matrix),\n                    trainable=False))\n\nmodel.add(Dropout(0.2))\n\nmodel.add(Conv1D(64,2,padding='valid',activation='relu'))\nmodel.add(MaxPooling1D())\nmodel.add(Conv1D(64,2,padding='valid',activation='relu'))\nmodel.add(MaxPooling1D())\n\nmodel.add(Conv1D(32,2,padding='valid',activation='relu'))\nmodel.add(MaxPooling1D())\nmodel.add(Conv1D(32,2,padding='valid',activation='relu'))\nmodel.add(GlobalMaxPooling1D())\n\nmodel.add(Dense(16, activation='relu'))\nmodel.add(Dense(16, activation='relu'))\nmodel.add(Dropout(0.2))\n\nmodel.add(Dense(3, activation='softmax'))\nmodel.summary()","436f01ea":"model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])","c7dbbe1d":"history = model.fit(x_train_val, y_train_plus_val, batch_size= 64, validation_split=0.2, epochs=10)","c5953bea":"# Plot loss\nplt.plot(history.history['loss'], label='train')\nplt.plot(history.history['val_loss'], label='validation')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.title('Training loss vs. Epochs')\nplt.legend()\nplt.show()","a282a71a":"# Plot accuracy\nplt.plot(history.history['accuracy'], label='train')\nplt.plot(history.history['val_accuracy'], label='validation')\nplt.xlabel('Epochs')\nplt.ylabel('Accuracy')\nplt.title('Training accuracy vs. Epochs')\nplt.legend()\nplt.show()","5bad59ac":"model.evaluate(x_test,y_test, batch_size = 64)","a91cd25d":"y_pred = model.predict(x_test)\ny_pred","8ee50f76":"y_pred = np.array( [ np.argmax (y) for y in y_pred ] )","71f5210b":"y_pred","fc4fa221":"y_test_labels = df_test.label","fcb90974":"cm = confusion_matrix(y_test_labels, y_pred)\nfig = sns.heatmap(cm, annot=True, fmt=\"d\")\nplt.title(\"Confusion Matrix\")\nplt.xlabel(\"Predicted Label\")\nplt.ylabel(\"Actual Label\")\nplt.show(fig)","33f357bd":"model.save('hate_davidson_dataset_1dcnn_glove.h5')","1435686e":"# Building 1 D CNN Model"}}