{"cell_type":{"575c1c83":"code","7d3f975f":"code","675f813f":"code","94192eab":"code","d2240b4d":"code","dae5e7f6":"code","dad4c963":"code","01a83c55":"code","3c3b2747":"code","d8754d3e":"code","f2a3179b":"code","875fa6c2":"code","64ae8e64":"markdown","31d5414f":"markdown","a30c1c2f":"markdown","630dcdd3":"markdown","925dc9d2":"markdown","6751801c":"markdown","08180935":"markdown","b118c589":"markdown","4ec558fe":"markdown","cff2459c":"markdown","6061be97":"markdown","b1357700":"markdown","6640c30c":"markdown","39ec862c":"markdown","2727c6b4":"markdown","57e8d1d9":"markdown"},"source":{"575c1c83":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport cv2\nimport time\nfrom timeit import default_timer as timer\nimport matplotlib.pyplot as plt\nimport pickle\n\nfrom keras.models import load_model\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\n# for dirname, _, filenames in os.walk('..\/input'):\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))\n\nprint(os.listdir('..\/input'))\n\n# Any results we write to the current directory are saved as output\n","7d3f975f":"# Reading csv file with labels' names\n# Loading two columns [0, 1] into Pandas dataFrame\nlabels = pd.read_csv('..\/input\/traffic-signs-preprocessed\/label_names.csv')\n\n# Check point\n# Showing first 5 rows from the dataFrame\nprint(labels.head())\nprint()\n\n# To locate by class number use one of the following\n# ***.iloc[0][1] - returns element on the 0 column and 1 row\nprint(labels.iloc[0][1])  # Speed limit (20km\/h)\n# ***['SignName'][1] - returns element on the column with name 'SignName' and 1 row\nprint(labels['SignName'][1]) # Speed limit (30km\/h)\n","675f813f":"# Loading trained CNN model to use it later when classifying from 4 groups into one of 43 classes\n#model = load_model('..\/input\/model3x3\/model-3x3.h5')\n# Better CNN model (larger resolution of filter)\n#model = load_model('..\/input\/model5x5\/model-5x5.h5')\n# Najbetter (uczony na 10k)\nmodel = load_model('..\/input\/modele\/model-3x3 (1).h5')\n\n# Loading mean image to use for preprocessing further\n# Opening file for reading in binary mode\nwith open('..\/input\/traffic-signs-preprocessed\/mean_image_rgb.pickle', 'rb') as f:\n    mean = pickle.load(f, encoding='latin1')  # dictionary type\n    \nprint(mean['mean_image_rgb'].shape)  # (3, 32, 32)\n","94192eab":"model.summary()","d2240b4d":"import math\nimport random\nfrom PIL import Image\n\npath_prefix = \"\/root\/darknet\/rmarkings\"\nvalidate_percentage = 0.2\n\ndef RepresentsInt(s):\n    try:\n        int(s)\n        return True\n    except ValueError:\n        return False\n\ndef parse():\n    train = {}\n    validate = {}\n    cls_labels = {}\n    \n    # Read data\n    with open('dataset_annotations.txt') as file:\n        classes = {}\n        for line in file.readlines():\n            lines = line.strip().split(\",\")\n            file_name, clas_spec = lines[-1], lines[-2]\n            file_name = file_name.replace(\".png\", \".jpg\")\n\n            xs = [int(float(lines[0])), int(float(lines[2])), int(float(lines[4])), int(float(lines[6]))]\n            ys = [int(float(lines[1])), int(float(lines[3])), int(float(lines[5])), int(float(lines[7]))]\n\n            x_min, x_max = min(xs), max(xs)\n            y_min, y_max = min(ys), max(ys)\n\n            width = x_max - x_min\n            height = y_max - y_min\n\n            im = Image.open(file_name)\n            im_width, im_height = im.size\n\n            center_x, center_y = (width \/ 2) + x_min, (height \/2) + y_min\n\n            if classes.get(clas_spec) is None:\n                classes[clas_spec] = []\n            data = {\"name\": file_name, \"x\": center_x \/ im_width, \"y\": center_y \/ im_height, \"width\": width \/ im_width, \"height\": height \/ im_height}\n            classes[clas_spec].append(data)\n            print(data)\n        \n        it = 0\n        for key, values in classes.items():\n            if len(values) > 20 and not RepresentsInt(key):\n                cls_labels[it] = key\n                random.shuffle(values)\n                test_len = math.floor(len(values) * validate_percentage)\n                train[key] = values[-(len(values) - test_len):]\n                validate[key] = values[:test_len]\n\n\n                print(f\"validate: {key}: {len(validate[key])}\")\n                print(f\"train: {key}: {len(train[key])}\")\n\n                for value in values:\n                    f_name = value[\"name\"].replace(\".jpg\", \".txt\")\n                    with open(f_name, \"w+\") as w_file:\n                        w_file.write(f\"{it} {value['x']} {value['y']} {value['width']} {value['height']}\\n\")\n                it += 1\n\n    with open(\"classes.names\", \"w+\") as cls_file:\n        for cls_name in cls_labels.values():\n            cls_file.write(f\"{cls_name}\\n\")\n\n    with open(\"train.txt\", \"w+\") as data_file:\n        for vls in train.values():\n            for val in vls:\n                data_file.write(f\"{path_prefix}\/{val['name']}\\n\")\n\n    with open(\"test.txt\", \"w+\") as data_file:\n        for vls in validate.values():\n            for val in vls:\n                data_file.write(f\"{path_prefix}\/{val['name']}\\n\")\n    \n    with open(\"data.data\", \"w+\") as data_file:\n        data_file.write(f\"\"\"classes = {len(cls_labels.keys())}\ntrain = {path_prefix}\/train.txt\nvalid = {path_prefix}\/test.txt\nnames = {path_prefix}\/classes.names\nbackup = backup1\"\"\")","dae5e7f6":"# Trained weights can be found in the course mentioned above\n# Tutaj po dodaniu do Data zmie\u0144 drog\u0119 do pliku kt\u00f3ry wrzucisz\npath_to_weights = '..\/input\/car-data\/znaki_rtx_final.weights'\npath_to_weights_markings = '..\/input\/car-data\/poziome_rtx_final.weights'\npath_to_cfg = '..\/input\/traffic-signs-dataset-in-yolo-format\/yolov3_ts_test.cfg'\npath_to_cfg_markings = '..\/input\/car-data\/markings_test.cfg'\n\n# Loading trained YOLO v3 weights and cfg configuration file by 'dnn' library from OpenCV\nnetwork = cv2.dnn.readNetFromDarknet(path_to_cfg, path_to_weights)\nnetwork_markings = cv2.dnn.readNetFromDarknet(path_to_cfg_markings, path_to_weights_markings)\n\n# To use with GPU\nnetwork.setPreferableBackend(cv2.dnn.DNN_BACKEND_OPENCV)\nnetwork.setPreferableTarget(cv2.dnn.DNN_TARGET_OPENCL_FP16)\n\nnetwork_markings.setPreferableBackend(cv2.dnn.DNN_BACKEND_OPENCV)\nnetwork_markings.setPreferableTarget(cv2.dnn.DNN_TARGET_OPENCL_FP16)\n","dad4c963":"# Getting names of all YOLO v3 layers\nlayers_all = network.getLayerNames()\nlayers_names_output = [layers_all[i[0] - 1] for i in network.getUnconnectedOutLayers()]\nprint(layers_names_output)\n\nprint(\"<===========>\")\n\n# Getting names of all YOLO v4 layers\nlayers_all_markings = network_markings.getLayerNames()\nlayers_names_output_markings = [layers_all_markings[i[0] - 1] for i in network_markings.getUnconnectedOutLayers()]\nprint(layers_names_output_markings)\n","01a83c55":"# Minimum probability to eliminate weak detections\nprobability_minimum = 0.1\n\n# Setting threshold to filtering weak bounding boxes by non-maximum suppression\nthreshold = 0.1\n\n# Generating colours for bounding boxes\n# randint(low, high=None, size=None, dtype='l')\ncolours = np.random.randint(0, 255, size=(len(labels), 3), dtype='uint8')\ncolours_markings = np.random.randint(0, 255, size=(1, 3), dtype='uint8')\n\n# Check point\nprint(type(colours))  # <class 'numpy.ndarray'>\nprint(colours.shape)  # (43, 3)\nprint(colours[0])  # [25  65 200]\n","3c3b2747":"# Reading video from a file by VideoCapture object\n#video = cv2.VideoCapture('..\/input\/car-data\/70maiMiniDashCam-Dzien.mp4')\n#video = cv2.VideoCapture('..\/input\/car-data\/DODRX8W(lusterko)-roadtestwsonecznydzien_podsonce1080p30.mp4')\nvideo = cv2.VideoCapture('..\/input\/traffic-signs-dataset-in-yolo-format\/traffic-sign-to-test.mp4')\n\n# Writer that will be used to write processed frames\nwriter = None\n\n# Variables for spatial dimensions of the frames\nh, w = None, None\n","d8754d3e":"%matplotlib inline\n\n# Setting default size of plots\nplt.rcParams['figure.figsize'] = (3, 3)\n\n# Variable for counting total amount of frames\nf = 0\n\n# Variable for counting total processing time\nt = 0\n\n# Catching frames in the loop\nwhile True:\n    # Capturing frames one-by-one\n    ret, frame = video.read()\n\n    # If the frame was not retrieved\n    if not ret:\n        break\n       \n    # Getting spatial dimensions of the frame for the first time\n    if w is None or h is None:\n        # Slicing two elements from tuple\n        h, w = frame.shape[:2]\n\n    # Blob from current frame\n    blob = cv2.dnn.blobFromImage(frame, 1 \/ 255.0, (416, 416), swapRB=True, crop=False)\n\n    # Forward pass with blob through output layers\n    network.setInput(blob)\n    network_markings.setInput(blob)\n    start = time.time()\n    output_from_network = network.forward(layers_names_output)\n    output_from_network_markings = network_markings.forward(layers_names_output_markings)\n    end = time.time()\n\n    # Increasing counters\n    f += 1\n    t += end - start\n\n    # Spent time for current frame\n    print('Frame number {0} took {1:.5f} seconds'.format(f, end - start))\n\n    # Lists for detected bounding boxes, confidences and class's number\n    bounding_boxes = []\n    bounding_boxes_markings = []\n    confidences = []\n    confidences_markings = []\n    class_numbers = []\n    class_numbers_markings = []\n\n    # Going through all output layers after feed forward pass\n    for result in output_from_network:\n        # Going through all detections from current output layer\n        for detected_objects in result:\n            # Getting 80 classes' probabilities for current detected object\n            scores = detected_objects[5:]\n            # Getting index of the class with the maximum value of probability\n            class_current = np.argmax(scores)\n            # Getting value of probability for defined class\n            confidence_current = scores[class_current]\n            # Eliminating weak predictions by minimum probability\n            if confidence_current > probability_minimum:\n                try:\n                    # Scaling bounding box coordinates to the initial frame size\n                    box_current = detected_objects[0:4] * np.array([w, h, w, h])\n\n                    # Getting top left corner coordinates\n                    x_center, y_center, box_width, box_height = box_current\n                    x_min = int(x_center - (box_width \/ 2))\n                    y_min = int(y_center - (box_height \/ 2))\n\n                    # Adding results into prepared lists\n                    bounding_boxes.append([x_min, y_min, int(box_width), int(box_height)])\n                    confidences.append(float(confidence_current))\n                    class_numbers.append(class_current)\n                except Exception as e:\n                    print(e)\n                \n\n    # Implementing non-maximum suppression of given bounding boxes\n    results = cv2.dnn.NMSBoxes(bounding_boxes, confidences, probability_minimum, threshold)\n    results_markings = cv2.dnn.NMSBoxes(bounding_boxes_markings, bounding_boxes_markings, probability_minimum, threshold)\n\n    # Checking if there is any detected object been left\n    if len(results) > 0:\n        # Going through indexes of results\n        for i in results.flatten():\n            # Bounding box coordinates, its width and height\n            x_min, y_min = bounding_boxes[i][0], bounding_boxes[i][1]\n            box_width, box_height = bounding_boxes[i][2], bounding_boxes[i][3]\n            \n            \n            # Cut fragment with Traffic Sign\n            c_ts = frame[y_min:y_min+int(box_height), x_min:x_min+int(box_width), :]\n            \n            if c_ts.shape[:1] == (0,) or c_ts.shape[1:2] == (0,):\n                pass\n            else:\n                # Getting preprocessed blob with Traffic Sign of needed shape\n                blob_ts = cv2.dnn.blobFromImage(c_ts, 1 \/ 255.0, size=(32, 32), swapRB=True, crop=False)\n                blob_ts[0] = blob_ts[0, :, :, :] - mean['mean_image_rgb']\n                blob_ts = blob_ts.transpose(0, 2, 3, 1)\n\n                # Feeding to the Keras CNN model to get predicted label among 43 classes\n                scores = model.predict(blob_ts)\n\n                # Scores is given for image with 43 numbers of predictions for each class\n                # Getting only one class with maximum value\n                prediction = np.argmax(scores)\n\n\n                # Colour for current bounding box\n                colour_box_current = colours[class_numbers[i]].tolist()\n\n                # Drawing bounding box on the original current frame\n                cv2.rectangle(frame, (x_min, y_min),\n                              (x_min + box_width, y_min + box_height),\n                              colour_box_current, 2)\n\n                # Preparing text with label and confidence for current bounding box\n                text_box_current = '{}: {:.4f}'.format(labels['SignName'][prediction],\n                                                       confidences[i])\n\n                # Putting text with label and confidence on the original image\n                cv2.putText(frame, text_box_current, (x_min, y_min - 5),\n                            cv2.FONT_HERSHEY_SIMPLEX, 0.5, colour_box_current, 2)\n\n    # For markings\n    for result in output_from_network_markings:\n        for detected_objects in result:\n            scores = detected_objects[5:]\n            class_current = np.argmax(scores)\n            confidence_current = scores[class_current]\n            if confidence_current > probability_minimum:\n                try:\n                    box_current = detected_objects[0:4] * np.array([w, h, w, h])\n\n                    x_center, y_center, box_width, box_height = box_current\n                    x_min = int(x_center - (box_width \/ 2))\n                    y_min = int(y_center - (box_height \/ 2))\n\n                    bounding_boxes_markings.append([x_min, y_min, int(box_width), int(box_height)])\n                    confidences_markings.append(float(confidence_current))\n                    class_numbers_markings.append(class_current)\n                except Exception as e:\n                    print(e)\n\n    if len(results_markings) > 0:\n        for i in results_markings.flatten():\n            x_min, y_min = bounding_boxes[i][0], bounding_boxes[i][1]\n            box_width, box_height = bounding_boxes[i][2], bounding_boxes[i][3]\n\n            cv2.rectangle(frame, (x_min, y_min),\n                          (x_min + box_width, y_min + box_height),\n                            colours[0].toList(), 2)\n\n\n    # Initializing writer only once\n    if writer is None:\n        fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n\n        # Writing current processed frame into the video file\n        writer = cv2.VideoWriter('result.mp4', fourcc, 25,\n                                 (frame.shape[1], frame.shape[0]), True)\n\n    # Write processed current frame to the file\n    writer.write(frame)\n\n\n# Releasing video reader and writer\nvideo.release()\nwriter.release()\n","f2a3179b":"print('Total number of frames', f)\nprint('Total amount of time {:.5f} seconds'.format(t))\nprint('FPS:', round((f \/ t), 1))\n","875fa6c2":"# Saving locally without committing\nfrom IPython.display import FileLink\nimport os\n\n#os.chdir(r'kaggle\/working')\nFileLink('result.mp4')\n","64ae8e64":"# \ud83d\udccd Loading trained Keras CNN model for Classification","31d5414f":"## Getting *output layers* where detections are made","a30c1c2f":"# \ud83c\udfac Reading input video","630dcdd3":"# \u26d4\ufe0f Traffic Signs Detection with YOLO v3, OpenCV and Keras","925dc9d2":"# \ud83d\udce5 Importing needed libraries","6751801c":"# \u27bf Processing frames in the loop","08180935":"## Parsing markings dataset","b118c589":"### \ud83d\udea9 Related Papers\n\n1. Sichkar V. N. **Real time detection and classification of traffic signs based on YOLO version 3 algorithm.** *Scientific and Technical Journal of Information Technologies, Mechanics and Optics*, 2020, vol. 20, no. 3, pp. 418\u2013424. DOI: 10.17586\/2226-1494-2020-20-3-418-424 (Full-text available on ResearchGate here: [Real time detection and classification of traffic signs based on YOLO version 3 algorithm](https:\/\/www.researchgate.net\/publication\/342638954_Real_time_detection_and_classification_of_traffic_signs_based_on_YOLO_version_3_algorithm)\n\n1. Sichkar V. N. **Effect of various dimension convolutional layer filters on traffic sign classification accuracy.** *Scientific and Technical Journal of Information Technologies, Mechanics and Optics*, 2019, vol. 19, no. 3, pp. 546\u2013552. DOI: 10.17586\/2226-1494-2019-19-3-546-552 (Full-text available on ResearchGate here: [Effect of various dimension convolutional layer filters on traffic sign classification accuracy](https:\/\/www.researchgate.net\/publication\/334074308_Effect_of_various_dimension_convolutional_layer_filters_on_traffic_sign_classification_accuracy)\n\n1. Tao Wu and Ananth Ranganathan **A Practical System for Road Marking Detection and Recognition\u201d, IEEE Intelligent Vehicles Symposium, 2012.** [More about dataset](http:\/\/www.ananth.in\/RoadMarkingDetection.html)","4ec558fe":"* Firstly, trained model in Darknet framework **detects Traffic Signs among 4 categories** by OpenCV dnn library.\n* Then, trained model in Keras **classifies** cut fragmets of Traffic Signs into one of **43 classes**.\n* Results are experimental, but can be used for further improvements.","cff2459c":"## \ud83c\udfc1 FPS results","6061be97":"## Loading *trained weights* and *cfg file* into the Network","b1357700":"## Setting *probability*, *threshold* and *colour* for bounding boxes","6640c30c":"### \ud83c\udf93 Related Course for Detection Tasks\n**Training YOLO v3 for Objects Detection with Custom Data.** *Build your own detector by labelling, training and testing on image, video and in real time with camera.* **Join here:** [https:\/\/www.udemy.com\/course\/training-yolo-v3-for-objects-detection-with-custom-data\/](https:\/\/www.udemy.com\/course\/training-yolo-v3-for-objects-detection-with-custom-data\/?referralCode=A283956A57327E37DDAD)\n\nExample of detections on video are shown below. **Trained weights** can be found in the course mentioned above.\n\n![](https:\/\/www.googleapis.com\/download\/storage\/v1\/b\/kaggle-user-content\/o\/inbox%2F3400968%2Fbcdae0b57021d6ac3e86a9aa2e8c4b08%2Fts_detections.gif?generation=1581700736851192&alt=media)","39ec862c":"![](https:\/\/www.googleapis.com\/download\/storage\/v1\/b\/kaggle-user-content\/o\/inbox%2F3400968%2Fa57f58b38e3caab6fbf72169895f5074%2Fresult.gif?generation=1585955236302060&alt=media)","2727c6b4":"# \ud83d\udcc2 Loading *labels*","57e8d1d9":"# \ud83d\udd0e Example of the result"}}