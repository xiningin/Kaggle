{"cell_type":{"940e85d9":"code","79672d68":"code","d20e9d9a":"code","ee43fbda":"code","6e23559f":"code","f32e56f9":"code","60226696":"code","b80066ad":"code","7b19af51":"code","dd3f1cd7":"code","de61889b":"code","c629aed7":"code","c14f682e":"code","0d7a6233":"code","a1e6af30":"code","45c10d50":"code","6cd26f01":"code","c3ad79bc":"code","5e1b93ca":"code","f4e18d7a":"code","76b0431c":"code","e927ce5a":"code","041e6b87":"code","c3b8fdcf":"code","0b9efac4":"markdown","437febe4":"markdown","97ae67b6":"markdown","c6502a98":"markdown","9766f6a7":"markdown","206f8f5d":"markdown","56eaa3be":"markdown","90816104":"markdown","6854d85e":"markdown","a3ddd037":"markdown","6feed92b":"markdown","a0141dae":"markdown","62f1b395":"markdown","3a6101ba":"markdown","bccf9fae":"markdown"},"source":{"940e85d9":"import numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt \nimport seaborn as sns \n","79672d68":"from keras.preprocessing.image import ImageDataGenerator","d20e9d9a":"# Load the data using Keras Image genarator \nimg_gen = ImageDataGenerator(rescale=1.\/255) # assign a image generator\n\ntraining_set = img_gen.flow_from_directory(directory='..\/input\/cat-and-dog\/training_set\/training_set', target_size=(224,224), \n                                           classes=['cats', 'dogs'], batch_size=10)\n","ee43fbda":"test_set = img_gen.flow_from_directory(directory= '..\/input\/cat-and-dog\/test_set\/test_set', target_size=(224,224), classes=['cats', 'dogs'], \n                                       batch_size=10, shuffle=False)","6e23559f":"imgs, labels = next(training_set)\n# note size of batch is determine when we create a train set","f32e56f9":"def plotImages(images_arr):\n    fig, axes = plt.subplots(1, 10, figsize=(20,20))\n    axes = axes.flatten()\n    for img, ax in zip( images_arr, axes):\n        ax.imshow(img)\n        ax.axis('off')\n    plt.tight_layout()\n    plt.show()","60226696":"plotImages(imgs)\nprint(labels)\n# So here 1. 0. represent cat and 0. 1. represent Dog \n# So here already one hot encoding is done ","b80066ad":"# 1st import the model from tensorflow \nfrom tensorflow.keras.models import Sequential\n\ncnn = Sequential()\n\n# Import the layers From tensorFlow\nfrom tensorflow.keras.layers import Conv2D,MaxPool2D,Flatten,Dense,Dropout\n\n# 1st Convolution and Pooling layers\ncnn.add(Conv2D(filters=32, kernel_size=(3, 3), activation='relu', padding = 'same', input_shape=(224,224,3)))\ncnn.add(MaxPool2D(pool_size=(2, 2), strides=2),)\n\n# 2nd Convolution and Pooling layers\ncnn.add(Conv2D(filters=64, kernel_size=(3, 3), activation='relu', padding = 'same'))\ncnn.add(MaxPool2D(pool_size=(2, 2), strides=2),)\n\n# Flatten the layers so that we pass this to dense layers\ncnn.add(Flatten())\n\n# output layers\ncnn.add(Dense(units=2, activation='softmax'))\n\n","7b19af51":"# 1st we have to compile the model \nfrom tensorflow.keras.optimizers import Adam\ncnn.compile(optimizer=Adam(learning_rate=0.0001), loss='categorical_crossentropy', metrics=['accuracy'])  ####","dd3f1cd7":"cnn.summary()","de61889b":"# Add early stopping if overfitting happen then apply \nfrom tensorflow.keras.callbacks import EarlyStopping\nearly_stop = EarlyStopping(monitor='val_loss', patience=2)","c629aed7":"# now fit the model \ncnn.fit(x=training_set, epochs=15,validation_data=test_set, callbacks=[early_stop])","c14f682e":"losses = pd.DataFrame(cnn.history.history) # Using the early stoping ","0d7a6233":"losses[['accuracy','val_accuracy']].plot()","a1e6af30":"losses[['loss','val_loss']].plot()\n","45c10d50":"# Save the model so that we can use later \ncnn.save('my_model.h5')\n!ls","6cd26f01":"y_pred = cnn.predict(x=test_set, verbose=0)","c3ad79bc":"from sklearn.metrics import classification_report,confusion_matrix","5e1b93ca":"cm = confusion_matrix(y_true=test_set.classes, y_pred=np.argmax(y_pred, axis=-1))\nsns.heatmap(cm,annot=True)\nprint(cm)","f4e18d7a":"report = classification_report(y_true=test_set.classes, y_pred=np.argmax(y_pred, axis=-1))\nprint(report)","76b0431c":"test_img, test_label = next(test_set)\n\nplotImages(test_img)\nprint(test_label)","e927ce5a":"img_pred = cnn.predict(test_img)","041e6b87":"np.round(img_pred) # img predict by the model ","c3b8fdcf":"dat = '..\/input\/cat-and-dog\/test_set\/test_set\/cats\/cat.4005.jpg' # For cat images\ndat2 = '..\/input\/cat-and-dog\/test_set\/test_set\/dogs\/dog.4020.jpg' # For Dog images\n\nimport numpy as np\nfrom keras.preprocessing import image\ntest_image1 = image.load_img(dat2, target_size = (224, 224)) # Change when directory change \n\ntest_image = image.load_img(dat2, target_size = (224, 224)) # Change when directory change \ntest_image = image.img_to_array(test_image)\ntest_image = np.expand_dims(test_image, axis = 0)\nresult = cnn.predict(test_image)\ntraining_set.class_indices\nif result[0][0] == 1:\n  prediction = 'cat'\nelse:\n  prediction = 'dog'\n\nprint(prediction)\nprint('\\n')\nplt.imshow(test_image1)","0b9efac4":"### Create clasification report and confusion matrix","437febe4":"# Build an CNN ","97ae67b6":"# Thats All!","c6502a98":"We use categorical crossentropy because here is 2 neuron in output layers or the out put layers have two output\nand use Adam optimizer for socrates gradient descent","9766f6a7":"So our overall accuracy is around 75% and f1 score on cat iamges is grater then the dog images so model will predict better on cat images ","206f8f5d":"# Import the data ","56eaa3be":"# Predicting with Test image","90816104":"note that the input shape is always same as terget size\n \nhere we select default parameter for Polling layers\n\nthe output layer has 2 nodes, one for cat and one for dog. the softmax activation function is used so that we see the probability of each outcome.","6854d85e":"# Import Library","a3ddd037":"# Traing the model CNN ","6feed92b":"# Evaluate the model","a0141dae":"Early Stopping used to avoid over fitting and also by early stoping we get an idea about number of epoch required to fit the model ","62f1b395":"# Visualize the Data","3a6101ba":"### Predict image from data directory","bccf9fae":"We see that without using early stopping led us to overfitting so we have to use early stopping "}}