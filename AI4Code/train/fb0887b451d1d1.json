{"cell_type":{"8f127ef1":"code","0726a36c":"code","9ce1acf8":"code","f382b19f":"code","3d1371c2":"code","36c84152":"code","1e77a9d1":"code","e7fbd22d":"code","d946e817":"code","5b52411a":"code","a25de27f":"code","f6d7bd32":"markdown","9c62e4f4":"markdown","affb3cf4":"markdown","e1399713":"markdown","acd7229e":"markdown","74ca1e2b":"markdown","2a3c7e31":"markdown","d09d8986":"markdown","2022f1f9":"markdown","9309b956":"markdown"},"source":{"8f127ef1":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\nimport time\nstart_time = time.time()\nimport gc","0726a36c":"train = pd.read_csv(\"..\/input\/cs6601ai-spring20-assign4-bonus\/kaggle_train_2020_spring.csv\", header=None)\ntest = pd.read_csv(\"..\/input\/cs6601ai-spring20-assign4-bonus\/kaggle_test_2020_spring_unlabelled.csv\", header=None)","9ce1acf8":"display(train)","f382b19f":"display(test)","3d1371c2":"X = train.drop([0], axis=1)\nY = train[0]","36c84152":"import matplotlib.pyplot as plt\nimport seaborn as sns\nsns.heatmap(X.corr(), annot=True, linewidth=0.02)\nfig=plt.gcf()\nfig.set_size_inches(20,20)\nplt.show()","1e77a9d1":"corr = X.corr()\ndrop_cols = []\nfor col in X.columns:\n    if sum(corr[col].map(lambda x: abs(x) > 0.1)) <= 1:\n        drop_cols.append(col)\nX.drop(drop_cols, axis=1, inplace=True)\ndisplay(X)","e7fbd22d":"import matplotlib.pyplot as plt\nimport seaborn as sns\nsns.heatmap(X.corr(), annot=True, linewidth=0.02)\nfig=plt.gcf()\nfig.set_size_inches(20,20)\nplt.show()","d946e817":"from sklearn.tree import DecisionTreeClassifier\nclf = DecisionTreeClassifier(max_depth=100)\nclf.fit(X, Y)\nclf.score(X, Y)","5b52411a":"pred = clf.predict(test.drop([x-1 for x in drop_cols], axis=1))\nsub = pd.DataFrame()\nsub['# Id'] = [x for x in range(len(pred))]\nsub['Class'] = [x for x in map(int, pred)]\nsub.to_csv(\"kaggle_result.csv\", index=False)\ndisplay(sub)","a25de27f":"end_time = time.time()\n\nprint(\"Total time spent on running this kernel: %i seconds\" %(end_time-start_time))","f6d7bd32":"# Peek\n\nGood day, everyone~! What I am doing in this kernel is a simplified baseline where I will not perform any feature engineering or parameter optimization. Everything here is only to help you guys to know about how I approach problems.\n\nFirst of all, we will need to peek at our data. We need to see what we are actually working with, and the path they are located.","9c62e4f4":"You can see that of all the columns, most of them are non-correlated variables (with correlation of -0.1 < corr < 0.1). Since these variables will not help in training our model, we will be dropping them.","affb3cf4":"# Model\n\nFor this one, I am using `DecisionTreeClassifier` from the `sklearn.tree` module. As this is just a baseline, I will not edit anything but the `max_depth` of the tree.","e1399713":"# That's all folks!\n\nHope this helps in your Machine Learning Journey~! Please upvote or comment if you liked this kernel~!","acd7229e":"Next, we will load both `train` and `test`.","74ca1e2b":"Based on the competition explanation, the first column of train is the target, so we will be separating them into `X` and `Y`.","2a3c7e31":"# Prediction\n\nNow, you remember how we `dropped column 0` from the `train` data? Checking back again, the column names of `test` also started with `0` instead of `1`! This means that we can't use the `drop_cols` variable as is, as there's a misalignment to their column names.\n\nWe will apply a little transformation, before predicting the test!","d09d8986":"The score achieved by training our classifier with the full data, is at `99.98%`. Of course, since we didn't do any train-test split, this is without a doubt overfitting. But, since this is just a baseline, we will ignore it and then proceed to the prediction.","2022f1f9":"# Data Visualization\n\nAs I said earlier, this is just a baseline model so we will only simplify our data with a simple correlation matrix.","9309b956":"Okay, now we're dow with `8 columns` left. Looking at the correlation matrix, we can see that we removed almost all the uncorrelated columns. This will help our model to train better and achieve higher accuracy with lesses system resources used."}}