{"cell_type":{"fb1280dd":"code","07e420bc":"code","07881e84":"code","35e4b166":"code","bd1809f7":"code","9c767721":"code","b5771fbd":"code","8d2f2197":"code","d2259946":"code","0d02327d":"code","d36bf1f5":"code","4bb723e6":"code","1b19d0dc":"code","41209066":"code","459f6073":"code","3f7e0fd5":"code","619dd81d":"code","a26b0238":"code","6a560866":"code","d7f4958e":"code","d229aced":"code","29440b7a":"code","41ba5a09":"code","de682765":"code","ebaf3a83":"code","d89df844":"code","f30dda2c":"code","f1b5a874":"code","5536afa2":"code","f289de02":"code","47843079":"code","be6a9dd6":"code","397065de":"code","ca5b2d4a":"code","f4f47c2a":"code","2993a4ff":"code","0f702788":"code","a46b4cd2":"code","481e56b0":"code","911be2ad":"code","ae7870aa":"code","ada539be":"code","68816394":"code","16db1bc3":"markdown","f288a3a2":"markdown","54f52f42":"markdown","35fe7966":"markdown","eff87b4c":"markdown","ad5c54b7":"markdown","a0b77153":"markdown","763c3017":"markdown","4e903f04":"markdown","30721cde":"markdown","39dcfcd7":"markdown","4d918753":"markdown","8a70b780":"markdown","139f59c6":"markdown","a021ab23":"markdown","979ce294":"markdown"},"source":{"fb1280dd":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","07e420bc":"import pandas as pd\nimport matplotlib.pyplot as plt","07881e84":"# read train dataframe:\ndf_train = pd.read_csv(\"..\/input\/hr-analytics-job-change-of-data-scientists\/aug_train.csv\")\ndf_train.head()","35e4b166":"## show type and number of non Null values from features:\ndf_train.info()","bd1809f7":"## count target values:\nprint(\"Absolute Observation:\\n{}\".format(df_train.target.value_counts()))\nprint(\"\\nRelative Observation:\\n{}\".format(df_train.target.value_counts() \/ df_train.shape[0]))","9c767721":"df_train.city.value_counts()","b5771fbd":"# cast to int city feature:\ndf_train['city'] = [int(i[1]) for i in df_train.city.str.split(\"_\")]","8d2f2197":"# Replace Nan values with label Unknown_gender \ndf_train['gender'] = df_train.gender.fillna(\"Unknown_gender\")\nplt.barh(df_train.gender.value_counts().index, df_train.gender.value_counts().values)\nplt.show()","d2259946":"# Map on dummies relevent_experience feature:\ndf_train.relevent_experience = df_train.relevent_experience.map({\"Has relevent experience\":1., \"No relevent experience\":0.,})\nplt.barh(df_train.relevent_experience.value_counts().index, df_train.relevent_experience.value_counts().values)","0d02327d":"# Replace Nan values with label Unknown and Map enrolled_university feature:\ndf_train.enrolled_university = df_train.enrolled_university.fillna(\"Unknown\")\ndf_train.enrolled_university = df_train.enrolled_university.map({\"Unknown\":1., \"Part time course\":2., \"Full time course\":3., \"no_enrollment\":0.})\nplt.barh(df_train.enrolled_university.value_counts().index, df_train.enrolled_university.value_counts().values)","d36bf1f5":"# Replace Nan values with label Unknown and Map education_level feature:\ndf_train.education_level = df_train.education_level.fillna(\"Unknown\")\ndf_train.education_level = df_train.education_level.map({\"Unknown\":0., \"Primary School\":1., \"High School\":2., \"Graduate\":3., \"Masters\":4., \"Phd\":5.})\nplt.barh(df_train.education_level.value_counts().index, df_train.education_level.value_counts().values)","4bb723e6":"# Replace Nan values with label Unknown_discipline in major_discipline feature:\ndf_train.major_discipline = df_train.major_discipline.fillna(\"Unknown_discipline\")\nplt.barh(df_train.major_discipline.value_counts().index, df_train.major_discipline.value_counts().values)","1b19d0dc":"##  Experience:\n##  Replace Nan values with -1 (cannot be forgotten!!! it's crucial information!!!).\n##  Replace >20 values with 21\n##  Replace <1 values with 0\narray_experience = []\nfor i in df_train.experience:\n    if i == \">20\": array_experience.append(21.)\n    elif i == \"<1\": array_experience.append(0.)\n    else: \n        try: array_experience.append(float(int(i)))\n        except: array_experience.append(i)\n\ndf_train.experience = array_experience\ndf_train.experience.fillna(-1., inplace=True)\nplt.barh(df_train.experience.value_counts().index, df_train.experience.value_counts().values)","41209066":"# Replace Nan values with label Unknown in company_size feature:\ndf_train.company_size.fillna(\"Unknown\", inplace=True)\nplt.barh(df_train.company_size.value_counts().index, df_train.company_size.value_counts().values)","459f6073":"# Map company_size values:\ndf_train.company_size = df_train.company_size.map({\"Unknown\": 0., \"<10\": 1., \"10\/49\":2., \"50-99\": 3., \"100-500\":4. , \"500-999\":5., \"1000-4999\":6., \"5000-9999\": 7., \"10000+\": 8.})\nplt.barh(df_train.company_size.value_counts().index, df_train.company_size.value_counts().values)","3f7e0fd5":"# Replace Nan values with label Unknown_company_type in company_type feature:\ndf_train.company_type.fillna(\"Unknown_company_type\", inplace=True)\nplt.barh(df_train.company_type.value_counts().index, df_train.company_type.value_counts().values)","619dd81d":"## Raplace Nan values with label never. \ndf_train.last_new_job.fillna(\"never\", inplace=True)\ndf_train.last_new_job = df_train.last_new_job.map({\"4\":4., \"3\":3., \"2\":2., \"1\":1., \">4\":5., \"never\":0.})\nplt.barh(df_train.last_new_job.value_counts().index, df_train.last_new_job.value_counts().values)","a26b0238":"df_train.head()","6a560866":"df_train.info()","d7f4958e":"gender_dummies = pd.get_dummies(df_train.gender)\ngender_dummies.columns = [\"gender_{}\".format(i) for i in gender_dummies.columns]\ndf_train = pd.concat([df_train, gender_dummies], axis=1)\n\nmajor_discipline_dummies = pd.get_dummies(df_train.major_discipline)\nmajor_discipline_dummies.columns = [\"major_discipl_{}\".format(i) for i in major_discipline_dummies.columns]\ndf_train = pd.concat([df_train, major_discipline_dummies], axis=1)\n\ncompany_type_dummies = pd.get_dummies(df_train.company_type)\ncompany_type_dummies.columns = [\"company_type_{}\".format(i) for i in company_type_dummies.columns]\ndf_train = pd.concat([df_train, company_type_dummies], axis=1)\n\ndf_train.drop(['gender', \"company_type\", \"major_discipline\"], axis=1, inplace=True)\ndf_train.head()","d229aced":"X = df_train.drop(['enrollee_id', \"target\"], axis=1)\ny = df_train['target']","29440b7a":"from sklearn.model_selection import train_test_split","41ba5a09":"X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=.33, stratify=y)\n\nprint(X_train.shape, X_valid.shape, y_train.shape, y_valid.shape)","de682765":"from sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import confusion_matrix, f1_score, classification_report","ebaf3a83":"model_tree = DecisionTreeClassifier(random_state=0)\nmodel_tree.fit(X_train, y_train)","d89df844":"path = model_tree.cost_complexity_pruning_path(X_train, y_train)\nccp_alphas, impurities = path.ccp_alphas, path.impurities","f30dda2c":"## Show level of imputiry correlate of alpha prining:\nfig, ax = plt.subplots(figsize=(10,5))\nax.plot(ccp_alphas[:-1], impurities[:-1], marker='o', drawstyle=\"steps-post\")\nax.plot(ccp_alphas[-1], impurities[-1], marker='o', drawstyle=\"steps-post\", color=\"red\", label=\"Albero con un solo nodo\")\nplt.legend()\nax.set_xlabel(\"effective alpha\")\nax.set_ylabel(\"total impurity of leaves\")\nax.set_title(\"Total Impurity vs effective alpha for training set\")","f1b5a874":"# We estimate a model for each alpha parameters:\nclfs = []\nfor ccp_alpha in ccp_alphas:\n    clf = DecisionTreeClassifier(random_state=0, ccp_alpha=ccp_alpha)\n    clf.fit(X_train, y_train)\n    clfs.append(clf)\nprint(\"Number of nodes in the last tree is: {} with ccp_alpha: {}\".format(\n      clfs[-1].tree_.node_count, ccp_alphas[-1]))","5536afa2":" # show when decrease the nodes\/depth as the parameter increases\n\nclfs = clfs[:-1]\nccp_alphas = ccp_alphas[:-1]\n\nnode_counts = [clf.tree_.node_count for clf in clfs]\ndepth = [clf.tree_.max_depth for clf in clfs]\nfig, ax = plt.subplots(2, 1, figsize=(10, 7))\nax[0].plot(ccp_alphas, node_counts, marker='o', drawstyle=\"steps-post\")\nax[0].set_xlabel(\"alpha\")\nax[0].set_ylabel(\"number of nodes\")\nax[0].set_title(\"Number of nodes vs alpha\")\nax[1].plot(ccp_alphas, depth, marker='o', drawstyle=\"steps-post\")\nax[1].set_xlabel(\"alpha\")\nax[1].set_ylabel(\"depth of tree\")\nax[1].set_title(\"Depth vs alpha\")\nfig.tight_layout()\nplt.plot()","f289de02":"# I choose best alpha model based on the best F1 score: \ntrain_scores = [f1_score(y_train, clf.predict(X_train), average='macro') for clf in clfs]\ntest_scores = [f1_score(y_valid, clf.predict(X_valid), average='macro') for clf in clfs]\n\nfig, ax = plt.subplots()\nax.set_xlabel(\"alpha\")\nax.set_ylabel(\"F1\")\nax.set_title(\"F1 vs alpha for training and testing sets\")\nax.plot(ccp_alphas, train_scores, marker='o', label=\"train\",\n        drawstyle=\"steps-post\")\nax.plot(ccp_alphas, test_scores, marker='o', label=\"test\",\n        drawstyle=\"steps-post\")\nax.legend()\nplt.show()\nprint(\"Max F1: {}. Index: {}\".format(round(max(test_scores),4), test_scores.index(max(test_scores))))","47843079":"# I choose best alpha model based on the best Accuracy score: \ntrain_scores = [clf.score(X_train, y_train) for clf in clfs]\ntest_scores = [clf.score(X_valid, y_valid) for clf in clfs]\n\nfig, ax = plt.subplots()\nax.set_xlabel(\"alpha\")\nax.set_ylabel(\"Accuracy\")\nax.set_title(\"Accuracy vs alpha for training and testing sets\")\nax.plot(ccp_alphas, train_scores, marker='o', label=\"train\",\n        drawstyle=\"steps-post\")\nax.plot(ccp_alphas, test_scores, marker='o', label=\"test\",\n        drawstyle=\"steps-post\")\nax.legend()\nplt.show()\nprint(\"Max F1: {}. Index: {}\".format(round(max(test_scores),4), test_scores.index(max(test_scores))))","be6a9dd6":"model_bestF1 = clfs[948]\nmodel_bestAcu = clfs[949]","397065de":"# risultati modelllo 923 (F1 Score):\nyh_train_bestF1 = model_bestF1.predict(X_train)\nyh_test_bestF1 = model_bestF1.predict(X_valid)\n\nprint(\"---\"*30, \"\\nTRAINING DATA:\\n\")\nprint(\"Confusion Matrix (train data):\\n{}\\n\".format(confusion_matrix(y_train, yh_train_bestF1)))\nprint(\"Classification Report (train data):\\n{}\\n\".format(classification_report(y_train, yh_train_bestF1)))\n\nprint(\"---\"*30, \"\\nVALIDATION DATA:\\n\")\nprint(\"Confusion Matrix (valid data):\\n{}\\n\".format(confusion_matrix(y_valid, yh_test_bestF1)))\nprint(\"Classification Report (valid data):\\n{}\\n\".format(classification_report(y_valid, yh_test_bestF1)))","ca5b2d4a":"# get importance\nimportance = model_bestF1.feature_importances_\n# summarize feature importance\nfor i,v in enumerate(importance):\n\tprint('Feature: %0d, Score: %.5f' % (i,v))\n# plot feature importance\nplt.figure(figsize=(7,10))\nplt.barh([X.columns[x] for x in range(len(importance))], importance)\nplt.show()","f4f47c2a":"from sklearn import tree","2993a4ff":"plt.figure(figsize=(20,15))\ntree.plot_tree(model_bestF1)\nplt.show()","0f702788":"def encodeGendere(X):\n    X['gender'] = X['gender'].fillna(\"Unknown_gender\")\n    gender_dummies = pd.get_dummies(X[\"gender\"])\n    gender_dummies.columns = [\"gender_{}\".format(i) for i in gender_dummies.columns]\n    X = pd.concat([X, gender_dummies], axis=1)\n    X.drop(['gender'], axis=1, inplace=True)\n    return X\n\ndef encodeRelevent_Experience(X):\n    X[\"relevent_experience\"] = X[\"relevent_experience\"].map({\"Has relevent experience\":1., \"No relevent experience\":0.,})\n\ndef encodeEnrolled_university(X):\n    X[\"enrolled_university\"] = X[\"enrolled_university\"].fillna(\"Unknown\")\n    X[\"enrolled_university\"] = X[\"enrolled_university\"].map({\"Unknown\":1., \"Part time course\":2., \"Full time course\":3., \"no_enrollment\":0.})\n\ndef encodeEducation_level(X):\n    X[\"education_level\"] = X[\"education_level\"].fillna(\"Unknown\")\n    X[\"education_level\"] = X[\"education_level\"].map({\"Unknown\":0., \"Primary School\":1., \"High School\":2., \"Graduate\":3., \"Masters\":4., \"Phd\":5.})\n    \ndef encodeMajor_discipline(X):\n    X[\"major_discipline\"] = X[\"major_discipline\"].fillna(\"Unknown_discipline\")\n    major_discipline_dummies = pd.get_dummies(X[\"major_discipline\"])\n    major_discipline_dummies.columns = [\"major_discipl_{}\".format(i) for i in major_discipline_dummies.columns]\n    X = pd.concat([X, major_discipline_dummies], axis=1)\n    X.drop([\"major_discipline\"], axis=1, inplace=True)\n    return X\n\n    \ndef encodeExperience(X):\n    array_experience = []\n    for i in X[\"experience\"]:\n        if i == \">20\": array_experience.append(21.)\n        elif i == \"<1\": array_experience.append(0.)\n        else: \n            try: array_experience.append(float(int(i)))\n            except: array_experience.append(i)\n\n    X[\"experience\"] = array_experience\n    X[\"experience\"].fillna(-1., inplace=True)\n\ndef encodeCompany_size(X):\n    X[\"company_size\"].fillna(\"Unknown\", inplace=True)\n    X[\"company_size\"] = X[\"company_size\"].map({\"Unknown\": 0., \"<10\": 1., \"10\/49\":2., \"50-99\": 3., \"100-500\":4. , \"500-999\":5., \"1000-4999\":6., \"5000-9999\": 7., \"10000+\": 8.})\n\ndef encodeCompany_type(X):\n    X[\"company_type\"].fillna(\"Unknown_company_type\", inplace=True)\n    company_type_dummies = pd.get_dummies(X[\"company_type\"])\n    company_type_dummies.columns = [\"company_type_{}\".format(i) for i in company_type_dummies.columns]\n    X = pd.concat([X, company_type_dummies], axis=1)\n    X.drop([\"company_type\"], axis=1, inplace=True)\n    return X\n    \ndef encodeLast_new_job(X):\n    X[\"last_new_job\"].fillna(\"never\", inplace=True)\n    X[\"last_new_job\"] = X[\"last_new_job\"].map({\"4\":4., \"3\":3., \"2\":2., \"1\":1., \">4\":5., \"never\":0.})\n\ndef encodeCity(X):\n    X['city'] = [int(i[1]) for i in X[\"city\"].str.split(\"_\")]\n\ndef MyPipiline_encoder(X):\n    X = encodeGendere(X)\n    encodeRelevent_Experience(X)\n    encodeEnrolled_university(X)\n    encodeEducation_level(X)\n    X = encodeMajor_discipline(X)\n    encodeExperience(X)\n    encodeCompany_size(X)\n    X = encodeCompany_type(X)\n    encodeLast_new_job(X)\n    encodeCity(X)\n    return X.drop(['enrollee_id'], axis=1), X.enrollee_id","a46b4cd2":"df_test = pd.read_csv(\"..\/input\/hr-analytics-job-change-of-data-scientists\/aug_test.csv\")\ndf_test.head()","481e56b0":"X_test, enrollee_id = MyPipiline_encoder(df_test)","911be2ad":"X_test.head()","ae7870aa":"enrollee_id.head()","ada539be":"submission_df = pd.DataFrame({\"enrollee_id\": enrollee_id,\n                             \"target\":model_bestF1.predict(X_test)})\nsubmission_df.head()","68816394":"submission_df.to_csv(\"submission.csv\", index=False, sep=\";\")","16db1bc3":"## I show the most significant variables:","f288a3a2":"## Basic EDA for understand what kind of transformation we need to do","54f52f42":"## We'll define the training and validation dataset:","35fe7966":"# Prediction with Test set:","eff87b4c":"### Considering that the dataset is a dichotomous variable matrix, we cannot apply linear models because the assumptions will not be respected.\n### Usually the models that work best with this type of matrix are DecisionTrees and RandomForest. \n### I usually prefer Trees because they are easy to represent.","ad5c54b7":"## Features such as city_development_index, company_size and education_level are the most important to help the model classify.","a0b77153":"### We are in a situation of unbalanced data...","763c3017":"## the model selected through cost complexity pruning, guarantees us an accuracy of 80% and F1 Score of 74%. The most important note is that the model thus selected does not suffer from overfitting, so it is able to generalize well.","4e903f04":"# Context and Content:\nA company which is active in Big Data and Data Science wants to hire data scientists among people who successfully pass some courses which conduct by the company. Many people signup for their training. Company wants to know which of these candidates are really wants to work for the company after training or looking for a new employment because it helps to reduce the cost and time as well as the quality of training or planning the courses and categorization of candidates. Information related to demographics, education, experience are in hands from candidates signup and enrollment.\n\nThis dataset designed to understand the factors that lead a person to leave current job for HR researches too. By model(s) that uses the current credentials,demographics,experience data you will predict the probability of a candidate to look for a new job or will work for the company, as well as interpreting affected factors on employee decision.\n\nThe whole data divided to train and test . Target isn't included in test but the test target values data file is in hands for related tasks. A sample submission correspond to enrollee_id of test set provided too with columns : enrollee _id , target\n\n## Note:\n\n- The dataset is imbalance.\n- Most features are categorical (Nominal, Ordinal, or Binary), some of these have high cardinality.\n- Missing imputation can be a part of your pipeline as well.\n\n## Features\n\n- enrollee_id : Unique ID for candidate\n\n- city: City code\n\n- city_development_index : Developement index of the city (scaled)\n\n- gender: Gender of candidate\n\n- relevent_experience: Relevant experience of candidate\n\n- enrolled_university: Type of University course enrolled if any\n\n- education_level: Education level of candidate\n\n- major_discipline :Education major discipline of candidate\n\n- experience: Candidate total experience in years\n\n- company_size: No of employees in current employer's company\n\n- company_type : Type of current employer\n\n- lastnewjob: Difference in years between previous job and current job\n\n- training_hours: training hours completed\n\n- target: 0 \u2013 Not looking for job change, 1 \u2013 Looking for a job change\n\n## Inspiration\n- Predict the probability of a candidate fit well with the company that it is hiring.\n- Interpret model(s) such a way that illustrate which features affect candidate decision.","30721cde":"## I prefer best F1 Score model (model_bestF1).","39dcfcd7":"## Predict:","4d918753":"# Defining my pipeline encoder:","8a70b780":"### I use the cost complexity pruning technique to prune the DecisionTree. This technique allows you to prune the tree according to the importance of the nodes.","139f59c6":"## Cast to dummy following features:\n- company type\n- major discipline\n- gender\n","a021ab23":"## Encoding:","979ce294":"## Tree rappresentation:"}}