{"cell_type":{"a11caef2":"code","8964ae82":"code","05f4d547":"code","7c8ff613":"code","89d7ae35":"code","f2a5236b":"code","984a0967":"code","0ebd3dce":"code","04e4892a":"code","7e0090c2":"markdown","aacf8539":"markdown","d01174ea":"markdown","c6a5f6bd":"markdown","63bd2d0a":"markdown","ddbbbf78":"markdown","9ba5ef12":"markdown","bab16eb9":"markdown","75c5069c":"markdown","b4869102":"markdown","d9d54218":"markdown","682c1455":"markdown"},"source":{"a11caef2":"!pip install -U efficientnet\n\nimport os\nimport sys\n\nimport math\nfrom random import shuffle\nimport time\n\nimport numpy as np\nimport pandas as pd\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nimport tensorflow as tf\nfrom tensorflow.keras import models, layers, metrics\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.losses import BinaryCrossentropy, CategoricalCrossentropy\nfrom tensorflow.keras.applications import EfficientNetB0, InceptionV3\n\nfrom sklearn.model_selection import train_test_split, KFold\n\nimport efficientnet.keras as efn","8964ae82":"def auto_select_accelerator():\n    \"\"\"\n    Reference: \n        * https:\/\/www.kaggle.com\/mgornergoogle\/getting-started-with-100-flowers-on-tpu\n        * https:\/\/www.kaggle.com\/xhlulu\/ranzcr-efficientnet-tpu-training\n    \"\"\"\n    try:\n        tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n        tf.config.experimental_connect_to_cluster(tpu)\n        tf.tpu.experimental.initialize_tpu_system(tpu)\n        strategy = tf.distribute.experimental.TPUStrategy(tpu)\n        print(\"Running on TPU:\", tpu.master())\n    except ValueError:\n        strategy = tf.distribute.get_strategy()\n    print(f\"Running on {strategy.num_replicas_in_sync} replicas\")\n    \n    return strategy","05f4d547":"strategy = auto_select_accelerator()\nbatch_size=16*strategy.num_replicas_in_sync","7c8ff613":"train_labels = pd.read_csv('..\/input\/seti-breakthrough-listen\/train_labels.csv')\n\ntrain_idx=train_labels['id'].values\ny = train_labels['target'].values\n\nx_train,x_valid,y_train,y_valid = train_test_split(train_idx,y,test_size=0.05,random_state=42)\n\n#oversample needles for training data\nwhere_needle = np.where(y_train == 1)[0]\n\noversample_x = x_train[where_needle]\noversample_y = y_train[where_needle]\n\noversample_x = np.concatenate([oversample_x for i in range(2)])\noversample_y = np.concatenate([oversample_y for i in range(2)])\n\nx_train = np.concatenate([x_train, oversample_x])\ny_train = np.concatenate([y_train, oversample_y])\n\n#shuffle data\nidx = shuffle([i for i in range(x_train.shape[0])])\n\n#reformat data into needed format\nx_train = x_train[idx].reshape(-1, 1)\ny_train = y_train[idx].reshape(-1, 1)\n\nx_valid = x_valid.reshape(-1, 1)\ny_valid = y_valid.reshape(-1, 1)\n\n#create tensorflow datasets\ndtrain = tf.data.Dataset.from_tensor_slices((x_train, y_train))\ndtrain = dtrain.shuffle(1024).batch(batch_size)\n\ndvalid = tf.data.Dataset.from_tensor_slices((x_valid, y_valid))\ndvalid = dvalid.batch(batch_size)","89d7ae35":"def create_model():\n    input_ = layers.Input((3*273,256,3))\n    \n    x = efn.EfficientNetB0(input_shape=(3*273,256,3),weights='noisy-student',include_top=False)(input_)\n    x = layers.GlobalAveragePooling2D()(x)\n    \n    x = layers.Dense(32)(x)\n    x = layers.Activation(\"relu\")(x)\n    \n    x = layers.Dense(1, activation=\"sigmoid\")(x)\n    \n    model = models.Model(inputs=input_, outputs=x)\n    \n    return model","f2a5236b":"with strategy.scope():\n    model = create_model()\n    \n    optimizer = Adam(learning_rate=1e-3)\n    \n    train_auc = metrics.AUC()\n    valid_auc = metrics.AUC()\n    \n    train_loss = metrics.Sum()\n    valid_loss = metrics.Sum()\n    \n    loss = lambda a,b: tf.nn.compute_average_loss(BinaryCrossentropy(reduction=tf.keras.losses.Reduction.NONE)(a,b),\n                                                  global_batch_size=batch_size)","984a0967":"def id_to_path(idx,train=True):\n    path = '..\/input\/seti-breakthrough-listen\/'\n    if train:\n        folder = 'train\/'\n    else:\n        folder = 'test\/'\n    path+=folder+idx[0]+'\/'+idx+'.npy'\n    return path\n\ndef decode_batch(batch_ids):\n    batch_x = list()\n    \n    for x in batch_ids.numpy():\n        \n        arr = np.load(id_to_path(x[0].decode(), train=True))\n  \n        x1 = arr[[0, 2, 4]].reshape(3*273, 256, 1)\n        x2 = arr[[1, 3, 5]].reshape(3*273, 256, 1)\n            \n        batch_x.append(np.concatenate([x1, x2, x1], axis=2).reshape(3*273, 256, 3))\n        \n    return np.array(batch_x)","0ebd3dce":"@tf.function\ndef train_step(x, y):\n    #apply gradient descent\n    with tf.GradientTape() as tape:\n        predictions = model(x, training=True)       \n        loss_value = loss(y, predictions)\n                \n    grads = tape.gradient(loss_value, model.trainable_weights) \n    optimizer.apply_gradients(zip(grads, model.trainable_weights))\n    \n    #update metrics\n    train_auc.update_state(y, predictions)\n    train_loss.update_state(loss_value)\n\n@tf.function\ndef valid_step(x, y):\n    predictions = model(x, training=False)\n    \n    loss_value = loss(y, predictions)\n    \n    #update metrics\n    valid_auc.update_state(y, predictions)\n    valid_loss.update_state(loss_value)","04e4892a":"#gets the number of batches in datasets\ntrain_batches = [i for i,_ in enumerate(dtrain)][-1] + 1\nvalid_batches = [i for i,_ in enumerate(dvalid)][-1] + 1\n\n#saves the best metric(auc) value as a type ModelCheckpoint callback\nbest_metric = 0\n\nfor epoch in range(5):\n    epoch_start_time = time.time()\n    \n    print(\"Starting Epoch {}:\".format(epoch + 1))\n    #train for all batches in dataset\n    for step, (batch_x, batch_y) in enumerate(dtrain):\n        batch_x = decode_batch(batch_x)\n        \n        strategy.run(train_step, args=(batch_x, batch_y))\n        \n        sys.stdout.write(\"\\rFinished Step {} \/ {} --> loss: {} | AUC: {}\".format(step + 1, train_batches, \n                                                                                 train_loss.result().numpy()\/(step + 1),\n                                                                                 train_auc.result().numpy()))\n        sys.stdout.flush()\n        \n    #shuffle dataset for next epoch\n    dtrain = dtrain.shuffle(1024)\n    \n    #validate with validation dataset\n    for batch_x, batch_y in dvalid:\n        batch_x = decode_batch(batch_x)\n        \n        strategy.run(valid_step, args=(batch_x, batch_y))\n        \n    print(\"val_loss: {} | val_AUC: {}\".format(valid_loss.result().numpy()\/(step + 1), valid_auc.result().numpy()))\n    print(\"Finishing after {} minutes...\".format((time.time() - epoch_start_time)\/(60)))\n    print(\"\")\n    \n    #save model if new best metric has been achieved\n    if best_metric < valid_auc.result().numpy():\n        model.save(\".\/model.h5\")\n        best_metric = valid_auc.result().numpy()\n    \n    #reset metrics\n    train_loss.reset_states()\n    train_auc.reset_states()\n    valid_loss.reset_states()\n    valid_auc.reset_states()","7e0090c2":"# Modeling","aacf8539":"# Dataset Creation","d01174ea":"**TPU selection Function**","c6a5f6bd":"# TPU Modeling\n\nThis Notebook aims to demonstrate how to circumvent the RAM restriction, while using the TPU. This Notebook uses a custom training loop, which unlike the tf.keras.utils.Sequence method of loading the data, also allows the usage of the TPU. If anybody has additional ideas on how to speed up the training process, feedback would be appreciated!","63bd2d0a":"**Model Function**","ddbbbf78":"**Training Functions**","9ba5ef12":"**Initialize Model & Metrics in TPU Strategy**","bab16eb9":"**Select TPU**","75c5069c":"**Data loading Functions**","b4869102":"**Training**","d9d54218":"# TPU Setup","682c1455":"**imports**"}}