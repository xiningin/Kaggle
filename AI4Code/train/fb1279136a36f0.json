{"cell_type":{"af311068":"code","fcf14269":"code","fc1841d6":"code","05f8d79f":"code","1ef2b92e":"code","63384313":"code","9cf4d2a4":"code","45d853b9":"code","5c3f6cd0":"code","b4347e6b":"code","5d926eeb":"code","bdbf95c0":"code","a96f7e64":"code","e52f309d":"code","bd0cdd91":"code","c49a1f56":"code","91ed83ac":"code","759a6c41":"code","b2825e61":"code","ea157b06":"code","8d7b76de":"code","8da44e70":"code","8afaa075":"code","b399e6ed":"code","3243e4bb":"code","6d2ac4dc":"code","327caa91":"markdown","697060c4":"markdown","095ec5d8":"markdown","c17b667f":"markdown","b060631d":"markdown","526cdc02":"markdown","6c4d1e12":"markdown"},"source":{"af311068":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","fcf14269":"import matplotlib.pyplot as plt\nimport math","fc1841d6":"ftPath = \"\/kaggle\/input\/housingprice\/ex1data2.txt\"\nlines = [] # Living area & number of bedroom\nwith open(ftPath) as f:\n    for line in f:\n        a = [float(i) for i in line.strip().split(',')]\n        lines.append(a)\ndata = np.array(lines)","05f8d79f":"data","1ef2b92e":"X = data[:,[0, 1]]\ny = np.c_[data[:,2]]","63384313":"print(len(X), len(y))","9cf4d2a4":"a = X[:, 0]\nprint(a)","45d853b9":"plt.figure(figsize=(10, 5))\nplt.plot(a, y, \"bo\")\n# plt.axis([140, 190, 45, 75])\nplt.xlabel(\"Living area\")\nplt.ylabel(\"Price\")\nxLine = np.array([700, 4500])\nyLine = 50000 + 60*xLine\nyOptima = 71270 + 134.5*xLine\nyTest = 4 + 165*xLine\nySGD = -78.34792489 + 361.51173555*xLine\nyMini = 165*xLine # similar to yTest\n \nplt.plot(xLine, yLine, marker=\"o\", color=\"r\")\nplt.plot(xLine, yOptima, marker=\"^\", color=\"g\")\nplt.plot(xLine, yTest, marker=\"x\", color=\"darkorange\")\nplt.plot(xLine, ySGD, color=\"lightblue\")\nplt.grid(True)\nplt.show()","5c3f6cd0":"m = len(a)\nA = 1\/(2*m) * (a.T.dot(a))\nB = -1\/(2*m) * (a.T.dot(y))\nC = 1\/(2*m) * (y.T.dot(y))\nprint(A, B, C)\n# 2310419.212765957 -382104564.09574467 65591548106.45744","b4347e6b":"plt.figure(figsize=(10, 5))\ntheta = np.linspace(-230, 400, 1000)\nJ = A*theta**2 + B*theta + C\nplt.plot(theta, J.reshape(-1, 1), color='r')\nplt.xlabel(\"Theta\")\nplt.ylabel(\"Cost function\")\nplt.grid(True)\nplt.show()","5d926eeb":"# Solve the problem for theta0, theta1\nfrom sklearn.linear_model import LinearRegression \nlin_reg = LinearRegression()\nlin_reg.fit(X, y)\nprint(lin_reg.intercept_, lin_reg.coef_)","bdbf95c0":"from sklearn.linear_model import LinearRegression \nlin_reg = LinearRegression()\nlin_reg.fit(X, y)\nprint(lin_reg.intercept_, lin_reg.coef_)","a96f7e64":"theta_best = np.array([[89597.9], [139.21067402], [-8738.01911233]])\nXb = np.c_[np.ones((m, 1)), X]\nerror = 1\/m * math.sqrt((Xb.dot(theta_best)-y).T.dot(Xb.dot(theta_best)-y))\nprint(\"MSE:\", error)","e52f309d":"m = len(X)","bd0cdd91":"c = len(Xb[0])\ntheta = np.random.randn(c,1)\nprint(theta)\neta = 2e-7","c49a1f56":"# Each iteration\ntmp1 = Xb.dot(theta) - y\ngradients = (2\/m) * Xb.T.dot(Xb.dot(theta) - y)\ntheta = theta - eta * gradients\nerr = 1\/m * math.sqrt((Xb.dot(theta)-y).T.dot(Xb.dot(theta)-y))\nprint(\"MSE:\", err) \nprint(theta)","91ed83ac":"# TODO: Add early stopping criteria if err doesn't decrease more than epsilon value\n# TODO: Check the number of iteration to converge\nepsilon = 2e-5\nfor i in range(1000):\n    tmp1 = Xb.dot(theta) - y\n    gradients = (2\/m) * Xb.T.dot(Xb.dot(theta) - y)\n    theta = theta - eta * gradients\n    oldErr = err\n    err = 1\/m * math.sqrt((Xb.dot(theta)-y).T.dot(Xb.dot(theta)-y))\n    if abs(err-oldErr) < epsilon:\n        break\nprint('err', err)\nprint('theta:', theta)\nprint(\"MSE:\", err)\nprint(\"abs err:\", abs(err-oldErr))\nprint(\"-\"*10)","759a6c41":"theta = np.array([[90000], [120], [-8000]])\nprint(theta)\neta = 2e-7","b2825e61":"tmp1 = Xb.dot(theta) - y\ngradients = (2\/m) * Xb.T.dot(Xb.dot(theta) - y)\ntheta = theta - eta * gradients\nerr = 1\/m * math.sqrt((Xb.dot(theta)-y).T.dot(Xb.dot(theta)-y))\nprint(\"MSE:\", err)\nprint(theta)","ea157b06":"# TODO: Add early stopping criteria if err doesn't decrease more than epsilon value\n# TODO: Check the number of iteration to converge\nepsilon = 2e-6\nfor i in range(1000):\n    tmp1 = Xb.dot(theta) - y\n    gradients = (2\/m) * Xb.T.dot(Xb.dot(theta) - y)\n    theta = theta - eta * gradients\n    oldErr = err\n    err = 1\/m * math.sqrt((Xb.dot(theta)-y).T.dot(Xb.dot(theta)-y))\n    if abs(err-oldErr) < epsilon:\n        break\nprint('gradients:', gradients)\nprint('theta:', theta)\nprint(\"MSE:\", err)\nprint(\"abs err:\", abs(err-oldErr))\nprint(\"-\"*10)","8d7b76de":"from sklearn.preprocessing import MinMaxScaler\nscaler = MinMaxScaler()\nscaler.fit(Xb)\nscaleX = scaler.transform(Xb)","8da44e70":"c = len(Xb[0])\ntheta = np.random.randn(c,1)\nprint(theta)\neta = 0.5","8afaa075":"for i in range(1000):\n    tmp1 = scaleX.dot(theta) - y\n    gradients = (2\/m) * scaleX.T.dot(scaleX.dot(theta) - y)\n    theta = theta - eta * gradients\n    err = 1\/m * math.sqrt((scaleX.dot(theta)-y).T.dot(scaleX.dot(theta)-y))\nprint(i)\nprint(theta)\nprint(\"MSE:\", err)\nprint(\"-\"*10)","b399e6ed":"# TODO Question: How to convert theta from scaled value back to normal value? Derive the equation.","3243e4bb":"nEpochs = 60\nt0, t1 = 2, 10000000 # learning schedule hyperparameters\n\ndef learningSchedule(t): \n    return t0\/(t+t1)\n\nc = len(Xb[0])\ntheta = np.random.randn(c,1) # random initialization\nprint(theta)\nfor epoch in range(nEpochs): \n    for i in range(m):\n        # TODO: Implement\n        # - randomly take 1 sample\n        rand_int = np.random.randint(0, m)\n        xb_i = Xb[rand_int,:].reshape(1, Xb.shape[1])\n        y_i  = y[rand_int].reshape(1,1)\n        # - update gradients value by that sample\n        eta = learningSchedule(epoch*m + i)\n        theta = theta - eta * gradients\n        err = 1\/m * math.sqrt((xb_i.dot(theta)-y_i).T.dot(xb_i.dot(theta)-y_i))\n#         print(\"Iteration:\", epoch*m + i)\n#         print(\"Eta:\", eta)\nprint(theta)\nprint(\"MSE:\", err)\nprint(\"-\"*10)","6d2ac4dc":"nEpochs = 100\nbatchSize = 10\nt0, t1 = 2, 10000000 # learning schedule hyperparameters\n\ndef learningSchedule(t): \n    return t0\/(t+t1)\nc = len(Xb[0])\ntheta = np.random.randn(c,1) # random initialization\nprint(theta)\nbatchSize = 10\nfor epoch in range(nEpochs): \n    # TODO: Implement\n    # - randomly take 10 samples (batchSize)\n    indices = np.random.permutation(m)\n    X = Xb[indices]\n    Y = y[indices]\n    # - update gradients value by those samples\n    # - change the batchSize value to see the difference\n    for i in range(0,m,batchSize):\n        xb_i = X[i:i+batchSize]\n        y_i  = Y[i:i+batchSize]\n        xb_i = np.c_[np.ones(len(xb_i)),xb_i]\n        \n        eta = learningSchedule(epoch)\n        theta = theta - eta * gradients\n        err = 1\/m * math.sqrt((xb_i.dot(theta)-y_i).T.dot(xb_i.dot(theta)-y_i))\n#         print(epoch)\n#         print(\"Eta:\", eta)\nprint(theta)\nprint(\"MSE:\", err)","327caa91":"### With chosen init","697060c4":"## Mini-batch Gradient Descent","095ec5d8":"## Batch gradient descent","c17b667f":"### With scale data","b060631d":"## Stochastic Gradient Descent","526cdc02":"### With dummy init","6c4d1e12":"## Using sklearn built-in function"}}