{"cell_type":{"474a5939":"code","ee45ab48":"code","a23d5e88":"code","b67bcf1e":"code","e5d1d248":"code","3554fdf7":"code","ffeca8dc":"code","f134832d":"code","29cfba79":"code","b90c5d5e":"code","38b35ee9":"code","89ce7d0a":"code","6052cfe7":"code","afe0f35a":"code","b867c3b5":"code","9ca7bdb7":"markdown","43610dbc":"markdown"},"source":{"474a5939":"%matplotlib inline\nimport os\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix, accuracy_score\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn import preprocessing\n\nimport matplotlib.image as mpimg\nfrom skimage.io import imread, imshow\nfrom skimage.color import rgb2gray\nfrom skimage import data, color, io, filters, morphology,transform, exposure, feature, util\nfrom scipy import ndimage\n\n\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Activation\nfrom tensorflow.keras.layers import Input, Dense, Dropout, Flatten, BatchNormalization,concatenate\nfrom tensorflow.keras.layers import Input, Dense, Conv2D, MaxPooling2D, UpSampling2D, Conv2DTranspose\nfrom tensorflow.keras import backend as K\nfrom tensorflow.keras.utils import to_categorical\nfrom tensorflow.keras.preprocessing import image\nfrom tensorflow.keras import regularizers\n\nfrom tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint","ee45ab48":"# Dice coefficient loss\n\ndef dice_coef(y_true, y_pred, smooth=1):\n    intersection = K.sum(K.abs(y_true * y_pred), axis=-1)\n    return (2. * intersection + smooth) \/ (K.sum(K.square(y_true),-1) + K.sum(K.square(y_pred),-1) + smooth)\n\ndef dice_coef_loss(y_true, y_pred):\n    return 1-dice_coef(y_true, y_pred)","a23d5e88":"# Uitwerking image segmentation\ntest_size = 20\nimages = []\nlabels = []\nbase_path = '..\/input\/lung-segmentation'\n\n\nfor lung_image in os.scandir(f'{base_path}\/Lung_images'):\n    try:\n        mask_name =  f'{lung_image.name}' if lung_image.name[0] == 'M' else f'{lung_image.name.split(\".\")[0]}_mask.png'\n        mask = imread(f'{base_path}\/Lung_masks\/{mask_name}', as_gray=True)\n        mask = transform.resize(mask, (400, 400))\n        labels.append(mask)\n    except:\n        continue\n    img = imread(lung_image.path, as_gray=True)\n    img = transform.resize(img, (400, 400))\n    images.append(img)\n    \n","b67bcf1e":"X_train, X_test, y_train, y_test = train_test_split(np.array(images), np.array(labels), test_size=20, random_state=42)","e5d1d248":"idx = np.random.randint(0, len(X_train))\nimshow(X_train[idx])\nplt.show()\nimshow(np.squeeze(y_train[idx]))\nplt.show()","3554fdf7":"in_layer = Input(shape=(400,400, 1))\n\nc1 = Conv2D(16, (3,3), activation='relu', padding='same')(in_layer)\nc1 = Dropout(0.2)(c1)\nc1 = Conv2D(16, (3,3), activation='relu', padding='same')(c1)\np1 = MaxPooling2D((2,2))(c1)\n\nc2 = Conv2D(32, (3,3), activation='relu', padding='same')(p1)\nc2 = Dropout(0.1)(c2)\nc2 = Conv2D(32, (3,3), activation='relu', padding='same')(c2)\np2 = MaxPooling2D((2,2))(c2)\n\nc3 = Conv2D(64, (3,3), activation='relu', padding='same')(p2)\nc3 = Dropout(0.3)(c3)\nc3 = Conv2D(64, (3,3), activation='relu', padding='same')(c3)\np3 = MaxPooling2D((2,2))(c3)\n\nc4 = Conv2D(128, (3,3), activation='relu', padding='same')(p3)\nc4 = Dropout(0.3)(c4)\nc4 = Conv2D(128, (3,3), activation='relu', padding='same')(c4)\n\nu5 = Conv2DTranspose(64, (2, 2), strides=(2, 2), padding='same')(c4)\nc5 = Conv2D(64, (3, 3), activation='relu', padding='same')(u5)\nc5 = Dropout(0.2)(c5)\nc5 = Conv2D(64, (3, 3), activation='relu', padding='same')(c5)\n\nu6 = Conv2DTranspose(32, (2, 2), strides=(2, 2), padding='same')(c5)\nc6 = Conv2D(32, (3, 3), activation='relu', padding='same')(u6)\nc6 = Dropout(0.1) (c6)\nc6 = Conv2D(32, (3, 3), activation='relu', padding='same')(c6)\n\nu7 = Conv2DTranspose(16, (2, 2), strides=(2, 2), padding='same')(c6)\nc7 = Conv2D(16, (3, 3), activation='relu', padding='same')(u7)\nc7 = Dropout(0.2) (c7)\nc7 = Conv2D(16, (3, 3), activation='relu', padding='same')(c7)\n\nout_layer = Conv2D(1, (1, 1), activation='sigmoid') (c7)\n\nmodel = Model(inputs=[in_layer], outputs=[out_layer])\n\nmodel.compile(optimizer='adam', loss=dice_coef_loss)\nmodel.summary()","ffeca8dc":"\n# Fit model\ncheckpointer = ModelCheckpoint('model-lung-segmentation-1.h5', verbose=1, save_best_only=True)\nhistory = model.fit(X_train, y_train, validation_split=0.1, batch_size=16, epochs=40, \n                    callbacks=[checkpointer], verbose=0)\n\n","f134832d":"# Predict on train, val and test\nfrom tensorflow.keras.models import load_model\n\nmodel = load_model('model-lung-segmentation-1.h5', custom_objects={'dice_coef_loss': dice_coef_loss})\npreds_test = model.predict(X_test, verbose=1)","29cfba79":"idx = np.random.randint(0, len(preds_test))\n\nplt.imshow(X_test[idx])\nplt.show()\nplt.imshow(np.squeeze(y_test[idx]))\nplt.show()\nplt.imshow(np.squeeze(preds_test[idx]))\nplt.show()","b90c5d5e":"fig, axs = plt.subplots(3, len(preds_test), figsize=(52 ,10))\n\nfor ix, pred in enumerate(preds_test):    \n    axs[0, ix].imshow(X_test[ix], cmap='gray')\n    axs[1, ix].imshow(np.squeeze(y_train[ix]), cmap='gray')\n    axs[2, ix].imshow(np.squeeze(pred))\n    \nfig.show()","38b35ee9":"# UNET implementation\n\ninputs = Input(shape=(400,400,1))\n\nc1 = Conv2D(16, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (inputs)\nc1 = Dropout(0.1) (c1)\nc1 = Conv2D(16, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (c1)\np1 = MaxPooling2D((2, 2)) (c1)\n\nc2 = Conv2D(32, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (p1)\nc2 = Dropout(0.1) (c2)\nc2 = Conv2D(32, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (c2)\np2 = MaxPooling2D((2, 2)) (c2)\n\nc3 = Conv2D(64, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (p2)\nc3 = Dropout(0.2) (c3)\nc3 = Conv2D(64, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (c3)\np3 = MaxPooling2D((2, 2)) (c3)\n\nc4 = Conv2D(128, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (p3)\nc4 = Dropout(0.2) (c4)\nc4 = Conv2D(128, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (c4)\np4 = MaxPooling2D(pool_size=(2, 2)) (c4)\n\nc5 = Conv2D(256, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (p4)\nc5 = Dropout(0.3) (c5)\nc5 = Conv2D(256, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (c5)\n\nu6 = Conv2DTranspose(128, (2, 2), strides=(2, 2), padding='same') (c5)\nu6 = concatenate([u6, c4])\nc6 = Conv2D(128, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (u6)\nc6 = Dropout(0.2) (c6)\nc6 = Conv2D(128, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (c6)\n\nu7 = Conv2DTranspose(64, (2, 2), strides=(2, 2), padding='same') (c6)\nu7 = concatenate([u7, c3])\nc7 = Conv2D(64, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (u7)\nc7 = Dropout(0.2) (c7)\nc7 = Conv2D(64, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (c7)\n\nu8 = Conv2DTranspose(32, (2, 2), strides=(2, 2), padding='same') (c7)\nu8 = concatenate([u8, c2])\nc8 = Conv2D(32, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (u8)\nc8 = Dropout(0.1) (c8)\nc8 = Conv2D(32, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (c8)\n\nu9 = Conv2DTranspose(16, (2, 2), strides=(2, 2), padding='same') (c8)\nu9 = concatenate([u9, c1], axis=3)\nc9 = Conv2D(16, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (u9)\nc9 = Dropout(0.1) (c9)\nc9 = Conv2D(16, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (c9)\n\noutputs = Conv2D(1, (1, 1), activation='sigmoid') (c9)\n\nmodel = Model(inputs=[inputs], outputs=[outputs])\nmodel.compile(optimizer='adam', loss=dice_coef_loss)\nmodel.summary()","89ce7d0a":"checkpointer = ModelCheckpoint('unet-lung-segmentation-1.h5', verbose=1, save_best_only=True)\nhistory = model.fit(X_train, y_train, validation_split=0.1, batch_size=32, epochs=40, \n                    callbacks=[checkpointer], verbose=0)","6052cfe7":"model = load_model('unet-lung-segmentation-1.h5', custom_objects={'dice_coef_loss': dice_coef_loss})\nunet_preds = model.predict(X_test, verbose=1)","afe0f35a":"idx = np.random.randint(0, len(unet_preds))\n\nplt.imshow(X_test[idx])\nplt.show()\nplt.imshow(np.squeeze(y_test[idx]))\nplt.show()\nplt.imshow(np.squeeze(unet_preds[idx]))\nplt.show()","b867c3b5":"fig, axs = plt.subplots(3, len(unet_preds), figsize=(52, 10))\n\nfor ix, pred in enumerate(unet_preds):\n    axs[0, ix].imshow(X_test[ix], cmap='gray')\n    axs[1, ix].imshow(np.squeeze(y_train[ix]), cmap='gray')\n    axs[2, ix].imshow(np.squeeze(pred))\n    \nfig.show()","9ca7bdb7":"# Session 04 - Image segmentation - Assignment ","43610dbc":"Autoencoders kunnen ook getraind worden om image segmentation uit te voeren. Segmentatie betekent dat je specifieke objecten in de afbeelding gaat afzonderen van de rest van de afbeelding. In de praktijk komt dit meestal neer op het inkleuren het desbetreffende object in de afbeelding. Image segmentation vindt onderandere zijn toepassing bij self-driving cars, video surveillance en medical imaging. Bij deze opdracht is het de bedoeling om longen te segmenteren uit X-ray afbeeldingen.\n\nFoto's van de longen zijn te vinden in de map Lung_images. \n\nDe labels zijn in dit geval mask afbeeldingen die de exacte locatie van de longen aanduiden. Deze masks bevinden zich in de map Lung_masks.\n\nGa bij deze opdracht als volgt te werk:\n\n- Lees de afbeeldingen in en schaal deze allemaal naar dezelfde grootte. Bijvoorbeeld naar 400x400 pixels. Lagere resolutie mag ook als je merkt dat de systeemvereisten niet volstaan. Voor deze toepassing mag alles naar grijswaarden worden omgezet.\n\n- Maak een training set en test set aan. Stop een 20-tal afbeeldingen in de test set.\n\n- Train een convolutional autoencoder op de training set. De output layer moet dezelfde dimensie hebben als de mask images. De autoencoder wordt namelijk getraind om op basis van een long-scan een mask te genereren die zo goed mogelijk de ground truth mask benadert. Dit vereist echter een aangepaste loss functie om de autoencoder mee te trainen. Cross-entropy gebaseerde loss functies zijn hiervoor niet altijd geschikt. In de praktijk wordt dikwijls de dice coefficient loss gebruikt. Deze kijkt naar de overlap tussen twee data samples. In dit geval hoeveel de door de autoencoder gegeneerde mask overlapt met de ground truth mask. Deze dice loss moet je zelf als custom loss meegeven met het neuraal netwerk.\n\n```Python\ndef dice_coef(y_true, y_pred, smooth=1):\n    intersection = K.sum(K.abs(y_true * y_pred), axis=-1)\n    return (2.*intersection + smooth)\/(K.sum(K.square(y_true),-1)+ K.sum(K.square(y_pred),-1) + smooth)\n\ndef dice_coef_loss(y_true, y_pred):\n    return 1-dice_coef(y_true, y_pred)\n\n```   \n\n- Test de autoencoder op de afbeeldingen uit de test set. Visualiseer de gegeneerde mask en vergelijk met de werkelijke mask. Probeer de segmentatie te verbeteren via hyperparameter tuning. Gebruik hiervoor naast visuele inspectie ook de dice coefficient als metric.\n\n- U-net is een veelgebruikt neuraal netwerk voor medical image segmentation. Een voorbeeld hiervan kan je vinden op https:\/\/www.kaggle.com\/keegil\/keras-u-net-starter-lb-0-277. Implementeer, train en test dit netwerk. Gebruik ook eens de dice coefficient loss als custom loss functie. Bespreek de resultaten.\n   "}}