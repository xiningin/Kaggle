{"cell_type":{"fd1b84ea":"code","d40ba836":"code","5dd7aa0d":"code","269a81ac":"code","8f9edaf1":"code","e750da89":"code","b44c4506":"code","4543a85b":"code","bd711a14":"code","79279a6f":"code","f6f27b1e":"code","ff3194b2":"code","eccabc67":"code","b348630b":"code","0470b2bd":"code","82080c29":"code","019c0d17":"code","60460a4a":"code","4002e40b":"code","cbc6b476":"code","f58eb0b2":"code","7fba5504":"code","174c43ce":"code","14dfe239":"code","c7290fd7":"markdown","b7a9600a":"markdown","5f5dac03":"markdown","abbcbaef":"markdown","c9edfb33":"markdown","b233bc68":"markdown","9b878584":"markdown","596511e6":"markdown","2dbf35ff":"markdown","a3501a40":"markdown","5aa9a55a":"markdown","63c9fbfd":"markdown","6687def0":"markdown","bb87f860":"markdown","1e7a8e48":"markdown"},"source":{"fd1b84ea":"## Importing the Libraries\nimport pandas as pd\nimport numpy as np","d40ba836":"dataset = pd.read_csv('..\/input\/churn-modelling\/Churn_Modelling.csv')","5dd7aa0d":"dataset.head()","269a81ac":"dataset.info()","8f9edaf1":"dataset.describe()","e750da89":"dataset.isnull().sum()","b44c4506":"X = dataset.iloc[:, 3:13]\ny = dataset.iloc[:, 13]","4543a85b":"geography = pd.get_dummies(X['Geography'], drop_first = True)\ngender = pd.get_dummies(X['Gender'], drop_first = True)","bd711a14":"X = pd.concat([X, geography, gender], axis = 1)","79279a6f":"X = X.drop(['Geography', 'Gender'], axis = 1)","f6f27b1e":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 101)","ff3194b2":"from sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nX_train = sc.fit_transform(X_train)\nX_test = sc.transform(X_test)","eccabc67":"from keras.wrappers.scikit_learn import KerasClassifier\nfrom sklearn.model_selection import GridSearchCV\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Activation, Embedding, Flatten, LeakyReLU, BatchNormalization, Dropout\nfrom tensorflow.keras.activations import relu, sigmoid","b348630b":"def create_model(layers, activation):\n    model = Sequential()\n    for i, nodes in enumerate(layers):\n        if i==0:\n            model.add(Dense(nodes,input_dim=X_train.shape[1]))\n            model.add(Activation(activation))\n            model.add(Dropout(0.3))\n        else:\n            model.add(Dense(nodes))\n            model.add(Activation(activation))\n            model.add(Dropout(0.3))\n            \n    model.add(Dense(units = 1, kernel_initializer= 'glorot_uniform', activation = 'sigmoid')) \n    \n    model.compile(optimizer='adam', loss='binary_crossentropy',metrics=['accuracy'])\n    return model\nmodel = KerasClassifier(build_fn=create_model, verbose=0)","0470b2bd":"model =  KerasClassifier(build_fn = create_model, verbose = 0)\n\nlayers = [[20], [40, 20], [45, 30, 15]]\nactivations = ['sigmoid', 'relu']\nparam_grid = dict(layers=layers, activation = activations, batch_size = [128, 256], epochs = [30])\ngrid = GridSearchCV(estimator = model, param_grid = param_grid, cv = 5)","82080c29":"grid_result = grid.fit(X_train, y_train)","019c0d17":"ann_model = Sequential()\nann_model.add(Dense(units = 40, activation = 'relu'))\nann_model.add(Dense(units = 20, activation = 'relu'))\n","60460a4a":"ann_model.add(Dense(units = 1, activation = 'sigmoid'))","4002e40b":"ann_model.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])","cbc6b476":"ann_model.fit(X_train, y_train, epochs = 30, batch_size = 128)","f58eb0b2":"y_pred = ann_model.predict(X_test)\ny_pred = y_pred > 0.5","7fba5504":"from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\ncm = confusion_matrix(y_pred, y_test)\naccuracy = accuracy_score(y_pred, y_test)\nprint(classification_report(y_test, y_pred))","174c43ce":"print(cm)","14dfe239":"accuracy","c7290fd7":"## Let us do some Hyperparameter Optimization\n## Importing necessary Libraries","b7a9600a":"## Now we do not need the Geography and Gender raw data in our dataset, we will drop\/delete them","5f5dac03":"## Let us do training testing splits","abbcbaef":"## Let us load the dataset","c9edfb33":"## Let us concatenate the dataset of dummies with the original dataset","b233bc68":"## Looks like we don't have any missing values, however let us confirm that below","9b878584":"## Function for creating the model that takes layers and activation functions as arguments","596511e6":"## Let us see our confusion matrix and classification report","2dbf35ff":"## We will do scaling of the features","a3501a40":"## Let us do some stats on the dataset","5aa9a55a":"## We can cleary see from the describe output that few columns don't make much interesting for our analysis\n## We have discarded RowNumber, CustomerId and Surname while splitting the data\n## Now, we will replace Geography and Gender into dummies","63c9fbfd":"# Churn Modelling","6687def0":"## Time to split the data into set of Independent and Dependent variables","bb87f860":"## Let us have a look at the dataset","1e7a8e48":"## There is some issue with wrappers class of keras\n## We have run the same code in Google Colab and it worked\n## So, we will just take the best parameters and then create our neural network\n## The best parameters are : activation:relu, batch_size: 128, epochs = 30, layers: 40, 20"}}