{"cell_type":{"d087be30":"code","bfac7c38":"code","3d82ec71":"code","59eca6ec":"code","9b698bff":"code","17041696":"code","c5e4664d":"code","4b5faa9c":"code","c9656b8c":"code","cc489b0c":"code","82db99b2":"code","eb4c6dbd":"code","cbae6be1":"code","3a8bce48":"code","4f02fcab":"code","df0276cf":"code","c068147f":"code","053857a7":"code","a76871e5":"code","254f3615":"code","3a883d81":"code","eda97d11":"code","373a09ea":"code","38d76488":"code","e1464223":"code","10d5ee3f":"code","84996047":"code","a0cadca7":"code","3557b872":"code","ef5f3ac4":"code","8154ade3":"code","396f8bd8":"code","e9cd4a02":"code","3f02c268":"code","62b9f3d8":"code","60607278":"code","d1268194":"code","c4037440":"code","7af08d88":"code","63b531bf":"code","43e2f2c4":"code","1f7397b2":"code","00c81ff3":"code","0f6f6d98":"code","c25c0396":"code","4740c3c3":"code","a6a5b6e8":"code","613796b1":"code","01aabde5":"code","3c81f8fa":"code","c29a4f14":"code","2ab3934a":"code","58533261":"code","beef5178":"code","175124da":"code","870c5be6":"code","d90d3dcc":"code","729e6fbf":"code","0a0d4145":"code","486410f3":"code","322ef2cf":"code","2b1c5c03":"code","00110319":"code","fab7f436":"code","98b17563":"code","7bd9f320":"markdown","74f6ff54":"markdown","96f6cfdd":"markdown","a372b736":"markdown","7099dabd":"markdown","bb78afcd":"markdown","f591af42":"markdown","c678343e":"markdown","f6b26a1e":"markdown","e6aa1483":"markdown","f1d069ad":"markdown","9f2ba98f":"markdown","a3e4a251":"markdown","09d6dda3":"markdown","5ce3330b":"markdown","9cac07ef":"markdown","2ea52865":"markdown","e41cfae2":"markdown","518d0e58":"markdown","e05128e4":"markdown","e11081fe":"markdown","04defca6":"markdown","36cf787b":"markdown","363d4ebb":"markdown","13efaa56":"markdown","ed4b76ff":"markdown","4e03c99d":"markdown","3170a8a5":"markdown","55d2e769":"markdown","6f3cef19":"markdown","c60d79b3":"markdown","95bd6fcc":"markdown","5826478d":"markdown","0b8ed763":"markdown","f75862e6":"markdown","6db6dda9":"markdown","c8e48e18":"markdown","2433430e":"markdown","833b64f7":"markdown","062473a3":"markdown","5722fbbb":"markdown","9e575dae":"markdown","0d147515":"markdown","319bdf97":"markdown","c09f6c52":"markdown"},"source":{"d087be30":"%%javascript\n$.getScript('https:\/\/kmahelona.github.io\/ipython_notebook_goodies\/ipython_notebook_toc.js')","bfac7c38":"import numpy as np \nimport pandas as pd \n\nfrom sklearn.metrics import fbeta_score\nfrom xgboost import XGBClassifier\n\n\nimport seaborn as sns\nimport matplotlib.pylab as plt\nimport matplotlib.mlab as mlab\n%matplotlib inline\nfrom matplotlib.pylab import rcParams\nrcParams['figure.figsize'] = 15, 6","3d82ec71":"employees = pd.read_csv(\"\/kaggle\/input\/softserve-ds-hackathon-2020\/employees.csv\")\nhistory = pd.read_csv(\"\/kaggle\/input\/softserve-ds-hackathon-2020\/history.csv\")\nhistory = history.merge(employees, how='inner', on='EmployeeID')\n\nhistory['Date'] = pd.to_datetime(history['Date'])\nhistory['HiringDate'] = pd.to_datetime(history['HiringDate'])\nhistory['DismissalDate'] = pd.to_datetime(history['DismissalDate'], errors='coerce')\n\nhistory['month'] = history['Date'].dt.month\n\nhistory['set'] = np.where((history['Date'] < '2018-12-01'), \"train\", \"test\")\nhistory['validation'] = history['Date'] >= '2018-09-01'\n\nhistory['days_to_dismissal'] = (history['DismissalDate'] - history['Date'])\/ np.timedelta64(1, 'D')\nhistory['HiringDate'] = (history['Date'] - history['HiringDate'])\/ np.timedelta64(1, 'D')\nhistory.loc[history.ProjectID.isnull(),'ProjectID'] = 'noprj'\n\nhistory.sample(2)","59eca6ec":"history.EmployeeID.nunique(), history.loc[~history.DismissalDate.isnull(),'EmployeeID'].nunique()","9b698bff":"hd = history.loc[history.DismissalDate==history.Date,['EmployeeID','Date']].groupby('Date').count().rename(columns={'EmployeeID': 'emp_dis'}).reset_index()\nh = history.loc[:,['EmployeeID','Date']].groupby('Date').count().rename(columns={'EmployeeID': 'emp'}).reset_index()\nh = h.merge(hd, on = 'Date', how = 'left')\nh['dis_ratio'] = h.emp_dis\/h.emp\nh","17041696":"mad_emp = list(history[history.days_to_dismissal<0]['EmployeeID'].unique())\nhistory = history[(history['days_to_dismissal']!=0) & (~history.EmployeeID.isin(mad_emp))]","c5e4664d":"df_dis_prj = history.loc[~history.DismissalDate.isnull(),['ProjectID', 'EmployeeID','Date']].groupby(['ProjectID','Date']).count().reset_index()\n\ndf_emp_on_pos = history[['PositionID', 'Date','HiringDate' ]].groupby(['PositionID', 'Date']).agg({'median', 'count'})\ndf_emp_on_pos.columns =df_emp_on_pos.columns.map('{0[0]}_{0[1]}'.format) \ndf_emp_on_pos = df_emp_on_pos.rename(columns={'HiringDate_count': 'emp_on', 'HiringDate_median':'ltime_median'}).add_suffix('_pos').reset_index()\n\ndf_emp_on_prj = history[['ProjectID', 'Date','HiringDate' ]].groupby(['ProjectID', 'Date']).agg({'median', 'count'})\ndf_emp_on_prj.columns =df_emp_on_prj.columns.map('{0[0]}_{0[1]}'.format) \ndf_emp_on_prj = df_emp_on_prj.rename(columns={'HiringDate_count': 'emp_on', 'HiringDate_median':'ltime_median'}).add_suffix('_prj').reset_index()\n\ndf_emp_on_dev = history[['DevCenterID', 'Date','HiringDate' ]].groupby(['DevCenterID', 'Date']).agg({'median', 'count'})\ndf_emp_on_dev.columns =df_emp_on_dev.columns.map('{0[0]}_{0[1]}'.format) \ndf_emp_on_dev = df_emp_on_dev.rename(columns={'HiringDate_count': 'emp_on', 'HiringDate_median':'ltime_median'}).add_suffix('_dev').reset_index()\n\ndf_emp_on_sub = history[['SBUID', 'Date','HiringDate' ]].groupby(['SBUID', 'Date']).agg({'median', 'count'})\ndf_emp_on_sub.columns =df_emp_on_sub.columns.map('{0[0]}_{0[1]}'.format) \ndf_emp_on_sub = df_emp_on_sub.rename(columns={'HiringDate_count': 'emp_on', 'HiringDate_median':'ltime_median'}).add_suffix('_sub').reset_index()","4b5faa9c":"df_wage_pr_pos = history[['ProjectID', 'PositionID', 'Date','WageGross' ]].\\\n    groupby(['ProjectID', 'PositionID', 'Date']).median().rename(columns={'WageGross': 'WageGross_prj_pos'}).reset_index()\ndf_wage_pos = history[[ 'PositionID', 'Date','WageGross' ]].\\\n    groupby(['PositionID', 'Date']).median().rename(columns={'WageGross': 'WageGross_pos'}).reset_index()\n\ndf_emp_on_prj_pos = history[['ProjectID', 'PositionID', 'Date','HiringDate' ]].groupby(['ProjectID', 'PositionID', 'Date']).agg({'median', 'count'})\ndf_emp_on_prj_pos.columns =df_emp_on_prj_pos.columns.map('{0[0]}_{0[1]}'.format) \ndf_emp_on_prj_pos = df_emp_on_prj_pos.rename(columns={'HiringDate_count': 'emp_on', 'HiringDate_median':'ltime_median'}).add_suffix('_prj_pos').reset_index()\n\ndf_emp_on_dev_pos = history[['DevCenterID', 'PositionID', 'Date','HiringDate' ]].groupby(['DevCenterID', 'PositionID', 'Date']).agg({'median', 'count'})\ndf_emp_on_dev_pos.columns =df_emp_on_dev_pos.columns.map('{0[0]}_{0[1]}'.format) \ndf_emp_on_dev_pos = df_emp_on_dev_pos.rename(columns={'HiringDate_count': 'emp_on', 'HiringDate_median':'ltime_median'}).add_suffix('_dev_pos').reset_index()\ndf_emp_on_dev_pos.sample(5)","c9656b8c":"history = history.merge(df_emp_on_pos, on = ['PositionID', 'Date'], how = 'left') \\\n    .merge(df_emp_on_prj, on = ['ProjectID', 'Date'], how = 'left') \\\n    .merge(df_emp_on_dev, on = ['DevCenterID', 'Date'], how = 'left') \\\n    .merge(df_emp_on_sub, on = ['SBUID', 'Date'], how = 'left') \\\n    .merge(df_emp_on_dev_pos, on = ['DevCenterID', 'PositionID', 'Date'], how = 'left') \\\n    .merge(df_emp_on_prj_pos, on = ['ProjectID', 'PositionID', 'Date'], how = 'left') \\\n    .merge(df_wage_pr_pos, on = ['ProjectID', 'PositionID', 'Date'], how = 'left') \\\n    .merge(df_wage_pos, on = ['PositionID', 'Date'], how = 'left')\n","cc489b0c":"for i in ['_pos','_prj','_dev','_sub' ,'_prj_pos','_dev_pos']:\n    history['ltime_ratio' + i] = history['HiringDate']\/history['ltime_median'+i]\n\nfor i in ['_pos','_prj_pos']:\n    history['WageGross_ratio' + i] = history['WageGross']\/history['WageGross'+i]","82db99b2":"cols_prj = list(history.loc[:,history.columns.str.endswith('_prj') | history.columns.str.endswith('_prj_pos')].columns) \nfor i in cols_prj :\n    history.loc[(history.ProjectID=='noprj'), i] = history.loc[(history.ProjectID!='noprj'), i].median()","eb4c6dbd":"history['emp_on_pos_change'] = history.groupby('EmployeeID')['emp_on_pos'].shift(1) - history['emp_on_pos']\nhistory['emp_on_prj_change'] = history.groupby('EmployeeID')['emp_on_prj'].shift(1) - history['emp_on_prj']\n\nhistory['Wage_plus'] = (history['WageGross'] - history.groupby('EmployeeID')['WageGross'].shift(1)).fillna(0)\nhistory['ProjectID'] = np.where(history['ProjectID']=='noprj', 1, 0)\nhistory['Date_up_wage'] = pd.to_datetime('2017-01-01')\nhistory.loc[history['Wage_plus'] != 0, 'Date_up_wage'] =  history.loc[history['Wage_plus'] != 0, 'Date']\nhistory['Date_up_wage'] = history.groupby('EmployeeID')['Date_up_wage'].cummax()\nhistory['Date_up_wage'] = (history['Date'] - history['Date_up_wage']).dt.days\n\nhistory['PositionLevel'] = history.groupby('PositionLevel')['WageGross'].transform('median')\nhistory['PositionLevel_Wage'] = history['WageGross'] - history.groupby(['PositionLevel', 'Date'])['WageGross'].transform('median')\nhistory['PositionID'] = history.groupby('PositionID')['WageGross'].transform('median')\nhistory['Position_Wage'] = history['WageGross'] - history.groupby(['PositionID', 'Date'])['WageGross'].transform('median')\nhistory['Position_count'] = history.groupby(['PositionID', 'Date'])['EmployeeID'].transform('count')\nhistory['LanguageLevelID'] = history.groupby('LanguageLevelID')['WageGross'].transform('median')       \nhistory['CompetenceGroupID'] = history.groupby('CompetenceGroupID')['WageGross'].transform('median')\nhistory['FunctionalOfficeID'] = history.groupby('FunctionalOfficeID')['WageGross'].transform('median')\nhistory['PaymentTypeId'] = history.groupby('PaymentTypeId')['WageGross'].transform('median')\nhistory['SBUID_count'] = history.groupby(['SBUID', 'Date'])['EmployeeID'].transform('count')\nhistory['SBUID'] = history.groupby('SBUID')['WageGross'].transform('median')\nhistory['SBUID_Wage'] = history['WageGross'] - history.groupby(['SBUID', 'Date'])['WageGross'].transform('median')\nhistory['DevCenter_count'] = history.groupby(['DevCenterID', 'Date'])['EmployeeID'].transform('count')\nhistory['DevCenterID'] = history.groupby('DevCenterID')['WageGross'].transform('median')\n   \nhistory['CompetenceGroupID_wage'] = history.groupby('CompetenceGroupID')['WageGross'].transform('median')\n\nhistory['SBUID_wage'] = history.groupby('SBUID')['WageGross'].transform('median')\nhistory.loc[history['Date_up_wage'] > 1000000, 'Date_up_wage'] = 0\nhistory['target'] = np.where(history['days_to_dismissal'] <= 92, 1, 0) \nhistory = history.loc[~(history['Date'] <= '2017-08-01'), :]\n\nhistory = history.drop(columns=['days_to_dismissal'])\nhistory.head(2)","cbae6be1":"history.replace([np.inf, -np.inf], np.nan,inplace=True)\nhistory = history.fillna(0)","3a8bce48":"history.groupby('target').size()","4f02fcab":"history.groupby('Date')['target'].mean()","df0276cf":"plt.figure()\nrcParams['figure.figsize'] = 15, 10\nsns.distplot(history.loc[history.target==0,'MonthOnSalary' ], norm_hist=True,  label=\"0\", kde=True, bins = 100) \nsns.distplot(history.loc[history.target==1,'MonthOnSalary' ], norm_hist=True,  label=\"1\", kde=True, bins = 100) \n\nplt.legend();\nrcParams['figure.figsize'] = 9, 6","c068147f":"h_1 = pd.melt(history, value_vars=['ltime_median_sub', 'ltime_median_pos','ltime_median_dev_pos','ltime_median_prj_pos','HiringDate'], id_vars='target')\nsns.violinplot(x='variable', y='value', hue='target', data=h_1, split=True, palette=\"Paired\")\nplt.show()","053857a7":"ones_from_train = history.loc[(history['set'] == \"train\") & (~history['validation']) & (history['target'] == 1), 'EmployeeID']\n\nvalidation = history.loc[(history['set'] == \"train\") & (history['validation']) & (~history['EmployeeID'].isin(ones_from_train)),:]\nvalidation.groupby('target').size()","a76871e5":"validation_list = [(str(_.date()), x.select_dtypes(include=['int', 'float'])) for _, x in validation.groupby('Date')]\nprint(f\"We have {len(validation_list)} validation splits!\")","254f3615":"train = history.loc[(history['set'] == \"train\") & (history['validation'] == False),:]\ntrain = train.select_dtypes(include=['int', 'float'])\nones_count = train.loc[train['target'] == 1, :].shape[0]\nones_count","3a883d81":"from pandas_profiling import ProfileReport\n\nprofile = ProfileReport(train, title='HR features Report')\n\nprofile.to_notebook_iframe()","eda97d11":"train.columns","373a09ea":"nulls = train.loc[train['target'] == 0, :]\nones = train.loc[train['target'] == 1, :]","38d76488":"train_list = [pd.concat([i.head(ones_count), ones]).sample(frac=1.0, random_state=666) for i in np.array_split(nulls.sample(frac=1.0, random_state=666), 16)]\ntrain_splits = [(train.iloc[:, :(train.shape[1]-1)].values, train.iloc[:, (train.shape[1]-1)].values) for train in train_list]","e1464223":"classifiers = [XGBClassifier(\n    objective='binary:logistic', booster= 'gbtree', learning_rate =0.01, n_estimators=200,\n    max_depth=7, min_child_weight=2, subsample=0.9, colsample_bytree=0.5, \n    reg_lambda=0.1, reg_alpha=0.1, nthread=-1,seed=27).fit(X_train, y_train) for X_train, y_train in train_splits]","10d5ee3f":"validation_results = {}\nfor k, i in validation_list:\n    X_test = i.iloc[:, :(train.shape[1]-1)].values\n    y_test = i.iloc[:, (train.shape[1]-1)].values\n    p = np.median(np.array([clf.predict_proba(X_test)[:,1] for clf in classifiers]), 0)\n    validation_results[k] = fbeta_score(y_test, (p > 0.5).astype('int'), beta=1.7)\n    \nprint(validation_results)\nprint(\"Mean f-beta: \", np.array(list(validation_results.values())).mean())","84996047":"pd.Series(p).hist(bins=100)","a0cadca7":"import lime\nimport lime.lime_tabular\nexplainer = lime.lime_tabular.LimeTabularExplainer(train_splits[0][0], feature_names=history.columns, class_names=['Not At Risk', 'At Risk'], verbose=True, mode='classification')","3557b872":"print(f\"Real label: {str(y_test[0])}\")\nexp = explainer.explain_instance(X_test[0], classifiers[0].predict_proba, num_features=10)\nexp.show_in_notebook(show_table=True)","ef5f3ac4":"print(f\"Real label: {str(y_test[100])}\")\nexp = explainer.explain_instance(X_test[100], classifiers[1].predict_proba, num_features=10)\nexp.show_in_notebook(show_table=True)","8154ade3":"print(f\"Real label: {str(y_test[1000])}\")\nexp = explainer.explain_instance(X_test[1000], classifiers[3].predict_proba, num_features=10)\nexp.show_in_notebook(show_table=True)","396f8bd8":"print(f\"Real label: {str(y_test[666])}\")\nexp = explainer.explain_instance(X_test[666], classifiers[0].predict_proba, num_features=30)\nexp.show_in_notebook(show_table=True)","e9cd4a02":"pd.DataFrame({\n    'column': train.columns[:-1],\n    'imp': np.array([clf.feature_importances_ for clf in classifiers]).mean(0)\n}).sort_values('imp', ascending=False)","3f02c268":"import shap\n\nshap.initjs()\n\nexplainer = shap.TreeExplainer(classifiers[0], feature_names=train.columns[:-1])\nshap_values = explainer.shap_values(train_splits[0][0])","62b9f3d8":"shap_feature = np.abs(shap_values).sum(axis=0)\nshap_feature = shap_feature \/ shap_feature.sum()\nfeature_importances = pd.DataFrame({'SHAP importance': shap_feature, 'feature name': train.columns[:-1]})","60607278":"feature_importances.sort_values('SHAP importance', ascending=False)","d1268194":"import altair as alt\n\nbase = alt.Chart(feature_importances)\n\nbar = base.mark_bar().encode(\n    x='SHAP importance:Q',\n    y=alt.Y(\"feature name:O\",\n            sort=alt.EncodingSortField(\n                     field='SHAP importance',\n                     order='descending')))\n\nrule = base.mark_rule(color='red').encode(\n    x='mean(SHAP importance):Q')\n\n(bar + rule).properties(width=630)","c4037440":"shap.force_plot(explainer.expected_value, shap_values[0,:], train_splits[0][0][0,:], feature_names=train.columns[:-1])","7af08d88":"shap.force_plot(explainer.expected_value, shap_values[1,:], train_splits[0][0][1,:], feature_names=train.columns[:-1])","63b531bf":"shap.force_plot(explainer.expected_value, shap_values[2,:], train_splits[0][0][2,:], feature_names=train.columns[:-1])","43e2f2c4":"shap.force_plot(explainer.expected_value, shap_values[3,:], train_splits[0][0][3,:], feature_names=train.columns[:-1])","1f7397b2":"shap.force_plot(explainer.expected_value, shap_values[4,:], train_splits[0][0][4,:], feature_names=train.columns[:-1])","00c81ff3":"shap.force_plot(explainer.expected_value, shap_values, train_splits[0][0], feature_names=train.columns[:-1])","0f6f6d98":"top_inds = np.argsort(-np.sum(np.abs(shap_values), 0))\n\nfor i in range(20):\n    shap.dependence_plot(top_inds[i], shap_values, train_splits[0][0], feature_names=train.columns[:-1])","c25c0396":"shap.summary_plot(shap_values, train_splits[0][0], feature_names=train.columns[:-1])","4740c3c3":"shap.summary_plot(shap_values, train_splits[0][0], plot_type=\"bar\", feature_names=train.columns[:-1])","a6a5b6e8":"test = history.loc[history['set'] == 'test', :].groupby('EmployeeID').tail(1)\ntest","613796b1":"(classifiers[0].predict_proba(X_test)[:,1] > 0.5).mean()","01aabde5":"vr = {i:fbeta_score(y_test, (classifiers[0].predict_proba(X_test)[:, 1] > i).astype('int'), beta=1.7) for i in np.array(list(range(10, 1000, 10)))\/1000}\nvr","3c81f8fa":"pd.DataFrame({\n    'threshold': np.array(list(range(10, 1000, 10)))\/1000,\n    'fbeta': list(vr.values())\n}).plot.line(x='threshold', y='fbeta')","c29a4f14":"tt = test.select_dtypes(include=['int', 'float']).iloc[:, :(train.shape[1]-1)].values\n\npr_submit = np.array([clf.predict_proba(tt)[:,1] for clf in classifiers]).mean(0)\n\npr_submit","2ab3934a":"pr_valid = np.array([clf.predict_proba(X_test)[:,1] for clf in classifiers]).mean(0)","58533261":"pd.Series(pr_valid).plot(kind='density')\npd.Series(pr_submit).plot(kind='density')","beef5178":"pr_valid.mean(), pr_submit.mean()","175124da":"(pr_valid > 0.5).astype('int').mean(), (pr_submit > 0.5).astype('int').mean()","870c5be6":"pr_binary = (pr_submit > 0.51).astype('int')","d90d3dcc":"sub = pd.read_csv(\"\/kaggle\/input\/softserve-ds-hackathon-2020\/submission.csv\")\n\nsub.loc[:,['EmployeeID']].merge(pd.DataFrame({\n    'EmployeeID': test['EmployeeID'].values,\n    'target': pr_binary#.astype('int')\n}), on='EmployeeID', how='left').to_csv('submission.csv', index=False)","729e6fbf":"sub.loc[:,['EmployeeID']].merge(pd.DataFrame({\n    'EmployeeID': test['EmployeeID'].values,\n    'target': pr_submit#.astype('int')\n}), on='EmployeeID', how='left').to_csv('scores.csv', index=False)","0a0d4145":"pd.DataFrame({\n    'column': train.columns[:-1],\n    'imp': np.array([clf.feature_importances_ for clf in classifiers]).mean(0)\n}).sort_values('imp', ascending=False).head(20)","486410f3":"plt.figure()\n#rcParams['figure.figsize'] = 15, 10\nsns.distplot(history.loc[history.target==0,'MonthOnSalary' ], norm_hist=True,  label=\"0\", kde=True, bins = 100) \nsns.distplot(history.loc[history.target==1,'MonthOnSalary' ], norm_hist=True,  label=\"1\", kde=True, bins = 100) \n\nplt.legend();\nrcParams['figure.figsize'] = 9, 6","322ef2cf":"df_wtf = validation[validation.Date=='2018-11-01'].select_dtypes(include=['int', 'float']).copy()\ndf_wtf[(df_wtf.MonthOnSalary>6 )& (df_wtf.MonthOnSalary<=9)].shape","2b1c5c03":"df_wtf.loc[(df_wtf.MonthOnSalary>6 )& (df_wtf.MonthOnSalary<=9), 'MonthOnSalary'] = 5\n\n\nX_test = df_wtf.iloc[:, :(train.shape[1]-1)].values\ny_test = df_wtf.iloc[:, (train.shape[1]-1)].values\np_wtf = np.median(np.array([clf.predict_proba(X_test)[:,1] for clf in classifiers]), 0)\n\nwtf_pred = df_wtf.target.to_frame()\nwtf_pred['pred'] = p\nwtf_pred['pred_wif'] = p_wtf","00110319":"plt.figure()\n#rcParams['figure.figsize'] = 15, 10\nsns.distplot(wtf_pred.loc[(validation.MonthOnSalary>6 )& (validation.MonthOnSalary<=9),'pred' ], norm_hist=True,  label=\"predicted\", kde=True, bins = 100) \nsns.distplot(wtf_pred.loc[(validation.MonthOnSalary>6 )& (validation.MonthOnSalary<=9),'pred_wif' ], norm_hist=True,  label=\"after_wif\", kde=True, bins = 100) \n\nplt.legend();\nrcParams['figure.figsize'] = 9, 6","fab7f436":"(wtf_pred.loc[(validation.MonthOnSalary>6 )& (validation.MonthOnSalary<=9),'pred' ] > 0.5).astype('int').mean()","98b17563":"(wtf_pred.loc[(validation.MonthOnSalary>6 )& (validation.MonthOnSalary<=9),'pred_wif' ] > 0.5).astype('int').mean()","7bd9f320":"Lets check first model with SHAP.","74f6ff54":"## EDA and Feature engineering","96f6cfdd":"Emploees by month:","a372b736":"## What-if analysis","7099dabd":"We have derived several graphs that explain what the model score for each case consists of. They can be interesting for microanalysis, but it is difficult to distinguish general rules and patterns.","bb78afcd":"#### MonthOnSalary visualization","f591af42":"### Validation score distribution","c678343e":"## Conclusions\n\nInstead of conclusions we will write recommendations to the customer. The biggest risks lie in the plane of solidarity and stability. The more stable the situation on the project, the less often employees leave. If there is low staff turnover at the position \/ project \/ location, this contributes to the fact that new employees stay there longer.\n\nIt is also important to raise employees' salaries on time. Being late has bad consequences (we suspect that due to serious competition in the labor market).","f6b26a1e":"Score distribution after.\n\nAs we see new distribution is slightly more skewed to zero.","e6aa1483":"All omissions and infinite values were replaced by 0.","f1d069ad":"We also published from the dataset those employees who worked in the company after the dismissal (for reasons unknown to us):","9f2ba98f":"<p id=\"tocheading\">Table of Contents<\/p>\n<div id=\"toc\"><\/div>","a3e4a251":"#### Visualizing main life time features","09d6dda3":"From the examples below, we can identify the following important factors that reduce the risk of dismissal:\n\n- high salary (WageGross> 1279.00);\n- work on an external project (IsInternalProject <= 0.00);\n- high median duration of work on the location (ltime_median_dev), position (ltime_median_pos) and project (ltime_median_prj)\n- stability of work on the position (MonthOnPosition> 9.00)\n\nNegative factors:\n\n- low salary\n- low median duration of work on location (ltime_median_dev), position (ltime_median_pos) and project (ltime_median_prj)\n- high AMP","5ce3330b":"## Feauture importances with SHAP","9cac07ef":"Mean importances for all 16 models","2ea52865":"## Models training","e41cfae2":"### MonthOnSalary\n\nMonthOnSalary - month without salary increasing as on the last month\nMostly peple don't plan to leave during first 3 month after salary increasing. Interesting, that peple don't do either after 12 month without salary increasing... ","518d0e58":"What if we take emploees that had their salary increased 6-9 mnth ago and \" increase\" salary for them 5 month ago.","e05128e4":"Merge array and correct date formatting:","e11081fe":"### Results obtained from Exploratory Data Analysis (EDA)\n\n1. There over all 5373 unique employees, 1017 of them dismissed.\n2. Some employees prefer to work after dessmisial date. We've desided to stay safe and removed those 5 pepple from dataset.\n3. Monthly data distributon \n    * emploes count quite evenly distributed by month and varies from 4003 to 4721\n    * dismissal info only evaliable from 2017-12-01    \n    * dismissal rate varies from 1% to 2% monthly\n    * mean monthly target varies from 4% to 6% monthly\n4. Some emploees have no Main Project\n\n","04defca6":"Here we count features in which we counted the number of employees on the project \/ position \/ center \/ customer:","36cf787b":"### pandas_profiling","363d4ebb":"We also determined how the duration of work and salary of each employee differed from the average position \/ project \/ customer \/ center \/ specialization:","13efaa56":"## Write prediction","ed4b76ff":"Proportion of target:","4e03c99d":"# Employees Turnover factors","3170a8a5":"Lets look closer to our top features. \nSome of them can not be influenced by Managers. \nThose are features that represent Emploees life time characteristics, but those features describe avarage emploees life time in industry or even companie's culture and envionment.","55d2e769":"Let's chek features' distributions and correlation matrix.\n\nObvously not that many features have high linear correlation with target, but correlation by itself does not always explain the relationship between data.","6f3cef19":"We use here LIME to explain our black-box models predictions.\nAs we're using 16 models for ach train split top features can differ from model to model.","c60d79b3":"In the graphs of the week you can see how the importance of the variable for the model changes depending on its value for the top 20 features.","95bd6fcc":"## Test inference","5826478d":"## F-beta threshold","0b8ed763":"Summary: accordng to our model after improving salary increasing frequency from 7-9 month to 5, company can retain up to 6% employees.","f75862e6":"For id columns, we made a target coding through the median salary. That is, each such feature actually meant what the median salary of each of its unique values.","6db6dda9":"## Validation","c8e48e18":"### Some feature visualisations","2433430e":"The graph below shows the layout of the contribution of each feature to the final result of the forecast. Each case is sorted so that it stands next to those most similar to it. Thus, in this graph you can see the clusters of employees.\n\nTwo clusters are distinguished the most. The blue cluster on the left united the employees with the greatest risk. The cluster on the right (red) brought together employees with the least risk.","833b64f7":"# The Best model\n\nThe main goal of this kernel is to explain our winner xgboost model. It is not simply because all our models are \"black-box models\" that contain many non-linear rules. We aim to reduce these rules using pandas-profiling LIME and SHAP libraries.\n\n- [pandas-profiling](https:\/\/github.com\/pandas-profiling\/pandas-profiling) help us create great EDA for our augmented dataset\n- [LIME](https:\/\/github.com\/marcotcr\/lime) creates an excellent prediction explanation for each case\n- [SHAP](https:\/\/github.com\/slundberg\/shap\/) creates great feature importances explanation\n\nThe code below includes the technical implementation of our approach and an explanation of the impact on the target of each feature of the model. However, it is important to note that the signs should be divided into two groups:\n- those that the company can influence (salary increase, its size, disposal, duration of vacations, the size of the team on projects, equality in remuneration of specialists in the department \/ on the project,\n\u00a0corporate culture, knowledge of English)\n- those that the company can not influence (duration of work in the company, specialty, location)\n\nThe second group we took into account only when analyzing the factors in the release. In the what-if analysis, we considered only the first group of factors.\n\n## Approach description\n\nYou can find the most successful ones in the respective versions of these two kernels (if you can decipher the idea):\n\n- [xgboost](https:\/\/www.kaggle.com\/kirichenko17roman\/fork-of-ox-xgb-validated?scriptVersionId=34199941) at 0.284\n- [lightgbm](https:\/\/www.kaggle.com\/kirichenko17roman\/initial-kernel?scriptVersionId=34171764) at 0.278\n\nWe tried a total of 4 models and different author's features from each team member.\n\nOn the last day, we run all our models in one scheme:\n\n1) **feature engineering**. There are no secrets, tried different shifts, target encoding of categorical values. . Each member of the team had his own set of features; here, we did not unify the approach.\n\n2) **train and validation split**. The training took September 2017 - August 2018 (the first two months did not consider, because there was no target with value \"one\"). For validation, only November 2018. Why only November? In September and October, some employees had \"one\" in July-August. In December-February, they did not take it because there was incomplete information on \"ones.\" For example, a person could be released in March, and for December-February, he should have \"one,\" and according to the available data, we have \"zero.\" This validation was generally correlated with the leaderboard.\n\n3) **balancing classes**. Colleagues wanted to do and sometimes even did oversampling. But finally we've desided that undersampling is better. But that's not all. It turned out that the \"zeros\" are somewhere in the 16-18 times more than \"ones.\" We made a separate dataset of ones and 16 arrays of zeros with the same size (randomly divided). And then to each array of \"zeros\" (unique) glued the same array of \"ones.\"\n\n4) **run of models**. Here are these 16 arrays with an equal number of \"zeros\" and \"ones,\" we cycled on our models. Validation generally showed the following f-beta results:\n\n- **lightgbm** - 0.25-0.255\n- **catboost** - 0.24-0.25\n- **xgboost** - 0.22-0.23\n- **random forest** - 0.23-0.24\n\n5) **aggregation of the result**. Each time we had 16 models, and as a prediction, we took their median score. Like everything we have balanced in classes, it was not necessary to select a threshold. We have converted scores into 0 and 1 through just <0.5.\n\nAt the finish stopped the final submissions on:\n\n- **lightgbm** - 0.251 on public (chosen final due to the best validation score), 0.278 on private\n- **xgboost** - 0.262 on public (chosen because of the greater stability of scores and good result in public, despite worse validation score), 0.262 on private\n\nThere was another stage 6 - blending, but we did not believe it, as in public it gave worse results than the single model, but in private the blending of these four models had a result of 0.28+","062473a3":"## Results explanation with LIME\n\nLocal Interpretable Model-agnostic Explanations (LIME). The output of LIME is a list of explanations, reflecting the contribution of each feature to the prediction of a data sample. This provides local interpretability, and it also allows to determine which feature changes will have most impact on the prediction.","5722fbbb":"Unique Employees count and dismissed count:","9e575dae":"## Train and validation split","0d147515":"## Importances","319bdf97":"Proportion of target by month:","c09f6c52":"LIME perturbs data around an individual prediction to build a model, while SHAP is a united approach that provides global and local consistency and interpretability."}}