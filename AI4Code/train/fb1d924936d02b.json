{"cell_type":{"32077efa":"code","98b16400":"code","6580bdc0":"code","95056d9a":"code","d41b69c5":"code","90839909":"code","1130d4db":"code","0c8af1c1":"code","e9ad68a9":"code","0d3f1c78":"code","2a00bbbe":"code","9f2d64ad":"code","57bbf30b":"markdown","24371cf5":"markdown","8c2c2746":"markdown","4b4b8355":"markdown","c838c887":"markdown","3aef1bea":"markdown","2a8664f2":"markdown","951f2288":"markdown","3e2ddc4b":"markdown","b5940a34":"markdown","676f881d":"markdown","994d5169":"markdown","312c9825":"markdown","60b5b4f2":"markdown"},"source":{"32077efa":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\n## For tokenization and data pre-processing \nfrom sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\nfrom sklearn.base import TransformerMixin\n# for pipelining the process \nfrom sklearn.pipeline import Pipeline\n## fro cleaning the data \nimport string\nimport spacy\nfrom spacy.lang.en.stop_words import STOP_WORDS\nfrom spacy.lang.en import English\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","98b16400":"data = pd.read_csv(\"\/kaggle\/input\/spam-filter\/emails.csv\")","6580bdc0":"display(data.info(),data.head())","95056d9a":"import plotly.express as px\ndf= data['spam'].value_counts()\nfig = px.bar(df,hover_data=[df.index],width=500,height = 300)\nfig.update_traces(marker_color='rgb(158,202,225)', marker_line_color='rgb(8,48,107)',\n                  marker_line_width=1.5, opacity=0.6)\nfig.update_layout(title='Spam 0 or 1')\nfig.show()","d41b69c5":"\n# Create our list of punctuation marks\npunctuations = string.punctuation\n\n# Create our list of stopwords\nnlp = spacy.load('en')\nstop_words = spacy.lang.en.stop_words.STOP_WORDS\n\n# Load English tokenizer, tagger, parser, NER and word vectors\nparser = English()\n\n# Creating our tokenizer function\ndef spacy_tokenizer(sentence):\n    # Creating our token object, which is used to create documents with linguistic annotations.\n    mytokens = parser(sentence)\n\n    # Lemmatizing each token and converting each token into lowercase\n    mytokens = [ word.lemma_.lower().strip() if word.lemma_ != \"-PRON-\" else word.lower_ for word in mytokens ]\n\n    # Removing stop words\n    mytokens = [ word for word in mytokens if word not in stop_words and word not in punctuations ]\n\n    # return preprocessed list of tokens\n    return mytokens","90839909":"# Custom transformer using spaCy\nclass predictors(TransformerMixin):\n    def transform(self, X, **transform_params):\n        # Cleaning Text\n        return [clean_text(text) for text in X]\n\n    def fit(self, X, y=None, **fit_params):\n        return self\n\n    def get_params(self, deep=True):\n        return {}\n\n# Basic function to clean the text\ndef clean_text(text):\n    # Removing spaces and converting text into lowercase\n    return text.strip().lower()","1130d4db":"bow_vector = CountVectorizer(tokenizer = spacy_tokenizer, ngram_range=(1,1))","0c8af1c1":"tfidf_vector = TfidfVectorizer(tokenizer = spacy_tokenizer)","e9ad68a9":"from sklearn.model_selection import train_test_split\n\nX = data['text'] # the features we want to analyze\nylabels = data['spam'] # the labels, or answers, we want to test against\n\nX_train, X_test, y_train, y_test = train_test_split(X, ylabels, test_size=0.3)","0d3f1c78":"# Logistic Regression Classifier\nfrom sklearn.linear_model import LogisticRegression\nclassifier = LogisticRegression()\n\n# Create pipeline using Bag of Words\npipe = Pipeline([(\"cleaner\", predictors()),\n                 ('vectorizer', bow_vector),\n                 ('classifier', classifier)])\n\n# model generation\npipe.fit(X_train,y_train)","2a00bbbe":"from sklearn import metrics\n# Predicting with a test dataset\npredicted = pipe.predict(X_test)\n\n# Model Accuracy\nprint(\"Logistic Regression Accuracy:\",metrics.accuracy_score(y_test, predicted))\nprint(\"Logistic Regression Precision:\",metrics.precision_score(y_test, predicted))\nprint(\"Logistic Regression Recall:\",metrics.recall_score(y_test, predicted))","9f2d64ad":"# Create pipeline using Bag of Words\npipe2 = Pipeline([(\"cleaner\", predictors()),\n                 ('vectorizer', tfidf_vector),\n                 ('classifier', classifier)])\n\n# model generation\npipe2.fit(X_train,y_train)\n# Predicting with a test dataset\npredicted2 = pipe.predict(X_test)\n\n# Model Accuracy\nprint(\"Logistic Regression Accuracy:\",metrics.accuracy_score(y_test, predicted2))\nprint(\"Logistic Regression Precision:\",metrics.precision_score(y_test, predicted2))\nprint(\"Logistic Regression Recall:\",metrics.recall_score(y_test, predicted2))","57bbf30b":"## Creating a Pipeline and Generating the Model using Count vectorization\n### Now that we\u2019re all set up, it\u2019s time to actually build our model! We\u2019ll start by importing the LogisticRegression module and creating a LogisticRegression classifier object.\n\n### Then, we\u2019ll create a pipeline with three components: a cleaner, a vectorizer, and a classifier. The cleaner uses our predictors class object to clean and preprocess the text. The vectorizer uses countvector objects to create the bag of words matrix for our text. The classifier is an object that performs the logistic regression to classify whether the mail is spam or not.","24371cf5":"### Here we have various text material where we are trying to find those words for which we can classify the mail as a spam mail or not.\n### And we do this by First, we extract the features we want from our source text (and any tags or metadata it came with), and then we feed our cleaned data into a machine learning algorithm that do the classification for us.<\/font>","8c2c2746":"# <font color = 'blue'>Spam or Not \ud83e\udd37\u200d\u2642\ufe0f using SpaCy (98% Recall score)","4b4b8355":"## 1. Tokenization ","c838c887":"# Conclusion :- \n### The results are good and I am happy with the Recall score that I got 98%. This means that the spam mails are getting identified correctly even after there being a bias towards not smap mails in the dataset. \n### Thank You for reviewing the notebook. Comment and UpVote if it helped you in any way. \ud83d\ude4f\ud83d\ude03","3aef1bea":"![text-classification-python-spacy.png](attachment:text-classification-python-spacy.png)","2a8664f2":"### we\u2019ll create a spacy_tokenizer() function that accepts a sentence as input and processes the sentence into tokens, performing lemmatization, lowercasing, and removing stop words.","951f2288":"### This class overrides the transform, fit and get_parrams methods. We\u2019ll also create a clean_text() function that removes spaces and converts text into lowercase.","3e2ddc4b":"### No. of spam emails are less as compaired to them being spam. However our main objective is to find whether Recall is good aor not as there will be a heavy bias towards getting not spam emails but the real challenge is to find the spam emails. \n### Lets dig deeper into this and see the flow diagram of how we are going to implement this. ","b5940a34":"## Creating a Pipeline and Generating the Model using tfidf vectorization","676f881d":"## Importing Liberaries","994d5169":"## Aim : -- \n\n### In this tutorial, we\u2019ll take a look at how we can transform our unstructured text data into something more useful for analysis and natural language processing, using the helpful Python package spaCy , \n## But why SpaCy??\n###  spaCy is an open-source natural language processing library for Python. It is designed particularly for production use, and it can help us to build applications that process massive volumes of text efficiently. \n   ## So Let's start\n","312c9825":"## Data Cleaning\/Data Per-processing","60b5b4f2":"## Before starting do comment and UpVote if this notebook was helpful and interesting to read.. Thank You\ud83d\ude4f"}}