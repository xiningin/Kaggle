{"cell_type":{"8a337f0e":"code","edca153e":"code","0d3e0a29":"code","198132ef":"code","fa476b91":"code","31ba4a26":"code","1bc1d57e":"code","7513c7ec":"code","ed8d3bd8":"code","15995b27":"code","171a4f5d":"code","89ecab9e":"code","3d24c681":"code","cf69e09c":"code","b656a28c":"code","55e1a96c":"code","e3d47bac":"code","da4457d0":"code","e04589ae":"markdown","cbd6affc":"markdown","9825db41":"markdown"},"source":{"8a337f0e":"import os\nimport glob\nfrom joblib import Parallel, delayed\nimport pandas as pd\nimport numpy as np\nimport scipy as sc\nfrom sklearn.model_selection import KFold\nimport lightgbm as lgb\nimport warnings\nwarnings.filterwarnings('ignore')\npd.set_option('max_columns', 300)\nfrom tqdm import tqdm\nimport time\nimport gc","edca153e":"from tensorflow.keras.models import Model, Sequential\nfrom tensorflow.keras.layers import Conv1D, MaxPool1D, LSTM\nfrom tensorflow.keras.optimizers import Adam\n\nfrom tensorflow.keras.layers import Input, Dense, Activation, Dropout, Flatten\n\nimport tensorflow.keras.callbacks as callbacks\nimport tensorflow as tf\nfrom keras import backend as K","0d3e0a29":"# data directory\ndata_dir = '..\/input\/optiver-realized-volatility-prediction\/'","198132ef":"# Function to read our base train and test set\ndef read_train_test():\n    train = pd.read_csv('..\/input\/optiver-realized-volatility-prediction\/train.csv')\n    test = pd.read_csv('..\/input\/optiver-realized-volatility-prediction\/test.csv')\n    # Create a key to merge with book and trade data\n    train['row_id'] = train['stock_id'].astype(str) + '-' + train['time_id'].astype(str)\n    test['row_id'] = test['stock_id'].astype(str) + '-' + test['time_id'].astype(str)\n    print(f'Our training set has {train.shape[0]} rows')\n    return train, test\n\n# Read train and test\ntrain, test = read_train_test()","fa476b91":"pricelist = ['bid_price1','ask_price1','bid_price2','ask_price2','wap1']\n\ntrain_stock_ids = train['stock_id'].unique()\ntest_stock_ids = test['stock_id'].unique()","31ba4a26":"# \u4eca\u56de\u4f7f\u7528\u3057\u306a\u3044\ndef book_preprocessor(file_path, col, stock_id):\n    df = pd.read_parquet(file_path)\n    if col == \"wap1\":\n        df['wap1'] = calc_wap1(df)\n    seconds = pd.DataFrame(index=range(0,600)).rename_axis('seconds_in_bucket')\n    tmp = pd.pivot_table(df, index=[\"time_id\"], columns=[\"seconds_in_bucket\"], values=[col]).T#.reset_index()\n    tmp = tmp.set_axis(tmp.columns.tolist(),axis=\"columns\").reset_index().drop([\"level_0\"],axis=1).set_index('seconds_in_bucket')\n    tmp = pd.concat([seconds, tmp], axis=1).fillna(method='ffill').fillna(method='bfill').reset_index(drop=True).T.reset_index()\n    tmp.columns = [\"row_id\"] + [f'{col}_t{i}' for i in range(600)]\n    tmp[\"row_id\"] = tmp[\"row_id\"].apply(lambda x: f'{stock_id}-{x}')\n    return tmp\n\ndef trade_1dcnn_preprocessor(file_path, stock_id,bins):\n    df = pd.read_parquet(file_path)\n    df['sec_cut'] = pd.cut(df['seconds_in_bucket'],bins = np.arange(0,600.1,bins).tolist(),right=False,labels=np.arange(0,600,bins).tolist())\n    df = df[['time_id','sec_cut','price']].groupby(['time_id','sec_cut']).last().reset_index()\n    df = pd.pivot_table(df, index=[\"time_id\"], columns=[\"sec_cut\"], values=['price'])\n    df = df.set_axis(np.arange(0,600,bins).astype(str).tolist(),axis=\"columns\")\n    df.columns = [f'{col}sec' for col in df.columns]\n    df = df.reset_index()\n    df['stock_id'] = stock_id\n    return df","1bc1d57e":"def preprocessor(list_stock_ids, is_train=True, bins=60):\n    \n    # Parrallel for loop\n    def for_joblib(stock_id, bins=60):\n        # Train\n        if is_train:\n            file_path_book = data_dir + \"book_train.parquet\/stock_id=\" + str(stock_id)\n            file_path_trade = data_dir + \"trade_train.parquet\/stock_id=\" + str(stock_id)\n        # Test\n        else:\n            file_path_book = data_dir + \"book_test.parquet\/stock_id=\" + str(stock_id)\n            file_path_trade = data_dir + \"trade_test.parquet\/stock_id=\" + str(stock_id)\n\n        trade = trade_1dcnn_preprocessor(file_path_trade,stock_id,bins)\n        return trade\n\n    df = Parallel(n_jobs = -1, verbose = 1)(delayed(for_joblib)(stock_id,bins) for stock_id in list_stock_ids)\n    df = pd.concat(df, ignore_index = True)\n    \n    # fillna\n    keys = ['stock_id','time_id']\n    tmp = df[keys]\n    values = df.drop(keys, axis=1).T.fillna(method='ffill').fillna(1).T\n    df = pd.concat([tmp,values],axis=1)\n    \n    return df","7513c7ec":"#\u79d2\u6570\u3092\u6307\u5b9a\nbins=30\ntrain_ = preprocessor(train_stock_ids, is_train = True, bins=30)","ed8d3bd8":"train_.head()","15995b27":"train = train.merge(train_, on=['stock_id','time_id'],how='left')\n#\u6b20\u640d\u304c\u3042\u308b\u884c\u3092\u78ba\u8a8d\ndisplay(train.loc[train.isna().any(axis=1)].shape)\ndisplay(train.loc[train.isna().any(axis=1)].head())\n##\u6b20\u640d\u304c\u3042\u308b\u884c\u3092\u524a\u9664?\u30001\u3067\u57cb\u3081\u308b\u304b\uff1f\ntrain = train.loc[~train.isna().any(axis=1)]","171a4f5d":"tlen = len(np.arange(0,600,bins))\ny = np.array(train['target']).reshape(-1,1)\nX = np.array(train.drop(['stock_id','time_id','target','row_id'],axis=1)).reshape(-1,tlen,1)","89ecab9e":"X.shape","3d24c681":"def root_mean_squared_per_error(y_true, y_pred):\n         return K.sqrt(K.mean(K.square( (y_true - y_pred)\/ y_true )))","cf69e09c":"def base_model(tlen):\n    \n    inputs = Input(shape=(tlen, 1))\n\n    x = Conv1D(16, 2, padding='same', activation='relu')(inputs)\n    x = MaxPool1D(pool_size=2, padding='same')(x)\n\n    x = Conv1D(10, 2, padding='same', activation='relu')(x)\n    x = MaxPool1D(pool_size=2, padding='same')(x)\n\n    x = Flatten()(x)\n    x = Dense(100, activation='relu')(x)\n    x = Dense(1, activation='tanh')(x)\n\n    model = Model(inputs, outputs=x)\n\n    return model","b656a28c":"model = base_model(tlen)\noptimizer = Adam(lr=1e-3)","55e1a96c":"folds_valid_rmspe = []\n\nn_folds = 5\nkf = KFold(n_splits=n_folds, shuffle=True, random_state=2020)\ncounter = 1\n\n\nes = tf.keras.callbacks.EarlyStopping(\n    monitor='val_loss', min_delta=1e-05, patience=6, verbose=1,\n    mode='min')\n\nplateau = tf.keras.callbacks.ReduceLROnPlateau(\n    monitor='val_loss', factor=0.1, patience=4, verbose=1, min_lr=5e-7,\n    mode='min')\n\nfor dev_index, val_index in kf.split(range(len(X))):\n    \n    #\u8a66\u9a13\u7684\u306b\u898b\u308b\u305f\u3081\u306b1fold\u3067\u5207\u308b\u7528\u306e\u5206\u5c90\n    if counter>1:\n        pass #break\n    \n    model = base_model(tlen)\n    \n    model.compile(\n        Adam(learning_rate=0.0004), #0.0004\n        loss='mean_squared_error',\n        metrics=[root_mean_squared_per_error],\n    )\n    \n    model.fit(X[dev_index], \n              y[dev_index], \n              #sample_weight = 1\/np.square(y[dev_index]),\n              batch_size=256,\n              epochs=2,\n              validation_data=(X[val_index], y[val_index]), #1\/np.square(y[val_index])\n              callbacks=[es, plateau],\n              shuffle=True,\n             verbose = 1)\n    \n    preds = model.predict(X[val_index]).reshape(1,-1)[0]\n    \n    def rmspe(y_true, y_pred):\n        y_pred = y_pred.reshape(-1,1)\n        return  (np.sqrt(np.mean(np.square((y_true - y_pred) \/ y_true))))\n    \n    score = round(rmspe(y_true = y[val_index], y_pred = preds),5)\n    print('Fold {} 1DCNN: {}'.format(counter, score))\n    folds_valid_rmspe.append(score)\n    print(len(preds))\n    \n    counter += 1\ngc.collect()","e3d47bac":"#\u5404fold\u306eRMSPE\u306e\u30b9\u30b3\u30a2\nfolds_valid_rmspe","da4457d0":"model.summary()","e04589ae":"## target\u3068\u7279\u5fb4\u91cf\u3092\u30de\u30fc\u30b8","cbd6affc":"## target\u30c7\u30fc\u30bf\u52a0\u5de5","9825db41":"## 1D CNN"}}