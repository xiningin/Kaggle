{"cell_type":{"b1d889ea":"code","5cbe4995":"code","bf92e50c":"code","623a8914":"code","2deb7d1d":"code","7bc557c5":"code","f828d88d":"code","7aa5d34d":"code","3e5096c9":"code","7bf779de":"code","3e19397f":"code","218940c5":"code","59733e0d":"code","118f2749":"code","fe43d817":"code","926666c8":"code","a68d5185":"code","6fdff5a5":"code","6429cf5c":"code","ee03f5c0":"code","d36ea24a":"code","f201bd09":"code","da72a78d":"code","3fc685e2":"code","8a88ce96":"code","9a764a7f":"code","2c106ef9":"code","6ba4c8d2":"code","d8ace901":"code","9a3f65b2":"code","5cfaf86f":"code","07e70466":"code","a1673389":"code","95315021":"code","c5be9bc8":"code","9a6cb964":"code","ecb8162a":"code","71d6cf4b":"code","c50f3513":"code","d0805d8b":"code","8f904687":"code","eecb8aa6":"code","23028b46":"code","c491448f":"code","5e84703a":"code","a2dd7402":"markdown","4e1c96a0":"markdown","2d8f5faf":"markdown","e2f63d30":"markdown","c8d8b48e":"markdown"},"source":{"b1d889ea":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","5cbe4995":"df = pd.read_csv('\/kaggle\/input\/ramen-ratings\/ramen-ratings.csv')","bf92e50c":"df.head()","623a8914":"df = df.drop(columns=['Top Ten'])","2deb7d1d":"df.isnull().sum()","7bc557c5":"df = df.dropna()","f828d88d":"df.isnull().sum()","7aa5d34d":"df.describe()","3e5096c9":"x = df.drop(columns=['Style'])\nx","7bf779de":"y = df['Style']\ny","3e19397f":"import matplotlib.pyplot as plt\n# create figure and axis\nfig, ax = plt.subplots()\n# plot histogram\nax.hist(df['Style'])\n# set title and labels\nax.set_title('Style of Presenting')\nax.set_xlabel('Types')\nax.set_ylabel('Frequency')","218940c5":"from sklearn import preprocessing \nlabel_encoder = preprocessing.LabelEncoder()  \nx= x.apply(label_encoder.fit_transform)\nx","59733e0d":"y= label_encoder.fit_transform(y)\ny","118f2749":"import seaborn as sns\nsns.jointplot(x=x['Brand'], y=x['Stars'], kind=\"kde\")","fe43d817":"plt.figure(figsize=(12,6))\nsns.boxplot(x=\"Country\", y=\"Brand\", data=x)","926666c8":"features=['Style', 'Country'] # Subplot for count plot\nfig=plt.subplots(figsize=(25,20))\nfor i, j in enumerate(features):\n    plt.subplot(4, 2, i+1)\n    plt.subplots_adjust(hspace = 1.0)\n    sns.countplot(x=j,data = df)\n    plt.xticks(rotation=90)\n    plt.title(\"Ramen\")\n    \nplt.show()","a68d5185":"import seaborn as sns\nsns.kdeplot(data=x['Country'], shade=True)","6fdff5a5":"from sklearn.naive_bayes import GaussianNB\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import train_test_split","6429cf5c":"x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=4)","ee03f5c0":"# data normalization with sklearn\nfrom sklearn.preprocessing import MinMaxScaler\n\n# fit scaler on training data\nnorm = MinMaxScaler().fit(x_train)\n\n# transform training data\nX_train_norm = norm.transform(x_train)\n\n\n# transform testing dataabs\nX_test_norm = norm.transform(x_test)\n","d36ea24a":"# fit scaler on training data\nnorm = MinMaxScaler().fit(x_train)\n\n# transform training data\nX_train_norm = norm.transform(x_train)\nprint(\"Scaled Train Data: \\n\\n\")\nprint(X_train_norm)","f201bd09":"# transform testing dataabs\nX_test_norm = norm.transform(x_test)\nprint(\"\\n\\nScaled Test Data: \\n\\n\")\nprint(X_test_norm)","da72a78d":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score","3fc685e2":"from sklearn.tree import DecisionTreeClassifier\n# Create Decision Tree classifer object\nclf = DecisionTreeClassifier()\n# Train Decision Tree Classifer\nclf.fit(X_train_norm,y_train)","8a88ce96":"#Predict the response for test dataset\ny_pred = clf.predict(X_test_norm)\n\n\nprint(\"Accuracy:\",accuracy_score(y_test, y_pred))","9a764a7f":"from sklearn.neighbors import KNeighborsClassifier  \nclassifier= KNeighborsClassifier(n_neighbors=5, metric='minkowski', p=2 )  \nclassifier.fit(X_train_norm, y_train)  ","2c106ef9":"y_pred= classifier.predict(X_test_norm)  \n#Creating the Confusion matrix  \nfrom sklearn.metrics import confusion_matrix  \nconfusion_matrix(y_test, y_pred) ","6ba4c8d2":"print(accuracy_score(y_test, y_pred))","d8ace901":"from sklearn.cluster import KMeans\nkmeans = KMeans(n_clusters=4, n_init = 10, random_state=251)\nkmeans.fit(x)","9a3f65b2":"centroids = kmeans.cluster_centers_\ncentroid_df = pd.DataFrame(centroids, columns = list(x) )","5cfaf86f":"centroid_df = pd.DataFrame(centroids, columns = list(x) )\ndf_labels = pd.DataFrame(kmeans.labels_ , columns = list(['labels']))","07e70466":"snail_df_labeled = x.join(df_labels)","a1673389":"df_analysis = (snail_df_labeled.groupby(['labels'] , axis=0)).head(4177) \ndf_analysis.head()","95315021":"df_analysis.isnull().sum()","c5be9bc8":"df_analysis = df_analysis.dropna()","9a6cb964":"df_analysis.isnull().sum()","ecb8162a":"from sklearn.model_selection import train_test_split  \n\nX= df_analysis.drop('labels',axis =1)\ny= df_analysis['labels']\n\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=1)","71d6cf4b":"from sklearn.linear_model import LogisticRegression\nclassifier = LogisticRegression(random_state = 0)\nclassifier.fit(X_train,y_train)","c50f3513":"# predict Model\ny_pred = classifier.predict(X_test)\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import classification_report\ncm = confusion_matrix(y_true=y_test,y_pred=y_pred)\nprint('Confusion Matrix \\n',cm)\naccuracy_score(y_test,y_pred)","d0805d8b":"# DTfrom sklearn.tree import DecisionTreeClassifier\nclassifier = DecisionTreeClassifier(criterion = 'gini',random_state = 0)\nclassifier.fit(X_train,y_train)\ny_pred = classifier.predict(X_test)\ncm = confusion_matrix(y_true=y_test,y_pred=y_pred)\nprint('Confusion Matrix \\n',cm)\nprint(accuracy_score(y_test,y_pred))","8f904687":"rclf = RandomForestClassifier(n_estimators= 100)\nrclf.fit(X_train,y_train)\ny_pred = rclf.predict(X_test)\ncm = confusion_matrix(y_true=y_test,y_pred=y_pred)\nprint('Confusion Matrix \\n',cm)\nprint(accuracy_score(y_test,y_pred))","eecb8aa6":"classifier = KNeighborsClassifier(n_neighbors= 5)\nclassifier.fit(X_train,y_train)","23028b46":"y_pred = classifier.predict(X_test)\ncm = confusion_matrix(y_true=y_test,y_pred=y_pred)\nprint('Confusion Matrix \\n',cm)\nprint(accuracy_score(y_test,y_pred))","c491448f":"model = GaussianNB()\nmodel.fit(X_train,y_train)\npredicted = model.predict(X_test)\nprint('Predicted Value',predicted)","5e84703a":"cm = confusion_matrix(y_true=y_test,y_pred=predicted)\nprint('Confusion Matrix \\n',cm)\nprint(accuracy_score(y_test,predicted))","a2dd7402":"# Joint Plot","4e1c96a0":"# Count Plot","2d8f5faf":"Label encoding to convert string to numeric type","e2f63d30":"# Bar Plot","c8d8b48e":"# Box Plot"}}