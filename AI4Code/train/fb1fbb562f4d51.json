{"cell_type":{"6cbd2c7d":"code","c99afee1":"code","b76e67ad":"code","772decc6":"code","6a5167ba":"code","7662fd9b":"code","b115bf95":"code","4b7ea38d":"code","ad323d78":"code","4142ccf7":"code","87431237":"code","57f12534":"code","382cce45":"code","db483ca5":"code","780f2595":"code","9c15016d":"code","b2b2f998":"code","9d93872b":"code","ed012e5e":"code","f1ac0316":"code","c5440116":"code","43a8670e":"code","515320a8":"code","3e3db900":"code","64475495":"code","140b1973":"code","e4daecd4":"code","60aa655b":"code","4dbed88b":"code","64e3cc89":"code","6075dc66":"code","d42bc49e":"code","919d605a":"code","61e47c1c":"code","bec0cb5b":"code","541b2efd":"code","88d72ae5":"code","836024cd":"code","93549a22":"code","53b1816b":"code","88d7d94e":"code","1ee0779c":"code","5029292f":"code","85272f55":"code","ba5696a3":"code","3a62c95f":"code","ca591d82":"code","49fa7cb8":"code","cb528aa4":"code","9b2c0a82":"code","fe4b58dd":"code","df861a3f":"code","cd720ff2":"code","9d493b7c":"code","8ed674c4":"code","0f8481ca":"code","787c9da4":"code","232dd876":"code","f88bf334":"code","33693807":"code","d53280d3":"code","be6b7321":"code","3baab22f":"code","087d6573":"code","20b1405f":"code","346f0d6b":"code","152b3663":"code","e852c249":"markdown","a5f6eaa7":"markdown","ee99b7ee":"markdown","c5ff9f9f":"markdown","c267e0c6":"markdown","4c6f0259":"markdown","727b56b3":"markdown","4d34ccb6":"markdown","f1d52ad4":"markdown","de8a265b":"markdown","c5ef966d":"markdown","87b09c2f":"markdown","3d91922d":"markdown","e6c4fdca":"markdown","c87a0270":"markdown","e6afc296":"markdown","80b722e3":"markdown","442290b2":"markdown","a9ec3833":"markdown","08297d86":"markdown","e7abf988":"markdown","0ce1b31e":"markdown","b0a14777":"markdown","fbd0abf3":"markdown","98f87da0":"markdown","8fa12e5f":"markdown","985ae35c":"markdown","40e04a33":"markdown","c7384136":"markdown","844bcd22":"markdown","87c1e682":"markdown","4421682f":"markdown"},"source":{"6cbd2c7d":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","c99afee1":"# Read Dataset\n\nfullCorpus = pd.read_csv('..\/input\/SMSSpamCollection', sep='\\t', header=None)\nfullCorpus.columns = ['label', 'body_text']\n\nfullCorpus.head()","b76e67ad":"# Dataset shape (rows, columns)\nlen(fullCorpus), len(fullCorpus.columns)","772decc6":"# number of \"spam\" items vs \"ham\" items\nlen(fullCorpus[fullCorpus['label']=='spam']), len(fullCorpus[fullCorpus['label']=='ham'])","6a5167ba":"# check for null values\nfullCorpus['label'].isnull().sum(), fullCorpus['body_text'].isnull().sum()","7662fd9b":"# Read in the raw text\nrawData = open('..\/input\/SMSSpamCollection').read()\n\n# Print the raw data\nrawData[0:500]","b115bf95":"parsedData = rawData.replace('\\t', '\\n').split('\\n')\nparsedData[0:6]","4b7ea38d":"labelList = parsedData[0::2]\ntextList = parsedData[1::2]","ad323d78":"labelList[0:5]","4142ccf7":"textList[0:5]","87431237":"print(labelList[0:5])\nprint(textList[0:5])","57f12534":"print(len(labelList))\nprint(len(textList))","382cce45":"print(labelList[-5:])\nprint(textList[0:5])","db483ca5":"# correct version for reading file with correct data shape\nfullCorpus = pd.DataFrame({\n    'label': labelList[:-1],\n    'body_list': textList\n})\n\nfullCorpus.head()","780f2595":"pd.set_option('display.max_colwidth', 100)\n\ndata = pd.read_csv('..\/input\/SMSSpamCollection', sep='\\t', header=None)\ndata.columns = ['label', 'body_text']\n\ndata.head()","9c15016d":"import string\n\ndef remove_punct(text):\n    text_nopunct = \"\".join([char for char in text if char not in string.punctuation])\n    return text_nopunct","b2b2f998":"# make new \"clean data\" column\ndata['body_text_clean'] = data['body_text'].apply(lambda x: remove_punct(x))\ndata.head()","9d93872b":"import re\n\ndef tokenize(text):\n    tokens = re.split('\\W+', text)\n    return tokens","ed012e5e":"# make new \"tokenized data\" column\ndata['body_text_tokenized'] = data['body_text_clean'].apply(lambda x: tokenize(x.lower()))\ndata.head()","f1ac0316":"import nltk\n\nstopword = nltk.corpus.stopwords.words('english')\n\ndef remove_stopwords(tokenized_list):\n    text = [word for word in tokenized_list if word not in stopword]\n    return text","c5440116":"# make new \"nostop data\" column\ndata['body_text_nostop'] = data['body_text_tokenized'].apply(lambda x: remove_stopwords(x))\ndata.head()","43a8670e":"def wrangle_text(text):\n    text = \"\".join([word for word in text if word not in string.punctuation])\n    tokens = re.split('\\W+', text)\n    text = [word for word in tokens if word not in stopword]\n    return text","515320a8":"ps = nltk.PorterStemmer()\n\ndef stemming(tokenized_text):\n    text = [ps.stem(word) for word in tokenized_text]\n    return text","3e3db900":"# make new \"stemmed words data\" column\ndata['body_text_stemed'] = data['body_text_nostop'].apply(lambda x: stemming(x))\ndata.head()","64475495":"wn = nltk.WordNetLemmatizer()\n\ndef lemmatizing(tokenized_text):\n    text = [wn.lemmatize(word) for word in tokenized_text]\n    return text","140b1973":"# make new \"lemmatized words data\" column\ndata['body_text_lemmatized'] = data['body_text_nostop'].apply(lambda x: lemmatizing(x))\ndata.head()","e4daecd4":"def clean_text(text):\n    text = \"\".join([word.lower() for word in text if word not in string.punctuation])\n    tokens = re.split('\\W+', text)\n    text = \" \".join([ps.stem(word) for word in tokens if word not in stopword])\n    return text\n\ndata['cleaned_text'] = data['body_text'].apply(lambda x: clean_text(x))\ndata.head()","60aa655b":"from sklearn.feature_extraction.text import CountVectorizer","4dbed88b":"count_vect = CountVectorizer(analyzer=wrangle_text)\nX_counts = count_vect.fit_transform(data['body_text'])\nprint(X_counts.shape)\nprint(count_vect.get_feature_names()[:20])","64e3cc89":"X_counts_df = pd.DataFrame(X_counts.toarray())\nX_counts_df.head()","6075dc66":"X_counts_df.columns = count_vect.get_feature_names()\nX_counts_df.head()","d42bc49e":"from sklearn.feature_extraction.text import CountVectorizer\n\nngram_vect = CountVectorizer(ngram_range=(2,2))\nX_counts = ngram_vect.fit_transform(data['cleaned_text'])\nprint(X_counts.shape)\nprint(ngram_vect.get_feature_names()[:20])","919d605a":"X_counts_df = pd.DataFrame(X_counts.toarray())\nX_counts_df.columns = ngram_vect.get_feature_names()\nX_counts_df.head()","61e47c1c":"from sklearn.feature_extraction.text import TfidfVectorizer\n\ntfidf_vect = TfidfVectorizer(analyzer=clean_text)\nX_tfidf = tfidf_vect.fit_transform(data['body_text'])\nprint(X_tfidf.shape)\nprint(tfidf_vect.get_feature_names()[:20])","bec0cb5b":"X_tfidf_df = pd.DataFrame(X_tfidf.toarray())\nX_tfidf_df.columns = tfidf_vect.get_feature_names()\nX_tfidf_df.head()","541b2efd":"import string\n\ndef count_punct(text):\n    count = sum([1 for char in text if char in string.punctuation])\n    return round(count\/(len(text) - text.count(\" \")), 3)*100\n\ndata['body_len'] = data['body_text'].apply(lambda x: len(x) - x.count(\" \"))\ndata['punct%'] = data['body_text'].apply(lambda x: count_punct(x))\n\ndata.head()","88d72ae5":"from sklearn.feature_extraction.text import TfidfVectorizer\n\n# decision to use the TF-IDF vecorizer to account for token relevance\ntfidf_vect = TfidfVectorizer(analyzer=clean_text)\nX_tfidf = tfidf_vect.fit_transform(data['body_text'])\n\n# new DF without labels\nX_features = pd.concat([data['body_len'], \n                        data['punct%'], \n                        pd.DataFrame(X_tfidf.toarray())], \n                        axis=1)\nX_features.head()","836024cd":"from sklearn.ensemble import RandomForestClassifier","93549a22":"print(dir(RandomForestClassifier))\nprint(RandomForestClassifier())","53b1816b":"from sklearn.model_selection import KFold, cross_val_score","88d7d94e":"# only set n_jobs paramenter to -1 to enable building the individual decision trees in parallel.\nrf = RandomForestClassifier(n_jobs=-1)\n# Kfold, only hyperparameter is how many splits: how many folds in our cross-validation\nk_fold = KFold(n_splits=5)\n\n# show cross-validation scores of 5 parallel jobs\ncross_val_score(rf, X_features, data['label'], cv=k_fold, scoring='accuracy', n_jobs=-1)","1ee0779c":"from sklearn.metrics import precision_recall_fscore_support as score\nfrom sklearn.model_selection import train_test_split","5029292f":"# setting and performing train\/test split\nX_train, X_test, y_train, y_test = train_test_split(X_features, data['label'], test_size=0.2)","85272f55":"# select settings for calling the random forest classifier\nrf = RandomForestClassifier(n_estimators=50, max_depth=20, n_jobs=-1)\n# fit the model\nrf_model = rf.fit(X_train, y_train)","ba5696a3":"# random forest allows for feature importances to be explored\nsorted(zip(rf_model.feature_importances_, X_train.columns), reverse=True)[0:10]","3a62c95f":"# y-pred is an array of predictions for each of the elements in the test set\ny_pred = rf_model.predict(X_test)\n# tell the model to score y-labels searching for spam probability\nprecision, recall, fscore, support = score(y_test,             # pass in the actual y labels\n                                           y_pred,             # pass in the predictions\n                                           pos_label='spam',   # positive label (searched for) is spam\n                                           average='binary')   # scoring setting","ca591d82":"print('Precision: {} \/ Recall: {} \/ Accuracy: {}'.format(round(precision, 3),\n                                                        round(recall, 3),\n                                                        round((y_pred==y_test).sum() \/ len(y_pred),3))) #sum True(y_pred==y_test) results and divide that by the total length of the test set","49fa7cb8":"# (copy) setting and performing train\/test split\nX_train, X_test, y_train, y_test = train_test_split(X_features, data['label'], test_size=0.2)","cb528aa4":"def train_RF(n_est, depth):\n    # instanciate random forest classifier\n    rf = RandomForestClassifier(n_estimators=n_est, max_depth=depth, n_jobs=-1)\n    # call rf and fit train the model\n    rf_model = rf.fit(X_train, y_train)\n    # call the model and predict on X_test\n    y_pred = rf_model.predict(X_test)\n    # call saved predictions y_pred and print scores\n    precision, recall, fscore, support = score(y_test, y_pred, pos_label='spam', average='binary')\n    print('Est: {} \/ Depth: {} ---- Precision: {} \/ Recall: {} \/ Accuracy: {}'.format(\n        n_est, depth, round(precision, 3), round(recall, 3),\n        round((y_pred==y_test).sum() \/ len(y_pred), 3)))","9b2c0a82":"# building the grid search functionality\nfor n_est in [10, 50, 100]:\n    for depth in [10, 20, 30, None]:\n        train_RF(n_est, depth)","fe4b58dd":"from sklearn.model_selection import GridSearchCV","df861a3f":"# call tf-idf and stored that as X_tfidf_feat. To test which of these vectorizing frameworks works better. \ntfidf_vect = TfidfVectorizer(analyzer=clean_text)\nX_tfidf = tfidf_vect.fit_transform(data['body_text'])\nX_tfidf_feat = pd.concat([data['body_len'], data['punct%'], pd.DataFrame(X_tfidf.toarray())], axis=1)\n\n# estimator and parameter grid\nrf = RandomForestClassifier()\nparam = {'n_estimators': [10, 150, 300],\n        'max_depth': [30, 60, 90, None]}\n\n# construct CV object\ngs = GridSearchCV(rf, param, cv=5, n_jobs=-1)\n# train stored grid search object\ngs_fit = gs.fit(X_tfidf_feat, data['label'])\n#  wrapping this gs_fit.cv_results_ in pd.DataFrame\npd.DataFrame(gs_fit.cv_results_).sort_values('mean_test_score', ascending=False)[0:5]","cd720ff2":"# run X_count and stored that as X_count_feat. To test which of these vectorizing frameworks works better. \ncount_vect = CountVectorizer(analyzer=clean_text)\nX_count = count_vect.fit_transform(data['body_text'])\nX_count_feat = pd.concat([data['body_len'], data['punct%'], pd.DataFrame(X_count.toarray())], axis=1)\n\n# estimator and parameter grid\nrf = RandomForestClassifier()\nparam = {'n_estimators': [10, 150, 300],\n        'max_depth': [30, 60, 90, None]}\n\n# construct CV object\ngs = GridSearchCV(rf, param, cv=5, n_jobs=-1)\n# train stored grid search object\ngs_fit = gs.fit(X_count_feat, data['label'])\n#  wrapping this gs_fit.cv_results_ in pd.DataFrame\npd.DataFrame(gs_fit.cv_results_).sort_values('mean_test_score', ascending=False)[0:5]","9d493b7c":"from sklearn.ensemble import GradientBoostingClassifier","8ed674c4":"print(dir(GradientBoostingClassifier))\nprint(GradientBoostingClassifier())","0f8481ca":"from sklearn.metrics import precision_recall_fscore_support as score\nfrom sklearn.model_selection import train_test_split","787c9da4":"X_train, X_test, y_train, y_test = train_test_split(X_features, data['label'], test_size=0.2)","232dd876":"def train_GB(est, max_depth, lr):\n    gb = GradientBoostingClassifier(n_estimators=est, max_depth=max_depth, learning_rate=lr)\n    gb_model = gb.fit(X_train, y_train)\n    y_pred = gb_model.predict(X_test)\n    precision, recall, fscore, train_support = score(y_test, y_pred, pos_label='spam', average='binary')\n    print('Est: {} \/ Depth: {} \/ LR: {} ---- Precision: {} \/ Recall: {} \/ Accuracy: {}'.format(\n        est, max_depth, lr, round(precision, 3), round(recall, 3), \n        round((y_pred==y_test).sum()\/len(y_pred), 3)))","f88bf334":"for n_est in [50, 100, 150]:\n    for max_depth in [3, 7, 11, 15]:\n        for lr in [0.01, 0.1, 1]:\n            train_GB(n_est, max_depth, lr)","33693807":"from sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.model_selection import GridSearchCV","d53280d3":"# TF-IDF\ntfidf_vect = TfidfVectorizer(analyzer=clean_text)\nX_tfidf = tfidf_vect.fit_transform(data['body_text'])\nX_tfidf_feat = pd.concat([data['body_len'], data['punct%'], pd.DataFrame(X_tfidf.toarray())], axis=1)\n\ngb = GradientBoostingClassifier()\nparam = {\n    'n_estimators': [100, 150], \n    'max_depth': [7, 11, 15],\n    'learning_rate': [0.1]\n}\n\nclf = GridSearchCV(gb, param, cv=5, n_jobs=-1)\ncv_fit = clf.fit(X_tfidf_feat, data['label'])\npd.DataFrame(cv_fit.cv_results_).sort_values('mean_test_score', ascending=False)[0:5]","be6b7321":"# CountVectorizer\ncount_vect = CountVectorizer(analyzer=clean_text)\nX_count = count_vect.fit_transform(data['body_text'])\nX_count_feat = pd.concat([data['body_len'], data['punct%'], pd.DataFrame(X_count.toarray())], axis=1)\n\ngb = GradientBoostingClassifier()\nparam = {\n    'n_estimators': [50, 100, 150], \n    'max_depth': [7, 11, 15],\n    'learning_rate': [0.1]\n}\n\nclf = GridSearchCV(gb, param, cv=5, n_jobs=-1)\ncv_fit = clf.fit(X_count_feat, data['label'])\npd.DataFrame(cv_fit.cv_results_).sort_values('mean_test_score', ascending=False)[0:5]","3baab22f":"from sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(data[['body_text', 'body_len', 'punct%']], data['label'], test_size=0.2)","087d6573":"# vectorize text\ntfidf_vect = TfidfVectorizer(analyzer=clean_text)\ntfidf_vect_fit = tfidf_vect.fit(X_train['body_text'])\n\ntfidf_train = tfidf_vect_fit.transform(X_train['body_text'])\ntfidf_test = tfidf_vect_fit.transform(X_test['body_text'])\n\nX_train_vect = pd.concat([X_train[['body_len', 'punct%']].reset_index(drop=True), \n           pd.DataFrame(tfidf_train.toarray())], axis=1)\nX_test_vect = pd.concat([X_test[['body_len', 'punct%']].reset_index(drop=True), \n           pd.DataFrame(tfidf_test.toarray())], axis=1)\n\nX_train_vect.head()","20b1405f":"from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\nfrom sklearn.metrics import precision_recall_fscore_support as score\nimport time","346f0d6b":"rf = RandomForestClassifier(n_estimators=150, max_depth=None, n_jobs=-1)\n\nstart = time.time()\nrf_model = rf.fit(X_train_vect, y_train)\nend = time.time()\nfit_time = (end - start)\n\nstart = time.time()\ny_pred = rf_model.predict(X_test_vect)\nend = time.time()\npred_time = (end - start)\n\nprecision, recall, fscore, train_support = score(y_test, y_pred, pos_label='spam', average='binary')\nprint('Fit time: {} \/ Predict time: {} ---- Precision: {} \/ Recall: {} \/ Accuracy: {}'.format(\n    round(fit_time, 3), round(pred_time, 3), round(precision, 3), round(recall, 3), round((y_pred==y_test).sum()\/len(y_pred), 3)))","152b3663":"gb = GradientBoostingClassifier(n_estimators=150, max_depth=11)\n\nstart = time.time()\ngb_model = gb.fit(X_train_vect, y_train)\nend = time.time()\nfit_time = (end - start)\n\nstart = time.time()\ny_pred = gb_model.predict(X_test_vect)\nend = time.time()\npred_time = (end - start)\n\nprecision, recall, fscore, train_support = score(y_test, y_pred, pos_label='spam', average='binary')\nprint('Fit time: {} \/ Predict time: {} ---- Precision: {} \/ Recall: {} \/ Accuracy: {}'.format(\n    round(fit_time, 3), round(pred_time, 3), round(precision, 3), round(recall, 3), round((y_pred==y_test).sum()\/len(y_pred), 3)))","e852c249":"### GridSearchCV on CountVectorizer","a5f6eaa7":"# 7. Machine Learnign Classifiers: Model Selection","ee99b7ee":"# 7. Machine Learning Classifier: Gradient Boosting\n Gradient boosting is an ensemble method that takes an iterative approach to combining weak learners to create a strong learner by focusing on the mistakes of prior iterations","c5ff9f9f":"### GridSearchCV on TF-IDF","c267e0c6":"## Data wrangling summary function","4c6f0259":"Random forest is an ensemble learning method that constructs a collection of decision trees and then aggregates the predictions of each tree to determine the final prediction. So in this case, the weak models are the individual decision trees, and then those are combined into the strong model that is the aggregated random forest model.","727b56b3":"# 3. Supplemental data cleaning\n## Stemming Text\nStemming is the process of reducing inflected or derived words to their word stem or root. <br>\n`nltk.PorterStemmer(mode='NLTK_EXTENSIONS')`\nDocstring:     \nA word stemmer based on the Porter stemming algorithm.\n    Porter, M. \"An algorithm for suffix stripping.\"\n    Program 14.3 (1980): 130-137.","4d34ccb6":"## Evaluate Gradient Boosting with GridSearchCV","f1d52ad4":"### Gradient Boosting Attributes and Hyperparameters","de8a265b":"# 2. Data Wrangling for NLP\n1. Raw Text: model can't distinguish words\n2. Tokenize: tell the model what to look at\n3. Clean text: remove stopwords, ponctuation, stemming, etc.\n4. Vectorize: convert to numeric form\n5. ML algorythm: fit\/train model","c5ef966d":"##  Remove Ponctuation","87b09c2f":"## Dataset Checks","3d91922d":"## Build custom Grid Search","e6c4fdca":"### Cleaned Data + Stemming + Join function","c87a0270":"## Data Preparation","e6afc296":"### Explore RandomForestClassifier through Holdout Set","80b722e3":"### Explore RandomForestClassifier through Cross-Validation","442290b2":"## Vectorizing Raw Data: TF-IDF\n### TF-IDF\n\nCreates a document-term matrix where the columns represent single unique terms (unigrams) but the cell represents a weighting meant to represent how important a word is to a document.\n","a9ec3833":"## Exploring parameter settings using GridSearchCV","08297d86":"## Lemmatizing Text\nLemmatizing is using vocabulary analysis of words to remove inflectional endings and return to the dictionary form of a word.\nThis text cleaning method is more accurate but uses more resources. <br>\nThe WordNet Lemmatizer `nltk.WordNetLemmatizer()` lemmatizes using WordNet's built-in morphy function.\nReturns the input word unchanged if it cannot be found in WordNet.","e7abf988":"# 4. Vectorizing\nVectorizing is defined as the process of encoding text as integers to create feature vectors. ","0ce1b31e":"## Vectorizing Raw Data: N-Grams\n### N-Grams \n\nCreates a document-term matrix where counts still occupy the cell but instead of the columns representing single terms, they represent all combinations of adjacent words of length n in your text.\n\n\"NLP is an interesting topic\"\n\n| n | Name      | Tokens                                                         |\n|---|-----------|----------------------------------------------------------------|\n| 2 | bigram    | [\"nlp is\", \"is an\", \"an interesting\", \"interesting topic\"]      |\n| 3 | trigram   | [\"nlp is an\", \"is an interesting\", \"an interesting topic\"] |\n| 4 | four-gram | [\"nlp is an interesting\", \"is an interesting topic\"]    |","b0a14777":"## Tokenization","fbd0abf3":"# 6. Machine Learning Classifier: Random Forest","98f87da0":"## Final Evaluation of models","8fa12e5f":"### Random Forest Attributes and Hyperparameters","985ae35c":"# 1. Data Analysis","40e04a33":"### Explore RandomForestClassifier with Grid Search","c7384136":"## Count Vectorizer","844bcd22":"# Setup","87c1e682":"## Remove Stopwords","4421682f":"# 5. Feature Engineering\nCreating new features or transformin existing features to get the most out of available data\n\n### Create feature for % of text that is punctuation"}}