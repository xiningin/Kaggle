{"cell_type":{"e2e189ea":"code","4d83d432":"code","972b5ee4":"code","d537a661":"code","9ac7442d":"code","4df96f73":"code","4bcaa599":"code","d8d185cf":"code","3cdb403f":"code","b82fcdf4":"code","eff031dc":"code","8d6ea971":"code","6ab7b6ea":"code","69c3c632":"code","b8334ec1":"markdown","c93b04de":"markdown","af89d954":"markdown"},"source":{"e2e189ea":"import pandas as pd\nimport numpy as np\nimport lightgbm as lgbm\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import mean_squared_error\n","4d83d432":"from tensorflow.keras.layers import Input,Dense,Dropout\nfrom tensorflow.keras import Model\nfrom  tensorflow.keras.regularizers import l2\nimport tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense,BatchNormalization,Dropout,Embedding,Flatten,Concatenate\nfrom keras import backend as K\nimport tensorflow as tf\nfrom tensorflow.keras import initializers\nfrom tensorflow.keras import regularizers","972b5ee4":"train = pd.read_csv(\"..\/input\/tabular-playground-series-jan-2021\/train.csv\")\ntest = pd.read_csv(\"..\/input\/tabular-playground-series-jan-2021\/test.csv\")","d537a661":"cont_features = [col for col in train.columns if col.startswith(\"cont\")]\nlen(cont_features)","9ac7442d":"from sklearn.preprocessing import LabelEncoder\nalldata = pd.concat([train[cont_features],test[cont_features]],axis=0)\nalldata.shape\nn=2\n\nmax_cat_values = []\nfor c in cont_features:\n    le = LabelEncoder()\n    x = le.fit_transform(alldata[c].round(n))\n    train[c+'_cat'] = le.transform(train[c].round(n))\n    test[c+'_cat'] = le.transform(test[c].round(n))\n    max_cat_values.append(np.max(x)+1)\n    \nn=1\n\nmax_cat_values3 = []\nfor c in cont_features:\n    le = LabelEncoder()\n    x = le.fit_transform(alldata[c].round(n))\n    train[c+'_cat3'] = le.transform(train[c].round(n))\n    test[c+'_cat3'] = le.transform(test[c].round(n))\n    max_cat_values3.append(np.max(x)+1)\n    \nn=10\n\nmax_cat_values4 = []\nfor c in cont_features:\n    alldata[c] = pd.cut(alldata[c],bins=n,labels=np.arange(n))\n    train[c+'_cat4'] = alldata.iloc[:len(train)][c].astype(\"int32\")\n    test[c+'_cat4'] = alldata.iloc[len(train):][c].astype(\"int32\")\n    max_cat_values4.append(np.max(alldata[c])+1)","4df96f73":"train.head()","4bcaa599":"def get_model():\n    inputs = []\n    flatten_layers = []\n    for e, c in enumerate(cont_features):\n        input_c = Input(shape=(1, ), dtype='int32')\n        num_c = max_cat_values[e]\n        embed_c = Embedding(\n            num_c,\n            3,\n        )(input_c)\n        embed_c = Dropout(0.6)(embed_c)\n        flatten_c = Flatten()(embed_c)\n        inputs.append(input_c)\n        flatten_layers.append(flatten_c)\n        \n    flatten_layers2 = []\n    for e, c in enumerate(cont_features):\n        input_c = Input(shape=(1, ), dtype='int32')\n        num_c = max_cat_values3[e]\n        embed_c = Embedding(\n            num_c,\n            12,\n        )(input_c)\n        embed_c = Dropout(0.5)(embed_c)\n        flatten_c = Flatten()(embed_c)\n        inputs.append(input_c)\n        flatten_layers2.append(flatten_c)\n        \n    flatten_layers3 = []\n    for e, c in enumerate(cont_features):\n        input_c = Input(shape=(1, ), dtype='int32')\n        num_c = max_cat_values4[e]\n        embed_c = Embedding(\n            num_c,\n            12,\n        )(input_c)\n        embed_c = Dropout(0.5)(embed_c)\n        flatten_c = Flatten()(embed_c)\n        inputs.append(input_c)\n        flatten_layers3.append(flatten_c)\n\n    input_num = Input(shape=(14,), dtype='float32')\n    inputs.append(input_num)\n    \n    # 1st embeddings with main continous features.\n    flatten_layers.append(input_num) # This line is important. I'm concatenating original features with the first type of categorical features' embeddings.\n    flatten = Concatenate()(flatten_layers)\n    x = Dense(1024, activation='relu',bias_initializer=\"normal\",kernel_regularizer=regularizers.l2(0.0001),\n              bias_regularizer=regularizers.l2(0.00000),\n             activity_regularizer = regularizers.l2(0.0001))(flatten) \n    x = Dropout(0.)(x)\n    x = Dense(1024, activation='relu',kernel_initializer='normal')(x) \n    \n    # 2nd embeddings.\n    flatten2 = Concatenate()(flatten_layers2)\n    x2 = Dense(2048, activation='relu',bias_initializer=\"normal\")(flatten2) \n    x2 = Dense(2048, activation='relu',kernel_initializer='normal')(x2) # 1500 original\n    \n    # 3rd embeddings.\n    flatten3 = Concatenate()(flatten_layers3)\n    x3 = Dense(2048, activation='relu',bias_initializer=\"normal\")(flatten3) \n    x3 = Dense(2048, activation='relu',kernel_initializer='normal')(x3)\n    \n    \n    x = Concatenate()([x,x2,x3]) # Concating all there outputs.\n  \n    \n    outputs = Dense(1, activation='linear')(x)\n    model = Model(inputs=inputs, outputs=outputs)\n    opt = tf.keras.optimizers.Adam(learning_rate=0.001)\n    model.compile(loss=\"mse\", optimizer=opt)\n    return model","d8d185cf":"model=get_model()\nmodel.summary()","3cdb403f":"from keras.callbacks import ModelCheckpoint, Callback, EarlyStopping\n\ndef scheduler(epoch, lr):\n    if epoch < 10:\n        return lr\n    else: \n        return lr * 0.95\n    \n","b82fcdf4":"features = [col for col in train.columns if col not in [\"target\",\"id\",\"preds\",\"scaled_target\",\"lgbm_preds\"]][-14:]\nlen(features)","eff031dc":"#X = X.abs()\ny = train[\"target\"]\nkf = KFold(n_splits=5, shuffle=True, random_state=1)\noof = np.zeros(len(train))\nscore_list = []\nfold = 1\ntest_preds = []\n\n    \nfor train_index, test_index in kf.split(train):\n    X_train, X_val = train.iloc[train_index], train.iloc[test_index]\n    y_train, y_val = y.iloc[train_index], y.iloc[test_index]\n    \n\n    # Preparing model inputs.\n    X_train_list=[]\n    X_val_list = []\n    test_list = []\n    for col in cont_features:\n        X_train_list.append(X_train[col+'_cat'].values)\n        X_val_list.append(X_val[col+'_cat'].values)\n        test_list.append(test[col+'_cat'].values)\n    for col in cont_features:\n        X_train_list.append(X_train[col+'_cat3'].values)\n        X_val_list.append(X_val[col+'_cat3'].values)\n        test_list.append(test[col+'_cat3'].values)\n    for col in cont_features:\n        X_train_list.append(X_train[col+'_cat4'].values)\n        X_val_list.append(X_val[col+'_cat4'].values)\n        test_list.append(test[col+'_cat4'].values)\n        \n    X_train_list.append(X_train[cont_features].values)\n    X_val_list.append(X_val[cont_features].values)\n    test_list.append(test[cont_features].values)\n\n    y_pred_list = []\n\n    # Callbacks\n    estop = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=15, restore_best_weights=True)\n    lr_callback = tf.keras.callbacks.LearningRateScheduler(scheduler)\n    \n    model = get_model()\n    history = model.fit(X_train_list, y_train,validation_data=(X_val_list,y_val), \n              callbacks=[lr_callback,estop], epochs=10000, batch_size=1024, verbose=1)\n   \n   \n        \n    y_pred_list.append(model.predict(X_val_list).ravel())\n    oof[test_index] = np.mean(y_pred_list,axis=0)\n    score = np.sqrt(mean_squared_error(y_val, oof[test_index]))\n    score_list.append(score)\n    print(f\"RMSE Fold-{fold} : {score}\")\n    \n    test_preds.append(model.predict(test_list).ravel())\n    \n    fold+=1\n\nnp.mean(score_list)","8d6ea971":"print(np.mean(score_list))\nscore_list","6ab7b6ea":"train[\"4_preds\"] = oof\ntest[\"4_preds\"] = np.mean(test_preds,axis=0)\nnp.sqrt(mean_squared_error(train[\"target\"], train[\"4_preds\"]))","69c3c632":"train[[\"id\",\"4_preds\"]].to_csv(\"nn_preds_train.csv\",index=False)\ntest[[\"id\",\"4_preds\"]].to_csv(\"nn_preds_test.csv\",index=False)","b8334ec1":"max_cat_values, max_cat_values3 and max_cat_values4 lists will be used in Embedding layer in the NN model.","c93b04de":"As a first thing, I've created 3 categorical features for each cont feature. 2 of my categorical features are generated by only rounding numerical columns to lower decimals. One with 2 decimals and the other with 1 decimal. It's something like binning. My third categorical feature is obtained via pandas.cut function. I binned the continos features into 10 bins.\n\nI also applied labelencoder to get final categorical feature values.","af89d954":"### In this notebook I want to share how I got a 0.700X CV score with a single NN model with the help of categorical embedding."}}