{"cell_type":{"4da9f2a6":"code","25fe862b":"code","9d4524d0":"code","8ac3036b":"code","075d8347":"code","e74dca1c":"code","741f8d6a":"code","303ef275":"code","d3b9e495":"code","fc57cbbb":"code","90303a04":"code","f8de6d25":"code","182db848":"code","4e888d7b":"code","0151f5a5":"code","faa6a3c0":"code","42530a33":"code","2c8ee2f8":"code","8de1d82e":"code","4799fe83":"code","9aa64ca8":"code","52aceb35":"code","4ae757b9":"code","ce23cb0d":"code","141bf171":"code","948b5787":"code","1fb30b4e":"code","f94e8bf9":"code","7717b86b":"code","761545f1":"code","d793dba0":"code","256acd29":"code","b23a9e7f":"code","9db4769c":"code","e880e73a":"code","fa4d48c1":"code","274803f3":"code","06e15479":"code","6882fd04":"code","ca38a40a":"code","1c05d7c4":"code","3a2e5633":"code","d329d445":"code","7fe206cd":"code","097381a9":"code","72900b18":"code","3167912a":"code","5bf6b00c":"code","95905532":"code","5b595d1a":"code","51366561":"code","854491a2":"code","2a26ac0f":"code","958e68bc":"code","c03ea5c2":"code","a3901695":"code","2c142987":"code","dbfdbf30":"code","499570c9":"code","acff0e9a":"code","f3087529":"code","d16d8275":"code","060b28f5":"code","fcd342a0":"code","7b4c7808":"code","11ca258b":"code","53a52ada":"code","6512af57":"code","d4bed8c4":"code","1568fd8d":"code","2a0f5efc":"code","faf8160d":"code","e452f501":"code","81d0ced2":"code","29dcf799":"code","c1d3c353":"code","84423257":"code","2a827c57":"code","001d3824":"code","74b5980f":"code","e03085b0":"code","0acf024c":"code","ba8cbec5":"code","67076ecf":"code","d1d46a0d":"code","2343bbb9":"code","00626286":"code","ef4fde69":"code","52417737":"code","18ee4857":"code","237a3a31":"code","edf3dda7":"code","eecb04f1":"code","1eb81564":"code","6cccc574":"markdown","490325cf":"markdown","0266a12c":"markdown","c0af9fb1":"markdown","d4fcdb37":"markdown","51068c9e":"markdown","6e42d9e9":"markdown","a035fd2f":"markdown","27ab5c69":"markdown","376ff032":"markdown","2c9d6cd4":"markdown","8423c178":"markdown","3820cb5b":"markdown","9584a40f":"markdown","dad1c285":"markdown","1d3c2a65":"markdown","866f1d8d":"markdown","be109e28":"markdown","16116f58":"markdown","9377d806":"markdown","a0fdc1d7":"markdown","998e76cb":"markdown","aad0b851":"markdown","5107579a":"markdown","5e28bedf":"markdown","1e9a0f06":"markdown","23bf0e72":"markdown","3aed7501":"markdown","50937870":"markdown","25f65b3c":"markdown","f2ad2766":"markdown","567fcc24":"markdown","9846d7bc":"markdown","7111ed79":"markdown","5112d7a9":"markdown","7eda3d03":"markdown","1702e688":"markdown","88e255ef":"markdown","6f044bc9":"markdown","39630b2b":"markdown","3dca5ab4":"markdown","997d6892":"markdown","d92dab75":"markdown","d6ce124e":"markdown","9f7ba978":"markdown","78d1dcff":"markdown","c42e7ca6":"markdown","c98194de":"markdown","cf570f0c":"markdown","f2ae22dc":"markdown","0883519f":"markdown","e69081b5":"markdown","81f03ae5":"markdown","366f1fa5":"markdown","7f0c5bf0":"markdown","2bf01b48":"markdown","1a2ecf43":"markdown","25dfc9d6":"markdown","70b1468e":"markdown","8f79a905":"markdown","c015a7ff":"markdown","2138017d":"markdown","99ff6a58":"markdown","95ba46db":"markdown","a654b0fa":"markdown","5b25a9db":"markdown","3e89e10d":"markdown","0b7c809f":"markdown","4b97798c":"markdown","2332af04":"markdown","b4442ce0":"markdown","90cf9e69":"markdown","0adb42e8":"markdown"},"source":{"4da9f2a6":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport re\nimport time\nimport warnings\nimport numpy as np\nfrom nltk.corpus import stopwords\nfrom sklearn.decomposition import TruncatedSVD\nfrom sklearn.preprocessing import normalize\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.manifold import TSNE\nimport seaborn as sns\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics.classification import accuracy_score, log_loss\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.linear_model import SGDClassifier\nfrom imblearn.over_sampling import SMOTE\nfrom collections import Counter\nfrom scipy.sparse import hstack\nfrom sklearn.multiclass import OneVsRestClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.model_selection import train_test_split\nfrom collections import Counter, defaultdict\nfrom sklearn.calibration import CalibratedClassifierCV\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import GridSearchCV\nimport math\nfrom sklearn.metrics import normalized_mutual_info_score\nfrom sklearn.ensemble import RandomForestClassifier\nwarnings.filterwarnings(\"ignore\")\n\nfrom mlxtend.classifier import StackingClassifier\n\nfrom sklearn import model_selection\nfrom sklearn.linear_model import LogisticRegression\nimport os","25fe862b":"# exploring the data files\nfrom subprocess import check_output\n# checking for subfolder name\nprint(check_output([\"ls\", \"..\/input\"]).decode(\"utf8\"))","9d4524d0":"# exploring available data files\nprint(check_output([\"ls\", \"..\/input\/msk-redefining-cancer-treatment\"]).decode(\"utf8\"))","8ac3036b":"# unzipping training data : https:\/\/www.kaggle.com\/verzivolli\/personalized-medicine-redefining-cancer-treatmen\nimport zipfile\n\n# declaring datasets to unzip\nDatasets = [\n    \"..\/input\/msk-redefining-cancer-treatment\/\" + \"training_text.zip\",\n    \"..\/input\/msk-redefining-cancer-treatment\/\" + \"training_variants.zip\"\n]\n\n# unzipping declared datasets\nfor dataset in Datasets:\n    with zipfile.ZipFile(dataset,\"r\") as z:\n        z.extractall(\".\")","075d8347":"# Checking that unziped file is created\nfor dataset in Datasets:\n    # dataset name is in both cases the 3 index after split \"\/\" in this particular example\n    print(check_output([\"ls\",dataset.split(\"\/\")[3][:-4]]).decode(\"utf8\"))","e74dca1c":"data = pd.read_csv('training_variants')\nprint('Number of data points : ', data.shape[0])\nprint('Number of features : ', data.shape[1])\nprint('Features : ', data.columns.values)\ndata.head()","741f8d6a":"# note the seprator in this file\ndata_text =pd.read_csv(\"training_text\",sep=\"\\|\\|\",engine=\"python\",names=[\"ID\",\"TEXT\"],skiprows=1)#skiprows=1 used to skip heading\nprint('Number of data points : ', data_text.shape[0])\nprint('Number of features : ', data_text.shape[1])\nprint('Features : ', data_text.columns.values)\ndata_text.head()","303ef275":"# loading stop words from nltk library\nstop_words = set(stopwords.words('english'))\n\n\ndef nlp_preprocessing(total_text, index, column):\n    if type(total_text) is not int:\n        string = \"\"\n        # replace every special char with space\n        total_text = re.sub('[^a-zA-Z0-9\\n]', ' ', total_text)\n        # replace multiple spaces with single space\n        total_text = re.sub('\\s+',' ', total_text)\n        # converting all the chars into lower-case.\n        total_text = total_text.lower()\n        \n        for word in total_text.split():\n        # if the word is a not a stop word then retain that word from the data\n            if not word in stop_words:\n                string += word + \" \"\n        \n        data_text[column][index] = string","d3b9e495":"#text processing stage.\nstart_time = time.clock()\nfor index, row in data_text.iterrows():\n    if type(row['TEXT']) is str:\n        nlp_preprocessing(row['TEXT'], index, 'TEXT')\n    else:\n        print(\"there is no text description for id:\",index)\nprint('Time took for preprocessing the text :',time.clock() - start_time, \"seconds\")","fc57cbbb":"#merging both gene_variations and text data based on ID\nresult = pd.merge(data, data_text,on='ID', how='left')\nresult.head()","90303a04":"result.info()","f8de6d25":"result.isnull().any(axis=0)","182db848":"result.isnull().any(axis=1)","4e888d7b":"result[result.isnull().any(axis=1)]","0151f5a5":"result.loc[result['TEXT'].isnull(),'TEXT'] = result['Gene'] +' '+result['Variation']","faa6a3c0":"result[result['ID']==1109]","42530a33":"y_true = result['Class'].values\nresult.Gene      = result.Gene.str.replace('\\s+', '_')\nresult.Variation = result.Variation.str.replace('\\s+', '_')\n\n# split the data into test and train by maintaining same distribution of output varaible 'y_true' [stratify=y_true]\nX_train, test_df, y_train, y_test = train_test_split(result, y_true, stratify=y_true, test_size=0.2)\n# split the train data into train and cross validation by maintaining same distribution of output varaible 'y_train' [stratify=y_train]\ntrain_df, cv_df, y_train, y_cv = train_test_split(X_train, y_train, stratify=y_train, test_size=0.2)","2c8ee2f8":"print('Number of data points in train data:', train_df.shape[0])\nprint('Number of data points in test data:', test_df.shape[0])\nprint('Number of data points in cross validation data:', cv_df.shape[0])","8de1d82e":"# it returns a dict, keys as class labels and values as the number of data points in that class\ntrain_class_distribution = train_df['Class'].value_counts().sort_index()\ntest_class_distribution = test_df['Class'].value_counts().sort_index()\ncv_class_distribution = cv_df['Class'].value_counts().sort_index()\n\n#my_colors = 'rgbkymc'\ncolor=['red', 'blue','green','brown','yellow','purple', 'orange', 'pink','lime']\ntrain_class_distribution.plot(kind='bar', color=color)\nplt.xlabel('Class')\nplt.ylabel('Data points per Class')\nplt.title('Distribution of yi in train data')\nplt.grid()\nplt.show()\n\n# ref: argsort https:\/\/docs.scipy.org\/doc\/numpy\/reference\/generated\/numpy.argsort.html\n# -(train_class_distribution.values): the minus sign will give us in decreasing order\nsorted_yi = np.argsort(-train_class_distribution.values)\nfor i in sorted_yi:\n    print('Number of data points in class', i+1, ':',train_class_distribution.values[i], '(', np.round((train_class_distribution.values[i]\/train_df.shape[0]*100), 3), '%)')\n\n    \nprint('-'*80)\ntest_class_distribution.plot(kind='bar', color=color)\nplt.xlabel('Class')\nplt.ylabel('Data points per Class')\nplt.title('Distribution of yi in test data')\nplt.grid()\nplt.show()\n\n# ref: argsort https:\/\/docs.scipy.org\/doc\/numpy\/reference\/generated\/numpy.argsort.html\n# -(train_class_distribution.values): the minus sign will give us in decreasing order\nsorted_yi = np.argsort(-test_class_distribution.values)\nfor i in sorted_yi:\n    print('Number of data points in class', i+1, ':',test_class_distribution.values[i], '(', np.round((test_class_distribution.values[i]\/test_df.shape[0]*100), 3), '%)')\n\nprint('-'*80)\ncv_class_distribution.plot(kind='bar', color=color)\nplt.xlabel('Class')\nplt.ylabel('Data points per Class')\nplt.title('Distribution of yi in cross validation data')\nplt.grid()\nplt.show()\n\n# ref: argsort https:\/\/docs.scipy.org\/doc\/numpy\/reference\/generated\/numpy.argsort.html\n# -(train_class_distribution.values): the minus sign will give us in decreasing order\nsorted_yi = np.argsort(-train_class_distribution.values)\nfor i in sorted_yi:\n    print('Number of data points in class', i+1, ':',cv_class_distribution.values[i], '(', np.round((cv_class_distribution.values[i]\/cv_df.shape[0]*100), 3), '%)')\n","4799fe83":"# This function plots the confusion matrices given y_i, y_i_hat.\ndef plot_confusion_matrix(test_y, predict_y):\n    C = confusion_matrix(test_y, predict_y)\n    # C = 9,9 matrix, each cell (i,j) represents number of points of class i are predicted class j\n    \n    A =(((C.T)\/(C.sum(axis=1))).T)\n    #divid each element of the confusion matrix with the sum of elements in that column\n    \n    # C = [[1, 2],\n    #     [3, 4]]\n    # C.T = [[1, 3],\n    #        [2, 4]]\n    # C.sum(axis = 1)  axis=0 corresonds to columns and axis=1 corresponds to rows in two diamensional array\n    # C.sum(axix =1) = [[3, 7]]\n    # ((C.T)\/(C.sum(axis=1))) = [[1\/3, 3\/7]\n    #                           [2\/3, 4\/7]]\n\n    # ((C.T)\/(C.sum(axis=1))).T = [[1\/3, 2\/3]\n    #                           [3\/7, 4\/7]]\n    # sum of row elements = 1\n    \n    B =(C\/C.sum(axis=0))\n    #divid each element of the confusion matrix with the sum of elements in that row\n    # C = [[1, 2],\n    #     [3, 4]]\n    # C.sum(axis = 0)  axis=0 corresonds to columns and axis=1 corresponds to rows in two diamensional array\n    # C.sum(axix =0) = [[4, 6]]\n    # (C\/C.sum(axis=0)) = [[1\/4, 2\/6],\n    #                      [3\/4, 4\/6]] \n    \n    labels = [1,2,3,4,5,6,7,8,9]\n    # representing A in heatmap format\n    print(\"-\"*20, \"Confusion matrix\", \"-\"*20)\n    plt.figure(figsize=(20,7))\n    sns.heatmap(C, annot=True, cmap=\"YlGnBu\", fmt=\".3f\", xticklabels=labels, yticklabels=labels)\n    plt.xlabel('Predicted Class')\n    plt.ylabel('Original Class')\n    plt.show()\n\n    print(\"-\"*20, \"Precision matrix (Columm Sum=1)\", \"-\"*20)\n    plt.figure(figsize=(20,7))\n    sns.heatmap(B, annot=True, cmap=\"YlGnBu\", fmt=\".3f\", xticklabels=labels, yticklabels=labels)\n    plt.xlabel('Predicted Class')\n    plt.ylabel('Original Class')\n    plt.show()\n    \n    # representing B in heatmap format\n    print(\"-\"*20, \"Recall matrix (Row sum=1)\", \"-\"*20)\n    plt.figure(figsize=(20,7))\n    sns.heatmap(A, annot=True, cmap=\"YlGnBu\", fmt=\".3f\", xticklabels=labels, yticklabels=labels)\n    plt.xlabel('Predicted Class')\n    plt.ylabel('Original Class')\n    plt.show()","9aa64ca8":"# we need to generate 9 numbers and the sum of numbers should be 1\n# one solution is to genarate 9 numbers and divide each of the numbers by their sum\n# ref: https:\/\/stackoverflow.com\/a\/18662466\/4084039\ntest_data_len = test_df.shape[0]\ncv_data_len = cv_df.shape[0]\n\n# we create a output array that has exactly same size as the CV data\ncv_predicted_y = np.zeros((cv_data_len,9))\nfor i in range(cv_data_len):\n    rand_probs = np.random.rand(1,9)\n    cv_predicted_y[i] = ((rand_probs\/sum(sum(rand_probs)))[0])\nprint(\"Log loss on Cross Validation Data using Random Model\",log_loss(y_cv,cv_predicted_y, eps=1e-15))\n\n\n# Test-Set error.\n#we create a output array that has exactly same as the test data\ntest_predicted_y = np.zeros((test_data_len,9))\nfor i in range(test_data_len):\n    rand_probs = np.random.rand(1,9)\n    test_predicted_y[i] = ((rand_probs\/sum(sum(rand_probs)))[0])\nprint(\"Log loss on Test Data using Random Model\",log_loss(y_test,test_predicted_y, eps=1e-15))\n\npredicted_y =np.argmax(test_predicted_y, axis=1)\nplot_confusion_matrix(y_test, predicted_y+1)","52aceb35":"# code for response coding with Laplace smoothing.\n# alpha : used for laplace smoothing\n# feature: ['gene', 'variation']\n# df: ['train_df', 'test_df', 'cv_df']\n# algorithm\n# ----------\n# Consider all unique values and the number of occurances of given feature in train data dataframe\n# build a vector (1*9) , the first element = (number of times it occured in class1 + 10*alpha \/ number of time it occurred in total data+90*alpha)\n# gv_dict is like a look up table, for every gene it store a (1*9) representation of it\n# for a value of feature in df:\n# if it is in train data:\n# we add the vector that was stored in 'gv_dict' look up table to 'gv_fea'\n# if it is not there is train:\n# we add [1\/9, 1\/9, 1\/9, 1\/9,1\/9, 1\/9, 1\/9, 1\/9, 1\/9] to 'gv_fea'\n# return 'gv_fea'\n# ----------------------\n\n# get_gv_fea_dict: Get Gene varaition Feature Dict\ndef get_gv_fea_dict(alpha, feature, df):\n    # value_count: it contains a dict like\n    # print(train_df['Gene'].value_counts())\n    # output:\n    #        {BRCA1      174\n    #         TP53       106\n    #         EGFR        86\n    #         BRCA2       75\n    #         PTEN        69\n    #         KIT         61\n    #         BRAF        60\n    #         ERBB2       47\n    #         PDGFRA      46\n    #         ...}\n    # print(train_df['Variation'].value_counts())\n    # output:\n    # {\n    # Truncating_Mutations                     63\n    # Deletion                                 43\n    # Amplification                            43\n    # Fusions                                  22\n    # Overexpression                            3\n    # E17K                                      3\n    # Q61L                                      3\n    # S222D                                     2\n    # P130S                                     2\n    # ...\n    # }\n    value_count = train_df[feature].value_counts()\n    \n    # gv_dict : Gene Variation Dict, which contains the probability array for each gene\/variation\n    gv_dict = dict()\n    \n    # denominator will contain the number of time that particular feature occured in whole data\n    for i, denominator in value_count.items():\n        # vec will contain (p(yi==1\/Gi) probability of gene\/variation belongs to perticular class\n        # vec is 9 diamensional vector\n        vec = []\n        for k in range(1,10):\n            # print(train_df.loc[(train_df['Class']==1) & (train_df['Gene']=='BRCA1')])\n            #         ID   Gene             Variation  Class  \n            # 2470  2470  BRCA1                S1715C      1   \n            # 2486  2486  BRCA1                S1841R      1   \n            # 2614  2614  BRCA1                   M1R      1   \n            # 2432  2432  BRCA1                L1657P      1   \n            # 2567  2567  BRCA1                T1685A      1   \n            # 2583  2583  BRCA1                E1660G      1   \n            # 2634  2634  BRCA1                W1718L      1   \n            # cls_cnt.shape[0] will return the number of rows\n\n            cls_cnt = train_df.loc[(train_df['Class']==k) & (train_df[feature]==i)]\n            \n            # cls_cnt.shape[0](numerator) will contain the number of time that particular feature occured in whole data\n            vec.append((cls_cnt.shape[0] + alpha*10)\/ (denominator + 90*alpha))\n\n        # we are adding the gene\/variation to the dict as key and vec as value\n        gv_dict[i]=vec\n    return gv_dict\n\n# Get Gene variation feature\ndef get_gv_feature(alpha, feature, df):\n    # print(gv_dict)\n    #     {'BRCA1': [0.20075757575757575, 0.03787878787878788, 0.068181818181818177, 0.13636363636363635, 0.25, 0.19318181818181818, 0.03787878787878788, 0.03787878787878788, 0.03787878787878788], \n    #      'TP53': [0.32142857142857145, 0.061224489795918366, 0.061224489795918366, 0.27040816326530615, 0.061224489795918366, 0.066326530612244902, 0.051020408163265307, 0.051020408163265307, 0.056122448979591837], \n    #      'EGFR': [0.056818181818181816, 0.21590909090909091, 0.0625, 0.068181818181818177, 0.068181818181818177, 0.0625, 0.34659090909090912, 0.0625, 0.056818181818181816], \n    #      'BRCA2': [0.13333333333333333, 0.060606060606060608, 0.060606060606060608, 0.078787878787878782, 0.1393939393939394, 0.34545454545454546, 0.060606060606060608, 0.060606060606060608, 0.060606060606060608], \n    #      'PTEN': [0.069182389937106917, 0.062893081761006289, 0.069182389937106917, 0.46540880503144655, 0.075471698113207544, 0.062893081761006289, 0.069182389937106917, 0.062893081761006289, 0.062893081761006289], \n    #      'KIT': [0.066225165562913912, 0.25165562913907286, 0.072847682119205295, 0.072847682119205295, 0.066225165562913912, 0.066225165562913912, 0.27152317880794702, 0.066225165562913912, 0.066225165562913912], \n    #      'BRAF': [0.066666666666666666, 0.17999999999999999, 0.073333333333333334, 0.073333333333333334, 0.093333333333333338, 0.080000000000000002, 0.29999999999999999, 0.066666666666666666, 0.066666666666666666],\n    #      ...\n    #     }\n    gv_dict = get_gv_fea_dict(alpha, feature, df)\n    # value_count is similar in get_gv_fea_dict\n    value_count = train_df[feature].value_counts()\n    \n    # gv_fea: Gene_variation feature, it will contain the feature for each feature value in the data\n    gv_fea = []\n    # for every feature values in the given data frame we will check if it is there in the train data then we will add the feature to gv_fea\n    # if not we will add [1\/9,1\/9,1\/9,1\/9,1\/9,1\/9,1\/9,1\/9,1\/9] to gv_fea\n    for index, row in df.iterrows():\n        if row[feature] in dict(value_count).keys():\n            gv_fea.append(gv_dict[row[feature]])\n        else:\n            gv_fea.append([1\/9,1\/9,1\/9,1\/9,1\/9,1\/9,1\/9,1\/9,1\/9])\n#             gv_fea.append([-1,-1,-1,-1,-1,-1,-1,-1,-1])\n    return gv_fea","4ae757b9":"unique_genes = train_df['Gene'].value_counts()\nprint('Number of Unique Genes :', unique_genes.shape[0])\n# the top 10 genes that occured most\nprint(unique_genes.head(10))","ce23cb0d":"print(\"Ans: There are\", unique_genes.shape[0] ,\"different categories of genes in the train data, and they are distibuted as follows\",)","141bf171":"s = sum(unique_genes.values);\nh = unique_genes.values\/s;\nplt.plot(h, label=\"Histrogram of Genes\")\nplt.xlabel('Index of a Gene')\nplt.ylabel('Number of Occurances')\nplt.legend()\nplt.grid()\nplt.show()","948b5787":"c = np.cumsum(h)\nplt.plot(c,label='Cumulative distribution of Genes')\nplt.grid()\nplt.legend()\nplt.show()","1fb30b4e":"#response-coding of the Gene feature\n# alpha is used for laplace smoothing\nalpha = 1\n# train gene feature\ntrain_gene_feature_responseCoding = np.array(get_gv_feature(alpha, \"Gene\", train_df))\n# test gene feature\ntest_gene_feature_responseCoding = np.array(get_gv_feature(alpha, \"Gene\", test_df))\n# cross validation gene feature\ncv_gene_feature_responseCoding = np.array(get_gv_feature(alpha, \"Gene\", cv_df))","f94e8bf9":"print(\"train_gene_feature_responseCoding is converted feature using respone coding method. The shape of gene feature:\", train_gene_feature_responseCoding.shape)","7717b86b":"# one-hot encoding of Gene feature.\ngene_vectorizer = CountVectorizer()\ntrain_gene_feature_onehotCoding = gene_vectorizer.fit_transform(train_df['Gene'])\ntest_gene_feature_onehotCoding = gene_vectorizer.transform(test_df['Gene'])\ncv_gene_feature_onehotCoding = gene_vectorizer.transform(cv_df['Gene'])","761545f1":"train_df['Gene'].head()","d793dba0":"gene_vectorizer.get_feature_names()","256acd29":"print(\"train_gene_feature_onehotCoding is converted feature using one-hot encoding method. The shape of gene feature:\", train_gene_feature_onehotCoding.shape)","b23a9e7f":"alpha = [10 ** x for x in range(-5, 1)] # hyperparam for SGD classifier.\n\n# read more about SGDClassifier() at http:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.linear_model.SGDClassifier.html\n# ------------------------------\n# default parameters\n# SGDClassifier(loss=\u2019hinge\u2019, penalty=\u2019l2\u2019, alpha=0.0001, l1_ratio=0.15, fit_intercept=True, max_iter=None, tol=None, \n# shuffle=True, verbose=0, epsilon=0.1, n_jobs=1, random_state=None, learning_rate=\u2019optimal\u2019, eta0=0.0, power_t=0.5, \n# class_weight=None, warm_start=False, average=False, n_iter=None)\n\n# some of methods\n# fit(X, y[, coef_init, intercept_init, \u2026])\tFit linear model with Stochastic Gradient Descent.\n# predict(X)\tPredict class labels for samples in X.\n\n#-------------------------------\n# video link: \n#------------------------------\n\n\ncv_log_error_array=[]\nfor i in alpha:\n    clf = SGDClassifier(alpha=i, penalty='l2', loss='log', random_state=42)\n    clf.fit(train_gene_feature_onehotCoding, y_train)\n    sig_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\n    sig_clf.fit(train_gene_feature_onehotCoding, y_train)\n    predict_y = sig_clf.predict_proba(cv_gene_feature_onehotCoding)\n    cv_log_error_array.append(log_loss(y_cv, predict_y, labels=clf.classes_, eps=1e-15))\n    print('For values of alpha = ', i, \"The log loss is:\",log_loss(y_cv, predict_y, labels=clf.classes_, eps=1e-15))\n\nfig, ax = plt.subplots()\nax.plot(alpha, cv_log_error_array,c='g')\nfor i, txt in enumerate(np.round(cv_log_error_array,3)):\n    ax.annotate((alpha[i],np.round(txt,3)), (alpha[i],cv_log_error_array[i]))\nplt.grid()\nplt.title(\"Cross Validation Error for each alpha\")\nplt.xlabel(\"Alpha i's\")\nplt.ylabel(\"Error measure\")\nplt.show()\n\n\nbest_alpha = np.argmin(cv_log_error_array)\nclf = SGDClassifier(alpha=alpha[best_alpha], penalty='l2', loss='log', random_state=42)\nclf.fit(train_gene_feature_onehotCoding, y_train)\nsig_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\nsig_clf.fit(train_gene_feature_onehotCoding, y_train)\n\npredict_y = sig_clf.predict_proba(train_gene_feature_onehotCoding)\nprint('For values of best alpha = ', alpha[best_alpha], \"The train log loss is:\",log_loss(y_train, predict_y, labels=clf.classes_, eps=1e-15))\npredict_y = sig_clf.predict_proba(cv_gene_feature_onehotCoding)\nprint('For values of best alpha = ', alpha[best_alpha], \"The cross validation log loss is:\",log_loss(y_cv, predict_y, labels=clf.classes_, eps=1e-15))\npredict_y = sig_clf.predict_proba(test_gene_feature_onehotCoding)\nprint('For values of best alpha = ', alpha[best_alpha], \"The test log loss is:\",log_loss(y_test, predict_y, labels=clf.classes_, eps=1e-15))","9db4769c":"print(\"Q6. How many data points in Test and CV datasets are covered by the \", unique_genes.shape[0], \" genes in train dataset?\")\n\ntest_coverage=test_df[test_df['Gene'].isin(list(set(train_df['Gene'])))].shape[0]\ncv_coverage=cv_df[cv_df['Gene'].isin(list(set(train_df['Gene'])))].shape[0]\n\nprint('Ans\\n1. In test data',test_coverage, 'out of',test_df.shape[0], \":\",(test_coverage\/test_df.shape[0])*100)\nprint('2. In cross validation data',cv_coverage, 'out of ',cv_df.shape[0],\":\" ,(cv_coverage\/cv_df.shape[0])*100)","e880e73a":"unique_variations = train_df['Variation'].value_counts()\nprint('Number of Unique Variations :', unique_variations.shape[0])\n# the top 10 variations that occured most\nprint(unique_variations.head(10))","fa4d48c1":"print(\"Ans: There are\", unique_variations.shape[0] ,\"different categories of variations in the train data, and they are distibuted as follows\",)","274803f3":"s = sum(unique_variations.values);\nh = unique_variations.values\/s;\nplt.plot(h, label=\"Histrogram of Variations\")\nplt.xlabel('Index of a Variation')\nplt.ylabel('Number of Occurances')\nplt.legend()\nplt.grid()\nplt.show()","06e15479":"c = np.cumsum(h)\nprint(c)\nplt.plot(c,label='Cumulative distribution of Variations')\nplt.grid()\nplt.legend()\nplt.show()","6882fd04":"# alpha is used for laplace smoothing\nalpha = 1\n# train gene feature\ntrain_variation_feature_responseCoding = np.array(get_gv_feature(alpha, \"Variation\", train_df))\n# test gene feature\ntest_variation_feature_responseCoding = np.array(get_gv_feature(alpha, \"Variation\", test_df))\n# cross validation gene feature\ncv_variation_feature_responseCoding = np.array(get_gv_feature(alpha, \"Variation\", cv_df))","ca38a40a":"print(\"train_variation_feature_responseCoding is a converted feature using the response coding method. The shape of Variation feature:\", train_variation_feature_responseCoding.shape)","1c05d7c4":"# one-hot encoding of variation feature.\nvariation_vectorizer = CountVectorizer()\ntrain_variation_feature_onehotCoding = variation_vectorizer.fit_transform(train_df['Variation'])\ntest_variation_feature_onehotCoding = variation_vectorizer.transform(test_df['Variation'])\ncv_variation_feature_onehotCoding = variation_vectorizer.transform(cv_df['Variation'])","3a2e5633":"print(\"train_variation_feature_onehotEncoded is converted feature using the onne-hot encoding method. The shape of Variation feature:\", train_variation_feature_onehotCoding.shape)","d329d445":"alpha = [10 ** x for x in range(-5, 1)]\n\n# read more about SGDClassifier() at http:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.linear_model.SGDClassifier.html\n# ------------------------------\n# default parameters\n# SGDClassifier(loss=\u2019hinge\u2019, penalty=\u2019l2\u2019, alpha=0.0001, l1_ratio=0.15, fit_intercept=True, max_iter=None, tol=None, \n# shuffle=True, verbose=0, epsilon=0.1, n_jobs=1, random_state=None, learning_rate=\u2019optimal\u2019, eta0=0.0, power_t=0.5, \n# class_weight=None, warm_start=False, average=False, n_iter=None)\n\n# some of methods\n# fit(X, y[, coef_init, intercept_init, \u2026])\tFit linear model with Stochastic Gradient Descent.\n# predict(X)\tPredict class labels for samples in X.\n\n#-------------------------------\n# video link: \n#------------------------------\n\n\ncv_log_error_array=[]\nfor i in alpha:\n    clf = SGDClassifier(alpha=i, penalty='l2', loss='log', random_state=42)\n    clf.fit(train_variation_feature_onehotCoding, y_train)\n    \n    sig_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\n    sig_clf.fit(train_variation_feature_onehotCoding, y_train)\n    predict_y = sig_clf.predict_proba(cv_variation_feature_onehotCoding)\n    \n    cv_log_error_array.append(log_loss(y_cv, predict_y, labels=clf.classes_, eps=1e-15))\n    print('For values of alpha = ', i, \"The log loss is:\",log_loss(y_cv, predict_y, labels=clf.classes_, eps=1e-15))\n\nfig, ax = plt.subplots()\nax.plot(alpha, cv_log_error_array,c='g')\nfor i, txt in enumerate(np.round(cv_log_error_array,3)):\n    ax.annotate((alpha[i],np.round(txt,3)), (alpha[i],cv_log_error_array[i]))\nplt.grid()\nplt.title(\"Cross Validation Error for each alpha\")\nplt.xlabel(\"Alpha i's\")\nplt.ylabel(\"Error measure\")\nplt.show()\n\n\nbest_alpha = np.argmin(cv_log_error_array)\nclf = SGDClassifier(alpha=alpha[best_alpha], penalty='l2', loss='log', random_state=42)\nclf.fit(train_variation_feature_onehotCoding, y_train)\nsig_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\nsig_clf.fit(train_variation_feature_onehotCoding, y_train)\n\npredict_y = sig_clf.predict_proba(train_variation_feature_onehotCoding)\nprint('For values of best alpha = ', alpha[best_alpha], \"The train log loss is:\",log_loss(y_train, predict_y, labels=clf.classes_, eps=1e-15))\npredict_y = sig_clf.predict_proba(cv_variation_feature_onehotCoding)\nprint('For values of best alpha = ', alpha[best_alpha], \"The cross validation log loss is:\",log_loss(y_cv, predict_y, labels=clf.classes_, eps=1e-15))\npredict_y = sig_clf.predict_proba(test_variation_feature_onehotCoding)\nprint('For values of best alpha = ', alpha[best_alpha], \"The test log loss is:\",log_loss(y_test, predict_y, labels=clf.classes_, eps=1e-15))","7fe206cd":"print(\"Q12. How many data points are covered by total \", unique_variations.shape[0], \" genes in test and cross validation data sets?\")\ntest_coverage=test_df[test_df['Variation'].isin(list(set(train_df['Variation'])))].shape[0]\ncv_coverage=cv_df[cv_df['Variation'].isin(list(set(train_df['Variation'])))].shape[0]\nprint('Ans\\n1. In test data',test_coverage, 'out of',test_df.shape[0], \":\",(test_coverage\/test_df.shape[0])*100)\nprint('2. In cross validation data',cv_coverage, 'out of ',cv_df.shape[0],\":\" ,(cv_coverage\/cv_df.shape[0])*100)","097381a9":"# cls_text is a data frame\n# for every row in data fram consider the 'TEXT'\n# split the words by space\n# make a dict with those words\n# increment its count whenever we see that word\n\ndef extract_dictionary_paddle(cls_text):\n    dictionary = defaultdict(int)\n    for index, row in cls_text.iterrows():\n        for word in row['TEXT'].split():\n            dictionary[word] +=1\n    return dictionary","72900b18":"import math\n#https:\/\/stackoverflow.com\/a\/1602964\ndef get_text_responsecoding(df):\n    text_feature_responseCoding = np.zeros((df.shape[0],9))\n    for i in range(0,9):\n        row_index = 0\n        for index, row in df.iterrows():\n            sum_prob = 0\n            for word in row['TEXT'].split():\n                sum_prob += math.log(((dict_list[i].get(word,0)+10 )\/(total_dict.get(word,0)+90)))\n            text_feature_responseCoding[row_index][i] = math.exp(sum_prob\/len(row['TEXT'].split()))\n            row_index += 1\n    return text_feature_responseCoding","3167912a":"# building a CountVectorizer with all the words that occured minimum 3 times in train data\ntext_vectorizer = CountVectorizer(min_df=3)\ntrain_text_feature_onehotCoding = text_vectorizer.fit_transform(train_df['TEXT'])\n# getting all the feature names (words)\ntrain_text_features= text_vectorizer.get_feature_names()\n\n# train_text_feature_onehotCoding.sum(axis=0).A1 will sum every row and returns (1*number of features) vector\ntrain_text_fea_counts = train_text_feature_onehotCoding.sum(axis=0).A1\n\n# zip(list(text_features),text_fea_counts) will zip a word with its number of times it occured\ntext_fea_dict = dict(zip(list(train_text_features),train_text_fea_counts))\n\n\nprint(\"Total number of unique words in train data :\", len(train_text_features))","5bf6b00c":"dict_list = []\n# dict_list =[] contains 9 dictoinaries each corresponds to a class\nfor i in range(1,10):\n    cls_text = train_df[train_df['Class']==i]\n    # build a word dict based on the words in that class\n    dict_list.append(extract_dictionary_paddle(cls_text))\n    # append it to dict_list\n\n# dict_list[i] is build on i'th  class text data\n# total_dict is buid on whole training text data\ntotal_dict = extract_dictionary_paddle(train_df)\n\n\nconfuse_array = []\nfor i in train_text_features:\n    ratios = []\n    max_val = -1\n    for j in range(0,9):\n        ratios.append((dict_list[j][i]+10 )\/(total_dict[i]+90))\n    confuse_array.append(ratios)\nconfuse_array = np.array(confuse_array)","95905532":"#response coding of text features\ntrain_text_feature_responseCoding  = get_text_responsecoding(train_df)\ntest_text_feature_responseCoding  = get_text_responsecoding(test_df)\ncv_text_feature_responseCoding  = get_text_responsecoding(cv_df)","5b595d1a":"# https:\/\/stackoverflow.com\/a\/16202486\n# we convert each row values such that they sum to 1  \ntrain_text_feature_responseCoding = (train_text_feature_responseCoding.T\/train_text_feature_responseCoding.sum(axis=1)).T\ntest_text_feature_responseCoding = (test_text_feature_responseCoding.T\/test_text_feature_responseCoding.sum(axis=1)).T\ncv_text_feature_responseCoding = (cv_text_feature_responseCoding.T\/cv_text_feature_responseCoding.sum(axis=1)).T","51366561":"# don't forget to normalize every feature\ntrain_text_feature_onehotCoding = normalize(train_text_feature_onehotCoding, axis=0)\n\n# we use the same vectorizer that was trained on train data\ntest_text_feature_onehotCoding = text_vectorizer.transform(test_df['TEXT'])\n# don't forget to normalize every feature\ntest_text_feature_onehotCoding = normalize(test_text_feature_onehotCoding, axis=0)\n\n# we use the same vectorizer that was trained on train data\ncv_text_feature_onehotCoding = text_vectorizer.transform(cv_df['TEXT'])\n# don't forget to normalize every feature\ncv_text_feature_onehotCoding = normalize(cv_text_feature_onehotCoding, axis=0)","854491a2":"#https:\/\/stackoverflow.com\/a\/2258273\/4084039\nsorted_text_fea_dict = dict(sorted(text_fea_dict.items(), key=lambda x: x[1] , reverse=True))\nsorted_text_occur = np.array(list(sorted_text_fea_dict.values()))","2a26ac0f":"# Number of words for a given frequency.\nprint(Counter(sorted_text_occur))","958e68bc":"# Train a Logistic regression+Calibration model using text features whicha re on-hot encoded\nalpha = [10 ** x for x in range(-5, 1)]\n\n# read more about SGDClassifier() at http:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.linear_model.SGDClassifier.html\n# ------------------------------\n# default parameters\n# SGDClassifier(loss=\u2019hinge\u2019, penalty=\u2019l2\u2019, alpha=0.0001, l1_ratio=0.15, fit_intercept=True, max_iter=None, tol=None, \n# shuffle=True, verbose=0, epsilon=0.1, n_jobs=1, random_state=None, learning_rate=\u2019optimal\u2019, eta0=0.0, power_t=0.5, \n# class_weight=None, warm_start=False, average=False, n_iter=None)\n\n# some of methods\n# fit(X, y[, coef_init, intercept_init, \u2026])\tFit linear model with Stochastic Gradient Descent.\n# predict(X)\tPredict class labels for samples in X.\n\n#-------------------------------\n# video link: \n#------------------------------\n\n\ncv_log_error_array=[]\nfor i in alpha:\n    clf = SGDClassifier(alpha=i, penalty='l2', loss='log', random_state=42)\n    clf.fit(train_text_feature_onehotCoding, y_train)\n    \n    sig_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\n    sig_clf.fit(train_text_feature_onehotCoding, y_train)\n    predict_y = sig_clf.predict_proba(cv_text_feature_onehotCoding)\n    cv_log_error_array.append(log_loss(y_cv, predict_y, labels=clf.classes_, eps=1e-15))\n    print('For values of alpha = ', i, \"The log loss is:\",log_loss(y_cv, predict_y, labels=clf.classes_, eps=1e-15))\n\nfig, ax = plt.subplots()\nax.plot(alpha, cv_log_error_array,c='g')\nfor i, txt in enumerate(np.round(cv_log_error_array,3)):\n    ax.annotate((alpha[i],np.round(txt,3)), (alpha[i],cv_log_error_array[i]))\nplt.grid()\nplt.title(\"Cross Validation Error for each alpha\")\nplt.xlabel(\"Alpha i's\")\nplt.ylabel(\"Error measure\")\nplt.show()\n\n\nbest_alpha = np.argmin(cv_log_error_array)\nclf = SGDClassifier(alpha=alpha[best_alpha], penalty='l2', loss='log', random_state=42)\nclf.fit(train_text_feature_onehotCoding, y_train)\nsig_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\nsig_clf.fit(train_text_feature_onehotCoding, y_train)\n\npredict_y = sig_clf.predict_proba(train_text_feature_onehotCoding)\nprint('For values of best alpha = ', alpha[best_alpha], \"The train log loss is:\",log_loss(y_train, predict_y, labels=clf.classes_, eps=1e-15))\npredict_y = sig_clf.predict_proba(cv_text_feature_onehotCoding)\nprint('For values of best alpha = ', alpha[best_alpha], \"The cross validation log loss is:\",log_loss(y_cv, predict_y, labels=clf.classes_, eps=1e-15))\npredict_y = sig_clf.predict_proba(test_text_feature_onehotCoding)\nprint('For values of best alpha = ', alpha[best_alpha], \"The test log loss is:\",log_loss(y_test, predict_y, labels=clf.classes_, eps=1e-15))","c03ea5c2":"def get_intersec_text(df):\n    df_text_vec = CountVectorizer(min_df=3)\n    df_text_fea = df_text_vec.fit_transform(df['TEXT'])\n    df_text_features = df_text_vec.get_feature_names()\n\n    df_text_fea_counts = df_text_fea.sum(axis=0).A1\n    df_text_fea_dict = dict(zip(list(df_text_features),df_text_fea_counts))\n    len1 = len(set(df_text_features))\n    len2 = len(set(train_text_features) & set(df_text_features))\n    return len1,len2","a3901695":"len1,len2 = get_intersec_text(test_df)\nprint(np.round((len2\/len1)*100, 3), \"% of word of test data appeared in train data\")\nlen1,len2 = get_intersec_text(cv_df)\nprint(np.round((len2\/len1)*100, 3), \"% of word of Cross Validation appeared in train data\")","2c142987":"#Data preparation for ML models.\n\n#Misc. functionns for ML models\n\n\ndef predict_and_plot_confusion_matrix(train_x, train_y,test_x, test_y, clf):\n    clf.fit(train_x, train_y)\n    sig_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\n    sig_clf.fit(train_x, train_y)\n    pred_y = sig_clf.predict(test_x)\n\n    # for calculating log_loss we willl provide the array of probabilities belongs to each class\n    print(\"Log loss :\",log_loss(test_y, sig_clf.predict_proba(test_x)))\n    # calculating the number of data points that are misclassified\n    print(\"Number of mis-classified points :\", np.count_nonzero((pred_y- test_y))\/test_y.shape[0])\n    plot_confusion_matrix(test_y, pred_y)","dbfdbf30":"def report_log_loss(train_x, train_y, test_x, test_y,  clf):\n    clf.fit(train_x, train_y)\n    sig_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\n    sig_clf.fit(train_x, train_y)\n    sig_clf_probs = sig_clf.predict_proba(test_x)\n    return log_loss(test_y, sig_clf_probs, eps=1e-15)","499570c9":"# this function will be used just for naive bayes\n# for the given indices, we will print the name of the features\n# and we will check whether the feature present in the test point text or not\ndef get_impfeature_names(indices, text, gene, var, no_features):\n    gene_count_vec = CountVectorizer()\n    var_count_vec = CountVectorizer()\n    text_count_vec = CountVectorizer(min_df=3)\n    \n    gene_vec = gene_count_vec.fit(train_df['Gene'])\n    var_vec  = var_count_vec.fit(train_df['Variation'])\n    text_vec = text_count_vec.fit(train_df['TEXT'])\n    \n    fea1_len = len(gene_vec.get_feature_names())\n    fea2_len = len(var_count_vec.get_feature_names())\n    \n    word_present = 0\n    for i,v in enumerate(indices):\n        if (v < fea1_len):\n            word = gene_vec.get_feature_names()[v]\n            yes_no = True if word == gene else False\n            if yes_no:\n                word_present += 1\n                print(i, \"Gene feature [{}] present in test data point [{}]\".format(word,yes_no))\n        elif (v < fea1_len+fea2_len):\n            word = var_vec.get_feature_names()[v-(fea1_len)]\n            yes_no = True if word == var else False\n            if yes_no:\n                word_present += 1\n                print(i, \"variation feature [{}] present in test data point [{}]\".format(word,yes_no))\n        else:\n            word = text_vec.get_feature_names()[v-(fea1_len+fea2_len)]\n            yes_no = True if word in text.split() else False\n            if yes_no:\n                word_present += 1\n                print(i, \"Text feature [{}] present in test data point [{}]\".format(word,yes_no))\n\n    print(\"Out of the top \",no_features,\" features \", word_present, \"are present in query point\")","acff0e9a":"# merging gene, variance and text features\n\n# building train, test and cross validation data sets\n# a = [[1, 2], \n#      [3, 4]]\n# b = [[4, 5], \n#      [6, 7]]\n# hstack(a, b) = [[1, 2, 4, 5],\n#                [ 3, 4, 6, 7]]\n\ntrain_gene_var_onehotCoding = hstack((train_gene_feature_onehotCoding,train_variation_feature_onehotCoding))\ntest_gene_var_onehotCoding = hstack((test_gene_feature_onehotCoding,test_variation_feature_onehotCoding))\ncv_gene_var_onehotCoding = hstack((cv_gene_feature_onehotCoding,cv_variation_feature_onehotCoding))\n\ntrain_x_onehotCoding = hstack((train_gene_var_onehotCoding, train_text_feature_onehotCoding)).tocsr()\ntrain_y = np.array(list(train_df['Class']))\n\ntest_x_onehotCoding = hstack((test_gene_var_onehotCoding, test_text_feature_onehotCoding)).tocsr()\ntest_y = np.array(list(test_df['Class']))\n\ncv_x_onehotCoding = hstack((cv_gene_var_onehotCoding, cv_text_feature_onehotCoding)).tocsr()\ncv_y = np.array(list(cv_df['Class']))\n\n\ntrain_gene_var_responseCoding = np.hstack((train_gene_feature_responseCoding,train_variation_feature_responseCoding))\ntest_gene_var_responseCoding = np.hstack((test_gene_feature_responseCoding,test_variation_feature_responseCoding))\ncv_gene_var_responseCoding = np.hstack((cv_gene_feature_responseCoding,cv_variation_feature_responseCoding))\n\ntrain_x_responseCoding = np.hstack((train_gene_var_responseCoding, train_text_feature_responseCoding))\ntest_x_responseCoding = np.hstack((test_gene_var_responseCoding, test_text_feature_responseCoding))\ncv_x_responseCoding = np.hstack((cv_gene_var_responseCoding, cv_text_feature_responseCoding))\n","f3087529":"print(\"One hot encoding features :\")\nprint(\"(number of data points * number of features) in train data = \", train_x_onehotCoding.shape)\nprint(\"(number of data points * number of features) in test data = \", test_x_onehotCoding.shape)\nprint(\"(number of data points * number of features) in cross validation data =\", cv_x_onehotCoding.shape)","d16d8275":"print(\" Response encoding features :\")\nprint(\"(number of data points * number of features) in train data = \", train_x_responseCoding.shape)\nprint(\"(number of data points * number of features) in test data = \", test_x_responseCoding.shape)\nprint(\"(number of data points * number of features) in cross validation data =\", cv_x_responseCoding.shape)","060b28f5":"# find more about Multinomial Naive base function here http:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.naive_bayes.MultinomialNB.html\n# -------------------------\n# default paramters\n# sklearn.naive_bayes.MultinomialNB(alpha=1.0, fit_prior=True, class_prior=None)\n\n# some of methods of MultinomialNB()\n# fit(X, y[, sample_weight])\tFit Naive Bayes classifier according to X, y\n# predict(X)\tPerform classification on an array of test vectors X.\n# predict_log_proba(X)\tReturn log-probability estimates for the test vector X.\n# -----------------------\n# video link: https:\/\/www.appliedaicourse.com\/course\/applied-ai-course-online\/lessons\/naive-bayes-algorithm-1\/\n# -----------------------\n\n\n# find more about CalibratedClassifierCV here at http:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.calibration.CalibratedClassifierCV.html\n# ----------------------------\n# default paramters\n# sklearn.calibration.CalibratedClassifierCV(base_estimator=None, method=\u2019sigmoid\u2019, cv=3)\n#\n# some of the methods of CalibratedClassifierCV()\n# fit(X, y[, sample_weight])\tFit the calibrated model\n# get_params([deep])\tGet parameters for this estimator.\n# predict(X)\tPredict the target of new samples.\n# predict_proba(X)\tPosterior probabilities of classification\n# ----------------------------\n# video link: https:\/\/www.appliedaicourse.com\/course\/applied-ai-course-online\/lessons\/naive-bayes-algorithm-1\/\n# -----------------------\n\n\nalpha = [0.00001, 0.0001, 0.001, 0.1, 1, 10, 100,1000]\ncv_log_error_array = []\nfor i in alpha:\n    print(\"for alpha =\", i)\n    clf = MultinomialNB(alpha=i)\n    clf.fit(train_x_onehotCoding, train_y)\n    sig_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\n    sig_clf.fit(train_x_onehotCoding, train_y)\n    sig_clf_probs = sig_clf.predict_proba(cv_x_onehotCoding)\n    cv_log_error_array.append(log_loss(cv_y, sig_clf_probs, labels=clf.classes_, eps=1e-15))\n    # to avoid rounding error while multiplying probabilites we use log-probability estimates\n    print(\"Log Loss :\",log_loss(cv_y, sig_clf_probs)) \n\nfig, ax = plt.subplots()\nax.plot(np.log10(alpha), cv_log_error_array,c='g')\nfor i, txt in enumerate(np.round(cv_log_error_array,3)):\n    ax.annotate((alpha[i],str(txt)), (np.log10(alpha[i]),cv_log_error_array[i]))\nplt.grid()\nplt.xticks(np.log10(alpha))\nplt.title(\"Cross Validation Error for each alpha\")\nplt.xlabel(\"Alpha i's\")\nplt.ylabel(\"Error measure\")\nplt.show()\n\n\nbest_alpha = np.argmin(cv_log_error_array)\nclf = MultinomialNB(alpha=alpha[best_alpha])\nclf.fit(train_x_onehotCoding, train_y)\nsig_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\nsig_clf.fit(train_x_onehotCoding, train_y)\n\n\npredict_y = sig_clf.predict_proba(train_x_onehotCoding)\nprint('For values of best alpha = ', alpha[best_alpha], \"The train log loss is:\",log_loss(y_train, predict_y, labels=clf.classes_, eps=1e-15))\npredict_y = sig_clf.predict_proba(cv_x_onehotCoding)\nprint('For values of best alpha = ', alpha[best_alpha], \"The cross validation log loss is:\",log_loss(y_cv, predict_y, labels=clf.classes_, eps=1e-15))\npredict_y = sig_clf.predict_proba(test_x_onehotCoding)\nprint('For values of best alpha = ', alpha[best_alpha], \"The test log loss is:\",log_loss(y_test, predict_y, labels=clf.classes_, eps=1e-15))","fcd342a0":"# find more about Multinomial Naive base function here http:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.naive_bayes.MultinomialNB.html\n# -------------------------\n# default paramters\n# sklearn.naive_bayes.MultinomialNB(alpha=1.0, fit_prior=True, class_prior=None)\n\n# some of methods of MultinomialNB()\n# fit(X, y[, sample_weight])\tFit Naive Bayes classifier according to X, y\n# predict(X)\tPerform classification on an array of test vectors X.\n# predict_log_proba(X)\tReturn log-probability estimates for the test vector X.\n# -----------------------\n# video link: https:\/\/www.appliedaicourse.com\/course\/applied-ai-course-online\/lessons\/naive-bayes-algorithm-1\/\n# -----------------------\n\n\n# find more about CalibratedClassifierCV here at http:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.calibration.CalibratedClassifierCV.html\n# ----------------------------\n# default paramters\n# sklearn.calibration.CalibratedClassifierCV(base_estimator=None, method=\u2019sigmoid\u2019, cv=3)\n#\n# some of the methods of CalibratedClassifierCV()\n# fit(X, y[, sample_weight])\tFit the calibrated model\n# get_params([deep])\tGet parameters for this estimator.\n# predict(X)\tPredict the target of new samples.\n# predict_proba(X)\tPosterior probabilities of classification\n# ----------------------------\n\nclf = MultinomialNB(alpha=alpha[best_alpha])\nclf.fit(train_x_onehotCoding, train_y)\nsig_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\nsig_clf.fit(train_x_onehotCoding, train_y)\nsig_clf_probs = sig_clf.predict_proba(cv_x_onehotCoding)\n# to avoid rounding error while multiplying probabilites we use log-probability estimates\nprint(\"Log Loss :\",log_loss(cv_y, sig_clf_probs))\nprint(\"Number of missclassified point :\", np.count_nonzero((sig_clf.predict(cv_x_onehotCoding)- cv_y))\/cv_y.shape[0])\nplot_confusion_matrix(cv_y, sig_clf.predict(cv_x_onehotCoding.toarray()))","7b4c7808":"test_point_index = 1\nno_feature = 100\npredicted_cls = sig_clf.predict(test_x_onehotCoding[test_point_index])\nprint(\"Predicted Class :\", predicted_cls[0])\nprint(\"Predicted Class Probabilities:\", np.round(sig_clf.predict_proba(test_x_onehotCoding[test_point_index]),4))\nprint(\"Actual Class :\", test_y[test_point_index])\nindices=np.argsort(abs(-clf.coef_))[predicted_cls-1][:,:no_feature]\nprint(\"-\"*50)\nget_impfeature_names(indices[0], test_df['TEXT'].iloc[test_point_index],test_df['Gene'].iloc[test_point_index],test_df['Variation'].iloc[test_point_index], no_feature)","11ca258b":"test_point_index = 100\nno_feature = 100\npredicted_cls = sig_clf.predict(test_x_onehotCoding[test_point_index])\nprint(\"Predicted Class :\", predicted_cls[0])\nprint(\"Predicted Class Probabilities:\", np.round(sig_clf.predict_proba(test_x_onehotCoding[test_point_index]),4))\nprint(\"Actual Class :\", test_y[test_point_index])\nindices = np.argsort(abs(-clf.coef_))[predicted_cls-1][:,:no_feature]\nprint(\"-\"*50)\nget_impfeature_names(indices[0], test_df['TEXT'].iloc[test_point_index],test_df['Gene'].iloc[test_point_index],test_df['Variation'].iloc[test_point_index], no_feature)","53a52ada":"# find more about KNeighborsClassifier() here http:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.neighbors.KNeighborsClassifier.html\n# -------------------------\n# default parameter\n# KNeighborsClassifier(n_neighbors=5, weights=\u2019uniform\u2019, algorithm=\u2019auto\u2019, leaf_size=30, p=2, \n# metric=\u2019minkowski\u2019, metric_params=None, n_jobs=1, **kwargs)\n\n# methods of\n# fit(X, y) : Fit the model using X as training data and y as target values\n# predict(X):Predict the class labels for the provided data\n# predict_proba(X):Return probability estimates for the test data X.\n#-------------------------------------\n# video link: https:\/\/www.appliedaicourse.com\/course\/applied-ai-course-online\/lessons\/k-nearest-neighbors-geometric-intuition-with-a-toy-example-1\/\n#-------------------------------------\n\n\n# find more about CalibratedClassifierCV here at http:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.calibration.CalibratedClassifierCV.html\n# ----------------------------\n# default paramters\n# sklearn.calibration.CalibratedClassifierCV(base_estimator=None, method=\u2019sigmoid\u2019, cv=3)\n#\n# some of the methods of CalibratedClassifierCV()\n# fit(X, y[, sample_weight])\tFit the calibrated model\n# get_params([deep])\tGet parameters for this estimator.\n# predict(X)\tPredict the target of new samples.\n# predict_proba(X)\tPosterior probabilities of classification\n#-------------------------------------\n# video link:\n#-------------------------------------\n\n\nalpha = [5, 11, 15, 21, 31, 41, 51, 99]\ncv_log_error_array = []\nfor i in alpha:\n    print(\"for alpha =\", i)\n    clf = KNeighborsClassifier(n_neighbors=i)\n    clf.fit(train_x_responseCoding, train_y)\n    sig_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\n    sig_clf.fit(train_x_responseCoding, train_y)\n    sig_clf_probs = sig_clf.predict_proba(cv_x_responseCoding)\n    cv_log_error_array.append(log_loss(cv_y, sig_clf_probs, labels=clf.classes_, eps=1e-15))\n    # to avoid rounding error while multiplying probabilites we use log-probability estimates\n    print(\"Log Loss :\",log_loss(cv_y, sig_clf_probs)) \n\nfig, ax = plt.subplots()\nax.plot(alpha, cv_log_error_array,c='g')\nfor i, txt in enumerate(np.round(cv_log_error_array,3)):\n    ax.annotate((alpha[i],str(txt)), (alpha[i],cv_log_error_array[i]))\nplt.grid()\nplt.title(\"Cross Validation Error for each alpha\")\nplt.xlabel(\"Alpha i's\")\nplt.ylabel(\"Error measure\")\nplt.show()\n\n\nbest_alpha = np.argmin(cv_log_error_array)\nclf = KNeighborsClassifier(n_neighbors=alpha[best_alpha])\nclf.fit(train_x_responseCoding, train_y)\nsig_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\nsig_clf.fit(train_x_responseCoding, train_y)\n\npredict_y = sig_clf.predict_proba(train_x_responseCoding)\nprint('For values of best alpha = ', alpha[best_alpha], \"The train log loss is:\",log_loss(y_train, predict_y, labels=clf.classes_, eps=1e-15))\npredict_y = sig_clf.predict_proba(cv_x_responseCoding)\nprint('For values of best alpha = ', alpha[best_alpha], \"The cross validation log loss is:\",log_loss(y_cv, predict_y, labels=clf.classes_, eps=1e-15))\npredict_y = sig_clf.predict_proba(test_x_responseCoding)\nprint('For values of best alpha = ', alpha[best_alpha], \"The test log loss is:\",log_loss(y_test, predict_y, labels=clf.classes_, eps=1e-15))\n","6512af57":"# find more about KNeighborsClassifier() here http:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.neighbors.KNeighborsClassifier.html\n# -------------------------\n# default parameter\n# KNeighborsClassifier(n_neighbors=5, weights=\u2019uniform\u2019, algorithm=\u2019auto\u2019, leaf_size=30, p=2, \n# metric=\u2019minkowski\u2019, metric_params=None, n_jobs=1, **kwargs)\n\n# methods of\n# fit(X, y) : Fit the model using X as training data and y as target values\n# predict(X):Predict the class labels for the provided data\n# predict_proba(X):Return probability estimates for the test data X.\n#-------------------------------------\n# video link: https:\/\/www.appliedaicourse.com\/course\/applied-ai-course-online\/lessons\/k-nearest-neighbors-geometric-intuition-with-a-toy-example-1\/\n#-------------------------------------\nclf = KNeighborsClassifier(n_neighbors=alpha[best_alpha])\npredict_and_plot_confusion_matrix(train_x_responseCoding, train_y, cv_x_responseCoding, cv_y, clf)","d4bed8c4":"clf = KNeighborsClassifier(n_neighbors=alpha[best_alpha])\nclf.fit(train_x_responseCoding, train_y)\nsig_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\nsig_clf.fit(train_x_responseCoding, train_y)\n\ntest_point_index = 1\npredicted_cls = sig_clf.predict(test_x_responseCoding[0].reshape(1,-1))\nprint(\"Predicted Class :\", predicted_cls[0])\nprint(\"Actual Class :\", test_y[test_point_index])\nneighbors = clf.kneighbors(test_x_responseCoding[test_point_index].reshape(1, -1), alpha[best_alpha])\nprint(\"The \",alpha[best_alpha],\" nearest neighbours of the test points belongs to classes\",train_y[neighbors[1][0]])\nprint(\"Fequency of nearest points :\",Counter(train_y[neighbors[1][0]]))","1568fd8d":"clf = KNeighborsClassifier(n_neighbors=alpha[best_alpha])\nclf.fit(train_x_responseCoding, train_y)\nsig_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\nsig_clf.fit(train_x_responseCoding, train_y)\n\ntest_point_index = 100\n\npredicted_cls = sig_clf.predict(test_x_responseCoding[test_point_index].reshape(1,-1))\nprint(\"Predicted Class :\", predicted_cls[0])\nprint(\"Actual Class :\", test_y[test_point_index])\nneighbors = clf.kneighbors(test_x_responseCoding[test_point_index].reshape(1, -1), alpha[best_alpha])\nprint(\"the k value for knn is\",alpha[best_alpha],\"and the nearest neighbours of the test points belongs to classes\",train_y[neighbors[1][0]])\nprint(\"Fequency of nearest points :\",Counter(train_y[neighbors[1][0]]))","2a0f5efc":"\n# read more about SGDClassifier() at http:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.linear_model.SGDClassifier.html\n# ------------------------------\n# default parameters\n# SGDClassifier(loss=\u2019hinge\u2019, penalty=\u2019l2\u2019, alpha=0.0001, l1_ratio=0.15, fit_intercept=True, max_iter=None, tol=None, \n# shuffle=True, verbose=0, epsilon=0.1, n_jobs=1, random_state=None, learning_rate=\u2019optimal\u2019, eta0=0.0, power_t=0.5, \n# class_weight=None, warm_start=False, average=False, n_iter=None)\n\n# some of methods\n# fit(X, y[, coef_init, intercept_init, \u2026])\tFit linear model with Stochastic Gradient Descent.\n# predict(X)\tPredict class labels for samples in X.\n\n#-------------------------------\n# video link: https:\/\/www.appliedaicourse.com\/course\/applied-ai-course-online\/lessons\/geometric-intuition-1\/\n#------------------------------\n\n\n# find more about CalibratedClassifierCV here at http:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.calibration.CalibratedClassifierCV.html\n# ----------------------------\n# default paramters\n# sklearn.calibration.CalibratedClassifierCV(base_estimator=None, method=\u2019sigmoid\u2019, cv=3)\n#\n# some of the methods of CalibratedClassifierCV()\n# fit(X, y[, sample_weight])\tFit the calibrated model\n# get_params([deep])\tGet parameters for this estimator.\n# predict(X)\tPredict the target of new samples.\n# predict_proba(X)\tPosterior probabilities of classification\n#-------------------------------------\n# video link:\n#-------------------------------------\n\nalpha = [10 ** x for x in range(-6, 3)]\ncv_log_error_array = []\nfor i in alpha:\n    print(\"for alpha =\", i)\n    clf = SGDClassifier(class_weight='balanced', alpha=i, penalty='l2', loss='log', random_state=42)\n    clf.fit(train_x_onehotCoding, train_y)\n    sig_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\n    sig_clf.fit(train_x_onehotCoding, train_y)\n    sig_clf_probs = sig_clf.predict_proba(cv_x_onehotCoding)\n    cv_log_error_array.append(log_loss(cv_y, sig_clf_probs, labels=clf.classes_, eps=1e-15))\n    # to avoid rounding error while multiplying probabilites we use log-probability estimates\n    print(\"Log Loss :\",log_loss(cv_y, sig_clf_probs)) \n\nfig, ax = plt.subplots()\nax.plot(alpha, cv_log_error_array,c='g')\nfor i, txt in enumerate(np.round(cv_log_error_array,3)):\n    ax.annotate((alpha[i],str(txt)), (alpha[i],cv_log_error_array[i]))\nplt.grid()\nplt.title(\"Cross Validation Error for each alpha\")\nplt.xlabel(\"Alpha i's\")\nplt.ylabel(\"Error measure\")\nplt.show()\n\n\nbest_alpha = np.argmin(cv_log_error_array)\nclf = SGDClassifier(class_weight='balanced', alpha=alpha[best_alpha], penalty='l2', loss='log', random_state=42)\nclf.fit(train_x_onehotCoding, train_y)\nsig_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\nsig_clf.fit(train_x_onehotCoding, train_y)\n\npredict_y = sig_clf.predict_proba(train_x_onehotCoding)\nprint('For values of best alpha = ', alpha[best_alpha], \"The train log loss is:\",log_loss(y_train, predict_y, labels=clf.classes_, eps=1e-15))\npredict_y = sig_clf.predict_proba(cv_x_onehotCoding)\nprint('For values of best alpha = ', alpha[best_alpha], \"The cross validation log loss is:\",log_loss(y_cv, predict_y, labels=clf.classes_, eps=1e-15))\npredict_y = sig_clf.predict_proba(test_x_onehotCoding)\nprint('For values of best alpha = ', alpha[best_alpha], \"The test log loss is:\",log_loss(y_test, predict_y, labels=clf.classes_, eps=1e-15))","faf8160d":"# read more about SGDClassifier() at http:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.linear_model.SGDClassifier.html\n# ------------------------------\n# default parameters\n# SGDClassifier(loss=\u2019hinge\u2019, penalty=\u2019l2\u2019, alpha=0.0001, l1_ratio=0.15, fit_intercept=True, max_iter=None, tol=None, \n# shuffle=True, verbose=0, epsilon=0.1, n_jobs=1, random_state=None, learning_rate=\u2019optimal\u2019, eta0=0.0, power_t=0.5, \n# class_weight=None, warm_start=False, average=False, n_iter=None)\n\n# some of methods\n# fit(X, y[, coef_init, intercept_init, \u2026])\tFit linear model with Stochastic Gradient Descent.\n# predict(X)\tPredict class labels for samples in X.\n\n#-------------------------------\n# video link: https:\/\/www.appliedaicourse.com\/course\/applied-ai-course-online\/lessons\/geometric-intuition-1\/\n#------------------------------\nclf = SGDClassifier(class_weight='balanced', alpha=alpha[best_alpha], penalty='l2', loss='log', random_state=42)\npredict_and_plot_confusion_matrix(train_x_onehotCoding, train_y, cv_x_onehotCoding, cv_y, clf)","e452f501":"def get_imp_feature_names(text, indices, removed_ind = []):\n    word_present = 0\n    tabulte_list = []\n    incresingorder_ind = 0\n    for i in indices:\n        if i < train_gene_feature_onehotCoding.shape[1]:\n            tabulte_list.append([incresingorder_ind, \"Gene\", \"Yes\"])\n        elif i< 18:\n            tabulte_list.append([incresingorder_ind,\"Variation\", \"Yes\"])\n        if ((i > 17) & (i not in removed_ind)) :\n            word = train_text_features[i]\n            yes_no = True if word in text.split() else False\n            if yes_no:\n                word_present += 1\n            tabulte_list.append([incresingorder_ind,train_text_features[i], yes_no])\n        incresingorder_ind += 1\n    print(word_present, \"most importent features are present in our query point\")\n    print(\"-\"*50)\n    print(\"The features that are most importent of the \",predicted_cls[0],\" class:\")\n    print (tabulate(tabulte_list, headers=[\"Index\",'Feature name', 'Present or Not']))","81d0ced2":"# from tabulate import tabulate\nclf = SGDClassifier(class_weight='balanced', alpha=alpha[best_alpha], penalty='l2', loss='log', random_state=42)\nclf.fit(train_x_onehotCoding,train_y)\ntest_point_index = 1\nno_feature = 500\npredicted_cls = sig_clf.predict(test_x_onehotCoding[test_point_index])\nprint(\"Predicted Class :\", predicted_cls[0])\nprint(\"Predicted Class Probabilities:\", np.round(sig_clf.predict_proba(test_x_onehotCoding[test_point_index]),4))\nprint(\"Actual Class :\", test_y[test_point_index])\nindices = np.argsort(abs(-clf.coef_))[predicted_cls-1][:,:no_feature]\nprint(\"-\"*50)\nget_impfeature_names(indices[0], test_df['TEXT'].iloc[test_point_index],test_df['Gene'].iloc[test_point_index],test_df['Variation'].iloc[test_point_index], no_feature)","29dcf799":"test_point_index = 100\nno_feature = 500\npredicted_cls = sig_clf.predict(test_x_onehotCoding[test_point_index])\nprint(\"Predicted Class :\", predicted_cls[0])\nprint(\"Predicted Class Probabilities:\", np.round(sig_clf.predict_proba(test_x_onehotCoding[test_point_index]),4))\nprint(\"Actual Class :\", test_y[test_point_index])\nindices = np.argsort(abs(-clf.coef_))[predicted_cls-1][:,:no_feature]\nprint(\"-\"*50)\nget_impfeature_names(indices[0], test_df['TEXT'].iloc[test_point_index],test_df['Gene'].iloc[test_point_index],test_df['Variation'].iloc[test_point_index], no_feature)","c1d3c353":"# read more about SGDClassifier() at http:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.linear_model.SGDClassifier.html\n# ------------------------------\n# default parameters\n# SGDClassifier(loss=\u2019hinge\u2019, penalty=\u2019l2\u2019, alpha=0.0001, l1_ratio=0.15, fit_intercept=True, max_iter=None, tol=None, \n# shuffle=True, verbose=0, epsilon=0.1, n_jobs=1, random_state=None, learning_rate=\u2019optimal\u2019, eta0=0.0, power_t=0.5, \n# class_weight=None, warm_start=False, average=False, n_iter=None)\n\n# some of methods\n# fit(X, y[, coef_init, intercept_init, \u2026])\tFit linear model with Stochastic Gradient Descent.\n# predict(X)\tPredict class labels for samples in X.\n\n#-------------------------------\n# video link: https:\/\/www.appliedaicourse.com\/course\/applied-ai-course-online\/lessons\/geometric-intuition-1\/\n#------------------------------\n\n\n\n# find more about CalibratedClassifierCV here at http:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.calibration.CalibratedClassifierCV.html\n# ----------------------------\n# default paramters\n# sklearn.calibration.CalibratedClassifierCV(base_estimator=None, method=\u2019sigmoid\u2019, cv=3)\n#\n# some of the methods of CalibratedClassifierCV()\n# fit(X, y[, sample_weight])\tFit the calibrated model\n# get_params([deep])\tGet parameters for this estimator.\n# predict(X)\tPredict the target of new samples.\n# predict_proba(X)\tPosterior probabilities of classification\n#-------------------------------------\n# video link:\n#-------------------------------------\n\nalpha = [10 ** x for x in range(-6, 1)]\ncv_log_error_array = []\nfor i in alpha:\n    print(\"for alpha =\", i)\n    clf = SGDClassifier(alpha=i, penalty='l2', loss='log', random_state=42)\n    clf.fit(train_x_onehotCoding, train_y)\n    sig_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\n    sig_clf.fit(train_x_onehotCoding, train_y)\n    sig_clf_probs = sig_clf.predict_proba(cv_x_onehotCoding)\n    cv_log_error_array.append(log_loss(cv_y, sig_clf_probs, labels=clf.classes_, eps=1e-15))\n    print(\"Log Loss :\",log_loss(cv_y, sig_clf_probs)) \n\nfig, ax = plt.subplots()\nax.plot(alpha, cv_log_error_array,c='g')\nfor i, txt in enumerate(np.round(cv_log_error_array,3)):\n    ax.annotate((alpha[i],str(txt)), (alpha[i],cv_log_error_array[i]))\nplt.grid()\nplt.title(\"Cross Validation Error for each alpha\")\nplt.xlabel(\"Alpha i's\")\nplt.ylabel(\"Error measure\")\nplt.show()\n\n\nbest_alpha = np.argmin(cv_log_error_array)\nclf = SGDClassifier(alpha=alpha[best_alpha], penalty='l2', loss='log', random_state=42)\nclf.fit(train_x_onehotCoding, train_y)\nsig_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\nsig_clf.fit(train_x_onehotCoding, train_y)\n\npredict_y = sig_clf.predict_proba(train_x_onehotCoding)\nprint('For values of best alpha = ', alpha[best_alpha], \"The train log loss is:\",log_loss(y_train, predict_y, labels=clf.classes_, eps=1e-15))\npredict_y = sig_clf.predict_proba(cv_x_onehotCoding)\nprint('For values of best alpha = ', alpha[best_alpha], \"The cross validation log loss is:\",log_loss(y_cv, predict_y, labels=clf.classes_, eps=1e-15))\npredict_y = sig_clf.predict_proba(test_x_onehotCoding)\nprint('For values of best alpha = ', alpha[best_alpha], \"The test log loss is:\",log_loss(y_test, predict_y, labels=clf.classes_, eps=1e-15))","84423257":"# read more about SGDClassifier() at http:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.linear_model.SGDClassifier.html\n# ------------------------------\n# default parameters\n# SGDClassifier(loss=\u2019hinge\u2019, penalty=\u2019l2\u2019, alpha=0.0001, l1_ratio=0.15, fit_intercept=True, max_iter=None, tol=None, \n# shuffle=True, verbose=0, epsilon=0.1, n_jobs=1, random_state=None, learning_rate=\u2019optimal\u2019, eta0=0.0, power_t=0.5, \n# class_weight=None, warm_start=False, average=False, n_iter=None)\n\n# some of methods\n# fit(X, y[, coef_init, intercept_init, \u2026])\tFit linear model with Stochastic Gradient Descent.\n# predict(X)\tPredict class labels for samples in X.\n\n#-------------------------------\n# video link: \n#------------------------------\n\nclf = SGDClassifier(alpha=alpha[best_alpha], penalty='l2', loss='log', random_state=42)\npredict_and_plot_confusion_matrix(train_x_onehotCoding, train_y, cv_x_onehotCoding, cv_y, clf)","2a827c57":"clf = SGDClassifier(alpha=alpha[best_alpha], penalty='l2', loss='log', random_state=42)\nclf.fit(train_x_onehotCoding,train_y)\ntest_point_index = 1\nno_feature = 500\npredicted_cls = sig_clf.predict(test_x_onehotCoding[test_point_index])\nprint(\"Predicted Class :\", predicted_cls[0])\nprint(\"Predicted Class Probabilities:\", np.round(sig_clf.predict_proba(test_x_onehotCoding[test_point_index]),4))\nprint(\"Actual Class :\", test_y[test_point_index])\nindices = np.argsort(abs(-clf.coef_))[predicted_cls-1][:,:no_feature]\nprint(\"-\"*50)\nget_impfeature_names(indices[0], test_df['TEXT'].iloc[test_point_index],test_df['Gene'].iloc[test_point_index],test_df['Variation'].iloc[test_point_index], no_feature)","001d3824":"test_point_index = 100\nno_feature = 500\npredicted_cls = sig_clf.predict(test_x_onehotCoding[test_point_index])\nprint(\"Predicted Class :\", predicted_cls[0])\nprint(\"Predicted Class Probabilities:\", np.round(sig_clf.predict_proba(test_x_onehotCoding[test_point_index]),4))\nprint(\"Actual Class :\", test_y[test_point_index])\nindices = np.argsort(abs(-clf.coef_))[predicted_cls-1][:,:no_feature]\nprint(\"-\"*50)\nget_impfeature_names(indices[0], test_df['TEXT'].iloc[test_point_index],test_df['Gene'].iloc[test_point_index],test_df['Variation'].iloc[test_point_index], no_feature)","74b5980f":"# read more about support vector machines with linear kernals here http:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.svm.SVC.html\n\n# --------------------------------\n# default parameters \n# SVC(C=1.0, kernel=\u2019rbf\u2019, degree=3, gamma=\u2019auto\u2019, coef0=0.0, shrinking=True, probability=False, tol=0.001, \n# cache_size=200, class_weight=None, verbose=False, max_iter=-1, decision_function_shape=\u2019ovr\u2019, random_state=None)\n\n# Some of methods of SVM()\n# fit(X, y, [sample_weight])\tFit the SVM model according to the given training data.\n# predict(X)\tPerform classification on samples in X.\n# --------------------------------\n# video link: https:\/\/www.appliedaicourse.com\/course\/applied-ai-course-online\/lessons\/mathematical-derivation-copy-8\/\n# --------------------------------\n\n\n\n# find more about CalibratedClassifierCV here at http:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.calibration.CalibratedClassifierCV.html\n# ----------------------------\n# default paramters\n# sklearn.calibration.CalibratedClassifierCV(base_estimator=None, method=\u2019sigmoid\u2019, cv=3)\n#\n# some of the methods of CalibratedClassifierCV()\n# fit(X, y[, sample_weight])\tFit the calibrated model\n# get_params([deep])\tGet parameters for this estimator.\n# predict(X)\tPredict the target of new samples.\n# predict_proba(X)\tPosterior probabilities of classification\n#-------------------------------------\n# video link:\n#-------------------------------------\n\nalpha = [10 ** x for x in range(-5, 3)]\ncv_log_error_array = []\nfor i in alpha:\n    print(\"for C =\", i)\n#     clf = SVC(C=i,kernel='linear',probability=True, class_weight='balanced')\n    clf = SGDClassifier( class_weight='balanced', alpha=i, penalty='l2', loss='hinge', random_state=42)\n    clf.fit(train_x_onehotCoding, train_y)\n    sig_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\n    sig_clf.fit(train_x_onehotCoding, train_y)\n    sig_clf_probs = sig_clf.predict_proba(cv_x_onehotCoding)\n    cv_log_error_array.append(log_loss(cv_y, sig_clf_probs, labels=clf.classes_, eps=1e-15))\n    print(\"Log Loss :\",log_loss(cv_y, sig_clf_probs)) \n\nfig, ax = plt.subplots()\nax.plot(alpha, cv_log_error_array,c='g')\nfor i, txt in enumerate(np.round(cv_log_error_array,3)):\n    ax.annotate((alpha[i],str(txt)), (alpha[i],cv_log_error_array[i]))\nplt.grid()\nplt.title(\"Cross Validation Error for each alpha\")\nplt.xlabel(\"Alpha i's\")\nplt.ylabel(\"Error measure\")\nplt.show()\n\n\nbest_alpha = np.argmin(cv_log_error_array)\n# clf = SVC(C=i,kernel='linear',probability=True, class_weight='balanced')\nclf = SGDClassifier(class_weight='balanced', alpha=alpha[best_alpha], penalty='l2', loss='hinge', random_state=42)\nclf.fit(train_x_onehotCoding, train_y)\nsig_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\nsig_clf.fit(train_x_onehotCoding, train_y)\n\npredict_y = sig_clf.predict_proba(train_x_onehotCoding)\nprint('For values of best alpha = ', alpha[best_alpha], \"The train log loss is:\",log_loss(y_train, predict_y, labels=clf.classes_, eps=1e-15))\npredict_y = sig_clf.predict_proba(cv_x_onehotCoding)\nprint('For values of best alpha = ', alpha[best_alpha], \"The cross validation log loss is:\",log_loss(y_cv, predict_y, labels=clf.classes_, eps=1e-15))\npredict_y = sig_clf.predict_proba(test_x_onehotCoding)\nprint('For values of best alpha = ', alpha[best_alpha], \"The test log loss is:\",log_loss(y_test, predict_y, labels=clf.classes_, eps=1e-15))","e03085b0":"# read more about support vector machines with linear kernals here http:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.svm.SVC.html\n\n# --------------------------------\n# default parameters \n# SVC(C=1.0, kernel=\u2019rbf\u2019, degree=3, gamma=\u2019auto\u2019, coef0=0.0, shrinking=True, probability=False, tol=0.001, \n# cache_size=200, class_weight=None, verbose=False, max_iter=-1, decision_function_shape=\u2019ovr\u2019, random_state=None)\n\n# Some of methods of SVM()\n# fit(X, y, [sample_weight])\tFit the SVM model according to the given training data.\n# predict(X)\tPerform classification on samples in X.\n# --------------------------------\n# video link: https:\/\/www.appliedaicourse.com\/course\/applied-ai-course-online\/lessons\/mathematical-derivation-copy-8\/\n# --------------------------------\n\n\n# clf = SVC(C=alpha[best_alpha],kernel='linear',probability=True, class_weight='balanced')\nclf = SGDClassifier(alpha=alpha[best_alpha], penalty='l2', loss='hinge', random_state=42,class_weight='balanced')\npredict_and_plot_confusion_matrix(train_x_onehotCoding, train_y,cv_x_onehotCoding,cv_y, clf)","0acf024c":"clf = SGDClassifier(alpha=alpha[best_alpha], penalty='l2', loss='hinge', random_state=42)\nclf.fit(train_x_onehotCoding,train_y)\ntest_point_index = 1\n# test_point_index = 100\nno_feature = 500\npredicted_cls = sig_clf.predict(test_x_onehotCoding[test_point_index])\nprint(\"Predicted Class :\", predicted_cls[0])\nprint(\"Predicted Class Probabilities:\", np.round(sig_clf.predict_proba(test_x_onehotCoding[test_point_index]),4))\nprint(\"Actual Class :\", test_y[test_point_index])\nindices = np.argsort(abs(-clf.coef_))[predicted_cls-1][:,:no_feature]\nprint(\"-\"*50)\nget_impfeature_names(indices[0], test_df['TEXT'].iloc[test_point_index],test_df['Gene'].iloc[test_point_index],test_df['Variation'].iloc[test_point_index], no_feature)","ba8cbec5":"test_point_index = 100\nno_feature = 500\npredicted_cls = sig_clf.predict(test_x_onehotCoding[test_point_index])\nprint(\"Predicted Class :\", predicted_cls[0])\nprint(\"Predicted Class Probabilities:\", np.round(sig_clf.predict_proba(test_x_onehotCoding[test_point_index]),4))\nprint(\"Actual Class :\", test_y[test_point_index])\nindices = np.argsort(abs(-clf.coef_))[predicted_cls-1][:,:no_feature]\nprint(\"-\"*50)\nget_impfeature_names(indices[0], test_df['TEXT'].iloc[test_point_index],test_df['Gene'].iloc[test_point_index],test_df['Variation'].iloc[test_point_index], no_feature)","67076ecf":"# --------------------------------\n# default parameters \n# sklearn.ensemble.RandomForestClassifier(n_estimators=10, criterion=\u2019gini\u2019, max_depth=None, min_samples_split=2, \n# min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features=\u2019auto\u2019, max_leaf_nodes=None, min_impurity_decrease=0.0, \n# min_impurity_split=None, bootstrap=True, oob_score=False, n_jobs=1, random_state=None, verbose=0, warm_start=False, \n# class_weight=None)\n\n# Some of methods of RandomForestClassifier()\n# fit(X, y, [sample_weight])\tFit the SVM model according to the given training data.\n# predict(X)\tPerform classification on samples in X.\n# predict_proba (X)\tPerform classification on samples in X.\n\n# some of attributes of  RandomForestClassifier()\n# feature_importances_ : array of shape = [n_features]\n# The feature importances (the higher, the more important the feature).\n\n# --------------------------------\n# video link: https:\/\/www.appliedaicourse.com\/course\/applied-ai-course-online\/lessons\/random-forest-and-their-construction-2\/\n# --------------------------------\n\n\n# find more about CalibratedClassifierCV here at http:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.calibration.CalibratedClassifierCV.html\n# ----------------------------\n# default paramters\n# sklearn.calibration.CalibratedClassifierCV(base_estimator=None, method=\u2019sigmoid\u2019, cv=3)\n#\n# some of the methods of CalibratedClassifierCV()\n# fit(X, y[, sample_weight])\tFit the calibrated model\n# get_params([deep])\tGet parameters for this estimator.\n# predict(X)\tPredict the target of new samples.\n# predict_proba(X)\tPosterior probabilities of classification\n#-------------------------------------\n# video link:\n#-------------------------------------\n\nalpha = [100,200,500,1000,2000]\nmax_depth = [5, 10]\ncv_log_error_array = []\nfor i in alpha:\n    for j in max_depth:\n        print(\"for n_estimators =\", i,\"and max depth = \", j)\n        clf = RandomForestClassifier(n_estimators=i, criterion='gini', max_depth=j, random_state=42, n_jobs=-1)\n        clf.fit(train_x_onehotCoding, train_y)\n        sig_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\n        sig_clf.fit(train_x_onehotCoding, train_y)\n        sig_clf_probs = sig_clf.predict_proba(cv_x_onehotCoding)\n        cv_log_error_array.append(log_loss(cv_y, sig_clf_probs, labels=clf.classes_, eps=1e-15))\n        print(\"Log Loss :\",log_loss(cv_y, sig_clf_probs)) \n\n'''fig, ax = plt.subplots()\nfeatures = np.dot(np.array(alpha)[:,None],np.array(max_depth)[None]).ravel()\nax.plot(features, cv_log_error_array,c='g')\nfor i, txt in enumerate(np.round(cv_log_error_array,3)):\n    ax.annotate((alpha[int(i\/2)],max_depth[int(i%2)],str(txt)), (features[i],cv_log_error_array[i]))\nplt.grid()\nplt.title(\"Cross Validation Error for each alpha\")\nplt.xlabel(\"Alpha i's\")\nplt.ylabel(\"Error measure\")\nplt.show()\n'''\n\nbest_alpha = np.argmin(cv_log_error_array)\nclf = RandomForestClassifier(n_estimators=alpha[int(best_alpha\/2)], criterion='gini', max_depth=max_depth[int(best_alpha%2)], random_state=42, n_jobs=-1)\nclf.fit(train_x_onehotCoding, train_y)\nsig_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\nsig_clf.fit(train_x_onehotCoding, train_y)\n\npredict_y = sig_clf.predict_proba(train_x_onehotCoding)\nprint('For values of best estimator = ', alpha[int(best_alpha\/2)], \"The train log loss is:\",log_loss(y_train, predict_y, labels=clf.classes_, eps=1e-15))\npredict_y = sig_clf.predict_proba(cv_x_onehotCoding)\nprint('For values of best estimator = ', alpha[int(best_alpha\/2)], \"The cross validation log loss is:\",log_loss(y_cv, predict_y, labels=clf.classes_, eps=1e-15))\npredict_y = sig_clf.predict_proba(test_x_onehotCoding)\nprint('For values of best estimator = ', alpha[int(best_alpha\/2)], \"The test log loss is:\",log_loss(y_test, predict_y, labels=clf.classes_, eps=1e-15))","d1d46a0d":"# --------------------------------\n# default parameters \n# sklearn.ensemble.RandomForestClassifier(n_estimators=10, criterion=\u2019gini\u2019, max_depth=None, min_samples_split=2, \n# min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features=\u2019auto\u2019, max_leaf_nodes=None, min_impurity_decrease=0.0, \n# min_impurity_split=None, bootstrap=True, oob_score=False, n_jobs=1, random_state=None, verbose=0, warm_start=False, \n# class_weight=None)\n\n# Some of methods of RandomForestClassifier()\n# fit(X, y, [sample_weight])\tFit the SVM model according to the given training data.\n# predict(X)\tPerform classification on samples in X.\n# predict_proba (X)\tPerform classification on samples in X.\n\n# some of attributes of  RandomForestClassifier()\n# feature_importances_ : array of shape = [n_features]\n# The feature importances (the higher, the more important the feature).\n\n# --------------------------------\n# video link: https:\/\/www.appliedaicourse.com\/course\/applied-ai-course-online\/lessons\/random-forest-and-their-construction-2\/\n# --------------------------------\n\nclf = RandomForestClassifier(n_estimators=alpha[int(best_alpha\/2)], criterion='gini', max_depth=max_depth[int(best_alpha%2)], random_state=42, n_jobs=-1)\npredict_and_plot_confusion_matrix(train_x_onehotCoding, train_y,cv_x_onehotCoding,cv_y, clf)","2343bbb9":"# test_point_index = 10\nclf = RandomForestClassifier(n_estimators=alpha[int(best_alpha\/2)], criterion='gini', max_depth=max_depth[int(best_alpha%2)], random_state=42, n_jobs=-1)\nclf.fit(train_x_onehotCoding, train_y)\nsig_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\nsig_clf.fit(train_x_onehotCoding, train_y)\n\ntest_point_index = 1\nno_feature = 100\npredicted_cls = sig_clf.predict(test_x_onehotCoding[test_point_index])\nprint(\"Predicted Class :\", predicted_cls[0])\nprint(\"Predicted Class Probabilities:\", np.round(sig_clf.predict_proba(test_x_onehotCoding[test_point_index]),4))\nprint(\"Actual Class :\", test_y[test_point_index])\nindices = np.argsort(-clf.feature_importances_)\nprint(\"-\"*50)\nget_impfeature_names(indices[:no_feature], test_df['TEXT'].iloc[test_point_index],test_df['Gene'].iloc[test_point_index],test_df['Variation'].iloc[test_point_index], no_feature)","00626286":"test_point_index = 100\nno_feature = 100\npredicted_cls = sig_clf.predict(test_x_onehotCoding[test_point_index])\nprint(\"Predicted Class :\", predicted_cls[0])\nprint(\"Predicted Class Probabilities:\", np.round(sig_clf.predict_proba(test_x_onehotCoding[test_point_index]),4))\nprint(\"Actuall Class :\", test_y[test_point_index])\nindices = np.argsort(-clf.feature_importances_)\nprint(\"-\"*50)\nget_impfeature_names(indices[:no_feature], test_df['TEXT'].iloc[test_point_index],test_df['Gene'].iloc[test_point_index],test_df['Variation'].iloc[test_point_index], no_feature)","ef4fde69":"# --------------------------------\n# default parameters \n# sklearn.ensemble.RandomForestClassifier(n_estimators=10, criterion=\u2019gini\u2019, max_depth=None, min_samples_split=2, \n# min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features=\u2019auto\u2019, max_leaf_nodes=None, min_impurity_decrease=0.0, \n# min_impurity_split=None, bootstrap=True, oob_score=False, n_jobs=1, random_state=None, verbose=0, warm_start=False, \n# class_weight=None)\n\n# Some of methods of RandomForestClassifier()\n# fit(X, y, [sample_weight])\tFit the SVM model according to the given training data.\n# predict(X)\tPerform classification on samples in X.\n# predict_proba (X)\tPerform classification on samples in X.\n\n# some of attributes of  RandomForestClassifier()\n# feature_importances_ : array of shape = [n_features]\n# The feature importances (the higher, the more important the feature).\n\n# --------------------------------\n# video link: https:\/\/www.appliedaicourse.com\/course\/applied-ai-course-online\/lessons\/random-forest-and-their-construction-2\/\n# --------------------------------\n\n\n# find more about CalibratedClassifierCV here at http:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.calibration.CalibratedClassifierCV.html\n# ----------------------------\n# default paramters\n# sklearn.calibration.CalibratedClassifierCV(base_estimator=None, method=\u2019sigmoid\u2019, cv=3)\n#\n# some of the methods of CalibratedClassifierCV()\n# fit(X, y[, sample_weight])\tFit the calibrated model\n# get_params([deep])\tGet parameters for this estimator.\n# predict(X)\tPredict the target of new samples.\n# predict_proba(X)\tPosterior probabilities of classification\n#-------------------------------------\n# video link:\n#-------------------------------------\n\nalpha = [10,50,100,200,500,1000]\nmax_depth = [2,3,5,10]\ncv_log_error_array = []\nfor i in alpha:\n    for j in max_depth:\n        print(\"for n_estimators =\", i,\"and max depth = \", j)\n        clf = RandomForestClassifier(n_estimators=i, criterion='gini', max_depth=j, random_state=42, n_jobs=-1)\n        clf.fit(train_x_responseCoding, train_y)\n        sig_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\n        sig_clf.fit(train_x_responseCoding, train_y)\n        sig_clf_probs = sig_clf.predict_proba(cv_x_responseCoding)\n        cv_log_error_array.append(log_loss(cv_y, sig_clf_probs, labels=clf.classes_, eps=1e-15))\n        print(\"Log Loss :\",log_loss(cv_y, sig_clf_probs)) \n'''\nfig, ax = plt.subplots()\nfeatures = np.dot(np.array(alpha)[:,None],np.array(max_depth)[None]).ravel()\nax.plot(features, cv_log_error_array,c='g')\nfor i, txt in enumerate(np.round(cv_log_error_array,3)):\n    ax.annotate((alpha[int(i\/4)],max_depth[int(i%4)],str(txt)), (features[i],cv_log_error_array[i]))\nplt.grid()\nplt.title(\"Cross Validation Error for each alpha\")\nplt.xlabel(\"Alpha i's\")\nplt.ylabel(\"Error measure\")\nplt.show()\n'''\n\nbest_alpha = np.argmin(cv_log_error_array)\nclf = RandomForestClassifier(n_estimators=alpha[int(best_alpha\/4)], criterion='gini', max_depth=max_depth[int(best_alpha%4)], random_state=42, n_jobs=-1)\nclf.fit(train_x_responseCoding, train_y)\nsig_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\nsig_clf.fit(train_x_responseCoding, train_y)\n\npredict_y = sig_clf.predict_proba(train_x_responseCoding)\nprint('For values of best alpha = ', alpha[int(best_alpha\/4)], \"The train log loss is:\",log_loss(y_train, predict_y, labels=clf.classes_, eps=1e-15))\npredict_y = sig_clf.predict_proba(cv_x_responseCoding)\nprint('For values of best alpha = ', alpha[int(best_alpha\/4)], \"The cross validation log loss is:\",log_loss(y_cv, predict_y, labels=clf.classes_, eps=1e-15))\npredict_y = sig_clf.predict_proba(test_x_responseCoding)\nprint('For values of best alpha = ', alpha[int(best_alpha\/4)], \"The test log loss is:\",log_loss(y_test, predict_y, labels=clf.classes_, eps=1e-15))","52417737":"# --------------------------------\n# default parameters \n# sklearn.ensemble.RandomForestClassifier(n_estimators=10, criterion=\u2019gini\u2019, max_depth=None, min_samples_split=2, \n# min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features=\u2019auto\u2019, max_leaf_nodes=None, min_impurity_decrease=0.0, \n# min_impurity_split=None, bootstrap=True, oob_score=False, n_jobs=1, random_state=None, verbose=0, warm_start=False, \n# class_weight=None)\n\n# Some of methods of RandomForestClassifier()\n# fit(X, y, [sample_weight])\tFit the SVM model according to the given training data.\n# predict(X)\tPerform classification on samples in X.\n# predict_proba (X)\tPerform classification on samples in X.\n\n# some of attributes of  RandomForestClassifier()\n# feature_importances_ : array of shape = [n_features]\n# The feature importances (the higher, the more important the feature).\n\n# --------------------------------\n# video link: https:\/\/www.appliedaicourse.com\/course\/applied-ai-course-online\/lessons\/random-forest-and-their-construction-2\/\n# --------------------------------\n\nclf = RandomForestClassifier(max_depth=max_depth[int(best_alpha%4)], n_estimators=alpha[int(best_alpha\/4)], criterion='gini', max_features='auto',random_state=42)\npredict_and_plot_confusion_matrix(train_x_responseCoding, train_y,cv_x_responseCoding,cv_y, clf)","18ee4857":"clf = RandomForestClassifier(n_estimators=alpha[int(best_alpha\/4)], criterion='gini', max_depth=max_depth[int(best_alpha%4)], random_state=42, n_jobs=-1)\nclf.fit(train_x_responseCoding, train_y)\nsig_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\nsig_clf.fit(train_x_responseCoding, train_y)\n\n\ntest_point_index = 1\nno_feature = 27\npredicted_cls = sig_clf.predict(test_x_responseCoding[test_point_index].reshape(1,-1))\nprint(\"Predicted Class :\", predicted_cls[0])\nprint(\"Predicted Class Probabilities:\", np.round(sig_clf.predict_proba(test_x_responseCoding[test_point_index].reshape(1,-1)),4))\nprint(\"Actual Class :\", test_y[test_point_index])\nindices = np.argsort(-clf.feature_importances_)\nprint(\"-\"*50)\nfor i in indices:\n    if i<9:\n        print(\"Gene is important feature\")\n    elif i<18:\n        print(\"Variation is important feature\")\n    else:\n        print(\"Text is important feature\")","237a3a31":"test_point_index = 100\npredicted_cls = sig_clf.predict(test_x_responseCoding[test_point_index].reshape(1,-1))\nprint(\"Predicted Class :\", predicted_cls[0])\nprint(\"Predicted Class Probabilities:\", np.round(sig_clf.predict_proba(test_x_responseCoding[test_point_index].reshape(1,-1)),4))\nprint(\"Actual Class :\", test_y[test_point_index])\nindices = np.argsort(-clf.feature_importances_)\nprint(\"-\"*50)\nfor i in indices:\n    if i<9:\n        print(\"Gene is important feature\")\n    elif i<18:\n        print(\"Variation is important feature\")\n    else:\n        print(\"Text is important feature\")","edf3dda7":"# read more about SGDClassifier() at http:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.linear_model.SGDClassifier.html\n# ------------------------------\n# default parameters\n# SGDClassifier(loss=\u2019hinge\u2019, penalty=\u2019l2\u2019, alpha=0.0001, l1_ratio=0.15, fit_intercept=True, max_iter=None, tol=None, \n# shuffle=True, verbose=0, epsilon=0.1, n_jobs=1, random_state=None, learning_rate=\u2019optimal\u2019, eta0=0.0, power_t=0.5, \n# class_weight=None, warm_start=False, average=False, n_iter=None)\n\n# some of methods\n# fit(X, y[, coef_init, intercept_init, \u2026])\tFit linear model with Stochastic Gradient Descent.\n# predict(X)\tPredict class labels for samples in X.\n\n#-------------------------------\n# video link: https:\/\/www.appliedaicourse.com\/course\/applied-ai-course-online\/lessons\/geometric-intuition-1\/\n#------------------------------\n\n\n# read more about support vector machines with linear kernals here http:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.svm.SVC.html\n# --------------------------------\n# default parameters \n# SVC(C=1.0, kernel=\u2019rbf\u2019, degree=3, gamma=\u2019auto\u2019, coef0=0.0, shrinking=True, probability=False, tol=0.001, \n# cache_size=200, class_weight=None, verbose=False, max_iter=-1, decision_function_shape=\u2019ovr\u2019, random_state=None)\n\n# Some of methods of SVM()\n# fit(X, y, [sample_weight])\tFit the SVM model according to the given training data.\n# predict(X)\tPerform classification on samples in X.\n# --------------------------------\n# video link: https:\/\/www.appliedaicourse.com\/course\/applied-ai-course-online\/lessons\/mathematical-derivation-copy-8\/\n# --------------------------------\n\n\n# read more about support vector machines with linear kernals here http:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.ensemble.RandomForestClassifier.html\n# --------------------------------\n# default parameters \n# sklearn.ensemble.RandomForestClassifier(n_estimators=10, criterion=\u2019gini\u2019, max_depth=None, min_samples_split=2, \n# min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features=\u2019auto\u2019, max_leaf_nodes=None, min_impurity_decrease=0.0, \n# min_impurity_split=None, bootstrap=True, oob_score=False, n_jobs=1, random_state=None, verbose=0, warm_start=False, \n# class_weight=None)\n\n# Some of methods of RandomForestClassifier()\n# fit(X, y, [sample_weight])\tFit the SVM model according to the given training data.\n# predict(X)\tPerform classification on samples in X.\n# predict_proba (X)\tPerform classification on samples in X.\n\n# some of attributes of  RandomForestClassifier()\n# feature_importances_ : array of shape = [n_features]\n# The feature importances (the higher, the more important the feature).\n\n# --------------------------------\n# video link: https:\/\/www.appliedaicourse.com\/course\/applied-ai-course-online\/lessons\/random-forest-and-their-construction-2\/\n# --------------------------------\n\n\nclf1 = SGDClassifier(alpha=0.001, penalty='l2', loss='log', class_weight='balanced', random_state=0)\nclf1.fit(train_x_onehotCoding, train_y)\nsig_clf1 = CalibratedClassifierCV(clf1, method=\"sigmoid\")\n\nclf2 = SGDClassifier(alpha=1, penalty='l2', loss='hinge', class_weight='balanced', random_state=0)\nclf2.fit(train_x_onehotCoding, train_y)\nsig_clf2 = CalibratedClassifierCV(clf2, method=\"sigmoid\")\n\n\nclf3 = MultinomialNB(alpha=0.001)\nclf3.fit(train_x_onehotCoding, train_y)\nsig_clf3 = CalibratedClassifierCV(clf3, method=\"sigmoid\")\n\nsig_clf1.fit(train_x_onehotCoding, train_y)\nprint(\"Logistic Regression :  Log Loss: %0.2f\" % (log_loss(cv_y, sig_clf1.predict_proba(cv_x_onehotCoding))))\nsig_clf2.fit(train_x_onehotCoding, train_y)\nprint(\"Support vector machines : Log Loss: %0.2f\" % (log_loss(cv_y, sig_clf2.predict_proba(cv_x_onehotCoding))))\nsig_clf3.fit(train_x_onehotCoding, train_y)\nprint(\"Naive Bayes : Log Loss: %0.2f\" % (log_loss(cv_y, sig_clf3.predict_proba(cv_x_onehotCoding))))\nprint(\"-\"*50)\nalpha = [0.0001,0.001,0.01,0.1,1,10] \nbest_alpha = 999\nfor i in alpha:\n    lr = LogisticRegression(C=i)\n    sclf = StackingClassifier(classifiers=[sig_clf1, sig_clf2, sig_clf3], meta_classifier=lr, use_probas=True)\n    sclf.fit(train_x_onehotCoding, train_y)\n    print(\"Stacking Classifer : for the value of alpha: %f Log Loss: %0.3f\" % (i, log_loss(cv_y, sclf.predict_proba(cv_x_onehotCoding))))\n    log_error =log_loss(cv_y, sclf.predict_proba(cv_x_onehotCoding))\n    if best_alpha > log_error:\n        best_alpha = log_error","eecb04f1":"lr = LogisticRegression(C=0.1)\nsclf = StackingClassifier(classifiers=[sig_clf1, sig_clf2, sig_clf3], meta_classifier=lr, use_probas=True)\nsclf.fit(train_x_onehotCoding, train_y)\n\nlog_error = log_loss(train_y, sclf.predict_proba(train_x_onehotCoding))\nprint(\"Log loss (train) on the stacking classifier :\",log_error)\n\nlog_error = log_loss(cv_y, sclf.predict_proba(cv_x_onehotCoding))\nprint(\"Log loss (CV) on the stacking classifier :\",log_error)\n\nlog_error = log_loss(test_y, sclf.predict_proba(test_x_onehotCoding))\nprint(\"Log loss (test) on the stacking classifier :\",log_error)\n\nprint(\"Number of missclassified point :\", np.count_nonzero((sclf.predict(test_x_onehotCoding)- test_y))\/test_y.shape[0])\nplot_confusion_matrix(test_y=test_y, predict_y=sclf.predict(test_x_onehotCoding))","1eb81564":"#Refer:http:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.ensemble.VotingClassifier.html\nfrom sklearn.ensemble import VotingClassifier\nvclf = VotingClassifier(estimators=[('lr', sig_clf1), ('svc', sig_clf2), ('rf', sig_clf3)], voting='soft')\nvclf.fit(train_x_onehotCoding, train_y)\nprint(\"Log loss (train) on the VotingClassifier :\", log_loss(train_y, vclf.predict_proba(train_x_onehotCoding)))\nprint(\"Log loss (CV) on the VotingClassifier :\", log_loss(cv_y, vclf.predict_proba(cv_x_onehotCoding)))\nprint(\"Log loss (test) on the VotingClassifier :\", log_loss(test_y, vclf.predict_proba(test_x_onehotCoding)))\nprint(\"Number of missclassified point :\", np.count_nonzero((vclf.predict(test_x_onehotCoding)- test_y))\/test_y.shape[0])\nplot_confusion_matrix(test_y=test_y, predict_y=vclf.predict(test_x_onehotCoding))","6cccc574":"<h3>Feature Importance<\/h3>","490325cf":"<h5>Correctly Classified point<\/h5>","0266a12c":"### Merging two datasets together based on ID","c0af9fb1":"<h1>Machine Learning Models<\/h1>","d4fcdb37":"<p style=\"font-size:18px;\"> <b>Q11.<\/b> Is the Variation feature stable across all the data sets (Test, Train, Cross validation)?<\/p>\n<p style=\"font-size:16px;\"> <b>Ans.<\/b> Not sure! But lets be very sure using the below analysis. <\/p>","51068c9e":"## Distribution of y_i's in Train, Test and Cross Validation datasets","6e42d9e9":"<h2>Logistic Regression<\/h2>","a035fd2f":"<h2>Stack the models <\/h2>","27ab5c69":"<h2>Random Forest Classifier<\/h2>","376ff032":"<h4>Hyper paramter tuning<\/h4>","2c9d6cd4":"<h4>Hyper paramter tuning<\/h4>","8423c178":"<h2>Base Line Model<\/h2>","3820cb5b":"<h3>Testing model with best hyper parameters<\/h3>","9584a40f":"<h3>Feature Importance<\/h3>","dad1c285":"<h3>Sample Query point -1<\/h3>","1d3c2a65":"<h3>Feature Importance<\/h3>","866f1d8d":"<h3>Univariate Analysis on Variation Feature<\/h3>","be109e28":"<h4>Incorrectly Classified point<\/h4>","16116f58":"<p style=\"font-size:18px;\"> <b>Q7.<\/b> Variation, What type of feature is it ?<\/p>\n<p style=\"font-size:16px;\"><b>Ans.<\/b> Variation is a categorical variable <\/p>\n<p style=\"font-size:18px;\"> <b>Q8.<\/b> How many categories are there?<\/p>","9377d806":"### Handling Missing data","a0fdc1d7":"<h3>Testing model with best hyper parameters (One Hot Encoding)<\/h3>","998e76cb":"<h4>Feature Importance, Correctly classified point<\/h4>","aad0b851":"<h4>Correctly Classified point<\/h4>","5107579a":"## Importing Modules","5e28bedf":"<h4>Hyper parameter tuning<\/h4>","1e9a0f06":"<h3>Univariate Analysis on Gene Feature<\/h3>","23bf0e72":"<h3>Testing the model with the best hyper parameters<\/h3>","3aed7501":"<h4>Feature Importance, Correctly Classified point<\/h4>","50937870":"![](http:\/\/)<h3>Sample Query Point-2 <\/h3>","25f65b3c":"<h3>Hyper parameter tuning<\/h3>","f2ad2766":"## Prediction using a 'Random' Model","567fcc24":"<h3>Hyper paramter tuning<\/h3>","9846d7bc":"<h3>Univariate Analysis on Text Feature<\/h3>","7111ed79":"<h4>Incorrectly Classified point<\/h4>","5112d7a9":"<h3>Hyper paramter tuning (With Response Coding)<\/h3>","7eda3d03":"<p style=\"font-size:18px;\"> <b>Q.<\/b> Is the Text feature stable across all the data sets (Test, Train, Cross validation)?<\/p>\n<p style=\"font-size:16px;\"> <b>Ans.<\/b> Yes, it seems like! <\/p>","1702e688":"## Preprocessing of text","88e255ef":"1. How many unique words are present in train data?\n2. How are word frequencies distributed?\n3. How to featurize text field?\n4. Is the text feature useful in predicitng y_i?\n5. Is the text feature stable across train, test and CV datasets?","6f044bc9":"<h4>Correctly Classified point<\/h4>","39630b2b":"when we caculate the probability of a feature belongs to any particular class, we apply laplace smoothing\n<li>(numerator + 10\\*alpha) \/ (denominator + 90\\*alpha) <\/li>","3dca5ab4":"<h3>With Class balancing<\/h3>","997d6892":"<h4>Testing model with best hyper parameters<\/h4>","d92dab75":"<h3>Hyper paramter tuning (With One hot Encoding)<\/h3>","d6ce124e":"<h3>Testing with hyper parameter tuning<\/h3>","9f7ba978":"<p style=\"font-size:18px;\"> <b>Q3.<\/b> How to featurize this Gene feature ?<\/p>\n\n<p style=\"font-size:16px;\"><b>Ans.<\/b>there are two ways we can featurize this variable\ncheck out this video: https:\/\/www.appliedaicourse.com\/course\/applied-ai-course-online\/lessons\/handling-categorical-and-numerical-features\/\n<ol><li>One hot Encoding<\/li><li>Response coding<\/li><\/ol><\/p>\n<p> We will choose the appropriate featurization based on the ML model we use.  For this problem of multi-class classification with categorical features, one-hot encoding is better for Logistic regression while response coding is better for Random Forests. <\/p>","78d1dcff":"<p style=\"font-size:18px;\"> <b>Q9.<\/b> How to featurize this Variation feature ?<\/p>\n\n<p style=\"font-size:16px;\"><b>Ans.<\/b>There are two ways we can featurize this variable\ncheck out this video: https:\/\/www.appliedaicourse.com\/course\/applied-ai-course-online\/lessons\/handling-categorical-and-numerical-features\/\n<ol><li>One hot Encoding<\/li><li>Response coding<\/li><\/ol><\/p>\n<p> We will be using both these methods to featurize the Variation Feature <\/p>","c42e7ca6":"<h3>Testing the model with best hyper paramters<\/h3>","c98194de":"## Reading Gene and Variance Data","cf570f0c":"<h3>Maximum Voting classifier <\/h3>","f2ae22dc":"<h3>Naive Bayes<\/h3>","0883519f":"<p style=\"font-size:24px;text-align:Center\"> <b>Stacking the three types of features <\/b><p>","e69081b5":"<h4>Feature Importance<\/h4>","81f03ae5":"<h2>Linear Support Vector Machines<\/h2>","366f1fa5":"<h4>Feature Importance, Inorrectly Classified point<\/h4>","7f0c5bf0":"<p style=\"font-size:18px;\"> <b>Q5.<\/b> Is the Gene feature stable across all the data sets (Test, Train, Cross validation)?<\/p>\n<p style=\"font-size:16px;\"> <b>Ans.<\/b> Yes, it is. Otherwise, the CV and Test errors would be significantly more than train error. <\/p>","2bf01b48":"<h5>Incorrectly Classified point<\/h5>","1a2ecf43":"## Reading Text Data","25dfc9d6":"<h4>Feature Importance, Incorrectly classified point<\/h4>","70b1468e":"<p style=\"font-size:18px;\"> <b>Q10.<\/b> How good is this Variation feature  in predicting y_i?<\/p>\nLet's build a model just like the earlier!","8f79a905":"## Listing data","c015a7ff":"Fields are\n\n* ID : the id of the row used to link the mutation to the clinical evidence\n* Gene : the gene where this genetic mutation is located\n* Variation : the aminoacid change for this mutations\n* Class : 1-9 the class this genetic mutation has been classified on","2138017d":"<h2>Univariate Analysis<\/h2>","99ff6a58":"<p style=\"font-size:18px;\"> <b>Q4.<\/b> How good is this gene feature  in predicting y_i?<\/p>","95ba46db":"<h4>For Correctly classified point<\/h4>","a654b0fa":"## Test, Train and Cross Validation Split","5b25a9db":"<h2>K Nearest Neighbour Classification<\/h2>","3e89e10d":"There are many ways to estimate how good a feature is, in predicting y_i. One of the good methods is to build a proper ML model using just this feature. In this case, we will build a logistic regression model using only Gene feature (one hot encoded) to predict y_i.","0b7c809f":"<p style=\"font-size:18px;\"> <b>Q1.<\/b> Gene, What type of feature it is ?<\/p>\n<p style=\"font-size:16px;\"><b>Ans.<\/b> Gene is a categorical variable <\/p>\n<p style=\"font-size:18px;\"> <b>Q2.<\/b> How many categories are there and How they are distributed?<\/p>","4b97798c":"<h4>For Incorrectly classified point<\/h4>","2332af04":"<h3>Testing model with best hyper parameters (Response Coding)<\/h3>","b4442ce0":"<h4>Testing the model with best hyper paramters<\/h4>","90cf9e69":"Splitting data into train, test and cross validation (64:20:16)","0adb42e8":"<h3>Without Class balancing<\/h3>"}}