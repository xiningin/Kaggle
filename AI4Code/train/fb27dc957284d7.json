{"cell_type":{"2d840a00":"code","0df15eda":"code","ca3ee866":"code","d2f7520e":"code","c482f1b3":"code","32760594":"code","7e718251":"code","52deaf6a":"code","6bbbe79b":"markdown","f4ffc074":"markdown","c0a27eb2":"markdown","8203da05":"markdown","5b379291":"markdown","47600d44":"markdown","63ead995":"markdown","51573bd7":"markdown","ba64af4e":"markdown","83e679f4":"markdown"},"source":{"2d840a00":"import numpy as np\nimport matplotlib.pyplot as plt","0df15eda":"def batch_gradient_descent(X,Y,learning_rate,iterations):\n    \n    W = np.array([0.0 , 0.0])\n    T = np.array([0.0 , 0.0])\n    J = np.array([0.0] * iterations)\n    \n    # Gradient Descent Algorithm\n    \n    for i in range(iterations):\n        H = np.array(W[0] + W[1]*X)\n        T[0] = W[0] - learning_rate * ((1\/len(X))*np.sum(np.subtract(H,Y)))\n        T[1] = W[1] - learning_rate * ((1\/len(X))*np.sum(np.multiply(np.subtract(H,Y),X)))\n        W = T\n        J[i] = (1\/(2*len(X)))*np.sum(np.square(H - Y))\n    \n    # Plotting Learning Curves\n    \n    print('                  ================== Learning Curve ==================')\n    plt.plot(J)\n    plt.title(\"Learning Curve\")\n    plt.xlabel(\"No. of iterations\")\n    plt.ylabel(\"Cost Function\")\n    plt.show()\n    \n    # Plotting Regression line\n    \n    print('                 ================== Regression Plot ==================')\n    Y_pred = np.add(X*W[1] , W[0])\n    plt.plot(X,Y,'rs')\n    plt.plot(X,Y_pred,'b')\n    plt.title(\"Regression Plot\")\n    plt.xlabel(\"feature value\")\n    plt.ylabel(\"target value\")\n    plt.show()\n    return W","ca3ee866":"def stochastic_gradient_descent(X,Y,learning_rate,iterations):\n    \n    W = np.array([0.0 , 0.0])\n    T = np.array([0.0 , 0.0])\n    J = np.array([0] * iterations)\n    \n    # Stochastic Gradient Descent Algorithm\n    \n    for j in range(iterations):\n        for i in range(len(X)):\n            H = np.array(W[0] + W[1]*X)\n            T[0] = W[0] - learning_rate * (H[i] - Y[i])\n            T[1] = W[1] - learning_rate * (H[i] - Y[i]) * X[i]\n            W = T\n        J[j] = (1\/(2*len(X)))*np.sum(np.square(H - Y))\n    \n    # Plotting Learning Curves\n    \n    print('                  ================== Learning Curve ==================')\n    plt.plot(J)\n    plt.title(\"Learning Curve\")\n    plt.xlabel(\"No. of iterations\")\n    plt.ylabel(\"Cost Function\")\n    plt.show()\n    \n    # Plotting Regression line\n    \n    print('                 ================== Regression Plot ==================')\n    Y_pred = np.add(X*W[1] , W[0])\n    plt.plot(X,Y,'rs')\n    plt.plot(X,Y_pred,'b')\n    plt.title(\"Regression Plot\")\n    plt.xlabel(\"feature value\")\n    plt.ylabel(\"target value\")\n    plt.show()\n    return W","d2f7520e":"def mini_batch_gradient_descent(X,Y,learning_rate,iterations,batch_size):\n    \n    W = np.array([0.0 , 0.0])\n    T = np.array([0.0 , 0.0])\n    J = np.array([0] * iterations)\n    \n    # Mini-Batch Gradient Descent Algorithm\n    \n    for j in range(int(len(X)\/batch_size)):\n        X_ = X[j*batch_size:j*batch_size + batch_size]\n        Y_ = Y[j*batch_size:j*batch_size + batch_size]\n        for i in range(iterations):\n            H = np.array(W[0] + W[1]*X_)\n            T[0] = W[0] - learning_rate * ((1\/len(X_))*np.sum(np.subtract(H,Y_)))\n            T[1] = W[1] - learning_rate * ((1\/len(X_))*np.sum(np.multiply(np.subtract(H,Y_),X_)))\n            W = T\n            J[i] = (1\/(2*len(X)))*np.sum(np.square(H - Y_))\n    \n    # Plotting Learning Curves\n    \n    print('                  ================== Learning Curve ==================')\n    plt.plot(J)\n    plt.title(\"Learning Curve\")\n    plt.xlabel(\"No. of iterations\")\n    plt.ylabel(\"Cost Function\")\n    plt.show()\n    \n    # Plotting Regression line\n    \n    print('                 ================== Regression Plot ==================')\n    Y_pred = np.add(X*W[1] , W[0])\n    plt.plot(X,Y,'rs')\n    plt.plot(X,Y_pred,'b')\n    plt.title(\"Regression Plot\")\n    plt.xlabel(\"feature value\")\n    plt.ylabel(\"target value\")\n    plt.show()\n    return W","c482f1b3":"X = np.array([1,2,3,4,5])\nY = np.array([14,27,44,55,63])","32760594":"batch_gradient_descent(X,Y,0.01,90)","7e718251":"stochastic_gradient_descent(X,Y,0.023,60)","52deaf6a":"mini_batch_gradient_descent(X,Y,0.05,60,3)","6bbbe79b":"## Stochastic Gradient Descent","f4ffc074":"### Stochastic Gradient Descent","c0a27eb2":"## Evaluation of Algorithms","8203da05":"## Batch Gradient Descent","5b379291":"### Mini-Batch Gradient Descent","47600d44":"## Mini-Batch Gradient Descent","63ead995":"### Data","51573bd7":"# Function Definitions","ba64af4e":"### Batch Gradient Descent","83e679f4":"# Batch Gradient Descent , Stochastic Gradient Descent & Mini-Batch Gradient Descent ( Without using in-built Functions) \n\n"}}