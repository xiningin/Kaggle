{"cell_type":{"8049df12":"code","e2ff3908":"code","3363dfdc":"code","617a10b6":"code","e3530d77":"code","20bfe7e6":"code","fe79c014":"code","ef2c1bdb":"code","28696af1":"code","df983a18":"code","550299fb":"code","7b72a140":"code","506f9f7c":"code","46b72b35":"code","d1cd9df6":"code","162fde06":"code","991b6e54":"code","137a5a8c":"code","ba1169ee":"code","a1d7d793":"code","c38828af":"markdown","e84e90f7":"markdown","a5a04fb7":"markdown","b1ba284e":"markdown","c0135657":"markdown","61a18ced":"markdown","e8d50a0f":"markdown","c198cf83":"markdown","ddbf4fdc":"markdown","d9712f20":"markdown"},"source":{"8049df12":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom numpy import zeros\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nimport random\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\nfor dirname, _, filenames in os.walk('\/kaggle\/working'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.\n\nimport matplotlib.pyplot as plt     \nimport seaborn as sns  # visualization tool\n\nfrom tqdm import tqdm \n\nimport nltk, re\n#nltk.download('stopwords') # load english stopwords\n#nltk.download('wordnet')\nfrom nltk.corpus import stopwords\nSTOPWORDS = set(stopwords.words('english'))\n\nfrom nltk.tokenize import TweetTokenizer\nfrom nltk.corpus import wordnet as wn\nfrom nltk.tokenize import word_tokenize\n\nfrom collections import Counter\nfrom itertools import chain\nimport pickle\n\nimport warnings\nwarnings.simplefilter(\"ignore\")\nwarnings.warn(\"deprecated\", DeprecationWarning)\nwarnings.simplefilter(\"ignore\")\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report, confusion_matrix\nfrom sklearn.utils import resample\nfrom sklearn.metrics import accuracy_score, roc_auc_score\n\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing import sequence\nfrom keras.preprocessing.sequence import pad_sequences\n\nfrom keras.layers import Input, Dense, Embedding, Dropout, LSTM, Bidirectional, Flatten, CuDNNLSTM, Conv1D\nfrom keras.models import Model, Sequential\nfrom keras.optimizers import Adam\nfrom keras.constraints import unit_norm, max_norm\nfrom keras.layers import Input, Dense, Embedding, SpatialDropout1D, add, concatenate, BatchNormalization\nfrom keras.layers import LSTM, Bidirectional, GlobalMaxPooling1D, GlobalAveragePooling1D\nfrom keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n\nimport tensorflow as tf","e2ff3908":"def seed_everything(seed=1234):\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    random.seed(seed)\n    np.random.seed(seed)\n    tf.random.set_seed(seed)\n\nseed_everything()","3363dfdc":"PATH = '\/kaggle\/input\/nlp-getting-started\/'\n\ntrain = pd.read_csv(PATH + 'train.csv')\ntest = pd.read_csv(PATH + 'test.csv')","617a10b6":"## globals\n\nREPLACE_BY_SPACE_RE = re.compile('[\/(){}\\[\\]\\|@,;-_]')\nBAD_SYMBOLS_RE = re.compile('[!\"#$%&()*+,.\/:;<=>?@[\\\\]^`{|}~\\t\\n\u201c\u201d\u2019\\'\u221e\u03b8\u00f7\u03b1\u2022\u00e0\u2212\u03b2\u2205\u00b3\u03c0\u2018\u20b9\u00b4\u00b0\u00a3\u20ac\\\u00d7\u2122\u221a\u00b2\u2014]')\nUNKNOWN = 'UNK'\nURL = 'URL'\n\n#Emoji patterns\nemoji_pattern = re.compile(\"[\"\n         u\"\\U0001F600-\\U0001F64F\"  # emoticons\n         u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n         u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n         u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n         u\"\\U00002702-\\U000027B0\"\n         u\"\\U000024C2-\\U0001F251\"\n         \"]+\", flags=re.UNICODE)","e3530d77":"def clean_tweet(tweet):\n\n    if pd.isnull(tweet):\n        return ''\n\n    tweet = tweet.lower()\n    tweet = tweet.replace('...', ' ... ').strip()\n    tweet = tweet.replace(\"'\", \" ' \").strip()\n    tweet = tweet.replace('%20', ' ').strip() \n\n    tweet = re.sub(r'https?:\/\/\\S+|www\\.\\S+', URL, tweet, flags=re.MULTILINE)\n    tweet = re.sub(REPLACE_BY_SPACE_RE,' ',tweet)\n    tweet = re.sub(BAD_SYMBOLS_RE,'',tweet)\n    tweet = re.sub(r'\\d+',' ',tweet)\n    tweet = re.sub(r'\\s+',' ',tweet)\n    tweet = re.sub(r'<.*?>', '', tweet)\n    tweet = emoji_pattern.sub(r'',tweet)\n    \n    # delete stopwords from text\n    tweet = ' '.join(w for w in nltk.wordpunct_tokenize(tweet))\n    tweet = ' '.join([i for i in tweet.split() if i not in STOPWORDS])\n    return tweet\n\ntrain['cleaned_text'] = train['text'].apply(clean_tweet)\ntest['cleaned_text'] = test['text'].apply(clean_tweet)","20bfe7e6":"train.head(10)","fe79c014":"## from: https:\/\/www.kaggle.com\/shahules\/basic-eda-cleaning-and-glove\n\n# WordNet lexical database for lemmatization\nfrom nltk.stem import WordNetLemmatizer\nwordnet_lemmatizer = WordNetLemmatizer()\n\ndef create_corpus(df):\n    corpus=[]\n    tokenizer = TweetTokenizer()\n    for tweet in tqdm(df['cleaned_text']):\n        words = [word.lower() for word in tokenizer.tokenize(tweet) if((word.isalpha()==1))]\n        words = [wordnet_lemmatizer.lemmatize(word) for word in words] \n        corpus.append(words)\n    return corpus\n\nfull = pd.concat([train,test])\ncorpus = create_corpus(full)","ef2c1bdb":"MAX_LEN = 50\n\ntokenizer = Tokenizer()\ntokenizer.fit_on_texts(corpus)\n\nX_train = tokenizer.texts_to_sequences(train['cleaned_text'])\nX_train = pad_sequences(X_train, maxlen=MAX_LEN, truncating='post', padding='post')    \ny_train = train['target']\n\nX_test = tokenizer.texts_to_sequences(test['cleaned_text'])\nX_test = pad_sequences(X_test, maxlen=MAX_LEN, truncating='post', padding='post')    \n\nword_index = tokenizer.word_index\nprint('Number of unique words:',len(word_index))","28696af1":"with open('..\/input\/\/glove-6b-50d-txt-glove-6b-50d-pkl\/glove.6B.50d.pkl', 'rb') as fp:\n#with open('..\/input\/pickled-glove840b300d-for-10sec-loading\/glove.840B.300d.pkl', 'rb') as fp:\n    glove = pickle.load(fp)\n    \nnum_words = len(word_index)+1\nembedding_matrix = np.zeros((num_words, MAX_LEN))\n\nfor word,i in tqdm(word_index.items()):\n    if i > num_words:\n        continue\n    \n    emb_vec = glove.get(word)\n    if emb_vec is not None:\n        embedding_matrix[i]=emb_vec\n","df983a18":"## Principal component analysis (PCA)\n\n## Linear dimensionality reduction using Singular Value Decomposition of the data to project it to a lower dimensional space. \n## The input data is centered but not scaled for each feature before applying the SVD.\nfrom sklearn.decomposition import PCA\n\nX_pca_train = PCA(n_components=2).fit_transform(X_train)\nX_pca_test = PCA(n_components=2).fit_transform(X_test)\n\nf, (ax1, ax2) = plt.subplots(nrows = 1, ncols = 2, figsize=(15, 6))\nax1.scatter(X_pca_train[:,0],X_pca_train[:,1],c=train['target'],cmap='rainbow')\nax2.scatter(X_pca_test[:,0],X_pca_test[:,1],cmap='rainbow')","550299fb":"sns.countplot(x=y_train)","7b72a140":"from sklearn.utils import class_weight\n\nclass_weights = class_weight.compute_class_weight('balanced', np.unique(y_train), y_train)\n\nprint(class_weights) ","506f9f7c":"def build_model(embedding_matrix, units = 64):\n    words = Input(shape=(None,))\n    x = Embedding(*embedding_matrix.shape, weights=[embedding_matrix], trainable=False)(words)\n    x = SpatialDropout1D(0.1)(x)\n    x = Bidirectional(LSTM(units, return_sequences=True, kernel_constraint=unit_norm(), dropout=0.4, recurrent_dropout=0.4))(x)\n    x = Conv1D(units, kernel_size=3, padding='valid', kernel_initializer='glorot_uniform')(x)\n    x = Dropout(0.1)(x)\n    x = concatenate([GlobalMaxPooling1D()(x), GlobalAveragePooling1D()(x)])    \n    x = add([x, Dense(units*2, activation='relu', kernel_constraint=unit_norm())(x)])\n    x = BatchNormalization()(x)\n    result = Dense(1, activation='sigmoid')(x)\n    \n    model = Model(inputs=words, outputs=result)\n    adam = Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.999, amsgrad=False)\n    model.compile(loss='binary_crossentropy', optimizer=adam, metrics=['acc'])\n    return model","46b72b35":"\n# evaluate a single mlp model\ndef evaluate_model(fold, trainX, trainy, testX, testy, embedding_matrix, class_weights, epochs = 5, batch_size=100, units = 128):\n    ckpt = ModelCheckpoint(f'lstm_{fold}.hdf5', save_best_only = True, save_weights_only = True)\n    es = EarlyStopping(monitor='val_loss', patience=5, mode='min', baseline=None, restore_best_weights=False)\n    # define model\n    model = build_model(embedding_matrix, units)    \n    # fit model\n    history = model.fit(trainX, trainy, epochs=epochs, verbose=1, validation_data=(testX, testy), \n                        class_weight=class_weights, batch_size=batch_size, callbacks=[es, ckpt])\n    print('loss %.3f (%.3f)' % (np.mean(history.history['loss']), np.std(history.history['loss'])))   \n    print('val-loss %.3f (%.3f)' % (np.mean(history.history['val_loss']), np.std(history.history['val_loss'])))   \n    print('acc %.3f (%.3f)' % (np.mean(history.history['acc']), np.std(history.history['acc'])))   \n    print('val-acc %.3f (%.3f)' % (np.mean(history.history['val_acc']), np.std(history.history['val_acc'])))\n    # make predictions\n    y_pred = model.predict(testX)\n    predictions = (y_pred > 0.5).astype(int).ravel()\n    return model, predictions, history\n\n# make an ensemble prediction for binary classification\ndef ensemble_predictions(members, testX):\n    # make predictions\n    yhats = [model.predict(testX) for model in members]\n    predictions = np.mean(np.array(yhats), axis=0)\n    return (predictions > 0.5).astype(int).ravel()\n \n# evaluate a specific number of members in an ensemble\ndef evaluate_n_members(members, n_members, testX, testy):\n    # select a subset of members\n    subset = members[:n_members]\n    # make prediction\n    yhat = ensemble_predictions(subset, testX)\n    # calculate accuracy\n    return accuracy_score(testy, yhat), yhat\n\n# splits data into train and test sets\ndef split_dataset(X, y, n_samples):\n    ix = [i for i in range(len(X))]\n    train_ix = resample(ix, replace=True, n_samples=n_samples)\n    test_ix = resample(ix, replace=True, n_samples=len(X)-len(train_ix))\n    return X[train_ix], y[train_ix], X[test_ix], y[test_ix]","d1cd9df6":"BATCH_SIZE = 512\nEPOCHS = 100\nUNITS = 64\nFOLDS = 3\n\nseed_everything(42)    \n\npredictions, scores, models = [], [], []\nfor fold in range(FOLDS):\n    trainX, trainy, testX, testy = split_dataset(X_train, y_train, 6000)\n    model, y_pred, history = evaluate_model(fold, trainX, trainy, testX, testy, \n                                            embedding_matrix, class_weights, epochs = EPOCHS, batch_size=BATCH_SIZE, units=UNITS)\n    scores.append(np.mean(history.history['acc']))\n    predictions.append(y_pred)\n    models.append(model)\n\n# summarize expected performance\nprint(\"Estimated Accuracy: %.2f%% (+\/- %.2f%%)\" % (np.mean(scores), np.std(scores)))        ","162fde06":"# evaluate different numbers of ensembles on hold out set\nensemble_scores = list()\nfor i in range(1, FOLDS+1):\n    ensemble_score, _ = evaluate_n_members(models, i, X_train, y_train)\n    print('> %d: single=%.3f, ensemble=%.3f' % (i, scores[i-1], ensemble_score))\n    ensemble_scores.append(ensemble_score)\n\nprint('Single Accuracy %.3f (%.3f)' % (np.mean(scores), np.std(scores)))   \nprint('Ensemble accuracy %.3f (%.3f)' % (np.mean(ensemble_score), np.std(ensemble_score)))","991b6e54":"x_axis = [i for i in range(1, FOLDS+1)]\nplt.plot(x_axis, scores, marker='o', linestyle='None')\nplt.plot(x_axis, ensemble_scores, marker='o')\nplt.show()","137a5a8c":"for i in range(FOLDS):\n    models[i].load_weights(f'\/kaggle\/working\/lstm_{i}.hdf5')\n\n# make predictions for train set for comparison\ny_pred = ensemble_predictions(models, X_train)\n\nprint('ROC AUC')\nprint(roc_auc_score(y_train, y_pred))\nprint('Confusion Matrix')\nprint(confusion_matrix(y_train, y_pred))\nprint('Classification Report')\nprint(classification_report(y_train, y_pred))\n","ba1169ee":"y_pred = ensemble_predictions(models, X_test)\n\nsample_sub = pd.read_csv('..\/input\/nlp-getting-started\/sample_submission.csv')\nsub = pd.DataFrame({'id':sample_sub['id'].values.tolist(),'target':y_pred})\nsub.to_csv('submission.csv',index=False)","a1d7d793":"sub['tweet'] = test['text']\nsub.head(25)","c38828af":"## Submission","e84e90f7":"## basic cleanup","a5a04fb7":"## Preprocess text","b1ba284e":"## create full dictionary and vectorize","c0135657":"The idea for this notebook is to get better understanding of NLP, LSTM, transfer learning, and ensemble techinqes. \n\nBaseline notebook: from https:\/\/www.kaggle.com\/bibek777\/lstm-baseline\n\nEnsemble starting point is from https:\/\/machinelearningmastery.com\/how-to-create-a-random-split-cross-validation-and-bagging-ensemble-for-deep-learning-in-keras\n\nObservations:\n\n- Weights averaging helps score a lot \n- PCA shows some missing data for test set on the chart (some words were artificially removed?)\n- Baseline model overfits right after 4\/5th epoch very badly\n- No significant score change when using glove-6b-50d vs glove-840b-300d\n- Lemmatization reduces the score \n- No significant improvement score improvement with class_weights","61a18ced":"## now load best saved weights and get scrores","e8d50a0f":"## plot single vs ensemble scores","c198cf83":"## Visualizing embedding vectors","ddbf4fdc":"## getting class weights to balance target counts","d9712f20":"## using GLOVE to create embedding vectors"}}