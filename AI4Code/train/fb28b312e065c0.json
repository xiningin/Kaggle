{"cell_type":{"1e18b359":"code","2f754576":"code","a3d650a9":"code","f58f3c51":"code","311589be":"code","8d1e1884":"code","781720d3":"code","a32e4216":"code","bdf02201":"code","c90d43af":"code","c8987980":"code","687da104":"code","cf737c06":"code","e25c5060":"code","5d20d4be":"code","211fd6d0":"code","303a6ce8":"code","86f9db24":"code","db652b73":"code","f6d47a01":"code","5cdce74b":"code","755db2a9":"code","00911593":"code","250797e0":"code","b2f3118e":"code","67c4f8f4":"code","0bdc0734":"code","1b57334e":"code","4426c6df":"code","f353faf0":"code","adbaeab4":"code","64803ec6":"code","6c98cfbb":"code","9d9d324b":"code","3a9a7c20":"code","97198450":"code","ff2679ca":"code","3ae3cff1":"code","da26a140":"code","849bee82":"code","990a6240":"code","2cb2cf74":"code","b3353e0b":"code","9a9f9c1f":"code","fd69517d":"code","951a831a":"code","cd2311c7":"code","e1230fc6":"code","0a94adae":"code","ebe108f2":"code","f1db0353":"code","4b2a3bf1":"markdown","d792b766":"markdown","6e7662d3":"markdown","2e447bed":"markdown","79ffb050":"markdown","7ad3dd0f":"markdown","d2dd0684":"markdown","ee92b118":"markdown","8067c600":"markdown","5fa86a85":"markdown","b33362c6":"markdown"},"source":{"1e18b359":"import numpy as np \nimport pandas as pd \nimport random as rd\nimport datetime\nimport time\nimport sys\nimport gc\nimport pickle\nsys.version_info\n\nfrom mpl_toolkits.mplot3d import Axes3D\nimport matplotlib.pyplot as plt \nfrom matplotlib.pylab import rcParams\nrcParams['figure.figsize'] = 12, 4\nimport seaborn as sns \n\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.dummy import DummyRegressor\n\nfrom statsmodels.tsa.arima_model import ARIMA\nfrom statsmodels.tsa.statespace.sarimax import SARIMAX\nfrom pandas.plotting import autocorrelation_plot\nfrom statsmodels.tsa.stattools import adfuller, acf, pacf,arma_order_select_ic\nimport statsmodels.formula.api as smf\nimport statsmodels.tsa.api as smt\nimport statsmodels.api as sm\nimport scipy.stats as scs\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","2f754576":"train = pd.read_csv(\"..\/input\/competitive-data-science-predict-future-sales\/sales_train.csv\")\nitem_categories = pd.read_csv(\"..\/input\/competitive-data-science-predict-future-sales\/item_categories.csv\")\nitems = pd.read_csv(\"..\/input\/competitive-data-science-predict-future-sales\/items.csv\")\nshops = pd.read_csv(\"..\/input\/competitive-data-science-predict-future-sales\/shops.csv\")\ntest = pd.read_csv(\"..\/input\/competitive-data-science-predict-future-sales\/test.csv\")\nsubmission = pd.read_csv(\"..\/input\/competitive-data-science-predict-future-sales\/sample_submission.csv\")","a3d650a9":"print(sales_train.info())\nprint('----------------------------------')\nprint(items.info())\nprint('----------------------------------')\nprint(item_categories.info())\nprint('----------------------------------')\nprint(shops.info())\nprint('----------------------------------')\nprint(test.info())\nprint('----------------------------------')\nprint(submission.info())","f58f3c51":"print(train.isnull().sum())\nprint('----------------------------------')\nprint(items.isnull().sum())\nprint('----------------------------------')\nprint(item_categories.isnull().sum())\nprint('----------------------------------')\nprint(shops.isnull().sum())\nprint('----------------------------------')\nprint(test.isnull().sum())","311589be":"plt.figure(figsize=(10,4))\nplt.xlim(-100, 3000)\nsns.boxplot(x=train.item_cnt_day)\n\nplt.figure(figsize=(10,4))\nplt.xlim(train.item_price.min(), train.item_price.max()*1.1)\nsns.boxplot(x=train.item_price)\n\nprint(len(train[train.item_cnt_day>999]))\nprint(len(train[train.item_cnt_day>500]))\nprint(len(train[train.item_cnt_day<501]))\n\ntrain = train[train.item_price<100000]\ntrain = train[train.item_cnt_day<1000]","8d1e1884":"plt.figure(figsize=(10,4))\nplt.xlim(-100, 1000)\nsns.boxplot(x=train.item_cnt_day)\n\nplt.figure(figsize=(10,4))\nplt.xlim(0, 100000)\n#plt.xlim(train.item_price.min(), train.item_price.max()*1.1)\nsns.boxplot(x=train.item_price)","781720d3":"train = train[train.item_price > 0].reset_index(drop=True)\ntrain[train.item_cnt_day <= 0].item_cnt_day.unique()\ntrain.loc[train.item_cnt_day < 1, 'item_cnt_day'] = 0","a32e4216":"plt.figure(figsize=(10,4))\nplt.xlim(-100, 1000)\nsns.boxplot(x=train.item_cnt_day)\n\nplt.figure(figsize=(10,4))\n#plt.xlim(0, 100000)\nplt.xlim(train.item_price.min(), train.item_price.max()*1.1)\nsns.boxplot(x=train.item_price)","bdf02201":"shops.shop_name.unique()","c90d43af":"shops.groupby(['category']).sum()","c8987980":"monthly_sales=sales_train.groupby([\"date_block_num\",\"shop_id\",\"item_id\"])[\n    \"date\",\"item_price\",\"item_cnt_day\"].agg({\"date\":[\"min\",'max'],\"item_price\":\"mean\",\"item_cnt_day\":\"sum\"})","687da104":"monthly_sales.head(10)","cf737c06":"x=items.groupby(['item_category_id']).count()\nx=x.sort_values(by='item_id',ascending=False)\nx=x.iloc[0:10].reset_index()\n\nplt.figure(figsize=(8,4))\nax= sns.barplot(x.item_category_id, x.item_id, alpha=0.8)\nplt.title(\"Items per Category\")\nplt.ylabel('# of items', fontsize=12)\nplt.xlabel('Category', fontsize=12)\nplt.show()","e25c5060":"ts=sales_train.groupby([\"date_block_num\"])[\"item_cnt_day\"].sum()\nts.astype('float')\nplt.figure(figsize=(16,8))\nplt.title('Total Sales of the company')\nplt.xlabel('Time')\nplt.ylabel('Sales')\nplt.plot(ts);","5d20d4be":"import statsmodels.api as sm\nres = sm.tsa.seasonal_decompose(ts.values,freq=12,model=\"multiplicative\")\nplt.figure(figsize=(16,12))\nfig = res.plot()\nfig.show()","211fd6d0":"res = sm.tsa.seasonal_decompose(ts.values,freq=12,model=\"additive\")\nplt.figure(figsize=(16,12))\nfig = res.plot()\nfig.show()","303a6ce8":"# Stationarity tests\ndef test_stationarity(timeseries):\n    \n    #Perform Dickey-Fuller test:\n    print('Results of Dickey-Fuller Test:')\n    dftest = adfuller(timeseries, autolag='AIC')\n    dfoutput = pd.Series(dftest[0:4], index=['Test Statistic','p-value','#Lags Used','Number of Observations Used'])\n    for key,value in dftest[4].items():\n        dfoutput['Critical Value (%s)'%key] = value\n    print (dfoutput)\n\ntest_stationarity(ts)","86f9db24":"# to remove trend\nfrom pandas import Series as Series\n# create a differenced series\ndef difference(dataset, interval=1):\n    diff = list()\n    for i in range(interval, len(dataset)):\n        value = dataset[i] - dataset[i - interval]\n        diff.append(value)\n    return Series(diff)\n\n# invert differenced forecast\ndef inverse_difference(last_ob, value):\n    return value + last_ob","db652b73":"ts=sales_train.groupby([\"date_block_num\"])[\"item_cnt_day\"].sum()\nts.astype('float')\nplt.figure(figsize=(16,16))\nplt.subplot(311)\nplt.title('Original')\nplt.xlabel('Time')\nplt.ylabel('Sales')\nplt.plot(ts)\nplt.subplot(312)\nplt.title('After De-trend')\nplt.xlabel('Time')\nplt.ylabel('Sales')\nnew_ts=difference(ts)\nplt.plot(new_ts)\nplt.plot()\n\nplt.subplot(313)\nplt.title('After De-seasonalization')\nplt.xlabel('Time')\nplt.ylabel('Sales')\nnew_ts=difference(ts,12)       # assuming the seasonality is 12 months long\nplt.plot(new_ts)\nplt.plot()","f6d47a01":"# now testing the stationarity again after de-seasonality\ntest_stationarity(new_ts)","5cdce74b":"def tsplot(y, lags=None, figsize=(10, 8), style='bmh',title=''):\n    if not isinstance(y, pd.Series):\n        y = pd.Series(y)\n    with plt.style.context(style):    \n        fig = plt.figure(figsize=figsize)\n        #mpl.rcParams['font.family'] = 'Ubuntu Mono'\n        layout = (3, 2)\n        ts_ax = plt.subplot2grid(layout, (0, 0), colspan=2)\n        acf_ax = plt.subplot2grid(layout, (1, 0))\n        pacf_ax = plt.subplot2grid(layout, (1, 1))\n        qq_ax = plt.subplot2grid(layout, (2, 0))\n        pp_ax = plt.subplot2grid(layout, (2, 1))\n        \n        y.plot(ax=ts_ax)\n        ts_ax.set_title(title)\n        smt.graphics.plot_acf(y, lags=lags, ax=acf_ax, alpha=0.5)\n        smt.graphics.plot_pacf(y, lags=lags, ax=pacf_ax, alpha=0.5)\n        sm.qqplot(y, line='s', ax=qq_ax)\n        qq_ax.set_title('QQ Plot')        \n        scs.probplot(y, sparams=(y.mean(), y.std()), plot=pp_ax)\n\n        plt.tight_layout()\n    return ","755db2a9":"# Simulate an AR(1) process with alpha = 0.6\nnp.random.seed(1)\nn_samples = int(1000)\na = 0.6\nx = w = np.random.normal(size=n_samples)\n\nfor t in range(n_samples):\n    x[t] = a*x[t-1] + w[t]\nlimit=12    \n_ = tsplot(x, lags=limit,title=\"AR(1)process\")","00911593":"# Simulate an AR(2) process\n\nn = int(1000)\nalphas = np.array([.444, .333])\nbetas = np.array([0.])\n\n# Python requires us to specify the zero-lag value which is 1\n# Also note that the alphas for the AR model must be negated\n# We also set the betas for the MA equal to 0 for an AR(p) model\n# For more information see the examples at statsmodels.org\nar = np.r_[1, -alphas]\nma = np.r_[1, betas]\n\nar2 = smt.arma_generate_sample(ar=ar, ma=ma, nsample=n) \n_ = tsplot(ar2, lags=12,title=\"AR(2) process\")\n","250797e0":"max_lag = 12\n\nn = int(5000) # lots of samples to help estimates\nburn = int(n\/10) # number of samples to discard before fit\n\nalphas = np.array([0.8, -0.65])\nbetas = np.array([0.5, -0.7])\nar = np.r_[1, -alphas]\nma = np.r_[1, betas]\n\narma22 = smt.arma_generate_sample(ar=ar, ma=ma, nsample=n, burnin=burn)\n_ = tsplot(arma22, lags=max_lag,title=\"ARMA(2,2) process\")","b2f3118e":"# pick best order by aic \n# smallest aic value wins\nbest_aic = np.inf \nbest_order = None\nbest_mdl = None\n\nrng = range(5)\nfor i in rng:\n    for j in rng:\n        try:\n            tmp_mdl = smt.ARMA(arma22, order=(i, j)).fit(method='mle', trend='nc')\n            tmp_aic = tmp_mdl.aic\n            if tmp_aic < best_aic:\n                best_aic = tmp_aic\n                best_order = (i, j)\n                best_mdl = tmp_mdl\n        except: continue\n\n\nprint('aic: {:6.5f} | order: {}'.format(best_aic, best_order))\n","67c4f8f4":"best_aic = np.inf \nbest_order = None\nbest_mdl = None\n\nrng = range(5)\nfor i in rng:\n    for j in rng:\n        try:\n            tmp_mdl = smt.ARMA(new_ts.values, order=(i, j)).fit(method='mle', trend='nc')\n            tmp_aic = tmp_mdl.aic\n            if tmp_aic < best_aic:\n                best_aic = tmp_aic\n                best_order = (i, j)\n                best_mdl = tmp_mdl\n        except: continue\n\n\nprint('aic: {:6.5f} | order: {}'.format(best_aic, best_order))","0bdc0734":"# adding the dates to the Time-series as index\nts=sales_train.groupby([\"date_block_num\"])[\"item_cnt_day\"].sum()\nts.index=pd.date_range(start = '2013-01-01',end='2015-10-01', freq = 'MS')\nts=ts.reset_index()\nts.head()","1b57334e":"from fbprophet import Prophet\n#prophet reqiures a pandas df at the below config \n# ( date column named as DS and the value column as Y)\nts.columns=['ds','y']\nmodel = Prophet( yearly_seasonality=True) #instantiate Prophet with only yearly seasonality as our data is monthly \nmodel.fit(ts) #fit the model with your dataframe","4426c6df":"# predict for five months in the furure and MS - month start is the frequency\nfuture = model.make_future_dataframe(periods = 5, freq = 'MS')  \n# now lets make the forecasts\nforecast = model.predict(future)\nforecast[['ds', 'yhat', 'yhat_lower', 'yhat_upper']].tail()","f353faf0":"model.plot(forecast)","adbaeab4":"model.plot_components(forecast)","64803ec6":"total_sales=sales_train.groupby(['date_block_num'])[\"item_cnt_day\"].sum()\ndates=pd.date_range(start = '2013-01-01',end='2015-10-01', freq = 'MS')\n\ntotal_sales.index=dates\ntotal_sales.head()","6c98cfbb":"# get the unique combinations of item-store from the sales data at monthly level\nmonthly_sales=sales_train.groupby([\"shop_id\",\"item_id\",\"date_block_num\"])[\"item_cnt_day\"].sum()\n# arrange it conviniently to perform the hts \nmonthly_sales=monthly_sales.unstack(level=-1).fillna(0)\nmonthly_sales=monthly_sales.T\ndates=pd.date_range(start = '2013-01-01',end='2015-10-01', freq = 'MS')\nmonthly_sales.index=dates\nmonthly_sales=monthly_sales.reset_index()\nmonthly_sales.head()","9d9d324b":"import time\nstart_time=time.time()\n\n# Bottoms up\n# Calculating the base forecasts using prophet\n# From HTSprophet pachage -- https:\/\/github.com\/CollinRooney12\/htsprophet\/blob\/master\/htsprophet\/hts.py\nforecastsDict = {}\nfor node in range(len(monthly_sales)):\n    # take the date-column and the col to be forecasted\n    nodeToForecast = pd.concat([monthly_sales.iloc[:,0], monthly_sales.iloc[:, node+1]], axis = 1)\n#     print(nodeToForecast.head())  # just to check\n# rename for prophet compatability\n    nodeToForecast = nodeToForecast.rename(columns = {nodeToForecast.columns[0] : 'ds'})\n    nodeToForecast = nodeToForecast.rename(columns = {nodeToForecast.columns[1] : 'y'})\n    growth = 'linear'\n    m = Prophet(growth, yearly_seasonality=True)\n    m.fit(nodeToForecast)\n    future = m.make_future_dataframe(periods = 1, freq = 'MS')\n    forecastsDict[node] = m.predict(future)\n    if (node== 10):\n        end_time=time.time()\n        print(\"forecasting for \",node,\"th node and took\",end_time-start_time,\"s\")\n        break","3a9a7c20":"monthly_shop_sales=sales_train.groupby([\"date_block_num\",\"shop_id\"])[\"item_cnt_day\"].sum()\n# get the shops to the columns\nmonthly_shop_sales=monthly_shop_sales.unstack(level=1)\nmonthly_shop_sales=monthly_shop_sales.fillna(0)\nmonthly_shop_sales.index=dates\nmonthly_shop_sales=monthly_shop_sales.reset_index()\nmonthly_shop_sales.head()","97198450":"start_time=time.time()\n\n# Calculating the base forecasts using prophet\n# From HTSprophet pachage -- https:\/\/github.com\/CollinRooney12\/htsprophet\/blob\/master\/htsprophet\/hts.py\nforecastsDict = {}\nfor node in range(len(monthly_shop_sales)):\n    # take the date-column and the col to be forecasted\n    nodeToForecast = pd.concat([monthly_shop_sales.iloc[:,0], monthly_shop_sales.iloc[:, node+1]], axis = 1)\n#     print(nodeToForecast.head())  # just to check\n# rename for prophet compatability\n    nodeToForecast = nodeToForecast.rename(columns = {nodeToForecast.columns[0] : 'ds'})\n    nodeToForecast = nodeToForecast.rename(columns = {nodeToForecast.columns[1] : 'y'})\n    growth = 'linear'\n    m = Prophet(growth, yearly_seasonality=True)\n    m.fit(nodeToForecast)\n    future = m.make_future_dataframe(periods = 1, freq = 'MS')\n    forecastsDict[node] = m.predict(future)","ff2679ca":"#predictions = np.zeros([len(forecastsDict[0].yhat),1]) \nnCols = len(list(forecastsDict.keys()))+1\nfor key in range(0, nCols-1):\n    f1 = np.array(forecastsDict[key].yhat)\n    f2 = f1[:, np.newaxis]\n    if key==0:\n        predictions=f2.copy()\n       # print(predictions.shape)\n    else:\n       predictions = np.concatenate((predictions, f2), axis = 1)","3ae3cff1":"predictions_unknown=predictions[-1]\npredictions_unknown","da26a140":"test.tail()","849bee82":"gp = sales_train.groupby(['date_block_num', 'shop_id', 'item_id']).agg({'item_cnt_day': ['sum']})","990a6240":"X = np.array(list(map(list, gp.index.values)))\ny_train = gp.values","2cb2cf74":"test['date_block_num'] = train['date_block_num'].max() + 1\nX_test = test[['date_block_num', 'shop_id', 'item_id']].values","b3353e0b":"samp = np.random.permutation(np.arange(len(y_train)))[:int(len(y_train)*.2)]\nfig = plt.figure()\nax = fig.add_subplot(111, projection='3d')\nax.scatter(X[samp,1], X[samp,2], y_train[samp].ravel(), c=X[samp,0], marker='.')","9a9f9c1f":"oh0 = OneHotEncoder(categories='auto').fit(X[:,0].reshape(-1, 1))\nx0 = oh0.transform(X[:,0].reshape(-1, 1))","fd69517d":"oh1 = OneHotEncoder(categories='auto').fit(X[:,1].reshape(-1, 1))\nx1 = oh1.transform(X[:,1].reshape(-1, 1))\nx1_t = oh1.transform(X_test[:,1].reshape(-1, 1))","951a831a":"print(X[:, :1].shape, x1.toarray().shape, X[:, 2:].shape)\nX_train = np.concatenate((X[:, :1], x1.toarray(), X[:, 2:]), axis=1)\nX_test = np.concatenate((X_test[:, :1], x1_t.toarray(), X_test[:, 2:]), axis=1)","cd2311c7":"dmy = DummyRegressor().fit(X_train, y_train)\n\nreg = LinearRegression().fit(X_train, y_train)\n\nrfr = RandomForestRegressor().fit(X_train, y_train.ravel())","e1230fc6":"rmse_dmy = np.sqrt(mean_squared_error(y_train, dmy.predict(X_train)))\nprint('Dummy RMSE: %.4f' % rmse_dmy)\nrmse_reg = np.sqrt(mean_squared_error(y_train, reg.predict(X_train)))\nprint('LR RMSE: %.4f' % rmse_reg)\nrmse_rfr = np.sqrt(mean_squared_error(y_train, rfr.predict(X_train)))\nprint('RFR RMSE: %.4f' % rmse_rfr)","0a94adae":"y_test = rfr.predict(X_test)","ebe108f2":"sample_submission['item_cnt_month'] = y_test","f1db0353":"sample_submission.to_csv('xgb_submission.csv', index=False)","4b2a3bf1":"visualization after dropping the negative values","d792b766":"![](http:\/\/)![](http:\/\/)![](http:\/\/) <font color='green'> <font color='blue'> ** *Inspired by Some other Kernal* ** <\/font> \n<\/font>\n","6e7662d3":"Number of Items per Catagory","2e447bed":"After removing the Outliers, visualization of the item price and item cnt day","79ffb050":"# Import the Required Module\n   ###       First thing First","7ad3dd0f":"Elimination the item price below zero and negative values of item_cnt_day","d2dd0684":"**Let's Put into dataframe every dataset**","ee92b118":"Now we need to understand about the dataset, so that we can work through on it.","8067c600":"## Checking Missing Value","5fa86a85":"# Idetifying the Outlier","b33362c6":"### Shops Data Preprocessing"}}