{"cell_type":{"467f7397":"code","26c22d1e":"code","15ff1076":"code","ee576f2b":"code","683a93dd":"code","95545c01":"code","adce42d8":"code","630711ba":"code","d5390da3":"code","944a5537":"code","371adce9":"code","bd0fc67f":"code","ee4edd8e":"code","ea29c0c2":"code","7e4da70d":"code","9b53c979":"code","341558c8":"code","a23fd94c":"code","3047f907":"code","3792b4ab":"code","37a23d73":"code","fabacef3":"code","f3766975":"code","73b349b3":"code","59f9bfad":"code","40677324":"code","a2f57ccd":"code","0ad9ec1a":"markdown","91aa46c7":"markdown","0bfa2f4d":"markdown","3caa4e27":"markdown"},"source":{"467f7397":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","26c22d1e":"## Import bigquery package\nfrom google.cloud import bigquery","15ff1076":"# Create a \"Client\" object\nclient = bigquery.Client()","ee576f2b":"# Construct a reference to the \"hacker_news\" dataset\ndataset_ref = client.dataset(\"stackoverflow\", project=\"bigquery-public-data\")\n\n# API request - fetch the dataset\ndataset = client.get_dataset(dataset_ref)","683a93dd":"# Show all the tables in the Stack OverFlow dataset\n\ntables = list(client.list_tables(dataset))\n\n# Print names of all tables in the dataset\nfor table in tables:  \n    print(table.table_id)","95545c01":"# Construct a reference to the \"badges\" table\ntable_ref = dataset_ref.table(\"badges\")\n\n# API request - fetch the table\ntable = client.get_table(table_ref)","adce42d8":"# Print information on all the columns in the \"badges\" table in the \"Stack Overflow\" dataset\ntable.schema","630711ba":"# Preview the first five lines of the \"badge\" table\nclient.list_rows(table, max_results=5).to_dataframe()","d5390da3":"# Preview the first five entries in the \"name\" column of the \"badge\" table\nclient.list_rows(table, selected_fields=table.schema[:2], max_results=5).to_dataframe()","944a5537":"## Query 1:- select the IDs from badges table based on a condition\n\nsql = \"\"\"\n        SELECT id,name\n        FROM `bigquery-public-data.stackoverflow.badges`\n        where id > 84000 and id <85000    \n        \"\"\"\n\nresults = client.query(sql).to_dataframe()\n\n# Create a QueryJobConfig object to estimate size of query without running it\ndry_run_config = bigquery.QueryJobConfig(dry_run=True)\n\n# API request - dry run query to estimate costs\ndry_run_query_job = client.query(sql, job_config=dry_run_config)\n\nprint(\"This query will process {} bytes.\".format(dry_run_query_job.total_bytes_processed\/1000000))\nresults.head()","371adce9":"## Query 2:- Date and time of post_history\n\nsql = \"\"\"\n        SELECT \n        creation_date,\n        date(creation_date) as Date,\n        time(creation_date) as Time\n        FROM `bigquery-public-data.stackoverflow.post_history`\n        limit 5\n        \"\"\"\n\nresults = client.query(sql).to_dataframe()\n\n# Create a QueryJobConfig object to estimate size of query without running it\ndry_run_config = bigquery.QueryJobConfig(dry_run=True)\n\n# API request - dry run query to estimate costs\ndry_run_query_job = client.query(sql, job_config=dry_run_config)\n\nprint(\"\\n This query will process {} Mega bytes.\".format(dry_run_query_job.total_bytes_processed\/1000000))\nresults.head()","bd0fc67f":"## Query 3:- Dive deep on the badges vs users table\nsql = \"\"\"\n        SELECT \n        b.id,\n        b.name,\n        b.date,\n        b.user_id,\n        b.class,\n        b.tag_based,\n        u.display_name,\n        u.about_me,\n        u.age,\n        u.creation_date,\n        u.last_access_date,\n        u.location,\n        u.reputation,\n        u.up_votes,\n        u.down_votes,\n        u.views,\n        u.profile_image_url\n        FROM `bigquery-public-data.stackoverflow.badges` b\n        join `bigquery-public-data.stackoverflow.users` u\n        on b.user_id=u.id\n        limit 5\n        \"\"\"\n\nresults = client.query(sql).to_dataframe()\n\n# Create a QueryJobConfig object to estimate size of query without running it\ndry_run_config = bigquery.QueryJobConfig(dry_run=True)\n\n# API request - dry run query to estimate costs\ndry_run_query_job = client.query(sql, job_config=dry_run_config)\n\nprint(\"\\n This query will process {} Mega bytes.\".format(dry_run_query_job.total_bytes_processed\/1000000))\nresults.head()","ee4edd8e":"## Query 4:- Post with max views\nsql = \"\"\"\n       \n        select * from (\n        select\n        Badges_Users_Table.display_name as Name,\n        sum(Badges_Users_Table.views) as Most_Views\n        from (\n        SELECT \n        b.id,\n        b.name,\n        b.date,\n        b.user_id,\n        b.class,\n        b.tag_based,\n        u.display_name,\n        u.about_me,\n        u.age,\n        u.creation_date,\n        u.last_access_date,\n        u.location,\n        u.reputation,\n        u.up_votes,\n        u.down_votes,\n        u.views,\n        u.profile_image_url\n        FROM `bigquery-public-data.stackoverflow.badges` b\n        join `bigquery-public-data.stackoverflow.users` u\n        on b.user_id=u.id ) Badges_Users_Table \n        group by Badges_Users_Table.display_name) a\n        order by a.Most_Views desc\n        limit 1\n        \"\"\"\n\nresults = client.query(sql).to_dataframe()\n\n# Create a QueryJobConfig object to estimate size of query without running it\ndry_run_config = bigquery.QueryJobConfig(dry_run=True)\n\n# API request - dry run query to estimate costs\ndry_run_query_job = client.query(sql, job_config=dry_run_config)\n\nprint(\"\\n This query will process {} Mega bytes.\".format(dry_run_query_job.total_bytes_processed\/1000000))\nresults.head()","ea29c0c2":"## Query 5:- Overall summary of the KPIs associated within the Project\nsql_badges = \"\"\"\n       \n        select * from bigquery-public-data.stackoverflow.badges\n        limit 1\n        \"\"\"\nsql_comments = \"\"\"\n       \n        select * from bigquery-public-data.stackoverflow.comments\n        limit 1\n        \"\"\"\n\nsql_post_history = \"\"\"\n       \n        select * from bigquery-public-data.stackoverflow.post_history\n        limit 1\n        \"\"\"\nsql_post_links = \"\"\"\n       \n        select * from bigquery-public-data.stackoverflow.post_links\n        limit 1\n        \"\"\"\nsql_posts_answers = \"\"\"\n       \n        select * from bigquery-public-data.stackoverflow.posts_answers\n        limit 1\n        \"\"\"\nsql_posts_moderator_nomination = \"\"\"\n       \n        select * from bigquery-public-data.stackoverflow.posts_moderator_nomination\n        limit 1\n        \"\"\"\nsql_posts_orphaned_tag_wiki = \"\"\"\n       \n        select * from bigquery-public-data.stackoverflow.posts_orphaned_tag_wiki\n        limit 1\n        \"\"\"\nsql_posts_privilege_wiki = \"\"\"\n       \n        select * from bigquery-public-data.stackoverflow.posts_privilege_wiki\n        limit 1\n        \"\"\"\nsql_posts_questions = \"\"\"\n       \n        select * from bigquery-public-data.stackoverflow.posts_questions\n        limit 1\n        \"\"\"\nsql_posts_tag_wiki = \"\"\"\n       \n        select * from bigquery-public-data.stackoverflow.posts_tag_wiki\n        limit 1\n        \"\"\"\nsql_posts_tag_wiki_excerpt = \"\"\"\n       \n        select * from bigquery-public-data.stackoverflow.posts_tag_wiki_excerpt\n        limit 1\n        \"\"\"\nsql_posts_wiki_placeholder = \"\"\"\n       \n        select * from bigquery-public-data.stackoverflow.posts_wiki_placeholder\n        limit 1\n        \"\"\"\nsql_stackoverflow_posts = \"\"\"\n       \n        select * from bigquery-public-data.stackoverflow.stackoverflow_posts\n        limit 1\n        \"\"\"\nsql_tags = \"\"\"\n       \n        select * from bigquery-public-data.stackoverflow.tags\n        limit 1\n        \"\"\"\nsql_users = \"\"\"\n       \n        select * from bigquery-public-data.stackoverflow.users\n        limit 1\n        \"\"\"\nsql_votes = \"\"\"\n       \n        select * from bigquery-public-data.stackoverflow.votes\n        limit 1\n        \"\"\"\nresults_badges = client.query(sql_badges).to_dataframe()\nresults_comments = client.query(sql_comments).to_dataframe()\nresults_post_history = client.query(sql_post_history).to_dataframe()\nresults_post_links = client.query(sql_post_links).to_dataframe()\nresults_posts_answers = client.query(sql_posts_answers).to_dataframe()\nresults_posts_moderator_nomination = client.query(sql_posts_moderator_nomination).to_dataframe()\nresults_posts_orphaned_tag_wiki = client.query(sql_posts_orphaned_tag_wiki).to_dataframe()\nresults_posts_privilege_wiki = client.query(sql_posts_privilege_wiki).to_dataframe()\nresults_posts_questions = client.query(sql_posts_questions).to_dataframe()\nresults_posts_tag_wiki = client.query(sql_posts_tag_wiki).to_dataframe()\nresults_posts_tag_wiki_excerpt = client.query(sql_posts_tag_wiki_excerpt).to_dataframe()\nresults_posts_wiki_placeholder = client.query(sql_posts_wiki_placeholder).to_dataframe()\nresults_stackoverflow_posts = client.query(sql_stackoverflow_posts).to_dataframe()\nresults_tags = client.query(sql_tags).to_dataframe()\nresults_users = client.query(sql_users).to_dataframe()\nresults_votes = client.query(sql_votes).to_dataframe()\n\n# Create a QueryJobConfig object to estimate size of query without running it\ndry_run_config = bigquery.QueryJobConfig(dry_run=True)\n\n# API request - dry run query to estimate costs\ndry_run_query_job_badges = client.query(sql_badges, job_config=dry_run_config)\ndry_run_query_job_comments = client.query(sql_comments, job_config=dry_run_config)\ndry_run_query_job_post_history = client.query(sql_post_history, job_config=dry_run_config)\ndry_run_query_job_post_links = client.query(sql_post_links, job_config=dry_run_config)\ndry_run_query_job_posts_answers = client.query(sql_posts_answers, job_config=dry_run_config)\ndry_run_query_job_posts_moderator_nomination = client.query(sql_posts_moderator_nomination, job_config=dry_run_config)\ndry_run_query_job_posts_orphaned_tag_wiki= client.query(sql_posts_orphaned_tag_wiki, job_config=dry_run_config)\ndry_run_query_job_posts_privilege_wiki = client.query(sql_posts_privilege_wiki, job_config=dry_run_config)\ndry_run_query_job_posts_questions = client.query(sql_posts_questions, job_config=dry_run_config)\ndry_run_query_job_posts_tag_wiki = client.query(sql_posts_tag_wiki, job_config=dry_run_config)\ndry_run_query_job_posts_tag_wiki_excerpt = client.query(sql_posts_tag_wiki_excerpt, job_config=dry_run_config)\ndry_run_query_job_posts_wiki_placeholder = client.query(sql_posts_wiki_placeholder, job_config=dry_run_config)\ndry_run_query_job_stackoverflow_posts = client.query(sql_stackoverflow_posts, job_config=dry_run_config)\ndry_run_query_job_tags = client.query(sql_tags, job_config=dry_run_config)\ndry_run_query_job_users = client.query(sql_users, job_config=dry_run_config)\ndry_run_query_job_votes = client.query(sql_votes, job_config=dry_run_config)\n\n\n\nprint(\"\\n This query will process {} Mega bytes for the badges table.\".format(dry_run_query_job_badges.total_bytes_processed\/1000000))\nprint(\"\\n This query will process {} Mega bytes for the comments table.\".format(dry_run_query_job_comments.total_bytes_processed\/1000000))\nprint(\"\\n This query will process {} Mega bytes for the post_history table.\".format(dry_run_query_job_post_history.total_bytes_processed\/1000000))\nprint(\"\\n This query will process {} Mega bytes for the post_links table.\".format(dry_run_query_job_post_links.total_bytes_processed\/1000000))\nprint(\"\\n This query will process {} Mega bytes for the posts_answers table.\".format(dry_run_query_job_posts_answers.total_bytes_processed\/1000000))\nprint(\"\\n This query will process {} Mega bytes for the posts_moderator_nomination table.\".format(dry_run_query_job_posts_moderator_nomination.total_bytes_processed\/1000000))\nprint(\"\\n This query will process {} Mega bytes for the posts_orphaned_tag_wiki table.\".format(dry_run_query_job_posts_orphaned_tag_wiki.total_bytes_processed\/1000000))\nprint(\"\\n This query will process {} Mega bytes for the posts_privilege_wiki table.\".format(dry_run_query_job_posts_privilege_wiki.total_bytes_processed\/1000000))\nprint(\"\\n This query will process {} Mega bytes for the posts_questions table.\".format(dry_run_query_job_posts_questions.total_bytes_processed\/1000000))\nprint(\"\\n This query will process {} Mega bytes for the posts_tag_wiki table.\".format(dry_run_query_job_posts_tag_wiki.total_bytes_processed\/1000000))\nprint(\"\\n This query will process {} Mega bytes for the posts_tag_wiki_excerpt table.\".format(dry_run_query_job_posts_tag_wiki_excerpt.total_bytes_processed\/1000000))\nprint(\"\\n This query will process {} Mega bytes for the posts_wiki_placeholder table.\".format(dry_run_query_job_posts_wiki_placeholder.total_bytes_processed\/1000000))\nprint(\"\\n This query will process {} Mega bytes for the stackoverflow_posts table.\".format(dry_run_query_job_stackoverflow_posts.total_bytes_processed\/1000000))\nprint(\"\\n This query will process {} Mega bytes for the tags table.\".format(dry_run_query_job_tags.total_bytes_processed\/1000000))\nprint(\"\\n This query will process {} Mega bytes for the users table.\".format(dry_run_query_job_users.total_bytes_processed\/1000000))\nprint(\"\\n This query will process {} Mega bytes for the votes table.\".format(dry_run_query_job_votes.total_bytes_processed\/1000000))\n\nprint('\\n Badge Table contains :- ',results_badges.columns.to_list())\nprint('\\n Comments Table contain :-',results_comments.columns.to_list())\nprint('\\n post_history Table contain :-',results_post_history.columns.to_list())\nprint('\\n Post_links Table contain :-',results_post_links.columns.to_list())\nprint('\\n posts_answers Table contain :-',results_posts_answers.columns.to_list())\nprint('\\n posts_moderator_nomination Table contain :-',results_posts_moderator_nomination.columns.to_list())\nprint('\\n Post_posts_orphaned_tag_wiki :-',results_posts_orphaned_tag_wiki.columns.to_list())\nprint('\\n Post_posts_privilege_wiki :-',results_posts_privilege_wiki.columns.to_list())\nprint('\\n Post_posts_questions :-',results_posts_tag_wiki.columns.to_list())\nprint('\\n posts_tag_wiki Table contain :-',results_post_history.columns.to_list())\nprint('\\n posts_wiki_placeholder Table contain :-',results_posts_wiki_placeholder.columns.to_list())\nprint('\\n stackoverflow_posts Table contain :-',results_stackoverflow_posts.columns.to_list())\nprint('\\n tags Table contain :-',results_tags.columns.to_list())\nprint('\\n users Table contain :-',results_users.columns.to_list())\nprint('\\n votes Table contain :-',results_votes.columns.to_list())","7e4da70d":"## Query 6:- Use of with function to extract few parametrs\nsql = \"\"\"\n       with post_questions_small as  (SELECT \n        *\n        FROM `bigquery-public-data.stackoverflow.posts_questions` pq)\n        \n        select title,body from post_questions_small\n        limit 1\n        \"\"\"\n\nresults = client.query(sql).to_dataframe()\n\n# Create a QueryJobConfig object to estimate size of query without running it\ndry_run_config = bigquery.QueryJobConfig(dry_run=True)\n\n# API request - dry run query to estimate costs\ndry_run_query_job = client.query(sql, job_config=dry_run_config)\n\nprint(\"\\n This query will process {} Mega bytes.\".format(dry_run_query_job.total_bytes_processed\/1000000))\nresults.head()","9b53c979":"## Query 6:- Use of with function to extract few parametrs\nsql = \"\"\"\n       with post_questions_small as  (SELECT \n        *\n        FROM `bigquery-public-data.stackoverflow.posts_questions` pq)\n        \n        select title,body from post_questions_small\n        limit 1\n        \"\"\"\n\nresults = client.query(sql).to_dataframe()\n\n# Create a QueryJobConfig object to estimate size of query without running it\ndry_run_config = bigquery.QueryJobConfig(dry_run=True)\n\n# API request - dry run query to estimate costs\ndry_run_query_job = client.query(sql, job_config=dry_run_config)\n\nprint(\"\\n This query will process {} Mega bytes.\".format(dry_run_query_job.total_bytes_processed\/1000000))\nresults.head()","341558c8":"## Beginner Query 1:-  Select all\n\n\nsql = \"\"\"\n\n   select * FROM `bigquery-public-data.stackoverflow.users`\n   limit 1\n        \n        \"\"\"\n\nresults = client.query(sql).to_dataframe()\n\n# Create a QueryJobConfig object to estimate size of query without running it\ndry_run_config = bigquery.QueryJobConfig(dry_run=True)\n\n# API request - dry run query to estimate costs\ndry_run_query_job = client.query(sql, job_config=dry_run_config)\n\nprint(\"\\n This query will process {} Mega bytes.\".format(dry_run_query_job.total_bytes_processed\/1000000))\nresults.head()\n","a23fd94c":"## Beginner Query 2:-  Select where\n\nsql = \"\"\"\n\n   select * FROM `bigquery-public-data.stackoverflow.users`\n   where display_name='Joel Spolsky'\n   limit 1\n        \n        \"\"\"\n\nresults = client.query(sql).to_dataframe()\n\n# Create a QueryJobConfig object to estimate size of query without running it\ndry_run_config = bigquery.QueryJobConfig(dry_run=True)\n\n# API request - dry run query to estimate costs\ndry_run_query_job = client.query(sql, job_config=dry_run_config)\n\nprint(\"\\n This query will process {} Mega bytes.\".format(dry_run_query_job.total_bytes_processed\/1000000))\nresults.head()\n","3047f907":"## Beginner Query 3:-  Select group by, having\n\nsql = \"\"\"\n\n   select display_name FROM `bigquery-public-data.stackoverflow.users`\n   group by display_name\n   limit 5\n        \n        \"\"\"\n\nresults = client.query(sql).to_dataframe()\n\n# Create a QueryJobConfig object to estimate size of query without running it\ndry_run_config = bigquery.QueryJobConfig(dry_run=True)\n\n# API request - dry run query to estimate costs\ndry_run_query_job = client.query(sql, job_config=dry_run_config)\n\nprint(\"\\n This query will process {} Mega bytes.\".format(dry_run_query_job.total_bytes_processed\/1000000))\nresults.head()\n","3792b4ab":"## Beginner Query 4:-  Select group by, having more than 1000 upvotes\n\nsql = \"\"\"\n\n   select display_name,up_votes FROM `bigquery-public-data.stackoverflow.users`\n   group by display_name,up_votes having up_votes > 1000\n   limit 5\n        \n        \"\"\"\n\nresults = client.query(sql).to_dataframe()\n\n# Create a QueryJobConfig object to estimate size of query without running it\ndry_run_config = bigquery.QueryJobConfig(dry_run=True)\n\n# API request - dry run query to estimate costs\ndry_run_query_job = client.query(sql, job_config=dry_run_config)\n\nprint(\"\\n This query will process {} Mega bytes.\".format(dry_run_query_job.total_bytes_processed\/1000000))\nresults.head()\n","37a23d73":"## Begineer Query 5 :-  Select using order by , the top 3 user ids with min of 1000\n\nsql = \"\"\"\n\n   select display_name,up_votes FROM `bigquery-public-data.stackoverflow.users`\n   group by display_name,up_votes having up_votes > 1000\n   order by up_votes desc\n   limit 3\n        \n        \"\"\"\n\nresults = client.query(sql).to_dataframe()\n\n# Create a QueryJobConfig object to estimate size of query without running it\ndry_run_config = bigquery.QueryJobConfig(dry_run=True)\n\n# API request - dry run query to estimate costs\ndry_run_query_job = client.query(sql, job_config=dry_run_config)\n\nprint(\"\\n This query will process {} Mega bytes.\".format(dry_run_query_job.total_bytes_processed\/1000000))\nresults.head()\n","fabacef3":"## Beginner Query 6 :-  Rename the alias display_name as user_name\n\nsql = \"\"\"\n\n   select display_name as user_name FROM `bigquery-public-data.stackoverflow.users`\n   limit 3\n        \n        \"\"\"\n\nresults = client.query(sql).to_dataframe()\n\n# Create a QueryJobConfig object to estimate size of query without running it\ndry_run_config = bigquery.QueryJobConfig(dry_run=True)\n\n# API request - dry run query to estimate costs\ndry_run_query_job = client.query(sql, job_config=dry_run_config)\n\nprint(\"\\n This query will process {} Mega bytes.\".format(dry_run_query_job.total_bytes_processed\/1000000))\nresults.head()\n","f3766975":"## Beginner Query 7 :-  Rename the entire table using with\n\nsql = \"\"\"\n\nwith user_table as (\n   select display_name as user_name FROM `bigquery-public-data.stackoverflow.users`\n   limit 3)\n   select user_name from user_table\n   limit 1\n        \n        \"\"\"\n\nresults = client.query(sql).to_dataframe()\n\n# Create a QueryJobConfig object to estimate size of query without running it\ndry_run_config = bigquery.QueryJobConfig(dry_run=True)\n\n# API request - dry run query to estimate costs\ndry_run_query_job = client.query(sql, job_config=dry_run_config)\n\nprint(\"\\n This query will process {} Mega bytes.\".format(dry_run_query_job.total_bytes_processed\/1000000))\nresults.head()\n","73b349b3":"## Beginner Query 7 :-  Rename the entire table using with\n\nsql = \"\"\"\n\n\n   select ph.post_id\n   FROM `bigquery-public-data.stackoverflow.post_links` pl \n   join `bigquery-public-data.stackoverflow.post_history` ph\n   on pl.post_id=ph.post_id\n   group by ph.post_id\n   \n   limit 2\n\n        \n        \"\"\"\n\nresults = client.query(sql).to_dataframe()\n\n# Create a QueryJobConfig object to estimate size of query without running it\ndry_run_config = bigquery.QueryJobConfig(dry_run=True)\n\n# API request - dry run query to estimate costs\ndry_run_query_job = client.query(sql, job_config=dry_run_config)\n\nprint(\"\\n This query will process {} Mega bytes.\".format(dry_run_query_job.total_bytes_processed\/1000000))\nresults.head()\n","59f9bfad":"## Beginner Query 8 :-  Rename the entire table using with\n\nsql = \"\"\"\n\n\n   select ph.post_id\n   FROM `bigquery-public-data.stackoverflow.post_links` pl \n   join `bigquery-public-data.stackoverflow.post_history` ph\n   on pl.post_id=ph.post_id\n   group by ph.post_id\n   \n   limit 2\n\n        \n        \"\"\"\n\nresults = client.query(sql).to_dataframe()\n\n# Create a QueryJobConfig object to estimate size of query without running it\ndry_run_config = bigquery.QueryJobConfig(dry_run=True)\n\n# API request - dry run query to estimate costs\ndry_run_query_job = client.query(sql, job_config=dry_run_config)\n\nprint(\"\\n This query will process {} Mega bytes.\".format(dry_run_query_job.total_bytes_processed\/1000000))\nresults.head()\n","40677324":"## Beginner Query 9 :-  union function example\n\nsql = \"\"\"\n\nwith table_name as\n   (select ph.post_id\n   FROM `bigquery-public-data.stackoverflow.post_links` pl \n   join `bigquery-public-data.stackoverflow.post_history` ph\n   on pl.post_id=ph.post_id\n   group by ph.post_id\n   limit 2)\n   \n   select * from table_name\n   \n   union all\n   \n   select * from table_name\n   limit 1\n   \n\n\n\n        \n        \"\"\"\n\nresults = client.query(sql).to_dataframe()\n\n# Create a QueryJobConfig object to estimate size of query without running it\ndry_run_config = bigquery.QueryJobConfig(dry_run=True)\n\n# API request - dry run query to estimate costs\ndry_run_query_job = client.query(sql, job_config=dry_run_config)\n\nprint(\"\\n This query will process {} Mega bytes.\".format(dry_run_query_job.total_bytes_processed\/1000000))\nresults.head()\n","a2f57ccd":"## Beginner Query 10 :-  union function example\n\nsql = \"\"\"\n\nwith table_name as\n   (select ph.post_id,p.post_id\n   FROM `bigquery-public-data.stackoverflow.post_links` p\n   join `bigquery-public-data.stackoverflow.post_history` ph\n   on p.post_id=ph.post_id\n   group by ph.post_id,p.post_id\n   limit 2),\n  table_name_1 as\n   (select p.post_id,ph.post_id\n   FROM `bigquery-public-data.stackoverflow.post_links` p\n   join `bigquery-public-data.stackoverflow.post_history` ph\n   on p.post_id=ph.post_id\n   group by p.post_id,ph.post_id\n   limit 2) \n   \n  \n select * from table_name\n union all\n select * from table_name_1\n limit 2\n\n\n\n        \n        \"\"\"\n\nresults = client.query(sql).to_dataframe()\n\n# Create a QueryJobConfig object to estimate size of query without running it\ndry_run_config = bigquery.QueryJobConfig(dry_run=True)\n\n# API request - dry run query to estimate costs\ndry_run_query_job = client.query(sql, job_config=dry_run_config)\n\nprint(\"\\n This query will process {} Mega bytes.\".format(dry_run_query_job.total_bytes_processed\/1000000))\nresults.head()\n","0ad9ec1a":"## This is just an introduction to usage of BigQuery, if you'd like this dataset to tell more stories, please feel free to drop your inputs in the comments (For example: SQL queries you'd want in the dataset, problems you'd like to solve etc, new use cases, new approaches etc.\n\n## Please feel free to reach out for any communications related to this","91aa46c7":"# The goal of the notebook is to provide the audience a step-by-step learning approach in Big Query, the queries designed are targeted to all audiences(Beginner -> Advanced) and can serve as a one-stopper for all references related to Big Query.\n\n# The notebook is designed using the Kaggle SQL Courses as reference, we will follow a similar roadmap to sharpen our skills in SQL and BigQuery","0bfa2f4d":"# Beginner Query series","3caa4e27":"\n# Hey Kaggle Fam :)! The below is an exhaustive SQL EDA of the Stack OverFlow BigQuery Dataset,\n# If you'd like a buddy mentor for SQL and Data Analytics i'm just a ping away : )"}}