{"cell_type":{"82a0b20b":"code","7937e81e":"code","716f0f70":"code","22df8ec9":"code","19e0263c":"code","74ab29ae":"code","0edaece0":"code","74d576fa":"code","f9eb6571":"code","4b5078fe":"code","866317ed":"code","53dee0e8":"code","74ea526c":"code","3703a15f":"code","8166bc20":"code","5505aa09":"code","44111c9a":"code","a67d98a6":"code","2a884632":"code","71824c70":"code","ad987c65":"code","bfe71d7c":"code","1a7a72b5":"code","f7767577":"code","a2c48c70":"code","41b850bc":"code","7f1318e9":"code","5100f9c3":"code","8bb15e21":"code","68d7d4da":"code","0d89b292":"code","d6fed559":"code","a3b26fd6":"markdown","db61dddb":"markdown","bc3e4345":"markdown","e7116e8f":"markdown","2deb9436":"markdown","da625e84":"markdown","97a7249c":"markdown","79e22f91":"markdown","3fa9101c":"markdown","a46975c2":"markdown","6167d6ea":"markdown","cfa75a0f":"markdown","4b6fd8bf":"markdown","10f568fc":"markdown","b17f2b9b":"markdown","088ec6c3":"markdown","460df1bb":"markdown","982583ba":"markdown","84ff7649":"markdown","852bdca1":"markdown","59663779":"markdown","682baec3":"markdown","906ee902":"markdown","db08d4e2":"markdown","393cfc33":"markdown","46085ef4":"markdown","59756c87":"markdown","7f33b0f9":"markdown","6f2b98d4":"markdown","83707836":"markdown","8094fdb6":"markdown","0d9851ca":"markdown","7c6ca385":"markdown","69bcd50e":"markdown","d88be2f6":"markdown","184e8c09":"markdown","4c060b1d":"markdown"},"source":{"82a0b20b":"# numpy\nimport numpy as np\n\n# pandas stuff\nimport pandas as pd\npd.set_option('display.max_rows', None)\npd.set_option('display.max_columns', None)\n\n# plotting stuff\nfrom pandas.plotting import lag_plot\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly.express as px\nimport plotly.graph_objects as go\ncolorMap = sns.light_palette(\"blue\", as_cmap=True)\n#plt.rcParams.update({'font.size': 12})\n\n\n# install dabl\n!pip install dabl > \/dev\/null\nimport dabl\n# install datatable\n!pip install datatable > \/dev\/null\nimport datatable as dt\n\n# misc\nimport missingno as msno\n\n# system\nimport warnings\nwarnings.filterwarnings('ignore')\n# for the image import\nimport os\nfrom IPython.display import Image\n# garbage collector to keep RAM in check\nimport gc","7937e81e":"!wc -l ..\/input\/jane-street-market-prediction\/train.csv","716f0f70":"train_data_datatable = dt.fread('..\/input\/jane-street-market-prediction\/train.csv')\ntrain_data = train_data_datatable.to_pandas()","22df8ec9":"fig, ax = plt.subplots(figsize=(15, 5))\nbalance= pd.Series(train_data['resp']).cumsum()\nax.set_xlabel (\"Trade\", fontsize=18)\nax.set_ylabel (\"Cumulative resp\", fontsize=18);\nbalance.plot(lw=3);\ndel balance\ngc.collect();","19e0263c":"fig, ax = plt.subplots(figsize=(15, 5))\nbalance= pd.Series(train_data['resp']).cumsum()\nresp_1= pd.Series(train_data['resp_1']).cumsum()\nresp_2= pd.Series(train_data['resp_2']).cumsum()\nresp_3= pd.Series(train_data['resp_3']).cumsum()\nresp_4= pd.Series(train_data['resp_4']).cumsum()\nax.set_xlabel (\"Trade\", fontsize=18)\nax.set_title (\"Cumulative resp and time horizons 1, 2, 3, and 4 (500 days)\", fontsize=18)\nbalance.plot(lw=3)\nresp_1.plot(lw=3)\nresp_2.plot(lw=3)\nresp_3.plot(lw=3)\nresp_4.plot(lw=3)\nplt.legend(loc=\"upper left\");\ndel resp_1\ndel resp_2\ndel resp_3\ndel resp_4\ngc.collect();","74ab29ae":"plt.figure(figsize = (12,5))\nax = sns.distplot(train_data['resp'], \n             bins=3000, \n             kde_kws={\"clip\":(-0.05,0.05)}, \n             hist_kws={\"range\":(-0.05,0.05)},\n             color='darkcyan', \n             kde=False);\nvalues = np.array([rec.get_height() for rec in ax.patches])\nnorm = plt.Normalize(values.min(), values.max())\ncolors = plt.cm.jet(norm(values))\nfor rec, col in zip(ax.patches, colors):\n    rec.set_color(col)\nplt.xlabel(\"Histogram of the resp values\", size=14)\nplt.show();\ngc.collect();","0edaece0":"min_resp = train_data['resp'].min()\nprint('The minimum value for resp is: %.5f' % min_resp)\nmax_resp = train_data['resp'].max()\nprint('The maximum value for resp is:  %.5f' % max_resp)","74d576fa":"print(\"Skew of resp is:      %.2f\" %train_data['resp'].skew() )\nprint(\"Kurtosis of resp is: %.2f\"  %train_data['resp'].kurtosis() )","f9eb6571":"from scipy.optimize import curve_fit\n# the values\nx = list(range(len(values)))\nx = [((i)-1500)\/30000 for i in x]\ny = values\n\ndef Lorentzian(x, x0, gamma, A):\n    return A * gamma**2\/(gamma**2+( x - x0 )**2)\n\n# seed guess\ninitial_guess=(0, 0.001, 3000)\n\n# the fit\nparameters,covariance=curve_fit(Lorentzian,x,y,initial_guess)\nsigma=np.sqrt(np.diag(covariance))\n\n# and plot\nplt.figure(figsize = (12,5))\nax = sns.distplot(train_data['resp'], \n             bins=3000, \n             kde_kws={\"clip\":(-0.05,0.05)}, \n             hist_kws={\"range\":(-0.05,0.05)},\n             color='darkcyan', \n             kde=False);\nvalues = np.array([rec.get_height() for rec in ax.patches])\n#norm = plt.Normalize(values.min(), values.max())\n#colors = plt.cm.jet(norm(values))\n#for rec, col in zip(ax.patches, colors):\n#    rec.set_color(col)\nplt.xlabel(\"Histogram of the resp values\", size=14)\nplt.plot(x,Lorentzian(x,*parameters),'--',color='black',lw=3)\nplt.show();\ndel values\ngc.collect();","4b5078fe":"percent_zeros = (100\/train_data.shape[0])*((train_data.weight.values == 0).sum())\nprint('Percentage of zero weights is: %i' % percent_zeros +\"%\")","866317ed":"max_weight = train_data['weight'].max()\nprint('The maximum weight was: %.2f' % max_weight)","53dee0e8":"train_data[train_data['weight']==train_data['weight'].max()]","74ea526c":"plt.figure(figsize = (12,5))\nax = sns.distplot(train_data['weight'], \n             bins=1400, \n             kde_kws={\"clip\":(0.001,1.4)}, \n             hist_kws={\"range\":(0.001,1.4)},\n             color='darkcyan', \n             kde=False);\nvalues = np.array([rec.get_height() for rec in ax.patches])\nnorm = plt.Normalize(values.min(), values.max())\ncolors = plt.cm.jet(norm(values))\nfor rec, col in zip(ax.patches, colors):\n    rec.set_color(col)\nplt.xlabel(\"Histogram of non-zero weights\", size=14)\nplt.show();\ndel values\ngc.collect();","3703a15f":"train_data['weight_resp']   = train_data['weight']*train_data['resp']\ntrain_data['weight_resp_1'] = train_data['weight']*train_data['resp_1']\ntrain_data['weight_resp_2'] = train_data['weight']*train_data['resp_2']\ntrain_data['weight_resp_3'] = train_data['weight']*train_data['resp_3']\ntrain_data['weight_resp_4'] = train_data['weight']*train_data['resp_4']\n\nfig, ax = plt.subplots(figsize=(15, 5))\nresp    = pd.Series(1+(train_data.groupby('date')['weight_resp'].mean())).cumprod()\nresp_1  = pd.Series(1+(train_data.groupby('date')['weight_resp_1'].mean())).cumprod()\nresp_2  = pd.Series(1+(train_data.groupby('date')['weight_resp_2'].mean())).cumprod()\nresp_3  = pd.Series(1+(train_data.groupby('date')['weight_resp_3'].mean())).cumprod()\nresp_4  = pd.Series(1+(train_data.groupby('date')['weight_resp_4'].mean())).cumprod()\nax.set_xlabel (\"Day\", fontsize=18)\nax.set_title (\"Cumulative daily return for resp and time horizons 1, 2, 3, and 4 (500 days)\", fontsize=18)\nresp.plot(lw=3, label='resp x weight')\nresp_1.plot(lw=3, label='resp_1 x weight')\nresp_2.plot(lw=3, label='resp_2 x weight')\nresp_3.plot(lw=3, label='resp_3 x weight')\nresp_4.plot(lw=3, label='resp_4 x weight')\n# day 85 marker\nax.axvline(x=85, linestyle='--', alpha=0.3, c='red', lw=1)\nax.axvspan(0, 85 , color=sns.xkcd_rgb['grey'], alpha=0.1)\nplt.legend(loc=\"lower left\");","8166bc20":"train_data_no_0 = train_data.query('weight > 0').reset_index(drop = True)\ntrain_data_no_0['wAbsResp'] = train_data_no_0['weight'] * (train_data_no_0['resp'])\n#plot\nplt.figure(figsize = (12,5))\nax = sns.distplot(train_data_no_0['wAbsResp'], \n             bins=1500, \n             kde_kws={\"clip\":(-0.02,0.02)}, \n             hist_kws={\"range\":(-0.02,0.02)},\n             color='darkcyan', \n             kde=False);\nvalues = np.array([rec.get_height() for rec in ax.patches])\nnorm = plt.Normalize(values.min(), values.max())\ncolors = plt.cm.jet(norm(values))\nfor rec, col in zip(ax.patches, colors):\n    rec.set_color(col)\nplt.xlabel(\"Histogram of the weights * resp\", size=14)\nplt.show();","5505aa09":"trades_per_day = train_data.groupby(['date'])['ts_id'].count()\nfig, ax = plt.subplots(figsize=(15, 5))\nplt.plot(trades_per_day)\nax.set_xlabel (\"Day\", fontsize=18)\nax.set_title (\"Total number of ts_id for each day\", fontsize=18)\n# day 85 marker\nax.axvline(x=85, linestyle='--', alpha=0.3, c='red', lw=1)\nax.axvspan(0, 85 , color=sns.xkcd_rgb['grey'], alpha=0.1)\nax.set_xlim(xmin=0)\nax.set_xlim(xmax=500)\nplt.show()","44111c9a":"fig, ax = plt.subplots(figsize=(15, 5))\nplt.plot(23400\/trades_per_day)\nax.set_xlabel (\"Day\", fontsize=18)\nax.set_ylabel (\"Av. time between trades (s)\", fontsize=18)\nax.set_title (\"Average time between trades for each day\", fontsize=18)\nax.axvline(x=85, linestyle='--', alpha=0.3, c='red', lw=1)\nax.axvspan(0, 85 , color=sns.xkcd_rgb['grey'], alpha=0.1)\nax.set_xlim(xmin=0)\nax.set_xlim(xmax=500)\nax.set_ylim(ymin=0)\nax.set_ylim(ymax=12)\nplt.show()","a67d98a6":"plt.figure(figsize = (12,4))\n# the minimum has been set to 1000 so as not to draw the partial days like day 2 and day 294\n# the maximum number of trades per day is 18884\n# I have used 125 bins for the 500 days\nax = sns.distplot(trades_per_day, \n             bins=125, \n             kde_kws={\"clip\":(1000,20000)}, \n             hist_kws={\"range\":(1000,20000)},\n             color='darkcyan', \n             kde=True);\nvalues = np.array([rec.get_height() for rec in ax.patches])\nnorm = plt.Normalize(values.min(), values.max())\ncolors = plt.cm.jet(norm(values))\nfor rec, col in zip(ax.patches, colors):\n    rec.set_color(col)\nplt.xlabel(\"Number of trades per day\", size=14)\nplt.show();","2a884632":"train_data['feature_0'].value_counts()","71824c70":"fig, ax = plt.subplots(figsize=(15, 4))\nfeature_0 = pd.Series(train_data['feature_0']).cumsum()\nax.set_xlabel (\"Trade\", fontsize=18)\nax.set_ylabel (\"feature_0 (cumulative)\", fontsize=18);\nfeature_0.plot(lw=3);","ad987c65":"feature_0_is_plus_one  = train_data.query('feature_0 ==  1').reset_index(drop = True)\nfeature_0_is_minus_one = train_data.query('feature_0 == -1').reset_index(drop = True)\n# the plot\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 4))\nax1.plot((pd.Series(feature_0_is_plus_one['resp']).cumsum()), lw=3, label='resp')\nax1.plot((pd.Series(feature_0_is_plus_one['resp']*feature_0_is_plus_one['weight']).cumsum()), lw=3, label='return')\nax2.plot((pd.Series(feature_0_is_minus_one['resp']).cumsum()), lw=3, label='resp')\nax2.plot((pd.Series(feature_0_is_minus_one['resp']*feature_0_is_minus_one['weight']).cumsum()), lw=3, label='return')\nax1.set_title (\"feature 0 = 1\", fontsize=18)\nax2.set_title (\"feature 0 = -1\", fontsize=18)\nax1.legend(loc=\"lower left\")\nax2.legend(loc=\"upper left\");\n\ndel feature_0_is_plus_one\ndel feature_0_is_minus_one\ngc.collect();","bfe71d7c":"fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2,figsize=(20,10))\n\nax1.plot((pd.Series(train_data['feature_1']).cumsum()), lw=3, color='red')\nax1.set_title (\"Linear\", fontsize=22);\nax1.axvline(x=514052, linestyle='--', alpha=0.3, c='green', lw=2)\nax1.axvspan(0, 514052 , color=sns.xkcd_rgb['grey'], alpha=0.1)\nax1.set_xlim(xmin=0)\nax1.set_ylabel (\"feature_1\", fontsize=18);\n\nax2.plot((pd.Series(train_data['feature_3']).cumsum()), lw=3, color='green')\nax2.set_title (\"Noisy\", fontsize=22);\nax2.axvline(x=514052, linestyle='--', alpha=0.3, c='red', lw=2)\nax2.axvspan(0, 514052 , color=sns.xkcd_rgb['grey'], alpha=0.1)\nax2.set_xlim(xmin=0)\nax2.set_ylabel (\"feature_3\", fontsize=18);\n\nax3.plot((pd.Series(train_data['feature_55']).cumsum()), lw=3, color='darkorange')\nax3.set_title (\"Hybryd (Tag 21)\", fontsize=22);\nax3.set_xlabel (\"Trade\", fontsize=18)\nax3.axvline(x=514052, linestyle='--', alpha=0.3, c='green', lw=2)\nax3.axvspan(0, 514052 , color=sns.xkcd_rgb['grey'], alpha=0.1)\nax3.set_xlim(xmin=0)\nax3.set_ylabel (\"feature_55\", fontsize=18);\n\nax4.plot((pd.Series(train_data['feature_73']).cumsum()), lw=3, color='blue')\nax4.set_title (\"Negative\", fontsize=22)\nax4.set_xlabel (\"Trade\", fontsize=18)\nax4.set_ylabel (\"feature_73\", fontsize=18);\ngc.collect();","1a7a72b5":"day_0 = train_data.loc[train_data['date'] == 0]\nday_1 = train_data.loc[train_data['date'] == 1]\nday_3 = train_data.loc[train_data['date'] == 3]\nthree_days = pd.concat([day_0, day_1, day_3])\nthree_days.plot.scatter(x='ts_id', y='feature_41', s=0.5, figsize=(15,3));\nthree_days.plot.scatter(x='ts_id', y='feature_42', s=0.5, figsize=(15,3));\nthree_days.plot.scatter(x='ts_id', y='feature_43', s=0.5, figsize=(15,3));\ndel day_1\ndel day_3\ngc.collect();","f7767577":"fig, ax = plt.subplots(1, 3, figsize=(17, 4))\nlag_plot(day_0['feature_41'], lag=1, s=0.5, ax=ax[0])\nlag_plot(day_0['feature_42'], lag=1, s=0.5, ax=ax[1])\nlag_plot(day_0['feature_43'], lag=1, s=0.5, ax=ax[2])\nax[0].title.set_text('feature_41')\nax[0].set_xlabel(\"ts_id (n)\")\nax[0].set_ylabel(\"ts_id (n+1)\")\nax[1].title.set_text('feature_42')\nax[1].set_xlabel(\"ts_id (n)\")\nax[1].set_ylabel(\"ts_id (n+1)\")\nax[2].title.set_text('feature_43')\nax[2].set_xlabel(\"ts_id (n)\")\nax[2].set_ylabel(\"ts_id (n+1)\")\n\nax[0].plot(0, 0, 'r.', markersize=15.0)\nax[1].plot(0, 0, 'r.', markersize=15.0)\nax[2].plot(0, 0, 'r.', markersize=15.0);\ngc.collect();","a2c48c70":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","41b850bc":"import os\nimport time\nimport pickle\nimport random\nimport numpy as np\nimport pandas as pd\nfrom tqdm import tqdm\nfrom collections import namedtuple\nfrom sklearn.metrics import log_loss, roc_auc_score\n\nfrom random import choices\n\nimport torch\nimport torch.nn as nn\nfrom torch.autograd import Variable\nfrom torch.utils.data import DataLoader\nfrom torch.nn import CrossEntropyLoss, MSELoss\nfrom torch.nn.modules.loss import _WeightedLoss\nimport torch.nn.functional as F\n\nimport warnings\nwarnings.filterwarnings (\"ignore\")\n\npd.set_option('display.max_columns', 100)\npd.set_option('display.max_rows', 100)\n\nDATA_PATH = '..\/input\/jane-street-market-prediction\/'\n\nNFOLDS = 5\n\nTRAIN = False\nCACHE_PATH = '..\/input\/mlp012003weights'\n\ndef save_pickle(dic, save_path):\n    with open(save_path, 'wb') as f:\n        pickle.dump(dic, f)\n\ndef load_pickle(load_path):\n    with open(load_path, 'rb') as f:\n        message_dict = pickle.load(f)\n    return message_dict\n\nfeat_cols = [f'feature_{i}' for i in range(130)]\n\ntarget_cols = ['action', 'action_1', 'action_2', 'action_3', 'action_4']\n\nf_mean = np.load(f'{CACHE_PATH}\/f_mean_online.npy')\n\n##### Making features\nall_feat_cols = [col for col in feat_cols]\nall_feat_cols.extend(['cross_41_42_43', 'cross_1_2'])\n\n##### Model&Data fnc\nclass Model(nn.Module):\n    def __init__(self):\n        super(Model, self).__init__()\n        self.batch_norm0 = nn.BatchNorm1d(len(all_feat_cols))\n        self.dropout0 = nn.Dropout(0.2)\n\n        dropout_rate = 0.2\n        hidden_size = 256\n        self.dense1 = nn.Linear(len(all_feat_cols), hidden_size)\n        self.batch_norm1 = nn.BatchNorm1d(hidden_size)\n        self.dropout1 = nn.Dropout(dropout_rate)\n\n        self.dense2 = nn.Linear(hidden_size+len(all_feat_cols), hidden_size)\n        self.batch_norm2 = nn.BatchNorm1d(hidden_size)\n        self.dropout2 = nn.Dropout(dropout_rate)\n\n        self.dense3 = nn.Linear(hidden_size+hidden_size, hidden_size)\n        self.batch_norm3 = nn.BatchNorm1d(hidden_size)\n        self.dropout3 = nn.Dropout(dropout_rate)\n\n        self.dense4 = nn.Linear(hidden_size+hidden_size, hidden_size)\n        self.batch_norm4 = nn.BatchNorm1d(hidden_size)\n        self.dropout4 = nn.Dropout(dropout_rate)\n\n        self.dense5 = nn.Linear(hidden_size+hidden_size, len(target_cols))\n\n        self.Relu = nn.ReLU(inplace=True)\n        self.PReLU = nn.PReLU()\n        self.LeakyReLU = nn.LeakyReLU(negative_slope=0.01, inplace=True)\n        self.RReLU = nn.RReLU()\n\n    def forward(self, x):\n        x = self.batch_norm0(x)\n        x = self.dropout0(x)\n\n        x1 = self.dense1(x)\n        x1 = self.batch_norm1(x1)\n        x1 = self.LeakyReLU(x1)\n        x1 = self.dropout1(x1)\n\n        x = torch.cat([x, x1], 1)\n\n        x2 = self.dense2(x)\n        x2 = self.batch_norm2(x2)\n        x2 = self.LeakyReLU(x2)\n        x2 = self.dropout2(x2)\n\n        x = torch.cat([x1, x2], 1)\n\n        x3 = self.dense3(x)\n        x3 = self.batch_norm3(x3)\n        x3 = self.LeakyReLU(x3)\n        x3 = self.dropout3(x3)\n\n        x = torch.cat([x2, x3], 1)\n\n        x4 = self.dense4(x)\n        x4 = self.batch_norm4(x4)\n        x4 = self.LeakyReLU(x4)\n        x4 = self.dropout4(x4)\n\n        x = torch.cat([x3, x4], 1)\n\n        x = self.dense5(x)\n\n        return x\n\nif True:\n    device = torch.device(\"cpu\")\n    \n    model_list = []\n    tmp = np.zeros(len(feat_cols))\n    for _fold in range(NFOLDS):\n        torch.cuda.empty_cache()\n        model = Model()\n        model.to(device)\n        model_weights = f\"{CACHE_PATH}\/online_model{_fold}.pth\"\n        model.load_state_dict(torch.load(model_weights, map_location=torch.device('cpu')))\n        model.eval()\n        model_list.append(model)","7f1318e9":"!ls ..\/input\/jane-street-with-keras-nn-overfit","5100f9c3":"from tensorflow.keras.layers import Input, Dense, BatchNormalization, Dropout, Concatenate, Lambda, GaussianNoise, Activation\nfrom tensorflow.keras.models import Model, Sequential\nfrom tensorflow.keras.losses import BinaryCrossentropy\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom tensorflow.keras.layers.experimental.preprocessing import Normalization\nimport tensorflow as tf\nimport tensorflow_addons as tfa\n\nSEED = 1111\n\nnp.random.seed(SEED)\n\n# fit\ndef create_mlp(\n    num_columns, num_labels, hidden_units, dropout_rates, label_smoothing, learning_rate\n):\n\n    inp = tf.keras.layers.Input(shape=(num_columns,))\n    x = tf.keras.layers.BatchNormalization()(inp)\n    x = tf.keras.layers.Dropout(dropout_rates)(x)\n    for i in range(3):\n        x = tf.keras.layers.Dense(hidden_units)(x)\n        x = tf.keras.layers.BatchNormalization()(x)\n        x = tf.keras.layers.Activation(tf.keras.activations.swish)(x)\n        x = tf.keras.layers.Dropout(dropout_rates)(x)\n    \n    x = tf.keras.layers.Dense(num_labels)(x)\n    out = tf.keras.layers.Activation(\"sigmoid\")(x)\n\n    model = tf.keras.models.Model(inputs=inp, outputs=out)\n    model.compile(\n        optimizer=tfa.optimizers.RectifiedAdam(learning_rate=learning_rate),\n        loss=tf.keras.losses.BinaryCrossentropy(label_smoothing=label_smoothing),\n        metrics=tf.keras.metrics.AUC(name=\"AUC\"),\n    )\n\n    return model\n\nepochs = 200\nbatch_size = 4096\nhidden_units = 160\ndropout_rates = 0.2\nlabel_smoothing = 1e-2\nlearning_rate = 1e-3\n\ntf.keras.backend.clear_session()\ntf.random.set_seed(SEED)\nclf = create_mlp(\n    len(feat_cols), 5, hidden_units, dropout_rates, label_smoothing, learning_rate\n    )\nclf.load_weights('..\/input\/jane-street-with-keras-nn-overfit\/model.h5')\n\ntf_models = [clf]","8bb15e21":"N_FEAT_TAGS = 29\nDEVICE = device\nN_FEATURES = 130\nTHREE_HIDDEN_LAYERS = [400, 400, 400]\n\nclass FFN (nn.Module):\n    \n    def __init__(self, inputCount=130, outputCount=5, hiddenLayerCounts=[150, 150, 150], \n                 drop_prob=0.2, nonlin=nn.SiLU (), isOpAct=False):\n        \n        super(FFN, self).__init__()\n        \n        self.nonlin     = nonlin\n        self.dropout    = nn.Dropout (drop_prob)\n        self.batchnorm0 = nn.BatchNorm1d (inputCount)\n        self.dense1     = nn.Linear (inputCount, hiddenLayerCounts[0])\n        self.batchnorm1 = nn.BatchNorm1d (hiddenLayerCounts[0])\n        self.dense2     = nn.Linear(hiddenLayerCounts[0], hiddenLayerCounts[1])\n        self.batchnorm2 = nn.BatchNorm1d (hiddenLayerCounts[1])\n        self.dense3     = nn.Linear(hiddenLayerCounts[1], hiddenLayerCounts[2])\n        self.batchnorm3 = nn.BatchNorm1d (hiddenLayerCounts[2])        \n        self.outDense   = None\n        if outputCount > 0:\n            self.outDense   = nn.Linear (hiddenLayerCounts[-1], outputCount)\n        self.outActivtn = None\n        if isOpAct:\n            if outputCount == 1 or outputCount == 2:\n                self.outActivtn = nn.Sigmoid ()\n            elif outputCount > 0:\n                self.outActivtn = nn.Softmax (dim=-1)\n        return\n\n    def forward (self, X):\n        \n        # X = self.dropout (self.batchnorm0 (X))\n        X = self.batchnorm0 (X)\n        X = self.dropout (self.nonlin (self.batchnorm1 (self.dense1 (X))))\n        X = self.dropout (self.nonlin (self.batchnorm2 (self.dense2 (X))))\n        X = self.dropout (self.nonlin (self.batchnorm3 (self.dense3 (X))))\n        if self.outDense:\n            X = self.outDense (X)\n        if self.outActivtn:\n            X = self.outActivtn (X)\n        return X\n    \n    \nclass Emb_NN_Model (nn.Module):\n    \n    def __init__(self, three_hidden_layers=THREE_HIDDEN_LAYERS, embed_dim=(N_FEAT_TAGS),\n                 csv_file='..\/input\/jane-street-market-prediction\/features.csv'):\n        \n        super (Emb_NN_Model, self).__init__()\n        global N_FEAT_TAGS\n        N_FEAT_TAGS = 29\n        dtype = {'tag_0' : 'int8'}\n        for i in range (1, 29):\n            k = 'tag_' + str (i)\n            dtype[k] = 'int8'\n        t_df = pd.read_csv (csv_file, usecols=range (1,N_FEAT_TAGS+1), dtype=dtype)\n        t_df['tag_29'] = np.array ([1] + ([0] * (t_df.shape[0]-1)) ).astype ('int8')\n        self.features_tag_matrix = torch.tensor (t_df.to_numpy ())\n        N_FEAT_TAGS += 1\n        \n        self.embed_dim     = embed_dim\n        self.tag_embedding = nn.Embedding (N_FEAT_TAGS+1, embed_dim)\n        self.tag_weights   = nn.Linear (N_FEAT_TAGS, 1)\n        \n        drop_prob          = 0.5\n        self.ffn           = FFN (inputCount=(130+embed_dim), outputCount=0,\n                                  hiddenLayerCounts=[(three_hidden_layers[0]+embed_dim),\n                                                     (three_hidden_layers[1]+embed_dim),\n                                                     (three_hidden_layers[2]+embed_dim)],\n                                  drop_prob=drop_prob)\n        self.outDense      = nn.Linear (three_hidden_layers[2]+embed_dim, 5)\n        return\n    \n    def features2emb (self):\n        \n        all_tag_idxs = torch.LongTensor (np.arange (N_FEAT_TAGS))\n        tag_bools    = self.features_tag_matrix\n        f_emb        = self.tag_embedding (all_tag_idxs).repeat (130, 1, 1)\n        f_emb        = f_emb * tag_bools[:, :, None]\n        s            = torch.sum (tag_bools, dim=1)       \n        f_emb        = torch.sum (f_emb, dim=-2) \/ s[:, None]\n        return f_emb\n    \n    def forward (self, cat_featrs, features):\n        \n        cat_featrs = None\n        features   = features.view (-1, N_FEATURES)\n        f_emb      = self.features2emb ()\n        features_2 = torch.matmul (features, f_emb)\n        features   = torch.hstack ((features, features_2))\n        x          = self.ffn (features)\n        out_logits = self.outDense (x)\n        return out_logits","68d7d4da":"embNN_model = Emb_NN_Model ()\n\ntry:\n    embNN_model.load_state_dict (torch.load (\"..\/input\/jane-embnn5-auc-400-400-400\/Jane_EmbNN5_auc_400_400_400.pth\"))\nexcept:\n    embNN_model.load_state_dict (torch.load (\"..\/input\/jane-embnn5-auc-400-400-400\/Jane_EmbNN5_auc_400_400_400.pth\", map_location='cpu'))\n    \nembNN_model = embNN_model.eval ()","0d89b292":"import janestreet\nenv = janestreet.make_env()\nenv_iter = env.iter_test()","d6fed559":"if True:\n\n    for (test_df, pred_df) in tqdm(env_iter):\n        if test_df['weight'].item() > 0:\n            x_tt = test_df.loc[:, feat_cols].values\n            if np.isnan(x_tt.sum()):\n                x_tt = np.nan_to_num(x_tt) + np.isnan(x_tt) * f_mean\n\n            cross_41_42_43 = x_tt[:, 41] + x_tt[:, 42] + x_tt[:, 43]\n            cross_1_2 = x_tt[:, 1] \/ (x_tt[:, 2] + 1e-5)\n            feature_inp = np.concatenate((\n                x_tt,\n                np.array(cross_41_42_43).reshape(x_tt.shape[0], 1),\n                np.array(cross_1_2).reshape(x_tt.shape[0], 1),\n            ), axis=1)\n\n            # torch_pred\n            torch_pred = np.zeros((1, len(target_cols)))\n            for model in model_list:\n                torch_pred += model(torch.tensor(feature_inp, dtype=torch.float).to(device)).sigmoid().detach().cpu().numpy() \/ NFOLDS\n            torch_pred = np.median(torch_pred)\n            \n            # tf_pred\n            tf_pred = np.median(np.mean([model(x_tt, training = False).numpy() for model in tf_models],axis=0))\n            \n            # torch embedding_NN pred\n            x_tt    = torch.tensor (x_tt).float ().view (-1, 130)\n            embnn_p = np.median (torch.sigmoid (embNN_model (None, x_tt)).detach ().cpu ().numpy ().reshape ((-1, 5)), axis=1)\n            \n            # avg\n            pred_pr = torch_pred*0.4 + tf_pred*0.4 + embnn_p*0.2\n            \n            pred_df.action = np.where (pred_pr >= 0.5, 1, 0).astype (int)\n        else:\n            pred_df.action = 0\n        env.predict(pred_df)","a3b26fd6":"**as well as four time horizons**","db61dddb":"Let us plot the number of ts_id per day. Note: I have taken to drawing a vertical dashed line in my plots because I started to wonder did Jane Street modify their trading model around day 85? Thanks to comments on that forum the general consenus seems to be that a change in the market took place around that time (perhaps a mean reverting market changing to a momentum market, or vice versa).","bc3e4345":"Also, feature_0 is the only feature in the features.csv file that has no True tags.","e7116e8f":"**feature_{1...129}**","2deb9436":"# **5.Cumulative return**","da625e84":"# **Inference**","97a7249c":"**Finally, let us fit a Cauchy distribution to this data**","79e22f91":"**Let us also calculate the skew and kurtosis of this distribution:**","3fa9101c":"**Let us take a look at a histogram of the non-zero weights**","a46975c2":"**\"Machine learning (ML) at Jane Street begins, unsurprisingly, with data. We collect and store around 2.3TB of market data every day. Hidden in those petabytes of data are the relationships and statistical regularities which inform the models inside our strategies. But it\u2019s not just awesome models. ML work in a production environment like Jane Street\u2019s involves many interconnected pieces.\" -- Jane Street Tech Blog \"Real world machine learning\".**","6167d6ea":"# **2.The train.csv file is big**","cfa75a0f":"# **4.Weight**","4b6fd8bf":"**We can see that it has a total of 2,390,492 rows. I recommend reading this magnificent Tutorial on reading large datasets by Vopani.\n\nI have used pandas to load in the train.csv and it took almost 2 minutes. To speed things up here I shall use datatable:**","10f568fc":"# **Tensorflow part**","b17f2b9b":"If we assume a trading day is 6\u00bd hours long (i.e. 23400 seconds) then","088ec6c3":"**An now to find the maximum weight used**","460df1bb":"# **3.Deal with feather---resp**","982583ba":"\nThere seem to be four general 'types' of features, here is a plot of an example of one of each:","84ff7649":"# **8.Data prediction and processing results**","852bdca1":"We can see that the shortest time horizons, resp_1, resp_2 and resp_3, representing a more conservative strategy, result in the lowest return.\n\nWe shall now plot a histogram of the weight multiplied by the value of resp (after removing the 0 weights)","59663779":"The Tag 14 set are interesting as they appear to be \"stratified\"; only adopting discrete values throughout the day (could these be a value of a security?). Here are scatter plots of these three features for days 0, 1 and 3 (Note that I have omitted day 2, which I shall discuss in the missing data section below):","682baec3":"**which occured on day 446**","906ee902":"Each trade has an associated weight and resp, which together represents a return on the trade. Trades with weight = 0 were intentionally included in the dataset for completeness, although such trades will not contribute towards the scoring evaluation.","db08d4e2":"\"This dataset contains an anonymized set of features, feature_{0...129}, representing real stock market data.\"","393cfc33":"# **6.Time**","46085ef4":"Let us take a look at the cumulative daily return over time, which is given by weight multiplied by the value of resp","59756c87":"**This distribution has very long tails**","7f33b0f9":"# **EMbeddings NN**","6f2b98d4":"# **7.The features**","83707836":"# **1.Import**","8094fdb6":"# **Pytorch Resnet part**","0d9851ca":"**There are a total of 500 days of data in train.csv (i.e. two years of trading data). Let us take a look at the cumulative values of resp over time**","7c6ca385":"Here is a histogram of the number of trades per day (it has been suggested that the number of trades per day is an indication of the volatility that day)","69bcd50e":"It is also very interesting to plot the cumulative resp and return (resp*weight) for feature_0 = +1 and feature_0 = -1 individually (Credit: \"An observation about feature_0\" by therocket290)","d88be2f6":"These three features also have very interesting lag plots, where we plot the value of the feature at ts_id  (n)  with respect to the next value of the feature, i.e. at ts_id  (n+1) , (here for day 0). Red markers have been placed at (0,0) as a visual aid.","184e8c09":"# **Jane Street Market Prediction: A simple EDA and prediction**\n","4c060b1d":"**Let us now plot a histogram of all of the resp values (here only shown for values between -0.05 and 0.05)**"}}