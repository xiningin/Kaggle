{"cell_type":{"bdd5e057":"code","4fefa606":"code","97e48f30":"code","66b23a1a":"code","d500f9dc":"code","bf8151e9":"code","2a31e24e":"code","a1266e29":"code","96de2753":"code","810c1f6f":"code","4353b3be":"code","8e84a392":"code","17b5dece":"markdown","56ecae7b":"markdown","f7834beb":"markdown","ccb5599c":"markdown","ab80b760":"markdown","e254bf7c":"markdown","995313ad":"markdown","298ab64c":"markdown","f500c9b6":"markdown","1fada0d4":"markdown","298a1f67":"markdown","281cbb95":"markdown","31136e73":"markdown"},"source":{"bdd5e057":"from os import walk\nfrom os import path\n# --------- Data processing --------- #\nimport numpy as np # linear algebra\nimport pandas as pd\nfrom keras.utils import to_categorical\nfrom keras.preprocessing.text import Tokenizer\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\n# --------- Model design --------- #\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom keras.layers import Dense, Dropout, BatchNormalization\nfrom keras.regularizers import l2\nfrom keras import Sequential\n# --------- Visualization --------- #\nfrom wordcloud import WordCloud\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport seaborn as sns","4fefa606":"data_path = []\nfor dirname, _, filenames in walk('\/kaggle\/input'):\n    for filename in filenames:\n        data_path.append(path.join(dirname, filename))\n\ndata_path = iter(data_path)\n# add all csv in to dataframe\ndataframes = (pd.read_csv(data) for data in data_path)\n\n# show all dataframe in one dataframe\ndf = pd.concat(dataframes, ignore_index=True, sort=False)\n\n# drop useless cloumn\ndf.drop(['Unnamed: 0', 'url', 'id', 'year', 'month'], axis=1, inplace=True)","97e48f30":"del dataframes, data_path, dirname, filename, filenames","66b23a1a":"print('\\033[93m Info:\\033[30m')\nprint(df.info())\nprint('\\033[93m Nan of each column:\\033[30m')\nprint(df.isna().sum())\nprint('\\033[93m Count of unique author:\\033[30m', end=' ')\nprint(len(df.author.unique()))\nprint('\\033[91m NOTE: we use only top 10 of authors\\033[30m')\n\n# Preview data\ndf.head()","d500f9dc":"# we use only top 10 of authors for classification\ntop_wirter = df.author.value_counts()[:10] \n\n# Drop rows that are not in the top_writer\ndf_top = pd.DataFrame(columns=df.columns)\nfor i in top_wirter.keys():\n    df_top = df_top.append(df[df.author == i], ignore_index=True)\n\ndf = df_top","bf8151e9":"del df_top, i","2a31e24e":"f, (ax1, ax2) = plt.subplots(2, 1, figsize=(10,15))\nax1.pie(top_wirter.values.tolist(), labels=df.author.unique(), autopct=\"%.1f%%\")\nsns.histplot(df.author, ax=ax2)\nax2.tick_params(labelrotation=45)\n\nplt.show()","a1266e29":"def process_row(row):\n    import re\n    from textblob import Word\n    from string import punctuation\n    from nltk.stem.snowball import SnowballStemmer\n    from gensim.parsing.preprocessing import STOPWORDS\n    \n    \n    #Mail address\n    row = re.sub('(\\S+@\\S+)(com|\\s+com)', ' ', row)\n    #Username\n    row = re.sub('(\\S+@\\S+)', ' ', row)\n    #punctuation & Lower case\n    punctuation = punctuation + '\\n' + '\u2014\u201c,\u201d\u2018-\u2019' + '0123456789'\n    row = ''.join(word.lower() for word in row if word not in punctuation)\n    #Stopwords & Lemma\n    stop = STOPWORDS\n    row = ' '.join(Word(word).lemmatize() for word in row.split() if word not in stop)\n    #Stemming\n    stemmer = SnowballStemmer(language='english')\n    row = ' '.join([stemmer.stem(word) for word in row.split() if len(word) > 2])\n    #Extra whitespace\n    row = re.sub('\\s{1,}', ' ', row)\n\n    return row\n\ndf['content'] = df['content'].apply(process_row)\n\nle = LabelEncoder()\nlabels = to_categorical(le.fit_transform(df.author)) # One-Hot encoding\n\n\ncorpus = df.content.values.tolist()\ncorpus = [c.split() for c in corpus] # add articles in a list\n\n# Tokenize words to tf-idf matrix\ntokenizer = Tokenizer(num_words=20000)\ntokenizer.fit_on_texts(corpus)\n\nsequences = tokenizer.texts_to_matrix(corpus, mode='tfidf')\n\n# Divide the data into two categories: train and test\nX_train, X_test, y_train, y_test = train_test_split(sequences, labels, test_size=.3)","96de2753":"model = keras.Sequential([\n    Dense(16,input_shape=(sequences[0].shape), kernel_regularizer=l2(0.1)),\n    Dropout(.3),\n    Dense(len(le.classes_), activation='softmax')\n])\n\nmodel.compile(loss='categorical_crossentropy', optimizer='sgd', metrics=['accuracy'])\nmodel.summary()","810c1f6f":"history = model.fit(X_train, y_train, epochs=5, validation_split=.33)","4353b3be":"f, (ax1, ax2) = plt.subplots(1, 2, figsize=(19,5))\nax1.plot(history.epoch, history.history['loss'], 'o--', label='Train loss')\nax1.plot(history.epoch, history.history['val_loss'], 'o--', label='Validation loss')\n\nax2.plot(history.epoch, history.history['accuracy'], 'o--' , label='Train accuracy')\nax2.plot(history.epoch, history.history['val_accuracy'], 'o--', label='Validation accuracy',)\nax1.legend()\nax2.legend()\nf.show()","8e84a392":"eval_ = model.evaluate(X_test, y_test)\nprint(\"Loss: {0:.5}\".format(eval_[0]))\nprint(\"Accuracy: {0:.2%}\".format(eval_[1]))","17b5dece":"### Extract 10 class from dataset","56ecae7b":"### Visualize count of news of each author","f7834beb":"### Evaluate model in test data","ccb5599c":"### Train model","ab80b760":"#### Delete useless variables","e254bf7c":"### Visualize accuracy\/loss plot","995313ad":"# Introduce\n\n## Goals\nIn this notebook I'll attempt to build models to correctly predict the author of a given article. The scope will be limited to 10 authors.\\\nThe techniques I'll use will include Tf-idf Analysis for feature-generation, and Supervised Learning (Keras library) for classification.\n\n## Dataset\nFrom: [snapcrack\/all-the-news](https:\/\/www.kaggle.com\/snapcrack\/all-the-news)\n\nThis dataset contains news articles scraped from various publications, labeled by publication and author name, as well as date and title. The original source on [kaggle.com](https:\/\/www.kaggle.com) contains three `.csv` files. Accross the three, there are over 140,000 articles from a total of 15 publications.","298ab64c":"#### Delete useless variables","f500c9b6":"### Preprocess","1fada0d4":"### first meet with data","298a1f67":"### Input data","281cbb95":"# Code Time\n\n### Import Libraries","31136e73":"### Create model"}}