{"cell_type":{"cb26eb3b":"code","5901b3f6":"code","9ece7749":"code","a5ff9b98":"code","bceea68a":"code","e83edb72":"code","b4e5bca6":"code","3b538d7b":"code","b5b50626":"code","18125a9e":"code","677dc19e":"code","c9b98726":"code","30a02c27":"code","e8086fa5":"code","9b23bbb5":"code","3ad4a1ce":"code","7142c0e2":"code","32ac65cc":"code","a2ab8340":"code","42e64f4f":"code","59896282":"code","5fd140c7":"code","dd6037e0":"code","435cf01a":"code","d53c45c1":"code","fe372f50":"code","ec1fd3d6":"code","ad928b0c":"code","664956e8":"code","bc347732":"code","d26aff5e":"markdown","68d99de8":"markdown","1f4fcd8a":"markdown","01b70ee8":"markdown"},"source":{"cb26eb3b":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom random import randint\nimport matplotlib.pyplot as plt\n\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\nprint(os.listdir(\"..\/input\/digit-rec-model\"))\nprint(os.listdir(\"..\/input\/d-r-model\"))\n# Any results you write to the current directory are saved as output.","5901b3f6":"train_data=pd.read_csv('..\/input\/digit-recognizer\/train.csv')\ntest_data=pd.read_csv('..\/input\/digit-recognizer\/test.csv')\nsub_sample=pd.read_csv('..\/input\/digit-recognizer\/sample_submission.csv')","9ece7749":"train_data.head(10)","a5ff9b98":"test_data.head()","bceea68a":"test_data.shape","e83edb72":"sub_sample.head()","b4e5bca6":"Y_train = train_data[\"label\"]\nY_train1 = np.array(Y_train, np.uint8)","3b538d7b":"train_images = train_data.drop(labels = [\"label\"],axis = 1) \ntrain_images = np.array(train_images)\ntest_images=np.array(test_data)","b5b50626":"print(train_images.shape)\nprint(Y_train1.shape)\nprint(test_images.shape)\nY_train1","18125a9e":"#Convert train datset to (num_images, img_rows, img_cols) format \nX_train1 = train_images.reshape(train_images.shape[0], 28, 28,1)\n#Convert test datset to (num_images, img_rows, img_cols) format \nX_test = test_images.reshape(test_images.shape[0], 28, 28,1)","677dc19e":"print(X_train1.shape)\nprint(X_test.shape)","c9b98726":"def plot_images(images, classes):\n    assert len(images) == len(classes) == 9\n    \n    # Create figure with 3x3 sub-plots.\n    fig, axes = plt.subplots(3, 3,figsize=(28,28),sharex=True)\n    fig.subplots_adjust(hspace=0.3, wspace=0.3)\n   \n    for i, ax in enumerate(axes.flat):\n        # Plot image.\n        \n        ax.imshow(images[i][:,:,0], cmap=plt.get_cmap('gray'))    \n        xlabel = \"the number is: {0}\".format(classes[i])\n    \n        # Show the classes as the label on the x-axis.\n        ax.set_xlabel(xlabel)\n        ax.xaxis.label.set_size(28)\n        # Remove ticks from the plot.\n        ax.set_xticks([])\n        ax.set_yticks([])\n    \n    # Ensure the plot is shown correctly with multiple plots\n    # in a single Notebook cell.\n    \n    plt.show()","30a02c27":"random_numbers = [randint(0, len(X_train1)) for p in range(0,9)]\nimages_to_show = [X_train1[i] for i in random_numbers]\nclasses_to_show = [Y_train[i] for i in random_numbers]\nprint(\"Images to show: {0}\".format(len(images_to_show)))\nprint(\"Classes to show: {0}\".format(len(classes_to_show)))\n#plot the images\nplot_images(images_to_show, classes_to_show)","e8086fa5":"\nfrom keras.utils.np_utils import to_categorical\n\nY_train1= to_categorical(Y_train1)","9b23bbb5":"Y_train1.shape","3ad4a1ce":"#Splitting the train_images into the Training set and validation set\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_val, Y_train, Y_val= train_test_split(X_train1, Y_train1,\n               test_size=0.1, random_state=42,stratify=Y_train1)\n\nprint(X_train.shape)\nprint(Y_train.shape)\nprint(X_val.shape)\nprint(Y_val.shape)\nprint(X_test.shape)","7142c0e2":"X_train1 = X_train1.astype('float32')\/255\nX_val=X_val.astype('float32')\/255\nX_test = X_test.astype('float32')\/255","32ac65cc":"import keras\nfrom keras.models import Sequential\nfrom keras.models import load_model\nfrom keras.layers import Dense, Dropout, Flatten,BatchNormalization\nfrom keras.layers import Conv2D, MaxPooling2D,AveragePooling2D\nfrom keras import regularizers\nfrom keras.optimizers import SGD\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.callbacks import LearningRateScheduler,ReduceLROnPlateau,ModelCheckpoint,EarlyStopping","a2ab8340":"def lr_schedule(epoch):\n    lrate = 0.001\n    if epoch > 10:\n        lrate = 0.0003\n    if epoch > 20:\n        lrate = 0.00003\n    elif epoch > 30:\n        lrate = 0.000003       \n    return lrate","42e64f4f":"lr_scheduler=LearningRateScheduler(lr_schedule)\n#we can reduce the LR by half if the accuracy is not improved after 3 epochs.using the following code\nreduceOnPlateau = ReduceLROnPlateau(monitor='val_acc', \n                                            patience=5, \n                                            verbose=1, \n                                            factor=0.5, \n                                            min_lr=0.00001, mode='auto')\n\n#Save the model after every decrease in val_loss \ncheckpoint = ModelCheckpoint(filepath='bestmodel.hdf5', verbose=0,monitor='val_loss',save_best_only=True,save_weights_only=False)\n\n#Stop training when a monitored quantity has stopped improving.\nearlyStopping=EarlyStopping(monitor='val_loss', min_delta=0, patience=3, verbose=0, mode='auto')","59896282":"model=load_model(\"..\/input\/d-r-model\/bestmodel (1).hdf5\")","5fd140c7":"datagen = ImageDataGenerator(\n        rotation_range=3,  # randomly rotate images in the range (degrees, 0 to 180)\n        zoom_range = 0.01, # Randomly zoom image \n        width_shift_range=0.05,  # randomly shift images horizontally (fraction of total width)\n        height_shift_range=0.05,  # randomly shift images vertically (fraction of total height)\n        horizontal_flip=False,  # randomly flip images\n        vertical_flip=False)  # randomly flip images\n\ndatagen.fit(X_train1)","dd6037e0":"callbacks_list = [reduceOnPlateau,checkpoint]","435cf01a":"sgd = SGD(lr=0.001, decay=1e-6, momentum=0.9)\nrmsprp_opt = keras.optimizers.rmsprop(lr=0.00003 ,decay=1e-4)\nadam=keras.optimizers.adam(lr=0.00003)\nmodel.compile(loss='categorical_crossentropy',\n              optimizer=adam,\n              metrics=['accuracy'])\nH1=model.fit_generator(datagen.flow(X_train, Y_train, batch_size=250),\n                    steps_per_epoch=len(X_train)\/\/225, epochs=30,\n                    verbose=1,callbacks=callbacks_list,\n                    validation_data=(X_val, Y_val))","d53c45c1":"plt.figure(0)\nplt.plot(H1.history['acc'],'r')\nplt.plot(H1.history['val_acc'],'g')\nplt.xticks(np.arange(0, 51, 1.0))\nplt.rcParams['figure.figsize'] = (14, 8)\nplt.xlabel(\"Num of Epochs\")\nplt.ylabel(\"Accuracy\")\nplt.title(\"Training Accuracy vs Validation Accuracy\")\nplt.legend(['train','validation'])","fe372f50":"plt.figure(1)\nplt.plot(H1.history['loss'],'r')\nplt.plot(H1.history['val_loss'],'g')\nplt.xticks(np.arange(0, 51, 1.0))\nplt.rcParams['figure.figsize'] = (14, 8)\nplt.xlabel(\"Num of Epochs\")\nplt.ylabel(\"Loss\")\nplt.title(\"Training Loss vs Validation Loss\")\nplt.legend(['train','validation'])","ec1fd3d6":"score = model.evaluate(X_val, Y_val, verbose=0)\nprint('Test loss:', score[0])\nprint('Test accuracy:', score[1])","ad928b0c":"from sklearn.metrics import classification_report\n\npreds = model.predict_classes(X_val)\ny_lable = [y.argmax() for y in Y_val]\nprint(classification_report(y_lable,preds))\npreds1 = model.predict_classes(X_train)\nytr_lable = [y.argmax() for y in Y_train]\nprint(classification_report(ytr_lable,preds1))","664956e8":"# predict results\nTest_perdect = model.predict(X_test)\n\n# select the indix with the maximum probability\nTest_perdect = np.argmax(Test_perdect,axis = 1)\n\nTest_perdect = pd.Series(Test_perdect,name=\"Label\")\n\nsubmission1 = pd.concat([pd.Series(range(1,28001),name = \"ImageId\"),Test_perdect],axis = 1)\n\nsubmission1.to_csv(\"submission1.csv\",index=False)","bc347732":"model.save(\"cifar-10_model.h5\")","d26aff5e":"\n## One Hot encoding\n\nEncode labels to one hot vectors (ex : 4 ---> [0,0,0,0,1,0,0,0,0,0] , 9 ---> [0,0,0,0,0,0,0,0,0,1])\n","68d99de8":"### Data Visualization","1f4fcd8a":"model = Sequential()\nmodel.add(Conv2D(32, (3, 3),padding='valid',\n                 activation='elu',input_shape=(28,28,1)))\nmodel.add(BatchNormalization())\nmodel.add(Conv2D(32, (3, 3),padding='valid',activation='elu'))\nmodel.add(BatchNormalization())\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\n\nmodel.add(Conv2D(64, (3, 3),padding='same',\n                 kernel_regularizer=regularizers.l2(0.001),activation='elu'))\nmodel.add(BatchNormalization())\nmodel.add(Conv2D(64, (3, 3),padding='same',\n                 kernel_regularizer=regularizers.l2(0.001),activation='elu'))\nmodel.add(BatchNormalization())\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\nmodel.add(Dropout(0.15))\n\nmodel.add(Conv2D(128, (3, 3),padding='same',\n                 kernel_regularizer=regularizers.l2(0.001),activation='elu'))\nmodel.add(BatchNormalization())\nmodel.add(Conv2D(128, (3, 3),padding='same',\n                 kernel_regularizer=regularizers.l2(0.001),activation='elu'))\nmodel.add(BatchNormalization())\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\nmodel.add(Dropout(0.2))\n\nmodel.add(Flatten())\nmodel.add(Dense(256, activation='relu'))\nmodel.add(Dropout(0.25))\nmodel.add(Dense(10, activation='softmax'))\nmodel.summary()","01b70ee8":"## Define Model by keras\u00b6"}}