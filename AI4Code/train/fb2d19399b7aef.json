{"cell_type":{"9fc37107":"code","1758e913":"code","699f567a":"code","b62dd729":"code","db219b5a":"code","ccc701a3":"code","a8a87974":"code","26339bd6":"code","d751d6e3":"code","7c13f837":"code","c4d5a7c8":"code","406e13d8":"code","b5e310f8":"code","9c0824e7":"code","3cf9676c":"code","71d6ee3a":"code","422f56dd":"markdown","689b2135":"markdown","c054a020":"markdown","6aa1983e":"markdown"},"source":{"9fc37107":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","1758e913":"import numpy as np\nimport pandas as pd\nimport tensorflow as tf\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPooling2D, Activation, LeakyReLU\nfrom keras.layers.noise import AlphaDropout\nfrom keras.optimizers import Adam\nfrom keras.utils.generic_utils import get_custom_objects\nfrom keras import backend as K\nfrom keras.callbacks import TensorBoard\nfrom keras.utils.np_utils import to_categorical","699f567a":"orlfaces = np.load(\"\/kaggle\/input\/orlfaces\/ORL_faces.npz\")\norlfaces.files","b62dd729":"print(orlfaces['trainX'].shape)\nprint(orlfaces['trainY'].shape)\nprint(orlfaces['testX'].shape)\nprint(orlfaces['testY'].shape)","db219b5a":"X = np.reshape(orlfaces['trainX'], (240, 112, 92,1))\nY = orlfaces['trainY']\nX_test = np.reshape(orlfaces['testX'], (160, 112, 92,1))\nY_test = orlfaces['testY']","ccc701a3":"import matplotlib.pyplot as plt\n%matplotlib inline\n\ndef show_images(images) -> None:\n    n: int = len(images)\n    for i in range(n):\n        plt.figure()\n        plt.imshow(images[i])","a8a87974":"show_images(X)","26339bd6":"show_images(X_test)","d751d6e3":"np.unique(Y)","7c13f837":"np.unique(Y_test)","c4d5a7c8":"# Attempt #1\nshape=(112,92,1)\n\nmodel = tf.keras.models.Sequential()\n#First Convolve Layer\nmodel.add(tf.keras.layers.Conv2D(32, (3,3) , input_shape=shape , activation='relu', strides=(1,1) ,padding='same', kernel_initializer= tf.keras.initializers.GlorotNormal()))\nmodel.add(tf.keras.layers.MaxPooling2D(pool_size=(2,2) , strides=(1,1)))\n#Second Convolve Layer\nmodel.add(tf.keras.layers.Conv2D(16, (3,3) , input_shape=shape , activation='relu', strides=(1,1) ,padding='same', kernel_initializer= tf.keras.initializers.GlorotNormal()))\nmodel.add(tf.keras.layers.MaxPooling2D(pool_size=(2,2) , strides=(1,1)))\n#Second Convolve Layer\nmodel.add(tf.keras.layers.Conv2D(16, (3,3) , input_shape=shape , activation='relu', strides=(1,1) ,padding='same', kernel_initializer= tf.keras.initializers.GlorotNormal()))\nmodel.add(tf.keras.layers.MaxPooling2D(pool_size=(2,2) , strides=(1,1)))\n\n# Flatten\nmodel.add(tf.keras.layers.Flatten())\n\n# FC Layer\/ANN ---- Prashant Nair says for units always divide by 4 or 16 depending on size of neurons and intended\n# number of hidden layer\n\n\nmodel.add(tf.keras.layers.Dense(units=465, activation='relu'))\nmodel.add(tf.keras.layers.Dropout(0.2))\nmodel.add(tf.keras.layers.Dense(units=116, activation='relu'))\nmodel.add(tf.keras.layers.Dropout(0.2))\nmodel.add(tf.keras.layers.Dense(units=29, activation='relu'))\n\n#Output Layer \nmodel.add(tf.keras.layers.Dense(units=20, activation='softmax'))\n\n# Compile CNN Model\n\nmodel.compile(optimizer=tf.keras.optimizers.RMSprop(),\n              loss='sparse_categorical_crossentropy',\n              metrics=['accuracy'])\n","406e13d8":"model.summary()","b5e310f8":"# Fit Model (Start Training)\nbatch_size = 50\nhistory = model.fit(x = X, y=Y,\n                    validation_data=(X_test, Y_test),\n                    epochs=300,\n                    steps_per_epoch= len(X) \/\/ batch_size,\n                    )","9c0824e7":"# Attempt #2. Reduce the number of layers\ndef create_model():\n    activation = 'relu'\n    model = Sequential() #initialize Sequential model\n\n    model.add(Conv2D(32, kernel_size=3,\n          activation=activation,\n          input_shape=shape)) #32 filter with kernel size of 3 x 3 with input shape\n    model.add(MaxPooling2D(pool_size=2)) \n\n    model.add(Conv2D(64,3, activation=activation)) #64 filter with kernel size of 3 x 3\n    model.add(MaxPooling2D(pool_size=2)) #Max pool with size of 2\n\n    model.add(Flatten())\n\n    model.add(Dense(2024, activation=activation))\n    model.add(Dropout(0.5))\n    model.add(Dense(1024, activation=activation))\n    model.add(Dropout(0.5))\n    model.add(Dense(512, activation=activation))\n    model.add(Dropout(0.5))\n\n    model.add(Dense(20, activation='softmax')) #Output layer\n    model.summary()\n    return model","3cf9676c":"model = create_model()\nmodel.compile(\n        loss='sparse_categorical_crossentropy', \n        optimizer=Adam(clipvalue=0.5), \n        #optimizer=Adam(), \n        metrics=['accuracy']\n    )\nmodel.fit(\n    x = X, y=Y,\n    validation_data=(X_test, Y_test),\n    epochs=75,\n    batch_size=512,\n     verbose=2)","71d6ee3a":"# Normalizing each image as each image is between 0-255 pixels\nX_normal = X.astype(np.float32) \/ 255.0\nX_test_normal = X_test.astype(np.float32) \/ 255.0\n\nmodel = create_model()\nmodel.compile(\n        loss='sparse_categorical_crossentropy', \n        optimizer=Adam(clipvalue=0.5), \n        #optimizer=Adam(), \n        metrics=['accuracy']\n    )\nmodel.fit(\n    x = X_normal, y=Y,\n    validation_data=(X_test_normal, Y_test),\n    epochs=75,\n    batch_size=512,\n     verbose=2)","422f56dd":"**Normalization seemed to have improved the accuracy to 0.93. I gotta believe the theories!**","689b2135":"**Great. We crossed the 0.90 threshold**\n\n**So far we tried it without normalizing the images**\n\n**Let us try the same model with normalizing**","c054a020":"**Check the number of persons whose images were taken**","6aa1983e":"**Looks like there are 20**"}}