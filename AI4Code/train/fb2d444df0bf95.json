{"cell_type":{"a38b239e":"code","17e3c516":"code","b0353b64":"code","5ae3c138":"code","1283ef67":"code","0100cbff":"code","69925d7e":"code","58960ac6":"code","049c9c1e":"code","33111ab9":"code","26f671f2":"code","01b9c92b":"code","23cb5041":"code","5c5507f8":"code","ed8e72e4":"code","18046d0f":"code","455e8c5d":"code","e488ba2a":"code","70901f83":"code","42f908e4":"code","8b3cf31b":"code","c3983a48":"code","5afbf753":"code","b98beaa4":"code","7ce67954":"code","7e5ec31e":"code","93208018":"markdown","85bdd610":"markdown","d905d0b6":"markdown"},"source":{"a38b239e":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt","17e3c516":"def Score_data(pred, real):\n    # computing errors\n    errors = np.abs(pred - real).flatten()\n    # estimation\n    mean = sum(errors)\/len(errors)\n    cov = 0\n    for e in errors:\n        cov += (e - mean)**2\n    cov \/= len(errors)\n\n    print('mean : ', mean)\n    print('cov : ', cov)\n    return errors, cov, mean\n\n# calculate Mahalanobis distance\ndef Mahala_distantce(x,mean,cov):\n    return (x - mean)**2 \/ cov\n\n\ndef scale(A):\n    return (A-np.min(A))\/(np.max(A) - np.min(A))\n\n\ndef stats_dfs(path):\n    df = pd.read_csv(path,sep=\";\")\n    print(\"\\n_________________\\n\")\n    print(path)\n    print(\"\\n_________________\\n\")\n    print(df.shape)\n    print(\"\\n_________________\\n\")\n    print(df.anomaly.value_counts())\n    print(\"\\n_________________\\n\")\n    print(df.anomaly.value_counts()\/df.shape[0]*100)\n    print(\"\\n_________________\\n\")\n    print(df.changepoint.value_counts())\n    print(\"\\n_________________\\n\")\n    print(df.changepoint.value_counts()\/df.shape[0]*100)\n    return df\n\n\ndef stats_dfs_freeanomaly(path):\n    df = pd.read_csv(path,sep=\";\")\n    print(\"\\n_________________\\n\")\n    print(path)\n    print(\"\\n_________________\\n\")\n    print(df.shape)\n    print(\"\\n_________________\\n\")\n    return df\n","b0353b64":"list_df_1 = [\"\/kaggle\/input\/skoltech-anomaly-benchmark-skab\/SKAB\/valve2\/1.csv\"]\ndf = stats_dfs(list_df_1[0])\ndf.head()","5ae3c138":"# df = df.loc[:550]\ntest = df.loc[550:]","1283ef67":"list_a_free = [\"\/kaggle\/input\/skoltech-anomaly-benchmark-skab\/SKAB\/anomaly-free\/anomaly-free.csv\"]\ndf_a_free = stats_dfs_freeanomaly(list_a_free[0])\ndf_a_free.head()","0100cbff":"raw_data = pd.read_csv(\"..\/input\/benckmark-anomaly-timeseries-skab\/alldata_skab.csv\")\nprint(\"anomaly \", raw_data.anomaly.value_counts())\nprint(\"changepoint \",raw_data.changepoint.value_counts())\n\n# # Plotting\npd.DataFrame(raw_data[['Volume Flow RateRMS', 'anomaly', 'changepoint']].values, columns=['Volume Flow RateRMS', 'anomaly', 'changepoint'], index = raw_data.index).plot(figsize=(12,6))\n\nplt.xlabel('Values')\nplt.ylabel('Values')\nplt.title('Residuals')\nplt.show()","69925d7e":"raw_data = df.copy()\nraw_data.set_index('datetime')\n# # Plotting\npd.DataFrame(raw_data.values, columns=raw_data.columns, index = raw_data.index).plot(figsize=(12,6))\nplt.xlabel('Time')\nplt.ylabel('Residuals')\nplt.title('Residuals')\nplt.show()","58960ac6":"import matplotlib.pyplot as plt# Standardize\/scale the dataset and apply PCA\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\nfrom sklearn.pipeline import make_pipeline\n# Extract the names of the numerical columns\n\n# x = df[['Accelerometer1RMS', 'Accelerometer2RMS', 'Current', 'Pressure', 'Temperature', 'Thermocouple', 'Voltage', 'Volume Flow RateRMS']]\nx = df[['Volume Flow RateRMS']]","049c9c1e":"scaler = StandardScaler()\npca = PCA()\npipeline = make_pipeline(scaler, pca)\n# pipeline.fit(x.values.reshape(-8, 8))\npipeline.fit(x.values.reshape(-1, 1))","33111ab9":"# Plot the principal components against their inertia\nfeatures = range(pca.n_components_)\n_ = plt.figure(figsize=(15, 5))\n_ = plt.bar(features, pca.explained_variance_)\n_ = plt.xlabel('PCA feature')\n_ = plt.ylabel('Variance')\n_ = plt.xticks(features)\n_ = plt.title(\"Importance of the Principal Components based on inertia\")\nplt.show()","26f671f2":"# # Calculate PCA with 8 components\n# pca = PCA(n_components=8)\n# principalComponents = pca.fit_transform(x.values.reshape(-8,8))\n# principalDf = pd.DataFrame(data = principalComponents, columns = ['pc1', 'pc2', 'pc3', 'pc4', 'pc5', 'pc6', 'pc7', 'pc8'])\n\n# Calculate PCA with 1 components\npca = PCA(n_components=1)\nprincipalComponents = pca.fit_transform(x.values.reshape(-1,1))\nprincipalDf = pd.DataFrame(data = principalComponents, columns = ['pc1'])\n","01b9c92b":"from statsmodels.tsa.stattools import adfuller\n# Run Augmented Dickey Fuller Test\nresult = adfuller(principalDf['pc1'])\n# Print p-value\nprint(result[1] >0.05, result[1])","23cb5041":"!pip install changefinder\nimport changefinder","5c5507f8":"def changeFinderALLData(data, r, order, smooth):\n    cf = changefinder.ChangeFinder(r=r, order=order, smooth=smooth)\n    scores = [cf.update(p) for p in data]\n    return scores\n\nlist_cfresults = changeFinderALLData(principalDf['pc1'].loc[550:], r=0.01, order=1, smooth=10)    ","ed8e72e4":"errors, cov, mean = Score_data(list_cfresults , principalDf['pc1'].loc[550:].values)\n\nmahala_dist = []\nfor e in errors:\n    mahala_dist.append(Mahala_distantce(e, mean, cov))\n\n","18046d0f":"test['pca1_value'] = principalDf['pc1'].loc[550:]\ntest['pca1_scores'] = mahala_dist\n\ntest['pca1_scores_norm'] = scale(mahala_dist)\nplt.figure(figsize=(12, 8))\nplt.hist(test['pca1_scores_norm'], bins=50);","455e8c5d":"q1_pc1, q3_pc1 = test['pca1_scores'].quantile([0.15, 0.5])\niqr_pc1 = q3_pc1 - q1_pc1\n\n# Calculate upper and lower bounds for outlier for pc1\nlower_pc1 = q1_pc1 - (1.5*iqr_pc1)\nupper_pc1 = q3_pc1 + (1.5*iqr_pc1)\n# Filter out the outliers from the pc1\ntest['outlier_pca1'] = ((test['pca1_scores']>upper_pc1) | (test['pca1_scores']<lower_pc1)).astype('int')\ntest['outlier_pca1'].value_counts()","e488ba2a":"# fig, axes = plt.subplots(nrows=2, figsize=(15,10))\n# axes[0].plot(test[['pca1_scores']], color='blue')\n# axes[1].plot(np.array(mahala_dist).ravel(), color='red')\n\n# axes[0].set_title('original data', fontsize=20)\n# axes[1].set_title('outlier score', fontsize=20)\n\n# # axes[0].grid()\n# # axes[1].grid()\n# plt.tight_layout()\n# plt.show()","70901f83":"# visualization\na = test.loc[test['anomaly'] == 1] \n_ = plt.figure(figsize=(18,6))\n_ = plt.plot(test[['pca1_scores']], color='blue', label='Inline')\n_ = plt.plot(a[['pca1_scores']], linestyle='none', marker='X', color='red', markersize=12, label='Anomaly')\n_ = plt.xlabel('Series')\n_ = plt.ylabel('Readings')\n_ = plt.title('True Anomaly')\n_ = plt.legend(loc='best')\nplt.show();","42f908e4":"# visualization\na = test.loc[test['outlier_pca1'] == 1] \n_ = plt.figure(figsize=(18,6))\n_ = plt.plot(test[['pca1_scores']], color='blue', label='Inline')\n_ = plt.plot(a[['pca1_scores']], linestyle='none', marker='X', color='red', markersize=12, label='Anomaly')\n_ = plt.xlabel('Series')\n_ = plt.ylabel('Readings')\n_ = plt.title('Anomaly')\n_ = plt.legend(loc='best')\nplt.show();","8b3cf31b":"N = test.shape[0]\nplt.scatter(range(N),test['pca1_scores_norm'][:N].cumsum(),marker='1',label='PCA ')\nplt.xlabel('Readings')\nplt.ylabel('anomalies frequency')\nplt.legend()\nplt.show()","c3983a48":"#2 -- Distributions of Predicted Probabilities of both classes\nlabels=['Positive','Negative']\nplt.hist(test[test['outlier_pca1']==1]['pca1_scores_norm'], density=False, bins=100,\n             alpha=.5, color='green',  label=labels[0])\nplt.hist(test[test['outlier_pca1']==0]['pca1_scores_norm'], density=False, bins=100,\n             alpha=.5, color='red', label=labels[1])\nplt.axvline(.5, color='blue', linestyle='--', label='decision boundary')\n# plt.xlim([0,1])\nplt.title('Distributions', size=13)\nplt.xlabel('Norm values', size=13)\nplt.ylabel('Readings (norm.)', size=13)\nplt.legend(loc=\"upper right\")","5afbf753":"from sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix\n\nprint(classification_report(test['anomaly'], test['outlier_pca1']))\nconfusion_matrix(test['anomaly'], test['outlier_pca1'])","b98beaa4":"print(classification_report(test['changepoint'], test['outlier_pca1']))\nconfusion_matrix(test['changepoint'], test['outlier_pca1'])","7ce67954":"from sklearn.metrics import roc_auc_score\n\nroc_auc_score(test['outlier_pca1'], test['anomaly'])","7e5ec31e":"roc_auc_score(test['outlier_pca1'], test['changepoint'])","93208018":"The test have value very small number (much smaller than 0.05). Thus, I will reject the Null Hypothesis and say the data is stationary","85bdd610":"## Using PCA1 component with Change Finger\nDetecting Outliers and Change Points from Time Series","d905d0b6":"# Introduction\nAnomaly detection has applications in many fields, such as system health monitoring, fraud detection, and intrusion detection.\n\n![](https:\/\/www.googleapis.com\/download\/storage\/v1\/b\/kaggle-user-content\/o\/inbox%2F3595464%2F4088133a20318f4e47e1e2d738509d12%2F__results___5_0.png?generation=1590869249365044&alt=media)\n\n\n### Change Finger\n\nOur change point detection scheme in which we employ two-stage time-series learning, which we\nnamed ChangeFinder. We illustrate ChangeFinder in Figure below. A remarkable aspect of ChangeFinder is that it repeats the\nlearning process twice, where the outlier detection is first done using the model learned in the first stage and change\npoint detection is done using the learned model in the second one.\n![Flow%20of%20ChangeFinder.PNG](attachment:Flow%20of%20ChangeFinder.PNG)\n"}}