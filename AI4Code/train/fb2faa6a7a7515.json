{"cell_type":{"5d5cd09d":"code","8262dc8d":"code","ebe02ada":"code","46d9e0ef":"code","1fee6cdb":"code","295ba10d":"code","9025c61c":"code","6874066d":"code","a09196db":"code","addb0475":"code","aee2163b":"code","50530bf9":"code","c3282cf7":"code","182a65a6":"code","0d377439":"code","00ea0cd0":"code","d0411527":"code","1d437c01":"code","a5c58d2e":"code","5a0b0122":"code","a60b740c":"code","fd6fd320":"code","13baeb3e":"code","ab77cf78":"code","1077275a":"code","5d2fc991":"code","65253587":"code","3e175147":"code","03595038":"code","d7fdeb3b":"code","6b13f232":"code","4297504e":"code","57bfd15d":"code","8b9c3c7a":"code","6ffd177b":"code","25ec89a6":"code","b6e43bc6":"code","c39c7480":"code","c4ae80fb":"code","b757b630":"code","19ac6610":"markdown","0f6c6741":"markdown","c6f96591":"markdown","78021965":"markdown","67188a74":"markdown","5200ef7f":"markdown","f372d7da":"markdown","ba0e1be4":"markdown","5e95724f":"markdown","87291857":"markdown","6ee6203e":"markdown","5116f915":"markdown","21904552":"markdown","95f18406":"markdown","5fb5373e":"markdown","428769a6":"markdown","e8229371":"markdown","d690c560":"markdown","13226ccf":"markdown","d4bfaef7":"markdown","61f09876":"markdown","3772a144":"markdown","a6f43d20":"markdown","8bf3ec0e":"markdown","56bd371e":"markdown","bb7079bd":"markdown","eb07c21a":"markdown","252de260":"markdown","1f6e2dbb":"markdown","fd395571":"markdown","d08941c7":"markdown","16ade162":"markdown","ac1dc57b":"markdown","ed147db6":"markdown","ab62c4fa":"markdown","aa328f2c":"markdown","5da8112d":"markdown","7fc3387b":"markdown","42054130":"markdown","bdbcec95":"markdown","c4fdd105":"markdown","fbf51f1c":"markdown","bc1cceb7":"markdown","32bc061b":"markdown","9ae3224e":"markdown","5deabc54":"markdown","c9a054fe":"markdown","2c70c832":"markdown","4033e5a2":"markdown","13eeb5ff":"markdown","74404134":"markdown","c4a74599":"markdown"},"source":{"5d5cd09d":"#Importing the basic librarires\n\nimport os\nimport math\nimport scipy\nimport random\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom sklearn import tree\nfrom scipy.stats import randint\nfrom scipy.stats import loguniform\nfrom IPython.display import display\n\nfrom sklearn.decomposition import PCA\nfrom imblearn.over_sampling import SMOTE\nfrom sklearn.feature_selection import RFE\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom imblearn.under_sampling import OneSidedSelection\nfrom scikitplot.metrics import plot_roc_curve as auc_roc\nfrom imblearn.under_sampling import CondensedNearestNeighbour\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\n\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.model_selection import RepeatedStratifiedKFold\n\nfrom sklearn.svm import SVC\nfrom xgboost import XGBClassifier\nfrom sklearn.naive_bayes import BernoulliNB\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\n\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.preprocessing import PolynomialFeatures\n\nfrom sklearn.metrics import accuracy_score, confusion_matrix, classification_report, \\\nf1_score, roc_auc_score, roc_curve, precision_score, recall_score\n\nimport matplotlib.pyplot as plt\nplt.rcParams['figure.figsize'] = [10,6]\n\nimport warnings \nwarnings.filterwarnings('ignore')","8262dc8d":"#Importing the dataset\n\ndf = pd.read_csv('..\/input\/credit-card-fraud-detection-dataset\/creditcard.csv')\n#df.drop(['ID','year'],axis=1, inplace=True)\n\ntarget = 'Class'\nlabels = ['Non - Fraudulent','Fraudulent']\nfeatures = [i for i in df.columns.values if i not in [target]]\n\noriginal_df = df.copy(deep=True)\ndisplay(df.head())\n\nprint('\\n\\033[1mInference:\\033[0m The Datset consists of {} features & {} samples.'.format(df.shape[1], df.shape[0]))","ebe02ada":"#Checking the dtypes of all the columns\n\ndf.info()","46d9e0ef":"#Checking number of unique rows in each feature\n\ndf.nunique().sort_values()","1fee6cdb":"#Checking number of unique rows in each feature\n\nnu = df[features].nunique().sort_values()\nnf = []; cf = []; nnf = 0; ncf = 0; #numerical & categorical features\n\nfor i in range(df[features].shape[1]):\n    if nu.values[i]<=15:cf.append(nu.index[i])\n    else: nf.append(nu.index[i])\n\nprint('\\n\\033[1mInference:\\033[0m The Datset has {} numerical & {} categorical features.'.format(len(nf),len(cf)))","295ba10d":"#Checking the stats of all the columns\n\ndisplay(df.describe())","9025c61c":"#Let us first analyze the distribution of the target variable\n\nMAP={}\nfor e, i in enumerate(df[target].unique()):\n    MAP[i]=labels[e]\n#MAP={0:'Not-Survived',1:'Survived'}\ndf1 = df.copy()\ndf1[target]=df1[target].map(MAP)\nexplode=np.zeros(len(labels))\nexplode[-1]=0.07\nprint('\\033[1mTarget Variable Distribution'.center(55))\nplt.pie(df1[target].value_counts(), labels=df1[target].value_counts().index, counterclock=False, shadow=True, \n        explode=explode, autopct='%1.1f%%', radius=1, startangle=-30)\nplt.show()","6874066d":"#Visualising the Fraudulent Transactions for various amounts across timstamps\n\nplt.scatter(df[df.Class==0]['Time'], df[df.Class==0]['Amount'], label='Non-Fraudulent')\nplt.scatter(df[df.Class==1]['Time'], df[df.Class==1]['Amount'], label='Fraudulent')\nplt.xlabel('Timestamp')\nplt.ylabel('Amount')\nplt.grid()\nplt.legend()\nplt.show()","a09196db":"#Visualising the categorical features \n\nprint('\\033[1mVisualising Categorical Features:'.center(100))\n\nn=4\nplt.figure(figsize=[15,3*math.ceil(len(cf)\/n)])\n\nfor i in range(len(cf)):\n    if df[cf[i]].nunique()<=15:\n        plt.subplot(math.ceil(len(cf)\/n),n,i+1)\n        sns.countplot(df[cf[i]])\n    #else:\n    #    plt.subplot(2,2,i)\n    #    sns.countplot(df[cf[i]])\nplt.tight_layout()\nplt.show()","addb0475":"#Understanding the feature set\n\nprint('\\033[1mFeatures Distribution'.center(100))\n\nn=6\nnf = [i for i in features if i not in cf]\n\nplt.figure(figsize=[15,3*math.ceil(len(features)\/n)])\nfor c in range(len(nf)):\n    plt.subplot(math.ceil(len(features)\/n),n,c+1)\n    sns.distplot(df[nf[c]])\nplt.tight_layout()\nplt.show()\n\nplt.figure(figsize=[15,3*math.ceil(len(features)\/n)])\nfor c in range(len(nf)):\n    plt.subplot(math.ceil(len(features)\/n),n,c+1)\n    df.boxplot(nf[c])\nplt.tight_layout()\nplt.show()","aee2163b":"#Understanding the relationship between all the features\n\n# ppc=[i for i in df1.columns if i not in cf]\n# g=sns.pairplot(df1[ppc], hue=target, size=4)\n# #g.map_upper(sns.kdeplot, levels=1, color=\".2\")\n# plt.show()","50530bf9":"#Check for empty elements\n\nnvc = pd.DataFrame(df.isnull().sum().sort_values(), columns=['Total Null Values'])\nnvc['Percentage'] = round(nvc['Total Null Values']\/df.shape[0],3)*100\nprint(nvc)\nprint('\\n\\033[1mInference:\\033[0m The dataset doesn\\'t have any null elements')","c3282cf7":"#Removal of any Duplicate rows (if any)\n\ncounter = 0\nr,c = original_df.shape\n\ndf1 = df.copy()\ndf1.drop_duplicates(inplace=True)\ndf1.reset_index(drop=True,inplace=True)\n\nif df1.shape==(r,c):\n    print('\\n\\033[1mInference:\\033[0m The dataset doesn\\'t have any duplicates')\nelse:\n    print(f'\\n\\033[1mInference:\\033[0m Number of duplicates dropped ---> {r-df1.shape[0]}')","182a65a6":"#Converting categorical Columns to Numeric\n\ndf1 = df.copy()\n\n#Target Variable\n#MAP={}\n#for i,e in enumerate(df1[target].unique()):\n#    MAP[e]=i\n#df1[target]=df1[target].map(MAP)\n#print('Mapping Target variable --->',MAP)\n\n#One-Hot Binay Encoding\noh=True\ndm=True\nfor i in cf:\n    #print(i)\n    if df1[i].nunique()==2:\n        if oh==True: print(\"One-Hot Encoding on features:\")\n        print(i);oh=False\n        df1[i]=pd.get_dummies(df1[i], drop_first=True, prefix=str(i))\n    if (df1[i].nunique()>2 and df1[i].nunique()<15):\n        if dm==True: print(\"\\nDummy Encoding on features:\")\n        print(i);dm=False\n        df1 = pd.concat([df1.drop([i], axis=1), pd.DataFrame(pd.get_dummies(df1[i], drop_first=True, prefix=str(i)))],axis=1)","0d377439":"#Removal of outlier:\n\ndf3 = df1.copy()\n\nfor i in [i for i in df3.columns]:\n    if df3[i].nunique()>=12:\n        Q1 = df3[i].quantile(0.0001)\n        Q3 = df3[i].quantile(0.9999)\n        IQR = Q3 - Q1\n        df3 = df3[df3[i] <= (Q3+(1.5*IQR))]\n        df3 = df3[df3[i] >= (Q1-(1.5*IQR))]\ndf3 = df3.reset_index(drop=True)\ndisplay(df3.head())\nprint('\\n\\033[1mInference:\\033[0m Before removal of outliers, The dataset had {} samples.'.format(df1.shape[0]))\nprint('\\033[1mInference:\\033[0m After removal of outliers, The dataset now has {} samples.'.format(df3.shape[0]))","00ea0cd0":"df3.Class.value_counts()","d0411527":"#Fixing the imbalance using SMOTE Technique\n\ndf4 = df3.copy()\n\nprint('Original class distribution:')\nprint(df4[target].value_counts())\n\n#xf = df4.columns\n#X = df4.drop([target],axis=1)\n#Y = df4[target]\n\n#smote = SMOTE()\n#undersample = OneSidedSelection(n_neighbors=10, n_seeds_S=200)\n#X, Y = undersample.fit_resample(X, Y)\n\n#df5 = pd.DataFrame(X, columns=xf)\n#df5[target] = Y\n\ndf5_1 = df4[df4.Class==1]\ndf5_0 = df4[df4.Class==0]\n\ndf5_0=df5_0.loc[random.choices(df5_0.index.values,k=2000)]\n\ndf5 = pd.concat([df5_0,df5_1],axis=0)\n\nprint('\\nClass distribution after applying SMOTE Technique:',)\nprint(df5[target].value_counts())","1d437c01":"#Visualising the Fraudulent Transactions for various amounts across timstamps after undersampling\n\nplt.scatter(df5[df5.Class==0]['Time'], df5[df5.Class==0]['Amount'], label='Non-Fraudulent')\nplt.scatter(df5[df5.Class==1]['Time'], df5[df5.Class==1]['Amount'], label='Fraudulent')\nplt.xlabel('Timestamp')\nplt.ylabel('Amount')\nplt.grid()\nplt.legend()\nplt.show()","a5c58d2e":"#Final Dataset size after performing Preprocessing\n\ndf = df5.copy()\nplt.title('Final Dataset Samples')\nplt.pie([df.shape[0], original_df.shape[0]-df4.shape[0], df4.shape[0]-df5.shape[0]], radius = 1, shadow=True,\n        labels=['Retained','Dropped','Augmented'], counterclock=False, autopct='%1.1f%%', pctdistance=0.9, explode=[0,0,0])\nplt.pie([df.shape[0]], labels=['100%'], labeldistance=-0, radius=0.78, shadow=True, colors=['powderblue'])\nplt.show()\n\nprint('\\n\\033[1mInference:\\033[0mThe final dataset after cleanup has {} samples & {} columns.'.format(df.shape[0], df.shape[1]))","5a0b0122":"#Splitting the data intro training & testing sets\n\ndf = df5.copy()\ndf.reset_index(inplace=True, drop=True)\n\nX = df.drop([target],axis=1)\nY = df[target]\nTrain_X, Test_X, Train_Y, Test_Y = train_test_split(X, Y, train_size=0.8, test_size=0.2, random_state=0)\n\nprint('Original set  ---> ',X.shape,Y.shape,'\\nTraining set  ---> ',Train_X.shape,Train_Y.shape,'\\nTesting set   ---> ', Test_X.shape,'', Test_Y.shape)","a60b740c":"#Feature Scaling (Standardization)\n\nstd = StandardScaler()\n\nprint('\\033[1mStandardardization on Training set'.center(100))\nTrain_X_std = std.fit_transform(Train_X)\nTrain_X_std = pd.DataFrame(Train_X_std, columns=X.columns)\ndisplay(Train_X_std.describe())\n\nprint('\\n','\\033[1mStandardardization on Testing set'.center(100))\nTest_X_std = std.transform(Test_X)\nTest_X_std = pd.DataFrame(Test_X_std, columns=X.columns)\ndisplay(Test_X_std.describe())","fd6fd320":"#Checking the correlation\n\nfeatures = df.columns\nplt.figure(figsize=[24,20])\nplt.title('Features Correlation-Plot')\nsns.heatmap(df[features].corr(), vmin=-1, vmax=1, center=0, annot=True) #, \nplt.show()","13baeb3e":"# Calculate the VIFs to remove multicollinearity\n\nDROP=[]; scores=[]\n#scores.append(f1_score(Test_Y,LogisticRegression().fit(Train_X_std, Train_Y).predict(Test_X_std)))\n\nfor i in range(len(X.columns.values)-10):\n    vif = pd.DataFrame()\n    Xs = X.drop(DROP,axis=1)\n    #print(DROP)\n    vif['Features'] = Xs.columns\n    vif['VIF'] = [variance_inflation_factor(Xs.values, i) for i in range(Xs.shape[1])]\n    vif['VIF'] = round(vif['VIF'], 2)\n    vif = vif.sort_values(by = \"VIF\", ascending = False)\n    vif.reset_index(drop=True, inplace=True)\n    DROP.append(vif.Features[0])\n    if vif.VIF[0]>1.1:\n        scores.append(recall_score(Test_Y,RandomForestClassifier().fit(Train_X_std.drop(DROP,axis=1), Train_Y).predict(Test_X_std.drop(DROP,axis=1)))*100)\n    #print(scores)\n    \nplt.plot(scores)\n#plt.ylim([0.7,0.85])\nplt.grid()\nplt.show()","ab77cf78":"# Applying Recurrsive Feature Elimination\n\n# Running RFE with the output number of the variable equal to 10\nscores=[]\n\nfor i in range(29):\n    LR = LogisticRegression(solver='liblinear')\n    rfe = RFE(LR,n_features_to_select=len(Train_X_std.columns)-i)   \n    rfe = rfe.fit(Train_X_std, Train_Y)\n    scores.append(recall_score(Test_Y,RandomForestClassifier().fit(Train_X_std[Train_X_std.columns[rfe.support_]], Train_Y).predict(Test_X_std[Train_X_std.columns[rfe.support_]])))\n    #print(scores)\n    \nplt.plot(scores)\n#plt.ylim([0.80,0.84])\nplt.grid()\nplt.show()","1077275a":"from sklearn.decomposition import PCA\n\npca = PCA().fit(Train_X_std)\n\nfig, ax = plt.subplots(figsize=(14,6))\nx_values = range(1, pca.n_components_+1)\nax.bar(x_values, pca.explained_variance_ratio_, lw=2, label='Explained Variance')\nax.plot(x_values, np.cumsum(pca.explained_variance_ratio_), lw=2, label='Cumulative Explained Variance', color='red')\nplt.plot([0,pca.n_components_+1],[0.90,0.90],'g--')\nplt.plot([16,16],[0,1], 'g--')\nax.set_title('Explained variance of components')\nax.set_xlabel('Principal Component')\nax.set_ylabel('Explained Variance')\nplt.grid()\nplt.legend()\nplt.show()","5d2fc991":"#Applying PCA Transformations\n\nscores=[]\nfor i in range(20):\n    pca = PCA(n_components=Train_X_std.shape[1]-i)\n    Train_X_std_pca = pca.fit_transform(Train_X_std)\n    #print('The shape of final transformed training feature set:')\n    #print(Train_X_std_pca.shape)\n    Train_X_std_pca = pd.DataFrame(Train_X_std_pca)\n\n    Test_X_std_pca = pca.transform(Test_X_std)\n    #print('\\nThe shape of final transformed testing feature set:')\n    #print(Test_X_std_pca.shape)\n    Test_X_std_pca = pd.DataFrame(Test_X_std_pca)\n\n    scores.append(recall_score(Test_Y,RandomForestClassifier().fit(Train_X_std_pca, Train_Y).predict(Test_X_std_pca))*100)\n\nplt.plot(scores)\n#plt.ylim([0.80,0.84])\nplt.grid()\nplt.show()","65253587":"# Shortlisting Features\n\nLR = LogisticRegression(solver='liblinear')\nrfe = RFE(LR,n_features_to_select=len(Train_X_std.columns)-15)   \nrfe = rfe.fit(Train_X_std, Train_Y)\n\nprint('Shortlisting Features based on automated RFE Technique:')\nprint(Train_X_std.columns[rfe.support_].values)\n\n# Train_X_std = Train_X_std[Train_X_std.columns[rfe.support_]]\n# Test_X_std  = Test_X_std[Test_X_std.columns[rfe.support_]]","3e175147":"#Let us create first create a table to store the results of various models \n\nEvaluation_Results = pd.DataFrame(np.zeros((7,5)), columns=['Accuracy', 'Precision','Recall','F1-score','AUC-ROC score'])\nEvaluation_Results.index=['Logistic Regression (LR)','Decision Tree Classifier (DT)','Random Forest Classifier (RF)','Na\u00efve Bayes Classifier (NB)',\n                         'K Nearest Neighbours (KNN)', 'Gradient Boosting (GB)','Extreme Gradient Boosting (XGB)']\nEvaluation_Results","03595038":"#Let us define functions to summarise the Prediction's scores .\n\n#Classification Summary Function\ndef Classification_Summary(pred,pred_prob,i):\n    Evaluation_Results.iloc[i]['Accuracy']=round(accuracy_score(Test_Y, pred),3)*100   \n    Evaluation_Results.iloc[i]['Precision']=round(precision_score(Test_Y, pred),3)*100 #, average='weighted'\n    Evaluation_Results.iloc[i]['Recall']=round(recall_score(Test_Y, pred),3)*100 #, average='weighted'\n    Evaluation_Results.iloc[i]['F1-score']=round(f1_score(Test_Y, pred),3)*100 #, average='weighted'\n    Evaluation_Results.iloc[i]['AUC-ROC score']=round(roc_auc_score(Test_Y, pred_prob[:, 1]),3)*100 #, multi_class='ovr'\n    print('{}{}\\033[1m Evaluating {} \\033[0m{}{}\\n'.format('<'*3,'-'*35,Evaluation_Results.index[i], '-'*35,'>'*3))\n    print('Accuracy = {}%'.format(round(accuracy_score(Test_Y, pred),3)*100))\n    print('F1 Score = {}%'.format(round(f1_score(Test_Y, pred),3)*100)) #, average='weighted'\n    print('\\n \\033[1mConfusiton Matrix:\\033[0m\\n',confusion_matrix(Test_Y, pred))\n    print('\\n\\033[1mClassification Report:\\033[0m\\n',classification_report(Test_Y, pred))\n    \n    auc_roc(Test_Y, pred_prob, curves=['each_class'])\n    plt.show()\n\n#Visualising Function\ndef AUC_ROC_plot(Test_Y, pred):    \n    ref = [0 for _ in range(len(Test_Y))]\n    ref_auc = roc_auc_score(Test_Y, ref)\n    lr_auc = roc_auc_score(Test_Y, pred)\n\n    ns_fpr, ns_tpr, _ = roc_curve(Test_Y, ref)\n    lr_fpr, lr_tpr, _ = roc_curve(Test_Y, pred)\n\n    plt.plot(ns_fpr, ns_tpr, linestyle='--')\n    plt.plot(lr_fpr, lr_tpr, marker='.', label='AUC = {}'.format(round(roc_auc_score(Test_Y, pred)*100,2))) \n    plt.xlabel('False Positive Rate')\n    plt.ylabel('True Positive Rate')\n    plt.legend()\n    plt.show()","d7fdeb3b":"# Building Logistic Regression Classifier\n\nLR_model = LogisticRegression(solver='liblinear')\n\nspace = dict()\nspace['solver'] = ['newton-cg', 'lbfgs', 'liblinear']\nspace['penalty'] = ['none', 'l1', 'l2', 'elasticnet']\nspace['C'] = loguniform(1e-5, 100)\n\ncv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n\n# RCV = RandomizedSearchCV(LR_model, space, n_iter=50, scoring='roc_auc', n_jobs=-1, cv=5, random_state=1)\n\nLR = LR_model.fit(Train_X_std, Train_Y)#.best_estimator_\npred = LR.predict(Test_X_std)\npred_prob = LR.predict_proba(Test_X_std)\nClassification_Summary(pred,pred_prob,0)\n\nprint('\\n\\033[1mInterpreting the Output of Logistic Regression:\\n\\033[0m')\n\nprint('intercept ', LR.intercept_[0])\nprint('classes', LR.classes_)\ndisplay(pd.DataFrame({'coeff': LR.coef_[0]}, index=Train_X_std.columns))","6b13f232":"#Building Decision Tree Classifier\n\nDT_model = DecisionTreeClassifier()\n\nparam_dist = {\"max_depth\": [3, None],\n              \"max_features\": randint(1, 9),\n              \"min_samples_leaf\": randint(1, 9),\n              \"criterion\": [\"gini\", \"entropy\"]}\n\ncv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n\nRCV = RandomizedSearchCV(DT_model, param_dist, n_iter=50, scoring='roc_auc', n_jobs=-1, cv=5, random_state=1)\n\nDT = RCV.fit(Train_X_std, Train_Y).best_estimator_\npred = DT.predict(Test_X_std)\npred_prob = DT.predict_proba(Test_X_std)\nClassification_Summary(pred,pred_prob,1)\n\nprint('\\n\\033[1mInterpreting the output of Decision Tree:\\n\\033[0m')\ntree.plot_tree(DT)\nplt.show()","4297504e":"# Building Random-Forest Classifier\n\nRF_model = RandomForestClassifier()\n\nparam_dist={'bootstrap': [True, False],\n            'max_depth': [10, 20, 50, 100, None],\n            'max_features': ['auto', 'sqrt'],\n            'min_samples_leaf': [1, 2, 4],\n            'min_samples_split': [2, 5, 10],\n            'n_estimators': [50, 100]}\n\ncv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n\nRCV = RandomizedSearchCV(RF_model, param_dist, n_iter=50, scoring='roc_auc', n_jobs=-1, cv=5, random_state=1)\n\nRF = RCV.fit(Train_X_std, Train_Y).best_estimator_\npred = RF.predict(Test_X_std)\npred_prob = RF.predict_proba(Test_X_std)\nClassification_Summary(pred,pred_prob,2)\n\nprint('\\n\\033[1mInterpreting the output of Random Forest:\\n\\033[0m')\nrfi=pd.Series(RF.feature_importances_, index=Train_X_std.columns).sort_values(ascending=False)\nplt.barh(rfi.index,rfi.values)\nplt.show()","57bfd15d":"# Building Naive Bayes Classifier\n\nNB_model = BernoulliNB()\n\nparams = {'alpha': [0.01, 0.1, 0.5, 1.0, 10.0]}\ncv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n\nRCV = RandomizedSearchCV(NB_model, params, n_iter=50, scoring='roc_auc', n_jobs=-1, cv=5, random_state=1)\n\nNB = RCV.fit(Train_X_std, Train_Y).best_estimator_\npred = NB.predict(Test_X_std)\npred_prob = NB.predict_proba(Test_X_std)\nClassification_Summary(pred,pred_prob,3)","8b9c3c7a":"# Building Support Vector Machine Classifier\n\nSVM_model = SVC(probability=True).fit(Train_X_std, Train_Y)\n\nsvm_param = {\"C\": [.01, .1, 1, 5, 10, 100],             \n             \"gamma\": [.01, .1, 1, 5, 10, 100],\n             \"kernel\": [\"rbf\"],\n             \"random_state\": [1]}\n\ncv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n\nRCV = RandomizedSearchCV(SVM_model, svm_param, n_iter=50, scoring='roc_auc', n_jobs=-1, cv=5, random_state=1)\n\nSVM = RCV.fit(Train_X_std, Train_Y).best_estimator_\npred = SVM.predict(Test_X_std)\npred_prob = SVM.predict_proba(Test_X_std)\nClassification_Summary(pred,pred_prob,4)","6ffd177b":"# Building K-Neareset Neighbours Classifier\n\nKNN_model = KNeighborsClassifier()\n\nknn_param = {\"n_neighbors\": [i for i in range(1,30,5)],\n             \"weights\": [\"uniform\", \"distance\"],\n             \"algorithm\": [\"ball_tree\", \"kd_tree\", \"brute\"],\n             \"leaf_size\": [1, 10, 30],\n             \"p\": [1,2]}\n\ncv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n\nRCV = RandomizedSearchCV(KNN_model, knn_param, n_iter=50, scoring='roc_auc', n_jobs=-1, cv=5, random_state=1)\n\nKNN = RCV.fit(Train_X_std, Train_Y).best_estimator_\npred = KNN.predict(Test_X_std)\npred_prob = KNN.predict_proba(Test_X_std)\nClassification_Summary(pred,pred_prob,4)","25ec89a6":"# Building Gradient Boosting Classifier\n\nGB_model = GradientBoostingClassifier()#.fit(Train_X_std, Train_Y)\nparam_dist = {\n    \"n_estimators\":[5,20,100,500],\n    \"max_depth\":[1,3,5,7,9],\n    \"learning_rate\":[0.01,0.1,1,10,100]\n}\n\ncv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n\n# RCV = RandomizedSearchCV(GB_model, param_dist, n_iter=50, scoring='roc_auc', n_jobs=-1, cv=5, random_state=1)\n\nGB = GB_model.fit(Train_X_std, Train_Y)#.best_estimator_\npred = GB.predict(Test_X_std)\npred_prob = GB.predict_proba(Test_X_std)\nClassification_Summary(pred,pred_prob,5)","b6e43bc6":"# Building Extreme Gradient Boosting Classifier\n\nXGB_model = XGBClassifier()#.fit(Train_X_std, Train_Y)\n\nparam_dist = {\n \"learning_rate\" : [0.05,0.10,0.15,0.20,0.25,0.30],\n \"max_depth\" : [ 3, 4, 5, 6, 8, 10, 12, 15],\n \"min_child_weight\" : [ 1, 3, 5, 7 ],\n \"gamma\": [ 0.0, 0.1, 0.2 , 0.3, 0.4 ],\n \"colsample_bytree\" : [ 0.3, 0.4, 0.5 , 0.7 ]\n}\n\ncv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n\n#RCV = RandomizedSearchCV(XGB_model, param_dist, n_iter=50, scoring='roc_auc', n_jobs=-1, cv=5, random_state=1)\n\nXGB = XGB_model.fit(Train_X_std, Train_Y.values)#.best_estimator_\npred = XGB.predict(Test_X_std)\npred_prob = XGB.predict_proba(Test_X_std)\nClassification_Summary(pred,pred_prob,6)\n\nplt.bar( Train_X_std.columns,XGB.feature_importances_,)\nplt.show()","c39c7480":"#Plotting Confusion-Matrix of all the predictive Models\n\ndef plot_cm(y_true, y_pred):\n    cm = confusion_matrix(y_true, y_pred, labels=np.unique(y_true))\n    cm_sum = np.sum(cm, axis=1, keepdims=Tr\/ue)\n    cm_perc = cm \/ cm_sum.astype(float) * 100\n    annot = np.empty_like(cm).astype(str)\n    nrows, ncols = cm.shape\n    for i in range(nrows):\n        for j in range(ncols):\n            c = cm[i, j]\n            p = cm_perc[i, j]\n            if i == j:\n                s = cm_sum[i]\n                annot[i, j] = '%.1f%%\\n%d\/%d' % (p, c, s)\n            elif c == 0:\n                annot[i, j] = ''\n            else:\n                annot[i, j] = '%.1f%%\\n%d' % (p, c)\n    cm = pd.DataFrame(cm, index=np.unique(y_true), columns=np.unique(y_true))\n    cm.columns=labels\n    cm.index=labels\n    cm.index.name = 'Actual'\n    cm.columns.name = 'Predicted'\n    #fig, ax = plt.subplots()\n    sns.heatmap(cm, annot=annot, fmt='')# cmap= \"GnBu\"\n    \ndef conf_mat_plot(all_models):\n    plt.figure(figsize=[20,3.5*math.ceil(len(all_models)\/4)])\n    \n    for i in range(len(all_models)):\n        if len(labels)<=4:\n            plt.subplot(2,4,i+1)\n        else:\n            plt.subplot(math.ceil(len(all_models)\/3),3,i+1)\n        pred = all_models[i].predict(Test_X_std)\n        #plot_cm(Test_Y, pred)\n        sns.heatmap(confusion_matrix(Test_Y, pred), annot=True, cmap='BuGn', fmt='.0f') #vmin=0,vmax=5\n        plt.title(Evaluation_Results.index[i])\n    plt.tight_layout()\n    plt.show()\n\nconf_mat_plot([LR,DT,RF,NB,KNN,GB,XGB])","c4ae80fb":"#Comparing all the models Scores\n\n#plt.figure(figsize=[12,5])\nsns.heatmap(Evaluation_Results, annot=True, vmin=80, vmax=100, cmap='BuGn', fmt='.1f')\nplt.show()","b757b630":"#<<<--------------------------------------------------THE END--------------------------------------------------------->>>","19ac6610":"**Inference:** In VIF, RFE & PCA Techniques, we did notice any better scores upon dropping some multicollinear features. But in order to avoid the curse of dimensionality, we can capture top 90% of the data Variance explained by top 33 PCA components.","0f6c6741":"## <center> 4. Data Manipulation","c6f96591":"**Inference:** We shall avoid performing dimensionality reduction for the current problem.","78021965":"---","67188a74":"---","5200ef7f":"## <center> 3. Data Preprocessing","f372d7da":"---","ba0e1be4":"### Here are some of the key outcomes of the project:\n- The Dataset was quiet large totalling around 28Lakh samples & after preprocessing 34.6% of the datasamples were dropped. \n- The samples were highly imbalanced after processing, hence SMOTE Technique was applied on the data to  balance the classes, adding 21.7% more samples to the dataset.\n- Visualising the distribution of data & their relationships, helped us to get some insights on the relationship between the feature-set.\n- Feature Selection\/Eliminination was carried out and appropriate features were shortlisted.\n- Testing multiple algorithms with fine-tuning hyperparamters gave us some understanding on the model performance for various algorithms on this specific dataset.\n- The Boosting & Random Forest Classifier performed exceptionally well on the current dataset, considering Recall Score as the key-metric.\n- Yet it wise to also consider simpler model like Logistic Regression as it is more generalisable & is computationally less expensive, but comes at the cost of slight misclassifications.","5e95724f":"**Inference:** Visualizing the categorical features reveal lot of information about the dataset.","87291857":"**Inference:** The data samples of most of the features do show some patterns. Also they seem\nto have lot of overlap for the outcome classes, making it difficult to be distingusihable. \nLet is proceed to perform cleanup on the data to remove the irregularities...","6ee6203e":"**Inference:** The stats seem to be fine, let us gain more undestanding by visualising the dataset.","5116f915":"## <center> Stractegic Plan of Action:","21904552":"**We aim to solve the problem statement by creating a plan of action, Here are some of the necessary steps:**\n1. Data Exploration\n2. Exploratory Data Analysis (EDA)\n3. Data Pre-processing\n4. Data Manipulation\n5. Feature Selection\/Extraction\n6. Predictive Modelling\n7. Project Outcomes & Conclusion","95f18406":"## <center>1. Data Exploration","5fb5373e":"---","428769a6":"# <center> \u2605 AI \/ ML Project - Credit Card Fraud Detection \u2605\n#### <center> ***Domain: Finance***","e8229371":"## 6. K-Nearest Neighbours Classfier:","d690c560":"## 2. Decisoin Tree Classfier:","13226ccf":"---","d4bfaef7":"**Inference:** The Target Variable seems to be slightly imbalanced! Hence we shall try to perform data augmentation.","61f09876":"---","3772a144":"## <center> 7. Project Outcomes & Conclusions","a6f43d20":"---","8bf3ec0e":"## 5b. Automatic Method - RFE","56bd371e":"---","bb7079bd":"### Description:\n\nIt is important that credit card companies are able to recognize fraudulent credit card transactions so that customers are not charged for items that they did not purchase.\n\nThe dataset contains transactions made by credit cards in September 2013 by European cardholders.\nThis dataset presents transactions that occurred in two days, where we have 492 frauds out of 284,807 transactions. The dataset is highly unbalanced, the positive class (frauds) account for 0.172% of all transactions.\n\nIt contains only numerical input variables which are the result of a PCA transformation. Unfortunately, due to confidentiality issues, we cannot provide the original features and more background information about the data. Features V1, V2, \u2026 V28 are the principal components obtained with PCA, the only features which have not been transformed with PCA are 'Time' and 'Amount'. Feature 'Time' contains the seconds elapsed between each transaction and the first transaction in the dataset. The feature 'Amount' is the transaction Amount, this feature can be used for example-dependant cost-sensitive learning. Feature 'Class' is the response variable and it takes value 1 in case of fraud and 0 otherwise.\n\nGiven the class imbalance ratio, we recommend measuring the accuracy using the Area Under the Precision-Recall Curve (AUPRC). Confusion matrix accuracy is not meaningful for unbalanced classification.\n\n### Acknowledgements:\nThis dataset has been referred from Kaggle: \\\nhttps:\/\/www.kaggle.com\/mlg-ulb\/creditcardfraud\n\n### Objective:\n- Understand the Dataset & cleanup (if required).\n- Build classification model to predict weather the the transaction is fraudulent or not.\n- Also fine-tune the hyperparameters & compare the evaluation metrics of vaious classification algorithms.","eb07c21a":"---","252de260":"---","1f6e2dbb":"<center><img src=\"https:\/\/raw.githubusercontent.com\/Masterx-AI\/Project-Credit-Card-Fraud-Detection\/main\/credit_card.jpeg\" style=\"width: 700px;\"\/>","fd395571":"---","d08941c7":"## <center> 6. Predictive Modeling","16ade162":"## 4. Naive Bayes Classfier:","ac1dc57b":"---","ed147db6":"## 8. Extreme Gradient Boosting Classfier:","ab62c4fa":"## 5a. Manual Method - VIF","aa328f2c":"## 1. Logistic Regression:","5da8112d":"**Inference:** \\\nCorrelation plt between the variables convey lot of information about the realationship betweem them. Especially in case of gender & survived.Hence it is clear that probably women were given more importance to save first. Similiary we have obvious strong correlation between fare & Passenger-Class. \n\nLet us check with different techniques if we can improve the model's performance by performing Feature Selection\/Extraction steps to take care of these multi-collinearity...","7fc3387b":"---","42054130":"## 3. Random Forest Classfier:","bdbcec95":"---","c4fdd105":"---","fbf51f1c":"## <center> 2. Exploratory Data Analysis (EDA)","bc1cceb7":"---","32bc061b":"## 7. Gradient Boosting Classfier:","9ae3224e":"**Insights:** For the current problem statement, it is more important to focus on the Recall score. We can note from the above heatmap that the Boosting & Ensemble Models Performed the best on the current dataset...","5deabc54":"## 5. Support Vector Machine Classfier:","c9a054fe":"**Inference:** The data is somewhat normally distributed. And there are many outliers present in the dataset. We shall fix these outliers..","2c70c832":"---","4033e5a2":"**Strategy:** \\\nWe can fix these multicollinearity with two techniques:\n1. Manual Method - Variance Inflation Factor (VIF)\n2. Automatic Method - Recursive Feature Elimination (RFE)\n3. Decomposition Method - Principle Component Analysis (PCA)","13eeb5ff":"---","74404134":"## <center> 5. Feature Selection\/Extraction","c4a74599":"---"}}