{"cell_type":{"ca3a890f":"code","62c33d4c":"code","177d395e":"code","b9e6b06a":"code","8e2a539f":"code","31b3a776":"code","5dd96e0c":"code","4068709b":"code","7b3c4882":"code","0fcf9ba5":"code","ce38bf87":"code","bdd7faee":"code","a4564355":"code","0c247023":"code","bb460e3f":"code","e58d6a4a":"code","fcdbec1b":"code","c219eee6":"code","f2422289":"code","f909e0a3":"markdown","8c7ea750":"markdown","c4ce4afa":"markdown","11ceadc4":"markdown","a23d6534":"markdown","edfd1e85":"markdown","a38d6b12":"markdown","70448eb2":"markdown","6717aec7":"markdown","3ef6544b":"markdown","ea00ee3e":"markdown"},"source":{"ca3a890f":"# import shutil\n# shutil.rmtree('\/tmp\/autoencoder')","62c33d4c":"import keras\nclass Histories(keras.callbacks.Callback):\n    def on_train_begin(self, logs={}):\n        self.weights = []\n\n    def on_train_end(self, logs={}):\n        return\n\n    def on_epoch_begin(self, epoch, logs={}):\n        return\n\n    def on_epoch_end(self, epoch, logs={}):\n        nx = []\n        x = [x.reshape(-1) for x in self.model.get_weights()]\n        for xi in x :\n            nx += list(xi)\n        self.weights.append(nx)\n        return\n\n    def on_batch_begin(self, batch, logs={}):\n        return\n\n    def on_batch_end(self, batch, logs={}):\n        return","177d395e":"import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt\n%matplotlib inline","b9e6b06a":"data = pd.read_csv(\"..\/input\/Iris.csv\")\nx_train, x_test, y_train, y_test = train_test_split(data[['SepalLengthCm', 'SepalWidthCm',\n                                                          'PetalLengthCm', 'PetalWidthCm']],\n                                                    data['Species'],test_size=0.1, random_state=1)","8e2a539f":"x_train.head()","31b3a776":"# !wget https:\/\/bin.equinox.io\/c\/4VmDzA7iaHb\/ngrok-stable-linux-amd64.zip\n# !unzip ngrok-stable-linux-amd64.zip\n# LOG_DIR = '\/tmp\/autoencoder' # Here you have to put your log directory\n# get_ipython().system_raw(\n#     'tensorboard --logdir {} --host 0.0.0.0 --port 6006 &'\n#     .format(LOG_DIR)\n# )\n# get_ipython().system_raw('.\/ngrok http 6006 &')\n# ! curl -s http:\/\/localhost:4040\/api\/tunnels | python3 -c \\\n#     \"import sys, json; print(json.load(sys.stdin)['tunnels'][0]['public_url'])\"","5dd96e0c":"from keras.layers import Input, Dense\nfrom keras.models import Model\nfrom keras.callbacks import TensorBoard\n\n# this is the size of our encoded representations\nencoding_dim = 2\ninput_dim = 4\n\n# this is our input placeholder\ninput_img = Input(shape=(input_dim,))\n# \"encoded\" is the encoded representation of the input\nencoded = Dense(encoding_dim, activation='relu')(input_img)\n# \"decoded\" is the lossy reconstruction of the input\ndecoded = Dense(input_dim, activation='relu')(encoded)\n\n# this model maps an input to its reconstruction\nautoencoder = Model(input_img, decoded)\n\n# this model maps an input to its encoded representation\nencoder = Model(input_img, encoded)\n\n# create a placeholder for an encoded (2-dimensional) input\nencoded_input = Input(shape=(encoding_dim,))\n# retrieve the last layer of the autoencoder model\ndecoder_layer = autoencoder.layers[-1]\n# create the decoder model\ndecoder = Model(encoded_input, decoder_layer(encoded_input))\n\nautoencoder.compile(loss='mean_squared_error', optimizer='sgd')\n\n\nfrom keras.utils.vis_utils import plot_model\nplot_model(autoencoder, to_file='model_plot.png', show_shapes=True, show_layer_names=True)\n\nfrom matplotlib import pyplot as plt\nimport matplotlib.image as mpimg\nplt.axis(\"off\")\nplt.imshow(mpimg.imread('model_plot.png'))\nplt.show()","4068709b":"x_hist = Histories()\nwhile (True):\n    x_hist = Histories()\n    # this is our input placeholder\n    input_img = Input(shape=(input_dim,))\n    # \"encoded\" is the encoded representation of the input\n    encoded = Dense(encoding_dim, activation='relu')(input_img)\n    # \"decoded\" is the lossy reconstruction of the input\n    decoded = Dense(input_dim, activation='relu')(encoded)\n    \n    # this model maps an input to its reconstruction\n    autoencoder = Model(input_img, decoded)\n\n    # this model maps an input to its encoded representation\n    encoder = Model(input_img, encoded)\n\n    # create a placeholder for an encoded (2-dimensional) input\n    encoded_input = Input(shape=(encoding_dim,))\n    # retrieve the last layer of the autoencoder model\n    decoder_layer = autoencoder.layers[-1]\n    # create the decoder model\n    decoder = Model(encoded_input, decoder_layer(encoded_input))\n\n    autoencoder.compile(loss='mean_squared_error', optimizer='sgd')\n    \n    weight_list = []\n    \n    history = autoencoder.fit(x_train, x_train,\n                    epochs=50,\n                    batch_size=135,\n                    shuffle=True,\n                    validation_data=(x_test, x_test),\n                    verbose=0,\n                   callbacks=[TensorBoard(log_dir='\/tmp\/autoencoder'),x_hist])\n    \n    if (autoencoder.history.history['loss'][-1] < 1):\n        break\n\n# encode and decode some data points\n# note that we take them from the *test* set\nencoded_datapoints = encoder.predict(x_test)\ndecoded_datapoints = decoder.predict(encoded_datapoints)\n\nprint('Original Datapoints :')\nprint(x_test)\nprint('Reconstructed Datapoints :')\nprint(decoded_datapoints)","7b3c4882":"encoded_dataset = encoder.predict(data[['SepalLengthCm', 'SepalWidthCm','PetalLengthCm', 'PetalWidthCm']])\n\nplt.scatter(encoded_dataset[:,0], encoded_dataset[:,1], c=data['Species'].astype('category').cat.codes)\nplt.show()","0fcf9ba5":"autoencoder.get_weights()","ce38bf87":"from matplotlib import pyplot as plt\nimport matplotlib.image as mpimg\nplt.axis(\"off\")\nplt.imshow(mpimg.imread('model_plot.png'))\nplt.show()","bdd7faee":"import matplotlib.pyplot as plt\n\n# Plot training & validation loss values\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('Model loss')\nplt.ylabel('Loss')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Test'], loc='upper left')\nplt.show()","a4564355":"import matplotlib.pyplot as plt\nimport numpy as np\n\nfor i in range(len(x_hist.weights[0])):\n    plt.plot(list(range(len(x_hist.weights))), [x[i] for x in x_hist.weights])\n\nplt.show()","0c247023":"x_hist = Histories()\n# this is our input placeholder\ninput_img = Input(shape=(input_dim,))\n# \"encoded\" is the encoded representation of the input\nencoded = Dense(encoding_dim, activation='relu')(input_img)\n# \"decoded\" is the lossy reconstruction of the input\ndecoded = Dense(input_dim, activation='relu')(encoded)\n\n# this model maps an input to its reconstruction\nautoencoder = Model(input_img, decoded)\n\n# this model maps an input to its encoded representation\nencoder = Model(input_img, encoded)\n\n# create a placeholder for an encoded (2-dimensional) input\nencoded_input = Input(shape=(encoding_dim,))\n# retrieve the last layer of the autoencoder model\ndecoder_layer = autoencoder.layers[-1]\n# create the decoder model\ndecoder = Model(encoded_input, decoder_layer(encoded_input))\n\nautoencoder.compile(loss='mean_squared_error', optimizer='sgd')\n\nweight_list = []\n\nhistory = autoencoder.fit(x_train, x_train,\n                epochs=50,\n                batch_size=135,\n                shuffle=True,\n                validation_data=(x_test, x_test),\n                verbose=0,\n               callbacks=[TensorBoard(log_dir='\/tmp\/autoencoder'),x_hist])","bb460e3f":"import matplotlib.pyplot as plt\nimport numpy as np\n\nfor i in range(len(x_hist.weights[0])):\n    plt.plot(list(range(len(x_hist.weights))), [x[i] for x in x_hist.weights])\n\nplt.show()","e58d6a4a":"# encode and decode some data points\n# note that we take them from the *test* set\nencoded_datapoints = encoder.predict(x_test)\ndecoded_datapoints = decoder.predict(encoded_datapoints)\n\nprint('Original Datapoints :')\nprint(x_test)\nprint('Reconstructed Datapoints :')\nprint(decoded_datapoints)","fcdbec1b":"x_hist = Histories()\n# this is our input placeholder\ninput_img = Input(shape=(input_dim,))\n# \"encoded\" is the encoded representation of the input\nencoded = Dense(encoding_dim, activation='relu',\n               kernel_initializer=keras.initializers.RandomUniform(minval=0, maxval=1, seed=None),\n                bias_initializer=keras.initializers.RandomUniform(minval=0, maxval=1, seed=None))(input_img)\n# \"decoded\" is the lossy reconstruction of the input\ndecoded = Dense(input_dim, activation='relu',\n               kernel_initializer=keras.initializers.RandomUniform(minval=0, maxval=1, seed=None),\n                bias_initializer=keras.initializers.RandomUniform(minval=0, maxval=1, seed=None))(encoded)\n\n# this model maps an input to its reconstruction\nautoencoder = Model(input_img, decoded)\n\n# this model maps an input to its encoded representation\nencoder = Model(input_img, encoded)\n\n# create a placeholder for an encoded (2-dimensional) input\nencoded_input = Input(shape=(encoding_dim,))\n# retrieve the last layer of the autoencoder model\ndecoder_layer = autoencoder.layers[-1]\n# create the decoder model\ndecoder = Model(encoded_input, decoder_layer(encoded_input))\n\nautoencoder.compile(loss='mean_squared_error', optimizer='sgd')\n\nweight_list = []\n\nhistory = autoencoder.fit(x_train, x_train,\n                epochs=50,\n                batch_size=135,\n                shuffle=True,\n                validation_data=(x_test, x_test),\n                verbose=0,\n               callbacks=[TensorBoard(log_dir='\/tmp\/autoencoder'),x_hist])\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nfor i in range(len(x_hist.weights[0])):\n    plt.plot(list(range(len(x_hist.weights))), [x[i] for x in x_hist.weights])\n\nplt.show()\n\n\n# encode and decode some data points\n# note that we take them from the *test* set\nencoded_datapoints = encoder.predict(x_test)\ndecoded_datapoints = decoder.predict(encoded_datapoints)\n\nprint('Original Datapoints :')\nprint(x_test)\nprint('Reconstructed Datapoints :')\nprint(decoded_datapoints)\nprint('Training Loss : ',history.history['loss'][-1])\nprint('Validation Loss : ',history.history['val_loss'][-1])","c219eee6":"x_hist = Histories()\n# this is our input placeholder\ninput_img = Input(shape=(input_dim,))\n# \"encoded\" is the encoded representation of the input\nencoded = Dense(encoding_dim, activation='relu',\n               kernel_initializer=keras.initializers.RandomUniform(minval=0, maxval=1, seed=None),\n                bias_initializer=keras.initializers.RandomUniform(minval=0, maxval=1, seed=None))(input_img)\n# \"decoded\" is the lossy reconstruction of the input\ndecoded = Dense(input_dim, activation='relu',\n               kernel_initializer=keras.initializers.RandomUniform(minval=0, maxval=1, seed=None),\n                bias_initializer=keras.initializers.RandomUniform(minval=0, maxval=1, seed=None))(encoded)\n\n# this model maps an input to its reconstruction\nautoencoder = Model(input_img, decoded)\n\n# this model maps an input to its encoded representation\nencoder = Model(input_img, encoded)\n\n# create a placeholder for an encoded (2-dimensional) input\nencoded_input = Input(shape=(encoding_dim,))\n# retrieve the last layer of the autoencoder model\ndecoder_layer = autoencoder.layers[-1]\n# create the decoder model\ndecoder = Model(encoded_input, decoder_layer(encoded_input))\n\nautoencoder.compile(loss='mean_squared_error', optimizer='sgd')\n\nweight_list = []\n\nhistory = autoencoder.fit(x_train, x_train,\n                epochs=50,\n                batch_size=15,\n                shuffle=True,\n                validation_data=(x_test, x_test),\n                verbose=0,\n               callbacks=[TensorBoard(log_dir='\/tmp\/autoencoder'),x_hist])\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nfor i in range(len(x_hist.weights[0])):\n    plt.plot(list(range(len(x_hist.weights))), [x[i] for x in x_hist.weights])\n\nplt.show()\n\n# Ploting encodings\nencoded_dataset = encoder.predict(data[['SepalLengthCm', 'SepalWidthCm','PetalLengthCm', 'PetalWidthCm']])\n\nplt.scatter(encoded_dataset[:,0], encoded_dataset[:,1], c=data['Species'].astype('category').cat.codes)\nplt.show()\n\n\n# encode and decode some data points\n# note that we take them from the *test* set\nencoded_datapoints = encoder.predict(x_test)\ndecoded_datapoints = decoder.predict(encoded_datapoints)\n\nprint('Original Datapoints :')\nprint(x_test)\nprint('Reconstructed Datapoints :')\nprint(decoded_datapoints)\nprint('Training Loss : ',history.history['loss'][-1])\nprint('Validation Loss : ',history.history['val_loss'][-1])","f2422289":"x_hist = Histories()\n# this is our input placeholder\ninput_img = Input(shape=(input_dim,))\n# \"encoded\" is the encoded representation of the input\nencoded = Dense(8, activation='relu',\n               kernel_initializer=keras.initializers.RandomUniform(minval=0, maxval=1, seed=None),\n                bias_initializer=keras.initializers.RandomUniform(minval=0, maxval=1, seed=None))(input_img)\n\nencoded = Dense(encoding_dim, activation='relu',\n               kernel_initializer=keras.initializers.RandomUniform(minval=0, maxval=1, seed=None),\n                bias_initializer=keras.initializers.RandomUniform(minval=0, maxval=1, seed=None))(encoded)\n\n# \"decoded\" is the lossy reconstruction of the input\ndecoded = Dense(8, activation='relu',\n               kernel_initializer=keras.initializers.RandomUniform(minval=0, maxval=1, seed=None),\n                bias_initializer=keras.initializers.RandomUniform(minval=0, maxval=1, seed=None))(encoded)\n\ndecoded = Dense(input_dim, activation='relu',\n               kernel_initializer=keras.initializers.RandomUniform(minval=0, maxval=1, seed=None),\n                bias_initializer=keras.initializers.RandomUniform(minval=0, maxval=1, seed=None))(decoded)\n\n# this model maps an input to its reconstruction\nautoencoder = Model(input_img, decoded)\n\n# this model maps an input to its encoded representation\nencoder = Model(input_img, encoded)\n\n# create a placeholder for an encoded (2-dimensional) input\nencoded_input = Input(shape=(encoding_dim,))\n# retrieve the last layer of the autoencoder model\ndecoder_layer1 = autoencoder.layers[-2]\ndecoder_layer2 = autoencoder.layers[-1]\n# create the decoder model\ndecoder = Model(encoded_input, decoder_layer2(decoder_layer1(encoded_input)))\n\nautoencoder.compile(loss='mean_squared_error', optimizer='sgd')\n\nfrom keras.utils.vis_utils import plot_model\nplot_model(autoencoder, to_file='model_plot.png', show_shapes=True, show_layer_names=True)\n\nweight_list = []\n\nhistory = autoencoder.fit(x_train, x_train,\n                epochs=50,\n                batch_size=135,\n                shuffle=True,\n                validation_data=(x_test, x_test),\n                verbose=0,\n               callbacks=[TensorBoard(log_dir='\/tmp\/autoencoder'),x_hist])\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nfor i in range(len(x_hist.weights[0])):\n    plt.plot(list(range(len(x_hist.weights))), [x[i] for x in x_hist.weights])\n\nplt.show()\n\n# Ploting encodings\nencoded_dataset = encoder.predict(data[['SepalLengthCm', 'SepalWidthCm','PetalLengthCm', 'PetalWidthCm']])\n\nplt.scatter(encoded_dataset[:,0], encoded_dataset[:,1], c=data['Species'].astype('category').cat.codes)\nplt.show()\n\n\n# encode and decode some data points\n# note that we take them from the *test* set\nencoded_datapoints = encoder.predict(x_test)\ndecoded_datapoints = decoder.predict(encoded_datapoints)\n\nprint('Original Datapoints :')\nprint(x_test)\nprint('Reconstructed Datapoints :')\nprint(decoded_datapoints)\nprint('Training Loss : ',history.history['loss'][-1])\nprint('Validation Loss : ',history.history['val_loss'][-1])\n\nfrom PIL import Image\nImage.open('model_plot.png')","f909e0a3":"###  Effect of initializing weights and biases in between 0 and 1","8c7ea750":"### Add one more layer of size 8 on both sides of middle layer","c4ce4afa":"## Encoder and Decoder","11ceadc4":"# References \n* [Building Autoencoders in Keras](https:\/\/blog.keras.io\/building-autoencoders-in-keras.html)\n* [Matplotlib Gallery](https:\/\/matplotlib.org\/gallery\/index.html)","a23d6534":"### Visualize weights","edfd1e85":"## Importing Data","a38d6b12":"## Launching TensorBoard","70448eb2":"## For single iteration visualizing weights","6717aec7":"### Effect of reducing batch size from 135 to 15","3ef6544b":"# Content\n* Compress the 4 features of Iris Dataset to 2 features using Autoencoder\n* Visualize training using TensorBoard\n* Plot the obtained 2 features and assign different colors to different species\n\n# Issues \n* Reconstructed data is not similar to input data.\n\n# To be Done\n* Try to use dropout in encoding layers\n    * Dropout: A Simple Way to Prevent Neural Networks from Overfitting - http:\/\/jmlr.org\/papers\/volume15\/srivastava14a\/srivastava14a.pdf\n* Try different activation functions\n\n# What's New\n* Visualized weights\n    * Initializing weights and biases in between 0 and 1 is giving better results than default initialization (which is glorot_uniform initialization)\n* Set a threshold on error\n* Tried to reduce batch_size in sgd\n    * Result is improved drastically\n* Added one more layer of length 8 on both sides of middle layer\n    * Model stopped training due to some unknown reasons","ea00ee3e":"## Plotting Encoded Features"}}