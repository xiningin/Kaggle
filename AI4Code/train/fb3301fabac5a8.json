{"cell_type":{"451f8ae1":"code","41238892":"code","14c0be15":"code","3d2514a1":"code","2035fe4c":"code","a0202508":"code","145d4acc":"code","32f19aa4":"code","fe78159f":"code","9ca6993e":"code","a95685b2":"code","71425b29":"code","280009fd":"code","d170ee6e":"code","a5d050bc":"code","5b8be9fb":"code","a7757d2d":"code","fd8a1b22":"code","79ced872":"code","da1545c1":"code","19e8defa":"code","5b57ca0b":"code","de7befdb":"code","11176a44":"code","a5d86f1c":"code","75346be5":"code","ef191cda":"code","f55877cd":"code","bcd5b502":"markdown","2d5e66f6":"markdown","0608c194":"markdown","8fe11da8":"markdown","1b5b08c5":"markdown","48fb145e":"markdown","824b14a1":"markdown","eb834699":"markdown"},"source":{"451f8ae1":"\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport matplotlib.gridspec as gridspec\nimport seaborn as sns\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import cross_val_predict, cross_val_score\nfrom sklearn.model_selection import RandomizedSearchCV, GridSearchCV\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler\nfrom sklearn.metrics import confusion_matrix, classification_report, precision_recall_curve\nfrom sklearn.metrics import f1_score,recall_score,precision_score,accuracy_score\nfrom sklearn.metrics import roc_curve,roc_auc_score\n\nfrom collections import Counter\n\nfrom scipy.stats import norm, multivariate_normal\n\nplt.rcParams['axes.labelsize'] = 14\nplt.rcParams['xtick.labelsize'] = 12\nplt.rcParams['ytick.labelsize'] = 12\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport random\nrandom.seed(42)\n","41238892":"#reading the data and high level understanding\n\ndf = pd.read_csv('..\/input\/creditcardfraud\/creditcard.csv')\nprint(df.shape)\ndf.head()","14c0be15":"df.describe()","3d2514a1":"#checking null data. from output we can see, we do not have any null data. So not going ahead to check feature specific null data.\nnp.sum(df.isnull().any())","2035fe4c":"#checking the distribution of each class. We can see we have 492 (< 1%) records are fraud case records. rest > 99% are not fraud. \n# So this is clearly an imbalanced data set.\n\ndf['Class'].value_counts()","a0202508":"#Let's compare the distribution for Genuine cases & Fraud cases for each feature\nfeatures_list = df.columns\nplt.figure(figsize=(12,31*4))\ngs = gridspec.GridSpec(31,1)\n\nfor i, col in enumerate(features_list):\n    ax = plt.subplot(gs[i])\n    sns.distplot(df[col][df['Class']==0],color='g',label='Genuine Case')\n    sns.distplot(df[col][df['Class']==1],color='r',label='Fraud Case')\n    ax.legend()\nplt.show()","145d4acc":"df.drop(labels = ['V8','V13','V15','V20','V22','V23','V24','V25','V26','V27','V28','Time'], axis = 1, inplace=True)\ndf.columns","32f19aa4":"df.drop(labels = ['Amount','V1','V2','V5','V6','V7','V21'], axis = 1, inplace=True)\ndf.columns","fe78159f":"#Lets understand the relationship between features and data pattern. We will use pair plot from seaborn\nsns.pairplot(df,hue=\"Class\",diag_kind='kde')","9ca6993e":"#Will use this method to select threshold with best F1-score\ndef _SelectThresholdByF1score(probs,y):\n    best_threshold = 0\n    best_f1 = 0\n    f = 0\n\n    best_precision = 0\n    precision =0\n    best_recall = 0\n    recall=0\n    \n    thresholds = sorted(np.unique(probs))\n    #print(thresholds)\n    \n    precisions_list=[]\n    recalls_list=[]\n    for threshold in thresholds:\n        predictions = (probs < threshold)\n        f = f1_score(y, predictions)\n        precision = precision_score(y, predictions)\n        recall = recall_score(y, predictions)\n        #print(\"Theshold {0},Precision {1},Recall {2}\".format(threshold,precision,recall))\n          \n        if f > best_f1:\n            best_f1 = f\n            best_precision = precision\n            best_recall = recall\n            best_threshold = threshold\n        \n        precisions_list.append(precision)\n        recalls_list.append(recall)\n\n    #Precision-Recall Trade-off\n    plt.plot(thresholds,precisions_list,label='Precision')\n    plt.plot(thresholds,recalls_list,label='Recall')\n    plt.xlabel(\"threshold\")\n    plt.title('Precision Recall Trade Off')\n    plt.legend()\n    plt.show()\n\n    print ('Best F1 Score %f' %best_f1)\n    print ('Best Precision Score %f' %best_precision)\n    print ('Best Recall Score %f' %best_recall)\n    print ('Best threshold', best_threshold)","a95685b2":"#Will this method to calculate parameters Mu and Co-variance\ndef _GaussianEstimate(data):\n    mu = np.mean(data,axis=0)\n    sigma = np.cov(data.T)\n    return mu,sigma","71425b29":"# Will use this method to calculate probability density using multivariate gaussian distribution\ndef _CalculateMultivariateGaussianPDF(data,mu,sigma):\n    p = multivariate_normal.pdf(data, mean=mu, cov=sigma)\n    p_transformed = np.power(p,1\/100) # Due to very low value of P (up to e-150), transforming the probability scores by p^1\/100 \n    return p_transformed","280009fd":"#Will use this method to print the scores\ndef _Scores(y,y_pred):\n    print(\"Precision Score: \", precision_score(y,y_pred))\n    print(\"Recall Score: \", recall_score(y,y_pred))\n    print(\"F1 Score: \", f1_score(y,y_pred))\n","d170ee6e":"genuine_data = df[df['Class']==0]\nfraud_data = df[df['Class']==1]","a5d050bc":"#Split genuine data into train & test - 60:40 ratio\ngenuine_train,genuine_test = train_test_split(genuine_data,test_size=0.4,random_state=0)\nprint(genuine_train.shape)\nprint(genuine_test.shape)","5b8be9fb":"#Now split 40% of 'genuine test records' into cross validation & test again (50:50 ratio)\ngenuine_cv,genuine_test = train_test_split(genuine_test,test_size=0.5,random_state=0)\nprint(genuine_cv.shape)\nprint(genuine_test.shape)","a7757d2d":"#Lets split fraud records into cross validation & test records (50:50 ratio)\nfraud_cv,fraud_test = train_test_split(fraud_data,test_size=0.5,random_state=0)","fd8a1b22":"#Drop classification data from Train set\ntrain_set = genuine_train.drop(labels='Class',axis=1)\nprint(train_set.shape)","79ced872":"#Cross validation set\ncv_set = pd.concat([genuine_cv,fraud_cv])\ncv_set_y = cv_set['Class']\ncv_set.drop(labels='Class',axis=1,inplace=True)\nprint(cv_set.shape)","da1545c1":"#Test set\ntest_set = pd.concat([genuine_test,fraud_test])\ntest_set_y = test_set['Class']\ntest_set.drop(labels='Class',axis=1,inplace=True)\nprint(test_set.shape)","19e8defa":"#Let's find out the parameters Mu and Covariance for train set\nmu,sigma = _GaussianEstimate(train_set)\n","5b57ca0b":"#Calculating probabilities using Multivariate Gaussian distribution.\np_train = _CalculateMultivariateGaussianPDF(train_set,mu,sigma)\nprint(p_train.mean())\nprint(p_train.std())\nprint(p_train.max())\nprint(p_train.min())","de7befdb":"#Calculating probabilities for cross validation and test records using the mean and co-variance matrix derived from train set\np_cv = _CalculateMultivariateGaussianPDF(cv_set,mu,sigma)\np_test = _CalculateMultivariateGaussianPDF(test_set,mu,sigma)","11176a44":"#Let find the best threshold with highest F1 -score\n_SelectThresholdByF1score(p_cv,cv_set_y)","a5d86f1c":"#Predictions - on CV data\npred_cv = (p_cv < 0.2425)\n_Scores(cv_set_y, pred_cv)","75346be5":"#Confusion matrix - on CV data\ncnfs_matrix = confusion_matrix(cv_set_y,pred_cv)\nrow_sum = cnfs_matrix.sum(axis=1,keepdims=True)\ncnfs_matrix_norm =cnfs_matrix \/ row_sum \nsns.heatmap(cnfs_matrix_norm, cmap='YlGnBu', annot=True)\nplt.title(\"Normalized Confusion Matrix - Cross Validation set\")","ef191cda":"#We will use the best threshold from CV set and calculate F1-score - on Test data\npred_test = (p_test < 0.2425)\n_Scores(test_set_y,pred_test)","f55877cd":"#Confusion matrix - on Test data\ncnfs_matrix = confusion_matrix(test_set_y,pred_cv)\nrow_sum = cnfs_matrix.sum(axis=1,keepdims=True)\ncnfs_matrix_norm =cnfs_matrix \/ row_sum \nsns.heatmap(cnfs_matrix_norm, cmap='YlGnBu', annot=True)\nplt.title(\"Normalized Confusion Matrix - Test data set\")","bcd5b502":"\n#### we do not find much insight form this pairplot except that most of features have clear separation for fraud cases versus genuine cases. We can observe that distribution of fraud cases are quite different compared to genuine cases in the diagonal plots (kde). All the features looks to be normally distributed. This will fit the criteria and using the original features, we can train the Multivariate Gaussian Distribution algorithm to get the probability.","2d5e66f6":"#### Feature scaling is not required since all the features are already standardized via PCA","0608c194":"#### We can observe 'false negatives' are around 24%. I tried to reduce this false negatives to improve recall score by increasing the threshold. I was able to bringing up the recall score above 80%, however precision was going down to 70% quite fast. \n\n#### So I decided to keep the threshold with the best f1-score, i.e: 0.2425\n","8fe11da8":"## Creating data file: \n\n#### We will split the whole data into multiple division and use them as Train_set, Cross validation set and test set. \n\n#### In *train set*, we will use ONLY genuine records. We will calculate the probability of each feature and keep them as baseline. Later we will calculate the probability for features in CV and test set data and compare with these baseline to classify if the data is genuine or fraud. We will consider 60% of overall genuine records.\n#### Now we are left with 40% of genuine record and 100% of fraud records.\n#### In *Cross Validation set*: we will consider 20% of Genuine records and 50% of fraud record.\n#### In *Test data set*: we will consider 20% of Genuine records and 50% of fraud record.","1b5b08c5":"## Conclusion: \n#### With above Anomaly detection algorithm  we have achieved a decent results with F1-score of 83. We can improve the recall score & thus f1-score further by deriving few new features based on the business knowledge. Here the features already transformed from PCA output and we couldn't evaluate their purpose and do further feature engineering.","48fb145e":"# Credit card fraud detection using normal (Gaussian) distribution.\n\n### 1. Data Preparation \n        In train set, we will use ONLY genuine records. We will calculate the probability of each feature and keep them as baseline. \n        Later we will calculate the probability for features in CV and test set data and compare with these baseline to classify if   \n        the data is genuine or fraud. We will consider 60% of overall genuine records. Now we are left with 40% of genuine record\n        and 100% of fraud records. In Cross Validation set: we will consider 20% of Genuine records and 50% of fraud record. In Test  \n        data set we will consider 20% of Genuine records and 50% of fraud record.    \n\n### 2. Algorithm:\n        For this data set we will use Multivariate normal probability density function. It automatically calculates the relationship \n        (correlation) within features to calculate the probabilities.\n        For a large dataset it might be computationally expensive compared with normal Gaussian probability density function. So if \n        require we can use this alternative option as well.\n\n### 3 . Feature Selection:\n        In data available features are outcome of PCA. So we cannot understand the importance of each features.\n        However for this algorithm we need to choose normally distributes features. Otherwise we need to transform them using log or sqrt function.\n        We will choose the features which has different behavior for genuine and fraud case. also we should have much distinct values (large or very small) in case of anomaly. \n\n","824b14a1":"#### On further precise investigation, we can see for few features, even the distributions are not same for genuine cases and fraud cases, distributions are not unusual as well. We can remove those data as well. We are now most interested in the data, which shows a different behavior for genuine case and fraud case. This will be most useful to modeling anomaly detection algorithm. We will remove the following features. 'V1','V2','V5','V6','V7','V21' and 'Amount'","eb834699":"## Feature selection: \n#### 1) From above series of distributions, we can see Normal Distribution of genuine transactions (class = 0) is matching with Normal Distribution of fraud transactions (class = 1) for 'V8','V13','V15','V20','V22','V23','V24','V25','V26','V27','V28' features. There is no difference in behavior for these features if the transaction is genuine or fraud. We can delete these features as they may not be useful in finding fraud records. \n\n#### 2) Time contains the seconds elapsed between the transaction for that record and the first transaction in the dataset. Also the data is in increasing order. This is not a useful feature and we can remove this as well.\n"}}