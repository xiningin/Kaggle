{"cell_type":{"d6983f52":"code","0a98d22f":"code","0f67fbf4":"code","7a014d3f":"code","8cd05093":"code","5a8c7233":"code","dc1afe59":"code","4948a503":"code","9015bb00":"code","9e77ea78":"code","f978b468":"code","4436e8ee":"code","1733162c":"code","6011b115":"code","1d850863":"code","0f9253c9":"code","ce374292":"code","cc6d5c4e":"code","529fcfa2":"code","daa637df":"code","d5fec476":"code","cef61784":"code","75d1d7ab":"code","526bda82":"code","8287361d":"code","9dff9bdd":"code","776dc81b":"code","cd806f0f":"code","4dee5d99":"code","d5a0e9f2":"code","a774a69d":"code","7f5259d8":"code","815218f8":"code","3912f7be":"code","3ee8de4e":"code","ab3006c6":"code","fcf1a2ed":"code","012ae930":"code","021c0604":"code","a5d00914":"code","510fc4f0":"code","766ff32b":"code","9636e6c8":"code","17d387b9":"code","1def01ff":"code","9cdd39ab":"code","522bf699":"code","641a80ea":"code","4e43bbbb":"code","39c1688c":"code","dbc0c2ff":"code","7757bcb7":"code","6b219738":"code","6226d80c":"code","87104ba1":"code","d64f0974":"code","f5e493cc":"code","b73d10c3":"code","e204347a":"code","e2de12a2":"code","dc58fd6e":"code","752798be":"code","b3456dc5":"code","17fb7039":"code","2b841a2d":"code","45191766":"code","610d1b74":"code","36d09343":"code","53e1071d":"code","673197bc":"code","7dfc6cb7":"code","55596892":"code","7a59b950":"code","89e113e1":"code","d02b846b":"code","8b33a83a":"code","4fa3688f":"code","b9e6be72":"code","efeaabea":"code","3eb05aea":"code","f5676ac2":"code","94fb46a0":"code","bf2e009f":"code","254e3347":"code","f64d0f12":"code","9df0b2bf":"code","75ddbdfb":"code","c50b36bb":"markdown","c58482f8":"markdown","2d706b54":"markdown","8dd37df3":"markdown","4bce77d1":"markdown","23dd0204":"markdown","898071ed":"markdown","7c712063":"markdown","05826dca":"markdown","560e7cc7":"markdown","cb101203":"markdown","72d28b39":"markdown","b0ca8d25":"markdown","9b6b457b":"markdown","cd2ed191":"markdown","424b7757":"markdown","503089c4":"markdown","e43d7f9b":"markdown","376aa5d7":"markdown","62f03624":"markdown","31da598c":"markdown","ae7953d9":"markdown","2fce763b":"markdown","c9400543":"markdown","a829e53c":"markdown","00b0f080":"markdown","aa28b9b6":"markdown","ce528baa":"markdown","3b51a08d":"markdown","c6bc4c69":"markdown","2e0a6e4d":"markdown","063fd6cc":"markdown","2a08fa50":"markdown","b99e16bd":"markdown","8a3edf4a":"markdown","d4b07a72":"markdown","442f845b":"markdown","3c01df15":"markdown","ea19bf6b":"markdown","84cd1d3f":"markdown","7b26240a":"markdown","acd0c004":"markdown","c15fd9d0":"markdown","8668a221":"markdown","aac78e7d":"markdown","642b4888":"markdown","7b072e8d":"markdown","6d3ff221":"markdown","3ea45a39":"markdown","4361f0b1":"markdown","3188444e":"markdown","c340302c":"markdown","22122ba9":"markdown","77805fdb":"markdown","0f64aa62":"markdown","50b16d70":"markdown","95fe216b":"markdown"},"source":{"d6983f52":"import numpy as np \nimport pandas as pd \nfrom IPython.display import display, HTML\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom datetime import datetime\nimport matplotlib.dates as md\nfrom multiprocessing import  Pool\nfrom datetime import datetime\nfrom sklearn.decomposition import PCA\nfrom sklearn.cluster import AgglomerativeClustering\nfrom sklearn.metrics import silhouette_score\nfrom sklearn.preprocessing import QuantileTransformer\nfrom sklearn.model_selection import GridSearchCV, train_test_split, RepeatedKFold, KFold, cross_val_score, ShuffleSplit\nimport xgboost as xgb\nimport tensorflow as tf\nfrom sklearn.metrics import mean_squared_error\nimport math","0a98d22f":"# loading data:\nPATH = \"\/kaggle\/input\/competitive-data-science-predict-future-sales\/\"\nitems_df = pd.read_csv(PATH + \"items.csv\")\nitem_categories_df = pd.read_csv(PATH + \"item_categories.csv\")\nshops_df = pd.read_csv(PATH + \"shops.csv\")\ntrain_df = pd.read_csv(PATH + \"sales_train.csv\")\ntest_df = pd.read_csv(PATH + \"test.csv\")","0f67fbf4":"items_df.head(2)","7a014d3f":"item_categories_df.head(2)","8cd05093":"shops_df.head(2)","5a8c7233":"train_df.head(3)","dc1afe59":"train_df.tail(3)","4948a503":"test_df.head(3)","9015bb00":"train_df.isnull().sum()","9e77ea78":"\"\"\"\nBefore we change anything in the original Dataframe, we create a copy:\n\nTip: This is always a good idea.\n\"\"\"\n\neda_df = train_df.copy()\neda_df[\"date\"]=  pd.to_datetime(eda_df[\"date\"], format='%d.%m.%Y')\neda_df.sort_values(by=\"date\", ascending=True, inplace=True)\nprint(eda_df[\"date\"].min())\nprint(eda_df[\"date\"].max())","f978b468":"eda_df.tail(3)","4436e8ee":"train_df.describe()","1733162c":"train_df.info()","6011b115":"train_df.nunique()","1d850863":"test_df.nunique()","0f9253c9":"\"\"\"\nThis takes quite a long time, so I just copied the numbers and left the code for you to see.\n\nTip: This should go much faster if we combine both data frames and then operate in one data frame. (Try it out using the pd.DataFrame.merge operation \u261c(\uff9f\u30ee\uff9f\u261c) )\nYou will find an example of this in this notebook.\n\"\"\"\n\n# missing_shops = [s for s in test_df[\"shop_id\"].unique() if s not in train_df[\"shop_id\"].unique()]\n# missing_items = [i for i in test_df[\"item_id\"].unique() if i not in train_df[\"item_id\"].unique()]\n# missing_shops_test = [s for s in train_df[\"shop_id\"].unique() if s not in test_df[\"shop_id\"].unique()]\n# missing_items_test = [i for i in train_df[\"item_id\"].unique() if i not in test_df[\"item_id\"].unique()]\nprint(45*\"*\")\nprint(f\"Number of missing shops in train_df: {0}\")\nprint(f\"Number of missing items in train_df: {363}\")\nprint(f\"Number of missing shops in test_df: {18}\")\nprint(f\"Number of missing items in test_df: {17070}\")\nprint(45*\"*\")","ce374292":"len(eda_df[eda_df[\"item_cnt_day\"]<0])","cc6d5c4e":"len(eda_df[eda_df[\"item_cnt_day\"]==0])","529fcfa2":"eda_df[\"item_cnt_day\"] = eda_df[\"item_cnt_day\"].map(lambda x: 0 if x<0 else x)\n# Check:\nlen(eda_df[eda_df[\"item_cnt_day\"]<0])","daa637df":"\"\"\"\nWe will map the top 5% for \"item_price\" and \"item_cnt_day\" to the top qualtile value for the following plots. \n(We have seen before that there are some outliers)\n\"\"\"\nfor col in [\"item_price\", \"item_cnt_day\"]:\n    upper_quantile = eda_df[col].quantile(0.95)\n    eda_df[col] = np.where(eda_df[col]>upper_quantile, upper_quantile,eda_df[col])\n\n\"\"\"\nFurthermore, let us create a column for the actual turnover per item.\n\"\"\"\neda_df[\"sales_per_item\"] = eda_df[\"item_price\"] * eda_df[\"item_cnt_day\"]","d5fec476":"\"\"\"\nLet's start with a general colour setting:\n\"\"\"\nsns.set(rc={'axes.facecolor':\"#F8F9F9\",\n            \"figure.facecolor\":\"#CACFD2\",\n            \"grid.color\":\"#E5E7E9\",\n            \"axes.edgecolor\":\"#17202A\",\n            \"axes.labelcolor\":\"#17202A\",\n            \"text.color\":\"#17202A\"\n           }) \n\"\"\"\nChange the axis label size: (We will set the other sizes individually).\n\"\"\"\nsns.set_context(rc={\"axes.labelsize\" : 20})","cef61784":"fig = plt.figure(figsize=(25,7))\ngs = fig.add_gridspec(1, 3)\nax00 = fig.add_subplot(gs[0,0])\nax01 = fig.add_subplot(gs[0,1])\nax02 = fig.add_subplot(gs[0,2])\nax00.tick_params(axis='both', labelsize=15)\nax01.tick_params(axis='both', labelsize=15)\nax02.tick_params(axis='both', labelsize=15)\nax00.set_title('Sales per item per day', fontsize=20)\nax01.set_title('Item price distribution', fontsize=20)\nax02.set_title('Item count distribution', fontsize=20)\nsns.histplot(data = eda_df ,x=\"sales_per_item\", kde=True, bins=50, ax=ax00, color=\"violet\")\nsns.histplot(data = eda_df ,x=\"item_price\", kde=True, bins=50, ax=ax01, color=\"tomato\")\nsns.histplot(data = eda_df ,x=\"item_cnt_day\", kde=False, bins=10, ax=ax02, color=\"cornflowerblue\")\n\nfig.subplots_adjust(top=0.8)\nfig.suptitle('Sales Feature Distributions per Day', fontsize=\"28\");","75d1d7ab":"eda_df","526bda82":"fig, ax =  plt.subplots(figsize=(25,5))\nax.tick_params(axis='both', labelsize=15)\nax.set_title('Item Price Development', fontsize=20)\nprict_df = eda_df[(eda_df[\"item_price\"] < 500) & (eda_df[\"item_price\"] > 100)].copy()\n\nfor i in prict_df[\"item_id\"].unique()[50:60]:\n    tmp_df = prict_df[prict_df[\"item_id\"]==i].copy()\n    sns.lineplot(x=\"date_block_num\", y=\"item_price\", data=tmp_df, palette=\"Set2\", ax=ax)","8287361d":"item_categories_df.nunique()","9dff9bdd":"\"\"\"\nDon`t do it like this:\n\"\"\"\n#eda_df[\"item_category\"] = eda_df[\"item_id\"].map(lambda x : items_df[items_df[\"item_id\"]==x][\"item_category_id\"].values[0])\n\"\"\"\nDo it like this:\n\"\"\"\neda_df = pd.merge(eda_df, items_df, on='item_id', how='inner')\neda_df = pd.merge(eda_df, item_categories_df, on='item_category_id', how='inner')\neda_df.head()","776dc81b":"\"\"\"\nSo how can we answer this question with pandas? \n\nIn SQL you would do something like \n\nSELECT item_id, countd(item_category_id)\nFROM eda_table \nGROUP BY item_id \nHAVING countd(item_category_id) > 0;\n\nLet's do the same thing. We group by item_id and count the number of distinct category_ids. \nWe save this information an a new dataframe and then check if we have entries with more then one item_catgory_id\n\"\"\"\nunique_item_to_cat_df = eda_df[[\"item_id\", \"item_category_id\"]].groupby([\"item_id\"]).nunique(\"item_category_id\")\nlen(unique_item_to_cat_df[unique_item_to_cat_df[\"item_category_id\"] > 1])","cd806f0f":"eda_test_df = test_df.copy()\neda_test_df = pd.merge(eda_test_df, items_df, on=\"item_id\", how=\"inner\")\nprint(f\"Number of unique item categories in test_df: {eda_test_df.item_category_id.nunique()}\")","4dee5d99":"item_category_info_df = pd.DataFrame(\n    columns=[\"name\", \"num_products\", \"first_sold\", \"last_sold\", \"min_price\", \"max_price\", \"mean_price\", \"median_price\",\n              \"mean_item_cnt_month\", \"median_item_cnt_month\", \"mean_cnt_jan\", \"mean_cnt_feb\", \"mean_cnt_mar\", \"mean_cnt_apr\",\n              \"mean_cnt_may\", \"mean_cnt_jun\", \"mean_cnt_jul\", \"mean_cnt_aug\", \"mean_cnt_sep\", \"mean_cnt_oct\", \"mean_cnt_nov\", \"mean_cnt_dez\"],\n    index = item_categories_df[\"item_category_id\"].unique())\n\nfor cid in item_category_info_df.index:\n    item_category_info_df.at[cid, \"name\"] = item_categories_df[item_categories_df[\"item_category_id\"]==cid][\"item_category_name\"].values\n    item_category_info_df.at[cid, \"num_products\"] = items_df[items_df[\"item_category_id\"]==cid][\"item_id\"].nunique()\n    cdf= eda_df[eda_df[\"item_category_id\"]==cid].copy()\n    item_category_info_df.at[cid, \"first_sold\"] = cdf[\"date\"].min()\n    item_category_info_df.at[cid, \"last_sold\"] = cdf[\"date\"].max()\n    item_category_info_df.at[cid, \"min_price\"] = cdf[\"item_price\"].min()\n    item_category_info_df.at[cid, \"max_price\"] = cdf[\"item_price\"].max()\n    item_category_info_df.at[cid, \"mean_price\"] = cdf[\"item_price\"].mean()\n    item_category_info_df.at[cid, \"median_price\"] = cdf[\"item_price\"].median()\n    \"\"\"\n    The following part is a little tricky. We first change the date feature to the first of each month so that each date in a month is mapped to the first day of the month.\n    Then we group by this new date. We then get a dataframe with the values sum_sales and sum_item_cnt for each month in each year.\n    As a final step, we group by month to get the mean value for each month. I hope I didn't mess it up, it's getting late \u00af\\_(\u30c4)_\/\u00af. If you have any suggestions on how to achieve the same\n    result with less effort, let me know in the comments section.\n    \"\"\"\n    # ------------------------------------------------------------------------------------------------------\n    cdf[\"month\"] = cdf[\"date\"].dt.month\n    cdf[\"year\"] = cdf[\"date\"].dt.year\n    cdf[\"date\"] = pd.to_datetime(cdf[[\"year\", \"month\"]].assign(DAY=1))\n    cdf = cdf[[\"date\", \"item_cnt_day\", \"sales_per_item\"]].groupby(\"date\").sum().reset_index()\n    cdf = cdf[[\"date\", \"item_cnt_day\", \"sales_per_item\"]].groupby(cdf[\"date\"].dt.month).mean().reset_index()\n    cdf.rename(columns ={\"item_cnt_day\": \"item_cnt_month\", \"sales_per_item\": \"sales_per_month\"}, inplace=True)\n    # ------------------------------------------------------------------------------------------------------\n    item_category_info_df.at[cid, \"mean_item_cnt_month\"] = cdf[\"item_cnt_month\"].mean()\n    item_category_info_df.at[cid, \"median_item_cnt_month\"] = cdf[\"item_cnt_month\"].median()\n    \"\"\"\n    If we now want to have the average number of items sold per month, we have to take into account that not all item categories are sold every month.\n    \"\"\"\n    month_mapping = {1: \"mean_cnt_jan\", 2: \"mean_cnt_feb\", 3: \"mean_cnt_mar\", 4: \"mean_cnt_apr\", 5: \"mean_cnt_may\", 6: \"mean_cnt_jun\",\n              7: \"mean_cnt_jul\", 8: \"mean_cnt_aug\", 9: \"mean_cnt_sep\", 10: \"mean_cnt_oct\", 11: \"mean_cnt_nov\", 12: \"mean_cnt_dez\"}\n    for m in cdf[\"date\"].unique():\n        item_category_info_df.at[cid, month_mapping[m]] = cdf[cdf[\"date\"]==m][\"item_cnt_month\"].values[0]  # we could also do cdf.iloc[m-1] but i found this more readable.","d5a0e9f2":"item_category_info_df","a774a69d":"fig = plt.figure(figsize=(25,5))\ngs = fig.add_gridspec(1, 2)\nax00 = fig.add_subplot(gs[0,0])\nax01 = fig.add_subplot(gs[0,1])\nax00.tick_params(axis='both', labelsize=15)\nax01.tick_params(axis='both', labelsize=15)\nax00.set_title('first_sold distribution', fontsize=20)\nax01.set_title('last_sold distribution', fontsize=20)\nsns.histplot(data = item_category_info_df ,x=\"first_sold\", bins=50, ax=ax00).set(xlabel=None)\nsns.histplot(data = item_category_info_df ,x=\"last_sold\", bins=50, ax=ax01).set(xlabel=None)\nfig.subplots_adjust(top=0.82)\nfig.suptitle('Date Feature Distribution', fontsize=\"28\");","7f5259d8":"fig = plt.figure(figsize=(25,15))\ngs = fig.add_gridspec(3, 2)\nax00 = fig.add_subplot(gs[0,0])\nax01 = fig.add_subplot(gs[0,1])\nax10 = fig.add_subplot(gs[1,0])\nax11 = fig.add_subplot(gs[1,1])\nax20 = fig.add_subplot(gs[2,0])\nax21 = fig.add_subplot(gs[2,1])\nax00.tick_params(axis='both', labelsize=15)\nax01.tick_params(axis='both', labelsize=15)\nax10.tick_params(axis='both', labelsize=15)\nax11.tick_params(axis='both', labelsize=15)\nax20.tick_params(axis='both', labelsize=15)\nax21.tick_params(axis='both', labelsize=15)\nax00.set_title('mean_item_cnt_month distribution', fontsize=20)\nax01.set_title('median_item_cnt_month distribution', fontsize=20)\nax10.set_title('min_price distribution', fontsize=20)\nax11.set_title('max_price distribution', fontsize=20)\nax20.set_title('mean_price distribution', fontsize=20)\nax21.set_title('median_price distribution', fontsize=20)\nsns.histplot(data = item_category_info_df ,x=\"mean_item_cnt_month\", bins=100, ax=ax00).set(xlabel=None)\nsns.histplot(data = item_category_info_df ,x=\"median_item_cnt_month\", bins=100, ax=ax01).set(xlabel=None)\nsns.histplot(data = item_category_info_df ,x=\"min_price\", bins=100, ax=ax10).set(xlabel=None)\nsns.histplot(data = item_category_info_df ,x=\"max_price\", bins=100, ax=ax11).set(xlabel=None)\nsns.histplot(data = item_category_info_df ,x=\"mean_price\",bins=100, ax=ax20).set(xlabel=None)\nsns.histplot(data = item_category_info_df ,x=\"median_price\", bins=100, ax=ax21).set(xlabel=None)\nfig.subplots_adjust(top=0.92)\nfig.suptitle('Numeric Feature Distribution', fontsize=\"28\");","815218f8":"item_cat_sales_dev_df = pd.DataFrame(columns=[\"item_cat_id\",\"month\", \"mean_cnt\"])\ni = 0\nfor index, row in item_category_info_df.iterrows():\n    for m in month_mapping.keys():\n        item_cat_sales_dev_df.at[i, \"item_cat_id\"] = index\n        item_cat_sales_dev_df.at[i, \"month\"] = m\n        item_cat_sales_dev_df.at[i, \"mean_cnt\"] = row[month_mapping[m]]\n        i += 1;\nitem_cat_sales_dev_df = item_cat_sales_dev_df.astype({\"item_cat_id\": \"int8\", \"month\": \"int32\", \"mean_cnt\": \"float32\"});","3912f7be":"print(\"Sales development for categoriy 15:\")\nitem_cat_sales_dev_df[item_cat_sales_dev_df[\"item_cat_id\"]==15]","3ee8de4e":"fig = plt.figure(figsize=(25,25))\ngs = fig.add_gridspec(3, 2)\nax0x = fig.add_subplot(gs[0,:])\nax10 = fig.add_subplot(gs[1,0])\nax11 = fig.add_subplot(gs[1,1])\nax20 = fig.add_subplot(gs[2,0])\nax21 = fig.add_subplot(gs[2,1])\nax0x.tick_params(axis='both', labelsize=15)\nax10.tick_params(axis='both', labelsize=15)\nax11.tick_params(axis='both', labelsize=15)\nax20.tick_params(axis='both', labelsize=15)\nax21.tick_params(axis='both', labelsize=15)\nax0x.set_title('Overall mean item count per month', fontsize=20)\nax10.set_title('Mean item count per month (max count < 100)', fontsize=20)\nax11.set_title('Mean item count per month (100 <= max count < 1000)', fontsize=20)\nax20.set_title('Mean item count per month (1000 <= max count < 5000)', fontsize=20)\nax21.set_title('Mean item count per month (5000 <max count)', fontsize=20)\n\nsns.lineplot(x=\"month\", y=\"mean_cnt\", data=item_cat_sales_dev_df, palette=\"Set2\", ax=ax0x)\n\n\"\"\"\nPlot each category, depending on its max mean_cnt:\n\"\"\"\n\nfor c in item_cat_sales_dev_df[\"item_cat_id\"].unique():\n    tmp_df = item_cat_sales_dev_df[item_cat_sales_dev_df[\"item_cat_id\"]==c].copy()\n    max_mean_count = tmp_df[\"mean_cnt\"].max()\n    #sns.lineplot(x=\"month\", y=\"mean_cnt\", data=tmp_df, palette=\"Set2\", ax=ax0x)\n    if max_mean_count < 100:\n        sns.lineplot(x=\"month\", y=\"mean_cnt\", data=tmp_df, palette=\"Set2\", ax=ax10)\n    elif 100 <= max_mean_count < 1000 :\n        sns.lineplot(x=\"month\", y=\"mean_cnt\", data=tmp_df, palette=\"Set2\", ax=ax11)\n    elif 1000 <= max_mean_count < 5000 :\n        sns.lineplot(x=\"month\", y=\"mean_cnt\", data=tmp_df, palette=\"Set2\", ax=ax20)\n    else: \n        # > 5000\n        sns.lineplot(x=\"month\", y=\"mean_cnt\", data=tmp_df, palette=\"Set2\", ax=ax21)          \nplt.legend([],[], frameon=False)\nfig.subplots_adjust(top=0.94)\nfig.suptitle('Turnover Trend for Each Item Category', fontsize=\"28\");","ab3006c6":"m_df = eda_df.copy()\n\"\"\"\nFist we need to cast date to datetime and then we can use all the sweet \".dt\"-features\n\"\"\" \nm_df['date']= pd.to_datetime(m_df['date'])\nm_df[\"month\"] = m_df[\"date\"].dt.month\nm_df[\"year\"] = m_df[\"date\"].dt.year\nm_df['date'] = pd.to_datetime(m_df[['year', 'month']].assign(DAY=28)) # workaround for end of month (its just for the visuals so we should be ok here)\nm_df = m_df[[\"date\", \"item_cnt_day\", \"sales_per_item\"]].groupby(\"date\").sum().reset_index()\nm_df.rename(columns ={\"item_cnt_day\": \"item_cnt_month\", \"sales_per_item\": \"sales_per_month\"}, inplace=True)\nm_df","fcf1a2ed":"fig = plt.figure(figsize=(25,15))\ngs = fig.add_gridspec(2, 1)\nax0 = fig.add_subplot(gs[0,0])\nax1 = fig.add_subplot(gs[1,0])\nax0.tick_params(axis='both', which='major', labelsize=15)\nax1.tick_params(axis='both', which='major', labelsize=15)\nax0.set_title('Sales per Month', fontsize=20)\nax1.set_title('Items Sold per Month', fontsize=20)\n\n# adding hrozontal lines\nax0.hlines(y=m_df[\"sales_per_month\"].mean(), color='b', linewidth=2, alpha=.7, ls='--', xmin=m_df[\"date\"].min() , xmax = m_df[\"date\"].max(), label=\"MEAN\") \nax0.hlines(y=m_df[\"sales_per_month\"].quantile(0.95), color='g', linewidth=2, alpha=.7, ls='--', xmin=m_df[\"date\"].min() , xmax = m_df[\"date\"].max(), label=\"95_QUANTIL\") \nax0.hlines(y=m_df[\"sales_per_month\"].quantile(0.05), color='r', linewidth=2, alpha=.7, ls='--', xmin=m_df[\"date\"].min() , xmax = m_df[\"date\"].max(), label=\"05_QUANTIL\")\nax1.hlines(y=m_df[\"item_cnt_month\"].mean(), color='b', linewidth=2, alpha=.7, ls='--', xmin=m_df[\"date\"].min() , xmax = m_df[\"date\"].max(), label=\"MEAN\") \nax1.hlines(y=m_df[\"item_cnt_month\"].quantile(0.95), color='g', linewidth=2, alpha=.7, ls='--', xmin=m_df[\"date\"].min() , xmax = m_df[\"date\"].max(), label=\"95_QUANTIL\") \nax1.hlines(y=m_df[\"item_cnt_month\"].quantile(0.05), color='r', linewidth=2, alpha=.7, ls='--', xmin=m_df[\"date\"].min() , xmax = m_df[\"date\"].max(), label=\"05_QUANTIL\")\n\n# adding vertical lines\nax0.vlines(x=datetime.strptime(\"2013-11-01\", \"%Y-%m-%d\"), ymin=m_df[\"sales_per_month\"].min(), ymax=m_df[\"sales_per_month\"].max(), color=\"black\", linewidth=2)\nax0.vlines(x=datetime.strptime(\"2013-12-01\", \"%Y-%m-%d\"), ymin=m_df[\"sales_per_month\"].min(), ymax=m_df[\"sales_per_month\"].max(), color=\"black\", linewidth=2)\nax0.vlines(x=datetime.strptime(\"2014-11-01\", \"%Y-%m-%d\"), ymin=m_df[\"sales_per_month\"].min(), ymax=m_df[\"sales_per_month\"].max(), color=\"black\", linewidth=2)\nax0.vlines(x=datetime.strptime(\"2014-12-01\", \"%Y-%m-%d\"), ymin=m_df[\"sales_per_month\"].min(), ymax=m_df[\"sales_per_month\"].max(), color=\"black\", linewidth=2)\nax1.vlines(x=datetime.strptime(\"2013-11-01\", \"%Y-%m-%d\"), ymin=m_df[\"item_cnt_month\"].min(), ymax=m_df[\"item_cnt_month\"].max(), color=\"black\", linewidth=2)\nax1.vlines(x=datetime.strptime(\"2013-12-01\", \"%Y-%m-%d\"), ymin=m_df[\"item_cnt_month\"].min(), ymax=m_df[\"item_cnt_month\"].max(), color=\"black\", linewidth=2)\nax1.vlines(x=datetime.strptime(\"2014-11-01\", \"%Y-%m-%d\"), ymin=m_df[\"item_cnt_month\"].min(), ymax=m_df[\"item_cnt_month\"].max(), color=\"black\", linewidth=2)\nax1.vlines(x=datetime.strptime(\"2014-12-01\", \"%Y-%m-%d\"), ymin=m_df[\"item_cnt_month\"].min(), ymax=m_df[\"item_cnt_month\"].max(), color=\"black\", linewidth=2)\n\n# adding some arrows (this can be a bit trickt and frustrating ;) )\nstyle = dict(size=15, color='black')\n\nx_date_1 = md.date2num(datetime.strptime(\"2013-9-15\", \"%Y-%m-%d\"))\nx_date_1_dt = md.date2num(datetime.strptime(\"2013-11-15\", \"%Y-%m-%d\")) - x_date_1\nx_date_2 = md.date2num(datetime.strptime(\"2014-9-15\", \"%Y-%m-%d\"))\nx_date_2_dt = md.date2num(datetime.strptime(\"2014-11-15\", \"%Y-%m-%d\")) - x_date_2\n\nax0.arrow(x_date_1, 125000000, x_date_1_dt, -35000000, head_width=10, head_length=5000000, width=3)\nax0.text(x_date_1, 125000000, \"November 2013\", ha='right', **style)\nax0.arrow(x_date_2, 125000000, x_date_2_dt, -25000000, head_width=10, head_length=5000000, width=3)\nax0.text( x_date_2, 125000000, \"November 2014\", ha='right', **style)\n\nax1.arrow(x_date_1, 130000, x_date_1_dt, -20000, head_width=15, head_length=1500, width=3)\nax1.text(x_date_1, 125000, \"November 2013\", ha='right', **style)\nax1.arrow(x_date_2, 130000, x_date_2_dt, -33000, head_width=15, head_length=1500, width=3)\nax1.text( x_date_2, 125000, \"November 2014\", ha='right', **style)\n\n\nsns.lineplot(data=m_df, y=\"sales_per_month\", x = \"date\", palette=\"tab10\", linewidth=5, ax=ax0, label=\"sales_per_month\")\nsns.lineplot(data=m_df, y=\"item_cnt_month\", x = \"date\", palette=\"tab10\", linewidth=5, ax=ax1, label=\"item_cnt_month\")\nfig.subplots_adjust(top=0.915)\nfig.suptitle('Sales Numbers per Month', fontsize=\"28\")\nplt.legend();","012ae930":"m_df[m_df[\"date\"].dt.year == 2013]","021c0604":"fig = plt.figure(figsize=(25,7))\ngs = fig.add_gridspec(1, 2)\nax0 = fig.add_subplot(gs[0,0])\nax1 = fig.add_subplot(gs[0,1])\nax0.tick_params(axis='both', which='major', labelsize=15)\nax1.tick_params(axis='both', which='major', labelsize=15)\nax0.set_title('Sales per Month', fontsize=20)\nax1.set_title('Items Sold per Month', fontsize=20)\n\"\"\"\nTip: See how we can use the .dt property since \"date\" is a datetime column. \n     You can also replace the x-values as described below.\n\"\"\"\nmonth_ = [\"jan\",\"feb\",\"mar\",\"apr\",\"may\",\"jun\",\"jul\",\"aug\",\"sep\",\"oct\",\"nov\",\"dec\"]\nsns.lineplot(data=m_df[m_df[\"date\"].dt.year == 2013], y=\"sales_per_month\", x=month_, palette=\"tab10\", linewidth=5, ax=ax0, label=\"2013\")\nsns.lineplot(data=m_df[m_df[\"date\"].dt.year == 2014], y=\"sales_per_month\", x=month_, palette=\"tab10\", linewidth=5, ax=ax0, label=\"2014\")\nsns.lineplot(data=m_df[m_df[\"date\"].dt.year == 2015], y=\"sales_per_month\", x=month_[:-2], palette=\"tab10\", linewidth=5, ax=ax0, label=\"2015\")\n\nsns.lineplot(data=m_df[m_df[\"date\"].dt.year == 2013], y=\"item_cnt_month\", x=month_, palette=\"tab10\", linewidth=5, ax=ax1, label=\"2013\")\nsns.lineplot(data=m_df[m_df[\"date\"].dt.year == 2014], y=\"item_cnt_month\", x=month_, palette=\"tab10\", linewidth=5, ax=ax1, label=\"2014\")\nsns.lineplot(data=m_df[m_df[\"date\"].dt.year == 2015], y=\"item_cnt_month\", x=month_[:-2], palette=\"tab10\", linewidth=5, ax=ax1, label=\"2015\")\nfig.subplots_adjust(top=0.8)\nfig.suptitle('Sales Numbers per Month', fontsize=\"28\")\nax0.legend()\nax1.legend();","a5d00914":"\"\"\"\nWe do the same as before, but now we also add the shot_id in our groupby-step.\n\nAs always, if you have any questions about the code, feel free to ask them in the comments section.\nThe same goes for suggestions for improvements ofcorse!\n\"\"\"\nm_shop_df = eda_df.copy()\n\nm_shop_df['date']= pd.to_datetime(m_shop_df['date'])\nm_shop_df[\"month\"] = m_shop_df[\"date\"].dt.month\nm_shop_df[\"year\"] = m_shop_df[\"date\"].dt.year\nm_shop_df['date'] = pd.to_datetime(m_shop_df[['year', 'month']].assign(DAY=28)) # workaround for end of month (its just for the visuals so we should be ok here)\nm_shop_df = m_shop_df[[\"date\", \"item_cnt_day\", \"sales_per_item\", \"shop_id\"]].groupby([\"date\", \"shop_id\"]).sum().reset_index()\nm_shop_df.rename(columns ={\"item_cnt_day\": \"item_cnt_month\", \"sales_per_item\": \"sales_per_month\"}, inplace=True)\nm_shop_df","510fc4f0":"fig = plt.figure(figsize=(25,10))\ngs = fig.add_gridspec(1, 2)\nax0 = fig.add_subplot(gs[0,0])\nax1 = fig.add_subplot(gs[0,1])\nax0.tick_params(axis='both', which='major', labelsize=15)\nax1.tick_params(axis='both', which='major', labelsize=15)\nax0.set_title('Sale per Month', fontsize=20)\nax1.set_title('Items Sold per Month', fontsize=20)\nsns.boxplot(x=m_shop_df[\"date\"].dt.month, y=\"sales_per_month\", data=m_shop_df, ax=ax0, palette=\"Paired\")\nsns.boxplot(x=m_shop_df[\"date\"].dt.month, y=\"item_cnt_month\", data=m_shop_df, ax=ax1, palette=\"Paired\")\nfig.subplots_adjust(top=0.87)\nfig.suptitle('Sales Numbers per Month and Shop', fontsize=\"28\");","766ff32b":"\"\"\"\nCreate a data dataframe with some basic information about our 60 shops:\n\"\"\"\nshop_info_df = pd.DataFrame(columns=[\"shop_name\", \"num_products\", \"fist_business_m\", \"last_business_m\",\n                                     \"min_price\", \"max_price\", \"mean_price\", \"median_price\", \"mean_sales_pm\", \"median_sales_pm\"],\n                            index = m_shop_df[\"shop_id\"].unique())\n\nfor sid in m_shop_df[\"shop_id\"].unique():\n    shop_info_df.at[sid, \"shop_name\"] = shops_df[shops_df[\"shop_id\"]==sid][\"shop_name\"].values[0]\n    shop_info_df.at[sid, \"num_products\"] = train_df[train_df[\"shop_id\"]==sid][\"item_id\"].nunique()\n    sdf= m_shop_df[m_shop_df[\"shop_id\"]==sid]\n    shop_info_df.at[sid, \"fist_business_m\"] = sdf[\"date\"].min()\n    shop_info_df.at[sid, \"last_business_m\"] = sdf[\"date\"].max()\n    shop_info_df.at[sid, \"min_price\"] = train_df[train_df[\"shop_id\"]==sid][\"item_price\"].min()\n    shop_info_df.at[sid, \"max_price\"] = train_df[train_df[\"shop_id\"]==sid][\"item_price\"].max()\n    shop_info_df.at[sid, \"mean_price\"] = train_df[train_df[\"shop_id\"]==sid][\"item_price\"].mean()\n    shop_info_df.at[sid, \"median_price\"] = train_df[train_df[\"shop_id\"]==sid][\"item_price\"].median()\n    shop_info_df.at[sid, \"mean_sales_pm\"] = sdf[\"sales_per_month\"].median()\n    shop_info_df.at[sid, \"median_sales_pm\"] = sdf[\"sales_per_month\"].median()\n    \"\"\"\n    We will do it a little bit different for this shop dataframe. Here, the average turnover per month would\n    not be so interesting.Instead, we can use our m_shop_df for the sales figures per month.\n    \"\"\"\n# changing datatypes from object to datetime float and int:\nshop_info_df['fist_business_m']= pd.to_datetime(shop_info_df['fist_business_m'])\nshop_info_df['last_business_m']= pd.to_datetime(shop_info_df['last_business_m'])\nshop_info_df[\"fist_business_m\"] = shop_info_df[\"fist_business_m\"].dt.strftime(\"%Y-%m\")\nshop_info_df[\"last_business_m\"] = shop_info_df[\"last_business_m\"].dt.strftime(\"%Y-%m\")\nshop_info_df = shop_info_df.astype({'num_products': 'int32',\n                                    \"min_price\": 'float32',\n                                    \"max_price\": 'float32',\n                                    \"mean_price\": 'float32',\n                                    \"median_price\": 'float32',\n                                    \"mean_sales_pm\": 'float32',\n                                    \"median_sales_pm\": 'float32'})","9636e6c8":"shop_info_df.head(5)","17d387b9":"fig = plt.figure(figsize=(25,5))\ngs = fig.add_gridspec(1, 2)\nax00 = fig.add_subplot(gs[0,0])\nax01 = fig.add_subplot(gs[0,1])\nax00.tick_params(axis='both', labelsize=15)\nax01.tick_params(axis='both', labelsize=15)\nax00.set_title('fist_business_m distribution', fontsize=20)\nax01.set_title('last_business_m distribution', fontsize=20)\nax00.tick_params(labelrotation=45, axis=\"x\")\nax01.tick_params(labelrotation=45, axis=\"x\")\nsns.histplot(data = shop_info_df ,x=\"fist_business_m\", bins=15, ax=ax00)\nsns.histplot(data = shop_info_df ,x=\"last_business_m\", bins=15, ax=ax01);\nfig.subplots_adjust(top=0.8)\nfig.suptitle('First and Last Day of Business Distribution', fontsize=\"28\");","1def01ff":"fig = plt.figure(figsize=(25,20))\ngs = fig.add_gridspec(4, 2)\nax00 = fig.add_subplot(gs[0,0])\nax01 = fig.add_subplot(gs[0,1])\nax10 = fig.add_subplot(gs[1,0])\nax11 = fig.add_subplot(gs[1,1])\nax20 = fig.add_subplot(gs[2,0])\nax21 = fig.add_subplot(gs[2,1])\nax3 = fig.add_subplot(gs[3,:])\nax00.tick_params(axis='both', labelsize=15)\nax01.tick_params(axis='both', labelsize=15)\nax10.tick_params(axis='both', labelsize=15)\nax11.tick_params(axis='both', labelsize=15)\nax20.tick_params(axis='both', labelsize=15)\nax21.tick_params(axis='both', labelsize=15)\nax3.tick_params(axis='both', labelsize=15)\nax00.set_title('mean_sales_pm distribution', fontsize=20)\nax01.set_title('median_sales_pm distribution', fontsize=20)\nax10.set_title('min_price distribution', fontsize=20)\nax11.set_title('max_price distribution', fontsize=20)\nax20.set_title('mean_price distribution', fontsize=20)\nax21.set_title('median_price distribution', fontsize=20)\nax3.set_title('num_products distribution', fontsize=20)\nsns.histplot(data = shop_info_df ,x=\"mean_sales_pm\", bins=100, ax=ax00).set(xlabel=None)\nsns.histplot(data = shop_info_df ,x=\"median_sales_pm\", bins=100, ax=ax01).set(xlabel=None)\nsns.histplot(data = shop_info_df ,x=\"min_price\", bins=100, ax=ax10).set(xlabel=None)\nsns.histplot(data = shop_info_df ,x=\"max_price\", bins=100, ax=ax11).set(xlabel=None)\nsns.histplot(data = shop_info_df ,x=\"mean_price\",bins=100, ax=ax20).set(xlabel=None)\nsns.histplot(data = shop_info_df ,x=\"median_price\", bins=100, ax=ax21).set(xlabel=None)\nsns.histplot(data = shop_info_df ,x=\"num_products\", bins=50, ax=ax3).set(xlabel=None)\nfig.subplots_adjust(top=0.92)\nfig.suptitle('Numeric Feature Distribution', fontsize=\"28\");","9cdd39ab":"shop_info_df[shop_info_df[\"min_price\"] < 1].head(10)","522bf699":"\"\"\"\nItems that cost 0.1:\n\"\"\"\nten_cent_items = train_df[train_df[\"item_price\"]==0.1][\"item_id\"].unique()\nten_cent_items","641a80ea":"items_df[items_df[\"item_id\"].isin(ten_cent_items)][\"item_name\"].unique()","4e43bbbb":"fig = plt.figure(figsize=(25,15))\ngs = fig.add_gridspec(2, 2)\n\nax00 = fig.add_subplot(gs[0,0])\nax01 = fig.add_subplot(gs[0,1])\nax10 = fig.add_subplot(gs[1,0])\nax11 = fig.add_subplot(gs[1,1])\nax00.tick_params(axis='both', labelsize=15)\nax01.tick_params(axis='both', labelsize=15)\nax10.tick_params(axis='both', labelsize=15)\nax11.tick_params(axis='both', labelsize=15)\nax00.set_title('Item count per month (Small stores)', fontsize=20)\nax01.set_title('Item count per month (Medium stores)', fontsize=20)\nax10.set_title('Item count per month (Big stores)', fontsize=20)\nax11.set_title('Item count per month (Huge stores)', fontsize=20)\n\nfor c in m_shop_df[\"shop_id\"].unique():\n    tmp_df = m_shop_df[m_shop_df[\"shop_id\"]==c].copy()\n    max_mean_count = tmp_df[\"item_cnt_month\"].max()\n    #sns.lineplot(x=\"month\", y=\"mean_cnt\", data=tmp_df, palette=\"Set2\", ax=ax0x)\n    if max_mean_count < 2000:\n        sns.lineplot(x=\"date\", y=\"item_cnt_month\", data=tmp_df, palette=\"Set2\", ax=ax00)\n    elif 2000 <= max_mean_count < 5000 :\n        sns.lineplot(x=\"date\", y=\"item_cnt_month\", data=tmp_df, palette=\"Set2\", ax=ax01)\n    elif 5000 <= max_mean_count < 10000 :\n        sns.lineplot(x=\"date\", y=\"item_cnt_month\", data=tmp_df, palette=\"Set2\", ax=ax10)\n    else: \n        # > 5000\n        sns.lineplot(x=\"date\", y=\"item_cnt_month\", data=tmp_df, palette=\"Set2\", ax=ax11)\n            \nplt.legend([],[], frameon=False)\nfig.subplots_adjust(top=0.9)\nfig.suptitle('Total Sales Development per Shop', fontsize=\"28\");","39c1688c":"def preprocessing(data, item_data=items_df, shop_data=shops_df, category_data=item_categories_df):\n    \"\"\"\n    Some basic stuff.\n    \"\"\" \n    print(50*'-')\n    print(\"preprocessing...\")\n    # 1). Create a copy of the Dataframe.\n    df = data.copy()\n    # 2). Remove all values with item_cnt_day < 1.\n    df = df[df[\"item_cnt_day\"]>0]\n    #3). Add Month feature\n    df[\"date\"]= pd.to_datetime(df[\"date\"], format='%d.%m.%Y')\n    df[\"month\"] = df[\"date\"].dt.month\n    #4). Group by date_block_num.\n    df = df[[\"month\", \"date_block_num\", \"shop_id\", \"item_id\", \"item_price\", \"item_cnt_day\"]].groupby(\n        [\"date_block_num\", \"shop_id\", \"item_id\"]).agg(\n        {\"item_price\": \"mean\",\"item_cnt_day\": \"sum\", \"month\": \"min\"}).reset_index()\n    df.rename(columns={\"item_cnt_day\": \"item_cnt_month\"}, inplace=True)\n    #5). Add category_id and item_name.\n    df = pd.merge(df, item_data, on=\"item_id\", how=\"inner\")\n    #6). Add shop_name \n    df = pd.merge(df, shop_data, on=\"shop_id\", how=\"inner\")\n    #7). Add category_name\n    df = pd.merge(df, category_data, on=\"item_category_id\", how=\"inner\")\n    print(\"done.\")\n    print(50*'-')\n    return df","dbc0c2ff":"\"\"\"\nTesting preprocessing:\n\"\"\"\npiped_df = preprocessing(data=train_df)\npiped_df.head()","7757bcb7":"def add_city_feature(data):\n    data.loc[data[\"shop_name\"] == '\u0421\u0435\u0440\u0433\u0438\u0435\u0432 \u041f\u043e\u0441\u0430\u0434 \u0422\u0426 \"7\u042f\"', \"shop_name\"] = '\u0421\u0435\u0440\u0433\u0438\u0435\u0432\u041f\u043e\u0441\u0430\u0434 \u0422\u0426 \"7\u042f\"'\n    data[\"city\"] = data[\"shop_name\"].str.split(\" \").map(lambda x: x[0])\n    data.loc[data[\"city\"] == \"!\u042f\u043a\u0443\u0442\u0441\u043a\", \"city\"] = \"\u042f\u043a\u0443\u0442\u0441\u043a\"\n    data[\"city_code\"] = data[\"city\"].factorize()[0]\n    return data","6b219738":"\"\"\"\nTesting add_city_feature:\n\"\"\"\npiped_df=add_city_feature(piped_df)\npiped_df.head()","6226d80c":"\"\"\"\nWe will use or shop_if df as origin.\n\"\"\"\nshop_info_df.head(2)","87104ba1":"# add city:\nshop_info_df=add_city_feature(shop_info_df)","d64f0974":"shop_info_df.head(2)","f5e493cc":"\"\"\"\nConverting data to int by using the the days protperty of timedelta. (Seems a bit laborious. Is there a cleaner method?)\n\"\"\"\nfist_date = datetime.strptime(\"2013-01-01\", \"%Y-%m-%d\").date()\n\nshop_info_df['fist_business_m']= pd.to_datetime(shop_info_df['fist_business_m'])\nshop_info_df['last_business_m']= pd.to_datetime(shop_info_df['last_business_m'])\n\nshop_info_df[\"fist_business_m\"] = shop_info_df[\"fist_business_m\"].map(lambda x: (x.date() - fist_date).days)\nshop_info_df[\"last_business_m\"] = shop_info_df[\"last_business_m\"].map(lambda x: (x.date() - fist_date).days)\nshop_info_df.head(3)","b73d10c3":"def get_cluster_feature(data, columns, index_name, cluster_name, num_cluster=5, linkage=\"average\", n_pca_components=3, svd_solver=\"auto\", visualize=True):\n    \"\"\"\n    If you have any questions, write me in the comment section.  \n    \"\"\"\n    df = data[columns].copy()\n    \n    # PCA:\n    pca = PCA(n_components=n_pca_components)\n    components = pca.fit_transform(df)\n    components = pd.DataFrame(components)\n    \n    # Clustering:\n    clusterer = AgglomerativeClustering(n_clusters=num_cluster, linkage=linkage)\n    labels = clusterer.fit_predict(components)\n    x = components[0]\n    y = components[1]\n    \n    # Evaluation:\n    scorelist = []\n    nrange = range(2, 6)\n    for n in nrange:\n        clusterer = AgglomerativeClustering(n_clusters=n)\n        l = clusterer.fit_predict(components)\n        silscore = silhouette_score(df, l)\n        scorelist.append(silscore)\n        \n    for i in df.index:\n        df.at[i, cluster_name] = labels[i]\n        df = df[cluster_name].reset_index()\n        df.rename(columns={\"index\": index_name}, inplace = True)\n    \n    # plotting:    \n    if visualize:\n        fig = plt.figure(figsize=(25,15))\n        gs = fig.add_gridspec(2, 2)\n        ax00 = fig.add_subplot(gs[0,0])\n        ax01 = fig.add_subplot(gs[0,1])\n        ax02 = fig.add_subplot(gs[1,:])\n        ax00.tick_params(axis='both', labelsize=15)\n        ax01.tick_params(axis='both', labelsize=15)\n        ax02.tick_params(axis='both', labelsize=15)\n        ax00.set_title('PCA Components', fontsize=20)\n        ax01.set_title('Cluster Score by Number of Clusters', fontsize=20)\n        ax02.set_title('Clustering', fontsize=20)\n\n        ax00.set(xlabel='component number', ylabel='covered variance')\n        ax01.set(xlabel='number of clusters', ylabel='silhouette score')\n        ax02.set(xlabel='component 1 score', ylabel='component 2 score')\n\n        sns.barplot(x=list(range(pca.n_components_)), y=pca.explained_variance_ratio_, ax=ax00, palette=\"Set2\")\n        sns.lineplot(x=nrange, y=scorelist, ax=ax01, color=\"darkblue\")\n        sns.scatterplot(x=x, y=y, hue=labels, ax=ax02, palette=\"dark\")\n\n        fig.subplots_adjust(top=0.9)\n        fig.suptitle(f\"PCA and Clustering Output (num_cluster = {num_cluster})\", fontsize=\"28\")\n    \n    return df","e204347a":"shop_info_df.info()","e2de12a2":"\"\"\"\nWe can use the select_dtypes function of pandas.DataFrame to easily find all numeric columns in this case.\n\"\"\"\nnumeric_shop_columns = shop_info_df.select_dtypes(include=[\"int32\", \"int64\", \"float32\"]).columns\nshop_cluster_df = get_cluster_feature(data=shop_info_df, columns=numeric_shop_columns, n_pca_components=2, index_name=\"shop_id\", cluster_name=\"shop_cluster\");","dc58fd6e":"\"\"\"\nAdd shop cluster to piped_df and check th outcome\n\"\"\"\npiped_df = pd.merge(piped_df, shop_cluster_df, on=\"shop_id\", how=\"inner\")\npiped_df.head() ","752798be":"item_category_info_df.head(3)","b3456dc5":"\"\"\"\nConverting data columns to int by using the the days protperty of timedelta.\n\"\"\"\nitem_category_info_df['first_sold']= pd.to_datetime(item_category_info_df['first_sold'])\nitem_category_info_df['last_sold']= pd.to_datetime(item_category_info_df['last_sold'])\n\nitem_category_info_df[\"first_sold\"] = item_category_info_df[\"first_sold\"].map(lambda x: (x.date() - fist_date).days)\nitem_category_info_df[\"last_sold\"] = item_category_info_df[\"last_sold\"].map(lambda x: (x.date() - fist_date).days)\nitem_category_info_df.head(3)","17fb7039":"\"\"\"\nGet numeric columns and fill NANs\n\"\"\"\nitem_category_info_df = item_category_info_df.astype({\"name\": \"string\"})\nnumeric_item_columns = item_category_info_df.select_dtypes(include=[\"float64\", \"int64\", \"object\"]).columns\nitem_category_info_df.fillna(0, inplace=True);","2b841a2d":"item_cat_claster = get_cluster_feature(data=item_category_info_df, columns=numeric_item_columns, index_name=\"item_category_id\", cluster_name=\"item_cat_cluster\")","45191766":"\"\"\"\nAdd item categorie cluster to piped_df and check the outcome\n\"\"\"\npiped_df = pd.merge(piped_df, item_cat_claster, on=\"item_category_id\", how=\"inner\")\npiped_df.head() ","610d1b74":"def add_lag_feature_and_label(args):\n    \n    def add_lags(df, date_block):\n        for lag in range(num_lags):\n            if (date_block-lag-1) in item_df[\"date_block_num\"].values:\n                lag_value = item_df[item_df[\"date_block_num\"]==date_block-lag-1][\"item_cnt_month\"].values[0]\n                df.at[index, f\"lag_{lag+1}\"] = lag_value\n        return df \n    df = args[0].copy()\n    num_lags = args[1]\n    target_date_block = args[2]\n    for lag in range(num_lags):\n        df[f\"lag_{lag+1}\"] = 0\n    for shop in df[\"shop_id\"].unique():\n        shop_df = df[df[\"shop_id\"]==shop].copy()\n        for item in shop_df[\"item_id\"].unique():\n            item_df = shop_df[shop_df[\"item_id\"]==item].copy()\n            last_index = 0\n            for index, row in item_df.iterrows():\n                date_block = row[\"date_block_num\"]\n                if target_date_block and date_block == target_date_block:\n                    df = add_lags(df, date_block)\n                if target_date_block is None:\n                    df = add_lags(df, date_block)\n    if target_date_block:\n        df = df[df[\"date_block_num\"]==target_date_block].copy()\n    df.rename(columns={\"item_cnt_month\":\"label\"}, inplace=True)\n    return df\n\ndef parallelize_lag_and_target_processing(df, num_lags, target_date_block_num=None, func=add_lag_feature_and_label, n_cores=4, shops=None, items=None):\n    if target_date_block_num:\n        # get list of valid date_block_num values:\n        valid_date_blocks = range(target_date_block_num - num_lags, target_date_block_num + 1)\n        df = df[df[\"date_block_num\"].isin(valid_date_blocks)].copy()\n    if shops:\n        df = df[df[\"shop_id\"].isin(shops)].copy()\n    if items:\n        df = df[df[\"item_id\"].isin(items).copy()]\n    df.sort_values(by=\"shop_id\", inplace=True)\n    df_split = np.array_split(df, n_cores)\n    param_list = [[df_, num_lags, target_date_block_num] for df_ in df_split]\n    pool = Pool(n_cores)\n    df = pd.concat(pool.map(func, param_list))\n    pool.close()\n    pool.join()\n    return df","36d09343":"\"\"\"\nTesting add_lag_feature_and_label and parallelize_lag_and_target_processing:\n\"\"\"\nlag_test_df = parallelize_lag_and_target_processing(df=piped_df, shops=[10], num_lags=12, target_date_block_num=24)\n# We only need a few columns to validate the result\ncolumns = [col for col in lag_test_df.columns if col[:3]==\"lag\"]\ncolumns = [\"item_id\", \"date_block_num\", \"label\"] + columns\nlag_test_df[columns].head(3)","53e1071d":"def pipeline(data, num_lags, target_date_block_num=None, first_data_str=\"2013-01-01\",\n             shop_info_data=shop_info_df, categorie_info_data=item_category_info_df,\n             shops=None, items=None):\n    \"\"\"\n    This function combines all previous steps into a single pipeline function without the preprocessing function.\n    \"\"\"\n    print(100*\"#\")\n    print(f\"running pipeline for target_date_block_num {target_date_block_num}  with {num_lags} lags...\")\n    fist_date = datetime.strptime(first_data_str, \"%Y-%m-%d\").date()\n    print(\"adding city feature...\")\n    df = add_city_feature(data)\n    print(\"done.\")\n    print(\"adding shop_cluster feature...\")\n    # preprocessing shop_info_df:\n    shop_info_data['fist_business_m']= pd.to_datetime(shop_info_data['fist_business_m'])\n    shop_info_data['last_business_m']= pd.to_datetime(shop_info_data['last_business_m'])\n    shop_info_data[\"fist_business_m\"] = shop_info_data[\"fist_business_m\"].map(lambda x: (x.date() - fist_date).days)\n    shop_info_data[\"last_business_m\"] = shop_info_data[\"last_business_m\"].map(lambda x: (x.date() - fist_date).days)\n    numeric_shop_columns = shop_info_data.select_dtypes(include=[\"int32\", \"int64\", \"float32\", \"float64\"]).columns\n    shop_cluster_df = get_cluster_feature(data=shop_info_data, columns=numeric_shop_columns, n_pca_components=2,\n                                          index_name=\"shop_id\", cluster_name=\"shop_cluster\", visualize=False)\n    # adding shop_cluster to df:\n    df = pd.merge(df, shop_cluster_df, on=\"shop_id\", how=\"inner\")\n    print(\"done.\")\n    print(\"adding item_cat_cluster feature...\")\n    # preprocessing categorie_info_data:\n    categorie_info_data['first_sold']= pd.to_datetime(categorie_info_data['first_sold'])\n    categorie_info_data['last_sold']= pd.to_datetime(categorie_info_data['last_sold'])\n    categorie_info_data[\"first_sold\"] = categorie_info_data[\"first_sold\"].map(lambda x: (x.date() - fist_date).days)\n    categorie_info_data[\"last_sold\"] = categorie_info_data[\"last_sold\"].map(lambda x: (x.date() - fist_date).days)\n    categorie_info_data = categorie_info_data.astype({\"name\": \"string\"})\n    numeric_item_columns = categorie_info_data.select_dtypes(include=[\"int32\", \"int64\", \"float32\", \"float64\", \"object\"]).columns\n    categorie_info_data.fillna(0, inplace=True)\n    item_cluster_df = get_cluster_feature(data=categorie_info_data, columns=numeric_item_columns, n_pca_components=3,\n                                          index_name=\"item_category_id\", cluster_name=\"item_cat_cluster\", visualize=False)\n    # adding item_cat_cluster to df:\n    df = pd.merge(df, item_cluster_df, on=\"item_category_id\", how=\"inner\")\n    print(\"done.\")\n    print(\"adding lag features...\")\n    df = parallelize_lag_and_target_processing(df, shops=shops, items=items, num_lags=num_lags, target_date_block_num=target_date_block_num)\n    print(\"done.\")\n    \n    print(\"dropping item_name, shop_name, item_category_name and city...\")\n    df.drop([\"item_name\", \"shop_name\", \"item_category_name\", \"city\"], axis=1, inplace=True)\n    print(\"done.\")\n    print(100*\"#\")\n    return df","673197bc":"pipeline_test_df = preprocessing(data=train_df)\npipeline_test_df = pipeline(data=pipeline_test_df, num_lags=12, target_date_block_num = 12, shops=[10])","7dfc6cb7":"\"\"\"\nLet's see what we have here.\n\"\"\"\npipeline_test_df","55596892":"\"\"\"\nLet's start by defining a function for the required transformations for our test_df.\n\"\"\"\ndef test_df_pipeline(num_lags, target_data_plock_number, test_data=test_df, train_data=train_df,\n                     item_data=items_df, shop_data=shops_df, item_category_data=item_categories_df):\n    # add some important features to our test_df. (we preprcess it for concating)\n    df = test_data.copy()\n    df[\"date_block_num\"]=34\n    df[\"month\"] = 11\n    df[\"item_cnt_month\"] = 0\n    df = pd.merge(df, item_data, on=\"item_id\", how=\"inner\")\n    df = pd.merge(df, shop_data, on=\"shop_id\", how=\"inner\")\n    df = pd.merge(df, item_category_data, on=\"item_category_id\", how=\"inner\")\n    \n    df_train = preprocessing(data=train_data)\n    \n    # concat df and df_train\n    index_cols = ['shop_id', 'item_id', 'date_block_num']\n    df = pd.concat([df_train, df], axis=0, ignore_index=True, keys=index_cols)\n    df.fillna(0, inplace=True)\n    \n    # add last known price (we do not know the price for November 2015, so we take the last known price)\n    df.sort_values(by=[\"shop_id\", \"item_id\"], inplace=True)\n    df[\"item_price\"] = df[\"item_price\"].replace(to_replace=0, method='ffill')\n    \n    # run pipeline with num lags and target month\n    X = pipeline(data=df,  num_lags=num_lags, target_date_block_num=target_data_plock_number)\n    \n    # drop the label:\n    X.drop(\"label\", axis=1, inplace=True)\n    \n    # clapping all lag features:\n    columns = [col for col in X.columns if col[:3]==\"lag\"]\n    for col in columns:\n        X[col] = np.where(X[col]>20, 20,X[col])\n        \n    # extraction of the ID for the final submission dataframe:\n    X.sort_values(by=\"ID\", inplace=True)\n    submission_ID = X[\"ID\"]\n    submission_ID=submission_ID.astype(\"int32\")\n    X.drop([\"ID\"], axis=1, inplace=True)\n    return X, submission_ID\n\n\n\"\"\"\nAt this point, we just want to test different model architectures with different numbers of lags and target_date_block_nums. \nSo let's define a function that takes care of some final preparation steps so that we only need one line of code to get what we want.\n\"\"\"\ndef get_training_data(num_lags, target_data_block_number=None, train_data=train_df):\n    df = pipeline(data= preprocessing(data=train_data), num_lags=num_lags,\n                  target_date_block_num=target_data_block_number)\n    # clapping the label and all lag features:\n    columns = [col for col in df.columns if col[:3]==\"lag\"]\n    columns = [\"label\"] + columns\n    for col in columns:\n        df[col] = np.where(df[col]>20, 20,df[col])\n    # delete the the f\u00fcrst \"num_lags\" date_block_nums\n    valid_data_block_num = range(num_lags,34)\n    df = df[df[\"date_block_num\"].isin(valid_data_block_num)]\n    # get label\n    y = df[\"label\"]\n    X = df.copy()\n    X.drop([\"label\"], axis=1, inplace=True)\n    return X,y\n\n\"\"\"\nLet's also write a function which helps us to evaluate our xgBoost model:\n\"\"\"\ndef evaluate_xgboost(model):\n        results = model.evals_result()\n        fig = plt.figure(figsize=(25,10))\n        gs = fig.add_gridspec(1, 2)\n        ax00 = fig.add_subplot(gs[0,0])\n        ax01 = fig.add_subplot(gs[0,1])\n        ax00.tick_params(axis='both', labelsize=15)\n        ax01.tick_params(axis='both', labelsize=15)\n        ax00.set_title('Feature Importance', fontsize=20)\n        ax01.set_title('loss vs validation loss', fontsize=20)\n        ax00.set(xlabel='Importance', ylabel='Feature')\n        ax01.set(xlabel='n_estimators', ylabel='rmse')\n        sns.barplot(y=model.get_booster().feature_names, x=model.feature_importances_, ax=ax00, palette=\"Set2\", orient=\"h\")\n        sns.lineplot(x=range(model.n_estimators), y=results[\"validation_0\"][\"rmse\"], ax=ax01, color=\"darkblue\", label=\"loss\")\n        sns.lineplot(x=range(model.n_estimators), y=results[\"validation_1\"][\"rmse\"], ax=ax01, color=\"orange\", label=\"validation loss\")\n        fig.subplots_adjust(top=0.9)\n        fig.suptitle(\"Model Evaluation\", fontsize=\"28\")","7a59b950":"X, y = get_training_data(num_lags=3)","89e113e1":"# split:\nX_train, X_test, y_train, y_test = train_test_split(X, y)","d02b846b":"X_train.head(5)","8b33a83a":"X_test_november = X_test.copy()\nX_test_november[\"label\"] = y_test\nX_test_november = X_test_november[X_test_november[\"date_block_num\"]==22]\ny_test_november = X_test_november[\"label\"]\nX_test_november.drop([\"label\"], axis=1, inplace=True)","4fa3688f":"evalset = [(X_train, y_train), (X_test,y_test)]\nxgb_model = xgb.XGBRegressor(max_depth=10, n_estimators=3000, subsample=0.2, eval_metric='rmse', learning_rate=0.1)\nxgb_model.fit(X_train, y_train, eval_set=evalset)","b9e6be72":"\"\"\"\nEvaluation of firts attempt:\n\"\"\"\ntest_prediction_november = xgb_model.predict(X_test_november)\ntest_prediction = xgb_model.predict(X_test)\ntrain_prediction = xgb_model.predict(X_train)\nrmse_test_november = mean_squared_error(y_true = y_test_november, y_pred = test_prediction_november)**(0.5)\nrmse_test = mean_squared_error(y_true = y_test, y_pred = test_prediction)**(0.5)\nrmse_train = mean_squared_error(y_true = y_train, y_pred = train_prediction)**(0.5)\nprint(50*\"*\")\nprint(f\"RMSE test november: {rmse_test_november}\")\nprint(f\"RMSE test: {rmse_test}\")\nprint(f\"RMSE train: {rmse_train}\")\nprint(50*\"*\")\nevaluate_xgboost(xgb_model)","efeaabea":"submission_df, submission_id = test_df_pipeline(3, 34)","3eb05aea":"submission_df.head()","f5676ac2":"\"\"\"\nPredicting and submission:\n\"\"\"\nsubmission_prediction = xgb_model.predict(submission_df)\nsubmission = pd.DataFrame({\n    \"ID\": submission_id, \n    \"item_cnt_month\": submission_prediction\n})\nsubmission.to_csv('xgb_submission.csv', index=False)\nprint(\"submission worked\")","94fb46a0":"\"\"\"\nLet's start by writing a function for our avg_lags per item feature.\n\"\"\"\ndef add_avg_lag_feature(args):\n    \n    def add_avg_lags(df, date_block):\n        for lag in range(num_lags):\n            if (date_block-lag-1) in item_df[\"date_block_num\"].values:\n                lag_df = item_df[item_df[\"date_block_num\"]==date_block-lag-1]\n                lag_value = lag_df[\"item_cnt_month\"].mean()\n                df.at[index, f\"avg_lag_{lag+1}\"] = lag_value\n        return df \n    \n    df = args[0].copy()\n    num_lags = args[1]\n    target_date_block = args[2]\n    for lag in range(num_lags):\n        df[f\"avg_lag_{lag+1}\"] = 0\n\n    for item in df[\"item_id\"].unique():\n        item_df = df[df[\"item_id\"]==item].copy()\n        last_index = 0\n        for index, row in item_df.iterrows():\n            date_block = row[\"date_block_num\"]\n            if target_date_block and date_block == target_date_block:\n                df = add_avg_lags(df, date_block)\n            if target_date_block is None:\n                df = add_avg_lags(df, date_block)\n    return df\n\ndef parallelize_avg_lag_processing(df, num_lags, target_date_block_num=None, func=add_avg_lag_feature, n_cores=4, shops=None, items=None):\n    if target_date_block_num:\n        # get list of valid date_block_num values:\n        valid_date_blocks = range(target_date_block_num - num_lags, target_date_block_num + 1)\n        df = df[df[\"date_block_num\"].isin(valid_date_blocks)].copy()\n    if shops:\n        df = df[df[\"shop_id\"].isin(shops)].copy()\n    if items:\n        df = df[df[\"item_id\"].isin(items).copy()]\n    df.sort_values(by=\"item_id\", inplace=True)\n    df_split = np.array_split(df, n_cores)\n    param_list = [[df_, num_lags, target_date_block_num] for df_ in df_split]\n    pool = Pool(n_cores)\n    df = pd.concat(pool.map(func, param_list))\n    pool.close()\n    pool.join()\n    return df","bf2e009f":"X_train.head()","254e3347":"\"\"\"\nNow we need to adjust the pipeline a bit.\n\"\"\"\ndef pipeline(data, num_lags, num_avg_lags, target_date_block_num=None, first_data_str=\"2013-01-01\",\n             shop_info_data=shop_info_df, categorie_info_data=item_category_info_df,\n             shops=None, items=None):\n    \"\"\"\n    This function combines all previous steps into a single pipeline function without the preprocessing function.\n    \"\"\"\n    print(100*\"#\")\n    print(f\"running pipeline for target_date_block_num {target_date_block_num}  with {num_lags} lags...\")\n    fist_date = datetime.strptime(first_data_str, \"%Y-%m-%d\").date()\n    print(\"adding city feature...\")\n    df = add_city_feature(data)\n    print(\"done.\")\n    print(\"adding shop_cluster feature...\")\n    # preprocessing shop_info_df:\n    shop_info_data['fist_business_m']= pd.to_datetime(shop_info_data['fist_business_m'])\n    shop_info_data['last_business_m']= pd.to_datetime(shop_info_data['last_business_m'])\n    shop_info_data[\"fist_business_m\"] = shop_info_data[\"fist_business_m\"].map(lambda x: (x.date() - fist_date).days)\n    shop_info_data[\"last_business_m\"] = shop_info_data[\"last_business_m\"].map(lambda x: (x.date() - fist_date).days)\n    numeric_shop_columns = shop_info_data.select_dtypes(include=[\"int32\", \"int64\", \"float32\", \"float64\"]).columns\n    shop_cluster_df = get_cluster_feature(data=shop_info_data, columns=numeric_shop_columns, n_pca_components=2,\n                                          index_name=\"shop_id\", cluster_name=\"shop_cluster\", visualize=False)\n    # adding shop_cluster to df:\n    df = pd.merge(df, shop_cluster_df, on=\"shop_id\", how=\"inner\")\n    print(\"done.\")\n    print(\"adding item_cat_cluster feature...\")\n    # preprocessing categorie_info_data:\n    categorie_info_data['first_sold']= pd.to_datetime(categorie_info_data['first_sold'])\n    categorie_info_data['last_sold']= pd.to_datetime(categorie_info_data['last_sold'])\n    categorie_info_data[\"first_sold\"] = categorie_info_data[\"first_sold\"].map(lambda x: (x.date() - fist_date).days)\n    categorie_info_data[\"last_sold\"] = categorie_info_data[\"last_sold\"].map(lambda x: (x.date() - fist_date).days)\n    categorie_info_data = categorie_info_data.astype({\"name\": \"string\"})\n    numeric_item_columns = categorie_info_data.select_dtypes(include=[\"int32\", \"int64\", \"float32\", \"float64\", \"object\"]).columns\n    categorie_info_data.fillna(0, inplace=True)\n    item_cluster_df = get_cluster_feature(data=categorie_info_data, columns=numeric_item_columns, n_pca_components=3,\n                                          index_name=\"item_category_id\", cluster_name=\"item_cat_cluster\", visualize=False)\n    # adding item_cat_cluster to df:\n    df = pd.merge(df, item_cluster_df, on=\"item_category_id\", how=\"inner\")\n    print(\"done.\")\n    print(\"adding avg lag features...\")\n    df = parallelize_avg_lag_processing(df=df, shops=shops, items=items, num_lags=num_lags, target_date_block_num=target_date_block_num)\n    #df = add_avg_lag_feature(df, num_avg_lags, target_date_block_num)\n    print(\"done.\")\n    print(\"adding lag features...\")\n    df = parallelize_lag_and_target_processing(df, shops=shops, items=items, num_lags=num_lags, target_date_block_num=target_date_block_num)\n    print(\"done.\")\n    \n    print(\"dropping item_name, shop_name, item_category_name and city...\")\n    df.drop([\"item_name\", \"shop_name\", \"item_category_name\", \"city\"], axis=1, inplace=True)\n    print(\"done.\")\n    print(100*\"#\")\n    return df\n\ndef get_training_data(num_lags, num_avg_lags,  target_data_block_number=None, train_data=train_df):\n    df = pipeline(data=preprocessing(data=train_data), num_lags=num_lags, num_avg_lags=num_avg_lags,\n                  target_date_block_num=target_data_block_number)\n    # clapping the label and all lag features:\n    columns = [col for col in df.columns if col[:3]==\"lag\"]\n    columns = [\"label\"] + columns\n    for col in columns:\n        df[col] = np.where(df[col]>20, 20,df[col])\n    # delete the the f\u00fcrst \"num_lags\" date_block_nums\n    valid_data_block_num = range(num_lags,34)\n    df = df[df[\"date_block_num\"].isin(valid_data_block_num)]\n    # get label\n    y = df[\"label\"]\n    X = df.copy()\n    X.drop([\"label\"], axis=1, inplace=True)\n    return X,y","f64d0f12":"\"\"\"\nLet's take a look at the impact of our newly created features.\n\"\"\"\nX, y = get_training_data(num_lags=3, num_avg_lags=3)\n# split:\nX_train, X_test, y_train, y_test = train_test_split(X, y)\n\nX_test_november = X_test.copy()\nX_test_november[\"label\"] = y_test\nX_test_november = X_test_november[X_test_november[\"date_block_num\"]==22]\ny_test_november = X_test_november[\"label\"]\nX_test_november.drop([\"label\"], axis=1, inplace=True)","9df0b2bf":"evalset = [(X_train, y_train), (X_test,y_test)]\nxgb_model2 = xgb.XGBRegressor(max_depth=5, n_estimators=1000, subsample=0.6, eval_metric='rmse', learning_rate=0.1)\nxgb_model2.fit(X_train, y_train, eval_set=evalset)","75ddbdfb":"test_prediction_november = xgb_model2.predict(X_test_november)\ntest_prediction = xgb_model2.predict(X_test)\ntrain_prediction = xgb_model2.predict(X_train)\nrmse_test_november = mean_squared_error(y_true = y_test_november, y_pred = test_prediction_november)**(0.5)\nrmse_test = mean_squared_error(y_true = y_test, y_pred = test_prediction)**(0.5)\nrmse_train = mean_squared_error(y_true = y_train, y_pred = train_prediction)**(0.5)\nprint(50*\"*\")\nprint(f\"RMSE test november: {rmse_test_november}\")\nprint(f\"RMSE test: {rmse_test}\")\nprint(f\"RMSE train: {rmse_train}\")\nprint(50*\"*\")\nevaluate_xgboost(xgb_model2)","c50b36bb":"<div style=\"color: #505541;\n           display:fill;\n           border-radius:0px;\n           text-indent: 10px;\n           border-style: solid;\n           border-color:#424949;\n           text-align:center;\n           background-color:#F8F9F9 ;\n           font-size:15px;\n           letter-spacing:0.5px;\n           padding: 0.2em;\n           text-align:left\">  \n    \nAs we can see, only one principal component covers almost all of the variance. This shows us that most of the features generated are correlated for our shop dataset and that we do not lose much information if we reduce the dimension down to R^2. Here we will use five groups to classify the different shops.\n<\/div>","c58482f8":"<div style=\"color: #505541;\n           display:fill;\n           border-radius:0px;\n           border-style: solid;\n           border-color:#424949;\n           text-align:center;\n           background-color:#F8F9F9 ;\n           font-size:15px;\n           letter-spacing:0.5px;\n           padding: 0.2em;\n           text-align:left\">  \n\ud83d\udcdd <b> Note: <\/b> We can see that most item categories have a wide range of items (price-wise), but the mean and median_price distribution show us that the overall price structure is different. \nWhat I would like to see is the sales trend (in terms of mean numbers per month) for each item category (as this is our final target for the forecast). But the way we have built our data framework is not really well suited to displaying this.\nSo let's write some code to solve this homemade issue. We can also reuse this later when we look at sales trends per shop.\n<\/div>","2d706b54":"<div style=\"color: #505541;\n           display:fill;\n           border-radius:0px;\n           text-indent: 10px;\n           border-style: solid;\n           border-color:#424949;\n           text-align:center;\n           background-color:#F8F9F9 ;\n           font-size:15px;\n           letter-spacing:0.5px;\n           padding: 0.2em;\n           text-align:left\">  \n<b> To be continued... (Please let me know what you think in the comment section). Feel free to copy this notebook and try some of the methods mentioned yourself. I hope this notebook has been helpful to some readers. <\/b>","8dd37df3":"<a id=\"2.2\"><\/a>\n<a id=\"2.2.1\"><\/a>\n<div style=\"color: #505541;\n           display:fill;\n           border-radius:0px;\n           text-indent: 10px;\n           border-style: solid;\n           border-color:#424949;\n           text-align:center;\n           background-color:#F8F9F9 ;\n           font-size:15px;\n           letter-spacing:0.5px;\n           padding: 0.2em;\n           text-align:left\">  \n<center> <h2> \ud83d\udd27 2.2 Feature Engeneering \ud83d\udee0<\/h2> <\/center>\n<hr>\n<center> <h3> 2.2.1 City Feature \ud83c\udfd9<\/h3> <\/center>","4bce77d1":"<div style=\"color: #505541;\n           display:fill;\n           border-radius:0px;\n           border-style: solid;\n           border-color:#424949;\n           text-align:center;\n           background-color:#F8F9F9 ;\n           font-size:15px;\n           letter-spacing:0.5px;\n           padding: 0.2em;\n           text-align:left\">  \n\ud83d\udcdd <b> Note: <\/b> \nWhat we can see is that there is an overall decline in the number of items sold per month, even though the sales figures for 2013 and 214 are quite similar. This means that people tend to buy fewer but more expensive items. Also, we can now see how weak the sales figures for 2015 look compared to the previous two years.\n    \n<hr>\n    \nPreviously we took a closer look at the individual item categories, let's wrap this section up by taking a closer look at the individual shops. I don't think we will see anything surprising in the sales figures per shop, but it is always advisable to look at the whole dataset before moving on to preprocessing and modelling. \n<\/div>","23dd0204":"<div style=\"color: #505541;\n           display:fill;\n           border-radius:0px;\n           border-style: solid;\n           border-color:#424949;\n           text-align:center;\n           background-color:#F8F9F9 ;\n           font-size:15px;\n           letter-spacing:0.5px;\n           padding: 0.2em;\n           text-align:left\">  \n\ud83d\udcdd <b> Note: <\/b>  Similar to the item categories, most shops exist for the entire period, with some exceptions, as we have already seen. Let's now take a closer look at the other shop-specific features.\n<\/div>","898071ed":"<div style=\"color: #505541;\n           display:fill;\n           border-radius:0px;\n           border-style: solid;\n           border-color:#424949;\n           text-align:center;\n           background-color:#F8F9F9 ;\n           font-size:15px;\n           letter-spacing:0.5px;\n           padding: 0.2em;\n           text-align:left\"> \nThis looks as expected. Let's plot all the item categories by size and see if we can find some category-specific seasonalities.","7c712063":"<a id=\"1.4\"><\/a>\n<div style=\"color: #505541;\n           display:fill;\n           border-radius:0px;\n           text-indent: 10px;\n           border-style: solid;\n           border-color:#424949;\n           text-align:center;\n           background-color:#F8F9F9 ;\n           font-size:15px;\n           letter-spacing:0.5px;\n           padding: 0.2em;\n           text-align:left\">  \n<center> <h2> \ud83c\udfe0\ud83c\udfe6\ud83d\udc92 1.4 Shop Analysis \ud83c\udfe9\ud83c\udfea\ud83c\udfec<\/h2> <\/center>\n<\/div>","05826dca":"<a id=\"3.\"><\/a>\n<div style=\"color: #505541;\n           display:fill;\n           border-radius:0px;\n           text-indent: 10px;\n           border-style: solid;\n           border-color:#424949;\n           text-align:center;\n           background-color:#F8F9F9 ;\n           font-size:15px;\n           letter-spacing:0.5px;\n           padding: 0.2em;\n           text-align:left\">  \n<center> <h1> 3. Modeling \ud83d\udd0e\ud83d\uddc2<\/h1> <\/center>\n<hr>\nThere are some things we should keep in mind:\n<ul>\n    <li> First, let's think about the final goal of predicting sales figures for a group of shops and items for November 2015.<\/li>    \n    <li> Here we face the challenge that we do not know sales figures for this month to train a model and evaluate our result.<\/li>  \n    <li> However, we can train models from previous years and hope they do well for 2015. <\/li>  \n    <li> \"Submissions are evaluated by root mean squared error (RMSE). True target values are clipped into [0,20] range.\" !! This also means that we could try to train a classification model.<\/li>\n    <li> Most labels are 0, and a model could learn to always predict 0 as an outcome.<\/li>\n    <li> We have 362 items that are not included in our train_df and therefore have never been sold before.<\/li> \n    <li> The number of lags is an additional parameter that needs to be optimised.<\/li>\n<\/ul>\n<hr>\nWe start this chapter with the definition of some last helper function, so that we can fully concentrate on modeling in the coming sections. I have two main ideas here. First, I want to treat this task as a regression problem. Then, I will try to define it as a classification problem. Since we classify all values in [0,20], we have 20 classes to predict.\n<\/div>","560e7cc7":"<div style=\"color: #505541;\n           display:fill;\n           border-radius:0px;\n           text-indent: 10px;\n           border-style: solid;\n           border-color:#424949;\n           text-align:center;\n           background-color:#F8F9F9 ;\n           font-size:15px;\n           letter-spacing:0.5px;\n           padding: 0.2em;\n           text-align:left\">  \n    \nI dont know if the translation failed me but it seems to be some sort of toy snakes. Please let me know in the comment section if you have a better translation.\nNow back to topic.<br> ( \u2022_\u2022) -- \u2310\u25a0-\u25a0......(\u2310\u25a0_\u25a0) \n\n<hr>\n    \nLet's conclude this chapter with a look at the total sales development per shop.\n<\/div>","cb101203":"<a id=\"1.2\"><\/a>\n<div style=\"color: #505541;\n           display:fill;\n           border-radius:0px;\n           border-style: solid;\n           border-color:#424949;\n           text-align:center;\n           background-color:#F8F9F9 ;\n           font-size:15px;\n           letter-spacing:0.5px;\n           padding: 0.2em;\n           text-align:left\">  \n    \n\ud83e\udd14\u2754 <b> Question: <\/b> Are there items that are in more than one category?\n<\/div>","72d28b39":"<div style=\"color: #505541;\n           display:fill;\n           border-radius:0px;\n           border-style: solid;\n           border-color:#424949;\n           text-align:center;\n           background-color:#F8F9F9 ;\n           font-size:20px;\n           letter-spacing:0.5px;\n           padding: 0.2em;\n           text-align:left\">  \n<center>\n<h1> \ud83d\udcc8\ud83d\udcc9 Sales Prediction \ud83d\udcca\ud83d\udccf<\/h1>\n<\/center>\n<\/div>","b0ca8d25":"<div style=\"color: #505541;\n           display:fill;\n           border-radius:0px;\n           text-indent: 10px;\n           border-style: solid;\n           border-color:#424949;\n           text-align:center;\n           background-color:#F8F9F9 ;\n           font-size:15px;\n           letter-spacing:0.5px;\n           padding: 0.2em;\n           text-align:left\">  \n<h3>\ud83d\udcdd What have we got so far? \ud83d\udcdd<\/h3>\n<hr>\n<ul>\n<li>We have 60 shops in our training dataset and 42 shops in our test dataset. <\/li>\n<li> We have 21807 distinct items in our training dataset and 5100 distinct items in our test dataset. <\/li>\n<li> All the shops in our test dataset are also in our training dataset. <\/li>\n<li> 362 items in our test dataset are not included in our train_df<\/li>.\n    <ul>\n    <li>This also means that we do not have the price for these items. <\/li>\n    <\/ul>\n<\/ul>\n<h3>What is our target? <\/h3>\n<hr>\n<ul>\n<li>We want to predict the number of items sold for November 2015. <\/li> \n    <ul>\n    <li> More specifically, we want to predict the number of items sold for each of the 5100 items in our test_df and for each shop that sells these items.\ud83e\udd2f\n    <li> So we have 214200 different rows (pairs of shop_id and item_id represented by the ID) that need to be predicted. \n    <li> Furthermore, 363 elements are not represented in our train_df. Here we may have to use the category to make some predictions.\n    <\/ul>\n<\/ul>\nLet's go ahead and take a closer look at the items. There was a hint in the project description that some items are returend. So maybe they have item_cnt_day < 0?\n\n<\/div>","9b6b457b":"## Table of Content\n* [1. EDA](#1.)\n    - [1.1 First Look](#1.1)\n    - [1.2 Item Categories](#1.2)\n    - [1.3 Overall Sales per Month](#1.3)\n    - [1.4 Shop Analysis](#1.4)\n* [2. Data Preparation](#2.)\n    - [2.1 Data Cleaning, Merging and Grouping by Month2.1 Data Cleaning, Merging and Grouping by Month](#2.1)\n    - [2.2 Feature Engeneering](#2.2)\n        * [2.2.1 Add City Feature](#2.2.1)\n        * [2.2.2 Shop Clustering with PCA](#2.2.2)\n        * [2.2.3 Item Category Clustering with PCA](#2.2.3)\n        * [2.2.4 Adding Lag Features and Label](#2.2.4)\n        * [2.2.5 Pipeline Function](#2.2.5)\n* [3. Modeling](#3.)\n    - [3.1 Submission](#3.1)\n    - [3.2 Next Steps](#3.3)","cd2ed191":"<div style=\"color: #505541;\n           display:fill;\n           border-radius:0px;\n           border-style: solid;\n           border-color:#424949;\n           text-align:center;\n           background-color:#F8F9F9 ;\n           font-size:15px;\n           letter-spacing:0.5px;\n           padding: 0.2em;\n           text-align:left\">  \n    \n\ud83d\udcdd <b> Note: <\/b> Each line contains information about a product group and we could also use this dataframe to find some groups of categories. \nLet's dig into it a bit to get a better understanding of how these item categories differ from each other.\nLet's start with the characteristics first_sold and last_sold. \n<hr>\n\ud83e\udd14\u2754 <b> Question: <\/b> Are there some item categories that disappeared between 2013 and 2015? \n<\/div>","424b7757":"<a id=\"2.2.5\"><\/a>\n<div style=\"color: #505541;\n           display:fill;\n           border-radius:0px;\n           text-indent: 10px;\n           border-style: solid;\n           border-color:#424949;\n           text-align:center;\n           background-color:#F8F9F9 ;\n           font-size:15px;\n           letter-spacing:0.5px;\n           padding: 0.2em;\n           text-align:left\">  \n<center> <h3> 2.2.5 Pipeline Function \ud83d\uddc3\ud83d\uddc2<\/h3> <\/center>\n   ","503089c4":"<div style=\"color: #505541;\n           display:fill;\n           border-radius:0px;\n           text-indent: 10px;\n           border-style: solid;\n           border-color:#424949;\n           text-align:center;\n           background-color:#F8F9F9 ;\n           font-size:15px;\n           letter-spacing:0.5px;\n           padding: 0.2em;\n           text-align:left\">  \nLooks good. Let's plot the total sales numbers per month.\n<\/div>","e43d7f9b":"<div style=\"color: #505541;\n           display:fill;\n           border-radius:0px;\n           border-style: solid;\n           border-color:#424949;\n           text-align:center;\n           background-color:#F8F9F9 ;\n           font-size:15px;\n           letter-spacing:0.5px;\n           padding: 0.2em;\n           text-align:left\">  \nThis is a more suitable data structure for a machine. Now we can represent mean_cnt per month and use the \nseaborn hue parameter on item_cat_id for example. \n\n<hr>\n    \n\ud83e\udd14\u2754 <b> Question: <\/b> So what does this look like for a single category?","376aa5d7":"<div style=\"color: #505541;\n           display:fill;\n           border-radius:0px;\n           text-indent: 10px;\n           border-style: solid;\n           border-color:#424949;\n           text-align:center;\n           background-color:#F8F9F9 ;\n           font-size:15px;\n           letter-spacing:0.5px;\n           padding: 0.2em;\n           text-align:left\">  \nThere are acutalle several items that cost 0.1. Let's look at the names and see if we can find out more. \n<\/div>","62f03624":"<a id=\"1.\"><\/a>\n<div style=\"color: #505541;\n           display:fill;\n           border-radius:0px;\n           text-indent: 10px;\n           border-style: solid;\n           border-color:#424949;\n           text-align:center;\n           background-color:#F8F9F9 ;\n           font-size:15px;\n           letter-spacing:0.5px;\n           padding: 0.2em;\n           text-align:left\">  \n<center> <h1>\ud83d\udcc4\ud83d\udd0d\ud83d\udcca 1. EDA \ud83d\udcdc \ud83d\udd2c\ud83e\uddfe <\/h1> <\/center>\n<hr>   \n    \n<h3> Data Informations: <\/h3>\n    \nLet's first look at the different data files and their features.   \nWe have four different files for training and one file for submissions.   \nFor a better understanding of the relationships between the files, I will highlight the keys (the key structure) that connect these files.\n    \n<hr>\n \n<h4> items.csv <\/h4>\n<ul>\n<li> item_name - name of item <\/li>\n<li> <b> <span style=\"color:red;\"> item_id <\/span> <\/b> - unique identifier of a product <\/li>\n<li> <b> <span style=\"color:purple;\"> item_category_id <\/span> <\/b> - unique identifier of item category <\/li>\n<\/ul> \n\n<hr>\n    \n<h4> item_categories.csv <\/h4>   \n<ul>\n<li> item_category_name - name of item category <\/li>\n<li> <b> <span style=\"color:purple;\"> item_category_id <\/span> <\/b> - unique identifier of item category <\/li>  \n<\/ul>\n    \n<hr> \n\n<h4> shops.csv <\/h4>\n<ul>\n<li> <b> <span style=\"color:green;\"> shop_id <\/span> <\/b> - unique identifier of a shop <\/li>   \n<li> shop_name - name of shop <\/li>  \n<\/ul>\n\n<hr>\n    \n<h4> sales_train.csv <\/h4>\n<ul>\n<li> date - date in format dd\/mm\/yyyy <\/li>\n<li> date_block_num - a consecutive month number, used for convenience. January 2013 is 0, February 2013 is 1,..., October 2015 is 33 <\/li>  \n<li> <b> <span style=\"color:green;\"> shop_id <\/span> <\/b> - unique identifier of a shop <\/li>\n<li> <b> <span style=\"color:red;\"> item_id <\/span> <\/b> - unique identifier of a product <\/li>   \n<li> item_price - current price of an item <\/li>\n<li> item_cnt_day - number of products sold. You are predicting a monthly amount of this measure <\/li>\n<\/ul>\n\n\n<hr>\n\n<h4> test.csv <\/h4>\n<ul>   \n<li> ID - an Id that represents a (Shop, Item) tuple within the test set <\/li>\n<li> <b> <span style=\"color:green;\"> shop_id <\/span> <\/b> - unique identifier of a shop <\/li>\n<li> <b> <span style=\"color:red;\"> item_id <\/span> <\/b> - unique identifier of a product <\/li>\n<\/ul>  \n\n<\/div>","31da598c":"<div style=\"color: #505541;\n           display:fill;\n           border-radius:0px;\n           border-style: solid;\n           border-color:#424949;\n           text-align:center;\n           background-color:#F8F9F9 ;\n           font-size:15px;\n           letter-spacing:0.5px;\n           padding: 0.2em;\n           text-align:left\">  \n\ud83d\udcdd <b> Note: <\/b> The number of shops and articles is significantly lower. This could be a problem, since we\nneed to predict the sales figures given a shop_id and an item_id.\n\n<hr>\n    \n\ud83e\udd14\u2754 <b> Question: <\/b> Can we find all shops and items from the test_df also in our train_df?  ","ae7953d9":"<a id=\"2.\"><\/a>\n<a id=\"2.1\"><\/a>\n<div style=\"color: #505541;\n           display:fill;\n           border-radius:0px;\n           text-indent: 10px;\n           border-style: solid;\n           border-color:#424949;\n           text-align:center;\n           background-color:#F8F9F9 ;\n           font-size:15px;\n           letter-spacing:0.5px;\n           padding: 0.2em;\n           text-align:left\">  \n<center> <h1> \ud83d\udcdc\ud83d\udcd2\u2709 --> 2. Data Preparation \ud83c\udfed --> \ud83d\udcc4<\/h1> <\/center>\n<hr>\nThis time we are not dealing with zero values, so we can start directly with preparing the data for modelling.\nWhat we want is a model that takes information about a particular item and its development in n months and predicts the number of items for the following month. \nBut we have to consider some other aspects:\n<ul>\n    <li>The distribution of the labels is not quite normally distributed, which makes learning for our model difficult.<\/li>\n    <li>Some items have never been sold before and have no sales history.<\/li>\n    <li>We want a flexible pipeline where the label month and the length of the sales history can be adjusted in a modular way to see what works best.<\/li>\n    <li>Last but not least, the dataset is not very small and power might be an issue. So we should use all 4 CPU\u00b4s, not just the one we normally use when working with pandas.<\/li>\n<\/ul>\nSo let's start with the clean-up and grouping and continue with some feature engineering.\n<hr>\n<center> <h2> 2.1 Data Cleaning, Merging and Grouping by Month<\/h2> <\/center>\nSince all the information about our items and shops is divided into several dataframes, the aim of this chapter is to combine this information into one dataframe for further operations. In addition, we will group by month, as we want to evaluate the number of items sold per shop and month and not per day. So let's write a basic pre-processing function that we can use later in our pipeline.","2fce763b":"<div style=\"color: #505541;\n           display:fill;\n           border-radius:0px;\n           text-indent: 10px;\n           border-style: solid;\n           border-color:#424949;\n           text-align:center;\n           background-color:#F8F9F9 ;\n           font-size:15px;\n           letter-spacing:0.5px;\n           padding: 0.2em;\n           text-align:left\">  \n<h3> \ud83d\udcdd Notes: <\/h3>\n<hr>\n<ul>\n<li>We are looking at the salse figures on a daily basis. <\/li>\n<li>So the item count distribution does not represent our target distribution. <\/li>\n    <ul>\n    <li>As already mentioned the records do not contain an entry for an item that was not purchased on a specific date. <\/li>\n    <\/ul>\n<li> But we get an idea of the price range (excluding the top 5%) and it should be noted that most items, when sold, are sold once a day and rarely more than once.<\/li>\n<\/ul>\nI assume that the prices of the individual items are not the same for every shop and every month. Let's take a quick look at some items within the same price range to verify our assumption.\n<\/div>","c9400543":"<div style=\"color: #505541;\n           display:fill;\n           border-radius:0px;\n           border-style: solid;\n           border-color:#424949;\n           text-align:center;\n           background-color:#F8F9F9 ;\n           font-size:15px;\n           letter-spacing:0.5px;\n           padding: 0.2em;\n           text-align:left\">  \n\ud83d\udcdd <b> Note: <\/b> As we can see, most element categories start at the beginning of our timeline (or before) and last until the end. But there are a few exceptions. For example, new item categories evolve and some of them had their last sold item in 2013. Nothing surprising, but good to know. Let's take a closer look at the min, max, median and mean values.\n<\/div>","a829e53c":"<div style=\"color: #505541;\n           display:fill;\n           border-radius:0px;\n           text-indent: 10px;\n           border-style: solid;\n           border-color:#424949;\n           text-align:center;\n           background-color:#F8F9F9 ;\n           font-size:15px;\n           letter-spacing:0.5px;\n           padding: 0.2em;\n           text-align:left\"> \n\ud83d\udca1 <b> Tip: <\/b> If you want to combine several data frames or simply do something like a join in SQL,\nyou should use the pandas.merge method for performance reasons. You could also use df.map(lambda x: ...), but this is \nnot advisable if the data set has a certain size.\n<\/div>","00b0f080":"<div style=\"color: #505541;\n           display:fill;\n           border-radius:0px;\n           text-indent: 10px;\n           border-style: solid;\n           border-color:#424949;\n           text-align:center;\n           background-color:#F8F9F9 ;\n           font-size:15px;\n           letter-spacing:0.5px;\n           padding: 0.2em;\n           text-align:left\">  \n\ud83d\udcdd <b> Note: <\/b> These must be the returned items. Since we want to predict the number of items sold, we should ignore these entries.\n\n<hr>\n    \n\ud83e\udd14\u2754 <b> Question: <\/b> Are there also items with item_cnt = 0? \n\nThese should then be products that were bought and returned on the same day. (An entry is only made in the data record when a product is purchased. And the number is aggregated on a daily basis, so this could be possible).\n<\/div>","aa28b9b6":"![sales_prediction_kaggle.jpg](attachment:6c60bccf-6f39-4341-843a-0910131d597a.jpg)","ce528baa":"<div style=\"color: #505541;\n           display:fill;\n           border-radius:0px;\n           border-style: solid;\n           border-color:#424949;\n           text-align:center;\n           background-color:#F8F9F9 ;\n           font-size:15px;\n           letter-spacing:0.5px;\n           padding: 0.2em;\n           text-align:left\">  \n\ud83d\udcdd <b> Note: <\/b> As we can see, we have perhaps two handfuls of shops that are much bigger than all the others. (Or we might have something like 10 extremely strong individual months for random shops. However, I think the former is more likely.) For better understanding, we will now create a new data frame with key numbers for each shop. (As we did for the item categories)\n<\/div>","3b51a08d":"<div style=\"color: #505541;\n           display:fill;\n           border-radius:0px;\n           border-style: solid;\n           border-color:#424949;\n           text-align:center;\n           background-color:#F8F9F9 ;\n           font-size:15px;\n           letter-spacing:0.5px;\n           padding: 0.2em;\n           text-align:left\">  \n\ud83d\udcdd <b> Note: <\/b> As we can see, the price range is very wide for most shops, but I am a bit suspicious about the number of shops with min_price close to zero. The rest looks good, mean and average sales are relatively close, which is good. This is not the case for mean and average price. This indicates that the price distribution for individual shops is not normally distributed.\n<hr>\n<h4> Let's take a closer look at the shops with min_price close to zero. \ud83d\udd0e<\/h4>\n<\/div>\n","c6bc4c69":"<a id=\"1.3\"><\/a>\n<div style=\"color: #505541;\n           display:fill;\n           border-radius:0px;\n           text-indent: 10px;\n           border-style: solid;\n           border-color:#424949;\n           text-align:center;\n           background-color:#F8F9F9 ;\n           font-size:15px;\n           letter-spacing:0.5px;\n           padding: 0.2em;\n           text-align:left\">   \n<center> <h2> 1.3 Overall Sales per Month \ud83d\udd26\ud83d\udcc6\ud83d\udcca<\/h2> <\/center>\n<hr>\n    \nWe start by creating a new data frame that shows the total sales per month. This can easily be done with the groupby() function of pandas datafremes.\n<\/div>","2e0a6e4d":"<div style=\"color: #505541;\n           display:fill;\n           border-radius:0px;\n           border-style: solid;\n           border-color:#424949;\n           text-align:center;\n           background-color:#F8F9F9 ;\n           font-size:15px;\n           letter-spacing:0.5px;\n           padding: 0.2em;\n           text-align:left\">  \n\ud83e\udd14\u2754 <b> Question: <\/b> How many shops and products do we have?\n<\/div>","063fd6cc":"<div style=\"color: #505541;\n           display:fill;\n           border-radius:0px;\n           border-style: solid;\n           border-color:#424949;\n           text-align:center;\n           background-color:#F8F9F9 ;\n           font-size:15px;\n           letter-spacing:0.5px;\n           padding: 0.2em;\n           text-align:left\"> \n\ud83d\udca1 <b> Tip: <\/b> You can use horizontal and vertical lines to draw attention to a specific section and to show reference values. In addition, you can add arrows and annotations to your chart to provide even more information. Take a close look at the code below if you are interested. And don't hesitate to ask questions in the comments section.\n<\/div>","2a08fa50":"<div style=\"color: #505541;\n           display:fill;\n           border-radius:0px;\n           border-style: solid;\n           border-color:#424949;\n           text-align:center;\n           background-color:#F8F9F9 ;\n           font-size:15px;\n           letter-spacing:0.5px;\n           padding: 0.2em;\n           text-align:left\">  \n\ud83d\udcdd <b> Note: <\/b> Don't forget to use the format property when you use pandas.to_datetime()! Otherwise the result would be completely different and you wouldn't even get an error message. \n    Just comment it out and check the result with df[df[\"date_block_num\"][\"month\"].unique() if you are interested.\n<\/div>","b99e16bd":"<a id=\"1.1\"><\/a>\n<div style=\"color: #505541;\n           display:fill;\n           border-radius:0px;\n           text-indent: 10px;\n           border-style: solid;\n           border-color:#424949;\n           text-align:center;\n           background-color:#F8F9F9 ;\n           font-size:15px;\n           letter-spacing:0.5px;\n           padding: 0.2em;\n           text-align:left\">  \n<center> <h2>\ud83d\udd0d\ud83d\udc40 1.1 First Look: \ud83d\udc40\ud83d\udd0e<\/h2> <\/center>\n<\/div>","8a3edf4a":"<a id=\"2.2.2\"><\/a>\n<div style=\"color: #505541;\n           display:fill;\n           border-radius:0px;\n           text-indent: 10px;\n           border-style: solid;\n           border-color:#424949;\n           text-align:center;\n           background-color:#F8F9F9 ;\n           font-size:15px;\n           letter-spacing:0.5px;\n           padding: 0.2em;\n           text-align:left\">  \n<center> <h3> 2.2.2 Shop Clustering with PCA <\/h3> <\/center>\n<hr>\nThe idea behind this feature is that we have a lot of information about the different shops and we want to combine this information into a single feature for later use. We could simply group all the shops by size, which might give us the same result. Instead, we will use PCA to reduce the dimension of the dataframe and then use a simple clustering algorithm to generate group for all the shops. To do this, we first need to convert all the features into numerical values, if possible.","d4b07a72":"<div style=\"color: #505541;\n           display:fill;\n           border-radius:0px;\n           text-indent: 10px;\n           border-style: solid;\n           border-color:#424949;\n           text-align:center;\n           background-color:#F8F9F9 ;\n           font-size:15px;\n           letter-spacing:0.5px;\n           padding: 0.2em;\n           text-align:left\">  \nThis look's as aspected. Let's move on to some feature engeneering. \ud83d\ude0e\ud83d\udd27\n<\/div>","442f845b":"<div style=\"color: #505541;\n           display:fill;\n           border-radius:0px;\n           border-style: solid;\n           border-color:#424949;\n           text-align:center;\n           background-color:#F8F9F9 ;\n           font-size:15px;\n           letter-spacing:0.5px;\n           padding: 0.2em;\n           text-align:left\">  \n\ud83d\udcdd <b> Note: <\/b> As expected, we see the same phenomenon in total sales. \nWhat we did not see when we focused on average sales per item category is <br> that total sales in 2015 are slightly lower compared to the previous two years. We should keep that in mind.\n\n<hr>\nLet's change the perspective a slightly to compare the sales figures of the individual years.\n<\/div>","3c01df15":"<div style=\"color: #505541;\n           display:fill;\n           border-radius:0px;\n           border-style: solid;\n           border-color:#424949;\n           text-align:center;\n           background-color:#F8F9F9 ;\n           font-size:15px;\n           letter-spacing:0.5px;\n           padding: 0.2em;\n           text-align:left\">  \n\ud83d\udcdd <b> Note: <\/b> I think we can say that overall we have an increase in sales in november and december. Not all product categories are effective, but most of them. In fact, we can't see much difference in seasonality between categories. This will be very useful to us when we start training a model, as we may train our first model on a category level and we may be able to predict category x when our model has been trained on all the other categories. \n\n<hr>\n    \nLet's take a closer look at total sales per month from 2013 - 2015. To do this, we create a new dataframe grouped by month only.\n<\/div>","ea19bf6b":"<div style=\"color: #505541;\n           display:fill;\n           border-radius:0px;\n           text-indent: 10px;\n           border-style: solid;\n           border-color:#424949;\n           text-align:center;\n           background-color:#F8F9F9 ;\n           font-size:15px;\n           letter-spacing:0.5px;\n           padding: 0.2em;\n           text-align:left\">  \nThis is not the case. Let's omit all entries with item_cnt_day < 0 and map outliers for our first visuals.","84cd1d3f":"<div style=\"color: #505541;\n           display:fill;\n           border-radius:0px;\n           text-indent: 10px;\n           border-style: solid;\n           border-color:#424949;\n           text-align:center;\n           background-color:#F8F9F9 ;\n           font-size:15px;\n           letter-spacing:0.5px;\n           padding: 0.2em;\n           text-align:left\">  \nOk. There seems to be an item that costs 0.1 and is stocked by many shops. Maybe a plastic bag? \n<\/div>","7b26240a":"<div style=\"color: #505541;\n           display:fill;\n           border-radius:0px;\n           text-indent: 10px;\n           border-style: solid;\n           border-color:#424949;\n           text-align:center;\n           background-color:#F8F9F9 ;\n           font-size:15px;\n           letter-spacing:0.5px;\n           padding: 0.2em;\n           text-align:left\">  \n\ud83d\udcdd <b> Note: <\/b> There are still 62 different categories and we still know very little about them. So let's create a data framework with some more information about these categories. Maybe we can summarise some of them. For better readability, we will create a data frame where each category is represented in one line. For later visualisations we can change the structure a bit. \n<\/div>","acd0c004":"<a id=\"1.2\"><\/a>\n<div style=\"color: #505541;\n           display:fill;\n           border-radius:0px;\n           text-indent: 10px;\n           border-style: solid;\n           border-color:#424949;\n           text-align:center;\n           background-color:#F8F9F9 ;\n           font-size:15px;\n           letter-spacing:0.5px;\n           padding: 0.2em;\n           text-align:left\">  \n<center> <h2> 1.2 Item Categories  \ud83e\uddf4\ud83e\uddf9\ud83c\udf81\ud83e\udd7c\ud83d\udc8d <\/h2> <\/center>\n    \n<hr>\n    \n\ud83e\udd14\u2754 <b> Question: <\/b> How many different categories do we have? \n<\/div>","c15fd9d0":"<div style=\"color: #505541;\n           display:fill;\n           border-radius:0px;\n           text-indent: 10px;\n           border-style: solid;\n           border-color:#424949;\n           text-align:center;\n           background-color:#F8F9F9 ;\n           font-size:15px;\n           letter-spacing:0.5px;\n           padding: 0.2em;\n           text-align:left\">  \n<h3> \ud83d\udcdd Notes: <\/h3>\n<hr>\n<ul>\n<li> As we can see, the price for each item changes per month and is not the same for each shop. (This is indicated by the spaces around the lines for some months) <\/li>\n<li> The price can change in both directions. <\/li>\n<\/ul>\nLet's go further and take a closer look at the different item categories.\n<\/div>","8668a221":"<div style=\"color: #505541;\n           display:fill;\n           border-radius:0px;\n           text-indent: 10px;\n           border-style: solid;\n           border-color:#424949;\n           text-align:center;\n           background-color:#F8F9F9 ;\n           font-size:15px;\n           letter-spacing:0.5px;\n           padding: 0.2em;\n           text-align:left\">  \nLooks as expected. Let's start the inspection with a boxplot for turnover and items sold per month per shop. This should give us an overall impression of the size distribution for the shops.\n<\/div>","aac78e7d":"<div style=\"color: #505541;\n           display:fill;\n           border-radius:0px;\n           border-style: solid;\n           border-color:#424949;\n           text-align:center;\n           background-color:#F8F9F9 ;\n           font-size:15px;\n           letter-spacing:0.5px;\n           padding: 0.2em;\n           text-align:left\">  \n\ud83d\udcdd <b> Note: <\/b> So we have 21807 different items and 60 different shops in our train_df.\nThe instructions say that the number of shops varies over the years. We should take a closer look at that.\n<hr>\n    \nBut first, we will look at the number of shops and items in the test_df.\n<\/div>","642b4888":"<div style=\"color: #505541;\n           display:fill;\n           border-radius:0px;\n           text-indent: 10px;\n           border-style: solid;\n           border-color:#424949;\n           text-align:center;\n           background-color:#F8F9F9 ;\n           font-size:15px;\n           letter-spacing:0.5px;\n           padding: 0.2em;\n           text-align:left\">  \n\ud83d\udcdd <b> Note: <\/b> So we have 85 different item categories. Let's include them in our eda_df and see if we can identify some seasonal trends.\n<\/div>","7b072e8d":"<a id=\"3.1\"><\/a>\n<div style=\"color: #505541;\n           display:fill;\n           border-radius:0px;\n           text-indent: 10px;\n           border-style: solid;\n           border-color:#424949;\n           text-align:center;\n           background-color:#F8F9F9 ;\n           font-size:15px;\n           letter-spacing:0.5px;\n           padding: 0.2em;\n           text-align:left\">  \n<center> <h2> 3.1 Submission<\/h2> <\/center>\n<hr>\nLet's see where we stand so far.\n<\/div> ","6d3ff221":"<a id=\"2.2.4\"><\/a>\n<div style=\"color: #505541;\n           display:fill;\n           border-radius:0px;\n           text-indent: 10px;\n           border-style: solid;\n           border-color:#424949;\n           text-align:center;\n           background-color:#F8F9F9 ;\n           font-size:15px;\n           letter-spacing:0.5px;\n           padding: 0.2em;\n           text-align:left\">  \n<center> <h3> 2.2.4 Adding Lag Features and Label<\/h3> <\/center>\n<hr>\nIn this chapter we will add the lag features. We will use multiprocessing to speed things up.","3ea45a39":"<div style=\"color: #505541;\n           display:fill;\n           border-radius:0px;\n           text-indent: 10px;\n           border-style: solid;\n           border-color:#424949;\n           text-align:center;\n           background-color:#F8F9F9 ;\n           font-size:15px;\n           letter-spacing:0.5px;\n           padding: 0.2em;\n           text-align:left\">  \n\ud83d\udca1 <b> Tip: <\/b> Defining a general style at the beginning of your notebook will give all your plots a consistent look. I use seaborn as the plotting library in this notebook. Since this library is completely based on matplotlib, the same procedure is also possible using matplotlib.\n\nSo let's do that first.","4361f0b1":"<div style=\"color: #505541;\n           display:fill;\n           border-radius:0px;\n           border-style: solid;\n           border-color:#424949;\n           text-align:center;\n           background-color:#F8F9F9 ;\n           font-size:15px;\n           letter-spacing:0.5px;\n           padding: 0.2em;\n           text-align:left\">  \n\ud83d\udcdd <b> Note: <\/b> Judging by the mean and maximum values of item_price and item_cnt_day, there are some outliers. <br> We need to keep this in mind and will exclude them for our first visualisations.\n<\/div>","3188444e":"<div style=\"color: #505541;\n           display:fill;\n           border-radius:0px;\n           border-style: solid;\n           border-color:#424949;\n           text-align:center;\n           background-color:#F8F9F9 ;\n           font-size:15px;\n           letter-spacing:0.5px;\n           padding: 0.2em;\n           text-align:left\"> \n\ud83d\udca1 <b> Tip: <\/b> If you want to look at the distribution of certain columns in your data frame, perhaps even categorised by a certain characteristic, you can use seaborn.distplot(). However, I prefer to create each plot from scratch to have more control. The code might be a bit repetitive, so I will hide it. Take a look if you are interested.\n<\/div>","c340302c":"<div style=\"color: #505541;\n           display:fill;\n           border-radius:0px;\n           text-indent: 10px;\n           border-style: solid;\n           border-color:#424949;\n           text-align:center;\n           background-color:#F8F9F9 ;\n           font-size:15px;\n           letter-spacing:0.5px;\n           padding: 0.2em;\n           text-align:left\">  \n\nNow we will write a function that does the pca and the clustering. Since we will do the exact same thing for our item_category_info_df in the next section, we will write the function a little bit more generally.We will also include the option to get a visual output to validate the pca and clustering steps.\n<\/div>","22122ba9":"<div style=\"color: #505541;\n           display:fill;\n           border-radius:0px;\n           border-style: solid;\n           border-color:#424949;\n           text-align:center;\n           background-color:#F8F9F9 ;\n           font-size:15px;\n           letter-spacing:0.5px;\n           padding: 0.2em;\n           text-align:left\">  \n\ud83d\udcdd <b> Note: <\/b> Contrary to my expectation, we see quite similar sales trends for most of the shops. We also see some shops that closed and opened between 2013 and 2015.\nThis has been a long EDA chapter and we have gathered a lot of information on our underlying date. Let's summarise what we know, think about what we want and then define our game plan.\n\n<hr>\n\n<h3>EDA Section Conclusion: \ud83d\udcca\ud83d\ude35\ud83d\udcc9<\/h3>\n<hr>  \n    \n<h4>So what do we know?<\/h4>\n<ul>\n<li>As we saw in 1.1, we have multiple items, shops, item categories and associated figures on sales per item and shop.<\/li>\n<li>In addition, we have seen that most items are not sold more than once per day and that most items cost between 0.1 and 2500 Russian Rubel.<\/li>\n<li> In Chapter 1.2, we took a closer look at the different item categories and found that most of them have their strongest months in November and December.<\/li>\n<li> Then in chapter 1.3 we saw that the overall sales show the same picture. So we should expect an increase in sales for november.<\/li>\n<li> In chapter 1.4 we completed the EDA section by looking at the different shops, their size development and item assortment.<\/li>\n    <ul>\n    <li>If a shop selling a particular item is on the upswing, this is a very important indicator of the expected sales of that item in the following months.<\/li>\n    <li>For example, we saw in the last chart that two major shops closed down just before our prediction period.\n    <\/ul>\n<\/ul>\n<hr>\n<h4>What do we want?<\/h4>\n<ul>\n<li>We want to predict the number of items sold for 214200 different rows (pairs of shop_id and item_id represented by the ID) in our test_df for November 2015.<\/li>\n<\/ul>\n<hr>\n<h4>What is the game plan?<\/h4>\n<ul>\n<li>First of all, we need to prepare the data in such a way that we can use all the information collected to train a model. <\/li>\n<li>Then I would like to train a NN with Tensorflwow 2.0 but i think we start with some basic xgBoost Regression as benchmark. <\/li>\n<\/ul>\nSo let's get on with the data preparation!\n<\/div>","77805fdb":"a id=\"3.2\"><\/a>\n<div style=\"color: #505541;\n           display:fill;\n           border-radius:0px;\n           text-indent: 10px;\n           border-style: solid;\n           border-color:#424949;\n           text-align:center;\n           background-color:#F8F9F9 ;\n           font-size:15px;\n           letter-spacing:0.5px;\n           padding: 0.2em;\n           text-align:left\">  \n<center> <h1> 3.2 Next Steps \ud83e\udd14\ud83d\udcc4<\/h1> <\/center>\n<hr>\nSo far, we have come a long way in terms of data screening. We trained our initial model using transformed data from our initial pipeline with some initial features that I thought would be well suited for this task. This initial data transformation is based on several assumptions, such as that stores of comparable size (store clusters) sell comparable amounts of items of certain types and that the algorithm is able to learn store-specific sales figures. We also train our model on all date_block_nums (except the first \"num_lags\" date_block_nums). We could also try to train our model to predict only November sales numbers. With this decision, some features like \"month\" would be obsolete and we would have less data to train on, but the task would be easier overall. Before we continue, let's outline a few more steps that we could do to improve our score.\n<ul>\n    <li> Add some more general features, like avg_lags_N. This could be the average number of sold items of the specific ID (or more generally the item_category) for a given shop_cluster.<\/li>    \n    <li> Parameter tuning with optuna or gridsearch. (the parameters selected above can most likely be improved for this task).<\/li>  \n    <li> Maybe you train some models only for november.<\/li>  \n    <li> Try different numbers of lags.<\/li>\n    <li> A way must be found to handle the elements not included in our train data. This could be done implicitly by adding some more general features, as explained earlier.<\/li>\n    <li> One could also think about defining this task as a classification task, since we classify all target values in [0,20].<\/li> \n<\/ul>","0f64aa62":"<div style=\"color: #505541;\n           display:fill;\n           border-radius:0px;\n           text-indent: 10px;\n           border-style: solid;\n           border-color:#424949;\n           text-align:center;\n           background-color:#F8F9F9 ;\n           font-size:15px;\n           letter-spacing:0.5px;\n           padding: 0.2em;\n           text-align:left\">  \n\ud83d\udcdd <b> Note: <\/b> Good. All items are in exactly one category. One less thing to worry about.\n    \n<hr>\n   \nPredicting sales for each of these categories will be challenging. Let's see how many categories are included in our test_df.\n<\/div>","50b16d70":"<a id=\"2.2.3\"><\/a>\n<div style=\"color: #505541;\n           display:fill;\n           border-radius:0px;\n           text-indent: 10px;\n           border-style: solid;\n           border-color:#424949;\n           text-align:center;\n           background-color:#F8F9F9 ;\n           font-size:15px;\n           letter-spacing:0.5px;\n           padding: 0.2em;\n           text-align:left\">  \n<center> <h3> 2.2.3 Item Category Clustering with PCA <\/h3> <\/center>\n<hr>\nIn this chapter we will do the same as in the previous chapter by collecting clusters for our item categories.","95fe216b":"<div style=\"color: #505541;\n           display:fill;\n           border-radius:0px;\n           text-indent: 10px;\n           border-style: solid;\n           border-color:#424949;\n           text-align:center;\n           background-color:#F8F9F9 ;\n           font-size:15px;\n           letter-spacing:0.5px;\n           padding: 0.2em;\n           text-align:left\">  \nLooks good. Let's create the same representations as before for our item categories to get a deeper understanding of the overall structure of the shops. Again, starting with the first and last day of business.\n<\/div>"}}