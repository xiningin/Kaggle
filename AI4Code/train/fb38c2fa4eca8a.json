{"cell_type":{"f2fe6bcc":"code","de4b2c98":"code","e7eafe87":"code","0b52ce2b":"code","d12993ef":"code","44f25920":"code","1b6954b8":"code","ee6448cb":"code","0caf2750":"code","7f40738c":"code","1de5e438":"code","a62e8980":"code","94ac1a9a":"code","93b135f6":"code","9c4c822c":"code","f4de0ec0":"code","bcbc3ea8":"code","656d102d":"code","8aa86c86":"code","80d69d09":"code","9ff094e8":"code","351b998b":"code","278b6d5a":"code","f5bd8121":"code","d0332ca6":"code","07d10c81":"code","f6b9f938":"code","eee55118":"code","61e4ee67":"markdown","8d6eca10":"markdown","93675862":"markdown","3889cd86":"markdown","fd77adf0":"markdown","7b88f192":"markdown","66f15723":"markdown","3c5fdbf8":"markdown","f0df6c8e":"markdown","3f5323bd":"markdown","f8132c3f":"markdown","59ff891d":"markdown","4c89c031":"markdown","2d1c36aa":"markdown"},"source":{"f2fe6bcc":"#load library\nimport os\nimport pandas as pd\nimport numpy as np\nimport gensim\nfrom gensim.utils import simple_preprocess\nfrom gensim import corpora, models\nfrom gensim.parsing.preprocessing import STOPWORDS\nfrom nltk.stem import WordNetLemmatizer, SnowballStemmer\nfrom nltk.stem.porter import *\n\nimport datetime\nimport time\nfrom sklearn.decomposition import PCA\nfrom sklearn.manifold import TSNE\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D\n\nimport nltk","de4b2c98":"meta = pd.read_csv(\"\/kaggle\/input\/CORD-19-research-challenge\/metadata.csv\")\nprint(meta.shape)","e7eafe87":"### first filter by meta file. select only papers after 2020\nmeta[\"publish_time\"] = pd.to_datetime(meta[\"publish_time\"])\nmeta[\"publish_year\"] = (pd.DatetimeIndex(meta['publish_time']).year)\nmeta[\"publish_month\"] = (pd.DatetimeIndex(meta['publish_time']).month)\nmeta = meta[meta[\"publish_year\"] == 2020]\nprint(meta.shape[0], \" papers are available after 2020 Jan 1.\")","0b52ce2b":"#count how many has abstract\ncount = 0\nindex = []\nfor i in range(len(meta)):\n    #print(i)\n    if type(meta.iloc[i, 8])== float:\n        count += 1\n    else:\n        index.append(i)\n\nprint(len(index), \" papers have abstract available.\")","d12993ef":"##extract the abstract to pandas \ndocuments = meta.iloc[index, 8]\ndocuments=documents.reset_index()\ndocuments.drop(\"index\", inplace = True, axis = 1)\n\n##create pandas data frame with all abstracts, use as input corpus\ndocuments[\"index\"] = documents.index.values\ndocuments.head(3)","44f25920":"np.random.seed(400)\nstemmer = SnowballStemmer(\"english\")","1b6954b8":"##lemmatize and stemming\n\ndef lemmatize_stemming(text):\n    return stemmer.stem(WordNetLemmatizer().lemmatize(text, pos='v'))\n\n# Tokenize and lemmatize\ndef preprocess(text):\n    result=[]\n    for token in gensim.utils.simple_preprocess(text) :\n        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 3:\n            # TODO: Apply lemmatize_stemming on the token, then add to the results list\n            result.append(lemmatize_stemming(token))\n    return result","ee6448cb":"## use example to check the preprocessing step\n\ndocument_num = 1000  ##randomly pick one abstract\ndoc_sample = documents[documents[\"index\"] == document_num].values[0][0]\n\nprint(\"Original document: \")\nwords = []\nfor word in doc_sample.split(' '):\n    words.append(word)\nprint(words)\nprint(\"\\n\\nTokenized and lemmatized document: \")\nprint(preprocess(doc_sample))","0caf2750":"##preprocess all abstracts\nprocessed_docs = documents['abstract'].map(preprocess)\nprocessed_docs[:5]","7f40738c":"##create dictionary based on the preprocessed_documents\ndictionary = gensim.corpora.Dictionary(processed_docs)\n\n##check the dictionary\ncount = 0\nfor k, v in dictionary.iteritems():\n    print(k, v)\n    count += 1\n    if count > 5:\n        break","1de5e438":"## remove extreme words (very common and very rare)\ndictionary.filter_extremes(no_below=15, no_above=0.1)\n\n##create bag-of-word model for each documents\nbow_corpus = [dictionary.doc2bow(doc) for doc in processed_docs]","a62e8980":"## check the bow_corpus\nbow_doc_1000 = bow_corpus[document_num]\n\nfor i in range(len(bow_doc_1000)):\n    print(\"Word {} (\\\"{}\\\") appears {} time.\".format(bow_doc_1000[i][0], \n                                                     dictionary[bow_doc_1000[i][0]], \n                                                     bow_doc_1000[i][1]))","94ac1a9a":"#create tf-idf from bow_corpus\ntfidf = models.TfidfModel(bow_corpus)\ncorpus_tfidf = tfidf[bow_corpus]\n\n#preview the corpus_tfidf\nfrom pprint import pprint\nfor doc in corpus_tfidf:\n    pprint(doc)\n    break","93b135f6":"now = datetime.datetime.now()\nprint (\"start model building at \",now.strftime(\"%Y-%m-%d %H:%M:%S\"))\nlda_model = gensim.models.LdaMulticore(bow_corpus, \n                                       num_topics=5, \n                                       id2word = dictionary, \n                                       passes = 50, \n                                       workers=4) \n\nnow = datetime.datetime.now()\nprint ('Model training finished at ',now.strftime(\"%Y-%m-%d %H:%M:%S\"))","9c4c822c":"##print out the key words of five topics\nfor idx, topic in lda_model.print_topics(-1):\n    print(\"Topic: {} \\nWords: {}\".format(idx, topic))\n    print(\"\\n\")","f4de0ec0":"now = datetime.datetime.now()\nprint (\"start model building at \",now.strftime(\"%Y-%m-%d %H:%M:%S\"))\n\nlda_model_tfidf = gensim.models.LdaMulticore(corpus_tfidf, \n                                             num_topics=5, \n                                             id2word = dictionary, \n                                             passes = 50, \n                                             workers=4)\nnow = datetime.datetime.now()\nprint ('Model training finished at ',now.strftime(\"%Y-%m-%d %H:%M:%S\"))","bcbc3ea8":"## check the key words of five topics\nfor idx, topic in lda_model_tfidf.print_topics(-1):\n    print(\"Topic: {} Word: {}\".format(idx, topic))\n    print(\"\\n\")","656d102d":"documents_lda_topics = pd.DataFrame(columns = [\"topic1\", \"topic2\", \"topic3\", \"topic4\", \"topic5\"])\ndocuments_lda_tfidf_topics = pd.DataFrame(columns = [\"topic1\", \"topic2\", \"topic3\", \"topic4\", \"topic5\"])\nfor i in range(len(bow_corpus)):\n    if i % 500 ==0:\n        print(i)\n    documents_lda_topics.loc[i] = [0] * 5\n    documents_lda_tfidf_topics.loc[i] = [0] * 5\n    \n    output = lda_model.get_document_topics(bow_corpus[i])\n    for j in range(len(output)):\n        a = output[j][0]\n        b = output[j][1]\n        documents_lda_topics.iloc[i,a] = b\n    \n    output_tfidf = lda_model_tfidf.get_document_topics(bow_corpus[i])\n    for k in range(len(output_tfidf)):\n        a = output_tfidf[k][0]\n        b = output_tfidf[k][1]\n        documents_lda_tfidf_topics.iloc[i, a] = b\n        \nprint(\"Data processing finished\")","8aa86c86":"## pick the final topic for each abstract based on max-probability\nfor i in range(5):\n    documents_lda_topics.iloc[:, i] = documents_lda_topics.iloc[:, i].astype('float64', copy=False)\n    \ndocuments_lda_topics[\"final_topic\"] =documents_lda_topics.iloc[:, :5].idxmax(axis=1)\n\nfor i in range(5):\n    documents_lda_tfidf_topics.iloc[:, i] = documents_lda_tfidf_topics.iloc[:, i].astype('float64', copy=False)\n\ndocuments_lda_tfidf_topics[\"final_topic\"] =documents_lda_tfidf_topics.iloc[:, :5].idxmax(axis=1)","80d69d09":"##preview the dataframe for both models\nprint(\"LDA + bow_corpus: topic probability:\")\ndocuments_lda_topics.head(3)\nprint(\"LDA + TF-IDF_corpus: topic probability:\")\ndocuments_lda_tfidf_topics.head(3)","9ff094e8":"pca = PCA(n_components=3)\npca_result = pca.fit_transform(documents_lda_topics.iloc[:, :5])","351b998b":"## with 3 components, variance explained\npca.explained_variance_ratio_","278b6d5a":"##create dataframe with projected vectors from PCA\npca_df = pd.DataFrame()\npca_df['pca-one'] = pca_result[:,0]\npca_df['pca-two'] = pca_result[:,1] \npca_df[\"pca-three\"] = pca_result[:, 2]\npca_df[\"topic\"] = documents_lda_topics.iloc[:, 5].replace({\"topic1\": \"red\", \"topic2\": \"blue\", \"topic3\": \"green\", \"topic4\": \"yellow\", \"topic5\": \"black\"})","f5bd8121":"plt.figure(figsize=(16,10))\nsns.scatterplot(\n    x=\"pca-one\", y=\"pca-two\",\n    hue= documents_lda_topics.iloc[:, 5].replace({\"topic1\": \"red\", \"topic2\": \"blue\", \"topic3\": \"green\", \"topic4\": \"yellow\", \"topic5\": \"black\"}),\n    data=pca_df,\n    legend=\"full\",\n    alpha=0.3)","d0332ca6":"ax = plt.figure(figsize=(16,10)).gca(projection='3d')\nax.scatter(\n    xs=pca_df[\"pca-one\"], \n    ys=pca_df[\"pca-two\"], \n    zs=pca_df[\"pca-three\"], \n    cmap='tab10',\n    c = documents_lda_topics.iloc[:, 5].replace({\"topic1\": \"red\", \"topic2\": \"blue\", \"topic3\": \"green\", \"topic4\": \"yellow\", \"topic5\": \"black\"})\n)\nax.set_xlabel('pca-one')\nax.set_ylabel('pca-two')\nax.set_zlabel('pca-three')\nplt.show()","07d10c81":"##first run TSNE\nimport time\ntime_start = time.time()\ntsne = TSNE(n_components=2, verbose=1, perplexity=40, n_iter=300)\ntsne_results = tsne.fit_transform(documents_lda_topics.iloc[:, :5])","f6b9f938":"##create dataframe with TSNE results\ntsne_df = pd.DataFrame()\ntsne_df['tsne-2d-one'] = tsne_results[:,0]\ntsne_df['tsne-2d-two'] = tsne_results[:,1]","eee55118":"plt.figure(figsize=(16,10))\nsns.scatterplot(\n    x=\"tsne-2d-one\", y=\"tsne-2d-two\",\n    hue=documents_lda_topics.iloc[:, 5].replace({\"topic1\": \"red\", \"topic2\": \"blue\", \"topic3\": \"green\", \"topic4\": \"yellow\", \"topic5\": \"black\"}),\n    #palette=sns.color_palette(\"hls\", 10),\n    data=tsne_df,\n    legend=\"full\",\n    alpha=0.3)","61e4ee67":"Based on the key words selected above, we can somehow summarized the five major topics as below:\n1. immunology\n2. hubei social, individual quarantin\n3. healthcare, recommendation\n4. genomic sequence\n5. symptoms (fever, chest image) + admision","8d6eca10":"## 6.1 PCA-2D","93675862":"## 6.2 PCA-3D","3889cd86":"## 3.1 Bag of words on the dataset","fd77adf0":"Based on the keywords above, we can summarize the five topics as:\n1. healthcare and research\n2. disease co-morbidities\n3. Drug and genomic sequencing, biomedical\n4. Disease spread\n5. Fever, chest image, symptoms\n  \nWe can see, the topics selected out by LDA+TF_DF are not exactly the same as above but very similar.","7b88f192":"## 4.2 Run LDA + TF-IDF corpus\n\nHere, we use the TF-IDF corpus as our input and compare the topics with what we obtained above.","66f15723":"## 5. Apply model to get all abstracts' topic\n\nWith the model we trained above (LDA + bow_copus, and LDA + TF-IDF_corpus), we applied all our abstracts into them and save the probability of each topic to data frame.","3c5fdbf8":"## 4.1 Run LDA with bow_corpus\n\nHere, we use the LDA model from gensim. I arbitary choose 5 topics. This can be changed based on domain knowledge.","f0df6c8e":"## 6.3 T-SNE-2D","3f5323bd":"## 1. Load library and prepare data","f8132c3f":"## 3.2 TF-IDF\n\nCreate the TF-IDF and use it as input for LDA also.","59ff891d":"# TOPIC MODELING + LDA (latent dirichlet allocation)\n\nWe used only papers after 2020: (1) reduce the computation time; (2) most papers directly describe about COVID-19 are available after 2020. In order to increase speed and model accuracy, we included abstracts (papers without abstracts were excluded) as our main corpus, followed","4c89c031":"## 2. Data Processing\n\nThis section will go through simple text processing, tokenization, remove stop words, lemmatization, and stemming.","2d1c36aa":"## 6. Abstracts' topic visualization\n\nIn this section, we used PCA-2D, PCA-3D, and T-SNE to visualize how the topics are distributed in all abstracts. Only the LDA+bow_corpus model will be visualized here."}}