{"cell_type":{"0eecb6fb":"code","1b6d8dfb":"code","67e6ed2a":"code","3da18c76":"code","767c362e":"code","1103995c":"code","2fa3376b":"code","1ec75da8":"code","d108ab3c":"code","f07cf472":"code","c181b179":"code","0c6f3f0a":"code","2b792750":"code","1c28a1f1":"code","c951f43f":"code","8fe84ba8":"code","f6dc91fc":"code","b438a5d3":"code","4a90cd15":"code","4505dbbb":"code","e42efe5c":"code","e51b6c85":"code","5b636759":"code","0aec1c8f":"code","fc3f9dbc":"code","8b37cc4e":"code","c6c88193":"code","8d0a41f9":"code","07049845":"code","fb9ecb84":"code","32ae715b":"code","23f44f9c":"code","2a8ead37":"code","45b9219a":"code","53ae5182":"code","a81867ae":"code","1f6075d3":"code","aebe6c4e":"code","743eb76c":"code","29b47c6b":"code","86a53198":"code","42bfc331":"code","7dfb4104":"code","fdcb32e5":"code","3d0bea4d":"code","4b5a28c8":"code","f27312c5":"code","8452af06":"code","0b5c4e57":"code","4f5c0c4a":"code","e694d47f":"code","25312e0c":"code","166b45ad":"code","e7139cc8":"code","2d49fd8b":"code","49bcb7aa":"code","58cda36e":"code","59920851":"code","dbb39c32":"code","02c6c2b8":"code","ef2e6f98":"code","8e361c23":"code","161fcd2c":"code","a4ae4427":"code","d3ed004b":"code","3d7ca0db":"code","4cbaad91":"code","96ab1557":"code","5c23aaeb":"code","bb1f55d9":"markdown","dbf7ab7f":"markdown","0d42e58a":"markdown","8dd6eca5":"markdown","ecd8b310":"markdown","2c973610":"markdown","31d040e5":"markdown","f81fafee":"markdown","70476f1d":"markdown","e5a0c79a":"markdown","58d9609c":"markdown","d2aad4b2":"markdown","d2882bc6":"markdown","66cfcdb6":"markdown","58a430d2":"markdown","ce5ade0b":"markdown","0182342c":"markdown","4ccec81a":"markdown","7db06b0b":"markdown","97579bb4":"markdown","8a5b54ea":"markdown","95d8c282":"markdown","cbf53d3c":"markdown","3af1158f":"markdown","b1719f24":"markdown","fcc30063":"markdown","bdec7ba2":"markdown","3c3df29e":"markdown","f746c10e":"markdown","af682d08":"markdown","6760dd52":"markdown","48e66806":"markdown","7b0e82b8":"markdown","bfa6e9e0":"markdown"},"source":{"0eecb6fb":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport tensorflow as tf\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","1b6d8dfb":"df_tweet_train = pd.read_csv('..\/input\/nlp-getting-started\/train.csv')\ndf_tweet_test = pd.read_csv('..\/input\/nlp-getting-started\/test.csv')","67e6ed2a":"df_tweet_train.shape","3da18c76":"df_tweet_test.shape","767c362e":"print(df_tweet_train.columns)\nprint(df_tweet_test.columns)","1103995c":"df_tweet_train.info()","2fa3376b":"df_tweet_test.info()","1ec75da8":"print(round(((((df_tweet_train.isnull().sum())\/(df_tweet_train.shape[0])).sort_values(ascending=False))*100),3))\nprint('\\n')\nprint(round(((((df_tweet_test.isnull().sum())\/(df_tweet_test.shape[0])).sort_values(ascending=False))*100),3))","d108ab3c":"df_tweet_train['keyword'].mode()[0]","f07cf472":"df_tweet_train['location'] = df_tweet_train['location'].fillna(\n                             df_tweet_train['location'].mode()[0])\n\ndf_tweet_test['location'] = df_tweet_test['location'].fillna(\n                            df_tweet_test['location'].mode()[0])\n","c181b179":"df_tweet_train['keyword'] = df_tweet_train['keyword'].fillna(\n                            df_tweet_train['keyword'].mode()[0])\n\ndf_tweet_test['keyword'] = df_tweet_test['keyword'].fillna(\n                           df_tweet_test['keyword'].mode()[0])","0c6f3f0a":"print(round(((((df_tweet_train.isnull().sum())\/(df_tweet_train.shape[0])).sort_values(ascending=False))*100),3))\nprint()\nprint(round(((((df_tweet_test.isnull().sum())\/(df_tweet_test.shape[0])).sort_values(ascending=False))*100),3))","2b792750":"import matplotlib.pyplot as plt\nimport seaborn as sns\n\n%matplotlib inline\nsns.set_style('whitegrid')","1c28a1f1":"sns.countplot(df_tweet_train['target'])","c951f43f":"fig,(ax1,ax2)=plt.subplots(1,2,figsize=(10,5))\n\ntlen=df_tweet_train[df_tweet_train['target']==1]['text'].str.len()\nax1.hist(tlen)\nax1.set_title('Real tweets')\n\ntlen=df_tweet_train[df_tweet_train['target']==0]['text'].str.len()\nax2.hist(tlen)\nax2.set_title('Fake tweets')\n\nplt.show()","8fe84ba8":"plt.figure(figsize=(10,8))\ndf_tweet_train['keyword'].value_counts()[:20].plot.bar()\nplt.xticks(rotation=50)\nplt.xlabel(\"Keywords\")\nplt.ylabel(\"Number of Tweets\")","f6dc91fc":"plt.figure(figsize=(10,8))\ndf_tweet_train['location'].value_counts()[:20].plot.bar()\nplt.xticks(rotation=50)\nplt.xlabel(\"Location\")\nplt.ylabel(\"Number of Tweets\")","b438a5d3":"target = df_tweet_train.groupby(['keyword','target'])\n\nplt.figure(figsize=(15,10))\ntarget.count()['id'][:30].plot.bar()\nplt.xticks(rotation=50)\nplt.xlabel(\"Keywords (Fake and Real Tweets) \")\nplt.ylabel(\"Number of Tweets\")\n\nplt.show()","4a90cd15":"from spacy.lang.en.stop_words import STOP_WORDS\nfrom wordcloud import WordCloud","4505dbbb":"def word(text):\n    \n    comment_words = ' '\n    stopwords = list(STOP_WORDS) \n    \n    for val in text: \n\n         \n        val = str(val)   \n        tokens = val.split() \n\n        \n        for i in range(len(tokens)): \n            tokens[i] = tokens[i].lower() \n\n        for words in tokens: \n            comment_words = comment_words + words + ' '\n\n\n    wordcloud = WordCloud(width = 500, height = 400, \n                    background_color ='black', \n                    stopwords = stopwords, \n                    min_font_size = 10).generate(comment_words) \n\n                            \n    plt.figure(figsize = (12, 12), facecolor = None ) \n    plt.imshow(wordcloud, interpolation='bilinear') \n    plt.axis(\"off\") \n    plt.tight_layout(pad = 0) \n\n    plt.show() ","e42efe5c":"text = df_tweet_train.text.values\nword(text)","e51b6c85":"text = df_tweet_train.location.values\nword(text)","5b636759":"import nltk, re\ndata = [df_tweet_train, df_tweet_test]","0aec1c8f":"def html_tag(value):\n    \n    result = re.sub(r\"<[^>]+#>\", \"\", value)\n    return result\n\ndef hyperlink(value):\n    \n    result = re.sub(r\"https?:\/\/\\S+|www\\.\\S+\", \"\", value)\n    return result\n\ndef hashtag(value):\n    \n    result = re.sub(r\"#\", \"\", value)\n    return result\n\ndef emoticon(value):\n    regrex_pattern = re.compile(pattern = \"[\"\n        u\"\\U0001F600-\\U0001F64F\"  \n        u\"\\U0001F300-\\U0001F5FF\"  \n        u\"\\U0001F680-\\U0001F6FF\"  \n        u\"\\U0001F1E0-\\U0001F1FF\"  \n                           \"]+\", flags = re.UNICODE)\n    return regrex_pattern.sub(r'',value)","fc3f9dbc":"for dataset in data:\n    \n    dataset['text'] = dataset['text'].apply(html_tag)\n    dataset['text'] = dataset['text'].apply(hyperlink)\n    dataset['text'] = dataset['text'].apply(hashtag)\n    dataset['text'] = dataset['text'].apply(emoticon)","8b37cc4e":"import pycountry\n\ndef findcon1(text):\n        \n    for country in pycountry.countries:\n        if country.name in text:   \n            a = country.name\n            return a\n        else:\n            try:\n                a = pycountry.countries.search_fuzzy(text.split()[-1])[0].name\n                return a\n            \n            except:\n                return text","c6c88193":"for dataset in data:\n    \n    dataset['nlocation'] = dataset['location'].apply(lambda x : findcon1(x))\n    ","8d0a41f9":"plt.figure(figsize=(10,8))\ndf_tweet_train['nlocation'].value_counts()[:20].plot.bar()\nplt.xticks(rotation=50)\nplt.xlabel(\"Location\")\nplt.ylabel(\"Number of Tweets\")","07049845":"df = df_tweet_train[['target','text']]","fb9ecb84":"from sklearn.model_selection import train_test_split\nX = df['text']\ny = df['target']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)","32ae715b":"from sklearn.naive_bayes import MultinomialNB\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.feature_extraction.text import CountVectorizer,TfidfTransformer\nfrom sklearn.metrics import confusion_matrix\n\npipe1 = Pipeline([('vectorize', CountVectorizer()),('tfidf', TfidfTransformer()),('classifier', MultinomialNB())])\n\npipe1.fit(X_train, y_train)\n\nprediction1 = pipe1.predict(X_test)\n\naccuracy1 = round((pipe1.score(X_test, y_test)*100),0)\nprint('Accuracy: ',accuracy1,'%')\nprint()\nprint('Confusion Matrix: \\n',confusion_matrix(y_test,prediction1))\n","23f44f9c":"from sklearn.linear_model import SGDClassifier\n\npipe2 = Pipeline([('vectorize', CountVectorizer()),('tfidf', TfidfTransformer()),('classifier', SGDClassifier())])\n\npipe2.fit(X_train, y_train)\n\nprediction2 = pipe2.predict(X_test)\n\naccuracy2 = round((pipe2.score(X_test, y_test)*100),0)\nprint('Accuracy: ',accuracy2,'%')\nprint()\nprint('Confusion Matrix: \\n',confusion_matrix(y_test, prediction2))","2a8ead37":"from sklearn.svm import SVC\n\npipe3 = Pipeline([('vectorize', CountVectorizer()),('tfidf', TfidfTransformer()),('classifier', SVC())])\n\npipe3.fit(X_train, y_train)\n\nprediction3 = pipe3.predict(X_test)\n\naccuracy3 = round((pipe3.score(X_test, y_test)*100),0)\nprint('Accuracy: ',accuracy3,'%')\nprint()\nprint('Confusion Matrix: \\n',confusion_matrix(y_test, prediction3))","45b9219a":"import spacy\nimport string\nfrom spacy.lang.en.stop_words import STOP_WORDS\nfrom spacy.lang.en import English","53ae5182":"stopwords = list(STOP_WORDS)\npunctuations = string.punctuation\nparser = English()","a81867ae":"def tokenizer(sentence):\n    tokens = parser(sentence)\n    tokens = [ word.lemma_.lower() if word.lemma_ != \"-PRON-\" else word.lower_ for word in tokens ]\n    tokens = [ word for word in tokens if word not in stopwords and word not in punctuations ]\n    return tokens","1f6075d3":"from sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.base import TransformerMixin \nfrom sklearn.svm import LinearSVC","aebe6c4e":"class predictors(TransformerMixin):\n    def transform(self, X, **transform_params):\n        return [clean_text(text) for text in X]\n    def fit(self, X, y=None, **fit_params):\n        return self\n    def get_params(self, deep=True):\n        return {}\n\n \n    def clean_text(text):     \n        return text.strip().lower()","743eb76c":"vectorizer = CountVectorizer(tokenizer = tokenizer, ngram_range=(1,1)) \nclassifier = SVC()","29b47c6b":"pipe4 = Pipeline([('cleaner', predictors()),('vectorizer', vectorizer),('classifier', classifier)])\n\npipe4.fit(X_train,y_train)","86a53198":"prediction4 = pipe4.predict(X_test)\n\naccuracy4 = round((pipe4.score(X_test, y_test)*100),0)\nprint('Accuracy: ',accuracy4,'%')\nprint()\nprint('Confusion Matrix: \\n',confusion_matrix(y_test,prediction4))","42bfc331":"tfvectorizer = TfidfVectorizer(tokenizer = tokenizer)","7dfb4104":"pipe5 = Pipeline([('cleaner', predictors()),('vectorizer', tfvectorizer),('classifier', classifier)])\n\npipe5.fit(X_train,y_train)","fdcb32e5":"prediction5 = pipe4.predict(X_test)\n\naccuracy5 = round((pipe5.score(X_test, y_test)*100),0)\nprint('Accuracy: ',accuracy5,'%')\nprint()\nprint('Confusion Matrix: \\n',confusion_matrix(y_test,prediction5))","3d0bea4d":"import tensorflow as tf\n\nfrom keras.models import Sequential\nfrom keras import layers\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences","4b5a28c8":"sentences = df['text'].values\ny = df['target'].values\n\nsentences_train, sentences_test, y_train, y_test = train_test_split(sentences, y, test_size=0.20, random_state=42)","f27312c5":"vectorizer = CountVectorizer()\nvectorizer.fit(sentences_train)\n\nX_train = vectorizer.transform(sentences_train)\nX_test  = vectorizer.transform(sentences_test)","8452af06":"model1 = Sequential()\nmodel1.add(layers.Dense(32, input_dim=X_train.shape[1], activation='relu'))\nmodel1.add(layers.Dense(64,activation='relu'))\nmodel1.add(layers.Dense(1, activation='sigmoid'))\n\nmodel1.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n\nmodel1.summary()","0b5c4e57":"history = model1.fit(X_train, y_train,\n                    epochs=10,\n                    verbose=False,\n                    validation_data=(X_test, y_test),\n                    batch_size=10)","4f5c0c4a":"loss, accuracy = model1.evaluate(X_test, y_test, verbose=False)\n\naccuracy6 = round((accuracy*100),0)\nprint('Accuracy: ',accuracy6,'%')","e694d47f":"vocab = {}  \nword_encoding = 1\ndef bow(sentence):\n    \n    text = parser(sentence)\n    text = [ word.lemma_.lower().strip() if word.lemma_ != \"-PRON-\" else word.lower_ for word in text ]\n    text = [ word for word in text if word not in stopwords and word not in punctuations ]\n         \n    global word_encoding\n    words = text \n    bag = {}  \n\n    for word in words:\n        \n        if word in vocab:\n            encoding = vocab[word]  \n        else:\n            vocab[word] = word_encoding\n            encoding = word_encoding\n            word_encoding += 1\n    \n        if encoding in bag:\n            bag[encoding] += 1\n        else:\n            bag[encoding] = 1\n  \n    return bag","25312e0c":"vectorizer = TfidfVectorizer(tokenizer = bow)\nvectorizer.fit(sentences_train)\n\nX_train = vectorizer.transform(sentences_train)\nX_test  = vectorizer.transform(sentences_test)","166b45ad":"model2 = Sequential()\nmodel2.add(layers.Dense(32, input_dim=X_train.shape[1], activation='relu'))\nmodel2.add(layers.Dense(64,activation='relu'))\nmodel2.add(layers.Dense(128,activation='relu'))\nmodel2.add(layers.Dense(256,activation='relu'))\nmodel2.add(layers.Dense(1, activation='sigmoid'))\n\nmodel2.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n\nmodel2.summary()","e7139cc8":"history = model2.fit(X_train, y_train,\n                    epochs=10,\n                    verbose=False,\n                    validation_data=(X_test, y_test),\n                    batch_size=10)","2d49fd8b":"loss, accuracy = model2.evaluate(X_test, y_test, verbose=False)\n\naccuracy7 = round((accuracy*100),0)\nprint('Accuracy: ',accuracy7,'%')","49bcb7aa":"tokenizer = Tokenizer(num_words=5000)\ntokenizer.fit_on_texts(sentences_train)\n\nX_train = tokenizer.texts_to_sequences(sentences_train)\nX_test = tokenizer.texts_to_sequences(sentences_test)","58cda36e":"vsize = len(tokenizer.word_index) + 1\nedim = 50\nmaxlen = 100","59920851":"X_train = pad_sequences(X_train, padding='post', maxlen=maxlen)\nX_test = pad_sequences(X_test, padding='post', maxlen=maxlen)","dbb39c32":"model3 = Sequential()\nmodel3.add(layers.Embedding(input_dim=vsize,output_dim=edim,input_length=maxlen))\nmodel3.add(layers.Flatten())\nmodel3.add(layers.Dense(256, activation='relu'))\nmodel3.add(layers.Dense(128, activation='relu'))\nmodel3.add(layers.Dense(64, activation='relu'))\nmodel3.add(layers.Dense(32, activation='relu'))\nmodel3.add(layers.Dense(1, activation='sigmoid'))\n\nmodel3.compile(optimizer='adam', loss='binary_crossentropy',metrics=['accuracy'])\n\nmodel3.summary()","02c6c2b8":"history = model3.fit(X_train, y_train,\n                    epochs=10,\n                    verbose=False,\n                    validation_data=(X_test, y_test),\n                    batch_size=32)\n","ef2e6f98":"loss, accuracy = model3.evaluate(X_test, y_test, verbose=False)\n\naccuracy8 = round((accuracy*100),0)\nprint('Accuracy: ',accuracy8,'%')","8e361c23":"model4 = Sequential()\nmodel4.add(layers.Embedding(input_dim=vsize,output_dim=edim,input_length=maxlen))\nmodel4.add(layers.LSTM(50))\nmodel4.add(layers.Dense(10, activation='relu'))\nmodel4.add(layers.Dense(1, activation='sigmoid'))\n\nmodel4.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy'])\nmodel4.summary()","161fcd2c":"history = model4.fit(X_train, y_train,\n                    epochs=10,\n                    verbose=False,\n                    validation_data=(X_test, y_test),\n                    batch_size=10)","a4ae4427":"loss, accuracy = model4.evaluate(X_test, y_test, verbose=False)\n\naccuracy9 = round((accuracy*100),0)\nprint('Accuracy: ',accuracy9,'%')","d3ed004b":"model5 = Sequential()\nmodel5.add(layers.Embedding(input_dim=vsize, output_dim=edim,input_length=maxlen))\nmodel5.add(layers.GlobalMaxPooling1D())\nmodel5.add(layers.Dense(10, activation='relu'))\nmodel5.add(layers.Dense(1, activation='sigmoid'))\n\nmodel5.compile(optimizer='adam',loss='binary_crossentropy', metrics=['accuracy'])\nmodel5.summary()","3d7ca0db":"history = model5.fit(X_train, y_train,\n                    epochs=10,\n                    verbose=False,\n                    validation_data=(X_test, y_test),\n                    batch_size=10)\n","4cbaad91":"loss, accuracy = model5.evaluate(X_test, y_test, verbose=False)\n\naccuracy10 = round((accuracy*100),0)\nprint('Accuracy: ',accuracy10,'%')","96ab1557":"data = [['Naive Bayes Classifier',accuracy1],\n       ['Linear Support Vector Machine',accuracy2],\n       ['Support Vector Machine',accuracy3],\n       ['Count Vectorizer along with Support Vector Classifier',accuracy4],\n       ['TF-IDF Vectorizer along with Support Vector Classifier',accuracy5],\n       ['Basic Neural Model',accuracy6],\n       ['Bag Of Words (BOW)',accuracy7],\n       ['Word Embedding',accuracy8],\n       ['Long-Short Term Memory (LSTM)',accuracy9],\n       ['Neural Network with Pooling layers',accuracy10]]\n\nfinal = pd.DataFrame(data,columns=['Algorithm','Precision'],index=[1,2,3,4,5,6,7,8,9,10])\n","5c23aaeb":"print(\"The results of Data Modeling are as follows:\\n \")\nprint(final)","bb1f55d9":"* Linear Support Vector Machine","dbf7ab7f":"# **Data Cleaning**","0d42e58a":"* Count Vectorizer along with Support Vector Classifier","8dd6eca5":"Fatalities is the most used keyword to indicate an accident within the tweets.","ecd8b310":"The dataset consists of 7613 records with 5 distinct features.","2c973610":"The features has 1 numeric values and 3 categoric values.","31d040e5":"Maximum tweets have been tweeted from USA. \nWe notice there are inconsistencies with respect to the location as some tweets are labelled with only the country, some along with the state and the rest with only the state. We will furthur look into this inconsistency","f81fafee":"# **References**\n1. SapCy  \nhttps:\/\/github.com\/Jcharis\/Natural-Language-Processing-Tutorials\/blob\/master\/Text%20Classification%20With%20Machine%20Learning,SpaCy,Sklearn(Sentiment%20Analysis)\/Text%20Classification%20&%20Sentiment%20Analysis%20with%20SpaCy,Sklearn.ipynb\n2. Neural Network  \nhttps:\/\/www.kaggle.com\/sanikamal\/text-classification-with-python-and-keras\n3. Neural Network (BOW) https:\/\/colab.research.google.com\/drive\/1ysEKrw_LE2jMndo1snrZUh5w87LQsCxk#forceEdit=true&sandboxMode=true&scrollTo=Fo3WY-e86zX2","70476f1d":"# **Data Visualization**","e5a0c79a":"* TF-IDF Vectorizer along with Support Vector Classifier","58d9609c":"# **SpaCy**","d2aad4b2":"The number of Fake tweets are more than Real tweets which are denoted as 0 and 1 respectively.","d2882bc6":"* Neural Network with Pooling layers","66cfcdb6":"Location and keywords have minor missing values in the training as well as the testing datasets, thus we need to handle these values. ","58a430d2":"# **Neural Network**","ce5ade0b":"During the analysis we notice many inconsitencies in the loction feature. We now try to extract the correct loctation of each tweet with the help of pycountry library.","0182342c":"* Basic Neural Model","4ccec81a":"* Support Vector Machine","7db06b0b":"The features has 2 numeric values and 3 categoric values.","97579bb4":"As compared to the previous inconistent locaton output, we get a better view so as to which countries have the highest number of tweets. The highest number of tweets belong to The United States.","8a5b54ea":"We notice that our \"Text\" is very dirty. Thus it is necessary that we filter out all the unwanted signs, symbols, hyperlinks, hastags, etc.","95d8c282":"* Naive Bayes Classifier","cbf53d3c":"* Long-Short Term Memory (LSTM)","3af1158f":"# **Conclusion**","b1719f24":"# **Feature Engineering**","fcc30063":"# **Data Modeling**","bdec7ba2":"# **Tweet Predictor**","3c3df29e":"* Word Embedding  ","f746c10e":"The dataset consists of 3263 records with 4 distinct features.","af682d08":"# **Data Loading**","6760dd52":"# **Machine Learning**","48e66806":"* Bag Of Words (BOW)","7b0e82b8":"Maximum length of tweets of Disastrous tweets and Non-Disastrous tweets lie between 120-150.","bfa6e9e0":"Ignoring certain inconsistencies we can identify that accident, apocalypse and armagedon are the three trending words posted in a tweet to indicate a fatality. Whereas aftershock, army, annihilation are less trending words used. "}}