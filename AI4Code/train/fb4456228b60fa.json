{"cell_type":{"4d65ac47":"code","330f41d4":"code","2b4c34a4":"code","78200da7":"code","e8312758":"code","bfe14bf1":"code","a37e7563":"code","347a5311":"code","2bcebf94":"code","fd7530a4":"code","cb4c3a87":"code","06898c9d":"code","29573ce0":"code","6e7fdeba":"code","5601496b":"code","2b08b6b3":"code","0c55f8c9":"code","97b92cd6":"code","297caa1e":"code","fbfe4687":"code","e9d132ec":"code","42fbbab2":"code","121d6ec4":"code","2c505e57":"code","d6e8e467":"code","6ff62617":"code","39419b45":"code","4d9fdf66":"markdown","f91c00e3":"markdown","2a934f31":"markdown","f51e6636":"markdown","13214a3a":"markdown","3025e2fb":"markdown","2babbef0":"markdown","79d7d6a2":"markdown","d2d7a0d2":"markdown","f2138190":"markdown","12e5f167":"markdown","bb40d629":"markdown","2b059c71":"markdown","3a032115":"markdown","811b5ef0":"markdown"},"source":{"4d65ac47":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","330f41d4":"# import packages\nimport json\nimport os \nimport tensorflow as tf\nimport sklearn\nimport seaborn as sbs\nimport sklearn.naive_bayes \nimport sklearn.model_selection\nimport sklearn.metrics","2b4c34a4":"json_1 = '..\/input\/news-headlines-dataset-for-sarcasm-detection\/Sarcasm_Headlines_Dataset.json'\njson_2 = '..\/input\/news-headlines-dataset-for-sarcasm-detection\/Sarcasm_Headlines_Dataset_v2.json'\n","78200da7":"def load_json(jfile):\n    data = []\n    with open(jfile) as f:\n        for line in f.readlines():\n            j = json.loads(line)\n            url, headline, sarcastic = j['article_link'], j['headline'], j['is_sarcastic']\n            data.append([url, headline, sarcastic])\n    return pd.DataFrame(data, columns=['article_link', 'headline', 'is_sarcastic'])\n\nprint(\"\u2705\u2705\u2705 SESSION DONE\")","e8312758":"df = load_json(json_1)\ndf2 = load_json(json_2)\ndf2","bfe14bf1":"df.is_sarcastic.value_counts(normalize=True), df.is_sarcastic.value_counts()","a37e7563":"# A birdview of headline length. Seems the majority has a length of 70, that's about 5 to 15 words, which makes sence.\nsbs.distplot(df.headline.str.len())\n","347a5311":"vocab_size = 10000 # max_features \ntokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=vocab_size)\ntokenizer.fit_on_texts(df.headline)\ntrain_inputs = tokenizer.texts_to_sequences(df.headline)\nsbs.distplot([len(l) for l in train_inputs])","2bcebf94":"train_inputs = tf.keras.preprocessing.sequence.pad_sequences(train_inputs, padding='post', maxlen=20)\ntrain_labels = df['is_sarcastic']\n\n# Split data into train \/validation \nX_train, X_val, y_train, y_val = sklearn.model_selection.train_test_split(train_inputs, train_labels, test_size=0.2, random_state=0)\ntrain_inputs[0]","fd7530a4":"nb = sklearn.naive_bayes.MultinomialNB()\nnb.fit(X_train, y_train)","cb4c3a87":"y_preds = nb.predict(X_val)\nprint(f\"Accuracy score\", sklearn.metrics.accuracy_score(y_val, y_preds))\nprint(f\"Classification report\\n\", sklearn.metrics.classification_report(y_val, y_preds))","06898c9d":"max_len = 20\ntext_input = tf.keras.Input(shape=(max_len, ))\nembed_text = tf.keras.layers.Embedding(vocab_size, 128)(text_input)\n\nnet = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32, return_sequences=True))(embed_text)\nnet = tf.keras.layers.GlobalMaxPool1D()(net)\nnet = tf.keras.layers.Dense(64, activation='relu')(net)\nnet = tf.keras.layers.Dropout(0.4)(net)\nnet = tf.keras.layers.Dense(32, activation='relu')(net)\nnet = tf.keras.layers.Dropout(0.4)(net)\n\noutput = tf.keras.layers.Dense(1, activation='sigmoid')(net)\nmodel = tf.keras.models.Model(text_input, output)\nmodel.summary()","29573ce0":"model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n\nmc = tf.keras.callbacks.ModelCheckpoint('model_best.hdf5', monitor='val_accuracy', \n                                        verbose=1, save_best_only=True, mode='max')\nes = tf.keras.callbacks.EarlyStopping(monitor=\"val_accuracy\", mode=\"max\", patience=5)\n    \nhistory = model.fit(X_train, y_train,\n                    epochs=30, batch_size=256, callbacks=[mc, es], \n                    validation_split=0.1, verbose=1)","6e7fdeba":"model = tf.keras.models.load_model('model_best.hdf5')\ny_preds = model.predict(X_val, batch_size=1024).round().astype(int)\nprint(\"Test accracy score\", sklearn.metrics.accuracy_score(y_val, y_preds))","5601496b":"%%time\nimport gensim.downloader as api\n\ndef embed_word_vector(word_index, pretrained='word2vec-google-news-300'):\n    embed_size = 300 # Google news vector is 300-dimensional\n    vector = api.load(pretrained)\n    zeros = [0] * embed_size\n    matrix = np.zeros((vocab_size, embed_size)) \n    \n    for word, i in word_index.items():\n        if i >= vocab_size or word not in vector: continue \n        matrix[i] = vector[word]\n    \n    print(\"Embed word vector completed.\")\n    return matrix","2b08b6b3":"%%time\npretrained = 'glove-wiki-gigaword-300'\nmatrix = embed_word_vector(tokenizer.word_index, pretrained=pretrained)","0c55f8c9":"max_len = 20\ntext_input = tf.keras.Input(shape=(max_len, ))\nembed_text = tf.keras.layers.Embedding(vocab_size, 300, weights=[matrix], trainable=False)(text_input)\n\nnet = tf.keras.layers.Bidirectional(tf.keras.layers.GRU(128, return_sequences=True))(embed_text)\nnet = tf.keras.layers.GlobalMaxPool1D()(net)\nnet = tf.keras.layers.Dense(64, activation='relu')(net)\nnet = tf.keras.layers.Dropout(0.4)(net)\nnet = tf.keras.layers.Dense(32, activation='relu')(net)\nnet = tf.keras.layers.Dropout(0.4)(net)\n\noutput = tf.keras.layers.Dense(1, activation='sigmoid')(net)\nmodel = tf.keras.models.Model(text_input, output)\nmodel.summary()","97b92cd6":"model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n\nmc = tf.keras.callbacks.ModelCheckpoint('model_best_embed.hdf5', monitor='val_accuracy', \n                                        verbose=1, save_best_only=True, mode='max')\nes = tf.keras.callbacks.EarlyStopping(monitor=\"val_accuracy\", mode=\"max\", patience=5)\n    \nhistory = model.fit(X_train, y_train,\n                    epochs=30, batch_size=256, callbacks=[mc, es], \n                    validation_split=0.1, verbose=1)","297caa1e":"model = tf.keras.models.load_model('model_best_embed.hdf5')\ny_preds = model.predict(X_val, batch_size=1024).round().astype(int)\nprint(\"Test accracy score\", sklearn.metrics.accuracy_score(y_val, y_preds))","fbfe4687":"!wget --quiet https:\/\/raw.githubusercontent.com\/tensorflow\/models\/master\/official\/nlp\/bert\/tokenization.py","e9d132ec":"%%time\nimport tensorflow_hub as hub \nimport tokenization\n\nmodule_url = 'https:\/\/tfhub.dev\/tensorflow\/bert_en_uncased_L-12_H-768_A-12\/2'\nbert_layer = hub.KerasLayer(module_url, trainable=True)","42fbbab2":"vocab_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()\ndo_lower_case = bert_layer.resolved_object.do_lower_case.numpy()\ntokenizer = tokenization.FullTokenizer(vocab_file, do_lower_case)\n","121d6ec4":"def bert_encode(texts, tokenizer, max_len=512):\n    all_tokens = []\n    all_masks = []\n    all_segments = []\n    \n    for text in texts:\n        text = tokenizer.tokenize(text)\n            \n        text = text[:max_len-2]\n        input_sequence = [\"[CLS]\"] + text + [\"[SEP]\"]\n        pad_len = max_len - len(input_sequence)\n        \n        tokens = tokenizer.convert_tokens_to_ids(input_sequence) + [0] * pad_len\n        pad_masks = [1] * len(input_sequence) + [0] * pad_len\n        segment_ids = [0] * max_len\n        \n        all_tokens.append(tokens)\n        all_masks.append(pad_masks)\n        all_segments.append(segment_ids)\n    \n    return np.array(all_tokens), np.array(all_masks), np.array(all_segments)\n\ndef build_model(bert_layer, max_len=512):\n    input_word_ids = tf.keras.Input(shape=(max_len,), dtype=tf.int32, name=\"input_word_ids\")\n    input_mask = tf.keras.Input(shape=(max_len,), dtype=tf.int32, name=\"input_mask\")\n    segment_ids = tf.keras.Input(shape=(max_len,), dtype=tf.int32, name=\"segment_ids\")\n\n    pooled_output, sequence_output = bert_layer([input_word_ids, input_mask, segment_ids])\n    clf_output = sequence_output[:, 0, :]\n    net = tf.keras.layers.Dense(64, activation='relu')(clf_output)\n    net = tf.keras.layers.Dropout(0.2)(net)\n    net = tf.keras.layers.Dense(32, activation='relu')(net)\n    net = tf.keras.layers.Dropout(0.2)(net)\n    out = tf.keras.layers.Dense(1, activation='sigmoid')(net)\n    \n    model = tf.keras.models.Model(inputs=[input_word_ids, input_mask, segment_ids], outputs=out)\n    model.compile(tf.keras.optimizers.Adam(lr=1e-5), loss='binary_crossentropy', metrics=['accuracy'])\n    \n    return model","2c505e57":"max_len = 100\nX_train, X_val, y_train, y_val = sklearn.model_selection.train_test_split(df.headline, df['is_sarcastic'], \n                                                                          test_size=0.1, random_state=0)\nX_train = bert_encode(X_train, tokenizer, max_len=max_len)\nX_val = bert_encode(X_val, tokenizer, max_len=max_len)","d6e8e467":"model = build_model(bert_layer, max_len=max_len)\nmodel.summary()","6ff62617":"%%time\ncheckpoint = tf.keras.callbacks.ModelCheckpoint('model.h5', monitor='val_accuracy', save_best_only=True, verbose=1)\nearlystopping = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=3, verbose=1)\n\ntrain_history = model.fit(\n    X_train, y_train, \n    validation_split=0.1,\n    epochs=3,\n    callbacks=[checkpoint, earlystopping],\n    batch_size=16,\n    verbose=1\n)","39419b45":"%%time\nmodel.load_weights('model.h5')\ny_preds = model.predict(X_val).round().astype(int)\nprint(\"Validation accuracy: \", sklearn.metrics.accuracy_score(y_val, y_preds))","4d9fdf66":"See, the above graph confirms our guess: most headline has 5 - 15 words. \n","f91c00e3":"Remember in previous sections, we have explored on the headline length. Most of them has a length between 50 to 150, and the majority has a length of 70. We set the length to 120.","2a934f31":"Load data into `Pandas.DataFrame`","f51e6636":"# WordVector Embedding + LSTM\n\nWe now use pretrained wordvector https:\/\/code.google.com\/archive\/p\/word2vec\/ of Google News (after all, we're dealing with article headlines). \n\nThe vectors trained on a part of the Google News dataset (about 100 billion words). The model contains 300-dimensional vectors for 3 million words and phrases. The phrases were obtained using a simple data-driven approach described in 'Distributed Representations of Words and Phrases and their Compositionality' (https:\/\/code.google.com\/archive\/p\/word2vec\/).","13214a3a":"# BERT, NLP text classification killer","3025e2fb":"Re run our model to see if any improvements we got.","2babbef0":"The data is well-balanced, which is rare in reality but great for our practicing purpose.\n\nNow we will try Naive Bayes as our base model.\n\n## Building Base Model: Naive Bayes\nFirst step, we should tokenize texts into vectors.","79d7d6a2":"Now we can see, the result is better. The validation accuracy has improved from `0.58` to `0.8521`, which is not ideal, but good enough to detect sarcasm by reading a headline of an article.\n\nA close view will also finds out that this model is apparrenly overfitting our dataset. We will also do experiments on word embedding and BERT later to seen any improvement of our model. ","d2d7a0d2":"# Another model: Keras LSTM","f2138190":"From the training history, we can see performance of this model is pretty similiar to our previous one. Thus, we know pretrained NLP models are not going help us much. We have to find other ways to improve the score.","12e5f167":"* The validation accuracy is `92%`, impressive !\n\nThe result can be further improved by: \n1. increasing the length of `max_len`, i.e., the length of input headline length \n2. fine-tuning parameters \n3. trying other pretrained bert modules.\n\nTo summarize, BERT is definitely the winner of text classification tasks.","bb40d629":"Now check the distribution (ratio) of our target `is_sarcastic`.","2b059c71":"Let's build the model and train on it.","3a032115":"Update embedding layer weights of our model","811b5ef0":"Validate how the model performs. \n\nTurs out that the `accuracy_score` is only 0.58, which is merely a little better than random guess.\nBut at least wew know what to expect now. We're going to try more powerful models, e.g., CNN, to see whether we can reach a `accuracy_score = 0.7` or higher."}}