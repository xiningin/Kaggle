{"cell_type":{"bd61626e":"code","deede69b":"code","225d0cb2":"code","a4c42128":"code","0a35a40a":"code","7cd63da2":"code","6a898e06":"code","a1a7acae":"code","96edba44":"code","e03d085f":"code","2a9af120":"code","8ce95fa2":"code","3fe43f63":"code","61432cce":"code","8f328c23":"code","76da0fc3":"code","d346073a":"code","c632e872":"code","59397d74":"code","44892e1d":"code","1b211496":"code","f35ac7d7":"code","020718a5":"code","b85b40f8":"code","80211392":"code","fab6e11b":"code","361a5754":"code","76f05b03":"code","c28a6bef":"code","d0054962":"code","6b43a706":"code","28959634":"code","b2211011":"code","428b76a3":"markdown","e9a6e1f3":"markdown","d0d5269f":"markdown","e1083e55":"markdown","dfb4e822":"markdown","196b2d98":"markdown","e6f116dc":"markdown","6865a6a8":"markdown","fe32f63e":"markdown","cc0b52f8":"markdown"},"source":{"bd61626e":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport numpy as np\nfrom fastai.vision import Module\nimport matplotlib.pyplot as plt\nfrom matplotlib import animation, rc\nfrom IPython.display import HTML","deede69b":"# Defining the model\nclass LinearRegression(Module):\n    def __init__(self, number_of_inputs, number_of_outputs):\n        self.linear = nn.Linear(number_of_inputs, number_of_outputs)\n        \n    def forward(self, x):\n        return self.linear(x) ","225d0cb2":"# Defining the fit function\ndef fit(inputs, targets, model, criterion, optimizer, num_epochs):\n    loss_history = [] # to save the loss at each epoch.\n    \n    for epoch in range(num_epochs):\n        # forward pass\n        out = model(inputs)          \n        loss = criterion(out, targets) \n\n        # backward pass\n        optimizer.zero_grad() \n        loss.backward()\n        optimizer.step()\n        \n        # store value of loss\n        loss_history.append(loss.item())\n        \n    print('Epoch[{}\/{}], loss:{:.6f}'.format(epoch+1, num_epochs, loss.item()))\n    return loss_history","a4c42128":"# Model, Criterion and Optimizer\nmodel = LinearRegression(1,1)\ncriterion = nn.MSELoss()\noptimizer = optim.Adam(model.parameters(), lr=0.1) ","0a35a40a":"# Create data\nx_train = torch.linspace(0, 1, 10000) \ny_train = 2*x_train + 1 + torch.randn(x_train.size())*x_train*0.1 \nx_train = x_train.unsqueeze(-1)\ny_train = y_train.unsqueeze(-1)","7cd63da2":"# Visualize our data\nplt.figure(figsize=(6,3), dpi=120)\nplt.scatter(x_train.numpy(), y_train.numpy(), color='gray', label='Noisy data (model inputs)') \nplt.plot(x_train.numpy(), (2*x_train + 1).numpy(), color='black', label='True data')\nplt.xlabel('x')\nplt.ylabel('y = 2x + 1 + noise')\nplt.legend()\nplt.show();\nprint(x_train.size(), y_train.size()) ","6a898e06":"%time # Measure the time of execussion\nloss = fit(x_train.requires_grad_(True), \n           y_train, \n           model, \n           criterion, \n           optimizer, \n           num_epochs=200)","a1a7acae":"plt.figure(figsize=(6,3), dpi=120)\nplt.plot(loss, color='gray')\nplt.xlabel('epoch')\nplt.ylabel('Loss')\nplt.show();","96edba44":"list(model.parameters())","e03d085f":"# Compute model estimates\nmodel.eval()        # set model to evaluation mode\nye = model(x_train) # compute the y estimate","2a9af120":"# Visualize our data\nplt.figure(figsize=(6,3), dpi=120)\nplt.scatter(x_train.detach().numpy(), y_train.numpy(), color='gray', label='Noisy data (model inputs)') \nplt.plot(x_train.detach().numpy(), (2*x_train + 1).detach().numpy(), color='black', label='True data')\nplt.plot(x_train.detach().numpy(), ye.detach().numpy(), color='blue', label='Model estimate')\nplt.xlabel('x')\nplt.ylabel('y = 2x + 1 + noise')\nplt.legend()\nplt.show();","8ce95fa2":"x_true = torch.linspace(-2, 2, 1000)\ny_true = 3*x_true**2 + 2*x_true + 1\ny_train =  y_true + torch.randn(x_true.size())\n\nx_true.unsqueeze_(-1)\ny_train.unsqueeze_(-1)\n\nx_train = torch.cat((x_true**2, x_true), dim=1) # Concatenate x and x**2 to a N x 2 x_train matrix.\nprint(x_train.size(), y_train.size()) ","3fe43f63":"# Visualize our data\nplt.figure(figsize=(6,3), dpi=120)\nplt.scatter(x_true.numpy(), y_train.numpy(), color='gray', label='Observed data') \nplt.plot(x_true.numpy(), y_true.numpy(), color='black', label='True data')\nplt.xlabel('x')\nplt.ylabel(r'$y = 3x^2 + 2x + 1 + noise$') # Notice you can use latex in the label string\nplt.legend()\nplt.show();","61432cce":"model = LinearRegression(2, 1)\ncriterion = nn.MSELoss()\noptimizer = optim.Adam(model.parameters(), lr=0.1) ","8f328c23":"%time \nloss = fit(x_train, y_train, model, criterion, optimizer, num_epochs=250)","76da0fc3":"list(model.parameters())","d346073a":"model.eval()\nye = model(x_train) # compute the y estimate\nye = ye.detach().numpy() # get the values from the variable, them pass them to the cpu and convert to a numpy array","c632e872":"# Visualize our data\nplt.figure(figsize=(6,3), dpi=120)\nplt.scatter(x_true.numpy(), y_train.numpy(), color='gray', label='Observed data') \nplt.plot(x_true.numpy(), y_true.numpy(), color='black', label='True data')\nplt.plot(x_true.numpy(), ye, color='blue', label='Predicted data')\nplt.xlabel('x')\nplt.ylabel(r'$y = 3x^2 + 2x + 1 + \\epsilon$') # Notice you can use latex in the label string\nplt.legend()\nplt.show();","59397d74":"def fit2(inputs, targets, model, criterion, optimizer, num_epochs):\n    loss_history = [] # to save the loss at each epoch.\n    out_history = [] # to save the parameters at each epoch\n    for ii, epoch in enumerate(range(num_epochs)):\n        # forward\n        out = model(inputs)\n        loss = criterion(out, targets)\n\n        # backward\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        \n        loss_history.append(loss.item())\n        \n        if ii == 0:\n            out_history = out.detach().numpy()\n        else:\n            out_history = np.concatenate((out_history, out.detach().numpy()), axis=-1)\n        \n    print('Epoch[{}\/{}], loss:{:.6f}'.format(epoch+1, num_epochs, loss.item()))\n    return loss_history, out_history","44892e1d":"model = LinearRegression(2, 1)\ncriterion = nn.MSELoss()\noptimizer = optim.Adam(model.parameters(), lr=0.1) ","1b211496":"%time \nloss, out = fit2(x_train, y_train, model, criterion, optimizer, num_epochs=250)","f35ac7d7":"%%capture\n# First set up the figure, the axis, and the plot element we want to animate\nfig, ax = plt.subplots(figsize=(6,3), dpi=120)\nax.set_xlim((-2.2, 2.2))\nax.set_ylim((-5, 20))\nax.plot(x_true.numpy(), y_true.numpy(), lw=2, color='black', label='True model')\nax.scatter(x_true.numpy(), y_train.numpy(), color='gray', label='Observed data') \nax.set_ylabel(r'$y = 3x^2 + 2x + 1 + noise$') # Notice you can use latex in the label string\nline, = ax.plot([], [], lw=2, label='Predicted model')\nax.legend()\n\n# animation function. This is called sequentially\ndef animate(i):\n    line.set_data(x_true.numpy(), out[...,i])\n    return (line,)\n\n# call the animator. blit=True means only re-draw the parts that have changed.\nanim = animation.FuncAnimation(fig, animate, frames=out.shape[1], interval=30, blit=True)","020718a5":"HTML(anim.to_html5_video())","b85b40f8":"x_train = torch.linspace(-2, 2, 1000)\nx_train.unsqueeze_(-1)\ny_true = 6*x_train**3 + 3*x_train**2 + 2*x_train + 1\ny_train =  y_true + torch.randn(x_train.size())*5\n\nprint(x_train.size(), y_train.size()) ","80211392":"# Visualize our data\nplt.figure(figsize=(6,3), dpi=120)\nplt.scatter(x_train.numpy(), y_train.numpy(), color='gray', label='Observed data') \nplt.plot(x_train.numpy(), y_true.numpy(), color='black', label='True model')\nplt.xlabel('time')\nplt.ylabel('y') # Notice you can use latex in the label string\nplt.legend()\nplt.show();","fab6e11b":"class GeneralFit(Module):\n    def __init__(self, input_size, output_size, hidden_size=100):\n        self.linear_in  = nn.Linear(input_size, hidden_size)\n        self.hidden     = nn.Linear(hidden_size, hidden_size)\n        self.linear_out = nn.Linear(hidden_size, output_size)\n        \n    def forward(self, x):\n        x = torch.relu(self.linear_in(x))\n        x = torch.relu(self.hidden(x))\n        x = self.linear_out(x)\n        return x","361a5754":"model = GeneralFit(1, 1)\ncriterion = nn.MSELoss() # Tentar tamb\u00e9m L1loss\noptimizer = optim.Adam(model.parameters(), lr=0.01)","76f05b03":"%time \nloss, out = fit2(x_train.requires_grad_(True),\n                y_train, \n                model, \n                criterion, \n                optimizer, \n                num_epochs=200)","c28a6bef":"plt.figure(figsize=(6,3), dpi=120)\nplt.plot(loss, color='gray')\nplt.xlabel('epoch')\nplt.ylabel('Loss')\nplt.show();","d0054962":"ye = model(x_train) # compute the y estimate\nye = ye.detach().numpy() # get the values from the variable, them pass them to the cpu and convert to a numpy array","6b43a706":"# Visualize our data\nplt.figure(figsize=(6,3), dpi=120)\nplt.scatter(x_train.detach().numpy(), y_train.numpy(), color='gray', label='Observed data') \nplt.plot(x_train.detach().numpy(), y_true.numpy(), color='black', label='True model')\nplt.plot(x_train.detach().numpy(), ye, color='blue', label='Predicted model')\nplt.xlabel('time')\nplt.ylabel('y')\nplt.legend()\nplt.show();","28959634":"%%capture\n# First set up the figure, the axis, and the plot element we want to animate\nfig, ax = plt.subplots(figsize=(6,3), dpi=120)\n#ax.set_xlim((-2.2, 2.2))\n#ax.set_ylim((-5, 20))\nax.plot(x_train.detach().numpy(), y_true.numpy(), lw=2, color='black', label='True model')\nax.scatter(x_train.detach().numpy(), y_train.numpy(), color='gray', label='Observed data') \nax.set_ylabel('y') # Notice you can use latex in the label string\nline, = ax.plot([], [], lw=2, label='Predicted model')\nax.legend()\n\n# animation function. This is called sequentially\ndef animate(i):\n    line.set_data(x_train.detach().numpy(), out[...,i])\n    return (line,)\n\n# call the animator. blit=True means only re-draw the parts that have changed.\nanim = animation.FuncAnimation(fig, animate, frames=out.shape[1], interval=30, blit=True)","b2211011":"HTML(anim.to_html5_video())","428b76a3":"## 1. Linear Regression in\u00a0PyTorch","e9a6e1f3":"**If you want to receive updates consider joining my mailing list at [learn-ai-today.com](http:\/\/learn-ai-today.com)**","d0d5269f":"# Note: The full description of the code and steps is available on [Medium at this link](https:\/\/towardsdatascience.com\/learn-ai-today-01-getting-started-with-pytorch-2e3ba25a518#c476-ac019ce53dce). ","e1083e55":"**What you will learn:**\n* How to Create a PyTorch Model\n* How to Train Your Model\n* How the Learning Rate Affects the Training\n* Visualize the Training Prgress Dynamically","dfb4e822":"## 2. Stepping Up to Polynomial Regression","196b2d98":"## 3. Neural Network\u00a0Model","e6f116dc":"# Homework\nI can show you a thousand examples but you will learn more if you can make one or two experiments by yourself! The complete code for these experiments that I showed you are available on a Jupyter notebook in this repository.\n* Try to play with the learning rate, number of epochs, number of hidden layers and the size of the hidden layers;\n* Try also SGD optimizer and play with the learning rate and maybe also with the momentum (I didn\u2019t cover it in this story but now that you know about it you can do some research);\n\nIf you create interesting notebooks with nice animations as a result of your experiments, go ahead and share it on GitHub, Kaggle or write a Medium story about it!","6865a6a8":"The **out** variable stored the output for every epoch. Now it's time to plot and animate.\n\nStarting a cell with **%%caputre** prevents it from displaying the output, this will be useful in the next cell so we can suppress the plot from displaying since we will show the animation in the cell that follows.","fe32f63e":"# Learn AI Today: 01 - Getting started with Pytorch","cc0b52f8":"Now we show the animation."}}