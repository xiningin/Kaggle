{"cell_type":{"75e155da":"code","00fa5361":"code","16f64c60":"code","478e9350":"code","6e1c808c":"code","def022f4":"code","185ec8c5":"code","36117bc7":"code","574e442d":"code","78b3cee9":"code","7bd4ac91":"code","dcc731ea":"code","737e5168":"code","f883a230":"code","1173a0c9":"code","4e0bc77f":"code","851cf404":"code","8072fbfd":"code","4e878fd5":"code","42872fa6":"code","3e18f5b7":"code","ac65d020":"code","7ac16224":"code","87e61adb":"code","6071ccf8":"code","5a3df24d":"code","65f52013":"code","da125cca":"code","cc2c5db1":"code","45c3d638":"markdown","af6bc527":"markdown","eb2d578e":"markdown","83440cd9":"markdown","7988b5e8":"markdown","a369277b":"markdown","0a497083":"markdown","e05af4cd":"markdown","e413301c":"markdown","a52d5079":"markdown","d0ba679f":"markdown","01a45adc":"markdown","9131c188":"markdown","38362e4c":"markdown","8f85b9c4":"markdown","0bdf879e":"markdown","a91ab11e":"markdown","ecca9567":"markdown","9a17cfa8":"markdown","c1c01467":"markdown","6e897d15":"markdown","cf291610":"markdown"},"source":{"75e155da":"!pip install ..\/input\/datatable0110\/datatable-0.11.0-cp37-cp37m-manylinux2010_x86_64.whl","00fa5361":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","16f64c60":"import os\nimport datatable as dt\nimport janestreet\nimport xgboost as xgb\nimport lightgbm as lgbm\nfrom sklearn.metrics import make_scorer\n\nfrom sklearn import ensemble \nfrom sklearn import metrics\nfrom sklearn import model_selection\nfrom sklearn.model_selection import cross_val_score,StratifiedKFold\nfrom sklearn import decomposition\nfrom sklearn import preprocessing \nfrom sklearn import pipeline\nfrom skopt import gp_minimize\nfrom functools import partial\nfrom skopt import space \nfrom hyperopt import hp,fmin,tpe,Trials\nfrom hyperopt.pyll.base import scope \nimport lightgbm as lgbm\n\nfrom catboost import CatBoostClassifier, Pool\nimport torch","478e9350":"## Thanks to this kernal -  https:\/\/www.kaggle.com\/eikedehling\/tune-and-compare-xgb-lightgbm-rf-with-hyperopt\n\ndef gini(truth, predictions):\n    g = np.asarray(np.c_[truth, predictions, np.arange(len(truth)) ], dtype=np.float)\n    g = g[np.lexsort((g[:,2], -1*g[:,1]))]\n    gs = g[:,0].cumsum().sum() \/ g[:,0].sum()\n    gs -= (len(truth) + 1) \/ 2.\n    return gs \/ len(truth)\n\ndef gini_xgb(predictions, truth):\n    truth = truth.get_label()\n    return 'gini', -1.0 * gini(truth, predictions) \/ gini(truth, truth)\n\ndef gini_lgb(truth, predictions):\n    score = gini(truth, predictions) \/ gini(truth, truth)\n    return 'gini', score, True\n\ndef gini_sklearn(truth, predictions):\n    return gini(truth, predictions) \/ gini(truth, truth)\n\ngini_scorer = make_scorer(gini_sklearn, greater_is_better=True, needs_proba=True)\n\n","6e1c808c":"%%time\ntrain = dt.fread('\/kaggle\/input\/jane-street-market-prediction\/train.csv').to_pandas()","def022f4":"train.head()","185ec8c5":"#train = train[1590491:]","36117bc7":"train['action'] = (train['resp'] > 0).astype('int')\nfeatures = [c for c in train.columns if 'feature' in c] + ['weight']\nX_Train = train.loc[:, features]\ny_train = train.loc[:, 'action']","574e442d":"final_df = pd.DataFrame()","78b3cee9":"final_df['ts_id'] = train['ts_id']\nfinal_df['y_Act'] = train['action']\nfinal_df.loc[:,\"k_fold\"]=-1","7bd4ac91":"params = dict(\n    objective='binary:logistic',\n    max_depth=8,\n    learning_rate=0.01,\n    subsample=0.9,\n    colsample_bytree=0.9,\n    random_state=42,\n    tree_method='gpu_hist')","dcc731ea":"dtrain = xgb.DMatrix(X_Train, y_train)\nxg_boost_clf = xgb.train(params, dtrain, num_boost_round=500)","737e5168":"from sklearn.model_selection import train_test_split","f883a230":"X_train, X_test, y_train, y_test = train_test_split(X_Train, y_train, random_state=42, test_size=0.2)","1173a0c9":"X_train = X_train.fillna(-999)\nX_test = X_test.fillna(-999)","4e0bc77f":"from xgboost import XGBClassifier\n\nfrom sklearn.metrics import accuracy_score\n\nimport optuna\nfrom optuna.samplers import TPESampler","851cf404":"sampler = TPESampler(seed=666)\n\ndef create_model(trial):\n    max_depth = trial.suggest_int(\"max_depth\", 2, 9)\n    n_estimators = trial.suggest_int(\"n_estimators\", 2, 500)\n    learning_rate = trial.suggest_uniform('learning_rate', 0.0001, 0.99)\n    subsample = trial.suggest_uniform('subsample', 0.0001, 1.0)\n    \n    model = XGBClassifier(\n        n_estimators=n_estimators, \n        max_depth=max_depth, \n        learning_rate=learning_rate,\n        subsample=subsample,\n        random_state=42,\n        tree_method='gpu_hist'\n    )\n    return model\n\ndef objective(trial):\n    model = create_model(trial)\n    model.fit(X_train, y_train)\n    score = accuracy_score(\n        y_test, \n        model.predict(X_test)\n    )\n    return score","8072fbfd":"#study = optuna.create_study(direction=\"maximize\", sampler=sampler)\n#study.optimize(objective, n_trials=70)\n#param_xgb = study.best_params\n#param_xgb['random_state'] = 42\n#param_xgb['tree_method'] = 'gpu_hist'","4e878fd5":"param_xgb = {'max_depth': 8, 'n_estimators': 431, 'learning_rate': 0.18307961625782698, 'subsample': 0.9693510180089355,'random_state':42,'tree_method':'gpu_hist'}","42872fa6":"model_xgb = XGBClassifier(**param_xgb)\nmodel_xgb.fit(X_train, y_train)\nprint('XGB score: ', accuracy_score(y_test, model_xgb.predict(X_test)))","3e18f5b7":"params = {\n    'loss_function': 'Logloss',\n    'eval_metric': 'AUC',\n    'task_type': 'GPU' if torch.cuda.is_available() else 'CPU',\n    'grow_policy': 'Lossguide',\n    'iterations': 2000,\n    'learning_rate': 0.08,\n    'random_seed': 0,\n    'l2_leaf_reg': 10,\n    'depth': 10,\n    'max_leaves': 5,\n    'border_count': 128,\n    'verbose': 50,\n}","ac65d020":"train_set = Pool(X_train.values, label = y_train.values)\nval_set = Pool(X_test.values, label = y_test.values)\n","7ac16224":"model_catboost = CatBoostClassifier(**params)","87e61adb":"model_catboost.fit(train_set,eval_set=val_set,use_best_model=True)","6071ccf8":"# Hyper-params taken from https:\/\/www.kaggle.com\/lasmith\/house-price-regression-with-lightgbm#Light-GBM\n\nparam_lgbm = {\n    'task': 'train',\n    'boosting_type': 'gbdt',\n    'objective': 'binary',\n    'metric': ['AUC'],\n    'learning_rate': 0.005,\n    'feature_fraction': 0.9,\n    'bagging_fraction': 0.7,\n    'bagging_freq': 10,\n    'verbose': 0,\n    \"max_depth\": 8,\n    \"num_leaves\": 128,  \n    \"max_bin\": 512,\n    \"num_iterations\": 200,\n    \"n_estimators\": 100,\n    'gpu_use_dp':True\n}","5a3df24d":"model_lgbm = lgbm.LGBMClassifier(**param_lgbm)","65f52013":"model_lgbm.fit(X_train,y_train,eval_set=[(X_test,y_test)],eval_metric='auc',early_stopping_rounds=100)","da125cca":"try:\n    env = janestreet.make_env()\nexcept:\n    pass\niter_test = env.iter_test()\nprior_test_df = None\n","cc2c5db1":"%%time\nfor (test_df, sample_prediction_df) in iter_test:\n    X_test = test_df[features]\n    # For the rest, fill with -999\n    X_test.fillna(-999, inplace = True)\n    #y_preds = model.predict(X_test)\n    y_preds = ( 0.75 * model_lgbm.predict(X_test) + 0.15 * model_xgb.predict(X_test) + 0.10 * model_catboost.predict(X_test))\n    sample_prediction_df.action = y_preds\n    env.predict(sample_prediction_df)","45c3d638":"# Inference","af6bc527":"# Jane Street Market Prediction","eb2d578e":"### Below is the Optimal param for XGBoost from Optuna ","83440cd9":"# Introduction ","7988b5e8":"<font color=\"red\" size=3>Please upvote this kernel if you like it. It motivates me to create kernal with great content  :) <\/font>","a369277b":"## Let see how the sample data looks like","0a497083":"## Hyperparameter tuning with Optuna ","e05af4cd":"# Load and preprocess CSV with datatable","e413301c":"# Model -2 Catboost\n\n#### Thanks to shinomoriaoshi -  https:\/\/www.kaggle.com\/shinomoriaoshi\/janestreet-catboost-baseline Do Upvote ","a52d5079":"## Baseline Model ","d0ba679f":"# Model 1 - Fit ","01a45adc":"### Jane Street has spent decades developing their own trading models and machine learning solutions to identify profitable opportunities and quickly decide whether to execute trades. These models help Jane Street trade thousands of financial products each day across 200 trading venues around the world.\n\n### Admittedly, this challenge far oversimplifies the depth of the quantitative problems Jane Streeters work on daily, and Jane Street is happy with the performance of its existing trading model for this particular question. However, there\u2019s nothing like a good puzzle, and this challenge will hopefully serve as a fun introduction to a type of data science problem that a Jane Streeter might tackle on a daily basis. Jane Street looks forward to seeing the new and creative approaches the Kaggle community will take to solve this trading challenge.","9131c188":"# Conclusion \n\nThis notebook explains the Weighted Average Ensembling with Xgboost,lightgbm and Catboost with Hyperprama tuning with Optuna\n\nFuture work :- \n\n* Model stacking\n* Feature engineering ","38362e4c":"### Weighted Average Ensembling ","8f85b9c4":"### Model 2 - Fit","0bdf879e":"# Prepare data and train xgboost on GPU","a91ab11e":"# Intializing libraries","ecca9567":"# Optuna \n\n#### Optuna is an automatic hyperparameter optimization software framework, particularly designed for machine learning. It features an imperative, define-by-run style user API. Thanks to our define-by-run API, the code written with Optuna enjoys high modularity, and the user of Optuna can dynamically construct the search spaces for the hyperparameters.\n\n[What is Optuna](https:\/\/github.com\/optuna\/optuna)","9a17cfa8":"### Consider last 800,000 records for ease of model building, While submitting prediction consider full dataset","c1c01467":"## <a name=\"dataset_description\">Mission of this kernal<\/a>: \n\n### This kernal won't you to beat the leaderbord , It helps you to understand in implementation of Ensembling of gradient boosting classifer model. Hyperparameter tuning with Optuna ","6e897d15":"## Ensembling and Prediction Submission","cf291610":"# Model 3 - Lightgbm"}}