{"cell_type":{"7e855082":"code","8d259754":"code","1f0e5e53":"code","deb590a6":"code","603ccd2b":"code","72f7261f":"code","e03c0905":"code","bc0c1a63":"code","3245c1bc":"code","0cf277a9":"code","24fd0d0c":"code","4ef7a455":"code","cd0af3a7":"code","9d4cef95":"code","f09f95fb":"code","8dcff61c":"code","6aad145e":"code","6c687bfd":"code","f6cfba81":"code","fddce706":"code","16d9a27e":"code","04cd9f33":"code","62a296cb":"code","bbfb85b4":"code","4567efc2":"code","6edb067c":"code","c362114a":"code","855f41d9":"code","81549351":"markdown","dfc48663":"markdown","8f2cb03e":"markdown","ca6ba9ed":"markdown","3d4e1887":"markdown","6358ab27":"markdown","412e9f3c":"markdown","f8bd2d29":"markdown","e3882d13":"markdown","c394e979":"markdown","72d20642":"markdown","9046051f":"markdown","695d5f3a":"markdown","87b70b1d":"markdown","a590fc58":"markdown","2e425047":"markdown","fcb0a421":"markdown","bff6f5c6":"markdown","18e07760":"markdown","c22f791f":"markdown","ba7e8948":"markdown","97ab5c84":"markdown","ae4d3c50":"markdown","cf552983":"markdown","0db9f251":"markdown","c07ff631":"markdown","27ea327c":"markdown","73b8b661":"markdown","be2c144f":"markdown","1ab51538":"markdown","22443581":"markdown"},"source":{"7e855082":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt #plottting\nimport seaborn as sns # more plotting\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","8d259754":"# Load in the test and training data and then check the top 5 rows of each\ntrain_data = pd.read_csv(\"\/kaggle\/input\/titanic\/train.csv\")\ntest_data = pd.read_csv(\"\/kaggle\/input\/titanic\/test.csv\")\ntrain_data.head()","1f0e5e53":"test_data.head()","deb590a6":"# Print the features that are available in the data\nprint(\"# of training Rows = \", len(train_data))\nprint(\"# of testing Rows = \", len(test_data))\nprint()\nprint(\"Train Features = \", train_data.columns.values)\nprint(\"Test Features = \", test_data.columns.values)\nprint()\n\n# Check which, if any, features contain NaNs and how many they contain\nprint(\"NaNs in each training Feature\")\nprint(train_data.isnull().sum())\nprint()\nprint(\"NaNs in each testing Feature\")\nprint(test_data.isnull().sum())","603ccd2b":"# Visualise the numerical values\ntrain_data.hist(figsize=(12,12), color='darkblue', bins=25)\nplt.show()\n\n# Visualise the relevent categorical text-based features\ntrain_data['Sex'].value_counts().plot(kind='bar', rot=0.0, fontsize=16, color='darkblue', title='Sex')\nplt.show()\ntrain_data['Embarked'].value_counts().plot(kind='bar', rot=0.0, fontsize=16, color='darkblue', title='Embarked')\nplt.show()","72f7261f":"# Check the correlations between the non-text variables\nsns.heatmap(train_data.corr(), annot = True, cmap='viridis')","e03c0905":"# Make a list of the continuous and categorical features to plot\ncontinuous_keys = ['Age', 'Fare']\ncategorical_keys = ['Survived', 'Pclass', 'SibSp', 'Parch']\n\n# Iterate through each feature and plot\nfor i in continuous_keys:\n    sns.catplot(x = 'Embarked',y=i, data = train_data)\nfor i in categorical_keys:\n    sns.catplot(x = 'Embarked',y=i, kind='bar', data = train_data)","bc0c1a63":"# Now repeat the above for the Sex feature\nfor i in continuous_keys:\n    sns.catplot(x = 'Sex',y=i, data = train_data)\nfor i in categorical_keys:\n    sns.catplot(x = 'Sex',y=i, kind='bar', data = train_data)","3245c1bc":"# Just to check Embarked against Sex\nsns.countplot(x = 'Embarked', data = train_data, hue = 'Sex', color = 'blue')","0cf277a9":"# Drop the Cabin feature from the dataset\ntrain_data.drop(columns='Cabin', inplace=True)\n\n# Drop Ticket feature\ntrain_data.drop(columns='Ticket', inplace=True)","24fd0d0c":"from collections import Counter\n\ndef detect_outliers(df,n,features):\n\n    outlier_indices = []\n    for col in features:\n        Q1 = np.percentile(df[col], 25) # 1st quartile (25%)\n        Q3 = np.percentile(df[col],75) # 3rd quartile (75%)\n        IQR = Q3 - Q1 # Interquartile range (IQR)\n        outlier_step = 1.5 * IQR # outlier step\n        \n        # Determine a list of indices of outliers for feature col\n        outlier_list_col = df[(df[col] < Q1 - outlier_step) | (df[col] > Q3 + outlier_step )].index\n        \n        # append the found outlier indices for col to the list of outlier indices \n        outlier_indices.extend(outlier_list_col)\n        \n    # select observations containing more than 2 outliers\n    outlier_indices = Counter(outlier_indices)        \n    multiple_outliers = list( k for k, v in outlier_indices.items() if v > n )\n    \n    return multiple_outliers   ","4ef7a455":"# Fill in the missing Embarked information\ntrain_data['Embarked'].fillna('S', inplace=True)","cd0af3a7":"# Calculate the mean of the Age feature\nmeanAges = train_data.groupby(['Pclass'])['Age'].mean()\nprint(meanAges)","9d4cef95":"# Fill in the missing age values\nfor i in range(3):\n    train_data.loc[train_data['Pclass']==int(i+1), 'Age'] = train_data.loc[train_data['Pclass']==int(i+1), 'Age'].fillna(meanAges.values[i])\n\n# Check everything has been filled as expected\nprint(train_data.isnull().sum())\n\n# And plot to check the values have been filled as expected\nplt.figure()\nplt.hist(train_data['Age'].values, bins=20, color=\"orange\", ec='black', label=\"New Total Age Dist\", density=True)\nplt.xlabel(\"Age\")","f09f95fb":"# Create a new column in the data frame\ntrain_data['newGender'] = train_data['Sex']\n\n# Find all the children\ntrain_data.loc[train_data['Age']<16., 'newGender'] = 'child'\n\n# Drop the Sex feature as it is not needed anymore\ntrain_data.drop(columns='Sex', inplace=True)\n\nprint(train_data.head(8))","8dcff61c":"train_data['Title'] = train_data['Name']\n\n# Extracting Title\nfor name_string in train_data['Name']:\n    train_data['Title'] = train_data['Name'].str.extract('([A-Za-z]+)\\.', expand=True)\n\n# Replacing some titles using a mapping\nmapping = {'Mlle': 'Miss', 'Major': 'Rare', 'Col': 'Rare', 'Sir': 'Rare', 'Don': 'Rare', 'Mme': 'Miss',\n          'Jonkheer': 'Rare', 'Lady': 'Rare', 'Capt': 'Rare', 'Countess': 'Rare', 'Ms': 'Miss', 'Dona': 'Mrs', 'Rev': 'Rare'}\ntrain_data.replace({'Title': mapping}, inplace=True)\n\n# Remove the Name column\ntrain_data.drop(columns='Name', inplace=True)","6aad145e":"# Calculate the number if members in the family (excluding the individual)\ntrain_data['familyNumber'] = train_data['SibSp'] + train_data['Parch']\nprint(train_data.head(8))\n\n# Detect outliers from Age, familyNumber and Fare\nOutliers_to_drop = detect_outliers(train_data,1,[\"Age\",\"familyNumber\",\"Fare\"])\ntrain_data = train_data.drop(Outliers_to_drop, axis = 0).reset_index(drop=True)\n\n# Create the familySize feature\ntrain_data['familySize'] = np.nan\n\n# Fill it with text-based categorical data\ntrain_data.loc[train_data['familyNumber']==0, 'familySize'] = 'alone'\ntrain_data.loc[train_data['familyNumber']==1, 'familySize'] = 'couple'\ntrain_data.loc[train_data['familyNumber']==2, 'familySize'] = 'small'\ntrain_data.loc[train_data['familyNumber']==3, 'familySize'] = 'medium'\ntrain_data.loc[train_data['familyNumber']>=4, 'familySize'] = 'large'\n\n# Drop the columns that are no longer needed\ntrain_data.drop(columns=['familyNumber', 'SibSp', 'Parch'], inplace=True)","6c687bfd":"# Turn Pclass into a text-based categorical features\ntrain_data['newPclass'] = np.nan\ntrain_data.loc[train_data['Pclass']==1, 'newPclass'] = 'first'\ntrain_data.loc[train_data['Pclass']==2, 'newPclass'] = 'second'\ntrain_data.loc[train_data['Pclass']==3, 'newPclass'] = 'third'\ntrain_data.drop(columns='Pclass', inplace=True)\n\n# Specify which columns to one hot encode\ncolumns = ['newGender', 'Embarked', 'Title', 'familySize', 'newPclass']\n\n# Do the encoding for the two dataframes we are comapring\none_hot = pd.get_dummies(train_data.loc[:, columns], drop_first=True)\ntrain_data.drop(columns=columns, inplace=True)\ntrain_data = train_data.join(one_hot)\n\n# Print the output to check it has done what we wanted\nprint(train_data.head(2))\nprint(train_data.columns.values)","f6cfba81":"# Import some required modules for the fitting\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report, confusion_matrix\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.neighbors import KNeighborsClassifier","fddce706":"# Fit the scaler on all the training data\ntrainScaler = StandardScaler()\ntrainScaler.fit(train_data.drop(['Survived', 'PassengerId'], axis = 1))\n\n# Scale data\nX_train_scaled = trainScaler.transform(train_data.drop(['Survived', 'PassengerId'], axis = 1))\n\n# Split the training data into X and y data\nX_train = train_data.drop(['Survived', 'PassengerId'], axis = 1)\ny_train = train_data['Survived']","16d9a27e":"# Try to improve the KNN results with hyperparameter tuning with grid search\ngrid_params = {\n    'n_neighbors': [3, 5, 7, 9, 11, 13],\n    'weights': ['uniform', 'distance'],\n    'metric':['euclidean', 'manhattan']\n                }\nKNN_CV = GridSearchCV(estimator = KNeighborsClassifier(), param_grid=grid_params, cv = 3)\nKNN_CV.fit(X_train_scaled, y_train)\n\n# Print the best score\nprint(KNN_CV.best_score_)\n\n# Get the parameters from the best fit\nweights = KNN_CV.best_params_.get('weights')\nn = KNN_CV.best_params_.get('n_neighbors')\nmetric = KNN_CV.best_params_.get('metric')\n\n# Train the classifier on all the data for predicting the test data (use best hyperparams)\nbestKNN = KNeighborsClassifier(n_neighbors=n, weights=weights, metric=metric)\nbestKNN.fit(X_train_scaled, y_train)","04cd9f33":"# Search for the optimal hyperparameters for RF\nparam_grid = { \n    'criterion' : ['gini'],\n    'n_estimators': [70, 80, 90, 100, 110, 120],\n    'max_features': ['auto', 'log2'],\n    'max_depth' : [5, 7, 9, 11, 13]    \n                }\nrandomForest_CV = GridSearchCV(estimator = RandomForestClassifier(), param_grid = param_grid, cv = 3)\nrandomForest_CV.fit(X_train, y_train)\n#print(randomForest_CV.best_params_)\n\n# Print the best score\nprint(randomForest_CV.best_score_)\n\n# Get the parameters from the best fit\ncriterion = randomForest_CV.best_params_.get('criterion')\nnRF = randomForest_CV.best_params_.get('n_estimators')\nmax_features = randomForest_CV.best_params_.get('max_features')\nmax_depth = randomForest_CV.best_params_.get('max_depth')\n\n# Train the classifier on all the data for predicting the test data (use best hyperparams)\nbestRF = RandomForestClassifier(n_estimators=1000, max_depth=max_depth, max_features=max_features, criterion=criterion)\nbestRF.fit(X_train, y_train)","62a296cb":"# Fill the missing Age values in the test data set with the averages found from the training data\nfor i in range(3):\n    test_data.loc[test_data['Pclass']==int(i+1), 'Age'] = test_data.loc[test_data['Pclass']==int(i+1), 'Age'].fillna(meanAges.values[i])\n\n# Fill the missing Fare values in the test data set\nmeanFare = test_data['Fare'].median()\ntest_data['Fare'].fillna(meanFare, inplace=True)","bbfb85b4":"# Expand the gender feature in the test set\ntest_data['newGender'] = test_data['Sex']\ntest_data.loc[test_data['Age']<18., 'newGender'] = 'child'\ntest_data.drop(columns='Sex', inplace=True)\n\n# Get the titles for the test set\ntest_data['Title'] = test_data['Name']\nfor name_string in test_data['Name']:\n    test_data['Title'] = test_data['Name'].str.extract('([A-Za-z]+)\\.', expand=True)\nmapping = {'Mlle': 'Miss', 'Major': 'Rare', 'Col': 'Rare', 'Sir': 'Rare', 'Don': 'Rare', 'Mme': 'Miss',\n          'Jonkheer': 'Rare', 'Lady': 'Rare', 'Capt': 'Rare', 'Countess': 'Rare', 'Ms': 'Miss', 'Dona': 'Mrs', 'Rev': 'Rare'}\ntest_data.replace({'Title': mapping}, inplace=True)\ntest_data.drop(columns='Name', inplace=True)\n\n# Expand the family feature\ntest_data['familyNumber'] = test_data['SibSp'] + test_data['Parch']\ntest_data['familySize'] = np.nan\ntest_data.loc[test_data['familyNumber']==0, 'familySize'] = 'alone'\ntest_data.loc[test_data['familyNumber']==1, 'familySize'] = 'couple'\ntest_data.loc[test_data['familyNumber']==2, 'familySize'] = 'small'\ntest_data.loc[test_data['familyNumber']==3, 'familySize'] = 'medium'\ntest_data.loc[test_data['familyNumber']>=4, 'familySize'] = 'large'\ntest_data.drop(columns=['familyNumber', 'SibSp', 'Parch'], inplace=True)\n\n# Create the new Pclass feature\ntest_data['newPclass'] = np.nan\ntest_data.loc[test_data['Pclass']==1, 'newPclass'] = 'first'\ntest_data.loc[test_data['Pclass']==2, 'newPclass'] = 'second'\ntest_data.loc[test_data['Pclass']==3, 'newPclass'] = 'third'\ntest_data.drop(columns='Pclass', inplace=True)","4567efc2":"# Drop the features that we have not trained on\ntest_data.drop(columns=['Cabin', 'Ticket'], inplace=True)\n\n# One hot encode the test data features\none_hot = pd.get_dummies(test_data.loc[:, columns], drop_first=True)\ntest_data.drop(columns=columns, inplace=True)\ntest_data = test_data.join(one_hot)\n\n# Check all the NaNs have been dealt with\nprint(\"NaNs in each testing Feature\")\nprint(test_data.isnull().sum())","6edb067c":"# Scale the test data by the same amount as the train data\nX_test_scaled = trainScaler.transform(test_data.drop(['PassengerId'], axis = 1))\nX_test = test_data.drop(columns='PassengerId')\n\n# Use the models to predict the output of the test data\ntestPredictionsKNN = bestKNN.predict(X_test_scaled)\ntestPredictionsRF = bestRF.predict(X_test)\n\n# Save the outputs to csv files\noutputKNN = pd.DataFrame({'PassengerId': test_data.PassengerId, 'Survived': testPredictionsKNN})\noutputKNN.to_csv('my_submissionKNN.csv', index=False)\noutputRF = pd.DataFrame({'PassengerId': test_data.PassengerId, 'Survived': testPredictionsRF})\noutputRF.to_csv('my_submissionRF.csv', index=False)","c362114a":"from sklearn.ensemble import GradientBoostingClassifier\n\n\nparam_grid = { \n    'learning_rate' : [0.1, 0.2],\n    'n_estimators': [90, 100, 110],\n    'loss': ['deviance', 'exponential']    \n                }\ngradBoost_CV = GridSearchCV(estimator = GradientBoostingClassifier(), param_grid = param_grid, cv = 3)\ngradBoost_CV.fit(X_train, y_train)\n#print(randomForest_CV.best_params_)\n\n# Print the best score\nprint(gradBoost_CV.best_score_)\n\n# Get the parameters from the best fit\nlearning = gradBoost_CV.best_params_.get('learning_rate')\nnGB = gradBoost_CV.best_params_.get('n_estimators')\nloss = gradBoost_CV.best_params_.get('loss')\n\n# Train the classifier on all the data for predicting the test data (use best hyperparams)\nbestGB = GradientBoostingClassifier(learning_rate=learning, n_estimators=nGB, loss=loss)\nbestGB.fit(X_train, y_train)\n\ntestPredictionsGB = bestGB.predict(X_test)\n\noutputGB = pd.DataFrame({'PassengerId': test_data.PassengerId, 'Survived': testPredictionsGB})\noutputGB.to_csv('my_submissionGB.csv', index=False)","855f41d9":"from xgboost import XGBClassifier\n\nparam_grid = { \n    'learning_rate' : [0.1, 0.2],\n    'max_depth': [3, 5, 7],   \n                }\nxgBoost_CV = GridSearchCV(estimator = XGBClassifier(), param_grid = param_grid, cv = 3)\nxgBoost_CV.fit(X_train, y_train)\n#print(randomForest_CV.best_params_)\n\n# Print the best score\nprint(xgBoost_CV.best_score_)\n\n\n# Get the parameters from the best fit\nlearning = xgBoost_CV.best_params_.get('learning_rate')\ndepth = xgBoost_CV.best_params_.get('max_depth')\n\n\n# Train the classifier on all the data for predicting the test data (use best hyperparams)\nbestXGB = XGBClassifier(learning_rate=learning, max_depth=depth)\nprint(bestXGB)\nbestXGB.fit(X_train, y_train)\n\ntestPredictionsXGB = bestXGB.predict(X_test)\n\noutputGB = pd.DataFrame({'PassengerId': test_data.PassengerId, 'Survived': testPredictionsXGB})\noutputGB.to_csv('my_submissionXGB.csv', index=False)","81549351":"Now all we have to do is submit the outputs to the competition. The Random Forest Model gives a higher test score than the k-NN model. **If you've got this far, please give my notebook an upvote! It really helps!**\n\n## Just trying some other models here quickly...","dfc48663":"# 2.2. Feature Distributions\n\nNow let's look at the distributions of each feature. We can do this using the .plot() and .hist() pandas methods.\n\nSome comments about the distributions:\n* It is worth noting that everyone on board had a different *Name* and that almost everyone had a different *Ticket*. Therefore, there is no point in visualising these distributions as they will just be uniform with counts of 1. \n* Furthermore, *passengerID* is simply a number assigned to each passenger and as such, the distribution does not tell us much. \n* The *Cabin* data is mainly NaNs and so plotting the distribution is not particularly useful.","8f2cb03e":"# 2.1. Features and Missing Data\nNow we want to have a look at the data and understand the information we have been given. A good way to do this is to find out how much data we have and where there is missing data (e.g. NaNs). This will help later on when we decide how to handle the missing data.\n\nLet's start by seeing how many rows of data we have and also the features that are provided. We can then also check whether there are any NaNs in the dataframe and which features contain them.","ca6ba9ed":"By looking at the above correlation values we can see that there are some reasonable (anti-)correlations.\n\nThese include:\n* *Fare* and *Class* (this makes sense because 1st class tickets are more expensive just as we predicted)\n* *Survival* and *Class* (this could be interesting)\n* *Survival* and *Fare* (closely related to *Survival* and *Class* correlation)\n* *Class* and *Age* (this makes sense because older passengers likely have more money and are therefore more likely to have 1st or 2nd class tickets)\n* *Age* and *Siblings \/ Spouses aboard*\n* *Age* and *Parents \/ Childern aboard*\n* *Siblings \/ Spouses aboard* and *Parents \/ Children aboard*\n* *Fare* and *Siblings \/ Spouses aboard*\n* *Fare* and *Parents \/ Children aboard*\n\n\nThere may also be some correlations between the non-numerical values, so let's check some of these.\n\n## 2.3.2. Text-Based Features\nLet's consider the categorical text-based features *Embarked* and *Sex*.\n\n### 2.3.2.1. Embarked","3d4e1887":"**5.2 Expand some of the Test Features**","6358ab27":"# 3.4. Handling Categorical Text Data\nIt is important to convert text labels into something more understandable for the model. To do this with category based text data we can use one hot encoding as below.","412e9f3c":"**3.2.2. Using the Mean for Age**\n\nLet's calculate the mean of the *Age* feature and use this to fill in the missing values. As *Age* is strongly correlated with *Pclass*, let's use *Pclass* in order to fill the *Age* values.","f8bd2d29":"# 5. Generating a Submission File\nNow we just need to generate a submission file for the competition using our trained model. This can be done as outlined below (Note that we could have done all of the processing of the test data with the training data earlier on but have kept it separate for clarity here).\n\n**5.1 Fill the missing Values** ","e3882d13":"# 1. **Loading Modules and Data**\n\nWe have to import modules and the data before we can do any analysis.","c394e979":"# 3.2. Filling in Missing Values\n**3.2.1. Embarked**\n\nWe first need to fill in the 2 missing *Embarked* values. As the overwhelming majority of people embarked at \"S\" we are going to assume that these two missing values are \"S\". We can fill these in as shown below.","72d20642":"Now we just have to fill in the missing *Age* values","9046051f":"**5.3. Predictions!**","695d5f3a":"### 2.3.2.2. Sex","87b70b1d":"**3.3.1. Gender Feature**\n\nFirst let's expand this feature to include children as well.","a590fc58":"Now let's load in the data using pandas as visualise the top of the training and testing data.","2e425047":"# 3.3. Expanding Existing Features\nHere we will try to adjust some of the features to provide better insights into the situation and improve the model.","fcb0a421":"Both the RF and k-NN models give us over **83% accuracy** on the training data. Now we can use the best parameters to fit the models and predict the test data outcomes.\n","bff6f5c6":"# Titanic Kaggle Competition\nThis is my second attempt at the Titanic Kaggle competition. I have used a lot of content that I initially wrote for my [first notebook](https:\/\/www.kaggle.com\/thomaswoolley\/random-forest-titanic-beginners-0-76-score). In this notebook I try to improve my score by doing more feature engineering and by using a Grid Search to tune the hyperparameters of my models. I also try using the k-Nearest-Neighbours classifier and a Random Forest model.\n\nIf this notebook is helpful for you, please give it an upvote! Enjoy!\n\nThis notebook is available on kaggle [here](https:\/\/www.kaggle.com\/thomaswoolley\/rf-and-k-nn-titanic-0-79-score) and GitHub [here](https:\/\/github.com\/Woolley12345\/kaggle-competitions).","18e07760":"# 4. Fitting Models\n\nAs the aim of this task is to predict whether passengers survived or not, it is a classification task. For this we can used a variety of classification models to predict the outcome. It is also worth noting that the success of this task is determined by the accuracy (the percentage of passenger outcomes successfully predicted). \n","c22f791f":"**3.3.2. Name Feature**\n\nNow let's get the title of the each person and feed this into the model. Some of this code was from: https:\/\/www.kaggle.com\/konstantinmasich\/titanic-0-82-0-83","ba7e8948":"The above output tells us that there are 891 rows of data in the training dataframe as well as the features that are provided (columns in the dataframe). The only feature that appears in the training set but not in the test set is *Survival* because this is the output we would like to predict with a model.\n\nWe can also see that the training features: *Age*, *Cabin* and *Embarked* contain NaN values. For now this is ok, but it may need to be addressed before fitting models later on. The testing features Age, Cabin and Fare also contain NaN values.","97ab5c84":"# 3.1. Removing Unfillable Values\nAs there are so many missing values for the *Cabin* feature, we should just remove this feature altogether. It would be extremely difficult to fill these values in and there would likely be a lot of errors introduced. We will also drop the *Ticket* feature.\n\n\n","ae4d3c50":"Now these plots above give us some more information about the *Sex* and *Embarked* features. This could be useful later in our analysis.\n\n# 3. Feature Engineering\nNow that we understand a bit about our dataset, we can start getting ready to apply some models to it. In order to this, we should try to make the features more \"model friendly\". This includes filling in values that are missing and removing some data.\n\n\n","cf552983":"**4.2. k-Nearest Neighbours**\n\nOne model that we can fit to classify our data is called k-NN. We can fit it and check that accuracy as shown below.\n\nFor k-NN, the features must be scaled first. This is not necessary for RF though.","0db9f251":"**3.3.3. Family Size Feature**\n\nNow let's work out the size of families on board","c07ff631":"From the above distributions we can get a feel for the data set that we are using. \n\nFor example: \n   * The most represented *Age* range was between 20 and 40 years old\n   * We can see that most people paid a *Fare* of under 20 for their tickets.\n   * Most people on board had no parents or children with them \n   * Over half of the passengers were in third class\n   * Most people had no siblings or spouses on board\n   * Only around 40% of passengers survived (1.0)\n   * Around two thirds of the passengers were male\n   * The overwhelming majority of passengers *Embarked* at S\n","27ea327c":"**4.1. Importing**","73b8b661":"**4.3. Random Forest**\n\nNow let's fit a Random Forest Model to the data.","be2c144f":"# **2. Data Exploration**\nBefore we start fitting models to the data, we want to understand it a bit better. This will help with debugging problems later and will also allow more bespoke models to be implemented. The easiest way to get to know the dataset that is in front of us is to play around with it.","1ab51538":"# 2.3. Feature Correlations\n\nNow that we roughly know the distributions of each feature, we can start to see if two features are correlated with each other. Here we will not consider: *Cabin* (too much missing data), *Ticket* (not really categorical or numerical) or *Name* (all individual but covered later).\n\n## 2.3.1. Numerical Features\nBy using the .corr() pandas method we can build up an idea of the relationships between each of the numerical features. This is a good sense check as to whether the data is reliable. For example, we would expect a strong correlation between *Fare* and *Class* as 1st class tickets cost more than 3rd class tickets.","22443581":"Let's also define a function for removing outliers that will be used later on. I have based this outlier code on some of the code in: https:\/\/www.kaggle.com\/yassineghouzam\/titanic-top-4-with-ensemble-modeling"}}