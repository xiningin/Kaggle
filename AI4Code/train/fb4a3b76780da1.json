{"cell_type":{"1a2c6be8":"code","d7813ed7":"code","da5dfed5":"code","1668fa75":"code","2a10b6e8":"code","ac0963fe":"code","19d261f8":"code","f6d09557":"code","cdc62645":"code","5f16d537":"code","f0800db7":"code","3a625088":"code","0ba39d68":"code","025f64a6":"code","cecbe6ad":"code","a89cb7ef":"markdown","fed75888":"markdown","b0b34475":"markdown","4cb74157":"markdown","941b3aad":"markdown","fef0efc3":"markdown","2f3f2746":"markdown","f04c5f7e":"markdown","73236c80":"markdown","82013b85":"markdown","94007215":"markdown","1e026c33":"markdown"},"source":{"1a2c6be8":"%%writefile RPS_Agent.py\n\nimport random\n\nmoves = [0, 1, 2]\ndna_encode = {\n    '11': '1', '10': '2', '12': '3',\n    '01': '4', '02': '5', '00': '6',\n    '22': '7', '21': '8', '20': '9' }\n\ndef beat_move(x):\n    return (x + 1) % 3\n\ndef agent (observation, configuration):\n    global opp_history, action, dna\n    if observation.step == 0:\n        opp_history = ''\n        dna = ''\n        action = random.choice([0, 1, 2])\n    else:\n        opp_history += str(observation.lastOpponentAction)\n        dna += dna_encode[str(observation.lastOpponentAction) + str(action)]\n\n        for length in (100, 90, 80, 70, 60, 50, 40, 30, 20, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1):\n            # Search for the last longest chain\n            x = dna[:-1].rfind (dna[-length:])\n            if x >= 0:\n                # If found: Pick what will be the next move and play against it\n                next_move = opp_history[x + length]\n                action = beat_move(int(next_move))\n                break\n    \n    return action","d7813ed7":"%%writefile hit_the_last_own_action.py\n\nmy_last_action = 0\n\ndef hit_the_last_own_action(observation, configuration):\n    global my_last_action\n    my_last_action = (my_last_action + 1) % 3\n    \n    return my_last_action","da5dfed5":"%%writefile rock.py\n\ndef rock(observation, configuration):\n    return 0","1668fa75":"%%writefile paper.py\n\ndef paper(observation, configuration):\n    return 1","2a10b6e8":"%%writefile scissors.py\n\ndef scissors(observation, configuration):\n    return 2","ac0963fe":"%%writefile copy_opponent.py\n\nimport random\nfrom kaggle_environments.envs.rps.utils import get_score\n\ndef copy_opponent(observation, configuration):\n    if observation.step > 0:\n        return observation.lastOpponentAction\n    else:\n        return random.randrange(0, configuration.signs)","19d261f8":"%%writefile reactionary.py\n\nimport random\nfrom kaggle_environments.envs.rps.utils import get_score\n\nlast_react_action = None\n\n\ndef reactionary(observation, configuration):\n    global last_react_action\n    if observation.step == 0:\n        last_react_action = random.randrange(0, configuration.signs)\n    elif get_score(last_react_action, observation.lastOpponentAction) <= 1:\n        last_react_action = (observation.lastOpponentAction + 1) % configuration.signs\n\n    return last_react_action","f6d09557":"%%writefile markov_agent.py\n\nimport numpy as np\nimport collections\n\ndef markov_agent(observation, configuration):\n    k = 2\n    global table, action_seq\n    if observation.step % 250 == 0: # refresh table every 250 steps\n        action_seq, table = [], collections.defaultdict(lambda: [1, 1, 1])    \n    if len(action_seq) <= 2 * k + 1:\n        action = int(np.random.randint(3))\n        if observation.step > 0:\n            action_seq.extend([observation.lastOpponentAction, action])\n        else:\n            action_seq.append(action)\n        return action\n    # update table\n    key = ''.join([str(a) for a in action_seq[:-1]])\n    table[key][observation.lastOpponentAction] += 1\n    # update action seq\n    action_seq[:-2] = action_seq[2:]\n    action_seq[-2] = observation.lastOpponentAction\n    # predict opponent next move\n    key = ''.join([str(a) for a in action_seq[:-1]])\n    if observation.step < 500:\n        next_opponent_action_pred = np.argmax(table[key])\n    else:\n        scores = np.array(table[key])\n        next_opponent_action_pred = np.random.choice(3, p=scores\/scores.sum()) # add stochasticity for second part of the game\n    # make an action\n    action = (next_opponent_action_pred + 1) % 3\n    # if high probability to lose -> let's surprise our opponent with sudden change of our strategy\n    if observation.step > 900:\n        action = next_opponent_action_pred\n    action_seq[-1] = action\n    return int(action)","cdc62645":"%%writefile memory_patterns.py\n\nimport random\n\n# how many steps in a row are in the pattern (multiplied by two)\nmemory_length = 6\n# current memory of the agent\ncurrent_memory = []\n# list of memory patterns\nmemory_patterns = []\n\ndef find_pattern(memory):\n    \"\"\" find appropriate pattern in memory \"\"\"\n    for pattern in memory_patterns:\n        actions_matched = 0\n        for i in range(memory_length):\n            if pattern[\"actions\"][i] == memory[i]:\n                actions_matched += 1\n            else:\n                break\n        # if memory fits this pattern\n        if actions_matched == memory_length:\n            return pattern\n    # appropriate pattern not found\n    return None\n\ndef my_agent(obs, conf):\n    \"\"\" your ad here \"\"\"\n    # if it's not first step, add opponent's last action to agent's current memory\n    if obs[\"step\"] > 0:\n        current_memory.append(obs[\"lastOpponentAction\"])\n    # if length of current memory is bigger than necessary for a new memory pattern\n    if len(current_memory) > memory_length:\n        # get momory of the previous step\n        previous_step_memory = current_memory[:memory_length]\n        previous_pattern = find_pattern(previous_step_memory)\n        if previous_pattern == None:\n            previous_pattern = {\n                \"actions\": previous_step_memory.copy(),\n                \"opp_next_actions\": [\n                    {\"action\": 0, \"amount\": 0, \"response\": 1},\n                    {\"action\": 1, \"amount\": 0, \"response\": 2},\n                    {\"action\": 2, \"amount\": 0, \"response\": 0}\n                ]\n            }\n            memory_patterns.append(previous_pattern)\n        for action in previous_pattern[\"opp_next_actions\"]:\n            if action[\"action\"] == obs[\"lastOpponentAction\"]:\n                action[\"amount\"] += 1\n        # delete first two elements in current memory (actions of the oldest step in current memory)\n        del current_memory[:2]\n    my_action = random.randint(0, 2)\n    pattern = find_pattern(current_memory)\n    if pattern != None:\n        my_action_amount = 0\n        for action in pattern[\"opp_next_actions\"]:\n            # if this opponent's action occurred more times than currently chosen action\n            # or, if it occured the same amount of times, choose action randomly among them\n            if (action[\"amount\"] > my_action_amount or\n                    (action[\"amount\"] == my_action_amount and random.random() > 0.5)):\n                my_action_amount = action[\"amount\"]\n                my_action = action[\"response\"]\n    current_memory.append(my_action)\n    return my_action","5f16d537":"%%writefile multi_armed_bandit.py\n\n\nimport pandas as pd\nimport numpy as np\nimport json\n\n\n# base class for all agents, random agent\nclass agent():\n    def initial_step(self):\n        return np.random.randint(3)\n    \n    def history_step(self, history):\n        return np.random.randint(3)\n    \n    def step(self, history):\n        if len(history) == 0:\n            return int(self.initial_step())\n        else:\n            return int(self.history_step(history))\n    \n# agent that returns (previousCompetitorStep + shift) % 3\nclass mirror_shift(agent):\n    def __init__(self, shift=0):\n        self.shift = shift\n    \n    def history_step(self, history):\n        return (history[-1]['competitorStep'] + self.shift) % 3\n    \n    \n# agent that returns (previousPlayerStep + shift) % 3\nclass self_shift(agent):\n    def __init__(self, shift=0):\n        self.shift = shift\n    \n    def history_step(self, history):\n        return (history[-1]['step'] + self.shift) % 3    \n\n\n# agent that beats the most popular step of competitor\nclass popular_beater(agent):\n    def history_step(self, history):\n        counts = np.bincount([x['competitorStep'] for x in history])\n        return (int(np.argmax(counts)) + 1) % 3\n\n    \n# agent that beats the agent that beats the most popular step of competitor\nclass anti_popular_beater(agent):\n    def history_step(self, history):\n        counts = np.bincount([x['step'] for x in history])\n        return (int(np.argmax(counts)) + 2) % 3\n    \n    \n# simple transition matrix: previous step -> next step\nclass transition_matrix(agent):\n    def __init__(self, deterministic = False, counter_strategy = False, init_value = 0.1, decay = 1):\n        self.deterministic = deterministic\n        self.counter_strategy = counter_strategy\n        if counter_strategy:\n            self.step_type = 'step' \n        else:\n            self.step_type = 'competitorStep'\n        self.init_value = init_value\n        self.decay = decay\n        \n    def history_step(self, history):\n        matrix = np.zeros((3,3)) + self.init_value\n        for i in range(len(history) - 1):\n            matrix = (matrix - self.init_value) \/ self.decay + self.init_value\n            matrix[int(history[i][self.step_type]), int(history[i+1][self.step_type])] += 1\n\n        if  self.deterministic:\n            step = np.argmax(matrix[int(history[-1][self.step_type])])\n        else:\n            step = np.random.choice([0,1,2], p = matrix[int(history[-1][self.step_type])]\/matrix[int(history[-1][self.step_type])].sum())\n        \n        if self.counter_strategy:\n            # we predict our step using transition matrix (as competitor can do) and beat probable competitor step\n            return (step + 2) % 3 \n        else:\n            # we just predict competitors step and beat it\n            return (step + 1) % 3\n    \n\n# similar to the transition matrix but rely on both previous steps\nclass transition_tensor(agent):\n    \n    def __init__(self, deterministic = False, counter_strategy = False, init_value = 0.1, decay = 1):\n        self.deterministic = deterministic\n        self.counter_strategy = counter_strategy\n        if counter_strategy:\n            self.step_type1 = 'step' \n            self.step_type2 = 'competitorStep'\n        else:\n            self.step_type2 = 'step' \n            self.step_type1 = 'competitorStep'\n        self.init_value = init_value\n        self.decay = decay\n        \n    def history_step(self, history):\n        matrix = np.zeros((3,3, 3)) + 0.1\n        for i in range(len(history) - 1):\n            matrix = (matrix - self.init_value) \/ self.decay + self.init_value\n            matrix[int(history[i][self.step_type1]), int(history[i][self.step_type2]), int(history[i+1][self.step_type1])] += 1\n\n        if  self.deterministic:\n            step = np.argmax(matrix[int(history[-1][self.step_type1]), int(history[-1][self.step_type2])])\n        else:\n            step = np.random.choice([0,1,2], p = matrix[int(history[-1][self.step_type1]), int(history[-1][self.step_type2])]\/matrix[int(history[-1][self.step_type1]), int(history[-1][self.step_type2])].sum())\n        \n        if self.counter_strategy:\n            # we predict our step using transition matrix (as competitor can do) and beat probable competitor step\n            return (step + 2) % 3 \n        else:\n            # we just predict competitors step and beat it\n            return (step + 1) % 3\n\n    \nagents = {\n    'mirror_0': mirror_shift(0),\n    'mirror_1': mirror_shift(1),  \n    'mirror_2': mirror_shift(2),\n    'self_0': self_shift(0),\n    'self_1': self_shift(1),  \n    'self_2': self_shift(2),\n    'popular_beater': popular_beater(),\n    'anti_popular_beater': anti_popular_beater(),\n    'random_transitison_matrix': transition_matrix(False, False),\n    'determenistic_transitison_matrix': transition_matrix(True, False),\n    'random_self_trans_matrix': transition_matrix(False, True),\n    'determenistic_self_trans_matrix': transition_matrix(True, True),\n    'random_transitison_tensor': transition_tensor(False, False),\n    'determenistic_transitison_tensor': transition_tensor(True, False),\n    'random_self_trans_tensor': transition_tensor(False, True),\n    'determenistic_self_trans_tensor': transition_tensor(True, True),\n    \n    'random_transitison_matrix_decay': transition_matrix(False, False, decay = 1.05),\n    'random_self_trans_matrix_decay': transition_matrix(False, True, decay = 1.05),\n    'random_transitison_tensor_decay': transition_tensor(False, False, decay = 1.05),\n    'random_self_trans_tensor_decay': transition_tensor(False, True, decay = 1.05),\n    \n    'determenistic_transitison_matrix_decay': transition_matrix(True, False, decay = 1.05),\n    'determenistic_self_trans_matrix_decay': transition_matrix(True, True, decay = 1.05),\n    'determenistic_transitison_tensor_decay': transition_tensor(True, False, decay = 1.05),\n    'determenistic_self_trans_tensor_decay': transition_tensor(True, True, decay = 1.05),\n}\n\n    \ndef multi_armed_bandit_agent (observation, configuration):\n    \n    # bandits' params\n    step_size = 2 # how much we increase a and b \n    decay_rate = 1.05 # how much do we decay old historical data\n    \n    # I don't see how to use any global variables, so will save everything to a CSV file\n    # Using pandas for this is too much, but it can be useful later and it is convinient to analyze\n    def save_history(history, file = 'history.csv'):\n        pd.DataFrame(history).to_csv(file, index = False)\n\n    def load_history(file = 'history.csv'):\n        return pd.read_csv(file).to_dict('records')\n    \n    \n    def log_step(step = None, history = None, agent = None, competitorStep = None):\n        if step is None:\n            step = np.random.randint(3)\n        if history is None:\n            history = []\n        history.append({'step': step, 'competitorStep': competitorStep, 'agent': agent})\n        save_history(history)\n        return step\n    \n    def update_competitor_step(history, competitorStep):\n        history[-1]['competitorStep'] = int(competitorStep)\n        return history\n        \n    \n    # load history\n    if observation.step == 0:\n        history = []\n        bandit_state = {k:[1,1] for k in agents.keys()}\n    else:\n        history = update_competitor_step(load_history(), observation.lastOpponentAction)\n        \n        # load the state of the bandit\n        with open('bandit.json') as json_file:\n            bandit_state = json.load(json_file)\n        \n        # updating bandit_state using the result of the previous step\n        # we can update all states even those that were not used\n        for name, agent in agents.items():\n            agent_step = agent.step(history[:-1])\n            bandit_state[name][1] = (bandit_state[name][1] - 1) \/ decay_rate + 1\n            bandit_state[name][0] = (bandit_state[name][0] - 1) \/ decay_rate + 1\n            \n            if (history[-1]['competitorStep'] - agent_step) % 3 == 1:\n                bandit_state[name][1] += step_size\n            elif (history[-1]['competitorStep'] - agent_step) % 3 == 2:\n                bandit_state[name][0] += step_size\n            else:\n                bandit_state[name][0] += step_size\/2\n                bandit_state[name][1] += step_size\/2\n            \n    with open('bandit.json', 'w') as outfile:\n        json.dump(bandit_state, outfile)\n    \n    \n    # generate random number from Beta distribution for each agent and select the most lucky one\n    best_proba = -1\n    best_agent = None\n    for k in bandit_state.keys():\n        proba = np.random.beta(bandit_state[k][0],bandit_state[k][1])\n        if proba > best_proba:\n            best_proba = proba\n            best_agent = k\n        \n    step = agents[best_agent].step(history)\n    \n    return log_step(step, history, best_agent)","f0800db7":"import numpy as np\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n\nlist_names = [\n    \"rock\", \n    \"paper\", \n    \"scissors\",\n    \"hit_the_last_own_action\",  \n    \"copy_opponent\", \n    \"reactionary\", \n    \"markov_agent\", \n    \"memory_patterns\", \n    \"multi_armed_bandit\",\n]\nlist_agents = [agent_name + \".py\" for agent_name in list_names]\n\nn_agents = len(list_names)\n\nscores = np.zeros((n_agents, n_agents), dtype=np.int)","3a625088":"from kaggle_environments import make, evaluate\nenv = make(\n    \"rps\", \n    configuration={\n        \"episodeSteps\": 1000\n    }\n)","0ba39d68":"for ind_agent_1 in range(len(list_names)):\n\n    current_score = evaluate(\n        \"rps\", \n        [\"RPS_Agent.py\", list_agents[ind_agent_1]], \n        configuration={\"episodeSteps\": 1000}\n    )\n\n    scores[ind_agent_1, 0] = current_score[0][0]","025f64a6":"res = []\nfor i in scores:\n    res.append(i[0])","cecbe6ad":"df_scores = pd.DataFrame(\n    res, \n    index=list_names, \n    columns=[\"RSP Agent\"],\n)\n\n\nplt.figure(figsize=(2, 10))\nsns.heatmap(\n    df_scores, annot=True, cbar=False, \n    cmap=\"coolwarm\", linewidths=1, linecolor=\"black\", \n    fmt=\"d\", vmin=-500, vmax=500,\n)\nplt.xticks(rotation=90, fontsize=15)\nplt.yticks(rotation=360, fontsize=15);","a89cb7ef":"# Rock Paper Scissors - Agent\n\nEnjoy !!!","fed75888":"<h2 style=' border:0; color:black'><center>Agent: Hit The Last Own Action<center><h2>","b0b34475":"<h2 style=' border:0; color:black'><center>Agent: Paper<center><h2>","4cb74157":"<h2 style=' border:0; color:black'><center>Agent: Writing reactionary<center><h2>","941b3aad":"<h2 style=' border:0; color:black'><center>Agent: Markov Agent<center><h2>","fef0efc3":"Let now comparison in a fair battle of each agent with each in 1 round with 1000 steps.\nThe original notebook for the comparison [https:\/\/www.kaggle.com\/ihelon\/rock-paper-scissors-agents-comparison](http:\/\/)","2f3f2746":"<h2 style=' border:0; color:black'><center>Agent: Scissors<center><h2>","f04c5f7e":"<h2 style=' border:0; color:black'><center>Agent: Memory Patterns<center><h2>","73236c80":"<h2 style=' border:0; color:black'><center>Agent: Copy Opponent<center><h2>","82013b85":"<h2 style=' border:0; color:black'><center>Agent: Rock<center><h2>","94007215":"<h2 style=' border:0; color:black'><center>Agent: Multi Armed Bandit<center><h2>","1e026c33":"# Comparison In Battle section"}}