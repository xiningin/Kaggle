{"cell_type":{"1b0a5294":"code","be7f14a2":"code","71e4fdd7":"code","3c701466":"code","1fdb0643":"code","c0a307e7":"code","6c0dab67":"code","6383badb":"code","d0eb2f54":"code","2c6bd157":"code","25537232":"code","90293bd3":"code","fef14b28":"code","601153b7":"code","31a47f3b":"code","f9bceebf":"code","a73df831":"code","be1123b5":"code","4dbc8fd2":"code","2d0c3724":"code","8ff16a74":"code","7fd1698f":"code","08ab322f":"code","fdb706f1":"code","b2b93287":"code","8af052a4":"code","8b8294e4":"code","bfb6ad57":"code","01870507":"code","5629d683":"code","8c4a85c9":"code","4631cd00":"code","aab3703f":"code","817642c4":"code","07724b79":"code","c0022b51":"code","7c8fa3d7":"code","cb4f9ac5":"code","49495f3f":"code","6881f5bc":"code","fc9481cd":"code","03205822":"code","389a7743":"code","4aadc042":"code","f2ad9d58":"code","0da138bd":"code","1b55bd4f":"code","495d3b12":"code","d9a86b00":"code","cc529f8f":"code","06fb2cab":"code","36f99d27":"code","92b0bb30":"markdown","7717bf67":"markdown","60d3284d":"markdown","f85ebf27":"markdown","c8e066d8":"markdown","e38a5636":"markdown","b6083fec":"markdown","c7380ec0":"markdown","cf9bab37":"markdown","13a78e03":"markdown","bc01091e":"markdown","5705a13f":"markdown","15ac8a60":"markdown","25ff2537":"markdown","a7c781fd":"markdown","b1c98805":"markdown","7341f110":"markdown","8136dbd5":"markdown","374b5d43":"markdown","070d1954":"markdown","43718a97":"markdown","bfaf1110":"markdown","04746cfc":"markdown","aa4b7743":"markdown","467e54cf":"markdown","406db062":"markdown","791fa1c3":"markdown","4729777e":"markdown","3ede9320":"markdown","309f2979":"markdown","6e8fd202":"markdown","0879187d":"markdown","5c96f7b4":"markdown","a7dc8697":"markdown","40b9319b":"markdown","688f1634":"markdown","9440a21d":"markdown","e42ecf4c":"markdown","390e2073":"markdown","88e5a85a":"markdown","f2fffae2":"markdown","9197360d":"markdown","f7fb75b8":"markdown","81519247":"markdown","ff56b77b":"markdown","de9319f5":"markdown","647b36d1":"markdown","b765aa1d":"markdown","ef31951c":"markdown","cad6802e":"markdown","41a7944b":"markdown","118e9bb4":"markdown","b2ecb3da":"markdown"},"source":{"1b0a5294":"!git clone https:\/\/github.com\/wcchin\/EpiRank.git\n%cd EpiRank\n!pip install -e .\n!pip install datapackage\n!pip install tika\n!pip install fredapi\n!pip install pydotplus\n\nimport pandas as pd\nimport numpy as np\nfrom tqdm import tqdm\nimport datapackage\nimport requests\nimport io\nimport json\nfrom EpiRank import epirank\nfrom EpiRank import epirank\nfrom math import log\nfrom EpiRank import additional_analysis as aa\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\npd.options.display.max_rows = 4000\npd.options.display.max_columns = 4000\npd.set_option('display.max_colwidth', 2000)","be7f14a2":"from tika import parser\nimport re\nimport matplotlib.pyplot as plt\nfrom matplotlib import cm\nfrom scipy.stats import linregress\nfrom scipy import optimize\nimport numpy as np\nimport numpy.polynomial.polynomial as npoly\nfrom scipy import optimize\nimport matplotlib.pyplot as plt\nfrom scipy.stats import linregress\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import scale\nfrom sklearn import metrics\nfrom sklearn.model_selection import RepeatedKFold\nfrom sklearn.linear_model import LinearRegression, Ridge\nfrom scipy.stats import spearmanr, kendalltau, f_oneway, pearsonr\nimport statsmodels.regression.mixed_linear_model as sm\nimport statsmodels.api as sm\nfrom fredapi import Fred\nfrom tqdm import tqdm\nfrom itertools import chain\nimport seaborn as sns\nfrom sklearn.neighbors import NearestNeighbors\nfrom bisect import bisect_right\nfrom sklearn.externals.six import StringIO  \nfrom IPython.display import Image  \nfrom sklearn.tree import export_graphviz\nfrom IPython.display import Image, display\nfrom scipy import stats\nimport pydotplus\nimport os \nfrom sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n\n\nnp.random.seed(2017)\n%matplotlib inline\n\n\ndef get_datahub_data():\n\n    data_url = 'https:\/\/datahub.io\/core\/covid-19\/datapackage.json'\n    # to load Data Package into storage\n    package = datapackage.Package(data_url)\n\n    # to load only tabular data\n    resource = package.resources[5]\n\n    if resource.tabular:\n        response = requests.get(resource.descriptor['path'])\n        file_object = io.StringIO(response.content.decode('utf-8'))\n        case_data = pd.read_csv(file_object)\n        \n    return case_data\n\ndef get_state_code_map():\n\n    file_path = '\/kaggle\/input\/state-code\/states_hash.json'\n\n    state_data = pd.read_json(file_path, orient = 'index')\n    \n    return state_data\n\ndef  get_state_square_map():\n\n    file_path = '\/kaggle\/input\/state-square\/state_square.json'\n    state_square = pd.DataFrame.from_dict(pd.read_json(file_path))\n    state_square['State'] = state_square['State'].str.replace('of', 'Of')\n    \n    return state_square\n\ndef get_full_testing_data():\n    \n    data_url = 'https:\/\/covidtracking.com\/api\/states\/daily'\n\n    response = requests.get(data_url)\n    file_object = io.StringIO(response.content.decode('utf-8'))\n    test_data = pd.read_json(file_object)\n    test_data['Province\/State'] = test_data['state'].map(state_data.iloc[:,0])\n    \n    return test_data\n\ndef get_icu_bed_capacity_data():\n\n    data_url = 'https:\/\/covid19-server.chrismichael.now.sh\/api\/v1\/AggregatedFacilityCapacityCounty'\n\n    response = requests.get(data_url)\n    file_object = io.StringIO(response.content.decode('utf-8'))\n    beds_data = pd.DataFrame.from_dict(dict(pd.read_json(file_object).iloc[0]).get('data').get('table'))\n    \n    return beds_data\n\ndef state_capital_geo_coordinates():\n\n    file_path = '\/kaggle\/input\/state-capitals\/state_capitals.json'\n\n    state_geo_cap = pd.read_json(file_path)\n\n    state_geo_cap_add = [{\n        \"abbr\": \"AS\",\n        \"name\": \"American Samoa\",\n        \"capital\": \"Pago Pago\",\n        \"lat\": \"-14.328056\",\n        \"long\": \"-170.711945\"\n        },\n        {\n        \"abbr\": \"DC\",\n        \"name\": \"District of Columbia\",\n        \"capital\": \"Washington, D.C.\",\n        \"lat\": \"38.89511\",\n        \"long\": \"-77.03637\"\n        },\n        {\n        \"abbr\": \"FM\",\n        \"name\": \"Federated States of Micronesia\",\n        \"capital\": \"Palikir\",\n        \"lat\": \"6.92477\",\n        \"long\": \"158.16109\"\n        }, \n        {\n        \"abbr\": \"PW\",\n        \"name\": \"Palau\",\n        \"capital\": \"Melekeok\",\n        \"lat\": \"7.50043\",\n        \"long\": \"134.62355\"\n        },\n        {\n        \"abbr\": \"MH\",\n        \"name\": \"Marshall Islands\",\n        \"capital\": \"Majuro\",\n        \"lat\": \"7.08971\",\n        \"long\": \"171.38027\"\n        }, \n        {\n        \"abbr\": \"GU\",\n        \"name\": \"Guam\",\n        \"capital\": \"Dededo\",\n        \"lat\": \"13.51777\",\n        \"long\": \"144.8391\"\n        },\n        {\n        \"abbr\": \"MP\",\n        \"name\": \"Northern Mariana Islands\",\n        \"capital\": \"Saipan\",\n        \"lat\": \"15.21233\",\n        \"long\": \"145.7545\"\n        },\n        {\n        \"abbr\": \"VI\",\n        \"name\": \"Virgin Islands\",\n        \"capital\": \"Charlotte Amalie\",\n        \"lat\": \"18.3419\",\n        \"long\": \"-64.9307\"\n        },\n        {\n        \"abbr\": \"PR\",\n        \"name\": \"Puerto Rico\",\n        \"capital\": \"San Juan\",\n        \"lat\": \"18.466333\",\n        \"long\": \"-66.105721\"\n        }]\n\n    state_geo_cap_add = pd.DataFrame.from_dict(state_geo_cap_add)\n    state_geo = pd.concat((state_geo_cap, state_geo_cap_add), axis = 0)\n    \n    return state_geo\n\ndef get_weather(case_data, state_geo, start_date='2020-03-07', end_date='2020-04-07', mode = 'read'):\n    \n    if (mode == 'load'):\n        final_weather = pd.DataFrame()\n        states = case_data['Province\/State'].unique()[:-2]\n        for st in tqdm(states):\n\n            lat = state_geo[state_geo['name'] == st]['lat'].unique()\n            lng = state_geo[state_geo['name'] == st]['long'].unique()\n\n            stations = list()\n            for (lat, lng) in list(zip(lat, lng)):\n                try:\n                    data_url = 'https:\/\/api.meteostat.net\/v1\/stations\/nearby?lat=' + str(lat) + '&lon=' + str(lng) + '&limit=3&key=JNA6UgJF'\n                    response = requests.get(data_url)\n                    file_object = io.StringIO(response.content.decode('utf-8'))\n                    stations += list(pd.DataFrame(json.load(file_object)['data'])['id'].unique())\n                except Exception:\n                    continue\n\n            station_weather = pd.DataFrame()\n            for station_id in list(stations):\n                data_url = 'https:\/\/api.meteostat.net\/v1\/history\/daily?station=' + str(station_id) + '&start=' + str(start_date) + '&end=' + str(end_date) + '&key=JNA6UgJF'\n                response = requests.get(data_url)\n                file_object = io.StringIO(response.content.decode('utf-8'))\n                if (station_weather.empty):\n                    try:\n                        station_weather = pd.DataFrame(json.load(file_object)['data'])\n                    except Exception:\n                        print(station_id)\n                        continue\n                else:\n                      station_weather = pd.concat((station_weather, pd.DataFrame(json.load(file_object)['data'])), axis = 0)\n\n\n            station_weather = station_weather.set_index('date')\n            station_weather = station_weather.astype('float')\n            station_weather = station_weather.groupby('date').median()\n\n            if (final_weather.empty):\n                final_weather = station_weather\n                final_weather['Province\/State'] = st\n            else:\n                station_weather['Province\/State'] = st\n                final_weather = pd.concat((final_weather, station_weather), axis = 0)\n                \n        final_weather.to_csv('weather_0703_0704.csv', index = False)\n        \n    else:\n        \n        final_weather = pd.read_csv('\/kaggle\/input\/weather\/weather_0703_0704.csv')\n\n    return final_weather\n\ndef drop_constant_column(dataframe):\n    \"\"\"\n    Drops constant value columns of pandas dataframe.\n    \"\"\"\n    return dataframe.loc[:, (dataframe != dataframe.iloc[0]).any()]\n\ndef get_dp05_census_data():\n    dp05 = pd.read_csv('\/kaggle\/input\/dp05-data-with-overlays-v2\/ACSDP1Y2018.DP05_data_with_overlays_2020-04-13T234101.csv')\n    dp05 = pd.DataFrame(dp05.iloc[1:,:].values, columns = [col.replace('!!','_').replace(' ','_').replace('$','') for col in list(dp05.iloc[:1,:].values[0])])\n    dp05_2 = drop_constant_column(dp05.filter(regex='Estimate').replace('N', np.nan)).astype('float')\n    dp05_2['Geographic_Area_Name'] = dp05['Geographic_Area_Name']\n    return dp05_2\n\ndef get_dp03_census_data():\n\n    dp03 = pd.read_csv('\/kaggle\/input\/dp03-data-with-overlays\/ACSDP1Y2018.DP03_data_with_overlays_2020-04-08T153457.csv')\n    dp03 = pd.DataFrame(dp03.iloc[1:,:].values, columns = [col.replace('!!','_').replace(' ','_').replace('$','') for col in list(dp03.iloc[:1,:].values[0])])\n\n    dp03_2 = drop_constant_column(dp03.filter(regex='Estimate').replace('N', np.nan)).astype('float')\n    dp03_2['Geographic_Area_Name'] = dp03['Geographic_Area_Name']\n    \n    return dp03_2\n\ndef get_google_mob_report(date):\n    \n    states = ['Alabama', 'Alaska', 'Arizona', 'Arkansas', 'California',\n       'Colorado', 'Connecticut', 'Delaware', 'District of Columbia',\n       'Florida', 'Georgia', 'Hawaii', 'Idaho', 'Illinois', 'Indiana',\n       'Iowa', 'Kansas', 'Kentucky', 'Louisiana', 'Maine', 'Maryland',\n       'Massachusetts', 'Michigan', 'Minnesota', 'Mississippi',\n       'Missouri', 'Montana', 'Nebraska', 'Nevada', 'New Hampshire',\n       'New Jersey', 'New Mexico', 'New York', 'North Carolina',\n       'North Dakota', 'Ohio', 'Oklahoma', 'Oregon', 'Pennsylvania',\n       'Rhode Island', 'South Carolina', 'South Dakota', 'Tennessee',\n       'Texas', 'Utah', 'Vermont', 'Virginia', 'Washington',\n       'West Virginia', 'Wisconsin', 'Wyoming']\n    mob_val = np.zeros((len(states), 6))\n    mob_col = ['Retail_and_Recreation', 'Grocery_and_Pharmacy', 'Parks', 'Transit_Stations', 'Workplaces', 'Residential']\n    k = 0\n\n    for st in states:\n\n        raw = parser.from_file(\"https:\/\/www.gstatic.com\/covid19\/mobility\/\" + str(date) + \"_US_\" + st.replace(' ','_') + \"_Mobility_Report_en.pdf\")\n        text = raw['content']\n        text = text.replace('\\n', '')\n        mob_val[k, :] = np.array([int(num.replace('%c','')) for num in re.findall(r'-?\\d+%c', text)])\n        k += 1\n\n    mob = pd.DataFrame(mob_val, columns = mob_col)\n    mob['state'] = states\n    mob['state']=mob['state'].str.replace('of','Of')\n    return mob\n\ndef get_lock_dates():\n    \n    data_path = '\/kaggle\/input\/us-lockdown-dates-dataset\/lockdown_us.csv'\n\n    lock_dates = pd.read_csv(data_path, parse_dates=['Date'])\n    lock_dates['Date_Num']=lock_dates['Date'].dt.strftime('%Y%m%d').astype(int)\n    lock_dates = pd.to_datetime(lock_dates.groupby('State')['Date_Num'].max(), format='%Y%m%d').reset_index()\n    \n    return lock_dates\n\ndef get_containment_data():\n    \n    cont_mit_data = pd.read_csv('\/kaggle\/input\/covid19-containment-and-mitigation-measures\/COVID 19 Containment measures data.csv', parse_dates = ['Date Start', 'Date end intended'])\n    cont_mit_data = cont_mit_data[cont_mit_data['Country'].fillna('').str.contains('US')].sort_values(by='Date Start', ascending = True)\n    cont_mit_data['Date_Keywords'] = '{' + cont_mit_data['Date Start'].astype('str') + ' : ' + cont_mit_data['Keywords'] + '}'\n    cont_mit_data['Date_Measure'] = '{' + cont_mit_data['Date Start'].astype('str') + ': ' + cont_mit_data['Description of measure implemented'] + '}'\n    cont_mit_data = cont_mit_data[['Country','Date_Keywords', 'Date_Measure']].astype('str').groupby('Country').agg(', '.join)\n    cont_mit_data = cont_mit_data.reset_index()\n    cont_mit_data['state'] = cont_mit_data.reset_index()['Country'].str.replace('US:','').str.strip()\n    cont_mit_data['state']= cont_mit_data['state'].str.replace('of','Of')\n    \n    return cont_mit_data[['state','Date_Keywords', 'Date_Measure']].sort_values(by='state', ascending = True)\n\ndef calc_log_cumulative_tests(test_data, state_data):\n    test_targ = test_data[(test_data['date']>20200307)].groupby(['state', 'date'])[['positive', 'negative']].sum()\n\n    test_targ['test_rate'] = np.log1p(test_targ['negative']) \n    test_targ['inf_rate'] = np.log1p(test_targ['positive']) \n    test_targ = test_targ.reset_index()\n    \n    return test_targ\n\n\ndef plot_log_cumulative_tests(test_targ, lock_dates, state_data):\n    plotting = test_targ.groupby('state').first()\n\n    for state in plotting.index:\n        \n        fig = plt.figure(figsize=(20,10))\n        ax = fig.add_subplot(221)\n\n        dates = test_targ[(test_targ['state']==state)]['date']\n        dates = pd.to_datetime(dates, format = '%Y%m%d')\n        levels = test_targ[(test_targ['state']==state)]['inf_rate']\n\n        # Configure x-ticks\n        ax.plot(dates, levels, ls='-', marker='o', color='red', label = 'positive')\n\n        group = state\n\n        ax.set_title('Rates ' + state_data.loc[state].iloc[0])\n\n        ax.grid(True)\n\n        dates = test_targ[(test_targ['state']==state)]['date']\n        dates = pd.to_datetime(dates, format = '%Y%m%d')\n        levels = test_targ[(test_targ['state']==state)]['test_rate']\n        # Configure x-ticks\n        ax.plot(dates, levels, ls='-', marker='o', color='green', label = 'negative')\n\n        try:\n            province_state = state_data.loc[state].iloc[0]\n            xposition = [lock_dates[lock_dates['State']==province_state]['Date_Num'].iloc[0]]\n            plt.axvline(x=xposition, color='k', linestyle='--')\n        except IndexError:\n            pass\n\n        group = state\n\n        ax.grid(True)\n        ax.legend(loc='best')\n\n        # Format the x-axis for dates (label formatting, rotation)\n        fig.autofmt_xdate(rotation=45)\n        fig.tight_layout()\n\n    fig.show()\n    plt.show()\n    \ndef calculate_growth_rate_slope(test_targ, change_date = 20200329):\n\n    test_targ = test_targ[test_targ['positive']>0]\n    test_targ['rank'] = test_targ.groupby('state')['date'].rank()\n    new_df3 = pd.DataFrame()\n    new_df3['inf_rate_inc'] = test_targ[(test_targ['date']<=change_date)].groupby('state').apply(lambda v: linregress(v['rank'], v['inf_rate'])[0]) * 14\n    new_df3['inf_rate_dec'] = test_targ[(test_targ['date']>change_date)].groupby('state').apply(lambda v: linregress(v['rank'], v['inf_rate'])[0]) * 14\n\n    new_df3.reset_index(inplace=True)\n    \n    return new_df3\n\ndef func(breakpoints, x, y, fcache):\n    breakpoints = tuple(map(int, sorted(breakpoints)))\n    if breakpoints not in fcache:\n        total_error = 0\n        for f, xi, yi, coefs in find_best_piecewise_polynomial(breakpoints, x, y):\n            total_error += ((f(xi) - yi)**2).sum()\n        fcache[breakpoints] = total_error\n    # print('{} --> {}'.format(breakpoints, fcache[breakpoints]))\n    return fcache[breakpoints]\n\ndef find_best_piecewise_polynomial(breakpoints, x, y):\n    breakpoints = tuple(map(int, sorted(breakpoints)))\n    xs = np.split(x, breakpoints)\n    ys = np.split(y, breakpoints)\n    result = []\n    for xi, yi in zip(xs, ys):\n        if len(xi) < 2: continue\n        coefs = npoly.polyfit(xi, yi, 1)\n        f = npoly.Polynomial(coefs)\n        result.append([f, xi, yi, coefs])\n    return result\n    \n\ndef plot_log_cumulative_tests_with_change(test_targ, lock_dates, state_data):\n    \n    st_list = test_targ['state'].unique()\n    st_dec = np.zeros((len(st_list)))\n    st_inc = np.zeros((len(st_list)))\n    k = 0\n\n    for st in tqdm(st_list):\n\n        y = test_targ[test_targ['state']==st][test_targ['positive']>0]['inf_rate'].values\n        x = test_targ[test_targ['state']==st][test_targ['positive']>0]['date'].values\n\n        if len(y)<2:\n            continue\n        \n        num_breakpoints = 1\n        breakpoints = [optimize.brute(\n            func, [slice(1, len(x), 1)]*num_breakpoints, args=(x, y, {}), finish=None)]\n\n        try:\n            province_state = state_data.loc[st].iloc[0]\n            dt = int(lock_dates[lock_dates['State']==province_state]['Date_Num'].iloc[0].strftime('%Y%m%d'))\n            xposition = np.where(x==dt)[0]\n            ax.axvline(x=xposition, color='k', linestyle='--')\n        except Exception:\n            pass\n\n        x = np.arange(0, len(x))\n\n        plt.scatter(x, y, c='blue', s=50, label = 'positive')\n        i = 0\n        for f, xi, yi, coefs in find_best_piecewise_polynomial(breakpoints, x, y):\n            x_interval = np.array([xi.min(), xi.max()])\n            print('y = {:35s}, if x in [{}, {}]'.format(str(f), *x_interval))\n            if (i == 1):\n                label = ''\n            else:\n                label = 'linear_fit'\n            plt.plot(x_interval, f(x_interval), 'ro-', label = label)\n            \n            if (i == 0):\n                st_inc[k] = coefs[1]*14\n                prev0 = len(yi)\n                prev1 = yi[-1:]\n            else:\n                st_dec[k] = coefs[1]*14\n    \n            i+=1\n\n        plt.grid(True)\n        plt.legend(loc='best')\n        plt.title('Rate change '+ state_data.loc[st].iloc[0])\n\n        k += 1\n    \n        plt.show()\n\n    new_df = pd.DataFrame(st_dec, columns = ['inf_rate_dec'])   \n    new_df['inf_rate_inc'] = st_inc\n    new_df['state'] = st_list\n    return new_df\n\ndef run_variable_selection(growth_rate, mob2, columns, NFOLDS=3, RANDOM_STATE=42, N_REPEATS=30):\n\n\n    growth_rate['Province\/State'] = growth_rate['state'].map(state_data.iloc[:,0])\n    new_df3 = pd.merge(growth_rate, mob2, left_on = 'Province\/State', right_on = 'state', how='inner')\n    cols = list(new_df3.select_dtypes('float').columns)\n    cols = ['Retail_and_Recreation', 'Residential',\n           'Workplaces', 'inf_rate_dec', 'inf_rate_inc']\n    new_df3 = new_df3[new_df3['inf_rate_inc']>0]\n\n    Y = new_df3[cols]['inf_rate_dec'] \n    X = new_df3[cols].drop(['inf_rate_inc', 'inf_rate_dec'], axis = 1)\n    X = X.apply(lambda x: x.fillna(x.median()))\n    use_cols = list(X.columns)\n\n    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=1, random_state=RANDOM_STATE)\n    X_train_enc_sc = scale(X_train)\n    X_test_enc_sc = scale(X_test)\n\n    NFOLDS =  NFOLDS\n    RANDOM_STATE = RANDOM_STATE\n    N_REPEATS = N_REPEATS\n\n    clfs = []\n    mae_mod = []\n    mae_bas = []\n    folds = RepeatedKFold(n_splits=NFOLDS, n_repeats = N_REPEATS, random_state=RANDOM_STATE)\n    oof_preds_mod = np.zeros((len(X_train), 1))\n    test_preds_mod = np.zeros((len(X_test), 1))\n    oof_preds_bas = np.zeros((len(X_train), 1))\n    test_preds_bas = np.zeros((len(X_test), 1))\n    Y_train = Y_train.reset_index(drop=True)\n\n    for fold_, (trn_, val_) in tqdm(enumerate(folds.split(X_train, Y_train))):\n        # print(\"Current Fold: {}\".format(fold_))\n        X_trn, Y_trn = X_train.iloc[trn_, :], Y_train[trn_]\n        X_val, Y_val = X_train.iloc[val_, :], Y_train[val_]\n\n        X_trn_enc_sc = scale(X_trn)\n        X_val_enc_sc = scale(X_val)\n\n        clf = LinearRegression()\n        clf.fit(X_trn_enc_sc, Y_trn)\n        val_pred = clf.predict(X_val_enc_sc)\n\n        X_test_enc_sc = pd.DataFrame(X_test_enc_sc, columns = use_cols)\n        test_fold_pred = clf.predict(X_test_enc_sc)\n\n        mae_mod.append(metrics.mean_absolute_error(Y_val, val_pred))\n        mae_bas.append(metrics.mean_absolute_error(Y_val, np.repeat(np.mean(Y_trn), len(val_))))\n        oof_preds_mod[val_, :] = val_pred.reshape((-1, 1))\n        oof_preds_bas[val_, :] = np.repeat(np.mean(Y_trn), len(val_)).reshape((-1, 1))\n        test_preds_mod += test_fold_pred.reshape((-1,1))\n        test_preds_bas += np.repeat(np.mean(Y_trn), len(X_test_enc_sc)).reshape((-1, 1))\n\n    test_preds_mod \/= NFOLDS * N_REPEATS\n    test_preds_bas \/= NFOLDS * N_REPEATS\n    print('')\n\n    mae_score_cv_mod = np.round(metrics.mean_absolute_error(Y_train, oof_preds_mod.ravel()),4)\n    print('')\n    print(\"MAE OOF Model = {}\".format(np.round(mae_score_cv_mod, 4)))\n    print(\"MAE CV Model = {}\".format(np.round(np.mean(mae_mod), 4)))\n    print(\"MAE STD Model = {}\".format(np.round(np.std(mae_mod),4)))\n\n    mae_score_cv_bas = np.round(metrics.mean_absolute_error(Y_train, oof_preds_bas.ravel()),4)\n    print('')\n    print(\"MAE OOF Baseline = {}\".format(np.round(mae_score_cv_bas, 4)))\n    print(\"MAE CV Baseline = {}\".format(np.mean(mae_bas)))\n    print(\"MAE STD Baseline = {}\".format(np.round(np.std(mae_bas),4)))\n\n    print('')\n    mae_score_test_mod = np.round(metrics.mean_absolute_error(Y_test, test_preds_mod),4)\n    print(\"MAE Test Model = {}\".format(mae_score_test_mod))\n    mae_score_test_bas = np.round(metrics.mean_absolute_error(Y_test, test_preds_bas),4)\n    print(\"MAE Test Baseline = {}\".format(mae_score_test_bas))\n\n    coefficients = pd.concat([pd.DataFrame(X_train.columns),pd.DataFrame(np.transpose(clf.coef_))], axis = 1)\n    coefficients.columns = ['variable', 'weight']\n    coefficients['percent'] = np.abs(coefficients['weight'])\n    coefficients['percent'] \/= coefficients['percent'].sum()\n    \n    return list(coefficients['variable'].unique()), coefficients\n\ndef run_stat_significance(growth_rate, mob4, columns):\n\n    growth_rate['Province\/State'] = growth_rate['state'].map(state_data.iloc[:,0])\n    mob4['state']=mob4['state'].str.replace('of','Of')\n    new_df3 = pd.merge(growth_rate, (mob4.set_index('state')).reset_index(), left_on = 'Province\/State', right_on = 'state', how='inner')\n    cols = list(new_df3.select_dtypes('float').columns)\n    new_df3 = new_df3[new_df3['inf_rate_inc']>0]\n\n    new_df3['targ'] = (new_df3[cols]['inf_rate_dec'] - new_df3[cols]['inf_rate_inc']) \/ new_df3[cols]['inf_rate_inc']\n    Y = new_df3['targ']\n    X = new_df3[cols].drop(['inf_rate_inc', 'inf_rate_dec'], axis = 1)\n    X = X.apply(lambda x: x.fillna(x.mean()))[columns]\n    use_cols = list(X.columns)\n\n    mod = sm.OLS(Y, X)\n    res = mod.fit()\n\n    return res, X, Y\n\ndef show_correlations(X, Y):\n    vars = []\n    for col in X.columns:\n        if (spearmanr(X[col], Y)[1] < 0.9):\n            print(col)\n            plt.scatter(X[col], Y)\n            plt.ylabel('Growth_rate_decrease')\n            plt.xlabel(col)\n            vars.append(col)\n            print(\"\")\n            plt.show()\n    print(vars)\n    \ndef show_correlations_color_target_work(X, Y):\n\n    fig = plt.figure(figsize=(6,6))\n    ax = fig.add_subplot(111)\n    ax.set_title(\"Workplaces vs Residential AVG\",fontsize=14)\n    ax.set_xlabel(\"Workplaces\",fontsize=12)\n    ax.set_ylabel(\"Residential\",fontsize=12)\n    ax.grid(True,linestyle='-',color='0.75')\n    x = X['Workplaces']\n    y = X['Residential']\n    z = Y\n\n    # scatter with colormap mapping to z value\n    ax.scatter(x,y,s=60,c=z, marker = 'o', cmap = cm.jet );\n\n    plt.show()\n\n    spearmanr(x, y)\n\ndef show_correlations_color_target_retail(X, Y):\n    fig = plt.figure(figsize=(6,6))\n    ax = fig.add_subplot(111)\n    ax.set_title(\"Retail_and_Recreation vs Residential AVG\",fontsize=14)\n    ax.set_xlabel(\"Retail_and_Recreation\",fontsize=12)\n    ax.set_ylabel(\"Residential\",fontsize=12)\n    ax.grid(True,linestyle='-',color='0.75')\n    x = X['Retail_and_Recreation']\n    y = X['Residential']\n    z = Y\n\n    # scatter with colormap mapping to z value\n    ax.scatter(x,y,s=60,c=z, marker = 'o', cmap = cm.jet );\n\n    plt.show()\n\n    spearmanr(x, y)\n    \ndef get_dp03_vars(growth_rate, dp03, pvalue = 0.01):\n    growth_rate['Province\/State'] = growth_rate['state'].map(state_data.iloc[:,0])\n    new_df4 = pd.merge(growth_rate, dp03, left_on = 'Province\/State', right_on = 'Geographic_Area_Name', how='inner')\n    cols = list(new_df4.select_dtypes('float').columns)\n    new_df4 = new_df4[new_df4['inf_rate_dec']>0]\n\n    Y = new_df4[cols]['inf_rate_dec']\n    X = new_df4[cols].drop(['inf_rate_inc', 'inf_rate_dec'], axis = 1)\n    X = X.apply(lambda x: x.fillna(x.mean()))\n    use_cols = list(X.columns)\n\n    vars03 = []\n    for col in X.columns:\n        if (spearmanr(X[col], Y).pvalue < pvalue):\n            print(col)\n            print(spearmanr(X[col], Y))\n            vars03.append(col)\n            print(\"\")\n    print(vars03)\n    return vars03\n\ndef get_dp05_vars(growth_rate, dp05, pvalue = 0.001):\n    \n    growth_rate['Province\/State'] = growth_rate['state'].map(state_data.iloc[:,0])\n    dp05 = dp05.loc[:,~dp05.columns.duplicated()]\n    new_df5 = pd.merge(growth_rate, dp05, left_on = 'Province\/State', right_on = 'Geographic_Area_Name', how='inner')\n    cols = list(new_df5.select_dtypes('float').columns)\n    new_df5 = new_df5[new_df5['inf_rate_dec']>0]\n\n    Y = new_df5[cols]['inf_rate_dec']\n    X = new_df5[cols].drop(['inf_rate_inc', 'inf_rate_dec'], axis = 1)\n    X = X.apply(lambda x: x.fillna(x.mean()))\n    use_cols = list(X.columns)\n\n    vars05 = []\n    for col in X.columns:\n        if (spearmanr(X[col], Y).pvalue < pvalue):\n            print(col)\n            print(spearmanr(X[col], Y))\n            vars05.append(col)\n            print(\"\")\n    print(vars05)\n    return vars05\n\ndef get_weather_vars(growth_rate, final_weather, pvalue = 0.05):\n\n    cols = ['temperature', 'temperature_min', 'temperature_max', 'precipitation', 'snowfall',\n                     'snowdepth', 'winddirection', 'windspeed', 'pressure']\n\n    df_w = final_weather.groupby('Province\/State')[cols].agg([np.mean, np.std, np.max, np.min]).reset_index()\n    df_w.columns = ['_'.join(col).strip() for col in df_w.columns.values]\n    df_w.head(10)\n\n    new_df = growth_rate\n    new_df['Province\/State'] = new_df['state'].map(state_data.iloc[:,0])\n    new_df6 = pd.merge(new_df, df_w, left_on = 'Province\/State', right_on = 'Province\/State_', how='inner')\n    cols = list(new_df6.select_dtypes('float').columns)\n    new_df6 = new_df6[new_df6['inf_rate_dec']>0]\n\n    Y = new_df6[cols]['inf_rate_dec']\n    X = new_df6[cols].drop(['inf_rate_inc', 'inf_rate_dec'], axis = 1)\n    X = X.apply(lambda x: x.fillna(x.mean()))\n    use_cols = list(X.columns)\n\n    vars07 = []\n    for col in X.columns:\n        if (spearmanr(X[col], Y).pvalue < pvalue):\n            print(col)\n            print(spearmanr(X[col], Y))\n            vars07.append(col)\n            print(\"\")\n    print(vars07)\n    return df_w, vars07\n\ndef get_epirank_vars(growth_rate, sc_data, sc_codes):\n\n    sc_agg = sc_data.groupby(['ORIG_STATE', 'DEST_STATE'])[['SHIPMT_VALUE','SHIPMT_WGHT','SHIPMT_DIST_GC', 'SHIPMT_DIST_ROUTED']].median().reset_index()\n    sc_agg['ORIG_ST'] = sc_agg['ORIG_STATE'].map(sc_codes.set_index('StateCode').iloc[:,0])\n    sc_agg['DEST_ST'] = sc_agg['DEST_STATE'].map(sc_codes.set_index('StateCode').iloc[:,0])\n    sc_agg = sc_agg[sc_agg.ORIG_STATE != 0]\n\n    g_val = epirank.make_DiGraph(sc_agg, origin_col='ORIG_ST', destination_col='DEST_ST', flow_col='SHIPMT_VALUE',\n                             largest_connected_component=False, exclude_selfloop=False)\n    g_wght = epirank.make_DiGraph(sc_agg, origin_col='ORIG_ST', destination_col='DEST_ST', flow_col='SHIPMT_WGHT', \n                             largest_connected_component=False, exclude_selfloop=False)\n    g_dist_g = epirank.make_DiGraph(sc_agg, origin_col='ORIG_ST', destination_col='DEST_ST', flow_col='SHIPMT_DIST_GC', \n                             largest_connected_component=False, exclude_selfloop=False)\n    g_dist_r = epirank.make_DiGraph(sc_agg, origin_col='ORIG_ST', destination_col='DEST_ST', flow_col='SHIPMT_DIST_ROUTED', \n                             largest_connected_component=False, exclude_selfloop=False)\n\n\n    epi_val = epirank.run_epirank(g_val, daytime=0.55, d=1.)\n    epi_wght = epirank.run_epirank(g_wght, daytime=0.55, d=1.)\n    epi_dist_g = epirank.run_epirank(g_dist_g, daytime=0.55, d=1.)\n    epi_dist_r = epirank.run_epirank(g_dist_r, daytime=0.55, d=1.)\n\n    epi_data = pd.DataFrame(sc_agg['ORIG_ST'].unique(), columns = ['Province\/State'])\n    epi_data['epirank_val'] = epi_data['Province\/State'].map(epi_val)\n    epi_data['epirank_wght'] = epi_data['Province\/State'].map(epi_wght)\n    epi_data['epirank_dist_r'] = epi_data['Province\/State'].map(epi_dist_r)\n    epi_data['epirank_dist_g'] = epi_data['Province\/State'].map(epi_dist_g)\n\n    new_df3 = growth_rate\n    new_df3['Province\/State'] = new_df3['state'].map(state_data.iloc[:,0])\n    new_df7 = pd.merge(new_df3, epi_data, left_on = 'Province\/State', right_on = 'Province\/State', how='inner')\n    cols = list(new_df7.select_dtypes('float').columns)\n    new_df7 = new_df7[new_df7['inf_rate_dec']>0]\n\n    Y = new_df7[cols]['inf_rate_dec']\n    X = new_df7[cols].drop(['inf_rate_inc', 'inf_rate_dec'], axis = 1)\n    X = X.apply(lambda x: x.fillna(x.mean()))\n    use_cols = list(X.columns)\n\n    vars09 = []\n    for col in X.columns:\n        if (spearmanr(X[col], Y).pvalue < 0.01):\n            print(col)\n            print(spearmanr(X[col], Y))\n            vars09.append(col)\n            print(\"\")\n    print(vars09)\n    return epi_data, vars09\n\ndef calc_leontieff_inverse(proj_use_ci, proj_make_ic, sec_plan_312):\n    total_industry = proj_use_ci.sum(axis=0)[:205]\n    total_commodity = proj_make_ic.sum(axis=0)[:205]\n    scrap = proj_use_ci.iloc[202, :205]\n\n    U = proj_use_ci.iloc[:205, :205].values\n    g = total_industry.values[:205]\n    V = proj_make_ic.iloc[:205, :205].values\n    q = total_commodity.values[:205]\n    p = np.matmul(scrap.values, np.linalg.pinv(np.diag(g)))\n\n    B = np.matmul(U, np.linalg.pinv(np.diag(g)))\n    D = np.matmul(V, np.linalg.pinv(np.diag(q)))\n    W = np.matmul(np.linalg.inv(np.identity(205)-np.diag(p)), D)\n\n    i2i = np.linalg.inv(np.identity(205)-np.matmul(W,B))\n    c2c = np.linalg.inv(np.identity(205)-np.matmul(B,W))\n\n    return i2i, c2c, g, W\n\ndef get_industry_by_num(i2i, sec_plan_312, N=48):\n    N = N\n    query = sec_plan_312.iloc[N,4]\n    df = pd.DataFrame(pd.DataFrame(i2i).iloc[N,:])\n    df.columns = [query]\n    df = pd.merge(df, sec_plan_312, left_on = df.index, right_on = 'index').sort_values(by=query, ascending = False)[[query, 'SectorTitle']]\n    df[query] = df[query] \/ df[query].sum(axis=0)\n    return df\n\ndef get_ppi_fred_data(sec_plan_312, N=48, start_date = '2019-03-01'):\n\n    naics_code = str(sec_plan_312.iloc[N, :]['NAICS_2017'])\n    fred_code = 'PCU' + naics_code + naics_code\n\n    # %matplotlib inline\n    fred = Fred(api_key='48812f25f6b377b35d3bb913269ce624')\n    data = fred.get_series(fred_code)\n\n    dct = {111:'WPU01', \n           112:'WPU01', \n           5615:'WPU473', \n           1131:'PCU11331133', \n           1132:'PCU11331133', \n           114:'PCU31171031171021', \n           2213:'PCU3121123121120', \n           23:'WPUSI012011',\n           115: 'WPU01',\n           3116: 'PCU3116131161',\n           42: 'WPU578101',\n           485: 'WPU302',\n           486: 'PCU486110486110312',\n           487: 'WPU302',\n           488: 'PCU488190488190P',\n           512: 'REV512TAXABL144QSA'\n           }\n\n    final_df = pd.DataFrame()\n\n    for code in tqdm(sec_plan_312.iloc[:206,:]['NAICS_2017'].unique()):\n        naics_code = str(code).split('-', 1)[0]\n        fred_code = 'PCU' + naics_code + naics_code\n        if fred_code.find(',') != -1:\n            code_list = naics_code.split(',')\n            for c in code_list:\n                c = c.split('-', 1)[0]\n                fred_code = 'PCU' + c.strip() + c.strip()\n                try: \n                    df = pd.DataFrame(fred.get_series(fred_code, observation_start = start_date))\n                    df.columns = ['value']\n                    df['code'] = c\n                    if (final_df.empty):\n                        final_df = df\n                    else:\n                        final_df = pd.concat((final_df, df), axis=0)\n                except ValueError:\n                    try:\n                        df = pd.DataFrame(fred.get_series(dct.get(int(c.strip())), observation_start = start_date))\n                        df.columns = ['value']\n                        df['code'] = c\n                        if (final_df.empty):\n                            final_df = df\n                        else:\n                            final_df = pd.concat((final_df, df), axis=0)\n                    except ValueError:\n                          pass\n    #                     print(\"\")\n    #                     print(sec_plan_312[sec_plan_312['NAICS_2017'] == naics_code]['SectorTitle'])\n        else:\n            try:\n                    df = pd.DataFrame(fred.get_series(fred_code, observation_start = start_date))\n                    df.columns = ['value']\n                    df['code'] = naics_code\n                    if (final_df.empty):\n                         final_df = df\n                    else:\n                         final_df = pd.concat((final_df, df), axis=0)\n            except ValueError:\n                    try:\n                        df = pd.DataFrame(fred.get_series(dct.get(int(code)), observation_start = start_date))\n                        df.columns = ['value']\n                        df['code'] = naics_code\n                        if (final_df.empty):\n                             final_df = df\n                        else:\n                             final_df = pd.concat((final_df, df), axis=0)\n                    except ValueError:\n                        pass\n    #                     print(sec_plan_312[sec_plan_312['NAICS_2017'] == naics_code]['SectorTitle'])\n\n    return final_df\n\ndef calc_industry_slope(final_df):\n\n    final_df2 = final_df.reset_index()\n    final_df2.columns = ['date', 'value', 'code']\n    final_df2['rank'] = final_df2.groupby('code')['date'].rank()\n\n    new_df3 = pd.DataFrame()\n    median = final_df2.groupby('code')['value'].median()\n    slope = (final_df2.groupby('code').apply(lambda v: linregress(v['rank'], np.log1p(v['value']))[0])) * 12\n    new_df3['value_dec'] = slope\n\n    new_df3.reset_index(inplace=True)\n    return new_df3\n\ndef show_industry_effects(sec_plan_312, industry_slope, i2i, N=48):\n\n    def chainer(s):\n        return list(chain.from_iterable(s.str.split(',')))\n    lens = sec_plan_312['NAICS_2017'].astype('str').str.split(',').map(len)\n    res = pd.DataFrame({'NAICS_2017': np.repeat(sec_plan_312['NAICS_2017'], lens),\n                        'NAICS_2017_split': chainer(sec_plan_312['NAICS_2017'].astype(str))})\n\n\n    N = N\n    sec_plan_312_v2 = pd.merge(sec_plan_312, res, left_on = 'NAICS_2017', right_on = 'NAICS_2017', how='left')\n    sec_plan_312_v2 = pd.merge(sec_plan_312_v2, industry_slope, left_on = 'NAICS_2017_split', right_on = 'code', how='left')\n    sec_plan_312_v2 = sec_plan_312_v2.groupby(['NAICS_2017'])[['value_dec']].mean().reset_index()\n    sec_plan_312_v2 = pd.merge(sec_plan_312, sec_plan_312_v2, on ='NAICS_2017', how='left')\n\n    query = sec_plan_312_v2[sec_plan_312_v2['index'] == N].iloc[0,4]\n    df = pd.DataFrame(pd.DataFrame(i2i).iloc[N,:])\n    df.columns = [query]\n    df = pd.merge(df, sec_plan_312_v2, left_on = df.index, right_on = 'index').sort_values(by=query, ascending = False)[[query, 'SectorTitle', 'value_dec']]\n    df[query] = df[query] \/ df[query].sum(axis=0)\n    return df, sec_plan_312_v2\n\n\ndef show_industry_network_effects(proj_use_ci, proj_make_ic, sec_plan_312_v2, c2c, g, W, N=48):\n    e = proj_use_ci.iloc[:205, 205]\n    dec = (1 + sec_plan_312_v2['value_dec'].fillna(0))[:205]\n    e = np.matmul(e.T, np.diag(dec))\n    g1 = np.matmul(W, np.matmul(c2c, e))\n\n    g_206 = np.zeros(206)\n    g_bef = np.zeros(206)\n    g_aft = np.zeros(206)\n    g_bef[:205] = g\n    g_aft[:205] = g1\n    g_206[:205] = g1 \/ g\n    sec_plan_312_v2['industry_dec'] = g_206\n    sec_plan_312_v2['industry_after'] = g_aft\n    sec_plan_312_v2['industry_before'] = g_bef\n\n    N = N\n    query = sec_plan_312.iloc[N,4]\n    df = pd.DataFrame(pd.DataFrame(i2i).iloc[N,:])\n    df.columns = [query]\n    df = pd.merge(df, sec_plan_312_v2, left_on = df.index, right_on = 'index').sort_values(by=query, ascending = False)[[query, 'SectorTitle', 'industry_before', 'industry_after','value_dec', 'industry_dec']]\n    df[query] = df[query] \/ df[query].sum(axis=0)\n    df['harmonic_mean'] = (df['industry_dec']-1)*df.iloc[:,0]\/((df['industry_dec']-1) + df.iloc[:,0])\n    return df\n\n\ndef map_naics_to_supply_chain(df1,df2,df3):\n    # return list from series of comma-separated strings\n    def chainer(s):\n        return list(chain.from_iterable(s.str.split('\/')))\n\n    # calculate lengths of splits\n    lens = df3['Mapping_NAICS_2017'].astype('str').str.split('\/').map(len)\n\n\n    res = pd.DataFrame({'NAICS': np.repeat(df3['NAICS'], lens),\n                        'NAICS_2017': chainer(df3['Mapping_NAICS_2017'].astype(str))})\n\n    df4 = pd.merge(res, df2, on ='NAICS_2017')\n    df5 = pd.merge(df1, df4, on = 'NAICS')\n    df5['value_dec'] = df5['value_dec'] * df5['industry_before']\n    df5['industry_dec'] = df5['industry_dec'] * df5['industry_before']\n    df5 = df5.groupby(['NAICS'])[['industry_before', 'industry_after','value_dec','industry_dec']].sum()\n    df5['value_dec'] = df5['value_dec'] \/ df5['industry_before']\n    df5['industry_dec'] = df5['industry_dec'] \/ df5['industry_before']\n    df5 = df5.groupby(['NAICS'])[['industry_before', 'industry_after','value_dec','industry_dec']].median()\n\n    return df5\n\ndef get_supply_chain_industry_features(sc_data, sc_codes, df5):\n    sc_ind = sc_data.groupby(['ORIG_STATE', 'NAICS'])[['SHIPMT_VALUE']].sum().reset_index()\n    sc_ind['ORIG_ST'] = sc_ind['ORIG_STATE'].map(sc_codes.set_index('StateCode').iloc[:,0])\n    sc_ind['NAICS'] = sc_ind['NAICS']\n    sc_ind = sc_ind[sc_ind.ORIG_STATE != 0]\n    sc_est = pd.merge(sc_ind, df5, left_on = 'NAICS', right_on = 'NAICS', how = 'left')\n    sc_est['value_dec'] = sc_est['value_dec'] * sc_est['SHIPMT_VALUE']\n    sc_est['industry_dec'] = sc_est['industry_dec'] * sc_est['SHIPMT_VALUE']\n    sc_est = sc_est.groupby('ORIG_ST')[['SHIPMT_VALUE','value_dec','industry_dec']].sum().reset_index()\n    sc_est['value_dec'] = sc_est['value_dec'] \/ sc_est['SHIPMT_VALUE']\n    sc_est['industry_dec'] = sc_est['industry_dec'] \/ sc_est['SHIPMT_VALUE']\n    sc_est['SHIPMT_VALUE'] = sc_est['SHIPMT_VALUE'].astype(int)\n    return sc_est\n\n\ndef train_explanatory_model(growth_rate, mob2, dp03, dp05, df_weather, epi_data, mob_cols, vars03, vars05, vars07, vars09, N_FOLDS = 3, RANDOM_STATE = 42, N_REPEATS = 30, ALPHA = 100):\n\n    new_df3 = growth_rate.reset_index()\n    new_df3.columns = ['Province\/State', 'inf_rate_dec']\n    sc_est['ORIG_ST']=sc_est['ORIG_ST'].str.replace('of','Of').str.replace('sana','siana').str.strip()\n    dp03['Geographic_Area_Name']=dp03['Geographic_Area_Name'].str.replace('of','Of')\n    dp05['Geographic_Area_Name']=dp05['Geographic_Area_Name'].str.replace('of','Of')\n    df_weather['Province\/State_']=df_weather['Province\/State_'].str.replace('of','Of')\n    epi_data['Province\/State_']=epi_data['Province\/State'].str.replace('of','Of')\n\n    new_df3 = pd.merge(new_df3, mob2[mob_cols], left_on = 'Province\/State', right_on = 'state', how='inner')\n    new_df3 = pd.merge(new_df3, dp03[vars03 + ['Geographic_Area_Name']], left_on = 'Province\/State', right_on = 'Geographic_Area_Name', how='left')\n    new_df3 = pd.merge(new_df3, dp05[vars05 + ['Geographic_Area_Name']], left_on = 'Province\/State', right_on = 'Geographic_Area_Name', how='left')\n    new_df3 = pd.merge(new_df3, df_weather[vars07 + ['Province\/State_']], left_on = 'Province\/State', right_on = 'Province\/State_', how='left')\n    new_df3 = pd.merge(new_df3, epi_data[vars09 + ['Province\/State']], left_on = 'Province\/State', right_on = 'Province\/State', how='left')\n    # new_df3 = pd.merge(new_df3, sc_est, left_on = 'Province\/State', right_on = 'ORIG_ST', how='left')\n    subway = ['New York', 'Illinois', 'Massachusetts', 'District Of Columbia', 'California', 'Florida', 'Pennsylvania', 'Georgia', 'Maryland', 'Ohio']\n    new_df3['has_subway']= new_df3['Province\/State'].isin(subway).astype(float)\n\n    cols = list(new_df3.select_dtypes('float').columns)\n\n    Y = new_df3[cols]['inf_rate_dec'] \n    X = new_df3[cols].drop(['inf_rate_dec'], axis = 1)\n    X = X.apply(lambda x: x.fillna(x.median()))\n    use_cols = list(X.columns)\n\n    Y_train = Y\n    X_train = X\n\n\n    N_FOLDS = N_FOLDS\n    RANDOM_STATE = RANDOM_STATE\n    N_REPEATS = N_REPEATS\n\n    clfs = []\n    mae_mod = []\n    mae_bas = []\n    folds = RepeatedKFold(n_splits=N_FOLDS, n_repeats = N_REPEATS, random_state=RANDOM_STATE)\n    oof_preds_mod = np.zeros((len(X), 1))\n    oof_preds_bas = np.zeros((len(X), 1))\n    Y_train = Y_train.reset_index(drop=True)\n\n    for fold_, (trn_, val_) in tqdm(enumerate(folds.split(X_train, Y_train))):\n        # print(\"Current Fold: {}\".format(fold_))\n        X_trn, Y_trn = X_train.iloc[trn_, :], Y_train[trn_]\n        X_val, Y_val = X_train.iloc[val_, :], Y_train[val_]\n\n        X_trn_enc_sc = scale(X_trn)\n        X_val_enc_sc = scale(X_val)\n\n    #    clf = LinearRegression()\n    #     clf = ElasticNet(alpha=0.001, l1_ratio=0.5, random_state = 42)\n        clf = Ridge(alpha = ALPHA, random_state = RANDOM_STATE)\n        clf.fit(X_trn_enc_sc, Y_trn)\n\n        # clf = sm.GLM(Y_trn, X_trn_enc_sc, family=sm.families.Poisson())\n        # clf = clf.fit_regularized(alpha = 1)\n\n        val_pred = clf.predict(X_val_enc_sc)\n\n        mae_mod.append(metrics.mean_absolute_error(Y_val, val_pred))\n        mae_bas.append(metrics.mean_absolute_error(Y_val, np.repeat(np.mean(Y_trn), len(val_))))\n        oof_preds_mod[val_, :] = val_pred.reshape((-1, 1))\n        oof_preds_bas[val_, :] = np.repeat(np.mean(Y_trn), len(val_)).reshape((-1, 1))\n\n    print('')\n\n    mae_score_cv_mod = np.round(metrics.mean_absolute_error(Y_train, oof_preds_mod.ravel()),4)\n    print('')\n    print(\"MAE OOF Model = {}\".format(np.round(mae_score_cv_mod, 4)))\n    print(\"MAE CV Model = {}\".format(np.round(np.mean(mae_mod), 4)))\n    print(\"MAE STD Model = {}\".format(np.round(np.std(mae_mod),4)))\n\n    mae_score_cv_bas = np.round(metrics.mean_absolute_error(Y_train, oof_preds_bas.ravel()),4)\n    print('')\n    print(\"MAE OOF Baseline = {}\".format(np.round(mae_score_cv_bas, 4)))\n    print(\"MAE CV Baseline = {}\".format(np.round(np.mean(mae_bas),4)))\n    print(\"MAE STD Baseline = {}\".format(np.round(np.std(mae_bas),4)))\n\n    coefficients = pd.concat([pd.DataFrame(X_train.columns),pd.DataFrame(np.transpose(clf.coef_))], axis = 1)\n    coefficients.columns = ['variable', 'weight']\n    coefficients['percent'] = np.abs(coefficients['weight'])\n    coefficients['percent'] \/= coefficients['percent'].sum()\n\n    return coefficients, oof_preds_mod, new_df3, X\n\ndef show_feature_importance(coefficients, top = 50):\n    plt.figure(figsize=(20, 10))\n    sns.barplot(x=\"percent\", y=\"variable\", data=coefficients.replace('$','').sort_values(by=\"percent\", ascending=False).head(top))\n    plt.title('Feature Importance')\n    plt.tight_layout()\n    \ndef show_risky_states(all_data, beds_data, test_data, oof_preds_mod, X, sc_est, beta = 0.01):\n\n    beds_data[['StaffedICUBeds', 'ICUBedOccupancyRate']] = beds_data[['StaffedICUBeds', 'ICUBedOccupancyRate']].replace('','0').astype('float')\n    test_agg = test_data[test_data['date'] == np.max(test_data['date'])].groupby('state')[['positive','inIcuCurrently','onVentilatorCurrently']].sum()\n    test_agg['ICUOccupied_act'] = test_agg[['inIcuCurrently','onVentilatorCurrently']].sum(axis=1) \/ beds_data.groupby('State')['StaffedICUBeds'].sum()\n    beds_data['ICUOccupied_hist'] = beds_data['StaffedICUBeds'] * beds_data['ICUBedOccupancyRate']\n    beds_agg = (beds_data.groupby('State')['ICUOccupied_hist'].sum() \/ beds_data.groupby('State')['StaffedICUBeds'].sum())\n    beds_agg = pd.concat((beds_agg, test_agg[['ICUOccupied_act', 'positive']]), axis=1).reset_index()\n    beds_agg.columns = ['State','icu_occup_hist', 'icu_occup_act', 'positive']\n    beds_agg['Province\/State'] = beds_agg['State'].map(state_data.iloc[:,0])\n\n    b = beta\n    X['pred']=oof_preds_mod.ravel()\n    X['Province\/State']=all_data['Province\/State']\n    X1 = pd.merge(X, sc_est, left_on = 'Province\/State', right_on = 'ORIG_ST', how='left')\n    X2 = pd.merge(X1, beds_agg[['Province\/State', 'icu_occup_act', 'icu_occup_hist', 'positive']], on = 'Province\/State')\n    X2['epirank'] = X['epirank_dist_r'] * 100\n    X2['epirank_risk_place'] = X2['epirank'].rank().astype(int)\n    X2['harmonic_mean']=(1+b*b)*(X2['pred'])*X2['icu_occup_act'] \/ (b*b*(X2['pred'])+X2['icu_occup_act'])\n    X2['economy_shock_place']=np.round(1+X1['value_dec'],4).rank().astype(int)\n    X2['economy_year_place']=np.round(X1['industry_dec'],4).rank().astype(int)\n    return X2[['Province\/State','pred', 'icu_occup_act', 'icu_occup_hist', 'harmonic_mean', 'positive', 'epirank_risk_place', 'economy_shock_place', 'economy_year_place']].sort_values(by='harmonic_mean', ascending = False)\n\ndef build_what_if_model():\n    md = sm.MixedLM.from_formula(\"inf_rate ~ week + is_locked + Retail_and_Recreation + Workplaces + Residential\", df, groups=df[\"state\"], re_formula=\"~week\")\n    mdf = md.fit()\n    return mdf, md\n\ndef run_simulation(test_targ, df3, mdf, md, DAYS_FORECAST, INCREASE_WORK_RECREATION, INCREASE_RESIDENTIAL):\n\n    test = df3.set_index('state')\n    test['week'] = 1\/14 * int(np.array(DAYS_FORECAST).clip(1,7))\n    low_w = np.min(test[['Retail_and_Recreation', 'Workplaces']])\n    upp_w = np.max(test[['Retail_and_Recreation', 'Workplaces']])\n    test[['Retail_and_Recreation', 'Workplaces']] = (test[['Retail_and_Recreation', 'Workplaces']] * (1 - INCREASE_WORK_RECREATION)).clip(low_w, upp_w, axis = 1)\n    low_r = np.min(test['Residential'])\n    upp_r = np.max(test['Residential'])\n    test['Residential'] = (test['Residential'] * (1 + INCREASE_RESIDENTIAL)).clip(low_r, upp_r)\n    test = pd.concat((test, pd.DataFrame(mdf.random_effects).T), axis = 1)\n    test.insert(1, 'Intercept', np.ones(len(test)))\n    test['pred'] = md.predict(mdf.fe_params, test.iloc[:,1:-2])\n    test['pred'] = test['pred'] + test['Group'] + test.iloc[:,-2]*test.iloc[:,2]\n\n    test_num = test_targ[(test_targ['date']==DATE_FORECAST)][['state','positive']].set_index('state')\n    test_num = test_num.reset_index()\n    test_num.columns = ['state', 'positive_act']\n    prev_num = test_targ[test_targ['date']==DATE_LAST_USED][['state', 'positive']]\n    prev_num.columns = ['state', 'positive_pred']\n    test_num = pd.merge(test_num, prev_num, on='state')\n\n    test_num['state'] = test_num.reset_index()['state'].map(state_data.iloc[:,0])\n    test_num = test_num.set_index('state')\n    test = pd.concat((test, test_num), axis = 1)\n    test['positive_pred'] = np.expm1(np.log1p(test['positive_pred'] * test['pred']))\n    test = test[~test['positive_pred'].isna()]\n    mae = metrics.mean_absolute_error(np.log1p(test['positive_act']), np.log1p(test['positive_pred']))\n    test['positive_inc']=test['positive_pred'] - test['positive_act']\n    print('Positive Inc:' + str(np.round(test['positive_inc'].sum(),2)) + ' +\/- ' + str(np.round(mae*100,2)) + '%')\n    return test\n\ndef find_median_states_for_experiment(X2, test):\n\n    df6 = pd.concat((X2.set_index('Province\/State'), test), axis=1)\n    df6 = df6.select_dtypes(exclude=['O'])\n    neigh = NearestNeighbors(5)\n    neigh.fit(df6)\n    nb = neigh.kneighbors(df6.median().values.reshape(1,-1), 5, return_distance=True)\n    df7 = df6.iloc[nb[1][0], :]\n    df7['distance'] = np.round(nb[0][0])\n    df8 = pd.DataFrame(df6.median()).T\n    df8.index = ['Median']\n    df8['distance']=0\n    df9 = pd.concat((df7,df8), axis=0)\n    return df9\n\ndef prepare_what_if_data(test_targ, mob, mob2, mob4, lock_dates, state_data):\n\n    test_targ = test_targ[test_targ['positive']>0]\n    test_targ['rank'] = test_targ.groupby('state')['date'].rank()\n    lock_dates['Date_Num'] = lock_dates['Date_Num'].dt.strftime('%Y%m%d').astype(int)\n\n    df1 = pd.DataFrame()\n    df1['inf_rate'] = test_targ[(test_targ['date']>20200322) & (test_targ['date']<=20200405)].groupby('state').apply(lambda v: linregress(v['rank'], v['inf_rate'])[0]) * 14\n    df1['week'] = -1\n    df1.reset_index(inplace=True)\n    df1['state'] = df1['state'].map(state_data.iloc[:,0])\n    df1['is_locked'] = df1['state'].isin(lock_dates[(lock_dates['Date_Num']<=20200405)]['State'].unique()).astype(int)\n    mob['state']=mob['state'].str.replace('of','Of')\n\n    df2 = pd.DataFrame()\n    df2['inf_rate'] = test_targ[(test_targ['date']>20200329) & (test_targ['date']<=20200412)].groupby('state').apply(lambda v: linregress(v['rank'], v['inf_rate'])[0]) * 14\n    df2['week'] = 0\n    df2.reset_index(inplace=True)\n    df2['state'] = df2['state'].map(state_data.iloc[:,0])\n    df2['is_locked'] = df2['state'].isin(lock_dates[(lock_dates['Date_Num']<=20200412)]['State'].unique()).astype(int)\n    mob2['state']=mob2['state'].str.replace('of','Of')\n\n    df3 = pd.DataFrame()\n    df3['inf_rate'] = test_targ[(test_targ['date']>20200405) & (test_targ['date']<=20200416)].groupby('state').apply(lambda v: linregress(v['rank'], v['inf_rate'])[0]) * 14\n    df3['week'] = 1\n    df3.reset_index(inplace=True)\n    df3['state'] = df3['state'].map(state_data.iloc[:,0])\n    df3['is_locked'] = df3['state'].isin(lock_dates[(lock_dates['Date_Num']<=20200416)]['State'].unique()).astype(int)\n    mob4['state']=mob4['state'].str.replace('of','Of')\n\n    df1 = pd.merge(df1, mob[['Retail_and_Recreation', 'Workplaces', 'Residential', 'state']], on='state')\n    df2 = pd.merge(df2, mob2[['Retail_and_Recreation', 'Workplaces', 'Residential', 'state']], on='state')\n    df3 = pd.merge(df3, mob4[['Retail_and_Recreation', 'Workplaces', 'Residential', 'state']], on='state')\n\n    df = pd.concat((df1, df2), axis=0)\n    df = pd.concat((df, df3), axis=0)\n\n    return df, df1, df2, df3\n\ndef draw_feature(selected_cols, max_depth=2, min_samples_leaf = 5):\n        \n        Y = X4['Group']\n        clf = DecisionTreeRegressor(max_depth=max_depth, min_samples_leaf=min_samples_leaf, criterion = 'mae')\n        X4[selected_cols] = X4[selected_cols].fillna(X4[selected_cols].median())\n        clf.fit(X4[selected_cols], Y)\n\n        pred_te = clf.predict(X4[selected_cols])\n        print(\"MAE Test Model: %.4f\" % metrics.mean_absolute_error(Y, pred_te))\n        print(\"MAE Test Naive: %.4f\" % metrics.mean_absolute_error(Y, np.repeat(np.mean(Y), len(Y))))\n        \n        dot_data = StringIO()\n        export_graphviz(clf, out_file=dot_data, feature_names = selected_cols,\n                        filled=True, rounded=True, proportion = True, impurity = False,\n                        special_characters=True,\n                        class_names = ['GOOD', 'BAD']\n                       )\n        graph = pydotplus.graph_from_dot_data(dot_data.getvalue())  \n        graph.write_png('original_tree.png')\n        graph.set_size('\"20,!\"')\n        plt = Image(graph.create_png())\n        display(plt)\n        \ndef show_correlations_color_target(X, Y):\n\n    fig = plt.figure(figsize=(6,6))\n    ax = fig.add_subplot(111)\n    ax.set_title(X.columns[0] + \" vs \" + X.columns[1],fontsize=14)\n    ax.set_xlabel(X.columns[0],fontsize=12)\n    ax.set_ylabel(X.columns[1],fontsize=12)\n    ax.grid(True,linestyle='-',color='0.75')\n    x = X.iloc[:,0]\n    y = X.iloc[:,1]\n    z = Y\n\n    # scatter with colormap mapping to z value\n    ax.scatter(x,y,s=60,c=z, marker = 'o', cmap = cm.jet );\n\n    plt.show()\n    spearmanr(x, y)\n\ndef format_tex(float_number):\n    exponent = np.floor(np.log10(float_number))\n    mantissa = float_number\/10**exponent\n    mantissa_format = str(mantissa)[0:3]\n    return \"${0}\\times10^{{{1}}}$\"\\\n           .format(mantissa_format, str(int(exponent)))","71e4fdd7":"case_data = get_datahub_data()\nprint('Columns:', list(case_data.columns))\ndf = case_data[case_data['Province\/State'] == 'Alabama'].groupby('Date').sum()[['Case']]\ndf.tail(5)","3c701466":"state_data = get_state_code_map()\nstate_data.head(5)","1fdb0643":"state_square = get_state_square_map()\nstate_square.head(5)","c0a307e7":"test_data = get_full_testing_data()\ntest_data.head(10)","6c0dab67":"beds_data = get_icu_bed_capacity_data()\nbeds_data.head(10)","6383badb":"state_geo = state_capital_geo_coordinates()\nstate_geo.head(5)","d0eb2f54":"final_weather = get_weather(case_data, state_geo)\nfinal_weather.head(10)","2c6bd157":"dp05 = get_dp05_census_data()\ndp05.head(10)","25537232":"dp03 = get_dp03_census_data()\ndp03.head(5)","90293bd3":"mob =  get_google_mob_report(date = '2020-03-29')\nmob.head(10)","fef14b28":"mob.head(10)","601153b7":"mob2 =  get_google_mob_report(date = '2020-04-05')\nmob2.head(10)","31a47f3b":"mob2.head(10)","f9bceebf":"mob4 = get_google_mob_report(date = '2020-04-11')\nmob4.head(10)","a73df831":"mob4.head(10)","be1123b5":"lock_dates = get_lock_dates()\nlock_dates.head(10)","4dbc8fd2":"cont_mit_data = get_containment_data()\ncont_mit_data.head(5)","2d0c3724":"test_targ = calc_log_cumulative_tests(test_data, state_data)\ntest_targ.head(10)","8ff16a74":"plot_log_cumulative_tests(test_targ, lock_dates, state_data)","7fd1698f":"growth_rate = calculate_growth_rate_slope(test_targ)\ngrowth_rate.head(10)","08ab322f":"change_rate = plot_log_cumulative_tests_with_change(test_targ, lock_dates, state_data)","fdb706f1":"columns = ['Retail_and_Recreation', 'Residential',\n       'Workplaces', 'inf_rate_dec', 'inf_rate_inc']\n\ncolumns, coefficients = run_variable_selection(growth_rate, mob2, columns, NFOLDS=3, RANDOM_STATE=42, N_REPEATS=30)\ncoefficients.sort_values(by = 'weight', ascending = False).head(10)","b2b93287":"result, X, Y = run_stat_significance(growth_rate, mob4, columns)\nresult.summary()","8af052a4":"show_correlations(X, Y)","8b8294e4":"show_correlations_color_target_work(X, Y)","bfb6ad57":"show_correlations_color_target_retail(X, Y)","01870507":"vars03 = get_dp03_vars(growth_rate, dp03)","5629d683":"vars05 = get_dp05_vars(growth_rate, dp05, pvalue = 0.01)\n\nvars05 += ['density', 'Estimate_SEX_AND_AGE_Total_population', 'Square']\ndp05 = pd.merge(dp05, state_square, left_on = 'Geographic_Area_Name', right_on = 'State', how = 'left')\ndp05['Square'] = dp05['Square'].str.replace(',','').astype('float')\ndp05['density'] = dp05['Estimate_SEX_AND_AGE_Total_population'] \/ dp05['Square']","8c4a85c9":"df_weather, vars07 = get_weather_vars(growth_rate, final_weather, pvalue = 0.01)","4631cd00":"sc_data = pd.read_csv('\/kaggle\/input\/us-supply-chain-information-for-covid19\/cfs-2012-pumf-csv\/cfs_2012_pumf_csv.txt', sep=',')\nsc_codes = pd.read_csv('\/kaggle\/input\/us-supply-chain-information-for-covid19\/state_code_to_name.csv', sep=',')\n\nepi_data, vars09 = get_epirank_vars(growth_rate, sc_data, sc_codes)","aab3703f":"path = '\/kaggle\/input\/io-tables\/'\nfilename = 'PROJECTED_USE_2028.csv'\nproj_use_ci = pd.read_csv(path + filename, sep = ';').set_index('Sector')\n\nfilename = 'PROJECTED_MAKE_2028.csv'\nproj_make_ic = pd.read_csv(path + filename, sep = ';').set_index('Sector')\n\nfilename = 'SectorPlan312.csv'\nsec_plan_312 = pd.read_csv(path + filename, sep = ';')\n\ni2i, c2c, g, W = calc_leontieff_inverse(proj_use_ci, proj_make_ic, sec_plan_312)\nsec_plan_312.reset_index(inplace=True)\ndf = get_industry_by_num(i2i, sec_plan_312, N=48)\n\ndf.head(10)","817642c4":"final_df = get_ppi_fred_data(sec_plan_312)","07724b79":"industry_slope =  calc_industry_slope(final_df)\nindustry_slope.head(10)","c0022b51":"df, sec_plan_312_v2 = show_industry_effects(sec_plan_312, industry_slope, i2i)\ndf.head(10)","7c8fa3d7":"df = show_industry_network_effects(proj_use_ci, proj_make_ic, sec_plan_312_v2, c2c, g, W)\n# df[df['value_dec'] > 0].sort_values(by='value_dec', ascending = False).head(15)\ndf.head(15)","cb4f9ac5":"df[~df.value_dec.isna()].iloc[:,1:].sort_values(by='industry_dec', ascending = True).head(30)","49495f3f":"df.iloc[:,1:].sort_values(by='industry_dec', ascending = False).head(30)","6881f5bc":"df1 = pd.read_excel('\/kaggle\/input\/us-supply-chain-information-for-covid19\/cfs-2012-pum-file-users-guide-app-a-jun2015.xlsx', sheet_name = 'App A2', header = 1)\ndf2 = sec_plan_312_v2\ndf3 = pd.read_csv('\/kaggle\/input\/mapping\/Mapping_SC_NAICS_2017.csv', sep=';')\n\ndf5 = map_naics_to_supply_chain(df1,df2,df3)","fc9481cd":"sc_est = get_supply_chain_industry_features(sc_data, sc_codes, df5)","03205822":"df, df1, df2, df3 = prepare_what_if_data(test_targ, mob, mob2, mob4, lock_dates, state_data)\ndf.head(10)","389a7743":"mdf, md = build_what_if_model()\nprint(mdf.summary())","4aadc042":"DAYS_FORECAST = 6\nINCREASE_WORK_RECREATION = -0.05\nINCREASE_RESIDENTIAL = -0.05\nDATE_LAST_USED = 20200522\nDATE_FORECAST = 20200528\n\ntest = run_simulation(test_targ, df3, mdf, md, DAYS_FORECAST, INCREASE_WORK_RECREATION, INCREASE_RESIDENTIAL)\ntest.sort_values(by='pred', ascending = False)","f2ad9d58":"mob_cols = ['Retail_and_Recreation', 'Residential',\n           'Workplaces', 'state']\n\nstate_static_diff = pd.DataFrame()\nstate_static_diff = test[['Group']]\n\ncoefficients, oof_preds_mod, all_data, X = train_explanatory_model(state_static_diff, mob4, dp03, dp05, df_weather, epi_data, mob_cols, vars03, vars05, vars07, vars09, ALPHA = 100)\ncoefficients.sort_values(by = 'weight', ascending = False).head(50)","0da138bd":"Y = all_data['inf_rate_dec']\nselected_cols = ['density', 'epirank_dist_r']\nX = all_data[selected_cols].fillna(0)\nclf = sm.OLS(Y, X)\nclf = clf.fit()\nprint(clf.summary())","1b55bd4f":"show_feature_importance(coefficients)","495d3b12":"target = state_static_diff.reset_index()\nall_data['Province\/State'] = all_data['Province\/State'].str.replace('of','Of')\nX4 = pd.merge(all_data, target, left_on = 'Province\/State', right_on = 'index')\n        \ndraw_feature(['density', 'epirank_dist_r'], min_samples_leaf=10)\nshow_correlations_color_target(X4[['density', 'epirank_dist_r']], X4['Group'])","d9a86b00":"X4[(X4.density > 400) & (X4.epirank_dist_r < 0.02)]","cc529f8f":"X2 = show_risky_states(all_data, beds_data, test_data, oof_preds_mod, X, sc_est, beta=0.3)\nX2","06fb2cab":"df9 = find_median_states_for_experiment(X, test)\ndf9","36f99d27":"cont_mit_data","92b0bb30":"# **Load Full Testing Data**","7717bf67":"# **Pre-filter Census Age and Population Variables by fixed PValue = 0.01**","60d3284d":"# **Calculate Growth Rate BEFORE and AFTER 29\/03\/2020**\n* BEFORE we will call it Infection Rate Increasing\n* AFTER we will call it infection Rate Decreased\n\nWe will shows in a minute how we came to this decision and cutoff.","f85ebf27":"# **Run Simulations**","c8e066d8":"![image.png](attachment:image.png)","e38a5636":"# **Show Correlation between Decrease in Workplaces, Increase in Residential and Growth Rate Decrease**\n\n* Growth Rate decrease is colored - dark values are better, so you can see: proportional exchange between areas pays off.\n* In other words: you should go to Residential not in larger extent, than you go to work","b6083fec":"# **Train Final Explanatory Model**\n\nFeatures included:\n* Google Mobility (Retail and Recreation, Workplaces, Residential)\n* Age and Population (PRE-FILTERED: Percent Commute to Work by Types by Age, Percent Population 65+)\n* Income and Etnicity (PRE-FILTERED: \n* Weather Data (PRE-FILTERED: windspeed max + std, snowdepth max)\n* EpiRank Indicators of Supply Chain \"Abandonness\" (PRE-FILTERED)\n* Supply Chain State Industry Growth\/Decrease Indicators (average effect across STATE industry codes)\n* Do we have subway in our state? :)\n\nWe are able to beat the CV and OOF baseline with proper regularization and feature preparation!","c7380ec0":"# **Pre-filter Weather Min, Max, Median, Std Statistics by fixed PValue = 0.05**","cf9bab37":"# **Load Stay-Home Order (LockDown) Dates Per States**","13a78e03":"# **Load Producer Price Index Data from Federal Reserve Bank API**\n(including manual mapping of some NAICS codes)","bc01091e":"# **Show Significant Variables**","5705a13f":"# **Load US Census DP05 Age and Population Statistics**","15ac8a60":"# **Prepare Dataset for What-If Analysis**","25ff2537":"# **Load ICU Beds Capacity**","a7c781fd":"# **Bottom Industries**","b1c98805":"# **Show Variable-Target Correlations**\nThey are not THAT strong - we made quarantine for all areas at the same time :(","7341f110":"# **Estimate Statistical Significane for Selected Variables**\n\n* Target: Relative Decrease in Growth Rate:\n(['inf_rate_dec'] - ['inf_rate_inc']) \/ ['inf_rate_inc']\n\n* Ordinary Least Squares without Constant (we assume we have all factors of mobility decrease)\n* See output below (p-value less than 0.1 for all variables)","8136dbd5":"# **Show Interesting Rulesets**","374b5d43":"3. We load 2019-2020 Producer Price Index data from Federal Reserve Bank of St.Louis: https:\/\/fred.stlouisfed.org\/ to evaluate current state for every particular industry. We calculate growth rate as simple linear regression slope based on one year of monthly data. \n\n4. We take current final demand values, mutiply it by projected yearly decrease and then multiplu it with Leontief Inverse to get network effects.\n\n5. We can see which indsutries will rise and fall!","070d1954":"# **Load Containment and Mitigation Measures Data**","43718a97":"# **Load US Census DP03 Income and Ethnical Statistics**","bfaf1110":"# **Show States for Ruleset on Map**","04746cfc":"# **Calculate Growth Rate **\n* Cumulative number of cases on log-scale\n* test_rate - growth rate for negative tests\n* inf_rate - growth rate for positive tests","aa4b7743":"# **Pre-filter Census Income and Ethnicity Variables by fixed PValue = 0.0005**","467e54cf":"# **Show Separated Effects for Particular Industry and Connected Industries**","406db062":"# **Calculate Linear Regression Slope for Every Industry based on 2019-2020 PPI Stats**","791fa1c3":"# **Find Nearest Neighbours to Average State for Experiment**","4729777e":"# **Calculate Piecewise Linear Fit to Growth Rate Curve**\nWhy? To find regions of increasing and decreased growth, and explore causes later.\nWe will use scatterplot for growth rate this time to show change in behaviour more clearly.\n\nIn output we print also coefficients for Piecewise Linear Functions.\n\nWe suppose the whole task of non-linear epidemic forecasting becomes more complex than just forecasting raw number of cases.\nIt becomes the task of forecasting and predicting two things at once: current growth rate + changepoint time (like in plots).\nThen we can use piecewise linear fit and capture non-linear dynamics with complex differential equations.","3ede9320":"# **Top Industries**","309f2979":"# **Show Feature Importances**\n\n(More Domain Knowledge could be incorporated into Selection)","6e8fd202":"# **Show Growth Rate per State **\n* With Lockdown Date - Black Dotted Line\n* RED - positive log-transformed cumulative cases\n* GREEN - negative log-transformed cumulative cases\n\nHere we can start first discussion. Overall we have three effects of test growth:\n1. Doctors increase coverage (should be like STEP function in both)\n2. People are are getting infected (smooth growth in both)\n3. People panic (smooth\/STEP ? growth with increasing NEGATIVE proportion)\n\nOverall the pictures shows smooth and proportional growth of positive and negative tests for most states.\nSome states show \"bottleneck\" behaviour - positive increase with decrease of negative (people are coming with severe symptoms).\nSome states show probably some data issues\/zero proportion of negative tests - may be they were not ready for testing completely?\nYou could correlate it with LockDown Date show in Black.","0879187d":"# **Pre-filter EpiRank Indicators by fixed PValue = 0.01**\n\nWe get significance for distance indicators - it means \"value of abandoncy\" is really useful with explaining current Growth Rate.","5c96f7b4":"# **Import Libraries**","a7dc8697":"# **Prepare Initial Matrices and Show Industry Connections**","40b9319b":"# **Connect Industry Projected Performance to the Supply Chain Network **\n\n* Based on NAICS code for ORIGIN_STATE\n* We've assumed to have bigger NAICS code intersection","688f1634":"# **Show States for Ruleset**\n","9440a21d":"# **Load DataHub Data (Number of Infected)**","e42ecf4c":"Show on Map:\n\nhttps:\/\/www.amcharts.com\/visited_states\/#US-AL,US-CT,US-DC,US-DE,US-FL,US-GA,US-IN,US-KY,US-MA,US-MD,US-NC,US-NJ,US-NY,US-OH,US-PA,US-RI,US-SC,US-VA,US-WV\n\nShow with Humidity and Wind Direction: \n\nhttps:\/\/www.windy.com\/-Humidity-rh?rh,40.835,-84.710,5\n\nhttps:\/\/www.windy.com\/?40.835,-84.710,5","390e2073":"# **Load State-Capital-GeoCoordinates Mapping (+Manual enrichment)**","88e5a85a":"# **Load Google Community Mobility Report for 11\/04\/2020**","f2fffae2":"# **Load Google Community Mobility Report for 29\/03\/2020**","9197360d":"# **Let's see which industries will rise and fall!**\n* Calculate Final Demand (see io_description.pdf)\n* Final Demand Decrease with Regression Slope\n* Multiply Decreased Final Demand by Leontief Inverse","f7fb75b8":"# **Load Google Community Mobility Report for 05\/04\/2020**","81519247":"# **Show Correlation between Decrease in Retail and Recreation, Increase in Residential and Growth Rate Decrease**\n\n* Growth Rate decrease is colored - dark values are better, so you can see: proportional exchange between areas pays off.\n* In other words: you should go to Residential not in larger extent, than you go to Retail and Recreation ","ff56b77b":"# **Calculate EpiRank values (like PageRank in Google but for Epidemics Networks)**\n\n* Input: US Census Supply Chain Network Data with Origin - Destination connections\n* Output: Measure of network value of particular state by Goods value, weight, distance delivered.\n\nMore about EpiRank: https:\/\/www.nature.com\/articles\/s41598-019-41719-8\n\n![image.png](attachment:image.png)","de9319f5":"# **Load Weather for Previous Month (one API Key for ~1000 requests)**","647b36d1":"# **The last chapter is dedicated to Wassily Leontief**\nhttps:\/\/en.wikipedia.org\/wiki\/Wassily_Leontief \n\nRussian, born in Germany, worked in USA. Created Leontief Input-Output Analysis which allows to study inter-industry network effect in economics.\nIn the end, multiplication of effects could be modeled with single matrix NxN where N - number of industries. This matrix is called Leontief Inverse.\n","b765aa1d":"# **Let's get more explanatory variables!**","ef31951c":"# Run Cross-Validation to Determine Best Mobility Variables to influence Growth Rate decrease\n\n* We define target as growth rate after 29\/03\/2020 - decreased growth rate\n* We use Google Mobility variables for 05\/04\/2020 - final effect of stay-home\n* We run Repeated K-Fold Cross-Validation with Out-of-Fold and CV estimates of stability\n* We use MAE as evaluation metric - it shows percentage error, if appled on log-scale\n\nAfter several iterations we've found most stable variables - Retail_and_Recreation, Workplaces and Residential.\nFirst two are decreasing and having positive effect on rate decrease, last is increasing with positive effect.\nIt means basically (almost!) that grocery and pharmacy shopping is not that dangerous.\nThat's what is included in latter categories:\n\n![image.png](attachment:image.png)","cad6802e":"# **Show Prediction about Most Risky States**\n\n* Harmonic mean: Growth Rate Risk combined with ICU Occupancy Ratio Risk\n* EpiRank Risk: Worst place - 1, best - 50. Rating of \"abandonness\" in supply chain network.","41a7944b":"# **Which industries will rise and which industries will fall after crisis?**\n\nWe will do the following:\n1. We take input-output tables structured by industry NAICS codes from US Bureau of Labour Statistics: https:\/\/www.bls.gov\/emp\/data\/input-output-matrix.htm\n2. We calculate Leotief Inverse according to the following calculation and naming conventions, found in ZIP above: https:\/\/drive.google.com\/file\/d\/1G5pwk0QFm_dBuTNLJzkUVPsGAfebiX48\/view?usp=sharing\n![image.png](attachment:image.png)","118e9bb4":"# **Show Containment and Mitigation Measures**","b2ecb3da":"# **Build What-If Analysis Model**"}}