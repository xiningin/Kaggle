{"cell_type":{"230213fe":"code","41dcaf83":"code","1064be87":"code","06e94a6e":"code","a95d2c5b":"code","03d0a0d7":"code","658d5e2c":"code","ed39b32d":"code","af4a1949":"code","f62b0cf7":"code","85a9c8d0":"code","97e78340":"code","4135c584":"code","d58c0549":"code","1cdc3e0d":"code","d0a09c89":"code","9e13ff84":"code","dc723db8":"code","54a03fcf":"code","6c68a3af":"code","2df95eb4":"markdown","9b647060":"markdown","b42265aa":"markdown","5f34a79c":"markdown","e137d14f":"markdown","bc716837":"markdown","04493b8a":"markdown","b7fbfdca":"markdown","79875f3e":"markdown","0b7a52d1":"markdown","6df15d63":"markdown","256877af":"markdown","59fea589":"markdown","0d5f23be":"markdown","77b4824c":"markdown","077f56a9":"markdown","e8651930":"markdown","513392bd":"markdown"},"source":{"230213fe":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\nimport seaborn as sns\n%matplotlib inline\nimport tensorflow.keras as keras\nfrom keras.utils.np_utils import to_categorical # convert to one-hot-encoding\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPool2D\nfrom keras.optimizers import RMSprop\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.callbacks import ReduceLROnPlateau\nnp.random.seed(2)","41dcaf83":"train = pd.read_csv(\"..\/input\/digit-recognizer\/train.csv\")\ntest = pd.read_csv(\"..\/input\/digit-recognizer\/test.csv\")","1064be87":"#now as label is the target we will saparate for training \nY_train = train[\"label\"]\nX_train = train.drop(labels = [\"label\"],axis = 1) \nX_train.shape","06e94a6e":"X_train.isnull().sum()\nY_train.isnull().sum()","a95d2c5b":"sns.countplot(Y_train)","03d0a0d7":"# Normalize the data\nX_train = X_train \/ 255.0\ntest = test \/ 255.0","658d5e2c":"X_train = X_train.values.reshape(-1,28,28,1)\ntest = test.values.reshape(-1,28,28,1)","ed39b32d":"plt.figure( figsize = (10,10))\nfor i in range(1,10):\n    plt.subplot(3,4,i)\n    plt.imshow(X_train[i+1].reshape([28,28]),cmap=\"gray\")\n","af4a1949":"Y_train = to_categorical(Y_train, num_classes = 10)","f62b0cf7":"from sklearn.model_selection import train_test_split\nX_train, X_val, Y_train, Y_val = train_test_split(X_train, Y_train, test_size = 0.1, random_state=2)","85a9c8d0":"model = keras.models.Sequential([\n    keras.layers.Conv2D(128, (3,3), input_shape=(28,28,1), activation='relu'),\n    keras.layers.MaxPooling2D(2,2),\n    keras.layers.BatchNormalization(),\n    keras.layers.Conv2D(256, (3,3), activation='relu'),\n    keras.layers.MaxPooling2D(2,2),\n    keras.layers.BatchNormalization(),\n    keras.layers.Conv2D(512, (3,3), activation='relu'),\n    keras.layers.MaxPooling2D(2,2),\n    keras.layers.BatchNormalization(),\n    keras.layers.Flatten(),\n    keras.layers.Dense(128),\n    keras.layers.BatchNormalization(),\n    keras.layers.Activation('relu'), \n    keras.layers.Dropout(0.1),\n    keras.layers.Dense(64),\n    keras.layers.BatchNormalization(),\n    keras.layers.Activation('relu'),\n    keras.layers.Dense(10, activation='softmax')\n])","97e78340":"model.compile(optimizer = 'adam',loss = \"categorical_crossentropy\", metrics=[\"accuracy\"])\nmodel.summary()","4135c584":"X_train ,X_test_test,Y_train,Y_test= train_test_split(X_train,Y_train,test_size=0.2)","d58c0549":"from tensorflow.keras.callbacks import EarlyStopping,ModelCheckpoint\nes = EarlyStopping(monitor='accuracy', mode='min', verbose=1, patience=5,baseline=0.99)\n\nmodel.fit(X_train,Y_train, epochs=20, callbacks=[es], validation_data = (X_test_test,Y_test))","1cdc3e0d":"model.evaluate(X_test_test,Y_test)","d0a09c89":"\ndatagen = ImageDataGenerator(\n        featurewise_center=False,  # set input mean to 0 over the dataset\n        samplewise_center=False,  # set each sample mean to 0\n        featurewise_std_normalization=False,  # divide inputs by std of the dataset\n        samplewise_std_normalization=False,  # divide each input by its std\n        zca_whitening=False,  # apply ZCA whitening\n        rotation_range=10,  # randomly rotate images in the range (degrees, 0 to 180)\n        zoom_range = 0.1, # Randomly zoom image \n        width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)\n        height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)\n        horizontal_flip=False,  # randomly flip images\n        vertical_flip=False)  # randomly flip images\n\n\ndatagen.fit(X_train)","9e13ff84":"checkpoint = ModelCheckpoint('best_weigths.h5', monitor='val_loss', verbose=0, save_best_only=True, mode='auto', period=1)\n\nhistory = model.fit_generator(datagen.flow(X_train,Y_train),\n                              epochs = 30, validation_data = (X_test_test,Y_test),\n                              verbose = 1, callbacks=[es,checkpoint])","dc723db8":"X_test_np = np.array(test)\n\nprint(X_test_np.shape)\n\nplt.figure( figsize = (10,10))\nfor i in range(1,20):\n    toBePredicted = X_test_np[i].reshape(-1,28,28,1)\n    result2 = model.predict(toBePredicted)\n    print(np.argmax(result2,axis = 1))\n    \n    plt.subplot(10,4,i)\n    plt.imshow(X_test_np[i].reshape([28,28]))","54a03fcf":"# predict results\nresults = model.predict(X_test_np)\n# select the indix with the maximum probability\nresults = np.argmax(results,axis = 1)\nresults = pd.Series(results,name=\"Label\")\nsubmission = pd.concat([pd.Series(range(1,28001),name = \"ImageId\"),results],axis = 1)\nprint(results)\nsubmission.to_csv(\"cnn_mnist_datagen.csv\",index=False)","6c68a3af":"acc = history.history['acc']\nepochs_s = range(0,30)\n\nplt.plot(epochs_s, acc , label='accuracy')\nplt.xlabel('epochs')\nplt.ylabel('accuracy')\n\nplt.title('accuracy vs epochs')\nplt.legend()","2df95eb4":"after Data Augmentation its making good prediction ....now our model is well trained  lets see how accracy vary with no.of epochs","9b647060":"so there is no missing values in the data we have so next is to we will see there count and normalize them","b42265aa":"missing data handling is one another big task so lets see if any missing values ","5f34a79c":"we are with 98% accuracy goood . but it is always better to avoid overfitting problem. The idea is to alter the training data with small transformations to reproduce the variations occuring when someone is writing a digit.this we generally call as Data augmentation . lets do ...","e137d14f":"once our layers are added to the model, we need to set up  \na loss function and an optimisation algorithm.\n\nThe most important function is the optimizer. This function will iteratively improve parameters (filters kernel values, weights and bias of neurons in order to minimise the loss.\n\nhere we will use adam . The adam update adjusts the Adagrad method in a very simple way ","bc716837":"we now split the train set in two parts a small fraction (10%) became the validation set which the model is evaluated and the rest (90%) is used to train the model.","04493b8a":"ok now lets Split training and valdiation set ","b7fbfdca":"its time to evaluate our model","79875f3e":"Before modifying our data lets see how CNN works..!\n\nCNN image classifications takes an input image, process it and classify it under certain categories (Eg., Dog, Cat, Tiger, Lion). Computers sees an input image as array of pixels and it depends on the image resolution. Based on the image resolution, it will see h x w x d( h = Height, w = Width, d = Dimension ). Eg., An image of 6 x 6 x 3 array of matrix of RGB (3 refers to RGB values) and an image of 4 x 4 x 1 array of matrix of grayscale image.\n\nTechnically, deep learning CNN models to train and test, each input image will pass it through a series of convolution layers with filters (Kernals), Pooling, fully connected layers (FC) and apply Softmax function to classify an object with probabilistic values between 0 and 1. The below figure is a complete flow of CNN to process an input image and classifies the objects based on values.","0b7a52d1":"Wow ... it look good ok now lets  Encode labels to one hot vectors (ex : 2 -> [0,0,1,0,0,0,0,0,0,0])","6df15d63":"**you just liked it place upvotes that would be very much appreciated - That will keep me motivated ..**","256877af":"now lets do the fitting process again i \nthink we should end up with good accracy this time ","59fea589":"again we have similar counts for the 10 digits so now lets start playing with our data ","0d5f23be":" so now we Reshape image in 3 dimensions (height = 28px, width = 28px , canal = 1). now lets see how it looks","77b4824c":" Firstly, we will concentrate on the data then we will focus on the CNN modeling and evaluation.we have 3 main task they are data prepation and cnn modeling and The results prediction and submission","077f56a9":"**Build CNN **\n\nnow we try to build Sequential Convolutional Neural Network \n\nthe first convolutional (Conv2D) layer ,this layer creates a convolution kernel that is wind with layers input which helps produce a tensor of outputs.genneraly It is like a set of learnable filters . CNN cab able to do feature maps i.e The CNN can isolate features that are useful everywhere from these transformed images.\nhere in side CNN there are some works going on like Convolution,Strided Convolution,padding,pooling \nconvolution It is a process where we take a small matrix of numbers (called kernel or filter), we pass it over our image and transform it based on the values from filter.and calculate by following formula \n\n![image.png](attachment:image.png)\n\nThe second important layer in CNN is the pooling (MaxPool2D) layer. Max pooling is a sample-based discretization process. The objective is to down-sample an input representation by reducing its dimensionality\n\nDropout is a regularization method,i.e to reduce overfitting and improve generalization error in deep neural networks of all kinds.\n\n'relu' is the rectifier (activation function max(0,x). The rectifier activation function is used to add non linearity to the network.\n\nThe Flatten layer is use to convert the final feature maps into a one single 1D vector. It combines all the found local features of the previous convolutional layers.\n","e8651930":"so now lets normalization our data . We perform a grayscale normalization to reduce the effect of illumination's differences.\n\nMoreover the CNN converg faster on [0..1] data than on [0..255].","513392bd":"**CNN**\n\nIn neural networks, Convolutional neural network (ConvNets or CNNs) is one of the main categories to do images recognition, images classifications. Objects detections, recognition faces etc., are some of the areas where CNNs are widely used.where we will use a Sequential Convolutional Neural Network for digits recognition trained on MNIST dataset. we choosed to build it with keras API which is very powerfull"}}