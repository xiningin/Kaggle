{"cell_type":{"d5b54f5d":"code","af2ab1be":"code","16e6b277":"code","3ab6d55a":"code","d66c5651":"code","d8d3245a":"code","22a00cd2":"code","1cba6827":"code","c87a02de":"code","de4e46e2":"code","58a7e7ad":"code","821790ff":"code","e7e29c92":"code","1892bc5a":"code","9fdb6e10":"code","8b0d2d19":"code","f0b00416":"code","a5968b08":"code","ec1ea42d":"code","8deed2c0":"code","f13eb4e8":"code","71c1c09f":"code","3bd2a820":"code","323dc3ac":"code","d130ef40":"code","fce90cc7":"code","482281ed":"code","cb02c031":"code","885a72b8":"code","12cab8d1":"code","1969ee37":"code","2fce8471":"code","f9ab0539":"code","a195b5f2":"code","77ae4950":"code","df1cdd8c":"code","226259c6":"code","b20e5287":"code","6905c571":"code","37a6e683":"code","5b49ef40":"code","138a3439":"code","73b8704f":"code","f527e0f8":"code","970696b8":"code","88f3502b":"code","45d705c7":"code","4129db5c":"code","ae570f22":"code","bfaf244b":"code","fff943ed":"code","3d4f16b3":"code","108807a0":"code","b0f0ac44":"code","3db5a11b":"code","438f978d":"code","a9e37c73":"code","39019c69":"code","ec774a0b":"code","6fff705e":"markdown","1eb3c8ea":"markdown","211096ed":"markdown","26b1f77d":"markdown","148a523c":"markdown","5c63406d":"markdown","b6889bc7":"markdown","a984a486":"markdown","a0b2a7a8":"markdown","7608da5f":"markdown","fbdd1425":"markdown","3b52ad63":"markdown","81fabb08":"markdown","e740604c":"markdown","7adeacb5":"markdown","823c051a":"markdown","93eda148":"markdown","71cc308e":"markdown","0fd7030e":"markdown","f86a6b66":"markdown","5d207c70":"markdown","63705b70":"markdown","4086b76e":"markdown","6b0db0f0":"markdown","547a7d38":"markdown","1f7985fa":"markdown","33775569":"markdown","608b7fe9":"markdown","ad7d040d":"markdown"},"source":{"d5b54f5d":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","af2ab1be":"from sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import mean_absolute_error\n\nimport lightgbm as lgb\nimport xgboost as xgb\nimport datetime\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import StratifiedKFold, KFold, RepeatedKFold, GroupKFold, GridSearchCV, train_test_split, TimeSeriesSplit\nfrom sklearn import metrics\n\nimport plotly.express as px\nimport plotly.graph_objects as go\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\npd.set_option(\"display.max_columns\", 500)\n\n#matplotlib inline","16e6b277":"# To calculate missing values\n\ndef missing_vals(df):\n    missing_vals = df.isnull().sum()\n    missing_per = missing_vals\/len(train)*100\n    missing_per = missing_per.sort_values(ascending=False).reset_index()\n    missing_table = missing_per.rename({'index':'Column', 0:'Missing %'}, axis=1)\n    return missing_table\n\n\n# To draw data insights\n\ndef data_insights(df):\n    \n    print(f'Dataset Shape : {df.shape}')\n\n    summary = pd.DataFrame(df.dtypes).reset_index().rename({'index':\"Column\", 0:'DataType'}, axis=1)\n    summary['Missing %'] = round((df.isnull().sum()\/df.shape[0])*100,2).values\n    summary['No. of Unique Values'] = df.nunique().values\n    summary['First Value'] = df.loc[0].values\n    summary['Second Value'] = df.loc[1].values\n\n    return summary\n\n#correlation matrix\n\ndef cormat(df):\n    cols = df.columns\n    corrmat = df[cols].corr()\n    f, ax = plt.subplots(figsize=(14,10))\n    summary = sns.heatmap(corrmat, vmax=.8, square=True, annot=True, fmt='.2f')\n    return summary\n    \n\n# Outliers detection\n\ndef CalcOutliers(df_num): \n\n    # calculating mean and std of the array\n    data_mean, data_std = np.mean(df_num), np.std(df_num)\n\n    # seting the cut line to both higher and lower values\n    # You can change this value\n    cut = data_std * 3\n\n    #Calculating the higher and lower cut values\n    lower, upper = data_mean - cut, data_mean + cut\n\n    # creating an array of lower, higher and total outlier values \n    outliers_lower = [x for x in df_num if x < lower]\n    outliers_higher = [x for x in df_num if x > upper]\n    outliers_total = [x for x in df_num if x < lower or x > upper]\n\n    # array without outlier values\n    outliers_removed = [x for x in df_num if x > lower and x < upper]\n    outliers_removed.sort()\n    \n    print(f\"Lowest Value : {outliers_removed[0]}\") # printing lowest value\n    print(f\"Highest Value : {outliers_removed[-1]}\") # printing highest value\n    print('Identified lowest outliers: %d' % len(outliers_lower)) # printing total number of values in lower cut of outliers\n    print('Identified upper outliers: %d' % len(outliers_higher)) # printing total number of values in higher cut of outliers\n    print('Total outlier observations: %d' % len(outliers_total)) # printing total number of values outliers of both sides\n    print('Non-outlier observations: %d' % len(outliers_removed)) # printing total number of non outlier values\n    print(\"Total percentual of Outliers: \", round((len(outliers_total) \/ len(outliers_removed) )*100, 4)) # Percentual of outliers in points\n    \n    return ","3ab6d55a":"# Train Dataset\ntrain_trans = pd.read_csv(\"\/kaggle\/input\/ieee-fraud-detection\/train_transaction.csv\")\ntrain_id = pd.read_csv(\"\/kaggle\/input\/ieee-fraud-detection\/train_identity.csv\")","d66c5651":"# Test Dataset\ntest_trans = pd.read_csv(\"\/kaggle\/input\/ieee-fraud-detection\/test_transaction.csv\")\ntest_id = pd.read_csv(\"\/kaggle\/input\/ieee-fraud-detection\/test_identity.csv\")","d8d3245a":"id_train = [col for col in train_id.columns if \"id\" in col]\nid_test = [col for col in test_id.columns if \"id\" in col]\ncol_map = dict(zip(id_test, id_train))","22a00cd2":"test_id = test_id.rename(col_map, axis=1)","1cba6827":"train = train_trans.merge(train_id, how='left', on=\"TransactionID\")\ntrain.head()","c87a02de":"test = test_trans.merge(test_id, how='left', on=\"TransactionID\")\ntest.head()","de4e46e2":"print(f\"Train dataset has {train.shape[0]} rows and {train.shape[1]} columns\")\nprint(f\"There are {train.isnull().any().sum()} columns which contains NULL values\")","58a7e7ad":"missing_table = missing_vals(train)\nmissing_table","821790ff":"missing_num = missing_table[missing_table['Missing %']>50].shape[0]\n\nprint(f\"There are {missing_num} columns where missing % is greater than 50%\")","e7e29c92":"plt.figure(figsize=(10,5))\n\ntrain_trans['TransactionDT'].plot(kind='hist',title='Train vs Test TransactionDT distribution',\n                                 xlabel='Train', label='Train')\n\ntest_trans['TransactionDT'].plot(kind='hist',\n                                 xlabel='Test', label='Test')\n\nplt.legend()\nplt.tight_layout()","1892bc5a":"plt.subplots(1,2, figsize=(14,5))\n\nplt.subplot(1,2,1)\nsns.countplot(x=\"isFraud\", data=train)\nplt.title(\"Target Variable Count Distribution\", fontsize=16, loc='center')\n\nplt.subplot(1,2,2)\ntrain.groupby('isFraud')['TransactionAmt'].sum().plot(kind='bar')\nplt.title(\"Transaction Amount Sum Distribution by Target Variable\", fontsize=16, loc='center')\n\nplt.tight_layout()\nplt.show(),\ntrain.isFraud.value_counts(normalize=True)*100 #Imbalance dataset","9fdb6e10":"data_insights(train[['TransactionAmt']])","8b0d2d19":"plt.subplots(2,2, figsize=(16,12))\nplt.suptitle('Transaction Values Distribution', fontsize=22)\n\nplt.subplot(2,2,1)\ntrain[\"TransactionAmt\"].plot()\nplt.title(\"Transaction Amount Distribution\")\n\nplt.subplot(2,2,2)\nsns.boxplot(y=\"TransactionAmt\", data=train[train[\"TransactionAmt\"]<1000])\nplt.title(\"Transaction Amount <1000 Outliers Check\")\n\nplt.subplot(2,2,3)\nplt.scatter(range(train[train['isFraud'] == 0].shape[0]),\n                 np.sort(train[train['isFraud'] == 0]['TransactionAmt'].values),label='NoFraud')\nplt.title(\"Transaction Amount of Non-Fraud Entries\")\n\nplt.subplot(2,2,4)\nplt.scatter(range(train[train['isFraud'] == 1].shape[0]),\n                 np.sort(train[train['isFraud'] == 1]['TransactionAmt'].values),label='NoFraud')\nplt.title(\"Transaction Amount of Fraud Entries\")\n\nplt.tight_layout()","f0b00416":"plt.subplots(2,2, figsize=(14,5))\nplt.suptitle('Train Transaction Amount Distribution', fontsize=16)\n\nplt.subplot(221)\ntrain.loc[train[\"isFraud\"]==1][\"TransactionAmt\"].plot(kind='hist', bins=100, title=\"Fraud Distribution\")\n\nplt.subplot(222)\ntrain.loc[train[\"isFraud\"]==1][\"TransactionAmt\"].apply(np.log)\\\n                        .plot(kind='hist', bins=100, title=\"Log Transaformed Fraud Distribution\")\n\nplt.subplot(223)\ntrain.loc[train[\"isFraud\"]==0][\"TransactionAmt\"]\\\n                        .plot(kind='hist', bins=100, title=\"Non-Fraud Distribution\")\n\nplt.subplot(224)\ntrain.loc[train[\"isFraud\"]==0][\"TransactionAmt\"].apply(np.log)\\\n                        .plot(kind='hist', bins=100, title=\"Log Transaformed Non-Fraud Distribution\")\n\nplt.tight_layout()\nplt.show()","a5968b08":"CalcOutliers(train['TransactionAmt']), train[\"TransactionAmt\"].describe()","ec1ea42d":"data_insights(train[['ProductCD']])","8deed2c0":"df_prod = train.groupby(\"ProductCD\")['isFraud'].sum().reset_index()\ndf_prod1 = (pd.crosstab(index=train[\"ProductCD\"], columns=train['isFraud'], normalize='index')*100).reset_index()\n\nfig, ax1 = plt.subplots(figsize=(14,7))\nplt.suptitle(\"Fraud Transactions by Product Code\", fontsize=16)\n\ncolor = 'tab:red'\nax1.set_xlabel('Product Code')\nax1.set_ylabel('Number of Fraud Transactions', color=color)\nax1.plot(df_prod.ProductCD, df_prod['isFraud'], color=color)\nax1.tick_params(axis='y', labelcolor=color)\n\nax2 = ax1.twinx()\nax3 = ax1.twinx() # instantiate a second axes that shares the same x-axis\n\nax3.spines.right.set_position((\"axes\", 1.2))\n\ncolor = 'tab:blue'\nax2.set_ylabel('% of Non Fraud Transactions', color=color)  # we already handled the x-label with ax1\nax2.plot(df_prod1.ProductCD, df_prod1[0], color=color, label=\"Non Fraud\")\nax2.tick_params(axis='y', labelcolor=color)\n\ncolor = 'tab:green'\nax3.set_ylabel('% of Fraud Transactions', color=color)  # we already handled the x-label with ax1\nax3.plot(df_prod1.ProductCD, df_prod1[1], color=color, label=\"Fraud\")\nax3.tick_params(axis='y', labelcolor=color)\n\nax2.legend()\nax3.legend()\n\nfig.tight_layout()  # otherwise the right y-label is slightly clipped\nplt.show()","f13eb4e8":"df = pd.crosstab(index=train['ProductCD'], columns=train['isFraud'], normalize='columns') * 100\ndf = df.reset_index()\ndf.rename(columns={0:'NoFraud', 1:'Fraud'}, inplace=True)\n\nimport plotly.graph_objects as go\n\nfig = go.Figure()\nfig.add_trace(go.Bar(\n    x=df[\"ProductCD\"],\n    y=df[\"NoFraud\"],\n    name='NoFraud',\n    marker_color='indianred',\n    text=df[\"NoFraud\"]\n))\n\nfig.add_trace(go.Bar(\n    x=df[\"ProductCD\"],\n    y=df[\"Fraud\"],\n    name='Fraud',\n    marker_color='lightsalmon',\n    text=df[\"Fraud\"]\n))\n\n# Here we modify the tickangle of the xaxis, resulting in rotated labels.\nfig.update_layout(barmode='group', xaxis_tickangle=-45, title='% of Fraud Transactions by Product')\nfig.update_traces(texttemplate='%{text:.2s}%', textposition='outside')\nfig.show()","71c1c09f":"plt.figure(figsize=(14,7))\nsns.boxplot(x='ProductCD', y='TransactionAmt', data=train[train['TransactionAmt']<1000], hue='isFraud')\n\nplt.tight_layout()","3bd2a820":"card_cols = [c for c in train.columns if 'card' in c]\n\n# Card dataset insights\ndata_insights(train[card_cols])","323dc3ac":"train[card_cols].describe()","d130ef40":"train_df = pd.crosstab(index=train['card4'], columns=train['isFraud'], normalize='index')\n\nx = np.arange(len(train_df.index))  # the label locations\nwidth = 0.35  # the width of the bars\n\nfig, ax = plt.subplots(figsize=(12,5))\nrects1 = ax.bar(x - width\/2, round(train_df[0]*100,2), width, label='Not Fraud')\nrects2 = ax.bar(x + width\/2, round(train_df[1]*100,2), width, label='Fraud')\n\n# Add some text for labels, title and custom x-axis tick labels, etc.\nax.set_ylabel('Percentage')\nax.set_title('Percentage by Card 4 and isFraud')\nax.set_xticks(x)\nax.set_xticklabels(train_df.index)\nax.legend()\n\nax.bar_label(rects1, padding=3)\nax.bar_label(rects2, padding=3)\n\nfig.tight_layout()\nplt.show()","fce90cc7":"plt.subplots(2,2, figsize=(18,12))\nplt.suptitle(\"Cards Distribution\", fontsize=22)\n\nplt.subplot(2,2,1)\nsns.countplot(train['card4'])\nplt.title(\"Card 4 Distribution\")\n\nplt.subplot(2,2,2)\ntrain.groupby('card4')['TransactionAmt'].sum().plot(kind='bar')\nplt.title(\"Card 4 Distribution by Transacton Amount\")\n\nplt.subplot(2,2,3)\nsns.countplot(train['card6'])\nplt.title(\"Card 6 Distribution\")\n\nplt.subplot(2,2,4)\ntrain.groupby('card6')['TransactionAmt'].sum().plot(kind='bar')\nplt.title(\"Card 6 Distribution by Transacton Amount\")\n\nplt.tight_layout()\nplt.show()","482281ed":"df_c3 = pd.crosstab(index=train['card3'], columns=train['isFraud'], normalize='index').reset_index()\ndf_c3 = df_c3.sort_values(by=1, ascending=False).head(30)\n\ndf_c5 = pd.crosstab(index=train['card5'], columns=train['isFraud'], normalize='index').reset_index()\ndf_c5 = df_c5.sort_values(by=1, ascending=False).head(30)\n\n\nplt.subplots(figsize=(16,7))\n\nplt.subplot(211)\nsns.pointplot(x='card3',y=1, data=df_c3, )\nplt.title('Top 30 Fraudlant Transactons by Card 3', fontsize=16)\nplt.ylabel(\"% of Fraudlant Transactons\", fontsize=14)\nplt.xlabel(\"card 3 Values\", fontsize=14)\n\nplt.subplot(212)\nsns.pointplot(x='card5',y=1, data=df_c5)\nplt.title('Top 30 Fraudlant Transactons by Card 5', fontsize=16)\nplt.ylabel(\"% of Fraudlant Transactons\", fontsize=14)\nplt.xlabel(\"card 5 Values\", fontsize=14)\n\n\nplt.tight_layout()\nplt.show()","cb02c031":"c_cols = [c for c in train.columns if c[0]=='C']\ndata_insights(train[c_cols])","885a72b8":"#correlation matrix\ncormat(train[c_cols])","12cab8d1":"plt.subplots(3,5, figsize=(18,14))\n\nx=1\nfor c in c_cols:\n    plt.subplot(5,3,x)\n    sns.kdeplot(train[c])\n    plt.title(f\"{c}'s Density Distribution\")\n    x+=1\n    \nplt.tight_layout()\nplt.show()","1969ee37":"train_C = train[c_cols]\ntopC1_index = list(train_C.C1.value_counts(normalize=True, sort=True).head(10).index)\nC1_df = (pd.crosstab(index=train['C1'], columns=train['isFraud'], normalize=True)*100).reset_index()\nC1_df = C1_df[C1_df['C1'].isin(topC1_index)]\n\nfig = go.Figure()\nfig.add_trace(go.Bar(\n    y=C1_df['C1'],\n    x=C1_df[0],\n    name='Not Fraud',\n    orientation='h',\n    marker=dict(\n        color='rgba(246, 78, 139, 0.6)',\n        line=dict(color='rgba(246, 78, 139, 1.0)', width=3)\n    )\n))\nfig.add_trace(go.Bar(\n    y=C1_df['C1'],\n    x=C1_df[1],\n    name='Fraud',\n    orientation='h',\n    marker=dict(\n        color='rgba(58, 71, 80, 0.6)',\n        line=dict(color='rgba(58, 71, 80, 1.0)', width=3)\n    )\n))\n\nfig.update_layout(barmode='stack', title='Top 10 most frequent values in C1 with their Fraud percentage')\nfig.show()","2fce8471":"d_cols = [c for c in train.columns if c[0]=='D' and len(c)<5]\ndata_insights(train[d_cols])","f9ab0539":"plt.subplots(3,5, figsize=(18,14))\n\nx=1\nfor c in d_cols:\n    plt.subplot(5,3,x)\n    sns.kdeplot(train[c])\n    plt.title(f\"{c}'s Density Distribution\")\n    x+=1\n    \nplt.tight_layout()\nplt.show()","a195b5f2":"cormat(train[d_cols])","77ae4950":"train[d_cols].describe()","df1cdd8c":"m_cols = [c for c in train.columns if c[0]=='M']\ntrain_M = train[m_cols]\ndata_insights(train_M)","226259c6":"[train_M[c].unique() for c in train_M.columns]","b20e5287":"train_M_plot = train_M.fillna(\"None\")\n\nplt.subplots(nrows = 3, ncols=3, figsize=(18,14))\n\nx=1\nfor c in list(train_M_plot.columns):\n    \n    plt.subplot(3,3,x)\n    sns.countplot(x = c, data= train_M_plot, label=c)\n    plt.title(\"Distinct Value Counts across \"+ c + \" Column\" )\n    x+=1\n\nplt.tight_layout()\nplt.show()","6905c571":"v_cols = [c for c in train.columns if c[0]=='V']\ntrain_V = train[v_cols]\ndata_insights(train_V).head(5)","37a6e683":"train_V.describe()","5b49ef40":"id_cols = [c for c in train.columns if 'id' in c]\ntrain_id = train.loc[:,\"id_01\":\"id_38\"]\ndata_insights(train_id)","138a3439":"train_id = pd.DataFrame(train[id_cols].dtypes).reset_index()\\\n        .rename({'index':'column', 0:'Dtype'}, axis=1)\n\ntrain_id.groupby('Dtype')['column'].count()","73b8704f":"train.loc[:,\"id_01\":\"id_38\"].describe()","f527e0f8":"train_id_plot = train[id_cols]\n\nplt.subplots(6, 4, figsize=(18,14))\n\nx=1\nfor c in list(train_id[train_id[\"Dtype\"]== 'float64'].column.unique()):\n    plt.subplot(6,4,x)\n    plt.hist(train_id_plot[c])\n    plt.title(f'Distribution of {c} variable')\n    x+=1\n\nplt.tight_layout()\nplt.show()","970696b8":"lst = ['id_12', 'id_15', 'id_16', 'id_23', 'id_27', 'id_28', 'id_29','id_34', 'id_35', 'id_36', 'id_37','id_38']\n\nplt.subplots(4, 3, figsize=(18,14))\n\nx=1\nfor c in lst:\n    plt.subplot(4,3,x)\n    sns.countplot(x= c, data = train_id_plot)\n    plt.title(f'Distribution of {c} variable')\n    x+=1\n\nplt.tight_layout()\nplt.show()","88f3502b":"train['id_30'].value_counts().plot(kind='bar', figsize=(18,7))","45d705c7":"id_colss = [c for c in test.columns if 'id' in c]\nid_colss_ = [c.replace('-','_') for c in id_colss]\n\ndictionary = dict(zip(id_colss, id_colss_))\ntest.rename(columns=dictionary,inplace=True)","4129db5c":"p = data_insights(train_id)\nlst = list(p[p['DataType']!='object'].Column)","ae570f22":"sns.countplot(x='DeviceType',data=train)","bfaf244b":"pie_frame = pd.DataFrame((train['DeviceInfo'].value_counts(normalize=True))*100).reset_index()\nfig = px.pie(pie_frame.head(10), values='DeviceInfo', names='index', title='Top 10 Device Infomation %')\nfig.show()","fff943ed":"R_frame = pd.DataFrame(train['R_emaildomain'].value_counts(normalize=True)*100).reset_index().head()\nP_frame = pd.DataFrame(train['P_emaildomain'].value_counts(normalize=True)*100).reset_index().head()\n\nfig1 = px.pie(R_frame.head(10), values='R_emaildomain', names='index', title='R_emaildomain Distribution %')\nfig2 = px.pie(P_frame.head(10), values='P_emaildomain', names='index', title='P_emaildomain Distribution %')\nfig1.show()\nfig2.show()","3d4f16b3":"missing_vals(train[['R_emaildomain','P_emaildomain']])","108807a0":"R_fraud_pct = (pd.crosstab(index=train['R_emaildomain'], columns=train['isFraud'], normalize='index')*100).reset_index()\\\n                .rename({0:'Not Fraud', 1:'Fraud'}, axis=1)\n\nfig = px.bar(R_fraud_pct, x=\"R_emaildomain\", y=['Not Fraud','Fraud'], title=\"Fraud % by R_email domain\")\nfig.show()","b0f0ac44":"P_fraud_pct = (pd.crosstab(index=train['P_emaildomain'], columns=train['isFraud'], normalize='index')*100).reset_index()\\\n                .rename({0:'Not Fraud', 1:'Fraud'}, axis=1)\n\nfig = px.bar(P_fraud_pct, x=\"P_emaildomain\", y=['Not Fraud','Fraud'], title=\"Fraud % by P_email domain\")\nfig.show()","3db5a11b":"# Reference - https:\/\/www.kaggle.com\/c\/ieee-fraud-detection\/discussion\/100071#latest-577632\n\nSTART_DATE = '2017-12-01'\nstartdate = datetime.datetime.strptime(START_DATE, \"%Y-%m-%d\")\ntrain[\"Date\"] = train['TransactionDT'].apply(lambda x: (startdate + datetime.timedelta(seconds=x)))\ntrain['_Weekdays'] = train['Date'].dt.dayofweek\ntrain['_Hours'] = train['Date'].dt.hour\ntrain['_Days'] = train['Date'].dt.day","438f978d":"train['TransactionAmt_to_mean_card1'] = train['TransactionAmt'] \/ train.groupby(['card1'])['TransactionAmt'].transform('mean')\ntrain['TransactionAmt_to_mean_card4'] = train['TransactionAmt'] \/ train.groupby(['card4'])['TransactionAmt'].transform('mean')\ntrain['TransactionAmt_to_std_card1'] = train['TransactionAmt'] \/ train.groupby(['card1'])['TransactionAmt'].transform('std')\ntrain['TransactionAmt_to_std_card4'] = train['TransactionAmt'] \/ train.groupby(['card4'])['TransactionAmt'].transform('std')\n\ntest['TransactionAmt_to_mean_card1'] = test['TransactionAmt'] \/ test.groupby(['card1'])['TransactionAmt'].transform('mean')\ntest['TransactionAmt_to_mean_card4'] = test['TransactionAmt'] \/ test.groupby(['card4'])['TransactionAmt'].transform('mean')\ntest['TransactionAmt_to_std_card1'] = test['TransactionAmt'] \/ test.groupby(['card1'])['TransactionAmt'].transform('std')\ntest['TransactionAmt_to_std_card4'] = test['TransactionAmt'] \/ test.groupby(['card4'])['TransactionAmt'].transform('std')\n\ntrain['id_02_to_mean_card1'] = train['id_02'] \/ train.groupby(['card1'])['id_02'].transform('mean')\ntrain['id_02_to_mean_card4'] = train['id_02'] \/ train.groupby(['card4'])['id_02'].transform('mean')\ntrain['id_02_to_std_card1'] = train['id_02'] \/ train.groupby(['card1'])['id_02'].transform('std')\ntrain['id_02_to_std_card4'] = train['id_02'] \/ train.groupby(['card4'])['id_02'].transform('std')\n\ntest['id_02_to_mean_card1'] = test['id_02'] \/ test.groupby(['card1'])['id_02'].transform('mean')\ntest['id_02_to_mean_card4'] = test['id_02'] \/ test.groupby(['card4'])['id_02'].transform('mean')\ntest['id_02_to_std_card1'] = test['id_02'] \/ test.groupby(['card1'])['id_02'].transform('std')\ntest['id_02_to_std_card4'] = test['id_02'] \/ test.groupby(['card4'])['id_02'].transform('std')\n\ntrain['D15_to_mean_card1'] = train['D15'] \/ train.groupby(['card1'])['D15'].transform('mean')\ntrain['D15_to_mean_card4'] = train['D15'] \/ train.groupby(['card4'])['D15'].transform('mean')\ntrain['D15_to_std_card1'] = train['D15'] \/ train.groupby(['card1'])['D15'].transform('std')\ntrain['D15_to_std_card4'] = train['D15'] \/ train.groupby(['card4'])['D15'].transform('std')\n\ntest['D15_to_mean_card1'] = test['D15'] \/ test.groupby(['card1'])['D15'].transform('mean')\ntest['D15_to_mean_card4'] = test['D15'] \/ test.groupby(['card4'])['D15'].transform('mean')\ntest['D15_to_std_card1'] = test['D15'] \/ test.groupby(['card1'])['D15'].transform('std')\ntest['D15_to_std_card4'] = test['D15'] \/ test.groupby(['card4'])['D15'].transform('std')\n\ntrain['D15_to_mean_addr1'] = train['D15'] \/ train.groupby(['addr1'])['D15'].transform('mean')\ntrain['D15_to_mean_addr2'] = train['D15'] \/ train.groupby(['addr2'])['D15'].transform('mean')\ntrain['D15_to_std_addr1'] = train['D15'] \/ train.groupby(['addr1'])['D15'].transform('std')\ntrain['D15_to_std_addr2'] = train['D15'] \/ train.groupby(['addr2'])['D15'].transform('std')\n\ntest['D15_to_mean_addr1'] = test['D15'] \/ test.groupby(['addr1'])['D15'].transform('mean')\ntest['D15_to_mean_addr2'] = test['D15'] \/ test.groupby(['addr2'])['D15'].transform('mean')\ntest['D15_to_std_addr1'] = test['D15'] \/ test.groupby(['addr1'])['D15'].transform('std')\ntest['D15_to_std_addr2'] = test['D15'] \/ test.groupby(['addr2'])['D15'].transform('std')","a9e37c73":"train[['P_emaildomain_1', 'P_emaildomain_2', 'P_emaildomain_3']] = train['P_emaildomain'].str.split('.', expand=True)\ntrain[['R_emaildomain_1', 'R_emaildomain_2', 'R_emaildomain_3']] = train['R_emaildomain'].str.split('.', expand=True)\ntest[['P_emaildomain_1', 'P_emaildomain_2', 'P_emaildomain_3']] = test['P_emaildomain'].str.split('.', expand=True)\ntest[['R_emaildomain_1', 'R_emaildomain_2', 'R_emaildomain_3']] = test['R_emaildomain'].str.split('.', expand=True)","39019c69":"# train_miss = missing_vals(train)\n# many_null_cols = train_miss[train_miss['Missing %']>90].Column.to_list()\n\n# test_miss = missing_vals(test)\n# many_null_cols_test =  test_miss[test_miss['Missing %']>90].Column.to_list()\n\n# big_top_value_cols = [col for col in train.columns if train[col].value_counts(dropna=False, normalize=True).values[0] > 0.9]\n# big_top_value_cols_test = [col for col in test.columns if test[col].value_counts(dropna=False, normalize=True).values[0] > 0.9]\n\n# one_value_cols = [col for col in train.columns if train[col].nunique()<=1]\n# one_value_cols_test = [col for col in test.columns if test[col].nunique()<=1]\n\n# cols_to_drop = list(set(many_null_cols + many_null_cols_test + big_top_value_cols + big_top_value_cols_test + one_value_cols + one_value_cols_test))\n# cols_to_drop.remove(\"isFraud\")\n# print(f\"we will drop {len(cols_to_drop)} columns from our test and train data\")\n\n# train = train.drop(cols_to_drop, axis=1)\n# test = test.drop(cols_to_drop, axis=1)","ec774a0b":"*Now, we can use this data to feed into various models.* \n\n## Thank You ##","6fff705e":"## C1-C14 Features","1eb3c8ea":"## TransactionAmt Distribution","211096ed":"## M1-M9 Columns","26b1f77d":"Clearly there are outliers present in each of the C# columns.","148a523c":"## V1-V339 Columns","5c63406d":"The TransactionDT feature is a timedelta from a given reference datetime (not an actual timestamp). It seems like the train and test are splitted by time. There is a slight gap inbetween.\nThe training set is from an earlier period of time and test is from a later period of time. This will act as a key reason while choosing the right cross validation techniques later.","b6889bc7":"## Data Glimpse","a984a486":"## Product Code Features","a0b2a7a8":"There are a lot of missing values in M# columns. ","7608da5f":"If we consider only values between >= 0 to 800 we will avoid the outliers and has more confidence in our distribution.\nWe have 10k rows with outliers that represents 1.74% of total rows.","fbdd1425":"## Train and Test Distribution","3b52ad63":"## Outlier Detection ","81fabb08":"> W has the most Fraud and Non Fraud transactions, followed by C and R.\n\n> ProductCD C has the most fraud with >11%\n\n> ProductCD W has the least with ~2%","e740604c":"### Non Numerical Features","7adeacb5":"## D1-D15 Features","823c051a":"## Transaction Date","93eda148":"## Handy Functions","71cc308e":"## id# Columns","0fd7030e":"We have 3.5% of Fraud transactions in our dataset.","f86a6b66":"## Device Features","5d207c70":"## Card Features","63705b70":"You can observe that there is a high correlation among C3 features. Example below:\n\n> C1, C2, C4, C6, C7, C8, C10, C11, C12, C14 are highly correlated with each other. We can keep one of them and drop the rest.","4086b76e":"- D1, D2 are highly correlated with each other. We can keep one of them and drop the other.\n- Also, a lot of missing values, we will treat them later.","6b0db0f0":"### Identity info as a function of Transaction Date","547a7d38":"## Missing Values","1f7985fa":"### Numerical Features","33775569":"## Target Distribution","608b7fe9":"Most of columns have missing data, which is normal in real world.","ad7d040d":"## Feature Engineering\n\nhttps:\/\/www.kaggle.com\/artgor\/eda-and-models#Data-Exploration\nI have referenced this amazing kernels where I could see some amazing feature transformations."}}