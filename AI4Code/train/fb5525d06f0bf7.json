{"cell_type":{"c8a1d43b":"code","6b33cbcd":"code","33b3efa7":"code","551daeee":"code","a6402ef4":"code","4dd96add":"code","dfa03f16":"code","b1fcd16a":"code","e587b2d6":"code","e097208f":"code","f3a24831":"code","e833e02d":"code","a44d0004":"code","b340d70c":"code","e40b7871":"code","0fd1120f":"code","09ff2d3a":"code","1a68e420":"code","4c84a8f9":"code","242d9666":"code","8fd3b6f3":"code","60a889a1":"code","e56e6eed":"code","e7b1afe7":"code","d58dad99":"code","0a72fe88":"code","efedeffd":"code","67715fbc":"code","b1ba4da5":"code","4628b4fc":"code","a788c615":"code","948e266c":"code","2cb44c9b":"code","805aa252":"code","6726c4e1":"code","f1e63657":"code","b855991c":"code","474bc793":"code","b019aeb4":"code","465f9b43":"code","14d7e30f":"code","9722f38b":"code","6cdee2ab":"code","ca10ed1a":"code","8bcd50a8":"code","f6e6978d":"code","d1790fa8":"code","39f01a6b":"code","a5e3d1b0":"code","273e64be":"code","c01c082f":"code","f85f705a":"code","4cc29214":"code","ea4f6af1":"code","dbb19864":"code","7ab2fd31":"code","688adcc3":"code","c0ed99af":"code","d8b4c874":"code","bbaad02f":"code","1da182b4":"code","b4026b5a":"code","2fdd3e35":"code","96cf4c3a":"code","482fbbb8":"code","c38c87c2":"code","4933cf73":"code","c9940d32":"code","cd646687":"code","5b83bbbb":"code","fab2348b":"code","5346d007":"code","70e50bb8":"code","ad386726":"code","7151c193":"code","65200824":"code","fa49258b":"code","d895f124":"code","688f9471":"code","e287c931":"code","6477f619":"code","956d8c0d":"code","86a70341":"code","56f71bc6":"code","700ccbc0":"code","fd64dd04":"code","0ea4bea9":"code","70b1d05c":"code","65c1a38a":"code","986d94bb":"code","de4fe578":"code","1e1d8a2f":"code","c67b1c01":"code","5c5d6e6c":"code","4ff77fe6":"code","2a4fb1fa":"code","9bc36813":"code","96b1c320":"code","6a975367":"code","9184a42a":"code","901f1b5b":"code","3e5a407c":"code","8db2e669":"code","6c455958":"code","9d9d34d8":"code","cb764631":"code","9baf9977":"code","66333013":"code","b5ca8126":"code","895dd7fd":"code","2aa35a6c":"code","e8de56d1":"code","3c8a3e55":"code","62268982":"code","13693de0":"code","6d4288d2":"markdown","a0e352e6":"markdown","914cfcf2":"markdown","1c344fc1":"markdown","4879ac4c":"markdown","703cb39f":"markdown","03bc0143":"markdown","35b60dde":"markdown","0d07404d":"markdown","5897aae8":"markdown","b4309909":"markdown","b2f9d3b7":"markdown","ea8f38eb":"markdown","6233794c":"markdown","910a9d12":"markdown","20e624ff":"markdown","c1bb8c93":"markdown","f25dc954":"markdown","449f6627":"markdown","629ab06c":"markdown","81b98a15":"markdown","1c23c314":"markdown","698cd522":"markdown","e744654e":"markdown","eb68d2b4":"markdown","72a01a30":"markdown","b5cb9ad9":"markdown","0266a6b6":"markdown","b41c8cee":"markdown","62f84eee":"markdown","72cfe04c":"markdown","d048f7b1":"markdown","4c12e22f":"markdown","b8b9efd0":"markdown","b205ca23":"markdown","3e58ae41":"markdown","57760a7f":"markdown","08fb561f":"markdown","2de2b0b9":"markdown","c7906c74":"markdown","19e45c63":"markdown","29efc5ce":"markdown","8f387077":"markdown"},"source":{"c8a1d43b":"import numpy as np \nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport matplotlib.cm as cm\nimport seaborn as sns\nimport os\nimport cv2\nfrom kaggle_datasets import KaggleDatasets\nimport tensorflow as tf\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nplt.style.use('fivethirtyeight')\nplt.rcParams['figure.figsize'] = [16, 8]\n\nprint('Using Tensorflow version:', tf.__version__)","6b33cbcd":"try:\n    # TPU detection. No parameters necessary if TPU_NAME environment variable is\n    # set: this is always the case on Kaggle.\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n    print('Running on TPU ', tpu.master())\nexcept ValueError:\n    tpu = None\n\nif tpu:\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\nelse:\n    # Default distribution strategy in Tensorflow. Works on CPU and single GPU.\n    strategy = tf.distribute.get_strategy()\n\nprint(\"REPLICAS: \", strategy.num_replicas_in_sync)","33b3efa7":"TRAIN_PATH = '..\/input\/thaimnist\/train'\nTEST_PATH = '..\/input\/thaimnist\/test'\nGCS_DS_PATH = KaggleDatasets().get_gcs_path('thaimnist')","551daeee":"df_train_1digit = pd.read_csv('..\/input\/thaimnist\/mnist.train.map.csv')\ndf_train_more_digit = pd.read_csv('..\/input\/thaimnist\/train.rules.csv')","a6402ef4":"number_1digit_null = df_train_1digit.isnull().sum()\nnumber_moredigit_null = df_train_more_digit.isnull().sum()\n\nprint(\"Number of null 1 digit\\n\",number_1digit_null, end=\"\\n\\n\")\nprint(\"Number of null more than 1 digit\\n\",number_moredigit_null)","4dd96add":"img = cv2.imread(os.path.join(TRAIN_PATH,df_train_1digit['id'][0]))\nimg = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\nimg.shape","dfa03f16":"df_train_1digit.head(10)","b1fcd16a":"df_train_1digit['category'].value_counts().to_frame().plot(kind='bar')","e587b2d6":"df_train_more_digit.head(10)","e097208f":"df_train_more_digit['predict'].value_counts().to_frame().plot(kind='bar')","f3a24831":"df_train_more_digit.head(10)","e833e02d":"def show_train_img(df, cat):\n    \n    fig, axes = plt.subplots(nrows=2, ncols=5, figsize=(24, 10))\n    ten_random_samples = df[df['category'] == cat]['id'].sample(10).values\n    \n    for idx, image in enumerate(ten_random_samples):\n        final_path = os.path.join(TRAIN_PATH,image)\n        img = cv2.imread(final_path)\n        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n        axes.ravel()[idx].imshow(img)\n        axes.ravel()[idx].axis('off')\n    plt.tight_layout()","a44d0004":"show_train_img(df_train_1digit,0)","b340d70c":"show_train_img(df_train_1digit,4)","e40b7871":"show_train_img(df_train_1digit,5)","0fd1120f":"IMG_SIZE_h = 456\nIMG_SIZE_w = 456\nBATCH_SIZE = 32\nLR = 0.0005\nEPOCHS = 50\nWARMUP = 10\nAUTO = tf.data.experimental.AUTOTUNE","09ff2d3a":"def joinPathTrain(path):\n    new_path = os.path.join(GCS_DS_PATH,'train',path)\n    return new_path","1a68e420":"df_train_1digit['path'] = df_train_1digit['id'].apply(joinPathTrain)\ndf_train_1digit['category'] = df_train_1digit['category'].apply(float)","4c84a8f9":"df_train_1digit.head(10)","242d9666":"drop_list = pd.read_csv('..\/input\/drop-list-byp0002\/drop_lists.csv')['id'].to_list()","8fd3b6f3":"df_train_1digit = df_train_1digit[~df_train_1digit['id'].isin(drop_list)]","60a889a1":"from sklearn.model_selection import train_test_split\nfrom tensorflow.keras.utils import to_categorical\nfrom sklearn.model_selection import KFold","e56e6eed":"KFoldNo = 5\nkf = KFold(n_splits=KFoldNo,random_state=2020, shuffle=True)","e7b1afe7":"KFold_df = []\nfor train_index, test_index in kf.split(df_train_1digit):\n  KFold_df.append({\"train\": df_train_1digit.iloc[train_index],\n                   \"val\": df_train_1digit.iloc[test_index]\n                   }\n                  )","d58dad99":"def decode_image(filename, label=None, image_size=(IMG_SIZE_h, IMG_SIZE_w) , file =True):\n    if file:\n        filename = tf.io.read_file(filename)\n    image = tf.image.decode_jpeg(filename, channels=3)\n    image = tf.image.resize(image, image_size)\n    image = tf.image.rgb_to_grayscale(image)\n    image = tf.cast(image,tf.float32) \/ 255.0\n    if label is None:\n        return image\n    else:\n        return image, label","0a72fe88":"dataset_KFOLD = []\nfor k in range(KFoldNo):\n    train_paths = np.array(KFold_df[k][\"train\"]['path'].to_list())\n    train_labels = np.array(KFold_df[k][\"train\"]['category'].to_list())\n    train_labels = to_categorical(train_labels)\n    \n    valid_paths = np.array(KFold_df[k][\"val\"]['path'].to_list())\n    valid_labels = np.array(KFold_df[k][\"val\"]['category'].to_list())\n    valid_labels = to_categorical(valid_labels)\n\n    train_dataset = (\n        tf.data.Dataset\n        .from_tensor_slices((train_paths, train_labels))\n        .map(decode_image, num_parallel_calls=AUTO)\n        .cache()\n        .repeat()\n        .shuffle(2048)\n        .batch(BATCH_SIZE)\n        .prefetch(AUTO)\n    )\n\n    val_dataset = (\n        tf.data.Dataset\n        .from_tensor_slices((valid_paths, valid_labels))\n        .map(decode_image, num_parallel_calls=AUTO)\n        .batch(BATCH_SIZE)\n        .cache()\n        .prefetch(AUTO)\n    )\n    \n    dataset_KFOLD.append({\n        \"train\":train_dataset,\n        \"val\":val_dataset\n    })","efedeffd":"from tensorflow.keras.optimizers import RMSprop,Adam\n\noptimizer_RMS = RMSprop(lr=LR, rho=0.9, epsilon=1e-08, decay=0.0)\noptimizer_Adam = Adam(lr=LR)","67715fbc":"import keras.backend as K\n\ndef categorical_focal_loss_with_label_smoothing(gamma=2.0, alpha=0.25, ls=0.1, classes=10.0):\n    \"\"\"\n    Implementation of Focal Loss from the paper in multiclass classification\n    Formula:\n        loss = -alpha*((1-p)^gamma)*log(p)\n        y_ls = (1 - \u03b1) * y_hot + \u03b1 \/ classes\n    Parameters:\n        alpha -- the same as wighting factor in balanced cross entropy\n        gamma -- focusing parameter for modulating factor (1-p)\n        ls    -- label smoothing parameter(alpha)\n        classes     -- No. of classes\n    Default value:\n        gamma -- 2.0 as mentioned in the paper\n        alpha -- 0.25 as mentioned in the paper\n        ls    -- 0.1\n        classes -- 10\n    \"\"\"\n    def focal_loss(y_true, y_pred):\n        # Define epsilon so that the backpropagation will not result in NaN\n        # for 0 divisor case\n        epsilon = K.epsilon()\n        # Add the epsilon to prediction value\n        #y_pred = y_pred + epsilon\n        #label smoothing\n        y_pred_ls = (1 - ls) * y_pred + ls \/ classes\n        # Clip the prediction value\n        y_pred_ls = K.clip(y_pred_ls, epsilon, 1.0-epsilon)\n        # Calculate cross entropy\n        cross_entropy = -y_true*K.log(y_pred_ls)\n        # Calculate weight that consists of  modulating factor and weighting factor\n        weight = alpha * y_true * K.pow((1-y_pred_ls), gamma)\n        # Calculate focal loss\n        loss = weight * cross_entropy\n        # Sum the losses in mini_batch\n        loss = K.sum(loss, axis=1)\n        return loss\n    \n    return focal_loss","b1ba4da5":"!pip install -q efficientnet","4628b4fc":"from tensorflow.keras import Sequential\nfrom tensorflow.keras.layers import Conv2D, MaxPool2D, Dropout, Flatten, Dense, BatchNormalization, Input\nfrom efficientnet.tfkeras import EfficientNetB3","a788c615":"model_KFOLD = []\nfor k in range(KFoldNo):\n    with strategy.scope():\n        model = tf.keras.Sequential([\n        Input(shape=(IMG_SIZE_h,IMG_SIZE_w,1),name=\"Input\"),\n        Conv2D(3,1,name=\"ConvLayerInput\"),\n        EfficientNetB3(weights='noisy-student',\n                       include_top=False,\n                       pooling='avg'),\n\n            Dense(10, activation='softmax')\n        ])\n\n        model.layers[0].trainable = False\n\n        model.compile(optimizer = 'adam',\n                      loss = categorical_focal_loss_with_label_smoothing(gamma=2.0, alpha=0.75, ls=0.125, classes=10.0), # num classes\n                      metrics=['accuracy'])\n\n        model_KFOLD.append(model)","948e266c":"model_KFOLD[0].summary()","2cb44c9b":"from tensorflow.keras.callbacks import EarlyStopping\nimport math\n\ndef get_cosine_schedule_with_warmup(lr, num_warmup_steps, num_training_steps, num_cycles=0.5):\n    def lrfn(epoch):\n        if epoch < num_warmup_steps:\n            return float(epoch) \/ float(max(1, num_warmup_steps)) * lr\n        progress = float(epoch - num_warmup_steps) \/ float(max(1, num_training_steps - num_warmup_steps))\n        return max(0.0, 0.5 * (1.0 + math.cos(math.pi * float(num_cycles) * 2.0 * progress))) * lr\n\n    return tf.keras.callbacks.LearningRateScheduler(lrfn, verbose=True)\n\nlr_schedule= get_cosine_schedule_with_warmup(lr=LR, num_warmup_steps=WARMUP, num_training_steps=EPOCHS)\n\n\nes = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=10)","805aa252":"history_KFOLD = []\nfor k in range(KFoldNo):\n    print(\"#\"*50)\n    print(\"FOLD\",k)\n    print(\"#\"*50)\n    history = model_KFOLD[k].fit(dataset_KFOLD[k][\"train\"],\n                    epochs = EPOCHS, \n                    validation_data = dataset_KFOLD[k][\"val\"],\n                    verbose = 1, \n                    steps_per_epoch=KFold_df[0][\"train\"].shape[0] \/\/ BATCH_SIZE, \n                    callbacks=[lr_schedule,es])","6726c4e1":"fig, ax = plt.subplots(2,1)\nax[0].plot(history.history['loss'], color='b', label=\"Training loss\")\nax[0].plot(history.history['val_loss'], color='r', label=\"validation loss\",axes =ax[0])\nlegend = ax[0].legend(loc='best', shadow=True)\n\nax[1].plot(history.history['accuracy'], color='b', label=\"Training accuracy\")\nax[1].plot(history.history['val_accuracy'], color='r',label=\"Validation accuracy\")\nlegend = ax[1].legend(loc='best', shadow=True)","f1e63657":"from sklearn.metrics import confusion_matrix\n\n\ndef plot_confusion_matrix(cm, classes,\n                          normalize=False,\n                          title='Confusion matrix',\n                          cmap=plt.cm.Blues):\n    \"\"\"\n    This function prints and plots the confusion matrix.\n    Normalization can be applied by setting `normalize=True`.\n    \"\"\"\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=45)\n    plt.yticks(tick_marks, classes)\n\n    if normalize:\n        cm = cm.astype('float') \/ cm.sum(axis=1)[:, np.newaxis]\n\n    thresh = cm.max() \/ 2.\n    for i in range (cm.shape[0]):\n        for j in range (cm.shape[1]):\n            plt.text(j, i, cm[i, j],\n                     horizontalalignment=\"center\",\n                     color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    plt.tight_layout()\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n\n# Predict the values from the validation dataset\nY_pred = model_KFOLD[0].predict(dataset_KFOLD[0][\"val\"])\n# Convert predictions classes to one hot vectors \nY_pred_classes = np.argmax(Y_pred,axis = 1) \n# Convert validation observations to one hot vectors'\nvalid_labels = np.array(KFold_df[0][\"val\"]['category'].to_list())\nvalid_labels = to_categorical(valid_labels)\nY_true = np.argmax(valid_labels,axis = 1) \n# compute the confusion matrix\nconfusion_mtx = confusion_matrix(Y_true, Y_pred_classes) \n# plot the confusion matrix\nplot_confusion_matrix(confusion_mtx, classes = range(10)) ","b855991c":"df_train_1digit = pd.read_csv('..\/input\/thaimnist\/mnist.train.map.csv')","474bc793":"df_train_mse = df_train_more_digit.copy()\ndf_train_mse_model = df_train_more_digit.copy()","b019aeb4":"df_train_mse.isnull().sum()","465f9b43":"id_train_list_id = df_train_1digit.loc[:,['id','category']]['id'].to_list()\nid_train_list_cat = df_train_1digit.loc[:,['id','category']]['category'].to_list()","14d7e30f":"dict_map = {}\nfor i in zip(id_train_list_id,id_train_list_cat):\n    dict_map[i[0]] = i[1]","9722f38b":"df_train_mse['feature1'] = df_train_mse['feature1'].map(dict_map)\ndf_train_mse['feature2'] = df_train_mse['feature2'].map(dict_map)\ndf_train_mse['feature3'] = df_train_mse['feature3'].map(dict_map)\n\ndf_train_mse_model['feature1'] = df_train_mse_model['feature1'].map(dict_map)\ndf_train_mse_model['feature2'] = df_train_mse_model['feature2'].map(dict_map)\ndf_train_mse_model['feature3'] = df_train_mse_model['feature3'].map(dict_map)","6cdee2ab":"df_train_mse.isnull().sum()","ca10ed1a":"from sklearn.metrics import mean_absolute_error as MAE","8bcd50a8":"F2 = df_train_mse[df_train_mse['feature1'].isnull()]['feature2']\nF3 = df_train_mse[df_train_mse['feature1'].isnull()]['feature3'] \ny_true_rule = df_train_mse[df_train_mse['feature1'].isnull()]['predict'].to_list()","f6e6978d":"y_pred_rule = F2 + F3","d1790fa8":"MAE(y_true_rule,y_pred_rule.to_list())","39f01a6b":"F2 = df_train_mse[df_train_mse['feature1'] == 0]['feature2']\nF3 = df_train_mse[df_train_mse['feature1'] == 0]['feature3'] \ny_true_rule = df_train_mse[df_train_mse['feature1'] == 0]['predict'].to_list()","a5e3d1b0":"y_pred_rule = F2 * F3","273e64be":"MAE(y_true_rule,y_pred_rule.to_list())","c01c082f":"F2 = df_train_mse[df_train_mse['feature1'] == 1]['feature2']\nF3 = df_train_mse[df_train_mse['feature1'] == 1]['feature3'] \ny_true_rule = df_train_mse[df_train_mse['feature1'] == 1]['predict'].to_list()","f85f705a":"y_pred_rule = np.abs(F2 - F3)","4cc29214":"MAE(y_true_rule,y_pred_rule.to_list())","ea4f6af1":"F2 = df_train_mse[df_train_mse['feature1'] == 2]['feature2']\nF3 = df_train_mse[df_train_mse['feature1'] == 2]['feature3']\ny_true_rule = df_train_mse[df_train_mse['feature1'] == 2]['predict'].to_list()","dbb19864":"y_pred_rule = (F2 + F3) * np.abs(F2 - F3)","7ab2fd31":"MAE(y_true_rule,y_pred_rule.to_list())","688adcc3":"F2 = df_train_mse[df_train_mse['feature1'] == 3]['feature2']\nF3 = df_train_mse[df_train_mse['feature1'] == 3]['feature3']\ny_true_rule = df_train_mse[df_train_mse['feature1'] == 3]['predict'].to_list()","c0ed99af":"y_pred_rule = np.abs(((F3*(F3+1)) - (F2*(F2-1)))\/2)","d8b4c874":"MAE(y_true_rule,y_pred_rule.to_list())","bbaad02f":"F2 = df_train_mse[df_train_mse['feature1'] == 4]['feature2']\nF3 = df_train_mse[df_train_mse['feature1'] == 4]['feature3']\ny_true_rule = df_train_mse[df_train_mse['feature1'] == 4]['predict'].to_list()","1da182b4":"y_pred_rule = 50 + (F2-F3)","b4026b5a":"MAE(y_true_rule,y_pred_rule.to_list())","2fdd3e35":"F2 = df_train_mse[df_train_mse['feature1'] == 5]['feature2']\nF3 = df_train_mse[df_train_mse['feature1'] == 5]['feature3']\ny_true_rule = df_train_mse[df_train_mse['feature1'] == 5]['predict'].to_list()","96cf4c3a":"y_pred_rule = np.minimum(F2,F3)","482fbbb8":"MAE(y_true_rule,y_pred_rule.to_list())","c38c87c2":"F2 = df_train_mse[df_train_mse['feature1'] == 6]['feature2']\nF3 = df_train_mse[df_train_mse['feature1'] == 6]['feature3']\ny_true_rule = df_train_mse[df_train_mse['feature1'] == 6]['predict'].to_list()","4933cf73":"y_pred_rule = np.maximum(F2,F3)","c9940d32":"MAE(y_true_rule,y_pred_rule.to_list())","cd646687":"F2 = df_train_mse[df_train_mse['feature1'] == 7]['feature2']\nF3 = df_train_mse[df_train_mse['feature1'] == 7]['feature3']\ny_true_rule = df_train_mse[df_train_mse['feature1'] == 7]['predict'].to_list()","5b83bbbb":"y_pred_rule = ((F2 * F3) % 9) * 11","fab2348b":"MAE(y_true_rule,y_pred_rule.to_list())","5346d007":"F2 = df_train_mse[df_train_mse['feature1'] == 8]['feature2']\nF3 = df_train_mse[df_train_mse['feature1'] == 8]['feature3']\ny_true_rule = df_train_mse[df_train_mse['feature1'] == 8]['predict'].to_list()","70e50bb8":"y_pred_rule = ((F2**2)+1)*(F2) + (F3)*(F3+1)","ad386726":"def moreThan99(value):\n    if value > 99:\n        return value % 99\n    else:\n        return value","7151c193":"y_pred_rule = y_pred_rule.apply(moreThan99)","65200824":"MAE(y_true_rule,y_pred_rule.to_list())","fa49258b":"F2 = df_train_mse[df_train_mse['feature1'] == 9]['feature2']\nF3 = df_train_mse[df_train_mse['feature1'] == 9]['feature3']\ny_true_rule = df_train_mse[df_train_mse['feature1'] == 9]['predict'].to_list()","d895f124":"y_pred_rule = 50 + F2","688f9471":"MAE(y_true_rule,y_pred_rule.to_list())","e287c931":"def ruleBaseModel(df):\n    F1_list = df.loc[:,['feature1']]['feature1'].to_numpy()\n    F2_list = df.loc[:,['feature2']]['feature2'].to_numpy()\n    F3_list = df.loc[:,['feature3']]['feature3'].to_numpy()\n    y_pred = np.zeros([len(F1_list)])\n    c = 0\n    for value in zip(F1_list,F2_list,F3_list):\n        F1 = value[0]\n        F2 = value[1]\n        F3 = value[2]\n        if F1 != F1: # F1 == NaN\n            y_pred[c] = F2 + F3\n        elif F1 == 0:\n            y_pred[c] = F2 * F3\n        elif F1 == 1:\n            y_pred[c] = np.abs(F2 - F3)\n        elif F1 == 2:\n            y_pred[c] = (F2 + F3)*np.abs(F2 - F3)\n        elif F1 == 3:\n            y_pred[c] = np.abs(((F3*(F3+1)) - (F2*(F2-1)))\/2)\n        elif F1 == 4:\n            y_pred[c] = 50 + (F2 - F3)\n        elif F1 == 5:\n            y_pred[c] = np.minimum(F2,F3)\n        elif F1 == 6:\n            y_pred[c] = np.maximum(F2,F3)\n        elif F1 == 7:\n            y_pred[c] = ((F2 * F3) % 9) * 11\n        elif F1 == 8:\n            temp = ((F2**2)+1)*(F2) + (F3)*(F3+1)\n            y_pred[c] = moreThan99(temp)\n        elif F1 == 9:\n            y_pred[c] = 50 + F2\n        c+=1\n    return y_pred","6477f619":"y_pred = ruleBaseModel(df_train_mse)","956d8c0d":"y_true = df_train_mse['predict'].to_list()","86a70341":"MAE(y_true,y_pred)","56f71bc6":"from sklearn.tree import DecisionTreeRegressor\nfrom sklearn.model_selection import GridSearchCV","700ccbc0":"df_train_mse_model.isnull().sum()","fd64dd04":"np.random.seed(0)\nnan_rows = df_train_mse_model['feature1'].isna()\nrandom_age = np.random.choice(df_train_mse_model['feature1'][~nan_rows], replace=True, size=sum(nan_rows))\ndf_train_mse_model.loc[nan_rows,'feature1'] = random_age","0ea4bea9":"X_train_value = df_train_mse_model.loc[:,['feature1','feature2','feature3']].to_numpy()\nY_train_value = df_train_mse_model.loc[:,['predict']].to_numpy()","70b1d05c":"X_train_value","65c1a38a":"Y_train_value","986d94bb":"X_train, X_test, y_train, y_test = train_test_split(X_train_value, Y_train_value, test_size=0.1, random_state=42)","de4fe578":"regressor = DecisionTreeRegressor(random_state=2020,)","1e1d8a2f":"regressor.fit(X_train,y_train)","c67b1c01":"regressor.score(X_train,y_train)","5c5d6e6c":"y_pred = regressor.predict(X_test)","4ff77fe6":"MAE(y_test,y_pred)","2a4fb1fa":"test_more_onedigit = pd.read_csv('..\/input\/thaimnist\/test.rules.csv')\ndf_test = test_more_onedigit.copy()","9bc36813":"def joinPathTest(path):\n    new_path = os.path.join(GCS_DS_PATH,'test',path)\n    return new_path","96b1c320":"df_test.isnull().sum()","6a975367":"df_test.loc[~df_test['feature1'].isnull(),'feature1'] = df_test.loc[~df_test['feature1'].isnull(),'feature1'].apply(joinPathTest)\ndf_test['feature2'] = df_test['feature2'].apply(joinPathTest)\ndf_test['feature3'] = df_test['feature3'].apply(joinPathTest)","9184a42a":"df_test","901f1b5b":"feature1_list = df_test.loc[~df_test['feature1'].isnull(),'feature1'].to_list()\nfeature2_list = df_test['feature2'].to_list()\nfeature3_list = df_test['feature3'].to_list()","3e5a407c":"test_feature1_dataset = (\n    tf.data.Dataset\n    .from_tensor_slices(feature1_list)\n    .map(decode_image, num_parallel_calls=AUTO)\n    .batch(BATCH_SIZE)\n    .cache()\n)\n\ntest_feature2_dataset = (\n    tf.data.Dataset\n    .from_tensor_slices(feature2_list)\n    .map(decode_image, num_parallel_calls=AUTO)\n    .batch(BATCH_SIZE)\n    .cache()\n)\n\ntest_feature3_dataset = (\n    tf.data.Dataset\n    .from_tensor_slices(feature3_list)\n    .map(decode_image, num_parallel_calls=AUTO)\n    .batch(BATCH_SIZE)\n    .cache()\n)","8db2e669":"Feature1_KFold_pred = []\nfor k in range(KFoldNo):\n  pred_feature1 = model_KFOLD[k].predict(test_feature1_dataset, verbose=1)\n  pred_feature1 = pred_feature1.argmax(axis=1)\n  Feature1_KFold_pred.append(pred_feature1)\nFeature1_KFold_pred = np.array(Feature1_KFold_pred)","6c455958":"Feature2_KFold_pred = []\nfor k in range(KFoldNo):\n  pred_feature2 = model_KFOLD[k].predict(test_feature2_dataset, verbose=1)\n  pred_feature2 = pred_feature2.argmax(axis=1)\n  Feature2_KFold_pred.append(pred_feature2)\nFeature2_KFold_pred = np.array(Feature2_KFold_pred)","9d9d34d8":"Feature3_KFold_pred = []\nfor k in range(KFoldNo):\n  pred_feature3 = model_KFOLD[k].predict(test_feature3_dataset, verbose=1)\n  pred_feature3 = pred_feature3.argmax(axis=1)\n  Feature3_KFold_pred.append(pred_feature3)\nFeature3_KFold_pred = np.array(Feature3_KFold_pred)","cb764631":"from scipy.stats import mode","9baf9977":"pred_feature1 = mode(Feature1_KFold_pred)[0]\npred_feature2 = mode(Feature2_KFold_pred)[0]\npred_feature3 = mode(Feature3_KFold_pred)[0]","66333013":"df_test.loc[~df_test['feature1'].isnull(),'feature1'] = pred_feature1[0]\ndf_test['feature2'] = pred_feature2[0]\ndf_test['feature3'] = pred_feature3[0]","b5ca8126":"df_test","895dd7fd":"y_pred = ruleBaseModel(df_test)","2aa35a6c":"df_test['predict'] = y_pred","e8de56d1":"df_test = df_test.loc[:,['id','predict']]","3c8a3e55":"df_test = df_test.set_index('id')\ndf_test['predict'] = df_test['predict'].apply(int)","62268982":"df_test","13693de0":"df_test.to_csv(\"submissions.csv\")","6d4288d2":"# Prepare Data","a0e352e6":"### F1 == 2\nP == (F2+F3)*abs(F2-F3)","914cfcf2":"## Prepare Data","1c344fc1":"### F1==7:\n\n((F2 * F3)%9)*11","4879ac4c":"### F1==8 \nP=((F2^2)+1)(F2) +(F3)(F3+1\n\nif more than 99, mod 99","703cb39f":"# Visualization","03bc0143":"# Model","35b60dde":"## Define optimizer","0d07404d":"## TPU Check","5897aae8":"### F1==5\nP=min(F2,F3)","b4309909":"### F1 == 4\nP=50+(F2-F3)\n","b2f9d3b7":"### F1==6:\nP=max(F2,F3)","ea8f38eb":"## Drop noise data\nThank you P.chitip 22p24c0002","6233794c":"## Define model","910a9d12":"## Declare Gobal variable","20e624ff":"## Rule base","c1bb8c93":"## Define model","f25dc954":"# Train","449f6627":"### Predict Feature 1","629ab06c":"## Callback function","81b98a15":"## Import Data","1c23c314":"## Define Loss","698cd522":"## Preprocessing","e744654e":"## Prepare data","eb68d2b4":"With one digit","72a01a30":"### F1==3\nabs((F3(F3 +1) - F2(F2-1))\/2)","b5cb9ad9":"## Import library ","0266a6b6":"# Submit","b41c8cee":"# CNN Model","62f84eee":"### F1 == NaN\nP=F2+F3","72cfe04c":"### Predict Feature 2","d048f7b1":"### F1==1\nP==abs(F2-F3)","4c12e22f":"# Import","b8b9efd0":"### Create rule base model","b205ca23":"### Predict","3e58ae41":"## Rule Base ","57760a7f":"### Predict Feature 3","08fb561f":"# Result","2de2b0b9":"## Prepare Data","c7906c74":"## Submit","19e45c63":"### F1 == 0\nP=F2*F3\n","29efc5ce":"### F1==9:\nP=50+F2","8f387077":"## KFOLD"}}