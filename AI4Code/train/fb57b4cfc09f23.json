{"cell_type":{"a7aa1537":"code","8917c719":"code","b8f2bf90":"code","e56612f8":"code","d720f6a4":"markdown"},"source":{"a7aa1537":"import os\nimport shutil\nimport feather\nimport numpy as np\nimport pandas as pd\nimport lightgbm as lgb\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.model_selection import train_test_split\nimport eli5\nfrom eli5.sklearn import PermutationImportance\n\nNROUNDS = 1000000\nEARLY_STOPPING = 3000\nRANDOM_STATE = 44000\n\n###############################################################\n# Loading data\n###############################################################\n\nprint(\"Loading and spliting data...\")\n\ntrain = feather.read_dataframe('..\/input\/data-serialization\/train.feather')\ntest = feather.read_dataframe('..\/input\/data-serialization\/test.feather')\ny = feather.read_dataframe('..\/input\/data-serialization\/target.feather')\n\"\"\"\ntrn_x, val_x, trn_y, val_y = train_test_split(train.values, y.values.ravel(), \n                                              random_state=RANDOM_STATE)\n\"\"\"\ntrn_x, val_x, trn_y, val_y = train_test_split(train, y.values.ravel(), random_state=RANDOM_STATE)","8917c719":"print(\"Training the classifier...\")\n\nclf = lgb.LGBMClassifier(n_estimators=NROUNDS,\n                         #**params, \n                         num_leaves=13,\n                         boost_from_average=False,\n                         objective='binary',\n                         max_depth=-1,\n                         learning_rate=0.01,\n                         boosting='gbdt',\n                         min_data_in_leaf=80,\n                         bagging_freq=5,\n                         bagging_fraction=0.4,\n                         feature_fraction=0.05,\n                         min_sum_hessian_in_leaf=10.0,\n                         tree_learner='serial',\n                         #bagging_seed=11,\n                         #reg_alpha=5,\n                         #reg_lambda=5,\n                         metric='auc',\n                         verbosity=1,\n                         #subsample=0.81,\n                         #min_gain_to_split=0.01077313523861969,\n                         #min_child_weight=19.428902804238373,\n                         num_threads=4)\n\n     \nclf.fit(trn_x, trn_y, eval_set=[(val_x, val_y)], verbose=3000,\n        early_stopping_rounds=EARLY_STOPPING)","b8f2bf90":"print(\"Computing permtation importances...\")\n\nperm = PermutationImportance(clf, random_state=RANDOM_STATE).fit(val_x, val_y)","e56612f8":"print(\"Computing the ELI5 weights for LightGBM...\")\n\neli5.show_weights(perm, feature_names = val_x.columns.tolist(), top=200)","d720f6a4":"This notebook computes ELI5 weights for LightGBM. Currently, there is a number of public kernels showing ELI5 weights for Random Forest Classifiers but I have not seen the LGBM version. I think the latter would be much more relevant for this competition.\n\nIf you decide to run this notebook be aware that it takes a few hours to compute the permutation importance (see the step\n\n`perm = PermutationImportance(clf, random_state=RANDOM_STATE).fit(val_x, val_y)` \n\nbelow). "}}