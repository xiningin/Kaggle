{"cell_type":{"625343b6":"code","fd5976ba":"code","6cc2a863":"code","ac0305d2":"code","1989f740":"code","d28df169":"code","921ef822":"code","e8d14ded":"code","69339a63":"code","839379ec":"code","f87caea0":"markdown","68626627":"markdown","a28d50b2":"markdown","4f108adc":"markdown","983888f1":"markdown","84ee6149":"markdown","fda4d1f7":"markdown"},"source":{"625343b6":"!pip uninstall fsspec -qq -y\n!pip install --no-index --find-links ..\/input\/hf-datasets\/wheels datasets -qq\n!pip install -U --no-build-isolation --no-deps ..\/input\/transformers-master\/\n","fd5976ba":"APEX_INSTALLED = False\nprint(APEX_INSTALLED)","6cc2a863":"import sys\nsys.path.append(\"..\/input\/tez-lib\/\")\nfrom collections import Counter\nimport tez \nfrom functools import partial\nimport os\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\nimport gc\ngc.enable()\nimport math\nimport json\nimport time\nimport random\nimport multiprocessing\nimport warnings\nwarnings.filterwarnings(\"ignore\", category=UserWarning)\n\nimport numpy as np\nimport pandas as pd\nfrom tqdm import tqdm, trange\nfrom sklearn import model_selection\nfrom string import punctuation\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.nn import Parameter\nimport torch.optim as optim\nfrom torch.utils.data import (\n    Dataset, DataLoader,\n    SequentialSampler, RandomSampler\n)\nfrom torch.utils.data.distributed import DistributedSampler\nimport sys \nfrom datasets import Dataset as HFDataset\nfrom transformers import AutoModelForQuestionAnswering, TrainingArguments, Trainer\nfrom transformers import default_data_collator\n\nimport transformers\nfrom transformers import (\n    WEIGHTS_NAME,\n    AdamW,\n    AutoConfig,\n    AutoModel,\n    AutoTokenizer,\n    get_cosine_schedule_with_warmup,\n    get_linear_schedule_with_warmup,\n    logging,\n    MODEL_FOR_QUESTION_ANSWERING_MAPPING,\n)","ac0305d2":"def fix_all_seeds(seed):\n    np.random.seed(seed)\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n\ndef optimal_num_of_loader_workers():\n    num_cpus = multiprocessing.cpu_count()\n    num_gpus = torch.cuda.device_count()\n    optimal_value = min(num_cpus, num_gpus*4) if num_gpus else num_cpus - 1\n    return optimal_value\n\n#print(f\"Apex AMP Installed :: {APEX_INSTALLED}\")\nMODEL_CONFIG_CLASSES = list(MODEL_FOR_QUESTION_ANSWERING_MAPPING.keys())\nMODEL_TYPES = tuple(conf.model_type for conf in MODEL_CONFIG_CLASSES)\n\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2021-09-25T15:05:01.923913Z\",\"iopub.execute_input\":\"2021-09-25T15:05:01.924388Z\",\"iopub.status.idle\":\"2021-09-25T15:05:01.936163Z\",\"shell.execute_reply.started\":\"2021-09-25T15:05:01.924353Z\",\"shell.execute_reply\":\"2021-09-25T15:05:01.935458Z\"}}\nclass DatasetRetriever(Dataset):\n    def __init__(self, features, mode='train'):\n        super(DatasetRetriever, self).__init__()\n        self.features = features\n        self.mode = mode\n        \n    def __len__(self):\n        return len(self.features)\n    \n    def __getitem__(self, item):   \n        feature = self.features[item]\n        if self.mode == 'train':\n            return {\n                'input_ids':torch.tensor(feature['input_ids'], dtype=torch.long),\n                'attention_mask':torch.tensor(feature['attention_mask'], dtype=torch.long),\n                'offset_mapping':torch.tensor(feature['offset_mapping'], dtype=torch.long),\n                'start_position':torch.tensor(feature['start_position'], dtype=torch.long),\n                'end_position':torch.tensor(feature['end_position'], dtype=torch.long)\n            }\n        else:\n            return {\n                'input_ids':torch.tensor(feature['input_ids'], dtype=torch.long),\n                'attention_mask':torch.tensor(feature['attention_mask'], dtype=torch.long),\n                'offset_mapping':feature['offset_mapping'],\n                'sequence_ids':feature['sequence_ids'],\n                'id':feature['example_id'],\n                'context': feature['context'],\n                'question': feature['question']\n            }\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2021-09-25T15:05:01.938192Z\",\"iopub.execute_input\":\"2021-09-25T15:05:01.938481Z\",\"iopub.status.idle\":\"2021-09-25T15:05:01.950199Z\",\"shell.execute_reply.started\":\"2021-09-25T15:05:01.938449Z\",\"shell.execute_reply\":\"2021-09-25T15:05:01.949435Z\"}}\nclass Model(nn.Module):\n    def __init__(self, modelname_or_path, config):\n        super(Model, self).__init__()\n        self.config = config\n        self.xlm_roberta = AutoModel.from_pretrained(modelname_or_path, config=config)\n        self.qa_outputs = nn.Linear(config.hidden_size, 2)\n        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n        self._init_weights(self.qa_outputs)\n        \n    def _init_weights(self, module):\n        if isinstance(module, nn.Linear):\n            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n            if module.bias is not None:\n                module.bias.data.zero_()\n\n    def forward(\n        self, \n        input_ids, \n        attention_mask=None, \n    ):\n        outputs = self.xlm_roberta(\n            input_ids,\n            attention_mask=attention_mask,\n        )\n\n        sequence_output = outputs[0]\n        #pooled_output = outputs[1]\n        \n        # sequence_output = self.dropout(sequence_output)\n        qa_logits = self.qa_outputs(sequence_output)\n        \n        start_logits, end_logits = qa_logits.split(1, dim=-1)\n        start_logits = start_logits.squeeze(-1)\n        end_logits = end_logits.squeeze(-1)\n    \n        return start_logits, end_logits\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2021-09-25T15:05:01.951716Z\",\"iopub.execute_input\":\"2021-09-25T15:05:01.951979Z\",\"iopub.status.idle\":\"2021-09-25T15:05:01.96162Z\",\"shell.execute_reply.started\":\"2021-09-25T15:05:01.951948Z\",\"shell.execute_reply\":\"2021-09-25T15:05:01.96087Z\"}}\ndef make_model(config):\n    model_config = AutoConfig.from_pretrained(config.config_name)\n    tokenizer = AutoTokenizer.from_pretrained(config.tokenizer_name)\n    model = Model(config.tokenizer_name, config=model_config)\n    return config, tokenizer, model\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2021-09-25T15:05:01.962756Z\",\"iopub.execute_input\":\"2021-09-25T15:05:01.963103Z\",\"iopub.status.idle\":\"2021-09-25T15:05:01.975451Z\",\"shell.execute_reply.started\":\"2021-09-25T15:05:01.963068Z\",\"shell.execute_reply\":\"2021-09-25T15:05:01.97473Z\"}}\ndef prepare_test_features(args, example, tokenizer):\n    example[\"question\"] = example[\"question\"].lstrip()\n    \n    tokenized_example = tokenizer(\n        example[\"question\"],\n        example[\"context\"],\n        truncation=\"only_second\",\n        max_length=args.max_seq_length,\n        stride=args.doc_stride,\n        return_overflowing_tokens=True,\n        return_offsets_mapping=True,\n        padding=\"max_length\",\n    )\n\n    features = []\n    for i in range(len(tokenized_example[\"input_ids\"])):\n        feature = {}\n        feature[\"example_id\"] = example['id']\n        feature['context'] = example['context']\n        feature['question'] = example['question']\n        feature['input_ids'] = tokenized_example['input_ids'][i]\n        feature['attention_mask'] = tokenized_example['attention_mask'][i]\n        feature['offset_mapping'] = tokenized_example['offset_mapping'][i]\n        feature['sequence_ids'] = [0 if i is None else i for i in tokenized_example.sequence_ids(i)]\n        features.append(feature)\n    return features\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2021-09-25T15:05:01.978299Z\",\"iopub.execute_input\":\"2021-09-25T15:05:01.97853Z\",\"iopub.status.idle\":\"2021-09-25T15:05:01.996283Z\",\"shell.execute_reply.started\":\"2021-09-25T15:05:01.978506Z\",\"shell.execute_reply\":\"2021-09-25T15:05:01.995601Z\"}}\nimport collections\n\ndef postprocess_qa_predictions(examples, features, raw_predictions, n_best_size = 20, max_answer_length = 30, tokenizer=None):\n    all_start_logits, all_end_logits = raw_predictions\n    \n    example_id_to_index = {k: i for i, k in enumerate(examples[\"id\"])}\n    features_per_example = collections.defaultdict(list)\n    for i, feature in enumerate(features):\n        features_per_example[example_id_to_index[feature[\"example_id\"]]].append(i)\n\n    \n    #predictions = collections.OrderedDict()\n    predictions1 = []\n    predictions2 = []\n    predictions3 = []\n    \n    print(f\"Post-processing {len(examples)} example predictions split into {len(features)} features.\")\n\n    for example_index, example in examples.iterrows():\n        feature_indices = features_per_example[example_index]\n\n        min_null_score = None\n        valid_answers = []\n        \n        context = example[\"context\"]\n        for feature_index in feature_indices:\n            start_logits = all_start_logits[feature_index]\n            end_logits = all_end_logits[feature_index]\n\n            sequence_ids = features[feature_index][\"sequence_ids\"]\n            context_index = 1\n\n            features[feature_index][\"offset_mapping\"] = [\n                (o if sequence_ids[k] == context_index else None)\n                for k, o in enumerate(features[feature_index][\"offset_mapping\"])\n            ]\n            offset_mapping = features[feature_index][\"offset_mapping\"]\n            cls_index = features[feature_index][\"input_ids\"].index(tokenizer.cls_token_id)\n            feature_null_score = start_logits[cls_index] + end_logits[cls_index]\n            if min_null_score is None or min_null_score < feature_null_score:\n                min_null_score = feature_null_score\n\n            start_indexes = np.argsort(start_logits)[-1 : -n_best_size - 1 : -1].tolist()\n            end_indexes = np.argsort(end_logits)[-1 : -n_best_size - 1 : -1].tolist()\n            for start_index in start_indexes:\n                for end_index in end_indexes:\n                    if (\n                        start_index >= len(offset_mapping)\n                        or end_index >= len(offset_mapping)\n                        or offset_mapping[start_index] is None\n                        or offset_mapping[end_index] is None\n                    ):\n                        continue\n                    # Don't consider answers with a length that is either < 0 or > max_answer_length.\n                    if end_index < start_index or end_index - start_index + 1 > max_answer_length:\n                        continue\n\n                    start_char = offset_mapping[start_index][0]\n                    end_char = offset_mapping[end_index][1]\n                    #if ')' in context[start_char: end_char] or '(' in context[start_char: end_char] or '...' in context[start_char: end_char]:\n                    #    continue\n                    valid_answers.append(\n                        {\n                            \"score\": start_logits[start_index] + end_logits[end_index],\n                            \"text\": context[start_char: end_char]\n                        }\n                    )\n        \n        if len(valid_answers) > 0:\n            answer_candidates = sorted(valid_answers, key=lambda x: x[\"score\"], reverse=True)\n            best_answer1 = answer_candidates[0][\"text\"]\n            try:\n                best_answer2 = answer_candidates[1][\"text\"]\n            except: \n                best_answer2 = ''\n            try:\n                best_answer3 = answer_candidates[2][\"text\"]\n            except: \n                best_answer3 = ''   \n        else:\n            best_answer1 = best_answer2 = best_answer3 = '' #{\"text\": \"\", \"score\": 0.0}\n            \n        # Multi white space removal\n        best_answer1 = \" \".join(best_answer1.split())\n        # Punc removal \n        best_answer1 = best_answer1.strip(punctuation)\n        predictions1.append(best_answer1)\n        \n        # Multi white space removal\n        best_answer2 = \" \".join(best_answer2.split())\n        # Punc removal \n        best_answer2 = best_answer2.strip(punctuation)\n        predictions2.append(best_answer2)\n        \n        \"\"\"\n        # Multi white space removal\n        best_answer3 = \" \".join(best_answer3.split())\n        # Punc removal \n        best_answer3 = best_answer3.strip(punctuation)\n        predictions3.append(best_answer3)\n        \"\"\"\n        \n    return predictions1, predictions2 #, predictions3\n\n\n\ndef get_predictions(global_config, checkpoint_path, test_dataloader):\n    config, tokenizer, model = make_model(global_config)\n    model.cuda()\n    model.load_state_dict(torch.load(checkpoint_path)) #, map_location='cuda:0'))\n    model.half()\n    model.eval()\n    print(f'Running inference for model: {checkpoint_path}') \n    start_logits = []\n    end_logits = []\n    for batch in test_dataloader:\n        with torch.no_grad():\n            outputs_start, outputs_end = model(batch['input_ids'].cuda(), batch['attention_mask'].cuda())\n            start_logits.append(outputs_start.cpu().numpy().tolist())\n            end_logits.append(outputs_end.cpu().numpy().tolist())\n            del outputs_start, outputs_end\n    del model, tokenizer, config\n    gc.collect()\n    return np.vstack(start_logits), np.vstack(end_logits)\n\n\n\nclass ChaiiModel(tez.Model):\n    def __init__(self, model_name, num_train_steps, steps_per_epoch, learning_rate):\n        super().__init__()\n        self.learning_rate = learning_rate\n        self.steps_per_epoch = steps_per_epoch\n        self.model_name = model_name\n        self.num_train_steps = num_train_steps\n        self.step_scheduler_after = \"batch\"\n\n        hidden_dropout_prob: float = 0.0\n        layer_norm_eps: float = 1e-7\n\n        config = transformers.AutoConfig.from_pretrained(model_name)\n        config.update(\n            {\n                \"output_hidden_states\": True,\n                \"hidden_dropout_prob\": hidden_dropout_prob,\n                \"layer_norm_eps\": layer_norm_eps,\n                \"add_pooling_layer\": False,\n            }\n        )\n        self.transformer = transformers.AutoModel.from_pretrained(model_name, config=config)\n        self.output = nn.Linear(config.hidden_size, config.num_labels)\n\n    def forward(self, ids, mask, token_type_ids=None, start_positions=None, end_positions=None):\n        transformer_out = self.transformer(ids, mask)\n        sequence_output = transformer_out[0]\n        logits = self.output(sequence_output)\n        start_logits, end_logits = logits.split(1, dim=-1)\n        start_logits = start_logits.squeeze(-1).contiguous()\n        end_logits = end_logits.squeeze(-1).contiguous()\n\n        return (start_logits, end_logits), 0, {}\n\n\nclass ChaiiDataset:\n    def __init__(self, data):\n        self.data = data\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, item):\n        return {\n            \"ids\": torch.tensor(self.data[item][\"input_ids\"], dtype=torch.long),\n            \"mask\": torch.tensor(self.data[item][\"attention_mask\"], dtype=torch.long),\n        }\n\n\ndef get_predictions_tez(model_path, data_loader):\n    print(f'Running inference for model {model_path}')\n    model = ChaiiModel(model_name=\"..\/input\/xlm-roberta-squad2\/deepset\/xlm-roberta-large-squad2\", num_train_steps=0, steps_per_epoch=0, learning_rate=0)\n    model.load(model_path, weights_only=True)\n    model.half()\n    model.to(\"cuda\")\n    model.eval()\n\n    start_logits = []\n    end_logits = []\n\n    for b_idx, data in enumerate(data_loader):\n        with torch.no_grad():\n            for key, value in data.items():\n                data[key] = value.to(\"cuda\")\n            output, _, _ = model(**data)\n            start = output[0].detach().cpu().numpy()\n            end = output[1].detach().cpu().numpy()\n            start_logits.append(start)\n            end_logits.append(end)\n\n    start_logits = np.vstack(start_logits)\n    end_logits = np.vstack(end_logits)\n\n    return start_logits, end_logits\n\n\n","1989f740":"def clean_pred(predictions):\n  \n    bad_starts = [\".\", \",\", \"(\", \")\", \"-\", \"\u2013\",  \",\", \";\"]\n    bad_endings = [\"...\", \"-\", \"(\", \")\", \"\u2013\", \",\", \";\"]\n\n    tamil_ad = \"\u0b95\u0bbf.\u0baa\u0bbf\"\n    tamil_bc = \"\u0b95\u0bbf.\u0bae\u0bc1\"\n    tamil_km = \"\u0b95\u0bbf.\u0bae\u0bc0\"\n    hindi_ad = \"\u0908\"\n    hindi_bc = \"\u0908.\u092a\u0942\"\n    hindi_ad1 = \"\u090f.\u0921\u0940\"\n\n    cleaned_preds = []\n    for pred in predictions:\n        while any([pred.startswith(y) for y in bad_starts]):\n            pred = pred[1:]\n            \n        while any([pred.endswith(y) for y in bad_endings]):\n            if pred.endswith(\"...\"):\n                pred = pred[:-3]\n            else:\n                pred = pred[:-1]\n\n            if any([pred.endswith(tamil_ad), pred.endswith(tamil_bc), pred.endswith(tamil_km), pred.endswith(hindi_ad), pred.endswith(hindi_ad1), pred.endswith(hindi_bc)]) and pred+\".\" in context:\n                pred = pred+\".\"\n        \n        cleaned_preds.append(pred)\n\n    return cleaned_preds","d28df169":"model1 = '..\/input\/rembert-pt'\nmodel2 = model3 = '..\/input\/xlm-roberta-squad2\/deepset\/xlm-roberta-large-squad2'\nmodel4 = '..\/input\/muril-large-pt\/muril-large-cased'\nmodel_archs = [model1, model2, model3, model4]\n\ntokenizers = [\n                 AutoTokenizer.from_pretrained(model1),\n                 AutoTokenizer.from_pretrained(model2),\n                 AutoTokenizer.from_pretrained(model3),\n                 AutoTokenizer.from_pretrained(model4),\n                ]\n    \n\n    \ntk_rembert_base = '..\/input\/d\/trushk\/chaii-rembert-1024-all\/'\ntk_rembert_trial = [\n                tk_rembert_base+'checkpoint-fold-0',\n                tk_rembert_base+'checkpoint-fold-1',\n                #rembert_base+'checkpoint-fold-2',\n                tk_rembert_base+'checkpoint-fold-3',\n                tk_rembert_base+'checkpoint-fold-4'\n                ]\n\nrembert_base = '..\/input\/rembert-5fold-1-epoch\/rembert-5fold-1-epoch\/'\nrembert_best = [\n                rembert_base+'checkpoint-fold-0',\n                rembert_base+'checkpoint-fold-2',\n                rembert_base+'checkpoint-fold-3',\n                rembert_base+'checkpoint-fold-4'\n                ]\n\nrembert_trial = [\n                rembert_base+'checkpoint-fold-2',\n                tk_rembert_base+'checkpoint-fold-3',\n                tk_rembert_base+'checkpoint-fold-4',\n                rembert_base+'checkpoint-fold-0',\n                ]\n\n\nxlmr_best =  ['..\/input\/5foldsroberta\/output\/checkpoint-fold-0','..\/input\/5foldsroberta\/output\/checkpoint-fold-1',\n                   '..\/input\/5foldsroberta\/output\/checkpoint-fold-2', '..\/input\/5foldsroberta\/output\/checkpoint-fold-3',\n                   '..\/input\/5foldsroberta\/output\/checkpoint-fold-4'\n                  ]\n\nxlmr_base = '..\/input\/public-xlm-2pochs\/'\nxlmr_trial = [\n    xlmr_base + 'checkpoint-fold-0',\n    xlmr_base + 'checkpoint-fold-1',\n    xlmr_base + 'checkpoint-fold-2',\n    xlmr_base + 'checkpoint-fold-3',\n    xlmr_base + 'checkpoint-fold-4',\n\n]\n\ntez_base = '..\/input\/xlrm-large-tez-1015-10fold\/'\ntez_models = [ tez_base + 'xlm-roberta-large-squad2_fold_0.bin', tez_base + 'xlm-roberta-large-squad2_fold_1.bin',\n              tez_base + 'xlm-roberta-large-squad2_fold_2.bin', tez_base + 'xlm-roberta-large-squad2_fold_3.bin',\n              tez_base + 'xlm-roberta-large-squad2_fold_4.bin', tez_base + 'xlm-roberta-large-squad2_fold_5.bin',\n              tez_base + 'xlm-roberta-large-squad2_fold_6.bin', tez_base + 'xlm-roberta-large-squad2_fold_7.bin',\n              tez_base + 'xlm-roberta-large-squad2_fold_8.bin', tez_base + 'xlm-roberta-large-squad2_fold_9.bin',\n             ]\n    \ntez_best = [ \n              tez_base + 'xlm-roberta-large-squad2_fold_7.bin',\n              tez_base + 'xlm-roberta-large-squad2_fold_0.bin',\n              tez_base + 'xlm-roberta-large-squad2_fold_9.bin',\n              #tez_base + 'xlm-roberta-large-squad2_fold_1.bin', \n              #tez_base + 'xlm-roberta-large-squad2_fold_4.bin',\n              #tez_base + 'xlm-roberta-large-squad2_fold_3.bin',\n              #tez_base + 'xlm-roberta-large-squad2_fold_8.bin',\n             ]\n\n\ntez_trial = [ \n              tez_base + 'xlm-roberta-large-squad2_fold_7.bin',\n              tez_base + 'xlm-roberta-large-squad2_fold_0.bin',\n              tez_base + 'xlm-roberta-large-squad2_fold_9.bin',\n              tez_base + 'xlm-roberta-large-squad2_fold_1.bin', \n              #tez_base + 'xlm-roberta-large-squad2_fold_6.bin', \n              #tez_base + 'xlm-roberta-large-squad2_fold_4.bin',\n              #tez_base + 'xlm-roberta-large-squad2_fold_3.bin',\n              #tez_base + 'xlm-roberta-large-squad2_fold_8.bin',\n             ]\n\nmuril_best = [\n    '..\/input\/chaii-muril-large-1020\/checkpoint-fold-0',\n    '..\/input\/chaii-muril-large-1020\/checkpoint-fold-1',  \n    #'..\/input\/chaii-muril-large-1020\/checkpoint-fold-4',  #Drop to replace new muril\n    '..\/input\/chaii-muril-large-1020\/checkpoint-fold-3',\n    '..\/input\/chaii-muril-large-1111\/checkpoint-fold-0',\n    #'..\/input\/chaii-muril-large-1111\/checkpoint-fold-3'  Untested\n                  ]\n\nmuril_trial = [\n    '..\/input\/chaii-muril-large-1020\/checkpoint-fold-0', # val loss 1.29339.  CV 0.72\n    '..\/input\/chaii-muril-large-1020\/checkpoint-fold-3',\n    '..\/input\/chaii-muril-large-1111\/checkpoint-fold-0',\n    '..\/input\/rembert-new\/output\/checkpoint-fold-0', # labeled incorrectly. muril val loss 1.21\n   # '..\/input\/chaii-muril-large-1020\/checkpoint-fold-1',  #Drop for more time for sim voter or tez. \n                  ]\n    \nmodel_paths = [ \n                    rembert_best,\n                    xlmr_best,\n                    tez_trial,\n                    muril_best\n                 ]\n\n\"\"\"\n# For debug\nmodel_paths = [ \n                [rembert_base+'checkpoint-fold-3',],\n                [xlmr_base + 'checkpoint-fold-0'],\n                #['..\/input\/5foldsroberta\/output\/checkpoint-fold-0'],\n                #[ tez_base + 'xlm-roberta-large-squad2_fold_0.bin'],\n                [tez_new_base + 'deepsetxlm-roberta-large-squad2__fold_0.bin'],\n                ['..\/input\/chaii-muril-large-1020\/checkpoint-fold-0'], \n              ]\n\"\"\"\n\ntest = pd.read_csv('..\/input\/chaii-hindi-and-tamil-question-answering\/test.csv')        \ntest['context'] = test['context'].apply(lambda x: ' '.join(x.split()))\ntest['question'] = test['question'].apply(lambda x: ' '.join(x.split()))\n\nall_predictions1 = []\nall_predictions2 = []\n#all_predictions3 = []\n\n# Iterate through all models and get features, predictions\nfor i in range(len(model_archs)):\n        tokenizer = tokenizers[i]\n        \n        tez_index = 2\n        # Reuse for tez xlmr \n        if i != tez_index:\n             test_features = []\n        \n        if True:\n            class Config:\n                #model\n                model_arch = model_archs[i]\n                model_name_or_path = model_arch\n                config_name = model_arch\n                tokenizer_name = model_arch\n                fp16 = True if APEX_INSTALLED else False\n                fp16_opt_level = \"O1\"\n                gradient_accumulation_steps = 2\n                # tokenizer\n                max_seq_length = 400\n                doc_stride = 135\n                if 'rembert' in model_paths[i][0]:\n                    eval_batch_size = 128\n                else:\n                    eval_batch_size = 128\n                # optimzer\n                optimizer_type = 'AdamW'\n                learning_rate = 1e-5\n                weight_decay = 1e-2\n                seed = 2021\n\n            args = Config()\n            print(f'Batch size: {args.eval_batch_size}')\n            \n            if i==tez_index:\n                #test_features = [item for sublist in test_features for item in sublist]\n                test_dataloader = torch.utils.data.DataLoader(\n                     ChaiiDataset(test_features),\n                     batch_size= args.eval_batch_size,\n                     num_workers=optimal_num_of_loader_workers(),\n                     pin_memory=True,\n                     shuffle=False\n                )\n            else:\n                for _, row in test.iterrows():\n                    test_features += prepare_test_features(Config(), row, tokenizer)                \n                test_dataset = DatasetRetriever(test_features, mode='test')\n                test_dataloader = DataLoader(\n                    test_dataset,\n                    batch_size=args.eval_batch_size,\n                    sampler=SequentialSampler(test_dataset),\n                    num_workers=optimal_num_of_loader_workers(),\n                    pin_memory=True,\n                    drop_last=False\n                )\n                \n        print(f'Running inference for model {model_archs[i]}')\n        test_shape = test.shape[0]\n                  \n        all_start_logits = []\n        all_end_logits = []\n        fin_start_logits  = None\n        for j in range(len(model_paths[i])):\n            if 'tez' in model_paths[i][0]:\n                start_logits, end_logits = get_predictions_tez(f'{model_paths[i][j]}', test_dataloader)\n            else:\n                start_logits, end_logits = get_predictions(Config(), f'{model_paths[i][j]}\/pytorch_model.bin', test_dataloader)\n            if fin_start_logits is None:\n                fin_start_logits = start_logits\n                fin_end_logits = end_logits\n            else:\n                fin_start_logits += start_logits\n                fin_end_logits += end_logits\n            gc.collect()\n            torch.cuda.empty_cache()\n            start_logits = fin_start_logits \/ len(model_paths[i])\n            end_logits = fin_end_logits \/ len(model_paths[i])\n        \n        raw_predictions1, raw_predictions2 = postprocess_qa_predictions(test, test_features, (start_logits, end_logits), tokenizer=tokenizer)\n\n        gc.collect()\n        torch.cuda.empty_cache()\n\n        predictions1 = [pred.strip() for pred in raw_predictions1]\n        cleaned_predictions1 = clean_pred(predictions1)\n        all_predictions1.append(cleaned_predictions1)\n        \n        predictions2 = [pred.strip() for pred in raw_predictions2]\n        cleaned_predictions2 = clean_pred(predictions2)\n        all_predictions2.append(cleaned_predictions2)\n        \n        \"\"\"\n        predictions3 = [pred.strip() for pred in raw_predictions3]\n        cleaned_predictions3 = clean_pred(predictions3)\n        all_predictions3.append(cleaned_predictions3)\n        \"\"\"\n        if test_shape == 5:\n            print(predictions1)\n            print(predictions2)\n            #print(predictions3)\n\n\n","921ef822":"from Levenshtein import ratio\n\ndef calc_jaccard(str1, str2):\n    a = set(str1.lower().split())\n    b = set(str2.lower().split())\n    c = a.intersection(b)\n    jac = float(len(c)) \/ (len(a) + len(b) - len(c))\n    print(f'str1: {str1} str2:{str2}, jac:{jac}')\n    return jac\n\ndef check_lev(str1, str2, verbose=False):\n    l_ratio = ratio(str1,str2)\n    if verbose:\n        print(f'str1: {str1} str2:{str2}, lev:{l_ratio}')\n    return l_ratio","e8d14ded":"def check_similarity(votes, verbose=False):\n    if verbose:\n        print(f'votes {votes}')\n    counter_len = len(votes)\n    adder=0\n    best_pred = votes[0][0]\n    best_vote_count = votes[0][1]\n    # If more than one vote find second best\n    if counter_len > 1:\n        second_best_pred = votes[1][0]\n        second_best_vote_count = votes[1][1]\n    else:\n        second_best_pred = None\n        second_best_vote_count = 0\n    # If more than 2 votes, check 2nd best vs rest\n    if counter_len > 2:\n        for i in range(2, counter_len):\n            score = check_lev(votes[1][0], votes[i][0], verbose=verbose)\n            if score >= 0.3:\n                adder +=1\n    \n    if adder > 0:\n        second_best_vote_count += adder\n        if verbose:\n            print(f'Increasing vote by {adder}: New second best {second_best_vote_count}')\n        if second_best_vote_count > best_vote_count:\n            return second_best_pred\n        else:\n            return best_pred\n    else:\n        return best_pred\n","69339a63":"#### Iterate through all predictions and get predictions with max votes\nfinal_predictions = []\nfor i in range(len(test)):\n        # Vote top 1 \n        predictions = [all_predictions1[0][i]] + [all_predictions1[1][i]]  + [all_predictions1[2][i]]  + [all_predictions1[3][i]]\n        # Vote top 2\n        #predictions = [all_predictions1[0][i]] + [all_predictions1[1][i]]  + [all_predictions1[2][i]]  + [all_predictions1[3][i]] + [all_predictions2[0][i]] + [all_predictions2[1][i]]  + [all_predictions2[2][i]]  + [all_predictions2[3][i]]\n        #predictions = [all_predictions1[0][i]] + [all_predictions1[1][i]]  + [all_predictions2[0][i]] + [all_predictions2[1][i]] \n        x = Counter(predictions)\n        if test_shape == 5:\n            print(f'Most common: {x.most_common(1)[0]}')\n            print(f'All predictions {x}')\n        prediction, vote_count = x.most_common(1)[0]\n        # Select majority \n        if vote_count > 1:\n            final_pred = prediction\n        # If no majority \n        else:\n            #final_pred = calc_levenshtein_pred(all_predictions[0][i], all_predictions[2][i], all_predictions[1][i])\n            # Vote top 2\n            predictions = [all_predictions1[0][i]] + [all_predictions1[1][i]]  + [all_predictions1[2][i]]  + [all_predictions1[3][i]] + [all_predictions2[0][i]] + [all_predictions2[1][i]]  + [all_predictions2[2][i]]  + [all_predictions2[3][i]]\n            x = Counter(predictions)\n            if test_shape == 5:\n                print(f'Most common: {x.most_common(1)[0]}')\n                print(f'All predictions {x}')\n            votes = x.most_common()\n            # Max vote\n            prediction, vote_count = votes[0]\n            if len(votes) > 1 :\n                _, vote_count2 = votes[1]\n                if vote_count > vote_count2:\n                    final_pred = prediction\n                else:\n                    if test_shape == 5:\n                        verbose= True\n                    else:\n                        verbose = False\n                    final_pred = check_similarity(votes, verbose=verbose)\n            else:\n                # lb 792 reference\n                final_pred = all_predictions1[1][i]\n            if test_shape == 5:\n                print(f'Max votes for {final_pred} with {vote_count} num votes')\n        final_predictions.append(final_pred)\n\n\nprint(final_predictions)\n","839379ec":"cleaned_predictions = final_predictions\ntest_data = pd.read_csv('..\/input\/chaii-hindi-and-tamil-question-answering\/test.csv')\ntest_data[\"PredictionString\"] = cleaned_predictions\ntest_data[['id', 'PredictionString']].to_csv('submission.csv', index=False)\nprint(test_data[\"PredictionString\"])","f87caea0":"## Voting Serial\n","68626627":"## Change vote count if votes are similar","a28d50b2":"!export CUDA_HOME=\/usr\/local\/cuda-11.0\n!env | grep CUDA","4f108adc":"## Results","983888f1":"## Post Processing","84ee6149":"!cd ..\/input\/apex-master-10-27-2021\/apex-master\/ && pip install -v --disable-pip-version-check --no-cache-dir --global-option=\"--cpp_ext\" --global-option=\"--cuda_ext\" .","fda4d1f7":"## Voting Function"}}