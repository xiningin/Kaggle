{"cell_type":{"b5fc2b54":"code","64cf8168":"code","a1ff2fb2":"code","85b5cf76":"code","edaea403":"code","209f6d49":"code","34995392":"code","467bad0a":"code","4b78be04":"code","fead624b":"code","d1c4441a":"code","554c1eaf":"code","1e0e2ac8":"markdown","6a32097e":"markdown","238f7d1e":"markdown","4305aa04":"markdown","44379ca8":"markdown","2560c614":"markdown","fbeb3a93":"markdown","ba64959f":"markdown","39fe0f6c":"markdown","b04f108f":"markdown"},"source":{"b5fc2b54":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.feature_selection import VarianceThreshold\nimport category_encoders as ce\nfrom sklearn.model_selection import StratifiedKFold\nimport gc","64cf8168":"def target_encoder(class_='', smoothing=0.2, X_train=None, X_test=None):\n    # Inspired by this great kernel - please upvote: https:\/\/www.kaggle.com\/caesarlupum\/2020-20-lines-target-encoding\n    train = X_train.copy()\n    train['target'] = np.where(tr['target']==class_, 1, 0)\n    test = X_test.copy()\n    train_y = train['target']\n    train_id = train['id']\n    test_id = test['id']\n    train.drop(['target', 'id'], axis=1, inplace=True)\n    test.drop('id', axis=1, inplace=True)\n    \n    cat_feat_to_encode = train.columns.tolist()\n    \n    oof = pd.DataFrame([])\n    \n    for tr_idx, oof_idx in StratifiedKFold(n_splits=5, random_state=42, shuffle=True).split(train, train_y):\n        ce_target_encoder = ce.TargetEncoder(cols = cat_feat_to_encode, smoothing=smoothing)\n        ce_target_encoder.fit(train.iloc[tr_idx, :], train_y.iloc[tr_idx])\n        oof = oof.append(ce_target_encoder.transform(train.iloc[oof_idx, :]), ignore_index=False)\n    \n    ce_target_encoder = ce.TargetEncoder(cols = cat_feat_to_encode, smoothing=smoothing)\n    ce_target_encoder.fit(train, train_y)\n    train = oof.reindex(train.index) #.sort_index()\n    test = ce_target_encoder.transform(test)\n    train.columns = [class_ + '_' + str(col) for col in train.columns]\n    test.columns = [class_ + '_' + str(col) for col in test.columns]\n    \n    return train, test","a1ff2fb2":"tr = pd.read_csv(\"\/kaggle\/input\/tabular-playground-series-may-2021\/train.csv\")\nte = pd.read_csv(\"\/kaggle\/input\/tabular-playground-series-may-2021\/test.csv\")\ntr_te = pd.concat([tr, te], axis=0).reset_index(drop=True)","85b5cf76":"enc = OneHotEncoder(sparse=False, handle_unknown='ignore')\ntr_te_ohe = pd.DataFrame(enc.fit_transform(tr_te.drop(['id','target'], axis=1).copy())).astype(int)\nsel = VarianceThreshold(threshold=0.01) # remove sparse features\ntr_te_ohe = pd.DataFrame(sel.fit_transform(tr_te_ohe))\ntr_te_ohe.columns = ['ohe_' + str(col) for col in tr_te_ohe.columns]\ntr_te_ohe = pd.concat([tr_te[['id','target']],tr_te_ohe], axis=1)","edaea403":"tr_class_1, te_class_1 = target_encoder(class_='Class_1', X_train=tr, X_test=te)\ntr_class_2, te_class_2 = target_encoder(class_='Class_2', X_train=tr, X_test=te)\ntr_class_3, te_class_3 = target_encoder(class_='Class_3', X_train=tr, X_test=te)\ntr_class_4, te_class_4 = target_encoder(class_='Class_4', X_train=tr, X_test=te)\ntr_tgt = pd.concat([tr_class_1,tr_class_2,tr_class_3,tr_class_4], axis=1)\nte_tgt = pd.concat([te_class_1,te_class_2,te_class_3,te_class_4], axis=1)\ntr_te_tgt = pd.concat([tr_tgt, te_tgt], axis=0).reset_index(drop=True)","209f6d49":"tr_te_fin = pd.concat([tr_te_ohe, tr_te_tgt], axis=1)\ntr_fin = tr_te_fin[tr_te_fin['target'].notnull()].drop(['id'], axis=1).copy()\nte_fin = tr_te_fin[tr_te_fin['target'].isnull()].drop(['id','target'], axis=1).copy()","34995392":"del tr_class_1, te_class_1, tr_class_2, te_class_2, tr_class_3, te_class_3, tr_class_4, te_class_4, tr_te_ohe, tr_tgt, te_tgt, tr_te_tgt, tr_te_fin\ngc.collect()","467bad0a":"# H2O ML MODEL ======================================================================================================================\n# preproc ===========================\nimport h2o\nfrom h2o.estimators.glm import H2OGeneralizedLinearEstimator\n\n# init ==============================\nh2o.init(max_mem_size='8G')\n\n# import data =======================\ntrain = h2o.H2OFrame(tr_fin)\ntrain[\"target\"] = train[\"target\"].asfactor()\ntest = h2o.H2OFrame(te_fin)\n\ny = \"target\"\nx = test.columns\n\n# fit model =========================\nglm_model = H2OGeneralizedLinearEstimator(\n    family=\"multinomial\", \n    solver='AUTO', \n    alpha=0.5,\n    #lambda_=0.6,\n    link='Family_Default',\n    intercept=True,\n    lambda_search=True, \n    nlambdas=100,\n    max_iterations = 1000,\n    #missing_values_handling='MeanImputation',\n    standardize=True,\n    nfolds = 5, \n    seed = 1333\n)\nglm_model.train(x=x, y=y, training_frame=train)","4b78be04":"# Eval mod ==========================\nglm_model.model_performance(xval=True)","fead624b":"# Model pred ========================\npreds = glm_model.predict(test).as_data_frame()","d1c4441a":"h2o.cluster().shutdown()","554c1eaf":"subm = pd.read_csv(\"\/kaggle\/input\/tabular-playground-series-may-2021\/sample_submission.csv\")\nsubm = pd.concat([subm['id'], preds[['Class_1','Class_2','Class_3','Class_4']]],axis=1)\nsubm.to_csv(\"submission.csv\", index=False)","1e0e2ac8":"#### Functions","6a32097e":"#### Format out and cleanup","238f7d1e":"#### Model","4305aa04":"Hopefully you liked it (please upvote) - and saw something new today. Let me know if you have any questions in the comments :)","44379ca8":"#### Target Encoding","2560c614":"#### Read data","fbeb3a93":"#### One Hot Encoding","ba64959f":"#### Imports ","39fe0f6c":"#### Submission","b04f108f":"##### Hey Dear Kagglers,\n##### Wanted to put my 2 cents in;\n##### While I see most of the public kernels on this competition focusing on parameter tunning\/automl\/cutting edge ML - I decided to show a slightly different approach -  and also a gentle reminder that the above is NOT a substitution for the good old Feature Engineering in any way.\n##### So - what's happening here:\n* Some FE:\n     - One Hot Encoding + subsequent feature selection (see below)\n     - Target Encoding (generally very useful in case the test set is similar enough to your train (confirmed it is\/adversarial validation))\n* SUPER SIMPLE Generalized Linear Model (Multinomial)  \n\n##### No fancy stuff here ... still performing better than most of the fancy stuff out there...\n##### Stay tuned - more is to come (fancy stuff as well) ...\n##### ... and Happy Kaggling :)"}}