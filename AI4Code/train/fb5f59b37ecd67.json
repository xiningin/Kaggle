{"cell_type":{"dbd886d3":"code","9093aee6":"code","5221a10d":"code","8af7591f":"code","3ee0dd4e":"code","119a1a69":"code","f09a9102":"code","5fdbfd22":"code","dcf52c75":"code","0e7eed4c":"code","ad992075":"code","bb455ccc":"code","ecccb9ed":"code","a1c8889a":"code","47b2e485":"code","2ce662a4":"code","64ae31ad":"code","ca12c3b2":"code","9f43dbdf":"code","bcb86a7b":"code","ea1ab4eb":"code","29bb0beb":"code","ebe3ebbb":"code","77ad4572":"code","a998b556":"code","31c9150e":"code","2fc890df":"code","e0a6e1a8":"code","8b24b220":"code","a9c7b981":"code","dbe892c8":"code","7baf62b5":"code","c0ba19e7":"code","1aaf7ccf":"code","034f4a0c":"markdown","7f2c7e2e":"markdown","e73fe56c":"markdown","5afebf30":"markdown","29d7458a":"markdown","d2d7c8ea":"markdown","ce494426":"markdown","6019199e":"markdown","c8e9af49":"markdown","13cc85dc":"markdown","2e76af7f":"markdown","ef94c33f":"markdown","e7e7a6a2":"markdown","6aa72251":"markdown","9b2d5479":"markdown","dd91b5b0":"markdown","6300a60d":"markdown","11e99201":"markdown","31149045":"markdown","62556cc1":"markdown","a6491239":"markdown","029e0ff2":"markdown","8a61c65c":"markdown","cd07605a":"markdown","1f34de96":"markdown"},"source":{"dbd886d3":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt","9093aee6":"raw_data = pd.read_csv('breast_cancer.csv')","5221a10d":"raw_data.info()","8af7591f":"raw_data.drop(columns = ['id', 'Unnamed: 32'], axis = 1, inplace = True)","3ee0dd4e":"raw_data.replace({'B': 0, 'M': 1}, inplace = True)","119a1a69":"# Fonction affichant une matrice de corr\u00e9lation sur un set de donn\u00e9es.\ndef corr_matrix(data, l = 25):\n    corr = data.corr()\n    plt.figure(figsize=(l,l))\n    sns.heatmap(corr, cmap='coolwarm', linecolor='white', annot=True)\n    plt.show()  \n    \ncorr_matrix(raw_data)","f09a9102":"# Pour un jeu de donn\u00e9es, une target et un seuil, on retourne un dataframe ne contenant que les features corr\u00e9l\u00e9es \u00e0 plus de x% avec la target choisie.\ndef features_selection(raw_data, target, x):\n    corr = raw_data.corr()\n    names = corr[(corr[target] > x)].index\n    filtered_data = pd.DataFrame()\n    for i in names:\n        filtered_data = pd.concat([filtered_data, raw_data[i]], axis = 1)\n    return filtered_data","5fdbfd22":"# Application de notre fonction avec un seuil de 0.6 \nfiltered_data = features_selection(raw_data, 'diagnosis', 0.75)","dcf52c75":"corr_matrix(filtered_data, l = 8) ","0e7eed4c":"# filtered_data.drop(columns = ['radius_mean', 'area_mean', 'radius_worst', 'area_worst'], inplace = True, axis = 1)\nfiltered_data.drop(columns = ['radius_worst', 'concave points_mean'], inplace = True, axis = 1)","ad992075":"corr_matrix(filtered_data, l = 6) ","bb455ccc":"sns.countplot(filtered_data['diagnosis'], label = 'Count')\nB, M = filtered_data['diagnosis'].value_counts()\nprint('Number of Benign: ',B)\nprint('Number of Malignant : ',M)","ecccb9ed":"%matplotlib inline\nimport matplotlib.pyplot as plt\nfiltered_data.drop('diagnosis', axis = 1).hist(bins = 50 , figsize = (15,5))\nplt.show()","a1c8889a":"sns.pairplot(filtered_data, hue = 'diagnosis')","47b2e485":"# S\u00e9paration des sets, premi\u00e8re op\u00e9ration \u00e0 faire sur les donn\u00e9es\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(filtered_data.drop('diagnosis', axis = 1), filtered_data['diagnosis'], test_size = 0.2, random_state = 10)","2ce662a4":"# Importation des diff\u00e9rentes m\u00e9thodes de normalisation\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import MinMaxScaler \nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.preprocessing import Normalizer","64ae31ad":"# Fonction permettant de retourner dans un dictionnaire les donn\u00e9es normalis\u00e9es suivant diff\u00e9rentes m\u00e9thodes de normalisation.\n# La cl\u00e9 repr\u00e9sente le nom de la m\u00e9thode de normalisation. Sa valeur les donn\u00e9es normalis\u00e9es par elles.\ndef scale_data(data):\n    sc_data = {'Raw' : data}\n    noms = ['StandardScaler', 'MinMaxScaler', 'RobustScaler', 'Normalizer']\n    scalers = [StandardScaler(), MinMaxScaler(), RobustScaler(), Normalizer()]\n    sc_data[noms[0]] = data\n    for i in range(0, len(scalers)):\n        sc = scalers[i]\n        sc_data[noms[i]] = sc.fit_transform(data)\n    return sc_data","ca12c3b2":"sc_data = scale_data(X_train)","9f43dbdf":"# Cross-validation\nfrom sklearn.model_selection import cross_val_predict\n\n# Mod\u00e8les de classification\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.svm import LinearSVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import RandomForestClassifier\n\n# M\u00e9triques\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score","bcb86a7b":"def train_validation(X_train, y_train):\n    result_table = pd.DataFrame(np.empty((0, 6)))\n    result_table.columns = ['Normalization','Model', 'Precision', 'Recall', 'F1', 'ROC']\n    for k, v in X_train.items():\n        # S\u00e9paration des donn\u00e9es en un jeu de train et de test\n        # X_train, X_test, y_train, y_test = train_test_split(v, y, test_size = 0.2, random_state = 10)\n\n        noms = ['SGDClassifier', 'LinearSVC', 'KNeighborsClassifier', 'RandomForestClassifier']\n        classifier = [SGDClassifier(random_state = 10), LinearSVC(), KNeighborsClassifier(), RandomForestClassifier()]\n\n        for i in range(0, len(classifier)):\n            cl = classifier[i]\n            # cl.fit(X_train, y_train)\n            # On effectue une cross validation. \n            y_pred = cross_val_predict(cl, v, y_train, cv = 5)\n            precision = round(precision_score(y_train, y_pred), 2)\n            recall = round(recall_score(y_train, y_pred), 2)\n            f = round(f1_score(y_train, y_pred), 2)\n            roc = round(roc_auc_score(y_train, y_pred), 2)\n            df_new_line = pd.DataFrame([[k, noms[i], precision, recall, f, roc]], columns=['Normalization','Model','Precision', 'Recall', 'F1', 'ROC'] )\n            result_table = pd.concat([result_table, df_new_line], ignore_index=True)\n    return result_table","ea1ab4eb":"results = train_validation(sc_data, y_train)","29bb0beb":"results.sort_values(by = ['Recall','Precision','F1'], ascending = False)","ebe3ebbb":"# sc_data_MinMax = sc_data['MinMaxScaler']\nsc_data_MinMax = sc_data['StandardScaler']","77ad4572":"from sklearn.model_selection import GridSearchCV","a998b556":"# Diff\u00e9rents hyperparam\u00e8tres de l'algorithme sont modul\u00e9s, de fa\u00e7on \u00e0 avoir in fine, les meilleurs param\u00e8tres.\nparam_grid = [\n    {'loss' : ['hinge', 'squared_hinge'],\n     'C' : range(1,100)\n    }\n    ]\n\ncl = LinearSVC(max_iter = 1000000)\n\ngrid_search_linearSVC = GridSearchCV(cl, param_grid, cv = 5, scoring = 'recall', return_train_score = True)\n\ngrid_search_linearSVC.fit(sc_data_MinMax, y_train)\nprint(grid_search_linearSVC.best_estimator_)\nprint(grid_search_linearSVC.best_score_)","31c9150e":"# Diff\u00e9rents hyperparam\u00e8tres de l'algorithme sont modul\u00e9s, de fa\u00e7on \u00e0 avoir in fine, les meilleurs param\u00e8tres.\nparam_grid = [\n    {'n_neighbors' : range(1,25),\n     'algorithm' : ['ball_tree', 'kd_tree', 'brute'],\n     'metric' : ['euclidean', 'manhattan']\n    }\n    ]\n\ncl = KNeighborsClassifier()\n\ngrid_search_knn = GridSearchCV(cl, param_grid, cv = 5, scoring = 'recall', return_train_score = True)\n\ngrid_search_knn.fit(sc_data_MinMax, y_train)\nprint(grid_search_knn.best_estimator_)\nprint(grid_search_knn.best_score_)","2fc890df":"final_model = grid_search_linearSVC.best_estimator_","e0a6e1a8":"sc = MinMaxScaler()\ntest_scale_data = sc.fit_transform(X_test)","8b24b220":"final_predictions = final_model.predict(test_scale_data)","a9c7b981":"precision = round(precision_score(y_test, final_predictions), 2)\nrecall = round(recall_score(y_test, final_predictions), 2)\nf = round(f1_score(y_test, final_predictions), 2)\nroc = round(roc_auc_score(y_test, final_predictions), 2)\n\nprint('-----------------------------------')\nprint('Final model results')\nprint('-----------------------------------')\nprint('Accuracy : ', precision * 100, ' %')\nprint('Recall : ', recall * 100, ' %')\nprint('F1 : ', f * 100, ' %')\nprint('Auc ROC : ', roc)\nprint('-----------------------------------')","dbe892c8":"from sklearn.metrics import confusion_matrix\ncm = confusion_matrix(y_test, final_predictions)\nsns.heatmap(cm, annot = True, cmap = 'Blues_r')","7baf62b5":"import pickle","c0ba19e7":"# Sauvegarde du mod\u00e8le\nfilename = 'final_model.sav'\npickle.dump(final_model, open(filename, 'wb'))","1aaf7ccf":"# Sauvegarde du scaler\nfilename = 'sc_std.sav'\npickle.dump(sc, open(filename, 'wb'))","034f4a0c":"#### 3.7 Comparaison de diff\u00e9rents mod\u00e8les","7f2c7e2e":"#### 4.2 KNeighborsClassifier","e73fe56c":"#### 3.3 Encodage de la target\nLa target \"diagnosis\" est encod\u00e9 de mani\u00e8re \u00e0 avoir 0 pour la valeur M et 1 pour la valeur B.","5afebf30":"#### 4.1 LinearSVC","29d7458a":"### 5. Test du mod\u00e8le final ","d2d7c8ea":"### 4. Utilisation du GridSearchCV","ce494426":"D'apr\u00e8s l'ensemble de nos essais, nous pouvons conlure que la m\u00e9thode de standardisation des donn\u00e9es importe peu, sauf dans le cas du Normalizer() qui offre des r\u00e9sultats bien moindre que les autres m\u00e9thodes. Pour ce qui est des mod\u00e8les, le LinearSVC() et le KNClassifier() semblent \u00e9quivalent. En revanche, le SGDClassifier offre de moins bons r\u00e9sultats.\n\nPar cons\u00e9quent, une \u00e9tude approfondie sur le mod\u00e8le LinearSVC() et KNeighborsClassifier() va \u00eatre r\u00e9alis\u00e9es par la suite dans le but de s\u00e9lectionner les meilleurs hyperparam\u00e8tres du meilleur des deux mod\u00e8le pour la r\u00e9solution de notre probl\u00e9matique. ","6019199e":"<div style=\"text-align: center;\">\n\n<h5 style=\"text-align: center; width:100%\">D\u00e9veloppement d'un mod\u00e8le de machine learning pour d\u00e9tecter les tumeurs canc\u00e9reuses dans le cadre du cancer du sein.<\/h5>\n\n<\/div>","c8e9af49":"#### 5.1 R\u00e9sultats sur les m\u00e9triques ","13cc85dc":"D'apr\u00e8s la m\u00e9thode .info(), seule la colonne 32 compte des NaN. Les autres colonnes comportent chacune 569 valeurs non nulles. ","2e76af7f":"### 3. Traitement des donn\u00e9es\nAnalyse et exploration des donn\u00e9es.\n* Traitement des NaN\n* Encodage de la target\n* S\u00e9lection des features\n#### 3.1 Affichage des informations","ef94c33f":"Le mod\u00e8le LinearSVC offre le meilleur recall. \nPour cette raison, il sera s\u00e9lectionn\u00e9 comme algorithme pour notre mod\u00e8le final.","e7e7a6a2":"#### 3.2 Suppression de la colonne vide 32\nLa colonne 32 \"Unname: 32\" est une colonne totalement vide. Celle-ci est supprim\u00e9e.","6aa72251":"Apr\u00e8s une application de la fonction features_selection avec un seuil de 0.6, on conserve 10 features.","9b2d5479":"Apr\u00e8s exploration des donn\u00e9es, on conserve 6 features pour poursuivre l'\u00e9tude.","dd91b5b0":"Il y a un nombre raisonnable de cas malins par rapport au nombre de cas b\u00e9nins. Autrement, si par exemple le nombre de cas malins \u00e9tait tr\u00e8s faible, notre futur algorithme aurait toujours pr\u00e9dit un r\u00e9sultat b\u00e9nin, ayant consid\u00e9r\u00e9 que la probabilit\u00e9 d'un cas malin \u00e9tait tr\u00e8s faible.","6300a60d":"Dans la mesure o\u00f9 nous avons r\u00e9duit le nombre de dimension \u00e0 deux features, l'int\u00e9r\u00eat d'une PCA est inexistant. C'est pour cette raison qu'elle n'a pas \u00e9t\u00e9 test\u00e9 dans le cadre du projet. ","11e99201":"#### 3.5 Features corr\u00e9l\u00e9es entre elles\n\nDans un second temps, on retire les features corr\u00e9l\u00e9es entre elles.","31149045":"### 6. Sauvegarde du mod\u00e8le et du scaler ","62556cc1":"#### 3.6 Normalisation des donn\u00e9es \nPlusieurs m\u00e9thodes de normalisation vont \u00eatre utilis\u00e9es dans le but de les comparer et de garder la plus optimale. ","a6491239":"#### 5.2 R\u00e9sultats de la matrice de confusion","029e0ff2":"#### 3.4 Matrice de corr\u00e9lation\nL'objectif est d'avoir une vue g\u00e9n\u00e9rale sur les donn\u00e9es, de savoir qu'elles sont les features corr\u00e9l\u00e9es \u00e0 la target et qu'elles sont les features corr\u00e9l\u00e9es entre elles. Et ensuite, de s\u00e9lectionner les features les plus int\u00e9ressantes en cons\u00e9quence.","8a61c65c":"### 1. Importation des librairies g\u00e9n\u00e9rales  ","cd07605a":"### 2. Chargement des donn\u00e9es","1f34de96":"<div style=\"text-align: center;\">\n<!-- <img src=\"jupiter.jpg\" style=\"width:150px; border-radius:100px\" \/> <br \/> -->\n<h1 style=\"text-align: center; width:100%\">Projet Analyse de Donn\u00e9es<\/h1>\n    Equipe : <a href=\"mailto:Othmene.BENAZIEB@ecam-strasbourg.eu\">Othm\u00e8ne Benazieb<\/a>, <a href=\"mailto:Bounphathay.CHANTHASAY@ecam-strasbourg.eu\">Bounphathay Chanthasay<\/a>, <a href=\"mailto:lefoulervincent@gmail.com\">Vincent Le Fouler<\/a> <br \/>\n\n    Formateur : <a href=\"mailto:manuel.simoes@cpc-analytics.fr\">Manuel Simoes<\/a>\n\n<\/div>\n\n\n\n\n\n\n<style>\ndiv.warn {    \n    background-color: #fcf2f2;\n    border-color: #dFb5b4;\n    border-left: 5px solid #dfb5b4;\n    padding: 0.5em;\n    }\n<\/style>"}}