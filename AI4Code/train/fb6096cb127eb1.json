{"cell_type":{"50cfebe5":"code","e26932ef":"code","43af3d4a":"code","fe0f0e8f":"code","f3cccac4":"code","0db25e6f":"code","b36bcfe6":"code","74f1a31d":"code","2433d295":"code","9f0a8567":"code","99000650":"code","18c5aac9":"code","4335e264":"code","7e7cc42a":"code","99c6d861":"code","4991c154":"code","800ae267":"code","e517379b":"code","baaa60ff":"code","d726a516":"code","ddffa229":"code","d9039d66":"code","a46f6ad7":"code","f7872172":"code","687ecbea":"code","05411284":"code","593528d9":"code","e855c115":"code","f4e76480":"code","b8b09fb1":"code","83620aec":"code","051fb3f3":"code","9ba24008":"code","949cfee0":"markdown","69b702f6":"markdown","4ffe4d8b":"markdown","aa122583":"markdown","c258893e":"markdown","c5a57c07":"markdown","ce72f888":"markdown","7e5b11ba":"markdown","dc099c97":"markdown","f985461c":"markdown","dc2afe2b":"markdown","4ff2ece8":"markdown"},"source":{"50cfebe5":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split, RandomizedSearchCV, cross_val_score\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder\nfrom sklearn.metrics import roc_auc_score\nimport pandas as pd\nimport bqplot\nimport matplotlib.pyplot as plt\nimport time\nimport numpy as np\nfrom tqdm.notebook import tqdm, trange\nimport seaborn as sns\nimport optuna\nimport ipywidgets as widgets\nsns.set({'figure.figsize': (12, 8)})\nsns.set_color_codes(\"pastel\")\nrng = np.random.default_rng()","e26932ef":"from IPython.display import display\nC = widgets.IntSlider(min=10, max=10000, step=10, value=100)\nl1_ratio = widgets.IntSlider(min=10, max=10000, step=10, value=100)","43af3d4a":"def mul(a, b):\n    print(f\"search space has {a*b} parameters\")","fe0f0e8f":"?LogisticRegression","f3cccac4":"widgets.interact(mul, a = C, b = l1_ratio);","0db25e6f":"#create a gene_pool millions of possible combinations with just two variables!\nparams = dict()\nparams[\"penalty\"] = [\"elasticnet\"]\nparams[\"C\"] = np.linspace(0.001, 100, C.value)\nparams[\"l1_ratio\"] = np.linspace(0.0001, 1, l1_ratio.value)\nparams[\"solver\"] = ['saga']\nparams[\"random_state\"] = [42]","b36bcfe6":"params['C']","74f1a31d":"#generates a dictionary from the pool of genes\ndef generate_parent(gene_pool):\n    parent = dict()\n    for gene in gene_pool.keys():\n        parent[gene] = rng.choice(gene_pool[gene])\n    fitness = get_fitness(estimator(**parent), fitness_function)\n    return Individual(parent, fitness)","2433d295":"def mutate(parent, gene_pool):\n    gene = rng.choice(list(params))\n    child = parent.copy()\n    new_gene, alternate = rng.choice(gene_pool[gene], 2)\n    #help make sure the gene get's mutated\n    child[gene] = alternate if new_gene == child[gene] else new_gene\n    return child","9f0a8567":"class Individual:\n    def __init__(self, genes, fitness):\n        self._genes = genes\n        self._fitness = fitness\n        self._fp = 0 #fitness_proportion to be used when selection == fp\n        \n    def __eq__(self, other):\n        return self.genes == other.genes\n\n    def __lt__(self, other):\n        return self.fitness < other.fitness\n    \n    def __gt__(self, other):\n        return self.fitness > other.fitness\n    \n    def __str__(self):\n        return f\"Individual with genes: {self._genes} and fitness:{self._fitness}\"\n        \n    @property    \n    def genes(self):\n        return self._genes\n    \n    @property\n    def fitness(self):\n        return self._fitness\n    \n    #may add cv = cv\/skf as an option\n    def set_fitness(self):\n        self._fitness = get_fitness(estimator(**self.genes), fitness_function)\n    \n    def set_gene(self, gene, value):\n        self._genes[gene] = value\n        \n    def get_gene_from_window(self, gene):\n        min_c = gene_pool[gene].min()\n        max_c = gene_pool[gene].max()\n        dist_1 = self._genes[gene] - min_c\n        dist_2 = max_c - self._genes[gene]\n        dist = min(dist_1, dist_2)*gnp_window\n        lb = self._genes[gene] - dist\n        ub = self._genes[gene] + dist\n        new_gene, alternate = rng.choice(gene_pool[gene][(gene_pool[gene] >= lb) & (gene_pool[gene] <= ub)], 2)\n        return new_gene, alternate\n        \n    \n    def mutate(self):\n        gene = rng.choice(list(self._genes))\n        if restrict_gnp and isinstance(gene_pool[gene], float):\n            #give chance of diversity = 1-p_mutate until 10% chance\n            if rng.random() < p_outlier:\n                print(f\"got an outlier\")\n                new_gene, alternate = rng.choice(gene_pool[gene], 2)\n            else:\n                new_gene, alternate = self.get_gene_from_window(gene)\n        else:\n            new_gene, alternate = rng.choice(gene_pool[gene], 2)\n    #help make sure the gene get's mutated\n        self._genes[gene] = alternate if new_gene == self._genes[gene] else new_gene\n        return\n    \n    def get_estimator(self):\n        return estimator(**self._genes)","99000650":"#creates a population of size size with parameters from gene_pool\ndef create_population(gene_pool, size = 10):\n    population = []\n    for i in range(size):\n        population.append(generate_parent(gene_pool))\n    population.sort(reverse = True)\n    return population","18c5aac9":"class Population:\n    #create initial population\n    def __init__(self, gene_pool, size = 10):\n        self._population = create_population(gene_pool, size)\n        self._size = size\n      \n    #note that if several individuals have == best fitness anyone of them is returned in the sorted list\n    @property\n    def best_individual(self):\n        return self._population[0]\n    \n    @property\n    def best_fitness(self):\n        return self._population[0].fitness\n    \n    @property\n    def population(self):\n        return self._population\n    \n    @property\n    def size(self):\n        return self._size\n    \n    def replace_generation(self, new_gen):\n        new_gen.sort(reverse = True)\n        self._population = new_gen","4335e264":"def get_fitness(individual, fitness_function, cv = 3):\n    score = cross_val_score(individual, X_train, y_train, cv=cv, scoring = fitness_function)\n    return score.mean()","7e7cc42a":"def select_breeding(population, selection = 'truncation', frac = 0.75):\n    if selection == 'truncation':\n        cut = int(len(population.population)*frac)\n        breeding = population.population[:cut]\n        return breeding\n    elif selection == 'fitness_proportionate' or selection == 'fp':\n        size = int(population.size * frac)\n        return fp_selection(population, size)\n    elif selection == 'tournament':\n        size = int(population.size * frac)\n        return tournament_selection(population, size)\n    elif selection == 'sus':\n        size = int(population.size * frac)\n        return sus_selection(population, size)","99c6d861":"def fp_selection(pop, size):\n    p = np.array([ind.fitness for ind in pop.population])\n    total_fitness = p.sum()\n    p = p \/ total_fitness\n    #p = np.cumsum(p) nice alternative solution\n    return rng.choice(pop.population, size = size, p = p).tolist()","4991c154":"#stochastic universal sampling\ndef sus_selection(pop, size):\n    p = np.array([ind.fitness for ind in pop.population]).cumsum()\n    total_fitness = np.array([ind.fitness for ind in pop.population]).sum()\n    step = total_fitness \/ size\n    start = rng.uniform(0, step)\n    steps = [(start + i*step) for i in range(size)]\n    i = 0\n    breeding = []\n    for s in steps:\n        while p[i] < s:\n            i = i + 1\n            breeding.append(pop.population[i])\n    return breeding","800ae267":"#add requirement size and elitism have to be even!\n#also elitism is almost unnecessary if tournament, almost!\ndef tournament_selection(pop, size):\n    participants = [ind for ind in pop.population]\n    breeding = []\n    #could implement different rounds here\n    #but I think that's almost the same as calling tournament different times with smaller sizes\n    for i in range(size):\n        a, b = rng.choice(participants, 2)\n        if a > b:\n            breeding.append(a)\n            participants.remove(a)\n        else:\n            breeding.append(b)\n            participants.remove(b)\n    return breeding\n        \n    \n#reverse tournament, eliminates need for elitism\n#could use with parallelism\ndef rev_tournament_selection(pop, size):\n    breeding = [ind for ind in pop.population]\n    num_eliminated = len(breeding) - size\n    for i in range(num_eliminated):\n        a, b = rng.choice(participants, 2)\n        if a > b:\n            breeding.remove(b)\n        else:\n            breeding.remove(a)\n    return breeding\n        ","e517379b":"def breed(parent_1, parent_2, p_cross, p_mutate):\n    # check for recombination\n    # if crossover happens at probability p then not crossover would happen at probability 1-p\n    #rand() will draw a number larger than p_cross 1-p times\n    #and a number < p_cross p times\n    # children are copies of parents by default\n    child_1, child_2 = Individual(parent_1.genes, parent_1.fitness), Individual(parent_2.genes, parent_2.fitness)\n    if np.random.rand() < p_cross: \n        \n        genes = list(child_1.genes)\n        child_1, child_2 = crossover(parent_1, parent_2, child_1, child_2)\n        #mutate if p\n    if np.random.rand() < p_mutate:\n        child_1.mutate()\n    if np.random.rand() < p_mutate:\n        child_2.mutate()\n        \n        child_1.set_fitness()\n        child_2.set_fitness()\n    return child_1, child_2","baaa60ff":"# crossover two parents to create two children\n# should not be called by itself because it doesn't set fitness \ndef crossover(parent_1, parent_2, child_1, child_2):\n    # children are copies of parents by default\n    genes = list(child_1.genes) #make global to make more efficient!\n    # select crossover point that is not on the end of the string\n    start = rng.choice(range(len(genes) - 1))\n    #no crossover happening\n    if start == len(genes) -1:\n        return [child_1, child_2]\n    cut = rng.choice(range(start, len(genes)))\n    #no crossover happening\n    if cut == start:\n        return [child_1, child_2]\n    # perform crossover\n    for gene in genes[start:cut]:\n        if isinstance(gene_pool[gene], float): #introduce more diversity by modified crossover for continous values\n            #could also solve this with algebra, but I like using the predefined gene_pool\n            lower = parent_1[gene]\n            higher = parnt_2[gene]\n            if parent_1[gene] > parent_2[gene]:\n                lower = parent_2[gene]\n                higher = parent_1[gene]\n                \n            new_gene_1, new_gene_2, = rng.choice(gene_pool[gene][(gene_pool[gene] >= lower) & (gene_pool[gene] <= higher)], 2)\n            child_1.set_gene(gene, new_gene_1)\n            child_2.set_gene(gene, new_gene_2)\n        else:\n            child_1.set_gene(gene, parent_2.genes[gene])\n            child_2.set_gene(gene, parent_1.genes[gene])\n        \n    return child_1, child_2","d726a516":"train = pd.read_csv(\"..\/input\/titanic\/train.csv\")\ntest = pd.read_csv(\"..\/input\/titanic\/test.csv\")\ngs = pd.read_csv(\"..\/input\/titanic\/gender_submission.csv\")\nX, y = train.drop([\"Survived\", \"PassengerId\", \"Name\", \"Cabin\", \"Ticket\"], axis=1).copy(), train[\"Survived\"].copy()\nenc = LabelEncoder()\nX.Embarked = enc.fit_transform(X.Embarked)\nX.Sex = enc.fit_transform(X.Sex)\nX_train, X_test, y_train, y_test = train_test_split(X, y)\nscaler = StandardScaler()\nX_train[X.columns] = scaler.fit_transform(X_train)\nX_test[X.columns] = scaler.transform(X_test)\nX_train.fillna(0, inplace = True)\nX_test.fillna(0, inplace = True)","ddffa229":"def simulate(params,\n             scorer,\n             iterations,\n             model,\n             train_set,\n             train_labels,\n             selection = 'fp',\n             p_cross = 1.0,\n             cv = 3,\n             p_mutate = 1.0,\n             sim_ann = True, \n             restrict_gene_pool = True, #narrow genes i.e. finetune\n             gene_pool_window = 1.0, #initial size of window\n             decay = None,\n             elitism = 2):\n    #add some fixed genes\n    global X_train, y_train, estimator, fitness_function, gene_pool, restrict_gnp, gnp_window, rng\n    global p_outlier\n    generations = np.arange(0, iterations)\n    fitness_prog = []\n    p_outlier = 1 - p_mutate\n    rng = np.random.default_rng()\n    X_train, y_train = train_set, train_labels\n    fitness_function = scorer\n    estimator = model\n    gene_pool = params\n    restrict_gnp = restrict_gene_pool\n    gnp_window = gene_pool_window\n    population = Population(gene_pool)\n    best_fitness = population.best_fitness\n    if decay == None:\n        decay = 1 \/ iterations\n    print(f\"best initial fitness: {population.best_fitness}\")\n    for i in trange(iterations):\n        fitness_prog.append(best_fitness)\n        new_gen = []\n        breeding = select_breeding(population, selection)\n        for elite in range(elitism):\n            new_gen.append(population.population[elite])\n       \n        #elitism to be implemented here\n        while(len(new_gen) < population.size): #let populatin size oscillate +1 -1?\n            parent_1, parent_2 = rng.choice(breeding, 2) #possibility of selecting the same individual\n            child_1, child_2 = breed(parent_1, parent_2, p_cross, p_mutate)\n            new_gen.append(child_1)\n            new_gen.append(child_2)\n        #replace the previous generation\n        population.replace_generation(new_gen)\n        #are you better than the last?\n        if (best_fitness < population.best_fitness):\n            diff = population.best_fitness - best_fitness\n            best_fitness = population.best_fitness\n            display(f\"child {population.best_individual} with fitness {population.best_fitness}, which is {diff} better than before\")\n        if sim_ann:\n            p_cross = p_cross - p_cross*decay\n            p_mutate = p_mutate - p_mutate*decay\n            if p_outlier > 0.1:\n                p_outlier = 1 - p_mutate\n    #note if several individuals have same fitness anyone of them is returned\n    return population.best_individual, (generations, fitness_prog)","d9039d66":"reg = LogisticRegression()\nreg.fit(X_train, y_train)\nprint(f\"baseline score: {roc_auc_score(y_test, reg.predict(X_test))}\")","a46f6ad7":"start = time.time()\nbest, history= simulate(params, 'roc_auc', 1000, LogisticRegression, X_train, y_train, selection = 'tournament')\nend = time.time()\nga_time = end - start","f7872172":"print(best)","687ecbea":"sns.regplot(x = history[0], y = history[1])\nplt.xlabel('generation')\nplt.ylabel('fitness')\nplt.show()","05411284":"start = time.time()\nran_search = RandomizedSearchCV(LogisticRegression(), params, scoring = 'roc_auc', random_state = 42, n_iter =10000, cv = 3)\nran_search.fit(X_train, y_train)\nend = time.time()\nrandom_search_time = end - start","593528d9":"ran_search.best_score_","e855c115":"ran_search.best_estimator_","f4e76480":"optuna.logging.set_verbosity(0)","b8b09fb1":"# 1. Define an objective function to be maximized.\ndef objective(trial):\n    # 2. Suggest values for the hyperparameters using a trial object.\n    \n    #create a gene_pool millions of possible combinations with just two variables!\n    parameters = dict()\n    parameters[\"penalty\"] = \"elasticnet\"\n    parameters[\"C\"] = trial.suggest_float('C', 0, 100)\n    parameters[\"l1_ratio\"] = trial.suggest_uniform('l1_ratio', 0, 1)\n    parameters[\"solver\"] = 'saga'\n    reg = LogisticRegression(**parameters)\n    score = cross_val_score(reg, X_train, y_train, n_jobs=-1, cv=3, scoring = 'roc_auc')\n    roc = score.mean()\n    return roc\n\n# 3. Create a study object and optimize the objective function.\nstart = time.time()\nstudy = optuna.create_study(direction='maximize')\nstudy.optimize(objective, n_trials=100, gc_after_trial = True)\nend = time.time()\noptuna_time = end - start","83620aec":"print(study.best_trial.value)","051fb3f3":"times = np.array([ga_time, random_search_time, optuna_time])\nx = ['GA', 'Random', 'Optuna']","9ba24008":"sns.barplot(x =x , y = times)\nplt.show()","949cfee0":"# Genetic Algorithms for Model Selection","69b702f6":"###  *Two major options to improving performance ML Problems*\n\n\n<ol>\n<li>Feature Engineering<\/li>\n<li>Hyperparameter Tuning<\/li>\n<\/ol>\n\nGenetic Algorithms can be applied to both, feature selection in Feature Engineering and Hyperparameter Search in Model Selection, or fine tuning.\n\nHere, I will focus on Hyperparameter Tuning.\n\nThere are many existing solutions for this problem, sklearn has built in GridSearchCV, RandomizedSearchCV for this. There are countless companies and open source libraries like H2O, Optuna, Hyperopt and raytunes. \n\nAll using different algorithms. Brute Force, randomization, AI and bayesian optimization.\n\nThe former is not feasible for large spaces and even the latter two are quite slow.\n\nEven a simple Model like Logistic Regression with only two continous parameters can have a huge search space.","4ffe4d8b":"Alright, we have our population, we let them loose in the wild. And now they get to pass on their genes to the next generation!\n\nThe population automatically sorts it's individuals by fitness, depending on the selection process that is used in one way or another to produce the new generation.\n\n\nThere is recombination with more than two individuals and those are quite interesting implementations. \n\nFor my project, I use just two parents, which simply produce two children.\n\nChildren are first direct copies of their parents and then there is a possibility of crossover\/recombination and mutation to produce new individuals","aa122583":"This makes it very hard to pick the right model from the crowd","c258893e":"## Fitness\n\nHow do we know that one individual is better than another?\n\nWe use a fitness function. \nFor our models there a many different metrics that qualify depending on your problem.\nMSE, RMSE, MAE for regression, AUC, F1, Precision, Accuracy etc. for Classification.\n\nEach individual gets a score via cross validation on the fitness function.\nCross validation actually has a nice tangent to actuall biology.\n\nIf an individual doesn't hunt gazelles well, maybe it catches fish better. (I.e. get different chances on different subsets of the training data!) So the one who does best overall gets picked as the fittest.","c5a57c07":"So now that we have a gene pool defined (the parameter space), lets create some individuals by random choice.\n\nThese functions below do the basic and are here to just test it's working, they are defined in the Individual Class again","ce72f888":"\n\n![display image](https:\/\/thumbs.gfycat.com\/DazzlingPresentAlbacoretuna-size_restricted.gif)","7e5b11ba":"*I picked discrete values because I feel it's more in line with an actual gene_pool, i.e. we have a building block 20 amino acids which aren't continous.\nEverything could easily be changed to search with continous values with arithmetic.*","dc099c97":"### Hmmm, ok, you!","f985461c":"Alright, lets get ready have natural selection do it's thing.\n\n![display image](https:\/\/www.publicdomainpictures.net\/pictures\/10000\/nahled\/1-1276250040fO7C.jpg)","dc2afe2b":"Stochastic Universal Sampling is super weird, sometimes you can get so stuck that there will be zero improvements after the first iteration\n","4ff2ece8":"Hi all, \n\nthis is part of a Homework Project I did am doing for school.\nI am building a pypi package (pip install galearn) to do hyperparameter tuning compatible with the sklearn api.\nIt's a work in progress, but here it is what I've got so far for the titanic set (I'll upload one for Boston Housing as well).\nIt performs better than randomized search in most of my test cases and is fairly fast.\n\nI would be very grateful if you try it out for your own problems and share your experiences with me so I may improve it (lots of work still to be done). \nYou don't need to install the package but can also just copy paste any functions from here.\n\nIf anything is unclear or you have some questions please ask, any input is greatly appreciated.\n\ncheers!\n\nOli\n"}}