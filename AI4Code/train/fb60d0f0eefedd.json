{"cell_type":{"86bdb757":"code","4a4524b3":"code","100b6189":"code","3218ee80":"code","ebb081ce":"code","6d708215":"code","92d0f918":"code","9b8937c5":"code","cbbc62b8":"code","6e33c9c4":"code","601f7f36":"code","4a213f36":"code","9c8f182d":"code","5f0ae105":"code","d2d20153":"code","481ae409":"code","d656a5e6":"code","f1fd981f":"code","3dc06b2e":"code","995a2c70":"code","636628f9":"code","f5549233":"code","c8a89478":"code","8d143d43":"code","4f09494a":"code","85391bdf":"code","c0afa64a":"code","69e01a3c":"code","7a07a163":"code","9b291850":"code","7f0c4cb2":"code","ee7d1d38":"code","c45706b5":"code","4cb48b16":"code","2834b42a":"code","343be905":"code","3ba6bb26":"code","d3f12219":"markdown","52b9dcf7":"markdown","53639bef":"markdown","8fafede2":"markdown","cdf93402":"markdown","9a70689b":"markdown","3687c8d6":"markdown","b44d6cc7":"markdown","da3081cc":"markdown","15a6a135":"markdown","33cb147c":"markdown","8584f0ec":"markdown","7efc8fbe":"markdown","62ad56e1":"markdown","8222ca08":"markdown","b5fe74ee":"markdown","d455fe89":"markdown","a262c8ca":"markdown","5afd1c2d":"markdown","f487e0c7":"markdown","b2125498":"markdown","f9be59de":"markdown","9ef6bcc3":"markdown","2f49ecb7":"markdown","066c5f8d":"markdown","58fb315d":"markdown","a388e90d":"markdown","c012ec64":"markdown","a6cf0b36":"markdown","d7740c4b":"markdown","d19920c6":"markdown","9af46ec8":"markdown","2c9b8452":"markdown"},"source":{"86bdb757":"from fastai.text.all import *","4a4524b3":"path = Path('\/kaggle\/input\/')\npath.ls()","100b6189":"vax_tweets = pd.read_csv(path\/'pfizer-vaccine-tweets\/vaccination_tweets.csv')\nvax_tweets.head()","3218ee80":"tweets = pd.read_csv(path\/'complete-tweet-sentiment-extraction-data\/tweet_dataset.csv')\ntweets.head()","ebb081ce":"# Code via https:\/\/www.kaggle.com\/garyongguanjie\/comments-analysis\ndef de_emojify(inputString):\n    return inputString.encode('ascii', 'ignore').decode('ascii')\n\n# Code via https:\/\/www.kaggle.com\/pawanbhandarkar\/generate-smarter-word-clouds-with-log-likelihood\ndef tweet_proc(df, text_col='text'):\n    df['orig_text'] = df[text_col]\n    # Remove twitter handles\n    df[text_col] = df[text_col].apply(lambda x:re.sub('@[^\\s]+','',x))\n    # Remove URLs\n    df[text_col] = df[text_col].apply(lambda x:re.sub(r\"http\\S+\", \"\", x))\n    # Remove emojis\n    df[text_col] = df[text_col].apply(de_emojify)\n    # Remove hashtags\n    df[text_col] = df[text_col].apply(lambda x:re.sub(r'\\B#\\S+','',x))\n    return df[df[text_col]!='']\n\n# Clean the text data and combine the dfs\ntweets = tweets[['old_text', 'new_sentiment']].rename(columns={'old_text':'text', 'new_sentiment':'sentiment'})\nvax_tweets['sentiment'] = np.nan\ntweets = tweet_proc(tweets)\nvax_tweets = tweet_proc(vax_tweets)\ndf_lm = tweets[['text', 'sentiment']].append(vax_tweets[['text', 'sentiment']])\ndf_clas = df_lm.dropna(subset=['sentiment'])\nprint(len(df_lm), len(df_clas))","6d708215":"df_clas.head()","92d0f918":"dls_lm = TextDataLoaders.from_df(df_lm, text_col='text', is_lm=True, valid_pct=0.1)","9b8937c5":"dls_lm.show_batch(max_n=2)","cbbc62b8":"learn = language_model_learner(dls_lm, AWD_LSTM, drop_mult=0.3, metrics=[accuracy, Perplexity()]).to_fp16()","6e33c9c4":"learn.lr_find()","601f7f36":"learn.fit_one_cycle(1, 3e-2)","4a213f36":"learn.unfreeze()\nlearn.lr_find()","9c8f182d":"learn.fit_one_cycle(4, 1e-3)","5f0ae105":"# Text generation using the language model\nTEXT = \"I love\"\nN_WORDS = 30\nN_SENTENCES = 2\nprint(\"\\n\".join(learn.predict(TEXT, N_WORDS, temperature=0.75) for _ in range(N_SENTENCES)))","d2d20153":"learn.save_encoder('finetuned_lm')","481ae409":"dls_clas = DataBlock(\n    blocks = (TextBlock.from_df('text', seq_len=dls_lm.seq_len, vocab=dls_lm.vocab), CategoryBlock),\n    get_x=ColReader('text'),\n    get_y=ColReader('sentiment'),\n    splitter=RandomSplitter()\n).dataloaders(df_clas, bs=64)","d656a5e6":"dls_clas.show_batch(max_n=2)","f1fd981f":"learn = text_classifier_learner(dls_clas, AWD_LSTM, drop_mult=0.5, metrics=accuracy).to_fp16()","3dc06b2e":"learn = learn.load_encoder('finetuned_lm')","995a2c70":"learn.fit_one_cycle(1, 3e-2)","636628f9":"learn.freeze_to(-2)\nlearn.fit_one_cycle(1, slice(1e-2\/(2.6**4),1e-2))","f5549233":"learn.freeze_to(-3)\nlearn.fit_one_cycle(1, slice(5e-3\/(2.6**4),5e-3))","c8a89478":"learn.unfreeze()\nlearn.fit_one_cycle(3, slice(1e-3\/(2.6**4),1e-3))","8d143d43":"learn.save('classifier')","4f09494a":"learn.predict(\"I love\")","85391bdf":"learn.predict(\"I hate\")","c0afa64a":"pred_dl = dls_clas.test_dl(vax_tweets['text'])","69e01a3c":"preds = learn.get_preds(dl=pred_dl)","7a07a163":"# Get predicted sentiment\nvax_tweets['sentiment'] = preds[0].argmax(dim=-1)\nvax_tweets['sentiment'] = vax_tweets['sentiment'].map({0:'negative', 1:'neutral', 2:'positive'})\n\n# Save to csv\nvax_tweets.to_csv('vax_tweets_sentiment.csv')\n\n# Plot sentiment value counts\nvax_tweets['sentiment'].value_counts(normalize=True).plot.bar();","9b291850":"# Get counts of number of tweets by sentiment for each date\nvax_tweets['date'] = pd.to_datetime(vax_tweets['date']).dt.date\ntimeline = vax_tweets.groupby(['date', 'sentiment']).agg(**{'tweets': ('id', 'count')}).reset_index()\n\n# Plot results\nimport plotly.express as px\nfig = px.line(timeline, x='date', y='tweets', color='sentiment')\nfig.show()","7f0c4cb2":"!pip install wordninja\n!pip install pyspellchecker","ee7d1d38":"from wordcloud import WordCloud, ImageColorGenerator\nimport wordninja\nfrom spellchecker import SpellChecker\nfrom collections import Counter\nimport nltk\nnltk.download('wordnet')\nnltk.download('stopwords')\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.corpus import stopwords \nstop_words = set(stopwords.words('english'))  \nstop_words.add(\"amp\")","c45706b5":"# FUNCTIONS REQUIRED\n\ndef flatten_list(l):\n    return [x for y in l for x in y]\n\ndef is_acceptable(word: str):\n    return word not in stop_words and len(word) > 2\n\n# Color coding our wordclouds \ndef red_color_func(word, font_size, position, orientation, random_state=None,**kwargs):\n    return f\"hsl(0, 100%, {random.randint(25, 75)}%)\" \n\ndef green_color_func(word, font_size, position, orientation, random_state=None,**kwargs):\n    return f\"hsl({random.randint(90, 150)}, 100%, 30%)\" \n\ndef yellow_color_func(word, font_size, position, orientation, random_state=None,**kwargs):\n    return f\"hsl(42, 100%, {random.randint(25, 50)}%)\" \n\n# Reusable function to generate word clouds \ndef generate_word_clouds(neg_doc, neu_doc, pos_doc):\n    # Display the generated image:\n    fig, axes = plt.subplots(1,3, figsize=(20,10))\n    \n    wordcloud_neg = WordCloud(max_font_size=50, max_words=100, background_color=\"white\").generate(\" \".join(neg_doc))\n    axes[0].imshow(wordcloud_neg.recolor(color_func=red_color_func, random_state=3), interpolation='bilinear')\n    axes[0].set_title(\"Negative Words\")\n    axes[0].axis(\"off\")\n\n    wordcloud_neu = WordCloud(max_font_size=50, max_words=100, background_color=\"white\").generate(\" \".join(neu_doc))\n    axes[1].imshow(wordcloud_neu.recolor(color_func=yellow_color_func, random_state=3), interpolation='bilinear')\n    axes[1].set_title(\"Neutral Words\")\n    axes[1].axis(\"off\")\n\n    wordcloud_pos = WordCloud(max_font_size=50, max_words=100, background_color=\"white\").generate(\" \".join(pos_doc))\n    axes[2].imshow(wordcloud_pos.recolor(color_func=green_color_func, random_state=3), interpolation='bilinear')\n    axes[2].set_title(\"Positive Words\")\n    axes[2].axis(\"off\")\n\n    plt.tight_layout()\n    plt.show();\n\ndef get_top_percent_words(doc, percent):\n    # Returns a list of \"top-n\" most frequent words in a list \n    top_n = int(percent * len(set(doc)))\n    counter = Counter(doc).most_common(top_n)\n    top_n_words = [x[0] for x in counter]\n    \n    return top_n_words\n    \ndef clean_document(doc):\n    spell = SpellChecker()\n    lemmatizer = WordNetLemmatizer()\n    \n    # Lemmatize words (needed for calculating frequencies correctly )\n    doc = [lemmatizer.lemmatize(x) for x in doc]\n    \n    # Get the top 10% of all words. This may include \"misspelled\" words \n    top_n_words = get_top_percent_words(doc, 0.1)\n\n    # Get a list of misspelled words \n    misspelled = spell.unknown(doc)\n    \n    # Accept the correctly spelled words and top_n words \n    clean_words = [x for x in doc if x not in misspelled or x in top_n_words]\n    \n    # Try to split the misspelled words to generate good words (ex. \"lifeisstrange\" -> [\"life\", \"is\", \"strange\"])\n    words_to_split = [x for x in doc if x in misspelled and x not in top_n_words]\n    split_words = flatten_list([wordninja.split(x) for x in words_to_split])\n    \n    # Some splits may be nonsensical, so reject them (\"llouis\" -> ['ll', 'ou', \"is\"])\n    clean_words.extend(spell.known(split_words))\n    \n    return clean_words\n\ndef get_log_likelihood(doc1, doc2):    \n    doc1_counts = Counter(doc1)\n    doc1_freq = {\n        x: doc1_counts[x]\/len(doc1)\n        for x in doc1_counts\n    }\n    \n    doc2_counts = Counter(doc2)\n    doc2_freq = {\n        x: doc2_counts[x]\/len(doc2)\n        for x in doc2_counts\n    }\n    \n    doc_ratios = {\n        # 1 is added to prevent division by 0\n        x: math.log((doc1_freq[x] +1 )\/(doc2_freq[x]+1))\n        for x in doc1_freq if x in doc2_freq\n    }\n    \n    top_ratios = Counter(doc_ratios).most_common()\n    top_percent = int(0.1 * len(top_ratios))\n    return top_ratios[:top_percent]\n\n# Function to generate a document based on likelihood values for words \ndef get_scaled_list(log_list):\n    counts = [int(x[1]*100000) for x in log_list]\n    words = [x[0] for x in log_list]\n    cloud = []\n    for i, word in enumerate(words):\n        cloud.extend([word]*counts[i])\n    # Shuffle to make it more \"real\"\n    random.shuffle(cloud)\n    return cloud","4cb48b16":"# Convert string to a list of words\nvax_tweets['words'] = vax_tweets.text.apply(lambda x:re.findall(r'\\w+', x ))\n\nneg_doc = flatten_list(vax_tweets[vax_tweets['sentiment']=='negative']['words'])\nneg_doc = [x for x in neg_doc if is_acceptable(x)]\n\npos_doc = flatten_list(vax_tweets[vax_tweets['sentiment']=='positive']['words'])\npos_doc = [x for x in pos_doc if is_acceptable(x)]\n\nneu_doc = flatten_list(vax_tweets[vax_tweets['sentiment']=='neutral']['words'])\nneu_doc = [x for x in neu_doc if is_acceptable(x)]\n\n# Clean all the documents\nneg_doc_clean = clean_document(neg_doc)\nneu_doc_clean = clean_document(neu_doc)\npos_doc_clean = clean_document(pos_doc)\n\n# Combine classes B and C to compare against A (ex. \"positive\" vs \"non-positive\")\ntop_neg_words = get_log_likelihood(neg_doc_clean, flatten_list([pos_doc_clean, neu_doc_clean]))\ntop_neu_words = get_log_likelihood(neu_doc_clean, flatten_list([pos_doc_clean, neg_doc_clean]))\ntop_pos_words = get_log_likelihood(pos_doc_clean, flatten_list([neu_doc_clean, neg_doc_clean]))\n\n# Generate syntetic a corpus using our loglikelihood values \nneg_doc_final = get_scaled_list(top_neg_words)\nneu_doc_final = get_scaled_list(top_neu_words)\npos_doc_final = get_scaled_list(top_pos_words)\n\n# Visualise our synthetic corpus\ngenerate_word_clouds(neg_doc_final, neu_doc_final, pos_doc_final)","2834b42a":"vax_tweets['has_url'] = np.where(vax_tweets['orig_text'].str.contains('http'), 'yes', 'no')\nvax_tweets['has_url'].value_counts(normalize=True).plot.bar();","343be905":"def get_cloud(df, string, c_func):\n    string_l = string.lower()\n    df[string_l] = np.where(df['text'].str.lower().str.contains(string_l), 1, 0)\n    cloud_df = df.copy()[df[string_l]==1]\n    doc = flatten_list(cloud_df['words'])\n    doc = [x for x in doc if is_acceptable(x)]\n    doc = clean_document(doc)\n    fig, axes = plt.subplots(figsize=(9,5))\n    wordcloud = WordCloud(max_font_size=50, max_words=100, background_color=\"white\").generate(\" \".join(doc))\n    axes.imshow(wordcloud.recolor(color_func=c_func, random_state=3), interpolation='bilinear')\n    axes.set_title(\"Tweets Containg '%s'\" % (string))\n    axes.axis(\"off\")\n    plt.show();\n    print(cloud_df['orig_text'].head(5))\n    \nget_cloud(vax_tweets, 'Norway', red_color_func)","3ba6bb26":"get_cloud(vax_tweets, 'NHS', green_color_func)","d3f12219":"### Fine-tuning the classifier\nNow we can train the classifier using *discriminative learning rates* and *gradual unfreezing*, which has been found to give better results for this type of model. First let's freeze all but the last layer:","52b9dcf7":"We have a new column, `text_`, which is `text` offset by one. This is the dependent variable [`fastai`](https:\/\/docs.fast.ai\/) created for us. By default [`fastai`](https:\/\/docs.fast.ai\/) uses *word tokenization*, which splits the text on spaces and punctuation marks and breaks up words like *can't* into two separate tokens. [`fastai`](https:\/\/docs.fast.ai\/) also has some special tokens starting with 'xx' that are designed to make things easier for the model; for example [`xxmaj`](https:\/\/docs.fast.ai\/text.data.html) indicates that the next word begins with a capital letter and [`xxunk`](https:\/\/docs.fast.ai\/text.data.html) represents an unknown word that doesn't appear in the vocabulary very often. You could experiment with *subword tokenization* instead, which will split the text on commonly occuring groups of letters instead of spaces. This might help if you wanted to leave hashtags in since they often contain multiple words joined together with no spaces, e.g. #CovidVaccine. The [`fastai`](https:\/\/docs.fast.ai\/) tokenization process is explained in much more detail [here](https:\/\/youtu.be\/WjnwWeGjZcM?t=626) for those interested.","53639bef":"This looks pretty good! The positive tweets appear to be from people who have just received their first vaccine or are grateful for the job scientists and healthcare workers are doing, whereas the negative tweets seem to be from people who have suffered adverse reactions to the vaccine. The neutral tweets seem to be more like news, which could explain why it is the most prevelant sentiment; in fact, the vast majority of tweets contain urls:","8fafede2":"Let's go ahead and check out the results.","cdf93402":"We could use the `text` column of this dataset to train a Twitter language model, but since our end goal is sentiment analysis we will need to find another dataset that also contains sentiment labels to train our classifier. Let's use ['Complete Tweet Sentiment Extraction Data'](https:\/\/www.kaggle.com\/maxjon\/complete-tweet-sentiment-extraction-data), which contains 40,000 tweets labelled as either negative, neutral or positive sentiment. For more accurate results you could use the ['sentiment140'](https:\/\/www.kaggle.com\/kazanova\/sentiment140) dataset instead, which contains 1.6m tweets labelled as either positive or negative.","9a70689b":"Here we passed [`language_model_learner`](https:\/\/docs.fast.ai\/text.learner.html#language_model_learner) our [`DataLoaders`](https:\/\/docs.fast.ai\/data.core.html#DataLoaders), `dls_lm`, and the pre-trained [RNN](https:\/\/www.simplilearn.com\/tutorials\/deep-learning-tutorial\/rnn) model, [*AWD_LSTM*](https:\/\/docs.fast.ai\/text.models.awdlstm.html), which is built into [`fastai`](https:\/\/docs.fast.ai\/). [`drop_mult`](https:\/\/docs.fast.ai\/text.learner.html#text_classifier_learner) is a multiplier applied to all [dropouts](https:\/\/machinelearningmastery.com\/dropout-for-regularizing-deep-neural-networks\/) in the AWD_LSTM model to reduce overfitting. For example, by default [`fastai`](https:\/\/docs.fast.ai\/)'s AWD_LSTM applies [`EmbeddingDropout`](https:\/\/docs.fast.ai\/text.models.awdlstm.html#EmbeddingDropout) with 10% probability (at the time of writing), but we told [`fastai`](https:\/\/docs.fast.ai\/) that we want to reduce that to 3%. The [`metrics`](https:\/\/docs.fast.ai\/metrics.html) we want to track are *perplexity*, which is the exponential of the loss (in this case cross entropy loss), and *accuracy*, which tells us how often our model predicts the next word correctly. We can also train with fp16 to use less memory and speed up the training process.\n\nWe can find a good learning rate for training using [`lr_find`](https:\/\/docs.fast.ai\/callback.schedule.html#Learner.lr_find) and use that to fit our model.","3687c8d6":"## Conclusion\n`fastai` make NLP really easy and we were able to get quite good results with a limited dataset and not a lot of training time by using the ULMFiT approach. To summarise, the steps are:\n1. Fine-tune a language model to predict the next word in a tweet, using a model pre-trained on Wikipedia.\n2. Fine-tune a classification model to predict tweet sentiment using the pre-trained language model.\n3. Apply the classifier to unlabelled tweets to analyse sentiment.\n\nHopefully you found this useful!","b44d6cc7":"The overall sentiment about the NHS appears to be positive, however:","da3081cc":"For our language model, the only input we need is the tweet text. As we will see in a moment [`fastai`](https:\/\/docs.fast.ai\/) can handle text preprocessing and tokenization for us, but it might be a good idea to remove things like twitter handles, urls, hashtags and emojis first. You could experiment with leaving these in for your own models and see how it affects the results. There are also some rows with blank tweets which need to be removed.\n\nWe ideally want the language model to learn not just about tweet language, but more specifically about vaccine tweet language. We can therefore use text from both datasets as input for the language model. For the classification model we need to remove all rows with missing sentiment, however.","15a6a135":"Our model correctly predicts sentiment just under 77% of the time. We could perhaps do better with a larger dataset as mentioned earlier, or different model hyperparameters. It might be worth experimenting with this yourself to see if you can improve the accuracy.\n\nWe can quickly sense check the model by calling [`predict`](https:\/\/docs.fast.ai\/learner.html#Learner.predict), which returns the predicted sentiment, the index of the prediction and predicted probabilities for negative, neutral and positive sentiment.","33cb147c":"Finally, let's unfreeze the entire model and train a bit more:","8584f0ec":"Initialising the [`Learner`](https:\/\/docs.fast.ai\/learner.html#Learner) is similar to before, but in this case we want a [`text_classifier_learner`](https:\/\/docs.fast.ai\/text.learner.html#text_classifier_learner).","7efc8fbe":"We can see that the predominant sentiment is neutral, with more positive tweets than negative. It's encouraging that negative sentiment isn't higher! We can also visualise how sentiment changes over time:","62ad56e1":"### Fine-tuning the language model\nThe next step is to create a language model using [`language_model_learner`](https:\/\/docs.fast.ai\/text.learner.html#language_model_learner).","8222ca08":"Here we told [`fastai`](https:\/\/docs.fast.ai\/) that we are working with text data, which is contained in the `text` column of a [`pandas`](https:\/\/pandas.pydata.org\/docs\/) [`DataFrame`](https:\/\/pandas.pydata.org\/docs\/reference\/api\/pandas.DataFrame.html) called `df_lm`. We set [`is_lm=True`](https:\/\/docs.fast.ai\/text.data.html#TextDataLoaders) since we want to train a language model, so [`fastai`](https:\/\/docs.fast.ai\/) needs to label the input data for us. Finally, we told [`fastai`](https:\/\/docs.fast.ai\/) to hold out a random 10% of our data for a validation set using [`valid_pct=0.1`](https:\/\/docs.fast.ai\/text.data.html#TextDataLoaders).\n\nLet's take a look at the first two rows of the [`DataLoader`](https:\/\/docs.fast.ai\/data.load.html#DataLoader) using [`show_batch`](https:\/\/docs.fast.ai\/data.core.html#TfmdDL.show_batch).","b5fe74ee":"This notebook will walk through steps 2 and 3 with [`fastai`](https:\/\/docs.fast.ai\/). Afterwards we can use our new classifier to analyse sentiment in the COVID-19 vaccine tweets.","d455fe89":"People have died after receiving the vaccine in Norway, which explains why it shows up in the negative sentiment word cloud:","a262c8ca":"## Transfer learning in NLP - the ULMFiT approach\n\nWe will be making use of *transfer learning* to help us create a model to analyse tweet sentiment. The idea behind transfer learning is that neural networks learn information that generalises to new problems, [particularly the early layers of the network](https:\/\/arxiv.org\/pdf\/1311.2901.pdf). In computer vision, for example, we can take a model that was trained on the ImageNet dataset to recognise different features of images such as circles, then apply that to a smaller dataset and *fine-tune* the model to be more suited to a specific task (e.g. classifying images as cats or dogs). This technique allows us to train neural networks much faster and with far less data than we would otherwise need.\n\nIn 2018 [a paper](https:\/\/arxiv.org\/abs\/1801.06146) introduced a transfer learning technique for NLP called 'Universal Language Model Fine-Tuning' (ULMFiT). The approach is as follows:\n1. Train a *language model* to predict the next word in a sentence. This step is already done for us; with [`fastai`](https:\/\/docs.fast.ai\/) we can download a model that has been pre-trained for this task on millions of Wikipedia articles. A good language model already knows a lot about how language works in general - for  instance, given the sentence 'Tokyo is the capital of', the model might predict 'Japan' as the next word. In this case the model understands that Tokyo is closely related to Japan and that 'capital' refers to 'city' here instead of 'upper-case' or 'money'.\n2. Fine-tune the language model to a more specific task. The pre-trained language model is good at understanding Wikipedia English, but Twitter English is a bit different. We can take the information the Wikipedia model has learned and apply that to a Twitter dataset to get a Twitter language model that is good at predicting the next word in a tweet.\n3. Fine-tune a *classification model* to identify sentiment using the pre-trained language model. The idea here is that since our language model already knows a lot about Twitter English, it's not a huge leap from there to train a classifier that understands that 'love' refers to positive sentiment and 'hate' refers to negative sentiment. If we tried to train a classifier without using a pre-trained model it would have to learn the whole language from scratch first, which would be very difficult and time consuming.\n\n<img alt=\"Diagram of the ULMFiT process (source: course.fast.ai)\" width=\"700\" align=\"left\" caption=\"The ULMFiT process\" id=\"ulmfit_process\" src=https:\/\/i.imgur.com\/8XLluAn.png>","5afd1c2d":"We can then make predictions using [`get_preds`](https:\/\/docs.fast.ai\/learner.html#Learner.get_preds):","f487e0c7":"To use the API, [`fastai`](https:\/\/docs.fast.ai\/) needs the following:\n* [`blocks`](https:\/\/docs.fast.ai\/data.block.html#TransformBlock):\n    * [`TextBlock`](https:\/\/docs.fast.ai\/text.data.html#TextBlock): Our x variable will be text contained in a [`pandas`](https:\/\/pandas.pydata.org\/docs\/) [`DataFrame`](https:\/\/pandas.pydata.org\/docs\/reference\/api\/pandas.DataFrame.html). We want to use the same sequence length and vocab as the language model [`DataLoaders`](https:\/\/docs.fast.ai\/data.core.html#DataLoaders) so we can make use of our pre-trained model.\n    * [`CategoryBlock`](https:\/\/docs.fast.ai\/data.block.html#CategoryBlock): Our y variable will be a single-label category (negative, neutral or positive sentiment).\n* [`get_x`](https:\/\/docs.fast.ai\/data.transforms.html#ColReader), [`get_y`](https:\/\/docs.fast.ai\/data.transforms.html#ColReader): Get data for the model by reading the `text` and `sentiment` columns from the [`DataFrame`](https:\/\/pandas.pydata.org\/docs\/reference\/api\/pandas.DataFrame.html).\n* [`splitter`](https:\/\/docs.fast.ai\/data.transforms.html#RandomSplitter): We will use [`RandomSplitter()`](https:\/\/docs.fast.ai\/data.transforms.html#RandomSplitter) to randomly split the data into a training set (80% by default) and a validation set (20%).\n* [`dataloaders`](https:\/\/docs.fast.ai\/data.block#DataBlock.dataloaders): Builds the [`DataLoaders`](https:\/\/docs.fast.ai\/data.core.html#DataLoaders) using the [`DataBlock`](https:\/\/docs.fast.ai\/tutorial.datablock.html#Text) template we just defined, the *df_clas* [`DataFrame`](https:\/\/pandas.pydata.org\/docs\/reference\/api\/pandas.DataFrame.html) and a batch size of 64.\n\nWe can call show batch as before; this time the dependent variable is sentiment.","b2125498":"Let's save the model *encoder* so we can use it to fine-tune our classifier. The encoder is all of the model except for the final layer, which converts activations to probabilities of picking each token in the vocabulary. We want to keep the knowledge the model has learned about tweet language but we won't be using our classifier to predict the next word in a sentence, so we won't need the final layer any more.","f9be59de":"## Training a sentiment classifier\nTo get the [`DataLoaders`](https:\/\/docs.fast.ai\/data.core.html#DataLoaders) for our classifier let's use the [`DataBlock`](https:\/\/docs.fast.ai\/tutorial.datablock.html#Text) API this time, which is more customisable.","9ef6bcc3":"Finally, we want to load the encoder from the language model we trained earlier, so our classifier uses pre-trained weights.","2f49ecb7":"### Further analysis using 'smarter' word clouds\nTo dig a bit deeper, let's generate some word clouds to see which words are indicative of each sentiment. The code below is from [this notebook](https:\/\/www.kaggle.com\/pawanbhandarkar\/generate-smarter-word-clouds-with-log-likelihood), which contains a more detailed explanation of the methodology used to generate 'smarter' word clouds. Please go and upvote the original notebook if you find this part useful!","066c5f8d":"After one epoch our language model is predicting the next word in a tweet around 23% of the time - not too bad! We can [`unfreeze`](https:\/\/docs.fast.ai\/learner.html#Learner.unfreeze) the entire model, find a more suitable learning rate and train for a few more epochs to improve the accuracy further.","58fb315d":"Now all but the last three:","a388e90d":"## Loading the data\nFirst, let's import [`fastai`](https:\/\/docs.fast.ai\/)'s [`text`](https:\/\/docs.fast.ai\/tutorial.text.html) module and take a look at our data.","c012ec64":"# Pfizer Vaccine Sentiment Analysis with fastai\nIn this notebook we will perform sentiment analysis on tweets about COVID-19 vaccines using the [`fastai`](https:\/\/docs.fast.ai\/) library. I will provide a brief overview of the process here, but a much more in-depth explanation of NLP with [`fastai`](https:\/\/docs.fast.ai\/) can be found in [lesson 8](https:\/\/course.fast.ai\/videos\/?lesson=8) of the [`fastai`](https:\/\/docs.fast.ai\/) course. For convenience clicking on inline code written like [`this`](https:\/\/docs.fast.ai\/tutorial.text.html) will take you to the relevant part of the [`fastai`](https:\/\/docs.fast.ai\/) documentation where appropriate.","a6cf0b36":"Now freeze all but the last two layers:","d7740c4b":"## Analysing the tweets\nTo carry out sentiment analysis on the vaccine tweets, we can add them to the [`DataLoaders`](https:\/\/docs.fast.ai\/data.core.html#DataLoaders) as a test set:","d19920c6":"## Training a language model\nTo train our language model we can use self-supervised learning; we just need to give the model some text as an independent variable and [`fastai`](https:\/\/docs.fast.ai\/) will automatically preprocess it and create a dependent variable for us. We can do this in one line of code using the [`DataLoaders`](https:\/\/docs.fast.ai\/data.core.html#DataLoaders) class, which converts our input data into a [`DataLoader`](https:\/\/docs.fast.ai\/data.load.html#DataLoader) object that can be used as an input to a [`fastai`](https:\/\/docs.fast.ai\/) [`Learner`](https:\/\/docs.fast.ai\/learner.html#Learner).","9af46ec8":"When we created our [`Learner`](https:\/\/docs.fast.ai\/learner.html#Learner) the embeddings from the pre-trained AWD_LSTM model were merged with random embeddings added for words that weren't in the vocabulary. The pre-trained layers were also automatically frozen for us. Using [`fit_one_cycle`](https:\/\/docs.fast.ai\/callback.schedule.html#Learner.fit_one_cycle) with our [`Learner`](https:\/\/docs.fast.ai\/learner.html#Learner) will train only the *new random embeddings* (i.e. words that are in our Twitter vocab but not the Wikipedia vocab) in the last layer of the neural network.","2c9b8452":"After a bit more training we can predict the next word in a tweet just under 26% of the time. Let's test the model out by using it to write some random tweets (in this case it will generate some text following 'I love')."}}