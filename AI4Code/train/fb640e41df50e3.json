{"cell_type":{"4bb5852c":"code","1b245873":"code","ddc821d8":"code","e2d2f963":"code","1e75c2a2":"code","4773028b":"code","bb8559a4":"code","b1429de5":"code","2860e4ae":"code","b22ae539":"code","29e877ea":"code","9057397f":"code","e8c8a435":"code","e39e7a4f":"code","9d61c399":"code","ef6a15d1":"code","dc085076":"code","1792d538":"code","a0f8ee90":"code","f875966c":"code","f4f5f1aa":"code","c77ba947":"code","4f9a2a51":"code","725bf91e":"code","d3d738aa":"code","70804ec5":"code","c28631e1":"code","d1040a8a":"code","b93081c6":"code","3f3a718e":"code","1db511c4":"code","4f66f639":"code","d0f0d2d3":"code","f2e1d9d0":"code","0af51337":"code","5d16bdb1":"markdown"},"source":{"4bb5852c":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","1b245873":"import torch\nif torch.cuda.is_available():  \n    device = torch.device(\"cuda\")\n    print('I will use the GPU:', torch.cuda.get_device_name(0))\n    \nelse:\n    print('No GPU available, using the CPU instead.')\n    device = torch.device(\"cpu\")","ddc821d8":"updated_train=pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/train.csv\")\nupdated_test=pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/test.csv\")","e2d2f963":"updated_train.head()","1e75c2a2":"import re\ndef preprocess(text):\n\n    text=text.lower()\n    # remove hyperlinks\n    text = re.sub(r'https?:\\\/\\\/.*[\\r\\n]*', '', text)\n    text = re.sub(r'http?:\\\/\\\/.*[\\r\\n]*', '', text)\n    #Replace &amp, &lt, &gt with &,<,> respectively\n    text=text.replace(r'&amp;?',r'and')\n    text=text.replace(r'&lt;',r'<')\n    text=text.replace(r'&gt;',r'>')\n    #remove hashtag sign\n    #text=re.sub(r\"#\",\"\",text)   \n    #remove mentions\n    text = re.sub(r\"(?:\\@)\\w+\", '', text)\n    #text=re.sub(r\"@\",\"\",text)\n    #remove non ascii chars\n    text=text.encode(\"ascii\",errors=\"ignore\").decode()\n    #remove some puncts (except . ! ?)\n    text=re.sub(r'[:\"#$%&\\*+,-\/:;<=>@\\\\^_`{|}~]+','',text)\n    text=re.sub(r'[!]+','!',text)\n    text=re.sub(r'[?]+','?',text)\n    text=re.sub(r'[.]+','.',text)\n    text=re.sub(r\"'\",\"\",text)\n    text=re.sub(r\"\\(\",\"\",text)\n    text=re.sub(r\"\\)\",\"\",text)\n    \n    text=\" \".join(text.split())\n    return text\n\nupdated_train['text'] = updated_train['text'].apply(preprocess)\nupdated_test['text'] = updated_test['text'].apply(preprocess)\nupdated_train = updated_train[updated_train[\"text\"]!='']","4773028b":"updated_train = updated_train[[\"text\",\"target\"]]\nupdated_train.head()","bb8559a4":"updated_train[\"target\"].value_counts()","b1429de5":"# Get the lists of lyrics and their labels.\ntexts = updated_train.text.values\nlabels = updated_train.target.values","2860e4ae":"from transformers import ElectraTokenizer, ElectraForSequenceClassification,AdamW\nimport torch\ntokenizer = ElectraTokenizer.from_pretrained('google\/electra-base-discriminator')\nmodel = ElectraForSequenceClassification.from_pretrained('google\/electra-base-discriminator',num_labels=2)\nmodel.cuda()","b22ae539":"#to show length of embedding will be helpful to determine maximum length of comments and padding threshold\nimport matplotlib.pyplot as plt\ndef plot_sentence_embeddings_length(text_list, tokenizer):\n    tokenized_texts = list(map(lambda t: tokenizer.tokenize(t), text_list))\n    tokenized_texts_len = list(map(lambda t: len(t), tokenized_texts))\n    fig, ax = plt.subplots(figsize=(8, 5));\n    ax.hist(tokenized_texts_len, bins=40);\n    ax.set_xlabel(\"Length of Comment Embeddings\");\n    ax.set_ylabel(\"Number of Comments\");\n    return\nplot_sentence_embeddings_length(texts, tokenizer)","29e877ea":"indices=tokenizer.batch_encode_plus(texts,max_length=64,add_special_tokens=True, return_attention_mask=True,pad_to_max_length=True,truncation=True)\n\ninput_ids=indices[\"input_ids\"]\nattention_masks=indices[\"attention_mask\"]","9057397f":"from sklearn.model_selection import train_test_split\n\n# Use 99% for training and 1% for validation.\ntrain_inputs, validation_inputs, train_labels, validation_labels = train_test_split(input_ids, labels, \n                                                            random_state=42, test_size=0.2)\n# Do the same for the masks.\ntrain_masks, validation_masks, _, _ = train_test_split(attention_masks, labels,\n                                             random_state=42, test_size=0.2)","e8c8a435":"# Convert all of our data into torch tensors, the required datatype for our model\ntrain_inputs = torch.tensor(train_inputs)\nvalidation_inputs = torch.tensor(validation_inputs)\ntrain_labels = torch.tensor(train_labels, dtype=torch.long)\nvalidation_labels = torch.tensor(validation_labels, dtype=torch.long)\ntrain_masks = torch.tensor(train_masks, dtype=torch.long)\nvalidation_masks = torch.tensor(validation_masks, dtype=torch.long)","e39e7a4f":"from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n\nbatch_size = 32\n\n# Create the DataLoader for our training set.\ntrain_data = TensorDataset(train_inputs, train_masks, train_labels)\ntrain_sampler = RandomSampler(train_data)\ntrain_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n\n# Create the DataLoader for our validation set.\nvalidation_data = TensorDataset(validation_inputs, validation_masks, validation_labels)\nvalidation_sampler = SequentialSampler(validation_data)\nvalidation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)","9d61c399":"optimizer = AdamW(model.parameters(),\n                  lr = 6e-6, # args.learning_rate - default is 5e-5, our notebook had 2e-5\n                  eps = 1e-8 # args.adam_epsilon  - default is 1e-8.\n                )\n\nfrom transformers import get_linear_schedule_with_warmup\n\n# Number of training epochs (authors recommend between 2 and 4)\nepochs = 5\n\n# Total number of training steps is number of batches * number of epochs.\ntotal_steps = len(train_dataloader) * epochs\n\n# Create the learning rate scheduler.\nscheduler = get_linear_schedule_with_warmup(optimizer, \n                                            num_warmup_steps = 0, \n                                            num_training_steps = total_steps)","ef6a15d1":"import numpy as np\n\n# Function to calculate the accuracy of our predictions vs labels\ndef flat_accuracy(preds, labels):\n    pred_flat = np.argmax(preds, axis=1).flatten()\n    labels_flat = labels.flatten()\n    return np.sum(pred_flat == labels_flat) \/ len(labels_flat)","dc085076":"import time\nimport datetime\n\ndef format_time(elapsed):\n    '''\n    Takes a time in seconds and returns a string hh:mm:ss\n    '''\n    # Round to the nearest second.\n    elapsed_rounded = int(round((elapsed)))\n    \n    # Format as hh:mm:ss\n    return str(datetime.timedelta(seconds=elapsed_rounded))","1792d538":"import random\n\n# This training code is based on the `run_glue.py` script here:\n# https:\/\/github.com\/huggingface\/transformers\/blob\/5bfcd0485ece086ebcbed2d008813037968a9e58\/examples\/run_glue.py#L128\n\n# Set the seed value all over the place to make this reproducible.\nseed_val = 42\n\nrandom.seed(seed_val)\nnp.random.seed(seed_val)\ntorch.manual_seed(seed_val)\ntorch.cuda.manual_seed_all(seed_val)\n\n# Store the average loss after each epoch so we can plot them.\nloss_values = []\n\n# For each epoch...\nfor epoch_i in range(0, epochs):\n    print(\"\")\n    print('======== Epoch {:} \/ {:} ========'.format(epoch_i + 1, epochs))\n    print('Training...')\n\n    # Measure how long the training epoch takes.\n    t0 = time.time()\n\n    # Reset the total loss for this epoch.\n    total_loss = 0\n\n    model.train()\n\n    for step, batch in enumerate(train_dataloader):\n\n        # Progress update every 100 batches.\n        if step % 50 == 0 and not step == 0:\n            # Calculate elapsed time in minutes.\n            elapsed = format_time(time.time() - t0)\n            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n\n        # `batch` contains three pytorch tensors: [0]: input ids ,[1]: attention masks,[2]: labels \n        b_input_ids = batch[0].to(device)\n        b_input_mask = batch[1].to(device)\n        b_labels = batch[2].to(device)\n\n        # Clear any previously calculated gradients.\n        # (source: https:\/\/stackoverflow.com\/questions\/48001598\/why-do-we-need-to-call-zero-grad-in-pytorch)\n        model.zero_grad()        \n\n        # Evaluate the model on this training batch.\n        outputs = model(b_input_ids, \n                    token_type_ids=None, \n                    attention_mask=b_input_mask, \n                    labels=b_labels)\n        loss = outputs[0]\n\n        # Accumulate the training loss over all of the batches \n        total_loss += loss.item()\n        loss.backward()\n\n        # Clip the norm of the gradients to 1.0. to prevent the \"exploding gradients\" problem.\n        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n        optimizer.step()\n\n        # Update the learning rate.\n        scheduler.step()\n\n    # Calculate the average loss over the training data.\n    avg_train_loss = total_loss \/ len(train_dataloader)            \n    \n    # Store the loss value for plotting the learning curve.\n    loss_values.append(avg_train_loss)\n\n    print(\"\")\n    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n    print(\"  Training epoch took: {:}\".format(format_time(time.time() - t0)))\n      \nprint(\"\")\nprint(\"Training complete!\")","a0f8ee90":"# Validation\nprint(\"\")\nprint(\"Running Validation...\")\n\nt0 = time.time()\nmodel.eval()\n\npreds=[]\ntrue=[]\n\n# Tracking variables \neval_loss, eval_accuracy = 0, 0\nnb_eval_steps, nb_eval_examples = 0, 0\n\n# Evaluate data for one epoch\nfor batch in validation_dataloader:\n    \n    # Add batch to GPU\n    batch = tuple(t.to(device) for t in batch)\n    \n    # Unpack the inputs from our dataloader\n    b_input_ids, b_input_mask, b_labels = batch\n    \n    # Telling the model not to compute or store gradients, saving memory and speeding up validation\n    with torch.no_grad():        \n\n        outputs = model(b_input_ids, \n                        token_type_ids=None, \n                        attention_mask=b_input_mask)\n    \n    # Get the \"logits\" output by the model. The \"logits\" are the output values prior to applying an activation function like the softmax.\n    logits = outputs[0]\n\n    # Move logits and labels to CPU\n    logits = logits.detach().cpu().numpy()\n    label_ids = b_labels.to('cpu').numpy()\n    \n    preds.append(logits)\n    true.append(label_ids)\n    # Calculate the accuracy for this batch.\n    tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n    \n    # Accumulate the total accuracy.\n    eval_accuracy += tmp_eval_accuracy\n\n    # Track the number of batches\n    nb_eval_steps += 1\n\n# Report the final accuracy for this validation run.\nprint(\"  Accuracy: {0:.2f}\".format(eval_accuracy\/nb_eval_steps))\nprint(\"  Validation took: {:}\".format(format_time(time.time() - t0)))","f875966c":"# Combine the predictions for each batch into a single list of 0s and 1s.\nflat_predictions = [item for sublist in preds for item in sublist]\nflat_predictions = np.argmax(flat_predictions, axis=1).flatten()\n\n# Combine the correct labels for each batch into a single list.\nflat_true_labels = [item for sublist in true for item in sublist]","f4f5f1aa":"from sklearn.metrics import classification_report\nprint(classification_report(flat_predictions,flat_true_labels))","c77ba947":"comments1 = updated_test.text.values\n\nindices1=tokenizer.batch_encode_plus(comments1,max_length=128,add_special_tokens=True, return_attention_mask=True,pad_to_max_length=True,truncation=True)\ninput_ids1=indices1[\"input_ids\"]\nattention_masks1=indices1[\"attention_mask\"]\n\nprediction_inputs1= torch.tensor(input_ids1)\nprediction_masks1 = torch.tensor(attention_masks1)\n\n\n# Set the batch size.  \nbatch_size = 32 \n\n# Create the DataLoader.\nprediction_data1 = TensorDataset(prediction_inputs1, prediction_masks1)\nprediction_sampler1 = SequentialSampler(prediction_data1)\nprediction_dataloader1 = DataLoader(prediction_data1, sampler=prediction_sampler1, batch_size=batch_size)","4f9a2a51":"print('Predicting labels for {:,} test sentences...'.format(len(prediction_inputs1)))\n\n# Put model in evaluation mode\nmodel.eval()\n\n# Tracking variables \npredictions = []\n\n# Predict \nfor batch in prediction_dataloader1:\n  # Add batch to GPU\n  batch = tuple(t.to(device) for t in batch)\n  \n  # Unpack the inputs from our dataloader\n  b_input_ids1, b_input_mask1 = batch\n  \n  # Telling the model not to compute or store gradients, saving memory and \n  # speeding up prediction\n  with torch.no_grad():\n      # Forward pass, calculate logit predictions\n      outputs1 = model(b_input_ids1, token_type_ids=None, \n                      attention_mask=b_input_mask1)\n\n  logits1 = outputs1[0]\n\n  # Move logits and labels to CPU\n  logits1 = logits1.detach().cpu().numpy()\n  \n  \n  # Store predictions and true labels\n  predictions.append(logits1)\n\nflat_predictions = [item for sublist in predictions for item in sublist]\nflat_predictions = np.argmax(flat_predictions, axis=1).flatten()","725bf91e":"sample_sub=pd.read_csv('..\/input\/nlp-getting-started\/sample_submission.csv')\nsubmit=pd.DataFrame({'id':sample_sub['id'].values.tolist(),'target':flat_predictions})\n#submit.to_csv('submission.csv',index=False)","d3d738aa":"submit.head()","70804ec5":"df_leak = pd.read_csv('\/kaggle\/input\/disasters-on-social-media\/socialmedia-disaster-tweets-DFE.csv', encoding ='ISO-8859-1')[['choose_one', 'text']]\n\n# Creating target and id\ndf_leak['target'] = (df_leak['choose_one'] == 'Relevant').astype(np.int8)\ndf_leak['id'] = df_leak.index.astype(np.int16)\ndf_leak.drop(columns=['choose_one', 'text'], inplace=True)\n\n# Merging target to test set\nupdated_test = updated_test.merge(df_leak, on=['id'], how='left')\n\nprint('Leaked Data Set Shape = {}'.format(df_leak.shape))\nprint('Leaked Data Set Memory Usage = {:.2f} MB'.format(df_leak.memory_usage().sum() \/ 1024**2))\n\nperfect_submission = pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/sample_submission.csv\")\nperfect_submission['target'] = updated_test['target'].values\nperfect_submission.to_csv('submission.csv',index=False)\nperfect_submission.describe()","c28631e1":"import re\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom collections import defaultdict, Counter\nfrom sklearn.feature_extraction.text import CountVectorizer\nimport nltk\nfrom nltk.corpus import stopwords\nfrom wordcloud import WordCloud \nfrom nltk.tokenize import word_tokenize \n\n\nnltk.download('stopwords', quiet=True)\nstopwords = stopwords.words('english')\nsns.set(style=\"white\", font_scale=1.2)\nplt.rcParams[\"figure.figsize\"] = [20,8]\npd.set_option.display_max_columns = 0\npd.set_option.display_max_rows = 0","d1040a8a":"train = pd.read_csv(\"..\/input\/nlp-getting-started\/train.csv\")\ntest = pd.read_csv(\"..\/input\/nlp-getting-started\/test.csv\")","b93081c6":"keywords_vc = pd.DataFrame({\"Count\": train[\"keyword\"].value_counts()})\nsns.barplot(y=keywords_vc[0:30].index, x=keywords_vc[0:30][\"Count\"], orient='h')\nplt.title(\"Top 30 Keywords\")\nplt.show()","3f3a718e":"disaster_keywords = train.loc[train[\"target\"] == 1][\"keyword\"].value_counts()\nnondisaster_keywords = train.loc[train[\"target\"] == 0][\"keyword\"].value_counts()\n\nfig, ax = plt.subplots(1,2, figsize=(20,8))\nsns.barplot(y=disaster_keywords[0:10].index, x=disaster_keywords[0:10], orient='h', ax=ax[0], palette=\"Reds_d\")\nsns.barplot(y=nondisaster_keywords[0:10].index, x=nondisaster_keywords[0:10], orient='h', ax=ax[1], palette=\"Greens_d\")\nax[0].set_title(\"Top 10 Keywords - Disaster Tweets\")\nax[0].set_xlabel(\"Keyword Frequency\")\nax[1].set_title(\"Top 10 Keywords - Non-Disaster Tweets\")\nax[1].set_xlabel(\"Keyword Frequency\")\nplt.tight_layout()\nplt.show()","1db511c4":"tsunami_tweets = train[(train[\"keyword\"].fillna(\"\").str.contains(\"tsunami\")) & (train[\"target\"] == 0)]\nprint(\"An example tweet:\\n\", tsunami_tweets.iloc[20, 3])\ntsunami_tweets.head()","4f66f639":"def keyword_disaster_probabilities(x):\n    tweets_w_keyword = np.sum(train[\"keyword\"].fillna(\"\").str.contains(x))\n    tweets_w_keyword_disaster = np.sum(train[\"keyword\"].fillna(\"\").str.contains(x) & train[\"target\"] == 1)\n    return tweets_w_keyword_disaster \/ tweets_w_keyword\n\nkeywords_vc[\"Disaster_Probability\"] = keywords_vc.index.map(keyword_disaster_probabilities)\nkeywords_vc.sort_values(by=\"Disaster_Probability\", ascending=False).head(10)","d0f0d2d3":"locations_vc = train[\"location\"].value_counts()\nsns.barplot(y=locations_vc[0:30].index, x=locations_vc[0:30], orient='h')\nplt.title(\"Top 30 Locations\")\nplt.show()","f2e1d9d0":"disaster_locations = train.loc[train[\"target\"] == 1][\"location\"].value_counts()\nnondisaster_locations = train.loc[train[\"target\"] == 0][\"location\"].value_counts()\n\nfig, ax = plt.subplots(1,2, figsize=(20,8))\nsns.barplot(y=disaster_locations[0:10].index, x=disaster_locations[0:10], orient='h', ax=ax[0], palette=\"Reds_d\")\nsns.barplot(y=nondisaster_locations[0:10].index, x=nondisaster_locations[0:10], orient='h', ax=ax[1], palette=\"Greens_d\")\nax[0].set_title(\"Top 10 Locations - Disaster Tweets\")\nax[0].set_xlabel(\"Keyword Frequency\")\nax[1].set_title(\"Top 10 Locations - Non-Disaster Tweets\")\nax[1].set_xlabel(\"Keyword Frequency\")\nplt.tight_layout()\nplt.show()","0af51337":"target_vc = train[\"target\"].value_counts(normalize=True)\nprint(\"Not Disaster: {:.2%}, Disaster: {:.2%}\".format(target_vc[0], target_vc[1]))\nsns.barplot(x=target_vc.index, y=target_vc)\nplt.title(\"Histogram of Disaster vs. Non-Disaster\")\nplt.xlabel(\"0 = Non-Disaster, 1 = Disaster\")\nplt.show()","5d16bdb1":"**Exploratory Data Analysis**"}}