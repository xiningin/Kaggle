{"cell_type":{"abd5782f":"code","4f836c98":"code","ec6e8c48":"code","50452060":"code","db7464e2":"code","10246f93":"code","993ee80e":"code","74f5f3a3":"code","84aa2ba5":"code","e494e863":"code","21c19982":"code","a8356f12":"code","e9e2597a":"code","e5e9783e":"code","5a447720":"code","8e1e1db9":"code","f85bf9ec":"code","6e044ff6":"code","e92a12ca":"code","36ac77a0":"code","768bcc5e":"code","adae4ebc":"code","e3169605":"code","d8da1fdf":"code","4bc82f3d":"code","97d7b028":"code","9935d085":"code","103f56ac":"code","5b953d71":"code","27e50c1d":"code","ceae5d21":"code","b2665b9e":"code","7f7666b8":"code","9bd5997a":"code","db1a4b53":"code","f1cbd77c":"code","7ec6d54c":"code","a909978d":"code","b0c1a0b1":"code","d872f51b":"code","5bb68c41":"code","ce3fad91":"markdown","41f1dab6":"markdown","2733c907":"markdown","89873d4a":"markdown","ea938773":"markdown","6bd0efc7":"markdown","740222e9":"markdown","13e65212":"markdown","7776a663":"markdown","644e8290":"markdown","84de36a7":"markdown","994e19b4":"markdown","05b8529d":"markdown","b586d96c":"markdown","44a4cf6f":"markdown","cd7e56be":"markdown","074b4bf7":"markdown","d15a0492":"markdown","c30bf20d":"markdown","eb3b29a0":"markdown","21145047":"markdown","9730ed85":"markdown","6417caee":"markdown","78f13831":"markdown","a192cac5":"markdown","a30dd4f4":"markdown","d3b057a5":"markdown","6ef36752":"markdown","c85fb2d5":"markdown","33a3c28a":"markdown","3b20491b":"markdown"},"source":{"abd5782f":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nwarnings.simplefilter(action='ignore', category=FutureWarning)\nwarnings.simplefilter(action='ignore', category=RuntimeWarning)\n\n\n\n\npd.set_option(\"display.max_columns\",100)\npd.set_option('display.max_colwidth', -1)","4f836c98":"print('Importing data...')\ndata = {\n    'train': pd.read_csv('..\/input\/application_train.csv'),\n    'test': pd.read_csv('..\/input\/application_test.csv'),\n    'bb': pd.read_csv('..\/input\/bureau_balance.csv'),\n    'b': pd.read_csv('..\/input\/bureau.csv'),\n    'ccb': pd.read_csv('..\/input\/credit_card_balance.csv'),\n    'ip': pd.read_csv('..\/input\/installments_payments.csv'),\n    'POSb': pd.read_csv('..\/input\/POS_CASH_balance.csv'),\n    'previous': pd.read_csv('..\/input\/previous_application.csv')\n    }","ec6e8c48":"def cat_features(df):\n    cat_f = df.select_dtypes(include = ['object']).apply(lambda x: x.nunique(dropna=False), axis = 0)\n    return cat_f\n\ndef cat_levels(df,cat_info):\n    cat_f = df[cat_info.index]\n    levels = {}\n    for c in cat_f:\n        level = list(df[c].replace(np.nan,'NaN').unique())\n        levels.update({c:level})\n    return pd.DataFrame.from_dict(levels,orient='index').fillna('')\n\ndef bin_num(df):\n    binary_f = []\n    for c in df.columns:\n        if len(df[c].unique())==2 and c not in cat_features(df).index:\n            binary_f.append(c)\n    return binary_f\n\ndef cat_plot(df,cols,r,c,figsize):\n    fig, ax = plt.subplots(r,c,figsize=figsize)\n    for i in range(len(cols)):\n        colname = cols[i]\n        row = i\/\/c\n        col = i%c\n        axa = sns.countplot(x=colname, data=df,ax = ax[row,col])\n        plt.setp(axa.xaxis.get_majorticklabels(), rotation=-45)\n        plt.tight_layout()\n    plt.show()\n\ndef cat_plot_target(df,cols,r,c,figsize):\n    plt.figure(figsize=figsize)\n    for i in range(len(cols)):\n        colname = cols[i]\n        df_plot = df[['TARGET',colname]].dropna().melt(['TARGET'],value_name=colname)\n        df_group = df_plot.groupby(['TARGET',colname],as_index = False).count()\n        sums = df_group.groupby('TARGET',as_index=False).sum()['variable']\n        df_group['sum'] = df_group['TARGET'].apply(lambda x: sums[0] if x==0 else sums[1])\n        df_group['percent'] = df_group['variable']\/df_group['sum']\n        plt.subplot(r,c,i+1)\n        axa = sns.barplot(x = colname, y = 'percent', hue = 'TARGET',data = df_group)\n        plt.setp(axa.xaxis.get_majorticklabels(), rotation=-45)\n    plt.tight_layout()\n    plt.show()\n\ndef num_abberrant(df):\n    num_df = df.describe()\n    abb = []\n    for n in num_df.columns:\n        low = num_df[n]['mean']-3*num_df[n]['std']\n        up = num_df[n]['mean']+3*num_df[n]['std']\n        if num_df[n]['min'] < low or num_df[n]['max'] > up:\n            abb.append(n)\n    return num_df[abb]\n\n# all values in normalized data should be in [0,1]\ndef norm_abb(df):\n    norm_abb = []\n    for c in df.columns:\n        if df[c].dropna().between(0,1).all() == False:\n            print('Values of column ' + c + ' not in range [0,1].')\n            norm_abb.append(c)\n    return norm_abb\n\n# all values in 'time only relative to the application' data should be negative\ndef time_abb(df):\n    time_abb = []\n    for c in df.columns:\n        if df[c].dropna().le(0).all() == False:\n            print('Values of column ' + c + ' has value greater than 0.')\n            time_abb.append(c)\n    return time_abb\n\n# all values in 'rounded' data should be integer\ndef round_abb(df):\n    round_abb = []\n    for c in df.columns:\n        if df[c].dropna().dtype != int:\n            print('Values of column ' + c + ' has value greater than 0.')\n            round_abb.append(c)\n    return round_abb\n\ndef sns_distplot(df,cols,r,c):\n    plt.figure(figsize = (24,12))\n    for i in range(len(cols)):\n        plt.subplot(r,c,i+1)\n        sns.distplot(df[cols[i]].dropna())\n    plt.tight_layout()\n    plt.show()\n\ndef sns_distplot_target(df,cols,r,c,figsize):\n    plt.figure(figsize = figsize)\n    for i in range(len(cols)):\n        plt.subplot(r,c,i+1)\n        df_plot = df[['TARGET',cols[i]]].melt(['TARGET'],value_name=cols[i])\n        sns.distplot(df_plot[df_plot['TARGET']==0][cols[i]].dropna())\n        sns.distplot(df_plot[df_plot['TARGET']==1][cols[i]].dropna())\n    plt.tight_layout()\n    plt.show()\n\ndef NA_finder(df):\n    NA_f = df.isnull().sum() \n    NA_f = NA_f[NA_f != 0].sort_values(ascending=False)\n    NA_f_percent = NA_f.sort_values(ascending=False)\/len(df)*100.0\n    plt.figure(figsize=(20,20))\n    NA_f_percent.plot.bar()\n    plt.title('NA percentage distribution for NA containing features.')\n    plt.ylabel('Percentage (%)')\n    plt.show()\n    return NA_f_percent\n\ndef cat_plot_bureau(df,cols,r,c,figsize):\n    plt.figure(figsize=figsize)\n    for i in range(len(cols)):\n        colname = cols[i]\n        df_plot = df[['CREDIT_ACTIVE',colname]].dropna().melt(['CREDIT_ACTIVE'],value_name=colname)\n        df_group = df_plot.groupby(['CREDIT_ACTIVE',colname],as_index = False).count()\n        df_group['counts'] = df_group['variable']\n        plt.subplot(r,c,i+1)\n        axa = sns.barplot(x = colname, y = 'counts', hue = 'CREDIT_ACTIVE',data = df_group)\n        plt.setp(axa.xaxis.get_majorticklabels(), rotation=-45)\n    plt.tight_layout()\n    plt.show()\n    \ndef sns_distplot_bureau(df,cols,r,c,figsize):\n    plt.figure(figsize = figsize)\n    for i in range(len(cols)):\n        plt.subplot(r,c,i+1)\n        df_plot = df[['CREDIT_ACTIVE',cols[i]]].melt(['CREDIT_ACTIVE'],value_name=cols[i])\n        sns.distplot(df_plot[df_plot['CREDIT_ACTIVE']=='Active'][cols[i]].dropna())\n        sns.distplot(df_plot[df_plot['CREDIT_ACTIVE']=='Closed'][cols[i]].dropna())\n    plt.tight_layout()\n    plt.show()","50452060":"train = data['train']\ntrain.head()","db7464e2":"plt.hist(train['TARGET'])\nplt.show()","10246f93":"df = pd.concat([data['train'],data['test']])\ntrain_row = data['train'].shape[0]","993ee80e":"# summary of categorical features\ncats = cat_features(train)\ncat_levels = cat_levels(df,cats)\ncat_levels","74f5f3a3":"# find 'XNA'\ndf.replace('XNA',np.nan,inplace = True)\ntrain.replace('XNA',np.nan,inplace = True)\n# find binary features\nbin_cats = cats[cats == 2].index\nprint('binary categorical features are: ' + str(bin_cats))\nbin_nums = bin_num(df)\nprint('binary numerical features are: ' + str(bin_nums))","84aa2ba5":"cat_plot_target(train,cats.index,4,4,(15,15))","e494e863":"print(df['DAYS_EMPLOYED'][df['DAYS_EMPLOYED']>0].unique())","21c19982":"df['DAYS_EMPLOYED'].replace(365243,np.nan,inplace=True)\ntrain['DAYS_EMPLOYED'].replace(365243,np.nan,inplace=True)","a8356f12":"num_f = [f for f in df.columns if (f not in cats.index)]\nnum_f.remove('TARGET')\nnum_f.remove('SK_ID_CURR')\ndist_plot_f = [f for f in num_f if df[f].nunique()>50]\nprint(str(len(dist_plot_f)) + ' numerical features will be compared in the distribution plots.')\nsns_distplot_target(train,dist_plot_f,10,5,(24,24))","e9e2597a":"bar_plot_f = [f for f in num_f if f not in dist_plot_f]\nprint(str(len(bar_plot_f)) + ' numerical features will be compared in the percentage bar plots.')\ncat_plot_target(train,bar_plot_f,11,5,(24,48))","e5e9783e":"'EMERGENCYSTATE_MODE' in bar_plot_f","5a447720":"drop_f = ['NONLIVINGAPARTMENTS_MEDI', 'NONLIVINGAPARTMENTS_MODE', 'AMT_REQ_CREDIT_BUREAU_HOUR', 'AMT_REQ_CREDIT_BUREAU_WEEK','FLAG_CONT_MOBILE', 'FLAG_DOCUMENT_10', 'FLAG_DOCUMENT_11', 'FLAG_DOCUMENT_12', 'FLAG_DOCUMENT_15', 'FLAG_DOCUMENT_17', 'FLAG_DOCUMENT_19', 'FLAG_DOCUMENT_20', 'FLAG_DOCUMENT_21', 'FLAG_DOCUMENT_4', 'FLAG_DOCUMENT_5', 'FLAG_DOCUMENT_7', 'FLAG_DOCUMENT_9', 'FLAG_EMAIL', 'FLAG_MOBIL', 'LIVE_REGION_NOT_WORK_REGION', 'REG_REGION_NOT_LIVE_REGION']\ndf = df.drop(drop_f,axis=1)\ntrain = train.drop(drop_f,axis=1)","8e1e1db9":"NA_f = NA_finder(df)","f85bf9ec":"bb = data['bb']\nbb.head()","6e044ff6":"bb['STATUS'].unique()","e92a12ca":"bb_fresh = bb[bb['MONTHS_BALANCE'] == -1]\nsns.countplot(x=\"STATUS\", data=bb_fresh)","36ac77a0":"# length of balance history is a good feature\n# scoring of previous status is also a good one\ncount = bb[['SK_ID_BUREAU','MONTHS_BALANCE']].groupby('SK_ID_BUREAU').count().rename(columns = {'MONTHS_BALANCE':'HIST_LEN'}).reset_index()\nstatus = bb[bb['MONTHS_BALANCE'] == -1][['SK_ID_BUREAU','STATUS']]\nbb_join = pd.merge(count,status,on='SK_ID_BUREAU')\nbb_dum = pd.get_dummies(bb)\nbb_dum_sum = bb_dum.groupby('SK_ID_BUREAU',as_index=False).sum()\nweights = np.array([1,2,3,4,5])\n# score1 is the weighted sum of DPD status\nbb_dum_sum['BUREAU_SCORE1'] = (bb_dum_sum.iloc[:,2:7]*weights).sum(axis=1)\nbb_score = pd.merge(bb_dum_sum[['SK_ID_BUREAU','BUREAU_SCORE1']], bb_join, on='SK_ID_BUREAU')\n# score2 is the ratio of score1 and history length\nbb_score['BUREAU_SCORE2'] = bb_score['BUREAU_SCORE1']\/bb_score['HIST_LEN']\n# score3 is the reward score of not have any DPD\nbb_score['BUREAU_SCORE3'] = bb_score['BUREAU_SCORE2'].apply(lambda x: 1 if x==0 else 0)\nbb_score.head()","768bcc5e":"b = data['b']\ncredit_status_check = pd.merge(bb_fresh,b[['SK_ID_BUREAU','CREDIT_ACTIVE']],on = 'SK_ID_BUREAU',how = 'inner')\ncredit_status_check.head()","adae4ebc":"bureau = pd.merge(b,bb_score,on='SK_ID_BUREAU')\nbureau.head()","e3169605":"bureau_cats = cat_features(bureau)\nbureau_cats","d8da1fdf":"cat_plot_bureau(bureau,bureau_cats.index.drop('CREDIT_ACTIVE'),1,3,(15,5))","4bc82f3d":"bureau_active = bureau[bureau['CREDIT_ACTIVE'] == 'Active'].drop('CREDIT_ACTIVE',axis=1)\nbureau_closed = bureau[bureau['CREDIT_ACTIVE'] == 'Closed'].drop('CREDIT_ACTIVE',axis=1)","97d7b028":"bureau_active = bureau_active.drop([col for col in bureau_active.columns \n                                    if bureau_active[col].nunique(dropna=False) == 1],axis=1)\nprint(str(bureau.shape[1]-1-bureau_active.shape[1]) + ' columns dropped due to columns contain only 1 value.')\nbureau_closed = bureau_closed.drop([col for col in bureau_closed.columns \n                                    if bureau_closed[col].nunique(dropna=False) == 1],axis=1)\nprint(str(bureau.shape[1]-1-bureau_closed.shape[1]) + ' columns dropped due to columns contain only 1 value.')","9935d085":"bureau_active.isnull().sum()\/len(bureau_active)","103f56ac":"bureau_closed.isnull().sum()\/len(bureau_closed)","5b953d71":"bureau_active.drop('DAYS_ENDDATE_FACT',axis=1,inplace=True)","27e50c1d":"# no binary categorical variable, use one-hot-encoding\nbureau_active_dum = pd.get_dummies(bureau_active)\nbureau_closed_dum = pd.get_dummies(bureau_closed)\nbureau_active_dum.head()","ceae5d21":"bureau_f = bureau.drop(['SK_ID_CURR','SK_ID_BUREAU'],axis=1).columns\nbureau_num_f = [f for f in bureau_f if f not in bureau_cats.index]\nsns_distplot_bureau(bureau,bureau_num_f,3,6,(24,12))","b2665b9e":"bureau_agg = {\n    'SK_ID_BUREAU': ['count'],\n    'DAYS_CREDIT': ['min','max','mean','median','var'],\n    'CREDIT_DAY_OVERDUE': ['min','max'],\n    'DAYS_CREDIT_ENDDATE': ['min','max','mean'],\n    'AMT_CREDIT_MAX_OVERDUE': ['min','max','mean'],\n    'CNT_CREDIT_PROLONG': ['mean','sum'],\n    'AMT_CREDIT_SUM': ['sum','mean'],\n    'AMT_CREDIT_SUM_DEBT': ['sum','mean'],\n    'AMT_CREDIT_SUM_LIMIT': ['max','mean'],\n    'AMT_CREDIT_SUM_OVERDUE': ['sum'],\n    'DAYS_CREDIT_UPDATE': ['min','max'],\n    'AMT_ANNUITY': ['min','max','mean'],\n    'BUREAU_SCORE1': ['mean','min','max'],\n    'HIST_LEN': ['max','min','mean'],\n    'BUREAU_SCORE2': ['mean'],\n    'BUREAU_SCORE3': ['mean','max'],\n}","7f7666b8":"bureau_active_agg = {}\nbureau_active_agg.update(bureau_agg)\nfor col in bureau_active_dum.columns:\n    if col not in bureau_active_agg.keys():\n        bureau_active_agg.update({col:['mean']})\n\nbureau_closed_agg = {}\nbureau_closed_agg.update(bureau_agg)\nfor col in bureau_closed_dum.columns:\n    if col not in bureau_closed_agg.keys():\n        bureau_closed_agg.update({col:['mean']})\n\ndel bureau_agg","9bd5997a":"bureau_active_grouped = bureau_active_dum.groupby('SK_ID_CURR').agg(bureau_active_agg)\nbureau_active_grouped.columns = pd.Index(['BUREAU_ACTIVE_' + e[0] + '_' + e[1].upper() for e in bureau_active_grouped.columns.tolist()])\nbureau_closed_grouped = bureau_closed_dum.groupby('SK_ID_CURR').agg(bureau_closed_agg)\nbureau_closed_grouped.columns = pd.Index(['BUREAU_CLOSED_' + e[0] + '_' + e[1].upper() for e in bureau_closed_grouped.columns.tolist()])\nbureau_grouped = pd.merge(bureau_active_grouped,bureau_closed_grouped, left_index = True, right_index = True).reset_index()\nbureau_df = pd.merge(train[['SK_ID_CURR','TARGET']],bureau_grouped,on='SK_ID_CURR',how='left')","db1a4b53":"bureau_df.iloc[:,:4].head()","f1cbd77c":"plot_cols = bureau_df.columns[2:]\ncat_plot_f = [f for f in plot_cols if bureau_df[f].nunique()<50]\ncat_plot_target(bureau_df,cat_plot_f,11,5,(24,48))","7ec6d54c":"dist_plot_f = [f for f in plot_cols if f not in cat_plot_f]\nsns_distplot_target(bureau_df,dist_plot_f,15,5,(24,50))","a909978d":"drop_f = ['CREDIT_CURRENCY']\ndrop_dum_f = ['BUREAU_ACTIVE_CREDIT_TYPE_Another type of loan_MEAN',\n              'BUREAU_ACTIVE_CREDIT_TYPE_Cash loan (non-earmarked)_MEAN',\n              'BUREAU_ACTIVE_CREDIT_TYPE_Loan for business development_MEAN',\n              'BUREAU_ACTIVE_CREDIT_TYPE_Loan for the purchase of equipment_MEAN',\n              'BUREAU_ACTIVE_CREDIT_TYPE_Mobile operator loan_MEAN',\n              'BUREAU_ACTIVE_CREDIT_TYPE_Real estate loan_MEAN',\n              'BUREAU_ACTIVE_CREDIT_TYPE_Unknown type of loan_MEAN',\n              'BUREAU_ACTIVE_STATUS_2_MEAN',\n              'BUREAU_ACTIVE_STATUS_3_MEAN',\n              'BUREAU_ACTIVE_STATUS_4_MEAN',\n              'BUREAU_ACTIVE_STATUS_5_MEAN',\n              'BUREAU_CLOSED_CREDIT_DAY_OVERDUE_MIN',\n             'BUREAU_CLOSED_CREDIT_DAY_OVERDUE_MAX',\n             'BUREAU_CLOSED_CNT_CREDIT_PROLONG_MEAN',\n             'BUREAU_CLOSED_CNT_CREDIT_PROLONG_SUM',\n             'BUREAU_CLOSED_AMT_CREDIT_SUM_OVERDUE_SUM',\n             'BUREAU_CLOSED_CREDIT_TYPE_Another type of loan_MEAN',\n             'BUREAU_CLOSED_CREDIT_TYPE_Cash loan (non-earmarked)_MEAN',\n             'BUREAU_CLOSED_CREDIT_TYPE_Loan for business development_MEAN',\n             'BUREAU_CLOSED_CREDIT_TYPE_Loan for the purchase of equipment_MEAN',\n             'BUREAU_CLOSED_CREDIT_TYPE_Loan for working capital replenishment_MEAN',\n             'BUREAU_CLOSED_CREDIT_TYPE_Real estate loan_MEAN',\n             'BUREAU_CLOSED_CREDIT_TYPE_Unknown type of loan_MEAN',\n             'BUREAU_CLOSED_STATUS_1_MEAN',\n             'BUREAU_CLOSED_STATUS_2_MEAN',\n             'BUREAU_CLOSED_STATUS_3_MEAN',\n             'BUREAU_CLOSED_STATUS_4_MEAN',\n             'BUREAU_CLOSED_STATUS_5_MEAN',\n              'BUREAU_CLOSED_DAYS_CREDIT_ENDDATE_MAX'\n             ]","b0c1a0b1":"credit = data['ccb']\ncredit.head()","d872f51b":"credit_cats = cat_features(credit)\ncredit_num_f = credit.iloc[:,2:].drop(credit_cats.index,axis=1).columns\n","5bb68c41":"sns_distplot(credit,credit_num_f,4,5)","ce3fad91":"For the features created by one-hot-encodeing, I think taking the mean is reasonable.","41f1dab6":"From the distribution plots, I think all above features should be retained in the dataset. Here are the aggregation functions I decided after observing each feature:","2733c907":"__2. categorical features__<br\/>\nThe nunique function has dropna=True as default, which is not what I want. I want to find the binary features for label encoding, if it contains NAN values, then OneHotEncoder will be more useful here.","89873d4a":"Like bureau data, 1 SK_ID_CURR has multiple SK_ID_PREV, aggregation functions need to be assigned. From the above exploration, I have the following aggregation functions:","ea938773":"To further investigate the categorical features' effect on target, I plotted the percentage distribution on different TARGET conditions for all categorical features.","6bd0efc7":"__2. Categorical variables__","740222e9":"__3. NA values__","13e65212":"The categroical features countplots look very different for active and closed accounts. So I think it's necessary to analyze them seperately when apply feature enigneering.","7776a663":"__6. Aggregation function exploration__","644e8290":"I find for some feature contains a lot of NA values, a good way to impute these data is important since the goal for this project is to help people with few credit history.","84de36a7":"__1. Overview__ <br\/>\nBureau balance and bureau data are connected tightly, I will merge the two tables together to perform EDA. Before merging, I found like bureau balance data, bureau data has a column showing the status of account, I want to see if they show the same information.","994e19b4":"So these two columns are actually different information, I cannot combine the two information together.","05b8529d":"# Bureau balance data","b586d96c":"__1. target distribution__","44a4cf6f":"From the above paired plots, I find 'HOUSETYPE_MODE' feature value of 'block of flats' and 'EMERGENCYSTATE_MODE' feature do not affect the TARGET value. So I will clean them from my dataset for modeling.<br\/>\n<br\/>\n__3. numerical features__<br\/>\nI first look at description table and try to group different numerical features.","cd7e56be":"__3. data split into two sets based on 'CREDIT_ACTIVE' feature__<br\/>\nThe 'CREDIT_ACTIVE' is an important feature when we consider historical data. The number of bureau records that are still active or already closed can affect other features as well. So I want to examine if 'CREDIT_ACTIVE' condition can affect a lot of other feature distributions.","074b4bf7":"From the distribution and bar plots, I found the following features have almost the same distribution for both target values and will be cleaned in my dataset for modeling:\n            ['COMMONAREA_MEDI','COMMONAREA_MODE','LANDAREA_MEDI','LANDAREA_MODE','LIVINGAPARTMENTS_MEDI',\n              'LIVINGAPARTMENTS_MODE','LIVINGAREA_MEDI','LIVINGAREA_MODE','NONLIVINGAPARTMENTS_MEDI',\n              'NONLIVINGAPARTMENTS_MODE','NONLIVINGAREA_MEDI','NONLIVINGAREA_MODE','YEARSBEGINEXPLUATATION_MEDI',\n                'YEARSBEGINEXPLUATATION_MODE','YEARSBUILD_MEDI','YEARSBUILD_MODE', 'AMT_REQ_CREDIT_BUREAU_HOUR', \n              'AMT_REQ_CREDIT_BUREAU_WEEK','DEF_60_CNT_SOCIAL_CIRCLE','ELEVATORS_MODE',\n              'FLAG_CONT_MOBILE', 'FLAG_DOCUMENT_10', 'FLAG_DOCUMENT_11', \n              'FLAG_DOCUMENT_12', 'FLAG_DOCUMENT_15', 'FLAG_DOCUMENT_17', 'FLAG_DOCUMENT_19', 'FLAG_DOCUMENT_20', \n              'FLAG_DOCUMENT_21', 'FLAG_DOCUMENT_4', 'FLAG_DOCUMENT_5', 'FLAG_DOCUMENT_7', 'FLAG_DOCUMENT_9', \n              'FLAG_EMAIL', 'FLAG_MOBIL', 'FLOORSMAX_MODE','FLOORSMIN_MODE','OBS_60_CNT_SOCIAL_CIRCLE',\n              'LIVE_REGION_NOT_WORK_REGION', 'REG_REGION_NOT_LIVE_REGION']","d15a0492":"Since each SK_ID_CURR has multiple SK_BUREAU_ID, I will need aggregation function for different features. I first examine the characteristics of different features by looking at the description table.<br\/>\nTo apply aggregation function, I should first encode the categorical variables.","c30bf20d":"The label looks highly imbalanced. Use ROC AUC score as performance metric will be helpful for model validation. To perform feature engineering on training and testing data together, I concatened the two datasets.","eb3b29a0":"# Credit card balance","21145047":"# Bureau data","9730ed85":"The bureau balance data contains one numerical data and one categorical data, and this two data seems to be related. According to the description table, 'MONTHS_BALANCE' == -1 means the freshest balance date.","6417caee":"# Training and testing dataset","78f13831":"Unfortunately, the 'DAYS_ENDDATE_FACT' column is not found by nunique method. I have to mannually remove this column. In case of the samething happen, I first looked at the NA value distribution in all variable.","a192cac5":"__4. Single value column exploration__<br\/>\nThe 'DAYS_ENDDATE_FACT' feature only has value in closed bureau credit account (in feature description). I need to find features like this and delete them.","a30dd4f4":"######### Summary feature engineering for train\/test dataset\n\ndef traintest_processing(df):\n    df.replace('XNA',np.nan,inplace = True)\n    df['DAYS_EMPLOYED'].replace(365243,np.nan,inplace=True)\n    bin_f = bin_finder(df)\n    for b in bin_f:\n        le.fit(df[b])\n        df[b] = le.transform(df[b])\n    def df['EMERGENCYSTATE_MODE']\n    drop_f = ['NONLIVINGAPARTMENTS_MEDI', 'NONLIVINGAPARTMENTS_MODE', 'AMT_REQ_CREDIT_BUREAU_HOUR', \n              'AMT_REQ_CREDIT_BUREAU_WEEK','FLAG_CONT_MOBILE', 'FLAG_DOCUMENT_10', 'FLAG_DOCUMENT_11', \n              'FLAG_DOCUMENT_12', 'FLAG_DOCUMENT_15', 'FLAG_DOCUMENT_17', 'FLAG_DOCUMENT_19', 'FLAG_DOCUMENT_20', \n              'FLAG_DOCUMENT_21', 'FLAG_DOCUMENT_4', 'FLAG_DOCUMENT_5', 'FLAG_DOCUMENT_7', 'FLAG_DOCUMENT_9', \n              'FLAG_EMAIL', 'FLAG_MOBIL', 'LIVE_REGION_NOT_WORK_REGION', 'REG_REGION_NOT_LIVE_REGION']\n    df = df.drop(drop_f,axis=1)\n    df_dum = pd.get_dummies(df)\n    ### new features learnt from other kernels\n    df_dum['DAYS_EMPLOYED_PERC'] = df_dum['DAYS_EMPLOYED'] \/ df_dum['DAYS_BIRTH']\n    df_dum['INCOME_CREDIT_PERC'] = df_dum['AMT_INCOME_TOTAL'] \/ df_dum['AMT_CREDIT']\n    df_dum['INCOME_PER_PERSON'] = df_dum['AMT_INCOME_TOTAL'] \/ df_dum['CNT_FAM_MEMBERS']\n    df_dum['ANNUITY_INCOME_PERC'] = df_dum['AMT_ANNUITY'] \/ df_dum['AMT_INCOME_TOTAL']\n    del df_dum['HOUSETYPE_MODE_block of flats']\n    return df_dum","d3b057a5":"To make good assumption about the features and target value, I will use the training set data for correlation analysis.","6ef36752":"From the above information, I find only 'DAYS_ENDDATE_FACT' should be removed from bureau_active.","c85fb2d5":"By examing the categorical features, I found some categorical data use 'XNA' instead of np.nan as NA values. So next I will replace 'XNA' as np.nan values in both train and test set. I also found 'WEEKDAY_APPR_PROCESS_START' is a time feature that may be encoded by LabelEncoder, but I still chosed to use OneHotEncoding to avoid possible missing feature characteristics.","33a3c28a":"__5. 'ground truth' validation__<br\/>\nLike when exploring the training and test set data, I will check for some ground truth restrictions.","3b20491b":"The 'DAYS_CREDIT_ENDDATE' feature has 46039 values above 0, the description of this feature was clear about that it was the date decided by the loan company. I will still keep the positive values."}}