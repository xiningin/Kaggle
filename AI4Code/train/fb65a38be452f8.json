{"cell_type":{"48a01b40":"code","2880c2e7":"code","92fd1364":"code","f7bb7e46":"code","e194452b":"code","3ead6449":"code","44883011":"code","07e42581":"code","016486c5":"markdown","aea50470":"markdown","0e6bd7f9":"markdown","20f58e7a":"markdown","945baf81":"markdown","646d0bc2":"markdown","a2c39b92":"markdown","d948c9b6":"markdown","4b60a447":"markdown","c17789fd":"markdown","433759e0":"markdown"},"source":{"48a01b40":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","2880c2e7":"import matplotlib.pyplot as plt\nimport seaborn as sns\n\ndf = pd.read_csv('\/kaggle\/input\/gamestop-historical-stock-prices\/GME_stock.csv')\nprint(plt.style.available) # look at available plot styles\nplt.style.use('ggplot')","92fd1364":"x = np.array(df.loc[:,'open_price']).reshape(-1,1)\ny = np.array(df.loc[:,'high_price']).reshape(-1,1)\n\nplt.figure(figsize=[10,10])\nplt.scatter(x, y)\nplt.xlabel('open_price')\nplt.ylabel('high_price')\nplt.show()","f7bb7e46":"from sklearn.linear_model import LinearRegression\n\nlinear_regression = LinearRegression()\nlinear_regression.fit(x,y)\n","e194452b":"y_head = linear_regression.predict(x)\n\nplt.scatter(x, y)\nplt.plot(x, y_head, color='green', linewidth=3)\nplt.xlabel('open_price')\nplt.ylabel('high_price')\nplt.show()\n","3ead6449":"from sklearn.metrics import r2_score\nprint(\"r_square score = \", r2_score(y,y_head))","44883011":"print(\"Prediction of high_price : \", linear_regression.predict([[350]]))","07e42581":"print(\"Prediction of high_price : \", linear_regression.predict([[354.8299865722656]]))","016486c5":"# Linear Regression\n\nFirst we need to import necessary libraries to get dataset which will be used. Kaggle does it automatically for us. ","aea50470":"We can check also this model by giving an input over real values and compare with each other. Let's see how *open_price = 354.8299865722656* value on this dataset give a result when go this regression model.","0e6bd7f9":"We can see there might be a linearity to a large extent as far as it is observed. But it is too early to say for sure. To be sure, it is necessary to create and fit the linear regression model. And then we need to calculate how accurately it is fitted. \n* Linear regression : y = ax + b where y = target, x = feature and a = parameter of model\n\nTo do this we should import *sklearn.linear_model* library to create and fit the linear regression model.","20f58e7a":"\n*Residual* is the name given to the difference between the *y* real values and the *y_head* prediction values. But we can have negative residuals and positive *residuals* both and this may cause the values to reset each other if we sum up all. That's why we need to sum square of *residuals* up and divide into sample count (n) to get *Min Squared Error*.\n\nIn these algorithms our main purpose is to reach minimum *Min Squared Error* value.\n* Min Squared Error = sum((residual)^2) \/ n -> n = sample count\n\nNow we need to calculate R^2 scores which shows how accurately the line is fitted on real values. The closer this value is to 1, the better this model is fitted.,\n* residual = y-y_head\n* square_residual = (residual)^2\n* sum_square_residual = sum((residual)^2) -> *we call it here SSR*\n* sum_square_total = sum((y-Yavg)^2) -> *we call it here SST*\n* R-square = 1 - (SSR\/SST)\n\nActually we don't need to do these calculations manually one by one. There are ready-made libraries for this process in Python.","945baf81":"Now, we specify the x and y axes. X means my features, and Y is my target values. According to that features, target values vary. In this scenario as the x-value increases, so does the y-value.\n\nLet's visualize this by plotting it.","646d0bc2":"Looking the dataset, we can see the real value of 380.0, which is very close to the prediction.","a2c39b92":"By giving these *open_price* values to the regression model we need to determine the predictions. We call it \"*y_head*\" . Later on, we need to visualize the fitted line on the real values.\n","d948c9b6":"Next, we should read the *.csv* file using *pandas* and assign it to a dataframe variable. And then we should import *matplotlib.pyplot* library for visualizing the data.","4b60a447":"# **Regression Model on GameStop Historical Stock Prices Dataset**\n\nWithin the framework of Machine Learning algorithms, in this study i took the dataset and worked on it using regression algorithm. I used Linear Regression and observed r-square score. ","c17789fd":"As it seems, this result is very close to \"1\". Now we can make a prediction how much \"*high_price*\" value is when chosen specific \"*open_price*\". \n\nTo give an example: *open_price* = 350 so what can be the *high_price*?","433759e0":"**Gamestop Historical Stock Prices Dataset**\n\nThe dataset contains a lof of fields but as far as i observed two of them are increasing linearly. I can choose 2 columns to implement on Regression model, The more open-price of the stock the more high-price of that day :\n\nOpen_price: The opening price of the stock\n\nHigh_price: The high price of that day\n\n"}}