{"cell_type":{"32892135":"code","7435f682":"code","d1775296":"code","7a7a27c3":"code","a6e21661":"code","b82e1e08":"code","20f1583f":"code","760189f1":"code","067df63c":"code","48c0ed15":"code","6487d322":"code","7c440944":"code","2d3fb7ee":"code","d0441434":"code","0c6aff70":"code","dc53d20d":"code","9d43ccd9":"code","578fd446":"code","881ca776":"code","52509feb":"code","1125db04":"code","6b7cbd13":"code","7adfd668":"code","f9653807":"code","bc371863":"code","be89ecd9":"code","b223ed3e":"code","d2d507db":"code","6e072b38":"code","e0276dde":"code","2ebde205":"code","024c172d":"code","a8c3bb9f":"code","9d9cf283":"code","b2a95293":"code","ee9692d9":"code","5e520c13":"code","6b8aa450":"code","42973460":"code","62faf427":"code","e8f162ed":"code","d4e2b2ba":"code","43056cbd":"code","0821cd88":"code","777a17de":"code","8bc8f94f":"code","ff8d6ac4":"markdown","3f45a981":"markdown","1353a695":"markdown","db18befc":"markdown","9f01db1f":"markdown","a3c0f49a":"markdown","8aa74dd3":"markdown","fceb71a8":"markdown","635d4b1c":"markdown","90d7cb1f":"markdown","b5fc0e0f":"markdown","643828cd":"markdown","cc238de2":"markdown","d0c1349d":"markdown","54b1543b":"markdown","c3a53b7a":"markdown","dc448ef0":"markdown","56484390":"markdown","613a9b46":"markdown","e5a2c47f":"markdown","be72c884":"markdown","9c8ba2f1":"markdown","59ce8332":"markdown","445de367":"markdown","55923b9d":"markdown"},"source":{"32892135":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","7435f682":"import matplotlib.pyplot as plt\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import cohen_kappa_score\nfrom scipy.stats import mode\nfrom sklearn.feature_selection import SelectFromModel\nfrom sklearn.model_selection import train_test_split\n\nimport lightgbm as lgb\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import KFold\nfrom tqdm import tqdm","d1775296":"%%time\ntrain=pd.read_csv('..\/input\/data-science-bowl-2019\/train.csv')\ntrain_labels=pd.read_csv('..\/input\/data-science-bowl-2019\/train_labels.csv')\ntest = pd.read_csv('..\/input\/data-science-bowl-2019\/test.csv')\nsubmission=pd.read_csv('..\/input\/data-science-bowl-2019\/sample_submission.csv')","7a7a27c3":"from sklearn.base import BaseEstimator, TransformerMixin\ndef qwk(a1, a2):\n    \"\"\"\n    Source: https:\/\/www.kaggle.com\/c\/data-science-bowl-2019\/discussion\/114133#latest-660168\n\n    :param a1:\n    :param a2:\n    :param max_rat:\n    :return:\n    \"\"\"\n    max_rat = 3\n    a1 = np.asarray(a1, dtype=int)\n    a2 = np.asarray(a2, dtype=int)\n\n    hist1 = np.zeros((max_rat + 1, ))\n    hist2 = np.zeros((max_rat + 1, ))\n\n    o = 0\n    for k in range(a1.shape[0]):\n        i, j = a1[k], a2[k]\n        hist1[i] += 1\n        hist2[j] += 1\n        o +=  (i - j) * (i - j)\n\n    e = 0\n    for i in range(max_rat + 1):\n        for j in range(max_rat + 1):\n            e += hist1[i] * hist2[j] * (i - j) * (i - j)\n\n    e = e \/ a1.shape[0]\n\n    return 1 - o \/ e","a6e21661":"not_req=(set(train.installation_id.unique()) - set(train_labels.installation_id.unique()))\ntrain_new=~train['installation_id'].isin(not_req)\ntrain.where(train_new,inplace=True)\ntrain.dropna(inplace=True)\ntrain['event_code']=train.event_code.astype(int)","b82e1e08":"def extract_time_features(df):\n    df['timestamp'] = pd.to_datetime(df['timestamp'])\n    return df\nextract_time_features(train)\nextract_time_features(test)","20f1583f":"# encode title\n# make a list with all the unique 'titles' from the train and test set\nlist_of_user_activities = list(set(train['title'].value_counts().index).union(set(test['title'].value_counts().index)))\n# make a list with all the unique 'event_code' from the train and test set\nlist_of_event_code = list(set(train['event_code'].value_counts().index).union(set(test['event_code'].value_counts().index)))\n# create a dictionary numerating the titles\nactivities_map = dict(zip(list_of_user_activities, np.arange(len(list_of_user_activities))))\nactivities_labels = dict(zip(np.arange(len(list_of_user_activities)), list_of_user_activities))\n\nassess_titles = list(set(train[train['type'] == 'Assessment']['title'].value_counts().index).union(set(test[test['type'] == 'Assessment']['title'].value_counts().index)))\n\n# replace the text titles withing the number titles from the dict\ntrain['title'] = train['title'].map(activities_map)\ntest['title'] = test['title'].map(activities_map)\ntrain_labels['title'] = train_labels['title'].map(activities_map)","760189f1":"from sklearn.preprocessing import LabelEncoder\nencoder = LabelEncoder()\ndata = pd.concat([train,test],sort=False)\n\ndata['installation_id_encoder'] = 0\nencode = data[['installation_id']].apply(encoder.fit_transform)\ndata['installation_id_encoder'] = encode\ndata.head(10)\n\ntrain = data[:len(train)]\ntest = data[len(train):]","067df63c":"win_code = dict(zip(activities_map.values(), (4100*np.ones(len(activities_map))).astype('int')))\n# then, it set one element, the 'Bird Measurer (Assessment)' as 4110, 10 more than the rest\nwin_code[activities_map['Bird Measurer (Assessment)']] = 4110","48c0ed15":"# this is the function that convert the raw data into processed features\ndef get_data(user_sample, test_set=False):\n    '''\n    The user_sample is a DataFrame from train or test where the only one \n    installation_id is filtered\n    And the test_set parameter is related with the labels processing, that is only requered\n    if test_set=False\n    '''\n    # Constants and parameters declaration\n    last_activity = 0\n    user_activities_count = {'Clip':0, 'Activity': 0, 'Assessment': 0, 'Game':0}\n    \n    # news features: time spent in each activity\n    time_spent_each_act = {actv: 0 for actv in list_of_user_activities}\n    event_code_count = {eve: 0 for eve in list_of_event_code}\n    last_session_time_sec = 0\n    \n    accuracy_groups = {0:0, 1:0, 2:0, 3:0}\n    all_assessments = []\n    accumulated_accuracy_group = 0\n    accumulated_accuracy=0\n    accumulated_correct_attempts = 0 \n    accumulated_uncorrect_attempts = 0 \n    accumulated_actions = 0\n    \n    last_accuracy_title = {'acc_' + title: -1 for title in assess_titles}\n    last_game_time_title = {'lgt_' + title: 0 for title in assess_titles}\n    ac_game_time_title = {'agt_' + title: 0 for title in assess_titles}\n    ac_true_attempts_title = {'ata_' + title: 0 for title in assess_titles}\n    ac_false_attempts_title = {'afa_' + title: 0 for title in assess_titles}\n    \n    counter = 0\n    time_first_activity = float(user_sample['timestamp'].values[0])\n    durations = []\n    time_play = 0\n    title_just_before = 0\n    title_assessment_before = 0\n    assessment_before_accuracy = 0\n    dif2030 = 0\n    dif4070 = 0\n    dif3010 = 0\n    dif3020 = 0\n    dif4030 = 0\n    dif3110 = 0\n    dif4025 = 0\n    dif4035 = 0\n    dif3120 = 0\n    dif2010 = 0\n    somme_clip_game_activity = 0\n    \n    # itarates through each session of one instalation_id\n    for i, session in user_sample.groupby('game_session', sort=False):\n        # i = game_session_id\n        # session is a DataFrame that contain only one game_session\n        \n        # get some sessions information\n        session_type = session['type'].iloc[0]\n        session_title = session['title'].iloc[0]\n        \n        # get current session time in seconds\n        if session_type != 'Assessment':\n            time_spent = int(session['game_time'].iloc[-1] \/ 1000)\n            time_spent_each_act[activities_labels[session_title]] += time_spent\n            time_play += time_spent\n            \n            title_just_before = session_title\n            session_title_text = activities_labels[session_title]\n            \n        # for each assessment, and only this kind off session, the features below are processed\n        # and a register are generated\n        if (session_type == 'Assessment') & (test_set or len(session)>1):\n            # search for event_code 4100, that represents the assessments trial\n            all_attempts = session.query(f'event_code == {win_code[session_title]}')\n            # then, check the numbers of wins and the number of losses\n            true_attempts = all_attempts['event_data'].str.contains('true').sum()\n            false_attempts = all_attempts['event_data'].str.contains('false').sum()\n            # copy a dict to use as feature template, it's initialized with some itens: \n            # {'Clip':0, 'Activity': 0, 'Assessment': 0, 'Game':0}\n            features = user_activities_count.copy()\n            \n            features.update(last_accuracy_title.copy())\n            features.update(last_game_time_title.copy())\n            features.update(ac_game_time_title.copy())\n            features.update(ac_true_attempts_title.copy())\n            features.update(ac_false_attempts_title.copy())\n            \n            session_title = session['title'].iloc[0]\n            time_spent = int(session['game_time'].iloc[-1] \/ 1000)\n            time_spent_each_act[activities_labels[session_title]] += time_spent\n            \n            features.update(time_spent_each_act.copy())\n            features.update(event_code_count.copy())\n            # add title as feature, remembering that title represents the name of the game\n            features['session_title'] = session['title'].iloc[0] \n            # the 4 lines below add the feature of the history of the trials of this player\n            # this is based on the all time attempts so far, at the moment of this assessment\n            features['accumulated_correct_attempts'] = accumulated_correct_attempts\n            features['accumulated_uncorrect_attempts'] = accumulated_uncorrect_attempts\n            accumulated_correct_attempts += true_attempts \n            accumulated_uncorrect_attempts += false_attempts\n            \n            session_title_text = activities_labels[session_title]\n            ac_true_attempts_title['ata_' + session_title_text] += true_attempts\n            ac_false_attempts_title['afa_' + session_title_text] += false_attempts\n            last_game_time_title['lgt_' + session_title_text] = session['game_time'].iloc[-1]\n            ac_game_time_title['agt_' + session_title_text] += session['game_time'].iloc[-1]\n            \n            # the time spent in the app so far\n            if durations == []:\n                features['duration_mean'] = 0\n            else:\n                features['duration_mean'] = np.mean(durations)\n            durations.append((session.iloc[-1, 2] - session.iloc[0, 2] ).seconds)\n            \n            accuracy = true_attempts\/(true_attempts+false_attempts) if (true_attempts+false_attempts) != 0 else 0\n            accumulated_accuracy += accuracy\n            last_accuracy_title['acc_' + session_title_text] = accuracy\n            # a feature of the current accuracy categorized\n            # it is a counter of how many times this player was in each accuracy group\n            \n            # assessment_before_accuracy\n            features['assessment_before_accuracy'] = assessment_before_accuracy\n                     \n            if accuracy == 0:\n                features['accuracy_group'] = 0\n                assessment_before_accuracy = 0\n            elif accuracy == 1:\n                features['accuracy_group'] = 3\n                assessment_before_accuracy = 3\n            elif accuracy == 0.5:\n                features['accuracy_group'] = 2\n                assessment_before_accuracy = 2\n            else:\n                features['accuracy_group'] = 1\n                assessment_before_accuracy = 1\n                \n            features.update(accuracy_groups)\n            accuracy_groups[features['accuracy_group']] += 1\n            # mean of the all accuracy groups of this player\n            features['accumulated_accuracy_group'] = accumulated_accuracy_group\/counter if counter > 0 else 0\n            accumulated_accuracy_group += features['accuracy_group']\n            # how many actions the player has done so far, it is initialized as 0 and updated some lines below\n            features['accumulated_actions'] = accumulated_actions\n            \n            # encode installation_id\n            features['installation_id_encoder'] = session['installation_id_encoder'].iloc[0]\n            \n            # time play on the app\n            features['time_play'] = time_play\n            time_play += int(session['game_time'].iloc[-1] \/ 1000)\n            \n            # title_assessment_before\n            features['title_assessment_before'] = title_assessment_before\n            \n            # concat (session_title + title_assessment_before) \/ title_assessment_before is the title of the previous assessment :\n            features['title*title_assessment_before'] = int( str(session['title'].iloc[0]) + str(title_assessment_before) )\n            title_assessment_before = session['title'].iloc[0]\n            \n            # concat (session_title + title_just_before) \/ title_just_before is title game play just before the assessment :\n            features['title*title_just_before'] = int( str(session['title'].iloc[0]) + str(title_just_before) )\n            title_just_before = session['title'].iloc[0]\n            \n            \n            # 4070 dif\n            if features['Assessment'] == 0:\n                features['4070_dif'] = features[4070]\n                dif4070 = features[4070]\n            else:\n                features['4070_dif'] = features[4070] - dif4070\n                dif4070 = features[4070]\n                \n            # 2030 dif\n            if features['Assessment'] == 0:\n                features['2030_dif'] = features[2030]\n                dif2030 = features[2030]\n            else:\n                features['2030_dif'] = features[2030] - dif2030\n                dif2030 = features[2030]\n                \n            # 3010 dif\n            if features['Assessment'] == 0:\n                features['3010_dif'] = features[3010]\n                dif3010 = features[3010]\n            else:\n                features['3010_dif'] = features[3010] - dif3010\n                dif3010 = features[3010]\n                \n            # 3020 dif\n            if features['Assessment'] == 0:\n                features['3020_dif'] = features[3020]\n                dif3020 = features[3020]\n            else:\n                features['3020_dif'] = features[3020] - dif3020\n                dif3020 = features[3020]\n                \n            # 4030 dif\n            if features['Assessment'] == 0:\n                features['4030_dif'] = features[4030]\n                dif4030 = features[4030]\n            else:\n                features['4030_dif'] = features[4030] - dif4030\n                dif4030 = features[4030]\n                \n            # 3110 dif\n            if features['Assessment'] == 0:\n                features['3110_dif'] = features[3110]\n                dif3110 = features[3110]\n            else:\n                features['3110_dif'] = features[3110] - dif3110\n                dif3110 = features[3110]\n                \n            # 4035 dif\n            if features['Assessment'] == 0:\n                features['4035_dif'] = features[4035]\n                dif4035 = features[4035]\n            else:\n                features['4035_dif'] = features[4035] - dif4035\n                dif4035 = features[4035]\n                \n            # 4025 dif\n            if features['Assessment'] == 0:\n                features['4025_dif'] = features[4025]\n                dif4025 = features[4025]\n            else:\n                features['4025_dif'] = features[4025] - dif4025\n                dif4025 = features[4025]\n                \n            # 3120 dif\n            if features['Assessment'] == 0:\n                features['3120_dif'] = features[3120]\n                dif3120 = features[3120]\n            else:\n                features['3120_dif'] = features[3120] - dif3120\n                dif3120 = features[3120]\n                \n            # 2010 dif\n            if features['Assessment'] == 0:\n                features['2010_dif'] = features[2010]\n                dif2010 = features[2010]\n            else:\n                features['2010_dif'] = features[2010] - dif2010\n                dif2010 = features[2010]\n                \n                \n            # time play assessment\n            features['time_play_assessment'] = sum(durations)\n            \n            # clip+game+activity before assessment\n            somme = features['Clip']+features['Game']+features['Activity']\n            features['somme_clip_game_activity'] = somme - somme_clip_game_activity\n            somme_clip_game_activity = somme\n            \n            # there are some conditions to allow this features to be inserted in the datasets\n            # if it's a test set, all sessions belong to the final dataset\n            # it it's a train, needs to be passed throught this clausule: session.query(f'event_code == {win_code[session_title]}')\n            # that means, must exist an event_code 4100 or 4110\n            if test_set:\n                all_assessments.append(features)\n            elif true_attempts+false_attempts > 0:\n                all_assessments.append(features)\n                \n            counter += 1\n        \n        # this piece counts how many actions was made in each event_code so far\n        n_of_event_codes = Counter(session['event_code'])\n        \n        for key in n_of_event_codes.keys():\n            event_code_count[key] += n_of_event_codes[key]\n\n        # counts how many actions the player has done so far, used in the feature of the same name\n        accumulated_actions += len(session)\n        if last_activity != session_type:\n            user_activities_count[session_type] += 1\n            last_activitiy = session_type\n    # if it't the test_set, only the last assessment must be predicted, the previous are scraped\n    if test_set:\n        return all_assessments[-1]\n    # in the train_set, all assessments goes to the dataset\n    return all_assessments[:-1]","6487d322":"from tqdm import tqdm_notebook as tqdm\nfrom collections import Counter\n# here the get_data function is applyed to each installation_id and added to the compile_data list\ncompiled_data = []\ncompiled_data_last = []\n# tqdm is the library that draws the status bar below\nfor i, (ins_id, user_sample) in tqdm(enumerate(train.groupby('installation_id', sort=False)), total=3614):\n    # user_sample is a DataFrame that contains only one installation_id\n    L = get_data(user_sample)\n    compiled_data += L\n    a = get_data(user_sample,test_set=True )\n    compiled_data_last.append(a)\n\n# the compiled_data is converted to DataFrame and deleted to save memmory\nfinal_train = pd.DataFrame(compiled_data)","7c440944":"final_train_last = pd.DataFrame(compiled_data_last)","2d3fb7ee":"name_colonne = list(final_train.iloc[:,4:29])","d0441434":"pd.set_option('display.max_columns', None)\nfinal_train","0c6aff70":"final_train = final_train[~final_train.installation_id_encoder.isin([2572,4351,2901,3064,3759,2012,3804,1406,2055,2096,3766,3347,3994,4209,4444])]\nfinal_train_last = final_train_last[~final_train_last.installation_id_encoder.isin([2572,4351,2901,3064,3759,2012,3804,1406,2055,2096,3766,3347,3994,4209,4444])]","dc53d20d":"# add a column index_t helping the separation between last assessment of final_train and others assessment\nfinal_train = final_train.reset_index()\nfinal_train.rename(columns={'index':'index_t'},inplace=True)","9d43ccd9":"# every last assessment of installation_id from final_train\nfinal_train_last2 = final_train.groupby('installation_id_encoder', sort=False,as_index=False).last()","578fd446":"final_train_last2","881ca776":"not_req=(set(final_train.index_t.unique()) - set(final_train_last2.index_t.unique()))\nfinal_train2=final_train['index_t'].isin(not_req)\nfinal_train2 = final_train.where(final_train2,try_cast=True)\nfinal_train2.dropna(inplace=True)\nfinal_train2['index_t']=final_train2.index.astype(int)\n\ncolonne = list(final_train2)\ncolonne_float = ['accumulated_accuracy_group','duration_mean','accumulated_accuracy']\nfor name in colonne:\n    if name not in colonne_float:\n        final_train2[name] = final_train2[name].astype(int)","52509feb":"final_train2","1125db04":"final_train_last3 = final_train2.groupby('installation_id_encoder', sort=False,as_index=False).last()","6b7cbd13":"not_req=(set(final_train2.index_t.unique()) - set(final_train_last3.index_t.unique()))\nfinal_train3=final_train2['index_t'].isin(not_req)\nfinal_train3 = final_train2.where(final_train3,try_cast=True)\nfinal_train3.dropna(inplace=True)\nfinal_train3['index_t']=final_train3.index.astype(int)\n\ncolonne = list(final_train3)\ncolonne_float = ['accumulated_accuracy_group','duration_mean','accumulated_accuracy']\nfor name in colonne:\n    if name not in colonne_float:\n        final_train3[name] = final_train3[name].astype(int)","7adfd668":"final_train = final_train.drop(['index_t'],1)\nfinal_train2 = final_train2.drop(['index_t'],1)\nfinal_train3 = final_train3.drop(['index_t'],1)\nfinal_train_last2 = final_train_last2.drop(['index_t'],1)\nfinal_train_last3 = final_train_last3.drop(['index_t'],1)","f9653807":"# process test set, the same that was done with the train set\nnew_test = []\ntest_to_train = []\nfor ins_id, user_sample in tqdm(test.groupby('installation_id', sort=False), total=1000):\n    L = get_data(user_sample)\n    a = get_data(user_sample,test_set=True )\n    new_test.append(a)\n    test_to_train += L\n    \nfinal_test = pd.DataFrame(new_test)\ntest_to_train = pd.DataFrame(test_to_train)\nfinal_test.shape","bc371863":"final_train = pd.concat([final_train,test_to_train])\nfinal_train = final_train.reset_index(drop=True)\nfinal_train2 = pd.concat([final_train2,test_to_train])\nfinal_train2 = final_train2.reset_index(drop=True)\nfinal_train3 = pd.concat([final_train3,test_to_train])\nfinal_train3 = final_train3.reset_index(drop=True)","be89ecd9":"keep = ['accuracy_group','session_title','accumulated_accuracy_group','4070_dif','2030_dif',\n        'duration_mean','4030_dif','accumulated_uncorrect_attempts','Chow Time','Clip',\n        'somme_clip_game_activity','assessment_before_accuracy','accumulated_actions',0,3] + name_colonne\nfinal_train = final_train[keep]\nfinal_train_last = final_train_last[keep]\nfinal_train2 = final_train2[keep]\nfinal_train_last2 = final_train_last2[keep]\nfinal_train3 = final_train3[keep]\nfinal_train_last3 = final_train_last3[keep]\nfinal_test = final_test[keep]","b223ed3e":"print(final_train_last.shape)\nprint(final_train_last2.shape)\nprint(final_train_last3.shape)","d2d507db":"final_train","6e072b38":"params = {'n_estimators':2000,\n            'boosting_type': 'gbdt',\n            'objective': 'regression',\n            'metric': 'rmse',\n            'subsample': 0.75,\n            'subsample_freq': 1,\n            'learning_rate': 0.04,\n            'feature_fraction': 0.9,\n         'max_depth': 15,\n            'lambda_l1': 1,  \n            'lambda_l2': 1,\n            'verbose': 100,\n            'early_stopping_rounds': 100, 'eval_metric': 'cappa'\n            }","e0276dde":"def lgb_regression(x_train, y_train, x_val, y_val, **kwargs):\n    \n    models = []\n    scores=[]\n    categoricals = ['session_title']\n        \n    train_set = lgb.Dataset(x_train, y_train, categorical_feature=categoricals)\n    val_set = lgb.Dataset(x_val, y_val, categorical_feature=categoricals)\n        \n    model = lgb.train(train_set=train_set, valid_sets=[train_set, val_set],**kwargs)\n    models.append(model)\n        \n    pred_val=model.predict(x_val)\n    oof = pred_val.reshape(len(x_val))\n        \n    return models,oof","2ebde205":"X_train3 = final_train3.drop('accuracy_group',axis=1)\ny_train3 = final_train3['accuracy_group'].astype(float)\nX_end3 = final_train_last3.drop('accuracy_group',axis=1)\ny_end3 = final_train_last3['accuracy_group'].astype(float)\n\nmodels3,oof3 = lgb_regression(X_train3, y_train3, X_end3, y_end3, params=params, num_boost_round=100000,\n                  early_stopping_rounds=500, verbose_eval=40)","024c172d":"X_train2 = final_train2.drop('accuracy_group',axis=1)\ny_train2 = final_train2['accuracy_group'].astype(float)\nX_end2 = final_train_last2.drop('accuracy_group',axis=1)\ny_end2 = final_train_last2['accuracy_group'].astype(float)\n\nmodels2,oof2 = lgb_regression(X_train2, y_train2, X_end2, y_end2, params=params, num_boost_round=40000,\n                  early_stopping_rounds=500, verbose_eval=40)","a8c3bb9f":"X_train1 = final_train.drop('accuracy_group',axis=1)\ny_train1 = final_train['accuracy_group'].astype(float)\nX_end1 = final_train_last.drop('accuracy_group',axis=1)\ny_end1 = final_train_last['accuracy_group'].astype(float)\n\nmodels1,oof1 = lgb_regression(X_train1, y_train1, X_end1, y_end1, params=params, num_boost_round=40000,\n                  early_stopping_rounds=500, verbose_eval=40)\n\nmodels_LGBM = models1 + models2 + models3","9d9cf283":"lgb.plot_importance(models_LGBM[0],max_num_features=20,importance_type='gain')","b2a95293":"from functools import partial\nimport scipy as sp\nclass OptimizedRounder(object):\n    \"\"\"\n    An optimizer for rounding thresholds\n    to maximize Quadratic Weighted Kappa (QWK) score\n    # https:\/\/www.kaggle.com\/naveenasaithambi\/optimizedrounder-improved\n    \"\"\"\n    def __init__(self):\n        self.coef_ = 0\n\n    def _kappa_loss(self, coef, X, y,X2, y2,X3, y3):\n        \"\"\"\n        Get loss according to\n        using current coefficients\n        \n        :param coef: A list of coefficients that will be used for rounding\n        :param X: The raw predictions of final_train_last\n        :param y: The ground truth labels\n        :param X2: The raw predictions of final_train_last2\n        :param y2: The ground truth labels\n        :param X3: The raw predictions of final_train_last3\n        :param y3: The ground truth labels\n        \"\"\"\n        X_p = pd.cut(X, [-np.inf] + list(np.sort(coef)) + [np.inf], labels = [0, 1, 2, 3])\n        X_p2 = pd.cut(X2, [-np.inf] + list(np.sort(coef)) + [np.inf], labels = [0, 1, 2, 3])\n        X_p3 = pd.cut(X3, [-np.inf] + list(np.sort(coef)) + [np.inf], labels = [0, 1, 2, 3])\n        \n        print(\"validation score of the last assessment: score1 = {}\".format(qwk(y, X_p)))\n        \n        return ( qwk(y2, X_p2) - qwk(y, X_p) ) + ( qwk(y3, X_p3) - qwk(y, X_p) )  + (0.51 - qwk(y, X_p))*2\n\n    def fit(self, X, y,X2, y2,X3, y3,coef_ini):\n        \"\"\"\n        Optimize rounding thresholds\n        \n        :param X: The raw predictions\n        :param y: The ground truth labels\n        \"\"\"\n        loss_partial = partial(self._kappa_loss, X=X, y=y,X2=X2, y2=y2,X3=X3, y3=y3)\n        initial_coef = coef_ini\n        self.coef_ = sp.optimize.minimize(loss_partial, initial_coef, method='nelder-mead')\n\n    def predict(self, X, coef):\n        \"\"\"\n        Make predictions with specified thresholds\n        \n        :param X: The raw predictions\n        :param coef: A list of coefficients that will be used for rounding\n        \"\"\"\n        return pd.cut(X, [-np.inf] + list(np.sort(coef)) + [np.inf], labels = [0, 1, 2, 3])\n\n\n    def coefficients(self):\n        \"\"\"\n        Return the optimized coefficients\n        \"\"\"\n        return self.coef_['x']","ee9692d9":"%%time\n\noptR = OptimizedRounder()\noptR.fit(oof1,y_end1,oof2,y_end2,oof3,y_end3,[1.12, 1.83, 2.27])\ncoefficients = optR.coefficients()\nprint(coefficients)","5e520c13":"oof3[oof3 <= coefficients[0]] = 0\noof3[np.where(np.logical_and(oof3 > coefficients[0], oof3 <= coefficients[1]))] = 1\noof3[np.where(np.logical_and(oof3 > coefficients[1], oof3 <=coefficients[2]))] = 2\noof3[oof3 > coefficients[2]] = 3\npred3 = np.round(oof3).astype('int')\n\nscore3 = qwk(y_end3, pred3)\nprint(\"validation score of the third last assessment: score3 = {}\".format(score3))","6b8aa450":"oof2[oof2 <= coefficients[0]] = 0\noof2[np.where(np.logical_and(oof2 > coefficients[0], oof2 <= coefficients[1]))] = 1\noof2[np.where(np.logical_and(oof2 > coefficients[1], oof2 <=coefficients[2]))] = 2\noof2[oof2 > coefficients[2]] = 3\npred2 = np.round(oof2).astype('int')\n\nscore2 = qwk(y_end2, pred2)\nprint(\"validation score of the second last assessment: score2 = {}\".format(score2))","42973460":"oof1[oof1 <= coefficients[0]] = 0\noof1[np.where(np.logical_and(oof1 > coefficients[0], oof1 <= coefficients[1]))] = 1\noof1[np.where(np.logical_and(oof1 > coefficients[1], oof1 <=coefficients[2]))] = 2\noof1[oof1 > coefficients[2]] = 3\npred1 = np.round(oof1).astype('int')\n\nscore1 = qwk(y_end1, pred1)\nprint(\"validation score of the last assessment: score1 = {}\".format(score1))","62faf427":"params = {'n_estimators':1000,\n            'boosting_type': 'gbdt',\n            'objective': 'regression',\n            'metric': 'rmse',\n            'subsample': 0.75,\n            'subsample_freq': 1,\n            'learning_rate': 0.04,\n            'feature_fraction': 0.9,\n          'seed':42,\n         'max_depth': 15,\n            'lambda_l1': 1,  \n            'lambda_l2': 1,\n            'verbose': 100,\n            'early_stopping_rounds': 100, 'eval_metric': 'cappa'\n            }\nX = pd.concat([X_train1,X_end1])\nY = pd.concat([y_train1,y_end1])\nmodels_all = []\noof_all = np.zeros(len(X))\nkf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\nfor train, val in kf.split(X, Y):\n    model_all,oof = lgb_regression(X.iloc[train], Y.iloc[train], X.iloc[val], Y.iloc[val], params=params, num_boost_round=40000,\n                     early_stopping_rounds=500, verbose_eval=40)\n    models_all.append(model_all[0])\n    oof_all[val] = oof","e8f162ed":"lgb.plot_importance(model_all[0],max_num_features=20,importance_type='gain')","d4e2b2ba":"oof_all[oof_all <= coefficients[0]] = 0\noof_all[np.where(np.logical_and(oof_all > coefficients[0], oof_all <= coefficients[1]))] = 1\noof_all[np.where(np.logical_and(oof_all > coefficients[1], oof_all <=coefficients[2]))] = 2\noof_all[oof_all > coefficients[2]] = 3\noof_all = np.round(oof_all).astype('int')\n\nscore = qwk(Y, oof_all)\nprint(\"validation score with StratifiedKFold_n=5: score = {}\".format(score))","43056cbd":"params = {'n_estimators':1000,\n            'boosting_type': 'gbdt',\n            'objective': 'regression',\n            'metric': 'rmse',\n            'subsample': 0.75,\n            'subsample_freq': 1,\n            'learning_rate': 0.04,\n            'feature_fraction': 0.9,\n          'seed':15,\n         'max_depth': 15,\n            'lambda_l1': 1,  \n            'lambda_l2': 1,\n            'verbose': 100,\n            'early_stopping_rounds': 100, 'eval_metric': 'cappa'\n            }\n\nkf = StratifiedKFold(n_splits=5, shuffle=True, random_state=15)\nfor train, val in kf.split(X, Y):\n    model_all,oof = lgb_regression(X.iloc[train], Y.iloc[train], X.iloc[val], Y.iloc[val], params=params, num_boost_round=40000,\n                     early_stopping_rounds=500, verbose_eval=200)\n    models_all.append(model_all[0])\n\nparams = {'n_estimators':1000,\n            'boosting_type': 'gbdt',\n            'objective': 'regression',\n            'metric': 'rmse',\n            'subsample': 0.75,\n            'subsample_freq': 1,\n            'learning_rate': 0.04,\n            'feature_fraction': 0.9,\n          'seed':12,\n         'max_depth': 15,\n            'lambda_l1': 1,  \n            'lambda_l2': 1,\n            'verbose': 100,\n            'early_stopping_rounds': 100, 'eval_metric': 'cappa'\n            }\n\nkf = StratifiedKFold(n_splits=5, shuffle=True, random_state=12)\nfor train, val in kf.split(X, Y):\n    model_all,oof = lgb_regression(X.iloc[train], Y.iloc[train], X.iloc[val], Y.iloc[val], params=params, num_boost_round=40000,\n                     early_stopping_rounds=500, verbose_eval=200)\n    models_all.append(model_all[0])\n    \nparams = {'n_estimators':1000,\n            'boosting_type': 'gbdt',\n            'objective': 'regression',\n            'metric': 'rmse',\n            'subsample': 0.75,\n            'subsample_freq': 1,\n            'learning_rate': 0.04,\n            'feature_fraction': 0.9,\n          'seed':11,\n         'max_depth': 15,\n            'lambda_l1': 1,  \n            'lambda_l2': 1,\n            'verbose': 100,\n            'early_stopping_rounds': 100, 'eval_metric': 'cappa'\n            }\n\nkf = StratifiedKFold(n_splits=5, shuffle=True, random_state=11)\nfor train, val in kf.split(X, Y):\n    model_all,oof = lgb_regression(X.iloc[train], Y.iloc[train], X.iloc[val], Y.iloc[val], params=params, num_boost_round=40000,\n                     early_stopping_rounds=500, verbose_eval=200)\n    models_all.append(model_all[0])","0821cd88":"X_test = final_test.drop(columns=['accuracy_group'])","777a17de":"predictions = []\nfor model in models_all:\n    predictions.append(model.predict(X_test))\n\nL=[]\nfor i in range (len(predictions[0])):\n    mean = []\n    for j in range (len(predictions)):\n        mean.append(predictions[j][i])\n    L.append(np.mean(mean))\n    \npredictions = np.array(L)\n\npredictions[predictions <= coefficients[0]] = 0\npredictions[np.where(np.logical_and(predictions > coefficients[0], predictions <= coefficients[1]))] = 1\npredictions[np.where(np.logical_and(predictions > coefficients[1], predictions <=coefficients[2]))] = 2\npredictions[predictions > coefficients[2]] = 3\n\npred = np.round(predictions).astype('int')\n\nsub_LGB_test=pd.DataFrame({'installation_id':submission.installation_id,'accuracy_group':pred})\nsub_LGB_test.to_csv('submission.csv',index=False)\nsub_LGB_test['accuracy_group'].plot(kind='hist')","8bc8f94f":"sub_LGB_test['accuracy_group'].value_counts()","ff8d6ac4":"I will use the **lgb regression model** with parameters of **Andrew Lukyanenko kernel** and also his optimization cutoffs.","3f45a981":"**Feature selection**","1353a695":"**final_train_last2** brings together every second last assessment of installation_id from **train** :\n    \n   equivalent to bring together every last assessment of installation_id from **final_train**","db18befc":"**final_train3** as a train data and **final_train_last3** as a validation data :","9f01db1f":"score1 = validation score on final_last_assessment\n\nscore2 = validation score on final_last_assessment2\n\nscore3 = validation score of on final_last_assessment3\n\nIn my previous versions, I noted that the quadratic weighted kappa score on final_train_last is low compared to others and the differences (score2 - score1) and (score3 - score1) seem to be negatively correlated to test score.\n\nSo I modificated the function OptimizedRounder. I try now to reduce the differences between scores. My goal is to have the same score on the 3 validation scores and also a good score (it's why I added (0.49 - qwk(y, X_p))*2 ). 0.49 seem to be the limit of my validation score on final_train_last with this model.","a3c0f49a":"**Engineering test data**","8aa74dd3":"**LGBregressor \/ params from Andrew Lukyanenko kernel**","fceb71a8":"**final_train2**  = **final_train**   -   **final_train_last2** ","635d4b1c":"**final_train2** as a train data and **final_train_last2** as a validation data :","90d7cb1f":"<font size=4 color='red'> Upvote if you think this kernel was helpful !<\/font>","b5fc0e0f":"4070_dif variable is the difference between the number of 4070 of the assessment and his previous assessment of an installation_id","643828cd":"We can note that the scores on validation data are really different. **The validation rmse score on final_train_last is quite high.**\n\nMaybe the model is not really robust considering different model.","cc238de2":"We do exactly the same to get every third last assessment of installation_id from **train**:\n\n   equivalent to bring together every last assessment of installation_id from **final_train2**","d0c1349d":"**Training on all train dataset with seed averaging**","54b1543b":"**Engineering train data**","c3a53b7a":"<font size=4 color='blue'>solutions:<\/font>\n\n- I just don't use information group by installation_id but I will use data between an assessment and his previous assessment (e.g: the number of 4070 between two assess)\n \n\n- I use specific validation datasets:\n\n  1. Validation dataset1: last assessment of every installation_id of train_label\n      \n     Train dataset1 = **final_train**: other assessments of train_label\n  \n  2. I do the same thing on **final_train**:\n   \n     Validation dataset2: second last assessment of every installation_id on train_labels\n      \n     Train dataset2 = **final_train2**: other assessments of **final_train**\n      \n  3. I do the same thing on **final_train2**:\n   \n     Validation dataset3: third last assessment of every installation_id on train_labels\n      \n     Train dataset3 = **final_train3**: other assessments of **final_train2**\n      \n\n**The goal of the point 2** is to only have the last assessments in the validation dataset.\n\n**You can use these 3 validation datasets to evaluate your model.** The 3 validation datasets are really different. I fit a lot of models with these dataset and I have always a big difference between validation scores(particularly with the validation dataset with last assessment). It would mean that the models are not really good.\n\nHave a good and same score on the 3 validation scores + good score score on test data would mean your model is less impact by overfitting.","dc448ef0":"We can now use **final_train2** as a train dataset and **final_train_last2** as a validation dataset","56484390":"**final_train_last** brings together every last assessment of installation_id from train dataset :","613a9b46":"**final_train** doesn't contain data from **final_train_last**\n\nAnd we can now use **final_train** as a train dataset and **final_train_last** as a validation dataset","e5a2c47f":"**final_train** as a train data and **final_train_last** as a validation data :","be72c884":"**test_to_train** contains data that we don't use for final_test, I put them in train dataset:","9c8ba2f1":"**Importing libraries and data**","59ce8332":"# I remove outliers id : ","445de367":"<font size=4 color='blue'>2 problems of overfitting that I try to resolve in this kernel:<\/font>\n\n> > \n- use information group by installation_id could lead to overfitting\n> > \n- use all data of train_label for validation dataset. Indeed, the validation score is low if I only use the last assessment of every installation_id as validation dataset compared to a validation score where I use random validation dataset.","55923b9d":"We can now use **final_train3** as a train dataset and **final_train_last3** as a validation dataset"}}