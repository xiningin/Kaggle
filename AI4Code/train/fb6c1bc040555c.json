{"cell_type":{"48f4b0f8":"code","59764337":"code","7731e72a":"code","3b7f95de":"code","6eefc9a0":"code","f6edd229":"code","c25fc6d4":"code","b210f2de":"code","ed04355c":"code","4ec88178":"code","7b4b389f":"code","635cb85d":"code","410be56c":"code","38999f14":"code","126cced7":"code","794e0016":"code","a158e0d6":"code","c04da51c":"code","54543952":"code","546051fe":"code","21300aa7":"code","9f398b84":"code","2348150f":"code","3b5a8cfb":"code","857e172d":"code","e6f80ced":"code","bd1c4f85":"code","bf3c4663":"code","16f8e429":"code","2be45aba":"code","f000e07e":"code","05c6cbb5":"code","0f201bb9":"code","32b2e49b":"code","fd84eca4":"code","7f0e19bc":"code","66bb12a5":"code","49edde2a":"code","df815343":"code","61332264":"code","8217f45e":"code","6ab0dafb":"code","9a14dce2":"code","ddd566a4":"code","9b1a0082":"markdown","88aee8b1":"markdown","fb0a06b6":"markdown","161176f2":"markdown","5d8dd7d7":"markdown","b569b6c3":"markdown","6b490ddd":"markdown","6bbf1f60":"markdown","8cba8e5f":"markdown","f19ae0bc":"markdown","0e9ebaef":"markdown","37a0218b":"markdown","c9d83f13":"markdown"},"source":{"48f4b0f8":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n\n# Any results you write to the current directory are saved as output.","59764337":"train = pd.read_csv('\/kaggle\/input\/data-without-drift\/train_clean.csv')\ntest = pd.read_csv('\/kaggle\/input\/data-without-drift\/test_clean.csv')","7731e72a":"train.head()","3b7f95de":"train.info()","6eefc9a0":"batch_indices = [slice(500000*i,500000*(i+1)) for i in range(10)]","f6edd229":"import matplotlib.pyplot as plt\nimport seaborn as sns","c25fc6d4":"fig, axes = plt.subplots(2,5,sharex=False,sharey=True, figsize=(25,10))\nfor i,ax in enumerate(axes.ravel()):\n    train.iloc[batch_indices[i]].plot(kind='line',x='time',y=['signal'],ax=ax,linewidth=.1)\n    ax.set_title('Batch_'+str(i))\nax.set_ylim(-5,14)\nax.legend()\nfig.suptitle('Training Data',y=1.05)\n\nplt.tight_layout()","b210f2de":"fig, axes = plt.subplots(1,4,sharex=False,sharey=True, figsize=(25,10))\nfor i,ax in enumerate(axes.ravel()):\n    test.iloc[batch_indices[i]].plot(kind='line',x='time',y=['signal'],ax=ax,linewidth=.1)\n    ax.set_title('Batch_'+str(i))\nax.set_ylim(-5,11)\nax.legend()\nfig.suptitle('Testing Data',y=1.05)\n\nplt.tight_layout()","ed04355c":"fig, axes = plt.subplots(2,5,sharex=True,sharey=True, figsize=(20,8))\nfor i,ax in enumerate(axes.ravel()):\n        sns.distplot(train.iloc[batch_indices[i]].query('open_channels == 0')['signal'],ax=ax,label='0')\n        sns.distplot(train.iloc[batch_indices[i]].query('open_channels == 1')['signal'],ax=ax,label='1')\n        sns.distplot(train.iloc[batch_indices[i]].query('open_channels == 2')['signal'],ax=ax,label='2')\n        sns.distplot(train.iloc[batch_indices[i]].query('open_channels == 3')['signal'],ax=ax,label='3')\n        sns.distplot(train.iloc[batch_indices[i]].query('open_channels == 4')['signal'],ax=ax,label='4')\n        sns.distplot(train.iloc[batch_indices[i]].query('open_channels == 5')['signal'],ax=ax,label='5')\n        sns.distplot(train.iloc[batch_indices[i]].query('open_channels == 6')['signal'],ax=ax,label='6')\n        sns.distplot(train.iloc[batch_indices[i]].query('open_channels == 7')['signal'],ax=ax,label='7')\n        sns.distplot(train.iloc[batch_indices[i]].query('open_channels == 8')['signal'],ax=ax,label='8')\n        sns.distplot(train.iloc[batch_indices[i]].query('open_channels == 9')['signal'],ax=ax,label='9')\n        sns.distplot(train.iloc[batch_indices[i]].query('open_channels == 10')['signal'],ax=ax,label='10')\n        ax.set_title('Batch_'+str(i))\nax.legend()\nfig.suptitle('Training Data',y=1.05)\n\nax.set_ylim(0,2)\nplt.tight_layout()","4ec88178":"train_seg_boundaries = np.concatenate([[0,500000,600000], np.arange(1000000,5000000+1,500000)])\ntrain_signal = np.split(np.zeros(5000000), train_seg_boundaries[1:-1])\ntest_seg_boundaries = np.concatenate([np.arange(0,1000000+1,100000), [1500000,2000000]])\ntest_signal = np.split(np.zeros(2000000), test_seg_boundaries[1:-1])","7b4b389f":"test['signal_type']=np.concatenate([test_signal[0]+1,test_signal[1]+3,\n                             test_signal[2]+4,test_signal[3]+1,\n                             test_signal[4]+2,test_signal[5]+5,\n                             test_signal[6]+4,test_signal[7]+5,\n                             test_signal[8]+1,test_signal[9]+3,\n                             test_signal[10]+1,test_signal[11]+1])\ntest['signal'].plot(kind='line',linewidth=.2, label='Test Signal')\ntest['signal_type'].plot(kind='line', label='Signal Type')\nplt.legend()\ndel test_signal","635cb85d":"train['signal_type']= np.concatenate([train_signal[0]+1,train_signal[1]+1,\n                               train_signal[2]+1,train_signal[3]+2,\n                               train_signal[4]+3,train_signal[5]+5,\n                               train_signal[6]+4,train_signal[7]+2,\n                               train_signal[8]+3,train_signal[9]+4,\n                               train_signal[10]+5])\ntrain['signal'].plot(kind='line',linewidth=.2, label='Train Signal')\ntrain['signal_type'].plot(kind='line', label='Signal Type')\nplt.legend()\ndel train_signal","410be56c":"del train['time'] # we don't need the time anymore","38999f14":"means = train.groupby(['signal_type','open_channels']).mean().signal\ntrain['scaled_signal'] = train['signal']\ntest['scaled_signal'] = test['signal']","126cced7":"from sklearn.metrics import f1_score, classification_report","794e0016":"def shift_model(x,sig_type):\n    scaled = (train.loc[train.signal_type == sig_type,'signal']-x[0])*x[1]\n    target = train.loc[train.signal_type == sig_type,'open_channels']\n    return -f1_score(target,scaled.clip(0,10).round(),average='weighted')","a158e0d6":"from scipy.optimize import minimize","c04da51c":"for i in range(1,5):\n    print(i)\n    min_f=minimize(shift_model,[means.loc[i,0],1\/(means.loc[i,1]-means.loc[i,0])],args=(i),method='Powell')\n    train.loc[train.signal_type == i,'scaled_signal'] = (train.loc[train.signal_type == i,'signal']-min_f['x'][0])*min_f['x'][1]\n    test.loc[test.signal_type == i,'scaled_signal'] = (test.loc[test.signal_type == i,'signal']-min_f['x'][0])*min_f['x'][1]\n\ni=5\nmin_f=minimize(shift_model,[means.loc[i,1]-1,5\/(means.loc[i,6]-means.loc[i,1])],args=(i),method='Powell')\ntrain.loc[train.signal_type == i,'scaled_signal'] = (train.loc[train.signal_type == i,'signal']-min_f['x'][0])*min_f['x'][1]\ntest.loc[test.signal_type == i,'scaled_signal'] = (test.loc[test.signal_type == i,'signal']-min_f['x'][0])*min_f['x'][1]\ndel means","54543952":"fig, axes = plt.subplots(1,5,sharex=True,sharey=True, figsize=(25,5))\nfor i,ax in enumerate(axes.ravel()):\n    for j in range(11):\n        sns.distplot(train[(train.signal_type==i+1)&(train.open_channels == j)]['scaled_signal'],ax=ax,label='%i'%j)\n        \naxes[3].legend()\nax.set_ylim(0,2.2)\nax.set_xticks(np.arange(11))\nplt.tight_layout()","546051fe":"print(f1_score(train.open_channels,train['scaled_signal'].clip(0,10).round(),average='macro'))","21300aa7":"print(classification_report(train.open_channels,train['scaled_signal'].clip(0,10).round()))","9f398b84":"from sklearn.metrics import confusion_matrix\ncm=confusion_matrix(train.open_channels,train['scaled_signal'].clip(0,10).round(),normalize='true')","2348150f":"fig, axes = plt.subplots(1,1,sharex=True,sharey=True, figsize=(25,10))\naxes=sns.heatmap(cm,annot=True)","3b5a8cfb":"plt.plot(np.histogram(train['scaled_signal'].clip(0,10).round(),bins=11)[0]\/np.histogram(train.open_channels,bins=11)[0]-1)\nplt.title('% Error for each label')","857e172d":"fig, axes = plt.subplots(1,1,sharex=True,sharey=True, figsize=(25,10))\nsns.distplot(train['scaled_signal'].clip(0,10).round(),ax=axes, kde=False, label='prediction')\nsns.distplot(train.open_channels,ax=axes, kde=False, label='truth')\nplt.legend()","e6f80ced":"train.drop(['signal'],axis=1,inplace=True)\ntest.drop(['signal'],axis=1,inplace=True)","bd1c4f85":"def calc_gradients(s, n_grads=2):\n    grads = pd.DataFrame()\n    g = s.values\n    for i in range(n_grads):\n        g = np.gradient(g)\n        grads['grad_' + str(i+1)] = g\n    return grads\n\nfrom scipy import signal\ndef calc_low_pass(s, n_filts=10):\n    \n    wns = np.logspace(-2, -0.3, n_filts)\n    \n    low_pass = pd.DataFrame()\n    x = s.values\n    for wn in wns:\n        b, a = signal.butter(1, Wn=wn, btype='low')\n        zi = signal.lfilter_zi(b, a)\n        low_pass['lowpass_lf_' + str('%.4f' %wn)] = signal.lfilter(b, a, x, zi=zi*x[0])[0]\n        low_pass['lowpass_ff_' + str('%.4f' %wn)] = signal.filtfilt(b, a, x)\n        \n    return low_pass\n\ndef calc_high_pass(s, n_filts=10):\n    \n    wns = np.logspace(-2, -0.1, n_filts)\n    \n    high_pass = pd.DataFrame()\n    x = s.values\n    for wn in wns:\n        b, a = signal.butter(1, Wn=wn, btype='high')\n        zi = signal.lfilter_zi(b, a)\n        high_pass['highpass_lf_' + str('%.4f' %wn)] = signal.lfilter(b, a, x, zi=zi*x[0])[0]\n        high_pass['highpass_ff_' + str('%.4f' %wn)] = signal.filtfilt(b, a, x)\n        \n    return high_pass\n\ndef calc_roll_stats(s, windows=[10,50, 100, 500,1000]):\n    '''\n    Calculates rolling stats like mean, std, min, max...\n    '''\n    roll_stats = pd.DataFrame()\n    for w in windows:\n        roll_stats['roll_mean_' + str(w)] = s.rolling(window=w).mean()\n        roll_stats['roll_median_' + str(w)] = s.rolling(window=w).median()\n        roll_stats['roll_std_' + str(w)] = s.rolling(window=w).std()\n        roll_stats['roll_min_' + str(w)] = s.rolling(window=w).min()\n        roll_stats['roll_max_' + str(w)] = s.rolling(window=w).max()\n        roll_stats['roll_range_' + str(w)] = roll_stats['roll_max_' + str(w)] - roll_stats['roll_min_' + str(w)]\n        roll_stats['roll_q10_' + str(w)] = s.rolling(window=w).quantile(0.10)\n        roll_stats['roll_q25_' + str(w)] = s.rolling(window=w).quantile(0.25)\n        roll_stats['roll_q75_' + str(w)] = s.rolling(window=w).quantile(0.75)\n        roll_stats['roll_q90_' + str(w)] = s.rolling(window=w).quantile(0.90)\n        \n    \n    # add zeros when na values (std)\n    roll_stats = roll_stats.fillna(value=0)\n             \n    return roll_stats\n\ndef calc_ewm(s, windows=[10,50, 100,500, 1000]):\n    \n    ewm = pd.DataFrame()\n    for w in windows:\n        ewm['ewm_mean_' + str(w)] = s.ewm(span=w, min_periods=1).mean()\n        ewm['ewm_std_' + str(w)] = s.ewm(span=w, min_periods=1).std()\n        \n    # add zeros when na values (std)\n    ewm = ewm.fillna(value=0)\n        \n    return ewm\n\ndef calc_shifts(s, periods=[-3,-2,-1,1,2,3]):\n    \n    sft = pd.DataFrame()\n    for p in periods:\n        if p>0:\n            sft['signal_shift_' + str(p)] = s.shift(periods=p).fillna(method='bfill')\n        else:\n            sft['signal_shift_' + str(p)] = s.shift(periods=p).fillna(method='ffill')\n    return sft\n\ndef add_features(s):\n    \n    gradients = calc_gradients(s)\n    low_pass = calc_low_pass(s)\n    high_pass = calc_high_pass(s)\n    roll_stats = calc_roll_stats(s)\n    ewm = calc_ewm(s)\n    sft = calc_shifts(s)\n    \n    return pd.concat([gradients, low_pass, high_pass, roll_stats, ewm, sft], axis=1)\n\ndef divide_and_add_features(s, splits):\n    \n    ls = []\n    for i,v in enumerate(splits[:-1]):\n        print(i)\n        sig = s[v:splits[i+1]].copy().reset_index(drop=True)\n        sig_featured = add_features(sig)\n        ls.append(sig_featured)\n    \n    return pd.concat(ls, axis=0).set_index(s.index)","bf3c4663":"df = divide_and_add_features(train['scaled_signal'],train_seg_boundaries)\ntrain = pd.concat([train,df],axis=1)\ndel df","16f8e429":"train['signal_quad'] = train.scaled_signal**2\ntrain['signal_cubic'] = train.scaled_signal**3","2be45aba":"train.drop(range(3640000,3830000),axis=0,inplace=True)\ndrop_indexes=train[(train.signal_type==5) & (train.open_channels==0)].index\ntrain.drop(drop_indexes,inplace=True)","f000e07e":"train_type=[]\nfrom sklearn.model_selection import train_test_split\nfor t,df_type in train.groupby('signal_type'):\n    train_type.append(dict(zip(['X_train','X_test','y_train','y_test'],\n                               train_test_split(df_type.drop(['signal_type','open_channels'],axis=1),\n                                                df_type['open_channels'],\n                                                random_state=42,\n                                                stratify=df_type['open_channels'],\n                                                test_size=.2))))\n    \ndel train\n    ","05c6cbb5":"from lightgbm import LGBMRegressor\nfrom sklearn.ensemble import RandomForestRegressor, VotingRegressor\nfrom sklearn.linear_model import Ridge\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\nfrom sklearn.neural_network import MLPRegressor\nfrom sklearn.model_selection import GridSearchCV\n\n# def train_models():\n#     models = []\n#     predictions = []\n#     tests = []\n#     t = train_type[-1]\n#     pipeline=Pipeline([('scaler',PCA(n_components=95,whiten=True)),\n#         ('clf',Ridge())])\n\n#     param_grid = {'scaler__n_components':range(10,len(t['X_test'].columns),10),'clf__alpha':[2,5]}\n\n#     grid_model = GridSearchCV(pipeline, param_grid=param_grid, cv=5, scoring='neg_mean_squared_error', refit=True)\n\n#     grid_model.fit(t['X_train'],t['y_train'])\n#     max_y = np.max(t['y_test'])\n#     min_y = np.min(t['y_test'])\n#     y_pred = np.round(np.clip(grid_model.predict(t['X_test']),min_y,max_y))\n#     print('mse:',grid_model.score(t['X_test'], t['y_test']))\n#     print('CV mse:',grid_model.best_score_,grid_model.best_params_)\n#     print(f1_score(t['y_test'],y_pred, average='weighted'))\n#     print(classification_report(t['y_test'],y_pred))\n#     models.append(grid_model)\n#     predictions.append(y_pred)\n#     tests.append(t['y_test'])\n#     print(f1_score(np.concatenate(tests),np.concatenate(predictions), average='macro'))\n#     print(classification_report(np.concatenate(tests),np.concatenate(predictions)))\n#     return models\n# models=train_models()","0f201bb9":"# from lightgbm import LGBMRegressor\n# from sklearn.ensemble import RandomForestRegressor, VotingRegressor\n# from sklearn.linear_model import Ridge\n# from sklearn.pipeline import Pipeline\n# from sklearn.preprocessing import StandardScaler\n# from sklearn.decomposition import PCA\n# from sklearn.neural_network import MLPRegressor\n# from sklearn.model_selection import GridSearchCV\n\n# def train_models():\n#     models = []\n#     predictions = []\n#     tests = []\n#     t = train_type[-1]\n#     lgb=LGBMRegressor(num_leaves=30,\n#             learning_rate=0.1,\n#             n_estimators=900,\n#             subsample=.8,\n#             colsample_bytree=1,\n#             random_state=42,\n#             n_jobs=-1\n#             )\n\n#     param_grid = {\n#     'learning_rate': [ 0.01,.05],\n#     'num_leaves': [30,50], \n#     'boosting_type' : [ 'dart'],\n#     'subsample' : [0.6,0.8],\n#     'min_child_samples': [10,20]\n#     }\n\n#     grid_model = GridSearchCV(lgb, param_grid=param_grid, cv=5, scoring='neg_mean_squared_error', refit=True, n_jobs=-1)\n\n#     grid_model.fit(t['X_train'],t['y_train'])\n#     max_y = np.max(t['y_test'])\n#     min_y = np.min(t['y_test'])\n#     y_pred = np.round(np.clip(grid_model.predict(t['X_test']),min_y,max_y))\n#     print('mse:',grid_model.score(t['X_test'], t['y_test']))\n#     print('CV mse:',grid_model.best_score_,grid_model.best_params_)\n#     print(f1_score(t['y_test'],y_pred, average='weighted'))\n#     print(classification_report(t['y_test'],y_pred))\n#     models.append(grid_model)\n#     predictions.append(y_pred)\n#     tests.append(t['y_test'])\n#     print(f1_score(np.concatenate(tests),np.concatenate(predictions), average='macro'))\n#     print(classification_report(np.concatenate(tests),np.concatenate(predictions)))\n#     return models\n# models=train_models()","32b2e49b":"from lightgbm import LGBMRegressor\nfrom sklearn.ensemble import RandomForestRegressor, VotingRegressor\nfrom sklearn.linear_model import Ridge\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\nfrom sklearn.neural_network import MLPRegressor\nfrom sklearn.model_selection import GridSearchCV\n# def train_models():\n#     models = []\n#     predictions = []\n#     tests = []\n#     t = train_type[-1]\n#     pipeline=Pipeline([('scaler',PCA(n_components=20,whiten=True)),\n#         ('clf',MLPRegressor(hidden_layer_sizes=(50,20,15),early_stopping=True))])\n\n#     param_grid = {'scaler__n_components':range(21,39)}\n\n#     grid_model = GridSearchCV(pipeline, param_grid=param_grid, cv=5, scoring='neg_mean_squared_error', refit=True)\n\n#     grid_model.fit(t['X_train'],t['y_train'])\n#     max_y = np.max(t['y_test'])\n#     min_y = np.min(t['y_test'])\n#     y_pred = np.round(np.clip(grid_model.predict(t['X_test']),min_y,max_y))\n#     print('mse:',grid_model.score(t['X_test'], t['y_test']))\n#     print('CV mse:',grid_model.best_score_,grid_model.best_params_)\n#     print(f1_score(t['y_test'],y_pred, average='weighted'))\n#     print(classification_report(t['y_test'],y_pred))\n#     models.append(grid_model)\n#     predictions.append(y_pred)\n#     tests.append(t['y_test'])\n#     print(f1_score(np.concatenate(tests),np.concatenate(predictions), average='macro'))\n#     print(classification_report(np.concatenate(tests),np.concatenate(predictions)))\n#     return models\n# models=train_models()","fd84eca4":"from lightgbm import LGBMRegressor\nfrom sklearn.ensemble import RandomForestRegressor, VotingRegressor\nfrom sklearn.linear_model import Ridge\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\nfrom sklearn.neural_network import MLPRegressor\ndef train_models():\n    models = []\n    predictions = []\n    tests = []\n    for t in train_type:\n         \n        clf1 = Pipeline([('scaler',PCA(n_components=91,whiten=True)),\n            ('clf',Ridge(alpha=5))])\n        clf2 = Pipeline([\n            ('clf',LGBMRegressor(num_leaves=30,\n            learning_rate=0.05,\n            n_estimators=900,\n            subsample=.8,\n            colsample_bytree=1,\n            random_state=42,\n            min_child_samples=10,\n            n_jobs=-1\n            ))])\n        \n        clf3 = Pipeline([('scaler',PCA(n_components=25,whiten=True)),\n            ('clf',MLPRegressor(hidden_layer_sizes=(50,20,15),early_stopping=True))])\n        eclf1 = VotingRegressor(estimators=[('lr', clf1), ('bm', clf2),('nn',clf3)])\n        steps = [('clf',eclf1)]\n        pipeline = Pipeline(steps)\n        \n        pipeline.fit(t['X_train'],t['y_train'])\n        max_y = np.max(t['y_test'])\n        min_y = np.min(t['y_test'])\n        y_pred = np.round(np.clip(pipeline.predict(t['X_test']),min_y,max_y)).astype(int)\n        \n        print(f1_score(t['y_test'],y_pred, average='weighted'))\n        print(classification_report(t['y_test'],y_pred))\n        models.append(pipeline)\n        predictions.append(y_pred)\n        tests.append(t['y_test'])\n    print(f1_score(np.concatenate(tests),np.concatenate(predictions), average='macro'))\n    print(classification_report(np.concatenate(tests),np.concatenate(predictions)))\n    return models\nmodels=train_models()","7f0e19bc":"y_pred = []\ny_test = []\nfor i,m in enumerate(models):\n    y_pred.append(m.predict(train_type[i]['X_test']))\n    y_test.append(train_type[i]['y_test'])\ny_pred=np.concatenate(y_pred)\ny_test=np.concatenate(y_test)","66bb12a5":"comps=pd.DataFrame({'pred':y_pred,'test':y_test})","49edde2a":"def threshold_model(x,df):\n    s = df.pred.copy()\n    s[s<x[0]] = 0\n    s[(s>=x[0])&(s<x[1])] = 1\n    s[(s>=x[1])&(s<x[2])] = 2\n    s[(s>=x[2])&(s<x[3])] = 3\n    s[(s>=x[3])&(s<x[4])] = 4\n    s[(s>=x[4])&(s<x[5])] = 5\n    s[(s>=x[5])&(s<x[6])] = 6\n    s[(s>=x[6])&(s<x[7])] = 7\n    s[(s>=x[7])&(s<x[8])] = 8\n    s[(s>=x[8])&(s<x[9])] = 9\n    s[s>=x[9]] = 10\n    return -f1_score(df.test,s.values,average='macro')","df815343":"thresholds=minimize(threshold_model,np.arange(.5,10.5,1),args=(comps),method='Powell',tol=.0001)","61332264":"def threshold_func(x):\n    if x<thresholds['x'][0]:\n        return 0\n    elif (x>thresholds['x'][0]) and (x<thresholds['x'][1]):\n        return 1\n    elif (x>thresholds['x'][1]) and (x<thresholds['x'][2]):\n        return 2\n    elif (x>thresholds['x'][2]) and (x<thresholds['x'][3]):\n        return 3\n    elif (x>thresholds['x'][3]) and (x<thresholds['x'][4]):\n        return 4\n    elif (x>thresholds['x'][4]) and (x<thresholds['x'][5]):\n        return 5\n    elif (x>thresholds['x'][5]) and (x<thresholds['x'][6]):\n        return 6\n    elif (x>thresholds['x'][6]) and (x<thresholds['x'][7]):\n        return 7\n    elif (x>thresholds['x'][7]) and (x<thresholds['x'][8]):\n        return 8\n    elif (x>thresholds['x'][8]) and (x<thresholds['x'][9]):\n        return 9\n    else:\n        return 10","8217f45e":"comps['pred']=comps.pred.apply(threshold_func)","6ab0dafb":"f1_score(comps.test,comps.pred,average='macro')","9a14dce2":"df = divide_and_add_features(test['scaled_signal'],test_seg_boundaries)\ntest=pd.concat([test,df],axis=1)\ndel df\ntest['signal_quad'] = test.scaled_signal**2\ntest['signal_cubic'] = test.scaled_signal**3","ddd566a4":"test['open_channels']=test.scaled_signal*0\nfor i in range(1,6):\n     data=test.loc[(test.signal_type == i)].drop(['open_channels','time','signal_type'],axis=1)\n    \n     test.loc[test.signal_type == i,'open_channels'] = models[i-1].predict(data)\ntest['open_channels']=test['open_channels'].apply(threshold_func)\ntest[['time','open_channels']].to_csv('ion_submission_en.csv', index=False, float_format='%.4f')","9b1a0082":"## The plot below shows when there is no drift the signal for each label (number of open channels) is roughly normally distributed with overlap between labels in the tails of the distributions.\n\n#### In Batch 4 the thin blue bar is due to a smaller number of samples with zero open channels","88aee8b1":"## Now make an ensemble model with the best hyperparameters","fb0a06b6":"## After making the various features we can start modeling. We will use an ensemble model with linear\/LGBM\/MLP models","161176f2":"## It seems there are 5 types of signal and the distance between the centroids of each label are roughly the same. It may be possible to scale the data so the mean signal for each label distribution falls on the labels value.","5d8dd7d7":"## Now apply the model to the test data and submit","b569b6c3":"## With a macro f1 score of .927 we can see this basic model does fairly well. The confusion matrix below shows moderate mixing between neighboring classes, which is what we would expect based on the earlier plot showing the distribution for each label having overlapping tails with its neighbors","6b490ddd":"## The signal between steps 3640000 and 3830000 is very noisy and is not replicated in the test data so we will just drop it. Also there are a couple of labels in the 5th signal type with a value of zero, since there are no similar signal values in the testing data we will drop these as well.","6bbf1f60":"## Now for feature engineering. The distributions for each label overlap so we need to develop extra features that will make the distributions separable.","8cba8e5f":"## Now lets scale each signal type so that the centroid for each label falls on the label value. Then we will round the signal and use this as a baseline classification model","f19ae0bc":"## Data comes in batches. Create slices to select different batches","0e9ebaef":"## First we need to identify the sections that belong to each signal type by eye.","37a0218b":"## First we do a grid search to determine suitable hyperparameters","c9d83f13":"**## Load data thanks to https:\/\/www.kaggle.com\/cdeotte\/data-without-drift"}}