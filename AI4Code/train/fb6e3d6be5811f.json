{"cell_type":{"3beba10d":"code","1bb3c37a":"code","c8f0fb26":"code","f8e12b5a":"code","b814168c":"code","1c64f7d9":"code","a3de2892":"code","b993bfc2":"markdown","9a0dde09":"markdown","c5187e29":"markdown"},"source":{"3beba10d":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","1bb3c37a":"import numpy as np\nimport random\nimport sklearn.datasets\nfrom sklearn.datasets.samples_generator import make_regression \nimport pylab\nfrom scipy import stats\nimport matplotlib.pyplot as plt","c8f0fb26":"\n\ndef gradient_descent(alpha, x, y, ep=0.0001, max_iter=10000):\n    converged = False\n    iter = 0\n    m = x.shape[0] # number of samples\n\n    # initial theta\n    t0 = np.random.random(x.shape[1])\n    t1 = np.random.random(x.shape[1])\n\n    # total error, J(theta)\n    J = sum([(t0 + t1*x[i] - y[i])**2 for i in range(m)])\n\n    # Iterate Loop\n    while not converged:\n        # for each training sample, compute the gradient (d\/d_theta j(theta))\n        grad0 = 1.0\/m * sum([(t0 + t1*x[i] - y[i]) for i in range(m)]) \n        grad1 = 1.0\/m * sum([(t0 + t1*x[i] - y[i])*x[i] for i in range(m)])\n\n        # update the theta_temp\n        temp0 = t0 - alpha * grad0\n        temp1 = t1 - alpha * grad1\n    \n        # update theta\n        t0 = temp0\n        t1 = temp1\n\n        # mean squared error\n        e = sum( [ (t0 + t1*x[i] - y[i])**2 for i in range(m)] ) \n\n        if abs(J-e) <= ep:\n            print ('Converged, iterations: ', iter)\n            converged = True\n    \n        J = e   # update error \n        iter += 1  # update iter\n    \n        if iter == max_iter:\n            print('Max interactions exceeded!')\n            converged = True\n\n    return t0,t1\n","f8e12b5a":"\nif __name__ == '__main__':\n\n    x, y = make_regression(n_samples=100, n_features=1, n_informative=1, \n                        random_state=0, noise=35) \n    print ('x.shape = {} y.shape = {}'.format(x.shape, y.shape))\n\n","b814168c":"    plt.plot(x,y,'o')\n    plt.xlim([-2,2.5])\n    plt.ylim([-150,150])\n    plt.show()","1c64f7d9":"\n    alpha = 0.01 # learning rate\n    ep = 0.01 # convergence criteria\n\n    # call gredient decent, and get intercept(=theta0) and slope(=theta1)\n    theta0, theta1 = gradient_descent(alpha, x, y, ep, max_iter=1000)\n    print ('theta0 = {} theta1 = {}'.format(theta0, theta1))\n\n    # check with scipy linear regression \n    slope, intercept, r_value, p_value, slope_std_error = stats.linregress(x[:,0], y)\n    print ('intercept ={} slope ={}'.format(intercept, slope))","a3de2892":"    # plot\n    for i in range(x.shape[0]):\n        y_predict = theta0 + theta1*x \n\n    pylab.plot(x,y,'o')\n    pylab.plot(x,y_predict,'k-')\n    pylab.show()","b993bfc2":"imports ","9a0dde09":"define function for gradient descent","c5187e29":"plot the function"}}