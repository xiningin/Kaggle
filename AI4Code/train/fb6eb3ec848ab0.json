{"cell_type":{"8b5d3fb3":"code","1f8ed583":"code","e6f868cc":"code","5e3a9168":"code","e9aeb092":"code","d4807bed":"code","f4f2f78a":"code","28c830c9":"code","6c0ba0e9":"code","8d20f903":"code","8fae1cdc":"code","403a5da8":"code","f833ba16":"code","f877d779":"code","5692303f":"code","a58ef2a5":"markdown","9ced7ac4":"markdown","0807fa50":"markdown","65fb6d26":"markdown","bf55df16":"markdown","89cc8cd1":"markdown","7f497645":"markdown","c34cc3c8":"markdown","45fa2d5e":"markdown","9c3fadf2":"markdown","ee21626f":"markdown"},"source":{"8b5d3fb3":"import warnings\nimport os\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.linear_model import Ridge\nfrom functools import partial\nprint(os.listdir(\"..\/input\"))","1f8ed583":"# Loading training data\nbase_df = pd.read_csv(\"..\/input\/train.csv\")\nbase_df.shape","e6f868cc":"df = pd.read_csv(\"..\/input\/train.csv\")\ndf.groupby(\"ExterQual\")[\"SalePrice\"].mean().plot.bar()","5e3a9168":"def transform_rating_to_number(label):\n    return {\"Ex\": 5, \"Gd\": 4, \"TA\": 3, \"Fa\": 2, \"Po\": 1}.get(label, 0)\n\ndf = pd.read_csv(\"..\/input\/train.csv\")\ndf[\"ExterQual\"] = df[\"ExterQual\"].apply(transform_rating_to_number)\ndf.groupby(\"ExterQual\")[\"SalePrice\"].mean().plot.bar()","e9aeb092":"def transform_rating_to_number(label):\n    mapping = {\"Ex\": 5, \"Gd\": 4, \"TA\": 3, \"Fa\": 2, \"Po\": 1}\n    return mapping.get(label, 0)\n\n\ndef transform_to_number_bsmt_exposure(label):\n    mapping = {\"Gd\": 5, \"Av\": 4, \"Mn\": 3, \"No\": 2, \"NA\": 1}\n    return mapping.get(label, 0)\n\n\ndef transform_to_boolean(value_for_ok, feature):\n    return 1 if value_for_ok == feature else 0\n\n\ndef transform_to_garage_finish(label):\n    mapping = {\"Fin\": 3, \"RFn\": 2, \"Unf\": 1, \"NA\": 0}\n    return mapping.get(label, 0)\n\n\ndef transform_paved_drive(label):\n    mapping = {\"Y\": 2, \"P\": 1, \"N\": 0}\n    return mapping.get(label, 0)\n\n\ndef transform_sale_type(label):\n    mapping = {\"New\": 2, \"WD\": 1, \"NAN\": 0}\n    return mapping.get(label, 0)\n\n\ndef transform_sale_condition(label):\n    mapping = {\"Partial\": 3, \"Normal\": 2, \"Abnorml\": 1, \"NAN\": 0}\n    return mapping.get(label, 0)\n\n\ndef transform_data(_df):\n    _df[\"SaleCondition\"] = _df[\"SaleCondition\"].apply(transform_sale_condition)\n    _df[\"FireplaceQu\"] = _df[\"FireplaceQu\"].apply(transform_rating_to_number)\n    _df[\"KitchenQual\"] = _df[\"KitchenQual\"].apply(transform_rating_to_number)\n    _df[\"HeatingQC\"] = _df[\"HeatingQC\"].apply(transform_rating_to_number)\n    _df[\"BsmtQual\"] = _df[\"BsmtQual\"].apply(transform_rating_to_number)\n    _df[\"BsmtCond\"] = _df[\"BsmtCond\"].apply(transform_rating_to_number)\n    _df[\"ExterQual\"] = _df[\"ExterQual\"].apply(transform_rating_to_number)\n    _df[\"BsmtExposure\"] = _df[\"BsmtExposure\"].apply(transform_to_number_bsmt_exposure)\n    _df[\"GarageFinish\"] = _df[\"GarageFinish\"].apply(transform_to_garage_finish)\n    _df[\"Foundation\"] = _df[\"Foundation\"].apply(partial(transform_to_boolean, \"PConc\"))\n    _df[\"CentralAir\"] = _df[\"CentralAir\"].apply(partial(transform_to_boolean, \"Y\"))\n    _df[\"PavedDrive\"] = _df[\"PavedDrive\"].apply(transform_paved_drive)\n    _df[\"SaleType\"] = _df[\"SaleType\"].apply(transform_sale_type)\n\n    _df[\"GarageCond\"] = _df[\"GarageCond\"].apply(transform_rating_to_number)\n    _df[\"GarageQual\"] = _df[\"GarageQual\"].apply(transform_rating_to_number)\n    _df[\"ExterCond\"] = _df[\"ExterCond\"].apply(transform_rating_to_number)\n\n    return _df","d4807bed":"plt.scatter(df[\"LotFrontage\"], df[\"SalePrice\"])\nplt.scatter(df[df[\"LotFrontage\"] > 250][\"LotFrontage\"], df[df[\"LotFrontage\"] > 250][\"SalePrice\"], color=\"red\")","f4f2f78a":"def filter_numerical_data_for_training(_df):\n    _df = _df[_df.Id != 496]  # OpenPorchSF outlier\n\n    _df = _df[_df[\"LotFrontage\"] < 250]\n    _df = _df[_df[\"LotArea\"] < 100000]\n    _df = _df[_df[\"TotalBsmtSF\"] < 3100]\n\n    _df = _df[_df[\"GarageArea\"] < 1200]\n    _df = _df[_df[\"MasVnrArea\"] < 1300]\n    _df = _df[_df[\"EnclosedPorch\"] < 500]\n\n    return _df\n\n\ndef transform_numerical_data(_df):\n    _df[\"TotalIndorArea\"] = _df[\"1stFlrSF\"] + _df[\"2ndFlrSF\"]\n    return _df","28c830c9":"def remove_unused_features(_df):\n    _DELETE = [\"Street\", \"Alley\", \"LotShape\", \"LandContour\", \"Utilities\", \"LotConfig\", \"LandSlope\",\n               \"Condition1\", \"Condition2\", \"BldgType\", \"HouseStyle\", \"RoofStyle\", \"RoofMatl\",\n               \"Exterior1st\", \"Exterior2nd\", \"MasVnrType\", \"BsmtFinType1\", \"BsmtFinType2\",\n               \"Heating\", \"Electrical\", \"PoolQC\", \"Fence\", \"MiscFeature\", \"Functional\"]\n    _DELETE_2 = [\"GarageType\", \"MSZoning\", \"Neighborhood\"]\n\n    _NUM_DELETE = [\"MSSubClass\", \"BsmtUnfSF\", \"1stFlrSF\", \"2ndFlrSF\", \"LowQualFinSF\",\n                   \"BsmtFullBath\", \"BsmtHalfBath\", \"KitchenAbvGr\", \"3SsnPorch\", \"ScreenPorch\",\n                   \"PoolArea\", \"MiscVal\", \"MoSold\", \"YrSold\"]\n    _NUM_DELETE_2 = [\"BsmtFinSF1\", \"BsmtFinSF2\", \"HalfBath\", \"BedroomAbvGr\", \"WoodDeckSF\"]\n\n    _df = _df.drop(_DELETE + _DELETE_2 + _NUM_DELETE + _NUM_DELETE_2, 1)\n    return _df","6c0ba0e9":"df = pd.read_csv(\"..\/input\/train.csv\")\ndf.SalePrice.hist(bins=100)","8d20f903":"np.log(df.SalePrice).hist(bins=100)","8fae1cdc":"# Wrapper function to apply cross validation\ndef evaluate_model_cv(_df, _sale_price, _model):\n    scores = -1 * cross_val_score(_model, _df, _sale_price, cv=5, scoring=\"neg_mean_squared_error\")\n    return scores.mean()","403a5da8":"#Read data set and apply feature engineering\ndf = pd.read_csv(\"..\/input\/train.csv\")\n# fill missing values with 0\ndf = df.fillna(0)\n\ndf = transform_data(df)\ndf = transform_numerical_data(df)\ndf = filter_numerical_data_for_training(df)\ndf = remove_unused_features(df)\n\nsale_price = np.log(df[\"SalePrice\"])\ndf = df.drop([\"Id\", \"SalePrice\"], 1)","f833ba16":"print(\"LinearRegression|{:.10f}\".format(evaluate_model_cv(df, sale_price, LinearRegression())))","f877d779":"alphas = np.linspace(0, 30, num=100)\ntuned_parameters = [{'alpha': alphas}]\nclf = GridSearchCV(Ridge(), tuned_parameters, cv=5, scoring=\"neg_mean_squared_error\")\nclf.fit(df, sale_price)\nprint(\"Ridge|best_score:{}\".format(-clf.best_score_))\nprint(\"Ridge|best_estimator_:{}\".format(clf.best_estimator_.alpha))","5692303f":"ridge_final_model = Ridge(alpha=clf.best_estimator_.alpha)\nridge_final_model.fit(df, sale_price)\n\ndf_test = pd.read_csv(\"..\/input\/test.csv\")\n# Test data indexes\ntest_indexes = df_test.Id\n\n# Transform testing data \ndf_test = transform_data(df_test)\ndf_test = transform_numerical_data(df_test)\ndf_test = remove_unused_features(df_test)\ndf_test = df_test.drop([\"Id\"], 1)\ndf_test = df_test.fillna(0)\n\n# get predictions\npredictions = ridge_final_model.predict(df_test)\n# predictions are made on np.log(SalePrice) so transform it back by using np.exp\npredictions = np.exp(predictions)\n\n# Print results into a file\nprediction = pd.DataFrame({\"SalePrice\": predictions})\nprediction[\"Id\"] = test_indexes\nprediction.to_csv(\"solution.csv\", index=False, columns=[\"Id\", \"SalePrice\"])\nprint(prediction.head())","a58ef2a5":"## Let's try to improve the prediction by using regularization.\n\n### I use `GridSearchCV` to find the best value for parameter `alpha`","9ced7ac4":"# Analysis of non-numerical features.\n\n### Let's see how ExterQual(Exterior material quality) looks like","0807fa50":"### This approach can be applied to many feature as follows:","65fb6d26":"### We can see that this can be sorted in the following format: `Po > Fa > Gd > Gd > Ex` and make it perfectly correlated to the *SalePrice*\n\n","bf55df16":"# Analysis of numerical features.\n\n### By plotting the relatationship between features and SalePrice we can detect outliers.\n\n### Here an example using LotFrontage(Linear feet of street connected to property)","89cc8cd1":"### Let's use a base model and see the result:","7f497645":"## We can make it look \"normal\" by applying `np.log`\n\n### This process can be reverted by applying `np.exp`","c34cc3c8":"# Study SalePrice feature\nWe can see that it is skewed to the right","45fa2d5e":"### We can see that by filtering every sample that has LotFrontage higher than 250 we can remove the outliers(points in red).\n\n### Same approach is applied to other numerical features as follows:","9c3fadf2":"## No useful features\n\n### After analysing all features I found that many of them didn't provide much value, so I remove them","ee21626f":"# In this notebook I'm going to be solving [House Prices](https:\/\/www.kaggle.com\/c\/house-prices-advanced-regression-techniques)\n\nIn other to improve results I do feature engineering, remove outliers, use regularization and cross validation\n\nI also use [GridSearchCV](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.model_selection.GridSearchCV.html#sklearn.model_selection.GridSearchCV) to find the optimum value of `alpha` of a [Ridge](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.linear_model.Ridge.html) model\n\nThis problem is evaluated using [Mean-Squared-Error(MSE)](https:\/\/en.wikipedia.org\/wiki\/Root-mean-square_deviation), with final score of **0.13493**"}}