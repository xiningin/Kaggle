{"cell_type":{"fe7b7896":"code","d6e8ef01":"code","f5014c26":"code","be013daa":"code","199fc552":"code","40fe1052":"code","efdd6bbd":"code","9bc174fb":"code","6e62d1b3":"code","f10434dc":"code","01c6e447":"code","74bc8696":"code","1f2ef08d":"code","48cd26ae":"markdown","384d5f91":"markdown","0c399ff7":"markdown","1586add7":"markdown","f84a8afa":"markdown","ae842eb3":"markdown","d99f7d0d":"markdown","72e2c6b0":"markdown","7fa94c8a":"markdown","b180d0e9":"markdown","4ffb554e":"markdown","ef58bc67":"markdown"},"source":{"fe7b7896":"#Ignoring the warnings\nimport warnings\nwarnings.filterwarnings('ignore')\n\n#Importing the required libraries\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport re, string, unicodedata\nimport nltk\nfrom nltk import word_tokenize, sent_tokenize\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.models import Sequential\nfrom keras.layers.pooling import GlobalMaxPooling1D\nfrom keras.callbacks import EarlyStopping, ModelCheckpoint\nfrom keras.models import load_model\nfrom keras.layers import *\nfrom keras import backend\nfrom sklearn.metrics import f1_score, confusion_matrix","d6e8ef01":"#Importing the dataset\n\ndataset = pd.read_csv('..\/input\/imdb_master.csv', encoding = \"ISO-8859-1\")\ndataset.head()","f5014c26":"#Splitting into training and test set\ndataset = dataset.drop(['Unnamed: 0', 'file'], axis = 1)\ndataset = dataset[dataset.label != 'unsup']\ndataset['label'] = dataset['label'].map({'pos': 1, 'neg': 0})\ndataset_test = dataset[dataset['type'] == 'test']\ndataset_train = dataset[dataset['type'] == 'train']\nX_test = dataset_test.iloc[:, 1:2].values\ny_test = dataset_test.iloc[:, 2].values\nX_train = dataset_train.iloc[:, 1:2].values\ny_train = dataset_train.iloc[:, 2].values","be013daa":"#Function for Text Preprocessing\nstop_words = set(stopwords.words(\"english\")) \nlemmatizer = WordNetLemmatizer()\n\ndef clean_text(X):\n    processed = []\n    for text in X:\n        text = text[0]\n        text = re.sub(r'[^\\w\\s]','',text, re.UNICODE)\n        text = re.sub('<.*?>', '', text)\n        text = text.lower()\n        text = [lemmatizer.lemmatize(token) for token in text.split(\" \")]\n        text = [lemmatizer.lemmatize(token, \"v\") for token in text]\n        text = [word for word in text if not word in stop_words]\n        text = \" \".join(text)\n        processed.append(text)\n    return processed","199fc552":"X_train_final = clean_text(X_train)\nX_test_final = clean_text(X_test)","40fe1052":"# Attention Layer\nclass AttentionWithContext(Layer):\n    \"\"\"\n    Attention operation, with a context\/query vector, for temporal data.\n    Supports Masking.\n    Follows the work of Yang et al. [https:\/\/www.cs.cmu.edu\/~diyiy\/docs\/naacl16.pdf]\n    \"Hierarchical Attention Networks for Document Classification\"\n    by using a context vector to assist the attention\n    # Input shape\n        3D tensor with shape: `(samples, steps, features)`.\n    # Output shape\n        2D tensor with shape: `(samples, features)`.\n    How to use:\n    Just put it on top of an RNN Layer (GRU\/LSTM\/SimpleRNN) with return_sequences=True.\n    The dimensions are inferred based on the output shape of the RNN.\n    Note: The layer has been tested with Keras 2.0.6\n    Example:\n        model.add(LSTM(64, return_sequences=True))\n        model.add(AttentionWithContext())\n        # next add a Dense layer (for classification\/regression) or whatever...\n    \"\"\"\n\n    def __init__(self, W_regularizer=None, u_regularizer=None, b_regularizer=None,\n                 W_constraint=None, u_constraint=None, b_constraint=None,\n                 bias=True, **kwargs):\n\n        self.supports_masking = True\n        self.init = initializers.get('glorot_uniform')\n\n        self.W_regularizer = regularizers.get(W_regularizer)\n        self.u_regularizer = regularizers.get(u_regularizer)\n        self.b_regularizer = regularizers.get(b_regularizer)\n\n        self.W_constraint = constraints.get(W_constraint)\n        self.u_constraint = constraints.get(u_constraint)\n        self.b_constraint = constraints.get(b_constraint)\n\n        self.bias = bias\n        super(AttentionWithContext, self).__init__(**kwargs)\n\n    def build(self, input_shape):\n        assert len(input_shape) == 3\n\n        self.W = self.add_weight((input_shape[-1], input_shape[-1],),\n                                 initializer=self.init,\n                                 name='{}_W'.format(self.name),\n                                 regularizer=self.W_regularizer,\n                                 constraint=self.W_constraint)\n        if self.bias:\n            self.b = self.add_weight((input_shape[-1],),\n                                     initializer='zero',\n                                     name='{}_b'.format(self.name),\n                                     regularizer=self.b_regularizer,\n                                     constraint=self.b_constraint)\n\n        self.u = self.add_weight((input_shape[-1],),\n                                 initializer=self.init,\n                                 name='{}_u'.format(self.name),\n                                 regularizer=self.u_regularizer,\n                                 constraint=self.u_constraint)\n\n        super(AttentionWithContext, self).build(input_shape)\n\n    def compute_mask(self, input, input_mask=None):\n        # do not pass the mask to the next layers\n        return None\n\n    def call(self, x, mask=None):\n        uit = dot_product(x, self.W)\n\n        if self.bias:\n            uit += self.b\n\n        uit = K.tanh(uit)\n        ait = dot_product(uit, self.u)\n\n        a = K.exp(ait)\n\n        # apply mask after the exp. will be re-normalized next\n        if mask is not None:\n            # Cast the mask to floatX to avoid float64 upcasting in theano\n            a *= K.cast(mask, K.floatx())\n\n        # in some cases especially in the early stages of training the sum may be almost zero\n        # and this results in NaN's. A workaround is to add a very small positive number \u03b5 to the sum.\n        # a \/= K.cast(K.sum(a, axis=1, keepdims=True), K.floatx())\n        a \/= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n\n        a = K.expand_dims(a)\n        weighted_input = x * a\n        return K.sum(weighted_input, axis=1)\n\n    def compute_output_shape(self, input_shape):\n        return input_shape[0], input_shape[-1]\n\ndef dot_product(x, kernel):\n    \"\"\"\n    Wrapper for dot product operation, in order to be compatible with both\n    Theano and Tensorflow\n    Args:\n        x (): input\n        kernel (): weights\n    Returns:\n    \"\"\"\n    if K.backend() == 'tensorflow':\n        return K.squeeze(K.dot(x, K.expand_dims(kernel)), axis=-1)\n    else:\n        return K.dot(x, kernel)","efdd6bbd":"#Tokenization and Padding\nvocab_size = 60000\nmaxlen = 250\nencode_dim = 20\nbatch_size = 32\ntokenizer = Tokenizer()\ntokenizer.fit_on_texts(X_train_final)\ntokenized_word_list = tokenizer.texts_to_sequences(X_train_final)\nX_train_padded = pad_sequences(tokenized_word_list, maxlen = maxlen, padding='post')","9bc174fb":"#EarlyStopping and ModelCheckpoint\n\nes = EarlyStopping(monitor = 'val_loss', mode = 'min', verbose = 1, patience = 5)\nmc = ModelCheckpoint('model_best.h5', monitor = 'val_acc', mode = 'max', verbose = 1, save_best_only = True)","6e62d1b3":"#Building the model\nmodel = Sequential()\nembed = Embedding(input_dim = vocab_size, output_dim = 20, input_length = X_train_padded.shape[1], dropout = 0.4) \nmodel.add(embed)\nmodel.add(Bidirectional(CuDNNLSTM(200, return_sequences = True)))\nmodel.add(Dropout(0.3))\nmodel.add(AttentionWithContext())\nmodel.add(Dropout(0.5))\nmodel.add(Dense(512))\nmodel.add(LeakyReLU(alpha=0.2))\nmodel.add(Dense(1, activation = 'sigmoid'))\nmodel.compile(loss = 'binary_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\nmodel.summary()","f10434dc":"from sklearn.model_selection import train_test_split\nX_train_final2, X_val, y_train_final2, y_val = train_test_split(X_train_padded, y_train, test_size = 0.2)","01c6e447":"#Fitting the model\nmodel.fit(X_train_final2, y_train_final2, epochs = 50, batch_size = batch_size, verbose = 1, validation_data = [X_val, y_val], callbacks = [es, mc])","74bc8696":"#Padding the test data\ntokenized_word_list_test = tokenizer.texts_to_sequences(X_test_final)\nX_test_padded = pad_sequences(tokenized_word_list_test, maxlen = maxlen, padding = 'post')","1f2ef08d":"#Evaluating the model\nfrom keras.models import load_model\nmodel = load_model('model_best.h5', custom_objects = {\"AttentionWithContext\" : AttentionWithContext, \"backend\" : backend})\nscore, acc = model.evaluate(X_test_padded, y_test)\nprint('The accuracy of the model on the test set is ', acc*100)\nprediction = model.predict(X_test_padded)\ny_pred = (prediction > 0.5)\nprint('F1-score: ', (f1_score(y_pred, y_test)*100))\nprint('Confusion matrix:')\nprint(confusion_matrix(y_pred, y_test))","48cd26ae":"### Training\n***\nSplitting the Training set into Training set and Validation set","384d5f91":"Training the model","0c399ff7":"### Attention Layer\n***\n\nThe basic concept of attention is that not all words contribute equally to the meaning of a sentence. Hence, their contribution must be weighted.  \nHow attention works is, it basically extracts words that are important to the meaning of the sentence and aggregate the representation of those informative words to form a sentence vector.","1586add7":"### Importing the dataset\n***\n\nThe dataset: 'imdb_master.csv' is read and loaded as pandas dataframe.  \nLet's have a look at the data","f84a8afa":"### Building the Model\n***\nThe model used comprises of BiDirectional LSTM with Attention layer on top of it, followed by a dense layer and finally a dense layer with sigmoid activation function to get the sentiment or the class.  \nOptimiser used is ADAM","ae842eb3":"# IMDB Review\nGiven a IMDB movie review, classify it as positive or negative. Basically, it is a sentiment analysis.  \n\nFor doing so, I have used Bidirectional LSTM with Attention for better understanding the context of the review.  \n\nThis notebook is divided into the following sections:\n* Importing the libraries\n* Importing the dataset\n* Text Preprocessing\n* Attention\n* Building the model\n* Training\n* Testing\n***\n### Importing the libraries\nThe cell below is for importing the required libraries and for silencing the warnings","d99f7d0d":"Preprocessing the Training Set and Test set","72e2c6b0":"### Text Preprocessing\n***\nPreprocessing the text so as to have a better data for our model.  \nIt comprises of steps such as removing non-ASCII characters, removing HTML tags, converting to lower-case, lemmatizing.","7fa94c8a":"Some Useful Variables  \n","b180d0e9":"The columns which are not essential are removed.   \nUnlabeled reviews are removed and the class names are converted to numerical digits: 1 for positive and 0 for negative.\nThe dataset is now split into Training set and Test set","4ffb554e":"### Testing\n***\nConverting the test data into sequences of integers and padding them.  \nLoading the best model and calculating the accuracy","ef58bc67":"**EarlyStopping**  \nIt can be used to prevent overfitting.It basically waits a few epochs (5), monitoring the loss for the validation dataset.If the loss doesn't decrease for 2 epochs, it stops the training process.\n\n**ModelCheckpoint**  \nIt is used for saving the best model during training. After each epoch, it takes a look at the Validation accuracy, if it improves globally, this is the best model we have seen till now during the training process and hence, saves it."}}