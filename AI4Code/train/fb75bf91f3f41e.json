{"cell_type":{"6d31b86f":"code","98a38735":"code","9d1c1b36":"code","4b77378b":"code","2400c2bc":"code","7cfeaadd":"code","7734b7a3":"code","874fe706":"code","fb479661":"code","0af3215e":"code","bf84e400":"code","ea352ca5":"code","51cae631":"code","d79a3525":"code","671410ba":"code","8a07689a":"code","49a9a07c":"code","7dc0ae7e":"markdown","be1f19d2":"markdown","014ff51f":"markdown","e947a898":"markdown","14cb176f":"markdown","88dd89f6":"markdown","ff5a16eb":"markdown","7416a209":"markdown","b3a7b743":"markdown","ff74dd79":"markdown","fc9450ec":"markdown","2242d97a":"markdown","12bf1679":"markdown","58aa080d":"markdown","29bc6b5d":"markdown","8787dbd4":"markdown","f5aba034":"markdown","999af8d9":"markdown","3cd7ee49":"markdown","09844f60":"markdown","e6776073":"markdown"},"source":{"6d31b86f":"import nltk","98a38735":"from nltk.tokenize import word_tokenize,sent_tokenize\n\nexample = \"Hi there!! How are you doing. I am doing just fine. Hope you like this kernel.\"\n# tokenizing each word in example text\nprint(word_tokenize(example))","9d1c1b36":"# tokenizing each sentence in example text\nprint(sent_tokenize(example))","4b77378b":"from nltk.corpus import stopwords\n\nSTOPWORDS = stopwords.words('english')\nprint(STOPWORDS)","2400c2bc":"example = \"Hi there I am Sarthak Rana!! How are you doing. I am doing just fine. Hope you learn a lot from this kernel.\"\n\n#convert to lowercase\nexample = example.lower()\nprint(\"Length of text before removing stopwords : {0}\".format(len(example.split())))\n\n# remove stopwords\n[word for word in example.split() if word not in STOPWORDS]","7cfeaadd":"print(\"Length of text after removing stopwords : {0}\".format(len([word for word in example.split() if word not in STOPWORDS])))","7734b7a3":"example = [\"Hi there I am Sarthak Rana!! How are you doing. I am doing just fine. Hope you like this kernel.\", \n           \"Today was a very great day. I got promoted. Might surprise my girl with a present :P\"]\n\nlowercase_example = []\nfor text in example:\n    lowercase_example.append(text.lower())\n\n[[word for word in sentence.split() if word not in STOPWORDS] for sentence in lowercase_example]","874fe706":"from nltk import pos_tag","fb479661":"# with single example\nprint(\"WITH SINGLE TEXT\\n\")\nexample = \"John is an intelligent individual. He intelligently does smart work. He is a top performer at Google.\"\nexample = word_tokenize(example)\nprint(pos_tag(example))","0af3215e":"# With multiple examples\nprint(\"\\nWITH MULTIPLE TEXTS\\n\")\nexample_list = [\"Hi there I am Sarthak Rana!! How are you doing. I am doing just fine. Hope you like this kernel.\", \n               \"Today was a very great day. I got promoted. Might surprise my girl with a present!!\"]\nfor example in example_list:\n    example = word_tokenize(example)\n    print(pos_tag(example))","bf84e400":"from nltk.stem import PorterStemmer\n\nstemmer = PorterStemmer()\ntext = \"John is an intelligent individual. He intelligently does smart work. He is a top performer in the company.\"\n[stemmer.stem(word) for word in text.split()]","ea352ca5":"from nltk.stem import WordNetLemmatizer\n\nlemmatizer = WordNetLemmatizer()\nexample = \"John is an intelligent individual. He intelligently does smart work. He is a top performer at Google.\"\n[lemmatizer.lemmatize(word) for word in word_tokenize(example)]","51cae631":"lemmatizer = WordNetLemmatizer()\nexample = \"John is an intelligent individual. He intelligently does smart work. He is a top performer at Google.\"\nexample = word_tokenize(example)\nlemmatized_tokens = []\nfor token, tag in pos_tag(example):\n    if tag.startswith('NN'):\n        pos = 'n'\n    elif tag.startswith('VB'):\n        pos = 'v'\n    else:\n        pos = 'a' \n    lemmatized_tokens.append(lemmatizer.lemmatize(token, pos))","d79a3525":"lemmatized_tokens","671410ba":"from nltk.tokenize import PunktSentenceTokenizer\n\n# Now, let's create our training and testing data:\ntrain_txt=\"Crocodiles (subfamily Crocodylinae) or true crocodiles are large aquatic reptiles that live throughout the tropics in Africa, Asia, the Americas and Australia. Crocodylinae, all of whose members are considered true crocodiles, is classified as a biological subfamily. A broader sense of the term crocodile, Crocodylidae that includes Tomistoma, is not used in this article. The term crocodile here applies to only the species within the subfamily of Crocodylinae. The term is sometimes used even more loosely to include all extant members of the order Crocodilia, which includes the alligators and caimans (family Alligatoridae), the gharial and false gharial (family Gavialidae), and all other living and fossil Crocodylomorpha.\"\nsample_text =\"Crocodiles are large aquatic reptiles which are carnivorous.Allegators belong to this same reptile species\"\n\n# Next, we can train the Punkt tokenizer like:\ncust_tokenizer = PunktSentenceTokenizer(train_txt)\n\n# Then we can actually tokenize, using:\n\ntokenized = cust_tokenizer.tokenize(sample_text)\nprint(\"Chunked Output\")\ndef process_text():\n    try:\n        for i in tokenized:\n            words = nltk.word_tokenize(i)\n            tagged = nltk.pos_tag(words)\n            chunkGram = r\"\"\"Chunk:{<NNS.?>*<JJ>+}\"\"\"\n            chunkParser = nltk.RegexpParser(chunkGram)\n            chunked = chunkParser.parse(tagged)\n            print(chunked)\n\n    except Exception as e:\n        print(str(e))\n\nprocess_text()","8a07689a":"# Now, let's create our training and testing data:\ntrain_txt=\"Crocodiles (subfamily Crocodylinae) or true crocodiles are large aquatic reptiles that live throughout the tropics in Africa, Asia, the Americas and Australia. Crocodylinae, all of whose members are considered true crocodiles, is classified as a biological subfamily. A broader sense of the term crocodile, Crocodylidae that includes Tomistoma, is not used in this article. The term crocodile here applies to only the species within the subfamily of Crocodylinae. The term is sometimes used even more loosely to include all extant members of the order Crocodilia, which includes the alligators and caimans (family Alligatoridae), the gharial and false gharial (family Gavialidae), and all other living and fossil Crocodylomorpha.\"\nsample_text =\"Crocodiles are large aquatic reptiles which are carnivorous.Allegators belong to this same reptile species\"\n\n# Next, we can train the Punkt tokenizer like:\ncust_tokenizer = PunktSentenceTokenizer(train_txt)\n\n# Then we can actually tokenize, using:\n\ntokenized = cust_tokenizer.tokenize(sample_text)\n\nprint(\"Chinked Output\")\ndef process_text():\n    try:\n        for i in tokenized:\n            words = nltk.word_tokenize(i)\n            tagged = nltk.pos_tag(words)\n            chunkGram = r\"\"\"Chunk: {<.*>+}\n                                    }<VB.?|IN|DT|TO>+{\"\"\"\n            chunkParser = nltk.RegexpParser(chunkGram)\n            chunked = chunkParser.parse(tagged)\n            print(chunked)\n\n    except Exception as e:\n        print(str(e))\n\nprocess_text()","49a9a07c":"# Now, let's create our training and testing data:\ntrain_txt=\"Crocodiles (subfamily Crocodylinae) or true crocodiles are large aquatic reptiles that live throughout the tropics in Africa, Asia, the Americas and Australia. Crocodylinae, all of whose members are considered true crocodiles, is classified as a biological subfamily. A broader sense of the term crocodile, Crocodylidae that includes Tomistoma, is not used in this article. The term crocodile here applies to only the species within the subfamily of Crocodylinae. The term is sometimes used even more loosely to include all extant members of the order Crocodilia, which includes the alligators and caimans (family Alligatoridae), the gharial and false gharial (family Gavialidae), and all other living and fossil Crocodylomorpha.\"\nsample_text =\"Crocodiles are large aquatic reptiles which are carnivorous.Allegators belong to this same reptile species\"\n\n# Next, we can train the Punkt tokenizer like:\ncust_tokenizer = PunktSentenceTokenizer(train_txt)\n\n# Then we can actually tokenize, using:\n\ntokenized = cust_tokenizer.tokenize(sample_text)\n\nprint(\"Named Entity Output\")\ndef process_text():\n    try:\n        for i in tokenized:\n            words = nltk.word_tokenize(i)\n            tagged = nltk.pos_tag(words)\n            namedEnt = nltk.ne_chunk(tagged,binary = True)\n            print(namedEnt)\n\n    except Exception as e:\n        print(str(e))\n\nprocess_text()","7dc0ae7e":"![image.png](attachment:image.png)","be1f19d2":"### 7. Lemmatization\nIt is same as stemming process but the intermediate representation\/root has a meaning.It is also a preprocessing step in natural language processing.\n\nExamples: Words like\n\n- **going ,goes,gone** - when we do lemmatization we get \"go\"\n- **intelligence,intelligently** - when we do lemmatization we get \"intelligent\".\n\nSo lemmatization produces intermediate representation of the word which has a meaning.In this case \"intelligent\" has meaning.","014ff51f":"### 3. Tokenizing Words & Sentences\n\nTokenization is the process of breaking up the given text into units called tokens. The tokens may be words or number or punctuation mark or even sentences. Tokenization does this task by locating word boundaries. Ending point of a word and beginning of the next word is called word boundaries. Tokenization is also known as word segmentation.\n\nChallenges in tokenization depends on the type of language. Languages such as English and French are referred to as space-delimited as most of the words are separated from each other by white spaces. Languages such as Chinese and Thai are referred to as unsegmented as words do not have clear boundaries. Tokenising unsegmented language sentences requires additional lexical and morphological information. Tokenization is also affected by writing system and the typographical structure of the words. Structures of languges can be grouped into three categories:\n\n**Isolating**: Words do not divide into smaller units. Example: Mandarin Chinese\n\n**Agglutinative**: Words divide into smaller units. Example: Japanese, Tamil\n\n**Inflectional**: Boundaries between morphemes are not clear and ambiguous in terms of grammatical meaning. Example: Latin.\n\nLet us understand some more basic terminology.\n\n**What is Corpora?**<br>\nIt is a body of text e.g Medical journal, Presidential speech, English language\n\n**What is Lexicon?**<br>\nLexicon is nothing but words and their means .E.g Investor speak vs. Regular English speak\n\ni.e Investor talk about \"BULL\" as some stock going positive in the market which bullish as to the regular word of \"BULL\" describing the usual animal.\n\nSo in simple for now let us look at Word Tokenizer and Sentence Tokenizer using NLTK.","e947a898":"# Reader's Note\n\n1. This notebook is divided into two parts. Welcome to Pt.1.\n\n2. After reading this notebook, you\u2019ll know some basic techniques to extract features from some text, so you can use these features as input for machine learning models.\n\n2. As the title suggest, this notebook will provide a walkthrough on all basic concepts of NLP that everyone who is stepping into or has already stepped into NLP, should know.\n\n3. If you are just a beginner in NLP,  this is the perfect notebook to start from. \n\n4. If you are already somewhat familiar to NLP, then this is a good notebook to brush up ypur concepts.\n\n5. I will be using the very famous **NLTK** library throughout this notebook.\n\n**<font color = 'red'>If you like this kernel \ud83d\ude0a then please show some love \u2764\ufe0f and upvote it \ud83d\udc4d. It really motivates me to work even harder and bring you guys more such quality content.<\/font>**","14cb176f":"# What is NLP?\n\nNatural language processing (NLP) is a subfield of linguistics, computer science, information engineering, and artificial intelligence concerned with the interactions between computers and human (natural) languages, in particular how to program computers to process and analyze large amounts of natural language data.\n\nNatural Language Processing, or NLP for short, is broadly defined as the automatic manipulation of natural language, like speech and text, by software.\n\nSome Examples :\n- Cortana\n- Siri\n- Gmail (Spam filtering)\n\nThe study of natural language processing has been around for more than 50 years and grew out of the field of linguistics with the rise of computers.\n\n**Before we get into details of NLP first let us try to answer the below question**\n\n### So...How is NLP different from other types of data?\n\nNatural language refers to the way we, humans, communicate with each other namely speech and text.We are surrounded by text. Think about how much text you see each day:\n\n- Signs<br>\n- Menus<br>\n- Email<br>\n- SMS<br>\n- Web Pages<br><br>\nand so much more\u2026\n\nThe list is endless.\n\nNow think about speech.We may speak to each other, as a species, more than we write. It may even be easier to learn to speak than to write.Voice and text are how we communicate with each other. Given the importance of this type of data, we must have methods to understand and reason about natural language, just like we do for other types of data.","88dd89f6":"### 2. Importing the relevant library\n\nAs mentioned, we will be using the very famous NLTK library. So let's import it first of all","ff5a16eb":"### 9. Chinking\n\nYou may find that, after a lot of chunking, you have some words in your chunk you still do not want, but you have no idea how to get rid of them by chunking. You may find that chinking is your solution.\n\nChinking is a lot like chunking, it is basically a way for you to remove a chunk from a chunk. The chunk that you remove from your chunk is your chink.\n\nThe code is very similar, you just denote the chink, after the chunk, with }...{ instead of the chunk's {...}","7416a209":"### 1. Introduction to NLTK\n\nNLTK is a leading platform for building Python programs to work with human language data. It provides easy-to-use interfaces to over 50 corpora and lexical resources such as WordNet, along with a suite of text processing libraries for classification, tokenization, stemming, tagging, parsing, and semantic reasoning, wrappers for industrial-strength NLP libraries, and an active discussion forum.\n\nNLTK has been called \u201ca wonderful tool for teaching, and working in, computational linguistics using Python,\u201d and \u201can amazing library to play with natural language.\u201d\n\nIn our path to learning how to do sentiment analysis with NLTK, we're going to learn the following:\n\n- Tokenizing - Splitting sentences and words from the body of text.\n- Part of Speech tagging\n- Machine Learning with the Naive Bayes classifier\n- How to tie in Scikit-learn (sklearn) with NLTK\n- Training classifiers with datasets\n- Performing live, streaming, sentiment analysis with Twitter.<br>\n<br>...and much more.\n\nSo now, lets get into details of this tutorial...","b3a7b743":"### 5. Part-Of-Speech Tagging (POS Tagging)\n\nOne of the more powerful aspects of the NLTK is the Part of Speech tagging that it can do. This means labeling words in a sentence as nouns, adjectives, verbs...etc. Even more impressive, it also labels by tense, and more. Here's a list of the tags, what they mean, and some examples:\n\nPOS Tag List :\n* CC coordinating conjunction\n* CD cardinal digit\n* DT determiner\n* EX existential there (like: \"there is\" ... think of it like \"there exists\")\n* FW foreign word\n* IN preposition\/subordinating conjunction\n* JJ adjective 'big'\n* JJR adjective, comparative 'bigger'\n* JJS adjective, superlative 'biggest'\n* LS list marker 1)\n* MD modal could, will\n* NN noun, singular 'desk'\n* NNS noun plural 'desks'\n* NNP proper noun, singular 'Harrison'\n* NNPS proper noun, plural 'Americans'\n* PDT predeterminer 'all the kids'\n* POS possessive ending parent\\'s\n* PRP personal pronoun I, he, she\n* PRPdollar possessive pronoun my, his, hers\n* RB adverb very, silently,\n* RBR adverb, comparative better\n* RBS adverb, superlative best\n* RP particle give up\n* TO to go 'to' the store.\n* UH interjection errrrrrrrm\n* VB verb, base form take\n* VBD verb, past tense took\n* VBG verb, gerund\/present participle taking\n* VBN verb, past participle taken\n* VBP verb, sing. present, non-3d take\n* VBZ verb, 3rd person sing. present takes\n* WDT wh-determiner which\n* WP wh-pronoun who, what\n* WPdollar possessive wh-pronoun whose\n* WRB wh-abverb where, when","ff74dd79":"### 11. References :-\n- https:\/\/pythonprogramming.net\/chunking-nltk-tutorial\/\n2. https:\/\/pythonprogramming.net\/chinking-nltk-tutorial\/\n3. https:\/\/pythonprogramming.net\/named-entity-recognition-nltk-tutorial\/\n4. https:\/\/www.guru99.com\/pos-tagging-chunking-nltk.html\n5. https:\/\/towardsdatascience.com\/introduction-to-natural-language-processing-for-text-df845750fb63\n6. **MUST READ **- https:\/\/www.datacamp.com\/community\/tutorials\/stemming-lemmatization-python","fc9450ec":"### 4. Stopwords\n![image.png](attachment:image.png)\n\nStopwords are natural language words which have very little meaning, such as \"and\", \"the\", \"a\", \"an\", and similar words.\n\nBasically during the pre processing of natural language text we eliminate the stopwords as they are redundant and do not convey any meaning insight in the data.\n\nLets have a look at stopwords present in English stopwords corpora...","2242d97a":"<font color = 'red'> REMEMBER ! A POS tagger requires a list of tokens to perform tagging. So always remember to perform tokenization step prior to tagging.<\/font>","12bf1679":"# Wait, what.... ?\n\n### There are 2 things to focus on...\n\n#### <font color = 'blue'> 1. Performing Lemmatization gave the same result as the example text we used.<\/font>\n#### <font color = 'blue'> 2. After going through stemming and lemmatization, you might have observed that these both were supposed to perform the same job -- convert the word to its root\/ canonical form.<\/font>\n\n\n**Coming to the first point..**<br><br>\nThis happened so because Lemmatization requires tokenization and POS tagging as first steps in order to perform conversion. Lemmatization algorithm, before rooting the word, looks at the context in which the word has been used in the text and then roots accordingly. Thus, the resultant words formed after Lemmatization have a meaning (unlike words formed from Stemming which have no meaning).\n\nSo, let's perform Lemmatization again now with the correct approach ...","58aa080d":"### 6. Stemming\nStemming is the process of reducing infected or derived words to their word stem,base or root form. It basically affixes to suffixes and prefixes or to the roots of words known as a lemma.It is also a preprocessing step in natural language processing.\n\nExamples: Words like\n\n- **organise, organising ,organisation** the root of its stem is organis.\n- **intelligence,intelligently** the root of its stem is intelligen.\n\nSo stemming produces intermediate representation of the word which may not have any meaning.In this case \"intelligen\" has no meaning.\n\nThe idea of stemming is a sort of normalizing method. Many variations of words carry the same meaning, other than when tense is involved.\n\nThe reason why we stem is to shorten the lookup, and normalize sentences.\n\nOne of the most popular stemming algorithms is the Porter stemmer, which has been around since 1979.","29bc6b5d":"### 10. Named Entity Recognition\n\nOne of the most major forms of chunking in natural language processing is called \"Named Entity Recognition.\" The idea is to have the machine immediately be able to pull out \"entities\" like people, places, things, locations, monetary figures, and more.\n\nThis can be a bit of a challenge, but NLTK is this built in for us. There are two major options with NLTK's named entity recognition: either recognize all named entities **(binary = True)**, or recognize named entities as their respective type, like people, places, locations, etc. **(binary = False)**\n\nFollowing are the NE in python NLTK library:\n\n- NE Type and Examples<br>\n- ORGANIZATION - Georgia-Pacific Corp., WHO<br>\n- PERSON - Eddy Bonte, President Obama<br>\n- LOCATION - Murray River, Mount Everest<br>\n- DATE - June, 2008-06-29<br>\n- TIME - two fifty a m, 1:30 p.m.<br>\n- MONEY - 175 million Canadian Dollars, GBP 10.40<br>\n- PERCENT - twenty pct, 18.75 %<br>\n- FACILITY - Washington Monument, Stonehenge<br>\n- GPE - South East Asia, Midlothian<br>","8787dbd4":"### 8. Chunking\n\nNow that we know the parts of speech, we can do what is called chunking, and group words into hopefully meaningful chunks. One of the main goals of chunking is to group into what are known as \"noun phrases.\" These are phrases of one or more words that contain a noun, maybe some descriptive words, maybe a verb, and maybe something like an adverb. The idea is to group nouns with the words that are in relation to them.\n\nIn order to chunk, we combine the part of speech tags with regular expressions. Mainly from regular expressions, we are going to utilize the following:\n\n\"+\" = match 1 or more\n\n\"?\" = match 0 or 1 repetitions.\n\n\"*\" = match 0 or MORE repetitions\n\n\".\" = Any character except a new line\n\nThe last things to note is that the part of speech tags are denoted with the \"<\" and \">\" and we can also place regular expressions within the tags themselves, so account for things like \"all nouns\" (<N.*>)\n\nLet's include chunking for noun plural (NNS) and adjective (JJ)...","f5aba034":"# Index\n1. Introduction to NLTK\n2. Importing the relevant library\n3. Tokenizing Words & Sentences\n4. Stopwords\n5. Part of Speech(POS) Tagging  \n6. Stemming Words\n7. Lemmatization \n8. Chunking\n9. Chinking\n10. Named Entity Recognition\n11. References","999af8d9":"#### **There are only 9 words left after removing stopwords compared to 23 in text. This is so as all stopwords(irrelevant words) have been removed from text. We now see how important it is to remove stopwords. Otherwise, our model would have trained with 14 extra irrelevant words.**\n\nLets see this for multiple text sentences...","3cd7ee49":"# End Note\n\nHi everyone. There are a lot of things which I learnt while writing this kernel and there are still a lot of concepts left untouched. Kaggle provides us with the opportunity to learn and grow through kernels\/notebooks sharing and a wide variety of datasets. Here is my contribution to the Kaggle community. I promise to keep putting kernels for everyone to learn and to learn myself and to comeback each time with more and more knowledge, concepts and implementations.\n\nIf you have any doubts, feedbacks or concerns, feel free to comment down below and I will try to get back to you as soon as possible. It would be great if you find any mistakes and point them out in comments... I will try to resolve them ASAP.\n\n**<font color = 'red'>If you like this kernel \ud83d\ude0a then please show some love \u2764\ufe0f and upvote it \ud83d\udc4d. It really motivates me to work even harder and bring you guys more such quality content.<\/font>**\n\n### Stay tuned for Part 2 . In Pt.2 we will see much more complex concepts using NLTK....","09844f60":"**Coming to the second point...**<br>\n#### You may be asking yourself when should I use Stemming and when should I use Lemmatization? \n\nThe answer itself is in whatever you have learned from this notebook. You have seen the following points:\n\n- Stemming and Lemmatization both generate the root form of the inflected words. The difference is that stem might not be an actual word whereas, lemma is an actual language word.\n\n- Stemming follows an algorithm with steps to perform on the words which makes it faster. Whereas, in lemmatization, you used WordNet corpus and a corpus for stop words as well to produce lemma which makes it slower than stemming. You also had to define a parts-of-speech to obtain the correct lemma.\n\n### The Speed v\/s Accuracy Trade-off\n\nThe above points show that if speed is focused then stemming should be used since lemmatizers scan a corpus which consumed time and processing. It depends on the application you are working on that decides if stemmers should be used or lemmatizers. If you are building a language application in which language is important you should use lemmatization as it uses a corpus to match root forms.\n\n### So choose wisely :)","e6776073":"If we have a closer look at the stopwords list, we can see that it includes words only in lowercase. So we have to be very careful to first convert our text to lowercase before removing stopwords. Otherwise, some stopwords will slide in while building our model and may affect its accuracy or prediction power.\n\nNow let's see how to remove stopwords from text..."}}