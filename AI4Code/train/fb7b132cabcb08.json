{"cell_type":{"a744ca51":"code","aa2e6c73":"code","7002c47e":"code","0d9aaffe":"code","0a303749":"code","c6d7751e":"code","342397d7":"code","74bbedef":"code","77cc1a24":"code","439fb945":"code","95160b74":"code","fbf91558":"code","cd68114b":"code","be8d4e32":"code","49c2f53e":"code","5fa73a6d":"code","1bfd3c04":"code","ef7a7ed1":"code","70a486ad":"code","f2b9fc0c":"code","7db3757a":"code","6b62d9f1":"code","7b7a3d4e":"code","2cf66369":"code","22c0e8d8":"code","5e2d2288":"code","42523444":"markdown","404feddb":"markdown","a76c2012":"markdown","0b8bae41":"markdown","b631f3fb":"markdown","cd84f08b":"markdown"},"source":{"a744ca51":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport json\nimport requests\nimport time\n%matplotlib inline\n\ndef get_indicator(ind_code, ind_text):\n    BASE_URL = 'https:\/\/ghoapi.azureedge.net\/api\/'\n    DATE_2000S = '?$filter=date(TimeDimensionBegin) ge 2000-01-01'    \n    service_url = BASE_URL + ind_code + DATE_2000S\n    response = requests.get(service_url)\n\n# make sure we got a valid response\n    if(response.ok):\n        data_j = response.json()\n# SpatialDim = country_code, TimeDim = year, Numeric_Value\n        data = pd.DataFrame(data_j[\"value\"]).rename(\n            columns = {'NumericValue':ind_text, 'SpatialDim':'country_code', 'TimeDim':'year'})\n        data = data[(data.SpatialDimType != 'REGION') & (data.SpatialDimType != 'WORLDBANKINCOMEGROUP')]\n\n        print(\"Data for \\\"{}\\\" loaded, set {} rows {} columns\".format(ind_text, data.shape[0], data.shape[1]))\n        return data\n    else:\n        print(\"Response was not OK\", response)\n        return None\n\ndef remove_duplicates(data_set):\n    dup_set = data_set.duplicated(subset=[\"country_code\", \"year\"], keep='last')\n    return data_set[~dup_set]\n\ndef test_dump(data_set):\n    print(data_set.shape)\n    print(data_set.info())\n    print(data_set[data_set['country_code'] == 'BEL'])\n\n    col_names = list(data_set.columns.values)\n    for name in col_names:\n        print(name,data_set[name].nunique())\n\n# Import Drive API and authenticate.\nfrom google.colab import drive\n# Mount your Drive to the Colab VM.\ndrive.mount('\/gdrive')\n\nprint (time.asctime( time.localtime(time.time()) ))","aa2e6c73":"# Create basic table, with countries and years\n\nservice_url0 = 'https:\/\/ghoapi.azureedge.net\/api\/DIMENSION\/COUNTRY\/DimensionValues\/'\nresponse0 = requests.get(service_url0)\n\n# make sure we got a valid response\nprint(response0)\nif (response0.ok):\n    # get the full data from the response\n    data0j = response0.json()\n    print(data0j.keys())\nelse:\n    print(\"Response was not OK\")\n\ndata0a = pd.DataFrame(data0j[\"value\"])\ndata0a = data0a[data0a['Title'] != 'SPATIAL_SYNONYM']\n\nremove_list = ['PRI', 'KNA', 'DMA', 'PSE', 'AND', 'SMR', 'MCO', 'LIE', 'COK', \n               'TUV', 'PLW', 'TKL', 'MHL', 'NIU', 'NRU', 'ME1', 'SDF']\n\n#['Puerto Rico' 'Saint Kitts and Nevis' 'Dominica'\n# 'occupied Palestinian territory, including east Jerusalem' 'Andorra'\n# 'San Marino' 'Monaco' 'Liechtenstein' 'Cook Islands' 'Tuvalu' 'Palau'\n# 'Tokelau' 'Marshall Islands' 'Niue' 'Nauru'\n# 'The former state union Serbia and Montenegro' 'Sudan (former)']\n\ndata0a = data0a[~data0a['Code'].isin(remove_list)]\n\nnum_years = 17 # from 2000-2016\ncountry_year_list =[] \nfor index, rows in data0a.iterrows(): \n    for year in range(2000, (2000+num_years), 1):\n        sub_list = [rows.Title, rows.Code, rows.ParentTitle, year] \n        country_year_list.append(sub_list) \ndata0 = pd.DataFrame(country_year_list, columns=['country','country_code','region','year'])\n\nprint(data0.shape)\nprint(data0.info())","7002c47e":"#  WHOSIS_000001\n#      Life expectancy at birth (years)\n#  choose:  Dim1 = BTSX  (ignore mle and fmle)\n\nind_code = 'WHOSIS_000001'\nind_text = 'life_expect'\ndata_raw = get_indicator(ind_code, ind_text)\n\ndata01 = data_raw[data_raw['Dim1'] == 'BTSX'][['country_code', 'year', ind_text]]\ndata01 = remove_duplicates(data01)\ntest_dump(data01)\n\ndata0 = data0.merge(data01, how='left')","0d9aaffe":"#  WHOSIS_000015\n#      Life expectancy at age 60 (years)\n#  choose:  Dim1 = BTSX  (ignore mle and fmle)\n\nind_code = 'WHOSIS_000015'\nind_text = 'life_exp60'\ndata_raw = get_indicator(ind_code, ind_text)\n\ndata01a = data_raw[data_raw['Dim1'] == 'BTSX'][['country_code', 'year', ind_text]]\ndata01a = remove_duplicates(data01a)\ntest_dump(data01a)\n\ndata0 = data0.merge(data01a, how='left')","0a303749":"#  WHOSIS_000004\n#      Adult mortality rate (probability of dying between 15 and 60 years per 1000 population)\n#  choose:  Dim1 = BTSX  (ignore mle and fmle)\n\nind_code = 'WHOSIS_000004'\nind_text = 'adult_mortality'\ndata_raw = get_indicator(ind_code, ind_text)\n\ndata02 = data_raw[data_raw['Dim1'] == 'BTSX'][['country_code', 'year', ind_text]]\ndata02 = remove_duplicates(data02)\ntest_dump(data02)\n\ndata0 = data0.merge(data02, how='left')","c6d7751e":"#  LIFE_0000000029\n#      nMx - age-specific death rate between ages x and x+n\n#  A few countries have a gender-averaged rate, but most don't. Need to use the average.\n#  choose:  Dim1 = BTSX  (calculate average of mle and fmle)\n#           Dim2 = AGELT1, AGE1-4\n\nind_code = 'LIFE_0000000029'\nind_text = 'mortality'\ndata_raw = get_indicator(ind_code, ind_text)\n\ndata_temp = data_raw[data_raw['Dim2'] == 'AGELT1'][['country_code', 'year', ind_text, 'Dim1']]\ndata_temp = data_temp.pivot_table(index=[\"country_code\", \"year\"], columns=[\"Dim1\"], values=ind_text)\ndata_temp.reset_index(inplace=True)\ndata_temp['BTSX'] = 0.5 * (data_temp['MLE'] + data_temp['FMLE'])\ndata_temp = data_temp.melt(id_vars=['country_code', 'year'], var_name='Dim1', value_name='infant_mort')\n\ndata03 = data_temp[data_temp['Dim1'] == 'BTSX'][['country_code', 'year', 'infant_mort']]\ndata03 = remove_duplicates(data03)\ntest_dump(data03)\n\ndata_temp = data_raw[data_raw['Dim2'] == 'AGE1-4'][['country_code', 'year', ind_text, 'Dim1']]\ndata_temp = data_temp.pivot_table(index=[\"country_code\", \"year\"], columns=[\"Dim1\"], values=ind_text)\ndata_temp.reset_index(inplace=True)\ndata_temp['BTSX'] = 0.5 * (data_temp['MLE'] + data_temp['FMLE'])\ndata_temp = data_temp.melt(id_vars=['country_code', 'year'], var_name='Dim1', value_name='age1-4mort')\n\ndata04 = data_temp[data_temp['Dim1'] == 'BTSX'][['country_code', 'year', 'age1-4mort']]\ndata04 = remove_duplicates(data04)\ndata04 = data04.rename(columns = {'age1-4 mortality':'age1-4_mort'})\ntest_dump(data04)\n\ndata0 = data0.merge(data03, how='left')\ndata0 = data0.merge(data04, how='left')","342397d7":"#  SA_0000001400\n#      Alcohol, recorded per capita (15+) consumption (in litres of pure alcohol)\n#  choose:  Dim1 = SA_TOTAL  (ignore beer, wine, spirits, other)\nind_code = 'SA_0000001400'\nind_text = 'alcohol'\ndata_raw = get_indicator(ind_code, ind_text)\n\ndata05 = data_raw[data_raw['Dim1'] == 'SA_TOTAL'][['country_code', 'year', ind_text]]\ndata05 = remove_duplicates(data05)\ntest_dump(data05)\n\ndata0 = data0.merge(data05, how='left')","74bbedef":"#  NCD_BMI_MEAN\n#      Mean BMI (kg\/m^2) (age-standardized estimate)\n#  choose:  Dim1 = BTSX  (same number as mle and fmle)\n#           Dim2 is always \"YEARS18-PLUS\"\n# For the Sudan (former) entry, there are two sets of data, entered January 2017 and September 2017                                               \nind_code = 'NCD_BMI_MEAN'\nind_text = 'bmi'\ndata_raw = get_indicator(ind_code, ind_text)\n\n# remove duplicated entries for Sudan (former)\ndup_list = data_raw.duplicated(subset=[\"country_code\", \"year\", 'Dim1'], keep='last')\ndata_raw = data_raw[~dup_list]\n\ndata06 = data_raw[data_raw['Dim1'] == 'BTSX'][['country_code', 'year', ind_text]]\ndata06 = remove_duplicates(data06)\ntest_dump(data06)\n\ndata0 = data0.merge(data06, how='left')","77cc1a24":"#  NCD_BMI_MINUS2C\n#      Prevalence of thinness among children and adolescents,\n#      BMI < -2 standard deviations below the median (crude estimate) (%)\n#  choose:  Dim1 = BTSX  (same number as mle and fmle)\n#           Dim2 can be YEARS05-09, YEARS10-19, and YEARS05-19\n\nind_code = 'NCD_BMI_MINUS2C'\nind_text = 'age5-19thinness'\ndata_raw = get_indicator(ind_code, ind_text)\n\ndata07 = data_raw[(data_raw['Dim1'] == 'BTSX') & (data_raw['Dim2'] == 'YEARS05-19')][['country_code', 'year', ind_text]]\ndata07 = remove_duplicates(data07)\ntest_dump(data07)\n\ndata0 = data0.merge(data07, how='left')","439fb945":"#  NCD_BMI_PLUS2C\n#      Prevalence of thinness among children and adolescents,\n#      BMI > +2 standard deviations above the median (crude estimate) (%)\n#  choose:  Dim1 = BTSX  (same number as mle and fmle)\n#           Dim2 can be YEARS05-09, YEARS10-19, and YEARS05-19\n\nind_code = 'NCD_BMI_PLUS2C'\nind_text = 'age5-19obesity'\ndata_raw = get_indicator(ind_code, ind_text)\n\ndata08 = data_raw[(data_raw['Dim1'] == 'BTSX') & (data_raw['Dim2'] == 'YEARS05-19')][['country_code', 'year', ind_text]]\ndata08 = remove_duplicates(data08)\ntest_dump(data08)\n\ndata0 = data0.merge(data08, how='left')","95160b74":"#  WHS4_117\n#      Hepatitis B (HepB3) immunization coverage among 1-year-olds (%)\n#  choose:  no \"DIM1\" settings; 19 years and 186 countries\n\nind_code = 'WHS4_117'\nind_text = 'hepatitis'\ndata_raw = get_indicator(ind_code, ind_text)\n\ndata09 = data_raw[['country_code', 'year', ind_text]]\ndata09 = remove_duplicates(data09)\ntest_dump(data09)\n\ndata0 = data0.merge(data09, how='left')","fbf91558":"#  WHS8_110\n#      Measles-containing-vaccine first-dose (MCV1) immunization coverage among 1-year-olds (%)\n#  choose:  no \"DIM1\" settings; 19 years and 194 countries\n#      mslv, vmsl looked unusable - many missing entries, sporadic years, multiple data sources, etc.\n\nind_code = 'WHS8_110'\nind_text = 'measles'\ndata_raw = get_indicator(ind_code, ind_text)\n\ndata10 = data_raw[['country_code', 'year', ind_text]]\ndata10 = remove_duplicates(data10)\ntest_dump(data10)\n\ndata0 = data0.merge(data10, how='left')","cd68114b":"#  WHS4_544\n#      Polio (Pol3) immunization coverage among 1-year-olds (%)\n#  choose:  no \"DIM1\" settings; 19 years and 194 countries\n\nind_code = 'WHS4_544'\nind_text = 'polio'\ndata_raw = get_indicator(ind_code, ind_text)\n\ndata11 = data_raw[['country_code', 'year', ind_text]]\ndata11 = remove_duplicates(data11)\ntest_dump(data11)\n\ndata0 = data0.merge(data11, how='left')","be8d4e32":"#  WHS4_100\n#      Diphtheria tetanus toxoid and pertussis (DTP3) immunization coverage among 1-year-olds (%)\n#  choose:  no \"DIM1\" settings; 19 years and 194 countries (missing 19 non-null values)\n\nind_code = 'WHS4_100'\nind_text = 'diphtheria'\ndata_raw = get_indicator(ind_code, ind_text)\n\ndata12 = data_raw[['country_code', 'year', ind_text]]\ndata12 = remove_duplicates(data12)\ntest_dump(data12)\n\ndata0 = data0.merge(data12, how='left')","49c2f53e":"#  WSH_WATER_BASIC\n#      Population using at least basic drinking-water services (%)\n#  choose:  Dim1 = TOTL  (ignore RUR and URB)\n\nind_code = 'WSH_WATER_BASIC'\nind_text = 'basic_water'\ndata_raw = get_indicator(ind_code, ind_text)\n\ndata13 = data_raw[data_raw['Dim1'] == 'TOTL'][['country_code', 'year', ind_text]]\ndata13 = remove_duplicates(data13)\ntest_dump(data13)\n\ndata0 = data0.merge(data13, how='left')","5fa73a6d":"#  HWF_0001\n#      Medical doctors (per 10,000)\n#  choose:  193 country codes, but about half the year entries are missing\n\nind_code = 'HWF_0001'\nind_text = 'doctors'\ndata_raw = get_indicator(ind_code, ind_text)\n\ndata14 = data_raw[['country_code', 'year', ind_text]]\ndata14 = remove_duplicates(data14)\ntest_dump(data14)\n\ndata0 = data0.merge(data14, how='left')","1bfd3c04":"#  DEVICES00\n#      Total density per 100 000 population: Hospitals\n\nind_code = 'DEVICES00'\nind_text = 'hospitals'\ndata_raw = get_indicator(ind_code, ind_text)\n\ndata14a = data_raw[['country_code', 'year', ind_text]]\ndata14a = remove_duplicates(data14a)\ntest_dump(data14a)\n\ndata0 = data0.merge(data14a, how='left')","ef7a7ed1":"#  WHS9_93\n#      Gross national income per capita (PPP int. $)\n#  choose:  only up to 2013\n\nind_code = 'WHS9_93'\nind_text = 'gni_capita'\ndata_raw = get_indicator(ind_code, ind_text)\n\ndata15 = data_raw[['country_code', 'year', ind_text]]\ndata15 = remove_duplicates(data15)\ntest_dump(data15)\n\ndata0 = data0.merge(data15, how='left')","70a486ad":"#  GHED_GGHE-DGDP_SHA2011\n#      Domestic general government health expenditure (GGHE-D) as percentage of gross domestic product (GDP) (%)\n#  choose:  191 country codes, 18 years\n\nind_code = 'GHED_GGHE-DGDP_SHA2011'\nind_text = 'gghe-d'\ndata_raw = get_indicator(ind_code, ind_text)\n\ndata16 = data_raw[['country_code', 'year', ind_text]]\ndata16 = remove_duplicates(data16)\ntest_dump(data16)\n\ndata0 = data0.merge(data16, how='left')","f2b9fc0c":"#  GHED_CHEGDP_SHA2011\n#      Current health expenditure (CHE) as percentage of gross domestic product (GDP) (%)\n#  choose:  191 country codes, 18 years\n\nind_code = 'GHED_CHEGDP_SHA2011'\nind_text = 'che_gdp'\ndata_raw = get_indicator(ind_code, ind_text)\n\ndata17 = data_raw[['country_code', 'year', ind_text]]\ndata17 = remove_duplicates(data17)\ntest_dump(data17)\n\ndata0 = data0.merge(data17, how='left')","7db3757a":"# save the GHO data to a local CSV file\n\n#data0.to_csv('gho_data.csv',index=False)\n\ndata0.to_csv('\/gdrive\/My Drive\/Thinkful\/capstone2_who_life_exp\/gho_data.csv', index=False)\n\nprint ('GHO data finished at: ',time.asctime( time.localtime(time.time()) ))","6b62d9f1":"# if the GHO file has already been made, read it in\n#gho_data = pd.read_csv('gho_data.csv', skipinitialspace=True)\n\n# read in the CSV files for the UNESCO features\nun1_data = pd.read_csv('unesco_population.csv', skipinitialspace=True).rename(\n    columns = {'LOCATION':'country_code', 'TIME':'year'})\nun2_data = pd.read_csv('unesco_gni.csv', skipinitialspace=True).rename(\n    columns = {'LOCATION':'country_code', 'TIME':'year'})\nun3_data = pd.read_csv('unesco_educ.csv', skipinitialspace=True).rename(\n    columns = {'LOCATION':'country_code', 'TIME':'year'})","7b7a3d4e":"indicators = [('une_pop', 'Total population '),\n              ('une_infant', 'Mortality rate, infant (per 1,000 live births)'),\n              ('une_life', 'Life expectancy at birth, total (years)'),\n              ('une_hiv', 'Prevalence of HIV, total (% of population ages 15-49)')]\n\nfor ind_name, ind_text in indicators:\n    print(\"Trying\",ind_name,\"indicator:\",ind_text, len(un1_data[un1_data['Indicator'] == ind_text]))\n\n    temp_df = un1_data[un1_data['Indicator'] == ind_text][['country_code', 'year', 'Value']].rename(\n                  columns = {'Value':ind_name})\n    gho_data = gho_data.merge(temp_df, how='left')\n\n# The database is missing population information from Japan and Lebanon.\n\njapan_pop = [127524, 127714, 127893, 128058, 128204, 128326, 128423, 128494, 128539, 128555, 128542, 128499, 128424, 128314, 128169, 127985, 127763]\ngho_data.loc[(gho_data['country_code'] == 'JPN'), 'une_pop'] = japan_pop\n\nlebanon_pop = [3843, 3991, 4182, 4388, 4569, 4699, 4760, 4767, 4765, 4813, 4953, 5202, 5538, 5913, 6261, 6533, 6714]\ngho_data.loc[(gho_data['country_code'] == 'LBN'), 'une_pop'] = lebanon_pop","2cf66369":"indicators = [('une_gni', 'GNI per capita, PPP (current international $)'),\n              ('une_poverty', 'Poverty headcount ratio at $1.90 a day (PPP) (% of population)')]\n\nfor ind_name, ind_text in indicators:\n    print(\"Trying\",ind_name,\"indicator:\",ind_text, len(un2_data[un2_data['Indicator'] == ind_text]))\n\n    temp_df = un2_data[un2_data['Indicator'] == ind_text][['country_code', 'year', 'Value']].rename(\n                  columns = {'Value':ind_name})\n    gho_data = gho_data.merge(temp_df, how='left')","22c0e8d8":"indicators = [('une_edu_spend', 'Government expenditure on education as a percentage of GDP (%)'),\n              ('une_literacy', 'Adult literacy rate, population 15+ years, both sexes (%)'),\n              ('une_school', 'Mean years of schooling (ISCED 1 or higher), population 25+ years, both sexes')]\n\nfor ind_name, ind_text in indicators:\n    print(\"Trying\",ind_name,\"indicator:\",ind_text, len(un3_data[un3_data['Indicator'] == ind_text]))\n\n    temp_df = un3_data[un3_data['Indicator'] == ind_text][['country_code', 'year', 'Value']].rename(\n                  columns = {'Value':ind_name})\n    gho_data = gho_data.merge(temp_df, how='left')","5e2d2288":"print(gho_data.info())\n\ngho_data.to_csv('who_life_exp.csv',index=False)\n\nprint ('Notebook finished at: ',time.asctime( time.localtime(time.time()) ))","42523444":"## Introduction ##\n\nI generated a data set with features that affect national life expectancy,\nbased on information from [WHO (World Health Organization)][who].\nThe servers are found at the [GHO (Global Health Observatory)][ghodb]\nand the [UNESCO Databases of Resources on Education][unesco_ed].\n\nThis shows how I generated the data set, running the notebook on Google Drive.\n**This will not work on Kaggle**, but it should be easily modified to run on your\nhome computer or Kaggle. Hopefully this example will help other analysts if\nthey are unfamiliar with these particular data servers.\n\nThis code also assumes that several CSV files are already made from the UNESCO\ndata servers, so this process requires some manual action outside of running\nthis code.\n\nAlso be aware that sometimes the GHO and\/or UNESCO data servers are down for the\nwhole day, without an explanation given. I assume this is maintenance or data updating,\nbut I do not know what days it is scheduled. If you see that the server is down, you will\nprobably have to wait for the next day and check again.\n\n[who]: https:\/\/www.who.int\n[ghodb]: https:\/\/www.who.int\/gho\/database\/en\/\n[unesco_ed]: https:\/\/en.unesco.org\/themes\/education\/databases","404feddb":"<a name=\"unesco_web\"><\/a>\n## Create UNESCO CSV file using web interface\n\nThis code assumes that several CSV files are already made from the [UNESCO\ndata servers][unesco_ed], and present in the same directory that this notebook\nis running. That process requires some manual action outside of running\nthis code.\n\nI went to the web page for the [National Statistics on Education][unesco_data], which\nprovides access to data compiled by the UNESCO Institute for Statistics (UIS).\n\nThe basic steps are:\n- pick a \"theme\" in the left bar to select a group of indicators\n- use the table editor to select the specific indicators desired, and limit the time period from 2000-2016\n- click the button to download the table as a CSV file\n\n(My Internet connection is slow, so I kept the tables small to reduce browser latency.\nI made 3 files, but if you had the patience, it could be done in 1 file.)\n\nFor this example, these are the names of the 3 CSV files, and the indicators for each one.\n\n1. unesco_population.csv\n   - Total population\n   - Mortality rate, infant (per 1,000 live births)\n   - Life expectancy at birth, total (years)\n   - Prevalence of HIV, total (% of population ages 15-49)\n\n2. unesco_gni.csv\n   - GNI per capita, PPP (current international \\$)\n   - Poverty headcount ratio at \\$1.90 a day (PPP) (% of population)\n\n3. unesco_educ.csv\n   - Government expenditure on education as a percentage of GDP (%)\n   - Adult literacy rate, population 15+ years, both sexes (%)\n   - Mean years of schooling (ISCED 1 or higher), population 25+ years, both sexes\n\n[unesco_ed]: https:\/\/en.unesco.org\/themes\/education\/databases\n[unesco_data]: http:\/\/data.uis.unesco.org\/Index.aspx?DataSetCode=EDULIT_DS&popupcustomise=true&lang=en","a76c2012":"## Conclusion\n\nTime to show the info for the data frame (to make sure none of the indicators are blank), and save it\nto a CSV file to use for life expectancy analysis.","0b8bae41":"<a name=\"combine_files\"><\/a>\n## Merge the data files into one dataframe and write to CSV file\n\nNow I read in the UNESCO CSV files, and merge them with the existing GHO table.\n\nOddly enough, the UNESCO database is missing population information from Japan and Lebanon.\nFortunately, I found that information in an Excel sheet, proved by:\n- United Nations, Department of Economic and Social Affairs, Population Division (2019). \"World Population Prospects 2019\".\n\nThat information has been hard-coded here, so keep an eye on that if the UNESCO population\ndata gets updated in the future.","b631f3fb":"## Instructions ##\n1. [Create UNESCO CSV file using web interface](#unesco_web) (done manually)\n2. [Create GHO CSV file using http API](#gho_api)\n3. [Merge the data files into one dataframe and write to CSV file](#combine_files)\n","cd84f08b":"<a name=\"gho_api\"><\/a>\n## Create GHO CSV file using http API\n\nI looked over the instruction examples on how to use the [GHO OData API portal][whodb_ins]. Each indicator might be saved in a slightly different format, so some trial and error is\nneeded to access the information correctly. (I have a\nshort note in each code block to indicate some of the choices made.)\n\nI made a table from the list of countries, then added a \"year\" for each country to cover 2000-2016.\nThere are 17 countries in the GHO list that did not have data for many of the indicators I wanted to\nuse, so I removed those countries.\n\nWith that framework set up, I then loaded the indicators, one at a time, and merged it into\nthe master dataframe. (At the end, I saved this GHO table, so I don't have to rerun this part of\nthe code if I wanted to change one of the UNESCO features.)\n\nI included small dumps of the data (using the country of Belgium), to make sure everything was working\nokay, but \"info\" and \"test_dump\" commands can be commented out if you don't want the text.\n\n[whodb_ins]: https:\/\/www.who.int\/data\/gho\/info\/gho-odata-api"}}