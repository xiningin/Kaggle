{"cell_type":{"b9a49ca9":"code","b5a99825":"code","c12fc9ee":"code","a44913ca":"code","309456b9":"code","0687e0b0":"code","0390eb2b":"code","1edc3159":"code","aa5fce3d":"code","f5c15c26":"code","70283c17":"code","61497d52":"code","888db271":"code","9eb0f96a":"code","0f5e1cba":"code","63e57fc6":"code","ee30c66c":"code","a44a9547":"markdown","8348dcf8":"markdown","12b9cfd7":"markdown","8b889459":"markdown","ca4c2776":"markdown","cb8024ef":"markdown","e1b15457":"markdown","0b78bfac":"markdown","90db1412":"markdown"},"source":{"b9a49ca9":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport cartopy.crs as ccrs\nimport cartopy\n\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","b5a99825":"raw_data = pd.read_csv('\/kaggle\/input\/gtd\/globalterrorismdb_0718dist.csv', engine='python')","c12fc9ee":"pd.options.display.max_columns = 200\nraw_data.head()","a44913ca":"yearlyacts = raw_data.groupby('iyear',as_index = True, group_keys = False)['eventid'].count()\nyearlyacts.plot(kind='line', figsize = (20,5),xticks = range(1970,2018),rot = 45, title='Yearly Cases of Terrorist Attacks',\\\n                linewidth = 3);\n\n\nyearlyacts = raw_data.groupby(['iyear','region_txt'],as_index = False, group_keys = False)['eventid'].count()\n#yearlyacts.region_txt.value_counts()\nyearlyacts = yearlyacts.rename(columns={'region_txt': \"Region\", \"eventid\": \"Cases\"})\n\nyearlyacts.pivot_table(index='iyear', columns='Region', aggfunc=np.sum, fill_value=0).plot(kind='line', figsize = (20,10),\\\n                                                                                              xticks = range(1970,2018),rot = 45,\\\n                                                                                              linewidth=5,title='Cases by Region');","309456b9":"atk_types = raw_data.groupby('attacktype1_txt', group_keys=False)['eventid'].count()\natk_types.nlargest(50).plot(kind = 'bar', figsize = (20,5),grid=True, rot=45, title='Types of Attack');","0687e0b0":"n_killsum = raw_data.groupby('country_txt',group_keys=False)['nkill'].sum()\nn_cases = raw_data.groupby('country_txt', group_keys=False)['eventid'].count()\nn_cases.nlargest(50).plot(kind = 'bar', figsize = (20,5),yticks = range(0,25001,2500),grid=True, title='Cases by Countries');","0390eb2b":"weapons = raw_data.groupby('weaptype1_txt', group_keys=False)['eventid'].count()\nweapons.nlargest(50).plot(kind = 'bar', figsize = (20,5),grid=True, rot = 45, title= 'Weapon Used');\n","1edc3159":"targets = raw_data.groupby('targtype1_txt', group_keys=False)['eventid'].count()\ntargets.nlargest(50).plot(kind = 'bar', figsize = (20,5),grid=True, rot=70, title='Targets');","aa5fce3d":"print('Average number of perpetrators: ', round(raw_data[(raw_data.nperps > 0)]['nperps'].mean(),1))\nprint('Median of perpetrators: ', round(raw_data[(raw_data.nperps > 0)]['nperps'].median(),1))","f5c15c26":"#print(lon[lon < -180])\nprint(raw_data.at[17658,'longitude']) #change this wrong data\n\nraw_data.at[17658,'longitude'] = -86.185896\n\nprint(raw_data.at[17658,'longitude'])","70283c17":"lat = raw_data[raw_data['iyear']>=2003]['latitude']\nlon = raw_data[raw_data['iyear']>=2003]['longitude']\n\n\nplt.figure(figsize = (20,15))\n\n#extent = [-20, 40, 30, 60]\n#central_lon = np.mean(extent[:2])\n#central_lat = np.mean(extent[2:])\n\n#ax = plt.axes(projection=ccrs.AlbersEqualArea(central_lon, central_lat))\nax = plt.axes(projection=ccrs.Mercator())\n\n#ax.set_extent(extent,crs=ccrs.PlateCarree())\nax.scatter(lon,lat,transform=ccrs.PlateCarree(),s=1, c = 'red')\nax.coastlines()\nax.add_feature(cartopy.feature.BORDERS, linestyle=\"-\")\nax.gridlines(crs=ccrs.PlateCarree(), draw_labels=True)\nplt.title('Worldwide Terrorists Attacks Since 2003', fontsize = 20);\n","61497d52":"df_filtered = raw_data[(raw_data['latitude']>25) & (raw_data['latitude']<60) & \\\n                (raw_data['longitude']> -20) & (raw_data['longitude']< 60) & (raw_data['INT_LOG']==0) & (raw_data['iyear']>=2003)]\n\nlat2 = df_filtered['latitude']\nlon2 = df_filtered['longitude']\n\ndata = pd.DataFrame({'latitude':lat2, 'longitude': lon2})\ndata = data.dropna(how=\"any\")\nfrom sklearn.cluster import DBSCAN\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import MinMaxScaler\nscaler = StandardScaler()\ndata_scaled = scaler.fit_transform(data)\n\ndb = DBSCAN(eps=0.3, min_samples=50).fit(data_scaled)\n\nlabels = db.labels_\n\ndata['cluster'] = labels\ndf_filtered['cluster'] = data['cluster']\n# Number of clusters in labels, ignoring noise if present.\nn_clusters_ = len(set(labels)) - (1 if -1 in labels else 0)\nn_noise_ = list(labels).count(-1)\n\nprint('Estimated number of clusters: %d' % n_clusters_)\nprint('Estimated number of noise points: %d' % n_noise_)","888db271":"plt.figure(figsize = (20,15))\n\n#extent = [-20, 60, 25, 60]\n#central_lon = np.mean(extent[:2])\n#central_lat = np.mean(extent[2:])\n\nlegendname = []\nfor items in np.sort(df_filtered.cluster.unique()):\n    if items == -1:\n        legendname.append('Others \/ Cluster'+str(items))\n    else:\n        legendname.append('Cluster'+str(items))\n\n#ax = plt.axes(projection=ccrs.AlbersEqualArea(central_lon, central_lat))\nax = plt.axes(projection=ccrs.Mercator())\n#ax.set_extent(extent,crs=ccrs.PlateCarree())\nscatter = ax.scatter(df_filtered.longitude,df_filtered.latitude,transform=ccrs.PlateCarree(), c = df_filtered.cluster,\\\n                     cmap = 'tab10', s = df_filtered['nkill']*10) #we define the marker size to corresponds to the number of casuality in the attack\nax.coastlines(linewidth = 2);\nax.add_feature(cartopy.feature.BORDERS, linestyle=\"-\", linewidth = 2);\nax.add_feature(cartopy.feature.OCEAN);\n#ax.add_feature(cartopy.feature.LAND, edgecolor='black')\nax.add_feature(cartopy.feature.LAKES, edgecolor='black');\nax.gridlines(crs=ccrs.PlateCarree());\nplt.title('Europe and Middle East Domestic Terrorists Attacks Since 2003', fontsize = 20);\nplt.legend(handles=scatter.legend_elements()[0], labels= legendname,fontsize=20, loc='upper right');","9eb0f96a":"x = df_filtered.groupby(['gname'],as_index = False,group_keys = False).filter(lambda x: len(x) >= 50)\nx.groupby(['cluster','gname']).count()['eventid']","0f5e1cba":"df_filtered_sa = raw_data[(raw_data['region_txt'] == 'South Asia')& (raw_data['INT_LOG']==0) & (raw_data['iyear']>=2003)]\n\nlat_sa = df_filtered_sa['latitude']\nlon_sa = df_filtered_sa['longitude']\n\ndata_sa = pd.DataFrame({'latitude':lat_sa, 'longitude': lon_sa}, index = df_filtered_sa.index)\ndata_sa = data_sa.dropna(how=\"any\")\n\ndata_scaled_sa = scaler.fit_transform(data_sa)\n\ndb_sa = DBSCAN(eps=0.3, min_samples=100).fit(data_scaled_sa)\n\nlabels_sa = db_sa.labels_\n\ndata_sa['cluster'] = labels_sa\n\ndf_filtered_sa['cluster'] = data_sa['cluster']\n\n# Number of clusters in labels, ignoring noise if present.\nn_clusters_sa = len(set(labels_sa)) - (1 if -1 in labels_sa else 0)\nn_noise_sa = list(labels_sa).count(-1)\n\nprint('Estimated number of clusters: %d' % n_clusters_sa)\nprint('Estimated number of noise points: %d' % n_noise_sa)","63e57fc6":"plt.figure(figsize = (20,15))\n\n#extent = [-20, 60, 25, 60]\n#central_lon = np.mean(extent[:2])\n#central_lat = np.mean(extent[2:])\n\nlegendname_sa = []\nfor items in np.sort(data_sa.cluster.unique()):\n    if items == -1:\n        legendname_sa.append('Others \/ Cluster'+str(items))\n    else:\n        legendname_sa.append('Cluster'+str(items))\n\n#ax = plt.axes(projection=ccrs.AlbersEqualArea(central_lon, central_lat))\nax = plt.axes(projection=ccrs.Mercator())\n#ax.set_extent(extent,crs=ccrs.PlateCarree())\nscatter = ax.scatter(df_filtered_sa.longitude,df_filtered_sa.latitude,transform=ccrs.PlateCarree(), c = df_filtered_sa.cluster,\\\n                    s = df_filtered_sa.nkill*10);\nax.coastlines(linewidth = 2);\nax.add_feature(cartopy.feature.BORDERS, linestyle=\"-\", linewidth = 2);\nax.add_feature(cartopy.feature.OCEAN);\n#ax.add_feature(cartopy.feature.LAND, edgecolor='black')\nax.add_feature(cartopy.feature.LAKES, edgecolor='black');\nax.gridlines(crs=ccrs.PlateCarree());\nplt.title('South Asian Domestic Terrorists Attacks Since 2003', fontsize = 20);\nplt.legend(handles=scatter.legend_elements()[0], labels= legendname_sa,fontsize=20);","ee30c66c":"x_sa = df_filtered_sa.groupby('gname',group_keys = False).filter(lambda x: len(x) >= 50)\nx_sa.groupby(['cluster','gname']).count()['eventid']","a44a9547":"After looking at the raw data and reading the explanation of its columns (https:\/\/start.umd.edu\/gtd\/downloads\/Codebook.pdf), let us think of what information do we want to get out of the data, given the columns only (for the purpose of this notebook)\n\nSome of the things that we want to know intuitively among other things are:\n\n- What is the trend of terrorism attacks? Increasing, decreasing or relatively constant?\n- Which countries has the most cases of terrorist attacks?\n- Who \/ which group(s) are mostly responsible for the attacks?\n- From which countries do these groups originate from?\n- How many perpetrators usually is there for an attack if it is a group attack?\n- How many casualities are there for some of the deadliest attacks?\n- Can we see where are the attacks took place? \n- What kind of attacks are there and what is the most common one?\n- Who are the targets of the attacks? \n- What are the weapons mostly used in the attacks?\n\nLet's try to answer some of them\n","8348dcf8":"Now lets look at the cases in South Asia","12b9cfd7":"# Global Terrorism Data\n\nIn this notebook we will look at the Global Terrorism Database owned by the University of Mariland and try to answer a few limited questions regarding terrorism. We would also do a clustering of terrorist attacks based on latitude and longitude data using Density-based spatial clustering of applications with noise (DBSCAN) algorithm. \n\n\nNote that all data used in this notebook belongs to:\n\nNational Consortium for the Study of Terrorism and Responses to Terrorism (START), University of Maryland. (2018). The Global Terrorism Database (GTD) [Data file]. Retrieved from https:\/\/www.start.umd.edu\/gtd","8b889459":"We can imagine that for most of the cases, the perpetrator is an individual \/ affiliated individual, hence the median of 2 person. While at the other end of the spectrum, there are organized terrorist groups which attacks with a large number of perpetrators, hence skewing the average way past the median","ca4c2776":"We can see above that since 2003 there had been an increase in the yearly number of the cases. If we divide by region, we can see that the increase was contributed mostly by the regions of Middle East & North Africa, South Asia, and Sub-saharan Africa.\n\nHence to narrow down our scope, we would use data from 2003 onwards for our clustering","cb8024ef":"Above we can see the clusters on the map for Europe and Middle East region. The circle size corresponds to the number of casualities in the attacks. Bigger circle means the number of casualities are higher and vice versa.\n\nNow let's see how our clusters corresponds to the perpetrator group","e1b15457":"We can see that we ended up with 9 clusters, we can also see that we have 258 noise points which corresponds to the 258 un-clusterable attacks. We can think of the noise points as 'Others' cluster. ","0b78bfac":"As we can see above, we can see the hotspots for terrorist attacks. Next we will try to examine the clusters of the attacks for the region of Europe & Middle East and South Asia.\n\nWe will use SKLearn's DBSCAN clustering library which clusters the attacks based on its density on the latitude longitude map. Here we define the minimum size of a cluster to be 50 attacks. \n\nWe would also look at only domestic case of terrorist attacks meaning that the perpetrator is a citizen of the country where the attack took place. This is because we would like to find the origin of the attack groups by looking at their cluster, hence adding international attacks will have potentially unintended meaning for our result. ","90db1412":"Above we can see the clusters on the map for South Asia region. The circle size corresponds to the number of casualities in the attacks. Bigger circle means the number of casualities are higher and vice versa."}}