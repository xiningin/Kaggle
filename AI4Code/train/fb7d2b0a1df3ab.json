{"cell_type":{"3b4386a8":"code","a9181d3e":"code","cef23ae8":"code","103c1f5d":"code","21fe8c14":"code","816d65a8":"code","0c83a477":"code","1bfee3cc":"code","d5291907":"code","1ced93ae":"code","5318f59d":"code","50bd1ee9":"code","3a857163":"code","b6dc8058":"code","9ea3340c":"code","08dd29a0":"code","b0d2352d":"code","69c100e0":"code","7e575356":"code","a356a1b3":"code","ba89014c":"code","a371ccf5":"markdown","eedae79c":"markdown","fa10f8ca":"markdown","57c26543":"markdown","6c992574":"markdown","9e673dc6":"markdown","a58fca25":"markdown","13c9c715":"markdown","92f07a30":"markdown","efae4842":"markdown","c0c6e4ca":"markdown"},"source":{"3b4386a8":"import os\nimport numpy as np\nimport pandas as pd\n\nPATH='\/kaggle\/input\/titanic'\n\ndef read_csv(filename):\n    file = os.path.join(PATH, filename)\n    return pd.read_csv(file)","a9181d3e":"train = read_csv(\"train.csv\")\ntrain","cef23ae8":"train.info()","103c1f5d":"train[\"Age\"].value_counts()","21fe8c14":"train[\"Cabin\"].value_counts()","816d65a8":"train[\"Embarked\"].value_counts()","0c83a477":"train[\"SibSp\"].value_counts()","1bfee3cc":"train[\"Parch\"].value_counts()","d5291907":"from sklearn.base import BaseEstimator, TransformerMixin\n\nclass DeleteInessentialColumns(BaseEstimator, TransformerMixin):\n    def __init__(self):\n        pass\n    \n    def fit(self, X, y=None):\n        return self\n    \n    def transform(self, X, y=None):\n        features = [\"PassengerId\", \"Name\", \"Ticket\", \"Cabin\"]\n        return X.drop(features, axis=1)","1ced93ae":"from sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import StandardScaler\n\ndata_cleaning_pipeline = Pipeline([\n    ('delete_inessential_columns', DeleteInessentialColumns())\n])\n\none_hot_columns = ColumnTransformer([\n    ('one_hot_encode', OneHotEncoder(), [\"Pclass\", \"Sex\", \"Embarked\"]),\n], remainder='passthrough')\n\npipeline = Pipeline([\n    ('data_cleaning', data_cleaning_pipeline),\n    ('categorical', one_hot_columns),\n    ('imputer', SimpleImputer(strategy='median')),\n    ('scale', StandardScaler())\n])","5318f59d":"train.dropna(subset=[\"Embarked\"], inplace=True)\n\nX_data = train.drop(\"Survived\", axis=1)\ny = train[\"Survived\"].copy()\n\npipeline.fit(X_data)\nX = pipeline.transform(X_data)\nX_df = pd.DataFrame(X, columns=np.array(['1', '2', '3', 'female', 'male', 'C', 'Q', 'S', 'Age', 'SibSp', 'Parch', 'Fare']))\nX_df","50bd1ee9":"from sklearn.model_selection import cross_val_score, cross_val_predict\nfrom sklearn.metrics import roc_curve, roc_auc_score\n\ndef model_stats(name, estimator, cv=10, method='predict_proba', returnAucScore=True, returnAccuracy=True):\n    _scores = cross_val_predict(estimator, X, y, cv=cv, method=method) if method != None else cross_val_predict(estimator, X, y, cv=cv)\n    scores = _scores[:, 1] if method == 'predict_proba' else _scores\n    fpr, tpr, ths = roc_curve(y, scores)\n    auc = None\n    accuracy = None\n    if returnAucScore:\n        auc = roc_auc_score(y, scores)\n    if returnAccuracy:\n        accuracy = cross_val_score(estimator, X, y, cv=cv, scoring='accuracy').mean()\n    return {'name': name, 'fpr': fpr, 'tpr': tpr, 'auc': auc, 'accuracy': accuracy}","3a857163":"from sklearn.linear_model import LinearRegression, SGDClassifier, LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\n\nsgd = model_stats(\"SGD\", SGDClassifier(), method='decision_function')\nlinear = model_stats(\"LinearRegression\", LinearRegression(), method=None, returnAccuracy=False)\nlogistic = model_stats(\"LogisticRegression\", LogisticRegression(), method=None, returnAccuracy=False)\nsvc = model_stats(\"SVC\", SVC(), method='decision_function')\nkn = model_stats(\"KNeighbors\", KNeighborsClassifier())\ntree = model_stats(\"DecisionTree\", DecisionTreeClassifier())\nforest = model_stats(\"RandomForest\", RandomForestClassifier())","b6dc8058":"%matplotlib inline\nimport matplotlib.pyplot as plt\n\nplt.figure(figsize=(16, 8))\nplt.plot([0, 1], [0, 1])\nplt.axis([0, 1, 0, 1])\nplt.xlabel('False Positive Rate', fontsize=16)\nplt.ylabel('True Positive Rate', fontsize=16)\nplt.grid(True)\n\nfor clf in [sgd, linear, logistic, svc, kn, tree, forest]:\n    plt.plot(clf['fpr'], clf['tpr'], linewidth=2, label=f\"{clf['name']} [AUC: {clf['auc']}, Accuracy: {clf['accuracy']}]\")\n\nplt.legend()","9ea3340c":"from sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import train_test_split\n\ndef plot_learning_curves(name, model, X, y, round=False):\n    plt.figure(figsize=(16, 8))\n    plt.xlabel('Sample Size', fontsize=16)\n    plt.ylabel('Error Rate', fontsize=16)\n    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=27)\n    train_errors, val_errors = [], []\n    for m in range(2, len(X_train)):\n        model.fit(X_train[:m], y_train[:m])\n        y_train_pred = model.predict(X_train[:m])\n        y_val_pred = model.predict(X_val)\n        train_errors.append(mean_squared_error(y_train[:m], y_train_pred if round == False else np.round(y_train_pred)))\n        val_errors.append(mean_squared_error(y_val, y_val_pred if round == False else np.round(y_val_pred)))\n    plt.plot(np.sqrt(train_errors), linewidth=2, label=f\"{name} train set\")\n    plt.plot(np.sqrt(val_errors), linewidth=2, label=f\"{name} validation set\")\n    plt.legend()\n    plt.show()","08dd29a0":"plot_learning_curves(\"RandomForest\", RandomForestClassifier(), X, y)  # overfitting","b0d2352d":"plot_learning_curves(\"LinearRegression\", LinearRegression(), X, y)  # underfitting?","69c100e0":"test = read_csv(\"test.csv\")","7e575356":"def create_submission_file(model, data, round=False):\n    prepared = pipeline.transform(data)\n    predictions = model.predict(prepared)\n    return pd.DataFrame({'PassengerId': test.PassengerId, 'Survived': predictions if round == False else np.round(predictions).astype('int64')})","a356a1b3":"lr_clf = LinearRegression()\nlr_clf.fit(X, y)\nlr_clf_submission = create_submission_file(lr_clf, test, round=True)\nlr_clf_submission","ba89014c":"lr_clf_submission.to_csv('lr_clf_submission.csv', index=False)","a371ccf5":"Let's create submission file of predictions.","eedae79c":"Okay, so **Age** contains the genuine value of the passenger's age, and replacing those NaN with the median value of the column seems reasonable.","fa10f8ca":"## Dataset exploration","57c26543":"We have data of 891 passengers. Out of which, we don't know the age of 177, and the port of embarkation of 2. Also, comparatively we have very little information about their cabin. Let's see what kind of data those columns have.","6c992574":"## Model selection\n\nLet's train a couple of classifier models and plot their _ROC curve_ to figure out the best model to predict passenger's survival chance. We will also note down their _accuracy score_, and let's have at least\u00a080%\u00a0accuracy.\u00a0","9e673dc6":"Apparently, the Random Forest Classifier has high AUC and accuracy scores.<sup>1<\/sup>\n\n[1] I tried to fine-tune it, but scores didn't improve much.\n\n## Predict passenger survival","a58fca25":"At the first glance, **Cabin** seems to have not-so-useful data, and because a lot of them have NaN, it _seems plausible to ignore_ the column along with **Name** and **Ticket**.","13c9c715":"**Embarked** has categorical data, and we don't have value for two of the passengers, we can ignore those two passengers for now.\n\nLet's explore **SibSp** and **Parch** columns.","92f07a30":"# Titanic ML\n\nWe should start with splitting a dataset into a train and test set, but Kaggle already has provided the test set, so let's explore the train set.","efae4842":"Hm.. obviously **PassengerId** is not relevant to train a model, and so are **Name** and **Ticket**. Later both contain string values and seem hard to convert them into any meaningful numerical value. Maybe we could find correlation between those values and survivals, but that seems like a long shot, so let's ignore them for now. **Pclass**, **Sex** and **Embarked** have categorical data. **Cabin** has some NaN values, let's see what other columns also contain NaN values.","c0c6e4ca":"**SibSp** and **Parch** are the number of relatives of a passenger boarded the Titanic, siblings\/spouses and parents\/children respectively. I'm not sure how they add value to the dataset, but because both columns have similar in nature, we can add them up and create a new feature called \"Relatives\".\n\n## Prepare data to train a model\n\n- details gathered so far\n    1. remove \"PassengerId\", \"Name\", \"Ticket\" and \"Cabin\" columns\n    2. remove rows where \"Embarked\" is NaN\n    3. encode \"Pclass\", \"Sex\" and \"Embarked\" features using one-hot scheme\n    4. for any remaining NaN values in columns, replace them with column's median value"}}