{"cell_type":{"18fe9a58":"code","730721dc":"code","dd9f58ab":"code","dd828c1c":"code","4527a6d2":"code","b5d97683":"code","ea37462e":"code","39d166af":"code","8acec9f6":"code","17d493b8":"code","a361897b":"code","451fa717":"code","30db778e":"code","ae536f9a":"code","f78d7589":"code","33e4797a":"code","07552a22":"code","70b69233":"code","fde30395":"code","fe6cb106":"code","23c59e47":"code","11b0c1d4":"code","70818173":"code","35c5bd04":"code","8a099c8e":"markdown","f8ba1dc6":"markdown","4ba54823":"markdown","e4f98176":"markdown","918ae2f8":"markdown","a30d295f":"markdown"},"source":{"18fe9a58":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns \nimport matplotlib.pyplot as plt\n# machine learning\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC, LinearSVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.linear_model import Perceptron\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.tree import DecisionTreeClassifier\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","730721dc":"#pip install plotly","dd9f58ab":"df = pd.read_csv('..\/input\/covid19-case-surveillance-public-use-dataset\/COVID-19_Case_Surveillance_Public_Use_Data.csv')","dd828c1c":"#df.head()\n#df.shape\n#df.isnull().sum()\n#df['current_status'].unique()","4527a6d2":"df.describe()","b5d97683":"import plotly.express as px\n\nfig = px.histogram(df['age_group'])\nfig.show()","ea37462e":"grouped_df = df.groupby(['sex','age_group','Race and ethnicity (combined)','hosp_yn','medcond_yn']).count()\ndf1 = pd.DataFrame()\ndf1['sex'] = np.array(list(grouped_df.index))[:,0]\ndf1['age_group'] = np.array(list(grouped_df.index))[:,1]\ndf1['race'] = np.array(list(grouped_df.index))[:,2]\ndf1['hosp'] = np.array(list(grouped_df.index))[:,3]\ndf1['medc'] = np.array(list(grouped_df.index))[:,4]\ndf1['Count'] = grouped_df['icu_yn'].values","39d166af":"fig = px.sunburst(df1, path=['hosp','medc','age_group','race','sex'], values='Count', color='hosp')\nfig.show()","8acec9f6":"del df['cdc_report_dt']\ndel df['pos_spec_dt']\ndel df['onset_dt']\n\n\ndf.drop(df[df['current_status'] == 'Probable Case'].index, inplace = True)\ndf.drop(df[df['hosp_yn'] == 'Missing'].index, inplace = True)\ndf.drop(df[df['Race and ethnicity (combined)'] == 'Unknown'].index, inplace = True)\n\ndel df['current_status']","17d493b8":"df = df.dropna()","a361897b":"df1 = df[:1000]\ndf1","451fa717":"from sklearn import preprocessing \n\nlabel_encoder = preprocessing.LabelEncoder() \n\ndf1 = df1.apply(label_encoder.fit_transform)","30db778e":"df.head()","ae536f9a":"y = df1['hosp_yn']\nX = df1[['sex','age_group','Race and ethnicity (combined)','medcond_yn']]","f78d7589":"from sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=1)","33e4797a":"from sklearn import svm\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import classification_report","07552a22":"tuned_parameters = [{'kernel': ['rbf'], 'gamma': [1e-3, 1e-4],\n                     'C': [1, 10, 100, 1000]},\n                    {'kernel': ['linear'], 'C': [1, 10, 100, 1000]}]\n\nCV_svm = GridSearchCV(SVC(), tuned_parameters)\nCV_svm.fit(X_train, y_train)\nCV_svm.best_params_","70b69233":"svm = SVC(C=1.0, kernel='linear', gamma=0.001)\nsvm.fit(X_train, y_train)\ny_pred_svm = svm.predict(X_test)","fde30395":"from sklearn.metrics import accuracy_score\nfrom sklearn.metrics import confusion_matrix\n\nprint(accuracy_score(y_test, y_pred_svm))\nprint(confusion_matrix(y_test, y_pred_svm))","fe6cb106":"from sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import GridSearchCV\n\ntree_para = {'criterion':['gini','entropy'],\n             'max_depth':[1,2,3,5,10,50,100]}\nCV_tree = GridSearchCV(DecisionTreeClassifier(), tree_para, cv=5)\nCV_tree.fit(X_train, y_train)\nCV_tree.best_params_","23c59e47":"from sklearn.tree import DecisionTreeClassifier\n\ntree = DecisionTreeClassifier(max_depth=2, criterion='gini')\ntree.fit(X_train, y_train)\n\ny_pred_tree = tree.predict(X_test)","11b0c1d4":"print(accuracy_score(y_test, y_pred_tree))\nprint(confusion_matrix(y_test, y_pred_tree))","70818173":"import matplotlib.pyplot as plt\n\nfrom sklearn.ensemble import ExtraTreesClassifier\n\n# Build a classification task using 3 informative features\n\n# Build a forest and compute the impurity-based feature importances\nforest = ExtraTreesClassifier(n_estimators=4,\n                              random_state=0)\n\nforest.fit(X_train, y_train)\nimportances = forest.feature_importances_\nstd = np.std([tree.feature_importances_ for tree in forest.estimators_],\n             axis=0)\nindices = np.argsort(importances)[::-1]\n\n# Print the feature ranking\nprint(\"Feature ranking:\")\n\nfor f in range(X_train.shape[1]):\n    print(\"%d. feature %d (%f)\" % (f + 1, indices[f], importances[indices[f]]))\n\n# Plot the impurity-based feature importances of the forest\nplt.figure()\nplt.title(\"Feature importances\")\nplt.bar(range(X_train.shape[1]), importances[indices],\n        color=\"b\", yerr=std[indices], align=\"center\")\nplt.xticks(range(X_train.shape[1]), indices)\nplt.xlim([-1, X_train.shape[1]])\nplt.show()","35c5bd04":"X_train, y_train","8a099c8e":"Let's build an ExtraTrees classifier to get feature importances of the datase","f8ba1dc6":"We start with SVM.","4ba54823":"# Let's begin exploring the algorithms!","e4f98176":"Now let's create a pipeline for a standard procedure for other algorithms to use.","918ae2f8":"**The purpose of this project is to build a ML algorithm to define whether or not the person with the features from the dataset will be hospitalized.**","a30d295f":"**Looks like the only important value is whether or not the patient has a medical condition.**"}}