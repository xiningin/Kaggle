{"cell_type":{"61f5b9dd":"code","aea91a15":"code","f7878c8a":"code","d397fc29":"code","93b8cc4c":"code","6354eb66":"code","580ec80a":"code","ffb80c63":"code","4edd73b4":"code","3ba23b47":"markdown","a80db05a":"markdown","976f66a9":"markdown","d522754c":"markdown","d3938826":"markdown"},"source":{"61f5b9dd":"# TensorFlow\nimport tensorflow as tf\nprint(\"Tensorflow version \" + tf.__version__)\n\n# TF 2.3 version\n# Detect and init the TPU\n# try: # detect TPUs\n#     tpu = tf.distribute.cluster_resolver.TPUClusterResolver.connect() # TPU detection\n#     strategy = tf.distribute.TPUStrategy(tpu)\n# except ValueError: # detect GPUs\n#     strategy = tf.distribute.get_strategy() # default strategy that works on CPU and single GPU\n# print(\"Number of accelerators: \", strategy.num_replicas_in_sync)\n\n# TF 2.2 version\ntry:\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\nexcept ValueError:\n    strategy = tf.distribute.get_strategy() # default strategy that works on CPU and single GPU\nprint(\"Number of accelerators: \", strategy.num_replicas_in_sync)\n    \n# Plotting\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# Matplotlib defaults\nplt.style.use('seaborn-whitegrid')\nplt.rc('figure', autolayout=True)\nplt.rc('axes', labelweight='bold', labelsize='large',\n       titleweight='bold', titlesize=18, titlepad=10)\n\n\n# Data\nfrom kaggle_datasets import KaggleDatasets\nfrom tensorflow.io import FixedLenFeature\nAUTO = tf.data.experimental.AUTOTUNE\n\n\n# Model\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom tensorflow.keras import callbacks","aea91a15":"# Model Configuration\nUNITS = 2 ** 11 # 2048\nACTIVATION = 'relu'\nDROPOUT = 0.1\n\n# Training Configuration\nBATCH_SIZE_PER_REPLICA = 2 ** 11","f7878c8a":"def make_decoder(feature_description):\n    def decoder(example):\n        example = tf.io.parse_single_example(example, feature_description)\n        features = tf.io.parse_tensor(example['features'], tf.float32)\n        features = tf.reshape(features, [28])\n        label = example['label']\n        return features, label\n    return decoder\n\ndef load_dataset(filenames, decoder, ordered=False):\n    AUTO = tf.data.experimental.AUTOTUNE\n    ignore_order = tf.data.Options()\n    if not ordered:\n        ignore_order.experimental_deterministic = False\n    dataset = (\n        tf.data\n        .TFRecordDataset(filenames, num_parallel_reads=AUTO)\n        .with_options(ignore_order)\n        .map(decoder, AUTO)\n    )\n    return dataset","d397fc29":"dataset_size = int(11e6)\nvalidation_size = int(5e5)\ntraining_size = dataset_size - validation_size\n\n# For model.fit\nbatch_size = BATCH_SIZE_PER_REPLICA * strategy.num_replicas_in_sync\nsteps_per_epoch = training_size \/\/ batch_size\nvalidation_steps = validation_size \/\/ batch_size\n\n# For model.compile\nsteps_per_execution = steps_per_epoch","93b8cc4c":"feature_description = {\n    'features': FixedLenFeature([], tf.string),\n    'label': FixedLenFeature([], tf.float32),\n}\ndecoder = make_decoder(feature_description)\n\ndata_dir = KaggleDatasets().get_gcs_path('higgs-boson')\ntrain_files = tf.io.gfile.glob(data_dir + '\/training' + '\/*.tfrecord')\nvalid_files = tf.io.gfile.glob(data_dir + '\/validation' + '\/*.tfrecord')\n\nds_train = load_dataset(train_files, decoder, ordered=False)\nds_train = (\n    ds_train\n    .cache()\n    .repeat()\n    .shuffle(2 ** 19)\n    .batch(batch_size)\n    .prefetch(AUTO)\n)\n\nds_valid = load_dataset(valid_files, decoder, ordered=False)\nds_valid = (\n    ds_valid\n    .batch(batch_size)\n    .cache()\n    .prefetch(AUTO)\n)","6354eb66":"def dense_block(units, activation, dropout_rate, l1=None, l2=None):\n    def make(inputs):\n        x = layers.Dense(units)(inputs)\n        x = layers.BatchNormalization()(x)\n        x = layers.Activation(activation)(x)\n        x = layers.Dropout(dropout_rate)(x)\n        return x\n    return make\n\nwith strategy.scope():\n    # Wide Network\n    wide = keras.experimental.LinearModel()\n\n    # Deep Network\n    inputs = keras.Input(shape=[28])\n    x = dense_block(UNITS, ACTIVATION, DROPOUT)(inputs)\n    x = dense_block(UNITS, ACTIVATION, DROPOUT)(x)\n    x = dense_block(UNITS, ACTIVATION, DROPOUT)(x)\n    x = dense_block(UNITS, ACTIVATION, DROPOUT)(x)\n    x = dense_block(UNITS, ACTIVATION, DROPOUT)(x)\n    outputs = layers.Dense(1)(x)\n    deep = keras.Model(inputs=inputs, outputs=outputs)\n    \n    # Wide and Deep Network\n    wide_and_deep = keras.experimental.WideDeepModel(\n        linear_model=wide,\n        dnn_model=deep,\n        activation='sigmoid',\n    )\n\nwide_and_deep.compile(\n    loss='binary_crossentropy',\n    optimizer='adam',\n    metrics=['AUC', 'binary_accuracy'],\n#     experimental_steps_per_execution=steps_per_execution,\n)","580ec80a":"early_stopping = callbacks.EarlyStopping(\n    patience=2,\n    min_delta=1e-3,\n    restore_best_weights=True,\n)\n\nlr_schedule = callbacks.ReduceLROnPlateau()","ffb80c63":"history = wide_and_deep.fit(\n    ds_train,\n    validation_data=ds_valid,\n    epochs=50,\n    steps_per_epoch=steps_per_epoch,\n    validation_steps=validation_steps,\n    callbacks=[early_stopping, lr_schedule],\n)","4edd73b4":"history_frame = pd.DataFrame(history.history)\nhistory_frame.loc[:, ['loss', 'val_loss']].plot(title='Cross-entropy Loss')\nhistory_frame.loc[:, ['auc', 'val_auc']].plot(title='AUC');","3ba23b47":"# Configuration #","a80db05a":"# Load Data #","976f66a9":"# Training #","d522754c":"# Setup #","d3938826":"# Model #\n"}}