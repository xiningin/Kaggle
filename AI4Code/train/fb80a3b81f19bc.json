{"cell_type":{"fcfbd6ec":"code","5fef0227":"code","ebe3bfd5":"code","29ba8b35":"code","6ca89df8":"code","e3873c10":"markdown","da82cc01":"markdown","eefe5865":"markdown","fb4548aa":"markdown","b3d238aa":"markdown","f95f714c":"markdown","cb6f09e8":"markdown"},"source":{"fcfbd6ec":"from sklearn import datasets, linear_model\nfrom sklearn.model_selection import train_test_split\n\n# Load the Diabetes dataset\ndiabetes = datasets.load_diabetes() # Call the diabetes dataset from sklearn\nX = diabetes.data \ny = diabetes.target # define the target variable (dependent variable) as y\n\n# create training and testing vars\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, test_size=0.2)\n# To see the effect of random_state, run train_test_split WITHOUT parameter random_state=<some_number>\n# You would get a different output of the train\/test data everytime (size will be same because test_size has been set to 0.2)\n# eg. X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n\nprint (\"Train set size: \", X_train.shape, y_train.shape)\nprint (\"Test set size:\", X_test.shape, y_test.shape)\nprint(\"Few train data: \", X_train[:5], y_train[:5])","5fef0227":"import numpy as np\nfrom sklearn.model_selection import KFold, LeaveOneOut, StratifiedKFold\n\nX = np.ones(10)\ny = [0, 0, 0, 0, 1, 1, 1, 1, 1, 1]\n\nkf = KFold(n_splits=3)\nprint(\"KFold:\")\nfor train, test in kf.split(X):\n    print(\"%s %s\" % (train, test))\n\nloo = LeaveOneOut()\nprint(\"LeaveOneOut:\")\nfor train, test in loo.split(X):\n    print(\"%s %s\" % (train, test))\n\n\nskf = StratifiedKFold(n_splits=3)\nprint(\"StratifiedKFold:\")\n# Notice, in the output of test set, '0', '1', '2', '3' belongs to class y=0. Proportion has been maintained in each split\nfor train, test in skf.split(X, y):\n    print(\"%s %s\" % (train, test))\n","ebe3bfd5":"import numpy as np\n\nfrom time import time\nfrom scipy.stats import randint as sp_randint\n\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.datasets import load_digits\nfrom sklearn.ensemble import RandomForestClassifier\n\n# get some data\ndigits = load_digits()\nX, y = digits.data, digits.target\n\n# build a classifier\nclf = RandomForestClassifier(n_estimators=20)\n\n\n# Utility function to report best scores\ndef report(results, n_top=3):\n    for i in range(1, n_top + 1):\n        candidates = np.flatnonzero(results['rank_test_score'] == i)\n        for candidate in candidates:\n            print(\"Model with rank: {0}\".format(i))\n            print(\"Mean validation score: {0:.3f} (std: {1:.3f})\".format(\n                  results['mean_test_score'][candidate],\n                  results['std_test_score'][candidate]))\n            print(\"Parameters: {0}\".format(results['params'][candidate]))\n            print(\"\")\n\n\n# Specify parameters and distributions to sample from. \nparam_dist = {\"max_depth\": [3, None],\n              \"max_features\": sp_randint(1, 11),\n              \"min_samples_split\": sp_randint(2, 11),\n              \"bootstrap\": [True, False],\n              \"criterion\": [\"gini\", \"entropy\"]}\n\n# run randomized search\nn_iter_search = 20\nrandom_search = RandomizedSearchCV(clf, param_distributions=param_dist, n_iter=n_iter_search, cv=5, iid=False)\nstart = time()\nrandom_search.fit(X, y)\nprint(\"RandomizedSearchCV took %.2f seconds for %d candidates\"\n      \" parameter settings.\" % ((time() - start), n_iter_search))\nreport(random_search.cv_results_)\n\n# use a full grid over all parameters\nparam_grid = {\"max_depth\": [3, None],\n              \"max_features\": [1, 3, 10],\n              \"min_samples_split\": [2, 3, 10],\n              \"bootstrap\": [True, False],\n              \"criterion\": [\"gini\", \"entropy\"]}\n\n# run grid search\ngrid_search = GridSearchCV(clf, param_grid=param_grid, cv=5, iid=False)\nstart = time()\ngrid_search.fit(X, y)\nprint(\"GridSearchCV took %.2f seconds for %d candidate parameter settings.\"\n      % (time() - start, len(grid_search.cv_results_['params'])))\nreport(grid_search.cv_results_)","29ba8b35":"from sklearn.preprocessing import MinMaxScaler\nfrom sklearn.preprocessing import StandardScaler\n\ndata = [[-1, 2], [-0.5, 6], [0, 10], [1, 18]]\n\nscaler = MinMaxScaler()\nscaler.fit(data)\nprint(\"With MinMaxScaler:\")\nprint(scaler.transform(data))\n\nscaler = StandardScaler()\nscaler.fit(data)\nprint(\"With StandardScaler:\")\nprint(scaler.transform(data))","6ca89df8":"from sklearn.preprocessing import OneHotEncoder\nenc = OneHotEncoder()\nX = [['male', 'from US', 'uses Safari'], ['female', 'from Europe', 'uses Firefox']]\nenc.fit(X)  \nenc.transform([['female', 'from US', 'uses Safari'], ['male', 'from Europe', 'uses Safari']]).toarray()","e3873c10":"### Cross Validation\nWhat if the split we make with train_test_split isn't random enough? What if our split includes only the first few targets (maybe of the same class everytime) of the dataset? This wouldn't be a fair split. This is where cross validation comes in.<br>\nWe split our data into k subsets, and train on k-1 subsets. We hold out the k<sup>th<\/sup> subset for test. We do it for each of the subsets. Now we look at two major cross validation methods.<br>\n* K-Folds Cross Validation<br>\nK-fold divides all the samples in groups of samples, called folds of equal sizes (if possible). The prediction function is learned using folds, and the fold left out is used for test.\n* Leave One Out Cross Validation (LOO)<br>\nIf k=n, this is equivalent to the Leave One Out strategy. Each learning set is created by taking all the samples except one, the test set being the sample left out. Thus, for n samples, we have n different training sets and n different tests set.\n* Stratified K Fold <br>\nStratified K Fold is a variation of k-fold which returns stratified folds: each set contains approximately the same percentage of samples of each target class as the complete set.\n<br><br>\nAs a general rule, most authors, and empirical evidence, suggest that 5- or 10- fold cross validation should be preferred to LOO.","da82cc01":"### Encoding categorical features\nOften features are not given as continuous values but categorical. For example a feature could have values [\"male\", \"female\"], [\"from Europe\", \"from US\", \"from Asia\"], etc. Such features can be efficiently coded as integers.<br>\nOneHotEncoder transforms each categorical feature with n_categories possible values into n_categories binary features, with one of them 1, and all others 0.<br>\nAnother one is sklearn.preprocessing.OrdinalEncoder which performs an ordinal (integer) encoding of the categorical features.","eefe5865":"References:<br>\n* https:\/\/scikit-learn.org\/\n* Applied Machine Learning by Kevyn Collins-Thompson","fb4548aa":"### Grid Search\nExhaustive search over specified parameter values for an estimator. The parameters of the estimator used to apply these methods are optimized by cross-validated grid-search over a parameter grid.<br>\nHyperparameters are model-specific properties that are \u2018fixed\u2019 even before the model is trained or tested on the data. For Example: In the case of a random forest, hyper parameters include the number of decision trees in the forest , for a neural network, there is the learning rate, the number of hidden layers, the number of units in each layer, and several other parameters.<br><br>\nTwo generic approaches to sampling search candidates are provided in scikit-learn: \n#### RandomizedSearchCV\nCan sample a given number of candidates from a parameter space with a specified distribution. Parameters are:\n* First argument is the estimator object. Either estimator needs to provide a score function, or scoring must be passed.\n* param_distributions : Dictionary with parameters names (string) as keys and distributions or lists of parameters to try. In our example below, 'max_depth', 'max_featues', etc. are various parameters for RandomForestClassifier. We specify the values that each of these parameters can take in param_dist. Visit [RandomForestClassifier](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.ensemble.RandomForestClassifier.html) to know more about these parameters\n* n_iter : int, default=10. Number of parameter settings that are sampled. n_iter trades off runtime vs quality of the solution\n* cv : int, cross-validation generator or an iterable, optional. Determines the cross-validation splitting strategy. Possible inputs for cv can be None (the default 3-fold cross validation), integer (to specify the number of folds in a KFold), CV splitter, an iterable yielding (train, test) splits as arrays of indices.<br>\nFor integer\/None inputs, if the estimator is a classifier and y is either binary or multiclass, StratifiedKFold is used. In all other cases, KFold is used.\n\n#### GridSearchCV \nExhaustively considers all parameter combinations. Parameters are:\n* First argument is the estimator object. Either estimator needs to provide a score function, or scoring must be passed.\n* param_grid : dict. Dictionary with parameters names (string) as keys and lists of parameter settings to try as values\n* cv : int, cross-validation generator or an iterable, optional. Determines the cross-validation splitting strategy. Possible inputs for cv can be None (the default 3-fold cross validation), integer (to specify the number of folds in a KFold), CV splitter, an iterable yielding (train, test) splits as arrays of indices.<br>\nFor integer\/None inputs, if the estimator is a classifier and y is either binary or multiclass, StratifiedKFold is used. In all other cases, KFold is used.\n\n\n    Important members are fit, predict.","b3d238aa":"Following notes are compiled with brief description of the model, syntax, parameters to be tuned, any tips\/tricks that I came across. These are my notes from my exploration.<br><br>\nTopics covered in part 1 are:\n* Splitting to train and test data\n* Cross Validation\n* Grid Search\n* Feature Normalization\n* Encoding categorical features","f95f714c":"### Splitting to train and test data\nPre Requisites:<br>\nIt is better to have basic understanding of overfitting\/ underfitting. If you want to revise it, kindly visit [The Problem of Overfitting](https:\/\/www.coursera.org\/lecture\/machine-learning\/the-problem-of-overfitting-ACpTQ) <br>\nTo know about why data needs to be split, visit [Splitting Data](https:\/\/www.coursera.org\/lecture\/machine-learning\/model-selection-and-train-validation-test-sets-QGKbr)<br><br>\nWe can't manually split the dataset everytime. Also, we have to ensure that it is randomnly split. \u2018train_test_split\u2019 in scikit helps us with this task.<br><br>\nParameters to be considered:<br>\n* test_size : float, int or None, optional (default=None)<br>\nIf float, should be between 0.0 and 1.0 and represent the proportion of the dataset to include in the test split. If int, represents the absolute number of test samples. If None, the value is set to the complement of the train size. If train_size is also None, it will be set to 0.25.\n* train_size : float, int, or None, (default=None)<br>\nIf float, should be between 0.0 and 1.0 and represent the proportion of the dataset to include in the train split. If int, represents the absolute number of train samples. If None, the value is automatically set to the complement of the test size.\n* random_state : int, RandomState instance or None, optional (default=None)<br>\ntrain_test_split splits arrays or matrices into random train and test subsets. That means that everytime you run it without specifying random_state you will get a different result. On the other hand if you use random_state=some_number, then you can guarantee that your split will be always the same. \n","cb6f09e8":"### Feature Normalization\nIt is important for some machine learning methods that all features are on the same scale (e.g. faster convergence in learning, more uniform or 'fair' influence for all weights) e.g. regularized regression, k-NN, support vector machines, neural networks, etc.<br>\nFew important scalers are:\n* MinMaxScaler<br>\n    Transforms features by scaling each feature to a given range. Parameters are: <br>\n    1. feature_range : tuple (min, max), default=(0, 1). Desired range of transformed data\n    2. copy : boolean, optional, default True. Set to False to perform inplace row normalization and avoid a copy (if the input is already a numpy array)\n* StandardScaler<br>\n    Standardize features by removing the mean and scaling to unit variance. Parameters are:<br>\n    1. copy : boolean, optional, default True. If False, try to avoid a copy and do inplace scaling instead\n    2. with_mean : boolean, True by default. If True, center the data before scaling. This does not work (and will raise an exception) when attempted on sparse matrices, because centering them entails building a dense matrix which in common use cases is likely to be too large to fit in memory\n    3. with_std : boolean, True by default. If True, scale the data to unit variance (or equivalently, unit standard deviation).\n    \n##### Important Note on Feature Normalization:\nThe test set must use identical scaling to the training set\n* Fit the scaler using the training set, then apply the same scaler to transform the test set.\n* Do not scale the training and test sets using different scalers: this could lead to random skew in the data.\n* Do not fit the scaler using any part of the test data: referencing the test data can lead to a form of data leakage."}}