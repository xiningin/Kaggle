{"cell_type":{"df90e88a":"code","9fca0bbe":"code","5e97f984":"code","2b74740b":"code","47762cad":"code","b92074a7":"code","20943584":"code","38366c75":"code","cd3a0f32":"code","81ea107d":"code","2f0c5d27":"code","3c7abee7":"code","fd0fc9d7":"code","b5e49116":"code","9656cb9e":"code","d7ed4e35":"code","cf4b3bbd":"code","30aead22":"code","4e5fb47f":"code","b961a84b":"code","9d706535":"code","744693bc":"code","1ec14dfd":"code","63a0c9f4":"code","08499fb1":"code","9f4d16f5":"code","409e6c63":"code","37a12050":"code","0e019a6e":"code","ec3fadef":"code","24184560":"code","b24f148f":"code","a242b941":"code","cf40b96f":"code","84acc57f":"code","2bb3784d":"code","44f0a154":"code","08c2c803":"code","7254ea47":"code","3152ff1e":"code","e260a679":"code","e8243b1d":"code","69703e72":"code","8c589deb":"code","7db607a1":"code","8994cd98":"code","52ce3a70":"code","e9b60eeb":"code","664b2963":"code","0f654021":"code","bbc1dac3":"code","1cc9ddb9":"code","a6b48b67":"code","31a22fed":"code","3c937433":"code","e4e0333b":"code","68efe0b5":"code","aa988178":"code","bf42d4ee":"code","42517d1a":"code","464867dc":"code","ee530b40":"code","1bdcd4f7":"code","ae1f85f8":"code","f3ec8e9b":"code","c7c0ee82":"code","f317105a":"code","21a0d96a":"code","0d8f206d":"code","9406abae":"code","3b98c2da":"code","4adc127e":"code","c510e582":"code","61e7faa2":"code","62855028":"code","338ec5f2":"code","054bc41e":"code","ee68c145":"code","bd5c8ec2":"code","37e33204":"code","a33dc06f":"code","e1ba1913":"code","a29d9fb5":"code","ffab6509":"code","3838b64e":"code","cba5e948":"code","77c0029c":"code","da44ac54":"code","cd825c04":"code","ff0ec214":"code","b97aa493":"code","61049ad9":"code","d861dad9":"code","d57fe04e":"code","77320fba":"code","3ee44b72":"code","96207d49":"code","53cfdff1":"code","5182e1c4":"code","56013a24":"code","2a753bf5":"code","3798326b":"code","358028e6":"code","9f7ed5ea":"code","8c864789":"code","5161b80b":"code","539e9992":"code","01f27b12":"code","ef1be1af":"code","1a968d98":"code","7e188e66":"code","e9766d59":"code","5820a76b":"code","fb5e9da2":"code","d6e29825":"code","2a281b3e":"code","6514c3e6":"code","36987e28":"code","792ffd53":"code","a675616e":"code","176daf0f":"code","ec7f950f":"code","31d1a520":"code","aaeed0ef":"code","e71e2ad2":"code","6c797429":"code","38cb3421":"code","b8267747":"code","2602526c":"code","fc1c89da":"code","bc6de65c":"code","453e6496":"code","a8772c0d":"code","db42654a":"code","2cc00327":"code","f162f1d8":"code","49d091ca":"code","fd02d777":"code","c09c44b8":"code","ea6cb9e0":"code","8f3f7252":"code","44934abf":"code","f8186894":"code","5511c0ae":"code","3acd27c6":"code","0375291c":"code","c841c2b6":"code","a07ea62f":"code","33b2a4a2":"code","4c6c26c1":"code","165ad4ff":"code","52fc7b0c":"code","9a53ed0d":"code","2a37f423":"code","b0cc8c25":"code","08772391":"code","4f8f1143":"code","33853135":"code","15cf96f4":"code","4696e262":"code","b4ceb31b":"code","cf7bf97b":"code","7d67a29c":"code","0068e151":"code","c0f55bb6":"markdown","67b0b283":"markdown","af9e6cc6":"markdown","704477c5":"markdown","e1ff5404":"markdown","16eee669":"markdown","dbb82f02":"markdown","f825012f":"markdown","f191eff3":"markdown","1ad6aba9":"markdown","bd88bdad":"markdown","59bde003":"markdown","3ec5202b":"markdown","635c717a":"markdown","09b51c41":"markdown","e98b2e19":"markdown","08444bee":"markdown","2270bd93":"markdown","05739f8b":"markdown","a8fee5e1":"markdown","5687358c":"markdown","8d478c6a":"markdown","2be4f0b5":"markdown","b24d4db9":"markdown","9b22f811":"markdown","b79fc7cd":"markdown","a2a8b4b0":"markdown","34134d35":"markdown","c869f080":"markdown","123ac9a9":"markdown","f11d5514":"markdown","c3b5eb1c":"markdown","7fe68484":"markdown","43dbdf03":"markdown","28106c71":"markdown","9ec77784":"markdown","85f83a93":"markdown","06d7be7b":"markdown","1d3ff3a3":"markdown","7aae7592":"markdown"},"source":{"df90e88a":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","9fca0bbe":"train_anno_df=pd.read_csv('..\/input\/cv2-csvs\/Train Annotations.csv')\ntest_anno_df=pd.read_csv('..\/input\/cv2-csvs\/Test Annotation.csv')","5e97f984":"train_anno_df.rename(columns={'Bounding Box coordinates': 'x0', 'Unnamed: 2': 'y0','Unnamed: 3': 'x1','Unnamed: 4': 'y1'}, inplace=True)\ntrain_anno_df.info()","2b74740b":"train_anno_df.head(10)","47762cad":"test_anno_df.rename(columns={'Bounding Box coordinates': 'x0', 'Unnamed: 2': 'y0','Unnamed: 3': 'x1','Unnamed: 4': 'y1'}, inplace=True)\ntest_anno_df.info()","b92074a7":"test_anno_df.head(10)","20943584":"#Loading name of cars \ncarNameDF= pd.read_csv(\"..\/input\/stanford-car-dataset-by-classes-folder\/names.csv\",header=None)","38366c75":"carNameDF.shape","cd3a0f32":"carNameDF.head(10)","81ea107d":"carNameDF.info()","2f0c5d27":"IMAGE_WIDTH=128\nIMAGE_HEIGHT=128\nCHANNEL=3","3c7abee7":"import os\nimport tensorflow\nfrom keras.preprocessing import image\n#from tqdm import tqdm\ndef createImageDF(imageSourceFolder,target_size_tuple,colormode,totalImagePerClassCap):\n    dfRows=[]\n    for dirname, _, filenames in os.walk(imageSourceFolder):\n            limit=0\n            if len(filenames) < totalImagePerClassCap or totalImagePerClassCap == 0:\n                limit=len(filenames)\n            else:\n                limit=totalImagePerClassCap\n            for i in range(0,limit):\n                    processedRows=[]\n                    filename=filenames[i]\n                    processedRows.append(filename)\n                    pathSplits=dirname.split('\/')\n                    #print(os.path.join(dirname, filename))\n                    imgPath=os.path.join(dirname, filename)\n                    #print(imgPath)\n                    img = image.load_img(imgPath)\n                    if target_size_tuple!='':\n                        img_scaled = image.load_img(imgPath, target_size=target_size_tuple, color_mode = colormode)\n                        image_width_scale_fact=target_size_tuple[0]\/img.width\n                        image_height_scale_fact=target_size_tuple[1]\/img.height\n                        imgArray = image.img_to_array(img_scaled)\n                    else:\n                        img\n                        image_width_scale_fact=img.width\n                        image_height_scale_fact=img.height\n                        #img = Image.open(imgPath)\n                        imgArray = image.img_to_array(img)\n                    #imgArray=np.asarray(img)\n                    processedRows.append(imgArray)\n                    processedRows.append(pathSplits[len(pathSplits)-1])\n                    processedRows.append(image_width_scale_fact)\n                    processedRows.append(image_height_scale_fact)\n                    dfRows.append(processedRows)\n    return pd.DataFrame(dfRows, columns=[\"Image Name\",\"image_array\",\"model\",\"image_width_scale_fact\",\"image_height_scale_fact\"])\n","fd0fc9d7":"def loadSingleScoringImage(imageSourcePath,target_size_tuple,colormode):\n    dfRows=[]\n    processedRow=[]\n    imgPath=imageSourcePath\n    img = image.load_img(imgPath)\n    pathSplits=imgPath.split('\/')\n    if target_size_tuple!='':\n        img_scaled = image.load_img(imgPath, target_size=target_size_tuple, color_mode = colormode)\n        image_width_scale_fact=target_size_tuple[0]\/img.width\n        image_height_scale_fact=target_size_tuple[1]\/img.height\n        imgArray = image.img_to_array(img_scaled)\n    else:\n        img\n        image_width_scale_fact=img.width\n        image_height_scale_fact=img.height\n        #img = Image.open(imgPath)\n        imgArray = image.img_to_array(img)\n    #imgArray=np.asarray(img)\n    processedRow.append(pathSplits[len(pathSplits)-1])\n    processedRow.append(imgArray)\n    processedRow.append(pathSplits[len(pathSplits)-2])\n    processedRow.append(image_width_scale_fact)\n    processedRow.append(image_height_scale_fact)\n    dfRows.append(processedRow)\n    return pd.DataFrame(dfRows, columns=[\"Image Name\",\"image_array\",\"model\",\"image_width_scale_fact\",\"image_height_scale_fact\"])","b5e49116":"#trainImageDF=createImageDF('..\/input\/stanford-car-dataset-by-classes-folder\/car_data\/car_data\/train','',\"rgb\",2)","9656cb9e":"#trainImageDF.head(10)","d7ed4e35":"# Lodaing annotation details for images from train_anno_df i.e. train annotations\n#trainMergedImageDF=pd.merge(trainImageDF,train_anno_df, on=['Image Name'])","cf4b3bbd":"#trainMergedImageDF.head(10)","30aead22":"#len(trainMergedImageDF)","4e5fb47f":"import matplotlib.pyplot as plt\nimport matplotlib.patches as patches\n\ndef drowBoxedImages(dataFrame,featureCol,x0,y0,x1,y1,boxlinewidth,boxedgecolor):\n  fig,ax=plt.subplots(len(dataFrame))\n  fig.set_size_inches(15,15)\n  # Display the image\n  for index, row in dataFrame.iterrows():\n    data=row[featureCol]\n    img = image.array_to_img(data)\n    ax[index].imshow(img)\n    image_height, image_width, _ = data.shape\n    rect=[]\n    rect.append(patches.Rectangle((row[x0], row[y0]), row[x1] - row[x0], row[y1] - row[y0], linewidth=boxlinewidth, edgecolor=boxedgecolor, facecolor='none'))\n    # Add the patch to the Axes\n    for l in range(0,len(rect)):\n      ax[index].add_patch(rect[l])\n  plt.show()","b961a84b":"def drowImages(dataFrame,featureCol):\n  fig,ax=plt.subplots(len(dataFrame))\n  fig.set_size_inches(15,15)\n  # Display the image\n  for index, row in dataFrame.iterrows():\n    data=row[featureCol]\n    img = image.array_to_img(data)\n    ax[index].imshow(img)\n  plt.show()","9d706535":"#drowBoxedImages(trainMergedImageDF.head(10),'image_array','x0','y0','x1','y1',2,'r')","744693bc":"#testImageDF=createImageDF('..\/input\/stanford-car-dataset-by-classes-folder\/car_data\/car_data\/test','',\"rgb\",2)","1ec14dfd":"#testImageDF.head(10)","63a0c9f4":"# Lodaing annotation details for images from train_anno_df i.e. train annotations\n#testMergedImageDF=pd.merge(testImageDF,test_anno_df, on=['Image Name'])","08499fb1":"#testMergedImageDF.head(10)","9f4d16f5":"#drowBoxedImages(testMergedImageDF.head(10),'image_array','x0','y0','x1','y1',2,'r')","409e6c63":"import seaborn as sns\nsns.set_theme(style=\"darkgrid\")\n#sns.histplot(x= trainMergedImageDF['Image class'],bins = 20)","37a12050":"#trainReducedImageDF=createImageDF('..\/input\/stanford-car-dataset-by-classes-folder\/car_data\/car_data\/train',(IMAGE_WIDTH,IMAGE_HEIGHT,CHANNEL),\"rgb\",5)","0e019a6e":"#trainReducedImageDF.head(5)","ec3fadef":"#drowImages(trainReducedImageDF.head(5),'image_array')","24184560":"# Lodaing annotation details for images from train_anno_df i.e. train annotations\n#trainReducedMergedImageDF=pd.merge(trainReducedImageDF,train_anno_df, on=['Image Name'])","b24f148f":"#trainReducedMergedImageDF.head()","a242b941":"def scaledBoundingBoxCreator(df):\n    df['x0_scaled']=df.x0 *df.image_width_scale_fact\n    df['y0_scaled']=df.y0*df.image_height_scale_fact\n    df['x1_scaled']=df.x1*df.image_width_scale_fact\n    df['y1_scaled']=df.y1*df.image_height_scale_fact\n    df['x0_scaled']=df['x0_scaled'].apply(np.floor)\n    df['y0_scaled']=df['y0_scaled'].apply(np.floor)\n    df['x1_scaled']=df['x1_scaled'].apply(np.floor)\n    df['y1_scaled']=df['y1_scaled'].apply(np.floor)","cf40b96f":"#scaledBoundingBoxCreator(trainReducedMergedImageDF)","84acc57f":"#trainReducedMergedImageDF.head()","2bb3784d":"#drowBoxedImages(trainReducedMergedImageDF.head(),'image_array','x0_scaled','y0_scaled','x1_scaled','y1_scaled',2,'g')","44f0a154":"#trainReducedMergedImageDF['image_array']=trainReducedMergedImageDF['image_array']\/255","08c2c803":"#trainReducedMergedImageDF.head(2)","7254ea47":"#drowBoxedImages(trainReducedMergedImageDF.head(),'image_array','x0_scaled','y0_scaled','x1_scaled','y1_scaled',2,'g')","3152ff1e":"#testReducedImageDF=createImageDF('..\/input\/stanford-car-dataset-by-classes-folder\/car_data\/car_data\/test',(IMAGE_WIDTH,IMAGE_HEIGHT,CHANNEL),\"grayscale\",5)","e260a679":"# Lodaing annotation details for images from test_anno_df i.e. test annotations\n#testReducedMergedImageDF=pd.merge(testReducedImageDF,test_anno_df, on=['Image Name'])","e8243b1d":"#scaledBoundingBoxCreator(testReducedMergedImageDF)","69703e72":"#testReducedMergedImageDF['image_array']=testReducedMergedImageDF['image_array']\/255","8c589deb":"#testReducedMergedImageDF['image_array'].shape","7db607a1":"# Creating an empty tensor to store image arrays\ndef createFeatureTensor(dfCol,imgWth,imgHit,chnl):\n    tensor = np.zeros((dfCol.shape[0],imgWth,imgHit,chnl))\n    # Iterating through image arrays to update tensor\n    for idx, i in enumerate(dfCol):\n        tensor[idx] = i\n    # Checking the tensor shape\n    tensor.shape\n    return tensor","8994cd98":"# Creating an empty tensor to store image arrays\ndef createBondingBoxTensor(df,x0,y0,x1,y1):\n    labelBox = []\n    # Iterating through image arrays to update tensor\n    for i in range(0,len(df[x0])):\n        params=[]\n        params.append(df[x0][i])\n        params.append(df[y0][i])\n        params.append(df[x1][i])\n        params.append(df[y1][i])\n        labelBox.append(params)\n    return np.array(labelBox)","52ce3a70":"#x_train = createFeatureTensor(trainReducedMergedImageDF['image_array'],IMAGE_WIDTH,IMAGE_HEIGHT,CHANNEL)\n#x_test = createFeatureTensor(testReducedMergedImageDF['image_array'],IMAGE_WIDTH,IMAGE_HEIGHT,CHANNEL)","e9b60eeb":"#x_train.shape","664b2963":"from scipy import ndimage as ndi\nfrom skimage import segmentation\nfrom skimage import filters\ndef createMask(img):\n    sobel = filters.sobel(img)\n    blurred = filters.gaussian(sobel, sigma=2.0,multichannel=False)\n    light_spots = np.array((img > 180).nonzero()).T\n    dark_spots = np.array((img < 2).nonzero()).T\n    bool_mask = np.zeros(img.shape, dtype=np.bool)\n    bool_mask[tuple(light_spots.T)] = True\n    bool_mask[tuple(dark_spots.T)] = True\n    seed_mask, num_seeds = ndi.label(bool_mask)\n    ws = segmentation.watershed(sobel, seed_mask)\n    return ws\n\ndef createMaskedImageDF(imageSourceFolder,target_size_tuple,colormode,totalImagePerClassCap):\n    dfRows=[]\n    for dirname, _, filenames in os.walk(imageSourceFolder):\n            limit=0\n            if len(filenames) < totalImagePerClassCap or totalImagePerClassCap == 0:\n                limit=len(filenames)\n            else:\n                limit=totalImagePerClassCap\n            for i in range(0,limit):\n                    processedRows=[]\n                    filename=filenames[i]\n                    processedRows.append(filename)\n                    pathSplits=dirname.split('\/')\n                    #print(os.path.join(dirname, filename))\n                    imgPath=os.path.join(dirname, filename)\n                    #print(imgPath)\n                    img = image.load_img(imgPath)\n                    if target_size_tuple!='':\n                        img_scaled = image.load_img(imgPath, target_size=target_size_tuple, color_mode = colormode)\n                        image_width_scale_fact=target_size_tuple[0]\/img.width\n                        image_height_scale_fact=target_size_tuple[1]\/img.height\n                        imgArray = createMask(image.img_to_array(img_scaled))\n                    else:\n                        image_width_scale_fact=img.width\n                        image_height_scale_fact=img.height\n                        #img = Image.open(imgPath)\n                        imgArray =createMask(image.img_to_array(img))\n                    #imgArray=np.asarray(img)\n                    processedRows.append(imgArray)\n                    processedRows.append(pathSplits[len(pathSplits)-1])\n                    processedRows.append(image_width_scale_fact)\n                    processedRows.append(image_height_scale_fact)\n                    dfRows.append(processedRows)\n    return pd.DataFrame(dfRows, columns=[\"Image Name\",\"image_array\",\"model\",\"image_width_scale_fact\",\"image_height_scale_fact\"])","0f654021":"img_hight_classif=150\nimg_hight_classif=150\nchannels=3","bbc1dac3":"trainClassifReducedImageDF=createMaskedImageDF('..\/input\/stanford-car-dataset-by-classes-folder\/car_data\/car_data\/train',(img_hight_classif,img_hight_classif,channels),\"grayscale\",6)","1cc9ddb9":"testClassifReducedImageDF=createMaskedImageDF('..\/input\/stanford-car-dataset-by-classes-folder\/car_data\/car_data\/test',(img_hight_classif,img_hight_classif,channels),\"grayscale\",6)","a6b48b67":"trainClassifReducedMergedImageDF=pd.merge(trainClassifReducedImageDF,train_anno_df, on=['Image Name'])","31a22fed":"testClassifReducedMergedImageDF=pd.merge(testClassifReducedImageDF,test_anno_df, on=['Image Name'])","3c937433":"x_train_classif = createFeatureTensor(trainClassifReducedMergedImageDF['image_array'],img_hight_classif,img_hight_classif,channels)","e4e0333b":"x_test_classif = createFeatureTensor(testClassifReducedMergedImageDF['image_array'],img_hight_classif,img_hight_classif,channels)","68efe0b5":"x_train_classif.shape","aa988178":"y_train_classi=trainClassifReducedMergedImageDF['Image class']\nfrom sklearn.preprocessing import LabelEncoder\nfrom keras.utils import to_categorical\nle=LabelEncoder()\ny_train_tensor=le.fit_transform(y_train_classi)\ny_train_tensor=to_categorical(y_train_tensor,196)\n","bf42d4ee":"y_train_tensor.shape","42517d1a":"y_test_classi=testClassifReducedMergedImageDF['Image class']\nfrom sklearn.preprocessing import LabelEncoder\nfrom keras.utils import to_categorical\nle=LabelEncoder()\ny_test_tensor=le.fit_transform(y_test_classi)\ny_test_tensor=to_categorical(y_test_tensor,196)","464867dc":"y_train_tensor[196]","ee530b40":"y_test_tensor.shape","1bdcd4f7":"len(x_train_classif)","ae1f85f8":"x_train_classif.shape","f3ec8e9b":"from tensorflow.keras.layers import Conv2D\nfrom keras.utils.vis_utils import plot_model\n\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense,Flatten,Dropout,MaxPool2D\nfrom tensorflow.keras import regularizers, optimizers\n\nnnmodel = Sequential()\n#hidden_nodes=128\noutput_nodes=196 #As range of image labels is up to 196\nnnmodel.add(Conv2D(filters=32, kernel_size=2, activation=\"relu\", input_shape=(img_hight_classif,img_hight_classif,channels)))\nnnmodel.add(Conv2D(filters=32, kernel_size=2, activation=\"relu\"))\nnnmodel.add(Flatten())\nnnmodel.add(Dense(200, activation='relu'))\nnnmodel.add(Dense(output_nodes, activation='sigmoid'))\nadam = optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08)","c7c0ee82":"nnmodel.summary()","f317105a":"from keras.optimizers import SGD\nopt = SGD(lr=0.001)","21a0d96a":"nnmodel.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])","0d8f206d":"history=nnmodel.fit( x=x_train_classif, y=y_train_tensor, batch_size=128, epochs=10,shuffle=True,verbose=1,validation_split=0.2)","9406abae":"nnmodel.evaluate(x_test_classif, y_test_tensor)","3b98c2da":"def plotModelPerformanceCurvs(history):\n  plt.figure(figsize=(20, 7))\n  plt.subplot(1, 2, 1)\n  plt.plot(history.history[\"accuracy\"])\n  plt.plot(history.history[\"val_accuracy\"])\n  plt.title(\"Model Accuracy\")\n  plt.ylabel(\"Accuracy\")\n  plt.xlabel(\"Epoch\")\n  plt.legend([\"Train\", \"Val\"], loc=\"upper left\")\n  #plt.xticks(np.arange(0, len(model.history.history[\"binary_accuracy\"]), 1))\n\n  #plt.xticks(np.arange(len(nnmodel.history.history[\"accuracy\"])), np.arange(1, len(nnmodel.history.history[\"accuracy\"])+1, 1))\n\n  plt.subplot(1, 2, 2)\n  plt.plot(history.history[\"loss\"])\n  plt.plot(history.history[\"val_loss\"])\n  plt.title(\"Model Loss\")\n  plt.ylabel(\"Loss\")\n  plt.xlabel(\"Epoch\")\n  plt.legend([\"Train\", \"Val\"], loc=\"upper right\")\n  #plt.xticks(np.arange(len(nnmodel.history.history[\"loss\"])), np.arange(1, len(nnmodel.history.history[\"loss\"])+1, 1))\n  plt.show()","4adc127e":"print(f\"model hist is : \\n {history.history}\")","c510e582":"plotModelPerformanceCurvs(history)","61e7faa2":"from matplotlib import pyplot\na4_dims = (30, 50)\nfig, ax = pyplot.subplots(figsize=a4_dims)\nsns.countplot(ax=ax,y=trainClassifReducedMergedImageDF['Image class'], data=trainClassifReducedMergedImageDF, palette=\"Greens_d\",\n              order=trainClassifReducedMergedImageDF['Image class'].value_counts().iloc[:196].index)","62855028":"print(trainClassifReducedMergedImageDF['Image class'].value_counts())","338ec5f2":" trainClassifReducedMergedGRPDImageDF=trainClassifReducedMergedImageDF.groupby('Image class').head(6)","054bc41e":"from matplotlib import pyplot\na4_dims = (30, 50)\nfig, ax = pyplot.subplots(figsize=a4_dims)\nsns.countplot(ax=ax,y=trainClassifReducedMergedGRPDImageDF['Image class'], data=trainClassifReducedMergedGRPDImageDF, palette=\"Greens_d\",\n              order=trainClassifReducedMergedGRPDImageDF['Image class'].value_counts().iloc[:196].index)","ee68c145":"trainClassifReducedMergedGRPDImageDF.info()","bd5c8ec2":"trainClassifReducedMergedGRPDImageDF['image_array'][0]","37e33204":"print('max val ' ,np.max(trainClassifReducedMergedGRPDImageDF['image_array'][0]))\nprint('min val ' ,np.min(trainClassifReducedMergedGRPDImageDF['image_array'][0]))","a33dc06f":"trainClassifReducedMergedGRPDImageDF['image_array']=trainClassifReducedMergedGRPDImageDF['image_array'].apply(lambda x: x\/255)","e1ba1913":"print('max val ' ,np.max(trainClassifReducedMergedGRPDImageDF['image_array'][0]))\nprint('min val ' ,np.min(trainClassifReducedMergedGRPDImageDF['image_array'][0]))","a29d9fb5":"x_train_norm_classif = createFeatureTensor(trainClassifReducedMergedGRPDImageDF['image_array'],img_hight_classif,img_hight_classif,channels)","ffab6509":"trainClassifReducedMergedGRPDImageDF.shape","3838b64e":"y_train_norm_classif=trainClassifReducedMergedGRPDImageDF['Image class']\nfrom sklearn.preprocessing import LabelEncoder\nfrom keras.utils import to_categorical\nle=LabelEncoder()\ny_train_norm_classif_tensor=le.fit_transform(y_train_norm_classif)\ny_train_norm_classif_tensor=to_categorical(y_train_norm_classif_tensor,196)","cba5e948":"from tensorflow.keras.preprocessing.image import ImageDataGenerator","77c0029c":"from tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Conv2D, Dropout, MaxPool2D,AveragePooling2D,ZeroPadding2D\nfrom tensorflow.keras.layers import BatchNormalization\n\n# Initialize the model\nnnpmodel = Sequential()\n\n# convolutional layer\nnnpmodel.add(Conv2D(16, kernel_size=(3,3), activation='relu', strides=(1, 1), input_shape=(img_hight_classif,img_hight_classif,channels)))\nnnpmodel.add(Conv2D(16, kernel_size=(3,3), activation='relu', strides=(1, 1)))\nnnpmodel.add(BatchNormalization())\nnnpmodel.add(ZeroPadding2D(padding=(1, 1)))\n# convolutional layer\nnnpmodel.add(Conv2D(32, kernel_size=(3,3), activation='relu', padding='valid', kernel_regularizer=regularizers.l2(0.05)))\nnnpmodel.add(Conv2D(32, kernel_size=(3,3), activation='relu', padding='valid', kernel_regularizer=regularizers.l2(0.05)))\nnnpmodel.add(BatchNormalization())\n\n\nnnpmodel.add(Conv2D(64, kernel_size=(3,3), activation='relu',strides=(1, 1), padding='valid'))\nnnpmodel.add(Conv2D(64, kernel_size=(3,3), activation='relu',strides=(1, 1), padding='valid'))\nnnpmodel.add(BatchNormalization())\nnnpmodel.add(MaxPool2D(pool_size=(2, 2), padding='same'))\nnnpmodel.add(ZeroPadding2D(padding=(1, 1)))\n\nnnpmodel.add(Conv2D(128, kernel_size=(3,3), activation='relu', padding='valid', kernel_regularizer=regularizers.l2(0.01)))\nnnpmodel.add(Conv2D(128, kernel_size=(2,2), activation='relu', strides=(1, 1), padding='valid'))\nnnpmodel.add(BatchNormalization())\n\nnnpmodel.add(Conv2D(64, kernel_size=(3,3), activation='relu', padding='valid', kernel_regularizer=regularizers.l2(0.02)))\nnnpmodel.add(Conv2D(64, kernel_size=(3,3), activation='relu', padding='valid', kernel_regularizer=regularizers.l2(0.02)))\nnnpmodel.add(BatchNormalization())\n\nnnpmodel.add(Conv2D(64, kernel_size=(3,3), activation='relu', strides=(1, 1)))\nnnpmodel.add(Conv2D(64, kernel_size=(3,3), activation='relu', strides=(1, 1)))\nnnpmodel.add(BatchNormalization())\nnnpmodel.add(MaxPool2D(pool_size=(2, 2)))\n\nnnpmodel.add(Conv2D(64, kernel_size=(3,3), activation='relu', padding='valid', kernel_regularizer=regularizers.l2(0.02)))\nnnpmodel.add(Conv2D(64, kernel_size=(3,3), activation='relu', padding='valid', kernel_regularizer=regularizers.l2(0.02)))\nnnpmodel.add(BatchNormalization())\n\nnnpmodel.add(Conv2D(128, kernel_size=(3,3), activation='relu', padding='valid', kernel_regularizer=regularizers.l2(0.05)))\nnnpmodel.add(Conv2D(128, kernel_size=(2,2), activation='relu', strides=(1, 1), padding='valid'))\nnnpmodel.add(BatchNormalization())\n\nnnpmodel.add(Flatten())\nnnpmodel.add(Dense(32, activation='relu'))\nnnpmodel.add(Dropout(0.3))\n\nnnpmodel.add(Dense(32, activation='relu'))\nnnpmodel.add(Dropout(0.2))\n\n# output layer\n#Add Fully Connected Layer with 196 units and activation function as 'softmax'\nnnpmodel.add(Dense(196, activation='softmax',name='visualized_layer'))","da44ac54":"nnpmodel.summary()","cd825c04":"#plot_model(nnpmodel, to_file='.\/nnpmodel_plot.png', show_shapes=True, show_layer_names=True)","ff0ec214":"INIT_LR = 1e-1\nBS = 64\nEPOCHS = 20","b97aa493":"from keras.optimizers import SGD,Adam\nopt = SGD(lr=0.001, momentum=0.9)\n#opt=SGD(lr=INIT_LR, momentum=0.9, decay=INIT_LR \/ EPOCHS)\n#opt = Adam(lr=0.001)\n","61049ad9":"x_train_norm_classif.shape","d861dad9":"from keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\nCallbacks=[EarlyStopping(patience=2, restore_best_weights=True), \n           ReduceLROnPlateau(patience=2), \n           ModelCheckpoint(filepath='ImageDataGen_Size150_oneHOT_ClassWeights_Callbacks_SGD_L2.h5', save_best_only=True)]   \n","d57fe04e":"#nnpmodel.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy'])\nnnpmodel.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n#nnpmodel.compile(loss='categorical_crossentropy', metrics=['accuracy'], optimizer='adam')","77320fba":"x_train_classif.shape","3ee44b72":"y_train_norm_classif_tensor.shape","96207d49":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(x_train_norm_classif,y_train_norm_classif_tensor, test_size=0.33)","53cfdff1":"nnphistory=nnpmodel.fit( x=X_train,y=y_train, batch_size=BS, epochs=EPOCHS,shuffle=True,verbose=1,validation_data=(X_test,y_test),callbacks=Callbacks)","5182e1c4":"#for layer in nnpmodel.layers: print(\"layer config :: \",layer.get_config(), \"layer weight :: \",layer.get_weights())","56013a24":"plotModelPerformanceCurvs(nnphistory)","2a753bf5":"nnpmodel.evaluate(x_test_classif, y_test_tensor)","3798326b":"\n# importing libraries\nimport numpy as np\nimport cv2\nfrom matplotlib import pyplot as plt\nfrom skimage import filters","358028e6":"# Reading image from folder where it is stored\n#img = cv2.imread('..\/input\/stanford-car-dataset-by-classes-folder\/car_data\/car_data\/train\/AM General Hummer SUV 2000\/02050.jpg')\n#img=cv2.imread('..\/input\/stanford-car-dataset-by-classes-folder\/car_data\/car_data\/train\/AM General Hummer SUV 2000\/00522.jpg')\nimg = cv2.imread('..\/input\/stanford-car-dataset-by-classes-folder\/car_data\/car_data\/train\/Acura Integra Type R 2001\/00198.jpg')\n  \n# denoising of image saving it into dst image\n#dst = cv2.fastNlMeansDenoisingColored(img, None, 10, 10, 7, 15)\n#dst = remove_background(img, threshold=128.)\n","9f7ed5ea":"\nsobel = filters.sobel(img)","8c864789":"import matplotlib.pyplot as plt\n%matplotlib inline\nplt.rcParams['image.interpolation'] = 'nearest'\nplt.rcParams['image.cmap'] = 'gray'\nplt.rcParams['figure.dpi'] = 200","5161b80b":"plt.imshow(sobel)","539e9992":"#cv2.imwrite('.\/dst.jpg', sobel)","01f27b12":"blurred = filters.gaussian(sobel, sigma=2.0,multichannel=False)\nplt.imshow(blurred)","ef1be1af":"light_spots = np.array((img > 180).nonzero()).T","1a968d98":"light_spots.shape","7e188e66":"plt.plot(light_spots[:, 1], light_spots[:, 0], 'o')\nplt.imshow(img)\nplt.title('light spots in image')","e9766d59":"dark_spots = np.array((img < 2).nonzero()).T","5820a76b":"dark_spots.shape","fb5e9da2":"plt.plot(dark_spots[:, 1], dark_spots[:, 0], 'o')\nplt.imshow(img)\nplt.title('dark spots in image')","d6e29825":"from scipy import ndimage as ndi\nbool_mask = np.zeros(img.shape, dtype=np.bool)\nbool_mask[tuple(light_spots.T)] = True\nbool_mask[tuple(dark_spots.T)] = True\nseed_mask, num_seeds = ndi.label(bool_mask)\nnum_seeds","2a281b3e":"from skimage import segmentation\nws = segmentation.watershed(sobel, seed_mask)\nplt.imshow(ws)","6514c3e6":"background = max(set(ws.ravel()), key=lambda g: np.sum(ws == g))\nbackground","36987e28":"background_mask = (ws == background)","792ffd53":"background_mask","a675616e":"#plt.imshow(background_mask)","176daf0f":"cv2.imwrite('.\/dst.jpg', ws)","ec7f950f":"from IPython.display import Image, display\nimport matplotlib.pyplot as plt\nimport matplotlib.cm as cm","31d1a520":"from keras import applications\nfrom keras import utils\nmodel_builder = applications.xception.Xception\nimg_size = (150, 150)\npreprocess_input = applications.xception.preprocess_input\ndecode_predictions = applications.xception.decode_predictions\n\nlast_conv_layer_name = \"visualized_layer\"\n\n# The local path to our target image\n#img_path = \"..\/input\/stanford-car-dataset-by-classes-folder\/car_data\/car_data\/train\/Acura Integra Type R 2001\/00198.jpg\"\n\n#img_path =\"..\/input\/stanford-car-dataset-by-classes-folder\/car_data\/car_data\/train\/AM General Hummer SUV 2000\/00522.jpg\"\n#img_path=\"..\/input\/stanford-car-dataset-by-classes-folder\/car_data\/car_data\/train\/AM General Hummer SUV 2000\/02050.jpg\"\nimg_path=\".\/dst.jpg\"\ndisplay(Image(img_path))","aaeed0ef":"from keras import preprocessing\n\ndef get_img_array(img_path, size):\n    # `img` is a PIL image of size 299x299\n    img = preprocessing.image.load_img(img_path, target_size=size)\n    # `array` is a float32 Numpy array of shape (299, 299, 3)\n    array = preprocessing.image.img_to_array(img)\n    # We add a dimension to transform our array into a \"batch\"\n    # of size (1, 299, 299, 3)\n    array = np.expand_dims(array, axis=0)\n    return array","e71e2ad2":"import tensorflow as tf\ndef make_gradcam_heatmap(img_array, model, last_conv_layer_name, pred_index=None):\n    # First, we create a model that maps the input image to the activations\n    # of the last conv layer as well as the output predictions\n    grad_model = tf.keras.models.Model(\n        [model.inputs], [model.get_layer(last_conv_layer_name).output, model.output]\n    )\n\n    # Then, we compute the gradient of the top predicted class for our input image\n    # with respect to the activations of the last conv layer\n    with tf.GradientTape() as tape:\n        last_conv_layer_output, preds = grad_model(img_array)\n        if pred_index is None:\n            pred_index = tf.argmax(preds[0])\n        class_channel = preds[:, pred_index]\n\n    # This is the gradient of the output neuron (top predicted or chosen)\n    # with regard to the output feature map of the last conv layer\n    grads = tape.gradient(class_channel, last_conv_layer_output)\n\n    # This is a vector where each entry is the mean intensity of the gradient\n    # over a specific feature map channel\n    print(grads)\n    #pooled_grads = tf.reduce_mean(grads, axis=(0, 1, 2))\n    pooled_grads = tf.reduce_mean(grads,(0,1,2))\n    print(pooled_grads)\n\n    # We multiply each channel in the feature map array\n    # by \"how important this channel is\" with regard to the top predicted class\n    # then sum all the channels to obtain the heatmap class activation\n    last_conv_layer_output = last_conv_layer_output[0]\n    heatmap = last_conv_layer_output @ pooled_grads[..., tf.newaxis]\n    heatmap = tf.squeeze(heatmap)\n\n    # For visualization purpose, we will also normalize the heatmap between 0 & 1\n    heatmap = tf.maximum(heatmap, 0) \/ tf.math.reduce_max(heatmap)\n    return heatmap.numpy()","6c797429":"img_array=preprocess_input(get_img_array(img_path, size=img_size))","38cb3421":"# Print what the top predicted class is\npreds = nnpmodel.predict(img_array)\npreds\nlabels = (preds > 0.7).astype(np.int)\nlabels\n#print(\"Predicted:\", decode_predictions(preds, top=1)[0])","b8267747":"# Remove last layer's softmax\nnnpmodel.layers[-1].activation = None","2602526c":"heatmap=make_gradcam_heatmap(img_array, nnpmodel, 'batch_normalization_7', pred_index=None)\n#heatmap=make_gradcam_heatmap(img_array, nnmodel, 'conv2d_1', pred_index=None)\n","fc1c89da":"# Display heatmap\nplt.matshow(heatmap)\nplt.show()","bc6de65c":"def save_and_display_gradcam(img_path, heatmap, cam_path=\"cam.jpg\", alpha=0.4):\n    # Load the original image\n    img = preprocessing.image.load_img(img_path)\n    img = preprocessing.image.img_to_array(img)\n\n    # Rescale heatmap to a range 0-255\n    heatmap = np.uint8(255 * heatmap)\n\n    # Use jet colormap to colorize heatmap\n    jet = cm.get_cmap(\"jet\")\n\n    # Use RGB values of the colormap\n    jet_colors = jet(np.arange(256))[:, :3]\n    jet_heatmap = jet_colors[heatmap]\n\n    # Create an image with RGB colorized heatmap\n    jet_heatmap = preprocessing.image.array_to_img(jet_heatmap)\n    jet_heatmap = jet_heatmap.resize((img.shape[1], img.shape[0]))\n    jet_heatmap = preprocessing.image.img_to_array(jet_heatmap)\n\n    # Superimpose the heatmap on original image\n    superimposed_img = jet_heatmap * alpha + img\n    superimposed_img = preprocessing.image.array_to_img(superimposed_img)\n\n    # Save the superimposed image\n    superimposed_img.save(cam_path)\n\n    # Display Grad CAM\n    display(Image(cam_path))\n\n\nsave_and_display_gradcam(img_path, heatmap)","453e6496":"from keras.preprocessing.image import ImageDataGenerator\nfrom keras.models import Sequential\nfrom keras.layers import Conv2D, MaxPooling2D\nfrom keras.layers import Activation, Dropout, Flatten, Dense\nfrom keras import backend as K","a8772c0d":"# dimensions of our images.\nimg_width, img_height = 150, 150\n\ntrain_data_dir = '..\/input\/stanford-car-dataset-by-classes-folder\/car_data\/car_data\/train'\nvalidation_data_dir = '..\/input\/stanford-car-dataset-by-classes-folder\/car_data\/car_data\/test'\nnb_train_samples = 8144\nnb_validation_samples = 1000\nepochs = 50\nbatch_size = 16","db42654a":"if K.image_data_format() == 'channels_first':\n    input_shape = (3, img_width, img_height)\nelse:\n    input_shape = (img_width, img_height, 3)","2cc00327":"idgmodel = Sequential()\nidgmodel.add(Conv2D(32, (3, 3), input_shape=input_shape))\nidgmodel.add(Activation('relu'))\nidgmodel.add(MaxPooling2D(pool_size=(2, 2)))\n\nidgmodel.add(Conv2D(32, (3, 3)))\nidgmodel.add(Activation('relu'))\nidgmodel.add(MaxPooling2D(pool_size=(2, 2)))\n\nidgmodel.add(Conv2D(64, (3, 3)))\nidgmodel.add(Activation('relu'))\nidgmodel.add(MaxPooling2D(pool_size=(2, 2)))\n\nidgmodel.add(Flatten())\nidgmodel.add(Dense(64))\nidgmodel.add(Activation('relu'))\nidgmodel.add(Dropout(0.5))\nidgmodel.add(Dense(196))\nidgmodel.add(Activation('sigmoid'))\n\nidgmodel.compile(loss='categorical_crossentropy',\n              optimizer='adam',\n              metrics=['accuracy'])\n","f162f1d8":"# this is the augmentation configuration we will use for training\ntrain_datagen = ImageDataGenerator(\n    rescale=1. \/ 255,\n    shear_range=0.4,\n    zoom_range=0.2,\n    rotation_range=40,\n    width_shift_range=0.2,\n    height_shift_range=0.2,\n    horizontal_flip=True,\n    #zca_whitening=True,\n    channel_shift_range=1.0,\n    brightness_range=(0.34,0.68),\n    vertical_flip=True,\n    fill_mode='nearest'\n)","49d091ca":"# this is the augmentation configuration we will use for testing:\n# only rescaling\ntest_datagen = ImageDataGenerator(rescale=1. \/ 255)","fd02d777":"train_generator = train_datagen.flow_from_directory(\n    train_data_dir,\n    target_size=(img_width, img_height),\n    batch_size=batch_size,\n    class_mode='categorical')\n\nvalidation_generator = test_datagen.flow_from_directory(\n    validation_data_dir,\n    target_size=(img_width, img_height),\n    batch_size=batch_size,\n    class_mode='categorical')","c09c44b8":"imgdgHist=idgmodel.fit(\n    train_generator,\n    steps_per_epoch=nb_train_samples \/\/ batch_size,\n    epochs=epochs,\n    validation_data=validation_generator,\n    validation_steps=nb_validation_samples \/\/ batch_size)\n\n","ea6cb9e0":"idgmodel.save_weights('img_aug_model_first_try.h5')","8f3f7252":"idgmodel.evaluate(x_test_classif, y_test_tensor)","44934abf":"plotModelPerformanceCurvs(imgdgHist)","f8186894":"from tensorflow.keras.applications import EfficientNetB0\nenmodel = EfficientNetB0(weights='imagenet')","5511c0ae":"# IMG_SIZE is determined by EfficientNet model choice\nIMG_SIZE = 224","3acd27c6":"batch_size = 64","0375291c":"size = (IMG_SIZE, IMG_SIZE)","c841c2b6":"trainTransfImageDF=createImageDF('..\/input\/stanford-car-dataset-by-classes-folder\/car_data\/car_data\/train',(IMG_SIZE,IMG_SIZE,channels),\"grayscale\",6)","a07ea62f":"y_train=createBondingBoxTensor(trainReducedMergedImageDF,'x0_scaled','y0_scaled','x1_scaled','y1_scaled')\ny_test=createBondingBoxTensor(testReducedMergedImageDF,'x0_scaled','y0_scaled','x1_scaled','y1_scaled')","33b2a4a2":"y_train.shape","4c6c26c1":"y_test.shape","165ad4ff":"from tensorflow.keras.applications.mobilenet import MobileNet\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import Conv2D, Reshape\n\nALPHA = 1.0 # Width hyper parameter for MobileNet (0.25, 0.5, 0.75, 1.0). Higher width means more accurate but slower\n\ndef create_model(imgWidth,imgHeight,trainable=True):\n    model = MobileNet(input_shape=(imgWidth, imgHeight, 3), include_top=False, alpha=ALPHA) # Load pre-trained mobilenet\n    # Do not include classification (top) layer\n\n    # to freeze layers, except the new top layer, of course, which will be added below\n    for layer in model.layers:\n        layer.trainable = trainable\n\n    # Add new top layer which is a conv layer of the same size as the previous layer so that only 4 coords of BBox can be output\n    x0 = model.layers[-1].output\n    x1 = Conv2D(4, kernel_size=4, name=\"coords\")(x0)\n    # In the line above kernel size should be 3 for img size 96, 4 for img size 128, 5 for img size 160 etc.\n    x2 = Reshape((4,))(x1) # These are the 4 predicted coordinates of one BBox\n\n    return Model(inputs=model.input, outputs=x2)","52fc7b0c":"def IOU(y_true, y_pred):\n    intersections = 0\n    unions = 0\n    # set the types so we are sure what type we are using\n\n    gt = y_true\n    pred = y_pred\n    # Compute interection of predicted (pred) and ground truth (gt) bounding boxes\n    diff_width = np.minimum(gt[:,0] + gt[:,2], pred[:,0] + pred[:,2]) - np.maximum(gt[:,0], pred[:,0])\n    diff_height = np.minimum(gt[:,1] + gt[:,3], pred[:,1] + pred[:,3]) - np.maximum(gt[:,1], pred[:,1])\n    intersection = diff_width * diff_height\n\n    # Compute union\n    area_gt = gt[:,2] * gt[:,3]\n    area_pred = pred[:,2] * pred[:,3]\n    union = area_gt + area_pred - intersection\n\n    # Compute intersection and union over multiple boxes\n    for j, _ in enumerate(union):\n      if union[j] > 0 and intersection[j] > 0 and union[j] >= intersection[j]:\n        intersections += intersection[j]\n        unions += union[j]\n\n    # Compute IOU. Use epsilon to prevent division by zero\n    iou = np.round(intersections \/ (unions + tensorflow.keras.backend.epsilon()), 4)\n    # This must match the type used in py_func\n    iou = iou.astype(np.float32)\n    return iou","9a53ed0d":"def IoU(y_true, y_pred):\n    iou = tensorflow.py_function(IOU, [y_true, y_pred], Tout=tensorflow.float32)\n    return iou","2a37f423":"model = create_model(IMAGE_WIDTH,IMAGE_HEIGHT,False) # Arg is False, if you want to freeze lower layers for fast training (but low accuracy)\nmodel.summary() # Print summary","b0cc8c25":"plot_model(model, to_file='.\/mb_model_plot.png', show_shapes=True, show_layer_names=True)","08772391":"# Compile the model\nmodel.compile(loss=\"mean_squared_error\", optimizer=\"adam\", metrics=[IoU]) # Regression loss is MSE\n#model.compile(loss=\"mean_squared_error\", optimizer=\"adam\") # Regression loss is MSE","4f8f1143":"pip install tensorflow-gpu==2.0.0-alpha0","33853135":"import tensorflow\ntensorflow.__version__","15cf96f4":"x_train[0]","4696e262":"y_train[0]","b4ceb31b":"# Use earlystopping\ncallback = tensorflow.keras.callbacks.EarlyStopping(monitor='val_IoU', patience=5, min_delta=0.01)\n\n# Fit the model\nmodel.fit(x_train, y_train, validation_data=(x_test, y_test), epochs=10, batch_size=32,callbacks=[callback])\n#model.fit(x_train, y_train, validation_data=(x_test, y_test), epochs=10, batch_size=32)","cf7bf97b":"model.evaluate(x_test, y_test)","7d67a29c":"def drowPredictedBBOnImage(fileName,imgResizeTuple,imageArrayCol,colorScheme):\n    df=loadSingleScoringImage(filename,imgResizeTuple,colorScheme)\n    df[imageArrayCol]=df[imageArrayCol]\/255\n    predImageTensor=createFeatureTensor(df[imageArrayCol],IMAGE_WIDTH,IMAGE_HEIGHT,CHANNEL)\n    region = model.predict(x=predImageTensor)[0] # Predict the BBox\n    x0=region[0]\n    y0=region[1]\n    x1=region[2]\n    y1=region[3]\n    # Create figure and axes\n    fig,ax = plt.subplots(1)\n    # Display the image\n    ax.imshow(df[imageArrayCol][0])\n    # Create a Rectangle patch\n    rect = patches.Rectangle((x0,y0),x1-x0,y1-y0, linewidth=2, edgecolor='b', facecolor='none')\n    # Add the patch to the Axes\n    ax.add_patch(rect)\n    plt.show()","0068e151":"filename = '..\/input\/customcarimages\/bmw.jpg'\ndrowPredictedBBOnImage(filename,(IMAGE_WIDTH,IMAGE_HEIGHT,CHANNEL),'image_array','rgb')","c0f55bb6":"### Creating lables for classification","67b0b283":"### Loading bigger data set for classification","af9e6cc6":"#### The Grad-CAM algorithm","704477c5":"### Creating multiclass classifier CNN model","e1ff5404":"### Training the model\nFit the model to the dataset<br\/>\n\n* Use early stopping\n* fit the model\n* give train data - training features and labels\n    ** batch size: 32\n    ** epochs: 10\n    ** give validation data - testing features and labels","16eee669":"### Let's check min,max values for tensors present in training data","dbb82f02":"### Define evaluation metric","f825012f":"**Step 1: Import the data**","f191eff3":"### From above histogram it is clear that we have almost equal density distribution among car classes in training data df","1ad6aba9":"#### Creating reduced image size test data","bd88bdad":"### PROBLEM STATEMENT\n**\u2022 DOMAIN:** Automotive. Surveillance.<br\/>\n**\u2022 CONTEXT:**<br\/>\nComputer vision can be used to automate supervision and generate action appropriate action trigger if the event is\npredicted from the image of interest. For example a car moving on the road can be easily identified by a camera as make of\nthe car, type, colour, number plates etc.<br\/>\n**\u2022 DATA DESCRIPTION:**<br\/>\nThe Cars dataset contains 16,185 images of 196 classes of cars. The data is split into 8,144 training images and 8,041 testing\nimages, where each class has been split roughly in a 50-50 split. Classes are typically at the level of Make, Model, Year, e.g.\n2012 Tesla Model S or 2012 BMW M3 coupe.<br\/>\n**Data description:**\n\u2023 Train Images: Consists of real images of cars as per the make and year of the car.<br\/>\n\u2023 Test Images: Consists of real images of cars as per the make and year of the car.<br\/>\n\u2023 Train Annotation: Consists of bounding box region for training images.<br\/>\n\u2023 Test Annotation: Consists of bounding box region for testing images.<br\/>","59bde003":"### Let's take top 25 rows per car class","3ec5202b":"**Function to create bounding box on image**","635c717a":"#### Checking distribution of classes in traing data set","09b51c41":"### Let's create X_train,y_train,x_test,y_test variables","e98b2e19":"### Step 2: Map training and testing images to its classes.","08444bee":"### Test the model on an image from test data","2270bd93":"#### Function to create scalled bounding boxes","05739f8b":"### Final loss and accuracy","a8fee5e1":"### To denoising image","5687358c":"### Compile the model\n* loss: \"mean_squared_error\"\n* metrics: IoU\n* optimizer: \"adam\"","8d478c6a":"#### Milestone 1:","2be4f0b5":"### Let's use tranfer learning: EfficientNet","b24d4db9":"###  Initialize the model and print summary","9b22f811":"**For test data**","b79fc7cd":"### Noramlizing image features as they are raingeing from 0 to 255","a2a8b4b0":"### Loading train and test annotation csv files","34134d35":"### Create the model for bounding box predictions","c869f080":"### Data imbalance has been settled","123ac9a9":"### Let's use keras image data generator for classification","f11d5514":"### Let's check distribution of each class in our training data","c3b5eb1c":"### Noramlizing image features as they are raingeing from 0 to 255","7fe68484":"### Here we can see there is imbalance in data as minimum number of image in a class is arround 25, let's take 25 images per class","43dbdf03":"### Let's reduce size of training data set images","28106c71":"### values are normalized between 0 to 1  ","9ec77784":"#### Function to create data frame with Image Name,Image np array,Car Make","85f83a93":"#### Final loss and accuracy","06d7be7b":"### creating bounding box labels","1d3ff3a3":"### Let's normalize the values","7aae7592":"### Final loss and accuracy"}}