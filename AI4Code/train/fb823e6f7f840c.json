{"cell_type":{"ebed5d98":"code","de590f20":"code","79b74613":"code","308252f4":"code","80293153":"code","855ab2ba":"code","7d10b96b":"code","dec9c49b":"code","f2574970":"code","bc85e369":"code","babe6292":"code","27a36bf5":"code","2791a29c":"code","40ae0d94":"code","b76c29f4":"code","f0f1ac44":"code","d39042cb":"code","dfe8ad38":"code","9c57f171":"code","b31c8b3c":"code","87704025":"code","b53c24cf":"code","7bbf13f5":"code","2956f521":"code","7ff0ee03":"code","04b02c66":"code","fa9d955f":"code","907fca48":"code","72c501a6":"code","82e0ba7f":"code","8e43ea13":"code","b143e9a3":"code","4472bdbd":"code","69be86bf":"code","effc69a9":"code","097c8482":"code","c0b3faf4":"code","5903d78f":"code","daa72b0e":"code","fdd09fd8":"code","835fc102":"code","4d9cc549":"code","6b77fb70":"code","7b40e312":"markdown","92fbee66":"markdown","d0430eeb":"markdown","4ebee849":"markdown","2a7fe791":"markdown","9ec1b348":"markdown","5800289b":"markdown","854c1aa6":"markdown","5bf75407":"markdown","bb71d65a":"markdown","6ed68433":"markdown","3d253f2e":"markdown","96a7db0f":"markdown","56b9b917":"markdown","ee5ff97e":"markdown","1c2f5d4e":"markdown","ce4bbebc":"markdown","604879b2":"markdown","e0cb9d36":"markdown","b171e638":"markdown"},"source":{"ebed5d98":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n\n##visual imports\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n##Missing data\nfrom sklearn.impute import SimpleImputer\n\n##Categorical Encoding\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.preprocessing import LabelEncoder\n\n##Feature Scaling\nfrom sklearn.preprocessing import StandardScaler\n\n##Splitting data\nfrom sklearn.model_selection import train_test_split\n\n#Splitting Data\nfrom sklearn.model_selection import train_test_split\n\n# Feature Scaling\nfrom sklearn.preprocessing import StandardScaler\n\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","de590f20":"diamond_data = pd.read_csv(\"\/kaggle\/input\/diamonds\/diamonds.csv\")","79b74613":"diamond_data.head()","308252f4":"diamond_data.info()","80293153":"diamond_data.describe()","855ab2ba":"print(\"Cut Breakdown\\n\")\nprint(diamond_data[\"cut\"].value_counts())\nprint(\"_\"*20)\nprint(\"Color Breakdown\\n\")\nprint(diamond_data[\"color\"].value_counts())\nprint(\"_\"*20)\nprint(\"Clarity Breakdown\\n\")\nprint(diamond_data[\"clarity\"].value_counts())","7d10b96b":"diamond_data.drop([\"Unnamed: 0\"], axis = 1, inplace = True)","dec9c49b":"sns.pairplot(diamond_data)","f2574970":"plt.figure(figsize=(12, 7))\ncorrel = diamond_data.corr()\nsns.heatmap(correl, annot = True)","bc85e369":"print(\"0 value x: {}\".format(diamond_data['x'].isin([0]).sum()))\nprint(\"0 value y: {}\".format(diamond_data['y'].isin([0]).sum()))\nprint(\"0 value z: {}\".format(diamond_data['z'].isin([0]).sum()))","babe6292":"diamond_data[[\"x\",\"y\",\"z\"]] = diamond_data[[\"x\",\"y\",\"z\"]].replace(0,np.NaN)\ndiamond_data.isnull().sum()","27a36bf5":"diamond_data.dropna(inplace=True)\ndiamond_data.shape","2791a29c":"diamond_data.describe()","40ae0d94":"plt.title('X Distribution Plot')\nsns.distplot(diamond_data[\"x\"], bins = 50)","b76c29f4":"plt.title('Y Distribution Plot')\nsns.distplot(diamond_data[\"y\"], bins = 50)","f0f1ac44":"plt.title('Z Distribution Plot')\nsns.distplot(diamond_data[\"z\"], bins = 50)","d39042cb":"x_rep =diamond_data['x'] < 9.5\ny_rep =diamond_data['y'] < 20\nz_rep =diamond_data['z'] < 20\ndiamond_data['x'].where(x_rep,np.NaN, inplace = True)\ndiamond_data['y'].where(y_rep,np.NaN, inplace = True)\ndiamond_data['z'].where(z_rep,np.NaN, inplace = True)\ndiamond_data.isnull().sum()","dfe8ad38":"diamond_data.dropna(inplace=True)\ndiamond_data.shape","9c57f171":"sns.pairplot(diamond_data)","b31c8b3c":"diamond_data['vol'] = diamond_data['x']*diamond_data['y']*diamond_data['z']\ndiamond_data.head()","87704025":"diamond_data.drop(['x','y','z'],axis =1, inplace=True)","b53c24cf":"sns.pairplot(diamond_data)","7bbf13f5":"print('The mean volume in the set is: {:.2f}'.format(diamond_data['vol'].mean()))\nprint('The maximum volume in the set is: {:.2f}'.format(diamond_data['vol'].max()))","2956f521":"plt.figure(figsize=(12, 7))\ncorrel = diamond_data.corr()\nsns.heatmap(correl, annot = True)","7ff0ee03":"from scipy.stats import kurtosis\nfrom scipy.stats import skew\nprint('excess kurtosis of normal distribution (should be 0): {}'.format(skew(diamond_data['price'])))\nprint('skewness of normal distribution (should be 0): {}'.format(kurtosis(diamond_data['price'])))","04b02c66":"sns.boxplot(x = \"price\", y = \"cut\", data = diamond_data)","fa9d955f":"sns.boxplot(x = \"price\", y = \"color\", data = diamond_data)","907fca48":"sns.boxplot(x = \"price\", y = \"clarity\", data = diamond_data)","72c501a6":"sns.jointplot(x = \"price\", y = \"carat\", data = diamond_data)","82e0ba7f":"plt.figure(figsize=(12, 7))\nsns.distplot(diamond_data[\"price\"], bins = 50)","8e43ea13":"diamond_data[\"price\"] = diamond_data[\"price\"].apply(np.log)\ndiamond_data[\"carat\"] = diamond_data[\"carat\"].apply(np.log)\ndiamond_data[\"vol\"] = diamond_data[\"vol\"].apply(np.log)","b143e9a3":"plt.figure(figsize=(12, 7))\nplt.title('Price Distribution')\nsns.distplot(diamond_data[\"price\"])\nprint('excess kurtosis of normal distribution (should be 0): {}'.format(skew(diamond_data['price'])))\nprint('skewness of normal distribution (should be 0): {}'.format(kurtosis(diamond_data['price'])))","4472bdbd":"plt.figure(figsize=(12, 7))\nplt.title('Volume Distribution')\nsns.distplot(diamond_data[\"vol\"])\nprint('excess kurtosis of normal distribution (should be 0): {}'.format(skew(diamond_data['vol'])))\nprint('skewness of normal distribution (should be 0): {}'.format(kurtosis(diamond_data['vol'])))","69be86bf":"plt.figure(figsize=(12, 7))\nplt.title('Carat Distribution')\nsns.distplot(diamond_data[\"carat\"])\nprint('excess kurtosis of normal distribution (should be 0): {}'.format(skew(diamond_data['carat'])))\nprint('skewness of normal distribution (should be 0): {}'.format(kurtosis(diamond_data['carat'])))","effc69a9":"sns.pairplot(diamond_data)","097c8482":"cut_mapping = {\"Fair\": 1, \"Good\": 2, \"Very Good\": 3, \"Premium\": 4, \"Ideal\": 5}\ncolor_mapping = {\"J\": 1, \"I\": 2, \"H\": 3, \"G\": 4, \"F\": 5, \"E\":6, \"D\":7}\nclarity_mapping = {\"I1\": 1, \"SI2\": 2, \"SI1\": 3, \"VS2\": 4, \"VS1\": 5, \"VVS2\":6,\"VVS1\":7,\"IF\":8}\n\ndiamond_data['cut'] = diamond_data['cut'].map(cut_mapping)\ndiamond_data['color'] = diamond_data['color'].map(color_mapping)\ndiamond_data['clarity'] = diamond_data['clarity'].map(clarity_mapping)\n\ndiamond_data.head()","c0b3faf4":"# Simple & Multi Linear Regression\nfrom sklearn.linear_model import LinearRegression, Lasso, Ridge, SGDRegressor\n\n# Polynomial Regression \nfrom sklearn.preprocessing import PolynomialFeatures\n\n#Support Vector Regression\nfrom sklearn.svm import SVR\n\n#CART Regression\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import RandomForestRegressor\n\n#K-Nearest Neighbours\nfrom sklearn.neighbors import KNeighborsRegressor\n\n#XG Boost\nfrom xgboost import XGBRegressor\n","5903d78f":"X_train, X_test, y_train, y_test = train_test_split(diamond_data.drop('price',axis=1), \n                                                    diamond_data['price'], test_size=0.25, \n                                                    random_state=101)","daa72b0e":"from sklearn.metrics import r2_score\n\nLR = LinearRegression()\nLR.fit(X_train,y_train)\ny_pred = LR.predict(X_test)\n\nR2 = r2_score(y_test, y_pred)\n\nn=diamond_data.shape[0]\np=diamond_data.shape[1] - 1\n\nadj_rsquared = 1 - (1 - R2) * ((n - 1)\/(n-p-1))\n\n\nfrom sklearn import metrics\n\nprint('MAE:', metrics.mean_absolute_error(y_test, y_pred))\nprint('MSE:', metrics.mean_squared_error(y_test, y_pred))\nprint('RMSE:', np.sqrt(metrics.mean_squared_error(y_test, y_pred)))\nprint(\"r2: \", r2_score(y_test, y_pred))\nprint(\"Adjusted r2:\", adj_rsquared )\n\n","fdd09fd8":"from sklearn.model_selection import cross_val_score\n\naccuracies = cross_val_score(estimator = LR, X = X_train, y = y_train, cv = 10)\nR2accuracies = cross_val_score(estimator = LR, X = X_train, y = y_train, cv = 10, scoring = 'r2')\nMSEaccuracies = cross_val_score(estimator = LR, X = X_train, y = y_train, cv = 10, scoring = 'neg_mean_squared_error')\n\nn=diamond_data.shape[0]\np=diamond_data.shape[1] - 1\n\nMSE = MSEaccuracies.mean()*-1\nR2 = R2accuracies.mean()*100\nadj_rsquared = 1 - (1 - R2) * ((n - 1)\/(n-p-1))\n\nprint(\"MSE: {:.2f}\".format(MSE))\nprint(\"RMSE: {:.2f}\".format((MSE**0.5)))\nprint(\"R2: {:.2f}\".format((R2)))\nprint(\"Adjusted R2: {:.2f}\".format(adj_rsquared))","835fc102":"from sklearn.model_selection import ShuffleSplit\ncv_split = ShuffleSplit(n_splits = 10, test_size = .3, train_size = .6, random_state = 0 )","4d9cc549":"MLA = [LinearRegression(), DecisionTreeRegressor(),KNeighborsRegressor(), XGBRegressor(), RandomForestRegressor()]\nMLA_columns = [\"MLA Name\",\"Mean Price\",\"MAE\",\"MSE\", \"RMSE\", \"R2\",\"Adjusted R2\"]\nMLA_compare = pd.DataFrame(columns = MLA_columns)\nn=diamond_data.shape[0]\np=diamond_data.shape[1] - 1\n\nrow_index = 0\nfor alg in MLA:\n    MLA_name = alg.__class__.__name__\n    MLA_compare.loc[row_index, 'MLA Name'] = MLA_name\n    \n    alg.fit(X_train, y_train)\n    pred = alg.predict(X_test)\n    \n    \n    R2accuracies = cross_val_score(estimator = alg, X = X_train, y = y_train, cv = cv_split)\n    MSEaccuracies = cross_val_score(estimator = alg, X = X_train, y = y_train, cv = cv_split, scoring = 'neg_mean_squared_error')\n    MAEaccuracies = cross_val_score(estimator = alg, X = X_train, y = y_train, cv = cv_split, scoring = 'neg_mean_absolute_error')\n    MSE = MSEaccuracies.mean()*-1\n    R2 = R2accuracies.mean()*100\n    MAE = MAEaccuracies.mean()*-1\n    adj_rsquared = 1 - (1 - R2) * ((n - 1)\/(n-p-1))\n    \n    MLA_compare.loc[row_index, \"Mean Price\"] = pred.mean()\n    MLA_compare.loc[row_index, \"MAE\"] = MAE\n    MLA_compare.loc[row_index, \"MSE\"] = int(MSE)\n    MLA_compare.loc[row_index, \"RMSE\"] = MSE**0.5\n    MLA_compare.loc[row_index, \"R2\"] = R2\n    MLA_compare.loc[row_index, \"Adjusted R2\"] = adj_rsquared\n\n    row_index +=1\n                                                       \n\nMLA_compare.sort_values(by = [\"R2\"], ascending = False, inplace = True)\nMLA_compare","6b77fb70":"plt.title(\"MLA Accuracy Rank\")\nsns.barplot(x = \"R2\", y = \"MLA Name\", data = MLA_compare)","7b40e312":"# Building the Models\n\nNow that we have converted all the data into numeric values and also removed any outliers we can start to build the models","92fbee66":"# Dealing with Outliers\n\nHere we will look at the outliers within the dimension data to see if anything can be removed. After reviewing the below distribution plots and the below data we can see that there are a few outliers within each dimension:\n\n1. X - Mean = 5.73, STD = 1.12, Min = 3.73, Max = 10.74\n1. Y - Mean = 5.53, STD = 1.14, Min = 3.68, Max = 58.90\n1. Z - Mean = 3.53, STD = 0.07, Min = 1.07, Max = 31.80","d0430eeb":"# Findings\n\n\nWhen comparing the VVS2, VVS1, IF (top level clarity) and ideal, premium and very good (top level cut), there was very little deviation from the main distribution of price \n\nThis can also be inferred by observing the box plot for each ordinal variable. It can be seen that there is not a high variance of the mean in each variable, with all the means being affected by large outliers","4ebee849":"# Diamond are forever!\n\nHello and welcome to my kaggle workbook!\n\nIn this sheet I will look to analyse the data of diamonds in America and then produce a model that can predict the price of a diamond. \n\nTo start I have detailed the features below\n\n**Feature details**\n\n* Price: price in US dollars (within range of 326 - 18,823)\n\n* Carat: weight of the diamond (0.2-5.01)\n\n* Cut: quality of the cut (in ascending order from Fair, Good, Very Good, Premium, Ideal)\n\n* Color: diamond colour, from J (worst) to D (best)\n\n* Clarity: a measurement of how clear the diamond is (I1 (worst), SI2, SI1, VS2, VS1, VVS2, VVS1, IF (best))\n\n* X: length in mm (0--10.74)\n\n* Y: width in mm (0--58.9)\n\n* Z: depth in mm (0--31.8)\n\n* Depth: total depth percentage = z \/ mean(x, y) = 2 * z \/ (x + y) (43--79)\n\n* Table: width of top of diamond relative to widest point (43--95)","2a7fe791":"Nulls have been removed, resulting in a drop of 20 entries from the dataset.","9ec1b348":"# Dimension Cleaning\n\nHere, we can begin to change the x,y and z data to make the categories useable in our models. \n\nTo start off with, we can look to drop the rows that contain x, y and z data that has 0 values in, followed by removing large outliers.\n\nFollowing this we can build a new feature, volume, which will be a combination of mentioned features. \n\n","5800289b":"# Findings \n\nFrom this we can see that the XGB and RandomForestRegressor yield the highest R2 value, which can infer that they are the most accurate. \n\nIf you liked this sheet then please leave a comment. \n\nI am new to the world of data science, with all my knowledge being self taught so I might have missunderstood some concepts or incorrectly applied some of logic. If you notice anything that is incorrect or something that looks a little wrong then please leave a comment and let me know. \n\nThanks!\n","854c1aa6":"# Correlation Findings\n\n+ The data is clearly affected by outlier within all categories. These can be removed to giver a clearer picture of the correlation\n\n* x,y and z are heavily correlated, this is no surprise as the dimensions of the diamond should be correlated. \n  These can be consolidated into a single variable, volume, to avoid multicollinearity \n\n+ Price is not normally distributed and does not hold a linear relationship with all variables, this needs to be log transformed. \n\n+ Carat is highly correlated to the price of the diamond\n\n+ depth and table seem to bare little relationship to other features, these could both be dropped","5bf75407":"# Exploratory Data Analysis\n\nNow that I taken a quick overview of what is in the data I will look at graphs related to the data to see what trends can be established\n","bb71d65a":"# DATA EXPLORATORION","6ed68433":"# Analysis of Ordinal Data\n\n**Cut Breakdown**\n* Skew towards the higher quality cut of diamond, not evenly distributed within the category\n\n**Color Breakdown**\n* More spread around the mid range the Color category, higher and lower quality seeming to be outliers compared to rest of data\n\n**Clarity Breakdown**\n* Top 4 are split relatively evenly between the higher and lower quality clarity. The highest and lowest are outliers within the data. Looks relatively evenly distributed between the clarity categories","3d253f2e":"# Non Normality and Homoscedasticity\n\nAs we can see from the below, we can see that the price is skewed and is not normallz distributed. When compared with some other features we can also see that the data is not fully homoscedastic. To fix this we will look to log transform the price feature. ","96a7db0f":"15 rows removed and large outliers removed. Following the removal of the outliers we can see a clearer correlation between the dimension in the pairplot above. Now it is time to combine the features!!","56b9b917":"# Quick Look\n\nNo null values, however there seem to be 0 values within the dimension categories that will need to be looked over. [](http:\/\/)\n\n\"Unnamed\" represents the index for the sheet, this can be dropped\n\nCut, color and clarity are all ordinal features, as per the detail above. These can be converted into numerical values so that they can be used in the model \n","ee5ff97e":"# Cross Validation\n\nUsing a cross validation model to run the model 10 times yields a similar result to the initial model that was run. \n\n","1c2f5d4e":"# Converting Ordinal Features!\n\nHere we can conver the features for clarity, cut and color into numerical features so they can be used to train our models\n*     Cut: quality of the cut (Fair, Good, Very Good, Premium, Ideal)   \n*     Color: diamond colour, from J, I, H, G ,F , E, D (best)\n*     Clarity: a measurement of how clear the diamond is (I1 (worst), SI2, SI1, VS2, VS1, VVS2, VVS1, IF (best))","ce4bbebc":"# NO NULL VALUES!","604879b2":"# Linear Regression\n\nUsing a simple Linear Regression model we get an R2 value of around 98 with an adjusted R2 of around 98. Which is not bad!","e0cb9d36":"Whilst these outliers may be correct data points they don't align with the full dataset, so including they could effect our end results. \n\nThe x, y and z are not normally distribution, however it is clear from above the the features centre around the mean with a small amount of larger outliers. \n\nTo remove the outliers I will exclude values over 25 in Y and Z and 9.5 over X","b171e638":"# Findings\n\nX, Y and Z all contain 0 values, a 0 value suggests that the diamond is dimensionless - which cannot be correct. \n\nThis will require some later investigation, if there are few 0 values in the data set then they can be dropped.  \n\nLarge positive outliers in both the carat and price sections, when compared with the mean. "}}