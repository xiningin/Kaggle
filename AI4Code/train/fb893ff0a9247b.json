{"cell_type":{"380d8439":"code","3b37bc3d":"code","eb7a06f2":"code","ec88fb13":"code","903c98cf":"code","d3de8f7f":"code","8457f5fb":"code","5d2a178c":"code","56edd1b6":"code","66b65330":"code","e59c9140":"code","43f00ef4":"code","c1a735df":"code","07fa84f3":"code","855e14cc":"code","b45230eb":"code","1306fba0":"code","68c0363d":"code","a48f2557":"code","b76a116b":"code","46ecec8d":"code","35c86eab":"code","11f16874":"code","343eb4a9":"code","7b0c5c19":"code","93cd79b9":"code","4a6110f8":"code","30189ef8":"code","6cbaa541":"code","4647569d":"code","0acb54e8":"code","e388f443":"code","f7376e34":"code","8309638d":"code","5f8d3628":"code","bd2f4f7a":"code","adab8096":"code","d372acb5":"code","cec2b911":"code","d084696b":"code","1ba00d69":"code","a8d00420":"code","973a8066":"code","bb7bddfb":"code","4a041096":"code","f452f52f":"code","6016cddd":"code","175b5fe6":"code","f5139523":"code","d2390a8e":"code","66cb6552":"code","57405c18":"code","1ac1b340":"code","88bf4afd":"code","65439422":"code","36319948":"code","d5673cee":"code","ba866fea":"code","b6c2b362":"code","3f6ed835":"code","00294638":"code","39e7d480":"code","05f9c192":"code","2599862b":"code","444dcca4":"code","d57d499c":"code","762f352b":"code","3bcee76a":"code","c1634d33":"code","360c7664":"code","20c82806":"code","462b6a63":"markdown","5c66489c":"markdown","6fe71516":"markdown","03e1c49d":"markdown","e6017f09":"markdown","9743fedf":"markdown","770bb826":"markdown","1bb46eec":"markdown","e197941b":"markdown","c45c93c2":"markdown","bca14d60":"markdown","89b021cb":"markdown"},"source":{"380d8439":"# Import packages\n# Basic packages\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Evaluation and bayesian optimization\nfrom math import floor\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.metrics import make_scorer, accuracy_score, classification_report, confusion_matrix\nfrom sklearn.model_selection import GridSearchCV, StratifiedKFold\nfrom hyperopt import hp, fmin, tpe\nfrom bayes_opt import BayesianOptimization\n\nimport warnings\nwarnings.filterwarnings('ignore')\npd.set_option(\"display.max_columns\", None)","3b37bc3d":"# Make scorer: accuracy and f1\nacc_score = make_scorer(accuracy_score)","eb7a06f2":"# Load dataset\ntrainSet = pd.read_csv('..\/input\/tabular-playground-series-apr-2021\/train.csv')\ntestSet = pd.read_csv('..\/input\/tabular-playground-series-apr-2021\/test.csv')\nsubmitSet = pd.read_csv('..\/input\/tabular-playground-series-apr-2021\/sample_submission.csv')\n\ntrainSet.head()","ec88fb13":"# Remove not used variables\ntrain = trainSet.drop(columns=['Name', 'Ticket'])\ntrain['Cabin_letter'] = train['Cabin'].str[0:1]\ntrain['Cabin_no'] = train['Cabin'].str[1:]\n\n# Feature generation: training data\ntrain = trainSet.drop(columns=['Name', 'Ticket', 'Cabin'])\ntrain = train.dropna(axis=0)\ntrain = pd.get_dummies(train)\n\n# Feature generation: test data\ntest = testSet.drop(columns=['Name', 'Ticket', 'Cabin'])\ntest = test.dropna(axis=0)\ntest = pd.get_dummies(test)\n\ntrain.head()","903c98cf":"train.info()","d3de8f7f":"# train validation split\nX_train, X_val, y_train, y_val = train_test_split(train.drop(columns=['PassengerId','Survived'], axis=0),\n                                                  train['Survived'],\n                                                  test_size=0.2, random_state=111,\n                                                  stratify=train['Survived'])","8457f5fb":"# Scaling\nscaler = MinMaxScaler()\nX_trainS = pd.DataFrame(scaler.fit_transform(X_train), columns=X_train.columns)\n\nX_valS = pd.DataFrame(scaler.transform(X_val), columns=X_val.columns)\n\ntestS = pd.DataFrame(scaler.transform(test.drop(columns=['PassengerId'])),\n                     columns=test.drop(columns=['PassengerId']).columns)","5d2a178c":"from sklearn.linear_model import LogisticRegression","56edd1b6":"# Hyperparameter-tuning: Bayesian Optimization, hyperpot\nspace_log = {'penalty': hp.choice('penalty', ['l2', 'none']),\n             'C': hp.loguniform('C', np.log(0.01), np.log(1000)),\n             'fit_intercept': hp.choice('fit_intercept',[True, False]),\n             'solver':hp.choice('solver', ['newton-cg', 'lbfgs', 'sag', 'saga'])}\n\ndef log_cl_bo(params_log):\n    params_log = {'penalty': params_log['penalty'],\n                  'C': params_log['C'],\n                  'fit_intercept': params_log['fit_intercept'],\n                  'solver': params_log['solver']}\n    \n    log_bo = LogisticRegression(random_state=123, **params_log)\n    best_score = cross_val_score(log_bo, X_trainS, y_train, scoring=acc_score, cv=5).mean()\n    return 1 - best_score\n\nlog_best_param = fmin(fn=log_cl_bo,\n                space=space_log,\n                max_evals=20,\n                rstate=np.random.RandomState(42),\n                algo=tpe.suggest)","66b65330":"# Best hyperparameters\nparams_log = log_best_param\npenaltyL = ['l2', 'none']\nfit_interceptL = [True, False]\nsolverL = ['newton-cg', 'lbfgs', 'sag', 'saga']\n\nparams_log['fit_intercept'] = fit_interceptL[round(params_log['fit_intercept'])]\nparams_log['penalty'] = penaltyL[round(params_log['penalty'])]\nparams_log['solver'] = solverL[round(params_log['solver'])]\nparams_log","e59c9140":"# Fit the training data\nlog_hyp =  LogisticRegression(**params_log, random_state=123)\nlog_hyp.fit(X_trainS, y_train)\n\n# Predict the validation data\npred_log = log_hyp.predict(X_valS)\n\n# Compute the accuracy\nprint('Accuracy: ' + str(accuracy_score(y_val, pred_log)))","43f00ef4":"# Prediction Result\nprint('confusion_matrix')\nprint(pd.DataFrame(confusion_matrix(y_val, pred_log)))\nprint(classification_report(y_val, pred_log))","c1a735df":"# Features Coefficients\nFeature_log = pd.DataFrame({'features': list(X_trainS.columns), 'coefficient':list(log_hyp.coef_[0])}).sort_values('coefficient')\nplt.figure(figsize=(16,4))\nsns.barplot(data=Feature_log, x='features', y='coefficient')\nplt.xticks(rotation=90)\nplt.show()","07fa84f3":"from sklearn.naive_bayes import GaussianNB","855e14cc":"# Hyperparameter-tuning: Grid Search\nvar_smoothing = [1e-11, 1e-10, 1e-9, 1e-8, 1e-7]\nparam_nb={'var_smoothing':var_smoothing}\nnb_grid = GridSearchCV(estimator=GaussianNB(), param_grid=param_nb, scoring=acc_score, cv=5)\n\nnb_grid.fit(X_train, y_train)\n\nprint('Best score: ' + str(nb_grid.best_score_))\nprint('Best parameter {}'.format(nb_grid.best_params_))","b45230eb":"# Fit the training data\nnb_hyp = GaussianNB(var_smoothing=nb_grid.best_params_['var_smoothing'])\nnb_hyp.fit(X_train, y_train)\n\n# Predict the validation data\npred_nb = nb_hyp.predict(X_val)\n\n# Compute the accuracy\nprint('Accuracy: ' + str(accuracy_score(y_val, pred_nb)))","1306fba0":"# Prediction Result\nprint('confusion_matrix')\nprint(pd.DataFrame(confusion_matrix(y_val, pred_nb)))\nprint(classification_report(y_val, pred_nb))","68c0363d":"from sklearn.naive_bayes import BernoulliNB","a48f2557":"# Hyperparameter-tuning: Grid Search\nparam_nbBer={'alpha':[0.2,0.4,0.6,0.8,1],\n             'fit_prior':[True, False]}\n\nnbBer_grid = GridSearchCV(estimator=BernoulliNB(), param_grid=param_nbBer, scoring=acc_score, cv=5)\n\nnbBer_grid.fit(X_train, y_train)\n\nprint('Best score: ' + str(nbBer_grid.best_score_))\nprint('Best parameter {}'.format(nbBer_grid.best_params_))","b76a116b":"# Fit the training data\nnbBer_hyp = BernoulliNB(alpha=nbBer_grid.best_params_['alpha'], fit_prior=nbBer_grid.best_params_['fit_prior'])\nnbBer_hyp.fit(X_train, y_train)\n\n# Predict the validation data\npred_nbBer = nbBer_hyp.predict(X_val)\n\n# Compute the accuracy\nprint('Accuracy: ' + str(accuracy_score(y_val, pred_nb)))","46ecec8d":"# Prediction Result\nprint('confusion_matrix')\nprint(pd.DataFrame(confusion_matrix(y_val, pred_nbBer)))\nprint(classification_report(y_val, pred_nbBer))","35c86eab":"from sklearn.neighbors import KNeighborsClassifier","11f16874":"# Hyperparameter-tuning: Bayesian Optimization, bayes_opt\ndef knn_cl_bo(n_neighbors, weights, p):\n    params_knn = {}\n    weightsL = ['uniform', 'distance']\n    \n    params_knn['n_neighbors'] = round(n_neighbors)\n    params_knn['weights'] = weightsL[round(weights)]\n    params_knn['p'] = round(p)\n    \n    score = cross_val_score(KNeighborsClassifier(**params_knn),\n                             X_trainS, y_train, cv=5, scoring=acc_score).mean()\n    return score\n\n# Set hyperparameters spaces\nparams_knn ={\n    'n_neighbors':(3, 20),\n    'weights':(0, 1),\n    'p':(1, 2)}\n\n# Run Bayesian Optimization\nknn_bo = BayesianOptimization(knn_cl_bo, params_knn, random_state=111)\nknn_bo.maximize(init_points=4, n_iter=25)","343eb4a9":"# Best hyperparameters\nparams_knn = knn_bo.max['params']\nweightsL = ['uniform', 'distance']\nparams_knn['n_neighbors'] = round(params_knn['n_neighbors'])\nparams_knn['weights'] = weightsL[round(params_knn['weights'])]\nparams_knn['p'] = round(params_knn['p'])\nparams_knn","7b0c5c19":"# Fit the training data\nknn_hyp = KNeighborsClassifier(**params_knn)\nknn_hyp.fit(X_trainS, y_train)\n\n# Predict the validation data\npred_knn = knn_hyp.predict(X_val)\n\n# Compute the accuracy\nprint('Accuracy: ' + str(accuracy_score(y_val, pred_knn)))","93cd79b9":"# Prediction Result\nprint('confusion_matrix')\nprint(pd.DataFrame(confusion_matrix(y_val, pred_knn)))\nprint(classification_report(y_val, pred_knn))","4a6110f8":"from sklearn.cluster import KMeans\nfrom sklearn.metrics.pairwise import euclidean_distances","30189ef8":"# Choosing number of clusters for test data\ninertia = []\nfor cluster in range(1,15):\n    kmeans = KMeans(n_clusters = cluster, init='k-means++', random_state=123)\n    kmeans.fit(X_trainS)\n    inertia.append(kmeans.inertia_)\n\n# Plot cluster numbers and inertia\ncluster_result = pd.DataFrame({'Cluster':range(1,15), 'inertia':inertia})\nplt.figure(figsize=(12,6))\nplt.plot(cluster_result['Cluster'], cluster_result['inertia'], marker='o')\nplt.xlabel('Number of clusters')\nplt.ylabel('Inertia')","6cbaa541":"# Set 8 clusters\nkmeans = KMeans(n_clusters = 8, init='k-means++', random_state=123)\nkmeans.fit(X_trainS)\nX_trainSclu = X_trainS.loc[:,:]\nX_trainSclu['cluster'] = kmeans.predict(X_trainS)\nprint(X_trainSclu.head())\n\n# Centroids of k-means\ncentroids = pd.DataFrame(kmeans.cluster_centers_)\ncentroids","4647569d":"# Combine X_trainS and y_trainsS\ntrainS = X_trainS.reset_index(drop=True)\ntrainS['Survived'] = list(y_train)\n\n# Measure Euclidian distance to respective centroid\ndef EucDist(Pclass, Age, SibSp, Parch, Fare, Sex_female, Sex_male, Embarked_C, Embarked_Q, Embarked_S, cluster):\n    dist = euclidean_distances([[Pclass, Age, SibSp, Parch, Fare, Sex_female, Sex_male, Embarked_C,\n                                 Embarked_Q, Embarked_S],\n                                list(centroids.loc[cluster,:])])[0,1]\n    \n    return dist\n\ntrainS['distance'] = X_trainSclu.apply(lambda a: EucDist(a['Pclass'], a['Age'], a['SibSp'],\n                                                              a['Parch'], a['Fare'], a['Sex_female'],\n                                                              a['Sex_male'], a['Embarked_C'],\n                                                              a['Embarked_Q'], a['Embarked_S'],\n                                                              a['cluster']), axis=1)\ntrainS.head()","0acb54e8":"n_smallest = 50\n\n# Split True and False training data\ntrainS_0 = trainS[trainS['Survived']==0]\ntrainS_1 = trainS[trainS['Survived']==1]\n\n# Split X and y for False training data\nX_trainS_0 = trainS_0.drop(columns=['Survived'], axis=0)\ny_trainS_0 = trainS_1.loc[:,'Survived']\n\nX_trainS_1 = trainS_1.drop(columns=['Survived'], axis=0)\ny_trainS_1 = trainS_1.loc[:,'Survived']\n\n# Filter 50 rows with smallest distance\nX_trainS_1 = X_trainS_1.sort_values('distance').reset_index().iloc[0:n_smallest,:]\nX_trainS_1.index = X_trainS_1['index']\nX_trainS_1 = X_trainS_1.drop(columns=['index'])\nX_trainS_1.head()\n\nX_trainS_0 = X_trainS_0.sort_values('distance').reset_index().iloc[0:n_smallest,:]\nX_trainS_0.index = X_trainS_0['index']\nX_trainS_0 = X_trainS_0.drop(columns=['index'])\nX_trainS_0.head()\n\n# Combine X_trainS_0 and X_trainS_1\nX_trainClus = pd.concat([X_trainS_0, X_trainS_1], axis=0).drop(columns=['distance', 'cluster']).reset_index(drop=True)\nX_trainClus.head()\n\n# Create y_trainClus\ny_trainClus = pd.DataFrame([0]*n_smallest)\ny_trainClus1 = pd.DataFrame([1]*n_smallest)\ny_trainClus = pd.concat([y_trainClus, y_trainClus1], axis=0).reset_index(drop=True)","e388f443":"from sklearn.svm import SVC","f7376e34":"# Hyperparameter-tuning: Grid Search\nparams_svm = {'C':[0.01,0.1,1,10]}\nsvm_grid = GridSearchCV(estimator=SVC(), param_grid=params_svm,\n                         scoring=acc_score, cv=5)\n\nsvm_grid.fit(X_trainClus, y_trainClus)\n\nprint('Best score: ' + str(svm_grid.best_score_))\nprint('Best parameter {}'.format(svm_grid.best_params_))","8309638d":"y_trainClus.shape","5f8d3628":"# Fit the training data\nsvm_hyp =  SVC(**svm_grid.best_params_, random_state=123)\nsvm_hyp.fit(X_trainClus, y_trainClus)\n\n# Predict the validation data\npred_svm = svm_hyp.predict(X_valS)\n\n# Compute the accuracy\nprint('Accuracy: ' + str(accuracy_score(y_val, pred_svm)))","bd2f4f7a":"# Prediction Result\nprint('confusion_matrix')\nprint(pd.DataFrame(confusion_matrix(y_val, pred_svm)))\nprint(classification_report(y_val, pred_svm))","adab8096":"from sklearn.tree import DecisionTreeClassifier","d372acb5":"# Hyperparameter-tuning: Bayesian Optimization, bayes_opt\ndef dt_cl_bo(criterion, splitter, max_depth, min_samples_split, min_samples_leaf):\n    params_dt = {}\n    criterionL = ['gini', 'entropy']\n    splitterL = ['best', 'random']\n    \n    params_dt['criterion'] = criterionL[round(criterion)]\n    params_dt['splitter'] = splitterL[round(splitter)]\n    params_dt['max_depth'] = round(max_depth)\n    params_dt['min_samples_split'] = round(min_samples_split)\n    params_dt['min_samples_leaf'] = round(min_samples_leaf)\n    \n    score = cross_val_score(DecisionTreeClassifier(random_state=123, **params_dt),\n                            X_train, y_train, scoring=acc_score, cv=5).mean()\n    return score\n\n# Set hyperparameters spaces\nparams_dt ={\n    'criterion':(0, 1),\n    'splitter':(0, 1),\n    'max_depth':(4, 15),\n    'min_samples_split':(2, 10),\n    'min_samples_leaf': (2, 10)\n}\n\n# Run Bayesian Optimization\ndt_bo = BayesianOptimization(dt_cl_bo, params_dt, random_state=123)\ndt_bo.maximize(init_points=4, n_iter=25)","cec2b911":"# Best hyperparameters\nparams_dt = dt_bo.max['params']\n\ncriterionL = ['gini', 'entropy']\nsplitterL = ['best', 'random']\n\nparams_dt['criterion'] = criterionL[int(round(params_dt['criterion']))]\nparams_dt['splitter'] = splitterL[int(round(params_dt['splitter']))]\nparams_dt['max_depth'] = round(params_dt['max_depth'])\nparams_dt['min_samples_split'] = round(params_dt['min_samples_split'])\nparams_dt['min_samples_leaf'] = round(params_dt['min_samples_leaf'])\nparams_dt","d084696b":"# Fit the training data\ndt_hyp =  DecisionTreeClassifier(**params_dt, random_state=123)\ndt_hyp.fit(X_train, y_train)\n\n# Predict the validation data\npred_dt = dt_hyp.predict(X_val)\n\n# Compute the accuracy\nprint('Accuracy: ' + str(accuracy_score(y_val, pred_dt)))","1ba00d69":"# Prediction Result\nprint('confusion_matrix')\nprint(pd.DataFrame(confusion_matrix(y_val, pred_dt)))\nprint(classification_report(y_val, pred_dt))","a8d00420":"# Feature importances\nFeature_dt = pd.DataFrame({'feature':X_train.columns, 'importance':list(dt_hyp.feature_importances_)}).sort_values('importance')\nplt.figure(figsize=(16,4))\nsns.barplot(data=Feature_dt, x='feature', y='importance')\nplt.xticks(rotation=90)\nplt.show()","973a8066":"from sklearn.ensemble import RandomForestClassifier","bb7bddfb":"# Hyperparameter-tuning: Bayesian Optimization, bayes_opt\ndef rf_cl_bo(n_estimators, criterion, max_depth, min_samples_split, min_samples_leaf,):\n    params_rf = {}\n    criterionL = ['gini', 'entropy']\n    \n    params_rf['n_estimators'] = round(n_estimators)\n    params_rf['criterion'] = criterionL[round(criterion)]\n    params_rf['max_depth'] = round(max_depth)\n    params_rf['min_samples_split'] = round(min_samples_split)\n    params_rf['min_samples_leaf'] = round(min_samples_leaf)\n    \n    score = cross_val_score(RandomForestClassifier(random_state=123, **params_rf),\n                             X_train, y_train, scoring=acc_score, cv=5).mean()\n    return score\n\n# Set hyperparameters spaces\nparams_rf ={\n    'n_estimators':(70, 150),\n    'criterion':(0, 1),\n    'max_depth':(4, 20),\n    'min_samples_split':(2, 10),\n    'min_samples_leaf': (2, 10)\n}\n\n# Run Bayesian Optimization\nrf_bo = BayesianOptimization(rf_cl_bo, params_rf, random_state=111)\nrf_bo.maximize(init_points=4, n_iter=25)","4a041096":"# Best hyperparameters\nparams_rf = rf_bo.max['params']\ncriterionL = ['gini', 'entropy']\n\nparams_rf['n_estimators'] = round(params_rf['n_estimators'])\nparams_rf['criterion'] = criterionL[int(round(params_rf['criterion']))]\nparams_rf['max_depth'] = round(params_rf['max_depth'])\nparams_rf['min_samples_split'] = round(params_rf['min_samples_split'])\nparams_rf['min_samples_leaf'] = round(params_rf['min_samples_leaf'])\nparams_rf","f452f52f":"# Fit the training data\nrf_hyp =  RandomForestClassifier(**params_rf, random_state=123)\nrf_hyp.fit(X_train, y_train)\n\n# Predict the validation data\npred_rf = rf_hyp.predict(X_val)\n\n# Compute the accuracy\nprint('Accuracy: ' + str(accuracy_score(y_val, pred_rf)))","6016cddd":"# Prediction Result\nprint('Confusion Matrix')\nprint(pd.DataFrame(confusion_matrix(y_val, pred_rf)))\nprint(classification_report(y_val, pred_rf))","175b5fe6":"# Feature importances\nFeature_rf = pd.DataFrame({'feature':X_train.columns, 'importance':list(rf_hyp.feature_importances_)}).sort_values('importance')\nplt.figure(figsize=(16,4))\nsns.barplot(data=Feature_rf, x='feature', y='importance')\nplt.xticks(rotation=90)\nplt.show()","f5139523":"from sklearn.ensemble import GradientBoostingClassifier","d2390a8e":"# Hyperparameter-tuning: Bayesian Optimization, bayes_opt\ndef gbm_cl_bo(max_depth, max_features, learning_rate, n_estimators, subsample):\n    params_gbm = {}\n    \n    params_gbm['max_depth'] = round(max_depth)\n    params_gbm['max_features'] = max_features\n    params_gbm['learning_rate'] = learning_rate\n    params_gbm['n_estimators'] = round(n_estimators)\n    params_gbm['subsample'] = subsample\n    \n    score = cross_val_score(GradientBoostingClassifier(random_state=123, **params_gbm),\n                             X_train, y_train, scoring=acc_score, cv=5).mean()\n    return score\n\n# Set hyperparameters spaces\nparams_gbm ={\n    'max_depth':(3, 10),\n    'max_features':(0.8, 1),\n    'learning_rate':(0.01, 1),\n    'n_estimators':(80, 150),\n    'subsample': (0.8, 1)\n}\n\n# Run Bayesian Optimization\ngbm_bo = BayesianOptimization(gbm_cl_bo, params_gbm, random_state=111)\ngbm_bo.maximize(init_points=4, n_iter=25)","66cb6552":"# Best hyperparameters\nparams_gbm = gbm_bo.max['params']\nparams_gbm['max_depth'] = round(params_gbm['max_depth'])\nparams_gbm['n_estimators'] = round(params_gbm['n_estimators'])\nparams_gbm","57405c18":"# Fit the training data\ngbm_hyp =  GradientBoostingClassifier(**params_gbm, random_state=123)\ngbm_hyp.fit(X_train, y_train)\n\n# Predict the validation data\npred_gbm = gbm_hyp.predict(X_val)\n\n# Compute the accuracy\naccuracy_score(y_val, pred_gbm)","1ac1b340":"# Prediction Result\nprint('Confusion Matrix')\nprint(pd.DataFrame(confusion_matrix(y_val, pred_nbBer)))\nprint(classification_report(y_val, pred_gbm))","88bf4afd":"# Feature importances\nFeature_gbm = pd.DataFrame({'feature':X_train.columns, 'importance':list(gbm_hyp.feature_importances_)}).sort_values('importance')\nplt.figure(figsize=(16,4))\nsns.barplot(data=Feature_gbm, x='feature', y='importance')\nplt.xticks(rotation=90)\nplt.show()","65439422":"import lightgbm","36319948":"# Hyperparameter-tuning: Bayesian Optimization, bayes_opt\ndef lgbm_cl_bo(max_depth, subsample, colsample_bytree,min_child_weight, learning_rate, num_leaves_percentage):\n    params_lgbm = {'objective': 'binary'}\n    \n    params_lgbm['max_depth'] = round(max_depth)\n    params_lgbm['subsample'] = subsample\n    params_lgbm['colsample_bytree'] = colsample_bytree\n    params_lgbm['min_child_weight'] = min_child_weight\n    params_lgbm['learning_rate'] = learning_rate\n    params_lgbm['num_leaves'] = round((2**round(max_depth))*num_leaves_percentage)\n    \n    lgbm_bo = lightgbm.LGBMClassifier(random_state=123, **params_lgbm)\n    score = cross_val_score(lgbm_bo, X_train, y_train, scoring=acc_score, cv=5).mean()\n    return score\n\n# Set parameters distribution\nparams_lgbm ={\n    'min_child_weight':(1e-5, 1e-1),\n    'subsample':(0.5, 1),\n    'colsample_bytree':(0.5, 1),\n    'max_depth': (3, 15),\n    'learning_rate': (0.01, 0.5),\n    'num_leaves_percentage':(0.5,0.9)\n}\n\n# Run Bayesian Optimization\nlgbm_bo = BayesianOptimization(lgbm_cl_bo, params_lgbm, random_state=111)\nlgbm_bo.maximize(init_points=4, n_iter=25)","d5673cee":"# Best hyperparameters\nparams_lgbm = lgbm_bo.max['params']\nparams_lgbm['objective'] = 'binary'\nparams_lgbm['max_depth'] = int(params_lgbm['max_depth'])\nparams_lgbm['num_leaves'] = round((2**round(params_lgbm['max_depth']))*params_lgbm['num_leaves_percentage'])\ndel params_lgbm[\"num_leaves_percentage\"]\nparams_lgbm","ba866fea":"# Fit the training data\nlgbm_hyp =  lightgbm.LGBMClassifier(**params_lgbm, random_state=123, n_jobs=-1)\nlgbm_hyp.fit(X_train, y_train)\n\n# Predict the validation data\npred_lgbm = lgbm_hyp.predict(X_val)\n\n# Compute the accuracy\n#print('Accuracy: ' + str(accuracy_score(y_val, pred_lgbm)))\naccuracy_score(y_val, pred_lgbm) ","b6c2b362":"# Prediction Result\nprint('Confusion Matrix')\nprint(pd.DataFrame(confusion_matrix(y_val, pred_lgbm)))\nprint(classification_report(y_val, pred_lgbm))","3f6ed835":"# Feature importances\nFeatureLgbm = pd.DataFrame({'feature':X_train.columns, 'importance':list(gbm_hyp.feature_importances_)}).sort_values('importance')\nplt.figure(figsize=(16,4))\nsns.barplot(data=FeatureLgbm, x='feature', y='importance')\nplt.xticks(rotation=90)\nplt.show()","00294638":"from xgboost import XGBClassifier","39e7d480":"# Hyperparameter tuning: Bayesian Optimization\ndef xgb_cl_bo(n_estimators, max_depth, learning_rate, gamma, min_child_weight, subsample, colsample_bytree):\n    params_xgb = {\n    'objective': 'binary:hinge',\n    'nthread':-1\n     }\n    params_xgb['n_estimators'] = round(n_estimators)\n    params_xgb['max_depth'] = round(max_depth)\n    params_xgb['learning_rate'] = learning_rate\n    params_xgb['gamma'] = gamma\n    params_xgb['min_child_weight'] = round(min_child_weight)\n    params_xgb['subsample'] = subsample\n    params_xgb['colsample_bytree'] = colsample_bytree\n        \n    score = cross_val_score(XGBClassifier(random_state=123, **params_xgb),\n                            X_train, y_train, scoring=acc_score, cv=5).mean()\n    return score\n\n# Set parameters distribution\nparams_xgb ={\n    'n_estimators':(80, 150),\n    'max_depth': (3, 15),\n    'learning_rate': (0.01, 0.5),\n    'gamma':(0, 10),\n    'min_child_weight':(3, 20),\n    'subsample':(0.5, 1),\n    'colsample_bytree':(0.1, 1)\n}\n\n# Run Bayesian Optimization\nxgb_bo = BayesianOptimization(xgb_cl_bo, params_xgb, random_state=111)\nxgb_bo.maximize(init_points=4, n_iter=25)","05f9c192":"# Best hyperparameters\nparams_xgb = xgb_bo.max['params']\nparams_xgb['objective'] = 'binary:hinge'\nparams_xgb['n_jobs'] = -1\nparams_xgb['n_estimators'] = round(params_xgb['n_estimators'])\nparams_xgb['max_depth'] = round(params_xgb['max_depth'])\nparams_xgb['min_child_weight'] = round(params_xgb['min_child_weight'])\n\nparams_xgb","2599862b":"# Fit the training data\nxgb_hyp =  XGBClassifier(**params_xgb, random_state=123, nthread=-1)\nxgb_hyp.fit(X_train, y_train)\n\n# Predict the validation data\npred_xgb = xgb_hyp.predict(X_val)\n\n# Compute the accuracy\nprint('Accuracy: ' + str(accuracy_score(y_val, pred_xgb)))","444dcca4":"# Prediction Result\nprint('Confusion Matrix')\nprint(pd.DataFrame(confusion_matrix(y_val, pred_xgb)))\nprint(classification_report(y_val, pred_xgb))","d57d499c":"# Feature importances\nFeatureXgb = pd.DataFrame({'feature':X_train.columns, 'importance':list(xgb_hyp.feature_importances_)}).sort_values('importance')\nplt.figure(figsize=(16,4))\nsns.barplot(data=FeatureXgb, x='feature', y='importance')\nplt.xticks(rotation=90)\nplt.show()","762f352b":"# Deep Learning packages\nfrom keras.models import Sequential\nfrom keras.layers import Dense, BatchNormalization, Dropout\nfrom keras.optimizers import Adam, SGD, RMSprop, Adadelta, Adagrad, Adamax, Nadam, Ftrl\nfrom keras.callbacks import EarlyStopping, ModelCheckpoint\nfrom keras.wrappers.scikit_learn import KerasClassifier\nfrom keras.layers import LeakyReLU\nLeakyReLU = LeakyReLU(alpha=0.1)","3bcee76a":"# Hyperparameter-tuning: Bayesian Optimization, bayes_opt\ndef nn_cl_bo(neurons, activation, optimizer, learning_rate, batch_size, epochs,\n              layers1, layers2, normalization, dropout, dropout_rate):\n    optimizerL = ['Adam', 'RMSprop', 'Adadelta', 'Adagrad', 'Adamax', 'Nadam', 'Ftrl','SGD', 'SGD']\n    optimizerD= {'Adam':Adam(lr=learning_rate), 'SGD':SGD(lr=learning_rate),\n                 'RMSprop':RMSprop(lr=learning_rate), 'Adadelta':Adadelta(lr=learning_rate),\n                 'Adagrad':Adagrad(lr=learning_rate), 'Adamax':Adamax(lr=learning_rate),\n                 'Nadam':Nadam(lr=learning_rate), 'Ftrl':Ftrl(lr=learning_rate)}\n    activationL = ['relu', 'sigmoid', 'softplus', 'softsign', 'tanh', 'selu',\n               'elu', 'exponential', LeakyReLU, LeakyReLU]\n        \n    neurons = round(neurons)\n    activation = activationL[floor(activation)]\n    optimizer = optimizerD[optimizerL[floor(optimizer)]]\n    batch_size = round(batch_size)\n    epochs = round(epochs)\n    layers1 = round(layers1)\n    layers2 = round(layers2)\n        \n    def nn_cl_fun():\n        nn = Sequential()\n        nn.add(Dense(neurons, input_dim=10, activation=activation))\n        if normalization > 0.5:\n            nn.add(BatchNormalization())\n        for i in range(layers1):\n            nn.add(Dense(neurons, activation=activation))\n        if dropout > 0.5:\n            nn.add(Dropout(dropout_rate, seed=123))\n        for i in range(layers2):\n            nn.add(Dense(neurons, activation=activation))\n        nn.add(Dense(2, activation='softmax'))\n        nn.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n        return nn\n        \n    es = EarlyStopping(monitor='accuracy', mode='max', verbose=0, patience=20)\n    nn = KerasClassifier(build_fn=nn_cl_fun, epochs=epochs, batch_size=batch_size, verbose=0)\n    \n    kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=123)\n    score = cross_val_score(nn, X_train, y_train, scoring=acc_score, cv=kfold, fit_params={'callbacks':[es]}).mean()\n    \n    return score\n\n# Set hyperparameters spaces\nparams_nn ={\n    'neurons': (10, 100),\n    'activation':(0, 9),\n    'optimizer':(0,7),\n    'learning_rate':(0.01, 1),\n    'batch_size':(5000, 10000),\n    'epochs':(200, 1000),\n    'layers1':(1,3),\n    'layers2':(1,3),\n    'normalization':(0,1),\n    'dropout':(0,1),\n    'dropout_rate':(0,0.3)\n}\n\n# Run Bayesian Optimization\nnn_bo = BayesianOptimization(nn_cl_bo, params_nn, random_state=123)\nnn_bo.maximize(init_points=4, n_iter=25)","c1634d33":"# Best hyperparameters\nparams_nn = nn_bo.max['params']\n\nlearning_rate = params_nn['learning_rate']\noptimizerL = ['Adam', 'RMSprop', 'Adadelta', 'Adagrad', 'Adamax', 'Nadam', 'Ftrl','SGD', 'SGD']\noptimizerD= {'Adam':Adam(lr=learning_rate), 'SGD':SGD(lr=learning_rate),\n             'RMSprop':RMSprop(lr=learning_rate), 'Adadelta':Adadelta(lr=learning_rate),\n             'Adagrad':Adagrad(lr=learning_rate), 'Adamax':Adamax(lr=learning_rate),\n             'Nadam':Nadam(lr=learning_rate), 'Ftrl':Ftrl(lr=learning_rate)}\nactivationL = ['relu', 'sigmoid', 'softplus', 'softsign', 'tanh', 'selu',\n               'elu', 'exponential', LeakyReLU, LeakyReLU]\nparams_nn['activation'] = activationL[round(params_nn['activation'])]\nparams_nn['batch_size'] = round(params_nn['batch_size'])\nparams_nn['epochs'] = round(params_nn['epochs'])\nparams_nn['layers1'] = round(params_nn['layers1'])\nparams_nn['layers2'] = round(params_nn['layers2'])\nparams_nn['neurons'] = round(params_nn['neurons'])\nparams_nn['optimizer'] = optimizerD[optimizerL[round(params_nn['optimizer'])]]\n\nparams_nn","360c7664":"# Fitting the training data\ndef nn_cl_fun():\n    nn = Sequential()\n    nn.add(Dense(params_nn['neurons'], input_dim=10, activation=params_nn['activation']))\n    if params_nn['normalization'] > 0.5:\n        nn.add(BatchNormalization())\n    for i in range(params_nn['layers1']):\n        nn.add(Dense(params_nn['neurons'], activation=params_nn['activation']))\n    if params_nn['dropout'] > 0.5:\n        nn.add(Dropout(params_nn['dropout_rate'], seed=123))\n    for i in range(params_nn['layers2']):\n        nn.add(Dense(params_nn['neurons'], activation=params_nn['activation']))\n    nn.add(Dense(1, activation='sigmoid'))\n    nn.compile(loss='binary_crossentropy', optimizer=params_nn['optimizer'], metrics=['accuracy'])\n    return nn\n        \nes = EarlyStopping(monitor='accuracy', mode='max', verbose=0, patience=20)\nnn_hyp = KerasClassifier(build_fn=nn_cl_fun, epochs=params_nn['epochs'], batch_size=params_nn['batch_size'],\n                         verbose=0)\n \nnn_hyp.fit(X_train, y_train, validation_data=(X_val, y_val), verbose=0)\n\n# Predict the validation data\npred_nn = nn_hyp.predict(X_val)\n\n# Compute the accuracy\nprint('Accuracy: ' + str(accuracy_score(y_val, pred_nn)))","20c82806":"# Prediction Result\nprint('Confusion Matrix')\nprint(pd.DataFrame(confusion_matrix(y_val, pred_nn)))\nprint(classification_report(y_val, pred_nn))","462b6a63":"# 5. Decision Tree","5c66489c":"# 6. Random Forest","6fe71516":"# 7. Gradient Boosting Machine","03e1c49d":"# 2a. Naive Bayes (Gaussian)","e6017f09":"# 4. Support Vector Machine","9743fedf":"# 1. Logistic Regression","770bb826":"# 2b. naive Bayes (Bernoulli)","1bb46eec":"## \ud83d\udea2 Binary Classification-Accuracy-Titanic Survival\n\nThis notebook provides commonly used Machine Learning algorithms. The task is binary classification. Feature generation or selection is just simply performed. The objective of this notebook is to serve as a cheat sheet.\n\nTen Machine Learning algorithms are developed to predict with accuracy as the scorer. All algorithms are applied with hyperparameter-tuning to search for the optimum model evaluation results. The hyperparameter-tuning methods consist of GridSearchCV and Bayesian Optimization (using bayes_opt or hyperopt packages) with 5-fold cross-validation.\n\nThe optimum hyperparameters are then used to train the training dataset and predict the unseen validation dataset. The model is evaluated using accuracy, followed by the confusion matrix and classification report. Useful attributes of the models are also displayed, such as the coefficients or feature importances.","e197941b":"# 8. LightGBM","c45c93c2":"# 10. Neural Network (Deep Learning)","bca14d60":"# 9. XGBoost","89b021cb":"# 3. K Nearest Neighbors"}}