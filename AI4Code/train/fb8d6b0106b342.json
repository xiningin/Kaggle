{"cell_type":{"3ebafb45":"code","445d430d":"code","37c798a2":"code","e11ede68":"code","8a1f4da2":"code","cdd0d037":"code","f01ad353":"code","50b7b616":"code","0f7ad1a6":"code","e7547323":"code","3d19d649":"code","eb81d0f3":"code","6681c19c":"code","99fe1952":"code","5577a3cb":"code","00aa4faa":"code","387464e7":"code","f3ecd801":"code","3b599733":"code","4e2be1b6":"code","04de9c74":"code","bc4f58c7":"code","ff607676":"code","c53897d7":"code","be247c75":"code","f55494aa":"code","f4db3175":"code","9be35325":"code","747970dd":"code","38e06787":"code","2a207047":"code","a52c8e18":"code","3c93178a":"code","c0fb3ea1":"code","47543293":"code","afd46201":"code","bcbe1ef8":"code","16400ae9":"code","de7ec59a":"code","8a66c9d5":"code","5804f54a":"code","71e00602":"code","67c81dc1":"code","24cf57eb":"code","77e00e72":"code","8f791814":"code","56460a48":"code","69e979ba":"code","4c7985ab":"code","fb84813f":"code","6cfd6337":"code","99d505ef":"code","4e40f9c7":"markdown","ddbc663d":"markdown","939ba823":"markdown","9cbd3cfd":"markdown","24cb1f51":"markdown","a062df29":"markdown","0534c568":"markdown","8a65de50":"markdown","936bd07f":"markdown","66bfc3df":"markdown","6a21951b":"markdown","b6ec5d27":"markdown","4c4fbe55":"markdown","99d08279":"markdown","f828f45d":"markdown","4d0a2170":"markdown","8d2fc2b8":"markdown","8e132c17":"markdown","5977fd07":"markdown","f87d142e":"markdown","703674ea":"markdown","89931476":"markdown","76568a72":"markdown","0bf3d129":"markdown","bf2e657c":"markdown","137f757c":"markdown","d86ab52c":"markdown","196d4452":"markdown","93024214":"markdown","a05c5a6d":"markdown","d02db22c":"markdown","3a3272b7":"markdown","d61d3959":"markdown","3eaf5120":"markdown","d763271f":"markdown","0eb784dc":"markdown","19e1ceed":"markdown","368e2aee":"markdown","2bfb7098":"markdown","40eba587":"markdown","8c919b11":"markdown","aad74f9e":"markdown","618e8742":"markdown","c73f4e5f":"markdown","ca3836f0":"markdown","8605c7b2":"markdown","14e4dd6c":"markdown","a0deadef":"markdown","975cc231":"markdown","756487b9":"markdown","45069c9d":"markdown","a0b325b5":"markdown","90cc9f85":"markdown","0eb5cc5a":"markdown","def5bce7":"markdown","cdab3500":"markdown","399a8c97":"markdown","c28150af":"markdown","3239a426":"markdown","086ec4a5":"markdown","d5f1c9b0":"markdown","2932b2b3":"markdown","2ba09865":"markdown","9b1c9e81":"markdown","86049758":"markdown","ae947a15":"markdown","f396ef8f":"markdown","4e27d9fe":"markdown","6e04cfd0":"markdown","31d75014":"markdown","73e05747":"markdown","9c60101c":"markdown","5185e8ba":"markdown","87c463d3":"markdown","3746174a":"markdown","a273bd70":"markdown","cd5230a8":"markdown","c420c36c":"markdown","1618b147":"markdown","bd4cf350":"markdown"},"source":{"3ebafb45":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","445d430d":"#import all the necessary packages\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\n%matplotlib inline\nfrom matplotlib import style\nstyle.use('ggplot')","37c798a2":"#read the train and test data\n\ntest = pd.read_csv('\/kaggle\/input\/kuc-hackathon-winter-2018\/drugsComTest_raw.csv') #train data\ntrain = pd.read_csv('\/kaggle\/input\/kuc-hackathon-winter-2018\/drugsComTrain_raw.csv') #test data","e11ede68":"#check the head of train data\ntrain.head(10)","8a1f4da2":"#check the head of test data\ntest.head(10)","cdd0d037":"#check the shape of the given dataset\nprint(f'train has {train.shape[0]} number of rows and {train.shape[1]} number of columns')\nprint(f'train has {test.shape[0]} number of rows and {test.shape[1]} number of columns')","f01ad353":"#check the columns in train\ntrain.columns","50b7b616":"#merge train and test data\n\nmerge = [train,test]\nmerged_data = pd.concat(merge,ignore_index=True)\n\nmerged_data.shape   #check the shape of merged_data","0f7ad1a6":"#check uniqueID\nmerged_data['uniqueID'].nunique()","e7547323":"merged_data.info()","3d19d649":"merged_data.describe(include='all')","eb81d0f3":"merged_data.isnull().sum()\/merged_data.shape[0]","6681c19c":"#check number of unique values in drugName\nprint(merged_data['drugName'].nunique())\n\n#check number of unique values in condition\nprint(merged_data['condition'].nunique())","99fe1952":"#plot a bargraph to check top 20 conditions\nplt.figure(figsize=(12,6))\nconditions = merged_data['condition'].value_counts(ascending = False).head(20)\n\nplt.bar(conditions.index,conditions.values)\nplt.title('Top-20 Conditions',fontsize = 20)\nplt.xticks(rotation=90)\nplt.ylabel('count')\nplt.show()","5577a3cb":"#plot a bargraph to check bottom 20 conditions\nplt.figure(figsize=(12,6))\nconditions_bottom = merged_data['condition'].value_counts(ascending = False).tail(20)\n\nplt.bar(conditions_bottom.index,conditions_bottom.values)\nplt.title('Bottom-20 Conditions',fontsize = 20)\nplt.xticks(rotation=90)\nplt.ylabel('count')\nplt.show()","00aa4faa":"#plot a bargraph to check top 20 drugName\nplt.figure(figsize=(12,6))\ndrugName_top = merged_data['drugName'].value_counts(ascending = False).head(20)\n\nplt.bar(drugName_top.index,drugName_top.values,color='blue')\nplt.title('drugName Top-20',fontsize = 20)\nplt.xticks(rotation=90)\nplt.ylabel('count')\nplt.show()","387464e7":"#plot a bargraph to check top 20 drugName\nplt.figure(figsize=(12,6))\ndrugName_bottom = merged_data['drugName'].value_counts(ascending = False).tail(20)\n\nplt.bar(drugName_bottom.index,drugName_bottom.values,color='blue')\nplt.title('drugName Bottom-20',fontsize = 20)\nplt.xticks(rotation=90)\nplt.ylabel('count')\nplt.show()","f3ecd801":"ratings_ = merged_data['rating'].value_counts().sort_values(ascending=False).reset_index().\\\n                    rename(columns = {'index' :'rating', 'rating' : 'counts'})\nratings_['percent'] = 100 * (ratings_['counts']\/merged_data.shape[0])\nprint(ratings_)","3b599733":"# Setting the Parameter\nsns.set(font_scale = 1.2, style = 'darkgrid')\nplt.rcParams['figure.figsize'] = [12, 6]\n\n#let's plot and check\nsns.barplot(x = ratings_['rating'], y = ratings_['percent'],order = ratings_['rating'])\nplt.title('Ratings Percent',fontsize=20)\nplt.show()","4e2be1b6":"#plot a distplot of usefulCount\nsns.distplot(merged_data['usefulCount'])\nplt.show()","04de9c74":"#check the descriptive summary\nsns.boxplot(y = merged_data['usefulCount'])\nplt.show()","bc4f58c7":"#lets check the number of drugs\/condition\nmerged_data.groupby('condition')['drugName'].nunique().sort_values(ascending=False).head(20)","ff607676":"span_data = merged_data[merged_data['condition'].str.contains('<\/span>',case=False,regex=True) == True]\nprint('Number of rows with <\/span> values : ', len(span_data))\nnoisy_data_ = 100 * (len(span_data)\/merged_data.shape[0])\nprint('Total percent of noisy data {} %  '.format(noisy_data_))","c53897d7":"#drop the nosie \nmerged_data.drop(span_data.index, axis = 0, inplace=True)","be247c75":"#check the percentage of 'not listed \/ othe' conditions\nnot_listed = merged_data[merged_data['condition'] == 'not listed \/ othe']\nprint('Number of not_listed values : ', len(not_listed))\npercent_not_listed = 100 * len(not_listed)\/merged_data.shape[0]\nprint('Total percent of noisy data {} %  '.format(percent_not_listed))","f55494aa":"# drop noisy data\nmerged_data.drop(not_listed.index, axis = 0, inplace=True)","f4db3175":"# after removing the noise, let's check the shape\nmerged_data.shape[0]","9be35325":"#lets check the number of drugs present in our dataset condition wise\nconditions_gp = merged_data.groupby('condition')['drugName'].nunique().sort_values(ascending=False)\n\n#plot the top 20\n# Setting the Parameter\ncondition_gp_top_20 = conditions_gp.head(20)\nsns.set(font_scale = 1.2, style = 'darkgrid')\nplt.rcParams['figure.figsize'] = [12, 6]\nsns.barplot(x = condition_gp_top_20.index, y = condition_gp_top_20.values)\nplt.title('Top-20 Number of drugs per condition',fontsize=20)\nplt.xticks(rotation=90)\nplt.ylabel('count',fontsize=10)\nplt.show()","747970dd":"#bottom-20\ncondition_gp_bottom_20 = conditions_gp.tail(20)\n#plot the top 20\n\nsns.barplot(x = condition_gp_bottom_20.index, y = condition_gp_bottom_20.values,color='blue')\nplt.title('Bottom-20 Number of drugs per condition',fontsize=20)\nplt.xticks(rotation=90)\nplt.ylabel('count',fontsize=10)\nplt.show()","38e06787":"#let's check if a single drug is used for multiple conditions\ndrug_multiple_cond = merged_data.groupby('drugName')['condition'].nunique().sort_values(ascending=False)\nprint(drug_multiple_cond.head(10))","2a207047":"#Let's check the Number of drugs with rating 10.\nmerged_data[merged_data['rating'] == 10]['drugName'].nunique()","a52c8e18":"#Check top 20 drugs with rating=10\/10\ntop_20_ratings = merged_data[merged_data['rating'] == 10]['drugName'].value_counts().head(20)\nsns.barplot(x = top_20_ratings.index, y = top_20_ratings.values )\nplt.xticks(rotation=90)\nplt.title('Top-20 Drugs with Rating - 10\/10', fontsize=20)\nplt.ylabel('count')\nplt.show()","3c93178a":"merged_data[merged_data['drugName'] == 'Levonorgestrel']['condition'].unique()","c0fb3ea1":"#check top 20 drugs with 1\/10 rating\n\ntop_20_ratings_1 = merged_data[merged_data['rating'] == 1]['drugName'].value_counts().head(20)\nsns.barplot(x = top_20_ratings_1.index, y = top_20_ratings_1.values )\nplt.xticks(rotation=90)\nplt.title('Top-20 Drugs with Rating - 1\/10', fontsize=20)\nplt.ylabel('count')\nplt.show()","47543293":"# convert date to datetime and create year andd month features\n\nmerged_data['date'] = pd.to_datetime(merged_data['date'])\nmerged_data['year'] = merged_data['date'].dt.year  #create year\nmerged_data['month'] = merged_data['date'].dt.month #create month","afd46201":"#plot number of reviews year wise\ncount_reviews = merged_data['year'].value_counts().sort_index()\nsns.barplot(count_reviews.index,count_reviews.values,color='blue')\nplt.title('Number of reviews Year wise')\nplt.show()","bcbe1ef8":"#check average rating per year\nyearly_mean_rating = merged_data.groupby('year')['rating'].mean()\nsns.barplot(yearly_mean_rating.index,yearly_mean_rating.values,color='green')\nplt.title('Mean Rating Yearly')\nplt.show()","16400ae9":"#check year wise drug counts and year wise conditions counts\n\nyear_wise_condition = merged_data.groupby('year')['condition'].nunique()\nsns.barplot(year_wise_condition.index,year_wise_condition.values,color='green')\nplt.title('Conditions Year wise',fontsize=20)\nplt.show()","de7ec59a":"#check drugs year wise\n\nyear_wise_drug = merged_data.groupby('year')['drugName'].nunique()\nsns.barplot(year_wise_drug.index,year_wise_drug.values,color='green')\nplt.title('Drugs Year Wise',fontsize=20)\nplt.show()","8a66c9d5":"# check the null values\nmerged_data.isnull().sum()","5804f54a":"# drop the null values\nmerged_data.dropna(inplace=True, axis=0)","71e00602":"#check first three reviews\nfor i in merged_data['review'][0:3]:\n    print(i,'\\n')","67c81dc1":"#import the libraries for pre-processing\nfrom bs4 import BeautifulSoup\nimport nltk\nimport re\nfrom nltk.corpus import stopwords\nfrom nltk.stem.snowball import SnowballStemmer\n\nstops = set(stopwords.words('english')) #english stopwords\n\nstemmer = SnowballStemmer('english') #SnowballStemmer\n\ndef review_to_words(raw_review):\n    # 1. Delete HTML \n    review_text = BeautifulSoup(raw_review, 'html.parser').get_text()\n    # 2. Make a space\n    letters_only = re.sub('[^a-zA-Z]', ' ', review_text)\n    # 3. lower letters\n    words = letters_only.lower().split()\n    # 5. Stopwords \n    meaningful_words = [w for w in words if not w in stops]\n    # 6. Stemming\n    stemming_words = [stemmer.stem(w) for w in meaningful_words]\n    # 7. space join words\n    return( ' '.join(stemming_words))","24cf57eb":"#apply review_to_words function on reviews\nmerged_data['review'] = merged_data['review'].apply(review_to_words)","77e00e72":"#create sentiment feature from ratings\n#if rating > 5 sentiment = 1 (positive)\n#if rating < 5 sentiment = 0 (negative)\nmerged_data['sentiment'] = merged_data[\"rating\"].apply(lambda x: 1 if x > 5 else 0)","8f791814":"#import all the necessary packages\n\nfrom sklearn.model_selection import train_test_split #import train_test_split\nfrom sklearn.feature_extraction.text import TfidfVectorizer #import TfidfVectorizer \nfrom sklearn.metrics import confusion_matrix #import confusion_matrix\nfrom sklearn.naive_bayes import MultinomialNB #import MultinomialNB\nfrom sklearn.ensemble import RandomForestClassifier  #import RandomForestClassifier","56460a48":"# Creates TF-IDF vectorizer and transforms the corpus\nvectorizer = TfidfVectorizer()\nreviews_corpus = vectorizer.fit_transform(merged_data.review)\nreviews_corpus.shape","69e979ba":"#dependent feature\nsentiment = merged_data['sentiment']\nsentiment.shape","4c7985ab":"#split the data in train and test\nfrom sklearn.model_selection import train_test_split\nX_train,X_test,Y_train,Y_test = train_test_split(reviews_corpus,sentiment,test_size=0.33,random_state=42)\nprint('Train data shape ',X_train.shape,Y_train.shape)\nprint('Test data shape ',X_test.shape,Y_test.shape)","fb84813f":"#fit the model and predicct the output\n\nclf = MultinomialNB().fit(X_train, Y_train) #fit the training data\n\npred = clf.predict(X_test) #predict the sentiment for test data\n\nprint(\"Accuracy: %s\" % str(clf.score(X_test, Y_test))) #check accuracy\nprint(\"Confusion Matrix\") \nprint(confusion_matrix(pred, Y_test)) #print confusion matrix","6cfd6337":"#fit the model and predicct the output\n\nclf = RandomForestClassifier().fit(X_train, Y_train)\n\npred = clf.predict(X_test)\n\nprint(\"Accuracy: %s\" % str(clf.score(X_test, Y_test)))\nprint(\"Confusion Matrix\")\nprint(confusion_matrix(pred, Y_test))","99d505ef":"#Refrences - https:\/\/www.kaggle.com\/chocozzz\/recommendation-medicines-by-using-a-review\n            #https:\/\/www.kaggle.com\/sumitm004\/eda-and-sentiment-analysis","4e40f9c7":"* We can see that Levonorgestrel has most of the ratings 10\/10. It seems it is used for the common condition and, it would be the most effective one.\n* Other drugs have ratings between 1000 and 500 from top-20 10\/10.","ddbc663d":"By looking at the head of train and test data we see that there are 7 features in our Dataset but we don't have any sentiment feature which can serve as our target variable. We will make a target feature out of rating. If Rating is greater than 5 we will assign it as positive else we will assign it as negative.","939ba823":"### TfidfVectorizer (Term frequency - Inverse document frequency)\n**TF - Term Frequency** :- \n\nHow often a term t occurs in a document d.\n\nTF = (_Number of occurences of a word in document_) \/ (_Number of words in that document_)\n\n**Inverse  Document Frequency**\n\nIDF = log(Number of sentences \/ Number of sentence containing word)\n\n**Tf - Idf = Tf * Idf**\n","9cbd3cfd":"## Conclusion \nAfter applying the TfidfVectorizer to transform our reviews in Vectors and applying NaiveBayes and RandomForestClassifier we see that RandomForestClassifier outperforms MulinomialNB. We have achieved accuracy of 89.7 % after applying RandomForestClassifier without any parameter tuning. We can tune the parameters of our classifier and improve our accuracy.","24cb1f51":"* The bottom 20 drugName has count 1. These might be the drugs used of rare conditions or are new in market.","a062df29":"### Check the Description","0534c568":"### Check bottom 20 drugName","8a65de50":"Top-3 of 1\/10 ratings have almost 700 counts. Which means they are not so useful drugs.","936bd07f":"We notice that most of the ratings are high with ratings 10 and 9.\nrating 1 is also high which shows the extreme ratings of the user. We can say that the users mostly prefer to rate when the drugs are either very useful to them or the drugs fails, or there is some side effects. About 70% of the values have rating greater than 7.","66bfc3df":"* Bottom 20 conditions have just single counts in our dataset. They may be the rare conditions.\n* And if we look at our plot we see that there are conditions whose name are strange starting with **\"61<_\/span_>users found this comment helpful\"** , these are the noise present in our data. We will deal with these noise later.","6a21951b":"### Per year drug count and Condition count","b6ec5d27":"### Apply RandomForestClassifier","4c4fbe55":"## Data Pre-Processing","99d08279":"### Check average rating per year","f828f45d":"* Most of the drugs are for pain, birth control and high blood pressure which are common conditions.\n* In top- 20 each condition has above 50 drugs.","4d0a2170":"### **Store Dependent feature in sentiment and split the Data into train and test**","8d2fc2b8":"* Rating has been almost constant from year 2009 - 2014 but after 2014 the ratings has started to decrease.\n* As the number of reviews has increased for last 3 years, the rating has decreased.","8e132c17":"### Check bottom 20 drugs per conditions","5977fd07":"## Exploratory Data Analysis","f87d142e":"### Check Number of reviews per year","703674ea":"### Check the top 20 conditions","89931476":"### Now Check number of drugs present per condition after removing noise","76568a72":"The purpose of EDA is to find out interesting insights and irregularities in our Dataset. We will look at Each feature and try to find out interesting facts and patterns from them. And see whether there is any relationship between the variables or not.","0bf3d129":"### Check information of the merged data","bf2e657c":"### Check the number of drugs with rating 10","137f757c":"### Check number of unique values in drugName and condition","d86ab52c":"### Data Description :\nThe data is split into a train (75%) a test (25%) partition.\n\n* drugName (categorical): name of drug\n* condition (categorical): name of condition\n* review (text): patient review\n* rating (numerical): 10 star patient rating\n* date (date): date of review entry\n* usefulCount (numerical): number of users who found review useful\n\nThe structure of the data is that a patient with a unique ID purchases a drug that meets his condition and writes a review and rating for the drug he\/she purchased on the date. Afterwards, if the others read that review and find it helpful, they will click usefulCount, which will add 1 for the variable.","196d4452":"The year 2015, 2016 and 2017 accounts for the most reviews. Almost 60% of the reviews are from these years.","93024214":"### Now let's check if a single drug can be used for Multiple conditions","a05c5a6d":"**Check the first few reviews**","d02db22c":"### Check number of Drugs per condition","3a3272b7":"We can see that there are huge outliers present in our dataset. Some drugs have extreme useful counts.","d61d3959":"We can see that there are 3671 drugName and only 916 conditions. So there are conditions which has multiple drugs.","3eaf5120":"### Checking Ratings Distribution","d763271f":"### Pre-Processing Reviews","0eb784dc":"### Check the distribution of usefulCount","19e1ceed":"### Load Data","368e2aee":"* Condition has increased in last 3 years. Which means the new conditions has been coming up.\n* Starting year 2008 had lowest number of conditions. ","2bfb7098":"**From above graph we can see that the :**\n* Birth control is twice as big as anyone, around 38,000.\n* Most of the conditions for top 20 conditions are between 5000 - 10000 ","40eba587":"If we look above the top value is not listed\/othe. \n* It might be possible that the user didn't mentioned his\/her condition as sometimes people doesn't want to reveal thier disorders. We can look up the drug names and fill up the conditions for which that drug is used.\n\n* Another point to note here is that there are values is condition like **'3 <_\/span_> user found this comment helpful'**, **4<_\/span_> users found this comment helpful**. These are the noises present in our dataset. The dataset appears to have been extracted through webscraping, the values are wrongly fed in here.","8c919b11":"### Now we will look at the Date column","aad74f9e":"## Problem Statement\nThe dataset provides patient reviews on specific drugs along with related conditions and a 10 star patient rating reflecting overall patient satisfaction. We have to create a target feature out of ratings and predict the sentiment of the reviews.","618e8742":"### Now let's look at the not listed\/other","c73f4e5f":"We will predict the sentiment using the reviews only. So let's start building our model.","ca3836f0":"### Check for what condition Levonorgestrel is used for","8605c7b2":"### Steps for reviews pre-processing.\n* **Remove HTML tags**\n     * Using BeautifulSoup from bs4 module to remove the html tags. We have already removed the html tags with pattern \"64<\/_span_>...\", we will use get_text() to remove the html tags if there are any.\n* **Remove Stop Words**\n     * Remove the stopwords like \"a\", \"the\", \"I\" etc.\n* **Remove symbols and special characters**\n     * We will remove the special characters from our reviews like '#' ,'&' ,'@' etc.\n* **Tokenize**\n     * We will tokenize the words. We will split the sentences with spaces e.g \"I might come\" --> \"I\", \"might\", \"come\"\n* **Stemming**\n     * Remove the suffixes from the words to get the root form of the word e.g 'Wording' --> \"Word\"","14e4dd6c":"### Apply Multinomial Naive Bayes","a0deadef":"Data Pre-processing is a vital part in model building. **\"Garbage In Garbage Out\"**, we all have heard this statement. But what does it mean. It means if we feed in garbage in our data like missing values, and different features which doesn't have any predictive power and provides the same information in our model. Our model will be just making a random guess and it won't be efficient enough for us to use it for any predictions.","975cc231":" There are only 0.54 % values with  <\/span  type data. We can remove these from our dataset as we won't lose much information by removing them.","756487b9":"### Top 10 drugs with 1\/10 Rating","45069c9d":"* The top drugName is Levonorgestrel, which we had seen in description as well.\n* The top 3 drugName has count around 4000 and above. \n* Most of the drugName counts are around 1500 if we look at top 20","a0b325b5":"### Import all the necessary packages\nHere we have imported the basic packages that are required to do basic processing. Feel free to use any library that you think can be useful here.","90cc9f85":"# Drug Sentiment Analysis","0eb5cc5a":"We have got accuracy score of 75.8% by using NaiveBayes","def5bce7":"We have 2907 drugs with rating 10.","cdab3500":"### Plot the bottom 20 conditions","399a8c97":"Now the time is to plot some beautiful graphs and find some interesting insights from our Data. **Here your detective skills are needed so be ready and interrogate the data as much as you can ** ","c28150af":"### Check top 20 drugName","3239a426":"There are many drugs which can be used for multiple conditions. ","086ec4a5":"There are 215063 uniqueIds meaning that every record is unique.","d5f1c9b0":"There are 592 unique drugs for \"not \/ listed othe \"  values. There are 2 options  to deal with these values  \n1. Check the condition associated with the drugs and replace the values.\n2. We can drop the values as these only accounts for 0.27 % of total data. To save our time we will drop the nosiy data.","2932b2b3":"* usefulCount is positively-skewed.\n* Most of the usefulCounts are distributed between 0 and 200.\n* There are extreme outliers present in our usefulCounts. We either have to remove them or transform them.","2ba09865":"## Parameter Tuning\nTry different sets of parameters for RandomForestClassifier using RandomSearchCV and check which sets of parameters gives the best accuracy. *A task for you to try*","9b1c9e81":"### Plot top-20 drugs with rating 10","86049758":"We all know that we cannot pass raw text features in our model. We have to convert them into numeric values. We will use TfidfVectorizer to convert our reviews in numeric features.","ae947a15":"Levonorgestrel is used for 3 different conditions. \n* emergency contraception\n* birth control\n* abnormal uterine bleeding","f396ef8f":"**Following things can be noticed from the description**\n* Top **drugName** is **Levonorgestrel**, It will be intresting to see for what condition it is used.\n* Top **condition** is **Birth Control**.\n* Top **review** is just a single word \"Good\", but it has very small count - 39. May be lazy people like me have written that comment.\n* Most single day review came on 1-Mar-16, it will be interesting to investigate this date and see for which drugName and which conditions these reviews were for.","4e27d9fe":"**We expect that as the the conditions has increased. Drugs should have also increased. Let's check that out.**","6e04cfd0":"### Check number of uniqueIds to see if there's any duplicate record in our dataset","31d75014":"Merge the train and test data as there are no target labels. We will perform our EDA and Pre-processing on merged data. Then we will dive the data into 70 : 30 ratio for training and testing","73e05747":"As expected number of drugs has also increased in last three years.","9c60101c":"### Now we will create our target variable \"Sentiment\" from rating","5185e8ba":"## Building Model","87c463d3":"We only have null values in condition. We will drop the records with null values as it only accounts for 0.5 % of total data.","3746174a":"### Check the percentage of null values in each column","a273bd70":"##### Let's look at ''3 <_\/span_> user found this comment helpful'","cd5230a8":"We have built reviews_corpus which are the independent feature in our model. ","c420c36c":"### Checking Out The Data","1618b147":"Bottom-20 conditions just have single drugs. These are the rare conditions.","bd4cf350":"We just have null values in just 1 column i.e **condition** . We will leave the null values in that column for now as the null values are very small."}}