{"cell_type":{"856b5194":"code","969d7ce4":"code","2335984b":"code","06d7d057":"code","4d803550":"code","a9ad297d":"code","5bef5019":"code","3d5f63d3":"code","4d25884f":"code","391db48c":"code","10c11134":"code","113d44d2":"code","3669d738":"code","8db088be":"code","fa506f84":"code","07198c4c":"code","b2949696":"code","273eebda":"code","d0173199":"code","5ea13d89":"code","5ace5624":"code","5be3a87d":"code","26bc5a01":"code","ea8b1543":"code","2775a4cf":"code","7c7fb31c":"code","f36160f1":"code","27d20327":"code","e4258149":"code","d8223653":"code","57607721":"code","38aa5d27":"code","0a09d0c7":"code","66514af4":"code","41ba296f":"code","debe6aab":"code","f18bc406":"code","49c1d88e":"code","b231194d":"code","4d3c4a53":"code","2500e284":"code","8a3ee518":"code","d23fb80b":"code","3db3fac8":"code","4b432ecd":"code","d4721a2a":"code","f1ac5e64":"code","e2f48e90":"code","2810c42f":"code","7d97dfc5":"code","f08d4dbe":"code","205c38ea":"code","4c801504":"code","b9a478d4":"code","7f3e26db":"code","eb7ec7a7":"code","5475f56f":"code","bf837eb8":"code","831c4bb4":"code","faae881f":"code","6bc74567":"code","7e94ec9d":"code","2405c6bf":"code","5e04ab1a":"code","718ed7d2":"code","532e428d":"code","c0a120a6":"code","7abe7869":"code","184906ce":"code","e275c1cb":"code","f9371c53":"code","3186b6ef":"code","518f5abd":"code","63f7f988":"code","f94d7ca7":"code","cfabd77a":"code","f3646a03":"code","f6347e78":"code","b4119b8e":"code","128e12fd":"code","c520a2f7":"code","8684daf8":"code","3494a5ac":"code","0a247c93":"code","d8bfa394":"code","0f68302f":"code","9e153bd0":"code","7982a109":"code","7de758b7":"code","f12c37e7":"code","98fe16f8":"code","7876f8ae":"code","8613b4d8":"code","85383146":"code","3abcc66d":"code","ce6015d5":"code","f81c219e":"code","f6ef3078":"markdown","4d2f8d50":"markdown","22471a19":"markdown","0cde9345":"markdown","fdcae218":"markdown","f831e18e":"markdown","9361c2fe":"markdown","465a5c23":"markdown","e01f158d":"markdown","c275d221":"markdown"},"source":{"856b5194":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","969d7ce4":"from sklearn.feature_extraction.text import CountVectorizer\nimport re\n# Tutorial about Python regular expressions: https:\/\/pymotw.com\/2\/re\/ import string\nfrom nltk.corpus import stopwords\nimport matplotlib.pyplot as plt\nfrom tqdm import tqdm\nimport os\nimport nltk\nimport spacy\nimport random\nfrom spacy.util import compounding\nfrom spacy.util import minibatch\nimport string\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\nplt.rcParams['figure.figsize']=10,6\nplt.rcParams['axes.grid']=True\nplt.gray()\n\nuse_cuda = True\npd.set_option('display.max_columns', None)","2335984b":"train = pd.read_csv(\"\/kaggle\/input\/tweet-sentiment-extraction\/train.csv\") \ntest = pd.read_csv(\"\/kaggle\/input\/tweet-sentiment-extraction\/test.csv\") \nsample_submission = pd.read_csv(\"\/kaggle\/input\/tweet-sentiment-extraction\/sample_submission.csv\") ","06d7d057":"train.head()","4d803550":"test.head()","a9ad297d":"test.shape","5bef5019":"sample_submission.head()","3d5f63d3":"# Checking the shape of train and test data\nprint(train.shape)\nprint(test.shape)","4d25884f":"# Checking Missing value in the training set\nprint(train.isnull().sum())\n# Checking Missing Value in the testing set\nprint(test.isnull().sum())","391db48c":"# Cr\u00e9ation d'une fonction permettant de calculer le total de valeurs manquantes, le pourcentage et le type de \n ## chaque colonne \ndef missing_data(data):\n    total = data.isnull().sum()\n    percent = (data.isnull().sum()\/data.isnull().count()*100)\n    tt = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\n    types = []\n    for col in data.columns:\n        dtype = str(data[col].dtype)\n        types.append(dtype)\n    tt['Types'] = types\n    return(np.transpose(tt))","10c11134":"missing_data(train)","113d44d2":"missing_data(test)","3669d738":"percent_sentiment = train.groupby('sentiment').count()\npercent_sentiment['percent'] = 100*(percent_sentiment['text']\/train['sentiment'].count())\npercent_sentiment.reset_index(level=0, inplace=True)\npercent_sentiment","8db088be":"# Droping the row with missing values\ntrain.dropna(axis = 0, how ='any',inplace=True)","fa506f84":"# Positive tweet\nprint(\"Positive Tweet example :\",train[train['sentiment']=='positive']['text'].values[0])\n#negative_text\nprint(\"Negative Tweet example :\",train[train['sentiment']=='negative']['text'].values[0])\n#neutral_text\nprint(\"Neutral tweet example  :\",train[train['sentiment']=='neutral']['text'].values[0])","07198c4c":"# Distribution of the Sentiment Column\ntrain['sentiment'].value_counts()","b2949696":"# Train data \nsns.countplot(x=train['sentiment'],data=train)\nplt.show()","273eebda":"train['sentiment'].value_counts(normalize=True)","d0173199":"f,ax=plt.subplots(1,2,figsize=(13,5))\ntrain['sentiment'].value_counts().plot.pie(explode=[0,0.05,0.5],autopct='%1.1f%%',ax=ax[0],shadow=True)\nax[0].set_title('sentiment')\nax[0].set_ylabel('')\nsns.countplot('sentiment',data=train,ax=ax[1])\nax[1].set_title('sentiment')\nplt.show()","5ea13d89":"from nltk.stem.wordnet import WordNetLemmatizer\nfrom nltk.stem import PorterStemmer, SnowballStemmer\nfrom nltk import wordnet, pos_tag\nfrom nltk.tokenize import word_tokenize\nfrom nltk.util import ngrams\nfrom nltk.corpus import stopwords, wordnet as wn\nimport re\nimport string\n\n#Cleaning data\n\ndef clean_str(chaine):\n    chaine = re.sub(r\"[^A-Za-z0-9(),!?\\'\\`]\", \" \", chaine)     \n    chaine = re.sub(r\"\\'s\", \" \\'s\", chaine) \n    chaine = re.sub(r\"\\'ve\", \" \\'ve\", chaine) \n    chaine = re.sub(r\"n\\'t\", \" n\\'t\", chaine) \n    chaine = re.sub(r\"\\'re\", \" \\'re\", chaine) \n    chaine = re.sub(r\"\\'d\", \" \\'d\", chaine) \n    chaine = re.sub(r\"\\'ll\", \" \\'ll\", chaine) \n    chaine = re.sub(r\",\", \" , \", chaine) \n    chaine = re.sub(r\"!\", \" ! \", chaine) \n    chaine = re.sub(r\"\\(\", \" \\( \", chaine) \n    chaine = re.sub(r\"\\)\", \" \\) \", chaine) \n    chaine = re.sub(r\"\\?\", \" \\? \", chaine) \n    chaine = re.sub(r\"\\s{2,}\", \" \", chaine)\n    chaine = chaine.lower() #convert all text in lower case\n    chaine = chaine.replace(' +', ' ') # Remove double space\n    chaine = chaine.strip() # Remove trailing space at the beginning or end\n    chaine = chaine.replace('[^a-zA-Z]', ' ' )# Everything not a alphabet character replaced with a space\n    #words =  [word for word in chaine.split() if word not in [i for i in string.punctuation]] #Remove punctuations\n    words =  [word for word in chaine.split() if word.isalpha()] #droping numbers and punctuations\n    return ' '.join(words)\n\n#Tokenization and punctuation removing and stopwords\ndef tokeniZ_stopWords(chaine):\n    chaine = word_tokenize(chaine)\n    list_stopWords = set(stopwords.words('english'))\n    words = [word for word in chaine if word not in list_stopWords]\n    return words\n\n#Stemming \nps = PorterStemmer()\nsb = SnowballStemmer('english')\n\n#Lemmatization\ndef lemat_words(tokens_list):\n    from collections import defaultdict\n    tag_map = defaultdict(lambda : wn.NOUN)\n    tag_map['J'] = wn.ADJ\n    tag_map['V'] = wn.VERB\n    tag_map['R'] = wn.ADV\n    lemma_function = WordNetLemmatizer()\n    return [lemma_function.lemmatize(token, tag_map[tag[0]]) for token, tag in pos_tag(tokens_list)]\n    #for token, tag in pos_tag(tokens_list):\n     #   lemma = lemma_function.lemmatize(token, tag_map[tag[0]])\n\n# Define Ngrams function\ndef get_ngrams(text, n ):\n    n_grams = ngrams(word_tokenize(text), n)\n    return [ ' '.join(grams) for grams in n_grams]","5ace5624":"#Cleaning the train data \ntrain['text_clean'] = train['text'].apply(clean_str)\n\n#Tokenizing and stopwords removing\ntrain['tokeniZ_stopWords_text'] = train['text_clean'].apply(tokeniZ_stopWords)\n#Words Stemming\ntrain['stemming_text'] = [[ps.stem(word) for word in words] for words in train['tokeniZ_stopWords_text'] ]\ntrain['stemming_text_for_tfidf'] = [' '.join(words) for words in train['stemming_text']] \n\n#Words lemmatization\ntrain['lemmatize_text'] = train['tokeniZ_stopWords_text'].apply(lemat_words)\ntrain['lemmatize_text_for_tfidf'] = [' '.join(x) for x in train['lemmatize_text'] ]\n\n#Calcul longueur des commentaires\ntrain['text_lenght'] = train['text'].apply(len)\n\n#Calcul du nombre de ponctuation par question\nfrom string import punctuation\ntrain['number_punctuation_text'] = train['text'].apply(\n    lambda doc: len([word for word in str(doc) if word in punctuation])) \n\n#Number of unique words in the text\ntrain['number_of_Unique_words_text'] = train['text_clean'].apply([lambda x : len(set(str(x).split()))])\n\n#Number of stopwords in the text\nlist_stopWords = set(stopwords.words('english'))\ntrain['number_of_StopWords_text'] = train['text_clean'].apply(\n    lambda x : len([w for w in x.lower().split() if w in list_stopWords ]))\n\n\n#Number of upper case words\ntrain['number_of_uppercase_text'] = train['text'].apply(\n    lambda x : len([w for w in x.split() if w.isupper()]))\n\n\n#Average length of words in the text (whithout stop words)\ntrain['average_of_wordsLength_text'] = train['text_clean'].apply(\n    lambda x : np.mean([len(w) for w in x.split()]))\n\n#Number of words in the text\ntrain['number_of_words_text'] = train['text_clean'].apply([lambda x : len(str(x).split())])","5be3a87d":"\n#Cleaning the train data \ntrain['selected_text_clean'] = train['selected_text'].apply(clean_str)\n\n#Tokenizing and stopwords removing\ntrain['tokeniZ_stopWords_text'] = train['selected_text_clean'].apply(tokeniZ_stopWords)\n\n#Words Stemming\ntrain['stemming_selected_text'] = [[ps.stem(word) for word in words] for words in train['tokeniZ_stopWords_text'] ]\ntrain['stemming_selected_text_for_tfidf'] = [' '.join(words) for words in train['stemming_selected_text']] \n\n#Words lemmatization\ntrain['lemmatize_selected_text'] = train['tokeniZ_stopWords_text'].apply(lemat_words)\ntrain['lemmatize_selected_text_for_tfidf'] = [' '.join(x) for x in train['lemmatize_selected_text'] ]\n\n\n#Calcul longueur des commentaires\ntrain['selected_text_lenght'] = train['selected_text'].apply(len)\n\n#Calcul du nombre de ponctuation par question\nfrom string import punctuation\ntrain['number_punctuation_selected_text'] = train['selected_text'].apply(\n    lambda doc: len([word for word in str(doc) if word in punctuation])) \n\n#Number of unique words in the text\ntrain['number_of_Unique_words_selected_text'] = train['selected_text_clean'].apply([lambda x : len(set(str(x).split()))])\n\n#Number of stopwords in the text\nlist_stopWords = set(stopwords.words('english'))\ntrain['number_of_StopWords_selected_text'] = train['selected_text_clean'].apply(\n    lambda x : len([w for w in x.lower().split() if w in list_stopWords ]))\n\n\n#Number of upper case words\ntrain['number_of_uppercase_selected_text'] = train['selected_text'].apply(\n    lambda x : len([w for w in x.split() if w.isupper()]))\n\n\n#Average length of words in the text (whithout stop words)\ntrain['average_of_wordsLength_selected_text'] = train['selected_text_clean'].apply(\n    lambda x : np.mean([len(w) for w in x.split()]))\n\n#Number of words in the text\ntrain['number_of_words_selected_text'] = train['selected_text_clean'].apply([lambda x : len(str(x).split())])\n\n","26bc5a01":"# Let's create three separate dataframes for positive, neutral and negative sentiments. \n#This will help in analyzing the text statistics separately for separate polarities.\n\npositive = train[train['sentiment']=='positive']\nnegative = train[train['sentiment']=='negative']\nneutral = train[train['sentiment']=='neutral']","ea8b1543":"# Sentence length analysis\n\nfig, ax = plt.subplots(1, 3, figsize=(15, 5))\nplt.subplot(1, 3, 1)\nplt.hist(positive['text_lenght'],bins=50,color='g')\nplt.title('Positive Text Length Distribution')\nplt.xlabel('text_lenght')\nplt.ylabel('count')\n\n\nplt.subplot(1, 3, 2)\nplt.hist(negative['text_lenght'],bins=50,color='r')\nplt.title('Negative Text Length Distribution')\nplt.xlabel('text_lenght')\nplt.ylabel('count')\n\n\nplt.subplot(1, 3, 3)\nplt.hist(neutral['text_lenght'],bins=50,color='y')\nplt.title('Neutral Text Length Distribution')\nplt.xlabel('text_lenght')\nplt.ylabel('count')\nplt.show()","2775a4cf":"#source of code : https:\/\/medium.com\/@cristhianboujon\/how-to-list-the-most-common-words-from-text-corpus-using-scikit-learn-dad4d0cab41d\ndef get_top_n_words(corpus, n=None):\n    \"\"\"\n    List the top n words in a vocabulary according to occurrence in a text corpus.\n    \"\"\"\n    vec = CountVectorizer(stop_words = 'english').fit(corpus)\n    bag_of_words = vec.transform(corpus)\n    sum_words = bag_of_words.sum(axis=0) \n    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n    return words_freq[:n]","7c7fb31c":"#Distribution of top unigrams\npositive_unigrams = get_top_n_words(positive['text_clean'],20)\nnegative_unigrams = get_top_n_words(negative['text_clean'],20)\nneutral_unigrams = get_top_n_words(neutral['text_clean'],20)\n\ndf1 = pd.DataFrame(positive_unigrams, columns = ['Text' , 'count'])\ndf1.groupby('Text').sum()['count'].sort_values(ascending=True).plot(kind='barh',color='g')\nplt.ylabel('Count')\nplt.title('Top 20 unigrams in positve text')\nplt.show()\n\ndf2 = pd.DataFrame(negative_unigrams, columns = ['Text' , 'count'])\ndf2.groupby('Text').sum()['count'].sort_values(ascending=True).plot(kind='barh',color='red')\nplt.title('Top 20 unigram in Negative text')\nplt.show()\n\ndf3 = pd.DataFrame(neutral_unigrams, columns = ['Text' , 'count'])\ndf3.groupby('Text').sum()['count'].sort_values(ascending=True).plot(kind='barh',color='yellow')\nplt.title('Top 20 unigram in Neutral text')\nplt.show()","f36160f1":"def get_top_n_gram(corpus,ngram_range,n=None):\n    vec = CountVectorizer(ngram_range=ngram_range,stop_words = 'english').fit(corpus)\n    bag_of_words = vec.transform(corpus)\n    sum_words = bag_of_words.sum(axis=0) \n    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n    return words_freq[:n]","27d20327":"#Distribution of top Bigrams\npositive_bigrams = get_top_n_gram(positive['text_clean'],(2,2),20)\nnegative_bigrams = get_top_n_gram(negative['text_clean'],(2,2),20)\nneutral_bigrams = get_top_n_gram(neutral['text_clean'],(2,2),20)\n\ndf1 = pd.DataFrame(positive_bigrams, columns = ['Text' , 'count'])\ndf1.groupby('Text').sum()['count'].sort_values(ascending=True).plot(kind='barh',color='g')\nplt.ylabel('Count')\nplt.title('Top 20 Bigrams in positve text')\nplt.show()\n\ndf2 = pd.DataFrame(negative_bigrams, columns = ['Text' , 'count'])\ndf2.groupby('Text').sum()['count'].sort_values(ascending=True).plot(kind='barh',color='red')\nplt.title('Top 20 Bigram in Negative text')\nplt.show()\n\ndf3 = pd.DataFrame(neutral_bigrams, columns = ['Text' , 'count'])\ndf3.groupby('Text').sum()['count'].sort_values(ascending=True).plot(kind='barh',color='yellow')\nplt.title('Top 20 Bigram in Neutral text')\nplt.show()","e4258149":"# Finding top trigram\npositive_trigrams = get_top_n_gram(positive['text_clean'],(3,3),20)\nnegative_trigrams = get_top_n_gram(negative['text_clean'],(3,3),20)\nneutral_trigrams = get_top_n_gram(neutral['text_clean'],(3,3),20)\n\ndf1 = pd.DataFrame(positive_trigrams, columns = ['Text' , 'count'])\ndf1.groupby('Text').sum()['count'].sort_values(ascending=True).plot(kind='barh',color='g')\nplt.ylabel('Count')\nplt.title('Top 20 trigrams in positve text')\nplt.show()\n\ndf2 = pd.DataFrame(negative_trigrams, columns = ['Text' , 'count'])\ndf2.groupby('Text').sum()['count'].sort_values(ascending=True).plot(kind='barh',color='red')\nplt.title('Top 20 trigram in Negative text')\nplt.show()\n\ndf3 = pd.DataFrame(neutral_trigrams, columns = ['Text' , 'count'])\ndf3.groupby('Text').sum()['count'].sort_values(ascending=True).plot(kind='barh',color='yellow')\nplt.title('Top 20 trigram in Neutral text')\nplt.show()\n\n","d8223653":"#  Exploring the selected_text column\n\npositive_text = train[train['sentiment'] == 'positive']['selected_text']\nnegative_text = train[train['sentiment'] == 'negative']['selected_text']\nneutral_text = train[train['sentiment'] == 'neutral']['selected_text']","57607721":"# Positive text\nprint(\"Positive Text example :\",positive_text.values[0])\n#negative_text\nprint(\"Negative Tweet example :\",negative_text.values[0])\n#neutral_text\nprint(\"Neutral tweet example  :\",neutral_text.values[0])","38aa5d27":"# Preprocess Selected_text\n\npositive_text_clean = positive_text.apply(clean_str)\nnegative_text_clean = negative_text.apply(clean_str)\nneutral_text_clean = neutral_text.apply(clean_str)","0a09d0c7":"#source of code : https:\/\/medium.com\/@cristhianboujon\/how-to-list-the-most-common-words-from-text-corpus-using-scikit-learn-dad4d0cab41d\ndef get_top_n_words(corpus, n=None):\n    \"\"\"\n    List the top n words in a vocabulary according to occurrence in a text corpus.\n    \"\"\"\n    vec = CountVectorizer(stop_words = 'english').fit(corpus)\n    bag_of_words = vec.transform(corpus)\n    sum_words = bag_of_words.sum(axis=0) \n    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n    return words_freq[:n]","66514af4":"top_words_in_positive_text = get_top_n_words(positive_text_clean)\ntop_words_in_negative_text = get_top_n_words(negative_text_clean)\ntop_words_in_neutral_text = get_top_n_words(neutral_text_clean)\n\np1 = [x[0] for x in top_words_in_positive_text[:20]]\np2 = [x[1] for x in top_words_in_positive_text[:20]]\n\n\nn1 = [x[0] for x in top_words_in_negative_text[:20]]\nn2 = [x[1] for x in top_words_in_negative_text[:20]]\n\n\nnu1 = [x[0] for x in top_words_in_neutral_text[:20]]\nnu2 = [x[1] for x in top_words_in_neutral_text[:20]]","41ba296f":"# Top positive word\nsns.barplot(x=p1,y=p2,color = 'green')\nplt.xticks(rotation=45)\nplt.title('Top 20 Positive Word')\nplt.show()\n\nsns.barplot(x=n1,y=n2,color='red')\nplt.xticks(rotation=45)\nplt.title('Top 20 Negative Word')\nplt.show()\n\nsns.barplot(x=nu1,y=nu2,color='yellow')\nplt.xticks(rotation=45)\nplt.title('Top 20 Neutral Word')\nplt.show()","debe6aab":"#Wordclouds\n# Wordclouds to see which words contribute to which type of polarity.\n\nfrom wordcloud import WordCloud\nfig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=[30, 15])\nwordcloud1 = WordCloud( background_color='white',\n                        width=600,\n                        height=400).generate(\" \".join(positive_text_clean))\nax1.imshow(wordcloud1)\nax1.axis('off')\nax1.set_title('Positive text',fontsize=40);\n\nwordcloud2 = WordCloud( background_color='white',\n                        width=600,\n                        height=400).generate(\" \".join(negative_text_clean))\nax2.imshow(wordcloud2)\nax2.axis('off')\nax2.set_title('Negative text',fontsize=40);\n\nwordcloud3 = WordCloud( background_color='white',\n                        width=600,\n                        height=400).generate(\" \".join(neutral_text_clean))\nax3.imshow(wordcloud3)\nax3.axis('off')\nax3.set_title('Neutral text',fontsize=40)","f18bc406":"# https:\/\/www.kaggle.com\/ekhtiar\/unintended-eda-with-tutorial-notes\ndef generate_word_cloud(df_data, text_col):\n    # convert stop words to sets as required by the wordcloud library\n    stop_words = set(stopwords.words(\"english\"))\n    \n    data_neutral = \" \".join(df_data.loc[df_data[\"sentiment\"]==\"neutral\", text_col].map(lambda x: str(x).lower()))\n    data_positive = \" \".join(df_data.loc[df_data[\"sentiment\"]==\"positive\", text_col].map(lambda x: str(x).lower()))\n    data_negative = \" \".join(df_data.loc[df_data[\"sentiment\"]==\"negative\", text_col].map(lambda x: str(x).lower()))\n\n    wc_neutral = WordCloud(max_font_size=100, max_words=100, background_color=\"white\", stopwords=stop_words).generate(data_neutral)\n    wc_positive = WordCloud(max_font_size=100, max_words=100, background_color=\"white\", stopwords=stop_words).generate(data_positive)\n    wc_negative = WordCloud(max_font_size=100, max_words=100, background_color=\"white\", stopwords=stop_words).generate(data_negative)\n\n    # draw the two wordclouds side by side using subplot\n    fig, ax = plt.subplots(1, 3, figsize=(20, 20))\n    ax[0].set_title(\"Neutral Wordcloud\" , fontsize=10)\n    ax[0].imshow(wc_neutral, interpolation=\"bilinear\")\n    ax[0].axis(\"off\")\n    \n    ax[1].set_title(\"Positive Wordcloud\", fontsize=10)\n    ax[1].imshow(wc_positive, interpolation=\"bilinear\")\n    ax[1].axis(\"off\")\n    \n    ax[2].set_title(\"Negative Wordcloud\", fontsize=10)\n    ax[2].imshow(wc_negative, interpolation=\"bilinear\")\n    ax[2].axis(\"off\")\n    plt.show()\n    \n    return [wc_neutral, wc_positive, wc_negative]","49c1d88e":"train_text_wc = generate_word_cloud(train, \"text\")\n","b231194d":"train_sel_text_wc = generate_word_cloud(train, \"selected_text\")","4d3c4a53":"train_text_wc = generate_word_cloud(train, \"text\")","2500e284":"missing_data(train)","8a3ee518":"train['Target'] = train['sentiment'].apply(lambda x: 2 if x == 'positive' else 1 if x == 'neutral' else 0)\ntrain.head()","d23fb80b":"percent_target = train.groupby('Target').count()\npercent_target['percent'] = 100*(percent_target['text']\/train['Target'].count())\npercent_target.reset_index(level=0, inplace=True)\npercent_target","3db3fac8":"train.head(2)","4b432ecd":"train.tail()","d4721a2a":"train[['text_lenght', 'number_punctuation_text', 'number_of_words_text',\n       'number_of_Unique_words_text', 'number_of_StopWords_text', 'number_of_uppercase_text',\n       'average_of_wordsLength_text']].sample(5)","f1ac5e64":"list_var=['text_lenght', 'number_punctuation_text', 'number_of_words_text',\n       'number_of_Unique_words_text', 'number_of_StopWords_text', 'number_of_uppercase_text','average_of_wordsLength_text']\ndef var_hist_global(df,X='Target',Y=list_var, Title='Features Engineering - Histograms', KDE=False):\n    fig, ((ax1, ax2),(ax3,ax4),(ax5,ax6),(ax6,ax7)) = plt.subplots(4, 2 ,figsize=(14,16))#, sharey=True )\n    aX = [ax1, ax2,ax3,ax4,ax5,ax6,ax6,ax7]\n    for i in range(len(list_var)):   \n        sns.distplot( df[list_var[i]][df[X]== 1 ].dropna(), label=\"Neutral\" , ax= aX[i], kde= KDE , color = 'red')           \n        sns.distplot( df[list_var[i]][df[X]== 0 ].dropna(), label=\"Negative\", ax= aX[i], kde= KDE , color = \"olive\")\n        sns.distplot( df[list_var[i]][df[X]== 2 ].dropna(), label=\"Positive\", ax= aX[i], kde= KDE , color = \"black\")\n    plt.legend()\n    plt.title(Title)\n    #plt.show()\n    plt.savefig(\"Features_Engineering_Histograms\")\n    \nvar_hist_global(df=train,X='Target',Y=list_var, KDE=True)","e2f48e90":"# Calculate number of obs per group & median to position labels\nlist_var = ['text_lenght', 'number_of_Unique_words_text', 'number_of_StopWords_text']\ndef violin_boxplott(df,X='Target',Y=list_var, Title='Features Engineering - Box plot'): \n    fig, (ax1, ax2 ,ax3) = plt.subplots(1,3 ,figsize=(14,8))#, sharey=True )\n    medians = train.groupby(['Target'])['text_lenght', 'number_of_Unique_words_text', 'number_of_StopWords_text'].median().values\n \n    sns.boxplot( y=list_var[0],  x=X , data = df, ax= ax1 , palette=['olive','red'])\n    sns.boxplot( y=list_var[1],  x=X , data = df, ax= ax2 , palette=['olive','red'])\n    sns.boxplot( y=list_var[2],  x=X , data = df, ax= ax3 , palette=['olive','red'])\n    #plt.title(Title)\n    plt.savefig(\"Features_Engineering_Boxplot\")\nviolin_boxplott(df=train)","2810c42f":"#Word2Vec with preprocessiong questions (without stopwords) \nfrom gensim.models.doc2vec import Doc2Vec, TaggedDocument\n\nd2v_training_data = []\nfor i, doc in enumerate(train['stemming_selected_text']):\n    d2v_training_data.append(TaggedDocument(words=doc,tags=[i]))\n\n# ========== learning doc embeddings with doc2vec ==========\n\n# PV stands for 'Paragraph Vector'\n# PV-DBOW (distributed bag-of-words) dm=0\n\nd2v = Doc2Vec(d2v_training_data, vector_size=300, window=10, alpha=0.1, min_alpha=1e-4, dm=0, negative=1, epochs=10, min_count=2, workers=4)\nd2v_vecs = np.zeros((len(train['stemming_selected_text']), 300))\nfor i in range(len(train['stemming_selected_text'])):\n    d2v_vecs[i,:] = d2v.docvecs[i]","7d97dfc5":"#Word2Vec with lemmatize words\nd2v_training_data = []\nfor i, doc in enumerate(train['lemmatize_selected_text']):\n    d2v_training_data.append(TaggedDocument(words=doc,tags=[i]))\n\n# ========== learning doc embeddings with doc2vec ==========\n\n# PV stands for 'Paragraph Vector'\n# PV-DBOW (distributed bag-of-words) dm=0\n\nd2v = Doc2Vec(d2v_training_data, vector_size=200, window=5, alpha=0.1, min_alpha=1e-4, \n              dm=0, negative=1, epochs=10, min_count=2, workers=4)\nd2v_vecs_bigram = np.zeros((len(train['lemmatize_selected_text']), 200))\nfor i in range(len(train['lemmatize_selected_text'])):\n    d2v_vecs_bigram[i,:] = d2v.docvecs[i]","f08d4dbe":"test.head()","205c38ea":"missing_data(test)","4c801504":"# Test data \nsns.countplot(x=test['sentiment'],data=train)\nplt.show()","b9a478d4":"percent_sentiment = test.groupby('sentiment').count()\npercent_sentiment['percent'] = 100*(percent_sentiment['text']\/train['sentiment'].count())\npercent_sentiment.reset_index(level=0, inplace=True)\npercent_sentiment","7f3e26db":"# Positive tweet\nprint(\"Positive Tweet example :\",test[test['sentiment']=='positive']['text'].values[0])\n#negative_text\nprint(\"Negative Tweet example :\",test[test['sentiment']=='negative']['text'].values[0])\n#neutral_text\nprint(\"Neutral tweet example  :\",test[test['sentiment']=='neutral']['text'].values[0])","eb7ec7a7":"# Distribution of the Sentiment Column\ntest['sentiment'].value_counts()","5475f56f":"#Cleaning the train data \ntest['text_clean_test'] = test['text'].apply(clean_str)\n\n#Tokenizing and stopwords removing\ntest['tokeniZ_stopWords_text_test'] = test['text_clean_test'].apply(tokeniZ_stopWords)\n#Words Stemming\ntest['stemming_text_test'] = [[ps.stem(word) for word in words] for words in test['tokeniZ_stopWords_text_test'] ]\ntest['stemming_text_for_tfidf_test'] = [' '.join(words) for words in test['stemming_text_test']] \n\n#Words lemmatization\ntest['lemmatize_text_test'] = test['tokeniZ_stopWords_text_test'].apply(lemat_words)\ntest['lemmatize_text_for_tfidf_test'] = [' '.join(x) for x in test['lemmatize_text_test'] ]\n\n#Calcul longueur des commentaires\ntest['text_lenght_test'] = test['text'].apply(len)\n\n#Calcul du nombre de ponctuation par question\nfrom string import punctuation\ntest['number_punctuation_text_test'] = test['text'].apply(\n    lambda doc: len([word for word in str(doc) if word in punctuation])) \n\n#Number of unique words in the text\ntest['number_of_Unique_words_text_test'] = test['text_clean_test'].apply([lambda x : len(set(str(x).split()))])\ntest\n#Number of stopwords in the text\nlist_stopWords = set(stopwords.words('english'))\ntest['number_of_StopWords_text_test'] = test['text_clean_test'].apply(\n    lambda x : len([w for w in x.lower().split() if w in list_stopWords ]))\n\n\n#Number of upper case words\ntest['number_of_uppercase_text_test'] = test['text'].apply(\n    lambda x : len([w for w in x.split() if w.isupper()]))\n\n\n#Average length of words in the text (whithout stop words)\ntest['average_of_wordsLength_text_test'] = test['text_clean_test'].apply(\n    lambda x : np.mean([len(w) for w in x.split()]))\n\n#Number of words in the text\ntest['number_of_words_text_test'] = test['text_clean_test'].apply([lambda x : len(str(x).split())])","bf837eb8":"# Let's create three separate dataframes for positive, neutral and negative sentiments. \n#This will help in analyzing the text statistics separately for separate polarities.\n\npositive_test = test[test['sentiment']=='positive']\nnegative_test = test[test['sentiment']=='negative']\nneutral_test = test[test['sentiment']=='neutral']","831c4bb4":"# Sentence length analysis\n\nfig, ax = plt.subplots(1, 3, figsize=(15, 5))\nplt.subplot(1, 3, 1)\nplt.hist(positive_test['text_lenght_test'],bins=50,color='g')\nplt.title('Positive Text Length Distribution test data')\nplt.xlabel('text_lenght_test')\nplt.ylabel('count')\n\n\nplt.subplot(1, 3, 2)\nplt.hist(negative_test['text_lenght_test'],bins=50,color='r')\nplt.title('Negative Text Length Distribution  test data')\nplt.xlabel('text_lenght_test')\nplt.ylabel('count')\n\n\nplt.subplot(1, 3, 3)\nplt.hist(neutral_test['text_lenght_test'],bins=50,color='y')\nplt.title('Neutral Text Length Distribution  test data')\nplt.xlabel('text_lenght_test')\nplt.ylabel('count')\nplt.show()","faae881f":"#Distribution of top unigrams\npositive_test_unigrams = get_top_n_words(positive_test['text_clean_test'],20)\nnegative_test_unigrams = get_top_n_words(negative_test['text_clean_test'],20)\nneutral_test_unigrams = get_top_n_words(neutral_test['text_clean_test'],20)\n\ndf1 = pd.DataFrame(positive_test_unigrams, columns = ['Text' , 'count'])\ndf1.groupby('Text').sum()['count'].sort_values(ascending=True).plot(kind='barh',color='g')\nplt.ylabel('Count')\nplt.title('Top 20 unigrams in positve text')\nplt.show()\n\ndf2 = pd.DataFrame(negative_test_unigrams, columns = ['Text' , 'count'])\ndf2.groupby('Text').sum()['count'].sort_values(ascending=True).plot(kind='barh',color='red')\nplt.title('Top 20 unigram in Negative text')\nplt.show()\n\ndf3 = pd.DataFrame(neutral_test_unigrams, columns = ['Text' , 'count'])\ndf3.groupby('Text').sum()['count'].sort_values(ascending=True).plot(kind='barh',color='yellow')\nplt.title('Top 20 unigram in Neutral text')\nplt.show()","6bc74567":"#  Exploring the selected_text column\n\npositive_text_test = test[test['sentiment'] == 'positive']['text']\nnegative_text_test = test[test['sentiment'] == 'negative']['text']\nneutral_text_test = test[test['sentiment'] == 'neutral']['text']","7e94ec9d":"# Preprocess Selected_text\n\npositive_text_clean_test = positive_text_test.apply(clean_str)\nnegative_text_clean_test = negative_text_test.apply(clean_str)\nneutral_text_clean_test = neutral_text_test.apply(clean_str)","2405c6bf":"top_words_in_positive_text_test = get_top_n_words(positive_text_clean_test)\ntop_words_in_negative_text_test = get_top_n_words(negative_text_clean_test)\ntop_words_in_neutral_text_test = get_top_n_words(neutral_text_clean_test)\n\np_test1 = [x[0] for x in top_words_in_positive_text_test[:20]]\np_test2 = [x[1] for x in top_words_in_positive_text_test[:20]]\n\n\nn_test1 = [x[0] for x in top_words_in_negative_text_test[:20]]\nn_test2 = [x[1] for x in top_words_in_negative_text_test[:20]]\n\n\nnu_test1 = [x[0] for x in top_words_in_neutral_text_test[:20]]\nnu_test2 = [x[1] for x in top_words_in_neutral_text_test[:20]]","5e04ab1a":"# Top positive word\nsns.barplot(x=p_test1,y=p_test2,color = 'green')\nplt.xticks(rotation=45)\nplt.title('Top 20 Positive Word')\nplt.show()\n\nsns.barplot(x=n_test1,y=n_test2,color='red')\nplt.xticks(rotation=45)\nplt.title('Top 20 Negative Word')\nplt.show()\n\nsns.barplot(x=nu_test1,y=nu_test2,color='yellow')\nplt.xticks(rotation=45)\nplt.title('Top 20 Neutral Word')\nplt.show()","718ed7d2":"#Wordclouds\n# Wordclouds to see which words contribute to which type of polarity.\n\nfrom wordcloud import WordCloud\nfig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=[30, 15])\nwordcloud1 = WordCloud( background_color='white',\n                        width=600,\n                        height=400).generate(\" \".join(positive_text_clean_test))\nax1.imshow(wordcloud1)\nax1.axis('off')\nax1.set_title('Positive text',fontsize=40);\n\nwordcloud2 = WordCloud( background_color='white',\n                        width=600,\n                        height=400).generate(\" \".join(negative_text_clean_test))\nax2.imshow(wordcloud2)\nax2.axis('off')\nax2.set_title('Negative text',fontsize=40);\n\nwordcloud3 = WordCloud( background_color='white',\n                        width=600,\n                        height=400).generate(\" \".join(neutral_text_clean_test))\nax3.imshow(wordcloud3)\nax3.axis('off')\nax3.set_title('Neutral text',fontsize=40)","532e428d":"test_text_wc = generate_word_cloud(test, \"text\")\n\n","c0a120a6":"test.head()","7abe7869":"test['Target'] = test['sentiment'].apply(lambda x: 2 if x == 'positive' else 1 if x == 'neutral' else 0)\ntest.head()","184906ce":"test[['text_lenght_test', 'number_punctuation_text_test', 'number_of_words_text_test',\n       'number_of_Unique_words_text_test', 'number_of_StopWords_text_test', 'number_of_uppercase_text_test',\n       'average_of_wordsLength_text_test']].sample(5)","e275c1cb":"list_var=['text_lenght_test', 'number_punctuation_text_test', 'number_of_words_text_test',\n       'number_of_Unique_words_text_test', 'number_of_StopWords_text_test', 'number_of_uppercase_text_test',\n       'average_of_wordsLength_text_test']\n\n    \nvar_hist_global(df=test,X='Target',Y=list_var, KDE=True)","f9371c53":"test.columns","3186b6ef":"X_train =  train[['text_clean', 'stemming_text_for_tfidf', 'lemmatize_text_for_tfidf','tokeniZ_stopWords_text', 'stemming_text', 'lemmatize_text',\n                             'text_lenght', 'number_punctuation_text', 'number_of_StopWords_text', 'number_of_Unique_words_text', 'number_of_uppercase_text','average_of_wordsLength_text']]\n    \n\n\n\ny_train = train['Target']\n\nX_test = test[['text_clean_test','stemming_text_for_tfidf_test', 'lemmatize_text_for_tfidf_test', 'stemming_text_test', 'lemmatize_text_test', 'tokeniZ_stopWords_text_test', \n               'text_lenght_test', 'number_punctuation_text_test','number_of_Unique_words_text_test', 'number_of_StopWords_text_test', \n              'number_of_uppercase_text_test','average_of_wordsLength_text_test']]\n\ny_test = test['Target']\n\nX_train.shape, X_test.shape, y_train.shape, y_test.shape","518f5abd":"from sklearn.feature_extraction.text import TfidfVectorizer\ntfidf_vectorizer = TfidfVectorizer(  ngram_range=(1,1), \n                                     analyzer='word',\n                                     stop_words='english', \n                                     lowercase=True, \n                                     max_df=0.9, # remove too frequent words\n                                     min_df=10, # remove too rare words\n                                     max_features = None, # max words in vocabulary, will keep most frequent words\n                                     binary=False #If True, all non zero counts are set to 1. This is useful for discrete probabilistic models that model binary events rather than integer counts.\n                                  )\n\n\n\n#Stemmed questions vectorzation\nX_text_tfidf_vectorizer_train = tfidf_vectorizer.fit_transform(X_train['stemming_text_for_tfidf'])\nX_text_tfidf_vectorizer_test = tfidf_vectorizer.transform(X_test['stemming_text_for_tfidf_test'])\n\n#Lemmentized questions vectorization\nX_text_tfidf_Lem_vect_train = tfidf_vectorizer.fit_transform(X_train['lemmatize_text_for_tfidf'])\nX_test_tfidf_Lem_vect_test = tfidf_vectorizer.transform(X_test['lemmatize_text_for_tfidf_test'])\n\n\n#bigram text vectorization\nbigram_vectorizer = TfidfVectorizer(  ngram_range=(1,2), \n                                     analyzer='word',\n                                     stop_words='english', \n                                     lowercase=True, \n                                     max_df=0.9, # remove too frequent words\n                                     min_df=10, # remove too rare words\n                                     max_features = None, # max words in vocabulary, will keep most frequent words\n                                     binary=False #If True, all non zero counts are set to 1. This is useful for discrete probabilistic models that model binary events rather than integer counts.\n                                  )\nX_text_bigram_vectorizer_train = bigram_vectorizer.fit_transform(X_train['stemming_text_for_tfidf'])\nX_text_bigram_vectorizer_test = bigram_vectorizer.transform(X_test['lemmatize_text_for_tfidf_test'])\n\n\n#T3gram questions vectorization\nt3gram_vectorizer = TfidfVectorizer(  ngram_range=(1,4), \n                                     analyzer='word',\n                                     stop_words='english', \n                                     lowercase=True, \n                                     max_df=0.9, # remove too frequent words\n                                     min_df=10, # remove too rare words\n                                     max_features = None, # max words in vocabulary, will keep most frequent words\n                                     binary=False #If True, all non zero counts are set to 1. This is useful for discrete probabilistic models that model binary events rather than integer counts.\n                                  )\nX_text_t3gram_vectorizer_train = t3gram_vectorizer.fit_transform(X_train['stemming_text_for_tfidf'])\nX_text_t3gram_vectorizer_test  = t3gram_vectorizer.transform(X_test['stemming_text_for_tfidf_test'])\n\n#Range single word to t3gram text vectorization\nst3gram_vectorizer = TfidfVectorizer(  ngram_range=(1,3), \n                                     analyzer='word',\n                                     stop_words='english', \n                                     lowercase=True, \n                                     max_df=0.9, # remove too frequent words\n                                     min_df=10, # remove too rare words\n                                     max_features = None, # max words in vocabulary, will keep most frequent words\n                                     binary=False #If True, all non zero counts are set to 1. This is useful for discrete probabilistic models that model binary events rather than integer counts.\n                                  )\nX_text_Singt3gram_vectorizer_train = st3gram_vectorizer.fit_transform(X_train['stemming_text_for_tfidf'])\nX_text_Singt3gram_vectorizer_test  = st3gram_vectorizer.transform(X_test['stemming_text_for_tfidf_test'])\n","63f7f988":"selected_text_tfidf_vectorizer_train = tfidf_vectorizer.fit_transform(train['stemming_selected_text_for_tfidf'])\n\nselected_text_tfidf_Lem_vect_train = tfidf_vectorizer.fit_transform(train['lemmatize_selected_text_for_tfidf'])\n\nselected_text_bigram_vectorizer_train = bigram_vectorizer.fit_transform(train['stemming_selected_text_for_tfidf'])\n\nselected_text_t3gram_vectorizer_train = t3gram_vectorizer.fit_transform(train['stemming_selected_text_for_tfidf'])\nselected_text_Singt3gram_vectorizer_train = st3gram_vectorizer.fit_transform(train['stemming_selected_text_for_tfidf'])\n\n","f94d7ca7":"#Word2Vec with preprocessiong text (without stopwords) \nfrom gensim.models.doc2vec import Doc2Vec, TaggedDocument\n\nd2v_training_data = []\nfor i, doc in enumerate(X_train['stemming_text']):\n    d2v_training_data.append(TaggedDocument(words=doc,tags=[i]))\n\n# ========== learning doc embeddings with doc2vec ==========\n\n# PV stands for 'Paragraph Vector'\n# PV-DBOW (distributed bag-of-words) dm=0\n\nd2v = Doc2Vec(d2v_training_data, vector_size=300, window=10, alpha=0.1, min_alpha=1e-4, dm=0, negative=1, epochs=10, min_count=2, workers=4)\nd2v_vecs = np.zeros((len(X_train['stemming_text']), 300))\nfor i in range(len(X_train['stemming_text'])):\n    d2v_vecs[i,:] = d2v.docvecs[i]\n    \nd2v_test = np.zeros((len(X_test['stemming_text_test']), 300))\nfor i in range(len(X_test['stemming_text_test'])):\n    d2v_test[i,:] = d2v.infer_vector(X_test['stemming_text_test'].iloc[i])\n    \n","cfabd77a":"#Word2Vec with lemmatize words\nd2v_training_data = []\nfor i, doc in enumerate(X_train['lemmatize_text']):\n    d2v_training_data.append(TaggedDocument(words=doc,tags=[i]))\n\n# ========== learning doc embeddings with doc2vec ==========\n\n# PV stands for 'Paragraph Vector'\n# PV-DBOW (distributed bag-of-words) dm=0\n\nd2v = Doc2Vec(d2v_training_data, vector_size=200, window=5, alpha=0.1, min_alpha=1e-4, \n              dm=0, negative=1, epochs=10, min_count=2, workers=4)\nd2v_vecs_bigram = np.zeros((len(X_train['lemmatize_text']), 200))\nfor i in range(len(X_train['lemmatize_text'])):\n    d2v_vecs_bigram[i,:] = d2v.docvecs[i]\n    \nd2v_test_bigram = np.zeros((len(X_test['lemmatize_text_test']), 200))\nfor i in range(len(X_test['lemmatize_text_test'])):\n    d2v_test_bigram[i,:] = d2v.infer_vector(X_test['lemmatize_text_test'].iloc[i])\n    \n","f3646a03":"X_train","f6347e78":"from sklearn.feature_selection import SelectKBest, chi2, f_classif, mutual_info_classif, SelectPercentile\nfrom sklearn.pipeline import Pipeline\n\nfeatures = SelectKBest(mutual_info_classif,k=2).fit(X_train[['text_lenght', 'number_punctuation_text','number_of_Unique_words_text', 'number_of_StopWords_text', \n                                                             'number_of_uppercase_text','average_of_wordsLength_text']].fillna(0),y_train)\nindependance_test = np.zeros((6,2))\nfor idx,i in enumerate(['text_lenght', 'number_punctuation_text', 'number_of_Unique_words_text', 'number_of_StopWords_text', 'number_of_uppercase_text',\n                        'average_of_wordsLength_text']):\n    #independance_test[idx,0]= features.pvalues_[idx]\n    independance_test[idx,1]= features.scores_[idx]\n    #print (i,features.pvalues_[idx],features.scores_[idx])\n    #print('%s  %s'%(i,features.scores_[idx]))\n\n    \n    \nlist_var=['text_lenght', 'number_punctuation_text','number_of_Unique_words_text', 'number_of_StopWords_text', 'number_of_uppercase_text',\n          'average_of_wordsLength_text']\nindependance_df = pd.DataFrame({'Variables': list_var, 'p_values': independance_test[:,0], 'MI': independance_test[:,1]},index=None)\nindependance_df","b4119b8e":"plt.figure(figsize=(12, 10))\n_ = sns.heatmap(train[['text_lenght', 'number_punctuation_text','number_of_Unique_words_text', 'number_of_StopWords_text', 'number_of_uppercase_text','average_of_wordsLength_text']].corr()\n                ,cmap=\"YlGnBu\", annot=True, fmt=\".2f\")\nplt.savefig(\"Correlation Matrice\")\nplt.show()","128e12fd":"#!pip install hpelm","c520a2f7":"from sklearn import svm ","8684daf8":"from sklearn.model_selection import cross_val_score, StratifiedShuffleSplit, KFold\nrandom_state = 42\nkf = KFold(n_splits=2,random_state=random_state)\nn_iter= 50","3494a5ac":"from sklearn.neural_network import MLPClassifier\nModel_final_MLPClassifier = MLPClassifier(random_state=random_state).fit(X_text_tfidf_vectorizer_train,y_train)","0a247c93":"## Predictions \n\nPredictions = Model_final_MLPClassifier.predict(X_text_tfidf_vectorizer_test)","d8bfa394":"sample_submission","0f68302f":"sample_submission['selected_text'] = Predictions\nsample_submission['selected_text'] = sample_submission['selected_text'].apply(lambda x: x.replace('!!!!', '!') if len(x.split())==1 else x)\nsample_submission['selected_text'] = sample_submission['selected_text'].apply(lambda x: x.replace('..', '.') if len(x.split())==1 else x)\nsample_submission['selected_text'] = sample_submission['selected_text'].apply(lambda x: x.replace('...', '.') if len(x.split())==1 else x)\nsample_submission.to_csv('submission.csv', index=False)\nsample_submission.head()","9e153bd0":"sentiment_list = { 0: 'negative', \n                  2 : 'positive', \n                  1: 'neutral'}","7982a109":"#sentiment_list","7de758b7":"\nsample_submission['Sentiment_pred'] = sample_submission['Sentiment_preds'].map(sentiment_list)\nsample_submission.head()","f12c37e7":"sample_submission['text2'] = sample_submission[\"text\"].apply(lambda x: x.split())\nsample_submission","98fe16f8":"text2 = sample_submission['text2']\ntext2","7876f8ae":"text2 = [l[-int(Predictions.tolist()[ind]):] for ind, l in enumerate(text2)]","8613b4d8":"text2[:5]","85383146":"sample_submission['text22'] = text2\nsample_submission.head()","3abcc66d":"sample_submission['selected_text'] = sample_submission[\"text22\"].apply(lambda x: \" \".join(x))\nsample_submission","ce6015d5":"submission   = sample_submission[['textID', \"selected_text\"]]\nsubmission","f81c219e":"submission.to_csv('submission.csv', index=False)","f6ef3078":"## Prediction ","4d2f8d50":"## Features engineering for training data ","22471a19":"# Vectorization \n## Text Tf-Idf Vectorizer training data ","0cde9345":"## Target variable creating ","fdcae218":"## Testing data ","f831e18e":"## Text Word embedding - Doc2Vec ","9361c2fe":"## text","465a5c23":"## selected_text","e01f158d":"## Modelisation ","c275d221":"## Feature selection "}}