{"cell_type":{"6bbdb460":"code","35f4564b":"code","5ff25c79":"code","44e05d7b":"code","32177341":"code","91d97b32":"code","cda6cebb":"code","88f20cf0":"code","18856778":"code","be12e9d2":"code","35f1d9b2":"code","88333a08":"code","b1ce1eff":"code","53f31e0b":"code","8bbfa018":"code","f3841e70":"code","fedba626":"code","41b613bc":"code","367cc515":"code","8cd0810a":"code","2dfbb251":"code","27832685":"code","a7e15705":"code","62d31bfd":"code","ef628cc6":"code","d4ce0425":"code","368ff1ba":"code","9a0465c2":"code","4169dfaf":"code","8d4a4e37":"code","62f3ff33":"code","473c664f":"code","773126db":"code","9b7e994e":"code","5cd13114":"code","c2c89c24":"code","984190d1":"code","111e1b9c":"code","cc30fc64":"code","a02db670":"code","7a9c275a":"code","68c91212":"code","107ae651":"code","39d83091":"code","52b3e773":"code","5f86b246":"code","c4ba9e18":"code","fe2a4b9a":"code","122bf705":"code","60635cef":"code","b963ecc7":"code","d2717528":"code","6508949e":"code","4041e307":"markdown","13c973d2":"markdown","9e0bd4b8":"markdown","9c5fc1da":"markdown","75896610":"markdown","1450916d":"markdown","004b9657":"markdown","64b1929c":"markdown","ee71b996":"markdown","d9accc10":"markdown","6f7d171f":"markdown","9494ffe2":"markdown","01c71bc6":"markdown","2bcbfea9":"markdown","6f208122":"markdown","31d15e52":"markdown","3e7b30a4":"markdown","54bdd5a5":"markdown","baefb502":"markdown","6f9aa182":"markdown","b320f2b1":"markdown","c87b7fd4":"markdown","8c6df3fa":"markdown","00214c1d":"markdown","e28f0469":"markdown","5f64a6b3":"markdown","c68ab3ca":"markdown","a3e93eb2":"markdown","24e1ad8f":"markdown","88689726":"markdown","2d036add":"markdown","ac2dbe58":"markdown"},"source":{"6bbdb460":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport tensorflow as tf\nfrom tensorflow import keras","35f4564b":"mnist = tf.keras.datasets.fashion_mnist\n(training_images, training_labels), (test_images, test_labels) = mnist.load_data()","5ff25c79":"training_images=training_images.reshape(60000, 28, 28, 1)\ntraining_images=training_images \/ 255.0\ntest_images = test_images.reshape(10000, 28, 28, 1)\ntest_images=test_images\/255.0","44e05d7b":"def plot(h):\n    plt.style.use('seaborn-darkgrid')\n\n    epochs = h.epoch\n\n    acc = h.history['accuracy']\n    val_acc = h.history['val_accuracy']\n\n    loss = h.history['loss']\n    val_loss = h.history['val_loss']\n\n    fig = plt.figure(figsize = (6,4),dpi = 100)\n    ax = fig.add_axes([1,1,1,1])\n\n    ax.plot(epochs,acc,label = 'Accuracy')\n    ax.plot(epochs,val_acc,label = 'Val Accuracy')\n\n    ax.plot(epochs,loss,label = 'Loss')\n    ax.plot(epochs,val_loss,label = 'Val Loss')\n    \n    ax.set(xlabel = 'Number of Epochs',ylabel = 'Measure',title = 'Learning Curve')\n\n    ax.legend()","32177341":"#Create Model\nmodel = tf.keras.models.Sequential([\n                                    tf.keras.layers.Conv2D(32, (3,3), activation='relu', input_shape=(28, 28, 1)),\n                                    tf.keras.layers.MaxPooling2D(2, 2),\n\n                                    tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n                                    tf.keras.layers.MaxPooling2D(2, 2),\n\n                                    tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n                                    tf.keras.layers.MaxPooling2D(2,2),\n  \n                                    tf.keras.layers.Flatten(),\n                                    tf.keras.layers.Dense(256, activation='relu'),                                    \n                                    tf.keras.layers.Dense(128, activation='relu'),\n                                    tf.keras.layers.Dense(10, activation='softmax')\n])\n\n#Define Optimizer\nsgd = tf.keras.optimizers.SGD(lr = 0.001)\n\n#Compile Model\nmodel.compile(loss = 'sparse_categorical_crossentropy',\n              optimizer = sgd,\n              metrics = ['accuracy'])","91d97b32":"tf.keras.utils.plot_model(\n    model, to_file='model.png', show_shapes=False,dpi=100)","cda6cebb":"#Train Model\nh = model.fit(training_images,training_labels,\n          epochs=25,batch_size = 32,steps_per_epoch = 60000\/\/32,\n          validation_data = (test_images,test_labels),validation_steps = 10000\/\/32)","88f20cf0":"plot(h)","18856778":"model = tf.keras.models.Sequential([\n                                    tf.keras.layers.Conv2D(32, (3,3), activation='relu',kernel_initializer = 'he_normal', input_shape=(28, 28, 1)),\n                                    tf.keras.layers.MaxPooling2D(2, 2),\n\n                                    tf.keras.layers.Conv2D(64, (3,3), activation='relu',kernel_initializer = 'he_normal'),\n                                    tf.keras.layers.MaxPooling2D(2, 2),\n\n                                    tf.keras.layers.Conv2D(64, (3,3), activation='relu',kernel_initializer = 'he_normal'),\n                                    tf.keras.layers.MaxPooling2D(2,2),\n  \n                                    tf.keras.layers.Flatten(),\n                                    tf.keras.layers.Dense(256, activation='relu',kernel_initializer = 'he_normal'),                                    \n                                    tf.keras.layers.Dense(128, activation='relu',kernel_initializer = 'he_normal'),\n                                    tf.keras.layers.Dense(10, activation='softmax')\n])\n\n\nsgd = tf.keras.optimizers.SGD(lr = 0.001)\n\nmodel.compile(loss = 'sparse_categorical_crossentropy',\n              optimizer = sgd,\n              metrics = ['accuracy'])\n\nh = model.fit(training_images,training_labels,\n          epochs=25,batch_size = 32,steps_per_epoch = 60000\/\/32,\n          validation_data = (test_images,test_labels),validation_steps = 10000\/\/32)","be12e9d2":"plot(h)","35f1d9b2":"model = tf.keras.models.Sequential([\n                                    tf.keras.layers.Conv2D(32, (3,3),kernel_initializer = 'he_normal',input_shape=(28, 28, 1)),\n                                    tf.keras.layers.LeakyReLU(alpha = 0.2),\n                                    tf.keras.layers.MaxPooling2D(2, 2),\n\n                                    tf.keras.layers.Conv2D(64, (3,3),kernel_initializer = 'he_normal'),\n                                    tf.keras.layers.LeakyReLU(alpha = 0.2),\n                                    tf.keras.layers.MaxPooling2D(2, 2),\n\n                                    tf.keras.layers.Conv2D(64, (3,3), kernel_initializer = 'he_normal'),\n                                    tf.keras.layers.LeakyReLU(alpha = 0.2),\n                                    tf.keras.layers.MaxPooling2D(2,2),\n  \n                                    tf.keras.layers.Flatten(),\n                                    tf.keras.layers.Dense(256, kernel_initializer = 'he_normal'),\n                                    tf.keras.layers.LeakyReLU(alpha = 0.2),\n\n                                    tf.keras.layers.Dense(128, kernel_initializer = 'he_normal'),\n                                    tf.keras.layers.LeakyReLU(alpha = 0.2),\n                                    \n                                    tf.keras.layers.Dense(10, activation='softmax')\n])\n\n\nsgd = tf.keras.optimizers.SGD(lr = 0.001)\n\nmodel.compile(loss = 'sparse_categorical_crossentropy',\n              optimizer = sgd,\n              metrics = ['accuracy'])\n\nh = model.fit(training_images,training_labels,\n          epochs=25,batch_size = 32,steps_per_epoch = 60000\/\/32,\n          validation_data = (test_images,test_labels),validation_steps = 10000\/\/32)","88333a08":"plot(h)","b1ce1eff":"model = tf.keras.models.Sequential([\n                                    tf.keras.layers.Conv2D(32, (3,3),kernel_initializer = 'he_normal',input_shape=(28, 28, 1)),\n                                    tf.keras.layers.PReLU(),\n                                    tf.keras.layers.MaxPooling2D(2, 2),\n\n                                    tf.keras.layers.Conv2D(64, (3,3),kernel_initializer = 'he_normal'),\n                                    tf.keras.layers.PReLU(),\n                                    tf.keras.layers.MaxPooling2D(2, 2),\n\n                                    tf.keras.layers.Conv2D(64, (3,3), kernel_initializer = 'he_normal'),\n                                    tf.keras.layers.PReLU(),\n                                    tf.keras.layers.MaxPooling2D(2,2),\n  \n                                    tf.keras.layers.Flatten(),\n                                    tf.keras.layers.Dense(256, kernel_initializer = 'he_normal'),\n                                    tf.keras.layers.PReLU(),\n\n                                    tf.keras.layers.Dense(128, kernel_initializer = 'he_normal'),\n                                    tf.keras.layers.PReLU(),\n                                    \n                                    tf.keras.layers.Dense(10, activation='softmax')\n])\n\n\nsgd = tf.keras.optimizers.SGD(lr = 0.001)\n\nmodel.compile(loss = 'sparse_categorical_crossentropy',\n              optimizer = sgd,\n              metrics = ['accuracy'])\n\nh = model.fit(training_images,training_labels,\n          epochs=25,batch_size = 32,steps_per_epoch = 60000\/\/32,\n          validation_data = (test_images,test_labels),validation_steps = 10000\/\/32)","53f31e0b":"plot(h)","8bbfa018":"model = tf.keras.models.Sequential([\n                                    tf.keras.layers.Conv2D(32, (3,3),kernel_initializer = 'he_normal',input_shape=(28, 28, 1)),\n                                    tf.keras.layers.ELU(),\n                                    tf.keras.layers.MaxPooling2D(2, 2),\n\n                                    tf.keras.layers.Conv2D(64, (3,3),kernel_initializer = 'he_normal'),\n                                    tf.keras.layers.ELU(),\n                                    tf.keras.layers.MaxPooling2D(2, 2),\n\n                                    tf.keras.layers.Conv2D(64, (3,3), kernel_initializer = 'he_normal'),\n                                    tf.keras.layers.ELU(),\n                                    tf.keras.layers.MaxPooling2D(2,2),\n  \n                                    tf.keras.layers.Flatten(),\n                                    tf.keras.layers.Dense(256, kernel_initializer = 'he_normal'),\n                                    tf.keras.layers.ELU(),\n\n                                    tf.keras.layers.Dense(128, kernel_initializer = 'he_normal'),\n                                    tf.keras.layers.ELU(),\n                                    \n                                    tf.keras.layers.Dense(10, activation='softmax')\n])\n\n\nsgd = tf.keras.optimizers.SGD(lr = 0.001)\n\nmodel.compile(loss = 'sparse_categorical_crossentropy',\n              optimizer = sgd,\n              metrics = ['accuracy'])\n\nh = model.fit(training_images,training_labels,\n          epochs=25,batch_size = 32,steps_per_epoch = 60000\/\/32,\n          validation_data = (test_images,test_labels),validation_steps = 10000\/\/32)","f3841e70":"plot(h)","fedba626":"model = tf.keras.models.Sequential([\n                                    tf.keras.layers.Conv2D(32, (3,3),kernel_initializer = 'he_normal',input_shape=(28, 28, 1)),\n                                    tf.keras.layers.LeakyReLU(alpha = 0.2),\n                                    tf.keras.layers.MaxPooling2D(2, 2),\n\n                                    tf.keras.layers.Conv2D(64, (3,3),kernel_initializer = 'he_normal'),\n                                    tf.keras.layers.LeakyReLU(alpha = 0.2),\n                                    tf.keras.layers.MaxPooling2D(2, 2),\n\n                                    tf.keras.layers.Conv2D(64, (3,3), kernel_initializer = 'he_normal'),\n                                    tf.keras.layers.LeakyReLU(alpha = 0.2),\n                                    tf.keras.layers.MaxPooling2D(2,2),\n  \n                                    tf.keras.layers.Flatten(),\n                                    tf.keras.layers.BatchNormalization(),\n                                    tf.keras.layers.Dense(256, kernel_initializer = 'he_normal',use_bias = False),\n                                    tf.keras.layers.LeakyReLU(alpha = 0.2),\n\n                                    tf.keras.layers.BatchNormalization(),\n                                    tf.keras.layers.Dense(128, kernel_initializer = 'he_normal',use_bias = False),\n                                    tf.keras.layers.LeakyReLU(alpha = 0.2),\n                                    \n                                    tf.keras.layers.BatchNormalization(),\n                                    tf.keras.layers.Dense(10, activation='softmax')\n])\n\n\nsgd = tf.keras.optimizers.SGD(lr = 0.001)\n\nmodel.compile(loss = 'sparse_categorical_crossentropy',\n              optimizer = sgd,\n              metrics = ['accuracy'])\n\nh = model.fit(training_images,training_labels,\n          epochs=25,batch_size = 32,steps_per_epoch = 60000\/\/32,\n          validation_data = (test_images,test_labels),validation_steps = 10000\/\/32)","41b613bc":"plot(h)","367cc515":"model = tf.keras.models.Sequential([\n                                    tf.keras.layers.Conv2D(32, (3,3),kernel_initializer = 'he_normal',input_shape=(28, 28, 1)),\n                                    tf.keras.layers.LeakyReLU(alpha = 0.2),\n                                    tf.keras.layers.MaxPooling2D(2, 2),\n\n                                    tf.keras.layers.Conv2D(64, (3,3),kernel_initializer = 'he_normal'),\n                                    tf.keras.layers.LeakyReLU(alpha = 0.2),\n                                    tf.keras.layers.MaxPooling2D(2, 2),\n\n                                    tf.keras.layers.Conv2D(64, (3,3), kernel_initializer = 'he_normal'),\n                                    tf.keras.layers.LeakyReLU(alpha = 0.2),\n                                    tf.keras.layers.MaxPooling2D(2,2),\n  \n                                    tf.keras.layers.Flatten(),\n                                    tf.keras.layers.BatchNormalization(),\n                                    tf.keras.layers.Dense(256, kernel_initializer = 'he_normal',use_bias = False),\n                                    tf.keras.layers.BatchNormalization(),                                    \n                                    tf.keras.layers.LeakyReLU(alpha = 0.2),\n\n                                    tf.keras.layers.BatchNormalization(),\n                                    tf.keras.layers.Dense(128, kernel_initializer = 'he_normal',use_bias = False),\n                                    tf.keras.layers.BatchNormalization(),\n                                    tf.keras.layers.LeakyReLU(alpha = 0.2),\n\n                                    tf.keras.layers.Dense(10, activation='softmax')\n])\n\n\nsgd = tf.keras.optimizers.SGD(lr = 0.001)\n\nmodel.compile(loss = 'sparse_categorical_crossentropy',\n              optimizer = sgd,\n              metrics = ['accuracy'])\n\nh = model.fit(training_images,training_labels,\n          epochs=25,batch_size = 32,steps_per_epoch = 60000\/\/32,\n          validation_data = (test_images,test_labels),validation_steps = 10000\/\/32)","8cd0810a":"plot(h)","2dfbb251":"model = tf.keras.models.Sequential([\n                                    tf.keras.layers.Conv2D(32, (3,3),kernel_initializer = 'he_normal',input_shape=(28, 28, 1)),\n                                    tf.keras.layers.LeakyReLU(alpha = 0.2),\n                                    tf.keras.layers.MaxPooling2D(2, 2),\n\n                                    tf.keras.layers.Conv2D(64, (3,3),kernel_initializer = 'he_normal'),\n                                    tf.keras.layers.LeakyReLU(alpha = 0.2),\n                                    tf.keras.layers.MaxPooling2D(2, 2),\n\n                                    tf.keras.layers.Conv2D(64, (3,3), kernel_initializer = 'he_normal'),\n                                    tf.keras.layers.LeakyReLU(alpha = 0.2),\n                                    tf.keras.layers.MaxPooling2D(2,2),\n  \n                                    tf.keras.layers.Flatten(),\n                                    tf.keras.layers.BatchNormalization(),\n                                    tf.keras.layers.Dense(256, kernel_initializer = 'he_normal',use_bias = False),\n                                    tf.keras.layers.LeakyReLU(alpha = 0.2),\n\n                                    tf.keras.layers.BatchNormalization(),\n                                    tf.keras.layers.Dense(128, kernel_initializer = 'he_normal',use_bias = False),\n                                    tf.keras.layers.LeakyReLU(alpha = 0.2),\n                                    \n                                    tf.keras.layers.BatchNormalization(),\n                                    tf.keras.layers.Dense(10, activation='softmax')\n])\n\n\nsgd = tf.keras.optimizers.SGD(lr = 0.001,clipvalue = 1.0)\n\nmodel.compile(loss = 'sparse_categorical_crossentropy',\n              optimizer = sgd,\n              metrics = ['accuracy'])\n\nh = model.fit(training_images,training_labels,\n          epochs=25,batch_size = 32,steps_per_epoch = 60000\/\/32,\n          validation_data = (test_images,test_labels),validation_steps = 10000\/\/32)","27832685":"plot(h)","a7e15705":"model = tf.keras.models.Sequential([\n                                    tf.keras.layers.Conv2D(32, (3,3),kernel_initializer = 'he_normal',input_shape=(28, 28, 1)),\n                                    tf.keras.layers.LeakyReLU(alpha = 0.2),\n                                    tf.keras.layers.MaxPooling2D(2, 2),\n\n                                    tf.keras.layers.Conv2D(64, (3,3),kernel_initializer = 'he_normal'),\n                                    tf.keras.layers.LeakyReLU(alpha = 0.2),\n                                    tf.keras.layers.MaxPooling2D(2, 2),\n\n                                    tf.keras.layers.Conv2D(64, (3,3), kernel_initializer = 'he_normal'),\n                                    tf.keras.layers.LeakyReLU(alpha = 0.2),\n                                    tf.keras.layers.MaxPooling2D(2,2),\n  \n                                    tf.keras.layers.Flatten(),\n                                    tf.keras.layers.BatchNormalization(),\n                                    tf.keras.layers.Dense(256, kernel_initializer = 'he_normal',use_bias = False),\n                                    tf.keras.layers.LeakyReLU(alpha = 0.2),\n\n                                    tf.keras.layers.BatchNormalization(),\n                                    tf.keras.layers.Dense(128, kernel_initializer = 'he_normal',use_bias = False),\n                                    tf.keras.layers.LeakyReLU(alpha = 0.2),\n                                    \n                                    tf.keras.layers.BatchNormalization(),\n                                    tf.keras.layers.Dense(10, activation='softmax')\n])\n\n\nsgd = tf.keras.optimizers.SGD(lr = 0.001,clipnorm = 1.0)\n\nmodel.compile(loss = 'sparse_categorical_crossentropy',\n              optimizer = sgd,\n              metrics = ['accuracy'])\n\nh = model.fit(training_images,training_labels,\n          epochs=25,batch_size = 32,steps_per_epoch = 60000\/\/32,\n          validation_data = (test_images,test_labels),validation_steps = 10000\/\/32)","62d31bfd":"plot(h)","ef628cc6":"model = tf.keras.models.Sequential([\n                                    tf.keras.layers.Conv2D(32, (3,3),kernel_initializer = 'he_normal',input_shape=(28, 28, 1)),\n                                    tf.keras.layers.LeakyReLU(alpha = 0.2),\n                                    tf.keras.layers.MaxPooling2D(2, 2),\n\n                                    tf.keras.layers.Conv2D(64, (3,3),kernel_initializer = 'he_normal'),\n                                    tf.keras.layers.LeakyReLU(alpha = 0.2),\n                                    tf.keras.layers.MaxPooling2D(2, 2),\n\n                                    tf.keras.layers.Conv2D(64, (3,3), kernel_initializer = 'he_normal'),\n                                    tf.keras.layers.LeakyReLU(alpha = 0.2),\n                                    tf.keras.layers.MaxPooling2D(2,2),\n  \n                                    tf.keras.layers.Flatten(),\n                                    tf.keras.layers.BatchNormalization(),\n                                    tf.keras.layers.Dense(256, kernel_initializer = 'he_normal',use_bias = False),\n                                    tf.keras.layers.LeakyReLU(alpha = 0.2),\n\n                                    tf.keras.layers.BatchNormalization(),\n                                    tf.keras.layers.Dense(128, kernel_initializer = 'he_normal',use_bias = False),\n                                    tf.keras.layers.LeakyReLU(alpha = 0.2),\n                                    \n                                    tf.keras.layers.BatchNormalization(),\n                                    tf.keras.layers.Dense(10, activation='softmax')\n])\n\n\nsgd = tf.keras.optimizers.SGD(lr = 0.001,momentum = 0.90,clipnorm = 1.0)\n\nmodel.compile(loss = 'sparse_categorical_crossentropy',\n              optimizer = sgd,\n              metrics = ['accuracy'])\n\nh = model.fit(training_images,training_labels,\n          epochs=25,batch_size = 32,steps_per_epoch = 60000\/\/32,\n          validation_data = (test_images,test_labels),validation_steps = 10000\/\/32)","d4ce0425":"plot(h)","368ff1ba":"model = tf.keras.models.Sequential([\n                                    tf.keras.layers.Conv2D(32, (3,3),kernel_initializer = 'he_normal',input_shape=(28, 28, 1)),\n                                    tf.keras.layers.LeakyReLU(alpha = 0.2),\n                                    tf.keras.layers.MaxPooling2D(2, 2),\n\n                                    tf.keras.layers.Conv2D(64, (3,3),kernel_initializer = 'he_normal'),\n                                    tf.keras.layers.LeakyReLU(alpha = 0.2),\n                                    tf.keras.layers.MaxPooling2D(2, 2),\n\n                                    tf.keras.layers.Conv2D(64, (3,3), kernel_initializer = 'he_normal'),\n                                    tf.keras.layers.LeakyReLU(alpha = 0.2),\n                                    tf.keras.layers.MaxPooling2D(2,2),\n  \n                                    tf.keras.layers.Flatten(),\n                                    tf.keras.layers.BatchNormalization(),\n                                    tf.keras.layers.Dense(256, kernel_initializer = 'he_normal',use_bias = False),\n                                    tf.keras.layers.LeakyReLU(alpha = 0.2),\n\n                                    tf.keras.layers.BatchNormalization(),\n                                    tf.keras.layers.Dense(128, kernel_initializer = 'he_normal',use_bias = False),\n                                    tf.keras.layers.LeakyReLU(alpha = 0.2),\n                                    \n                                    tf.keras.layers.BatchNormalization(),\n                                    tf.keras.layers.Dense(10, activation='softmax')\n])\n\n\nsgd = tf.keras.optimizers.SGD(lr = 0.001,momentum = 0.90,nesterov = True,clipnorm = 1.0)\n\nmodel.compile(loss = 'sparse_categorical_crossentropy',\n              optimizer = sgd,\n              metrics = ['accuracy'])\n\nh = model.fit(training_images,training_labels,\n          epochs=25,batch_size = 32,steps_per_epoch = 60000\/\/32,\n          validation_data = (test_images,test_labels),validation_steps = 10000\/\/32)","9a0465c2":"plot(h)","4169dfaf":"model = tf.keras.models.Sequential([\n                                    tf.keras.layers.Conv2D(32, (3,3),kernel_initializer = 'he_normal',input_shape=(28, 28, 1)),\n                                    tf.keras.layers.LeakyReLU(alpha = 0.2),\n                                    tf.keras.layers.MaxPooling2D(2, 2),\n\n                                    tf.keras.layers.Conv2D(64, (3,3),kernel_initializer = 'he_normal'),\n                                    tf.keras.layers.LeakyReLU(alpha = 0.2),\n                                    tf.keras.layers.MaxPooling2D(2, 2),\n\n                                    tf.keras.layers.Conv2D(64, (3,3), kernel_initializer = 'he_normal'),\n                                    tf.keras.layers.LeakyReLU(alpha = 0.2),\n                                    tf.keras.layers.MaxPooling2D(2,2),\n  \n                                    tf.keras.layers.Flatten(),\n                                    tf.keras.layers.BatchNormalization(),\n                                    tf.keras.layers.Dense(256, kernel_initializer = 'he_normal',use_bias = False),\n                                    tf.keras.layers.LeakyReLU(alpha = 0.2),\n\n                                    tf.keras.layers.BatchNormalization(),\n                                    tf.keras.layers.Dense(128, kernel_initializer = 'he_normal',use_bias = False),\n                                    tf.keras.layers.LeakyReLU(alpha = 0.2),\n                                    \n                                    tf.keras.layers.BatchNormalization(),\n                                    tf.keras.layers.Dense(10, activation='softmax')\n])\n\n\nrms = tf.keras.optimizers.RMSprop(lr = 0.001,rho = 0.90,clipnorm = 1.0)\n\nmodel.compile(loss = 'sparse_categorical_crossentropy',\n              optimizer = rms,\n              metrics = ['accuracy'])\n\nh = model.fit(training_images,training_labels,\n          epochs=25,batch_size = 32,steps_per_epoch = 60000\/\/32,\n          validation_data = (test_images,test_labels),validation_steps = 10000\/\/32)","8d4a4e37":"plot(h)","62f3ff33":"model = tf.keras.models.Sequential([\n                                    tf.keras.layers.Conv2D(32, (3,3),kernel_initializer = 'he_normal',input_shape=(28, 28, 1)),\n                                    tf.keras.layers.LeakyReLU(alpha = 0.2),\n                                    tf.keras.layers.MaxPooling2D(2, 2),\n\n                                    tf.keras.layers.Conv2D(64, (3,3),kernel_initializer = 'he_normal'),\n                                    tf.keras.layers.LeakyReLU(alpha = 0.2),\n                                    tf.keras.layers.MaxPooling2D(2, 2),\n\n                                    tf.keras.layers.Conv2D(64, (3,3), kernel_initializer = 'he_normal'),\n                                    tf.keras.layers.LeakyReLU(alpha = 0.2),\n                                    tf.keras.layers.MaxPooling2D(2,2),\n  \n                                    tf.keras.layers.Flatten(),\n                                    tf.keras.layers.BatchNormalization(),\n                                    tf.keras.layers.Dense(256, kernel_initializer = 'he_normal',use_bias = False),\n                                    tf.keras.layers.LeakyReLU(alpha = 0.2),\n\n                                    tf.keras.layers.BatchNormalization(),\n                                    tf.keras.layers.Dense(128, kernel_initializer = 'he_normal',use_bias = False),\n                                    tf.keras.layers.LeakyReLU(alpha = 0.2),\n                                    \n                                    tf.keras.layers.BatchNormalization(),\n                                    tf.keras.layers.Dense(10, activation='softmax')\n])\n\n\nadam = tf.keras.optimizers.Adam(lr = 0.001,beta_1 = 0.9,beta_2=0.999,clipnorm = 1.0)\n\nmodel.compile(loss = 'sparse_categorical_crossentropy',\n              optimizer = adam,\n              metrics = ['accuracy'])\n\nh = model.fit(training_images,training_labels,\n          epochs=25,batch_size = 32,steps_per_epoch = 60000\/\/32,\n          validation_data = (test_images,test_labels),validation_steps = 10000\/\/32)","473c664f":"plot(h)","773126db":"model = tf.keras.models.Sequential([\n                                    tf.keras.layers.Conv2D(32, (3,3),kernel_initializer = 'he_normal',input_shape=(28, 28, 1)),\n                                    tf.keras.layers.LeakyReLU(alpha = 0.2),\n                                    tf.keras.layers.MaxPooling2D(2, 2),\n\n                                    tf.keras.layers.Conv2D(64, (3,3),kernel_initializer = 'he_normal'),\n                                    tf.keras.layers.LeakyReLU(alpha = 0.2),\n                                    tf.keras.layers.MaxPooling2D(2, 2),\n\n                                    tf.keras.layers.Conv2D(64, (3,3), kernel_initializer = 'he_normal'),\n                                    tf.keras.layers.LeakyReLU(alpha = 0.2),\n                                    tf.keras.layers.MaxPooling2D(2,2),\n  \n                                    tf.keras.layers.Flatten(),\n                                    tf.keras.layers.BatchNormalization(),\n                                    tf.keras.layers.Dense(256, kernel_initializer = 'he_normal',use_bias = False),\n                                    tf.keras.layers.LeakyReLU(alpha = 0.2),\n\n                                    tf.keras.layers.BatchNormalization(),\n                                    tf.keras.layers.Dense(128, kernel_initializer = 'he_normal',use_bias = False),\n                                    tf.keras.layers.LeakyReLU(alpha = 0.2),\n                                    \n                                    tf.keras.layers.BatchNormalization(),\n                                    tf.keras.layers.Dense(10, activation='softmax')\n])\n\n\nadmx = tf.keras.optimizers.Adamax(lr = 0.001,beta_1 = 0.9,beta_2=0.999,clipnorm = 1.0)\n\nmodel.compile(loss = 'sparse_categorical_crossentropy',\n              optimizer = admx,\n              metrics = ['accuracy'])\n\nh = model.fit(training_images,training_labels,\n          epochs=25,batch_size = 32,steps_per_epoch = 60000\/\/32,\n          validation_data = (test_images,test_labels),validation_steps = 10000\/\/32)","9b7e994e":"plot(h)","5cd13114":"model = tf.keras.models.Sequential([\n                                    tf.keras.layers.Conv2D(32, (3,3),kernel_initializer = 'he_normal',input_shape=(28, 28, 1)),\n                                    tf.keras.layers.LeakyReLU(alpha = 0.2),\n                                    tf.keras.layers.MaxPooling2D(2, 2),\n\n                                    tf.keras.layers.Conv2D(64, (3,3),kernel_initializer = 'he_normal'),\n                                    tf.keras.layers.LeakyReLU(alpha = 0.2),\n                                    tf.keras.layers.MaxPooling2D(2, 2),\n\n                                    tf.keras.layers.Conv2D(64, (3,3), kernel_initializer = 'he_normal'),\n                                    tf.keras.layers.LeakyReLU(alpha = 0.2),\n                                    tf.keras.layers.MaxPooling2D(2,2),\n  \n                                    tf.keras.layers.Flatten(),\n                                    tf.keras.layers.BatchNormalization(),\n                                    tf.keras.layers.Dense(256, kernel_initializer = 'he_normal',use_bias = False),\n                                    tf.keras.layers.LeakyReLU(alpha = 0.2),\n\n                                    tf.keras.layers.BatchNormalization(),\n                                    tf.keras.layers.Dense(128, kernel_initializer = 'he_normal',use_bias = False),\n                                    tf.keras.layers.LeakyReLU(alpha = 0.2),\n                                    \n                                    tf.keras.layers.BatchNormalization(),\n                                    tf.keras.layers.Dense(10, activation='softmax')\n])\n\n\nndm = tf.keras.optimizers.Nadam(lr = 0.001,beta_1 = 0.9,beta_2=0.999,clipnorm = 1.0)\n\nmodel.compile(loss = 'sparse_categorical_crossentropy',\n              optimizer = ndm,\n              metrics = ['accuracy'])\n\nh = model.fit(training_images,training_labels,\n          epochs=25,batch_size = 32,steps_per_epoch = 60000\/\/32,\n          validation_data = (test_images,test_labels),validation_steps = 10000\/\/32)","c2c89c24":"plot(h)","984190d1":"model = tf.keras.models.Sequential([\n                                    tf.keras.layers.Conv2D(32, (3,3),kernel_initializer = 'he_normal',input_shape=(28, 28, 1)),\n                                    tf.keras.layers.LeakyReLU(alpha = 0.2),\n                                    tf.keras.layers.MaxPooling2D(2, 2),\n\n                                    tf.keras.layers.Conv2D(64, (3,3),kernel_initializer = 'he_normal'),\n                                    tf.keras.layers.LeakyReLU(alpha = 0.2),\n                                    tf.keras.layers.MaxPooling2D(2, 2),\n\n                                    tf.keras.layers.Conv2D(64, (3,3), kernel_initializer = 'he_normal'),\n                                    tf.keras.layers.LeakyReLU(alpha = 0.2),\n                                    tf.keras.layers.MaxPooling2D(2,2),\n  \n                                    tf.keras.layers.Flatten(),\n                                    tf.keras.layers.BatchNormalization(),\n                                    tf.keras.layers.Dense(256, kernel_initializer = 'he_normal',use_bias = False),\n                                    tf.keras.layers.LeakyReLU(alpha = 0.2),\n\n                                    tf.keras.layers.BatchNormalization(),\n                                    tf.keras.layers.Dense(128, kernel_initializer = 'he_normal',use_bias = False),\n                                    tf.keras.layers.LeakyReLU(alpha = 0.2),\n                                    \n                                    tf.keras.layers.BatchNormalization(),\n                                    tf.keras.layers.Dense(10, activation='softmax')\n])\n\n\nsgd = tf.keras.optimizers.SGD(lr = 0.01,clipnorm = 1.0,decay = 1e-4)\n\nmodel.compile(loss = 'sparse_categorical_crossentropy',\n              optimizer = sgd,\n              metrics = ['accuracy'])\n\nh = model.fit(training_images,training_labels,\n          epochs=25,batch_size = 32,steps_per_epoch = 60000\/\/32,\n          validation_data = (test_images,test_labels),validation_steps = 10000\/\/32)","111e1b9c":"plot(h)","cc30fc64":"def exp_decay_function(epoch):\n    return 0.01*(0.1**(epoch \/ 20))\n\nlr = tf.keras.callbacks.LearningRateScheduler(exp_decay_function)","a02db670":"model = tf.keras.models.Sequential([\n                                    tf.keras.layers.Conv2D(32, (3,3),kernel_initializer = 'he_normal',input_shape=(28, 28, 1)),\n                                    tf.keras.layers.LeakyReLU(alpha = 0.2),\n                                    tf.keras.layers.MaxPooling2D(2, 2),\n\n                                    tf.keras.layers.Conv2D(64, (3,3),kernel_initializer = 'he_normal'),\n                                    tf.keras.layers.LeakyReLU(alpha = 0.2),\n                                    tf.keras.layers.MaxPooling2D(2, 2),\n\n                                    tf.keras.layers.Conv2D(64, (3,3), kernel_initializer = 'he_normal'),\n                                    tf.keras.layers.LeakyReLU(alpha = 0.2),\n                                    tf.keras.layers.MaxPooling2D(2,2),\n  \n                                    tf.keras.layers.Flatten(),\n                                    tf.keras.layers.BatchNormalization(),\n                                    tf.keras.layers.Dense(256, kernel_initializer = 'he_normal',use_bias = False),\n                                    tf.keras.layers.LeakyReLU(alpha = 0.2),\n\n                                    tf.keras.layers.BatchNormalization(),\n                                    tf.keras.layers.Dense(128, kernel_initializer = 'he_normal',use_bias = False),\n                                    tf.keras.layers.LeakyReLU(alpha = 0.2),\n                                    \n                                    tf.keras.layers.BatchNormalization(),\n                                    tf.keras.layers.Dense(10, activation='softmax')\n])\n\n\nsgd = tf.keras.optimizers.SGD(lr = 0.01,clipnorm = 1.0)\n\nmodel.compile(loss = 'sparse_categorical_crossentropy',\n              optimizer = sgd,\n              metrics = ['accuracy'])\n\nh = model.fit(training_images,training_labels,\n          epochs=25,batch_size = 32,steps_per_epoch = 60000\/\/32,\n          validation_data = (test_images,test_labels),validation_steps = 10000\/\/32,callbacks = [lr])","7a9c275a":"plot(h)","68c91212":"def piecewise_constant_fn(epoch):\n    if epoch < 5:\n        return 0.01\n    elif epoch < 15:\n        return 0.005\n    else:\n        return 0.001\n\nlr = tf.keras.callbacks.LearningRateScheduler(piecewise_constant_fn)","107ae651":"model = tf.keras.models.Sequential([\n                                    tf.keras.layers.Conv2D(32, (3,3),kernel_initializer = 'he_normal',input_shape=(28, 28, 1)),\n                                    tf.keras.layers.LeakyReLU(alpha = 0.2),\n                                    tf.keras.layers.MaxPooling2D(2, 2),\n\n                                    tf.keras.layers.Conv2D(64, (3,3),kernel_initializer = 'he_normal'),\n                                    tf.keras.layers.LeakyReLU(alpha = 0.2),\n                                    tf.keras.layers.MaxPooling2D(2, 2),\n\n                                    tf.keras.layers.Conv2D(64, (3,3), kernel_initializer = 'he_normal'),\n                                    tf.keras.layers.LeakyReLU(alpha = 0.2),\n                                    tf.keras.layers.MaxPooling2D(2,2),\n  \n                                    tf.keras.layers.Flatten(),\n                                    tf.keras.layers.BatchNormalization(),\n                                    tf.keras.layers.Dense(256, kernel_initializer = 'he_normal',use_bias = False),\n                                    tf.keras.layers.LeakyReLU(alpha = 0.2),\n\n                                    tf.keras.layers.BatchNormalization(),\n                                    tf.keras.layers.Dense(128, kernel_initializer = 'he_normal',use_bias = False),\n                                    tf.keras.layers.LeakyReLU(alpha = 0.2),\n                                    \n                                    tf.keras.layers.BatchNormalization(),\n                                    tf.keras.layers.Dense(10, activation='softmax')\n])\n\n\nsgd = tf.keras.optimizers.SGD(lr = 0.01,clipnorm = 1.0)\n\nmodel.compile(loss = 'sparse_categorical_crossentropy',\n              optimizer = sgd,\n              metrics = ['accuracy'])\n\nh = model.fit(training_images,training_labels,\n          epochs=25,batch_size = 32,steps_per_epoch = 60000\/\/32,\n          validation_data = (test_images,test_labels),validation_steps = 10000\/\/32,callbacks = [lr])","39d83091":"plot(h)","52b3e773":"lr = tf.keras.callbacks.ReduceLROnPlateau(factor = 0.5, patience = 5)","5f86b246":"model = tf.keras.models.Sequential([\n                                    tf.keras.layers.Conv2D(32, (3,3),kernel_initializer = 'he_normal',input_shape=(28, 28, 1)),\n                                    tf.keras.layers.LeakyReLU(alpha = 0.2),\n                                    tf.keras.layers.MaxPooling2D(2, 2),\n\n                                    tf.keras.layers.Conv2D(64, (3,3),kernel_initializer = 'he_normal'),\n                                    tf.keras.layers.LeakyReLU(alpha = 0.2),\n                                    tf.keras.layers.MaxPooling2D(2, 2),\n\n                                    tf.keras.layers.Conv2D(64, (3,3), kernel_initializer = 'he_normal'),\n                                    tf.keras.layers.LeakyReLU(alpha = 0.2),\n                                    tf.keras.layers.MaxPooling2D(2,2),\n  \n                                    tf.keras.layers.Flatten(),\n                                    tf.keras.layers.BatchNormalization(),\n                                    tf.keras.layers.Dense(256, kernel_initializer = 'he_normal',use_bias = False),\n                                    tf.keras.layers.LeakyReLU(alpha = 0.2),\n\n                                    tf.keras.layers.BatchNormalization(),\n                                    tf.keras.layers.Dense(128, kernel_initializer = 'he_normal',use_bias = False),\n                                    tf.keras.layers.LeakyReLU(alpha = 0.2),\n                                    \n                                    tf.keras.layers.BatchNormalization(),\n                                    tf.keras.layers.Dense(10, activation='softmax')\n])\n\n\nsgd = tf.keras.optimizers.SGD(lr = 0.01,clipnorm = 1.0)\n\nmodel.compile(loss = 'sparse_categorical_crossentropy',\n              optimizer = sgd,\n              metrics = ['accuracy'])\n\nh = model.fit(training_images,training_labels,\n          epochs=25,batch_size = 32,steps_per_epoch = 60000\/\/32,\n          validation_data = (test_images,test_labels),validation_steps = 10000\/\/32,callbacks = [lr])","c4ba9e18":"plot(h)","fe2a4b9a":"model = tf.keras.models.Sequential([\n                                    tf.keras.layers.Conv2D(32, (3,3),kernel_initializer = 'he_normal',input_shape=(28, 28, 1)),\n                                    tf.keras.layers.LeakyReLU(alpha = 0.2),\n                                    tf.keras.layers.MaxPooling2D(2, 2),\n\n                                    tf.keras.layers.Conv2D(64, (3,3),kernel_initializer = 'he_normal'),\n                                    tf.keras.layers.LeakyReLU(alpha = 0.2),\n                                    tf.keras.layers.MaxPooling2D(2, 2),\n\n                                    tf.keras.layers.Conv2D(64, (3,3), kernel_initializer = 'he_normal'),\n                                    tf.keras.layers.LeakyReLU(alpha = 0.2),\n                                    tf.keras.layers.MaxPooling2D(2,2),\n  \n                                    tf.keras.layers.Flatten(),\n                                    tf.keras.layers.BatchNormalization(),\n                                    tf.keras.layers.Dense(256, kernel_initializer = 'he_normal',kernel_regularizer = tf.keras.regularizers.l2(0.001),use_bias = False),\n                                    tf.keras.layers.LeakyReLU(alpha = 0.2),\n\n                                    tf.keras.layers.BatchNormalization(),\n                                    tf.keras.layers.Dense(128, kernel_initializer = 'he_normal',kernel_regularizer = tf.keras.regularizers.l2(0.001),use_bias = False),\n                                    tf.keras.layers.LeakyReLU(alpha = 0.2),\n                                    \n                                    tf.keras.layers.BatchNormalization(),\n                                    tf.keras.layers.Dense(10, activation='softmax')\n])\n\n\nsgd = tf.keras.optimizers.SGD(lr = 0.01,clipnorm = 1.0)\n\nmodel.compile(loss = 'sparse_categorical_crossentropy',\n              optimizer = sgd,\n              metrics = ['accuracy'])\n\nh = model.fit(training_images,training_labels,\n          epochs=25,batch_size = 32,steps_per_epoch = 60000\/\/32,\n          validation_data = (test_images,test_labels),validation_steps = 10000\/\/32)","122bf705":"plot(h)","60635cef":"model = tf.keras.models.Sequential([\n                                    tf.keras.layers.Conv2D(32, (3,3),kernel_initializer = 'he_normal',input_shape=(28, 28, 1)),\n                                    tf.keras.layers.LeakyReLU(alpha = 0.2),\n                                    tf.keras.layers.MaxPooling2D(2, 2),\n\n                                    tf.keras.layers.Conv2D(64, (3,3),kernel_initializer = 'he_normal'),\n                                    tf.keras.layers.LeakyReLU(alpha = 0.2),\n                                    tf.keras.layers.MaxPooling2D(2, 2),\n\n                                    tf.keras.layers.Conv2D(64, (3,3), kernel_initializer = 'he_normal'),\n                                    tf.keras.layers.LeakyReLU(alpha = 0.2),\n                                    tf.keras.layers.MaxPooling2D(2,2),\n  \n                                    tf.keras.layers.Flatten(),\n                                    tf.keras.layers.BatchNormalization(),\n                                    tf.keras.layers.Dense(256, kernel_initializer = 'he_normal',use_bias = False),\n                                    tf.keras.layers.Dropout(0.3),\n                                    tf.keras.layers.LeakyReLU(alpha = 0.2),\n\n                                    tf.keras.layers.BatchNormalization(),\n                                    tf.keras.layers.Dense(128, kernel_initializer = 'he_normal',use_bias = False),\n                                    tf.keras.layers.Dropout(0.2),\n                                    tf.keras.layers.LeakyReLU(alpha = 0.2),\n                                    \n                                    tf.keras.layers.BatchNormalization(),\n                                    tf.keras.layers.Dense(10, activation='softmax')\n])\n\n\nsgd = tf.keras.optimizers.SGD(lr = 0.01,clipnorm = 1.0)\n\nmodel.compile(loss = 'sparse_categorical_crossentropy',\n              optimizer = sgd,\n              metrics = ['accuracy'])\n\nh = model.fit(training_images,training_labels,\n          epochs=25,batch_size = 32,steps_per_epoch = 60000\/\/32,\n          validation_data = (test_images,test_labels),validation_steps = 10000\/\/32)","b963ecc7":"plot(h)","d2717528":"model = tf.keras.models.Sequential([\n                                    tf.keras.layers.Conv2D(32, (3,3),kernel_initializer = 'he_normal',input_shape=(28, 28, 1)),\n                                    tf.keras.layers.LeakyReLU(alpha = 0.2),\n                                    tf.keras.layers.MaxPooling2D(2, 2),\n\n                                    tf.keras.layers.Conv2D(64, (3,3),kernel_initializer = 'he_normal'),\n                                    tf.keras.layers.LeakyReLU(alpha = 0.2),\n                                    tf.keras.layers.MaxPooling2D(2, 2),\n\n                                    tf.keras.layers.Conv2D(64, (3,3), kernel_initializer = 'he_normal'),\n                                    tf.keras.layers.LeakyReLU(alpha = 0.2),\n                                    tf.keras.layers.MaxPooling2D(2,2),\n  \n                                    tf.keras.layers.Flatten(),\n                                    tf.keras.layers.BatchNormalization(),\n                                    tf.keras.layers.Dense(256, kernel_initializer = 'he_normal',kernel_constraint = tf.keras.constraints.max_norm(1.),use_bias = False),\n                                    tf.keras.layers.LeakyReLU(alpha = 0.2),\n\n                                    tf.keras.layers.BatchNormalization(),\n                                    tf.keras.layers.Dense(128, kernel_initializer = 'he_normal',kernel_constraint = tf.keras.constraints.max_norm(1.),use_bias = False),\n                                    tf.keras.layers.LeakyReLU(alpha = 0.2),\n                                    \n                                    tf.keras.layers.BatchNormalization(),\n                                    tf.keras.layers.Dense(10, activation='softmax')\n])\n\n\nsgd = tf.keras.optimizers.SGD(lr = 0.01,clipnorm = 1.0)\n\nmodel.compile(loss = 'sparse_categorical_crossentropy',\n              optimizer = sgd,\n              metrics = ['accuracy'])\n\nh = model.fit(training_images,training_labels,\n          epochs=25,batch_size = 32,steps_per_epoch = 60000\/\/32,\n          validation_data = (test_images,test_labels),validation_steps = 10000\/\/32)","6508949e":"plot(h)","4041e307":"## Clip by Norm","13c973d2":"## BN After Activation Function","9e0bd4b8":"# ELU Activation Function\n\n![](https:\/\/360digit.b-cdn.net\/assets\/admin\/ckfinder\/userfiles\/images\/blog\/elu.png)\n\nIt looks a lot like the ReLU function, with a few major differences:\n\n1. First it takes on negative values when z < 0, which allows the unit to have an average output closer to 0. This helps alleviate the vanishing gradients problem, as discussed earlier. The hyperparameter \u03b1 defines the value that the ELU function approaches when z is a large negative number. It is usually set to 1, but you can tweak it like any other hyperparameter if you want.\n\n2. Second, it has a nonzero gradient for z < 0, which avoids the dying units issue.\n\n3. Third, the function is smooth everywhere, including around z = 0, which helps speed up Gradient Descent, since it does not bounce as much left and right of z = 0.\n\n> To implement this, Just use this layer instead of Activation Function parameter : tf.keras.layers.ELU()","9c5fc1da":"# Max Norm Regularization\n\nAnother regularization technique that is quite popular for neural networks is called max-norm regularization: for each neuron, it constrains the weights w of the incoming connections such that r, where r is the max-norm hyperparameter and \u00b7 2 is the l 2 norm. We typically implement this constraint by computing\n\nneeded (w 2 \u2264 w 2 after each training step and clipping w if). Reducing r increases the amount of regularization and helps reduce overfitting. Max-norm regularization can also help alleviate the vanishing\/exploding gradients problems (if you are not using Batch Normalization).\n\n> Implementation in Keras : kernel_constraint = tf.keras.constraints.max_norm(1.)","75896610":"# Learning Rate Schedulers\n\nFinding a good learning rate can be tricky. If you set it way too high, training may actually diverge. If you set it too low, training will eventually converge to the optimum, but it will take a very long time. If you set it slightly too high, it will make progress very quickly at first, but it will end up dancing around the optimum, never settling down (unless you use an adaptive learning rate optimization algorithm such as AdaGrad, RMSProp, or Adam, but even then it may take time to settle).\n\nYou can do better than a constant learning rate: if you start with a high learning rate and then reduce it once it stops making fast progress, you can reach a good solution faster than with the optimal constant learning rate. There are many different strategies to reduce the learning rate during training. These strategies are called learning schedules.","1450916d":"# He Initialization\n\nWe just simply multiply random initialization with :\n\n![](https:\/\/miro.medium.com\/max\/700\/1*zxD6Nr6TyAb8JEG6oXAjkg.png)\n\n> To implement in Keras , Just use kernel_initializer = 'he_normal'","004b9657":"# ADAM Optimizer\n\n![](https:\/\/miro.medium.com\/max\/960\/1*nuoD-tQylhMj-SF8WJfUjg.gif)\n\nAdam, stands for adaptive moment estimation, combines the ideas of Momentum optimization and RMSProp: just like Momentum optimization it keeps track of an exponentially decaying average of past gradients, and just like RMSProp it keeps track of an exponentially decaying average of past squared gradients.\n\n### Adam Configuration Parameters\n\nalpha : Also referred to as the learning rate or step size. The proportion that weights are updated (e.g.0.001). Larger values (e.g. 0.3) results in faster initial learning before the rate is updated. Smaller values (e.g. 1.0E-5) slow learning right down during training\n\nbeta1 : The exponential decay rate for the first moment estimates (e.g. 0.9).\n\nbeta2 : The exponential decay rate for the second-moment estimates (e.g. 0.999). This value should be set close to 1.0 on problems with a sparse gradient (e.g. NLP and computer vision problems).\n\nepsilon : Is a very small number to prevent any division by zero in the implementation (e.g. 10E-8).\n\n> Implementation in Keras : adam = tf.keras.optimizers.Adam(lr = 0.001,beta_1 = 0.9,beta_2=0.999,clipnorm = 1.0)","64b1929c":"# Utilities\n\n> I am making a plot function to plot different metrics after each training. So that we can visualize our results.","ee71b996":"## Clip By Value","d9accc10":"# Gradient Clipping\n\nA popular technique to lessen the exploding gradients problem is to simply clip the gradients during backpropagation so that they never exceed some threshold.\n\n> In Keras, Implementing Gradient Clipping is just setting clipvalue or cleepnorm argument when creating optimizer. Please read more about difference between cleepvalue and cleepnorm over Google.","6f7d171f":"# NADAM Optimizer\nNadam combines NAG and Adam.Nadam is employed for noisy gradients or for gradients with high curvatures The learning process is accelerated by summing up the exponential decay of the moving averages for the previous and current gradient.\n\n> Implementing NADAM : ndm = tf.keras.optimizers.Nadam(lr = 0.001,beta_1 = 0.9,beta_2=0.999,clipnorm = 1.0)","9494ffe2":"# Leaky ReLU\n\n![](https:\/\/i0.wp.com\/androidkt.com\/wp-content\/uploads\/2020\/05\/Selection_019.png?resize=760%2C364)\n\nThis function is defined as LeakyReLU \u03b1 (z) = max(\u03b1z, z) (see Figure). The hyperparameter \u03b1 defines how much the function \u201cleaks\u201d: it is the slope of the function for z < 0, and is typically set to 0.01. This small slope ensures that leaky ReLUs never die.\n\n> To implement this, Just use this layer instead of Activation Function parameter : tf.keras.layers.LeakyReLU(alpha = 0.2) where alpha is Leak.","01c71bc6":"# Exponential Scheduling\n\nSet the learning rate to a function of the iteration number t: \u03b7(t) = \u03b7 0 10 \u2013t\/r . This works great, but it\nrequires tuning \u03b7 0 and r. The learning rate will drop by a factor of 10 every r steps.","2bcbfea9":"# PReLU\n\n![](https:\/\/imgs.developpaper.com\/imgs\/2813863991-5e720eab30026_articlex.png)\n\nParametric leaky ReLU (PReLU), where \u03b1 is authorized to be learned during training (instead of being a hyperparameter, it becomes a parameter that can be modified by backpropagation like any other parameter). This was reported to strongly outperform ReLU on large image datasets, but on smaller datasets it runs the risk of overfitting the training set.\n\n> To implement this, Just use this layer instead of Activation Function parameter : tf.keras.layers.PReLU()","6f208122":"# Power Scheduling\n\nSet the learning rate to \u03b7(t) = \u03b7 0 (1 + t\/r) \u2013c . The hyperparameter c is typically set to 1. This is\nsimilar to exponential scheduling, but the learning rate drops much more slowly.\n\n> To Implement use decay Parameter : sgd = tf.keras.optimizers.SGD(lr = 0.01,clipnorm = 1.0,decay = 1e-4)","31d15e52":"# RMSProp Optimizer\n\n![](https:\/\/mlfromscratch.com\/content\/images\/2019\/12\/rmsprop.gif)\n\nThe RMSprop optimizer is similar to the gradient descent algorithm with momentum. The RMSprop optimizer restricts the oscillations in the vertical direction. Therefore, we can increase our learning rate and our algorithm could take larger steps in the horizontal direction converging faster. The difference between RMSprop and gradient descent is on how the gradients are calculated. The following equations show how the gradients are calculated for the RMSprop and gradient descent with momentum. The value of momentum is denoted by beta and is usually set to 0.9. If you are not interested in the math behind the optimizer :-\n\n![](https:\/\/miro.medium.com\/max\/524\/1*m_PC8M4y9UKYU9JNuOC9Jw.png)\n\n> Implementation in Keras : rms = tf.keras.optimizers.RMSprop(lr = 0.001,rho = 0.90,clipnorm = 1.0)","3e7b30a4":"# Momentum Optimizer\n\n![](https:\/\/mlfromscratch.com\/content\/images\/2019\/12\/momentum.gif)\n\nImagine a bowling ball rolling down a gentle slope on a smooth surface: it will start out slowly, but it will quickly pick up momentum until it eventually reaches terminal velocity (if there is some friction or air resistance). This is the very simple idea behind Momentum optimization.Momentum optimization cares a great deal about what previous gradients were?\n\n![](https:\/\/blog.paperspace.com\/content\/images\/2018\/06\/momentum2-1.png)\n\nTo simulate some sort of friction mechanism and prevent the momentum from growing too large, the algorithm introduces a new hyperparameter \u03b2, simply called the momentum, which must be set between 0 (high friction) and 1 (no friction). A typical momentum value is 0.9.\n\n> Implementing in Keras is easier just use momentum Parameter : sgd = tf.keras.optimizers.SGD(lr = 0.001,momentum = 0.90,clipnorm = 1.0)","54bdd5a5":"# AdaMax Optimizer\nAdamax was introduced by the authors of Adam in the same paper. The idea with Adamax is to look at the value v as the L2 norm of the individual current and past gradients. We can generalize it to Lp update rule, but it gets pretty unstable for large values of p. But if we use the special case of L-infinity norm, it results in a surprisingly stable and well-performing algorithm. Here\u2019s how to implement Adamax with python:\n\n> admx = tf.keras.optimizers.Adamax(lr = 0.001,beta_1 = 0.9,beta_2=0.999,clipnorm = 1.0)","baefb502":"# L1 and L2 Regularizers\n\nRegularisation is a process of introducing additional information in order to prevent overfitting.If you have studied the concept of regularization in machine learning, you will have a fair idea that regularization penalizes the coefficients. In deep learning, it actually penalizes the weight matrices of the nodes.\n\n> Implementation : Use kernel_regularizer = tf.keras.regularizers.l2(0.001)","6f9aa182":"# Data Acquisition\n\n> I am using fashion MNIST data which contain 10 classes and 70000 images of shape 28*28","b320f2b1":"> Let us see the architecture of model by using plot_model function.","c87b7fd4":"# BN Before Activation Function","8c6df3fa":"> **Thank you for reading my Notebook, I hope this is useful in some way. Good Luck for future!!**","00214c1d":"> Let us compare these optimizers with a GIF\n\n![](https:\/\/2.bp.blogspot.com\/-q6l20Vs4P_w\/VPmIC7sEhnI\/AAAAAAAACC4\/g3UOUX2r_yA\/s1600\/s25RsOr%2B-%2BImgur.gif)","e28f0469":"# Introduction\n\n![](https:\/\/static.packt-cdn.com\/products\/9781789345377\/graphics\/2770c767-b801-4225-a0af-0761808742a9.jpg)\n\nIf you need to tackle a very complex problem, such as detecting hundreds of types of objects in high-resolution images? You may need to train a much deeper DNN, perhaps with (say) 10 layers, each containing hundreds of neurons, connected by hundreds of thousands of connections. First, you would be faced with the tricky vanishing gradients problem (or the related exploding gradients problem) that affects deep neural networks and makes lower layers very hard to train. Second, with such a large network, training would be extremely slow. Third, a model with millions of parameters would severely risk overfitting the training set.\n\nIn this Notebook, I will go through each of these problems in turn and present techniques to solve them.\n\n**Topics Covered in this Notebook**\n\n* Vanishing Gradients Problems : Different Weight Initialization\n* Nonsaturating Activation Functions : ReLU, LeakyReLU,RReLU,PReLU,ELU,SELU\n* Batch Normalization\n* Exploding Gradients Problems : Gradient Clipping\n* Optimizers : Momentum, RMSProp, ADAM, NADAM, ADAMAX, NESTEROV\n* Learning Rate Schedulers : Power Scheduler, Exponential, Performance and PieceWise\n* l1 and l2 Regularizers\n* DropOut Regularization\n* MaxNorm Regularization\n\n> All the implementation are done in TensorFlow and Keras.","5f64a6b3":"# Performance Scheduling\n\nMeasure the validation error every N steps (just like for early stopping) and reduce the learning rate by a factor of \u03bb when the error stops dropping. Multiplly learning rate by 0.5 with patience 5.","c68ab3ca":"# Simple Model\n\n> Let us make a very simple model first and then we will start tweaking different parameters and architecture.","a3e93eb2":"# Batch Normalization\n\n![](https:\/\/images.deepai.org\/glossary-terms\/981e1ffea3814ae193c27461253faf63\/batch_normalization.png)\n\nAlthough using He initialization along with ELU (or any variant of ReLU) can significantly reduce the vanishing\/exploding gradients problems at the beginning of training, it doesn\u2019t guarantee that they won\u2019t come back during training.\n\nThe technique consists of adding an operation in the model just before the activation function of each layer, simply zero-centering and normalizing the inputs, then scaling and shifting the result using two new parameters per layer (one for scaling, the other for shifting). In other words, this operation lets the model learn the optimal scale and mean of the inputs for each layer.\n\n> To implement simply use this layer tf.keras.layers.batch_normalization(). We can apply BN Layer before activation function or even after activation function. It depends on work, So it is better to try both and compare results.","24e1ad8f":"![](https:\/\/miro.medium.com\/max\/1224\/1*064lT1SXq_6F7uoc00V1fw.gif)","88689726":"# Nesterov Optimizer\n\n![](https:\/\/image.slidesharecdn.com\/2dwfpvwoswquxlfmgkvr-signature-a67e94e385b0f51ee716eaa6622e12490992bac90a7a1bf9ae2be66c7fac4b82-poli-160812071230\/95\/08-distributed-optimization-9-638.jpg?cb=1470986354)\n\nOne small variant to Momentum optimization, proposed by Yurii Nesterov in 1983, 12 is almost always faster than vanilla Momentum optimization. The idea of Nesterov Momentum optimization, or Nesterov Accelerated Gradient (NAG), is to measure the gradient of the cost function not at the local position but slightly ahead in the direction of the momentum.\n\n> To implement in tf.keras , Just use nesterov = True parameter : sgd = tf.keras.optimizers.SGD(lr = 0.001,momentum = 0.90,nesterov = True,clipnorm = 1.0)","2d036add":"# Dropout Regularization\n\n![](https:\/\/miro.medium.com\/max\/1224\/1*064lT1SXq_6F7uoc00V1fw.gif)\n\nIt is a fairly simple algorithm: at every training step, every neuron (including the input neurons but excluding the output neurons) has a probability p of being temporarily \u201cdropped out,\u201d meaning it will be entirely ignored during this training step, but it may be active during the next step (see Figure 11-9). The hyperparameter p is called the dropout rate, and it is typically set to 50%. After training, neurons don\u2019t get dropped anymore.\n\n> Implementation : Simple use a layer : tf.keras.layers.Dropout(0.2)","ac2dbe58":"# Piecewise Constant Scheduling\n\nFor example, set the learning rate to \u03b7 0 = 0.1 at first, then to \u03b7 1 = 0.001 after 50 epochs. Although this solution can work very well, it often requires fiddling around to figure out the right learning rates and when to use them."}}