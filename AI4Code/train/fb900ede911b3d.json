{"cell_type":{"009b2e95":"code","c9826bf7":"code","34672a84":"code","3615255f":"code","fd1b3437":"code","94ffe7da":"code","25776abb":"code","0f7de2cb":"code","d5e41d62":"code","58d6f7c6":"code","3b69336d":"code","5b9ec7e0":"code","4be41bb2":"code","f5934213":"code","ea7f74a7":"code","76f10066":"code","c0037983":"code","b64f4a7e":"code","1c1814fc":"code","785c02ac":"code","62948428":"code","d54e09f2":"code","061d0911":"code","6c9f69f1":"code","cc2ff625":"code","292ae814":"code","2b40d826":"code","b65184dc":"code","4dda927b":"code","003d6523":"code","2fa2bd77":"code","fda46b1a":"code","3df3e6dc":"code","ef2950c1":"code","68f55548":"code","9c4f190b":"code","c12e48db":"code","1226b9fc":"code","9ee08622":"code","02c0eee8":"code","e525a778":"code","1800a4b4":"code","0508c44c":"code","dfee7d96":"code","b5552cb2":"code","c925fc55":"code","9f100502":"code","293d86c1":"code","6d996d06":"code","cfc507a6":"code","4da51224":"code","f61ac94e":"code","05b46cc1":"code","be61f6f7":"code","bb712b55":"code","4ef1647d":"code","d9022083":"code","ada58e48":"code","ab05c0f3":"code","60d904f1":"code","ed938169":"code","688a8565":"code","3a687487":"code","376354c8":"code","e7130969":"code","95de60a1":"code","3b47897f":"code","36dbf37d":"code","fe2ed890":"code","96a48e36":"code","ee753466":"code","308cecdc":"code","48d850d2":"code","66c20d7a":"code","672bfcc8":"code","ec664b21":"code","c446f0e6":"code","c8c38f66":"code","8e52335f":"code","8e82c8a0":"code","5b243382":"code","f50ae085":"code","95d1052e":"code","633a2cbe":"code","e6e77afd":"code","aa3feb0c":"code","23865be7":"code","f6c74f17":"code","e74505bd":"code","64cc1703":"code","46ebea68":"code","45d66a2b":"markdown","5b51c100":"markdown","4ad6de26":"markdown","9d19585a":"markdown","b1e5909b":"markdown","e644286d":"markdown","7e716984":"markdown","7639ff20":"markdown","513c395a":"markdown","cc3d8a99":"markdown","d6f428c0":"markdown","dd0be44f":"markdown","c0ecc9a7":"markdown","561be819":"markdown","3764ae83":"markdown","55edc13a":"markdown","f9747671":"markdown","fa49355a":"markdown","fe3e84ea":"markdown","cda4f210":"markdown","221d45cf":"markdown","db89ffe2":"markdown","bafd58a4":"markdown","f8468ab0":"markdown","987b5fea":"markdown","803cb171":"markdown","74bd0a4a":"markdown","f5de91ca":"markdown","4711bf70":"markdown","76644d77":"markdown","5a19c640":"markdown","6ada2749":"markdown","b5ef5689":"markdown","82c8853c":"markdown","647b75fb":"markdown","2213530d":"markdown","75bf1808":"markdown","266cd632":"markdown","3090dc38":"markdown","e5bcf191":"markdown","eb59b82d":"markdown","fbfad6df":"markdown","b409e683":"markdown","a2adec3a":"markdown","9143f715":"markdown","326743e4":"markdown","fd583c33":"markdown","02e4d34d":"markdown","fe271966":"markdown","06331d38":"markdown","2955cbfa":"markdown","ce3f7146":"markdown","65039137":"markdown","4bf1b2d6":"markdown","136dec56":"markdown","efbd98d0":"markdown","ecbea8f3":"markdown","55348a5a":"markdown","263bdcc9":"markdown","44476246":"markdown","1ba06bd7":"markdown","02a44ab0":"markdown","1efdfa6c":"markdown","6c4f25ad":"markdown","811583f5":"markdown","4971cf03":"markdown","53f07d5b":"markdown","3cddf91b":"markdown","2d793b6a":"markdown","533706a9":"markdown","0a383720":"markdown","f95d3728":"markdown","5501c7da":"markdown","365b53b6":"markdown","6c27a6f2":"markdown","d426b5b8":"markdown","7acac517":"markdown","bdaff8ec":"markdown","be0b296e":"markdown"},"source":{"009b2e95":"import warnings\nwarnings.simplefilter('ignore')\n\nimport os#Walking through directores\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nimport missingno as msno # Visualizing Missing Value\nfrom plotly.subplots import make_subplots #To Create Subplots\n\n\n\nimport seaborn as sns # stastical graphs \nsns.set_style('whitegrid')\nimport matplotlib.pyplot as plt\ncolors = ['rgb(240,128,128)', 'rgb(102,205,170)', 'rgb(0,206,209)']  #used for markers\nimport plotly.graph_objects as go # Generate Graphs\n\n\nfrom sklearn.preprocessing import MinMaxScaler # Scaling Purpose\n\nfrom sklearn.neighbors import KNeighborsClassifier #KNN Model\nfrom sklearn.ensemble import RandomForestClassifier #RandomForest Model\nfrom sklearn.linear_model import LogisticRegression #Logistic Model\n\nfrom sklearn.model_selection import train_test_split # Splitting into train and test\n\nfrom sklearn.model_selection import GridSearchCV# Hyperparameter Tuning\nfrom sklearn.model_selection import cross_val_score#cross validation score\n\nfrom sklearn.metrics import classification_report # text report showing the main classification metrics\nfrom sklearn.metrics import confusion_matrix #to get confusion_matirx ","c9826bf7":"missing_values = ['?', '--', ' ', 'NA', 'N\/A', '-'] #Sometimes Missing Values are't in form of NaN\ndf = pd.read_csv('..\/input\/heart-disease-uci\/heart.csv', delimiter = ',', na_values = missing_values)\nprint('There are Total {} datapoints in the dataset with {} Features listed as {}:'.format(df.shape[0], df.shape[1], df.columns.values))","34672a84":"df[df['ca']==4]","3615255f":"df.loc[df['ca']==4, 'ca'] = np.NaN","fd1b3437":"df.loc[48, 'thal'] = np.NaN\ndf.loc[281, 'thal'] = np.NaN","94ffe7da":"df.head()","25776abb":"df.isnull().sum()","0f7de2cb":"msno.matrix(df)","d5e41d62":"duplicate_sum = df.duplicated().sum()\nif duplicate_sum:\n    print('Duplicates Rows in Dataset are : {}'.format(duplicate_sum))\nelse:\n    print('Dataset contains no Duplicate Values')","58d6f7c6":"df","3b69336d":"duplicated = df[df.duplicated(keep=False)]\nduplicated.head()","5b9ec7e0":"df.drop_duplicates(keep = 'first', inplace = True)\nprint('Total {} datapoints remaining with {} features'.format(df.shape[0], df.shape[1]))","4be41bb2":"Continuous_features = [feature for feature in df.columns if len(df[feature].unique())>25]\nprint('Continuous Values are : {}'.format(Continuous_features))","f5934213":"df[Continuous_features].head()","ea7f74a7":"df[Continuous_features].describe()","76f10066":"fig = go.Figure()\n\nfig.add_trace(go.Box(x=df['age'], name = 'age', boxpoints='outliers'))\nfig.add_trace(go.Box(x=df['trestbps'], name = 'trestbps', boxpoints='outliers'))\nfig.add_trace(go.Box(x=df['chol'], name = 'chol', boxpoints='outliers'))\nfig.add_trace(go.Box(x=df['thalach'], name = 'thalach', boxpoints='outliers'))\nfig.add_trace(go.Box(x=df['oldpeak'], name = 'oldpeak', boxpoints='outliers'))\n\nfig.update_layout(title_text=\"Box Plot for Continuous features with Outliers\")\nfig.show()","c0037983":"fig = go.Figure()\nfig.add_trace(go.Box(x=df['oldpeak'], name = 'oldpeak', boxpoints='outliers'))\n\nfig.update_layout(title_text=\"Box Plot for oldpeak with Outliers\")\nfig.show()","b64f4a7e":"def outliers(df_out, drop = False):\n    for each_feature in df_out.columns:\n        feature_data = df_out[each_feature]\n        Q1 = np.percentile(feature_data, 25.) # 25th percentile of the data of the given feature\n        Q3 = np.percentile(feature_data, 75.) # 75th percentile of the data of the given feature\n        IQR = Q3-Q1 #Interquartile Range\n        outlier_step = IQR * 1.5 #That's we were talking about above\n        outliers = feature_data[~((feature_data >= Q1 - outlier_step) & (feature_data <= Q3 + outlier_step))].index.tolist()  \n        if not drop:\n            print('For the feature {}, No of Outliers is {}'.format(each_feature, len(outliers)))\n        if drop:\n            df.drop(outliers, inplace = True, errors = 'ignore')\n            print('Outliers from {} feature removed'.format(each_feature))","1c1814fc":"outliers(df[Continuous_features])","785c02ac":"outliers(df[Continuous_features], drop = True)","62948428":" df[Continuous_features].head()","d54e09f2":"fig = go.Figure(data=[go.Bar(x = df[df['target'] == 0]['age'].value_counts().index.to_list(), \n                             y =df[df['target'] == 0]['age'].value_counts().values, name = 'Heart Disease'),\n                      go.Bar(x = df[df['target'] == 1]['age'].value_counts().index.to_list(), \n                             y =df[df['target'] == 1]['age'].value_counts().values, name = 'No Heart Disease')]\n               )\n\nfig.update_layout(barmode='group', xaxis_tickangle=-45,title_text=\"Fequency of Age groups grouped by Target Value\", \n                  yaxis=dict(\n        title='Total Count',\n        titlefont_size=16,\n        tickfont_size=14,\n    ),     xaxis=dict(\n        title='Age',\n        titlefont_size=16,\n        tickfont_size=14,\n    ))\nfig.show()","061d0911":"fig = go.Figure()\nfig.add_trace(go.Violin(y=df['age'][df['target'] == 0], box_visible=True, line_color='lightseagreen',\n                               fillcolor='lightseagreen', opacity=0.6,points=\"all\",\n                               legendgroup='Heart Disease', scalegroup='Heart Disease', name='Heart Disease',side='negative',x0='Age'))\n\nfig.add_trace(go.Violin(y=df['age'][df['target'] == 1], box_visible=True, line_color='indigo',\n                               fillcolor='indigo', opacity=0.6,points=\"all\",\n                               legendgroup='No Heart Disease', scalegroup='No Heart Disease', name='No Heart Disease',side='positive',x0='Age'))\n\nfig.update_layout(yaxis_zeroline=False, title_text=\"Age Distribution grouped by Target Variable\",)\n\nfig.show()","6c9f69f1":"fig = go.Figure(data=[go.Bar(x = df[df['target'] == 0]['trestbps'].value_counts().index.to_list(), \n                             y =df[df['target'] == 0]['trestbps'].value_counts().values, name = 'Heart Disease'),\n                      go.Bar(x = df[df['target'] == 1]['trestbps'].value_counts().index.to_list(), \n                             y =df[df['target'] == 1]['trestbps'].value_counts().values, name = 'No Heart Disease')]\n               )\n\nfig.update_layout(barmode='group', xaxis_tickangle=-45,title_text=\"Fequency of Resting Blood Pressure grouped by Target Value\", \n                  yaxis=dict(\n        title='Total Count',\n        titlefont_size=16,\n        tickfont_size=14,\n    ),     xaxis=dict(\n        title='Resting Blood Pressure ',\n        titlefont_size=16,\n        tickfont_size=14,\n    ))\nfig.show()","cc2ff625":"fig = make_subplots(rows=1, cols=2,subplot_titles=(\"Heart Disease\", \"No Heart Disease\"), specs=[[{'type':'domain'}, {'type':'domain'}]])\n\nfig.add_trace(go.Pie(labels=df[df['target'] == 0]['trestbps'].value_counts().index.to_list(),values=df[df['target'] ==0]['trestbps'].value_counts().values, hovertemplate = \"RPV: %{label} <br>Popularity: %{percent}<\/br>\" ,showlegend=False,  name = 'Heart Disease'), 1, 1)\nfig.add_trace(go.Pie(labels=df[df['target'] == 1]['trestbps'].value_counts().index.to_list(),values=df[df['target'] ==1]['trestbps'].value_counts().values, hovertemplate = \"RPV: %{label} <br>Popularity: %{percent}<\/br>\" ,showlegend=False,  name = 'No Heart Disease'), 1, 2)\n\nfig.data[0].marker.line.color = 'rgb(255, 255, 255)'\nfig.data[0].marker.line.width = 2\n\nfig.data[1].marker.line.color = 'rgb(255, 255, 255)'\nfig.data[1].marker.line.width = 2\n\nfig.update_traces(hole=.4,)\nfig.update_layout(title_text=\"Distribution of Resting Blood Pressure\",title_x=0.5, title_y = 1)\n\nfig.show()","292ae814":"fig = go.Figure()\nfig.add_trace(go.Violin(y=df['chol'][df['target'] == 0], box_visible=True, line_color='lightseagreen',\n                               fillcolor='lightseagreen', opacity=0.6,points=\"all\",\n                               legendgroup='Heart Disease', scalegroup='Heart Disease', name='Heart Disease',side='negative',x0=' Serum Cholesterol Level'))\n\nfig.add_trace(go.Violin(y=df['chol'][df['target'] == 1], box_visible=True, line_color='indigo',\n                               fillcolor='indigo', opacity=0.6,points=\"all\",\n                               legendgroup='No Heart Disease', scalegroup='No Heart Disease', name='No Heart Disease',side='positive',x0=' Serum Cholesterol Level'))\n\nfig.update_layout(yaxis_zeroline=False, title_text=\" Serum Cholesterol Level Distribution grouped by Target Variable\",)\n\nfig.show()","2b40d826":"fig = go.Figure(data=[go.Bar(x = df[df['target'] == 0]['thalach'].value_counts().index.to_list(), \n                             y =df[df['target'] == 0]['thalach'].value_counts().values, name = 'Heart Disease'),\n                      go.Bar(x = df[df['target'] == 1]['thalach'].value_counts().index.to_list(), \n                             y =df[df['target'] == 1]['thalach'].value_counts().values, name = 'No Heart Disease')]\n               )\n\nfig.update_layout(barmode='group', xaxis_tickangle=-45,title_text=\"Fequency of Maximum Heart Rate achieved during Thalium Stress Test grouped by Target Value\", \n                  yaxis=dict(\n        title='Total Count',\n        titlefont_size=16,\n        tickfont_size=14,\n    ),     xaxis=dict(\n        title='Maximum Heart Rate achieved during Thalium Stress Test ',\n        titlefont_size=16,\n        tickfont_size=14,\n    ))\nfig.show()","b65184dc":"fig = go.Figure()\nfig.add_trace(go.Violin(y=df['thalach'][df['target'] == 0], box_visible=True, line_color='lightseagreen',\n                               fillcolor='lightseagreen', opacity=0.6,points=\"all\",\n                               legendgroup='Heart Disease', scalegroup='Heart Disease', name='Heart Disease',side='negative',x0=' Maximum Heart Rate achieved during Thalium Stress Test'))\n\nfig.add_trace(go.Violin(y=df['thalach'][df['target'] == 1], box_visible=True, line_color='indigo',\n                               fillcolor='indigo', opacity=0.6,points=\"all\",\n                               legendgroup='No Heart Disease', scalegroup='No Heart Disease', name='No Heart Disease',side='positive',x0=' Maximum Heart Rate achieved during Thalium Stress Test'))\n\nfig.update_layout(yaxis_zeroline=False, title_text=\"Distribution for Maximum Heart Rate achieved during Thalium Stress Test grouped by Target Variable\",)\n\nfig.show()","4dda927b":"fig = go.Figure(data=[go.Bar(x = df[df['target'] == 0]['oldpeak'].value_counts().index.to_list(), \n                             y =df[df['target'] == 0]['oldpeak'].value_counts().values, name = 'Heart Disease'),\n                      go.Bar(x = df[df['target'] == 1]['oldpeak'].value_counts().index.to_list(), \n                             y =df[df['target'] == 1]['oldpeak'].value_counts().values, name = 'No Heart Disease')]\n               )\n\nfig.update_layout(barmode='group', xaxis_tickangle=-45,title_text=\"ST depression Level Distribution grouped by Target Value\", \n                  yaxis=dict(\n        title='Total Count',\n        titlefont_size=16,\n        tickfont_size=14,\n    ),     xaxis=dict(\n        title='ST depression Level',\n        titlefont_size=16,\n        tickfont_size=14,\n    ))\nfig.show()","003d6523":"fig = go.Figure()\nfig.add_trace(go.Violin(y=df['oldpeak'][df['target'] == 0], box_visible=True, line_color='lightseagreen',\n                               fillcolor='lightseagreen', opacity=0.6,points=\"all\",\n                               legendgroup='Heart Disease', scalegroup='Heart Disease', name='Heart Disease',side='negative',x0='ST depression Level'))\n\nfig.add_trace(go.Violin(y=df['oldpeak'][df['target'] == 1], box_visible=True, line_color='indigo',\n                               fillcolor='indigo', opacity=0.6,points=\"all\",\n                               legendgroup='No Heart Disease', scalegroup='No Heart Disease', name='No Heart Disease',side='positive',x0='ST depression Level'))\n\nfig.update_layout(yaxis_zeroline=False, title_text=\"ST depression Level Distribution grouped by Target Variable\",)\n\nfig.show()","2fa2bd77":"Continuous_features.append('target')","fda46b1a":"g = sns.pairplot(df[Continuous_features], kind='scatter',hue='target', palette=\"husl\", corner=True)\n\ng._legend.set_title('Cases')\nnew_labels = ['Heart Disease', 'No Heart Disease']\nfor t, l in zip(g._legend.texts, new_labels): t.set_text(l)\n\nplt.show()","3df3e6dc":"Categorial_features = [feature for feature in df.columns if len(df[feature].unique())<=25]\nprint('Continuous Values are : {}'.format(Categorial_features))","ef2950c1":"for each_feature in Categorial_features:\n    print('No of Categorial Values in Feature {} is {} as {}'.format(each_feature, len(df[each_feature].unique()), df[each_feature].unique()))","68f55548":"\nfig = go.Figure([go.Pie(labels=['No Heart Disease', 'Heart Disease'],values=df['target'].value_counts().values,hovertemplate = '<br>Type: %{label}<\/br>Count: %{value}<br>Popularity: %{percent}<\/br>', name = '', marker_colors = colors)])\nfig.update_layout(title_text=\"Pie chart of Target Variable\", template=\"plotly_white\")\nfig.data[0].marker.line.color = 'rgb(255, 255, 255)'\nfig.data[0].marker.line.width = 2\nfig.update_traces(hole=.4,)\nfig.show()","9c4f190b":"fig = make_subplots(rows=1, cols=2,subplot_titles=(\"Heart Disease\", \"No Heart Disease\"), specs=[[{'type':'domain'}, {'type':'domain'}]])\n\n\ncolors = ['rgb(240,128,128)', 'rgb(102,205,170)', 'rgb(0,206,209)'] \n\nfig.add_trace(go.Pie(labels=df[df['target'] == 0]['sex'].value_counts().index.to_list(),values=df[df['target'] ==0]['sex'].value_counts().values ,showlegend=False,  name = 'Heart Disease', customdata=['Male', 'Female'], hovertemplate = '%{customdata} <br>Count: %{value}<\/br>Popularity: %{percent}<\/br>', marker_colors = colors), 1, 1)\nfig.add_trace(go.Pie(labels=df[df['target'] == 1]['sex'].value_counts().index.to_list(),values=df[df['target'] ==1]['sex'].value_counts().values ,showlegend=False,  name = 'No Heart Disease', customdata=['Male', 'Female'], hovertemplate = '%{customdata} <br>Count: %{value}<\/br>Popularity: %{percent}'), 1, 2)\n\nfig.data[0].marker.line.color = 'rgb(255, 255, 255)'\nfig.data[0].marker.line.width = 2\n\nfig.data[1].marker.line.color = 'rgb(255, 255, 255)'\nfig.data[1].marker.line.width = 2\n\nfig.update_traces(hole=.4)\n\nfig.update_layout(title_text=\"Distribution of Sex grouped by Target Feature\",title_x=0.5, title_y = 1)\n\nfig.show()","c12e48db":"fig = make_subplots(rows=1, cols=2,subplot_titles=(\"Heart Disease\", \"No Heart Disease\"), specs=[[{'type':'domain'}, {'type':'domain'}]])\n\n\ncolors = ['rgb(240,128,128)', 'rgb(102,205,170)', 'rgb(0,206,209)'] \n\nfig.add_trace(go.Pie(labels=df[df['target'] == 0]['cp'].value_counts().index.to_list(),values=df[df['target'] ==0]['cp'].value_counts().values ,showlegend=False,  name = 'Heart Disease', customdata=['Asymptomatic', 'Non-Anginal Pain', 'Atypical Angina', 'Typical Angina'], hovertemplate = '%{customdata} <br>Count: %{value}<\/br>Popularity: %{percent}<\/br>', marker_colors = colors), 1, 1)\nfig.add_trace(go.Pie(labels=df[df['target'] == 1]['cp'].value_counts().index.to_list(),values=df[df['target'] ==1]['cp'].value_counts().values ,showlegend=False,  name = 'No Heart Disease', customdata=['Asymptomatic', 'Non-Anginal Pain', 'Atypical Angina', 'Typical Angina'], hovertemplate = '%{customdata} <br>Count: %{value}<\/br>Popularity: %{percent}'), 1, 2)\n\nfig.data[0].marker.line.color = 'rgb(255, 255, 255)'\nfig.data[0].marker.line.width = 2\n\nfig.data[1].marker.line.color = 'rgb(255, 255, 255)'\nfig.data[1].marker.line.width = 2\n\nfig.update_traces(hole=.4)\n\nfig.update_layout(title_text=\"Distribution of Chest Pain Type grouped by Target Feature\",title_x=0.5)\n\nfig.show()","1226b9fc":"fig = make_subplots(rows=1, cols=2,subplot_titles=(\"Heart Disease\", \"No Heart Disease\"), specs=[[{'type':'domain'}, {'type':'domain'}]])\n\n\ncolors = ['rgb(240,128,128)', 'rgb(102,205,170)', 'rgb(0,206,209)'] \n\nfig.add_trace(go.Pie(labels=df[df['target'] == 0]['fbs'].value_counts().index.to_list(),values=df[df['target'] ==0]['fbs'].value_counts().values ,showlegend=False,  name = 'Heart Disease', customdata=['Less than 120 mg\/dl (False)', 'Greater than 120 mg\/dl (True)'], hovertemplate = '%{customdata} <br>Count: %{value}<\/br>Popularity: %{percent}<\/br>', marker_colors = colors), 1, 1)\nfig.add_trace(go.Pie(labels=df[df['target'] == 1]['fbs'].value_counts().index.to_list(),values=df[df['target'] ==1]['fbs'].value_counts().values ,showlegend=False,  name = 'No Heart Disease', customdata=['Less than 120 mg\/dl (False)', 'Greater than 120 mg\/dl (True)'], hovertemplate = '%{customdata} <br>Count: %{value}<\/br>Popularity: %{percent}'), 1, 2)\n\nfig.data[0].marker.line.color = 'rgb(255, 255, 255)'\nfig.data[0].marker.line.width = 2\n\nfig.data[1].marker.line.color = 'rgb(255, 255, 255)'\nfig.data[1].marker.line.width = 2\n\nfig.update_traces(hole=.4)\n\nfig.update_layout(title_text=\"Distribution of Fasting Blood Sugar grouped by Target Feature\",title_x=0.5)\n\nfig.show()","9ee08622":"fig = make_subplots(rows=1, cols=2,subplot_titles=(\"Heart Disease\", \"No Heart Disease\"), specs=[[{'type':'domain'}, {'type':'domain'}]])\n\n\ncolors = ['rgb(240,128,128)', 'rgb(102,205,170)', 'rgb(0,206,209)'] \n\nfig.add_trace(go.Pie(labels=df[df['target'] == 0]['restecg'].value_counts().index.to_list(),values=df[df['target'] ==0]['restecg'].value_counts().values ,showlegend=False,  name = 'Heart Disease', customdata=['Showing Probable or Definite Left Ventricular Hypertrophy', 'Normal', 'Having ST-T Wave Abnormality'], hovertemplate = '%{customdata} <br>Count: %{value}<\/br>Popularity: %{percent}<\/br>', marker_colors = colors), 1, 1)\nfig.add_trace(go.Pie(labels=df[df['target'] == 1]['restecg'].value_counts().index.to_list(),values=df[df['target'] ==1]['restecg'].value_counts().values ,showlegend=False,  name = 'No Heart Disease', customdata=['Showing Probable or Definite Left Ventricular Hypertrophy', 'Normal', 'Having ST-T Wave Abnormality'], hovertemplate = '%{customdata} <br>Count: %{value}<\/br>Popularity: %{percent}'), 1, 2)\n\nfig.data[0].marker.line.color = 'rgb(255, 255, 255)'\nfig.data[0].marker.line.width = 2\n\nfig.data[1].marker.line.color = 'rgb(255, 255, 255)'\nfig.data[1].marker.line.width = 2\n\nfig.update_traces(hole=.4)\n\nfig.update_layout(title_text=\"Distribution of Resting Electrocardiographic Results grouped by Target Feature\",title_x=0.5)\n\nfig.show()","02c0eee8":"fig = make_subplots(rows=1, cols=2,subplot_titles=(\"Heart Disease\", \"No Heart Disease\"), specs=[[{'type':'domain'}, {'type':'domain'}]])\n\n\ncolors = ['rgb(240,128,128)', 'rgb(102,205,170)', 'rgb(0,206,209)'] \n\nfig.add_trace(go.Pie(labels=df[df['target'] == 0]['slope'].value_counts().index.to_list(),values=df[df['target'] ==0]['slope'].value_counts().values ,showlegend=False,  name = 'Heart Disease', customdata=['Flat', 'Upsloping', 'Downsloping'], hovertemplate = '%{customdata} <br>Count: %{value}<\/br>Popularity: %{percent}<\/br>', marker_colors = colors), 1, 1)\nfig.add_trace(go.Pie(labels=df[df['target'] == 1]['slope'].value_counts().index.to_list(),values=df[df['target'] ==1]['slope'].value_counts().values ,showlegend=False,  name = 'No Heart Disease', customdata=['Upsloping', 'Flat', 'Downsloping'], hovertemplate = '%{customdata} <br>Count: %{value}<\/br>Popularity: %{percent}'), 1, 2)\n\nfig.data[0].marker.line.color = 'rgb(255, 255, 255)'\nfig.data[0].marker.line.width = 2\n\nfig.data[1].marker.line.color = 'rgb(255, 255, 255)'\nfig.data[1].marker.line.width = 2\n\nfig.update_traces(hole=.4)\n\nfig.update_layout(title_text=\"Distribution of Slope of peak exercise ST segment grouped by Target Feature\",title_x=0.5)\n\nfig.show()","e525a778":"fig = make_subplots(rows=1, cols=2,subplot_titles=(\"Heart Disease\", \"No Heart Disease\"), specs=[[{'type':'domain'}, {'type':'domain'}]])\n\n\ncolors = ['rgb(240,128,128)', 'rgb(102,205,170)', 'rgb(0,206,209)'] \n\nfig.add_trace(go.Pie(labels=df[df['target'] == 0]['ca'].value_counts().index.to_list(),values=df[df['target'] ==0]['ca'].value_counts().values ,showlegend=False,  name = 'Heart Disease', customdata=['1', '0', '2', '3'], hovertemplate = '%{customdata} <br>Count: %{value}<\/br>Popularity: %{percent}<\/br>', marker_colors = colors), 1, 1)\nfig.add_trace(go.Pie(labels=df[df['target'] == 1]['ca'].value_counts().index.to_list(),values=df[df['target'] ==1]['ca'].value_counts().values ,showlegend=False,  name = 'No Heart Disease', customdata=['0', '1', '2', '3'], hovertemplate = '%{customdata} <br>Count: %{value}<\/br>Popularity: %{percent}'), 1, 2)\n\nfig.data[0].marker.line.color = 'rgb(255, 255, 255)'\nfig.data[0].marker.line.width = 2\n\nfig.data[1].marker.line.color = 'rgb(255, 255, 255)'\nfig.data[1].marker.line.width = 2\n\nfig.update_traces(hole=.4)\n\nfig.update_layout(title_text=\"Distribution of Number of major vessels colored by Fluoroscopy grouped by Target Feature\",title_x=0.5)\n\nfig.show()","1800a4b4":"fig = make_subplots(rows=1, cols=2,subplot_titles=(\"Heart Disease\", \"No Heart Disease\"), specs=[[{'type':'domain'}, {'type':'domain'}]])\n\n\ncolors = ['rgb(240,128,128)', 'rgb(102,205,170)', 'rgb(0,206,209)'] \n\nfig.add_trace(go.Pie(labels=df[df['target'] == 0]['exang'].value_counts().index.to_list(),values=df[df['target'] ==0]['exang'].value_counts().values ,showlegend=False,  name = 'Heart Disease', customdata=['Yes', 'No'], hovertemplate = '%{customdata} <br>Count: %{value}<\/br>Popularity: %{percent}<\/br>', marker_colors = colors), 1, 1)\nfig.add_trace(go.Pie(labels=df[df['target'] == 1]['exang'].value_counts().index.to_list(),values=df[df['target'] ==1]['exang'].value_counts().values ,showlegend=False,  name = 'No Heart Disease', customdata=['No', 'Yes'], hovertemplate = '%{customdata} <br>Count: %{value}<\/br>Popularity: %{percent}'), 1, 2)\n\nfig.data[0].marker.line.color = 'rgb(255, 255, 255)'\nfig.data[0].marker.line.width = 2\n\nfig.data[1].marker.line.color = 'rgb(255, 255, 255)'\nfig.data[1].marker.line.width = 2\n\nfig.update_traces(hole=.4)\n\nfig.update_layout(title_text=\"Distribution of Exercise induced angina grouped by Target Feature\",title_x=0.5)\n\nfig.show()","0508c44c":"fig = make_subplots(rows=1, cols=2,subplot_titles=(\"Heart Disease\", \"No Heart Disease\"), specs=[[{'type':'domain'}, {'type':'domain'}]])\n\n\ncolors = ['rgb(240,128,128)', 'rgb(102,205,170)', 'rgb(0,206,209)'] \n\nfig.add_trace(go.Pie(labels=df[df['target'] == 0]['thal'].value_counts().index.to_list(),values=df[df['target'] ==0]['thal'].value_counts().values ,showlegend=False,  name = 'Heart Disease', customdata=['Reversable Defect', 'Normal', 'Fixed Defect'], hovertemplate = '%{customdata} <br>Count: %{value}<\/br>Popularity: %{percent}<\/br>', marker_colors = colors), 1, 1)\nfig.add_trace(go.Pie(labels=df[df['target'] == 1]['thal'].value_counts().index.to_list(),values=df[df['target'] ==1]['thal'].value_counts().values ,showlegend=False,  name = 'No Heart Disease', customdata=['Normal', 'Reversable Defect', 'Fixed Defect'], hovertemplate = '%{customdata} <br>Count: %{value}<\/br>Popularity: %{percent}'), 1, 2)\n\nfig.data[0].marker.line.color = 'rgb(255, 255, 255)'\nfig.data[0].marker.line.width = 2\n\nfig.data[1].marker.line.color = 'rgb(255, 255, 255)'\nfig.data[1].marker.line.width = 2\n\nfig.update_traces(hole=.4)\n\nfig.update_layout(title_text=\"Distribution of Thalium Stress Test result grouped by Target Feature\",title_x=0.5)\n\nfig.show()","dfee7d96":"df.isnull().sum()","b5552cb2":"df = df.fillna(df.median())","c925fc55":"df.info()","9f100502":"df = df.astype({'ca': int, 'thal': int}) ","293d86c1":"plt.rcParams['figure.figsize'] = (20, 15) \nsns.heatmap(df.corr(), annot = True, linewidths=.5, cmap=\"YlGnBu\")\nplt.title('Corelation Between Features', fontsize = 30)\nplt.show()","6d996d06":"X = df.drop(['fbs', 'chol', 'trestbps', 'restecg', 'target'], axis =1)\nY = df['target']","cfc507a6":"encoded_cp = pd.get_dummies(df['cp'], prefix = \"cp\")\nencoded_ca = pd.get_dummies(df['ca'], prefix = \"ca\")\nencoded_thal = pd.get_dummies(df['thal'], prefix = \"thal\")\nencoded_slope = pd.get_dummies(df['slope'], prefix = \"slope\")","4da51224":"X = pd.concat([X,encoded_cp, encoded_ca, encoded_thal, encoded_slope], axis = 1)\nX = X.drop(columns = ['cp', 'ca', 'thal', 'slope'], axis = 1)\nX.head()","f61ac94e":"Y.value_counts()","05b46cc1":"from sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nstandard_X = scaler.fit_transform(X)\n","be61f6f7":"pd.DataFrame(standard_X, columns = X.columns).head()","bb712b55":"X_train, X_test, y_train, y_test = train_test_split(standard_X,Y,test_size = 0.2,random_state=43, shuffle = True)","4ef1647d":"params = {'n_neighbors':list(range(0, 101)),\n          'weights':['uniform', 'distance'],\n          'p':[1,2]}","d9022083":"\"\"\"knn = KNeighborsClassifier()\nknn_grid_cv = GridSearchCV(knn, param_grid=params, cv=5) \nknn_grid_cv.fit(X_train, y_train)\nprint(\"Best Hyper Parameters:\\n\",knn_grid_cv.best_params_)\"\"\"\n\nprint(\"Best Hyper Parameters: {'n_neighbors': 22, 'p': 2, 'weights': 'uniform'}\")","ada58e48":"knn = KNeighborsClassifier(n_neighbors = 22, p=2, weights = 'uniform') \nknn.fit(X_train, y_train)","ab05c0f3":"params = { \n    'n_estimators': [300, 400, 500,600, 600, 700, 800, 900, 1000],\n    'max_depth' : [2,3,4,5,6,7, 8],\n    'criterion' : ['entropy','gini']\n}","60d904f1":"\"\"\"rfc_gridcv = RandomForestClassifier(random_state=42)\nrfc_gridcv = GridSearchCV(estimator=rfc_gridcv, param_grid=params, cv= 5)\nrfc_gridcv.fit(X_train, y_train)\nprint(\"Best Hyper Parameters:\\n\",rfc_gridcv.best_params_)\"\"\"\nprint(\"Best Hyper Parameters: {'criterion': 'gini', 'max_depth': 2, 'n_estimators': 1000}\")","ed938169":"rfc = RandomForestClassifier(random_state=42, n_estimators=1000, max_depth= 2, criterion = 'gini')\nrfc.fit(X_train, y_train)","688a8565":"params_for_l1 = { \n    'C' :  np.logspace(0, 4, 10),\n    'solver' : ['liblinear', 'saga']\n}\n\nparams_for_l2 = { \n    'C' :  np.logspace(0, 4, 10),\n    'solver' : ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga']\n}\n\nparams_for_elasticnet = { \n    'C' :  np.logspace(0, 4, 10),\n    'l1_ratio' : np.arange (0.1, 1.0, 0.1),\n    'solver' : ['saga']\n}","3a687487":"logreg_with_l1_gridcv = LogisticRegression(penalty = 'l1')\nlogreg_with_l1_gridcv = GridSearchCV(estimator=logreg_with_l1_gridcv, param_grid=params_for_l1, cv= 5)\nlogreg_with_l1_gridcv.fit(X_train, y_train)\nprint(\"Best Hyper Parameters:\\n\",logreg_with_l1_gridcv.best_params_)","376354c8":"logreg_with_l1 = LogisticRegression(penalty = 'l1', C = 1, solver = 'liblinear')\nlogreg_with_l1.fit(X_train, y_train)","e7130969":"logreg_with_l2_gridcv = LogisticRegression(penalty = 'l2')\nlogreg_with_l2_gridcv = GridSearchCV(estimator=logreg_with_l2_gridcv, param_grid=params_for_l2, cv= 5)\nlogreg_with_l2_gridcv.fit(X_train, y_train)\nprint(\"Best Hyper Parameters:\\n\",logreg_with_l2_gridcv.best_params_)","95de60a1":"logreg_with_l2_gridcv = LogisticRegression(penalty = 'l2', C = 1, solver = 'newton-cg')\nlogreg_with_l2_gridcv.fit(X_train, y_train)","3b47897f":"logreg_with_elasticnet_gridcv = LogisticRegression(penalty = 'elasticnet')\nlogreg_with_elasticnet_gridcv = GridSearchCV(estimator=logreg_with_elasticnet_gridcv, param_grid=params_for_elasticnet, cv= 5)\nlogreg_with_elasticnet_gridcv.fit(X_train, y_train)\nprint(\"Best Hyper Parameters:\\n\",logreg_with_elasticnet_gridcv.best_params_)","36dbf37d":"logreg_with_elasticnet_gridcv = LogisticRegression(penalty = 'elasticnet', C = 1, solver = 'saga', l1_ratio = 0.5)\nlogreg_with_elasticnet_gridcv.fit(X_train, y_train)","fe2ed890":"knn.score(X_test, y_test)","96a48e36":"X_train1, X_test1, y_train1, y_test1 = train_test_split(standard_X,Y,test_size = 0.2,random_state=0, shuffle = True)","ee753466":"knn1 = KNeighborsClassifier(n_neighbors = 22, p=2, weights = 'uniform') \nknn1.fit(X_train1, y_train1)\nknn1.score(X_test1, y_test1)","308cecdc":"scores = cross_val_score(knn, X_train, y_train, cv=5)\nprint('KNN Model gives an average accuracy of {0:.2f} % with minimun of {1:.2f} % and maximum of {2:.2f} % accuracy'.format(scores.mean() * 100, scores.min() * 100, scores.max() * 100))","48d850d2":"Y_hat = knn.predict(X_test)\nprint(classification_report(y_test, Y_hat))","66c20d7a":"plt.rcParams['figure.figsize'] = (5, 5) \nsns.heatmap(confusion_matrix(y_test, Y_hat), annot = True, linewidths=.5, cmap=\"YlGnBu\")\nplt.title('Corelation Between Features')\nplt.show()","672bfcc8":"print('True Positive Cases : {}'.format(confusion_matrix(y_test, Y_hat)[1][1]))\nprint('True Negative Cases : {}'.format(confusion_matrix(y_test, Y_hat)[0][0]))\nprint('False Positive Cases : {}'.format(confusion_matrix(y_test, Y_hat)[0][1]))\nprint('False Negative Cases : {}'.format(confusion_matrix(y_test, Y_hat)[1][0]))","ec664b21":"scores = cross_val_score(rfc, X_train, y_train, cv=5)\nprint('Random Forest Model gives an average accuracy of {0:.2f} % with minimun of {1:.2f} % and maximum of {2:.2f} % accuracy'.format(scores.mean() * 100, scores.min() * 100, scores.max() * 100))","c446f0e6":"Y_hat = rfc.predict(X_test)\nprint(classification_report(y_test, Y_hat))","c8c38f66":"plt.rcParams['figure.figsize'] = (5, 5) \nsns.heatmap(confusion_matrix(y_test, Y_hat), annot = True, linewidths=.5, cmap=\"YlGnBu\")\nplt.title('Corelation Between Features')\nplt.show()","8e52335f":"print('True Positive Cases : {}'.format(confusion_matrix(y_test, Y_hat)[1][1]))\nprint('True Negative Cases : {}'.format(confusion_matrix(y_test, Y_hat)[0][0]))\nprint('False Positive Cases : {}'.format(confusion_matrix(y_test, Y_hat)[0][1]))\nprint('False Negative Cases : {}'.format(confusion_matrix(y_test, Y_hat)[1][0]))","8e82c8a0":"scores = cross_val_score(logreg_with_l1, X_train, y_train, cv=5)\nprint('Logistic Model with L1 Penalty gives an average accuracy of {0:.2f} % with minimun of {1:.2f} % and maximum of {2:.2f} % accuracy'.format(scores.mean() * 100, scores.min() * 100, scores.max() * 100))","5b243382":"Y_hat = logreg_with_l1.predict(X_test)\nprint(classification_report(y_test, Y_hat))","f50ae085":"plt.rcParams['figure.figsize'] = (5, 5) \nsns.heatmap(confusion_matrix(y_test, Y_hat), annot = True, linewidths=.5, cmap=\"YlGnBu\")\nplt.title('Corelation Between Features')\nplt.show()","95d1052e":"print('True Positive Cases : {}'.format(confusion_matrix(y_test, Y_hat)[1][1]))\nprint('True Negative Cases : {}'.format(confusion_matrix(y_test, Y_hat)[0][0]))\nprint('False Positive Cases : {}'.format(confusion_matrix(y_test, Y_hat)[0][1]))\nprint('False Negative Cases : {}'.format(confusion_matrix(y_test, Y_hat)[1][0]))","633a2cbe":"scores = cross_val_score(logreg_with_l2_gridcv, X_train, y_train, cv=5)\nprint('Logistic Model with L2 Penalty gives an average accuracy of {0:.2f} % with minimun of {1:.2f} % and maximum of {2:.2f} % accuracy'.format(scores.mean() * 100, scores.min() * 100, scores.max() * 100))","e6e77afd":"Y_hat = logreg_with_l2_gridcv.predict(X_test)\nprint(classification_report(y_test, Y_hat))","aa3feb0c":"plt.rcParams['figure.figsize'] = (5, 5) \nsns.heatmap(confusion_matrix(y_test, Y_hat), annot = True, linewidths=.5, cmap=\"YlGnBu\")\nplt.title('Corelation Between Features')\nplt.show()","23865be7":"print('True Positive Cases : {}'.format(confusion_matrix(y_test, Y_hat)[1][1]))\nprint('True Negative Cases : {}'.format(confusion_matrix(y_test, Y_hat)[0][0]))\nprint('False Positive Cases : {}'.format(confusion_matrix(y_test, Y_hat)[0][1]))\nprint('False Negative Cases : {}'.format(confusion_matrix(y_test, Y_hat)[1][0]))","f6c74f17":"scores = cross_val_score(logreg_with_elasticnet_gridcv, X_train, y_train, cv=5)\nprint('Logistic Model with L2 Penalty gives an average accuracy of {0:.2f} % with minimun of {1:.2f} % and maximum of {2:.2f} % accuracy'.format(scores.mean() * 100, scores.min() * 100, scores.max() * 100))","e74505bd":"Y_hat = logreg_with_elasticnet_gridcv.predict(X_test)\nprint(classification_report(y_test, Y_hat))","64cc1703":"plt.rcParams['figure.figsize'] = (5, 5) \nsns.heatmap(confusion_matrix(y_test, Y_hat), annot = True, linewidths=.5, cmap=\"YlGnBu\")\nplt.title('Corelation Between Features')\nplt.show()","46ebea68":"print('True Positive Cases : {}'.format(confusion_matrix(y_test, Y_hat)[1][1]))\nprint('True Negative Cases : {}'.format(confusion_matrix(y_test, Y_hat)[0][0]))\nprint('False Positive Cases : {}'.format(confusion_matrix(y_test, Y_hat)[0][1]))\nprint('False Negative Cases : {}'.format(confusion_matrix(y_test, Y_hat)[1][0]))","45d66a2b":"## Evaluation for Random Forest","5b51c100":"Replacing np.NaN with median","4ad6de26":"## Outliers ? Ah Shit, Here We go again ..\n\n\n<img  src=\"https:\/\/pbs.twimg.com\/media\/EDANCjJXkAAOSjO.jpg\" alt=\"FDR\" width=\"600\" height=\"400\"\/>\n","9d19585a":"Time for a Matrix bois ..","b1e5909b":"## Evaluation for KNN","e644286d":"# Conclusion\n\n> * In Disease Detection We can't jeopardy with that person's life by predicting No Heart Disease to a patient with Heart Disease. \n> * We need to have a model with better accuracy and low False Negative Cases\n> * We trained KNN, RandomForest and Logistic Regression with different penality with Logistic Regression having slight advantage\n## With a Better Accuracy and Lower False Negative Case, Logistic Regression is our clear Winner\n","7e716984":"> * We have max number of cases with ST depression level with 0.0, dominated by No Heart Disease Cases \n> * With maximum spread of No Heart Disease Cases densed around 0.0 to 1.0, We can observe a sparse spread for Heart Disease Cases around 0.4 to 2.25","7639ff20":"> * n_neighbors :> That's our 'K'\n> * weights :> Uniform gives same weight to all points while in Distance, closer neighbors of a query point will have a greater influence than neighbors which are further away\n> * p:> if 1, use manhattan_distance to calculate distance , if 2 use euclidean_distance","513c395a":"We are free from these Outliers now. <br>\nTime for Some EDA Stuffs ","cc3d8a99":"## oldpeak Distribuition\n\nST depression induced by exercise relative to rest","d6f428c0":"If you are wondering what's Cross - Validation is the it's :\n\n<img  src=\"https:\/\/scikit-learn.org\/stable\/_images\/grid_search_cross_validation.png\" alt=\"FDR\" width=\"600\" height=\"400\"\/>\n\nBasically, dataset is divided in k equal parts. Then one of that part is used as test part and other for training. <br>\nAnd this step is repeated until every K-fold serve as the test set.","dd0be44f":"Right Now, We have a split with random state as 43 <br>\nLet's take KNN for our example","c0ecc9a7":"<h1><center>DataScience Tackling Heart Diseases !<\/center><\/h1>\n<img src=\"https:\/\/cdn.dnaindia.com\/sites\/default\/files\/styles\/full\/public\/2018\/09\/25\/735506-heart-disease.jpg\" alt=\"drawing\" width=\"600\" height=\"400\"\/>","561be819":"# Modeling\n\n<img  src=\"https:\/\/i.imgflip.com\/2ymba8.png\" alt=\"FDR\" width=\"600\" height=\"400\"\/>\n","3764ae83":"## trestbps Distribution\n\ntrestbps or you can say Resting Blood Pressure is the ratio of Systolic and Diastolic Pressure in Resting State","55edc13a":"'ca' and 'thal' aren't supposed to be float.<br>\nThey are categorial <br>\nHey! Stop wasting my memory","f9747671":"Now, We can move to our analysis..","fa49355a":"We'll be training few models and then select the best model for our use case <br>\nModels will be hypertuned via GridSearchCV because : \n\n<img  src=\"https:\/\/miro.medium.com\/max\/612\/1*iUkbA8Dlj-5B0S8u0oRNbQ.png\" alt=\"FDR\" width=\"600\" height=\"400\"\/>\n","fe3e84ea":"## Evaluation for Logistic Model with L1 Penalty","cda4f210":"## K-Nearest Neighbors\n\n> * Datapoint is classified by a plurality vote of its neighbors, with the object being assigned to the class most common among its k nearest neighbors\n\n<img  src=\"http:\/\/res.cloudinary.com\/dyd911kmh\/image\/upload\/f_auto,q_auto:best\/v1531424125\/KNN_final1_ibdm8a.png\" alt=\"FDR\" width=\"600\" height=\"400\"\/>\n\n\n","221d45cf":"I've talked about Outliers much in details in my [Previous Kernel](https:\/\/www.kaggle.com\/rahulgulia\/data-science-and-cardiovascular-diseases-cvds). You can have a look if you're looking for more info. <br>\nIn short, just remember Outliers are just the values that differs significantly from other observations ","db89ffe2":"## And Yeah, I'm always open for learning. If you want to correct something, advice, wanna share new strageties or techniques then feel free to comment them out. I'll love to hear some great advices \/ feedback from the community.","bafd58a4":"Age is showing an expected pattern :\n> * Younger age group are less prone to Older age group\n> * Age group between 41 and 54 are less prone to Heart Diseases\n> * Age group between 57 and 63 shows a stong relation with the case of suffering from Heart Diseases\n","f8468ab0":"Well, You know my drill :)","987b5fea":"Before futher proceeding with our analysis, let's correct some mistakes as mentioned in the [Discussion Form](https:\/\/www.kaggle.com\/ronitf\/heart-disease-uci\/discussion\/105877) <br>\nQuoting from the discussion form :\n> * Mistake 1 : data 93, 139, 164, 165 and 252 have ca=4 which is incorrect. In the original Cleveland dataset they are NaNs (so they should be removed)\n> * Mistake 2 : data 49 and 282 have thal = 0, also incorrect. They are also NaNs in the original dataset.\n> * Mistake 3 : Our Target Values are swapped i.e. 0 : Heart Disease and 1 : No Heart Disease","803cb171":"Good :) !","74bd0a4a":"## Missing \/ Duplicate Values","f5de91ca":"Dropping 'fbs', 'chol', 'trestbps', 'restecg' ..","4711bf70":"> * We have good quantity of people having 120 and 130 Resting blood Pressure, followed by 140 and 110 ","76644d77":"# Data Analysis\n\n\n<img  src=\"https:\/\/miro.medium.com\/max\/1400\/1*PKXC0FeXQc5LVmqhJ8HnVg.png\" alt=\"FDR\" width=\"600\" height=\"400\"\/>\n","5a19c640":"## chol Distribution\n\nA Serum Cholesterol Level is a measurement of certain elements in the blood, including the amount of high- and low-density lipoprotein cholesterol (HDL and LDL) in a person\u2019s blood.\n\n\n\n| Type    |      Level      |\n|----------|:-------------:|\n| Healthy Serum Cholesterol |  Less than 200 mg\/dL |\n| Healthy LDL Cholesterol |  Less than 130 mg\/dL |\n| Healthy HDL Cholesterol |    Higher than 55 mg\/dL for Women and 45 mg\/dL for Men   |\n| Healthy Triglycerides | Less than 150 mg\/dL |","6ada2749":"Commenting out Hypertuning part to save time. <br>Feel free to uncomment and tune some paramteres by yourself. Who knows you may find better results than mine ;)","b5ef5689":"We'll be using :\n> * cross_val_score :> Evaluate a Score by Cross-Validation\n> * classification_report :> Text Report showing the Main Classification Metrics. We'll gonna draw Confussion Matrix too","82c8853c":"# Model Evaluation\n\nTime to evaluate to find the best model for our use-case\n\n<img  src=\"https:\/\/im.indiatimes.in\/content\/2018\/Mar\/_1521531221.jpg\" alt=\"FDR\" width=\"600\" height=\"400\"\/>\n\n","647b75fb":"Well, We are blessed with a balanced dataset :)\n\n\n<img  src=\"https:\/\/scontent.fdel25-1.fna.fbcdn.net\/v\/t1.0-9\/48391658_625132244607946_7405943289678921728_n.png?_nc_cat=109&_nc_sid=730e14&_nc_ohc=rTg1qpGq8tMAX_hkJ7h&_nc_ht=scontent.fdel25-1.fna&oh=96e5ed6a028415dfe2dc5d17704c790d&oe=5EF44D22\" alt=\"FDR\" width=\"600\" height=\"400\"\/>\n","2213530d":"## Feature Scaling\n\n<img  src=\"https:\/\/miro.medium.com\/max\/1400\/1*CpKFmbqZdjgC5B7eCFUnkw.jpeg\" alt=\"FDR\" width=\"600\" height=\"400\"\/>\n\n\nWe have multiple feature with various scales. We don't want our model to give priority to smaller \/ bigger values just because of difference in scale. <br>\nHence, Feature Scaling is performed during the data pre-processing to handle highly varying magnitudes or values or units","75bf1808":"Again We'll be using StandardScaler\n\n<img  src=\"https:\/\/miro.medium.com\/max\/992\/1*dZlwWGNhFco5bmpfwYyLCQ.png\" alt=\"FDR\" width=\"600\" height=\"400\"\/>\n<br>\ni.e Mean as 0 and Standard Deviation as 1","266cd632":"As shown by Horizontal lines, We have 5 and 2 null values in ca and thal feature respectively. <br><br>\nWe'll gonna handle them in our Feature Engineering Section. Now let's move further..","3090dc38":">So, this Dataset contains the following Features :\n* age :> Age of the Patient in Years\n* sex :> Gender of the Patient\n* cp :> Chest Pain Type\n* trestbps :> Resting Blood Pressure in mm Hg\n* chol :> Serum Cholestoral in mg\/dl (Measurement of certain elements in the blood)\n* fbs :> Fasting Blood Sugar Lower than 100 mg\/dL is Normal, 100 mg\/dL - 125 mg\/dL is considered prediabetes. 125 mg\/dL + is cosidered to have Diabetes\n* restecg :> Resting Electrocardiographic Results (Test that measures the Electrical Activity of the Heart)\n* thalach :> Maximum Heart Rate achieved during Thalium Stress Test\n* exang :> Exercise induced angina (yes or no)\n* oldpeak :> ST depression induced by exercise relative to rest\n* slope :> Slope of peak exercise ST segment \n* ca :> Number of major vessels colored by Fluoroscopy\n* thal :> Thalium Stress Test result\n* target :> Target Value (Patient having Heart Disease or Not)","e5bcf191":"Clearly :\n> * 'cp', 'thalach', 'slope' shows good amount of positive correlation with our target\n> * 'exang', 'oldpeak', 'ca', 'thal', 'sex', 'age' shows good amount of negative correlation with our target\n> * 'fbs' being the lowest, 'chol', 'trestbps', 'restecg' carries low correlation with our target","eb59b82d":"With such a big difference between 75% percentile and max value, and 25% percentile and min value, We do possibly observe some outliers in 'trestbps', 'chol', 'thalach', 'oldpeak'","fbfad6df":"## Dealing with Missing Values","b409e683":"> * Our Age doesn't contain Outliers. I'm proud of you man :)\n> * Though We observe some outliers in our remaining Continuous features\n\n<img  src=\"https:\/\/media.makeameme.org\/created\/heisenberg-approves-knhd1a.jpg\" alt=\"FDR\" width=\"600\" height=\"400\"\/>","a2adec3a":"First of all, it's time for a classical split...","9143f715":"Since excluding outliers can cause your results to become statistically significant, We'll gonna drop our datapoints with Outliers","326743e4":"We'll gonna explore about distribution of our Categorial Features based on the Target Variable.","fd583c33":"Hey Mr. Duplicate ! Found you :) ","02e4d34d":"## thalach Distribution\n\nMaximum Heart Rate achieved during Thalium Stress Test <br>\nYour target heart rate during a stress test depends on your age. For adults, the maximum predicted heart rate is 220 minus your age. So, if you're 40 years old, the maximum predicted heart rate is 220 \u2013 40 = 180","fe271966":"## Evaluation for Logistic Model with L2 Penalty","06331d38":"> * C :> Defines Strength of regularization ( smaller values specify stronger regularization )\n> * penalty :> Used to specify the norm used in the penalization\n> * solver :> Algorithm to use in the optimization problem. Different Solver supports different penalty. Hence we cane 3 cases here\n> * l1_ratio :> Combination of L1 and L2(l1_ratio=0 means l2 and l1_ratio=1 means l1","2955cbfa":"## Evaluation for Logistic Model with Elasticnet Penalty","ce3f7146":"> * oldpeak feautre form quite a linear seperatable relation with the remaining continuous features for Heart and No Heart Disease Cases\n> * While thalach forms mild seperation, other features doesn't form any clear seperation","65039137":"## Logistic Regression\n\n> * Uses a Logistic Function to Model a Categorical Dependent variable\n\n<img  src=\"https:\/\/miro.medium.com\/max\/2400\/1*RqXFpiNGwdiKBWyLJc_E7g.png\" alt=\"FDR\" width=\"600\" height=\"400\"\/>\n","4bf1b2d6":"## Categorial Features","136dec56":"## Age Distribution","efbd98d0":"> * We can observe a mild seperation in the spread between Heart Disease and No Heart Disease caused by Maximum Heart Rate achieved during Thalium Stress Test \n> * Patient with higher value of Maximum Heart Rate achieved during Thalium Stress Test are less prone to Heart Diseases as compared to Heart Disease Cases","ecbea8f3":">In our Data Analysis, We will try to analyze to find out the below stuffs :\n* Missing \/ Duplicate Values \n* Find all the Continuous Features\n* Handling Outliers if Any\n* Distribution of the Continuous Features\n* Find all the Discrete Features\n* Cardinality of Discrete Features\n* Relation with Independent and Dependent Features","55348a5a":"## If You like these types of Kernals (DataScience + Memes) then don't forget to Upvote this Kernel. It'll boost my morale, encouraging me to create more similar kinds of Kernels.  <br>\n","263bdcc9":"Time to get some stastical stuffs","44476246":"We have dataset free of Null values (expect the one we introduced) <br>\nThough We can use tradational way of visualizing Missing Values by making a heatmap, but i came across a new library and just to introduce that to everyone, we'll gonna use that ...","1ba06bd7":"## One-Hot Encoding\n\nAlways a good idea to handle your categorial features <br>\nIn the process, we'll be encoding 'cp', 'ca', 'thal', 'slope'\n","02a44ab0":"## Random Forest\n\n> * Consist of combination of Multiple Decision Trees\n> * Each individual tree in the random forest spits out a class prediction and the class with the most votes becomes our model\u2019s prediction\n> * Based on Bagging Technique\n\n<img  src=\"https:\/\/static.javatpoint.com\/tutorial\/machine-learning\/images\/random-forest-algorithm2.png\" alt=\"FDR\" width=\"600\" height=\"400\"\/>\n","1efdfa6c":"<img  src=\"http:\/\/www.quickmeme.com\/img\/a6\/a6ec1cbce3a2b0f01dcdc59ad5f3d5fa7c03c817bcebc49dc570ec6e5dad48a2.jpg\" alt=\"FDR\" width=\"600\" height=\"400\"\/>\n","6c4f25ad":"Time for some Box Plot bois ..","811583f5":"> * n_estimators :> No of Decision Trees to be used\n> * max_depth :> Depth of Each Tree\n> * criterion :> Measure the quality of a split","4971cf03":"Now, Let's change our random_state as 0 and notice the difference..","53f07d5b":"# End Notes\n> * With this We end another analysis on Heart Disease related Dataset. \n> * Feel free to check out my [Previous Kernel](https:\/\/www.kaggle.com\/rahulgulia\/data-science-and-cardiovascular-diseases-cvds) based on similar kind of Dataset\n\n> * It was fun to analyise this data. Seems I'm starting loving Medical Datasets. There was alot of learning opportunity and yea came accross to quite \"Data Science Friendly Memes\" :D  <br>\n\n\n","3cddf91b":"## But Why Not Evaluate on Test Set from train_test_split ?\n\n> * In train_test_split, we get our accuracy according to the split of the data\n> * If we change re-split in different order, then we'll get another new accuracy\n\nWait wait ! Let me give you an example ..","2d793b6a":"That's not fair. We can hardly observe 'oldpeak' :\/","533706a9":"After going through the description provided by the Distributer, Following information is gathered :\n> * sex (gender): 2 Categorial Values { 1 : Male, 0 : Female )\n> * cp (Chest Pain Type): 4 Categorial Values { 0 : asymptomatic, 1 : atypical angina, 2 : non-anginal pain, 3 : typical angina  )\n> * fbs (Fasting Blood Sugar): 2 Categorial Values { 1 : true if fbs greater than 120 mg\/dl, 0 : false}\n> * restecg (Resting Electrocardiographic Results): 3 Categorial Values { 0 : showing probable or definite left ventricular hypertrophy, 1 : normal, 2 : having ST-T wave abnormality\n> * exang (Exercise induced angina): 2 Categorial Values { 1 : yes, 0 : no }\n> * slope (Slope of peak exercise ST segment): 3 Categorial Values { 0 : downsloping, 1 : flat, 2 : upsloping }\n> * ca (Number of major vessels colored by Fluoroscopy):  4 Categorial Values ranging from 0 - 3\n> * thal (Thalium Stress Test result): 3 Categorial Values { 1 : fixed defect, 2 : normal, 3 :  reversable defect }\n> * target (Target Feature): 2 Categorial Values { 1 : No Heart Disease, 0 : Heart Disease }","0a383720":"## Dealing with Mistake 1\n> * It's even mentioned in the dataset that value of 'ca' ranges from 0-3. So, clearly value 4 represents and error","f95d3728":"# Importing Necessary Libaraies","5501c7da":"Now, Our Data looks like this :","365b53b6":"# Another CVDs Dataset !\nSo, We have one more Dataset related to Heart Diseases. We tackled similar kind of problem in our [Previous Kernel](https:\/\/www.kaggle.com\/rahulgulia\/data-science-and-cardiovascular-diseases-cvds) based on Cardiovascular Disease Dataset provided by [Svetlana Ulianova](https:\/\/www.kaggle.com\/sulianova). <br><br>\nThis time we have Heart Disease UCI Dataset but after going through some discussion forms for this dataset, I realized this dataset is quite different to the [Original Dataset](https:\/\/archive.ics.uci.edu\/ml\/datasets\/heart+Disease) without updating necessary changes in the dataset description for some unknown reasons. <br><br>\nKeeping above issue in the mind, We will be following this [Discussion Form](https:\/\/www.kaggle.com\/ronitf\/heart-disease-uci\/discussion\/105877) as our reference for our features in this Kernel. A small shoutout to our Saviour [IntiPic](https:\/\/www.kaggle.com\/intipic). <br><br>\nI'll be revising some points I mentioned in my [Previous Kernel](https:\/\/www.kaggle.com\/rahulgulia\/data-science-and-cardiovascular-diseases-cvds), following the similar kind of pipeline<br><br>\nBy the way, if you're wondering why I used this specific pic in the title then it's because this pic reminded me of my favourite character Walter White from Breaking Bad series :D !<br><br>\n\n<center><img src=\"https:\/\/i.insider.com\/5dade9bc045a3139e8686c33?width=1300&format=jpeg&auto=webp\" alt=\"drawing\" width=\"600\" height=\"400\"\/><\/center><br>\n\n\nKeeping all jokes apart, Let's get it started !\n","6c27a6f2":"## Feature Selection\n\n<img  src=\"https:\/\/memegenerator.net\/img\/instances\/80593855\/with-many-features-comes-great-feature-selection.jpg\" alt=\"FDR\" width=\"600\" height=\"400\"\/>\n\nHaving irrelevant features in your data can decrease the accuracy of the models and make your model learn based on irrelevant features. <br>\nSo, We do feature selection ( automatically or manually ) to select good data","d426b5b8":"Let's Dive into our CSV File to get a glance of what we are dealing with ...","7acac517":"## Dealing with Mistake 2\n\n> * Data 49 and 282 have thal = 0, also incorrect. They are also NaNs in the original dataset.","bdaff8ec":"# Feature Engineering\n\n<img  src=\"https:\/\/img-a.udemycdn.com\/course\/750x422\/1304050_ee0f_8.jpg\" alt=\"FDR\" width=\"600\" height=\"400\"\/>\n","be0b296e":"## Continuous Features"}}