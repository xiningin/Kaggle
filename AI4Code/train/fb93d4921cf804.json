{"cell_type":{"771432eb":"code","5ecb3e4e":"code","8c7fa41f":"code","9f76b5b8":"code","0f14eb3c":"code","5f30a77f":"code","324029d0":"code","7eb256d9":"code","464fb4bc":"code","c059f6cd":"code","8da7c88e":"code","985be543":"code","615ed596":"code","19ce8259":"code","e828e506":"code","a1b848c4":"code","e6e7931b":"code","643eac02":"code","b3c7f48a":"code","0b2c7fab":"code","acbdeda9":"code","c89c4778":"code","aef27138":"code","649a8c21":"code","dbd31501":"code","00212a11":"code","ea5bc6f6":"code","66026be7":"code","290836bd":"code","3541ec8a":"code","42ad984f":"code","f68ebacd":"markdown","06259814":"markdown","57be74ab":"markdown","d86add93":"markdown","e834c49d":"markdown"},"source":{"771432eb":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session\n\n# https:\/\/www.kaggle.com\/anmolkumar\/health-insurance-cross-sell-prediction\/notebooks","5ecb3e4e":"train = pd.read_csv('\/kaggle\/input\/health-insurance-cross-sell-prediction\/train.csv')\ntest = pd.read_csv('\/kaggle\/input\/health-insurance-cross-sell-prediction\/test.csv')","8c7fa41f":"def eda(dfA, all=False, desc='Exploratory Data Analysis'):\n    print(desc)\n    print(f'\\nShape:\\n{dfA.shape}')\n    print(f'\\nIs Null: {dfA.isnull().sum().sum()}')\n    print(f'{dfA.isnull().mean().sort_values(ascending=False)}')\n    dup = dfA.duplicated()\n    print(f'\\nDuplicated: \\n{dfA[dup].shape}\\n')\n    try:\n        print(dfA[dfA.duplicated(keep=False)].sample(4))\n    except:\n        pass\n    if all:  # here you put yours prefered analysis that detail more your dataset\n\n        print(f'\\nDTypes - Numerics')\n        print(dfA.describe(include=[np.number]))\n        print(f'\\nDTypes - Categoricals')\n        print(dfA.describe(include=['object']))\n\n        # print(df.loc[:, df.dtypes=='object'].columns)\n        print(f'\\nHead:\\n{dfA.head()}')\n        print(f'\\nSamples:\\n{dfA.sample(2)}')\n        print(f'\\nTail:\\n{dfA.tail()}')","9f76b5b8":"eda(train)","0f14eb3c":"train.columns = train.columns.str.lower()","5f30a77f":"train.head()","324029d0":"eda(test)","7eb256d9":"test.columns = test.columns.str.lower()\ntest.head()","464fb4bc":"import seaborn as sns\nimport matplotlib.pyplot as plt","c059f6cd":"gender = train.gender.unique()\ngender","8da7c88e":"m = train[train.gender == gender[0]]['gender'].shape[0]\nf = train[train.gender == gender[1]]['gender'].shape[0]","985be543":"plt.pie([m,f], labels=gender, autopct='%1.1f%%')","615ed596":"age = train[['id', 'age']].groupby('age').count()\nage","19ce8259":"fig, ax1 = plt.subplots( sharey=True, figsize=(15,5))\nsns.barplot(x=age.index, y=age.id.values, ax=ax1).set_title('Age')","e828e506":"def sepColumns(dataset):\n    num = []\n    cat = []\n    for i in dataset.columns:\n        if dataset[i].dtype == 'object':\n            cat.append(i)\n        else:\n            num.append(i)\n    return num, cat","a1b848c4":"num, cat = sepColumns(train)\ntrain[cat]","e6e7931b":"vuCat = dict()\nfor c in cat:\n    v = train[c].unique().tolist()\n    vuCat[c] = v\nprint(vuCat)","643eac02":"for vc in vuCat:\n#     print(vc, vuCat[vc])\n    newCol = f'{vc}_N'\n    train[newCol] = train[vc].apply(lambda x: vuCat[vc].index(x))","b3c7f48a":"train.head()","0b2c7fab":"numTrain, catTrain = sepColumns(train)\ntrain[numTrain]","acbdeda9":"test.head()","c89c4778":"numTest, catTest = sepColumns(test)\ntest[catTest]","aef27138":"vuCatTest = dict()\nfor c in catTest:\n    v = test[c].unique().tolist()\n    vuCatTest[c] = v\nprint(vuCatTest)","649a8c21":"for vc in vuCatTest:\n    newCol = f'{vc}_N'\n    test[newCol] = test[vc].apply(lambda x: vuCatTest[vc].index(x))","dbd31501":"test","00212a11":"numTest, catTest = sepColumns(test)\ntest[numTest]","ea5bc6f6":"def correlation(df, varT, xpoint=-0.5, showGraph=True):\n    corr = df.corr()\n    print(f'\\nFeatures correlation:\\n'\n          f'Target: {varT}\\n'\n          f'Reference.: {xpoint}\\n'\n          f'\\nMain features:')\n    corrs = corr[varT]\n    features = []\n    for i in range(0, len(corrs)):\n        if corrs[i] > xpoint and corrs.index[i] != varT:\n            print(corrs.index[i], f'{corrs[i]:.2f}')\n            features.append(corrs.index[i])\n    if showGraph:\n        fig, ax1 = plt.subplots( sharey=True, figsize=(15,10))\n        sns.heatmap(corr,\n                    annot=True, fmt='.2f', vmin=-1, vmax=1, linewidth=0.01,\n                    linecolor='black', cmap='RdBu_r', ax=ax1\n                    )\n        plt.title('Correlations between features w\/ target')\n        plt.show()\n    return features","66026be7":"varTarget = 'response'\nvarFeatures = correlation(train[numTrain], varTarget, 0.01)","290836bd":"# ML Algoritmos\nfrom sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, AdaBoostRegressor\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.linear_model import LinearRegression, LogisticRegression, PoissonRegressor\nfrom sklearn.svm import SVR\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.dummy import DummyRegressor\n\n# ML selecao de dados de treino e teste\nfrom sklearn.model_selection import train_test_split\n# calcular o menor erro medio absoluto entre 2 dados apresentados\nfrom sklearn.metrics import mean_absolute_error","3541ec8a":"# I used this to choose what the Regressor fit better with data\n\n# nrs = np.random.randint(1,43)\n# nest = np.random.randint(1,43)\n# regressors = [\n#         LogisticRegression(random_state=nrs),\n#         DecisionTreeRegressor(random_state=nrs),\n#         RandomForestRegressor(n_estimators=nest, random_state=nrs),\n#         SVR(C=1.0, epsilon=0.2),\n#         LinearRegression(),\n#         GradientBoostingRegressor(n_estimators=nest, random_state=nrs),\n#         PoissonRegressor(),\n#         DummyRegressor(strategy=\"mean\"),\n#         GaussianNB(),\n#         AdaBoostRegressor(n_estimators=nest, random_state=nrs)\n#     ]\n\n# X = train[varFeatures]\n# y = train[varTarget]\n# Xtreino, Xteste, ytreino, yteste = train_test_split(X, y, test_size=0.3, random_state=42)\n\n# reg = []\n# mae = []\n# sco = []\n\n# for regressor in regressors:\n#     modelo = regressor\n#     modelo.fit(Xtreino, np.array(ytreino))\n#     sco.append(modelo.score(Xtreino, ytreino))\n#     previsao = modelo.predict(Xteste)\n#     mae.append(round(mean_absolute_error(yteste, previsao), 2))\n#     reg.append(regressor)\n\n# meuMae = pd.DataFrame(columns=['Regressor', 'mae', 'score'])\n# meuMae['Regressor'] = reg\n# meuMae['mae'] = mae\n# meuMae['score'] = sco\n# meuMae = meuMae.sort_values(by='score', ascending=False)\n\n# print('Best score: ', meuMae[\"Regressor\"].values[0])","42ad984f":"Xtreino = train[varFeatures]\nytreino = train[varTarget]\nXteste = test[varFeatures]\n\nmodelo = LogisticRegression(random_state=44)  #meuMae[\"Regressor\"].values[0]\nmodelo.fit(Xtreino, np.array(ytreino))\nscore = modelo.score(Xtreino, ytreino)\npredict = modelo.predict(Xteste)\n\nprint(f'Score: {score:.2f}')","f68ebacd":"Applying best regressor","06259814":"**correlation**","57be74ab":"**test data**","d86add93":"**Prediction**","e834c49d":"**choosing one regressor**"}}