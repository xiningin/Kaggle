{"cell_type":{"14457fb3":"code","326e5898":"code","0931828a":"code","be4ad1ce":"code","f19a0c3c":"code","077d5878":"code","2739a399":"code","26ae0bbc":"code","256b100c":"code","6ef87ed6":"code","ce24536e":"code","22246f09":"code","1432610e":"code","de09359b":"code","9ef77620":"code","b64f5eaf":"code","7dea87e5":"code","f7bc9a49":"code","2be77ddb":"code","2f3b0f04":"code","9da0017d":"code","99717354":"code","710ef1e4":"code","ba68628e":"code","51870c2f":"code","48653b67":"code","53d3290f":"code","3c5ad85c":"code","74ae825a":"code","c86519ce":"code","1b0c30c0":"code","da063f6c":"code","37a9adac":"code","ad303d8a":"code","c891dbef":"code","2aa52caa":"code","46c69851":"code","75aa8cd1":"code","b0e7f111":"code","c5b86f45":"code","7e44b956":"code","de55e669":"code","2ec6de0a":"markdown","2233b3ea":"markdown","4c792406":"markdown","e57e91e4":"markdown","e31d9d4b":"markdown","c6e0403e":"markdown","5377b872":"markdown","2a93f562":"markdown","f5e62b47":"markdown","0de31752":"markdown","d113a82b":"markdown","0cd88918":"markdown","67e9d832":"markdown","f1438899":"markdown","e87cc793":"markdown","432d5005":"markdown","1ebc6604":"markdown","bf860156":"markdown","ef990e12":"markdown","824c8d6f":"markdown","85af3946":"markdown"},"source":{"14457fb3":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","326e5898":"import numpy as np\nimport pandas as pd\nfrom matplotlib import pyplot as plt\nfrom sklearn.impute import SimpleImputer","0931828a":"# Load data set:\ndf= pd.read_csv('..\/input\/finance-accounting-courses-udemy-13k-course\/udemy_output_All_Finance__Accounting_p1_p626.csv')\ndf.head()","be4ad1ce":"df.info()","f19a0c3c":"#drop out irrelevant columns:\ndf=df.drop(['id','title','url'],axis=1)\ndf=df.drop(['price_detail__currency','discount_price__currency','price_detail__price_string'],axis=1)","077d5878":"# We will drop the first character of 'discount_price__price_string' and then tranform them to intergrate Dtype\ndf['discount_price__price_string']=pd.to_numeric(df['discount_price__price_string'].str[1:],errors='coerce')","2739a399":"df.info()","26ae0bbc":"df.isnull().sum()","256b100c":"df1=df.copy()\n","6ef87ed6":"df1= df1.dropna(axis=0)\ndf1.isnull().sum()","ce24536e":"df2=df.copy()\nmissing= ['discount_price__amount','discount_price__price_string','price_detail__amount'] #list all the missing values columns\nfor i in missing:\n    df2[i]=df2[i].fillna(df2[i].median())# now all the missing data in the column is replace by its column mean ","22246f09":"# check whether all the missing data in the column is replace by its column mean \ndf2['discount_price__amount'].mean()-df['discount_price__amount'].mean()==0","1432610e":"from sklearn.impute import SimpleImputer \ndf3=df.copy()\ndf3=df3.drop(columns=['is_paid','is_wishlisted','published_time','created'])\nimputer=SimpleImputer(missing_values=np.nan, strategy='mean')\nimputer=imputer.fit(df3)\ndf3=imputer.transform(df3)\nprint('Number of missing data is:', np.sum(np.isnan(df3)))","de09359b":"df4=df2.copy()\ndf4=df2.drop(columns=['is_paid','is_wishlisted','published_time','created'])\ncolumn= df4.columns\n#data normalization with sklearn\nfrom sklearn.preprocessing import MinMaxScaler\nnor= MinMaxScaler()\ndf4=nor.fit_transform(df4)\n#transform the array to dataframe \ndf_nor=pd.DataFrame(df4,columns=column)","9ef77620":"#New data frame after normalization\ndf_nor","b64f5eaf":"df5=df2.copy()\ndf5=df2.drop(columns=['is_paid','is_wishlisted','published_time','created'])\ncolumn= df5.columns\n#data normalization with sklearn\nfrom sklearn.preprocessing import StandardScaler\nstdard= StandardScaler()\ndf5=stdard.fit_transform(df5)\n#transform the array to dataframe \ndf_stdard=pd.DataFrame(df5,columns=column)\ndf_stdard","7dea87e5":"df2=df2.drop(columns=['is_paid','is_wishlisted','published_time','created'],axis=1)","f7bc9a49":"dataset =df2[df2.columns]\n","2be77ddb":"fig = plt.figure(figsize =(20, 10))\nplt.boxplot(dataset,labels=column)\nplt.xticks(rotation=45)\nplt.title('original data')\nplt.show()","2f3b0f04":"dataset =df_nor[df_nor.columns]\n","9da0017d":"fig = plt.figure(figsize =(20, 10))\nplt.boxplot(dataset,labels=column)\nplt.xticks(rotation=45)\nplt.title('normalized data')\nplt.show()","99717354":"\ndataset =df_stdard[df_stdard.columns]\n","710ef1e4":"fig = plt.figure(figsize =(20, 10))\nplt.boxplot(dataset,labels=column)\nplt.xticks(rotation=45)\nplt.title('standardized data')\nplt.show()","ba68628e":"import pandas as pd","51870c2f":"df_exam= pd.read_csv('..\/input\/students-performance-in-exams\/StudentsPerformance.csv')\ndf_exam.head()","48653b67":"df_exam.info()","53d3290f":"df_exam['lunch']=df_exam['lunch'].replace(to_replace =\"free\/reduced\", value ='free or reduced')","3c5ad85c":"df_exam","74ae825a":"# display all the elements in each categorical variable\ndisplay(df_exam['race\/ethnicity'].value_counts())\ndisplay(df_exam['gender'].value_counts())\ndisplay(df_exam['lunch'].value_counts())\ndisplay(df_exam['test preparation course'].value_counts())\ndisplay(df_exam['parental level of education'].value_counts())\n","c86519ce":"#Mapping race\/ethnicity\nsize_mapping1= {'group C':1,'group D':2,'group B':3,'group E':4,'group A':5}\n#Mapping gender\nsize_mapping2={'female':1,'male':2}\n#Mapping lunch\nsize_mapping3={'standard':1,\"free or reduced\":2}\n#Mapping test preparation course\nsize_mapping4={'none':1,\"completed\":2}\n#Mapping parental level of education\nsize_mapping5={\"some college\":1,\"associate's degree\": 2,'high school':3,'some high school':4,\"bachelor's degree\":5, \"master's degree\":6 }","1b0c30c0":"df_exam['race\/ethnicity']=df_exam['race\/ethnicity'].map(size_mapping1)\ndf_exam['gender']=df_exam['gender'].map(size_mapping2)\ndf_exam['lunch']=df_exam['lunch'].map(size_mapping3)\ndf_exam['test preparation course']=df_exam['test preparation course'].map(size_mapping4)\ndf_exam['parental level of education']=df_exam['parental level of education'].map(size_mapping5)","da063f6c":"df_exam","37a9adac":"df_score=pd.read_csv('..\/input\/students-performance-in-exams\/StudentsPerformance.csv')","ad303d8a":"df_score.info()","c891dbef":"from sklearn.preprocessing import LabelEncoder\n# creating instance of labelencoder\nlabelencoder = LabelEncoder()\n# Assigning numerical values and storing in another column\nX=['gender','race\/ethnicity','parental level of education','lunch','test preparation course']\nfor x in X:\n    df_score[x]= labelencoder.fit_transform(df_score[x])\ndf_score\n","2aa52caa":"df_encode=pd.read_csv('..\/input\/students-performance-in-exams\/StudentsPerformance.csv')\ndf_encode.info()","46c69851":"\ndf_dummy=df_encode.iloc[:,0:5]\ndf_dummy\n","75aa8cd1":"#get dummy for catergorical values\ndf_dummy=pd.get_dummies(df_dummy)\ndf_dummy","b0e7f111":"# merge with main df df_encode on key values\ndf_new=df_dummy.join(df_encode.iloc[:,5:])\ndf_new","c5b86f45":"df_per=pd.read_csv('..\/input\/students-performance-in-exams\/StudentsPerformance.csv')\ndf_cate=df_per.iloc[:,0:5]\ndf_cate","7e44b956":"#drop the redudant columns\ndf_cate=pd.get_dummies(df_cate,drop_first=True)\ndf_cate","de55e669":"# create the full data set \ndf_student=df_cate.join(df_per.iloc[:,5:])\ndf_student\n                        ","2ec6de0a":"Now use dropna command to eleminate the null data. we would like to drop the row that have null. \n\nSince we will use Importer command later on to remove NaN so I will copy the df data and continue the drop out missing value procedure.\n","2233b3ea":"It is important to understand various option for encoding categorical variables because each approach has its own pros and cons.","4c792406":"There is some redundancy in One-Hot encoding. For instance, in the above Gender One-Hot encoding, a person is either male or female. It can be a cause for multicollinear\n\nSo we only need to use one of these two dummy-coded variables as a predictor. To produce an actual dummy encoding from a DataFrame, we need to add drop_first=True in the get_dummies () command.\n\n","e57e91e4":"## B. Using Label Encoder\nThis approach is very simple and it involves converting each value in a column to a number. ","e31d9d4b":"\nNow the data is pretty much clean. Move to the next step: Handling the Missing data. \n\nThere are various way to solving this problem. The easiest way is to use the dropna() function. Let's give it a try!\n\nFirst, let's see which columns contain null value?","c6e0403e":"## B. STANDARDIZATION","5377b872":"# THE END\n\nThank you for your attention!\n\nI would be happy very much if you can leave any comments to help enhance my work. \n\nCredit: \nhttps:\/\/towardsdatascience.com\/categorical-encoding-using-label-encoding-and-one-hot-encoder-911ef77fb5bd\nhttps:\/\/towardsdatascience.com\/what-is-one-hot-encoding-and-how-to-use-pandas-get-dummies-function-922eb9bd4970\nhttps:\/\/towardsdatascience.com\/introduction-to-data-preprocessing-in-machine-learning-a9fa83a5dc9d","2a93f562":"# **2. DROP NULL VALUES**\n### A. DROP NA WITH PANDAS","f5e62b47":"Now, I will visuailize the original data, normalized data and standardized data","0de31752":"# **6.  MULTICOLLINEARITY **\nMulticollinearity occurs in our dataset when we have features that are strongly dependent on each other.\n\nRemoving collinear X-variables is the simplest method of solving the multicollinearity problem.","d113a82b":"# 5. ONE-HOT ENCODING\nWith One-Hot Encoding, the binary vector arrays representation allows a machine learning algorithm to leverage the information contained in a category value without the confusion caused by ordinality.\n","0cd88918":"### Data Processing is the task of converting data from a given form to a much more usable and desired form i.e. making it more meaningful and informative. \n\n### It's very important to preprocess our data before feeding it into model.\n![](https:\/\/media.geeksforgeeks.org\/wp-content\/uploads\/data-processing.png)\n\n### In this post, I will cover the below concept to processing data in Machine Learning:\n\n* Handling Missing Value\n* Standardization\/Normalization\n* Encoding Categorical Variable\n* Non-linear Tranformation\n* One-Hot Encoding\n* Multicollinearity\n\n\n### I'll use Udemy Course\/Finance Acouting data and Student-Perfomance in exams for my case study. \n\n### Let get started!!!\n","67e9d832":"### B. DROP NA WITH IMPUTATION IN MACHINE LEARNING","f1438899":"Instead of drop the missing data of particular column, we can replace it  mean, median, mode, standard deviation, min & max of that column\nFor the elaboration, I will replace the missing data with the mean of the column it belongs to.","e87cc793":"# 3. SCALING FOR MACHINE LEARNING: NORMALIZATION & STANDARDIZATION\n\n## A. NORMALIZATION \n\n* Normalization is a scaling technique in which values are shifted and rescaled so that they end up ranging between 0 and 1. It is also known as Min-Max scaling.\n\n* Normalization is good to use when you know that the distribution of your data does not follow a Gaussian distribution. ","432d5005":"  # **1. CLEAN THE DATA**","1ebc6604":"In this case study, I will introduce 3 ways to handling categorical variables:\n* map() function \n* Label Encoder in Machine learning\n* Using dummies values approach","bf860156":"* Standardization is another scaling technique where the values are centered around the mean with a unit standard deviation. \n\n* This means that the mean of the attribute becomes zero and the resultant distribution has a unit standard deviation.\n\n* Standardization, on the other hand, can be helpful in cases where the data follows a Gaussian distribution. However, this does not have to be necessarily true.\n\n* Unlike normalization, standardization does not have a bounding range. So, even if you have outliers in your data, they will not be affected by standardization.","ef990e12":"## A. map() function","824c8d6f":"We can simply perform imputation by using the SimpleImputer class provided by sklearn","85af3946":"# 4. HANDLING CATEGORICAL VARIABLES "}}