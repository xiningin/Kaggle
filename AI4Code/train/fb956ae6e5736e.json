{"cell_type":{"295f5b1e":"code","b9d86100":"code","050d6400":"code","58e7f867":"code","91c7377c":"code","57ba9298":"code","b935e0dc":"code","4b020cb9":"code","47d3e374":"code","28dec264":"code","aa26f35c":"code","7986b0cf":"code","5fb71287":"code","d0d28728":"code","2961491c":"code","632ad3bd":"code","04ad6aa6":"code","2fc1c0a0":"code","6978fae5":"code","d94b0699":"code","87510dc0":"code","9120a206":"code","f8cd0a98":"code","1ff4b74c":"code","1a0bc4fa":"code","25f11dc1":"code","9577b0d4":"code","2fd14499":"code","bd9460eb":"code","c4624922":"code","b9877737":"code","be77e312":"code","4992a5f3":"code","d654010e":"code","a937a1d9":"code","9aef1999":"code","3373e555":"code","8b38456e":"code","3dbe4fb3":"code","df351d16":"code","b09b8fd8":"code","97cbc649":"code","08551242":"code","9b1ce419":"code","134ebdc2":"code","af0c4b9e":"code","4add0e18":"code","2c010a78":"code","1cc7ab22":"markdown","fdd54ca4":"markdown","0e6acf69":"markdown","20f3c55c":"markdown","6959c89e":"markdown","cfc60294":"markdown","07049ab5":"markdown","c6356526":"markdown","de6be73b":"markdown","7fdc6795":"markdown","e625a043":"markdown","41a7f1ee":"markdown","ed49d894":"markdown","94cc0923":"markdown","d32737fb":"markdown"},"source":{"295f5b1e":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# dataset link: https:\/\/www.kaggle.com\/nitinchoudhary012\/engineering-graduate-salary-prediction\nsalary = pd.read_csv(\"..\/input\/engineering-graduate-salary-prediction\/Engineering_graduate_salary.csv\")\nsalary.head()","b9d86100":"print(salary.info())\n\n# Checking for outliers\nprint(salary.describe().T)","050d6400":"def draw_missing_data_table(df):\n    total = df.isnull().sum().sort_values(ascending=False)\n    percent = (df.isnull().sum()\/df.isnull().count()).sort_values(ascending=False)\n    missing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\n    return missing_data","58e7f867":"draw_missing_data_table(salary)","91c7377c":"salary.shape","57ba9298":"salary.columns","b935e0dc":"# dropping features which do not make any sense to predict salary\nsalary.drop(['ID', '10board','12graduation','12board' ,'CollegeID' , 'CollegeCityID','CollegeState', 'CollegeCityTier',\"DOB\",\"Gender\",\"GraduationYear\",\"CollegeTier\"], axis = 1, inplace = True)","4b020cb9":"salary.shape","47d3e374":"# replace -1 and then fill missing values\nsalary.replace(-1, np.NaN,inplace=True)","28dec264":"# lets check the missing values again\nsalary.isnull().sum()","aa26f35c":"# list of columns with null values \nmissing_values_columns = [col for col in salary.columns if salary.isnull().sum()[col] > 0]","7986b0cf":"# function for missing values substitution\ndef fill_missing_values(df,missing_values_columns):\n    data = df.copy()\n    '''Filling missing values with mean'''\n    for col in missing_values_columns:\n        data[col] = data[col].fillna(data[col].mean())\n     \n    return data\n\n# lets use this function to fill the missing values\nsalary = fill_missing_values(salary,missing_values_columns)","5fb71287":"plt.figure(figsize=(16,12))\nsns.heatmap(salary.corr(),annot=True,cmap='viridis')\nplt.show()","d0d28728":"plt.figure(figsize = (12, 6))\n\nplt.subplot(121)\nplt.title('Salary Distribuition')\nsns.distplot(salary['Salary'])\n\nplt.subplot(122)\ng1 = plt.scatter(range(salary.shape[0]), np.sort(salary.Salary.values))\ng1= plt.title(\"Salary Curve Distribuition\", fontsize=15)\ng1 = plt.xlabel(\"\")\ng1 = plt.ylabel(\"Salary\", fontsize=12)\n\nplt.subplots_adjust(wspace = 0.3, hspace = 0.5,\n                    top = 0.9)\nplt.show()","2961491c":"# checking the count of unique specialization present in dataframe\nsalary.Specialization.value_counts()","632ad3bd":"# create the copy of dataframe\ndata = salary.copy()\n# count of unique categories in specialization\nvalue_count = data['Specialization'].value_counts()\n\ndef map_to_other_specialization(var):\n    ''' if count of unique category is less than 10, replace the category as other '''\n    if var in value_count[value_count<=10]:\n        return 'other'\n    else:\n        return var\n    \n# apply the function to specialization to get the results    \nsalary['Specialization'] = salary.Specialization.apply(map_to_other_specialization)","04ad6aa6":"# electronics & instrumentation engineering is repeating here with slight change in name so converting it into one category\nsalary['Specialization'] = salary['Specialization'].str.replace('electronics & instrumentation eng','electronics and instrumentation engineering')","2fc1c0a0":"# count plot of unique categories in specialization \nplt.figure(figsize = (16, 8))\ntotal = float(len(salary))\nax = sns.countplot(x='Specialization',data=salary)\nfor p in ax.patches:\n    height = p.get_height()\n    ax.text(p.get_x()+p.get_width()\/2.,\n            height + 1,\n            '{:1.2f}%'.format((height\/total) * 100),\n            ha=\"center\",fontsize=10) \nplt.xticks(rotation = 90)\nplt.show()","6978fae5":"# average salary by specialization and sort them in decreasing order\navg_sal_per_specialization = salary.groupby('Specialization').agg(mean_salary =(\"Salary\", 'mean')).sort_values(by = 'mean_salary',ascending=False)\n\n# barplot of mean salary and specialization\nplt.figure(figsize = (12, 6))\nsns.barplot(x = avg_sal_per_specialization.index,y = 'mean_salary',data = avg_sal_per_specialization,palette='rocket')\nplt.xticks(rotation = 90)\nplt.show()","d94b0699":"# creating list of categorical columns for one hot encoding\ncategorical_columns = [col for col in salary.columns if salary.dtypes[col] == 'object']\n\n# creating list of numerical columns to standardized data \nnumerical_columns = [col for col in salary.columns if (salary.dtypes[col] != 'object')]\n\nprint('Numerical Features are : ',numerical_columns)\nprint('\\n')\nprint('Categorical Features are : ',categorical_columns)","87510dc0":"# one hot encoding function for categorical features \ndef onehot_encoder(df, cols):\n    df = df.copy()\n    for col in cols:\n        dummies = pd.get_dummies(df[col])\n        # concatenating dummies and original dataframe\n        df = pd.concat([df, dummies], axis=1)\n        \n        # dropping original coolumns for which encoding is applied.\n        df.drop(col, axis=1,inplace=True)\n    return df","9120a206":"salary = onehot_encoder(salary,categorical_columns)","f8cd0a98":"# lets drop one column from each encoded categorical feature to avoid dummy trap \nsalary.drop(['M.Sc. (Tech.)','biotechnology'],axis=1,inplace=True)","1ff4b74c":"from sklearn.preprocessing import MinMaxScaler\nscaler = MinMaxScaler()\n\n# Applying scaler() to all the columns except the 'yes-no' and 'dummy' variables\nnum_vars = ['10percentage', '12percentage', 'collegeGPA', 'English','Logical','Quant','ComputerProgramming','ElectronicsAndSemicon','ComputerScience','MechanicalEngg','ElectricalEngg','TelecomEngg','CivilEngg','Salary']\nsalary[num_vars] = scaler.fit_transform(salary[num_vars])\n\nsalary","1a0bc4fa":"salary.head(15)","25f11dc1":"# Create dummy variables of Degree, Gender and Specialization\n\n# dg = pd.get_dummies(salary['Degree'])\n\n# # Adding the sp to the original salary dataframe\n# salary = pd.concat([salary, dg], axis = 1)\n\n# # Dropping 'Degree' as we have created the dummies for it\n# salary.drop(['Degree'], axis = 1, inplace = True)\n\n# gd = pd.get_dummies(salary['Gender'])\n\n# # Adding the gd to the original salary dataframe\n# salary = pd.concat([salary, gd], axis = 1)\n\n# # Dropping 'Gender' as we have created the dummies for it\n# salary.drop(['Gender'], axis = 1, inplace = True)\n\n\n# sp = pd.get_dummies(salary['Specialization'])\n\n# # Adding the sp to the original salary dataframe\n# salary = pd.concat([salary, sp], axis = 1)\n\n# # Dropping 'Specialization' as we have created the dummies for it\n# salary.drop(['Specialization'], axis = 1, inplace = True)\n\nsalary.head()\n","9577b0d4":"from sklearn.model_selection import train_test_split\n\n# We specify random seed so that the train and test data set always have the same rows, respectively\nnp.random.seed(0)\ndf_train, df_test = train_test_split(salary, train_size = 0.7, test_size = 0.3, random_state = 100)\n","2fd14499":"df_train.head()","bd9460eb":"# Dividing the training data set into X and Y\ny_train = df_train.pop('Salary')\nX_train = df_train","c4624922":"X_train.head()","b9877737":"#Build a linear model\n\nimport statsmodels.api as sm\nX_train_lm = sm.add_constant(X_train)\n\n# lr_1 = sm.OLS(y_train, X_train_lm).fit()\nlr_1 = sm.OLS(y_train, X_train_lm.astype(float)).fit()\n# lr_1.summary()","be77e312":"lr_1.summary()","4992a5f3":"# Checking for the VIF values of the variables. \nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\n\n# Creating a dataframe that will contain the names of all the feature variables and their VIFs\nvif = pd.DataFrame()\nvif['Features'] = X_train.columns\nvif['VIF'] = [variance_inflation_factor(X_train.values, i) for i in range(X_train.shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","d654010e":"X_train.pop(\"10percentage\")","a937a1d9":"X_train_lm = sm.add_constant(X_train)\n\n# lr_1 = sm.OLS(y_train, X_train_lm).fit()\nlr_1 = sm.OLS(y_train, X_train_lm.astype(float)).fit()\n# lr_1.summary()","9aef1999":"# Checking for the VIF values of the variables. \nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\n\n# Creating a dataframe that will contain the names of all the feature variables and their VIFs\nvif = pd.DataFrame()\nvif['Features'] = X_train.columns\nvif['VIF'] = [variance_inflation_factor(X_train.values, i) for i in range(X_train.shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","3373e555":"print(lr_1.summary())","8b38456e":"y_train_salary = lr_1.predict(X_train_lm)\n# Plot the histogram of the error terms\nfig = plt.figure()\nsns.distplot((y_train - y_train_salary), bins = 20)\nfig.suptitle('Error Terms', fontsize = 20)                  # Plot heading \nplt.xlabel('Errors', fontsize = 18)                         # X-label","3dbe4fb3":"# Dividing the training data set into X and Y\ny_test = df_test.pop('Salary')\nX_test = df_test","df351d16":"X_test.pop(\"10percentage\")","b09b8fd8":"X_test.head()","97cbc649":"# Adding constant variable to test dataframe\nX_test = sm.add_constant(X_test)\n\n\n# Making predictions using the final model\ny_pred = lr_1.predict(X_test)","08551242":"from sklearn.metrics import r2_score\nr2_score(y_true = y_test, y_pred = y_pred)","9b1ce419":"# Adding constant variable to train dataframe\nX_train = sm.add_constant(X_train)\n\n\n# Making predictions using the final model\ny_pred_train = lr_1.predict(X_train)","134ebdc2":"from sklearn.metrics import r2_score\nr2_score(y_true = y_train, y_pred = y_pred_train)","af0c4b9e":"import sklearn.metrics as sm\nprint(\"Explain variance score =\", round(sm.explained_variance_score(y_test, y_pred), 2)) \nprint(\"R2 score =\", round(sm.r2_score(y_test, y_pred), 2))","4add0e18":"from xgboost import XGBRegressor\nxgb = XGBRegressor()\nxgb.fit(X_train, y_train)\ny_pred_xgb = xgb.predict(X_test)\n\nxgb_r2 = xgb.score(X_test, y_test)\n\nprint(\"XGBoost R^2 Score: {:.5f}\".format(xgb_r2))","2c010a78":"from sklearn.linear_model import LinearRegression\nlinear_reg = LinearRegression()\nlinear_reg.fit(X_train, y_train)\ny_pred_linear_reg = linear_reg.predict(X_test)\n\nlinear_reg_r2_score = linear_reg.score(X_test, y_test)\n\nprint(\"Linear Regression R^2 Score: {:.4f}\".format(linear_reg_r2_score))","1cc7ab22":"Now, we have to see if the final predicted model is best fitted or not. To do that, we\u2019ll calculate the R\u00b2 value for the expected test model.\nWe do that by importing the r2_score library from sklearn","fdd54ca4":"Exploratory Data Analysis","0e6acf69":"We have to see the multicollinearity between the variables. We do that by calculating the VIF value.\nVariance Inflation Factor or VIF is a quantitative value that says how much the feature variables are correlated with each other. It is an extremely important parameter to test our linear model. The formula for VIF is:\nVIF = 1\/(1-Ri\u00b2)\n\nWe consider the variables generally having a value <5.\n\nDrop variables having high p and high VIF.\n\nrepeat this process till every column\u2019s p-value is <0.005 and VIF is <5\n(optional here)\n\nWe will not drop Engineering Specializations here.","20f3c55c":"Trying LinearRegression and XGBoost Models using Libraries","6959c89e":"Most of the graduates having salaries under 10 lakhs.\nLong tail of distribution is longer on right hand side as compared to left hand side which shows that distribution is positively skewed.","cfc60294":"We\u2019ll add the variables except for the target variable to the model","07049ab5":"Splitting the Data into two different sets","c6356526":"We have to scale the test data.","de6be73b":"Before making predictions, we have to see whether the error terms are normally distributed or not. We\u2019ll do that by using Residual Analysis.\nError-terms = y_actual - y_predicted\nThe difference between the actual y-value and the predicted y-value using the model at that particular x-value is the error term.\n\nResidual Analysis of the train data\nWe have to check if the error terms are normally distributed (which is one of the major assumptions of linear regression); let us plot the error terms\u2019 histogram.","7fdc6795":"Correlation Analysis","e625a043":"trying XGBoost Model","41a7f1ee":"Since the R\u00b2 values for both the train and test data are almost equal, the model we built is the best-fitted model.\n\nFor the trained model, R\u00b2 calculated again to compare the results,\nelse the R\u00b2 value for trained model can also be seen in the Model Summary.","ed49d894":"Analysis of Variable Salary","94cc0923":"See for any null values in the dataset using .info() also, we have to check for any outliers using .describe()\n","d32737fb":"As we can see, the error terms resemble closely to a normal distribution. So we can move ahead and make predictions using the model in the test dataset."}}