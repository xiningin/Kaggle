{"cell_type":{"27594706":"code","ea2e27e4":"code","e382bcb5":"code","4330633b":"code","af0f0b25":"code","c3f1d778":"code","ba6f920b":"code","90f64ef8":"markdown","f77d0745":"markdown","62941e57":"markdown","088129d4":"markdown","86da7e49":"markdown","5edef771":"markdown","c58878ca":"markdown","7821bcf8":"markdown","e49c0428":"markdown"},"source":{"27594706":"%tensorflow_version 1.x #this works only in google colab","ea2e27e4":"import tensorflow as tf\ntf.__version__","e382bcb5":"import re\nfile_name = '\/content\/Beyond Good and Evil.txt'\nfile1 = open(file_name,\"rt\",encoding='utf8')\nfile = file1.readlines()\nfile1.close()\nsentences = []\nstart = 0\nfor line in file:\n  if line=='PREFACE\\n': #to start the text from Preface\n    start = 1\n  if start!=1:\n    continue \n  if line == 'FROM THE HEIGHTS\\n':\n    break\n  if line.startswith('CHAPTER'):\n    continue\n  # Removing unwanted punctuations\n  line = re.sub('--',' ',line)\n  words = line.split()\n  if len(words)<=2:\n    continue\n  # Removing numbers from the beginning of the line\n  if words[0][0].isnumeric(): \n    newline = ' '.join(words[1:]) +'\\n'\n  else:\n    newline = line\n  sentences.append(newline)\n# Combining the lines\ntraining_data = ''.join(sentences)","4330633b":"try:\n  newfile = open(\"\/content\/training_file.txt\",'x')\n  newfile.close()\nexcept:\n  print('Already Exists')\ntraining_file = open(\"\/content\/training_file.txt\",'w')\ntraining_file.write(training_data)\ntraining_file.close()","af0f0b25":"!pip install gpt-2-simple","c3f1d778":"import gpt_2_simple as gpt2\nimport os\n\nmodel_name = \"124M\" #any other model can also be chosen but 124M works fine here\nif not os.path.isdir(os.path.join(\"models\", model_name)):\n  gpt2.download_gpt2(model_name=model_name)   # model will be saved into current directory under \/models\/124M\/\n\n\nfile_name = '\/content\/training_file.txt'\n\n    \n\nsess = gpt2.start_tf_sess()\ngpt2.finetune(sess,\n              file_name,\n              model_name=model_name,\n              steps=500)","ba6f920b":"prefix = 'The purpose of life is'\ngpt2.generate(sess, model_name='\/content\/checkpoint\/run1',prefix = prefix)","90f64ef8":"Create a new text file and store the processed data in the new text file.\nIf you want to train on multiple text files, then combine all of them to the same text file","f77d0745":"Checking the tensorflow version","62941e57":"After the training is completed the model will be automatically saved.\nOnce trained, you can also use the model to generate philosphy using a prefix","088129d4":"The generated text output I got:\n\nThe purpose of life is to create God--to make him\nthe GREATEST EXPERIENCE OF LIFE make him the happiest man\nand woman have ever known. And to ask the question: Did the happiest man\nhave a conception about life? Probably: but he who has the\nquestion himself often enough, seldom forgets: \"Sir, could not you have\ndone more for happiness by being more?\" probably: he would have said\nyes, but he knew not what. And to ask the question: Did the happiest man\nhave a conception about life? Probably: he is the exception, the rule.","86da7e49":"For training the model, choose a few philosophical books and convert them to text files.\nAfter pre-processing, combine them all to a single text file for training","5edef771":"Try not to use historical books as the generated text be offensive.\nAll kinds of books can be used here. Fictional books work great.\nHope this helps.","c58878ca":"First the pre-trained model will be downloaded which will be fine-tuned and saved. Then the saved model can be again loaded to generate text.\nThe training will be done for 500 steps (you can continue training if the average loss keeps on decreasing by uploading the saved model and again training it) and a sample text will be generated every 100 steps.\nOn a GPU it takes around 10 minutes for 500 steps","7821bcf8":"Fine tuning GPT-2 for Generating philosophy using python package gpt-2-simple.\n\ngpt-2-simple doesn't work in tensorflow 2, Tensorflow 1.x is used\n\nSince I couln't find a way to use GPU in kaggle kernel with Tensorflow 1 versions, this notebook is designed for Google colab","e49c0428":"Pre-Processing"}}