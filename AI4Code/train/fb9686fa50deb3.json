{"cell_type":{"5027f090":"code","e84717cd":"code","9028e552":"code","742f74ad":"code","7a37236c":"code","1d33398f":"code","7794bd47":"code","d0cc7b32":"code","13b8a314":"code","837b0727":"code","7ff7d061":"code","0e1a99f1":"code","029d1a00":"code","02440f18":"code","fbf6403e":"code","411b1b87":"code","49c748ec":"code","4f578f3d":"code","49e5ee55":"code","01c53cac":"code","0c11f77d":"code","224f5f22":"code","a935ff7f":"code","8691fcfb":"code","09e7bdb3":"code","3da578c6":"code","3e3558f6":"code","da66d20e":"code","4c735074":"code","7281668f":"code","b9b3db62":"code","1e7ea9ce":"code","8ac14f10":"code","3ac646d3":"code","5b6d8728":"code","0c359aa9":"markdown","3921e03f":"markdown","00f17272":"markdown","6b843476":"markdown","8c23e123":"markdown","4b694f99":"markdown","7ad6483e":"markdown","cc8166b8":"markdown","552f5c3a":"markdown","450eb56e":"markdown","71afc806":"markdown","2b52a2f9":"markdown","43247f1e":"markdown","3cfab900":"markdown","63bf2837":"markdown","4468d2a7":"markdown","738e84b7":"markdown","d068d86a":"markdown","78beacf4":"markdown","cf47995a":"markdown","26b2938e":"markdown","bf5f9ee0":"markdown","5c79f42e":"markdown","03aa956a":"markdown","024fdafe":"markdown","eefe116d":"markdown","b309db22":"markdown","3c8d69d0":"markdown","ca867473":"markdown","ca8ac50f":"markdown","031c8167":"markdown","a084b468":"markdown","ee56dfcc":"markdown"},"source":{"5027f090":"DATA_DIR = 'data'\n!mkdir {DATA_DIR} -p","e84717cd":"# Only on Linux, Mac and Windows WSL\n!wget http:\/\/www.openslr.org\/resources\/45\/ST-AEDS-20180100_1-OS.tgz\n!tar -C {DATA_DIR} -zxf ST-AEDS-20180100_1-OS.tgz\n!rm ST-AEDS-20180100_1-OS.tgz","9028e552":"import os\nfrom IPython.display import Audio\n\naudio_files = os.listdir(DATA_DIR)\nlen(audio_files), audio_files[:10]","742f74ad":"example = DATA_DIR + \"\/\" + audio_files[0]\nAudio(example)","7a37236c":"Audio(DATA_DIR + \"\/\" + audio_files[1])","1d33398f":"Audio(DATA_DIR + \"\/\" + audio_files[823])","7794bd47":"import librosa","d0cc7b32":"y, sr = librosa.load(example, sr=None)","13b8a314":"print(\"Sample rate  :\", sr)\nprint(\"Signal Length:\", len(y))\nprint(\"Duration     :\", len(y)\/sr, \"seconds\")","837b0727":"print(\"Type  :\", type(y))\nprint(\"Signal: \", y)\nprint(\"Shape :\", y.shape)","7ff7d061":"Audio(y, rate=sr)","0e1a99f1":"Audio(y, rate=sr\/2)","029d1a00":"Audio(y, rate=sr*2)","02440f18":"y_new, sr_new = librosa.load(example, sr=sr*2)\nAudio(y_new, rate=sr_new)","fbf6403e":"y_new, sr_new = librosa.load(example, sr=sr\/2)\nAudio(y_new, rate=sr_new)","411b1b87":"%matplotlib inline\nimport matplotlib.pyplot as plt\nimport librosa.display","49c748ec":"plt.figure(figsize=(15, 5))\nlibrosa.display.waveplot(y, sr=sr);","4f578f3d":"import numpy as np","49e5ee55":"# Adapted from https:\/\/musicinformationretrieval.com\/audio_representation.html\n# An amazing open-source resource, especially if music is your sub-domain.\ndef make_tone(freq, clip_length=1, sr=16000):\n    t = np.linspace(0, clip_length, int(clip_length*sr), endpoint=False)\n    return 0.1*np.sin(2*np.pi*freq*t)\nclip_500hz = make_tone(500)\nclip_5000hz = make_tone(5000)","01c53cac":"Audio(clip_500hz, rate=sr)","0c11f77d":"Audio(clip_5000hz, rate=sr)","224f5f22":"plt.figure(figsize=(15, 5))\nplt.plot(clip_500hz[0:64]);","a935ff7f":"plt.figure(figsize=(15, 5))\nplt.plot(clip_5000hz[0:64]);","8691fcfb":"plt.figure(figsize=(15, 5))\nplt.plot((clip_500hz + clip_5000hz)[0:64]);","09e7bdb3":"Audio(clip_500hz + clip_5000hz, rate=sr)","3da578c6":"clip_500_to_1000 = np.concatenate([make_tone(500), make_tone(1000)])\nclip_5000_to_5500 = np.concatenate([make_tone(5000), make_tone(5500)])","3e3558f6":"# first half of the clip is 500hz, 2nd is 1000hz\nAudio(clip_500_to_1000, rate=sr)","da66d20e":"# first half of the clip is 5000hz, 2nd is 5500hz\nAudio(clip_5000_to_5500, rate=sr)","4c735074":"sg0 = librosa.stft(y)\nsg_mag, sg_phase = librosa.magphase(sg0)\nlibrosa.display.specshow(sg_mag);","7281668f":"sg1 = librosa.feature.melspectrogram(S=sg_mag, sr=sr)\nlibrosa.display.specshow(sg1);","b9b3db62":"sg2 = librosa.amplitude_to_db(sg1, ref=np.min)\nlibrosa.display.specshow(sg2, sr=16000, y_axis='mel', fmax=8000, x_axis='time')\nplt.colorbar(format='%+2.0f dB')\nplt.title('Mel spectrogram');","1e7ea9ce":"sg2.min(), sg2.max(), sg2.mean()","8ac14f10":"type(sg2), sg2.shape","3ac646d3":"plt.imshow(sg2);","5b6d8728":"# Clean up\n!rm -rf data","0c359aa9":"The mel scale is a human-centered metric of audio perception that was developed by asking participants to judge how far apart different tones were.\n\n![image.png](https:\/\/wikimedia.org\/api\/rest_v1\/media\/math\/render\/svg\/349ff3f61581b99c709f4ed29ab5e1eb6d52c98a)\n\n| Frequency | Mel Equivalent |\n| --- | --- |\n| 20 | 0 |\n| 160 | 250 |\n| 394 | 500 |\n| 670 | 750 |\n| 1000 | 1000 |\n| 1420 | 1250 |\n| 1900 | 1500 |\n| 2450 | 1750 |\n| 3120 | 2000 |\n| 4000 | 2250 |\n| 5100 | 2500 |\n| 6600 | 2750 |\n| 9000 | 3000 |\n| 14000 | 3250 |","3921e03f":"Let's do an experiment and increase the frequency of the above tones by 500hz each and see how much this moves our perception of them","00f17272":"500 cycles per second, 16000 samples per second, means 1 cycle = 16000\/500 = 32 samples, let's see 2 cycles.","6b843476":"Pitch is a musical term that means the human perception of frequency. This concept of human perception instead of actual values seems vague and non-scientific, but it is hugely important for machine learning because most of what we're interested in, speech, sound classification, music...etc are inseparable from human hearing and how it functions.","8c23e123":"It looks inverted because the y-axis is inverted. Also, the ticks on the y-axis now represent mel frequencies, and the ticks on the x-asis represent the actual sample length. ","4b694f99":"## Data dowload & exploration","7ad6483e":"Next, let's use the decibel scale, and labels the x & y axes.","cc8166b8":"## Waveforms, amplitude vs magnitude","552f5c3a":"## Frequency and Pitch","450eb56e":"Learn more about audio processing: https:\/\/www.kaggle.com\/deepaksinghrawat\/in-depth-introduction-to-audio-for-beginners","71afc806":"## Decibels","2b52a2f9":"The spectrogram itself is nothing special, simply a 2d numpy array","43247f1e":"## Mel scale","3cfab900":"Most of us remember frequency from physics as cycles per second of a wave. It's the same for sound, but really hard to see in the above image. How many cycles are there? How can there be cycles if it's not regular? The reality is that sound is extremely complex, and the above recording of human speech is the combination of many different frequencies added together. To talk about frequency and pitch, it's easier to start with a pure tone, so let's make one.\n\nHuman hearing ranges from 20hz to 20,000hz, hz=hertz=cycles per second. The higher the frequency, the more cycles per second, and the \"higher\" the pitch sounds to us. To demonstrate, let's make a sound at 500hz, and another at 5000hz.","63bf2837":"Now let's look at 5000Hz.","4468d2a7":"Next we use the mel-scale instead of raw frequency. ","738e84b7":"Next, let's download and unzip the data:","d068d86a":"Just like frequency, human perception of loudness occurs on a logarithmic scale. A constant increase in the amplitude of a wave will be perceived differently if the original sound is soft or loud.\n\nDecibels measure the ratio of power between 2 sounds, and each 10x increase in the energy of the wave (multiplicative) results in a 10dB increase in sound (additive). Thus something that is 20dB louder has 100x (10*10) the amount of energy, something that is 25dB louder has (10^2.5) = 316.23x more energy. \n\n![image.png](https:\/\/boomspeaker.com\/wp-content\/uploads\/2018\/04\/DB-LEVEL-CHART-LOUD.jpg)","78beacf4":"We can listen to audio files directly within Jupyter using a display widget.","cf47995a":"Audio is a continuous wave that is \"sampled\" by measuring the amplitude of the wave at a given time. How many times you sample per second is called the \"sample rate\" and can be thought of as the resolution of the audio. The higher the sample rate, the closer our discrete digital representation will be to the true continuous sound wave. Sample rates generally range from 8000-44100 but can go higher or lower.","26b2938e":"Now let's put the two sounds together.","bf5f9ee0":"Our signal is just a numpy array with the amplitude of the wave.","5c79f42e":"Amplitude and magnitude are often confused, but the difference is simple. Amplitude of a wave is just the distance, positive or negative, from the equilibrium (zero in our case), and magnitude is the absolute value of the amplitude. In audio we sample the amplitude.","03aa956a":"We begin by creating directory where we'll download our data.","024fdafe":"Every point in the square represents the energy at the frequency of it's y-coordinate at the time of it's x-coordinate. ","eefe116d":"## Audio signals & sampling\n\nWe'll use the library `librosa` to process and play around with audio files.","b309db22":"While there's a lot more to explore about audio processing, we are going to stop here, since we have successfully converted the audio into images, and now we can use same models that we use for computer vision with audio.","3c8d69d0":"A waveform is a curve showing the amplitude of the soundwave (y-axis) at time T (x-axis). Let's check out the waveform of our audio clip.","ca867473":"Notice that the pitch of the first clip seems to change more even though they were shifted by the same amount. This makes intuitive sense as the frequency of the first was doubled and the frequency of the second only increased 10%. Like other forms of human perception, hearing is not linear, it is logarithmic. This will be important later as the range of frequencies from 100-200hz convey as much information to us as the range from 10,000-20,000hz.","ca8ac50f":"Let's try some experiments now. Can you guess what the following will sound like?","031c8167":"In fact, we can we it as an image.","a084b468":"## Spectrogram - visual representation of audio\n\nWe'll plot the time on the x-axis, frequencies on the y-axis, and use the color to represent the amplitude of various frequencies.","ee56dfcc":"We can also display a play a numpy array using the `Audio` widget."}}