{"cell_type":{"ccfdbbd7":"code","b4e6387d":"code","d4b03d6d":"code","938bb23e":"code","453e66b9":"code","3ca29d04":"code","40648e66":"code","eaf171b7":"code","1f12df22":"code","52676895":"code","61c483c6":"code","a5f29d37":"code","a5e837f2":"code","f506b12f":"code","ab2677eb":"code","f70df2cb":"code","4fc74b20":"markdown","538b2bbe":"markdown","2a375757":"markdown","3c0e7eac":"markdown","16a09ca1":"markdown","56c9ebe9":"markdown","27415a48":"markdown","6a35c466":"markdown","4b7e7024":"markdown","b4952337":"markdown"},"source":{"ccfdbbd7":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.metrics import *\nsns.set_style('whitegrid')\n%matplotlib inline\nimport warnings\nwarnings.filterwarnings('ignore')\nnp.random.seed(42)","b4e6387d":"data = pd.read_csv('..\/input\/heart.csv', delimiter=',')","d4b03d6d":"data.head(3)","938bb23e":"# Let's look at the distribution of people by sex\nmale = len(data[data.sex == 1])\nfemale = len(data[data.sex == 0])\nsns.countplot('sex', hue='target', data=data)\nplt.title('Heart Disease: Sex')\nplt.xlabel('Sex (0 = Female, 1 = Male)')\nplt.xticks(rotation=0)\nplt.legend([\"Haven't Disease\", \"Have Disease\"])\nplt.ylabel('Frequency')\nplt.show()","453e66b9":"# Now, let's look at the distribution of people by age.\nplt.figure(figsize=(9, 9))\nplt.title('Heart Disease: Age')\nplt.xlabel('Age')\nplt.ylabel('Qantity')\ndata['age'].hist(bins=20)\nplt.show()","3ca29d04":"data_v = data.iloc[:, 0:13].values\nprint('Feature vector:', data_v[1])","40648e66":"#Reducing the dimension to 2 for visualization.\nfrom sklearn.decomposition import PCA\npca = PCA(n_components=2).fit(data_v)\ndata_2d = pca.transform(data_v)\n\n#Building a graph on a two-dimensional matrix\ncolormap = np.array(['red', 'lime'])\nplt.figure(figsize=(10, 10))\n\nfor i in range(0, data_2d.shape[0]):\n    if data['target'][i] == 1:\n        c1 = plt.scatter(data_2d[i, 0], data_2d[i, 1], c='red')\n    elif data['target'][i] == 0:\n        c2 = plt.scatter(data_2d[i, 0], data_2d[i, 1], c='lime')\n\nplt.title('People distribution')\nplt.legend([c1, c2], ['Sick', 'Healthy'])","eaf171b7":"plt.figure(figsize=(12, 12))\nsns.heatmap(data.corr(), annot=True)\nplt.show()","1f12df22":"print(data['target'].value_counts())","52676895":"plt.figure(figsize=(7, 7))\ndata['target'].value_counts().plot(kind='bar', label='Target')\nplt.legend()\nplt.title('Distribution of target')","61c483c6":"from sklearn.utils import resample\n\ndf_majority = data[data.target==1]\ndf_minority = data[data.target==0]\n \n\ndf_minority_upsampled = resample(df_minority, \n                                 replace=True,     \n                                 n_samples=165,    \n                                 random_state=123)\n \n\ndata = pd.concat([df_majority, df_minority_upsampled])\n \n\ndata['target'].value_counts()","a5f29d37":"from keras.models import Sequential\nfrom keras import metrics\nfrom keras.layers.core import Dense, Activation ,Dropout\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, f1_score","a5e837f2":"data_pop = data.drop(\"target\", axis=1)\ntarget = data[\"target\"]\nX_train, X_test, Y_train, Y_test = train_test_split(data_pop, target, test_size=0.3, random_state=0)\n\nfrom sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nX_train = sc.fit_transform(X_train)\nX_test = sc.transform(X_test)","f506b12f":"#Keras neural network\n\nmodel = Sequential()\nmodel.add(Dense(15, init = 'uniform', activation='relu', input_dim=13))\nmodel.add(Dense(10, init = 'uniform', activation='relu'))\nmodel.add(Dense(6, init = 'uniform', activation='relu'))\nmodel.add(Dense(1, init = 'uniform', activation='sigmoid'))\n\nmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n\n#Fitting\nmodel.fit(X_train, Y_train, epochs=130)","ab2677eb":"#Testing on a test sample\nY_pred_nn = model.predict(X_test)\nrounded = [round(x[0]) for x in Y_pred_nn]\nY_pred_nn = rounded","f70df2cb":"score_nn = round(accuracy_score(Y_pred_nn, Y_test)*100,2)\nscore_f1 = round(f1_score(Y_pred_nn, Y_test)*100, 2)\nprint(\"Accuracy score: \" + str(score_nn) + \" %\")\nprint(\"F1 score: \" + str(score_f1) + \"%\")","4fc74b20":"## Rebalancing target values","538b2bbe":"# Keras time","2a375757":"It's simple! Less thalach, more oldpeak, and everything will be fine","3c0e7eac":"## Let's look at the overall distribution. Maybe we will find out something","16a09ca1":"## Standardize data","56c9ebe9":"At the end, we look at the correlation of signs","27415a48":"## Let's look at the distribution of target","6a35c466":"According to the conditions of the problem at the university, I had to solve this task using only neural networks. I chose Keras for its simplicity.\n\nUnfortunately, the majority of the owners of the kernels flopped neural networks without any data preparation, and as a result they receive 80-85% score. I tried to make it just a bit smarter and got an accuracy of 89-90%.\n\nIn principle, just a little has been done: data is standardized, classes are balanced, and a pair of hidden layers in a neural network was added. You can also replace the output activation function to softmax, and make cross-validate. I tried it, but my accuracy was low. If you beat my record, please send a link to your work on email: vvpereverzev@edu.hse.ru","4b7e7024":"As we can see, they are not that different.","b4952337":"If at start you got less accuracy, then try restarting the cells with the neural network several times: until the scales are initialized with greater accuracy"}}