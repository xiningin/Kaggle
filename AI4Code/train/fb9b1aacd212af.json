{"cell_type":{"1f0e277a":"code","b3de90c3":"code","4cfff347":"code","d98e08e7":"code","06cb57be":"code","21c1b39f":"code","bb1d7537":"code","d690e814":"code","884df066":"code","228fc103":"code","1c375333":"code","a1017964":"code","e8916178":"code","66d5e3d8":"code","73f7fbac":"code","5324c2e6":"code","0c403473":"code","b795b859":"code","8b3ce8dd":"code","5adf0851":"code","66c6755a":"code","5c92d169":"code","98a9cc99":"code","a3e6e516":"code","32bf991e":"code","2baa6754":"code","3f794958":"code","3913367b":"code","7644a46f":"code","900f9944":"code","2fa8024a":"code","6aa7c7aa":"code","715e4ebd":"code","d41ba78d":"code","ea618604":"code","0a741dcd":"code","74ab4202":"code","1a7e9135":"code","0c1c5ca3":"code","cda46e93":"code","73a3eedb":"code","bc17dd37":"code","1a219154":"code","c65425b1":"code","f55dd601":"code","e1d1abec":"code","00d158be":"code","fc7c5d46":"code","ce19eeda":"code","f1b0f177":"code","e5a3b207":"code","5bf0dfa9":"code","db15f2c2":"code","4dbfde77":"code","03c5ec21":"code","9fb0fcc2":"code","817c75d1":"code","ab574ec4":"code","0ae0c7e5":"code","27766b5c":"code","58b707d9":"code","18915230":"code","f9644eac":"code","4e44b1ca":"code","dbca7d6e":"code","3eef5785":"code","4a00fa8f":"code","9b1501e4":"code","054901c6":"code","c86b40d6":"code","6d1376a9":"code","c8cfeee4":"markdown","b62276dc":"markdown","410aa495":"markdown","4049ef9e":"markdown","df30b482":"markdown","f187a404":"markdown","49832b89":"markdown","0a02ca84":"markdown","7818edac":"markdown","06942540":"markdown","0669372e":"markdown","65851638":"markdown","8f878d10":"markdown","e24d6469":"markdown","33027267":"markdown","bf6212b6":"markdown","f7108254":"markdown","254c5dd2":"markdown","5804e75f":"markdown","a2151e44":"markdown","87c71f8e":"markdown","81115bdd":"markdown","395386b8":"markdown","1a89ca19":"markdown","d85fe040":"markdown","6c0d1d87":"markdown","905f1fed":"markdown","05e51191":"markdown","8c948890":"markdown","f0de7619":"markdown","291eb636":"markdown","a9f9d239":"markdown","08d7e558":"markdown","807bd695":"markdown","ed0dfa19":"markdown","0b5f68d9":"markdown","5c07baaf":"markdown","873ef836":"markdown","ad596999":"markdown","c64484de":"markdown","06b99849":"markdown","bc9b1798":"markdown","f2462128":"markdown","1c6a7128":"markdown","07e9c720":"markdown","e881c214":"markdown"},"source":{"1f0e277a":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","b3de90c3":"import pandas as pd\n\nsubmission_sample_data = pd.read_csv(\"\/kaggle\/input\/titanic\/gender_submission.csv\")\ntrain_data = pd.read_csv(\"\/kaggle\/input\/titanic\/train.csv\")\ntest_data = pd.read_csv(\"\/kaggle\/input\/titanic\/test.csv\")","4cfff347":"submission_sample_data.shape, train_data.shape, test_data.shape","d98e08e7":"submission_sample_data.head()","06cb57be":"train_data.head()","21c1b39f":"survived = np.round(train_data['Survived'].mean(), 2) * 100\ndeceased = 100 - survived\nprint(\"{}% of people survived and {}% of people deceased\".format(survived, deceased))","bb1d7537":"# print(\"Train Data Null\/NaN check\")\n# print(train_data.isna().sum())\n# print(\"\\nTest Data Null\/NaN check\")\n# print(test_data.isna().sum())","d690e814":"# from colorama import Fore\n\nprint(\"Train data information\")\nprint(train_data.info())\nprint(\"\\nTest data information\")\nprint(test_data.info())","884df066":"train_data.describe()","228fc103":"test_data.describe()","1c375333":"train_data.describe(percentiles=[0, 0.02, 0.05, 0.95, 0.98, 1])","a1017964":"test_data.describe(percentiles=[0, 0.02, 0.05, 0.95, 0.98, 1])","e8916178":"train_data[train_data['Fare'] > 212] # 98 percentile value in train data","66d5e3d8":"test_data[test_data['Fare'] > 212] # 98 percentile value in train data","73f7fbac":"train_data[train_data['Fare']==0] # passengers with 0 Fare Value","5324c2e6":"train_data[train_data['SibSp'] > 4] # above 98 percentile data","0c403473":"test_data[test_data['SibSp']>4]","b795b859":"train_data[train_data['Parch'] > 2] # 98 percentile value of Train data","8b3ce8dd":"test_data[test_data['Parch'] > 2] # 98 percentile value of Train data","5adf0851":"def unique_values(data, feature): # to find number of unique values in feature\/column from data\n    uniqes = data[feature].nunique()\n    nulls = data[feature].isna().sum()\n    print(\"{} has {} unique and {} null values\".format(feature, uniqes, nulls))","66c6755a":"features = list(train_data.columns)\nprint(\"Train Data\")\nfor feature in features:\n    unique_values(train_data, feature)","5c92d169":"features_test = list(test_data.columns)\nprint(\"Test Data\")\nfor feature in features_test:\n    unique_values(test_data, feature)","98a9cc99":"print(train_data['Embarked'].value_counts())","a3e6e516":"# since the Embarked feature is categorical let's fill nan values with most frequent word\n# train_data['Embarked'].value_counts().values\n# train_data['Embarked'].value_counts().index\nembarked_frequent = train_data['Embarked'].value_counts().index[0]\ntrain_data['Embarked'] = train_data['Embarked'].fillna(embarked_frequent)\ntrain_data['Embarked'].value_counts()","32bf991e":"# we have one NaN value for column Fare on Test data\ntest_data[test_data['Fare'].isna()]","2baa6754":"train_data[(train_data['Pclass']==3) & (train_data['Sex']=='male') & (train_data['Embarked'] == 'S') & (train_data['Age'].between(55,65)) & (train_data['Cabin'].isna())]\n#test_data[(test_data['Pclass']==3) & (test_data['Sex']=='male') & (test_data['Embarked'] == 'S') & (test_data['Age'].between(55,65))]","3f794958":"# From those three records above we will fill the missing fare value with the mean of the fare value with the age criterion in between 59 and 62\nmiss_fare_mean = train_data['Fare'][(train_data['Pclass']==3) & (train_data['Sex']=='male') & (train_data['Embarked'] == 'S') & (train_data['Age'].between(59,62))].mean()\ntest_data['Fare'] = test_data['Fare'].fillna(miss_fare_mean)","3913367b":"def nan_percent(feature):\n    nan_count_train = train_data[feature].isna().sum()\n    nan_count_test = test_data[feature].isna().sum()\n    train_records = train_data.shape[0]\n    test_records = test_data.shape[0]\n    nan_perc_train = np.round(nan_count_train\/(nan_count_train+train_records)*100, 2)\n    nan_perc_test = np.round(nan_count_test\/(nan_count_test+test_records)*100, 2)\n    print(\"{}% of records have nan values out of {} in train data for feature {}\".format(nan_perc_train, train_records, feature))\n    print(\"{}% of records have nan values out of {} in test data for feature {}\".format(nan_perc_test, test_records, feature))","7644a46f":"nan_percent('Age')","900f9944":"nan_percent(\"Cabin\")","2fa8024a":"# https:\/\/matplotlib.org\/gallery\/lines_bars_and_markers\/barchart.html#sphx-glr-gallery-lines-bars-and-markers-barchart-py\nfrom matplotlib import pyplot as plt\n\ndef univariate_barplot(df, feature):\n    unique_values = dict(df[feature].value_counts())\n#     print(unique_values)\n    labels = list(sorted(unique_values.keys()))\n    values = [unique_values[label] for label in labels]\n    x = np.arange(len(unique_values.values()))\n#     print(labels)\n    fig, ax = plt.subplots()\n    width = 0.40\n    survived = []\n    deceased = []\n    for label in labels:\n        survived.append(len(df.loc[(df[feature] == label) & (df['Survived'] ==1)]))\n        deceased.append(len(df.loc[(df[feature] == label) & (df['Survived'] ==0)]))\n#     print(len(survived), len(deceased))\n    survived_percent = []\n    deceased_percent = []\n    for index in range(len(survived)):\n        sp = (survived[index]\/(survived[index]+deceased[index])) * 100\n        sp = np.round(sp, 2)\n        dp = np.round(100-sp, 2)\n#         print(survived[index], deceased[index], survived[index]+deceased[index], sp, dp)\n        survived_percent.append(sp)\n        deceased_percent.append(dp)\n#         survived_percent.append(survived[index]\/(survived[index]+deceased[index]))\n#         deceased_percent.append(deceased[index]\/(survived[index]+deceased[index]))\n        \n    rects1 = ax.bar(x-width\/2, survived, width, label='Survived')\n    rects2 = ax.bar(x+width\/2, deceased, width, label='Deceased')\n    ax.set_ylabel(\"Number of People\")\n    ax.set_title(\"Survived vs Deceased w.r.t \"+ feature)\n    ax.set_xticks(x)\n    ax.set_xticklabels(labels, rotation='vertical')\n    ax.legend()\n    def autolabel(rects):\n        \"\"\"Attach a text label above each bar in *rects*, displaying its height.\"\"\"\n        for rect in rects:\n            height = rect.get_height()\n            ax.annotate('{}'.format(height),\n                        xy=(rect.get_x() + rect.get_width() \/ 2, height),\n                        xytext=(0, 3),  # 3 points vertical offset\n                        textcoords=\"offset points\",\n                        ha='center', va='bottom')\n    autolabel(rects1)\n    autolabel(rects2)\n    fig.tight_layout()\n    plt.show()\n    for index in range(len(survived_percent)):\n        print(\"{}% survivied and {}% deceased out of {} people for category {}\".format(survived_percent[index], deceased_percent[index], values[index], labels[index]))\n#     return(feature, labels, survived, deceased)","6aa7c7aa":"univariate_barplot(train_data, \"Sex\")","715e4ebd":"univariate_barplot(train_data, \"Pclass\")","d41ba78d":"univariate_barplot(train_data, \"SibSp\")\nunivariate_barplot(train_data, \"Embarked\")\nunivariate_barplot(train_data, \"Parch\")","ea618604":"from colorama import Fore\nimport seaborn as sb\ndef univariate_pdf_cdf(feature):\n    print(Fore.BLUE+\"PDF and CDF for feature {}\".format(feature))\n    x0 = list(train_data[feature][train_data['Survived']==0])\n    x1 = list(train_data[feature][train_data['Survived']==1])\n    fig, axs = plt.subplots(ncols=2, figsize=(20,4))\n    sb.distplot(x0, bins=10, hist=False, ax=axs[0], label='pdf-deceased')\n    sb.distplot(x0, bins=10, hist=False, kde_kws={'cumulative':True}, ax=axs[1], label='cdf-deceased')\n    sb.distplot(x1, bins=10, hist=False, ax=axs[0], label='pdf-survived')\n    sb.distplot(x1, bins=10, hist=False, kde_kws={'cumulative':True}, ax=axs[1], label='cdf-survived')\n    plt.tight_layout()\n    plt.show()","0a741dcd":"univariate_pdf_cdf(\"Age\")","74ab4202":"univariate_pdf_cdf(\"Fare\")","1a7e9135":"train_data['Lfare'] = train_data['Fare'].map(lambda x : np.log(x+1))\ntest_data['Lfare'] = test_data['Fare'].map(lambda x : np.log(x+1))","0c1c5ca3":"univariate_pdf_cdf('Lfare')","cda46e93":"# we will do it later\n\n","73a3eedb":"train_data['Fsize'] = train_data['Parch'] + train_data['SibSp'] + 1 # adding 1 for his\/herself\ntest_data['Fsize'] = test_data['Parch'] + test_data['SibSp'] + 1 ","bc17dd37":"univariate_barplot(train_data, 'Fsize')","1a219154":"train_data['Cabin'].isna().value_counts()","c65425b1":"train_data['ACabin'] = train_data['Cabin'].isna()\ntrain_data['ACabin'] = [1 if val==True else 0 for val in train_data['ACabin']]\ntest_data['ACabin'] = test_data['Cabin'].isna()\ntest_data['ACabin'] = [1 if val==True else 0 for val in test_data['ACabin']]","f55dd601":"univariate_barplot(train_data, 'ACabin')","e1d1abec":"train_data['AAge'] = train_data['Age'].isna()\ntrain_data['AAge'] = [1 if val==True else 0 for val in train_data['AAge']]\ntest_data['AAge'] = test_data['Age'].isna()\ntest_data['AAge'] = [1 if val==True else 0 for val in test_data['AAge']]","00d158be":"univariate_barplot(train_data, 'AAge')","fc7c5d46":"train_data.Name.head()","ce19eeda":"# [title for title in train_data['Name'].str.extract(pat = '([A-Za-z]*\\.)', expand=False)]","f1b0f177":"train_data['Title'] = [title for title in train_data['Name'].str.extract(pat = '([A-Za-z]*\\.)', expand=False)]\ntest_data['Title'] = [title for title in test_data['Name'].str.extract(pat = '([A-Za-z]*\\.)', expand=False)]\ntrain_data['Title'].value_counts()","e5a3b207":"univariate_barplot(train_data, 'Title')","5bf0dfa9":"test_data['Title'].value_counts()","db15f2c2":"# code for creating title \"Other\" for all rare titile in test and train\n# train_data['Title'] = ['Other' if title not in ['Mr.', 'Miss.', 'Mrs.','Master.'] else title for title in train_data['Title']]\n# test_data['Title'] = ['Other' if title not in ['Mr.', 'Miss.', 'Mrs.','Master.'] else title for title in test_data['Title']]","4dbfde77":"train_data['Twcount'] = train_data['Ticket'].str.split().map(len)\ntest_data['Twcount'] = test_data['Ticket'].str.split().map(len)","03c5ec21":"univariate_barplot(train_data, 'Twcount')","9fb0fcc2":"sub_train = train_data[['Pclass', 'Sex', 'Embarked', 'Fsize', 'ACabin', 'AAge', 'Title', 'Twcount', 'Lfare', 'PassengerId']].astype(str)\nsub_test = test_data[['Pclass', 'Sex', 'Embarked', 'Fsize', 'ACabin', 'AAge', 'Title', 'Twcount', 'Lfare', 'PassengerId']].astype(str)","817c75d1":"from sklearn.feature_extraction.text import CountVectorizer\n\nvectorizer = CountVectorizer(vocabulary=list(sub_train['Pclass'].unique()), lowercase=False)\ntrain_pclass_ohe = vectorizer.fit_transform(sub_train['Pclass'])\ntest_pclass_ohe = vectorizer.transform(sub_test['Pclass'])\n\nvectorizer = CountVectorizer(vocabulary=list(sub_train['Sex'].unique()), lowercase=False)\ntrain_sex_ohe = vectorizer.fit_transform(sub_train['Sex'])\ntest_sex_ohe = vectorizer.transform(sub_test['Sex'])\n\nvectorizer = CountVectorizer(vocabulary=list(sub_train['Embarked'].unique()), lowercase=False)\ntrain_embrk_ohe = vectorizer.fit_transform(sub_train['Embarked'])\ntest_embrk_ohe = vectorizer.transform(sub_test['Embarked'])\n\nvectorizer = CountVectorizer(vocabulary=list(sub_train['Fsize'].unique()), lowercase=False)\ntrain_fsize_ohe = vectorizer.fit_transform(sub_train['Fsize'])\ntest_fsize_ohe = vectorizer.transform(sub_test['Fsize'])\n\nvectorizer = CountVectorizer(vocabulary=list(sub_train['ACabin'].unique()), lowercase=False)\ntrain_acabin_ohe = vectorizer.fit_transform(sub_train['ACabin'])\ntest_acabin_ohe = vectorizer.transform(sub_test['ACabin'])\n\nvectorizer = CountVectorizer(vocabulary=list(sub_train['AAge'].unique()), lowercase=False)\ntrain_aage_ohe = vectorizer.fit_transform(sub_train['AAge'])\ntest_aage_ohe = vectorizer.transform(sub_test['AAge'])\n\nvectorizer = CountVectorizer(vocabulary=list(sub_train['Title'].unique()), lowercase=False)\ntrain_title_ohe = vectorizer.fit_transform(sub_train['Title'])\ntest_title_ohe = vectorizer.transform(sub_test['Title'])\n\nvectorizer = CountVectorizer(vocabulary=list(sub_train['Twcount'].unique()), lowercase=False)\ntrain_twcount_ohe = vectorizer.fit_transform(sub_train['Twcount'])\ntest_twcount_ohe = vectorizer.transform(sub_test['Twcount'])","ab574ec4":"# have to do","0ae0c7e5":"from numpy import hstack\n\nX_train = hstack((train_aage_ohe.toarray(), train_acabin_ohe.toarray(), train_embrk_ohe.toarray(), train_fsize_ohe.toarray(), train_pclass_ohe.toarray(), train_sex_ohe.toarray(), train_title_ohe.toarray(), train_twcount_ohe.toarray()))\nX_test = hstack((test_aage_ohe.toarray(), test_acabin_ohe.toarray(), test_embrk_ohe.toarray(), test_fsize_ohe.toarray(), test_pclass_ohe.toarray(), test_sex_ohe.toarray(), test_title_ohe.toarray(), test_twcount_ohe.toarray()))\ny_train = train_data['Survived'].values\nX_train.shape, y_train.shape, X_test.shape","27766b5c":"from sklearn.model_selection import StratifiedKFold, cross_val_score\nfrom sklearn.neighbors import KNeighborsClassifier, NearestNeighbors\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.linear_model import LinearRegression, LogisticRegression, SGDClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.naive_bayes import GaussianNB, MultinomialNB\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n\nskfold = StratifiedKFold(n_splits = 2) \n# default n_splits = 5 but i made it 2 since i assume the test data has around 420 records \n# so it would be better to do cross validation with similar number of records\n# the number of records in train data is euqal to 890, 890\/3 = 297, 890\/2 = 445 impllies n_splits = 2.\n\nrandom_state = 0\n\nclassifiers = []\nclassifiers.append(KNeighborsClassifier())\n# classifiers.append(NearestNeighbors())\nclassifiers.append(DecisionTreeClassifier(random_state=random_state))\n# classifiers.append(LinearRegression())\nclassifiers.append(LogisticRegression(random_state=random_state))\nclassifiers.append(SGDClassifier(random_state=random_state))\nclassifiers.append(SVC(random_state=random_state))\nclassifiers.append(GaussianNB())\n# classifiers.append(MultinomialNB())\nclassifiers.append(RandomForestClassifier(random_state=random_state))\nclassifiers.append(GradientBoostingClassifier(random_state=random_state))","58b707d9":"# from tqdm import tqdm\ncv_scores = dict()\nfor classifier in classifiers:\n    cv_scores[classifier] = cross_val_score(classifier, X_train, y_train, scoring='accuracy', cv=skfold).mean()\ncv_scores","18915230":"classifiers.remove(classifiers[5]) # removing GaussianNB\nclassifiers","f9644eac":"# for label, score_function  in score_funs.items():\n#     print(label, score_function)","4e44b1ca":"from sklearn.feature_selection import SelectKBest, chi2, f_classif\nfrom prettytable import PrettyTable\nimport warnings\nwarnings.filterwarnings('ignore')\n\ntable = PrettyTable()\ntable.field_names=[\"K Features\", \"Data Shape\", \"Score Function\", \"Classifier\", \"CV Score\"]\nscore_funs = {'chi2':chi2, 'f_classif':f_classif}\n\nfor K in range(5, 41, 5):\n    for label, score_function  in score_funs.items():\n        kbest = SelectKBest(k=K, score_func=score_function).fit_transform(X_train, y_train)\n        for classifier in classifiers:\n            cv_score = cross_val_score(classifier, kbest, y_train, scoring='accuracy', cv=skfold).mean()\n            table.add_row([K, kbest.shape, label, classifier, cv_score])\nprint(table)","dbca7d6e":"from sklearn.feature_selection import SelectFpr, chi2, f_classif\nfrom prettytable import PrettyTable\nimport warnings\nwarnings.filterwarnings('ignore')\n\ntable = PrettyTable()\ntable.field_names=[\"Alpha\", \"Data Shape\", \"Score Function\", \"Classifier\", \"CV Score\"]\nscore_funs = {'chi2':chi2, 'f_classif':f_classif}\n\nfor Alpha in [0.02, 0.05, 0.10, 0.15, 0.20]:\n    for label, score_function  in score_funs.items():\n        kbest = SelectFpr(score_func=score_function, alpha=Alpha).fit_transform(X_train, y_train)\n        for classifier in classifiers:\n            cv_score = cross_val_score(classifier, kbest, y_train, scoring='accuracy', cv=10).mean()\n            table.add_row([Alpha, kbest.shape, label, classifier, cv_score])\nprint(table)","3eef5785":"# KNeighboursClassifier\n\nn_neighbors_score = dict()\n\nfor k in range(1,20,2):\n    n_neighbors_score[k] = cross_val_score(KNeighborsClassifier(n_neighbors=k), X_train, y_train, scoring='accuracy', cv=skfold)\nn_neighbors_score","4a00fa8f":"from sklearn.model_selection import GridSearchCV\n\nknc = KNeighborsClassifier()\nparams = {'n_neighbors':[i for i in range(3,20,2)], 'weights' : ['uniform', 'distance'],\n         'algorithm':['auto', 'ball_tree', 'kd_tree', 'brute'], \n         'leaf_size':[i for i in range(2, 15, 2)], 'p':[1,2]}\ngrid_search_knn = GridSearchCV(knc, param_grid=params, cv=2)\nprint(grid_search_knn.fit(X_train, y_train))\nprint(grid_search_knn.best_score_)\nprint(grid_search_knn.best_params_)","9b1501e4":"# we will use the above parameters for training our model with kneighboursclassifier here\n\nknnc = KNeighborsClassifier(algorithm='auto', leaf_size=2, n_neighbors=5, p=1, weights='distance')\nknnc.fit(X_train, y_train)\nprint(knnc.score(X_train, y_train))","054901c6":"dtc = DecisionTreeClassifier()\nparams = {'criterion':[\"gini\", \"entropy\"], 'splitter' : ['best', 'random'],\n         'max_depth':[2,4,6,8,10,12], \"min_samples_split\":[2,4,6,8,10],\n         'min_samples_leaf':[1,2,3]}\ngrid_search_dtc = GridSearchCV(dtc, param_grid=params, cv=10)\nprint(grid_search_dtc.fit(X_train, y_train))\nprint(grid_search_dtc.best_score_)\nprint(grid_search_dtc.best_params_)","c86b40d6":"test_y_predicted = knnc.predict(X_test)\ntest_y_predicted_df = pd.DataFrame({'PassengerId':test_data['PassengerId'].values, 'Survived':test_y_predicted})\ntest_y_predicted_df.to_csv(\"submission_file.csv\", index=False)","6d1376a9":"test_y_predicted_df.head()","c8cfeee4":"Lot of male passanger's deceased in comparision with survived and the converse applicable to female passangers. Ther might be chances that male passenger's helps female passanger's to survive or There will be preferance for femals on survival facilities.","b62276dc":"### 2.2 Filling Missing\/NaN\/Null Values","410aa495":"We have to look into the fields Sisp, Parch and Fare fields, since the diffrence between 75% value and max value is high","4049ef9e":"We have NaN entries at fields \"Age, Cabin and Embarked\" from train, and \"Age, Cabin and Fare\" from test data","df30b482":"#### Filling NaN\/Missing Values in Cabin Column","f187a404":"### 2.1 Removal of Outliers","49832b89":"### 2.3 Feature Engineering","0a02ca84":"## 1. Input Retrival","7818edac":"## 2. Exploratory Data Analysis","06942540":"Except GaussianNB every algorithm got around 79% score, let's do hyperparameter tuning for all these algorithms except GaussianNB","0669372e":"Let's build the machine learning model by using the categorical features Pclass, Sex, Embarked, Fsize, ACabin, AAge and Title","65851638":"Wow.. Here we got the answer for the data records which have >4 Sibsp's, All the data in Test and Train with Sibs>4 was quite good, it got distributed among Test and Train, We can see the same ticket number in trian and test data for similar Sibsp's value, and Even we can see the names starting with similar start words, we can say they all who has same ticket number belongs to one family, hence no outliers from these records","8f878d10":"Though the deceased rate higher than survived rate for both the categories, but the deseased or death rate is much higher for the people who provided the age.","e24d6469":"Word count in ticket: Given Ticket filed contains a number and some textual data too, so we will just get the count of words in each ticket and let's see it's importance.","33027267":"If we observe the data clearly all the data here we have with more than 2 Parch value is not belongs to outliers, since we can see two or more records with the same ticket number and also with different Gender\/Sex, indicating that they all belongs to one family who has same ticket number with differnt gender\/sex and has more that 2 Parch's(Parents + Children)","bf6212b6":"Yeah we got less skewness here.","f7108254":"The death rate was high for the family sizes 1,6,5,7,11,8 and it was low for the familysizes 2,3,4","254c5dd2":"### 3.2 Numerical Data","5804e75f":"#### Visualization\n","a2151e44":"We have to fill the missing Age values with mean\/meadian in general, but since we got lot of missing reocrds we have to deal with other features like \"Gender, Class, Cabin etc.\" whaterver the fields would be useful to subclassify the age filed and to fill missning the values.","87c71f8e":"Clearly we can see for all these higher Fare rates belongs to Pclass 1 and Even the ticket numbers same for the same Fare rates, so we can say that it's a combined Fare value for group of passengers, So these are not outliers, And we can divide the Fare value with the number of tickets which has the same fare value so that the Fare will be distributed among all individuals who has same tikect number. As of now we are not going to do here, but we have to scale the data.","81115bdd":"It's a strange situation to fill the null values in Cabin field, since almot 44% of missing values in it, let's look into it later on how to fill all these missing values or we can ignore filling missing values here and instead we can generate a new feature on the Cabin availability.","395386b8":"The death rate gradually increases w.r.t Pclas valeus 1,2 and 3. Yes, The passanges with Pclass 1 may got higher facilities to survive when any stange inceident happens, i mean there may be the case that the Pclass passangers can stay near to to cabins where alternate boats placed inorder to access them easily","1a89ca19":"### 3.1 Categorical Data","d85fe040":"Title : We can extract the title from the name of each passenger","6c0d1d87":"Family Size","905f1fed":"Death rate was high in the Fare interval of 0 to 20 and Survived rate was high from the Fare of 20 to 500. And ther is one more heck, yes that is the Fare data was Right Skewed a lot and inorder to reduce it's skewness we can go with two approaches., one is finding the cause of skewness(yes we saw there are some records have High Fare with same ticket numbers, cause may be the fare value didn't get distributed so we have to distribute the fare among all the passangers who have very high Fare and same ticket) and second is applying logrithmic trasformation. As of now i am going with logarithimc transformation developed below.","05e51191":"### 5.1 Parameter Tuning","8c948890":"Let's Try with some classification algoriths and will see what's suits\/performs best among","f0de7619":"## 4. Data Matrix Preparation","291eb636":"## 3. Encoding","a9f9d239":"## Notebook Contents\n1. Input Retrival\n2. Exploratory Data Analysis\n    1. Removing Outliers\n    2. Filling NaN\/Missing Values\n    3. Feature Enginering\n3. Encoding the Input Data\n4. Preparation of Data Matrix\n5. Appling different Machine Learnig Algorithms on Data Matrix Selecting best Among Them.\n6. Enseble Model Preparation and Predictng the resuts for submission","08d7e558":"#### Filling NaN\/Missing Values in Age Column","807bd695":"Total we have 41 Features in the data matrix, let's select the top k(integer) number of features which helps improving the model performance.","ed0dfa19":"#### In general Fare value depends on gender, age, class, cabin and even Embarked categeroies.","0b5f68d9":"#### From the above observations we found Null\/Nan values for the coulumns Age, Fare, Cabin and Embarked","5c07baaf":"Have to do parameter tuning for remaing algorithms","873ef836":"For all data records which has 0 Fare value, all fields looking good except Fare value, and found total of 15reocrds, so we can't drop these records as well, the reason might be anything may be offering free tickets\/ giving  a ticket on purchase of some tickets or crossing some total fare etc.","ad596999":"Cabin Availability : Since we observed almost 44% of nan\/missing records for Cabin field, we will derive the cabin availability as if there is cabin then Yes=1 else No=0.","c64484de":"Filling NaN\/Missing Values in Age filed","06b99849":"## 5. Training Various Classification Algorithms","bc9b1798":"For the age below 15 the surival rate is high over the death rate and from Age 15 to 30 death rate was high over the survival rate. And from age 30 to 80 the death and survival reates were oscilating","f2462128":"Age Availability : let's create a feature called AAge, if age is available for passanger it is 1 else 0.","1c6a7128":"Since the CountVectorizer from sklearn expects string or byte type values for encoding categorical features into one hot encoded, we converted all the fileds of test and train with str type from above code line snippet.","07e9c720":"We found 17 titles on train data but howerver only 9 titles found in test data, and in common the first 4 titles have an adequete number of records, and the rest of titles has very few reocrds. Here lwe can combine the very rare titiles into a seperate title called \"Other\" or else we can keep as is.","e881c214":"Yes, Cabin availabitiy tells us if a passenger has cabin there is high chance to decease and vice-versa"}}