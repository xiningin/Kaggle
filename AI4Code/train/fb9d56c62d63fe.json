{"cell_type":{"bc765717":"code","aef1bd9f":"code","42c2078f":"code","6b47fa06":"code","6ce8260c":"code","a39402c5":"code","06830d16":"code","f7047e8b":"markdown","183554d4":"markdown","a3c728f1":"markdown","5ef8de74":"markdown","67d8d9c5":"markdown"},"source":{"bc765717":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","aef1bd9f":"# Read train and test files\ntrain_df = pd.read_csv('..\/input\/train.csv')\ntest_df = pd.read_csv('..\/input\/test.csv')\n\n#Set labels for train\/test data\ntrain_df['TAR'] = 0\ntest_df['TAR'] = 1\n# Get the combined data\ntotal_df = pd.concat([train_df.drop('target', axis=1), test_df], axis=0).drop('ID', axis=1)\n# Train and test\ntrain_idx = range(0, len(train_df))\ntest_idx = range(len(train_df), len(total_df))","42c2078f":"#Get labels\ny = total_df.TAR.copy()\ntotal_df.drop('TAR', axis = 1, inplace = True)","6b47fa06":"#Shuffle and split our set\nfrom sklearn.model_selection import train_test_split\nX_train, X_valid, y_train, y_valid = train_test_split(total_df, y, test_size=0.20, shuffle = True, random_state = 42)","6ce8260c":"import lightgbm as lgb\nlgbm_params =  {\n    'task': 'train',\n    'boosting_type': 'gbdt',\n    'objective': 'binary',\n    'metric': 'auc',\n    'num_leaves': 32,\n    'learning_rate': 0.02,\n    'verbose': 0,\n    'lambda_l1': 1,\n    'scale_pos_weight': 8  #for unbalanced labels\n} \nlgtrain = lgb.Dataset(X_train, y_train)\n\nlgvalid = lgb.Dataset(X_valid, y_valid)\n\nlgb_clf = lgb.train(\n    lgbm_params,\n    lgtrain,\n    num_boost_round=10000,\n    valid_sets=[lgtrain, lgvalid],\n    valid_names=['train','valid'],\n    early_stopping_rounds=100,\n    verbose_eval=100\n        )\n","a39402c5":"train_preds = lgb_clf.predict(total_df.iloc[train_idx])\ntrain_df['prob_to_test'] = train_preds\nval_set = train_df[train_df.prob_to_test>0.9] #train set with prob more than 90% to be test set\nval_set.head()","06830d16":"# feature importance\nprint(\"Features Importance...\")\ngain = lgb_clf.feature_importance('gain')\nfeatureimp = pd.DataFrame({'feature':lgb_clf.feature_name(), \n                   'split':lgb_clf.feature_importance('split'), \n                   'gain':100 * gain \/ gain.sum()}).sort_values('gain', ascending=False)\nfeatureimp.head()","f7047e8b":"As many people mentioned, there is difference between train and test distributions. One of way to deal with it is to use Adversarial validation.\nThe main idea is:\n\n*     Train a classifier to identify whether data comes from the train or test set.\n*     Sort the training data by it\u2019s probability of being in the test set.\n*     Select the training data most similar to the test data as  validation set.","183554d4":"Now we training LightGBM classifier to predict train\/test set:","a3c728f1":"Thanks!","5ef8de74":"**Bonus**:\nAs LGBM model can show features importance, we can select and delete features, which make difference between train and test set. So it can make our train and test sets more similar. ","67d8d9c5":"Now lets predict and select our valid set:"}}