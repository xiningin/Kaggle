{"cell_type":{"cca81e29":"code","7c0d14b4":"code","9cbb1d38":"code","68268455":"code","8204cabc":"code","ce94597c":"code","baa4f474":"code","1f41341c":"code","371e2aa2":"code","36d694e2":"code","37303437":"code","04bb5727":"code","28e95c86":"code","8f12e6bf":"code","019db3fc":"code","4132e20e":"code","0abfb851":"code","bf8ea450":"code","33b4efcd":"code","8165b6bf":"code","e6f96217":"markdown","d182f3d5":"markdown","d14f71a4":"markdown","1f62df12":"markdown","2663b466":"markdown","b3ce307a":"markdown","507823bf":"markdown","71103908":"markdown","e830daa7":"markdown","806d22d6":"markdown","355353aa":"markdown","200da103":"markdown"},"source":{"cca81e29":"# File Settings\nROOT_FOLDERS = ['\/kaggle\/input\/spanish-single-speaker-speech-dataset\/', '\/kaggle\/input\/120h-spanish-speech\/asr-spanish-v1-carlfm01\/']\nCSV_FILE_PATH_1 = ROOT_FOLDERS[0] + 'transcript.txt'\nCSV_FILE_PATH_2 = ROOT_FOLDERS[1] + 'files.csv'\nCSV_FILE_PATH = [CSV_FILE_PATH_1, CSV_FILE_PATH_2]\n\nSAVE_RESULTS_PATH = '\/kaggle\/working\/'\nSAVE_MODELS_PATH = '\/kaggle\/working\/'\n\n\n  ","7c0d14b4":"# Systems Libraries\nimport os\nimport time\n\n# For audio processing\nimport librosa\nimport librosa.display\nimport IPython as ipd\n\n#from torchsummary import summary\n\n# For data processing\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# For Neural networks\nimport torch\nfrom torch import nn\nimport pickle as pkl\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\n\n\n# For visualization\nimport matplotlib\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n\n# For text processing\nimport string\n\n# Scripts created by me:\nimport utils\nimport models\nimport textprocessor\nimport speechdataset\n\n# Import Early Stop https:\/\/github.com\/Bjarten\/early-stopping-pytorch\n#from pytorchtools import EarlyStopping\n\nfrom datetime import datetime\n\n","9cbb1d38":"ipd.display.Audio(filename='..\/input\/spanish-single-speaker-speech-dataset\/batalla_arapiles\/batalla_arapiles_0010.wav')","68268455":"(waveform, sample_rate) = librosa.load('..\/input\/spanish-single-speaker-speech-dataset\/batalla_arapiles\/batalla_arapiles_0010.wav')\nspectrogram = librosa.feature.melspectrogram(y=waveform, sr=sample_rate)\nutils.plot_all(audio_data=waveform, spec=spectrogram, sr=sample_rate, file='batalla_arapiles_0010.wav')","8204cabc":"ipd.display.Audio(filename='..\/input\/120h-spanish-speech\/asr-spanish-v1-carlfm01\/audios\/00041a31-2e68-444a-9a46-d8140b532d9c.wav')","ce94597c":"(waveform, sample_rate) = librosa.load('..\/input\/120h-spanish-speech\/asr-spanish-v1-carlfm01\/audios\/00041a31-2e68-444a-9a46-d8140b532d9c.wav')\nspectrogram = librosa.feature.melspectrogram(y=waveform, sr=sample_rate)\nutils.plot_all(audio_data=waveform, spec=spectrogram, sr=sample_rate, file='00041a31-2e68-444a-9a46-d8140b532d9c.wav')","baa4f474":"\n\n# The following code will be for collat_fn for the pytorch dataloader function    \ndef data_processing(audio_data):\n    spectrograms = []\n    labels = []\n    input_lengths = []\n    label_lengths = []\n    #print(\"data processing\")\n    for (spec,label) in audio_data:\n        #The spectrogram is in (128, 407) and (128, 355) for example but later on for padding the function expects (407, 128) and (355, 128). So we need to transpose the matrices.\n        spectrograms.append(torch.Tensor(spec.transpose()))\n        t = textprocessor.TextProcessor()\n        label = torch.Tensor(t.text2int(text=label))\n        labels.append(label)\n        input_lengths.append(spec.shape[0]\/\/2)\n        label_lengths.append(len(label))\n    #print(\"Start padding\")\n    spec_pad = torch.nn.utils.rnn.pad_sequence(spectrograms, batch_first=True).unsqueeze(1).transpose(2,3)   #(batch, channel=1, features, time )\n    label_pad = torch.nn.utils.rnn.pad_sequence(labels, batch_first=True)\n    #print(\"Finish padding\")\n    return spec_pad, label_pad, input_lengths, label_lengths\n\n","1f41341c":"def load_dataset(csv_file, root_dir, n_samples=10000, f_type='spec'):\n  \n  # Create the dataset and split into validation dataset and test dataset.\n    total_dataset = speechdataset.SpanishSpeechDataSet(csv_files=csv_file, root_dir=root_dir, f_type=f_type, num_samples=n_samples)\n    train_size = int(0.8 * len(total_dataset))\n    val_test_size = len(total_dataset) - train_size\n    train_dataset, val_test_dataset = torch.utils.data.random_split(total_dataset, [train_size, val_test_size])\n    valid_size = int(0.9 * len(val_test_dataset))\n    test_size = len(val_test_dataset) - valid_size\n    val_dataset, test_dataset = torch.utils.data.random_split(val_test_dataset, [valid_size, test_size])\n    print(\"Total Training Dataset = {}, Valid Dataset = {} and Test Dataset = {}\".format(len(train_dataset),len(val_dataset), len(test_dataset) ))\n    print(\"Total = \", len(train_dataset) + len(val_dataset) + len(test_dataset))\n    sample = train_dataset[0]\n    if f_type =='spec':\n        print(\"*****Showing spectrogram with label:**** \\n\")\n        print(sample[1])\n        utils.plot_spec(sample[0], title=\"Spectrogram\")\n    else:\n        print(\"*****Showing MFCCs with label:**** \\n\")\n        print(sample[1])\n        utils.plot_mfccs(sample[0])\n        \n    return (train_dataset, val_dataset, test_dataset)\n\n\n","371e2aa2":"\ndef create_data_loaders(train_dataset, val_dataset, test_dataset, kwargs, batch_size):\n    train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size,shuffle=True,drop_last=True, collate_fn=lambda x: data_processing(x), **kwargs )\n    valid_loader = DataLoader(dataset=val_dataset, batch_size=batch_size,shuffle=False,drop_last=True, collate_fn=lambda x: data_processing(x), **kwargs)\n    test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size,shuffle=False,drop_last=True, collate_fn=lambda x: data_processing(x), **kwargs)\n    return (train_loader, valid_loader, test_loader)","36d694e2":"\ndef train(n_epochs, train_loader, valid_loader, model, criterion, clip, device, lr, batch_size, save_model_path, save_pkl_path, model_name, show_every_n_batch=50):\n    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n    scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=lr, \n                                            steps_per_epoch=int(len(train_loader)),\n                                            epochs=n_epochs,\n                                            anneal_strategy='linear')\n    train_data_len = len(train_loader.dataset)\n    valid_data_len = len(valid_loader.dataset)\n    epoch_train_loss = 0\n    epoch_val_loss = 0\n    train_losses = []\n    valid_losses = []\n    print(\"#######################\")\n    print(\"#  Start Training    #\")\n    print(\"#######################\")\n    \n    model.train()\n    for e in range(n_epochs):\n        t0 = time.time()\n        #Initialize hidden state\n        #h = model.init_hidden(batch_size, device)\n\n        #batch loop\n        running_loss = 0.0\n        for batch_idx, _data in enumerate(train_loader, 1):\n            specs, labels, input_lengths, label_lengths = _data\n            specs, labels = specs.to(device), labels.to(device)\n            #print(batch_idx)\n            # Creating new variables for the hidden state, otherwise\n            # we'd backprop through the entire training history\n            #h = h.detach()\n            # zero accumulated gradients\n            model.zero_grad()\n            # get the output from the model\n            #output, h = model(specs, h)\n            output = model(specs)\n            output = F.log_softmax(output, dim=2)\n            output = output.transpose(0,1)\n            # calculate the loss and perform backprop\n            loss = criterion(output, labels.float(), input_lengths, label_lengths)\n            loss.backward()\n            # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs \/ LSTMs.\n            nn.utils.clip_grad_norm_(model.parameters(), clip)\n            optimizer.step()\n            scheduler.step()\n            # loss stats\n            running_loss += loss.item() * specs.size(0)\n            if (batch_idx % 1000 == 0):\n                output = output.transpose(1,0)\n                #print(\"Training Batch Number: \", batch_idx)\n                (decoded_preds, decoded_targets) = textprocessor.GreedyDecoder(output, labels, label_lengths)\n                for j in range(len(decoded_preds)):\n                    #print(\"****************************************************************************\")\n                    #print(\"Predicted -- {}\".format(decoded_preds[j]))\n                    #print(\"Utterance -- {}\\n\\n\".format(decoded_targets[j]))\n                    utils.write_to_csv('\/kaggle\/working\/'+model_name+'_training_results.csv', decoded_preds[j], decoded_targets[j], running_loss, epoch=e+1)\n                \n        t_t = time.time() - t0\n\n            \n        ######################    \n        # validate the model #\n        ######################\n        with torch.no_grad():\n            model.eval() \n            tv = time.time()\n            running_val_loss = 0.0\n            for batch_idx_v, _data in enumerate(valid_loader, 1):\n                specs, labels, input_lengths, label_lengths = _data\n                specs, labels = specs.to(device), labels.to(device)\n                #val_h = model.init_hidden(batch_size, device)\n                #output, val_h = model(specs, val_h)\n                output = model(specs)\n                output = F.log_softmax(output, dim=2)\n                output = output.transpose(0,1)\n                val_loss = criterion(output, labels.float(), input_lengths, label_lengths)\n                running_val_loss += val_loss.item() * specs.size(0)\n                if (batch_idx_v % 200 == 0):\n                    output = output.transpose(1,0)\n                    #print(\"Validation Batch Number: \", batch_idx)\n                    (decoded_preds, decoded_targets) = textprocessor.GreedyDecoder(output, labels, label_lengths)\n                    for j in range(len(decoded_preds)):\n                        #print(\"****************************************************************************\")\n                        #print(\"Predicted -- {}\".format(decoded_preds[j]))\n                        #print(\"Utterance -- {}\\n\\n\".format(decoded_targets[j]))\n                        utils.write_to_csv('\/kaggle\/working\/'+model_name+'_validation_results.csv', decoded_preds[j], decoded_targets[j], running_val_loss, epoch=e+1)\n            print(\"Epoch {}: Training took {:.2f} [s]\\tValidation took: {:.2f} [s]\\n\".format(e+1, t_t, time.time() - tv))\n                \n                \n        epoch_train_loss = running_loss \/ train_data_len\n        epoch_val_loss = running_val_loss \/ valid_data_len\n        train_losses.append(epoch_train_loss)\n        valid_losses.append(epoch_val_loss)\n        print('Epoch: {} Losses\\tTraining Loss: {:.6f}\\tValidation Loss: {:.6f}'.format(\n                e+1, epoch_train_loss, epoch_val_loss))\n        model.train()\n        \n        print(\"-------------------------------------------------------------------------------------------\")\n        print('Epoch {} took total {} seconds'.format(e+1, time.time() - t0))\n        print(\"-------------------------------------------------------------------------------------------\")\n\n    with open(save_pkl_path, 'wb') as f:       #this will save the list as \"results.pkl\" which you can load in later \n        pkl.dump((epoch_train_loss, epoch_val_loss), f)\n    utils.save_model(save_path=save_model_path, model=model)\n    utils.save_checkpoint(save_path=save_model_path, model=model, optimizer=optimizer, epoch=e, loss=train_losses)\n    return (model, train_losses, valid_losses)","37303437":"def test_model(test_data, model, model_name,device, batch_size):\n    model.eval()\n    print(\"#######################\")\n    print(\"# Testing Model: {} #\".format(model_name))\n    print(\"#######################\\n\\n\")\n    test_cer, test_wer = [], []\n    test_loss = 0.0\n    #h = model.init_hidden(batch_size, device)\n    with torch.no_grad():\n        for batch_idx, _data in enumerate(test_data, 1):\n            specs, labels, input_lengths, label_lengths = _data\n            specs, labels = specs.to(device), labels.to(device)\n            # initialize the hidden state\n            # get the output of the rnn\n            #output, _ = model(specs, h)\n            output = model(specs) \n            output = output.transpose(0,1) #(time, batch,n_class)\n            loss = criterion(output, labels, input_lengths, label_lengths)\n            #Input should be [batch, time, n_classes]\n            output = output.transpose(1,0)\n            (decoded_preds, decoded_targets) = textprocessor.GreedyDecoder(output, labels, label_lengths)\n            test_loss += loss.item()* specs.size(0)\n            #print(test_loss.item())\n            for j in range(len(decoded_preds)):\n                print(\"****************************************************************************\")\n                print(\"Predicted -- {}\".format(decoded_preds[j]))\n                print(\"Utterance -- {}\\n\\n\".format(decoded_targets[j]))\n                test_cer.append(textprocessor.cer(decoded_targets[j], decoded_preds[j]))\n                test_wer.append(textprocessor.wer(decoded_targets[j], decoded_preds[j]))\n                utils.write_to_csv('\/kaggle\/working\/'+model_name + '_testing_results.csv', decoded_preds[j], decoded_targets[j], test_loss)\n    avg_cer = sum(test_cer)\/len(test_cer)\n    avg_wer = sum(test_wer)\/len(test_wer)\n    print('Test set:Average CER: {:4f} Average WER: {:.4f}\\n'.format(avg_cer, avg_wer)) \n\n    ","04bb5727":"use_cuda = torch.cuda.is_available()\nif use_cuda:\n    torch.manual_seed(7)\ndevice = torch.device(\"cuda\" if use_cuda else \"cpu\")","28e95c86":"#input_size = 64\nn_classes = 29\nhidden_dim = 256\nn_layers =1\nclip=2 # gradient clipping\n# If MFCC = 13 and 128 if Specs\nf_type = 'spec'\nif f_type == 'spec':\n    n_feats = 64\nelse:\n    n_feats = 13\nlr = 1e-4\n\n\nepochs = 30\n\n\n\nbatch_size = 32\nconv_n_layers = 1\ngru_n_layers = 2\n\n# Total samples of audio. At least 12000, otherwise it won't work\nn_samples = 90000\n\ncriterion = nn.CTCLoss(blank=28, zero_infinity=True)","8f12e6bf":"\nkwargs={'num_workers': 4, 'pin_memory': True} if use_cuda else {}\n(train_dataset, val_dataset, test_dataset) = load_dataset(csv_file=CSV_FILE_PATH, root_dir=ROOT_FOLDERS, n_samples=n_samples, f_type=f_type)\n(train_loader, valid_loader, test_loader) = create_data_loaders(train_dataset, val_dataset, test_dataset, kwargs=kwargs, batch_size=batch_size)\n    ","019db3fc":"conv_bi_gru = models.ASRConvBiGRU(in_channel=1, gru_input_size=512, hidden_dim=hidden_dim, n_layers=n_layers,\n                                  n_feats=n_feats, n_classes=n_classes, conv_n_layers=conv_n_layers,\n                                  gru_n_layers=gru_n_layers, drop_prob=0.2, bidir=True)\n#conv_bi_gru.apply(models.weight_init)\nconv_bi_gru.to(device)","4132e20e":"print(models.count_parameters(conv_bi_gru))","0abfb851":"\nmodel_name = 'Conv-Bi-GRU'\n(conv_bi_gru_trained, bi_train_losses, bi_val_losses) = train(n_epochs=epochs, train_loader=train_loader,\n                                                     valid_loader=valid_loader,\n                                                     model=conv_bi_gru, criterion=criterion, clip=clip,\n                                                     device=device, lr=lr, batch_size=batch_size,\n                                                     model_name=model_name,\n                                                     save_model_path=SAVE_RESULTS_PATH+\"conv_bigru\",\n                                                     save_pkl_path=SAVE_MODELS_PATH + \"training_conv_bigru_iteration.pkl\")\n\n","bf8ea450":"test_model(test_loader, conv_bi_gru_trained, \"Conv-BI-GRU\",device, batch_size=batch_size)","33b4efcd":"\nfig = plt.figure(figsize=(10,5))\nax = plt.subplot(111)\nbox = ax.get_position()\n\nax.plot(bi_train_losses, 'g',label='Conv-BI-GRU-train losses')\nax.plot(bi_val_losses, 'm',label='Conv-BI-GRU-valid losses')\n\n\nplt.xlabel('epochs')\nplt.ylabel('loss')\n\nax.set_position([box.x0, box.y0 + box.height * 0.1,\n                 box.width, box.height * 0.9])\n# Put a legend above current axis\nax.legend(loc='upper center',fontsize='small', bbox_to_anchor=(0.5, 1.09),\n          fancybox=True, shadow=True, ncol=4) \n# Limits for the Y axis\nplt.show()","8165b6bf":"\n#conv_gru = models.ASRConvBiGRU(in_channel=1, gru_input_size=512, hidden_dim=hidden_dim, n_layers=n_layers,\n#                              n_feats=n_feats, n_classes=n_classes, conv_n_layers=conv_n_layers,\n#                              gru_n_layers=gru_n_layers, drop_prob=0.2, bidir=False)\n#conv_gru.apply(models.weight_init)\n#conv_gru.to(device)","e6f96217":"# Training Function","d182f3d5":"# Create & train the BI-GRU Model","d14f71a4":"### Pad the data\nAs the audio files have different lengths. We need to pad the data and create batches. \n\nFor example if the batch size = 3\n spec1 = [1 2 3 4 5]\n spec2 = [6 7 8]\n spec3 = [9 3]\ndata_processing function will create batches like:\n\n1 batch:\n* [1 2 3 4 5]\n* [6 7 8 0 0]\n* [9 3 0 0 0]","1f62df12":"# The model-Stacked RNNs\n\nThe model used here is:\n* Convolutional Network Layer 1\n* ReLU\n* Dropout\n* Layer Normalization\n* Convolutional Network Layer 2 (layers can be modified by conv_n_layers)\n* ReLU\n* Dropout\n* GRU (Can be bidirectional) x gru_n_layers \n* Layer Normalization\n* ReLU\n* Dropout\n* Fully connected Layer\n* ReLU\n* Dropout\n* Classifier\n\nLayer Normalization: https:\/\/arxiv.org\/pdf\/1607.06450.pdf\n","2663b466":"# Load Dataset","b3ce307a":"# Plot Losses","507823bf":"# Defining Hyperparameters.","71103908":"# Test the Conv-GRU Model","e830daa7":"# Audio Sample\n* First hear an audio from the spanish-single-speaker-speech-dataset and plot.\n* Second hear an audio from the 120h-spanish-speech and plot.","806d22d6":"# Testing Function","355353aa":"# Data Pre-processing:\n\nBefore starting to analyse the data, let's do some clean up. The path which looks like 19demarzo\/19marzo_XXXX.wav will be split into two other columns and thus we will have a table like this:\n\n\n| index | path    |\tutterance  | label   | duration\t | dir\t   | file    |\n|------ |---------|------------|---------|-----------|---------|---------|\n|0\t| 19demarzo\/19demarzo_0000.wav\t| Durante nuestra conversaci\u00f3n advert\u00ed que la mu...|\tDurante nuestra conversaci\u00f3n advert\u00ed que la mu...|\t5.88\t| 19demarzo\t|19demarzo_0000.wav |\n|1|\t19demarzo\/19demarzo_0001.wav|\tCompon\u00edanla personas de ambos sexos y de todas...|\tCompon\u00edanla personas de ambos sexos y de todas...|\t4.52|\t19demarzo\t|19demarzo_0001.wav |\n|2|\t19demarzo\/19demarzo_0002.wav|\tespont\u00e1neamente venidas por uno de esos llamam...|\tespont\u00e1neamente venidas por uno de esos llamam...|\t8.31|\t19demarzo |\t19demarzo_0002.wav |\n|3|\t19demarzo\/19demarzo_0003.wav|\ty resuenan de improviso en los o\u00eddos de un pue...|\ty resuenan de improviso en los o\u00eddos de un pue...|\t7.20|\t19demarzo |\t19demarzo_0003.wav |\n|4|\t19demarzo\/19demarzo_0004.wav|\tLa campana de ese arrebato glorioso no suena s...|\tLa campana de ese arrebato glorioso no suena s...|\t7.50|\t19demarzo |\t19demarzo_0004.wav |\n\n\n\n\n\n### Clearing up unnecessary data\n\nIt is clear that there are accents, commas, etc. that we don't really care about. So, let's clear those up.\nWe will also clearup the data where the utterance is empty with the command d.dropna.\n","200da103":"## Processing the audio data.\nFor the audio, I need to get the spectrogram or mfccs and the utterance (as the label).\nThe custom dataset saved as a utility script as speechdataset was created following the Custom Dataset tutorial by pytorch:\nhttps:\/\/pytorch.org\/tutorials\/beginner\/data_loading_tutorial.html\n\n**The utility script can be found named as speechdataset.**"}}