{"cell_type":{"fb3a08d5":"code","be97129f":"code","0d2ba8f6":"code","9424fb46":"code","e9f44107":"code","2386259f":"code","3b94a0bf":"code","4427269d":"code","ebffd96a":"code","8f2cc27b":"code","f1899721":"code","a49bba72":"code","cf2fc366":"code","a1798cdf":"code","2cae65b7":"code","3b0007a0":"code","cd8d0d5b":"code","564b5785":"code","806d22b4":"code","9022771a":"code","dd3fda2c":"code","6f95e55c":"code","2cd5bea8":"code","1c7c8901":"code","8e3855d5":"code","b71795c8":"code","8a9523e6":"code","23a4aa2c":"code","35b04369":"code","5044552b":"code","5983703c":"code","7d2378b1":"code","50753599":"code","80338d92":"code","a5fc5c3b":"code","1ea8eeeb":"code","405935c8":"code","59a6ba89":"code","274c5202":"code","021ffe06":"code","23d002dc":"code","1b3c8f7b":"code","3d0f3bf0":"code","47ec8982":"code","1b51aed4":"code","7a7fcaf3":"code","a2a4c789":"code","35d62c28":"code","39002154":"code","74ca282e":"code","dc51d45b":"code","2074d024":"markdown","87b070ad":"markdown","af484eea":"markdown","5a78ee07":"markdown","bae19429":"markdown","b02832c1":"markdown","4e4dbb10":"markdown","292ef690":"markdown","d1136089":"markdown"},"source":{"fb3a08d5":"import os\nfrom glob import glob\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport seaborn as sn\nfrom sklearn.model_selection import train_test_split, KFold, cross_val_predict\nfrom sklearn.linear_model import LinearRegression, Lasso, Ridge, LogisticRegression\nfrom sklearn import metrics\nfrom sklearn.preprocessing import StandardScaler, PolynomialFeatures\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import accuracy_score,confusion_matrix,roc_auc_score,ConfusionMatrixDisplay,precision_score,recall_score,f1_score,classification_report,roc_curve,plot_roc_curve,auc,precision_recall_curve,plot_precision_recall_curve,average_precision_score\n\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","be97129f":"dataset = pd.read_csv('\/kaggle\/input\/stroke-prediction-dataset\/healthcare-dataset-stroke-data.csv',index_col= 0 )\ndataset_v2 = dataset.copy()\n","0d2ba8f6":"dataset","9424fb46":"dataset.shape","e9f44107":"#I'm resetting index here to ensure normal index distribution from 0-5109\ndataset.reset_index(inplace=True)","2386259f":"dataset.drop('id',axis=1,inplace=True)","3b94a0bf":"dataset","4427269d":"#checking for correlation with target variable (y).\nfeatures =dataset.columns[:-1]\ncorrelations = dataset[features].corrwith(dataset.stroke)\ncorrelations.sort_values(inplace=True, ascending=False)\ncorrelations","ebffd96a":"dataset.info()","8f2cc27b":"dataset.describe()","f1899721":"#201 NULL values in 'bmi' column.\ndataset.isnull().sum()","a49bba72":"plt.title('Missing Value Status',fontweight='bold')\nax = sn.heatmap(dataset.isna().sum().to_frame(),annot=True,fmt='d',cmap='vlag')\nax.set_xlabel('Amount Missing')\nplt.show()","cf2fc366":"#mean of bmi's\nbmi_mean = dataset.bmi.mean()\nbmi_mean","a1798cdf":"#apply mean to NULL values using pandas fillna function.\ndataset.bmi.fillna(bmi_mean,inplace=True)","2cae65b7":"dataset.bmi = dataset.bmi.round(2)","3b0007a0":"dataset.isnull().sum()","cd8d0d5b":"#we can see that 249 of 5110 people actually had stroke.\n(dataset.stroke==1).value_counts()","564b5785":"#Simple calculation shows us that for every 1 value theres 20.5 0s in stroke column. It means that our target variable\n# is very unbalanced. We need to focus on that as there might be huge problems with prediction.\na = 4861 + 249\nb = a\/249\nprint(b)","806d22b4":"dataset.avg_glucose_level","9022771a":"dataset_v2 = dataset.copy()","dd3fda2c":"dataset_v2['diabetes'] = np.where(dataset_v2['avg_glucose_level']>=128, '1', '0')","6f95e55c":"#changing columns order to make target variable last again.\ndataset_v2 = dataset_v2[['gender', 'age', 'hypertension', 'heart_disease', 'ever_married',\n       'work_type', 'Residence_type', 'avg_glucose_level', 'bmi',\n       'smoking_status', 'diabetes', 'stroke']]","2cd5bea8":"dataset_v2","1c7c8901":"data_uniques = pd.DataFrame([[i, len(dataset_v2[i].unique())] for i in dataset_v2.columns], columns=['Variable', 'Unique Values']).set_index('Variable')\ndata_uniques","8e3855d5":"plt.figure(figsize=(8,8))\nax = plt.axes()\nax.hist(dataset_v2.age, bins=25);\n\nax.set(xlabel='age', \n       ylabel='Frequency',\n       );","b71795c8":"plt.figure(figsize=(8,8))\nax = plt.axes()\nax.hist(dataset_v2.avg_glucose_level, bins=25);\n\nax.set(xlabel='avg_glucose_level', \n       ylabel='Frequency',\n       );","8a9523e6":"plt.figure(figsize=(8,8))\nax = plt.axes()\nax.hist(dataset_v2.bmi, bins=25);\n\nax.set(xlabel='bmi', \n       ylabel='Frequency',\n       );","23a4aa2c":"plt.figure(figsize=(10,10))\nax = plt.axes()\n\nax.scatter(dataset_v2.age, dataset_v2.bmi,s=10,c='r')\n\nax.set(xlabel='Age',\n       ylabel='BMI',\n       title='age-bmi distribution');","35b04369":"plt.figure(figsize=(10,10))\nax = plt.axes()\n\nax.scatter(dataset_v2.age, dataset_v2.avg_glucose_level,s=10,c='r')\n\nax.set(xlabel='Age',\n       ylabel='glucose',\n       title='age-glucose distribution');","5044552b":"#labels variable stores labels of stroke column (0,1)\nlabels =dataset_v2['stroke'].value_counts(sort = True).index\n#sizes variables stores how many 0s and 1s there are in stroke column.\nsizes = dataset_v2['stroke'].value_counts(sort = True)\n\ncolors = [\"lightblue\",\"red\"]\n#makes wedges to stand out.\nexplode = (0.3,0) \n \nplt.figure(figsize=(7,7))\nplt.pie(sizes, explode=explode, labels=labels, colors=colors,autopct='%1.1f%%',  shadow=True, startangle=90,)\n\nplt.title('Number of stroke in the dataset')\nplt.show()","5983703c":"#Quick check if living area has any impact on patient's health. As we see below that's a false assumption.\nresidence_stroke = dataset_v2[['Residence_type','stroke']]\nresidence_stroke.value_counts()","7d2378b1":"#encoding categorical features \nle = LabelEncoder()\ndataset_v2['gender'] = le.fit_transform(dataset_v2['gender'])\ndataset_v2['ever_married'] = le.fit_transform(dataset_v2['ever_married'])\ndataset_v2['work_type'] = le.fit_transform(dataset_v2['work_type'])\ndataset_v2['Residence_type'] = le.fit_transform(dataset_v2['Residence_type'])\ndataset_v2['smoking_status'] = le.fit_transform(dataset_v2['smoking_status'])","50753599":"dataset_v2.head()","80338d92":"from sklearn.preprocessing import MinMaxScaler\nscaler = MinMaxScaler()\ndataset_v2[['gender', 'age', 'hypertension', 'heart_disease', 'ever_married',\n       'work_type', 'Residence_type', 'avg_glucose_level', 'bmi',\n       'smoking_status', 'diabetes',]] = scaler.fit_transform(dataset_v2[['gender', 'age', 'hypertension', 'heart_disease', 'ever_married',\n       'work_type', 'Residence_type', 'avg_glucose_level', 'bmi',\n       'smoking_status', 'diabetes']])","a5fc5c3b":"dataset_v2","1ea8eeeb":"# creating X and y.\nX = dataset_v2.iloc[:,:11]\ny = dataset_v2.iloc[:,-1]","405935c8":"print('X Shape', X.shape)\nprint('Y Shape',y.shape)","59a6ba89":"#splitting into train and test sets. \nX_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.25,random_state=None)\n\n#scalling\nsc = StandardScaler()\nX_train = sc.fit_transform(X_train)\nX_test = sc.transform(X_test)\n#\nprint('Number transations x_train df',X_train.shape)\nprint('Number transations x_test df',X_test.shape)\nprint('Number transations y_train df',y_train.shape)\nprint('Number transations y_test df',y_test.shape)","274c5202":"print('Before OverSampling, the shape of train_x: {}'.format(X_train.shape))\nprint('Before OverSampling, the shape of train_y: {}'.format(y_train.shape))\nprint('Before OverSampling, counts of label 1: {}'.format(sum(y_train==1)))\nprint('Before OverSampling, counts of label 0: {} \\n'.format(sum(y_train==0)))","021ffe06":"from imblearn.over_sampling import SMOTE\nsm = SMOTE(random_state=2)\nX_train_res, y_train_res = sm.fit_resample(X_train,y_train.ravel())","23d002dc":"print('After OverSampling, the shape of train_x: {}'.format(X_train_res.shape))\nprint('After OverSampling, the shape of train_y: {}'.format(y_train_res.shape))\nprint('After OverSampling, counts of label 1: {}'.format(sum(y_train_res == 1)))\nprint('After OverSampling, counts of label 0: {}'.format(sum(y_train_res == 0)))","1b3c8f7b":"from sklearn.linear_model import LogisticRegressionCV\n\nlr=LogisticRegression().fit(X_train_res, y_train_res)\n\n#L1\nlr_l1 = LogisticRegressionCV(Cs=10, cv=4, penalty='l1', solver='liblinear',max_iter=2500).fit(X_train_res, y_train_res)\n\n#L2\nlr_l2 = LogisticRegressionCV(Cs=10, cv=4, penalty='l2', solver='liblinear',max_iter=1300).fit(X_train_res, y_train_res)","3d0f3bf0":"y_pred = list()\ny_prob = list()\n\ncoeff_labels = ['lr','lr1','lr2']\ncoeff_models = [lr,lr_l1,lr_l2 ]\n\nfor lab,mod in zip(coeff_labels, coeff_models):\n    y_pred.append(pd.Series(mod.predict(X_test), name=lab))\n    #Taking .max() of probability \n    y_prob.append(pd.Series(mod.predict_proba(X_test).max(axis=1), name=lab))\n    \ny_pred = pd.concat(y_pred, axis=1)\ny_prob = pd.concat(y_prob, axis=1)\n\n# y_pred.head()","47ec8982":"y_prob.head()","1b51aed4":"from sklearn.metrics import precision_recall_fscore_support as score\nfrom sklearn.metrics import confusion_matrix, accuracy_score, roc_auc_score\nfrom sklearn.preprocessing import label_binarize\n\nmetrics = list()\ncm = dict()\n\nfor lab in coeff_labels:\n\n    # Preciision, recall, f-score from the multi-class support function\n    precision, recall, fscore, _ = score(y_test, y_pred[lab], average='weighted')\n    \n    # The usual way to calculate accuracy\n    accuracy = accuracy_score(y_test, y_pred[lab])\n    \n    # ROC-AUC scores can be calculated by binarizing the data\n    auc = roc_auc_score(label_binarize(y_test, classes=[0,1]),\n              label_binarize(y_pred[lab], classes=[0,1]), \n              average='weighted')\n    \n    # Last, the confusion matrix\n    cm[lab] = confusion_matrix(y_test, y_pred[lab])\n    \n    metrics.append(pd.Series({'precision':precision, 'recall':recall, \n                              'fscore':fscore, 'accuracy':accuracy,\n                              'auc':auc}, \n                             name=lab))\n\nmetrics = pd.concat(metrics, axis=1)","7a7fcaf3":"metrics","a2a4c789":"from sklearn.neighbors import KNeighborsClassifier","35d62c28":"knn = KNeighborsClassifier(n_neighbors=3)\nknn = knn.fit(X_train_res, y_train_res)\ny_pred = knn.predict(X_test)\nprint(classification_report(y_test, y_pred))\nprint('Accuracy score: ', round(accuracy_score(y_test, y_pred), 2))\nprint('F1 Score: ', round(f1_score(y_test, y_pred), 2))","39002154":"# Plot confusion matrix\nsn.set_palette(sn.color_palette(colors))\n_, ax = plt.subplots(figsize=(12,12))\nax = sn.heatmap(confusion_matrix(y_test, y_pred), annot=True, fmt='d', cmap=colors, annot_kws={\"size\": 40, \"weight\": \"bold\"})  \nlabels = ['True', 'False']\nax.set_xticklabels(labels, fontsize=25);\nax.set_yticklabels(labels, fontsize=25);\nax.set_ylabel('Prediction', fontsize=30);\nax.set_xlabel('Ground Truth', fontsize=30)","74ca282e":"from xgboost import XGBClassifier","dc51d45b":"xgb = XGBClassifier(learning_rate=0.1,objective='binary:logistic',random_state=0,eval_metric='mlogloss',use_label_encoder=False)\nxgb = xgb.fit(X_train_res, y_train_res)\n\ny_pred = xgb.predict(X_test)\nprint(classification_report(y_test, y_pred))\nprint('Accuracy score: ', round(accuracy_score(y_test, y_pred), 2))\nprint('F1 Score: ', round(f1_score(y_test, y_pred), 2))","2074d024":"# MODEL 2 \nK-Nearest Neighbors. \n\n\nTHAT'S BASICALLY IT BUT TRY TO UNDERSTAND IT YES?","87b070ad":"# Summary Key Findings and Insights.\nAt the beginning I thought that there will be strong correlation between each features, like if patient is smoking cigarettes he\/she would have higher chance of getting stroke, of course each column had positive correlation with target variable but none was higher than 0.5.\n\nAlso Residence_type has almost nothing to do with higher or lower probability of having a stroke which for me was quite surprising.\n","af484eea":"# Exploratory data analysis (EDA)","5a78ee07":"# MODEL 3\nXGBoost","bae19429":"# Brief description of the data set and a summary of its attributes\n\nAccording to the World Health Organization (WHO) stroke is the 2nd leading cause of death globally, responsible for approximately 11% of total deaths.\nThis dataset is used to predict whether a patient is likely to get stroke based on the input parameters like gender, age, various diseases, and smoking status. Each row in the data provides relavant information about the patient.\n\n1. id: unique identifier\n\n2. gender: \"Male\", \"Female\" or \"Other\"\n\n3. age: age of the patient\n\n4. hypertension: 0 if the patient doesn't have hypertension, 1 if the patient has hypertension\n\n5. heart_disease: 0 if the patient doesn't have any heart diseases, 1 if the patient has a heart disease\n\n6. ever_married: \"No\" or \"Yes\"\n\n7. work_type: \"children\", \"Govt_jov\", \"Never_worked\", \"Private\" or \"Self-employed\"\n\n8. Residence_type: \"Rural\" or \"Urban\"\n\n9. avg_glucose_level: average glucose level in blood\n\n10. bmi: body mass index\n\n11. smoking_status: \"formerly smoked\", \"never smoked\", \"smokes\" or \"Unknown\"*\n\n12. stroke: 1 if the patient had a stroke or 0 if not","b02832c1":"# Recommendation for final model.\nI trained 3 different models on the same training and test splits.I would definitely recommend XGBoost beacuse it got the best result and it was very fast.","4e4dbb10":"# MODEL 1 \n\nLogistic Regression models with standard,l1 and l2 penalites. I had to use SMOTE module as my target variable (y) had 20 to 1 ratio.\n","292ef690":"# Main objective(s) of this analysis.\nMain objective is to classify\/predict if a particular person based on their parameters is likely to have a stroke. This anaylsis might be very helpful and interesting in case of real-world problems as strokes account for as much as 11% of all deaths in the world. To sum up, I'm going to create my target variable y(stroke 0=no,1=yes) and try to predict output using features(basically rest of the columns) using different classification models.","d1136089":"# Suggestions for next steps\nn my opinion most efficient way to improve prediction score would be to add specific features that have high correlation with our target variable (stroke). I would focus on gathering more specific information about each patient and their health."}}