{"cell_type":{"63baf293":"code","dbbe4b9b":"code","179737b8":"code","a0bf4718":"code","c98a6f99":"code","8401175d":"code","18af1a6f":"code","f96691b6":"code","a4e998c3":"code","7931493e":"code","5c38b4ad":"code","807bb058":"code","00c36bf0":"code","db8ca533":"code","e2582865":"markdown","f43f885f":"markdown","88e20c7e":"markdown","1f1e9ba0":"markdown","458ce8ac":"markdown","75df197a":"markdown","9e8cf93a":"markdown","2ad615a3":"markdown","9fc4436f":"markdown"},"source":{"63baf293":"import numpy as np \nimport pandas as pd\nfrom sklearn.metrics import accuracy_score \nfrom sklearn.model_selection import cross_val_score\n\nimport os\nprint(os.listdir(\"..\/input\"))","dbbe4b9b":"train_data = pd.read_csv('..\/input\/train.csv',header = None)\ntrain_labels = pd.read_csv('..\/input\/trainLabels.csv',header = None)\ntest_data =  pd.read_csv('..\/input\/test.csv',header = None)","179737b8":"train_data.head()","a0bf4718":"train_data.shape,test_data.shape,train_labels.shape","c98a6f99":"train_data.describe()","8401175d":"from sklearn.model_selection import train_test_split\nx_train,x_test,y_train,y_test = train_test_split(train_data,train_labels, test_size = 0.30, random_state = 101)\nx_train.shape,x_test.shape,y_train.shape,y_test.shape","18af1a6f":"# NAIBE BAYES\nfrom sklearn.naive_bayes import GaussianNB\n\nmodel = GaussianNB()\nmodel.fit(x_train,y_train.values.ravel())\npredicted= model.predict(x_test)\nprint('Naive Bayes',accuracy_score(y_test, predicted))\n\n#KNN\nfrom sklearn.neighbors import KNeighborsClassifier\n\nknn_model = KNeighborsClassifier()\nknn_model.fit(x_train,y_train.values.ravel())\npredicted= knn_model.predict(x_test)\nprint('KNN',accuracy_score(y_test, predicted))\n\n#RANDOM FOREST\nfrom sklearn.ensemble import RandomForestClassifier\n\nrfc_model = RandomForestClassifier(n_estimators = 100,random_state = 99)\nrfc_model.fit(x_train,y_train.values.ravel())\npredicted = rfc_model.predict(x_test)\nprint('Random Forest',accuracy_score(y_test,predicted))\n\n#LOGISTIC REGRESSION\nfrom sklearn.linear_model import LogisticRegression\n\nlr_model = LogisticRegression(solver = 'saga')\nlr_model.fit(x_train,y_train.values.ravel())\nlr_predicted = lr_model.predict(x_test)\nprint('Logistic Regression',accuracy_score(y_test, lr_predicted))\n\n#SVM\nfrom sklearn.svm import SVC\n\nsvc_model = SVC(gamma = 'auto')\nsvc_model.fit(x_train,y_train.values.ravel())\nsvc_predicted = svc_model.predict(x_test)\nprint('SVM',accuracy_score(y_test, svc_predicted))\n\n#DECISON TREE\nfrom sklearn.tree import DecisionTreeClassifier\n\ndtree_model = DecisionTreeClassifier()\ndtree_model.fit(x_train,y_train.values.ravel())\ndtree_predicted = dtree_model.predict(x_test)\nprint('Decision Tree',accuracy_score(y_test, dtree_predicted))\n\n#XGBOOST\nfrom xgboost import XGBClassifier\n\nxgb = XGBClassifier()\nxgb.fit(x_train,y_train.values.ravel())\nxgb_predicted = xgb.predict(x_test)\nprint('XGBoost',accuracy_score(y_test, xgb_predicted))\n","f96691b6":"from sklearn.preprocessing import StandardScaler, Normalizer\n\nnorm = Normalizer()\n#x_norm_train = norm.fit_transform(x_train)\n#x_norm_test = norm.transform(x_test)\nnorm_train_data = norm.fit_transform(train_data)","a4e998c3":"# NAIBE BAYES\nfrom sklearn.naive_bayes import GaussianNB\n\nnb_model = GaussianNB()\n#nb_model.fit(x_norm_train,y_train.values.ravel())\n#nb_predicted= nb_model.predict(x_norm_test)\n#print('Naive Bayes',accuracy_score(y_test, nb_predicted))\nprint('Naive Bayes',cross_val_score(nb_model,norm_train_data, train_labels.values.ravel(), cv=10).mean())\n\n#KNN\nfrom sklearn.neighbors import KNeighborsClassifier\n\nknn_model = KNeighborsClassifier(n_neighbors = 5)\n#knn_model.fit(x_norm_train,y_train.values.ravel())\n#knn_predicted= knn_model.predict(x_norm_test)\n#print('KNN',accuracy_score(y_test, knn_predicted))\nprint('KNN',cross_val_score(knn_model,norm_train_data, train_labels.values.ravel(), cv=10).mean())\n\n#RANDOM FOREST\nfrom sklearn.ensemble import RandomForestClassifier\n\nrfc_model = RandomForestClassifier(n_estimators = 100,random_state = 99)\n#rfc_model.fit(x_norm_train,y_train.values.ravel())\n#rfc_predicted = rfc_model.predict(x_norm_test)\n#print('Random Forest',accuracy_score(y_test,rfc_predicted))\nprint('Random Forest',cross_val_score(rfc_model,norm_train_data, train_labels.values.ravel(), cv=10).mean())\n\n#LOGISTIC REGRESSION\nfrom sklearn.linear_model import LogisticRegression\n\nlr_model = LogisticRegression(solver = 'saga')\n#lr_model.fit(x_norm_train,y_train.values.ravel())\n#lr_predicted = lr_model.predict(x_norm_test)\n#print('Logistic Regression',accuracy_score(y_test, lr_predicted))\nprint('Logistic Regression',cross_val_score(lr_model,norm_train_data, train_labels.values.ravel(), cv=10).mean())\n\n#SVM\nfrom sklearn.svm import SVC\n\nsvc_model = SVC(gamma = 'auto')\n#svc_model.fit(x_norm_train,y_train.values.ravel())\n#svc_predicted = svc_model.predict(x_norm_test)\n#print('SVM',accuracy_score(y_test, svc_predicted))\nprint('SVM',cross_val_score(svc_model,norm_train_data, train_labels.values.ravel(), cv=10).mean())\n\n#DECISON TREE\nfrom sklearn.tree import DecisionTreeClassifier\n\ndtree_model = DecisionTreeClassifier()\n#dtree_model.fit(x_norm_train,y_train.values.ravel())\n#dtree_predicted = dtree_model.predict(x_norm_test)\n#print('Decision Tree',accuracy_score(y_test, dtree_predicted))\nprint('Decision Tree',cross_val_score(dtree_model,norm_train_data, train_labels.values.ravel(), cv=10).mean())\n\n#XGBOOST\nfrom xgboost import XGBClassifier\n\nxgb = XGBClassifier()\n#xgb.fit(x_norm_train,y_train.values.ravel())\n#xgb_predicted = xgb.predict(x_norm_test)\n#print('XGBoost',accuracy_score(y_test, xgb_predicted))\nprint('XGBoost',cross_val_score(xgb,norm_train_data, train_labels.values.ravel(), cv=10).mean())","7931493e":"from sklearn.decomposition import PCA\n\npca  = PCA(n_components=12)\n#x_train = pca.fit_transform(x_train)\n#x_test = pca.transform(x_test)\npca_train_data = pca.fit_transform(train_data)\nexplained_variance = pca.explained_variance_ratio_ \nprint(explained_variance)","5c38b4ad":"pca_train_data.shape","807bb058":"# NAIBE BAYES\nfrom sklearn.naive_bayes import GaussianNB\n\nnb_model = GaussianNB()\n#nb_model.fit(pca_train_data,y_train.values.ravel())\n#nb_predicted= nb_model.predict(x_norm_test)\n#print('Naive Bayes',accuracy_score(y_test, nb_predicted))\nprint('Naive Bayes',cross_val_score(nb_model,pca_train_data, train_labels.values.ravel(), cv=10).mean())\n\n#KNN\nfrom sklearn.neighbors import KNeighborsClassifier\n\nknn_model = KNeighborsClassifier(n_neighbors = 5)\n#knn_model.fit(pca_train_data,y_train.values.ravel())\n#knn_predicted= knn_model.predict(x_norm_test)\n#print('KNN',accuracy_score(y_test, knn_predicted))\nprint('KNN',cross_val_score(knn_model,pca_train_data, train_labels.values.ravel(), cv=10).mean())\n\n#RANDOM FOREST\nfrom sklearn.ensemble import RandomForestClassifier\n\nrfc_model = RandomForestClassifier(n_estimators = 100,random_state = 99)\n#rfc_model.fit(pca_train_data,y_train.values.ravel())\n#rfc_predicted = rfc_model.predict(x_norm_test)\n#print('Random Forest',accuracy_score(y_test,rfc_predicted))\nprint('Random Forest',cross_val_score(rfc_model,pca_train_data, train_labels.values.ravel(), cv=10).mean())\n\n#LOGISTIC REGRESSION\nfrom sklearn.linear_model import LogisticRegression\n\nlr_model = LogisticRegression(solver = 'saga')\n#lr_model.fit(pca_train_data,y_train.values.ravel())\n#lr_predicted = lr_model.predict(x_norm_test)\n#print('Logistic Regression',accuracy_score(y_test, lr_predicted))\nprint('Logistic Regression',cross_val_score(lr_model,pca_train_data, train_labels.values.ravel(), cv=10).mean())\n\n#SVM\nfrom sklearn.svm import SVC\n\nsvc_model = SVC(gamma = 'auto')\n#svc_model.fit(x_norm_train,y_train.values.ravel())\n#svc_predicted = svc_model.predict(x_norm_test)\n#print('SVM',accuracy_score(y_test, svc_predicted))\nprint('SVM',cross_val_score(svc_model,pca_train_data, train_labels.values.ravel(), cv=10).mean())\n\n#DECISON TREE\n\nfrom sklearn.tree import DecisionTreeClassifier\n\ndtree_model = DecisionTreeClassifier()\n#dtree_model.fit(x_norm_train,y_train.values.ravel())\n#dtree_predicted = dtree_model.predict(x_norm_test)\n#print('Decision Tree',accuracy_score(y_test, dtree_predicted))\nprint('Decision Tree',cross_val_score(dtree_model,pca_train_data, train_labels.values.ravel(), cv=10).mean())\n\n#XGBOOST\nfrom xgboost import XGBClassifier\n\nxgb = XGBClassifier()\n#xgb.fit(x_norm_train,y_train.values.ravel())\n#xgb_predicted = xgb.predict(x_norm_test)\n#print('XGBoost',accuracy_score(y_test, xgb_predicted))\nprint('XGBoost',cross_val_score(xgb,pca_train_data, train_labels.values.ravel(), cv=10).mean())","00c36bf0":"from sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import GridSearchCV, cross_val_score\nfrom sklearn.mixture import GaussianMixture\nfrom sklearn.svm import SVC\n\nx_all = np.r_[train_data,test_data]\nprint('x_all shape :',x_all.shape)\n\n# USING THE GAUSSIAN MIXTURE MODEL \nlowest_bic = np.infty\nbic = []\nn_components_range = range(1, 7)\ncv_types = ['spherical', 'tied', 'diag', 'full']\nfor cv_type in cv_types:\n    for n_components in n_components_range:\n        gmm = GaussianMixture(n_components=n_components,covariance_type=cv_type)\n        gmm.fit(x_all)\n        bic.append(gmm.aic(x_all))\n        if bic[-1] < lowest_bic:\n            lowest_bic = bic[-1]\n            best_gmm = gmm\n            \nbest_gmm.fit(x_all)\ngmm_train = best_gmm.predict_proba(train_data)\ngmm_test = best_gmm.predict_proba(test_data)\n\n\n#Random Forest Classifier\nrfc = RandomForestClassifier(random_state=99)\n\n#USING GRID SEARCH\nn_estimators = [10, 50, 100, 200,400]\nmax_depth = [3, 10, 20, 40]\nparam_grid = dict(n_estimators=n_estimators,max_depth=max_depth)\n\ngrid_search_rfc = GridSearchCV(estimator=rfc, param_grid=param_grid, cv = 10,scoring='accuracy',n_jobs=-1).fit(gmm_train, train_labels.values.ravel())\nrfc_best = grid_search_rfc.best_estimator_\nprint('Random Forest Best Score',grid_search_rfc.best_score_)\nprint('Random Forest Best Parmas',grid_search_rfc.best_params_)\nprint('Random Forest Accuracy',cross_val_score(rfc_best,gmm_train, train_labels.values.ravel(), cv=10).mean())\n\n#KNN \nknn = KNeighborsClassifier()\n\n#USING GRID SEARCH\nn_neighbors=[3,5,6,7,8,9,10]\nparam_grid = dict(n_neighbors=n_neighbors)\n\ngrid_search_knn = GridSearchCV(estimator=knn, param_grid=param_grid, cv = 10, n_jobs=-1,scoring='accuracy').fit(gmm_train,train_labels.values.ravel())\nknn_best = grid_search_knn.best_estimator_\nprint('KNN Best Score', grid_search_knn.best_score_)\nprint('KNN Best Params',grid_search_knn.best_params_)\nprint('KNN Accuracy',cross_val_score(knn_best,gmm_train, train_labels.values.ravel(), cv=10).mean())\n\n#SVM\nsvc = SVC()\n\n#USING GRID SEARCH\nparameters = [{'kernel':['linear'],'C':[1,10,100]},\n              {'kernel':['rbf'],'C':[1,10,100],'gamma':[0.05,0.0001,0.01,0.001]}]\ngrid_search_svm = GridSearchCV(estimator=svc, param_grid=parameters, cv = 10, n_jobs=-1,scoring='accuracy').fit(gmm_train, train_labels.values.ravel())\nsvm_best = grid_search_svm.best_estimator_\nprint('SVM Best Score',grid_search_svm.best_score_)\nprint('SVM Best Params',grid_search_svm.best_params_)\nprint('SVM Accuracy',cross_val_score(svm_best,gmm_train, train_labels.values.ravel(), cv=10).mean())\n","db8ca533":"rfc_best.fit(gmm_train,train_labels.values.ravel())\npred  = rfc_best.predict(gmm_test)\nrfc_best_pred = pd.DataFrame(pred)\n\nrfc_best_pred.index += 1\n\nrfc_best_pred.columns = ['Solution']\nrfc_best_pred['Id'] = np.arange(1,rfc_best_pred.shape[0]+1)\nrfc_best_pred = rfc_best_pred[['Id', 'Solution']]\n\nrfc_best_pred.to_csv('Submission_GMM_RFC.csv',index=False)","e2582865":"**Train-Test Split**","f43f885f":"**KNN** gave maximum accuracy using Feature Scaling.","88e20c7e":"We select the above three algorithms (KNN, Random Forest and SVM) which  gave maximum accuracy for further analysis","1f1e9ba0":"## **Feature Scaling**","458ce8ac":"**KNN**, **Random** **Forest** and **SVM** gave maximum accuracy using Principal Component Analysis.\n","75df197a":"## **Applying Gaussian Mixture and Grid Search to improve the accuracy**","9e8cf93a":"## **Principal Component Analysis**","2ad615a3":"## **PRE-PROCESSING**","9fc4436f":"## **CLASSIFICATION**"}}