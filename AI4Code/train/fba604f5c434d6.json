{"cell_type":{"6a6eda3c":"code","7ed2d0c4":"code","0f57fc74":"code","4ded5440":"code","d5606b80":"code","ab88cfe0":"code","33fcd6e3":"code","5b0d8d89":"code","7c6881e4":"code","7432ff89":"code","96a37a7b":"code","ed1d8a6b":"code","fd2a9411":"code","3eef5500":"code","9dc34540":"code","86df7712":"code","88752bc1":"code","41539b61":"code","bd8cc071":"code","31b5e8d3":"code","792b1a94":"code","dadcf73a":"code","f2c38384":"code","23c117cf":"code","7bb40397":"code","ef52a8e4":"code","023ea7cd":"code","b53ae203":"code","b41a95e7":"code","1047850c":"code","fdafc27f":"markdown","4f7c3b7d":"markdown","b6eee866":"markdown","ddc799ee":"markdown","a574328a":"markdown","4f587b07":"markdown","2b47fe23":"markdown","77b1c4c6":"markdown"},"source":{"6a6eda3c":"!pip install nlp","7ed2d0c4":"import tensorflow as tf\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport random\nimport nlp","0f57fc74":"dataset = nlp.load_dataset('emotion')\ndataset","4ded5440":"train = dataset['train']\ntest = dataset['test']\nval = dataset['validation']","d5606b80":"def get_tweet(data):\n    tweets = [x['text'] for x in data]\n    labels = [x['label'] for x in data]\n    return tweets , labels","ab88cfe0":"tweets , labels = get_tweet(train)\ntweets[0] , labels[0]","33fcd6e3":"from tensorflow.keras.preprocessing.text import Tokenizer\n\ntokenizer = Tokenizer(num_words = 10000 , oov_token = '<UNK>')\ntokenizer.fit_on_texts(tweets)","5b0d8d89":"print(tokenizer.texts_to_sequences([tweets[0]]))\nprint(tweets[0])","7c6881e4":"#the x axis is the tweet lenghts 10 - 20 is the most\nlengths = [len(tweet.split(' ')) for tweet in tweets]\nplt.hist(lengths)\nplt.show()","7432ff89":"from tensorflow.keras.preprocessing.sequence import pad_sequences\n\nmaxlen = 50\n\ndef get_sequences(tokenizer , tweets):\n    \n    \n    sequences = tokenizer.texts_to_sequences(tweets)\n    padded = pad_sequences(sequences , truncating = 'post' ,\n                             padding = 'post' ,\n                             maxlen = maxlen)\n    return padded","96a37a7b":"padded_train_seq = get_sequences(tokenizer , tweets)","ed1d8a6b":"print('the tweet is : ' , tweets[0])\nprint()\nprint('after tokenizing and make it seq : ' ,tokenizer.texts_to_sequences([tweets[0]]))\nprint()\nprint('after padding : ' ,padded_train_seq[0])","fd2a9411":"classes = set(labels)\nprint(classes)","3eef5500":"plt.hist(labels , bins = 11)\nplt.show","9dc34540":"class_to_index = dict((c , i) for i , c in enumerate(classes))\nindex_to_class = dict((i , c) for c , i in class_to_index.items())","86df7712":"print(class_to_index)","88752bc1":"print(index_to_class)","41539b61":"names_to_ids = lambda labels : np.array([class_to_index.get(x) for x in labels])\ntrain_labels = names_to_ids(labels)\ntrain_labels[0]","bd8cc071":"model = tf.keras.models.Sequential([\n                           tf.keras.layers.Embedding(10000 , 16 , input_length = maxlen),\n                           tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(20 , return_sequences=True)),\n                           tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(20)),\n                           tf.keras.layers.Dense(6 , activation='softmax')\n])\n\nmodel.compile(\n    optimizer = 'adam',\n    loss = 'sparse_categorical_crossentropy',\n    metrics = ['accuracy']\n)\n","31b5e8d3":"model.summary()","792b1a94":"val_tweets , V_labels = get_tweet(val)\nval_padded = get_sequences(tokenizer , val_tweets)\nval_labels = names_to_ids(V_labels)","dadcf73a":"val_tweets[0] , val_labels[0]","f2c38384":"h = model.fit(padded_train_seq , train_labels ,\n              validation_data=(val_padded , val_labels),\n              epochs = 20,\n              callbacks = [tf.keras.callbacks.EarlyStopping(monitor = 'val_accuracy' , patience=2)\n              ])","23c117cf":"def show_history(h):\n    epochs_trained = len(h.history['loss'])\n    plt.figure(figsize=(16, 6))\n\n    plt.subplot(1, 2, 1)\n    plt.plot(range(0, epochs_trained), h.history.get('accuracy'), label='Training')\n    plt.plot(range(0, epochs_trained), h.history.get('val_accuracy'), label='Validation')\n    plt.ylim([0., 1.])\n    plt.xlabel('Epochs')\n    plt.ylabel('Accuracy')\n    plt.legend()\n\n    plt.subplot(1, 2, 2)\n    plt.plot(range(0, epochs_trained), h.history.get('loss'), label='Training')\n    plt.plot(range(0, epochs_trained), h.history.get('val_loss'), label='Validation')\n    plt.xlabel('Epochs')\n    plt.ylabel('Loss')\n    plt.legend()\n    plt.show()\n\n    \ndef show_confusion_matrix(y_true, y_pred, classes):\n    from sklearn.metrics import confusion_matrix\n    \n    cm = confusion_matrix(y_true, y_pred, normalize='true')\n\n    plt.figure(figsize=(8, 8))\n    sp = plt.subplot(1, 1, 1)\n    ctx = sp.matshow(cm)\n    plt.xticks(list(range(0, 6)), labels=classes)\n    plt.yticks(list(range(0, 6)), labels=classes)\n    plt.colorbar(ctx)\n    plt.show()","7bb40397":"show_history(h)","ef52a8e4":"test_tweets , test_labels = get_tweet(test)\npadded_test_tweets = get_sequences(tokenizer , test_tweets)\ntest_labels = names_to_ids(test_labels)","023ea7cd":"_  = model.evaluate(padded_test_tweets , test_labels)","b53ae203":"i = random.randint(0 , len(test_labels)-1)\n\nprint('sentence : ' , test_tweets[i])\nprint('emotion : ' , index_to_class[test_labels[i]])\n\np = model.predict(np.expand_dims(padded_test_tweets[i] , axis=0))[0]\npred_class = index_to_class[np.argmax(p)]\nprint('prediction : ' , pred_class)","b41a95e7":"preds = model.predict_classes(padded_test_tweets)","1047850c":"show_confusion_matrix(preds , test_labels , list(classes))","fdafc27f":"#  Tokenizer","4f7c3b7d":"# importing the data","b6eee866":"# building the model","ddc799ee":"# padding and truncating","a574328a":"# evaluating the model","4f587b07":"# preparing the classes","2b47fe23":"preparing the validation data","77b1c4c6":"# training the model"}}