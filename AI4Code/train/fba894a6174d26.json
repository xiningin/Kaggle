{"cell_type":{"dd5eef2c":"code","9e47c24f":"code","6176c1a1":"code","8675c5b8":"code","e61094f5":"code","0f833eb5":"code","4cec5ccc":"code","6bf41e0d":"code","2d4333cb":"code","da5140c4":"code","abae78ef":"code","abba667a":"code","dec97fe6":"code","44f1dc43":"code","21f24ed1":"code","0979ffd1":"code","eb207d0d":"code","61cd63c1":"code","949df534":"code","d8991807":"code","0045c8c8":"code","7cdd03e7":"code","8fc1389c":"code","411ae040":"code","b3ae481a":"code","81eca5b6":"code","403cf47e":"code","fd8caf7d":"code","a393c3ed":"code","df7cb97b":"code","b4390346":"code","3b631fee":"code","d7fa190d":"code","91fba097":"code","ea247670":"code","f857b05e":"code","6cc863ae":"code","a761ab33":"code","56fd5805":"code","2f8c93dd":"code","266f2f19":"code","b5f93d70":"code","c9485098":"code","c979ddd3":"markdown","3b11b33b":"markdown","c9b94f33":"markdown","ba64c33e":"markdown","28b3da5c":"markdown","d9c7e5cf":"markdown","2667daae":"markdown","c7fac607":"markdown","fc0a2634":"markdown","d6f4a461":"markdown","24f64fc9":"markdown","0c7f9715":"markdown","4b4b8307":"markdown","380b1c77":"markdown","65d29646":"markdown","9d1eee52":"markdown","966c9236":"markdown","a2462946":"markdown","8e2b8482":"markdown","7ccba66d":"markdown","c20bbfb2":"markdown","95fb79e7":"markdown","632eabdd":"markdown","af19ce13":"markdown","59ae2a18":"markdown","801f0d8e":"markdown","f1641c28":"markdown","55e2d094":"markdown","941290a9":"markdown","fe5c72f1":"markdown","02e708cd":"markdown","fffa1d03":"markdown","48c4d20f":"markdown","23fa4683":"markdown","5975df3c":"markdown","c67358d6":"markdown","e6879ad7":"markdown","3deb2f5b":"markdown","3f7736d4":"markdown","cd63ed0b":"markdown","174d6d09":"markdown","84c68cc5":"markdown","263c123e":"markdown","cc2e346b":"markdown"},"source":{"dd5eef2c":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","9e47c24f":"train = pd.read_csv('..\/input\/titanic\/train.csv')\ntest = pd.read_csv('..\/input\/titanic\/test.csv')\n\n\npd.DataFrame(train.dtypes, columns=['Type'])","6176c1a1":"train.head(10)","8675c5b8":"train.describe()","e61094f5":"train.isna().mean().round(4) * 100","0f833eb5":"test.isna().mean().round(4) * 100","4cec5ccc":"colors = ['#000099', '#ffff00'] # specify the colours - yellow is missing. blue is not missing.\nsns.heatmap(train.isnull(), cmap=sns.color_palette(colours))","6bf41e0d":"# create new df so we can keep the original one and manipulate a new one\ntrain_model = train.copy()\ntest_model = test.copy()\n#add the median for age column\ntrain_model['Age'] = train_model['Age'].fillna(train_model['Age'].median())\ntest_model['Age'] = test_model['Age'].fillna(train_model['Age'].median())\n\n#the train dataset had missing values for fare, so we will do the same but for fare feature on test set\ntest_model['Fare'] = test_model['Fare'].fillna(train_model['Fare'].median())\n\ntest.head(10)","2d4333cb":"train_model = train_model.drop(['Cabin'],axis=1)\ntest_model = test_model.drop(['Cabin'],axis=1)\ntest_model.head(10)","da5140c4":"#The test dataset didn't have missing values for Embarked, so we will apply the imputation only for the train dataset\ntrain_model['Embarked'] = train_model['Embarked'].fillna(train_model['Embarked'].mode()[0])\n\ntrain_model.isnull().mean().round(4) * 100","abae78ef":"test_model.isnull().mean().round(4) * 100","abba667a":"import matplotlib.pyplot as plt\n\n\nf, ax = plt.subplots(figsize=(10, 8))\nax = sns.violinplot(x=\"Survived\", y=\"Fare\", data=train_model)\nax.set_xticklabels(['no','yes']);\n\n","dec97fe6":"f, ax = plt.subplots(figsize=(10, 8))\nax = sns.violinplot(x=\"Sex\", y=\"Age\", hue=\"Survived\", data=train_model,split=True);\n","44f1dc43":"pclass_total = train_model.groupby(['Pclass']).sum()\npclass_total = pclass_total['Survived']\nplot = pclass_total.plot.pie(figsize=(8, 6),autopct='%1.1f%%')\nplt.title('Percentage of survivals by class')","21f24ed1":"#plot distribution for weekly sales\nf, ax = plt.subplots(figsize=(10, 8))\nax = sns.distplot(train_model['Fare'])\nplt.ylabel('Distribution');","0979ffd1":"f, ax = plt.subplots(figsize=(10, 8))\nax = sns.boxplot(x=\"Embarked\", y=\"Fare\", hue=\"Survived\",\n                 data=train_model, palette=\"Set3\");","eb207d0d":"def plot_bar(variable):\n    rating_probs_die = pd.DataFrame(train_model[train_model.Survived == 0].groupby(variable).size().div(len(train_model)))\n    rating_probs_survived = pd.DataFrame(train_model[train_model.Survived == 1].groupby(variable).size().div(len(train_model)))\n    df = rating_probs_die.merge(rating_probs_survived,how='outer',left_index=True, right_index=True)\n    df.columns = ['died', 'survived']\n    ax = df.plot.bar(rot=0,colormap='Paired')\n    \nplot_bar(\"SibSp\")    \n","61cd63c1":"plot_bar(\"Parch\")","949df534":"## Family_size seems like a good feature to create\ntrain_model['FamilySize'] = train_model.SibSp + train_model.Parch+1\ntest_model['FamilySize'] = test_model.SibSp + test_model.Parch+1\n\nplot_bar(\"FamilySize\")  ","d8991807":"## get the most important variables. \ncorr = train_model.corr()**2\ncorr.Survived.sort_values(ascending=False)\n\nmask = np.zeros_like(train.corr(), dtype=np.bool)\nmask[np.triu_indices_from(mask)] = True\nsns.set_style('whitegrid')\nplt.subplots(figsize = (15,12))\nsns.heatmap(train.corr(), \n            annot=True,\n            mask = mask,\n            cmap = 'RdBu', ## in order to reverse the bar replace \"RdBu\" with \"RdBu_r\"\n            linewidths=.9, \n            linecolor='white',\n            fmt='.2g',\n            center = 0,\n            square=True)\nplt.title(\"Correlations Among Features\", y = 1.03,fontsize = 20, pad = 40);","0045c8c8":"\ndef has_SibSp_Parch(dataset):\n    ''' this function creates a new feature if the person has 1 sibling \n    or spouse aboard the Titanic '''\n    dataset['has_SibSp'] = dataset['SibSp'].apply(lambda x: 'True' if x==1 else 'False')\n    dataset['has_Parch'] = dataset['Parch'].apply(lambda x: 'True' if x==1 else 'False')\n    \n    return dataset\n    \ntrain_model = has_SibSp_Parch(train_model)\ntest_model = has_SibSp_Parch(test_model)\n    ","7cdd03e7":"## get the title from the name\ntrain_model[\"title\"] = [i.split('.')[0] for i in train_model.Name]\ntrain_model[\"title\"] = [i.split(',')[1] for i in train_model.title]\n\ntest_model[\"title\"] = [i.split('.')[0] for i in test_model.Name]\ntest_model[\"title\"] = [i.split(',')[1] for i in test_model.title]\n\ntrain_model[\"title\"].value_counts()","8fc1389c":"import re\n\n\n\ndef has_gender(dataset):\n    ''' this function adds a new feature if it is mr, mrs, or miss. \n    It uses regular expressios in the case of Mr'''\n    dataset['Mr'] = dataset['title'].apply(lambda x: 'True'  if re.search(r'Mr\\b',x)  else 'False')\n    dataset['Mrs'] = dataset['title'].apply(lambda x: 'True'  if 'Mrs' in x else 'False')\n    dataset['Miss'] = dataset['title'].apply(lambda x: 'True'  if 'Miss' in x else 'False')\n    dataset['Master'] = dataset['title'].apply(lambda x: 'True'  if 'Master' in x else 'False')\n    dataset['Rare'] = dataset['title'].apply(lambda x: 'True'  if not re.search(r'Mr\\b',x)  and 'Mrs' not in x and 'Miss' not in x else 'False')\n    dataset = dataset.drop(['title'],axis=1)\n    \n    return dataset\n    \n\ntrain_model = has_gender(train_model)\ntest_model = has_gender(test_model)","411ae040":"def family_size(dataset):\n    ''' this function creates a new feature if the person has 1 sibling \n    or spouse aboard the Titanic '''\n    dataset['alone'] = dataset['FamilySize'].apply(lambda x: 'True' if x==1 else 'False')\n    dataset['small_family'] = dataset['FamilySize'].apply(lambda x: 'True' if x in np.arange(2, 4) else 'False')\n    dataset['big_family'] = dataset['FamilySize'].apply(lambda x: 'True' if x>=4 else 'False')\n    \n    return dataset\n\n#train_model = family_size(train_model)\n#test_model = family_size(test_model)","b3ae481a":"def clean(dataset):\n    dataset = dataset.drop(['Name'],axis=1)\n    dataset = dataset.drop(['PassengerId'],axis=1)\n    dataset = dataset.drop(['Ticket'],axis=1)\n    \n    return dataset\n\ntrain_model = clean(train_model)\ntest_model = clean(test_model)\n","81eca5b6":"train_model = pd.get_dummies(train_model)\ntest_model = pd.get_dummies(test_model)","403cf47e":"!pip install pycaret","fd8caf7d":"from pycaret.classification import * #this is not a good practice, but since we are only testing, I will import everything","a393c3ed":"model_setup = setup(data=train_model, target='Survived',session_id=1,silent = True)\n","df7cb97b":"compare_models()","b4390346":"model = create_model('gbc',fold=10)","3b631fee":"tuned_model = tune_model('gbc',fold=10)","d7fa190d":"# plotting a model\nplot_model(tuned_model,plot = 'auc')","91fba097":"# plotting a model\nplot_model(tuned_model,plot = 'feature')\n","ea247670":"# plotting a model\nplot_model(tuned_model,plot = 'pr')\n","f857b05e":"# plotting a model\nplot_model(tuned_model,plot = 'confusion_matrix')","6cc863ae":"y_pred = predict_model(tuned_model, data=test_model)\n\n","a761ab33":"# blending all models\nblend_all = blend_models(method='hard')","56fd5805":"y_pred_blend = predict_model(blend_all, data=test_model)","2f8c93dd":"# create individual models for stacking\n\nlightgbm = create_model('lightgbm')\nxgboost = create_model('xgboost')\ncatboost = create_model('catboost')\nlda = create_model('lda')\nridge = create_model('ridge')\ngbc = create_model('gbc')\n\n\n","266f2f19":"# stacking models\nstacker = stack_models(estimator_list = [lightgbm,xgboost,ridge,catboost,lda,gbc],\n                       meta_model = xgboost,method='hard')","b5f93d70":"y_pred_stack = predict_model(stacker, data=test_model)","c9485098":"submission = pd.DataFrame({\n        \"PassengerId\": test[\"PassengerId\"],\n        \"Survived\": y_pred['Label']\n    })\n#submission.to_csv(\"submission.csv\", index=False)","c979ddd3":"The Titanic stopped in 3 different ports before the tragedy. In the map below we can see where these ports were located\n\n![ports_titanic.jpg](attachment:ports_titanic.jpg)\nSource: [https:\/\/br.pinterest.com\/pin\/390616967663919768\/](http:\/\/)","3b11b33b":"# Change Log:\n\n* 21\/04\/2020 This kernel is currently in progress, but open to feedback. \n* 22\/04\/2020 I finished the explanations and proof-reading.","c9b94f33":"# Blending models ","ba64c33e":"In this notebook, I will explore a new library called PyCaret on the Titanic dataset and some of its powerful functions. \n\nIn the modelling part of the Kernel, there will be 3 main parts:\n\n       1. Normal modelling using 1 model \n       2. Blending models\n       3. Stacking models","28b3da5c":"Now we can finally predict our model","d9c7e5cf":"The first step when using PyCaret is setting up the environment. Most of this setup part is done automatically, but you can set manually some parameters. For instance: \n\n* The default split ratio is 70:30, but can be changed with ```train_size```\n* k-fold cross-validation is set to 10 by default \n* ```session_id``` is our classic ```random_state```","2667daae":"Each passenger had a different title (Mr., Mrs., Dr...). This is could affect whether a person would survive or not since it's linked to wealthiness. The following function will create a new feature taking into consideration the titles. We see that the most common titles are: Mr, Miss, Mrs, and Master. Each one of these will have a specific feature. The rest of the titles will be blended into a new feature called \"rare\".","c7fac607":"Once you choose your model, you can create the model and then tune the model. In the next following examples, I apply both methods. ","fc0a2634":"C = Cherbourg, Q = Queenstown, S = Southampton","d6f4a461":"# Modelling with PyCaret ","24f64fc9":"Luckily for us, the dataset is pretty clean, so we won't have to worry about dropping many features. The features with missing data are: Age, cabin (train and test), and embarked (only train). I will deal with these features differently:\n\n1. add the median to the missing value in age (continuos)\n2. drop cabin feature since it is missing a lot of data\n3. use the mode for embarked (categorical) variable\n\n","0c7f9715":"The ```compare_models()``` function allows you to compare many models at once. This is for me one of the big advantages of using PyCaret. In just one line, you have a table comparison between many models. Moreover, it returns some metrics like accuracy, AUC, F1, etc. Another cool thing is how the library automatically highlights the best results.","4b4b8307":"As you can see, there was no improvement in the mean accuracy compared to the 1 model method. ","380b1c77":"Now that our code is cleaner, we can plot some of the variables. First we will start checking the distribution of survival per fares","65d29646":"One feature we can create is the passengers who had at 1 ```SibSp``` or ```Parch```. These groups survived more than the others as we saw in the analysis.","9d1eee52":"We impute the Embarket column with the most common value (mode). This way, we won't mess too much with the distribution of these features.","966c9236":"Import the train\/test set and check data ","a2462946":"#  EDA","8e2b8482":"We see that poorer people were more likely to die. For instance, people who paid more than \u00a3300 did not die **at all** (Damn capitalism!). If you've seen the movie, it's clear that rich people had privileges. We must take it into account $ in our analysis. \n","7ccba66d":"People who embarked in the Cherbourg port paid the most expensives tickets, but also were the ones who survived the most. On the other hand, people who embarked in the Queenstown port in Ireland paid the cheapest tickets and had more fatalities than survivals. Therefore, where the embarked plays an important role in the survival.","c20bbfb2":"The feature ```SibSp``` shows us the number of siblings\/spouses for passengers aboard the Titanic. We can see that people who had at least 1 sibling\/spouse had more chances of survival. This pattern repeats in the next analysis. ","95fb79e7":"We can also create a new feature called family size. However, in my tests, I observed that this feature didn't help the improvement of the model so much.","632eabdd":"# Data Cleaning\/Preparation","af19ce13":"![image.png](attachment:image.png)\n\nI came across this library while I was navigating in a slack for data science. It is a versatile library in which you can apply many models\/tests at once. \n\nAccording to their definition: *\"PyCaret is an open-source, low-code machine learning library in Python that aims to reduce the cycle time from hypothesis to insights. It enables data scientists and analysts to perform iterative end-to-end data science experiments efficiently and allows them to reach conclusions faster due to far less time spent in coding.\"*\n\nThe library seems very similar to the caret library in R, but implemented in python. ","59ae2a18":"# Feature engineering","801f0d8e":"Now the fun part of PyCaret is that it allows you to evaluate your model performance as easy as you build the model. For me, it's a bit boring to create each of these graphs, so having an easy way to quickly evaluate my model is perfect! There are 15 different plots available. I show some options below:\n\n\n","f1641c28":"You create the model and tune it with 2 lines. However, one thing I noticed is that the tuning of the model did not improve the mean accuracy in my model overall. I checked the documentation and it says: \n\n***\"In order to tune hyperparameters, the tune_model() function is used. This function automatically tunes the hyperparameters of a model on a pre-defined search space and scores it using stratified cross-validation.\"***\n\nI show below the function which tunes the model for the ```DecisionTreeClassifier```, so we can have an idea what it does.\n\n```\nfrom sklearn.tree import DecisionTreeClassifier\n\n\nparam_grid = {\"max_depth\": np.random.randint(1, (len(X_train.columns)*.85),20),\n              \"max_features\": np.random.randint(3, len(X_train.columns),20),\n              \"min_samples_leaf\": [2,3,4,5,6],\n              \"criterion\": [\"gini\", \"entropy\"],\n                     }\n```\n\nThe ```tune_model()``` function applies ```RandomizedSearchCV``` for the model taking into consideration the dataframe size. \n","55e2d094":"# Submission","941290a9":"Another interesting thing about the accident is that size of the family plays some role in the chance of survival. We see that people who were alone were 4 times more likely to die. However, if you were part of a small family (2-3 people), your chance of survival increases.","fe5c72f1":"Now we can start one of the most important parts of a data science project: Feature engineering. \n\nFeature engineering is the process of using domain knowledge to extract features from raw data via data mining techniques. These features can be used to improve the performance of machine learning algorithms. In this section, we will apply some of the knowledge we gained in the data analysis and create some new features that could help our model performance.","02e708cd":"Finally! Our data is almost ready! Yay. Now we only need to convert the categorical variables into numerical, so our model understands the input. We will apply a technique called \"One hot encoding\". One hot encoding is a process by which categorical variables are converted into a form that could be provided to ML algorithms to do a better job in prediction.","fffa1d03":"Installing the library ","48c4d20f":"Another important remark is that more women survived the disaster than men. Furthermore, the survival for both men and women has a bimodal distribution: 1 peak in the distribution around 10 years old and second peak around 30 years old. Unfortunately, the distribution is almost normal for men with most of the fatalities around 30 years old. So we know that gender has an important impact in whether a person survive or not.","23fa4683":"Cabin has more than 77% of data missing, so we will drop this feature ","5975df3c":" We can clearly see the graph above, which shows that most of the tickets were relatively cheap, so the 3rd class had the biggest number of survivals, but also the biggest number of fatalities. ","c67358d6":"Another method that PyCaret allows us to implement is blending models. According to the documentation, ```blen_models()``` is a method of ensembling that uses consensus among estimators to generate final predictions. The idea behind blending is to combine different machine learning algorithms and use a majority vote or the average predicted probabilities in case of classification to predict the final outcome. \n\nWe will try to see if ```blen_models()``` has any impact on our results. ```blen_models()``` can be used with some pre-defined models, which you can pass using ```estimator_list```. Furthermore, in the case of ```Classification```, method parameter can be used to define \u2018soft\u2018 or \u2018hard\u2018 where soft uses predicted probabilities for voting and hard uses predicted labels. For the sake of simplicity, I won't pass the list of models.","e6879ad7":"Check if there is NaNs in the data","3deb2f5b":"Our mean accuracy improved a bit from 0.8301 to 0.8316. Nothing spetacular, but it's a bit of improvement.","3f7736d4":"![titanic.jpg](attachment:titanic.jpg)\nSource:FOX","cd63ed0b":"# Stacking models","174d6d09":"PyCaret also allows us to ```stack_models()```. In the documentation, it defines stacking models as a method of ensembling that uses meta-learning. The idea behind stacking is to build a meta-model that generates the final prediction using the prediction of multiple base estimators.\n\nDifferently than ```blen_models()```, ```stack_models()``` requires ```estimator_list()``` to be passed, so I selected some model to try this method.","84c68cc5":"The output of setup is a table with all the possible options you could've set. As you can see, the are many possibilities. It is super useful if you want to try different methods. For instance, you can use one of the feature selection options to see if the model improves. ","263c123e":"As we saw earlier, there were 3 different classes in the Titanic. Most of the survivals (39.8%) were from the 1st class, followed by 3rd class. However, there was more fatalities in the 3rd class because more people bought cheaper tickets tickets, so it explains why it's the second biggest percentage of survivals. ","cc2e346b":"Now a classical correlation analysis. We can see that ```SibSp``` has a good correlation with Parch. \n\n\nNow with all the information, we learned from the data, you can start wondering what would be your chance of survival if you were aboard the Titanic. \n![Nazare%CC%81-Confusa.jpg](attachment:Nazare%CC%81-Confusa.jpg)"}}