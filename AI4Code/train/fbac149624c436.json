{"cell_type":{"be8b89e1":"code","391745e6":"code","804d86f5":"code","478dfa36":"code","5766cccf":"code","dd0efc2b":"code","ea7ba0bb":"code","60c181ec":"code","1673f316":"code","80d16e43":"code","aee47158":"code","10f6be5e":"code","a9f16a7e":"code","143b8553":"code","589385fd":"code","824dbcb1":"code","1eeaa927":"code","907e03a7":"code","b95a8080":"code","22c43430":"code","51059439":"code","0051f3f1":"code","e4abc137":"code","17380e66":"code","768f9186":"code","c8773f9f":"code","e24b695d":"code","02049729":"code","f2e6e9e1":"code","01d49311":"code","9af546ec":"code","c01cef95":"code","6978934d":"code","79dc74b0":"code","997a818a":"code","0fd5d7cc":"code","0e78ed93":"code","d0d9ebab":"code","d37a7d21":"code","fd5b1793":"code","adbe5092":"markdown","851e6d56":"markdown","43f627ad":"markdown","26d32822":"markdown","055453be":"markdown","4d09eb47":"markdown","2e13b811":"markdown","10baaa4f":"markdown","c1627b7e":"markdown","9e46f9b5":"markdown","fa7dbf60":"markdown","10f62a8d":"markdown","0f5aab16":"markdown","f075581d":"markdown","cee0ddeb":"markdown","82798670":"markdown","8704982e":"markdown","66f7d411":"markdown","5cb0467f":"markdown","372fd0f0":"markdown","c63d4fe7":"markdown","c76d186e":"markdown","973c530d":"markdown","5b2a337f":"markdown","a327cc9b":"markdown","b34a2fff":"markdown","93dd0999":"markdown","39ec4cb2":"markdown","5a76f5bb":"markdown","029b301b":"markdown","4212b9d0":"markdown","c6372e11":"markdown","e6d5a6bb":"markdown","6b787307":"markdown","54b7087d":"markdown","d201e3f6":"markdown","08aa909f":"markdown","02c8a97f":"markdown","fa06e909":"markdown","814e13e3":"markdown","269938ce":"markdown","f1365aba":"markdown","c0278c89":"markdown","d55e3a49":"markdown","acd858e4":"markdown","eccf836d":"markdown","5728491f":"markdown","a35071e5":"markdown","da5a1e61":"markdown","5e7c33bb":"markdown","946aae8c":"markdown","b59c7d31":"markdown","9f30360f":"markdown","bcf794a1":"markdown","54f277c9":"markdown","a42eb2c1":"markdown","b38e87f3":"markdown","87834a8f":"markdown","61ce2244":"markdown","ab63ecbe":"markdown","379395e8":"markdown","aaf9c186":"markdown","f986571e":"markdown","a5c39e03":"markdown","4440395a":"markdown","69ce57aa":"markdown","b7690d6c":"markdown","26618f60":"markdown","0b7c718a":"markdown","4da17ff3":"markdown","284d4fa8":"markdown","63bca461":"markdown","52068644":"markdown","7b3ef4d2":"markdown","09696ec4":"markdown","802e8cec":"markdown","28bbcf73":"markdown","0ce4790b":"markdown","ff0b5670":"markdown"},"source":{"be8b89e1":"\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","391745e6":"#IMPORTING THE DATASET\ndata= pd.read_csv('..\/input\/kaggle-survey-2019\/multiple_choice_responses.csv')\ndata=data.iloc[1:,:]\ndata.head()\n","804d86f5":"plt.figure(figsize=(15,8))    \nvisl= sns.countplot(data.Q1.sort_values(ascending=True))\n            \nfor p in visl.patches:\n    visl.annotate(format(p.get_height(), '.0f'), (p.get_x() + p.get_width() \/ 2., p.get_height()), \n               ha = 'center', va = 'center', xytext = (0, 10), textcoords = 'offset points')\nplt.xlabel('AGE',fontsize=15)\nplt.ylabel('COUNT',fontsize=15)\nplt.title('AGE DISTRIBUTION',fontsize=18)","478dfa36":"plt.figure(figsize=(15,8))    \ngenvis = sns.countplot(x= data.Q2)\n\nfor p in genvis.patches:\n    genvis.annotate(format(p.get_height(), '.0f'), (p.get_x() + p.get_width() \/ 2., p.get_height()), \n               ha = 'center', va = 'center', xytext = (0, 10), textcoords = 'offset points')\nplt.xlabel('GENDER',fontsize=15)\nplt.ylabel('COUNT',fontsize=15)\nplt.title('GENDER DISTRIBUTION',fontsize=18)","5766cccf":"x1=data.Q3.value_counts().index[:10]\ny1=data.Q3.value_counts().iloc[:10]\nplt.figure(figsize=(12,8))\ncountvis= sns.barplot(x=x1,y=y1)\n\nfor p in countvis.patches:\n    countvis.annotate(format(p.get_height(), '.0f'), (p.get_x() + p.get_width() \/ 2., p.get_height()), \n               ha = 'center', va = 'center', xytext = (0, 10), textcoords = 'offset points')\nplt.xlabel('COUNTRY',fontsize=15)\nplt.ylabel('COUNT',fontsize=15)\nplt.title('TOP 10 COUNTRIES',fontsize=18)\nplt.xticks(rotation=75)\nplt.show()\n","dd0efc2b":"plt.figure(figsize=(8,10))\nplt.pie(y1,labels=x1,explode=[0.1,0,0,0,0,0,0,0,0,0],shadow=True)\nplt.legend(x1,bbox_to_anchor=(2,1),loc='upper right')\nplt.draw()\n","ea7ba0bb":"top3=data[(data['Q3']=='India') | (data['Q3']=='United States of America') | (data['Q3']=='Brazil')]\n\nA1=sns.catplot(y='Q1',kind='count',height=8, hue='Q3', data=top3,order=top3['Q1'].value_counts().index)\nplt.xlabel('COUNT',fontsize=15)\nplt.ylabel('AGE GROUP',fontsize=15)\nplt.title('AGE vs COUNTRY', fontsize=18, weight='bold' )\n\nplt.show()","60c181ec":"plt.figure(figsize=(12,8))\nedvis = sns.countplot(data.Q4)\nfor p in edvis.patches:\n    edvis.annotate(format(p.get_height(), '.0f'), (p.get_x() + p.get_width() \/ 2., p.get_height()), \n               ha = 'center', va = 'center', xytext = (0, 10), textcoords = 'offset points')\nplt.xlabel('EDUCATION',fontsize=15)\nplt.ylabel('COUNT',fontsize=15)\nplt.title('EDUCATION OF RESPONDENTS',fontsize=18,weight='bold')\nplt.xticks(rotation=75)\nplt.show()","1673f316":"A2=sns.catplot(y='Q4',kind='count',height=8, hue='Q3', data=top3,order=top3['Q4'].value_counts().index)\n\nplt.xlabel('COUNT',fontsize=15)\nplt.ylabel('EDUCATION',fontsize=15)\nplt.title('COUNTRY VS EDUCATION', fontsize=18, weight='bold' )\n\n\nplt.show()\n","80d16e43":"plt.figure(figsize=(12,8))\njobvis = sns.countplot(data.Q5.dropna())\nfor p in jobvis.patches:\n    jobvis.annotate(format(p.get_height(), '.0f'), (p.get_x() + p.get_width() \/ 2., p.get_height()), \n               ha = 'center', va = 'center', xytext = (0, 10), textcoords = 'offset points')\nplt.xlabel('JOB',fontsize=15)\nplt.ylabel('COUNT',fontsize=15)\nplt.title('JOB PROFILE',fontsize=18, weight= 'bold')\nplt.xticks(rotation=90)\nplt.show()\n","aee47158":"A3=sns.catplot(y='Q5',kind='count',height=8, hue='Q3', data=top3,order=top3['Q5'].value_counts().index)\n\nplt.xlabel('COUNT',fontsize=15)\nplt.ylabel('JOB',fontsize=15)\nplt.title('COUNTRY vs JOB', fontsize=18, weight='bold')\n\nplt.show()\n","10f6be5e":"plt.figure(figsize=(10,8))\njobvis = sns.countplot(data.Q6.dropna().sort_values(ascending = True))\n\nfor p in jobvis.patches:\n    jobvis.annotate(format(p.get_height(), '.0f'), (p.get_x() + p.get_width() \/ 2., p.get_height()), \n               ha = 'center', va = 'center', xytext = (0, 10), textcoords = 'offset points')\nplt.xlabel('COMPANY SIZE',fontsize= 15)\nplt.ylabel('COUNT',fontsize = 15)\nplt.xticks(rotation=90)\nplt.title('COMPANY SIZE',fontsize = 18,weight='bold')\nplt.show()","a9f16a7e":"A6=sns.catplot(y='Q6',kind='count',height=8, hue='Q3', data=top3,order=top3['Q6'].value_counts().index)\nplt.xlabel('COUNTRIES',fontsize=15)\nplt.ylabel('No.OF EMPLOYEES',fontsize=15)\nplt.title('COMPANY SIZE vs COUNTRY', fontsize=18, weight='bold' )\n\nplt.show()","143b8553":"plt.figure(figsize=(14,8))\nsalvis = sns.countplot(data.Q10.dropna().sort_values(ascending = True))\n\nfor p in salvis.patches:\n    salvis.annotate(format(p.get_height(), '.0f'), (p.get_x() + p.get_width() \/ 2., p.get_height()), \n               ha = 'center', va = 'center', xytext = (0, 10), textcoords = 'offset points')\n\nplt.xlabel('SALARY',fontsize=15)\nplt.ylabel('COUNT',fontsize=15)\nplt.title('SALARY OF RESPONDENTS',fontsize=18,weight='bold')\nplt.xticks(rotation=90)\nplt.show()\n\n","589385fd":"plt.figure(figsize=(12,6))\nA10=sns.catplot(y='Q10',kind='count',height=8, hue='Q3', data=top3,order=top3['Q10'].value_counts().index)\n\nplt.title('SALARY vs COUNTRY', fontsize=15, weight='bold' )\nplt.subplots_adjust(top=0.85)\n\nplt.show()","824dbcb1":"plt.figure(figsize=(12,8))\nmlvis = sns.countplot(data.Q11.dropna().sort_values(ascending = True))\n\nfor p in mlvis.patches:\n    mlvis.annotate(format(p.get_height(), '.0f'), (p.get_x() + p.get_width() \/ 2., p.get_height()), \n               ha = 'center', va = 'center', xytext = (0, 10), textcoords = 'offset points')\n\nplt.xlabel('MONEY SPENT',fontsize=15)\nplt.ylabel('COUNT',fontsize=15)\nplt.title('MONEY SPENT ON ML',fontsize=18,weight='bold')\nplt.xticks(rotation=90)\nplt.show()\n","1eeaa927":"plt.figure(figsize=(12,6))\nA10=sns.catplot(y='Q11',kind='count',height=8, hue='Q3', data=top3,order=top3['Q11'].value_counts().index)\n\nplt.title('Company Size', fontsize=15, weight='bold' )\nplt.subplots_adjust(top=0.85)\n\nplt.show()\n","907e03a7":"ds_media={}\n\nds_media['Twitter']=data.Q12_Part_1.value_counts().sum()\nds_media['Hacker News']=data.Q12_Part_2.value_counts().sum()\nds_media['Reddit']=data.Q12_Part_3.value_counts().sum()\nds_media['Kaggle']=data.Q12_Part_4.value_counts().sum()\nds_media['Course Forums']=data.Q12_Part_5.value_counts().sum()\nds_media['YouTube']=data.Q12_Part_6.value_counts().sum()\nds_media['Podcasts']=data.Q12_Part_7.value_counts().sum()\nds_media['Blogs']=data.Q12_Part_8.value_counts().sum()\nds_media['Journal Publications']=data.Q12_Part_9.value_counts().sum()\nds_media['Slack']=data.Q12_Part_10.value_counts().sum()\nds_media['Other']=data.Q12_Part_11.value_counts().sum()\n\n\nds_media = pd.DataFrame.from_dict(ds_media,orient='index',columns=['count'])\nds_media.reset_index(inplace=True)\n\n\nplt.figure(figsize = (10,7))\nds_media_vis = sns.barplot(x= 'index',y='count', data = ds_media)\nplt.xlabel('MEDIA SOURCES',fontsize=15)\nplt.ylabel('COUNT',fontsize=15)\nplt.title('FAVOURITE MEDIA SOURCES',fontsize=18,weight='bold')\nplt.xticks(rotation = 90)\nfor p in ds_media_vis.patches:\n    ds_media_vis.annotate(format(p.get_height(), '.0f'), (p.get_x() + p.get_width() \/ 2., p.get_height()), \n               ha = 'center', va = 'center', xytext = (0, 10), textcoords = 'offset points')\nplt.show()","b95a8080":"ds_plat={}\n\nds_plat['Udacity']=data.Q13_Part_1.value_counts().sum()\nds_plat['Coursera']=data.Q13_Part_2.value_counts().sum()\nds_plat['edX']=data.Q13_Part_3.value_counts().sum()\nds_plat['DataCamp']=data.Q13_Part_4.value_counts().sum()\nds_plat['DataQuest']=data.Q13_Part_5.value_counts().sum()\nds_plat['Kaggle Courses']=data.Q13_Part_6.value_counts().sum()\nds_plat['fast.ai']=data.Q13_Part_7.value_counts().sum()\nds_plat['Udemy']=data.Q13_Part_8.value_counts().sum()\nds_plat['LinkedIn Learning']=data.Q13_Part_9.value_counts().sum()\nds_plat['University Courses']=data.Q13_Part_10.value_counts().sum()\nds_plat['None']=data.Q13_Part_11.value_counts().sum()\nds_plat['Other']=data.Q13_Part_12.value_counts().sum()\n\nds_plat = pd.DataFrame.from_dict(ds_plat,orient='index',columns=['count'])\nds_plat.reset_index(inplace=True)\n\nplt.figure(figsize = (10,7))\nds_plat = sns.barplot(x= 'index',y='count', data = ds_plat,palette='rocket')\nplt.xlabel('PLATFORM',fontsize=15)\nplt.ylabel('COUNT',fontsize=15)\nplt.title('DS PLATFORM USED',fontsize=18,weight='bold')\nplt.xticks(rotation = 90)\nfor p in ds_plat.patches:\n    ds_plat.annotate(format(p.get_height(), '.0f'), (p.get_x() + p.get_width() \/ 2., p.get_height()), \n               ha = 'center', va = 'center', xytext = (0, 10), textcoords = 'offset points')\nplt.show()\n","22c43430":"plt.figure(figsize=(12,8))\nantoolvis = sns.countplot(data.Q14.dropna().sort_values(ascending = True))\n\nfor p in antoolvis.patches:\n    antoolvis.annotate(format(p.get_height(), '.0f'), (p.get_x() + p.get_width() \/ 2., p.get_height()), \n               ha = 'center', va = 'center', xytext = (0, 10), textcoords = 'offset points')\nplt.xlabel('ANALYSIS TOOL',fontsize=15)\nplt.ylabel('COUNT',fontsize=15)\nplt.title('DS ANALYSIS TOOL USED',fontsize=18,weight='bold')\nplt.xticks(rotation=75)\n\nplt.show()\n","51059439":"plt.figure(figsize=(12,8))\ncodexvis = sns.countplot(data.Q15.dropna().sort_values(ascending = True),palette='terrain')\n\nfor p in codexvis.patches:\n    codexvis.annotate(format(p.get_height(), '.0f'), (p.get_x() + p.get_width() \/ 2., p.get_height()), \n               ha = 'center', va = 'center', xytext = (0, 10), textcoords = 'offset points')\n\nplt.xlabel('EXPERIENCE',fontsize=15)\nplt.ylabel('COUNT',fontsize=15)\nplt.title('CODING EXPERIENCE OF RESPONDENTS',fontsize=18,weight='bold')\nplt.xticks(rotation=90)\nplt.show()\n","0051f3f1":"ds_IDE={}\n\nds_IDE['Jupyter']=data.Q16_Part_1.value_counts().sum()\nds_IDE['RStudio']=data.Q16_Part_2.value_counts().sum()\nds_IDE['PyCharm']=data.Q16_Part_3.value_counts().sum()\nds_IDE['Atom']=data.Q16_Part_4.value_counts().sum()\nds_IDE['MATLAB']=data.Q16_Part_5.value_counts().sum()\nds_IDE['Visual Studio']=data.Q16_Part_6.value_counts().sum()\nds_IDE['Spyder']=data.Q16_Part_7.value_counts().sum()\nds_IDE['Vim\/Emacs']=data.Q16_Part_8.value_counts().sum()\nds_IDE['Notepad++']=data.Q16_Part_9.value_counts().sum()\nds_IDE['Sublime Text']=data.Q16_Part_10.value_counts().sum()\nds_IDE['None']=data.Q16_Part_11.value_counts().sum()\nds_IDE['Other']=data.Q16_Part_12.value_counts().sum()\n\nds_IDE = pd.DataFrame.from_dict(ds_IDE,orient='index',columns=['count'])\nds_IDE.reset_index(inplace=True)\n\n\nplt.figure(figsize = (10,8))\nds_IDE = sns.barplot(x= 'index',y='count', data = ds_IDE,palette='ocean')\nplt.xlabel('IDEs',fontsize=15)\nplt.ylabel('COUNT',fontsize=15)\nplt.title('DS IDEs USED',fontsize=18,weight='bold')\nplt.xticks(rotation = 90)\nfor p in ds_IDE.patches:\n    ds_IDE.annotate(format(p.get_height(), '.0f'), (p.get_x() + p.get_width() \/ 2., p.get_height()), \n               ha = 'center', va = 'center', xytext = (0, 10), textcoords = 'offset points')\nplt.show()\n","e4abc137":"ds_note={}\nds_note['Kaggle Notebooks']=data.Q17_Part_1.value_counts().sum()\nds_note['Google Colab']=data.Q17_Part_2.value_counts().sum()\nds_note['MS Azure Notebook']=data.Q17_Part_3.value_counts().sum()\nds_note['Google Cloud Notebooks']=data.Q17_Part_4.value_counts().sum()\nds_note['Paperspace\/Gradient']=data.Q17_Part_5.value_counts().sum()\nds_note['FloydHub']=data.Q17_Part_6.value_counts().sum()\nds_note['Binder\/JupyterHub']=data.Q17_Part_7.value_counts().sum()\nds_note['IBM Watson Studio']=data.Q17_Part_8.value_counts().sum()\nds_note['Code Ocean']=data.Q17_Part_9.value_counts().sum()\nds_note['AWS Notebook']=data.Q17_Part_10.value_counts().sum()\nds_note['None']=data.Q17_Part_11.value_counts().sum()\nds_note['Other']=data.Q17_Part_12.value_counts().sum()\n\n\n\nds_note = pd.DataFrame.from_dict(ds_note,orient='index',columns=['count'])\nds_note.reset_index(inplace=True)\n\n\nplt.figure(figsize = (10,7))\nds_note = sns.barplot(x= 'index',y='count', data = ds_note,palette='inferno')\nplt.xlabel('NOTEBOOKS',fontsize=15)\nplt.ylabel('COUNT',fontsize=15)\nplt.title('DS NOTEBOOKS USED BY RESPONDENTS',fontsize=18,weight='bold')\nplt.xticks(rotation = 90)\nfor p in ds_note.patches:\n    ds_note.annotate(format(p.get_height(), '.0f'), (p.get_x() + p.get_width() \/ 2., p.get_height()), \n               ha = 'center', va = 'center', xytext = (0, 10), textcoords = 'offset points')\nplt.show()\n","17380e66":"ds_lan={}\nds_lan['Python']=data.Q18_Part_1.value_counts().sum()\nds_lan['R']=data.Q18_Part_2.value_counts().sum()\nds_lan['SQL']=data.Q18_Part_3.value_counts().sum()\nds_lan['C']=data.Q18_Part_4.value_counts().sum()\nds_lan['C++']=data.Q18_Part_5.value_counts().sum()\nds_lan['Java']=data.Q18_Part_6.value_counts().sum()\nds_lan['JavaScript']=data.Q18_Part_7.value_counts().sum()\nds_lan['TypeScript']=data.Q18_Part_8.value_counts().sum()\nds_lan['Bash']=data.Q18_Part_9.value_counts().sum()\nds_lan['MATLAB']=data.Q18_Part_10.value_counts().sum()\nds_lan['None']=data.Q18_Part_11.value_counts().sum()\nds_lan['Other']=data.Q18_Part_12.value_counts().sum()\n\n\n\nds_lan = pd.DataFrame.from_dict(ds_lan,orient='index',columns=['count'])\nds_lan.reset_index(inplace=True)\n\n\nplt.figure(figsize = (10,8))\nds_lan = sns.barplot(x= 'index',y='count', data = ds_lan,palette='hot')\nplt.xlabel('LANGUAGE',fontsize=15)\nplt.ylabel('COUNT',fontsize=15)\nplt.title('PROGRAMMING LANGUAGE USED BY RESPONDENTS',fontsize=18,weight='bold')\nplt.xticks(rotation = 90)\nfor p in ds_lan.patches:\n    ds_lan.annotate(format(p.get_height(), '.0f'), (p.get_x() + p.get_width() \/ 2., p.get_height()), \n               ha = 'center', va = 'center', xytext = (0, 10), textcoords = 'offset points')\nplt.show()\n","768f9186":"plt.figure(figsize=(12,8))\nlanvis = sns.countplot(data.Q19.dropna().sort_values(ascending = True))\n\nfor p in lanvis.patches:\n    lanvis.annotate(format(p.get_height(), '.0f'), (p.get_x() + p.get_width() \/ 2., p.get_height()), \n               ha = 'center', va = 'center', xytext = (0, 10), textcoords = 'offset points')\n\nplt.xlabel('LANGUAGE',fontsize=15)\nplt.ylabel('COUNT',fontsize=15)\nplt.title('LANGUAGE PREFERENCE OF RESPONDENTS',fontsize=18,weight='bold')\nplt.xticks(rotation=90)\nplt.show()\n","c8773f9f":"ds_tool={}\nds_tool['Ggplot\/ggplot2']=data.Q20_Part_1.value_counts().sum()\nds_tool['Matplotlib']=data.Q20_Part_2.value_counts().sum()\nds_tool['Altair']=data.Q20_Part_3.value_counts().sum()\nds_tool['Shiny']=data.Q20_Part_4.value_counts().sum()\nds_tool['D3.js']=data.Q20_Part_5.value_counts().sum()\nds_tool['Plotly\/Plotly Express']=data.Q20_Part_6.value_counts().sum()\nds_tool['Bokeh']=data.Q20_Part_7.value_counts().sum()\nds_tool['Seaborn']=data.Q20_Part_8.value_counts().sum()\nds_tool['Geoplotlib']=data.Q20_Part_9.value_counts().sum()\nds_tool['Leaflet\/Folium']=data.Q20_Part_10.value_counts().sum()\nds_tool['None']=data.Q20_Part_11.value_counts().sum()\nds_tool['Other']=data.Q20_Part_12.value_counts().sum()\n\n\n\nds_tool = pd.DataFrame.from_dict(ds_tool,orient='index',columns=['count'])\nds_tool.reset_index(inplace=True)\n\n\nplt.figure(figsize = (10,7))\nds_tool = sns.barplot(x= 'index',y='count', data = ds_tool,palette='icefire')\nplt.xlabel('VISUALIZATION TOOL',fontsize=15)\nplt.ylabel('COUNT',fontsize=15)\nplt.title('VISUALIZATION TOOLS USED BY RESPONDENTS',fontsize=18,weight='bold')\nplt.xticks(rotation = 80)\nfor p in ds_tool.patches:\n    ds_tool.annotate(format(p.get_height(), '.0f'), (p.get_x() + p.get_width() \/ 2., p.get_height()), \n               ha = 'center', va = 'center', xytext = (0, 10), textcoords = 'offset points')\nplt.show()\n","e24b695d":"ds_hard={}\n\nds_hard['CPUs']=data.Q21_Part_1.value_counts().sum()\nds_hard['GPUs']=data.Q21_Part_2.value_counts().sum()\nds_hard['TPUs']=data.Q21_Part_3.value_counts().sum()\nds_hard['None\/ I do not know']=data.Q21_Part_4.value_counts().sum()\nds_hard['Other']=data.Q21_Part_5.value_counts().sum()\n\nds_hard = pd.DataFrame.from_dict(ds_hard,orient='index',columns=['count'])\nds_hard.reset_index(inplace=True)\n\n\nplt.figure(figsize = (10,8))\nds_hard = sns.barplot(x= 'index',y='count', data = ds_hard,palette='tab20')\nplt.xlabel('HARDWARE',fontsize=15)\nplt.ylabel('COUNT',fontsize=15)\nplt.title('HARDWARE USED BY RESPONDENTS',fontsize=18,weight='bold')\nplt.xticks(rotation = 90)\nfor p in ds_hard.patches:\n    ds_hard.annotate(format(p.get_height(), '.0f'), (p.get_x() + p.get_width() \/ 2., p.get_height()), \n               ha = 'center', va = 'center', xytext = (0, 10), textcoords = 'offset points')\nplt.show()\n","02049729":"plt.figure(figsize=(12,8))\ntpuvis = sns.countplot(data.Q22.dropna().sort_values(ascending = True))\n\nfor p in tpuvis.patches:\n    tpuvis.annotate(format(p.get_height(), '.0f'), (p.get_x() + p.get_width() \/ 2., p.get_height()), \n               ha = 'center', va = 'center', xytext = (0, 10), textcoords = 'offset points')\n\nplt.xlabel('USE OF TPU',fontsize=15)\nplt.ylabel('COUNT',fontsize=15)\nplt.title('USAGE OF TPU BY RESPONDENTS',fontsize=18,weight='bold')\nplt.xticks(rotation=90)\nplt.show()\n","f2e6e9e1":"plt.figure(figsize=(12,8))\nmlexpvis = sns.countplot(data.Q23.dropna().sort_values(ascending = True),palette='PuBuGn_r')\n\nfor p in mlexpvis.patches:\n    mlexpvis.annotate(format(p.get_height(), '.0f'), (p.get_x() + p.get_width() \/ 2., p.get_height()), \n               ha = 'center', va = 'center', xytext = (0, 10), textcoords = 'offset points')\n\nplt.xlabel('EXPERIENCE',fontsize=15)\nplt.ylabel('COUNT',fontsize=15)\nplt.title('ML EXPERIENCE OF RESPONDENTS',fontsize=18,weight='bold')\nplt.xticks(rotation=90)\nplt.show()\n","01d49311":"ml_algo = {}\nml_algo['Linear or logistic Regression']=data.Q24_Part_1.value_counts().sum()\nml_algo['Decision Trees or Random Forests']=data.Q24_Part_2.value_counts().sum()\nml_algo['Gradient Boosting Machines']=data.Q24_Part_3.value_counts().sum()\nml_algo['Bayesian Approaches']=data.Q24_Part_4.value_counts().sum()\nml_algo['Evolutionary Approaches']=data.Q24_Part_5.value_counts().sum()\nml_algo['Dense Neural Networks']=data.Q24_Part_6.value_counts().sum()\nml_algo['Convolutional Neural Networks']=data.Q24_Part_7.value_counts().sum()\nml_algo['Generative Adversarial Networks']=data.Q24_Part_8.value_counts().sum()\nml_algo['Recurrent Neural Networks']=data.Q24_Part_9.value_counts().sum()\nml_algo[' Transformer Networks']=data.Q24_Part_10.value_counts().sum()\nml_algo['None']=data.Q24_Part_11.value_counts().sum()\nml_algo['Other']=data.Q24_Part_12.value_counts().sum()\n\nml_algo = pd.DataFrame.from_dict(ml_algo,orient='index',columns=['count'])\nml_algo.reset_index(inplace=True)\n\nplt.figure(figsize = (12,9))\nml_algo_vis = sns.barplot(x= 'index',y='count', data = ml_algo,palette='mako_r')\nplt.xlabel('ML ALGORITHMS',fontsize=15)\nplt.ylabel('COUNT',fontsize=15)\nplt.title('ML ALGORITHMS USING REGULARLY',fontsize=18,weight='bold')\n\nplt.xticks(rotation = 75)\nfor p in ml_algo_vis.patches:\n    ml_algo_vis.annotate(format(p.get_height(), '.0f'), (p.get_x() + p.get_width() \/ 2., p.get_height()), \n               ha = 'center', va = 'center', xytext = (0, 10), textcoords = 'offset points')\nplt.show()\n","9af546ec":"ml_tool = {}\nml_tool['Automated data augmentation']=data.Q25_Part_1.value_counts().sum()\nml_tool['Automated feature engineering\/selection']=data.Q25_Part_2.value_counts().sum()\nml_tool['Automated model selection']=data.Q25_Part_3.value_counts().sum()\nml_tool['Automated model architecture searches']=data.Q25_Part_4.value_counts().sum()\nml_tool['Automated hyperparameter tuning']=data.Q25_Part_5.value_counts().sum()\nml_tool['Automation of full ML pipelines']=data.Q25_Part_6.value_counts().sum()\nml_tool['None']=data.Q25_Part_7.value_counts().sum()\nml_tool['Other']=data.Q25_Part_8.value_counts().sum()\n\n\nml_tool = pd.DataFrame.from_dict(ml_tool,orient='index',columns=['count'])\nml_tool.reset_index(inplace=True)\n\nplt.figure(figsize = (10,7))\nml_tool_vis = sns.barplot(x= 'index',y='count', data = ml_tool,palette='Spectral_r')\nplt.xlabel('ML TOOLS',fontsize=15)\nplt.ylabel('COUNT',fontsize=15)\nplt.title('ML TOOLS USING REGULARLY',fontsize=18,weight='bold')\nplt.xticks(rotation = 75)\nfor p in ml_tool_vis.patches:\n    ml_tool_vis.annotate(format(p.get_height(), '.0f'), (p.get_x() + p.get_width() \/ 2., p.get_height()), \n               ha = 'center', va = 'center', xytext = (0, 10), textcoords = 'offset points')\nplt.show()","c01cef95":"cv_method = {}\ncv_method['General purpose image\/video tools']=data.Q26_Part_1.value_counts().sum()\ncv_method['Image segmentation methods']=data.Q26_Part_2.value_counts().sum()\ncv_method['Object detection methods']=data.Q26_Part_3.value_counts().sum()\ncv_method['Image classification and other general purpose networks']=data.Q26_Part_4.value_counts().sum()\ncv_method['Generative Networks']=data.Q26_Part_5.value_counts().sum()\ncv_method['None']=data.Q26_Part_6.value_counts().sum()\ncv_method['Other']=data.Q26_Part_7.value_counts().sum()\ncv_method = pd.DataFrame.from_dict(cv_method,orient='index',columns=['count'])\ncv_method.reset_index(inplace=True)\n\nplt.figure(figsize = (10,7))\ncv_method_vis = sns.barplot(x= 'index',y='count', data = cv_method,palette='bone_r')\nplt.xlabel('CV METHODS',fontsize=15)\nplt.ylabel('COUNT',fontsize=15)\nplt.title('CV METHODS USING REGULARLY',fontsize=18,weight='bold')\nplt.xticks(rotation = 75)\nfor p in cv_method_vis.patches:\n\n    cv_method_vis.annotate(format(p.get_height(), '.0f'), (p.get_x() + p.get_width() \/ 2., p.get_height()), \n\n               ha = 'center', va = 'center', xytext = (0, 10), textcoords = 'offset points')\nplt.show()\n","6978934d":"nlp_method = {}\n\nnlp_method['Word embeddings\/vectors']=data.Q27_Part_1.value_counts().sum()\n\nnlp_method['Encoder-decorder models']=data.Q27_Part_2.value_counts().sum()\n\nnlp_method['Contextualized embeddings']=data.Q27_Part_3.value_counts().sum()\n\nnlp_method['Transformer language models']=data.Q27_Part_4.value_counts().sum()\n\nnlp_method['None']=data.Q27_Part_5.value_counts().sum()\n\nnlp_method['Other']=data.Q27_Part_6.value_counts().sum()\n\n\n\nnlp_method = pd.DataFrame.from_dict(nlp_method,orient='index',columns=['count'])\n\nnlp_method.reset_index(inplace=True)\n\n\n\n\n\nplt.figure(figsize = (10,7))\n\nnlp_method_vis = sns.barplot(x= 'index',y='count', data = nlp_method,palette='PRGn_r')\nplt.xlabel('NLP METHODS',fontsize=15)\nplt.ylabel('COUNT',fontsize=15)\nplt.title('NLP METHODS USING REGULARLY',fontsize=18,weight='bold')\nplt.xticks(rotation = 90)\nfor p in nlp_method_vis.patches:\n    nlp_method_vis.annotate(format(p.get_height(), '.0f'), (p.get_x() + p.get_width() \/ 2., p.get_height()), \n\n               ha = 'center', va = 'center', xytext = (0, 10), textcoords = 'offset points')\nplt.show()\n\n","79dc74b0":"ml_frame = {}\n\nml_frame['Scikit-learn']=data.Q28_Part_1.value_counts().sum()\n\nml_frame['TensorFlow']=data.Q28_Part_2.value_counts().sum()\n\nml_frame['Keras']=data.Q28_Part_3.value_counts().sum()\n\nml_frame['RandomForest']=data.Q28_Part_4.value_counts().sum()\n\nml_frame['Xgboost']=data.Q28_Part_5.value_counts().sum()\n\nml_frame['Pytorch']=data.Q28_Part_6.value_counts().sum()\n\nml_frame['Caret']=data.Q28_Part_7.value_counts().sum()\n\nml_frame['LightGBM']=data.Q28_Part_8.value_counts().sum()\n\nml_frame['Spark MLib']=data.Q28_Part_9.value_counts().sum()\n\nml_frame['Fast.ai']=data.Q28_Part_10.value_counts().sum()\n\nml_frame['None']=data.Q28_Part_11.value_counts().sum()\n\nml_frame['Other']=data.Q28_Part_12.value_counts().sum()\n\n\n\nml_frame = pd.DataFrame.from_dict(ml_frame,orient='index',columns=['count'])\n\nml_frame.reset_index(inplace=True)\n\n\n\n\n\nplt.figure(figsize = (10,7))\n\nml_frame_vis = sns.barplot(x= 'index',y='count', data = ml_frame,palette='gist_earth_r')\nplt.xlabel('ML FRAMEWORKS',fontsize=15)\nplt.ylabel('COUNT',fontsize=15)\nplt.title('ML FRAMEWORKS USING REGULARLY',fontsize=18,weight='bold')\n\nplt.xticks(rotation = 90)\n\nfor p in ml_frame_vis.patches:\n\n    ml_frame_vis.annotate(format(p.get_height(), '.0f'), (p.get_x() + p.get_width() \/ 2., p.get_height()), \n\n               ha = 'center', va = 'center', xytext = (0, 10), textcoords = 'offset points')\n\nplt.show()\n\n\n","997a818a":"cloud_plat = {}\n\ncloud_plat['Google Cloud Platform (GCP)']=data.Q29_Part_1.value_counts().sum()\n\ncloud_plat['Amazon Web Services (AWS)']=data.Q29_Part_2.value_counts().sum()\n\ncloud_plat['Microsoft Azure']=data.Q29_Part_3.value_counts().sum()\n\ncloud_plat['IBM Cloud']=data.Q29_Part_4.value_counts().sum()\n\ncloud_plat['Alibaba Cloud']=data.Q29_Part_5.value_counts().sum()\n\ncloud_plat['Salesforce Cloud']=data.Q29_Part_6.value_counts().sum()\n\ncloud_plat['Oracle Cloud']=data.Q29_Part_7.value_counts().sum()\n\ncloud_plat['SAP Cloud']=data.Q29_Part_8.value_counts().sum()\n\ncloud_plat['VMware Cloud']=data.Q29_Part_9.value_counts().sum()\n\ncloud_plat['Red Hat Cloud']=data.Q29_Part_10.value_counts().sum()\n\ncloud_plat['None']=data.Q29_Part_11.value_counts().sum()\n\ncloud_plat['Other']=data.Q29_Part_12.value_counts().sum()\n\n\n\n\n\ncloud_plat = pd.DataFrame.from_dict(cloud_plat,orient='index',columns=['count'])\n\ncloud_plat.reset_index(inplace=True)\n\n\n\n\n\nplt.figure(figsize = (10,7))\n\ncloud_plat_vis = sns.barplot(x= 'index',y='count', data = cloud_plat,palette='BuPu_r')\nplt.xlabel('CLOUD PLATFORMS',fontsize=15)\nplt.ylabel('COUNT',fontsize=15)\nplt.title('CLOUD COMPUTING PLATFORMS',fontsize=18,weight='bold')\n\nplt.xticks(rotation = 90)\n\nfor p in cloud_plat_vis.patches:\n\n    cloud_plat_vis.annotate(format(p.get_height(), '.0f'), (p.get_x() + p.get_width() \/ 2., p.get_height()), \n\n               ha = 'center', va = 'center', xytext = (0, 10), textcoords = 'offset points')\n\nplt.show()\n\n","0fd5d7cc":"cloud_prod = {}\n\ncloud_prod['AWS Elastic Compute Cloud (EC2)']=data.Q30_Part_1.value_counts().sum()\ncloud_prod['Google Compute Engine (GCE)']=data.Q30_Part_2.value_counts().sum()\ncloud_prod['AWS Lambda']=data.Q30_Part_3.value_counts().sum()\ncloud_prod['Azure Virtual Machines']=data.Q30_Part_4.value_counts().sum()\ncloud_prod['Google App Engine']=data.Q30_Part_5.value_counts().sum()\ncloud_prod['Google Cloud Functions']=data.Q30_Part_6.value_counts().sum()\ncloud_prod['AWS Elastic Beanstalk']=data.Q30_Part_7.value_counts().sum()\ncloud_prod['Google Kubernetes Engine']=data.Q30_Part_8.value_counts().sum()\ncloud_prod['AWS Batch']=data.Q30_Part_9.value_counts().sum()\ncloud_prod['Azure Container Service']=data.Q30_Part_10.value_counts().sum()\ncloud_prod['None']=data.Q30_Part_11.value_counts().sum()\ncloud_prod['Other']=data.Q30_Part_12.value_counts().sum()\n\n\n\ncloud_prod = pd.DataFrame.from_dict(cloud_prod,orient='index',columns=['count'])\n\ncloud_prod.reset_index(inplace=True)\n\n\n\n\n\nplt.figure(figsize = (10,7))\n\ncloud_prod_vis = sns.barplot(x= 'index',y='count', data = cloud_prod,palette='Oranges_r')\nplt.xlabel('CLOUD COMPUTING PRODUCTS',fontsize=15)\nplt.ylabel('COUNT',fontsize=15)\nplt.title('CLOUD COMPUTING PRODUCTS USING REGULARLY ',fontsize=18,weight='bold')\n\nplt.xticks(rotation = 90)\n\nfor p in cloud_prod_vis.patches:\n\n    cloud_prod_vis.annotate(format(p.get_height(), '.0f'), (p.get_x() + p.get_width() \/ 2., p.get_height()), \n\n               ha = 'center', va = 'center', xytext = (0, 10), textcoords = 'offset points')\n\nplt.show()\n\n","0e78ed93":"big_prod = {}\nbig_prod['Google BigQuery']=data.Q31_Part_1.value_counts().sum()\nbig_prod['AWS Redshift']=data.Q31_Part_2.value_counts().sum()\nbig_prod['Databricks']=data.Q31_Part_3.value_counts().sum()\nbig_prod['AWS Elastic MapReduce']=data.Q31_Part_4.value_counts().sum()\nbig_prod['Teradata']=data.Q31_Part_5.value_counts().sum()\nbig_prod['Microsoft Analysis Services']=data.Q31_Part_6.value_counts().sum()\n\nbig_prod['Google Cloud Dataflow']=data.Q31_Part_7.value_counts().sum()\n\nbig_prod['AWS Athena']=data.Q31_Part_8.value_counts().sum()\n\nbig_prod['AWS Kinesis']=data.Q31_Part_9.value_counts().sum()\n\nbig_prod['Google Cloud Pub\/Sub']=data.Q31_Part_10.value_counts().sum()\n\nbig_prod['None']=data.Q31_Part_11.value_counts().sum()\n\nbig_prod['Other']=data.Q31_Part_12.value_counts().sum()\n\n\n\nbig_prod = pd.DataFrame.from_dict(big_prod,orient='index',columns=['count'])\n\nbig_prod.reset_index(inplace=True)\n\n\n\n\n\nplt.figure(figsize = (10,7))\n\nbig_prod_vis = sns.barplot(x= 'index',y='count', data = big_prod,palette='rainbow_r')\nplt.xlabel('BIG DATA PRODUCTS',fontsize=15)\nplt.ylabel('COUNT',fontsize=15)\nplt.title('BIG DATA PRODUCTS USING REGULARLY ',fontsize=18,weight='bold')\n\nplt.xticks(rotation = 90)\n\nfor p in big_prod_vis.patches:\n\n    big_prod_vis.annotate(format(p.get_height(), '.0f'), (p.get_x() + p.get_width() \/ 2., p.get_height()), \n\n               ha = 'center', va = 'center', xytext = (0, 10), textcoords = 'offset points')\n\nplt.show()\n","d0d9ebab":"ml_prod = {}\nml_prod['SAS']=data.Q32_Part_1.value_counts().sum()\nml_prod['Cloudera']=data.Q32_Part_2.value_counts().sum()\nml_prod['Azure Machine Learning Studio']=data.Q32_Part_3.value_counts().sum()\nml_prod['Google Cloud Machine Learning Engine']=data.Q32_Part_4.value_counts().sum()\nml_prod['Google Cloud Visio']=data.Q32_Part_5.value_counts().sum()\nml_prod['Google Cloud Speech-to-Text']=data.Q32_Part_6.value_counts().sum()\nml_prod['Google Cloud Natural Language']=data.Q32_Part_7.value_counts().sum()\nml_prod['RapidMiner']=data.Q32_Part_8.value_counts().sum()\nml_prod['Google Cloud Translation']=data.Q32_Part_9.value_counts().sum()\nml_prod['Amazon SageMaker']=data.Q32_Part_10.value_counts().sum()\nml_prod['None']=data.Q32_Part_11.value_counts().sum()\nml_prod['Other']=data.Q32_Part_12.value_counts().sum()\n\nml_prod = pd.DataFrame.from_dict(ml_prod,orient='index',columns=['count'])\nml_prod.reset_index(inplace=True)\n\nplt.figure(figsize = (10,7))\nml_prod_vis = sns.barplot(x= 'index',y='count', data = ml_prod,palette='Set1_r')\nplt.xlabel('ML PRODUCTS',fontsize=15)\nplt.ylabel('COUNT',fontsize=15)\nplt.title('ML  PRODUCTS USING REGULARLY ',fontsize=18,weight='bold')\nplt.xticks(rotation = 90)\nfor p in ml_prod_vis.patches:\n    ml_prod_vis.annotate(format(p.get_height(), '.0f'), (p.get_x() + p.get_width() \/ 2., p.get_height()), \n               ha = 'center', va = 'center', xytext = (0, 10), textcoords = 'offset points')\nplt.show()","d37a7d21":"ml_par = {}\nml_par['Google AutoML']=data.Q33_Part_1.value_counts().sum()\nml_par['H20 Driverless AI']=data.Q33_Part_2.value_counts().sum()\nml_par['Databricks AutoML']=data.Q33_Part_3.value_counts().sum()\nml_par['DataRobot AutoML']=data.Q33_Part_4.value_counts().sum()\nml_par['Tpot']=data.Q33_Part_5.value_counts().sum()\nml_par['Auto-Keras']=data.Q33_Part_6.value_counts().sum()\nml_par['Auto-Sklearn']=data.Q33_Part_7.value_counts().sum()\nml_par['Auto_ml']=data.Q33_Part_8.value_counts().sum()\nml_par['Xcessiv']=data.Q33_Part_9.value_counts().sum()\nml_par['MLbox']=data.Q33_Part_10.value_counts().sum()\nml_par['None']=data.Q33_Part_11.value_counts().sum()\nml_par['Other']=data.Q33_Part_12.value_counts().sum()\n\nml_par = pd.DataFrame.from_dict(ml_par,orient='index',columns=['count'])\nml_par.reset_index(inplace=True)\n\nplt.figure(figsize = (10,7))\nml_par_vis = sns.barplot(x= 'index',y='count', data = ml_par,palette='CMRmap_r')\nplt.xlabel('AUTOMATED ML TOOLS',fontsize=15)\nplt.ylabel('COUNT',fontsize=15)\nplt.title('AUTOMATED ML TOOLS USING REGULARLY ',fontsize=18,weight='bold')\nplt.xticks(rotation = 90)\nfor p in ml_par_vis.patches:\n    ml_par_vis.annotate(format(p.get_height(), '.0f'), (p.get_x() + p.get_width() \/ 2., p.get_height()), \n               ha = 'center', va = 'center', xytext = (0, 10), textcoords = 'offset points')\nplt.show()","fd5b1793":"db_prod = {}\ndb_prod['MySQL']=data.Q34_Part_1.value_counts().sum()\ndb_prod['PostgresSQL']=data.Q34_Part_2.value_counts().sum()\ndb_prod['SQLite']=data.Q34_Part_3.value_counts().sum()\ndb_prod['Microsoft SQL Server']=data.Q34_Part_4.value_counts().sum()\ndb_prod['Oracle Database']=data.Q34_Part_5.value_counts().sum()\ndb_prod['Microsoft Access']=data.Q34_Part_6.value_counts().sum()\ndb_prod['AWS Relational Database Service']=data.Q34_Part_7.value_counts().sum()\ndb_prod['AWS DynamoDB']=data.Q34_Part_8.value_counts().sum()\ndb_prod['Azure SQL Database']=data.Q34_Part_9.value_counts().sum()\ndb_prod['Google Cloud SQL']=data.Q34_Part_10.value_counts().sum()\ndb_prod['None']=data.Q34_Part_11.value_counts().sum()\ndb_prod['Other']=data.Q34_Part_12.value_counts().sum()\n\ndb_prod = pd.DataFrame.from_dict(db_prod,orient='index',columns=['count'])\ndb_prod.reset_index(inplace=True)\n\nplt.figure(figsize = (10,7))\ndb_prod_vis = sns.barplot(x= 'index',y='count', data = db_prod)\nplt.xlabel('DATABASE PRODUCTS',fontsize=15)\nplt.ylabel('COUNT',fontsize=15)\nplt.title('DATABASE PRODUCTS USING REGULARLY ',fontsize=18,weight='bold')\nplt.xticks(rotation = 90)\nfor p in db_prod_vis.patches:\n    db_prod_vis.annotate(format(p.get_height(), '.0f'), (p.get_x() + p.get_width() \/ 2., p.get_height()), \n               ha = 'center', va = 'center', xytext = (0, 10), textcoords = 'offset points')\nplt.show()\n","adbe5092":"**While analysing the responses, it is revealed that most of the companies are not interested in spending money on ML.**\nHowever, there are a considerable number of companies who spent a medium range of amount on ML and a few who spent huge amounts on the same.","851e6d56":"**Majority of respondents chose Word Embeddings\/vectors as the most commonly used NLP method.\nEncoder-Decoder models comes second in the ranking.**","43f627ad":"**Most of the respondents from India are Students whereas those from USA are Data Scientists.**\nLarge number of Software Engineers from India have knowledge of this domain but are still unable to acquire a Data Science job.","26d32822":"**It is clear that most of the youngsters are Indians and the middle-aged are from USA.**","055453be":"**Lets analyse the Respondents from these 3 countries**","4d09eb47":"So far, we have analysed the general details of the respondents.\nNow, lets analyze their knowledge and skills in data science and how they acquired it!","2e13b811":"**Majority Indian respondents are Graduates whereas those from USA are Post-Graduates and Phd's.**","10baaa4f":"****MACHINE LEARNING ALGORITHM USING REGULARLY****","c1627b7e":"**COUNTRY VS EDUCATION**","9e46f9b5":"**The yearly compensation of Indians are lesser than USA.**\nDEAR INDIANS, DO YOU FEEL BORN IN THE WRONG COUNTRY ? Lol !!!","fa7dbf60":"**PREFERRED LANGUAGE**","10f62a8d":"****COMPUTER VISION METHODS USING REGULARLY****","0f5aab16":"**Most of the respondents are Data Scientists and Students.**\nIt is quite interesting to note that there are more Software Engineers than Data Analysts who knows Machine Learning. ","f075581d":"11495 people out of 19000 respondents never used TPU.","cee0ddeb":"**Most of the Respondents are from INDIA , USA and BRAZIL, around 55% of the respondents are from these 3 countries.**\nIndia has been the center of software and IT industry. With the gradual degradation of traditional IT positions through automation, the Indian IT industry is experiencing a major transformation. This is the new age of data and it is a need of the hour for professionals to update themselves in order to sustain their relevancy.","82798670":"**Matplotlib and seaborn are most favoured visualisation tool and ggplot2 are prefered by R respondents.**","8704982e":"**CODING EXPERIENCE**","66f7d411":"****MONEY SPENT ON MACHINE LEARNING BY DIFFERENT COUNTRIES****","5cb0467f":"**Most of the respondents are in the age range of 25-29 and 22-24.**\nNo wonder youngsters are moving forward to expertise in the so called \"SEXIEST JOB OF THE 21st CENTURY!\"","372fd0f0":"**Majority of the respondents are having less than 1 year or 1-2 years of coding experience.**\nRespondents with 10+ years of coding experience are comparitively less. However, there are considerable number of respondents with 3-5 years of experience.It is rather interesting to note that there are respondents who have never written a code.","c63d4fe7":"**HARDWARE USING FOR DATA SCIENCE**","c76d186e":"**Well no wonder in that... Python is the prefered programming language among data scientists.**","973c530d":"****COMPANY SIZE vs COUNTRY****","5b2a337f":"**Likewise, most of them dont use any of these products too.**","a327cc9b":"**MySQL is the major Database product used by the repondents followed by PostgresSQL and Microsoft SQL Server.**","b34a2fff":"**INTEGRATED DEVELOPMENT ENVIRONMENTS USING FOR DATA SCIENCE**","93dd0999":"**It seems like majority of the respondents doesn't require the help of Bigdata products at their work or may be they are not aware of these products.**","39ec4cb2":"By the end of this analysis, we found that the basic skills required for data scientists are python programming,R programming and SQL. For beginners, the most reliable platform to learn data science is Coursera and Kaggle. Also, it is better to learn the techniques of AWS Cloud computing platform.\nApart from this, NLP techniques(GLoVe,fastText,word2vec) and Image classification methods(VGG,Inception,ResNet) are essential for higher computation.\n\nAlthough contributions of Indians are remarkable in this domain, the salary and job opportunities are less compared to other nations. STRANGE...!\n\nThanks for going through this kernel,We hope to bring more enjoyable and resourceful information as we are gaining more experience in our journey of becoming Data Scientists.\n\n\n","5a76f5bb":"**Most of the respondents uses CPUs and half of that people uses GPU.**","029b301b":"**Most of the people prefer IDE's Over notebook, but some people prefer Kaggle notebook and google colab**","4212b9d0":"**Scikit-learn is the most commonly used ML Framework as per the respondents. TensorFlow and Keras are the most used framework in deep learning.  **","c6372e11":"**Jupyter is the most preferred IDE followed by Visual Studio, RStudio and PyCharm.**","e6d5a6bb":"**PLATFORM USED FOR LEARNING DATA SCIENCE**","6b787307":"As expected most of the respodents are Male.","54b7087d":"**It is clear that American respondents are spending huge amounts on ML than Indians.**\nMost of the Indian people don't spend any money to learn machine learning and cloud computing.","d201e3f6":"**CLOUD PLATFORM USING REGULARLY**","08aa909f":"****AUTOMATED MACHINE LEARNING TOOLS****","02c8a97f":"****BIG DATA PRODUCTS USING REGULARLY****","fa06e909":"**Linear and Logistic regression are the most commonly used machine learning Algorithm and also noted that considerable number of respondents are using CNN. **","814e13e3":"****SALARY OF RESPONDENTS****","269938ce":"**JOB PROFILE**","f1365aba":"****CLOUD COMPUTING PRODUCTS USING REGULARLY****","c0278c89":"**NOTEBOOKS USING FOR DATA SCIENCE **","d55e3a49":"**Majority of the respondents are not using any cloud computing products. However, some of them are using AWS Elastic Compute Cloud(EC2).**","acd858e4":"**Majority of the respondents acquire Master's Degree and Bachelor's Degree.**\nIts quite improbable that more than 200 respondents  having no education past high school are using Kaggle!","eccf836d":"**PRIMARY TOOL USED TO ANALYZE DATA**","5728491f":"****AGE vs COUNTRY****","a35071e5":"****NATURAL LANGUAGE PROCESSING(NLP) METHODS USING REGULARLY****","da5a1e61":"****COMPANY SIZE****","5e7c33bb":"** AGE**","946aae8c":"**More than 3000 respondents prefer methods like Image Classification and other general purpose networks. And the next regularly used methods include General purpose image\/video tools,image segmentation methods and object detection methods respectively.**","b59c7d31":"****CONCLUSION****","9f30360f":"** GENDER**","bcf794a1":"**COUNTRY vs JOB**","54f277c9":"**75% of the respondents started using Maching Learning in less than 3 years but barely 500 respondents have experience of more than 10 years.**","a42eb2c1":"**FAVOURITE MEDIA SOURCES ON DATA SCIENCE**","b38e87f3":"**Obviously the prefered language would be Python.**","87834a8f":"**MACHINE LEARNING EXPERIENCE**","61ce2244":"****MACHINE LEARNING PRODUCTS USING REGULARLY****","ab63ecbe":"**Kaggle without any doubt is the most reliable media source for aspiring data scientists as well as for working professionals in the analytics field.**\nData Science Blogs are the next popular source followed by Youtube and Slack.","379395e8":"**Similar to the above cases, these products are also not used by majority of the respondents !**","aaf9c186":"**EVER USED TPU ?**","f986571e":"**VISUALIZATION TOOLS FOR DATA SCIENCE**","a5c39e03":"**Majority of respondents are not using any ML tools. However, more than 3000 respondents are using Automated Model Selection tools.**","4440395a":"**SALARY vs COUNTRY**","69ce57aa":"Considerable amount of respondents are from Startups which is quite satisfactory. This marks the growth of Data Science field. Also, more than 3000 respondents are from MNC's.","b7690d6c":"**PROGRAMMING LANGUAGE USING FOR DATA SCIENCE**","26618f60":"****MACHINE LEARNING TOOLS USING REGULARLY****","0b7c718a":"**Coursera is voted as the top platform for learning data science by the respondents.**\nCourses offered by Kaggle,Udemy and those of Universities are also preferred.","4da17ff3":"****MONEY SPENT ON MACHINE LEARNING****","284d4fa8":"****RELATIONAL DATABASE PRODUCTS****","63bca461":"We have seen that most of the respondents are from Startups. But according to the responses from India and USA, majority are from MNC's.","52068644":"**EDUCATION**","7b3ef4d2":"**Local Development Environments like RStudio,JupyterLab etc. are the most popular analysis tools followed by Basic Statistical softwares like Excel and Google sheets.**","09696ec4":"**Amazon Web Services and Google Cloud Platform are far ahead of other cloud computing platforms.**","802e8cec":"****LIFE OF A DATA SCIENTIST****\n\nThe job of a data scientist (DS) has changed radically over the last 10 years. 10 years ago it was believed you had have a PHD to become a data scientist because literally very few people knew what an ML algorithm was and people thought machine learning could only be done by a PHD.\n\nFast forward 10 years, now your grandmother could probably do machine learning on her laptop and today there is no clear definition of what a data scientist is\n\nData Science has emerged out as one of the most popular fields of 21st Century. Companies employ Data Scientists to help them gain insights about the market and to better their products. Data Scientists work as decision makers and are largely responsible for analyzing and handling a large amount of unstructured and structured data. In order to do so, he requires various tools and programming languages for Data Science to mend the day in the way he wants. We will go through some of these data science tools utilizes to analyze and generate predictions.\n\nIn this Notebook, we will explore the results of the survey based on all of the responses.\n\nGear up lads, its gonna be a long journey !\n\n","28bbcf73":"****MACHINE LEARNING FRAMEWORKS USING REGULARLY****","0ce4790b":"**Majority have an yearly compensation below $1000.**\n**Just 83 respondents have the highest salary range. **","ff0b5670":"**COUNTRY**\n"}}