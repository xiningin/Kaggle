{"cell_type":{"fca9b434":"code","9b47183e":"code","60b07319":"code","aff50e9a":"code","c2b5da49":"code","6b92f4fd":"code","fac2c779":"code","97d6169b":"code","95580262":"code","c206e189":"code","5468e0bb":"code","aa3ce19a":"code","6bc5c90e":"code","d792ec1e":"code","33d8412b":"code","dd89e500":"code","f18e7f83":"markdown","f6a8a30d":"markdown","802d914f":"markdown","ca3bee40":"markdown","173d02af":"markdown","83c0eb9f":"markdown","2f682916":"markdown","52321b76":"markdown","c62001e2":"markdown","d0120a0a":"markdown","3c3be5f3":"markdown","2b6fb508":"markdown","56e01014":"markdown","ffcd66e7":"markdown","70291d2e":"markdown","184db752":"markdown","4115dc10":"markdown"},"source":{"fca9b434":"# Load from pkl files\n\n!pip3 install pickle5\nimport pickle5 as pickle\n\nwith open('..\/input\/salmonella\/8-mers\/mic_dframe.pkl', 'rb') as file:\n    mic_dframe = pickle.load(file)\nwith open('..\/input\/salmonella\/8-mers\/suscep_classes.pkl', 'rb') as file:\n    suscep_classes = pickle.load(file)\n    \nmic_dframe","9b47183e":"# Function that creates a dataframe with the number of genomes per class\n\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nfrom matplotlib.colors import LinearSegmentedColormap\nimport matplotlib.pyplot as plt\n\ndef genomes_per_class(df, antibiotics):\n    amounts = []\n    for antibiotic in antibiotics:\n        single_amr = df[['Genome ID',antibiotic]].loc[pd.notnull(df[antibiotic])].drop_duplicates()\n        if single_amr.shape[0]==0:\n            mics = [4]\n            amount = [0]\n        else:\n            # Unique mic values\n            mics = df[antibiotic].loc[pd.notnull(df[antibiotic])].sort_values().unique()\n            amount = [len(single_amr.loc[single_amr[antibiotic]==mic]) for mic in mics]\n        amounts.append(pd.DataFrame(amount, index=mics).T)\n    amounts_df = amounts[0].append(amounts[1:len(antibiotics)], sort=False, ignore_index=True)\n    return amounts_df\n\ndef susceptibility_heatmap(data, annot, antibiotics, title):\n    sns.set(font_scale=.8, style='white')\n    myColors = ((0.0, 0.8, 0.0, 1.0), (0.0, 0.8, 0.8, 1.0), (0.8, 0.0, 0.0, 1.0))\n    cmap = LinearSegmentedColormap.from_list('Custom', myColors, len(myColors))\n    # Abbreviate the antibiotic names to 5 letters\n    antibiotic_abbre = {i:antibiotics[i][:5].upper() for i in range(len(antibiotics))}\n    # Number of genomes by Antibiotic and MIC.\n    data = data.rename(index=antibiotic_abbre)\n    # Set the width and height of the figure\n    plt.figure(figsize=(10,6))\n    # Add title\n    plt.title(title, fontsize=12)\n    # Heatmap showing the amount of genomes with the same MIC for each MIC, by antibiotic\n    hp = sns.heatmap(data=data, annot=annot, cmap=cmap, fmt='.4g')\n    # Manually specify colorbar labelling after it's been generated\n    colorbar = hp.collections[0].colorbar\n    colorbar.set_ticks([0.333, 1, 1.667])\n    colorbar.set_ticklabels(['Susceptible', 'Intermediate', 'Resistant'])\n    colorbar.ax.tick_params(labelsize=12)\n    # Add label for horizontal axes\n    plt.xlabel('MIC (micrograms per milliliter)', fontsize=12)\n    plt.ylabel('Antibiotic', fontsize=12)","60b07319":"# Antibiotics\nantibiotics = mic_dframe.columns[-12:]\n\n# Comupute the distribution of MIC measurements\nmic_distribution = genomes_per_class(mic_dframe, antibiotics)\n\n# Combine the MIC distribution and the susceptibility map\ntitle = 'Number of Genomes per Antibiotic and MIC'\nsusceptibility_heatmap(data=suscep_classes, annot=mic_distribution, antibiotics=antibiotics, title=title)","aff50e9a":"# Load the k-mer basis of k-mer counts\n\nkmers = np.load('..\/input\/salmonella\/8-mers\/kmers_basis.npy')\n\nprint(kmers)\nprint('size = {}'.format(kmers.shape[0]))","c2b5da49":"# Load a k-mer count example\n\ncount = np.load('..\/input\/salmonella\/8-mers\/counts\/1079901.3.npy')\n\nprint(count)\nprint('size = {}'.format(count.shape[0]))","6b92f4fd":"from sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import chi2\n\ndef best_N_features(target_df, antibiotic, N):\n    path = '..\/input\/salmonella\/8-mers\/counts\/'\n    genome_ids = target_df['Genome ID'].loc[pd.notnull(target_df[antibiotic])].values\n    X = np.array([np.load(path + genome_id + '.npy') for genome_id in genome_ids])\n    y = target_df[antibiotic].loc[pd.notnull(target_df[antibiotic])].values\n    n_best = SelectKBest(chi2, k=N)\n    X_new = n_best.fit_transform(X, y*10**5)\n    kmers = np.load('..\/input\/salmonella\/8-mers\/kmers_basis.npy')\n    selected_kmers = [column[0]  for column in zip(kmers, n_best.get_support()) if column[1]]\n    #scores = k_best.fit(X,y).scores_\n    best_feature_df = pd.DataFrame(X_new, columns = selected_kmers)\n    return best_feature_df\n\nkmer_dframe = best_N_features(target_df=mic_dframe, antibiotic='ampicillin', N=200)\n\nkmer_dframe","fac2c779":"# Define the features input (X) and the target output (y) variables\n\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.model_selection import train_test_split\n\n# Select the antibiotic\nantibiotic = 'ampicillin'\n\n# Features\nX = kmer_dframe.values\n\n# Standardize the input data\nscaler_X = StandardScaler().fit(X)\nX_scaled = scaler_X.transform(X)\n\n# Target\n# list of MIC values\ny = mic_dframe[antibiotic].loc[pd.notnull(mic_dframe[antibiotic])].values\n# reshape the list of mics\ny_reshape = np.reshape(y,(-1,1))\n# define encoder function\nencoder = OneHotEncoder(sparse=False)\n# transform the target categorical data to onehot code\ny_onehot = encoder.fit_transform(y_reshape)\n\n# Split into the training and test data\nX_train, X_test, y_train, y_test = train_test_split(X_scaled, y_onehot, test_size=0.1, random_state=0)","97d6169b":"# Define the classification model\n\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.callbacks import EarlyStopping\n\n# Model building function\ndef make_model(optimizer='adam',\n        loss='categorical_crossentropy',\n        metrics=['categorical_accuracy'],):\n    \n    model = keras.Sequential([\n        layers.Dense(64, activation='relu', input_shape=[X.shape[1]]),\n        layers.Dense(y_onehot.shape[1], activation='softmax'),\n    ])\n\n    # Add the cross-entropy loss and accuracy metric for threshold probability\n    model.compile(\n        optimizer=optimizer,\n        loss=loss,\n        metrics=metrics,\n    )\n    \n    return model\n\n# Define the model\nmodel = make_model()\n\n# Include an early stopping callback for convenience\nearly_stopping = keras.callbacks.EarlyStopping(\n    # monitor (loss or val_loss)\n    monitor='val_loss',\n    # how many epochs to wait before stopping (minimum epochs)\n    patience=100,\n    # minimium amount of change to count as an improvement\n    min_delta=0.001,\n    restore_best_weights=True,\n)","95580262":"def class_weighting(df, antibiotic, cv):\n    # Unique mic values\n    mics = df[antibiotic].loc[pd.notnull(df[antibiotic])].unique()\n    # Samples per class\n    samples = {mic : len(df.loc[df[antibiotic]==mic]) for mic in mics}\n    # Sorted classes\n    mics = sorted([key for key in samples.keys()])\n    # total data\n    total = len(df.loc[pd.notnull(df[antibiotic])])\n    # class weights\n    class_weight = {i: (1 \/ samples[mic])*(total\/len(mics))*(1\/cv) for i, mic in enumerate(mics)}\n    \n    return class_weight\n\nclass_weight = class_weighting(mic_dframe, antibiotic, cv=1)\n\nclass_weight","c206e189":"# Training with early stopping and class weights\n\nimport time\n\nstarttime = time.time()\n\nhistory = model.fit(\n    X_train, y_train,\n    shuffle=True,\n    validation_split=0.3,\n    #validation_data=(X_valid, y_valid),\n    batch_size=1024,\n    epochs=1000,\n    callbacks=[early_stopping],\n    class_weight = class_weight,\n    verbose=0, # hide the output because we have so many epochs\n)\n\nprint('Training Time: {:0.2f} seconds'.format(time.time() - starttime))","5468e0bb":"# Plot the loss (accuracy) and validation loss (accuracy) functions\n\nhistory_df = pd.DataFrame(history.history)\n# Start the plot at epoch 5\nhistory_df.loc[:, ['loss', 'val_loss']].plot()\nhistory_df.loc[:, ['categorical_accuracy', 'val_categorical_accuracy']].plot()\n\nprint((\"Best Validation Loss: {:0.2f}\" +\\\n      \"\\nBest Validation Accuracy: {:0.2f}\")\\\n      .format(history_df['val_loss'].min(), \n              history_df['val_categorical_accuracy'].max()))","aa3ce19a":"# Make predictions and compute the confusion matrix\n\nfrom sklearn.metrics import confusion_matrix\n\n# Make predictions and decode the OneHot\nmics_pred = [str(item[0]) for item in encoder.inverse_transform(model.predict(X_test))]\n# True mic values in string format\nmics_test = [str(item[0]) for item in encoder.inverse_transform(y_test)]\n# Unique mic values\nmics = [str(mic) for mic in sorted(mic_dframe[antibiotic].loc[pd.notnull(mic_dframe[antibiotic])].unique())]\n# Compute the confusion matrix\ncm = confusion_matrix(mics_test, mics_pred, normalize='true', labels=mics)\n# Plot the cm\nplt.figure(figsize=(5,5))\nsns.heatmap(cm, annot=True, fmt='.3g', cmap=\"YlGnBu\")\nplt.title('Confusion matrix')\nplt.ylabel('Actual label')\nplt.xlabel('Predicted label')","6bc5c90e":"from sklearn.metrics import roc_curve, auc, f1_score\nfrom sklearn.multiclass import OneVsRestClassifier\nfrom sklearn.svm import LinearSVC\n\ndef plot_multiclass_roc(clf, X_test, y_test, labels, antibiotic):\n    # predicted target\n    y_pred= model.predict(X_test)\n    # 1-dimension test and predicted values\n    y_test_1d = [str(item[0]) for item in encoder.inverse_transform(y_test)]\n    y_pred_1d = [str(item[0]) for item in encoder.inverse_transform(model.predict(X_test))]\n    \n    # structures\n    fpr = dict()\n    tpr = dict()\n    roc_auc = dict()\n    # roc curves\n    for i in range(len(mics)):\n        fpr[i], tpr[i], _ = roc_curve(y_test[:, i], y_pred[:, i])\n        roc_auc[i] = auc(fpr[i], tpr[i])\n    f1 = f1_score(y_test_1d, y_pred_1d, average=None)\n    # roc for each class\n    fig, ax = plt.subplots(figsize=(6, 4))\n    ax.plot([0, 1], [0, 1], 'k--')\n    ax.set_xlim([0.0, 1.0])\n    ax.set_ylim([0.0, 1.05])\n    ax.set_xlabel('False Positive Rate')\n    ax.set_ylabel('True Positive Rate')\n    #ax.set_title('Receiver operating characteristic example')\n    for i in range(len(mics)):\n        ax.plot(fpr[i], tpr[i], label='MIC = %i mg\/L: AUC= %0.2f, F1= %0.2f' % (mics[i], roc_auc[i], f1[i]))\n    ax.legend(loc=\"best\")\n    # Add legend\n    plt.legend(title='ROC curve: '+antibiotic[0].upper()+antibiotic[1:])\n    plt.show()\n    \n    \n# Unique mic values\nmics = sorted(np.unique(y))\nplot_multiclass_roc(model, X_test, y_test, labels=mics, antibiotic=antibiotic)","d792ec1e":"def kfold_cv_split(X, y, n_splits, kfold):\n\n    total_data = len(X)\n    fold_size = total_data \/\/ n_splits\n    X_folds = []\n    y_folds = []\n    for i in range(n_splits):\n        start  = i * fold_size\n        if i < n_splits-1:\n            end = start + fold_size\n        else:\n            end = total_data\n        X_folds.append(X[start:end,:])\n        y_folds.append(y[start:end])\n    \n    if kfold == 0:\n        train   = kfold + n_splits-2\n        val         = kfold + n_splits-2\n        test        = kfold + n_splits-1\n        X_train, X_val, X_test = np.concatenate(X_folds[kfold:train]), X_folds[val], X_folds[test]\n        y_train, y_val, y_test = np.concatenate(y_folds[kfold:train]), y_folds[val], y_folds[test]\n        \n    elif kfold == 1:\n        train   = kfold + (n_splits-2)\n        val         = kfold + (n_splits-2)\n        test        = kfold + (n_splits-1) - n_splits\n        X_train, X_val, X_test = np.concatenate(X_folds[kfold:train]), X_folds[val], X_folds[test]\n        y_train, y_val, y_test = np.concatenate(y_folds[kfold:train]), y_folds[val], y_folds[test]\n        \n    elif kfold == 2:\n        train   = kfold + (n_splits-2)\n        val         = kfold + (n_splits-2) - n_splits\n        test        = kfold + (n_splits-1) - n_splits\n        X_train, X_val, X_test = np.concatenate(X_folds[kfold:train]), X_folds[val], X_folds[test]\n        y_train, y_val, y_test = np.concatenate(y_folds[kfold:train]), y_folds[val], y_folds[test]\n    \n    else:\n        train   = kfold + (n_splits-2) - n_splits\n        val         = kfold + (n_splits-2) - n_splits\n        test        = kfold + (n_splits-1) - n_splits\n        X_train, X_val, X_test = np.concatenate((np.concatenate(X_folds[kfold:]),np.concatenate(X_folds[:train]))), X_folds[val], X_folds[test]\n        y_train, y_val, y_test = np.concatenate((np.concatenate(y_folds[kfold:]),np.concatenate(y_folds[:train]))), y_folds[val], y_folds[test]\n    \n    return X_train, X_val, X_test, y_train, y_val, y_test\n\n\ndef kfold_cross_val(X, y, cv):\n    n_splits=cv\n    kfold_accuracy = []\n    for kfold in range(n_splits):\n        # Split into the training, validation and test data\n        X_train, X_val, X_test, y_train, y_val, y_test = kfold_cv_split(X, y, n_splits, kfold)\n        # Define the model\n        model = make_model()\n        # Training\n        history = model.fit(\n            X_train, y_train,\n            validation_data=(X_val, y_val),\n            batch_size=1024,\n            epochs=1000,\n            callbacks=[early_stopping],\n            class_weight = class_weights,\n            verbose=0,\n        )\n        # Make predictions, compute the confusion matrix and extract the (per-class) accuracy\n        mic_pred = [str(item[0]) for item in encoder.inverse_transform(model.predict(X_test))]\n        mic_test = [str(item[0]) for item in encoder.inverse_transform(y_test)]\n        cm = confusion_matrix(mic_test, mic_pred, normalize='true')\n        accuracy = 100*np.diagonal(cm)\n        # Save the accuracy per fold\n        kfold_accuracy.append(accuracy)\n    # Get the average (per-class) accuracy from k-fold cross-validation\n    average_accuracy= [a.mean() for a in np.transpose(kfold_accuracy)]\n    return average_accuracy","33d8412b":"# Initialize a vector to save the average accuracy per antibiotic\n\naverage_accuracies = []\nfor antibiotic in antibiotics:\n    \n    start_time = time.time()\n    \n    # Features\n    kmer_dframe = best_N_features(target_df=mic_dframe, antibiotic=antibiotic, N=300)\n    X = kmer_dframe.values\n    \n    # Standardize the training data\n    scaler_X = StandardScaler().fit(X)\n    X_scaled = scaler_X.transform(X)\n\n    # Target\n    y = mic_dframe[antibiotic].loc[pd.notnull(mic_dframe[antibiotic])].values\n    y_reshape = np.reshape(y,(-1,1))\n    encoder = OneHotEncoder(sparse=False)\n    y_onehot = encoder.fit_transform(y_reshape)\n    \n    # Class weighting\n    class_weights = class_weighting(mic_dframe, antibiotic, cv=5)\n    \n    # Start modeling\n    start_modeling_time = time.time()\n\n    # Get the average (per-class) accuracy from k-fold cross-validation\n    average_accuracy = kfold_cross_val(X_scaled, y_onehot, cv=5)\n    \n    # Save dataframes with the average accuracy per mic value\n    mics = sorted(np.unique(y))\n    average_accuracies.append(pd.DataFrame(average_accuracy, index=mics).T)\n    \n    final_modeling_time = time.time()\n    \n    final_time = time.time()\n    \n    print('Time: {:0.2f} s, Modeling Time: {:0.2f} s'.format(final_time - start_time, final_modeling_time - start_modeling_time))      \n\n# Overall average accuracy\noverall_accuracy = average_accuracies[0].append(average_accuracies[1:len(antibiotics)], sort=False, ignore_index=True)","dd89e500":"# Plot the overall average accuracy\n\ntitle = 'Average Accuracy of the MIC prediction Model\\n (8-mer analysis)'\nsusceptibility_heatmap(data=suscep_classes, annot=overall_accuracy, antibiotics=antibiotics, title=title)","f18e7f83":"### 2.4 Training the network\n\nTraining the network means adjusting its weights, initially from random numbers, to values such that it can transform the features into the target. This work is done by the SDG-optimizer in steps. One step of training of the optimizer algorithm consists of at least 3 stages:\n\n1. Sample some training data and run it through the network to make predictions.\n2. Measure the loss between these predictions and the true values.\n3. Finally, adjust the weights in a direction that makes the loss smaller.\n\nFor validation purposes, before each step the training data will be shuffled and splitted into a training and validation datasets. The optimizer algorithm will use only the last defined training set to adjust the model weights, and use the validation dataset to save the model weights that make the validation loss smaller. Shuffling the data before splitting makes sure that training and validation datasets are always different. This way we can get better insights of model\u2019s performance.\n\nThe number of samples to run on each iteration is called batch size and can influence results when training with highly unbalanced datasets. In these cases the batch size must be large in order to have the chance of learning the classes with fewer examples. Although we are already dealing with this defining the model class weights. A complete round of the training data is called an epoch and will be required more than one epoch to finish the training.\n\nRun the next cell to train the model","f6a8a30d":"For this particular analysis, the size of these vectors is 65.537 and the weight for 7000 genomes reaches 4 GB. Therefore, if we consider all the features to train the models this would have an unnecessarily large memory footprint and we would not know which features are the most important. We solve these two matters by considerig a feature selection. This is a statistical procedure that allows us to work with only the most relevant features.\n\nIn the next cell we consider a $\\chi2$ test to select the best 200 features and give them into pandas dataframe.","802d914f":"Now compute the overall average accuracy of the MIC prediction model.","ca3bee40":"ROC curves typically feature true positive rate on the Y axis, and false positive rate on the X axis. This means that the top left corner of the plot is the \u201cideal\u201d point - a false positive rate of zero, and a true positive rate of one. This is not very realistic, but it does mean that a larger area under the curve (AUC) is usually better.\n\nAUC ranges in value from 0 to 1. A model whose predictions are 100% wrong has an AUC of 0.0; one whose predictions are 100% correct has an AUC of 1.0.\n\nThe F1 score is the harmonic mean of presicion and recall, where an F1 score reaches its best value at 1 and worst score at 0. The relative contribution of precision and recall to the F1 score are equal. The formula for the F1 score is:\n\nPresicion = TP \/ (TP + FP). It corresponds to the percentage of predictive positives correctly classified\n\nRecall = TP \/ (TP + FN)\n\nF1 = 2 Presicion * Recall \/ (Presicion + Recall) = TP \/ (TP + (FP + FN)\/2)\n\nBoth, the AUC and the F1 score were computed in the multi-label classification problem and were added in the plot labels above.","173d02af":"## 2 Model building\n\nWhen building a ML model we start by defining the validation procedure with which to measure the model performance, together with an appropiate data partition. Here, I first estimate the model performance computing different metrics in a randomly sampled test dataset for a particular antibiotic, showing the details of the modeling. Then, I measure the model performance on all antibiotics as a 5-fold cross-validation experiment in order to obtain a more accurate estimate from multiple measures of model quality. \n\nIn both procedures, we split the total data (the rows) into the test, training and validation datasets:\n\n1. The training dataset is used to train or fit the network in steps.\n2. The validation dataset to evaluate any metric of performance after each training step.\n3. The test dataset is used at the end to evaluate the model performance on unseen data. \n\nIn these datasets, the $k$-mer counts will represent the features (the input columns), and the MICs of a given antibiotic the target (the output column).\n\n### 2.1 Classification variables: feature selection, feature scaling and target encoding\n\nIn the input folder, the k-mer counts are separated from the k-mer basis. Let us load the latter and show the size of the these counts.","83c0eb9f":"### 2.3 Class weighting\n\nRemember we are working with unbalanced data. Before training, let us definde class weights to help the model to prevent learning only the most represented class. By class weighting we tell the model how much attention to pay in each class. These weights are roughly the inverse of the number of samples per class.","2f682916":"The diagonal of the cofusion matrix shows the per-class accuracy, i.e., the rates of classes correctly classified. In this case, the model can reach good and balanced accuracies for 2 of 3 classification labels. Later we will see that these two labels corresponds to \"susceptible\" and \"resistant\".\n\n### 2.7 ROC curve, ROC-AUC and F1 score\n\nNow let us evaluate the Receiver Operating Characteristic (ROC) curve and other metrics for our multi-label classifier. An ROC curve illustrates the performance ability at all classification thresholds of the logistic regression, and for multi-label classifiers this is done on each classification label. This curve plots two parameters:\n\n1. True Positive Rate = TP \/ (TP + FN) (It is also known as Recall or Sensitivity; the percentage of actual positives correctly classified)\n2. False Positive Rate = FP \/ (TN + FP)\n\nLowering the classification threshold classifies more items as positive, thus increasing both False Positives and True Positives.\n\nRun the next cell to plot the multi-label ROC curve and some measured metrics.","52321b76":"The k-mer counts for each genome are vectors whose entries show the times such k-mers appeared in the assembly. These vectors are all the same size and are carefully arranged to show the counts of the basis above.","c62001e2":"### 2.2 Define the Neural Network\n\nThe model architecture is a neural network implemented in Keras (Python). It includes an input dense layer of 64 neurons width and an output layer with as many neurons as the dimension of the onehot encoding. The Adam optimezer and the categorical crossentropy loss function are chosen to train the model, and the categorical accuracy to measure the model performance. We also include an early stopping in order to stop the train when a maximum accuracy is reached.","d0120a0a":"The above dataframe shows MIC measurements for 12 antibiotics in a 2-fold dilution series. The\nMinimum Inhibitory Concentration is defined as the minimum concentration of an antibiotic to prevent\nthe further growth of an infectious agent in vitro and is expressed in units of mg\/L. For each row, the first column shows the \"Genome ID\" of the sequencied bacterial strain, while the other columns show the measured MIC values. \n\nDifferent MIC values are found for the selected antibiotics, thus the problem of transforming a $k$-mer spectrum into one of such values is a problem of classification. In these problems, the success of the model depends on the number of samples per class. In this case, this corresponds to the number of genome samples tested for the different MIC values. Although for some antibiotics not all genomes have a measured MIC value (NAN values are present), it is still possible to train machine learning models by selecting only those genomes with associated MIC values.\n\nIn the input folder, it is also included a susceptibility map. This summarizes the information on whether a given sequenced strain, for which the antibioitc MIC was measured, is \"susceptible\", \"intermediate\" or \"resistant\", and has been build based on the definitions of SIR terms and the AMR metadata selected from PATRIC platform. It is a map in the MIC-antibiotic plane.\n\nIn the next cell, two functions are defined to count the number of informed genomes per antibiotic and MIC, and to show them in a susceptibility color map. Combining the distribution of MIC values with the susceptibility map we can see how many strains are susceptible, intermediate and resistant.","3c3be5f3":"Once the relation between MIC values and SIR terms is known it is possible to obtain the antibiotic sensitivity of the infective organisms from the predicted MIC value. Here, we present the average accuracy of the MIC prediction model to 12 antibiotics. We can identify four critical factors that influence the model accuracy per antibiotic: The number of samples per MIC class, the balance between MIC classes, the number of MIC classes per antibiotic (or the cardinality) and the length of the oligonucleotide $k$-mers. A high and balanced number of samples per antibiotic, as well as the length of $k$-mers, increase the accuracy of model's predictions. Regarding the cardinality, the quality of the model decreases as the number of classes increases.\n\nThe figure above shows the overall average accuracy obtained in a 5-fold cross-validation. The model has a great performance recognizing resistant and intermediate strains from susceptible ones for ampicillin, amoxicillin\/clavulanic acid, tetracycline and chloramphenicol. For other antibiotics the performance is regular, with some level of balance. In particular, for ceftriaxone and trimethoprim\/sulfamethoxazole the large differences in the number of samples, of around 5000, reduces the accuracy. For $k=8$, the model can reach an average accuracy near to 90\\% for some antibiotics. However, better results are expected for $k \\gtrsim 10$ (Nguyen et al. https:\/\/doi.org\/10.1128\/JCM.01260-18).\n\nMuch things can still be done concerning the model performance and feature selection, as well as the interpretation of the selected k-mers, which may cover known or novel genes associated to acquired resistance.","2b6fb508":"### 2.5 Plot the loss and accuracy functions\nDuring the training the loss function represents the difference between predictions and their true values and the accuracy function represents a measure of model performance. The validation loss and validation accuracy are the loss and accuracy functions evaluated on validation data. The loss functions should reach small values, while the accuracy functions values near 1.\n\nLet us plot the loss and accuracy functions over the epochs and show the best validation loss and validation accuracy values.","56e01014":"### 2.6 Per-class accuracy and confusion matrix\n\nLet us now evaluate the model per-class accuracy. Here, we use the test dataset to evaluate the model and extract the exact accuracy from the confusion matrix. This is a square matrix that shows the percentage of examples correctly and incorrectly classified per class. \n\nNotice that the model did not see the test dataset during training and so it is appropriate for model validation.","ffcd66e7":"## 3 Overall average accuracy\n\nIn this section we evaluate the model performance to 12 antibiotics using a 5-fold cross-validation. Here, the entire dataset is divided into folds, and used alternately as training, validation and test datasets. In this way we obtain a more accurate measure of model quality using the whole dataset for the different purposes in the modeling process.\n\nIn the next hidden cell it is defined a k-fold split function to provide indices for train, validation, and test datasets, and a cross-validation function to compute the accuracy per fold and return an average measure of the model accuracy.","70291d2e":"The best validation accuracy is an overall accuracy given by the ratio of samples correctly classified. However, in classification problems we seek for the per-class accuracy, the percentage of right predictions per class.","184db752":"Selecting the best 200 features we have reduced the data to 0.3% and stay with the most relevant k-mers associated to acquired ampicillin resistance.\n\nNow, let us define the input ($k$-mers) and the output (MIC) variables for the ampicillin antibiotic and split them into the test and training datasets. The validation dataser will be a fraction of the training dataset separated by Keras model during training. \n\nThe input data, i.e., the $k$-mers counts, are numbers greater than one. However, neural networks works preferably in normalized input data. Thus, we can use the StandarScaler from scikit-learn to standardize the input data.\n\nSince the MIC target is a categorical variable we can encode it using the OneHotEncoder class from scikit-learn. \n\nRun the next cell to define the input and output variables, standardize the features and encode the target.","4115dc10":"# Predicting Antibiotic Minimum Inhibitory Concentration using Artificial Neural Networks and Public Genomic Databases\n\nSince the emergence and rise of antimicrobial resistance (AMR) of infectious organisms worldwide, the task for predicting antimicrobial susceptibility testing (AST) outcomes has become relevant. For the purpose of struggle against AMR, genome databases, as OneBr (http:\/\/onehealthbr.com\/) in Brazil and PATRIC (https:\/\/www.patricbrc.org\/) in U.S., are working to include genomic data and metadata, which can be used to train machine learning (ML) models in order to predict phenotype features of genomic samples. In this work we consider around 7000 Salmonella genomes with AMR metadata from PATRIC database to build ML classifiers to predict AST outcomes, based on a gene-independent $k$-mers analysis and artificial Neural Networks (NN).\n\nThe Salmonella collection is the most abundant collection of AMR metadata at PATRIC and uncovers an alarming number of antibiotic resistant strains. We identify acquired-resistance and intermediate resistance to 12 of the most used antibiotics. Among the reasons that could lead to the acquired AMR resistance are the misuse an overuse of antibiotics. In this work, in order to prevent the further increase of AMR, we build ML classifiers to predict the Minimum Inhibitory Concentration (MIC) and the antibiotic sensitivity of sequenced strains. This could help microbiologist to compare their AST outcomes with automated predictions and to assist physicians with accurate and faster diagnostics for antibiotic stewardship.\n\nAlthough the MIC metadata available at PATRIC platform covers a broad range of MIC measurements, coming from different sources, these are representatively unbalanced and disconnected in term of serial dilutions. Thus, in order to simplify the output for ML models we approach those values to those of a 2-fold dilution series.\n\nConsidering the interpretative categories stated at different AST standard documents, for those antibiotics that Salmonella developed resistance we build classifiers to directly predict MIC values and indirectly predict the AMR phenotype, i.e., whether an isolate is \"susceptible\", \"intermediate\" or \"resistant\" (SIR terms).\n\nAmong the tasks covered in this study are:\n\n1. Explore the datasets and define the categorical problem.\n2. Implement a NN as a ML model and compute different measures of perfomance, including the per-class accuracy (or simply accuracy), the ROC curve, the ROC-AUC and the F1 score.\n3. Measure the model accuracy using k-fold cross-validation over different antibiotics.\n\n\n\n### 1.1 k-mer analysis\n\nIn order to provide the model with spectral data, I consider the $k$-mer counts of genome assemblies as the input of the model, and the corresponding antibiotic MIC measurments as the output. Here, $k$-mers means subsequences of length $k$ contained in a genome assembly and are composed of nucleotides (i.e. A, T, G, and C). The number of $k$-mers in a biological sequence of length $L$ is $L \u2212 k + 1$, being the number of unique $k$-mers less than the total number in a sample since repetitions throughout the sequence. These repetitions are precisely the counts carrying the information required by the model. The value of $k$ is representative of how deep and refined the genomic analysis is. For this preliminary analysis I consider $k=8$.\n\nThe produced $k$-mer counts were made public in Kaggle platform, together with MIC metadata selected from Patric platform. The $k$-mer counts was computed locally using only Python routines and Unix shell scripts allowing the k-mer analysis of thousands of genomes in a matter of few hours in regular desk computers, transforming the genomic data into lighter data. Although the processing time and the $k$-mer data weight strongly depends on the length of the oligonucleotide $k$-mer.\n\nThe AMR data and $k$-mer counts are added in the input data folder.\n\n### 1.2 AMR data\n\nLet us start by examining the AMR data. Run the next cell to load the MIC measurements and the susceptibility classes."}}