{"cell_type":{"dd02df5c":"code","dc1e6a3c":"code","0c43108e":"code","5cb17052":"code","9e7fd8b5":"code","554f1a60":"code","48e84846":"code","e3b731f2":"code","18fec0c7":"code","abaa22e9":"code","d7a2bc0c":"code","190890cb":"code","fe3e14bb":"code","176fa570":"code","8084eb82":"code","eaff394e":"code","64882e34":"code","bfe4b2ad":"code","efc89bbf":"code","31f6e450":"code","9fae6693":"code","d4e6b55d":"code","418af0d8":"code","c937de72":"code","3daa84f0":"code","a3c1701e":"code","3076c963":"code","92b40156":"code","0ea8cb76":"code","01d16911":"code","5ff3aa38":"code","f956aa55":"code","56d918e4":"code","8a38a988":"code","7e3974b6":"code","b0825131":"code","65aa0bc9":"code","20009447":"code","cd85167d":"code","32817011":"code","d9f2583f":"code","991d1c43":"code","817c8768":"code","b04258f2":"code","df55bd3b":"code","5e7d8062":"code","d629e720":"code","6cadd5ad":"code","3498c36c":"code","5cb0828a":"code","ced3cb2a":"code","8b1dca93":"code","6ba79e46":"code","98639e3a":"code","7a8a3eb6":"code","48fea598":"code","33b0a08b":"code","39e66bb6":"code","69e001d4":"code","94b02ada":"code","55f897b7":"code","71f04d40":"code","854010a3":"code","6828ed54":"code","6b1ce411":"code","25146a58":"code","b56e4e43":"code","c2d7707d":"code","bdace13e":"code","03a67139":"code","622fd476":"code","c2121cb7":"code","a20d8d7a":"code","98fd5075":"code","5e432dd9":"code","fbb79397":"code","d46bb6ea":"code","abbea190":"code","4d64d96a":"code","aba0581b":"code","a4aeeac3":"code","5e4bd4fa":"code","4b661e7b":"code","8a59943f":"code","828f0331":"markdown"},"source":{"dd02df5c":"# organize imports\nfrom keras.models import Sequential\nfrom keras.layers.core import Dense, Activation, Dropout\nfrom sklearn.model_selection import train_test_split","dc1e6a3c":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport warnings\nwarnings.filterwarnings('ignore')","0c43108e":"# seed for reproducing same results\nseed = 9\nnp.random.seed(seed)","5cb17052":"# load pima indians dataset\ndf = pd.read_csv('\/kaggle\/input\/parkinsons-data-set\/parkinsons.data')\ndata = df.copy()","9e7fd8b5":"data.head()","554f1a60":"def draw_missing_data_table(df):\n    total = df.isnull().sum().sort_values(ascending=False)\n    percent = (df.isnull().sum()\/df.isnull().count()).sort_values(ascending=False)\n    missing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\n    return missing_data\n\ndraw_missing_data_table(data)","48e84846":"predictors = data.drop(['name'], axis = 1)\n\npredictors = predictors.drop(['status'], axis = 1)\nX = predictors\nY = data['status']","e3b731f2":"# from imblearn.under_sampling import RandomUnderSampler\n# rus = RandomUnderSampler(random_state=0, replacement=True)\n# X_resampled, y_resampled = rus.fit_resample(X,Y)\n# len(X_resampled)","18fec0c7":"from sklearn.model_selection import train_test_split\nX_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = .25, random_state = 7)\n\nfrom sklearn.preprocessing import StandardScaler  \nscaler = StandardScaler()\nscaler.fit(X_train)\nX_train = scaler.transform(X_train)\nX_test = scaler.transform(X_test)","abaa22e9":"data.describe(include='all')","d7a2bc0c":"sns.pairplot(data, hue='status')","190890cb":"data.hist(figsize=(28,28))","fe3e14bb":"data.hist(bins=50, figsize=(28,28))\n","176fa570":"plt.figure(figsize=(20,15))\nsns.heatmap(data.corr() ,annot=True)\nplt.show()","8084eb82":"data.plot(kind='box', subplots=True, layout=(5,5), sharex=False, sharey=False, figsize=(28, 28))\nplt.show()","eaff394e":"dataset = data\nall_cols = list(dataset.columns.values)\nall_cols.remove('name')\nall_cols","64882e34":"bins = np.linspace(-10, 10, 30)\nplt.figure(figsize=(15,30))\nfor i in range(1, 22):\n    plt.subplot(14, 2, i)\n    col = all_cols[i]\n    plt.title(all_cols[i])\n    plt.hist(data[col][data.status == 1], alpha=0.5, label='x')\n    plt.hist(data[col][data.status == 0], alpha=0.5, label='y')\n    \n\nplt.legend(loc='upper right')\nplt.show()","bfe4b2ad":"from sklearn import metrics\nfrom sklearn.neighbors import KNeighborsClassifier\nmodel = KNeighborsClassifier()\nmodel.fit(X_train, Y_train)\ny_pred = model.predict(X_test)\n# summarize the fit of the model\nprint(\"KNeighborsClassifier: \")\nprint(metrics.accuracy_score(Y_test, y_pred))","efc89bbf":"from sklearn.neural_network import MLPClassifier\nmlp = MLPClassifier(hidden_layer_sizes=(22,18,12),max_iter=1500)\nmlp.fit(X_train,Y_train)\ny_pred = mlp.predict(X_test)\nprint(\"MLPClassifier: \")\nprint(metrics.accuracy_score(Y_test, y_pred))\n","31f6e450":"from xgboost import XGBClassifier\nfrom sklearn.metrics import accuracy_score\nmodel = XGBClassifier()\nmodel.fit(X_train, Y_train)\ny_pred = model.predict(X_test)\nprint(\"XGBClassifier: \")\nprint(metrics.accuracy_score(Y_test, y_pred))","9fae6693":"classifier = Sequential()\n#First Hidden Layer\nclassifier.add(Dense(16, activation='relu', kernel_initializer='random_normal', input_dim=22))\n#Second  Hidden Layer\nclassifier.add(Dense(8, activation='relu', kernel_initializer='random_normal'))\n#Output Layer\nclassifier.add(Dense(1, activation='sigmoid', kernel_initializer='random_normal'))\n#Compiling the neural network\nclassifier.compile(optimizer ='adam',loss='binary_crossentropy', metrics =['accuracy'])\n#Fitting the data to the training dataset\nclassifier.fit(X_train,Y_train, batch_size=10, epochs=100)","d4e6b55d":"# evaluate the model\ntscores = classifier.evaluate(X_test, Y_test)\nprint(\"Test Accuracy: %.2f%%\" %(tscores[1]*100))","418af0d8":"trscores=classifier.evaluate(X_train, Y_train)\nprint(\"Train Accuracy: %.2f%%\" %(trscores[1]*100))","c937de72":"y_pred=classifier.predict(X_test)","3daa84f0":"\nclassifier.save_weights(\"model0.h5\")\nprint(\"Saved model to disk\")","a3c1701e":"model_json = classifier.to_json()\nwith open(\"model0.json\", \"w\") as json_file:\n    json_file.write(model_json)","3076c963":"from keras.models import model_from_json\n\njson_file = open('.\/model0.json', 'r')\nloaded_model_json = json_file.read()\njson_file.close()\nloaded_model = model_from_json(loaded_model_json)\n# load weights into new model\nloaded_model.load_weights(\".\/model0.h5\")\nprint(\"Loaded model from disk\")","92b40156":"# evaluate loaded model on test data\nloaded_model.compile(optimizer ='adam',loss='binary_crossentropy', metrics =['accuracy'])\nscore = loaded_model.evaluate(X_test, Y_test, verbose=0)\nprint(\"%s: %.2f%%\" % (loaded_model.metrics_names[1], score[1]*100))","0ea8cb76":"\nfrom xgboost import Booster\n\nmodel._Booster.save_model('model.bin')\n\ndef load_xgb_model():\n    _m = XGBClassifier()\n    _b = Booster()\n    _b.load_model('model.bin')\n    _m._Booster = _b\n    return _m\n\nmodel = load_xgb_model()","01d16911":"model.fit(X_train, Y_train)","5ff3aa38":"y_pred = model.predict(X_test)","f956aa55":"model.score(X_test , Y_test) ","56d918e4":"dataset = data","8a38a988":"dataset['status'].value_counts(normalize=True)*100","7e3974b6":"# from pandas.plotting import scatter_matrix\n\n# attributes = list(data.columns.values)\n# scatter_matrix(data[attributes], figsize=(500.0, 500.0))","b0825131":"from sklearn import datasets\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import VotingClassifier\n\n\nclf1 = LogisticRegression(random_state=1)\nclf2 = RandomForestClassifier(n_estimators=50, random_state=1)\nclf3 = GaussianNB()\n\neclf = VotingClassifier(\n    estimators=[('lr', clf1), ('rf', clf2), ('gnb', clf3)],\n    voting='hard')\n\nfor clf, label in zip([clf1, clf2, clf3, eclf], ['Logistic Regression', 'Random Forest', 'naive Bayes', 'Ensemble']):\n    scores = cross_val_score(clf, X, Y, scoring='accuracy', cv=5)\n    print(\"Accuracy: %0.2f (+\/- %0.2f) [%s]\" % (scores.mean(), scores.std(), label))\n    \n    ","65aa0bc9":"eclf.fit(X_train,Y_train)\ny_pred=eclf.predict(X_test)\neclf.score(X_test , Y_test) ","20009447":"from sklearn import datasets\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom itertools import product\nfrom sklearn.ensemble import VotingClassifier\n\n# Loading some example data\n\n# Training classifiers\nclf1 = DecisionTreeClassifier(max_depth=4)\nclf2 = KNeighborsClassifier(n_neighbors=7)\nclf3 = SVC(kernel='rbf', probability=True)\neclf = VotingClassifier(estimators=[('dt', clf1), ('knn', clf2), ('svc', clf3)],\n                        voting='soft', weights=[4, 5, 3])\n\nclf1 = clf1.fit(X_train, Y_train)\nclf2 = clf2.fit(X_train, Y_train)\nclf3 = clf3.fit(X_train, Y_train)\neclf = eclf.fit(X_train, Y_train)","cd85167d":"eclf.score(X_test, Y_test)","32817011":"from sklearn.model_selection import cross_val_score\nfrom sklearn.datasets import make_blobs\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import ExtraTreesClassifier\nfrom sklearn.tree import DecisionTreeClassifier\n\n\nclf = DecisionTreeClassifier(max_depth=None, min_samples_split=2,\n    random_state=0)\nclf.fit(X_train,Y_train)\ny_pred=clf.predict(X_test)\nclf.score(X_test , Y_test) ","d9f2583f":"clf = RandomForestClassifier(n_estimators=10, max_depth=None,\n    min_samples_split=2, random_state=0)\nscores = cross_val_score(clf, X, Y, cv=5)\nscores.mean()\nclf.fit(X_train,Y_train)\ny_pred=clf.predict(X_test)\nclf.score(X_test , Y_test) ","991d1c43":"clf = ExtraTreesClassifier(n_estimators=100, max_depth=10,\n    min_samples_split=2, random_state=0)\nclf.fit(X_train,Y_train)\ny_pred=clf.predict(X_test)\nclf.score(X_test , Y_test) ","817c8768":"\n# Voting Ensemble for Classification\nimport pandas\nfrom sklearn import model_selection\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import VotingClassifier\n\nseed = 7\nkfold = model_selection.KFold(n_splits=10, random_state=seed)\n# create the sub models\nestimators = []\nmodel1 = LogisticRegression()\nestimators.append(('logistic', model1))\nmodel2 = DecisionTreeClassifier()\nestimators.append(('cart', model2))\nmodel3 = SVC()\nestimators.append(('svm', model3))\n# create the ensemble model\nensemble = VotingClassifier(estimators)","b04258f2":"ensemble.fit(X_train,Y_train)\ny_pred=ensemble.predict(X_test)\nensemble.score(X_test , Y_test) ","df55bd3b":"# Stochastic Gradient Boosting Classification\nimport pandas\nfrom sklearn import model_selection\nfrom sklearn.ensemble import GradientBoostingClassifier\n# url = \"https:\/\/raw.githubusercontent.com\/jbrownlee\/Datasets\/master\/pima-indians-diabetes.data.csv\"\n# names = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\n# dataframe = pandas.read_csv(url, names=names)\n# array = dataframe.values\n# X = array[:,0:8]\n# Y = array[:,8]\n# seed = 7\nnum_trees = 100\nkfold = model_selection.KFold(n_splits=10, random_state=seed)\nmodel = GradientBoostingClassifier(n_estimators=num_trees, random_state=seed)\nresults = model_selection.cross_val_score(model, X, Y, cv=kfold)\nprint(results.mean())","5e7d8062":"results","d629e720":"clf = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0,max_depth=1, random_state=0).fit(X_train, Y_train)\nclf.score(X_test, Y_test)","6cadd5ad":"clf = RandomForestClassifier(n_estimators=10, max_depth=10,\n    min_samples_split=2, random_state=0)\nclf.fit(X_train,Y_train)\ny_pred=clf.predict(X_test)\nclf.score(X_test , Y_test) ","3498c36c":"\nimport itertools\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport matplotlib.gridspec as gridspec\n\nfrom sklearn import datasets\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB \nfrom sklearn.ensemble import RandomForestClassifier\nfrom mlxtend.classifier import StackingClassifier\n\nfrom sklearn.model_selection import cross_val_score, train_test_split\n\nfrom mlxtend.plotting import plot_learning_curves\nfrom mlxtend.plotting import plot_decision_regions","5cb0828a":"from sklearn.model_selection import RandomizedSearchCV,GridSearchCV\nfrom scipy.stats import randint\n\nest = RandomForestClassifier(n_jobs=-1)\nrf_p_dist={'max_depth':[1,2,3,4,5,6,7,8,9,10],\n           'n_estimators':[1,2,3,4,5,10,11,12,13,100,200,300,400,500],\n              'max_features':[1,2,3,4,5],\n               'criterion':['gini','entropy'],\n               'bootstrap':[True,False],\n               'min_samples_leaf':[1,2,3,4,5]\n         \n              \n              }\n\nrs=GridSearchCV(estimator=est,param_grid=rf_p_dist)\n","ced3cb2a":"y = np.array(Y_train)\n\nx = np.array(X_train)\n","8b1dca93":"# iris = datasets.load_iris()\n# X, y = iris.data[:, 1:3], iris.target\n\n\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.neighbors import KNeighborsClassifier\n#create new a knn model\nknn = KNeighborsClassifier()\n#create a dictionary of all values we want to test for n_neighbors\nparams_knn = {'n_neighbors': np.arange(1, 2)}\n#use gridsearch to test all values for n_neighbors\nknn_gs = GridSearchCV(knn, params_knn, cv=5)\n\n\nfrom sklearn.ensemble import AdaBoostClassifier\n\n# abcl = AdaBoostClassifier( n_estimators= 50)\n# clf2 = RandomForestClassifier(random_state=1)\n# clf3 = abcl\n\nclf2 = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0,max_depth=1, random_state=0)\nclf3 = RandomForestClassifier(n_estimators=10, max_depth=None,min_samples_split=2, random_state=0)\n\nlr = LogisticRegression()\nsclf = StackingClassifier(classifiers=[clf2, clf3], \n                          meta_classifier=lr)","6ba79e46":"label = ['KNN', 'Random Forest', 'Naive Bayes', 'Stacking Classifier']\nclf_list = [clf1, clf2, clf3, sclf]\n    \nfig = plt.figure(figsize=(10,8))\ngs = gridspec.GridSpec(2, 2)\ngrid = itertools.product([0,1],repeat=2)\n\nclf_cv_mean = []\nclf_cv_std = []\nfor clf, label, grd in zip(clf_list, label, grid):\n        \n    scores = cross_val_score(clf, x, y, cv=3, scoring='accuracy')\n    print(\"Accuracy: %.2f (+\/- %.2f) [%s]\" %(scores.mean(), scores.std(), label))\n    clf_cv_mean.append(scores.mean())\n    clf_cv_std.append(scores.std())\n        \n    clf.fit(x, y)\n\nplt.show()","98639e3a":"y_pred = clf.predict(X_test)","7a8a3eb6":"clf.score(X_test , Y_test) ","48fea598":"#plot learning curves\nX_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3, random_state=42)\n    \nplt.figure()\nplot_learning_curves(X_train, Y_train, X_test, Y_test, sclf, print_model=False, style='ggplot')\nplt.show()","33b0a08b":"import itertools\nimport numpy as np\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport matplotlib.gridspec as gridspec\n\nfrom sklearn import datasets\n\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.linear_model import LogisticRegression\n\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.model_selection import cross_val_score, train_test_split\n\nfrom mlxtend.plotting import plot_learning_curves\nfrom mlxtend.plotting import plot_decision_regions","39e66bb6":"    \n#XOR dataset\n#X = np.random.randn(200, 2)\n#y = np.array(map(int,np.logical_xor(X[:, 0] > 0, X[:, 1] > 0)))\n    \nclf = DecisionTreeClassifier(criterion='entropy', max_depth=1)\n\nnum_est = [1, 2, 3, 10]\nlabel = ['AdaBoost (n_est=1)', 'AdaBoost (n_est=2)', 'AdaBoost (n_est=3)', 'AdaBoost (n_est=10)']","69e001d4":"fig = plt.figure(figsize=(10, 8))\ngs = gridspec.GridSpec(2, 2)\ngrid = itertools.product([0,1],repeat=2)\n\nfor n_est, label, grd in zip(num_est, label, grid):     \n    boosting = AdaBoostClassifier(base_estimator=clf, n_estimators=n_est)   \n    boosting.fit(X, Y)\n\nplt.show()","94b02ada":"\n#Ensemble Size\nnum_est = map(int, np.linspace(1,100,20))\nbg_clf_cv_mean = []\nbg_clf_cv_std = []\nfor n_est in num_est:\n    ada_clf = AdaBoostClassifier(base_estimator=clf, n_estimators=n_est)\n    scores = cross_val_score(ada_clf, X, Y, cv=3, scoring='accuracy')\n    bg_clf_cv_mean.append(scores.mean())\n    bg_clf_cv_std.append(scores.std())","55f897b7":"\n%matplotlib inline\n\nimport itertools\nimport numpy as np\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport matplotlib.gridspec as gridspec\n\nfrom sklearn import datasets\n\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\n\nfrom sklearn.ensemble import BaggingClassifier\nfrom sklearn.model_selection import cross_val_score, train_test_split\n\nfrom mlxtend.plotting import plot_learning_curves\nfrom mlxtend.plotting import plot_decision_regions\n\nnp.random.seed(0)","71f04d40":"\n    \nclf1 = DecisionTreeClassifier(criterion='entropy', max_depth=1)\nclf2 = KNeighborsClassifier(n_neighbors=1)    \n\nbagging1 = BaggingClassifier(base_estimator=clf1, n_estimators=10, max_samples=0.8, max_features=0.8)\nbagging2 = BaggingClassifier(base_estimator=clf2, n_estimators=10, max_samples=0.8, max_features=0.8)","854010a3":"\nlabel = ['Decision Tree', 'K-NN', 'Bagging Tree', 'Bagging K-NN']\nclf_list = [clf1, clf2, bagging1, bagging2]\n\nfig = plt.figure(figsize=(10, 8))\ngs = gridspec.GridSpec(2, 2)\ngrid = itertools.product([0,1],repeat=2)\n\nfor clf, label, grd in zip(clf_list, label, grid):        \n    scores = cross_val_score(clf, X_train, Y_train, cv=3, scoring='accuracy')\n    print(\"Accuracy: %.2f (+\/- %.2f) [%s]\" %(scores.mean(), scores.std(), label))\n        \n    clf.fit(X_train, Y_train)\n\n\nplt.show()","6828ed54":"clf.score(X_test,Y_test)","6b1ce411":"from sklearn.ensemble import RandomForestClassifier\nrfcl = RandomForestClassifier(n_estimators = 70)\nrfcl = rfcl.fit(X_train, Y_train)\ny_pred = rfcl.predict(X_test)\nrfcl.score(X_test , Y_test)","25146a58":"rfcl.score(X_test , Y_test)","b56e4e43":"from sklearn.tree import DecisionTreeClassifier\ndt = DecisionTreeClassifier()\ndt.fit(X_train, Y_train)\ny_pred = dt.predict(X_test)","c2d7707d":"from sklearn.metrics import roc_curve, auc\nfalse_positive_rate, true_positive_rate, thresholds = roc_curve(Y_test, y_pred)\nroc_auc = auc(false_positive_rate, true_positive_rate)\nroc_auc","bdace13e":"max_depths = np.linspace(1, 32, 32, endpoint=True)\ntrain_results = []\ntest_results = []\nfor max_depth in max_depths:\n   dt = DecisionTreeClassifier(max_depth=max_depth)\n   dt.fit(X_train, Y_train)\n   train_pred = dt.predict(X_train)\n   false_positive_rate, true_positive_rate, thresholds = roc_curve(Y_train, train_pred)\n   roc_auc = auc(false_positive_rate, true_positive_rate)\n   # Add auc score to previous train results\n   train_results.append(roc_auc)\n   y_pred = dt.predict(X_test)\n   false_positive_rate, true_positive_rate, thresholds = roc_curve(Y_test, y_pred)\n   roc_auc = auc(false_positive_rate, true_positive_rate)\n   # Add auc score to previous test results\n   test_results.append(roc_auc)\nfrom matplotlib.legend_handler import HandlerLine2D\nline1, = plt.plot(max_depths, train_results, 'b', label='Train AUC')\nline2, = plt.plot(max_depths, test_results, 'r', label='Test AUC')\nplt.legend(handler_map={line1: HandlerLine2D(numpoints=2)})\nplt.ylabel('AUC score')\nplt.xlabel('Tree depth')\nplt.show()","03a67139":"min_samples_splits = np.linspace(0.1, 1.0, 10, endpoint=True)\ntrain_results = []\ntest_results = []\nfor min_samples_split in min_samples_splits:\n   dt = DecisionTreeClassifier(min_samples_split=min_samples_split)\n   dt.fit(X_train, Y_train)\n   train_pred = dt.predict(X_train)\n   false_positive_rate, true_positive_rate, thresholds =    roc_curve(Y_train, train_pred)\n   roc_auc = auc(false_positive_rate, true_positive_rate)\n   train_results.append(roc_auc)\n   y_pred = dt.predict(X_test)\n   false_positive_rate, true_positive_rate, thresholds = roc_curve(Y_test, y_pred)\n   roc_auc = auc(false_positive_rate, true_positive_rate)\n   test_results.append(roc_auc)\nfrom matplotlib.legend_handler import HandlerLine2D\nline1, = plt.plot(min_samples_splits, train_results, 'b', label='Train AUC')\nline2, = plt.plot(min_samples_splits, test_results, 'r', label='Test AUC')\nplt.legend(handler_map={line1: HandlerLine2D(numpoints=2)})\nplt.ylabel('AUC score')\nplt.xlabel('min samples split')\nplt.show()\n","622fd476":"min_samples_leafs = np.linspace(0.1, 0.5, 5, endpoint=True)\ntrain_results = []\ntest_results = []\nfor min_samples_leaf in min_samples_leafs:\n   dt = DecisionTreeClassifier(min_samples_leaf=min_samples_leaf)\n   dt.fit(X_train, Y_train)\n   train_pred = dt.predict(X_train)\n   false_positive_rate, true_positive_rate, thresholds = roc_curve(Y_train, train_pred)\n   roc_auc = auc(false_positive_rate, true_positive_rate)\n   train_results.append(roc_auc)\n   y_pred = dt.predict(X_test)\n   false_positive_rate, true_positive_rate, thresholds = roc_curve(Y_test, y_pred)\n   roc_auc = auc(false_positive_rate, true_positive_rate)\n   test_results.append(roc_auc)\nfrom matplotlib.legend_handler import HandlerLine2D\nline1, = plt.plot(min_samples_leafs, train_results, 'b', label='Train AUC')\nline2, = plt.plot(min_samples_leafs, test_results, 'r', label='Test AUC')\nplt.legend(handler_map={line1: HandlerLine2D(numpoints=2)})\nplt.ylabel('AUC score')\nplt.xlabel('min samples leaf')\nplt.show()","c2121cb7":"max_features = list(range(1,X_train.shape[1]))\ntrain_results = []\ntest_results = []\nfor max_feature in max_features:\n   dt = DecisionTreeClassifier(max_features=max_feature)\n   dt.fit(X_train, Y_train)\n   train_pred = dt.predict(X_train)\n   false_positive_rate, true_positive_rate, thresholds = roc_curve(Y_train, train_pred)\n   roc_auc = auc(false_positive_rate, true_positive_rate)\n   train_results.append(roc_auc)\n   y_pred = dt.predict(X_test)\n   false_positive_rate, true_positive_rate, thresholds = roc_curve(Y_test, y_pred)\n   roc_auc = auc(false_positive_rate, true_positive_rate)\n   test_results.append(roc_auc)\nfrom matplotlib.legend_handler import HandlerLine2D\nline1, = plt.plot(max_features, train_results, 'b', label='Train AUC')\nline2, = plt.plot(max_features, test_results, 'r', label='Test AUC')\nplt.legend(handler_map={line1: HandlerLine2D(numpoints=2)})\nplt.ylabel('AUC score')\nplt.xlabel('max features')\nplt.show()","a20d8d7a":"max_features","98fd5075":"#Count mis-classified one\ncount_misclassified = (Y_test != y_pred).sum()\nprint('Misclassified samples: {}'.format(count_misclassified))","5e432dd9":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB as gnb\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, confusion_matrix\nfrom sklearn import tree\nfrom os import system","fbb79397":"from sklearn.ensemble import RandomForestClassifier\nDecisionTreeClassifier(class_weight=None, criterion='entropy', max_depth=20,\n            max_features=None, max_leaf_nodes=None,\n            min_impurity_split=1e-07, min_samples_leaf=6,\n            min_samples_split=1, min_weight_fraction_leaf=0.0,\n            presort=False, random_state=0, splitter='best')\nrfcl = RandomForestClassifier(n_estimators = 50)\nrfcl = rfcl.fit(X_train, Y_train)\ny_pred = rfcl.predict(X_test)\nrfcl.score(X_test , Y_test)","d46bb6ea":"from sklearn.ensemble import GradientBoostingClassifier\ngbcl = GradientBoostingClassifier(n_estimators = 50, learning_rate = 0.05)\ngbcl = gbcl.fit(X_train,Y_train)\ny_pred = gbcl.predict(X_test)\ngbcl.score(X_test , Y_test)","abbea190":"# Fitting classifier to the Training set\nfrom sklearn.ensemble import RandomForestClassifier\nclassifier = RandomForestClassifier(max_depth=5, random_state=0)\nclassifier.fit(X_train, Y_train)","4d64d96a":"from sklearn.ensemble import AdaBoostClassifier\nabcl = AdaBoostClassifier( n_estimators= 50)\nabcl = abcl.fit(X_train,Y_train)\ny_pred = abcl.predict(X_test)\nabcl.score(X_test , Y_test)","aba0581b":"print('Class 0', round(dataset['status'].value_counts()[0]\/len(df) * 100,2), '% of the dataset-----', round(dataset['status'].value_counts()[0]))\nprint('Class 1', round(dataset['status'].value_counts()[1]\/len(df) * 100,2), '% of the dataset-----', round(dataset['status'].value_counts()[1]))","a4aeeac3":"sns.countplot('status',data=dataset)\nplt.title('New Distributin Dataset')\nplt.show()","5e4bd4fa":"# Fitting classifier to the Training set\nfrom sklearn.svm import SVC\nclassifier = SVC(kernel = \"rbf\", random_state = 9)\nclassifier.fit(X_train, Y_train)","4b661e7b":"# Predicting the Test set results\ny_pred = classifier.predict(X_test)\n# Making the Confusion Matrix\nfrom sklearn.metrics import confusion_matrix\ncm = confusion_matrix(Y_test, y_pred)\ncm","8a59943f":"classifier.score(X_train, Y_train)","828f0331":"# Try different machine learning models and algorithms"}}