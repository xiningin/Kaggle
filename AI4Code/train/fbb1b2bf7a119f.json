{"cell_type":{"bd74f494":"code","f246d044":"code","bf91d1a7":"code","b9c4dc47":"code","95d97f18":"code","22d7cdc5":"code","573ded72":"code","4cb9991b":"code","5b7bb59d":"code","865e4a07":"code","5938182d":"code","ecdbd8fa":"code","6010b115":"code","82ae3839":"code","006f3c92":"code","60551a45":"code","8caec09d":"code","e7118754":"code","19a31a12":"code","b011bdbc":"markdown","3539947e":"markdown","b5a9bc79":"markdown","f832ff26":"markdown","372d1909":"markdown","0165e887":"markdown","7e47854f":"markdown","06756d95":"markdown","c1d0aa27":"markdown","a375c6fb":"markdown","bb9d39f9":"markdown","fe6b54ff":"markdown","79867759":"markdown","d8c17065":"markdown","1cc5de04":"markdown","774ec91c":"markdown","f47774a8":"markdown"},"source":{"bd74f494":"import numpy as np\nimport pandas as pd\nimport gc,random, os, sys, pickle\nfrom catboost import CatBoostRegressor\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import StratifiedKFold\n\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\nimport tensorflow as tf ; print(tf.__version__)\nfrom tensorflow import keras \nfrom tensorflow.keras import backend as K \n\ntf.config.optimizer.set_jit(True)\nphysical_devices = tf.config.list_physical_devices('GPU')\ntry: tf.config.experimental.set_memory_growth(physical_devices[0], True)\nexcept: pass \n\nsys.path.append('..\/input\/swintransformertf')\nfrom swintransformer import SwinTransformer","f246d044":"SwinTransformer?","bf91d1a7":"CFGS = {\n    'swin_tiny_224' : dict(input_size=(224, 224), window_size=7,  embed_dim=96,  \n                           depths=[2, 2, 6, 2],  num_heads=[3, 6, 12, 24]),\n    \n    'swin_small_224': dict(input_size=(224, 224), window_size=7,  embed_dim=96,  \n                           depths=[2, 2, 18, 2], num_heads=[3, 6, 12, 24]),\n    \n    'swin_base_224' : dict(input_size=(224, 224), window_size=7,  embed_dim=128, \n                           depths=[2, 2, 18, 2], num_heads=[4, 8, 16, 32]),\n    \n    'swin_base_384' : dict(input_size=(384, 384), window_size=12, embed_dim=128, \n                           depths=[2, 2, 18, 2], num_heads=[4, 8, 16, 32]),\n    \n    'swin_large_224': dict(input_size=(224, 224), window_size=7,  embed_dim=192, \n                           depths=[2, 2, 18, 2], num_heads=[6, 12, 24, 48]),\n    \n    'swin_large_384': dict(input_size=(384, 384), window_size=12, embed_dim=192, \n                           depths=[2, 2, 18, 2], num_heads=[6, 12, 24, 48])\n}","b9c4dc47":"# set 'TPU' for train + enable internet.\nDEVICE = 'GPU' \n\nif DEVICE == \"TPU\":\n    print(\"connecting to TPU...\")\n    try:\n        tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n        print('Running on TPU ', tpu.master())\n    except ValueError:\n        print(\"Could not connect to TPU\")\n        tpu = None\n\n    if tpu:\n        # ISSUE: PERFORMANCE PENALTY, BE AWARE!!!\n        tf.config.set_soft_device_placement(True) \n        \n        try:\n            print(\"initializing  TPU ...\")\n            tf.config.experimental_connect_to_cluster(tpu)\n            tf.tpu.experimental.initialize_tpu_system(tpu)\n            strategy = tf.distribute.experimental.TPUStrategy(tpu)\n            print(\"TPU initialized\")\n        except _:\n            print(\"failed to initialize TPU\")\n    else:\n        DEVICE = \"GPU\"\n\nif DEVICE != \"TPU\":\n    print(\"Using default strategy for CPU and single GPU\")\n    strategy = tf.distribute.get_strategy()\n\nif DEVICE == \"GPU\":\n    print(\"Num GPUs Available: \", \n          len(tf.config.experimental.list_physical_devices('GPU')))\n    \nAUTO     = tf.data.AUTOTUNE\nREPLICAS = strategy.num_replicas_in_sync\nprint(f'REPLICAS: {REPLICAS}')","95d97f18":"Q = 30\nIMG_SIZE   = 896\nBATCH_SIZE = 12\nCHANNELS   = 3\n\nEPOCHS = 30\nFOLDS  = 10\nSEED   = 987\n\nVERBOSE = 1\nLR      = 0.000005\n\nFEATURE_FOLDS = 10\n\nif DEVICE == \"TPU\":\n    from kaggle_datasets import KaggleDatasets\n    DATA_DIR = '\/kaggle\/input\/petfinder-pawpularity-score\/'\n    GCS_PATH  = KaggleDatasets().get_gcs_path('petfinder-pawpularity-score')\n    TRAIN_DIR = GCS_PATH + '\/train\/'\n    TEST_DIR  = GCS_PATH + '\/test\/'\n    FEATURE_TRAIN = True \nelse:\n    DATA_DIR  = '\/kaggle\/input\/petfinder-pawpularity-score\/'\n    TRAIN_DIR = DATA_DIR + 'train\/'\n    TEST_DIR  = DATA_DIR + 'test\/'\n    FEATURE_TRAIN = False","22d7cdc5":"def seed_all(s):\n    random.seed(s)\n    np.random.seed(s)\n    tf.random.set_seed(s)\n    os.environ['TF_CUDNN_DETERMINISTIC'] = '1'\n    os.environ['PYTHONHASHSEED'] = str(s) \n    \nseed_all(SEED)\n\n# Load Train Data\ntrain_df = pd.read_csv(f'{DATA_DIR}train.csv')\ntrain_df['Id'] = train_df['Id'].apply(lambda x: f'{TRAIN_DIR}{x}.jpg')\n\n# Set a specific label to be able to perform stratification\n# train_df.Pawpularity.max(), train_df.Pawpularity.min() <- (100, 1)\ntrain_df['stratify_label'] = pd.qcut(train_df['Pawpularity'], q = Q, labels = range(Q))\n\n# Label value to be used for feature model 'classification' training.\ntrain_df['target_value'] = train_df['Pawpularity'] \/ 100. # normalizing \n\n# Summary\nprint(f'train_df: {train_df.shape}')\ntrain_df.head()","573ded72":"# Load Test Data\ntest_df = pd.read_csv(f'{DATA_DIR}test.csv')\ntest_df['Id'] = test_df['Id'].apply(lambda x: f'{TEST_DIR}{x}.jpg')\ntest_df['Pawpularity'] = 0\n\n# Summary\nprint(f'test_df: {test_df.shape}')\ntest_df.head()","4cb9991b":"# SetAutoTune\nAUTOTUNE = tf.data.AUTOTUNE  \n\ndef build_augmenter(is_labelled):\n    def augment(img):\n        img = tf.image.random_flip_left_right(img)\n        img = tf.image.random_flip_up_down(img)\n        img = tf.image.random_saturation(img, 0.95, 1.05)\n        img = tf.image.random_brightness(img, 0.05)\n        img = tf.image.random_contrast(img, 0.95, 1.05)\n        img = tf.image.random_hue(img, 0.05)\n        return img\n    def augment_with_labels(img, label):\n        return augment(img), label\n    return augment_with_labels if is_labelled else augment\n\ndef build_decoder(is_labelled):\n    def decode(path):\n        file_bytes = tf.io.read_file(path)\n        img = tf.image.decode_jpeg(file_bytes, channels = CHANNELS)\n        img = tf.image.resize(img, (IMG_SIZE, IMG_SIZE))\n        return img\n    def decode_with_labels(path, label):\n        return decode(path), label\n    return decode_with_labels if is_labelled else decode\n\ndef create_dataset(df, \n                   batch_size     = 32, \n                   is_labelled    = False, \n                   augment        = False, \n                   repeat         = False, \n                   shuffle        = False):\n    decode_fn    = build_decoder(is_labelled)\n    augmenter_fn = build_augmenter(is_labelled)\n    \n    # Create Dataset\n    if is_labelled:\n        dataset = tf.data.Dataset.from_tensor_slices((df['Id'].values, \n                                                      df['target_value'].values))\n    else:\n        dataset = tf.data.Dataset.from_tensor_slices((df['Id'].values))\n    dataset = dataset.map(decode_fn, num_parallel_calls=AUTOTUNE)\n    dataset = dataset.map(augmenter_fn, num_parallel_calls=AUTOTUNE) if augment else dataset\n    dataset = dataset.repeat() if repeat else dataset\n    dataset = dataset.shuffle(1024, reshuffle_each_iteration=True) if shuffle else dataset\n    dataset = dataset.batch(batch_size*REPLICAS, drop_remainder=FEATURE_TRAIN)\n    dataset = dataset.prefetch(AUTOTUNE)\n    return dataset","5b7bb59d":"from tensorflow.keras import applications\n\ncheck_base = applications.EfficientNetB0(\n    include_top=False,\n    weights=None,\n    input_tensor=keras.Input((896, 896, 3))\n    )\n\nfor l in check_base.layers:\n    print(l.name, l.output_shape)\n    break\n    \nblock2b_add = (None, 224, 224, 24)","865e4a07":"EfficientNetWeightsPath = '..\/input\/efficientnet-keras-noisystudent-weights-b0b7\/'\n\nif DEVICE == \"TPU\": # TRUE for Training \n    efnet_wg = f\"{EfficientNetWeightsPath}noisystudent\/noisy.student.notop-b0.h5\"\n    swins_wg = True\n    use_tpu  = True\n    \nelse: # TRUE for Inference \n    efnet_wg = None\n    swins_wg = False\n    use_tpu  = False","5938182d":"class EfficientHybridSwinTransformer(keras.Model):\n    def __init__(self):\n        super(EfficientHybridSwinTransformer, self).__init__()\n        # base models \n        self.inputx = keras.Input((IMG_SIZE, IMG_SIZE, CHANNELS), name='input_hybrids')\n        base = applications.EfficientNetB0(\n            include_top  = False,\n            weights      = efnet_wg,\n            input_tensor = self.inputx\n        )\n        for layer in base.layers:\n            if not isinstance(layer, tf.keras.layers.BatchNormalization):\n                layer.trainable = True\n            else:\n                layer.trainable = False\n\n        # base model with compatible output which will be an input of transformer model \n        self.new_base = keras.Model(\n            [base.inputs], \n            [base.get_layer('block2b_add').output, base.output], # output with 224 feat_maps\n            name='efficientnet'\n        )\n        self.conv = keras.layers.Conv2D(3, 3, padding='same')\n        self.swin_blocks = SwinTransformer('swin_large_224', \n                                           num_classes=1, \n                                           include_top=False, \n                                           pretrained=swins_wg, \n                                           use_tpu=use_tpu, \n                                           cfgs=CFGS)\n        self.fc = keras.layers.Dense(1, activation = 'sigmoid')\n\n    # calling \n    def call(self, inputs, training=None, **kwargs):\n        base_x, base_y = self.new_base(inputs)\n        from_swin = self.swin_blocks(self.conv(base_x))\n        cating = tf.concat([from_swin, \n                            keras.layers.GlobalAveragePooling2D()(base_y)], axis=-1)\n        if training:\n            return self.fc(cating)\n        else:\n            return self.fc(cating), cating # need cating o\/p in inference time. \n\n    def build_graph(self):\n        x = keras.Input(shape=(IMG_SIZE, IMG_SIZE, CHANNELS))\n        return keras.Model(inputs=[x], outputs=self.call(x))","ecdbd8fa":"def get_model(plot_model, print_summary, with_compile):\n    model = EfficientHybridSwinTransformer()\n\n    if plot_model:\n        print(model(tf.ones((1, IMG_SIZE, IMG_SIZE, CHANNELS)))[0].shape)\n        display(keras.utils.plot_model(model.build_graph(), \n                                          show_shapes=True, \n                                          show_layer_names=True,\n                                          expand_nested=False))\n    if print_summary:\n        print(model(tf.ones((1, IMG_SIZE, IMG_SIZE, CHANNELS)))[0].shape)\n        print(model.summary())\n        \n    if with_compile:\n        print(model(tf.ones((1, IMG_SIZE, IMG_SIZE, CHANNELS)))[0].shape)\n        model.compile(\n            optimizer = keras.optimizers.Adam(learning_rate = LR), \n            loss = keras.losses.BinaryCrossentropy(), \n            metrics = [keras.metrics.RootMeanSquaredError('rmse')])  \n        \n    return model \n\nmodel = get_model(plot_model    = True,\n                  print_summary = True, \n                  with_compile  = False)","6010b115":"def get_lr_callback(batch_size=8):\n    lr_start   = 0.000005\n    lr_max     = 0.00000125 * batch_size * REPLICAS\n    lr_min     = 0.000001\n    lr_ramp_ep = 5\n    lr_sus_ep  = 0\n    lr_decay   = 0.8\n    def lrfn(epoch):\n        if epoch < lr_ramp_ep:\n            lr = (lr_max - lr_start) \/ lr_ramp_ep * epoch + lr_start\n        elif epoch < lr_ramp_ep + lr_sus_ep:\n            lr = lr_max\n        else:\n            lr = (lr_max - lr_min) * lr_decay**(epoch - lr_ramp_ep - lr_sus_ep) + lr_min\n        return lr\n    return keras.callbacks.LearningRateScheduler(lrfn, verbose=True)\n\n\n# Set Callbacks\ndef model_checkpoint(fold):\n    return keras.callbacks.ModelCheckpoint(f'feature_models_{fold}.h5',\n                                           verbose = 1, \n                                           monitor = 'val_rmse', \n                                           mode    = 'min', \n                                           save_weights_only = True,\n                                           save_best_only    = True)","82ae3839":"if FEATURE_TRAIN: # TRUE if DEVICE == TPU \n    # OOF RMSE Placeholder\n    all_val_rmse = []\n\n    # Stratified Training\n    kfold = StratifiedKFold(n_splits = FEATURE_FOLDS, \n                            shuffle = True, random_state = SEED)\n\n    for fold, (train_index, val_index) in enumerate(kfold.split(train_df.index,\n                                                                train_df['stratify_label'])):\n        if fold == 0: # Train on fold sequentially. 0,1,2,3,4,....FEATURE_FOLDS\n            print(f'\\nFold {fold}\\n')\n            # Pre model.fit cleanup\n            tf.keras.backend.clear_session()\n            gc.collect()\n\n            # Create Model\n            with strategy.scope():\n                model = get_model(plot_model=False, print_summary=False, with_compile=True)\n\n            # Create TF Datasets\n            trn = train_df.iloc[train_index]\n            val = train_df.iloc[val_index]\n            training_dataset = create_dataset(trn, \n                                              batch_size  = BATCH_SIZE, \n                                              is_labelled = True, \n                                              augment     = True, \n                                              repeat      = True, \n                                              shuffle     = True)\n            validation_dataset = create_dataset(val, \n                                                batch_size  = BATCH_SIZE, \n                                                is_labelled = True,\n                                                augment     = False, \n                                                repeat      = True, \n                                                shuffle     = False)\n\n            # Fit Model\n            history = model.fit(training_dataset,\n                                epochs = EPOCHS,\n                                steps_per_epoch  = trn.shape[0] \/ BATCH_SIZE \/\/ REPLICAS,\n                                validation_steps = val.shape[0] \/ BATCH_SIZE \/\/ REPLICAS,\n                                callbacks = [model_checkpoint(fold), get_lr_callback(BATCH_SIZE)],\n                                validation_data = validation_dataset,\n                                verbose = 1)   \n\n            # Validation Information\n            best_val_rmse = min(history.history['val_rmse'])\n            all_val_rmse.append(best_val_rmse)\n            print(f'\\nValidation RMSE: {best_val_rmse}\\n')\n\n    # Summary\n    print(f'Final Mean RMSE for {FEATURE_FOLDS} Fold CV Training: {np.mean(all_val_rmse)}')","006f3c92":"# Placeholders\npreds_final   = np.zeros((test_df.shape[0], 1))\nall_oof_score = []\n\n# Stratification and Label values\nY_strat = train_df['stratify_label'].values\nY_pawpularity = train_df['Pawpularity'].values","60551a45":"# CV: 0.1880158931016922\nmodel_a = get_model(plot_model=False,  print_summary=False,  with_compile=True)\nmodel_a.load_weights('..\/input\/pet-test-wg\/hyb_eff_attn_wg\/feature_models_0.h5', by_name=True)\n\n# CV: 0.1864963173866272\nmodel_b = get_model(plot_model=False,  print_summary=False,  with_compile=True)\nmodel_b.load_weights('..\/input\/pet-test-wg\/hyb_eff_attn_wg\/feature_models_1.h5', by_name=True)\n\n# CV: 0.17805276811122894\nmodel_c = get_model(plot_model=False,  print_summary=False,  with_compile=True)\nmodel_c.load_weights('..\/input\/pet-test-wg\/hyb_eff_attn_wg\/feature_models_2.h5', by_name=True)\n\n# CV: 0.1805611252784729\nmodel_d = get_model(plot_model=False,  print_summary=False,  with_compile=True)\nmodel_d.load_weights('..\/input\/pet-test-wg\/hyb_eff_attn_wg\/feature_models_3.h5', by_name=True)\n\n# CV: 0.18030136823654175\nmodel_e = get_model(plot_model=False,  print_summary=False,  with_compile=True)\nmodel_e.load_weights('..\/input\/pet-test-wg\/hyb_eff_attn_wg\/feature_models_4.h5', by_name=True)\n\n# CV: 0.18239009380340576\nmodel_f = get_model(plot_model=False,  print_summary=False,  with_compile=True)\nmodel_f.load_weights('..\/input\/pet-test-wg\/hyb_eff_attn_wg\/feature_models_5.h5', by_name=True)\n\n# CV: 0.17896604537963867\nmodel_g = get_model(plot_model=False,  print_summary=False,  with_compile=True)\nmodel_g.load_weights('..\/input\/pet-test-wg\/hyb_eff_attn_wg\/feature_models_6.h5', by_name=True)\n\n# CV: 0.1817626655101776\nmodel_h = get_model(plot_model=False,  print_summary=False,  with_compile=True)\nmodel_h.load_weights('..\/input\/pet-test-wg\/hyb_eff_attn_wg\/feature_models_7.h5', by_name=True)\n\n# CV: 0.18311156332492828\nmodel_i = get_model(plot_model=False,  print_summary=False,  with_compile=True)\nmodel_i.load_weights('..\/input\/pet-test-wg\/hyb_eff_attn_wg\/feature_models_8.h5', by_name=True)\n\n# CV: 0.17778289318084717\nmodel_j = get_model(plot_model=False,  print_summary=False,  with_compile=True)\nmodel_j.load_weights('..\/input\/pet-test-wg\/hyb_eff_attn_wg\/feature_models_9.h5', by_name=True)\n\n# ALL Hybrid-Swins \nmodels = [model_a, model_b, model_c, model_d, model_e, \n          model_f, model_g, model_h, model_i, model_j]\nweights = [model.get_weights() for model in models]\n\n# Mnaully Averaging \nnew_weights = list()\nfor weights_list_tuple in zip(*weights): \n    if weights_list_tuple[0].any() != 0:\n        new_weights.append(\n            np.array([np.array(w).mean(axis=0) for w in zip(*weights_list_tuple)])\n        )\n    else:\n        new_weights.append(np.array(weights_list_tuple).mean(axis=0))\n        \n# Set the New Averaged Weights\nmodel = get_model(plot_model=False,  print_summary=False,  with_compile=True)\nmodel.set_weights(new_weights)\nmodel.save_weights('merged_hybrid_10folds.h5')","8caec09d":"# Pre model.fit cleanup\nK.clear_session()\ngc.collect()\n\n# Feature Extraction\nprint('\\nExtracting Features')\ncb_train_set = create_dataset(train_df, \n                              batch_size  = BATCH_SIZE, \n                              is_labelled = True, \n                              augment     = False,\n                              repeat      = False, \n                              shuffle     = False)\ncb_test_set = create_dataset(test_df, \n                             batch_size  = BATCH_SIZE,\n                             is_labelled = False, \n                             augment     = False, \n                             repeat      = False, \n                             shuffle     = False)\n\ncb_train_features = model.predict(cb_train_set, verbose = VERBOSE)[1]\ncb_test_features  = model.predict(cb_test_set, verbose = VERBOSE)[1]\n\n# Feature Set Shapes\nprint(f'Train Feature Set Shape: {cb_train_features.shape}')\nprint(f'Test Feature Set Shape : {cb_test_features.shape}')","e7118754":"CAT_BOOST_TRAIN = False\nLOAD_CAT_FROM_PATH = '..\/input\/pet-test-wg\/catboost\/'\n\n# Placeholders\noof_score = []\n\nkfold = StratifiedKFold(n_splits = FOLDS, shuffle  = True, random_state = SEED)\n\nfor idx, (train, val) in enumerate(kfold.split(cb_train_features, Y_strat)):\n    print(f'\\n----- CatBoost Fold {idx} -----')\n    \n    train_x, train_y = cb_train_features[train], Y_pawpularity[train]\n    val_x, val_y     = cb_train_features[val], Y_pawpularity[val]\n\n    # Set CatBoost Parameters\n    cb_params = {'loss_function'   : 'RMSE',\n                 'eval_metric'     : 'RMSE',\n                 'iterations'      :  1000,\n                 'grow_policy'     : 'SymmetricTree',\n                 'depth'           :   6,\n                 'l2_leaf_reg'     : 2.0,\n                 'random_strength' : 1.0,\n                 'learning_rate'   : 0.05,\n                 'task_type'       : 'CPU',\n                 'devices'         : '0',\n                 'verbose'         :  0,\n                 'random_state'    : SEED}\n\n    name = f\"catboost_fold_{idx}.pkl\"\n    \n    if CAT_BOOST_TRAIN:\n        # Create and Fit CatBoost Model\n        cb_model = CatBoostRegressor(**cb_params)\n        cb_model.fit(train_x, train_y, \n                     eval_set = [(val_x, val_y)],\n                     early_stopping_rounds = 100, \n                     verbose = 250)\n        pickle.dump(cb_model, open(name, \"wb\"))\n    else:\n        print('Loading Catboost...',LOAD_CAT_FROM_PATH+name)\n        cb_model = pickle.load(open(LOAD_CAT_FROM_PATH+name, \"rb\"))\n\n    # pred on test \n    preds_final += np.array([cb_model.predict(cb_test_features)]).T\n    \n    # Update OOF Score\n    y_pred = cb_model.predict(val_x)\n    oof_score.append(np.sqrt(mean_squared_error(val_y, y_pred)))       \n\n    # Cleanup\n    del cb_model, y_pred\n    del train_x, train_y\n    del val_x, val_y\n    gc.collect()   \n\n    # OOF Score for CatBoost run\n    print(f'CatBoost OOF Score: {oof_score[-1]}')\n    # Increase to improve randomness on the next feature model run\n    SEED += 1\n    \n# Final OOF score for All Feature Models\nprint(f'\\n\\nFinal OOF RMSE Score for all feature models: {np.mean(oof_score)}')","19a31a12":"preds_final \/= FOLDS\nsubmission_df = pd.read_csv(f'{DATA_DIR}sample_submission.csv')\nsubmission_df['Pawpularity'] = preds_final.ravel()\nsubmission_df.to_csv('submission.csv', index = False)\n\n# Summary\nsubmission_df.head(10)","b011bdbc":"# Train: Feature Extractor : Hybrid Swin Transformer","3539947e":"# TF Dataset","b5a9bc79":"# CatBoost 10 Fold CV Training + Ensemble Inference\n\nHere, we use `CatBoost` but we can also try [RAPIDS SVR](https:\/\/www.kaggle.com\/cdeotte\/rapids-svr-boost-17-8). However, here is the workflow:\n\n- We take Image Model embeddings and use it to `CatBoost` model.\n- Again, we trained `10` fold of the Image Model, so that gives `10` weights. So next, we will average these Image Models.\n- After that, we will do another `10` fold for the `CatBost` mdoel. \n\nIf you're confuse somehow about **why swin transformer** and **why now catboost model**, then please see the below diagram for clarification. In the first stage we used hybrid swin transformer and trained this model as classification model. And after training, we want to extract the feature of the image by this trained model (from stage 1) and use these features as a input for next stage training. In the next stage training, we use catboost model and will train it with regression loss function (RMSE). \n\n![Presentation1](https:\/\/user-images.githubusercontent.com\/17668390\/149220379-e91edde9-3d71-4d11-bb09-4679de292c67.png)","f832ff26":"**Checking**","372d1909":"# Hybrid EfficientNet Swin Transformer \n\nThis notebook shows the implementation of **Hybrid EfficientNet Swin Transformer (HENetSwinT)** model, with the training script for **TPU** and also inference code for **GPU**. Such approach (**HENetSwinT**) was used in [Google Landmark Recognition 2021](https:\/\/www.kaggle.com\/c\/landmark-recognition-2021) competition by [Dieter](https:\/\/www.kaggle.com\/christofhenkel) originally written in PyTorch, solution explained [here](https:\/\/www.kaggle.com\/c\/landmark-recognition-2021\/discussion\/277098) in great details. In this notebook, we'll try to accomplish it in `tf.keras`. The whole hybrid architecture (**HENetSwinT**) looks something like as follows:\n\n![](https:\/\/i.imgur.com\/2iXNuBA.png)\n\n<div align=\"center\">\n  Model Architecture of Hybrid EfficientNet Swin Transformer (HENetSwinT)\n<\/div>","0165e887":"## Note\n\nAs we already train the hybrid model in **TPU**, that's why here we skip the training part on tpu. That's why we set, `FEATURE_TRAIN == FALSE`.","7e47854f":"# Create Submission Files","06756d95":"# Efficient Hybrid Swin-Transformer ","c1d0aa27":"---\n\n**DISCLAIMER**\n\n- The implementation of swin-transformer in `tf.keras` is mostly borrowed from [rishigami\/Swin-Transformer-TF ](https:\/\/github.com\/rishigami\/Swin-Transformer-TF).\n- For general training pipelines for next stage training (catbost model), we'll be using [ROBIN SMITS](https:\/\/www.kaggle.com\/rsmits\/effnet-b2-feature-models-catboost)'s great kernel. ","a375c6fb":"# Additional Resources\n1. **How to use it on my own dataset?**\n    - First, understand the competition task and its data format. And try to relate with yours. \n    - Second, run this notebook successfully on the competition data. \n    - Lastly, replace the dataset with yours. \n2. **More Code Exampels.** \n    - [Hybrid External MHA Transformer + RAPIS SVR](https:\/\/www.kaggle.com\/ipythonx\/hybrid-external-mha-transformer-rapis-svr) - [Discussion](https:\/\/www.kaggle.com\/c\/petfinder-pawpularity-score\/discussion\/281917). \n    - [[TF.Keras]:Learning to Resize Images for ViT Model](https:\/\/www.kaggle.com\/ipythonx\/tf-keras-learning-to-resize-images-for-vit-model) - [Discussion](https:\/\/www.kaggle.com\/c\/petfinder-pawpularity-score\/discussion\/280438).\n    - [Discussion: DOLG Models in TensorFlow 2 (Keras) Implementation](https:\/\/www.kaggle.com\/c\/petfinder-pawpularity-score\/discussion\/281914) - [TF.Keras Code](https:\/\/github.com\/innat\/DOLG-TensorFlow)\n    - [TF: Hybrid EfficientNet Swin-Transformer : GradCAM](https:\/\/www.kaggle.com\/ipythonx\/tf-hybrid-efficientnet-swin-transformer-gradcam)","bb9d39f9":"# Modeling\n![](https:\/\/i.imgur.com\/2iXNuBA.png)\n\n\n### Change-Logs\n\n- **No Subcenter Arcface Head**\n- **EfficientNet B0 + Swin-Transformers**\n- **Modification: Please see the `Attention` note at the beginning.**","fe6b54ff":"**Get Embeddings from Image Model** \n\n- In previous section, we took average of the `10` image models.\n- Next, we will use that **average weight** to get embeddings of the image files.\n- Next, fit them on `CatBoost` Model.\n","79867759":"**TPU \/ GPU Config**","d8c17065":"## Abstraction \n\n- In the given [cat-dog pet dataset](https:\/\/www.kaggle.com\/c\/petfinder-pawpularity-score\/data), both image files and also meta informations are provided which can be used in training. The overall task is to analyze this raw images and metadata to predict the **Pawpularity** of pet photos. The target labels are given as a continuous quantity. The leaderboard score will be evaluated by **Root Mean Squared Error (RMSE)**, ([details](https:\/\/www.kaggle.com\/c\/petfinder-pawpularity-score\/overview\/evaluation)). So, we can frame this task as a regression category.\n\n- Though it's regression problem, it's shared by the community that it might be better to treat such regression problem as a classification problem. It's done by normalizing the regression labels within `0-1` range and use cross-entropy as a loss function. In this code example, we will do the same. More details are [here](https:\/\/www.kaggle.com\/c\/petfinder-pawpularity-score\/discussion\/275094).\n\n- For the modeling part, we first train a vision model on the **N** fold of the given samples. After training with the all folds, we'll take average of them and extract the features (embeddings) from the image files and further use it to train [CatBoost](https:\/\/catboost.ai\/) model with another **N** folds. For the vision mdoel, we'll be using the **Hybrid-Swin Transformer** as described above. \n\n- The whole modeling approach is for **Experimentational** purposes. We can do different type of modeling approach in the same setup, dicussed in the another code examples, [Hybrid External MHA Transformer + RAPIS SVR](https:\/\/www.kaggle.com\/ipythonx\/hybrid-external-mha-transformer-rapis-svr) - [Discussion](https:\/\/www.kaggle.com\/c\/petfinder-pawpularity-score\/discussion\/286973). In summary, we can take both deep-network model's prediction and catboost model's prediction and then take average of them. \n\n- Also, here we're not using the given structural data yet. But of course, one can build a multi-input vision model that take image data and structure data; that is shown in this [notebook](https:\/\/www.kaggle.com\/ipythonx\/hybrid-external-mha-transformer-rapis-svr). For modeling with structure data or meta information, we can use one of the following models: \n    - [TF-Decision-Forests](https:\/\/www.tensorflow.org\/decision_forests) - [Discussion and Starter](https:\/\/www.kaggle.com\/c\/petfinder-pawpularity-score\/discussion\/275182). \n    - [Behavior Sequence Transformer for Structured Data](https:\/\/arxiv.org\/pdf\/1905.06874.pdf) - [Discussion](https:\/\/www.kaggle.com\/c\/petfinder-pawpularity-score\/discussion\/287421) - [Starter](https:\/\/keras.io\/examples\/structured_data\/movielens_recommendations_transformers\/).","1cc5de04":"**Image Model's Weights Ensemble**\n\n- 10 Image Model Weights for 10  FOLDS Training.\n- Take Average. \n- Use the Averaged Weights.","774ec91c":"# Next Step : CatBoost Modeling\n\n- In previous section, we first average all `10` model weights. \n- Next, we used that averaged weights and compute image embeddings in the previous cell.\n- Now, we can use this image embeddings to train `CatBoost` (or `SVR` if we want). \n- In the next cell, we can set `CAT_BOOST_TRAIN` to `True` to start training, Or,\n- We can simply reload the trained cat-boost models. ","f47774a8":"<div class = \"alert alert-block alert-info\">\n    <h3><font color = \"red\">ATTENTION<\/font><\/h3>\n<ul>\n  <li>This notebook is configured to train the <strong>swin-transformer<\/strong> mdoel on <strong>TPU<\/strong>. But we already did train on TPU and saved the weights files (..\/input\/pet-test-wg).\n  <li>So, this notebook uses the trained embeddings of the <strong>swin-transformer<\/strong> and uses for next stage training with <strong>CatBoost<\/strong> model followed by inference.<\/li>\n  <li>Additionally, from the above diagram, we made few changes in the architecture. We've used image size <code>896<\/code> to train the <strong>EfficientNetB0<\/strong> and use layer <code>blcok2b_add: (None, 224, 224, 24)<\/code> embeddings for <strong>swin-transformer<\/strong>. The final output of <strong>EfficientNet<\/strong> also gets concated with the <strong>swin-transformer's<\/strong> output. <\/li>\n    <li>Reference: https:\/\/github.com\/innat\/EfficientNet-Hybrid-Swin-Transformer<\/li>\n<\/ul>\n<\/div>\n\nSo, the overal model architectue in this notebook would be:\n\n![download](https:\/\/user-images.githubusercontent.com\/17668390\/139668055-2d19ea63-25cc-4665-b711-749c54b97a40.png)\n"}}