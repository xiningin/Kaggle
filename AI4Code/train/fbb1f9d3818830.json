{"cell_type":{"808b9930":"code","9aece5f2":"code","ea280ec9":"code","9ed3b08a":"code","63b497c6":"code","37613844":"code","f9b5f918":"code","d6310b6b":"markdown","c6c4a8c0":"markdown","c3c8e4fe":"markdown","f70ba429":"markdown","e107030e":"markdown","7ec3509c":"markdown","6c550a0b":"markdown","d96e70ed":"markdown","02ce4116":"markdown","b106a750":"markdown","281293cc":"markdown"},"source":{"808b9930":"# Vector handling\nimport numpy as np \n\n# Plotting Images & overlaying clustering results\nimport matplotlib.pyplot as plt \nimport seaborn as sns \nfrom pylab import rcParams \nfrom matplotlib import colors\n\n# CNN pretrained model & inference\nimport tensorflow as tf\nfrom tensorflow.keras.applications import MobileNet\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.applications.mobilenet import preprocess_input as mob_preprocess\n\n# Performing the task of clustering\nfrom sklearn.cluster import KMeans\n\n# Path & directory setup\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","9aece5f2":"home_dir = \"\/kaggle\/input\/\"\ntest_images_dir = home_dir + \"k-means-feature-clustering\/\"\nprint(\"Images we have for the test : \",os.listdir(test_images_dir))","ea280ec9":"plt.figure(figsize=(25,25))\nfor cnt,i in enumerate(os.listdir(test_images_dir)):\n    tmp = tf.io.read_file(test_images_dir + i)\n    tmp = tf.image.decode_jpeg(tmp, channels = 3) \n    tmp = tf.image.resize(tmp, [224, 224])\n    \n    ax = plt.subplot(1, 5, cnt+1)\n    plt.imshow(tmp.numpy().astype('uint8'), interpolation = 'none')","9ed3b08a":"mob_model = MobileNet(weights = \"imagenet\", include_top = False, input_shape = (224,224,3), pooling = \"max\")\n# Can add whatever layer you like\nlayer_name_list = ['conv_pw_2_relu']","63b497c6":"def get_cmap(n, name='BrBG', return_cmap=True):\n\n    '''\n    Returns a function that maps each index in 0, 1, ..., n-1 to a distinct color\n    Args :\n     n : (int) integers to map\n     name : (str) the keyword argument name must be a standard mpl colormap name.\n     return_cmap : (bool) return function or not\n    Returns :\n     function or color attribute\n    '''\n\n    cmap = plt.cm.get_cmap(name, n)\n    if return_cmap:\n        return cmap \n    else:\n        return cmap.colors","37613844":"def kmeans_feature_visualize(img_path,\n                             model,\n                             op_layer_list,\n                             num_clusters,\n                             pre_process_func,\n                             IMG_H = 224,\n                             IMG_W = 224):\n    \"\"\"\n    Main function that takes an image path as input, returns the features clustering for respective layers\n    Args:\n        img_path (str) : path of image input into the model\n        model (tf.model) : base model used for inference\n        op_layer_list (list) : name of output layers whose feature learning we want to visualise\n        num_clusters (int) : number of clusters for K-Means i.e. number of groups we are ready to see\n        pre_process_func (func) : function associated with the base model to preprocess the input according to its architecture & training procedure\n        IMG_H & IMG_W (int) : dimensions of the input image into the base model\n\n    Returns:\n        none\n    \"\"\"\n\n    # Value holders\n    num_blocks = len(op_layer_list)\n    final_list_of_color_maps = np.zeros(shape = (num_blocks, IMG_H, IMG_W))\n\n    # Extracting paths for intermediate activation map visualization\n    layer_list = [model.get_layer(i).output for i in op_layer_list]\n    layer_list.append(model.output)\n\n    # Model with multiple outputs i.e. graph has outputs returning intermediate activations\n    intermediate_op_model = Model(inputs = model.input,  outputs = layer_list)\n  \n\n    # Image fetch \n    tmp = tf.io.read_file(img_path)\n    tmp = tf.image.decode_png(tmp, channels = 3) \n    unproc_img = tf.image.resize(tmp, [IMG_H, IMG_W]) \n    proc_img = pre_process_func(unproc_img)\n\n    # Model inference\n    output_list = intermediate_op_model.predict(np.expand_dims(proc_img, axis = 0))\n\n    # K-Means clustering calculation according to the feature maps\n    for i in range(num_blocks):\n\n        # Reshaping activation maps from (h,w,c) -> (h*w,c)\n        # Each channel is nothing but a feature & each pixel location on activation map is a data point\n        tmp_op = np.squeeze(output_list[i])\n        vectors_list = tmp_op.reshape(-1,tmp_op.shape[-1])\n\n        # Fitting K-Means to find the centers\n        km = KMeans(n_clusters = num_clusters, \n                    init = 'random',\n                    n_init = 10,\n                    max_iter = 300, \n                    tol = 1e-04, \n                    random_state = 0)\n        y_km = km.fit_predict(vectors_list)\n    \n        # To ovelay downsampled activation map to the original image\n        color_map = np.zeros(shape=(IMG_H,IMG_W,1))\n        factor = IMG_H\/\/tmp_op.shape[0]\n\n        # Iterating pixel location by location to get the cluster to which a location belongs to\n        for row in range(tmp_op.shape[0]):\n            for col in range(tmp_op.shape[1]):\n                # Getting the cluster to which a data point i.e. a pixel location vector belongs\n                classVal = km.predict(tmp_op[row,col,:].reshape(1,-1))\n                tmp = [[classVal]*factor]*factor\n                color_map[row*factor:(row+1)*factor,col*factor:(col+1)*factor] = tmp\n    \n            # Collecting final color maps block by block\n            final_list_of_color_maps[i,:,:] = np.squeeze(color_map)\n\n    plt.figure(figsize=(10, 10))\n    for i in range(num_blocks):\n        ax = plt.subplot(1, num_blocks+1, i+1)\n        plt.imshow(unproc_img.numpy().astype('uint8'), interpolation = 'none')\n        plt.imshow(final_list_of_color_maps[i], interpolation='nearest', cmap = get_cmap(num_clusters,name = 'inferno'),alpha = 0.6)\n        ax.set_title(op_layer_list[i])\n    \n    ax = plt.subplot(1, num_blocks+1, num_blocks+1)\n    plt.imshow(unproc_img.numpy().astype('uint8'), interpolation = 'none')\n    ax.set_title(\"Original Image for reference\")\n    \n    plt.show()","f9b5f918":"for f in os.listdir(test_images_dir):\n    kmeans_feature_visualize(img_path = test_images_dir + f,\n                             model = mob_model,\n                             op_layer_list =layer_name_list,\n                             num_clusters = 2,\n                             pre_process_func = mob_preprocess)","d6310b6b":"# Main function\n\nAll the operations are explained in the comments","c6c4a8c0":"# Basic Imports","c3c8e4fe":"# Results\n\nAs we know that a CNN model performs heirarchial feature extraction i.e. the earlier layers extract local features & the deeper layers use those local features to extract abstract concepts in a way that helps the model to nail the downstream task. \n\nSince we make use of conv_pw_2_relu layer therefore I expected the model to extract some fairly low level features. For easy deciphering of results, I thought to keep number of clusters to 2.\n\n1. In the first example the MobilNet was able to differentiate between Sherlock's suit & hand. Since it had only 2 clusters the model felt keeping suit & background as the 2 most seperate clusters. (mainly I feel pixel intensity i.e. dark (suit, hair, flowers in background etc.) & light (Cumberbatch's face, yellow walls, the glass etc.)).\n\n2. It is a tricky picture but the again the model decided to use pixel intensity as the differentiating factor i.e dark (suit, the floral background on Mrs Hudson's wall & the dark regions in skull) & light(shirt, face, yellow part of the wall, white part of skull etc.).\n\n3. Again the same thing : light ( \"I am sherlocked\", faces) & dark (sherlock's back).\n\n4. This is the result I was looking for, the model was able to differentiate between the background vs Sherlock & Moriarity.\n\n5. Again clothes were differentiated from lighter background.\n\n\n*Pixel Intensity quite an important feature for MobileNetV1's earlier layer*","f70ba429":"# Deep Feature K-Means\n\nThis notebook is a tutorial for using K-Means clustering to visualize the quality of features\/representations learned by a CNN model.","e107030e":"Recently I was going throught the paper : [Intriguing Properties of Contrastive Losses](https:\/\/arxiv.org\/pdf\/2011.02803.pdf) published by the researchers at Google Brain. The paper was mainly a follow up for the **self supervised learning (SSL)** technique [SimCLRv1](http:\/\/arxiv.org\/abs\/2002.05709), explaining why the contrastive loss based pretext task of augmentation & random cropping worked well. \n\n\nThe authors of \"Intriguing Properties of Contrastive Losses\" mainly summarize their results into 4 points:\n\n1. Building a generalized family of contrastive loss functions & proving that a deep non-linear projection head(**an innovation explained in SimCLRv1**) reduces the performance gap between various functions & also allows the models to make use of smaller batch sizes to learn good\/generalised features (*since in contrastive losses larger batch size help in faster convergance*).\n2. Contrastive loss function based models make use of instance based objective (**i.e. the models that learn on entire image summary**) but can learn on images with multiple objects & learn good quality local features.\n3. The authors build custom datasets with controllable features to understand the competing feature suppression phenomenon in contrastive based learning.\n4. Lastly they show how dominant, easy to learn features in an image can interefere with learning of other features present.\n\n\nI have summarized this as a very raw write-up & those who are interested in performing a deep dive in this topic can refer : [Intriguing](https:\/\/contrastive-learning.github.io\/intriguing\/.)\n\n","7ec3509c":"# Image Visualisation\n\nMy dear Watson selecting these images was not a coincidence since no human action is ever truly random. ( **sorry for the Sherlock lingo** )\n\nBut on a serious note the reason why I chose these images is to see the quality of features learned by a pretrained model on imageNet. As mentioned a model with good grasp of features should mainly be able to group pixels of the same\/similar objects into the same cluster ( **We give clusters color & overlay it on the image. Therefore pixels belonging to same cluster will have same color**). \n\nEach of these images has multiple actors, objects, interesting texts on the image etc. (**sorry for the I am SHER-locked spoiler**). \n\n*Mainly I wanted to see can the pretrained model differentiate between these features.*\n","6c550a0b":"# Utility Functions","d96e70ed":"# Paths Setup","02ce4116":"# Model\n\nIn this test, I make use of the famous MobileNetV1 model with imageNet weights. You can use any model you like but remember to update the name of the layer whose features you want to visualise, since different models have different naming conventions based on the architecture they have. Those names can be easily found using model.summary()\n\n**Blocks to visualise**\n\nSince MobileNetV1 makes use of depthwise seperable convolutions i.e. depth wise followed by point wise Conv2D forms one proper unit(1 block). In this notebook I only evaluate the activation maps of the point wise Conv2D o\/p [that will be seen as 1 block] & also the blocks used will be of earlier layers since the captured local features are easier to understand when compared to the deeper layer's abstract concepts.","b106a750":"# Conclusion\n\nThis exercise is quite useful for developers devising new models or techniques in order to see wether their model is capturing the right items or not. In this notebook the K-means was done using a single image inference but keen developers can try by doing batch inference with slight tweak to the function & also increase the number of clusters since the variance in feature maps will shoot up in case of a batch inference ( theoretically it should not be too different for single image inference). Knowing these details can help a lot in the project you are trying to develop or these details can be used to understand why your model works !\n\n**Improvements to my code, any mistakes found or any other suggestions are always welcome.**\n\nThank You for your time.","281293cc":"# Inspiration\n\n\nNow coming to the main point, why am I writing this notebook when all code-repository links are widely available & developers ( **way more qualified\/experienced that me** ) have replicated the SimCLRv1 pipeline results. While going through the paper : \"**Intriguing Properties of Contrastive Losses**\", I was fascinated by one of the techniques used by the authors to understand the quality of features learnt by their model (that was trained using the **SSL(self supervised learning)**). This was a crucial step because knowing what your model learns using SSL & comparing it with its supervised counterpart will allow the authors to know whether their research is going in the right direction or not. Also knowing what kind of features your model has learnt is a good indicator in getting a rough estimate whether the model is focusing on the right aspects(i.e. features) of the image or not.\n\nFeature visualisation in CNN models is a task that is well researched. Techniques such as gradCams & filter visualisation have proven quite useful when it comes to finding what the model has learned ( i.e. to what part of the image the model is paying attention while making a prediciton). For beginners there is an excellent notebook by Aakash Nain, [What does a CNN see](https:\/\/www.kaggle.com\/aakashnain\/what-does-a-cnn-see) explaining these techniques hands-on with code. \n\nThe problem in SSL is slighlty different because there are no explicit class labels available to us & as far as I know the gradCam technique is highly label\/class dependent ( there could be a way to use this in SSL setting but I have no clue, **suggestions are highly welcomed**) & the filter visualisation method only tells us the basic contours\/features learned by each filter. Therefore the authors of the paper : \"Intriguing Properties of Contrastive Losses\" make use of K-means clustering to see how well the model has performed its learning (**how well it groups the local regions of an image**). \n\nI could not find a code for this task therefore I embarked upon this journey to understand & write it myself. Maybe this could serve useful to **you**.\n\n# Idea\n\nThanks to [Marc Felix](https:\/\/stackoverflow.com\/questions\/69632019\/how-to-use-k-means-clustering-to-visualise-learnt-features-of-a-cnn-model\/69632937#69632937)\n\nOnce a model learns its feature\/filter weights, we pass an image for inference. As the image is passed through each operation\/filter in a feature extractor CNN, the original image starts to downsample & after every stage we get an activation\/feature map with some dimensions as (H',W',C'). The reduced : H' & W' dimmension correspond to the spatial location on the activation map & the C' dimension is the list of features the model extracted from the image ( features which the model feels important for the final task at hand).The K-means algorithm is applied to these activation maps in order to see how these extracted features correspond to the original image. This works by flattening the spatial dimension of activation maps from (H',W',C') to (H'x W',C') [3D to 2D]. This step is done because, if we see the activation feature map, all the C' maps have the same spatial dimension but each of the C' map has extracted some different feature from the image based on the objective on which it was trained.\n\nFor e.g. in case of a dogs vs cats classifier ( **just an assumption** ) the filters for a trained CNN model could extract whiskers, ears, face, eyes, nose etc. as features, therefore an intermediate activation map in the particular CNN could have :\n\n1. 1st activation map containing ears as extracted features\n2. 2nd activation map containing whiskers as extracted features \n3. 3rd activation map containing eyes as extracted features \n\n& so on. At every spatial location we have C'(value varies from layer to layer) features being extracted by every layer of the model.\n\n\nNow coming back to our problem, we need to reshape our activation maps for using the K-means algorithm. Each location on the feature map has C'-features, this can be seen as (C' = 3 in this case):\n\n![Screenshot 2021-10-23 at 11.34.53 AM.png](attachment:1481459c-cdad-4c55-ab0d-fd41ad95edfe.png)\n\nThe goal of K-means is to form clusters on the right. The major driving force behind this idea is that : **If good\/generalised representations are learnt by the model we should be able to group the regions of similar objects in the same cluster**. This can be simplified as (based on previous dogs Vs cat classifier example): if a filter's goal is to find all the whiskers in an image then all the pixel regions having cat whiskers present in them will have the whisker activation map being maximally activated (having a high value in them) as compared to other locations having no whisker in them. Then on the C' dimensional axes ( **I cannot draw beyond 3D**) those pixels postions (**whisker ones**) will come under the same cluster since their activation vector will close by in the C' dimensional space. \n\nIf good features are learnt by the filters then we can scale these activation maps to overlay on original image & find out whether proper regions of same objects in the image are being grouped together or not.\n\n\n\n*Enough Talk let's code*"}}