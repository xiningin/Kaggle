{"cell_type":{"e9110712":"code","ce992138":"code","a75d04c4":"code","136dc9dc":"code","094cc62f":"code","36a6f494":"code","cd121a0b":"code","5ec1f0fe":"code","cf807941":"code","ae92e62d":"code","52bc4aa8":"code","51ecd7f6":"code","951f4a37":"markdown","749d61c2":"markdown","5b54a286":"markdown","5d30337f":"markdown","a29ff224":"markdown","5acbe80d":"markdown","d7ff9ca0":"markdown"},"source":{"e9110712":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","ce992138":"data = pd.read_csv(\"\/kaggle\/input\/voicegender\/voice.csv\")","a75d04c4":"data.head()\n","136dc9dc":"data.label = [1 if i=='male' else 0 for i in data.label]\nx = data.drop([\"label\"],axis=1)\ny = data.label.values ","094cc62f":"x = (x-np.min(x))\/(np.max(x)-np.min(x)).values","36a6f494":"from sklearn.model_selection import train_test_split\n    \nx_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.2,random_state=20)\n\nx_train = x_train.T\nx_test = x_test.T\ny_train = y_train.T\ny_test = y_test.T","cd121a0b":"def init(dimension):\n    w = np.full((dimension,1),0.01)\n    b = 0.0\n    return w,b\n\ndef sigmoid(z):\n    y_head = 1\/(1+np.exp(-z))\n    return y_head","5ec1f0fe":"def forward_backward_pro(w,b,x_train,y_train):\n    z = np.dot(w.T,x_train) + b\n    y_head = sigmoid(z)\n    loss = -(y_train*np.log(y_head) + (1-y_train)*np.log(1-y_head))\n    cost = np.sum(loss)\/(x_train.shape[1])\n    \n    weight = (np.dot(x_train,(y_head-y_train).T))\/x_train.shape[1]\n    bias = np.sum(y_head-y_train)\/x_train.shape[1]\n    \n    grad = {\"weight\":weight, \"bias\":bias}\n    \n    return cost,grad\n    ","cf807941":"def update(w,b,x_train,y_train,learning_rate,num):\n    cost_list = []\n    index = []\n    \n    for i in range(num):\n        cost,grad = forward_backward_pro(w,b,x_train,y_train)\n        \n        w = w - learning_rate * grad[\"weight\"]\n        b = b - learning_rate * grad[\"bias\"]\n        if i % 100 == 0:\n            cost_list.append(cost)\n            index.append(i)\n            print (\"Cost after iteration %i: %f\" %(i, cost))\n        \n        \n    parameters = {\"weight\":w, \"bias\":b}\n    plt.plot(index,cost_list)\n    plt.xlabel(\"number of iteration\")\n    plt.ylabel(\"cost\")\n    plt.show()\n    return parameters,grad,cost_list","ae92e62d":"def predict(w,b,x_test):\n    z = sigmoid(np.dot(w.T,x_test)+b)\n    prediction = np.zeros((1,x_test.shape[1]))\n    \n    for i in range(z.shape[1]):\n        if(z[0,i]<=0.5):\n            prediction[0,i]=0\n        else:\n            prediction[0,i]=1\n    return prediction","52bc4aa8":"def logistic_regression(x_train,y_train,x_test,y_test,learning_rate,num):\n    w,b = init(x_train.shape[0])\n    parameters,grad,cost_list = update(w,b,x_train,y_train,learning_rate,num)\n    \n    prediction = predict(parameters[\"weight\"],parameters[\"bias\"],x_test)\n    print(\"test accuracy: {} %\".format(100 - np.mean(np.abs(prediction - y_test)) * 100))","51ecd7f6":"logistic_regression(x_train,y_train,x_test,y_test,learning_rate=0.1,num=5000)","951f4a37":"## Predict","749d61c2":"So we separated the label column. We assigned 1 for male and 0 for \nfemale.","5b54a286":"## Normalization\n\n* Let's normalize the values of x between 0 and 1","5d30337f":"## Update","a29ff224":"## Sigmoid","5acbe80d":"\n * Let's split our data for training and testing","d7ff9ca0":"## Forward and Backward Propagation"}}