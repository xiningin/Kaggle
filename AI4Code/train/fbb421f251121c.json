{"cell_type":{"b3582812":"code","880b1bd7":"code","f2e1f4b9":"code","76fc45ca":"code","0406271b":"code","a02235fb":"code","e98e9630":"code","48a164f2":"code","4c41233b":"code","3331f778":"code","839defb9":"code","13ae3b3e":"markdown","b61b1a40":"markdown","6ed5a533":"markdown","f3593c7c":"markdown","24e913d5":"markdown"},"source":{"b3582812":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nfrom tqdm import tqdm\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session\n\nimport warnings\nwarnings.simplefilter('ignore')","880b1bd7":"train_df = pd.read_csv(\"..\/input\/ventilator-pressure-prediction\/train.csv\")\ntest_df = pd.read_csv(\"..\/input\/ventilator-pressure-prediction\/test.csv\")","f2e1f4b9":"def physics_info(df_breath):\n    \n    lag = 1\n\n    inhale_ind = np.argmax(df_breath[\"u_out\"])+lag\n\n    df_breath[\"physics_info\"] = 0\n\n    # Time constant\n    T = 400 * df_breath['R'].astype(\"float\")*df_breath['C'].astype(\"float\")\n\n    # inhale\n    exponent = (- df_breath['time_step'])\/T\n    factor=np.exp(exponent)\n    df_breath['vf']=(df_breath['u_in_cumsum']*df_breath['R'])\/factor\n\n    inhale = (df_breath[\"vf\"]\/(450) + df_breath[\"intercept\"]).values\n\n    # exhale\n    time_exhale_start = df_breath[\"time_step\"].iloc[inhale_ind]\n    exponent=(- (df_breath['time_step'] - time_exhale_start)) \/ T * 4500000\n    factor=np.exp(exponent)\n\n    df_breath[\"intercept\"] = 0.00\n    for R in df_breath[\"R\"].unique():\n        for C in df_breath[\"C\"].unique():\n            df_breath.loc[(df_breath[\"R\"]==R) & (df_breath[\"C\"]==C), \"intercept\"] = get_intercept(R, C)\n\n    K_T = inhale[inhale_ind] - df_breath[\"area_devided_C\"].values[0]\n    exhale = K_T * (1 - factor)\n\n    # store\n    physics_info = np.zeros(80)\n    physics_info[:inhale_ind] = inhale[:inhale_ind]\n    physics_info[inhale_ind:] = inhale[inhale_ind] - exhale[inhale_ind:]\n\n    return physics_info.tolist()","76fc45ca":"def add_physics_info(df):\n    \n    _physics_info = []\n    \n    for i in tqdm(df[\"breath_id\"].unique()):\n        _physics_info.extend(physics_info(df.loc[df[\"breath_id\"]==i]))\n            \n    return _physics_info","0406271b":"def memory_usage_mb(df, *args, **kwargs):\n    \"\"\"Dataframe memory usage in MB. \"\"\"\n    return df.memory_usage(*args, **kwargs).sum() \/ 1024**2\n\ndef reduce_memory_usage(df, deep=True, verbose=True, categories=True):\n    # All types that we want to change for \"lighter\" ones.\n    # int8 and float16 are not include because we cannot reduce\n    # those data types.\n    # float32 is not include because float16 has too low precision.\n    numeric2reduce = [\"int16\", \"int32\", \"int64\", \"float64\"]\n    start_mem = 0\n    if verbose:\n        start_mem = memory_usage_mb(df, deep=deep)\n\n    for col, col_type in df.dtypes.iteritems():\n        best_type = None\n        if col_type == \"object\":\n            df[col] = df[col].astype(\"category\")\n            best_type = \"category\"\n        elif col_type in numeric2reduce:\n            downcast = \"integer\" if \"int\" in str(col_type) else \"float\"\n            df[col] = pd.to_numeric(df[col], downcast=downcast)\n            best_type = df[col].dtype.name\n        # Log the conversion performed.\n        if verbose and best_type is not None and best_type != str(col_type):\n            print(f\"Column '{col}' converted from {col_type} to {best_type}\")\n\n    if verbose:\n        end_mem = memory_usage_mb(df, deep=deep)\n        diff_mem = start_mem - end_mem\n        percent_mem = 100 * diff_mem \/ start_mem\n        print(f\"Memory usage decreased from\"\n              f\" {start_mem:.2f}MB to {end_mem:.2f}MB\"\n              f\" ({diff_mem:.2f}MB, {percent_mem:.2f}% reduction)\")\n    \n    return df\n\nget_intercept = lambda R, C : train_df.loc[train_df[\"time_step\"]==0].groupby([\"R\", \"C\"])[\"pressure\"].mean().loc[(R, C)]\n\n\ndef add_features(df):\n    df['area'] = df['time_step'] * df['u_in']\n    df['area'] = df.groupby('breath_id')['area'].cumsum()\n    df['time_step_cumsum'] = df.groupby(['breath_id'])['time_step'].cumsum()\n    df['u_in_cumsum'] = (df['u_in']).groupby(df['breath_id']).cumsum()\n    print(\"Step-1...Completed\")\n    df = reduce_memory_usage(df)\n    \n    # my features\n    ## TV \/ C\n    area_devided_C = df.loc[(df[\"u_out\"]==0), [\"breath_id\", \"area\"]].groupby(\"breath_id\").max().values \/ \\\n        df.loc[(df[\"u_out\"]==0), [\"breath_id\", \"C\"]].groupby(\"breath_id\").mean().values\n    _area_devided_C = np.repeat(area_devided_C, 80, axis=1).flatten()\n    df[\"area_devided_C\"] = _area_devided_C\n\n    df[\"intercept\"] = 0.00\n    for R in df[\"R\"].unique():\n        for C in df[\"C\"].unique():\n            df.loc[(df[\"R\"]==R) & (df[\"C\"]==C), \"intercept\"] = get_intercept(R, C)\n    \n    df[\"predicted_pressure_by_physics\"] = add_physics_info(df)\n    \n    print(\"Step-1.5(My-Features)...Completed\")\n    \n    \n    return df","a02235fb":"print(\"Train data...\\n\")\ntrain = add_features(train_df)\ntest = add_features(test_df)","e98e9630":"train[\"mae_pp\"] = np.abs(train[\"pressure\"]-train[\"predicted_pressure_by_physics\"])\nerror=train.loc[:, [\"breath_id\", \"mae_pp\"]].groupby(\"breath_id\").mean()\nmin_error_index = error.sort_values(by=\"mae_pp\").index.values","48a164f2":"for _id in min_error_index[:10]:\n\n    fig, ax1 = plt.subplots(figsize = (12, 8))\n\n    breath_1 = train.loc[train['breath_id'] == _id]\n\n    ax2 = ax1.twinx()\n    plt.title(f\"breath_id={_id}\")\n    ax1.plot(breath_1['time_step'], breath_1['pressure'], 'r-', label='pressure')\n    ax1.plot(breath_1['time_step'], breath_1['u_in'], 'g-', label='u_in')\n    ax2.plot(breath_1['time_step'], breath_1['u_out'], 'b-', label='u_out')\n    ax1.plot(breath_1['time_step'], breath_1[\"predicted_pressure_by_physics\"], 'k--', label='ppbp')\n\n    ax1.set_xlabel('Timestep')\n\n    ax1.legend(loc=(1.1, 0.8))\n    ax2.legend(loc=(1.1, 0.7))\n    plt.show()","4c41233b":"for _id in min_error_index[-10:]:\n\n    fig, ax1 = plt.subplots(figsize = (12, 8))\n\n    breath_1 = train.loc[train['breath_id'] == _id]\n\n    ax2 = ax1.twinx()\n    plt.title(f\"breath_id={_id}\")\n    ax1.plot(breath_1['time_step'], breath_1['pressure'], 'r-', label='pressure')\n    ax1.plot(breath_1['time_step'], breath_1['u_in'], 'g-', label='u_in')\n    ax2.plot(breath_1['time_step'], breath_1['u_out'], 'b-', label='u_out')\n    ax1.plot(breath_1['time_step'], breath_1[\"predicted_pressure_by_physics\"], 'k--', label='ppbp')\n\n    ax1.set_xlabel('Timestep')\n\n    ax1.legend(loc=(1.1, 0.8))\n    ax2.legend(loc=(1.1, 0.7))\n    plt.show()","3331f778":"train[\"predicted_pressure_by_physics\"].to_csv(\"train_predictd_pressure_by_physics.csv\", index=False)","839defb9":"sub = pd.read_csv('..\/input\/ventilator-pressure-prediction\/sample_submission.csv')\nsub['pressure'] = test[\"predicted_pressure_by_physics\"]\nsub.to_csv('submission.csv', index=False)\nsub.head(5)","13ae3b3e":"## good results","b61b1a40":"## bad results","6ed5a533":"## I try to predict directory pressure using no machine learning. \n## Specificaly, I use first-order lag of a domain of a control engineering.","f3593c7c":"## Feature engineering using a first-order lag system","24e913d5":"# Non ML Approach"}}