{"cell_type":{"c5b3c059":"code","348806c4":"code","32641bb1":"code","ca9ef824":"code","282c8e4c":"code","99f84680":"code","82957827":"code","3ade3774":"code","8ab6e86a":"code","9799163c":"code","7f27d92c":"code","ca99b53a":"code","971ceb06":"code","4f5eece1":"code","2b61b76a":"code","5e28d010":"code","3eec8a02":"code","a88f0491":"code","3e9521fc":"code","d9af19f6":"code","6090c784":"code","9237c957":"code","9a1afcfd":"markdown","a812a097":"markdown","d8803a3e":"markdown","d0989e0a":"markdown","7e78631f":"markdown","f175013b":"markdown","9f6be83a":"markdown","95a9a25d":"markdown","53e1966d":"markdown","c871fadf":"markdown","d29796ad":"markdown","b2d012bd":"markdown","d2ac8829":"markdown","7d83899d":"markdown","85cf81ac":"markdown","a397dcaf":"markdown","118974fd":"markdown","2da6df52":"markdown","672451bb":"markdown"},"source":{"c5b3c059":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","348806c4":"df=pd.read_csv('..\/input\/heart-failure-clinical-data\/heart_failure_clinical_records_dataset.csv')","32641bb1":"df.head()","ca9ef824":"y=df['DEATH_EVENT']\nx=df.drop(\"DEATH_EVENT\",axis=1)","282c8e4c":"df.isnull().sum()","99f84680":"df.describe()","82957827":"for i in df.columns:\n    if df[i].nunique() == 2:\n        print(i)","3ade3774":"import plotly.express as px\nfig = px.histogram(df, x=\"age\", color=\"DEATH_EVENT\", marginal=\"box\", hover_data=df.columns)\nfig.show()","8ab6e86a":"import plotly.express as px\nfig = px.histogram(df, x=\"time\", color=\"DEATH_EVENT\", marginal=\"box\", hover_data=df.columns)\nfig.show()","9799163c":"import plotly.graph_objs as go\nmale = df[df[\"sex\"]==1]\nfemale = df[df[\"sex\"]==0]\n\n\nlabels = ['Male - Survived','Male - Not Survived', \"Female -  Survived\", \"Female - Not Survived\"]\nvalues = [len(male[df[\"DEATH_EVENT\"]==0]),len(male[df[\"DEATH_EVENT\"]==1]),\n          len(female[df[\"DEATH_EVENT\"]==0]),len(female[df[\"DEATH_EVENT\"]==1])]\nfig = go.Figure(data=[go.Pie(labels=labels, values=values, hole=.4)])\nfig.update_layout(title_text=\"Analysis on Survival - Gender\")\nfig.show()","7f27d92c":"import plotly.graph_objs as go\ndiabetic = df[df[\"diabetes\"]==1]\nnon_diabetic = df[df[\"diabetes\"]==0]\n\n\nlabels = ['diabetic - Survived','diabetic - Not Survived', \"non diabetic -  Survived\", \"non diabetic - Not Survived\"]\nvalues = [len(diabetic[df[\"DEATH_EVENT\"]==0]),len(diabetic[df[\"DEATH_EVENT\"]==1]),\n          len(non_diabetic[df[\"DEATH_EVENT\"]==0]),len(non_diabetic[df[\"DEATH_EVENT\"]==1])]\nfig = go.Figure(data=[go.Pie(labels=labels, values=values, hole=.4)])\nfig.update_layout(title_text=\"Analysis on Survival - Gender\")\nfig.show()","ca99b53a":"import matplotlib.pyplot as plt\nimport seaborn as sns\nplt.figure(figsize=(20,20))\nsns.heatmap(df.corr(),annot=True,center=0)","971ceb06":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(x, y, test_size = 0.25, random_state = 0)","4f5eece1":"from sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nX_train = sc.fit_transform(X_train)\nX_test = sc.transform(X_test)","2b61b76a":"from sklearn.linear_model import LogisticRegression\nclassifier1 = LogisticRegression(random_state = 0)\nclassifier1.fit(X_train, y_train)\ny_pred = classifier1.predict(X_test)\nfrom sklearn.metrics import confusion_matrix, accuracy_score\ncm = confusion_matrix(y_test, y_pred)\nprint(cm)\naccuracy_score(y_test, y_pred)","5e28d010":"from sklearn.neighbors import KNeighborsClassifier\nclassifier2 = KNeighborsClassifier(n_neighbors = 5, metric = 'minkowski', p = 2)\nclassifier2.fit(X_train, y_train)\ny_pred = classifier2.predict(X_test)\nfrom sklearn.metrics import confusion_matrix, accuracy_score\ncm = confusion_matrix(y_test, y_pred)\nprint(cm)\naccuracy_score(y_test, y_pred)","3eec8a02":"from sklearn.svm import SVC\nclassifier3 = SVC(kernel = 'linear', random_state = 0)\nclassifier3.fit(X_train, y_train)\ny_pred = classifier3.predict(X_test)\nfrom sklearn.metrics import confusion_matrix, accuracy_score\ncm = confusion_matrix(y_test, y_pred)\nprint(cm)\naccuracy_score(y_test, y_pred)","a88f0491":"from sklearn.svm import SVC\nclassifier4 = SVC(kernel = 'rbf', random_state = 0)\nclassifier4.fit(X_train, y_train)\ny_pred = classifier4.predict(X_test)\nfrom sklearn.metrics import confusion_matrix, accuracy_score\ncm = confusion_matrix(y_test, y_pred)\nprint(cm)\naccuracy_score(y_test, y_pred)","3e9521fc":"from sklearn.naive_bayes import GaussianNB\nclassifier5 = GaussianNB()\nclassifier5.fit(X_train, y_train)\ny_pred = classifier5.predict(X_test)\nfrom sklearn.metrics import confusion_matrix, accuracy_score\ncm = confusion_matrix(y_test, y_pred)\nprint(cm)\naccuracy_score(y_test, y_pred)","d9af19f6":"from sklearn.tree import DecisionTreeClassifier\nclassifier6 = DecisionTreeClassifier(criterion = 'entropy', random_state = 0)\nclassifier6.fit(X_train, y_train)\ny_pred = classifier6.predict(X_test)\nfrom sklearn.metrics import confusion_matrix, accuracy_score\ncm = confusion_matrix(y_test, y_pred)\nprint(cm)\naccuracy_score(y_test, y_pred)","6090c784":"from sklearn.ensemble import RandomForestClassifier\nclassifier7 = RandomForestClassifier(n_estimators = 11, criterion = 'entropy', random_state = 0)\nclassifier7.fit(X_train, y_train)\ny_pred = classifier7.predict(X_test)\nfrom sklearn.metrics import confusion_matrix, accuracy_score\ncm = confusion_matrix(y_test, y_pred)\nprint(cm)\naccuracy_score(y_test, y_pred)","9237c957":"from xgboost import XGBClassifier\nclassifier8 = XGBClassifier()\nclassifier8.fit(X_train, y_train)\nfrom sklearn.metrics import confusion_matrix, accuracy_score\ny_pred = classifier8.predict(X_test)\ncm = confusion_matrix(y_test, y_pred)\nprint(cm)\naccuracy_score(y_test, y_pred)","9a1afcfd":"**WE CAN SEE ACCURACY FROM VARIOUS MODELS IS:-**\n* Logistic Regression - 77.3%\n* K-nearest Neighbours - 73.3%\n* SVM - 78.6%\n* Kernel SVM - 81.3%\n* Naive Bayes - 66.6%\n* Decision Tree - 81.3%\n* Random forest - 90.6%\n* Xg boost - 84%\n\n**HIGHEST ACCURACY IS FROM RANDOM FOREST CLASSIFICATION WHICH IS 90.6%**","a812a097":"**Here we can see maximum deaths are in range of 58-62\nAnd as age is increasing the chances of dying is increasing as compared to not dying**","d8803a3e":"**Applying Descision tree classification**","d0989e0a":"**Not much relation is seen in features**","7e78631f":"**Applying Logistic Regression**","f175013b":"NO NULL VALUES PRESENT","9f6be83a":"# ANALYSISING DATA","95a9a25d":"**Here we can see maximum number of patient died within 20-39 days of follow period\nAnother thing to notice is that if patient survive more than 80 days the chances of him surving are much higher than dying**","53e1966d":"# APPLYING VARIOUS MODELS","c871fadf":"**Applying Naive Bayes**","d29796ad":"**chances of Surviving without dibatese is greater than chances of surviving with dibatese**","b2d012bd":"**Applying SVM**","d2ac8829":"**Applying K-nearest Neighbours**","7d83899d":"We can see that these 6 features have only 2 unique values.","85cf81ac":"**Applying Kernel SVM**","a397dcaf":"Splitting the data into test set and train set","118974fd":"**Applying Random Ftores**","2da6df52":"Applying Xgboost","672451bb":"Applying Feature Scaling"}}