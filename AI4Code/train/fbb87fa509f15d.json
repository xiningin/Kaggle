{"cell_type":{"674a7192":"code","50c1163a":"code","b0cfe590":"code","7ec65759":"code","bce238f6":"code","d587a19f":"code","0f5195ba":"code","462286c8":"code","e44fd999":"code","b9c9160a":"code","d2ca7a96":"code","a45e5116":"code","4897f8e9":"code","22468abd":"code","0fd88d1d":"code","52836c9c":"code","efd24471":"code","7c7b9128":"code","8340af72":"code","072e738c":"code","2bb9358e":"code","1e4cc2d9":"code","1950bdd3":"code","17ca6329":"code","8af81f00":"code","e4b50cd1":"code","a07b01df":"code","39e8b8a2":"code","1e2e0521":"code","2c3aafd1":"code","3522d0e4":"code","ac63c789":"code","1aea5416":"code","deb87e83":"code","b12b7906":"code","920eaf12":"code","ec9a96c4":"code","9f6d069f":"code","59c1358d":"code","4f67603f":"code","beb3c8fc":"code","f4744236":"code","ca2e1084":"code","86cc8717":"code","13dcb740":"code","a8f8cf93":"code","869fadf9":"code","0b9d7cfc":"code","d7f31ea5":"code","6f58ebaf":"code","b5d990b3":"code","36b6d02e":"code","82b10638":"markdown","953e4b90":"markdown","0b10b39c":"markdown","7c1578b8":"markdown","4218cdbe":"markdown","b309c8bc":"markdown","36a92b81":"markdown","ab27f94a":"markdown","36123fc2":"markdown","603eb9fc":"markdown","d4918e0a":"markdown","25d044fd":"markdown","72fe1ce9":"markdown","816d15de":"markdown","41566492":"markdown","98fef455":"markdown","5eb48d7f":"markdown","193c39d0":"markdown","ef910d51":"markdown","8dbc4479":"markdown","353e29d6":"markdown","4a31c305":"markdown","260656b5":"markdown","d07cea6a":"markdown"},"source":{"674a7192":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom scipy import stats\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\nimport warnings\nwarnings.filterwarnings(\"ignore\")","50c1163a":"pd.options.display.max_columns=100\nsmall=0.00001 #to prevent division by zero","b0cfe590":"train_df=pd.read_csv(\"..\/input\/beyond-analysis\/train.csv\")\ntest_df=pd.read_csv(\"..\/input\/beyond-analysis\/test.csv\")","7ec65759":"print(\"shape of train:\",train_df.shape)\nprint(\"shape of test:\",test_df.shape)","bce238f6":"# check if there are null values or not\ntrain_df.isnull().sum()","d587a19f":"# check if there are null values or not\ntest_df.isnull().sum()","0f5195ba":"train_df.info()","462286c8":"print(\"the maximum value in the column Y1 is:\",train_df['Y1'].max())\nprint(\"the minimum value in the column Y1 is:\",train_df['Y1'].min())\nprint(\"the mean value in the column Y1 is:\",train_df['Y1'].mean())\nprint(\"the standard deviation value in the column Y1 is:\",train_df['Y1'].std())\nprint('-'*100)\nprint(\"the maximum value in the column Y1 is:\",train_df['Y2'].max())\nprint(\"the minimum value in the column Y1 is:\",train_df['Y2'].min())\nprint(\"the mean value in the column Y1 is:\",train_df['Y2'].mean())\nprint(\"the standard deviation value in the column Y1 is:\",train_df['Y2'].std())","e44fd999":"data=train_df.groupby(['UNIQUE_IDENTIFIER'])['Y1','Y2'].mean()","b9c9160a":"print('-'*100)\nprint(\"the maximum value in the column Y1 is:\",data['Y1'].max())\nprint(\"the minimum value in the column Y1 is:\",data['Y1'].min())\nprint(\"the mean value in the column Y1 is:\",data['Y1'].mean())\nprint(\"the standard deviation value in the column Y1 is:\",data['Y1'].std())\nprint('-'*100)\nprint(\"the maximum value in the column Y1 is:\",data['Y2'].max())\nprint(\"the minimum value in the column Y1 is:\",data['Y2'].min())\nprint(\"the mean value in the column Y1 is:\",data['Y2'].mean())\nprint(\"the standard deviation value in the column Y1 is:\",data['Y2'].std())\nprint('-'*100)","d2ca7a96":"#display(train_df.target.describe())\nf, ax = plt.subplots(nrows=2, ncols=3, figsize=(18, 4))\nsns.distplot(train_df.Y1, ax=ax[0,0])\nsns.boxplot(train_df.Y1, ax=ax[0,1])\nstats.probplot(train_df['Y1'], plot=ax[0,2])\n\nsns.distplot(train_df.Y2, ax=ax[1,0])\nsns.boxplot(train_df.Y2, ax=ax[1,1])\nstats.probplot(train_df['Y2'], plot=ax[1,2])\n\nplt.tight_layout()\nplt.show()","a45e5116":"features = [feature for feature in train_df.columns if feature not in ['UNIQUE_IDENTIFIER', 'Y1','Y2','CATEGORY_1','CATEGORY_2']]\n\nfig = plt.figure(figsize=(12, 12), facecolor='#f6f6f6')\ngs = fig.add_gridspec(5, 4)\ngs.update(wspace=0.1, hspace=0.4)\n\nbackground_color = \"#f6f6f6\"\n\nrun_no = 0\nfor row in range(0, 5):\n    for col in range(0, 4):\n        locals()[\"ax\"+str(run_no)] = fig.add_subplot(gs[row, col])\n        locals()[\"ax\"+str(run_no)].set_facecolor(background_color)\n        locals()[\"ax\"+str(run_no)].tick_params(axis='y', left=False)\n        locals()[\"ax\"+str(run_no)].get_yaxis().set_visible(False)\n        for s in [\"top\",\"right\",\"left\"]:\n            locals()[\"ax\"+str(run_no)].spines[s].set_visible(False)\n        run_no += 1\n\nrun_no = 0\nfor feature in features:\n        sns.kdeplot(train_df[feature] ,ax=locals()[\"ax\"+str(run_no)], color='#ffd514', shade=True, linewidth=1.5, alpha=0.9, zorder=3, legend=False)\n        locals()[\"ax\"+str(run_no)].grid(which='major', axis='x', zorder=0, color='gray', linestyle=':', dashes=(1,5))\n        locals()[\"ax\"+str(run_no)].set_xlabel(feature)\n        run_no += 1\n","4897f8e9":"f, ax = plt.subplots(nrows=1, ncols=1, figsize=(20, 20))\nax.set_title(\"Correlation Matrix\", fontsize=16)\nsns.heatmap(train_df[train_df.columns[train_df.columns != 'UNIQUE_IDENTIFIER']].corr(), vmin=-1, vmax=1, cmap='coolwarm', annot=True)\n\nfor tick in ax.xaxis.get_major_ticks():\n    tick.label.set_fontsize(14) \n    tick.label.set_rotation(90) \nfor tick in ax.yaxis.get_major_ticks():\n    tick.label.set_fontsize(14)\n    tick.label.set_rotation(0) \nplt.show()","22468abd":"print(\"unique values in category 1 are:(train set) \",train_df['CATEGORY_1'].unique())\nprint(\"unique values in category 1 are(test set): \",test_df['CATEGORY_1'].unique())","0fd88d1d":"print(\"unique values in category 2 are:(train set) \",train_df['CATEGORY_2'].unique())\nprint(\"unique values in category 2 are(test set): \",test_df['CATEGORY_2'].unique())","52836c9c":"# corr_featu=[\"DEPOSIT\", \"ENTRY\", \"REVENUE\", \"WINNINGS_1\"]","efd24471":"#function to find the difference in the series\ndef differ(Series):\n    return Series.diff()","7c7b9128":"train_df[train_df['DISCOUNT']<=train_df['ENTRY']]","8340af72":"# difference in the cards to be purchased an actual cards purchased\ntrain_df['pct']=(train_df['DEPOSIT']-train_df['ENTRY'])\ntest_df['pct']=(test_df['DEPOSIT']-test_df['ENTRY']) \n#\ntrain_df['fract']=train_df['PRACTICE_WINNINGS_NUMBER']\/(train_df['PRACTICE_ENTRY_NUMBER']+0.00001)\ntest_df['fract']=test_df['PRACTICE_WINNINGS_NUMBER']\/(test_df['PRACTICE_ENTRY_NUMBER']+0.00001)\n# ENTRY_NUMBER - Contests participated\n# WINNINGS_NUMBER - Contests won\ntrain_df['fract1']=train_df['WINNINGS_NUMBER']\/(train_df['ENTRY_NUMBER']+0.00001)\ntest_df['fract1']=test_df['WINNINGS_NUMBER']\/(test_df['ENTRY_NUMBER']+0.00001)\n\ntrain_df['fract2']=train_df['PRACTICE_WINNINGS']\/(train_df['PRACTICE_ENTRY']+0.00001)\ntest_df['fract2']=test_df['PRACTICE_WINNINGS']\/(test_df['PRACTICE_ENTRY']+0.00001)\n\ntrain_df['fract3']=(train_df['WITHDRAW']-train_df['DEPOSIT'])\/(train_df['WITHDRAW']+train_df['DEPOSIT']+0.00001)\ntest_df['fract3']=(test_df['WITHDRAW']-test_df['DEPOSIT'])\/(test_df['WITHDRAW']+test_df['DEPOSIT']+0.000001)\n\ntrain_df['paid']=train_df['REVENUE']-train_df['DISCOUNT']\ntest_df['paid']=test_df['REVENUE']-test_df['DISCOUNT']\n\ntrain_df['fract4']=train_df['ENTRY']-train_df['WINNINGS_1']\ntest_df['fract4']=test_df['ENTRY']-test_df['WINNINGS_1']\n\ntrain_df['fract5']=train_df['REVENUE']\/(train_df['ENTRY']+0.01)\ntest_df['fract5']=test_df['REVENUE']\/(test_df['ENTRY']+0.01)\n\ntrain_df['fract6']=train_df['DISCOUNT']\/(train_df['ENTRY']+0.01)\ntest_df['fract6']=test_df['DISCOUNT']\/(test_df['ENTRY']+0.01)","072e738c":"features = [feature for feature in train_df.columns if feature not in ['UNIQUE_IDENTIFIER','SEQUENCE_NO', 'Y1','Y2','CATEGORY_1','CATEGORY_2']]\nfor i in features:\n    train_df[i+'_diff']=differ(train_df[i])\n    test_df[i+'_diff']=differ(test_df[i])\n\nfor i in features:\n    train_df[i+'_diff'][train_df['SEQUENCE_NO']==1]=0.00\n    test_df[i+'_diff'][test_df['SEQUENCE_NO']==1]=0.00","2bb9358e":"train_df[train_df['SEQUENCE_NO']==1]","1e4cc2d9":"import string\ncat_columns=['CATEGORY_1', 'CATEGORY_2']\nfrom sklearn.preprocessing import LabelEncoder\nlb=LabelEncoder()\nlb.fit(list(string.ascii_uppercase))\nfor x in cat_columns:\n    train_df[x]=lb.transform(train_df[x])\n    test_df[x]=lb.transform(test_df[x])","1950bdd3":"def percentile(n):\n    def percentile_(x):\n        return np.percentile(x, n)\n    percentile_.__name__ = 'percentile_%s' % n\n    return percentile_\ni=[np.mean,np.max,np.min,np.sum,np.std,np.median]\n# i=[np.mean,np.max,np.min,np.sum,np.std,np.median]\nfe_dict = {\n        'SEQUENCE_NO':[np.size],\n        'STATUS_CHECK':[np.mean],\n        'CATEGORY_1':[np.mean],\n        'CATEGORY_2':[np.mean],\n        'ACTIVE_YN':[np.mean,np.sum],\n        'ENTRY':i,\n        'REVENUE':i,\n        'WINNINGS_1':i,\n        'WINNINGS_2':i,\n        'DISCOUNT':i,\n        'DEPOSIT':i,\n        'DEPOSIT_NUMBER':i,\n        'DEPOSIT_2':i,\n        'WITHDRAW':i,\n        'WITHDRAW_NUMBER':i,\n        'DEPOSIT_TRAILS':i,\n        'ENTRY_NUMBER':i,\n        'WINNINGS_NUMBER':i,\n        'PRACTICE_ENTRY':i,\n        'PRACTICE_WINNINGS':i,\n        'PRACTICE_ENTRY_NUMBER':i,\n        'PRACTICE_WINNINGS_NUMBER':i,\n        'ACTIVE_YN_diff':[np.mean,np.sum],\n        'ENTRY_diff':i,\n        'REVENUE_diff':i,\n        'WINNINGS_1_diff':i,\n         'WINNINGS_2_diff':i,\n        'DISCOUNT_diff':i,\n        'DEPOSIT_diff':i,\n        'DEPOSIT_NUMBER_diff':i,\n        'DEPOSIT_2_diff':i,\n        'WITHDRAW_diff':i,\n        'WITHDRAW_NUMBER_diff':i,\n        'DEPOSIT_TRAILS_diff':i,\n        'ENTRY_NUMBER_diff':i,\n        'WINNINGS_NUMBER_diff':i,\n        'PRACTICE_ENTRY_diff':i,\n        'PRACTICE_WINNINGS_diff':i,\n        'PRACTICE_ENTRY_NUMBER_diff':i,\n        'PRACTICE_WINNINGS_NUMBER_diff':i,\n        'pct':i,\n        'fract':i,\n        'fract1':i,\n        'fract2':i,\n        'fract3':i,\n        'fract4':i,\n        'fract5':i,\n        'fract6':i,\n        'paid':i,\n        'Y1':[np.mean],\n        'Y2':[np.mean],\n         }\ntrain=train_df.groupby(['UNIQUE_IDENTIFIER']).agg(fe_dict).reset_index()\ntrain.columns = ['_'.join(col) for col in train.columns]","17ca6329":"f, ax = plt.subplots(nrows=1, ncols=1, figsize=(6,6))\n#f.suptitle('Distribution of Features', fontsize=16)\nsns.distplot(train['DEPOSIT_sum'])\nplt.tight_layout()\nplt.show()","8af81f00":"fe_dict = {\n        'SEQUENCE_NO':[np.size],\n        'STATUS_CHECK':[np.mean],\n        'CATEGORY_1':[np.mean],\n        'CATEGORY_2':[np.mean],\n        'ACTIVE_YN':[np.mean,np.sum],\n        'ENTRY':i,\n        'REVENUE':i,\n        'WINNINGS_1':i,\n        'WINNINGS_2':i,\n        'DISCOUNT':i,\n        'DEPOSIT':i,\n        'DEPOSIT_NUMBER':i,\n        'DEPOSIT_2':i,\n        'WITHDRAW':i,\n        'WITHDRAW_NUMBER':i,\n        'DEPOSIT_TRAILS':i,\n        'ENTRY_NUMBER':i,\n        'WINNINGS_NUMBER':i,\n        'PRACTICE_ENTRY':i,\n        'PRACTICE_WINNINGS':i,\n        'PRACTICE_ENTRY_NUMBER':i,\n        'PRACTICE_WINNINGS_NUMBER':i,\n        'ACTIVE_YN_diff':[np.mean,np.sum],\n        'ENTRY_diff':i,\n        'REVENUE_diff':i,\n        'WINNINGS_1_diff':i,\n         'WINNINGS_2_diff':i,\n        'DISCOUNT_diff':i,\n        'DEPOSIT_diff':i,\n        'DEPOSIT_NUMBER_diff':i,\n        'DEPOSIT_2_diff':i,\n        'WITHDRAW_diff':i,\n        'WITHDRAW_NUMBER_diff':i,\n        'DEPOSIT_TRAILS_diff':i,\n        'ENTRY_NUMBER_diff':i,\n        'WINNINGS_NUMBER_diff':i,\n        'PRACTICE_ENTRY_diff':i,\n        'PRACTICE_WINNINGS_diff':i,\n        'PRACTICE_ENTRY_NUMBER_diff':i,\n        'PRACTICE_WINNINGS_NUMBER_diff':i,\n        'pct':i,\n        'fract':i,\n        'fract1':i,\n        'fract2':i,\n         'fract3':i,\n        'fract4':i,\n        'fract5':i,\n        'fract6':i,\n        'paid':i,\n         }\ntest=test_df.groupby(['UNIQUE_IDENTIFIER']).agg(fe_dict).reset_index()\ntest.columns = ['_'.join(col) for col in test.columns]","e4b50cd1":"test","a07b01df":"# STATUS_CHECK_mean SEQUENCE_NO_size\ntrain['fract1_']=train['WINNINGS_NUMBER_sum']\/(train['ENTRY_NUMBER_sum']+0.00001)\ntest['fract1_']=test['WINNINGS_NUMBER_sum']\/(test['ENTRY_NUMBER_sum']+0.00001)","39e8b8a2":"train=train.fillna(0.0)\ntest=test.fillna(0.0)","1e2e0521":"features=['Y1_mean','Y2_mean']","2c3aafd1":"from sklearn.cluster import KMeans\nfrom sklearn.decomposition import PCA\nkmeans = KMeans(n_clusters=10, random_state=2021)\nkmeans.fit(train[features])\n\npca = PCA(n_components=2)\nstocks_2d = pca.fit_transform(train[features])\n\nfig, ax = plt.subplots(figsize=(32, 10))\nax.scatter(stocks_2d[:, 0], stocks_2d[:, 1], s=200, c=kmeans.labels_, cmap='RdBu')\n# for idx, stock_id in enumerate(train['UNIQUE_IDENTIFIER_'].values):\n#     ax.annotate(stock_id, (stocks_2d[idx, 0], stocks_2d[idx, 1]), fontsize=20)\n    \nax.tick_params(axis='x', labelsize=20, pad=10)\nax.tick_params(axis='y', labelsize=20, pad=10)\nax.set_title('ID Clusters', size=25, pad=20)\nplt.show()","3522d0e4":"print(\"the maximum value in the column Y1 is:\",train['Y1_mean'].max())\nprint(\"the minimum value in the column Y1 is:\",train['Y1_mean'].min())\nprint(\"the mean value in the column Y1 is:\",train['Y1_mean'].mean())\nprint(\"the standard deviation value in the column Y1 is:\",train['Y1_mean'].std())\nprint('-'*100)\nprint(\"the maximum value in the column Y1 is:\",train['Y2_mean'].max())\nprint(\"the minimum value in the column Y1 is:\",train['Y2_mean'].min())\nprint(\"the mean value in the column Y1 is:\",train['Y2_mean'].mean())\nprint(\"the standard deviation value in the column Y1 is:\",train['Y2_mean'].std())","ac63c789":"features = [col for col in train.columns if col not in {\"Y1_mean\",\"Y2_mean\",\"UNIQUE_IDENTIFIER_\"}]","1aea5416":"# Train\nfrom sklearn.preprocessing import QuantileTransformer\ntrain_model_sc = train.copy()\ntrain_label = train_model_sc[[\"Y1_mean\",\"Y2_mean\"]]\ntrain_model_sc = train_model_sc[features]\ncolumns = train_model_sc.columns\n# Test\ntest_model_sc = test.copy()\ntest_model_sc = test_model_sc[features]\n#Scaler\nqt = QuantileTransformer(n_quantiles=1000, random_state=0,output_distribution = \"normal\")\n# Scaling\ntrain[features] = qt.fit_transform(train_model_sc)\ntest[features]  = qt.transform(test_model_sc)\n\ntrain=train.fillna(0.0)\ntest=test.fillna(0.0)","deb87e83":"cont_features =[col for col in train.columns if col not in {\"Y1_mean\",\"Y2_mean\",\"UNIQUE_IDENTIFIER_\"}]\ncat_features = []\ntarget = train[\"Y1_mean\"]","b12b7906":"# from category_encoders import LeaveOneOutEncoder\n# from sklearn.preprocessing import LabelEncoder\n\nxgb_cat_features = []\nlgb_cat_features = []\ncb_cat_features = []\nridge_cat_features = []\nsgd_cat_features = []\nhgbc_cat_features = []\n\n# loo_features = []\n# le_features = []\n# def loo_encode(train_df, test_df, column):\n#     loo = LeaveOneOutEncoder()\n#     new_feature = \"{}_loo\".format(column)\n#     loo.fit(train[column], target)\n#     train[column] = loo.transform(train[column])\n#     test[column] = loo.transform(test[column])\n#     return new_feature\n\n# for feature in cat_features:\n#     loo_features.append(loo_encode(train, test, feature))\n# #     le_features.append(label_encode(train, test, feature))\n    \n# # xgb_cat_features.extend(loo_features)\n# # lgb_cat_features.extend(le_features)\n# cb_cat_features.extend(cat_features)\n# # ridge_cat_features.extend(loo_features)\n# # sgd_cat_features.extend(loo_features)\n# # hgbc_cat_features.extend(loo_features)","920eaf12":"import warnings\nwarnings.filterwarnings(\"ignore\")\nimport pandas\nimport lightgbm as lgb\nfrom xgboost import XGBRegressor\nfrom catboost import CatBoostRegressor\nfrom sklearn.linear_model import Ridge\nfrom sklearn.linear_model import SGDRegressor\n# from sklearn.calibration import CalibratedRegressorCV\n\nfrom sklearn.experimental import enable_hist_gradient_boosting\nfrom sklearn.ensemble import HistGradientBoostingRegressor\n\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import roc_auc_score\n\n\ndef rmse(y_true, y_pred):\n    return np.sqrt(np.mean(np.square((y_true - y_pred))))\ndef feval_rmse(y_pred, lgb_train):\n    y_true = lgb_train.get_label()\n    return 'RMSE', rmse(y_true, y_pred), False\nrandom_state = 2021\nn_folds = 10\nk_fold = KFold(n_splits=n_folds, random_state=random_state, shuffle=True)\n\nxgb_train_preds = np.zeros(len(train.index), )\nxgb_test_preds = np.zeros(len(test.index), )\nxgb_features = xgb_cat_features + cont_features\n\nlgb_train_preds = np.zeros(len(train.index), )\nlgb_test_preds = np.zeros(len(test.index), )\nlgb_features = lgb_cat_features + cont_features\n\ncb_train_preds = np.zeros(len(train.index), )\ncb_test_preds = np.zeros(len(test.index), )\ncb_features = cb_cat_features + cont_features\n\n# ridge_train_preds = np.zeros(len(train.index), )\n# ridge_test_preds = np.zeros(len(test.index), )\n# ridge_features = ridge_cat_features + cont_features\n\n# sgd_train_preds = np.zeros(len(train.index), )\n# sgd_test_preds = np.zeros(len(test.index), )\n# sgd_features = sgd_cat_features + cont_features\n\nhgbc_train_preds = np.zeros(len(train.index), )\nhgbc_test_preds = np.zeros(len(test.index), )\nhgbc_features = hgbc_cat_features + cont_features\n\nfor fold, (train_index, test_index) in enumerate(k_fold.split(train)):\n    print(\"--> Fold {}\".format(fold + 1))\n    y_train = target.iloc[train_index]\n    y_valid = target.iloc[test_index]\n\n    xgb_x_train = pandas.DataFrame(train[xgb_features].iloc[train_index])\n    xgb_x_valid = pandas.DataFrame(train[xgb_features].iloc[test_index])\n\n    lgb_x_train = pandas.DataFrame(train[lgb_features].iloc[train_index])\n    lgb_x_valid = pandas.DataFrame(train[lgb_features].iloc[test_index])\n\n    cb_x_train = pandas.DataFrame(train[cb_features].iloc[train_index])\n    cb_x_valid = pandas.DataFrame(train[cb_features].iloc[test_index])\n\n#     ridge_x_train = pandas.DataFrame(train[ridge_features].iloc[train_index])\n#     ridge_x_valid = pandas.DataFrame(train[ridge_features].iloc[test_index])\n\n#     sgd_x_train = pandas.DataFrame(train[sgd_features].iloc[train_index])\n#     sgd_x_valid = pandas.DataFrame(train[sgd_features].iloc[test_index])\n\n    hgbc_x_train = pandas.DataFrame(train[hgbc_features].iloc[train_index])\n    hgbc_x_valid = pandas.DataFrame(train[hgbc_features].iloc[test_index])\n\n    xgb_model = XGBRegressor(\n        seed=2021,\n        n_estimators=5000,\n        verbosity=1,\n        eval_metric=\"rmse\",\n        tree_method=\"gpu_hist\",\n        gpu_id=0,\n        alpha=7.105038963844129,\n        colsample_bytree=0.25505629740052566,\n        gamma=0.4999381950212869,\n        reg_lambda=1.7256912198205319,\n        learning_rate=0.011823142071967673,\n        max_bin=338,\n        max_depth=8,\n        min_child_weight=2.286836198630466,\n        subsample=0.618417952155855,\n    )\n    xgb_model.fit(\n        xgb_x_train,\n        y_train,\n        eval_set=[(xgb_x_valid, y_valid)], \n        verbose=200,\n        early_stopping_rounds=500\n    )\n\n    train_oof_preds = xgb_model.predict(xgb_x_valid)\n    test_oof_preds = xgb_model.predict(test[xgb_features])\n    xgb_train_preds[test_index] = train_oof_preds\n    xgb_test_preds += test_oof_preds \/ n_folds\n    print(\": XGB - RMSE Score = {}\".format(rmse(y_valid, train_oof_preds)))\n    seed0=2021\n    params0 = {\n    'objective': 'rmse',\n    'boosting_type': 'gbdt',\n    'max_depth': -1,\n    'max_bin':100,\n    'min_data_in_leaf':500,\n    'learning_rate': 0.05,\n    'subsample': 0.72,\n    'subsample_freq': 4,\n    'feature_fraction': 0.5,\n    'lambda_l1': 0.5,\n    'lambda_l2': 1.0,\n    'categorical_column':[0],\n    'seed':seed0,\n    'feature_fraction_seed': seed0,\n    'bagging_seed': seed0,\n    'drop_seed': seed0,\n    'data_random_seed': seed0,\n    'n_jobs':-1,\n    'verbose': -1}\n    train_dataset = lgb.Dataset(lgb_x_train, y_train)\n    val_dataset = lgb.Dataset(lgb_x_valid, y_valid)\n    model = lgb.train(params = params0,\n                          num_boost_round=5000,\n                          train_set = train_dataset, \n                          valid_sets = [train_dataset, val_dataset], \n                          verbose_eval = 250,\n                          early_stopping_rounds=1000,\n                          feval = feval_rmse)\n    \n#     lgb.fit(\n#         lgb_x_train,\n#         y_train,\n#         eval_set=[(lgb_x_valid, y_valid)], \n#         verbose=200,\n#     )\n    train_oof_preds = model.predict(lgb_x_valid)\n    test_oof_preds = model.predict(test[lgb_features])\n    lgb_train_preds[test_index] = train_oof_preds\n    lgb_test_preds += test_oof_preds \/ n_folds\n    print(\": LGB- RMSE Score = {}\".format(rmse(y_valid, train_oof_preds)))\n\n    cb_model = CatBoostRegressor(\n        verbose=0,\n        eval_metric=\"RMSE\",\n        loss_function=\"RMSE\",\n        random_state=random_state,\n        num_boost_round=5000,\n        od_type=\"Iter\",\n        od_wait=1000,\n        task_type=\"GPU\",\n        devices=\"0\",\n        cat_features=[x for x in range(len(cb_cat_features))],\n        bagging_temperature=1.288692494969795,\n        grow_policy=\"Depthwise\",\n        l2_leaf_reg=9.847870133539244,\n        learning_rate=0.01877982653902465,\n        max_depth=8,\n        min_data_in_leaf=1,\n        penalties_coefficient=2.1176668909602734,\n    )\n    cb_model.fit(\n        cb_x_train,\n        y_train,\n        eval_set=[(cb_x_valid, y_valid)], \n        verbose=100,\n    )\n\n    train_oof_preds = cb_model.predict(cb_x_valid)\n    test_oof_preds = cb_model.predict(test[cb_features])\n    cb_train_preds[test_index] = train_oof_preds\n    cb_test_preds += test_oof_preds \/ n_folds\n    print(\": CATBOOST - RMSE Score = {}\".format(rmse(y_valid, train_oof_preds)))\n    \n#     ridge_model = CalibratedClassifierCV(\n#         RidgeClassifier(random_state=random_state),\n#         cv=3,\n#     )\n#     ridge_model.fit(\n#         ridge_x_train,\n#         y_train,\n#     )\n\n#     train_oof_preds = ridge_model.predict(ridge_x_valid)[:,-1]\n#     test_oof_preds = ridge_model.predict(test[ridge_features])\n#     ridge_train_preds[test_index] = train_oof_preds\n#     ridge_test_preds += test_oof_preds \/ n_folds\n#     print(\": Ridge - ROC AUC Score = {}\".format(roc_auc_score(y_valid, train_oof_preds, average=\"micro\")))\n    \n#     sgd_model = CalibratedClassifierCV(\n#         SGDClassifier(\n#             random_state=random_state,\n#             n_jobs=-1,\n#             loss=\"squared_hinge\",\n#         ),\n#         cv=3,\n#     )\n#     sgd_model.fit(\n#         sgd_x_train,\n#         y_train,\n#     )\n\n#     train_oof_preds = sgd_model.predict(sgd_x_valid)[:,-1]\n#     test_oof_preds = sgd_model.predict(test[sgd_features])[:,-1]\n#     sgd_train_preds[test_index] = train_oof_preds\n#     sgd_test_preds += test_oof_preds \/ n_folds\n#     print(\": SGD - ROC AUC Score = {}\".format(roc_auc_score(y_valid, train_oof_preds, average=\"micro\")))\n    \n    hgbc_model = HistGradientBoostingRegressor(\n        l2_regularization=1.766059063693552,\n        learning_rate=0.10675193678150449,\n        max_bins=128,\n        max_depth=31,\n        max_leaf_nodes=185,\n        random_state=2021\n    )\n    hgbc_model.fit(\n        hgbc_x_train,\n        y_train,\n    )\n\n    train_oof_preds = hgbc_model.predict(hgbc_x_valid)\n    test_oof_preds = hgbc_model.predict(test[hgbc_features])\n    hgbc_train_preds[test_index] = train_oof_preds\n    hgbc_test_preds += test_oof_preds \/ n_folds\n    print(\": HGBC- RMSE Score = {}\".format(rmse(y_valid, train_oof_preds)))\n    \n    \nprint(\"--> Overall metrics\")\nprint(\": XGB - rmse = {}\".format(rmse(target, xgb_train_preds)))\nprint(\": LGB - rmse = {}\".format(rmse(target, lgb_train_preds)))\nprint(\": CB - rmse = {}\".format(rmse(target, cb_train_preds)))\nprint(\": HGBC - rmse= {}\".format(rmse(target, hgbc_train_preds)))\n\n","ec9a96c4":"from scipy.special import expit\nfrom sklearn.calibration import CalibratedClassifierCV\n\nrandom_state = 2021\nn_folds = 10\n# k_fold =KFold(n_splits=n_folds, random_state=random_state, shuffle=True)\n\nl1_train = pandas.DataFrame(data={\n    \"xgb\": xgb_train_preds.tolist(),\n    \"lgb\": lgb_train_preds.tolist(),\n    \"cb\": cb_train_preds.tolist(),\n#     \"ridge\": ridge_train_preds.tolist(),\n#     \"sgd\": sgd_train_preds.tolist(),\n    \"hgbc\": hgbc_train_preds.tolist(),\n    \"target\": target.tolist()\n})\nl1_test = pandas.DataFrame(data={\n    \"xgb\": xgb_test_preds.tolist(),\n    \"lgb\": lgb_test_preds.tolist(),\n    \"cb\": cb_test_preds.tolist(),\n#     \"sgd\": sgd_test_preds.tolist(),\n#     \"ridge\": ridge_test_preds.tolist(),    \n    \"hgbc\": hgbc_test_preds.tolist(),\n})\n\ntrain_preds = np.zeros(len(l1_train.index), )\ntest_preds = np.zeros(len(l1_test.index), )\nfeatures = [\"xgb\", \"lgb\",\"cb\", \"hgbc\"]\nx_train = pandas.DataFrame(l1_test[features])\ntest_preds=(x_train[\"xgb\"]+x_train[\"lgb\"]+x_train[\"cb\"]+x_train[\"hgbc\"])\/4","9f6d069f":"submission = pandas.read_csv(\"..\/input\/beyond-analysis\/sample_submission_random.csv\")\nsubmission[\"Y1\"] = test_preds.tolist()\nsubmission","59c1358d":"\ntarget=train[\"Y2_mean\"]\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nimport pandas\nimport lightgbm as lgb\nfrom xgboost import XGBRegressor\nfrom catboost import CatBoostRegressor\nfrom sklearn.linear_model import Ridge\nfrom sklearn.linear_model import SGDRegressor\n# from sklearn.calibration import CalibratedRegressorCV\n\nfrom sklearn.experimental import enable_hist_gradient_boosting\nfrom sklearn.ensemble import HistGradientBoostingRegressor\n\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import roc_auc_score\n\n\ndef rmse(y_true, y_pred):\n    return np.sqrt(np.mean(np.square((y_true - y_pred))))\ndef feval_rmse(y_pred, lgb_train):\n    y_true = lgb_train.get_label()\n    return 'RMSE', rmse(y_true, y_pred), False\nrandom_state = 2021\nn_folds = 10\nk_fold = KFold(n_splits=n_folds, random_state=random_state, shuffle=True)\n\nxgb_train_preds = np.zeros(len(train.index), )\nxgb_test_preds = np.zeros(len(test.index), )\nxgb_features = xgb_cat_features + cont_features\n\nlgb_train_preds = np.zeros(len(train.index), )\nlgb_test_preds = np.zeros(len(test.index), )\nlgb_features = lgb_cat_features + cont_features\n\ncb_train_preds = np.zeros(len(train.index), )\ncb_test_preds = np.zeros(len(test.index), )\ncb_features = cb_cat_features + cont_features\n\nridge_train_preds = np.zeros(len(train.index), )\nridge_test_preds = np.zeros(len(test.index), )\nridge_features = ridge_cat_features + cont_features\n\nsgd_train_preds = np.zeros(len(train.index), )\nsgd_test_preds = np.zeros(len(test.index), )\nsgd_features = sgd_cat_features + cont_features\n\nhgbc_train_preds = np.zeros(len(train.index), )\nhgbc_test_preds = np.zeros(len(test.index), )\nhgbc_features = hgbc_cat_features + cont_features\n\nfor fold, (train_index, test_index) in enumerate(k_fold.split(train)):\n    print(\"--> Fold {}\".format(fold + 1))\n    y_train = target.iloc[train_index]\n    y_valid = target.iloc[test_index]\n\n    xgb_x_train = pandas.DataFrame(train[xgb_features].iloc[train_index])\n    xgb_x_valid = pandas.DataFrame(train[xgb_features].iloc[test_index])\n\n    lgb_x_train = pandas.DataFrame(train[lgb_features].iloc[train_index])\n    lgb_x_valid = pandas.DataFrame(train[lgb_features].iloc[test_index])\n\n    cb_x_train = pandas.DataFrame(train[cb_features].iloc[train_index])\n    cb_x_valid = pandas.DataFrame(train[cb_features].iloc[test_index])\n\n#     ridge_x_train = pandas.DataFrame(train[ridge_features].iloc[train_index])\n#     ridge_x_valid = pandas.DataFrame(train[ridge_features].iloc[test_index])\n\n#     sgd_x_train = pandas.DataFrame(train[sgd_features].iloc[train_index])\n#     sgd_x_valid = pandas.DataFrame(train[sgd_features].iloc[test_index])\n\n    hgbc_x_train = pandas.DataFrame(train[hgbc_features].iloc[train_index])\n    hgbc_x_valid = pandas.DataFrame(train[hgbc_features].iloc[test_index])\n\n    xgb_model = XGBRegressor(\n        seed=2021,\n        n_estimators=4000,\n        verbosity=1,\n        eval_metric=\"rmse\",\n        tree_method=\"gpu_hist\",\n        gpu_id=0,\n        alpha=7.105038963844129,\n        colsample_bytree=0.25505629740052566,\n        gamma=0.4999381950212869,\n        reg_lambda=1.7256912198205319,\n        learning_rate=0.011823142071967673,\n        max_bin=338,\n        max_depth=8,\n        min_child_weight=2.286836198630466,\n        subsample=0.618417952155855,\n    )\n    xgb_model.fit(\n        xgb_x_train,\n        y_train,\n        eval_set=[(xgb_x_valid, y_valid)], \n        verbose=200,\n        early_stopping_rounds=500\n    )\n\n    train_oof_preds = xgb_model.predict(xgb_x_valid)\n    test_oof_preds = xgb_model.predict(test[xgb_features])\n    xgb_train_preds[test_index] = train_oof_preds\n    xgb_test_preds += test_oof_preds \/ n_folds\n    print(\": XGB - RMSE Score = {}\".format(rmse(y_valid, train_oof_preds)))\n    seed0=2021\n    params0 = {\n    'objective': 'rmse',\n    'boosting_type': 'gbdt',\n    'max_depth': -1,\n    'max_bin':100,\n    'min_data_in_leaf':500,\n    'learning_rate': 0.05,\n    'subsample': 0.72,\n    'subsample_freq': 4,\n    'feature_fraction': 0.5,\n    'lambda_l1': 0.5,\n    'lambda_l2': 1.0,\n    'categorical_column':[0],\n    'seed':seed0,\n    'feature_fraction_seed': seed0,\n    'bagging_seed': seed0,\n    'drop_seed': seed0,\n    'data_random_seed': seed0,\n    'n_jobs':-1,\n    'verbose': -1}\n    train_dataset = lgb.Dataset(lgb_x_train, y_train)\n    val_dataset = lgb.Dataset(lgb_x_valid, y_valid)\n    model = lgb.train(params = params0,\n                          num_boost_round=5000,\n                          train_set = train_dataset, \n                          valid_sets = [train_dataset, val_dataset], \n                          verbose_eval = 250,\n                          early_stopping_rounds=1000,\n                          feval = feval_rmse)\n    \n#     lgb.fit(\n#         lgb_x_train,\n#         y_train,\n#         eval_set=[(lgb_x_valid, y_valid)], \n#         verbose=200,\n#     )\n    train_oof_preds = model.predict(lgb_x_valid)\n    test_oof_preds = model.predict(test[lgb_features])\n    lgb_train_preds[test_index] = train_oof_preds\n    lgb_test_preds += test_oof_preds \/ n_folds\n    print(\": lgb - RMSE Score = {}\".format(rmse(y_valid, train_oof_preds)))\n\n    cb_model = CatBoostRegressor(\n        verbose=0,\n        eval_metric=\"RMSE\",\n        loss_function=\"RMSE\",\n        random_state=random_state,\n        num_boost_round=5000,\n        od_type=\"Iter\",\n        od_wait=500,\n        task_type=\"GPU\",\n        devices=\"0\",\n        cat_features=[x for x in range(len(cb_cat_features))],\n        bagging_temperature=1.288692494969795,\n        grow_policy=\"Depthwise\",\n        l2_leaf_reg=9.847870133539244,\n        learning_rate=0.01877982653902465,\n        max_depth=8,\n        min_data_in_leaf=1,\n        penalties_coefficient=2.1176668909602734,\n    )\n    cb_model.fit(\n        cb_x_train,\n        y_train,\n        eval_set=[(cb_x_valid, y_valid)], \n        verbose=100,\n    )\n\n    train_oof_preds = cb_model.predict(cb_x_valid)\n    test_oof_preds = cb_model.predict(test[cb_features])\n    cb_train_preds[test_index] = train_oof_preds\n    cb_test_preds += test_oof_preds \/ n_folds\n    print(\": CATBOOST - RMSE Score = {}\".format(rmse(y_valid, train_oof_preds)))\n    \n#     ridge_model = CalibratedClassifierCV(\n#         RidgeClassifier(random_state=random_state),\n#         cv=3,\n#     )\n#     ridge_model.fit(\n#         ridge_x_train,\n#         y_train,\n#     )\n\n#     train_oof_preds = ridge_model.predict(ridge_x_valid)[:,-1]\n#     test_oof_preds = ridge_model.predict(test[ridge_features])\n#     ridge_train_preds[test_index] = train_oof_preds\n#     ridge_test_preds += test_oof_preds \/ n_folds\n#     print(\": Ridge - ROC AUC Score = {}\".format(roc_auc_score(y_valid, train_oof_preds, average=\"micro\")))\n    \n#     sgd_model = CalibratedClassifierCV(\n#         SGDClassifier(\n#             random_state=random_state,\n#             n_jobs=-1,\n#             loss=\"squared_hinge\",\n#         ),\n#         cv=3,\n#     )\n#     sgd_model.fit(\n#         sgd_x_train,\n#         y_train,\n#     )\n\n#     train_oof_preds = sgd_model.predict(sgd_x_valid)[:,-1]\n#     test_oof_preds = sgd_model.predict(test[sgd_features])[:,-1]\n#     sgd_train_preds[test_index] = train_oof_preds\n#     sgd_test_preds += test_oof_preds \/ n_folds\n#     print(\": SGD - ROC AUC Score = {}\".format(roc_auc_score(y_valid, train_oof_preds, average=\"micro\")))\n    \n    hgbc_model = HistGradientBoostingRegressor(\n        l2_regularization=1.766059063693552,\n        learning_rate=0.10675193678150449,\n        max_bins=128,\n        max_depth=31,\n        max_leaf_nodes=185,\n        random_state=2021\n    )\n    hgbc_model.fit(\n        hgbc_x_train,\n        y_train,\n    )\n\n    train_oof_preds = hgbc_model.predict(hgbc_x_valid)\n    test_oof_preds = hgbc_model.predict(test[hgbc_features])\n    hgbc_train_preds[test_index] = train_oof_preds\n    hgbc_test_preds += test_oof_preds \/ n_folds\n    print(\": hgbc - RMSE Score = {}\".format(rmse(y_valid, train_oof_preds)))\n    \n    \nprint(\"--> Overall metrics\")\nprint(\": XGB - rmse = {}\".format(rmse(target, xgb_train_preds)))\nprint(\": LGB - rmse = {}\".format(rmse(target, lgb_train_preds)))\nprint(\": CB - rmse = {}\".format(rmse(target, cb_train_preds)))\n# print(\": Ridge - ROC AUC Score = {}\".format(roc_auc_score(target, ridge_train_preds, average=\"micro\")))\n# print(\": SGD - ROC AUC Score = {}\".format(roc_auc_score(target, sgd_train_preds, average=\"micro\")))\nprint(\": HGBC - rmse= {}\".format(rmse(target, hgbc_train_preds)))\n","4f67603f":"\nfrom scipy.special import expit\nfrom sklearn.calibration import CalibratedClassifierCV\n\nrandom_state = 2021\nn_folds = 10\n# k_fold =KFold(n_splits=n_folds, random_state=random_state, shuffle=True)\n\nl1_train = pandas.DataFrame(data={\n    \"xgb\": xgb_train_preds.tolist(),\n    \"lgb\": lgb_train_preds.tolist(),\n    \"cb\": cb_train_preds.tolist(),\n#     \"ridge\": ridge_train_preds.tolist(),\n#     \"sgd\": sgd_train_preds.tolist(),\n    \"hgbc\": hgbc_train_preds.tolist(),\n    \"target\": target.tolist()\n})\nl1_test = pandas.DataFrame(data={\n    \"xgb\": xgb_test_preds.tolist(),\n    \"lgb\": lgb_test_preds.tolist(),\n    \"cb\": cb_test_preds.tolist(),\n#     \"sgd\": sgd_test_preds.tolist(),\n#     \"ridge\": ridge_test_preds.tolist(),    \n    \"hgbc\": hgbc_test_preds.tolist(),\n})\n\ntrain_preds = np.zeros(len(l1_train.index), )\ntest_preds1 = np.zeros(len(l1_test.index), )\nfeatures = [\"xgb\", \"lgb\",\"cb\", \"hgbc\"]\nx_train = pandas.DataFrame(l1_test[features])\ntest_preds1=(x_train[\"xgb\"]+x_train[\"lgb\"]+x_train[\"cb\"]+x_train[\"hgbc\"])\/4\n","beb3c8fc":"submission[\"Y2\"] = test_preds1.tolist()\nsubmission\nsubmission.to_csv(\"submission.csv\", index=False)","f4744236":"from tensorflow.keras.layers import Input,Dense,Dropout\nfrom tensorflow.keras import Model\nfrom  tensorflow.keras.regularizers import l2\nimport tensorflow as tf\nfrom numpy.random import seed\nseed(42)\nimport tensorflow as tf\ntf.random.set_seed(42)\nfrom tensorflow import keras\nimport numpy as np\nfrom keras import backend as K\n\n\ndef root_mean_squared_error(y_true, y_pred):\n         return K.sqrt(K.mean(K.square( (y_true - y_pred) )))\n    \n    \nfrom keras.backend import sigmoid\ndef swish(x, beta = 1):\n    return (x * sigmoid(beta * x))\nfrom keras.utils.generic_utils import get_custom_objects\nfrom keras.layers import Activation\nget_custom_objects().update({'swish': Activation(swish)})","ca2e1084":"features = [col for col in train.columns if col not in {\"Y1_mean\",\"Y2_mean\",\"UNIQUE_IDENTIFIER_\",\"SEQUENCE_NO_size\"}]","86cc8717":"def get_DAE():\n    # denoising autoencoder\n    inputs = Input((266,))\n    x = Dense(500, activation='swish')(inputs) # 1500 original\n    x = Dense(500, activation='swish', name=\"feature\")(x) # 1500 original\n    x = Dense(500, activation='swish')(x) # 1500 original\n    outputs = Dense(266, activation='linear')(x)\n    model = Model(inputs=inputs, outputs=outputs)\n    model.compile(optimizer='adam', loss=root_mean_squared_error)\n    return model","13dcb740":"alldata = pd.concat([train[features],test[features]],axis=0)\nprint(alldata.shape)\nautoencoder = get_DAE()\nautoencoder.fit(alldata[features], alldata[features],\n                    epochs=50,\n                    batch_size=256,\n                    shuffle=True\n                    )","a8f8cf93":"test_denoised = test.copy()\ntest[features] = autoencoder.predict(test_denoised[features])\ntrain_denoised = train.copy()\ntrain[features] = autoencoder.predict(train_denoised[features])","869fadf9":"\n\nfrom sklearn.model_selection import KFold\nfolds=[]\nkfold = KFold(n_splits = 5, random_state = 2021, shuffle = True)\nfor fold, (trn_ind, val_ind) in enumerate(kfold.split(train)):\n    print(trn_ind,val_ind)\n    folds.append([trn_ind,val_ind])\n\n","0b9d7cfc":"target=['Y1_mean','Y2_mean']","d7f31ea5":"# from sklearn.model_selection import KFold\nimport lightgbm as lgb\n\nseed0=2021\nparams0 = {\n    'objective': 'rmse',\n    'boosting_type': 'gbdt',\n    'max_depth': -1,\n    'max_bin':100,\n    'min_data_in_leaf':500,\n    'learning_rate': 0.05,\n    'subsample': 0.72,\n    'subsample_freq': 4,\n    'feature_fraction': 0.5,\n    'lambda_l1': 0.5,\n    'lambda_l2': 1.0,\n    'categorical_column':[0],\n    'seed':seed0,\n    'feature_fraction_seed': seed0,\n    'bagging_seed': seed0,\n    'drop_seed': seed0,\n    'data_random_seed': seed0,\n    'n_jobs':-1,\n    'verbose': -1}\nseed1=42\nparams1 = {\n        'learning_rate': 0.1,        \n        'lambda_l1': 2,\n        'lambda_l2': 7,\n        'num_leaves': 800,\n        'min_sum_hessian_in_leaf': 20,\n        'feature_fraction': 0.8,\n        'feature_fraction_bynode': 0.8,\n        'bagging_fraction': 0.9,\n        'bagging_freq': 42,\n        'min_data_in_leaf': 700,\n        'max_depth': 4,\n        'categorical_column':[0],\n        'seed': seed1,\n        'feature_fraction_seed': seed1,\n        'bagging_seed': seed1,\n        'drop_seed': seed1,\n        'data_random_seed': seed1,\n        'objective': 'rmse',\n        'boosting': 'gbdt',\n        'verbosity': -1,\n        'n_jobs':-1,\n    }\n# Function to early stop with root mean squared percentage error\ndef rmspe(y_true, y_pred):\n    return np.sqrt(np.mean(np.square((y_true - y_pred))))\n\ndef feval_rmspe(y_pred, lgb_train):\n    y_true = lgb_train.get_label()\n    return 'RMSPE', rmspe(y_true, y_pred), False\n# X=train[features]\n\ndef train_and_evaluate_lgb(train, test, params, features):\n    # Hyperparammeters (just basic)\n    y = train[target[0]]\n    X=train[features]\n    # Create out of folds array\n    oof_predictions = np.zeros(train.shape[0])\n    # Create test array to store predictions\n    test_predictions = np.zeros(test.shape[0])\n    # Create a KFold object\n    kfold = KFold(n_splits = 5, random_state = 2021, shuffle = True)\n    # Iterate through each fold\n    fold=0\n    for trn_ind,val_ind in folds:\n        print(f'Training fold {fold + 1}')\n        fold+=1\n        x_train, x_val = X.iloc[trn_ind], X.iloc[val_ind]\n        y_train, y_val = y.iloc[trn_ind], y.iloc[val_ind]\n        # Root mean squared percentage error weights\n        train_dataset = lgb.Dataset(x_train[features], y_train)\n        val_dataset = lgb.Dataset(x_val[features], y_val)\n        model = lgb.train(params = params,\n                          num_boost_round=2000,\n                          train_set = train_dataset, \n                          valid_sets = [train_dataset, val_dataset], \n                          verbose_eval = 250,\n                          early_stopping_rounds=250,\n                          feval = feval_rmspe)\n        # Add predictions to the out of folds array\n        oof_predictions[val_ind] = model.predict(x_val[features])\n        # Predict the test set\n        test_predictions += model.predict(test[features]) \/ 5\n    rmspe_score = rmspe(y, oof_predictions)\n    print(f'Our out of folds RMSPE is {rmspe_score}')\n    lgb.plot_importance(model,max_num_features=20)\n    # Return test predictions\n    return test_predictions\n# Traing and evaluate\nfeatures = [col for col in train.columns if col not in {\"Y1_mean\",\"Y2_mean\",\"UNIQUE_IDENTIFIER_\"}]\n#features = [col for col in features if col not in cols_drop]\npredictions_lgb= train_and_evaluate_lgb(train_denoised, test_denoised, params0, features)\ntest_denoised[target[0]] = predictions_lgb\n\n# cont_features =[col for col in train.columns if col not in {\"Y1_mean\",\"Y2_mean\",\"UNIQUE_IDENTIFIER_\",\"STATUS_CHECK_mean\",\"SEQUENCE_NO_size\"}]\n# cat_features = [\"STATUS_CHECK_mean\",\"SEQUENCE_NO_size\"]","6f58ebaf":"from sklearn.model_selection import KFold\nimport lightgbm as lgb\n\nseed0=2021\nparams0 = {\n    'objective': 'rmse',\n    'boosting_type': 'gbdt',\n    'max_depth': -1,\n    'max_bin':100,\n    'min_data_in_leaf':500,\n    'learning_rate': 0.05,\n    'subsample': 0.72,\n    'subsample_freq': 4,\n    'feature_fraction': 0.5,\n    'lambda_l1': 0.5,\n    'lambda_l2': 1.0,\n    'categorical_column':[0],\n    'seed':seed0,\n    'feature_fraction_seed': seed0,\n    'bagging_seed': seed0,\n    'drop_seed': seed0,\n    'data_random_seed': seed0,\n    'n_jobs':-1,\n    'verbose': -1}\nseed1=42\nparams1 = {\n        'learning_rate': 0.1,        \n        'lambda_l1': 2,\n        'lambda_l2': 7,\n        'num_leaves': 800,\n        'min_sum_hessian_in_leaf': 20,\n        'feature_fraction': 0.8,\n        'feature_fraction_bynode': 0.8,\n        'bagging_fraction': 0.9,\n        'bagging_freq': 42,\n        'min_data_in_leaf': 700,\n        'max_depth': 4,\n        'categorical_column':[0],\n        'seed': seed1,\n        'feature_fraction_seed': seed1,\n        'bagging_seed': seed1,\n        'drop_seed': seed1,\n        'data_random_seed': seed1,\n        'objective': 'rmse',\n        'boosting': 'gbdt',\n        'verbosity': -1,\n        'n_jobs':-1,\n    }\n# Function to early stop with root mean squared percentage error\ndef rmspe(y_true, y_pred):\n    return np.sqrt(np.mean(np.square((y_true - y_pred))))\n\ndef feval_rmspe(y_pred, lgb_train):\n    y_true = lgb_train.get_label()\n    return 'RMSPE', rmspe(y_true, y_pred), False\nX=train_denoised[features]\n\ndef train_and_evaluate_lgb(train, test, params, features):\n    # Hyperparammeters (just basic)\n    y = train[target[1]]\n    X=train[features]\n    # Create out of folds array\n    oof_predictions = np.zeros(train.shape[0])\n    # Create test array to store predictions\n    test_predictions = np.zeros(test.shape[0])\n    # Create a KFold object\n    kfold = KFold(n_splits = 5, random_state = 2021, shuffle = True)\n    # Iterate through each fold\n    fold=0\n    for trn_ind,val_ind in folds:\n        print(f'Training fold {fold + 1}')\n        fold+=1\n        x_train, x_val = X.iloc[trn_ind], X.iloc[val_ind]\n        y_train, y_val = y.iloc[trn_ind], y.iloc[val_ind]\n        # Root mean squared percentage error weights\n        train_dataset = lgb.Dataset(x_train[features], y_train)\n        val_dataset = lgb.Dataset(x_val[features], y_val)\n        model = lgb.train(params = params,\n                          num_boost_round=2000,\n                          train_set = train_dataset, \n                          valid_sets = [train_dataset, val_dataset], \n                          verbose_eval = 250,\n                          early_stopping_rounds=500,\n                          feval = feval_rmspe)\n        # Add predictions to the out of folds array\n        oof_predictions[val_ind] = model.predict(x_val[features])\n        # Predict the test set\n        test_predictions += model.predict(test[features]) \/ 5\n    rmspe_score = rmspe(y, oof_predictions)\n    print(f'Our out of folds RMSPE is {rmspe_score}')\n    lgb.plot_importance(model,max_num_features=20)\n    # Return test predictions\n    return test_predictions\n# Traing and evaluate\nfeatures = [col for col in train.columns if col not in {\"Y1_mean\",\"Y2_mean\",\"UNIQUE_IDENTIFIER_\"}]\n#features = [col for col in features if col not in cols_drop]\npredictions_lgb= train_and_evaluate_lgb(train_denoised, test_denoised, params0, features)\ntest_denoised[target[1]] = predictions_lgb","b5d990b3":"submission1 = pandas.read_csv(\"..\/input\/beyond-analysis\/sample_submission_random.csv\")\nsubmission1['Y1']=0.50*submission['Y1']+0.50*test_denoised['Y1_mean']\nsubmission1['Y2']=0.50*submission['Y2']+0.50*test_denoised['Y2_mean']","36b6d02e":"submission1.to_csv(\"submission1.csv\", index=False)","82b10638":"# **CORRELATION OBSERVATIONS**\n* there is sinificant correlation between Y2 and sequence size and status check.\n* There seems to be huge correlation between **ENTRY ,REVENUE AND WINNINGS_1**\n* Y2 seems to be fairly correlated with **SEQUENCE_NO,STATUS_CHECK** \n* there seems to be fair correlation between variouse columns hence combined polynomial featues can be build \n","953e4b90":"# **FEATURE ENGINEERING\/EXTRACTION**","0b10b39c":"# **Check NULL values in our data**","7c1578b8":"# **Check NULL values in our data**","4218cdbe":"# **GROUPING**\n* Grouping the data on the basis of unique identifer, this is surely going to reduce the 0 values in the data and possibly introduce less skewed variables it helped me in reducing standard deviation.","b309c8bc":"\n# **Scaling Data**\n\n* **QuantileTransformer**\n\nTransform features using quantiles information.\n\nThis method transforms the features to follow a uniform or a normal distribution. Therefore, for a given feature, this transformation tends to spread out the most frequent values. It also reduces the impact of (marginal) outliers: this is therefore a robust preprocessing scheme.\n\nThe transformation is applied on each feature independently. First an estimate of the cumulative distribution function of a feature is used to map the original values to a uniform distribution. The obtained values are then mapped to the desired output distribution using the associated quantile function. Features values of new\/unseen data that fall below or above the fitted range will be mapped to the bounds of the output distribution. Note that this transform is non-linear. It may distort linear correlations between variables measured at the same scale but renders variables measured at different scales more directly comparable.\n","36a92b81":"# OBSERVATIONS:\n*  based on y1 and y2 there are outliers in our data.\n*  I had an idea to produce startified kfold based on clusters i performed it but it didn't worked well so i didn't proceed with it","ab27f94a":"# **OBSERVATIONS:**\n* There is positively skewed skewness in the metrices to be predicted,the reason is for a particular id there are many sparse values so this also indicates that we should try grouping our data to remove the skewness.\n* for metrix Y2 prob plot showed logistic distribution between observed and theoritical values\n* There are huge  outliers in our data (for ex there is a value above 800 that should be removed).","36123fc2":"# **Lets gain information about the columns to be predicted**","603eb9fc":"# **OBSERVATIONS:**\n* all the continuous features are also left skewed they reason is obvioiusly because of huge amount of 0 values present in the data.\n* we can try to remove this skewness by converting the scaling the data with the help of quantile transformer.(in processing section).\n* also since it is required that we have to predict one value for each unique identifier we can also group the data which significantly reduces the 0 values.","d4918e0a":"# Description\n* To deal with categorical features i made a pipeline containing leave one out encoder and label encoder however i didnt felt the need of using it since all the features i used were continuous except category1 and category2.I used label encoder before grouping the data on the categorical features.","25d044fd":"# **MODEL PREPARATION**","72fe1ce9":"**OBSERVATIONS** \n* **Y1 and Y2 are labels to be predicted** \n* **This is a r problem**\n* **there are no missing values in test and train set**","816d15de":"# **Description of columns**\n* pct is simply the difference between deposit and entry(important).\n* fract is the ratio fun contest wins and the fun contests participated.\n* fract1 is the ratio  contest wins and the fun contests participated(very important).\n* fract2 is the ratio  contest wins and the fun contests participated.\n* paid is difference of revenue and discount\n* fract4 is difference of  entry and winnings1(total chips wins)","41566492":"# **PREPROCESSING UNGROUPED DATA**","98fef455":"# **Check data types of values values in our data**","5eb48d7f":"# MODELS DESCRIPTION\n* four models were used and there output was averaged\n>      1.   lightgbm regressor\n>      2.   catboost regressor\n>      3.   xgboost regressor\n>      4.   hyper gradient boosting regressor\n* one pipeline was for predicting Y1 and other for prediction Y2\n","193c39d0":"# **OBSERVATIONS**\n* Not all the values in train and test data are same.\n\n* We can perform simple label encoding on the categorical features.","ef910d51":"#                                                 **FINAL MODEL+EDA+OBSERVATIONS+ENSEMBLE**","8dbc4479":"# **Categorical Features** ","353e29d6":"# **OBSERVATIONS**\n* there is a huge standard deviation in our Y2 column so with reference to the evaluation metric of the competition it is more important to reduce the RMSE of Y2 metric than Y1 metric\n* so we recieved a significant improvement in the variance of both columns if we group the data on the basis of unique identifier.\n* this will lead to loss of information therefore the main crux we have to do is how can we retain a lot of information for my model to perform better we can use min, max, mean, std values, appart from that we can use a lot amount of feature engineering which is done below","4a31c305":"# **DAE is Used to add diversity to the model**","260656b5":"# **CV OBSERVATIONS**\n*  cv was correlating with public leaderboard.\n* i was able to get best rmse for y1 round of  6.70 and for y2 =119.31","d07cea6a":"# **Lets check shape of data**"}}