{"cell_type":{"0d9b7909":"code","03c2de8f":"code","6f562810":"code","b9f18ee3":"code","3876a566":"code","190488e6":"code","437d79b6":"code","c41d3e0a":"code","2cbb76d5":"code","874765d1":"code","8dcd7183":"code","c41bb4e4":"code","051f302b":"code","9264acde":"code","55a8e427":"code","e9d140eb":"code","42321935":"code","c789432c":"code","190d4a90":"code","13e8c032":"code","2f9158f4":"code","fcc7c6d0":"code","d84a4e3e":"code","00c7a71a":"code","ecc6b6f9":"code","4ad3f45e":"code","0dbf56a0":"code","4d6dd6e1":"code","60e29474":"code","43ef62f7":"code","394268eb":"code","f0df3979":"code","0686fe70":"code","0e1c7084":"code","af366e3f":"code","f9c785a3":"code","f43ea33a":"code","caa711f1":"code","7f8c339c":"code","09cdd8e9":"code","77b40702":"code","e3533b24":"code","4de01c7a":"code","52f7b171":"code","23bfd1e3":"code","bbfc05a2":"code","049faac6":"code","652d9f32":"code","d9789bd0":"code","06bb7f85":"code","3dcbe087":"code","50d0d512":"code","20386000":"code","2d8a5705":"code","2f85ac63":"code","2f493e42":"code","8a832270":"code","761cc83f":"code","7efd2852":"code","bc485708":"code","144a06f7":"code","4531c8ac":"code","21ac7c4b":"code","b8aeda3c":"code","022a895e":"code","4f26e185":"code","6c6c21ad":"code","d8961d4e":"code","a68a4456":"code","9d28a1ec":"code","7b7039e4":"code","2c358e9d":"code","b2cd1111":"code","13fa35d2":"code","09f6d637":"code","ecd46daf":"code","1b7fa1f6":"code","41e5f9d8":"code","bd950c6d":"code","69c63077":"code","0ef428d5":"code","cb06624b":"markdown","58b4cc3c":"markdown","eb84de37":"markdown","50ece6bb":"markdown","22d4ba33":"markdown","c9f287a0":"markdown","25f7e252":"markdown","ee941723":"markdown","e3c86e04":"markdown","ff95c100":"markdown","3be188dc":"markdown","48d70c8c":"markdown","d3cd150f":"markdown","648f54c6":"markdown","841e9e05":"markdown","ce68e6ee":"markdown","834864d7":"markdown","d699e1c0":"markdown","5adf81be":"markdown","5ed96c35":"markdown","3993a2bc":"markdown","9e5007ff":"markdown","c43f4acf":"markdown","670e6529":"markdown","7e56ac0a":"markdown","3e732d1b":"markdown","ff4b76a4":"markdown","7e0a5a6a":"markdown","914d935b":"markdown","b2696cbb":"markdown","4ac6983e":"markdown","c116a87a":"markdown","828c8416":"markdown","fcb6cd72":"markdown","241d3674":"markdown","4d8b2612":"markdown","f1c5b931":"markdown","72e6d674":"markdown","d07846ef":"markdown","5e0ef3a8":"markdown","6d7b2b64":"markdown","8c9a7f44":"markdown","0e8ed0d7":"markdown","baba5622":"markdown","88b74d76":"markdown","c345f03d":"markdown","9dec432c":"markdown","b055ad70":"markdown","ef9aca58":"markdown","cddead8d":"markdown","7f06d6ac":"markdown","5cc48424":"markdown","bc2c2cf8":"markdown","9961674c":"markdown","aa57d319":"markdown","e371f653":"markdown","e573f265":"markdown","d5887a9a":"markdown","a9580140":"markdown","1d20af48":"markdown","c43e5dc5":"markdown","da122834":"markdown","debc9e44":"markdown","ff748edb":"markdown","61ef1a88":"markdown","2430ade2":"markdown","9fcf1ebb":"markdown","16179175":"markdown","23242132":"markdown","fb28e611":"markdown","341818cd":"markdown","76d0cd41":"markdown","5bac854f":"markdown","ed10e82a":"markdown","6117b554":"markdown","1d3a83e3":"markdown","e2eb2d96":"markdown","b7dce498":"markdown","5977b44b":"markdown"},"source":{"0d9b7909":"# Importing the necessary libraries\nimport numpy as np #importing numpy library\nimport pandas as pd  # To read the dataset as dataframe\nimport seaborn as sns # For Data Visualization \nimport matplotlib.pyplot as plt # Necessary module for plotting purpose\n%matplotlib inline\nfrom sklearn.model_selection import train_test_split # For train-test split\n# getting methods for confusion matrix, F1 score, Accuracy Score\nfrom sklearn.metrics import confusion_matrix,f1_score,accuracy_score, classification_report\nfrom sklearn.linear_model import LogisticRegression # For logistic Regression\nfrom sklearn.naive_bayes import GaussianNB # For Naive Bayes classifier\nfrom sklearn.neighbors import KNeighborsClassifier # For K-NN Classifier\nfrom sklearn.svm import SVC # For support vector machine based classifier","03c2de8f":"# Reading the data as a data frame\ndf_orig = pd.read_excel('..\/input\/bank-loan-modelling\/Bank_Personal_Loan_Modelling.xlsx',sheet_name = 'Data')\ndf_orig.head()","6f562810":"# Creating copy of original dataframe \ndf = df_orig.copy()\n# For our convinience, let us make target attribute \"Personal Loan\" as the last column \n# of our dataframe.\ndf.drop('Personal Loan',axis=1,inplace=True)\ndf['Personal Loan'] = df_orig['Personal Loan']","b9f18ee3":"df.head()","3876a566":"# Shape of dataframe\ndf.shape","190488e6":"# More info about columns\ndf.info()","437d79b6":"# The column (attribute) names are:\nfor name in df.columns:\n    print(name)","c41d3e0a":"# Let us see datatypes of the column attributes\ndf.dtypes","2cbb76d5":"# Number of unique datatypes and their value count\ndf.dtypes.value_counts()","874765d1":"# Let us check the dataset for missing values\ndf.isnull().sum()","8dcd7183":"# Let us see the 5-point summary of for the attributes\ndf.describe()","c41bb4e4":"# Let us see how many negative entries are there in the Experience column\ndf[df['Experience']<0].Experience.count()","051f302b":"# Let us see how many unique negative entries are there?\ndf[df['Experience']<0].Experience.value_counts()","9264acde":"df.corr()","55a8e427":"# Above table represented more elegently using heatmap\nplt.figure(figsize=(12,10))\nsns.heatmap(df.corr(),annot=True)","e9d140eb":"# Let us find the unique ages which have -1, -2 and -3 entries in the Experience column\ndf[df['Experience'] == -1]['Age'].value_counts()","42321935":"# We will find the mean of positive experience values for above ages and use it to replace all the experience entries \n# having -1 value\nl1 = df[df['Experience'] == -1]['Age'].value_counts().index.tolist()\nind_1 = df[df['Experience'] == -1]['Experience'].index.tolist()\nfor i in ind_1:\n    df.loc[i,'Experience'] = df[(df['Age'].isin(l1)) & (df.Experience > 0)].Experience.mean()","c789432c":"# Let us check the values are correctly replaced.\ndf[df['Experience'] == -1]['Age'].value_counts()","190d4a90":"df[df['Experience'] == -2]['Age'].value_counts()","13e8c032":"# We will find the mean of positive experience values for above ages and use it to replace all the experience entries \n# having -2 value\nl2 = df[df['Experience'] == -2]['Age'].value_counts().index.tolist()\nind_2 = df[df['Experience'] == -2]['Experience'].index.tolist()\nfor i in ind_2:\n    df.loc[i,'Experience'] = df[(df['Age'].isin(l2)) & (df.Experience > 0)].Experience.mean()","2f9158f4":"df[df['Experience'] == -3]['Age'].value_counts()","fcc7c6d0":"# We will find the mean of positive experience values for above ages and use it to replace all the experience entries \n# having -3 value\nl3 = df[df['Experience'] == -3]['Age'].value_counts().index.tolist()\nind_3 = df[df['Experience'] == -3]['Experience'].index.tolist()\nfor i in ind_3:\n    df.loc[i,'Experience'] = df[(df['Age'].isin(l3)) & (df.Experience > 0)].Experience.mean()","d84a4e3e":"df.Experience.describe()","00c7a71a":"# Let us see the distribution 5000 entries in target column\ndf['Personal Loan'].value_counts()","ecc6b6f9":"# The column attribute \"ID\" doesn't provide any significant information about a customer\n# buying a personal loan hence we will skip analysis of the same.","4ad3f45e":"plt.figure(figsize=(12,8))\nsns.distplot(df[df['Personal Loan'] == 0]['Age'],kde=False, color='b', label='Personal Loan=0')\nsns.distplot(df[df['Personal Loan'] == 1]['Age'],kde=False, color='r',label='Personal Loan=1')\nplt.legend()\nplt.title(\"Age Distribution\")","0dbf56a0":"age_cut = pd.cut(df['Age'],bins=[20,30,40,50,60])\npd.crosstab(age_cut,df['Personal Loan']).apply(lambda r: r\/r.sum()*100, axis=1)","4d6dd6e1":"plt.figure(figsize=(12,8))\nsns.distplot(df[df['Personal Loan'] == 0]['Experience'],kde=False, color='b', label='Personal Loan=0')\nsns.distplot(df[df['Personal Loan'] == 1]['Experience'],kde=False, color='r',label='Personal Loan=1')\nplt.legend()\nplt.title(\"Experience Distribution\")","60e29474":"exp_cut = pd.cut(df['Experience'],bins=[0,10,20,30,40,50])\npd.crosstab(exp_cut,df['Personal Loan']).apply(lambda r: r\/r.sum()*100, axis=1)","43ef62f7":"plt.figure(figsize=(12,8))\nsns.distplot(df[df['Personal Loan'] == 0]['Income'],kde=False, color='b', label='Personal Loan=0')\nsns.distplot(df[df['Personal Loan'] == 1]['Income'],kde=False, color='r',label='Personal Loan=1')\nplt.legend()\nplt.title(\"Income Distribution\")","394268eb":"inc_cut = pd.cut(df['Income'],bins=[0,50,100,150,200,250])\npd.crosstab(inc_cut,df['Personal Loan']).apply(lambda r: r\/r.sum()*100, axis=1)","f0df3979":"# Since it is a ordinal categorical variable, we will use countplot\nsns.countplot(x='Family',hue='Personal Loan',data=df)","0686fe70":"pd.crosstab(df['Family'],df['Personal Loan']).apply(lambda r: r\/r.sum()*100, axis=1)","0e1c7084":"# Similar to ID attribute, we drop ZIP Code column analysis since it is not relevant to customer \n# buying the personal loan ","af366e3f":"plt.figure(figsize=(12,8))\nsns.distplot(df[df['Personal Loan'] == 0]['CCAvg'],kde=False, color='b', label='Personal Loan=0')\nsns.distplot(df[df['Personal Loan'] == 1]['CCAvg'],kde=False, color='r',label='Personal Loan=1')\nplt.legend()\nplt.title(\"CCAvg Distribution\")","f9c785a3":"ccavg_cut = pd.cut(df['CCAvg'],bins=[0,2,4,6,8,10])\npd.crosstab(ccavg_cut,df['Personal Loan']).apply(lambda r: r\/r.sum()*100, axis=1)","f43ea33a":"# Since Education is an ordinal categorical variable, we will use countplot\nsns.countplot(df['Education'],hue=df['Personal Loan'])","caa711f1":"pd.crosstab(df['Education'],df['Personal Loan']).apply(lambda r: r\/r.sum()*100, axis=1)","7f8c339c":"plt.figure(figsize=(8,8))\nsns.distplot(df[df['Personal Loan'] == 0]['Mortgage'],kde=False, color='b', label='Personal Loan=0')\nsns.distplot(df[df['Personal Loan'] == 1]['Mortgage'],kde=False, color='r',label='Personal Loan=1')\nplt.legend()\nplt.title(\"Mortgage Distribution\")","09cdd8e9":"mort_cut = pd.cut(df['Mortgage'],bins=[0,100,200,300,400,500,600])\npd.crosstab(mort_cut,df['Personal Loan']).apply(lambda r: r\/r.sum()*100, axis=1)","77b40702":"plt.figure(figsize=(12,12))\nplt.subplot(2,2,1)\nsns.countplot(df['Securities Account'],hue=df['Personal Loan'])\nplt.subplot(2,2,2)\nsns.countplot(df['CD Account'],hue=df['Personal Loan'])\nplt.subplot(2,2,3)\nsns.countplot(df['Online'],hue=df['Personal Loan'])\nplt.subplot(2,2,4)\nsns.countplot(df['CreditCard'],hue=df['Personal Loan'])","e3533b24":"pd.crosstab(df['Securities Account'],df['Personal Loan']).apply(lambda r: r\/r.sum()*100, axis=1)","4de01c7a":"pd.crosstab(df['CD Account'],df['Personal Loan']).apply(lambda r: r\/r.sum()*100, axis=1)","52f7b171":"pd.crosstab(df['Online'],df['Personal Loan']).apply(lambda r: r\/r.sum()*100, axis=1)","23bfd1e3":"pd.crosstab(df['CreditCard'],df['Personal Loan']).apply(lambda r: r\/r.sum()*100, axis=1)","bbfc05a2":"sns.FacetGrid(data=df,row='Education',col='Family',hue='Personal Loan').map(plt.hist,'Income').add_legend()","049faac6":"sns.boxplot(x='Education',y='Income',hue='Personal Loan',data=df)","652d9f32":"sns.boxplot(x='Family',y='Income',hue='Personal Loan',data=df)","d9789bd0":"# Before moving further, let us plot the pairplot using all attributes\nsns.pairplot(df_orig,hue='Personal Loan',diag_kind='hist')","06bb7f85":"# Let us see the correlation of all independent attributes with target attribute i.e., personal loan \ndf.corr().loc['Personal Loan',:].sort_values(ascending=False)","3dcbe087":"# Significance test for numerical columns\nimport statsmodels.api as sm\ndf_num = df.loc[:,['Personal Loan', 'Income', 'CCAvg', 'CD Account', 'Mortgage', 'Education', 'Family', 'Securities Account', 'Age']]\ndf_num['intercept'] = 1\nlog_mod = sm.Logit(df_num['Personal Loan'], df_num[['intercept', 'Income', 'CCAvg', 'Mortgage', 'Age']]).fit()","50d0d512":"log_mod.summary()","20386000":"# Let see the statistical significance of ordinal categorical variables Family and Education\ndf_ordc = df.loc[:,['Personal Loan','Family','Education']]\ndf_ordc['intercept'] = 1\nlog_mod = sm.Logit(df_ordc['Personal Loan'], df_ordc[['intercept', 'Family', 'Education']]).fit()","2d8a5705":"log_mod.summary()","2f85ac63":"df_bc = df.loc[:,['Personal Loan','CD Account','Securities Account']]\ndf_bc['intercept'] = 1\nlog_mod = sm.Logit(df_bc['Personal Loan'], df_bc[['intercept', 'CD Account','Securities Account']]).fit()","2f493e42":"log_mod.summary()","8a832270":"df.head()","761cc83f":"# Since target attribute is binary in nature, let us see count for each class\ndf['Personal Loan'].value_counts()","7efd2852":"# Converting above target class distribution as dataframe\ndf_target = df['Personal Loan'].value_counts()\ndf_target = pd.DataFrame({'class':df_target.index, 'count':df_target.values})","bc485708":"df_target","144a06f7":"# barplot for target column distribution\nsns.barplot(x='class',y = 'count',data=df_target);","4531c8ac":"# Let us add the percentage column to the dataframe.\ndf_target['Percentage'] = df_target['count']\/df_target['count'].sum()*100\ndf_target","21ac7c4b":"# Let us plot the Pie Plot\nplt.pie(df_target['Percentage'],labels=['No Personal Loan','Personal Loan'],autopct= '%1.1f%%');","b8aeda3c":"# Train test split\n# We will drop the Age, ID columns from training as well as test dataset\nX = df.iloc[:,2:-1]\ny = df['Personal Loan']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state = 7 )","022a895e":"# create model using constructor\nLogRegModel = LogisticRegression()\n# fit the model to training set\nLogRegModel.fit(X_train,y_train)\n# Predict the test data to get y_pred\ny_pred = LogRegModel.predict(X_test)\n# get accuracy of model\nlr_acc_score = accuracy_score(y_test,y_pred)\n# get F1-score of model\nlr_f1_score = f1_score(y_test,y_pred) \n# get the confusion matrix\nlr_confmat = confusion_matrix(y_test,y_pred)\n# get the classification report\nlr_classrep = classification_report(y_test,y_pred)\n\nprint(\"The accuracy of the model is {} %\".format(lr_acc_score*100))\nprint(\"The f1-score of the model is {} %\".format(lr_f1_score*100))\nprint(\"The confusion matrix for logistic regression is: \\n\",lr_confmat)\nprint(\"Detailed classification report for logistic regression is: \\n\",lr_classrep)","4f26e185":"# create model using constructor\nNBModel = GaussianNB()\n# fit the model to training set\nNBModel.fit(X_train,y_train)\n# Predict the test data to get y_pred\ny_pred = NBModel.predict(X_test)\n# get accuracy of model\nnb_acc_score = accuracy_score(y_test,y_pred)\n# get F1-score of model\nnb_f1_score = f1_score(y_test,y_pred) \n# get the confusion matrix\nnb_confmat = confusion_matrix(y_test,y_pred)\n# get the classification report\nnb_classrep = classification_report(y_test,y_pred)\n\nprint(\"The accuracy of the model is {} %\".format(nb_acc_score*100))\nprint(\"The f1-score of the model is {} %\".format(nb_f1_score*100))\nprint(\"The confusion matrix for Naive Bayes classifier is: \\n\",nb_confmat)\nprint(\"Detailed classification report for Naive Bayes classifier is: \\n\",nb_classrep)","6c6c21ad":"# create model using constructor\nKNNModel = KNeighborsClassifier() # Calling default constructor\n# fit the model to training set\nKNNModel.fit(X_train,y_train)\n# Predict the test data to get y_pred\ny_pred = KNNModel.predict(X_test)\n# get accuracy of model\nknn_acc_score = accuracy_score(y_test,y_pred)\n# get F1-score of model\nknn_f1_score = f1_score(y_test,y_pred) \n# get the confusion matrix\nknn_confmat = confusion_matrix(y_test,y_pred)\n# get the classification report\nknn_classrep = classification_report(y_test,y_pred)\n\nprint(\"The accuracy of the model is {} %\".format(knn_acc_score*100))\nprint(\"The f1-score of the model is {} %\".format(knn_f1_score*100))\nprint(\"The confusion matrix for K-NN classifier is: \\n\",knn_confmat)\nprint(\"Detailed classification report for K-NN classifier is: \\n\",knn_classrep)","d8961d4e":"df_comp = pd.DataFrame({'Classification Algorithm':['Logistic Regression', 'Naive Bayes', 'K-Nearest Neighbor'],'Accuracy (%)':[lr_acc_score*100,nb_acc_score*100,knn_acc_score*100],'f1-score (%)':[lr_f1_score*100,nb_f1_score*100,knn_f1_score*100]})\n\nprint(\"Following table shows comparison of the classification algorithms (using unscaled data and default parameters): \")\ndf_comp","a68a4456":"plt.figure(figsize=(12,10))\nsns.heatmap(X.corr(),annot=True)","9d28a1ec":"# KNN Accuracy for neighbors = 1,3,...99\nknn_acc=[]\nknn_f1 = []\nfor i in range(1,100,2):\n    print(\"Calculating the K-NN classifier accuracy for {} neighbors.\".format(i))\n    # create model using constructor\n    KNNModel = KNeighborsClassifier(n_neighbors=i) # Calling default constructor\n    # fit the model to training set\n    KNNModel.fit(X_train,y_train)\n    # Predict the test data to get y_pred\n    y_pred = KNNModel.predict(X_test)\n    # get accuracy of model\n    knn_acc_score = accuracy_score(y_test,y_pred)\n    knn_acc.append(knn_acc_score*100)\n    # get F1-score of model\n    knn_f1_score = f1_score(y_test,y_pred) \n    knn_f1.append(knn_f1_score*100)\ndf_knn = pd.DataFrame({'n_neighbors':list(range(1,100,2)), 'Accuracy':knn_acc,'f1-score':knn_f1})  ","7b7039e4":"df_knn","2c358e9d":"# Let us scale train as well as test data using MinMaxScaler\nfrom sklearn.preprocessing import MinMaxScaler\nscaler = MinMaxScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.fit_transform(X_test)","b2cd1111":"# Repeating all three algorithms again on the scaled data","13fa35d2":"# create model using constructor\nLogRegModel = LogisticRegression()\n# fit the model to training set\nLogRegModel.fit(X_train_scaled,y_train)\n# Predict the test data to get y_pred\ny_pred = LogRegModel.predict(X_test_scaled)\n# get accuracy of model\nlr_acc_score = accuracy_score(y_test,y_pred)\n# get F1-score of model\nlr_f1_score = f1_score(y_test,y_pred) \n# get the confusion matrix\nlr_confmat = confusion_matrix(y_test,y_pred)\n# get the classification report\nlr_classrep = classification_report(y_test,y_pred)\n\nprint(\"The accuracy of the model is {} %\".format(lr_acc_score*100))\nprint(\"The f1-score of the model is {} %\".format(lr_f1_score*100))\nprint(\"The confusion matrix for logistic regression is: \\n\",lr_confmat)\nprint(\"Detailed classification report for logistic regression is: \\n\",lr_classrep)","09f6d637":"# create model using constructor\nNBModel = GaussianNB()\n# fit the model to training set\nNBModel.fit(X_train_scaled,y_train)\n# Predict the test data to get y_pred\ny_pred = NBModel.predict(X_test_scaled)\n# get accuracy of model\nnb_acc_score = accuracy_score(y_test,y_pred)\n# get F1-score of model\nnb_f1_score = f1_score(y_test,y_pred) \n# get the confusion matrix\nnb_confmat = confusion_matrix(y_test,y_pred)\n# get the classification report\nnb_classrep = classification_report(y_test,y_pred)\n\nprint(\"The accuracy of the model is {} %\".format(nb_acc_score*100))\nprint(\"The f1-score of the model is {} %\".format(nb_f1_score*100))\nprint(\"The confusion matrix for Naive Bayes classifier is: \\n\",nb_confmat)\nprint(\"Detailed classification report for Naive Bayes classifier is: \\n\",nb_classrep)","ecd46daf":"# create model using constructor\nKNNModel = KNeighborsClassifier() # Calling default constructor\n# fit the model to training set\nKNNModel.fit(X_train_scaled,y_train)\n# Predict the test data to get y_pred\ny_pred = KNNModel.predict(X_test_scaled)\n# get accuracy of model\nknn_acc_score = accuracy_score(y_test,y_pred)\n# get F1-score of model\nknn_f1_score = f1_score(y_test,y_pred) \n# get the confusion matrix\nknn_confmat = confusion_matrix(y_test,y_pred)\n# get the classification report\nknn_classrep = classification_report(y_test,y_pred)\n\nprint(\"The accuracy of the model is {} %\".format(knn_acc_score*100))\nprint(\"The f1-score of the model is {} %\".format(knn_f1_score*100))\nprint(\"The confusion matrix for K-NN classifier is: \\n\",knn_confmat)\nprint(\"Detailed classification report for K-NN classifier is: \\n\",knn_classrep)","1b7fa1f6":"# KNN Accuracy for neighbors = 1,3,...99\nknn_acc=[]\nknn_f1 = []\nfor i in range(1,100,2):\n    print(\"Calculating the K-NN classifier accuracy for {} neighbors.\".format(i))\n    # create model using constructor\n    KNNModel = KNeighborsClassifier(n_neighbors=i) # Calling default constructor\n    # fit the model to training set\n    KNNModel.fit(X_train_scaled,y_train)\n    # Predict the test data to get y_pred\n    y_pred = KNNModel.predict(X_test_scaled)\n    # get accuracy of model\n    knn_acc_score = accuracy_score(y_test,y_pred)\n    knn_acc.append(knn_acc_score*100)\n    # get F1-score of model\n    knn_f1_score = f1_score(y_test,y_pred) \n    knn_f1.append(knn_f1_score*100)\ndf_knn = pd.DataFrame({'n_neighbors':list(range(1,100,2)), 'Accuracy':knn_acc,'f1-score':knn_f1})","41e5f9d8":"df_knn","bd950c6d":"# create model using constructor\nSVMModel = SVC() # Calling default constructor\n# fit the model to training set\nSVMModel.fit(X_train_scaled,y_train)\n# Predict the test data to get y_pred\ny_pred = SVMModel.predict(X_test_scaled)\n# get accuracy of model\nsvm_acc_score = accuracy_score(y_test,y_pred)\n# get F1-score of model\nsvm_f1_score = f1_score(y_test,y_pred) \n# get the confusion matrix\nsvm_confmat = confusion_matrix(y_test,y_pred)\n# get the classification report\nsvm_classrep = classification_report(y_test,y_pred)\n\nprint(\"The accuracy of the model is {} %\".format(svm_acc_score*100))\nprint(\"The f1-score of the model is {} %\".format(svm_f1_score*100))\nprint(\"The confusion matrix for SVM classifier is: \\n\",svm_confmat)\nprint(\"Detailed classification report for SVM classifier is: \\n\",svm_classrep)","69c63077":"svm_acc=[]\nsvm_f1 = []\nfor i in range(1,1000,100):\n    print(\"Calculating the SVM classifier accuracy for C = {}.\".format(i))\n    # create model using constructor\n    SVMModel = SVC(C=i) # Calling default constructor\n    # fit the model to training set\n    SVMModel.fit(X_train_scaled,y_train)\n    # Predict the test data to get y_pred\n    y_pred = SVMModel.predict(X_test_scaled)\n    # get accuracy of model\n    svm_acc_score = accuracy_score(y_test,y_pred)\n    svm_acc.append(svm_acc_score*100)\n    # get F1-score of model\n    svm_f1_score = f1_score(y_test,y_pred) \n    svm_f1.append(svm_f1_score*100)\ndf_svm = pd.DataFrame({'C':list(range(1,1000,100)), 'Accuracy':svm_acc,'f1-score':svm_f1})","0ef428d5":"df_svm","cb06624b":"# For Support Vector Classifier with C = 1, we get accuracy of 97.46% and f1-score around 85% which is the best result for the given problem in comparison with the Logistic Regression, Naive Bayes Classifier and K-NN Classifiers used in this notebook.","58b4cc3c":"**If we consider the algorithm comparison with their default parameters and unscaled data, Naive bayes classifier works best which has accuracy of 88% and f1-score of 47.67%. The reason it performs better because Naive Bayes classifier assumes the independence of the X variables (i.e., independent variables) and for this particular problem this is approximately true which is clear from the following heatmap**","eb84de37":"**Similar to Online attribute, customer using using a credit card issued by UniversalBank has no effect on buying the personal loans.**","50ece6bb":"**Analyzing the Income attribute**","22d4ba33":"**In each education category, one can see that customers with higher the income tend to buy personal Loans.**","c9f287a0":"## Comparison of Classification Algorithms used for this problem","25f7e252":"## 2) Getting the target column distribution","ee941723":"#### Now we have a clean data to work with...!","e3c86e04":"**One can see that except CCAvg and income, there is no significant correlation among other column attributes of X**","ff95c100":"**Looks like customer using internet banking facilities has no effect on buying the personal loans..!**","3be188dc":"## Reasoning for the best model and why it performs better? (Assuming unscaled data and algorithms with default parameters)","48d70c8c":"### Logistic Regression","d3cd150f":"**By closely observing the data and description given about each column attribute we can say that:**\n* **Numeric data columns (Interval or Ratio) are: Age, Experience, Income, Mortgage and CCAvg**\n* **Ordinal Categorical columns are: Family and Education**\n* **Nominal Categorical columns are: ID, ZIP Code, Securities Account, CD Account, Online, CreditCard, Personal Loan**","648f54c6":"**Analyzing the Family attribute**","841e9e05":"## Logistic Regression","ce68e6ee":"**Customers with CD account have very high percentage (46.3%) of buying the personal loan than the customers with no CD account (7.2%)**","834864d7":"**K-NN for different values of neighbors**","d699e1c0":"**Customers with Securities account have slightly higher percentage of buying the personal loan than the customers with no Securities account**","5adf81be":"**About metrics used for evaluation** \n\nIn order to minimize the cost of campaigning for the bank, a classifation algorithm in this context, should minimize both false positives (FP) as well as false negatives (FN). Hence, along with accuracy score, we will also observe f1-score. Ideally, both accuracy as well as f1-score should be 1 (or 100 in terms of percentage).","5ed96c35":"**One can see that, $p$-values for Income, CCAvg are less than $\\alpha = 0.05$. Hence with 95% confidence, we can say that they are significant for predicting the target attribute class. Age attribute seems to be insignificant for the given problem**","3993a2bc":"**Playing with the C value of SVC constructor**","9e5007ff":"## Learning Outcomes:\n\n* Exploratory Data Analysis\n* Preparing the data to train a model\n* Training and making predictions using a classification model\n* Model evaluation","c43f4acf":"**From above table as well as distribution plot of Age attribute, one can observe that most of the customers lie in the age group of 30 to 60. Also,one can observe that 10.5% of the total customers in age group 20-30 have bought personal loan from the bank, while in age groups (30-40), (40-50) and (50-60), there is a conversion rate of around 9%.**","670e6529":"## 4) Split the data into training and test set in the ratio of 70:30 respectively","7e56ac0a":"**From qualitative and quantitative exploratory data analysis done until now, one can see that Income and CCAvg are two most important features deciding whether a customer will buy a personal loan or not. Other attributes such as CD Account, Mortgage, Education, Family, Securities Account are also of some importance as per their mentioned order. But, let us verify the importance of above attributes using statistical approach.**","3e732d1b":"**Data type of column 'CCAvg' is float64, whereas remaining attributes are of type int64**","ff4b76a4":"**Observations: One can see that the although accuracy is decreased slightly than logistic regression case, use of Naive Bayes Classifier for this problem shows improved f1-score. Recall value is improved in case of class 1**","7e0a5a6a":"## About the dataset, column attributes (independent variables) and target variable (dependent variable)\nThe dataset is related to the Banking sector where a bank (namely Thera bank) having information of about 5000 liability customers wants to benefit from the information to explore ways of converting its liability customers to personal loan customers (while retaining them as depositors). The data includes customer demographic information (age, income, etc.), the customer's relationship with the bank (mortgage, securities account, etc.), and the customer response to the last personal loan campaign (Personal Loan) which saw a healthy conversion rate of 9.6% i.e., 480 out of 5000 liablility customers opted for personal loans. \n\nNow, the retail department of the bank wants to build a machine learning model in order devise campaigns with better target marketing to increase the success ratio with minimal budget. The model should help them identify the potential customers who have a higher probability of purchasing the loan which will eventually increase their success ratio at the reduced campaign cost. \n\n**There are total 14 attributes in the dataset and in the context of the given problem, the target (or dependent) attribute is \"Personal Loan\" whereas the remaining are independent attributes.**\n\n**Attribute Information:**\n* ID : Customer ID\n* Age : Customer's age in completed years\n* Experience : #years of professional experience\n* Income : Annual income of the customer (\\$000)\n* ZIP Code : Home Address ZIP code.\n* Family : Family size of the customer\n* CCAvg : Avg. spending on credit cards per month (\\$000)\n* Education : Education Level. 1: Undergrad; 2: Graduate; 3: Advanced\/Professional\n* Mortgage : Value of house mortgage if any. (\\$000)\n* Securities Account : Does the customer have a securities account with the bank?\n* CD Account : Does the customer have a certificate of deposit (CD) account with the bank?\n* Online : Does the customer use internet banking facilities?\n* Credit card : Does the customer use a credit card issued by UniversalBank?\n* Personal Loan : Did this customer accept the personal loan offered in the last campaign? (**Target Attribute**)","914d935b":"**There are 52 negative entries in the Experience column**","b2696cbb":"## 5) Use different classification models (Logistic, K-NN and Na\u00efve Bayes) to predict the likelihood of a customer buying personal loans","4ac6983e":"**480 customers out of 5000 had opted for personal loan in the previous campaign**","c116a87a":"**One can see that, there are no missing values in any of the column attributes**","828c8416":"**In comparison with the logistic regression with unscaled data, in this case the algorithm accuracy is less than the former but the f1-score is increased. One can note that recall value is .91 for class-1 which is a class of interest.**\n\n**Recall value of 91% means that, out of all the customers who would actually buy the loan, 91% were correctly predicted to be positive (would buy the personal loan).**","fcb6cd72":"**Observations:**\n* **'Experience' column shows presence of negative entries (min=-3) which is wrong and needs to be handled appropriately.**\n* Remaining numeric columns such as Age, Income, CCAvg, Mortgage seem to be fine and no data cleaning is required\n* The customer ages are in the range of 23 to 67 with average age as 45. Quarter of the customer population is below 35.\n* Columns with binary information such as Securities Account, CD Account, Online, CreditCard, Personal Loan are also clean.","241d3674":"**Observations**\n* Age has a positive linear relationship with Experience\n* Income, CCAvg, Mortgage histograms are not normally distributed","4d8b2612":"**Analyzing the Experience attribute**","f1c5b931":"## K-NN Classifier","72e6d674":"### Naive Bayes Classifier","d07846ef":"**One can see that, 9.6% of the total customers bought the personal loan in the previous campaign** ","5e0ef3a8":"**By observing p-values of Family and Education, we can say that both the attributes are significant for predicting the target attribute class**","6d7b2b64":"**One can observe that K-NN algorithm with different values for neighbors doesn't provide much improvement both in accuracy as well as f1-score over best performing Naive Bayes Classifier algorithm on unscaled data.**","8c9a7f44":"# Effect of scaling on the algorithm accuracy and f1-score for the compared models\nSome algorithms perform well provided the data is standardized (e.g., K-NN), thus we will investigate the effect of scaling on algorithm accuracy and f1-score","0e8ed0d7":"**Scaling actually decreases the performance of Naive Bayes classifier, although no significant change in accuracy however the f1-score is merely 8% which is a below par performance.**","baba5622":"**Analyzing the CCAvg attribute**","88b74d76":"**Since the p-values are less than 0.05, both the attributes are useful for target class prediction**","c345f03d":"**Customers with \"graduate\" and \"Advanced\/Professional\" education levels show a good conversion rate of about 13% and 13.65%, respectively.**","9dec432c":"**We will first evaluate the model performances on unscaled data with their default parameters**","b055ad70":"**About 13% customers with family size of 3 and 11% customers with family size of 4, purchased personal loans from the bank.**","ef9aca58":"## Naive Bayes Classifier","cddead8d":"### Analyzing the statistical significance of relevant independent attributes for target attribute prediction ","7f06d6ac":"**One can see that out of 5000 customers 480 cutomers opted for the personal loan**","5cc48424":"**Observations: One can see that although accuracy is good, f1-score is low in this case i.e., precision and recall values are not upto the mark especially recall value is low (.28) for class 1**\n\n**Recall value of 28% means, out of all the customers who would actually buy the loan, only 28% were correctly predicted to be positive (would buy the personal loan) thus bank is missing on 72% of potential customers resulting in recampaigning cost**\n\n**58% of precision value means out of all positively predicted customers (who would buy the personal loan) only 58% were correctly predicted.**","bc2c2cf8":"## Trying K-NN with different neighbors on unscaled data.\nIn this section, we will try to find the accuracy and f1 score for KNN model with different values for neighbors.","9961674c":"## 1) Analyzing the data distribution of each attribute and listing the relevant findings\n**In this section, we will use Exploratory Data Analysis (EDA) tools and Applied Statistics concepts to verify the significance of each independent attribute towards predicting probability of a cutomer for buying the personal loan.**","aa57d319":"There there unique negative entries -1,-2 and -3 in the experience column.\n\n**Let us clean the Experience column by removing the negative entries with appropriate values**\n\nIn order to get the appropriate values, let take a cue from correlation of Experience attribute with other attributes","e371f653":"Note: since, Age and Experience are highly correlated, we can drop one of them in the analysis.","e573f265":"**One can see that the although accuracy is similar to logistic regression case, f1-score is lowest among the compared algorithms. Recall value is merely .24 in case of class 1 i.e., out of all the customers who would actually buy the loan, only 24% were correctly predicted to be positive (would buy the personal loan) thus bank is missing on 76% of potential customers**","d5887a9a":"**For customers with 1,2,3 or 4 family members, higher income is an important factor to  buy personal Loans.**","a9580140":"**Customers having house Mortgage value in the ranges (300 to 400), (400 to 500) and (500 to 600) show good tendency to buy the personal loans.** ","1d20af48":"**No customer with income < 50,000\\$ opted for the personal loan where as half of the cutomers with income within the range of 150 to 200 thousand dollars purchased personal loan...! Customers within range of (100 to 150) and (200 to 250) thousand dollars showed a conversion rate of about 28.5% and 18.75%, respectively.**","c43e5dc5":"**Analyzing the binary variables Securities Account, CD Account, Online and CreditCard**\n\nSince they are nominal variables we will use count plot and box plots for analysis","da122834":"**One can see that, CCAvg shows good correlation with target attribute. Customers with more average spending on credit cards per month also show more tendancy to buy the personal loans. Customers with avg. credit card spending in the range of 4 to 6 thousand dollars show around 47% conversion rate.**","debc9e44":"**Analyzing the Mortgage attribute**","ff748edb":"## Concluding the notebook with SVM performance on the scaled dataset..!","61ef1a88":"**One can observe that out of the total customers with experience in the range 40-50 show a good conversion rate of almost 13% for buying the personal loan, a healthy conversion rate of about 10.30% in the experience range 0 to 10, while in the ranges (10-20), (20-30) and (30-40) years of experience it is around 9%.**","2430ade2":"**In the context of independent categorical variables with binary response, one can see by observing their correlation with target attribute that only CD Account shows better correlation whereas Securities Account is slightly correlated. Let us verify them statistically for their significance**","9fcf1ebb":"**Irrespective of their income, undergraduate customers with 1 or 2 family members generally do not opt for personal loans.**","16179175":"# So finally the winner is K-NN classifier with 5 neighbors on the scaled data...! ","23242132":"**Since, Age and Experience have very low correlation with target attribute, they seem to be ineffective for predicting whether customer will buy a personal loan or not. But is this statement statistically correct? Whether Age and Experience have no influence in predicting the target attribute?**","fb28e611":"**Analyzing the Education attribute**","341818cd":"**Evidently K-NN with default parameters (5-neighbor) on scaled data performs very well..The accuracy is 97% and f1-score is also very good.**","76d0cd41":"**Reason: One can see that performance of K-NN classifier is dramatically increased when we scale the data since it is a distance-based algorithm. It's good performance can be attributed to K-NN classifier's robustness against outliers. Also, one can observe that class distributions of the important independent features such as Income, CCAvg show approximately non-overlapping nature for which K-NN is suitable.**","5bac854f":"**One can see that K-NN with 5 neighbor performs the best in terms of accuracy (97%) as well as f1-score(81.63%).**","ed10e82a":"### Dataset Information:\n\n**Name: Bank_Personal_Loan_Modelling.xlsx**\n\n**Domain: Banking**\n\n**Aim: To classify in order to predict the likelihood of a liability bank customer for buying personal loan.**","6117b554":"### K-NN Classifier","1d3a83e3":"**Analyzing the Age attribute**","e2eb2d96":"**One can see that Experience attribute is highly correlated with the Age attribute so we will use this for our data cleaning purpose**\n\n**Note: Since Age and Experience are highly correlated (0.99), so one can drop the experience column. I did the cleaning of experience column for understanding purpose.**","b7dce498":"**Observation: Clearly, there is a high amount of class imbalance in the target attribute..!**","5977b44b":"**There are 5000 rows and 14 columns in this dataset.**"}}