{"cell_type":{"c66a4ff4":"code","491f4c0e":"code","e69f43d9":"code","85d8f905":"code","8f297b85":"code","1b21ea83":"code","f47202f0":"code","d85f69e3":"code","36f22b6e":"markdown","834b295b":"markdown","3f32c222":"markdown","32a21513":"markdown","3c1e4c76":"markdown","92eaceff":"markdown","75491a95":"markdown"},"source":{"c66a4ff4":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","491f4c0e":"#to use BigQuery, we'll import the Python package below\nfrom google.cloud import bigquery","e69f43d9":"# Creating a \"Client\" object\nclient = bigquery.Client()","85d8f905":"# Construct a reference to the \"hacker_news\" dataset\ndataset_refer = client.dataset(\"hacker_news\", project=\"bigquery-public-data\")\n\n# API request - fetch the dataset\ndataset = client.get_dataset(dataset_refer) ","8f297b85":"# Construct a reference to the \"full\" table\ntable_refer = dataset_refer.table(\"full\")\n\n# API request - fetch the table\ntable = client.get_table(table_refer)","1b21ea83":"# Print information on all the columns in the \"full\" table in the \"hacker_news\" dataset\ntable.schema","f47202f0":"# Preview the first five lines of the \"full\" table\nclient.list_rows(table, max_results=5).to_dataframe()","d85f69e3":"# Preview the first five entries in the \"by\" column of the \"full\" table\nclient.list_rows(table, selected_fields=table.schema[:1], max_results=5).to_dataframe()","36f22b6e":"# Table schema\nThe structure of a table is called its schema. We need to understand a table's schema to effectively pull out the data we want.\n\nIn this example, we'll investigate the full table that we fetched above.\n\n\n\n\n\n\n","834b295b":"Every dataset is just a collection of tables. You can think of a dataset as a spreadsheet file containing multiple tables, all composed of rows and columns.\n\nWe use the list_tables() method to list the tables in the dataset.","3f32c222":"Each SchemaField tells us about a specific column (which we also refer to as a field). In order, the information is:\n\nThe name of the column\nThe field type (or datatype) in the column\nThe mode of the column ('NULLABLE' means that a column allows NULL values, and is the default)\nA description of the data in that column\nThe first field has the SchemaField:\n\nSchemaField('by', 'string', 'NULLABLE', \"The username of the item's author.\",())\n\nThis tells us:\n\nthe field (or column) is called by,\nthe data in this field is strings,\nNULL values are allowed, and\nit contains the usernames corresponding to each item's author.\nWe can use the list_rows() method to check just the first five lines of of the full table to make sure this is right. (Sometimes databases have outdated descriptions, so it's good to check.) This returns a BigQuery RowIterator object that can quickly be converted to a pandas DataFrame with the to_dataframe() method.\n\n\n\n\n\n\n","32a21513":"Here, we are dealing with a dataset of posts on computer science and cybersecurity news website Hacker News.\nIn BigQuery, each dataset is contained in a corresponding project. In this case, our hacker_news dataset is contained in the bigquery-public-data project. To access a dataset,\n\nLet's creat a reference to the dataset using the dataset () method.\nWe then use the get_dataset () method along with the link we just created to get the dataset.","3c1e4c76":"Our first stepwould be creating a Client object. The Client object will play a central role in retrieving information from BigQuery datasets.","92eaceff":"Disclaimer\nBefore we go into the coding exercise, a quick disclaimer for those who already know some SQL:\n\nEach Kaggle user can scan 5TB every 30 days for free. Once you hit that limit, you'll have to wait for it to reset.\n\nThe commands you've seen so far won't demand a meaningful fraction of that limit. But some BiqQuery datasets are huge. So, if you already know SQL, wait to run SELECT queries until you've seen how to use your allotment effectively. If you are like most people reading this, you don't know how to write these queries yet, so you don't need to worry about this disclaimer.\n\n\nPractice the commands you've seen to explore the structure of a dataset with crimes in the city of Chicago.\n\n","75491a95":"The list_rows() method will also let us look at just the information in a specific column. If we want to see the first five entries in the by column, for example, we can do that!"}}