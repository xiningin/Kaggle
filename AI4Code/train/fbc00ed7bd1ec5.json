{"cell_type":{"f245a63e":"code","275717b0":"code","02331ede":"code","5c1d728d":"code","4fb86f35":"code","835f9cb8":"code","a407c99c":"code","e63a138b":"code","605c0bd0":"code","48539596":"code","eaa572c4":"code","87326b37":"code","83ec0f1d":"code","c67f4d72":"code","ec2df15c":"code","fd791950":"code","e498ece6":"code","88ce1a75":"code","37cae466":"code","02658554":"code","82652133":"code","00cb3278":"code","d4e42fb8":"code","5dfd4724":"code","5cd4868d":"code","10839935":"code","7fc22c86":"markdown","fc5ae06c":"markdown","549f7b19":"markdown","0a3c7597":"markdown","513cb876":"markdown","c584685f":"markdown","d7c8a39a":"markdown","90e71fca":"markdown","844932e3":"markdown","6de0154c":"markdown","1ad97eb6":"markdown","27da1e9d":"markdown","4d089688":"markdown","3a6f6c33":"markdown","5760edef":"markdown","154b315b":"markdown","53b7df8d":"markdown","8c542b94":"markdown"},"source":{"f245a63e":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport warnings\nwarnings.simplefilter(\"ignore\")\nimport numpy as np\nimport pandas as pd\nfrom scipy.special import boxcox\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n#PLOTLY\nimport plotly.plotly as py\nimport plotly.offline as offline\nimport plotly.graph_objs as go\nfrom plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\ninit_notebook_mode(connected=True)\nimport cufflinks as cf\nfrom plotly.graph_objs import Scatter, Figure, Layout\ncf.set_config_file(offline=True)","275717b0":"print(\">> Loading Data...\")\ntrain = pd.read_csv(\"..\/input\/train.csv\")\ntest = pd.read_csv(\"..\/input\/test.csv\")","02331ede":"print(\"Train shape {}\".format(train.shape))\nprint(\"Test shape {}\".format(test.shape))","5c1d728d":"target = train['Target'].astype('int')","4fb86f35":"data = [go.Histogram(x=target)]\nlayout = go.Layout(title = \"Target Histogram\")\nfig = go.Figure(data=data, layout=layout)\niplot(fig)","835f9cb8":"plt.figure(figsize=(20,8))\nsns.countplot(train.Target)\nplt.title(\"Value Counts of Target Variable\")","a407c99c":"print(f\"Numer of Missing values in train: \", train.isnull().sum().sum())\nprint(f\"Number of Missing values in train: \", test.isnull().sum().sum())","e63a138b":"from scipy.stats import spearmanr\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nlabels = []\nvalues = []\nfor col in train.columns:\n    if col not in [\"Id\", \"Target\"]:\n        labels.append(col)\n        values.append(spearmanr(train[col].values, train[\"Target\"].values)[0])\ncorr_df = pd.DataFrame({'col_labels':labels, 'corr_values':values})\ncorr_df = corr_df.sort_values(by='corr_values')\n \ncorr_df = corr_df[(corr_df['corr_values']>0.1) | (corr_df['corr_values']<-0.1)]\nind = np.arange(corr_df.shape[0])\nwidth = 0.9\nfig, ax = plt.subplots(figsize=(12,30))\nrects = ax.barh(ind, np.array(corr_df.corr_values.values), color='red')\nax.set_yticks(ind)\nax.set_yticklabels(corr_df.col_labels.values, rotation='horizontal')\nax.set_xlabel(\"Correlation coefficient\")\nax.set_title(\"Correlation coefficient of the variables\")\nplt.show()","605c0bd0":"plt.figure(figsize=(15,15))\nsns.heatmap(train[corr_df.col_labels[:50]].corr())","48539596":"plt.figure(figsize=(15,15))\nsns.heatmap(train[corr_df.col_labels[:10]].corr(), annot=True)","eaa572c4":"from sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\nfrom lightgbm import LGBMClassifier\nfrom xgboost import XGBClassifier\nfrom sklearn.metrics import mean_squared_error\nfrom tqdm import tqdm","87326b37":"train.head()","83ec0f1d":"train.drop(['Id','Target'], axis=1, inplace=True)","c67f4d72":"obj_columns = [f_ for f_ in train.columns if train[f_].dtype == 'object']\nfor col in tqdm(obj_columns):\n    le = LabelEncoder()\n    le.fit(train[col].astype(str))\n    train[col] = le.transform(train[col].astype(str))","ec2df15c":"lgbm = LGBMClassifier()\nxgbm = XGBClassifier()\ntrain = train.astype('float32') # For faster computation\nlgbm.fit(train, target , verbose=False)\nxgbm.fit(train, target ,verbose=False)","fd791950":"LGBM_FEAT_IMP = pd.DataFrame({'Features':train.columns, \"IMP\": lgbm.feature_importances_}).sort_values(by='IMP', ascending=False)\n\nXGBM_FEAT_IMP = pd.DataFrame({'Features':train.columns, \"IMP\": xgbm.feature_importances_}\n                            ).sort_values(\n                              by='IMP', ascending=False)","e498ece6":"LGBM_FEAT_IMP.head(10).transpose()","88ce1a75":"XGBM_FEAT_IMP.head(10).transpose()","37cae466":"data = [go.Bar(\n            x= LGBM_FEAT_IMP.head(50).Features,\n            y= LGBM_FEAT_IMP.head(50).IMP, \n            marker=dict(color='green',))\n       ]\nlayout = go.Layout(title = \"LGBM Top 50 Feature Importances\")\nfig = go.Figure(data=data, layout=layout)\niplot(fig)","02658554":"data = [go.Bar(\n            x= XGBM_FEAT_IMP.head(50).Features,\n            y= XGBM_FEAT_IMP.head(50).IMP, \n            marker=dict(color='blue',))\n       ]\nlayout = go.Layout(title = \"XGBM Top 50 Feature Importances\")\nfig = go.Figure(data=data, layout=layout)\niplot(fig)","82652133":"cols_imp = list(set(LGBM_FEAT_IMP[LGBM_FEAT_IMP.IMP > 0 ].Features.values) & set(\n    XGBM_FEAT_IMP[XGBM_FEAT_IMP.IMP > 0 ].Features.values))\nMUTUAL_50 = cols_imp[:50]\nDIFF_DESCRIBE = train[MUTUAL_50].describe().transpose() - test[MUTUAL_50].describe().transpose()\nDIFF_DESCRIBE.style.format(\"{:.2f}\").bar(align='mid', color=['#d65f5f', '#5fba7d'])","00cb3278":"from sklearn.cluster import KMeans\nfrom sklearn.metrics import silhouette_samples, silhouette_score\nfrom scipy.spatial.distance import cdist","d4e42fb8":"X = train[cols_imp].dropna()\ndistortions = []\nfor k in tqdm(range(1,8)):\n    kmeanModel = KMeans(n_clusters=k).fit(X)\n    kmeanModel.fit(X)\n    distortions.append(sum(np.min(cdist(X, kmeanModel.cluster_centers_, 'euclidean'), axis=1)) \/ X.shape[0])","5dfd4724":"# Create a trace\ntrace = go.Line(\n    x = [1,2,3,4,5,6,7,8],\n    y = distortions,\n    line = dict(\n    color = 'red',\n    width = 2),\n    mode = 'lines+markers',\n    name = 'lines+markers'\n)\ndata = [trace]\nlayout = go.Layout(title = \"Elbow Method Optimal Clusters - 3 (From Graph)\")\nfig = go.Figure(data=data, layout=layout)\niplot(fig)","5cd4868d":"from sklearn.metrics import silhouette_score\nk_clusters = []\nsil_coeffecients = []\n\nfor n_cluster in range(2,6):\n    kmeans = KMeans(n_clusters = n_cluster).fit(X)\n    label = kmeans.labels_\n    sil_coeff = silhouette_score(X, label)\n    print(\"For n_clusters={}, Silhouette Coefficient = {}\".format(n_cluster, sil_coeff))\n    sil_coeffecients.append(sil_coeff)\n    k_clusters.append(n_cluster)","10839935":"# Create a trace\ntrace = go.Line(\n    x = [1,2,3,4,5,6],\n    y = sil_coeffecients,\n    line = dict(\n    color = 'orange',\n    width = 2),\n    mode = 'lines+markers',\n    name = 'lines+markers'\n)\ndata = [trace]\nlayout = go.Layout(title = \"Silhouette Optimal Clusters - 3 (From Graph)\")\nfig = go.Figure(data=data, layout=layout)\niplot(fig)","7fc22c86":"## Correlation Heatmap of Top 50 correlated features with target","fc5ae06c":"Top 10 features as seen by LightGBM model and XGBM model","549f7b19":"## Difference of Describe of Train - Test","0a3c7597":"## Finding Feature Importance with Light GBM","513cb876":"## Heatmap of top 10 correlated features","c584685f":"## Feature Importance by Duo\n\nI always go with LGBM and XGBoost Model Importances. Reason being they are quick and always catch the most relevant features and interactions. Also, at this point I would like to caution the use case of regular feature importance by Random Forest. \n\n**The scikit-learn Random Forest feature importance and R's default Random Forest feature importance strategies are biased. To get reliable results in Python, use permutation importance, provided here and in  rfpimp package (via pip). For R, use importance=T in the Random Forest constructor then type=1 in R's importance() function. In addition, your feature importance measures will only be reliable if your model is trained with suitable hyper-parameters.**","d7c8a39a":"## Correlations with target variable","90e71fca":"Remember to Label, OneHot and Dummy Encode your features. As most of them are categorical here ...","844932e3":"## Whats next ? \n\n1.  Baseline scores using XGBoost, CatBoost and LGBM\n2. Recursive Feature Engineering\n3. Categorical Feature Interaction\n4. In dept feature engineering tutorials ahead...","6de0154c":"**Let's find if the unique value of the target match the optimal K cluster through Unsupervised Learning K Means Algorithm**\n\n**How many number of clusters? How to decide ?**\n\nAs unsupervised models dont have a metric as true labels are absent. We often wonder what should be the actual k values.\n\nThere are two methods to determine the optimum k clusters:\n\n**1. Elbow Method**\n\nThis method looks at the percentage of variance explained as a function of the number of clusters: One should choose a number of clusters so that adding another cluster doesn't give much better modeling of the data. More precisely, if one plots the percentage of variance explained by the clusters against the number of clusters, the first clusters will add much information (explain a lot of variance), but at some point the marginal gain will drop, giving an angle in the graph. The number of clusters is chosen at this point, hence the \"elbow criterion\".\n\n\n**Silhouette Scores**\na\nThe Silhouette Coefficient is a metric to estimate the optimum number of clusters. It uses average intra-cluster distance and average nearest-cluster distance for each sample. Higher the value of the score, the better the estimate. Typically the silhoutte scores go high and then fall peaking at an optimum cluster number. The values lie between -1.0 and 1.0.\n\nSilhouette coefficients (as these values are referred to as) near +1 indicate that the sample is far away from the neighboring clusters. A value of 0 indicates that the sample is on or very close to the decision boundary between two neighboring clusters and negative values indicate that those samples might have been assigned to the wrong cluster.","1ad97eb6":"Quick Look on shapes ...","27da1e9d":"## Unsupervised Clustering","4d089688":"Taking Mutual Importance features by two models","3a6f6c33":"Introduction to **Inter-American Development Bank** and **Costa Rican Household Poverty Level Prediction\n**\nThe Inter-American Development Bank (IADB or IDB or BID) is the largest source of development financing for Latin America and the Caribbean.[1] Established in 1959, the IDB supports Latin American and Caribbean economic development, social development and regional integration by lending to governments and government agencies, including State corporations.\n\nPoverty headcount ratio at national poverty line (% of population) in Costa Rica was reported at 20.5 % in 2016, according to the World Bank collection of development indicators, compiled from officially[ recognized sources](https:\/\/tradingeconomics.com\/costa-rica\/poverty-headcount-ratio-at-national-poverty-line-percent-of-population-wb-data.html).\n\n<a href=\"https:\/\/ibb.co\/h2k1ry\"><img src=\"https:\/\/preview.ibb.co\/ckOXyd\/Screenshot_from_2018_07_20_08_50_31.png\" alt=\"Screenshot_from_2018_07_20_08_50_31\" border=\"0\"><\/a>\n\nThe data science is so amazingly universal now that we can help find cancerous nuclie, to predit stocks, recommendations, save lives and with this competition we get to help people find their **SWEET HOME**.","5760edef":"Lets have a look at unique value counts of the target variable","154b315b":"## Target Distributions","53b7df8d":"## Numer of Missing values in datasets","8c542b94":"## Target Value Counts"}}