{"cell_type":{"0bd0c607":"code","0f3c9954":"code","fd14ce6d":"code","00925cc8":"markdown","ce6eacb7":"markdown","5e261b14":"markdown","d4970206":"markdown"},"source":{"0bd0c607":"import base64\nimport gzip\nimport os\nimport re\nimport time\nfrom typing import Any\nfrom typing import Union\n\nimport dill\nimport humanize\n\n\n# _base64_file__test_base64_static_import = \"\"\"\n# H4sIAPx9LF8C\/2tgri1k0IjgYGBgKCxNLS7JzM8rZIwtZNLwZvBm8mYEkjAI4jFB2KkRbED1iXnF\n# 5alFhczeWqV6AEGfwmBHAAAA\n# \"\"\"\n\n\ndef base64_file_varname(filename: str) -> str:\n    # ..\/data\/AntColonyTreeSearchNode.dill.zip.base64 -> _base64_file__AntColonyTreeSearchNode__dill__zip__base64\n    varname = re.sub(r'^.*\/',   '',   filename)  # remove directories\n    varname = re.sub(r'[.\\W]+', '__', varname)   # convert dots and non-ascii to __\n    varname = f\"_base64_file__{varname}\"\n    return varname\n\n\ndef base64_file_var_wrap(base64_data: Union[str,bytes], varname: str) -> str:\n    return f'{varname} = \"\"\"\\n{base64_data.strip()}\\n\"\"\"'                    # add varname = \"\"\"\\n\\n\"\"\" wrapper\n\n\ndef base64_file_var_unwrap(base64_data: str) -> str:\n    output = base64_data.strip()\n    output = re.sub(r'^\\w+ = \"\"\"|\"\"\"$', '', output)  # remove varname = \"\"\" \"\"\" wrapper\n    output = output.strip()\n    return output\n\n\ndef base64_file_encode(data: Any) -> str:\n    encoded = dill.dumps(data)\n    encoded = gzip.compress(encoded)\n    encoded = base64.encodebytes(encoded).decode('utf8').strip()\n    return encoded\n\n\ndef base64_file_decode(encoded: str) -> Any:\n    data = base64.b64decode(encoded)\n    data = gzip.decompress(data)\n    data = dill.loads(data)\n    return data\n\n\ndef base64_file_save(data: Any, filename: str, vebose=True) -> float:\n    \"\"\"\n        Saves a base64 encoded version of data into filename, with a varname wrapper for importing via kaggle_compile.py\n        # Doesn't create\/update global variable.\n        Returns filesize in bytes\n    \"\"\"\n    varname    = base64_file_varname(filename)\n    start_time = time.perf_counter()\n    try:\n        os.makedirs(os.path.dirname(filename), exist_ok=True)\n        with open(filename, 'wb') as file:\n            encoded = base64_file_encode(data)\n            output  = base64_file_var_wrap(encoded, varname)\n            output  = output.encode('utf8')\n            file.write(output)\n            file.close()\n        if varname in globals(): globals()[varname] = encoded  # globals not shared between modules, but update for saftey\n\n        filesize = os.path.getsize(filename)\n        if vebose:\n            time_taken = time.perf_counter() - start_time\n            print(f\"base64_file_save(): {filename:40s} | {humanize.naturalsize(filesize)} in {time_taken:4.1f}s\")\n        return filesize\n    except Exception as exception:\n        pass\n    return 0.0\n\n\ndef base64_file_load(filename: str, vebose=True) -> Union[Any,None]:\n    \"\"\"\n        Performs a lookup to see if the global variable for this file alread exists\n        If not, reads the base64 encoded file from filename, with an optional varname wrapper\n        # Doesn't create\/update global variable.\n        Returns decoded data\n    \"\"\"\n    varname    = base64_file_varname(filename)\n    start_time = time.perf_counter()\n    try:\n        # Hard-coding PyTorch weights into a script - https:\/\/www.kaggle.com\/c\/connectx\/discussion\/126678\n        encoded = None\n\n        if varname in globals():\n            encoded = globals()[varname]\n\n        if encoded is None and os.path.exists(filename):\n            with open(filename, 'rb') as file:\n                encoded = file.read().decode('utf8')\n                encoded = base64_file_var_unwrap(encoded)\n                # globals()[varname] = encoded  # globals are not shared between modules\n\n        if encoded is not None:\n            data = base64_file_decode(encoded)\n\n            if vebose:\n                filesize = os.path.getsize(filename)\n                time_taken = time.perf_counter() - start_time\n                print(f\"base64_file_load(): {filename:40s} | {humanize.naturalsize(filesize)} in {time_taken:4.1f}s\")\n            return data\n    except Exception as exception:\n        print(f'base64_file_load({filename}): Exception:', exception)\n    return None\n","0f3c9954":"from pytest import fixture\n\nfrom util.base64_file import *\n\n\n@fixture\ndef data():\n    return { \"question\": [ 0,2,1,0,0,0,0, 0,2,1,2,0,0,0 ], \"answer\": 42 }\n\n\ndef test_base64_file_varname():\n    input    = '.\/data\/MontyCarloNode.pickle.zip.base64'\n    expected = '_base64_file__MontyCarloNode__pickle__zip__base64'\n    actual   = base64_file_varname(input)\n    assert actual == expected\n\n\ndef test_base64_wrap_unwrap(data):\n    varname   = base64_file_varname('test')\n    input     = base64.encodebytes(dill.dumps(data)).decode('utf8').strip()\n    wrapped   = base64_file_var_wrap(input, varname)\n    unwrapped = base64_file_var_unwrap(wrapped)\n\n    assert isinstance(input,   str)\n    assert isinstance(wrapped, str)\n    assert isinstance(unwrapped, str)\n    assert varname     in wrapped\n    assert varname not in unwrapped\n    assert input != wrapped\n    assert input == unwrapped\n\n\ndef test_base64_save_load(data):\n    assert data == data\n\n    filename = '\/tmp\/test_base64_save_load'\n    if os.path.exists(filename): os.remove(filename)\n    assert not os.path.exists(filename)\n\n    loaded   = base64_file_load(filename)\n    assert loaded is None\n\n    varname  = base64_file_varname(filename)\n    filesize = base64_file_save(data, filename)\n    loaded   = base64_file_load(filename)\n\n    assert os.path.exists(filename)\n    assert filesize < 1024  # less than 1kb\n    assert data == loaded\n\n    # assert varname in globals()          # globals are not shared between modules\n    # assert output == globals()[varname]  # globals are not shared between modules\n\n\ndef test_base64_static_import(data):\n    assert data == data\n\n    filename = '\/tmp\/test_base64_static_import'\n    if os.path.exists(filename): os.remove(filename)\n    assert not os.path.exists(filename)\n\n\n    varname  = base64_file_varname(filename)\n    filesize = base64_file_save(data, filename)\n    loaded   = base64_file_load(filename)\n\n    if varname in globals(): del globals()[varname]  # globals are not shared between modules\n    contents = open(filename, 'r').read()\n    exec(contents, globals())\n    assert varname in globals()\n    encoded = globals()[varname]\n    decoded = base64_file_decode(encoded)\n\n    assert varname in contents\n    assert data == loaded == decoded","fd14ce6d":"# Source: https:\/\/github.com\/JamesMcGuigan\/ai-games\/blob\/master\/games\/connectx\/core\/PersistentCacheAgent.py\n\nimport atexit\nimport math\n\n# from util.base64_file import base64_file_load\n# from util.base64_file import base64_file_save\n\n\nclass PersistentCacheAgent:\n    persist = False\n    cache   = {}\n    verbose = True\n\n    def __new__(cls, *args, **kwargs):\n        # noinspection PyUnresolvedReferences\n        for parentclass in cls.__mro__:  # https:\/\/stackoverflow.com\/questions\/2611892\/how-to-get-the-parents-of-a-python-class\n            if cls is parentclass: continue\n            if cls.cache is getattr(parentclass, 'cache', None):\n                cls.cache = {}  # create a new cls.cache for each class\n                break\n        instance = object.__new__(cls)\n        return instance\n\n    def __init__(self, *args, **kwargs):\n        super().__init__()\n        if not self.persist: return  # disable persistent caching\n        self.load()\n        self.autosave()\n\n    def autosave( self ):\n        # Autosave on Ctrl-C\n        atexit.unregister(self.__class__.save)\n        atexit.register(self.__class__.save)\n\n    # def __del__(self):\n    #     self.save()\n\n    @classmethod\n    def filename( cls ):\n        return f'.\/data\/{cls.__name__}_base64'\n\n    @classmethod\n    def load( cls ):\n        if not cls.persist: return  # disable persistent caching\n        if cls.cache:       return  # skip loading if the file is already in class memory, empty dict is False\n        filename = cls.filename()\n        loaded   = base64_file_load(filename, vebose=cls.verbose)\n        if loaded:  # cls.cache should not be set to None\n            cls.cache = loaded\n\n    @classmethod\n    def save( cls ):\n        if not cls.persist: return  # disable persistent caching\n        # cls.load()                # update any new information from the file\n        if cls.cache:\n            filename = cls.filename()\n            base64_file_save(cls.cache, filename, vebose=cls.verbose)\n\n\n    @staticmethod\n    def cache_size( data ):\n        return sum([\n            len(value) if isinstance(key, str) and isinstance(value, dict) else 1\n            for key, value in data.items()\n        ])\n\n    @classmethod\n    def reset( cls ):\n        cls.cache = {}\n        cls.save()\n\n\n    ### Caching\n    @classmethod\n    def cache_function( cls, function, game, player_id, *args, **kwargs ):\n        hash = (player_id, game)  # QUESTION: is player_id required for correct caching between games?\n        if function.__name__ not in cls.cache:   cls.cache[function.__name__] = {}\n        if hash in cls.cache[function.__name__]: return cls.cache[function.__name__][hash]\n\n        score = function(game, *args, **kwargs)\n        cls.cache[function.__name__][hash] = score\n        return score\n\n    @classmethod\n    def cache_infinite( cls, function, game, player_id, *args, **kwargs ):\n        # Don't cache heuristic values, only terminal states\n        hash = (player_id, game)  # QUESTION: is player_id required for correct caching between games?\n        if function.__name__ not in cls.cache:   cls.cache[function.__name__] = {}\n        if hash in cls.cache[function.__name__]: return cls.cache[function.__name__][hash]\n\n        score = function(game, player_id, *args, **kwargs)\n        if abs(score) == math.inf: cls.cache[function.__name__][hash] = score\n        return score\n","00925cc8":"# base64_file.py\n- https:\/\/github.com\/JamesMcGuigan\/ai-games\/blob\/master\/games\/connectx\/util\/base64_file.py\n- https:\/\/github.com\/JamesMcGuigan\/ai-games\/blob\/master\/games\/connectx\/util\/base64_file_test.py\n- https:\/\/github.com\/JamesMcGuigan\/ai-games\/blob\/master\/games\/connectx\/core\/PersistentCacheAgent.py\n\n\n`base64_file.py` implements two main methods: \n- `base64_file_save(data: Any, filename: str, vebose=True) -> filesize: float` \n- `base64_file_load(filename: str, vebose=True)`\n\n`base64_file_save()` will export a pickle.zip.base64.py encoded version of the data to a standalone file, wrapped in a python variable declaration like this.  \n\n```\n_base64_file__test_base64_static_import = \"\"\"\nH4sIAPx9LF8C\/2tgri1k0IjgYGBgKCxNLS7JzM8rZIwtZNLwZvBm8mYEkjAI4jFB2KkRbED1iXnF\n5alFhczeWqV6AEGfwmBHAAAA\n\"\"\"\n```\n\n`base64_file_load()` will test to see if the expected variable name is available in the global namespace (such as when concatinated to `submission.py`) and if so use that as a datasource instead of doing a filesystem lookup.\n\n\nThe idea is that this file can be concatinated to the end of your submission.py file using a script such as [kaggle_compile.py](https:\/\/www.kaggle.com\/jamesmcguigan\/kaggle-compile-py-python-ide-to-kaggle-compiler)\n```\nkaggle_compile.py agents\/MontyCarlo\/MontyCarloBitsquares.py .\/data\/MontyCarloBitsquaresNode_base64.py > .\/submissions\/MontyCarloBitsquares.py\n```\n\nNOTE: there is a known issue with pickle having trouble resolving module-namespaces for pickled class-instances when the data was generated and saved in multi-file mode, but reloaded in single-file mode. There is a workaround to this: compile the training runtime loop into single-file mode and rerun regenerate the pickle.zip.base64.py files within the same namespace as submission.py. This issue shouldn't affect pytorch models or datasets containing only python primatives, it will however effect pickling of object oriented classes defined in a different module.","ce6eacb7":"# Usage - PersistentCacheAgent\n\nHere I have implemented a PersistentCacheAgent base class, that automagically handles loading and saving of the `cls.cache` object. It can be subclassed to implement custom agent logic.\n\n`cls.load()` is ideopotently called on the first instantiation of a class instance. `cls.save()` will trigger on program exit via `atexit.register()`.\n\nThe `cls.cache_function()` and `cls.cache_infinite()` methods allow for persstant caching of function call return values.","5e261b14":"# pickle.zip.base64.py File Encoding\n\nOne of the (old) requirements for the Simulations competitons such as [ConnectX](https:\/\/www.kaggle.com\/c\/connectx\/) was that the submission needed to be encoded as a single python submission.py script. Thus using persistent data, such as a pytorch model or a cache of the data model requires base64 encoding the data as an inline variable.\n\nThe idea of using pickle.zip.base64 File Encoding was inspired [Peter Cnudde](https:\/\/www.kaggle.com\/petercnudde) but I have taken the idea and extended it further.\n\nAs reported by Sam Harris, in [New Feature: Multifile Agents](https:\/\/www.kaggle.com\/c\/halite\/discussion\/177686) it is now possible to submit a `.tar.gz` archive with a top level called main.py.\n\nBUGFIX: `base64_file_load() Exception: No module named 'agents'`: There is a issue with pickle being unable to re-resolve modules after being compiled into a single script. Thus as a workaround, the training script itself should be run through kaggle_compile.py and the training done within a single-file namespace. Alteratively, this can be solved by using dill rather than pickle.","d4970206":"# Unit Tests\n\nCore infrastructure code should have unit tests to validate it's functionality"}}