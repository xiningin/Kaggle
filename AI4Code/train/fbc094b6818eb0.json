{"cell_type":{"ce1d9e72":"code","abb831ab":"code","30e40fb5":"code","e5c126a2":"code","a885a2ac":"code","2bf6dac6":"code","f090df86":"code","aa3ed411":"code","de2962ee":"code","a6914dd7":"code","82d02763":"code","46c8973f":"code","02fbb6ad":"code","100c3575":"code","c574162b":"code","15f19f08":"code","c5440641":"code","d5ba3a7e":"code","5bebbb84":"code","366c1005":"code","7df36eda":"code","e6dd46c9":"code","027cc52f":"code","c88a3ce3":"code","2bee0388":"code","7d0a517b":"code","a746b2ef":"code","5197e401":"code","18a37bab":"code","90c4263b":"code","2e235ff3":"code","d1e2bc84":"code","d12815a9":"code","09b9bed5":"code","75937d6c":"code","623a6944":"code","149829b2":"code","e2e00e0a":"code","3544dfa6":"code","e1eee537":"code","4170a188":"code","5236938d":"code","9f57cc45":"markdown","e171bb89":"markdown","839fb071":"markdown","f27ed50c":"markdown","fe522558":"markdown","6140c2b7":"markdown","2eb43651":"markdown","eaa1df74":"markdown","5ceb8673":"markdown","b89cdfdb":"markdown","66409f6f":"markdown","90207130":"markdown","7d7ec39b":"markdown","aa7bc5ad":"markdown","68356a7a":"markdown","e49e34e1":"markdown","d29f9ef8":"markdown","6883fd9b":"markdown","f8672ce6":"markdown","fd94539f":"markdown","fcb88c3c":"markdown","9197cf28":"markdown"},"source":{"ce1d9e72":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","abb831ab":"import pandas as pd\npd.set_option('use_inf_as_na', True)\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler \nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import accuracy_score as acs\nfrom sklearn.metrics import precision_recall_fscore_support as score\nfrom sklearn.utils import resample\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.decomposition import PCA\nfrom xgboost import XGBClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.neural_network import MLPClassifier\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","30e40fb5":"data=pd.read_csv(\"\/kaggle\/input\/breast-cancer-wisconsin-data\/data.csv\")\ndata.drop([\"Unnamed: 32\"],axis=1,inplace=True)\ndisplay(data.head())","e5c126a2":"#New data with \"M\"=1 and \"B\"=0\ndata1=data.copy()\ndef classifier(data1):\n    if data1[\"diagnosis\"]==\"M\":\n        return \"1\"\n    else:\n        return \"0\"\ndata1[\"diagnosis\"] = data1.apply(classifier, axis=1)\ndata1.replace([np.inf, -np.inf], np.nan, inplace=True)\ndata1[\"diagnosis\"]=pd.to_numeric(data1[\"diagnosis\"],errors=\"coerce\")","a885a2ac":"print(data1.columns,data.shape)\n# print(data1.info())\n# print(data1.describe().T)\n# print(data1.nunique())","2bf6dac6":"sns.set(style=\"whitegrid\")\nprint(data['diagnosis'].value_counts())\nfig = plt.figure(figsize = (10,6))\nsns.countplot('diagnosis', data=data, palette='gist_heat')\nplt.show()","f090df86":"color = sns.color_palette(\"pastel\")\n\nfig, ax1 = plt.subplots(8,4, figsize=(30,60))\nk = 0\ncolumns = list(data1.columns)\nfor i in range(8):\n    for j in range(4):\n            sns.distplot(data1[columns[k]], ax = ax1[i][j], color = 'red')\n            plt.xlabel(columns[k],size=20)\n            k += 1\nplt.show()","aa3ed411":"#Log transform\ndef log_transform(col):\n    return np.log(col[0])\n\ndata1[\"compactness_mean\"]=data1[[\"compactness_mean\"]].apply(log_transform, axis=1)\ndata1[\"concavity_mean\"]=data1[[\"concavity_mean\"]].apply(log_transform, axis=1)\ndata1[\"concave points_mean\"]=data1[[\"concave points_mean\"]].apply(log_transform, axis=1)\ndata1[\"radius_se\"]=data1[[\"radius_se\"]].apply(log_transform, axis=1)\ndata1[\"perimeter_se\"]=data1[[\"perimeter_se\"]].apply(log_transform, axis=1)\ndata1[\"smoothness_se\"]=data1[[\"smoothness_se\"]].apply(log_transform, axis=1)\ndata1[\"compactness_se\"]=data1[[\"compactness_se\"]].apply(log_transform, axis=1)\ndata1[\"concavity_se\"]=data1[[\"concavity_se\"]].apply(log_transform, axis=1)\ndata1[\"symmetry_se\"]=data1[[\"symmetry_se\"]].apply(log_transform, axis=1)\ndata1[\"fractal_dimension_se\"]=data1[[\"fractal_dimension_se\"]].apply(log_transform, axis=1)\ndata1[\"area_worst\"]=data1[[\"area_worst\"]].apply(log_transform, axis=1)\ndata1[\"compactness_worst\"]=data1[[\"compactness_worst\"]].apply(log_transform, axis=1)\ndata1[\"concavity_worst\"]=data1[[\"concavity_worst\"]].apply(log_transform, axis=1)","de2962ee":"color = sns.color_palette(\"pastel\")\n\nfig, ax1 = plt.subplots(8,4, figsize=(30,60))\nk = 0\ncolumns = list(data1.columns)\nfor i in range(8):\n    for j in range(4):\n        sns.distplot(data1[columns[k]], ax = ax1[i][j], color = 'green')\n        k += 1\nplt.show()","a6914dd7":"plt.figure(figsize=(16,8))\ncorr=data1.drop([\"id\"],axis=1).corr()\nsns.heatmap(corr,annot=True,linewidth=1)\nplt.show()\n\n#Correaltion of features in descending order\nprint(data1.corr()['diagnosis'].sort_values(ascending=False))\n\nplt.figure(figsize=(16,8))\nplt.plot(data1.corr()['diagnosis'].sort_values(ascending=False)[1:],color=\"cyan\")\nplt.title(\"Correlation of different features with 'Diagnosis'\")\nplt.xticks(rotation=90)\nplt.show()","82d02763":"sns.boxplot(data=data,x=\"diagnosis\",y=\"concave points_worst\")\nplt.show()\nsns.boxplot(data=data,x=\"diagnosis\",y=\"perimeter_worst\")\nplt.show()\nsns.boxplot(data=data,x=\"diagnosis\",y=\"concave points_mean\")\nplt.show()\nsns.boxplot(data=data,x=\"diagnosis\",y=\"radius_worst\")\nplt.show()\nsns.boxplot(data=data,x=\"diagnosis\",y=\"perimeter_mean\")\nplt.show()","46c8973f":"data_M = data1[data1.diagnosis==1]     #Minority\ndata_B = data1[data1.diagnosis==0]     #Majority\n\ndata_M_upsampled=resample(data_M,replace=True, n_samples=300, random_state=12)\ndata_B_downsampled= data_B.sample(n=300).reset_index(drop=True)\n\n#New dataset for balanced data\nBalanced_df = pd.concat([data_M_upsampled, data_B_downsampled]).reset_index(drop=True)","02fbb6ad":"print(Balanced_df[\"diagnosis\"].value_counts())\nplt.figure(figsize=(10,6))\nsns.countplot(x='diagnosis', data=Balanced_df, palette='gist_heat')\nplt.show()","100c3575":"plt.figure(figsize=(15,15))\nBalanced_df.corr().diagnosis.apply(lambda x: abs(x)).sort_values(ascending=False).iloc[1:21][::-1].plot(kind='barh',color='cyan') \n# calculating the top 20 highest correlated features\n# with respect to the target variable i.e. \"quality\"\nplt.title(\"Top 20 highly correlated features\", size=20, pad=26)\nplt.xlabel(\"Correlation coefficient\")\nplt.ylabel(\"Features\")\nplt.show()","c574162b":"selected_features=Balanced_df.corr().diagnosis.sort_values(ascending=False).iloc[1:21][::-1].index\n\nX = Balanced_df[selected_features]\nY = Balanced_df.diagnosis","15f19f08":"X=data.iloc[:,2:32]\nY=data.iloc[:,1]\n\nX_train,X_test,Y_train,Y_test=train_test_split(X,Y,test_size=0.2,random_state=0) \n\n#Feature Scaling\nscaler=StandardScaler()\nX_train=scaler.fit_transform(X_train)\nX_test=scaler.fit_transform(X_test)","c5440641":"LR_model=LogisticRegression(random_state=0)\nLR_model.fit(X_train,Y_train)","d5ba3a7e":"Y_pred=LR_model.predict(X_test)","5bebbb84":"cm=confusion_matrix(Y_pred,Y_test)\nclass_label = [\"malignant\", \"benign\"]\ndf_cm = pd.DataFrame(cm, index=class_label,columns=class_label)\n\nprecision, recall, fscore, train_support = score(Y_test, Y_pred, pos_label=\"M\", average='binary')\nLogistic_Regression_accuracy=round(acs(Y_test,Y_pred), 4)*100\nprint('Precision: {} \\nRecall: {} \\nF1-Score: {} \\nAccuracy: {}'.format(\n    round(precision, 3), round(recall, 3), round(fscore,3), Logistic_Regression_accuracy) +\"% \\n\")\n\nsns.heatmap(df_cm,annot=True,cmap='Pastel1',linewidths=2,fmt='d')\nplt.title(\"Confusion Matrix\",fontsize=15)\nplt.xlabel(\"Predicted\")\nplt.ylabel(\"True\")\nplt.show()","366c1005":"svm=SVC(kernel=\"rbf\",random_state=0)\nsvm.fit(X_train,Y_train)","7df36eda":"Y_pred=svm.predict(X_test)","e6dd46c9":"cm=confusion_matrix(Y_pred,Y_test)\nclass_label = [\"malignant\", \"benign\"]\ndf_cm = pd.DataFrame(cm, index=class_label,columns=class_label)\n\nprecision, recall, fscore, train_support = score(Y_test, Y_pred, pos_label=\"M\", average='binary')\nSVM_accuracy=round(acs(Y_test,Y_pred), 4)*100\nprint('Precision: {} \\nRecall: {} \\nF1-Score: {} \\nAccuracy: {}'.format(\n    round(precision, 3), round(recall, 3), round(fscore,3), SVM_accuracy) +\"% \\n\")\n\nsns.heatmap(df_cm,annot=True,cmap='Pastel1',linewidths=2,fmt='d')\nplt.title(\"Confusion Matrix\",fontsize=15)\nplt.xlabel(\"Predicted\")\nplt.ylabel(\"True\")\nplt.show()","027cc52f":"tree=DecisionTreeClassifier(random_state=10)\ntree.fit(X_train,Y_train)","c88a3ce3":"Y_pred=tree.predict(X_test)","2bee0388":"cm=confusion_matrix(Y_pred,Y_test)\nclass_label = [\"malignant\", \"benign\"]\ndf_cm = pd.DataFrame(cm, index=class_label,columns=class_label)\n\nprecision, recall, fscore, train_support = score(Y_test, Y_pred, pos_label=\"M\", average='binary')\nDecision_Tree_accuracy=round(acs(Y_test,Y_pred), 4)*100\nprint('Precision: {} \\nRecall: {} \\nF1-Score: {} \\nAccuracy: {}'.format(\n    round(precision, 3), round(recall, 3), round(fscore,3), Decision_Tree_accuracy) +\"% \\n\")\n\nsns.heatmap(df_cm,annot=True,cmap='Pastel1',linewidths=2,fmt='d')\nplt.title(\"Confusion Matrix\",fontsize=15)\nplt.xlabel(\"Predicted\")\nplt.ylabel(\"True\")\nplt.show()","7d0a517b":"rfc=RandomForestClassifier(n_estimators=60,random_state=0)\nrfc.fit(X_train,Y_train)","a746b2ef":"Y_pred=rfc.predict(X_test)","5197e401":"cm=confusion_matrix(Y_pred,Y_test)\nclass_label = [\"malignant\", \"benign\"]\ndf_cm = pd.DataFrame(cm, index=class_label,columns=class_label)\n\nprecision, recall, fscore, train_support = score(Y_test, Y_pred, pos_label=\"M\", average='binary')\nRandom_forest_classifier_accuracy=round(acs(Y_test,Y_pred), 4)*100\nprint('Precision: {} \\nRecall: {} \\nF1-Score: {} \\nAccuracy: {}'.format(\n    round(precision, 3), round(recall, 3), round(fscore,3), Random_forest_classifier_accuracy) +\"% \\n\")\n\nsns.heatmap(df_cm,annot=True,cmap='Pastel1',linewidths=2,fmt='d')\nplt.title(\"Confusion Matrix\",fontsize=15)\nplt.xlabel(\"Predicted\")\nplt.ylabel(\"True\")\nplt.show()","18a37bab":"gbc=GradientBoostingClassifier(random_state=11)\ngbc.fit(X_train,Y_train)","90c4263b":"Y_pred=gbc.predict(X_test)","2e235ff3":"cm=confusion_matrix(Y_pred,Y_test)\nclass_label = [\"malignant\", \"benign\"]\ndf_cm = pd.DataFrame(cm, index=class_label,columns=class_label)\n\nprecision, recall, fscore, train_support = score(Y_test, Y_pred, pos_label=\"M\", average='binary')\nGradient_Boosting_Classifier_accuracy=round(acs(Y_test,Y_pred), 4)*100\nprint('Precision: {} \\nRecall: {} \\nF1-Score: {} \\nAccuracy: {}'.format(\n    round(precision, 3), round(recall, 3), round(fscore,3), Gradient_Boosting_Classifier_accuracy) +\"% \\n\")\n\nsns.heatmap(df_cm,annot=True,cmap='Pastel1',linewidths=2,fmt='d')\nplt.title(\"Confusion Matrix\",fontsize=15)\nplt.xlabel(\"Predicted\")\nplt.ylabel(\"True\")\nplt.show()","d1e2bc84":"xgb=XGBClassifier(random_state=0,booster=\"gbtree\")\nxgb.fit(X_train,Y_train)","d12815a9":"Y_pred=xgb.predict(X_test)","09b9bed5":"cm=confusion_matrix(Y_pred,Y_test)\nclass_label = [\"malignant\", \"benign\"]\ndf_cm = pd.DataFrame(cm, index=class_label,columns=class_label)\n\nprecision, recall, fscore, train_support = score(Y_test, Y_pred, pos_label=\"M\", average='binary')\nXG_boost_classifier_accuracy=round(acs(Y_test,Y_pred), 4)*100\nprint('Precision: {} \\nRecall: {} \\nF1-Score: {} \\nAccuracy: {}'.format(\n    round(precision, 3), round(recall, 3), round(fscore,3), XG_boost_classifier_accuracy) +\"% \\n\")\n\nsns.heatmap(df_cm,annot=True,cmap='Pastel1',linewidths=2,fmt='d')\nplt.title(\"Confusion Matrix\",fontsize=15)\nplt.xlabel(\"Predicted\")\nplt.ylabel(\"True\")\nplt.show()","75937d6c":"knn=KNeighborsClassifier(n_neighbors=5)\nknn.fit(X_train,Y_train)","623a6944":"Y_pred=knn.predict(X_test)","149829b2":"cm=confusion_matrix(Y_pred,Y_test)\nclass_label = [\"malignant\", \"benign\"]\ndf_cm = pd.DataFrame(cm, index=class_label,columns=class_label)\n\nprecision, recall, fscore, train_support = score(Y_test, Y_pred, pos_label=\"M\", average='binary')\nKNN_accuracy=round(acs(Y_test,Y_pred), 4)*100\nprint('Precision: {} \\nRecall: {} \\nF1-Score: {} \\nAccuracy: {}'.format(\n    round(precision, 3), round(recall, 3), round(fscore,3), KNN_accuracy) +\"% \\n\")\n\nsns.heatmap(df_cm,annot=True,cmap='Pastel1',linewidths=2,fmt='d')\nplt.title(\"Confusion Matrix\",fontsize=15)\nplt.xlabel(\"Predicted\")\nplt.ylabel(\"True\")\nplt.show()","e2e00e0a":"val=10   #Max value of n_neighbor\nmodel=knn  #Name of model you want to train (I'm training my KNN model)\nfor K in range(val):\n    K_value = K+1\n    model = KNeighborsClassifier(n_neighbors=K_value)\n    model.fit(X_train,Y_train)\n    Y_pred = model.predict(X_test)\n    print(\"Accuracy is : \", acs(Y_test,Y_pred)*100,\"% for n_neighbors: \", K_value)","3544dfa6":"classifier = MLPClassifier(random_state=1,hidden_layer_sizes=(150,100,50), max_iter=300,activation = 'relu',solver='adam')\nclassifier.fit(X_train, Y_train)","e1eee537":"Y_pred=classifier.predict(X_test)","4170a188":"cm=confusion_matrix(Y_pred,Y_test)\nclass_label = [\"malignant\", \"benign\"]\ndf_cm = pd.DataFrame(cm, index=class_label,columns=class_label)\n\nprecision, recall, fscore, train_support = score(Y_test, Y_pred, pos_label=\"M\", average='binary')\nMLP_classifier_accuracy=round(acs(Y_test,Y_pred), 4)*100\nprint('Precision: {} \\nRecall: {} \\nF1-Score: {} \\nAccuracy: {}'.format(\n    round(precision, 3), round(recall, 3), round(fscore,3), MLP_classifier_accuracy) +\"% \\n\")\n\nsns.heatmap(df_cm,annot=True,cmap='Pastel1',linewidths=2,fmt='d')\nplt.title(\"Confusion Matrix\",fontsize=15)\nplt.xlabel(\"Predicted\")\nplt.ylabel(\"True\")\nplt.show()","5236938d":"accuracies={\"Random Forest\": Random_forest_classifier_accuracy,\n            \"SVM\": SVM_accuracy,\n            \"MLP Classifier\": MLP_classifier_accuracy,\n            \"Gradient Boosting\": Gradient_Boosting_Classifier_accuracy,\n            \"XG Boost\": XG_boost_classifier_accuracy,\n            \"KNN\": KNN_accuracy,\n            \"Logistic regression\": Logistic_Regression_accuracy,\n            \"Decision Tree\": Decision_Tree_accuracy}\n\n#Plot accuracy for different models\nplt.figure(figsize=(14,6))\nplt.bar(accuracies.keys(),accuracies.values(),label=\"Accuracy\")\nplt.xlabel(\"Classifier Used\")\nplt.ylabel(\"Accuracy (%)\")\nplt.ylim(90,100)\nplt.legend()\nplt.tight_layout()\nplt.show()","9f57cc45":"# Model 2=>Support Vector Machine","e171bb89":"**Correlation plot between the features:**","839fb071":"# Model 5=>Gradient Boosting Classifier","f27ed50c":"**Method to find the best value of *n_neighbors* based on accuracy**","fe522558":"# Model 7=>K-Nearest Neighbor (KNN) classification","6140c2b7":"# Model 3=>Decision Tree","2eb43651":"**Comparison of Accuracy**","eaa1df74":"# Model 8 => MLP Classifier","5ceb8673":"**Thank you!**","b89cdfdb":"# Model 1=>Logistic Regression","66409f6f":"# Model 6=>XGBoost Classifier","90207130":"We will only be using the top 20 correlated features to train our model, this will hellp improve the accuacy. ","7d7ec39b":"# Analysis\n__After using 8 different algorithms, we got the following accuracies:__\n1. Logistic Regression - **95.61%**\n2. Support Vector Machine - **98.25%**\n3. Decision Tree - **93.86%**\n4. Random Forest Classifier - **98.25%**\n5. Gradient Boosting Classfier - **96.49%**\n6. XGBoost Classifier - **96.49%**\n7. K-nearest neighbor classification - **96.49%**\n8. MLP Classifier - **97.37%**\n\nThis clearly shows that **SVM** and **Random Forest Classifier** are the most efficient and accurate algorithms, and hence they are most widely used for classification problems.\n\n> **NOTE:** XGBoost is also a very powerful algorithm when it comes to classification. The reason we got just 96.49% accuracy using XGboost is because the training data (X_train) was scaled at the beginning using **StandardScaler**.\n> To obtain a better accuracy with XGBoost (almost 99%), train the model without scaling the training data.","aa7bc5ad":"**If you have any suggestions or doubts, feel free to comment below!**","68356a7a":"# __Data Preprocessing__ ","e49e34e1":"**If you found this notebook useful, please do upvote!**","d29f9ef8":"**Split data into training and testing sets**","6883fd9b":"**Now the count for our output variable \"diagnosis\" has been made equal**","f8672ce6":"**Now we will be doing undersampling and oversampling.**\n\n* The simplest implementation of over-sampling is to duplicate random records from the minority class, which can cause overfishing.\n* In under-sampling, the simplest technique involves removing random records from the majority class, which can cause loss of information.","fd94539f":"# Model 4=>Random Forest Classifier","fcb88c3c":"**Boxplot of top 5 corrrelated features**","9197cf28":"> **Note:** Most of our columns are highly skewed towards right. These columns include compactness_mean, concavity_mean, concave points_mean, radius_se, perimeter_se, smoothness_se, compactness_se, concavity_se, symmetry_se, fractal_dimension_se, area_worst, compactness_worst, concavity_worst. So we need to tranform them. Applying a log transfrom will solve the problem!"}}