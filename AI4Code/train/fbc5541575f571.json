{"cell_type":{"b047e013":"code","ab450151":"code","871f9423":"code","89f201ef":"code","39750ea7":"code","3fd94179":"code","8bef28aa":"code","c7f0efe7":"code","ae88f655":"code","2f0dc529":"code","9dcbaa21":"code","cc09718b":"code","f9197cf4":"code","a6513ad6":"code","6d08ca6b":"code","7dd84106":"code","9b6ace59":"code","d0133284":"code","e806beca":"markdown","77d8ddd3":"markdown","4a8994a0":"markdown","00ed5803":"markdown","74af7ace":"markdown","fd6e5d3f":"markdown","891d6cc9":"markdown","01302dcc":"markdown","d9da9f91":"markdown","c1541d5d":"markdown","8d2c3dcb":"markdown","c9d0c164":"markdown","b0a3116a":"markdown","7bbef196":"markdown","aac418cb":"markdown","51aabfe6":"markdown"},"source":{"b047e013":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport os\nimport inspect","ab450151":"%load_ext autoreload\n%autoreload 2","871f9423":"!pip install chess","89f201ef":"!pip install --upgrade git+https:\/\/github.com\/arjangroen\/RLC.git  # RLC is the Reinforcement Learning package","39750ea7":"!pip list | grep keras","3fd94179":"from tensorflow.keras.optimizers import SGD","8bef28aa":"%%writefile \/opt\/conda\/lib\/python3.7\/site-packages\/RLC\/capture_chess\/agent.py\n\nfrom keras.models import Model, clone_model\nfrom keras.layers import Input, Conv2D, Dense, Reshape, Dot, Activation, Multiply\nfrom tensorflow.keras.optimizers import SGD\nimport numpy as np\nimport keras.backend as K\n\n\ndef policy_gradient_loss(Returns):\n    def modified_crossentropy(action, action_probs):\n        cost = (K.categorical_crossentropy(action, action_probs, from_logits=False, axis=1) * Returns)\n        return K.mean(cost)\n\n    return modified_crossentropy\n\n\nclass Agent(object):\n\n    def __init__(self, gamma=0.5, network='linear', lr=0.01, verbose=0):\n        \"\"\"\n        Agent that plays the white pieces in capture chess\n        Args:\n            gamma: float\n                Temporal discount factor\n            network: str\n                'linear' or 'conv'\n            lr: float\n                Learning rate, ideally around 0.1\n        \"\"\"\n        self.gamma = gamma\n        self.network = network\n        self.lr = lr\n        self.verbose = verbose\n        self.init_network()\n        self.weight_memory = []\n        self.long_term_mean = []\n\n    def init_network(self):\n        \"\"\"\n        Initialize the network\n        Returns:\n\n        \"\"\"\n        if self.network == 'linear':\n            self.init_linear_network()\n        elif self.network == 'conv':\n            self.init_conv_network()\n        elif self.network == 'conv_pg':\n            self.init_conv_pg()\n\n    def fix_model(self):\n        \"\"\"\n        The fixed model is the model used for bootstrapping\n        Returns:\n        \"\"\"\n        optimizer = SGD(lr=self.lr, momentum=0.0, decay=0.0, nesterov=False)\n        self.fixed_model = clone_model(self.model)\n        self.fixed_model.compile(optimizer=optimizer, loss='mse', metrics=['mae'])\n        self.fixed_model.set_weights(self.model.get_weights())\n\n    def init_linear_network(self):\n        \"\"\"\n        Initialize a linear neural network\n        Returns:\n\n        \"\"\"\n        optimizer = SGD(lr=self.lr, momentum=0.0, decay=0.0, nesterov=False)\n        input_layer = Input(shape=(8, 8, 8), name='board_layer')\n        reshape_input = Reshape((512,))(input_layer)\n        output_layer = Dense(4096)(reshape_input)\n        self.model = Model(inputs=[input_layer], outputs=[output_layer])\n        self.model.compile(optimizer=optimizer, loss='mse', metrics=['mae'])\n\n    def init_conv_network(self):\n        \"\"\"\n        Initialize a convolutional neural network\n        Returns:\n\n        \"\"\"\n        optimizer = SGD(lr=self.lr, momentum=0.0, decay=0.0, nesterov=False)\n        input_layer = Input(shape=(8, 8, 8), name='board_layer')\n        inter_layer_1 = Conv2D(1, (1, 1), data_format=\"channels_first\")(input_layer)  # 1,8,8\n        inter_layer_2 = Conv2D(1, (1, 1), data_format=\"channels_first\")(input_layer)  # 1,8,8\n        flat_1 = Reshape(target_shape=(1, 64))(inter_layer_1)\n        flat_2 = Reshape(target_shape=(1, 64))(inter_layer_2)\n        output_dot_layer = Dot(axes=1)([flat_1, flat_2])\n        output_layer = Reshape(target_shape=(4096,))(output_dot_layer)\n        self.model = Model(inputs=[input_layer], outputs=[output_layer])\n        self.model.compile(optimizer=optimizer, loss='mse', metrics=['mae'])\n\n    def init_conv_pg(self):\n        \"\"\"\n        Convnet net for policy gradients\n        Returns:\n\n        \"\"\"\n        optimizer = SGD(lr=self.lr, momentum=0.0, decay=0.0, nesterov=False)\n        input_layer = Input(shape=(8, 8, 8), name='board_layer')\n        R = Input(shape=(1,), name='Rewards')\n        legal_moves = Input(shape=(4096,), name='legal_move_mask')\n        inter_layer_1 = Conv2D(1, (1, 1), data_format=\"channels_first\")(input_layer)  # 1,8,8\n        inter_layer_2 = Conv2D(1, (1, 1), data_format=\"channels_first\")(input_layer)  # 1,8,8\n        flat_1 = Reshape(target_shape=(1, 64))(inter_layer_1)\n        flat_2 = Reshape(target_shape=(1, 64))(inter_layer_2)\n        output_dot_layer = Dot(axes=1)([flat_1, flat_2])\n        output_layer = Reshape(target_shape=(4096,))(output_dot_layer)\n        softmax_layer = Activation('softmax')(output_layer)\n        legal_softmax_layer = Multiply()([legal_moves, softmax_layer])  # Select legal moves\n        self.model = Model(inputs=[input_layer, R, legal_moves], outputs=[legal_softmax_layer])\n        self.model.compile(optimizer=optimizer, loss=policy_gradient_loss(R))\n\n    def network_update(self, minibatch):\n        \"\"\"\n        Update the Q-network using samples from the minibatch\n        Args:\n            minibatch: list\n                The minibatch contains the states, moves, rewards and new states.\n\n        Returns:\n            td_errors: np.array\n                array of temporal difference errors\n\n        \"\"\"\n\n        # Prepare separate lists\n        states, moves, rewards, new_states = [], [], [], []\n        td_errors = []\n        episode_ends = []\n        for sample in minibatch:\n            states.append(sample[0])\n            moves.append(sample[1])\n            rewards.append(sample[2])\n            new_states.append(sample[3])\n\n            # Episode end detection\n            if np.array_equal(sample[3], sample[3] * 0):\n                episode_ends.append(0)\n            else:\n                episode_ends.append(1)\n\n        # The Q target\n        q_target = np.array(rewards) + np.array(episode_ends) * self.gamma * np.max(\n            self.fixed_model.predict(np.stack(new_states, axis=0)), axis=1)\n\n        # The Q value for the remaining actions\n        q_state = self.model.predict(np.stack(states, axis=0))  # batch x 64 x 64\n\n        # Combine the Q target with the other Q values.\n        q_state = np.reshape(q_state, (len(minibatch), 64, 64))\n        for idx, move in enumerate(moves):\n            td_errors.append(q_state[idx, move[0], move[1]] - q_target[idx])\n            q_state[idx, move[0], move[1]] = q_target[idx]\n        q_state = np.reshape(q_state, (len(minibatch), 4096))\n\n        # Perform a step of minibatch Gradient Descent.\n        self.model.fit(x=np.stack(states, axis=0), y=q_state, epochs=1, verbose=0)\n\n        return td_errors\n\n    def get_action_values(self, state):\n        \"\"\"\n        Get action values of a state\n        Args:\n            state: np.ndarray with shape (8,8,8)\n                layer_board representation\n\n        Returns:\n            action values\n\n        \"\"\"\n        return self.fixed_model.predict(state) + np.random.randn() * 1e-9\n\n    def policy_gradient_update(self, states, actions, rewards, action_spaces, actor_critic=False):\n        \"\"\"\n        Update parameters with Monte Carlo Policy Gradient algorithm\n        Args:\n            states: (list of tuples) state sequence in episode\n            actions: action sequence in episode\n            rewards: rewards sequence in episode\n\n        Returns:\n\n        \"\"\"\n        n_steps = len(states)\n        Returns = []\n        targets = np.zeros((n_steps, 64, 64))\n        for t in range(n_steps):\n            action = actions[t]\n            targets[t, action[0], action[1]] = 1\n            if actor_critic:\n                R = rewards[t, action[0] * 64 + action[1]]\n            else:\n                R = np.sum([r * self.gamma ** i for i, r in enumerate(rewards[t:])])\n            Returns.append(R)\n\n        if not actor_critic:\n            mean_return = np.mean(Returns)\n            self.long_term_mean.append(mean_return)\n            train_returns = np.stack(Returns, axis=0) - np.mean(self.long_term_mean)\n        else:\n            train_returns = np.stack(Returns, axis=0)\n        # print(train_returns.shape)\n        targets = targets.reshape((n_steps, 4096))\n        self.weight_memory.append(self.model.get_weights())\n        self.model.fit(x=[np.stack(states, axis=0),\n                          train_returns,\n                          np.concatenate(action_spaces, axis=0)\n                          ],\n                       y=[np.stack(targets, axis=0)],\n                       verbose=self.verbose\n                       )","c7f0efe7":"import chess\nfrom chess.pgn import Game\nimport RLC\n\nfrom RLC.capture_chess.environment import Board\nfrom RLC.capture_chess.learn import Q_learning\nfrom RLC.capture_chess.agent import Agent","ae88f655":"board = Board()\nboard.board","2f0dc529":"board.layer_board[1,::-1,:].astype(int)","9dcbaa21":"board = Board()\nagent = Agent(network='conv',gamma=0.1,lr=0.07)\nR = Q_learning(agent,board)\nR.agent.fix_model()\nR.agent.model.summary()","cc09718b":"print(inspect.getsource(agent.network_update))","f9197cf4":"print(inspect.getsource(R.play_game))","a6513ad6":"pgn = R.learn(iters=750)","6d08ca6b":"reward_smooth = pd.DataFrame(R.reward_trace)\nreward_smooth.rolling(window=125,min_periods=0).mean().plot(figsize=(16,9),title='average performance over the last 125 steps')","7dd84106":"with open(\"final_game.pgn\",\"w\") as log:\n    log.write(str(pgn))","9b6ace59":"board.reset()\nbl = board.layer_board\nbl[6,:,:] = 1\/10  # Assume we are in move 10\nav = R.agent.get_action_values(np.expand_dims(bl,axis=0))\n\nav = av.reshape((64,64))\n\np = board.board.piece_at(20)#.symbol()\n\n\nwhite_pieces = ['P','N','B','R','Q','K']\nblack_piece = ['_','p','n','b','r','q','k']\n\ndf = pd.DataFrame(np.zeros((6,7)))\n\ndf.index = white_pieces\ndf.columns = black_piece\n\nfor from_square in range(16):\n    for to_square in range(30,64):\n        from_piece = board.board.piece_at(from_square).symbol()\n        to_piece = board.board.piece_at(to_square)\n        if to_piece:\n            to_piece = to_piece.symbol()\n        else:\n            to_piece = '_'\n        df.loc[from_piece,to_piece] = av[from_square,to_square]\n        \n        ","d0133284":"df[['_','p','n','b','r','q']]","e806beca":"### The Agent\n* The agent is no longer a single piece, it's a chess player\n* Its action space consist of 64x64=4096 actions:\n    * There are 8x8 = 64 piece from where a piece can be picked up\n    * And another 64 pieces from where a piece can be dropped. \n* Of course, only certain actions are legal. Which actions are legal in a certain state is part of the environment (in RL, anything outside the control of the agent is considered part of the environment). We can use the python-chess package to select legal moves. (It seems that AlphaZero uses a similar approach https:\/\/ai.stackexchange.com\/questions\/7979\/why-does-the-policy-network-in-alphazero-work)","77d8ddd3":"The PGN file is exported to the output folder. You can analyse is by pasting it on the [chess.com analysis board](https:\/\/www.chess.com\/analysis)","4a8994a0":"### Notebook III: Q-networks\nIn this notebook I implement an simplified version of chess named capture chess. In this environment the agent (playing white) is rewarded for capturing pieces (not for checkmate).  After running this notebook, you end up with an agent that can capture pieces against a random oponnent as demonstrated in the gif below. The main difference between this notebook and the previous one is that I use Q-networks as an alternative to Q-tables. Q-tables are nice and straightforward, but can only contain a limited amount of action values. Chess has state space complexity of 10<sup>47<\/sup>. Needless to say, this is too much information to put in a Q-table. This is where supervised learning comes in. A Q-network can represent a generalized mapping from state to action values.\n\n![](https:\/\/images.chesscomfiles.com\/uploads\/game-gifs\/90px\/green\/neo\/0\/cc\/0\/0\/aXFZUWpyN1Brc1BPbHQwS211WEhudkh6cXohMGFPMExPUTJNUTY4MDY1OTI1NFpSND8yOT85M1Y5MTA3MUxLQ3RDUkpDSjcwTE0wN293V0d6Rzc2cHhWTXJ6NlhzQVg0dUM0WGNNWDU,.gif)\n\n","00ed5803":"[Notebook 1: Policy Iteration](https:\/\/www.kaggle.com\/arjanso\/reinforcement-learning-chess-1-policy-iteration)  \n[Notebook 2: Model-free learning](https:\/\/www.kaggle.com\/arjanso\/reinforcement-learning-chess-2-model-free-methods)  \n[Notebook 4: Policy Gradients](https:\/\/www.kaggle.com\/arjanso\/reinforcement-learning-chess-4-policy-gradients)","74af7ace":"#### Numerical representation of the pawns (layer 0)\nChange the index of the first dimension to see the other pieces","fd6e5d3f":"#### Board representation of python-chess:","891d6cc9":"### The environment: Capture Chess\nIn this notebook we'll upgrade our environment to one that behaves more like real chess. It is mostly based on the Board object from python-chess.\nSome modifications are made to make it easier for the algorithm to converge:\n* There is a maximum of 25 moves, after that the environment resets\n* Our Agent only plays white\n* The Black player is part of the environment and returns random moves\n* The reward structure is not based on winning\/losing\/drawing but on capturing black pieces:\n    - pawn capture: +1\n    - knight capture: +3\n    - bishop capture: +3\n    - rook capture: +5\n    - queen capture: +9\n* Our state is represent by an 8x8x8 array\n    - Plane 0 represents pawns\n    - Plane 1 represents rooks\n    - Plane 2 represents knights\n    - Plane 3 represents bishops\n    - Plane 4 represents queens\n    - Plane 5 represents kings\n    - Plane 6 represents 1\/fullmove number (needed for markov property)\n    - Plane 7 represents can-claim-draw\n* White pieces have the value 1, black pieces are minus 1\n       \n","01302dcc":"#### Demo","d9da9f91":"## References\nReinforcement Learning: An Introduction  \n> Richard S. Sutton and Andrew G. Barto  \n> 1st Edition  \n> MIT Press, march 1998  \n\nRL Course by David Silver: Lecture playlist  \n> https:\/\/www.youtube.com\/watch?v=2pWv7GOvuf0&list=PLqYmG7hTraZDM-OYHWgPebj2MfCFzFObQ  \n\nExperience Replay  \n> https:\/\/datascience.stackexchange.com\/questions\/20535\/what-is-experience-replay-and-what-are-its-benefits","c1541d5d":"#### Q learning with a Q-network\n**Theory**\n- The Q-network is usually either a linear regression or a (deep) neural network. \n- The input of the network is the state (S) and the output is the predicted action value of each Action (in our case, 4096 values). \n- The idea is similar to learning with Q-tables. We update our Q value in the direction of the discounted reward + the max successor state action value\n- I used prioritized experience replay to de-correlate the updates. If you want to now more about it, check the link in the references\n> - I used fixed-Q targets to stabilize the learning process. ","8d2c3dcb":"## Reinforcement Learning Chess\nReinforcement Learning Chess is a series of notebooks where I implement Reinforcement Learning algorithms to develop a chess AI. I start of with simpler versions (environments) that can be tackled with simple methods and gradually expand on those concepts untill I have a full-flegded chess AI.","c9d0c164":"#### Import and Install","b0a3116a":"* ### Learned action values for capturing black (lower case) with white (upper case) pieces.\nUnderscore represents capturing an empty square","7bbef196":"#### Implementation\n- I built two networks, A linear one and a convolutional one\n- The linear model maps the state (8,8,8) to the actions (64,64), resulting in over 32k trainable weights! This is highly inefficient because there is no parameter sharing, but it will work.\n- The convolutional model uses 2 1x1 convulutions and takes the outer product of the resulting arrays. This results in only 18 trainable weights! \n    - Advantage: More parameter sharing -> faster convergence\n    - Disadvantage: Information gets lost -> lower performance\n- For a real chess AI we need bigger neural networks. But now the neural network only has to learn to capture valuable pieces.","aac418cb":"## Learned action values analysis\nSo what has the network learned? The code below checks the action values of capturing every black piece for every white piece. \n- We expect that the action values for capturing black pieces is similar to the (Reinfeld) rewards we put in our environment. \n- Of course the action values also depend on the risk of re-capture by black and the opportunity for consecutive capture. ","51aabfe6":"#### Implementation"}}