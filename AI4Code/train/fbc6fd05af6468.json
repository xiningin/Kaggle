{"cell_type":{"90c2ec9a":"code","55a71865":"code","b4fdc3a3":"code","cde867c2":"code","7e910a78":"code","aa25bd47":"code","cb21ab5b":"code","c2e4c21f":"code","fdd9d5fc":"code","36af61c8":"code","dd53e606":"code","c58561b9":"markdown","4133800c":"markdown","48111b25":"markdown","05844a5f":"markdown","0b5827fc":"markdown","0e89d6cb":"markdown","511c00ff":"markdown","d0e4517f":"markdown","87f7cf44":"markdown","2f99d13f":"markdown","0391e08a":"markdown","58a84bb2":"markdown","8faee03b":"markdown","7adf283d":"markdown","abdf4bcd":"markdown"},"source":{"90c2ec9a":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\n\nfrom surprise import Dataset, Reader, NormalPredictor, KNNBasic, KNNWithMeans, SVD, accuracy\nfrom surprise.model_selection import cross_validate\n\nfrom sklearn.neighbors import NearestNeighbors\n\nplt.style.use('seaborn-colorblind')\nnp.random.seed(8)\nwarnings.filterwarnings(action=\"ignore\")","55a71865":"df_ratings = pd.read_csv(\"\/kaggle\/input\/anime-recommendations-database\/rating.csv\")\n\ndisplay(df_ratings.sample(5, random_state=8))\n\nprint(f\"\"\"Number of total ratings: {df_ratings.shape[0]}. \nNumber of different users: {df_ratings.user_id.nunique()}. \nNumber of different animes: {df_ratings.anime_id.nunique()}.\"\"\")","b4fdc3a3":"def countplot_with_percentages(serie):\n    ax = sns.countplot(serie)\n    total_count = serie.count()\n\n    for p in ax.patches:\n        x = p.get_bbox().get_points()[:, 0]\n        y = p.get_bbox().get_points()[1, 1]\n        percentage = p.get_height() \/ total_count * 100\n        ax.annotate(f'\\n{percentage: .2f}%',\n                    (x.mean(), y), ha='center', size=14)\n\n\nplt.figure(figsize=(12, 6))\nplt.axes(yscale=\"log\")\n\ncountplot_with_percentages(df_ratings.rating)\n\nplt.title(\"Distribution of Ratings in log scale\")\nplt.ylabel(\"Count in log scale\")\nplt.xlabel(\"Ratings\");","cde867c2":"cleaned_df_ratings = df_ratings[df_ratings.rating > 0].iloc[:50000, :]\nreader = Reader(line_format='user item rating')\ndata = Dataset.load_from_df(cleaned_df_ratings, reader)","7e910a78":"plt.figure(figsize=(12, 6))\n#plt.axes(yscale=\"log\")\n\ncountplot_with_percentages(cleaned_df_ratings.rating)\n\nplt.title(\"Distribution of Ratings after cleaning the data\")\nplt.ylabel(\"Count\")\nplt.xlabel(\"Ratings\");","aa25bd47":"algorithm = KNNBasic(k=40, random_state=8, sim_options={\n    'name': 'pearson',\n    'user_based': True\n})\n\ncross_validate(algorithm, data, measures=['RMSE'], cv=5, verbose=True);","cb21ab5b":"algorithm = KNNWithMeans(k=40, random_state=8, sim_options={\n    'name': 'pearson',\n    'user_based': True\n})\n\ncross_validate(algorithm, data, measures=['RMSE'], cv=5, verbose=True);","c2e4c21f":"algorithm = SVD(random_state=8, n_factors=100)\n\ncross_validate(algorithm, data, measures=['RMSE'], cv=5, verbose=True);","fdd9d5fc":"df_animes = pd.read_csv(\"\/kaggle\/input\/anime-recommendations-database\/anime.csv\", index_col=\"anime_id\")\ndf_animes.head()","36af61c8":"# Drop the animes with null values\ndf_clean_animes = df_animes[df_animes.genre.notna() & df_animes.type.notna()]\n\n# First, split the genre column by comma and expand the list so there is\n# a column for each genre. Now we have 13 columns, because the anime with\n# most genres tags has 13 tags\ngenres = df_clean_animes.genre.str.split(\", \", expand=True)\n\n# Now we can get the list of unique genres. We \"convert\" the dataframe into\n# a single dimension array and take the unique values\nunique_genres = pd.Series(genres.values.ravel('K')).dropna().unique()\n\n# Getting the dummy variables will result in having a lot more columns\n# than unique genres\ndummies = pd.get_dummies(genres)\n\n# So we sum up the columns with the same genre to have a single column for\n# each genre\nfor genre in unique_genres:\n    df_clean_animes[\"Genre: \" + genre] = dummies.loc[:, dummies.columns.str.endswith(genre)].sum(axis=1)\n    \n# Add the type dummies\ntype_dummies = pd.get_dummies(df_clean_animes.type, prefix=\"Type:\", prefix_sep=\" \")\ndf_clean_animes = pd.concat([df_clean_animes, type_dummies], axis=1)\n\ndf_clean_animes = df_clean_animes.drop(columns=[\"name\", \"type\", \"genre\", \"episodes\", \"rating\", \"members\"])\ndf_clean_animes.head()","dd53e606":"# Helper function to get the features of an anime given its name\ndef get_features_from_anime_name(name):\n    return df_clean_animes.loc[df_animes[df_animes.name == name].index]\n\n\n# Build and \"train\" the model\nneigh = NearestNeighbors(15)\nneigh.fit(df_clean_animes.values)\n\n# Get the features of this anime\nitem_to_compare = get_features_from_anime_name(\"Dragon Ball Z\")\n\n# Get the indices of the most similar items found\n# Note: these are ignoring the dataframe indices and starting from 0\nindex = neigh.kneighbors(item_to_compare, return_distance=False)\n\n# Show the details of the items found\ndf_animes.loc[df_animes.index[index][0]]","c58561b9":"Looks like many of the Dragon Ball series and movies have the same genre tags as \"Dragon Ball Z\" so the nearest neighbors did a good returning those.","4133800c":"The ratings go from 1 to 10. Where 1 means that the user didn't like the anime at all. And 10 means that the user liked it very much. \n\nThere is a special -1 rating that represents users that have watched an anime but didn't rate it. If we had more information like watch time we could infer an explicit rating. But we don't, so for simplicity we'll treat these ratings as missing ratings by removing them.","48111b25":"<a id=\"Content-based-filtering\"><\/a>\n# Content-based Filtering\n\nSo far we've only looked at Collaborative filtering approaches. They can be very powerful but they only focus on the interactions between users and\/or items. We may have more information about the items that isn't exploited with collaborative filtering methods.\n\nContent-based filtering uses item features to recommend similar items. It is usually seen applied to text because we can extract a lot of information from them with Natural Language Processing methods. But we can also use content-based filtering in this context because we have access to some information about the animes.","05844a5f":"<a id=\"Model-based-methods\"><\/a>\n## Model-based methods\n\nNeighborhood-based methods are very simple but they have several disadvantages. Two fundamental problems are the following:\n\n- They need to use the matrix of ratings to predict a new rating. Hence, they are not scalable to large datasets. \n- When the dataset gets very sparse, these methods get less accurate: as a dataset gets more sparse, it gets more likely that two users or items don't have many overlapping values, which makes the similarity measures unreliable or not computable if no overlapping value is found.\n\nModel-based methods build a model from the matrix of ratings. This implies a important decrease of the memory needed. And one of the most popular methods also deals well with the sparsity of the data: SVD (Singular Value Decomposition). SVD is a well-known method for matrix factorization that can describe any matrix as the product of three matrices.\n\n<!-- from https:\/\/tex.stackexchange.com\/questions\/183090\/replicate-the-singular-value-decomposition-figure-in-latex -->\n$$\n\\begin{equation}\n\\underbrace{\\mathbf{R}}_{m \\times n} = \\underbrace{\\mathbf{U}}_{m \\times m} \\cdot \\underbrace{\\mathbf{\\Sigma}}_{n\\times n} \\cdot \\underbrace{\\mathbf{V}^{\\text{T}}}_{n \\times n}\n\\end{equation}\n$$\n\nThe key component for using this method in the context of recommendation systems is the $\\Sigma$ matrix. It is a diagonal matrix whose entries are ordered from higher to lower values. Thanks to this, **we can keep only the first few entries and still have a very good approximation of $\\mathbf{R}$** with a much lower memory cost. \n\nSo we can represent the matrix of ratings as the product of two matrices that are much smaller:\n\n$$\n\\begin{equation}\n\\underbrace{\\mathbf{R}}_{m \\times n} \\approx (\\underbrace{\\mathbf{U}}_{m \\times r} \\cdot \\underbrace{\\sqrt{\\mathbf{\\Sigma}}}_{r\\times r}) \\cdot (\\underbrace{\\sqrt{\\mathbf{\\Sigma}}}_{r\\times r} \\cdot \\underbrace{\\mathbf{V}^{\\text{T}}}_{r \\times n})\n\\end{equation}\n$$\n\n$r$ has to be specified by the user and it can be much smaller than $n$. This representation of the data in a low-dimensional space is known in Machine Learning as an embedding. And a very interesting property of embeddings is that, to do such a good job representing the original data in a lower dimensional space, they have to capture some of the semantics of the original data. Though we cannot know for sure what that could be. \n\nIn the book \"Recommender Systems The Textbook\" by Charu C. Aggarwal, the author gives an illustrative example showing how powerful these embeddings can be. Though we cannot interpret the embeddings, we know that they have to capture some semantics of the original data, so they could be capturing, for example, the genre of the items and the preference of the users for each genre.\n\n![Screenshot%202020-05-06%20at%2023.12.46.png](attachment:Screenshot%202020-05-06%20at%2023.12.46.png)\n\nThere are many ways to compute singular value decompositions, the one that we are using next is the one that Simon Funk used during the Netflix Price competition, where his team got third place. He explains it in detail in [a blog post](https:\/\/sifter.org\/~simon\/journal\/20061211.html). But the the gist of it, in his own words:\n\n> But, just because there are five hundred really complicated ways of computing singular value decompositions in the literature doesn't mean there isn't a really simple way too: Just take the derivative of the approximation error and follow it. This has the added bonus that we can choose to simply ignore the unknown error on the 8.4B empty slots.","0b5827fc":"<a id=\"User-based-KNN-with-means\"><\/a>\n### User-based KNN with means\n\nNot every user rate items the same way, some tend to give generous ratings, whereas others tend to rate all items negatively. Yet they may be correlated so there is the posibility that, when trying to predict the rating that a \"generous\" user would give to an item, many of the similar users tend to give lower ratings. So the predicted rating will not be as high as the ones that the user tends to give.\n\nA modification of the basic KNN method is to mean-center the ratings to account for this difference of scales between different users:\n\n$$\n\\hat{r}_{ui} = \\mu_u + \\frac{ \\sum\\limits_{v \\in N_i(u)} \\text{pearson_sim}(u, v) \\cdot (r_{vi} - \\mu_v)} {\\sum\\limits_{v \\in N_i(u)} \\text{pearson_sim}(u, v)}\n$$","0e89d6cb":"<a id=\"Collaborative-filtering\"><\/a>\n# Collaborative filtering\n\nThe format of the data can be seen as a sparse representation of a matrix of ratings. Its dimensions are $m \\times n$, where $m$ is the number of users and $n$ is the number of animes and each cell of the matrix represents the rating that an user gives to an anime.\n\n![SparseToMatrix.png](attachment:SparseToMatrix.png)\n\nThe basic idea of collaborative filtering algorithms is that we can look for similar users and\/or similar items (animes in this case) to decide what to recommend a certain user. For example, **if Alice and Bob like similar items they will have similar ratings on those items, so we can recommend to Alice something that Bob likes and that she hasn't seen yet**.\n\nTwo major types of collaborative filtering algorithms are neighborhood-based methods and model-based methods.\n\n<a id=\"Neighborhood-based-methods\"><\/a>\n## Neighborhood-based methods\n\nThese were among the first methods developed for recommender systems. To know what to recommend, these methods calculate a similarity function (for instance, cosine similarity or Pearson correlation) between all users or items. Then, a prediction of the rating that the user would give to a certain item is calculated based on the k most similar items. Finally, these predictions are used to know what would be best to recommend to the user.\n\n<a id=\"User-based-Basic-KNN\"><\/a>\n### User-based Basic KNN\n\nLets see how a very simple neighborhood-based algorithm works on our data. This algorithm first calculates the pearson similarity between all pair of users. Then, to predict a rating for an item, we take the ratings that the 40 most similar users gave to the item. Finally, the average of the ratings weighted by the similarities is returned:\n\n$$\n\\hat{r}_{ui} = \\frac{ \\sum\\limits_{v \\in N_i(u)} \\text{pearson_sim}(u, v) \\cdot r_{vi}} {\\sum\\limits_{v \\in N_i(u)} \\text{pearson_sim}(u, v)}\n$$\n\nNote: $\\hat{r}_{ui}$ is the prediction of the rating that user $u$ would give to the item $i$. And $N_i(u)$ is the neigborhood of the 40 most similar users to $u$ that have rated the item $i$.\n\nBelow we can see the results after 5-folds cross-validation using the [Surprise library](https:\/\/surprise.readthedocs.io\/en\/stable\/index.html)","511c00ff":"There doesn't seem to be a significant improvement. Again, this could be due to the reduced dataset that we are using. And\/or maybe because most of the users tend to give \"generous\" ratings.","d0e4517f":"<a id=\"Resources\"><\/a>\n## Resources\n\n- [Google recommendation systems course](https:\/\/developers.google.com\/machine-learning\/recommendation)\n- [Book \"Recommender Systems The Textbook\" by Charu C. Aggarwal](https:\/\/link.springer.com\/book\/10.1007\/978-3-319-29659-3). A great book that is available for free at the time of writting this notebook. \n- [Simon Funk's blog post](https:\/\/sifter.org\/~simon\/journal\/20061211.html). A post explaining his version of the SVD algorithm that got him third place on the Netflix price competition.\n- [Paper \"Deep Neural Networks for YouTube Recommendations\" by Paul Covington, Jay Adams & Emre Sargin](https:\/\/research.google\/pubs\/pub45530\/). A paper explaining a high level overview of the YouTube recommendation system\n- [Surprise library](https:\/\/github.com\/NicolasHug\/Surprise)","87f7cf44":"![bruce-tang-nKO_1QyFh9o-unsplash.jpg](attachment:bruce-tang-nKO_1QyFh9o-unsplash.jpg)\n\n<center>Photo by Bruce Tang on Unsplash<\/center>\n\n\n# Table of Contents\n1. [Recommender systems](#Recommender-Systems)\n2. [Anime ratings](#Anime-ratings)\n3. [Collaborative filtering](#Collaborative-filtering)\n    1. [Neighborhood-based methods](#Neighborhood-based-methods)\n        1. [User-based Basic KNN](#User-based-Basic-KNN)\n        2. [User-based KNN with means](#User-based-KNN-with-means)\n    2. [Model-based methods](#Model-based-methods)\n4. [Content-based filtering](#Content-based-filtering)\n4. [Resources](#Resources)\n\n\n<a id=\"Recommender-Systems\"><\/a>\n# Recommender systems\n\nOn large databases, users may not know what to search for or they miss items that they could have liked. Recommender systems aim to help users find items that they may like in a large database. \n\nAccording to Google:\n\n> - 40% of app installs on Google Play come from recommendations.\n- 60% of watch time on YouTube comes from recommendations.\n\nSource: [Google developer's recommendation systems course](https:\/\/developers.google.com\/machine-learning\/recommendation\/overview)\n\nIn this notebook we'll see the basics of recommender systems applied to the recommendation of animes using the Surprise library.","2f99d13f":"Again, we don't see a significant improvement due to the bad subset selection of the data.","0391e08a":"In particular we'll use the genre and type of anime as features to use as input to a k-nearest neighbor algorithm.\n\nFirst, for simplicity, lets drop the few animes that have missing values and represent the genres in a simple one-hot encoding way.","58a84bb2":"Now we are ready to build and use a k-nearest neighbors model. Lets see what are the 15 most similar animes to \"Dragon Ball Z\" that the model returns","8faee03b":"Moreover, I'll just take the first 50,000 ratings from the data to keep a reasonable time and memory cost for running this notebook. Tough the results will be bad, the purpose of this notebook is educational, for a large scale system we can look at implementations of recommender system methods that can deal with a huge ammount of data. For instance, there are some [implementations for Apache Spark](https:\/\/spark.apache.org\/docs\/latest\/mllib-collaborative-filtering.html).","7adf283d":"<a id=\"Anime-ratings\"><\/a>\n# Anime ratings\n\nThe file `ratings.csv` contains a line for each explicit rating that a user has given to an anime. For example:","abdf4bcd":"We have around 3 units of Root Mean Squared Error in each fold. These results are bad taking into account that the ratings vary from 1 to 10 and that most of the ratings are around 7 to 10. However this is expected as we are using about 0.6% of the data to keep the computation time and memory reasonable."}}