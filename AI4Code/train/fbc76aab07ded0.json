{"cell_type":{"9d8e86b2":"code","656c8fa6":"code","ab079c8a":"code","875eb6d4":"code","fa3e6a74":"code","98fc7c32":"code","0fa1f12f":"code","ef3124aa":"code","f3aa58a6":"code","909d6ea1":"code","2c19641b":"code","7b2ba033":"code","8e6bb954":"code","41c14703":"code","7a26ee97":"code","65ce142d":"code","16b9cce9":"code","4842c987":"code","5abeb638":"code","9d5d9542":"code","50b85c92":"code","67fcdecf":"code","27f3791f":"code","ad671205":"markdown","a2a43421":"markdown","e43a7ea3":"markdown","65e39a6c":"markdown","b1a6ae43":"markdown","8142d403":"markdown","65495dfc":"markdown","ff244468":"markdown","a4fc8bca":"markdown","8f0a0f12":"markdown","a4c5d16a":"markdown","89b82d08":"markdown","edb089e9":"markdown","4ee1a144":"markdown","af6905b7":"markdown","326ac651":"markdown"},"source":{"9d8e86b2":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n%matplotlib inline\nsns.set_style('whitegrid')","656c8fa6":"from sklearn.datasets import load_breast_cancer\n\ncancer = load_breast_cancer()\n\ncol_names = list(cancer.feature_names)\ncol_names.append('target')\ndf = pd.DataFrame(np.c_[cancer.data, cancer.target], columns=col_names)\ndf.head()","ab079c8a":"print(cancer.target_names)","875eb6d4":"df.describe()","fa3e6a74":"df.info()","98fc7c32":"df.columns","0fa1f12f":"sns.pairplot(df, hue='target', vars=['mean radius', 'mean texture', 'mean perimeter', 'mean area', \n                                     'mean smoothness', 'mean compactness', 'mean concavity',\n                                     'mean concave points', 'mean symmetry', 'mean fractal dimension'])","ef3124aa":"sns.countplot(df['target'], label = \"Count\")","f3aa58a6":"plt.figure(figsize=(10, 8))\nsns.scatterplot(x = 'mean area', y = 'mean smoothness', hue = 'target', data = df)","909d6ea1":"# Let's check the correlation between the variables \n# Strong correlation between the mean radius and mean perimeter, mean area and mean primeter\nplt.figure(figsize=(20,10)) \nsns.heatmap(df.corr(), annot=True) ","2c19641b":"from sklearn.model_selection import cross_val_score, train_test_split\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler\n\nX = df.drop('target', axis=1)\ny = df.target\n\nprint(f\"'X' shape: {X.shape}\")\nprint(f\"'y' shape: {y.shape}\")\n\npipeline = Pipeline([\n    ('min_max_scaler', MinMaxScaler()),\n    ('std_scaler', StandardScaler())\n])\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)","7b2ba033":"from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n\ndef print_score(clf, X_train, y_train, X_test, y_test, train=True):\n    if train:\n        pred = clf.predict(X_train)\n        clf_report = pd.DataFrame(classification_report(y_train, pred, output_dict=True))\n        print(\"Train Result:\\n================================================\")\n        print(f\"Accuracy Score: {accuracy_score(y_train, pred) * 100:.2f}%\")\n        print(\"_______________________________________________\")\n        print(f\"CLASSIFICATION REPORT:\\n{clf_report}\")\n        print(\"_______________________________________________\")\n        print(f\"Confusion Matrix: \\n {confusion_matrix(y_train, pred)}\\n\")\n        \n    elif train==False:\n        pred = clf.predict(X_test)\n        clf_report = pd.DataFrame(classification_report(y_test, pred, output_dict=True))\n        print(\"Test Result:\\n================================================\")        \n        print(f\"Accuracy Score: {accuracy_score(y_test, pred) * 100:.2f}%\")\n        print(\"_______________________________________________\")\n        print(f\"CLASSIFICATION REPORT:\\n{clf_report}\")\n        print(\"_______________________________________________\")\n        print(f\"Confusion Matrix: \\n {confusion_matrix(y_test, pred)}\\n\")","8e6bb954":"from sklearn.svm import LinearSVC\n\nmodel = LinearSVC(loss='hinge', dual=True)\nmodel.fit(X_train, y_train)\n\nprint_score(model, X_train, y_train, X_test, y_test, train=True)\nprint_score(model, X_train, y_train, X_test, y_test, train=False)","41c14703":"from sklearn.svm import SVC\n\n# The hyperparameter coef0 controls how much the model is influenced by high degree ploynomials \nmodel = SVC(kernel='poly', degree=2, gamma='auto', coef0=1, C=5)\nmodel.fit(X_train, y_train)\n\nprint_score(model, X_train, y_train, X_test, y_test, train=True)\nprint_score(model, X_train, y_train, X_test, y_test, train=False)","7a26ee97":"model = SVC(kernel='rbf', gamma=0.5, C=0.1)\nmodel.fit(X_train, y_train)\n\nprint_score(model, X_train, y_train, X_test, y_test, train=True)\nprint_score(model, X_train, y_train, X_test, y_test, train=False)","65ce142d":"X_train = pipeline.fit_transform(X_train)\nX_test = pipeline.transform(X_test)","16b9cce9":"print(\"=======================Linear Kernel SVM==========================\")\nmodel = SVC(kernel='linear')\nmodel.fit(X_train, y_train)\n\nprint_score(model, X_train, y_train, X_test, y_test, train=True)\nprint_score(model, X_train, y_train, X_test, y_test, train=False)\n\nprint(\"=======================Polynomial Kernel SVM==========================\")\nfrom sklearn.svm import SVC\n\nmodel = SVC(kernel='poly', degree=2, gamma='auto')\nmodel.fit(X_train, y_train)\n\nprint_score(model, X_train, y_train, X_test, y_test, train=True)\nprint_score(model, X_train, y_train, X_test, y_test, train=False)\n\nprint(\"=======================Radial Kernel SVM==========================\")\nfrom sklearn.svm import SVC\n\nmodel = SVC(kernel='rbf', gamma=1)\nmodel.fit(X_train, y_train)\n\nprint_score(model, X_train, y_train, X_test, y_test, train=True)\nprint_score(model, X_train, y_train, X_test, y_test, train=False)","4842c987":"from sklearn.model_selection import GridSearchCV\n\nparam_grid = {'C': [0.01, 0.1, 0.5, 1, 10, 100], \n              'gamma': [1, 0.75, 0.5, 0.25, 0.1, 0.01, 0.001], \n              'kernel': ['rbf', 'poly', 'linear']} \n\ngrid = GridSearchCV(SVC(), param_grid, refit=True, verbose=1, cv=5, iid=True)\ngrid.fit(X_train, y_train)\n\nbest_params = grid.best_params_\nprint(f\"Best params: {best_params}\")\n\nsvm_clf = SVC(**best_params)\nsvm_clf.fit(X_train, y_train)\nprint_score(svm_clf, X_train, y_train, X_test, y_test, train=True)\nprint_score(svm_clf, X_train, y_train, X_test, y_test, train=False)","5abeb638":"df.head()","9d5d9542":"scaler = StandardScaler()\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)","50b85c92":"from sklearn.decomposition import PCA\n\npca = PCA(n_components=3)\nscaler = StandardScaler()\n\nX_train = pca.fit_transform(X_train)\nX_test = pca.transform(X_test)\n\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)","67fcdecf":"plt.figure(figsize=(8,6))\nplt.scatter(X_train[:,0],X_train[:,1],c=y_train,cmap='plasma')\nplt.xlabel('First principal component')\nplt.ylabel('Second Principal Component')","27f3791f":"param_grid = {'C': [0.01, 0.1, 0.5, 1, 10, 100], \n              'gamma': [1, 0.75, 0.5, 0.25, 0.1, 0.01, 0.001], \n              'kernel': ['rbf', 'poly', 'linear']} \n\ngrid = GridSearchCV(SVC(), param_grid, refit=True, verbose=1, cv=5, iid=True)\ngrid.fit(X_train, y_train)\nbest_params = grid.best_params_\nprint(f\"Best params: {best_params}\")\n\nsvm_clf = SVC(**best_params)\nsvm_clf.fit(X_train, y_train)\n\nprint_score(svm_clf, X_train, y_train, X_test, y_test, train=True)\nprint_score(svm_clf, X_train, y_train, X_test, y_test, train=False)","ad671205":"# 4. Principal Component Analysis\n\nPCA is:\n* Linear dimensionality reduction using Singular Value Decomposition of the data to project it to a lower dimensional space.\n* Unsupervised Machine Learning\n* A transformation of your data and attempts to find out what features explain the most variance in your data. For example:\n\n![PCA.png](attachment:PCA.png)","a2a43421":"## 2. 3. Support Vector Machines (Kernels)\n\n- `C parameter`: Controlls trade-off between classifying training points correctly and having a smooth decision boundary.\n    - Small C (loose) makes cost (penalty) of misclassification low (soft margin)\n    - Large C (strict) makes cost of misclassification high (hard margin), forcing the model to explain input data stricter and potentially over it.\n- `gamma parameter`: Controlls how far the influence of a single training set reaches.\n    - Large gamma: close reach (closer data points have high weight)\n    - Small gamma: far reach (more generalized solution)\n- `degree parameter` : Degree of the polynomial kernel function (`'poly'`). Ignored by all other kernels.\n\nA common approach to find the right hyperparameter values is to use grid search. It is often faster to first do a very coarse grid search, then a finer grid search around the best values found. Having a good sence of the what each hyperparameter actually does can also help you search in the right part of the hyperparameter space.\n****\n### 2. 3. 1. Linear Kernel SVM","e43a7ea3":"## 2. 2. MODEL TRAINING (FINDING A PROBLEM SOLUTION)","65e39a6c":"## 2. 1. VISUALIZING THE DATA","b1a6ae43":"# 5. Summary\n\nIn this notebook you discovered the Support Vector Machine Algorithm for machine learning.  You learned about:\n- What is support vector machine?.\n- Support vector machine implementation in Python.\n- Support Vector Machine kernels (Linear, Polynomial, Radial).\n- How to prepare the data for support vector machine algorithm.\n- Support vector machine hyperparameter tuning.\n- Principal Compenent Analysis and how to use it to reduce the complexity of a problem.\n- How to calculate the Principal Component Analysis for reuse on more data in scikit-learn.\n\n## References:\n- [Support Vector Machine \u2014 Introduction to Machine Learning Algorithms](https:\/\/towardsdatascience.com\/support-vector-machine-introduction-to-machine-learning-algorithms-934a444fca47)\n- [Support Vector Machines for Machine Learning](https:\/\/machinelearningmastery.com\/support-vector-machines-for-machine-learning\/)\n- [Support Vector Machines documentations](https:\/\/scikit-learn.org\/stable\/modules\/svm.html#svm-kernels)\n- [scikit-learn Doc](http:\/\/scikit-learn.org\/stable\/modules\/decomposition.html#pca)\n- [scikit-learn Parameters](http:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.decomposition.PCA.html#sklearn.decomposition.PCA)\n- [How to Calculate Principal Component Analysis (PCA) from Scratch in Python](https:\/\/machinelearningmastery.com\/calculate-principal-component-analysis-scratch-python\/)","8142d403":"Other kernels exist but are not used much more rarely. For example, some kernels are specialized for specific data structures. string kernels are sometimes used when classifying text document on DNA sequences.\n\nWith so many kernels to choose from, how can you decide which one to use? As a rule of thumb, you should always try the linear kernel first, especially if the training set is very large or if it has plenty of features. If the training se is not too large, you should try the Gaussian RBF kernel as well. ","65495dfc":"## 4. 1. PCA Visualization\n\nAs we've noticed before it is difficult to visualize high dimensional data, we can use PCA to find the first two principal components, and visualize the data in this new, two-dimensional space, with a single scatter-plot. Before we do this though, we'll need to scale our data so that each feature has a single unit variance.","ff244468":"### 2. 3. 3. Radial Kernel SVM\nJust like the polynomial features method, the similarity features can be useful with any ","a4fc8bca":"Clearly by using these two components we can easily separate these two classes.\n\n## 4. 2. Interpreting the components \n\nUnfortunately, with this great power of dimensionality reduction, comes the cost of being able to easily understand what these components represent.\n\nThe components correspond to combinations of the original features, the components themselves are stored as an attribute of the fitted PCA object:","8f0a0f12":"PCA with Scikit Learn uses a very similar process to other preprocessing functions that come with SciKit Learn. We instantiate a PCA object, find the principal components using the fit method, then apply the rotation and dimensionality reduction by calling transform().\n\nWe can also specify how many components we want to keep when creating the PCA object.","a4c5d16a":"# 3. Support Vector Machine Hyperparameter tuning","89b82d08":"**Note:**\n\nPrincipal Component Analysis:\n* Used in exploratory data analysis (EDA) \n* Visualize genetic distance and relatedness between populations. \n\n* Method:\n  * Eigenvalue decomposition of a data covariance (or correlation) matrix\n  * Singular value decomposition of a data matrix (After mean centering \/ normalizing ) the data matrix for each attribute.\n\n* Output\n  * Component scores, sometimes called **factor scores** (the transformed variable values)\n  * **loadings** (the weight)\n\n* Data compression and information preservation \n* Visualization\n* Noise filtering\n* Feature extraction and engineering","edb089e9":"# 2. SVM Implementation in Python\n\nWe will use support vector machine in Predicting if the cancer diagnosis is benign or malignant based on several observations\/features.\n\n- 30 features are used, examples:\n        - radius (mean of distances from center to points on the perimeter)\n        - texture (standard deviation of gray-scale values)\n        - perimeter\n        - area\n        - smoothness (local variation in radius lengths)\n        - compactness (perimeter^2 \/ area - 1.0)\n        - concavity (severity of concave portions of the contour)\n        - concave points (number of concave portions of the contour)\n        - symmetry \n        - fractal dimension (\"coastline approximation\" - 1)\n\n- Datasets are linearly separable using all 30 input features\n- Number of Instances: 569\n- Class Distribution: 212 Malignant, 357 Benign\n- Target class:\n         - Malignant\n         - Benign","4ee1a144":"## 2. 4. Data Preparation for SVM\nThis section lists some suggestions for how to best prepare your training data when learning an SVM model.\n\n- **Numerical Inputs:** SVM assumes that your inputs are numeric. If you have categorical inputs you may need to covert them to binary dummy variables (one variable for each category).\n- **Binary Classification:** Basic SVM as described in this post is intended for binary (two-class) classification problems. Although, extensions have been developed for regression and multi-class classification.","af6905b7":"### 2. 3. 2. Polynomial Kernel SVM\n\nThis code trains a SVM classifier using 2rd degree ploynomial kernel.","326ac651":"# Support Vector Machine\n\nA Support Vector Machine (SVM) is a very powerful and versatile Machine Learning model, capable of performing linear or nonliner classification, regression, and even outlier detection. In this notebook, we will discover the support vector machine algorithm as well as it implementation in scikit-learn. We will also discover the Principal Component Analysis and its implementation with scikit-learn.\n\n# 1. Support Vector Machine \u2014 (SVM)\n\nSupport vector machine is another simple algorithm that every machine learning expert should have in his\/her arsenal. Support vector machine is highly preferred by many as it produces significant accuracy with less computation power. Support Vector Machine, abbreviated as SVM can be used for both regression and classification tasks. But, it is widely used in classification objectives.\n\n## 1. 1. What is Support Vector Machine?\nThe objective of the support vector machine algorithm is to find a hyperplane in an N-dimensional space(N \u2014 the number of features) that distinctly classifies the data points.\n\n![svm.png](attachment:svm.png)![svm_2.png](attachment:svm_2.png)\n\nTo separate the two classes of data points, there are many possible hyperplanes that could be chosen. Our objective is to find a plane that has the maximum margin, i.e the maximum distance between data points of both classes. Maximizing the margin distance provides some reinforcement so that future data points can be classified with more confidence.\n\n## 1. 2. Hyperplanes and Support Vectors\n\n![support_vector.png](attachment:support_vector.png)\nHyperplanes are decision boundaries that help classify the data points. Data points falling on either side of the hyperplane can be attributed to different classes. Also, the dimension of the hyperplane depends upon the number of features. If the number of input features is 2, then the hyperplane is just a line. If the number of input features is 3, then the hyperplane becomes a two-dimensional plane. It becomes difficult to imagine when the number of features exceeds 3.\n\n![support_vector_2.jpg](attachment:support_vector_2.jpg)\n\nSupport vectors are data points that are closer to the hyperplane and influence the position and orientation of the hyperplane. Using these support vectors, we maximize the margin of the classifier. Deleting the support vectors will change the position of the hyperplane. These are the points that help us build our SVM.\n\n## 1. 3. Large Margin Intuition\nIn logistic regression, we take the output of the linear function and squash the value within the range of [0,1] using the sigmoid function. If the squashed value is greater than a threshold value(0.5) we assign it a label 1, else we assign it a label 0. In SVM, we take the output of the linear function and if that output is greater than 1, we identify it with one class and if the output is -1, we identify is with another class. Since the threshold values are changed to 1 and -1 in SVM, we obtain this reinforcement range of values([-1,1]) which acts as margin."}}