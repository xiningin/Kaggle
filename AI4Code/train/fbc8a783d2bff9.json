{"cell_type":{"ada5fd47":"code","e6637d15":"code","c09cbef7":"code","9d42048b":"code","85be29db":"code","6ee5e1c9":"code","4fec0c06":"code","345c1594":"code","6b042d1c":"code","ea72f5e6":"code","4552bdaa":"code","c254a8cb":"code","293a1776":"code","e255fbd9":"code","8032d8f4":"code","b412447a":"code","f3f5ab23":"code","c2693c8e":"code","4570c507":"code","58a6e8d1":"code","9781dcda":"code","6d021697":"code","afc20d06":"code","265b037c":"code","6fd55e3d":"markdown","e96ce962":"markdown"},"source":{"ada5fd47":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom sklearn.preprocessing import LabelEncoder, MinMaxScaler, StandardScaler\nfrom sklearn.model_selection import train_test_split\n\nimport matplotlib.pyplot as plt\n%matplotlib inline \n\nimport math\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.\n\n\n\n\n","e6637d15":"def rmsle(y, y_pred):\n    assert len(y) == len(y_pred)\n    to_sum = [(math.log(y_pred[i] + 1) - math.log(y[i] + 1)) ** 2.0 for i,pred in enumerate(y_pred)]\n    return (sum(to_sum) * (1.0\/len(y))) ** 0.5\n#Source: https:\/\/www.kaggle.com\/marknagelberg\/rmsle-function","c09cbef7":"#LOAD DATA\nprint(\"Loading data...\")\ntrain = pd.read_table(\"..\/input\/train.tsv\")\ntest = pd.read_table(\"..\/input\/test.tsv\")\nprint(train.shape)\nprint(test.shape)","9d42048b":"train.info()","85be29db":"test.info()","6ee5e1c9":"train.head()","4fec0c06":"train.describe(include='all').T","345c1594":"train.isnull().sum()","6b042d1c":"#HANDLE MISSING VALUES\nprint(\"Handling missing values...\")\ndef handle_missing(dataset):\n    dataset.category_name.fillna(value=\"missing\", inplace=True)\n    dataset.brand_name.fillna(value=\"missing\", inplace=True)\n    dataset.item_description.fillna(value=\"missing\", inplace=True)\n    return (dataset)\n\ntrain = handle_missing(train)\ntest = handle_missing(test)\nprint(train.shape)\nprint(test.shape)","ea72f5e6":"train.head(3)","4552bdaa":"#PROCESS CATEGORICAL DATA\nprint(\"Handling categorical variables...\")\nle = LabelEncoder()\n\nle.fit(np.hstack([train.category_name, test.category_name]))\ntrain.category_name = le.transform(train.category_name)\ntest.category_name = le.transform(test.category_name)\n\nle.fit(np.hstack([train.brand_name, test.brand_name]))\ntrain.brand_name = le.transform(train.brand_name)\ntest.brand_name = le.transform(test.brand_name)\ndel le\n\ntrain.head(3)\n","c254a8cb":"#PROCESS TEXT: RAW\nprint(\"Text to seq process...\")\nfrom keras.preprocessing.text import Tokenizer\nraw_text = np.hstack([train.item_description.str.lower(), train.name.str.lower()])\n\nprint(\"   Fitting tokenizer...\")\ntok_raw = Tokenizer()\ntok_raw.fit_on_texts(raw_text)\nprint(\"   Transforming text to seq...\")\n\ntrain[\"seq_item_description\"] = tok_raw.texts_to_sequences(train.item_description.str.lower())\ntest[\"seq_item_description\"] = tok_raw.texts_to_sequences(test.item_description.str.lower())\ntrain[\"seq_name\"] = tok_raw.texts_to_sequences(train.name.str.lower())\ntest[\"seq_name\"] = tok_raw.texts_to_sequences(test.name.str.lower())\ntrain.head(3)","293a1776":"#SEQUENCES VARIABLES ANALYSIS\nmax_name_seq = np.max([np.max(train.seq_name.apply(lambda x: len(x))), np.max(test.seq_name.apply(lambda x: len(x)))])\nmax_seq_item_description = np.max([np.max(train.seq_item_description.apply(lambda x: len(x)))\n                                   , np.max(test.seq_item_description.apply(lambda x: len(x)))])\nprint(\"max name seq \"+str(max_name_seq))\nprint(\"max item desc seq \"+str(max_seq_item_description))","e255fbd9":"train.seq_name.apply(lambda x: len(x)).hist()","8032d8f4":"train.seq_item_description.apply(lambda x: len(x)).hist()","b412447a":"#EMBEDDINGS MAX VALUE\n#Base on the histograms, we select the next lengths\nMAX_NAME_SEQ = 10\nMAX_ITEM_DESC_SEQ = 75\nMAX_TEXT = np.max([np.max(train.seq_name.max())\n                   , np.max(test.seq_name.max())\n                  , np.max(train.seq_item_description.max())\n                  , np.max(test.seq_item_description.max())])+2\nMAX_CATEGORY = np.max([train.category_name.max(), test.category_name.max()])+1\nMAX_BRAND = np.max([train.brand_name.max(), test.brand_name.max()])+1\nMAX_CONDITION = np.max([train.item_condition_id.max(), test.item_condition_id.max()])+1","f3f5ab23":"#SCALE target variable\ntrain[\"target\"] = np.log(train.price+1)\ntarget_scaler = MinMaxScaler(feature_range=(-1, 1))\ntrain[\"target\"] = target_scaler.fit_transform(train.target.values.reshape(-1,1))\npd.DataFrame(train.target).hist()","c2693c8e":"#EXTRACT DEVELOPTMENT TEST\ndtrain, dvalid = train_test_split(train, random_state=123, train_size=0.99)\nprint(dtrain.shape)\nprint(dvalid.shape)","4570c507":"#KERAS DATA DEFINITION\nfrom keras.preprocessing.sequence import pad_sequences\n\ndef get_keras_data(dataset):\n    X = {\n        'name': pad_sequences(dataset.seq_name, maxlen=MAX_NAME_SEQ)\n        ,'item_desc': pad_sequences(dataset.seq_item_description, maxlen=MAX_ITEM_DESC_SEQ)\n        ,'brand_name': np.array(dataset.brand_name)\n        ,'category_name': np.array(dataset.category_name)\n        ,'item_condition': np.array(dataset.item_condition_id)\n        ,'num_vars': np.array(dataset[[\"shipping\"]])\n    }\n    return X\n\nX_train = get_keras_data(dtrain)\nX_valid = get_keras_data(dvalid)\nX_test = get_keras_data(test)","58a6e8d1":"#KERAS MODEL DEFINITION\nfrom keras.layers import Input, Dropout, Dense, BatchNormalization, Activation, concatenate, GRU, Embedding, Flatten, BatchNormalization\nfrom keras.models import Model\nfrom keras.callbacks import ModelCheckpoint, Callback, EarlyStopping\nfrom keras import backend as K\n\ndef get_callbacks(filepath, patience=2):\n    es = EarlyStopping('val_loss', patience=patience, mode=\"min\")\n    msave = ModelCheckpoint(filepath, save_best_only=True)\n    return [es, msave]\n\ndef rmsle_cust(y_true, y_pred):\n    first_log = K.log(K.clip(y_pred, K.epsilon(), None) + 1.)\n    second_log = K.log(K.clip(y_true, K.epsilon(), None) + 1.)\n    return K.sqrt(K.mean(K.square(first_log - second_log), axis=-1))\n\ndef get_model():\n    #params\n    dr_r = 0.1\n    \n    #Inputs\n    name = Input(shape=[X_train[\"name\"].shape[1]], name=\"name\")\n    item_desc = Input(shape=[X_train[\"item_desc\"].shape[1]], name=\"item_desc\")\n    brand_name = Input(shape=[1], name=\"brand_name\")\n    category_name = Input(shape=[1], name=\"category_name\")\n    item_condition = Input(shape=[1], name=\"item_condition\")\n    num_vars = Input(shape=[X_train[\"num_vars\"].shape[1]], name=\"num_vars\")\n    \n    #Embeddings layers\n    emb_name = Embedding(MAX_TEXT, 50)(name)  #embedding comes from word2vector, I is only used in initial layer and it purpose is to recognize possible similarities in the mapped (here 50-dim, 10-dim and 5-dim space)\n    emb_item_desc = Embedding(MAX_TEXT, 50)(item_desc)\n    emb_brand_name = Embedding(MAX_BRAND, 10)(brand_name)\n    emb_category_name = Embedding(MAX_CATEGORY, 10)(category_name)\n    emb_item_condition = Embedding(MAX_CONDITION, 5)(item_condition)\n    \n    #rnn layer\n    \n    rnn_layer1 = GRU(16) (emb_item_desc)  #GRU, part of the recurennt NN, notice that we applied it only on the textual data that we transofrem into sequnce ,i.e. numerical\n    rnn_layer2 = GRU(8) (emb_name)  # 16,8 and stands for the dimensionality of the output space, i.e. what we are going to \"give to the next layer\"\n    \n    #main layer. Note its a keras concatenate, meaning it will merge layers of neural network\n    #Role of flatten in keras: Let us say that  \"emb_brand_name\" has elements of dimension 3x2 for example. To make it 1-d we use flatten\n    # Further layers may need 1-d vectors as input\n    \n    main_l = concatenate([\n        Flatten() (emb_brand_name)\n        , Flatten() (emb_category_name)\n        , Flatten() (emb_item_condition)\n        , rnn_layer1\n        , rnn_layer2\n        , num_vars\n    ])\n    main_l = Dropout(dr_r) (Dense(128) (main_l))\n    main_l = Dropout(dr_r) (Dense(64) (main_l))\n    \n    #output (1 stands for one output neuron and it should tell us the cost of the item)\n    output = Dense(1, activation=\"linear\") (main_l)\n    \n    #model\n    model = Model([name, item_desc, brand_name\n                   , category_name, item_condition, num_vars], output)\n    model.compile(loss=\"mse\", optimizer=\"adam\", metrics=[\"mae\", rmsle_cust])\n    \n    return model\n\n    \nmodel = get_model()\nmodel.summary()\n    \n","9781dcda":"#FITTING THE MODEL\nBATCH_SIZE = 20000\nepochs = 5\n\nmodel = get_model()\nmodel.fit(X_train, dtrain.target, epochs=epochs, batch_size=BATCH_SIZE\n          , validation_data=(X_valid, dvalid.target)\n          , verbose=1)","6d021697":"#EVLUEATE THE MODEL ON DEV TEST: What is it doing?\nval_preds = model.predict(X_valid)\nval_preds = target_scaler.inverse_transform(val_preds)\nval_preds = np.exp(val_preds)+1\n\n#mean_absolute_error, mean_squared_log_error\ny_true = np.array(dvalid.price.values)\ny_pred = val_preds[:,0]\nv_rmsle = rmsle(y_true, y_pred)\nprint(\" RMSLE error on dev test: \"+str(v_rmsle))","afc20d06":"#CREATE PREDICTIONS\npreds = model.predict(X_test, batch_size=BATCH_SIZE)\npreds = target_scaler.inverse_transform(preds)\npreds = np.exp(preds)-1\n\nsubmission = test[[\"test_id\"]]\nsubmission[\"price\"] = preds","265b037c":"submission.to_csv(\".\/NNsubmission_1.csv\", index=False)\nsubmission.price.hist()\n","6fd55e3d":"# Mercari Price Suggestion Challenge\n\n**NOTE** Architecture of this NN is modified to serve this particular data set which contains, categorical, numerical and text data. That makes it a perfect example when one wants to modify it to perform some other regression on data which contains the above mentioned variable types. Also a standard approach to handle the data was performed.\n\n\n\n\n\n\n\n\n## Neural Network Model using keras\n\n**This project was run on Kaggle competition** (https:\/\/www.kaggle.com\/c\/mercari-price-suggestion-challenge)\n\nIn this competition, Mercari(Japan\u2019s biggest community-powered shopping app) challenging you to build an algorithm that automatically suggests the right product prices. You\u2019ll be provided user-inputted text descriptions of their products, including details like product category name, brand name, and item condition.\n\n\nProduct pricing gets even harder at scale, considering just how many products are sold online. Clothing has strong seasonal pricing trends and is heavily influenced by brand names, while electronics have fluctuating prices based on product specs.\n\n**Acknowledgement**\n\nThis kernal influenced by excellent kernal from [noobhound](https:\/\/www.kaggle.com\/knowledgegrappler\/a-simple-nn-solution-with-keras-0-48611-pl).","e96ce962":"Why do we need sequence models i.e. rNN when we already have deep learning and CNN? \"Given a sentence, when looking at a word, sequence models try to derive relations from the previous words in the same sentence.\" Thats where performance of the rNN stands out. When we want to have a look at the context of the whole sentence, not just isolated words."}}