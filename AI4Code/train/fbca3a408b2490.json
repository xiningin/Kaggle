{"cell_type":{"f23362dc":"code","1831ebf8":"code","cc7960c4":"code","b268d1ff":"code","ff1bd088":"code","e6d8f97a":"code","aed4a8ab":"code","a7d79315":"code","b32dd863":"code","c8b96784":"code","3e12117d":"code","67ca4490":"code","ae9c4a8a":"code","57fc855c":"code","45c74237":"code","4abed789":"code","98923cab":"code","8db6a03b":"code","8a86b9d4":"code","474b121f":"code","3d9a709a":"code","902fe3cf":"code","e80ed988":"code","32a9e211":"code","6d5cbc68":"code","93d2c9cf":"code","ec11567d":"code","505d6316":"code","5b45c085":"code","c04983e0":"code","cba358e1":"markdown","e16dd241":"markdown","90cd757c":"markdown","1f13840a":"markdown","cb89f2c0":"markdown","d0ccf398":"markdown","3ac7170a":"markdown","93a7926f":"markdown","08339741":"markdown","1c3aabe5":"markdown","2cd29f5a":"markdown","a591b4ab":"markdown","44d2f291":"markdown","fcea625e":"markdown","9403ec36":"markdown","82d52d9d":"markdown","9e7a5a01":"markdown","6c0c0eef":"markdown"},"source":{"f23362dc":"import numpy as np # linear algebra\nimport pandas as pd # data processing\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","1831ebf8":"train_data = pd.read_csv(\"\/kaggle\/input\/titanic\/train.csv\")\ntrain_data.head()","cc7960c4":"test_data = pd.read_csv(\"\/kaggle\/input\/titanic\/test.csv\")\ntest_data.head()","b268d1ff":"train_data.describe(include='all')","ff1bd088":"train_data.isnull().sum()","e6d8f97a":"test_data.isnull().sum()","aed4a8ab":"import seaborn as sns\n\nsns.barplot(x=\"Sex\", y=\"Survived\", data=train_data)","a7d79315":"sns.barplot(x=\"SibSp\", y=\"Survived\", data=train_data)","b32dd863":"sns.barplot(x=\"Pclass\", y=\"Survived\", data=train_data)","c8b96784":"train_data[['Pclass', 'Survived']].groupby(['Pclass'], as_index=False).mean().sort_values(by='Survived', ascending=False)","3e12117d":"sns.barplot(x=\"Parch\", y=\"Survived\", data=train_data)","67ca4490":"import matplotlib.pyplot as plt\n%matplotlib inline\n\nage_graph= sns.FacetGrid(train_data, col='Survived')\nage_graph.map(plt.hist, 'Age', bins=20)","ae9c4a8a":"#sort the ages into logical categories\ntrain_data[\"Age\"] = train_data[\"Age\"].fillna(-0.5)\ntest_data[\"Age\"] = test_data[\"Age\"].fillna(-0.5)\nbins = [-1, 0, 5, 12, 18, 24, 35, 60, np.inf]\nlabels = ['Unknown', 'Baby', 'Child', 'Teenager', 'Student', 'Young Adult', 'Adult', 'Senior']\ntrain_data['AgeGroup'] = pd.cut(train_data[\"Age\"], bins, labels = labels)\ntest_data['AgeGroup'] = pd.cut(test_data[\"Age\"], bins, labels = labels)\n\n#draw a bar plot of Age vs. survival\nsns.barplot(x=\"AgeGroup\", y=\"Survived\", data=train_data)","57fc855c":"#Missing values for Embarked Column in training set\ntrain_data[train_data.Embarked.isnull()]","45c74237":"#Filling missing values in Embarked Column\ntrain_data = train_data.fillna({\"Embarked\": \"S\"})","4abed789":"train_data = train_data.drop(['Ticket', 'Cabin'], axis=1)\ntest_data = test_data.drop(['Ticket', 'Cabin'], axis=1)","98923cab":"#create a combined group of both datasets\ncombine = [train_data, test_data]\n\n#extract a title for each Name in the train and test datasets\nfor dataset in combine:\n    dataset['Title'] = dataset.Name.str.extract(' ([A-Za-z]+)\\.', expand=False)\n\npd.crosstab(train_data['Title'], train_data['Sex'])","8db6a03b":"#replace various titles with more common names\nfor dataset in combine:\n    dataset['Title'] = dataset['Title'].replace(['Lady', 'Capt', 'Col',\n    'Don', 'Dr', 'Major', 'Rev', 'Jonkheer', 'Dona'], 'Rare')\n    \n    dataset['Title'] = dataset['Title'].replace(['Countess', 'Lady', 'Sir'], 'Royal')\n    dataset['Title'] = dataset['Title'].replace('Mlle', 'Miss')\n    dataset['Title'] = dataset['Title'].replace('Ms', 'Miss')\n    dataset['Title'] = dataset['Title'].replace('Mme', 'Mrs')\n\ntrain_data[['Title', 'Survived']].groupby(['Title'], as_index=False).mean()","8a86b9d4":"title_mapping = {\"Mr\": 1, \"Miss\": 2, \"Mrs\": 3, \"Master\": 4, \"Rare\": 5} #Replacing title groups with numerical values\nfor dataset in combine:\n    dataset['Title'] = dataset['Title'].map(title_mapping)\n    dataset['Title'] = dataset['Title'].fillna(0)\n\ntrain_data.head()","474b121f":"# Filling missing ages with mode age group for each title\nmr_age = train_data[train_data[\"Title\"] == 1][\"AgeGroup\"].mode() #Young Adult\nmiss_age = train_data[train_data[\"Title\"] == 2][\"AgeGroup\"].mode() #Student\nmrs_age = train_data[train_data[\"Title\"] == 3][\"AgeGroup\"].mode() #Adult\nmaster_age = train_data[train_data[\"Title\"] == 4][\"AgeGroup\"].mode() #Baby\nroyal_age = train_data[train_data[\"Title\"] == 5][\"AgeGroup\"].mode() #Adult\nrare_age = train_data[train_data[\"Title\"] == 6][\"AgeGroup\"].mode() #Adult\n\nage_title_mapping = {1: \"Young Adult\", 2: \"Student\", 3: \"Adult\", 4: \"Baby\", 5: \"Adult\", 6: \"Adult\"}\nfor x in range(len(train_data[\"AgeGroup\"])):\n    if train_data[\"AgeGroup\"][x] == \"Unknown\":\n        train_data[\"AgeGroup\"][x] = age_title_mapping[train_data[\"Title\"][x]]\n        \nfor x in range(len(test_data[\"AgeGroup\"])):\n    if test_data[\"AgeGroup\"][x] == \"Unknown\":\n        test_data[\"AgeGroup\"][x] = age_title_mapping[test_data[\"Title\"][x]]","3d9a709a":"#Assign each age value to a numerical value\nage_mapping = {'Baby': 1, 'Child': 2, 'Teenager': 3, 'Student': 4, 'Young Adult': 5, 'Adult': 6, 'Senior': 7}\ntrain_data['AgeGroup'] = train_data['AgeGroup'].map(age_mapping)\ntest_data['AgeGroup'] = test_data['AgeGroup'].map(age_mapping)\n\ntrain_data.head()\n\ntrain_data = train_data.drop(['Age'], axis = 1)\ntest_data = test_data.drop(['Age'], axis = 1)","902fe3cf":"#Drop the name feature because the titles are already extracted.\ntrain_data = train_data.drop(['Name'], axis = 1)\ntest_data = test_data.drop(['Name'], axis = 1)","e80ed988":"#map each Sex value to a numerical value\nsex_mapping = {\"male\": 0, \"female\": 1}\ntrain_data['Sex'] = train_data['Sex'].map(sex_mapping)\ntest_data['Sex'] = test_data['Sex'].map(sex_mapping)\n\ntrain_data.head()","32a9e211":"embarked_mapping = {\"S\": 1, \"C\": 2, \"Q\": 3}\ntrain_data['Embarked'] = train_data['Embarked'].map(embarked_mapping)\ntest_data['Embarked'] = test_data['Embarked'].map(embarked_mapping)\n\ntrain_data.head()","6d5cbc68":"#drop Fare values\ntrain_data = train_data.drop(['Fare'], axis = 1)\ntest_data= test_data.drop(['Fare'], axis = 1)","93d2c9cf":"train_data.head()","ec11567d":"test_data.head()","505d6316":"#Splitting train dataset\n\nfrom sklearn.model_selection import train_test_split\n\nX = train_data.drop(['Survived', 'PassengerId'], axis=1)\nY = train_data[\"Survived\"]\nx_train, x_val, y_train, y_val = train_test_split(X,Y, test_size = 0.22, random_state = 0)","5b45c085":"# Gaussian Naive Bayes\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.metrics import accuracy_score\n\nG = GaussianNB()\nG.fit(x_train, y_train)\ny_pred = G.predict(x_val)\nAccuracy_for_gaussian = round(accuracy_score(y_pred, y_val) * 100, 2)\nprint(\"Accuracy_for_gaussian: \",Accuracy_for_gaussian)\n\n# Logistic Regression\nfrom sklearn.linear_model import LogisticRegression\n\nL = LogisticRegression()\nL.fit(x_train, y_train)\ny_pred = L.predict(x_val)\nAccuracy_for_logistic_regression = round(accuracy_score(y_pred, y_val) * 100, 2)\nprint(\"Accuracy_for_logistic_regression: \",Accuracy_for_logistic_regression)\n\n# Random Forest\nfrom sklearn.ensemble import RandomForestClassifier\n\nR = RandomForestClassifier()\nR.fit(x_train, y_train)\ny_pred = R.predict(x_val)\nAccuracy_for_Random_forest = round(accuracy_score(y_pred, y_val) * 100, 2)\nprint(\"Accuracy_for_Random_forest: \",Accuracy_for_Random_forest)\n\n#Linear Support Vector Classification\nfrom sklearn.svm import LinearSVC\nSVC = LinearSVC()\nSVC.fit(x_train, y_train)\ny_pred = SVC.predict(x_val)\nAccuracy_for_LinearSVC = round(accuracy_score(y_pred, y_val) * 100, 2)\nprint(\"Accuracy_for_LinearSVC: \",Accuracy_for_LinearSVC)\n\n#Support Vector Machine\nfrom sklearn import svm\nclf = svm.SVC()\nclf.fit(x_train, y_train)\ny_pred = clf.predict(x_val)\nAccuracy_for_SVM = round(accuracy_score(y_pred, y_val) * 100, 2)\nprint(\"Accuracy_for_SVM: \",Accuracy_for_SVM)\n\n#Multinomial Naves Bayes\nfrom sklearn.naive_bayes import MultinomialNB\nMNB = MultinomialNB()\nMNB.fit(x_train, y_train)\ny_pred = MNB.predict(x_val)\nAccuracy_for_MultinomialNB = round(accuracy_score(y_pred, y_val) * 100, 2)\nprint(\"Accuracy_for_MultinomialNB: \",Accuracy_for_MultinomialNB)\n\n#Decision Tree Classifier\nfrom sklearn.tree import DecisionTreeClassifier\nDTC = DecisionTreeClassifier()\nDTC.fit(x_train, y_train)\ny_pred = DTC.predict(x_val)\nAccuracy_for_DecisionTreeClassifier = round(accuracy_score(y_pred, y_val) * 100, 2)\nprint(\"Accuracy_for_DecisionTreeClassifier: \",Accuracy_for_DecisionTreeClassifier)\n\n#Gradient Boosting Classifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nGBC = GradientBoostingClassifier()\nGBC.fit(x_train, y_train)\ny_pred = GBC.predict(x_val)\nAccuracy_for_GradientBoostingClassifier = round(accuracy_score(y_pred, y_val) * 100, 2)\nprint(\"Accuracy_for_GradientBoostingClassifier: \",Accuracy_for_GradientBoostingClassifier)\n\n#Stochastic Gradient Boosting Classifier\nfrom sklearn.linear_model import SGDClassifier\nSGD = SGDClassifier(loss=\"log\", penalty=\"l2\", max_iter=5)\nSGD.fit(x_train, y_train)\ny_pred = SGD.predict(x_val)\nAccuracy_for_SGDClassifier = round(accuracy_score(y_pred, y_val) * 100, 2)\nprint(\"Accuracy_for_SGDClassifier: \",Accuracy_for_SGDClassifier)\n\n#K-Neighbor Classifier\nfrom sklearn.neighbors import KNeighborsClassifier\nKN = KNeighborsClassifier()\nKN.fit(x_train, y_train)\ny_pred = KN.predict(x_val)\nAccuracy_for_KNeighborsClassifier = round(accuracy_score(y_pred, y_val) * 100, 2)\nprint(\"Accuracy_for_KNeighborsClassifier: \",Accuracy_for_KNeighborsClassifier)","c04983e0":"ID = test_data['PassengerId']\nPredictions = GBC.predict(test_data.drop('PassengerId', axis=1))\n\noutput = pd.DataFrame({ 'PassengerId' : ID, 'Survived': Predictions })\noutput.to_csv('submission.csv', index=False)","cba358e1":"# Building Machine Learning Model","e16dd241":"# Sex Feature","90cd757c":"# **If you find my notebook helpful then kindly upvote this. Thank You.**","1f13840a":"> ****Missing Values for train and test set****","cb89f2c0":"# Handling Age Feature","d0ccf398":"For age we need to divide them in categories otherwise it will be difficult to understand like the above.","3ac7170a":"parch: The dataset defines family relations in this way...\nParent = mother, father\nChild = daughter, son, stepdaughter, stepson\nSome children travelled only with a nanny, therefore parch=0 for them.","93a7926f":"pclass: A proxy for socio-economic status (SES)\n[1st = Upper ,\n2nd = Middle ,\n3rd = Lower]\n\nSo the people belonging to higher socio-economic status have higher survival chance than others.","08339741":"# Title feature\n*Name feature can be engineered to extract titles and test correlation between titles and survival, before dropping Name and PassengerId features.*\n\nIn the following code we extract Title feature using regular expressions. The RegEx pattern (\\w+\\.) matches the first word which ends with a dot character within Name feature. The expand=False flag returns a DataFrame.","1c3aabe5":"# **Dropping unnecessary columns from the dataset**","2cd29f5a":"Embarked Mapping is required to convert string to numeric values of \"Embarked\" column","a591b4ab":"# **Handling \"Embarked\" Feature**","44d2f291":"As per the graph above it is clearly shown that the survival rate of females are higher than that of males.","fcea625e":"# Different ML Models","9403ec36":"# Fare Feature","82d52d9d":"From the above graph we can understand that the survival rate of those people who have siblings and who doesn't have.","9e7a5a01":"Submission File","6c0c0eef":"# Visual Representation of Features with respect to Survival"}}