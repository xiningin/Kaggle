{"cell_type":{"84f9255e":"code","262bd497":"code","c10457aa":"code","d70f59c3":"code","d5540d20":"code","47030238":"code","6c4f9ca4":"code","b87b6e43":"code","dd62d0ce":"code","f75161c9":"code","49b2aca3":"code","26fb020f":"code","6c58497f":"code","888e3237":"code","01feb7f1":"code","37e1a6da":"code","0cde48b3":"code","90a7cd85":"code","bb1ce7a0":"code","6e602783":"code","5942aae4":"code","16f6361e":"code","43339eb1":"code","fd9c3b50":"code","ab08d801":"code","e410a0ed":"code","e70fdab0":"code","d2a6479b":"code","7291e1f1":"code","0ac0abaa":"code","a34d4e6a":"code","ec9ee1f3":"code","86265035":"code","5cfcad60":"code","c08766d0":"code","dff4dac0":"code","45e21564":"code","21e64646":"code","67c3c847":"code","4d60e8f0":"code","918cb974":"code","401403c7":"code","1b3b44f7":"code","128547e5":"code","8ba1df00":"code","cc3cff88":"markdown","d85754fe":"markdown","949a5b39":"markdown","8d647b04":"markdown","01ea8948":"markdown","98fee377":"markdown","a290a851":"markdown","57f062de":"markdown","3fd28dc3":"markdown","f1c96b85":"markdown","3e30e023":"markdown","10eda55f":"markdown","19562512":"markdown","745dc183":"markdown","ccd2bba4":"markdown","d1654ec7":"markdown","0b7a0b4d":"markdown","9a051d03":"markdown","e0b5f14d":"markdown","13b4e209":"markdown","6708cf2d":"markdown","fc228af4":"markdown","a8599a90":"markdown","b5bbca95":"markdown"},"source":{"84f9255e":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","262bd497":"#!pip install tensorflow\n#!pip install tensorflow_hub\n#!pip3 install tensorflow_text","c10457aa":"from bs4 import BeautifulSoup\nfrom collections import Counter,defaultdict\nimport gc\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as mpatches\nimport matplotlib\nimport nltk\nfrom nltk import word_tokenize\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import TweetTokenizer\nfrom nltk.stem import WordNetLemmatizer,PorterStemmer\nfrom nltk.corpus import stopwords\nstop_words = stopwords.words('english')\nimport plotly.graph_objs as go\nimport plotly.offline as py\nfrom plotly import tools\npy.init_notebook_mode(connected=True)\nimport re\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.layers import LSTM, Dense,Flatten,Conv2D,Conv1D,GlobalMaxPooling1D,GlobalMaxPool1D\nfrom keras.optimizers import Adam\nimport numpy as np  \nimport pandas as pd \nimport keras.backend as k\nfrom keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.layers import Input, LSTM, Embedding, Dense, Concatenate, TimeDistributed, Bidirectional,GRU\nfrom tensorflow.keras.models import Model,Sequential\nfrom tensorflow.keras.callbacks import EarlyStopping\nimport tensorflow_hub as hub\nimport tensorflow as tf\nfrom sklearn.preprocessing import OneHotEncoder\nfrom keras.utils import to_categorical\nfrom keras.utils.vis_utils import plot_model\nimport string\nimport math\nfrom scipy.spatial.distance import cosine\nimport transformers\nfrom transformers import BertTokenizer,TFBertModel,AutoTokenizer, pipeline\nfrom unidecode import unidecode\nfrom wordcloud import WordCloud, STOPWORDS\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n\n%matplotlib inline\n%matplotlib inline","d70f59c3":"train_df= pd.read_json(\"\/kaggle\/input\/github-bugs-prediction\/embold_train.json\").reset_index(drop=True)\ntrain_df.head()","d5540d20":"test_df= pd.read_json(\"\/kaggle\/input\/github-bugs-prediction\/embold_test.json\").reset_index(drop=True)\ntest_df.head()","47030238":"train_ex_df= pd.read_json(\"\/kaggle\/input\/github-bugs-prediction\/embold_train_extra.json\").reset_index(drop=True)\ntrain_ex_df.head()","6c4f9ca4":"mas_train_df = pd.concat([train_df,train_ex_df],ignore_index=True)\nmas_train_df.head()","b87b6e43":"mas_train_df.info()","dd62d0ce":"mas_train_df.drop_duplicates(keep='first').count()","f75161c9":"def fx(x):\n    return x['title'] + \" \" + x['body']   \nmas_train_df['text']= mas_train_df.apply(lambda x : fx(x),axis=1)\ntest_df['text']= test_df.apply(lambda x : fx(x),axis=1)","49b2aca3":"mas_train_df.head()","26fb020f":"cList = {\n            \"i'm\": \"i am\",\n            \"you're\": \"you are\",\n            \"it's\": \"it is\",\n            \"we're\": \"we are\",\n            \"we'll\": \"we will\",\n            \"That's\":\"that is\",\n            \"haven't\":\"have not\",\n            \"let's\":\"let us\",\n            \"ain't\": \"am not \/ are not \/ is not \/ has not \/ have not\",\n            \"aren't\": \"are not \/ am not\",\n            \"can't\": \"cannot\",\n            \"can't've\": \"cannot have\",\n            \"'cause\": \"because\",\n            \"could've\": \"could have\",\n            \"couldn't\": \"could not\",\n            \"couldn't've\": \"could not have\",\n            \"didn't\": \"did not\",\n            \"doesn't\": \"does not\",\n            \"don't\": \"do not\",\n            \"hadn't\": \"had not\",\n            \"hadn't've\": \"had not have\",\n            \"hasn't\": \"has not\",\n            \"haven't\": \"have not\",\n            \"he'd\": \"he had \/ he would\",\n            \"he'd've\": \"he would have\",\n            \"he'll\": \"he shall \/ he will\",\n            \"he'll've\": \"he shall have \/ he will have\",\n            \"he's\": \"he has \/ he is\",\n            \"how'd\": \"how did\",\n            \"how'd'y\": \"how do you\",\n            \"how'll\": \"how will\",\n            \"how's\": \"how has \/ how is \/ how does\",\n            \"I'd\": \"I had \/ I would\",\n            \"I'd've\": \"I would have\",\n            \"I'll\": \"I shall \/ I will\",\n            \"I'll've\": \"I shall have \/ I will have\",\n            \"I'm\": \"I am\",\n            \"I've\": \"I have\",\n            \"isn't\": \"is not\",\n            \"it'd\": \"it had \/ it would\",\n            \"it'd've\": \"it would have\",\n            \"it'll\": \"it shall \/ it will\",\n            \"it'll've\": \"it shall have \/ it will have\",\n            \"it's\": \"it has \/ it is\",\n            \"let's\": \"let us\",\n            \"ma'am\": \"madam\",\n            \"mayn't\": \"may not\",\n            \"might've\": \"might have\",\n            \"mightn't\": \"might not\",\n            \"mightn't've\": \"might not have\",\n            \"must've\": \"must have\",\n            \"mustn't\": \"must not\",\n            \"mustn't've\": \"must not have\",\n            \"needn't\": \"need not\",\n            \"needn't've\": \"need not have\",\n            \"o'clock\": \"of the clock\",\n            \"oughtn't\": \"ought not\",\n            \"oughtn't've\": \"ought not have\",\n            \"shan't\": \"shall not\",\n            \"sha'n't\": \"shall not\",\n            \"shan't've\": \"shall not have\",\n            \"she'd\": \"she had \/ she would\",\n            \"she'd've\": \"she would have\",\n            \"she'll\": \"she shall \/ she will\",\n            \"she'll've\": \"she shall have \/ she will have\",\n            \"she's\": \"she has \/ she is\",\n            \"should've\": \"should have\",\n            \"shouldn't\": \"should not\",\n            \"shouldn't've\": \"should not have\",\n            \"so've\": \"so have\",\n            \"so's\": \"so as \/ so is\",\n            \"that'd\": \"that would \/ that had\",\n            \"that'd've\": \"that would have\",\n            \"that's\": \"that has \/ that is\",\n            \"there'd\": \"there had \/ there would\",\n            \"there'd've\": \"there would have\",\n            \"there's\": \"there has \/ there is\",\n            \"they'd\": \"they had \/ they would\",\n            \"they'd've\": \"they would have\",\n            \"they'll\": \"they shall \/ they will\",\n            \"they'll've\": \"they shall have \/ they will have\",\n            \"they're\": \"they are\",\n            \"they've\": \"they have\",\n            \"to've\": \"to have\",\n            \"wasn't\": \"was not\",\n            \"we'd\": \"we had \/ we would\",\n            \"we'd've\": \"we would have\",\n            \"we'll\": \"we will\",\n            \"we'll've\": \"we will have\",\n            \"we're\": \"we are\",\n            \"we've\": \"we have\",\n            \"weren't\": \"were not\",\n            \"what'll\": \"what shall \/ what will\",\n            \"what'll've\": \"what shall have \/ what will have\",\n            \"what're\": \"what are\",\n            \"what's\": \"what has \/ what is\",\n            \"what've\": \"what have\",\n            \"when's\": \"when has \/ when is\",\n            \"when've\": \"when have\",\n            \"where'd\": \"where did\",\n            \"where's\": \"where has \/ where is\",\n            \"where've\": \"where have\",\n            \"who'll\": \"who shall \/ who will\",\n            \"who'll've\": \"who shall have \/ who will have\",\n            \"who's\": \"who has \/ who is\",\n            \"who've\": \"who have\",\n            \"why's\": \"why has \/ why is\",\n            \"why've\": \"why have\",\n            \"will've\": \"will have\",\n            \"won't\": \"will not\",\n            \"won't've\": \"will not have\",\n            \"would've\": \"would have\",\n            \"wouldn't\": \"would not\",\n            \"wouldn't've\": \"would not have\",\n            \"y'all\": \"you all\",\n            \"y'all'd\": \"you all would\",\n            \"y'all'd've\": \"you all would have\",\n            \"y'all're\": \"you all are\",\n            \"y'all've\": \"you all have\",\n            \"you'd\": \"you had \/ you would\",\n            \"you'd've\": \"you would have\",\n            \"you'll\": \"you shall \/ you will\",\n            \"you'll've\": \"you shall have \/ you will have\",\n            \"you're\": \"you are\",\n            \"you've\": \"you have\"\n           }","6c58497f":"c_re = re.compile('(%s)' % '|'.join(cList.keys()))","888e3237":"def expandContractions(text, c_re=c_re):\n    def replace(match):\n        return cList[match.group(0)]\n    return c_re.sub(replace, text)","01feb7f1":"def remove_emoji(string):\n        emoji_pattern = re.compile(\"[\"\n                               u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                               u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                               u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                               u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                               u\"\\U00002702-\\U000027B0\"\n                               u\"\\U000024C2-\\U0001F251\"\n                               \"]+\", flags=re.UNICODE)\n        return emoji_pattern.sub(r'', string) ","37e1a6da":"def remove_punctuations(data):\n    punct_tag=re.compile(r'[^\\w\\s]')\n    data=punct_tag.sub(r'',data)\n    return data","0cde48b3":"def removeSpecialChars(data):\n    '''\n    Removes special characters which are specifically found in tweets.\n    '''\n    #Converts HTML tags to the characters they represent\n    soup = BeautifulSoup(data, \"html.parser\")\n    data = soup.get_text()\n\n    #Convert www.* or https?:\/\/* to empty strings\n    data = re.sub('((www\\.[^\\s]+)|(https?:\/\/[^\\s]+))','',data)\n\n    #Convert @username to empty strings\n    data = re.sub('@[^\\s]+','',data)\n    \n    #remove org.apache. like texts\n    data =  re.sub('(\\w+\\.){2,}','',data)\n\n    #Remove additional white spaces\n    data = re.sub('[\\s]+', ' ', data)\n    \n    data = re.sub('\\.(?!$)', '', data)\n\n    #Replace #word with word\n    data = re.sub(r'#([^\\s]+)', r'\\1', data)\n\n    return data ","90a7cd85":"def remove_nonenglish_charac(string):\n    return re.sub('\\W+','', string )","bb1ce7a0":"extra_punctuations = ['','.', '``', '...', '\\'s', '--', '-', 'n\\'t', '_', '\u2013','&']\nstopword_list = stopwords.words('english') + list(string.punctuation)+ extra_punctuations + ['u','the','us','say','that','he','me','she','get','rt','it','mt','via','not','and','let','so','say','dont','use','you']","6e602783":"def clean_text(data):\n    wordnet_lemmatizer = WordNetLemmatizer()\n    stemmer = PorterStemmer() \n    tokenizer=TweetTokenizer()\n    data = unidecode(data)\n    data = expandContractions(data)\n    tokens = tokenizer.tokenize(data)\n    data = ' '.join([tok for tok in tokens if len(tok) > 2 if tok not in stopword_list and not tok.isdigit()])\n    data = re.sub('\\b\\w{,2}\\b', '', data)\n    data = re.sub(' +', ' ', data)\n    data = removeSpecialChars(data)\n    data = remove_emoji(data)\n    data= [stemmer.stem(w) for w in data.split()]\n    return ' '.join([wordnet_lemmatizer.lemmatize(word) for word in data])","5942aae4":"mas_train_df['text'].head()","16f6361e":"mas_train_df['text']=mas_train_df['text'].apply(lambda x: clean_text(x))","43339eb1":"mas_train_df['text'].head()","fd9c3b50":"# Convert the textual reviews to list for analysing sentences(sentence vectors)\ntext_lst = mas_train_df['text'].tolist()","ab08d801":"elmo = hub.load(\"https:\/\/tfhub.dev\/google\/elmo\/2\")","e410a0ed":"def create_elmo_embeddings(data):\n    embed=elmo(data,signature=\"default\",as_dict=True)[\"elmo\"]\n    print(embed.shape)\n    with tf.Session() as sess:\n        sess.run(tf.global_variables_initializer())\n        sess.run(tf.tables_initializer())\n        out_x=sess.run(embed)\n        # return average of ELMo features\n        return out_x","e70fdab0":"#elmo_input = text_lst[:2]\n#elmo_output = create_elmo_embeddings(elmo_input)","d2a6479b":"bert_tokenizer =  BertTokenizer.from_pretrained('bert-large-uncased', do_lower_case=True)\nbert_model = TFBertModel.from_pretrained('bert-large-uncased')","7291e1f1":"def bert_encode(data,max_len):\n    input_ids = []\n    attention_masks = []\n    for i in range(len(data)):\n        encoded = bert_tokenizer.encode_plus(\n        \n          data[i],\n          add_special_tokens = True,\n          max_length = max_len,\n          pad_to_max_length = True,        \n          return_attention_mask = True,        \n        )\n        input_ids.append(encoded['input_ids'])\n        attention_masks.append(encoded['attention_mask'])\n    return np.array(input_ids),np.array(attention_masks)\n        ","0ac0abaa":"train_input_ids,train_attention_masks = bert_encode(mas_train_df['text'][:5],1000)","a34d4e6a":"input_ids = tf.keras.layers.Input(shape=(128,), name='input_token', dtype='int32')\ninput_masks_ids = tf.keras.layers.Input(shape=(128,), name='masked_token', dtype='int32')\nbert_output=bert_model([input_ids,input_masks_ids])[0]\nbert_output.shape\nbert_output[:,0,:]\nmodel=Model(inputs=[input_ids,input_masks_ids],outputs=[bert_output])\nmodel.summary()","ec9ee1f3":"def Get_Transformer_Embedding(transformer_model,tokenizer,name,inp):\n    tokenizr = tokenizer.from_pretrained(name)\n    model = transformer_model.from_pretrained(name)\n    input_ids = tf.constant(tokenizr.encode(inp))[None,:]\n    outputs = model(input_ids)\n    lst_hidd_state = outputs[0]\n    return lst_hidd_state[0]    ","86265035":"cls_token = Get_Transformer_Embedding(TFBertModel,BertTokenizer,'bert-base-uncased',text_lst[0])\nprint(cls_token)\nprint(cls_token.shape)\nplt.plot(cls_token[0])\nplt.plot(cls_token[1])\nplt.show()","5cfcad60":"def build_transformer_embedding(name,inp,model_name,IsGpt = False):\n\n    model = model_name.from_pretrained(name)\n    tokenizer = AutoTokenizer.from_pretrained(name)\n    if IsGpt:\n        tokenizer.pad_token = \"[PAD]\"        \n    pipe = pipeline('feature-extraction', model=model,tokenizer=tokenizer)\n    features = pipe(inp)\n    features = np.squeeze(features)\n    return features","c08766d0":"def Generate_Embedding(name,model_name,IsGpt = False):\n    embedding_features1 = build_transformer_embedding(name,text_lst[0],model_name,IsGpt)\n    embedding_features2 = build_transformer_embedding(name,text_lst[1],model_name,IsGpt)\n    distance= 1- cosine(embedding_features1[0],embedding_features2[0])\n    print(distance)\n    plt.plot(embedding_features1[0])\n    plt.plot(embedding_features2[0])","dff4dac0":"from transformers import TFDistilBertModel\nGenerate_Embedding('distilbert-base-uncased',TFDistilBertModel)","45e21564":"from transformers import TFRobertaModel\nGenerate_Embedding('roberta-base',TFRobertaModel)","21e64646":"from transformers import TFXLNetModel\nGenerate_Embedding('xlnet-base-cased',TFXLNetModel)","67c3c847":"from transformers import BartModel\nGenerate_Embedding('facebook\/bart-base',BartModel)","4d60e8f0":"from transformers import TFAlbertModel\nGenerate_Embedding('albert-base-v1',TFAlbertModel)","918cb974":"from transformers import FlaubertModel\nGenerate_Embedding('flaubert\/flaubert_base_cased',FlaubertModel)","401403c7":"from transformers import TFOpenAIGPTModel\nGenerate_Embedding('openai-gpt',TFOpenAIGPTModel,True)","1b3b44f7":"from transformers import TFGPT2Model\nGenerate_Embedding('openai-gpt',TFGPT2Model,True)","128547e5":"from transformers import TFElectraModel\nGenerate_Embedding('google\/electra-small-discriminator',TFElectraModel)","8ba1df00":"from transformers import TFLongformerModel\nGenerate_Embedding('allenai\/longformer-base-4096',TFLongformerModel)","cc3cff88":"# DistilBERT\n- It's a distilled version of pretraining BERT to produce a lightweight version of it. It is analogous to teacher supervision of a neural network learning to optimize tis weights. DistilBERT Paper provides an insight why it is 40% smaller but preserves 95% of BERT's weights for transfer learning.","d85754fe":"# Embeddings from Language Models (ELMo)\n- ELMo is a novel way to represent words in vectors or embeddings. These word embeddings are helpful in achieving state-of-the-art (SOTA) results in several NLP tasks <br>\n- ELMo word vectors are computed on top of a two-layer bidirectional language model (biLM). This biLM model has two layers stacked together. Each layer has 2 passes \u2014 forward pass and backward pass\n- The architecture above uses a character-level convolutional neural network (CNN) to represent words of a text string into raw word vectors\n- These raw word vectors act as inputs to the first layer of biLM\n- The forward pass contains information about a certain word and the context (other words) before that word\n- The backward pass contains information about the word and the context after it\n- This pair of information, from the forward and backward pass, forms the intermediate word vectors\n  These intermediate word vectors are fed into the next layer of biLM\n- The final representation (ELMo) is the weighted sum of the raw word vectors and the 2 intermediate word vectors\n- As the input to the biLM is computed from characters rather than words, it captures the inner structure of the word\n- ELMo word representations take the entire input sentence into equation for calculating the word embeddings","949a5b39":"- TensorFlow Hub is a library that enables transfer learning by allowing the use of many machine learning models for different tasks. ELMo is one such example.","8d647b04":"# Dynamic Embeddings\n-  Traditional methods such as Skip-Gram and Continuous Bag-of-Words learn static embeddings by training lookup tables that translate words into dense vectors. Static embeddings are directly useful for solving lexical semantics tasks, and can be used as input representations for downstream problems.\n- The contextualized embeddings such as BERT have been shown more effective than static embeddings as NLP input embeddings. Such embeddings are dynamic, calculated according to a sentential context using a network structure.\n- Using dynamic embeddings to represent a target word gives us two salient advantages. First, it is useful to resolve ambiguities for polysemous words\n- Second, syntax and semantic information over the entire sentence is integrated into the context word representations, which makes them more informative.\n- Below Dynamic Word Embeddings have been shown useful for NLP tasks\n- ELMo provides deep word representations generated from word-level language modeling using LSTM.\n- GPT improves language understanding by generative pre-training based on Transformer.\n- BERT investigates a self-attention-network for pre-training deep bidirectional representations. Yang et al. \n- xlnet proposed XLNet, a generalized autoregressive pretraining model that integrating the Transformer-XL\n- Let's us try to build each dynamic embeding one by one","01ea8948":"## EDA(Explodatory Data Analysis)","98fee377":"## Text Cleaning\nlet us clean the dataset and remove the redundancies.This includes\n\n1. HTML codes\n1. URLs\n1. Emojis\n1. Stopwords\n1. Punctuations\n1. Expanding Abbreviations","a290a851":"# Embedding using pipeline\n- This is a robust and efficient way to generate sentence vectors and compute corrspoinding distances between those vectors. It is a faster way which applies to almost all transformers","57f062de":"Looks like both the train and train extra dataframe contains the same content so let's combine them using concatenation command of pandas on index axis","3fd28dc3":"Let's check the length of each dataframe","f1c96b85":"# BART Model\n- It is alternate SOTA model to denoise sentence2 sentence pretraining for natural language generation,comprehension etc. The most important points can be summarized as:\n\n- Bart uses a standard seq2seq\/machine translation architecture with a bidirectional encoder (like BERT) and a left-to-right decoder (like GPT).\n\n- The pretraining task involves randomly shuffling the order of the original sentences and a novel in-filling scheme, where spans of text are replaced with a single mask token.\n\n- BART is particularly effective when fine tuned for text generation but also works well for comprehension tasks. It matches the performance of RoBERTa with comparable training resources on GLUE and SQuAD, achieves new state-of-the-art results on a range of abstractive dialogue, question answering, and summarization tasks, with gains of up to 6 ROUGE.\n\n- The architecture contains these encoder -decoder modules :\n![image.png](attachment:image.png)","3e30e023":"* The size of the simple NN model built with BERT as intermediate Embedding Layer can be observed. Bert-large-uncased has 24-layer, 1024-hidden, 16-heads, 336M parameters and trained on lower-cased English text.","10eda55f":"# GPT-2\n- It is a robust model. GPT-2 is a large transformer-based language model with 1.5 billion parameters, trained on a dataset[1] of 8 million web pages. GPT-2 is trained with a simple objective: predict the next word, given all of the previous words within some text. The diversity of the dataset causes this simple goal to contain naturally occurring demonstrations of many tasks across diverse domains. GPT-2 is a direct scale-up of GPT, with more than 10X the parameters and trained on more than 10X the amount of data.Some important aspects:\n\n- GPT-2 is a model with absolute position embeddings so it\u2019s usually advised to pad the inputs on the right rather than the left.\n\n- GPT-2 was trained with a causal language modeling (CLM) objective and is therefore powerful at predicting the next token in a sequence. Leveraging this feature allows GPT-2 to generate syntactically coherent text as it can be observed in the run_generation.py example scrip\n- It is important to note the effect of Attention and masking in GPT-2 model. These are represented in the diagram:","19562512":"# Roberta Model Embedding\n- It is a robust and large model built by Facebook Research, to alleviate undertrained nature of BERT. It trains in much larger mini-batch sizes. This provides a good model of how to train Roberta on Google cloud.The original paper can be found here, and the model architecture is provided.","745dc183":"# BERT Embedding (Bidirectional Encoder Representations from Transformer)\n\n- The importance of using BERT is that it has 2 important aspects:\n\n* Msked Language Model (MLM)\n* Next Sentence Prediction(NSP)\n- They employed masked language modeling. In other words, they hid 15% of the words and used their position information to infer them. Finally, they also mixed it up a little bit to make the learning process more effective.\n- In fact, BERT developers created two main models:\n* The BASE: Number of transformer blocks (L): 12, Hidden layer size (H): 768 and Attention heads(A): 12\n* The LARGE: Number of transformer blocks (L): 24, Hidden layer size (H): 1024 and Attention heads(A): 16\nto make things clearer it is important to understand the special tokens that BERT authors used for fine-tuning and specific task training. These are the following:\n1. [CLS] : The first token of every sequence. A classification token which is normally used in conjunction with a softmax layer for classification tasks. For anything else, it can be safely ignored\n1. [SEP] : A sequence delimiter token which was used at pre-training for sequence-pair tasks (i.e. Next sentence prediction). Must be used when sequence pair tasks are required. When a single sequence is used it is just appended at the end.\n1. [MASK] : Token used for masked words. Only used for pre-training","ccd2bba4":"## XLNet Embeddings\n- It provides an important outline of the modifications made on top of BERT for producing XLNet. It applies an autoregressive language model and has the 2 most important points:\n\n- Enables learning bidirectional contexts by maximizing the expected likelihood over all permutations of the factorization order\n- Overcomes the limitations of BERT thanks to its autoregressive formulation.\n- It is a permutation language model and a pictorial representation can be :","d1654ec7":"# Albert Model\n- It is a lighter version of BERT which splits the embedding matrix into 2 smaller matrices and uses repeated splitting in between the transformer layers.Some important points:\n\n- ALBERT is a model with absolute position embeddings so it\u2019s usually advised to pad the inputs on the right rather than the left.\n\n- ALBERT uses repeating layers which results in a small memory footprint, however the computational cost remains similar to a BERT-like architecture with the same number of hidden layers as it has to iterate through the same number of (repeating) layers.","0b7a0b4d":"Attribute Description:\n1. Title - the title of the GitHub bug <br> \n1. Body - the body of the GitHub bug <br> \n1. Label - Represents various classes of Labels <br> \n - Bug - 0 <br> \n - Feature - 1 <br> \n - Question - 2 <br> ","9a051d03":"# BERT Embeddings","e0b5f14d":"# Using the Transfomer Method\nIn this case , we will be using the HuggingFace Transformer method for extracting sentence embeddings. This is a rather simpler method as we only need to extract the last layer from from the model output. The model in this case is bert-base-uncased (12-layer, 768-hidden, 12-heads, 110M parameters.Trained on lower-cased English text.).This code segment is model agnostic and can be used for any variats of BERT (except T5, GPT variants)","13b4e209":"Let's drop duplicate if exists no duplicates exists <br>\nNo duplicates present in training dataset after combining","6708cf2d":"# Transformers\n- We come to Transformer Embeddings for which the most important aspect is the Transformer architecture. Since we will be diving in depth into architectures in the Machine LEarning Training session (model building), it is safe to have a glimpse of a traditional Transformer Architecture.\n![image.png](attachment:image.png)\n- We will be working with the HuggingFace repository as it contains all SOTA Transformer models. In this context\n- While BERT, the first Transformer, relies on 2 tokens ([CLS] and [SEP]) ,extracting the sentence embedding vectors are done after extracting the last output layer. However , different models have different number of layers, and in this case, we will exploring a model agnostic way to extract sentence embeddings and performing similarity check with all of the models.","fc228af4":"# GPT (Generative Pretraining)\n- This is a different model from BERT and its variants built primarily for NLG (generative modelling). GPT has the following important points:\n\n- GPT is a model with absolute position embeddings so it\u2019s usually advised to pad the inputs on the right rather than the left.\n\n- GPT was trained with a causal language modeling (CLM) objective and is therefore powerful at predicting the next token in a sequence. Leveraging this feature allows GPT-2 to generate syntactically coherent text as it can be observed in the run_generation.py example script\n![image.png](attachment:image.png)","a8599a90":"## Importing required library for notebook","b5bbca95":"# sophisticated variants of BERT"}}