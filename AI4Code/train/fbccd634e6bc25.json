{"cell_type":{"f363e7a0":"code","379e4d2e":"code","c183f174":"code","04d992a4":"code","4593affd":"code","02c2ceee":"code","f92f3b35":"code","dce11f12":"code","f1ae88c8":"code","a70caaf6":"code","3cedf2e7":"code","018f430e":"code","68041a89":"code","8d0150a2":"code","dc82b16c":"code","39e32cd3":"code","ff15a5aa":"code","01fa780c":"code","81e46f57":"code","58c12a26":"code","c962ff62":"code","70607102":"code","744ac2bf":"code","f23bd128":"code","55ceddb1":"code","8b58aac6":"code","ce2d2171":"code","165d48e6":"code","d0687fa2":"code","c335a28c":"code","660bbbac":"code","7510f304":"code","83bf7b92":"code","6c7bad41":"code","db013f4c":"code","00e8cc81":"code","af5249a3":"code","3c1b2055":"markdown","c5f16a9f":"markdown","66f1b770":"markdown","6f886082":"markdown","9c91561a":"markdown","ea59885f":"markdown","4ba6cad6":"markdown","e40aafb7":"markdown","245637d5":"markdown","934456ee":"markdown","7f051dfb":"markdown","aad0816c":"markdown"},"source":{"f363e7a0":"pip install scikit-learn  -U","379e4d2e":"import pandas as pd\nimport numpy as np \nimport matplotlib.pyplot as plt\nimport matplotlib.dates as mdates\nimport seaborn as sns\n\n#import lightgbm as lgb \n#import xgboost as xgb\n#from catboost import CatBoostRegressor\n\nfrom sklearn.linear_model import LinearRegression,HuberRegressor,Ridge,TweedieRegressor\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.preprocessing import StandardScaler, RobustScaler, MinMaxScaler\n\nimport dateutil.easter as easter\n\nimport optuna\nimport math","c183f174":"LINEAR_DATE_AUG = True\n\nOPTUNA = False\nNUM_TRIALS = 400\n\n\n#Holidays\nHOLIDAYS = True     \nNEXT_HOLIDAY = False  \n\nPOST_PROCESSING = False\nMODEL_TYPE = \"Tweedie Regression\"\n\nVAL_SPLIT = \"2017-12-31\" #\"2018-05-31\"","04d992a4":"EPOCHS = 10000    \nEARLY_STOPPING = 30\nDEVICE = \"cpu\"\n\nSCALER_NAME = \"MinMax\"  #None MinMax Standard\nSCALER = MinMaxScaler()  #MinMaxScaler StandardScaler","4593affd":"train = pd.read_csv(\"..\/input\/tabular-playground-series-jan-2022\/train.csv\",index_col = 0)\ntest = pd.read_csv(\"..\/input\/tabular-playground-series-jan-2022\/test.csv\",index_col = 0)\ngdp_df = pd.read_csv('..\/input\/gdp-20152019-finland-norway-and-sweden\/GDP_data_2015_to_2019_Finland_Norway_Sweden.csv')\ngdp_df.set_index('year', inplace=True)\n\nif HOLIDAYS:\n    holidays = pd.read_csv(\"..\/input\/holidays-finland-norway-sweden-20152019\/Holidays_Finland_Norway_Sweden_2015-2019.csv\",usecols = [\"Date\",\"Country\",\"Name\"]                      )\n    holidays.rename(columns = {\"Date\":\"date\",\"Country\":\"country\",\"Name\":\"holiday\"},inplace= True)\n    holidays[\"holiday\"]= 1\n    holidays[\"holiday\"]= holidays[\"holiday\"].astype(\"int32\")\n    holidays[\"date\"] = pd.to_datetime(holidays[\"date\"])","02c2ceee":"#Make date\ntrain[\"date\"] = pd.to_datetime(train[\"date\"])\ntest[\"date\"] = pd.to_datetime(test[\"date\"])","f92f3b35":"def public_hols(df):\n    df = pd.merge(df, holidays, how='left', on=['date', 'country'])\n    df.fillna(value = 0,inplace=True)\n    return df","dce11f12":"def get_gdp(row):\n    country = 'GDP_' + row.country\n    return gdp_df.loc[row.date.year, country]","f1ae88c8":"if HOLIDAYS:\n    train = public_hols(train)\n    test = public_hols(test)","a70caaf6":"def engineer(df):\n   \n    #get GDP from file \n    df[\"gdp\"] = df.apply(get_gdp, axis=1)   #improves Huber & Tweedie\n    \n    df[\"day\"] = df[\"date\"].dt.day\n    df[\"dayofweek\"] = df[\"date\"].dt.dayofweek\n    df[\"month\"] = df[\"date\"].dt.month\n    df[\"year\"] = df[\"date\"].dt.year\n    \n    #play around with if Tree model - each varies \n    #df['dayofyear'] = df['date'].dt.dayofyear                ### This can cause noise - remove but keep inverse\n    df['inverse_dayofyear'] = 365 - df['date'].dt.dayofyear\n    #df['quarter'] = 'Q' + df['date'].dt.quarter.astype(str)\n    #df['daysinmonth'] = df['date'].dt.days_in_month           ## Also reduces performance\n     \n    if LINEAR_DATE_AUG:   \n        for country in ['Finland', 'Norway']:\n            df[country] = df.country == country\n        df['KaggleRama'] = df.store == 'KaggleRama'\n        for product in ['Kaggle Mug', 'Kaggle Sticker']:\n            df[product] = df['product'] == product\n\n        df[\"Friday\"] = df[\"dayofweek\"] ==4\n        df[\"Sat_sun\"] = (df[\"dayofweek\"] ==5) |(df[\"dayofweek\"] ==6)\n\n        df.drop([\"country\",\"store\",\"product\"], axis =1, inplace = True)\n\n        # Seasonal variations (Fourier series)\n        dayofyear = df.date.dt.dayofyear\n        for k in range(1, 21):\n            df[f'sin{k}'] = np.sin(dayofyear \/ 365 * 2 * math.pi * k)\n            df[f'cos{k}'] = np.cos(dayofyear \/ 365 * 2 * math.pi * k)\n            df[f'mug_sin{k}'] = df[f'sin{k}'] * df['Kaggle Mug']\n            df[f'mug_cos{k}'] = df[f'cos{k}'] * df['Kaggle Mug']\n            df[f'sticker_sin{k}'] = df[f'sin{k}'] * df['Kaggle Sticker']\n            df[f'sticker_cos{k}'] = df[f'cos{k}'] * df['Kaggle Sticker']\n\n    return df","3cedf2e7":"def engineer_more(df):\n    \"\"\"Return a new dataframe with more engineered features\"\"\"\n    new_df = engineer(df)\n\n    # End of year\n    new_df = pd.concat([new_df,\n                        pd.DataFrame({f\"dec{d}\":\n                                      (df.date.dt.month == 12) & (df.date.dt.day == d)\n                                      for d in range(24, 32)})],\n                       axis=1)\n    new_df = pd.concat([new_df,\n                        pd.DataFrame({f\"jan{d}\":\n                                      (df.date.dt.month == 1) & (df.date.dt.day == d) \n                                      for d in range(1, 13)})],\n                       axis=1)\n    \n    # May\n    new_df = pd.concat([new_df,\n                        pd.DataFrame({f\"may{d}\":\n                                      (df.date.dt.month == 5) & (df.date.dt.day == d) \n                                      for d in list(range(1, 10)) + list(range(17, 25))})],\n                       axis=1)\n    \n    # June\n    new_df = pd.concat([new_df,\n                        pd.DataFrame({f\"june{d}\":\n                                      (df.date.dt.month == 6) & (df.date.dt.day == d) \n                                      for d in list(range(6, 14))})],\n                       axis=1)\n    \n    # Last Wednesday of June\n    wed_june_date = df.date.dt.year.map({2015: pd.Timestamp(('2015-06-24')),\n                                         2016: pd.Timestamp(('2016-06-29')),\n                                         2017: pd.Timestamp(('2017-06-28')),\n                                         2018: pd.Timestamp(('2018-06-27')),\n                                         2019: pd.Timestamp(('2019-06-26'))})\n    new_df = pd.concat([new_df,\n                        pd.DataFrame({f\"wed_june{d}\": \n                                      (df.date - wed_june_date == np.timedelta64(d, \"D\"))\n                                      for d in list(range(-5, 6))})],\n                       axis=1)\n    \n    # First Sunday of November\n    sun_nov_date = df.date.dt.year.map({2015: pd.Timestamp(('2015-11-1')),\n                                         2016: pd.Timestamp(('2016-11-6')),\n                                         2017: pd.Timestamp(('2017-11-5')),\n                                         2018: pd.Timestamp(('2018-11-4')),\n                                         2019: pd.Timestamp(('2019-11-3'))})\n    new_df = pd.concat([new_df,\n                        pd.DataFrame({f\"sun_nov{d}\": \n                                      (df.date - sun_nov_date == np.timedelta64(d, \"D\"))\n                                      for d in list(range(0, 10))})],\n                       axis=1)\n    \n    # Easter\n    easter_date = df.date.apply(lambda date: pd.Timestamp(easter.easter(date.year)))\n    new_df = pd.concat([new_df,\n                        pd.DataFrame({f\"easter{d}\": \n                                      (df.date - easter_date == np.timedelta64(d, \"D\"))\n                                      for d in list(range(-2, 11)) + list(range(40, 48)) + list(range(50, 59))})],\n                       axis=1)\n    \n    # Growth is country-specific\n    #for country in ['Finland', 'Norway', 'Sweden']:\n    #    new_df[f\"{country}_year\"] = (df.country == country) * (df.date.dt.year - 2016)\n    #    new_df[f\"{country}_peak_year\"] = (df.country == country) * (new_df.dec29 | new_df.dec30 | new_df.easter_week) * (df.date.dt.year - 2016)\n        \n    return new_df\n\ntrain = engineer_more(train)\ntrain['num_sold'] = train.num_sold.astype(np.float32)\ntest = engineer_more(test)\n\n\n######################### CHECK THIS  -- WEIRD \n#test.year = 2018 # no growth patch, see https:\/\/www.kaggle.com\/c\/tabular-playground-series-jan-2022\/discussion\/298318\n\nfeatures = test.columns\nprint(list(features))","018f430e":"#train = engineer(train)\n#test = engineer(test)\n\n#Enconding the last features \ncategorical_feats = [\n    #country\",\"store\",\"product\",\n                     #\"quarter\",\n                     #Friday\",\n                     #Sat_Sun\"\n                    ]\n\n# Linear models improves with dropping first - Trees dont \ntrain = pd.get_dummies(train,columns= categorical_feats,drop_first=True)\ntest = pd.get_dummies(test,columns= categorical_feats,drop_first=True)","68041a89":"def next_holiday(x):\n    i=1\n    while sum(holidays[\"date\"] == pd.Timestamp(x) + pd.DateOffset(days=i)) ==0:\n        i+=1\n        if i >200:\n            i=0\n            break\n            break\n    return i\n\nif NEXT_HOLIDAY:\n    holidays[\"date\"] = pd.to_datetime(holidays[\"date\"])\n    train[\"to_holiday\"] = train[\"date\"].apply(lambda x : next_holiday(x))\n    test[\"to_holiday\"] = test[\"date\"].apply(lambda x : next_holiday(x))","8d0150a2":"def SMAPE(y_true, y_pred):\n    denominator = (y_true + np.abs(y_pred)) \/ 200.0\n    diff = np.abs(y_true - y_pred) \/ denominator\n    diff[denominator == 0] = 0.0\n    return np.mean(diff)","dc82b16c":"def scale_data(X_train, test, X_test= None):\n     \n    scaler= SCALER\n    \n    #this can be train or X_train \n    X_train_s = scaler.fit_transform(X_train)\n    test_s = scaler.transform(test)\n    \n   \n    if X_test is not None:\n        X_test_s = scaler.transform(X_test)\n        return X_train_s, test_s , X_test_s\n    \n    else:\n        return X_train_s, test_s ","39e32cd3":"def create_lag(DAYS,df):\n    df[f\"shift{DAYS}\"] = df.groupby([\"store\",\"product\",\"country\"])[\"num_sold\"].shift(DAYS,fill_value = 0)\n    return df","ff15a5aa":"def rolling_mean_std(day_roll_list, df):\n    shift_days = day_roll_list[0]\n    roll_window = day_roll_list[1]\n    col_name = 'rolling_'+str(shift_days)+'_'+str(roll_window)\n    df[col_name+\"_mean\"] = df.groupby([\"store\",\"product\",\"country\"])[\"num_sold\"].shift(shift_days).rolling(roll_window).mean()\n    df[col_name+\"_std\"] = df.groupby([\"store\",\"product\",\"country\"])[\"num_sold\"].shift(shift_days).rolling(roll_window).std()\n    \n    return df","01fa780c":"prior_2017 = train[train[\"date\"]<=\"2017-12-31\"].index\nafter_2017 = train[train[\"date\"]>\"2017-12-31\"].index","81e46f57":"train.index = train[\"date\"]\ntrain.drop(\"date\",axis=1,inplace=True)\n\ntest.index = test[\"date\"]\ntest.drop(\"date\",axis=1,inplace=True)","58c12a26":"train = train.astype(\"float64\")\ntest = test.astype(\"float64\")","c962ff62":"X = train.drop(\"num_sold\", axis=1)\ny= train[\"num_sold\"]","70607102":"X_train = train.iloc[prior_2017,:].drop(\"num_sold\", axis=1)\nX_test = train.iloc[after_2017,:].drop(\"num_sold\", axis=1)\ny_train= train.iloc[prior_2017,:][\"num_sold\"]\ny_test= train.iloc[after_2017,:][\"num_sold\"]","744ac2bf":"params = {\"power\":1.0,\n    \"alpha\":0.0,\n    \"fit_intercept\":False,\n    \"link\":'log', #\u2018auto\u2019, \u2018identity\u2019, \u2018log\u2019}\n    \"tol\":0.00000000001,\n    \"warm_start\":False\n         }","f23bd128":"def fit_model(X, y , test, X_test = None,y_test= None):\n    \n    model = TweedieRegressor(max_iter=EPOCHS, **params)\n    \n    # validation \n    if X_test is not None: \n        X_train_s, test_s , X_test_s = scale_data(X, test, X_test)\n        \n        model.fit(X_train_s,y)\n        preds = model.predict(X_test_s)\n        \n        smape = SMAPE(y_test,preds)\n        print(\"SMAPE:\",smape )\n        \n        return preds, model, smape\n    \n    # Full prediction \n    else:\n        X_s, test_s = scale_data(X, test)\n        \n        model.fit(X_s,y)\n        preds = model.predict(test_s)\n        \n        return preds, model","55ceddb1":"'''def fit_model(X_train,y_train,X_test = None,y_test= None):\n    \n    model = TweedieRegressor(max_iter=EPOCHS, **params)\n    \n    if X_test is None:\n        model.fit(X_train,y_train)\n        preds = model.predict(test)\n        \n    else:\n        model.fit(X_train,y_train)\n        preds = model.predict(X_test)\n        print(\"SMAPE:\", SMAPE(y_test,preds))\n\n        \n    return preds, model'''","8b58aac6":"val_predictions, model, smape = fit_model(X_train,y_train,test, X_test,y_test)","ce2d2171":"print(\"SMAPE :\",SMAPE(y_test,val_predictions) )\nprint(f\"\\n EPOCHS: {EPOCHS}\")\nprint(f\"\\n SCALER: {SCALER_NAME}\")\nprint(f\"\\n POST_PROCESSING: {POST_PROCESSING}\")","165d48e6":"test_predictions, model = fit_model(train.drop([\"num_sold\"],axis =1),train[\"num_sold\"] , test)","d0687fa2":"test_predictions","c335a28c":"sub = pd.read_csv(\"..\/input\/tabular-playground-series-jan-2022\/sample_submission.csv\",index_col = 0)","660bbbac":"final_predictions = test_predictions.round()","7510f304":"if POST_PROCESSING:\n    # from previous run we are under predicting, lets scale the values upwards\n    print(\"Scaling predictions \")\n    print(\"preds_prior:\", final_predictions)\n    \n    sub[\"num_sold\"] = final_predictions*1.043\n    \n    print(\"preds after:\", np.array(sub[\"num_sold\"]))\nelse:\n    sub[\"num_sold\"] = final_predictions","83bf7b92":"sub.to_csv(\"submission.csv\")","6c7bad41":"sub.head()","db013f4c":"sub.head()","00e8cc81":"#for visual only\ntest = pd.read_csv(\"..\/input\/tabular-playground-series-jan-2022\/test.csv\",index_col = 0)\ntest[\"date\"] = pd.to_datetime(test[\"date\"])\n\nfig,ax = plt.subplots(2,1, figsize=(25,20),sharey= True)\n\ndiff = y_test - val_predictions\nsns.lineplot(ax=ax[0], data= y_test, label=\"Train Actual\",ci=None)\nsns.lineplot(ax=ax[0], data = y_test,x = y_test.index , y = val_predictions, label =\"Validation Prediction\" ,ci=None)\nsns.lineplot(ax=ax[0],data =sub, x= test[\"date\"], y = \"num_sold\",label=\"Final Prediction\" ,ci=None) \n\nax[0].set_title(f\"Actual and Predicted Sales for {MODEL_TYPE}\")\n\nsns.lineplot(ax=ax[1], data = diff, label =\"Residuals\" )\nax[1].set_title(f\"Validation Residuals\")\n\nplt.show()","af5249a3":"plt.figure(figsize=(25,10))\n\nsns.lineplot(data= train[\"num_sold\"] ,label=\"Train Actual\",ci=None)\nsns.lineplot(data =sub, x= test[\"date\"], y = \"num_sold\",label=\"Final Prediction\" ,ci=None) \nplt.title(\"Actual and Predicted Sales\")\n\nplt.show()","3c1b2055":"# Functions ","c5f16a9f":"# Fit on Full Train data ","66f1b770":"# Libraries","6f886082":"# Run model","9c91561a":"Thanks to [ambrosm](https:\/\/www.kaggle.com\/anirudhg15)\n\nhttps:\/\/www.kaggle.com\/ambrosm\/tpsjan22-03-linear-model\/notebook#More-feature-engineering-(advanced-model)","ea59885f":"# Submission ","4ba6cad6":"# Load Data","e40aafb7":"#To DO\n1. Run a model for each product\n1. Recursive model \n1. Add Further Feature engineering - ambrosm https:\/\/www.kaggle.com\/ambrosm\/tpsjan22-03-linear-model\/","245637d5":"obj is the objective function of the algorithm, i.e. what it's trying to maximize or minimize, e.g. \"regression\" means it's minimizing squared residuals.\n\nMetric and eval are essentially the same. They are used for Early stopping ","934456ee":"# Post Processing ","7f051dfb":"# Split and Scale","aad0816c":"# Training Visualization"}}