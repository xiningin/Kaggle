{"cell_type":{"0183e19b":"code","343c62bc":"code","bb8d415d":"code","9f3ade63":"code","1c68c3f8":"code","d05412d6":"code","431af06e":"code","fb7f1dea":"code","ee16ff84":"code","2e6b3b99":"code","a333040f":"code","6f6482a6":"code","1f5d2040":"code","53298435":"code","a89ef5d7":"code","0947d393":"code","38bfd686":"code","155bdc85":"code","b26659c7":"code","5bdd43cf":"code","efab2f54":"code","f07ddf62":"code","bc152809":"code","f3b30705":"code","84212361":"code","ec5f1343":"markdown","f2fa7ccf":"markdown","18605f0f":"markdown","8067ffa8":"markdown","937f3743":"markdown","256aa3a1":"markdown","9d30492c":"markdown","becbf9b0":"markdown","ac9d3fbb":"markdown","46553031":"markdown","a9b0e1ce":"markdown","361c9db0":"markdown","b2e8b157":"markdown","6c8e88d3":"markdown","9a585cd6":"markdown","a7c07a68":"markdown","a8bcca96":"markdown","88a55100":"markdown","e9567212":"markdown","fc847a5a":"markdown","d30be4a5":"markdown","cc349fe9":"markdown","b7c7e1e6":"markdown","43b9983e":"markdown","1d853915":"markdown","bd10d60d":"markdown","38aed243":"markdown","1ec802ed":"markdown","7ff44bef":"markdown","6a677e12":"markdown","9e19f0b3":"markdown","0a4dbdba":"markdown","25f398bb":"markdown","35fcfd09":"markdown","fa1e63f3":"markdown","1bb4e95e":"markdown","f93b6cc7":"markdown","41b8a6a8":"markdown","136ffc37":"markdown","83ac6a48":"markdown","c714ca78":"markdown"},"source":{"0183e19b":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","343c62bc":"import numpy as np\nimport matplotlib.pyplot as plt \nimport pandas as pd \nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings('ignore') \nplt.style.use('fivethirtyeight')","bb8d415d":"dataset=pd.read_csv('..\/input\/social-network-ads\/Social_Network_Ads.csv')\ndataset.head(2)","9f3ade63":"print('Rows     :',dataset.shape[0])\nprint('Columns  :',dataset.shape[1])\nprint('\\nFeatures :\\n     :',dataset.columns.tolist())\nprint('\\nMissing values    :',dataset.isnull().values.sum())\nprint('\\nUnique values :  \\n',dataset.nunique())","1c68c3f8":"dataset.describe().T","d05412d6":"f,ax=plt.subplots(1,2,figsize=(18,8))\ndataset['Gender'].value_counts().plot.pie(explode=[0,0.05],autopct='%1.1f%%',ax=ax[0],shadow=True)\nax[0].set_title('Purchase by Gender')\nax[0].set_ylabel('Count')\nsns.countplot('Gender',data=dataset,ax=ax[1],order=dataset['Gender'].value_counts().index)\nax[1].set_title('Purchase by Gender')\nplt.show()","431af06e":"fig=plt.gcf()\nfig.set_size_inches(10,7)\nfig=sns.distplot(dataset['EstimatedSalary'],kde=True,bins=50);","fb7f1dea":"f,ax=plt.subplots(1,2,figsize=(18,8))\ndataset['Purchased'].value_counts().plot.pie(explode=[0,0.05],autopct='%1.1f%%',ax=ax[0],shadow=True)\nax[0].set_title('Purchase Distribution')\nax[0].set_ylabel('Count')\nsns.countplot('Purchased',data=dataset,ax=ax[1],order=dataset['Purchased'].value_counts().index)\nax[1].set_title('Purchase Distribution')\nplt.show()","ee16ff84":"X=dataset.iloc[:,[2,3]].values\ny=dataset.iloc[:,4].values","2e6b3b99":"from sklearn.model_selection import train_test_split   #cross_validation doesnt work any more\nX_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.2,random_state=0) \n#X_train","a333040f":"from sklearn.preprocessing import StandardScaler \nsc_X=StandardScaler()\nX_train=sc_X.fit_transform(X_train)\nX_test=sc_X.fit_transform(X_test)\n#X_train","6f6482a6":"from sklearn.neighbors import KNeighborsClassifier\nclassifier=KNeighborsClassifier(n_neighbors=1,metric='minkowski',p=2)\nclassifier.fit(X_train,y_train)","1f5d2040":"y_pred=classifier.predict(X_test)","53298435":"#from sklearn.metrics import confusion_matrix  #Class has capital at the begining function starts with small letters \nfrom sklearn.metrics import confusion_matrix,classification_report,accuracy_score\ncm=confusion_matrix(y_test,y_pred)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nf, ax = plt.subplots(figsize =(5,5))\nsns.heatmap(cm,annot = True,linewidths=0.5,linecolor=\"red\",fmt = \".0f\",ax=ax)\nplt.title(\"Test for Test Dataset\")\nplt.xlabel(\"predicted y values\")\nplt.ylabel(\"real y values\")\nplt.show()","a89ef5d7":"print(accuracy_score(y_test,y_pred))","0947d393":"print(classification_report(y_test,y_pred))","38bfd686":"from matplotlib.colors import ListedColormap\nX_set,y_set=X_train,y_train\nX1,X2=np.meshgrid(np.arange(start=X_set[:,0].min()-1,stop=X_set[:,0].max()+1,step=0.01),\n                 np.arange(start=X_set[:,1].min()-1,stop=X_set[:,1].max()+1,step=0.01))\nplt.contourf(X1,X2,classifier.predict(np.array([X1.ravel(),X2.ravel()]).T).reshape(X1.shape),\n            alpha=0.75,cmap=ListedColormap(('red','green')))\nplt.xlim(X1.min(),X1.max())\nplt.ylim(X2.min(),X2.max())\nfor i,j in enumerate(np.unique(y_set)):\n    plt.scatter(X_set[y_set==j,0],X_set[y_set==j,1],\n               c=ListedColormap(('red','green'))(i),label=j)\nplt.title('K-NN (Training set)')\nplt.xlabel('Age')\nplt.ylabel('Estimated Salary')\nplt.show()","155bdc85":"\nfrom matplotlib.colors import ListedColormap\nX_set,y_set=X_test,y_test\nX1,X2=np.meshgrid(np.arange(start=X_set[:,0].min()-1,stop=X_set[:,0].max()+1,step=0.01),\n                 np.arange(start=X_set[:,1].min()-1,stop=X_set[:,1].max()+1,step=0.01))\nplt.contourf(X1,X2,classifier.predict(np.array([X1.ravel(),X2.ravel()]).T).reshape(X1.shape),\n            alpha=0.75,cmap=ListedColormap(('red','green')))\nplt.xlim(X1.min(),X1.max())\nplt.ylim(X2.min(),X2.max())\nfor i,j in enumerate(np.unique(y_set)):\n    plt.scatter(X_set[y_set==j,0],X_set[y_set==j,1],\n               c=ListedColormap(('red','green'))(i),label=j)\nplt.title('K-NN (Test set)')\nplt.xlabel('Age')\nplt.ylabel('Estimated Salary')\nplt.legend()\nplt.show()","b26659c7":"error_rate=[]\n\nfor i in range(1,40):\n    knn=KNeighborsClassifier(n_neighbors=i)\n    knn.fit(X_train,y_train)\n    pred_i=knn.predict(X_test)\n    error_rate.append(np.mean(pred_i != y_test))","5bdd43cf":"plt.figure(figsize=(10,6))\nplt.plot(range(1,40),error_rate,color='blue',linestyle='dashed',marker='o',markerfacecolor='red',markersize=10)\nplt.title('Error Rate Vs K Value')\nplt.xlabel('K')\nplt.ylabel('Error Rate')\nplt.show()","efab2f54":"from sklearn.neighbors import KNeighborsClassifier\nclassifier=KNeighborsClassifier(n_neighbors=5,metric='minkowski',p=2)\nclassifier.fit(X_train,y_train)","f07ddf62":"y_pred=classifier.predict(X_test)","bc152809":"#from sklearn.metrics import confusion_matrix  #Class has capital at the begining function starts with small letters \nfrom sklearn.metrics import confusion_matrix,classification_report,accuracy_score\ncm=confusion_matrix(y_test,y_pred)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nf, ax = plt.subplots(figsize =(5,5))\nsns.heatmap(cm,annot = True,linewidths=0.5,linecolor=\"red\",fmt = \".0f\",ax=ax)\nplt.title(\"Test for Test Dataset\")\nplt.xlabel(\"predicted y values\")\nplt.ylabel(\"real y values\")\nplt.show()","f3b30705":"print(accuracy_score(y_test,y_pred))","84212361":"print(classification_report(y_test,y_pred))","ec5f1343":"### Building Model with K=5","f2fa7ccf":"# 2.Exploratory Data Analysis ","18605f0f":"To can refer to my other notebooks from https:\/\/www.kaggle.com\/binuthomasphilip\/code","8067ffa8":"# 4.Model Evaluation","937f3743":"### Importing Python Modules","256aa3a1":"# 5.Optimum Value of K ","9d30492c":"### Generating Matrix of Features","becbf9b0":"### Recently I published a self help book titled Inspiration: Thoughts on Spirituality, Technology, Wealth, Leadership and Motivation. The preview of the book can be read from the Amazon link https:\/\/lnkd.in\/gj7bMQA","ac9d3fbb":"### Accuracy Score K=5","46553031":"### Confusion Matrix K=5","a9b0e1ce":"### Predicting Test Results","361c9db0":"### Accoracy Score","b2e8b157":"We can see that mean age is arounf 37\n\nMean estimated salary is 69742 $","6c8e88d3":"### Feature Scaling ","9a585cd6":"## Model Performance with K=5","a7c07a68":"Correct predictions =54+21=78\n\nWrong predictions =1+4=5\n\nAccuracy = (78\/83)*100 =93.97 %","a8bcca96":"So Most people in the dataset have not brough the car.All our attempts should be towards selling more cars.","88a55100":"### Visualizing the Test Result","e9567212":"### KNN Model Fitting","fc847a5a":"### Importing Dataset","d30be4a5":"So we can see that when we increased the K number from 1 to 5 the accuracy,Precision and Recall of the model have improved.","cc349fe9":"# 3.Model Built","b7c7e1e6":"## Gender","43b9983e":"We can see like mean salary of the population is arond 75000 $.","1d853915":"Red area represent the people who didnt buy the car.Green area represent people who brought the car.We can see that the accuracy level obtained is greater than thats we might have obtained by other Algorithms like Logistic Regression.","bd10d60d":"# 6.Conclusion:\n\n1.KNN algorithm is used for classification.Here we have predicted purchase decision using KNN Algorithm.\n\n2.In case of KNN algorithm the critical factor is to find out the optimum value of K.The optimum value of K results in better model performance.\n\n3.We found out that K=5 is the optimum vale of K for this dataset.By using k=5 the accuracy of our model has increased from 91% to 93%.","38aed243":"So we have the User ID, Gender,Age,Salary and the data if Purchase made by a used.","1ec802ed":"### Describing the Data","7ff44bef":"Data is almost even between Male and Female with sligh inclination towards female","6a677e12":"# 1.Data Import and Preprocessing ","9e19f0b3":"### Summary of Dataset","0a4dbdba":"To can refer to my other notebooks from https:\/\/www.kaggle.com\/binuthomasphilip\/code","25f398bb":"KNN is a supervised machine learning algorithm.It can be used to classification of data set.Here I will be sharing code for implementing kNN and share some tricks for KNN.The dataset contains the Age,salary and the decision to purchase a particular Car.We will be using KNN to predict if a particular person will buy a car.The notebook will cover following topics \n\n1.Data Import and Preprocessing \n\n2.Exploratory Data Analysis \n\n3.Model Built \n\n4.Model Evaluation \n\n5.Optimum Value of k \n\n6.Conclusion \n\n### To can refer to my other notebooks from https:\/\/www.kaggle.com\/binuthomasphilip\/code","35fcfd09":"### Estimated Salary Distribution","fa1e63f3":"### Classification Report","1bb4e95e":"### Purchase Distribution","f93b6cc7":"### Test Train Split","41b8a6a8":"So here we have plotted the change in error with different values of K.We can see that with k=5 we do get a very low value of Error.So we can make a model with K=5 and see the results.","136ffc37":"### Visualising the Training Set Results","83ac6a48":"### Classification Report K=5","c714ca78":"### Confusion Matrix"}}