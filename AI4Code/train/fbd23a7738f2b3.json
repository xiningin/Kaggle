{"cell_type":{"a722b0cb":"code","c84b6637":"code","08304bef":"code","e0d07997":"code","7fedd940":"code","fde53f62":"code","a0204d4f":"code","f49e6068":"code","d539f361":"code","a0042b9e":"code","4aac4b8d":"code","7685b59f":"code","011ec14b":"code","c4fb7fcb":"code","71f0f7c6":"code","cd1f2321":"code","946bf459":"code","23f9aeb8":"code","4ebad59e":"code","dd8cbccf":"code","e0c94096":"code","18107fc9":"code","ffece457":"code","fb32dccb":"code","e4d27117":"code","df09570e":"code","0345d08e":"code","ada1a71d":"code","d66edec1":"code","fb919e79":"code","e0d72da2":"code","5bfa3cb8":"code","66982269":"code","057c7770":"code","42f1a0c3":"code","f2cb3afd":"code","36c3e51a":"code","11f95c0f":"code","275443f0":"code","6b2e52a3":"code","a0e02fa2":"code","74d83ca3":"code","c0ec74ad":"code","4dbce8c4":"code","ed8be165":"code","acaa76f7":"code","ac5818d7":"code","bf1c6394":"code","4f7c7200":"code","acbe4874":"code","641b75a7":"code","652a1891":"code","0c30b927":"code","45c01e0f":"code","b5d85837":"code","c86fde6c":"code","6265a1be":"code","ff34a346":"code","e40f48bf":"code","f9b915c4":"code","e21fe834":"code","14ed6698":"code","20e0b53c":"code","7a9387fd":"code","77e2c754":"code","be074082":"code","dc48989d":"code","5ac0b58e":"code","ca4e6cf2":"code","878e6f05":"code","bb3b7de0":"code","41741a37":"code","e08a4ae0":"code","6aec0c3d":"code","5e030b5b":"code","5c100709":"code","dee7698b":"code","a5394fab":"code","e11d0e43":"code","8160f5e5":"code","b5b8779b":"code","c4919bb0":"code","446880bc":"code","97b4be95":"code","ef834905":"code","fd1f01cb":"code","6f0eac78":"code","ce544e16":"markdown","cff23405":"markdown","3de8b750":"markdown","9dd62ddb":"markdown","abc2c38d":"markdown","133ace39":"markdown","282aa0fd":"markdown","4d0cd307":"markdown","611678cd":"markdown","cb4c36e8":"markdown","6b698628":"markdown","a4f34747":"markdown","a3693b8c":"markdown","ac8c6a6e":"markdown","b4cf3940":"markdown","0abeba56":"markdown","90c66070":"markdown","d436ed9d":"markdown","46369349":"markdown","c102f813":"markdown","7720b276":"markdown","50ee7572":"markdown","778dbcad":"markdown","ad9237cd":"markdown","42efafcb":"markdown","eda0c25a":"markdown","ea00b2f0":"markdown","73874568":"markdown","d9e75155":"markdown","31f26583":"markdown","1a5ca1ed":"markdown","fef9d866":"markdown","108bc173":"markdown","d64880cf":"markdown","fa71a898":"markdown","c7bd7a3a":"markdown","13d3ee3a":"markdown","57036dd5":"markdown","5867b8d3":"markdown","110ed75b":"markdown","3d71ff04":"markdown","33d6560e":"markdown"},"source":{"a722b0cb":"#Import necesary libraries \nimport pandas as pd # Basic tool for transform and clean Data. \npd.options.display.max_columns = 100 # This option lets us see on the screen 100 columns of informatio\n                                    # that is really usefull for a better visualitation of our data\nimport numpy as np\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport sklearn.metrics as metrics\nimport warnings\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom sklearn.preprocessing import PowerTransformer, MinMaxScaler\nimport tensorflow as tf\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import classification_report\nwarnings.filterwarnings('ignore')","c84b6637":"XY = pd.read_csv('..\/input\/online-shoppers-intention\/online_shoppers_intention.csv')","08304bef":"print(u'- El n\u00famero de filas en el dataset es: {}'.format(XY.shape[0]))\nprint(u'- El n\u00famero de columnas en el dataset es: {}'.format(XY.shape[1]))\nprint(u'- Los nombres de las variables son: {}'.format(list(XY.columns)))\nXY[:2]","e0d07997":"XY.info()","7fedd940":"XY.describe(include = 'bool')","fde53f62":"#We just have few NAN in our dataset so we'll just delete them.  \nXY.isnull().sum()","a0204d4f":"XY = XY.dropna()\nXY.isnull().sum()\n#Now we dont have nan.","f49e6068":"#Function for describe every column:\n\ndef describe_column(df, col):\n    print(f'Columna: {col}  -  Tipo de datos: {df[col].dtype}')\n    print(f'N\u00famero de valores nulos: {df[col].isnull().sum()}  -  N\u00famero de valores distintos: {df[col].nunique()}')\n    print('Valores m\u00e1s frecuentes:')\n    for i, v in df[col].value_counts().iloc[:20].items() :\n        print(i, '\\t', v)","d539f361":"describe_column(XY, 'Administrative')\n#All looks good.","a0042b9e":"describe_column(XY, 'Administrative_Duration')\n# Since this columns tell us the time spent on certain type of page, a -1 value\n# its a strange value, then we can infer that '-1' represent nan values also.  \n# we will take a look.","4aac4b8d":"menos_1 = XY['Administrative_Duration'] == -1\nmenos_uno = XY[menos_1]","7685b59f":"menos_uno.head(3)","011ec14b":"XY = XY.drop(XY[XY['Administrative_Duration']==-1].index)","c4fb7fcb":"#Now our columns looks better. \ndescribe_column(XY, 'Administrative_Duration')","71f0f7c6":"describe_column(XY, 'Informational') #All perfect. Next one","cd1f2321":"describe_column(XY, 'Informational_Duration') #All perfect. Next one","946bf459":"describe_column(XY, 'ProductRelated') #All perfect. Next one","23f9aeb8":"describe_column(XY, 'ProductRelated_Duration') #All perfect. Next one","4ebad59e":"describe_column(XY, 'BounceRates') #All perfect. Next one","dd8cbccf":"describe_column(XY, 'ExitRates') #All perfect. Next one","e0c94096":"describe_column(XY, 'PageValues') #All perfect. Next one","18107fc9":"describe_column(XY, 'SpecialDay') #All perfect. Next one","ffece457":"describe_column(XY, 'Month') #We have two months left. ","fb32dccb":"describe_column(XY, 'OperatingSystems') #Ok","e4d27117":"describe_column(XY, 'Browser') #Ok","df09570e":"describe_column(XY, 'Region') #Ok","0345d08e":"describe_column(XY, 'TrafficType') #Ok","ada1a71d":"describe_column(XY, 'VisitorType') #This 'other' values looks suspicius. But ill keep\n#them for the moment. ","d66edec1":"describe_column(XY, 'Weekend')","fb919e79":"describe_column(XY, 'Revenue')","e0d72da2":"# Transform the booleans object to numeric for our future ML or DL model and graphs\nXY['Weekend'] = XY['Weekend'].map({False:0, True:1})\nXY['Revenue'] = XY['Revenue'].map({False:0, True:1})","5bfa3cb8":"# 10 categories (Months)\nXY.Month.unique() ","66982269":"# Just 3 categories\nXY.VisitorType.unique()","057c7770":"X_D = pd.get_dummies(XY,columns=['Month','VisitorType'],drop_first=True)\nX_D[:3]","42f1a0c3":"X_numeric = XY.drop(columns=['Month', 'VisitorType', 'Revenue'])\n# We create this DF witouth categories columns for plot","f2cb3afd":"X = X_D.drop('Revenue', axis=1)\ny = X_D['Revenue']","36c3e51a":"plt.figure(figsize=(15,7))\nax = sns.boxplot(data=X)\nax.set_xticklabels(ax.get_xticklabels(),rotation=90)\nplt.title(u'Representaci\u00f3n de cajas de las variables independientes X')\nplt.ylabel('Valor de la variable normalizada')\n_ = plt.xlabel('Nombre de la variable')","11f95c0f":"# Detect the outliers on ProductRelated_Duration, which, according to our graphs,\n# was the only  column with possible outliers\n\nq = X[\"ProductRelated_Duration\"].quantile(0.99)\n\n# and then filter with:\n\ntime_spent=X[X[\"ProductRelated_Duration\"] > q]","275443f0":"time_spent_in_seconds = time_spent['ProductRelated_Duration'].mean()","6b2e52a3":"time_spent_in_hours = time_spent_in_seconds\/3600 # This columns its in seconds and here we transform it\n# in hours for better analysis.","a0e02fa2":"round(time_spent_in_hours, 2)","74d83ca3":"# Functions that we'll use. Its not really necessary at this point that you understand every line of\n# code.\n\ndef relaciones_vs_target(X, Y, return_type='axes'):\n    '''\n    Funci\u00f3n que representa gr\u00e1ficos de dispersi\u00f3n de las variables\n    en X en funci\u00f3n a la variable Y\n    '''\n    fig_tot = (len(X_numeric.columns))\n    fig_por_fila = 4.\n    tamanio_fig = 4.\n    num_filas = int( np.ceil(fig_tot\/fig_por_fila) )    \n    plt.figure( figsize=( fig_por_fila*tamanio_fig+5, num_filas*tamanio_fig+5 ) )\n    c = 0 \n    for i, col in enumerate(X.columns):\n        plt.subplot(num_filas, fig_por_fila, i+1)\n        sns.scatterplot(x=X_numeric[col], y=Y)\n        plt.title( '%s vs %s' % (col, 'target') )\n        plt.ylabel('Target')\n        plt.xlabel(col)\n    plt.show()\n\ndef represento_doble_hist(x_1, x_0, n_bins=11, title='', label_1='Clase 1', \n                          label_0='Clase 0', density=0):\n    '''\n    Funci\u00f3n que recibe dos distribuciones de probabilidad y las representa\n    en el mismo gr\u00e1fico\n    '''\n    bins = n_bins\n    plt.hist(x_1, bins, density = density, alpha=0.5, label=label_1, color='red')    \n    plt.hist(x_0, bins, density = density, alpha=0.5, label=label_0, color='green')\n    plt.title(title)\n    plt.legend(loc='best') \n\ndef hist_pos_neg_feat(x, y, density=0, nbins=11, targets=(0,1)):\n    '''\n    Representa las variables en x divididas en dos distribuciones\n    seg\u00fan su valor de y sea 1 o 0\n    '''\n    fig_tot = len(x.columns)\n    fig_tot_fila = 3.; fig_tamanio = 5.\n    num_filas = int( np.ceil(fig_tot\/fig_tot_fila) )\n    plt.figure( figsize=( fig_tot_fila*fig_tamanio+2, num_filas*fig_tamanio+2 ) )\n    target_neg, target_pos = targets\n    for i, feat in enumerate(x.columns):\n        plt.subplot(num_filas, fig_tot_fila, i+1);\n        plt.title('%s' % feat)\n        idx_pos = y == target_pos\n        idx_neg= y == target_neg\n        represento_doble_hist(x[feat][idx_pos].values, x[feat][idx_neg].values, nbins, \n                   density = density, title=('%s' % feat))","c0ec74ad":"hist_pos_neg_feat(X,y)","4dbce8c4":"matriz_correlaciones = X_numeric.corr(method='pearson')\nn_ticks = len(X_numeric.columns)\nplt.figure( figsize=(9, 9) )\nplt.xticks(range(n_ticks), X_numeric.columns, rotation='vertical')\nplt.yticks(range(n_ticks), X_numeric.columns)\nplt.colorbar(plt.imshow(matriz_correlaciones, interpolation='nearest', \n                            vmin=-1., vmax=1., \n                            cmap=plt.get_cmap('Blues')))\n_ = plt.title('Matriz de correlaciones de Pearson')","ed8be165":"# Correlation with Revenue\ndata_corr = XY.corr()['Revenue'] \nsns.barplot(data_corr[0:-1].index,data_corr[0:-1].values).set_title('Correlation with the Revenue')\nplt.xticks(rotation = 90)\nplt.show()","acaa76f7":"# Lets see the Ratio of Revenue in each types\n\nplt.style.use('fivethirtyeight')\nfig,ax = plt.subplots(nrows = 2, ncols = 4,figsize = (17,10))\nfig.tight_layout(pad = 3)\n\n\nadm_rev = XY[['Administrative','Revenue']]\nrev_p1 = pd.DataFrame(XY.groupby('Revenue')['Administrative'].sum()).T\nrev_p1.plot.bar(stacked=True,ax=ax[0,0])\nax[0,0].set_xticklabels(['Administrative'], rotation=360)\nplt.legend(loc='best')\n\n\ninfo_rev = XY[['Informational','Revenue']]\nrev_p2 = pd.DataFrame(XY.groupby('Revenue')['Informational'].sum()).T\nrev_p2.plot.bar(stacked=True,ax = ax[0,1],color = ['black','white'])\nax[0,1].set_xticklabels(['Informational'], rotation=360)\nplt.legend(loc='best')\n\ninfo_rev = XY[['ProductRelated','Revenue']]\nrev_p2 = pd.DataFrame(XY.groupby('Revenue')['ProductRelated'].sum()).T\nrev_p2.plot.bar(stacked=True,ax = ax[0,2],color = ['purple','red'])\nax[0,2].set_xticklabels(['ProductRelated'], rotation=360)\nplt.legend(loc='best')\n\n\n\ninfo_rev = XY[['OperatingSystems','Revenue']]\nrev_p2 = pd.DataFrame(XY.groupby('Revenue')['OperatingSystems'].sum()).T\nrev_p2.plot.bar(stacked=True,ax = ax[0,3],color = ['green','yellow'])\nax[0,3].set_xticklabels(['OperatingSystems'], rotation=360)\n\n\ninfo_rev = XY[['Browser','Revenue']]\nrev_p2 = pd.DataFrame(XY.groupby('Revenue')['Browser'].sum()).T\nrev_p2.plot.bar(stacked=True,ax = ax[1,0],color = ['navy','orange'])\nax[1,0].set_xticklabels(['Browser'], rotation=360)\n\n\ninfo_rev = XY[['Region','Revenue']]\nrev_p2 = pd.DataFrame(XY.groupby('Revenue')['Region'].sum()).T\nrev_p2.plot.bar(stacked=True,ax = ax[1,1],color = ['black','red'])\nax[1,1].set_xticklabels(['Region'], rotation=360)\n\n\ninfo_rev = XY[['TrafficType','Revenue']]\nrev_p2 = pd.DataFrame(XY.groupby('Revenue')['TrafficType'].sum()).T\nrev_p2.plot.bar(stacked=True,ax = ax[1,2],color = ['blue','green'])\nax[1,2].set_xticklabels(['TrafficType'], rotation=360)\n\nfig.delaxes(ax[1,3])","ac5818d7":"#All perfect. Continue. ","bf1c6394":"obj_escalar = StandardScaler()\nX_estandarizado = obj_escalar.fit_transform(X_D)\n#Al estandarizar los datos mi regresion logistica me entraga un ROC de 1.","4f7c7200":"X_estandarizado","acbe4874":"# Now, we divide our df in train and test for start applyng models","641b75a7":"X_train, X_test, Y_train, Y_test = train_test_split(X, y, test_size=0.2, random_state=1234)\n","652a1891":"modelo1 = LogisticRegression()\nparametros = {\"C\": [0.001, 0.008, 0.01, 0.02, 0.03, 0.04, 0.05, 0.06], \n              \"class_weight\":['balanced', None]}","0c30b927":"modelo_gs = GridSearchCV(modelo1, param_grid=parametros,\n                         cv = 5, scoring='roc_auc')\nmodelo_gs.fit(X_train, Y_train)","45c01e0f":"print(modelo_gs.best_params_, \"\\nROC AUC: {}\".format(round(modelo_gs.best_score_,2)))","b5d85837":"df_search = pd.DataFrame.from_dict(modelo_gs.cv_results_)","c86fde6c":"plt.xlabel('C')\nplt.ylabel('roc_auc')\n_ = plt.plot( df_search['param_C'], df_search['mean_test_score'], '.')","6265a1be":"reg_log =  LogisticRegression(C=modelo_gs.best_params_['C'],\n                              class_weight=modelo_gs.best_params_['class_weight'])","ff34a346":"reg_log.fit(X_train, Y_train)","e40f48bf":"y_test_pred_prob = reg_log.predict_proba(X_test)\ny_test_pred_prob_pos = y_test_pred_prob[np.where(Y_test == 1)[0]]\ny_test_pred_prob_neg = y_test_pred_prob[np.where(Y_test == 0)[0]]","f9b915c4":"preds = y_test_pred_prob[:,1]\nfpr, tpr, threshold = metrics.roc_curve(Y_test, preds)\nroc_auc = metrics.auc(fpr, tpr)\n\nplt.figure(figsize=(10,7))\nplt.title('Receiver Operating Characteristic')\nplt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\nplt.legend(loc = 'lower right')\nplt.plot([0, 1], [0, 1],'r--')\nplt.xlim([0, 1])\nplt.ylim([0, 1])\nplt.ylabel('True Positive Rate')\nplt.xlabel('False Positive Rate')\nplt.show()","e21fe834":"represento_doble_hist(y_test_pred_prob_pos[:, 1], y_test_pred_prob_neg[:, 1], n_bins=21, density=0)","14ed6698":"umbral = 0.6\ny_umbralizadas = 1*(y_test_pred_prob[:, 1] > umbral)","20e0b53c":"print(u\"Matriz de confusi\u00f3n\\n\", metrics.confusion_matrix(Y_test, y_umbralizadas))\nprint(\"\\nAccuracy\\t{}\".format(round(metrics.accuracy_score(Y_test, y_umbralizadas),2)))  \nprint(\"Sensitividad\\t{}\".format(round(metrics.recall_score(Y_test, y_umbralizadas),2)))\nprint(u\"Precisi\u00f3n\\t{}\".format(round(metrics.precision_score(Y_test, y_umbralizadas),2)))    ","7a9387fd":"umbral = 0.7\ny_umbralizadas = 1*(y_test_pred_prob[:, 1] > umbral)","77e2c754":"print(u\"Matriz de confusi\u00f3n\\n\", metrics.confusion_matrix(Y_test, y_umbralizadas))\nprint(\"\\nAccuracy\\t{}\".format(round(metrics.accuracy_score(Y_test, y_umbralizadas),2)))  \nprint(\"Sensitividad\\t{}\".format(round(metrics.recall_score(Y_test, y_umbralizadas),2)))\nprint(u\"Precisi\u00f3n\\t{}\".format(round(metrics.precision_score(Y_test, y_umbralizadas),2)))    ","be074082":"modelo = MLPClassifier()\nparametros = {'solver': ['lbfgs'], \n              'max_iter': [300,500, 800, 1000], # Iteraciones m\u00e1ximas en cada red\n              'alpha': 10.0 ** -np.arange(0.5, 2), # Par\u00e1metro de regularizaci\u00f3n L2 para evitar sobreajuste\n              'hidden_layer_sizes':np.arange(10, 35), # N\u00famero de neuronas en cada capa\n              'random_state':[0]}","dc48989d":"modelo_gs2 = GridSearchCV(modelo, param_grid=parametros, cv = 3, \n                         scoring='roc_auc', n_jobs=-1, verbose=10)\nmodelo_gs2.fit(X_train, Y_train)","5ac0b58e":"#In this case for GridSearch the Max Iter was 500, and we obtein the result that this param was the\n#Best, so now ill adjust GridSearch for testing with Higher Max Iter and looks if our model improve.\nprint(modelo_gs2.best_params_, \"\\nROC AUC: {}\".format(round(modelo_gs2.best_score_,2)))","ca4e6cf2":"#How we coud see, the ROC AUC improve using the max Iter again that i gave. First ill check with the\n# test DF for look if we also get the same improve or in the worst case we get and overfit. \nprint(modelo_gs2.best_params_, \"\\nROC AUC: {}\".format(round(modelo_gs2.best_score_,2)))","878e6f05":"mejor_modelo = MLPClassifier(**modelo_gs2.best_params_, verbose=10)","bb3b7de0":"mejor_modelo.fit(X_train, Y_train)","41741a37":"y_test_pred_prob = mejor_modelo.predict_proba(X_test) \ny_test_pred_prob_pos = y_test_pred_prob[np.where(Y_test == 1)[0]]\ny_test_pred_prob_neg = y_test_pred_prob[np.where(Y_test == 0)[0]]","e08a4ae0":"preds = y_test_pred_prob[:,1]\nfpr, tpr, threshold = metrics.roc_curve(Y_test, preds)\nroc_auc = metrics.auc(fpr, tpr)\n\nplt.figure(figsize=(10,7))\nplt.title('Receiver Operating Characteristic')\nplt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\nplt.legend(loc = 'lower right')\nplt.plot([0, 1], [0, 1],'r--')\nplt.xlim([0, 1])\nplt.ylim([0, 1])\nplt.ylabel('True Positive Rate')\nplt.xlabel('False Positive Rate')\nplt.show()","6aec0c3d":"represento_doble_hist(y_test_pred_prob_pos[:, 1], y_test_pred_prob_neg[:, 1], n_bins=21, density=0)","5e030b5b":"umbral = 0.5\ny_umbralizadas = 1*(y_test_pred_prob[:, 1] > umbral)","5c100709":"print(u\"Matriz de confusi\u00f3n\\n\", metrics.confusion_matrix(Y_test, y_umbralizadas))\nprint(\"\\nAccuracy\\t{}\".format(round(metrics.accuracy_score(Y_test, y_umbralizadas),2)))  \nprint(\"Sensitividad\\t{}\".format(round(metrics.recall_score(Y_test, y_umbralizadas),2)))\nprint(u\"Precisi\u00f3n\\t{}\".format(round(metrics.precision_score(Y_test, y_umbralizadas),2)))  ","dee7698b":"# Modifing the Maxiter from 500 to 1000, improve the model but only on the train df. it keept\n# 0.88 accurary. Probably we coud tried with a higher number of itermax.","a5394fab":"rfclf = RandomForestClassifier(n_estimators = 30,max_depth = 10,random_state = 101)\n\n\nrfclf.fit(X_train,Y_train)\npred = rfclf.predict(X_test)\nprint(classification_report(Y_test,pred))","e11d0e43":"# Lets Optimize the Random Forest Classifier using GridSearch\nparam_grid = {\n    'n_estimators' : [80,100],\n    'max_depth' : [10,15],\n    'min_samples_leaf' : [2,3],\n    'min_samples_split': [2,4]\n}\n\ngridsearch = GridSearchCV(estimator=rfclf,param_grid=param_grid,verbose = 1)\ngridsearch.fit(X_train,Y_train)","8160f5e5":"gridsearch.best_params_","b5b8779b":"rfclf = RandomForestClassifier(n_estimators = 100,max_depth = 10,min_samples_leaf = 3, min_samples_split = 2,random_state = 101)\nrfclf.fit(X_train,Y_train)\npred = rfclf.predict(X_test)\nprint(classification_report(Y_test,pred))\n\n# 0 is False, 1 is True, the precision of detecting True has increased\nfrom sklearn.metrics import accuracy_score\nrfacc = accuracy_score(Y_test,pred)","c4919bb0":"import eli5\nfrom eli5.sklearn import PermutationImportance\n\nperm = PermutationImportance(rfclf, random_state=1).fit(X_test, Y_test)\neli5.show_weights(perm, feature_names = X_test.columns.tolist())","446880bc":"compradores_si = XY['Revenue'] == 1\ncompradores = XY[compradores_si]","97b4be95":"compradores_no = XY['Revenue'] == 0\nno_compradores = XY[compradores_no]","ef834905":"for i,k in zip(compradores.mean(),no_compradores.mean()):\n    print (i,k)","fd1f01cb":"print(compradores.mean(), ), print(no_compradores.mean())","6f0eac78":"final_plot= XY.drop(columns= ['OperatingSystems', 'Browser', 'Region', 'TrafficType'])\nfinal_plot.groupby('Revenue').mean().plot(kind='bar', figsize=(15,7))\nplt.title('Gasto medio por producto en cada cl\u00faster')\nplt.xlabel(u'N\u00famero de cl\u00faster')\n_ = plt.ylabel('Valor medio de gasto')","ce544e16":"This figure represents in green the probabilities assigned by the model to data that are 0s (the closer the green distribution is to 0, the better) and in red the probabilities assigned to the data that are 1s (the closer the red distribution is to 1 best).\n\nEsta figura representa en verde las probabilidades que asigna el modelo a los datos que son 0s (cuanto m\u00e1s cerca de 0 la distribuci\u00f3n verde mejor) y en rojo las probabilidades asignadas a los datos que son 1s (cuanto m\u00e1s cerca est\u00e9 de 1 la distribuci\u00f3n roja mejor).","cff23405":"<center><h1> Conclusion","3de8b750":"Pleas, any question, doubt or suggestion I will be happy to answer you.","9dd62ddb":"<center><h1> Neural Network Clasification","abc2c38d":"<center><h1> Shap method","133ace39":"<center><H2> Logistic Regresion","282aa0fd":"All the variables are concentrated on the lower part of the graph, with the expeption of 'ProductRelated_Duration'. That is a really good reason for standarize the Data before to train our ML or DL model.\n\n**its Important** also, that big outlier in 'ProductRelated_Duration', but taking in consideretion that this variable is the time spent inside of the specefic shoping page probably is not a wrong value. ","4d0cd307":"#### <center> We finish, now we'll separete our target Y from the features X ","611678cd":"According to these graphs, we can see that customers who complete online purchases are evenly distributed within our data. Only with a visual analysis can not obtain new value data to contribute to the Marketing department.","cb4c36e8":"### Since, we just have 13 categories, use the dummies functions its a good options for our dataframe. In case of have so much more categories values probably we should choose another option.\nMainly because the Dummies function adds X amount of columns to our dataframe based on how many categories our base columns have.","6b698628":"0.88 ROC AUC its a really nice result. But we must continue looking for other models. \nImportant that in the code cell above, 'best_params_ give us the result of the best parameters for use finally in our model for this Dataset","a4f34747":"From this application of 'PermutationImportance' to two of our models 'PageValues' appears to us as the main feature of importance when defining who will or will not make the purchase. Faced with this, our next step, in order to generate insights and transform our analysis into money for the company, is to thoroughly analyze that column and how it behaves.\nThis could only be done by means of very detailed and advanced statistical graphics, which is not objective in this work, but it is undoubtedly what would make the difference between a successful case or just one more Machine Learning model.\n___\nDesde esta aplicacion de 'PermutationImportance' a dos de nuestros modelos 'PageValues' nos aparece como la principal feature de importancia al momento de definir quien realizar\u00e1 o no la compra. Frente a esto nuestro siguiente paso, de vista a generar insights y tranformar nuestro analisis en dinero para la empresa es analisar a fondo aquella columna y como se comporta. \nEsto solo se podria realizar mediantes graficas estadisticas muy detalladas y avanzadas, lo cual no es objetivo en el presente trabajo, pero sin duda es lo que marcar\u00eda la diferencia entre un caso de exito o solo un modelo m\u00e1s de Machine Learning.","a3693b8c":"Finally we train our model with the best parameters that GridSearch give us.","ac8c6a6e":"We see the same results again regarding which features are more important\n____\nVolvemos a ver los mismo resultados respecto a que featurees son mas importantes","b4cf3940":"###  Resultantly, Page Values has the highest correlation( around 0.5) with Revenue compare to all other features.","0abeba56":"We have to choose the umbral for separate our results which are continuous values into a binary option of Yes or Not (1 or 0) about the revenue probability.","90c66070":"<center> <h1> Outlier","d436ed9d":"<center><h1> Standardization of data:","46369349":"Columns :'Administrative', 'Informational' and 'ProductRelated' there are the most related with the revenue results. We have to keept that in mind. ","c102f813":"<center><h1> RandomForestClassifier","7720b276":"All this '-1' looks like wrong values so we'll drop them. We can do it without problems, because there are just few values and looks like there are conected with other -1 values in the nexts columns.","50ee7572":"## <center> Our next important step is analyze column by column, looking for stranger values","778dbcad":"<center><h1> Graphs","ad9237cd":"<h1><center>Clustering analysis Google Analytics, Full explained with GridSearch, Neural Networks, RandomForest and Logistic Regression models.","42efafcb":"<center><h1> Manual Insigths\n   ","eda0c25a":"At this point we coud see that we need a binary cluster Ml model to indetify what kind of user will buy in the online stores. So, we will start  \n# <center> **Preprocessing Data**","ea00b2f0":"### For graphs our variables, the first step is transform the categorical values to numeric.","73874568":"Here we coud see that the mean of spent time in the shop page for the 0.01 higher quantile of 'ProductRelated_Duration' columns is 3.59 hours. That looks really normal. \n\n\u00bfWho hasnt spent 3 hours choosing the best product?\n\n**The otliers will remain in our dataframe**","d9e75155":"### Looking for the explanation of ours models. ","31f26583":"As in the other models, we obtain the best parameters found and fit a model with those parameters:","1a5ca1ed":"GridSearchCV its a extremly useful code for ours ML model. Its lets us find the best combination of parameters. We shoud use it always","fef9d866":"### For take a desicion about wich method we'll use, first we need to know how many variables have 'Month' and 'VisitorType' colums.","108bc173":"Analizando el modelo con el mejor alpha\nEn este paso nos quedamos con los mejores par\u00e1metros obtenidos en el paso anterior:","d64880cf":"# ***Objetive:***\n\nWe work as data scientists for a retail company that, due to the change in customer consumption habits, is widely promoting the online sales service. The company wants to run a machine learning model to rank customers based on the likelihood of generating revenue when shopping on the web.\n\nThe goal is to perform a series of specific actions for customers who are most likely to make purchases on the web.","fa71a898":"## <center> GRAPHS","c7bd7a3a":"<center><H3> Now, We'll train 3 differents ML model looking for the best one ","13d3ee3a":"<h1><center> \"Permutation Importance\"","57036dd5":"We finally keep in 0.7 the threshold because it give us best performance. ","5867b8d3":"Here, is when the Test Set comes into play. When you want to validate an already chosen and optimized model.\nWith that model optimized, I predict tests to see how it behaves on data that you haven't seen before","110ed75b":"Finally we have developed the 3 ML models to predict whether a visitor to the e-commerce page will make a purchase or not. But as far as we have seen so far we have the big problem that these results are not easily transformed into value for the marketing department or someone else within the company.\n\nThis is due to the \"Black Box\" characteristic of the vast majority of the models used, which translates into not being able to extract important insights from the application of Machine Learning models.\n\nIt is because of the above that the \"Permutation Importance\" technique will finally be used to determine which of all our features are the ones that provide the closest approximation to determining whether the purchase will be made or not.\n\n-----------\n\nFinalmente hemos desarrollado los 3 modelos de ML para predecir si un visitante a la pagina de comercio electr\u00f3nico realizara una compra o no. Pero hasta donde hemos visto hasta ahora nos surge el gran problema de que estos resultados no son facilmente transformables en valor para el departamento de marketing o algun otro dentro de la compa\u00f1ia. \n\nEsto es debido a la caracteristica de \"Caja Negra\" de la gran mayoria de los modelos utilizados, lo que se traduce en no poder extraer importantes insigths desde la aplicacion de modelos de Machine Learning.\n\nEs en razon a lo anterior que se utilziara finalmente la tecnica de \"Permutation Importance\" para determinar cual de todas nuestras features son las que entregan mayor aproximacion a determinar si se realizar\u00e1 la compra o no. ","3d71ff04":"## Best parameters","33d6560e":"From my analysis, customers who are interested on buying the products :\nTend to stay longer on the website especially when they are on the website which is productrelated. Less likely to bounce or exit\n\nAll of the aforementioned would not charge an important value for the company if this model is not put into production in the cloud to analyze the data of potential buyers in real time and in this way direct marketing strategies to all those people who they are more likely to make a purchase.\n___\nFinalmente, como todo lo obtenido mediante el presente analisis, se puede corroborar las caracteristicas principales de los compradores quienes en promedio gastan mas tiempo visitando las paginas web, como tambien el numero de veces que las visitan, al mismo tiempo que los promedios de BounceRate y ExitRates son menores para estos grupos.\n\nTodo lo anteriomente expuesto no cobrar\u00eda un valor importante para la compa\u00f1ia si es que este modelo no se pone en produccion en la nube para analizar en tiempo real los datos de los posibles compradores y de esta forma direccionar las estrategias de marketing a todos aquellas personas que se encuentran mas propensas a realizar una compra. "}}