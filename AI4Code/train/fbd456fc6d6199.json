{"cell_type":{"9b09dbc8":"code","96278adf":"code","0aaf59b3":"code","45675aa9":"code","664b44f6":"code","c055118c":"code","b750c315":"code","e5ef0f08":"code","4e7e64a2":"markdown","8a79fae1":"markdown","88eba926":"markdown","c8d09d45":"markdown","9c8400b6":"markdown","fdaf6d42":"markdown","9bff1e15":"markdown"},"source":{"9b09dbc8":"\"\"\"Basic word2vec example.\"\"\"\n\n# \u5bfc\u5165\u4e00\u4e9b\u9700\u8981\u7684\u5e93\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport collections\nimport math\nimport os\nimport random\nimport zipfile\n\nimport numpy as np\nfrom six.moves import urllib\nfrom six.moves import xrange  # pylint: disable=redefined-builtin\nimport tensorflow as tf\nimport PIL","96278adf":"url = 'http:\/\/mattmahoney.net\/dc\/'\n\n\ndef maybe_download(filename, expected_bytes):\n    \"\"\"\n    \u8fd9\u4e2a\u51fd\u6570\u7684\u529f\u80fd\u662f\uff1a\n        \u5982\u679cfilename\u4e0d\u5b58\u5728\uff0c\u5c31\u5728\u4e0a\u9762\u7684\u5730\u5740\u4e0b\u8f7d\u5b83\u3002\n        \u5982\u679cfilename\u5b58\u5728\uff0c\u5c31\u8df3\u8fc7\u4e0b\u8f7d\u3002\n        \u6700\u7ec8\u4f1a\u68c0\u67e5\u6587\u5b57\u7684\u5b57\u8282\u6570\u662f\u5426\u548cexpected_bytes\u76f8\u540c\u3002\n    \"\"\"\n    if not os.path.exists(filename):\n        print('start downloading...')\n        filename, _ = urllib.request.urlretrieve(url + filename, filename)\n    statinfo = os.stat(filename)\n    if statinfo.st_size == expected_bytes:\n        print('Found and verified', filename)\n    else:\n        print(statinfo.st_size)\n        raise Exception(\n            'Failed to verify ' + filename + '. Can you get to it with a browser?')\n    return filename\n\n\n# \u4e0b\u8f7d\u8bed\u6599\u5e93text8.zip\u5e76\u9a8c\u8bc1\u4e0b\u8f7d\nfilename = maybe_download('text8.zip', 31344016)\n\n\n# \u5c06\u8bed\u6599\u5e93\u89e3\u538b\uff0c\u5e76\u8f6c\u6362\u6210\u4e00\u4e2aword\u7684list\ndef read_data(filename):\n    \"\"\"\n    \u8fd9\u4e2a\u51fd\u6570\u7684\u529f\u80fd\u662f\uff1a\n        \u5c06\u4e0b\u8f7d\u597d\u7684zip\u6587\u4ef6\u89e3\u538b\u5e76\u8bfb\u53d6\u4e3aword\u7684list\n    \"\"\"\n    with zipfile.ZipFile(filename) as f:\n        data = tf.compat.as_str(f.read(f.namelist()[0])).split()\n    return data\n\n\nvocabulary = read_data(filename)\nprint('Data size', len(vocabulary))  # \u603b\u957f\u5ea6\u4e3a1700\u4e07\u5de6\u53f3\n# \u8f93\u51fa\u524d100\u4e2a\u8bcd\u3002\nprint(vocabulary[0:100])","0aaf59b3":"# \u8bcd\u8868\u7684\u5927\u5c0f\u4e3a10\u4e07\uff08\u5373\u6211\u4eec\u53ea\u8003\u8651\u6700\u5e38\u51fa\u73b0\u768410\u4e07\u4e2a\u8bcd\uff09\nvocabulary_size = 100000\n\n\ndef build_dataset(words, n_words):\n    \"\"\"\n    \u51fd\u6570\u529f\u80fd\uff1a\u5c06\u539f\u59cb\u7684\u5355\u8bcd\u8868\u793a\u53d8\u6210index\n    \"\"\"\n    count = [['UNK', -1]]\n    count.extend(collections.Counter(words).most_common(n_words - 1))\n    dictionary = dict()\n    for word, _ in count:\n        dictionary[word] = len(dictionary)\n    data = list()\n    unk_count = 0\n    for word in words:\n        if word in dictionary:\n            index = dictionary[word]\n        else:\n            index = 0  # UNK\u7684index\u4e3a0\n            unk_count += 1\n        data.append(index)\n    count[0][1] = unk_count\n    reversed_dictionary = dict(zip(dictionary.values(), dictionary.keys()))\n    return data, count, dictionary, reversed_dictionary\n\n\ndata, count, dictionary, reverse_dictionary = build_dataset(vocabulary,\n                                                            vocabulary_size)\ndel vocabulary  # \u5220\u9664\u5df2\u8282\u7701\u5185\u5b58\n# \u8f93\u51fa\u6700\u5e38\u51fa\u73b0\u76845\u4e2a\u5355\u8bcd\nprint('Most common words (+UNK)', count[:5])\n# \u8f93\u51fa\u8f6c\u6362\u540e\u7684\u6570\u636e\u5e93data\uff0c\u548c\u539f\u6765\u7684\u5355\u8bcd\uff08\u524d10\u4e2a\uff09\nprint('Sample data', data[:10], [reverse_dictionary[i] for i in data[:10]])\n# \u6211\u4eec\u4e0b\u9762\u5c31\u4f7f\u7528data\u6765\u5236\u4f5c\u8bad\u7ec3\u96c6\ndata_index = 0","45675aa9":"\ndef generate_batch(batch_size, num_skips, skip_window):\n    # data_index\u76f8\u5f53\u4e8e\u4e00\u4e2a\u6307\u9488\uff0c\u521d\u59cb\u4e3a0\n    # \u6bcf\u6b21\u751f\u6210\u4e00\u4e2abatch\uff0cdata_index\u5c31\u4f1a\u76f8\u5e94\u5730\u5f80\u540e\u63a8\n    global data_index\n    assert batch_size % num_skips == 0\n    assert num_skips <= 2 * skip_window\n    batch = np.ndarray(shape=(batch_size), dtype=np.int32)\n    labels = np.ndarray(shape=(batch_size, 1), dtype=np.int32)\n    span = 2 * skip_window + 1  # [ skip_window target skip_window ]\n    buffer = collections.deque(maxlen=span)\n    # data_index\u662f\u5f53\u524d\u6570\u636e\u5f00\u59cb\u7684\u4f4d\u7f6e\n    # \u4ea7\u751fbatch\u540e\u5c31\u5f80\u540e\u63a81\u4f4d\uff08\u4ea7\u751fbatch\uff09\n    for _ in range(span):\n        buffer.append(data[data_index])\n        data_index = (data_index + 1) % len(data)\n    for i in range(batch_size \/\/ num_skips):  # \/\/\u8fd0\u7b97\u7b26\uff1a\u53d6\u6574\u9664 - \u8fd4\u56de\u5546\u7684\u6574\u6570\u90e8\u5206\uff08\u5411\u4e0b\u53d6\u6574\uff09\n        # \u5229\u7528buffer\u751f\u6210batch\n        # buffer\u662f\u4e00\u4e2a\u957f\u5ea6\u4e3a 2 * skip_window + 1\u957f\u5ea6\u7684word list\n        # \u4e00\u4e2abuffer\u751f\u6210num_skips\u4e2a\u6570\u7684\u6837\u672c\n        # print([reverse_dictionary[i] for i in buffer])\n        target = skip_window  # target label at the center of the buffer\n        # targets_to_avoid\u4fdd\u8bc1\u6837\u672c\u4e0d\u91cd\u590d\n        targets_to_avoid = [skip_window]\n        for j in range(num_skips):\n            while target in targets_to_avoid:\n                target = random.randint(0, span - 1)\n            targets_to_avoid.append(target)\n            batch[i * num_skips + j] = buffer[skip_window]\n            labels[i * num_skips + j, 0] = buffer[target]\n        buffer.append(data[data_index])\n        # \u6bcf\u5229\u7528buffer\u751f\u6210num_skips\u4e2a\u6837\u672c\uff0cdata_index\u5c31\u5411\u540e\u63a8\u8fdb\u4e00\u4f4d\n        data_index = (data_index + 1) % len(data)\n    data_index = (data_index + len(data) - span) % len(data)\n    return batch, labels\n\n\n# \u9ed8\u8ba4\u60c5\u51b5\u4e0bskip_window=1, num_skips=2\n# \u6b64\u65f6\u5c31\u662f\u4ece\u8fde\u7eed\u76843(3 = skip_window*2 + 1)\u4e2a\u8bcd\u4e2d\u751f\u62102(num_skips)\u4e2a\u6837\u672c\u3002\n# \u5982\u8fde\u7eed\u7684\u4e09\u4e2a\u8bcd['used', 'against', 'early']\n# \u751f\u6210\u4e24\u4e2a\u6837\u672c\uff1aagainst -> used, against -> early\nbatch, labels = generate_batch(batch_size=8, num_skips=2, skip_window=1)\nfor i in range(8):\n    print(batch[i], reverse_dictionary[batch[i]],\n          '->', labels[i, 0], reverse_dictionary[labels[i, 0]])","664b44f6":"batch_size = 128\nembedding_size = 128  # \u8bcd\u5d4c\u5165\u7a7a\u95f4\u662f128\u7ef4\u7684\u3002\u5373word2vec\u4e2d\u7684vec\u662f\u4e00\u4e2a128\u7ef4\u7684\u5411\u91cf\nskip_window = 1       # skip_window\u53c2\u6570\u548c\u4e4b\u524d\u4fdd\u6301\u4e00\u81f4\nnum_skips = 2         # num_skips\u53c2\u6570\u548c\u4e4b\u524d\u4fdd\u6301\u4e00\u81f4\n\n# \u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\uff0c\u4f1a\u5bf9\u6a21\u578b\u8fdb\u884c\u9a8c\u8bc1\n# \u9a8c\u8bc1\u7684\u65b9\u6cd5\u5c31\u662f\u627e\u51fa\u548c\u67d0\u4e2a\u8bcd\u6700\u8fd1\u7684\u8bcd\u3002\n# \u53ea\u5bf9\u524dvalid_window\u7684\u8bcd\u8fdb\u884c\u9a8c\u8bc1\uff0c\u56e0\u4e3a\u8fd9\u4e9b\u8bcd\u6700\u5e38\u51fa\u73b0\nvalid_size = 16     # \u6bcf\u6b21\u9a8c\u8bc116\u4e2a\u8bcd\nvalid_window = 100  # \u8fd916\u4e2a\u8bcd\u662f\u5728\u524d100\u4e2a\u6700\u5e38\u89c1\u7684\u8bcd\u4e2d\u9009\u51fa\u6765\u7684\nvalid_examples = np.random.choice(valid_window, valid_size, replace=False)\n\n# \u6784\u9020\u635f\u5931\u65f6\u9009\u53d6\u7684\u566a\u58f0\u8bcd\u7684\u6570\u91cf\nnum_sampled = 64\n\ngraph = tf.Graph()\n\nwith graph.as_default():\n\n    # \u8f93\u5165\u7684batch\n    train_inputs = tf.placeholder(tf.int32, shape=[batch_size])\n    train_labels = tf.placeholder(tf.int32, shape=[batch_size, 1])\n    # \u7528\u4e8e\u9a8c\u8bc1\u7684\u8bcd\n    valid_dataset = tf.constant(valid_examples, dtype=tf.int32)\n\n    # \u4e0b\u9762\u91c7\u7528\u7684\u67d0\u4e9b\u51fd\u6570\u8fd8\u6ca1\u6709gpu\u5b9e\u73b0\uff0c\u6240\u4ee5\u6211\u4eec\u53ea\u5728cpu\u4e0a\u5b9a\u4e49\u6a21\u578b\n    with tf.device('\/cpu:0'):\n        # \u5b9a\u4e491\u4e2aembeddings\u53d8\u91cf\uff0c\u76f8\u5f53\u4e8e\u4e00\u884c\u5b58\u50a8\u4e00\u4e2a\u8bcd\u7684embedding\n        embeddings = tf.Variable(\n            tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0))\n        # \u5229\u7528embedding_lookup\u53ef\u4ee5\u8f7b\u677e\u5f97\u5230\u4e00\u4e2abatch\u5185\u7684\u6240\u6709\u7684\u8bcd\u5d4c\u5165\n        embed = tf.nn.embedding_lookup(embeddings, train_inputs)\n\n        # \u521b\u5efa\u4e24\u4e2a\u53d8\u91cf\u7528\u4e8eNCE Loss\uff08\u5373\u9009\u53d6\u566a\u58f0\u8bcd\u7684\u4e8c\u5206\u7c7b\u635f\u5931\uff09\n        nce_weights = tf.Variable(\n            tf.truncated_normal([vocabulary_size, embedding_size],\n                                stddev=1.0 \/ math.sqrt(embedding_size)))\n        nce_biases = tf.Variable(tf.zeros([vocabulary_size]))\n\n    # tf.nn.nce_loss\u4f1a\u81ea\u52a8\u9009\u53d6\u566a\u58f0\u8bcd\uff0c\u5e76\u4e14\u5f62\u6210\u635f\u5931\u3002\n    # \u968f\u673a\u9009\u53d6num_sampled\u4e2a\u566a\u58f0\u8bcd\n    loss = tf.reduce_mean(\n        tf.nn.nce_loss(weights=nce_weights,\n                       biases=nce_biases,\n                       labels=train_labels,\n                       inputs=embed,\n                       num_sampled=num_sampled,\n                       num_classes=vocabulary_size))\n\n    # \u5f97\u5230loss\u540e\uff0c\u6211\u4eec\u5c31\u53ef\u4ee5\u6784\u9020\u4f18\u5316\u5668\u4e86\n    optimizer = tf.train.GradientDescentOptimizer(1.0).minimize(loss)\n\n    # \u8ba1\u7b97\u8bcd\u548c\u8bcd\u7684\u76f8\u4f3c\u5ea6\uff08\u7528\u4e8e\u9a8c\u8bc1\uff09\n    norm = tf.sqrt(tf.reduce_sum(tf.square(embeddings), 1, keep_dims=True))\n    normalized_embeddings = embeddings \/ norm\n    # \u627e\u51fa\u548c\u9a8c\u8bc1\u8bcd\u7684embedding\u5e76\u8ba1\u7b97\u5b83\u4eec\u548c\u6240\u6709\u5355\u8bcd\u7684\u76f8\u4f3c\u5ea6\n    valid_embeddings = tf.nn.embedding_lookup(\n        normalized_embeddings, valid_dataset)\n    similarity = tf.matmul(\n        valid_embeddings, normalized_embeddings, transpose_b=True)\n\n    # \u53d8\u91cf\u521d\u59cb\u5316\u6b65\u9aa4\n    init = tf.global_variables_initializer()","c055118c":"\nnum_steps = 230001\n\nwith tf.Session(graph=graph) as session:\n    # \u521d\u59cb\u5316\u53d8\u91cf\n    init.run()\n    print('Initialized')\n\n    average_loss = 0\n    for step in xrange(num_steps):\n        batch_inputs, batch_labels = generate_batch(\n            batch_size, num_skips, skip_window)\n        feed_dict = {train_inputs: batch_inputs, train_labels: batch_labels}\n\n        # \u4f18\u5316\u4e00\u6b65\n        _, loss_val = session.run([optimizer, loss], feed_dict=feed_dict)\n        average_loss += loss_val\n\n        if step % 2000 == 0:\n            if step > 0:\n                average_loss \/= 2000\n            # 2000\u4e2abatch\u7684\u5e73\u5747\u635f\u5931\n            print('Average loss at step ', step, ': ', average_loss)\n            average_loss = 0\n\n        # \u6bcf1\u4e07\u6b65\uff0c\u6211\u4eec\u8fdb\u884c\u4e00\u6b21\u9a8c\u8bc1\n        if step % 10000 == 0:\n            # sim\u662f\u9a8c\u8bc1\u8bcd\u4e0e\u6240\u6709\u8bcd\u4e4b\u95f4\u7684\u76f8\u4f3c\u5ea6\n            sim = similarity.eval()\n            # \u4e00\u5171\u6709valid_size\u4e2a\u9a8c\u8bc1\u8bcd\n            for i in xrange(valid_size):\n                valid_word = reverse_dictionary[valid_examples[i]]\n                top_k = 8  # \u8f93\u51fa\u6700\u76f8\u90bb\u76848\u4e2a\u8bcd\u8bed\n                nearest = (-sim[i, :]).argsort()[1:top_k + 1]\n                log_str = 'Nearest to %s:' % valid_word\n                for k in xrange(top_k):\n                    close_word = reverse_dictionary[nearest[k]]\n                    log_str = '%s %s,' % (log_str, close_word)\n                print(log_str)\n    # final_embeddings\u662f\u6211\u4eec\u6700\u540e\u5f97\u5230\u7684embedding\u5411\u91cf\n    # \u5b83\u7684\u5f62\u72b6\u662f[vocabulary_size, embedding_size]\n    # \u6bcf\u4e00\u884c\u5c31\u4ee3\u8868\u7740\u5bf9\u5e94index\u8bcd\u7684\u8bcd\u5d4c\u5165\u8868\u793a\n    final_embeddings = normalized_embeddings.eval()","b750c315":"# \u53ef\u89c6\u5316\u7684\u56fe\u7247\u4f1a\u4fdd\u5b58\u4e3a\u201ctsne.png\u201d\n\ndef plot_with_labels(low_dim_embs, labels, filename='tsne.png'):\n    assert low_dim_embs.shape[0] >= len(labels), 'More labels than embeddings'\n    plt.figure(figsize=(18, 18))  # in inches\n    for i, label in enumerate(labels):\n        x, y = low_dim_embs[i, :]\n        plt.scatter(x, y)\n        plt.annotate(label,\n                     xy=(x, y),\n                     xytext=(5, 2),\n                     textcoords='offset points',\n                     ha='right',\n                     va='bottom')\n\n    plt.savefig(filename)\n\n\ntry:\n    # pylint: disable=g-import-not-at-top\n    from sklearn.manifold import TSNE\n    import matplotlib\n    matplotlib.use('agg')\n    import matplotlib.pyplot as plt\n    # \u56e0\u4e3a\u6211\u4eec\u7684embedding\u7684\u5927\u5c0f\u4e3a128\u7ef4\uff0c\u6ca1\u6709\u529e\u6cd5\u76f4\u63a5\u53ef\u89c6\u5316\n    # \u6240\u4ee5\u6211\u4eec\u7528t-SNE\u65b9\u6cd5\u8fdb\u884c\u964d\u7ef4\n    tsne = TSNE(perplexity=30, n_components=2, init='pca', n_iter=8000)\n    # \u53ea\u753b\u51fa500\u4e2a\u8bcd\u7684\u4f4d\u7f6e\n    plot_only = 500\n    low_dim_embs = tsne.fit_transform(final_embeddings[:plot_only, :])\n    labels = [reverse_dictionary[i] for i in xrange(plot_only)]\n    plot_with_labels(low_dim_embs, labels)\n\nexcept ImportError:\n    print('Please install sklearn, matplotlib, and scipy to show embeddings.')","e5ef0f08":"PIL.Image.open(\".\/tsne1.png\")","4e7e64a2":"# \u7b2c\u4e8c\u6b65: \u5236\u4f5c\u4e00\u4e2a\u8bcd\u8868\uff0c\u5c06\u4e0d\u5e38\u89c1\u7684\u8bcd\u53d8\u6210\u4e00\u4e2aUNK\u6807\u8bc6\u7b26","8a79fae1":"\u8be6\u7ec6\u7b14\u8bb0: [\u8fd9\u91cc](https:\/\/coggle.it\/diagram\/XXIxd7Ml_i_-r37F\/t\/word2vec%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0)","88eba926":"# \u7b2c\u4e09\u6b65\uff1a\u5b9a\u4e49\u4e00\u4e2a\u51fd\u6570\uff0c\u7528\u4e8e\u751f\u6210skip-gram\u6a21\u578b\u7528\u7684batch","c8d09d45":"# \u7b2c\u4e94\u6b65\uff1a\u5f00\u59cb\u8bad\u7ec3","9c8400b6":"# \u7b2c\u56db\u6b65: \u5efa\u7acb\u6a21\u578b.","fdaf6d42":"# Step 6: \u53ef\u89c6\u5316","9bff1e15":"# \u7b2c\u4e00\u6b65: \u5728\u4e0b\u9762\u8fd9\u4e2a\u5730\u5740\u4e0b\u8f7d\u8bed\u6599\u5e93"}}