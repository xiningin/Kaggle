{"cell_type":{"651a2ac2":"code","245f29c9":"code","031de2b1":"code","a0268c9c":"code","61006989":"code","74c8a0a6":"code","35b42e5f":"code","bdf7a6ca":"code","a166c3d1":"code","5d0fa938":"code","3c114664":"code","9ac8ecb4":"code","4f5b6fa4":"code","90595e5b":"code","bc7d2556":"code","578ad74e":"code","c1415d27":"markdown","90ae09b7":"markdown","b9daa463":"markdown","7a9a0060":"markdown","1948c3d7":"markdown","eb419be2":"markdown","fb616aad":"markdown","d6ced30a":"markdown","9b81c77b":"markdown","5094348a":"markdown"},"source":{"651a2ac2":"pip install --upgrade torchmetrics","245f29c9":"from torchvision.models.detection import fasterrcnn_resnet50_fpn\nfrom torchvision.models.detection.faster_rcnn import FastRCNNPredictor\nimport torch\nimport os\nimport pandas as pd\nfrom torchvision.io import read_image\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision.transforms import ToTensor, Resize\nimport torchvision.transforms as transforms\nimport matplotlib.pyplot as plt\nfrom torchvision.models.detection import fasterrcnn_resnet50_fpn\nfrom tqdm import tqdm\n\nfrom torchmetrics.detection.map import MAP\n\nimport ast\n","031de2b1":"# TRAIN_MODE set to True if model already trained, False for inference\nTRAIN_MODE = True\n\n# Dataset Location\nIMG_DIR_PATH = '..\/input\/yandextoloka-water-meters-dataset\/WaterMeters\/images'\nANNOTATIONS_CSV_PATH = '..\/input\/yandextoloka-water-meters-dataset\/WaterMeters\/data.csv'\nMODEL_PATH = '' #### TODO: put the right path when TRAIN_MODE = False\n\n#Resize SIZE\nSIZE = 400\n\n# PYTORCH parameters\nDEVICE = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\nBATCH_SIZE = 8\nNB_EPOCHS = 8\nNB_ITERATION = 1","a0268c9c":"#first create a dictionnary linking index to image name\n# id values goes from 1 to 1244, I reframed that to have id from 0 to 1243.\n\nINT_TO_NAME = {}\nfor dirname, _, filenames in os.walk(IMG_DIR_PATH):\n    for filename in filenames:\n        split = filename.split(\"_\")\n        INT_TO_NAME[int(split[1])-1] = filename\n        \nprint('exemple: indice 2 file name -', INT_TO_NAME[1])\nprint('exemple: indice 1 file name -', INT_TO_NAME[0])\n","61006989":"location_test = pd.read_csv('..\/input\/yandextoloka-water-meters-dataset\/WaterMeters\/data.csv').iloc[108,2]\n\ndef location_to_bounding_boxe(location, size):\n    coordinate_list = ast.literal_eval(location)['data']\n    x_s = [pt['x']*size for pt in coordinate_list]\n    y_s = [pt['y']*size for pt in coordinate_list]\n    return max(min(x_s),0), min(min(y_s),size),  max(max(x_s),0), min(size,max(y_s))\n    \nprint(\"the location:\", location_test)\nprint(\"---\")\nprint(\"the corresponding bounding boxe\",location_to_bounding_boxe(location_test, 460))","74c8a0a6":"transform_ = transforms.Compose([\n        transforms.ToPILImage(),\n        transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1, hue=0.1),\n        transforms.Resize([SIZE,SIZE]),\n        transforms.ToTensor(),\n        #transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])#ResNet Normalization\n    ])\n\nclass ImageDataset(Dataset):\n    def __init__(self, annotations_file, img_dir, transform = None):\n        self.img_labels = pd.read_csv(annotations_file).set_index('photo_name')\n        self.img_dir = img_dir\n        \n        self.size = SIZE\n        \n        if transform:\n            self.transform = transform\n\n\n    def __len__(self):\n        return len(self.img_labels)\n\n    def __getitem__(self, idx):\n        name = INT_TO_NAME[idx]\n        img_path = os.path.join(self.img_dir, name)\n        image = read_image(img_path)\n        reading = self.img_labels.loc[name, 'value']\n        bounding_boxe =  location_to_bounding_boxe(self.img_labels.loc[name, 'location'], self.size)\n        \n        if self.transform:\n            image = self.transform(image)\n        \n        return image, reading, torch.as_tensor(bounding_boxe, dtype=torch.float32)","35b42e5f":"waterMeterDataset = ImageDataset(ANNOTATIONS_CSV_PATH, IMG_DIR_PATH, transform = transform_)","bdf7a6ca":"def show(imgs):\n    if not isinstance(imgs, list):\n        imgs = [imgs]\n    fix, axs = plt.subplots(ncols=len(imgs), squeeze=False)\n    for i, img in enumerate(imgs):\n        img = img.detach()\n        img = F.to_pil_image(img)\n        axs[0, i].imshow(np.asarray(img))\n        axs[0, i].set(xticklabels=[], yticklabels=[], xticks=[], yticks=[])","a166c3d1":"from torchvision.utils import draw_bounding_boxes\nimport torch\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nimport torchvision.transforms.functional as F\n\n\nplt.rcParams[\"savefig.bbox\"] = 'tight'\nplt.rcParams[\"figure.figsize\"] = (20,80)\n\nloader = DataLoader(waterMeterDataset, batch_size=8, shuffle=True)\nimg, _, bounding_boxes  = next(iter(loader))\nimg = (img.cpu()*255).type(torch.uint8)\nimages = [\n    draw_bounding_boxes(image, boxes=boxe.unsqueeze(0), width=4)\n    for image, boxe in zip(img, bounding_boxes)\n]\nshow(images)","5d0fa938":"figure = plt.figure(figsize=(8, 8))\ncols, rows = 3, 3\nfor i in range(1, cols * rows + 1):\n    sample_idx = torch.randint(len(waterMeterDataset), size=(1,)).item()\n    img, reading, bounding_boxe = waterMeterDataset[sample_idx]\n    figure.add_subplot(rows, cols, i)\n    plt.title(reading)\n    plt.axis(\"off\")\n    plt.imshow(img.squeeze().transpose(0,1).transpose(1,2), cmap=\"gray\")\nplt.show()","3c114664":"train_size = int(0.9*len(waterMeterDataset))","9ac8ecb4":"def give_model():\n    model = fasterrcnn_resnet50_fpn(pretrained=True, trainable_backbone_layers = 1)\n    num_classes = 2  # 1 class (watermeter) + background\n    # get number of input features for the classifier\n    in_features = model.roi_heads.box_predictor.cls_score.in_features\n    # replace the pre-trained head with a new one\n    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n    return model","4f5b6fa4":"import torchmetrics\n# adapted from https:\/\/debuggercafe.com\/custom-object-detection-using-pytorch-faster-rcnn\/\ndef train(train_dataloader, model, optimizer):\n    print('Training')\n    global train_itr\n    global train_loss_list\n    \n     # initialize tqdm progress bar\n    prog_bar = tqdm(train_dataloader, total=len(train_dataloader))\n    model.train()\n    for i, data in enumerate(prog_bar):\n        optimizer.zero_grad()\n        batch, _, bounding_boxes = data\n        \n        batch = list(image.to(DEVICE) for image in batch)\n        \n        #the faster R_CNN model takes as targets a list of dictionnary with boxe and label\n        targets = [{'boxes': bounding_boxe.unsqueeze(0).to(DEVICE), 'labels': torch.ones(1, dtype = torch.int64).to(DEVICE)} for bounding_boxe in bounding_boxes]\n        #it returns during training a dict containing the classification and regression losses.\n        loss_dict = model(batch, targets)\n        #print(loss_dict)\n        losses = sum(loss for loss in loss_dict.values())\n        \n        loss_value = losses.item()\n        \n        train_loss_list.append(loss_value)\n        \n        losses.backward()\n        optimizer.step()\n        train_itr += 1\n    \n        # update the loss value beside the progress bar for each iteration\n        prog_bar.set_description(desc=f\"Loss: {loss_value:.4f}\")\n        \n    return train_loss_list\n\ndef validate(test_dataloader, model):\n    print('Validating')\n    global val_itr\n    global val_loss_list\n    \n    # initialize tqdm progress bar\n    prog_bar = tqdm(test_dataloader, total=len(test_dataloader))\n    model.train()\n    for i, data in enumerate(prog_bar):\n        batch, _, bounding_boxes = data\n        \n        batch = list(image.to(DEVICE) for image in batch)\n        #the faster R_CNN model takes as targets a list of dictionnary with boxe and label\n        targets = [{'boxes': bounding_boxe.unsqueeze(0).to(DEVICE), 'labels': torch.ones(1, dtype = torch.int64).to(DEVICE)} for bounding_boxe in bounding_boxes]\n        \n        with torch.no_grad():\n            loss_dict = model(batch, targets)\n        #it returns during training a dict containing the classification and regression losses.\n        losses = sum(loss for loss in loss_dict.values())\n        loss_value = losses.item()\n        \n        val_loss_list.append(loss_value)\n        val_itr += 1\n        # update the loss value beside the progress bar for each iteration\n        prog_bar.set_description(desc=f\"Loss: {loss_value:.4f}\")\n        \n    model.eval()\n    prog_bar = tqdm(test_dataloader, total=len(test_dataloader))\n    metric = MAP().to(DEVICE)\n    for i, data in enumerate(prog_bar):\n        batch, _, bounding_boxes = data\n        \n        batch = list(image.to(DEVICE) for image in batch)\n        #the faster R_CNN model takes as targets a list of dictionnary with boxe and label\n        targets = [{'boxes': bounding_boxe.unsqueeze(0).to(DEVICE), 'labels': torch.ones(1, dtype = torch.int64).to(DEVICE)} for bounding_boxe in bounding_boxes]\n        with torch.no_grad():\n            preds = model(batch)\n        metric.update(preds, targets)  \n        \n    \n    return val_loss_list, metric.compute()\n","90595e5b":"import time\n\nplt.rcParams[\"figure.figsize\"] = (5,5)\n\nif TRAIN_MODE:\n    train_set, val_set = torch.utils.data.random_split(waterMeterDataset, [train_size, len(waterMeterDataset) - train_size])\n    train_dataloader = DataLoader(train_set, batch_size=BATCH_SIZE, shuffle=True)\n    test_dataloader = DataLoader(val_set, batch_size=BATCH_SIZE, shuffle=True)\n\n    model = give_model()\n    model = model.to(DEVICE)\n\n    params = [p for p in model.parameters() if p.requires_grad]\n\n    # define the optimizer\n    optimizer = torch.optim.SGD(params, lr=0.0001, momentum=0.8, weight_decay=0.0005)\n\n    train_itr = 1\n    val_itr = 1\n    train_loss_list = []\n    val_loss_list = []\n\n    # name to save the trained model with\n    MODEL_NAME = 'model'\n\n    # create two subplots, one for each, training and validation\n    figure_1, train_ax = plt.subplots()\n    figure_2, valid_ax = plt.subplots()\n\n    # start the training epochs\n    for epoch in range(NB_EPOCHS):\n        print(f\"\\nEPOCH {epoch+1} of {NB_EPOCHS}\")\n\n        # start timer and carry out training and validation\n        start = time.time()\n\n        train_loss = train(train_dataloader, model, optimizer)\n        val_loss, metric_compute = validate(test_dataloader, model)\n\n        end = time.time()\n        print(f\"Took {((end - start) \/ 60):.3f} minutes for epoch {epoch}\")\n        print(\"metrics\", metric_compute)\n\n    torch.save(model.state_dict(), \".\/\"+ MODEL_NAME + \".pth\")\n    print('SAVING MODEL COMPLETE...\\n')\n\n    train_ax.plot(train_loss_list, color='blue')\n    train_ax.set_xlabel('iterations')\n    train_ax.set_ylabel('train loss')\n    valid_ax.plot(val_loss_list, color='red')\n    valid_ax.set_xlabel('iterations')\n    valid_ax.set_ylabel('validation loss')\n    plt.show()","bc7d2556":"\nimport numpy as np\nimport cv2\n\nif not TRAIN_MODE:\n    model = model.load_state_dict(torch.load(MODEL_PATH, map_location=device))\n\nmodel = model.to(DEVICE)\nmodel.eval()\nprint('set model to evaluation mode')","578ad74e":"from torchvision.utils import draw_bounding_boxes\nimport torch\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nimport torchvision.transforms.functional as F\n\n\nplt.rcParams[\"savefig.bbox\"] = 'tight'\nplt.rcParams[\"figure.figsize\"] = (20,80)\n\ndetection_threshold = 0.1\n\n\ndef show(imgs):\n    if not isinstance(imgs, list):\n        imgs = [imgs]\n    fix, axs = plt.subplots(ncols=len(imgs), squeeze=False)\n    for i, img in enumerate(imgs):\n        img = img.detach()\n        img = F.to_pil_image(img)\n        axs[0, i].imshow(np.asarray(img))\n        axs[0, i].set(xticklabels=[], yticklabels=[], xticks=[], yticks=[])\n\nloader = DataLoader(waterMeterDataset, batch_size=8, shuffle=True)\nimg, _, bounding_boxes  = next(iter(loader))\nimg = img.to(DEVICE)\n\nmodel.eval()\nwith torch.no_grad():\n    outputs = model(img)\n\nimg = (img.cpu()*255).type(torch.uint8)\n#images = img.cpu().data.numpy()\n#load next operations to cpu\n#outputs = [{k: v.to('cpu') for k, v in t.items()} for t in outputs]\nimages = [\n    draw_bounding_boxes(image, boxes=output['boxes'][output['scores'] > detection_threshold], width=4)\n    for image, output in zip(img, outputs)\n]\nshow(images)","c1415d27":"## Inference","90ae09b7":"## The dataset\n\n**Dataset:** I used the following Kaggle repository: https:\/\/www.kaggle.com\/tapakah68\/yandextoloka-water-meters-dataset.\n\n\nI created a customed image dataset following the PyTorch's guide https:\/\/pytorch.org\/tutorials\/beginner\/basics\/data_tutorial.html.\n\n**Here,** we only one class of interest (the water meters), so we won't have to deal with multiclass dataset. So throughout the rest of my work, there will be only 2 classes: 'background' and 'water_meter'. ","b9daa463":"# Water Meters Detection\n\nI was asked to build a water meters detection model on the TolokaWaterMeters dataset (https:\/\/toloka.ai\/datasets).\n\n**Requirements:**\n* Neural Network based model.\n* At test time, the algorithm should work without GPU.\n* Provide the evaluation of my results. The accuracy doesn't have to be pushed up that high.\n* The code has to be flexible enough to train an image detector on a different dataset and be written in a way to allow anyone to use it. It has to be as easy as possible for anyone to use it.\n* Should be less than 4 hours of work.","7a9a0060":"First, import the model or reuse the model used in training. Then I used two different methods to show the resulting bounding boxes.","1948c3d7":"## Configuration","eb419be2":"\n## Evaluation\n\nThe dataset is really small (1244), I enriched it with a random color jitter but it is not huge enough to use a proper train-test-validation split. So I will only evaluate it with a random train\/test split. ","fb616aad":"**Show images**","d6ced30a":"**Transformation :** I applied the normalization to stitch with the ResNet normalization on real-world images.","9b81c77b":"## The model\nI will keep it as simple as possible and use the Torchvision's implementation of Faster-RCNN with a pretrained ResNet-50-FPN backbone.","5094348a":"## Libraries import"}}