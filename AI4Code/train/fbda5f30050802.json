{"cell_type":{"e27030bb":"code","161206db":"code","9276fd32":"code","b6551632":"code","fb3715ca":"code","8b139acb":"code","4b59c734":"code","5f21925d":"code","1602731e":"code","0f5542be":"code","777bcbd8":"code","9c90083e":"code","dd27386a":"code","ce585f22":"code","477391bd":"code","0bf93c96":"code","228007ac":"code","76d72c09":"code","38bf0040":"code","4c572599":"code","befd93b3":"code","08a2deda":"code","abef323c":"code","bec4c033":"code","62d80b1e":"code","df6b4dd3":"markdown","4f6ea653":"markdown","0e555f35":"markdown","d5309cd8":"markdown","aa09d1b3":"markdown","0ff5e6c4":"markdown","fba91421":"markdown","cb85bd7e":"markdown","9a81dcbe":"markdown","5a13b492":"markdown","bea75f1c":"markdown","8c4a7043":"markdown","0c7cfef5":"markdown","01662ea3":"markdown"},"source":{"e27030bb":"import pandas as pd\nimport numpy as np\nfrom nltk.stem import WordNetLemmatizer\nimport nltk\nfrom nltk.corpus import stopwords \nfrom nltk.tokenize import word_tokenize \nimport re\nimport spacy\nfrom nltk.corpus import sentiwordnet as swn\nfrom IPython.display import clear_output\nimport plotly.express as px\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom wordcloud import WordCloud\nimport plotly\nplotly.offline.init_notebook_mode (connected = True)","161206db":"data=pd.read_csv('..\/input\/imdb-movie-reviews-dataset\/movie_data.csv')","9276fd32":"data.head(5)","b6551632":"# Replacing Positive -> 1 and Negative -> 0\n\ndata.replace({\"positive\":1,\"negative\":0},inplace=True)","fb3715ca":"#Edits After Removing Stopwords\nEdited_Review = data['review'].copy()\ndata['Review_without_stopwords'] = Edited_Review","8b139acb":"data.head(5)","4b59c734":"# Function to preprocess Reviews data\ndef preprocess_Reviews_data(data,name):\n    # Proprocessing the data\n    data[name]=data[name].str.lower()\n    # Code to remove the Hashtags from the text\n    data[name]=data[name].apply(lambda x:re.sub(r'\\B#\\S+','',x))\n    # Code to remove the links from the text\n    data[name]=data[name].apply(lambda x:re.sub(r\"http\\S+\", \"\", x))\n    # Code to remove the Special characters from the text \n    data[name]=data[name].apply(lambda x:' '.join(re.findall(r'\\w+', x)))\n    # Code to substitute the multiple spaces with single spaces\n    data[name]=data[name].apply(lambda x:re.sub(r'\\s+', ' ', x, flags=re.I))\n    # Code to remove all the single characters in the text\n    data[name]=data[name].apply(lambda x:re.sub(r'\\s+[a-zA-Z]\\s+', '', x))\n    # Remove the twitter handlers\n    data[name]=data[name].apply(lambda x:re.sub('@[^\\s]+','',x))\n\n# Function to tokenize and remove the stopwords    \ndef rem_stopwords_tokenize(data,name):\n      \n    def getting(sen):\n        example_sent = sen\n        \n        filtered_sentence = [] \n\n        stop_words = set(stopwords.words('english')) \n\n        word_tokens = word_tokenize(example_sent) \n        \n        filtered_sentence = [w for w in word_tokens if not w in stop_words] \n        \n        return filtered_sentence\n    # Using \"getting(sen)\" function to append edited sentence to data\n    x=[]\n    for i in data[name].values:\n        x.append(getting(i))\n    data[name]=x\n","5f21925d":"lemmatizer = WordNetLemmatizer()\ndef Lemmatization(data,name):\n    def getting2(sen):\n        \n        example = sen\n        output_sentence =[]\n        word_tokens2 = word_tokenize(example)\n        lemmatized_output = [lemmatizer.lemmatize(w) for w in word_tokens2]\n        \n        # Remove characters which have length less than 2  \n        without_single_chr = [word for word in lemmatized_output if len(word) > 2]\n        # Remove numbers\n        cleaned_data_title = [word for word in without_single_chr if not word.isnumeric()]\n        \n        return cleaned_data_title\n    # Using \"getting2(sen)\" function to append edited sentence to data\n    x=[]\n    for i in data[name].values:\n        x.append(getting2(i))\n    data[name]=x","1602731e":"def make_sentences(data,name):\n    data[name]=data[name].apply(lambda x:' '.join([i+' ' for i in x]))\n    # Removing double spaces if created\n    data[name]=data[name].apply(lambda x:re.sub(r'\\s+', ' ', x, flags=re.I))","0f5542be":"# Using the preprocessing function to preprocess the hotel data\npreprocess_Reviews_data(data,'Review_without_stopwords')\n# Using tokenizer and removing the stopwords\nrem_stopwords_tokenize(data,'Review_without_stopwords')\n# Converting all the texts back to sentences\nmake_sentences(data,'Review_without_stopwords')\n\n#Edits After Lemmatization\nfinal_Edit = data['Review_without_stopwords'].copy()\ndata[\"After_lemmatization\"] = final_Edit\n\n# Using the Lemmatization function to lemmatize the hotel data\nLemmatization(data,'After_lemmatization')\n# Converting all the texts back to sentences\nmake_sentences(data,'After_lemmatization')","777bcbd8":"data.head(6)","9c90083e":"import nltk\nimport ssl\nfrom nltk.corpus import wordnet as wn\nfrom nltk.corpus import sentiwordnet as swn\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.corpus import stopwords","dd27386a":"pos=neg=obj=count=0\n\npostagging = []\n\nfor review in data['After_lemmatization']:\n    list = word_tokenize(review)\n    postagging.append(nltk.pos_tag(list))\n\ndata['pos_tags'] = postagging\n\ndef penn_to_wn(tag):\n    if tag.startswith('J'):\n        return wn.ADJ\n    elif tag.startswith('N'):\n        return wn.NOUN\n    elif tag.startswith('R'):\n        return wn.ADV\n    elif tag.startswith('V'):\n        return wn.VERB\n    return None\n\n\n# Returns list of pos-neg and objective score. But returns empty list if not present in senti wordnet.\ndef get_sentiment(word,tag):\n    wn_tag = penn_to_wn(tag)\n    \n    if wn_tag not in (wn.NOUN, wn.ADJ, wn.ADV):\n        return []\n\n    #Lemmatization\n    lemma = lemmatizer.lemmatize(word, pos=wn_tag)\n    if not lemma:\n        return []\n\n    #Synset is a special kind of a simple interface that is present in NLTK to look up words in WordNet. \n    #Synset instances are the groupings of synonymous words that express the same concept. \n    #Some of the words have only one Synset and some have several.\n    synsets = wn.synsets(word, pos=wn_tag)\n    if not synsets:\n        return []\n\n    # Take the first sense, the most common\n    synset = synsets[0]\n    swn_synset = swn.senti_synset(synset.name())\n\n    return [synset.name(), swn_synset.pos_score(),swn_synset.neg_score(),swn_synset.obj_score()]\n\n    pos=neg=obj=count=0\n    \n    ###################################################################################\nsenti_score = []\n\nfor pos_val in data['pos_tags']:\n    senti_val = [get_sentiment(x,y) for (x,y) in pos_val]\n    for score in senti_val:\n        try:\n            pos = pos + score[1]  #positive score is stored at 2nd position\n            neg = neg + score[2]  #negative score is stored at 3rd position\n        except:\n            continue\n    senti_score.append(pos - neg)\n    pos=neg=0    \n    \ndata['senti_score'] = senti_score\nprint(data['senti_score'])\n\nprint(data.head)","ce585f22":"overall=[]\nfor i in range(len(data)):\n    if data['senti_score'][i]>= 0.05:\n        overall.append('Positive')\n    elif data['senti_score'][i]<= -0.05:\n        overall.append('Negative')\n    else:\n        overall.append('Neutral')\ndata['Overall Sentiment']=overall","477391bd":"data.head(10)","0bf93c96":"import seaborn as sns\nsns.countplot(data['Overall Sentiment'])","228007ac":"data['reviews_text_new'] = data['After_lemmatization'].copy()","76d72c09":"# The following code creates a word-document matrix.\nfrom sklearn.feature_extraction.text import CountVectorizer\n\nvec = CountVectorizer()\nX = vec.fit_transform(data['reviews_text_new'])\ndf = pd.DataFrame(X.toarray(), columns = vec.get_feature_names())\ndf.head(3)","38bf0040":"vect = CountVectorizer()\nvect.fit(data['reviews_text_new'])\nvect.get_feature_names()\n# transform training data into a 'document-term matrix'\nsimple_train_dtm = vect.transform(data['reviews_text_new'])\nprint(simple_train_dtm)","4c572599":"### Creating a python object of the class CountVectorizer\nfrom sklearn.feature_extraction.text import CountVectorizer\nbow_counts = CountVectorizer(tokenizer= word_tokenize, # type of tokenization\n                             ngram_range=(1,3)) # number of n-grams\n\nbow_data = bow_counts.fit_transform(data['reviews_text_new'])","befd93b3":"from sklearn.model_selection import train_test_split\nX_train_bow, X_test_bow, y_train_bow, y_test_bow = train_test_split(bow_data, # Features\n                                                                    data['Overall Sentiment'], # Target variable\n                                                                    test_size = 0.2, # 20% test size\n                                                                    random_state = 0) # random","08a2deda":"from sklearn.linear_model import LogisticRegression\n### Training the model \nlr_model_all = LogisticRegression() # Logistic regression\nlr_model_all.fit(X_train_bow, y_train_bow) # Fitting a logistic regression model\n\n## Predicting the output\ntest_pred_lr_all = lr_model_all.predict(X_test_bow) # Class prediction\n\n\n## Calculate key performance metrics\n\nfrom sklearn.metrics import classification_report\n# Print a classification report\nprint(classification_report(y_test_bow,test_pred_lr_all))","abef323c":"from sklearn.feature_extraction.text import TfidfVectorizer\nnoise_words = []\n### Creating a python object of the class CountVectorizer\ntfidf_counts = TfidfVectorizer(tokenizer= word_tokenize, # type of tokenization\n                               stop_words=noise_words, # List of stopwords\n                               ngram_range=(1,1)) # number of n-grams\n\ntfidf_data = tfidf_counts.fit_transform(data['reviews_text_new'])","bec4c033":"X_train_tfidf, X_test_tfidf, y_train_tfidf, y_test_tfidf = train_test_split(tfidf_data,\n                                                                            data['Overall Sentiment'],\n                                                                            test_size = 0.2,\n                                                                            random_state = 0)","62d80b1e":"### Setting up the model class\nlr_model_tf_idf = LogisticRegression()\n\n## Training the model \nlr_model_tf_idf.fit(X_train_tfidf,y_train_tfidf)\n\n## Prediciting the results\ntest_pred_lr_all = lr_model_tf_idf.predict(X_test_tfidf)\n\n## Calculate key performance metrics\n\n\n# Print a classification report\nprint(classification_report(y_test_tfidf,test_pred_lr_all))","df6b4dd3":"# Having a look at 1st five reviews in the data","4f6ea653":"# Making two copies of Reviews to edit","0e555f35":"So when we are working with sentiwordnet we need to know the characterstic of the word for which we want to know the sentiment . So for finding that position of the word here we are gonna use nltk which tells us about the position of the word which then is used to get the sentiment using the sentiwordnet . We then average out the score for both the positive and the negative score from the whole sentence .\nThe positions compatible with the sentiwordnet are:\n* n - NOUN\n* v - VERB\n* a - ADJECTIVE\n* s - ADJECTIVE SATELLITE\n* r - ADVERB","d5309cd8":"# Divide into training and test sets","aa09d1b3":"# Preprocessing The Data","0ff5e6c4":"# Importing The Packages","fba91421":"# TF-IDF model","cb85bd7e":"# Results of Preprocessing data (Removing stopwords & Lemmatization)","9a81dcbe":"# Lemmatization Function","5a13b492":"# Importing The Data","bea75f1c":"# Converting all the texts back to sentences","8c4a7043":"# Applying logistic regression","0c7cfef5":"# Bag-of-words","01662ea3":"# Building a machine learning model"}}