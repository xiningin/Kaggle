{"cell_type":{"9c4cea59":"code","6a21f32f":"code","2e8e4762":"code","76f8acdb":"code","da62d732":"code","16628bda":"code","51131f6d":"code","15ddba3a":"code","8fb30f97":"code","57264bb1":"code","e9988ae2":"code","7143625b":"code","94fdbdb0":"code","ff7a5d49":"code","58be7bc1":"code","58852875":"code","bd553dca":"code","3b9055b7":"code","e62aa260":"code","708cec42":"code","6b5b3df4":"code","535f618a":"code","0986873b":"code","800c1ded":"code","e7898158":"code","c97df7b6":"code","b5857696":"code","76aca5c6":"code","296c6658":"code","a63ec403":"code","61177d4a":"code","e28b4721":"code","0416348d":"code","e817294b":"code","1cd9fc6a":"code","fd66a726":"code","46f37062":"code","f6f1d2ab":"code","e7f7ddd0":"code","b15f6680":"code","0720431e":"code","c2b6470d":"code","13fdd6d8":"code","de71e44f":"code","d27001ef":"code","9d79ad08":"code","6e4ae5c7":"code","cdc24645":"code","0d695bd3":"code","a37a3054":"code","40ea8699":"code","c0dadd09":"code","e5e49ccc":"code","314c6fb5":"code","ee952f82":"code","467efe7b":"code","e24b07ff":"code","77e87204":"code","7c9a13bb":"code","74a78ece":"code","58d77798":"markdown","327151f4":"markdown","2aaed257":"markdown","90efb404":"markdown","9a04258b":"markdown","05f2d52e":"markdown","1ea0e50b":"markdown","78e43fd6":"markdown","8cab1edf":"markdown","74307476":"markdown","9edbb72b":"markdown","c486bfac":"markdown"},"source":{"9c4cea59":"import numpy as np # linear algebra\nimport pandas as pd","6a21f32f":"df1 = pd.read_csv(\"\/kaggle\/input\/regression-with-neural-networking\/concrete_data.csv\")","2e8e4762":"df1.info()","76f8acdb":"## we can observe that there is no null value in the above data set \n## Total 8 Features","da62d732":"from matplotlib import pyplot as plt\nimport seaborn as sns\nsns.pairplot(df1,diag_kind='kde')","16628bda":"\ndf1.skew()","51131f6d":"## we can observe that data is highly skewed . Lets remove the skewness using power transformer \n\nfrom sklearn.preprocessing import PowerTransformer\npt = PowerTransformer()\nnormalized_age = pt.fit_transform(df1[['Age']])\n","15ddba3a":"# plotting the pair plot after removing the skewness of dataset using powertransformer , we can observe powertransformer helped alot in removing the skewness\nsns.pairplot(pd.DataFrame(pt.fit_transform(df1)),diag_kind='kde')","8fb30f97":"import seaborn as sns\nsns.heatmap(df1.corr(),annot=True)","57264bb1":"#!pip install dtale","e9988ae2":"# import dtale\n# import dtale.app as dtale_app","7143625b":"# dtale_app.USE_COLAB = True","94fdbdb0":"# dtale.show(df1)\n#d.open_browser()","ff7a5d49":"# dtale.instances()","58be7bc1":"# for i in range(1,9,1) : \n#   dtale.get_instance(i).kill()","58852875":"# dtale.instances()","bd553dca":"df1.columns","3b9055b7":"#! pip install AutoViz","e62aa260":"# from autoviz.AutoViz_Class import AutoViz_Class\n# AV = AutoViz_Class()\n# df = AV.AutoViz(\"\/kaggle\/input\/regression-with-neural-networking\/concrete_data.csv\")","708cec42":"from sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.preprocessing import StandardScaler","6b5b3df4":"X = df1.drop('Strength',axis= 1)\ny = df1[['Strength']]","535f618a":"Xtrain,Xtest,ytrain,ytest = train_test_split(X,y,test_size = 0.20,random_state = 10)\nlr = LinearRegression()","0986873b":"lr.fit(Xtrain,ytrain)","800c1ded":"df1.shape","e7898158":"print('TrainingR2')\nprint(lr.score(Xtrain,ytrain))\nprint('TestingR2')\nprint(lr.score(Xtest,ytest))","c97df7b6":"df1.shape","b5857696":"X = df1.drop('Strength',axis=1)\ny = df1[['Strength']]","76aca5c6":"Xtrain,Xtest,ytrain,ytest = train_test_split (X,y,test_size = 0.20,random_state = 10)","296c6658":"from sklearn.preprocessing import StandardScaler\nsc = StandardScaler()","a63ec403":"# Dont scale the Y()\nscaledXtrain = sc.fit_transform(Xtrain)\nscaledXtest = sc.transform(Xtest)","61177d4a":"lr = LinearRegression()","e28b4721":"lr.fit(scaledXtrain,ytrain)","0416348d":"print('Train R2')\nprint(lr.score(scaledXtrain,ytrain))\nprint('TestR2')\nprint(lr.score(scaledXtest,ytest))","e817294b":"lr.coef_","1cd9fc6a":"from sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import cross_val_score","fd66a726":"X = df1.drop('Strength',axis= 1)\ny = df1[['Strength']]","46f37062":"Xtrain,Xtest,ytrain,ytest = train_test_split(X,y,test_size = 0.20,random_state = 10)","f6f1d2ab":"pipe = Pipeline((\n    \n\n    ('pt',PowerTransformer()),\n    ('lr',LinearRegression()),\n))","e7f7ddd0":"pipe.fit(Xtrain,ytrain)","b15f6680":"print('Train R2 Score')\nprint(pipe.score(Xtrain,ytrain))\nprint('Test R2 Score')\nprint(pipe.score(Xtest,ytest))","0720431e":"scoredt = cross_val_score(pipe,Xtrain,ytrain,cv=10)\nprint(scoredt)","c2b6470d":"print('AverageR2')\nprint(np.mean(scoredt))","13fdd6d8":"df1.shape","de71e44f":"X = df1.drop('Strength',axis = 1)\ny = df1[['Strength']]","d27001ef":"Xtrain,Xtest,ytrain,ytest = train_test_split(X,y,test_size = 0.20,random_state = 10)","9d79ad08":"from sklearn.linear_model import Lasso,Ridge","6e4ae5c7":"pipe =  Pipeline((\n    \n  ('pt',PowerTransformer()),\n  ('ls',Lasso()),\n\n\n))","cdc24645":"pipe.fit(Xtrain,ytrain)","0d695bd3":"print('Training R2')\nprint(pipe.score(Xtrain,ytrain))\nprint('TestR2')\nprint(pipe.score(Xtest,ytest))","a37a3054":"scoresdt = cross_val_score(pipe,Xtrain,ytrain,cv=10)\nprint(scoresdt)\nprint('average')\nprint(np.mean(scoresdt))","40ea8699":"X = df1.drop('Strength',axis = 1)\ny = df1[['Strength']]\nXtrain,Xtest,ytrain,ytest = train_test_split(X,y,test_size = 0.20,random_state = 10)","c0dadd09":"pipe = Pipeline((\n    ('pt',PowerTransformer()),\n    ('Poly',PolynomialFeatures(degree=2)),\n    ('lr',LinearRegression()),\n\n\n))","e5e49ccc":"pipe.fit(Xtrain,ytrain)\nprint(\"Training R2\")\nprint(pipe.score(Xtrain,ytrain))\nprint(\"Testing R2\")\nprint(pipe.score(Xtest,ytest))\nscoresdt = cross_val_score(pipe,Xtrain,ytrain,cv=10)\nprint(scoresdt)\nprint(\"Average R2\")\nprint(np.mean(scoresdt))","314c6fb5":"X = df1.drop('Strength',axis = 1)\ny = df1[['Strength']]\n\n################\n\nXtrain,Xtest,ytrain,ytest = train_test_split(X,y,test_size = 0.20,random_state = 10)\npipe = Pipeline((\n    ('pt',PowerTransformer()),\n    ('Poly',PolynomialFeatures(degree=3)),\n    ('lr',LinearRegression()),\n\n\n))\n\n################\n\npipe.fit(Xtrain,ytrain)\nprint(\"Training R2\")\nprint(pipe.score(Xtrain,ytrain))\nprint(\"Testing R2\")\nprint(pipe.score(Xtest,ytest))\nscoresdt = cross_val_score(pipe,Xtrain,ytrain,cv=10)\nprint(scoresdt)\nprint(\"Average R2\")\nprint(np.mean(scoresdt))\n\n","ee952f82":"from sklearn.feature_selection import RFE","467efe7b":"X = df1.drop('Strength',axis = 1)\ny = df1[['Strength']]","e24b07ff":"Xtrain,Xtest,ytrain,ytest = train_test_split(X,y,test_size = 0.20,random_state = 10)    ","77e87204":"Xtrain,Xtest,ytrain,ytest = train_test_split(X,y,test_size = 0.20,random_state = 10)\npipe = Pipeline((\n    ('pt',PowerTransformer()),\n    ('Poly',PolynomialFeatures(degree=3)),\n    ('rfe',RFE(estimator=LinearRegression(),n_features_to_select=70,step=1,verbose=0)),\n    ('lr',LinearRegression()),\n\n\n))","7c9a13bb":"pipe.fit(Xtrain,ytrain)\nprint(\"Training R2\")\nprint(pipe.score(Xtrain,ytrain))\nprint(\"Testing R2\")\nprint(pipe.score(Xtest,ytest))\nscoresdt = cross_val_score(pipe,Xtrain,ytrain,cv=10)\nprint(scoresdt)\nprint(\"Average R2\")\nprint(np.mean(scoresdt))","74a78ece":"## Please do provide your feedback by adding comments as this is my first kaggle notebook","58d77798":"### Third Model ( USING PIPELINE ) ","327151f4":"## We can observe that we can get accuracy of maximum 94.09 % using basic linear models","2aaed257":"### Basic Linear Model","90efb404":"### Second Model","9a04258b":"### Finding the Co-relation between the features ","05f2d52e":"### Use D-Tale for Exploratory analysis","1ea0e50b":"## Lets Try Polynimals of features","78e43fd6":"### we can observe that features are not much co-related with each other . We need co related features for linear models ( we might try with polynomial features later)","8cab1edf":"### Checking the Skewness of features of dataset","74307476":"###  Fourth Model with Lasso Regularisation","9edbb72b":"### We can observe that Degree 2 is showing better results than Degree=3 Polynomials\n","c486bfac":"### Lets only choose important Co related features only using RFE in pipeline "}}