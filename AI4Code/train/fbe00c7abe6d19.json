{"cell_type":{"b00f5fe5":"code","6d23c54b":"code","17fa474d":"code","80d9b06c":"code","605e8e75":"code","cf867424":"code","cc528284":"code","1996a932":"code","91340f61":"code","1c277b1c":"code","af158e35":"code","9444b9b0":"code","9cc7af93":"code","ca7b74bd":"code","58cd34d8":"code","d216cbb2":"code","8dee8fe4":"code","1c374b27":"code","05538ce7":"code","84a862aa":"code","99750887":"markdown","c5ee7cc8":"markdown","792b8c60":"markdown","ab632be6":"markdown","785162d9":"markdown","a24c66c9":"markdown","982d3620":"markdown","0ad22523":"markdown","ef79a3f8":"markdown","a527f69a":"markdown","6d9f6c48":"markdown","fa564ae2":"markdown","1e84889c":"markdown"},"source":{"b00f5fe5":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nimport os\nimport sys\nfrom tqdm import tqdm, tqdm_notebook\nimport glob\nimport shutil\nimport time      # time.perf_counter()\nimport random\n\nimport matplotlib.pyplot as plt\nimport cv2\n\nimport xml.etree.ElementTree as ET\n\nfrom keras.layers import Input, Dense, Reshape, Flatten\nfrom keras.layers import BatchNormalization, Activation\nfrom keras.layers.advanced_activations import LeakyReLU\nfrom keras.layers.convolutional import UpSampling2D, Conv2D\nfrom keras.models import Model\nfrom keras import backend as K\nfrom keras.optimizers import Adam\nfrom keras.preprocessing.image import image","6d23c54b":"print(os.listdir(\"..\/input\"))","17fa474d":"kernel_start = time.perf_counter()\nkernel_time_limit = 60*60*8.5        #### running time\n\n# image size\nimg_size = 64\nchannels = 3\nimg_shape = (img_size, img_size, channels)    # (64,64,3)\n\n# z(latent variable) size\nz_dim = 100\nz_shape = (z_dim,)\n\n# gradient penalty coefficient \"\u03bb\"\npenaltyLambda = 10    # d_loss = f_loss - r_loss + \u03bb\uff65penalty\n\n# critic(discriminator) iterations per generator iteration\ntrainRatio = 5\n\nbatch_size = 32        # 16 or 64 better?\nrec_interval = 10000\n\nDIR = os.getcwd()\nDIRimg = \"..\/input\/all-dogs\/all-dogs\"\nDIRanno = \"..\/input\/annotation\/Annotation\"\nDIRout = \"..\/output_images\"","80d9b06c":"def loadImage(fPath, resize = True):\n    img = cv2.imread(fPath)\n    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)      # BGR to RGB\n    if resize:\n        xmin,ymin,xmax,ymax = clipImage(fPath)      # clip to square\n        if xmin >= 0:                               # exist Annotation\n            img = img[ymin:ymax, xmin:xmax, :]      # [h,w,c]\n        # Interpolation method\n        if xmax - xmin > img_size:\n            interpolation = cv2.INTER_AREA          # shrink\n        else:\n            interpolation = cv2.INTER_CUBIC         # expantion\n        img = cv2.resize(img, (img_size, img_size),\n                    interpolation = interpolation)  # resize\n    return img","605e8e75":"def clipImage(fPath):\n    imgName = os.path.basename(fPath)[:-4].split(\"_\")\n    breed = imgName[0]\n    dog = imgName[1]\n    path = glob.glob(os.path.join(DIRanno, breed + \"*\", breed +\"_\" + dog))\n    if len(path) > 0:\n        tree = ET.parse(path[0])\n        root = tree.getroot()    # get <annotation>\n        size = root.find('size')\n        width = int(size.find('width').text)\n        height = int(size.find('height').text)\n#        objects = root.findall('object')   # ToDo: correspond multi objects\n#        for object in objects:\n        object = root.find('object')\n        bndbox = object.find('bndbox') \n        xmin = int(bndbox.find('xmin').text)\n        ymin = int(bndbox.find('ymin').text)\n        xmax = int(bndbox.find('xmax').text)\n        ymax = int(bndbox.find('ymax').text)\n\n        xmin = max(0, xmin - 4)        # 4 : margin\n        xmax = min(width, xmax + 4)\n        ymin = max(0, ymin - 4)\n        ymax = min(height, ymax + 4)\n\n        w = max(xmax - xmin, ymax - ymin, img_size)   # ideal w\n        w = min(w, width, height)                     # available w\n    \n        if w > xmax - xmin:\n            xmin = min(max(0, xmin - int((w - (xmax - xmin))\/2)), width - w)\n            xmax = xmin + w\n        if w > ymax - ymin:\n            ymin = min(max(0, ymin - int((w - (ymax - ymin))\/2)), height - w)\n            ymax = ymin + w\n\n    else:\n        xmin = -1; ymin = -1; xmax = -1; ymax = -1;     # no annotation ?\n        \n    return xmin,ymin,xmax,ymax","cf867424":"all_fNames = os.listdir(DIRimg)\n\n# image sample\nsample_ids = random.sample(range(len(all_fNames)), 9)\nfig, axes = plt.subplots(nrows=3, ncols=3, figsize=(12,10))\nfor i, axis in enumerate(axes.flatten()):\n    img = loadImage(os.path.join(DIRimg,\n                    all_fNames[sample_ids[i]]), resize=False)\n    imgplot = axis.imshow(img)\n    axis.set_title(all_fNames[sample_ids[i]])\n    axis.set_axis_off()\nplt.tight_layout()","cc528284":"# train data\nx_train = np.zeros((len(all_fNames),img_size,img_size,3))\nfor i in tqdm(range(len(all_fNames))):\n    path = os.path.join(DIRimg, all_fNames[i])\n    x_train[i] = loadImage(path)\n\nx_train = x_train \/ 255.\nprint(x_train.shape)","1996a932":"fig, axes = plt.subplots(nrows=3, ncols=3, figsize=(12,10))\nfor indx, axis in enumerate(axes.flatten()):\n    img = image.array_to_img(x_train[sample_ids[indx]])    # ndarray \u2192 PIL\n    imgplot = axis.imshow(img)\n    axis.set_title(all_fNames[sample_ids[indx]])\n    axis.set_axis_off()\nplt.tight_layout()","91340f61":"def build_generator():\n    input = Input(shape=z_shape)\n    x = Dense(8*img_size*img_size, activation=\"relu\")(input)\n    x = Reshape((img_size\/\/8, img_size\/\/8, -1))(x)\n    x = UpSampling2D((2, 2))(x)\n    x = Conv2D(64*4, kernel_size=3, strides=1, padding=\"same\",\n                   use_bias=False)(x)\n    x = BatchNormalization(momentum=0.9, epsilon=1e-5)(x, training=1)\n    x = Conv2D(64*4, kernel_size=3, strides=1, padding=\"same\", use_bias=False)(x)\n    x = BatchNormalization(momentum=0.9, epsilon=1e-5, )(x, training=1)\n    x = Activation(\"relu\")(x)\n    x = Conv2D(64*4, kernel_size=3, strides=1, padding=\"same\", use_bias=False)(x)\n    x = BatchNormalization(momentum=0.9, epsilon=1e-5)(x, training=1)\n    x = Activation(\"relu\")(x)\n    x = UpSampling2D((2, 2))(x)\n    x = Conv2D(64*2, kernel_size=3, strides=1, padding=\"same\",\n               use_bias=False)(x)\n    x = BatchNormalization(momentum=0.9, epsilon=1e-5)(x, training=1)\n    x = Conv2D(64*2, kernel_size=3, strides=1, padding=\"same\", use_bias=False)(x)\n    x = BatchNormalization(momentum=0.9, epsilon=1e-5, )(x, training=1)\n    x = Activation(\"relu\")(x)\n    x = Conv2D(64*2, kernel_size=3, strides=1, padding=\"same\", use_bias=False)(x)\n    x = BatchNormalization(momentum=0.9, epsilon=1e-5)(x, training=1)\n    x = Activation(\"relu\")(x)\n    x = UpSampling2D((2, 2))(x)\n    x = Conv2D(64*1, kernel_size=3, strides=1, padding=\"same\",\n                   use_bias=False)(x)\n    x = BatchNormalization(momentum=0.9, epsilon=1e-5)(x,training=1)\n    x = Conv2D(64*1, kernel_size=3, strides=1, padding=\"same\", use_bias=False)(x)\n    x = BatchNormalization(momentum=0.9, epsilon=1e-5, )(x, training=1)\n    x = Activation(\"relu\")(x)\n    x = Conv2D(64*1, kernel_size=3, strides=1, padding=\"same\", use_bias=False)(x)\n    x = BatchNormalization(momentum=0.9, epsilon=1e-5)(x, training=1)\n    x = Activation(\"relu\")(x)\n    x = Conv2D(3, kernel_size=3, strides=1, padding=\"same\", activation=\"tanh\",\n               use_bias=False,)(x)\n\n    model = Model(input, x)\n    print(\"\u25cfgenerator\")\n    model.summary()\n    return model","1c277b1c":"def build_discriminator():\n    input = Input(shape=img_shape)\n    x = Conv2D(64, kernel_size=4, strides=2, padding=\"same\", use_bias=False)(input)\n    x = LeakyReLU(0.2)(x)\n    x = Conv2D(128, kernel_size=4, strides=2, padding=\"same\", use_bias=False)(x)\n    x = LeakyReLU(0.2)(x)\n    x = Conv2D(256, kernel_size=4, strides=2, padding=\"same\", use_bias=False)(x)\n    x = Conv2D(256, kernel_size=3, strides=1, padding=\"same\", use_bias=False)(x)\n    x = LeakyReLU(0.2)(x)\n    x = Conv2D(256, kernel_size=3, strides=1, padding=\"same\", use_bias=False)(x)\n    x = LeakyReLU(0.2)(x)\n    x = Conv2D(512, kernel_size=4, strides=2, padding=\"same\", use_bias=False)(x)\n    x = LeakyReLU(0.2)(x)\n    x = Conv2D(1, kernel_size=1, strides=1, padding=\"same\", use_bias=False)(x)\n    x = Flatten()(x)\n    x = Dense(units=1, activation=None)(x)   # activation = None\n    \n    model = Model(input, x)\n    print(\"\u25cfdiscriminator\")\n    model.summary()\n    return model","af158e35":"def build_WGANgp(generator, discriminator):\n    #### model\n    # generator image(fake image)\n    z = Input(shape=z_shape)\n    f_img = generator(z)\n    f_out = discriminator(f_img)\n    # real image\n    r_img = Input(shape=img_shape)\n    r_out = discriminator(r_img)\n    # average image\n    epsilon = K.placeholder(shape=(None,1,1,1))\n    a_img = Input(shape=(img_shape),\n                  tensor = epsilon * r_img + (1-epsilon) * f_img)\n    a_out = discriminator(a_img)\n\n    #### loss\n    # original critic(discriminator) loss\n    r_loss = K.mean(r_out)\n    f_loss = K.mean(f_out)\n    # gradient penalty  <this is point of WGAN-gp>\n    grad_mixed = K.gradients(a_out, [a_img])[0]\n    norm_grad_mixed = K.sqrt(K.sum(K.square(grad_mixed), axis=[1,2,3]))\n    grad_penalty = K.mean(K.square(norm_grad_mixed -1))\n    penalty = penaltyLambda * grad_penalty\n    # d loss\n    d_loss = f_loss - r_loss + penalty\n    \n    #### discriminator update function\n    d_updates = Adam(lr=1e-4, beta_1=0.5, beta_2=0.9). \\\n                get_updates(discriminator.trainable_weights,[],d_loss)\n    d_train = K.function([r_img, z, epsilon],\n                         [r_loss, f_loss, penalty, d_loss],\n                         d_updates)\n    \n    #### generator update function\n    g_loss = -1. * f_loss\n    g_updates = Adam(lr=1e-4, beta_1=0.5, beta_2=0.9). \\\n                get_updates(generator.trainable_weights,[],g_loss)\n    g_train = K.function([z], [g_loss], g_updates)\n\n    return g_train, d_train","9444b9b0":"# generator Model\ngenerator = build_generator()\n# discriminator Model\ndiscriminator = build_discriminator()\n# WGAN-gp Training Model\nG_train, D_train = build_WGANgp(generator, discriminator)","9cc7af93":"# fixed z for confirmation of generated image\nz_fix = np.random.normal(0, 1, (64, z_dim)) \n\n# list for store learning progress data\ng_loss_list = []\nr_loss_list = []\nf_loss_list = []\nf_r_loss_list = []\npenalty_list = []\nd_loss_list = []\n\n# (0\uff5e1) \u2192 (-1\uff5e+1)\nX_train = (x_train.astype(np.float32) - 0.5) \/ 0.5","ca7b74bd":"def sumple_images(imgs, rows=3, cols=3, figsize=(12,10)):\n    fig, axes = plt.subplots(nrows=rows, ncols=cols, figsize=figsize)\n    for indx, axis in enumerate(axes.flatten()):\n        img = image.array_to_img(imgs[indx])    # ndarray \u2192 PIL\n        imgplot = axis.imshow(img)\n        axis.set_axis_off()\n    plt.tight_layout()","58cd34d8":"iteration = 0\nwhile time.perf_counter() - kernel_start < kernel_time_limit:\n    \n    #### Discriminator\n    for j in range(trainRatio):\n        # Generator in\n        z = np.random.normal(0, 1, (batch_size, z_dim))\n        # Generator out Images\n        f_imgs = generator.predict(z)\n        # Real Images\n        idx = np.random.randint(0, X_train.shape[0], batch_size)\n        r_imgs = X_train[idx]\n        # train the discriminator\n        epsilon = np.random.uniform(size = (batch_size, 1,1,1))\n        r_loss, f_loss, penalty, d_loss = D_train([r_imgs, z, epsilon])\n\n    #### Generator\n    # Generator in\n    z = np.random.normal(0, 1, (batch_size, z_dim))\n    # train the generator\n    g_loss = G_train([z])\n\n    #### Record of learning progress\n    # loss\n    r_loss_list.append(r_loss)\n    f_loss_list.append(f_loss)\n    f_r_loss_list.append(f_loss - r_loss)\n    penalty_list.append(penalty)\n    d_loss_list.append(d_loss)\n    # generated image sumple\n    if (iteration in [100, 1000]) or (iteration % rec_interval == 0):\n        print(f'iteration:{iteration} \/ d_loss:{d_loss:.3f} \/ g_loss:{sum(g_loss)\/len(g_loss):.3f}')\n        g_imgs = generator.predict(z_fix)\n        imgs = g_imgs * 127.5 + 127.5\n        sumple_images(imgs, rows=1, cols=7)\n        plt.show()\n\n    iteration += 1\n    \nprint(\"last iteration:\",iteration - 1)","d216cbb2":"# plot loss\nfig, ax = plt.subplots(5, 2, figsize=(8.27,11.69))\nfor j in range(2):\n    ax[0,j].plot(r_loss_list, label=\"r_los\")\n    ax[1,j].plot(f_loss_list, label=\"f_loss\")\n    ax[2,j].plot(f_r_loss_list, label=\"f-r_loss\")\n    ax[3,j].plot(penalty_list, label=\"penalty\")\n    ax[4,j].plot(d_loss_list, label=\"d_loss\")\nfor i in range(5):\n    ax[i,0].set_xlim([0,200])\n    ax[i,1].set_xlim([200,iteration])\n    for j in range(2):\n        ax[i,j].grid()\n        ax[i,j].legend()\nplt.show()","8dee8fe4":"if os.path.exists(DIRout):\n    shutil.rmtree(DIRout)\nif not os.path.exists(DIRout):\n    os.mkdir(DIRout)","1c374b27":"# generate images for submit\nn = 10000\nbatch = 64\nfor i in tqdm(range(0, n, batch)):\n    z = np.random.normal(0,1,size=(batch, z_dim))\n    g_imgs = generator.predict(z)\n    imgs = g_imgs * 127.5 + 127.5\n    for j in range(batch):\n        img = image.array_to_img(imgs[j])      # ndarray \u2192 PIL \n        img.save(os.path.join(DIRout, 'image_' + str(i+j+1).zfill(5) + '.png'))\n        if i+j+1 == n:\n            break\nprint(len(os.listdir(DIRout)))","05538ce7":"# generated DOGs sumple\nsumple_images(g_imgs, rows=5, cols=7, figsize=(12,8))","84a862aa":"if os.path.exists('images.zip'):\n    os.remove('images.zip')\nshutil.make_archive('images', 'zip', DIRout)\nprint(\"<END>\")","99750887":"## Image samples after clipping","c5ee7cc8":"## Convert Images to Train Data","792b8c60":"## Constants","ab632be6":"## Build Training Model","785162d9":"## WGAN-gp Model","a24c66c9":"## Perform Training","982d3620":"## Submit","0ad22523":"## Image data loading and clipping Function","ef79a3f8":"## WGAN-gp (Wasserstein GAN gradient penalty)\n\u25cf**Reference Paper**<br>\nI.Gulrajani, F.Ahmed,M.Arjovsky, V.Dumoulin and A.Courville, \"***Improved Training of Wasserstein GANs***\" Proc.NIPS 2017,pp.5769-5779(2017)<br>\nhttps:\/\/arxiv.org\/pdf\/1704.00028.pdf<br><br>\n***GAN often suffers from training instability***. The Wasserstein GAN (WGAN) makes progress toward stable training of GANs, but sometimes can still generate only poor samples or fail to converge. These problems are often due to the use of ***weight clipping in WGAN to enforce a Lipschitz constraint on the critic* ***. So, they propose an alternative to clipping weights:***penalize the norm of gradient of the critic*** with respect to its input. They say this method performs better than standard WGAN and enables stable training.<br>\n*critic : WGAN calls the discriminator a critic","a527f69a":"![image.png](attachment:image.png)","6d9f6c48":"### Outline of WGAN-gp","fa564ae2":"## Image samples before clipping","1e84889c":"## Prepare Training"}}