{"cell_type":{"e4c04e1d":"code","8845ab5c":"code","39b014e3":"code","1ce58031":"code","1c7dfa39":"code","23ffe1a9":"code","209fbdfe":"code","60804c83":"code","9504cd58":"code","a51d0af7":"code","a21b96cc":"code","e905f6c5":"code","0c0db177":"code","55a13d9f":"code","3f71cfe2":"code","ae079e8b":"code","15787b65":"code","6908489b":"code","f7f1f15a":"code","796d3e53":"code","bd24f88e":"code","8251dfcb":"code","c9e613ea":"code","f7fb50c4":"markdown","9731a942":"markdown","d82edf55":"markdown","acfc7825":"markdown","afefabd6":"markdown","d7bc20fa":"markdown","89b41481":"markdown","5e363aee":"markdown"},"source":{"e4c04e1d":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport itertools\nfrom itertools import chain\nfrom sklearn.feature_selection import RFE\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LinearRegression, Lasso\nfrom sklearn.model_selection import GridSearchCV, cross_val_score, learning_curve, train_test_split\nfrom sklearn.metrics import precision_score, recall_score, confusion_matrix, roc_curve, precision_recall_curve, accuracy_score,mean_squared_error\nimport keras\nfrom keras.layers import Dense\nfrom keras.models import Sequential\nimport plotly.offline as py\npy.init_notebook_mode(connected=True)\nimport plotly.graph_objs as go\nimport plotly.tools as tls\nimport plotly.figure_factory as ff\n","8845ab5c":"data = pd.read_csv(\"..\/input\/kc_house_data.csv\")","39b014e3":"data.head()","1ce58031":"print(data.shape)","1c7dfa39":"## Column years passed after recent built or renovate is added ##\n\ndata['year_pass'] = 0\ndata['year_pass'] = data.yr_renovated-data.yr_built\ndata.year_pass = pd.Series([x+2018 if x <0 else x for x in data.year_pass])","23ffe1a9":"## Column 'id' and 'zipcode' are withdrawn since I thought that they don't have meaningful implication ## \n\ndata.drop(columns=['date','id','zipcode'],inplace=True)","209fbdfe":"def plot_distribution(column1,column2, size_bin) :  \n    tmp1 = data[column1].head()\n    tmp2 = data[column2].head()\n    hist_data = [tmp1, tmp2]\n    \n    group_labels = ['malignant', 'benign']\n    colors = ['#FFD700', '#7EC0EE']\n\n    fig = ff.create_distplot(hist_data, group_labels, colors = colors, show_hist = True, bin_size = size_bin, curve_type='kde')\n    \n    fig['layout'].update(title = column1)\n\n    py.iplot(fig, filename = 'Density plot')","60804c83":"import random\ndef dist_plot(column,ran):\n    \n    color = ['c','orange','lightgrey']\n    data[column].plot.hist(color=color[ran],figsize=(10,6),bins=100)\n    plt.title(column+\" Distribution\")\n    plt.show()","9504cd58":"for i,j in enumerate(data.columns[1:]):\n    col_index = int(i) % 3\n    dist_plot(j,col_index)","a51d0af7":"### Showing correlation matrix and heatmap figure\n\ncorr_mat = data.corr()\nplt.figure(figsize=(12,8))\nsns.heatmap(corr_mat)","a21b96cc":"target_pca = data['price']\ndata_pca = data.drop('price', axis=1)\n\ntarget_pca = pd.DataFrame(target_pca)\n\n### normalizing data\nX_pca = data_pca.values\nX_std = StandardScaler().fit_transform(X_pca)\n\npca = PCA(svd_solver='full')\npca_std = pca.fit(X_std, target_pca).transform(X_std)\n\npca_std = pd.DataFrame(pca_std)\npca_std = pca_std.merge(target_pca, left_index = True, right_index = True, how = 'left')","e905f6c5":"var_pca = pd.DataFrame(pca.explained_variance_ratio_)\nvar_pca = var_pca.T\n\n#----------SUM AND DROP COMP [7:30]\ncol_list = list(v for v in chain(pca_std.columns[10:18])) \nvar_pca['OTHERS_COMP'] = var_pca[col_list].sum(axis=1)\nvar_pca.drop(var_pca[col_list],axis=1,inplace=True)\nvar_pca = var_pca.T","0c0db177":"### table of variances explained by each components\nvar_pca","55a13d9f":"labels = ['Component1','Component2','Component3','Component4','Component5','Component6','Component7','Component8','Component9','Component10','other Components']\ntrace = go.Pie( labels=labels, values=var_pca[0],\n              opacity=1,\n              textfont=dict(size=15)\n              )\nlayout = dict(title=\"Variances explained by Each Componets: \" + \"10 Components among 17 are explaining 89.6%\")\n\nfig = dict(data=[trace], layout=layout)\npy.iplot(fig)","3f71cfe2":"train_data, test_data = train_test_split(data, train_size=0.8, random_state=3)\nY_train = train_data.price \nX_train = train_data.iloc[:,2:]\nY_test  = test_data.price\nX_test  = test_data.iloc[:,2:]\nY_train = np.array(Y_train, dtype=pd.Series).reshape(-1,1)\nY_test = np.array(Y_test, dtype=pd.Series).reshape(-1,1)\n","ae079e8b":"lm = LinearRegression()\nlm.fit(np.array(X_train), np.array(Y_train))\npred = lm.predict(X_test)","15787b65":"print( \"Mean Squared Error is : \"+ str(np.sqrt(mean_squared_error(Y_test,pred))))\nprint( \"Linear Regression Score is : \" + str(lm.score(X_test,Y_test)))","6908489b":"lasso = Lasso()\nlasso.fit(np.array(X_train), np.array(Y_train))\npred1 = lasso.predict(X_test)\n\nprint( \"Mean Squared Error is : \"+ str(np.sqrt(mean_squared_error(Y_test,pred1))))\nprint( \"Linear Regression Score is : \" + str(lasso.score(X_test,Y_test)))\n","f7f1f15a":"data.iloc[:,1:].shape","796d3e53":"### model fitting using deep learning\nfrom keras.callbacks import EarlyStopping\nearly_stopping_monitor = EarlyStopping(patience=5)\n\npredictors = X_train\ntarget = Y_train\n# Specify the model\nn_cols = predictors.shape[1]\nmodel = Sequential()\nmodel.add(Dense(50, activation='relu', input_shape = (n_cols,)))\nmodel.add(Dense(50, activation='relu'))\nmodel.add(Dense(50, activation='relu'))\nmodel.add(Dense(1))\n\n# Compile the model\n\nmodel.compile(optimizer='adam', loss='mean_squared_error',metrics=['mae'])\n\n# Fit the model\nhist = model.fit(predictors,target, batch_size=100, epochs=100, validation_split=0.2,verbose=False)\n","bd24f88e":"hist.history.keys()","8251dfcb":"plt.figure(figsize=(10,8))\nplt.plot(hist.history['mean_absolute_error'])\nplt.plot(hist.history['val_mean_absolute_error'])\nplt.xticks(range(0,100,5))\nplt.xlabel(\"epoch\",fontsize=15)\n#plt.plot(hist.history['val_loss'])\nplt.legend(['Train','Test'],loc='upper right',fontsize=13)\nplt.show()","c9e613ea":"print(\"Mean Absolute Error(MAE) for Training set is : \" + str(hist.history['mean_absolute_error'][99]))\nprint(\"Mean Absolute Error(MAE) for Validation set is : \" + str(hist.history['val_mean_absolute_error'][99]))","f7fb50c4":"# <a id=\"6\"><\/a><br> Neural Network Analysis","9731a942":"# Lasso Regularization doesn't provide better Result....","d82edf55":"# <a id=\"4\"><\/a><br> Dimension Reduction : PCA method","acfc7825":"# <a id=\"5\"><a\/><br> Regession Analysis","afefabd6":"# <a id=\"1\"><\/a><br> Dataset import","d7bc20fa":"# <a id=\"2\"><\/a><br> Data Preprocessing","89b41481":"# Contents\n1.  [Dataset Importing](#1)\n2. [Data preprocessing](#2)\n3. [Explorative Data Analysis](#3)\n4. [Dimension Reduction : PCA method](#4)\n5. [Regression Anlaysis](#5) \n5. [Neural Network Analysis](#6)","5e363aee":"# <a id=\"3\"><\/a><br> Explorative Data Analysis"}}