{"cell_type":{"1d1482df":"code","67e4c1cf":"code","e623ac36":"code","fd8040b6":"code","bcca9152":"code","f753239a":"code","2583efe5":"code","96b3b1f4":"code","d9197c68":"code","971363d4":"code","25d0a283":"code","a81ac872":"code","cfc277ba":"code","129f718c":"code","a7b9909a":"code","4430527a":"code","5de084ad":"code","fe4b519b":"code","4d1ee530":"code","7ea0b940":"code","b1f1d9f8":"code","166ec819":"code","e358a144":"code","6b2d43bb":"code","026b01b3":"code","4e9d7732":"code","103e5040":"code","8484845e":"code","08c7d7bf":"code","f3a2b967":"code","e2091111":"code","5a1d18ad":"code","70a8b37f":"code","714f47e3":"markdown","7a204fce":"markdown","b4de1e31":"markdown","9d1d324e":"markdown","5aae57df":"markdown","60c7a964":"markdown","1a9c18f1":"markdown","7b4f90c1":"markdown","b5a2a1bf":"markdown","fc561bc6":"markdown","bc2d013a":"markdown","062aa9e3":"markdown","63b62efb":"markdown","368fb8bb":"markdown","f185f69f":"markdown","e8bbcfab":"markdown","f074cd48":"markdown","7ddeda3b":"markdown","dadd1ea5":"markdown","342aaa49":"markdown","41c34f6c":"markdown","a7f7e65c":"markdown","9fa00045":"markdown"},"source":{"1d1482df":"import numpy as np\nimport pandas as pd\nimport spacy\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom spacy.lang.en.stop_words import STOP_WORDS\nfrom sklearn.metrics.pairwise import cosine_similarity ","67e4c1cf":"md = pd.read_csv('..\/input\/wikipedia-movie-plots\/wiki_movie_plots_deduped.csv')","e623ac36":"md.head()","fd8040b6":"md.describe()","bcca9152":"md_plot = md['Plot']","f753239a":"md_plot.head()","2583efe5":"md_nan = md_plot.isna()\nmd_nan.sum()","96b3b1f4":"nlp = spacy.load('en_core_web_sm') ","d9197c68":"doc = nlp(md_plot[0]) \nprint(doc) ","971363d4":"lemmas = [token.lemma_ for token in doc] \nprint(lemmas)","25d0a283":"a_lemmas = [lemma for lemma in lemmas \n            if lemma.isalpha() or lemma not in STOP_WORDS] \n\nprint(a_lemmas)","a81ac872":"print(' '.join(a_lemmas))","cfc277ba":"def preprocess(text):\n    doc = nlp(text)\n    lemmas = [token.lemma_ for token in doc]\n    a_lemmas = [lemma for lemma in lemmas \n            if lemma.isalpha() and lemma not in STOP_WORDS]\n    \n    return ' '.join(a_lemmas)","129f718c":"preprocess(md_plot[0]) # verificar o resultado da fun\u00e7\u00e3o\nprint(md_plot[0])","a7b9909a":"md_plot_test = [[md_plot[0]], [md_plot[1]], [md_plot[2]]] #selecionar apenas algumas linhas\nmd_plot_test = pd.DataFrame(md_plot_test, columns = ['Plot']) \n    \nmd_plot_test['test'] = md_plot_test['Plot'].apply(lambda x: preprocess(x))\n\nmd_plot_test #cria uma nova coluna","4430527a":"md_half = md[:len(md)\/\/2] ","5de084ad":"md_half['Plot_lemma'] = md_half['Plot'].apply(lambda x: preprocess(x)) ","fe4b519b":"md_half #verificar a nova coluna","4d1ee530":"md_half.head()","7ea0b940":"np.savez_compressed('md_half')\nmd_half.to_csv('csv_to_submit.csv', index = False)","b1f1d9f8":"vectorizer = TfidfVectorizer()","166ec819":"md_half_plot_lemma = md_half['Plot_lemma'] \nmd_half_plot_lemma.head()\nmd_half_plot_lemma.shape","e358a144":"tfidf_matrix_teste = vectorizer.fit_transform(md_plot_test['test'])\nprint(tfidf_matrix_teste) ","6b2d43bb":"tfidf_matrix_half = vectorizer.fit_transform(md_half['Plot_lemma']) #criar matriz de TFIDF","026b01b3":"md_half_plot_lemma.shape","4e9d7732":"print(tfidf_matrix_half) ","103e5040":"cosine_sim_test = cosine_similarity(tfidf_matrix_teste, tfidf_matrix_teste)","8484845e":"cosine_sim_half = cosine_similarity(tfidf_matrix_half, tfidf_matrix_half)","08c7d7bf":"print(cosine_sim_half)","f3a2b967":"cosine_sim_half.shape","e2091111":"indices_half = pd.Series(md_half.index, index=md_half['Title']).drop_duplicates() #pegar os nomes de cada filme\nindices_half","5a1d18ad":"\ndef get_recommendations(title, cosine_sim, indices):\n    idx = indices[title]\n    sim_scores = list(enumerate(cosine_sim[idx]))\n    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)\n    sim_scores = sim_scores[1:6]\n    movie_indices = [i[0] for i in sim_scores]\n    return md_half['Title'].iloc[movie_indices]","70a8b37f":"print(get_recommendations('The Godfather', cosine_sim_half, indices_half))","714f47e3":"Select the 'Plot' column","7a204fce":"Por quest\u00e3o de processamento foi necess\u00e1rio diminuir pela metade o tamanho do dataset escolhido","b4de1e31":"**Apply TFIDF**","9d1d324e":"Just some visualization of the dataset","5aae57df":"**Make the recommendations**","60c7a964":"**Pre-processing of the text**","1a9c18f1":"Test the function for simple cases","7b4f90c1":"The purpose of this code is to indicate 10 films based only on the analysis of the plot made available in the dataset \"wikipedia-movie-plots\"","b5a2a1bf":"Pre-processing using SPACY library","fc561bc6":"Verify if there is any missing value in 'Plot' column","bc2d013a":"Now apply on the correct dataset","062aa9e3":"Apply function to the dataset\n**here it was necessary to decrease the size of the dataset","63b62efb":"Visualization of some tests","368fb8bb":"Steps for implementation:\n1. Import the dataset;\n\n2. Check for plot information on all movies;\n\n3. Apply nlp () to the Plot column - Create tokens from each word;\n\n5. Apply pre-processing for each token;\n\n6. Calculates the importance of each word according to the whole document with TFIDF;\n\n7. Calculate the cosine_similarity between each plot;\n\n8. Create function to list the films with cosine_similarity closest to the calculated value for the chosen film.\n","f185f69f":"Salvar resultado","e8bbcfab":"**Import libraries**","f074cd48":"Create a function for this pre-processing and apply to all other Plots in the database","7ddeda3b":"Verify the TFIDF matrix on the test dataset","dadd1ea5":"**Apply o cosine simularity**","342aaa49":"***This notebook is based on the \"Feature Engineering for NLP in Python for ML\" course at DataCamp***","41c34f6c":"Apply TFIDF ","a7f7e65c":"**Import the dataset**","9fa00045":"**Check what is displayed in that database**"}}