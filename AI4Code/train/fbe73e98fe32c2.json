{"cell_type":{"0319108a":"code","6d43a856":"code","1a670e4c":"code","5d143dcd":"code","85adbad6":"code","9846821d":"code","28343a56":"code","47aa07d4":"code","637347fb":"code","af76da06":"code","ff3853f8":"code","305cecb9":"code","daceaee5":"code","81a0d464":"code","0ccccb88":"code","dd1c143b":"code","0f972c27":"code","04db83e2":"code","2669263b":"code","559aa698":"code","29b24c5b":"code","bf53edf2":"code","91ba2de7":"code","0bdcbafc":"code","12412293":"code","a5de302b":"code","8beb9917":"code","21cdfc1d":"code","caffc46b":"markdown","7e47c660":"markdown","98c39181":"markdown","6ebd5c2c":"markdown","a07fbeae":"markdown","db19cd2a":"markdown","52ffb3ab":"markdown","2c320ea3":"markdown","99ed28a2":"markdown","0269cd26":"markdown","4b82a510":"markdown","5ac1ecc9":"markdown","468053b3":"markdown","9ad46a90":"markdown","814d61ad":"markdown","1170c5fa":"markdown","456af547":"markdown","1979952d":"markdown","658a85f9":"markdown","964f52b3":"markdown"},"source":{"0319108a":"import numpy as np\nimport pandas as pd\n\nfrom matplotlib import pyplot as plt\nplt.style.use('ggplot')\nimport seaborn as sns","6d43a856":"from kaggle.competitions import twosigmanews\nenv = twosigmanews.make_env()","1a670e4c":"market_train_df, news_train_df = env.get_training_data()","5d143dcd":"import missingno\n\ndef plot_miss(df):\n    missingno.matrix(df)\n    plt.show()\n    \nplot_miss(market_train_df)","85adbad6":"def plot_distributions(df, labels, figsize, h, w):\n    i=0\n    fig = plt.figure(figsize=figsize)\n    for l in labels:\n        i+=1\n        fig.add_subplot(h,w,i)\n        plt.tight_layout()\n        plt.title(f'{l}, mean = {df[l].mean():.2f}')\n        lq = df[l].quantile(0.01)\n        uq = df[l].quantile(0.99)\n        feature = df[l].dropna()\n        plt.hist(feature[(feature > lq)&(feature < uq)], density=True, bins=100, alpha=0.7)\n    plt.show()","9846821d":"news_labels = ['wordCount', 'sentenceCount', 'companyCount', 'bodySize', 'urgency',\n              'firstMentionSentence', 'relevance', 'sentimentClass',\n              'sentimentNegative', 'sentimentNeutral', 'sentimentPositive',\n              'sentimentWordCount', 'noveltyCount12H', 'noveltyCount24H',\n              'noveltyCount3D', 'noveltyCount5D', 'noveltyCount7D', 'volumeCounts12H',\n              'volumeCounts24H', 'volumeCounts3D', 'volumeCounts5D', 'volumeCounts7D']\n\nplot_distributions(news_train_df[news_labels], news_labels, figsize=(20,25), h=6, w=4)","28343a56":"market_labels = ['returnsClosePrevMktres10', 'returnsClosePrevMktres1']\n\nplot_distributions(market_train_df[market_labels], market_labels, figsize=(15,5), h=1, w=3)","47aa07d4":"market_drop_features = ['volume', 'close', 'open', 'returnsClosePrevRaw1', \n    'returnsOpenPrevRaw1','returnsOpenPrevMktres1', 'returnsClosePrevRaw10', \n    'returnsOpenPrevRaw10', 'returnsOpenPrevMktres10']\nnews_drop_features = ['urgency', 'takeSequence','provider', 'subjects', \n    'audiences','sentimentClass', 'headlineTag', 'sourceTimestamp', 'firstCreated', \n    'sourceId', 'headline', 'marketCommentary', 'assetCodes']","637347fb":"sem_labels = ['sentimentPositive', 'sentimentNegative', 'sentimentNeutral']","af76da06":"from sklearn.cluster import MiniBatchKMeans\n\nkmeans = MiniBatchKMeans(n_clusters = 9, init = 'k-means++')\nsemant_kmeans = kmeans.fit_predict(news_train_df[sem_labels].values)\nsemant_kmeans = np.reshape(semant_kmeans, (semant_kmeans.shape[0], 1))","ff3853f8":"from sklearn.preprocessing import OneHotEncoder\n\nonehotencoder = OneHotEncoder(categories='auto')\nkmean_dummies = onehotencoder.fit_transform(semant_kmeans).toarray()\n\ndef dummies_concat(df, dummies, pref):\n    dummies = pd.DataFrame(dummies, index=df.index)#dummies[:, 1:]\n    dummies.columns = [pref + str(x) for x in dummies.columns]\n    return pd.concat([df, dummies], axis=1)","305cecb9":"def corr_plot(df):\n    corr = df.corr()\n\n    mask = np.zeros_like(corr, dtype=np.bool)\n    mask[np.triu_indices_from(mask)] = True\n\n    f, ax = plt.subplots(figsize=(11, 9))\n    cmap = sns.diverging_palette(220, 10, as_cmap=True)\n    sns.heatmap(corr, mask=mask, cmap=cmap, vmax=.3, center=0,\n                square=True, linewidths=.5, cbar_kws={\"shrink\": .5})\n    plt.show()","daceaee5":"num_labels = ['noveltyCount12H', 'noveltyCount24H','noveltyCount3D', 'noveltyCount5D', 'noveltyCount7D',\n             'volumeCounts12H', 'volumeCounts24H', 'volumeCounts3D', 'volumeCounts5D', 'volumeCounts7D',\n             'wordCount', 'sentenceCount', 'companyCount', 'bodySize', 'sentimentWordCount', 'relevance',\n             'firstMentionSentence']\n\ncorr_plot(news_train_df[num_labels])","81a0d464":"def plot_relevance(s_relevance):\n    plt.title(f'Relevance < 1, {sum(news_train_df[\"relevance\"]<1)\/news_train_df.shape[0]*100:.2f} % of data')\n    plt.hist(s_relevance[s_relevance<1], bins=100)\n    plt.show()\n    plt.title(f'Relevance = 1, {sum(news_train_df[\"relevance\"] == 1)\/news_train_df.shape[0]*100:.2f} % of data')\n    plt.hist(s_relevance[s_relevance<1], bins=100)\n    plt.hist(s_relevance[s_relevance==1], bins=100)\n    plt.show()","0ccccb88":"plot_relevance(news_train_df['relevance'])","dd1c143b":"def is_one(x):\n    return 1 if x == 1 else 0\n\ndef is_positive(x):\n    return 1 if x > 0 else 0\n\ndef is_abnormal(x, threshold):\n    return 1 if np.abs(x) > threshold else 0","0f972c27":"def max_pooling(df, features, index, agg_method):\n    agg_methods = {k: agg_method for k in features}\n    return df.groupby(index).agg(agg_methods).reset_index()","04db83e2":"def state_reduce(market, news, kmean_dummies):\n    # check positive return\n    market['returnsClosePrevMktres1'] = market['returnsClosePrevMktres1'].apply(is_positive)\n    market['returnsClosePrevMktres10'] = market['returnsClosePrevMktres10'].apply(is_positive)\n    # check abnormal return\n    std = market['returnsClosePrevMktres1'].std()\n    market['abnormalClosePrevMktres1'] = market['returnsClosePrevMktres1'].apply(lambda x: is_abnormal(x, 2.5*std))\n    # check relevance is equal to one\n    news['relevance'] = news['relevance'].apply(is_one)\n    # drop features, which unusefull for us\n    market = market.drop(market_drop_features, axis=1)\n    news = news.drop(news_drop_features + sem_labels + num_labels, axis=1)\n    # add semant clusters\n    news = dummies_concat(news, kmean_dummies, 'sem_cl_')\n    # choose features that will be in state\n    bin_features = [l for l in news.columns if l not in ['time', 'assetName']]\n    # truncat date for merging data\n    market['date'] = market['time'].dt.round('d') \n    news['date'] = news['time'].dt.round('d')\n    # pooling news in one date\n    news = max_pooling(news,  bin_features, ['date', 'assetName'], 'max')\n    # merging data and fill NA\n    market_train_df = pd.merge(market, news, how='left', on=['date', 'assetName']).fillna(2)\n    # reduce state features to str state id\n    state_features = bin_features + ['returnsClosePrevMktres1', 'returnsClosePrevMktres10', 'abnormalClosePrevMktres1']\n    state = market_train_df[state_features[0]].astype(int).astype(str)\n    for col in state_features[1:]:\n        state += market_train_df[col].astype(int).astype(str)\n    market_train_df['state'] = state\n    \n    return market_train_df.drop(state_features + ['date', 'assetName'], axis=1)","2669263b":"%%time\n\nmarket_train_df = state_reduce(market_train_df, news_train_df, kmean_dummies)","559aa698":"market_train_df.head()","29b24c5b":"print('Count of unique states in train data', len(market_train_df['state'].unique()))","bf53edf2":"def fit_state_table(df):\n    # positive return observations\n    pos_reward = df[df['returnsOpenNextMktres10'] > 0]\n    # positive return probability\n    pos_reward_prob = pos_reward.shape[0] \/ df.shape[0]\n    # probability of each state\n    state_probs = train.groupby(['state']).size() \/ df.shape[0]\n    # probability of each state with positive return\n    pos_state_probs = pos_reward.groupby(['state']).size() \/ pos_reward.shape[0]\n    # concat all of this to one table\n    state_table = pd.concat([state_probs, pos_state_probs], axis = 1, \n                            keys=['state_prob', 'pos_state_probs'], sort=False).fillna(0)\n    \n    state_table.index.name = 'state'\n    \n    state_table['pos_return_prob'] = state_table['pos_state_probs'] * pos_reward_prob \/ state_table['state_prob']\n    \n    return state_table","91ba2de7":"train, test = np.split(market_train_df[['time', 'universe', 'state', 'returnsOpenNextMktres10']], \n                       [int(.8*len(market_train_df))])\nprint(f'State coverage: { test[\"state\"].isin(train[\"state\"]).sum() \/ len(test) }')\n#test = test[test[\"state\"].isin(train[\"state\"])]","0bdcbafc":"state_table = fit_state_table(train)\nstate_table.head()","12412293":"def max_prob_prediction(df, st):\n    df = pd.merge(df, st[['pos_return_prob']], how='left', on=['state']).fillna(0)\n    return df['pos_return_prob'].apply(lambda p: p if p >= 0.5 else p-1).tolist()\n\ndef diff_prob_prediction(df, st):\n    df = pd.merge(df, st[['pos_return_prob']], how='left', on=['state']).fillna(0)\n    return df['pos_return_prob'].apply(lambda p: 2*p - 1).tolist()\n\ndef all_in_prob_prediction(df, st):\n    df = pd.merge(df, st[['pos_return_prob']], how='left', on=['state']).fillna(0)\n    return df['pos_return_prob'].apply(lambda p: 1 if p >= 0.5 else -1).tolist()","a5de302b":"max_pred = max_prob_prediction(test, state_table)\ndiff_pred = diff_prob_prediction(test, state_table)\nall_in_pred = all_in_prob_prediction(test, state_table)\nmax_rewards = test['returnsOpenNextMktres10'] * max_pred\ndiff_rewards = test['returnsOpenNextMktres10'] * diff_pred\nall_in_rewards = test['returnsOpenNextMktres10'] * all_in_pred","8beb9917":"def score(r):\n    return np.mean(r)\/np.std(r)\n    \nfig = plt.figure(figsize=(14,7))\nfig.add_subplot(1,2,1)\nplt.tight_layout()\nplt.title(f'Rewards')\nplt.hist(max_rewards.tolist(), bins=100, alpha=0.5,\n         label=f'MAX m={np.mean(max_rewards):.5f}, std={np.std(max_rewards):.5f}')\nplt.hist(diff_rewards.tolist(), bins=100, alpha=0.5,\n         label=f'DIFF m={np.mean(diff_rewards):.5f}, std={np.std(diff_rewards):.5f}')\nplt.hist(all_in_rewards.tolist(), bins=100, alpha=0.5,\n         label=f'ALL_IN m={np.mean(all_in_rewards):.5f}, std={np.std(all_in_rewards):.5f}')\nplt.legend()\nfig.add_subplot(1,2,2)\nplt.tight_layout()\nplt.title(f'Total reward')\nplt.plot(np.cumsum(max_rewards), \n         label=f'MAX sum={np.sum(max_rewards):.2f}, score={score(max_rewards):.4f}')\nplt.plot(np.cumsum(diff_rewards), \n         label=f'DIFF sum={np.sum(diff_rewards):.2f}, score={score(diff_rewards):.4f}')\nplt.plot(np.cumsum(all_in_rewards), \n         label=f'ALL_IN sum={np.sum(all_in_rewards):.2f}, score={score(all_in_rewards):.4f}')\nplt.legend()\nplt.show()","21cdfc1d":"preddays = env.get_prediction_days()\nfor marketdf, newsdf, predtemplatedf in preddays:\n    \n    # cpredict semant clusters\n    pred_kmeans = kmeans.predict(newsdf[sem_labels].values)\n    pred_kmeans = np.reshape(pred_kmeans, (pred_kmeans.shape[0], 1))\n    \n    # encode semant clusters\n    pred_dummies = onehotencoder.transform(pred_kmeans).toarray()\n    \n    # merge and pool data\n    states = state_reduce(marketdf, newsdf, pred_dummies)\n    \n    # predict confidence\n    preds = max_prob_prediction(states, state_table)\n    \n    #prediction\n    predsdf = pd.DataFrame({'ast':states['assetCode'],'conf':preds})\n    predtemplatedf.loc[predtemplatedf['assetCode'].isin(predsdf.ast), 'confidenceValue'] = predsdf['conf'].values\n    \n    env.predict(predtemplatedf)\n\nenv.write_submission_file()","caffc46b":"### Missing values","7e47c660":"# Introducting  \n\nBayes theorem is the foundation of machine learning. This kernel is about using this in the classification. Everything you see below is informative and entertaining.  \n\nYou will see how a simple non-parametric approach gives a competitive result.  \n  \nAre your friends lately talking about a priori and a posteriori probabilities? You want to keep the conversation going, but these words are not familiar to you.  \nRead this kernel and you will again become the life of the party!","98c39181":"You can see that almost all the observations in the data are filled and we can ignore the missing values.  \n  \nMany out-of-the-box models suggest that the distribution of features is normal. If we look at the distribution of our signs, we will understand that this approach is applicable in our case as much as the circle is square.","6ebd5c2c":"### Clustering sentiment features  \nIn the title of the competition we are urged to use the news, but all we have just a few semantic features. We will try to explore it in several states through clustering.\n","a07fbeae":"## Merging news and price data  \nThe following function will convert market and news data into a binarized state, such as 0100100010...","db19cd2a":"# Validation\nNow let's see what kind of profit this approach will bring.","52ffb3ab":"![](https:\/\/i.imgflip.com\/2lphog.jpg)","2c320ea3":"### Numeric features in news data","99ed28a2":"## Bayes  \nFor each state, it is necessary to obtain the posterior probability of a positive return. To do this, it is necessary to calculate all the components of the Bayes equation. We do not touch the negative return, because Positive and negative returns are mutually exclusive.  \n  \n$\\LARGE P(A \\mid B) = \\frac{P(B \\mid A) \\, P(A)}{P(B)}$  \n\nIf you multiply these letters into words, you get the following:  \n\"The probability of event A in the case of observing event B is equal to the product of the probability of event B in case of observing event A and the probability of occurrence of event A divided by the probability of event B\"  \nFor our problem, we can write the equation like this:  \n  \n$\\LARGE P(positiveReturn \\mid State_i) = \\frac{P(State_i \\mid positiveReturn) \\, P(positiveReturn)}{P(State_i)}$\n","0269cd26":"# News pooling  \nWe will not go deep, I will immediately say that for one day in the market we have a few news. In order to take them all, you need to do something like a pooling.\nHere, the cluster binarization performed above is useful. For each cluster we will see that it was on that day. And if there are two or more semantic clusters, max pooling will reflect this.","4b82a510":"### Numeric features in news data\nIf you look at the names of the variables in the news, then there is a suspicion that many of them will correlate with each other. Let's look at their correlation.","5ac1ecc9":"Let's compare two predictive approaches on test data, and for one we'll see what profit we would get for this period on the exchange. Our confidence will be the rate, the percentage of the starting amount.","468053b3":"# State truncating and demension reduction  \n\nTo demonstrate the essence of the Bayes equation, we need to translate our continuous features into a limited number of states.  \nIn market data, the most informative are relative features. This is what we will save, and so that there will not be too much of them, we will save only close features.  \nAbove, we have seen that some features in the news do not change their value. Let's drop these features and all the rest that contain something other than numbers from 0 to 9.","9ad46a90":"Lets get training data from enviroment","814d61ad":"If you do not call the broker yet, lets make submission.","1170c5fa":"Indeed, many features tell us about the same thing, and the sign of relevance generally explains all other features. Apparently relevance is some kind of formula that is calculated on the basis of all the features presented. I like the formulas, so I\u2019ll drop everything except this one!\nMoreover, I do not think that it is possible to cluster these features. Because of the exponential distributions, we get just one dominant cluster.  \nLet's take a closer look at relevance.\n","456af547":"And then we need encode cluster numbers to binary format. I will explain later","1979952d":"To translate continuous features into states, create several functions. Relevance can be equal to 1 or not. Returns may be positive or negative. Also, the Gaussian distribution allows us to tell us in which cases the return is abnormal.","658a85f9":"### Numeric features in price data  \n\n\nHowever, market data is normally distributed. If all the data in the world were distributed as well, then data scientists would earn much more.","964f52b3":"# Summary about data\n\n"}}