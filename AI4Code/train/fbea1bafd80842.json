{"cell_type":{"92523a15":"code","e85415fa":"code","6cf20ca4":"code","b330aa15":"code","84e3fbac":"code","4d87e2d6":"code","c61c95a8":"code","73703eda":"code","39d0f472":"code","eef1023c":"code","dd46bffb":"code","98279879":"code","df445f78":"code","24358bd7":"code","a0e3c1cd":"code","bd006284":"code","9b448acb":"code","58534d21":"code","5a59017c":"code","4a724819":"code","79c32b7c":"code","75b188c1":"code","b60be15c":"code","26e225ef":"code","a7d122ca":"code","ea1c03c9":"code","fbd0f968":"code","a33edd1f":"code","7ad2e7bd":"code","0d20be27":"code","5ede15d2":"code","19bb2c07":"code","044e6c39":"markdown","a05a6b8b":"markdown","b76b9890":"markdown","4d6c4016":"markdown","4bfe380e":"markdown","752b3512":"markdown","710f63c5":"markdown","441cf3bb":"markdown","a2ecb048":"markdown","94c4b6c8":"markdown","5dfdd02f":"markdown","f7840a2d":"markdown","ac8551bf":"markdown","cf79c1b3":"markdown"},"source":{"92523a15":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom wordcloud import WordCloud, STOPWORDS\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.cluster import KMeans, AgglomerativeClustering, DBSCAN\nimport warnings\nwarnings.filterwarnings(\"ignore\")","e85415fa":"data = pd.read_csv('..\/input\/fine-food-cleaned\/eda_final.csv')\n","6cf20ca4":"data_train = data.cleaned_text.sample(5000)\ndata_train.tail()","b330aa15":"# Initializing the tf-idf vectorizer\ntfidf = TfidfVectorizer(ngram_range=(1,2),min_df=10)\ndata_features = tfidf.fit_transform(data_train)","84e3fbac":"# Now we will scale it \nstd_Scaler = StandardScaler(with_mean=False)\nstd_data = std_Scaler.fit_transform(data_features)","4d87e2d6":"# Defining a function to find the optimal k\ndef find_optimal_k(std_data):\n    loss = []\n    k = list(range(2, 15))\n    for noc in k:\n        model = KMeans(n_clusters = noc)\n        model.fit(std_data)\n        loss.append(model.inertia_)\n    plt.plot(k, loss, \"-o\")\n    plt.title(\"Elbow method to choose k\")\n    plt.xlabel(\"K\")\n    plt.ylabel(\"Loss\")\n    plt.show()","c61c95a8":"find_optimal_k(std_data)","73703eda":"# Kmeans to \nk_Means = KMeans(n_clusters = 5)\nk_Means.fit(std_data)\npred = k_Means.predict(std_data)","39d0f472":"k_Means.cluster_centers_.shape","eef1023c":"# Plot each cluster features in a cloud\ndef plot_cluster_cloud(features, coef):\n    coef_df = pd.DataFrame(coef, columns = features)\n#     print(len(coef_df))\n    # Create a figure and set of 15 subplots because our k range is in between \n    fig, axes = plt.subplots(3, 3, figsize = (30, 20))\n    fig.suptitle(\"Top 20 words for each cluster \", fontsize = 50)\n    cent = range(len(coef_df))\n    for ax, i in zip(axes.flat, cent):\n        wordcloud = WordCloud(background_color = \"white\").generate_from_frequencies(coef_df.iloc[i,:].sort_values(ascending = False)[0:20])\n        ax.imshow(wordcloud)\n        ax.set_title(\"Cluster {}\".format(i+1), fontsize = 30)\n        ax.axis(\"off\")\n    plt.tight_layout()","dd46bffb":"features = tfidf.get_feature_names()\ncoef = k_Means.cluster_centers_","98279879":"plot_cluster_cloud(features, coef)","df445f78":"data_features.shape","24358bd7":"from sklearn.decomposition import PCA","a0e3c1cd":"pca = PCA(n_components=2, random_state=0)\nreduced_features = pca.fit_transform(std_data.toarray())\n\nreduced_cluster_centers = pca.transform(k_Means.cluster_centers_)","bd006284":"plt.scatter(reduced_features[:,0], reduced_features[:,1], c=k_Means.predict(std_data))\nplt.scatter(reduced_cluster_centers[:, 0], reduced_cluster_centers[:,1], marker='x', s=150, c='b')\nplt.rcParams[\"figure.figsize\"] = (8,4)","9b448acb":"from sklearn.metrics import silhouette_score\nsilhouette_score(std_data, labels=k_Means.predict(std_data))","58534d21":"data_pos = data[data.score_pos_neg == 'positive'].sample(2500)\ndata_pos.tail()\n\ndata_neg = data[data.score_pos_neg == 'negative'].sample(2500)\ndata_neg.tail()\n\n# type(data_pos)\ndata_train_equalized = pd.concat([data_pos,data_neg])\ndata_train_equalized= data_train_equalized.cleaned_text\ndata_train_equalized.shape","5a59017c":"# Initializing the tf-idf vectorizer\ntfidf_2 = TfidfVectorizer(ngram_range=(1,2),min_df=10)\ndata_features_2 = tfidf_2.fit_transform(data_train_equalized)","4a724819":"# Now we will scale it \nstd_Scaler_2 = StandardScaler(with_mean=False)\nstd_data_2 = std_Scaler_2.fit_transform(data_features_2)","79c32b7c":"# Defining a function to find the optimal k\ndef find_optimal_k(std_data):\n    loss = []\n    k = list(range(2, 15))\n    for noc in k:\n        model = KMeans(n_clusters = noc)\n        model.fit(std_data)\n        loss.append(model.inertia_)\n    plt.plot(k, loss, \"-o\")\n    plt.title(\"Elbow method to choose k\")\n    plt.xlabel(\"K\")\n    plt.ylabel(\"Loss\")\n    plt.show()","75b188c1":"find_optimal_k(std_data_2)","b60be15c":"# Kmeans to \nk_Means_2 = KMeans(n_clusters = 6)\nk_Means_2.fit(std_data_2)\npred_2 = k_Means_2.predict(std_data_2)","26e225ef":"k_Means_2.cluster_centers_.shape","a7d122ca":"# Plot each cluster features in a cloud\ndef plot_cluster_cloud(features, coef):\n    coef_df = pd.DataFrame(coef, columns = features)\n#     print(len(coef_df))\n    # Create a figure and set of 15 subplots because our k range is in between \n    fig, axes = plt.subplots(2, 5, figsize = (30, 20))\n    fig.suptitle(\"Top 20 words for each cluster \", fontsize = 50)\n    cent = range(len(coef_df))\n    for ax, i in zip(axes.flat, cent):\n        wordcloud = WordCloud(background_color = \"white\").generate_from_frequencies(coef_df.iloc[i,:].sort_values(ascending = False)[0:20])\n        ax.imshow(wordcloud)\n        ax.set_title(\"Cluster {}\".format(i+1), fontsize = 30)\n        ax.axis(\"off\")\n    plt.tight_layout()","ea1c03c9":"features_2 = tfidf_2.get_feature_names()\ncoef_2 = k_Means_2.cluster_centers_","fbd0f968":"plot_cluster_cloud(features_2, coef_2)","a33edd1f":"data_features_2.shape","7ad2e7bd":"from sklearn.decomposition import PCA","0d20be27":"pca_2 = PCA(n_components=2, random_state=0)\nreduced_features_2 = pca.fit_transform(std_data_2.toarray())\n\nreduced_cluster_centers_2 = pca.transform(k_Means_2.cluster_centers_)","5ede15d2":"plt.scatter(reduced_features_2[:,0], reduced_features_2[:,1], c=k_Means_2.predict(std_data_2))\nplt.scatter(reduced_cluster_centers_2[:, 0], reduced_cluster_centers_2[:,1], marker='x', s=150, c='b')\nplt.rcParams[\"figure.figsize\"] = (10,4)","19bb2c07":"from sklearn.metrics import silhouette_score\nsilhouette_score(std_data_2, labels=k_Means_2.predict(std_data_2))","044e6c39":"### Plotting the elbow curve to chose the k-Value","a05a6b8b":"#### We can see that after k=6 there not much difference so we are safe to chose the value of k=6.","b76b9890":"#### It's a little visible that the words grouped together are not of the same type.\n\n#### It's quiet clear that it would not be an easy task to visualize this. So we will use Dimensionality reduction to tackle this situation. Let us coonsider a 2D breakdown of the features.\n\n\n## PCA- For 2D Visualisation","4d6c4016":"## Now, performing some cluster validation to check the metrics for our cluster\n- As we do not have labelled data we cannot go ahead with homegenity score so we will go ahead with silhouette_score","4bfe380e":"#### It's a little visible that the words grouped together are not of the same type.\n#### It's quiet clear that it would not be an easy task to visualize this. So we will use Dimensionality reduction to tackle this situation. Let us coonsider a 2D breakdown of the features.\n\n\n## PCA- For 2D Visualisation","752b3512":"#### Understanding the dimensions of the features generated from tf-idf vectorizatiom","710f63c5":"#### A negative value indicates a not so good clustering of the data which is also visible from the cluster scatter plot.\n- But we see that there is some increase in the score here... which indicates a little better clustering","441cf3bb":"## Now, performing some cluster validation to check the metrics for our cluster\n- As we do not have labelled data we cannot go ahead with homegenity score so we will go ahead with silhouette_score","a2ecb048":"### Plotting the elbow curve to chose the k-Value","94c4b6c8":"#### We can see that after k=6 there not much difference so we are safe to chose the value of k=6","5dfdd02f":"### Taking random sample from the data ~5k","f7840a2d":"#### Understanding the dimensions of the features generated from tf-idf vectorizatiom","ac8551bf":"## Clustering Analysis on the cleaned and processed Data From the EDA layer","cf79c1b3":"#### A negative value indicates a not so good clustering of the data which is also visible from the cluster scatter plot.\n- A plausible reason for this maybe that the data has lot of positive sentiment so the cluster appears to be biased.\n\n### One solution can be to sample reviews from positive and negative sentiment to analyze the clusters thereafter. This is implemented below."}}