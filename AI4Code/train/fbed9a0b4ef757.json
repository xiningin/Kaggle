{"cell_type":{"8243214a":"code","1a167ce0":"code","25b9eefe":"code","088d3224":"code","7012578b":"code","7a326594":"code","b9aa7f19":"code","4fba5983":"code","86dc9874":"code","2bce8bd0":"code","29922b77":"code","e49d4de4":"code","33ba5c21":"code","1c918c28":"code","354299f1":"code","48791cc1":"code","2ae0d24d":"code","e4be7d5c":"code","ffe54ec1":"code","bdbbefa9":"code","07df65f3":"code","00052578":"code","d7205806":"markdown","efb2afa1":"markdown","96b2117e":"markdown","f4166f31":"markdown","6b07aa8a":"markdown","14f7f7cc":"markdown","2797e280":"markdown","c71b7b42":"markdown","090294df":"markdown","a45ac25c":"markdown","e16742c5":"markdown","ea77cf77":"markdown","0210e6b6":"markdown","b9e27ece":"markdown","8ef0d3d5":"markdown","4f3273a9":"markdown","0e8e5e05":"markdown","80bf4f6e":"markdown","d3275d61":"markdown","2ad1a4bc":"markdown","ef72e652":"markdown","183834cc":"markdown","a294195c":"markdown","1bd7cf3c":"markdown","4de11219":"markdown","1205295d":"markdown","6aa8ba04":"markdown","d453c69c":"markdown","6060ca20":"markdown","e77f8641":"markdown","d26635da":"markdown","8248f89b":"markdown","08f9228c":"markdown","ce9cdd2b":"markdown"},"source":{"8243214a":"from IPython.display import YouTubeVideo\nYouTubeVideo('JzoIHdkFcQU', width=800, height=450) ","1a167ce0":"import tensorflow as tf\nfrom tensorflow import keras\n\nimport numpy as np\n\nprint(tf.__version__)","25b9eefe":"import matplotlib.pyplot as plt\ndef plot_history(history):\n  plt.figure()\n  plt.xlabel('Epoch')\n  plt.ylabel('Mean Abs Error [1000$]')\n  plt.plot(history.epoch, np.array(history.history['mae']), \n           label='Train')\n  plt.plot(history.epoch, np.array(history.history['val_mae']),\n           label = 'Val')\n  plt.legend()\n  plt.ylim([0,max(history.history['val_mae'])])\n\ndef plot_prediction(test_labels, test_predictions):\n  plt.figure()\n  plt.scatter(test_labels, test_predictions)\n  plt.xlabel('True Values [1000$]')\n  plt.ylabel('Predictions [1000$]')\n  plt.axis('equal')\n  plt.xlim(plt.xlim())\n  plt.ylim(plt.ylim())\n  _ = plt.plot([-100, 100],[-100,100])\n\n  plt.figure()\n  error = test_predictions - test_labels\n  plt.hist(error, bins = 50)\n  plt.xlabel(\"Prediction Error [1000$]\")\n  _ = plt.ylabel(\"Count\")","088d3224":"boston_housing = tf.keras.datasets.boston_housing\n\n(train_data, train_labels), (test_data, test_labels) = boston_housing.load_data()","7012578b":"print(\"Training set: {}\".format(train_data.shape))  # 404 examples, 13 features\nprint(\"Testing set:  {}\".format(test_data.shape))   # 102 examples, 13 features","7a326594":"print(train_data[0])  # Display sample features, notice the different scales","b9aa7f19":"import pandas as pd\n\ncolumn_names = ['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD',\n                'TAX', 'PTRATIO', 'B', 'LSTAT']\n\ndf = pd.DataFrame(train_data, columns=column_names)\ndf.head()","4fba5983":"print(train_labels[0:10])  # Display first 10 entries","86dc9874":"# Test data is *not* used when calculating the mean and std.\nmean = train_data.mean(axis=0)\nstd = train_data.std(axis=0)\ntrain_data = (train_data - mean) \/ std\ntest_data = (test_data - mean) \/ std\n\nprint(train_data[0])  # First training sample, normalized","2bce8bd0":"def build_model():\n  model = keras.Sequential([\n    keras.layers.Input(shape=(train_data.shape[1],)),                  \n    keras.layers.Dense(64, activation=tf.nn.relu),\n    keras.layers.Dense(64, activation=tf.nn.relu),\n    keras.layers.Dense(1)\n  ], name=\"MLP_model\")\n\n  optimizer = tf.keras.optimizers.RMSprop(0.001)\n\n  model.compile(loss='mse',\n                optimizer=optimizer,\n                metrics=['mae'])\n  return model\n\nmodel = build_model()\nmodel.summary()","29922b77":"EPOCHS = 500\n# Store training stats\nhistory = model.fit(train_data, train_labels, epochs=EPOCHS,\n                    validation_split=0.2, verbose=1)","e49d4de4":"plot_history(history)","33ba5c21":"[loss, mae] = model.evaluate(test_data, test_labels, verbose=0)\nprint(\"Testing set Mean Abs Error: ${:7.2f}\".format(mae * 1000))","1c918c28":"test_predictions = model.predict(test_data).flatten()\nplot_prediction(test_labels, test_predictions)","354299f1":"print(train_data.shape)\nprint(train_data[0].shape)\nprint(train_data[0])","48791cc1":"sample_size = train_data.shape[0] # number of samples in train set\ntime_steps  = train_data.shape[1] # number of features in train set\ninput_dimension = 1               # each feature is represented by 1 number\n\ntrain_data_reshaped = train_data.reshape(sample_size,time_steps,input_dimension)\nprint(\"After reshape train data set shape:\\n\", train_data_reshaped.shape)\nprint(\"1 Sample shape:\\n\",train_data_reshaped[0].shape)\nprint(\"An example sample:\\n\", train_data_reshaped[0])","2ae0d24d":"test_data_reshaped = test_data.reshape(test_data.shape[0],test_data.shape[1],1)","e4be7d5c":"def build_conv1D_model():\n\n  n_timesteps = train_data_reshaped.shape[1] #13\n  n_features  = train_data_reshaped.shape[2] #1 \n  model = keras.Sequential(name=\"model_conv1D\")\n  model.add(keras.layers.Input(shape=(n_timesteps,n_features)))\n  model.add(keras.layers.Conv1D(filters=64, kernel_size=7, activation='relu', name=\"Conv1D_1\"))\n  model.add(keras.layers.Dropout(0.5))\n  model.add(keras.layers.Conv1D(filters=32, kernel_size=3, activation='relu', name=\"Conv1D_2\"))\n  \n  model.add(keras.layers.Conv1D(filters=16, kernel_size=2, activation='relu', name=\"Conv1D_3\"))\n  \n  model.add(keras.layers.MaxPooling1D(pool_size=2, name=\"MaxPooling1D\"))\n  model.add(keras.layers.Flatten())\n  model.add(keras.layers.Dense(32, activation='relu', name=\"Dense_1\"))\n  model.add(keras.layers.Dense(n_features, name=\"Dense_2\"))\n\n\n  optimizer = tf.keras.optimizers.RMSprop(0.001)\n\n  model.compile(loss='mse',optimizer=optimizer,metrics=['mae'])\n  return model\n\nmodel_conv1D = build_conv1D_model()\nmodel_conv1D.summary()","ffe54ec1":"# Store training stats\nhistory = model_conv1D.fit(train_data_reshaped, train_labels, epochs=EPOCHS,\n                    validation_split=0.2, verbose=1)","bdbbefa9":"plot_history(history)","07df65f3":"[loss, mae] = model_conv1D.evaluate(test_data_reshaped, test_labels, verbose=0)\nprint(\"Testing set Mean Abs Error: ${:7.2f}\".format(mae * 1000))","00052578":"test_predictions = model_conv1D.predict(test_data_reshaped).flatten()\nplot_prediction(test_labels, test_predictions)","d7205806":"## Create the MLP model\n\nLet's build an MLP model. Here, we'll use a `Sequential` model with two densely connected hidden layers, and an output layer that returns a single, continuous value. The model building steps are wrapped in a function, `build_model`, since we'll create a second model, later on.","efb2afa1":"We need to **reshape** the Test data as well:","96b2117e":"The dataset contains 13 different features:\n\n1.   Per capita crime rate.\n2.   The proportion of residential land zoned for lots over 25,000 square feet.\n3.   The proportion of non-retail business acres per town.\n4.   Charles River dummy variable (= 1 if tract bounds river; 0 otherwise).\n5.   Nitric oxides concentration (parts per 10 million).\n6.   The average number of rooms per dwelling.\n7.   The proportion of owner-occupied units built before 1940.\n8.   Weighted distances to five Boston employment centers.\n9.   Index of accessibility to radial highways.\n10.  Full-value property-tax rate per $10,000.\n11.  Pupil-teacher ratio by town.\n12.  1000 * (Bk - 0.63) ** 2 where Bk is the proportion of Black people by town.\n13.  Percentage lower status of the population.\n\nEach one of these input data features is stored using a different scale. Some features are represented by a proportion between 0 and 1, other features are ranges between 1 and 12, some are ranges between 0 and 100, and so on. This is often the case with real-world data, and understanding how to explore and clean such data is an important skill to develop.\n","f4166f31":"Use the [pandas](https:\/\/pandas.pydata.org) library to display the first few rows of the dataset in a nicely formatted table:","6b07aa8a":"Visualize the model's training progress using the stats stored in the `history` object. We want to use this data to determine how long to train *before* the model stops making progress.","14f7f7cc":"<img src=\"https:\/\/github.com\/kmkarakaya\/ML_tutorials\/blob\/master\/images\/conv1d.gif?raw=true\" width=\"500\">","2797e280":"## Reshape Data sets\nAs you might remember, Conv1D layer expects input shape in 3D as\n\n  `[batch_size, time_steps, input_dimension]`\n\nHowever, current data is in the shape of\n\n`[batch_size, features]`\n\nSee below:","c71b7b42":"## Train the model\n\nThe model is trained for 500 epochs, and record the training and validation accuracy in the `history` object.","090294df":"##You can watch this notebook:","a45ac25c":"Although the model *might* converge without feature normalization, it makes training more difficult, and it makes the resulting model more dependant on the choice of units used in the input.","e16742c5":"After conversion, we have a train data set whose shape is\n\n  `[batch_size, time_steps, input_dimension]` ---> `[404, 13, 1]`\n\nThat is, each sample has **13 time steps with 1 input dimension**. You can also think as `each sample has 13 rows 1 column`!\n\n","ea77cf77":"Now, we can create Conv1D model as below.","0210e6b6":"## Observations\n\nSo far, we implemented an ***MLP model*** and a ***Conv1D model*** to handle the \"**Boston House Prices**\" regression problem.\n\nMLP model generates Mean Absolute Error:\n* in Validation is around **\\$2,600** whereas in Testing, it is about **$2900**\n\nConv1D model generates Mean Absolute Error:\n* in Validation is around **\\$2,400** whereas in Testing, it is about **$2,700**\n\nThus, **Conv1D** model is a **competitive** approach considering **MLP** model in the regression problem at hand. \n\nTo use a Conv1D model, you need to **reshape** the input as \n\n`[batch_size, time_steps, input_dimension]`\n\n**I hope this tutorial helps you to use Conv1D layer successfuly!**\n\n\n","b9e27ece":"The graph shows the average error **in Validation set** is about $2,600 dollars. Is this good? \n\nWell, \\$2,600 is not an insignificant amount when some of the labels are only $15,000.\n\nLet's see how did the model performs on the **Test set**:","8ef0d3d5":"## Train the model\n\nThe model is trained for 500 epochs, and record the training and validation accuracy in the `history` object.","4f3273a9":"## The Boston Housing Prices dataset\n\nThis [dataset](https:\/\/www.cs.toronto.edu\/~delve\/data\/boston\/bostonDetail.html) is accessible directly in TensorFlow. Download and shuffle the training set:","0e8e5e05":"In a *regression* problem, we aim to predict the output of a continuous value, like a price or a probability. Contrast this with a *classification* problem, where we aim to predict a discrete label (for example, where a picture contains an apple or an orange). \n\nThis notebook builds two different models to predict the median price of homes in a Boston suburb during the mid-1970s. To do this, I'll provide the models with some data points about the suburb, such as the crime rate and the local property tax rate.\n","80bf4f6e":"### Examples and features \n\nThis dataset has 506 total examples which are split between **404** training examples and **102** test examples:","d3275d61":"## Predict\n\nFinally, predict some housing prices using data in the testing set:","2ad1a4bc":"<a href=\"https:\/\/colab.research.google.com\/github\/kmkarakaya\/ML_tutorials\/blob\/master\/Conv1d_Predict_house_prices.ipynb\" target=\"_parent\"><img src=\"https:\/\/colab.research.google.com\/assets\/colab-badge.svg\" alt=\"Open In Colab\"\/><\/a>","ef72e652":"# Include Libraries and Auxiliary Functions","183834cc":"## Normalize features\n\nIt's recommended to normalize features that use different scales and ranges. For each feature, subtract the mean of the feature and divide by the standard deviation:","a294195c":"## Observations\n\nSo far, we implemented a MLP model to handle the \"**Boston House Prices**\" regression problem.\n\nMean Absolute Error in Validation is around **\\$2,600** whereas in Testing, it is about **$2900**","1bd7cf3c":"The graph shows the average error **in Validation set** is about $2,400 dollars. Is this good? \n\nWell, \\$2,400 is not an insignificant amount when some of the labels are only $15,000.\n\nLet's see how did the model performs on the **Test set**:","4de11219":"That is, in the current data set each sample has 13 features and no timesteps!\n\n**Basically, we convert features to timesteps**\n\nTo convert 2D of input data into a 3D input, we simply **reshape** as follows:","1205295d":"Visualize the model's training progress using the stats stored in the `history` object. We want to use this data to determine how long to train *before* the model stops making progress.","6aa8ba04":"## Predict\n\nFinally, predict some housing prices using data in the testing set:","d453c69c":"##Reminder\n* `Conv1D(filters=1, kernel_size=7, activation='relu')` ","6060ca20":"# 1 Dimensional Convolution (Conv1D) for Regression: Predict house prices \n\n[In a previous tutorial](https:\/\/youtu.be\/WZdxt9xatrY), we focus on  **1 Dimensional Convolution (Conv1D)** and discuss how it works in a simple example. ***As I received several questions*** about how to apply 1 Dimensional Convolution onto a regression problem, I develop this notebook. If you need to **refresh your information** about 1 Dimensional Convolution, please **watch** the previous tutorial [on my Youtube channel](https:\/\/youtu.be\/WZdxt9xatrY)\n\nThus, today, we will use **Keras Conv1d** layer for a regression problem. \nAs you might know, **Boston House Prices** data set is a well known data set. Below, we will **first** train a **Multi-Layer Perceptron (MLP) model** to predict house prices. Then, we will develop a model using **Keras Conv1D** layer. To train and test the Conv1D model, we will **reshape the train and test data** such that Conv1D can work on them.\n\nIf you are interested in **Deep Neural Networks** and want to **learn them by coding**, please **subcribe** to [my YouTube Channel](https:\/\/www.youtube.com\/channel\/UCrCxCxTFL2ytaDrDYrN4_eA\/playlists) or **follow** [my blog on Medium](https:\/\/medium.com\/@kmkarakaya). Do not forget to turn on **Notifications** so that you will be notified when ***new content is uploaded***.\n\nYou can access this **Colab Notebook** using [the link](https:\/\/colab.research.google.com\/drive\/1zjh0tUPYJYgJJunpLC9fW5uf--O0LKeZ?usp=sharing) given in the video descriptions below.\n\nIf you are ready, let's get started!\n\n\n","e77f8641":"# Notice\n***As a base model***, I will use the **TensorFlow official example** for ***MLP model*** and compare its performance with **my Conv1D model**. Thus, we will be able to observe the relative success of **Conv1D model** with respect to **a professional sample model**.\n\nYou can access the original notebook [\"Predict house prices: regression\" with Multi-layer Perceptron here.](https:\/\/colab.research.google.com\/github\/MarkDaoust\/models\/blob\/add-regression-plots\/samples\/core\/tutorials\/keras\/basic_regression.ipynb) \n\nIf you run this notebook,  you would generate mean absolute error values different than the reported ones here due to stochastic nature of ANNs.  ","d26635da":"### Labels\n\nThe labels are the house prices in thousands of dollars. (You may notice the mid-1970s prices.)","8248f89b":"https:\/\/youtu.be\/JzoIHdkFcQU\n\n","08f9228c":"## Create the Conv1D model\n\nLet's build an Conv1D model. Here, we'll use a `Sequential` model with 3 Conv1D layers, one MaxPooling1D layer, and an output layer that returns a single, continuous value. The model building steps are wrapped in a function, `build_model` as we did above.","ce9cdd2b":"# What is regression? "}}