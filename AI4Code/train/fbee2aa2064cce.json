{"cell_type":{"f1d3415f":"code","589d1850":"code","b86601c9":"code","b959c20e":"code","e1dc842b":"code","7cf051bf":"code","554859eb":"code","5a80464e":"code","8b1e0173":"code","c696b5d9":"code","103c7043":"code","1b6e9cc5":"code","966f4491":"code","bfeae006":"code","28903dd5":"code","b0b91d75":"code","6df58e60":"code","179c53bb":"code","51f24e0e":"code","bfdba2ab":"code","df1668fd":"code","b4be6fdb":"code","2a0c2b6d":"code","24e3d7ee":"code","708d11b8":"code","70b33ccc":"markdown","d044067c":"markdown","9a97285f":"markdown","efa51e2f":"markdown","52b61fbb":"markdown","c75b8c37":"markdown","dafc7d4f":"markdown","6f02ac08":"markdown","9371491a":"markdown","5686078a":"markdown","987f5162":"markdown","5f2cfd96":"markdown","b63a1046":"markdown","23be3a5c":"markdown","9fef3a96":"markdown","ce1e6629":"markdown","ea1181cf":"markdown","662a170d":"markdown","c4ec591f":"markdown","aa5c945b":"markdown","23257387":"markdown","99db200f":"markdown","6099a383":"markdown","d860ec26":"markdown","79f1524e":"markdown","b261e921":"markdown"},"source":{"f1d3415f":"# --- CSS STYLE ---\nfrom IPython.core.display import HTML\ndef css_styling():\n    styles = open(\"..\/input\/styles\/alerts.css\", \"r\").read()\n    \n    return HTML(\"<style>\"+styles+\"<\/style>\")\ncss_styling()","589d1850":"# Libraries\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.patches as patches\nimport matplotlib.pyplot as plt\nfrom scipy.stats import pearsonr\nfrom matplotlib.offsetbox import AnnotationBbox, OffsetImage\nimport plotly.express as px\nfrom sklearn.cluster import KMeans\nfrom skimage import morphology, measure\nfrom skimage import morphology, measure\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","b86601c9":"data = pd.read_csv(\"..\/input\/breast-cancer-wisconsin-data\/data.csv\")\ndata.head()","b959c20e":"data.drop('Unnamed: 32', axis = 1, inplace = True) # drop last unnamed empty column","e1dc842b":"# Checking for the class imbalance\nfig = plt.figure(figsize = (10, 6))\naxis = sns.countplot(x = 'diagnosis', data = data);\naxis.set_title('Class Distribution for the Target Feature', size = 20);\n\nfor patch in axis.patches:\n    axis.text(x = patch.get_x() + patch.get_width()\/2, y = patch.get_height()\/2, \n            s = f\"{np.round(patch.get_height()\/len(data)*100, 1)}%\", \n            ha = 'center', size = 40, rotation = 0, weight = 'bold' ,color = 'white')\n    \naxis.set_xlabel('Diagnosis', size = 14)\naxis.set_ylabel('Count', size = 14);","7cf051bf":"data.describe()","554859eb":"y = data.diagnosis                          \nlist = ['id','diagnosis']\nx = data.drop(list, axis = 1)","5a80464e":"x_standard = (x - x.mean()) \/ (x.std()) # x standardised","8b1e0173":"x_standard.columns","c696b5d9":"mean_cols = [col for col in x_standard.columns if '_mean' in col]\nse_cols = [col for col in x_standard.columns if '_se' in col]\nworst_cols = [col for col in x_standard.columns if '_worst' in col]","103c7043":"df = pd.concat([y,x_standard[mean_cols]],axis=1)\n\nax = sns.pairplot(df, hue=\"diagnosis\")\nhandles = ax._legend_data.values()\nlabels = ax._legend_data.keys()\nax.fig.legend(handles=handles, labels=labels, loc='upper center', ncol=1)","1b6e9cc5":"df = pd.concat([y,x_standard[se_cols]],axis=1)\n\nax = sns.pairplot(df, hue=\"diagnosis\", palette=\"husl\")\nhandles = ax._legend_data.values()\nlabels = ax._legend_data.keys()\nax.fig.legend(handles=handles, labels=labels, loc='upper center', ncol=1)","966f4491":"df = pd.concat([y,x_standard[worst_cols]],axis=1)\n\nax = sns.pairplot(df, hue=\"diagnosis\", palette=\"Paired\")\nhandles = ax._legend_data.values()\nlabels = ax._legend_data.keys()\nax.fig.legend(handles=handles, labels=labels, loc='upper center', ncol=1)","bfeae006":"f,ax = plt.subplots(figsize=(18, 18))\nsns.heatmap(x_standard.corr(), annot=True, linewidths=.5, fmt= '.1f',ax=ax)","28903dd5":"df = pd.concat([y,x_standard[mean_cols]],axis=1)\ndf = pd.melt(df ,id_vars=\"diagnosis\",\n                    var_name=\"features\",\n                    value_name='value')\n\nplt.figure(figsize=(10,10))\nsns.swarmplot(x=\"features\", y=\"value\", hue=\"diagnosis\", data=df)\nplt.xticks(rotation=90)","b0b91d75":"df = pd.concat([y,x_standard[se_cols]],axis=1)\ndf = pd.melt(df ,id_vars=\"diagnosis\",\n                    var_name=\"features\",\n                    value_name='value')\n\nplt.figure(figsize=(10,10))\nsns.swarmplot(x=\"features\", y=\"value\", hue=\"diagnosis\", data=df, palette=\"husl\")\nplt.xticks(rotation=90)","6df58e60":"df = pd.concat([y,x_standard[worst_cols]],axis=1)\ndf = pd.melt(df ,id_vars=\"diagnosis\",\n                    var_name=\"features\",\n                    value_name='value')\n\nplt.figure(figsize=(10,10))\nsns.swarmplot(x=\"features\", y=\"value\", hue=\"diagnosis\", data=df, palette=\"Paired\")\nplt.xticks(rotation=90)","179c53bb":"# Dropping coloumns according to the conclusion in the paragraph above.\n\ndrop_columns = [\"radius_se\", \"radius_mean\", \"radius_worst\", \n               \"perimeter_se\", \"perimeter_mean\", \"perimeter_worst\",\n               \"compactness_se\", \"compactness_mean\", \"compactness_worst\",\n               \"concave points_se\", \"concave points_mean\", \"concave points_worst\",\n               \"area_worst\", \"texture_worst\"]\n\nx_final = x_standard.drop(drop_columns, axis=1)","51f24e0e":"x_final.head()","bfdba2ab":"# Installing necessary libraries\n\nfrom numpy import loadtxt\nfrom xgboost import XGBClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, f1_score, confusion_matrix","df1668fd":"# split data into train and test sets\nseed = 10\ntest_size = 0.30\nX_train, X_test, y_train, y_test = train_test_split(x_final, y, test_size=test_size, random_state=seed)","b4be6fdb":"model = XGBClassifier()\nmodel.fit(X_train, y_train)","2a0c2b6d":"y_pred = model.predict(X_test)","24e3d7ee":"accuracy = accuracy_score(y_test, y_pred)\nprint(\"Accuracy: %.2f%%\" % (accuracy * 100.0))","708d11b8":"cm = confusion_matrix(y_test, y_pred)\n\nax = plt.subplot()\nsns.heatmap(cm, annot=True, fmt='g', ax=ax);  \n\n# labels, title and ticks\nax.set_xlabel('Predicted Labels');ax.set_ylabel('True Labels'); \nax.set_title('Confusion Matrix'); \nax.xaxis.set_ticklabels(['B', 'M']); ax.yaxis.set_ticklabels(['M', 'B']);","70b33ccc":"> The mean, standard error and \"worst\" or largest (mean of the three largest values) of all features were computed for each image, resulting in 30 features. So we can group the columns by these three categories resulting in groups of 10 simpler. This will make visualisations simpler. ","d044067c":"<h4>\ud83d\udccd Feature Selection<\/h4>\n\n> * Looking at the <code>radius<\/code>, <code>perimeter<\/code> and <code>area<\/code> in all three swarm plots, I can choose <code>area_mean<\/code>, <code>area_se<\/code> and <code>area_worst<\/code>. Bceause 'B' and 'M' observations look separated for <code>area<\/code> features, for the most part.\n> * Similarly, for <code>compactness<\/code>,  <code>concavity<\/code> and <code>concave.points<\/code>, I will choose  <code>concavity_mean<\/code>, <code>concavity_se<\/code> and <code>concavity_worst<\/code>.\n> * Between <code>area_mean<\/code> and <code>area_worst<\/code>, I will choose <code>area_mean<\/code>.\n> * And between, <code>texture_mean<\/code> and <code>texture_worst<\/code>, I will go with <code>texture_mean<\/code>.\n> ","9a97285f":"These plots show the distribution of the observations based on two features and how they can be differentiated according to the target column. The diagonals show the degree of separation of a single feature in relation to the target column and what is the extent of overlap of the two classes of the target columns (M and B).","efa51e2f":"***Graph Interpretation:*** <br>\nWe do have as well tight superposition for some of the values, like <code>symmetry_se<\/code>, <code>smoothness_se<\/code>. So these features might contribute less to our classification goal. ","52b61fbb":"First off, there is an **'Unnamed'** column with no values. We will drop this column.","c75b8c37":"***Graph Interpretation:*** <br>\nFor <code>area_mean<\/code>, <code>perimeter_mean<\/code>, <code>concave.points_mean<\/code> we have good separations. But there is a superposition in the case of <code>fractal_dimension_mean<\/code>.","dafc7d4f":"<h3>Swarm Plots<\/h3>","6f02ac08":"***Swarm plot*** is a categorical scatterplot with non-overlapping points.<br>\nThis function is similar to stripplot(), but the points are adjusted (only along the categorical axis) so that they don\u2019t overlap. This gives a better representation of the distribution of values, but it does not scale well to large numbers of observations. This style of plot is sometimes called a **\u201cbeeswarm\u201d**.","9371491a":"<h1><center>Breast Cancer - EDA and Binary Classification \ud83d\udcc9\ud83d\udd0d<\/center><\/h1>","5686078a":"For the sake of simplicity in the upcoming steps, we are denoting all feature columns as <code>x<\/code> and the target column as <code>y<\/code>.","987f5162":"<div class=\"alert success-alert\">\n<b>Breast cancer<\/b> is the most common invasive cancer in women and the second leading cause of cancer death in women after lung cancer.\n    <br><br>It arises in the lining cells (epithelium) of the ducts (85%) or lobules (15%) in the glandular tissue of the breast. Initially, the cancerous growth is confined to the duct or lobule (\u201cin situ\u201d) where it generally causes no symptoms and has minimal potential for spread (metastasis).\n\n<bR>Most types of breast cancer are easy to diagnose by microscopic analysis of a sample - or biopsy - of the affected area of the breast. Also, there are types of breast cancer that require specialized lab exams.\n<\/div>","5f2cfd96":"Let's look at the basic statistics of the data.","b63a1046":"<h3>Pairplots<\/h3>","23be3a5c":"> <h3>This notebook is a work in progress.<\/h3>","9fef3a96":"<div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n<h1 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='background:orange; border:0; color:white' role=\"tab\" aria-controls=\"home\"><center>2. Feature Selection \ud83d\udcca<\/center><\/h1>","ce1e6629":"<h4>\ud83d\udccd Inference<\/h4>\n\n> * <code>radius<\/code>, <code>perimeter<\/code> and <code>area<\/code> (all three - 'mean', 'se' and 'worst') have high positive correlation among themselves and with number of concave points This is expected since as high values of these features are associated with malignancy.\n> * <code>compactness<\/code>,  <code>concavity<\/code> and <code>concave.points<\/code> (all three - 'mean', 'se' and 'worst') have high positive correlation.\n> * <code>texture_mean<\/code> and <code>texture_worst<\/code> are correlated.\n> * <code>area_mean<\/code>, <code>area_worst<\/code> are perfectly correlated. <br>\n\nThe question is: *which variables to choose and which variables to discard?* To choose my features, I will look at the the **swarm plots** and try to decide which features will represent by data best.","ea1181cf":"<div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n<h1 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='background:orange; border:0; color:white' role=\"tab\" aria-controls=\"home\"><center>1. EDA and Preprocessing \ud83d\udd0d \ud83d\udee0<\/center><\/h1>","662a170d":"Machine learning algorithms like linear regression, logistic regression, neural network, etc. that use **gradient descent** as an optimization technique require data to be scaled. Take a look at the formula for gradient descent below:\n\n![Capture.PNG](attachment:7a75c8c6-000f-484d-8377-eabc076f8505.PNG)\n\nThe presence of feature value X in the formula will affect the step size of the gradient descent. The difference in ranges of features will cause different step sizes for each feature. To ensure that the gradient descent moves smoothly towards the minima and that the steps for gradient descent are updated at the same rate for all the features, we scale the data before feeding it to the model.\n\n<br>For feature scaling, we will use ***standardization***, a scaling technique where the values are centered around the mean with a unit standard deviation. This means that the mean of the attribute becomes zero and the resultant distribution has a unit standard deviation.<br>\n\n\nYou can read more about standardisation and normalisation, another scaling technique, in this [great article](https:\/\/www.analyticsvidhya.com\/blog\/2020\/04\/feature-scaling-machine-learning-normalization-standardization\/).","c4ec591f":"<div class=\"alert simple-alert\">\n\ud83c\udfaf <b>Goal<\/b>: Conduct comprehensive EDA on this dataset. And later, build a binary classification model to predict the diagnosis as M = malignant or B = benign.\n\n","aa5c945b":"<h3>Correlation Matrix<\/h3>","23257387":"We are getting and accuracy of 97%. The confusion matrix gives us a more holistic view of how well our classification model is performing and what kinds of errors it is making. As we can see, we are getting 5 wrong predictions.\n\nWe can use other feature selection methods and check how our classifier performs.","99db200f":"Now, let's take a look at how our target column, <code>diagnosis<\/code>, is distributed.","6099a383":"> <h4>\ud83d\udccd Points to be noted:<\/h4>\n> 1. There are no missing values in the data. <br>\n> 2. We don't have to consider the ID column for our classification analysis, which we will come to later on.<br>\n> 3. The difference in range of the values of these features is very high. For example, the maximum value of <code>radius_mean<\/code> is <code>28.11<\/code>, for <code>perimeter_mean<\/code> it's <code>188.5<\/code>, and for <code>area_mean<\/code>, it goes all the way up to <code>2501<\/code>. We need to normalise the data to bring them to a uniform scale.","d860ec26":"We will now fit a binary classification model to this new datatset. We are going to use **XGBoost**.","79f1524e":"## Feature Scaling \u2696","b261e921":"***Graph Interpretation:*** <br>\nHere, <code>radius_worst<\/code> and <code>perimeter_worst<\/code>. plots seem similar. And <code>concavity_worst<\/code> and <code>concavity.points_worst<\/code> as well. We can infer more information about the similarities when we check the correlation between features."}}