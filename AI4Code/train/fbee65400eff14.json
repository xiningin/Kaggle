{"cell_type":{"a60f040f":"code","5bbfa96f":"code","32f1d318":"code","d695a8c4":"code","e6f7e1fb":"code","7a6103e1":"code","122a78b8":"code","799a9513":"code","90f2f3b8":"code","dc1b000f":"code","4f79d1aa":"code","7b221ab8":"code","46e8e4e7":"code","a478a9d9":"code","83523d2b":"code","ceda72c5":"code","f0abb0f1":"code","f7984f50":"code","125b8ae4":"code","e0239998":"code","7fe2390e":"code","a7ecbb53":"code","34d00534":"markdown","003152db":"markdown","a48ab864":"markdown","d315050b":"markdown","2d1cae00":"markdown","fe978de5":"markdown","2f0dc117":"markdown","47d62369":"markdown","36bd672c":"markdown","4d6d122f":"markdown","63ad0f0d":"markdown","732dbe56":"markdown","76543a05":"markdown","05e112f9":"markdown","d0fe3523":"markdown","53f679b9":"markdown","0d63ed6e":"markdown","f7d40c73":"markdown","2a980ce9":"markdown","913bc285":"markdown","ac3d6677":"markdown","f632f1c2":"markdown","841e3046":"markdown","e1edd3f3":"markdown","22abccdf":"markdown","87e62386":"markdown","31adcc8f":"markdown","6c3bea28":"markdown","43740f03":"markdown","b415e956":"markdown","63c35c03":"markdown","0f10c978":"markdown","ce1db808":"markdown","c42ebd79":"markdown","01fbdb8c":"markdown","4c1b73e0":"markdown","3e1f5e24":"markdown","ed8b0798":"markdown"},"source":{"a60f040f":"import tensorflow as tf\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nfrom keras.wrappers.scikit_learn import KerasClassifier\nfrom sklearn.model_selection import GridSearchCV\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.utils.vis_utils import plot_model\nfrom IPython.display import SVG\nfrom keras.utils import model_to_dot\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.preprocessing import LabelEncoder\nimport seaborn as sns\n\ndataset = pd.read_csv('..\/input\/breast-cancer-wisconsin-data\/data.csv')","5bbfa96f":"def analyze(data):\n    \n  # View features in data set\n  print(\"Dataset Features\")\n  print(data.columns.values)\n  print(\"=\" * 30)\n    \n  # View How many samples and how many missing values for each feature\n  print(\"Dataset Features Details\")\n  print(data.info())\n  print(\"=\" * 30)\n    \n  # view distribution of numerical features across the data set\n  print(\"Dataset Numerical Features\")\n  print(data.describe())\n  print(\"=\" * 30)\n    \n  # view distribution of categorical features across the data set\n  print(\"Dataset Categorical Features\")\n  print(data.describe(include=['O']))\n  print(\"=\" * 30)","32f1d318":"analyze(dataset)","d695a8c4":"sns.pairplot(dataset, hue=\"diagnosis\", size= 2.5)","e6f7e1fb":"X = dataset.iloc[:,2:32] \ny = dataset.iloc[:,1] ","7a6103e1":"print(\"Earlier: \")\nprint(y[100:110])\n\nlabelencoder_Y = LabelEncoder()\ny = labelencoder_Y.fit_transform(y)\n\nprint()\nprint(\"After: \")\nprint(y[100:110])","122a78b8":"from sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)","799a9513":"# Scale values from faster convergence\nfrom sklearn.preprocessing import StandardScaler\n\nsc = StandardScaler()\nX_train = sc.fit_transform(X_train)\nX_test = sc.transform(X_test)","90f2f3b8":"def build_classifier(optimizer):\n  classifier = Sequential()\n  classifier.add(Dense(units = 16, kernel_initializer = 'uniform', activation = 'relu', input_dim = 30))\n  classifier.add(Dense(units = 8, kernel_initializer = 'uniform', activation = 'relu'))\n  classifier.add(Dense(units = 6, kernel_initializer = 'uniform', activation = 'relu'))\n  classifier.add(Dense(units = 1, kernel_initializer = 'uniform', activation = 'sigmoid'))\n  classifier.compile(optimizer = optimizer, loss = 'binary_crossentropy', metrics = ['accuracy'])\n  return classifier","dc1b000f":"classifier = KerasClassifier(build_fn = build_classifier)","4f79d1aa":"parameters = {'batch_size': [1, 5],\n               'epochs': [100, 120],\n               'optimizer': ['adam', 'rmsprop']}","7b221ab8":"# Cross validation\ngrid_search = GridSearchCV(estimator = classifier,\n                            param_grid = parameters,\n                            scoring = 'accuracy',\n                            cv = 10)","46e8e4e7":"# Get best model\n# Note: this may take some time\ngrid_search = grid_search.fit(X_train, y_train)","a478a9d9":"classifier = Sequential()","83523d2b":"# Make the best classifier as we received earlier\nclassifier.add(Dense(units = 16, kernel_initializer = 'uniform', activation = 'relu', input_dim = 30))\nclassifier.add(Dense(units = 8, kernel_initializer = 'uniform', activation = 'relu'))\nclassifier.add(Dense(units = 6, kernel_initializer = 'uniform', activation = 'relu'))\nclassifier.add(Dense(units = 1, kernel_initializer = 'uniform', activation = 'sigmoid'))","ceda72c5":"classifier.compile(optimizer = 'rmsprop', loss = 'binary_crossentropy', metrics = ['accuracy'])","f0abb0f1":"classifier.fit(X_train, y_train, batch_size = 1, epochs = 100, verbose=1)","f7984f50":"y_pred = classifier.predict(X_test)\n# If probab is >= 0.5 classify as 1 or 0\ny_pred = [ 1 if y>=0.5 else 0 for y in y_pred ]","125b8ae4":"# Finally use scikit-learn to build a confusion_matrix\ncm = confusion_matrix(y_test, y_pred)\nprint(cm)","e0239998":"sns.heatmap(cm,annot=True)","7fe2390e":"# (True positive + True Negative)\/Total\naccuracy = (cm[0][0]+cm[1][1])\/(cm[0][0]+cm[0][1]+cm[1][0]+cm[1][1])\nprint(\"Accuracy: \"+ str(accuracy*100)+\"%\")","a7ecbb53":"plot_model(classifier, to_file='model_plot.png', show_shapes=True, show_layer_names=True)","34d00534":"# Breast cancer Prediction\n# -[Rishit Dagli](rishitdagli.ml)","003152db":"### Import required libraries","a48ab864":"This is just a simple code which stores $X$ or the features and $y$ or the labels in different variables","d315050b":"Search for the best model in the complete matrix","2d1cae00":"### Evaluate model","fe978de5":"Finally fit the classifier to training data","2f0dc117":"So, now you can see a png file of the model architecture created as `model_plot.png`","47d62369":"### Use Cross Vaildation to obtain best model","36bd672c":"Create a visualization of our ANN to see its layers at a glance","4d6d122f":"Scaling the values is not actually compulsory but I would recommend one to do it for a faster convergence, so we use `sklearn` to help us in this","63ad0f0d":"We cannot complete any AI algorithm or model without assesing it so we now measure the accuracy of our model.<br>\nThis happens to be a classification model so we can simply use the accuracy formula:<br>\n$Accuracy=\\frac{True Positive + True Negative}{Total}$","732dbe56":"### Make a feature pairplot","76543a05":"### Analyze the dataset","05e112f9":"### Finally build model according to above obtained results","d0fe3523":"### Seperate the the features and laabels","53f679b9":"Before tuning our hyper parameters we surely want to test the data with various of them and choose the best one so define few options for the model","0d63ed6e":"### Experiment with various hyper parameters","f7d40c73":"### Fit the classifier to the data","2a980ce9":"First make a y predictions list for all entries in x test list as we have the probabilities and not 1 or 0 corresponding to `M` and `B`\n<br>\nWe now receive the probabilities, This is the pseudo code used:\n* if the `prob >= 0.5`\n* we classify it as `1`\n* else `0`\n","913bc285":"Create a heat map of the confusion matrix","ac3d6677":"We had our labels as `M` and `B` depicting malignant and benign respectively. As these are `strings` or `char` as you might say and we are only concerned with numbers so we encode such that <br>\n\n\n*   All `M = 1`\n*   All `B = 0`\n\nFor this we use the `LabelEncoder` class.\n","f632f1c2":"We will now make a feature wise pairplot meaning we will plot labels $x_1$, $x_2$, $...$ and or label $y$ with each other. Where $x$ and $y$ have their usual meaning. We will use `seaborn` to help us with this. A pair plot allows us to see both distribution of single variables and relationships between two variables . Pair plots are a great method to identify trends for follow-up analysis. So this again becomes an important step for us.","841e3046":"Now we build the confuson matrix for easy interpretation of model accuracy. A confusion matrix is very helpful in interpreting our model results in this manner.<br><br>\n![](https:\/\/drive.google.com\/uc?id=1SaflBpLkDz753uijkjzGK70KpMCmp520)","e1edd3f3":"Complete the classifier","22abccdf":"We finally build a `tensorflow`, `keras` classifier","87e62386":"### Encode the **labels** to 1, 0","31adcc8f":"We use `matplotlib` and `seaborn` to create some wonderful visualizations of our data.<br>\n`pandas` to read our data and know some insights about the data, efficiently. With `pandas` by our side we can do many things with just simple functions which we will take a look at in the later part.<br>\n`sklearn` to<br>\n- Select the model with best hyper parameters\n- Encode the labels i.e. M and B\n- Print a confusion matrix with test data results\n- Make a train \/ test split easily\n- Scale values\n<br><br>\n`tensorflow` and `keras` to create our model (ANN) and make some plots of it","6c3bea28":"### Make a 80\/ 20 train, test split","43740f03":"I have used the Wisconsin Breast cancer dataset from the [UCI Machine Learning Repository](https:\/\/archive.ics.uci.edu\/ml) availaible [here](https:\/\/archive.ics.uci.edu\/ml\/datasets\/breast+cancer+wisconsin+(diagnostic)).\n<br>\nI have used Artificial Neural Networks for this problem and found out the best hyper parameters using cross validation.<br>For more details read my research paper [here](https:\/\/iarjset.com\/papers\/machine-learning-as-a-decision-aid-for-breast-cancer-diagnosis\/).\n<br><br>\n![](https:\/\/drive.google.com\/uc?id=1ETVCulfECkSBOcZXtXLnaDUIjnpoZMu5)\n<br>\n<br>\n![](https:\/\/drive.google.com\/uc?id=1mIKCJ6wyvSMrx-oa4IFRGG5FUR8pPOKN)\n\n---\n\n\n<font color=\"red\">**Data Set Information:**<\/font>\n\nFeatures are computed from a digitized image of a fine needle aspirate (FNA) of a breast mass. They describe characteristics of the cell nuclei present in the image. A few of the images can be found [here](www.cs.wisc.edu\/~street\/images\/)\n\n\n\n---\n\n\n\n<font color=\"red\">**Attribute Information:**<\/font>\n\n1) ID number<br>\n2) Diagnosis (M = malignant, B = benign)<br>\n3-32)<br>\n<br>\n<font color=\"light green\">*Ten real-valued features are computed for each cell nucleus:*<\/font><br>\n\na) radius (mean of distances from center to points on the perimeter)<br>\nb) texture (standard deviation of gray-scale values)<br>\nc) perimeter<br>\nd) area<br>\ne) smoothness (local variation in radius lengths)<br>\nf) compactness ($\\frac{perimeter^2}{area} - 1.0$)<br>\ng) concavity (severity of concave portions of the contour)<br>\nh) concave points (number of concave portions of the contour)<br>\ni) symmetry<br>\nj) fractal dimension (\"coastline approximation\" - 1)<br>\n","b415e956":"We use `BCE` or binary cross entropy which is suited best to sigmoid.\n<br> $BCE=CE_1+CE_2$\n<br> $BCE=-y log \\hat y - (1-y) log (1 - \\hat y)$","63c35c03":"So we make an ANN now with\n\n* Input Layer - **16** neurons and `ReLu` activator\n* Hidden Layer 1 - **8** neurons and `ReLu` activator\n* Hidden Layer 2 - **6** neurons and `ReLu` activator\n* Output Layer - **1** neuron (ok that was obvious !) and `sigmoid` activator\n\n","0f10c978":"Now print out the accuracy<br> $Accuracy=\\frac{True Positive + True Negative}{Total}$","ce1db808":"In the above step we already obtained the best model for this problem so now we are almost done all we need to do is build a classifier according to obtained results","c42ebd79":"Making a train test split is important for any AI problem without which we do not know how our model would perform to unseen values and also not overfit the data","01fbdb8c":"### Build a classifier","4c1b73e0":"Cross Vaalidation is a wonderful technique which often comes to our rescue while selecting the best model so we use a 10 fold CV here.<br>\nWe could also have used `AIC`, `BIC` or even Mallows $C_p$ if the CV does not give us a good result but that's not the case here","3e1f5e24":"![](https:\/\/drive.google.com\/uc?id=16c6UtqGFDrJNordq9lIursSR0Ks5W8k6)","ed8b0798":"We will now analyze our data :\n\n\n1.Print the features of dataset which are also mentioned above<br>\n2.View how many samples and missing values there are for each feature and display them accordingly<br>\nWe here see the missing samples and values:\n```\nRangeIndex: 568 entries, 0 to 567\nData columns (total 32 columns):\n842302      568 non-null int64\nM           568 non-null object\n17.99       568 non-null float64\n10.38       568 non-null float64\n122.8       568 non-null float64\n1001        568 non-null float64\n0.1184      568 non-null float64\n0.2776      568 non-null float64\n...\n...\n0.4601      568 non-null float64\n0.1189      568 non-null float64\ndtypes: float64(30), int64(1), object(1)\nmemory usage: 142.1+ KB\nNone\n```\n\n3.View numerical features of data set we will majorly focus on `mean`, `count`, `std`, `min value`, `max value`, `upper quartile`, `inter quartile` and `lower quartile`<br>\nFor this all we need to do is `dataset.describe()` So this justifies our use of `pandas` library.<br>\n4.We will now take a look at the label features specially `count`, `unique`, `top` and `frequency`. The `count` parameter just tells us the number of entries, the `unique` parameter is important.\nHere we receive -\n```\nunique: 2\n```\nWhich tells us to perform2 class classification.<br>\nThe `top` parameter is often used to check biases in the data set itself.\n\n\n"}}