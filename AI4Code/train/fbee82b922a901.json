{"cell_type":{"29a29c9e":"code","537f0249":"code","b0087aa2":"code","df32ff21":"code","d26e0214":"code","f4f8ede9":"code","0262d573":"code","6fc6cc8c":"code","869e4cad":"code","a1f622b7":"code","af79206f":"code","5722cc71":"code","8e243767":"code","e39c7556":"code","82084fcb":"markdown","dc39189c":"markdown","60331133":"markdown","f2840b82":"markdown","f1564eb5":"markdown","1fc0493f":"markdown","deedf03e":"markdown","acbe49ee":"markdown"},"source":{"29a29c9e":"import os\nbase_dir = '..\/input\/cats-dogs\/cats_and_dogs_filtered'\n\ntrain_dir = os.path.join(base_dir, 'train')\nvalidation_dir = os.path.join(base_dir, 'validation')\n\n# Directory with our training cat\/dog pictures\ntrain_cats_dir = os.path.join(train_dir, 'cats')\ntrain_dogs_dir = os.path.join(train_dir, 'dogs')\n\n# Directory with our validation cat\/dog pictures\nvalidation_cats_dir = os.path.join(validation_dir, 'cats')\nvalidation_dogs_dir = os.path.join(validation_dir, 'dogs')\n","537f0249":"train_cat_fnames = os.listdir(train_cats_dir)\ntrain_dog_fnames = os.listdir(train_dogs_dir)\n\nprint(train_cat_fnames[:10])\nprint(train_dog_fnames[:10])","b0087aa2":"print('total training cat images :', len(os.listdir(train_cats_dir)))\nprint('total training dog images :', len(os.listdir(train_dogs_dir)))\n\nprint('total validation cat images :', len(os.listdir(validation_cats_dir)))\nprint('total validation dog images :', len(os.listdir(validation_dogs_dir)))","df32ff21":"%matplotlib inline\n\nimport matplotlib.image as mpimg\nimport matplotlib.pyplot as plt\n\nnrows = 4\nncols = 4\n\npic_index = 0 ","d26e0214":"# Set up matplotlib fig, and size it to fit 4x4 pics\nfig = plt.gcf()\nfig.set_size_inches(ncols*4, nrows*4)\n\npic_index+=8\n\nnext_cat_pix = [os.path.join(train_cats_dir, fname) \n                for fname in train_cat_fnames[pic_index-8:pic_index] \n               ]\n\nnext_dog_pix = [os.path.join(train_dogs_dir, fname) \n                for fname in train_dog_fnames[ pic_index-8:pic_index]\n               ]\n\nfor i, img_path in enumerate(next_cat_pix+next_dog_pix):\n  # Set up subplot; subplot indices start at 1\n  sp = plt.subplot(nrows, ncols, i + 1)\n  sp.axis('Off') # Don't show axes (or gridlines)\n\n  img = mpimg.imread(img_path)\n  plt.imshow(img)\n\nplt.show()\n","f4f8ede9":"import tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras.models import Sequential \nfrom tensorflow.keras.layers import Conv2D\nfrom tensorflow.keras.layers import MaxPooling2D\nfrom tensorflow.keras.layers import Flatten\nfrom tensorflow.keras.layers import Dense\nfrom tensorflow.keras.optimizers import RMSprop\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator","0262d573":"model = Sequential([\n    Conv2D(16, (3,3), activation='relu', input_shape=(150, 150, 3)),\n    MaxPooling2D(2,2),\n    Conv2D(32, (3,3), activation='relu'),\n    MaxPooling2D(2,2), \n    Conv2D(64, (3,3), activation='relu'), \n    MaxPooling2D(2,2),\n    Flatten(), \n    Dense(512, activation='relu'), \n    # Only 1 output neuron. It will contain a value from 0-1 where 0 for 1 class ('cats') and 1 for the other ('dogs')\n    Dense(1, activation='sigmoid')  \n])","6fc6cc8c":"model.summary()","869e4cad":"model.compile(optimizer=RMSprop(lr=0.001),\n              loss='binary_crossentropy',\n              metrics = ['accuracy'])","a1f622b7":"train_datagen = ImageDataGenerator(\n      rescale=1.\/255,\n      rotation_range=40,\n      width_shift_range=0.2,\n      height_shift_range=0.2,\n      shear_range=0.2,\n      zoom_range=0.2,\n      horizontal_flip=True,\n      fill_mode='nearest')\ntest_datagen  = ImageDataGenerator( rescale = 1.0\/255. )\n\n# --------------------\n# Flow training images in batches of 20 using train_datagen generator\n# --------------------\ntrain_generator = train_datagen.flow_from_directory(train_dir,\n                                                    batch_size=20,\n                                                    class_mode='binary',\n                                                    target_size=(150, 150))     \n# --------------------\n# Flow validation images in batches of 20 using test_datagen generator\n# --------------------\nvalidation_generator =  test_datagen.flow_from_directory(validation_dir,\n                                                         batch_size=20,\n                                                         class_mode  = 'binary',\n                                                         target_size = (150, 150))\n","af79206f":"history = model.fit(train_generator,\n                              validation_data=validation_generator,\n                              steps_per_epoch=100,\n                              epochs=15,\n                              validation_steps=50,\n                              verbose=2)","5722cc71":"import numpy as np\n\nfrom google.colab import files\nfrom keras.preprocessing import image\n\nuploaded=files.upload()\n\nfor fn in uploaded.keys():\n \n  # predicting images\n  path='\/content\/' + fn\n  img=image.load_img(path, target_size=(150, 150))\n  \n  x=image.img_to_array(img)\n  x=np.expand_dims(x, axis=0)\n  images = np.vstack([x])\n  \n  classes = model.predict(images, batch_size=10)\n  \n  print(classes[0])\n  \n  if classes[0]>0:\n    print(fn + \" is a dog\")\n    \n  else:\n    print(fn + \" is a cat\")\n ","8e243767":"import numpy as np\nimport random\nfrom   tensorflow.keras.preprocessing.image import img_to_array, load_img\n\nsuccessive_outputs = [layer.output for layer in model.layers[1:]]\n\nvisualization_model = tf.keras.models.Model(inputs = model.input, outputs = successive_outputs)\n\ncat_img_files = [os.path.join(train_cats_dir, f) for f in train_cat_fnames]\ndog_img_files = [os.path.join(train_dogs_dir, f) for f in train_dog_fnames]\n\nimg_path = random.choice(cat_img_files + dog_img_files)\nimg = load_img(img_path, target_size=(150, 150)) \n\nx   = img_to_array(img)                           \nx   = x.reshape((1,) + x.shape)                   \nx \/= 255.0\n\nsuccessive_feature_maps = visualization_model.predict(x)\n\nlayer_names = [layer.name for layer in model.layers]\n\n# -----------------------------------------------------------------------\n# Now let's display our representations\n# -----------------------------------------------------------------------\nfor layer_name, feature_map in zip(layer_names, successive_feature_maps):\n  \n  if len(feature_map.shape) == 4:\n    \n    #-------------------------------------------\n    # Just do this for the conv \/ maxpool layers, not the fully-connected layers\n    #-------------------------------------------\n    \n    n_features = feature_map.shape[-1]  \n    size       = feature_map.shape[ 1]  \n    \n    display_grid = np.zeros((size, size * n_features))\n    \n    #-------------------------------------------------\n    # Postprocess the feature to be visually palatable\n    #-------------------------------------------------\n    \n    for i in range(n_features):\n      x  = feature_map[0, :, :, i]\n      x -= x.mean()\n      x \/= x.std ()\n      x *=  64\n      x += 128\n      x  = np.clip(x, 0, 255).astype('uint8')\n      display_grid[:, i * size : (i + 1) * size] = x \n\n    #-----------------\n    # Display the grid\n    #-----------------\n\n    scale = 20. \/ n_features\n    plt.figure( figsize=(scale * n_features, scale) )\n    plt.title ( layer_name )\n    plt.grid  ( False )\n    plt.imshow( display_grid, aspect='auto', cmap='viridis' ) ","e39c7556":"#-----------------------------------------------------------\n# Retrieve a list of list results on training and test data\n# sets for each training epoch\n#-----------------------------------------------------------\nacc      = history.history['accuracy']\nval_acc  = history.history['val_accuracy']\nloss     = history.history['loss']\nval_loss = history.history['val_loss']\n\nepochs   = range(len(acc)) # Get number of epochs\n\n#------------------------------------------------\n# Plot training and validation accuracy per epoch\n#------------------------------------------------\nplt.plot  (epochs,acc)\nplt.plot  (epochs,val_acc)\nplt.title ('Training and validation accuracy')\nplt.figure()\n\n#------------------------------------------------\n# Plot training and validation loss per epoch\n#------------------------------------------------\nplt.plot  (epochs,loss)\nplt.plot  (epochs,val_loss)\nplt.title ('Training and validation loss')","82084fcb":"## Training","dc39189c":"For both cats and dogs, we have 1,000 training images and 500 validation images.\n\nNow let's take a look at a few pictures to get a better sense of what the cat and dog datasets look like. ","60331133":"# Convolutional Neural Network With Image Augmentation\n\nIn this will be recognizing real images of Cats and Dogs in order to classify an incoming image as one or the other. As we have less training samples in our datatset therefore our model would be overfitting on train set. To overcome this issue I have used Image Augmentation that can be used to artificially expand the size of a training dataset by creating modified versions of images in the dataset.\n","f2840b82":"### Visualizing Intermediate Representations\n\nTo get a feel for what kind of features our convnet has learned, one fun thing to do is to visualize how an input gets transformed as it goes through the convnet.\n\nLet's pick a random cat or dog image from the training set, and then generate a figure where each row is the output of a layer, and each image in the row is a specific filter in that output feature map. Rerun this cell to generate intermediate representations for a variety of training images.","f1564eb5":"### Running the Model (Google Colab)\n\nLet's now take a look at actually running a prediction using the model. This code will allow you to choose 1 or more files from your file system, it will then upload them, and run them through the model, giving an indication of whether the object is a dog or a cat.","1fc0493f":"### Data Preprocessing\n\nLet's set up data generators that will read pictures in our source folders, convert them to `float32` tensors, and feed them (with their labels) to our network. We'll have one generator for the training images and one for the validation images. Our generators will yield batches of 20 images of size 150x150 and their labels (binary).\n\nAs we know, data that goes into neural networks should usually be normalized in some way to make it more amenable to processing by the network. (It is uncommon to feed raw pixels into a convnet.) In our case, we will preprocess our images by normalizing the pixel values to be in the `[0, 1]` range (originally all values are in the `[0, 255]` range).\n\nIn Keras this can be done via the `keras.preprocessing.image.ImageDataGenerator` class using the `rescale` parameter. This `ImageDataGenerator` class allows you to instantiate generators of augmented image batches (and their labels) via `.flow(data, labels)` or `.flow_from_directory(directory)`. These generators can then be used with the Keras model methods that accept data generators as inputs: `fit`, `evaluate_generator`, and `predict_generator`.","deedf03e":"The model.summary() method call prints a summary of the NN ","acbe49ee":"### Evaluating Accuracy and Loss for the Model\n\nLet's plot the training\/validation accuracy and loss as collected during training:"}}