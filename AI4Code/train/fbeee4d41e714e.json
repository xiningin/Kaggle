{"cell_type":{"d27716f0":"code","0d7bcd52":"code","081e0d05":"code","1c35e3a9":"code","5a4d7d15":"code","09ae7c10":"code","31a2073a":"code","efa8fa14":"code","4e30b424":"code","56d80303":"code","7871bc5e":"code","9b96cb0b":"code","7574a617":"code","8aa01f5e":"code","2b876d68":"code","c8b69f4d":"code","256b0489":"code","56684154":"code","54dcbb54":"code","0c79e170":"code","40aaa1b4":"code","ef43fa34":"code","efdb35bb":"code","7c93a297":"code","f361cf09":"code","0a72c870":"code","2d6987a1":"code","40902027":"code","fd7dcb76":"code","2d3e6f8d":"code","42ed8ca0":"code","5ff74fba":"code","3d85303e":"markdown","eb94bdbb":"markdown","d59d7933":"markdown","aa3b0a51":"markdown","5d3fc53a":"markdown","d5fd1d9e":"markdown","1cb6b743":"markdown"},"source":{"d27716f0":"import numpy as np\nimport pandas as pd \nimport seaborn as sns\n# import os\n# print(os.listdir(\"..\/input\"))\nfrom sklearn.utils import shuffle","0d7bcd52":"data1 = pd.read_csv(\"..\/input\/student-mat.csv\",sep=\",\")\ndata2 = pd.read_csv(\"..\/input\/student-por.csv\",sep=\",\")\ndata = [data1,data2]\ndata=pd.concat(data)\ndata=shuffle(data)\ndata.describe(include=\"all\")","081e0d05":"data=data.drop_duplicates([\"school\",\"sex\",\"age\",\"address\",\"famsize\",\"Pstatus\",\"Medu\",\"Fedu\",\"Mjob\",\"Fjob\",\"reason\",\"nursery\",\"internet\"])\ndata.describe(include=\"all\")","1c35e3a9":"# print(data.isnull().count())\ndata.replace([np.inf, -np.inf], np.nan, inplace=True) \ndata = data.dropna()\ndata.dropna(inplace=True) \ndata.describe(include=\"all\")","5a4d7d15":"sns.catplot(x=\"school\", hue = \"sex\" , data=data , kind=\"count\",height=6, aspect=.7)\nprint(\"percentage total female : \",(data[\"sex\"] == 'F').value_counts(normalize = True)[1]*100)\nprint(\"percentage total male : \",(data[\"sex\"] == 'M').value_counts(normalize = True)[1]*100)","09ae7c10":"sns.swarmplot(x=\"internet\",y=\"G3\",hue='address',data=data)","31a2073a":"sns.catplot(x=\"sex\", hue = \"romantic\", data=data , kind=\"count\",height=6, aspect=.7)","efa8fa14":"sns.swarmplot(x=\"Dalc\",y=\"G3\",hue=\"sex\",data=data)","4e30b424":"sns.swarmplot(x=\"Walc\",y=\"G3\",hue=\"sex\",data=data)","56d80303":"import seaborn as sns\nfrom matplotlib.pyplot import figure\nfigure(figsize=(15, 15))\nhmap = sns.heatmap(data.corr(), square=True, annot=True,linewidths=0.5)","7871bc5e":"#drop some features that have very less correlation values \ndatagrade = data.drop([\"traveltime\",\"famrel\",\"freetime\",\"goout\",\"health\",\"absences\"], axis=1)\ndata_alc = data.drop([\"traveltime\",\"famrel\", \"Fedu\", \"Medu\",\"goout\",\"health\",\"absences\"], axis=1)\n","9b96cb0b":"#to suppress the warnings\nimport sys\nimport warnings\n\nif not sys.warnoptions:\n    warnings.simplefilter(\"ignore\")","7574a617":"#data cleaning(binary and onehot encodings)\ndatagrade.school[datagrade.school == 'GP'] = 1\ndatagrade.school[datagrade.school == 'MS'] = 0\ndatagrade.sex[datagrade.sex == 'M'] = 1\ndatagrade.sex[datagrade.sex == 'F'] = 0\ndatagrade.address[datagrade.address == 'U'] = 1\ndatagrade.address[datagrade.address == 'R'] = 0\ndatagrade.famsize[datagrade.famsize == 'GT3'] = 1\ndatagrade.famsize[datagrade.famsize == 'LE3'] = 0\ndatagrade.Pstatus[datagrade.Pstatus == 'T'] = 1\ndatagrade.Pstatus[datagrade.Pstatus == 'A'] = 0\n#for Medu & Fedu its better in numeric as higher education has higher difference with primary than secondary\n#one hot encoding of necessary features\ncols_to_transform = [ 'Mjob','Fjob','reason','guardian' ]\ndatagrade=pd.get_dummies(datagrade,columns=cols_to_transform) \n#failures, traveltime, studytime does not require onehot  ","8aa01f5e":"datagrade.schoolsup[datagrade.schoolsup == 'yes'] = 1\ndatagrade.schoolsup[datagrade.schoolsup == 'no'] = 0\ndatagrade.famsup[datagrade.famsup == 'yes'] = 1\ndatagrade.famsup[datagrade.famsup == 'no'] = 0\ndatagrade.paid[datagrade.paid == 'yes'] = 1\ndatagrade.paid[datagrade.paid == 'no'] = 0\ndatagrade.activities[datagrade.activities == 'yes'] = 1\ndatagrade.activities[datagrade.activities == 'no'] = 0\ndatagrade.nursery[datagrade.nursery == 'yes'] = 1\ndatagrade.nursery[datagrade.nursery == 'no'] = 0\ndatagrade.higher[datagrade.higher == 'yes'] = 1\ndatagrade.higher[datagrade.higher == 'no'] = 0\ndatagrade.internet[datagrade.internet == 'yes'] = 1\ndatagrade.internet[datagrade.internet == 'no'] = 0\ndatagrade.romantic[datagrade.romantic == 'yes'] = 1\ndatagrade.romantic[datagrade.romantic == 'no'] = 0\n\n#remaining doesn't require onehot as distances are not same. one hot is used only when they are equidistant\n","2b876d68":"#data cleaning for alcohol consumption prediction objective\ndata_alc.school[data_alc.school == 'GP'] = 1\ndata_alc.school[data_alc.school == 'MS'] = 0\ndata_alc.sex[data_alc.sex == 'M'] = 1\ndata_alc.sex[data_alc.sex == 'F'] = 0\ndata_alc.address[data_alc.address == 'U'] = 1\ndata_alc.address[data_alc.address == 'R'] = 0\ndata_alc.famsize[data_alc.famsize == 'GT3'] = 1\ndata_alc.famsize[data_alc.famsize == 'LE3'] = 0\ndata_alc.Pstatus[data_alc.Pstatus == 'T'] = 1\ndata_alc.Pstatus[data_alc.Pstatus == 'A'] = 0\n\n\ncols_to_transform = [ 'Mjob','Fjob','reason','guardian' ]\ndata_alc=pd.get_dummies(data_alc,columns=cols_to_transform) \n","c8b69f4d":"data_alc.schoolsup[data_alc.schoolsup == 'yes'] = 1\ndata_alc.schoolsup[data_alc.schoolsup == 'no'] = 0\ndata_alc.famsup[data_alc.famsup == 'yes'] = 1\ndata_alc.famsup[data_alc.famsup == 'no'] = 0\ndata_alc.paid[data_alc.paid == 'yes'] = 1\ndata_alc.paid[data_alc.paid == 'no'] = 0\ndata_alc.activities[data_alc.activities == 'yes'] = 1\ndata_alc.activities[data_alc.activities == 'no'] = 0\ndata_alc.nursery[data_alc.nursery == 'yes'] = 1\ndata_alc.nursery[data_alc.nursery == 'no'] = 0\ndata_alc.higher[data_alc.higher == 'yes'] = 1\ndata_alc.higher[data_alc.higher == 'no'] = 0\ndata_alc.internet[data_alc.internet == 'yes'] = 1\ndata_alc.internet[data_alc.internet == 'no'] = 0\ndata_alc.romantic[data_alc.romantic == 'yes'] = 1\ndata_alc.romantic[data_alc.romantic == 'no'] = 0\n\n#remaining doesn't require onehot as distances are not same. one hot is used only when they are equidistant\n","256b0489":"def convertToGrades(x):\n    if x>=18 and x<=20:\n        return 5  #A grade\n    elif x>15:\n        return 4  #B grade\n    elif x>10:\n        return 3  #C grade\n    elif x>5:\n        return 2  #D grade\n    else:\n        return 1  #E grade\ndatagrade.G3 = datagrade.G3.map(convertToGrades)\n\ndata.head(10)","56684154":"#Final grades\ny_grade =  datagrade[[ 'G3']].mean(axis=1)\ndata_grade = datagrade.drop([\"G3\"], axis=1)\nFEATURE_GRADE = ['Index', 'School', 'Gender', 'Age', 'Address', 'Family Size', 'Parents status', 'Mother Education', 'Father Education', 'Study Time', 'Failures', 'SchoolSup', 'FamSup', 'Paid', 'Activites', 'Nursery', 'Higher Studies', 'Internet', 'Romantic Relationships', 'Weekday Alcohol Consumption', 'Weekend Alcohol Consumption', '1st Period Results', '2nd Period Results', 'Does Mother work at home', 'Does Mother work in health sector', 'Does Mother work somewhere else', 'Does Mother work in administrative services', 'Is mother teacher','Does Father work at home', 'Does Father work in health sector', 'Does Father work somewhere else', 'Does Father work in administrative services', 'Is father teacher' 'Reason Course', 'Reason Home', 'Reason Other', 'Reason Reputation', 'Guardian Father', 'Guardian Mother', 'Guardian Other']\ndata_grade.head()\n","54dcbb54":"#Alchohol Consumption\ny_alc_weekday =  data_alc[[ 'Dalc']].mean(axis=1)\ny_alc_weekend =  data_alc[[ 'Walc']].mean(axis=1)\ndata_alc = data_alc.drop([\"Dalc\", \"Walc\"], axis=1)\nFEATURE_ALC = ['Index', 'School', 'Gender', 'Age', 'Address', 'Family Size', 'Parents status', 'Study Time', 'Failures', 'SchoolSup', 'FamSup', 'Paid', 'Activites', 'Nursery', 'Higher Studies', 'Internet', 'Romantic Relationships', 'Freetime', '1st Period Results', '2nd Period Results', 'Final results', 'Does Mother work at home', 'Does Mother work in health sector', 'Does Mother work somewhere else', 'Does Mother work in administrative services', 'Is mother teacher','Does Father work at home', 'Does Father work in health sector', 'Does Father work somewhere else', 'Does Father work in administrative services', 'Is father teacher' 'Reason Course', 'Reason Home', 'Reason Other', 'Reason Reputation', 'Guardian Father', 'Guardian Mother', 'Guardian Other']\ndata_alc.head()","0c79e170":"#spliting data to train and test by 80% and 20% respectievely \nfrom sklearn.model_selection import train_test_split\nX_grade_train, X_grade_test, y_grade_train, y_grade_test = train_test_split(data_grade,y_grade, test_size=0.2)","40aaa1b4":"#spliting data to train and test by 80% and 20% respectievely \nX_alc_weekday_train, X_alc_weekday_test, y_alc_weekday_train, y_alc_weekday_test = train_test_split(data_alc,y_alc_weekday, test_size=0.2)\nX_alc_weekend_train, X_alc_weekend_test, y_alc_weekend_train, y_alc_weekend_test = train_test_split(data_alc,y_alc_weekend, test_size=0.2)","ef43fa34":"import matplotlib.pyplot as plt\nfrom sklearn.tree import DecisionTreeClassifier \nfrom sklearn.metrics import confusion_matrix \nfrom sklearn.metrics import mean_squared_error, r2_score\nfrom sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report, confusion_matrix\nfrom sklearn.tree import export_graphviz\nimport matplotlib.pyplot as plt\nimport cv2\n\n\ndtree_model = DecisionTreeClassifier(max_depth = 2).fit(X_grade_train, y_grade_train) \ndtree_predictions = dtree_model.predict(X_grade_test) \nexport_graphviz(dtree_model, 'tree.dot', feature_names = FEATURE_GRADE)  \n! dot -Tpng tree.dot -o tree.png\n%matplotlib inline\nimg = cv2.imread('tree.png')\nplt.figure(figsize = (20, 20))\nplt.imshow(img)\nacc = f1_score(y_grade_test, dtree_predictions, average=\"macro\")\nprint(\"F1 score for Predicting final grade using decision tree of max 2 depth: \" + str(acc))\n\n\n\n\n\n\n","efdb35bb":"\ndtree_model = DecisionTreeClassifier(max_depth = 2).fit(X_alc_weekday_train, y_alc_weekday_train) \ndtree_predictions = dtree_model.predict(X_alc_weekday_test) \nexport_graphviz(dtree_model, 'tree.dot', feature_names = FEATURE_ALC)  \n! dot -Tpng tree.dot -o tree.png\n%matplotlib inline\nimg = cv2.imread('tree.png')\nplt.figure(figsize = (20, 20))\nplt.imshow(img)  \nacc = f1_score(y_alc_weekday_test, dtree_predictions, average=\"macro\")\nprint(\"F1 score for Predicting alchohol consumption in weekdays using decision tree of max 2 depth: \" + str(acc))","7c93a297":"\n\n\ndtree_model = DecisionTreeClassifier(max_depth = 2).fit(X_alc_weekend_train, y_alc_weekend_train) \ndtree_predictions = dtree_model.predict(X_alc_weekend_test) \nexport_graphviz(dtree_model, 'tree.dot', feature_names = FEATURE_ALC)  \n! dot -Tpng tree.dot -o tree.png\n%matplotlib inline\nimg = cv2.imread('tree.png')\nplt.figure(figsize = (20, 20))\nplt.imshow(img)  \nacc = f1_score(y_alc_weekend_test, dtree_predictions, average=\"macro\")\nprint(\"F1 score for Predicting alchohol consumption in weekends using decision tree of max 2 depth: \" + str(acc))","f361cf09":"import matplotlib.pyplot as plt\nfrom sklearn.tree import DecisionTreeClassifier \nfrom sklearn.metrics import confusion_matrix \nfrom sklearn.metrics import mean_squared_error, r2_score\nfrom sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report, confusion_matrix\n\n\n\n\ndtree_model = DecisionTreeClassifier(max_depth = 10).fit(X_grade_train, y_grade_train) \ndtree_predictions = dtree_model.predict(X_grade_test) \nexport_graphviz(dtree_model, 'tree.dot', feature_names = FEATURE_GRADE)  \n! dot -Tpng tree.dot -o tree.png\n%matplotlib inline\nimg = cv2.imread('tree.png')\nplt.figure(figsize = (20, 20))\nplt.imshow(img)    \nacc = f1_score(y_grade_test, dtree_predictions, average=\"macro\")\nprint(\"F1 score for Predicting final grade using decision tree of max 10 depth: \" + str(acc))\n\n\n\n\n","0a72c870":"dtree_model = DecisionTreeClassifier(max_depth = 10).fit(X_alc_weekday_train, y_alc_weekday_train) \ndtree_predictions = dtree_model.predict(X_alc_weekday_test) \nexport_graphviz(dtree_model, 'tree.dot', feature_names = FEATURE_ALC)  \n! dot -Tpng tree.dot -o tree.png\n%matplotlib inline\nimg = cv2.imread('tree.png')\nplt.figure(figsize = (20, 20))\nplt.imshow(img)    \nacc = f1_score(y_alc_weekday_test, dtree_predictions, average=\"macro\")\nprint(\"F1 score for Predicting alchohol consumption in weekdays using decision tree of max 10 depth: \" + str(acc))","2d6987a1":"\ndtree_model = DecisionTreeClassifier(max_depth = 10).fit(X_alc_weekend_train, y_alc_weekend_train) \ndtree_predictions = dtree_model.predict(X_alc_weekend_test) \nexport_graphviz(dtree_model, 'tree.dot', feature_names = FEATURE_ALC)  \n! dot -Tpng tree.dot -o tree.png\n%matplotlib inline\nimg = cv2.imread('tree.png')\nplt.figure(figsize = (20, 20))\nplt.imshow(img)    \nacc = f1_score(y_alc_weekend_test, dtree_predictions, average=\"macro\")\nprint(\"F1 score for Predicting alchohol consumption in weekends using decision tree of max 10 depth: \" + str(acc))\n","40902027":"import matplotlib.pyplot as plt\nfrom sklearn.tree import DecisionTreeClassifier \nfrom sklearn.metrics import confusion_matrix \nfrom sklearn.metrics import mean_squared_error, r2_score\nfrom sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report, confusion_matrix\n\n\n\n\ndtree_model = DecisionTreeClassifier(max_depth = 50).fit(X_grade_train, y_grade_train) \ndtree_predictions = dtree_model.predict(X_grade_test) \nexport_graphviz(dtree_model, 'tree.dot', feature_names = FEATURE_GRADE)  \n! dot -Tpng tree.dot -o tree.png\n%matplotlib inline\nimg = cv2.imread('tree.png')\nplt.figure(figsize = (20, 20))\nplt.imshow(img)    \nacc = f1_score(y_grade_test, dtree_predictions, average=\"macro\")\nprint(\"F1 score for Predicting final grade using decision tree of max 50 depth: \" + str(acc))\n\n","fd7dcb76":"\ndtree_model = DecisionTreeClassifier(max_depth = 50).fit(X_alc_weekday_train, y_alc_weekday_train) \ndtree_predictions = dtree_model.predict(X_alc_weekday_test) \nexport_graphviz(dtree_model, 'tree.dot', feature_names = FEATURE_ALC)  \n! dot -Tpng tree.dot -o tree.png\n%matplotlib inline\nimg = cv2.imread('tree.png')\nplt.figure(figsize = (20, 20))\nplt.imshow(img)    \nacc = f1_score(y_alc_weekday_test, dtree_predictions, average=\"macro\")\nprint(\"F1 score for Predicting alchohol consumption in weekdays using decision tree of max 50 depth: \" + str(acc))","2d3e6f8d":"\ndtree_model = DecisionTreeClassifier(max_depth = 50).fit(X_alc_weekend_train, y_alc_weekend_train) \ndtree_predictions = dtree_model.predict(X_alc_weekend_test) \nexport_graphviz(dtree_model, 'tree.dot', feature_names = FEATURE_ALC)  \n! dot -Tpng tree.dot -o tree.png\n%matplotlib inline\nimg = cv2.imread('tree.png')\nplt.figure(figsize = (20, 20))\nplt.imshow(img)    \nacc = f1_score(y_alc_weekend_test, dtree_predictions, average=\"macro\")\nprint(\"F1 score for Predicting alchohol consumption in weekends using decision tree of max 50 depth: \" + str(acc))\n\n\n","42ed8ca0":"from sklearn.neighbors import KNeighborsClassifier \n\nknn = KNeighborsClassifier(n_neighbors = 7).fit(X_grade_train, y_grade_train) \nknn_predictions = knn.predict(X_grade_test) \n\nacc = f1_score(y_grade_test, knn_predictions, average=\"macro\")\nprint(\"F1 score for Predicting final grade using KNN classifier: \" + str(acc))\n\n\n\nknn = KNeighborsClassifier(n_neighbors = 7).fit(X_alc_weekday_train, y_alc_weekday_train) \nknn_predictions = knn.predict(X_alc_weekday_test) \n  \nacc = f1_score(y_alc_weekday_test, knn_predictions, average=\"macro\")\nprint(\"F1 score for Predicting alchohol consumption in weekdays using KNN classifier: \" + str(acc))\n\n\n\nknn = KNeighborsClassifier(n_neighbors = 7).fit(X_alc_weekend_train, y_alc_weekend_train) \nknn_predictions = knn.predict(X_alc_weekend_test) \n  \nacc = f1_score(y_alc_weekend_test, knn_predictions, average=\"macro\")\nprint(\"F1 score for Predicting alchohol consumption in weekends using KNN classifier: \" + str(acc))\n\n\n","5ff74fba":"from sklearn.naive_bayes import GaussianNB\n\n\n\ngnb = GaussianNB().fit(X_grade_train, y_grade_train) \ngnb_predictions = gnb.predict(X_grade_test) \n\nacc = f1_score(y_grade_test, gnb_predictions, average=\"macro\")\nprint(\"F1 score for Predicting final grade using Naive Bayes Classifier: \" + str(acc))\n\n\ngnb = GaussianNB().fit(X_alc_weekday_train, y_alc_weekday_train) \ngnb_predictions = gnb.predict(X_alc_weekday_test) \n  \nacc = f1_score(y_alc_weekday_test, gnb_predictions, average=\"macro\")\nprint(\"F1 score for Predicting alchohol consumption in weekdays using Naive Bayes Classifier: \" + str(acc))\n\n\ngnb = GaussianNB().fit(X_alc_weekend_train, y_alc_weekend_train) \ngnb_predictions = gnb.predict(X_alc_weekend_test) \n  \nacc = f1_score(y_alc_weekend_test, gnb_predictions, average=\"macro\")\nprint(\"F1 score for Predicting alchohol consumption in weekends using Naive Bayes Classifier: \" + str(acc))\n\n\n \n","3d85303e":"It seems females have a higher chance of being in a relationship than male students. ","eb94bdbb":"It seems very large amount of people drink very low alcohol(1) during both weekend and work days. Most of the people who drink high(4,5) are male and also people who drink high during work days are getting bit lower grades than that of others.  ","d59d7933":"It seems most of the urban people are having internet availability and also performing better than rural people.","aa3b0a51":"So from here we can conclude, that there were no tuples with NULL or infinite values.","5d3fc53a":"It is clear that number of female students are more than number of male students in both the  schools, with 'GP' - Gabriel Pereira school having more number of students overall.  ","d5fd1d9e":"**Statements based on heatmap values**\nWe can observe few good  correlation values between health and alcohol consumption also alcohol consumption with failures. And also as expected study time has a good correlation value with grades and father edu, mother edu and failures also have good correlation values with grades and first period and second period grades matter a lot to the final grade(high corrleation values). Surprisingly absences and health have low correlation values with the grades.","1cb6b743":"Removing duplicates leads to 662 tuples, which implies there were 382 duplicate tuples in the two datasets"}}