{"cell_type":{"bdc62b08":"code","8339f3a0":"code","9da79f7c":"code","aa081996":"code","bf9985e4":"code","b2842dd5":"code","28b48e23":"code","bcc18d1a":"code","d7c77966":"code","04a0d36e":"code","b7c89d34":"code","1763d4e1":"code","d09492e4":"code","9195e0ae":"code","15776e8a":"code","1eed5600":"code","98e0c085":"code","bdb69d19":"code","b7580ce7":"code","412e440c":"code","b7f0335c":"code","4a4b2d59":"code","326dc7fe":"code","6b8923bb":"code","d829c590":"code","eb5f281f":"code","5164f9c7":"code","58edafe0":"code","938d91fb":"code","80b79179":"code","4d783ca0":"code","0066ec29":"code","fabc2375":"code","09429a9b":"code","92cfe901":"code","fe5488e4":"code","330ff8b9":"markdown"},"source":{"bdc62b08":"import os\nos.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\nimport numpy as np\nimport pandas as pd\nfrom tqdm.auto import tqdm\ntqdm.pandas()\nimport Levenshtein\nimport cv2\nfrom PIL import Image\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\nimport time\nimport torch.backends.cudnn as cudnn\nimport torch.optim\nimport torch.utils.data\nimport torchvision.transforms as transforms\nfrom torch.nn.utils.rnn import pack_padded_sequence\nimport torchvision\nimport PIL.Image as Image\nfrom torchvision.transforms import ToTensor, ToPILImage\n\nimport torch.nn.functional as F\nimport torch.backends.cudnn as cudnn\nfrom nltk.translate.bleu_score import corpus_bleu\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")","8339f3a0":"!pip install timm","9da79f7c":"import timm\n\nfrom functools import partial\n\nimport cv2\nfrom PIL import Image\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.optim import Adam, SGD\nimport torchvision.models as models\nfrom torch.nn.parameter import Parameter\nfrom torch.utils.data import DataLoader, Dataset\nfrom torch.nn.utils.rnn import pad_sequence, pack_padded_sequence\nfrom torch.optim.lr_scheduler import CosineAnnealingWarmRestarts, CosineAnnealingLR, ReduceLROnPlateau\n\nfrom albumentations import(\n    Compose, OneOf, Normalize, Resize, RandomResizedCrop, RandomCrop, HorizontalFlip, VerticalFlip, \n    RandomBrightness, RandomContrast, RandomBrightnessContrast, Rotate, ShiftScaleRotate, Cutout, \n    IAAAdditiveGaussianNoise, Transpose, Blur\n    )\nfrom albumentations.pytorch import ToTensorV2\nfrom albumentations import ImageOnlyTransform\n\nimport timm\n\nimport warnings \nwarnings.filterwarnings('ignore')","aa081996":"\nimport gc\nimport re\nimport math\nimport time\nimport random\nimport shutil\nimport pickle\nfrom pathlib import Path\nfrom contextlib import contextmanager\nfrom collections import defaultdict, Counter\n\nimport scipy as sp\n\nfrom tqdm.auto import tqdm\n\nfrom functools import partial\n\nimport cv2\n\nimport torch.nn.functional as F\nfrom torch.optim import Adam, SGD\nimport torchvision.models as models\nfrom torch.nn.parameter import Parameter\nfrom torch.utils.data import DataLoader, Dataset\nfrom torch.nn.utils.rnn import pad_sequence, pack_padded_sequence\nfrom torch.optim.lr_scheduler import CosineAnnealingWarmRestarts, CosineAnnealingLR, ReduceLROnPlateau\n\nimport albumentations as A\nfrom albumentations.pytorch import ToTensorV2\nfrom albumentations import ImageOnlyTransform\nimport timm\nimport pytorch_lightning as pl\n","bf9985e4":" # Increase Batch size based on your hardware capacity, if you have powerful GPU try BS=128,256...\ntrain_path=r'..\/input\/moroccoai-data-challenge-edition-001\/train'\ntest_path=r'..\/input\/moroccoai-data-challenge-edition-001\/test'\ntrain_csv = pd.read_csv('..\/input\/moroccoai-data-challenge-edition-001\/train.csv')","b2842dd5":"target = list(train_csv['plate_string'])\nprint(target[0])\nprint(len(target[0]))\nmax_plate = 0\nfor plate in target:\n    if len(plate) > max_plate:\n        max_plate = len(plate)\nmax_plate += 2    \nprint(max_plate)  ","28b48e23":"caracter2index = { \"SOS\":1,\"EOS\":18,\"PAD\":0 }\ns=1\nindex2caracter = {1:\"SOS\", 18:\"EOS\", 0:\"PAD\"}\nfor plate in target:\n    plate = plate.strip()\n    for caracter in plate:\n        if caracter not in caracter2index:\n            caracter2index[caracter] = s\n            index2caracter[s+1] = caracter\n            s += 1","bcc18d1a":"print(index2caracter)\nprint(len(index2caracter))\nprint(len(caracter2index))\nprint(caracter2index)","d7c77966":"train_images_list = []\nwith os.scandir('..\/input\/moroccoai-data-challenge-edition-001\/train') as entrie:\n    for entry in entrie:\n        train_images_list.append(\"..\/input\/moroccoai-data-challenge-edition-001\/train\/\"+entry.name)","04a0d36e":"print(train_images_list[2])","b7c89d34":"class CFG:\n#     model_name='resnet34' # output_dim = encoder_dim = 512\n#     model_name = 'tf_efficientnet_b0_ns' # output_dim = encoder_dim = 1280\n    model_name = 'resnet101' # output_dim = encoder_dim = 1536\n\n#     model_name='resnet200d' # output_dim = encoder_dim = 2048\n    train = True\n    size = 256\n    max_len = 12\n    batch_size =32\n    num_workers = 1\n    debug = False\n    encoder_lr=1e-4\n    decoder_lr=4e-4\n    gradient_accumulation_steps=1\n    print_freq=2\n    max_grad_norm=5\n    attention_dim = 512\n    embed_dim = 512\n    encoder_dim = 2048\n    decoder_dim = 512\n    dropout = 0.5\n    seed = 42\n    n_fold = 5\n    trn_fold = [0] # [0, 1, 2, 3, 4]\n    weight_decay=1e-6\n    scheduler='CosineAnnealingLR' # ['ReduceLROnPlateau', 'CosineAnnealingLR', 'CosineAnnealingWarmRestarts']\n    T_max=4 # CosineAnnealingLR\n    min_lr=1e-6\n    epochs = 1\n    factor=0.2 # ReduceLROnPlateau\n    patience=4 # ReduceLROnPlateau\n    eps=1e-6 # ReduceLROnPlateau","1763d4e1":"def text_to_tensor(sequence,max_plate,caracter2index):\n    list_caracter = []\n    list_caracter.append(caracter2index[\"SOS\"])\n    for caracter in sequence:\n        list_caracter.append(caracter2index[caracter])\n    list_caracter.append(caracter2index[\"EOS\"])\n    len_seq = len(list_caracter)\n    tensor = torch.LongTensor(F.pad(torch.LongTensor(list_caracter), pad=(0, (max_plate)- len_seq) , mode='constant', value=0)) \n    return tensor,len_seq\n\n        \ndef get_transforms(*, data):\n    \n    if data == 'train':\n        return Compose([\n            Resize(CFG.size,CFG.size),\n            Normalize(\n                mean=[0.485, 0.456, 0.406],\n                std=[0.229, 0.224, 0.225],\n            ),\n            ToTensorV2(),\n        ])\n    \n    elif data == 'valid':\n        return Compose([\n            Resize(CFG.size, CFG.size),\n            Normalize(\n                mean=[0.485, 0.456, 0.406],\n                std=[0.229, 0.224, 0.225],\n            ),\n            ToTensorV2(),\n        ])\n\nclass dataset(torch.utils.data.Dataset):\n\n    def __init__(self,images_paths,captions_paths,max_len,transform=None):\n        self.images_paths = images_paths\n        self.captions_paths = captions_paths\n        self.max_len = max_len \n        self.transform = transform\n         \n    #dataset length\n    def __len__(self):\n        self.filelength = len(self.images_paths)\n        return self.filelength\n    \n    #load an one of images\n    def __getitem__(self,idx):\n     #   img_path = self.file_list[idx]\n        file = self.images_paths[idx].strip()\n        image = cv2.imread(file)\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB).astype(np.float32)\n        if self.transform:\n            augmented = self.transform(image=image)\n            image = augmented['image']\n        tensor_caption,len_seq = text_to_tensor(self.captions_paths[idx],self.max_len,caracter2index)\n        return image,tensor_caption,len_seq","d09492e4":"images_paths  = train_images_list[:416]\ncaptions_paths = target[:416]\nimages_paths_validation  = train_images_list[-32:]\ncaptions_paths_validation = target[-32:]","9195e0ae":"print(len(images_paths))\nprint(len(captions_paths))\nprint(len(images_paths_validation))\nprint(len(captions_paths_validation))","15776e8a":"train_data = dataset(images_paths=images_paths,captions_paths=captions_paths,max_len=max_plate,transform=get_transforms(data = \"train\"))\ntrain_loader = torch.utils.data.DataLoader(dataset = train_data, batch_size=32, shuffle=True)","1eed5600":"\n\n\ndef get_transforms(*, data):\n    \n    if data == 'train':\n        return Compose([\n            Resize(CFG.size,CFG.size),\n            Normalize(\n                mean=[0.485, 0.456, 0.406],\n                std=[0.229, 0.224, 0.225],\n            ),\n            ToTensorV2(),\n        ])\n    \n    elif data == 'valid':\n        return Compose([\n            Resize(CFG.size, CFG.size),\n            Normalize(\n                mean=[0.485, 0.456, 0.406],\n                std=[0.229, 0.224, 0.225],\n            ),\n            ToTensorV2(),\n        ])\n\nclass dataset1(torch.utils.data.Dataset):\n\n    def __init__(self,images_paths,captions_paths,max_len,transform=None):\n        self.images_paths = images_paths\n        self.captions_paths = captions_paths\n        self.max_len = max_len \n        self.transform = transform\n         \n    #dataset length\n    def __len__(self):\n        self.filelength = len(self.images_paths)\n        return self.filelength\n    \n    #load an one of images\n    def __getitem__(self,idx):\n     #   img_path = self.file_list[idx]\n        file = self.images_paths[idx].strip()\n        image = cv2.imread(file)\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB).astype(np.float32)\n        if self.transform:\n            augmented = self.transform(image=image)\n            image = augmented['image']\n        tensor_caption,len_seq = self.captions_paths[idx],len(self.captions_paths[idx])\n        return image,tensor_caption,len_seq","98e0c085":"validation_data = dataset1(images_paths=images_paths_validation,captions_paths=captions_paths_validation,max_len=max_plate,transform=get_transforms(data = \"valid\"))\nvalidation_loader = torch.utils.data.DataLoader(dataset = validation_data, batch_size=32, shuffle=True)","bdb69d19":"print(len(train_data))\nprint(len(train_loader))\nprint(len(validation_data))\nprint(len(validation_loader))","b7580ce7":"class AverageMeter(object):\n    \"\"\"Computes and stores the average and current value\"\"\"\n    def __init__(self):\n        self.reset()\n\n    def reset(self):\n        self.val = 0\n        self.avg = 0\n        self.sum = 0\n        self.count = 0\n\n    def update(self, val, n=1):\n        self.val = val\n        self.sum += val * n\n        self.count += n\n        self.avg = self.sum \/ self.count","412e440c":"def asMinutes(s):\n    m = math.floor(s \/ 60)\n    s -= m * 60\n    return '%dm %ds' % (m, s)\n\n\ndef timeSince(since, percent):\n    now = time.time()\n    s = now - since\n    es = s \/ (percent)\n    rs = es - s\n    return '%s (remain %s)' % (asMinutes(s), asMinutes(rs))","b7f0335c":"def seed_torch(seed=42):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n\nseed_torch(seed=CFG.seed)","4a4b2d59":"class Encoder(nn.Module):\n    def __init__(self, model_name='resnet200d', pretrained=False):\n        super().__init__()\n        self.cnn = timm.create_model(model_name, pretrained=pretrained)\n        self.model_name = model_name\n        if model_name == 'resnet101' or model_name == 'resnet34' or model_name == 'resnet200d':\n            self.n_features = self.cnn.fc.in_features\n            self.cnn.global_pool = nn.Identity()\n            self.cnn.fc = nn.Identity()\n        \n        if model_name == 'tf_efficientnet_b0_ns' or model_name == 'tf_efficientnet_b3_ns' or model_name == 'tf_efficientnet_b4_ns':\n            self.n_features = self.cnn.classifier.in_features\n            self.cnn.global_pool = nn.Identity()\n            self.cnn.classifier = nn.Identity()\n            \n    def forward(self, x):\n        bs = x.size(0)\n        features = self.cnn(x)\n        features = features.permute(0, 2, 3, 1)\n        return features","326dc7fe":"class Attention(nn.Module):\n    \"\"\"\n    Attention network for calculate attention value\n    \"\"\"\n    def __init__(self, encoder_dim, decoder_dim, attention_dim):\n        \"\"\"\n        :param encoder_dim: input size of encoder network\n        :param decoder_dim: input size of decoder network\n        :param attention_dim: input size of attention network\n        \"\"\"\n        super(Attention, self).__init__()\n        self.encoder_att = nn.Linear(encoder_dim, attention_dim)  # linear layer to transform encoded image\n        self.decoder_att = nn.Linear(decoder_dim, attention_dim)  # linear layer to transform decoder's output\n        self.full_att = nn.Linear(attention_dim, 1)  # linear layer to calculate values to be softmax-ed\n        self.relu = nn.ReLU()\n        self.softmax = nn.Softmax(dim=1) # softmax layer to calculate weights\n\n    def forward(self, encoder_out, decoder_hidden):\n        att1 = self.encoder_att(encoder_out)  # (batch_size, num_pixels, attention_dim)\n        att2 = self.decoder_att(decoder_hidden)  # (batch_size, attention_dim)\n        att = self.full_att(self.relu(att1 + att2.unsqueeze(1))).squeeze(2)  # (batch_size, num_pixels)\n        alpha = self.softmax(att)  # (batch_size, num_pixels)\n        attention_weighted_encoding = (encoder_out * alpha.unsqueeze(2)).sum(dim=1)  # (batch_size, encoder_dim)\n        return attention_weighted_encoding, alpha","6b8923bb":"class DecoderWithAttention(nn.Module):\n    \"\"\"\n    Decoder network with attention network used for training\n    \"\"\"\n\n    def __init__(self, attention_dim, embed_dim, decoder_dim, vocab_size, device, encoder_dim=2048, dropout=0.5):\n        \"\"\"\n        :param attention_dim: input size of attention network\n        :param embed_dim: input size of embedding network\n        :param decoder_dim: input size of decoder network\n        :param vocab_size: total number of characters used in training\n        :param encoder_dim: input size of encoder network\n        :param dropout: dropout rate\n        \"\"\"\n        super(DecoderWithAttention, self).__init__()\n        self.encoder_dim = encoder_dim\n        self.attention_dim = attention_dim\n        self.embed_dim = embed_dim\n        self.decoder_dim = decoder_dim\n        self.vocab_size = vocab_size\n        self.dropout = dropout\n        self.device = device\n        self.attention = Attention(encoder_dim, decoder_dim, attention_dim)  # attention network\n        self.embedding = nn.Embedding( self.vocab_size, embed_dim)  # embedding layer\n        self.dropout = nn.Dropout(p=self.dropout)\n        self.decode_step = nn.LSTMCell(embed_dim + encoder_dim, decoder_dim, bias=True)  # decoding LSTMCell\n        self.init_h = nn.Linear(encoder_dim, decoder_dim)  # linear layer to find initial hidden state of LSTMCell\n        self.init_c = nn.Linear(encoder_dim, decoder_dim)  # linear layer to find initial cell state of LSTMCell\n        self.f_beta = nn.Linear(decoder_dim, encoder_dim)  # linear layer to create a sigmoid-activated gate\n        self.sigmoid = nn.Sigmoid()\n        self.fc = nn.Linear(decoder_dim,vocab_size)  # linear layer to find scores over vocabulary\n        self.init_weights()  # initialize some layers with the uniform distribution\n        \n    def init_weights(self):\n        self.embedding.weight.data.uniform_(-0.1, 0.1)\n        self.fc.bias.data.fill_(0)\n        self.fc.weight.data.uniform_(-0.1, 0.1)\n\n    def load_pretrained_embeddings(self, embeddings):\n        self.embedding.weight = nn.Parameter(embeddings)\n\n    def fine_tune_embeddings(self, fine_tune=True):\n        for p in self.embedding.parameters():\n            p.requires_grad = fine_tune\n\n    def init_hidden_state(self, encoder_out):\n        mean_encoder_out = encoder_out.mean(dim=1)\n        h = self.init_h(mean_encoder_out)  # (batch_size, decoder_dim)\n        c = self.init_c(mean_encoder_out)\n        return h, c\n    def forward(self, encoder_out, encoded_captions, caption_lengths):\n        \"\"\"\n        :param encoder_out: output of encoder network\n        :param encoded_captions: transformed sequence from character to integer\n        :param caption_lengths: length of transformed sequence\n        \"\"\"\n        batch_size = 32\n        encoder_dim = encoder_out.size(-1)\n        vocab_size = self.vocab_size\n        encoder_out = encoder_out.view(batch_size, -1, encoder_dim)  # (batch_size, num_pixels, encoder_dim)\n        num_pixels = encoder_out.size(1)\n        caption_lengths, sort_ind = caption_lengths.sort(dim=0, descending=True)\n        encoder_out = encoder_out[sort_ind]\n        encoded_captions = encoded_captions[sort_ind]\n        # embedding transformed sequence for vector\n        embeddings = self.embedding(encoded_captions)  # (batch_size, max_caption_length, embed_dim)\n        # initialize hidden state and cell state of LSTM cell\n        h, c = self.init_hidden_state(encoder_out) # (batch_size, decoder_dim)\n        # set decode length by caption length - 1 because of omitting start token\n        decode_lengths = (caption_lengths - 1).tolist()\n        predictions = torch.zeros(batch_size, max(decode_lengths), vocab_size).to(self.device)\n        alphas = torch.zeros(batch_size, max(decode_lengths), num_pixels).to(self.device)\n        # predict sequence\n        for t in range(max(decode_lengths)):\n            batch_size_t = sum([l > t for l in decode_lengths])\n            attention_weighted_encoding, alpha = self.attention(encoder_out[:batch_size_t], h[:batch_size_t])\n            gate = self.sigmoid(self.f_beta(h[:batch_size_t]))  # gating scalar, (batch_size_t, encoder_dim)\n            attention_weighted_encoding = gate * attention_weighted_encoding\n            h, c = self.decode_step(\n                torch.cat([embeddings[:batch_size_t, t, :], attention_weighted_encoding], dim=1),\n                (h[:batch_size_t], c[:batch_size_t]))  # (batch_size_t, decoder_dim)\n            preds = self.fc(self.dropout(h))  # (batch_size_t, vocab_size)\n            predictions[:batch_size_t, t, :] = preds\n            alphas[:batch_size_t, t, :] = alpha\n        return predictions, encoded_captions, decode_lengths, alphas, sort_ind\n    \n    def predict(self, encoder_out, decode_lengths, caracter2index):\n        batch_size = encoder_out.size(0)\n        encoder_dim = encoder_out.size(-1)\n        vocab_size = self.vocab_size\n        encoder_out = encoder_out.view(batch_size, -1, encoder_dim)  # (batch_size, num_pixels, encoder_dim)\n        num_pixels = encoder_out.size(1)\n        # embed start tocken for LSTM input\n        start_tockens = torch.ones(batch_size, dtype=torch.long).to(self.device) * caracter2index[\"SOS\"]\n        embeddings = self.embedding(start_tockens)\n        # initialize hidden state and cell state of LSTM cell\n        h, c = self.init_hidden_state(encoder_out)  # (batch_size, decoder_dim)\n        predictions = torch.zeros(batch_size, decode_lengths, vocab_size).to(self.device)\n        # predict sequence\n        for t in range(decode_lengths):\n            attention_weighted_encoding, alpha = self.attention(encoder_out, h)\n            gate = self.sigmoid(self.f_beta(h))  # gatting scalar, (batch_size_t, encoder_dim)\n            attention_weighted_encoding = gate * attention_weighted_encoding\n            h, c = self.decode_step(\n                torch.cat([embeddings, attention_weighted_encoding], dim=1),\n                (h, c))  # (batch_size_t, decoder_dim)\n            preds = self.fc(self.dropout(h))  # (batch_size_t, vocab_size)\n            predictions[:, t, :] = preds\n            if np.argmax(preds.detach().cpu().numpy()) == caracter2index[\"EOS\"]:\n                break\n            embeddings = self.embedding(torch.argmax(preds, -1))\n        return predictions","d829c590":"    def predict_caption(sequences,caracter2index,index2caracter):\n        caption = ''\n        preds = []\n        for seq in sequences:\n            for index in seq:\n                if index == caracter2index[\"EOS\"] or index == caracter2index[\"PAD\"]:\n                    break\n                caption += index2caracter[index]\n            preds.append(caption)\n        return preds","eb5f281f":"def parse_to_caracter(sequence):\n    batch_seq = []\n    single_seq = \"\"\n    for seq in sequence:\n        for index in seq:\n            caracter = index2caracter[index]\n            single_seq = single_seq + caracter\n        batch_seq.append(single_seq)\n    batch_seq = np.concatenate(batch_seq) \n    return batch_seq","5164f9c7":"def valid_fn(validation_loader, encoder, decoder, caracter2index, criterion, device):\n    batch_time = AverageMeter()\n    data_time = AverageMeter()\n    # switch to evaluation mode\n    encoder.eval()\n    decoder.eval()\n    #text_preds = []\n    start = end = time.time()\n    for step, (images,label,lable_lenght) in enumerate(validation_loader):\n        lable_lenght,index = torch.sort(lable_lenght,descending = True)\n        label = label[index]\n        images = images[index]\n        # measure data loading time\n        data_time.update(time.time() - end)\n        images = images.to(device)\n        batch_size = images.size(0)\n        with torch.no_grad():\n            features = encoder(images)\n            predictions = decoder.predict(features, CFG.max_len, caracter2index)\n        predicted_sequence = torch.argmax(predictions.detach().cpu(), -1).numpy()\n        _text_preds = predict_caption(predicted_sequence,caracter2index,index2caracter)\n        #text_preds.append(_text_preds)\n        # measure elapsed time\n        batch_time.update(time.time() - end)\n        end = time.time()\n        if step % 1 == 0 or step == (len(validation_loader)-1):\n            print('EVAL: [{0}\/{1}] '\n                  'Data {data_time.val:.3f} ({data_time.avg:.3f}) '\n                  'Elapsed {remain:s} '\n                  .format(\n                   step, len(validation_loader), batch_time=batch_time,\n                   data_time=data_time,\n                   remain=timeSince(start, float(step+1)\/len(validation_loader)),\n                   ))\n    #label= parse_to_caracter(label)\n    return label , _text_preds","58edafe0":"def train_fn(train_loader, encoder, decoder, criterion, \n             encoder_optimizer, decoder_optimizer, epoch,\n             encoder_scheduler, decoder_scheduler, device):\n    batch_time = AverageMeter()\n    data_time = AverageMeter()\n    losses = AverageMeter()\n    # switch to train mode\n    encoder.train()\n    decoder.train()\n    start = end = time.time()\n    global_step = 0\n    for step, (images, labels, label_lengths) in enumerate(train_loader):\n        # measure data loading time\n        cap_lens,index = torch.sort(label_lengths,descending = True)\n        labels = labels[index]\n        images = images[index]\n        data_time.update(time.time() - end)\n        images = images.to(device)\n        labels = labels.to(device)\n        label_lengths = label_lengths.to(device)\n        batch_size = images.size(0)\n        features = encoder(images)\n        #features = features.squeeze(1).squeeze(1)\n        predictions, caps_sorted, decode_lengths, alphas, sort_ind = decoder(features, labels, label_lengths)\n        targets = caps_sorted[:, 1:]\n        predictions = pack_padded_sequence(predictions, decode_lengths, batch_first=True).data\n        targets = pack_padded_sequence(targets, decode_lengths, batch_first=True).data\n        loss = criterion(predictions, targets)\n        # record loss\n        losses.update(loss.item(), batch_size)\n        #if CFG.gradient_accumulation_steps > 1:\n        #    loss = loss \/ CFG.gradient_accumulation_steps\n        loss.backward()\n        encoder_grad_norm = torch.nn.utils.clip_grad_norm_(encoder.parameters(), CFG.max_grad_norm)\n        decoder_grad_norm = torch.nn.utils.clip_grad_norm_(decoder.parameters(), CFG.max_grad_norm)\n        if (step + 1) % CFG.gradient_accumulation_steps == 0:\n            encoder_optimizer.step()\n            decoder_optimizer.step()\n            encoder_optimizer.zero_grad()\n            decoder_optimizer.zero_grad()\n            global_step += 1\n         # measure elapsed time\n        batch_time.update(time.time() - end)\n        end = time.time()\n        if step % CFG.print_freq == 0 or step == (len(train_loader)-1):\n            print('Epoch: [{0}][{1}\/{2}] '\n                  'Data {data_time.val:.3f} ({data_time.avg:.3f}) '\n                  'Elapsed {remain:s} '\n                  'Loss: {loss.val:.4f}({loss.avg:.4f}) '\n                  'Encoder Grad: {encoder_grad_norm:.4f}  '\n                  'Decoder Grad: {decoder_grad_norm:.4f}  '\n                  #'Encoder LR: {encoder_lr:.6f}  '\n                  #'Decoder LR: {decoder_lr:.6f}  '\n                  .format(\n                   epoch+1, step, len(train_loader), batch_time=batch_time,\n                   data_time=data_time, loss=losses,\n                   remain=timeSince(start, float(step+1)\/len(train_loader)),\n                   encoder_grad_norm=encoder_grad_norm,\n                   decoder_grad_norm=decoder_grad_norm,\n                   #encoder_lr=encoder_scheduler.get_lr()[0],\n                   #decoder_lr=decoder_scheduler.get_lr()[0],\n                   ))\n\n    return losses.avg\n","938d91fb":"len(index2caracter)","80b79179":"    def get_scheduler(optimizer):\n        if CFG.scheduler=='ReduceLROnPlateau':\n            scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=CFG.factor, patience=CFG.patience, verbose=True, eps=CFG.eps)\n        elif CFG.scheduler=='CosineAnnealingLR':\n            scheduler = CosineAnnealingLR(optimizer, T_max=CFG.T_max, eta_min=CFG.min_lr, last_epoch=-1)\n        elif CFG.scheduler=='CosineAnnealingWarmRestarts':\n            scheduler = CosineAnnealingWarmRestarts(optimizer, T_0=CFG.T_0, T_mult=1, eta_min=CFG.min_lr, last_epoch=-1)\n        return scheduler\n     # ====================================================\n    # model & optimizer\n    # ====================================================\n    encoder = Encoder(CFG.model_name, pretrained=True)\n    encoder.to(device)\n    encoder_optimizer = Adam(encoder.parameters(), lr=CFG.encoder_lr, weight_decay=CFG.weight_decay, amsgrad=False)\n    encoder_scheduler = get_scheduler(encoder_optimizer)\n    \n    decoder = DecoderWithAttention(attention_dim=CFG.attention_dim,\n                                   embed_dim=CFG.embed_dim,\n                                   decoder_dim=CFG.decoder_dim,\n                                   vocab_size=len(index2caracter),\n                                   dropout=CFG.dropout,\n                                   device=device)\n    decoder.to(device)\n    decoder_optimizer = Adam(decoder.parameters(), lr=CFG.decoder_lr, weight_decay=CFG.weight_decay, amsgrad=False)\n    decoder_scheduler = get_scheduler(decoder_optimizer)","4d783ca0":"def get_score(y_true, y_pred):\n    scores = []\n    for true, pred in zip(y_true, y_pred):\n        score = Levenshtein.distance(true, pred)\n        scores.append(score)\n    avg_score = np.mean(scores)\n    return avg_score","0066ec29":" \ncriterion = nn.CrossEntropyLoss(ignore_index=0)\n\nbest_score = np.inf\nbest_loss = np.inf\n    \nfor epoch in range(10):\n    start_time = time.time()\n    avg_loss = train_fn(train_loader, encoder, decoder, criterion, \n                            encoder_optimizer, decoder_optimizer, epoch, \n                            encoder_scheduler, decoder_scheduler, device)\n            # eval\n    valid_labels,text_preds = valid_fn(validation_loader, encoder, decoder, caracter2index, criterion, device)\n    score = get_score(valid_labels, text_preds)\n    print(score)\n    if isinstance(encoder_scheduler, ReduceLROnPlateau):\n        encoder_scheduler.step(score)\n    elif isinstance(encoder_scheduler, CosineAnnealingLR):\n        encoder_scheduler.step()\n    elif isinstance(encoder_scheduler, CosineAnnealingWarmRestarts):\n        encoder_scheduler.step()\n            \n    if isinstance(decoder_scheduler, ReduceLROnPlateau):\n        decoder_scheduler.step(score)\n    elif isinstance(decoder_scheduler, CosineAnnealingLR):\n        decoder_scheduler.step()\n    elif isinstance(decoder_scheduler, CosineAnnealingWarmRestarts):\n        decoder_scheduler.step()\n\n    elapsed = time.time() - start_time\n\n    #(f'Epoch {epoch+1} - avg_train_loss: {avg_loss:.4f}  time: {elapsed:.0f}s')\n    #LOGGER.info(f'Epoch {epoch+1} - Score: {score:.4f}')\n        \n    if score < best_score:\n        best_score = score\n       # LOGGER.info(f'Epoch {epoch+1} - Save Best Score: {best_score:.4f} Model')\n        torch.save({'encoder': encoder.state_dict(), \n                        'encoder_optimizer': encoder_optimizer.state_dict(), \n                        'encoder_scheduler': encoder_scheduler.state_dict(), \n                        'decoder': decoder.state_dict(), \n                        'decoder_optimizer': decoder_optimizer.state_dict(), \n                        'decoder_scheduler': decoder_scheduler.state_dict(), \n                        'text_preds': text_preds,\n                       },'.\/'+f'{CFG.model_name}_fold_best.pth')","fabc2375":"def bms_collate(batch):\n    imgs, labels, label_lengths = [], [], []\n    for data_point in batch:\n        imgs.append(data_point[0])\n        labels.append(data_point[1])\n        label_lengths.append(data_point[2])\n    labels = pad_sequence(labels, batch_first=True, padding_value=tokenizer.stoi[\"<pad>\"])\n    return torch.stack(imgs), labels, torch.stack(label_lengths)","09429a9b":"decoder.train()\nfor images,captions,cap_lens in train_loader:\n    model_incp.eval()\n    batch=images.cuda()\n    captions.to(device)\n    pred = model_incp(batch)[0]\n    pred = pred.squeeze(-1).squeeze(-1)\n    print(pred.size())\n    fpred = decoder(captions.to(device),pred.to(device))\n    print(fpred)\n    ","92cfe901":"import torch\nfrom torch import nn\n\nclass Model(nn.Module):\n    def __init__(self, encoder_dim= 2048,decoder_dim = 128):\n        super(Model, self).__init__()\n        self.lstm_size = 128\n        self.embedding_dim = 128\n        self.num_layers = 32\n        self.n_vocab = len(index2caracter)\n        self.init_h = nn.Linear(encoder_dim, decoder_dim)  # linear layer to find initial hidden state of LSTMCell\n        self.init_c = nn.Linear(encoder_dim, decoder_dim)  # linear layer to find initial cell state of LSTMCell\n\n        self.embedding = nn.Embedding(\n            num_embeddings=self.n_vocab,\n            embedding_dim=self.embedding_dim,\n        )\n        self.lstm = nn.LSTM(\n            input_size=self.lstm_size,\n            hidden_size=self.lstm_size,\n            num_layers=self.num_layers,\n            dropout=0.2,\n        )\n        self.fc = nn.Linear(self.lstm_size, self.n_vocab)\n    def init_state(self, encoder_out):\n        mean_encoder_out = encoder_out.mean(dim=1)\n        return (self.init_h(encoder_out),\n                self.init_c(encoder_out))\n\n    def forward(self, x,encoder_output):\n        embed = self.embedding(x)\n        (h,c) = self.init_state(encoder_output)\n        output, state = self.lstm(embed,(h,c))\n        logits = self.fc(output)\n        return logits, state\n\n","fe5488e4":"decoder = Model()\ndecoder.to(device)\ndecoder_optimizer = Adam(decoder.parameters(), lr=CFG.decoder_lr, weight_decay=CFG.weight_decay, amsgrad=False)\n","330ff8b9":"def get_score(y_true, y_pred):\n    scores = []\n    avg_globl = []\n    for true, pred in zip(y_true, y_pred):\n        for t, p in zip(true,pred):\n            score = Levenshtein.distance(t, p)\n            scores.append(score)\n        avg_globl.append(np.mean(scores))\n    avg_score = np.mean(avg_globl)\n    return avg_score"}}