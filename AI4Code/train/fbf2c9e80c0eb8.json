{"cell_type":{"1ac435c9":"code","557adda9":"code","7194f10d":"code","0f657aa3":"code","4a4191f5":"code","a9aff170":"code","bd9baaa4":"code","d6518d90":"code","ec92336d":"code","7f998415":"code","27ba7813":"code","b0068087":"code","146ac41a":"code","5d46e408":"code","41ca746d":"code","44b0069b":"code","f09b2a0d":"code","e51f7522":"markdown","8accff4c":"markdown","82fc28a2":"markdown","d4921a43":"markdown","bec788e7":"markdown","6bb67574":"markdown","dac27d68":"markdown","43d2709a":"markdown","1b0bf4a0":"markdown","244949b2":"markdown"},"source":{"1ac435c9":"%reset -sf","557adda9":"import glob\nimport scipy as sp\nimport numpy as np\nimport pandas as pd\nfrom tqdm import tqdm_notebook\nfrom copy import deepcopy\nfrom collections import Counter\nfrom matplotlib import collections as mc\nfrom matplotlib.ticker import MaxNLocator\nimport matplotlib.pyplot as plt\n\nfrom sklearn.model_selection import StratifiedKFold, KFold\nfrom sklearn import linear_model\n\nimport lightgbm as lgb\n\n# so that we can print multiple dataframe in the same cell\nfrom IPython.display import display, HTML\ndef displayer(df): display(HTML(df.head(2).to_html()))\npd.set_option('display.max_columns', 500)","7194f10d":"# process dataset\nnrr = np.load(\"..\/input\/dbaproject\/full.npy\")\n# please only use trr to train your model\ntrr = nrr[:21*24*6]\nnrr.shape, trr.shape","0f657aa3":"linear_regression_range = [i for i in range(10)]\nfor x in range(6,7):\n    linear_regression_range.extend([i for i in range((x+1)*144-10, (x+1)*144)])\nlinear_regression_label = [pd.Timedelta('10 min'*(i+1)) for i in linear_regression_range]\nlinear_regression_label = [str(l)[:-3] for l in linear_regression_label]\nnp.save(\"linear_regression_range\", linear_regression_range)\nprint(linear_regression_range)","4a4191f5":"# investigate how an numpy array is flattened\n# to ensure we are excluding the future time slices\n# example = np.array([[[1.1, 1.2, 1.3],[1.4, 1.5, 1.6]],[[2.1, 2.2, 2.3],[2.4, 2.5, 2.6]]])\n# print(example)\n# print(example.flatten())\n# [1.1 1.2 1.3 1.4 1.5 1.6 2.1 2.2 2.3 2.4 2.5 2.6]","a9aff170":"# the target variable is the difference\ntrain_labels = trr[7*24*6+1:] - trr[7*24*6:-1]\nprint(train_labels.shape)\ntrain_labels = train_labels.flatten()\nprint(train_labels.shape)\n\n# preparing train data\ntrain_data = []\nfor i in tqdm_notebook(linear_regression_range):\n    diff = trr[7*24*6-i:-i-1] - trr[7*24*6-i-1:-i-2]\n    train_data.append(diff.flatten())\ntrain_data = np.array(train_data).transpose()","bd9baaa4":"clf = linear_model.LinearRegression()\nclf.fit(train_data, train_labels)\n\nprint(sum(clf.coef_), sum(abs(clf.coef_)))\nclf.coef_ = [c if c>0 else 0 for c in clf.coef_]\nclf.coef_ = clf.coef_\/sum(clf.coef_)","d6518d90":"plt.figure(figsize=(14,4))\nplt.title(\"Feature Importance\")\nplt.bar(linear_regression_label, clf.coef_)\nplt.xticks(rotation=45, ha='right')\nplt.xlabel(\"Time passed since timeframe\")\nplt.ylabel(\"Feature Importance\")\nplt.show()\nprint(sum(clf.coef_))","ec92336d":"np.sqrt(np.mean((clf.predict(train_data) - train_labels)**2))","7f998415":"def calc_square_error(preds, actual):\n    return (preds-actual)**2","27ba7813":"def calc_square_log_error(preds, actual):\n    preds = deepcopy(preds)\n    actual = deepcopy(actual)\n    MIN_VAL = 1\n    preds[preds<=MIN_VAL] = MIN_VAL\n    actual[actual<=MIN_VAL] = MIN_VAL\n    return (np.log(preds)-np.log(actual))**2","b0068087":"def calc_direction_error(preds, actual):\n    actual, previous, preds = actual[1:], actual[:-1], preds[1:], \n    rrr = np.random.uniform(low=-1, high=1, size=actual.shape)\n    diff_actual = actual - previous + rrr\n    diff_preds = preds - previous + rrr\n    diff_mul = diff_actual*diff_preds\n    return diff_mul<0","146ac41a":"STARTING_EVAL_INDEX = 21*24*6  # will no longer be changed\n\ndef evaluate(function_evaluated, initial_state, \n             starting_eval_index=STARTING_EVAL_INDEX):\n    k = starting_eval_index\n    square_error_loss = np.empty((0,38,26))\n    square_log_error_loss = np.empty((0,38,26))\n    direction_error_sum = np.empty((0,38,26))\n    state = initial_state\n    \n    preds = np.empty((0,38,26))\n    actual = np.empty((0,38,26))\n    for i,_ in tqdm_notebook(enumerate(nrr[k:])): # for each slice from k onwards\n        \n        # use everything before i+k to predict\n        pred, state = function_evaluated(nrr[:i+k], state) \n        preds = np.append(preds, [pred], axis=0)\n        actual = np.append(actual, [nrr[i+k]], axis=0)\n    else:\n        total_evaluated = (i+1.)*nrr.shape[1]*nrr.shape[2]\n        \n    assert preds.shape == actual.shape\n    return preds, actual","5d46e408":"def calc_errors(preds, actual):\n    square_error = calc_square_error(preds, actual)\n    square_log_error = calc_square_log_error(preds, actual)\n    direction_error = calc_direction_error(preds, actual)\n    \n    square_error_grid     = np.sqrt(np.mean(square_error, axis=0))\n    square_log_error_grid = np.sqrt(np.mean(square_log_error, axis=0))\n    direction_error_grid  = np.sqrt(np.mean(direction_error, axis=0))\n    error_grids = square_error_grid, square_log_error_grid, direction_error_grid\n    \n    results = {\n        \"root_mean_square_error_loss\": np.sqrt(np.mean(square_error)),\n        \"root_mean_square_log_error_loss\": np.sqrt(np.mean(square_log_error)),\n        \"average_direction_error\": np.mean(direction_error)\n    }\n    return error_grids, results","41ca746d":"def baseline_copylast(past_slices, state):\n    \n    # example of reading and modifying the state\n    if state[\"i\"]%1000 == 0: print(state[\"i\"])\n    state[\"i\"] = state[\"i\"]+1\n\n    # prediction, which is to take the previous slice\n    pred = past_slices[-1]\n    \n    # return the prediction and the next state\n    return pred, state\n\npreds, actual = evaluate(baseline_copylast, {\"i\": 1})\nerror_grids_baseline, results = calc_errors(preds, actual)\nresults\n# {'root_mean_square_error_loss': 235.4017819397244,\n#  'root_mean_square_log_error_loss': 0.08377936293715997}","44b0069b":"import pickle\nlinear_regression_range = np.load(\"linear_regression_range.npy\")\npickle.dump(clf, open(\"linear_regression.pkl\", 'wb'))\nclf = pickle.load(open(\"linear_regression.pkl\", 'rb'))","f09b2a0d":"def linear_regression(data, previous_state):\n    test = []\n    for i in linear_regression_range:\n        arr = data[-2-i:-1-i] - data[-3-i:-2-i]\n        test.append(arr.flatten())\n    test = np.array(test).transpose()\n    \n    preds = clf.predict(test)\n    preds = np.reshape(preds, (38,26), order='C')\n    current_forecast = preds + data[-1]\n    \n    return current_forecast, None\n\npreds, actual = evaluate(linear_regression, None)\nerror_grids_baseline, results = calc_errors(preds, actual)\nresults","e51f7522":"# Assessing the methods","8accff4c":"Specifying the range of past data to train the model on","82fc28a2":"# Training the model","d4921a43":"## Linear regression","bec788e7":"# Evaluation Module\nTo standardise evaluation, we will use an evaluation module to measure the quality of your predictions. We can add more measures as requested.\n\nYou are tasked to write a function, with a single input and output of the following specification: \n\n| Item   | Var name    | Expected type             |\n|:------:|-------------|--------------------------:|\n| Input  | past_slices | `(k, 38, 26)` numpy array |\n| Input  | state       | specified by you          |\n| Output | pred        | `(38, 26)` numpy array    |\n| Output | state       | specified by you          |\n\n<br>\n$$k > 7 \\times 24 \\times 6 = 1008$$\n<br>\n<br>\nThe evaluation function allow state to be passed, to reduce the required computation.","6bb67574":"# Testing the model\nWe carry out the same process of testing, similar to the notebook DBA-evaluation","dac27d68":"## Baseline function\nCopy the last time frame, as suggested by the client.","43d2709a":"### Preparing training data","1b0bf4a0":"# Loss metrics\nThese metrics evaluates the quality of our predictions.","244949b2":"Objective - predict the heatmap of the next time frame.\n\n$28 \\times 24 \\times 6$ frames of $38 \\times 26$ cells"}}