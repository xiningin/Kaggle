{"cell_type":{"c778f8c2":"code","f744ffb3":"code","c2e9b122":"code","4811406a":"code","44ebde33":"code","fd6b64f3":"code","7d14d198":"code","e8880c63":"code","79150638":"code","f962e40f":"code","7bcd6c2e":"code","e423f300":"code","e94087c2":"code","81a3b2ab":"code","c1ba21ed":"code","a8797756":"code","16f92c0d":"code","9c15367f":"code","20b469c3":"code","2d6a0b25":"code","432d2f62":"code","d9ab314f":"markdown","1d71759c":"markdown","81516c40":"markdown","f26673d8":"markdown","39878a44":"markdown","787a3686":"markdown","6279eefd":"markdown","86b59389":"markdown"},"source":{"c778f8c2":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom sklearn.metrics import log_loss\nfrom sklearn.model_selection import StratifiedKFold\nimport gc\nimport os\nimport matplotlib.pyplot as plt\nimport seaborn as sns \nimport lightgbm as lgb\nfrom catboost import Pool, CatBoostClassifier\nimport itertools\nimport pickle, gzip\nimport glob\nfrom sklearn.preprocessing import StandardScaler","f744ffb3":"gc.enable()\n\ntrain = pd.read_csv('..\/input\/training_set.csv')\ntrain['flux_ratio_sq'] = np.power(train['flux'] \/ train['flux_err'], 2.0)\ntrain['flux_by_flux_ratio_sq'] = train['flux'] * train['flux_ratio_sq']\n\naggs = {\n    'mjd': ['min', 'max', 'size'],\n    'passband': ['min', 'max', 'mean', 'median', 'std'],\n    'flux': ['min', 'max', 'mean', 'median', 'std','skew'],\n    'flux_err': ['min', 'max', 'mean', 'median', 'std','skew'],\n    'detected': ['mean'],\n    'flux_ratio_sq':['sum','skew'],\n    'flux_by_flux_ratio_sq':['sum','skew'],\n}\n\nagg_train = train.groupby('object_id').agg(aggs)\nnew_columns = [\n    k + '_' + agg for k in aggs.keys() for agg in aggs[k]\n]\nagg_train.columns = new_columns\nagg_train['mjd_diff'] = agg_train['mjd_max'] - agg_train['mjd_min']\nagg_train['flux_diff'] = agg_train['flux_max'] - agg_train['flux_min']\nagg_train['flux_dif2'] = (agg_train['flux_max'] - agg_train['flux_min']) \/ agg_train['flux_mean']\nagg_train['flux_w_mean'] = agg_train['flux_by_flux_ratio_sq_sum'] \/ agg_train['flux_ratio_sq_sum']\nagg_train['flux_dif3'] = (agg_train['flux_max'] - agg_train['flux_min']) \/ agg_train['flux_w_mean']\n\ndel agg_train['mjd_max'], agg_train['mjd_min']\nagg_train.head()\n\ndel train\ngc.collect()","c2e9b122":"meta_train = pd.read_csv('..\/input\/training_set_metadata.csv')\nmeta_train.head()\n\nfull_train = agg_train.reset_index().merge(\n    right=meta_train,\n    how='outer',\n    on='object_id'\n)\n\nif 'target' in full_train:\n    y = full_train['target']\n    del full_train['target']\nclasses = sorted(y.unique())\n\n# Taken from Giba's topic : https:\/\/www.kaggle.com\/titericz\n# https:\/\/www.kaggle.com\/c\/PLAsTiCC-2018\/discussion\/67194\n# with Kyle Boone's post https:\/\/www.kaggle.com\/kyleboone\nclass_weight = {\n    c: 1 for c in classes\n}\nfor c in [64, 15]:\n    class_weight[c] = 2\n\nprint('Unique classes : ', classes)","4811406a":"if 'object_id' in full_train:\n    oof_df = full_train[['object_id']]\n    del full_train['object_id'], full_train['distmod'], full_train['hostgal_specz']\n    del full_train['ra'], full_train['decl'], full_train['gal_l'],full_train['gal_b'],full_train['ddf']\n    \n    \ntrain_mean = full_train.mean(axis=0)\nfull_train.fillna(train_mean, inplace=True)\n\nfolds = StratifiedKFold(n_splits=5, shuffle=True, random_state=1)","44ebde33":"full_train_new = full_train.copy()\nss = StandardScaler()\nfull_train_ss = ss.fit_transform(full_train_new)","fd6b64f3":"from keras.models import Sequential\nfrom keras.layers import Dense,BatchNormalization,Dropout\nfrom keras.callbacks import ReduceLROnPlateau,ModelCheckpoint\nfrom keras.utils import to_categorical\nimport tensorflow as tf\nfrom keras import backend as K\nimport keras\nfrom keras import regularizers\nfrom collections import Counter\nfrom sklearn.metrics import confusion_matrix","7d14d198":"# https:\/\/www.kaggle.com\/c\/PLAsTiCC-2018\/discussion\/69795\ndef mywloss(y_true,y_pred):  \n    yc=tf.clip_by_value(y_pred,1e-15,1-1e-15)\n    loss=-(tf.reduce_mean(tf.reduce_mean(y_true*tf.log(yc),axis=0)\/wtable))\n    return loss","e8880c63":"def multi_weighted_logloss(y_ohe, y_p):\n    \"\"\"\n    @author olivier https:\/\/www.kaggle.com\/ogrellier\n    multi logloss for PLAsTiCC challenge\n    \"\"\"\n    classes = [6, 15, 16, 42, 52, 53, 62, 64, 65, 67, 88, 90, 92, 95]\n    class_weight = {6: 1, 15: 2, 16: 1, 42: 1, 52: 1, 53: 1, 62: 1, 64: 2, 65: 1, 67: 1, 88: 1, 90: 1, 92: 1, 95: 1}\n    # Normalize rows and limit y_preds to 1e-15, 1-1e-15\n    y_p = np.clip(a=y_p, a_min=1e-15, a_max=1-1e-15)\n    # Transform to log\n    y_p_log = np.log(y_p)\n    # Get the log for ones, .values is used to drop the index of DataFrames\n    # Exclude class 99 for now, since there is no class99 in the training set \n    # we gave a special process for that class\n    y_log_ones = np.sum(y_ohe * y_p_log, axis=0)\n    # Get the number of positives for each class\n    nb_pos = y_ohe.sum(axis=0).astype(float)\n    # Weight average and divide by the number of positives\n    class_arr = np.array([class_weight[k] for k in sorted(class_weight.keys())])\n    y_w = y_log_ones * class_arr \/ nb_pos    \n    loss = - np.sum(y_w) \/ np.sum(class_arr)\n    return loss","79150638":"from keras.layers import PReLU, ReLU, Activation","f962e40f":"K.clear_session()\ndef build_model(dropout_rate=0.25, activation='relu'):\n    start_neurons = 256\n    # create model\n    model = Sequential()\n    \n    model.add(Dense(start_neurons, input_dim=full_train_ss.shape[1], activation=None))\n    model.add(BatchNormalization())\n    model.add(Activation(activation))\n    model.add(Dropout(dropout_rate))\n    \n    model.add(Dense(start_neurons\/\/2,activation=None))\n    model.add(BatchNormalization())\n    model.add(Activation(activation))\n    model.add(Dropout(dropout_rate))\n    \n    model.add(Dense(start_neurons\/\/4,activation=None))\n    model.add(BatchNormalization())\n    model.add(Activation(activation))\n    model.add(Dropout(dropout_rate))\n    \n    model.add(Dense(start_neurons\/\/8,activation=None))\n    model.add(BatchNormalization())\n    model.add(Activation(activation))\n    model.add(Dropout(dropout_rate\/2))\n    \n    model.add(Dense(len(classes), activation='softmax'))\n    \n    return model","7bcd6c2e":"unique_y = np.unique(y)\nclass_map = dict()\nfor i,val in enumerate(unique_y):\n    class_map[val] = i\n        \ny_map = np.zeros((y.shape[0],))\ny_map = np.array([class_map[val] for val in y])\ny_categorical = to_categorical(y_map)","e423f300":"y_count = Counter(y_map)\nwtable = np.zeros((len(unique_y),))\nfor i in range(len(unique_y)):\n    wtable[i] = y_count[i]\/y_map.shape[0]","e94087c2":"def plot_loss_acc(history):\n    plt.plot(history.history['loss'][1:])\n    plt.plot(history.history['val_loss'][1:])\n    plt.title('model loss')\n    plt.ylabel('val_loss')\n    plt.xlabel('epoch')\n    plt.legend(['train','Validation'], loc='upper left')\n    plt.show()\n    \n    plt.plot(history.history['acc'][1:])\n    plt.plot(history.history['val_acc'][1:])\n    plt.title('model Accuracy')\n    plt.ylabel('val_acc')\n    plt.xlabel('epoch')\n    plt.legend(['train','Validation'], loc='upper left')\n    plt.show()","81a3b2ab":"clfs = []\noof_preds = np.zeros((len(full_train_ss), len(classes)))\nepochs = 200\nbatch_size = 100\ncheckPoint = ModelCheckpoint(\".\/keras.model\",monitor='val_loss',mode = 'min', save_best_only=True, verbose=0)\nfor fold_, (trn_, val_) in enumerate(folds.split(y_map, y_map)):\n    x_train, y_train = full_train_ss[trn_], y_categorical[trn_]\n    x_valid, y_valid = full_train_ss[val_], y_categorical[val_]\n    K.clear_session()\n    model = build_model(dropout_rate=0.25, activation='tanh')    \n    model.compile(loss=mywloss, optimizer='adam', metrics=['accuracy'])\n    history = model.fit(x_train, y_train,\n                    validation_data=[x_valid, y_valid], \n                    epochs=epochs,\n                    batch_size=batch_size,shuffle=True,verbose=0,callbacks=[checkPoint])       \n    \n    plot_loss_acc(history)\n    \n    print('Loading Best Model')\n    model.load_weights('.\/keras.model')\n    # # Get predicted probabilities for each class\n    oof_preds[val_, :] = model.predict_proba(x_valid,batch_size=batch_size)\n    print(multi_weighted_logloss(y_valid, model.predict_proba(x_valid,batch_size=batch_size)))\n    clfs.append(model)\n    \nprint('MULTI WEIGHTED LOG LOSS : %.5f ' % multi_weighted_logloss(y_categorical,oof_preds))","c1ba21ed":"# http:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.metrics.confusion_matrix.html\ndef plot_confusion_matrix(cm, classes,\n                          normalize=False,\n                          title='Confusion matrix',\n                          cmap=plt.cm.Blues):\n    \"\"\"\n    This function prints and plots the confusion matrix.\n    Normalization can be applied by setting `normalize=True`.\n    \"\"\"\n    if normalize:\n        cm = cm.astype('float') \/ cm.sum(axis=1)[:, np.newaxis]\n        print(\"Normalized confusion matrix\")\n    else:\n        print('Confusion matrix, without normalization')\n\n    print(cm)\n\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=45)\n    plt.yticks(tick_marks, classes)\n\n    fmt = '.2f' if normalize else 'd'\n    thresh = cm.max() \/ 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, format(cm[i, j], fmt),\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n    plt.tight_layout()","a8797756":"# Compute confusion matrix\ncnf_matrix = confusion_matrix(y_map, np.argmax(oof_preds,axis=-1))\nnp.set_printoptions(precision=2)","16f92c0d":"sample_sub = pd.read_csv('..\/input\/sample_submission.csv')\nclass_names = list(sample_sub.columns[1:-1])\ndel sample_sub;gc.collect()","9c15367f":"# Plot non-normalized confusion matrix\nplt.figure(figsize=(10,10))\nfoo = plot_confusion_matrix(cnf_matrix, classes=class_names,normalize=True,\n                      title='Confusion matrix')\n","20b469c3":"meta_test = pd.read_csv('..\/input\/test_set_metadata.csv')\n\nimport time\n\nstart = time.time()\nchunks = 5000000\nfor i_c, df in enumerate(pd.read_csv('..\/input\/test_set.csv', chunksize=chunks, iterator=True)):\n    df['flux_ratio_sq'] = np.power(df['flux'] \/ df['flux_err'], 2.0)\n    df['flux_by_flux_ratio_sq'] = df['flux'] * df['flux_ratio_sq']\n    # Group by object id\n    agg_test = df.groupby('object_id').agg(aggs)\n    agg_test.columns = new_columns\n    agg_test['mjd_diff'] = agg_test['mjd_max'] - agg_test['mjd_min']\n    agg_test['flux_diff'] = agg_test['flux_max'] - agg_test['flux_min']\n    agg_test['flux_dif2'] = (agg_test['flux_max'] - agg_test['flux_min']) \/ agg_test['flux_mean']\n    agg_test['flux_w_mean'] = agg_test['flux_by_flux_ratio_sq_sum'] \/ agg_test['flux_ratio_sq_sum']\n    agg_test['flux_dif3'] = (agg_test['flux_max'] - agg_test['flux_min']) \/ agg_test['flux_w_mean']\n\n    del agg_test['mjd_max'], agg_test['mjd_min']\n#     del df\n#     gc.collect()\n    \n    # Merge with meta data\n    full_test = agg_test.reset_index().merge(\n        right=meta_test,\n        how='left',\n        on='object_id'\n    )\n    full_test[full_train.columns] = full_test[full_train.columns].fillna(train_mean)\n    full_test_ss = ss.transform(full_test[full_train.columns])\n    # Make predictions\n    preds = None\n    for clf in clfs:\n        if preds is None:\n            preds = clf.predict_proba(full_test_ss) \/ folds.n_splits\n        else:\n            preds += clf.predict_proba(full_test_ss) \/ folds.n_splits\n    \n   # Compute preds_99 as the proba of class not being any of the others\n    # preds_99 = 0.1 gives 1.769\n    preds_99 = np.ones(preds.shape[0])\n    for i in range(preds.shape[1]):\n        preds_99 *= (1 - preds[:, i])\n    \n    # Store predictions\n    preds_df = pd.DataFrame(preds, columns=class_names)\n    preds_df['object_id'] = full_test['object_id']\n    preds_df['class_99'] = 0.14 * preds_99 \/ np.mean(preds_99)\n    \n    if i_c == 0:\n        preds_df.to_csv('predictions.csv',  header=True, mode='a', index=False)\n    else: \n        preds_df.to_csv('predictions.csv',  header=False, mode='a', index=False)\n        \n    del agg_test, full_test, preds_df, preds\n#     print('done')\n    if (i_c + 1) % 10 == 0:\n        print('%15d done in %5.1f' % (chunks * (i_c + 1), (time.time() - start) \/ 60))","2d6a0b25":"z = pd.read_csv('predictions.csv')\n\nprint(z.groupby('object_id').size().max())\nprint((z.groupby('object_id').size() > 1).sum())\n\nz = z.groupby('object_id').mean()\n\nz.to_csv('single_predictions.csv', index=True)","432d2f62":"z.head()","d9ab314f":"# Test Set Predictions","1d71759c":"# Loading Libraries","81516c40":"# Extracting Features from train set","f26673d8":"# Deep Learning Begins...","39878a44":"# Standard Scaling the input (imp.)","787a3686":"# Calculating the class weights","6279eefd":"# Defining simple model in keras","86b59389":"# Merging extracted features with meta data"}}