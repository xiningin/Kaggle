{"cell_type":{"19c50906":"code","5cb9fd6f":"code","94215906":"code","c8705131":"code","351abd74":"code","d63504ac":"code","1f5596cc":"code","00dbd10b":"code","834a7eef":"code","817ec1c3":"code","f80c1fbd":"code","40ea6d02":"code","2520b0ee":"code","2b99a26f":"code","52cc2d01":"code","9c599b1c":"code","d63112ea":"code","1086dc4b":"code","88bd2133":"code","00b109d9":"code","18eea282":"code","03b73615":"code","4a78f284":"code","4a7b4cd9":"code","2cb16856":"code","159aa3d6":"code","038e34d8":"code","589f5c88":"code","74878e9f":"code","b0d78130":"code","7b0f0ca3":"code","56aa0f56":"code","98f92ff7":"code","95072440":"code","9482ec28":"code","20977662":"code","bc4355ca":"code","c4ac264f":"code","f59aa7a7":"code","a233ee05":"code","bfc5ed8e":"code","595fd266":"code","64679275":"code","36b178a5":"code","d81503b0":"code","d0c55f5f":"code","2355e6ed":"code","97864f0c":"code","7456dd76":"code","46437e5e":"code","149ac923":"markdown","8cdb3846":"markdown","06e4f143":"markdown","6aae4789":"markdown","fba107ae":"markdown","bceb86fe":"markdown","ade16cb5":"markdown","10761fc2":"markdown","167b8989":"markdown","c17bfd1d":"markdown","bed5bfc2":"markdown","ab399312":"markdown","81bd222e":"markdown","d56bc661":"markdown","4c550c9e":"markdown","31d4461b":"markdown"},"source":{"19c50906":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","5cb9fd6f":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom scipy import stats, integrate\n%matplotlib inline\n\npurchase = pd.read_csv (\"..\/input\/SupermarketPurchase.csv\")","94215906":"purchase_copy = purchase.copy()","c8705131":"purchase.tail()","351abd74":"#rename data's columns\npurchase.rename(columns={'AVG_Actual_price_12':'Avg_MRP','Purchase_Value':'Purchase_amount', 'No_of_Items':'Items_Qty','MONTH_SINCE_LAST_TRANSACTION':'TRANSACTION_MONTH'},inplace=True)","d63504ac":"# we have noticed Total Discount has -ve discount values, which is incorrect because discount cannot be negative. So the -ve sign is removed from the Total Discount\n\npurchase['Total_Discount'] = purchase['Total_Discount'] .astype(str).str.strip('\\-')\npurchase['Total_Discount']=purchase['Total_Discount'].astype(np.float)","1f5596cc":"purchase.isnull().sum(),purchase.shape","00dbd10b":"purchase.dtypes","834a7eef":"purchase.describe()","817ec1c3":"# unique values \n\nprint(\"Avg_MRP:\",purchase['Avg_MRP'].nunique())\nprint(\"Purchase Value:\",purchase['Purchase_amount'].nunique())\nprint(\"Total_Discount:\",purchase['Total_Discount'].nunique())\nprint(\"Qty of Items:\",purchase['Items_Qty'].nunique())\nprint(\"TRANSACTION_MONTH:\",purchase['TRANSACTION_MONTH'].nunique())","f80c1fbd":"pd.DataFrame([{'Avg_MRP': len(purchase['Avg_MRP'].value_counts()),    \n               'Purchase_amount': len(purchase['Purchase_amount'].value_counts()),\n               'Total_Discount': len(purchase['Total_Discount'].value_counts()),\n               'Items_Qty': len(purchase['Items_Qty'].value_counts()),\n               'TRANSACTION_MONTH':len(purchase['TRANSACTION_MONTH'].value_counts()),\n              }], columns = ['Avg_MRP', 'Purchase_amount', 'Total_Discount','Items_Qty', 'TRANSACTION_MONTH'], index=['Cust_id'])","40ea6d02":"# \nduplicate ='Entr\u00e9es dupliqu\u00e9es:{}'.format(purchase.duplicated().sum())\n\nprint(duplicate)\n\n#purchase.drop_duplicates(inplace = True)","2520b0ee":"purchase['Purchase_without_discount'] = (purchase['Purchase_amount'] + purchase['Total_Discount'])\n","2b99a26f":"fig = plt.figure(figsize=(25, 7))\nPercentPurchase =  np.round((purchase.groupby([\"Cust_id\"]).Purchase_amount.sum().\\\n                          sort_values(ascending = False)[:51].sum()\/purchase.groupby([\"Cust_id\"]).\\\n                          Purchase_amount.sum().sort_values(ascending = False).sum()) * 100, 2)\n\ng = purchase.groupby([\"Cust_id\"]).Purchase_amount.sum().sort_values(ascending = False)[:51].\\\n    plot(kind='bar', title='Top Customers: {:3.2f}% Purchase Amount'.format(PercentPurchase))\n\nfig = plt.figure(figsize=(25, 7))\n\nf1 = fig.add_subplot(121)\nPercentPurchase1 =  np.round((purchase.groupby([\"Cust_id\"]).Purchase_amount.sum().\\\n                          sort_values(ascending = False)[:10].sum()\/purchase.groupby([\"Cust_id\"]).\\\n                          Purchase_amount.sum().sort_values(ascending = False).sum()) * 100, 2)\n\ng = purchase.groupby([\"Cust_id\"]).Purchase_amount.sum().sort_values(ascending = False)[:10]\\\n    .plot(kind='bar', title='Top 10 Customers: {:3.2f}% Purchase Amount'.format(PercentPurchase1))\n    \nf2 = fig.add_subplot(122)\nPercentPurchase2 =  np.round((purchase.groupby([\"Cust_id\"]).Purchase_amount.count().\\\n                          sort_values(ascending = False)[:10].sum()\/purchase.groupby([\"Cust_id\"]).\\\n                          Purchase_amount.count().sort_values(ascending = False).sum()) * 100, 2)\n\ng = purchase.groupby([\"Cust_id\"]).Purchase_amount.count().sort_values(ascending = False)[:10].\\\n    plot(kind='bar', title='Top 10 Customers: {:3.2f}% Event Purchase'.format(PercentPurchase2))","52cc2d01":"#\n\nplt.rcParams['figure.figsize'] = (15, 6)\nplt.rcParams['font.size'] = 12\n\nplt.subplot(1, 2, 1)\nsns.countplot(purchase['TRANSACTION_MONTH'], color = 'blue')\nplt.title('Nos of Transaction Month')\n\nplt.subplot(1, 2, 2)\nsns.lineplot(x=purchase['TRANSACTION_MONTH'], y=purchase['Purchase_amount'])\nplt.title('Avg. Last Transaction Month Purchase ')\n\npurchase['TRANSACTION_MONTH'].value_counts()","9c599b1c":"\nplt.rcParams['figure.figsize'] = (25, 15)\nplt.rcParams['font.size'] = 12\n\nplt.subplot(2, 2, 1)\nsns.distplot(purchase['Purchase_without_discount'],  color = 'orange')\nplt.title('Total_Purchase without Discount')\n\nplt.subplot(2, 2, 2)\nsns.distplot(purchase['Purchase_amount'],  color = 'red')\nplt.title('Purchase after Discount')\n\nplt.subplot(2, 2, 3)\nsns.distplot(purchase['Avg_MRP'],  color = 'green')\nplt.title('Average Sales Price')\n\nplt.subplot(2, 2, 4)\nsns.distplot(purchase['Total_Discount'],  color = 'blue')\nplt.title('Total_Discount')\n","d63112ea":"purchase['Purchase_amount']=np.log1p(purchase['Purchase_amount'])\npurchase['Purchase_without_discount']=np.log1p(purchase['Purchase_without_discount'])\npurchase['Avg_MRP']=np.log1p(purchase['Avg_MRP'])\npurchase['Total_Discount']=np.log1p(purchase['Total_Discount'])\npurchase['Items_Qty']=np.log1p(purchase['Items_Qty'])","1086dc4b":"purchase.head()","88bd2133":"\nplt.rcParams['figure.figsize'] = (25, 15)\nplt.rcParams['font.size'] = 12\n\nplt.subplot(2, 2, 1)\nsns.distplot(purchase['Purchase_without_discount'],  color = 'orange')\nplt.title('Total_Purchase without Discount')\n\nplt.subplot(2, 2, 2)\nsns.distplot(purchase['Purchase_amount'],  color = 'red')\nplt.title('Purchase after Discount')\n\nplt.subplot(2, 2, 3)\nsns.distplot(purchase['Avg_MRP'],  color = 'green')\nplt.title('Avg. Sale Price')\n\nplt.subplot(2, 2, 4)\nsns.distplot(purchase['Total_Discount'],  color = 'green')\nplt.title('Discount on Total Purchase')\n","00b109d9":"purchase.describe()\n","18eea282":"purchase.corr()","03b73615":"f, ax = plt.subplots(figsize=(12,6))\n\nsns.heatmap(purchase.corr(), square=True, cmap=\"RdYlGn\",linewidth=1,annot=True,fmt='.2f');\nplt.show()","4a78f284":"purchase.drop(['Cust_id','Purchase_without_discount'],axis=1,inplace=True)","4a7b4cd9":"from sklearn.decomposition import PCA\npca = PCA(n_components=5)\npca=PCA().fit(purchase)\nprint('Expalined variance ratio:', pca.explained_variance_ratio_) # we get the ratio dividing the eigen value riance to sum of eigen value\nprint(purchase.columns.values.tolist())\nprint()\nprint('Eigen value:', pca.explained_variance_)   # these are the eigen values \nprint()\nprint(pd.DataFrame(pca.components_, columns= purchase.columns.values.tolist(),index = ['PC-1', 'PC-2', 'PC-3', 'PC-4', 'PC-5']))","2cb16856":"plt.figure(figsize=[10,8])\nplt.rcParams['font.size'] = 12\nvariance_ratio_cum_sum=np.cumsum(np.round(pca.explained_variance_ratio_, decimals=5)*100)\nprint(variance_ratio_cum_sum)\n\nplt.plot(variance_ratio_cum_sum, linestyle='dashed', linewidth = 1, \n         marker='o', markerfacecolor='blue', markersize=4)\nplt.xlabel('Principal component')\nplt.ylabel('variance')","159aa3d6":"feature_names = ['Avg_MRP', 'Purchase_amount', 'Items_Qty', 'Total_Discount', 'TRANSACTION_MONTH']\n\nplt.figure(figsize=[15,6])\nplt.rcParams['font.size'] = 14\nsns.heatmap(pca.components_[0:2,:], annot=True, cmap='viridis')\nplt.yticks([0,1],[\"PC1\", \"PC2\"],rotation=360, ha='right')\nplt.xticks(range(len(feature_names)), feature_names,rotation= 40, ha='left' )\nplt.xlabel('Features')\nplt.ylabel('Principal Component')\nplt.show()","038e34d8":"plt.figure(figsize=[10,6])\nplt.rcParams['font.size'] = 12\npca=PCA(n_components=2)\npca.fit(purchase)\nreduced_data=pca.transform(purchase)\ninverse_data=pca.inverse_transform(reduced_data)\nplt.scatter(reduced_data[:,0],reduced_data[:,1],label='reduced')\nplt.xlabel('First Principal Component')\nplt.ylabel('Second Principal Component')\nplt.show()","589f5c88":"reduced_data=pd.DataFrame(reduced_data,columns=['Dim1','Dim2'])\nreduced_data[:10]","74878e9f":"from sklearn.cluster import KMeans\nfrom sklearn.metrics import silhouette_score\nfrom matplotlib.colors import LinearSegmentedColormap\n\ncmap=LinearSegmentedColormap.from_list('BlRd',['blue','red','cyan'])\n\nsilhouette_scores=[]\nfor i in range(2,15):\n    cl=KMeans(n_clusters=i,random_state=0)\n    result=cl.fit_predict(reduced_data)\n    silhouette=silhouette_score(reduced_data,result)\n    silhouette_scores.append(silhouette)\n    plt.subplot(5,3,i-1)\n    plt.scatter(reduced_data.Dim1.values,reduced_data.Dim2.values,c=result,cmap=cmap)\n    plt.title(str(i)+' Clusters, Silhouette score :'+ str(silhouette)[:5])\n    fig,ax=plt.gcf(),plt.gca()\n    fig.set_size_inches(20,12)\n    plt.tight_layout()\nplt.show()","b0d78130":"# Plotting the silhouette scores\n\nplt.figure(figsize=[10,8])\nplt.rcParams['font.size'] = 12\nplt.plot([i for i in range(2,15)],silhouette_scores)\nplt.xlabel('# of clusters')\nplt.ylabel('silhouette score')\nplt.show()","7b0f0ca3":"cl=KMeans(n_clusters=2,random_state=0)\nresult=cl.fit_predict(reduced_data)\nsilhouette=silhouette_score(reduced_data,result)\nplt.scatter(reduced_data.Dim1.values,reduced_data.Dim2.values,c=result,cmap=cmap)\nplt.title(str(2)+' Clusters, Silhouette score :'+str(silhouette)[:5])\nplt.xlabel('Purchase_amount')\nplt.ylabel('TRANSACTION_MONTH')\nfig,ax=plt.gcf(),plt.gca()\nfig.set_size_inches(5,5)\nplt.tight_layout()\nplt.show()","56aa0f56":"kmeans=KMeans(n_clusters=4,init='k-means++',max_iter=300,random_state=0)\ny_kmeans=kmeans.fit_predict(reduced_data)\n\nreduced_data_X=reduced_data.values\nplt.figure(figsize=[15,8])\nplt.rcParams['font.size'] = 12\nplt.subplot(1, 2, 1)\nplt.scatter(reduced_data_X[y_kmeans==0,0],reduced_data_X[y_kmeans==0,1],s=50,c='red',label='X1')\nplt.scatter(reduced_data_X[y_kmeans==1,0],reduced_data_X[y_kmeans==1,1],s=50,c='blue',label='X2')\nplt.scatter(reduced_data_X[y_kmeans==2,0],reduced_data_X[y_kmeans==2,1],s=50,c='gray',label='X3')\nplt.scatter(reduced_data_X[y_kmeans==3,0],reduced_data_X[y_kmeans==3,1],s=50,c='black',label='X4')\nplt.scatter(kmeans.cluster_centers_[:,0],kmeans.cluster_centers_[:,1],s=100,c='yellow',label='centroids')\nplt.title('clusters of #4')\nplt.xlabel('Purchase_amount')\nplt.ylabel('TRANSACTION_MONTH')\nplt.legend()\n\nk_means=KMeans(n_clusters=6,init='k-means++',max_iter=300,random_state=0)\ny_k_means=k_means.fit_predict(reduced_data)\nplt.subplot(1, 2, 2)\nplt.scatter(reduced_data_X[y_k_means==0,0],reduced_data_X[y_k_means==0,1],s=50,c='red',label='X1')\nplt.scatter(reduced_data_X[y_k_means==1,0],reduced_data_X[y_k_means==1,1],s=50,c='blue',label='X2')\nplt.scatter(reduced_data_X[y_k_means==2,0],reduced_data_X[y_k_means==2,1],s=50,c='gray',label='X3')\nplt.scatter(reduced_data_X[y_k_means==3,0],reduced_data_X[y_k_means==3,1],s=50,c='black',label='X4')\nplt.scatter(reduced_data_X[y_k_means==4,0],reduced_data_X[y_k_means==4,1],s=50,c='green',label='X5')\nplt.scatter(reduced_data_X[y_k_means==5,0],reduced_data_X[y_k_means==5,1],s=50,c='pink',label='X6')\nplt.scatter(k_means.cluster_centers_[:,0],k_means.cluster_centers_[:,1],s=100,c='yellow',label='centroids')\nplt.title('clusters of #6')\nplt.xlabel('Purchase_amount')\nplt.ylabel('TRANSACTION_MONTH')\n\nplt.legend()\nplt.show()","98f92ff7":"from sklearn import preprocessing\nX_subset = purchase[feature_names] #.as_matrix()\nscaler = preprocessing.StandardScaler().fit(X_subset)\nX_scaled = scaler.transform(X_subset)\npd.DataFrame(X_scaled, columns=X_subset.columns).describe().T","95072440":"cl = 15\ncorte = 0.1\n\nanterior = 100000000000000\ncost = [] \nK_best = cl\n\nfor k in range (1, cl+1):\n    # Create a kmeans model on our data, using k clusters.  random_state helps ensure that the algorithm returns the same results each time.\n    model = KMeans(\n        n_clusters=k, \n        init='k-means++', #'random',\n        n_init=10,\n        max_iter=300,\n        tol=1e-04,\n        random_state=101)\n\n    model = model.fit(X_scaled)\n\n    # These are our fitted labels for clusters -- the first cluster has label 0, and the second has label 1.\n    labels = model.labels_\n \n    # Sum of distances of samples to their closest cluster center\n    interia = model.inertia_\n    if (K_best == cl) and (((anterior - interia)\/anterior) < corte): K_best = k - 1\n    cost.append(interia)\n    anterior = interia\n\nplt.figure(figsize=(8, 6))\nplt.scatter(range (1, cl+1), cost, c='red')\nplt.grid(True)\nplt.show()\n\n# Create a kmeans model with the best K.\nprint('The best K suggest: ',K_best)\nmodel = KMeans(n_clusters=K_best, init='k-means++', n_init=10,max_iter=300, tol=1e-04, random_state=101)\n\n# Note I'm scaling the data to normalize it! Important for good results.\nmodel = model.fit(X_scaled)\n\n# These are our fitted labels for clusters -- the first cluster has label 0, and the second has label 1.\nlabels = model.labels_\n\n# And we'll visualize it:\n#plt.scatter(X_scaled[:,0], X_scaled[:,1], c=model.labels_.astype(float))\nfig = plt.figure(figsize=(20,10))\nax = fig.add_subplot(221)\nplt.scatter(x = X_scaled[:,1], y = X_scaled[:,0], c=model.labels_.astype(float))\nax.set_xlabel(feature_names[1])\nax.set_ylabel(feature_names[0])\n\nax = fig.add_subplot(222)\nplt.scatter(x = X_scaled[:,2], y = X_scaled[:,0], c=model.labels_.astype(float))\nax.set_xlabel(feature_names[2])\nax.set_ylabel(feature_names[0])\n\n\nax = fig.add_subplot(223)\nplt.scatter(x = X_scaled[:,3], y = X_scaled[:,0], c=model.labels_.astype(float))\nax.set_xlabel(feature_names[3])\nax.set_ylabel(feature_names[0])\n\nax = fig.add_subplot(224)\nplt.scatter(x = X_scaled[:,4], y = X_scaled[:,0], c=model.labels_.astype(float))\nax.set_xlabel(feature_names[4])\nax.set_ylabel(feature_names[0])\n\nplt.grid(True)\nplt.show()\n","9482ec28":"\nsize=model.labels_\nsize=list(size)\nsize","20977662":"## Size of each cluster \nprint(size.count(0))\nprint(size.count(1))\nprint(size.count(2))\nprint(size.count(3))\nprint(size.count(4))\nprint(size.count(5))\n","bc4355ca":"purchase['Cluster'] = model.labels_\npurchase.head()","c4ac264f":"clust_profile = pd.pivot_table(purchase, values= ['Avg_MRP', 'Purchase_amount', 'Items_Qty', 'Total_Discount', 'TRANSACTION_MONTH'],index='Cluster',aggfunc=np.mean)\nclust_profile","f59aa7a7":"cluster_profile_MRP = pd.DataFrame({'labels':purchase['Cluster'],'Items_Qty':purchase['Items_Qty'],'TRANSACTION_MONTH':purchase['TRANSACTION_MONTH'],'Avg_MRP':purchase['Avg_MRP']})\ncluster_profile_MRP.sort_values('labels').head()","a233ee05":"kmean = KMeans(n_clusters=6, init='k-means++', random_state=5000)\nkmean.fit(purchase[['Items_Qty', 'Avg_MRP']])\nfig = plt.figure(figsize=(20,10))\nplt.scatter(purchase['Items_Qty'],purchase['Avg_MRP'],50,c=kmean.labels_, alpha = 0.6)\nplt.xlabel('Items_Qty')\nplt.ylabel('Avg_MRP')\n[plt.text( row.Items_Qty,row.Avg_MRP, row.TRANSACTION_MONTH) for row in purchase.itertuples()]\n\nplt.grid(True)\nplt.show()","bfc5ed8e":"purchase_copy.tail()","595fd266":"purchase_copy['Total_Discount'] = purchase_copy['Total_Discount'] .astype(str).str.strip('\\-')\npurchase_copy['Total_Discount']=purchase_copy['Total_Discount'].astype(np.float)","64679275":"# scaling the values \n\npurchase_copy['AVG_Actual_price_12']=np.log1p(purchase_copy['AVG_Actual_price_12'])\npurchase_copy['Purchase_Value']=np.log1p(purchase_copy['Purchase_Value'])\npurchase_copy['Total_Discount']=np.log1p(purchase_copy['Total_Discount'])\npurchase_copy['No_of_Items']=np.log1p(purchase_copy['No_of_Items'])\n","36b178a5":"purchase_copy.head()","d81503b0":"purchase_copy.isnull().sum(),purchase_copy.shape","d0c55f5f":"# dropping the No_of items and AVG_Actual_price_12 due to high coorelation value.\nfeature1= purchase_copy.iloc[:,0:6]\nfeature1.drop(['Cust_id',],axis=1,inplace=True)\nfeature1 = pd.pivot_table(purchase_copy, values= ['AVG_Actual_price_12','Purchase_Value','No_of_Items'],index='Total_Discount',aggfunc=np.mean)\n#feature1.tail()","2355e6ed":"X=feature1.values\n","97864f0c":"from sklearn.cluster import KMeans\nwcss=[]\nfor i in range(1,10):\n    kmeans=KMeans(n_clusters=i,init='k-means++',)\n    kmeans.fit(X)\n    wcss.append(kmeans.inertia_)\n    \nplt.figure(figsize=[10,8])\nplt.rcParams['font.size'] = 12  \nplt.plot(range(1,10),wcss)\nplt.title('Elbow Method')\nplt.xlabel('No. of cluster')\nplt.ylabel('wcss: sum of dist. of sample to their closest cluster center' )\nplt.grid(True)\nplt.show()\n","7456dd76":"kmeans_1=KMeans(n_clusters=4, n_init=10,max_iter=300, tol=1e-04, random_state=10)\nkmeans_1.fit(X)\ncluster_pred=kmeans_1.predict(X)\ncluster_pred_2=kmeans_1.labels_\ncluster_center=kmeans_1.cluster_centers_","46437e5e":"# Visualising the clusters\nplt.figure(figsize=(10,8))\nplt.scatter(X[cluster_pred==0,0],X[cluster_pred==0,1], s = 50, c = 'red', label ='cluster 1' )\nplt.scatter(X[cluster_pred==1,0],X[cluster_pred==1,1], s = 50, c = 'blue', label ='cluster 2' )\nplt.scatter(X[cluster_pred==2,0],X[cluster_pred==2,1], s = 50, c = 'green', label ='cluster 3' )\nplt.scatter(X[cluster_pred==3,0],X[cluster_pred==3,1], s = 50, c = 'cyan', label = 'cluster 4')\n\nplt.scatter(cluster_center[:,0],cluster_center[:,1], s = 100, c = 'yellow', label = 'Centroids')\nplt.title('Clusters of customers')\nplt.ylabel('No_of_Items')\nplt.xlabel('AVG_Actual_price_12')\nplt.legend()\nplt.show()","149ac923":"# Model 3-  Kmean Clustering done with 6 clusters","8cdb3846":"# END","06e4f143":"The best number of clusters seem to be 4 or 2 in this case. So, the corresponding plot is save for the later usage:","6aae4789":"We managed to segment our customer in four different category based on the AVG_Actual_price and No_of_Items purchased.   \n\nFrom the correlation matrix, we know that the Avg actual price and Nos.of items having a weak negative correlation. So, when we look at the clusters, we can see a negative correlation. As the avg. price of items increases the nos. of items purchased decreases. \n\nThe cluster with high Avg. Actual price and high nos. of Items are the Target group customer (Dark Blue). <br>\nsimilarly the cluster with high avg. Actual price but low nos. of Items are the Careful group (Dark Red). <br>\nand the cluster with normal avg. Actual price and high nos. of Items are the Standard group (light Blue). <br>\nand cluster with low avg. Actual price and low nos. of Items are the Sensible group (Green).  <br>\n\n\nBy defining the segment of our customer base, we can easily focus on our target customer group for our new promotional offer, instead of focussing on each customer from different segment which will be waste of resource, time and money. \n\nBased on customer purchasing behavior from each cluster, the different promotional strategy can be offer to the customer from different clusters. Such as premium services, Discount, referral bonus, competition prize money, buy one get one free and cash back offers.  \n\nThus segmentation ease the problem of uncovering information about a firm's customer base, based on their interactions with the business.\n","fba107ae":"# Clustering Analysis \n\n1) K-Means with silhouette_scores:- starting from k=2 to k=15, to collect the silhouette scores for each of the results. Inorder to determine the best number of clusters.\n\n**Silhouette analysis on K-Means clustering** <br>\n\nSilhouette analysis can be used to study the separation distance between the resulting clusters, as a strategy to quantifying the quality of clustering via graphical tool to plot a measure of how tightly grouped the samples in the clusters are. The silhouette plot displays a measure of how close each point in one cluster is to points in the neighboring clusters and thus provides a way to assess parameters like number of clusters visually.\n\nIt can also be applied to clustering algorithms other than k-means\n\nSilhouette coefficients has a range of [-1, 1], it calculated by:\n\n1. Calculate the cluster cohesion a( i )as the average distance between a sample x( i ) and all other points in the same -cluster.<br>\n2. Calculate the cluster separation b( i ) from the next closest cluster as the average distance between the sample x( i ) and all samples in the nearest cluster.<br>\n3. Calculate the silhouette s( i ) as the difference between cluster cohesion and separation divided by the greater of the two, as shown here: <br>\n\n![image.png](attachment:image.png)\n\n\nWhich can be also written as: \n\n![image.png](attachment:image.png)\n\nWhere:\n\n1. If near +1, it indicate that the sample is far away from the neighboring clusters.<br> \n2. a high value indicates that the object is well matched to its own cluster and poorly matched to neighboring clusters.<br>\n3. If most objects have a high value, then the clustering configuration is appropriate.<br>\n4. If many points have a low or negative value, then the clustering configuration may have too many or too few clusters.<br>\n5. A value of 0 indicates that the sample is on or very close to the decision boundary between two neighboring clusters. <br>\n6. Negative values indicate that those samples might have been assigned to the wrong cluster.<br>\n\nThe silhouette plot can shows a bad K clusters pick for the given data due to the presence of clusters with below average\nsilhouette scores and also due to wide fluctuations in the size of the silhouette plots. A good k clusters can found when all the plots are more or less of similar thickness and hence are of similar sizes.<br>\n\nAlthough we have to keep in mind that in several cases and scenarios, sometimes we may have to drop the mathematical explanation given by the algorithm and look at the business relevance of the results obtained.<br>\n\nLet's see below how our data perform for each K clusters groups (2, 4 and 6) in the silhouette score of each cluster, along with the center of each of the cluster discovered in the scatter plots, by amount_log vs recency_log and vs frequency_log.","bceb86fe":"Purchase_amount and Total_Purchase_amount are strongly positively correlated to each other.So they tend to move in the same direction. ","ade16cb5":"After scaling the variable with log, we can see purchase_amount and Total_Purchase_amount are some where close to normal distribution. As we have small variation in the expected and actual mean. \n\nWhereas, the Avg_MRP is somewhere close to normaldistribution but we can see the distribution having a left tail, which shows the Avg_MRP is having is slightly higher than the actual mean.\n  ","10761fc2":"Positively skewed distribution for all variables, which means the variables are below the mean value than expected in the normal distribution. But still the distribution of Avg. MRP is better than others, as the difference between actual and expected mean and not much compare to other variables.  \n\nwe try to solve this problem by normalizing the variables . \n","167b8989":"# Visualising the clusters <br>\n\nThe cluster with high Purchase_amount and less gap in transaction month are the Target customer.<br>\n\nsimilarly the cluster with high Purchase_amount and more gap in transaction month can be prospect or impulsive customer making a buying decision in an instant, provided that the conditions are right. <br>\n\nand the cluster with low Purchase_amount and less gap in Transaction month can be Discount customer <br>\n\nand cluster with low purchase and more gap in transaction month can be Wandering customer.   <br>\n\nand cluster with normal or low purchase and more gap in transaction month can be focused Customer.  <br>\n\nand cluster with normal or low purchase and nominal gap in transaction month, the one in the middle is a Standard Customer.\n\n\n","c17bfd1d":"The first two principle components explains 93% of the data. The PCA feature is scaled with 0.5 level. So 0,1, 2,3,4 represent the principle component: PC-1, PC-2, PC-3, PC-4, PC-5\n\n","bed5bfc2":"The best number of clusters (k) is 6 with elbow method. From the scatter plot, we can infer the relation between avg_mrp and other variables for different clusters. \n\nFrom the scatter plot, we can infer that Transaction month is independent from Avg. MRP because its neither decreassing nor increasing continously.\n\nA Positive correlation is obeserved between the Avg. MRP and Purchase amount, whereas the initial datapoints reflecting parallel lines for Items_Qty and Total_Discount are neither decreassing nor increasing continously but later a negative relation is observed between Avg. MRP with Items_Qty and Total_Discount. ","ab399312":"The scatter plot shows the customer purchasing power, across the different cluster keeping in view the Avg_ MRP, Items_ Qty and last transaction month. We can see among all the cluster, the customers in light green cluster are better than other cluster. As we can see that the customer from this cluster having high purchasing power which is reflected in their purchasing pattern through the purchase of high quantity of items with high Avg_ price.","81bd222e":"The PC-1 explained 60% variance in the dataset and PC-2 explained 33% variancein dataset, together they explain 93% variance in dataset. \n\nThe values in features represent the contribution of that individual feature to the principal component. Higher the value, higher will be the contribution of that feature in the principal component.  \n\nConsidering PCA to the data with number of components = 2.\n\nThe reduced data can be seen on the plotting below.","d56bc661":"# Model 4- Kmean Clustering done with 4 clusters","4c550c9e":"# Dimensionality Reduction\n\nImplementing PCA to the data set.\nBut, before that, time to investigate the explained variance ratio and resulting principal components:","31d4461b":"# Model 2 - Elbow method \n\nFinding optimal number of cluster using Elbow method.\n\n\nThe idea behind the elbow method is to identify the value of k where the distortion begins to increase most rapidly. If k increases, the distortion will decrease, because the samples will be closer to the centroids they are assigned to.\n\nThis method looks at the percentage of variance explained as a function of the number of clusters. More precisely, if one plots the percentage of variance explained by the clusters against the number of clusters, the first clusters will add much information (explain a lot of variance), but at some point the marginal gain will drop, giving an angle in the graph. The number of clusters is chosen at this point, hence the \"elbow criterion\". This \"elbow\" cannot always be unambiguously identified.Percentage of variance explained is the ratio of the between-group variance to the total variance, also known as an F-test. A slight variation of this method plots the curvature of the within group variance.\n\n**K-Means Clustering** <br>\nThe K-means clustering belongs to the partition based\\centroid based hard clustering family of algorithms, a family of algorithms where each sample in a dataset is assigned to exactly one cluster. <br>\n\nBased on this Euclidean distance metric, we can describe the k-means algorithm as a simple optimization problem, an iterative approach for minimizing the within-cluster sum of squared errors (SSE), which is sometimes also called cluster inertia. So, the objective of K-Means clustering is to minimize total intra-cluster variance, or, the squared error function:<br>\n\n![image.png](attachment:image.png)\nThe steps that happen in the K-means algorithm for partitioning the data are as given follows:\n\nThe algorithm starts with random point initializations of the required number of centers. The \u201cK\u201d in K-means stands for the number of clusters.<br>\nIn the next step, each of the data point is assigned to the center closest to it. The distance metric used in K-means clustering is normal Euclidian distance.<br>\nOnce the data points are assigned, the centers are recalculated by averaging the dimensions of the points belonging to the cluster.<br>\nThe process is repeated with new centers until we reach a point where the assignments become stable. In this case, the algorithm terminates.<br>\n\n**K-means++** <br>\n\n* While placing the initial centroids far away from each other via the k-means++ algorithm, which leads to better and more consistent results than the classic k-means. <br>\n* To use k-means++ with scikit-learn's KMeans object, we just need to set the init parameter to k-means++ (the default setting) instead of random."}}