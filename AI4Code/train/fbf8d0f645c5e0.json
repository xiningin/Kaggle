{"cell_type":{"6334927d":"code","4436fb94":"code","ce4dfba2":"code","7b587274":"code","23e3eaf3":"code","ea8cc88c":"code","f432d6ab":"code","7ae3eb00":"code","bfd8cd41":"code","2a2a7d19":"code","5f507534":"code","843db8ce":"code","493c0153":"code","4ffa59c0":"code","4f1c19fb":"code","d673fc8f":"code","172af2e1":"code","d02c27fe":"code","3721aedf":"code","c3185e58":"code","9fdf8bd6":"code","9575ae1d":"code","1d3ab1f0":"code","ef254fcd":"code","b3db710e":"code","ec892215":"code","29de0298":"code","5ffe2e71":"code","82b3bf66":"code","f7d8f79b":"code","bc877d7a":"code","d0ad0db7":"code","7a120950":"code","143f7f91":"code","9147e376":"code","9d8eeba4":"code","d015ecc5":"code","f8059d68":"code","1586ca44":"code","95eff547":"code","d8a3bbe1":"code","00346b6f":"code","aa1936fb":"code","1c82cac9":"code","7ac3d9c8":"code","b0889720":"code","2b3e5e70":"code","5b52093b":"code","4417144a":"code","681c6e2a":"markdown","f7a28657":"markdown","b5e1989b":"markdown","c90db1db":"markdown","d67c154b":"markdown","71a519b1":"markdown","3e207546":"markdown","f1a4e1ed":"markdown","5f03836c":"markdown","d06840e0":"markdown","69dac070":"markdown","46450e4a":"markdown","5371036a":"markdown","a832e6a3":"markdown","e3fef1e6":"markdown","73494122":"markdown","bdb4f699":"markdown","c052648f":"markdown","905621d0":"markdown"},"source":{"6334927d":"import os\nimport torch\nimport tarfile\nimport torchvision\nimport torch.nn as nn \nimport matplotlib.pyplot as plt\nfrom torch.utils.data import random_split\nfrom torchvision.datasets.utils import download_url","4436fb94":"# Dowload the dataset\ndataset_url = \"https:\/\/s3.amazonaws.com\/fast-ai-imageclas\/cifar10.tgz\"\ndownload_url(dataset_url, '.\/input')","ce4dfba2":"with tarfile.open('.\/input\/cifar10.tgz', 'r:gz') as tar:\n    tar.extractall(path='.\/data')","7b587274":"data_dir = '.\/data\/cifar10'\n\nprint(os.listdir(data_dir))\nclasses = os.listdir(data_dir + \"\/train\")\nprint(classes)","23e3eaf3":"airplane_files = os.listdir(data_dir + \"\/train\/airplane\")\nprint('No. of training examples for airplanes:', len(airplane_files))\nprint(airplane_files[:5])","ea8cc88c":"ship_test_files = os.listdir(data_dir + \"\/test\/ship\")\nprint(\"No. of test examples for ship:\", len(ship_test_files))\nprint(ship_test_files[:5])","f432d6ab":"from torchvision.datasets import ImageFolder\nfrom torchvision.transforms import ToTensor","7ae3eb00":"dataset = ImageFolder(data_dir+'\/train', transform=ToTensor())","bfd8cd41":"img, label = dataset[0]\nprint(img.shape, label, len(dataset))\nimg","2a2a7d19":"def show_example(img, label):\n    print(f\"Label: {dataset.classes[label], ({label})}\")\n    plt.imshow(img.permute(1, 2, 0))    # Change  ","5f507534":"show_example(*dataset[0])","843db8ce":"show_example(*dataset[10099])","493c0153":"random_seed = 42\ntorch.manual_seed(random_seed)","4ffa59c0":"val_size = 5000\ntrain_size = len(dataset) - val_size\ntrain_ds, val_ds = random_split(dataset, [train_size, val_size])\nlen(train_ds), len(val_ds)","4f1c19fb":"from torch.utils.data.dataloader import DataLoader\n\nbatch_size = 128\ntrain_dl = DataLoader(train_ds, batch_size, shuffle=True, num_workers=4, pin_memory=True)\nval_dl = DataLoader(val_ds, batch_size*2, shuffle=False, num_workers=4, pin_memory=True)","d673fc8f":"for img, lbl in train_dl:\n    print(img.shape, lbl)\n    break","172af2e1":"from torchvision.utils import make_grid\n\ndef show_batch(dl):\n    for images, labels in dl:\n        fig, ax = plt.subplots(figsize=(12, 6))\n        ax.set_xticks([]); ax.set_yticks([])\n        ax.imshow(make_grid(images, nrow=16).permute(1, 2, 0))\n        break","d02c27fe":"show_batch(train_dl)","3721aedf":"def apply_kernel(image, kernel):\n    # both image and kernel are essentially matrices\n    ri, ci = image.shape        # Image Dimensions\n    rk, ck = kernel.shape       # kernel dimension\n    ro, co = ri-rk+1, ci-ck+1   # output dimension\n    output = torch.zeros([ro, co])\n    for i in range(ro):\n        for j in range(co):\n            output[i][j] = torch.sum(image[i:i+rk, j:j+ck]*kernel)\n            \n    return output\n    ","c3185e58":"sample_image = torch.tensor([\n    [3, 3, 2, 1, 0], \n    [0, 0, 1, 3, 1], \n    [3, 1, 2, 2, 3], \n    [2, 0, 0, 2, 2], \n    [2, 0, 0, 0, 1]\n], dtype=torch.float32)\n\nsample_kernel = torch.tensor([\n    [0, 1, 2], \n    [2, 2, 0], \n    [0, 1, 2]\n], dtype=torch.float32)\n\napply_kernel(sample_image, sample_kernel)","9fdf8bd6":"import torch.nn.functional as F ","9575ae1d":"simple_model = nn.Sequential(\n    nn.Conv2d(3, 8, kernel_size=3, stride=1, padding=1),\n    nn.MaxPool2d(2, 2) # Reduces image size to row\/2, col\/2\n)","1d3ab1f0":"for images, labels in train_dl:\n    print('Shape: ', images.shape)\n    out = simple_model(images)\n    print('output shape: ', out.shape)\n    break","ef254fcd":"class ImageClassificationBase(nn.Module):\n    def training_step(self, batch):\n        images, labels = batch\n        out = self(images)  \n        loss = F.cross_entropy(out, labels)\n        return loss\n    \n    def validation_step(self, batch):\n        images, labels = batch\n        out = self(images)\n        loss = F.cross_entropy(out, labels)\n        acc = accuracy(out, labels)\n        \n        return {'val_loss': loss, 'val_acc': acc}\n    \n    def validation_epoch_end(self, outputs):\n        batch_losses = [x['val_loss'] for x in outputs]\n        epoch_loss = torch.stack(batch_losses).mean()\n        batch_accs = [x['val_acc'] for x in outputs]\n        epoch_acc = torch.stack(batch_accs).mean()\n        \n        return {'val_loss': epoch_loss, 'val_acc': epoch_acc}\n        \n    def epoch_end(self, epoch, result):\n        print(\"Epoch [{}], train_loss: {:.4f}, val_loss: {:.4f}, val_acc: {:.4f}\".format(\n            epoch, result['train_loss'], result['val_loss'], result['val_acc']))\n        \n\ndef accuracy(outputs, labels):\n    _, preds = torch.max(outputs, dim=1)\n    return torch.tensor(torch.sum(preds == labels).item() \/ len(preds))","b3db710e":"# A quick reminder on how torch.max functions\na = torch.randn(4, 4)\na","ec892215":"_, p = torch.max(a, dim=1)\np \n","29de0298":"class CnnModel(ImageClassificationBase):\n    def __init__(self):\n        super().__init__()\n        self.network = nn.Sequential(\n            nn.Conv2d(3, 32, kernel_size=3, padding=1),\n            nn.ReLU(),\n            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),\n            nn.ReLU(),\n            nn.MaxPool2d(2, 2),  # output 64 x 16 x 16\n            \n            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),\n            nn.ReLU(),\n            nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=1),\n            nn.ReLU(),\n            nn.MaxPool2d(2, 2),  # output: 128 x 8 x 8\n            \n            nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1),\n            nn.ReLU(),\n            nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1),\n            nn.ReLU(),\n            nn.MaxPool2d(2, 2), # output: 256 x 4 x 4\n            \n            nn.Flatten(),\n            nn.Linear(256*4*4, 1024),\n            nn.ReLU(),\n            nn.Linear(1024, 512),\n            nn.Linear(512, 10))\n        \n    def forward(self, xb):\n        return self.network(xb)","5ffe2e71":"model = CnnModel()\nmodel","82b3bf66":"for images, labels in train_dl:\n    print(\"Images Shape: \", images.shape)\n    out = model(images)\n    print(\"Output Shape: \", out.shape)\n    print(\"Out[0]: \", out[0])\n    break","f7d8f79b":"def get_default_device():\n    \"\"\"Pick GPU if available, else CPU\"\"\"\n    if torch.cuda.is_available():\n        return torch.device('cuda')\n    else:\n        return torch.device('cpu')\n    \ndef to_device(data, device):\n    \"\"\"Move tensor(s) to chosen device\"\"\"\n    if isinstance(data, (list,tuple)):\n        return [to_device(x, device) for x in data]\n    return data.to(device, non_blocking=True)\n\nclass DeviceDataLoader():\n    \"\"\"Wrap a dataloader to move data to a device\"\"\"\n    def __init__(self, dl, device):\n        self.dl = dl\n        self.device = device\n        \n    def __iter__(self):\n        \"\"\"Yield a batch of data after moving it to device\"\"\"\n        for b in self.dl: \n            yield to_device(b, self.device)\n\n    def __len__(self):\n        \"\"\"Number of batches\"\"\"\n        return len(self.dl)","bc877d7a":"device = get_default_device()\ndevice","d0ad0db7":"train_dl = DeviceDataLoader(train_dl, device)\nval_dl = DeviceDataLoader(val_dl, device)\nto_device(model, device)","7a120950":"@torch.no_grad()\ndef evaluate(model, val_loader):\n    model.eval()\n    outputs = [model.validation_step(batch) for batch in val_loader]\n    return model.validation_epoch_end(outputs)\n\ndef fit(epochs, lr, model, train_loader, val_loader, opt_func=torch.optim.SGD):\n    history = []\n    optimizer = opt_func(model.parameters(), lr)\n    for epoch in range(epochs):\n        # Training Phase\n        model.train()\n        train_losses = []\n        for batch in train_loader:\n            loss = model.training_step(batch)\n            train_losses.append(loss)\n            loss.backward()     # Calculate Gradients\n            optimizer.step()\n            optimizer.zero_grad()\n            \n        # Validation Phase\n        result = evaluate(model, val_loader)\n        result['train_loss'] = torch.stack(train_losses).mean().item()\n        model.epoch_end(epoch, result)\n        history.append(result)\n        \n    return history","143f7f91":"# model = to_device(CnnModel(), device)","9147e376":"evaluate(model, train_dl)","9d8eeba4":"num_epochs = 10\nopt_func = torch.optim.Adam\nlr = 0.001 ","d015ecc5":"history = fit(num_epochs, lr, model, train_dl, val_dl, opt_func)","f8059d68":"def plot(history):\n    fig, ax1 = plt.subplots()\n    accuracies = [x['val_acc'] for x in history]\n    ax1.plot(accuracies, '-x')\n    plt.xlabel('epoch')\n    plt.ylabel('accuracies')\n    plt.title('Accuracy vs No. of Epochs')\n    \n    fig, ax2 = plt.subplots()\n    losses = [x['val_loss'].cpu() for x in history]\n    ax2.plot(losses, '-x')\n    plt.xlabel('epoch')\n    plt.ylabel('Losses')\n    plt.title('Losses vs No. of Epochs')\n    \n    t_losses = [x['train_loss'] for x in history]\n    ax2.plot(t_losses, '-rx')\n    plt.legend(['Training', 'Validation'])","1586ca44":"plot(history)","95eff547":"test_dataset = ImageFolder(data_dir+'\/test', transform=ToTensor())","d8a3bbe1":"def predict_image(image, model):\n    # Convert to a Batch of 1\n    xb = to_device(image.unsqueeze(0), device)\n    # Get predictions from model\n    yb = model(xb)\n    # Pick the appropriate index\n    _, preds = torch.max(yb, dim=1)\n    # retrieve class label\n    return dataset.classes[preds[0].item()]","00346b6f":"img, label = test_dataset[0]\nplt.imshow(img.permute(1, 2, 0))\nprint(f\"Label: {dataset.classes[label]}, Predicted: {predict_image(img, model)}\")","aa1936fb":"img, label = test_dataset[1002]\nplt.imshow(img.permute(1, 2, 0))\nprint('Label:', dataset.classes[label], ', Predicted:', predict_image(img, model))","1c82cac9":"img, label = test_dataset[6153]\nplt.imshow(img.permute(1, 2, 0))\nprint('Label:', dataset.classes[label], ', Predicted:', predict_image(img, model))","7ac3d9c8":"test_loader = DeviceDataLoader(DataLoader(test_dataset, batch_size*2), device)\nresult = evaluate(model, test_loader)\nresult","b0889720":"os.mkdir('.\/model')\ntorch.save(model.state_dict(), '.\/model\/cifar10-cnn.pth')","2b3e5e70":"model2 = to_device(CnnModel(), device)","5b52093b":"model2.load_state_dict(torch.load('.\/model\/cifar10-cnn.pth'))","4417144a":"# Verifying the model has been loaded\nevaluate(model2, test_loader)","681c6e2a":"Based on where you're running this notebook, your default device could be a CPU (`torch.device('cpu')`) or a GPU (`torch.device('cuda')`)","f7a28657":"## Training and Validation Datasets\n\nWhile building real world machine learning models, it is quite common to split the dataset into 3 parts:\n\n1. **Training set** - used to train the model i.e. compute the loss and adjust the weights of the model using gradient descent.\n2. **Validation set** - used to evaluate the model while training, adjust hyperparameters (learning rate etc.) and pick the best version of the model.\n3. **Test set** - used to compare different models, or different types of modeling approaches, and report the final accuracy of the model.\n\nSince there's no predefined validation set, we can set aside a small portion (5000 images) of the training set to be used as the validation set. We'll use the `random_split` helper method from PyTorch to do this. To ensure that we always create the same validation set, we'll also set a seed for the random number generator.","b5e1989b":"\nWe'll use `nn.Sequential` to chain the layers and activations functions into a single network architecture.","c90db1db":"Let's verify that the model produces the expected output on a batch of training data. The 10 outputs for each image can be interpreted as probabilities for the 10 target classes (after applying softmax), and the class with the highest probability is chosen as the label predicted by the model for the input image. Check out [Part 3 (logistic regression)](https:\/\/jovian.ml\/aakashns\/03-logistic-regression#C50) for a more detailed discussion on interpeting the outputs, applying softmax and identifying the predicted labels.","d67c154b":"## Defining the Model (Convolutional Neural Network)\n\n> The 2D convolution is a fairly simple operation at heart: you start with a kernel, which is simply a small matrix of weights. This kernel \u201cslides\u201d over the 2D input data, performing an elementwise multiplication with the part of the input it is currently on, and then summing up the results into a single output pixel. - [Source](https:\/\/towardsdatascience.com\/intuitively-understanding-convolutions-for-deep-learning-1f6f42faee1)\n\n<img src=\"https:\/\/miro.medium.com\/max\/1070\/1*Zx-ZMLKab7VOCQTxdZ1OAw.gif\" style=\"max-width:400px;\">\n\n\nLet us implement a convolution operation on a 1 channel image with a 3x3 kernel.","71a519b1":"Let's look inside a couple of folders, one from the training set and another from the test set. As an exercise, you can verify that that there are an equal number of images for each class, 5000 in the training set and 1000 in the test set.","3e207546":"The `Conv2d` layer transforms a 3-channel image to a 16-channel *feature map*, and the `MaxPool2d` layer halves the height and width. The feature map gets smaller as we add more layers, until we are finally left with a small feature map, which can be flattened into a vector. We can then add some fully connected layers at the end to get vector of size 10 for each image.\n\n<img src=\"https:\/\/i.imgur.com\/KKtPOKE.png\" style=\"max-width:540px\">\n\nLet's define the model by extending an `ImageClassificationBase` class which contains helper methods for training & validation.","f1a4e1ed":"Lets See how convolutions work for multiple channels in a colour image.\n- Each of the kernels of the filter \u201cslides\u201d over their respective input channels, producing a processed version of each. Some kernels may have stronger weights than others, to give more emphasis to certain input channels than others (eg. a filter may have a red kernel channel with stronger weights than others, and hence, respond more to differences in the red channel features than the others).\n\n![](https:\/\/miro.medium.com\/max\/2000\/1*8dx6nxpUh2JqvYWPadTwMQ.gif)\n\n- Each of the per-channel processed versions are then summed together to form one channel. The kernels of a filter each produce one version of each channel, and the filter as a whole produces one overall output channel.\n\n![](https:\/\/miro.medium.com\/max\/2000\/1*CYB2dyR3EhFs1xNLK8ewiA.gif)\n\n- Finally, then there\u2019s the bias term. The way the bias term works here is that each output filter has one bias term. The bias gets added to the output channel so far to produce the final output channel.\n\n![](https:\/\/miro.medium.com\/max\/588\/1*RYYucIh3U-YFxrIkyQKzRw.gif)\n\n","5f03836c":"We can now wrap our training and validation data loaders using `DeviceDataLoader` for automatically transferring batches of data to the GPU (if available), and use `to_device` to move our model to the GPU (if available).","d06840e0":"## Testing with individual images\n\nWhile we have been tracking the overall accuracy of a model so far, it's also a good idea to look at model's results on some sample images. Let's test out our model with some images from the predefined test dataset of 10000 images. We begin by creating a test dataset using the `ImageFolder` class.\n","69dac070":"Initialy, both the training and validation losses seem to decrease over time. However, if you train the model for long enough, you will notice that the training loss continues to decrease, while the validation loss stops decreasing, and even starts to increase after a certain point! \n\n<img src=\"https:\/\/i.stack.imgur.com\/1QU0m.png\" style=\"max-width:400px;\">\n\nThis phenomenon is called **overfitting**, and it is the no. 1 why many machine learning models give rather terrible results on real-world data. It happens because the model, in an attempt to minimize the loss, starts to learn patters are are unique to the training data, sometimes even memorizing specific training examples. Because of this, the model does not generalize well to previously unseen data.\n\n\nFollowing are some common stragegies for avoiding overfitting:\n\n- Gathering and generating more training data, or adding noise to it\n- Using regularization techniques like batch normalization & dropout\n- Early stopping of model's training, when validation loss starts to increase\n\nWe will cover these topics in more detail in the next tutorial in this series, and learn how we can reach an accuracy of **over 90%** by making minor but important changes to our model.","46450e4a":"The initial accuracy is around 10%, which is what one might expect from a randomly intialized model (since it has a 1 in 10 chance of getting a label right by guessing randomly).\n\nWe'll use the following *hyperparmeters* (learning rate, no. of epochs, batch_size etc.) to train our model. As an exercise, you can try changing these to see if you have achieve a higher accuracy in a shorter time. ","5371036a":"To seamlessly use a GPU, if one is available, we define a couple of helper functions (`get_default_device` & `to_device`) and a helper class `DeviceDataLoader` to move our model & data to the GPU as required. These are described in more detail in the [previous tutorial](https:\/\/jovian.ml\/aakashns\/04-feedforward-nn#C21).","a832e6a3":"The above directory structure (one folder per class) is used by many computer vision datasets, and most deep learning libraries provide utilites for working with such datasets. We can use the `ImageFolder` class from `torchvision` to load the data as PyTorch tensors.","e3fef1e6":"For multi-channel images, a different kernel is applied to each channels, and the outputs are added together pixel-wise. \n\nChecking out the following articles to gain a better understanding of convolutions:\n\n1. [Intuitively understanding Convolutions for Deep Learning](https:\/\/towardsdatascience.com\/intuitively-understanding-convolutions-for-deep-learning-1f6f42faee1) by Irhum Shafkat\n2. [Convolutions in Depth](https:\/\/sgugger.github.io\/convolution-in-depth.html) by Sylvian Gugger (this article implements convolutions from scratch)\n\nThere are certain advantages offered by convolutional layers when working with image data:\n\n* **Fewer parameters**: A small set of parameters (the kernel) is used to calculate outputs of the entire image, so the model has much fewer parameters compared to a fully connected layer. \n* **Sparsity of connections**: In each layer, each output element only depends on a small number of input elements, which makes the forward and backward passes more efficient.\n* **Parameter sharing and spatial invariance**: The features learned by a kernel in one part of the image can be used to detect similar pattern in a different part of another image.\n\nWe will also use a [max-pooling](https:\/\/computersciencewiki.org\/index.php\/Max-pooling_\/_Pooling) layers to progressively decrease the height & width of the output tensors from each convolutional layer.\n\n<img src=\"https:\/\/computersciencewiki.org\/images\/8\/8a\/MaxpoolSample2.png\" style=\"max-width:400px;\">\n\nBefore we define the entire model, let's look at how a single convolutional layer followed by a max-pooling layer operates on the data.","73494122":"# Image Classifiction using CNNs\n\n## Exploring the CIFAR10 Dataset\n\nIn this notebook, we'll use the CIFAR10 dataset, which consists of 60000 32x32 px colour images in 10 classes. Here are some sample images from the dataset:\n\n<img src=\"https:\/\/miro.medium.com\/max\/709\/1*LyV7_xga4jUHdx4_jHk1PQ.png\" style=\"max-width:480px\">\n\nWe will use `Convolutional Neural Networks` to make an Image Classification Model","bdb4f699":"The dataset is extracted to the directory `data\/cifar10`. It contains 2 folders `train` and `test`, containing the training set (50000 images) and test set (10000 images) respectively. Each of them contains 10 folders, one for each class of images. Let's verify this using `os.listdir`.","c052648f":"## Training the Model\n\nWe'll define two functions: `fit` and `evaluate` to train the model using gradient descent and evaluate its performance on the validation set. For a detailed walkthrough of these functions, check out the [previous tutorial](https:\/\/jovian.ai\/aakashns\/03-logistic-regression).","905621d0":"## Saving and loading the model\n\nSince we've trained our model for a long time and achieved a resonable accuracy, it would be a good idea to save the weights of the model to disk, so that we can reuse the model later and avoid retraining from scratch. Here's how you can save the model."}}