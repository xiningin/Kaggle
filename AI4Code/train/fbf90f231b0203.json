{"cell_type":{"8142b66a":"code","84f58875":"code","b997373a":"code","f18e1306":"code","cb45d2e0":"code","bbb7eecb":"code","a56e9ed4":"code","783edfb0":"code","bf1df47b":"code","db524505":"code","1d4726b2":"code","c5fffe05":"code","852b43ab":"code","e360b835":"code","fbd65da2":"code","dd31b59d":"code","a0b6f19b":"code","5d270928":"code","2938bf64":"code","17053ac0":"code","980e84c2":"code","4847098e":"code","52341299":"code","fd6e6bc9":"code","8760d2a0":"code","cbb47ccb":"code","5b8f747b":"code","4ce62c03":"code","4b5124d3":"code","f48f5eb4":"code","4c53c122":"code","8955a507":"code","1557fbaa":"code","e5168fcc":"code","adaca9fd":"code","ed1c557e":"code","59c07cc8":"code","a7f2de99":"code","42ce39db":"code","f129b8f9":"code","233dedd6":"code","ec65bfed":"code","bfbebd17":"code","b2ad5d62":"code","fb4997c6":"code","9a8dcd1d":"code","84550b7f":"code","a3bd28c2":"markdown","c5ea4293":"markdown","ae5f6eb4":"markdown","54be425a":"markdown","7aca9791":"markdown","c387fcd4":"markdown","b0949509":"markdown","ab6ac202":"markdown","7e17b86e":"markdown"},"source":{"8142b66a":"from functools import partial\nfrom collections import defaultdict\nimport pydicom\nimport os\nimport glob\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as patches\nimport seaborn as sns\nsns.set_style('whitegrid')\n%matplotlib inline\n\nnp.warnings.filterwarnings('ignore')","84f58875":"labels = pd.read_csv('..\/input\/rsna-pneumonia-detection-challenge\/stage_1_train_labels.csv')\ndetails = pd.read_csv('..\/input\/rsna-pneumonia-detection-challenge\/stage_1_detailed_class_info.csv')\n# duplicates in details just have the same class so can be safely dropped\ndetails = details.drop_duplicates('patientId').reset_index(drop=True)\nlabels_w_class = labels.merge(details, how='inner', on='patientId')","b997373a":"# get lists of all train\/test dicom filepaths\ntrain_dcm_fps = glob.glob('..\/input\/rsna-pneumonia-detection-challenge\/stage_1_train_images\/*.dcm')\ntest_dcm_fps = glob.glob('..\/input\/rsna-pneumonia-detection-challenge\/stage_1_test_images\/*.dcm')\n\ntrain_dcms = [pydicom.read_file(x, stop_before_pixels=True) for x in train_dcm_fps]\ntest_dcms = [pydicom.read_file(x, stop_before_pixels=True) for x in test_dcm_fps]","f18e1306":"def parse_dcm_metadata(dcm):\n    unpacked_data = {}\n    group_elem_to_keywords = {}\n    # iterating here to force conversion from lazy RawDataElement to DataElement\n    for d in dcm:\n        pass\n    # keys are pydicom.tag.BaseTag, values are pydicom.dataelem.DataElement\n    for tag, elem in dcm.items():\n        tag_group = tag.group\n        tag_elem = tag.elem\n        keyword = elem.keyword\n        group_elem_to_keywords[(tag_group, tag_elem)] = keyword\n        value = elem.value\n        unpacked_data[keyword] = value\n    return unpacked_data, group_elem_to_keywords\n\ntrain_meta_dicts, tag_to_keyword_train = zip(*[parse_dcm_metadata(x) for x in train_dcms])\ntest_meta_dicts, tag_to_keyword_test = zip(*[parse_dcm_metadata(x) for x in test_dcms])","cb45d2e0":"# join all the dicts\nunified_tag_to_key_train = {k:v for dict_ in tag_to_keyword_train for k,v in dict_.items()}\nunified_tag_to_key_test = {k:v for dict_ in tag_to_keyword_test for k,v in dict_.items()}\n\n# quick check to make sure there are no different keys between test\/train\nassert len(set(unified_tag_to_key_test.keys()).symmetric_difference(set(unified_tag_to_key_train.keys()))) == 0\n\ntag_to_key = {**unified_tag_to_key_test, **unified_tag_to_key_train}\ntag_to_key","bbb7eecb":"# using from_records here since some values in the dicts will be iterables and some are constants\ntrain_df = pd.DataFrame.from_records(data=train_meta_dicts)\ntest_df = pd.DataFrame.from_records(data=test_meta_dicts)\ntrain_df['dataset'] = 'train'\ntest_df['dataset'] = 'test'\ndf = pd.concat([train_df, test_df])","a56e9ed4":"df.head(1)","783edfb0":"# separating PixelSpacing list to single values\ndf['PixelSpacing_x'] = df['PixelSpacing'].apply(lambda x: x[0])\ndf['PixelSpacing_y'] = df['PixelSpacing'].apply(lambda x: x[1])\ndf = df.drop(['PixelSpacing'], axis='columns')\n\n# x and y are always the same\nassert sum(df['PixelSpacing_x'] != df['PixelSpacing_y']) == 0","bf1df47b":"# ReferringPhysicianName appears to just be empty strings\nassert sum(df['ReferringPhysicianName'] != '') == 0\n\n# SeriesDescription appears to be 'view: {}'.format(ViewPosition)\nset(df['SeriesDescription'].unique())\n\n# so these two columns don't have any useful info and can be safely dropped","db524505":"nunique_all = df.aggregate('nunique')\nnunique_all","1d4726b2":"# drop constant cols and other two from above\n#ReferringPhysicianName is all ''\n#PatientName is the same as PatientID\n#PixelSpacing_y is the same as PixelSpacing_x\n#The series and SOP UID's are just random numbers \/ id's, so I'm deleting them too\ndf = df.drop(nunique_all[nunique_all == 1].index.tolist() + ['SeriesDescription', 'ReferringPhysicianName', 'PatientName', 'PixelSpacing_y', 'SOPInstanceUID','SeriesInstanceUID','StudyInstanceUID'], axis='columns')\n\n# now that we have a clean metadata dataframe we can merge back to our initial tabular data with target and class info\ndf = df.merge(labels_w_class, how='left', left_on='PatientID', right_on='patientId')\n\ndf['PatientAge'] = df['PatientAge'].astype(int)","c5fffe05":"# df now has multiple rows for some patients (those with multiple bounding boxes in label_w_class)\n# so creating one with no duplicates for patients\ndf_deduped = df.drop_duplicates('PatientID', keep='first')","852b43ab":"df_deduped.head()","e360b835":"#Correct ages that are mistyped\ndf_deduped.loc[df_deduped['PatientAge'] > 140, 'PatientAge'] = df_deduped.loc[df_deduped['PatientAge'] > 140, 'PatientAge'] - 100","fbd65da2":"#Convert binary features from categorical to 0\/1\n# Categorical features with Binary encode (0 or 1; two categories)\nfor bin_feature in ['PatientSex', 'ViewPosition']:\n    df_deduped[bin_feature], uniques = pd.factorize(df_deduped[bin_feature])","dd31b59d":"#Drop the duplicated column patientID\ndel df_deduped['patientId']\n\n#Drop columns that are going to be repetitive\ndel df_deduped['dataset']","a0b6f19b":"df_deduped.head()","5d270928":"jonneoofs = pd.read_csv(\"..\/input\/jonneoofs\/jonne_oofs.csv\")\njonneoofs = jonneoofs.sort_values('patientID').reset_index(drop=True)\nandyharless_sub = pd.read_csv(\"..\/input\/andyharless\/submission (7).csv\")","2938bf64":"labels.head() #The real train","17053ac0":"jonneoofs.head() #The oofs from Jonne's kernel","980e84c2":"andyharless_sub.head() # The submission from Andy Harless, which is a fork from Jonne","4847098e":"jonneoofs['i_am_train'] = 1\nandyharless_sub['i_am_train'] = 0\ntr_te = jonneoofs.append(andyharless_sub)","52341299":"del tr_te['confidence'] #Not used in grading","fd6e6bc9":"tr_te.columns = ['PatientID','x_guess','y_guess','width_guess','height_guess','i_am_train']\ntr_te.head()","8760d2a0":"df_deduped.head()","cbb47ccb":"merged_df = tr_te.merge(df_deduped, how='left', on='PatientID')\nmerged_df.head()","5b8f747b":"filledmerged_df = merged_df.fillna(-1) #Fill in missings","4ce62c03":"import lightgbm as lgb\nfrom sklearn.model_selection import KFold, StratifiedKFold\n\ntrain_df = filledmerged_df[filledmerged_df['i_am_train']==1]\ntest_df = filledmerged_df[filledmerged_df['i_am_train']==0]\n             \n#Cross validate with K Fold, 5 splits\nfolds = KFold(n_splits= 5, shuffle=True, random_state=2222)\n\n# Create arrays and dataframes to store results\noof_preds = np.zeros(train_df.shape[0])\nsub_preds = np.zeros(test_df.shape[0])\n             \nfeats = [f for f in train_df.columns if f not in ['PatientID', 'i_am_train', 'x','y','width','height','Target','class']]\n\nfor n_fold, (train_idx, valid_idx) in enumerate(folds.split(train_df[feats])):\n    dtrain = lgb.Dataset(data=train_df[feats].iloc[train_idx], \n                         label=train_df['x'].iloc[train_idx], \n                         free_raw_data=False, silent=True)\n    dvalid = lgb.Dataset(data=train_df[feats].iloc[valid_idx], \n                         label=train_df['x'].iloc[valid_idx], \n                         free_raw_data=False, silent=True)\n\n    params = {\n        'objective': 'regression',\n        'boosting_type': 'gbdt',\n        'nthread': 4,\n        'learning_rate': 0.10, \n        'max_depth': 2,\n        #'reg_alpha': 0,\n        #'reg_lambda': 0,\n        #'min_split_gain': 0.0222415,\n        'seed': 15000,\n        'verbose': 50,\n        'metric': 'l2',\n    }\n\n    clf = lgb.train(\n        params=params,\n        train_set=dtrain,\n        num_boost_round=1000,\n        valid_sets=[dtrain, dvalid],\n        early_stopping_rounds=50,\n        verbose_eval=True\n    )\n\n    oof_preds[valid_idx] = clf.predict(dvalid.data)\n    sub_preds += clf.predict(test_df[feats]) \/ folds.n_splits\n","4b5124d3":"xpreds_oof = oof_preds.copy()\nxpreds_sub = sub_preds.copy()","f48f5eb4":"import lightgbm as lgb\nfrom sklearn.model_selection import KFold, StratifiedKFold\n\ntrain_df = filledmerged_df[filledmerged_df['i_am_train']==1]\ntest_df = filledmerged_df[filledmerged_df['i_am_train']==0]\n             \n#Cross validate with K Fold, 5 splits\nfolds = KFold(n_splits= 5, shuffle=True, random_state=2222)\n\n# Create arrays and dataframes to store results\noof_preds = np.zeros(train_df.shape[0])\nsub_preds = np.zeros(test_df.shape[0])\n             \nfeats = [f for f in train_df.columns if f not in ['PatientID', 'i_am_train', 'x','y','width','height','Target','class']]\n\nfor n_fold, (train_idx, valid_idx) in enumerate(folds.split(train_df[feats])):\n    dtrain = lgb.Dataset(data=train_df[feats].iloc[train_idx], \n                         label=train_df['y'].iloc[train_idx], \n                         free_raw_data=False, silent=True)\n    dvalid = lgb.Dataset(data=train_df[feats].iloc[valid_idx], \n                         label=train_df['y'].iloc[valid_idx], \n                         free_raw_data=False, silent=True)\n\n    params = {\n        'objective': 'regression',\n        'boosting_type': 'gbdt',\n        'nthread': 4,\n        'learning_rate': 0.10, \n        'max_depth': 2,\n        #'reg_alpha': 0,\n        #'reg_lambda': 0,\n        #'min_split_gain': 0.0222415,\n        'seed': 15000,\n        'verbose': 50,\n        'metric': 'l2',\n    }\n\n    clf = lgb.train(\n        params=params,\n        train_set=dtrain,\n        num_boost_round=1000,\n        valid_sets=[dtrain, dvalid],\n        early_stopping_rounds=50,\n        verbose_eval=True\n    )\n\n    oof_preds[valid_idx] = clf.predict(dvalid.data)\n    sub_preds += clf.predict(test_df[feats]) \/ folds.n_splits\n","4c53c122":"ypreds_oof = oof_preds.copy()\nypreds_sub = sub_preds.copy()","8955a507":"import lightgbm as lgb\nfrom sklearn.model_selection import KFold, StratifiedKFold\n\ntrain_df = filledmerged_df[filledmerged_df['i_am_train']==1]\ntest_df = filledmerged_df[filledmerged_df['i_am_train']==0]\n             \n#Cross validate with K Fold, 5 splits\nfolds = KFold(n_splits= 5, shuffle=True, random_state=2222)\n\n# Create arrays and dataframes to store results\noof_preds = np.zeros(train_df.shape[0])\nsub_preds = np.zeros(test_df.shape[0])\n             \nfeats = [f for f in train_df.columns if f not in ['PatientID', 'i_am_train', 'x','y','width','height','Target','class']]\n\nfor n_fold, (train_idx, valid_idx) in enumerate(folds.split(train_df[feats])):\n    dtrain = lgb.Dataset(data=train_df[feats].iloc[train_idx], \n                         label=train_df['width'].iloc[train_idx], \n                         free_raw_data=False, silent=True)\n    dvalid = lgb.Dataset(data=train_df[feats].iloc[valid_idx], \n                         label=train_df['width'].iloc[valid_idx], \n                         free_raw_data=False, silent=True)\n\n    params = {\n        'objective': 'regression',\n        'boosting_type': 'gbdt',\n        'nthread': 4,\n        'learning_rate': 0.10, \n        'max_depth': 2,\n        #'reg_alpha': 0,\n        #'reg_lambda': 0,\n        #'min_split_gain': 0.0222415,\n        'seed': 15000,\n        'verbose': 50,\n        'metric': 'l2',\n    }\n\n    clf = lgb.train(\n        params=params,\n        train_set=dtrain,\n        num_boost_round=1000,\n        valid_sets=[dtrain, dvalid],\n        early_stopping_rounds=50,\n        verbose_eval=True\n    )\n\n    oof_preds[valid_idx] = clf.predict(dvalid.data)\n    sub_preds += clf.predict(test_df[feats]) \/ folds.n_splits\n","1557fbaa":"widthpreds_oof = oof_preds.copy()\nwidthpreds_sub = sub_preds.copy()","e5168fcc":"import lightgbm as lgb\nfrom sklearn.model_selection import KFold, StratifiedKFold\n\ntrain_df = filledmerged_df[filledmerged_df['i_am_train']==1]\ntest_df = filledmerged_df[filledmerged_df['i_am_train']==0]\n             \n#Cross validate with K Fold, 5 splits\nfolds = KFold(n_splits= 5, shuffle=True, random_state=2222)\n\n# Create arrays and dataframes to store results\noof_preds = np.zeros(train_df.shape[0])\nsub_preds = np.zeros(test_df.shape[0])\n             \nfeats = [f for f in train_df.columns if f not in ['PatientID', 'i_am_train', 'x','y','width','height','Target','class']]\n\nfor n_fold, (train_idx, valid_idx) in enumerate(folds.split(train_df[feats])):\n    dtrain = lgb.Dataset(data=train_df[feats].iloc[train_idx], \n                         label=train_df['height'].iloc[train_idx], \n                         free_raw_data=False, silent=True)\n    dvalid = lgb.Dataset(data=train_df[feats].iloc[valid_idx], \n                         label=train_df['height'].iloc[valid_idx], \n                         free_raw_data=False, silent=True)\n\n    params = {\n        'objective': 'regression',\n        'boosting_type': 'gbdt',\n        'nthread': 4,\n        'learning_rate': 0.10, \n        'max_depth': 2,\n        #'reg_alpha': 0,\n        #'reg_lambda': 0,\n        #'min_split_gain': 0.0222415,\n        'seed': 15000,\n        'verbose': 50,\n        'metric': 'l2',\n    }\n\n    clf = lgb.train(\n        params=params,\n        train_set=dtrain,\n        num_boost_round=1000,\n        valid_sets=[dtrain, dvalid],\n        early_stopping_rounds=50,\n        verbose_eval=True\n    )\n\n    oof_preds[valid_idx] = clf.predict(dvalid.data)\n    sub_preds += clf.predict(test_df[feats]) \/ folds.n_splits\n","adaca9fd":"heightpreds_oof = oof_preds.copy()\nheightpreds_sub = sub_preds.copy()","ed1c557e":"# What is the number of rows where we have a box?\ntrain_df.loc[train_df['x'] > -1]['x'].shape[0] \/ train_df.shape[0]","59c07cc8":"train_df['xpredsoof'] = xpreds_oof\ntrain_df['ypredsoof'] = ypreds_oof\ntrain_df['widthpredsoof'] = widthpreds_oof\ntrain_df['heightpredsoof'] = heightpreds_oof","a7f2de99":"train_df.loc[train_df['widthpredsoof'] <= 100]","42ce39db":"#train_df.loc[(train_df['xpredsoof'] > 130) & (train_df['ypredsoof'] > 134)].shape[0] \/ train_df.shape[0]\ntrain_df.loc[(train_df['widthpredsoof'] > 100)].shape[0] \/ train_df.shape[0]","f129b8f9":"andyharless_sub['xpred'] = xpreds_sub\nandyharless_sub['ypred'] = ypreds_sub\nandyharless_sub['widthpred'] = widthpreds_sub\nandyharless_sub['heightpred'] = heightpreds_sub\n\nandyharless_sub['xpred'] = andyharless_sub['xpred'].round()\nandyharless_sub['ypred'] = andyharless_sub['ypred'].round()\nandyharless_sub['widthpred'] = andyharless_sub['widthpred'].round()\nandyharless_sub['heightpred'] = andyharless_sub['heightpred'].round()","233dedd6":"#andyharless_sub.loc[andyharless_sub['widthpred'] <= 100, 'xpred'] = ''\n#andyharless_sub.loc[andyharless_sub['widthpred'] <= 100, 'ypred'] = ''\n#andyharless_sub.loc[andyharless_sub['widthpred'] <= 100, 'heightpred'] = ''\n#andyharless_sub.loc[andyharless_sub['widthpred'] <= 100, 'widthpred'] = ''\nandyharless_sub['confidence'] = '1'","ec65bfed":"andyharless_sub.head()","bfbebd17":"#del andyharless_sub['x']\n#del andyharless_sub['y']\n#del andyharless_sub['width']\n#del andyharless_sub['height']\n#del andyharless_sub['i_am_train']","b2ad5d62":"andyharless_sub['PredictionString'] = andyharless_sub['confidence'].map(str)+' '+andyharless_sub['xpred'].map(str)+' '+andyharless_sub['ypred'].map(str)+' '+andyharless_sub['widthpred'].map(str)+' '+andyharless_sub['heightpred'].map(str)","fb4997c6":"andyharless_sub.loc[andyharless_sub['PredictionString']=='1    ', 'PredictionString'] = '' #Correct empties","9a8dcd1d":"andyharless_sub.loc[andyharless_sub['x'].isnull(), 'PredictionString'] = '' #Remove boxes if we predicted there were none","84550b7f":"andyharless_sub[['patientID','PredictionString']].to_csv('dicom_corrections.csv', index=False)","a3bd28c2":"# Predict for y","c5ea4293":"0.22 rows have a box, so now let's cull our predictions until only there is 0.22","ae5f6eb4":"# Remove any boxes below a threshold","54be425a":"Now that we have a data frame that links PatientID to DICOM data, let's merge this with train and the submission file.","7aca9791":"This kernel is a fork from Jonne's kernel (https:\/\/www.kaggle.com\/jonnedtc\/cnn-segmentation-connected-components). Jonne creates a submission using a convolutional neural network. However, Jonne does not use any DICOM data for the prediction. I am creating this kernel to improve on Jonne's predictions by using the DICOM data. The model I am using is LightGBM, since it is fast, often accurate, and reliable.","c387fcd4":"# Predict for height","b0949509":"# Predict for x","ab6ac202":"This kernel is also a fork from jtlowery's kernel (https:\/\/www.kaggle.com\/jtlowery\/intro-eda-with-dicom-metadata). Jtlowery's kernel has functions I can copy to read in the DICOM data.[](http:\/\/)","7e17b86e":"# Predict for width"}}