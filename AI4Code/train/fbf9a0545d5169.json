{"cell_type":{"7af6a220":"code","ab5f6e4a":"code","6f22e19a":"code","d9895283":"code","199eb935":"code","bb4249e3":"code","d0cab5ca":"code","43a60f33":"code","b54b99e1":"code","0a1ed144":"code","ec10c2dd":"code","1ead3093":"code","f67d8a76":"code","58d969c0":"markdown","3568ede8":"markdown","3952216b":"markdown","d6a0a07f":"markdown","0d08fc8e":"markdown"},"source":{"7af6a220":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\ngg=pd.read_csv(\"..\/input\/google-quest-challenge\/train.csv\")\nts=pd.read_csv(\"..\/input\/google-quest-challenge\/test.csv\")\nss=pd.read_csv(\"..\/input\/google-quest-challenge\/sample_submission.csv\")\nss\nss.to_csv(\"submission.csv\",index=False)\n","ab5f6e4a":"ac=gg[\"answer_satisfaction\"].value_counts()\nac\nfig,ax=plt.subplots(figsize=(20,20))\nax.set_xticklabels(ac.index, size=15,rotation=90)\nplt.scatter(ac.index,ac.values)\nplt.plot(ac.index,ac.values)\nplt.title(\"AnswerSatisfaction\",size=50)\nplt.show()\n","6f22e19a":"ac1=gg[\"question_body_critical\"].value_counts()\nac1\nfrom matplotlib import style\nfig,ax=plt.subplots(figsize=(20,20))\nax.set_xticklabels(ac1.index, size=15,rotation=90)\n\n\nstyle.use('ggplot') \n\nplt.plot(ac1.index,ac1.values,linewidth=10)\nplt.title(\"question_body_critical\",size=50)\nplt.show()","d9895283":"ct=ts[\"category\"].value_counts()\nct\nfig,ax=plt.subplots(figsize=(10,10))\nplt.bar(ct.index,ct.values)\nplt.show()","199eb935":"ct1=ts[\"host\"].value_counts()\nct1\nfig,ax=plt.subplots(figsize=(20,20))\nax.set_xticklabels(ct1.index, size=15,rotation=90)\nplt.scatter(ct1.index,ct1.values)\nplt.plot(ct1.index,ct1.values)\nplt.show()","bb4249e3":"xtrain=gg.iloc[:,[1,2]]\nytrain=gg.iloc[:,11]\nxtest=ts.iloc[:,[1,2]]\nfrom sklearn.preprocessing import LabelEncoder\nle=LabelEncoder()\nytrain=le.fit_transform(ytrain)\nytrain","d0cab5ca":"from sklearn.feature_extraction.text import CountVectorizer\nvocabulary_count=CountVectorizer()\n#print(len(vocabulary_count.vocabulary_))   \na= vocabulary_count.fit_transform(xtrain.iloc[:,0])\n\n\nat=vocabulary_count.transform(xtest.iloc[:,0])\n\nb= vocabulary_count.fit_transform(xtrain.iloc[:,1])\n\nbt=vocabulary_count.transform(xtest.iloc[:,1])\n","43a60f33":"a=a.toarray()\nat=at.toarray()\nb=b.toarray()\nbt=bt.toarray()\n","b54b99e1":"a.shape\nb.shape\nat.shape\nbt.shape\n","0a1ed144":"Xtr=np.hstack([a,b])\nXte=np.hstack([at,bt])\n","ec10c2dd":"xtrain.shape","1ead3093":"\nfrom sklearn.naive_bayes import MultinomialNB\nmodel = MultinomialNB()\nmodel.fit(Xtr,ytrain)\n\n\n#print('predicted:',model.predict(titletest))\nprediction=model.predict(Xte)\nprediction\n","f67d8a76":"plt.hist(prediction)\nplt.show()","58d969c0":"\nc= vocabulary_count.fit_transform(xtrain.iloc[:,2])\n\n\nct=vocabulary_count.transform(xtest.iloc[:,2])\n\nd = vocabulary_count.fit_transform(xtrain.iloc[:,3])\n\n\ndt=vocabulary_count.transform(xtest.iloc[:,3])\n\n","3568ede8":"\nfrom nltk.corpus import stopwords\nstop_words=stopwords.words('english')\n#print(stop_words)\n\nimport string\n\ndef text_cleaning(a):\n    remove_punctuation = [char for char in a if char not in string.punctuation]\n \n    remove_punctuation=''.join(remove_punctuation)\n \n    return [\"\".join(word) for word in remove_punctuation.split() if word.lower() not in stopwords.words('english')]","3952216b":"xtr=np.hstack([c,d])\nxte=np.hstack([ct,dt])\n\nxtrain=np.hstack([Xtr,xtr])\nxtest=np.hstack([Xte,xte])\n","d6a0a07f":"z=[]\nfor j in xtrain:\n    #print(text_cleaning(j))\n    text=\" \".join(text_cleaning(j))\n    z.append(text)\nz","0d08fc8e":"c=c.toarray()\nct=ct.toarray()\nd=d.toarray()\ndt=dt.toarray()\n"}}