{"cell_type":{"77dc1f8a":"code","082ec5f8":"code","fbe5a0cf":"code","0597f154":"code","47cf1bf5":"code","d5985b67":"code","ddc559ed":"code","53426e75":"code","ffd640e1":"code","fa92c849":"code","2f5a4895":"code","a6560fca":"code","305ab7f4":"code","2b951a72":"code","f8f744cc":"code","63b700ed":"code","9928e937":"code","a7de7ea4":"code","1fb7e34f":"code","64b332cd":"code","67d5cb8e":"code","aa5214b8":"code","a54ca839":"code","955de718":"code","2cbef01f":"code","eafe3ca8":"code","5ad8cfb6":"code","bc42eac9":"code","3994076e":"code","5896fc0a":"code","c2418b92":"code","5d62a98e":"code","8ee45c2d":"code","638ae5ac":"code","13ca1e3b":"code","9095d43c":"code","f5ac166e":"code","77c2be88":"code","b9036bb2":"code","5ceaa5b5":"code","5571225a":"code","00e888b4":"code","152902f4":"code","da106e72":"code","a661d2f6":"markdown","8d98c6a2":"markdown","492c9cf6":"markdown","2ee5c1ef":"markdown","0b1d5ba6":"markdown","b6f61e90":"markdown","a5564d6b":"markdown","d984542f":"markdown","34321539":"markdown","d0e659e6":"markdown","431a9fa9":"markdown","4dfc51d1":"markdown","60510e09":"markdown","fc23a5e5":"markdown","5debc24e":"markdown","3b2bb01b":"markdown","c41aa5ac":"markdown","8ad027f7":"markdown","e5cc9646":"markdown","ce3dcb50":"markdown","fa68d964":"markdown","58db95a1":"markdown","9b3f1296":"markdown","456881e1":"markdown","94757fab":"markdown","bbd13525":"markdown","7e300948":"markdown","e879f871":"markdown","eefa2fe8":"markdown","69d7f538":"markdown","2987833a":"markdown","28418c6d":"markdown","003f2606":"markdown","5328e9aa":"markdown","6fcde3b8":"markdown","93a2b739":"markdown","2d3878a2":"markdown","4bd8d52e":"markdown","8cb931ec":"markdown","9ff93b10":"markdown","014fe343":"markdown","09ca8f01":"markdown","7489f082":"markdown","6ad1188e":"markdown","95f90f28":"markdown","d6f4877c":"markdown","725ce050":"markdown","6beacec7":"markdown"},"source":{"77dc1f8a":"# Import the data processing and visualization libraries\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n%matplotlib inline","082ec5f8":"# Read the dataset in pandas\ndf_liver = pd.read_csv('\/kaggle\/input\/indian-liver-patient-records\/indian_liver_patient.csv')","fbe5a0cf":"# Access the first 5 rows of df_liver\ndf_liver.head()","0597f154":"# Access the last 5 rows of df_liver\ndf_liver.tail()","47cf1bf5":"# Retrieve the colunmn information\ndf_liver.columns.values","d5985b67":"# Retrieve the full information of df_liver regarding the features and response, in order to verify \n# if the values are unique or are there any missing data.\ndf_liver.info()","ddc559ed":"# Find the shape of the datframe df_liver\ndf_liver.shape","53426e75":"# We can performing some simple statistical inferences to get a good feel of the data\ndf_liver.describe()","ffd640e1":"# I can quickly perform some additional statistics to include all\ndf_liver.describe(include ='all')","fa92c849":"# Define a function that allows us to create a table of missing values in df_liver and their percentages in \n# descending order\ndef missing_values(data):\n    total = data.isnull().sum().sort_values(ascending=False)\n    percentage = (data.isnull().sum()\/data.isnull().count()).sort_values(ascending=False)\n    percentage_final = (round(percentage, 2) * 100)\n    total_percent = pd.concat(objs=[total, percentage_final], axis = 1, keys=['Total', '%'])\n    return total_percent","2f5a4895":"# Find the total count and % of missing values \nmissing_values(df_liver)","a6560fca":"# Replace missing values with the mean of feature column Albumin_and_Globulin_Ratio, \n# then check to see that it has been successfull, where the sum of missig values should be 0\ndf_liver['Albumin_and_Globulin_Ratio'].fillna(df_liver['Albumin_and_Globulin_Ratio'].mean(), inplace = True)\ndf_liver['Albumin_and_Globulin_Ratio'].isnull().sum()","305ab7f4":"# Repeat to see what is the % of missing values\nmissing_values(df_liver)","2b951a72":"# Correlation pairplot\nsns.set()\nsns.pairplot(df_liver, hue='Dataset', kind='reg')","f8f744cc":"# A more robust way of figuring out correlations other than observations as above is to generate a full correlation\n# table with the ranging from -1 to 1\ndf_liver.corr().style.background_gradient(cmap='coolwarm')","63b700ed":"# Change the current categorical feature Gender to a numerical feature of 0 or 1 (as ML algorithms prefer numerical \n# features)\ndf_liver['Gender'] = df_liver['Gender'].map({'Male': 1, 'Female': 0})\n# Alternatively, you can use the apply and lambda function\n# df_liver['Gender'] = df_liver['Gender'].apply(lambda x:1 if x == 'Male' else 0)\n\n# Check to make sure that the gender has been correctly converted\ndf_liver.head()","9928e937":"# Create a table for Dataset (with and without liver disease) and gender\ndf_liver_Gender = round(df_liver[['Gender', 'Dataset']].groupby(['Gender'], as_index=False).agg(np.sum), 3)\n\n# Generate plot to determine the effect of gender on the dataset (target feature)\n# Figure configuration\nplt.figure(figsize=(10,5))\n\nsns.barplot(x=\"Gender\", y=\"Dataset\", data=df_liver_Gender, ci=None)\nplt.title(\"Survival wrt Gender\")\nplt.ylim(0, 700)","a7de7ea4":"# Create a table for Dataset (with and without liver disease) and Albumin\ndf_liver_Albumin = round(df_liver[['Gender', 'Albumin', 'Dataset']]\n                         .groupby(['Albumin', 'Gender'], as_index=False).agg(np.sum), 1)\n\n# Generate plot to determine the effect of gender on the dataset (target feature) and Albumin\n# Figure configuration\nplt.figure(figsize=(18,10))\n\nsns.barplot(x=\"Albumin\", y=\"Dataset\", hue='Gender', data=df_liver_Albumin, ci=None)\nplt.title(\"Survival wrt Albumin conc.\")","1fb7e34f":"# Create a table for Dataset (with and without liver disease) and Total Proteins\ndf_liver_TP = round(df_liver[['Gender', 'Total_Protiens', 'Dataset']]\n                    .groupby(['Total_Protiens', 'Gender'], as_index=False).agg(np.sum), 2)\n\n# Generate plots to determine the effect of gender on the dataset (target feature) and total proteins\n# Figure configuration\nplt.figure(figsize=(20,15))\n\nsns.barplot(x=\"Total_Protiens\", y=\"Dataset\", hue='Gender', data=df_liver_TP, ci=None)\nplt.title(\"Survival wrt Total Proteins conc.\")","64b332cd":"# Create a table for Dataset (with and without liver disease) and Alkaline Phosphatase\ndf_liver_ALP = round(df_liver[['Gender', 'Albumin_and_Globulin_Ratio', 'Dataset']]\n                     .groupby(['Albumin_and_Globulin_Ratio', 'Gender'], as_index=False).agg(np.sum), 1)\n\n# Generate plots to determine the effect of gender on the dataset (target feature) and AGR\n# Figure configuration\nplt.figure(figsize=(18,10))\n\nsns.barplot(x=\"Albumin_and_Globulin_Ratio\", y=\"Dataset\", hue='Gender', data=df_liver_ALP, ci=None)\nplt.title(\"Survival wrt Albumin Globulin Ratio\")","67d5cb8e":"# Figure configuration\nfig, axes = plt.subplots(1, 2, figsize=(15, 5))\n\n# Visualize the effect of the dataset(with or without disease) based on both Age and Gender.\nld = 'Liver Disease'\nno_ld = 'No Liver Disease'\nax = sns.distplot(df_liver[df_liver['Gender'] == 0][df_liver[df_liver['Gender'] == 0]['Dataset'] == 1]['Age'], \n                  bins=18, label=ld, ax=axes[0], kde=False, color='blue')\nax = sns.distplot(df_liver[df_liver['Gender'] == 0][df_liver[df_liver['Gender'] == 0]['Dataset'] == 2]['Age'], \n                  bins=40, label=no_ld, ax=axes[0], kde=False, color='red')\nax.legend()\nax.set_title('Female')\nax.set_ylabel('Counts')\nax = sns.distplot(df_liver[df_liver['Gender'] == 1][df_liver[df_liver['Gender'] == 1]['Dataset'] == 1]['Age'], \n                  bins=18, label=ld, ax=axes[1], kde=False, color='blue')\nax = sns.distplot(df_liver[df_liver['Gender'] == 1][df_liver[df_liver['Gender'] == 1]['Dataset'] == 2]['Age'], \n                  bins=40, label=no_ld, ax=axes[1], kde=False, color='red')\nax.legend()\nax.set_title('Male')","aa5214b8":"# Figure configuration\nfig, axes = plt.subplots(1, 2, figsize=(15, 5))\n\n# Visualize the effect of the dataset(with or without disease) based on both Total Bilirubin  and Gender.\nld = 'Liver Disease'\nno_ld = 'No Liver Disease'\nax = sns.distplot(df_liver[df_liver['Gender'] == 0][df_liver[df_liver['Gender'] == 0]['Dataset'] == 1]['Total_Bilirubin'], \n                  bins=18, label=ld, ax=axes[0], kde=False, color='blue')\nax = sns.distplot(df_liver[df_liver['Gender'] == 0][df_liver[df_liver['Gender'] == 0]['Dataset'] == 2]['Total_Bilirubin'], \n                  bins=20, label=no_ld, ax=axes[0], kde=False, color='red')\nax.legend()\nax.set_title('Female')\nax.set_ylabel('Counts')\nax = sns.distplot(df_liver[df_liver['Gender'] == 1][df_liver[df_liver['Gender'] == 1]['Dataset'] == 1]['Total_Bilirubin'], \n                  bins=18, label=ld, ax=axes[1], kde=False, color='blue')\nax = sns.distplot(df_liver[df_liver['Gender'] == 1][df_liver[df_liver['Gender'] == 1]['Dataset'] == 2]['Total_Bilirubin'], \n                  bins=20, label=no_ld, ax=axes[1], kde=False, color='red')\nax.legend()\nax.set_title('Male') ","a54ca839":"# Figure configuration\nfig, axes = plt.subplots(1, 2, figsize=(15, 5))\n\n# Visualize the effect of the dataset(with or without disease) based on both Direct Bilirubin  and Gender.\nld = 'Liver Disease'\nno_ld = 'No Liver Disease'\nax = sns.distplot(df_liver[df_liver['Gender'] == 0][df_liver[df_liver['Gender'] == 0]['Dataset'] == 1]['Direct_Bilirubin'], \n                  bins=18, label=ld, ax=axes[0], kde=False, color='blue')\nax = sns.distplot(df_liver[df_liver['Gender'] == 0][df_liver[df_liver['Gender'] == 0]['Dataset'] == 2]['Direct_Bilirubin'], \n                  bins=20, label=no_ld, ax=axes[0], kde=False, color='red')\nax.legend()\nax.set_title('Female')\nax.set_ylabel('Counts')\nax = sns.distplot(df_liver[df_liver['Gender'] == 1][df_liver[df_liver['Gender'] == 1]['Dataset'] == 1]['Direct_Bilirubin'], \n                  bins=18, label=ld, ax=axes[1], kde=False, color='blue')\nax = sns.distplot(df_liver[df_liver['Gender'] == 1][df_liver[df_liver['Gender'] == 1]['Dataset'] == 2]['Direct_Bilirubin'], \n                  bins=20, label=no_ld, ax=axes[1], kde=False, color='red')\nax.legend()\nax.set_title('Male')","955de718":"# Figure configuration\nfig, axes = plt.subplots(1, 2, figsize=(15, 5))\n\n# Visualize the effect of the dataset(with or without disease) based on both ALP  and Gender.\nld = 'Liver Disease'\nno_ld = 'No Liver Disease'\nax = sns.distplot(df_liver[df_liver['Gender'] == 0][df_liver[df_liver['Gender'] == 0]['Dataset'] == 1]['Alkaline_Phosphotase'], \n                  bins=18, label=ld, ax=axes[0], kde=False, color='blue')\nax = sns.distplot(df_liver[df_liver['Gender'] == 0][df_liver[df_liver['Gender'] == 0]['Dataset'] == 2]['Alkaline_Phosphotase'], \n                  bins=20, label=no_ld, ax=axes[0], kde=False, color='red')\nax.legend()\nax.set_title('Female')\nax.set_ylabel('Counts')\nax = sns.distplot(df_liver[df_liver['Gender'] == 1][df_liver[df_liver['Gender'] == 1]['Dataset'] == 1]['Alkaline_Phosphotase'], \n                  bins=18, label=ld, ax=axes[1], kde=False, color='blue')\nax = sns.distplot(df_liver[df_liver['Gender'] == 1][df_liver[df_liver['Gender'] == 1]['Dataset'] == 2]['Alkaline_Phosphotase'], \n                  bins=20, label=no_ld, ax=axes[1], kde=False, color='red')\nax.legend()\nax.set_title('Male') ","2cbef01f":"# Figure configuration\nfig, axes = plt.subplots(1, 2, figsize=(15, 5))\n\n# Visualize the effect of the dataset(with or without disease) based on both AAT and Gender.\nld = 'Liver Disease'\nno_ld = 'No Liver Disease'\nax = sns.distplot(df_liver[df_liver['Gender'] == 0][df_liver[df_liver['Gender'] == 0]['Dataset'] == 1]['Alamine_Aminotransferase'], \n                  bins=18, label=ld, ax=axes[0], kde=False, color='blue')\nax = sns.distplot(df_liver[df_liver['Gender'] == 0][df_liver[df_liver['Gender'] == 0]['Dataset'] == 2]['Alamine_Aminotransferase'], \n                  bins=20, label=no_ld, ax=axes[0], kde=False, color='red')\nax.legend()\nax.set_title('Female')\nax.set_ylabel('Counts')\nax = sns.distplot(df_liver[df_liver['Gender'] == 1][df_liver[df_liver['Gender'] == 1]['Dataset'] == 1]['Alamine_Aminotransferase'], \n                  bins=18, label=ld, ax=axes[1], kde=False, color='blue')\nax = sns.distplot(df_liver[df_liver['Gender'] == 1][df_liver[df_liver['Gender'] == 1]['Dataset'] == 2]['Alamine_Aminotransferase'], \n                  bins=20, label=no_ld, ax=axes[1], kde=False, color='red')\nax.legend()\nax.set_title('Male') ","eafe3ca8":"# Figure configuration\nfig, axes = plt.subplots(1, 2, figsize=(15, 5))\n\n# Visualize the effect of the dataset(with or without disease) based on both AAT  and Gender.\nld = 'Liver Disease'\nno_ld = 'No Liver Disease'\nax = sns.distplot(df_liver[df_liver['Gender'] == 0][df_liver[df_liver['Gender'] == 0]['Dataset'] == 1]['Aspartate_Aminotransferase'], \n                  bins=18, label=ld, ax=axes[0], kde=False, color='blue')\nax = sns.distplot(df_liver[df_liver['Gender'] == 0][df_liver[df_liver['Gender'] == 0]['Dataset'] == 2]['Aspartate_Aminotransferase'], \n                  bins=20, label=no_ld, ax=axes[0], kde=False, color='red')\nax.legend()\nax.set_title('Female')\nax.set_ylabel('Counts')\nax = sns.distplot(df_liver[df_liver['Gender'] == 1][df_liver[df_liver['Gender'] == 1]['Dataset'] == 1]['Aspartate_Aminotransferase'], \n                  bins=18, label=ld, ax=axes[1], kde=False, color='blue')\nax = sns.distplot(df_liver[df_liver['Gender'] == 1][df_liver[df_liver['Gender'] == 1]['Dataset'] == 2]['Aspartate_Aminotransferase'], \n                  bins=20, label=no_ld, ax=axes[1], kde=False, color='red')\nax.legend()\nax.set_title('Male') ","5ad8cfb6":"# Figure configuration\nfig, axes = plt.subplots(1, 2, figsize=(15, 5))\n\n# Visualize the effect of the dataset(with or without disease) based on both Total Proteins  and Gender.\nld = 'Liver Disease'\nno_ld = 'No Liver Disease'\nax = sns.distplot(df_liver[df_liver['Gender'] == 0][df_liver[df_liver['Gender'] == 0]['Dataset'] == 1]['Total_Protiens'], \n                  bins=18, label=ld, ax=axes[0], kde=False, color='blue')\nax = sns.distplot(df_liver[df_liver['Gender'] == 0][df_liver[df_liver['Gender'] == 0]['Dataset'] == 2]['Total_Protiens'], \n                  bins=20, label=no_ld, ax=axes[0], kde=False, color='red')\nax.legend()\nax.set_title('Female')\nax.set_ylabel('Counts')\nax = sns.distplot(df_liver[df_liver['Gender'] == 1][df_liver[df_liver['Gender'] == 1]['Dataset'] == 1]['Total_Protiens'], \n                  bins=18, label=ld, ax=axes[1], kde=False, color='blue')\nax = sns.distplot(df_liver[df_liver['Gender'] == 1][df_liver[df_liver['Gender'] == 1]['Dataset'] == 2]['Total_Protiens'], \n                  bins=20, label=no_ld, ax=axes[1], kde=False, color='red')\nax.legend()\nax.set_title('Male') ","bc42eac9":"# Figure configuration\nfig, axes = plt.subplots(1, 2, figsize=(15, 5))\n\n# Visualize the effect of the dataset(with or without disease) based on both Albumin  and Gender.\nld = 'Liver Disease'\nno_ld = 'No Liver Disease'\nax = sns.distplot(df_liver[df_liver['Gender'] == 0][df_liver[df_liver['Gender'] == 0]['Dataset'] == 1]['Albumin'], \n                  bins=18, label=ld, ax=axes[0], kde=False, color='blue')\nax = sns.distplot(df_liver[df_liver['Gender'] == 0][df_liver[df_liver['Gender'] == 0]['Dataset'] == 2]['Albumin'], \n                  bins=20, label=no_ld, ax=axes[0], kde=False, color='red')\nax.legend()\nax.set_title('Female')\nax.set_ylabel('Counts')\nax = sns.distplot(df_liver[df_liver['Gender'] == 1][df_liver[df_liver['Gender'] == 1]['Dataset'] == 1]['Albumin'], \n                  bins=18, label=ld, ax=axes[1], kde=False, color='blue')\nax = sns.distplot(df_liver[df_liver['Gender'] == 1][df_liver[df_liver['Gender'] == 1]['Dataset'] == 2]['Albumin'], \n                  bins=20, label=no_ld, ax=axes[1], kde=False, color='red')\nax.legend()\nax.set_title('Male') ","3994076e":"# Figure configuration\nfig, axes = plt.subplots(1, 2, figsize=(15, 5))\n\n# Visualize the effect of the dataset(with or without disease) based on both Albumin and Globulin Ratio  and Gender.\nld = 'Liver Disease'\nno_ld = 'No Liver Disease'\nax = sns.distplot(df_liver[df_liver['Gender'] == 0][df_liver[df_liver['Gender'] == 0]['Dataset'] == 1]['Albumin_and_Globulin_Ratio'], \n                  bins=18, label=ld, ax=axes[0], kde=False, color='blue')\nax = sns.distplot(df_liver[df_liver['Gender'] == 0][df_liver[df_liver['Gender'] == 0]['Dataset'] == 2]['Albumin_and_Globulin_Ratio'], \n                  bins=20, label=no_ld, ax=axes[0], kde=False, color='red')\nax.legend()\nax.set_title('Female')\nax.set_ylabel('Counts')\nax = sns.distplot(df_liver[df_liver['Gender'] == 1][df_liver[df_liver['Gender'] == 1]['Dataset'] == 1]['Albumin_and_Globulin_Ratio'], \n                  bins=18, label=ld, ax=axes[1], kde=False, color='blue')\nax = sns.distplot(df_liver[df_liver['Gender'] == 1][df_liver[df_liver['Gender'] == 1]['Dataset'] == 2]['Albumin_and_Globulin_Ratio'], \n                  bins=20, label=no_ld, ax=axes[1], kde=False, color='red')\nax.legend()\nax.set_title('Male')","5896fc0a":"# Create a new dataframe for the simple hypothesis testing \ndf_liver_hyp = df_liver\ndf_liver_hyp.head()","c2418b92":"# Create a 'Hypothesis' column and set that equal to 0\ndf_liver_hyp['Hypothesis'] = 0\n\n# Our hypothesis is that if the patients have liver disease then set the hypothesis column to 1\ndf_liver_hyp.loc[df_liver_hyp['Dataset'] == 1, 'Hypothesis'] = 1\n\n# Next, to check if our hypothesis is correct I will create another column called Result and set that equal to 0\ndf_liver_hyp['Result'] = 0\n\n# If the Dataset column agrees with our Hypothesis column, I am going to update the 'Result' column by 1.\ndf_liver_hyp.loc[df_liver_hyp['Dataset'] == df_liver_hyp['Hypothesis'], 'Result'] = 1\n\ndf_liver_hyp.head()","5d62a98e":"# I will now find the percentage of passengers that have liver disease\nround(df_liver_hyp['Result'].value_counts(normalize=True) * 100, 3)","8ee45c2d":"# Machine learning libraries in sklearn\n\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.metrics import accuracy_score\n\nfrom sklearn.model_selection import StratifiedKFold, train_test_split, cross_val_score\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.svm import SVC","638ae5ac":"# Cross validate model with Kfold stratified cross val\n# K-fold cross validation: randomly splits the training set into (n_splits) 10 distinct subsets called folds, \n# then it trains and evaluates the models 10 times, picking a different fold for evaluation every time and \n# training on the other 9 folds.\nK_fold = StratifiedKFold(n_splits=10)","13ca1e3b":"# Separate train features and response\nX = df_liver.drop([\"Dataset\", \"Hypothesis\", \"Result\"],axis = 1)\nY = df_liver[\"Dataset\"]\n\n# It turns out the I get the error message of reaching the total number of iterations reached the limit. \n# In this case I may need to scale the data\n\n# Scale the data\nscaler=MinMaxScaler()\nscaled_values=scaler.fit_transform(X)\nX.loc[:,:]=scaled_values\n\n# Create the Train and Test sets\n# Splitting the train and test into 70% training and 30% testing\nX_train, X_test, Y_train, Y_test=train_test_split(X,Y,stratify=Y, test_size=0.3,random_state=42)\n\n\n# Find the shape of all sets\nX_train.shape, Y_train.shape, X_test.shape, Y_test.shape","9095d43c":"# Logistic Regression\nmodel_logreg = LogisticRegression()\nmodel_logreg.fit(X_train,Y_train)\ny_pred = model_logreg.predict(X_test)\n\nscores = cross_val_score(model_logreg, X_train, Y_train, cv=K_fold, n_jobs=4, scoring='accuracy')\n\nprint(scores)\nscore_logreg = round(np.mean(scores) * 100, 3)\nprint(\"Score: {}\".format(score_logreg))\nacc_logreg = round(np.mean(accuracy_score(Y_test, y_pred)) * 100, 3)\nprint(\"Accuracy: {}\".format(acc_logreg))","f5ac166e":"# K-Neighbors Classifier\nmodel_knn = KNeighborsClassifier(n_neighbors=5)\nmodel_knn.fit(X_train, Y_train)\ny_pred = model_knn.predict(X_test)\n\nscores = cross_val_score(model_knn, X_train, Y_train, cv=K_fold, n_jobs=4, scoring='accuracy')\n\nprint(scores)\nscore_knn = round(np.mean(scores) * 100, 3)\nprint(\"Score: {}\".format(score_knn))\nacc_knn = round(np.mean(accuracy_score(Y_test, y_pred)) * 100, 3)\nprint(\"Accuracy: {}\".format(acc_knn))","77c2be88":"model_MLP = MLPClassifier()\nmodel_MLP.fit(X_train, Y_train)\ny_pred = model_MLP.predict(X_test)\n\nscores = cross_val_score(model_MLP, X_train, Y_train, cv=K_fold, n_jobs=4, scoring='accuracy' )\n\nprint(scores)\nscore_MLP = round(np.mean(scores) * 100, 3)\nprint(\"Score: {}\".format(score_MLP))\nacc_MLP = round(np.mean(accuracy_score(Y_test, y_pred)) * 100, 3)\nprint(\"Accuracy: {}\".format(acc_MLP))","b9036bb2":"# Decision Tree Classifer\nmodel_dtc = DecisionTreeClassifier()\nmodel_dtc.fit(X_train, Y_train)\ny_pred = model_dtc.predict(X_test)\n\nscores = cross_val_score(model_dtc, X_train, Y_train, cv=K_fold, n_jobs=4, scoring='accuracy')\n\nprint(scores)\nscore_dtc = round(np.mean(scores) * 100, 3)\nprint(\"Score: {}\".format(score_dtc))\nacc_dtc = round(np.mean(accuracy_score(Y_test, y_pred)) * 100, 3)\nprint(\"Accuracy: {}\".format(acc_dtc))","5ceaa5b5":"# Random Forest Classifier\nmodel_rfc = RandomForestClassifier(n_estimators=50)\nmodel_rfc.fit(X_train, Y_train)\ny_pred = model_rfc.predict(X_test)\n\nscores = cross_val_score(model_rfc, X_train, Y_train, cv=K_fold, n_jobs=4, scoring ='accuracy')\n\nprint(scores)\nscore_rfc = round(np.mean(scores) * 100, 3)\nprint(\"Score: {}\".format(score_rfc))\nacc_rfc = round(np.mean(accuracy_score(Y_test, y_pred)) * 100, 3)\nprint(\"Accuracy: {}\".format(acc_rfc))","5571225a":"# Gaussian Naive Bayes\nmodel_gaussNB = GaussianNB()\nmodel_gaussNB.fit(X_train, Y_train)\ny_pred = model_gaussNB.predict(X_test)\n\nscores = cross_val_score(model_gaussNB, X_train, Y_train, cv=K_fold, n_jobs=4, scoring='accuracy')\n\nprint(scores)\nscore_gaussNB = round(np.mean(scores) * 100, 3)\nprint(\"Score: {}\".format(score_gaussNB))\nacc_gaussNB = round(np.mean(accuracy_score(Y_test, y_pred)) * 100, 3)\nprint(\"Accuracy: {}\".format(acc_gaussNB))","00e888b4":"# Support vector classification\nmodel_SVC = SVC()\nmodel_SVC.fit(X_train, Y_train)\ny_pred = model_SVC.predict(X_test)\n\nscores = cross_val_score(model_SVC, X_train, Y_train, cv=K_fold, n_jobs=4, scoring='accuracy')\n\nprint(scores)\nscore_SVC = round(np.mean(scores) * 100, 3)\nprint(\"Score: {}\".format(score_SVC))\nacc_SVC = round(np.mean(accuracy_score(Y_test, y_pred)) * 100, 3)\nprint(\"Accuracy: {}\".format(acc_SVC))","152902f4":"# Gradient Boosting Classifier\nmodel_GBC = GradientBoostingClassifier()\nmodel_GBC.fit(X_train, Y_train)\ny_pred = model_GBC.predict(X_test)\n\nscores = cross_val_score(model_GBC, X_train, Y_train, cv=K_fold, n_jobs=4, scoring='accuracy')\n\nprint(scores)\nscore_GBC = round(np.mean(scores) * 100, 3)\nprint(\"Score: {}\".format(score_GBC))\nacc_GBC = round(np.mean(accuracy_score(Y_test, y_pred)) * 100, 3)\nprint(\"Accuracy: {}\".format(acc_GBC))","da106e72":"results = pd.DataFrame({'Model': ['Logistic Regression','KNeighborsClassifer', 'MLP Classifier', \n                                  'Decision Tree Classifier', 'Random Forest Classifier', 'GaussianNB', 'SVC', \n                                  'GB Classifier'],\n                        'Accuracy': [acc_logreg, acc_knn, acc_MLP, acc_dtc, acc_rfc, acc_gaussNB, \n                                  acc_SVC, acc_GBC], \n                        'Score': [score_logreg, score_knn, score_MLP, score_dtc, score_rfc, score_gaussNB, \n                                  score_SVC, score_GBC],})\ndf_results = results.sort_values(by='Score', ascending=False)\ndf_results = df_results.set_index('Score')\ndf_results","a661d2f6":"##### Support Vector Classification","8d98c6a2":"##### Multi-Layer Perceptron Classifier","492c9cf6":"MLP classifier relies on an underlying Neural Network to perform the task of classification.","2ee5c1ef":"# II) Cleaning the data\n\n# a) Healthy ranges of the feature results","0b1d5ba6":"Random forest classifier creates decision trees on randomly selected data samples. The model obtains prediction from each tree and subsequently selects the best solution by means of voting. Furthermore, random forest classfier also provides a very good indicator of the feature importance.\n","b6f61e90":"##### v) Alamine Aminotransferase","a5564d6b":"# b) Dealing with missing values","d984542f":"Based on the correlative pair plots, we find some interesting results directly.\n\n-Positive correlations:\n\nTotal Bilirubin and Direct Bilirubin (vice-versa)\n\nAlamine Aminotransferase and Aspartate Aminotransferase (vice-versa)\n\nTotal Protein and Albumin (vice-versa)\n\nAlbumin and Globulin Ratio and Albumin (vice-versa)\n\nTotal Protein and Albumin and Globulin Ration (vice-versa)\n\n-Negative correlations:\n\nTotal Protein and age (vice-versa)\n\nAlbumin and age (vice-versa)\n\nAlbumin and Globulin Ration and age (vice-versa)\n","34321539":"##### i) Simple hypothesis testing","d0e659e6":"# I) Familiarizing ourselves with the data","431a9fa9":"Healthy Ranges for the 10 feature columns\n\nTotal_Bilirubin = 0.1 to 1.2 mg\/dL = 1.71 to 20.5 umol\/L\n\nDirect_Bilirubin = < 0.3 mg\/dL = < 5.1 umol\/L\n\nAlkaline_Phosphatase = 44 to 147 IU\/L (High levels of ALP are seen in children undergoing growth and pregnant women)\n\nAlamine_Aminotransferase = 29 to 33 IU\/L (Age and gender can affect the value)\n\nAspartate_Aminotransferase = 1 to 45 U\/L (Values are slightly lower in females) Total_Proteins = 6.0 to 8.3 g\/dL\n\nAlbumin = 3.4 to 5.4 g\/dL\n\nAlbumin_and_Globulin_Ratio = Adult: 3.7 to 5.2 g\/dL; Older Adult: 3.2 to 4.6 g\/dL; >90 yr: 2.9 to 4.5 g\/dL\n\nNote: These values may differ based on the different guidelines or hospitals. The values above were obtained from google.\n","4dfc51d1":"##### ii) Machine Learning approaches","60510e09":"This is a non-parametric test that searches for the K nearest measurements of the training data and draws a Euclidean distance. It then votes based on that information of how to classify the data.","fc23a5e5":"Before delving into more complicated models, let's use a simple hyopthesis test to estimate the percentage of patients with liver disease.","5debc24e":"The support vectors classifier tries to find the best hyperplane to separate the different features by maximizing the distance between sample points and the hyperplane.","3b2bb01b":"This distribution plot shows some interesting things,\n\n1)On average, women tend not to have liver disease than men.\n\n2)The greatest number of women without liver disease were about 38 yrs old.\n\n3)Girls around the age of 10 had liver disease, this may be a genetic link.\n\n4)Men are more prone to liver disease (may be due to alcoholism).\n\n5)The greatest number of men without liver diseae were about 38 yrs old.\n","c41aa5ac":"# e) Applying machine learning approaches to liver disease","8ad027f7":"Logistic regression models the probabilities for classification problems with two possible outcomes. Furthermore, this models is an extension of the linear regression model but for classification problems.","e5cc9646":"##### ii) Total Bilirubin","ce3dcb50":"##### KNeighborsClassifier","fa68d964":"##### Logistic regression","58db95a1":"It appears that there are only 4 missing values in the feture column Albumin_and_Globulin_Ratio, which equates to 1% of the the entire data.","9b3f1296":"Let's try to find any correlations between the features. Here I will use pairplot in seaborn.","456881e1":"##### Gaussian Naive Bayes","94757fab":"# d) Distribution plots for all liver function tests","bbd13525":"##### Gradient Boosting Classifier","7e300948":"##### Which is the best model?","e879f871":"##### viii) Albumin","eefa2fe8":"Gradient boosting classifiers combines many weak learning models together to create a strong predictive model. Decision trees are usually used when doing gradient boosting.\n","69d7f538":"I will employ the following supervised and un-supervised machine learning models, whilst evaluating the mean accuracy of each of them by a stratified kfold cross validation procedure.\n","2987833a":"##### vii) Total Proteins","28418c6d":"The above correlation heatmap demonstrates strong positive (closer to 1) and negative correlations (closer to -1) but also weak positive and negative correlations (closer to zero). Next, let us plot some of these features as a function of gender in order to determine whether gender effects the target feature and the concentration levels of some of those feautures, which are deterministic of liver disease. However, before doing so we need to change the gender to numerical values.\n","003f2606":"##### Decision Tree Classifier","5328e9aa":"##### iv) Alkaline Phosphatase","6fcde3b8":"# Exploring Liver Disease with Machine Learning\nWritten by UbuntuZar 01\/10\/2020","93a2b739":"From the descriptive statistics above, we notice that the minimum age is 4 and the maximum is 90. Based on the information on this dataset, it was suggested that anyone above the age of 85 should be treated as 90. So we can change that through the creation of a new dataframe. Furthermore, we notice missing values in the column \"Albumin_and_Globulin_Ratio\", which we can deal with shortly. Lastly, it would be a good idea to figure out the ranges of healthy patients in order to figure out where each patient lies.\n","2d3878a2":"The data is continously split according to a certain parameter, in this case we are basing it on the features.","4bd8d52e":"##### x) Albumin and Globulin Ratio","8cb931ec":"##### vi) Aspartate Aminotransferase","9ff93b10":"It appears that the two best machine learning models are logistic regression and SVC as had the highest score and accuracy. Interestingly, I wasn't that far off from using the simple hypothesis testing, which gave me 71.355%. Things to consider to help improve both the score and accuracy are to introduce feature engineering and obviously more data points.","014fe343":"##### Random Forest Classifier","09ca8f01":"Based on the simple hypothesis test above, we find that ~ 71% of patients have liver disease. Now we can proceed to using more sophisticated machine learning models to estimate and predict liver disease in df_liver using supervised and un-supervised models. I will also be printing out the percent score and accuracy of each model.","7489f082":"It appears from these plots that men normally have high liver function test results and hence are likely to have liver disease. This is likely because culturally men in NE of Andhra Pradesh in India consume alchohol more than women. Let us now look a dsitribution plot for all the liver function tests in relation to the target feature and gender.","6ad1188e":"**I would really appreciate it if you anyone can comment, provide feedback and also please vote. Thank you everyone!**","95f90f28":"# c) Exploring the data visually","d6f4877c":"##### i- Age","725ce050":"##### iii) Direct Bilirubin","6beacec7":"What we can see here is that the data contains (583 rows by 11 columns). This means the that df_liver contains 583 observations + 10 features + 1 response (or target) variable. The response variable is \"Dataset\". Furthermore, df_liver consists of 5 Floats, 5 integers and 1 object. Therefore, the goal is to convert the object to numerical values so we can apply machine learning (ML) algorithms. We also notice that the column, 'Albumin_and_Globulin_Ratio' contains missing values (Nan).\n"}}