{"cell_type":{"cbeb35ee":"code","331f689c":"code","cf928651":"code","d4f6e07a":"code","6558a688":"code","4ee4babc":"code","2a956cab":"code","e9bf33a3":"code","18c19e39":"code","de64e886":"code","608e3579":"code","a40ab301":"code","3d1b878d":"code","ee674056":"code","07475366":"code","bd7aa607":"code","5dfd74d3":"code","7aabc5db":"code","93d77cce":"code","f023d564":"code","1b7a50c5":"code","e5256de6":"code","ca52d75c":"markdown","5c8074e6":"markdown","74a57a3b":"markdown","abfc9e51":"markdown"},"source":{"cbeb35ee":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","331f689c":"data = pd.read_csv(\"..\/input\/column_2C_weka.csv\")","cf928651":"data","d4f6e07a":"data.head()","6558a688":"data.tail()","4ee4babc":"data.describe()","2a956cab":"data.columns","e9bf33a3":"data.corr()","18c19e39":"f,axis = plt.subplots(figsize=(12, 12))\nsns.heatmap(data.corr(), annot=True, linewidths=.4, fmt= '.2f', ax = axis)\nplt.show()","de64e886":"sns.countplot(x=\"class\", data=data)\ndata.loc[:,'class'].value_counts()","608e3579":"sns.pairplot(data,hue=\"class\",palette=\"Set2\")\nplt.show()","a40ab301":"#Linear Regression\ndata1=data[data['class'] == \"Abnormal\"] #We create a shorter dataset from data\ndata1","3d1b878d":"#Linear Regression\n#Firstly, we should import our library for Linear regression\n\nfrom sklearn.linear_model import LinearRegression\nlinear_reg = LinearRegression()\nx = np.array(data1.loc[:,'pelvic_incidence']).reshape(-1,1) \n#define an variable use by pelvic_incidance from data1\ny = np.array(data1.loc[:,'sacral_slope']).reshape(-1,1)\n#define an variable use by sacral_slope from data1\nlinear_reg.fit(x,y) #We should fit these.\ny_head = linear_reg.predict(x)\nplt.figure(figsize=(15,5))\nplt.scatter(x,y,color=\"green\") #Let's see our figure\nplt.plot(x,y_head,color=\"black\")\nplt.xlabel(\"pelvic_incidence\")\nplt.ylabel(\"sacrel_scope\")\nplt.show()","ee674056":"#Decision Tree Regression\nfrom sklearn.tree import DecisionTreeRegressor\ntree = DecisionTreeRegressor()\ntree.fit(x,y)\nx_ = np.arange(min(x),max(x),0.01).reshape(-1,1)\ny_ = tree.predict(x_)\ny_val = tree.predict(x)\n#Visualize\nplt.figure(figsize=(15,5))\nplt.scatter(x,y,color=\"green\")\nplt.plot(x_,y_,color=\"red\")\nplt.xlabel(\"pelvic_incidence\")\nplt.ylabel(\"sacrel_scope\")\nplt.show()","07475366":"#Random Forest Regression\nfrom sklearn.ensemble import RandomForestRegressor\nrandom_reg = RandomForestRegressor(n_estimators=100,random_state=42)\nrandom_reg.fit(x,y)\nx_1 = np.arange(min(x),max(x),0.01).reshape(-1,1)\ny_predicted = random_reg.predict(x_1)\ny_val1 = random_reg.predict(x)\n#visualize\nplt.figure(figsize=(15,5))\nplt.scatter(x,y,color=\"green\")\nplt.xlabel(\"pelvic_incidence\")\nplt.ylabel(\"sacrel_scope\")\nplt.plot(x_1,y_predicted,color=\"blue\")\nplt.show()","bd7aa607":"    plt.figure(figsize=(15,5))\n    plt.scatter(x,y,color=\"green\")\n    plt.plot(x,y_head,color=\"green\",label=\"Linear Regression\")\n    plt.plot(x_,y_,color=\"red\",label=\"Decision Tree Regression\")\n    plt.plot(x_1,y_predicted,color=\"pink\",label=\"Random Forest Regression\")\n    plt.legend()\n    plt.xlabel(\"pelvic_incidence\")\n    plt.ylabel(\"sacrel_scope\")\n    plt.show()","5dfd74d3":"#Let's see our R^2(R-square) for all of regressions\nfrom sklearn.metrics import r2_score #This is the library of R-square with linear regression!\nprint (\"For Linear Regression: \",r2_score(y,y_head))\nprint (\"For Decision Tree Regression: \",r2_score(y,y_val) )\nprint (\"For Random Forest Regression: \",r2_score(y,y_val1) )","7aabc5db":"#Read data again!\ndata = pd.read_csv(\"..\/input\/column_2C_weka.csv\")","93d77cce":"data.info()","f023d564":"y1 = data.loc[:,'class']\nx1 = data.loc[:,data.columns!='class']\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.neighbors import KNeighborsClassifier\nx_train, x_test,y_train, y_test = train_test_split(x1,y1,test_size=0.3,random_state=42)\nknn = KNeighborsClassifier(n_neighbors=3)\nknn.fit(x_train,y_train)\nprediction = knn.predict(x_test)","1b7a50c5":"#See our predictions\nprediction","e5256de6":"print(\"K={} nn score is: {}\".format(3,knn.score(x_test,y_test)))","ca52d75c":"**Let's see all of our visualizations!**","5c8074e6":"**KNN ALGORITHM**","74a57a3b":"**As we can see; Decision Tree Regression gave us to truest predictions.**","abfc9e51":"**As we can see easily; Abnormal datas are located on more separated than Normal datas. **"}}