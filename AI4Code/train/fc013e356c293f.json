{"cell_type":{"cc8b0444":"code","52094182":"code","55c4550d":"code","927b07f0":"code","12d6c60a":"code","58a589c6":"code","b9e93786":"code","69bc97a6":"code","49433d4e":"code","17a395a1":"code","6f24d6bb":"code","fb01b607":"code","42179cb4":"code","e21a74fb":"code","2b6eff18":"code","41538129":"code","f098904c":"code","ff952e1f":"code","8c383dfc":"code","6973cffb":"code","cb2bc526":"code","b3082fce":"code","1f324dee":"code","c6a6fbb6":"code","99cfdb91":"code","f11ec094":"code","526f61c1":"markdown","b08155d3":"markdown","37a04805":"markdown","b5ccc09d":"markdown","3744e181":"markdown","ab70cdae":"markdown","d3ab4897":"markdown","058ac188":"markdown","8be17615":"markdown","bb524981":"markdown","d3ed4e97":"markdown","8161cffc":"markdown","38e59f63":"markdown","dc5f1066":"markdown","73821ac4":"markdown","bb965d5e":"markdown","d04c97c4":"markdown","92ff1cca":"markdown","9ba0a8c1":"markdown","33e049d6":"markdown","7dc5d9ba":"markdown","2237f151":"markdown","4511961d":"markdown","481db241":"markdown","d9949f27":"markdown","da965eb6":"markdown","9407c7d0":"markdown","43656f63":"markdown","200445c0":"markdown","36fbfb9b":"markdown"},"source":{"cc8b0444":"import tensorflow as tf\nimport numpy as np\nimport pandas as pd\nfrom sklearn.cross_validation import train_test_split\nfrom sklearn.preprocessing import MinMaxScaler\nimport matplotlib.pyplot as plt\n%matplotlib inline","52094182":"colnames = ['Class','Alcohol','Malic acid','Ash','Alcalinity of ash','Magnesium','Total phenols','Flavanoids','Nonflavanoid phenols','Proanthocyanins','Color intensity','Hue','dilute','Proline']","55c4550d":"df = pd.read_csv('..\/input\/wine.data.txt',names = colnames,index_col = False)","927b07f0":"df.head()","12d6c60a":"df.isnull().sum()","58a589c6":"df.info()","b9e93786":"df.Class.value_counts()","69bc97a6":"df.corr()","49433d4e":"df = pd.get_dummies(df, columns=['Class'])","17a395a1":"labels = df.loc[:,['Class_1','Class_2','Class_3']]","6f24d6bb":"labels = labels.values","fb01b607":"features = df.drop(['Class_1','Class_2','Class_3','Ash'],axis = 1)","42179cb4":"features = features.values","e21a74fb":"print(type(labels))\nprint(type(features))","2b6eff18":"print(labels.shape)\nprint(features.shape)","41538129":"train_x,test_x,train_y,test_y = train_test_split(features,labels)","f098904c":"print(train_x.shape,train_y.shape,test_x.shape,test_y.shape)","ff952e1f":"scale = MinMaxScaler(feature_range = (0,1))","8c383dfc":"train_x = scale.fit_transform(train_x)\ntest_x = scale.fit_transform(test_x)","6973cffb":"print(train_x[0])","cb2bc526":"print(train_y[0])","b3082fce":"X = tf.placeholder(tf.float32,[None,12]) # Since we have 12 features as input\ny = tf.placeholder(tf.float32,[None,3])  # Since we have 3 outut labels","1f324dee":"weights1 = tf.get_variable(\"weights1\",shape=[12,80],initializer = tf.contrib.layers.xavier_initializer())\nbiases1 = tf.get_variable(\"biases1\",shape = [80],initializer = tf.zeros_initializer)\nlayer1out = tf.nn.relu(tf.matmul(X,weights1)+biases1)\n\nweights2 = tf.get_variable(\"weights2\",shape=[80,50],initializer = tf.contrib.layers.xavier_initializer())\nbiases2 = tf.get_variable(\"biases2\",shape = [50],initializer = tf.zeros_initializer)\nlayer2out = tf.nn.relu(tf.matmul(layer1out,weights2)+biases2)\n\nweights3 = tf.get_variable(\"weights3\",shape=[50,3],initializer = tf.contrib.layers.xavier_initializer())\nbiases3 = tf.get_variable(\"biases3\",shape = [3],initializer = tf.zeros_initializer)\nprediction =tf.matmul(layer2out,weights3)+biases3","c6a6fbb6":"cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=prediction, labels=y))\noptimizer = tf.train.AdamOptimizer(0.001).minimize(cost)","99cfdb91":"acc = []\nwith tf.Session() as sess:\n    sess.run(tf.global_variables_initializer())\n    for epoch in range(201):\n        opt,costval = sess.run([optimizer,cost],feed_dict = {X:train_x,y:train_y})\n        matches = tf.equal(tf.argmax(prediction, 1), tf.argmax(y, 1))\n        accuracy = tf.reduce_mean(tf.cast(matches, 'float'))\n        acc.append(accuracy.eval({X:test_x,y:test_y}))\n        if(epoch % 100 == 0):\n            print(\"Epoch\", epoch, \"--\" , \"Cost\",costval)\n            print(\"Accuracy on the test set ->\",accuracy.eval({X:test_x,y:test_y}))\n    print(\"FINISHED !!!\")\n","f11ec094":"plt.plot(acc)\nplt.ylabel(\"Accuracy\")\nplt.xlabel(\"Epochs\")","526f61c1":"Check if it has any null values","b08155d3":"Matches is a list(tensor) which takes 1, if the index of largest element in prediction and y are equal and 0 it the indices are not equal.\nAccuracy is calculated by taking the mean of those matches.","37a04805":"# Last words.\nThis is not the best dataset for the neural networks. I would suggest something huge, like really huge.\nThe accuracy we got might not the best we can get.We can tune the model more by changing the epochs, learning rate.\nThere is no much significane of this kernel, better accuracies might be achieved by using sklearn's logisticregression etc.This is just for my understanding of the neural networks.\nThankyou!","b5ccc09d":"# Neural Network on Winedata using Tensorflow","3744e181":"Convert the feature dataframe to numpy arrays","ab70cdae":"Everything looks good, so lets go further.","d3ab4897":"Neural Networks perform better if the data is scaled between (0,1). So, lets do it.","058ac188":"Now collect the features dataframe","8be17615":"Lets plot the Accuracy over epochs","bb524981":" The Column names and other details are at 'https:\/\/archive.ics.uci.edu\/ml\/machine-learning-databases\/wine\/wine.names'","d3ed4e97":"Lets create our model with 2 hidden layers with 80 and 50 nodes respectively.\nIt was suggested(by online tutor) to use xavier_initializer for weights and zeros initializer for biases, but not mandatory.( I have been used to it, so i continued with this)\nI have also ran the model with random weights, they eventually optimize.","8161cffc":"Print the shapes of the split datasets.","38e59f63":"The graph of the accuracy over training steps(Epochs)","dc5f1066":"Frankly speaking, this dataset is not a worth of Neural Networks, but I chose this dataset, since this is my first attempt on NN's and i want to keep it simple.","73821ac4":"I'm expecting that everyone has an idea about the below process so I'm not going to elaborate much on this.","bb965d5e":"This a simple neural network classification on the winedata, I have kept it simple, not much of the data preprocessing and visualization. ","d04c97c4":"Now only take the labels into to new dataframe","92ff1cca":"To know about the class distribution.","9ba0a8c1":"Snapshot of the features and labels","33e049d6":"Check which columns are correlated ","7dc5d9ba":"We can see that they cost(loss) is reducing and the Accuracy is increasing, which shows that our model is training.","2237f151":"For Neural Nets the data should be in numpy arrays, so convert them,","4511961d":"As shown above Ash has very very low correlation in detecting the class, so it can be removed to improve our model. I'm dropping the ash columns in the following code.","481db241":"# The Neural Network part begins here.","d9949f27":"The dataset is available at 'https:\/\/archive.ics.uci.edu\/ml\/machine-learning-databases\/wine\/wine.data'","da965eb6":"Check the shape of the arrays","9407c7d0":"Snapshot of the data","43656f63":"Define the loss function, softmax_cross_entropy_with_logits_v2 is suggested over softmax_cross_entropy_with_logits because of label backpropagation, and then optimize the loss function. I have choose the learning rate as 0.001\n","200445c0":"Split into the training and testing sets, We have just 178 columns, which is a very very very small data for NN's","36fbfb9b":"# We convert the Class labels into the Onehot format."}}