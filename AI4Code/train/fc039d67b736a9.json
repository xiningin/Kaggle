{"cell_type":{"133ba6fa":"code","f691c88e":"code","625bfa7f":"code","0e3b1da3":"code","f3e41300":"code","1316aab6":"code","0a0372cf":"code","6173edef":"code","34b2e38a":"code","c206271b":"code","26ad851f":"code","a8c18430":"code","e5a7ce42":"code","5fd72b76":"code","6c9a82a4":"code","fafe5c40":"code","d5dd7a79":"code","33d11aec":"code","665b82fc":"code","f6ff59b1":"code","9eaeae28":"code","b3060816":"code","b5e5e736":"code","165f6775":"code","8be2c94e":"code","61234930":"code","7a657bf8":"code","a3fd3650":"code","e771fe73":"code","60fdb4cf":"code","c1518a4b":"code","3718bb74":"code","d1d5022b":"code","84f74ca5":"code","66c6a18a":"markdown","68219fa7":"markdown","d79ecc0a":"markdown","a2cb8704":"markdown","0689243d":"markdown","c1089678":"markdown","a2c14e2d":"markdown","797ab3e1":"markdown","e37829c5":"markdown","73cdc6c7":"markdown","aa56772b":"markdown","0db555e7":"markdown","2d956a66":"markdown","7d68ca63":"markdown","b9ae6880":"markdown"},"source":{"133ba6fa":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","f691c88e":"df=pd.read_csv('\/kaggle\/input\/landslide-events\/catalog.csv')\ndf.info()","625bfa7f":"features=df.columns.values\n\nmissing_val=df.isnull().sum()\nnon_missing_val= df.notnull().sum()\ntotal_val=df.shape[0]\n\npercentage_missing=missing_val\/total_val*100\npercentage_missing","0e3b1da3":"#convert date object to datetime\ndf['date_parsed']=pd.to_datetime(df['date'], format=\"%m\/%d\/%y\",errors='coerce')\n#convert date object to datetime\ndf[df.date_parsed.isnull()] #inspect null values \n","f3e41300":"df.dropna(subset=['date_parsed'], how='any', inplace=True)\ndf.info() #inspect df again ","1316aab6":"df['day']=df['date_parsed'].dt.day_name()\ndf['month']=df['date_parsed'].dt.month_name()","0a0372cf":"plt.figure(figsize=(15,6))\nsns.countplot(x=df['day'])\nplt.title('Days of the week from year 2007 to 2016')","6173edef":"plt.figure(figsize=(15,6))\nsns.countplot(x=df['month'])\nplt.title('Landslides across month from the year 2007 to 2016')","34b2e38a":"df['time'].unique()\n","c206271b":"df['time'].value_counts()","26ad851f":"df['time_parsed']=df['time']\ndf['time_parsed']=pd.to_datetime(df['time_parsed'],errors='coerce') \ndf.index=df['time_parsed'] #set index to timeparsed","a8c18430":"df[df.time_parsed.isnull()].head() #to check consistency between time and new index","e5a7ce42":"morning=df['time_parsed'].between_time('6:00:00','8:59:00') \nmorning.index\ndf.loc[morning.index] #check the rows ","5fd72b76":"df.loc[morning.index,'time']='Morning' #replace name of the rows\ndf.loc[morning.index] #check again to verify changes","6c9a82a4":"early_morning=df['time_parsed'].between_time('2:00:00','5:59:00') \nlate_morning=df['time_parsed'].between_time('9:00:00','11:59:00') \nafternoon=df['time_parsed'].between_time('12:00:00','16:00:00') \nlate_afternoon=df['time_parsed'].between_time('16:01:00','17:00:00') \nearly_evening=df['time_parsed'].between_time('17:01:00','19:00:00') \nevening=df['time_parsed'].between_time('19:01:00','21:00:00') \nlate_evening=df['time_parsed'].between_time('21:01:00','22:59:00') \nnight=df['time_parsed'].between_time('23:00:00','23:59:00') \novernight=df['time_parsed'].between_time('00:00:00','1:59:00') \n\n\ndf.loc[early_morning.index,'time']='Early Morning'\ndf.loc[late_morning.index,'time']='Late Morning'\ndf.loc[afternoon.index,'time']='Afternoon'\ndf.loc[late_afternoon.index,'time']='Late Afternoon'\ndf.loc[early_evening.index,'time']='Early Evening'\ndf.loc[evening.index,'time']='Evening'\ndf.loc[late_evening.index,'time']='Late Evening'\ndf.loc[night.index,'time']='Night'\ndf.loc[overnight.index,'time']='Overnight'\n\n\n","fafe5c40":"df['time'].value_counts()","d5dd7a79":"import fuzzywuzzy\nfrom fuzzywuzzy import process\nimport chardet","33d11aec":"matches = fuzzywuzzy.process.extract(\"Early Morning\",df.time.unique(), limit=5, scorer=fuzzywuzzy.fuzz.token_sort_ratio)\nmatches","665b82fc":"# function to replace rows in the provided column of the provided dataframe\n# that match the provided string above the provided ratio with the provided string\ndef replace_matches_in_column(df, column, string_to_match, min_ratio):\n    # get a list of unique strings\n    strings = df[column].unique()\n    \n    # get the top 10 closest matches to our input string\n    matches = fuzzywuzzy.process.extract(string_to_match, strings, \n                                         limit=10, scorer=fuzzywuzzy.fuzz.token_sort_ratio)\n\n    # only get matches with a ratio > 90\n    close_matches = [matches[0] for matches in matches if matches[1] >= min_ratio]\n\n    # get the rows of all the close matches in our dataframe\n    rows_with_matches = df[column].isin(close_matches)\n\n    # replace all rows with close matches with the input matches \n    df.loc[rows_with_matches, column] = string_to_match\n    \n    # let us know the function's done\n    print(\"All done!\")","f6ff59b1":"replace_matches_in_column(df,'time','Early Morning',99) # all values from 99 to 78 as the min ratio would've worked as well. ","9eaeae28":"df['time'].value_counts()","b3060816":"df.loc[((df['time']=='overnight') | (df['time']=='Late night') ),'time']='Overnight'\ndf.loc[df['time']=='Late morning','time']='Late Morning'\ndf.loc[df['time']=='evening','time']='Evening'\ndf.loc[df['time']=='Late evening','time']='Late Evening'\ndf.loc[df['time']=='Late afternoon','time']='Late Afternoon'\ndf.loc[df['time']=='Before Dawn','time']='Early Morning'\ndf.loc[df['time']=='Before dawn','time']='Early Morning'\n\nunknown =[' ','****' ]\n\nfor character in unknown:\n    df.time = df.time.replace(character, \"Unknown\")\n","b5e5e736":"df['time'].value_counts()","165f6775":"plt.figure(figsize=(15,5))\nsns.countplot(x=df['time'])\nplt.xlabel('Occurance of landslide')\nplt.title(\"After parsing and cleaning up the Time column values\")","8be2c94e":"df.info()","61234930":"df.country_name.unique()","7a657bf8":"NA_countries=[\"Anguilla\",\n\"Antigua and Barbuda\",\n\"Aruba\",\n\"Bahamas\",\n\"Barbados\",\n\"Belize\",\n\"Bermuda\",\n\"Bonaire\",\n\"British Virgin Islands\",\n\"Canada\",\n\"Cayman Islands\",\n\"Costa Rica\",\n\"Cuba\",\n\"Curacao\",\n\"Dominica\",\n\"Dominican Republic\",\n\"El Salvador\",\n\"Greenland\",\n\"Grenada and Carriacuou\",\n\"Guadeloupe\",\n\"Guatemala\",\n\"Haiti\",\n\"Honduras\",\n\"Jamaica\",\n\"Martinique\",\n\"Mexico\",\n\"Miquelon\",\n\"Montserrat\",\n\"Netherlands Antilles\",\n\"Nevis\",\n\"Nicaragua\",\n\"Panama\",\n\"Puerto Rico\",\n\"Saba\",\n\"Sint Eustatius\",\n\"Sint Maarten\",\n\"St. Kitts\",\n\"St. Lucia\",\n\"St. Pierre and Miquelon\",\n\"Saint Vincent\",\n\"Trinidad and Tobago\",\n\"Turks and Caicos Islands\",\n\"United States\",\n\"US Virgin Islands\"]\n","a3fd3650":"df.loc[df.country_name.isin(NA_countries), 'continent_code']='NA'\n\nprint (df.continent_code.count()) #count non null values\nprint(df.country_name.count()) #count non null values","e771fe73":"filter1=df.loc[~df.country_name.isin(NA_countries), 'country_name'].unique()\nfilter1","60fdb4cf":"for countries in filter1:\n    matches=fuzzywuzzy.process.extract(countries,NA_countries, limit=5, scorer=fuzzywuzzy.fuzz.token_sort_ratio)\n    print(matches)","c1518a4b":"def replace_matches_in_column(df, column, string_to_match, min_ratio):\n    # get a list of unique strings\n    strings = df[column].unique()\n    \n    # get the top 10 closest matches to our input string\n    for string in string_to_match:\n        matches = fuzzywuzzy.process.extract(string, strings, \n                                             limit=5, scorer=fuzzywuzzy.fuzz.token_sort_ratio)\n\n        # only get matches with a ratio > 90\n        close_matches = [matches[0] for matches in matches if matches[1] >= min_ratio]\n\n        # get the rows of all the close matches in our dataframe\n        rows_with_matches = df[column].isin(close_matches)\n\n        # replace all rows with close matches with the input matches \n        df.loc[rows_with_matches, column] = string\n    \n    # let us know the function's done\n    print(\"All done!\")","3718bb74":"replace_matches_in_column(df, 'country_name', NA_countries, 80) #correct all country_name that have a ratio score above 80 to North American list. \ndf.country_name.unique() ","d1d5022b":"NA_countries = NA_countries +['Saint Vincent and the Grenadines','Grenada' ] #append NA america list.\ndf.loc[df.country_name.isin(NA_countries), 'continent_code']='NA' #update continent_code\nprint (df.continent_code.count()) #count non null values\nprint(df.country_name.count()) #count non null values ","84f74ca5":"sns.countplot(x=df['continent_code'])\nplt.title('Imputing NA into continent_code column')","66c6a18a":"### Using fuzzywuzzy to clean similar strings\n\nFinally, we can clean up the rest of the time column using fuzzy wuzzy package. Here is an example of cleaning Early morning. We should get 91 entries after combining Early morning and Early Morning values. \nThe reason I'm introducing fuzzy wuzzy package is because it will come in handy in cleaning string columns later on. ","68219fa7":"# Impute Continent_Code \n\nContinent codes are defined as the following:\nAF\tAfrica\t\nAN\tAntarctica\t\nAS\tAsia\t\nEU\tEurope\t\nNA\tNorth america\t\nOC\tOceania\t\nSA\tSouth america\t\n\nKaggle doesn't have the module **pycountry_convert** to simplify this cleaning process. Luckily, it is not too much of a manual work to clean with what modules we have. From the unique() function, the countries are from North America and South America. So if a country is not in NA then it is in SA. I have obtained a list of NA in excel :\n\n![image.png](attachment:978d9458-f491-49c0-84dc-e43d14491a76.png)\n","d79ecc0a":"# Parse 'Time' Column\n\nThe function unique() reveals the *time* column is recorded as string in day-time descriptors and string numerics (HH:MM:SS). It is tempting to drop the rows with day-time descriptors but inspection with value_counts() reveals a large number of rows with time descriptors. It is best to convert numerical strings into one of the fourteen descriptors to keep the consistency in the column. \n\nHere is a [list of descriptor for time](https:\/\/urduesl.com\/parts-of-the-day\/) \n\nEarly morning\t2:00 a.m. to 5:59 a.m.\t\nDawn\taround sunrise\t\nMorning\t6:00 a.m to 8:59 a.m\t\nLate morning\t9:00 a.m to 11:59 a.m\t\nNoon\/ midday\t12:00 p.m.\t\n**Afternoon**\t12:01 p.m. to *5*:00 p.m.\t\n**Late afternoon**\t*4*:00 pm to 5:00 pm.\t\n**Early evening**\t5:01 pm to 7:00 pm.\t\nDusk\taround sunset\t\n**Evening**\t*5*:01 pm to 9:00 pm.\t\nLate evening\t9:01 p.m. to 10:59 p.m.\t\n**Night**\t11:00 pm till sunrise\t\n**Midnight**\t12:00 a.m.\t\n**Middle of the night**\t1:00 a.m. to 3:00 a.m.\n\nWe will not have to worry about the ill-edfine descriptor dawn, and dusk since no row has a Dawn\/Dusk entry. \n\nWe see time descriptors that overlaps (indicated in bold). Since this is a practice data cleaning set, I will redefine the above time descriptors as:\n\nEarly morning\t2:00 a.m. to 5:59 a.m.\t\nMorning\t6:00 a.m to 8:59 a.m\t\nLate morning\t9:00 a.m to 11:59 a.m\t\n**Afternoon**\t12:00 p.m. to *4*:00 p.m.\t\n**Late afternoon**\t*4*:01 pm to 5:00 pm.\t\n**Early evening**\t5:01 pm to 7:00 pm.\t\n**Evening**\t*7*:01 pm to 9:00 pm.\t\nLate evening\t9:01 p.m. to 10:59 p.m.\t\n**Night**\t11:00 pm to 11.59am\t\n**Overnight**\t12:00 a.m. to 1:59 a.m.\n\nI will define *late night* as *overnight*. \nI will also set  **** to Unknown\n","a2cb8704":"After updating the continent_code columns for North american countries to NA, notice that there are still 6 uncounted country codes where countries values are recorded. A high possibility is that the North American list names does not match the country names in the database. ","0689243d":"It seems that landslide occurs during the middle of the year and the least in early of the year. ","c1089678":"We can automate the countries that has a high ratio (>80). But for Grenada and Saint Vicent we will have to redefine the North American list. ","a2c14e2d":"### Using loc[] and replace () to clean string\nIt is a little more work but we can use loc to convert the rest of the data. \n*Please see the cleaning exercise on continent_code columns where I had redefined the fuzzywuzzy function to loop from a list to clean.*\n\nLate Morning+Late morning=36\t\nOvernight+overnight + Late night =36\t\nEvening+evening=43\t\nLate Afternoon +Late afternoon=15\t\nBefore Dawn= Early morning so Early Morning needs to be updated to 92. \t\n\nFor all of those null, star characters that can't be detected by fuzzy wuzzy and too long to include loc, we can use replace.\n\n\n","797ab3e1":"'Saint Vincent and the Grenadines', 'Saint Lucia,', 'US Virgin Islands', and 'Grenada' are not spelled\/defined the same as those in the list I had obtained. We can anticipate that there will always be ambiguities if the data entry is not standardized. It would be nice if we can use fuzzy logic to loop all the left over countries names that are similar to the North American list. ","e37829c5":"Let's first convert all times from 6am to 8.59 am to 'Morning'. Apply between_time() to select the time frame, retrieve the row indices, and replace the numerical value in time column to 'Morning'.","73cdc6c7":"# Parse 'date' column \n\nEnsure date_parsed is in datetime. Usually I would drop rows where date are null, but since the date is in an object format, there could be blanks that could've been a space. Using *coerce* set incorrect entries into NaT. Then, we can safely drop the rows with NaT. isnull() and dropna() works for NaT. \n\nThe error catch in parsing date in pandas is the following\n\n*errors{\u2018ignore\u2019, \u2018raise\u2019, \u2018coerce\u2019}, default \u2018raise\u2019*\n* If \u2018raise\u2019, then invalid parsing will raise an exception.\n* If \u2018coerce\u2019, then invalid parsing will be set as NaT.\n* If \u2018ignore\u2019, then invalid parsing will return the input.","aa56772b":"#Cleaning list\nThe function info() reveals a cleaning list:\n1. Convert date, into datetime type. \n2. Convert time into time descriptors.\n3. Clean columns having char datatype.\n4. Fill continent_code's missing values given the country_name\/country_code values. \n5. Fill city\/town's missing values given the state\/provice values.\n5. Remove columns that has more than 50% missing data.\n6. Remove duplicated id. \n\n*I have noticed those that attempted to clean this data set stopped at converting date to datetime. This notebook is my attempt in cleaning all of them. So far, 1,2 and 4.*\n\nIt also provides possible EDA list:\n1. Proportion of landslide_type to landslide_size.\n2. Proportion of landslide_type to trigger.\n3. Distribution of landslide events according to month. \n4. Distribution of population affected by landslides.\n","0db555e7":"## Parse into day of the month and month of the year","2d956a66":"## between_time()\nI would like to use between_time() to select the desired times for conversion.  between_time() function references from a datetime index. ","7d68ca63":"Now that we are convinced it works, we can do the same for the rest of the time descriptors.\n\nEarly morning\t2:00 a.m. to 5:59 a.m.\t\nMorning\t6:00 a.m to 8:59 a.m\t\nLate morning\t9:00 a.m to 11:59 a.m\t\n**Afternoon**\t12:00 p.m. to *4*:00 p.m.\t\n**Late afternoon**\t*4*:01 pm to 5:00 pm.\t\n**Early evening**\t5:01 pm to 7:00 pm.\t\n**Evening**\t*7*:01 pm to 9:00 pm.\t\nLate evening\t9:01 p.m. to 10:59 p.m.\t\n**Night**\t11:00 pm to 11.59am\t\n**Overnight**\t12:00 a.m. to 1:59 a.m.","b9ae6880":"Below is the percentage of missing values for every columns. Normally, we remove the columns for which 50% are missing, however since this is a data cleaning challenge, I will attempt to clean most the columns as practice and fill the missing value for some. The most ambitious cleaning task thus far is time.  "}}