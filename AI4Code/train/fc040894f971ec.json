{"cell_type":{"97f7fb47":"code","d6877d38":"code","1ee62df4":"code","5a9deb04":"code","fe3e6a9f":"code","75963933":"code","1abfa45e":"code","bf6ec2a0":"code","bd1c1215":"code","971ff9ca":"code","9ebc6a9a":"code","1bb0fb50":"code","7a67374d":"code","c990ba2c":"markdown","2e022c8b":"markdown","ce2f0073":"markdown","74c62ff1":"markdown","6f44db94":"markdown","4ec84b39":"markdown","faf094a0":"markdown","eb1f97f2":"markdown","501a5eb1":"markdown","ed40c04a":"markdown","b6a85408":"markdown","12cf1e72":"markdown","c1dfaeda":"markdown","b232873d":"markdown","4462c3df":"markdown","c2f0d1f5":"markdown","9c750351":"markdown","70bc1def":"markdown"},"source":{"97f7fb47":"import os\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt \nprint(os.listdir(\"..\"))","d6877d38":"mydata = pd.read_csv(\"..\/input\/breast-cancer-wisconsin-data\/data.csv\")\nprint(mydata.shape)\nprint(mydata.head(2))\n# Looking at the class\nprint(mydata.iloc[:,1].describe())\n# Looking at first few variables\nprint(mydata.iloc[:,2:6].describe())","1ee62df4":"mydata.boxplot(column=[ 'texture_mean','perimeter_mean'],by='diagnosis',  grid=False)","5a9deb04":"# Dropping Columns\nmydata=mydata.drop(['id', 'Unnamed: 32'], axis = 1) \nprint(mydata.shape)","fe3e6a9f":"y = mydata.iloc[:,0]\nx = mydata.iloc[:,1:31]","75963933":"from sklearn.preprocessing import MinMaxScaler\nscaler = MinMaxScaler(feature_range=(0, 1))\nx_scaled = scaler.fit_transform(x)\nx_scaled","1abfa45e":"from sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(x_scaled, y, test_size=0.2, random_state=42)","bf6ec2a0":"from sklearn.neighbors import KNeighborsClassifier\n#Initalize the classifier\nknn = KNeighborsClassifier(n_neighbors=15)\n#Fitting the training data\nknn.fit(x_train, y_train)\n#Predicting on test\ny_pred=knn.predict(x_test)","bd1c1215":"#Accuracy\nprint('Accuracy = ', knn.score(x_test, y_test))\n\n#Confusion Matrix\nfrom sklearn.metrics import confusion_matrix\nprint('\\nConfusion matrix')\nprint(confusion_matrix(y_test, y_pred))\n\n#Classification Report\nfrom sklearn.metrics import classification_report\nprint('\\nClassification Report')\nprint(classification_report(y_test, y_pred))  ","971ff9ca":"nc=np.arange(10,1,-2)\nnc","9ebc6a9a":"nc=np.arange(1,100,2)\nacc=np.empty(50)\ni=0\nfor k in np.nditer(nc):\n    knn = KNeighborsClassifier(n_neighbors=int(k))\n    knn.fit(x_train, y_train)\n    temp= knn.score(x_test, y_test)\n    print(temp)\n    acc[i]=temp\n    i = i + 1\nacc","1bb0fb50":"x=pd.Series(acc,index=nc)\nx.plot()\n# Add title and axis names\nplt.title('Neighbor vs Accuracy')\nplt.xlabel('Count of Neighbor')\nplt.ylabel('Accuracy')\nplt.show() \n","7a67374d":"from sklearn.model_selection import GridSearchCV\nparameters = {\"n_neighbors\":(3,5,7,15,25,100),\"weights\":(\"uniform\", \"distance\"), 'metric':(\"euclidean\",\"manhattan\",\"chebyshev\")}\nknn = KNeighborsClassifier()\nclf = GridSearchCV(knn, parameters,cv=5,return_train_score=True)\nclf.fit(x_scaled, y)\nsorted(clf.cv_results_.keys())\nprint(\"The best parameters are %s with a score of %0.2f\"\n      % (clf.best_params_, clf.best_score_))\nclf.cv_results_","c990ba2c":"![image.png](attachment:image.png)","2e022c8b":"# Fitting the model <a id=2.3><\/a>","ce2f0073":"![image.png](attachment:image.png)","74c62ff1":"# Importing and Inspecting the Data <a id=\"1.1\"><\/a>","6f44db94":"30,42,38,74,89  -> 30,38,42,74,89","4ec84b39":"* Feature Scaling is a technique to standardize the independent features present in the data in a fixed range. \n* It is performed during the data pre-processing to handle highly varying magnitudes or values or units\n* Min-Max Normalization: This technique re-scales a feature or observation value with distribution value  0 and 1\n* Standardization: It is a very effective technique which re-scales a feature value so that it has distribution with 0 mean value and variance equals to 1.\n* Min-Max is often preferred as it maintains the shape of the distribution","faf094a0":"# Getting Started <a id=\"1\"><\/a>\nHere we describe importing the library, impoting the datset and some basic checks on the dataset","eb1f97f2":"# Setting up the input and output variable <a id=\"2.1\"><\/a>","501a5eb1":"Splitting data into training and testing sets","ed40c04a":"# Model Evaluation <a id=\"2.4\"><\/a>","b6a85408":"# Scaling the data <a id=\"2.2\"><\/a>","12cf1e72":"# Contents\n\n* [<font size=4>Getting Started<\/font>](#1)\n    * [Importing the Libraries](#1.1)\n    * [Importing and Inspecting the Data](#1.2)\n   \n   \n* [<font size=4>Fitting the model<\/font>](#2)\n    * [Setting up the input and the output variable](#2.1)\n    * [Scaling Data](#2.2)\n    * [Fitting the Model](#2.3)\n    * [Model Evaluation](#2.4)\n    * [Optimal value of K](#2.5)\n    \n\n","c1dfaeda":"Features are computed from a digitized image of a fine needle aspirate (FNA) of a breast mass. They describe characteristics of the cell nuclei present in the image.\n1)ID number\n2)Diagnosis (M = malignant, B = benign)\n3-32)","b232873d":"# Assignment\n* Report classification accuracy with k value of 100\n* Change the weight parameter to distance and report the result with k =15","4462c3df":"# Fitting the model <a id=\"2\"><\/a>\nIn this section we first set up the input and output variable.   Build kNN with 15 neighbors.  Look at the classification metric.  A test on diffrent values of 'k' also has  been performed.","c2f0d1f5":"**Plotting the accuracy with number of neighborhoods**","9c750351":"# Importing the Libraries <a id=\"1.1\"><\/a>","70bc1def":"# Optimal value of K <a id=\"2.5\"><\/a>"}}