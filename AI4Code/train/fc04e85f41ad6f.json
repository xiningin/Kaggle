{"cell_type":{"4973ddec":"code","0d0b95e3":"code","ae278d79":"code","388d0da4":"code","f0f35a45":"code","1dc7f70a":"code","14489b01":"code","ee4d9e0c":"code","86af020b":"code","a76070fa":"code","59cc8a1c":"code","c226ab87":"code","d5cdd4d6":"code","bc173cd8":"code","303030a2":"code","71c65cdd":"code","c365a1c3":"code","95fbd191":"code","7d691948":"code","77948e55":"code","c0f6ec42":"code","03d2ea9c":"code","a34addfa":"code","03b5fb49":"code","22fe4d43":"code","62bb8c1c":"code","c71d3d1f":"code","a477cc95":"code","3d7be5e1":"code","b96afd96":"code","401c0f91":"code","a6fd0ee4":"code","1a48ac5d":"code","dbe663df":"code","4fed33cc":"code","05e55a3b":"code","18c4ed74":"code","61353082":"code","35e64509":"code","72e44ef6":"code","1364a713":"code","cdcdc406":"code","f8be9efb":"code","eff87a56":"code","ece6a9a8":"code","2e715537":"code","1e3643b2":"code","c9ac5828":"code","d0f55bde":"code","c480b11a":"markdown","b2f1b75b":"markdown","33ab4417":"markdown","f065dda4":"markdown","fd15031a":"markdown","1e5d66d8":"markdown","5ad74bd1":"markdown","081d82af":"markdown","4c1cede0":"markdown","b97da1b0":"markdown","dd13d8d6":"markdown","e57e073e":"markdown","aecd532f":"markdown","7c49f049":"markdown","7a8263fb":"markdown","5a7e2d92":"markdown","11f51a01":"markdown","92f6e64a":"markdown","ebb7a3a6":"markdown","f66f2d9c":"markdown","a45eb746":"markdown","beb775d1":"markdown","96acce03":"markdown","f361150b":"markdown"},"source":{"4973ddec":"import numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport pandas as pd\n%matplotlib inline","0d0b95e3":"df = pd.read_csv('..\/input\/real-estate-price-prediction\/Real estate.csv')","ae278d79":"df.head()","388d0da4":"ax = sns.heatmap(df.corr(),annot=True,linewidths=.5)","f0f35a45":"sns.jointplot(data=df, x='X4 number of convenience stores', y='Y house price of unit area', kind='kde')","1dc7f70a":"X = df.drop('Y house price of unit area' , axis=1)\ny = df['Y house price of unit area']","14489b01":"from sklearn.preprocessing import PolynomialFeatures\npolynomial_converter = PolynomialFeatures(degree=2 ,include_bias=False)\npolynomial_features = polynomial_converter.fit_transform(X)","ee4d9e0c":"print(f'number of original features:{X.shape[1]}')","86af020b":"print(f'number of features after expand is:{polynomial_features.shape[1]}')","a76070fa":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(polynomial_features, y, test_size=0.3, random_state = 101 )","59cc8a1c":"from sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()","c226ab87":"X_train[0] # without scaling","d5cdd4d6":"sns.kdeplot(X_train[0])","bc173cd8":"scaler.fit(X_train) # calculate statistics\n","303030a2":"#scale on data set\nX_train=scaler.transform(X_train)\nX_test=scaler.transform(X_test)","71c65cdd":"X_train[0] #now our features are scaled","c365a1c3":"sns.kdeplot(X_train[0])","95fbd191":"from sklearn.linear_model import Ridge\nridge_model = Ridge(alpha=40) # note that hyperparameter(lambda) in sklearn is alpha. ","7d691948":"ridge_model.fit(X_train, y_train)","77948e55":"y_pred = ridge_model.predict(X_test)","c0f6ec42":"#let's Evaluate\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error\nMAE = mean_absolute_error(y_test, y_pred)\nMSE = mean_squared_error (y_test, y_pred)\nRMSE = np.sqrt(MSE)","03d2ea9c":"pd.DataFrame([MAE,MSE,RMSE],index=['MAE','MSE','RMSE'],columns=['metrics'])","a34addfa":"from sklearn.linear_model import RidgeCV\nridge_cv_model = RidgeCV(alphas=(0.1, 1.0, 10.0), scoring='neg_mean_absolute_error') #scoring calculate based on our metrics. cv=None is equals to leave_one_out technique\nridge_cv_model.fit(X_train, y_train)","03b5fb49":"print(f'the best hyperparameters value is: {ridge_cv_model.alpha_}')","22fe4d43":"y_pred_ridge = ridge_cv_model.predict(X_test)","62bb8c1c":"#let's Evaluate again\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error\nMAE = mean_absolute_error(y_test, y_pred_ridge)\nMSE = mean_squared_error (y_test, y_pred_ridge)\nRMSE = np.sqrt(MSE)","c71d3d1f":"pd.DataFrame([MAE,MSE,RMSE],index=['MAE','MSE','RMSE'],columns=['metrics'])","a477cc95":"ridge_cv_model.coef_ #now coefficients are smaller","3d7be5e1":"A=ridge_cv_model.coef_\nlen(A)","b96afd96":"plt.figure()\nplt.bar(np.arange(1, len(A) + 1), height=np.abs(A))\nplt.show()","401c0f91":"from sklearn.linear_model import LassoCV","a6fd0ee4":"lasso_cv_model = LassoCV(eps = 0.1 , n_alphas=100, cv=5) #here we have a period for alpha that we show it with eps and  n_alphas. cv=None is equal to three fold","1a48ac5d":"lasso_cv_model.fit(X_train,y_train)","dbe663df":"lasso_cv_model.alpha_","4fed33cc":"y_pred_lasso = lasso_cv_model.predict(X_test)","05e55a3b":"#let's Evaluate again\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error\nMAE = mean_absolute_error(y_test, y_pred_lasso)\nMSE = mean_squared_error (y_test, y_pred_lasso)\nRMSE = np.sqrt(MSE)","18c4ed74":"pd.DataFrame([MAE,MSE,RMSE],index=['MAE','MSE','RMSE'],columns=['metrics'])","61353082":"lasso_cv_model.coef_ #eliminated some features","35e64509":"B=lasso_cv_model.coef_","72e44ef6":"plt.figure()\nplt.bar(np.arange(1, len(B) + 1), height=np.abs(B))\nplt.show()","1364a713":"from sklearn.linear_model import ElasticNetCV","cdcdc406":"elastic_model = ElasticNetCV(l1_ratio=[.1, .5, .7, .9, .95, .99, 1],cv=5,max_iter=100000) #l1_ratio: say that which l1(close to 1) or l2(close to 0) are better?","f8be9efb":"elastic_model.fit(X_train,y_train)","eff87a56":"y_pred_elastic = elastic_model.predict(X_test)","ece6a9a8":"#let's Evaluate again\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error\nMAE = mean_absolute_error(y_test, y_pred_elastic)\nMSE = mean_squared_error (y_test, y_pred_elastic)\nRMSE = np.sqrt(MSE)","2e715537":"pd.DataFrame([MAE,MSE,RMSE],index=['MAE','MSE','RMSE'],columns=['metrics'])","1e3643b2":"elastic_model.coef_","c9ac5828":"C=elastic_model.coef_","d0f55bde":"plt.figure()\nplt.bar(np.arange(1, len(C) + 1), height=np.abs(C))\nplt.show()","c480b11a":"as you saw we converted our features to polynomial ","b2f1b75b":"## Step6: Preprocessing(Creating polynomial features)","33ab4417":"# how to know the optimum value of hyperparameter(here is alpha)?","f065dda4":"# Cross Validation(CV):\nIt is an advanced set of methods that systematically splitting data into train and test set.(ISLR book Section 5.1 for more information)\n\nwe can achieve two goals by this method:\n* Train on all the data \n* Evaluate on all the data\n\nIt has a **high accuracy** but **high complexity**.\nthis is known as K-fold cross-validation(usually k=10) \n\nwe have to be aware of data leakage( we can split data set into **Train|Validation|Test**)","fd15031a":"# HI ^_^\n**In this notebook we will learn how to regularize with sickit learn library on Linear Regression algorithm step by step**\n\n**I used \"real estate price prediction\" data set.**\n\n**Let's get started**","1e5d66d8":"# why do we need Feature Scaling?\n* we need this technique for learning models that rely on metrics(like KNN).\n* different features scale cause different ratio in updating the values.\n* we can compare features.\n* increase the performance.\n\n**Remember that: we have to scale the unseen data in the feauture. harder or different interpretability of feature coeff.**\n","5ad74bd1":"## Step7: split the data to Train and Test","081d82af":"## Step8: Scaling the data:","4c1cede0":"## Three types of regularization:\n* L1 regularization(**LASSO** regularization) : add a penatly equal to **absolut value** of the magnitude of coefficients and limit the size of the coefficients and yield **sparse** model. It is like it chooses the important coefficients and gives zero number to another.\n\n![L1.png](attachment:54bfe804-c994-4816-a4ca-70e90598980b.png)\nRSS=Residual Sum of Squared\n* L2 regularization(**Ridge** regularization): does not necessarily **eliminate** coefficients. and it is equal to **square value** of the magnitude of coefficients.\n\n![image.png](attachment:0adb8bc0-3d9c-4c63-bdd1-811944452458.png)\n* combination of L1 & L2(Elastic Net): we have alpha that decide the ratio between them.(0=<alpha<=1)\n\n![image.png](attachment:eeb817ff-db65-4787-890c-812326d214f2.png)","b97da1b0":"# **Why do we use Regularization?**\n*  it is use to **minimize complexity** to overcome **overfitting**(add more bias to **reduce model variance**, this happens because of bias-variance trade off)\n*  penalizing the loss function (it uses a kind of hyperparameter like **lamda** to balance the formula between Regularization & loss function)\n\n**It is something like automatically feature extracting and we prefer smaller features**","dd13d8d6":"**what is lamda?**\n> As I told you before this is trade of between **Error** and **Regularization**. and when lamda is lower we insist on Error and if lambda=0 we face a higher risk to have overfitting.and with higher lambda we have smoother prediction on data and lower risk for overfitting, but it is not necessarily the best answer. as I told you this a **trade-off** and a **hyperparameter**. \n","e57e073e":"**Note: we can tune our hyperparameters by Cross Validation**","aecd532f":"### 2-LASSO regularization:","7c49f049":"## Step5: Determine the Features and labels:","7a8263fb":"pay attention to the scale of these two kde plots.","5a7e2d92":"**Important Note: we do not do Regularization on bias.**\n\n**Note: But these additional hyperparameters have cost for our models because as you know, it needs experience to find the best value for them**","11f51a01":"### 3-ELastic Net","92f6e64a":"### two techniques of scaling:\n* **Standardization**: rescale data to have mean=0 and standard_deviation=1\n* **Normalization**: all data values to be between 0-1.\n\n\n![image.png](attachment:7704cdcd-4adb-4c14-8d9d-2ab3d36d43c9.png)\n\n**Note: we do not use scaling on labels**","ebb7a3a6":"### 1-Ridge Regression","f66f2d9c":"**An important note about fit & transform methods:**\n\nImagine that A is our features. so **A.fit()** method, calculates the necessary statistics (Xmin,Xman,mean,standard deviation).\n\n**A.transform()**, scales data by means of data scaling techniques.\n\n> the note is: we only fit to **Training Data**, because we don't want to assume prior knowledge on the test set(**we don't want data leakage**) and then we can use A.transform() on Test and Train data set.\n","a45eb746":"## Step2:Import Data Set as a Data Frame","beb775d1":"## Step3 & 4:EDA and data overview","96acce03":"## Step9: Regularization","f361150b":"## Step1: Import Libraries"}}