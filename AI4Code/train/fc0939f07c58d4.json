{"cell_type":{"ad442a82":"code","3bef482a":"code","d55d7d25":"code","40435b9d":"code","b4cb66a5":"code","ef834ae2":"code","1f67b446":"code","8cff5052":"code","5a731810":"code","b5b38bab":"code","4ed16fa2":"code","f9c5a417":"code","4808f93c":"markdown","8e636a8c":"markdown","df9ad765":"markdown","42e51f02":"markdown","3338f8ae":"markdown","54017771":"markdown","e1e6e72e":"markdown","9d7de21f":"markdown","276c1982":"markdown","df6da04b":"markdown","cc90cc8a":"markdown"},"source":{"ad442a82":"! pip install ..\/input\/mmdetectionv260\/addict-2.4.0-py3-none-any.whl","3bef482a":"! pip install ..\/input\/mmdetectionv260\/mmcv_full-latesttorch1.6.0cu102-cp37-cp37m-manylinux1_x86_64.whl","d55d7d25":"! pip install ..\/input\/mmdetectionv260\/mmpycocotools-12.0.3-cp37-cp37m-linux_x86_64.whl","40435b9d":"! pip install ..\/input\/mmdetectionv260\/mmdet-2.6.0-py3-none-any.whl","b4cb66a5":"import copy\nimport os.path as osp\n\n# Check Pytorch installation\nimport torch, torchvision\nprint(torch.__version__, torch.cuda.is_available())\n\n# Check MMDetection installation\nimport mmdet\nprint(mmdet.__version__)\n\n# Check mmcv installation\nimport mmcv\nfrom mmcv.ops import get_compiling_cuda_version, get_compiler_version\nfrom mmcv import Config\nprint(get_compiling_cuda_version())\nprint(get_compiler_version())\n\nfrom mmdet.datasets import build_dataset, CocoDataset\nfrom mmdet.models import build_detector\nfrom mmdet.datasets.builder import DATASETS\nfrom mmdet.datasets.custom import CustomDataset\nfrom mmdet.apis import train_detector, set_random_seed, init_detector, inference_detector\n\nfrom pycocotools.coco import COCO\nfrom pycocotools.cocoeval import COCOeval\n\nimport nflimpact\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib.animation as animation\n\nfrom IPython.core.display import display, HTML","ef834ae2":"@DATASETS.register_module()\nclass ImpactDataset(CocoDataset):\n    CLASSES = ('Helmet')","1f67b446":"config_file = '..\/input\/nflweightutils\/configs.py'\ncfg = Config.fromfile(config_file)","8cff5052":"cfg.total_epochs = 1\ncfg.work_dir = '.\/'\ncfg.seed = 0\nset_random_seed(0, deterministic=False)\ncfg.gpu_ids = range(1)\ncfg.data_root = '..\/input\/nfl-impact-detection\/'\n\ncfg.data.train.ann_file='..\/input\/nflweightutils\/train.json'\ncfg.data.train.img_prefix= cfg.data_root + 'images\/'\n\ncfg.data.val.ann_file='..\/input\/nflweightutils\/valid.json',\ncfg.data.val.img_prefix= cfg.data_root + 'images\/'\n\ncfg.data.test.ann_file='..\/input\/nflweightutils\/valid.json', ## I just set validation data for test because it's not our main purpose\ncfg.data.test.img_prefix= cfg.data_root + 'images\/'\ncfg.model.pretrained = '..\/input\/nflweightutils\/resnet50-19c8e357.pth'","5a731810":"print(f'Config:\\n{cfg.pretty_text}')","b5b38bab":"# Build dataset\ndatasets = [build_dataset(cfg.data.train)]\n\n# Build the detector\nmodel = build_detector(\n    cfg.model, train_cfg=cfg.train_cfg, test_cfg=cfg.test_cfg)\n# Add an attribute for visualization convenience\nmodel.CLASSES = datasets[0].CLASSES\n\n# Create work_dir\nmmcv.mkdir_or_exist(osp.abspath(cfg.work_dir))\ntrain_detector(model, datasets, cfg, distributed=False, validate=True)","4ed16fa2":"fig = plt.figure()\n\n# Specify the path to model config and checkpoint file\ncheckpoint_file = '..\/input\/nflweightutils\/latest.pth' # 10 epochs\n\n# build the model from a config file and a checkpoint file\nmodel = init_detector(config_file, checkpoint_file, device='cuda:0')\n\n# test a video and show the results\nvideo = mmcv.VideoReader('..\/input\/nfl-impact-detection\/test\/57906_000718_Endzone.mp4')\n\nims = []\n\nfor frame in video:\n    result = inference_detector(model, frame)\n    single_img = model.show_result(frame, result, wait_time=1)\n    im = plt.imshow(single_img, animated=True)\n    ims.append([im])\n    \nani = animation.ArtistAnimation(fig, ims, interval=50, blit=True,\n                                repeat_delay=1000)","f9c5a417":"display(HTML(ani.to_html5_video()))","4808f93c":"# Display video\n\nSorry for the low resolution. please let's me know if there a way to improve video rendering resolution !","8e636a8c":"# Get Config\n\nI shared [NFL-weight-utils Dataset](https:\/\/www.kaggle.com\/jinssaa\/nflweightutils). there are files below.\n\n- configs.py : Default faster R-CNN configs. you can customize, please check [Official MMdetection configs](https:\/\/github.com\/open-mmlab\/mmdetection\/tree\/master\/configs)\n- train.json : train.json made by [Create COCO Anntations](https:\/\/www.kaggle.com\/jinssaa\/create-coco-anntations)\n- valid.json : valid.json made by [Create COCO Anntations](https:\/\/www.kaggle.com\/jinssaa\/create-coco-anntations)\n- resnet50-19c8e357.pth : Official pytorch resnet50 weight\n- latest.pth : pretrained faster R-CNN model trained by below configs, 10 epochs","df9ad765":"# Introduction\n\n**Main Topic**\n\nThis notebook is for **Detecting Helmet using [MMdetection](https:\/\/github.com\/open-mmlab\/mmdetection) without internet** using **images** and **image_labels.csv**\n\nBefore you start this Kernel you need to make COCO format train, val.json, you can check [Create COCO Anntations](https:\/\/www.kaggle.com\/jinssaa\/create-coco-anntations) nb or refer [mydataset](https:\/\/www.kaggle.com\/jinssaa\/nflweightutils).\n\n\n**References**\n\n**Chris Deotte Grand master** : [How to Install Without Internet](https:\/\/www.kaggle.com\/c\/severstal-steel-defect-detection\/discussion\/113195)\n\n[**MMdetection Official Documents**](https:\/\/mmdetection.readthedocs.io\/)\n","42e51f02":"Let's check configs","3338f8ae":"# Next Step\n\nI think this Competition is quite a hard competition. it's hard to find way to predict `impact`. So please share your idea :)\n\nI'will try method below\n\n1. Get `impact` frames from  `train` folder. I made [full frame train images](https:\/\/www.kaggle.com\/jinssaa\/nfl-frame)(14GB) from train *.mp4. take only `impact` images and build `impact` detection model.\n\n2. train `impact` detection model, and configure using rule-based way(ex. get intersection pred vs pred)\n\n3. **any ideas?**\n\nThank you! Happy Kaggle !\n","54017771":"# Set Dataset\n\nThis class is from [MMdetection COCO.py](https:\/\/github.com\/open-mmlab\/mmdetection\/blob\/master\/mmdet\/datasets\/coco.py)\n\n\nI just set `CLASSES` only `Helmet`, So you can customize if you want.\n","e1e6e72e":"# Install MMdetection from scratch\n\n**Version info.**\n\n- MMdetection 2.6.0\n- mmcv-full 1.2.0, torch 1.6, cu102\n\nBecause this Competetion is [Notebook Competetion](https:\/\/www.kaggle.com\/docs\/competitions#notebooks-only-FAQ), we need to inference .mp4 video without interent. \n\nSo I made `*.whl` files to install MMdetection, mmcv-full without internet. you can use this files from [mmdetection-v2.6.0 dataset](https:\/\/www.kaggle.com\/jinssaa\/mmdetectionv260).\n\n- note I think we don't need to `train` without internet so use local env if you want. this step for `inference` using weight.","9d7de21f":"# Test video\n\nLet's test video and visualize","276c1982":"You need to modify some configs below.","df6da04b":"# Train model\n\nWhen we submit our result, we need to install MMdetection without internet. so install MMdetection locally.\n\njust build datasets and train model. I just train 1 epoch in this cell because it's not our main purpose. You can use MMdetection in local envs","cc90cc8a":"# Set up environment"}}