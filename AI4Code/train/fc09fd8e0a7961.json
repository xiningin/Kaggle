{"cell_type":{"3912c59e":"code","ddd3a642":"code","0ba5e435":"code","60d85172":"code","433fa5d0":"code","58d65d42":"code","98d15d42":"code","c48e6d47":"code","5ec14f50":"code","6ad60b73":"code","1adee983":"code","f5d84caf":"code","d2d88078":"code","d19ee610":"code","052adb9c":"code","9abb0216":"code","a6dbf378":"code","f1798c9d":"code","f1036f18":"code","3316ceea":"code","dcc1991e":"code","c9342766":"code","cbc78ce4":"code","29441d6e":"code","837de7b6":"code","2e0c1692":"code","e65cc0e0":"code","b8d250f3":"code","86718e6a":"code","f07079db":"code","b53313ec":"code","becee43a":"code","13690cb3":"code","8e2bb59f":"markdown","8eb4fb23":"markdown","74f802e5":"markdown","b05cbd2a":"markdown","4542c79d":"markdown","e8886328":"markdown","2399e789":"markdown","dcaa2318":"markdown","d37defc3":"markdown","6f264f6f":"markdown","c0dfdbdb":"markdown","d43f265d":"markdown","2873dc90":"markdown","aba81243":"markdown","351592c3":"markdown","322fe4ef":"markdown","8d2584b0":"markdown","7b24d346":"markdown","e4fe0f7a":"markdown","9a85f7f6":"markdown","d5029d3a":"markdown","3da18f21":"markdown","55753c61":"markdown","48dbb08a":"markdown","0a53beb6":"markdown","42c22033":"markdown","91ece322":"markdown","bfa951e1":"markdown","740165e8":"markdown","8d7046f8":"markdown","fa0e4f90":"markdown","0ee9a3e9":"markdown","f3d924c9":"markdown","079d65f5":"markdown","8b32318f":"markdown","31799cac":"markdown","d2315026":"markdown","d2f301a7":"markdown","117ca87e":"markdown","b7cab9af":"markdown","c8bb3c52":"markdown","74e6f201":"markdown","4c4abfbb":"markdown","fbcf56ea":"markdown","cccf1261":"markdown","f4c113a2":"markdown","d66602a1":"markdown","fe175fa7":"markdown","2abbd7eb":"markdown","544b2adf":"markdown","9a65d00b":"markdown","33abb7f5":"markdown","7d7146a5":"markdown","89ae09af":"markdown"},"source":{"3912c59e":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","ddd3a642":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np \nimport math","0ba5e435":"pd.set_option('display.max_rows', 500)\npd.set_option('display.max_columns', 500)","60d85172":"\ndf1=pd.read_csv('..\/input\/bike-sharing-demand\/train.csv',parse_dates=['datetime'],index_col=0)\ndf_test=pd.read_csv('..\/input\/bike-sharing-demand\/test.csv',parse_dates=['datetime'],index_col=0)\ndf1.head(2)","433fa5d0":"\n#df1.info()\ndef add_feature(df1):\n    df1['year']=df1.index.year\n    df1['month']=df1.index.month\n    df1['dayofmonth']=df1.index.day\n    df1['dayofweek']=df1.index.dayofweek\n    df1['hour']=df1.index.hour","58d65d42":"add_feature(df1)\nadd_feature(df_test)\ndf1.head(2)","98d15d42":"df1.shape","c48e6d47":"df =df1.copy()","5ec14f50":"df = df.rename(columns={'count':'demand'})","6ad60b73":"df.reset_index(drop=True, inplace=True)\ndf = df.drop(['casual','registered'],axis=1)\ndf.head()","1adee983":"df.isnull().sum()","f5d84caf":"df.hist(figsize = (15,10))\nplt.tight_layout()\nplt.show()","d2d88078":"plt.figure(figsize=(15,10))\nplt.subplot(2,2,1,)\nplt.title('Temperature Vs Demand')\nplt.xlabel('Temperature')\nplt.ylabel('Demand')\nplt.scatter(df['temp'],df['demand'],s=2,c='g')\n\nplt.subplot(2,2,2)\nplt.title('Abs Temperature Vs Demand')\nplt.xlabel('Abs Temperature')\nplt.ylabel('Demand')\nplt.scatter(df['atemp'],df['demand'],s=2,c='b')\n\nplt.subplot(2,2,3)\nplt.title('Humidity Vs Demand')\nplt.xlabel('Humidity')\nplt.ylabel('Demand')\nplt.scatter(df['humidity'],df['demand'],c='r')\n\nplt.subplot(2,2,4)\nplt.title('Windspeed Vs Demand')\nplt.xlabel('Windspeed')\nplt.ylabel('Demand')\nplt.scatter(df['windspeed'],df['demand'],c='c')\n\nplt.tight_layout()\n\npass","d19ee610":"colors = ['g','r','m','b']\n\nplt.figure(figsize=(15,10))\nplt.subplot(3,3,1,)\nplt.title('Seasons Vs Demand')\nplt.xlabel('Seasons')\nplt.ylabel('Demand')\ncat_list=df['season'].unique()\ncat_average=df.groupby('season').mean()['demand']\nplt.bar(cat_list,cat_average,color=colors)\n\nplt.subplot(3,3,2,)\nplt.title('Holiday Vs Demand')\nplt.xlabel('Holiday')\nplt.ylabel('Demand')\ncat_list=df['holiday'].unique()\ncat_average=df.groupby('holiday').mean()['demand']\nplt.bar(cat_list,cat_average,color=colors)\n\nplt.subplot(3,3,3,)\nplt.title('Working Day Vs Demand')\nplt.xlabel('Working Day')\nplt.ylabel('Demand')\ncat_list=df['workingday'].unique()\ncat_average=df.groupby('workingday').mean()['demand']\nplt.bar(cat_list,cat_average,color=colors)\n\nplt.subplot(3,3,4,)\nplt.title('Weather Vs Demand')\nplt.xlabel('Weather')\nplt.ylabel('Demand')\ncat_list=df['weather'].unique()\ncat_average=df.groupby('weather').mean()['demand']\nplt.bar(cat_list,cat_average,color=colors)\n\nplt.subplot(3,3,5,)\nplt.title('Year Vs Demand')\nplt.xlabel('Year')\nplt.ylabel('Demand')\ncat_list=df['year'].unique()\ncat_average=df.groupby('year').mean()['demand']\nplt.bar(cat_list,cat_average,color=colors)\n\nplt.subplot(3,3,6,)\nplt.title('Month Vs Demand')\nplt.xlabel('Month')\nplt.ylabel('Demand')\ncat_list=df['month'].unique()\ncat_average=df.groupby('month').mean()['demand']\nplt.bar(cat_list,cat_average,color=colors)\n\nplt.subplot(3,3,7,)\nplt.title('Day Of Month Vs Demand')\nplt.xlabel('Day Of Month')\nplt.ylabel('Demand')\ncat_list=df['dayofmonth'].unique()\ncat_average=df.groupby('dayofmonth').mean()['demand']\nplt.bar(cat_list,cat_average,color=colors)\n\nplt.subplot(3,3,8,)\nplt.title('Day Of Week Vs Demand')\nplt.xlabel('Day Of Week')\nplt.ylabel('Demand')\ncat_list=df['dayofweek'].unique()\ncat_average=df.groupby('dayofweek').mean()['demand']\nplt.bar(cat_list,cat_average,color=colors)\n\nplt.subplot(3,3,9,)\nplt.title('Hour Vs Demand')\nplt.xlabel('Hour')\nplt.ylabel('Demand')\ncat_list=df['hour'].unique()\ncat_average=df.groupby('hour').mean()['demand']\nplt.bar(cat_list,cat_average,color=colors)\n\n\nplt.tight_layout()\npass","052adb9c":"df_prep = df.copy()\n#df_prep","9abb0216":"df_prep['demand'].describe()","a6dbf378":"df_prep['demand'].quantile([0.05,0.1,0.15,0.9,0.99])","f1798c9d":"correlation = df_prep[['temp','atemp','humidity','demand']].corr()\ncorrelation","f1036f18":"df_prep = df_prep.drop(['atemp','holiday','workingday','year','dayofmonth','dayofweek'],axis=1)\ndf_prep.head()","3316ceea":"df2 = pd.to_numeric(df_prep['demand'],downcast='float')\nplt.acorr(df2,maxlags=12)\npass","dcc1991e":"\nd1 = df_prep['demand']\nd2 = np.log(d1)\n\nplt.figure(figsize=(15,10))\nplt.subplot(1,2,1,)\nplt.title('Actual Demand Plot')\nplt.xlabel('Frequncy')\nplt.ylabel('Demand')\nd1.hist(rwidth=0.9)\n\nplt.subplot(1,2,2,)\nplt.title('Log Transform Demand Plot')\nplt.xlabel('Frequncy')\nplt.ylabel('Demand')\nd2.hist(rwidth=0.9)\n\npass","c9342766":"df_prep['demand'] = np.log(df_prep['demand'])\ndf_prep.head(2)","cbc78ce4":"t_1 = df_prep['demand'].shift(+1).to_frame()\nt_1.columns = ['t-1']\n\nt_2 = df_prep['demand'].shift(+2).to_frame()\nt_2.columns = ['t-2']\n\nt_3 = df_prep['demand'].shift(+3).to_frame()\nt_3.columns = ['t-3']","29441d6e":"df_prep_lag = pd.concat([df_prep,t_1,t_2,t_3],axis=1)\ndf_prep_lag = df_prep_lag.dropna()\ndf_prep_lag.head(2)","837de7b6":"df_prep_lag.dtypes","2e0c1692":"df_prep_lag['season'] = df_prep_lag['season'].astype('category')\ndf_prep_lag['weather'] = df_prep_lag['weather'].astype('category')\ndf_prep_lag['month'] = df_prep_lag['month'].astype('category')\ndf_prep_lag['hour'] = df_prep_lag['hour'].astype('category')\ndummy_df = pd.get_dummies(df_prep_lag,drop_first=True)\ndummy_df.head(2)\n#dummy_df.shape","e65cc0e0":"y = df_prep_lag[['demand']]\nX = df_prep_lag.drop(['demand'],axis=1)","b8d250f3":"# Creating the training set at 70%\ntr_size = 0.7*len(X)\ntr_size = int(tr_size)\n\nX_train = X.values[0:tr_size]\nX_test = X.values[tr_size:len(X)]\n\ny_train = y.values[0:tr_size]\ny_test = y.values[tr_size:len(y)]","86718e6a":"from sklearn.linear_model import LinearRegression \n\nstd_reg = LinearRegression()\nstd_reg.fit(X_train,y_train)","f07079db":"r2_train = std_reg.score(X_train,y_train)\nr2_test = std_reg.score(X_test,y_test)\nprint('R Suared Error for Train set:',r2_train)\nprint('R Suared Error for Test set:',r2_test)","b53313ec":"# Create Predictions \ny_predict = std_reg.predict(X_test)","becee43a":"from sklearn.metrics import mean_squared_error\nrmse = math.sqrt(mean_squared_error(y_test,y_predict))\nprint('RMSE of the model:',rmse)","13690cb3":"y_test_e = []\ny_predict_e = []\n\n\nfor i in range(0,len(y_test)):\n    y_test_e.append(math.exp(y_test[i]))\n    y_predict_e.append(math.exp(y_predict[i]))\n    \n# Calculate the sum\nlog_sq_sum = 0.0\n\nfor i in range(0,len(y_test_e)):\n    log_a = math.log(y_test_e[i] +1)\n    log_p = math.log(y_predict_e[i] +1)\n    log_diff = (log_p - log_a)**2\n    log_sq_sum = log_sq_sum + log_diff\n    \nrmsle = math.sqrt(log_sq_sum\/len(y_test))\nprint(rmsle)","8e2bb59f":"For making the dummies command to work we need the categorical data in object or category type.Let covert the type of data.","8eb4fb23":"We can see that there are no missing values in the dataset.So we can proceed further.","74f802e5":"From the above stats we can see that 50% of the values lie between demand value 42 to 284.Now we will analyse the outliers in the dataset.","b05cbd2a":"We can see that the demand remains same for all the fours seasons.So we can drop this feature while predicting the demand.The count(demand) is not normally distributed.We need to transform the count(demand) feature.","4542c79d":"# 7.Train Test Split","e8886328":"### Checking Outliers","2399e789":"# 8.Fit and Model Evaluation","dcaa2318":"# 4.Dropping Unwanted Features","d37defc3":"### Checking Multicollinearity","6f264f6f":"We can see that the features working day,holiday,day of month,day of week, dont have any affect on the bike demand.So we can drop these features from our prediction model.Also we have data for only two years.So the column of year can also be dropped from our dataset.From the Hour plot we can clearly make out that the demand for bikes is highest at 8 am and 5 pm.This is due to more demand during office going hours.","c0dfdbdb":"### Transforming Demand to Normal Distribution","d43f265d":"From the above plot we can see that there is autocorrelation in the demand. Auto correlation means the value of demand at time t is dependent on the value at time t-1 or t-2 ... The idependent variable demand has auto correlation and it has a log normal distribution.We need to fix this to improve our results.","2873dc90":"### Importing Dataset","aba81243":"From the above plot we can see that by doing a log transform on the demand we can convert it to a normal distribution curve.","351592c3":"# 9.Conclusion:\n\n1.We have worked with the time series data of bike rides.We have done exploratory data analysis and found out the affect continous and categorical data on Bike Demand.\n\n2.From the above plots we can see that temperature and Windspeed have an affect on Bike demand.Temperature and Absolute temperature plots are alsmost similar in nature.This indicated high colinearity between the two feature.We dropped one feature to avoid multi collinearity.\n\n3.We can see that the features working day,holiday,day of month,day of week, dont have any affect on the bike demand.So we can drop these features from our prediction model.Also we have data for only two years.So the column of year can also be dropped from our dataset.From the Hour plot we can clearly make out that the demand for bikes is highest at 8 am and 5 pm.This is due to more demand during office going hours.\n\n4.We have done a log transformation on the predicted variable Demand to make it into normal distribution.This helps in improving the accuracy of the model.Demand is autolinear and we have used values of t-1, t-2  and t-3 demand values to over come auto colinearity problem.\n\n5.We used a Linear regression model to predict the bike demand.We can see that we got good accuracy from the R square,RMSE and RMSLE values.","322fe4ef":"From the data we can say 5% of the time the demand was less than 5 and 1 % of the time the demand was more than 774.","8d2584b0":"### RMSLE ","7b24d346":"### Datetime Conversion","e4fe0f7a":"So we have dropped features like atemp,holiday,workingday,year,dayofmonth and dayofweek because they have no impact on the outcome of our prediction.","9a85f7f6":"### You can refer to my other notebooks from https:\/\/www.kaggle.com\/binuthomasphilip\/code","d5029d3a":"# 5.Creating and Modifying Features","3da18f21":"### Histogram","55753c61":"### Checking Missing Values","48dbb08a":"### Recently I published a self help book titled Inspiration: Thoughts on Spirituality, Technology, Wealth, Leadership and Motivation. The preview of the book can be read from the Amazon link https:\/\/lnkd.in\/gj7bMQA\n\n### You can refer to my other notebooks from https:\/\/www.kaggle.com\/binuthomasphilip\/code","0a53beb6":"### Demand into Log Values","42c22033":"So we have created dummy variables and avoided the dummy variable trap by removing the unwanted column of categorical data.","91ece322":"### Taking Care of Auto Correlation","bfa951e1":"Low value of RMSE tells us that our model is quite good.","740165e8":"From the above plots we can see that temperature and Windspeed Have an affect on Bike demand.\n\nTemperature and Absolute temperature plots are alsmost similar in nature.This indicated high colinearity between the two feature.We can drop one of the features to avoid the colinearity.","8d7046f8":"### RMSE","fa0e4f90":"# 6.Creating Dummy Variables","0ee9a3e9":"We have dropped the index, Datetime,Casual and Registered columns from our dataset.The column count is a sum of Casual and Registered,We will be predicting the column count so we habe dropped the columns casual and registered from our dataset.","f3d924c9":"The R square value for the test set is higher.Which tells us that there is no overfitting in the model.","079d65f5":"Today we live in era of Sharing Economy.The biggest company like Amazon,Airbnb,Uber,Swiggy etc dont own much physical assets.But they are using software to optimize the use of the assets available.In this kernel we will be covering following topics.\n\n### 1.Data Import and Preprocessing \n\n### 2.Exploratory Data Analysis \n\n### 3.Checking Multi Linear Regression Assumptions like\n\n      -Normality\n\n      -Linear Correlation\n\n      -Multicollinearity\n\n      -Autocorrelation\n\n      -Sample Size\n\n### 4.Drop Irrevalent features\n\n### 5.Creating and Modifying features\n\n### 6.Create Dummy Variables\n\n### 7.Train Test Split\n\n### 8.Fit and Score Model\n\n### 9.Present the Results","8b32318f":"### Checking Autocorrelation","31799cac":"### Vizualise the Continous features Vs Demand (Count)","d2315026":"### Dropping unwanted columns","d2f301a7":"### Renaming Columns ","117ca87e":"# 2.Exploratory Data Analysis ","b7cab9af":"Our demand data is a time series data.So the sequence of the data for features are important to predict demand.So while splitting the data we have maintained the sequence of the data.","c8bb3c52":"### Outliers Based On Percentile","74e6f201":"### Importing Python Modules","4c4abfbb":"### Concanating t_1,t-2,t-3 with Demand Dataframe","fbcf56ea":"# 1.Data Import and Preprocessing ","cccf1261":"# 3.Checking Multi Linear Regression Assumptions","f4c113a2":"### Making Copy of Dataset","d66602a1":"From the above table one can observe that we have created addition column of features t-1,t-2,t-3 to take care of autocorrelation of the demand feature This additional created features will be used to predict the demand.","fe175fa7":"We can see that there is a high correlation between temp and atemp.There is a neegative correlation between the demand and humidity.","2abbd7eb":"### Shape of Data ","544b2adf":"### Setting Pandas Display ","9a65d00b":"### Creating a Copy of Dataframe","33abb7f5":"### You can refer to my other notebooks from https:\/\/www.kaggle.com\/binuthomasphilip\/code","7d7146a5":"### Visualse the categorical features","89ae09af":"### R Squared Error "}}