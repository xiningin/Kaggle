{"cell_type":{"20893b98":"code","906cb77f":"code","e1c59e0a":"code","63c82a4a":"code","67142529":"code","700fb00c":"code","bd94ad37":"code","56c2f9da":"code","fd4f911c":"code","bae5610a":"code","970f3133":"code","97e3ec97":"code","914c1b83":"code","02c16ff6":"code","cf7fbc7a":"code","7f80edb0":"code","4a52db34":"code","ec317699":"markdown","a57e9c1f":"markdown","31dc346c":"markdown","b62bfbd6":"markdown","fddc166f":"markdown","631025be":"markdown","a22534f1":"markdown","469afff1":"markdown","547e5891":"markdown","de3ea949":"markdown","ce2277f1":"markdown","9b170c82":"markdown","bb938dbd":"markdown","b56bae7a":"markdown","923d4b8e":"markdown","116392fe":"markdown","2f481284":"markdown","65c3aa22":"markdown"},"source":{"20893b98":"%matplotlib inline\n\n# Use pathlib, it's better that os\nfrom pathlib import Path\n\nimport numpy as np\nimport pandas as pd\npd.set_option('display.max_columns', None)\n\n# For visualization\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set_style('whitegrid')\n\n# For Bayesian Optimization\nfrom bayes_opt import BayesianOptimization\n\n# Basic metrics and functions\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.model_selection import StratifiedKFold, cross_val_score\n\n# Lightgbm for model\nimport lightgbm as lgb\n\nimport warnings\nwarnings.simplefilter('ignore', category=FutureWarning)","906cb77f":"PATH = Path('..\/input')\n[f.name for f in PATH.iterdir()]","e1c59e0a":"train_df = pd.read_csv(PATH \/ 'flight_delays_train.csv')\nprint(train_df.shape)\ntrain_df.head()","63c82a4a":"test_df = pd.read_csv(PATH \/ 'flight_delays_test.csv')\nprint(test_df.shape)\ntest_df.head()","67142529":"train_df['dep_delayed_15min'].value_counts(normalize=True)","700fb00c":"# Hour and minute\ntrain_df['hour'] = train_df['DepTime'] \/\/ 100\ntrain_df.loc[train_df['hour'] == 24, 'hour'] = 0\ntrain_df.loc[train_df['hour'] == 25, 'hour'] = 1\ntrain_df['minute'] = train_df['DepTime'] % 100\n\ntest_df['hour'] = test_df['DepTime'] \/\/ 100\ntest_df.loc[test_df['hour'] == 24, 'hour'] = 0\ntest_df.loc[test_df['hour'] == 25, 'hour'] = 1\ntest_df['minute'] = test_df['DepTime'] % 100\n\n# Season\ntrain_df['summer'] = (train_df['Month'].isin([6, 7, 8])).astype(np.int32)\ntrain_df['autumn'] = (train_df['Month'].isin([9, 10, 11])).astype(np.int32)\ntrain_df['winter'] = (train_df['Month'].isin([12, 1, 2])).astype(np.int32)\ntrain_df['spring'] = (train_df['Month'].isin([3, 4, 5])).astype(np.int32)\n\ntest_df['summer'] = (test_df['Month'].isin([6, 7, 8])).astype(np.int32)\ntest_df['autumn'] = (test_df['Month'].isin([9, 10, 11])).astype(np.int32)\ntest_df['winter'] = (test_df['Month'].isin([12, 1, 2])).astype(np.int32)\ntest_df['spring'] = (test_df['Month'].isin([3, 4, 5])).astype(np.int32)\n\n# Daytime\ntrain_df['daytime'] = pd.cut(train_df['hour'], bins=[0, 6, 12, 18, 23], include_lowest=True)\ntest_df['daytime'] = pd.cut(test_df['hour'], bins=[0, 6, 12, 18, 23], include_lowest=True)\n\n# Extract the labels\ntrain_y = train_df.pop('dep_delayed_15min')\ntrain_y = train_y.map({'N': 0, 'Y': 1})\n\n# Concatenate for preprocessing\ntrain_split = train_df.shape[0]\nfull_df = pd.concat((train_df, test_df))\nfull_df['Distance'] = np.log(full_df['Distance'])","bd94ad37":"# String to numerical\nfor col in ['Month', 'DayofMonth', 'DayOfWeek']:\n    full_df[col] = full_df[col].apply(\n        lambda x: x.split('-')[1]).astype(np.int32) - 1\n\n# Label Encoding\nfor col in ['Origin', 'Dest', 'UniqueCarrier', 'daytime']:\n    full_df[col] = pd.factorize(full_df[col])[0]\n\n# Categorical columns\ncat_cols = ['Month', 'DayofMonth', 'DayOfWeek', 'Origin', 'Dest',\n            'UniqueCarrier', 'hour', 'summer', 'autumn', 'winter', 'spring', 'daytime']\n\n# Converting categorical columns to type 'category' as required by LGBM\nfor c in cat_cols:\n    full_df[c] = full_df[c].astype('category')\n\n# Split into train and test\ntrain_df, test_df = full_df.iloc[:train_split], full_df.iloc[train_split:]\ntrain_df.shape, train_y.shape, test_df.shape","56c2f9da":"def cross_val_scheme(model, train_df, train_y, cv):\n    cv_scores = cross_val_score(\n        model, train_df, train_y, scoring='roc_auc', cv=cv, n_jobs=-1)\n    print(f'CV Scores: {cv_scores}')\n    print(f'CV mean: {cv_scores.mean()} \\t CV Std: {cv_scores.std()}')\n    model.fit(train_df, train_y)\n    feat_imp = pd.DataFrame({'col': full_df.columns.values, 'imp': model.feature_importances_}).sort_values(\n        by='imp', ascending=False)\n    return cv_scores, feat_imp","fd4f911c":"skf = StratifiedKFold(n_splits=5, random_state=7, shuffle=True)\nclf = lgb.LGBMClassifier(random_state=7)\ncv_scores, feat_imp = cross_val_scheme(clf, train_df, train_y, skf)\nplt.figure(figsize=(8, 10))\nsns.barplot(x='imp', y='col', data=feat_imp[1:], orient='h')","bae5610a":"bay_tr_ix, bay_val_ix = list(StratifiedKFold(\n    n_splits=2, shuffle=True, random_state=7).split(train_df, train_y))[0]","970f3133":"def LGB_bayesian(num_leaves, min_data_in_leaf, learning_rate, min_sum_hessian_in_leaf, feature_fraction, lambda_l1,\n                 lambda_l2, min_gain_to_split, max_depth):\n\n    num_leaves = int(np.round(num_leaves))\n    min_data_in_leaf = int(np.round(min_data_in_leaf))\n    max_depth = int(np.round(max_depth))\n\n    params = {\n        'feature_fraction': feature_fraction,\n        'lambda_l1': lambda_l1,\n        'lambda_l2': lambda_l2,\n        'learning_rate': learning_rate,\n        'max_depth': max_depth,\n        'min_data_in_leaf': min_data_in_leaf,\n        'min_gain_to_split': min_gain_to_split,\n        'min_sum_hessian_in_leaf': min_sum_hessian_in_leaf,\n        'num_leaves': num_leaves,\n        'max_bin': 255,\n        'bagging_fraction': 0.8,\n        'bagging_freq': 6,\n        'save_binary': True,\n        'seed': 7,\n        'feature_fraction_seed': 7,\n        'bagging_seed': 7,\n        'drop_seed': 7,\n        'data_random_seed': 7,\n        'objective': 'binary',\n        'boosting_type': 'gbdt',\n        'verbose': 1,\n        'metric': 'auc',\n        'is_unbalance': True,\n        'boost_from_average': True,\n        'n_jobs': -1\n    }\n\n    lgb_train = lgb.Dataset(train_df.loc[bay_tr_ix], train_y.loc[\n                            bay_tr_ix], free_raw_data=False)\n    lgb_valid = lgb.Dataset(train_df.loc[bay_val_ix], train_y.loc[\n                            bay_val_ix], free_raw_data=False)\n\n    num_rounds = 5000\n    clf = lgb.train(params, lgb_train, num_rounds, valid_sets=[\n                    lgb_train, lgb_valid], verbose_eval=250, early_stopping_rounds=50)\n    val_preds = clf.predict(\n        train_df.loc[bay_val_ix], num_iterations=clf.best_iteration)\n\n    score = roc_auc_score(train_y.loc[bay_val_ix], val_preds)\n    return score","97e3ec97":"bounds_lgb = {\n    'feature_fraction': (0.5, 1),\n    'lambda_l1': (0., 10.),\n    'lambda_l2': (0., 10.),\n    'learning_rate': (0.01, 0.1),\n    'max_depth': (2, 6),\n    'min_data_in_leaf': (5, 30),\n    'min_gain_to_split': (0, 1),\n    'min_sum_hessian_in_leaf': (0.01, 1),\n    'num_leaves': (10, 30)\n}\n\nLGB_BO = BayesianOptimization(LGB_bayesian, bounds_lgb, random_state=7)","914c1b83":"with warnings.catch_warnings():\n    warnings.filterwarnings('ignore')\n    LGB_BO.maximize(init_points=10, n_iter=10, acq='ucb')\n\n\nLGB_BO.max['target'], LGB_BO.max['params']","02c16ff6":"def test_kfold(params, train_df, train_y, test_df, cv):\n    test_preds = 0.\n    valid_preds = np.zeros(train_y.shape)\n\n    for fold, (train_ix, valid_ix) in enumerate(cv.split(train_df, train_y)):\n        print(f\"\\nFOLD: {fold+1} {'='*50}\")\n        X_train, X_valid = train_df.iloc[train_ix], train_df.iloc[valid_ix]\n        y_train, y_valid = train_y.iloc[train_ix], train_y.iloc[valid_ix]\n\n        lgb_train = lgb.Dataset(X_train, y_train, free_raw_data=False)\n        lgb_valid = lgb.Dataset(X_valid, y_valid, free_raw_data=False)\n\n        clf = lgb.train(params, lgb_train, 5000, valid_sets=[\n                        lgb_train, lgb_valid], verbose_eval=250, early_stopping_rounds=50)\n        valid_preds[valid_ix] = clf.predict(\n            train_df.iloc[valid_ix], num_iterations=clf.best_iteration)\n        test_preds += clf.predict(test_df, num_iterations=clf.best_iteration)\n\n    print(f'Valid CV: {roc_auc_score(train_y, valid_preds)}')\n    test_preds \/= cv.n_splits\n\n    return test_preds","cf7fbc7a":"params = {\n    'feature_fraction': LGB_BO.max['params']['feature_fraction'],\n    'lambda_l1': LGB_BO.max['params']['lambda_l1'],\n    'lambda_l2': LGB_BO.max['params']['lambda_l2'],\n    'learning_rate': LGB_BO.max['params']['learning_rate'],\n    'max_depth': int(np.round(LGB_BO.max['params']['max_depth'])),\n    'min_data_in_leaf': int(np.round(LGB_BO.max['params']['min_data_in_leaf'])),\n    'min_gain_to_split': LGB_BO.max['params']['min_gain_to_split'],\n    'min_sum_hessian_in_leaf': LGB_BO.max['params']['min_sum_hessian_in_leaf'],\n    'num_leaves': int(np.round(LGB_BO.max['params']['num_leaves'])),\n    'max_bin': 255,\n    'bagging_fraction': 0.8,\n    'bagging_freq': 3,\n    'save_binary': True,\n    'seed': 7,\n    'feature_fraction_seed': 7,\n    'bagging_seed': 7,\n    'drop_seed': 7,\n    'data_random_seed': 7,\n    'objective': 'binary',\n    'boosting_type': 'gbdt',\n    'verbose': 1,\n    'metric': 'auc',\n    'is_unbalance': True,\n    'boost_from_average': True,\n    'n_jobs': -1\n}","7f80edb0":"with warnings.catch_warnings():\n    warnings.filterwarnings('ignore')\n    test_preds = test_kfold(params, train_df, train_y, test_df, StratifiedKFold(\n        n_splits=5, random_state=7, shuffle=True))","4a52db34":"final_df = pd.DataFrame(\n    {'id': range(test_preds.shape[0]), 'dep_delayed_15min': test_preds})\nfinal_df.to_csv('lightgbm_5fold_sub_29.csv', header=True, index=False)\npd.read_csv('lightgbm_5fold_sub_29.csv').head()","ec317699":"## Training and testing","a57e9c1f":"#### Get some train and valid indexes","31dc346c":"## Testing","b62bfbd6":"## Load the data","fddc166f":"## Bayesian optimization\nFor the sake of kernel let's look at some theory and visualizations to understand how bayesian-optimization works. <br>\n\nBayesian Optimization uses some kind of approximation. Consider, if you don\u2019t know a function, what you usually do? Of course, we will try to guess or approximate it with some know prior knowledge. The same idea is behind the **posterior probability**. The criteria here is that we have observations, where the data are coming records by records (online-learning), so we need to train this model. The trained model will obviously obey a function. That function that we don\u2019t know will absolutely depend on the learned data. So our task is to find the hyper-parameters that maximizes the learning schema.\n\n<img src=\"https:\/\/cdn-images-1.medium.com\/max\/1600\/1*PhKGj_bZlND8IEfII426wA.png\"><\/img>","631025be":"Remember to use the optimized hyperparameters only.","a22534f1":"Let's see the target distribution first","469afff1":"If (you want to go deep into the algorithm and learn the math behind it):<br>\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;You can check the following blogs <br>\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;- [Bayesian Optimization](http:\/\/krasserm.github.io\/2018\/03\/21\/bayesian-optimization\/) <br>\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;- [Shallow Understanding on Bayesian Optimization](https:\/\/towardsdatascience.com\/shallow-understanding-on-bayesian-optimization-324b6c1f7083)\n\nElse (if you want some practical guide): <br>\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;follow up!!","547e5891":"## Modeling\nLet's create a baseline by creating a basic LightGBM model and training it on the data. For that, we need a cross-validation scheme. This is a simple one but pretty enough for this kernel.","de3ea949":"Bayesian optimization needs a function which returns the metric score. Optimal hyperparameters returned by the bayesian-optimization algorithm based on the previous score are passed in this function and we need to create train a new model with these hyperparameters to get a new validation metric score. Then this score is again passed in the algorithm and new hyperparameters are generated until we perform the specified iterations.","ce2277f1":"Also, we need to define the bounds under which we need to perform hyperparameter search for each hyperparameter. In the end, we just need to pass our function and bounds along with random_state if you want to set the seed to reproducibility.","9b170c82":"## Let's load some libraries","bb938dbd":"## Creating new features","b56bae7a":"Remember that this BayesianOptimization library maximizes the score which is not the case if you use hyperopt, so in the function above return the value, you want to maximize. Here we need to maximize the ROC_AUC score so we just return the score and if you use some other metric such as MSE or MAE you need to return the negative of the metric score.\n\n**Let's find our parameters!!**","923d4b8e":"## Submission","116392fe":"Basic preprocessing before we feed the data to our model because we need numerical data but still most of our data contain non-numerical values.","2f481284":"# Bayesian Optimization\n\nHyperparameter tuning is the most essential part of training a machine-learning model. Although it comes after feature-engineering, good hyperparameters can you a better score because all we need is a robust model which can generalize our data. With an increase in data and hyperparameters it's difficult to use a basic brute force technique like **GridSearch** which iterates over all the possible hyperparameter combinations and train a new model every time. Instead of **GridSearch**, we can use **RandomSearch** which runs for some defined iterations and chooses hyperparameter combination randomly each time. <br>\n\nTo tackle this part of training a model **Bayesian Optimization** comes to rescue. In this kernel, we will go through all you need to perform Bayesian optimization along with [LightGBM](https:\/\/lightgbm.readthedocs.io\/en\/latest\/). Don't worry you don't need to implement the whole optimization algorithm yourself, an open-source library such as [hyperopt](https:\/\/github.com\/hyperopt\/hyperopt) and [BayesianOptimization\n](https:\/\/github.com\/fmfn\/BayesianOptimization) can help us with that.\n\nFor this tutorial, we will be using [BayesianOptimization](https:\/\/github.com\/fmfn\/BayesianOptimization).","65c3aa22":"We perform K_fold cross validation and K_fold blending to predict on our test data."}}