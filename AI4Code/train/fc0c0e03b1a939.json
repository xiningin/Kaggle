{"cell_type":{"ae096a9b":"code","c1666b86":"code","ed65dafe":"code","0ec3acf2":"code","b4f119e7":"code","dfc4d644":"code","31ff4b0e":"code","f09e9ff0":"code","26cb1b56":"code","46bae70a":"code","9ae4f831":"code","5932933b":"code","b10fbfc0":"code","854e6af6":"code","933786ef":"code","a948572d":"code","5ba00022":"code","e1c25db9":"code","76e66834":"code","05be6ec5":"code","2a4d04f3":"code","8c1621a7":"code","f795e9e0":"code","22062f43":"code","d259beec":"code","b05946a4":"code","a9015742":"code","438b0a0f":"code","c6598b63":"code","0dcb029b":"code","3f5e9529":"code","96d1c4e7":"code","98b030d2":"code","7d42fad0":"code","d7dc81c7":"code","9a2ed998":"code","b266833e":"code","7b368eba":"code","db5ba024":"code","b454870e":"code","f773f4db":"code","fc7d707c":"code","bd239ce8":"code","e0dc0f95":"code","5e2a847d":"markdown","340d834a":"markdown","49d23623":"markdown","d716a707":"markdown","4eb8ae6c":"markdown","de01506f":"markdown","58d15972":"markdown","9c98d7a3":"markdown","4fad92dd":"markdown","f071c259":"markdown","322560bb":"markdown","4852c02e":"markdown","6af1dfb0":"markdown","f1eecb49":"markdown","7ab3c46b":"markdown","cbbc394d":"markdown"},"source":{"ae096a9b":"import pandas as pd\nimport numpy as np\nimport re\nimport os\nfrom IPython.display import HTML\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.feature_extraction import text \nfrom sklearn.decomposition import PCA\n\nfrom tensorflow.python.keras.models import Sequential, load_model\nfrom tensorflow.python.keras.layers import Dense, Dropout\nfrom tensorflow.python.keras import optimizers\n\nimport nltk\nfrom nltk.stem.porter import PorterStemmer\nfrom nltk.stem.snowball import SnowballStemmer\nfrom nltk.corpus import words\nfrom nltk.corpus import wordnet \nallEnglishWords = words.words() + [w for w in wordnet.words()]\nallEnglishWords = np.unique([x.lower() for x in allEnglishWords])\n\nimport plotly.offline as py\nimport plotly.graph_objs as go\npy.init_notebook_mode(connected=True)\n\nimport warnings\nwarnings.filterwarnings('ignore')","c1666b86":"path = \"\/kaggle\/input\/aclimdb\/aclImdb\/\"\npositiveFiles = [x for x in os.listdir(path+\"train\/pos\/\") if x.endswith(\".txt\")]\nnegativeFiles = [x for x in os.listdir(path+\"train\/neg\/\") if x.endswith(\".txt\")]\ntestFiles = [x for x in os.listdir(path+\"test\/\") if x.endswith(\".txt\")]","ed65dafe":"positiveReviews, negativeReviews, testReviews = [], [], []\nfor pfile in positiveFiles:\n    with open(path+\"train\/pos\/\"+pfile, encoding=\"latin1\") as f:\n        positiveReviews.append(f.read())\nfor nfile in negativeFiles:\n    with open(path+\"train\/neg\/\"+nfile, encoding=\"latin1\") as f:\n        negativeReviews.append(f.read())\nfor tfile in testFiles:\n    with open(path+\"test\/\"+tfile, encoding=\"latin1\") as f:\n        testReviews.append(f.read())","0ec3acf2":"reviews = pd.concat([\n    pd.DataFrame({\"review\":positiveReviews, \"label\":1, \"file\":positiveFiles}),\n    pd.DataFrame({\"review\":negativeReviews, \"label\":0, \"file\":negativeFiles}),\n    pd.DataFrame({\"review\":testReviews, \"label\":-1, \"file\":testFiles})\n], ignore_index=True).sample(frac=1, random_state=1)\nreviews.head()","b4f119e7":"reviews = reviews[[\"review\", \"label\", \"file\"]].sample(frac=1, random_state=1)\ntrain = reviews[reviews.label!=-1].sample(frac=0.6, random_state=1)\nvalid = reviews[reviews.label!=-1].drop(train.index)\ntest = reviews[reviews.label==-1]","dfc4d644":"print(train.shape)\nprint(valid.shape)\nprint(test.shape)","31ff4b0e":"HTML(train.review.iloc[0])","f09e9ff0":"class Preprocessor(object):\n    ''' Preprocess data for NLP tasks. '''\n\n    def __init__(self, alpha=True, lower=True, stemmer=True, english=False):\n        self.alpha = alpha\n        self.lower = lower\n        self.stemmer = stemmer\n        self.english = english\n        \n        self.uniqueWords = None\n        self.uniqueStems = None\n        \n    def fit(self, texts):\n        texts = self._doAlways(texts)\n\n        allwords = pd.DataFrame({\"word\": np.concatenate(texts.apply(lambda x: x.split()).values)})\n        self.uniqueWords = allwords.groupby([\"word\"]).size().rename(\"count\").reset_index()\n        self.uniqueWords = self.uniqueWords[self.uniqueWords[\"count\"]>1]\n        if self.stemmer:\n            self.uniqueWords[\"stem\"] = self.uniqueWords.word.apply(lambda x: PorterStemmer().stem(x)).values\n            self.uniqueWords.sort_values([\"stem\", \"count\"], inplace=True, ascending=False)\n            self.uniqueStems = self.uniqueWords.groupby(\"stem\").first()\n        \n        #if self.english: self.words[\"english\"] = np.in1d(self.words[\"mode\"], allEnglishWords)\n        print(\"Fitted.\")\n            \n    def transform(self, texts):\n        texts = self._doAlways(texts)\n        if self.stemmer:\n            allwords = np.concatenate(texts.apply(lambda x: x.split()).values)\n            uniqueWords = pd.DataFrame(index=np.unique(allwords))\n            uniqueWords[\"stem\"] = pd.Series(uniqueWords.index).apply(lambda x: PorterStemmer().stem(x)).values\n            uniqueWords[\"mode\"] = uniqueWords.stem.apply(lambda x: self.uniqueStems.loc[x, \"word\"] if x in self.uniqueStems.index else \"\")\n            texts = texts.apply(lambda x: \" \".join([uniqueWords.loc[y, \"mode\"] for y in x.split()]))\n        #if self.english: texts = self.words.apply(lambda x: \" \".join([y for y in x.split() if self.words.loc[y,\"english\"]]))\n        print(\"Transformed.\")\n        return(texts)\n\n    def fit_transform(self, texts):\n        texts = self._doAlways(texts)\n        self.fit(texts)\n        texts = self.transform(texts)\n        return(texts)\n    \n    def _doAlways(self, texts):\n        # Remove parts between <>'s\n        texts = texts.apply(lambda x: re.sub('<.*?>', ' ', x))\n        # Keep letters and digits only.\n        if self.alpha: texts = texts.apply(lambda x: re.sub('[^a-zA-Z0-9 ]+', ' ', x))\n        # Set everything to lower case\n        if self.lower: texts = texts.apply(lambda x: x.lower())\n        return texts  ","26cb1b56":"train.head()","46bae70a":"preprocess = Preprocessor(alpha=True, lower=True, stemmer=True)","9ae4f831":"%%time\ntrainX = preprocess.fit_transform(train.review)\nvalidX = preprocess.transform(valid.review)","5932933b":"trainX.head()","b10fbfc0":"print(preprocess.uniqueWords.shape)\npreprocess.uniqueWords[preprocess.uniqueWords.word.str.contains(\"disappoint\")]","854e6af6":"print(preprocess.uniqueStems.shape)\npreprocess.uniqueStems[preprocess.uniqueStems.word.str.contains(\"disappoint\")]","933786ef":"stop_words = text.ENGLISH_STOP_WORDS.union([\"thats\",\"weve\",\"dont\",\"lets\",\"youre\",\"im\",\"thi\",\"ha\",\n    \"wa\",\"st\",\"ask\",\"want\",\"like\",\"thank\",\"know\",\"susan\",\"ryan\",\"say\",\"got\",\"ought\",\"ive\",\"theyre\"])\ntfidf = TfidfVectorizer(min_df=2, max_features=10000, stop_words=stop_words) #, ngram_range=(1,3)","a948572d":"%%time\ntrainX = tfidf.fit_transform(trainX).toarray()\nvalidX = tfidf.transform(validX).toarray()","5ba00022":"print(trainX.shape)\nprint(validX.shape)","e1c25db9":"trainY = train.label\nvalidY = valid.label","76e66834":"print(trainX.shape, trainY.shape)\nprint(validX.shape, validY.shape)","05be6ec5":"from scipy.stats.stats import pearsonr","2a4d04f3":"getCorrelation = np.vectorize(lambda x: pearsonr(trainX[:,x], trainY)[0])\ncorrelations = getCorrelation(np.arange(trainX.shape[1]))\nprint(correlations)","8c1621a7":"allIndeces = np.argsort(-correlations)\nbestIndeces = allIndeces[np.concatenate([np.arange(1000), np.arange(-1000, 0)])]","f795e9e0":"vocabulary = np.array(tfidf.get_feature_names())\nprint(vocabulary[bestIndeces][:10])\nprint(vocabulary[bestIndeces][-10:])","22062f43":"trainX = trainX[:,bestIndeces]\nvalidX = validX[:,bestIndeces]","d259beec":"print(trainX.shape, trainY.shape)\nprint(validX.shape, validY.shape)","b05946a4":"DROPOUT = 0.5\nACTIVATION = \"tanh\"\n\nmodel = Sequential([    \n    Dense(int(trainX.shape[1]\/2), activation=ACTIVATION, input_dim=trainX.shape[1]),\n    Dropout(DROPOUT),\n    Dense(int(trainX.shape[1]\/2), activation=ACTIVATION, input_dim=trainX.shape[1]),\n    Dropout(DROPOUT),\n    Dense(int(trainX.shape[1]\/4), activation=ACTIVATION),\n    Dropout(DROPOUT),\n    Dense(100, activation=ACTIVATION),\n    Dropout(DROPOUT),\n    Dense(20, activation=ACTIVATION),\n    Dropout(DROPOUT),\n    Dense(5, activation=ACTIVATION),\n    Dropout(DROPOUT),\n    Dense(1, activation='sigmoid'),\n])","a9015742":"model.compile(optimizer=optimizers.Adam(0.00005), loss='binary_crossentropy', metrics=['accuracy'])\nmodel.summary()","438b0a0f":"EPOCHS = 30\nBATCHSIZE = 1500","c6598b63":"model.fit(trainX, trainY, epochs=EPOCHS, batch_size=BATCHSIZE, validation_data=(validX, validY))","0dcb029b":"x = np.arange(EPOCHS)\nhistory = model.history.history\n\ndata = [\n    go.Scatter(x=x, y=history[\"acc\"], name=\"Train Accuracy\", marker=dict(size=5), yaxis='y2'),\n    go.Scatter(x=x, y=history[\"val_acc\"], name=\"Valid Accuracy\", marker=dict(size=5), yaxis='y2'),\n    go.Scatter(x=x, y=history[\"loss\"], name=\"Train Loss\", marker=dict(size=5)),\n    go.Scatter(x=x, y=history[\"val_loss\"], name=\"Valid Loss\", marker=dict(size=5))\n]\nlayout = go.Layout(\n    title=\"Model Training Evolution\", font=dict(family='Palatino'), xaxis=dict(title='Epoch', dtick=1),\n    yaxis1=dict(title=\"Loss\", domain=[0, 0.45]), yaxis2=dict(title=\"Accuracy\", domain=[0.55, 1]),\n)\npy.iplot(go.Figure(data=data, layout=layout), show_link=False)","3f5e9529":"train[\"probability\"] = model.predict(trainX)\ntrain[\"prediction\"] = train.probability-0.5>0\ntrain[\"truth\"] = train.label==1\ntrain.tail()","96d1c4e7":"print(model.evaluate(trainX, trainY))\nprint((train.truth==train.prediction).mean())","98b030d2":"valid[\"probability\"] = model.predict(validX)\nvalid[\"prediction\"] = valid.probability-0.5>0\nvalid[\"truth\"] = valid.label==1\nvalid.tail()","7d42fad0":"print(model.evaluate(validX, validY))\nprint((valid.truth==valid.prediction).mean())","d7dc81c7":"trainCross = train.groupby([\"prediction\", \"truth\"]).size().unstack()\ntrainCross","9a2ed998":"validCross = valid.groupby([\"prediction\", \"truth\"]).size().unstack()\nvalidCross","b266833e":"truepositives = valid[(valid.truth==True)&(valid.truth==valid.prediction)]\nprint(len(truepositives), \"true positives.\")\ntruepositives.sort_values(\"probability\", ascending=False).head(3)","7b368eba":"truenegatives = valid[(valid.truth==False)&(valid.truth==valid.prediction)]\nprint(len(truenegatives), \"true negatives.\")\ntruenegatives.sort_values(\"probability\", ascending=True).head(3)","db5ba024":"falsepositives = valid[(valid.truth==True)&(valid.truth!=valid.prediction)]\nprint(len(falsepositives), \"false positives.\")\nfalsepositives.sort_values(\"probability\", ascending=True).head(3)","b454870e":"falsenegatives = valid[(valid.truth==False)&(valid.truth!=valid.prediction)]\nprint(len(falsenegatives), \"false negatives.\")\nfalsenegatives.sort_values(\"probability\", ascending=False).head(3)","f773f4db":"HTML(valid.loc[22148].review)","fc7d707c":"unseen = pd.Series(\"this movie very good\")","bd239ce8":"unseen = preprocess.transform(unseen)       # Text preprocessing\nunseen = tfidf.transform(unseen).toarray()  # Feature engineering\nunseen = unseen[:,bestIndeces]              # Feature selection\nprobability = model.predict(unseen)[0,0]  # Network feedforward","e0dc0f95":"print(probability)\nprint(\"Positive!\") if probability > 0.5 else print(\"Negative!\")","5e2a847d":"---\n\n## Feature Engineering\nNext, we take the preprocessed texts as input and calculate their TF-IDF's ([info](http:\/\/www.tfidf.com)). We retain 10000 features per text.","340d834a":"---\n\n## Model Application","49d23623":"### Accuracy & Loss\nLet's first centralize the probabilities and predictions with the original train and validation dataframes. Then we can print out the respective accuracies and losses.","d716a707":"### Error Analysis\nError analysis gives us great insight in the way the model is making its errors. Often, it shows data quality issues.","4eb8ae6c":"---\n\n## Model Architecture\nWe choose a very simple dense network with 6 layers, performing binary classification.","de01506f":"### Custom Reviews\nTo use this model, we would store the model, along with the preprocessing vectorizers, and run the unseen texts through following pipeline.","58d15972":"This is the review that got predicted as positive most certainly - while being labeled as negative. However, we can easily recognize it as a poorly labeled sample.","9c98d7a3":"---\n\n## Model Training\nLet's go.","4fad92dd":"---\n\n## Feature Selection\nNext, we take the 10k dimensional tfidf's as input, and keep the 2000 dimensions that correlate the most with our sentiment target. The corresponding words - see below - make sense.","f071c259":"## Data Import\nFirst, we need to import the data.","322560bb":"With everything centralized in 1 dataframe, we now perform train, validation and test set splits.","4852c02e":"# Sentiment Analysis on Movie Reviews","6af1dfb0":"<h1>Content<span class=\"tocSkip\"><\/span><\/h1>\n<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Data-Import\" data-toc-modified-id=\"Data-Import-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;<\/span>Data Import<\/a><\/span><\/li><li><span><a href=\"#Data-Preprocessing\" data-toc-modified-id=\"Data-Preprocessing-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;<\/span>Data Preprocessing<\/a><\/span><\/li><li><span><a href=\"#Feature-Engineering\" data-toc-modified-id=\"Feature-Engineering-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;<\/span>Feature Engineering<\/a><\/span><\/li><li><span><a href=\"#Feature-Selection\" data-toc-modified-id=\"Feature-Selection-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;<\/span>Feature Selection<\/a><\/span><\/li><li><span><a href=\"#Model-Architecture\" data-toc-modified-id=\"Model-Architecture-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;<\/span>Model Architecture<\/a><\/span><\/li><li><span><a href=\"#Model-Training\" data-toc-modified-id=\"Model-Training-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;<\/span>Model Training<\/a><\/span><\/li><li><span><a href=\"#Model-Evaluation\" data-toc-modified-id=\"Model-Evaluation-7\"><span class=\"toc-item-num\">7&nbsp;&nbsp;<\/span>Model Evaluation<\/a><\/span><ul class=\"toc-item\"><li><span><a href=\"#Accuracy-&amp;-Loss\" data-toc-modified-id=\"Accuracy-&amp;-Loss-7.1\"><span class=\"toc-item-num\">7.1&nbsp;&nbsp;<\/span>Accuracy &amp; Loss<\/a><\/span><\/li><li><span><a href=\"#Error-Analysis\" data-toc-modified-id=\"Error-Analysis-7.2\"><span class=\"toc-item-num\">7.2&nbsp;&nbsp;<\/span>Error Analysis<\/a><\/span><\/li><\/ul><\/li><li><span><a href=\"#Model-Application\" data-toc-modified-id=\"Model-Application-8\"><span class=\"toc-item-num\">8&nbsp;&nbsp;<\/span>Model Application<\/a><\/span><ul class=\"toc-item\"><li><span><a href=\"#Test-Predictions\" data-toc-modified-id=\"Test-Predictions-8.1\"><span class=\"toc-item-num\">8.1&nbsp;&nbsp;<\/span>Test Predictions<\/a><\/span><\/li><li><span><a href=\"#Custom-Reviews\" data-toc-modified-id=\"Custom-Reviews-8.2\"><span class=\"toc-item-num\">8.2&nbsp;&nbsp;<\/span>Custom Reviews<\/a><\/span><\/li><\/ul><\/li><\/ul><\/div>","f1eecb49":"---\n\n## Data Preprocessing\nThe next step is data preprocessing. The following class behaves like your typical SKLearn vectorizer.\n\nIt can perform the following operations.\n* Discard non alpha-numeric characters\n* Set everything to lower case\n* Stems all words using PorterStemmer, and change the stems back to the most occurring existent word.\n* Discard non-Egnlish words (not by default).","7ab3c46b":"In this notebook Sentiment Analysis is performed on movie reviews.\n\n---","cbbc394d":"---\n\n## Model Evaluation"}}