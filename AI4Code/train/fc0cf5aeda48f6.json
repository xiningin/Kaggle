{"cell_type":{"7a54ad12":"code","15c42a29":"code","3e1eed8b":"code","c8d73df0":"code","15f42581":"code","4f236480":"code","aac8b0f4":"code","846d97f1":"code","31f3c9fa":"code","2e9f5562":"code","ca23431c":"code","5ee6db68":"code","7e9e84ef":"code","db59e809":"code","bc304925":"code","6a7d89dd":"code","8230743e":"code","048cc7ef":"code","098b059e":"code","ae180aec":"code","43e4be42":"code","0332f191":"code","37b67213":"code","945de286":"code","317de787":"markdown","d8d895c8":"markdown","795bda21":"markdown","56493741":"markdown","4ea4a8a9":"markdown","58c53dc4":"markdown","a9406e3b":"markdown","727e7ba6":"markdown","20643216":"markdown","82e6126b":"markdown","df08a74c":"markdown","5e1c3e35":"markdown","dd032b9b":"markdown","dbafeecc":"markdown","075212c0":"markdown","3debbb68":"markdown","91b4ffcf":"markdown","5b7e93e3":"markdown","030a58db":"markdown","a3dab459":"markdown","994bd79b":"markdown","651cca33":"markdown","da0f2d4c":"markdown","fc8de71b":"markdown","51197e23":"markdown","858b0dc2":"markdown","88e6801d":"markdown","d93984e0":"markdown","ac2ea280":"markdown","5e540969":"markdown","b4df13dd":"markdown","661fe20c":"markdown","41836a80":"markdown","9d56fed7":"markdown","09054b65":"markdown","96a1e060":"markdown"},"source":{"7a54ad12":"#Data Analysis Libraries\nimport numpy as np\nimport pandas as pd\nfrom pandas import Series,DataFrame\nimport statistics as st\nfrom scipy import stats\nfrom scipy import interp\nimport statistics as st\nimport math\nimport os\nfrom datetime import datetime\nimport itertools\n\n#Visualization Libraries\nimport matplotlib\nimport matplotlib.pyplot as plt\nfrom matplotlib import rcParams\nimport seaborn as sns\nsns.set(color_codes=True)\nfrom IPython.display import HTML\nfrom IPython.display import display\nimport plotly.offline as py\nfrom plotly.offline import init_notebook_mode, iplot\nimport plotly.graph_objs as go\ninit_notebook_mode(connected = True)\n%matplotlib inline\nfrom IPython.core.display import HTML\nfrom pdpbox import pdp, info_plots\nimport shap\nshap.initjs()\ndef multi_table(table_list):\n    ''' Acceps a list of IpyTable objects and returns a table which contains each IpyTable in a cell\n    '''\n    return HTML(\n        '<table><tr style=\"background-color:white;\">' + \n        ''.join(['<td>' + table._repr_html_() + '<\/td>' for table in table_list]) +\n        '<\/tr><\/table>'\n    )\n\n#sklearn\nfrom sklearn import metrics\nfrom sklearn.metrics import roc_curve, roc_auc_score, auc, accuracy_score,confusion_matrix, classification_report, confusion_matrix, jaccard_similarity_score, f1_score, fbeta_score\n\nfrom sklearn import preprocessing\nfrom sklearn.preprocessing import StandardScaler, Imputer,MinMaxScaler, LabelEncoder\n\nfrom sklearn import model_selection\nfrom sklearn.model_selection import GridSearchCV,train_test_split,cross_val_score, validation_curve, RandomizedSearchCV, cross_val_predict, StratifiedKFold\n\nfrom sklearn import linear_model\nfrom sklearn.linear_model import LogisticRegression, LinearRegression\n\nfrom sklearn import naive_bayes\nfrom sklearn.naive_bayes import GaussianNB\n\nfrom sklearn import neighbors\nfrom sklearn.neighbors import KNeighborsClassifier\n\nfrom sklearn import tree\nfrom sklearn.tree import DecisionTreeClassifier\n\nfrom sklearn import ensemble\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostRegressor\n\nfrom sklearn.svm import SVC\n\nfrom sklearn.ensemble import AdaBoostClassifier\n\nfrom sklearn import datasets\n\n#misc\nfrom functools import singledispatch\nimport eli5\nfrom eli5.sklearn import PermutationImportance\nimport shap\nfrom mpl_toolkits.mplot3d import Axes3D\nimport os\nimport warnings\nwarnings.filterwarnings('ignore')\nprint(os.listdir(\"..\/input\"))\nfrom xgboost import XGBClassifier\nimport lightgbm as lgb\nimport warnings\n","15c42a29":"heart = pd.read_csv(\"..\/input\/heart.csv\")","3e1eed8b":"heart2= heart.drop(heart.index[164])\nheart2.columns=['age', 'sex', 'cpain','resting_BP', 'chol', 'fasting_BS', 'resting_EKG', \n                'max_HR', 'exercise_ANG', 'ST_depression', 'm_exercise_ST', 'no_maj_vessels', 'thal', 'target']\n\nheart2['chol']=heart2['chol'].replace([417, 564], 240)\nheart2['chol']=heart2['chol'].replace([407, 409], 249)\n\nheart2['ST_depressionAB']=heart2['ST_depression'].apply(lambda row: 1 if row > 0 else 0)\nheart2A=heart2.iloc[:,0:11]\nheart2B=heart2.iloc[:,11:14]\nheart2C=heart2.loc[:,'ST_depressionAB']\nheart2C=pd.DataFrame(heart2C)\nheart2C.head()\nheart2 = pd.concat([heart2A, heart2C, heart2B], axis=1, join_axes=[heart2A.index])\n\nheart2.loc[48, 'thal']=2.0\nheart2.loc[281, 'thal']=3.0\n\nPHD=heart2.loc[heart2.loc[:,'target']==1]\nAHD=heart2.loc[heart2.loc[:,'target']==0]\nheart2.head()","c8d73df0":"#heart3 (descriptive)\nheart3=pd.DataFrame.copy(heart2)\n\nheart3['sex']=heart3['sex'].replace([1, 0], ['Male', 'Female'])\nheart3['cpain']=heart3['cpain'].replace([0, 1, 2, 3], ['Asymptomatic', 'Typical Angina', 'Atypical Angina', 'Non-Angina'])\nheart3['fasting_BS']=heart3['fasting_BS'].replace([1, 0], ['BS > 120 mg\/dl', 'BS < 120 mg\/dl'])\nheart3['resting_EKG']=heart3['resting_EKG'].replace([0, 1, 2], ['Normal', 'Left Ventricular Hypertrophy', 'ST-T Wave Abnormality'])\nheart3['exercise_ANG']=heart3['exercise_ANG'].replace([0, 1], ['Absent', 'Present'])\nheart3['m_exercise_ST']=heart3['m_exercise_ST'].replace([0, 1, 2], ['Upsloping', 'Flat', 'Downsloping'])\nheart3['thal']=heart3['thal'].replace([1, 2, 3], ['Fixed Defect', 'Normal', 'Reversible Defect'])\nheart3['target']=heart3['target'].replace([0, 1], ['Absent', 'Present'])\n\nheart3['chol']=heart3['chol'].replace([417, 564], 240)\nheart3['chol']=heart3['chol'].replace([407, 409], 249)\n\nheart3.loc[48, 'thal']=\"Normal\"\nheart3.loc[281, 'thal']=\"Reversible Defect\"\n\nPHD3=heart3.loc[heart3.loc[:,'target']==\"Present\"]\nAHD3=heart3.loc[heart3.loc[:,'target']==\"Absent\"]\nheart3.head()\n","15f42581":"numrows= heart3.shape[0]\nnumcolumns=heart3.shape[1]\ndisplay(heart3.head(5), heart3.describe(), print(\"Number of Rows:\", numrows),print(\"Number of Columns:\", numcolumns))","4f236480":"ax1 = sns.countplot(heart3['target'], palette=\"BuPu\")\nplt.title(\"Distribution of HD Diagnosis\", size=30)\nplt.ylabel(\"Frequency\", labelpad=40, size=20)\nplt.xlabel(\"HD Diagnosis\", labelpad=40, size=20)","aac8b0f4":"fig = plt.figure(figsize=(20,20))\n\n\nplt.subplot(3, 2, 1)\nwarnings.filterwarnings('ignore')\nax1 = sns.distplot(heart2['age'], kde=False, color='blueviolet')\nax1.set_xlabel(\"Age (yrs)\")\nsecond_ax1 = ax1.twinx()\nsecond_ax1.yaxis.set_label_position(\"left\")\nsns.distplot(heart2['age'], ax=second_ax1, kde=True, hist=False, color='blue')\nsecond_ax1.set_yticks([])\nplt.title(\"Distribution of Age\", size=15)\nplt.ylabel(\"Frequency\")\n\nplt.subplot(3, 2, 2)\nwarnings.filterwarnings('ignore')\nax1 = sns.distplot(heart2['max_HR'], kde=False, color='blueviolet')\nax1.set_xlabel(\"Maximum HR (bpm)\")\nsecond_ax1 = ax1.twinx()\nsecond_ax1.yaxis.set_label_position(\"left\")\nsns.distplot(heart2['max_HR'], ax=second_ax1, kde=True, hist=False, color='blue')\nsecond_ax1.set_yticks([])\nplt.title(\"Distribution of Maximum Heart Rate\", size=15)\nplt.ylabel(\"Frequency\")\n\nplt.subplot(3, 2, 3)\nwarnings.filterwarnings('ignore')\nax1 = sns.distplot(heart2['resting_BP'], kde=False, color='blueviolet')\nax1.set_xlabel(\"Resing Systolic BP (mm Hg)\")\nsecond_ax1 = ax1.twinx()\nsecond_ax1.yaxis.set_label_position(\"left\")\nsns.distplot(heart2['resting_BP'], ax=second_ax1, kde=True, hist=False, color='blue')\nsecond_ax1.set_yticks([])\nplt.title(\"Distribution of Resting Systolic Blood Pressure\", size=15)\nplt.ylabel(\"Frequency\")\n\nplt.subplot(3, 2, 4)\nwarnings.filterwarnings('ignore')\nax1 = sns.distplot(heart2['ST_depression'], kde=False, color='blueviolet')\nax1.set_xlabel(\"ST Depression\")\nsecond_ax1 = ax1.twinx()\nsecond_ax1.yaxis.set_label_position(\"left\")\nsns.distplot(heart2['ST_depression'], ax=second_ax1, kde=True, hist=False, color='blue')\nsecond_ax1.set_yticks([])\nplt.title(\"Distribution of Exercise Induced ST Depression\", size=15)\nplt.ylabel(\"Frequency\")\nplt.xlim(0,7)\n\nplt.subplot(3, 2, 5)\nwarnings.filterwarnings('ignore')\nax1 = sns.distplot(heart2['chol'], kde=False, color='blueviolet')\nax1.set_xlabel(\"Serum Cholesterol (mg\/dl)\")\nsecond_ax1 = ax1.twinx()\nsecond_ax1.yaxis.set_label_position(\"left\")\nsns.distplot(heart2['chol'], ax=second_ax1, kde=True, hist=False, color='blue')\nsecond_ax1.set_yticks([])\nplt.title(\"Distribution of Serum Cholesterol\",size=15)\nplt.ylabel(\"Frequency\")\n\nplt.show()\n\n","846d97f1":"fig = plt.figure(figsize=(20,20))\nplt.subplot(3, 3, 1)\nsns.countplot(heart3['sex'], palette=\"BuPu\")\nplt.title(\"Gender Distribution\", size=15)\nplt.ylabel(\"Frequency\")\nplt.xlabel(\"Provider-Identified Gender\")\n\nplt.subplot(3, 3, 2)\nsns.countplot(heart3['cpain'], palette=\"BuPu\")\nplt.title(\"Distribution of Chest Pain Type\", size=15)\nplt.ylabel(\"Frequency\")\nplt.xlabel(\"Chest Pain Description\")\n\nplt.subplot(3, 3, 3)\nsns.countplot(heart3['fasting_BS'], palette=\"BuPu\")\nplt.title(\"Fasting Blood Sugar Distribution\", size=15)\nplt.ylabel(\"Frequency\")\nplt.xlabel(\"Level of Fasting BS (mmol\/L)\")\n\nplt.subplot(3, 3, 4)\nsns.countplot(heart3['resting_EKG'], palette=\"BuPu\")\nplt.title(\"Distribution of Resting EKG Results\", size=15)\nplt.ylabel(\"Frequency\")\nplt.xlabel(\"EKG Results\")\n\nplt.subplot(3, 3, 5)\nsns.countplot(heart3['exercise_ANG'], palette=\"BuPu\")\nplt.title(\"Distribution of Exercise Induced Angina\", size=15)\nplt.ylabel(\"Frequency\")\nplt.xlabel(\"Exercise Induced Angina\")\n\nplt.subplot(3, 3, 6)\nsns.countplot(heart3['m_exercise_ST'], palette=\"BuPu\")\nplt.title(\"Distribution of the ST Segment Slope\", size=15)\nplt.ylabel(\"Frequency\")\nplt.xlabel(\"Slope  (Peak Exercise)\")\n\nplt.subplot(3, 3, 7)\nsns.countplot(heart3['ST_depressionAB'], palette=\"BuPu\")\nplt.title(\"ST Depression Abnormalities\", size=15)\nplt.ylabel(\"Frequency\")\nplt.xlabel(\"ST Depression Abnormalities\")\n\nplt.subplot(3, 3, 8)\nsns.countplot(heart3['no_maj_vessels'], palette=\"BuPu\")\nplt.title(\"No. of Major Vessels Colored by Flouroscopy\", size=15)\nplt.ylabel(\"Frequency\")\nplt.xlabel(\"Number of Major Vessels\")\n\nplt.subplot(3, 3, 9)\nsns.countplot(heart3['thal'], palette=\"BuPu\")\nplt.title(\"Thalium Stress Test Results\", size=15)\nplt.ylabel(\"Frequency\")\nplt.xlabel(\"Results\")","31f3c9fa":"mask = np.zeros_like(heart2.corr(), dtype=np.bool)\nmask[np.triu_indices_from(mask)] = True \nplt.figure(figsize=(20,20))\nsns.heatmap(heart2.corr(),vmax=.8, center=0,\n            square=True, linewidths=.1, mask=mask, cbar_kws={\"shrink\": .5},annot=True)","2e9f5562":"heart2.corr()","ca23431c":"corre=heart2.corr()\nTargetCorr=corre.loc[:'thal','target']\nTargetCorr=pd.DataFrame(TargetCorr)\nTargetCorr['AbsVal']=TargetCorr['target'].apply(lambda row: abs(row))\nTargetCorr['Rank']=pd.DataFrame.rank(TargetCorr['AbsVal'])\nTargetCorr['Feature']=TargetCorr.index\nTargetCorr = TargetCorr.set_index('Rank') \nTargetCorr = TargetCorr.sort_index(ascending=0)\nTargetCorr = TargetCorr.set_index('Feature') \nTargetCorr=TargetCorr.loc[:,'target']\nTargetCorr=pd.DataFrame(TargetCorr)\nTargetCorr.columns=[\"Correlation with Target\"]\nTargetCorr","5ee6db68":"PHD=heart2.loc[heart2.loc[:,\"target\"]==1]\nAHD=heart2.loc[heart2.loc[:,\"target\"]==0]\n\nfrom scipy.stats import ttest_ind\ndef rowz(ttest): \n    name=ttest_ind(PHD[ttest], AHD[ttest])\n    name=list(name)\n    name = pd.DataFrame(np.array(name))\n    name=name.T\n    col=[\"t-statistic\", \"p_value\"]\n    name.columns=col\n    return name\n\nAGE=rowz('age')\nAGE.loc[:,\"Names\"]=\"Age\"\nRESTING_BP=rowz('resting_BP')\nRESTING_BP.loc[:,\"Names\"]=\"Resting_BP\"\nCHOLESTEROL=rowz('chol')\nCHOLESTEROL.loc[:,\"Names\"]=\"Cholesterol\"\nMAX_HR=rowz('max_HR')\nMAX_HR.loc[:,\"Names\"]=\"Max_HR\"\nST_DEP=rowz('ST_depression')\nST_DEP.loc[:,\"Names\"]=\"ST_Depression\"\n\nPVALS = pd.concat([AGE, RESTING_BP,CHOLESTEROL,MAX_HR, ST_DEP], axis=0)\nPVALS=PVALS.set_index(PVALS[\"Names\"])\nP_VALS= PVALS.drop('Names',axis=1)\n\nP_VALS","7e9e84ef":"sns.pairplot(heart2,vars = ['resting_BP', 'chol','max_HR','ST_depression', 'age'],hue='target')","db59e809":"#seperate independent (feature) and dependent (target) variables\n#KNN cannot process text\/ categorical data unless they are be converted to numbers\n#For this reason I did not input the heart3 DataFrame created above\nX=heart2.drop('target',1)\ny=heart2.loc[:,'target']\n\n#Scale the data\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)\n\n#Split the data into training and testing sets\nX_train,X_test,y_train,y_test = train_test_split(X_scaled, y,test_size=.2,random_state=40)\n\n#Call classifier and, using GridSearchCV, find the best parameters\nknn = KNeighborsClassifier()\nparams = {'n_neighbors':[i for i in range(1,33,2)]}\nmodelKNN = GridSearchCV(knn,params,cv=10)\nmodelKNN.fit(X_train,y_train)\nmodelKNN.best_params_   \n\n#Use the above model (modelKNN) to predict the y values corresponding to the X testing set\npredictKNN = modelKNN.predict(X_test)\n\n#Compare the results of the model's predictions (predictKNN) to the actual y values\naccscoreKNN=accuracy_score(y_test,predictKNN)\nprint('Accuracy Score: ',accuracy_score(y_test,predictKNN))\nprint('Using k-NN we get an accuracy score of: ',\n      round(accuracy_score(y_test,predictKNN),5)*100,'%')","bc304925":"perm = PermutationImportance(modelKNN).fit(X_test, y_test)\neli=eli5.show_weights(perm, feature_names = X.columns.tolist())\neli","6a7d89dd":"X_train,X_test,y_train,y_test = train_test_split(X, y,test_size=.2)\nX_test=pd.DataFrame(X_test)\nX_test\n\nbase_features = X.columns.values.tolist()\n\nfeat_name = 'no_maj_vessels'\n\npdp_dist = pdp.pdp_isolate(model=modelKNN, dataset=X, model_features=base_features, feature=feat_name)\n\npdp.pdp_plot(pdp_dist, feat_name)\nplt.ylim(-0.015,0.01)\n#plt.xticks(np.arange(0, 4, step=1))\nplt.show()","8230743e":"X_train,X_test,y_train,y_test = train_test_split(X, y,test_size=.2)\nX_test=pd.DataFrame(X_test)\nX_test\n\nbase_features = X.columns.values.tolist()\n\nfeat_name = 'age'\n\npdp_dist = pdp.pdp_isolate(model=modelKNN, dataset=X, model_features=base_features, feature=feat_name)\n\npdp.pdp_plot(pdp_dist, feat_name)\n#plt.ylim(-0.025,0.01)\n#plt.xticks(np.arange(0, 4, step=1))\nplt.show()","048cc7ef":"X_train,X_test,y_train,y_test = train_test_split(X, y,test_size=.2)\nX_test=pd.DataFrame(X_test)\nX_test\n\nbase_features = X.columns.values.tolist()\n\nfeat_name = 'chol'\n\npdp_dist = pdp.pdp_isolate(model=modelKNN, dataset=X, model_features=base_features, feature=feat_name)\n\npdp.pdp_plot(pdp_dist, feat_name)\n#plt.ylim(-0.025,0.01)\n#plt.xticks(np.arange(0, 4, step=1))\nplt.show()","098b059e":"X_train,X_test,y_train,y_test = train_test_split(X, y,test_size=.2)\nX_test=pd.DataFrame(X_test)\nX_test\n\nbase_features = X.columns.values.tolist()\n\nfeat_name = 'ST_depression'\n\npdp_dist = pdp.pdp_isolate(model=modelKNN, dataset=X, model_features=base_features, feature=feat_name)\n\npdp.pdp_plot(pdp_dist, feat_name)\n#plt.ylim(-0.025,0.01)\n#plt.xticks(np.arange(0, 4, step=1))\nplt.show()","ae180aec":"X_train,X_test,y_train,y_test = train_test_split(X, y,test_size=.2)\nX_test=pd.DataFrame(X_test)\nX_test\n\nbase_features = X.columns.values.tolist()\n\nfeat_name = 'max_HR'\n\npdp_dist = pdp.pdp_isolate(model=modelKNN, dataset=X, model_features=base_features, feature=feat_name)\n\npdp.pdp_plot(pdp_dist, feat_name)\nplt.ylim(-0.005,0.2)\n#plt.xticks(np.arange(0, 4, step=1))\nplt.show()","43e4be42":"X_train,X_test,y_train,y_test = train_test_split(X, y,test_size=.2)\nX_test=pd.DataFrame(X_test)\nX_test\n\nbase_features = X.columns.values.tolist()\n\nfeat_name = 'resting_BP'\n\npdp_dist = pdp.pdp_isolate(model=modelKNN, dataset=X, model_features=base_features, feature=feat_name)\n\npdp.pdp_plot(pdp_dist, feat_name)\nplt.ylim(-0.005,0.2)\n#plt.xticks(np.arange(0, 4, step=1))\nplt.show()","0332f191":"X= heart2.drop('target',1)\ny= heart2['target']\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)\nX_train,X_test,y_train,y_test = train_test_split(X_scaled,y, test_size=.3,random_state=40)\n\nclf=RandomForestClassifier(n_estimators=100)\nclf.fit(X_train,y_train)\ny_pred=clf.predict(X_test)\n\n","37b67213":"feature_imp = pd.Series(clf.feature_importances_,index=X.columns).sort_values(ascending=False)\nfeature_imp","945de286":"# Creating a bar plot\nsns.barplot(x=feature_imp, y=feature_imp.index)\n# Add labels to your graph\nplt.xlabel('Feature Importance Score')\nplt.ylabel('Features')\nplt.title(\"Visualizing Important Features\")\nplt.legend()\nplt.show()                 ","317de787":"# <a id='1'>1. Librairies and Data<\/a> \n## <a id='#1.1'>1.1. Loding Libraries<\/a> ","d8d895c8":"It appears that as the maximum heart rate increases, so does the likelihood of a Heart Disease diagnosis. ","795bda21":"## <a id='3.2'>3.2. Random Forest<\/a> ","56493741":"# <a id='4'>4. Conclusions<\/a> ","4ea4a8a9":"Change DataFrame to match preprocessing changes made in [Part 1](https:\/\/www.kaggle.com\/maurerm\/kaggleproject-part-1-exploratory-data-analysis) of this Kaggle Assignment.","58c53dc4":"For more in-depth bivariate analysis, please go to [Part 1](https:\/\/www.kaggle.com\/maurerm\/kaggleproject-part-1-exploratory-data-analysis) of this Kaggle Assignment.","a9406e3b":"## <a id='2.2'>2.2. Target Distribution<\/a> ","727e7ba6":"## <a id='2.3'>2.3. Univariate analysis<\/a> ","20643216":"## <a id='1.1'>1.1. Feature Directory<\/a> \nIt's a fairly simple dataset to understand set of data. However, due to the numeric qualifiers given to the categorical data, what the values mean is not obvious. \n\n1.  age: The person's age in years\n2.  sex: Provider-identified gender \n    * 0 = Female\n    * 1 = Male\n3. cpain: The type of chest pain experienced\n   * 0: Asymptomatic Pain\n   * 1: Typical Angina Pain\n   * 2: Atypical Angina Pain\n   * 3: Non-Angina Pain\n4. resting_BP: Resting Systolic Blood Pressure (mm Hg) upon Hospital Admission\n5. chol: Serum Cholesterol (mg\/dl)\n6. fasting_BS: Fasting Blood Sugar (mmol\/L)\n   * 0: Lower than 120 mmol\/L\n   * 1: Greater than 120 mmol\/L\n7. resting_EKG: Resting EKG Results\n   * 0: Normal EKG results\n   * 1: Showing probable or definite left ventricular hypertrophy by Estes' criteria\n   * 2: Having ST-T wave abnormality (T wave inversions and\/or ST elevation or depression of > 0.05 mV)\n8. max_HR: Maximum Heart Rate Achieved (bpm)\n9. 'exercise_ANG': Exercise Induced Angina (EIA)\n   * 0: No, they did not experience EIA\n   * 1: Yes, they experienced EIA\n10. ST_depression: ST Depression (mm) Induced by Exercise Relative to Rest\n11. ST_depressionAB: ST Depression Abnormalitiles\n    * 0: Normal-> The pts ST depression was 0\n    * 1: Abnormal-> The pts ST depression greater than 0\n    * This is a engineered feature.  If you would like to see how or why I engineered it, please examine [Part 1](https:\/\/www.kaggle.com\/maurerm\/kaggleproject-part-1-exploratory-data-analysis) of this Kaggle Assignment.\n12. m_exercise_ST: The Slope of the Peak Exercise ST Segment\n    * 0: Upsloping\n    * 1: Flat\n    * 2: Downsloping\n13. no_maj_vessels: Number of Major Vessels (0-3) Colored by Flourosopy\n    * Either 0, 1, 2, 3, or 4\n13. thal: Thalium Stress Test Result Results\n    * 1: Fixed defect\n    * 2: Normal\n    * 3: Reversible defect\n14. target': Absence or Presence of Heart Disease\n   * 0: no heart disease\n   * 1: heart disease present","82e6126b":"To make the dataset a little more intuiative, I made a second dataset, heart3, where the numeric descriptors have been changed to words.  This will improve interpretation later on.","df08a74c":"## <a id='3.1'>3.1. K-Nearest Neighbors<\/a> ","5e1c3e35":"## <a id='2.5'>2.5. Multivariate analysis<\/a> ","dd032b9b":"As we can see, almost all of the p-values are significant (<0.05).\n* ST_Depression: 0.000000000000005815\n* Maximum Heart Rate: 0.00000000000002476\n* Age: 0.001039\n* Resting Blood Pressure: 0.010927\n\nThe only non-significant p-value is cholesterol (0.07985).\n\n\nThis means that for ST depression, maximum heart rate, age, and resting blood pressure there is less than a 5% chance that the differences between the target sample^ means could have occured by chance  alone.\n   \n   ^ Target Sample: Absence or presence of heart disease\n","dbafeecc":"### <a id=''>Paired T-Test<\/a>\n\nI am only going to run the paired T-Test on the quantitative features.  As the discrete feautes have, at maximum, 5 discrete values, comparing their means would not provide meaningful information.  ","075212c0":"Let's look more specifically at the correlations between the features and the target. \n\nThe following dataframe is organized in descending order of the absolute value of the correlation between the feature and the target.","3debbb68":"Let's take a closer look at the numerical features using a Partial Dependence Plot. \n\nThese plots change a single variable in a single row across a range of values and calculate the effect those changes have on the outcome. It does this for several rows and plots the average effect.","91b4ffcf":"- <a href='#1'>1. Introduction<\/a>  \n    - <a href='#1.2'>1.2. Feature Directory<\/a> \n- <a href='#2'>2. Libraries and Data<\/a>  \n    - <a href='#2.1'>2.1. Loading libraries<\/a> \n    - <a href='#2.2'>2.2. Reading data<\/a> \n- <a href='#3'>3. Exploratory Data Analysis (EDA)<\/a> \n    - <a href='#3.1'>3.1. Shape, Head, Describe<\/a> \n    - <a href='#3.2'>3.2. Target Distribution<\/a> \n    - <a href='#3.3'>3.3. Univariate analysis<\/a> \n    - <a href='#3.4'>3.4 Bivariate analysis (Feature vs Target)<\/a> \n    - <a href='#3.5'>3.5 Multivariate analysis<\/a> \n- <a href='#4'>4. Predictive Modeling<\/a>\n    - <a href='#4.1'>4.1. K-Nearest Neighbors<\/a> \n    - <a href='#4.2'>4.2. Random Forests<\/a> \n- <a href='#5'>5. Conclusions<\/a>","5b7e93e3":"# <a id='1'>1. Introduction<\/a> \nOf all the applications of machine-learning, diagnosing any serious medical disease using a predictive model is going to difficult. If the output of a preditive model is treatment, such as surgery or medication, or even the absence of treatment, people are going to want to know why the model predicted their particular course of action.\n\nThis dataset gives 13 features along with a target condition (the presence or absence of Heart Disease). Below, the data is explored using K-Nearest Neighbors and then investigated using Machiene Learning explainability tools and techniques.\n\nThis dataset was created by the following:\n* Hungarian Institute of Cardiology, Budapest: Andras Janosi, M.D. \n* V.A. Medical Center, Long Beach and Cleveland Clinic Foundation, Robert Detrano, M.D., Ph.D.\n* University Hospital, Zurich, Switzerland: William Steinbrunn, M.D. \n* University Hospital, Basel, Switzerland: Matthias Pfisterer, M.D.","030a58db":"I am using Random Forest because my model in [Part 2](https:\/\/www.kaggle.com\/maurerm\/kaggleproject-part-2-modeling) of this Kaggle Assignment had decent accuracy values and because the Random Forest has the ability to rank the important.  In fact, Random Forests are often used for feature selection in a data science workflow.","a3dab459":"### <a id=''>Categorical data<\/a> ","994bd79b":"Resting Blood Pressure also doesn't give a lot of weight to this model, but it dose appear that the lowest likelihood of Heart Disease is from 120-140 systolic blood pressure, which is still high.  But the more extreme values seem more, albiet slighlty, associated with Heart Disease. ","651cca33":"ST_depression doesn't seem to affect the liklihood of Heart Disease.  ","da0f2d4c":"# <a id='2'>2. Exploratory Data Analysis (EDA)<\/a>  \n## <a id='2.1'>2.1. Shape, Head, Describe<\/a> ","fc8de71b":"Permutation importance is a great tool for understanding the affects of features on the Machiene Learing model.  Specifically, after a model has been fit, it shuffels individual variables in the validation data and looks at their the effect on accuracy. ","51197e23":"In [Part 2](https:\/\/www.kaggle.com\/maurerm\/kaggleproject-part-2-modeling) of this Kaggle Assignment I tested 6 predictive models Logisitic Regression, K-Nearest Neighbors, Decision Tree, AdaBoost with a Decision Tree Base, and XGBoost).\n\nI am first examining K-Nearest Neighbors because the model fitted by GridSearchCV had high measures of accuracy, even in the face of cross validation.  ","858b0dc2":"## <a id='2.4'>2.4. Bivariate analysis (Feature vs Target)<\/a>","88e6801d":"### <a id='1'>1. Please read in order.  I reference a lot of findings from part 1 and 2.  Thank you for all of your help this past 6 months!!<\/a> ","d93984e0":"Interestingly, it appears as if the likelyhood of Heart Disease goes down with age.  I wonder why. ","ac2ea280":"### <a id=''>Numerical data<\/a> ","5e540969":"So as the number of major blood vessels increases, the probability of heart disease decreases. \n\nThat makes sense, as the more major blood vessels that are colored indicates more bloodflow to the heart. \nHowever, the blue confidence are very large and show that this might not be true.","b4df13dd":"### <a id=''>Correlation<\/a>","661fe20c":"## <a id='1.2'>1.2. Reading data<\/a> ","41836a80":"* Data indicates that as the number of major blood vessels increases, the probability of heart disease decreases. However, the blue confidence intervals are quite large, so this may not be accurate. \n* There appears that the likelyhood of a Heart Disease diagnosis goes down as age and cholesterol increase.\n* According to the Partial Dependence Plot, ST depression doesn't seem to affect the liklihood of Heart Disease.  However, it is ranked high as an important feature using the Random Forest model.\n* It appears that as the maximum heart rate increases, so does the likelihood of a Heart Disease diagnosis. \n* Interestingly, it dose appear that the lowest likelihood of Heart Disease is from 120-140 systolic blood pressure, which is still high.  But the more extreme values seem more, albiet slighlty, associated with Heart Disease.  ","9d56fed7":"Interestingly, it appears as if the likelyhood of Heart Disease goes down with cholesterol levels.  Hmm...","09054b65":"## <a id='3'>3. Predictive Modeling<\/a> ","96a1e060":"----------\n**Kaggle Mini-Project III: Presentation**\n=====================================\nMaggie Maurer\n\nCoderGirl, DataScience Cohort\n\nJuly 2019\n\n----------"}}