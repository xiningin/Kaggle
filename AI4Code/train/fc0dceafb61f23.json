{"cell_type":{"f145da5d":"code","9878369c":"code","b2e86484":"code","bd8ce350":"code","fbea0d51":"code","b8996504":"code","056eef35":"code","7a125104":"code","b8c04bdd":"code","a3980c17":"code","0523116c":"markdown","53f7208b":"markdown","f9f17233":"markdown","3b001bbc":"markdown","50ea636e":"markdown"},"source":{"f145da5d":"!pip install kaggle-environments --upgrade -q","9878369c":"from kaggle_environments import make\nenv = make(\"mab\", debug=True)","b2e86484":"%%writefile skanderovitch.py\n\n# IMPORT\n\nimport numpy as np\nfrom scipy.stats import beta\n\n# AGENT CONFIG\n\n\n# How do we evaluate the bandit performance ? \nest_method = ['mean','thompson','ucb'][2]      # choosing UCB here\nest_ucb_percentile = 0.75                      # percentile for UCB : higher is more optimistic\n\n# Having evaluated the bandits, how do we select a candidate (exploration \/ exploitation) ?\npick_method = ['epsilon','greedy','weighted','random','stupid'][0]    # choosing epsilon greedy here\npick_weighted_alpha = 1                                               # for the weighted sampling, higher is greedier\npick_epsilon = 0.5                                                    # for epsilon-greedy, higher is more exploration\npick_epsilon_decay = 0.997                                            # lower means we shift to exploitation faster\n\n# How can we use the information provided by the actions of the other \/ rival bot?\nmin_opp_quality = 0.2     # higher means we believe the other bot knows what they are doing, even if they seem to play poorly\nopp_retry_factor = 1.     # higher means we care about exploiting a bandit found by the other bot (stealing) more than a bandit we identified\n\n# GAME CONFIG\n\ndecay = 0.97\nn_levers = None\n\n# Global variables, will be explained below, as they are initialised\nmy_score = 0      # keep track of my score\nbeta_a = None\nbeta_b = None\nmy_pulls = None\nopp_pulls = None\nall_pulls = None\nopp_quality = 0   # this will reflect how well we believe the other bot is playing\n\n\n# EXECUTION\n\ndef logic(observation, configuration):\n    \n    global n_levers,my_score, beta_a, beta_b, my_pulls, opp_pulls,all_pulls, pick_epsilon\n    \n    # FIRST ROUND ?\n    \n    if observation.step == 0: \n        \n        # We initialise the global vars\n        \n        n_levers = configuration.banditCount   # Number of bandits\n        \n        beta_a = np.ones(n_levers)             # Beta distribution with parameters (1,1),\n        beta_b = np.ones(n_levers)             # means we have a uniform prior on the probability of each bandit\n        \n        my_pulls = np.zeros(n_levers)          # We keep track of how many times we pull each bandit\n        opp_pulls = np.zeros(n_levers)         # Same for the rival bot\n        all_pulls = np.zeros(n_levers)         # Same across both bots\n        \n    else:  \n\n        # We update our knowledge\n        \n        my_choice,opp_choice = get_actions(observation)  # what did we each play at the previous round ?\n        my_reward = compute_reward(observation)          # did I get a reward ?\n        \n        beta_a[my_choice] += my_reward                   # we compute the posterior distribution,\n        beta_b[my_choice] += 1 - my_reward               # ignoring the decay for now (dealt with later)\n        \n        my_pulls[my_choice] += 1                         # Update how many times the bandits were pulled\n        opp_pulls[opp_choice] += 1\n        all_pulls[my_choice] += 1\n        all_pulls[opp_choice] += 1\n        \n    a,b = merge_all_info()                               # What is the best estimate we can get for the distribution of each bandit\n                                                         # using both what we learnt from the rewards, but also the rival bot's actions\n    my_est = compute_est(a,b)                            # We sample an estimate for each bandit from these distributions\n    decayed_est = my_est * decay**all_pulls              # We decay the estimates based on how many times the bandits were used\n    my_choice = pick_bandit(decayed_est)                 # We pick one using the chosen strategy ()\n\n    pick_epsilon  *= pick_epsilon_decay                  # We progressively favour exploitation vs exploration    \n    \n    return int(my_choice)\n\n\n# MECHANICS    \n\n\ndef compute_est(a,b):\n    # Given some distributions for each bandit, how do we compute the estimate ?\n\n    if est_method == 'thompson':      # we sample from the distribution\n        return np.random.beta(a, b)\n    elif est_method == 'ucb':         # we pick the value at percentile X\n        \n        # Note : the Bayesian UCB sampler template in the competition is written as \n        # post_a \/ (post_a + post_b) + beta.std(post_a, post_b) * c\n        # which I don't understand (eg could give values >> 1 for c large enough)\n        # if anyone can explain, please let me know\n        # I am using the PPF here, as it makes more sense to me\n        \n        return beta.ppf(est_ucb_percentile,a,b)\n    elif est_method == 'mean':        # we pick the mean, this ignores the uncertainty\n        return a \/ (a + b)\n\n\ndef pick_bandit(est_prob,pick_method=pick_method):\n    # Given some estimes, how do we pick our candidate ?\n    \n    if pick_method == 'greedy':          # always pick the highest\n        return int(np.argmax(est_prob))\n    elif pick_method == 'epsilon':       # same, but sometimes explore\n        if np.random.random() < pick_epsilon:\n                                         # exploration is done via the weighted method\n            return pick_bandit(est_prob,pick_method='weighted')  \n        else:                            # default to greedy \n            return pick_bandit(est_prob,pick_method='greedy')\n    elif pick_method == 'weighted':      # we will pick high estimates more often than low ones\n        p = est_prob**pick_weighted_alpha\n        p = p \/ p.sum()\n        return np.random.choice(range(len(est_prob)),p=p)\n    elif pick_method == 'random':        # pure random\n        return np.random.choice(range(len(est_prob)))\n    elif pick_method == 'stupid':        # always pick the lowest \/ worst\n        return int(np.argmin(est_prob))\n\n\n\n    \n\n# INCORPORATE OPPONENT INFORMATION\n\ndef compute_opp_quality():\n    # How well is the opponent playing ?\n    # Should we use their choices to inform our knowledge ?\n    \n    global opp_quality\n    # What do we independly believe about the bandits, based on what we observed ?\n    indep_est = compute_est(beta_a,beta_b)\n    # How well do the rival's actions correlate with our knowledge ?\n    # Ie did they pull the right bandits ?\n    opp_quality = np.corrcoef(opp_pulls,indep_est)[0,1]\n    # Note : this can be improved, as it ignores\n    # - what they can not know, ie the bandits they never pulled\n    # - the decay of the bandits, ie they may have pulled lots from a bandit that is now very low probability\n\ndef merge_all_info():\n    # How do we bring together\n    # - what we observed\n    # - what we can infer from the rival's actions ?\n    \n    # How good \/ believable is the opponent ?\n    compute_opp_quality()\n    # We will use their information based on :\n    # - our estimate of the opponent quality\n    # - a minimum value (to give them the benefit of doubt, esp early in the game)\n    # - how much we prefer to steal \/ ruin their bandits vs exploiting the ones we found\n    opp_retry_value = max(min_opp_quality,opp_quality) * opp_retry_factor\n    \n    # The good bandits discovered by the rival are identified by them playing more than once\n    opp_wins = np.maximum(opp_pulls-1,0)\n    opp_losses = opp_pulls - opp_wins\n    \n    # we combine our estimate with the additional information\n    a = beta_a + opp_wins*opp_retry_value\n    b = beta_b + opp_losses*opp_retry_value\n    return a,b\n\n\n\n\n####### BORING\n\n\ndef compute_reward(observation):\n    global my_score\n    reward = observation.reward - my_score\n    my_score = observation.reward\n    return reward\n\ndef get_actions(obs):\n    opponentIndex = 1 - obs.agentIndex\n    oppAction = obs.lastActions[opponentIndex]\n    myAction = obs.lastActions[obs.agentIndex]\n    return myAction,oppAction\n\n\ndef agent(observation, configuration):\n    # just because this needs to be last\n    return logic(observation, configuration)\n\n\n\n","bd8ce350":"env.run([\"skanderovitch.py\", \"skanderovitch.py\"])\nenv.render(mode=\"ipython\", width=800, height=500)","fbea0d51":"import numpy as np\nimport pandas as pd\nimport sys\nfrom scipy.stats import beta\nfrom tqdm import tqdm_notebook as tqdm\n\n####### GAME MECHANICS : this implements the same as the above\n\ndef pull(lever,game):\n    prob = game['true_prob'][lever]\n    game['true_prob'][lever] *= game['decay']\n    reward = np.random.random() < prob\n    return reward\n    \ndef pick_bandit(est_prob,pick_method,alpha,epsilon):\n    if pick_method == 'greedy':\n        return int(np.argmax(est_prob))\n    elif pick_method == 'epsilon':\n        if np.random.random() < epsilon:\n            return pick_bandit(est_prob,pick_method='weighted',alpha=alpha,epsilon=epsilon)\n        else:\n            return pick_bandit(est_prob,pick_method='greedy',alpha=alpha,epsilon=epsilon)\n    elif pick_method == 'weighted':\n        p = est_prob**alpha\n        p = p \/ p.sum()\n        return np.random.choice(range(len(est_prob)),p=p)\n    elif pick_method == 'random':\n        return np.random.choice(range(len(est_prob)))\n    elif pick_method == 'stupid':\n        return int(np.argmin(est_prob))\n\ndef opp_choose(game):\n    return pick_bandit(game['true_prob'],game['opp_pick_method'],game['opp_alpha'],game['opp_epsilon'])\n\n####### ESTIMATION\n\ndef compute_opp_quality(game):\n    indep_est = compute_est(game['beta_a'],game['beta_b'],game)\n    return np.corrcoef(game['opp_pulls'],indep_est)[0,1]\n\ndef compute_est(a,b,game):\n    if game['est_method'] == 'thomson':\n        return np.random.beta(a, b)\n    elif game['est_method'] == 'ucb':\n        return beta.ppf(game['est_ucb_percentile'],a,b)\n    elif game['est_method'] == 'mean':\n        return a \/ (a + b)\n        \ndef decay_est(est,game):\n    return est * game['decay']**game['all_pulls']\n\n\ndef merge_all_info(game):\n    game['opp_quality'] = compute_opp_quality(game)\n    opp_retry_value = max(game['min_opp_quality'],game['opp_quality'])* game['opp_retry_factor']\n    \n    opp_wins = np.maximum(game['opp_pulls']-1,0)\n    opp_losses = game['opp_pulls'] - opp_wins\n    a = game['beta_a'] + opp_wins*opp_retry_value\n    b = game['beta_b'] + opp_losses*opp_retry_value\n    return a,b\n\n\n#### SIMULATION\n\ndef simulate(game):\n    try:\n        game['original_epsilon'] = game['pick_epsilon']\n\n        for r in range(game['n_rounds']):\n            opp_choice = opp_choose(game)\n\n            opp_reward = pull(opp_choice,game)\n            game['opp_score'] += opp_reward\n\n\n            a,b = merge_all_info(game)\n            game['my_est'] = compute_est(a,b,game)\n            game['decayed_est'] = decay_est(game['my_est'],game)\n            my_choice = pick_bandit(game['decayed_est'],game['pick_method'],game['pick_weighted_alpha'],game['pick_epsilon'])\n\n            my_reward = pull(my_choice,game)\n            game['my_score'] += my_reward\n            if my_reward:\n                game['beta_a'][my_choice] += 1\n            else:\n                game['beta_b'][my_choice] += 1\n\n            game['my_pulls'][my_choice] += 1\n            game['opp_pulls'][opp_choice] += 1\n            game['all_pulls'][my_choice] += 1\n            game['all_pulls'][opp_choice] += 1\n\n            game['pick_epsilon'] *= game['pick_epsilon_decay']\n    \n    except:\n        print('\\nError',sys.exc_info())\n        print(game)\n        \n    return game","b8996504":"def run_sim():\n    \n    ### CREATE A RANDOM CONFIGURATION FOR OUR AGENT AND THE OPPONENT\n    \n    game = dict()\n\n    game['n_levers'] = 80\n    game['n_rounds'] = 2000\n    game['decay'] = 0.97\n    game['my_score'] = 0\n    game['opp_score'] = 0\n    game['beta_a'] = np.ones(game['n_levers'])\n    game['beta_b'] = np.ones(game['n_levers'])\n    game['my_pulls'] = np.zeros(game['n_levers'])\n    game['opp_pulls'] = np.zeros(game['n_levers'])\n    game['all_pulls'] = np.zeros(game['n_levers'])\n    game['true_prob'] = np.random.random(size=game['n_levers'])\n\n\n    est_method = np.random.choice(['ucb','thomson','mean'])\n    est_ucb_percentile = (0.25 + np.random.random()*0.75) if est_method == 'ucb' else None\n    pick_method = np.random.choice(['weighted','greedy','epsilon','random','stupid'])\n    pick_weighted_alpha = (1 + np.random.random()*5) if pick_method in ['weighted','epsilon'] else None\n    pick_epsilon = np.random.random() if pick_method in ['weighted','epsilon'] else -1\n    pick_epsilon_decay = np.random.random() if pick_method in ['weighted','epsilon'] else -1\n    min_opp_quality = np.random.random()\n    opp_retry_factor = np.random.random()*2\n    opp_pick_method = np.random.choice(['weighted','greedy','epsilon','random','stupid'])\n    opp_alpha = (1 + np.random.random()*3) if opp_pick_method in ['weighted','epsilon'] else None\n    opp_epsilon = np.random.random() if opp_pick_method in ['weighted','epsilon'] else None\n\n\n    config = dict(zip('est_method,est_ucb_percentile,pick_method,pick_weighted_alpha,pick_epsilon,pick_epsilon_decay,min_opp_quality,opp_retry_factor,opp_pick_method,opp_alpha,opp_epsilon'.split(','),\n                      (est_method,est_ucb_percentile,pick_method,pick_weighted_alpha,pick_epsilon,pick_epsilon_decay,min_opp_quality,opp_retry_factor,opp_pick_method,opp_alpha,opp_epsilon)))\n    game.update(config)\n\n\n    game = simulate(game)\n\n\n    game['score'] = -game['opp_score']+game['my_score']\n\n    return game\n    \n\nn_samples = 2000\nsamples = []\n    \nfor _ in tqdm(range(n_samples)):\n    samples.append(run_sim())\n    \n","056eef35":"data = pd.DataFrame(samples).sort_values('score',ascending=False)['score,my_score,opp_score,est_method,est_ucb_percentile,pick_method,pick_weighted_alpha,original_epsilon,pick_epsilon_decay,min_opp_quality,opp_pick_method,opp_alpha,opp_epsilon'.split(',')]\n\nto_study = data[~data['opp_pick_method'].isin(['stupid','random']) & ~data['pick_method'].isin(['stupid','random'])]","7a125104":"import matplotlib.pyplot as plt\nimport seaborn as sns\n\ndef analyse(df,cat='box'):\n    \n\n    for col in df.columns[3:]:\n        n_unique = df[col].nunique()\n        if n_unique < 10:\n            plt.figure(figsize=(16,5))\n            if cat == 'box':\n                ax = sns.boxplot(x=col, y=\"score\", data=df)\n            else:\n                ax = sns.violinplot(x=col, y=\"score\", data=df)\n            ax.set_title(col)\n        else:\n            ax = sns.lmplot(x=col, y=\"score\", data=df[df[col] != -1], height=5, aspect=2)\n            plt.title(col)\n        plt.show()\n","b8c04bdd":"analyse(data,cat='violin')","a3980c17":"analyse(to_study)","0523116c":"# Quick run","53f7208b":"# Tuning of parameters","f9f17233":"# Restricting to non-random\/stupid\n","3b001bbc":"- Hard to tell which sampling method performs best (will dig deeper)\n- A higher UCB percentile seems better\n- Stupid and random perform worst as expected ;-)\n- A lower alpha (less greedy) seems better\n- Slower epsilon decay seems best","50ea636e":"## In the hope of learning some new tricks, I am sharing my current thinking and code. \n\nThe code is commented, but let me know if you have any question ! :-)\n\nIn particular, I am keen to understand:\n- why the discrepancy between the way I compute the UCB bound and the Bayesian UCB starter notebook\n- ideas on how to better incorporate the opponent's actions (ie improving the opp_quality metric)\n- tuning of all parameters\n\n**Feedback and ideas for improvement welcome.**"}}