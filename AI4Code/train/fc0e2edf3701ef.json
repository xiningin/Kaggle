{"cell_type":{"7635d93a":"code","b129e707":"code","fa15cf82":"code","494e485f":"code","56d231ee":"code","84102426":"code","a255f02d":"code","bd672677":"code","777f939e":"code","3882e06f":"code","38e29714":"code","ce4a210b":"code","9afa9acf":"code","ba75980d":"code","9bded1d9":"code","79241734":"code","f8cf89a1":"code","3b0c1000":"code","ba2c2118":"code","f7eb3b12":"code","bc703e15":"code","0dc1d690":"code","f291f4e8":"code","da7e7f82":"code","2d5ad14e":"code","98ad2028":"code","aed081e9":"code","a9bf53a7":"code","4fd34f12":"code","63c5d78b":"code","241cfc46":"code","ad6d3c81":"code","dfa9a055":"code","2c0e0730":"markdown","a4da793f":"markdown","d11bf532":"markdown","32c7a620":"markdown","23213c62":"markdown","4fec7e27":"markdown","9d776f74":"markdown","a45c4258":"markdown","e75cb29c":"markdown","07a6bbb2":"markdown","f0479811":"markdown","7f631f7d":"markdown","af34685b":"markdown","48f48dc8":"markdown","51f3e4a5":"markdown","1dd967ae":"markdown","086dca0c":"markdown","31d9cb9d":"markdown","edb2f2e0":"markdown","8d5aa572":"markdown","19c5dc84":"markdown","ee1f7782":"markdown","efc2160a":"markdown","3d49e2a3":"markdown"},"source":{"7635d93a":"import torch\nimport torchvision\nfrom torchvision.transforms import ToTensor, Normalize, Compose\nfrom torchvision.datasets import MNIST\n\nmnist = MNIST(root='data', \n              train=True, \n              download=True,\n              transform=Compose([ToTensor(), Normalize(mean=(0.5,), std=(0.5,))]))","b129e707":"img, label = mnist[0]\nprint('Label: ', label)\nprint(img[:,10:15,10:15])\ntorch.min(img), torch.max(img)","fa15cf82":"def denorm(x):\n    out = (x + 1) \/ 2\n    return out.clamp(0, 1)","494e485f":"import matplotlib.pyplot as plt\n%matplotlib inline\n\nimg_norm = denorm(img)\nplt.imshow(img_norm[0], cmap='gray')\nprint('Label:', label)","56d231ee":"from torch.utils.data import DataLoader\n\nbatch_size = 100\ndata_loader = DataLoader(mnist, batch_size, shuffle=True)","84102426":"for img_batch, label_batch in data_loader:\n    print('first batch')\n    print(img_batch.shape)\n    plt.imshow(img_batch[0][0], cmap='gray')\n    print(label_batch)\n    break","a255f02d":"# Device configuration\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')","bd672677":"device","777f939e":"image_size = 784\nhidden_size = 256","3882e06f":"import torch.nn as nn\n\nD = nn.Sequential(\n    nn.Linear(image_size, hidden_size),\n    nn.LeakyReLU(0.2),\n    nn.Linear(hidden_size, hidden_size),\n    nn.LeakyReLU(0.2),\n    nn.Linear(hidden_size, 1),\n    nn.Sigmoid())","38e29714":"D.to(device);","ce4a210b":"latent_size = 64","9afa9acf":"G = nn.Sequential(\n    nn.Linear(latent_size, hidden_size),\n    nn.ReLU(),\n    nn.Linear(hidden_size, hidden_size),\n    nn.ReLU(),\n    nn.Linear(hidden_size, image_size),\n    nn.Tanh())","ba75980d":"y = G(torch.randn(2, latent_size))\ngen_imgs = denorm(y.reshape((-1, 28,28)).detach())","9bded1d9":"plt.imshow(gen_imgs[0], cmap='gray');","79241734":"plt.imshow(gen_imgs[1], cmap='gray');","f8cf89a1":"G.to(device);","3b0c1000":"criterion = nn.BCELoss()\nd_optimizer = torch.optim.Adam(D.parameters(), lr=0.0002)","ba2c2118":"def reset_grad():\n    d_optimizer.zero_grad()\n    g_optimizer.zero_grad()\n\ndef train_discriminator(images):\n    # Create the labels which are later used as input for the BCE loss\n    real_labels = torch.ones(batch_size, 1).to(device)\n    fake_labels = torch.zeros(batch_size, 1).to(device)\n        \n    # Loss for real images\n    outputs = D(images)\n    d_loss_real = criterion(outputs, real_labels)\n    real_score = outputs\n\n    # Loss for fake images\n    z = torch.randn(batch_size, latent_size).to(device)\n    fake_images = G(z)\n    outputs = D(fake_images)\n    d_loss_fake = criterion(outputs, fake_labels)\n    fake_score = outputs\n\n    # Combine losses\n    d_loss = d_loss_real + d_loss_fake\n    # Reset gradients\n    reset_grad()\n    # Compute gradients\n    d_loss.backward()\n    # Adjust the parameters using backprop\n    d_optimizer.step()\n    \n    return d_loss, real_score, fake_score","f7eb3b12":"g_optimizer = torch.optim.Adam(G.parameters(), lr=0.0002)","bc703e15":"def train_generator():\n    # Generate fake images and calculate loss\n    z = torch.randn(batch_size, latent_size).to(device)\n    fake_images = G(z)\n    labels = torch.ones(batch_size, 1).to(device)\n    g_loss = criterion(D(fake_images), labels)\n\n    # Backprop and optimize\n    reset_grad()\n    g_loss.backward()\n    g_optimizer.step()\n    return g_loss, fake_images","0dc1d690":"import os\n\nsample_dir = 'samples'\nif not os.path.exists(sample_dir):\n    os.makedirs(sample_dir)","f291f4e8":"from IPython.display import Image\nfrom torchvision.utils import save_image\n\n# Save some real images\nfor images, _ in data_loader:\n    images = images.reshape(images.size(0), 1, 28, 28)\n    save_image(denorm(images), os.path.join(sample_dir, 'real_images.png'), nrow=10)\n    break\n   \nImage(os.path.join(sample_dir, 'real_images.png'))","da7e7f82":"sample_vectors = torch.randn(batch_size, latent_size).to(device)\n\ndef save_fake_images(index):\n    fake_images = G(sample_vectors)\n    fake_images = fake_images.reshape(fake_images.size(0), 1, 28, 28)\n    fake_fname = 'fake_images-{0:0=4d}.png'.format(index)\n    print('Saving', fake_fname)\n    save_image(denorm(fake_images), os.path.join(sample_dir, fake_fname), nrow=10)\n    \n# Before training\nsave_fake_images(0)\nImage(os.path.join(sample_dir, 'fake_images-0000.png'))","2d5ad14e":"%%time\n\nnum_epochs = 300\ntotal_step = len(data_loader)\nd_losses, g_losses, real_scores, fake_scores = [], [], [], []\n\nfor epoch in range(num_epochs):\n    for i, (images, _) in enumerate(data_loader):\n        # Load a batch & transform to vectors\n        images = images.reshape(batch_size, -1).to(device)\n        \n        # Train the discriminator and generator\n        d_loss, real_score, fake_score = train_discriminator(images)\n        g_loss, fake_images = train_generator()\n        \n        # Inspect the losses\n        if (i+1) % 200 == 0:\n            d_losses.append(d_loss.item())\n            g_losses.append(g_loss.item())\n            real_scores.append(real_score.mean().item())\n            fake_scores.append(fake_score.mean().item())\n            print('Epoch [{}\/{}], Step [{}\/{}], d_loss: {:.4f}, g_loss: {:.4f}, D(x): {:.2f}, D(G(z)): {:.2f}' \n                  .format(epoch, num_epochs, i+1, total_step, d_loss.item(), g_loss.item(), \n                          real_score.mean().item(), fake_score.mean().item()))\n        \n    # Sample and save images\n    save_fake_images(epoch+1)","98ad2028":"# Save the model checkpoints \ntorch.save(G.state_dict(), 'G.ckpt')\ntorch.save(D.state_dict(), 'D.ckpt')","aed081e9":"Image('.\/samples\/fake_images-0010.png')","a9bf53a7":"Image('.\/samples\/fake_images-0050.png')","4fd34f12":"Image('.\/samples\/fake_images-0100.png')","63c5d78b":"Image('.\/samples\/fake_images-0300.png')","241cfc46":"import cv2\nimport os\nfrom IPython.display import FileLink\n\nvid_fname = 'gans_training.avi'\n\nfiles = [os.path.join(sample_dir, f) for f in os.listdir(sample_dir) if 'fake_images' in f]\nfiles.sort()\n\nout = cv2.VideoWriter(vid_fname,cv2.VideoWriter_fourcc(*'MP4V'), 8, (302,302))\n[out.write(cv2.imread(fname)) for fname in files]\nout.release()\nFileLink('gans_training.avi')","ad6d3c81":"plt.plot(d_losses, '-')\nplt.plot(g_losses, '-')\nplt.xlabel('epoch')\nplt.ylabel('loss')\nplt.legend(['Discriminator', 'Generator'])\nplt.title('Losses');","dfa9a055":"plt.plot(real_scores, '-')\nplt.plot(fake_scores, '-')\nplt.xlabel('epoch')\nplt.ylabel('score')\nplt.legend(['Real Score', 'Fake score'])\nplt.title('Scores');","2c0e0730":"We use the TanH activation function for the output layer of the generator.\n\n<img src=\"https:\/\/nic.schraudolph.org\/teach\/NNcourse\/figs\/tanh.gif\" width=\"420\" >\n\n> \"The ReLU activation (Nair & Hinton, 2010) is used in the generator with the exception of the output layer which uses the Tanh function. We observed that using a bounded activation allowed the model to learn more quickly to saturate and cover the color space of the training distribution. Within the discriminator we found the leaky rectified activation (Maas et al., 2013) (Xu et al., 2015) to work well, especially for higher resolution modeling.\" - [Source](https:\/\/stackoverflow.com\/questions\/41489907\/generative-adversarial-networks-tanh)\n\n\nNote that since the outputs of the TanH activation lie in the range `[-1,1]`, we have applied the same transformation to the images in the training dataset. Let's generate an output vector using the generator and view it as an image by transforming and denormalizing the output.","a4da793f":"## Generator Training\n\nSince the outputs of the generator are images, it's not obvious how we can train the generator. This is where we employ a rather elegant trick, which is to use the discriminator as a part of the loss function. Here's how it works:\n\n- We generate a batch of images using the generator, pass the into the discriminator.\n\n- We calculate the loss by setting the target labels to 1 i.e. real. We do this because the generator's objective is to \"fool\" the discriminator.\u00a0\n\n- We use the loss to perform gradient descent i.e. change the weights of the generator, so it gets better at generating real-like images.\n\nHere's what this looks like in code.","d11bf532":"Note that we are are transforming the pixel values from the range `[0, 1]` to the range `[-1, 1]`. The reason for doing this will become clear when define the generator network. Let's look at a sample tensor from the data.","32c7a620":"Let's save a batch of real images that we can use for visual comparision while looking at the generated images.","23213c62":"## Training the Model\n\nLet's create a directory where we can save intermediate outputs from the generator to visually inspect the progress of the model","4fec7e27":"## Generator Network\n\nThe input to the generator is typically a vector or a matrix which is used as a seed for generating an image. Once again, to keep things simple, we'll use a feedfoward neural network with 3 layers, and the output will be a vector of size 784, which can be transformed to a 28x28 px image.","9d776f74":"We are now ready to train the model. In each epoch, we train the discriminator first, and then the generator. The training might take a while if you're not using a GPU.","a45c4258":"## Discriminator Network\n\nThe discriminator takes an image as input, and tries to classify it as \"real\" or \"generated\". In this sense, it's like any other neural network. While we can use a CNN for the discriminator, we'll use a simple feedforward network with 3 linear layers to keep things since. We'll treat each 28x28 image as a vector of size 784.","e75cb29c":"We'll also create a `device` which can be used to move the data and models to a GPU, if one is available.","07a6bbb2":"Let's define helper functions to reset gradients and  train the discriminator.","f0479811":"## Discriminator Training\n\nSince the discriminator is a binary classification model, we can use the binary cross entropy loss function to quantify how well it is able to differentiate between real and generated images.\n\n<img src=\"https:\/\/image.slidesharecdn.com\/chrishokamp-dublinnlp3-160805110319\/95\/task-based-learning-for-nlp-going-beyond-cross-entropy-chris-hokamp-10-638.jpg?cb=1470395213\" width=\"420\" >","7f631f7d":"We'll also define a helper function to save a batch of generated images to disk at the end of every epoch. We'll use a fixed set of input vectors to the generator to see how the individual generated images evolve over time as we train the model.","af34685b":"Now that we have trained the models, we can save checkpoints.","48f48dc8":"Here's how the generated images look, after the 10th, 50th, 100th and 300th epochs of training.","51f3e4a5":"We use the Leaky ReLU activation for the discriminator.\n\n<img src=\"https:\/\/cdn-images-1.medium.com\/max\/1600\/1*ypsvQH7kvtI2BhzR2eT_Sw.png\" width=\"420\">\n\n\n>  Different from the regular ReLU function, Leaky ReLU allows the pass of a small gradient signal for negative values. As a result, it makes the gradients from the discriminator flows stronger into the generator. Instead of passing a gradient (slope) of 0 in the back-prop pass, it passes a small negative gradient.  - [Source](https:\/\/sthalles.github.io\/advanced_gans\/)\n\nJust like any other binary classification model, the output of the discriminator is a single number between 0 and 1, which can be interpreted as the probability of the input image being fake i.e. generated.\n\nLet's move the discriminator model to the chosen device.","1dd967ae":"# Generative Adverserial Networks in PyTorch\n\nThis code is from a Zero to GANs course i did with Jovian.ml\nSource: https:\/\/jovian.ml\/aakashns\/06-mnist-gan\n\n\n\nIf you like this, you may also enjoy, GAN to make Anime Faces!\n\nLink: https:\/\/www.kaggle.com\/kmldas\/gan-in-pytorch-deep-fake-anime-faces\n\n\n\n\nDeep neural networks are used mainly for supervised learning: classification or regression. Generative Adverserial Networks or GANs, however, use neural networks for a very different purpose: Generative modeling\n\n> Generative modeling is an unsupervised learning task in machine learning that involves automatically discovering and learning the regularities or patterns in input data in such a way that the model can be used to generate or output new examples that plausibly could have been drawn from the original dataset. - [Source](https:\/\/machinelearningmastery.com\/what-are-generative-adversarial-networks-gans\/)\n\nWhile there are many approaches used for generative modeling, a Generative Adverserial Network takes the following approach: \n\n![GAN Flowchart](https:\/\/i.imgur.com\/6NMdO9u.png)\n\nThere are two neural networks: a *Generator* and a *Discriminator*. The generator generates a \"fake\" sample given a random vector\/matrix, and the discriminator attempts to detect whether a given sample is \"real\" (picked from the training data) or \"fake\" (generated by the generator). Training happens in tandem: we train the discriminator for a few epochs, then train the generator for a few epochs, and repeat. This way both the generator and the discriminator get better at doing their jobs. This rather simple approach can lead to some astounding results. The following images ([source](https:\/\/machinelearningmastery.com\/resources-for-getting-started-with-generative-adversarial-networks\/)), for instances, were all generated using GANs:\n\n<img src=\"https:\/\/3qeqpr26caki16dnhd19sv6by6v-wpengine.netdna-ssl.com\/wp-content\/uploads\/2019\/04\/Example-of-Photorealistic-Human-Faces-Generated-by-a-GAN.png\" alt=\"gans_results\" width=\"480\">\n\n\nGANs however, can be notoriously difficult to train, and are extremely sensitive to hyperparameters, activation functions and regularization. In this tutorial, we'll train a GAN to generate images of handwritten digits similar to those from the MNIST database.\n\n<img src=\"https:\/\/i.imgur.com\/CAYnuo1.jpg\" width=\"360\" >\n\nMost of the code for this tutorial has been borrowed for this excellent repository of PyTorch tutorials: [github.com\/yunjey\/pytorch-tutorial](https:\/\/github.com\/yunjey\/pytorch-tutorial). Here's what we're going to do:\n\n* Define the problem statement\n* Load the data (with transforms and normalization)\n    * Denormalize for visual inspection of samples\n* Define the Discriminator network\n    * Study the activation function: Leaky ReLU\n* Define the Generator network\n    * Explain the output activation function: TanH\n    * Look at some sample outputs\n* Define losses, optimizers and helper functions for training\n    * For discriminator\n    * For generator\n* Train the model\n    * Save intermediate generated images to file\n* Look at some outputs\n* Save the models\n* Commit to Jovian.ml","086dca0c":"# Try more! \n\nIf you like this, you may also enjoy, GAN to make Anime Faces!\n\nLink: https:\/\/www.kaggle.com\/kmldas\/gan-in-pytorch-deep-fake-anime-faces","31d9cb9d":"As expected, the pixel values range from -1 to 1. Let's define a helper to denormalize and view the images. This function will also be useful for viewing the generated images.","edb2f2e0":"You can view the animated training video here: https:\/\/www.youtube.com\/watch?v=R7HTX79JlBg\n\nWe can also visualize how the loss changes over time. Visualizing losses is quite useful for debugging the training process. For GANs, we expect the generator's loss to reduce over time, without the discriminator's loss getting too high.","8d5aa572":"We can visualize the training process by combining the sample images generated after each epoch into a video using OpenCV.","19c5dc84":"## Load the Data\n\nWe begin by downloading and importing the data as a PyTorch dataset using the `MNIST` helper class from `torchvision.datasets`.","ee1f7782":"Here are the steps involved in training the discriminator.\n\n- We expect the discriminator to output 1 if the image was picked from the real MNIST dataset, and 0 if it was generated.\u00a0\n\n- We first pass a batch of real images, and compute the loss, setting the target labels to 1.\u00a0\n\n- Then, we generate a batch of fake images using the generator, pass them into the discriminator, and compute the loss, setting the target labels to 0.\u00a0\n\n- Finally we add the two losses and use the overall loss to perform gradient descent to adjust the weights of the discriminator.\n\nIt's important to note that we don't change the weights of the generator model while training the discriminator (`d_optimizer` only affects the `D.parameters()`)","efc2160a":"Finally, let's create a dataloader to load the images in batches.","3d49e2a3":"As one might expect, the output from the generator is basically random noise. Let's define a helper function which can save a batch of outputs from the generator to a file.\n\nLet's move the generator to the chosen device."}}