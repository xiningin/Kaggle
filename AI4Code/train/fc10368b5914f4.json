{"cell_type":{"e6b6b37e":"code","58d07d82":"code","e167c17a":"code","702cfbb0":"code","4d3e4098":"code","eedbace2":"code","fe981773":"code","bc84e502":"code","1fd8a343":"code","39027460":"code","8c95d678":"code","13d98fc4":"code","e623e72a":"code","eec1e06b":"code","bcc48236":"code","94c9671a":"code","6db79e08":"code","ce76c5f5":"code","6e639321":"code","4992aa9a":"code","3df45ff6":"code","ec024839":"code","baeebf5e":"markdown","22c5a31c":"markdown","9a1c94d0":"markdown","1167c072":"markdown","b7edf52f":"markdown","e81ec893":"markdown","9a4dd2b9":"markdown","120a0b08":"markdown","1211e90b":"markdown","1f4f1348":"markdown","b5dcda17":"markdown"},"source":{"e6b6b37e":"# Importing libraries\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd","58d07d82":"# Configuring visualizations\n%matplotlib inline\nplt.rcParams['figure.figsize'] = 12, 8","e167c17a":"# Setting random state\nRANDOM_STATE = 123","702cfbb0":"# Loading datasets\ntrain_set = pd.read_csv('..\/input\/titanic\/train.csv')\ntest_set = pd.read_csv('..\/input\/titanic\/test.csv')\nX_train = train_set.iloc[:, [2, 4, 5, 6, 7, 9, 11]].values\nX_test = test_set.iloc[:, [1, 3, 4, 5, 6, 8, 10]].values\ny_train = train_set.iloc[:, 1].values","4d3e4098":"# Exploring train set\ntrain_set.info()\ntrain_set.describe(include='all')","eedbace2":"# Exploring test set\ntest_set.info()\ntest_set.describe(include='all')","fe981773":"# Taking care of missing data (Age, Embarked, Fare)\nfrom sklearn.impute import SimpleImputer\nimputer_age = SimpleImputer(missing_values=np.nan, strategy='mean')\nX_train[:, 2:3] = imputer_age.fit_transform(X_train[:, 2:3])\nX_test[:, 2:3] = imputer_age.fit_transform(X_test[:, 2:3])\nimputer_embarked = SimpleImputer(missing_values=np.nan, strategy='most_frequent')\nX_train[:, 6:7] = imputer_embarked.fit_transform(X_train[:, 6:7])\nimputer_fare = SimpleImputer(missing_values=np.nan, strategy='median')\nX_test[:, 5:6] = imputer_fare.fit_transform(X_test[:, 5:6])","bc84e502":"# Encoding categorical data (PClass, Sex, Embarked)\nfrom sklearn.preprocessing import LabelEncoder\nlabelencoder_pclass = LabelEncoder()\nX_train[:, 0] = labelencoder_pclass.fit_transform(X_train[:, 0])\nX_test[:, 0] = labelencoder_pclass.transform(X_test[:, 0])\nlabelencoder_sex = LabelEncoder()\nX_train[:, 1] = labelencoder_sex.fit_transform(X_train[:, 1])\nX_test[:, 1] = labelencoder_sex.transform(X_test[:, 1])\nlabelencoder_embarked = LabelEncoder()\nX_train[:, 6] = labelencoder_embarked.fit_transform(X_train[:, 6])\nX_test[:, 6] = labelencoder_embarked.transform(X_test[: ,6])","1fd8a343":"# Plotting OOB scores against number of trees and number of features\nfrom collections import OrderedDict\nfrom sklearn.ensemble import RandomForestClassifier\nensemble_clfs = [     \n                 ('max_features=2', RandomForestClassifier(max_features=2)), \n#                 ('max_features=3', RandomForestClassifier(max_features=3)), \n#                 ('max_features=4', RandomForestClassifier(max_features=4)), \n#                 ('max_features=5', RandomForestClassifier(max_features=5)), \n#                 ('max_features=6', RandomForestClassifier(max_features=6)), \n                 ('max_features=7', RandomForestClassifier(max_features=7))\n                 ]\nerror_rate = OrderedDict((label, []) for label, _ in ensemble_clfs)\n[min_estimators, max_estimators] = [20, 750]\nfor label, clf in ensemble_clfs:\n    for i in range(min_estimators, max_estimators + 1, 10):\n        clf.set_params(n_estimators=i, warm_start=True, oob_score=True, \n                       min_impurity_decrease=1e-4, random_state=RANDOM_STATE)\n        clf.fit(X_train, y_train)\n        error_rate[label].append((i, clf.oob_score_))\nfor label, clf_err in error_rate.items():\n    xs, ys = zip(*clf_err)\n    plt.plot(xs, ys, label=label)\nplt.xlim(min_estimators, max_estimators)\nplt.xlabel('n_estimators')\nplt.ylabel('OOB error rate')\nplt.legend(loc='best')\nplt.grid()\nplt.show()","39027460":"# Building a Random Forest model\nfrom sklearn.ensemble import RandomForestClassifier\nclassifier = RandomForestClassifier(n_estimators=400, bootstrap=True, \n                                    criterion='gini', max_depth=None, \n                                    min_samples_split=2, min_samples_leaf=1, \n                                    max_features='auto', max_leaf_nodes=None, \n                                    min_impurity_decrease=1e-3, oob_score=True, \n                                    n_jobs=-1, random_state=RANDOM_STATE)\nclassifier.fit(X_train, y_train)\nprint('OOB score:', classifier.oob_score_)\nprint('feature importances:', classifier.feature_importances_)","8c95d678":"# Cross validation\nfrom sklearn.model_selection import cross_validate\nresults = cross_validate(classifier, X_train, y_train, cv=10, scoring='accuracy',\n                         return_train_score=True, return_estimator=False, n_jobs=-1)\nprint('train score:', results['train_score'].mean())\nprint('test score:', results['test_score'].mean())","13d98fc4":"# Plotting learning curves\nfrom sklearn.model_selection import learning_curve\nm_range = np.linspace(.05, 1.0, 20)\ntrain_sizes, train_scores, test_scores = learning_curve(classifier, X_train, y_train, cv=10,  \n                                                        train_sizes=m_range, shuffle=False,\n                                                        scoring='accuracy', n_jobs=-1)\nplt.figure()\nplt.title('Learning Curves')\nplt.ylim(.6, 1.05)\nplt.xlabel('Training examples')\nplt.ylabel('Score')\ntrain_scores_mean = np.mean(train_scores, axis=1)\ntrain_scores_std = np.std(train_scores, axis=1)\ntest_scores_mean = np.mean(test_scores, axis=1)\ntest_scores_std = np.std(test_scores, axis=1)\nplt.grid()\nplt.plot(train_sizes, train_scores_mean, 'o-', color='r', label='Training score')\nplt.plot(train_sizes, test_scores_mean, 'o-', color='g', label='Cross-validation score')\nplt.fill_between(train_sizes, train_scores_mean - train_scores_std, \n                 train_scores_mean + train_scores_std, alpha=0.1, color='r')\nplt.fill_between(train_sizes, test_scores_mean - test_scores_std, \n                 test_scores_mean + test_scores_std, alpha=0.1, color='g')\nplt.legend(loc='best')\nplt.show()","e623e72a":"# Plotting validation curves\nfrom sklearn.model_selection import validation_curve\nparam_range = np.geomspace(1e-5, 1e-1, 21)\ntrain_scores, test_scores = validation_curve(classifier, X_train, y_train, cv=10,\n                                             param_name='min_impurity_decrease', \n                                             param_range=param_range, \n                                             scoring='accuracy', n_jobs=-1)\nplt.figure()\nplt.title('Validation Curves')\nplt.ylim(.7, 1.05)\nplt.xlabel('Size of Trees')\nplt.ylabel('Score')\ntrain_scores_mean = np.mean(train_scores, axis=1)\ntrain_scores_std = np.std(train_scores, axis=1)\ntest_scores_mean = np.mean(test_scores, axis=1)\ntest_scores_std = np.std(test_scores, axis=1)\nplt.grid()\nplt.semilogx(param_range, train_scores_mean, label='Training score', color='darkorange', lw=2)\nplt.semilogx(param_range, test_scores_mean, label='Cross-validation score', color='navy', lw=2)\nplt.fill_between(param_range, train_scores_mean - train_scores_std, \n                 train_scores_mean + train_scores_std, alpha=0.2, color='darkorange', lw=2)\nplt.fill_between(param_range, test_scores_mean - test_scores_std,\n                 test_scores_mean + test_scores_std, alpha=0.2, color='navy', lw=2)\nplt.legend(loc='best')\nplt.show()","eec1e06b":"# Grid search\nfrom sklearn.model_selection import GridSearchCV\nparam_grid = {'n_estimators': [400], 'max_features': ['auto', None], \n              'min_impurity_decrease': list(np.geomspace(1e-4, 1e-3, 6))}\ngrid_search = GridSearchCV(classifier, param_grid, iid=False, refit=True, cv=10,\n                           return_train_score=False, scoring='accuracy', n_jobs=-1)\ngrid_search.fit(X_train, y_train)\nprint('best parameters:', grid_search.best_params_)\nprint('best score:', grid_search.best_score_)","bcc48236":"# Createing a new feature representing family sizes and conducting preprocessing\nfamily_train = X_train[:, 3] + X_train[:, 4] + 1\nfamily_train = family_train.reshape(len(family_train), 1)\nfamily_test = X_test[:, 3] + X_test[:, 4] + 1\nfamily_test = family_test.reshape(len(family_test), 1)\nX_train = np.concatenate((X_train[:, :3], family_train, X_train[:, 5:]), axis=1)\nX_test = np.concatenate((X_test[:, :3], family_test, X_test[:, 5:]), axis=1)\ndel family_train, family_test","94c9671a":"# Dropping feature Embarked\nX_train = X_train[:, :-1]\nX_test = X_test[:, :-1]","6db79e08":"# Building a Random Forest model\nfrom sklearn.ensemble import RandomForestClassifier\nbest_mid = grid_search.best_params_['min_impurity_decrease']\nclassifier = RandomForestClassifier(n_estimators=400, bootstrap=True, \n                                    criterion='gini', max_depth=None, \n                                    min_samples_split=2, min_samples_leaf=1, \n                                    max_features='auto', max_leaf_nodes=None, \n                                    min_impurity_decrease=best_mid, oob_score=True, \n                                    n_jobs=-1, random_state=RANDOM_STATE)\nclassifier.fit(X_train, y_train)\nprint('OOB score:', classifier.oob_score_)\nprint('feature importances:', classifier.feature_importances_)","ce76c5f5":"# Cross validation\nfrom sklearn.model_selection import cross_validate\nresults = cross_validate(classifier, X_train, y_train, cv=10, scoring='accuracy',\n                         return_train_score=True, return_estimator=False, n_jobs=-1)\nprint('train score:', results['train_score'].mean())\nprint('test score:', results['test_score'].mean())","6e639321":"# Plotting learning curves\nfrom sklearn.model_selection import learning_curve\nm_range = np.linspace(.05, 1.0, 20)\ntrain_sizes, train_scores, test_scores = learning_curve(classifier, X_train, y_train, cv=10,  \n                                                        train_sizes=m_range, shuffle=False,\n                                                        scoring='accuracy', n_jobs=-1)\nplt.figure()\nplt.title('Learning Curves')\nplt.ylim(.6, 1.05)\nplt.xlabel('Training examples')\nplt.ylabel('Score')\ntrain_scores_mean = np.mean(train_scores, axis=1)\ntrain_scores_std = np.std(train_scores, axis=1)\ntest_scores_mean = np.mean(test_scores, axis=1)\ntest_scores_std = np.std(test_scores, axis=1)\nplt.grid()\nplt.plot(train_sizes, train_scores_mean, 'o-', color='r', label='Training score')\nplt.plot(train_sizes, test_scores_mean, 'o-', color='g', label='Cross-validation score')\nplt.fill_between(train_sizes, train_scores_mean - train_scores_std, \n                 train_scores_mean + train_scores_std, alpha=0.1, color='r')\nplt.fill_between(train_sizes, test_scores_mean - test_scores_std, \n                 test_scores_mean + test_scores_std, alpha=0.1, color='g')\nplt.legend(loc='best')\nplt.show()","4992aa9a":"# Plotting validation curves\nfrom sklearn.model_selection import validation_curve\nparam_range = np.geomspace(1e-5, 1e-1, 21)\ntrain_scores, test_scores = validation_curve(classifier, X_train, y_train, cv=10,\n                                             param_name='min_impurity_decrease', \n                                             param_range=param_range, \n                                             scoring='accuracy', n_jobs=-1)\nplt.figure()\nplt.title('Validation Curves')\nplt.ylim(.7, 1.05)\nplt.xlabel('Size of Trees')\nplt.ylabel('Score')\ntrain_scores_mean = np.mean(train_scores, axis=1)\ntrain_scores_std = np.std(train_scores, axis=1)\ntest_scores_mean = np.mean(test_scores, axis=1)\ntest_scores_std = np.std(test_scores, axis=1)\nplt.grid()\nplt.semilogx(param_range, train_scores_mean, label='Training score', color='darkorange', lw=2)\nplt.semilogx(param_range, test_scores_mean, label='Cross-validation score', color='navy', lw=2)\nplt.fill_between(param_range, train_scores_mean - train_scores_std, \n                 train_scores_mean + train_scores_std, alpha=0.2, color='darkorange', lw=2)\nplt.fill_between(param_range, test_scores_mean - test_scores_std,\n                 test_scores_mean + test_scores_std, alpha=0.2, color='navy', lw=2)\nplt.legend(loc='best')\nplt.show()","3df45ff6":"# Grid search\nfrom sklearn.model_selection import GridSearchCV\nparam_grid = {'n_estimators': [400], 'max_features': [2, 3, 4, 5], \n              'min_impurity_decrease': list(np.geomspace(1e-4, 1e-2, 11))}\ngrid_search = GridSearchCV(classifier, param_grid, iid=False, refit=True, cv=10,\n                           return_train_score=False, scoring='accuracy', n_jobs=-1)\ngrid_search.fit(X_train, y_train)\nprint('best parameters:', grid_search.best_params_)\nprint('best score:', grid_search.best_score_)","ec024839":"# Making predictions and submitting\ny_pred = grid_search.predict(X_test)\nsubmission = pd.DataFrame({'PassengerId': test_set.iloc[:, 0].values,\n                           'Survived': y_pred})\nsubmission.to_csv('submission_RandomForest.csv', index=False)","baeebf5e":"We'd like to quickly build a messy model before heading into further process of model refinement. Obviously, features like Age, Sex and PClass are more likely to be linked with survival probabilities of passengers. We also want to include features like SibSp, ParCh, Fare, Embarked into our first model before deciding whether to keep them. Features like Name, Cabin and Ticket contain may contain information to be find, but processing these features my be kind of uncertain and bothering, and Cabin contains lots of missing values. We want to leave these features for our last considerastion to engineer.","22c5a31c":"The plot above shows that OOB scores become stable after the number of trees reaches 400. We shall set n_estimator to be 400, and still use cross validation and grid search to find an optimal max_features.","9a1c94d0":"Now we are ready to build our first Random Forest model. Both OOB score and cross validation score give us a good indication of how our model works. In addition, we can see that these two scores are fairly close to each other. ","1167c072":"We can use various arguments to control the sizes of trees, such as max_depth, min_samples_split, min_samples_leaf, max_leaf_nodes and min_impurity_decrease. These arguments have similar effects and we choose the last one over others for no specific reasons. ","b7edf52f":"The main parameters to adjust when using Random Forest algorithm is the number of trees and the size of the random subsets of features to consider when splittinga node. The larger the better for the number of trees, but also the longer it will take to compute. In addition, results will stop getting significantly better beyond a critical number of trees. For the size of the random subsets of features, the lower the greater the reduction of variance, but also the greater the increase in bias. We want to snatch an impression about appropriate choices for the two parameters early, since it may be time-consuming to cross-validate. OOB scores provide us with a helpful tool since they allow the Random Forest classifier to be fit and validated whilst being trained. ","e81ec893":"In tree-based algorithms, random state has to be fixed to obtain a deterministic behaviour during fitting. We can change RANDOM_STATE in the shell below to obtain different outputs.","9a4dd2b9":"We can still plot learning curves to give our model a quick diagnosis. The training curve and the cross-validation cruve didn't converge in the plot below thus showed a moderate sign of overfitting. We can tune the parameters max_features to reduce the variance. In addition, we can control the sizes of trees to reduce overfitting. Other possible measures includes manually deleting some features. ","120a0b08":"We also obtained an attribute named feature importance previously. It seems that features SibSp, ParCh and Embarked are not really significant in our model. Let's create a new feature by combining the former two and drop the last one.","1211e90b":"References:\n\n[Decision Trees](https:\/\/scikit-learn.org\/stable\/modules\/tree.html#tree)\n\n[Forests of randomized trees](https:\/\/scikit-learn.org\/stable\/modules\/ensemble.html#forest)\n\n[OOB Scores for Random Forest](https:\/\/scikit-learn.org\/stable\/auto_examples\/ensemble\/plot_ensemble_oob.html#)\n\n[Plotting Learning Curves](https:\/\/scikit-learn.org\/stable\/auto_examples\/model_selection\/plot_learning_curve.html#sphx-glr-auto-examples-model-selection-plot-learning-curve-py)\n\n[Plottign Validation Curves](https:\/\/scikit-learn.org\/stable\/auto_examples\/model_selection\/plot_validation_curve.html#sphx-glr-auto-examples-model-selection-plot-validation-curve-py)","1f4f1348":"One of the advantages of tree-based algorithms is that they need little data preprocessing. We still need to encode categorical data and to take care of missing data, but creating dummy variables or feature scaling are not indispensible any more. ","b5dcda17":"Now we can submit our predictions."}}