{"cell_type":{"69881012":"code","831501db":"code","cb63d824":"code","aa376cbf":"code","169289a7":"code","73576799":"code","582f31b6":"code","5e3481a2":"code","07998b7e":"code","712558fc":"code","f30ea8e8":"code","30411078":"code","9a31f7da":"code","c9a52aac":"code","dd764c65":"code","232a4429":"code","025d2cd7":"code","c4fdae57":"code","b7f16bfc":"code","3a214f21":"code","05bf9eb2":"code","365ea191":"code","55fa1da3":"code","b2a919b4":"code","ac21679b":"code","5c74a863":"code","9d51476d":"code","47b85ded":"code","9006d781":"code","c0965278":"code","193cf1df":"code","bc34ce8a":"code","363bb947":"code","0ddf0324":"code","8961c12c":"code","d5ecb43a":"code","9fd29cbf":"markdown","1daed977":"markdown","22630636":"markdown","35340520":"markdown","dca14d44":"markdown","e9fc4b20":"markdown","51e49acb":"markdown","7d351aeb":"markdown","47d89ab7":"markdown","aa831e4c":"markdown","abb48cfc":"markdown","c0faeea6":"markdown","92c2d9d7":"markdown","89327ba8":"markdown"},"source":{"69881012":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","831501db":"#Importing required packages.\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.metrics import confusion_matrix, classification_report\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder\nfrom sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n%matplotlib inline","cb63d824":"wine = pd.read_csv('..\/input\/red-wine-quality-cortez-et-al-2009\/winequality-red.csv')","aa376cbf":"wine.head()","169289a7":"wine.info()","73576799":"#Here we see that fixed acidity does not give any specification to classify the quality.\nfig = plt.figure(figsize = (10,5))\nsns.barplot(x = 'quality', y = 'fixed acidity', data = wine)","582f31b6":"#Here we see that its quite a downing trend in the volatile acidity as we go higher the quality \nfig = plt.figure(figsize = (10,6))\nsns.barplot(x = 'quality', y = 'volatile acidity', data = wine)","5e3481a2":"#Composition of citric acid go higher as we go higher in the quality of the wine\nfig = plt.figure(figsize = (10,6))\nsns.barplot(x = 'quality', y = 'citric acid', data = wine)","07998b7e":"#Residual sugar doesn't have such important info on quality\nfig = plt.figure(figsize = (10,6))\nsns.barplot(x = 'quality', y = 'residual sugar', data = wine)","712558fc":"#Composition of chloride also go down as we go higher in the quality of the wine\nfig = plt.figure(figsize = (10,6))\nsns.barplot(x = 'quality', y = 'chlorides', data = wine)","f30ea8e8":"#No important info can be be figured out from here \nfig = plt.figure(figsize = (10,6))\nsns.barplot(x = 'quality', y = 'free sulfur dioxide', data = wine)","30411078":"fig = plt.figure(figsize = (10,6))\nsns.barplot(x = 'quality', y = 'total sulfur dioxide', data = wine)","9a31f7da":"#Sulphates level goes higher with the quality of wine\nfig = plt.figure(figsize = (10,6))\nsns.barplot(x = 'quality', y = 'sulphates', data = wine)","c9a52aac":"#Alcohol level also goes higher as te quality of wine increases\nfig = plt.figure(figsize = (10,6))\nsns.barplot(x = 'quality', y = 'alcohol', data = wine)","dd764c65":"#Making binary classificaion for the response variable.\n#Dividing wine as good and bad by giving the limit for the quality\nbins = (2, 6.5, 8)\ngroup_names = ['bad', 'good']\nwine['quality'] = pd.cut(wine['quality'], bins = bins, labels = group_names)","232a4429":"#Now lets assign a labels to our quality variable\nlabel_quality = LabelEncoder()","025d2cd7":"#Bad becomes 0 and good becomes 1 \nwine['quality'] = label_quality.fit_transform(wine['quality'])","c4fdae57":"wine['quality'].value_counts()","b7f16bfc":"sns.countplot(wine['quality'])","3a214f21":"#Now seperate the dataset as response variable and feature variabes\nX = wine.drop('quality', axis = 1)\ny = wine['quality']","05bf9eb2":"#Train and Test splitting of data \nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42)","365ea191":"#Applying Standard scaling to get optimized result\nsc = StandardScaler()","55fa1da3":"X_train = sc.fit_transform(X_train)\nX_test = sc.fit_transform(X_test)","b2a919b4":"rfc = RandomForestClassifier(n_estimators=200)\nrfc.fit(X_train, y_train)\npred_rfc = rfc.predict(X_test)","ac21679b":"#Let's see how our model performed\nprint(classification_report(y_test, pred_rfc))","5c74a863":"#Confusion matrix for the random forest classification\nprint(confusion_matrix(y_test, pred_rfc))","9d51476d":"sgd = SGDClassifier(penalty=None)\nsgd.fit(X_train, y_train)\npred_sgd = sgd.predict(X_test)","47b85ded":"print(classification_report(y_test, pred_sgd))","9006d781":"print(confusion_matrix(y_test, pred_sgd))","c0965278":"svc = SVC()\nsvc.fit(X_train, y_train)\npred_svc = svc.predict(X_test)","193cf1df":"print(classification_report(y_test, pred_svc))","bc34ce8a":"#Finding best parameters for our SVC model\nparam = {\n    'C': [0.1,0.8,0.9,1,1.1,1.2,1.3,1.4],\n    'kernel':['linear', 'rbf'],\n    'gamma' :[0.1,0.8,0.9,1,1.1,1.2,1.3,1.4]\n}\ngrid_svc = GridSearchCV(svc, param_grid=param, scoring='accuracy', cv=10)","363bb947":"grid_svc.fit(X_train, y_train)","0ddf0324":"#Best parameters for our svc model\ngrid_svc.best_params_","8961c12c":"#Let's run our SVC again with the best parameters.\nsvc2 = SVC(C = 1.2, gamma =  0.9, kernel= 'rbf')\nsvc2.fit(X_train, y_train)\npred_svc2 = svc2.predict(X_test)\nprint(classification_report(y_test, pred_svc2))","d5ecb43a":"#Now lets try to do some evaluation for random forest model using cross validation.\nrfc_eval = cross_val_score(estimator = rfc, X = X_train, y = y_train, cv = 10)\nrfc_eval.mean()","9fd29cbf":"84% accuracy using stochastic gradient descent classifier","1daed977":"Machine learning algorithm","22630636":"Support vector classifier gets 86%","35340520":"Grid Search CV","dca14d44":"Stochastic Gradient Decent Classifier","e9fc4b20":"SVC improves from 86% to 90% using Grid Search CV","51e49acb":"Cross Validation to increase Accuracy","7d351aeb":"Support Vector Classifier","47d89ab7":"Preprocessing Data","aa831e4c":"Random forest gives the accuracy of 87%","abb48cfc":"Random forest accuracy increases from 87% to 91 % using cross validation score","c0faeea6":"Cross Validation Score for random forest and SGD","92c2d9d7":"**Study and Analysis of Data with respect to dependent variable**","89327ba8":"Random Forest Classifier"}}