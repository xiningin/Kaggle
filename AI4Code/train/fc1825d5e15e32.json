{"cell_type":{"21d2766d":"code","95acca61":"code","21408ef8":"code","561950ab":"code","b365916a":"code","af4aa434":"code","21e36384":"code","f01b57ad":"code","8ee09a74":"code","cbc1a336":"code","7e1d8d68":"code","643abf7a":"code","ba8aeb26":"code","ba33a1f8":"code","86cc5e65":"code","88da4c20":"code","e8d25e4c":"code","1cd875a9":"code","e24d83cf":"code","7b08f5af":"code","6eca9feb":"code","61b12c6d":"code","98bb7169":"code","b84e44c8":"code","efad0bcd":"code","9b9d879f":"code","d6be4641":"code","74442e8a":"code","95e015c3":"code","5e7c71b0":"code","f4ff0577":"code","36ecaf2e":"code","6cc97f01":"code","2d9fdc83":"code","ed0d8652":"code","09a9c584":"code","e0e5929f":"code","7ef1a690":"code","a3bbd748":"markdown","cc99b1d8":"markdown","6abe0948":"markdown","9b8c4f2b":"markdown","ec2d5a3c":"markdown","1d0a1000":"markdown","d6101d38":"markdown","1ce3c5de":"markdown","091b8db7":"markdown","87aa32d0":"markdown","d09cf254":"markdown","2cbb7f8c":"markdown","84648b3e":"markdown","094c9268":"markdown","c90ac9a7":"markdown","843fcbe1":"markdown","545f639b":"markdown","7ce83a02":"markdown","cb4de5ed":"markdown","872f5493":"markdown","f314a4ac":"markdown","2e462889":"markdown","27953b57":"markdown","717eda69":"markdown","4c19200a":"markdown","7074a7d7":"markdown","f21f420a":"markdown","d8ad70b9":"markdown","d6935bed":"markdown","318af509":"markdown","9cd09f31":"markdown","5fa2cd2e":"markdown","efa1e4f8":"markdown","d22c3323":"markdown","e71c0c08":"markdown"},"source":{"21d2766d":"import warnings\nwarnings.filterwarnings(\"ignore\")\nimport pandas as pd\nimport numpy as np\nimport datetime\nimport matplotlib.pyplot as plt\nimport matplotlib\nimport seaborn as sns\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.ensemble import RandomForestRegressor\nimport math \nnp.random.seed(2019)\nfrom scipy.stats import skew\nfrom scipy import stats\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.ensemble import RandomForestClassifier\nfrom xgboost.sklearn import XGBClassifier\nfrom xgboost.sklearn import XGBRegressor\n\nimport statsmodels\n\n#!pip install ml_metrics\nfrom ml_metrics import rmsle\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\nprint(\"done\")","95acca61":"def read_and_concat_dataset(training_path, test_path):\n    train = pd.read_csv(training_path)\n    train['train'] = 1\n    test = pd.read_csv(test_path)\n    test['train'] = 0\n    data = train.append(test, ignore_index=True)\n    return train, test, data\n\ntrain, test, data = read_and_concat_dataset('..\/input\/train.csv', '..\/input\/test.csv')\ndata = data.set_index('Id')","21408ef8":"data.columns[data.isnull().sum()>0]","561950ab":"def filling_missing_values(data,variable, new_value):\n    data[variable] = data[variable].fillna(new_value)","b365916a":"filling_missing_values(data,'GarageCond','None')\nfilling_missing_values(data,'GarageQual','None')\nfilling_missing_values(data,'FireplaceQu','None')\nfilling_missing_values(data,'BsmtCond','None')\nfilling_missing_values(data,'BsmtQual','None')\nfilling_missing_values(data,'PoolQC','None')\nfilling_missing_values(data,'MiscFeature','None')","af4aa434":"data['MSSubClass'][data['MSSubClass'] == 20] = '1-STORY 1946 & NEWER ALL STYLES'\ndata['MSSubClass'][data['MSSubClass'] == 30] = '1-STORY 1945 & OLDER'\ndata['MSSubClass'][data['MSSubClass'] == 40] = '1-STORY W\/FINISHED ATTIC ALL AGES'\ndata['MSSubClass'][data['MSSubClass'] == 45] = '1-1\/2 STORY - UNFINISHED ALL AGES'\ndata['MSSubClass'][data['MSSubClass'] == 50] = '1-1\/2 STORY FINISHED ALL AGES'\ndata['MSSubClass'][data['MSSubClass'] == 60] = '2-STORY 1946 & NEWER'\ndata['MSSubClass'][data['MSSubClass'] == 70] = '2-STORY 1945 & OLDER'\ndata['MSSubClass'][data['MSSubClass'] == 75] = '2-1\/2 STORY ALL AGES'\ndata['MSSubClass'][data['MSSubClass'] == 80] = 'SPLIT OR MULTI-LEVEL'\ndata['MSSubClass'][data['MSSubClass'] == 85] = 'SPLIT FOYER'\ndata['MSSubClass'][data['MSSubClass'] == 90] = 'DUPLEX - ALL STYLES AND AGES'\ndata['MSSubClass'][data['MSSubClass'] == 120] = '1-STORY PUD (Planned Unit Development) - 1946 & NEWER'\ndata['MSSubClass'][data['MSSubClass'] == 150] = '1-1\/2 STORY PUD - ALL AGES'\ndata['MSSubClass'][data['MSSubClass'] == 160] = '2-STORY PUD - 1946 & NEWER'\ndata['MSSubClass'][data['MSSubClass'] == 180] = 'PUD - MULTILEVEL - INCL SPLIT LEV\/FOYER'\ndata['MSSubClass'][data['MSSubClass'] == 190] = '2 FAMILY CONVERSION - ALL STYLES AND AGES'","21e36384":"def fixing_ordinal_variables(data, variable):\n    data[variable][data[variable] == 'Ex'] = 5\n    data[variable][data[variable] == 'Gd'] = 4\n    data[variable][data[variable] == 'TA'] = 3\n    data[variable][data[variable] == 'Fa'] = 2\n    data[variable][data[variable] == 'Po'] = 1\n    data[variable][data[variable] == 'None'] = 0","f01b57ad":"fixing_ordinal_variables(data,'ExterQual')\nfixing_ordinal_variables(data,'ExterCond')\nfixing_ordinal_variables(data,'BsmtCond')\nfixing_ordinal_variables(data,'BsmtQual')\nfixing_ordinal_variables(data,'HeatingQC')\nfixing_ordinal_variables(data,'KitchenQual')\nfixing_ordinal_variables(data,'FireplaceQu')\nfixing_ordinal_variables(data,'GarageQual')\nfixing_ordinal_variables(data,'GarageCond')\nfixing_ordinal_variables(data,'PoolQC')","8ee09a74":"data['PavedDrive'][data['PavedDrive'] == 'Y'] = 3\ndata['PavedDrive'][data['PavedDrive'] == 'P'] = 2\ndata['PavedDrive'][data['PavedDrive'] == 'N'] = 1","cbc1a336":"colu = data.columns[(data.isnull().sum()<50) & (data.isnull().sum()>0)]\nfor i in colu:\n    print(data[colu].isnull().sum())","7e1d8d68":"colu = data.columns[data.isnull().sum()>=50]\nfor i in colu:\n    print(data[colu].isnull().sum())","643abf7a":"filling_missing_values(data, 'GarageArea',0)\nfilling_missing_values(data, 'GarageCars',0)\ndata['GarageFinish'][(data.GarageFinish.isnull()==True) & (data.GarageCond==0)] =0\ndata['GarageType'][(data.GarageType.isnull()==True) & (data.GarageCond==0)] =0\ndata['GarageYrBlt'][(data.GarageYrBlt.isnull()==True) & (data.GarageCond==0)] =0","ba8aeb26":"print(data[['MiscFeature','MiscVal']][(data.MiscFeature=='None') & (data.MiscVal>0)])\ndata.MiscVal.loc[2550] = 0\n\nprint(data[['MiscFeature','MiscVal']][(data.MiscVal==0) & (data.MiscFeature!='None')])\nc=data[['MiscFeature','MiscVal']][(data.MiscVal==0) & (data.MiscFeature!='None')].index\ndata.MiscFeature.loc[c] = 'None'","ba33a1f8":"def inputing(variab):\n    y = data[variab]\n    data2 = data.drop([variab],axis=1)\n    col = data2.columns[data2.isnull().sum()==0]\n    data2 = data2[col]\n    data2 = pd.get_dummies(data2)\n    c_train = y[y.notnull()==True].index\n    y_train = y[c_train]\n    columny = data2.columns\n    X_train = data2[columny].loc[c_train]\n    c_test = y[y.notnull()!=True].index\n    y_test = y[c_test]\n    X_test = data2[columny].loc[c_test]\n    #Model\n    model = RandomForestClassifier()\n    model.fit(X_train, y_train)\n    y_pred = model.predict(X_test)\n    #Filling missing data\n    y_pred = pd.Series(y_pred, index=c_test)\n    data[variab].loc[c_test] = y_pred.loc[c_test]\n    \ndef inputingnum(variab):\n    y = data[variab]\n    data2 = data.drop([variab],axis=1)\n    col = data2.columns[data2.isnull().sum()==0]\n    data2 = data2[col]\n    data2 = pd.get_dummies(data2)\n    c_train = y[y.notnull()==True].index\n    y_train = y[c_train]\n    columny = data2.columns\n    X_train = data2[columny].loc[c_train]\n    c_test = y[y.notnull()!=True].index\n    y_test = y[c_test]\n    X_test = data2[columny].loc[c_test]\n    #Model\n    model = RandomForestRegressor()\n    model.fit(X_train, y_train)\n    y_pred = model.predict(X_test)\n    #Filling missing data\n    y_pred = pd.Series(y_pred, index=c_test)\n    data[variab].loc[c_test] = y_pred.loc[c_test]","86cc5e65":"inputing(variab='Electrical')\ninputing(variab='Exterior2nd')\ninputing(variab='Exterior1st')\ninputing(variab='MasVnrType')\ninputing(variab='Functional')\ninputing(variab='MSZoning')\ninputing(variab='SaleType')\ninputing(variab='Alley')\ninputing(variab='BsmtExposure')\ninputing(variab='BsmtFinType1')\ninputing(variab='BsmtFinType2')\ninputing(variab='Fence')\n\ninputingnum(variab='KitchenQual')\ndata['KitchenQual'] = data.KitchenQual.astype(int)\ninputingnum(variab='BsmtFullBath')\ndata['BsmtFullBath'] = data.BsmtFullBath.astype(int)\ninputingnum(variab='BsmtHalfBath')\ndata['BsmtHalfBath'] = data.BsmtHalfBath.astype(int)\n\ninputingnum(variab='TotalBsmtSF')\ninputingnum(variab='BsmtFinSF1')\ninputingnum(variab='BsmtFinSF2')\ninputingnum(variab='MasVnrArea')\ninputingnum(variab='BsmtUnfSF')\ninputingnum(variab='LotFrontage')","88da4c20":"print(data['Utilities'].value_counts())\ndata  = data.drop(['Utilities'],axis=1)","e8d25e4c":"data.columns[data.isnull().sum()>0]","1cd875a9":"data.describe()","e24d83cf":"from scipy.stats import norm\nplt.figure(figsize=(15,8))\nsns.distplot(data['SalePrice'][data.SalePrice.isnull()==False], fit= norm,kde=True)\nplt.show()","7b08f5af":"print(data.plot.scatter(x='LotFrontage',y='SalePrice'))","6eca9feb":"def dropping_outliers(data, condition):\n    #put condition with with reference to the data table, use brackets and (& |) operators, remember about you can drop observation only from train dataset\n    condition_to_drop = data[condition].index\n    data = data.drop(condition_to_drop)","61b12c6d":"dropping_outliers(data, (data.SalePrice<100000) & (data.train==1) & (data.LotFrontage>150))\ndropping_outliers(data, (data.LotFrontage>200) & (data.train==1))\ndropping_outliers(data, (data.SalePrice>700000) & (data.train==1))\ndropping_outliers(data, (data.SalePrice>700000) & (data.train==1))\ndropping_outliers(data, (data.LotArea>60000) & (data.train==1))\ndropping_outliers(data, (data.MasVnrArea>1450) & (data.train==1))\ndropping_outliers(data, (data.BedroomAbvGr==8) & (data.train==1))\ndropping_outliers(data, (data.KitchenAbvGr==3) & (data.train==1))\ndropping_outliers(data, (data['3SsnPorch']>400) & (data.train==1))\ndropping_outliers(data, (data.LotArea>100000) & (data.train==1))\ndropping_outliers(data, (data.MasVnrArea>1300) & (data.train==1))\ndropping_outliers(data, (data.BsmtFinSF1>2000) & (data.train==1) & (data.SalePrice<300000))\ndropping_outliers(data, (data.BsmtFinSF2>200) & (data.SalePrice>350000)  & (data.train==1))\ndropping_outliers(data, (data.BedroomAbvGr==8) & (data.train==1))\ndropping_outliers(data, (data.KitchenAbvGr==3) & (data.train==1))\ndropping_outliers(data, (data.TotRmsAbvGrd==2) & (data.train==1))","98bb7169":"# c=data[(data['SalePrice']<100000) & (data.train==1) & (data['LotFrontage']>150)].index\n# data = data.drop(c)\n# c=data[(data['LotFrontage']>200) & (data.train==1)].index\n# data = data.drop(c)\n# c=data[(data['SalePrice']>700000) & (data.train==1)].index\n# data = data.drop(c)\n# c = data[(data['SalePrice']>700000) & (data.train==1)].index\n# data = data.drop(c)\n# c = data[(data['LotArea']>60000) & (data.train==1)].index\n# data = data.drop(c)\n# c = data[(data['MasVnrArea']>1450) & (data.train==1)].index\n# data = data.drop(c)\n# c = data[(data['BedroomAbvGr']==8) & (data.train==1)].index\n# data = data.drop(c)\n# c = data[(data['KitchenAbvGr']==3) & (data.train==1)].index\n# data = data.drop(c)\n# c = data[(data['3SsnPorch']>400) & (data.train==1)].index\n# data = data.drop(c)\n# c=data[(data.LotArea>100000) & (data.train==1)].index\n# data = data.drop(c)\n# c=data[(data.MasVnrArea>1300) & (data.train==1)].index\n# data = data.drop(c)\n# c=data[(data.BsmtFinSF1>2000) & (data.train==1) & (data.SalePrice<300000)].index\n# data = data.drop(c)\n# c=data[(data.BsmtFinSF2>200) & (data.SalePrice>350000)  & (data.train==1)].index\n# data = data.drop(c)\n# c=data[(data.BedroomAbvGr==8) & (data.train==1)].index\n# data = data.drop(c)\n# c=data[(data.KitchenAbvGr==3) & (data.train==1)].index\n# data = data.drop(c)\n# c=data[(data.TotRmsAbvGrd==2) & (data.train==1)].index\n# data = data.drop(c)","b84e44c8":"#CentralAir\nprint(data['CentralAir'].value_counts())\ndata['CentralAir'] = pd.Series(np.where(data['CentralAir'].values == 'Y', 1, 0),\n          data.index)","efad0bcd":"data['2ndFloor'] = pd.Series(np.where(data['2ndFlrSF'].values == 0, 0, 1),data.index)\ndata['Floors'] = data['1stFlrSF'] + data['2ndFlrSF']\ndata = data.drop(['1stFlrSF'],axis=1)\ndata = data.drop(['2ndFlrSF'],axis=1)\ndata['TotBath'] = data['FullBath'] + (0.5 * data['HalfBath']) + data['BsmtFullBath'] + (0.5 * data['BsmtHalfBath'])\ndata['Porch'] = data['OpenPorchSF'] + data['3SsnPorch'] + data['EnclosedPorch'] + data['ScreenPorch']\ndata['TotalSF'] = data['BsmtFinSF1'] + data['BsmtFinSF2'] + data['Floors'] \ndata['Pool'] = pd.Series(np.where(data['PoolArea'].values == 0, 0, 1),data.index)\ndata['Bsmt'] = pd.Series(np.where(data['TotalBsmtSF'].values == 0, 0, 1),data.index)\ndata['Garage'] = pd.Series(np.where(data['GarageArea'].values == 0, 0, 1),data.index)\ndata['Fireplace'] = pd.Series(np.where(data['Fireplaces'].values == 0, 0, 1),data.index)\ndata['Remod'] = pd.Series(np.where(data['YearBuilt'].values == data['YearRemodAdd'].values, 0, 1),data.index)\ndata['NewHouse'] = pd.Series(np.where(data['YearBuilt'].values == data['YrSold'].values, 1, 0),data.index)\ndata['Age'] = data['YrSold'] - data['YearRemodAdd']","9b9d879f":"c = data[(data['Floors']>4000) & (data.train==1)].index\ndata = data.drop(c)\nc = data[(data['SalePrice']>500000) & (data['TotalSF']<3500) & (data.train==1)].index\ndata = data.drop(c)","d6be4641":"data = data.drop(['PoolQC'],axis=1)\ndata = data.drop(['GrLivArea'],axis=1)\ndata = data.drop(['Street'],axis=1)\ndata = data.drop(['GarageYrBlt'],axis=1)\ndata = data.drop(['PoolArea'],axis=1)\ndata = data.drop(['MiscFeature'],axis=1)","74442e8a":"Results = pd.DataFrame({'Model': [],'Accuracy Score': []})","95e015c3":"data = pd.get_dummies(data)","5e7c71b0":"from xgboost.sklearn import XGBRegressor\nfrom sklearn.model_selection import train_test_split\n\ntrainX, testX, trainY, testY = train_test_split(data[data.SalePrice.isnull()==False].drop('SalePrice',axis=1),data.SalePrice[data.SalePrice.isnull()==False],test_size=0.30, random_state=2019)\ntrainY = np.log(trainY)\n\nmodel = XGBRegressor(learning_rate=0.001,n_estimators=4600,\n                                max_depth=7, min_child_weight=0,\n                                gamma=0, subsample=0.7,\n                                colsample_bytree=0.7,\n                                scale_pos_weight=1, seed=27,\n                                reg_alpha=0.00006)\nmodel.fit(trainX,trainY)\ny_pred = model.predict(testX)\ny_pred = np.exp(y_pred)\n\nres = pd.DataFrame({\"Model\":['XGBoost'],\n                    \"Accuracy Score\": [rmsle(testY, y_pred)]})\nResults = Results.append(res)","f4ff0577":"from sklearn.tree import DecisionTreeRegressor\nfrom sklearn.model_selection import train_test_split\n\ntrainX, testX, trainY, testY = train_test_split(data[data.SalePrice.isnull()==False].drop('SalePrice',axis=1),data.SalePrice[data.SalePrice.isnull()==False],test_size=0.30, random_state=2019)\ntrainY = np.log(trainY)\n\nmodel = DecisionTreeRegressor(max_depth=6)\nmodel.fit(trainX,trainY)\ny_pred = model.predict(testX)\ny_pred = np.exp(y_pred)\n\nprint(rmsle(testY, y_pred))\n\nres = pd.DataFrame({\"Model\":['Decision Tree'],\n                    \"Accuracy Score\": [rmsle(testY, y_pred)]})\nResults = Results.append(res)","36ecaf2e":"from sklearn.ensemble import RandomForestRegressor\nfrom sklearn.model_selection import train_test_split\n\ntrainX, testX, trainY, testY = train_test_split(data[data.SalePrice.isnull()==False].drop('SalePrice',axis=1),data.SalePrice[data.SalePrice.isnull()==False],test_size=0.30, random_state=2019)\ntrainY = np.log(trainY)\n\nmodel = RandomForestRegressor(n_estimators=1500,\n                                max_depth=6)\nmodel.fit(trainX,trainY)\ny_pred = model.predict(testX)\ny_pred = np.exp(y_pred)\nprint(rmsle(testY, y_pred))\n\nres = pd.DataFrame({\"Model\":['Random Forest'],\n                    \"Accuracy Score\": [rmsle(testY, y_pred)]})\nResults = Results.append(res)","6cc97f01":"from sklearn.linear_model import Lasso\nfrom sklearn.model_selection import train_test_split\n\ntrainX, testX, trainY, testY = train_test_split(data[data.SalePrice.isnull()==False].drop('SalePrice',axis=1),data.SalePrice[data.SalePrice.isnull()==False],test_size=0.30, random_state=2019)\ntrainY = np.log(trainY)\n\nmodel = Lasso(alpha=0.0005)\n\nmodel.fit(trainX,trainY)\ny_pred = model.predict(testX)\ny_pred = np.exp(y_pred)\nprint(rmsle(testY, y_pred))\n\nres = pd.DataFrame({\"Model\":['LASSO'],\n                    \"Accuracy Score\": [rmsle(testY, y_pred)]})\nResults = Results.append(res)","2d9fdc83":"import statsmodels.api as sm\nfrom sklearn.model_selection import train_test_split\n\ntrainX, testX, trainY, testY = train_test_split(data[data.SalePrice.isnull()==False].drop('SalePrice',axis=1),data.SalePrice[data.SalePrice.isnull()==False],test_size=0.30, random_state=2019)\ntrainY = np.log(trainY)\n\nX2 = sm.add_constant(trainX)\no=0\nfor i in X2.columns:\n    o+=1\n    print(o)\n    model = sm.OLS(trainY, X2.astype(float))\n    model = model.fit()\n    p_values = pd.DataFrame(model.pvalues)\n    p_values = p_values.sort_values(by=0, ascending=False)\n    if float(p_values.loc[p_values.index[0]])>=0.05:\n        X2=X2.drop(p_values.index[0],axis=1)\n    else:\n        break\n\nkolumny = X2.columns\ntestX = sm.add_constant(testX)\ntestX = testX[kolumny]\n\ny_pred = model.predict(testX)\ny_pred = np.exp(y_pred)\n\n\nres = pd.DataFrame({\"Model\":['Stepwise Regression'],\n                    \"Accuracy Score\": [rmsle(testY, y_pred)]})\nResults = Results.append(res)","ed0d8652":"from sklearn.linear_model import Ridge\nfrom sklearn.model_selection import train_test_split\n\ntrainX, testX, trainY, testY = train_test_split(data[data.SalePrice.isnull()==False].drop('SalePrice',axis=1),data.SalePrice[data.SalePrice.isnull()==False],test_size=0.30, random_state=2019)\ntrainY = np.log(trainY)\n\nmodel = Ridge(alpha=0.0005)\n\nmodel.fit(trainX,trainY)\ny_pred = model.predict(testX)\ny_pred = np.exp(y_pred)\nprint(rmsle(testY, y_pred))\n\nres = pd.DataFrame({\"Model\":['Ridge'],\n                    \"Accuracy Score\": [rmsle(testY, y_pred)]})\nResults = Results.append(res)","09a9c584":"from sklearn.linear_model import Lasso\nfrom sklearn.model_selection import train_test_split\n\ntrainX, testX, trainY, testY = train_test_split(data[data.SalePrice.isnull()==False].drop('SalePrice',axis=1),data.SalePrice[data.SalePrice.isnull()==False],test_size=0.30, random_state=2019)\ntrainY = np.log(trainY)\n\nmodel = Lasso(alpha=0)\n\nmodel.fit(trainX,trainY)\ny_pred = model.predict(testX)\ny_pred = np.exp(y_pred)\nprint(rmsle(testY, y_pred))\n\nres = pd.DataFrame({\"Model\":['Linear Regression'],\n                    \"Accuracy Score\": [rmsle(testY, y_pred)]})\nResults = Results.append(res)","e0e5929f":"Results","7ef1a690":"trainX = data[data.SalePrice.isnull()==False].drop(['SalePrice','train'],axis=1)\ntrainY = data.SalePrice[data.SalePrice.isnull()==False]\ntestX = data[data.SalePrice.isnull()==True].drop(['SalePrice','train'],axis=1)\ntrainY = np.log(trainY)\nmodel = Lasso(alpha=0.0005)\nmodel.fit(trainX, trainY)\ntest = data[data.train==0]\ntest['SalePrice'] = model.predict(testX)\ntest['SalePrice'] = np.exp(test['SalePrice'] )\ntest = test.reset_index()\ntest[['Id','SalePrice']].to_csv(\"submissionLASSO.csv\",index=False)\nprint(\"done1\")","a3bbd748":"On the scatter charts, I checked which observations could be considered outliers and I decided to delete them.\nI must be very careful because I don't want to remove observations from the test set.","cc99b1d8":"CentalAir variable needs transformation to binary variable.","6abe0948":"For example, let's look at scatter plot of SalePrice and Lot Frontage.","9b8c4f2b":"##**Missing values**","ec2d5a3c":"**Droping a few variables**","1d0a1000":"First of all I'm gonna look how many variables have less than 50 missing values and fix it. Then I'll look how about variables with more than 50 missing values.","d6101d38":"There are a few variables with NaN value but in these cases 'NaN' means something else than missing value. For example 'NaN' in 'GarageCond' means that this house hasn't a garage. I'm gonna change 'NaN' values to 'None' string. ","1ce3c5de":"**XGBoost Regressor**","091b8db7":"LASSO Regression model gives the best results. This model helps me to get 0.12903 (RMSLE) on competition test dataset and it gives me place in 17% best results on Leaderboard.","87aa32d0":"A few categorical variables are ordinal variables, so let's fix them. ","d09cf254":"Let's understand a data set variable after variable, check basic statistics and drop a few outliers. I'll also drop variables with little differentiation. ","2cbb7f8c":"**Ridge Regression**","84648b3e":"##**Fixing variables**","094c9268":"**Linear Regression**\n\nWhen you change alpha to 0 value in LASSO, you have simple Linear Regression model.","c90ac9a7":"I'm gonna put 0 in MiscVal for house which don't have any MiscFeature and 'None' value for house with 0 in MiscValue and some value in MiscFeature.","843fcbe1":"Let's imput missing values using two functions which I wrote. In KitchenQual, BsmtFullBath and BsmtHalfBath cases I'm gonna use Regressor model and convert them to integer.","545f639b":"**Import Data**","7ce83a02":"It's everything about imputing missing values.","cb4de5ed":"MSSubClass is not a numerical variables, so let's transform it to caterogical variable.","872f5493":"##**Modeling:**\n\n- XGB Regressor\n- Decision Tree Regressor\n- Random Forest Regressor\n- LASSO Regression\n\n\nFor each model I tuned the parameters using loops and each model contains SalePrice variable tranformed to logarithm.","f314a4ac":"Feature engineering is an important part of machine learning process so I want to spend more time for this part. I'm gonna try I few models and tell you which work the best with train dataset from this competition. ","2e462889":"I'm gonna drop more observations.","27953b57":"##**Feature engineering**","717eda69":"Now I'm gonna write two functions to help me in imputing missing values in variables. I'm using here Random Forest Regressor and Classifier. ","4c19200a":"##**Results**","7074a7d7":"I'm putting 0 in GarageArea, GarageFinish, GarageType, GarageYrBlt and GarageCars where houses don't have garage. ","f21f420a":"**Stepwise Regression**","d8ad70b9":"**Decision Tree Regressor**","d6935bed":"* 2ndFloor - if the house has a second floor\n* Floors - total area of the first and second floor\n* TotBath - how many bathrooms house has\n* Porch - total area of the porch\n* TotalSF - total area of the house\n* Pool - if the house has a swimming pool\n* Bsmt - if the house has a basement\n* Garage - if the house has a garage\n* Fireplace - if the house has a fireplace\n* Remod - if the house was renovated\n* NewHouse - if the house is new\n* Age - ages of house\n","318af509":"I'm adding here 'train' variable in order to check in the easiest way which observations are from train and test dataset because I'm gonna join train and test datasets.","9cd09f31":"**Preparing to modeling:**\n- dummies variables\n- two data frames with independent variables for train and test set\n- vector y with Sale Price variable for train set","5fa2cd2e":"**Import the Libraries**","efa1e4f8":"##**Dropping outliers**","d22c3323":"**LASSO Regression**","e71c0c08":"..and one more but in different way."}}