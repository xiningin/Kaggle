{"cell_type":{"3ae9e4a1":"code","70208978":"code","7b1745d0":"code","287d1c6a":"code","535aee29":"code","d926c127":"code","e5bad63e":"code","c3525e7f":"code","9755870d":"code","1ccaaa0c":"code","43db22f4":"code","2895b457":"code","3856ca84":"code","7f88c208":"code","bc72f420":"code","3a9f2388":"markdown","fdc3e1ea":"markdown","37b430a2":"markdown","70c44024":"markdown","975160fe":"markdown","aab59880":"markdown","b168d0ad":"markdown","9a1169dd":"markdown","2ab7920b":"markdown","4c5a3948":"markdown","38cc1a60":"markdown","cd1e7d48":"markdown","71eac8cb":"markdown","858dc74b":"markdown","8177e3c8":"markdown"},"source":{"3ae9e4a1":"import pandas as pd\nimport os\nimport numpy as np\n\nimport tensorflow as tf\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.layers import Embedding, LSTM, Dense, Bidirectional\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.optimizers import Adam","70208978":"medium_data = pd.read_csv('..\/input\/medium-articles-dataset\/medium_data.csv')\nmedium_data.head()","7b1745d0":"print(\"Number of records: \", medium_data.shape[0])\nprint(\"Number of fields: \", medium_data.shape[1])","287d1c6a":"medium_data['title']","535aee29":"medium_data['title'] = medium_data['title'].apply(lambda x: x.replace(u'\\xa0',u' '))\nmedium_data['title'] = medium_data['title'].apply(lambda x: x.replace('\\u200a',' '))","d926c127":"tokenizer = Tokenizer(oov_token='<oov>') # For those words which are not found in word_index\ntokenizer.fit_on_texts(medium_data['title'])\ntotal_words = len(tokenizer.word_index) + 1\n\nprint(\"Total number of words: \", total_words)\nprint(\"Word: ID\")\nprint(\"------------\")\nprint(\"<oov>: \", tokenizer.word_index['<oov>'])\nprint(\"Strong: \", tokenizer.word_index['strong'])\nprint(\"And: \", tokenizer.word_index['and'])\nprint(\"Consumption: \", tokenizer.word_index['consumption'])","e5bad63e":"input_sequences = []\nfor line in medium_data['title']:\n    token_list = tokenizer.texts_to_sequences([line])[0]\n    #print(token_list)\n    \n    for i in range(1, len(token_list)):\n        n_gram_sequence = token_list[:i+1]\n        input_sequences.append(n_gram_sequence)\n\n# print(input_sequences)\nprint(\"Total input sequences: \", len(input_sequences))","c3525e7f":"# pad sequences \nmax_sequence_len = max([len(x) for x in input_sequences])\ninput_sequences = np.array(pad_sequences(input_sequences, maxlen=max_sequence_len, padding='pre'))\ninput_sequences[1]","9755870d":"# create features and label\nxs, labels = input_sequences[:,:-1],input_sequences[:,-1]\nys = tf.keras.utils.to_categorical(labels, num_classes=total_words)","1ccaaa0c":"print(xs[5])\nprint(labels[5])\nprint(ys[5][14])","43db22f4":"model = Sequential()\nmodel.add(Embedding(total_words, 100, input_length=max_sequence_len-1))\nmodel.add(Bidirectional(LSTM(150)))\nmodel.add(Dense(total_words, activation='softmax'))\nadam = Adam(lr=0.01)\nmodel.compile(loss='categorical_crossentropy', optimizer=adam, metrics=['accuracy'])\nhistory = model.fit(xs, ys, epochs=50, verbose=1)\n#print model.summary()\nprint(model)\n","2895b457":"import matplotlib.pyplot as plt\n\n\ndef plot_graphs(history, string):\n    plt.plot(history.history[string])\n    plt.xlabel(\"Epochs\")\n    plt.ylabel(string)\n    plt.show()","3856ca84":"plot_graphs(history, 'accuracy')","7f88c208":"plot_graphs(history, 'loss')","bc72f420":"seed_text = \"implementation of\"\nnext_words = 2\n  \nfor _ in range(next_words):\n    token_list = tokenizer.texts_to_sequences([seed_text])[0]\n    token_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')\n    predicted = model.predict_classes(token_list, verbose=0)\n    output_word = \"\"\n    for word, index in tokenizer.word_index.items():\n        if index == predicted:\n            output_word = word\n            break\n    seed_text += \" \" + output_word\nprint(seed_text)","3a9f2388":"<a name=\"intro\"><\/a>\n\n# Introduction\n\n**Next Word Prediction (also called Language Modeling) is the task of predicting what word comes next. It is one of the fundamental tasks of NLP.**\n\nImage reference: https:\/\/medium.com\/@antonio.lopardo\/the-basics-of-language-modeling-1c8832f21079\n\n![gg.png](attachment:426089b0-5844-4928-a797-40e0015c1a93.png)\n\n#### Application Language Modelling \n\n**1) Mobile keyboard text recommandation**\n\n![fff.jpg](attachment:0cd813a1-ea03-40b9-86d7-0585d994a36e.jpg)\n\n**2) Whenever we search for something on any search engine, we get many suggestions and,  as we type new words in it, we get better recommendations according to our searching context. So, how will it happen??? **\n\n![Screenshot (21).png](attachment:72ee772e-4ef9-4e79-a364-5dcf8f558e4a.png)\n\n\nIt is poosible through natural language processing (NLP) technique. Here, we will use NLP and try to make a prediction model using Bidirectional LSTM (Long short-term memory) model that will predict next words of sentence.\n ","fdc3e1ea":"<a name=\"new\"><\/a>\n# Predicting next word of title","37b430a2":"<a name=\"remv\"><\/a>\n#### Removing unwanted characters and words in titles\n\nLooking at titles, we can see there are some of unwanted characters and words in it which can not be useful for us to predict infact it might decrease our model accuracy so we have to remove it.","70c44024":"### Table Content\n------------------\n\n- [Introduction](#intro)\n- [Import libraries and packages](#ilp)\n- [Dataset Information](#di)\n- [Separate 'Title' field and preprocess it](#preprocess)\n    - [Removing unwanted charaters and words](#remv)\n    - [Tokenization and word_index (vocabulary) ](#token)\n    - [Convert titles into sequences and Make n_gram model](#ngram)\n    - [Make all titles with same length and padding them](#pad)\n- [Preprare features (X) and labels (Y)](#xy)\n- [Architechture of Bidirectional LSTM neural network](#blstm)\n- [Train Bi-LSTM neural network](#train)\n- [Plotting accuracy and loss graph](#acc)\n- [Predict new title (Testing)](#new)\n\n----------------\n\n\n","975160fe":"Here, we have a **10 different fields and 6508 records** but we will only use **title field** for predicting next word. ","aab59880":"<a name=\"di\"><\/a>\n# Dataset information\n\n**Import Medium-articles-dataset:**\n\nThis dataset contains information about randomly chosen medium articles published in 2019 from these 7 publications:\n\n- Towards Data Science\n- UX Collective\n- The Startup\n- The Writing Cooperative\n- Data Driven Investor\n- Better Humans\n- Better Marketing\n","b168d0ad":"<a name=\"xy\"><\/a>\n# Prepare features and labels\n\nHere, we consider **last element of all sequences as a label**.Then,\nWe need to perform **onehot encoding on labels corresponding to total_words.**","9a1169dd":"<a name=\"acc\"><\/a>\n# Plotting model accuracy and loss","2ab7920b":"<a name=\"train\"><\/a>\n# Bi- LSTM Neural Network Model training","4c5a3948":"<a name=\"ngram\"><\/a>\n#### Titles text into sequences and make n_gram model\n\nsuppose, we have sentence like **\"I am Yash\"** and this will convert into a sequence with their respective tokens **{'I': 1,'am': 2,'Yash': 3}**. Thus, output will be  **[ '1' ,'2' ,'3' ]**\n\nLikewise, our all titles will be converted into sequences.\n\nThen,\nwe will make a n_gram model for good prediction.\n\nBelow image explain about everything.\n\n![Capture.PNG](attachment:48ad80b3-90bf-4cf6-99f8-7dcfd467d1f8.PNG)\n","38cc1a60":"<a name=\"ilp\"><\/a>\n# Import necessary libraries and packages ","cd1e7d48":"<a name=\"token\"><\/a>\n#### Tokenzation\n\nTokenzaion is the process in which we provide an unique id to all the words and make a word index or we can say vocabulary.","71eac8cb":"<a name=\"pad\"><\/a>\n#### Make all titles with same length by using padding\n\nThe length of every title has to be the same. To make it, we need to find a title that has a maximum length, and based on that length, we have to pad rest of titles.","858dc74b":"<a name=\"blstm\"><\/a>\n# Architechture of Bidirectional LSTM Neural Network\n\nLong Short-Term Memory (LSTM) networks is an advance recurrent neural network which is apable to store order states by using its cell state feature.\n\nImage reference: https:\/\/www.researchgate.net\/figure\/The-structure-of-the-Long-Short-Term-Memory-LSTM-neural-network-Reproduced-from-Yan_fig8_334268507\n![lstm.png](attachment:c34341f6-d243-478a-b4bd-bf242759cd50.png)\n\n**Bidirectional LSTM**\nImage reference: https:\/\/paperswithcode.com\/method\/bilstm\n![bi.png](attachment:d26c6b0c-cbdf-45a5-b88b-2b352d7b7d63.png)","8177e3c8":"<a name=\"preprocess\"><\/a>\n# Display titles of various articles  and preprocess them"}}