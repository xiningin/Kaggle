{"cell_type":{"fb713122":"code","eaa48d48":"code","912b17b0":"code","60437e5a":"code","3ea4bcc1":"code","d7b4894d":"code","bc40914c":"code","37089c1a":"code","4e94c89c":"code","6adf2769":"code","b0aacdf2":"code","1f0ba619":"code","4fce0f77":"code","f47b6596":"code","0ad8cd44":"code","e6dc9cb3":"code","2f2b93e1":"code","fccdb046":"code","abc67492":"code","f4514f7c":"code","7a3a8fec":"code","1a1c60fa":"code","5e3e1362":"code","87b7cf3d":"code","a788e92d":"markdown","0ea1d070":"markdown","535141d4":"markdown","c3c62e8b":"markdown","2f1a6ca1":"markdown","6d39b60f":"markdown","329868ef":"markdown","573f8c40":"markdown","0fda0e0b":"markdown"},"source":{"fb713122":"import os\nimport numpy as np\nimport pandas as pd\nimport math\nimport shutil\nimport csv\n\nimport matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\n\nfrom sklearn.metrics import roc_curve, auc, roc_auc_score, accuracy_score, average_precision_score, classification_report, confusion_matrix, plot_confusion_matrix\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.utils import class_weight\n\nimport tensorflow as tf\nfrom tensorflow.keras.models import Model, Sequential\nfrom tensorflow.keras.layers import Input, Dense, Flatten, Dropout, GlobalAveragePooling2D, Conv2D, MaxPooling2D, BatchNormalization, Activation\nfrom tensorflow.keras.applications.efficientnet import EfficientNetB0, EfficientNetB1, EfficientNetB2, EfficientNetB3, EfficientNetB4, preprocess_input, decode_predictions\n# from tensorflow.keras.applications.vgg16 import VGG16, preprocess_input, decode_predictions\n# from tensorflow.keras.applications.resnet_v2 import ResNet50V2, preprocess_input, decode_predictions\nfrom tensorflow.keras.losses import BinaryCrossentropy\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.optimizers import Adam, RMSprop\nfrom tensorflow.keras.callbacks import ReduceLROnPlateau, ModelCheckpoint, EarlyStopping, LearningRateScheduler","eaa48d48":"device_name = tf.test.gpu_device_name()\nif \"GPU\" not in device_name:\n    print(\"GPU device not found\")\nelse:\n    print('Found GPU at: {}'.format(device_name))\n    physical_devices = tf.config.list_physical_devices('GPU')\n    tf.config.experimental.set_memory_growth(physical_devices[0], True)","912b17b0":"path = '..\/input\/chest-xray-pneumonia\/chest_xray\/'\ndestination = '.\/'\n\n#organize everything into a csv file!! This will make everything easier\n\nwith open('.\/train.csv', 'w+', newline='') as csvfile:\n    writer = csv.writer(csvfile)\n    writer.writerow(['image_path', 'label'])\n    for s in ['train\/','val\/']:\n        for root, dirs, files in os.walk(path + s):\n            for f in files:\n                fullpath = os.path.join(root, f)\n                \n                if fullpath.split('\/')[-2] == 'NORMAL':\n                    label = '0'\n                else:\n                    label = '1'\n                    \n                writer.writerow([fullpath, label])\n                \nwith open('.\/test.csv', 'w+', newline='') as csvfile:\n    writer = csv.writer(csvfile)\n    writer.writerow(['image_path', 'label'])\n    for s in ['test\/']:\n        for root, dirs, files in os.walk(path + s):\n            for f in files:\n                fullpath = os.path.join(root, f)\n                \n                if fullpath.split('\/')[-2] == 'NORMAL':\n                    label = '0'\n                else:\n                    label = '1'\n                    \n                writer.writerow([fullpath, label])\n\nprint(\"done\")","60437e5a":"data_dir = \".\/\"\nlabels = ['NORMAL','PNEUMONIA']\nBATCH_SIZE = 16\nIMG_SIZE = (150, 150) #could change size (perhaps larger) -> larger size = better rez = better models.Options: (150, 224, 256, 300, 512)","3ea4bcc1":"#converting csv into data-frame (using pandas)\n\npd.set_option('max_colwidth', 800)\ndf = pd.read_csv('.\/train.csv')\ntest_df = pd.read_csv('.\/test.csv')\n\ndf.head(3)","d7b4894d":"#converting '1' and '0' into string bc it is a label\n\ndf['label'] = df['label'].astype('str')\ntest_df['label'] = test_df['label'].astype('str')","bc40914c":"#Ratio of data -> we want 50:50 -> if not, it's called data-imbalance -> but still train first then we'll see\n\nneg, pos = np.bincount(df['label'])\ntotal = neg + pos\nprint('Examples:\\n    Total: {}\\n    Positive: {} ({:.2f}% of total)\\n'.format(\n    total, pos, 100 * pos \/ total))","37089c1a":"#startify makes sure the twp types of data are properly ratioed... bc or else it will be randomly sliced\n\ntrain_df, valid_df = train_test_split(df, test_size=0.20, random_state=13, stratify=df['label'])\nlen(train_df), len(valid_df)","4e94c89c":"neg, pos = np.bincount(train_df['label'])\ntotal = neg + pos\nprint('Examples:\\n    Total: {}\\n    Positive: {} ({:.2f}% of total)\\n'.format(\n    total, pos, 100 * pos \/ total))","6adf2769":"weight_for_0 = (1 \/ neg) * (total \/ 2.0)\nweight_for_1 = (1 \/ pos) * (total \/ 2.0)\n\nclass_weights = {0: weight_for_0, 1: weight_for_1}\n\nprint('Weight for class 0: {:.2f}'.format(weight_for_0))\nprint('Weight for class 1: {:.2f}'.format(weight_for_1))","b0aacdf2":"bool_train_labels = train_df['label']!='0'\n\npos_features = train_df['image_path'][bool_train_labels]\nneg_features = train_df['image_path'][~bool_train_labels]\n\npos_labels = train_df['label'][bool_train_labels]\nneg_labels = train_df['label'][~bool_train_labels]\n\nlen(pos_features), len(neg_features)","1f0ba619":"ids = np.arange(len(pos_features))\nchoices = np.random.choice(ids, len(neg_features))\n\nres_pos_features = pos_features[pos_features.index[choices]]\nres_pos_labels = pos_labels[pos_labels.index[choices]]\n\nres_pos_features.head(3)","4fce0f77":"resampled_features = np.concatenate([res_pos_features, neg_features], axis=0)\nresampled_labels = np.concatenate([res_pos_labels, neg_labels], axis=0)\n\norder = np.arange(len(resampled_labels))\nnp.random.shuffle(order)\nresampled_features = resampled_features[order]\nresampled_labels = resampled_labels[order]\n\nresampled_data = {'image_path': resampled_features,\n        'label': resampled_labels\n        }\n\nresampled_df = pd.DataFrame(resampled_data, columns = ['image_path', 'label'])\nresampled_df.head(3)","f47b6596":"#augmenting\/altering data so that there are more samples -> increase variety -> better model + prevents over-fitting\n#There are a lot more (types) than what is written here!!\n#some data sets don't fit the augment style (for example: it doesn't make sense to flip a loung)\n\nabstrain_datagen = ImageDataGenerator(\n    rescale=1.\/255,\n#     preprocessing_function=preprocess_input,\n    rotation_range=20,\n#     width_shift_range=0.2,\n#     height_shift_range=0.2,\n    brightness_range=[0.6, 1.3],\n    shear_range=0.3,\n    zoom_range=[0.8, 1.0],\n    horizontal_flip=True,\n#     vertical_flip=True,\n    fill_mode='constant'\n)\n\n#you souldnt augment test and val data sets -> maybe only rescale...\n\ntest_datagen = ImageDataGenerator(\n    rescale=1.\/255,\n#     preprocessing_function=preprocess_input,\n)\n\n\n\ntrain_generator = train_datagen.flow_from_dataframe(\n                                        dataframe=train_df,\n                                        x_col='image_path',\n                                        y_col='label',\n                                        class_mode='binary',\n                                        target_size=IMG_SIZE,\n                                        batch_size=BATCH_SIZE)\n\n#dont shuffle test and val\n\nvalidation_generator = test_datagen.flow_from_dataframe(\n                                        dataframe=valid_df,\n                                        x_col='image_path',\n                                        y_col='label',\n                                        class_mode='binary',\n                                        shuffle=False,\n                                        target_size=IMG_SIZE,\n                                        batch_size=BATCH_SIZE)\n\ntest_generator = test_datagen.flow_from_dataframe(\n                                        dataframe=test_df,\n                                        x_col='image_path',\n                                        y_col='label',\n                                        class_mode='binary',\n                                        shuffle=False,\n                                        target_size=IMG_SIZE,\n                                        batch_size=BATCH_SIZE)","0ad8cd44":"fig, axes = plt.subplots(1, 3)\nfig.tight_layout()\n\nfor i in range(3):\n    img, label = train_generator.next()\n    axes[i].set_title(label[0])  \n    axes[i].imshow(img[0])","e6dc9cb3":"IMG_SHAPE = IMG_SIZE + (3,)\ndr = 0.5\n\n#kernel is used to extract features\n#more layers = more features...but we may extract wrong\/irrelevent features\n#Conv2D(16, kernel_size=(3, 3), padding=\"same\", activation=\"relu\", input_shape=IMG_SHAPE),\n\n\nmodel = Sequential([\n    Conv2D(16, kernel_size=(3, 3), activation=\"relu\", input_shape=IMG_SHAPE),\n    MaxPooling2D(pool_size=(2, 2)),\n    \n    Conv2D(32, kernel_size=(3, 3), activation=\"relu\"),\n    MaxPooling2D(pool_size=(2, 2)),\n    \n#     Flatten(),\n    GlobalAveragePooling2D(),\n    Dropout(dr),  #decreases over-fitting\n    Dense(1, activation='sigmoid'),\n    \n    #the '1' in Dense is the types of data -> binary = 1, \"dog, horse, cat\" = 3\n])\n\n#model = Sequential()\n#model.add(Conv2D(16, kernel_size=(3, 3), activation=\"relu\", input_shape=IMG_SHAPE))\n#or\n#x = Conv2D(16, kernel_size=(3, 3), activation=\"relu\", input_shape=IMG_SHAPE)(model)\n#x = MaxPooling2D(pool_size=(2, 2))(x)\n\nmodel.summary()","2f2b93e1":"metrics = [\n    tf.keras.metrics.BinaryAccuracy(name=\"binary_acc\"),\n    tf.keras.metrics.AUC(name=\"AUC\"),\n    tf.keras.metrics.Precision(name=\"precision\"),\n    tf.keras.metrics.Recall(name=\"recall\"),\n]\n\nmodel.compile(optimizer=Adam(learning_rate=1e-3),\n              loss=BinaryCrossentropy(),\n              metrics=metrics)","fccdb046":"callbacks = [\n#              ModelCheckpoint(\"model_at_epoch_{epoch}.h5\"),\n#              ReduceLROnPlateau(monitor='val_loss',\n#                             patience=2,\n#                             verbose=1,\n#                             factor=0.07,\n#                             min_lr=1e-9),\n#              EarlyStopping(monitor='val_loss', patience=3, verbose=1)\n]\n\n#if over-fitting doesn't happen the larger the epoch\/time the better\n\nhistory = model.fit(train_generator, \n                    epochs=15,\n                    batch_size=BATCH_SIZE,\n                    callbacks=callbacks,\n                    validation_data=validation_generator,\n                    class_weight=class_weights\n)","abc67492":"def plot_metrics(history, name, bot=0.0, top=0.0):\n    plt.plot(history.history[name])\n    plt.plot(history.history['val_'+name])\n    plt.title('Model '+name)\n    plt.ylabel(name)\n    plt.xlabel('Epoch')\n    plt.legend(['Train', 'Test'], loc='upper left')\n    if top != 0.0:\n        plt.ylim([bot, top])\n    plt.show()\n    \nplot_metrics(history, 'loss')\nfor i in range(len(metrics)):\n    plot_metrics(history, metrics[i].name, 0.5, 1.0)","f4514f7c":"model.evaluate(test_generator, batch_size=BATCH_SIZE)","7a3a8fec":"Y_pred = model.predict(test_generator, batch_size=BATCH_SIZE)\n\ny_pred = np.rint(Y_pred)","1a1c60fa":"conf_matrix = confusion_matrix(test_generator.classes, y_pred)\n\nplt.matshow(conf_matrix, cmap=plt.cm.Blues)\nfor (i, j), z in np.ndenumerate(conf_matrix):\n    plt.text(j, i, z, ha='center', va='center')","5e3e1362":"print('Classification Report')\ntarget_names = ['0','1']\nprint(classification_report(test_generator.classes, y_pred, target_names=target_names))","87b7cf3d":"def plot_top_losses(actual, pred, k=9, figsize=(10,10)):\n    loss = BinaryCrossentropy(from_logits=False, reduction=tf.keras.losses.Reduction.NONE)\n    loss_values = loss(actual, pred).numpy()\n    top_k = loss_values.argsort()[-k:][::-1]\n    cols = math.ceil(math.sqrt(k))\n    rows = math.ceil(k\/cols)\n    fig, axes = plt.subplots(rows, cols, figsize=figsize)\n    fig.suptitle('Prediction\/Actual\/Loss\/Prediction_Probability', weight='bold', size=14)\n    fig.tight_layout()\n    for i, index in enumerate(top_k):\n        image = validation_generator[int(index\/16)][0][index%16]\n        actual = validation_generator.classes[index]\n        loss_value = loss_values[index]\n        predicted = np.argmax(pred[index])\n        prob = pred[index][predicted]\n        title = f'{predicted}\/{actual}\/{loss_value:.2f}\/{prob:.2f}'\n        ax = axes.flat[i]\n        ax.imshow(image)\n        ax.set_title(title)\n        \nplot_top_losses(test_generator.classes, Y_pred, 9)","a788e92d":"# Calculate Class Weight (Optional)","0ea1d070":"# Build and Train Model","535141d4":"# Data Preprocessing","c3c62e8b":"## Plot Top Losses","2f1a6ca1":"# Image Data Generator","6d39b60f":"## Plot Confusion Matrix","329868ef":"# Make CSV","573f8c40":"# Oversampling (Optional)","0fda0e0b":"# Evaluate Model"}}