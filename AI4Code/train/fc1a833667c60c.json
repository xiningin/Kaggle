{"cell_type":{"343d5538":"code","13e7b46e":"code","e3f96001":"code","15be0d11":"code","088ec8c1":"code","8c090917":"code","82893cc0":"code","f75f565f":"code","40af3c84":"code","2dcada6c":"code","0c289d21":"code","dd1ca4fb":"code","08e221da":"code","37c62f87":"code","ba88d600":"code","6b717ba0":"code","196f8327":"code","af732b4e":"code","1f8b577c":"code","dcd9e075":"code","2bbd3408":"code","c84da72c":"code","a7f7cbad":"code","d90a633d":"code","001ce1f5":"code","d11f9f26":"code","af28b930":"code","f8a7d4e5":"code","88b9d511":"code","96951aa6":"code","85202b1d":"code","7bc86c7b":"code","cd2ccec7":"code","1bc46513":"code","fd7d2cf1":"code","2f78a9c6":"code","a322f0a8":"code","e6756483":"code","9ab204c4":"code","d84ec1e2":"code","676f30bc":"code","b5ad5996":"code","0e3f3395":"code","8e968e5f":"code","d4abb017":"code","73851924":"code","896232a6":"code","dbffad9d":"code","8cecb592":"code","473fb5bb":"code","d3537eaf":"code","21bfe17c":"code","64f2339e":"code","fa34f564":"code","ab14c83e":"code","a6b842c4":"code","012458cc":"code","a31bc4fe":"code","32fcbc52":"markdown","c97ba2c4":"markdown","e1236371":"markdown","e07e76c3":"markdown","67a50dc9":"markdown","da1a6a39":"markdown","9881c58b":"markdown","5ac9c5c6":"markdown","0da530e9":"markdown","ba582406":"markdown","a264cc96":"markdown","3bc84c6f":"markdown","99156cef":"markdown","2cae73e2":"markdown","b8b3e958":"markdown","ddd63e27":"markdown","b54612c9":"markdown","22e99c70":"markdown","ab1b719e":"markdown","16db4374":"markdown","534c4026":"markdown","9fe66838":"markdown","79ec1e0e":"markdown"},"source":{"343d5538":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport os\nprint(os.listdir(\"..\/input\"))\n\nimport warnings\nwarnings.simplefilter(action='ignore', category=FutureWarning)","13e7b46e":"#Checking top few rows of the data\nheart = pd.read_csv(\"..\/input\/heart.csv\")\nheart.head()","e3f96001":"#Checking the structure of the data set\n#There are 303 observations and 14 variables in the data set\n\nprint(heart.shape)","15be0d11":"print(heart.info())","088ec8c1":"#Let's sum the null values for each column\n\nheart.isnull().sum()","8c090917":"#This calculates the percent of missing values in eac column of the data set.\n#The results are 0 because there are no null values here.\n#I am doing this for practice purpose.\n\nround(100*(heart.isnull().sum()\/len(heart.index)),2)","82893cc0":"#Finding the statistical features of the data set using describe function.\n\nheart.describe()","f75f565f":"#Density and histogram plots\n\nsns.boxplot(y= heart['age'])\nplt.title('Boxplot of Age')\nplt.show()\n\n#We can see that the median age here is around 55 years\n#Also most of the observations have age between 47-61 years","40af3c84":"#Creating subplots\nplt.figure(figsize=(15, 12))\n\n#Subplot1 \nplt.subplot(2,2,1)\nplt.title('Distribution of Age')\nsns.distplot(heart['age'], rug = True)\n\n\n#Subplot2\nplt.subplot(2,2,2)\nplt.title('Distribution of trestbps')\nsns.distplot(heart['trestbps'], rug = True)\n\n\n#Subplot3\nplt.subplot(2,2,3)\nplt.title('Distribution of chol')\nsns.distplot(heart['chol'], rug = True)\n\n\n#Subplot4\nplt.subplot(2,2,4)\nplt.title('Distribution of thalach')\nsns.distplot(heart['thalach'], rug = True)\n\nplt.show()","2dcada6c":"#plt.figure(figsize=(10, 6))\n\n#scatter plot of age and trestbps\n#sns.jointplot('age','trestbps',heart)\n#plt.show()\n## found it as not relevant for this analysis##","0c289d21":"#Let's dig deeper into the concentrated part, where trestbps is less than 150 and age is between 40-70\n\ndf1 = heart[(heart.trestbps <150) & (heart.trestbps > 100) & (heart.age <= 70) & (heart.age >=40)]\n\nsns.jointplot('age','trestbps',df1, kind=\"hex\", color = 'k')\nplt.show()\n\n#More concentration can be found in case of age between 55-58 and trestbps 130","dd1ca4fb":"#scatter plot of age and trestbps\n#sns.jointplot('age','chol',heart)\n#plt.show()\n## found it as not relevant for this analysis##","08e221da":"#Visualizing the concentrated part for chol\n\ndf2 = heart[(heart.chol <=300)]\nsns.jointplot('age','chol',df2, kind = 'hex', color = 'k')\nplt.show()\n\n#More concentration can be found in case of age between 50-60 and chol 200-250","37c62f87":"#scatter plot of age and thalach\n\n#sns.jointplot('age','thalach', heart)\n#plt.show()\n\n#shows a decreasing pattern\n## found it as not relevant for this analysis##","ba88d600":"#Let's create pairwise scatterplots\ndf3 = heart[['age', 'trestbps', 'chol', 'thalach', 'oldpeak']]\n\n\nplt.figure(figsize = (15,12))\nsns.pairplot(df3)\nplt.show()","6b717ba0":"#correlation between the variables\n\ncor = heart.corr()\nround(cor,2)","196f8327":"#Making heatmap to understand the correlation better\n\nplt.figure(figsize= (15,12))\n\nsns.heatmap(cor, cmap = 'YlGnBu', annot = True)\nplt.show()\n\n#We can see that there is a positive correlation between the target and cp, thalach and slope.\n#There is a negative correaltion between target and age, gender, oldpeak, thal etc. ","af732b4e":"heart.head()","1f8b577c":"for col in ['sex','cp','fbs','restecg','exang','slope','ca','thal','target']:\n    heart[col]= heart[col].astype('category')","dcd9e075":"#Now let's look at some categorical data.\n\n#Gender\nsns.countplot(x='target', data = heart, hue = 'sex')\nplt.title('Number of Male & Female with heart disease')\nplt.show()","2bbd3408":"#Chest Pain type\n\nsns.countplot(x='target', hue= 'cp',data = heart)\nplt.title('Chest Pain Type')\nplt.show()\n\n#There are 4 types of chest pain(0,1,2,3) out of which cp = 2 is the most common type of chest pain in case of people who have heart disease.\n#Those who do not have a heart disease experience cp = 0 type of chest pain.","c84da72c":"#fbs (Fasting Blood Sugar > 120 mg\/dl)\n\nsns.countplot(hue='fbs',x ='target',data = heart)\nplt.title('Fasting Blood Sugar > 120 mg\/dl')\nplt.show()\n\n#Patients with heart disease doesn't necessarily have fbs >120 mg\/dl.  ","a7f7cbad":"#restecg(resting electrocardiographic results (values 0,1,2))\n\nsns.countplot(x='restecg',hue ='target',data = heart)\nplt.title('resting electrocardiographic results')\nplt.show()","d90a633d":"#exang- Exercise induced angina (1 = yes; 0 = no)\n\nsns.countplot(hue='exang',x ='target',data = heart)\nplt.title('Exercise induced angina (1 = yes; 0 = no)')\nplt.show()","001ce1f5":"#the slope of the peak exercise ST segment\n\nsns.countplot(hue='slope',x ='target',data = heart)\nplt.title('Slope of the peak exercise ST segment')\nplt.show()\n\n#Most of the patients who have a heart disease have a slope = 2","d11f9f26":"#ca- number of major vessels (0-3) colored by flourosopy\n\nsns.countplot(hue='ca',x ='target',data = heart)\nplt.title('Number of major vessels (0-3) colored by flourosopy')\nplt.show()\n\n#Most of the patients who have a heart disease have a ca = 0","af28b930":"#thal- no explanation provided, but probably thalassemia (3 normal; 6 fixed defect; 7 reversable defect)\n\nsns.countplot(hue='thal',x ='target',data = heart)\nplt.title('thal')\nplt.show()\n\n#Most of the patients who have a heart disease have a thal = 2","f8a7d4e5":"#Target\n\nsns.countplot(x ='target',data = heart)\nplt.title('Have heart disease or not')\nplt.show()","88b9d511":"heart.tail()","96951aa6":"print(\" Unique Entries for column 'heart' are \", heart['cp'].unique())\nprint(\" Unique Entries for column 'fbs' are \", heart['fbs'].unique())\nprint(\" Unique Entries for column 'restecg' are \", heart['restecg'].unique())\nprint(\" Unique Entries for column 'exang' are \", heart['exang'].unique())\nprint(\" Unique Entries for column 'oldpeak' are \", heart['oldpeak'].unique())\nprint(\" Unique Entries for column 'slope' are \", heart['slope'].unique())\nprint(\" Unique Entries for column 'ca' are \", heart['ca'].unique())\nprint(\" Unique Entries for column 'that' are \", heart['thal'].unique())","85202b1d":"df= heart[['age','trestbps','thalach','cp','chol','restecg','oldpeak','slope','ca','thal']]\ndf= df.apply(pd.to_numeric)\nnormalized_df=(df-df.mean())\/df.std()","7bc86c7b":"df.info()","cd2ccec7":"heart= heart.drop(['age','trestbps','thalach','cp','chol','restecg','oldpeak','slope','ca','thal'],1)","1bc46513":"heart= pd.concat([heart,normalized_df], axis=1)\nheart.tail()","fd7d2cf1":"Heart_attack_rate= (sum(heart['target'])\/len(heart['target'].index))*100\nHeart_attack_rate #54.45% is the Heart attack rate","2f78a9c6":"heart.columns","a322f0a8":"X= heart.drop(['target'], axis=1)\ny= heart['target']\n\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import train_test_split\nimport statsmodels.api as sm\nimport functools\n# importing Logistic Regression\nfrom sklearn.linear_model import LogisticRegression\nX_train, X_test, y_train, y_test= train_test_split(X, y, train_size=.7, random_state=10)\n# Will Create a generalised linear model GLM for begining the model building\nlogml = sm.GLM(y_train,(sm.add_constant(X_train.astype(float))), family= sm.families.Binomial())\n\nprint(logml.fit().summary())\n\n","e6756483":"import matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\nplt.figure(figsize=(20,10))\nsns.heatmap(X_train.astype(float).corr(), annot=True)\nplt.show()","9ab204c4":"\ndef vif_cal(input_data, dependent_col):\n    vif_df = pd.DataFrame( columns = ['Var', 'Vif'])\n    x_vars=input_data.drop([dependent_col], axis=1)\n    xvar_names=x_vars.columns\n    for i in range(0,xvar_names.shape[0]):\n        y=x_vars[xvar_names[i]] \n        x=x_vars[xvar_names.drop(xvar_names[i])]\n        rsq=sm.OLS(y,x).fit().rsquared  \n        vif=round(1\/(1-rsq),2)\n        vif_df.loc[i] = [xvar_names[i], vif]\n    return vif_df.sort_values(by = 'Vif', axis=0, ascending=False, inplace=False)","d84ec1e2":"vif_cal(input_data= heart.astype(float), dependent_col='target')","676f30bc":"from sklearn.linear_model import LogisticRegression\nfrom sklearn import metrics\nlogsk = LogisticRegression()\nlogsk.fit(X_train.astype(float), y_train)\ny_predict= logsk.predict_proba(X_test.astype(float)) \n# Converting y_pred to a dataframe which is an array\ny_pred_df = pd.DataFrame(y_predict)\n# Converting to column dataframe\ny_pred_1 = y_pred_df.iloc[:,[1]]\ny_pred_1.head() # below data shows the probability of heart attack","b5ad5996":"# Converting y_test to dataframe\ny_test_df = pd.DataFrame(y_test).reset_index()","0e3f3395":"y_test_df.head()","8e968e5f":"y_pred_1.head()","d4abb017":"# Appending y_test_df and y_pred_1\ny_pred_final = pd.concat([y_test_df,y_pred_1], axis =1)\n# Renaming the column \ny_pred_final= y_pred_final.rename(columns={ 1 : 'Heart_attack_Prob'})\ny_pred_final.head()","73851924":"# Creating new column 'predicted' with 1 if Churn_Prob>0.5 else 0\ny_pred_final['predicted'] = y_pred_final.Heart_attack_Prob.map( lambda x: 1 if x > 0.5 else 0)\ny_pred_final.head()","896232a6":"from sklearn import metrics\n# Confusion matrix \nconfusion = metrics.confusion_matrix( y_pred_final.target, y_pred_final.predicted)\nconfusion","dbffad9d":"#Let's check the overall accuracy.\nmetrics.accuracy_score( y_pred_final.target, y_pred_final.predicted)# model accuracy is 78%","8cecb592":"TP = confusion[0,0] # true positive \nTN = confusion[1,1] # true negatives\nFP = confusion[0,1] # false positives\nFN = confusion[1,0] # false negatives","473fb5bb":"sensitivity = TP\/float(TP+FN)\nprint(\"Sensitivity= \",sensitivity)\nspecificity= TN\/float(TN+FP)\nprint(\"specificity= \", specificity)\nprint(\"Precision= \", TP\/float(TP+FP))\n","d3537eaf":"from sklearn.metrics import roc_curve, auc\ndef draw_roc( actual, probs ):\n    fpr, tpr, thresholds = metrics.roc_curve( actual, probs,\n                                              drop_intermediate = False )\n    auc_score = metrics.roc_auc_score( actual, probs )\n    plt.figure(figsize=(6, 4))\n    plt.plot( fpr, tpr, label='ROC curve (area = %0.2f)' % auc_score )\n    plt.plot([0, 1], [0, 1], 'k--')\n    plt.xlim([0.0, 1.0])\n    plt.ylim([0.0, 1.05])\n    plt.xlabel('False Positive Rate or [1 - True Negative Rate]')\n    plt.ylabel('True Positive Rate')\n    plt.title('Receiver operating characteristic example')\n    plt.legend(loc=\"lower right\")\n    plt.show()\n\n    return fpr, tpr, thresholds","21bfe17c":"draw_roc(y_pred_final.target, y_pred_final.predicted)","64f2339e":"# Let's create columns with different probability cutoffs \nnumbers = [float(x)\/10 for x in range(10)]\nfor i in numbers:\n    y_pred_final[i]= y_pred_final.Heart_attack_Prob.map( lambda x: 1 if x > i else 0)\ny_pred_final.head()\n","fa34f564":"# Now let's calculate accuracy sensitivity and specificity for various probability cutoffs.\ncutoff_df = pd.DataFrame( columns = ['prob','accuracy','sensi','speci'])\nfrom sklearn.metrics import confusion_matrix\nnum = [0.0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9]\nfor i in num:\n    cm1 = metrics.confusion_matrix( y_pred_final.target, y_pred_final[i] )\n    total1=sum(sum(cm1))\n    accuracy = (cm1[0,0]+cm1[1,1])\/total1\n    sensi = cm1[0,0]\/(cm1[0,0]+cm1[0,1])\n    speci = cm1[1,1]\/(cm1[1,0]+cm1[1,1])\n    cutoff_df.loc[i] =[ i ,accuracy,sensi,speci]\nprint(cutoff_df)","ab14c83e":"# Let's plot accuracy sensitivity and specificity for various probabilities.\ncutoff_df.plot.line(x='prob', y=['accuracy','sensi','speci'])\nplt.show()","a6b842c4":"y_pred_final['final_predicted'] = y_pred_final.Heart_attack_Prob.map( lambda x: 1 if x > 0.7 else 0)\ny_pred_final.head()","012458cc":"metrics.accuracy_score( y_pred_final.target, y_pred_final.final_predicted)","a31bc4fe":"metrics.confusion_matrix( y_pred_final.target, y_pred_final.final_predicted )","32fcbc52":"drop the orginal coulmns 'cp','restecg','oldpeak','slope','ca','thal' from the data frame as we would like to replace it with the normalized valuew","c97ba2c4":"Plotting ROC curve","e1236371":"Lets Build a model to predict Heart attack through Logistic regreesion approach. Will use \"Multivariate Logistic Regression analysis\"","e07e76c3":"Above data shows as only 8 cases are predicted as missed now as per confusion matrix which was 16 earlier.","67a50dc9":"Lets Run the Logistic rgression model and prdict the values","da1a6a39":"Now lets find out the optimum cut-off predicted probability through ROC curve","9881c58b":"Checking the columns","5ac9c5c6":"From above plot of ROC curve, the optimum cut-off seems to be .7 around","0da530e9":"Modelling : Lets define the independent and Target variable","ba582406":"Feature selection using RFE","a264cc96":"Lets Plot the Correlation matrix","3bc84c6f":"Checking no. of unique entries for columns","99156cef":"Lets update heart data frame with normalized values","2cae73e2":"Lets Check the Heart attack rate in as is condition prior to model building","b8b3e958":"Calculating VIF value","ddd63e27":"Its 79% now , which was 78% earlier","b54612c9":"'exang' & 'fbs' cplumns have binary entries already, all others needs to be normalised for Logistic regression analysis. We will normalise by standard deviation method","22e99c70":"It shows as 16 cases were predicted as missed as per confusion matrix","ab1b719e":"Let's check the overall accuracy.\n","16db4374":"**INTRODUCTION:**\n\nThe reason behind this work was the say \" Learning by doing\"..So be it!! I am learning now. \n\nThis dataset being clean and small and relevant to everybody , was easy to analyze. By end of the analysis,the self expectation is to run anyone's report with 13 independent variables and get a sense if he\/she is more likely to get heart attack. It may help in treating the right person and at the right time to minimize the damage. \n\nIn the following analysis  I tried to corelate the results with two different approaches.\n\n1)EDA with basic python libraries usage\n2)Analysing problem through Logistic regression Modeling","534c4026":"**Finding Optimal Cutoff Point**","9fe66838":"Lets check the confusion matrix again","79ec1e0e":"UDF for calculating vif (Variance inflation factor)value to drop independent outliers"}}