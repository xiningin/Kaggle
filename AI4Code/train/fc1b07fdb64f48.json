{"cell_type":{"e9b42ad9":"code","acfccdf3":"code","7eff0e61":"code","b50f6e8c":"code","4d9838f0":"code","5534d04b":"code","52f13cb5":"code","b383922f":"code","13e9b0a5":"code","cb9488f3":"code","ad5993e8":"code","9c37f393":"code","d9febf4a":"code","55f68f7d":"code","b9e9ef9c":"code","e6aff2e0":"code","29508cbe":"code","3958f070":"code","9bf89e49":"code","362547bb":"code","963b9968":"code","47dae4a1":"code","18897009":"code","fe534fe6":"code","6bfd292d":"code","50caf791":"code","bdee8f45":"code","c22b29b7":"code","849d0246":"code","dc147540":"code","b241f98e":"code","8ce82141":"code","c2f56ec3":"code","e62bd433":"code","dc61f498":"code","e655d384":"code","e1fb79f1":"code","0d8dbf0c":"code","cee64d79":"code","4e915aa5":"code","6aa2891f":"code","ea7a5aa7":"code","99c8202a":"code","321094d5":"code","3410f964":"code","75d84f05":"code","ae9cddc8":"code","6ae9510d":"code","d508d5e3":"code","f0eec365":"markdown","e46afe07":"markdown","3e3ab9e4":"markdown","aa725098":"markdown","408b53a0":"markdown","317b6ba3":"markdown","bbe5bec7":"markdown","62fb952f":"markdown","1645ac48":"markdown","ba058f94":"markdown","27d60050":"markdown","2d82db64":"markdown","260eb73f":"markdown","6edafcde":"markdown","e96f5fdd":"markdown","2f18f078":"markdown","78092583":"markdown","8f5caf0b":"markdown","1bdc8a56":"markdown","5b5220db":"markdown","75b1ce4a":"markdown","b4a23aab":"markdown","42ab5972":"markdown","60fe22d9":"markdown","a538a51e":"markdown","6156eb3a":"markdown","b8921c37":"markdown","770ed873":"markdown"},"source":{"e9b42ad9":"import numpy as np, pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn import metrics\nimport warnings\nwarnings.filterwarnings('ignore')","acfccdf3":"bank= pd.read_csv('..\/input\/bank-customers\/Churn Modeling.csv')\nbank.head()","7eff0e61":"bank.info()","b50f6e8c":"bank.describe().T","4d9838f0":"# checking null values\nbank.isna().sum()","5534d04b":"bank.drop(['RowNumber','Surname','CustomerId'],1,inplace=True)","52f13cb5":"bank.head()","b383922f":"# treating categorical columns\nbank.Geography.value_counts(dropna=False)","13e9b0a5":"from sklearn import preprocessing\nlabel_encoder= preprocessing.LabelEncoder()","cb9488f3":"bank.Geography=label_encoder.fit_transform(bank.Geography)","ad5993e8":"# similarly doing label encoding for other categorical columns\nbank.Gender=label_encoder.fit_transform(bank.Gender)","9c37f393":"# Let's have a look of our dataset\nbank.head()","d9febf4a":"from sklearn.model_selection import train_test_split","55f68f7d":"# train_test_split\ndf_train,df_test=train_test_split(bank,train_size=0.7, test_size=0.3, random_state=100)","b9e9ef9c":"df_train.describe().T","e6aff2e0":"df_test.describe().T","29508cbe":"from sklearn.preprocessing import MinMaxScaler\nscaler= MinMaxScaler()","3958f070":"col= df_train.columns\ncol= col.drop('Exited')","9bf89e49":"df_train[col]= scaler.fit_transform(df_train[col])","362547bb":"df_test[col]= scaler.transform(df_test[col])","963b9968":"# train dataset\ny_train= df_train.pop('Exited')\nX_train= df_train","47dae4a1":"# test dataset\ny_test= df_test.pop('Exited')\nX_test= df_test","18897009":"import statsmodels.api as sm","fe534fe6":"# Logistic Regression\nlr=sm.GLM(y_train,sm.add_constant(X_train),family=sm.families.Binomial())","6bfd292d":"# fitting the model\nlr1= lr.fit()\nprint(lr1.summary())","50caf791":"X_train.drop('HasCrCard',1,inplace=True)","bdee8f45":"lr=sm.GLM(y_train,sm.add_constant(X_train),family=sm.families.Binomial())","c22b29b7":"# fitting the model\nlr2= lr.fit()\nprint(lr2.summary())","849d0246":"X_train.drop('NumOfProducts',1,inplace=True)","dc147540":"lr=sm.GLM(y_train,sm.add_constant(X_train),family=sm.families.Binomial())","b241f98e":"# fitting the model\nlr3= lr.fit()\nprint(lr3.summary())","8ce82141":"from statsmodels.stats.outliers_influence import variance_inflation_factor\n\nvif = pd.DataFrame()\nvif['Features'] = X_train.columns\nvif['VIF'] = [variance_inflation_factor(X_train.values, i) for i in range(X_train.shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","c2f56ec3":"y_train_pred=lr3.predict(sm.add_constant(X_train))","e62bd433":"y_train_pred.head()","dc61f498":"# converting prediction in the form of a dataframe for better understanding\nscore=pd.DataFrame({'CustID':y_train.index,'Churn':y_train.values,'Churn_prob':y_train_pred})\nscore.set_index('CustID',inplace=True)\nscore.head()","e655d384":"# Let's create columns with different probability cutoffs\nnumbers = [float(x)\/10 for x in range(10)]\nfor i in numbers:\n    score[i]= score.Churn_prob.map(lambda x: 1 if x > i else 0)\nscore.head()","e1fb79f1":"def draw_roc( actual, probs ):\n    fpr, tpr, thresholds = metrics.roc_curve( actual, probs,\n                                              drop_intermediate = False)\n    auc_score = metrics.roc_auc_score( actual, probs )\n    plt.figure(figsize=(5, 5))\n    plt.plot( fpr, tpr, label='ROC curve (area = %0.2f)' % auc_score )\n    plt.plot([0, 1], [0, 1], 'k--')\n    plt.xlim([0.0, 1.0])\n    plt.ylim([0.0, 1.05])\n    plt.xlabel('False Positive Rate or [1 - True Negative Rate]')\n    plt.ylabel('True Positive Rate')\n    plt.title('Receiver operating characteristic example')\n    plt.legend(loc=\"lower right\")\n    plt.show()\n\n    return None","0d8dbf0c":"fpr, tpr, thresholds = metrics.roc_curve( score.Churn, score.Churn_prob, drop_intermediate = False )","cee64d79":"draw_roc(score.Churn, score.Churn_prob)","4e915aa5":"from sklearn.metrics import confusion_matrix\n\ncutoff_df = pd.DataFrame( columns = ['prob','accuracy','sensi','speci'])\n\n# TP = confusion[1,1] # true positive \n# TN = confusion[0,0] # true negatives\n# FP = confusion[0,1] # false positives\n# FN = confusion[1,0] # false negatives\n\nnum = [0.0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9]\nfor i in num:\n    cm1 = metrics.confusion_matrix(score.Churn, score[i] )\n    total1=sum(sum(cm1))\n    accuracy = (cm1[0,0]+cm1[1,1])\/total1\n    \n    speci = cm1[0,0]\/(cm1[0,0]+cm1[0,1])\n    sensi = cm1[1,1]\/(cm1[1,0]+cm1[1,1])\n    cutoff_df.loc[i] =[ i ,accuracy,sensi,speci]\nprint(cutoff_df)","6aa2891f":"# Let's plot accuracy sensitivity and specificity for various probabilities.\ncutoff_df.plot.line(x='prob', y=['accuracy','sensi','speci'])\nplt.show()","ea7a5aa7":"score['final_prediction'] = score.Churn_prob.map( lambda x: 1 if x > 0.3 else 0)\nscore.head()","99c8202a":"# Let's check the accuracy on train dataset\nmetrics.accuracy_score(score.Churn,score.final_prediction)","321094d5":"col= X_train.columns","3410f964":"y_test_pred=lr3.predict(sm.add_constant(X_test[col]))","75d84f05":"# DataFrame\nscore2=pd.DataFrame({'CustID':y_test.index,'Churn':y_test.values,'Churn_prob':y_test_pred})\nscore2.set_index('CustID',inplace=True)\nscore2.head()","ae9cddc8":"score2['Final_predction']=score2.Churn_prob.map(lambda x:1 if x>0.2 else 0)","6ae9510d":"score2.head()","d508d5e3":"# Let's check the accuracy on test dataset\nmetrics.accuracy_score(score2.Churn,score2.Final_predction)","f0eec365":"**Defining X and y**","e46afe07":"**If this kernel helped in your learning, then please UPVOTE \u2013 because they are the source of motivation!**","3e3ab9e4":"### ROC Curve","aa725098":"**Label Encoding** refers to converting the labels into numeric form so as to convert it into the machine-readable form. ","408b53a0":"**From the curve above, 0.2 is the optimum point to take it as a cutoff probability.**","317b6ba3":"Dropping ***NumOfProducts*** beacuse it's **p-value** is greater than our assumed signifiance level of 0.05","bbe5bec7":"## Model Building","62fb952f":"### **Summary**\n#### Accuracy on train is 77.6%\n#### Accuracy on test is 70.0%\n#### We can say that our model (lr3) is robust and generalisable.","1645ac48":"### Operations on the Test Dataset","ba058f94":"We will use our model **(lr3)** for prediction on the test dataset","27d60050":"## **Context**\nDataset contain list of customers who are either withdrawing or retaining their account with the bank. With the help of Logistic Regression model we will try to analyse the main factors behind the churn and maintain accuracy.","2d82db64":"A **variance inflation factor** **(VIF)** provides a measure of *multicollinearity* among the independent variables in a multiple regression model.\n\nDetecting *multicollinearity* is important because while multicollinearity does not reduce the explanatory power of the model, it does reduce the statistical significance of the independent variables. \n\nA large **variance inflation factor** **(VIF)** on an independent variable indicates a highly collinear relationship to the other variables that should be considered or adjusted for in the structure of the model and selection of independent variables.","260eb73f":"Dropping ***HasCrCard*** beacuse it's **p-value** is greater than our assumed signifiance level of 0.05","6edafcde":"We got an accuracy of **77.6%** on train dataset","e96f5fdd":"**VIF** less than 10 is acceptable.\nSo we will go with the current set of variables\/features\/columns","2f18f078":"**Feature Scaling** is a technique to standardize the *independent features* present in the data in a fixed range. It is performed during the data pre-processing to handle highly varying magnitudes or values or units. If feature scaling is not done, then a machine learning algorithm tends to weigh greater values, higher and consider smaller values as the lower values, regardless of the unit of the values.","78092583":"### Checking Multicollinearity using VIF","8f5caf0b":"![image.png](attachment:51aba10d-7a43-41db-bbb3-cb2581b5e6df.png)![image.png](attachment:2bf1075a-28b6-492c-86b7-907b1772e7e1.png)","1bdc8a56":"We can see that our **target variable** (*Exited*) in both the dataset (train and test) have equal distribution of 0's and 1's (see the mean).","5b5220db":"# **Bank Customer Churn Prediction using Logistic Regression**\nHello friends, In this kernel, I will predict the customer churn for a bank and understand the key factors behind their churn by using a famous supervised machine learning algorithm \"Logistic Regression\".","75b1ce4a":"Removing the unnecessary columns from bank dataset","b4a23aab":"### Feature Scaling","42ab5972":"After dropping we will again build our model with the remaing features\/columns","60fe22d9":"We got an accuracy of **70%** on our test dataset","a538a51e":"## **Thank You**","6156eb3a":"We have kept train and test size as 70% and 30% respectively","b8921c37":"### Prediction","770ed873":"### Finding Optimal Cutoff Probability \nNow let's calculate accuracy sensitivity and specificity for various probability cutoffs."}}