{"cell_type":{"587dea96":"code","a0d65019":"code","f5e1269d":"code","d1f025b0":"code","5efbb4ff":"code","eef9e36d":"code","5ccd4392":"code","b9a00a93":"code","1e3a0ee3":"code","f9a185e7":"code","c39611cf":"code","8259ba7e":"code","7677220e":"code","5d5f6bd6":"code","bb7b595b":"code","e90d3d11":"code","3f12fb0c":"code","39d7e7db":"code","6514d639":"code","24ce347f":"code","676d4fc1":"code","f036b31d":"code","423bc6aa":"code","dfb35dbb":"code","bd2f97d9":"code","5bf284ae":"code","14d21518":"markdown","5b50ecd1":"markdown","72add716":"markdown","aac90810":"markdown","ebefdb19":"markdown"},"source":{"587dea96":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","a0d65019":"raw_data = pd.read_csv(\"\/kaggle\/input\/malware-detection\/Malware dataset.csv\")\nraw_data.head()","f5e1269d":"raw_data.columns","d1f025b0":"# read some statistics of the dataset\nraw_data.describe(include=\"all\")","5efbb4ff":"# Check the DataType of our dataset\nraw_data.info()","eef9e36d":"#Start Processing\ndata = raw_data","5ccd4392":"data[\"classification\"].value_counts()","b9a00a93":"data['classification'] = data.classification.map({'benign':0, 'malware':1})\ndata.head()","1e3a0ee3":"# Shuffle data\ndata = data.sample(frac=1).reset_index(drop=True)\ndata.head()","f9a185e7":"#Import drawing tools\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns","c39611cf":"sns.countplot(data[\"classification\"])\nplt.show()","8259ba7e":"corrMatrix = data.corr()\nsns.heatmap(corrMatrix, annot=True)\nplt.show()","7677220e":"X = data.drop([\"hash\",\"classification\",'vm_truncate_count','shared_vm','exec_vm','nvcsw','maj_flt','utime'],axis=1)\nY = data[\"classification\"]","5d5f6bd6":"from sklearn.model_selection import train_test_split\nx_train,x_test,y_train,y_test=train_test_split(X,Y,test_size=0.2,random_state=1)","bb7b595b":"# Data normalization\n\nfrom sklearn.preprocessing import StandardScaler\n\nscaler = StandardScaler()","e90d3d11":"x_train = scaler.fit_transform(x_train)\nx_test = scaler.transform(x_test)","3f12fb0c":"import tensorflow as tf","39d7e7db":"#Number of attributes\ninput_size = 27 \n\n#Number of Outputs\noutput_size = 2 \n\n# Use same hidden layer size for both hidden layers. Not a necessity.\nhidden_layer_size = 50\n    \n# define how the model will look like\nmodel = tf.keras.Sequential([\n    tf.keras.layers.Dense(hidden_layer_size, input_shape=(input_size,), activation='relu'), # 1st hidden layer\n    tf.keras.layers.Dense(hidden_layer_size, activation='relu'),\n    tf.keras.layers.Dense(hidden_layer_size, activation='relu'),\n    tf.keras.layers.Dense(hidden_layer_size, activation='relu'),\n    tf.keras.layers.Dense(hidden_layer_size, activation='relu'),\n    tf.keras.layers.Dense(hidden_layer_size, activation='relu'),\n    tf.keras.layers.Dense(output_size, activation='softmax') # output layer\n])\n\nmodel.summary()","6514d639":"model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n\n#from keras.optimizers import SGD\n#opt = SGD(lr=0.01)\n#sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n#model.compile(optimizer = sgd, loss = \"sparse_categorical_crossentropy\", metrics=['accuracy'])","24ce347f":"# set the batch size\nbatch_size = 100\n\n# set a maximum number of training epochs\nmax_epochs = 20\n\n# set an early stopping mechanism\n# let's set patience=2, to be a bit tolerant against random validation loss increases\nearly_stopping = tf.keras.callbacks.EarlyStopping(patience=2)","676d4fc1":"result = model.fit(x=x_train,\n                   y=y_train,\n                   batch_size=batch_size,\n                   epochs=max_epochs,\n                   verbose=1,\n                   #callbacks=[early_stopping],\n                   validation_split=0.2)\n","f036b31d":"# Visualize the result\nacc = result.history['accuracy']\nval_acc = result.history['val_accuracy']\nloss = result.history['loss']\nval_loss = result.history['val_loss']\n\nepochs = range(1, len(acc) + 1)\n\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\nsns.set_style(\"white\")\nplt.suptitle('Train history', size = 15)\n\nax1.plot(epochs, acc, \"bo\", label = \"Training acc\")\nax1.plot(epochs, val_acc, \"b\", label = \"Validation acc\")\nax1.set_title(\"Training and validation acc\")\nax1.legend()\n\nax2.plot(epochs, loss, \"bo\", label = \"Training loss\", color = 'red')\nax2.plot(epochs, val_loss, \"b\", label = \"Validation loss\", color = 'red')\nax2.set_title(\"Training and validation loss\")\nax2.legend()\n\nplt.show()","423bc6aa":"test_loss, test_accuracy = model.evaluate(x_test, y_test)\n\nprint('\\nTest loss: {0:.6f}. Test accuracy: {1:.6f}%'.format(test_loss, test_accuracy*100.))","dfb35dbb":"from keras.optimizers import SGD\nsgd = SGD(lr=0.001, decay=1e-6, momentum=0.9, nesterov=True)\nmodel.compile(optimizer = sgd, loss = \"sparse_categorical_crossentropy\", metrics=['accuracy'])","bd2f97d9":"result = model.fit(x=x_train,\n                   y=y_train,\n                   batch_size=batch_size,\n                   epochs=30,\n                   verbose=1,\n                   initial_epoch=10, #start from epoch 11\n                   callbacks=[early_stopping], #prevent overfitting\n                   validation_split=0.2)","5bf284ae":"test_loss, test_accuracy = model.evaluate(x_test, y_test)\n\nprint('\\nTest loss: {0:.6f}. Test accuracy: {1:.6f}%'.format(test_loss, test_accuracy*100.))","14d21518":"Based on the characteristics of the observations, the dataset was created in a Unix \/ Lunix-based\nvirtual machine for classification purposes, which are harmless with malware software for Android\ndevices. The data set consists of 100,000 observation data and 35 features. Below is a table of\nspecifications and descriptions.","5b50ecd1":"| Features Description \t| Properties                                                      \t|\n|----------------------\t|-----------------------------------------------------------------\t|\n| hash APK\/ SHA256     \t| file name                                                       \t|\n| milisecond           \t| time                                                            \t|\n| classification       \t| malware\/beign                                                   \t|\n| state                \t| flag of unrunable\/runnable\/stopped tasks                        \t|\n| usage_counter        \t| task structure usage counter                                    \t|\n| prio                 \t| keeps the dynamic priority of a process                         \t|\n| static_prio          \t| static priority of a process                                    \t|\n| normal_prio          \t| priority without taking RT-inheritance into account             \t|\n| policy               \t| planning policy of the process                                  \t|\n| vm_pgoff             \t| the offset of the area in the file, in pages.                   \t|\n| vm_truncate_count    \t| used to mark a vma as now dealt with                            \t|\n| task_size            \t| size of current task.                                           \t|\n| cached_hole_size     \t| size of free address space hole.                                \t|\n| free_area_cache      \t| first address space hole                                        \t|\n| mm_users             \t| address space users                                             \t|\n| map_count            \t| number of memory areas                                          \t|\n| hiwater_rss          \t| peak of resident set size                                       \t|\n| total_vm             \t| total number of pages                                           \t|\n| shared_vm            \t| number of shared pages.                                         \t|\n| exec_vm              \t| number of executable pages.                                     \t|\n| reserved_vm          \t| number of reserved pages.                                       \t|\n| nr_ptes              \t| number of page table entries                                    \t|\n| end_data             \t| end address of code component                                   \t|\n| last_interval        \t| last interval time before thrashing                             \t|\n| nvcsw                \t| number of volunteer context switches.                           \t|\n| nivcsw               \t| number of in-volunteer context switches                         \t|\n| min_flt              \t| min\u00f6r page faults                                               \t|\n| maj_flt              \t| maj\u00f6r page faults                                               \t|\n| fs_excl_counter      \t| \u0131t holds file system exclusive resources.                       \t|\n| lock                 \t| the read-write synchronization lock used for file system access \t|\n| utime                \t| user time                                                       \t|\n| stime                \t| system time                                                     \t|\n| gtime                \t| guest time                                                      \t|\n| cgtime               \t| cumulative group time. Cumulative resource counter              \t|\n| signal_nvcsw         \t| used as cumulative resource counter.                            \t|\n","72add716":"Before we feed to a NN, we need to normalize the data","aac90810":"The data is already clean.","ebefdb19":"Further train the model using SGD with lr=0.001"}}