{"cell_type":{"155e1e16":"code","d152034c":"code","626cda9c":"code","d2de51fa":"code","27ad7566":"code","1a49f5b2":"code","02b6b090":"code","538f9929":"code","8857f957":"code","bc767f9c":"code","e4fee554":"code","00cd5cb6":"code","d9199838":"code","75fc3353":"code","6d608386":"code","2a07f62e":"code","735abdc6":"code","d6830236":"code","2617f1dc":"code","ddfd1b1f":"code","73f19def":"code","d0673d09":"code","bdd8a914":"code","6fdc8389":"code","e3b352df":"code","8be31a9f":"code","1db35b47":"code","8097ce83":"code","06029a5c":"code","7dad8503":"markdown","f1459a15":"markdown","a7953a4a":"markdown","b27b1fee":"markdown","395a8b09":"markdown","1d143a13":"markdown","4226deb5":"markdown","e5d5003d":"markdown","6383d25b":"markdown","1fa2fcfd":"markdown","a6c0e9ca":"markdown","492b7412":"markdown","088e920d":"markdown","17d116ea":"markdown","a6a7b58c":"markdown","e7e3dcf8":"markdown","e0a113e6":"markdown","52e60a03":"markdown","fa4bf7d6":"markdown","effb6c98":"markdown","9e99406a":"markdown","6b213de4":"markdown","f6f21593":"markdown","e39d35d7":"markdown","38c1757b":"markdown","d178ecc6":"markdown","b379ba32":"markdown","52789ecc":"markdown","24292d8f":"markdown","0299b169":"markdown","b8a7d8b7":"markdown","85b3f318":"markdown","9c59972d":"markdown","cfe24ebf":"markdown","4a9298df":"markdown","e06e9a48":"markdown","54ce1630":"markdown","2bfdf496":"markdown","fe16b71f":"markdown"},"source":{"155e1e16":"%matplotlib inline\n%config InlineBackend.figure_format = 'retina'\nimport matplotlib.pyplot as plt\n\nplt.style.use('ggplot')\nimport matplotlib.cm as cm\nimport seaborn as sns\n\nimport pandas as pd\nimport pandas_profiling\nimport numpy as np\nfrom numpy import percentile\nfrom scipy import stats\nfrom scipy.stats import skew\nfrom scipy.special import boxcox1p\n\nimport os, sys\nimport calendar\n\nfrom sklearn.preprocessing import StandardScaler, RobustScaler\nfrom sklearn.preprocessing import LabelEncoder, OneHotEncoder\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.dummy import DummyRegressor\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline, make_pipeline\nfrom sklearn.model_selection import cross_val_score, cross_validate\nfrom sklearn.model_selection import GridSearchCV, KFold\nfrom sklearn.decomposition import PCA\nfrom sklearn.metrics import mean_squared_log_error, make_scorer\nfrom sklearn.metrics.scorer import neg_mean_squared_error_scorer\n\nfrom sklearn.linear_model import LinearRegression, ElasticNet, Lasso, Ridge, RidgeCV\nfrom sklearn.svm import SVR\nfrom sklearn.kernel_ridge import KernelRidge\nfrom sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n\n\n\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nplt.rc('font', size=18)        \nplt.rc('axes', titlesize=22)      \nplt.rc('axes', labelsize=18)      \nplt.rc('xtick', labelsize=12)     \nplt.rc('ytick', labelsize=12)     \nplt.rc('legend', fontsize=12)   \n\nplt.rcParams['font.sans-serif'] = ['Verdana']\n\n# function that converts to thousands\n# optimizes visual consistence if we plot several graphs on top of each other\ndef format_1000(value, tick_number):\n    return int(value \/ 1_000)\n\npd.options.mode.chained_assignment = None\npd.options.display.max_seq_items = 500\npd.options.display.max_rows = 500\npd.set_option('display.float_format', lambda x: '%.5f' % x)\n\nlocal = False\ndebug = False\n\nif(local):\n    BASE_PATH = \".\/\"\nelse:\n    BASE_PATH = \"..\/input\/titanic\/\"\n\n","d152034c":"df = pd.read_csv(f\"{BASE_PATH}train.csv\",index_col='PassengerId')\ndf_test = pd.read_csv(f\"{BASE_PATH}test.csv\",index_col='PassengerId')","626cda9c":"df.info(verbose=True, null_counts=True)","d2de51fa":"missing = [(c, df[c].isna().mean()*100) for c in df]\nmissing = pd.DataFrame(missing, columns=[\"column_name\", \"percentage\"])\nmissing = missing[missing.percentage > 0]\ndisplay(missing.sort_values(\"percentage\", ascending=False))","27ad7566":"def cabin_location(df):\n    df['Cabin'].unique()\n    df['Cabin'].fillna('Unknown',inplace = True)\n    df['Cabin_Location'] = df['Cabin'].str[0]\n    #Here we use the class data to determine the Fill Na\n    df['Cabin_Location'] = np.where((df.Pclass==1) & (df['Cabin_Location']=='U'),'C',\n                           np.where((df.Pclass==2) & (df['Cabin_Location']=='U'),'D',\n                           np.where((df.Pclass==3) & (df['Cabin_Location']=='U'),'G',\n                           np.where(df['Cabin_Location']=='T','C',df['Cabin_Location']))))\n    #    cabin_map = {'A':6,'B':5,'C':4,'D':3,'E':2,'F':1,'G':0}\n    #    df['Cabin_Location'] = df['Cabin_Location'].replace(cabin_map)\n    return df\n\n    \n    \ndef title_extract(df):\n    ## Mme is a french honorific for Madame: Similar to Mrs. \n    ## Mlle is a french honorific for Mademoiselle: Similar to Ms. \n    ## Miss = Ms\n    ## Don, Mr, and Sir will be combined into one set\n    ## Donna, Mrs and Lady will be combined into one set\n    ## We will keep: Mr, Mrs, Mss, Dr, Master, Rev, Other \n    name_splice = df['Name'].str.split(pat=', ',n=-1,expand=True)\n    df['Title'] = name_splice[1].str.split(pat='.',n=-1,expand=True)[0]\n    title_correction_map = {'Mme':'Mrs','Mlle':'Mss','Miss':'Mss', 'Don':'Mr','Donna':'Mrs',\n                           'Lady':'Mrs','Sir':'Mr'}\n    title_list = {'Mr','Mrs','Mss','Dr','Master','Rev'}\n    df['Title'].replace(title_list,inplace=True)\n    df['Title'] = df['Title'].apply(lambda x:'Other' if (not any(x in title for title in title_list)) else x )\n    return df\n    \ndef age_fill(df):\n    ## Since we will fill age based on title, we make sure title is called\n    ## Must be careful not to use information from test set so we will call a clean train set\n    ####\n    ## FOR TITANIC TRAIN + TEST MAKES THE POPULATION SO WE HAVE ACCESS TO POPULATION MEANS\n    ## WITHOUT THE RISK OF DATA LEAKING\n    ####\n    \n    df_train = pd.read_csv(f\"{BASE_PATH}train.csv\",index_col='PassengerId')\n    df_test = pd.read_csv(f\"{BASE_PATH}test.csv\",index_col='PassengerId')\n    df_total = pd.concat([df_train, df_test]).copy()\n    df_total = title_extract(df_total)\n    \n    df = title_extract(df)\n    mean_age_by_title = np.round(df_total.groupby('Title').Age.mean()).astype('int64')\n    df['Age'] = df.groupby('Title')['Age'].apply(lambda x: x.fillna(mean_age_by_title[x.name]))\n    return df\n\ndef fare_fill(df):\n    ## we will fill are with avg cost by class, again carefull not to leak data\n    ####\n    ## FOR TITANIC TRAIN + TEST MAKES THE POPULATION SO WE HAVE ACCESS TO POPULATION MEANS\n    ## WITHOUT THE RISK OF DATA LEAKING\n    ####\n    \n    df_train = pd.read_csv(f\"{BASE_PATH}train.csv\",index_col='PassengerId')\n    df_test = pd.read_csv(f\"{BASE_PATH}test.csv\",index_col='PassengerId')\n    df_total = pd.concat([df_train, df_test]).copy()\n    \n    \n    mean_fare_by_class = df_total.groupby('Pclass').Fare.mean()\n    df['Fare'] = df.groupby('Pclass')['Fare'].apply(lambda x: x.fillna(mean_fare_by_class[x.name]))\n    ## we will replace the 0's with the avg fare of their respective class\n    df['is_free'] = np.where(df['Fare']== 0,1,0)\n    df['Fare'] = np.where((df.Pclass==1) & (df['Fare']== 0),mean_fare_by_class[1],\n                           np.where((df.Pclass==2) & (df['Fare']== 0),mean_fare_by_class[2],\n                           np.where((df.Pclass==3) & (df['Fare']== 0),mean_fare_by_class[3],\n                           df['Fare'])))\n    return df\n\ndef embarked_fill(df):\n    ## we will fill are with avg cost by class, again carefull not to leak data\n    ####\n    ## FOR TITANIC TRAIN + TEST MAKES THE POPULATION SO WE HAVE ACCESS TO POPULATION MEANS\n    ## WITHOUT THE RISK OF DATA LEAKING\n    ####\n    \n    df_train = pd.read_csv(f\"{BASE_PATH}train.csv\",index_col='PassengerId')\n    df_test = pd.read_csv(f\"{BASE_PATH}test.csv\",index_col='PassengerId')\n    df_total = pd.concat([df_train, df_test]).copy()    \n    \n\n    mode_embarked = df_total['Embarked'].mode()\n    df['Embarked'] = df['Embarked'].fillna(mode_embarked[0])\n    return df\n\ndef totally_cleaned(df):\n    df_copy = df.copy()\n    df_copy = cabin_location(df)\n    df_copy = title_extract(df)\n    df_copy = age_fill(df)\n    df_copy = fare_fill(df)\n    df_copy = embarked_fill(df)\n    return df_copy\n    \n    \n ","1a49f5b2":"def is_female(df):\n    df['is_female']=df['Sex'].apply(lambda x: 1 if x=='female' else 0)\n    return df\n\ndef traveling_party(df):\n    df['traveling_party']=df['Parch'] + df['SibSp'] + 1 #Everyone travels with at least themself\n    df['is_mother'] = np.where(( (df['Sex']=='female') & (df['Parch'] > 0) & (df['Age'] > 30.)),1,0)\n    df['is_wife'] = np.where(( (df['Sex']=='female') & (df['Title'] == 'Mrs') & (df['Age'] > 30.)),1,0)\n    df['is_alone'] = np.where(df['traveling_party'] == 1,1,0)\n    return df   \n\ndef bin_data(df,columns,nbins,log=False):\n    frames = []\n    for cut_col in columns:\n        if (log):\n            tmp = pd.DataFrame(pd.cut(np.log1p(feat[cut_col]), bins=nbins, labels=np.arange(0,nbins)))\n        else:\n            tmp = pd.DataFrame(pd.cut(feat[cut_col], bins=nbins, labels=np.arange(0,nbins)))\n        tmp.columns = [cut_col + \"_binned\"]\n        frames.append(tmp)\n        binned = pd.concat(frames, axis=1).astype(int)\n        df = pd.concat([df, binned], axis=1)\n    return df\n\ndef ticket_pre(df):\n    ticket_prefix = []\n    for i in list(df.Ticket):\n        if not i.isdigit() :\n            ticket_prefix.append((i.replace(\".\",\"\").replace(\"\/\",\"\").strip().split(' ')[0]).upper()) #Take prefix\n        else:\n            ticket_prefix.append(\"X\")    \n    df[\"Ticket_Pre\"] = ticket_prefix\n    return df\n    \n    \n\ndef feature_engineering(df):\n    df = is_female(df)\n    df = traveling_party(df)\n    df = ticket_pre(df)\n    return df\n        ","02b6b090":"def catboost_encode(df,columns):\n    import category_encoders as ce\n    ## we will fill are with avg cost by class, again carefull not to leak data\n    \n    ####\n    ## FOR TITANIC TRAIN + TEST MAKES THE POPULATION SO WE HAVE ACCESS TO POPULATION MEANS\n    ## WITHOUT THE RISK OF DATA LEAKING\n    ####\n    \n    df_train = pd.read_csv(f\"{BASE_PATH}train.csv\",index_col='PassengerId')\n    df_test = pd.read_csv(f\"{BASE_PATH}test.csv\",index_col='PassengerId')\n    df_total = pd.concat([df_train, df_test]).copy()\n    \n      \n    df_total = totally_cleaned(df_total)\n    df_total = feature_engineering(df_total)\n    cb_enc = ce.CatBoostEncoder(cols=columns)\n    count_encoded = cb_enc.fit(df_total[columns],df_total['Survived'])\n    df = df.join(cb_enc.transform(df[columns]).add_suffix('_cb'))\n    return df\n    \n\ndef onehot_encode(df,columns):\n    #df_numeric = df.select_dtypes(exclude=['object'])\n    df_obj = df[columns].copy()\n\n    cols = []\n    for c in df_obj:\n        dummies = pd.get_dummies(df_obj[c])\n        dummies.columns = [c + \"_\" + str(x) for x in dummies.columns]\n        cols.append(dummies)\n    df_obj = pd.concat(cols, axis=1)\n\n    df = pd.concat([df, df_obj], axis=1)\n    df.reset_index(inplace=True, drop=True)\n    return df    \n \n    \ndef get_dummy(df,columns):\n    for col in columns:  \n        df = pd.get_dummies(df, columns = [col], prefix = col )\n    return df\n\ndef drop_catagotical(df):\n    catagorical_cols = [col for col in df.columns if df[col].dtype == \"object\"]\n    df = df.drop(catagorical_cols,axis=1)\n    return df\n\n","538f9929":"# Variable corrilation\ndef show_corrilations(df):\n    corr = df.select_dtypes(include=\"number\").corr()\n    plt.subplots(figsize=(8,8));\n    sns.heatmap(corr, cmap=\"RdBu\", square=True, cbar_kws={\"shrink\": .7})\n    plt.title(\"Correlation matrix of all numerical features\\n\")\n    plt.tight_layout()\n    plt.show()\n\n# Variable corrilation with survived    \ndef corr_with_survived(df):\n    corr = df.select_dtypes(include=\"number\").corr()\n    plt.figure(figsize=(8,8));\n    corr[\"Survived\"].sort_values(ascending=True)[:-1].plot(kind=\"barh\")\n    plt.title(\"Correlation of numerical features to Survival\")\n    plt.xlabel(\"Correlation to Survival\")\n    plt.tight_layout()\n    plt.show()\n    ","8857f957":"df = pd.read_csv(f\"{BASE_PATH}train.csv\",index_col='PassengerId')\ndf_test = pd.read_csv(f\"{BASE_PATH}test.csv\",index_col='PassengerId')\n\nfeat = pd.concat([df, df_test]).copy()","bc767f9c":"df = totally_cleaned(df)\ndf = feature_engineering(df)\ndf = catboost_encode(df,['Title','Cabin_Location'])\ndf = bin_data(df,['Age'],5,False)\ndf = bin_data(df,['Fare'],10,True)\ndf = onehot_encode(df,['Embarked'])\ndf = get_dummy(df,['Ticket_Pre'])\n#df.info(verbose=True, null_counts=True)","e4fee554":"corr_with_survived(df)\nshow_corrilations(df)","00cd5cb6":"df = pd.read_csv(f\"{BASE_PATH}train.csv\",index_col='PassengerId')\ndf_test = pd.read_csv(f\"{BASE_PATH}test.csv\",index_col='PassengerId')\n\nfeat = totally_cleaned(feat)\nfeat = feature_engineering(feat)\nfeat = catboost_encode(feat,['Title','Cabin_Location'])\nfeat = bin_data(feat,['Age'],5,False)\nfeat = bin_data(feat,['Fare'],10,True)\nfeat = onehot_encode(feat,['Embarked'])\nfeat = get_dummy(feat,['Ticket_Pre'])\nfeat = drop_catagotical(feat)\n\ndtrain = feat[feat.Survived.notnull()].copy()\ndtest  = feat[feat.Survived.isnull()].copy()\ndtest = dtest.drop(['Survived'], axis=1)\ndtest.index = df_test.index\nprint(f\"Raw data shape   : {df.shape}  {df_test.shape}\")\nprint(f\"Clean data shape : {dtrain.shape} {dtest.shape}\")","d9199838":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import KFold\nX = dtrain[dtrain['Fare']<300].drop([\"Survived\"], axis=1)\ny = dtrain[dtrain['Fare']<300].Survived\n\nmetric = 'accuracy'\nrf = RandomForestClassifier()\nkfold = KFold(n_splits=10, shuffle=True, random_state=1)\n\nprint(f\"{cross_val_score(rf, X, y, cv=kfold, scoring=metric).mean()*100:.4f} % Accuracy\")\n\n\n\n","75fc3353":"from sklearn.linear_model import RidgeClassifier,Perceptron,SGDClassifier,LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier,GradientBoostingClassifier,AdaBoostClassifier,VotingClassifier,BaggingClassifier\nfrom sklearn.gaussian_process import GaussianProcessClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nimport xgboost as xgb\nimport lightgbm as lgb\n\nmetric = 'accuracy'\n#Cross Validate Scores\nkfold = KFold(n_splits=10, shuffle=True)\n\n#set up the classifiers\nrandom_state = 1  \nclassifiers = [\n               GaussianProcessClassifier(random_state = random_state),  \n               Perceptron(random_state = random_state),\n               RidgeClassifier(random_state = random_state),\n               SGDClassifier(random_state = random_state),\n               SVC(random_state = random_state),\n               RandomForestClassifier(random_state = random_state),\n               GradientBoostingClassifier(random_state = random_state),\n               lgb.LGBMClassifier(random_state = random_state),\n               xgb.XGBClassifier(objective=\"reg:squarederror\",random_state = random_state),\n               xgb.XGBClassifier(objective=\"reg:squarederror\",booster='dart',random_state = random_state),\n               AdaBoostClassifier(random_state = random_state),\n #              VotingClassifier(estimators=None),\n               BaggingClassifier(random_state = random_state),\n               KNeighborsClassifier(),\n               LogisticRegression(random_state = random_state) \n]\n\nclf_names = [\n            \"GaussianProcessClassifier\",\n            \"Perceptron\",\n            \"Ridge\",\n            \"SGDClassifier\",\n            \"SVC\",\n            \"rndmforest\", \n            \"gbmC\", \n            \"lgbm\", \n            \"xgboost\",\n            \"xgboost_dart\",\n            \"AdaBoost\",\n#            \"Voting\",\n            \"Bagging\",\n            \"KNeighbors\",\n            \"Logistic\"\n]\n\ncv_results = []\nfor clf in classifiers:\n    cv_results.append(cross_val_score(clf, X, y, cv=kfold, scoring=metric,n_jobs=4))\n\n\n\n#for clf_name, clf in zip(clf_names, classifiers):\n#    kfold = KFold(n_splits=5, shuffle=True, random_state=1)\n#    print(f\"{clf_name} {cross_val_score(clf, X, y, cv=kfold, scoring=metric).mean()*100:.4f}% Accuracy\")\n    \n    \n \n\n\n    ","6d608386":"cv_mean = []\ncv_std = []\n\nfor cv_result in cv_results:\n    cv_mean.append(cv_result.mean())\n    cv_std.append(cv_result.std())\n\n\ncv_res = pd.DataFrame({'CrossValMeans':cv_mean,'CrossValStd':cv_std,'Classifiers':clf_names})\n\n\ncross_val_score_plot = sns.barplot('CrossValMeans','Classifiers',data = cv_res, palette=\"Set3\",orient = \"h\",**{'xerr':cv_std})\ncross_val_score_plot.set_xlabel('Mean Accuracy')\ncross_val_score_plot = cross_val_score_plot.set_title(\"Cross validation scores\")\n","2a07f62e":"ridge = RidgeClassifier()\n\nridge_pg = {'alpha':[1,10,100],'tol':[0.001,0.0001,0.00001]}\n\ngs_ridge = GridSearchCV(ridge,param_grid = ridge_pg, cv=kfold, scoring=\"accuracy\", n_jobs= 4, verbose = 1)\n\ngs_ridge.fit(X,y)\n\nridge_best = gs_ridge.best_estimator_","735abdc6":"lrc = LogisticRegression()\n\nlrc_pg = {'penalty':['l1','l2','elasticnet'],'max_iter':[100,500,1000],'warm_start':[True],'tol':[0.001,0.0001,0.00001],'solver':['saga']\n          ,'l1_ratio':[0.5]}\n\ngs_lrc = GridSearchCV(lrc,param_grid = lrc_pg, cv=kfold, scoring=\"accuracy\", n_jobs= 4, verbose = 1)\n\ngs_lrc.fit(X,y)\n\nlrc_best = gs_lrc.best_estimator_","d6830236":"RFC = RandomForestClassifier()\n\nrf_pg = {\"max_depth\": [None],\n              \"max_features\": [1, 3, 10],\n              \"min_samples_split\": [2, 3, 10],\n              \"min_samples_leaf\": [1, 3, 10],\n              \"bootstrap\": [False],\n              \"n_estimators\" :[100,300],\n              \"criterion\": [\"gini\"]}\n\ngsRFC = GridSearchCV(RFC,param_grid = rf_pg, cv=kfold, scoring=\"accuracy\", n_jobs= 4, verbose = 1)\n\ngsRFC.fit(X,y)\n\nRFC_best = gsRFC.best_estimator_","2617f1dc":"XGBC =xgb.XGBClassifier(objective=\"reg:squarederror\",random_state = random_state)\n\n\n#Avg time to fit is 20 mins therefore to debug we dont fit\nif(debug):\n    XGBC_pg = {'max_depth':[2]}\nelse:\n    XGBC_pg = {'max_depth':[2,4,100],'learning_rate':[0.0001,0.001,0.005],'n_estimators':[100,250,500,1000],\n                 'reg_alpha':[0.00001,0.00005,0.0001,0.0005],'colsample_bytree':[.5,.6,.7,.8]}\ngsXGBC = GridSearchCV(XGBC,param_grid = XGBC_pg, cv=kfold, scoring=\"accuracy\", n_jobs= 4, verbose = 1)\n\n\ngsXGBC.fit(X,y)\n\nXGBC_best = gsXGBC.best_estimator_","ddfd1b1f":"XGBDC =xgb.XGBClassifier(objective=\"reg:squarederror\",booster = 'dart',random_state = random_state)\n\n#Avg time to fit is 18 mins\nif(debug):\n    XGBDC_pg = {'max_depth':[2]}\nelse:\n    XGBDC_pg = {'max_depth':[2,4],'learning_rate':[0.0001,0.001,0.005],'n_estimators':[100,250,500],\n                 'reg_alpha':[0.00001,0.00005,0.0001,0.0005],'colsample_bytree':[.5,.7,.8]}\n\ngsXGBDC = GridSearchCV(XGBDC,param_grid = XGBDC_pg, cv=kfold, scoring=\"accuracy\", n_jobs= 4, verbose = 1)\n\n\ngsXGBDC.fit(X,y)\n\nXGBDC_best = gsXGBDC.best_estimator_","73f19def":"BaggC = BaggingClassifier()\n\nBaggC_pg = {'n_estimators':[5,10,25],'bootstrap':[False,True],'max_features':[.1,.4,.5,.7],\n                 'max_samples':[.2,.5,.7,1]}\n\ngsBaggC = GridSearchCV(BaggC,param_grid = BaggC_pg, cv=kfold, scoring=\"accuracy\", n_jobs= 4, verbose = 1)\n\n\ngsBaggC.fit(X,y)\n\nBaggC_best = gsBaggC.best_estimator_","d0673d09":"adac = AdaBoostClassifier()\n\nif(debug):\n    adac_pg = {'algorithm':['SAMME']}\nelse:\n    adac_pg = {'algorithm':['SAMME','SAMME.R'],'learning_rate':[.001,.00001,.01,.5,1],'n_estimators':[25,50,100,150,200]}\n    \ngsADAC =  GridSearchCV(adac,param_grid = adac_pg, cv=kfold, scoring=\"accuracy\", n_jobs= 4, verbose = 1)\n\ngsADAC.fit(X,y)\n\nADAC_best = gsADAC.best_estimator_","bdd8a914":"from sklearn.model_selection import learning_curve\ndef plot_learning_curve(estimator, title, X, y, ylim=None, cv=None,\n                        n_jobs=-1, train_sizes=np.linspace(.1, 1.0, 5)):\n    \"\"\"Generate a simple plot of the test and training learning curve\"\"\"\n    plt.figure()\n    plt.title(title)\n    if ylim is not None:\n        plt.ylim(*ylim)\n    plt.xlabel(\"Training examples\")\n    plt.ylabel(\"Score\")\n    train_sizes, train_scores, test_scores = learning_curve(\n        estimator, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes)\n    train_scores_mean = np.mean(train_scores, axis=1)\n    train_scores_std = np.std(train_scores, axis=1)\n    test_scores_mean = np.mean(test_scores, axis=1)\n    test_scores_std = np.std(test_scores, axis=1)\n    plt.grid()\n\n    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n                     train_scores_mean + train_scores_std, alpha=0.1,\n                     color=\"r\")\n    plt.fill_between(train_sizes, test_scores_mean - test_scores_std,\n                     test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n    plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n             label=\"Training score\")\n    plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n             label=\"Cross-validation score\")\n\n    plt.legend(loc=\"best\")\n    return plt","6fdc8389":"g = plot_learning_curve(BaggC_best,'Bagging Learning Curve',X,y,cv=kfold)\ng = plot_learning_curve(XGBDC_best,\"XGBoost Dart learning curves\",X,y,cv=kfold)\ng = plot_learning_curve(XGBC_best,\"XGBoost Learning curves\",X,y,cv=kfold)\ng = plot_learning_curve(RFC_best,\"Random Forest learning curves\",X,y,cv=kfold)\ng = plot_learning_curve(lrc_best,\"Linear Regression learning curves\",X,y,cv=kfold)\ng = plot_learning_curve(ridge_best,\"Ridge learning curves\",X,y,cv=kfold)\ng = plot_learning_curve(ADAC_best,\"ADAC learning curves\",X,y,cv=kfold)","e3b352df":"def important_features(names_classifiers):\n    ncols = 2\n    nrows = int(np.ceil(len(names_classifiers)\/ncols))\n    nclassifier = 0\n    for row in range(nrows):\n        for col in range(ncols):\n            if(nclassifier == len(names_classifiers)):\n                break\n            name = names_classifiers[nclassifier][0]\n            classifier = names_classifiers[nclassifier][1]\n            indices = np.argsort(classifier.feature_importances_)[::-1][:10]\n            ## LightGBM does not normalize importance so we renormalie, for normalized classifiers this does not make a difference\n            g = sns.barplot(y=X.columns[indices][:10],x = (classifier.feature_importances_\/(classifier.feature_importances_.sum()))[indices][:10] , orient='h',ax=axes[row][col])\n            g.set_xlabel(\"Relative importance\",fontsize=10)\n            g.set_ylabel(\"Features\",fontsize=10)\n            g.tick_params(labelsize=8)\n            g.set_title(name + \" feature importance\")\n            nclassifier += 1","8be31a9f":"nrows = 2\nncols = 2\nfig, axes = plt.subplots(nrows = nrows, ncols = ncols, sharex=\"all\", figsize=(20,20))\n\nnames_classifiers = [(\"XGBoost\",XGBC_best),(\"RandomForest\",RFC_best),('ADAC',ADAC_best)]\n\nimportant_features(names_classifiers)\n\n\n\n","1db35b47":"test_RFC = pd.Series(RFC_best.predict(dtest), name=\"RFC\")\ntest_lrc = pd.Series(lrc_best.predict(dtest), name=\"LRC\")\ntest_XGBC = pd.Series(XGBC_best.predict(dtest), name=\"XGBoost\")\ntest_XGBDC = pd.Series(XGBDC_best.predict(dtest), name=\"XGBoost_Dart\")\ntest_BaggC = pd.Series(BaggC_best.predict(dtest), name=\"Bagging\")\ntest_Ridge = pd.Series(ridge_best.predict(dtest), name=\"Ridge\")\ntest_ADAC = pd.Series(ADAC_best.predict(dtest), name=\"ADAC\")\n# Concatenate all classifier results\nensemble_results = pd.concat([test_RFC,test_lrc,test_XGBC,test_XGBDC, test_BaggC,test_Ridge,test_ADAC],axis=1)\n\n# We also need training fits\ntrain_RFC = pd.Series(RFC_best.predict(X), name=\"RFC\")\ntrain_lrc = pd.Series(lrc_best.predict(X), name=\"LRC\")\ntrain_XGBC = pd.Series(XGBC_best.predict(X), name=\"XGBoost\")\ntrain_XGBDC = pd.Series(XGBDC_best.predict(X), name=\"XGBoost_Dart\")\ntrain_BaggC = pd.Series(BaggC_best.predict(X), name=\"Bagging\")\ntrain_Ridge = pd.Series(ridge_best.predict(X), name=\"Ridge\")\ntrain_ADAC = pd.Series(ADAC_best.predict(X), name=\"ADAC\")\n# Concatenate all classifier results\nensemble_results_train = pd.concat([train_RFC,train_lrc,train_XGBC,train_XGBDC, train_BaggC,train_Ridge,train_ADAC],axis=1)\n\n\n\ng= sns.heatmap(ensemble_results.corr(),annot=True)\n\n","8097ce83":"lgbmc = lgb.LGBMClassifier()\n\nif(debug):\n    lgbmc_pg = {'num_leaves':[30]} \nelse:\n    lgbmc_pg = {'num_leaves':[30,50,60],'max_depth':[2,3,7,-1],'learning_rate':[.001,.00001,.01,.5],'n_estimators':[100,150,200,500]}\n\ngsLGBMC =  GridSearchCV(lgbmc,param_grid = lgbmc_pg, cv=kfold, scoring=\"accuracy\", n_jobs= 4, verbose = 1)\n\nensemble_results.head(3)\ngsLGBMC.fit(ensemble_results_train,y)\nLGBMC_best = gsLGBMC.best_estimator_\n\ng = plot_learning_curve(LGBMC_best,\"Emsemble LGBM learning curves\",ensemble_results_train,y,cv=kfold)\n\n","06029a5c":"test_Survived = pd.Series(LGBMC_best.predict(ensemble_results), name=\"Survived\")\noutput = pd.DataFrame({'PassengerId': dtest.index,\n                       'Survived': test_Survived.astype('int32')})\n#display(output.head(10))\noutput.to_csv(\"ensemble_python_voting.csv\",index=False)","7dad8503":"### XGBoost ","f1459a15":"## Learning curve plot from [Yassine Ghouzam](https:\/\/www.kaggle.com\/yassineghouzam\/titanic-top-4-with-ensemble-modeling)","a7953a4a":"### XGBoost dart","b27b1fee":"### Ridge","395a8b09":"# Data Analysis","1d143a13":"### We are missing the age of 20% of the data and Cabins for 77%, with very few Embarked missing. \n\n - #### We use the methods developed by Karolina, with some modifications, to fill the Cabin and Age missing data.","4226deb5":"The most important features to Survival are 'Title' and 'is_female'. We notice from our correlation matrix that these two features have a high correlation.\nThe next most important feature is cabin location which is not highly correlated with title or is_female.\n\nSince all the dummies from Ticket prefix are binary we have a lower block which has no correlation.","e5d5003d":"## Models we will use are:\n     - Ridge\n     - Logistic\n     - Random Forest\n     - XGBoost \n     - Bagging \n     - XGBoost Dart\n     - LGBMClassifier (Will be used for Lvl2) \n     - AdaBoost\n     ","6383d25b":"# What each classifier finds important","1fa2fcfd":"This notebook is a work in progress. Please feel free to comment with <span style='color:blue'> questions<\/span>, <span style='color:green'>suggestions<\/span>, or <span style=\"color:red\">concerns <\/span>!\n\nI have learned a lot making this notebook and am still learning as I update it. \n\n## Somethings still to do:\n* Look at usefull information from tickets: <span style='color:green'> If you have ideas please share in comments <\/span>\n* Reconsider cabins by floor (\u221a)\n* Better feature engineering: <span style='color:green'> If you have ideas please share in comments <\/span>\n* implement model testing (\u221a)","a6c0e9ca":"### LGBMClassifier","492b7412":"### Make the predictions","088e920d":"We write functions to do our data cleaning so we can apply it to the total data set as well as the training set for analysis\n\nIt makes for cleaner coding","17d116ea":"## Cat Boost Title and Cabin, One - Hot Embarked","a6a7b58c":"### Random Forest","e7e3dcf8":"### Bagging","e0a113e6":"### Linear Regression","52e60a03":"# Model Testing","fa4bf7d6":"### The object columns we have are: Name, Sex, Ticket, Cabin, Embarked. \n\n - #### The Sex column can be encoded into is_female or is_male to make it numerical\n\n - #### We can extract the titles from Name and use it to fill in the missing age data and encode it.\n\n - #### Embarked can be One Hot encoded\n\n - #### We can seperate the floor out of Cabin and encode it\n\n - ####  Ticket can be split into the number and prefix     \n  -  Numbers will be compared for cabin sharing\n  -  Prefix may have some predictive power\n\n","effb6c98":"# Ensemble","9e99406a":"# Data cleaning ","6b213de4":"# Feature Engineering","f6f21593":"# Update Log:\n\n    - Version 45: Ticket splicing added, ensemble updated, formatting. 9\/30\/2019","e39d35d7":"### function to plot multiple classifiers","38c1757b":" We have 12 columns. Survive is the target, PassangerId is the index, therefore we truley have 11 features to predict from.","d178ecc6":"We start with a new load of the data","b379ba32":"### Ada Boost","52789ecc":"#### Amount of data missing by columns","24292d8f":"## Some functions to make the data visualization easier","0299b169":"There are only a few fares greater then 300$ so we will drop them since they hold little to know predictive power","b8a7d8b7":"Now we take the estimation from each model","85b3f318":"## Test alternative models","9c59972d":"# Introduction","cfe24ebf":"# Hyperparameter Tuning","4a9298df":"### Learning Curves","e06e9a48":"### <span style=\"color:green\"> Ideas for more or better features welcome! <\/span>","54ce1630":"## Import the datasets again and clean","2bfdf496":"## Random Forest Classifier will be the Dummy ","fe16b71f":"# Citations:\n- [chmaxx](https:\/\/www.kaggle.com\/chmaxx): [Extensive Data Exploration & Modelling (Python)](https:\/\/www.kaggle.com\/chmaxx\/extensive-data-exploration-modelling-python)\n- [Karolina Pacocha](https:\/\/www.kaggle.com\/kpacocha): [top-6-titanic-machine-learning-from-disaster](https:\/\/www.kaggle.com\/kpacocha\/top-6-titanic-machine-learning-from-disaster)\n- [Arthur Tok](https:\/\/www.kaggle.com\/arthurtok): [introduction-to-ensembling-stacking-in-python](https:\/\/www.kaggle.com\/arthurtok\/introduction-to-ensembling-stacking-in-python)\n- [Yassine Ghouzam](https:\/\/www.kaggle.com\/yassineghouzam): [titanic-top-4-with-ensemble-modeling](https:\/\/www.kaggle.com\/yassineghouzam\/titanic-top-4-with-ensemble-modeling)"}}