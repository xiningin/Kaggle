{"cell_type":{"aba6b29b":"code","4e8f9e29":"code","a9544b99":"code","3e33d8e1":"code","27e5e67a":"code","405d6d85":"markdown","91a85047":"markdown","fe597fd3":"markdown","da4efca2":"markdown","3b60d428":"markdown"},"source":{"aba6b29b":"pip install --upgrade transformers","4e8f9e29":"import torch\nfrom transformers import PegasusForConditionalGeneration, PegasusTokenizer\nmodel_name = 'tuner007\/pegasus_paraphrase'\ntorch_device = 'cuda' if torch.cuda.is_available() else 'cpu'\ntokenizer = PegasusTokenizer.from_pretrained(model_name)\nmodel = PegasusForConditionalGeneration.from_pretrained(model_name).to(torch_device)","a9544b99":"def get_response(input_text,num_return_sequences,num_beams):\n    batch = tokenizer.prepare_seq2seq_batch([input_text],truncation=True,padding='longest',max_length=60).to(torch_device)\n    translated = model.generate(**batch,max_length=60,num_beams=num_beams, num_return_sequences=num_return_sequences, temperature=1.5)\n    tgt_text = tokenizer.batch_decode(translated, skip_special_tokens=True)\n    return tgt_text","3e33d8e1":"context = \"The ultimate test of your knowledge is your capacity to convey it to another.\"\nnum_return_sequences=10\nnum_beams=10\nget_response(context,num_return_sequences,num_beams)","27e5e67a":"context = \"Which course should I take to get started in data science?\"\nnum_return_sequences=10\nnum_beams=10\nget_response(context,num_return_sequences,num_beams)","405d6d85":"### **Example 2:**","91a85047":"In the last week of December 2019, Google research team introduced state of the art summarization model [PEGASUS](https:\/\/github.com\/google-research\/pegasus), which expands Pre-training with Extracted Gap-sentences for Abstractive Summarization.\n\nWe finetune same model for paraphrasing task and convert TF checkpoints to pytorch using [this](https:\/\/github.com\/huggingface\/transformers\/blob\/master\/src\/transformers\/convert_pegasus_tf_to_pytorch.py) script on [transformers](https:\/\/github.com\/huggingface\/transformers) library by huggingface\n\n## **Finetuning :**\n\nPlease checkout the official source library [here](https:\/\/github.com\/google-research\/pegasus#finetuning-on-downstream-datasets) \n\nSame finetuning script can be used to finetune PEGASUS on paraphrasing task with minimal changes.\n\n\n**input:** Text_to_paraphrase\n\n**target:** paraphrased_text","fe597fd3":"## **Inference :**","da4efca2":"### **Example 1:**","3b60d428":"* The amount of data to train your models impacts the model performance heavily.\n* Acquiring and labeling additional observations can be an expensive and time-consuming process.\n* Instead you can apply data augmentation to your labelled text data.\n* One common approach used is paraphrasing the text using back translation. (en -> de -> en)\n* With seq2seq transformer models we can now paraphrase text without using backtranslation."}}