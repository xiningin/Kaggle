{"cell_type":{"71feae3f":"code","307254fd":"code","90b658b6":"code","89bc81ff":"code","ba8bfa10":"code","d808075b":"code","a3f2e83d":"code","cebc68b3":"code","66c06946":"code","cd414629":"code","b9766e18":"code","679de6ba":"code","002ae7b4":"code","6c177f28":"markdown","020df6f0":"markdown","0d78626d":"markdown"},"source":{"71feae3f":"IS_KAGGLE_KERNEL = True","307254fd":"import datetime\nimport os\n\nimport tensorflow as tf\nfrom tensorflow import keras as keras\nfrom tensorflow.keras import layers, optimizers, activations, losses, backend\nfrom tensorflow.keras.callbacks import ReduceLROnPlateau, ModelCheckpoint\nimport tensorflow_addons as tfa\nimport numpy as np\n\n# from imblearn.under_sampling import RandomUnderSampler\nfrom sklearn.utils import resample, shuffle\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import recall_score\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler\nfrom sklearn.decomposition import PCA\nfrom sklearn.metrics import log_loss\n\nif IS_KAGGLE_KERNEL:\n    ! pip install \"\/kaggle\/input\/moa-env\/joblib-0.17.0-py3-none-any.whl\"\n    ! pip install \"\/kaggle\/input\/moa-env\/iterative_stratification-0.1.6-py3-none-any.whl\"\nfrom iterstrat.ml_stratifiers import MultilabelStratifiedKFold\n\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\n","90b658b6":"def with_input_path(s):\n    src = \".\/kaggle\/input\/lish-moa\/\"\n    if IS_KAGGLE_KERNEL:\n        src = src[1:]\n    return os.path.join(src, s)","89bc81ff":"# Decided against specialized stratification; really messed up validation scores\n# def mskf_cv(n_splits, seed, pkg):\n#     \"\"\"Multilabel Stratified K-fold cross validation\n    \n#     This algorithm is designed to do stratification in two ways:\n#         1. For frequent drugs (occuring >20 times in the training set), construct\n#             stratified folds per normal k-fold CV\n#         2. For less frequent drugs, allocate all of the drug to a single fold.\n#             This replicates the situation where the test set will contain\n#             drugs that did not occur in the train set\n#     \"\"\"\n    \n#     df_id, df_targets = pkg\n\n#     # Get drug_id's and count them\n#     drug_counts = df_id.drug_id.value_counts()\n    \n#     # Construct a composite df that includes drug_id alongside sig_id and all of the (multi)-labels\n#     targets = df_targets.columns[1:]\n#     composite = df_id.merge(right=df_targets, on='sig_id', how='left')\n    \n#     # Find the indices of all of the `sig_id` corresponding to frequent drugs counts\n#     frequent_drugs = drug_counts.loc[drug_counts >= 20].index.values\n#     infrequent_drugs = drug_counts.loc[drug_counts < 20].index.values\n\n#     freq_idx = composite.index[composite.drug_id.isin(frequent_drugs)]\n#     infreq_idx = composite.index[composite.drug_id.isin(infrequent_drugs)]\n\n#     test_folds = []\n#     train_folds = []\n\n#     # First split is straightfoward: vanilla Mulitlabel Stratified KF on the frequent drugs\n#     cv = MultilabelStratifiedKFold(n_splits=n_splits, shuffle=True, random_state=seed)\n#     y_freq = df_targets.loc[freq_idx][targets]\n#     for i, (train_locs, test_locs) in enumerate(cv.split(freq_idx, y_freq)):\n#         test_folds.append(freq_idx[test_locs])\n#         train_folds.append(freq_idx[train_locs])\n\n\n#     # Second split takes two steps: first, split on _drug labels_ (not rows), and then\n#     # recover row indices corresponding to each of these label(sets)\n#     y_dummy = range(len(infrequent_drugs))\n#     cv = KFold(n_splits=n_splits, shuffle=True, random_state=seed)\n#     for i, (train_locs, test_locs) in enumerate(cv.split(infrequent_drugs, np.ones_like(infrequent_drugs))):\n#         # Single out a st of drugs to put into this fold\n#         infreq_tst_fold = infrequent_drugs[test_locs]\n#         infreq_tst_fold_idx = composite.loc[composite.drug_id.isin(infreq_tst_fold)].index\n#         test_folds[i] = np.concatenate((test_folds[i], infreq_tst_fold_idx.values))\n\n#         infreq_tr_fold = infrequent_drugs[train_locs]\n#         infreq_tr_fold_idx = composite.loc[composite.drug_id.isin(infreq_tr_fold)].index\n\n#         train_folds[i] = np.concatenate((train_folds[i], infreq_tr_fold_idx.values))    \n        \n#     return zip(train_folds, test_folds)","ba8bfa10":"# Prepare train data\ndf_train = pd.read_csv(with_input_path(\"train_features.csv\"))\ndf_test = pd.read_csv(with_input_path(\"test_features.csv\"))","d808075b":"# id is meaningess signifier\ndf_train = df_train.drop(\"sig_id\", axis=1)\ndf_test = df_test.drop(\"sig_id\", axis=1)\n\n# cp_type indicates control (just vehicle) vs. drug. For now, we'll set all control experiments \n# to have zero MoA's before submission. We will therefore ignore this feature in training\ntrain = df_train.copy()\ndf_train = df_train[train[\"cp_type\"] != 'ctl_vehicle'].reset_index(drop=True)\n# train_control_locs = df_train.loc[df_train[\"cp_type\"] == 'ctl_vehicle'].index\ndf_train = df_train.drop(\"cp_type\", axis=1)\n\n# Save these to set control exp MoA's to zero after training\ntest_control_locs = df_test.loc[df_test[\"cp_type\"] == 'ctl_vehicle'].index\ndf_test = df_test.drop(\"cp_type\", axis=1)\n\n# Dosages are strings right now. I don't exactly know the dosages used but we can pretend it was either a single\n# dose or a double dose\ndf_train['cp_dose'].replace('D1', 1, inplace=True)\ndf_train['cp_dose'].replace('D2', 2, inplace=True)\ndf_test['cp_dose'].replace('D1', 1, inplace=True)\ndf_test['cp_dose'].replace('D2', 2, inplace=True)","a3f2e83d":"# Normalize train data and test data simultaneously\nscaler = MinMaxScaler(feature_range=(-1, 1))\nX_total = np.vstack((df_train, df_test))\nscaler.fit(X_total)\nX_train = scaler.transform(df_train)\nX_test = scaler.transform(df_test)","cebc68b3":"# # Compress cell viabilities with PCA since they're highly correlated\nn, _ = X_train.shape\npca = PCA(0.97) # Cutoff at 97% cum. explained variance\ncell_v_pca = pca.fit_transform(X_total[:,-100:])\n\nX_train = np.hstack((X_train[:,:-100], cell_v_pca[:n,:]))\nX_test = np.hstack((X_test[:,:-100], cell_v_pca[n:,:]))","66c06946":"# Prepare train labels\ndf_targets = pd.read_csv(with_input_path(\"train_targets_scored.csv\"))\ndf_targets = df_targets[train[\"cp_type\"] != 'ctl_vehicle'].reset_index(drop=True)\ny_train = df_targets.drop(\"sig_id\", axis=1).to_numpy()","cd414629":"n, input_dim = X_train.shape\nn, num_labels = y_train.shape\nn_test, _ = X_test.shape","b9766e18":"# Prediction Clipping Thresholds\n\np_min = 0.001\np_max = 0.999\ndef logloss(y_true, y_pred):\n    y_pred = tf.clip_by_value(y_pred, p_min, p_max)\n    return -backend.mean(y_true * backend.log(y_pred) + (1 - y_true) * backend.log(1 - y_pred))\n\n\ndef make_model(input_dim):    \n    # Generic feedforward NN\n    model = keras.Sequential()\n#     model.add(layers.Dense(2048, input_dim=input_dim, activation=\"relu\", name=\"layer1\", kernel_regularizer=tf.keras.regularizers.l2(0.01)))\n    model.add(layers.Input(input_dim))\n    model.add(layers.BatchNormalization())\n    model.add(layers.Dropout(0.2))\n    \n    model.add(tfa.layers.WeightNormalization(\n        layers.Dense(2048, activation=\"relu\", name=\"layer1\")))\n    model.add(layers.BatchNormalization())\n    model.add(layers.Dropout(.4))\n    model.add(tfa.layers.WeightNormalization(\n        layers.Dense(1024, activation=\"relu\", name=\"layer2\")))\n    model.add(layers.BatchNormalization())\n    model.add(layers.Dropout(.4))\n    model.add(tfa.layers.WeightNormalization(\n        layers.Dense(512, activation=\"relu\", name=\"layer3\")))\n    model.add(layers.Dense(num_labels, activation=\"sigmoid\", name=\"output\"))\n\n    optimizer = optimizers.Adam()\n    loss = losses.BinaryCrossentropy(label_smoothing=0.001)\n    \n    # Early stopping if model converges\n    early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_logloss', min_delta=1e-5, patience=5, verbose=0,\n                                                      mode='min', restore_best_weights=True)\n    model.compile(optimizer=optimizer, loss=loss, metrics=logloss)\n    \n    return model","679de6ba":"epochs = 25\nbatch_size = 128\n\n# Tensorboard callbacks; doesn't work with WeightNormalization layer....\n# log_dir = \"logs\/fit\/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n# tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n\n# To reduce variance, average the performance of many models trained with diffecrent \n# CV splits (different seeds)\nn_splits = 7\nseeds = [394, 388, 2772, 105]\nn_seeds = len(seeds)\n\n# Rolling averages for validation scores and test predictions\navg_score = 0\ntest_preds = np.zeros((n_test, num_labels))\n\nhistories = []\n\n\ndf_targets = pd.read_csv(with_input_path(\"train_targets_scored.csv\"))\ndf_targets = df_targets[train[\"cp_type\"] != 'ctl_vehicle'].reset_index(drop=True)\ndf_id = pd.read_csv(with_input_path(\"train_drug.csv\"))\ndf_id = df_id[train[\"cp_type\"] != 'ctl_vehicle'].reset_index(drop=True)\npkg = (df_id, df_targets)\n\nfor i, seed in enumerate(seeds):\n#     for j, (train_locs, val_locs) in enumerate(mskf_cv(n_splits=n_splits, seed=seed, pkg=pkg)):\n    for j, (train_locs, val_locs) in enumerate(MultilabelStratifiedKFold(n_splits=n_splits, random_state=seed, shuffle=True).split(X_train, y_train)):\n        model = make_model(input_dim=input_dim)\n        X_train_bal = X_train[train_locs]\n        y_train_bal = y_train[train_locs]\n        Xval = X_train[val_locs]\n        yval = y_train[val_locs]\n#         import pdb; pdb.set_trace()\n        reduce_lr_loss = ReduceLROnPlateau(\n            monitor='val_logloss', factor=0.1, patience=5, verbose=1, min_delta=1e-4, mode='min')\n        \n        \n        history = model.fit(x=X_train_bal, \n                            y=y_train_bal, \n                            epochs=epochs, \n                            batch_size=batch_size,\n                            validation_data=(Xval, yval), \n                            callbacks=[reduce_lr_loss])\n        histories.append(history)\n        # Average validation score\n        y_preds = model.predict(Xval)\n        fold_score = logloss(yval, y_preds)\n        print(\"\\t seed {}, fold {} validation score: {}\".format(i, j, fold_score))\n        avg_score += fold_score \/ (n_splits * n_seeds)\n\n        # Update test score from this fold\/cv\n        test_preds += model.predict(X_test) \/ (n_splits * n_seeds)\n    ","002ae7b4":"sub = pd.read_csv(with_input_path(\"sample_submission.csv\"))\nsub.iloc[:,1:].shape\nsub.iloc[:,1:] = np.clip(test_preds, p_min, p_max)\nsub.iloc[test_control_locs, 1:] = 0\n\nsub.to_csv(\"submission.csv\", index=False)","6c177f28":"## Construct folds stratified by drug label\n\nThe drugs are highly imbalanced(As visualized above). To K-Fold validation, we need to stratify (preserve class balance approximately) but also make sure that a trained model is able to _classify drugs not seen during training_. To mimic this effect, we will spread out the very frequently-occuring drugs (say, drugs that appear more than 20 times in the dataset) among all folds, but concentrate all of the instances of each infrequent drug into a single fold. Then when a fold is used for testing, it will have a lot of the seen train data, but a _few_ (couple hundred rows) drugs that haven't been seen during training.","020df6f0":"## A first dry-run model\n\nNN's support multilabel classification natively (instead of using the argmax of the output sigmoid layer, use a threshold to round up to one). This lets us test some preprocessing and smoothing techniques without having to convert multilabel to multiclass.\n\n\n\nImportant observations:\n - use logloss metric because thats the kaggle scoring metric\n - use \"label smoothing\": more on that below\n - Binary Crossentropy loss at train time to get multilabel predictions out\n - clipping on the predictions, because very confident predictions get penalized by logarithm loss\n ","0d78626d":"## Basic data preprocessing\n\nLoad the data, drop out the ID columns and control columns, replace dosages with numerics, and normalize."}}