{"cell_type":{"06b05aca":"code","e1884265":"code","4ce0df5a":"code","46690678":"code","3a03e02f":"code","5b38dbf5":"code","479f1965":"code","1035ef96":"code","9c3c0178":"code","5c3d89e9":"code","e791ce28":"code","a93085bb":"code","02919ed2":"code","01be896b":"code","854784f7":"code","1cddaf5c":"code","0201bcea":"code","c0fa489c":"code","78e0b691":"code","3dd86aeb":"code","844b464e":"code","577b55a5":"code","ec089d40":"code","32024862":"code","f880c27d":"code","58b0f069":"code","ed3d93a4":"code","6df5947d":"code","be1e4413":"code","dc071cd0":"code","a47d0c23":"code","0149de28":"code","61401c4f":"code","faedbf00":"code","173e6888":"code","9b61bc2a":"code","50c5ef9c":"code","321e79f8":"code","f9e36566":"code","ae7a640e":"code","86803d1e":"code","f97b4870":"code","93d642e1":"code","9a766101":"code","e47b0ae0":"code","c3d01e06":"code","a0e04832":"code","2d48da52":"code","1be213f7":"code","ef82d609":"code","23e22346":"code","9f80a389":"code","9cd2b78c":"code","78f5fbc1":"code","6961efd7":"code","e18b968f":"code","0cd41134":"code","d2400056":"code","05cfe5ec":"code","fdc81018":"code","73b4de50":"code","fc9fefd3":"code","a2cd7adf":"code","83fce80b":"code","6733278c":"code","565b7b3c":"code","a4e51ddf":"code","b5bc7cab":"code","d640661c":"code","f110df7c":"code","f1f9476f":"code","506e7c1c":"code","55beecc5":"code","5ffacad3":"code","507058fc":"code","609c2e52":"code","effa235e":"code","bcfd6911":"code","76c6cf93":"code","0a5ae6d6":"code","a9601e05":"code","879f2633":"code","844087e1":"code","21b3461d":"code","b1731d47":"code","b6a819a7":"code","28279dfe":"code","f40ad00e":"code","d0b7f9b3":"code","a68926bc":"code","f81c5990":"code","35f7a16d":"code","6881e3ff":"code","4d2091d2":"code","82cbba5c":"code","7d7fd573":"code","cc6a06f2":"code","4a935041":"code","3266e5e8":"code","5fa77b72":"code","18ec0965":"code","63e93714":"code","62a00fff":"code","b52bd0ff":"code","ce45c1d4":"code","101762c4":"code","add2e486":"code","a65f6b56":"code","1f15a53d":"code","788fdc17":"code","bd1a7468":"code","3b02c746":"code","9d8094e4":"code","6f7de9b4":"code","24621e93":"code","a5a846b9":"code","859fb079":"code","9eb2fd00":"code","edc1fda9":"code","e40cb745":"code","9c016196":"code","aa56ffc7":"code","700f48aa":"code","670c5d71":"code","ed0b1708":"code","8eef414c":"code","38275351":"code","8ac42441":"code","a55ea3e2":"code","f65e2f8b":"code","326f8b9a":"code","d72a176e":"code","83c9f9b5":"code","ed1d2d91":"code","ba38ee66":"code","1f1769e7":"code","d4a926a9":"code","3cc80d9a":"code","778b6104":"code","b3ee4c6d":"code","cff6cf91":"code","bb2a9a27":"code","c698ee20":"code","3549e60a":"code","2c6bcdfd":"code","93af7212":"markdown","5bf30094":"markdown","04a36bd7":"markdown","f47c9108":"markdown","76ae4c60":"markdown","b6c44b5b":"markdown","16f95bba":"markdown","9a071e7c":"markdown","e2c0fcd0":"markdown","ec4ddaa7":"markdown","b805d6fe":"markdown","8316203c":"markdown","02c07a7f":"markdown","f2895482":"markdown","151c94fe":"markdown","52d68aed":"markdown","19597bc0":"markdown","8bc4376d":"markdown","a8bb4fcc":"markdown","64a1272f":"markdown","b82c631a":"markdown","225e0cc6":"markdown","88f603e9":"markdown","363670e5":"markdown","e11160b3":"markdown","53423ce8":"markdown","48b16cf3":"markdown","d5523972":"markdown","338153c0":"markdown","929c1faa":"markdown","14c5fef6":"markdown","d214405c":"markdown","c9b63873":"markdown","9a7dc482":"markdown","b4edd2ac":"markdown","778e55f1":"markdown","012dcc32":"markdown","117bfe19":"markdown","495f6271":"markdown","1cd42506":"markdown","b7f9ed54":"markdown","784c14b6":"markdown","009353fc":"markdown","831391c1":"markdown","42294ad0":"markdown","aac22de3":"markdown","654eaedf":"markdown","fbc1ecfb":"markdown","94d8db87":"markdown","ec8cdc6e":"markdown","bad311b0":"markdown","62e11d20":"markdown","46dc4b48":"markdown","77049fc9":"markdown","35431169":"markdown","983b3a5c":"markdown","99880ad2":"markdown","8cc40aaa":"markdown","1aba8eb9":"markdown","97410002":"markdown","574f96ae":"markdown","7dc5aded":"markdown","468ef081":"markdown","de4f62ae":"markdown"},"source":{"06b05aca":"import json\nimport pandas as pd","e1884265":"frames_tip = []\nfor chunk in pd.read_json('..\/input\/yelp_academic_dataset_tip.json', lines=True, chunksize = 10000):\n    frames_tip.append(chunk)\ntip=pd.concat(frames_tip)","4ce0df5a":"tip.columns","46690678":"tip.head()","3a03e02f":"frames_checkin = []\nfor chunk in pd.read_json('..\/input\/yelp_academic_dataset_checkin.json', lines=True, chunksize = 10000):\n    frames_checkin.append(chunk)\ncheckin=pd.concat(frames_checkin)","5b38dbf5":"checkin.columns","479f1965":"checkin.shape","1035ef96":"checkin.head()","9c3c0178":"frames_review = []\nfor chunk in pd.read_json('..\/input\/yelp_academic_dataset_review.json', lines=True, chunksize = 20000):\n    frames_review.append(chunk)\nreview=pd.concat(frames_review)","5c3d89e9":"review.shape","e791ce28":"frames = []\nfor chunk in pd.read_json('..\/input\/yelp_academic_dataset_user.json', lines=True, chunksize = 10000):\n    frames.append(chunk)\nuser = pd.concat(frames)","a93085bb":"user.shape","02919ed2":"user.head()","01be896b":"frames_business = []\nfor chunk in pd.read_json('..\/input\/yelp_academic_dataset_business.json', lines=True, chunksize = 10000):\n    frames_business.append(chunk)\nbusiness = pd.concat(frames_business)\nbusiness.head()","854784f7":"business['city'].value_counts().head()","1cddaf5c":"business_vegas=business[business['city']=='Las Vegas']","0201bcea":"business_vegas=business_vegas.reset_index(drop=True)","c0fa489c":"import re\nbusiness_vegas['restaurant']=business_vegas['categories'].str.contains('Restaurants',flags=re.IGNORECASE)","78e0b691":"business_vegas_restaurant=business_vegas[business_vegas['restaurant']==True]","3dd86aeb":"business_vegas_restaurant.head()","844b464e":"business_vegas_restaurant.reset_index(drop=True).head()","577b55a5":"business_vegas_restaurant.shape","ec089d40":"business_vegas_restaurant.to_pickle('restaurant in vegas.pickle')\nreview=review.drop('text',axis=1)","32024862":"review_in_vegas=review.loc[review['business_id'].isin(business_vegas_restaurant['business_id'].unique())]","f880c27d":"review_in_vegas.reset_index(drop=True).head()","58b0f069":"review_in_vegas.to_pickle('vegas_review.pickle')","ed3d93a4":"user.columns","6df5947d":"user_in_vegas=user.loc[user['user_id'].isin(review_in_vegas['user_id'].unique())]","be1e4413":"user_in_vegas.to_pickle('vegas_users.pickle')","dc071cd0":"tip.columns","a47d0c23":"tip_in_vegas=tip.loc[tip['user_id'].isin(review_in_vegas['user_id'].unique())].reset_index(drop=True)","0149de28":"tip_in_vegas.to_pickle('tip_in_vegas.pickle')","61401c4f":"check_in_vegas=checkin.loc[checkin['business_id'].isin(business_vegas_restaurant['business_id'].unique())].reset_index(drop=True)","faedbf00":"check_in_vegas.to_pickle('checkin_vegas.pickle')","173e6888":"import pandas as pd\nrest = pd.read_pickle('restaurant in vegas.pickle')","9b61bc2a":"import numpy as np\nrest.fillna(value=pd.np.nan, inplace=True)","50c5ef9c":"Rest = rest.reset_index(drop=True)\nRest.index +=1\nRest.head()","321e79f8":"Rest.columns","f9e36566":"Rest_final = Rest[['name', 'business_id', 'address', 'categories', 'postal_code','attributes','hours','latitude','longitude','review_count','stars']]","ae7a640e":"categories=', '.join(list(Rest_final['categories'].unique()))\ncategories=categories.split(', ')\ncategories[:5]","86803d1e":"from collections import Counter, defaultdict\nc = Counter(categories)\nc.most_common(60)","f97b4870":"cuisine = 'American|Chinese|Italian|Japanese|Mexican|Asian Fusion|Thai|Korean|Mediterranean'\nRest_final['cuisine']=Rest_final['categories'].str.findall(cuisine)","93d642e1":"Rest_final['cuisine']=Rest_final['cuisine'].map(lambda x: list(x))\nRest_final['cuisine']=Rest_final['cuisine'].map(lambda x: ['Others'] if x==[] else x)","9a766101":"Rest_final['cuisine'].head(20)","e47b0ae0":"Rest_final['cuisine']=Rest_final['cuisine'].map(lambda x: list(dict.fromkeys(x)))\nRest_final['cuisine']=Rest_final['cuisine'].map(', '.join) # convert list of string to string\nRest_final['cuisine'].head(20)","c3d01e06":"Rest_final['cuisine'].unique()","a0e04832":"Rest_final['cuisine'].iloc[np.where(Rest_final['cuisine'].str.contains('Asian Fusion'))]='Asian Fusion'","2d48da52":"Rest_final['cuisine'].unique()","1be213f7":"Rest_final.isnull().sum()","ef82d609":"Rest_final['attributes'].apply(pd.Series).head()\n# Split the attributes dictionary into all its values","23e22346":"R = Rest_final['attributes'].apply(pd.Series)\nlist(R.columns)","9f80a389":"Rest_new = pd.concat([Rest_final.drop(['attributes'], axis=1), Rest_final['attributes'].apply(pd.Series)], axis=1)\nRest_new.head()","9cd2b78c":"Rest_new = Rest_new[['name', 'business_id', 'address', 'cuisine', 'postal_code','hours','latitude','longitude',\n                   'review_count','stars','OutdoorSeating','BusinessAcceptsCreditCards','RestaurantsDelivery',\n                   'RestaurantsReservations','WiFi','Alcohol','categories']]","78f5fbc1":"Rest_new.fillna(value=pd.np.nan, inplace=True)\nRest_new['WiFi'].unique()","6961efd7":"a=Rest_new['WiFi'].map(lambda x: 'No' if x in np.array([\"u'no'\", \"'no'\",'None']) else x)\na=a.map(lambda x: 'Free' if x in np.array([\"'free'\", \"u'free'\"]) else x)\na.unique()","e18b968f":"a=a.map(lambda x: 'Paid' if x in np.array([\"'paid'\", \"u'paid'\"]) else x)\na.unique()","0cd41134":"Rest_new['WiFi']=a","d2400056":"Rest_new['Alcohol'].unique()","05cfe5ec":"Alc = Rest_new['Alcohol'].map(lambda x: 'Full_Bar' if x in np.array([\"u'full_bar'\", \"'full_bar'\"]) else x)\nAlc.unique()","fdc81018":"Alc = Alc.map(lambda x: 'Beer&Wine' if x in np.array([\"u'beer_and_wine'\", \"'beer_and_wine'\"]) else x)\nAlc.unique()","73b4de50":"Alc = Alc.map(lambda x: 'No' if x in np.array([\"u'none'\", \"'none'\",'None']) else x)\nAlc.unique()","fc9fefd3":"Rest_new['Alcohol']= Alc\nRest_new.head()","a2cd7adf":"print(Rest_new['hours'][Rest_new['hours'].notnull()].map(lambda x: x.values()).map(len).sort_values().value_counts())","83fce80b":"def merge(x,y):\n    result = []\n    try:\n        for i in x:\n            index = x.index(i)\n            result.append(i)\n            result.append(y[index])\n        return result\n    except TypeError:\n        result = [np.NaN, np.NaN]","6733278c":"Rest_new['business_days']=Rest_new['hours'][Rest_new['hours'].notnull()].map(lambda x:list(x.keys()))\nRest_new['business_hours']=Rest_new['hours'][Rest_new['hours'].notnull()].map(lambda x:list(x.values()))\nRest_new['hours_day'] = Rest_new.apply(lambda row: merge(row['business_days'], row['business_hours']), axis=1)","565b7b3c":"Rest_new_hours = Rest_new[:]\nRest_new_hours.head(10)","a4e51ddf":"Rest_new_hours['hours_day'][Rest_new_hours['hours_day'].notnull()]=Rest_new_hours['hours_day'][Rest_new['hours_day'].notnull()].map(lambda x: ''.join(x))\nRest_new_hours.head()","b5bc7cab":"Rest_new_hours['Monday_Open']=Rest_new_hours['hours_day'].str.extract('[M][o][n][d][a][y](\\d*[:]\\d*)[-]\\d*[:]\\d*')\nRest_new_hours['Tuesday_Open']=Rest_new_hours['hours_day'].str.extract('[T][u][e][s][d][a][y](\\d*[:]\\d*)[-]\\d*[:]\\d*')\nRest_new_hours['Wednesday_Open']=Rest_new_hours['hours_day'].str.extract('[W][e][d][n][e][s][d][a][y](\\d*[:]\\d*)[-]\\d*[:]\\d*')\nRest_new_hours['Thursday_Open']=Rest_new_hours['hours_day'].str.extract('[T][h][u][r][s][d][a][y](\\d*[:]\\d*)[-]\\d*[:]\\d*')\nRest_new_hours['Friday_Open']=Rest_new_hours['hours_day'].str.extract('[F][r][i][d][a][y](\\d*[:]\\d*)[-]\\d*[:]\\d*')\nRest_new_hours['Saturday_Open']=Rest_new_hours['hours_day'].str.extract('[S][a][t][u][r][d][a][y](\\d*[:]\\d*)[-]\\d*[:]\\d*')\nRest_new_hours['Sunday_Open']=Rest_new_hours['hours_day'].str.extract('[S][u][n][d][a][y](\\d*[:]\\d*)[-]\\d*[:]\\d*')\nRest_new_hours['Monday_Close']=Rest_new_hours['hours_day'].str.extract('[M][o][n][d][a][y]\\d*[:]\\d*[-](\\d*[:]\\d*)')\nRest_new_hours['Tuesday_Close']=Rest_new_hours['hours_day'].str.extract('[T][u][e][s][d][a][y]\\d*[:]\\d*[-](\\d*[:]\\d*)')\nRest_new_hours['Wednesday_Close']=Rest_new_hours['hours_day'].str.extract('[[W][e][d][n][e][s][d][a][y]\\d*[:]\\d*[-](\\d*[:]\\d*)')\nRest_new_hours['Thursday_Close']=Rest_new_hours['hours_day'].str.extract('[T][h][u][r][s][d][a][y]\\d*[:]\\d*[-](\\d*[:]\\d*)')\nRest_new_hours['Friday_Close']=Rest_new_hours['hours_day'].str.extract('[F][r][i][d][a][y]\\d*[:]\\d*[-](\\d*[:]\\d*)')\nRest_new_hours['Saturday_Close']=Rest_new_hours['hours_day'].str.extract('[S][a][t][u][r][d][a][y]\\d*[:]\\d*[-](\\d*[:]\\d*)')\nRest_new_hours['Sunday_Close']=Rest_new_hours['hours_day'].str.extract('[S][u][n][d][a][y]\\d*[:]\\d*[-](\\d*[:]\\d*)')","d640661c":"Rest_new_hours.head(5)","f110df7c":"Rest_new_hours.drop(['hours_day','business_days','business_hours'],axis=1,inplace=True)\nRest_new_hours.columns","f1f9476f":"def str2time(val):\n    try:\n        return dt.datetime.strptime(val, '%H:%M').time()\n    except:\n        return pd.NaT","506e7c1c":"import datetime as dt\nRest_new_hours.iloc[:,17:31]=Rest_new_hours.iloc[:,17:31].astype(str)\nRest_new_hours.iloc[:,17:31]=Rest_new_hours.iloc[:,17:31].applymap(lambda x: str2time(x))\nRest_new_hours.iloc[:,17:31].head()","55beecc5":"Rest_new_hours.loc[3801]","5ffacad3":"Rest_new_hours.drop('hours',axis=1,inplace=True)\nRest_new_hours.head()","507058fc":"import pickle\nimport pandas as pd\nimport numpy as np","609c2e52":"pickle_review = open(\"vegas_review.pickle\",\"rb\")\nreview = pickle.load(pickle_review)\nreview.head()","effa235e":"Review = review.reset_index(drop=True)\nReview.index +=1\nReview.head()","bcfd6911":"Review = Review[['business_id', 'user_id', 'review_id', 'date', 'cool','funny','useful','stars']]\nReview.head()","76c6cf93":"pickle_users = open(\"vegas_users.pickle\",\"rb\")\nusers = pickle.load(pickle_users)","0a5ae6d6":"#dropping org index \nusers = users.reset_index(drop=True)\nusers.index +=1","a9601e05":"titles = ['user_id','name','average_stars','yelping_since','review_count','elite','fans','useful','cool','funny','friends']\nusers =users.reindex(columns=titles)\n\n#rename columns\nusers = users.rename(columns={'name':'user_name','review_count':'review'})   ","879f2633":"#converting timestamp to date \nusers['yelping_since'] = pd.to_datetime(users['yelping_since'])\nusers['yelping_since'] = users['yelping_since'].dt.date","844087e1":"import re\nusers['elite'] = users['elite'].apply(lambda x: re.findall('20\\d\\d',x))","21b3461d":"users['elite'] = users['elite'].apply(lambda x: len(x))","b1731d47":"users['friends'].str.split(',')\nusers['friends'] = users['friends'].apply(lambda x: len(x))","b6a819a7":"users = users.rename(columns={'elite':'years_of_elite'})\nusers.head()","28279dfe":"pickle_tip = open(\"tip_in_vegas.pickle\",\"rb\")\ntip = pickle.load(pickle_tip)\ntip = tip.set_index(keys='business_id')","f40ad00e":"#load in restaurant pickle file in order to get the restaurant names\npickle_restaurant = open(\"restaurant in vegas.pickle\",\"rb\")\nrestaurant = pickle.load(pickle_restaurant)\nrestaurant_new = restaurant[['name','business_id']]\nrestaurant_new = restaurant_new.set_index(keys='business_id')","d0b7f9b3":"tip_new = tip.join(restaurant_new,how='inner')","a68926bc":"tip_new['date'] = pd.to_datetime(tip_new['date'])\ntip_new['date'] = tip_new['date'].dt.date","f81c5990":"titles = ['name','date','text','user_id']\ntip_new = tip_new.reindex(columns=titles)","35f7a16d":"tip_new = tip_new.rename(columns={'name':'restaurant_name','text':'user_tips','date':'tips_date'})","6881e3ff":"tip_new.head()","4d2091d2":"tip1 = tip_new.reset_index()\ntip1.head()","82cbba5c":"import requests, re\nimport pandas as pd\nimport seaborn as sns\nimport nltk","7d7fd573":"import string, itertools\nfrom collections import Counter, defaultdict\nfrom nltk.text import Text\nfrom nltk.probability import FreqDist\nfrom nltk.tokenize import word_tokenize, sent_tokenize, regexp_tokenize\nfrom nltk.corpus import stopwords\nfrom nltk.stem import PorterStemmer, WordNetLemmatizer\nfrom wordcloud import WordCloud","cc6a06f2":"# Select user_tips for reviewing the sentiment\npid = 10\nreport = 'user_tips'\nprint(tip1[['restaurant_name','tips_date']].loc[pid])\ns = tip1.at[pid, report]\ns","4a935041":"report = 'user_tips'\ns = tip1[report]\ns.head()","3266e5e8":"tip1['word_count'] = tip1['user_tips'].apply(lambda x: len(str(x).split(\" \")))\ntip2 = tip1[['business_id','restaurant_name','user_tips','word_count']]","5fa77b72":"tip2['char_count'] = tip2['user_tips'].str.len() ## this also includes spaces\ntip2[['business_id','restaurant_name','user_tips','word_count','char_count']].head()","18ec0965":"stop = stopwords.words('english')\n\ntip2['stopwords'] = tip2['user_tips'].apply(lambda x: len([x for x in x.split() if x in stop]))\ntip2[['user_tips','word_count','char_count','stopwords']].head()","63e93714":"tip2['user_tips'] = tip2['user_tips'].apply(lambda x: \" \".join(x.lower() for x in x.split()))\ntip2['user_tips'].head()","62a00fff":"tip2['user_tips'] = tip2['user_tips'].str.replace('[^\\w\\s]','')\ntip2['user_tips'].head()","b52bd0ff":"tip2['user_tips'] = tip2['user_tips'].apply(lambda x: \" \".join(x for x in x.split() if x not in stop))\ntip2['user_tips'].head()","ce45c1d4":"from textblob import TextBlob\ntip2['user_tips'][:500].apply(lambda x: str(TextBlob(x).correct())).head(10)","101762c4":"frequency = pd.Series(' '.join(tip2['user_tips']).split()).value_counts()[-100:]\nfrequency.head(20)","add2e486":"frequency = list(frequency.index)\ntip2['user_tips'] = tip2['user_tips'].apply(lambda x: \" \".join(x for x in x.split() if x not in frequency))\ntip3 = tip2.copy()\ntip3['user_tips'].head()","a65f6b56":"TextBlob(tip3['user_tips'][10]).words","1f15a53d":"from textblob import Word\ntip3['user_tips'] = tip3['user_tips'].apply(lambda x: \" \".join([Word(word).lemmatize() for word in x.split()]))\ntip3['user_tips'].head()","788fdc17":"tip3['user_tips'][:5].apply(lambda x: TextBlob(x).sentiment)","bd1a7468":"tip3.head()","3b02c746":"tip3['sentiment'] = tip3['user_tips'].apply(lambda x: TextBlob(x).sentiment[0] )\ntip3[['business_id','restaurant_name','user_tips','sentiment']].head()","9d8094e4":"tip3.head()","6f7de9b4":"sentiment=tip3.groupby(by='business_id')['sentiment'].mean().sort_values(ascending = False)\nsentiment.head()","24621e93":"Rest_new_hours.set_index('business_id',inplace=True)\nRest_new_hours['sentiment']=sentiment","a5a846b9":"Rest_new_hours.head()","859fb079":"Review['review_year'] = Review['date'].dt.year\n##group by business_id and stars\nnew = Review.groupby(['business_id','review_year']).mean()\nnew.head()","9eb2fd00":"year_stars=new['stars'].unstack()","edc1fda9":"df=pd.concat([Rest_new_hours,year_stars],axis=1)\ndf = df.set_index(keys=['name','address']).sort_index(level=[0,1])\nprint(df.columns)\ndf.head()","e40cb745":"%pylab inline\nimport pandas as pd\nimport seaborn as sns","9c016196":"plt.figure(figsize=(25,10))\ncuisines=df[df['cuisine']!='Others']\nax=sns.pointplot(x='cuisine', y='sentiment',join=True,ci=None, estimator=mean,data=cuisines)\nplt.legend(['Sentiment Score'],prop={'size' : 20},bbox_to_anchor=(0.2,0.2));\nax2 = ax.twinx()\nsns.pointplot(x='cuisine', y='stars',join=True,ci=None, estimator=mean,data=cuisines,color='red',ax=ax2)\nax.set_xticklabels(ax.get_xticklabels(),rotation=90);\nplt.legend(['Average Stars'],bbox_to_anchor=(0.18,0.3),prop={'size': 20});\n","aa56ffc7":"def findmyresturant(WiFi=None,OutdoorSeating=None,RestaurantsDelivery=None,BusinessAcceptsCreditCards=None,RestaurantsReservations=None,Alcohol=None,cuisine=None):\n    if WiFi!=None:\n        df_1 = df[df['WiFi'].notnull()]\n        df_2 = df_1[df_1['WiFi']!=False]\n        df_wifi = df_2[df_2['WiFi']!='No']\n    else:\n        df_wifi = df\n    if OutdoorSeating != None:\n        df_3 = df_wifi[df_wifi['OutdoorSeating'].notnull()]\n        df_outdoor = df_3[df_3['OutdoorSeating']!=False]\n    else:\n        df_outdoor = df_wifi\n    if RestaurantsDelivery != None:\n        df_4 = df_outdoor[df_outdoor['RestaurantsDelivery'].notnull()]\n        df_delivery = df_4[df_4['RestaurantsDelivery']!=False]\n    else:\n        df_delivery = df_outdoor\n    if RestaurantsReservations != None:\n        df_5 = df_delivery[df_delivery['RestaurantsReservations'].notnull()]\n        df_reserve = df_5[df_5['RestaurantsReservations']!=False]\n    else:\n        df_reserve = df_delivery\n    if Alcohol != None:\n        df_6 = df_delivery[df_delivery['Alcohol'].notnull()]\n        df_alcohol = df_6[df_6['Alcohol']!='No']\n    else: \n        df_alcohol=df_reserve\n    if BusinessAcceptsCreditCards !=None:\n        df_cards = df_alcohol[df_alcohol['BusinessAcceptsCreditCards']==True]\n    else:\n        df_cards = df_alcohol\n    if cuisine != None:\n        df_cuisine = df_cards[df_cards['cuisine'].str.contains(cuisine)]\n    else:\n        df_cuisine = df_cards\n        \n    df_cuisine=df_cuisine.sort_values(['stars','review_count'],ascending=False)\n    new_df= df_cuisine[['cuisine','stars','postal_code','review_count','OutdoorSeating','BusinessAcceptsCreditCards',\n 'RestaurantsDelivery','RestaurantsReservations','WiFi','Alcohol',2016,2017,2018]]\n    %pylab inline\n    import pandas as pd\n    import seaborn as sns\n    plot=new_df.reset_index().iloc[:5]\n    g = sns.FacetGrid(plot, row='name', sharex=True, sharey=True, height=3,aspect=1.5)\n    g=g.map(plt.scatter, 2018, 'review_count',marker='s');\n    g=g.map(plt.scatter,'stars','review_count',color='red');\n    plt.legend(('2018 Stars','Average Stars'),bbox_to_anchor=(1,5))\n    return new_df.head(10)","700f48aa":"findmyresturant(WiFi='yes',Alcohol='yes',cuisine='Chinese')","670c5d71":"findmyresturant(WiFi='yes',OutdoorSeating='yes',cuisine='American',RestaurantsDelivery='yes')","ed0b1708":"df['review_count']=df['review_count'].fillna(0.0)\ndf_close_hour = df[df['Friday_Close'].notnull()].reset_index()\ndf_close_hour['Friday_Close'] = df_close_hour['Friday_Close'].map(lambda t: dt.datetime(year=2018, month=12, day=30, hour=t.hour, minute=t.minute))\ndf_close_hour.set_index('Friday_Close',inplace=True)","8eef414c":"df_close_hour['review_count'].resample('180Min').mean().plot(figsize=(10,5));\nplt.xlabel ('Friday Close Time');\nplt.ylabel ('Reivew Count');","38275351":"df_close_hour['sentiment'].resample('180Min').mean().plot(figsize=(10,5));\nplt.xlabel ('Friday Close Time');\nplt.ylabel ('Sentiment Score');","8ac42441":"df[df['Saturday_Close']==datetime.time(15, 30)][['cuisine','stars','categories']]","a55ea3e2":"users13 = users.loc[users['years_of_elite']==13]\nusers_elite = users13.sort_values('review',ascending=False).iloc[0:30]\nusers_elite.set_index('user_id',inplace=True)","f65e2f8b":"tip_new1 = tip_new.set_index('user_id')\nusers_elite1 = users_elite.join(tip_new1,how='inner',on='user_id')","326f8b9a":"elite_top5 = users_elite1['restaurant_name'].value_counts().iloc[0:10]\nelite_top5 = pd.DataFrame(elite_top5)\nelite_top5.rename(columns={'restaurant_name':'common_review'},inplace=True)","d72a176e":"plt.figure(figsize(15,8));\nelite_top5.plot(kind='bar',color='g',legend=False);\nplt.title('Most Reviewed by Elite Members');\nplt.xlabel('Restaurant');\nplt.ylabel('Number of Common Reviews');","83c9f9b5":"tip_new2 = tip_new.set_index('user_id')\nuser_tip = users.join(tip_new2, how='inner',on='user_id')","ed1d2d91":"Review['review_year'] = Review['date'].dt.year","ba38ee66":"new = pd.merge(Rest_new_hours, Review, on='business_id', how='outer')","1f1769e7":"new1 =new[['name','review_year','stars_y']]\nnew1 = new1.groupby(['name','review_year']).mean().reset_index()","d4a926a9":"##def a function for checking rate trending for a restaurant \n\ndef rate_trend(name):       \n    a =new1.loc[new1['name']==name]\n    plt.figure(figsize=(10,8))\n    sns.pointplot(x=a['review_year'],y=a['stars_y'],data=a,join=True,color='m')\n    plt.yticks(np.arange(0, 6, step=1))\n    plt.ylabel('Average Stars')  \n    plt.xlabel('Year')\n    plt.title('Rating Trend: '+str(name) )\n    ","3cc80d9a":"rate_trend('Kabuto')","778b6104":"chosen_words = ['awesome', 'service', 'cheap', 'expensive', 'price', 'yummy', 'delicious', 'again', 'great',\n                'fantastic', 'amazing', 'love', 'horrible', 'bad', 'terrible', 'place', 'strip', 'casino', 'ambience',\n                'night', 'open', 'bar', 'nice', 'friendly', 'hostile', 'excellent','awful', 'wow', 'hate','staff']\nprint(chosen_words)","b3ee4c6d":"from sklearn.feature_extraction.text import CountVectorizer\nvectorizer = CountVectorizer(vocabulary=chosen_words, lowercase=False)\n\nselected_word_count = vectorizer.fit_transform(tip1['user_tips'].values)\nprint(vectorizer.get_feature_names())","cff6cf91":"word_count_array = selected_word_count.toarray()\nword_count_array.shape","bb2a9a27":"word_count_array\nword_count_array.sum(axis=0)","c698ee20":"Yelp_words = pd.DataFrame(index=vectorizer.get_feature_names(),\n                    data=word_count_array.sum(axis=0)).rename(columns={0: 'Value Count'})","3549e60a":"Yelp_words.plot(kind='bar', stacked=False, figsize=[10,8], colormap='Greens_r');","2c6bcdfd":"cloud = WordCloud(width=1200, height= 1080,max_words= 1000).generate(' '.join(tip1['user_tips'].astype(str)))\nplt.figure(figsize=(15, 25))\nplt.imshow(cloud)\nplt.axis('off');","93af7212":"We have picked Las Vegas because the city has the highest number of restaurants and a lot of potential for diverse perspectives on the data owing to tourism. Because of various parameters to work with, Las Vegas forms the focal point of our data exploration.\n\n","5bf30094":"### 4. How is the most talked-about restaurant among elite members performing?","04a36bd7":"Summary of Actions:\n\n- Since the original tip dataset only contain business_id, we extracted 'business_id' and 'name' from restaurant dataset in order to add the 'name' column in the tip_new dataset. \n- We added the column through doing an inner join.\n- Also, we used the pandas to_datetime to drop the time of the 'date'column. \n- After that, we rearrange the column orders and renamed the columns names so we do not have same names accross different dataset.","f47c9108":"##### WENQI HOU, GAURAVI SAHA, MANYING (JANE) TSANG\n\n### YELP DATA - RECOMMENDATION SYSTEM FOR FINDING THE TOP 5 RESTAURANTS","76ae4c60":"#### WordCloud of User's tips:","b6c44b5b":"Two plots generated above are showing the relationship between how late a restaurant closes, its rating and review counts. Interestingly, we found that restaurants which close late or open 24 hours generally have high review counts but low ratings. \n\nThis indicates that business open late nights have a lot of customers coming in but may not serve food with good quality. While businesses closes early, those which are more likely to be restaurants serving food instead of bars serving alcohol primarily, serve better quality of food.","16f95bba":"## 3. In city like Las Vegas, does restaurant closing time affect review counts and sentiments?","9a071e7c":"### Cleaning of the Review Dataset:","e2c0fcd0":"### 1. Overall Project Objectives","ec4ddaa7":"### We are here on a holiday and we are keen for great quality service!\n\nBased on the analysis we did on what people have to say about Las Vegas, we see that majority of the people are in Las Vegas to enjoy, spend some quality time with family and friends, gamble and spend their time in fun and frolic.\n\nThe word Service was the most sought after in their reviews, followed by place or location.\n\nThis points us to the fact that people are more fascinated with the location of restaurants, how close are they to attractions and how good is the quality of staff and service rather than concentrating on cheap and inexpensive food or other price related parameters. \n\n\n\n\n\n","b805d6fe":"### 3. Data Processing Tasks","8316203c":"#### Generating a cleaned and transformed version of the data:","02c07a7f":"Summary of Actions:\n- Clean up hours to split into multiple columns regarding to open and close time of each day.\n- Check if every restaurant open and close once per day.\n- Use the defined function to split keys(days) and values (hours) of hours dictionary for later information extraction.","f2895482":"To reinforce what we mentioned above, we created a wordcloud and it confirms that -\n\n- Great food\n- Great service\n- Love place\n- Best food\n- Happy hour\n- Tasty, yummy, delicious\n \nare the words that pop out giving an overall positive vibe to Las Vegas.\n    ","151c94fe":"Focusing on Las Vegas restaurants, we are implementing a high fidelity system for a user, restaurant and Yelp to transform the restaurant recommendation experience. Gather regional specific insights about our customer base, develop strategic factors that would influence a customer\u2019s decision to visit a particular restaurant.\n\n - User Perspective: Trending cuisines, upscale bars, quality of restaurants to garner a wholesome experience for the customer.\n - Restaurant\u2019s Profitability: Identifying revenue from highly reviewed users, targeted success through region specific analytics. \n - Yelp\u2019s Perspective: Testing Yelp\u2019s tracking mechanism of restaurant hours, abreast with current status of restaurants (newly opened, permanently closed, etc). Develop a recommendation system for a new customer and identify the top 5 restaurants based on certain input parameters like cuisine, ambience, type of restaurant, etc.\n\n\n","52d68aed":"### Pre processing","19597bc0":"We started with 'business' since it contains \u2018attribute\u2019 which we can use it to extract all business at Las Vegas, and further extract restaurants based on \u2018categories\u2019 out of all business types. \n- By creating a new dataframe business_vegas_restaurant, we were able to filter 'review' table by matching its 'business_id' with 'business_id' in dataframe'business_vegas_restaurant', creating a new dataframe 'review_in_vegas'.\n\n- Using the same logic, we then were able to filter 'user' dataframe by matching its 'user_id' with 'user_id' in 'review_in_vegas'. \n\n- The new dataframe 'review_in_vegas'contains all customers who have been to at least one restaurant in Las Vegas and left a review. \n\n- Same as the rest two dataframes, new dataframes 'tip_vegas', 'checkin_vegas' were created by matching 'business_id'","8bc4376d":"## Data Analysis and Visualization:\n\n- Natural Language Processing\n- Developing insights based on our cleaned data\n- Seaborn visualization\n","a8bb4fcc":"Summary of Actions:\n- The first pre-processing step which we will do is transform our tips into lower case. \n- This avoids having multiple copies of the same words. \n- The next step is to remove punctuation, as it doesn\u2019t add any extra information while treating text data. Therefore removing all instances of it will help us reduce the size of the data.\n- Correct the first 500 spellings to do a refined analysis. It will actually take a lot of time to make all the corrections. \n- We have removed some of the rare words from the text. These words are dominated by noise. We have removed these words because they don't add any value to our analysis.","64a1272f":"We have improved and enhanced the data at every level by cleaning information within the columns. Further data cleaning and enhancements are covered in the data cleaning section.","b82c631a":"### Cleaning Users Dataset","225e0cc6":"### Enhancement to the Data:","88f603e9":"### Flow of Data Processing:","363670e5":"### 2. Description of Data","e11160b3":"# Conclusion: What is Las Vegas trying to tell us?","53423ce8":"# Summary of Findings","48b16cf3":"# Data Cleaning - Making the Data useful for analysis","d5523972":"1. Transfer json into pandas dataframe with proper indexing Extract data that includes restaurants in Las Vegas.\n2. Replace garbage data which includes incorrect states and postal codes, etc Replace missing values. \n3. Date transformations and standardization.\n4. Merge multiple dataframes and reshape.\n5. Delete unnecessary columns which could add ambiguity based on logical assumptions.\n6. Delete duplicate restaurants entries and combine their reviews.\n7. Fix typographical errors in reviews.\n8. Data discretize review counts.\n9. Count user\u2019s rating as a function of restaurants\u2019 type and find their preference Improve the accuracy of business category by tracking \u2018buzz words\u2019 in review\n","338153c0":"According to the plot, the derived score for sentimental analysis and average stars fall into very similar patterns. Hense we can say that customers are writing down what they are feeling. More importantly, this discovery can tell for a new customer who's looking for restaurant in Las Vegas that rating star is a reliable standard if they don't have time to go through every single reviews.\n\nIn addition, since we did plot based on cuisine, we found that American food is the most welcomed cuisine on average while Mexican food is at the bottom in Las Vegas.","929c1faa":"Check all cuisines and merge all resturants with cuisine - Asian into Asian fusion for ease.","14c5fef6":"### Cleaning Tip Dataset","d214405c":"##### Working with Business pickle file:","c9b63873":"\n\nSummary of Actions:\n- Reset the index to 1 - for ease of reading\n- Rearranging the columns in the dataframe\n- Updating the timestamp to include only the date format (YYYY-MM-DD).\n- We used the pandas to_datetime to drop the time of the 'date'column","9a7dc482":"## 1. User's Perspective: Are the customers rating the way they are feeling?","b4edd2ac":"To understand the sentiments associated with the user comments and reviews, we have performed natural language processing and analysed all the sentences in the user_tips column. These sentences have been assigned a polarity score which helps us understand if the reviewer is speaking good about the restaurant or has negative views or all in all a neutral general review. \n\nThe more positive the score is, (0 to positive scale), the more positive the review is. Negative scale from (0 to negative scale) indicates negative review.\n\nTo do this, we use TextBlob which generates sentiments for all sentences after we have preprocessed and cleaned for it to identify coherent words.","778e55f1":"### 5. Yelp's Perspective: What do users talk the most about and what can Yelp do to improve their website\/app experience?","012dcc32":"# Data Insights:","117bfe19":"1. The restaurants are amazing!\n2. People look for great service.\n3. Price is not the major concern for choosing restaurants.\n4. Staff, Service and Quality of food makes the major difference in choosing the best restaurants.\n5. The food is yummy, delicious and overall, people are pretty happy dining at Las Vegas.","495f6271":"5 datasets in json format retrieved from Yelp website : business.json, user.json, checkin.json, tip.json and review.json.\n\n- business_id: ID of the business\n- name: name of the business\n- neighborhood\n- address: address of the business\n- city: city of the business\n- state: state of the business\n- postal_code: postal code of the business\n- latitude: latitude of the business\n- longitude: longitude of the business\n- stars: average rating of the business\n- review_count: number of reviews received\n- is_open: 1 if the business is open, 0 therwise\n- categories: multiple categories of the business\n\nReview has the following attributes:\n\n- review_id: ID of the review\n- user_id: ID of the user\n- business_id: ID of the business\n- stars: ratings of the business\n- date: review date\n- text: review from the user\n- useful: number of users who vote a review as usefull\n- funny: number of users who vote a review as funny\n- cool: number of users who vote a review as cool\n\nUser data has these variables:\n- average stars\n- compliment_cool, compliment_cute, compliment_funny, compliment_hot, compliment_list, compliment_more, compliment_note, compliment_photos, compliment_plain, compliment_profile, compliment_writer\n- cool\n- elite\n- fans\n- friends\n- funny\n- name\n- review_counts\n- useful\n- user_id\n- yelping_since\n\nCheck in has two columns: \n\n- business_id\n- date\n\nAnd the most important data for our analysis: Tip data\n\n- business_id\n- compliment_count\n- date\n- text\n- user_id","1cd42506":"### Data Import","b7f9ed54":"\n## 2. For customers, how to find an ideal restaurant with features they want?\n","784c14b6":"The function is designed for customers to find their ideal restaurant based on their needs. When apply the function, the input paramters can be any attributes a resturant have in the dataset along with its cuisine. This include if a resturant accepts credit card, if it has delivery services, if it provides alcohol, if it takes reservation etc. \n\nThe output is the dataframe with top ten resturants with highest stars and review counts matching their requests. The function also generates a plot that can visualize where the top 5 restaurants' review counts and star rating set. Most importantly, the plot compares the average star rating to the newest star rating (i.e. star of 2018). Hence, customer can be aware that a restaurant with overall high rating may be going down recently and is not performing to the optimal based on its prior reputation.\n\nThis can also be used by food critics to analyse the performance of restaurants over the years and would help them rate their experience based on this benchmark.","009353fc":"1. Are the customers rating the way they are feeling? Yes, the customers are giving honest feedbacks about the restaurants. If they are happy about the restaurants, they tend to give higher rating.\n2. In city like Las Vegas, does how late the restaurant close affect review counts and sentiments? Yes, the later it closes, the lower the sentiments and review counts is. \n3. How is the most talked-about restaurant among elite members performing? The most talked-about restaurant among elite members- Kabuto is performing really well that the rating never drops below 4.5 out of 5.\n4. What do users talk the most about and what can Yelp do to improve their website\/app experience? \n","831391c1":"#### Summary of actions:\n- Concatenating the attributes to the dataframe.\n- Since there are a lot of missing values in most of the columns, we have cherry-picked a few columns out of the list and included a few filters for our analysis.\n- Clean up of the WiFi column.\n- Clean up of the Alcohol column.\n\n ","42294ad0":"###### Cleaned Version:","aac22de3":"### Splitting up restaurant hours:","654eaedf":"### Why Las Vegas?","fbc1ecfb":"#### Created a corpus of 30 selected words to understand the tips given by users","94d8db87":"### Follow the Expert Story\n\nWe would like to know which restaurants the longest-run elite members most talked about. Therefore, we first find out the users who have been elite members for the longest time. Then we grouped 30 of the users who have been elite members for 13 years. From the 'Most Reviewed by Elite Members' bar plot, we can see that 'Kabuto', a Japanese restaurant, has been talked about by 11 out of 30 elite members compared to the second one, Lee's sandwiches, was only talked about by 6 of them. \n\nAfter that, we used a function that can check restaurant rating trend throughout the years it is on Yelp! to determine whether this most-talked about restaurant among elite members are doing well or not. If the restaurant's rating is high, it means that the opinion from the 'experts' are actually trustworthy. \n\nFrom the 'Rating Trend: Kabuto' lineplot, we can see that the rating of this restaurant has never dropped below 4.5 and the rating is actually increasing in recent years. Therefore, customers who would like to follow the experts' footstep for finding good restaurant can trust the rating\/comments provided by the elite members. Of course, they can always check any restaurant rating with the function to see if their choice for dinner or lunch is good or not.","ec8cdc6e":"It returns a tuple representing polarity and subjectivity of each tip. \n\nHere, we only extract polarity as it indicates the sentiment as value nearer to 1 means a positive sentiment and values nearer to -1 means a negative sentiment. ","bad311b0":"### Sentiment Analysis of user tips about restaurants","62e11d20":"##### CONTEXT OF THE DATA\nWe have chosen to pick Yelp dataset for three main reasons: \n- The data is feasible and has potential due to large volumes (3.6GB)\n- Since we have gathered the information from the Yelp website, it is authentic and will help us develop practical insights. \n- The datasets include multitude of restaurants, 36 states, 1200 cities and users nationwide which enriches the quality of the data. ","46dc4b48":"We imported our large json file into dataframes by spliting each file into multiple chunks, then convert these chunks to a list, and concatenated them to a final dataframe.\nAfter creating one dataframe, we check the columns, the shapes and the head of the dataframe to get an overall idea of what our data looks like and its features.","77049fc9":"Yelp has been consistently awarded the best food rating and review app in the US. To further enhance their experience, we suggest that Yelp should listen to what the users are saying and add filters and other refinements to teir website and app for people to have an wholesome experience.\n\nIn other words, to understand what users talk about the most, we have created a dictionary of the most commonly used words in reviews about food and emotions associated with Gastronomy. ","35431169":"Summary of actions:\n- Reading the business pickle file for clean up\n- Missing values cleaned up\n- Using only a few selected columns for meaningful analysis\n- Extract useful information from categories column to investigate resturants' cuisine\n","983b3a5c":"#### Analysis of messy data in the attribute column:","99880ad2":"#### Analysis of Sentiment with Ratings\n    ","8cc40aaa":"#### Remove redundant entries (e.g: American, American)","1aba8eb9":"This file contains information about our restaurants and other related parameters. This dataframe acts as the focus of our analysis and we intend to derive meaningful insights from it.","97410002":"To fix this issue where each item inside is a dictionary with values, attributes acts as a filter on Yelp that customers can click to identify the restaurant. For eg. Wifi = Yes would be selected (or tick marked) while making a selection on Yelp.\n\nWe have split the atributes column with dictionary to different filters.","574f96ae":"#### To avoid importing data from the large json files every time, we converted the new dataframes to pickle files for future use.","7dc5aded":"#### The Pickling Process","468ef081":"### 4. Explanatory Data Analysis","de4f62ae":"Summary of Actions:\n- After processing the data, we have shrunk the dataset from 22 columns to 11 columns. The 'compliment' columns are all dropped because they function very similar to 'cool' and 'funny' columns which are also counting how many different kinds of compliments the user got from others. So, to remove the redundancy, we have eliminated those variables.\n- Since we have extracted only Las Vegas data, the index was not in order. Therefore, the first step is to reset the index and make the first index '1'. \n- Second, we re-arrange the columns order so the most important information will be shown first which makes it easier for readers to gain insights from the data frame.  \n- Third, the 'yelping_since' included data and time (hour and minute) which we do not need 'time' for our analysis. Therefore, we used pandas to_datatime function to drop the 'time' in that column.\n- After that, we worked on the multivalued columns: elite and friends. 'elite' columns contained all the years that the user was a elite member in a string format. \n- We decided that having the year details do not help with analyzing the dataset, instead, counting how many years the user is a elite member provides more useful information. \n- Therefore, we first used regular expression to find all the years which would also convert strings to lists.\n- The similar methods apply to 'friends' too, but  instead of regular expression, we used a string method to split the strings. \n- Consequently, we changed 'name' to 'user_name' to specify which dataset this column belongs to."}}