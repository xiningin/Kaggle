{"cell_type":{"6a0c50d3":"code","1aa77939":"code","ce1d80ae":"code","8b71f50d":"code","bec24d8a":"code","6666088d":"code","4e6d807d":"code","e5248534":"code","f182d49a":"code","4a88d5d2":"code","d64350a1":"code","019d3387":"code","72a68f85":"code","6149fecb":"code","5bc9d05f":"code","905ea097":"markdown","bba089dd":"markdown","231577b9":"markdown","e21d4161":"markdown","65d8d94a":"markdown","8790567a":"markdown","602ca7be":"markdown"},"source":{"6a0c50d3":"from fastai.vision.all import *\nfrom tqdm.notebook import  tqdm\n\nPATH = '..\/input\/optiver-realized-volatility-prediction\/'","1aa77939":"class ResBlock(nn.Module):\n    def __init__(self, ch):\n        super().__init__()\n        self.layers = nn.Sequential(\n            nn.Conv2d(ch, ch, kernel_size = (5,1), padding = (2,0), padding_mode='replicate'),\n            nn.BatchNorm2d(ch),\n            nn.ReLU(),\n            nn.Conv2d(ch, ch, kernel_size = (5,1), padding = (2,0), padding_mode='replicate'),\n            nn.BatchNorm2d(ch),\n        )\n        \n    def forward(self, x):\n        res = self.layers(x) + x\n        res = F.relu(res)\n        return res\n\nclass ResnetRegression(nn.Module):\n    def __init__(self,  chan):\n        super().__init__()\n        layers = [\n            nn.Conv2d(1,chan, kernel_size=(3,8), padding=(1,0)),\n            nn.BatchNorm2d(chan),\n            nn.ReLU()\n        ]\n        for _ in range(8):\n            layers += [ResBlock(chan), ResBlock(chan), nn.AvgPool2d((2,1))]            \n        layers += [Flatten(), nn.Dropout(),nn.Linear(2*chan, num_outputs)]        \n        self.stem = nn.Sequential(*layers)\n        self.classifier = nn.Sequential(\n            nn.Linear(6 * chan, 1),\n            SigmoidRange(0, .1)\n        )\n        \n    def forward(self, x):\n        return self.classifier(self.stem(x)).view(-1)","ce1d80ae":"data_dir = PATH+'book_test.parquet'\nmodel_file = '..\/input\/resnetmodel\/resnet_model.pth'\nmodel = torch.load(model_file)","8b71f50d":"means = tensor([  0.9997,   1.0003, 769.9902, 766.7346,   0.9995,   1.0005, 959.3417,\n        928.2203])\nstds = tensor([3.6881e-03, 3.6871e-03, 5.3541e+03, 4.9549e+03, 3.7009e-03, 3.6991e-03,\n        6.6838e+03, 5.7353e+03])","bec24d8a":"def fix_offsets(data_df):\n    offsets = data_df.groupby(['time_id']).agg({'seconds_in_bucket':'min'})\n    offsets.columns = ['offset']\n    data_df = data_df.join(offsets, on='time_id')\n    data_df.seconds_in_bucket = data_df.seconds_in_bucket - data_df.offset\n    return data_df","6666088d":"def ffill(data_df):\n    data_df=data_df.set_index(['time_id', 'seconds_in_bucket'])\n    data_df = data_df.reindex(pd.MultiIndex.from_product([data_df.index.levels[0], np.arange(0,600)], names = ['time_id', 'seconds_in_bucket']), method='ffill')\n    return data_df.reset_index()","4e6d807d":"def load_data(fname):\n    data = pd.read_parquet(fname)\n    stock_id = str(fname).split('=')[1]\n    time_ids = data.time_id.unique()\n    row_ids = list(map(lambda x:f'{stock_id}-{x}', time_ids))\n    data = fix_offsets(data)\n    data = ffill(data)\n    data = data[['bid_price1', 'ask_price1', 'bid_size1', 'ask_size1','bid_price2', 'ask_price2', 'bid_size2', 'ask_size2']].to_numpy()\n    data = torch.tensor(data.astype('float32'))\n    data = (data - means) \/ stds\n    return data, row_ids","e5248534":"train=pd.read_csv(PATH + 'train.csv')\ntest=pd.read_csv(PATH + 'test.csv')\nsubmi=pd.read_csv(PATH + 'sample_submission.csv')","f182d49a":"# for j in tqdm(train.stock_id.unique()):\n#     fname=PATH + 'book_train.parquet\/stock_id='+str(j)\n#     data=pd.read_parquet(fname)\n#     stock_id = str(fname).split('=')[1]\n#     time_ids = data.time_id.unique()\n#     row_ids = list(map(lambda x:f'{stock_id}-{x}', time_ids))\n#     data = fix_offsets(data)\n#     data = ffill(data)\n#     data = data[['bid_price1', 'ask_price1', 'bid_size1', 'ask_size1','bid_price2', 'ask_price2', 'bid_size2', 'ask_size2']].to_numpy()\n#     data = (data - np.array(means))\/ np.array(stds)\n#     data=data.reshape(-1, 20, 30,8)\n#     indi=np.where(j in train.stock_id== True)\n#     np.savez_compressed('stock_'+str(j), target=train.target[indi[0]], Images=data)\n ","4a88d5d2":"Stock=np.load('..\/input\/numpyfiles\/stock_0.npz')\nplt.figure(figsize=(20, 4))\nfor i in range(30):\n    row=np.random.choice(range(Stock['Images'].shape[0]), size=1)\n    plt.subplot(3, 10, i+1)\n    image=Stock['Images'][int(row),:,:,0:3]\n    for k in range(3):\n        image[:,:,k] = np.interp(image[:,:,k], (image[:,:,k].min(), image[:,:,k].max()), (0, 1))\n    plt.imshow(image,cmap=plt.cm.binary)\n    plt.axis('off')\nplt.show()","d64350a1":"def get_preds(data, model):\n    data = data.view(-1,1,600,8)\n    with torch.no_grad():\n        preds = model(data.cuda())\n\n    return preds","019d3387":"%%time\n\nall_preds = []\nfor j in tqdm(train.stock_id.unique()):\n    fname='..\/input\/numpyfiles\/stock_'+str(j)+'.npz'\n    data =np.load(fname)['Images'].reshape((-1,600,8))\n    data = torch.tensor(data.astype('float32'))\n    preds = get_preds(data, model)\n    df_pred = pd.DataFrame(zip(preds.tolist()),columns=['target'])\n    all_preds.append(df_pred)","72a68f85":"Predi=pd.DataFrame(all_preds[0])\n\nfor j in range(1, len(all_preds)):\n    Predi=pd.concat([Predi, all_preds[j]])\n    \nimport matplotlib.pyplot as plt\n\nyt=train.target.values\nyh=Predi.target.values\n\nplt.scatter(yt, yh, marker=\"o\", s=0.1)\nplt.plot(yt, yt, 'r')\n\nplt.title('RMSPE =' + str(np.round(np.sqrt(np.mean(((yt-yh)\/yt)**2)),5)))","6149fecb":"%%time\nall_preds = []\n\nfor j in tqdm(test.stock_id.unique()):\n    fname= PATH + 'book_test.parquet\/stock_id='+str(j)\n    data, row_ids = load_data(fname)\n    preds = get_preds(data, model)\n    df_pred = pd.DataFrame(zip(row_ids, preds.tolist()),columns=['row_id', 'target'])\n    all_preds.append(df_pred)","5bc9d05f":"df_pred = pd.concat(all_preds)\ndf_pred.to_csv('submission.csv', index=False)","905ea097":"### Stats from the train data used for normalization:","bba089dd":"# AWesome notebook previously at  https:\/\/www.kaggle.com\/slawekbiel\/deep-learning-approach-with-a-cnn-inference","231577b9":"## The model\nI'm using a convolutional neural network with architecture inspired by ResNet. With a total of 65 convolutional layers, followed by a single dense layer.\n\nWith a small number of channels and 5x1 convolutions this is still fairly lightweight and doesn't take long to infere, nor train.","e21d4161":"# Solution overview\n\n### This notebook demonstrates an approach where a neural network is trained on the raw book data. I'm not adding any engineered features, so the network starts with no concept of prices, returns, volatility or logarithms - and still achives score comparable to other public notebooks at the moment of writing.\n\n### Each input sample is simply a 600x8 tensor representing the 8 numerical columns of the book data at each second of the 10 minute window.","65d8d94a":"# Store numpy files for further training","8790567a":"### See the discussion [here](https:\/\/www.kaggle.com\/c\/optiver-realized-volatility-prediction\/discussion\/251775)","602ca7be":"### Explained [here](https:\/\/www.kaggle.com\/c\/optiver-realized-volatility-prediction\/discussion\/251277)"}}