{"cell_type":{"c3a843e6":"code","7a73ab8d":"code","b6af1625":"code","208f55cf":"code","daf9aea6":"code","0fd0bd9a":"code","7624cc20":"code","ee88cf2c":"code","5908db4f":"code","116a0774":"code","e56b35e4":"code","e0739064":"code","5bf604d8":"code","b1244225":"code","35a3c560":"code","8fae030b":"code","623dcd57":"code","b1526308":"code","a8d95f1d":"code","83c6daca":"code","45b4904b":"code","1b45c1ce":"code","bbd53c55":"code","58d7e197":"code","53be398f":"code","fe3fba5f":"code","f728e3a9":"code","84f90800":"code","c3be9a84":"code","8fd50d57":"code","d82e182e":"code","5fb6267e":"code","2f578c6a":"markdown","4882e56e":"markdown","7e20381e":"markdown","18885a0b":"markdown","2e179d6b":"markdown","c40fdf3a":"markdown","f2753f6b":"markdown","021385c0":"markdown","be2a7fca":"markdown","53cd8cca":"markdown","d1618aa8":"markdown","c0da1cf8":"markdown","ad962767":"markdown"},"source":{"c3a843e6":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","7a73ab8d":"tweets = pd.read_csv('\/kaggle\/input\/trainings\/narendramodi_tweets.csv')\ntweets['created_at'] = pd.to_datetime(tweets['created_at'], format=\"%Y-%m-%d %H:%M:%S\")\ntweets.shape","b6af1625":"tweets['created_at'].describe()","208f55cf":"docs = tweets['text'].str.lower()\ndocs = docs.str.replace('[^a-z\\s#@]', '') # Retain only alphabets, spaces, # and @ symbol. Remove everything else\ndocs_words = docs.str.split(' ') # Tokenization\nwords_all = []\nfor doc in docs_words:\n    words_all.extend(doc)\nwords_freq = pd.Series(words_all).value_counts()\nwords_freq.head(10)","daf9aea6":"import nltk","0fd0bd9a":"nltk.download('stopwords')","7624cc20":"import seaborn as sns\nimport matplotlib.pyplot as plt\n\ncommon_stopwords = nltk.corpus.stopwords.words('english')\ncustom_stopwords = ['amp', 'rt', '']\nall_stopwords = common_stopwords + custom_stopwords\ndf_words_freq = words_freq.reset_index().rename(columns={'index': 'token', 0: 'freq'})\ndf_words_freq = df_words_freq[~df_words_freq['token'].isin(all_stopwords)].reset_index(drop=True)\ndf_words_freq.head(25).plot.barh(x='token', y='freq', figsize=(14,5))","ee88cf2c":"from wordcloud import WordCloud\nsample_docs = ['today is yogaday', 'today i wish him a very happy birthday']\nsample_docs_str = ' '.join(sample_docs)\n#wc = WordCloud(background_color='white', stopwords=all_stopwords).generate(sample_docs_str)\n#plt.imshow(wc);\nsample_docs_str","5908db4f":"docs_string = ' '.join(docs)\nwc = WordCloud(background_color='white', stopwords=all_stopwords).generate(docs_string)\nplt.figure(figsize=(14,5))\nplt.imshow(wc);","116a0774":"hashtags = df_words_freq[df_words_freq['token'].str.startswith('#')]\nhashtags.head(25).plot.barh(x='token', y='freq', figsize=(14,4))","e56b35e4":"hashtag = '#swachhbharat'\ntweets['docs'] = docs\ntweets['hashtag'] = tweets['docs'].str.contains(hashtag)\ntweets['hashtag'] = tweets['hashtag'].apply(lambda v: 1 if v == True else 0)\ntweets['year_month'] = tweets['created_at'].dt.strftime('%Y_%m')\ntweets.groupby('year_month')['hashtag'].sum().plot.line();","e0739064":"# https:\/\/jmcauley.ucsd.edu\/data\/amazon\/\nreviews = pd.read_csv('\/kaggle\/input\/trainings\/amazon_reviews_big.csv').sample(10000)\nreviews['sentiment'] = reviews['overall'].apply(lambda v: 'positive' if v>=3 else 'negative')\nprint(reviews.shape)\nreviews.head()","5bf604d8":"#remove_stopwords('this movie is really good')\n#from nltk.stem import PorterStemmer\n\n# stemmer = PorterStemmer()\n# stemmer.stem('looking'), stemmer.stem('looks'), stemmer.stem('looked')\n# stemmer.stem('organization')\n# stemmer.stem('president')","b1244225":"from gensim.parsing.preprocessing import remove_stopwords\nfrom gensim.parsing.porter import PorterStemmer\n\nstemmer = PorterStemmer()\ndocs = reviews['reviewText'].fillna('NA').str.lower().str.replace('[^a-z\\s]', '')\ndocs = docs.apply(remove_stopwords)\ndocs = stemmer.stem_documents(docs)\ndocs = pd.Series(docs)","35a3c560":"from sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n\ntrain_x, test_x, train_y, test_y = train_test_split(docs, reviews['sentiment'],\n                                                   test_size=0.2, random_state=1)\n\nvectorizer = CountVectorizer().fit(train_x)","8fae030b":"vocab = vectorizer.get_feature_names()\nvocab_size = len(vocab)\nprint('We have %d words across %d documents' % (vocab_size, train_x.shape[0]))","623dcd57":"train_dtm = vectorizer.transform(train_x)\ntest_dtm = vectorizer.transform(test_x)\ntrain_dtm","b1526308":"uncompressed_matrix = train_dtm.toarray()\ndf_train_dtm = pd.DataFrame(uncompressed_matrix, columns=vocab, index=train_x.index)\ndf_test_dtm = pd.DataFrame(test_dtm.toarray(), columns=vocab, index=test_x.index)","a8d95f1d":"top_words = df_train_dtm.sum().sort_values(ascending=False).head(25)","83c6daca":"# Analysis on document length (i.e. no. of tokens per document)\ndf_train_dtm.sum(axis=1).sort_values(ascending=False).describe()","45b4904b":"v1 = df_train_dtm['camera'] # vector representation for the word camera\nv2 = df_train_dtm['tablet']\nfrom sklearn.metrics.pairwise import cosine_similarity\ncosine_similarity([v1, v2])[0][1]","1b45c1ce":"v1.shape","bbd53c55":"d1 = df_train_dtm.iloc[0]\nprint(d1.shape)\nd2 = df_train_dtm.iloc[1]\ncosine_similarity([d1, d2])[0][1]","58d7e197":"train_dtm","53be398f":"# min_df: Remove those terms which has appeared in less number of document\n# min_df=5; Retains only those terms which has appeared atleast in five documents\nvectorizer = CountVectorizer(min_df=5).fit(train_x)\ntrain_dtm = vectorizer.transform(train_x)\ntrain_dtm","fe3fba5f":"vectorizer = CountVectorizer(min_df=5, ngram_range=(2,2)).fit(train_x)\ntrain_dtm = vectorizer.transform(train_x)\nvocab = vectorizer.get_feature_names()\ndf_train_dtm = pd.DataFrame(train_dtm.toarray(), columns=vocab, index=train_x.index)\ndf_train_dtm.sum().sort_values(ascending=False).head(5)","f728e3a9":"vectorizer = CountVectorizer(min_df=5, ngram_range=(3,3)).fit(train_x)\ntrain_dtm = vectorizer.transform(train_x)\nvocab = vectorizer.get_feature_names()\ndf_train_dtm = pd.DataFrame(train_dtm.toarray(), columns=vocab, index=train_x.index)\ndf_train_dtm.sum().sort_values(ascending=False).head(5)","84f90800":"vectorizer = CountVectorizer(min_df=5, ngram_range=(1,3)).fit(train_x)\ntrain_dtm = vectorizer.transform(train_x)\nvocab = vectorizer.get_feature_names()\ndf_train_dtm = pd.DataFrame(train_dtm.toarray(), columns=vocab, index=train_x.index)\n#df_train_dtm.sum().sort_values(ascending=False).head(50)\n#vocab","c3be9a84":"vectorizer = CountVectorizer(min_df=5, ngram_range=(1,1)).fit(train_x)\ntrain_dtm = vectorizer.transform(train_x)\nvocab = vectorizer.get_feature_names()\ndf_train_dtm = pd.DataFrame(train_dtm.toarray(), columns=vocab, index=train_x.index)\ndf_train_dtm.head()","8fd50d57":"from sklearn.cluster import KMeans\nfrom sklearn.preprocessing import StandardScaler\n\nscaled_dtm = StandardScaler().fit_transform(df_train_dtm)\nmodel = KMeans(n_clusters=5).fit(df_train_dtm)","d82e182e":"df = pd.DataFrame({\n    'review': train_x,\n    'cluster': model.labels_\n})\n#df['cluster'].value_counts()","5fb6267e":"cluster_data = df[df['cluster'] == 2]\ndocs_string = ' '.join(cluster_data['review'])\nwc = WordCloud(background_color='white').generate(docs_string)\nplt.figure(figsize=(14,5))\nplt.imshow(wc)","2f578c6a":"### Top 5 Bigrams","4882e56e":"### Hashtag analysis","7e20381e":"### Document Similarity","18885a0b":"### Columwise sum (Terms frequency)","2e179d6b":"### Rowwise sum (Document size)","c40fdf3a":"## Word Similarity","f2753f6b":"### Document Clustering\n- Clustering algorithms: KMeans, DBSCan\n- Topic modeling: LSA, LDA","021385c0":"### Optional Exercises\n- Monthwise plot word cloud","be2a7fca":"### Top 5 Trigrams","53cd8cca":"### Optional Exercise\n- Identify sparsity of the matrix","d1618aa8":"### Text analytics library in python\n- nltk\n- gensim\n- spacy\n- Textblob\n- sklearn\/keras","c0da1cf8":"### Document Term Matrix","ad962767":"## N- Grams\n- Bigrams (two words per token\/term)\n- Trigrams (three words per token\/term)"}}