{"cell_type":{"c968b390":"code","842c310b":"code","60b23782":"code","abae5c6d":"code","28de51d9":"code","b7b201eb":"code","95e434ee":"code","520f8917":"code","e436e512":"code","62b854fa":"code","aa323b06":"code","32d08ad7":"code","02d3e9d6":"code","50dc0935":"code","1f94afe2":"code","aec3a066":"code","751752c5":"code","92371a71":"code","b932dbb1":"code","6cc618cb":"code","11a3c9ee":"code","4e5cda5a":"code","a34938c3":"code","8c587b58":"code","c7705bed":"code","503cbb10":"code","0a096236":"code","0ae7b944":"code","24439bf5":"code","31d86734":"code","149af403":"code","bb370943":"code","cc157701":"code","07c5acf0":"code","b857f4e8":"code","d1886111":"code","8ce3a94c":"code","b561faf7":"code","0c24a796":"code","40ff9203":"code","a1fc56cb":"code","88004d1e":"markdown","7c644032":"markdown","14bcf7f9":"markdown","47d33923":"markdown","721f8535":"markdown","a6fc58b7":"markdown","9ec2552b":"markdown","aa160f07":"markdown","4036b67e":"markdown","148079a9":"markdown","808867ab":"markdown","b7077a9a":"markdown","5826560b":"markdown","2674809d":"markdown","2379bfe2":"markdown","96e9e81a":"markdown","54a55880":"markdown","3cbc181c":"markdown","54b13272":"markdown","1346bab0":"markdown","23fa6047":"markdown","548f906f":"markdown","0e2a9d48":"markdown","240296b4":"markdown","4e29db62":"markdown","8f1ffb1d":"markdown","7b5faa41":"markdown","4bb9b554":"markdown","cb2b8066":"markdown","02bb5f9f":"markdown","d82cd27c":"markdown","b2ea3e55":"markdown","e900d549":"markdown","2c2f42f5":"markdown","b1215371":"markdown","25012991":"markdown","a7b10919":"markdown","ffb23486":"markdown","a462ea31":"markdown","ee75b2c8":"markdown"},"source":{"c968b390":"import numpy as np\nimport pandas as pd \nimport os\n\n            \ndata = pd.read_csv('..\/input\/travel-review-ratings\/google_review_ratings.csv') \n\n","842c310b":"data.info()","60b23782":"column_names = ['user_id', 'churches', 'resorts', 'beaches',\n                'parks', 'theatres', 'museums', 'malls', 'zoo',\n                'restaurants', 'pubs_bars', 'local_services',\n                'burger_pizza_shops', 'hotels_other_lodgings',\n                'juice_bars', 'art_galleries', 'dance_clubs',\n                 'swimming_pools', 'gyms', 'bakeries', 'beauty_spas',\n                'cafes', 'view_points', 'monuments', 'gardens', 'Unnamed: 25']\n\ndata.columns = column_names","abae5c6d":"data.isnull().sum()","28de51d9":"data.drop('Unnamed: 25', axis = 1, inplace = True)\n\ndata.drop('user_id', axis = 1, inplace = True)\n\ndata = data.fillna(0)\n\ndata['local_services'].astype('float')\n\n","b7b201eb":"local_services_mean = data['local_services'][data['local_services'] != '2\\t2.']\n\ndata['local_services'][data['local_services'] == '2\\t2.'] = np.mean(local_services_mean.astype('float'))\n\ndata['local_services'] = data['local_services'].astype('float')\n\n","95e434ee":"data.info()","520f8917":"pd.set_option('display.max_columns', 30)\ndata.describe()","e436e512":"import matplotlib.pyplot as plt\nimport warnings\n%matplotlib inline\nimport plotly.express as px\n\n# Plotting pretty figures and avoid blurry images\n%config InlineBackend.figure_format = 'retina'\n# Larger scale for plots in notebooks\n\n\n# Ignore warnings\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Enable multiple cell outputs\nfrom IPython.core.interactiveshell import InteractiveShell\nInteractiveShell.ast_node_interactivity = 'all'\n\n\ncolumn_names = ['churches', 'resorts', 'beaches', 'parks', 'theatres', 'museums', 'malls', 'zoo',\n                'restaurants', 'pubs_bars', 'local_services',\n                'burger_pizza_shops', 'hotels_other_lodgings', 'juice_bars', \n                'art_galleries', 'dance_clubs', 'swimming_pools',\n                'gyms', 'bakeries', 'beauty_spas', 'cafes', 'view_points', 'monuments', 'gardens']\n\n\n\ncounts = data[column_names[:]].astype(bool).sum(axis=0).sort_values()\n\ntest = []\nfor i in range(len(counts.index)):\n    test.append(counts.index[i])\n    \n\n\n\nfig = px.bar(counts, \n             x=counts, \n             y=test,\n             color=counts,\n                             labels={\n                     \"total ratings\": \"this is x\",\n                     \"categories\": \"this is y)\"\n                 },\n                height = 800,\n                title=\"Number of reviews under each category\")\n\nfig.show()\n\n","62b854fa":"reviews = data[column_names[:]].astype(bool).sum(axis=1).value_counts()\n\n\nfig = px.bar(reviews, \n             x=reviews.index, \n             y=reviews.values,\n             color=reviews.values,\n                height = 800,\n                title=\"Number of categories VS Number of reviews\")\n\nfig.show()\n\n","aa323b06":"avg_rating = data[column_names[:]].mean().sort_values()\n\n\n\nfig = px.bar(avg_rating,\n            x = avg_rating.index,\n            y = avg_rating.values,\n            color = avg_rating.values,\n            height = 800,\n            title = \"Average rating for each category\")\n\nfig.show()","32d08ad7":"\nfig = px.box(data, y = ['churches', 'resorts', 'beaches', 'parks', 'theatres', 'museums', 'malls', 'zoo',\n                'restaurants', 'pubs_bars', 'local_services',\n                'burger_pizza_shops', 'hotels_other_lodgings', 'juice_bars', \n                'art_galleries', 'dance_clubs', 'swimming_pools',\n                'gyms', 'bakeries', 'beauty_spas', 'cafes', 'view_points', 'monuments', 'gardens'] )\nfig.show()","02d3e9d6":"df = pd.DataFrame(data)\n\nQ1 = df.quantile(0.25)\nQ3 = df.quantile(0.75)\nIQR = Q3 - Q1\n\npd.set_option('display.max_info_rows', 30)\n\n((df < (Q1 - 1.5 * IQR)) | (df > (Q3 + 1.5 * IQR))).sum()\n\n","50dc0935":"import seaborn as sns\n\nplt.figure(figsize=(15,10))\ncor = data.corr() #Calculate the correlation of the above variables\nsns.heatmap(cor, square = True) #Plot the correlation as heat map","1f94afe2":"data.shape","aec3a066":"data = data.drop_duplicates()","751752c5":"data.shape","92371a71":"entertainment = ['theatres', 'dance_clubs', 'malls']\nfood_travel = ['restaurants', 'pubs_bars', 'burger_pizza_shops', 'juice_bars', 'bakeries', 'cafes']\nplaces_of_stay = ['hotels_other_lodgings', 'resorts']\nhistorical = ['churches', 'museums', 'art_galleries', 'monuments']\nnature = ['beaches', 'parks', 'zoo', 'view_points', 'gardens']\nservices = ['local_services', 'swimming_pools', 'gyms', 'beauty_spas']","b932dbb1":"df_categories = pd.DataFrame(columns = ['entertainment', 'food_travel', 'places_of_stay', 'historical', 'nature', 'services'])","6cc618cb":"df_categories['entertainment'] = data[entertainment].mean(axis=1)\ndf_categories['food_travel'] = data[food_travel].mean(axis = 1)\ndf_categories['places_of_stay'] = data[places_of_stay].mean(axis = 1)\ndf_categories['historical'] = data[historical].mean(axis = 1)\ndf_categories['nature'] = data[nature].mean(axis = 1)\ndf_categories['services'] = data[services].mean(axis = 1)","11a3c9ee":"df_categories.describe()","4e5cda5a":"\nfig = px.box(df_categories, y = ['entertainment', 'food_travel', 'places_of_stay', 'historical', 'nature', 'services'] )\nfig.show()","a34938c3":"!pip install pyclustertend\n\nfrom pyclustertend import hopkins \n\nhopkins(df_categories, df_categories.shape[0])","8c587b58":"from scipy.cluster.hierarchy import dendrogram\nfrom scipy.cluster import hierarchy\nfrom sklearn.cluster import AgglomerativeClustering\nfrom scipy.spatial.distance import pdist\nimport plotly.graph_objects as go\n\nZ = hierarchy.linkage(df_categories, 'ward')\n\nc, coph_dists = hierarchy.cophenet(Z, pdist(df_categories, 'hamming'))\n\nward = c\n\n\nA = hierarchy.linkage(df_categories,'average')\n\nc, coph_dists = hierarchy.cophenet(A, pdist(df_categories, 'hamming'))\n\naverage = c\n\n\nB = hierarchy.linkage(df_categories,'single')\n\nc, coph_dists = hierarchy.cophenet(B, pdist(df_categories, 'hamming'))\n\nsingle = c\n\n\nC = hierarchy.linkage(df_categories,'complete')\n\nc, coph_dists = hierarchy.cophenet(C, pdist(df_categories, 'hamming'))\n\ncomplete = c\n\n\nD = hierarchy.linkage(df_categories,'weighted')\n\nc, coph_dists = hierarchy.cophenet(D, pdist(df_categories, 'hamming'))\n\nweighted = c\n\n\nE = hierarchy.linkage(df_categories,'centroid')\n\nc, coph_dists = hierarchy.cophenet(E, pdist(df_categories, 'hamming'))\n\ncentroid = c\n\nF = hierarchy.linkage(df_categories,'median')\n\nc, coph_dists = hierarchy.cophenet(F, pdist(df_categories, 'hamming'))\n\nmedian = c\n\n\nmetrics=['ward', 'average', 'single', 'complete', 'weighted', 'centroid', 'median']\n\n\nfig = go.Figure([go.Bar(x=metrics, y=[ward, average, single, complete, weighted, centroid, median])])\nfig.show()\n","c7705bed":"from sklearn.metrics import silhouette_score\nfor n_clusters in range(2,10):\n    clusterer = AgglomerativeClustering (n_clusters=n_clusters, distance_threshold = None)\n    preds = clusterer.fit_predict(df_categories)\n    \n\n    score = silhouette_score (df_categories, preds)\n    print (\"For n_clusters = {}, silhouette score is {})\".format(n_clusters, score))","503cbb10":"from sklearn.metrics import davies_bouldin_score\n\nfor n_clusters in range(2,10):\n    clusterer = AgglomerativeClustering (n_clusters=n_clusters, distance_threshold = None)\n    preds = clusterer.fit_predict(df_categories)\n    \n    score = davies_bouldin_score (df_categories , preds)\n    print (\"For n_clusters = {}, the Davies-Bouldin score is {})\".format(n_clusters, score))","0a096236":"from sklearn.metrics import calinski_harabasz_score\n\nfor n_clusters in range(2,10):\n    clusterer = AgglomerativeClustering (n_clusters=n_clusters, distance_threshold = None)\n    preds = clusterer.fit_predict(df_categories)\n    \n    score = calinski_harabasz_score(df_categories, preds)\n    print (\"For n_clusters = {}, the Calinski-Harabasz score is {})\".format(n_clusters, score))","0ae7b944":"model = AgglomerativeClustering(distance_threshold=0, n_clusters = None)\nmodel = model.fit(df_categories)\n\nZ = hierarchy.linkage(model.children_, 'average')\n\nplt.figure(figsize=(20,10))\n\ndn = hierarchy.dendrogram(Z)","24439bf5":"df = pd.DataFrame(data)\n\nmodel = AgglomerativeClustering(distance_threshold=None, n_clusters = 3)\nmodel = model.fit(df_categories)\ny_agg=model.fit_predict(df_categories)\n\ndf_agg = df_categories.copy()\ndf_agg[\"AggLabels\"] = y_agg\n\n#uncomment the following line and let your machine explode!\nsns.pairplot( df_agg, hue=\"AggLabels\")  \n\ndf_agg[\"AggLabels\"].value_counts(0)\n\n\n","31d86734":"from sklearn.neighbors import NearestNeighbors\n\nnearest_neighbors = NearestNeighbors(n_neighbors=73) #sqrt(5456) = 73\nnearest_neighbors.fit(df_categories)\ndistances, indices = nearest_neighbors.kneighbors(df_categories)\ndistances = np.sort(distances, axis=0)[:, 1]\n#print(distances)\nplt.figure(figsize=(20,10))\nplt.plot(distances)\nplt.show()","149af403":"from sklearn.cluster import DBSCAN\n\nmodel2 = DBSCAN(eps = 0.6, min_samples = 43)\n\nmodel2 = model2.fit(df_categories)\n\nnp.unique(model2.labels_)\n\ny_db=model2.labels_\n\ndf_db = df_categories.copy()\ndf_db[\"DBLabels\"] = y_db\n\ndf_db[\"DBLabels\"].value_counts(0)\n\n\nsns.pairplot( df_db, hue=\"DBLabels\")  \n\n\n","bb370943":"df_db","cc157701":"# Get names of indexes for which column DB_Labels has value -1\nindexNames = df_db[ df_db['DBLabels'] == -1 ].index\n# Delete these row indexes from dataFrame\ndf_db.drop(indexNames , inplace=True)\n\ndf_db['DBLabels'].unique()","07c5acf0":"df_db","b857f4e8":"sns.pairplot( df_db, hue=\"DBLabels\")  ","d1886111":"y_db = df_db['DBLabels']\nsilhouette_score (df_db, y_db)","8ce3a94c":"from sklearn.mixture import GaussianMixture as GMM\n\nn_components = np.arange(1, 21)\nmodels = [GMM(n, covariance_type='full', random_state=0).fit(df_categories)\n          for n in n_components]\n\nplt.plot(n_components, [m.bic(df_categories) for m in models], label='BIC')\nplt.plot(n_components, [m.aic(df_categories) for m in models], label='AIC')\nplt.legend(loc='best')\nplt.xlabel('n_components');","b561faf7":"from matplotlib.patches import Ellipse\n\ndf_gmm = df_categories.copy()\n\ndf_gmm['gmm'] = GMM(n_components=6, random_state=42).fit_predict(df_categories)\n\nsns.pairplot( df_gmm, hue=\"gmm\")  ","0c24a796":"from sklearn.cluster import KMeans\n\nfor n_clusters in range(2,10):\n    clusterer = KMeans(n_clusters = n_clusters)\n    preds = clusterer.fit_predict(df_categories)\n    \n\n    score = silhouette_score (df_categories, preds)\n    print (\"For n_clusters = {}, silhouette score is {})\".format(n_clusters, score))","40ff9203":"from scipy.spatial.distance import cdist\n\ndistortions = [] \ninertias = [] \nmapping1 = {} \nmapping2 = {} \nK = range(1,10) \n  \nfor k in K: \n    #Building and fitting the model \n    kmeanModel = KMeans(n_clusters=k).fit(df_categories) \n    kmeanModel.fit(df_categories)     \n      \n    distortions.append(sum(np.min(cdist(df_categories, kmeanModel.cluster_centers_, \n                      'euclidean'),axis=1)) \/ df_categories.shape[0]) \n    inertias.append(kmeanModel.inertia_) \n  \n\n    mapping2[k] = kmeanModel.inertia_ \n    \nfor key,val in mapping2.items(): \n    print(str(key)+' : '+str(val))\n    \nplt.plot(K, inertias, 'bx-') \nplt.xlabel('Values of K') \nplt.ylabel('Inertia') \nplt.title('The Elbow Method using Inertia') \nplt.show()     ","a1fc56cb":"model4 = KMeans(n_clusters = 2, n_init = 40)\n\nmodel4 = model4.fit(df_categories)\n\ndf_kmeans = df_categories.copy()\n\ndf_kmeans['kmeans'] = model4.fit_predict(df_categories)\n\nsns.pairplot( df_kmeans, hue = 'kmeans')","88004d1e":"<h1 id=\"Exploratory_Data_Analysis\">\n1. Exploratory Data Analysis\n<a class=\"anchor-link\" href=\"https:\/\/www.kaggle.com\/johnmantios\/travel-review-ratings-dataset\/edit\/run\/47115230#Exploratory_Data_Analysis\">\u00b6<\/a>\n<\/h1>","7c644032":"Firstly, we are going to determine which linkage method to use. In order to do that we will calculate the cophenet index. Cophenet index is a measure of the correlation between the distance of points in feature space and distance on the dendrogram. If the distance between these points increases as the dendrogram distance between the clusters does then the Cophenet index is closer to 1. So, values closer to 1 mean a better linkage method.","14bcf7f9":"We begin the clustering process with the agglomerative clustering algorithm for one simple reason: it is a hierarchical clustering algorithm, so we simplify the problem of having to choose beforehand the number of clusters in our model.Hierarchical clustering does not avoid the problem with choosing the number of clusters. Simply - it constructs the tree spaning over all samples, which shows which samples (later on - clusters) merge together to create a bigger cluster. This happens recursively till you have just two clusters (this is why the default number of clusters is 2) which are merged to the whole dataset.","47d33923":"Outliers are expected for large sample sizes. Since the reviews in google do not occur by human mistake , we should not discard them. \n\nOutlier handling is even more difficult in unsupervised learning, since we are both trying to learn what the clusters are, and what data points correspond to \"no\" clusters.","721f8535":"Let's calculate some useful metrics that will help us decide the number of clusters. Since the ground truth labels are not known we will use such metrics like the silhouette coefficient, the Davies-Bouldin score and the Calinski-Harabasz Index.","a6fc58b7":"Let's assess the clusterability of the dataset using the hopkins statistic. According to the pyclustertend library, on a scale from 0 to 1, the lower the score, the better the clusterability of the dataset","9ec2552b":"Let's apply the Knee method using KNN to find the optimal eps value for our model. Since KNN is a supervised learning algorithm and our data is not labeled, we will apply a general rule of thumb popularized by the \"Pattern Classification\" book by Duda et al., saying that the optimal K value usually found is the square root of N, where N is the total number of samples.","aa160f07":"<h1 id=\"agglomerative\">\n2.1 Agglomerative Clustering\n<a class=\"anchor-link\" href=\"https:\/\/www.kaggle.com\/johnmantios\/travel-review-ratings-dataset\/edit\/run\/47115230#agglomerative\">\u00b6<\/a>\n<\/h1>","4036b67e":"We can easily understand that a total of 3725 users have given a review for all 24 categories and only 6 gave a review for 15 of all of them. \nLet's check now the average rating for each category","148079a9":"Great.All of our data is type 'float'. Let's check some of the descriptive statistics","808867ab":"We will rename the columns for ease of understanding","b7077a9a":"Optimal value for eps where a 'knee' is formed is 0.6.","5826560b":"By looking at the above dendrogram, we observe 3 distinct colors in the dendrogram, but this will not determine how many clusters are formed. Following the main criteria of cutting the dendrogram appropriately, we discover that there are basically 5 clusters. Observing the height of each dendrogram division we decided to go with 4000 where the line would be drawn.\n\nNow, let's plot our data using the labels that the algorithm generated.We are going to make a scatterplot.","2674809d":"We clearly see that the average linkage method is the preferred one.","2379bfe2":"<h1 id=\"kmeans\">\n2.4 KMeans\n<a class=\"anchor-link\" href=\"https:\/\/www.kaggle.com\/johnmantios\/travel-review-ratings-dataset\/edit\/run\/47115230#kmeans\">\u00b6<\/a>\n<\/h1>","96e9e81a":"Following, we apply to our data set the DBSCAN algorithm.DBSCAN works by running a connected components algorithm across the different core points. If two core points share border points, or a core point is a border point in another core point\u2019s neighborhood, then they\u2019re part of the same connected component, which forms a cluster.\nA low min_samples parameter means it will build more clusters from noise, so we shouldn't choose it too small.\nThe DBSCAN paper suggests to choose minPts based on the dimensionality, and eps based on the elbow in the k-distance graph.\nFor eps, we can try to do a knn distance histogram and choose a \"knee\" there, but there might be no visible one, or multiple.\n\nIn the more recent publication\n\nSchubert, E., Sander, J., Ester, M., Kriegel, H. P., & Xu, X. (2017).\nDBSCAN Revisited, Revisited: Why and How You Should (Still) Use DBSCAN.\nACM Transactions on Database Systems (TODS), 42(3), 19.\n\n\nthe authors suggest to use a larger minpts for large and noisy data sets, and to adjust epsilon depending on whether you get too large clusters (decrease epsilon) or too much noise (increase epsilon). Clustering requires iterations.\n\nAfter some trial and error, the min_samples value with the less noise is 43","54a55880":"Again, 6 clusters have the lowest score from a reasonable range of clusters","3cbc181c":"So, we have one string in our local_services column.Let's impute it with the column's input_data['local_services'][input_data['local_services']mean","54b13272":"### **In this notebook I will try to do an exploratory data analysis and clustering for the travel review ratings dataset, found in the UCI machine learning repository.**","1346bab0":"Here the metric shows that 2 clusters would be better than 3.","23fa6047":"We'll go with 2 clusters.","548f906f":"Kmeans algorithm is an iterative algorithm that tries to partition the dataset into K pre-defined distinct non-overlapping subgroups (clusters) where each data point belongs to only one group\nIt tries to make the intra-cluster data points as similar as possible while also keeping the clusters as different (far) as possible. It assigns data points to a cluster such that the sum of the squared distance between the data points and the cluster\u2019s centroid (arithmetic mean of all the data points that belong to that cluster) is at the minimum. The less variation we have within clusters, the more homogeneous (similar) the data points are within the same cluster.","0e2a9d48":"Let's analyze the outliers","240296b4":"Calinski-Harabasz index.We want as high a score as possible","4e29db62":"Let's check how many null values we have","8f1ffb1d":"We clearly see that gyms are users' least favorite venue with an average rating of 0.82 and on the other hand, malls are the leaders  when it comes to ratings with an average score of 3.35.","7b5faa41":"As we can observe, bakeries, gyms and beauty spas are the venues where users show the least amount of interest in rating them.\nLet's see how many reviews were given for each category. \n","4bb9b554":"It would help us significantly if we would basket the various categories into higher levels, both in terms of analysis and clustering","cb2b8066":"Let's remove the noise","02bb5f9f":"From the various silhouette score we can see that although 2 clusters would be a better choice for our data, the score itself is pretty low","d82cd27c":"Let's take a general first idea of our data with info()","b2ea3e55":"At its simplest, GMM is also a type of clustering algorithm. As its name implies, each cluster is modelled according to a different Gaussian distribution. This flexible and probabilistic approach to modelling the data means that rather than having hard assignments into clusters like k-means, we have soft assignments. This means that each data point could have been generated by any of the distributions with a corresponding probability.","e900d549":"The choice of number of components measures how well GMM works as a density estimator, not how well it works as a clustering algorithm. From the above plot it shows that optimal number of components is 13 (where the gradient stops decreasing)","2c2f42f5":"<h1 id=\"DBSCAN\">\n2.2 DBSCAN\n<a class=\"anchor-link\" href=\"https:\/\/www.kaggle.com\/johnmantios\/travel-review-ratings-dataset\/edit\/run\/47115230#DBSCAN\">\u00b6<\/a>\n<\/h1>","b1215371":"<h1 id=\"EM\">\n2.3 EM using GMM\n<a class=\"anchor-link\" href=\"https:\/\/www.kaggle.com\/johnmantios\/travel-review-ratings-dataset\/edit\/run\/47115230#EM\">\u00b6<\/a>\n<\/h1>","25012991":"Not clear at all","a7b10919":"Let's check the Davies-Bouldin score.We want a values as close to 0 as possible","ffb23486":"It seems that we will drop the entire unnamed 25th column and we will impute the 2 rows with zeros supposing that the user did not give any rating to these categories (the 2 null values in category 12 and 24).\nAlso we will drop the 'User' row as it is of no use to us.Finally, we will map the local_services column from object to float","a462ea31":"Let's visualize our first plot: we will examine the number of reviews under each category","ee75b2c8":"<h1 id=\"Clustering\">\n2. Clustering\n<a class=\"anchor-link\" href=\"https:\/\/www.kaggle.com\/johnmantios\/travel-review-ratings-dataset\/edit\/run\/47115230#Clustering\">\u00b6<\/a>\n<\/h1>"}}