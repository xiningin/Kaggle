{"cell_type":{"e65a42e6":"code","11bb9876":"code","fa0a299c":"code","c0ea1bd2":"code","ea164ed6":"code","ffe91bc1":"code","a52ccd33":"markdown","4ea76849":"markdown","3898974a":"markdown","16b0a607":"markdown"},"source":{"e65a42e6":"import tensorflow as tf\nfrom pathlib import Path","11bb9876":"feature_descriptions = {\n    'image': tf.io.FixedLenFeature([], tf.string),\n    'target_positions': tf.io.FixedLenFeature([], tf.string),\n    'target_yaws': tf.io.FixedLenFeature([], tf.string),\n    'target_availabilities': tf.io.FixedLenFeature([], tf.string),\n    'history_positions': tf.io.FixedLenFeature([], tf.string),\n    'history_yaws': tf.io.FixedLenFeature([], tf.string),\n    'history_availabilities': tf.io.FixedLenFeature([], tf.string),\n    'world_to_image': tf.io.FixedLenFeature([], tf.string),\n    'track_id': tf.io.FixedLenFeature([], tf.string),\n    'timestamp': tf.io.FixedLenFeature([], tf.string),\n    'centroid': tf.io.FixedLenFeature([], tf.string),\n    'yaw': tf.io.FixedLenFeature([], tf.string),\n    'extent': tf.io.FixedLenFeature([], tf.string),\n}\n\nfeature_dtypes = {\n    'image': tf.float32,\n    'target_positions': tf.float32,\n    'target_yaws': tf.float32,\n    'target_availabilities': tf.float32,\n    'history_positions': tf.float32,\n    'history_yaws': tf.float32,\n    'history_availabilities': tf.float32,\n    'world_to_image': tf.float64,\n    'track_id': tf.int64,\n    'timestamp': tf.int64,\n    'centroid': tf.float64,\n    'yaw': tf.float64,\n    'extent': tf.float32,\n}\n\ndef make_decoder(descriptions, dtypes):\n    def decode_example(example):\n        example_1 = tf.io.parse_single_example(example, descriptions)\n        example_2 = []\n        for key in dtypes.keys():\n            example_2.append(\n                tf.io.parse_tensor(example_1[key], dtypes[key])\n            )\n        return example_2\n    return decode_example\n\ndecoder = make_decoder(feature_descriptions, feature_dtypes)","fa0a299c":"DATA_DIR = Path('..\/input\/lyft-motion-prediction-tfrecords')\n\ntrain_files = tf.io.gfile.glob(str(DATA_DIR \/ 'training' \/ 'training' \/ '*.tfrecord'))\nvalid_files = tf.io.gfile.glob(str(DATA_DIR \/ 'validation' \/ 'validation' \/ '*.tfrecord'))\n\nds_train = (\n    tf.data.TFRecordDataset(train_files)\n    .map(decoder)\n    # etc\n)\n\nds_valid = (\n    tf.data.TFRecordDataset(valid_files)\n    .map(decoder)\n    # etc\n)","c0ea1bd2":"import matplotlib.pyplot as plt","ea164ed6":"for batch in ds_train.take(1):\n    image_0 = batch[0][0]\n    pos_0 = batch[0][1]\n    plt.figure(figsize=(16, 8))\n    for channel in range(5):\n        plt.subplot(2, 5, channel+1)\n        plt.imshow(image_0[channel], cmap='gray')\n        plt.subplot(2, 5, channel+1+5)\n        plt.imshow(pos_0[channel], cmap='gray')","ffe91bc1":"for batch in ds_valid.take(1):\n    image_0 = batch[0][0]\n    pos_0 = batch[0][1]\n    plt.figure(figsize=(16, 8))\n    for channel in range(5):\n        plt.subplot(2, 5, channel+1)\n        plt.imshow(image_0[channel], cmap='gray')\n        plt.subplot(2, 5, channel+1+5)\n        plt.imshow(pos_0[channel], cmap='gray')","a52ccd33":"# Examine Data #","4ea76849":"# Parsing TFRecords #","3898974a":"# Imports #","16b0a607":"# Data Pipeline #"}}