{"cell_type":{"7cd38b15":"code","f9070177":"code","4a7f4939":"code","443f5400":"code","029eecb7":"code","b544f875":"code","92efa0d8":"code","012f30c6":"code","5b55b390":"code","f7c122c7":"code","fd6c34f2":"code","f84e15b9":"markdown","a473432f":"markdown","152785e9":"markdown","21cff687":"markdown","d4ce8242":"markdown","172bdb3b":"markdown","23db4f04":"markdown","b9b34d29":"markdown","4fa33248":"markdown","2af35529":"markdown","b53f1142":"markdown"},"source":{"7cd38b15":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom xgboost import XGBClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error\nfrom sklearn.metrics import confusion_matrix, classification_report\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, BaggingClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import cross_val_score\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","f9070177":"train = pd.read_csv(\"..\/input\/video-games-rating-by-esrb\/Video_games_esrb_rating.csv\")\ntest = pd.read_csv(\"..\/input\/video-games-rating-by-esrb\/test_esrb.csv\")\n\nprint(train.head())","4a7f4939":"fig, ax = plt.subplots(figsize=(20,15))\nsns.heatmap(data=train.corr(), annot=True)","443f5400":"X_train = train.drop([\"title\",\"esrb_rating\",\"console\"], axis=1)\ny_train = train[\"esrb_rating\"]\nX_test = test.drop([\"title\",\"esrb_rating\",\"console\"], axis=1)\ny_test = test[\"esrb_rating\"]","029eecb7":"model = DecisionTreeClassifier()\nmodel = model.fit(X_train, y_train)\n\ny_pred = model.predict(X_test)\n\nprint(\"Training Score: \", model.score(X_train, y_train))\nprint(\"Test Score: \", model.score(X_test, y_test))\n\ncm = confusion_matrix(y_test, y_pred)\ncr = classification_report(y_test, y_pred)\n\nprint(cm)\nprint(cr)","b544f875":"model = LogisticRegression()\nmodel = model.fit(X_train, y_train)\n\ny_pred = model.predict(X_test)\n\nprint(\"Training Score: \", model.score(X_train, y_train))\nprint(\"Test Score: \", model.score(X_test, y_test))\n\ncm = confusion_matrix(y_test, y_pred)\ncr = classification_report(y_test, y_pred)\n\nprint(cm)\nprint(cr)","92efa0d8":"model = RandomForestClassifier()\nmodel = model.fit(X_train, y_train)\n\ny_pred = model.predict(X_test)\n\nprint(\"Training Score: \", model.score(X_train, y_train))\nprint(\"Test Score: \", model.score(X_test, y_test))\n\ncm = confusion_matrix(y_test, y_pred)\ncr = classification_report(y_test, y_pred)\n\nprint(cm)\nprint(cr)","012f30c6":"model = XGBClassifier()\nmodel = model.fit(X_train, y_train)\n\ny_pred = model.predict(X_test)\n\nprint(\"Training Score: \", model.score(X_train, y_train))\nprint(\"Test Score: \", model.score(X_test, y_test))\n\ncm = confusion_matrix(y_test, y_pred)\ncr = classification_report(y_test, y_pred)\n\nprint(cm)\nprint(cr)","5b55b390":"model = GradientBoostingClassifier()\nmodel = model.fit(X_train, y_train)\n\ny_pred = model.predict(X_test)\n\nprint(\"Training Score: \", model.score(X_train, y_train))\nprint(\"Test Score: \", model.score(X_test, y_test))\n\ncm = confusion_matrix(y_test, y_pred)\ncr = classification_report(y_test, y_pred)\n\nprint(cm)\nprint(cr)","f7c122c7":"model = BaggingClassifier()\nmodel = model.fit(X_train, y_train)\n\ny_pred = model.predict(X_test)\n\nprint(\"Training Score: \", model.score(X_train, y_train))\nprint(\"Test Score: \", model.score(X_test, y_test))\n\ncm = confusion_matrix(y_test, y_pred)\ncr = classification_report(y_test, y_pred)\n\nprint(cm)\nprint(cr)","fd6c34f2":"model = RandomForestClassifier(n_estimators=200, max_features='log2', criterion='gini', n_jobs=-1)\nmodel = model.fit(X_train, y_train)\n\ny_pred = model.predict(X_test)\n\nprint(\"Training Score: \", model.score(X_train, y_train))\nprint(\"Test Score: \", model.score(X_test, y_test))\n\ncm = confusion_matrix(y_test, y_pred)\ncr = classification_report(y_test, y_pred)\n\nprint(cm)\nprint(cr)","f84e15b9":"## Split Data","a473432f":"## Result (Test)\n\n- Decision Tree Classifier : 82.8%\n- Logistic Regression : 80%\n- Random Forest Classifier : 85.4%\n- XGB Classifier : 83.4%\n- Gradient Boosting Classifier : 79.4%\n- Bagging Classifier : 83.6%\n- Random Forest (Param) : 85.4%","152785e9":"## Fit Model","21cff687":"## Random Forest Testing","d4ce8242":"#### XGB Classifier","172bdb3b":"#### Decision Tree","23db4f04":"## Read Data","b9b34d29":"#### Random Forest","4fa33248":"#### Gradent Boosting Classifier","2af35529":"#### Bagging Classifier","b53f1142":"#### Logistic Regression"}}