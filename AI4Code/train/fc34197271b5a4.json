{"cell_type":{"19b099c3":"code","46b49c2f":"code","f39a9840":"code","d1ebd3d7":"code","7cf54c92":"code","5a0b0140":"code","cdcd62ca":"code","7469acee":"code","830e0d6e":"code","2c314406":"code","39262f8a":"code","119a1fc7":"code","5942a467":"code","f5317593":"code","55e14922":"code","60a5aa2f":"markdown","2cb6846f":"markdown","5ad2c515":"markdown","046861af":"markdown","3a824c2b":"markdown","2fd88ffd":"markdown","9fb4cd72":"markdown","b6717d7a":"markdown"},"source":{"19b099c3":"import pandas as pd, numpy as np\nimport tensorflow as tf\nimport tensorflow.keras.backend as K\nfrom sklearn.model_selection import StratifiedKFold\nfrom transformers import *\nimport tokenizers\nprint('TF version',tf.__version__)","46b49c2f":"MAX_LEN = 140\nPATH = '..\/input\/tf-roberta\/'\ntokenizer = tokenizers.ByteLevelBPETokenizer(\n    vocab_file=PATH+'vocab-roberta-base.json', \n    merges_file=PATH+'merges-roberta-base.txt', \n    lowercase=True,\n    add_prefix_space=True\n)\nsentiment_id = {'positive': 1313, 'negative': 2430, 'neutral': 7974}\ntrain = pd.read_csv('..\/input\/tweet-sentiment-extraction\/train.csv').fillna('')\ntrain.head()","f39a9840":"ct = train.shape[0]\ninput_ids = np.ones((ct,MAX_LEN),dtype='int32')\nattention_mask = np.zeros((ct,MAX_LEN),dtype='int32')\ntoken_type_ids = np.zeros((ct,MAX_LEN),dtype='int32')\nstart_tokens = np.zeros((ct,MAX_LEN),dtype='int32')\nend_tokens = np.zeros((ct,MAX_LEN),dtype='int32')\n\nfor k in range(train.shape[0]):\n    \n    # FIND OVERLAP\n    text1 = \" \"+\" \".join(train.loc[k,'text'].split())\n    text2 = \" \".join(train.loc[k,'selected_text'].split())\n    idx = text1.find(text2)\n    chars = np.zeros((len(text1)))\n    chars[idx:idx+len(text2)]=1\n    if text1[idx-1]==' ': chars[idx-1] = 1 \n    enc = tokenizer.encode(text1) \n        \n    # ID_OFFSETS\n    offsets = []; idx=0\n    for t in enc.ids:\n        w = tokenizer.decode([t])\n        offsets.append((idx,idx+len(w)))\n        idx += len(w)\n    \n    # START END TOKENS\n    toks = []\n    for i,(a,b) in enumerate(offsets):\n        sm = np.sum(chars[a:b])\n        if sm>0: toks.append(i) \n        \n    s_tok = sentiment_id[train.loc[k,'sentiment']]\n    input_ids[k,:len(enc.ids)+5] = [0] + enc.ids + [2,2] + [s_tok] + [2]\n    attention_mask[k,:len(enc.ids)+5] = 1\n    if len(toks)>0:\n        start_tokens[k,toks[0]+1] = 1\n        end_tokens[k,toks[-1]+1] = 1","d1ebd3d7":"test = pd.read_csv('..\/input\/tweet-sentiment-extraction\/test.csv').fillna('')\n\nct = test.shape[0]\ninput_ids_t = np.ones((ct,MAX_LEN),dtype='int32')\nattention_mask_t = np.zeros((ct,MAX_LEN),dtype='int32')\ntoken_type_ids_t = np.zeros((ct,MAX_LEN),dtype='int32')\n\nfor k in range(test.shape[0]):\n        \n    # INPUT_IDS\n    text1 = \" \"+\" \".join(test.loc[k,'text'].split())\n    enc = tokenizer.encode(text1)                \n    s_tok = sentiment_id[test.loc[k,'sentiment']]\n    input_ids_t[k,:len(enc.ids)+5] = [0] + enc.ids + [2,2] + [s_tok] + [2]\n    attention_mask_t[k,:len(enc.ids)+5] = 1","7cf54c92":"# def build_model():\n#     ids = tf.keras.layers.Input((MAX_LEN,), dtype=tf.int32)\n#     att = tf.keras.layers.Input((MAX_LEN,), dtype=tf.int32)\n#     tok = tf.keras.layers.Input((MAX_LEN,), dtype=tf.int32)\n\n#     config = RobertaConfig.from_pretrained(PATH+'config-roberta-base.json')\n#     bert_model = TFRobertaModel.from_pretrained(PATH+'pretrained-roberta-base.h5',config=config)\n#     x = bert_model(ids,attention_mask=att,token_type_ids=tok)\n    \n#     x1 = tf.keras.layers.Dropout(0.1)(x[0]) \n#     x1 = tf.keras.layers.Conv1D(1,1)(x1)\n#     x1 = tf.keras.layers.Flatten()(x1)\n#     x1 = tf.keras.layers.Activation('softmax')(x1)\n    \n#     x2 = tf.keras.layers.Dropout(0.1)(x[0]) \n#     x2 = tf.keras.layers.Conv1D(1,1)(x2)\n#     x2 = tf.keras.layers.Flatten()(x2)\n#     x2 = tf.keras.layers.Activation('softmax')(x2)\n\n#     model = tf.keras.models.Model(inputs=[ids, att, tok], outputs=[x1,x2])\n#     optimizer = tf.keras.optimizers.Adam(learning_rate=3e-5)\n#     model.compile(loss='categorical_crossentropy', optimizer=optimizer)\n\n#     return model","5a0b0140":"# def build_model():\n#     ids = tf.keras.layers.Input((MAX_LEN,), dtype=tf.int32)\n#     att = tf.keras.layers.Input((MAX_LEN,), dtype=tf.int32)\n#     tok = tf.keras.layers.Input((MAX_LEN,), dtype=tf.int32)\n\n#     config = RobertaConfig.from_pretrained(PATH+'config-roberta-base.json')\n#     bert_model = TFRobertaModel.from_pretrained(PATH+'pretrained-roberta-base.h5',config=config)\n#     x = bert_model(ids,attention_mask=att,token_type_ids=tok)\n    \n#     x1 = tf.keras.layers.Dropout(0.1)(x[0]) \n#     x1 = tf.keras.layers.Conv1D(128, 2,padding='same')(x1)\n#     x1 = tf.keras.layers.LeakyReLU()(x1)\n#     x1 = tf.keras.layers.Conv1D(64, 2,padding='same')(x1)\n#     x1 = tf.keras.layers.Dense(1)(x1)\n#     x1 = tf.keras.layers.Flatten()(x1)\n#     x1 = tf.keras.layers.Activation('softmax')(x1)\n    \n#     x2 = tf.keras.layers.Dropout(0.1)(x[0]) \n#     x2 = tf.keras.layers.Conv1D(128, 2, padding='same')(x2)\n#     x2 = tf.keras.layers.LeakyReLU()(x2)\n#     x2 = tf.keras.layers.Conv1D(64, 2, padding='same')(x2)\n#     x2 = tf.keras.layers.Dense(1)(x2)\n#     x2 = tf.keras.layers.Flatten()(x2)\n#     x2 = tf.keras.layers.Activation('softmax')(x2)\n\n#     model = tf.keras.models.Model(inputs=[ids, att, tok], outputs=[x1,x2])\n#     optimizer = tf.keras.optimizers.Adam(learning_rate=3e-5)\n#     model.compile(loss='categorical_crossentropy', optimizer=optimizer)\n\n#     return model","cdcd62ca":"# def build_model():\n#     ids = tf.keras.layers.Input((MAX_LEN,), dtype=tf.int32)\n#     att = tf.keras.layers.Input((MAX_LEN,), dtype=tf.int32)\n#     tok = tf.keras.layers.Input((MAX_LEN,), dtype=tf.int32)\n\n#     config = RobertaConfig.from_pretrained(PATH+'config-roberta-base.json')\n#     bert_model = TFRobertaModel.from_pretrained(PATH+'pretrained-roberta-base.h5',config=config)\n#     x = bert_model(ids,attention_mask=att,token_type_ids=tok)\n#     print(x)\n    \n#     x1 = tf.keras.layers.Dropout(0.1)(x[0])\n#     print(\"tf.keras.layers.Dropout(0.1)\")\n#     print(x1.shape)\n#     x1 = tf.keras.layers.Conv1D(256, 2,padding='same')(x1)\n#     print(\"tf.keras.layers.Conv1D(256, 2,padding='same')\")\n#     print(x1.shape)\n#     x1 = tf.keras.layers.LeakyReLU()(x1)\n#     print(\"tf.keras.layers.LeakyReLU()\")\n#     print(x1.shape)\n#     x1 = tf.keras.layers.Conv1D(128, 2,padding='same')(x1)\n#     print(\"tf.keras.layers.Conv1D(128, 2,padding='same')\")\n#     print(x1.shape)\n#     x1 = tf.keras.layers.LeakyReLU()(x1)\n#     print(\"tf.keras.layers.LeakyReLU()\")\n#     print(x1.shape)\n#     x1 = tf.keras.layers.Conv1D(64, 2,padding='same')(x1)\n#     print(\"tf.keras.layers.Conv1D(64, 2,padding='same')\")\n#     print(x1.shape)\n#     x1 = tf.keras.layers.Dense(1)(x1)\n#     print(\"tf.keras.layers.Dense(1)\")\n#     print(x1.shape)\n#     x1 = tf.keras.layers.Flatten()(x1)\n#     print(\"tf.keras.layers.Flatten()\")\n#     print(x1.shape)\n#     x1 = tf.keras.layers.Activation('softmax')(x1)\n#     print(\"tf.keras.layers.Activation('softmax')\")\n#     print(x1.shape)\n    \n#     x2 = tf.keras.layers.Dropout(0.1)(x[0]) \n#     x2 = tf.keras.layers.Conv1D(256, 2, padding='same')(x2)\n#     x2 = tf.keras.layers.LeakyReLU()(x2)\n#     x2 = tf.keras.layers.Conv1D(128, 2, padding='same')(x2)\n#     x2 = tf.keras.layers.LeakyReLU()(x2)\n#     x2 = tf.keras.layers.Conv1D(64, 2, padding='same')(x2)\n#     x2 = tf.keras.layers.Dense(1)(x2)\n#     x2 = tf.keras.layers.Flatten()(x2)\n#     x2 = tf.keras.layers.Activation('softmax')(x2)\n\n#     model = tf.keras.models.Model(inputs=[ids, att, tok], outputs=[x1,x2])\n#     optimizer = tf.keras.optimizers.Adam(learning_rate=3e-5)\n#     model.compile(loss='categorical_crossentropy', optimizer=optimizer)\n\n#     return model","7469acee":"def build_model():\n    ids = tf.keras.layers.Input((MAX_LEN,), dtype=tf.int32)\n    att = tf.keras.layers.Input((MAX_LEN,), dtype=tf.int32)\n    tok = tf.keras.layers.Input((MAX_LEN,), dtype=tf.int32)\n\n    config = RobertaConfig.from_pretrained(PATH+'config-roberta-base.json')\n    bert_model = TFRobertaModel.from_pretrained(PATH+'pretrained-roberta-base.h5',config=config)\n    x = bert_model(ids,attention_mask=att,token_type_ids=tok)\n    \n    x1 = tf.keras.layers.Dropout(0.15)(x[0]) \n    x1 = tf.keras.layers.Conv1D(512, 2,padding='same')(x1)\n    x1 = tf.keras.layers.LeakyReLU()(x1)\n    x1 = tf.keras.layers.Conv1D(128, 2,padding='same')(x1)\n    x1 = tf.keras.layers.Dense(1)(x1)\n    x1 = tf.keras.layers.Flatten()(x1)\n    x1 = tf.keras.layers.Activation('softmax')(x1)\n    \n    x2 = tf.keras.layers.Dropout(0.15)(x[0]) \n    x2 = tf.keras.layers.Conv1D(512, 2, padding='same')(x2)\n    x2 = tf.keras.layers.LeakyReLU()(x2)\n    x2 = tf.keras.layers.Conv1D(128, 2, padding='same')(x2)\n    x2 = tf.keras.layers.Dense(1)(x2)\n    x2 = tf.keras.layers.Flatten()(x2)\n    x2 = tf.keras.layers.Activation('softmax')(x2)\n\n    model = tf.keras.models.Model(inputs=[ids, att, tok], outputs=[x1,x2])\n    optimizer = tf.keras.optimizers.Adam(learning_rate=3e-5)\n    model.compile(loss='categorical_crossentropy', optimizer=optimizer)\n\n    return model","830e0d6e":"#model = build_model()","2c314406":"#model.summary()","39262f8a":"def jaccard(str1, str2): \n    a = set(str1.lower().split()) \n    b = set(str2.lower().split())\n    if (len(a)==0) & (len(b)==0): return 0.5\n    c = a.intersection(b)\n    return float(len(c)) \/ (len(a) + len(b) - len(c))","119a1fc7":"jac = []; VER='v0'; DISPLAY=1 # USE display=1 FOR INTERACTIVE\noof_start = np.zeros((input_ids.shape[0],MAX_LEN))\noof_end = np.zeros((input_ids.shape[0],MAX_LEN))\npreds_start = np.zeros((input_ids_t.shape[0],MAX_LEN))\npreds_end = np.zeros((input_ids_t.shape[0],MAX_LEN))\n\nskf = StratifiedKFold(n_splits=5,shuffle=True,random_state=777)\nfor fold,(idxT,idxV) in enumerate(skf.split(input_ids,train.sentiment.values)):\n\n    print('#'*25)\n    print('### FOLD %i'%(fold+1))\n    print('#'*25)\n    \n    K.clear_session()\n    model = build_model()\n        \n    sv = tf.keras.callbacks.ModelCheckpoint(\n        '%s-roberta-%i.h5'%(VER,fold), monitor='val_loss', verbose=1, save_best_only=True,\n        save_weights_only=True, mode='auto', save_freq='epoch')\n        \n    model.fit([input_ids[idxT,], attention_mask[idxT,], token_type_ids[idxT,]], [start_tokens[idxT,], end_tokens[idxT,]], \n        epochs=3, batch_size=32, verbose=DISPLAY, callbacks=[sv],\n        validation_data=([input_ids[idxV,],attention_mask[idxV,],token_type_ids[idxV,]], \n        [start_tokens[idxV,], end_tokens[idxV,]]))\n    \n    print('Loading model...')\n    model.load_weights('%s-roberta-%i.h5'%(VER,fold))\n    \n    print('Predicting OOF...')\n    oof_start[idxV,],oof_end[idxV,] = model.predict([input_ids[idxV,],attention_mask[idxV,],token_type_ids[idxV,]],verbose=DISPLAY)\n    \n    print('Predicting Test...')\n    preds = model.predict([input_ids_t,attention_mask_t,token_type_ids_t],verbose=DISPLAY)\n    preds_start += preds[0]\/skf.n_splits\n    preds_end += preds[1]\/skf.n_splits\n    \n    # DISPLAY FOLD JACCARD\n    all = []\n    for k in idxV:\n        a = np.argmax(oof_start[k,])\n        b = np.argmax(oof_end[k,])\n        if a>b: \n            st = train.loc[k,'text'] # IMPROVE CV\/LB with better choice here\n        else:\n            text1 = \" \"+\" \".join(train.loc[k,'text'].split())\n            enc = tokenizer.encode(text1)\n            st = tokenizer.decode(enc.ids[a-1:b])\n        all.append(jaccard(st,train.loc[k,'selected_text']))\n    jac.append(np.mean(all))\n    print('>>>> FOLD %i Jaccard ='%(fold+1),np.mean(all))\n    print()","5942a467":"print('>>>> OVERALL 5Fold CV Jaccard =',np.mean(jac))","f5317593":"all = []\nfor k in range(input_ids_t.shape[0]):\n    a = np.argmax(preds_start[k,])\n    b = np.argmax(preds_end[k,])\n    if a>b: \n        st = test.loc[k,'text']\n    else:\n        text1 = \" \"+\" \".join(test.loc[k,'text'].split())\n        enc = tokenizer.encode(text1)\n        st = tokenizer.decode(enc.ids[a-1:b])\n    all.append(st)","55e14922":"test['selected_text'] = all\ntest[['textID','selected_text']].to_csv('submission.csv',index=False)\npd.set_option('max_colwidth', 60)\ntest.sample(25)","60a5aa2f":"# Load Libraries, Data, Tokenizer\nWe will use HuggingFace transformers [here][1]\n\n[1]: https:\/\/huggingface.co\/transformers\/","2cb6846f":"# TensorFlow roBERTa Starter - LB 0.705\nThis notebook is a TensorFlow template for solving Kaggle's Tweet Sentiment Extraction competition as a question and answer roBERTa formulation. In this notebook, we show how to tokenize the data, create question answer targets, and how to build a custom question answer head for roBERTa in TensorFlow. Note that HuggingFace transformers don't have a `TFRobertaForQuestionAnswering` so we must make our own from `TFRobertaModel`. This notebook can achieve LB 0.715 with some modifications. Have fun experimenting!\n\nYou can also run this code offline and it will save the best model weights during each of the 5 folds of training. Upload those weights to a private Kaggle dataset and attach to this notebook. Then you can run this notebook with the line `model.fit()` commented out, and this notebook will instead load your offline models. It will use your offline models to predict oof and predict test. Hence this notebook can easily be converted to an inference notebook. An inference notebook is advantageous because it will only take 10 minutes to commit and submit instead of 2 hours. Better to train 2 hours offline separately.","5ad2c515":"# Build roBERTa Model\nWe use a pretrained roBERTa base model and add a custom question answer head. First tokens are input into `bert_model` and we use BERT's first output, i.e. `x[0]` below. These are embeddings of all input tokens and have shape `(batch_size, MAX_LEN, 768)`. Next we apply `tf.keras.layers.Conv1D(filters=1, kernel_size=1)` and transform the embeddings into shape `(batch_size, MAX_LEN, 1)`. We then flatten this and apply `softmax`, so our final output from `x1` has shape `(batch_size, MAX_LEN)`. These are one hot encodings of the start tokens indicies (for `selected_text`). And `x2` are the end tokens indicies.\n\n![bert.jpg](attachment:bert.jpg)","046861af":"# Train roBERTa Model\nWe train with 5 Stratified KFolds (based on sentiment stratification). Each fold, the best model weights are saved and then reloaded before oof prediction and test prediction. Therefore you can run this code offline and upload your 5 fold models to a private Kaggle dataset. Then run this notebook and comment out the line `model.fit()`. Instead your notebook will load your model weights from offline training in the line `model.load_weights()`. Update this to have the correct path. Also make sure you change the KFold seed below to match your offline training. Then this notebook will proceed to use your offline models to predict oof and predict test.","3a824c2b":"# Test Data\nWe must tokenize the test data exactly the same as we tokenize the training data","2fd88ffd":"# Metric","9fb4cd72":"# Training Data\nWe will now convert the training data into arrays that roBERTa understands. Here are example inputs and targets: \n![ids.jpg](attachment:ids.jpg)\nThe tokenization logic below is inspired by Abhishek's PyTorch notebook [here][1].\n\n[1]: https:\/\/www.kaggle.com\/abhishek\/roberta-inference-5-folds","b6717d7a":"# Kaggle Submission"}}