{"cell_type":{"bfcb46f7":"code","05b1505a":"code","da0c158b":"code","02461b68":"code","889a4322":"code","571ab75a":"code","7c2ea6a8":"code","6c5a0996":"code","4184de94":"code","baabaa01":"code","575d0d1b":"code","717a25eb":"code","7960624f":"code","705b428d":"code","1e80b35b":"code","c87a878b":"code","480125c9":"code","baa578c7":"code","7d909d37":"code","b0fb83ae":"code","252056f6":"code","1548f65b":"code","d61312ca":"code","6839ad4c":"code","c19ee96d":"code","48dd5bca":"code","2d35814b":"code","e6649a6a":"code","6d4ea888":"code","c9a648eb":"code","7a46174a":"code","ca8fe495":"code","76f78024":"code","78b00fc1":"code","b04cd1fb":"code","330d99b0":"code","359087c8":"code","4c87758a":"code","f8444196":"code","52e3d393":"markdown","23e23cf3":"markdown","0a8b1c04":"markdown","54429d12":"markdown","da2de222":"markdown","8520bff1":"markdown","ee6e4ae2":"markdown","cccd25db":"markdown","127c3685":"markdown","145003f8":"markdown","a0a17d0d":"markdown","935d00b8":"markdown","d9b9886e":"markdown","3f2b715f":"markdown","03fdd9f5":"markdown","46cca794":"markdown","0f4fd2b4":"markdown","f3250617":"markdown","73f14de7":"markdown","848eff47":"markdown","bfd4d29a":"markdown","ce03b1ca":"markdown","9ca7210b":"markdown","0dd81e9e":"markdown","5a195e88":"markdown","8c6ec9ac":"markdown","66d801ee":"markdown","616c088c":"markdown"},"source":{"bfcb46f7":"import pandas as pd #To hand with data \nimport numpy as np #To math \nimport seaborn as sns #to visualization\nimport matplotlib.pyplot as plt # to plot the graphs\nimport matplotlib.gridspec as gridspec # to do the grid of plots","05b1505a":"#loading the data\ndf_credit = pd.read_csv(\"..\/input\/creditcard.csv\")","da0c158b":"#looking the how data looks\ndf_credit.head()","02461b68":"#looking the type and searching for null values\ndf_credit.info()","889a4322":"# The data is stardarized, I will explore them later\n#For now I will look the \"normal\" columns\ndf_credit[[\"Time\",\"Amount\",\"Class\"]].describe()","571ab75a":"#Lets start looking the difference by Normal and Fraud transactions\nprint(\"Distribuition of Normal(0) and Frauds(1): \")\nprint(df_credit[\"Class\"].value_counts())\n\nplt.figure(figsize=(7,5))\nsns.countplot(df_credit['Class'])\nplt.title(\"Class Count\", fontsize=18)\nplt.xlabel(\"Is fraud?\", fontsize=15)\nplt.ylabel(\"Count\", fontsize=15)\nplt.show()","7c2ea6a8":"timedelta = pd.to_timedelta(df_credit['Time'], unit='s')\ndf_credit['Time_min'] = (timedelta.dt.components.minutes).astype(int)\ndf_credit['Time_hour'] = (timedelta.dt.components.hours).astype(int)","6c5a0996":"#Exploring the distribuition by Class types throught hours and minutes\nplt.figure(figsize=(12,5))\nsns.distplot(df_credit[df_credit['Class'] == 0][\"Time_hour\"], \n             color='g')\nsns.distplot(df_credit[df_credit['Class'] == 1][\"Time_hour\"], \n             color='r')\nplt.title('Fraud x Normal Transactions by Hours', fontsize=17)\nplt.xlim([-1,25])\nplt.show()","4184de94":"#Exploring the distribuition by Class types throught hours and minutes\nplt.figure(figsize=(12,5))\nsns.distplot(df_credit[df_credit['Class'] == 0][\"Time_min\"], \n             color='g')\nsns.distplot(df_credit[df_credit['Class'] == 1][\"Time_min\"], \n             color='r')\nplt.title('Fraud x Normal Transactions by minutes', fontsize=17)\nplt.xlim([-1,61])\nplt.show()","baabaa01":"#To clearly the data of frauds and no frauds\ndf_fraud = df_credit[df_credit['Class'] == 1]\ndf_normal = df_credit[df_credit['Class'] == 0]\n\nprint(\"Fraud transaction statistics\")\nprint(df_fraud[\"Amount\"].describe())\nprint(\"\\nNormal transaction statistics\")\nprint(df_normal[\"Amount\"].describe())","575d0d1b":"#Feature engineering to a better visualization of the values\ndf_credit['Amount_log'] = np.log(df_credit.Amount + 0.01)","717a25eb":"plt.figure(figsize=(14,6))\n#I will explore the Amount by Class and see the distribuition of Amount transactions\nplt.subplot(121)\nax = sns.boxplot(x =\"Class\",y=\"Amount\",\n                 data=df_credit)\nax.set_title(\"Class x Amount\", fontsize=20)\nax.set_xlabel(\"Is Fraud?\", fontsize=16)\nax.set_ylabel(\"Amount(US)\", fontsize = 16)\n\nplt.subplot(122)\nax1 = sns.boxplot(x =\"Class\",y=\"Amount_log\", data=df_credit)\nax1.set_title(\"Class x Amount\", fontsize=20)\nax1.set_xlabel(\"Is Fraud?\", fontsize=16)\nax1.set_ylabel(\"Amount(Log)\", fontsize = 16)\n\nplt.subplots_adjust(hspace = 0.6, top = 0.8)\n\nplt.show()","7960624f":"#Looking the Amount and time distribuition of FRAUD transactions\nax = sns.lmplot(y=\"Amount\", x=\"Time_min\", fit_reg=False,aspect=1.8,\n                data=df_credit, hue='Class')\nplt.title(\"Amounts by Minutes of Frauds and Normal Transactions\",fontsize=16)\nplt.show()","705b428d":"ax = sns.lmplot(y=\"Amount\", x=\"Time_hour\", fit_reg=False,aspect=1.8,\n                data=df_credit, hue='Class')\nplt.title(\"Amounts by Hour of Frauds and Normal Transactions\", fontsize=16)\n\nplt.show()","1e80b35b":"#Looking the V's features\ncolumns = df_credit.iloc[:,1:29].columns\n\nfrauds = df_credit.Class == 1\nnormals = df_credit.Class == 0\n\ngrid = gridspec.GridSpec(14, 2)\nplt.figure(figsize=(15,20*4))\n\nfor n, col in enumerate(df_credit[columns]):\n    ax = plt.subplot(grid[n])\n    sns.distplot(df_credit[col][frauds], bins = 50, color='g') #Will receive the \"semi-salmon\" violin\n    sns.distplot(df_credit[col][normals], bins = 50, color='r') #Will receive the \"ocean\" color\n    ax.set_ylabel('Density')\n    ax.set_title(str(col))\n    ax.set_xlabel('')\nplt.show()","c87a878b":"#I will select the variables where fraud class have a interesting behavior and might can help us predict\n\ndf_credit = df_credit[[\"Time_hour\",\"Time_min\",\"V2\",\"V3\",\"V4\",\"V9\",\"V10\",\"V11\",\"V12\",\"V14\",\"V16\",\"V17\",\"V18\",\"V19\",\"V27\",\"Amount\",\"Class\"]]","480125c9":"df_credit.Amount = np.log(df_credit.Amount + 0.001)","baa578c7":"#Looking the final df\ndf_credit.head()","7d909d37":"colormap = plt.cm.Greens\n\nplt.figure(figsize=(14,12))\n\nsns.heatmap(df_credit.corr(),linewidths=0.1,vmax=1.0, \n            square=True, cmap = colormap, linecolor='white', annot=True)\nplt.show()","b0fb83ae":"from imblearn.pipeline import make_pipeline as make_pipeline_imb # To do our transformation in a unique time\nfrom imblearn.over_sampling import SMOTE\nfrom sklearn.pipeline import make_pipeline\nfrom imblearn.metrics import classification_report_imbalanced\n\nfrom sklearn.model_selection import train_test_split\nfrom collections import Counter\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\n\nfrom sklearn.metrics import precision_score, recall_score, fbeta_score, confusion_matrix, precision_recall_curve, accuracy_score\n","252056f6":"X = df_credit.drop([\"Class\"], axis=1).values #Setting the X to do the split\ny = df_credit[\"Class\"].values # transforming the values in array","1548f65b":"# the function that we will use to better evaluate the model\ndef print_results(headline, true_value, pred):\n    print(headline)\n    print(\"accuracy: {}\".format(accuracy_score(true_value, pred)))\n    print(\"precision: {}\".format(precision_score(true_value, pred)))\n    print(\"recall: {}\".format(recall_score(true_value, pred)))\n    print(\"f2: {}\".format(fbeta_score(true_value, pred, beta=2)))\n\n# splitting data into training and test set\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=2, test_size=0.20)\n\nclassifier = RandomForestClassifier\n\n# build model with SMOTE imblearn\nsmote_pipeline = make_pipeline_imb(SMOTE(random_state=4), \\\n                                   classifier(random_state=42))\n\nsmote_model = smote_pipeline.fit(X_train, y_train)\nsmote_prediction = smote_model.predict(X_test)\n\n#Showing the diference before and after the transformation used\nprint(\"normal data distribution: {}\".format(Counter(y)))\nX_smote, y_smote = SMOTE().fit_sample(X, y)\nprint(\"SMOTE data distribution: {}\".format(Counter(y_smote)))","d61312ca":"print(\"Confusion Matrix: \")\nprint(confusion_matrix(y_test, smote_prediction))\n\nprint('\\nSMOTE Pipeline Score {}'.format(smote_pipeline.score(X_test, y_test)))\n\nprint_results(\"\\nSMOTE + RandomForest classification\", y_test, smote_prediction)","6839ad4c":"# Compute predicted probabilities: y_pred_prob\ny_pred_prob = smote_pipeline.predict_proba(X_test)[:,1]\n\n# Generate precision recall curve values: precision, recall, thresholds\nprecision, recall, thresholds = precision_recall_curve(y_test, y_pred_prob)\n\n# Plot ROC curve\nplt.plot(precision, recall)\nplt.xlabel('Recall')\nplt.ylabel('Precision')\nplt.title('Precision Recall Curve')\nplt.show()","c19ee96d":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import KFold, cross_val_score\nfrom sklearn.model_selection import GridSearchCV","48dd5bca":"#params of the model\nparam_grid = {\"max_depth\": [3,5, None],\n              \"n_estimators\":[3,5,10],\n              \"max_features\": [5,6,7,8]}\n\n# Creating the classifier\nmodel = RandomForestClassifier(max_features=3, max_depth=2 ,n_estimators=10, random_state=3, criterion='entropy', n_jobs=-1, verbose=1 )","2d35814b":"grid_search = GridSearchCV(model, param_grid=param_grid, cv=5, scoring='recall')\ngrid_search.fit(X_train, y_train)","e6649a6a":"print(grid_search.best_score_)\nprint(grid_search.best_params_)","6d4ea888":"# Running the fit\nrf = RandomForestClassifier(max_depth=5, max_features = 7, n_estimators = 10)\nrf.fit(X_train, y_train)","c9a648eb":"# Printing the Training Score\nprint(\"Training score data: \")\nprint(rf.score(X_train, y_train))","7a46174a":"#Testing the model \n#Predicting by X_test\ny_pred = rf.predict(X_test)\n\nprint(confusion_matrix(y_test, y_pred))\nprint_results(\"RF classification\", y_test, y_pred)","ca8fe495":"features = [\"Time_min\", 'Time_hours',\"V2\",\"V3\",\"V4\",\"V9\",\"V10\",\"V11\",\"V12\",\"V14\",\"V16\",\"V17\",\"V18\",\"V19\",\"V27\",\"Amount\"]\n\n# Credits to Gabriel Preda\n# https:\/\/www.kaggle.com\/gpreda\/credit-card-fraud-detection-predictive-models\nplt.figure(figsize = (9,5))\n\nfeat_import = pd.DataFrame({'Feature': features, 'Feature importance': rf.feature_importances_})\nfeat_import = feat_import.sort_values(by='Feature importance',ascending=False)\n\ng = sns.barplot(x='Feature',y='Feature importance',data=feat_import)\ng.set_xticklabels(g.get_xticklabels(),rotation=90)\ng.set_title('Features importance - Random Forest',fontsize=20)\nplt.show() ","76f78024":"#Predicting proba\ny_pred_prob = rf.predict_proba(X_test)[:,1]\n\n# Generate precision recall curve values: precision, recall, thresholds\nprecision, recall, thresholds = precision_recall_curve(y_test, y_pred_prob)\n\n# Plot ROC curve\nplt.plot(precision, recall)\nplt.xlabel('Recall')\nplt.ylabel('Precision')\nplt.title('Precision Recall Curve')\nplt.show()","78b00fc1":"results = cross_val_score(rf,X_train, y_train, cv=10, scoring='recall')\nresults","b04cd1fb":"param_grid = {'C': [0.01, 0.1, 1, 10],\n             'penalty':['l1', 'l2']}\n\nlogreg = LogisticRegression(random_state=2)\n\ngrid_search_lr = GridSearchCV(logreg, param_grid=param_grid, scoring='recall', cv=5)\n\ngrid_search_lr.fit(X_train, y_train)","330d99b0":"# The best recall obtained\nprint(grid_search_lr.best_score_)\n#Best parameter on trainning set\nprint(grid_search_lr.best_params_)","359087c8":"# Creating the model \nlogreg = LogisticRegression(C=10, penalty='l2',random_state=2)\n\n#Fiting the model\nlogreg.fit(X_train, y_train)\n           \n# Printing the Training Score\nprint(\"Cross Validation of X and y Train: \")\nprint(cross_val_score(logreg,X_train, y_train, cv=5, scoring='recall'))","4c87758a":"# Predicting with the best params\ny_pred = logreg.predict(X_test)\n\nprint(confusion_matrix(y_test, y_pred))\nprint(\"\")\nprint_results(\"LogReg classification\", y_test, y_pred)","f8444196":"#Predicting proba\ny_pred_prob = logreg.predict_proba(X_test)[:,1]\n\n# Generate precision recall curve values: precision, recall, thresholds\nprecision, recall, thresholds = precision_recall_curve(y_test, y_pred_prob)\n\n# Plot ROC curve\nplt.plot(precision, recall)\nplt.xlabel('Recall')\nplt.ylabel('Precision')\nplt.title('Precision Recall Curve')\nplt.show()","52e3d393":"- Interesting distribuition, but don't sounds like a clear pattern of action","23e23cf3":"<h2>Let's start importing the libraries and taking a glance at the data<\/h2>","0a8b1c04":"## Looking the statistics of our Amount class frauds and normal transactions","54429d12":"# CONCLUSION: \nThe highest values of Normal transactions are 25691.16 while of Fraudulent transactions are just 2125.87. <br>\nThe average value of normal transactions are small(USD 88.29) than fraudulent transactions that is USD 122.21\n\n\nWe got the best score when we use the SMOTE (OverSampling)  + RandomForest, that performed a f2 score of 0.8669~ \n\nThis is a considerably difference by the second best model that is 0.8252 which uses just RandomForests with some Hyper Parameters.\n\nThe worst model was Logreg where I used GridSearchCV to get the Best params to fit and predict where the recall was ~0.6666 and f2 ~0.70.\n\n\n","da2de222":"70%  accuracy is not bad, but we found a high vale on  Random Forest Model","8520bff1":"## Preprocessing","ee6e4ae2":"## Feature importance","cccd25db":"We can see a interesting different distribuition in some of our features like V4, V9, V16, V17 and a lot more.  <br>\nNow let's take a look at time distribuition","127c3685":"## Feature Engineering","145003f8":"The top 4 feature are V17, V14, V12, V10 which corresponds to 75% of total. \n\nAlso the f2 score that is the median of recall and precision are on a considerably value","a0a17d0d":"## ROC CURVE - Random Forest","935d00b8":"<h1>Welcome to my kernel on Credit Card fraud!<\/h1>\n\nIn this kernel lets do some explorations trying to understand the fraud transaction patterns and then I will implement some models of machine learning.<br>\n\nI will implement a technique called SMOTE, supervised models and semi supervised learning algorithms.","d9b9886e":"## Feature selections","3f2b715f":"<h2>We will use boxplot to search differents distribuitions: <\/h2>\n- We are searching for features that diverges from normal distribuition","03fdd9f5":"### Looking a scatter plot of the Time_min distribuition by Amount","46cca794":"## OBJECTIVE\n\nWe will explore the data distribuition as this is almost all numerical features that was standrized.","0f4fd2b4":"<h2>Firstly, I will explore 3 different columns:<\/h2>\n- Time\n- Amount\n- Class","f3250617":"# This ROC Curve is a overfitted curve, here's how we can avoid the overfitted curve","73f14de7":"## Logistic Regression with Hyper Parameters","848eff47":"We can see a slight difference in log amount of our two Classes. <br>\nThe IQR of fraudulent transactions are higher than normal transactions, but normal transactions have highest values","bfd4d29a":"### Looking a scatter plot of the Time_hour distribuition by Amount","ce03b1ca":"We have a clearly imbalanced data.<br>\nIt's very common when treating frauds... <br>\n\n<b>First<\/b> We will do some exploration through Time and Amount. <br>\n<b>Second<\/b> We will explore the V's Features, that are PCA's ","9ca7210b":"## Time Features and some Feature Engineering\nAs our Time feature are in seconds we will transform it into minutes and hours to get a better understanding about  patterns","0dd81e9e":"### Setting the best parameters","5a195e88":"\nUsing this informations I will filter the values to look for Amount by Class <br>\nI will filter the \"normal\" amounts by 3.000","8c6ec9ac":"<h2>Introduction to Dataset<\/h2>\n\nThe datasets contains transactions made by credit cards in September 2013 by european cardholders. This dataset presents transactions that occurred in two days, where we have 492 frauds out of 284,807 transactions. The dataset is highly unbalanced, the positive class (frauds) account for 0.172% of all transactions.\n\nIt contains only numerical input variables which are the result of a PCA transformation.Due to confidentiality issues, we cannot provide the original features and more background information about the data.\n\nFeatures V1, V2, ... V28 are the principal components obtained with PCA, the only features which have not been transformed with PCA are 'Time' and 'Amount'. Feature 'Time' contains the seconds elapsed between each transaction and the first transaction in the dataset. The feature 'Amount' is the transaction Amount, this feature can be used for example-dependant cost-senstive learning. Feature 'Class' is the response variable and it takes value 1 in case of fraud and 0 otherwise.\n\nGiven the class imbalance ratio, we recommend measuring the accuracy using the Area Under the Precision-Recall Curve (AUPRC). Confusion matrix accuracy is not meaningful for unbalanced classification.\n\nThe dataset has been collected and analysed during a research collaboration of Worldline and the Machine Learning Group (http:\/\/mlg.ulb.ac.be) of ULB (Universit\u00e9 Libre de Bruxelles) on big data mining and fraud detection. More details on current and past projects on related topics are available on http:\/\/mlg.ulb.ac.be\/BruFence and http:\/\/mlg.ulb.ac.be\/ARTML\n\nPlease cite: Andrea Dal Pozzolo, Olivier Caelen, Reid A. Johnson and Gianluca Bontempi. Calibrating Probability with Undersampling for Unbalanced Classification. In Symposium on Computational Intelligence and Data Mining (CIDM), IEEE, 2015","66d801ee":"## Precision Recall Curve of Logistic Regression","616c088c":"## Evaluating the model SMOTE + Random Forest"}}