{"cell_type":{"6a305062":"code","e3c94164":"code","481e8dcf":"code","264c5af3":"code","85a61dd3":"code","2ba44075":"code","4dcf6f13":"code","c1d7925d":"code","c1d2e11e":"code","1dbf8b8e":"code","59498365":"code","b34d2c2f":"code","b4518fc0":"code","7b89bf30":"code","d626993f":"code","da1e2fe9":"code","01a45574":"code","1685bf0f":"code","7f32c9ce":"code","ed09e218":"code","69b14d8a":"code","879f2b2e":"code","c86ffb8b":"code","8af9d497":"code","ec439a7c":"code","d76a1ec4":"code","7d96aa50":"code","3deab643":"code","b665c39b":"code","f5c02be3":"code","71547046":"code","5410f26a":"code","bde9fff5":"code","fa994298":"code","21998ce4":"code","becb433f":"code","c5b7992f":"code","58653388":"code","b2bbc6eb":"markdown","acf5f042":"markdown","276eb12a":"markdown","e207bd22":"markdown","b97cf1b0":"markdown","73046734":"markdown","4fbb882e":"markdown","66da12b5":"markdown","e7dfcdf5":"markdown","bf5ae182":"markdown"},"source":{"6a305062":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","e3c94164":"import re #regex applies a regular expression to a string and returns the matching substrings. \nimport matplotlib.pyplot as plt\nimport seaborn as sns \nimport nltk \nfrom sklearn import feature_extraction, model_selection, naive_bayes, pipeline, manifold, preprocessing\nimport nltk.corpus\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import BlanklineTokenizer\nfrom nltk.tokenize import TweetTokenizer\nnltk.download('punkt')\nnltk.download('wordnet')\nnltk.download('stopwords')\nfrom nltk.stem import WordNetLemmatizer\nimport string\nfrom nltk.util import ngrams\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom collections import defaultdict\nfrom collections import  Counter\nplt.style.use('ggplot')\nstop=set(stopwords.words('english'))\nimport re\nimport scikitplot as skplt\nfrom nltk.tokenize import word_tokenize\nimport gensim\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom tqdm import tqdm\nfrom keras.models import Sequential\nfrom keras.layers import Embedding,LSTM,Dense,SpatialDropout1D\nfrom keras.initializers import Constant\nfrom sklearn.model_selection import train_test_split\nfrom nltk.stem import PorterStemmer \nfrom nltk.tokenize import word_tokenize\nfrom wordcloud import WordCloud, STOPWORDS\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nimport pickle\nfrom multiprocessing import Pool\nfrom textblob import TextBlob\nimport seaborn as sns","481e8dcf":"from PIL import Image\nimport requests\n","264c5af3":"pip install emojis","85a61dd3":"data = pd.read_csv('\/kaggle\/input\/large-random-tweets-from-pakistan\/Random Tweets from Pakistan- Cleaned.csv')\ndata.head(10)","2ba44075":"data.rename(columns={'full_text': 'Tweets'}, inplace=True)\ndata.head()","4dcf6f13":"print('There are {} rows and {} columns in data'.format(data.shape[0],data.shape[1]))","c1d7925d":"data.isnull().values.any()","c1d2e11e":"data.isnull().sum()","1dbf8b8e":"data.dtypes","59498365":"data.fillna(0, inplace = True)\ndata.head()","b34d2c2f":"#Let's display one the tweets existed in the text column \ndata['Tweets'][5]","b4518fc0":"data['Tweets'].describe()","7b89bf30":"data['location'].describe()","d626993f":"data['name'].describe()","da1e2fe9":"text = data['Tweets']\ntarget = data['location']\n\ntest_text = data['Tweets']\n\n# Print random samples from the training text \nfor i in np.random.randint(500, size=5):\n    print(f'Tweet #{i}: ', text[i], '=> Target: ', target[i], end='\\n' * 2)","01a45574":"city = data.groupby('location')\n\n# Summary statistic of all countries\ncity.describe().head()","1685bf0f":"data = pd.DataFrame(data)\ndata.drop(['Unnamed: 0', 'created_at_tweet', 'id_tweet',\n       'retweet_count', 'favorite_count', 'reply_count', 'name', 'screen_name',\n       'location'], axis = 1,inplace=True)\ndata.head()","7f32c9ce":"data['Tweets']=data['Tweets'].apply(str)\ndef cleantxt(text):\n    text = re.sub(r'@[A-Za-z0-9]+', '',text)\n    text = re.sub(r'#', '',text)\n    text = re.sub(r'RT[\\s]+', '',text)\n    text = re.sub(r'https?:\\\/\\\/\\S+', '',text)\n    \n    text = re.sub(r\"that's\",\"that is\",text)\n    text = re.sub(r\"there's\",\"there is\",text)\n    text = re.sub(r\"what's\",\"what is\",text)\n    text = re.sub(r\"where's\",\"where is\",text)\n    text = re.sub(r\"it's\",\"it is\",text)\n    text = re.sub(r\"who's\",\"who is\",text)\n    text = re.sub(r\"i'm\",\"i am\",text)\n    text = re.sub(r\"she's\",\"she is\",text)\n    text = re.sub(r\"he's\",\"he is\",text)\n    text = re.sub(r\"they're\",\"they are\",text)\n    text = re.sub(r\"who're\",\"who are\",text)\n    text = re.sub(r\"ain't\",\"am not\",text)\n    text = re.sub(r\"wouldn't\",\"would not\",text)\n    text = re.sub(r\"shouldn't\",\"should not\",text)\n    text = re.sub(r\"can't\",\"can not\",text)\n    text = re.sub(r\"couldn't\",\"could not\",text)\n    text = re.sub(r\"won't\",\"will not\",text)\n\n    return text\n\ndata['Tweets'] = data['Tweets'].apply(cleantxt)\n\ndata['Tweets']","ed09e218":"def getSubjectivity(text):\n    return TextBlob(text).sentiment.subjectivity\n\ndef getPolarity(text):\n    return TextBlob(text).sentiment.polarity\n\ndata['Subjectivity'] = data['Tweets'].apply(getSubjectivity)\ndata['Polarity'] = data['Tweets'].apply(getPolarity)\n\ndata","69b14d8a":"def getAnalysis(score):\n    if score < 0:\n        return 'Negative'\n    elif score == 0:\n        return 'Neutral'\n    else:\n        return 'Positive'\ndata['Analysis'] = data['Polarity'].apply(getAnalysis)\ndata","879f2b2e":"data['Analysis'].value_counts()\n\nplt.title('Sentiment Analysis')\nplt.xlabel('Sentiment')\nplt.ylabel('Counts')\ndata['Analysis'].value_counts().plot(kind='bar')\nplt.show()","c86ffb8b":"comment_words = '' \nstopwords = set(STOPWORDS)","8af9d497":"for val in data.Tweets: \n      \n    val = str(val) \n  \n    tokens = val.split() \n      \n    for i in range(len(tokens)): \n        tokens[i] = tokens[i].lower() \n      \n    comment_words += \" \".join(tokens)+\" \"\n  \npic = np.array(Image.open(requests.get('http:\/\/www.clker.com\/cliparts\/O\/i\/x\/Y\/q\/P\/yellow-house-hi.png',stream=True).raw))\n\nwordcloud = WordCloud(width = 800, height = 800, \n                background_color ='white', \n                stopwords = stopwords, mask = pic, \n                min_font_size = 10).generate(comment_words)","ec439a7c":"plt.figure(figsize = (10, 10), facecolor = 'red', edgecolor='blue') \nplt.imshow(wordcloud) \nplt.axis(\"off\") \nplt.tight_layout(pad = 0) \n  \nplt.show()","d76a1ec4":"data.columns","7d96aa50":"words = data[data.Subjectivity==0].Tweets.apply(lambda x: [word.lower() for word in x.split()])\nh_words = Counter()\n\nfor Tweets_ in words:\n    h_words.update(Tweets_)\n    \nprint(h_words.most_common(50))","3deab643":"words = data[data.Subjectivity==1].Tweets.apply(lambda x: [word.lower() for word in x.split()])\nh_words = Counter()\n\nfor Tweets_ in words:\n    h_words.update(Tweets_)\n    \nprint(h_words.most_common(50))","b665c39b":"def  clean_text(df, text_field, new_text_field_name):\n    df[new_text_field_name] = df[text_field].str.lower() #Convert strings in the Series\/Index to lowercase.\n    \n    # remove numbers\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda elem: re.sub(r\"\\d+\", \"\", elem))\n    #remove url\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda elem: re.sub(r\"https?:\/\/\\S+|www\\.\\S+\", \"\", elem))\n    #remove HTML tags\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda elem: re.sub(r\"<.*?>\", \"\", elem))\n    #remove emojis \n    df[new_text_field_name] = df[new_text_field_name].apply(lambda elem: re.sub(r\"[\"\n                                                                                 u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                           u\"\\U00002702-\\U000027B0\"\n                           u\"\\U000024C2-\\U0001F251\"\n                           \"]+\", \"\", elem))\n    return df\ndata_clean = clean_text(data, 'Tweets', 'text_clean')\ndata_clean_test = clean_text(data,'Tweets', 'text_clean')\ndata_clean.head(20)","f5c02be3":"from nltk.corpus import stopwords\nstop = stopwords.words('english')\ndata_clean['text_clean'] = data_clean['text_clean'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))\ndata_clean.head()","71547046":"from nltk.tokenize import sent_tokenize, word_tokenize\ndata_clean['text_tokens'] = data_clean['text_clean'].apply(lambda x: word_tokenize(x))\ndata_clean.head()","5410f26a":"import nltk\nfrom nltk.stem.porter import PorterStemmer\nporter_stemmer  = PorterStemmer()\ntext = \"happy birthday brother n boss , may many many\"\ntokenization = nltk.word_tokenize(text)\nfor w in tokenization:\n    print(\"Stemming for {} is {}\".format(w,porter_stemmer.stem(w)))","bde9fff5":"#Lemmatization\nimport nltk\nfrom nltk.stem import \tWordNetLemmatizer\nwordnet_lemmatizer = WordNetLemmatizer()\ntext = \"studies studying cries cry\"\ntokenization = nltk.word_tokenize(text)\nfor w in tokenization:\n\tprint(\"Lemma for {} is {}\".format(w, wordnet_lemmatizer.lemmatize(w)))  ","fa994298":"from nltk.stem import PorterStemmer \nfrom nltk.tokenize import word_tokenize\ndef word_stemmer(text):\n    stem_text = [PorterStemmer().stem(i) for i in text]\n    return stem_text\ndata_clean['text_clean_tokens'] = data_clean['text_tokens'].apply(lambda x: word_stemmer(x))\ndata_clean.head()","21998ce4":"X_train, X_test, Y_train, Y_test = train_test_split(data_clean['text_clean'], \n                   \n                                                    data_clean['Tweets'], \n                                                    test_size = 0.2,\n                                                    random_state = 10)","becb433f":"print(X_train.shape)\nprint(X_test.shape)\nprint(Y_train.shape)\nprint(Y_test.shape)","c5b7992f":"vectorizer = CountVectorizer(analyzer='word', ngram_range=(1, 1))\nvectorized = vectorizer.fit_transform(X_train)\npd.DataFrame(vectorized.toarray(), \n            index=['sentence '+str(i) \n                   for i in range(1, 1+len(X_train))],\n            columns=vectorizer.get_feature_names())","58653388":"#Only alphabet, contains at least 3 letters\nvectorizer = CountVectorizer(analyzer='word', \n                              token_pattern=r'\\b[a-zA-Z]{3,}\\b',  \n                              ngram_range=(1, 1))\nvectorized = vectorizer.fit_transform(X_train)\npd.DataFrame(vectorized.toarray(), \n             index=['sentence '+str(i) \n                    for i in range(1, 1+len(X_train))],\n             columns=vectorizer.get_feature_names())","b2bbc6eb":"# Tokenizing\n","acf5f042":"# lemitization","276eb12a":"# **Exploratory Data Analysis**","e207bd22":"# Stemming","b97cf1b0":"#  **Removing Stop words**","73046734":"# Word Cloud of Most used Words","4fbb882e":"# Feature Extraction : tf-idf","66da12b5":"# In Details about each Subjectivity.","e7dfcdf5":"# Positive and negative words count.","bf5ae182":"# **Hashtag analysis**"}}