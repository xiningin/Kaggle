{"cell_type":{"314b313b":"code","ee4ba60e":"code","c32f02d0":"code","dd6fa097":"code","b5b0b4d3":"code","6f9de169":"code","2abd7887":"code","896ec5b6":"code","b343681d":"code","4c1ea203":"code","cac7c3cd":"code","0ebe3460":"code","a763e435":"code","91bc4ac4":"code","f6998d22":"code","13d616b9":"code","b0f24f7c":"code","a4201ef5":"code","32476eb1":"code","ea50757e":"code","14b3e116":"code","7e4cec8e":"code","6c9dc80a":"code","16b65016":"code","4bca00a6":"code","fa08c4a2":"code","edc05d98":"code","59b157b9":"code","6ce645b5":"code","f764b21e":"code","6c9c4661":"code","c1ffa64f":"code","79a857e1":"code","7c5a520b":"code","5fc50528":"markdown","57e1c04b":"markdown","74506c42":"markdown","010fdc57":"markdown","894f4488":"markdown","d4afcce1":"markdown","4da77eec":"markdown","158f9f19":"markdown","085a8c80":"markdown","9cc3731e":"markdown","0974b754":"markdown","0260913a":"markdown","57a02fb9":"markdown","501d1e51":"markdown","285a3b50":"markdown","d749a821":"markdown","00145948":"markdown","cf0f9efb":"markdown","695b7d66":"markdown","b90d9581":"markdown","66a991d5":"markdown","0e5a7057":"markdown","49e29419":"markdown","d453ba9c":"markdown","6316b5a4":"markdown","bbb7032c":"markdown","51638284":"markdown","1b384ec6":"markdown","c75d65f9":"markdown","b43c9857":"markdown","a526d9ce":"markdown","4bee08d8":"markdown","ba3608e5":"markdown","4b36d55c":"markdown","5ffd9761":"markdown","6e9a9f93":"markdown","7baf5735":"markdown","e3c19eb2":"markdown"},"source":{"314b313b":"# maximum number of articles you want returned for each query\nTOP_N_ARTICLES = 6\n\n# maximum number of points\/sentences you want returned for each article summary \nTOP_N_POINTS = 5\n\n# path to corpus of data\nCORONAVIRUS_LIBRARY_PATH = '\/kaggle\/input\/CORD-19-research-challenge'\n\n# path to Opaki BM25 model\nBM25_MODEL_PATH = '\/kaggle\/input\/bm25-model\/bm25_model'\n\n# path to Word2Vec model\nWORD2VEC_MODEL_PATH = '\/kaggle\/input\/word2vec-model\/word2vec_model'\n\n# path to article csv file\nARTICLE_INFO_PATH = '\/kaggle\/input\/article-data\/article_info.csv'","ee4ba60e":"import re\n\n# url links, 'doi preprint', [citations]\nARTIFACTS = r'https?:\\\/\\\/.[^\\s\\\\]*|doi: medRxiv|preprint|\\[\\d+\\]|\\[\\d+\\, \\d+\\]'\n\n\nclass Article():\n    '''`Article` object for storing article text information.'''\n\n    def __init__(self, article):\n        '''Initialize `Article` object.'''\n        self.article = article\n        \n    def get_title(self):\n        '''Article title.'''\n        return self.article['metadata']['title']\n    \n    def get_abstract(self):\n        '''Article abstract bodytext.'''\n        return self.clean_text_of_artifacts(\n            self.combine_bodytext(self.article.get('abstract', []))\n        )\n\n    def get_bodytext(self):\n        '''Article main text.'''\n        return self.clean_text_of_artifacts(\n            self.combine_bodytext(self.article.get('body_text', []))\n        )\n\n    def get_full_text(self):\n        '''Article abstract and body text.'''\n        return self.get_abstract() + ' ' + self.get_bodytext()\n    \n    @staticmethod\n    def clean_text_of_artifacts(text):\n        '''Remove URL links and other artifacts from text.\n        \n        Parameters\n        ----------\n        text : str\n\n        Returns\n        -------\n        str\n        '''\n        return re.sub(ARTIFACTS, '', text, flags=re.MULTILINE)\n\n    @staticmethod\n    def combine_bodytext(text_info):\n        '''Get combined text fields from list of dicts.\n        \n        Parameters\n        ----------\n        text_info : List[Dict[str]]\n            List of body text.\n\n        Returns\n        -------\n        str\n            `text_info` joined together into string.\n        '''\n        return ' '.join(x['text'] for x in text_info)\n","c32f02d0":"import json\nimport pickle\nimport re\n\nfrom nltk.corpus import stopwords\n\n\ndef is_match(text, keywords):\n    '''Check if any `keywords` exist in `text`.\n\n    Parameters\n    ----------\n    text : str\n    keywords : List[str]\n\n    Returns\n    -------\n    bool\n    '''\n    return any(f'{keyterm} ' in text.lower() for keyterm in keywords)\n\n\ndef clean_text(text):\n    '''Remove punctuation from query and lowercase letters.\n\n    Parameters\n    ----------\n    query : str\n\n    Returns\n    -------\n    str\n    '''\n    return re.sub('\\?|!|\\.|,|\\(|\\)', '', text).lower()\n\n\ndef load_json(filename):\n    '''Load json file.'''\n    with open(filename, 'r') as f:\n        return json.load(f)\n\n\ndef load_model(filename):\n    '''Load pickled model.'''\n    return pickle.load(open(filename, 'rb'))\n\n\ndef save_model(model, filename):\n    '''Write pickled model to file.'''\n    with open(filename, 'wb') as f:\n        pickle.dump(model, f)\n","dd6fa097":"import os\nimport pandas as pd\n\nfrom collections import Counter\n\n\ndef get_data_corpus():\n    '''Get corpus data.\n    \n    Returns\n    -------\n    str\n    '''\n    data = pd.read_csv(ARTICLE_INFO_PATH).loc[:, 'text']\n    corpus_text = ' '.join(data.values)\n    return corpus_text.replace(\"\\n\", \" \")\n\n\ndef get_stopwords_from_corpus(corpus):\n    '''Extract stopwords from corpus based on word frequency.\n    \n    Parameters\n    ----------\n    corpus : str\n    \n    Returns\n    -------\n    List[str]\n    '''\n    sample = corpus.lower().split(' ')\n\n    word_counts = Counter(sample)\n\n    max_word_count = max(word_counts.values())\n\n    threshold = 5000\n    return [word for word, count in word_counts.items() if count > threshold]\n\n\n# Run once and add `stopwords` to `constants` file.\n# sample = get_data_corpus()\n# corpus_stopwords = get_stopwords_from_corpus(sample)\n","b5b0b4d3":"CORPUS_STOPWORDS = [\n    'number',\n    'human',\n    'cases',\n    'also',\n    'reported',\n    'one',\n    'immune',\n    'response',\n    'within',\n    'influenza',\n    'among',\n    'different',\n    'high',\n    'found',\n    'showed',\n    'use',\n    'identified',\n    'two',\n    'used',\n    'results',\n    'analysis',\n    'performed',\n    'using',\n    'described',\n    'detected',\n    'including',\n    'group',\n    'could',\n    'observed',\n    'significant',\n    'based',\n    'shown',\n    'however,',\n    'compared',\n    'higher',\n    'may',\n    'specific',\n    'studies',\n    'study',\n    'type',\n    'well',\n    'although',\n    'levels',\n    'host',\n    'activity',\n    'data',\n    'associated',\n    'due',\n    'samples',\n    'figure',\n    'table',\n    'case',\n    'effect', \n    'effects', \n    'affected',\n    'across',\n    'within',\n    'humans',\n    'who',\n    'what',\n    'why',\n    'how',\n    'distribution',\n    'eg',\n    'ie',\n    'prevalence',\n    'particularly',\n    'whether',\n    'make',\n    'even',\n    'might',\n    '2019',\n]\n\nCOVID_19_TERMS = [\n    'covid-19', \n    'covid 19',\n    'covid-2019',\n    '2019 novel coronavirus', \n    'corona virus disease 2019',\n    'coronavirus disease 19',\n    'coronavirus 2019',\n    '2019-ncov',\n    'ncov-2019', \n    'wuhan virus',\n    'wuhan coronavirus',\n    'wuhan pneumonia',\n    'NCIP',\n    'sars-cov-2',\n    'sars-cov2',\n]\n\nVIRUS_TERMS = [\n    'epidemic', \n    'pandemic', \n    'viral',\n    'virus',\n    'viruses',\n    'coronavirus', \n    'respiratory',\n    'infectious',\n] + COVID_19_TERMS\n","6f9de169":"from nltk.corpus import stopwords\n    \n\ndef get_stopwords():\n    '''Get english stopwords and corpus stopwords.''' \n    return set(VIRUS_TERMS + CORPUS_STOPWORDS + stopwords.words('english'))\n","2abd7887":"from gensim.models import KeyedVectors, Word2Vec \nfrom nltk import sent_tokenize, word_tokenize\n\n\ndef tokenize_sentences(sample):\n    '''Tokenize sentences of text into words.\n    \n    Iterates through each sentence and tokenizes each sentence into words.\n    \n    Parameters\n    ----------\n    sample : str \n    \n    Returns\n    -------\n    List[List[str]]\n        sentences --> words\n    ''' \n    return [\n        [\n            word.lower() for word in word_tokenize(sent) \n            if word not in get_stopwords()\n        ] for sent in sent_tokenize(sample)\n    ]\n\n\ndef train_word2vec_model():\n    '''Train gensim Word2Vec model.\n    \n    Returns\n    -------\n    gensim.models.Word2Vec\n    '''\n    logging.info('Getting sample...')\n    sample = get_data_corpus()\n    \n    logging.info('Tokenizing sample...')\n    tokenized_corpus = tokenize_sentences(sample)\n    \n    # Create CBOW model\n    logging.info('Creating Word2Vec model...')  \n    model = Word2Vec(tokenized_corpus, min_count=100, size=100, window=5)\n    \n    model.save(WORD2VEC_MODEL_PATH)\n    \n    return model\n    \n\n# Run this only once.\n# Open saved model instead of re-running code afterwards.\n\n# word2vec_model = train_word2vec_model()\n","896ec5b6":"!pip install rank_bm25","b343681d":"import logging\nimport os\nimport time\nimport pandas as pd\n\nfrom nltk.tokenize import word_tokenize \nfrom rank_bm25 import BM25Okapi\n\n\nlogging.getLogger().setLevel(logging.INFO)\n\n\ndef get_data():\n    '''Get all research paper data in dataset.\n\n    Returns\n    -------\n    pandas.DataFrame\n        Includes columns ['title', 'abstract', 'text'].\n    '''\n    articles = []\n    for dirname, _, filenames in os.walk(CORONAVIRUS_LIBRARY_PATH):\n        for filename in filenames:\n            full_path = os.path.join(dirname, filename)\n            \n            if full_path.endswith('.json'):\n                article = Article(load_json(full_path))\n                \n                articles.append(\n                    [\n                        article.get_title(), \n                        article.get_abstract(), \n                        article.get_full_text(),\n                        article.id\n                    ]\n                )\n    \n    return pd.DataFrame(articles, columns=['title', 'abstract', 'text', 'id'])\n\n\ndef add_metadata(data):\n    '''Add article metadata to data.\n\n    Parameters\n    ----------\n    data : pandas.DataFrame\n\n    Returns\n    -------\n    pandas.DataFrame\n        Returns `data` with additional meta-information.\n    '''\n    metadata = pd.read_csv('CORD-19-research-challenge\/metadata.csv')\n    data = data.join(\n        metadata.set_index('sha'), on='id', how='left', rsuffix='_meta'\n    )\n    \n    return data\n\n\ndef deduplicate_data(data):\n    '''Remove duplicates in data.\n\n    Parameters\n    ----------\n    data : pandas.DataFrame\n\n    Returns\n    -------\n    pandas.DataFrame\n    '''\n    return data.drop_duplicates()\n\n\ndef filter_covid19_articles(df):\n    '''Filter DataFrame on articles that contain COVID-19 keyterms.\n\n    Parameters\n    ----------\n    df : pandas.DataFrame\n        Article info, including 'text' column.\n\n    Returns\n    -------\n    pandas.DataFrame\n        Article info of COVID-19 related papers.\n    '''\n    return df[\n        df['text'].apply(lambda x: is_match(x, set(COVID_19_TERMS)))\n    ]\n\n\ndef tokenize_documents(corpus):\n    '''Tokenize corpus of documents.\n    \n    Parameters\n    ----------\n    corpus : List[str]\n        Corpus of research paper documents.\n    \n    Returns\n    -------\n    List[List[str]]\n        documents --> words\n    '''\n    return [\n        [\n            word for word in word_tokenize(clean_text(doc)) \n            if word not in get_stopwords()\n        ] for doc in corpus \n    ] \n\n\ndef train_bm25_model(corpus):\n    '''Train an Okapi BM25 model on corpus of research articles.\n\n    Parameters\n    ----------\n    corpus : pandas.Series\n\n    Returns \n    -------\n    rankbm25.BM25Okapi\n        Okapi BM25 model trained on corpus data.\n    '''    \n    logging.info('Tokenizing documents...')\n    tokenized_corpus = tokenize_documents(corpus)\n        \n    logging.info('Training BM25 model...')\n    return BM25Okapi(tokenized_corpus)\n\n\ndef main():\n    data = get_data()\n    logging.info(f'Number of articles: {len(data)}')\n\n    # only 274 article titles containing COVID-19 terms\n    # 1716 articles wirh COVID-19 terms in the text    \n    data = filter_covid19_articles(data)\n    logging.info(f'Number of COVID-19 articles: {len(data)}')\n\n    data = deduplicate_data(data)\n    logging.info(f'Number of COVID-19 articles: {len(data)}')\n\n    data = add_metadata(data)\n    logging.info(f'Number of COVID-19 articles: {len(data)}')\n\n    data.to_csv(ARTICLE_INFO_PATH)\n\n    bm25_model = train_bm25_model(data['text'])\n    save_model(bm25_model, BM25_MODEL_PATH)\n\n\n# Run only once and save model.\n\nstart_time = time.time()\n\n# main()\nbm25_model = load_model(BM25_MODEL_PATH)\n    \nseconds = time.time() - start_time\nminutes = seconds \/ 60\nprint('Took {:.2f} minutes'.format(minutes))\n","4c1ea203":"class SearchQuery():\n    '''`SearchQuery` object for cleaning and processing a `query` input.'''\n\n    SIMILARITY_THRESHOLD = 0.62\n\n    def __init__(self, query):\n        '''Initialize `SearchQuery` object.\n        \n        Parameters\n        ----------\n        query : str\n        '''\n        self.query = clean_text(query)\n        self.init_query_keywords()\n        self.init_related_keywords(self.get_word2vec_model())\n\n    def init_query_keywords(self):\n        '''Initialize query keywords.'''\n        self.query_keywords = [\n            x for x in self.query.split() if x not in get_stopwords()\n        ]\n\n    def init_related_keywords(self, word2vec_model):\n        '''Initialize keywords related to `query_keywords`.\n\n        Iterates over each keyterm in `query_keywords` and finds related words \n        from the trained `Word2Vec` vocabulary. If there's a high enough \n        similarity score, adds it to `related_keywords`.\n\n        Parameters\n        ----------\n        word2vec_model : gensim.models.Word2Vec\n            `Word2Vec` model trained on corpus data.  \n        '''\n        self.related_keywords = []\n        for word in self.query_keywords:\n            if word in word2vec_model.wv.vocab:\n                self.related_keywords += [\n                    x[0] for x in word2vec_model.wv.most_similar(word, topn=10) \n                    if x[1] > SearchQuery.SIMILARITY_THRESHOLD\n                ]\n    \n    @staticmethod\n    def get_word2vec_model():\n        '''Load `Word2Vec` model previously trained on the dataset.\n        \n        Returns\n        -------\n        gensim.models.Word2Vec\n        '''\n        return Word2Vec.load(WORD2VEC_MODEL_PATH)\n","cac7c3cd":"import re\n\nfrom gensim.summarization.summarizer import summarize\n\n\nclass Summary():\n    '''`Summary` object for extracting executive summary from text.'''\n    \n    def __init__(self, text, query_keywords):\n        '''Initialize `Summary` object.\n        \n        Parameters\n        ----------\n        text : str\n        query_keywords : List[str]\n        '''\n        self.text = text\n        self.keywords = query_keywords\n\n    def get_topn_sentences(self):\n        '''Get top `n` sentences of text as summary.\n        \n        Returns\n        -------\n        List[str]\n        '''\n        ranked_sentences = summarize(self.text, split=True) \n        relevant_sentences = self.filter_relevant_sentences(ranked_sentences)\n\n        return relevant_sentences[:TOP_N_POINTS]\n    \n    @staticmethod\n    def is_decimal_value_in_text(text):\n        '''Check if there is a decimal value or percentage within the text.\n\n        Make sure that decimal value is not a Figure or Section number.\n\n        Parameters\n        ----------\n        text : str\n\n        Returns\n        -------\n        bool\n        '''\n        patterns = [\n            r'(?<!Section )([0-9]+\\.[0-9]+|%)',\n            r'(?<!SECTION )([0-9]+\\.[0-9]+|%)',\n            r'(?<!Figure )([0-9]+\\.[0-9]+|%)',\n            r'(?<!FIGURE )([0-9]+\\.[0-9]+|%)',\n            r'(?<!Fig )([0-9]+\\.[0-9]+|%)',\n            r'(?<!Fig. )([0-9]+\\.[0-9]+|%)',\n            r'(?<!Tables )([0-9]+\\.[0-9]+|%)',\n            r'(?<!Chapter )([0-9]+\\.[0-9]+|%)',\n            r'(?<!CHAPTER )([0-9]+\\.[0-9]+|%)',\n        ]\n\n        if all(re.search(pattern, text) for pattern in patterns):\n            return True \n        \n        return False\n\n    def filter_relevant_sentences(self, sentences):\n        '''Filter sentences on relevancy filter. \n        \n        If filters out all sentences, returns original unfiltered sentences instead.\n        \n        NOTE: Previously was filtering on whether keyword exists in sentence. Now\n        filters on whether decimal value exists in sentence.\n\n        Parameters\n        ----------\n        sentences : List[str]\n\n        Returns\n        -------\n        List[str]\n        '''\n        filtered_sentences = [\n            sentence for sentence in sentences \n            if self.is_decimal_value_in_text(sentence)\n        ]\n        \n        if not filtered_sentences:\n            return sentences\n        \n        return filtered_sentences\n","0ebe3460":"class SearchResult():\n    '''`SearchResult` object for storing search result article information.'''\n\n    def __init__(self, title, text, url, query_keywords):\n        '''Initialize `SearchResult` object.\n\n        Parameters\n        ----------\n        title : str\n            Article title.\n        text : str\n            Article text.\n        url : str\n            Article url link.\n        query_keywords: List[str]\n            Query search keywords.\n        '''\n        self.title = title\n        self.text = text\n        self.url = url\n        self.keywords = query_keywords \n\n        self.main_points = self.get_topn_points()\n\n        if isinstance(title, str):\n            text = text + title\n        \n        self.study_info = StudyInfo(text, url)\n    \n    def get_topn_points(self):\n        '''Keep `n` most highly ranked article points.'''\n        points = Summary(self.text, self.keywords)\n        \n        return points.get_topn_sentences()","a763e435":"class StudyInfo():\n    '''Object for extracting the level of evidence for findings in paper.'''\n\n    def __init__(self, article_text, url_link):\n        '''Initialize `StudyInfo` object.\n\n        Parameters\n        ----------\n        article_text : str\n        url_link : str\n        '''\n        self.article_text = article_text\n        self.url = url_link\n        self.peer_reviewed = self.is_peer_reviewed(article_text)\n        self.num_studies = self.extract_number_of_studies(article_text)\n        self.sample_size = self.extract_sample_size(article_text)\n        self.study_designs = self.extract_study_design(article_text) \n\n    @staticmethod\n    def is_peer_reviewed(text):\n        '''Check if paper is peer-reviewed.\n\n        Returns\n        -------\n        Optional[bool]\n            Returns None if unsure.\n        '''\n        non_peer_reviewed_clause = 'was not peer-reviewed'\n\n        # \"PMC does not include any non peer-reviewed research articles.\"\n        peer_review_terms = {\n            'peer-reviewed', \n            'peer reviewed', \n            'peer review', \n            'pubmed', \n            'ncbi', \n            'pmc',\n        }\n\n        if non_peer_reviewed_clause in text:\n            return False\n        elif is_match(text, peer_review_terms):\n            return True\n\n        return None\n\n    @staticmethod\n    def extract_number_of_studies(text):\n        '''Extract the number of studies performed in article research.\n\n        Does so by searching for the term 'studies' and returning the numeric \n        value right before it.\n        \n        Parameters\n        ----------\n        text : str\n\n        Returns\n        -------\n        Optional[int]\n            Returns None if no match found.\n        '''\n        pattern = r'(?:([0-9])[a-zA-Z ]{0,5}(?:studies))'\n\n        m = re.search(pattern, text)\n        if m:\n            return int(m.group(1))\n\n    @staticmethod\n    def extract_sample_size(text):\n        '''Extract the sample size of the article research.\n\n        Does so by searching for the term 'sample size of' and returning the \n        numeric value right after it.\n        \n        Parameters\n        ----------\n        text : str\n\n        Returns\n        -------\n        Optional[int]\n            Returns None if no match found.\n        '''\n        pattern1 = r'(?:total sample size of( about| over)?)(.[0-9,]+)'\n        pattern2 = r'(?:sample size of( about| over)?)(.[0-9,]+)'\n        pattern3 = r'(.[0-9,]+)(.{,14})(?: patients| participants)'\n\n        m1 = re.search(pattern1, text)\n        m2 = re.search(pattern2, text)\n        m3 = re.search(pattern3, text)\n        value = None\n        if m1:\n            value = m1.group(2).replace(',', '')\n        elif m2:\n            value = m2.group(2).replace(',', '')\n        elif m3:\n            value = m3.group(1).replace(',', '')\n\n        # 'SARS-2 patients' returns -2\n        # 'COVID-19 patients' returns -19\n        # added to prevent this\n        if value:\n            try:\n                if int(value) > 0:\n                    return int(value)\n            except:\n                return None\n\n    @staticmethod\n    def extract_study_design(text):\n        '''Extracts the type of study design in paper.\n        \n        Parameters\n        ----------\n        text : str\n        \n        Returns\n        -------\n        List[str]\n        '''\n        study_designs = [\n            'case control',\n            'case study',\n            'cross sectional',\n            'cross-sectional',\n            'descriptive study',\n            'ecological regression',\n            'experimental study',\n            'meta-analysis',\n            'non-randomized',\n            'non-randomized experimental study',\n            'observational study',\n            'prospective case-control',\n            'prospective cohort',\n            'prospective study',\n            'randomized',\n            'randomized experimental study',\n            'retrospective cohort',\n            'retrospective study',\n            'simulation', \n            'systematic review',\n            'time series analysis',    \n        ]\n        \n        return [design for design in study_designs if design in text]\n","91bc4ac4":"class ResultsHTMLText():\n    '''Object for storing search results in HTML text template format.''' \n\n    SPACES = '&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp;'\n    ARTICLE_LINK_HEADER = '<a href=\"{}\"> <i>{}<\/i><\/a><br>'\n    ARTICLE_HEADER = '<i>{}<\/i> <br>'\n\n    HIGHLIGHT = '<span style=\"background-color:#FFB6C1\">{}<\/span>'\n\n    def __init__(self, results_info):\n        '''Initizialize `ResrultsHTMLText` object.\n        \n        Parameters\n        ----------\n        results_info : List[SearchResult]\n        '''\n        self.results = results_info\n        self.results_text = ''\n\n    def print_peer_review_status(self, peer_reviewed):\n        '''Add information to on whether the paper had been peer-reviewed.\n\n        Parameters\n        ----------\n        peer_reviewed : bool\n        '''\n        if peer_reviewed:\n            self.results_text += (\n                '&#9830; This paper has been peer-reviewed.<br>'\n            )\n        elif peer_reviewed is False:\n            self.results_text += (\n                '&#9830; This paper has NOT been peer-reviewed.<br>'\n            )\n\n    def print_num_studies_info(self, num_studies):\n        '''Add information on the number of studies in the research.\n\n        Parameters\n        ----------\n        num_studies : Union[int, str]\n        '''\n        if num_studies:\n            self.results_text += (\n                f'&#9830; number of studies: {num_studies}<br>'\n            )\n    \n    def print_sample_size_info(self, sample_size):\n        '''Add information on the sample size of the paper study.\n\n        Parameters\n        ----------\n        sample_size : Union[int, str]\n        '''\n        if sample_size:\n            self.results_text += f'&#9830; sample size: {sample_size}<br>'\n\n    def print_study_design_info(self, design):\n        '''Add information of the study design type.\n\n        Parameters\n        ----------\n        design : List[str]\n        '''\n        if design:\n            self.results_text += (\n                f\"&#9830; study design: {', '.join(design)}<br>\"\n            )\n\n    def get_results_text(self):\n        '''Get results in HTML template format.\n\n        Returns\n        -------\n        str\n        '''\n        if not self.results:\n            return self.get_no_search_results_found_text()\n\n        self.results_text += '<br>'\n        for result in self.results:\n            if result.main_points:  \n                if isinstance(result.title, float):\n                    result.title = 'Title Unknown'\n\n                if isinstance(result.url, float):\n                    self.results_text += self.ARTICLE_HEADER.format(\n                        result.title\n                    )\n                else:\n                    self.results_text += self.ARTICLE_LINK_HEADER.format(\n                        result.url, result.title\n                    )\n                \n                info = result.study_info\n                self.print_peer_review_status(info.peer_reviewed)\n                # self.print_num_studies_info(info.num_studies)\n                self.print_sample_size_info(info.sample_size)\n                self.print_study_design_info(info.study_designs)\n                self.add_article_mainpoints_text(result) \n                self.results_text += '<br>'    \n\n        return self.results_text\n\n    def add_article_mainpoints_text(self, result):\n        '''Return text of main points within the article, in bullet format.\n        \n        Parameters\n        ----------\n        search_result : SearchResult\n        '''\n        self.results_text += '<p>'\n        for point in result.main_points:    \n            words = [\n                ResultsHTMLText.HIGHLIGHT.format(word) \n                if word.replace(',', '') in result.keywords else word \n                for word in point.split()\n            ]\n\n            point = ' '.join(words)\n            self.results_text += '{} -- {} <br><br>'.format(self.SPACES, point)\n    \n        self.results_text += '<\/p>'\n\n    def get_no_search_results_found_text(self):\n        '''Return text informing user no results were found.'''\n        return (\n            'No results found -- It appears not a lot of scientific research '\n            'has been done in this area.'\n        )\n","f6998d22":"class ResultsDataFrame():\n    '''Object for storing search results as pandas.DataFrame.''' \n\n    SPACES = '&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp;'\n    ARTICLE_LINK_HEADER = '<a href=\"{}\"> <i>{}<\/i><\/a><br>'\n    ARTICLE_HEADER = '<i>{}<\/i> <br>'\n\n    HIGHLIGHT = '<span style=\"background-color:#FFB6C1\">{}<\/span>'\n\n    COLS = [\n        'title', 'is_peer_reviewed', 'sample_size', 'study design', 'main_points'\n    ]\n    \n    def __init__(self, results_info):\n        '''Initizialize `ResrultsHTMLText` object.\n        \n        Parameters\n        ----------\n        results_info : List[SearchResult]\n        '''\n        self.results = results_info\n        self.results_df = pd.DataFrame(columns=self.COLS) \n\n    def get_results_df(self):\n        '''Get results in pandas.DataFrame.\n\n        Returns\n        -------\n        pandas.DataFrame\n        '''\n        for result in self.results:\n            \n            row = []\n            if result.main_points:  \n                if isinstance(result.title, float):\n                    result.title = 'Title Unknown'\n\n                if isinstance(result.url, float):\n                    row.append(self.ARTICLE_HEADER.format(result.title))\n                else:\n                    row.append(\n                        self.ARTICLE_LINK_HEADER.format(\n                                result.url, result.title\n                        )\n                    )\n                \n                info = result.study_info\n                row.append(info.peer_reviewed)\n                # row.append(info.num_studies)\n                row.append(info.sample_size)\n                row.append(', '.join(info.study_designs))\n                row.append(self.get_article_mainpoints_text(result))\n            \n            row_df = pd.DataFrame([row], columns=self.COLS)\n            self.results_df = pd.concat([self.results_df, row_df], ignore_index=True)\n\n        return self.results_df\n\n    def get_article_mainpoints_text(self, result):\n        '''Return text of main points within the article, in bullet format.\n        \n        Parameters\n        ----------\n        search_result : SearchResult\n        \n        Returns\n        -------\n        str\n        '''\n        text = ''\n        for point in result.main_points:    \n            words = [\n                self.HIGHLIGHT.format(word) \n                # if word.replace(',', '') in result.keywords else word \n                if Summary.is_decimal_value_in_text(word) else word\n                for word in point.split()\n            ]\n\n            point = ' '.join(words)\n            text += '{} -- {} <br><br>'.format(self.SPACES, point)\n        return text\n","13d616b9":"import pandas as pd\n\n\nSEARCH_SCORE_THRESHOLD = 10\n\n\ndef get_n(bm25_model, keywords):\n    '''Get number of articles that pass threshold.\n\n    NOTE: counts only the articles in the `TOP_N_ARTICLES`.\n\n    Parameters\n    ----------\n    bm25_model : rank_bm25.BM25Okapi\n        Ranking\/scoring model trained on corpus dataset.\n    keywords : List[str]\n        Search query keywords.\n\n    Returns\n    -------\n    int\n        Number of similarity scores of `top_n` articles that pass threshold.\n    '''\n    return len(\n        [score for score in sorted(\n            bm25_model.get_scores(keywords), reverse=True\n        )[:TOP_N_ARTICLES] if score > SEARCH_SCORE_THRESHOLD]\n    )\n\n\ndef get_search_results(search):\n    '''Get search results of search query.\n\n    Parameters\n    ----------\n    search : SearchQuery\n\n    Returns\n    -------\n    List[SearchResult]\n    '''\n    data = pd.read_csv(ARTICLE_INFO_PATH)\n    \n    all_keywords = search.query_keywords + search.related_keywords\n    \n    bm25_model = load_model(BM25_MODEL_PATH)\n    num_pass_threshold = get_n(bm25_model, all_keywords)\n\n    if num_pass_threshold == 0:\n        return []\n\n    results_text = bm25_model.get_top_n(\n        all_keywords, data['text'], n=num_pass_threshold\n    )\n    results_title = bm25_model.get_top_n(\n        all_keywords, data['title_meta'], n=num_pass_threshold\n    )\n    \n    results_url = bm25_model.get_top_n(\n        all_keywords, data['url'], n=num_pass_threshold\n    )\n    \n    return [\n        SearchResult(\n            title, text, url, all_keywords\n        ) for title, text, url in zip(results_title, results_text, results_url)\n    ]\n","b0f24f7c":"def get_query_results(query):\n    '''Get results of search query.\n    \n    Parameters\n    ----------\n    query : str\n    \n    Returns\n    -------\n    ResultsHTMLText\n    '''\n    search_query = SearchQuery(query)\n    \n    keywords = search_query.query_keywords + search_query.related_keywords\n    display(HTML(f'<h3>Search Terms: {\", \".join(keywords)}<\/h3>'))\n\n    results = get_search_results(search_query)\n    # return ResultsHTMLText(results).get_results_text()\n    return ResultsDataFrame(results).get_results_df()\n","a4201ef5":"from IPython.core.display import display, HTML\n\ndef print_answers(queries):\n    '''Print search results to each query.\n    \n    Parameters\n    ----------\n    tasks : List[str]\n    '''\n    for query in queries:\n        display(HTML(f'<h2>{query} \\n<\/h2>'))\n        final_result = get_query_results(query)\n\n        # display(HTML(final_result))\n        display(final_result.style)\n","32476eb1":"%%HTML\n<style type=\"text\/css\">\nh2 {\n     background-color: steelblue; \n     color: white; \n     padding: 8px; \n     padding-right: 30px; \n     font-size: 24px; \n     max-width: 1500px; \n     margin-top: 10px;\n     margin-bottom:4px;\n }\nh3 {\n     background-color: skyblue; \n     color: black; \n     padding: 8px; \n     padding-right: 30px; \n     font-size: 20px; \n     max-width: 1500px; \n     margin-top: 4px;\n     margin-bottom:4px;\n }\n<\/style>","ea50757e":"queries = [\n    'Efforts to develop prophylaxis clinical studies and prioritize in healthcare workers',\n    'prophylaxis clinical studies',\n]\n\nprint_answers(queries)","14b3e116":"%matplotlib inline\nimport numpy as np\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom collections import Counter\n\nplt.style.use('fivethirtyeight')\n\n\ndef label_fig_xaxis(labels, width=0):\n    '''Label x-axis.'''\n    x_range = np.arange(len(labels))\n    plt.xticks(x_range + width, labels, rotation='vertical', fontsize=10)\n    plt.xlim([x_range.min() - 0.5, x_range.max() + 0.5 + width])\n    \n\ndef plot_bar_graph(x, y):\n    '''Plot `matplotlib.pyplot` bar graph.'''\n    x_order = range(len(x))\n    plt.bar(x_order, y, alpha=0.8)\n    label_fig_xaxis(x)\n\n\ndef get_term_related_corpus(keyterms):\n    '''Get all data with any term in `keyterms` appearing in the text.\n    \n    Parameters\n    ----------\n    keyterms : List[str]\n    \n    Returns\n    -------\n    str\n    '''\n    data = pd.read_csv(ARTICLE_INFO_PATH)\n    \n    articles = data[data['text'].apply(lambda x: is_match(x, keyterms))]\n    \n    corpus = ''\n    for article in articles['text']:\n        corpus += article\n    \n    return clean_text(corpus)\n\n\ndef get_word_counts(corpus, words, min_freq=0):\n    '''Get counts of all `words` in `corpus`.\n    \n    Parameters\n    ----------\n    corpus : str\n    words : List[str]\n    \n    Returns\n    -------\n    Dict[str, int]\n    '''\n    return {\n        word: count for word, count in sorted(\n            Counter(corpus.split()).items(), key=lambda x: x[1], reverse=True\n        ) if (word in words) and (count > min_freq)\n    }\n","7e4cec8e":"queries = [\n    'Range of incubation periods for the disease in humans', \n    'how incubation varies across age and health status', \n    'how long individuals are contagious, even after recovery.',\n    'how long individuals are contagious',\n    'Prevalence of asymptomatic shedding and transmission (e.g., particularly children).',\n    'Seasonality of transmission.',\n    'temperature and humidity',\n    'Physical science of the coronavirus', \n    'charge distribution', \n    'adhesion to hydrophilic\/phobic surfaces', \n    'environmental survival to inform decontamination efforts for affected areas and provide information about viral shedding.',\n    'Persistence and stability on a multitude of substrates and sources (e.g., nasal discharge, sputum, urine, fecal matter, blood).',\n    'Persistence of virus on surfaces of different materials (e,g., copper, stainless steel, plastic).',\n    'Natural history of the virus and shedding of it from an infected person',\n    'Implementation of diagnostics and products to improve clinical processes',\n    'Disease models, including animal models for infection, disease and transmission',\n    'Tools and studies to monitor phenotypic change and potential adaptation of the virus',\n    'Immune response and immunity',\n    'Effectiveness of movement control strategies to prevent secondary transmission in health care and community settings',\n    'Effectiveness of personal protective equipment (PPE) and its usefulness to reduce risk of transmission in health care and community settings',\n    'Role of the environment in transmission',\n]\n\nprint_answers(queries)","6c9dc80a":"DISEASE_TERMS = [\n    'age', \n    'asthma', \n    'asthmatics',\n    'bladder',\n    'bone',\n    'brain',\n    'cancer', \n    'cardiovascular',  \n    'chemotherapy', \n    'children', \n    'colon', \n    'coronary', \n    'cholangiohepatitis', \n    'diabetes',\n    'emphysema', \n    'esophageal',\n    'gastric', \n    'gastrointestinal', \n    'hepatitis',\n    'hiv',\n    'hiv-infected',\n    'hormonal', \n    'hyperplasia',\n    'hyperthyroidism',\n    'hypoglycemia',\n    'hypothyroidism',\n    'immunocompromised', \n    'infants', \n    'intestinal',\n    'kidney',\n    'lesions',\n    'liver',\n    'lung',\n    'lymphadenopathy', \n    'lymphoma',\n    'malnutirion', \n    'myocardia', \n    'neoplasia', \n    'neurological', \n    'neutropenia', \n    'neutrophils',\n    'obesity', \n    'older',\n    'oropharyngeal',\n    'pcos',\n    'pregnancy', \n    'pregnant',\n    'pulmonary',\n    'smoking', \n    'smokers',\n    'urinary',\n    'vascular', \n    'vessels',  \n    'younger',\n]","16b65016":"RISK_TERMS = [\n    'risk factor', \n    'risk factors', \n    'at-risk', \n    'high risk individuals', \n    'vulnerable'\n]\n\nrisk_corpus = get_term_related_corpus(RISK_TERMS)\nword_counts = get_word_counts(risk_corpus, DISEASE_TERMS)\n\nfig = plt.figure(figsize=(7,5))\nplot_bar_graph(word_counts.keys(), word_counts.values())\nplt.ylabel('Term Frequency')\nplt.xlabel('Illness-Related Term')\nfig.patch.set_facecolor('white')\nplt.show()\n","4bca00a6":"queries = [\n    'Data on potential risks factors',\n    'Smoking, pre-existing pulmonary disease',\n    'Co-infections (whether co-existing respiratory\/viral infections make the virus more transmissible or virulent) and other co-morbidities',\n    'Neonates and pregnant women',\n    'Socio-economic and behavioral factors to understand the economic impact of the virus and whether there were differences.',\n    'Transmission dynamics of the virus, including the basic reproductive number, incubation period, serial interval, modes of transmission and environmental factors',\n    'Severity of disease, including risk of fatality among symptomatic hospitalized patients, and high-risk patient groups',\n    'Susceptibility of populations',\n    'Public health mitigation measures that could be effective for control',\n]\n\nprint_answers(queries)","fa08c4a2":"queries = [\n    'Real-time tracking of whole genomes and a mechanism for coordinating the rapid dissemination of that information to inform the development of diagnostics and therapeutics and to track variations of the virus over time.',\n    'Access to geographic and temporal diverse sample sets to understand geographic distribution and genomic differences, and determine whether there is more than one strain in circulation.',\n    'Multi-lateral agreements such as the Nagoya Protocol.',\n    'Evidence that livestock could be infected',\n    'livestock (e.g., field surveillance, genetic sequencing, receptor binding) and serve as a reservoir after the epidemic appears to be over.',\n    'Evidence of whether farmers are infected, and whether farmers could have played a role in the origin.',\n    'Surveillance of mixed wildlife- livestock farms for SARS-CoV-2 and other coronaviruses in Southeast Asia.',\n    'Experimental infections to test host range for this pathogen.',\n    'Animal host(s) and any evidence of continued spill-over to humans',\n    'Socioeconomic and behavioral risk factors for this spill-over',\n    'Sustainable risk reduction strategies',\n]\n\nprint_answers(queries)","edc05d98":"queries = [\n    'Drugs being developed and tried to treat COVID-19 patients.',\n    'Clinical and bench trials to investigate less common viral inhibitors against COVID-19 such as naproxen, clarithromycin, and minocyclinethat that may exert effects on viral replication.',\n    'Methods evaluating potential complication of Antibody-Dependent Enhancement (ADE) in vaccine recipients.',\n    'Antibody-Dependent Enhancement (ADE)',\n    'Exploration of use of best animal models and their predictive value for a human vaccine.',\n    'Capabilities to discover a therapeutic for the disease, and clinical effectiveness studies to discover therapeutics, to include antiviral agents.',\n    'Alternative models to aid decision makers in determining how to prioritize and distribute scarce, newly proven therapeutics as production ramps up. This could include identifying approaches for expanding production capacity to ensure equitable and timely distribution to populations in need.',\n    'Efforts targeted at a universal coronavirus vaccine.',\n    'Efforts to develop animal models and standardize challenge studies',\n    'Efforts to develop prophylaxis clinical studies and prioritize in healthcare workers',\n    'prophylaxis clinical studies',\n    'Approaches to evaluate risk for enhanced disease after vaccination',\n    'Assays to evaluate vaccine immune response and process development for vaccines, alongside suitable animal models [in conjunction with therapeutics',\n]\n\nprint_answers(queries)","59b157b9":"queries = [\n    'Guidance on ways to scale up NPIs in a more coordinated way (e.g., establish funding, infrastructure and authorities to support real time, authoritative (qualified participants) collaboration with all states to gain consensus on consistent guidance and to mobilize resources to geographic areas where critical shortfalls are identified) to give us time to enhance our health care delivery system capacity to respond to an increase in cases.',\n    'Rapid design and execution of experiments to examine and compare NPIs currently being implemented. DHS Centers for Excellence could potentially be leveraged to conduct these experiments.',\n    'Rapid assessment of the likely efficacy of school closures, travel bans, bans on mass gatherings of various sizes, and other social distancing approaches.',\n    'Methods to control the spread in communities, barriers to compliance and how these vary among different populations..',\n    'Models of potential interventions to predict costs and benefits that take account of such factors as race, income, disability, age, geographic location, immigration status, housing status, employment status, and health insurance status.',\n    'Policy changes necessary to enable the compliance of individuals with limited resources and the underserved with NPIs.',\n    'Research on why people fail to comply with public health advice, even if they want to do so (e.g., social or financial costs may be too high).',\n    'Research on the economic impact of this or any pandemic. This would include identifying policy and programmatic alternatives that lessen\/mitigate risks to critical government services, food distribution and supplies, access to critical household supplies, and access to health diagnoses, treatment, and needed care, regardless of ability to pay.]',\n]\nprint_answers(queries)","6ce645b5":"queries = [\n    'Resources to support skilled nursing facilities and long term care facilities.',\n    'Mobilization of surge medical staff to address shortages in overwhelmed communities',\n    'Age-adjusted mortality data for Acute Respiratory Distress Syndrome (ARDS) with\/without other organ failure \u2013 particularly for viral etiologies',\n    'Extracorporeal membrane oxygenation (ECMO) outcomes data of COVID-19 patients',\n    'Outcomes data for COVID-19 after mechanical ventilation adjusted for age.',\n    'Knowledge of the frequency, manifestations, and course of extrapulmonary manifestations of COVID-19, including, but not limited to, possible cardiomyopathy and cardiac arrest.',\n    'Application of regulatory standards (e.g., EUA, CLIA) and ability to adapt care to crisis standards of care level.',\n    'Approaches for encouraging and facilitating the production of elastomeric respirators, which can save thousands of N95 masks.',\n    'elastomeric respirators',\n    'Best telemedicine practices, barriers and faciitators, and specific actions to remove\/expand them within and across state boundaries.',\n    'Guidance on the simple things people can do at home to take care of sick people and manage disease.',\n    'Oral medications that might potentially work.',\n    'Oral medications',\n    'Use of AI in real-time health care delivery to evaluate interventions, risk factors, and outcomes in a way that could not be done manually.',\n    'Best practices and critical challenges and innovative solutions and technologies in hospital flow and organization, workforce protection, workforce allocation, community-based support resources, payment, and supply chain management to enhance capacity, efficiency, and outcomes.',\n    'Efforts to define the natural history of disease to inform clinical care, public health interventions, infection prevention control, transmission, and clinical trials',\n    'Efforts to develop a core clinical outcome set to maximize usability of data across a range of trials',\n    'Efforts to determine adjunctive and supportive interventions that can improve the clinical outcomes of infected patients (e.g. steroids, high flow oxygen)',\n]\n    \nprint_answers(queries)","f764b21e":"queries = [\n    'How widespread current exposure is to be able to make immediate policy recommendations on mitigation measures. Denominators for testing and a mechanism for rapidly sharing that information, including demographics, to the extent possible. Sampling methods to determine asymptomatic disease (e.g., use of serosurveys (such as convalescent samples) and early detection of disease (e.g., use of screening of neutralizing antibodies such as ELISAs).',\n    'Efforts to increase capacity on existing diagnostic platforms and tap into existing surveillance platforms.',\n    'Recruitment, support, and coordination of local expertise and capacity (public, private\u2014commercial, and non-profit, including academic), including legal, ethical, communications, and operational issues.',\n    'National guidance and guidelines about best practices to states (e.g., how states might leverage universities and private laboratories for testing purposes, communications to public health officials and the public).',\n    'Development of a point-of-care test (like a rapid influenza test) and rapid bed-side tests, recognizing the tradeoffs between speed, accessibility, and accuracy.',\n    'Rapid design and execution of targeted surveillance experiments calling for all potential testers using PCR in a defined area to start testing and report to a specific entity. These experiments could aid in collecting longitudinal samples, which are critical to understanding the impact of ad hoc local interventions (which also need to be recorded).',\n    'Separation of assay development issues from instruments, and the role of the private sector to help quickly migrate assays onto those devices.',\n    'Efforts to track the evolution of the virus (i.e., genetic drift or mutations) and avoid locking into specific reagents and surveillance\/detection schemes.',\n    'Latency issues and when there is sufficient viral load to detect the pathogen, and understanding of what is needed in terms of biological and environmental sampling.',\n    'Use of diagnostics such as host response markers (e.g., cytokines) to detect early disease or predict severe disease progression, which would be important to understanding best clinical practice and efficacy of therapeutic interventions.',\n    'Policies and protocols for screening and testing.',\n    'Policies to mitigate the effects on supplies associated with mass testing, including swabs and reagents.',\n    'Technology roadmap for diagnostics.',\n    'Barriers to developing and scaling up new diagnostic tests (e.g., market forces), how future coalition and accelerator models (e.g., Coalition for Epidemic Preparedness Innovations) could provide critical funding for diagnostics, and opportunities for a streamlined regulatory environment.',\n    'New platforms and technology (e.g., CRISPR) to improve response times and employ more holistic approaches to COVID-19 and future diseases.',\n    'Coupling genomics and diagnostic testing on a large scale.',\n    'Enhance capabilities for rapid sequencing and bioinformatics to target regions of the genome that will allow specificity for a particular variant.',\n    'Enhance capacity (people, technology, data) for sequencing with advanced analytics for unknown pathogens, and explore capabilities for distinguishing naturally-occurring pathogens from intentional.',\n    'One Health surveillance of humans and potential sources of future spillover or ongoing exposure for this organism and future pathogens, including both evolutionary hosts (e.g., bats) and transmission hosts (e.g., heavily trafficked and farmed wildlife and domestic food and companion species), inclusive of environmental, demographic, and occupational risk factors.',\n]\n\nprint_answers(queries)","6c9c4661":"import pycountry\n\nCOUNTRIES = [country.name for country in pycountry.countries]\n\ncorpus = get_data_corpus()\nword_counts = get_word_counts(corpus, COUNTRIES, min_freq=30)\n\nfig = plt.figure(figsize=(7,5))\nplot_bar_graph(word_counts.keys(), word_counts.values())\nplt.ylabel('Country Frequency')\nplt.xlabel('Country')\nfig.patch.set_facecolor('white')\nplt.show()","c1ffa64f":"queries = [\n    'Are there geographic variations in the rate of COVID-19 spread?',\n    'Are there geographic variations in the mortality rate of COVID-19?',\n    'Is there any evidence to suggest geographic based virus mutations?',\n]\n\nprint_answers(queries)","79a857e1":"queries = [\n    'Methods for coordinating data-gathering with standardized nomenclature.',\n    'Sharing response information among planners, providers, and others.',\n    'Understanding and mitigating barriers to information-sharing.',\n    'How to recruit, support, and coordinate local (non-Federal) expertise and capacity relevant to public health emergency response (public, private, commercial and non-profit, including academic).',\n    'Integration of federal\/state\/local public health surveillance systems.',\n    'Value of investments in baseline public health response infrastructure preparedness',\n    'Modes of communicating with target high-risk populations (elderly, health care workers).',\n    'Risk communication and guidelines that are easy to understand and follow (include targeting at risk populations\u2019 families too).',\n    'Communication that indicates potential risk of disease to all population groups.',\n    'Misunderstanding around containment and mitigation.',\n    'Action plan to mitigate gaps and problems of inequity in the Nation\u2019s public health capability, capacity, and funding to ensure all citizens in need are supported and can access information, surveillance, and treatment.',\n    'Measures to reach marginalized and disadvantaged populations.',\n    'Data systems and research priorities and agendas incorporate attention to the needs and circumstances of disadvantaged populations and underrepresented minorities.',\n    'Mitigating threats to incarcerated people from COVID-19, assuring access to information, prevention, diagnosis, and treatment.',\n    'Understanding coverage policies (barriers and opportunities) related to testing, treatment, and care',\n]\n\nprint_answers(queries)","7c5a520b":"queries = [\n    'Efforts to articulate and translate existing ethical principles and standards to salient issues in COVID-2019',\n    'Efforts to embed ethics across all thematic areas, engage with novel ethical issues that arise and coordinate to minimize duplication of oversight',\n    'Efforts to support sustained education, access, and capacity building in the area of ethics',\n    'Efforts to establish a team at WHO that will be integrated within multidisciplinary research and operational platforms and that will connect with existing and expanded global networks of social sciences.',\n    'Efforts to develop qualitative assessment frameworks to systematically collect information related to local barriers and enablers for the uptake and adherence to public health measures for prevention and control. This includes the rapid identification of the secondary impacts of these measures. (e.g. use of surgical masks, modification of health seeking behaviors for SRH, school closures)',\n    'Efforts to identify how the burden of responding to the outbreak and implementing public health measures affects the physical and psychological health of those providing care for Covid-19 patients and identify the immediate needs that must be addressed.',\n    'Efforts to identify the underlying drivers of fear, anxiety and stigma that fuel misinformation and rumor, particularly through social media.', \n]\n\nprint_answers(queries)","5fc50528":"<a id=\"utils\"><\/a>\n# utils functions","57e1c04b":"# TASK 2: EDA - top most mentioned illnesses in COVID-19-related research papers.\n\n### What do we know about COVID-19 risk factors?\n\nBelow I plot the top disease\/illness-related terms used in risk-context articles.\n\nNote: a term being used frequently in correlation to COVID-19 risk does not necessarily mean that illness is higly correlated with risk; it may be just that more research was done in that area than with other illnesses.\n\n\ud83d\udccc Note II: (I included 'age', 'children', 'younger' to the list of `illness_terms` -- this was done for simplicity and because age may be comorbid with certain illnesses, not because I believe the 'children' are a disease :) )","74506c42":"<a id=\"htmltext\"><\/a>\n# ResultsHTMLText\n\nObject that stores search results in HTML text template format. ","010fdc57":"<a id=\"task9\"><\/a>\n# TASK 9: What has been published about information sharing and inter-sectoral collaboration?\n\nWhat has been published about information sharing and inter-sectoral collaboration? What has been published about data standards and nomenclature? What has been published about governmental public health? What do we know about risk communication? What has been published about communicating with high-risk populations? What has been published to clarify community measures? What has been published about equity considerations and problems of inequity?","894f4488":"# Constants","d4afcce1":"<a id=\"rank25\"><\/a>\n# Train Okapi Rank25 Model.\n\nThis is the scoring\/ranking similarity model I used to rank the articles by relevancy.\n\nIt works by calculating how frequently query keywords appear in a document compared to how frequently they appear in the corpus in general. For example, for the query 'what is the effect of seasonality on coronavirus', it will detect that 'seasonality' is a more relevant keyword and should be weighed more compared to terms such as 'effect' or 'coronavirus', which are likely to appear more frequently in general. Note that stop words are removed before inputted into the model, so 'is' and 'the' will be ignored.  \n\nThe formula it uses is:\n\n![](https:\/\/wikimedia.org\/api\/rest_v1\/media\/math\/render\/svg\/43e5c609557364f7836b6b2f4cd8ea41deb86a96)\n\nwhere `D` refers to the document, and `q` to the query keyword (and f(q, D) is the frequency the query term `q` appears in the document `D`).\n\nThe IDF (Inverse Document Frequency) formula is:\n\n![](https:\/\/wikimedia.org\/api\/rest_v1\/media\/math\/render\/svg\/c652b6871ce4872c8e924ff0f806bc8b06dc94ed)\n\nwhere `N` is the total number of documents in the corpus, and `n(q)` is the number of documents containing the query `q`.","4da77eec":"<a id=\"searchresult\"><\/a>\n# 'SearchResult' object\n\nObject that stores article title and top main points.\n","158f9f19":"<a id=\"task8\"><\/a>\n# TASK 8 \n\nAt the time of writing, COVID-19 has spread to at least 114 countries. With viral flu, there are often geographic variations in how the disease will spread and if there are different variations of the virus in different areas. We\u2019d like to explore what the literature and data say about this through this Task.","085a8c80":"# 'ResultsDataFrame' object\n\nObject for storing search results in pandas DataFrame.","9cc3731e":"# EDA utils functions","0974b754":"# Table of Contents\n\n#### Creating Search Engine\n- [`Article` object](#article)\n- [utils functions](#utils)\n- [data processing](#processing)\n- [constants (stopwords)](#constants)\n- [utils functions part 2](#utils2)\n- [creating CBOW `Word2Vec` model](#word2vec)\n- [creating `Okapi Rank25` model](#rank25)\n- [`SearchQuery` object](#searchquery)\n- [`Summary` object](#summary)\n- [`SearchResult` object](#searchresult)\n- [`StudyInfo` object](#study)\n- [`ResultsHTMLText` object](#htmltext)\n- [functions to extract search result](#main)\n\n#### Testing Tasks\n- [Task 1](#task1)\n- [Task 2](#task2)\n- [Task 3](#task3)\n- [Task 4](#task4)\n- [Task 5](#task5)\n- [Task 6](#task6)\n- [Task 7](#task7)\n- [Task 8](#task8)\n- [Task 9](#task9)\n- [Task 10](#task10)","0260913a":"<a id=\"processing\"><\/a>\n# PROCESSING THE QUERY\n\n## DETECTING RELEVANT TERMS IN THE QUERY\n\nOne of the first tasks is having the search tool recognize and detect the relevant words in a search query. For example, in the following query:\n\n\"what is the effect of seasonality and weather on coronavirus?\",\n\nwe would like the search tool to pick out \u2018seasonality\u2019 and \u2018weather\u2019 as the relevant terms to search for, as opposed to searching for \u2018what\u2019 and \u2018is\u2019, which are unlikely to lead to salient responses.\n\nTo do this, I identified the stop words within the corpus by investigating the frequency the terms appear within all the articles. Words that appeared a large amount of times were disregarded, as they were less likely to have strong relevance to the ideal search result (see: [identifying corpus-specific stop words](#stopwords)). I also included a list of general stop words from the `nltk` library.  \n\nAdditionally, we would like the search to detect similar words related to the terms used in the search. For example, when searching for \u2018weather\u2019, we would like the algorithm to also include \u2018climate\u2019 and \u2018temperature\u2019 as potentially similar words. To do this, I trained the corpus of articles to a Word2Vec model.\n\nWords that had a high similarity score (>0.62) were included in the keywords the algorithm was searching for when iterating over the articles (see: `init_related_keywords` function in [SearchQuery object](#searchquery)).","57a02fb9":"### install `rank_bm25` library\n\nNOTE: turn internet on in Settings first.","501d1e51":"# ABOUT NOVEL 2019 CORONAVIRUS PANDEMIC\n\nThe COVID-19 outbreak was defined as a pandemic by WHO (World Health Organization) on March 11, 2020, about 3 months after it was first identified in Wuhan, Hubei, China. \n\nCommon symptoms of the disease include fever, cough, and shortness of breath, with complications including pneumonia and acute respiratory distress syndrome. There is currently no known vaccine.\n\n![image.png](attachment:image.png)","285a3b50":"<a id=\"task6\"><\/a>\n# TASK 6 : What has been published about medical care?\n\nWhat has been published about medical care? What has been published concerning surge capacity and nursing homes? What has been published concerning efforts to inform allocation of scarce resources? What do we know about personal protective equipment? What has been published concerning alternative methods to advise on disease management? What has been published concerning processes of care? What do we know about the clinical characterization and management of the virus?","d749a821":"<a id=\"task5\"><\/a>\n# TASK 5: What do we know about non-pharmaceutical interventions?","00145948":"<a id=\"study\"><\/a>\n# 'StudyInfo' object\n\nObject that extracts the level of evidence for the findings in the article.\n\nCurrently, it does this by checking if the paper is peer-reviewed. In process of adding more checks.\n\n### Reasoning behind some of the code decision\n\nWhen checking the accuracy of the `extract_sample_size_info`, I came across a couple of issues. For the example 'We aimed to obtain a minimum sample size of 20 participants from each of the three provinces, for a total sample size of over 60 participants', the function was initially capturing a sample size of 20 when the true value was at 60. I updated the function appropriately so that in these cases the total sample size would be returned. ","cf0f9efb":"# This project has been implemented here: http:\/\/nameless-basin-49871.herokuapp.com\/corona_search\n\n# www.rismakov.com \n\n### This search engine algorithm doesn't just return a link to or a title of an article, but also returns the main points of the article and tries to extract the level of evidence that exists in the article (i.e. by observing whether peer-reviewed, number of studies, sample size, study design, etc). This allows the user to get an answer without needing to waste time reading and extracting the relevant information from within every single research paper. ","695b7d66":"<a id=\"main\"><\/a>\n\n# functions to extract search results\n\nNOTE: Only articles that pass a certain score threshold are returned. This was added into the code to ensure that queries that do not have relevant papers do not return poor results.\n\nWe want only relevant articles returned, or nothing at all.\n\n\ud83d\udccc **PROS**:\n- This ensures that poor results that do not answer the query well aren't surfaced.\n\n\ud83d\udccc **CONS**:\n- Difficult to gauge where to place the threshold. May be eliminating some valid responses in the process of removing bad responses (i.e. precision vs recall). \n- Also difficult to measure what is or isn't a 'good' response.","b90d9581":"# TASK 3: What do we know about virus genetics, origin, and evolution?","66a991d5":"<a id=\"constants\"><\/a>\n# CONSTANTS","0e5a7057":"China is the top most mentioned country in all of the COVID-19-related research papers. Given that the virus originated in Wuhan, China, this is not unexpected. ","49e29419":"# Testing\n## test whether more specific or more granular queries produce better results.","d453ba9c":"<a id=\"searchquery\"><\/a>\n# 'SearchQuery' object \n\nThis class takes in a query and processes it. Processing includes removing punctuation and stopwords, and finding similar keywords to the words within the query.\n\nWe would like the algorithm to detect similar words related to the terms used in the search. For example, when searching for \u2018weather\u2019, we would like the algorithm to also include \u2018climate\u2019 and \u2018temperature\u2019 as potentially similar words. To do this, I trained the corpus of articles to a gensim Word2Vec model.\n\nWords that had a high similarity score (>0.62) were included in the key words the algorithm was searching for when iterating over the articles (see: `init_related_keywords` function in `SearchQuery` object below).\n\ni.e. \"what is the effect of seasonality and weather on coronavirus?\" --> [\n    'seasonality', 'weather', 'climate', 'rainfall', \n    'anthropogenic', 'socio-economic', 'drought', 'meteorological',\n    'flooding', 'floods', 'seasonal', 'temporal', 'trends', \n    'climatic', 'pollution'\n] ","6316b5a4":"<a id=\"task2\"><\/a>\n# TASK 2 continued: What do we know about COVID-19 risk factors?","bbb7032c":"<a id=\"article\"><\/a>\n# 'Article' object\n\nObject for storing infomation about the research article, such as title, abstract, body text, etc. \n\nPerforms basic cleaning of the data, including removing URL links and artifacts from the text. ","51638284":"<a id=\"task1\"><\/a>\n# TASK 1: What is known about transmission, incubation, and environmental stability?","1b384ec6":"<a id=\"task4\"><\/a>\n# TASK 4: What do we know about vaccines and therapeutics?","c75d65f9":"<a id=\"word2vec\"><\/a>\n# Train gensim Word2Vec model.\n\nHere I train a `Word2Vec` model in order to be able to extract similar keywords as the words in the search query. For example, for the query 'effects of weather on coronavirus', we would like the algorithm to also return articles related to 'climate' or 'humidity', as well as specifically 'weather'.\n\n`Word2Vec` works by taking a large corpus of text and from it, producing a vector space. Each distinct word is embedded as a corresponding vector in the space. This allows us to see relationships betweens words and to observe which word vectors are positioned close together as a way of calculating word similarity.\n\n![](https:\/\/i0.wp.com\/migsena.com\/wp-content\/uploads\/2017\/09\/word2vec.png?zoom=2.625&w=660)\n\n\ud83d\udccc **PROS**:\n- Being able to detect related words will allow the algorithm to find relevant results that it may have passed on otherwise (i.e. detecting articles related to 'climate' when you searched for 'weather'). \n\n\ud83d\udccc **CONS**:\n- Currently, the search tool doesn't detect which term is the exact term the user typed in, and which term was been defined by the `Word2Vec` model as a similar word, and thus weighs them both the same. However, it may make more sense to weigh the exact term stronger (i.e. if you search for 'humidity', 'temperature' may be a similar word, but there are still clear differences between the two and the user may not be seeking to use them interchangably). ","b43c9857":"<a id=\"stopwords\"><\/a>\n### identifying corpus-specific stop words\n\n`STOPWORDS` were compiled by finding the words that appear with the highest counts within the corpus of text. Additionally, a few additional terms that were found to be less relevant were added in to the list of stopwords during testing.\n\n\ud83d\udccc **PROS**:\n- Specific stopwords associated with the research paper corpus (that may not be stopwords within other sets) are removed, which will increase the liklihood of the search engine not scanning for keywords that may be irrelevant for the task.\n\n\ud83d\udccc **CONS**:\n- This may potentially lead us to remove words that are in fact relevant, and removing them may result in relevant articles not showing up in the search results. While some words are obviously stopwords, it is hard to decide what the threshold is and to gauge on the relevancy of certain words. ","a526d9ce":"# CALL TO ACTION\n\nIn the wake of the COVID-19 pandemic, Kaggle has posted a call to action for Data Scientists and AI experts to help answer a vast list of research questions and unknowns related to the novel 2019 coronavirus. To help do so, a dataset of around 40,000 scholarly articles, compiled and prepared by the White House and a coalition of leading research groups, has been made openly available to the public. \n\nGiven the rapidly increasing number of literature on this subject, the need to gather as much information as possible and to make sense of all the available knowledge is made much more urgent. The ability to make this process more efficient would help limit the spread of the disease and help flatten the curve.\n\nAs a method to start answering these questions and tackling the tasks posted by Kaggle\u2019s call to action, I created a search bar tool that is able to extract the main points within the relevant articles to answer the question posed by the search query. This makes the process of gathering answers more efficient, as it limits the need to search through a vast array of articles and the need to sift through a lot of detailed information within the articles themselves.\n\nBelow I will detail how I set out to do this, as well as explain both the strengths and the limitations of the algorithm as it currently stands. ","4bee08d8":"Additionally, the number of relevant articles returned was capped at maximum `N`, to keep the responses short and sweet, and to not reintroduce the issue of users needing to sift through the results to pick ot what is and isn\u2019t relevant. \n\n\n\ud83d\udccc **PROS**:\n- This ensures that a user isn't oversaturated with too many results, as that may make it harder for them to extract an exact answer by having to dig through too much text. \n\n\ud83d\udccc **CONS**:\n- Doesn't display articles that are ranked lower, though may still be relevant.\n- If there is conflicting information between two studies, may potentially only display one of those papers, thereby limiting the full story. ","ba3608e5":"# Search Results\n","4b36d55c":"<a id=\"utils2\"><\/a>\n# utils functions (part II)","5ffd9761":"<a id=\"task7\"><\/a>\n# TASK 7: What do we know about diagnostics and surveillance?\n","6e9a9f93":"# TASK 8: EDA- top most mentioned countries in COVID-19-related research papers.","7baf5735":"<a id=\"summary\"><\/a>\n# 'Summary' object\n\nSummarizes text and outputs main points using `gensim.summarization.summarizer.summarize` function.  \n\n`summarize` works by detecting which sentences are the most important within the text.\n\nTo not oversaturate users with too much text and too many responses to the search query, the algorithm caps the number of points that are returned per article to maximum `N`. The tool decides which points are likely to be more salient by using `gensim`'s `summarize` method.\n\n![](https:\/\/cdn.analyticsvidhya.com\/wp-content\/uploads\/2018\/10\/block_3.png)\n\n\ud83d\udccc **PROS**:\n- Summary allows the user to get the main points without needing to read the entire research paper (much of which may be filled with less relevant information).\n\n\ud83d\udccc **CONS**:\n- Summarizing the text naturally removes alot of information (much of which may potentially be relevant).\n\n## Relevancy Filter\n\nAdded into the `Summary` object is filter that removes sentences where no keywords are present (even if they are ranked high with the `summarize` function). This is to ensure that the summary doesn't just return a high level summirization of the journal article, but that this summary is also relevant to the search query.\n\n\ud83d\udccc **PROS**:\n- This ensures that the user is receiving a summary relevant to his\/her search query. \n\n\ud83d\udccc **CONS**\n- This might lead to the summary eliminating relevant information to the research done if that information is less relevant to the search query. Just because it's less relevant to the exact query, doesn't necessarily mean it's not necessary to fully understand the results of the research article. \n\nChanged `relevancy_filter` to instead filter on sentences that include a numeric demical value in the text.\n\n\ud83d\udccc **PROS**:\n- The relevant results within research papers tend to be statistical values (percentages and p-values), so those will likely be the results we want returned and the results most likely to answer our query questions. \n\n\ud83d\udccc **CONS**\n- Some research is more anecdotal or relies on different type of measures that are not necessarily numeric. This filter might not return the best results in those cases. ","e3c19eb2":"<a id=\"task10\"><\/a>\n# TASK 10: What has been published about ethical and social science considerations?"}}