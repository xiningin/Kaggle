{"cell_type":{"aa820f98":"code","92db2386":"code","11b27e4d":"code","0d838a7e":"code","aec654af":"code","9242f7d6":"code","883aa37a":"code","aecc9910":"code","fc8178d2":"code","04db794d":"code","d452653e":"code","8214515d":"code","3c966d3c":"code","5b641b70":"code","970d7e10":"code","4f0bdd2e":"code","938c7924":"code","767a14b6":"code","7dcf4f95":"code","97a70969":"code","b1162ae8":"code","eccc03a5":"code","2f1ad159":"code","51cbe716":"code","871fdd37":"code","e7940521":"code","e4c84039":"code","124829d4":"code","6fdf5f70":"code","655a044d":"code","2674696b":"code","1556999d":"markdown","aba8358c":"markdown","90785199":"markdown","9d39b860":"markdown","0244228e":"markdown","c3078314":"markdown","09cada00":"markdown","e8306a94":"markdown","5ed6f0f2":"markdown","d85d5518":"markdown","4c452402":"markdown","c2f0c088":"markdown","0600ebb1":"markdown","6e208ac5":"markdown","a2f835db":"markdown","a25e44cf":"markdown","c5f13654":"markdown","94c2e49a":"markdown","cf4619b6":"markdown","5c87af55":"markdown","63eb521c":"markdown","4731c3f6":"markdown","c6a5688d":"markdown","585c0ea6":"markdown","f380fa55":"markdown","48561802":"markdown","e7a658f2":"markdown","84bc851c":"markdown"},"source":{"aa820f98":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nsns.set()\n\nimport warnings\nwarnings.filterwarnings('ignore')","92db2386":"train = pd.read_csv('\/kaggle\/input\/loan-prediction-problem-dataset\/train_u6lujuX_CVtuZ9i.csv')","11b27e4d":"train.head()","0d838a7e":"train.info()","aec654af":"train.describe(include='all')","9242f7d6":"train.isnull().sum().sort_values(ascending=False)","883aa37a":"#plot the distribution of numerical features\ntrain.hist(bins=50,figsize=(10,10),grid=False)\nplt.tight_layout()\nplt.show()","aecc9910":"#drop feature\ntrain.drop(['Loan_ID'], axis=1, inplace=True)","fc8178d2":"#check unique values\ntrain['Dependents'].unique()","04db794d":"#replace '3+' with '3'\ntrain['Dependents'].replace('3+', '3', inplace=True)\n\n#change to numerical\ntrain['Dependents'] = train['Dependents'].astype('float')","d452653e":"#check unique values\ntrain['Credit_History'].unique()","8214515d":"#replace 1.0 with 'Y' and 0.0 with 'N'\ntrain['Credit_History'].replace({1.: 'Y', 0.: 'N'}, inplace=True)\n\n#change to categorical\ntrain['Credit_History'] = train['Credit_History'].astype('object')","3c966d3c":"#fill missing values with mode \nfeatures_fill_with_mode = ['Self_Employed',\n                           'Dependents',\n                           'Gender',\n                           'Loan_Amount_Term']\n\nfor feature in features_fill_with_mode:\n    train[feature].fillna(train[feature].mode()[0], inplace=True)","5b641b70":"#fill missing values with mean\ntrain['LoanAmount'].fillna(train['LoanAmount'].mean(), inplace=True)","970d7e10":"sns.countplot(x='Credit_History', hue='Loan_Status', data=train);","4f0bdd2e":"train['Credit_History'].fillna('Y', inplace=True)","938c7924":"#check Dependents and CoapplicantIncome\nmask = ((train['Dependents'] > 0) | (train['CoapplicantIncome'] > 0)) \\\n        & \\\n        train['Married'].isnull()\n\ntrain[mask][['Married','Dependents','CoapplicantIncome']]","767a14b6":"#Fill missing values\ntrain.loc[mask,'Married'] = 'Yes'\ntrain['Married'].fillna('No', inplace=True)","7dcf4f95":"sns.countplot(train['Loan_Status']);","97a70969":"#transform to numerical\ntrain['Loan_Status'] = train['Loan_Status'].apply(lambda x: 1 if x=='Y' else 0)\n\n#correlation\nsns.heatmap(train.corr(),annot=True);","b1162ae8":"#copy \ntarget_array = train['Loan_Status'].copy()\n\n#drop\ntrain.drop(['Loan_Status'], axis=1, inplace=True)","eccc03a5":"#create total income feature\ntrain['Total_Income'] = train['ApplicantIncome'] + train['CoapplicantIncome']\n\n#create average loan amount feature (per day)\ntrain['Loan_Amount_Avg'] = train['LoanAmount'] \/ train['Loan_Amount_Term']\n\n#drop\ntrain.drop(['ApplicantIncome','CoapplicantIncome'], axis=1, inplace=True)","2f1ad159":"#missing values\nprint(train.isnull().any().sum())","51cbe716":"#define a normality test function\ndef normalityTest(data, alpha=0.05):\n    \"\"\"data (array)   : The array containing the sample to be tested.\n\t   alpha (float)  : Significance level.\n\t   return True if data is normal distributed\"\"\"\n    \n    from scipy import stats\n    \n    statistic, p_value = stats.normaltest(data)\n    \n    #null hypothesis: array comes from a normal distribution\n    if p_value < alpha:  \n        #The null hypothesis can be rejected\n        is_normal_dist = False\n    else:\n        #The null hypothesis cannot be rejected\n        is_normal_dist = True\n    \n    return is_normal_dist","871fdd37":"#check normality of all numericaal features and transform it if not normal distributed\nfor feature in train.columns:\n    if (train[feature].dtype != 'object'):\n        if normalityTest(train[feature]) == False:\n            train[feature] = np.log1p(train[feature])","e7940521":"#create dummies\ntrain = pd.get_dummies(train, drop_first=True)\n\nprint(train.shape)\ndisplay(train.head())","e4c84039":"X = train\ny = target_array","124829d4":"from sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state = 0)","6fdf5f70":"from sklearn.tree import DecisionTreeClassifier\n\n#create a model\nmodel = DecisionTreeClassifier()","655a044d":"#search grid for optimal parameters\nfrom sklearn.model_selection import GridSearchCV\n\nparam_grid = {'random_state' : [0,42],\n              'max_depth': [1,10,100]}\n\ngrid = GridSearchCV(model, param_grid, cv=5)\n\ngrid.fit(X_train, y_train)\n\nprint(grid.best_params_)\nprint(grid.best_score_)","2674696b":"from sklearn.metrics import classification_report\n\n#use the best model\nmodel = grid.best_estimator_\n\n#make a prediction\ny_predict = model.predict(X_test)\n\n#calculate classification report\nprint(classification_report(y_test,y_predict))","1556999d":"We can see we got right-skewed and left-skewed. We will fix this in the next step by taking the log of the values to make it normally distributed. By making it normally distributed, we can improve our model.","aba8358c":"### Quick observation on the combined data\n- Total loaner: 614\n- Feature that can be dropped from training immediately:\n    - **Loan_ID**\n- The **Loan_Status** feature, as target array, can be used as a reference to fill missing values, so we will not drop it immediately.\n- The **Dependents** feature is given in categorical but contain numerical variables. Therefore, we have to converted it to numerical variables.\n- The **Credit_History** feature is given in numerical, with 1 means 'Yes' and 0 means 'No'. We will converted it to categorical feature.\n- Features that have missing values:\n    - **Credit_History:**        50\n    - **Self_Employed:**         32\n    - **LoanAmount:**            22\n    - **Dependents:**            15\n    - **Gender:**                14\n    - **Loan_Amount_Term:**      13\n    - **Married:**                3","90785199":"# Import The Data","9d39b860":"- Creating Dummies","0244228e":"- Credit_History Feature\n\nBefore we fill missing values in Credit_History feature, we will take a deeper look by plotting it.","c3078314":"- Fill with mean()","09cada00":"From the distribution above, we can consider that the data is not imbalanced. So, we can straight to the next step: change it to numerical feature.","e8306a94":"### Drop Features","5ed6f0f2":"### Fill Missing Value: ","d85d5518":"From the plot above, we can see that Credit_History is important feature. Most people with 0 credit history didn't get a loan. But, most people who got credit history have so much better chance to get a loan.\n\nSince Credit_History = 'Y' is the value that appears most often in both Loan_Status, so we will fill missing values with 'Y' ","4c452402":"- Chech for any missing values","c2f0c088":"### Target Array\nlet's look at the target distribution","0600ebb1":"- Married Feature\n\nFor start, we will check if the missing values in the Married feature have Dependets or CoapplicantIncome more than 0, and fill it with 'Yes' if true and 'No' if otherwise.","6e208ac5":"- Normality Test","a2f835db":"### Creating new features","a25e44cf":"# Import Libraries\nFirst, we import necessary libraries, such as:","c5f13654":"- Creating features matrix (X) and target array (y)","94c2e49a":"# Creating a Model\nWe begin by splitting data into two subsets: for training data and for testing data.","cf4619b6":"- Fill with mode()","5c87af55":"### Changet to Categorical","63eb521c":"# Read The Data\nFirst, let's see the first 5 rows to familiarize ourself with the data.","4731c3f6":"# Exploratory Data Analysis","c6a5688d":"Model training : Decision Tree Classifier","585c0ea6":"### Change to Numerical","f380fa55":"In [this dataset](https:\/\/www.kaggle.com\/altruistdelhite04\/loan-prediction-problem-dataset), we need to predict whether or not to approve a loan based on the past information of the person. This is a classification problem and we will use machine learning, Decision Tree Classifier model, to make the prediction.","48561802":"### Plot The Distribution of Numerical Features","e7a658f2":"### Epilogue","84bc851c":"To get more details, we are going to print ```info()``` and ```describe()``` to make a quick observation and gain some insight from it."}}