{"cell_type":{"0a3e658b":"code","f74f9bd6":"code","13e3cda9":"code","6f6ce38b":"code","112053e2":"code","0772c760":"code","d054f1a8":"code","7d0748cd":"code","8c7ecbfc":"code","126e9971":"code","bd499ed4":"code","3cdcf2ef":"code","b016a8c0":"markdown","a845abdc":"markdown","9bac6678":"markdown","44ef8595":"markdown","329fccc7":"markdown","6bb62a97":"markdown","1200d0a1":"markdown"},"source":{"0a3e658b":"import os\nimport gc\nimport numpy as np\nimport pandas as pd\nimport datatable as dt\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom tensorflow.keras import backend as K\n\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.preprocessing import RobustScaler, LabelEncoder\nfrom sklearn.metrics import confusion_matrix\n\nfrom warnings import filterwarnings\nfilterwarnings('ignore')\n\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'","f74f9bd6":"def feature_engineering(df):\n    df['Aspect'][df['Aspect'] < 0] += 360 \n    df['Aspect'][df['Aspect'] > 359] -= 360\n    \n    df.loc[df[\"Hillshade_9am\"] < 0, \"Hillshade_9am\"] = 0\n    df.loc[df[\"Hillshade_Noon\"] < 0, \"Hillshade_Noon\"] = 0\n    df.loc[df[\"Hillshade_3pm\"] < 0, \"Hillshade_3pm\"] = 0\n    df.loc[df[\"Hillshade_9am\"] > 255, \"Hillshade_9am\"] = 255\n    df.loc[df[\"Hillshade_Noon\"] > 255, \"Hillshade_Noon\"] = 255\n    df.loc[df[\"Hillshade_3pm\"] > 255, \"Hillshade_3pm\"] = 255\n    \n    features_Hillshade = ['Hillshade_9am', 'Hillshade_Noon', 'Hillshade_3pm']\n    df[\"hillshade_mean\"] = df[features_Hillshade].mean(axis = 1)\n    df['hillshade_amp'] = df[features_Hillshade].max(axis = 1) - df[features_Hillshade].min(axis = 1)\n    \n    df[\"ecldn_dist_hydrlgy\"] = (df[\"Horizontal_Distance_To_Hydrology\"] ** 2 + df[\"Vertical_Distance_To_Hydrology\"] ** 2) ** 0.5\n    df[\"mnhttn_dist_hydrlgy\"] = np.abs(df[\"Horizontal_Distance_To_Hydrology\"]) + np.abs(df[\"Vertical_Distance_To_Hydrology\"])\n    df['binned_elevation'] = [np.floor(v\/50.0) for v in df['Elevation']]\n    df['highwater'] = (df.Vertical_Distance_To_Hydrology < 0).astype(int)\n    \n    soil_features = [x for x in df.columns if x.startswith(\"Soil_Type\")]\n    df[\"soil_type_count\"] = df[soil_features].sum(axis=1)\n    \n    wilderness_features = [x for x in df.columns if x.startswith(\"Wilderness_Area\")]\n    df[\"wilderness_area_count\"] = df[wilderness_features].sum(axis = 1)\n    \n    df['soil_Type12_32'] = df['Soil_Type32'] + df['Soil_Type12']\n    df['soil_Type23_22_32_33'] = df['Soil_Type23'] + df['Soil_Type22'] + df['Soil_Type32'] + df['Soil_Type33']\n    \n    df['Horizontal_Distance_To_Roadways'][df['Horizontal_Distance_To_Roadways'] < 0] = 0\n    df['horizontal_Distance_To_Roadways_Log'] = [np.log(v+1) for v in df['Horizontal_Distance_To_Roadways']]\n    df['Horizontal_Distance_To_Fire_Points'][df['Horizontal_Distance_To_Fire_Points'] < 0] = 0\n    df['horizontal_Distance_To_Fire_Points_Log'] = [np.log(v+1) for v in df['Horizontal_Distance_To_Fire_Points']]\n    return df","13e3cda9":"# import train & test data\ndf_train = dt.fread('..\/input\/tabular-playground-series-dec-2021\/train.csv').to_pandas()\ndf_test = dt.fread('..\/input\/tabular-playground-series-dec-2021\/test.csv').to_pandas()\ndf_pseudo = dt.fread('..\/input\/tps12-pseudolabels\/tps12-pseudolabels_v2.csv').to_pandas()\n\ndf_train = pd.concat([df_train, df_pseudo], axis=0)\n\nsample_submission = pd.read_csv('..\/input\/tabular-playground-series-dec-2021\/sample_submission.csv')\n\n# drop underrepresented class\ndf_train = df_train[df_train['Cover_Type'] != 5]\n\n# apply feature-engineering\n# thanks to https:\/\/www.kaggle.com\/c\/tabular-playground-series-dec-2021\/discussion\/293373\n# thanks to https:\/\/www.kaggle.com\/teckmengwong\/dcnv2-softmaxclassification#Feature-Engineering\n\n# df_train = feature_engineering(df_train)\n# df_test = feature_engineering(df_test)\n\n# split dataframes for later modeling\nX = df_train.drop(columns=['Id','Cover_Type','Soil_Type7','Soil_Type15','Soil_Type1']).copy()\ny = df_train['Cover_Type'].copy()\n\nX_test = df_test.drop(columns=['Id','Soil_Type7','Soil_Type15','Soil_Type1']).copy()\n\n# create label-encoded one-hot-vector for softmax, mutliclass classification\nle = LabelEncoder()\ntarget = keras.utils.to_categorical(le.fit_transform(y))\n\ndel df_train, df_test\ngc.collect()\n\nprint(X.shape, y.shape, target.shape, X_test.shape)","6f6ce38b":"# define helper functions\ndef set_seed(seed):\n    np.random.seed(seed)\n    tf.random.set_seed(seed)\n    print(f\"Seed set to: {seed}\")\n\ndef plot_eval_results(scores, n_splits):\n    cols = 5\n    rows = int(np.ceil(n_splits\/cols))\n    \n    fig, ax = plt.subplots(rows, cols, tight_layout=True, figsize=(20,2.5))\n    ax = ax.flatten()\n\n    for fold in range(len(scores)):\n        df_eval = pd.DataFrame({'train_loss': scores[fold]['loss'], 'valid_loss': scores[fold]['val_loss']})\n\n        sns.lineplot(\n            x=df_eval.index,\n            y=df_eval['train_loss'],\n            label='train_loss',\n            ax=ax[fold]\n        )\n\n        sns.lineplot(\n            x=df_eval.index,\n            y=df_eval['valid_loss'],\n            label='valid_loss',\n            ax=ax[fold]\n        )\n\n        ax[fold].set_ylabel('')\n\n    sns.despine()\n\ndef plot_cm(cm):\n    metrics = {\n        'accuracy': cm \/ cm.sum(),\n        'recall' : cm \/ cm.sum(axis=1),\n        'precision': cm \/ cm.sum(axis=0)\n    }\n    \n    fig, ax = plt.subplots(1,3, tight_layout=True, figsize=(15,5))\n    ax = ax.flatten()\n\n    mask = (np.eye(cm.shape[0]) == 0) * 1\n\n    for idx, (name, matrix) in enumerate(metrics.items()):\n\n        ax[idx].set_title(name)\n\n        sns.heatmap(\n            data=matrix,\n            cmap=sns.dark_palette(\"#69d\", reverse=True, as_cmap=True),\n            cbar=False,\n            mask=mask,\n            lw=0.25,\n            annot=True,\n            fmt='.2f',\n            ax=ax[idx]\n        )\n    sns.despine()","112053e2":"# define callbacks\nlr = keras.callbacks.ReduceLROnPlateau(\n    monitor=\"val_loss\", \n    factor=0.5, \n    patience=5, \n    verbose=True\n)\n\nes = keras.callbacks.EarlyStopping(\n    monitor=\"val_acc\", \n    patience=10, \n    verbose=True, \n    mode=\"max\", \n    restore_best_weights=True\n)","0772c760":"# create custom layer\nclass DenseBlock(layers.Layer):\n    def __init__(self, units, activation='relu', dropout_rate=0, l2=0):\n        super().__init__()\n        self.dense = layers.Dense(\n            units, activation, \n            kernel_initializer=\"lecun_normal\", \n            kernel_regularizer=keras.regularizers.l2(l2)\n        )\n        self.batchn = layers.BatchNormalization()\n        self.dropout = layers.Dropout(dropout_rate)\n    \n    def call(self, inputs):\n        x = self.dense(inputs)\n        x = self.batchn(x)\n        x = self.dropout(x)\n        return x\n\n# create fully-connected NN\nclass MLP(keras.Model):\n    def __init__(self, hidden_layers, activation='relu', dropout_rate=0, l2=0):\n        super().__init__()\n        self.hidden_layers = [\n            DenseBlock(units, activation, l2)\n            for units in hidden_layers\n        ]\n        self.softmax = layers.Dense(units=target.shape[-1], activation='softmax')\n        \n    def call(self, inputs):\n        x = inputs\n        for layer in self.hidden_layers:\n            x = layer(x)\n        x = self.softmax(x)\n        return x","d054f1a8":"try:\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver.connect()\n    tf_strategy = tf.distribute.experimental.TPUStrategy(tpu)\n    print(\"Running on TPU:\", tpu.master())\nexcept:\n    tf_strategy = tf.distribute.get_strategy()\n    print(f\"Running on {tf_strategy.num_replicas_in_sync} replicas\")\n    print(\"Number of GPUs Available: \", len(tf.config.list_physical_devices('GPU')))","7d0748cd":"seed = 2021\nset_seed(seed)\n\ncv = StratifiedKFold(n_splits=20, shuffle=True, random_state=1)\n\npredictions = []\noof_preds = {'y_valid': list(), 'y_hat': list()}\nscores_nn = {fold:None for fold in range(cv.n_splits)}\n\nfor fold, (idx_train, idx_valid) in enumerate(cv.split(X,y)):\n    X_train, y_train = X.iloc[idx_train], target[idx_train]\n    X_valid, y_valid = X.iloc[idx_valid], target[idx_valid]\n    \n    scl = RobustScaler()\n    X_train = scl.fit_transform(X_train)\n    X_valid = scl.transform(X_valid)\n    \n    with tf_strategy.scope():\n        model = MLP(\n            hidden_layers=[384, 256, 128, 64],\n            activation='selu'\n        )\n\n        model.compile(\n            optimizer=keras.optimizers.Adam(learning_rate=1e-3),\n            loss=keras.losses.CategoricalCrossentropy(),\n            metrics=['acc']\n        )\n        \n    history = model.fit(\n        X_train, y_train,\n        validation_data=(X_valid, y_valid),\n        epochs=500,\n        batch_size=2048,\n        shuffle=True,\n        verbose=False,\n        callbacks=[lr,es]\n    )\n    \n    scores_nn[fold] = history.history\n    \n    oof_preds['y_valid'].extend(y.iloc[idx_valid])\n    oof_preds['y_hat'].extend(model.predict(X_valid, batch_size=2048))\n    \n    prediction = model.predict(scl.transform(X_test), batch_size=2048)\n    predictions.append(prediction)\n    \n    del model, prediction\n    gc.collect()\n    K.clear_session()\n    \n    print('_'*65)\n    print(f\"Fold {fold+1} || Min Val Loss: {np.min(scores_nn[fold]['val_loss'])}\")\n    \nprint('_'*65)\noverall_score = [np.min(scores_nn[fold]['val_loss']) for fold in range(cv.n_splits)]\nprint(f\"Overall Mean Validation Loss: {np.mean(overall_score)}\")","8c7ecbfc":"plot_eval_results(scores_nn, cv.n_splits)","126e9971":"# prepare oof_predictions\noof_y_true = np.array(oof_preds['y_valid'])\noof_y_hat = le.inverse_transform(np.argmax(oof_preds['y_hat'], axis=1))\n\n# create confusion matrix, calculate accuracy, recall & precision\ncm = pd.DataFrame(data=confusion_matrix(oof_y_true, oof_y_hat, labels=le.classes_), index=le.classes_, columns=le.classes_)\nplot_cm(cm)","bd499ed4":"#create final prediction, inverse labels to original classes\nfinal_predictions = le.inverse_transform(np.argmax(sum(predictions), axis=1))\n\nsample_submission['Cover_Type'] = final_predictions\nsample_submission.to_csv('.\/baseline_nn.csv', index=False)\n\nsns.countplot(final_predictions)\nsns.despine()","3cdcf2ef":"sample_submission.head()","b016a8c0":"\n<div style=\"font-size:110%;line-height:155%\">\n<p>Hi,<\/p>\n<p>while I'm still enjoying my deep-learning adventure, reading and learning a ton of stuff - I decided it's time to implement different kinds of networks with this month competition. This notebook right here, will be nothing special - just a plain-old <b>fully-connected neural-network<\/b> - serving as a simple baseline. However I'm also trying to write more reusable code, hence the use of subclassing (perhaps someone finds this interesting). Anyway enjoy this month competition and good luck.<\/p>\n    \n<blockquote><img src=\"https:\/\/i.ibb.co\/0rC92ry\/MLP.png\" alt=\"MLP\" border=\"0\"><\/blockquote>\n    \n<p>Feel free to take a look at my other notebooks, covering some different ideas and architectures:\n    <li><a href=\"https:\/\/www.kaggle.com\/mlanhenke\/tps-12-bn-autoencoder-nn-keras\">Bottleneck Autoencoder<\/a><\/li>\n    <li><a href=\"https:\/\/www.kaggle.com\/mlanhenke\/tps-12-deep-wide-nn-keras\">Deep & Wide NN <\/a><\/li>\n    <li><a href=\"https:\/\/www.kaggle.com\/mlanhenke\/tps-12-deep-cross-nn-keras\">Deep & Cross NN<\/a><\/li>\n    <li><a href=\"https:\/\/www.kaggle.com\/mlanhenke\/tps-12-denoising-autoencoder-nn-keras\">Deepstack Denoising Autoencoder<\/a><\/li>\n<\/p>\n    \n<em>Thank you very much for taking some time to read my notebook. Please leave an upvote if you find any of this information useful.<\/em>\n<\/div>","a845abdc":"# Training","9bac6678":"# Introduction","44ef8595":"<img src=\"https:\/\/i.ibb.co\/PWvpT9F\/header.png\" alt=\"header\" border=\"0\" width=800 height=300>","329fccc7":"# Evaluation & Submission","6bb62a97":"# Model Setup","1200d0a1":"# Import & Prepare Data"}}