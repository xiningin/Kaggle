{"cell_type":{"947e7e50":"code","671e087a":"code","72f34bc9":"code","53fab7e7":"code","6b13cf99":"code","fba49f7f":"code","00c4e612":"code","00f48c6a":"code","69d27f7d":"markdown","b623b87a":"markdown","ac3953ec":"markdown","6dd9f8a3":"markdown","221d4908":"markdown","efdff918":"markdown","e2b0a67f":"markdown"},"source":{"947e7e50":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport os\nfrom nltk.tokenize import word_tokenize\nfrom nltk.probability import FreqDist\nfrom nltk.corpus import stopwords\nimport nltk\nfrom sklearn.cluster import KMeans\nfrom sklearn.preprocessing import StandardScaler\nprint(os.listdir(\"..\/input\"))\npd.options.mode.chained_assignment = None","671e087a":"dataset1=pd.read_csv(r\"..\/input\/GBvideos.csv\")\ndataset1=dataset1.sort_values(by=['title','views']).reset_index(drop=True)","72f34bc9":"dataset1.category_id[dataset1.category_id==1]=\"Film & Animation\"\ndataset1.category_id[dataset1.category_id==2]=\"Autos & Vehicles\"\ndataset1.category_id[dataset1.category_id==10]=\"Music\"\ndataset1.category_id[dataset1.category_id==15]=\"Pets & Animals\"\ndataset1.category_id[dataset1.category_id==17]=\"Sports\"\ndataset1.category_id[dataset1.category_id==18]=\"Short Movies\"\ndataset1.category_id[dataset1.category_id==19]=\"Travel & Events\"\ndataset1.category_id[dataset1.category_id==20]=\"Gaming\"\ndataset1.category_id[dataset1.category_id==21]=\"Videoblogging\"\ndataset1.category_id[dataset1.category_id==22]=\"People & Blogs\"\ndataset1.category_id[dataset1.category_id==23]=\"Comedy\"\ndataset1.category_id[dataset1.category_id==24]=\"Entertainment\"\ndataset1.category_id[dataset1.category_id==25]=\"News & Politics\"\ndataset1.category_id[dataset1.category_id==26]=\"Howto & Style\"\ndataset1.category_id[dataset1.category_id==27]=\"Education\"\ndataset1.category_id[dataset1.category_id==28]=\"Science & Technology\"\ndataset1.category_id[dataset1.category_id==30]=\"Movies\"\ndataset1.category_id[dataset1.category_id==31]=\"Anime\/Animation\"\ndataset1.category_id[dataset1.category_id==32]=\"Action\/Adventure\"\ndataset1.category_id[dataset1.category_id==33]=\"Classics\"\ndataset1.category_id[dataset1.category_id==34]=\"Comedy\"\ndataset1.category_id[dataset1.category_id==35]=\"Documentary\"\ndataset1.category_id[dataset1.category_id==36]=\"Drama\"\ndataset1.category_id[dataset1.category_id==37]=\"Family\"\ndataset1.category_id[dataset1.category_id==38]=\"Foreign\"\ndataset1.category_id[dataset1.category_id==39]=\"Horror\"\ndataset1.category_id[dataset1.category_id==40]=\"Sci-Fi\/Fantasy\"\ndataset1.category_id[dataset1.category_id==41]=\"Thriller\"\ndataset1.category_id[dataset1.category_id==42]=\"Shorts\"\ndataset1.category_id[dataset1.category_id==43]=\"Shows\"\ndataset1.category_id[dataset1.category_id==44]=\"Trailers\"\ndataset1.category_id[dataset1.category_id==29]='Non-profits & Activism'","53fab7e7":"Q1 = ['WDT','WRB','WP','WRB'] # wh- phrases\n\ndataset1.title = dataset1.title.str.lower()\ndataset1['IsTitleAQuestion'] = dataset1.title.map(lambda x: 1 if any([True if word in ''.join(np.array(nltk.pos_tag(word_tokenize(x))).reshape(-1)[1::2]) else False for word in Q1])==True or '?' in x else 0)\n# tokenizing titles, getting their parts of speech and indicating that if it contains parts of speech from Q1 or if it has question mark it should be marked as question\ndataset1.IsTitleAQuestion[dataset1.category_id=='Music'] = 0 # I assume that music titles don't stimulate curiosity","6b13cf99":"Eng_personal = ['i', 'he','she']\nPersonal_classification = dataset1[['title','category_id']]\nPoS = ['PRPVB','PRPVBD','PRPVBG','PRPVBN','PRPVBZ']\n\n# Detecting parts of speech from PoS\nPersonal_classification['PartsOfSpeech'] = Personal_classification.title.map(lambda x: 1 if any([True if word in ''.join(\n                                                                                    np.array(nltk.pos_tag(word_tokenize(x.lower()))).reshape(-1)[1::2])\n                                                                                    else False for word in PoS])==True else 0) \n\n# Checks whether title contains question marks or equivalent\nPersonal_classification['QuotationMarks'] = Personal_classification.title.map(lambda x: 1 if sum([1 if '\\'' in title or '\"' in title else 0 for title in x])>=2 or\n                                                                  sum([1 if ':' in title or '|' in title else 0 for title in x])>=1 else 0) \n\n# Detecting phrases 'official video' and 'official music'\nPersonal_classification['Officialness'] = Personal_classification.title.map(lambda x: 1 if 'official video' in x.lower() or 'official music' in x.lower() else 0)\n\n# Classifies as having personal aspect if title contains words from Eng_personal list and doesn't belong to 'Music' category\nPersonalAspect = []\nfor i, s in enumerate(dataset1.title.map(lambda x: x.replace('\"','').replace('\\'','')),start=0):\n    if any(word in word_tokenize(s.lower()) for word in Eng_personal) and dataset1.category_id.iloc[i]!='Music':\n        PersonalAspect.append(1)\n    else:\n        PersonalAspect.append(0)\nPersonal_classification['PersonalAspect'] = pd.Series(PersonalAspect)\n\n\n\ndata = Personal_classification[['PartsOfSpeech','QuotationMarks','category_id','title']][(Personal_classification.PersonalAspect==1) & (Personal_classification.Officialness==0)]\n\n# Standardizing variables\ntraining_data = pd.get_dummies(data[['PartsOfSpeech','QuotationMarks','category_id']])\nscaler = StandardScaler()\nscaler.fit(training_data)\ntraining_data = scaler.transform(training_data)\n\ndata1 = pd.DataFrame(data['title'])\nmodel = KMeans(n_clusters=2,n_init=100).fit(training_data)\n#     data1['Labels{}'.format(i)]= model.labels_\ndata['IsThisPersonal'] = model.predict(training_data)\n\ndataset1['IsThisPersonal'] = data.IsThisPersonal","fba49f7f":"dataset1.title[dataset1.IsThisPersonal==1].unique()","00c4e612":"MainAnimalClassification = dataset1[['title','category_id','tags','description']]\nMainAnimalClassification['NoOfMentionsInTags'] = dataset1.tags.map(lambda x: x.lower().replace('|',' ').split(' ').count('cat') +\n                                                            x.lower().replace('|',' ').split(' ').count('kitten') +\n                                                            x.lower().replace('|',' ').split(' ').count('kitty') +\n                                                            x.lower().replace('|',' ').split(' ').count('dog') +\n                                                            x.lower().replace('|',' ').split(' ').count('doggy') +\n                                                            x.lower().replace('|',' ').split(' ').count('hound') +\n                                                            x.lower().replace('|',' ').split(' ').count('pup') +\n                                                            x.lower().replace('|',' ').split(' ').count('puppy') +\n                                                            x.lower().replace('|',' ').split(' ').count('pussy') +\n                                                            x.lower().replace('|',' ').split(' ').count('cats') +\n                                                            x.lower().replace('|',' ').split(' ').count('kitties') +\n                                                            x.lower().replace('|',' ').split(' ').count('kittens') +\n                                                            x.lower().replace('|',' ').split(' ').count('dogs') +\n                                                            x.lower().replace('|',' ').split(' ').count('doggies') +\n                                                            x.lower().replace('|',' ').split(' ').count('hounds') +\n                                                            x.lower().replace('|',' ').split(' ').count('pups') +\n                                                            x.lower().replace('|',' ').split(' ').count('puppies') +\n                                                            x.lower().replace('|',' ').split(' ').count('pussies'))\n\nMainAnimalClassification['NoOfMentionsInTitle'] = dataset1.title.map(lambda x: x.lower().split(' ').count('cat') +\n                                                                x.lower().split(' ').count('kitten') +\n                                                                x.lower().split(' ').count('kitty') +\n                                                                x.lower().split(' ').count('dog') +\n                                                                x.lower().split(' ').count('doggy') +\n                                                                x.lower().split(' ').count('hound') +\n                                                                x.lower().split(' ').count('pup') +\n                                                                x.lower().split(' ').count('puppy') +\n                                                                x.lower().split(' ').count('pussy') +\n                                                                x.lower().split(' ').count('cats') +\n                                                                x.lower().split(' ').count('kitties') +\n                                                                x.lower().split(' ').count('kittens') +\n                                                                x.lower().split(' ').count('dogs') +\n                                                                x.lower().split(' ').count('doggies') +\n                                                                x.lower().split(' ').count('hounds') +\n                                                                x.lower().split(' ').count('pups') +\n                                                                x.lower().split(' ').count('puppies') +\n                                                                x.lower().split(' ').count('pussies'))\n\n\n\nMainAnimalClassification['NoOfMentionsOverall'] = MainAnimalClassification['NoOfMentionsInTags'] + MainAnimalClassification['NoOfMentionsInTitle']\n# MainAnimalClassification['NoOfMentionsInTitle']\n\ntraining_data = pd.get_dummies(MainAnimalClassification[['NoOfMentionsInTitle','NoOfMentionsOverall']])\nscaler = StandardScaler()\nscaler.fit(training_data)\ntraining_data = scaler.transform(training_data)\n\nmodel = KMeans(n_clusters=2).fit(training_data)\ndataset1['AreAnimalsInvolved'] = model.predict(training_data)\n","00f48c6a":"HasCapitalWordItTitle = []\nNumberOfCapitalWordsInTitle = []\nNumberOfWordsInTitle = []\nfor title in dataset1.title:\n    tit1 = [word for word in word_tokenize(title) if len(word)>2]\n    NumberOfCapitalWordsInTitle.append(sum([z.isupper() for z in word_tokenize(title)]))\n    NumberOfWordsInTitle.append(len(tit1))\n\ndataset1['CapitalWordsInTitle'] = NumberOfCapitalWordsInTitle\ndataset1['NumberOfWordsInTitle'] = NumberOfWordsInTitle","69d27f7d":"**Is title a question?**\n\nAs we can see on YouTube, creators often use a title with a question. Probably, the main idea which stands behind this behavior is to stimulate the inner curiosity of people potentially interested in watching a specific video.\n\nFor the purpose of simplicity, we're going to assume that not every title qualified as question has to be a question, but may simply suggest it. Example of such title may be *This is what happens when you don't sleep in | Will Smith Vlogs 26* - it is not a question, but it creates a question in mind of the reader - *What happens when you don't sleep in?*\n\nWe classify title as question, when it has a question mark, or it contains at least one wh- phrase.","b623b87a":"I assumed that title without specific personal pronouns and with phrases specific to music and film industries cannot have personal aspect - that's why titles with *Personal Aspect*=0 or *Officialness*=1  were not included into K-means. ","ac3953ec":"The reason why I'm using K-means in this specific case is to prevent from classifying video as having dogs or cats only based on word appearance. As an example, we might mention Snoop Dog.","6dd9f8a3":"**Are animals included?**\n\nVideos with sweet animals (especially with cats) often have insane amount of views.","221d4908":"**Introduction**\n\nThe main idea behind this kernel is to suggest some ideas for creating additional variables by using NLTK - library used in Natural Language Processing, and K-means - unsupervised learning grouping algorithm.\n\nMethods suggested in this kernel are not 100% accurate, but in my opinion results are good enough to implement them into further analysis.","efdff918":"**Number of words and number of capital words**\n\nCreators usually suggest, that YouTube titles should have no more than 10 words. In addition, a lot of titles contains words written by using capital letters only to get more attention.\n","e2b0a67f":"**Is this personal?**\n\nTitles may also contain some kind of personal aspect. Creator may describe some emotions appearing in video, or just simply try to clickbait reader. \n\nK-means is used to determine, whether specific video title involves stronger or lesser personal aspect. Depending on the K-means algorithm grouping results, titles get value equal to 0 or 1. Titles not involved in K-means receive 0. \n"}}