{"cell_type":{"05940346":"code","b34f1e44":"code","d6ce3a24":"code","795a51b8":"code","36e9ebbc":"code","631830e3":"code","a9e489c1":"code","322b8b11":"code","ae31776b":"code","394fb465":"code","2ebef3f0":"code","12b68d4e":"code","fe0a409c":"code","24744a53":"code","96f028da":"code","560e83fa":"code","24c97631":"code","d8f89412":"code","3af99cd9":"code","f367a790":"code","7f70323c":"code","a41b1d66":"code","a6b56180":"code","a23e07b0":"code","82c91c41":"code","dbeb1bba":"code","b9c2ef7d":"code","62103fe5":"code","61fddc2f":"code","dc58766e":"code","d076c8fa":"code","5459f0b1":"code","ebdb14da":"code","eb3d09de":"code","83ccace2":"code","959ddc75":"code","8b043f55":"code","c47c6661":"code","dd359dff":"code","2d24eaee":"code","517c3555":"code","77cff715":"code","5013dde9":"code","b97a9722":"code","52373685":"code","8c4a7d05":"code","a850a603":"code","d10d78dd":"code","bdad63a7":"code","d5dca2ca":"code","89846343":"code","04bb7d41":"code","5d12ab4d":"code","659d80cc":"code","4a7d497b":"code","41e3589c":"code","b2fb0df5":"markdown","c5d1dc3b":"markdown","6630159d":"markdown","f61ab2de":"markdown","dce52fb7":"markdown","2fe01332":"markdown","b1071b4e":"markdown","264b6d21":"markdown"},"source":{"05940346":"import os\nimport torch\nimport pandas as pd\nimport numpy as np\nfrom torch.utils.data import Dataset, DataLoader\nfrom PIL import Image\nimport torchvision.models as models\nimport matplotlib.pyplot as plt\nfrom tqdm.notebook import tqdm\nimport torchvision.transforms as T\nimport torch.nn.functional as F\nimport torch.nn as nn\nfrom torchvision.utils import make_grid\n%matplotlib inline","b34f1e44":"DATA_DIR = '..\/input\/shopee-product-detection-student'\n\nTRAIN_DIR = DATA_DIR + '\/train\/train\/train'                           \nTEST_DIR = DATA_DIR + '\/test\/test\/test'                             \n\nTRAIN_CSV = DATA_DIR + '\/train.csv'                       \nTEST_CSV = DATA_DIR + '\/test.csv'   ","d6ce3a24":"data_df = pd.read_csv(TRAIN_CSV)\ndata_df","795a51b8":"class ProductImageDataset(Dataset):\n    def __init__(self, df, root_dir, transform=None):\n        self.df = df\n        self.transform = transform\n        self.root_dir = root_dir\n        \n    def __len__(self):\n        return len(self.df)    \n    \n    def __getitem__(self, idx):\n        row = self.df.loc[idx]\n        img_id, img_label = row['filename'], row['category']\n        img_fname = self.root_dir + \"\/\" + \"{0:0=2d}\".format(img_label) + \"\/\" + str(img_id)\n        img = Image.open(img_fname)\n        img = img.convert(\"RGB\")\n        if self.transform:\n            img = self.transform(img)\n        return img, img_label","36e9ebbc":"imagenet_stats = ([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n\ntrain_tfms = T.Compose([\n    T.Resize((224,224)),\n    T.RandomCrop(size=(224,224),padding=(10,10)),\n    T.RandomHorizontalFlip(p=0.5),\n    T.RandomRotation(degrees=15,fill=0),\n     T.ToTensor(), \n     T.Normalize(*imagenet_stats), \n    #T.RandomErasing(inplace=True)\n])\n\nvalid_tfms = T.Compose([\n    T.Resize((224,224)), \n    T.ToTensor(), \n     T.Normalize(*imagenet_stats)\n])","631830e3":"np.random.seed(42)\nfrom sklearn.model_selection import train_test_split\n#_, small_data_df = train_test_split(data_df, test_size=0.1, stratify=data_df.category)\ntrain_df, val_df = train_test_split(data_df, test_size=0.1, stratify=data_df.category)\ntrain_df = train_df.reset_index()\nval_df = val_df.reset_index()","a9e489c1":"train_ds = ProductImageDataset(train_df, TRAIN_DIR, transform=train_tfms)\nval_ds = ProductImageDataset(val_df, TRAIN_DIR, transform=valid_tfms)\nlen(train_ds), len(val_ds)","322b8b11":"def show_sample(img, target):\n    plt.imshow(img.permute(1, 2, 0))\n    print('Labels:', target)","ae31776b":"show_sample(*train_ds[154])","394fb465":"batch_size = 64","2ebef3f0":"train_dl = DataLoader(train_ds, batch_size, shuffle=True, \n                      num_workers=3, pin_memory=True)\nval_dl = DataLoader(val_ds, batch_size*2, \n                    num_workers=2, pin_memory=True)","12b68d4e":"def show_batch(dl):\n    for images, labels in dl:\n        fig, ax = plt.subplots(figsize=(16, 8))\n        ax.set_xticks([]); ax.set_yticks([])\n        ax.imshow(make_grid(images, nrow=16).permute(1, 2, 0))\n        break","fe0a409c":"show_batch(train_dl)","24744a53":"def accuracy(outputs, labels):\n    _, preds = torch.max(outputs, dim=1)\n    return torch.tensor(torch.sum(preds == labels).item() \/ len(preds))","96f028da":"class ImageClassificationBase(nn.Module):\n    def training_step(self, batch):\n        images, labels = batch \n        out = self(images)                  # Generate predictions\n        loss = F.cross_entropy(out, labels) # Calculate loss\n        return loss\n    \n    def validation_step(self, batch):\n        images, labels = batch \n        out = self(images)                    # Generate predictions\n        loss = F.cross_entropy(out, labels)   # Calculate loss\n        acc = accuracy(out, labels)           # Calculate accuracy\n        return {'val_loss': loss.detach(), 'val_acc': acc}\n        \n    def validation_epoch_end(self, outputs):\n        batch_losses = [x['val_loss'] for x in outputs]\n        epoch_loss = torch.stack(batch_losses).mean()   # Combine losses\n        batch_accs = [x['val_acc'] for x in outputs]\n        epoch_acc = torch.stack(batch_accs).mean()      # Combine accuracies\n        return {'val_loss': epoch_loss.item(), 'val_acc': epoch_acc.item()}\n    \n    def epoch_end(self, epoch, result):\n        print(\"Epoch [{}], last_lr: {:.5f}, train_loss: {:.4f}, val_loss: {:.4f}, val_acc: {:.4f}\".format(\n            epoch, result['lrs'][-1], result['train_loss'], result['val_loss'], result['val_acc']))","560e83fa":"class ShopeeResnet(ImageClassificationBase):\n    def __init__(self):\n        super().__init__()\n        # Use a pretrained model\n        self.network = models.resnet18(pretrained=True)\n        # Replace last layer\n        num_ftrs = self.network.fc.in_features\n        self.network.fc = nn.Linear(num_ftrs, 42)\n    \n    def forward(self, xb):\n        return self.network(xb)\n    \n    def freeze(self):\n        # To freeze the residual layers\n        for param in self.network.parameters():\n            param.require_grad = False\n        for param in self.network.fc.parameters():\n            param.require_grad = True\n    \n    def unfreeze(self):\n        # Unfreeze all layers\n        for param in self.network.parameters():\n            param.require_grad = True","24c97631":"def get_default_device():\n    \"\"\"Pick GPU if available, else CPU\"\"\"\n    if torch.cuda.is_available():\n        return torch.device('cuda')\n    else:\n        return torch.device('cpu')\n    \ndef to_device(data, device):\n    \"\"\"Move tensor(s) to chosen device\"\"\"\n    if isinstance(data, (list,tuple)):\n        return [to_device(x, device) for x in data]\n    return data.to(device, non_blocking=True)\n\nclass DeviceDataLoader():\n    \"\"\"Wrap a dataloader to move data to a device\"\"\"\n    def __init__(self, dl, device):\n        self.dl = dl\n        self.device = device\n        \n    def __iter__(self):\n        \"\"\"Yield a batch of data after moving it to device\"\"\"\n        for b in self.dl: \n            yield to_device(b, self.device)\n\n    def __len__(self):\n        \"\"\"Number of batches\"\"\"\n        return len(self.dl)","d8f89412":"device = get_default_device()\ndevice","3af99cd9":"train_dl = DeviceDataLoader(train_dl, device)\nval_dl = DeviceDataLoader(val_dl, device)","f367a790":"@torch.no_grad()\ndef evaluate(model, val_loader):\n    model.eval()\n    outputs = [model.validation_step(batch) for batch in val_loader]\n    return model.validation_epoch_end(outputs)\n\ndef get_lr(optimizer):\n    for param_group in optimizer.param_groups:\n        return param_group['lr']\n\ndef fit_one_cycle(epochs, max_lr, model, train_loader, val_loader, \n                  weight_decay=0, grad_clip=None, opt_func=torch.optim.SGD):\n    torch.cuda.empty_cache()\n    history = []\n    \n    # Set up cutom optimizer with weight decay\n    optimizer = opt_func(model.parameters(), max_lr, weight_decay=weight_decay)\n    # Set up one-cycle learning rate scheduler\n    sched = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr, epochs=epochs, \n                                                steps_per_epoch=len(train_loader))\n    \n    for epoch in range(epochs):\n        # Training Phase \n        model.train()\n        train_losses = []\n        lrs = []\n        for batch in tqdm(train_loader):\n            loss = model.training_step(batch)\n            train_losses.append(loss)\n            loss.backward()\n            \n            # Gradient clipping\n            if grad_clip: \n                nn.utils.clip_grad_value_(model.parameters(), grad_clip)\n            \n            optimizer.step()\n            optimizer.zero_grad()\n            \n            # Record & update learning rate\n            lrs.append(get_lr(optimizer))\n            sched.step()\n        \n        # Validation phase\n        result = evaluate(model, val_loader)\n        result['train_loss'] = torch.stack(train_losses).mean().item()\n        result['lrs'] = lrs\n        model.epoch_end(epoch, result)\n        history.append(result)\n    return history","7f70323c":"model = to_device(ShopeeResnet(), device)","a41b1d66":"history = [evaluate(model, val_dl)]\nhistory","a6b56180":"#First, freeze the ResNet layers and train some epochs. This only trains the final layer to start classifying the images.\nmodel.freeze()","a23e07b0":"epochs = 5\nmax_lr = 0.01\ngrad_clip = 0.1\nweight_decay = 1e-4\nopt_func = torch.optim.SGD","82c91c41":"%%time\nhistory += fit_one_cycle(epochs, max_lr, model, train_dl, val_dl, \n                         grad_clip=grad_clip, \n                         weight_decay=weight_decay, \n                         opt_func=opt_func)","dbeb1bba":"model.unfreeze()","b9c2ef7d":"%%time\nhistory += fit_one_cycle(epochs, 0.001, model, train_dl, val_dl, \n                         grad_clip=grad_clip, \n                         weight_decay=weight_decay, \n                         opt_func=opt_func)","62103fe5":"torch.save(model.state_dict(), 'SGD-shopee-resnet18.pth')","61fddc2f":"#train_time='5:23:00'","dc58766e":"def plot_accuracies(history):\n    accuracies = [x['val_acc'] for x in history]\n    plt.plot(accuracies, '-x')\n    plt.xlabel('epoch')\n    plt.ylabel('accuracy')\n    plt.title('Accuracy vs. No. of epochs');","d076c8fa":"plot_accuracies(history)","5459f0b1":"def plot_losses(history):\n    train_losses = [x.get('train_loss') for x in history]\n    val_losses = [x['val_loss'] for x in history]\n    plt.plot(train_losses, '-bx')\n    plt.plot(val_losses, '-rx')\n    plt.xlabel('epoch')\n    plt.ylabel('loss')\n    plt.legend(['Training', 'Validation'])\n    plt.title('Loss vs. No. of epochs');","ebdb14da":"plot_losses(history)","eb3d09de":"def plot_lrs(history):\n    lrs = np.concatenate([x.get('lrs', []) for x in history])\n    plt.plot(lrs)\n    plt.xlabel('Batch no.')\n    plt.ylabel('Learning rate')\n    plt.title('Learning Rate vs. Batch no.');","83ccace2":"plot_lrs(history)","959ddc75":"def decode_target(target):\n    _, preds  = torch.max(target, dim=0)\n    return preds","8b043f55":"def predict_single(image):\n    xb = image.unsqueeze(0)\n    xb = to_device(xb, device)\n    preds = model(xb)\n    prediction = decode_target(preds[0])\n    show_sample(image, prediction)","c47c6661":"class ProductImageDataset2(Dataset):\n    def __init__(self, df, root_dir, transform=None):\n        self.df = df\n        self.transform = transform\n        self.root_dir = root_dir\n        \n    def __len__(self):\n        return len(self.df)    \n    \n    def __getitem__(self, idx):\n        row = self.df.loc[idx]\n        img_id, img_label = row['filename'], row['category']\n        img_fname = self.root_dir + \"\/\" + str(img_id)\n        img = Image.open(img_fname)\n        img = img.convert(\"RGB\")\n        if self.transform:\n            img = self.transform(img)\n        return img, img_label\n    \ntest_df = pd.read_csv(TEST_CSV)\ntest_dataset = ProductImageDataset2(test_df, TEST_DIR, transform=valid_tfms)","dd359dff":"img, target = test_dataset[0]\nimg.shape","2d24eaee":"predict_single(test_dataset[100][0])","517c3555":"predict_single(test_dataset[74][0])","77cff715":"test_dl = DeviceDataLoader(DataLoader(test_dataset, batch_size, num_workers=3, pin_memory=True), device)","5013dde9":"@torch.no_grad()\ndef predict_dl(dl, model):\n    torch.cuda.empty_cache()\n    batch_probs = []\n    for xb, _ in tqdm(dl):\n        probs = model(xb)\n        batch_probs.append(probs.cpu().detach())\n    batch_probs = torch.cat(batch_probs)\n    return [decode_target(x) for x in batch_probs]","b97a9722":"test_preds = predict_dl(test_dl, model)","52373685":"test_preds = [p.item() for p in test_preds]\ntest_preds","8c4a7d05":"submission_df = pd.read_csv(TEST_CSV)\nsubmission_df.category = test_preds\nsubmission_df.sample(20)","a850a603":"submission_df[\"category\"] = submission_df.category.apply(lambda c: str(c).zfill(2))","d10d78dd":"submission_df.sample(20)","bdad63a7":"submission_df.to_csv('submission.csv', index=False)","d5dca2ca":"weights_fname = 'SGD-shopee-resnet34.pth'","89846343":"!pip install jovian --upgrade --quiet","04bb7d41":"import jovian","5d12ab4d":"jovian.reset()\njovian.log_hyperparams(arch='resnet18', \n                       epochs=2*epochs, \n                       lr=max_lr, \n                       scheduler='one-cycle', \n                       weight_decay=weight_decay, \n                       grad_clip=grad_clip,\n                       opt=opt_func.__name__)","659d80cc":"jovian.log_metrics(val_loss=history[-1]['val_loss'], \n                   val_score=history[-1]['val_acc'],\n                   train_loss=history[-1]['train_loss'],\n                   time=\"5:40:46\")","4a7d497b":"project_name='shopee-contest'","41e3589c":"jovian.commit(project=project_name, environment=None, outputs=[weights_fname])","b2fb0df5":"## Save and Commit","c5d1dc3b":"Now, unfreeze and train some more.","6630159d":"## Training","f61ab2de":"### Data augmentations","dce52fb7":"## Preparing the Data","2fe01332":"## Making predictions and submission","b1071b4e":"## Model","264b6d21":"### DataLoaders"}}