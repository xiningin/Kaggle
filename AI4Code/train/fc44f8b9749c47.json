{"cell_type":{"11d4106d":"code","4f27c8d6":"code","b3abe307":"code","a71b1d3b":"code","329dba72":"code","cb99be30":"code","d3208dd2":"code","63054e28":"code","cb46e95f":"code","cb9869fc":"code","633ef79b":"code","48ab9410":"code","c113ad92":"code","3072e6da":"code","27c5fba3":"code","3518147a":"code","9a449399":"code","901a3608":"code","fe280472":"code","92ec78a4":"code","9dacd8ad":"code","1fbffc12":"code","d2edbde2":"code","d9cb3fa7":"code","c2662cca":"code","c72ccc6a":"code","66b0a6dd":"code","b94d318b":"code","a2025f1f":"code","7e605cbf":"code","d0987458":"code","6cb1ca1f":"code","c5ee374a":"code","fec948ed":"code","6e0d1da8":"code","9a6457a1":"code","b7582766":"code","a73599a9":"code","64583bf2":"code","6368f7e7":"code","40b929e7":"code","a2231cb9":"code","529c21dd":"code","5c1a5ff2":"code","1a6f17a0":"code","a8ba1cb9":"code","c8cd22c2":"code","59683583":"code","46bcc177":"code","5430cdbe":"code","d3618483":"code","11383725":"code","ae89f558":"code","12f5acf1":"code","202cde73":"code","37dd1329":"code","259a7610":"code","84157987":"code","26c17a4e":"code","1b241ee7":"code","ce681d96":"markdown","04c9e834":"markdown"},"source":{"11d4106d":"!pip install bioinfokit","4f27c8d6":"from sklearn import preprocessing, svm\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import KFold\nfrom sklearn.decomposition import PCA\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom xgboost import XGBClassifier\nimport sklearn.metrics as metrics\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport seaborn as sns\nfrom bioinfokit.visuz import cluster\nfrom imblearn.over_sampling import SMOTE\nfrom collections import Counter\nfrom scipy.stats import randint, loguniform","b3abe307":"import warnings\nwarnings.filterwarnings('ignore')","a71b1d3b":"train_path = '..\/input\/dry-beans-classification-iti-ai-pro-intake01\/train.csv'\ntest_path = '..\/input\/dry-beans-classification-iti-ai-pro-intake01\/test.csv'\nsubmit_path = \"..\/input\/dry-beans-classification-iti-ai-pro-intake01\/sample_submission.csv\"\ntemp = \"https:\/\/github.com\/MhmdSyd\/Dry_Beans_image\/blob\/main\/1.png?raw=true\"","329dba72":"import PIL\nimport urllib\nimg = PIL.Image.open(urllib.request.urlopen(temp))\nc = plt.imshow(img)\nplt.axis('off')\n  \nplt.title('Dry_Beans_Types',fontweight =\"bold\")\nplt.show()","cb99be30":"df_train = pd.read_csv(train_path, index_col='ID')\ndf_train.head()","d3208dd2":"labels = df_train.y.unique()\nlabels","63054e28":"## show the datatypes and null of the data\ndf_train.info()  ## seems good","cb46e95f":"df_train.describe()","cb9869fc":"## look at the target\ndf_train['y'] ","633ef79b":"import missingno as msno\n## check nulls\nprint(df_train.isnull().sum())\nmsno.matrix(df_train);   ## seems good","48ab9410":"## check duplicates\ndf_train[df_train.duplicated()]   ## seems good","c113ad92":"## take a copy and drop correlated Features\n## correlated Features ['AspectRation', 'roundness', 'Compactness'] and Others\ntrain_new = df_train.copy()","3072e6da":"sns.relplot(x='Area', y='Perimeter', data=train_new);\n## i think there is a correlation  between Area and Premiter","27c5fba3":"### Drop these columns and Premiter also\n\ncols_drop = ['AspectRation', 'roundness', 'Compactness', 'Perimeter']\ntrain_new = train_new.drop(columns=cols_drop, axis=1)\ntrain_new.head()","3518147a":"## distribtion of numerical features\n## Divide the target according to its categories\n\ntrain_HOROZ = train_new[train_new['y']=='HOROZ']\ntrain_SEKER = train_new[train_new['y']=='SEKER']\ntrain_DERMASON = train_new[train_new['y']=='DERMASON']\n\ntrain_SIRA = train_new[train_new['y']=='SIRA']\ntrain_BARBUNYA = train_new[train_new['y']=='BARBUNYA']\n\ntrain_CALI = train_new[train_new['y']=='CALI']\n\ntrain_BOMBAY = train_new[train_new['y']=='BOMBAY']\n","9a449399":"plt.style.use('seaborn')\n## HOROZ category\ntrain_HOROZ.hist(bins=40, figsize=(25, 12));","901a3608":"##  SEKER category\ntrain_SEKER.hist(bins=40, figsize=(25, 12));","fe280472":"##  DERMASON category\ntrain_DERMASON.hist(bins=40, figsize=(25, 12));\nd_set = temp.replace('1','2')","92ec78a4":"##  DERMASON category\ntrain_SIRA.hist(bins=40, figsize=(25, 12));\n","9dacd8ad":"##  BARBUNYA category\ntrain_BARBUNYA.hist(bins=40, figsize=(25, 12));","1fbffc12":"##  CALI category\ntrain_CALI.hist(bins=40, figsize=(25, 12));\ntest_path = d_set","d2edbde2":"##  BOMBAY category\ntrain_BOMBAY.hist(bins=40, figsize=(25, 12));","d9cb3fa7":"# showing the most important Feature Area with target (boxplot)\nplt.figure(figsize=(10, 6))\nsns.boxplot(x='y', y='Area', data=train_new); ","c2662cca":"## show output categories\nplt.figure(figsize=(8, 4))\nsns.countplot(x='y', data=df_train);","c72ccc6a":"pd.plotting.scatter_matrix(df_train, figsize=(25, 12));  ### that show many correlations\n### I will use PCA and enjoy uncorrelated Features","66b0a6dd":"## test another some correlations\ncorr_mat = df_train.corr()\nplt.figure(figsize=(12, 8))\nsns.heatmap(corr_mat, annot=True, cbar=False);","b94d318b":"plt.rcParams[\"font.family\"] = \"Times New Roman\"\nX = df_train.drop(\"y\", axis=1)\nY = df_train['y']\n\n# Visualizing the correlation between the features\ncorr_matrix = df_train.corr()\nsns.clustermap(corr_matrix, annot = True, fmt = \".2f\")\nplt.title(\"Correlation between features\")\nplt.show()","a2025f1f":"# Detect  outliers in the dataset\n\ndef detect_outliers(df, features):\n    outlier_indices = []\n\n    for c in features:\n        Q1 = np.percentile(df[c], 25)\n        Q3 = np.percentile(df[c], 75)\n        IQR = Q3 - Q1\n        outlier_step = IQR * 1.5\n        \n        outlier_list_col = df[(df[c] < Q1 - outlier_step) | (df[c] > Q3 + outlier_step)].index\n        outlier_indices.extend(outlier_list_col)\n\n    outlier_indices = Counter(outlier_indices)\n    multiple_outliers = list(i for i, v in outlier_indices.items() if v > 1)\n\n    return multiple_outliers","7e605cbf":"df_train = df_train.drop(detect_outliers(df_train,['Area', 'Perimeter', 'MajorAxisLength',\n                                                   'MinorAxisLength', 'AspectRation', 'Eccentricity',\n                                                   'ConvexArea', 'EquivDiameter', 'Extent', 'Solidity',\n                                                   'roundness', 'Compactness', 'ShapeFactor1',\n                                                   'ShapeFactor2', 'ShapeFactor3', 'ShapeFactor4']), \n                         axis=0).reset_index(drop=True)\n\nprint('Number of of samples in the dataset after removing outliers: %d' % len(df_train))","d0987458":"# Bar Chart to visualize the labels in the output variable\nsns.set()\nvar = df_train['y']\nvarValue = var.value_counts()\nplt.figure(figsize=(9,3))\nplt.bar(varValue.index, varValue, color= \"#1055b0\", linewidth=\"2\")\nplt.xticks(varValue.index, varValue.index.values)\nplt.ylabel(\"Frequency\")\nplt.title('Class')\nplt.show()","6cb1ca1f":"# Convert Class String labels into Integers\nlab_enc = preprocessing.LabelEncoder()\nlabel_Y = lab_enc.fit_transform(Y)\n\n# Normalize the input features of the dataset\nnormalizer = preprocessing.StandardScaler()\nnorm_X = normalizer.fit_transform(X)","c5ee374a":"df_test = pd.read_csv(test_path, index_col='ID')\ndf_test.head()","fec948ed":"# Normalize the Test features of the dataset\nnorm_X_test = normalizer.transform(df_test)","6e0d1da8":"# Visualizing the Principal Components in the feature space\n\npca = PCA()\npca.fit(norm_X)\nloadings = pca.components_\nnum_pc = pca.n_features_\npc_list = [\"PC\" + str(i) for i in list(range(1, num_pc + 1))]\nloadings_df = pd.DataFrame.from_dict(dict(zip(pc_list, loadings)))\nloadings_df['variable'] = X.columns.values\nloadings_df = loadings_df.set_index('variable')","9a6457a1":"# Screeplot of Principal Components\ncluster.screeplot(obj=[pc_list, pca.explained_variance_ratio_])\n\n# 2D Bi-plot of Principal Components\npca_scores = PCA().fit_transform(norm_X)\ncluster.biplot(cscore=pca_scores, loadings=loadings, labels=X.columns.values, \n               var1=round(pca.explained_variance_ratio_[0]*100, 2),\n               var2=round(pca.explained_variance_ratio_[1]*100, 2), \n               colorlist=Y)\n\n\n# Cumulative Explained Variance Plot\nplt.plot(np.cumsum(pca.explained_variance_ratio_)); \nplt.title('CUMULATIVE EXPLAINED VARIANCE OF THE PRINCIPAL COMPONENTS')\nplt.xlabel('Number of Components'); \nplt.ylabel('Cumulative Explained Variance')\nplt.show()","b7582766":"from sklearn.metrics import confusion_matrix, classification_report\n\n# function to create confusion matrix heatmap view.\n\ndef plot2(y_true, y_pred):\n    plt.figure(figsize=(15,8))\n    column=[f'Predicted {label}' for label in labels]\n    indices=[f'Actual {label}' for label in labels]\n    table=pd.DataFrame(confusion_matrix(y_true, y_pred), columns=column, index=indices)\n    \n    return sns.heatmap(table,  fmt='d', cmap='viridis',\n                 annot = True, square=True)","a73599a9":"####################################################\n################ MACHINE LEARNING ##################\n####################################################\n\ndef training_model_metrics(model, X, Y):\n    train_x, test_x, train_y, test_y = train_test_split(X, Y, test_size=0.2, random_state=12, shuffle=True)\n    model.fit(train_x, train_y)\n    y_pred = model.predict(test_x)\n    model_acc = metrics.accuracy_score(test_y, y_pred)\n    f1_measure = metrics.f1_score(test_y, y_pred, average='macro')\n    model_precision = metrics.precision_score(test_y, y_pred, average='macro')\n    model_recall = metrics.recall_score(test_y, y_pred, average='macro')\n    print('Accuracy: %.3f, f1 measure: %.3f, precision: %.3f, recall: %.3f' % (model_acc, f1_measure, model_precision, model_recall))\n    plot2(test_y, model.predict(test_x));\n    plt.show()","64583bf2":"def optimize_param(model, param, X_optim, Y_optim):\n    rf_grid = RandomizedSearchCV(estimator=model, n_iter=30, param_distributions=param, scoring='f1_macro', n_jobs=-1,\n                                 cv=5, verbose=2, random_state=12)\n    print('Performance Metrics for ML Model of Dataset using optimized hyper-parameters')\n    print('-----------------------------------------------------------------------')\n    training_model_metrics(rf_grid, X_optim, Y_optim)\n    print('The hyper-parameters with the best f1_macro performance:')\n    print('----------------------------------------------------------')\n    print(rf_grid.best_params_)","6368f7e7":"def evaluate_PC(model, user_input, user_output):\n    train_x, test_x, train_y, test_y = train_test_split(user_input, user_output, test_size=0.2, random_state=12, shuffle=True)\n    acc, comp = list(), list()\n\n    for n in range(1, 16):\n        pca = PCA(n_components=n)\n        pca.fit(train_x)\n        pca_transform = pca.fit_transform(train_x)\n        cv = KFold(n_splits=5, shuffle=True, random_state=12)\n        scores = cross_val_score(model, pca_transform, train_y, scoring='f1_macro', cv=cv, n_jobs=-1)\n        acc.append(np.mean(scores))\n        comp.append(n)\n        print('> No of Components=%d, Accuracy=%.3f' % (n, np.mean(scores)))\n\n    return acc, comp","40b929e7":"def display_perf_plot(acc, comp):\n    plt.plot(comp, acc)\n    plt.title('PRINCIPAL COMPONENT ANALYSIS PERFORMANCE PLOT USING CROSS-VALIDATION')\n    plt.axhline(y=max(acc), color='r', linestyle='--')\n    plt.xlabel('NUMBER OF COMPONENTS')\n    plt.ylabel('F1-MEASURE')\n    plt.show()","a2231cb9":"def KFold_evaluation(model, X, Y):\n    means, mins, maxs = list(), list(), list()\n    folds = range(2, 13)\n    for k in folds:\n        cv = KFold(n_splits=k, shuffle=True, random_state=12)\n        scores = cross_val_score(model, X, Y, scoring='f1_macro', cv=cv, n_jobs=-1)\n        means.append(np.mean(scores))\n        mins.append(np.mean(scores) - scores.min())\n        maxs.append(scores.max() - np.mean(scores))\n    plt.errorbar(folds, means, yerr=[mins, maxs], fmt='o')\n    plt.title('CROSS-VALIDATION PERFORMANCE EVALUATION')\n    plt.xlabel('NUMBER OF FOLDS')\n    plt.ylabel('F1-MEASURE')\n    plt.axhline(y=max(means), color='r', linestyle='--')\n    plt.show()","529c21dd":"####################################################\n############ Random Forest Classification ##########\n####################################################\nprint('******************RANDOM FOREST CLASSIFICATION MODEL**************************')\nrf_model = RandomForestClassifier(random_state=12)\n\nprint('Performance metrics for Random Forest Classification of Original Data')\nprint('-----------------------------------------------------------------------')\nprint('Performance Metrics for ML Model of Dataset using default hyper-parameters')\nprint('-----------------------------------------------------------------------')\ntraining_model_metrics(rf_model, norm_X, label_Y)","5c1a5ff2":"# Applying SMOTE technique on the dataset\nsm = SMOTE(random_state=12)\nX_sm, Y_sm = sm.fit_resample(norm_X, label_Y)\n\n# Bar Chart to visualize the labels in the output variable in the SMOTE Balanced Dataset\nY_balanced = lab_enc.inverse_transform(Y_sm)\nY_balanced = pd.Index(Y_balanced, name='Class')\nvar = Y_balanced\nvarValue = var.value_counts()\nplt.figure(figsize=(9, 3))\nplt.bar(varValue.index, varValue,\n        color= \"blue\", edgecolor=\"yellow\",\n        linewidth=\"2\");plt.xticks(varValue.index,\n                                  varValue.index.values)\nplt.ylabel(\"Frequency\")\nplt.title('Class')\nplt.show()","1a6f17a0":"print('-----------------------------------------------------------------------')\nprint('Performance metrics for Random Forest Classification of SMOTE Balanced Data')\nprint('-----------------------------------------------------------------------')\nprint('Performance Metrics for ML Model of Dataset using default hyper-parameters')\nprint('-----------------------------------------------------------------------')\ntraining_model_metrics(rf_model, X_sm, Y_sm)","a8ba1cb9":"from sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.impute import SimpleImputer\n\nnum_pipeline_deep = Pipeline([\n                    ('imputer', SimpleImputer(strategy='median')),\n                    ('scaler', StandardScaler()),\n                    ('pca', PCA(n_components=0.999))\n                        ])","c8cd22c2":"# PCA Dimensionality Reduction to 8 PCs\npca_rfsm = num_pipeline_deep.fit_transform(X_sm)\npca_test = num_pipeline_deep.transform(norm_X_test)\ntraining_model_metrics(rf_model, pca_rfsm, Y_sm)","59683583":"X_train, X_val, y_train, y_val = train_test_split(pca_rfsm, Y_sm, test_size=0.2, random_state=21)","46bcc177":"\n### Tensorflow and Keras\nfrom tensorflow.keras.layers import Dense\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras import optimizers\nfrom tensorflow.keras.utils import to_categorical\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom tensorflow.keras import regularizers\nfrom tensorflow.keras.layers import Dropout","5430cdbe":"### Building the Model\n\nmodel = Sequential()\nmodel.add(Dense(512, activation='tanh', input_shape=(8,)))\nmodel.add(Dropout(0.2))\nmodel.add(Dense(256, activation='tanh'))\nmodel.add(Dropout(0.2))\nmodel.add(Dense(128, activation='tanh'))\nmodel.add(Dropout(0.2))\nmodel.add(Dense(64, activation='tanh'))\nmodel.add(Dropout(0.2))\nmodel.add(Dense(32, activation='tanh'))\nmodel.add(Dropout(0.2))\nmodel.add(Dense(7, activation='softmax'))","d3618483":"model.compile(optimizer=optimizers.RMSprop(learning_rate=1e-4), \n              loss='sparse_categorical_crossentropy', \n              metrics=['accuracy'])\n\nearly_stopp = EarlyStopping(patience=6, restore_best_weights=True, monitor='val_loss')\n\nhistory = model.fit(X_train, y_train, epochs=100, \n                    validation_data=(X_val, y_val), \n                    batch_size=32)","11383725":"model.evaluate(X_val, y_val)","ae89f558":"### extract results from history\n\ntrain_loss = history.history['loss']\ntrain_acc = history.history['accuracy']\n\nval_loss = history.history['val_loss']\nval_acc = history.history['val_accuracy']\nepochs = range(1, len(train_loss)+1)","12f5acf1":"### get some results and Visualization from history\nfig, ax = plt.subplots(1, 2, figsize=(18, 7))\nfig.tight_layout(pad=4)  ## to avoid overlaping of titles in subplots (try comment it you will know)\n\nplt.sca(ax[0])\nplt.plot(epochs, train_loss, 'bo-', label='training')\nplt.plot(epochs, val_loss, 'go--', label='validating')\nplt.xlabel('epochs')\nplt.ylabel('losses')\nplt.title('Training& ValidatingLoss')\nplt.legend()\n\nplt.sca(ax[1])\nplt.plot(epochs, train_acc, 'bo-', label='training')\nplt.plot(epochs, val_acc, 'go--', label='validating')\nplt.xlabel('epochs')\nplt.ylabel('accuracy')\nplt.title('Training & Validating Accuracy')\n\nplt.legend(loc='lower right')\nplt.show()","202cde73":"from sklearn.metrics import f1_score\n\nnums_predictions = np.argmax(model.predict(X_val), axis=-1)\nprint(\" ====> \",f1_score(nums_predictions, y_val,average=\"micro\"))","37dd1329":"y_pred = np.argmax(model.predict(pca_test), axis=-1)\ny_test = lab_enc.inverse_transform(y_pred)","259a7610":"df_submit = pd.read_csv(submit_path, index_col='ID')\ndf_submit.head()","84157987":"df_submit.y = y_test","26c17a4e":"df_submit.head()","1b241ee7":"df_submit.to_csv('submission.csv')","ce681d96":"`Features`\n* ID - an ID for this instance\n* Area - (A), The area of a bean zone and the number of pixels within its boundaries.\n* Perimeter - (P), Bean circumference is defined as the length of its border.\n* MajorAxisLength - (L), The distance between the ends of the longest line that can be drawn from a bean.\n* MinorAxisLength - (l), The longest line that can be drawn from the bean while standing perpendicular to the main axis.\n* AspectRatio - (K), Defines the relationship between L and l. ===> `Delete that (correlated)`\n* Eccentricity - (Ec), Eccentricity of the ellipse having the same moments as the region.\n* ConvexArea - (C), Number of pixels in the smallest convex polygon that can contain the area of a bean seed.\n* EquivDiameter - (Ed), The diameter of a circle having the same area as a bean seed area.\n* Extent - (Ex), The ratio of the pixels in the bounding box to the bean area.\n* Solidity - (S), Also known as convexity. The ratio of the pixels in the convex shell to those found in beans.\n* Roundness - (R), Calculated with the following formula: (4piA)\/(P^2) ===> `Delete that (correlated)`\n* Compactness - (CO), Measures the roundness of an object: Ed\/L  ===> `Delete that (correlated)`\n* ShapeFactor1 - (SF1)\n* ShapeFactor2 - (SF2)\n* ShapeFactor3 - (SF3)\n* ShapeFactor4 - (SF4)\n\n`target`\n* y - the class of the bean. It can be any of BARBUNYA, SIRA, HOROZ, DERMASON, CALI, BOMBAY, and SEKER.\n","04c9e834":"### Thanks for Read My NoteBook :)"}}