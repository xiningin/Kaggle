{"cell_type":{"61e85bae":"code","3a26ae85":"code","77c1a9f6":"code","0000a33b":"code","3a26857c":"code","21af65f5":"code","bddbc4a6":"code","e387eb68":"code","464c05a4":"code","73fcc938":"code","7a669ad9":"code","4f63fe1f":"code","19fe2cb4":"code","af2aafe2":"code","99b655de":"code","343bd89d":"code","c5891ce8":"code","e9c96a7b":"code","a2dd3a68":"code","90786737":"code","5259e7c6":"code","15fa4b7d":"code","e925058d":"code","6f0f5094":"code","e387b7f4":"code","db93ff49":"code","47fa86a7":"markdown","9f540ad1":"markdown","bf41e5f1":"markdown","fa17943c":"markdown","48605313":"markdown","60677c99":"markdown","19e5bf4d":"markdown","aefc24ca":"markdown","7dc75e93":"markdown","1876ac6a":"markdown","d71c9953":"markdown"},"source":{"61e85bae":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom sklearn.preprocessing import LabelEncoder # labelencoder \nfrom sklearn.model_selection import train_test_split #Data splitting\nfrom sklearn.metrics import accuracy_score,confusion_matrix #Testing model\n\n\n#Tensorflow \nimport tensorflow as tf\nfrom tensorflow import keras\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\nimport seaborn as sns\nsns.set(style=\"darkgrid\")\nimport os\nprint(os.listdir(\"..\/input\"))\nimport matplotlib.pyplot as plt\nimport scikitplot as skplt\n\n#Machine learning algorithms\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn import svm\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.linear_model import LinearRegression\n# Any results you write to the current directory are saved as output.","3a26ae85":"df = pd.read_csv('..\/input\/adult.csv')\ndisplay(tf.version)\ndisplay(df.head())\n","77c1a9f6":"df.describe()","0000a33b":"a4_dims = (14, 8.7)\nfig, ax = plt.subplots(figsize=a4_dims,ncols=1)\nsns.countplot(x=\"workclass\",data=df)","3a26857c":"print(\"percentage of Null values in workclass feature - {0:.2f}%\".format((len((df[df['workclass']=='?']))\/len(df['workclass']))*100))\nprint(\"Percentage of Null values in occupation feature - {0:.2f}%\".format((len((df[df['occupation']=='?']))\/len(df['occupation']))*100))\nprint(\"Percentage of Null values in Native-country -{0:.2f}%\".format((len((df[df['native-country']=='?']))\/len(df['native-country']))*100))","21af65f5":"shape_with_nullvalues= df.shape\nprint(\"Shape before removing null values  {}\".format(shape_with_nullvalues))\n\ndf = df[df['workclass'] != '?']\ndf = df[df['occupation'] != '?']\ndf = df[df['native-country'] != '?']\n\nprint('Shape after removing null values {}'.format(df.shape))\nprint(\"percentage of data removed - {0:.2f}%\".format(((shape_with_nullvalues[0]-df.shape[0])\/shape_with_nullvalues[0])*100))","bddbc4a6":"a4_dims = (14, 8.7)\nfig, ax = plt.subplots(figsize=a4_dims,ncols=2)\nsns.countplot(x=\"workclass\",hue='income',data=df,ax=ax[0])\nsns.countplot(x=\"occupation\",hue='income',data=df,ax=ax[1])","e387eb68":"a4_dims = (14, 8)\nfig, ax = plt.subplots(figsize=a4_dims,ncols=2)\nsns.countplot(x=\"education\",hue=\"income\", data=df,ax=ax[0])\nsns.countplot(x=\"race\",hue=\"income\", data=df,ax=ax[1])","464c05a4":"category_col =['workclass', 'race', 'education','marital-status', 'occupation',\n               'relationship', 'gender', 'native-country', 'income'] \nfor c in category_col:\n    print (c)\n    print (df[c].value_counts())","73fcc938":"import seaborn as sns\n#age\tfnlwgt\teducational-num\tcapital-gain\tcapital-loss\thours-per-week\na4_dims = (14, 8)\nfig, ax = plt.subplots(figsize=a4_dims,ncols=2)\nsns.boxplot(x=df['age'],ax=ax[0])\nsns.boxplot(x=df['fnlwgt'],ax=ax[1])","7a669ad9":"#converting income type into numerical data\nencoder= LabelEncoder()\ndf['income'] = encoder.fit_transform(df['income'].astype('str'))\n#Converting categorial data into numerical data using one-hot encoding \ndf=df=pd.get_dummies(df)\nprint(\"Number of feautres after one-hot encoding {}\".format(len(list(df))))","4f63fe1f":"#Test Train split \ntrainset,testset= train_test_split(df,test_size = 0.33)\ntrainlabel = trainset.pop('income')\ntestlabel = testset.pop('income')\nprint(trainset.shape)\nprint(testset.shape)","19fe2cb4":"#Declearing Machine learnig model with different algorithms\nDT = DecisionTreeClassifier()\nNB = GaussianNB()\nKNN=KNeighborsClassifier()\nLR = LinearRegression()\npredictions=dict()\n","af2aafe2":"DT.fit(trainset,trainlabel)\n#Prediction \nprediction = DT.predict(testset)\naccur = accuracy_score(testlabel,prediction) \npredictions['Decision Tree'] = accur\ndisplay(\"The accuracy score of Decision Tress is {}\".format(accur))\ndisplay(confusion_matrix(testlabel,prediction))\nskplt.metrics.plot_confusion_matrix(testlabel,prediction, normalize=True)\nplt.show()\n","99b655de":"NB.fit(trainset,trainlabel)\n#Prediction \nprediction = NB.predict(testset)\naccur = accuracy_score(testlabel,prediction) \npredictions['Naive Bayes'] = accur\ndisplay(\"The accuracy of Naive Bayes is {}\".format(accur))\ndisplay(confusion_matrix(testlabel,prediction))\nskplt.metrics.plot_confusion_matrix(testlabel,prediction, normalize=True)\nplt.show()","343bd89d":"KNN.fit(trainset,trainlabel)\n#Prediction \nprediction = KNN.predict(testset)\naccur = accuracy_score(testlabel,prediction) \npredictions['KNN'] = accur\ndisplay(\"The accuracy of Naive Bayes is {}\".format(accur))\ndisplay(confusion_matrix(testlabel,prediction))\nskplt.metrics.plot_confusion_matrix(testlabel,prediction, normalize=True)\nplt.show()","c5891ce8":"LR.fit(trainset,trainlabel)\n#Prediction \nprediction = LR.predict(testset)\nprediction[prediction > .5] = 1\nprediction[prediction <=.5] = 0\naccur = accuracy_score(testlabel,prediction) \npredictions['Linear Regression'] = accur\ndisplay(\"The accuracy of Naive Bayes is {}\".format(accur))\ndisplay(confusion_matrix(testlabel,prediction))\nskplt.metrics.plot_confusion_matrix(testlabel,prediction, normalize=True)\nplt.show()","e9c96a7b":"#final analysis\ndf=pd.DataFrame(list(predictions.items()),columns=['Algorithms','Percentage'])\ndisplay(df)\nfig, (ax1) = plt.subplots(ncols=1, sharey=True,figsize=(15,5))\nsns.pointplot(x=\"Algorithms\", y=\"Percentage\", data=df,ax=ax1);\nplt.show()","a2dd3a68":"df_train = trainset.values\ndf_test = testset.values\ndf_label = tf.one_hot(trainlabel,2)\nprint(df_train[0].shape)\nprint(df_label[0].shape)","90786737":"def labelMaker(val):\n    if val == 0:\n        return [1, 0]\n    elif val == 1:\n        return [0, 1]\ndf_labels = trainlabel.values\ndf_labels = np.array([labelMaker(i) for i in trainlabel])\ntest_label = np.array([labelMaker(i) for i in testlabel])\nprint(df_labels[0].shape)\nprint(trainlabel[0].shape)\nprint(df_train[0].shape)","5259e7c6":"print(df_test.shape)\nprint(test_label.shape)","15fa4b7d":"model = keras.Sequential([\n    keras.layers.Flatten(input_shape=df_train[0].shape),\n    keras.layers.Dense(52, activation=tf.nn.relu),\n     keras.layers.Dense(22, activation=tf.nn.relu),\n    keras.layers.Dense(2, activation=tf.nn.softmax)\n])\nmodel.compile(optimizer=tf.keras.optimizers.Adam(0.1), \n              loss='mean_squared_error',\n              metrics=['accuracy'])","e925058d":"model.summary()","6f0f5094":"history = model.fit(df_train, df_labels, epochs=10,validation_data=(df_test, test_label))","e387b7f4":"test_loss, test_acc = model.evaluate(df_test, test_label)\n\nprint('Test accuracy:', test_acc)","db93ff49":"import matplotlib.pyplot as plt\n\nplt.xlabel('Epoch Number')\nplt.ylabel('Loss Number')\nplt.plot(history.history['loss'])\nplt.show()","47fa86a7":"#  Exploratory Data Analysis\n","9f540ad1":"# **K-Nearest Neighbor**\n","bf41e5f1":"# Linear Regression ","fa17943c":"# Categorical into Numerical ","48605313":"# Missing values\n* workclass\n* Occuptaion \n* Native country \n\nAre those feature with missing value '?' ","60677c99":"# Training The Model \n\nAs of this dataset we are going to train with Four main algorithms\n* Decision Tree\n* Naive Bayes\n* SVM\n* K-Nearest Neighbor\n\nwe will train it again each of the above mentioned algorithm\nGet the prediction, check the accuracy \nThen pick the model which has the best accuracy","19e5bf4d":"## Final Analysis","aefc24ca":"# Naive Bayes","7dc75e93":"# Decision Treee","1876ac6a":"# TensorFlow Model ","d71c9953":"# Reading the Data"}}