{"cell_type":{"e2cdb4ec":"code","b9d46fa0":"code","89ba4b6e":"code","bf81e069":"code","a3010443":"code","20f45e2f":"code","c03451ac":"code","7662856d":"code","ec32db18":"code","3bc59a3b":"code","448919fd":"code","532863a2":"code","cd679106":"code","0ff82b02":"code","5ef5a240":"code","17605071":"code","923051ac":"code","6ea528ad":"code","6bd691e4":"code","0cdd2403":"code","f0d19cc4":"code","54ae8bcf":"code","09ff9419":"code","fbc335eb":"code","bbeeaaf5":"code","9e79d687":"code","71da329e":"code","5909baea":"code","befe17e5":"code","222d6fd7":"code","b14aab3a":"code","1209c275":"code","6e024a51":"code","aa8d4bf0":"code","303c1c36":"code","43446495":"code","f25fb28d":"code","20950eb4":"code","75b7de71":"code","60f700c7":"code","d59ff4e6":"code","1c7a3ee7":"code","281f9793":"code","8e45bc3a":"code","5982ee3a":"code","9d8b4afe":"code","5fdb3485":"code","6f951289":"markdown","7ced040d":"markdown","e04e3133":"markdown","75426f74":"markdown","c4447a3f":"markdown","2eea7c19":"markdown","36ebf7b5":"markdown","5841d86a":"markdown","c04629f3":"markdown","d10375dc":"markdown","8975e63f":"markdown","4ec37132":"markdown","6285f18c":"markdown","10f6be99":"markdown","68baa0fa":"markdown","a44fd40e":"markdown","bf1917df":"markdown","1f8e0bb6":"markdown","24c368b4":"markdown"},"source":{"e2cdb4ec":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom imblearn.under_sampling import RandomUnderSampler\nfrom imblearn.over_sampling import RandomOverSampler\nfrom sklearn.experimental import enable_iterative_imputer\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import cross_validate\nfrom sklearn.impute import IterativeImputer\nfrom sklearn.ensemble import BaggingClassifier\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import confusion_matrix,precision_recall_curve, \\\nroc_auc_score,roc_curve,recall_score,classification_report, f1_score\nfrom sklearn.ensemble import VotingClassifier\nfrom sklearn.svm import OneClassSVM\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.impute import KNNImputer\nfrom sklearn.ensemble import IsolationForest\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.model_selection import RepeatedStratifiedKFold\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom xgboost import XGBClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom catboost import CatBoostClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom collections import Counter\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.model_selection import LeaveOneOut\nfrom sklearn.model_selection import StratifiedShuffleSplit\nfrom lightgbm import LGBMClassifier\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.decomposition import PCA\nimport imblearn\nfrom sklearn.feature_extraction.text import TfidfTransformer\nfrom imblearn.over_sampling import SMOTE\nfrom sklearn.impute import KNNImputer","b9d46fa0":"df = pd.read_csv('..\/input\/iba-ml1-mid-project\/train.csv')\ndf.head(10)","89ba4b6e":"df.shape","bf81e069":"df.columns","a3010443":"df.info()","20f45e2f":"df['credit_line_utilization']","c03451ac":"df['credit_line_utilization'].astype(float, errors=\"raise\")","7662856d":"df['credit_line_utilization'] = df['credit_line_utilization'].str.replace(',', '.').astype(float)","ec32db18":"df['credit_line_utilization']","3bc59a3b":"# We will not Id columns for prediction so I will exclude it\ncolumns = df.iloc[:, 1:-1].columns\ncolumns","448919fd":"X, y = df[columns], df.iloc[:, -1]","532863a2":"Counter(y)","cd679106":"y.value_counts().plot(kind='bar', title='Count (target)');","0ff82b02":"rus = RandomUnderSampler()\nX_rus, y_rus = rus.fit_resample(X,y)\nCounter(y_rus)","5ef5a240":"new_df = pd.concat([X_rus, y_rus], axis=1)\nnew_df","17605071":"## SimpleImputer(mean)\nsss = StratifiedKFold()\nroc = []\nfor train_idx, test_idx in sss.split(X_rus, y_rus):\n    X_train, X_test = X_rus.iloc[train_idx], X_rus.iloc[test_idx]\n    y_train, y_test = y_rus.iloc[train_idx], y_rus.iloc[test_idx]\n    \n    pipe = Pipeline(steps=[\n        ('preprocessing', ColumnTransformer(transformers=[\n            ('numeric', Pipeline(steps=[\n                ('impute', SimpleImputer(strategy='mean')),\n            ]), columns),\n        ])),\n        ('classifier', RandomForestClassifier())\n    ])\n    pipe.fit(X_train, y_train)\n    y_predicted=pipe.predict_proba(X_test)\n    roc.append(roc_auc_score(y_test, y_predicted[:, 1]))\n    \nprint(np.mean(roc))","923051ac":"## SimpleImputer(median)\nsss = StratifiedKFold()\nroc = []\nfor train_idx, test_idx in sss.split(X_rus, y_rus):\n    X_train, X_test = X_rus.iloc[train_idx], X_rus.iloc[test_idx]\n    y_train, y_test = y_rus.iloc[train_idx], y_rus.iloc[test_idx]\n    \n    pipe = Pipeline(steps=[\n        ('preprocessing', ColumnTransformer(transformers=[\n            ('numeric', Pipeline(steps=[\n                ('impute', SimpleImputer(strategy='median')),\n            ]), columns),\n        ])),\n        ('classifier', RandomForestClassifier())\n    ])\n    pipe.fit(X_train, y_train)\n    y_predicted=pipe.predict_proba(X_test)\n    roc.append(roc_auc_score(y_test, y_predicted[:, 1]))\n    \nprint(np.mean(roc))","6ea528ad":"## IterativeImputer\nsss = StratifiedKFold()\nroc = []\nfor train_idx, test_idx in sss.split(X_rus, y_rus):\n    X_train, X_test = X_rus.iloc[train_idx], X_rus.iloc[test_idx]\n    y_train, y_test = y_rus.iloc[train_idx], y_rus.iloc[test_idx]\n    \n    pipe = Pipeline(steps=[\n        ('preprocessing', ColumnTransformer(transformers=[\n            ('numeric', Pipeline(steps=[\n                ('impute', IterativeImputer()),\n            ]), columns),\n        ])),\n        ('classifier', RandomForestClassifier())\n    ])\n    pipe.fit(X_train, y_train)\n    y_predicted=pipe.predict_proba(X_test)\n    roc.append(roc_auc_score(y_test, y_predicted[:, 1]))\n    \nprint(np.mean(roc))","6bd691e4":"## KNNImputer\nsss = StratifiedKFold()\nroc = []\nfor train_idx, test_idx in sss.split(X_rus, y_rus):\n    X_train, X_test = X_rus.iloc[train_idx], X_rus.iloc[test_idx]\n    y_train, y_test = y_rus.iloc[train_idx], y_rus.iloc[test_idx]\n    \n    pipe = Pipeline(steps=[\n        ('preprocessing', ColumnTransformer(transformers=[\n            ('numeric', Pipeline(steps=[\n                ('impute', KNNImputer()),\n            ]), columns),\n        ])),\n        ('classifier', RandomForestClassifier())\n    ])\n    pipe.fit(X_train, y_train)\n    y_predicted=pipe.predict_proba(X_test)\n    roc.append(roc_auc_score(y_test, y_predicted[:, 1]))\n    \nprint(np.mean(roc))","0cdd2403":"simple_impute = SimpleImputer(strategy='mean')\nX_rus = simple_impute.fit_transform(X_rus)\nX_rus = pd.DataFrame(X_rus)\nX_rus.columns = columns","f0d19cc4":"X_rus.isna().sum()","54ae8bcf":"f, (ax1, ax2) = plt.subplots(2, 1, figsize=(24,25))\n\n# Entire DataFrame\ncorr = df.corr()\nsns.heatmap(corr, cmap='coolwarm_r', annot_kws={'size':20}, ax=ax1)\nax1.set_title(\"Imbalanced Correlation Matrix \\n (don't use for reference)\", fontsize=14)\n\n\nsub_sample_corr = new_df.corr()\nsns.heatmap(sub_sample_corr, cmap='coolwarm_r', annot_kws={'size':20}, ax=ax2)\nax2.set_title('SubSample Correlation Matrix \\n (use for reference)', fontsize=14)\nplt.show()","09ff9419":"f, axes = plt.subplots(ncols=4, figsize=(20,4))\n\n\nsns.boxplot(x=\"defaulted_on_loan\", y=\"age\", data=new_df, ax=axes[0] )\naxes[0].set_title('age vs defaulted_on_loan')\n\nsns.boxplot(x=\"defaulted_on_loan\", y=\"number_of_previous_late_payments_up_to_59_days\", data=new_df, ax=axes[1] )\naxes[1].set_title('Up to 59 days vs defaulted_on_loan')\n\nsns.boxplot(x=\"defaulted_on_loan\", y=\"number_of_previous_late_payments_up_to_89_days\", data=new_df, ax=axes[2] )\naxes[2].set_title('Up to 89 days vs defaulted_on_loan')\n\nsns.boxplot(x=\"defaulted_on_loan\", y=\"number_of_previous_late_payments_90_days_or_more\", data=new_df, ax=axes[3] )\naxes[3].set_title('90 days or more vs defaulted_on_loan')\n\nplt.show()","fbc335eb":"import time\nfrom sklearn.manifold import TSNE\nfrom sklearn.decomposition import TruncatedSVD\nimport matplotlib.patches as mpatches\nfrom sklearn.decomposition import KernelPCA\nfrom sklearn.manifold import LocallyLinearEmbedding\nfrom sklearn.manifold import Isomap\nfrom sklearn.manifold import MDS\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import roc_curve\nfrom sklearn.model_selection import cross_val_predict","bbeeaaf5":"# T-SNE Implementation\nt0 = time.time()\nX_reduced_tsne = TSNE(n_components=2, random_state=42).fit_transform(X_rus.values)\nt1 = time.time()\nprint(\"T-SNE took {:.2} s\".format(t1 - t0))\n\n# PCA Implementation\nt0 = time.time()\nX_reduced_pca = PCA(n_components=2, random_state=42).fit_transform(X_rus.values)\nt1 = time.time()\nprint(\"PCA took {:.2} s\".format(t1 - t0))\n\n# TruncatedSVD\nt0 = time.time()\nX_reduced_svd = TruncatedSVD(n_components=2, algorithm='randomized', random_state=42).fit_transform(X_rus.values)\nt1 = time.time()\nprint(\"Truncated SVD took {:.2} s\".format(t1 - t0))","9e79d687":"t0 = time.time()\nX_reduced_pca_rbf = KernelPCA(n_components = 2, kernel=\"rbf\", gamma=0.04).fit_transform(X_rus.values)\nt1 = time.time()\nprint(\"RBF took {:.2} s\".format(t1 - t0))\n\nt0 = time.time()\nX_reduced_pca_sigmoid = KernelPCA(n_components = 2, kernel=\"sigmoid\", gamma=0.04).fit_transform(X_rus.values)\nt1 = time.time()\nprint(\"Sigmoid took {:.2} s\".format(t1 - t0))\n\nt0 = time.time()\nX_reduced_isomap = Isomap(n_components=2).fit_transform(X_rus.values)\nt1 = time.time()\nprint(\"Isomap took {:.2} s\".format(t1 - t0))","71da329e":"f, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(24,6))\n# labels = ['No Fraud', 'Fraud']\nf.suptitle('Clusters using Dimensionality Reduction', fontsize=14)\n\n\nblue_patch = mpatches.Patch(color='#0A0AFF', label='0')\nred_patch = mpatches.Patch(color='#AF0000', label='1')\n\n\n# t-SNE scatter plot\nax1.scatter(X_reduced_tsne[:,0], X_reduced_tsne[:,1], c=(y_rus == 0), cmap='coolwarm', label='0', linewidths=2)\nax1.scatter(X_reduced_tsne[:,0], X_reduced_tsne[:,1], c=(y_rus == 1), cmap='coolwarm', label='1', linewidths=2)\nax1.set_title('t-SNE', fontsize=14)\n\nax1.grid(True)\n\nax1.legend(handles=[blue_patch, red_patch])\n\n\n# PCA scatter plot\nax2.scatter(X_reduced_pca[:,0], X_reduced_pca[:,1], c=(y_rus == 0), cmap='coolwarm', label='0', linewidths=2)\nax2.scatter(X_reduced_pca[:,0], X_reduced_pca[:,1], c=(y_rus == 1), cmap='coolwarm', label='1', linewidths=2)\nax2.set_title('PCA', fontsize=14)\nax2.grid(True)\n\nax2.legend(handles=[blue_patch, red_patch])\n\n# TruncatedSVD scatter plot\nax3.scatter(X_reduced_svd[:,0], X_reduced_svd[:,1], c=(y_rus == 0), cmap='coolwarm', label='0', linewidths=2)\nax3.scatter(X_reduced_svd[:,0], X_reduced_svd[:,1], c=(y_rus == 1), cmap='coolwarm', label='1', linewidths=2)\nax3.set_title('Truncated SVD', fontsize=14)\n\nax3.grid(True)\n\nax3.legend(handles=[blue_patch, red_patch])\n\nplt.show()","5909baea":"fi, (ax4, ax5, ax6) = plt.subplots(1, 3, figsize=(24,6))\nfi.suptitle('Clusters using Dimensionality Reduction', fontsize=14)\n\n\nax4.scatter(X_reduced_pca_rbf[:,0], X_reduced_pca_rbf[:,1], c=(y_rus == 0), cmap='coolwarm', label='0', linewidths=2)\nax4.scatter(X_reduced_pca_rbf[:,0], X_reduced_pca_rbf[:,1], c=(y_rus == 1), cmap='coolwarm', label='1', linewidths=2)\nax4.set_title('PCA RBF', fontsize=14)\n\nax4.grid(True)\n\nax4.legend(handles=[blue_patch, red_patch])\n\n\nax5.scatter(X_reduced_pca_sigmoid[:,0], X_reduced_pca_sigmoid[:,1], c=(y_rus == 0), cmap='coolwarm', label='0', linewidths=2)\nax5.scatter(X_reduced_pca_sigmoid[:,0], X_reduced_pca_sigmoid[:,1], c=(y_rus == 1), cmap='coolwarm', label='1', linewidths=2)\nax5.set_title('PCA Sigmoid', fontsize=14)\n\nax5.grid(True)\n\nax5.legend(handles=[blue_patch, red_patch])\n\n# TruncatedSVD scatter plot\nax6.scatter(X_reduced_isomap[:,0], X_reduced_isomap[:,1], c=(y_rus == 0), cmap='coolwarm', label='0', linewidths=2)\nax6.scatter(X_reduced_isomap[:,0], X_reduced_isomap[:,1], c=(y_rus == 1), cmap='coolwarm', label='1', linewidths=2)\nax6.set_title('Isomap', fontsize=14)\n\nax6.grid(True)\n\nax6.legend(handles=[blue_patch, red_patch])\n\nplt.show()","befe17e5":"classifiers = {\n    \"RandomForestClassifier\": RandomForestClassifier(),\n    \"GradientBoostingClassifier\": GradientBoostingClassifier(),\n    \"CatBoostClassifier\": CatBoostClassifier(silent=True),\n    \"DecisionTreeClassifier\": BaggingClassifier(DecisionTreeClassifier(), n_estimators=200)\n}","222d6fd7":"sss = StratifiedKFold()\nroc = []\ndef train(classifier):\n    for train_idx, test_idx in sss.split(X_rus, y_rus):\n        X_train, X_test = X_rus.iloc[train_idx], X_rus.iloc[test_idx]\n        y_train, y_test = y_rus.iloc[train_idx], y_rus.iloc[test_idx]\n\n        pipe = Pipeline(steps=[\n            ('preprocessing', ColumnTransformer(transformers=[\n                ('numeric', Pipeline(steps=[\n                    ('impute', SimpleImputer(strategy='median')),\n                    ('scale', RobustScaler())\n                ]), columns),\n            ])),\n            ('classifier', classifier)\n        ])\n        pipe.fit(X_train, y_train)\n        y_predicted=pipe.predict_proba(X_test)\n        roc.append(roc_auc_score(y_test, y_predicted[:, 1]))\n\n    return np.mean(roc)","b14aab3a":"for key, classifier in classifiers.items():\n    print(key,'accucary',train(classifier))","1209c275":"sss = StratifiedKFold()\nroc = []\ndef train_reduced(classifier, reducer):\n    for train_idx, test_idx in sss.split(X_rus, y_rus):\n        X_train, X_test = X_rus.iloc[train_idx], X_rus.iloc[test_idx]\n        y_train, y_test = y_rus.iloc[train_idx], y_rus.iloc[test_idx]\n\n        pipe = Pipeline(steps=[\n            ('preprocessing', ColumnTransformer(transformers=[\n                ('numeric', Pipeline(steps=[\n                    ('impute', SimpleImputer(strategy='mean')),\n                    ('scale', RobustScaler())\n                ]), columns),\n            ])),\n            ('reducing', reducer),\n            ('classifier', classifier)\n        ])\n        pipe.fit(X_train, y_train)\n        y_predicted=pipe.predict_proba(X_test)\n        roc.append(roc_auc_score(y_test, y_predicted[:, 1]))\n\n    return np.mean(roc)","6e024a51":"reducers = {\n    \"PCA RBF\": KernelPCA(n_components = 2, kernel=\"rbf\"),\n    \"Isomap\": Isomap(n_components=2,),\n    \"SVD\":  TruncatedSVD(n_components=2, algorithm='randomized')\n}","aa8d4bf0":"for red, reducer in reducers.items():\n    for clas, classifier in classifiers.items():\n        print(red, clas,'accucary',train_reduced(classifier, reducer))","303c1c36":"# Let's Plot LogisticRegression Learning Curve\nfrom sklearn.model_selection import ShuffleSplit\nfrom sklearn.model_selection import learning_curve\n\ndef plot_learning_curve(estimator1, estimator2, estimator3, estimator4, X, y, ylim=None, cv=None,\n                        n_jobs=1, train_sizes=np.linspace(.1, 1.0, 5)):\n    f, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2,2, figsize=(20,14), sharey=True)\n    if ylim is not None:\n        plt.ylim(*ylim)\n    # First Estimator\n    train_sizes, train_scores, test_scores = learning_curve(\n        estimator1, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes, scoring='roc_auc')\n    train_scores_mean = np.mean(train_scores, axis=1)\n    train_scores_std = np.std(train_scores, axis=1)\n    test_scores_mean = np.mean(test_scores, axis=1)\n    test_scores_std = np.std(test_scores, axis=1)\n    \n    ax1.fill_between(train_sizes, train_scores_mean - train_scores_std,\n                     train_scores_mean + train_scores_std, alpha=0.1,\n                     color=\"#ff9124\")\n    ax1.fill_between(train_sizes, test_scores_mean - test_scores_std,\n                     test_scores_mean + test_scores_std, alpha=0.1, color=\"#2492ff\")\n    ax1.plot(train_sizes, train_scores_mean, 'o-', color=\"#ff9124\",\n             label=\"Training score\")\n    ax1.plot(train_sizes, test_scores_mean, 'o-', color=\"#2492ff\",\n             label=\"Cross-validation score\")\n    ax1.set_title(\"GradientBoostingClassifier Learning Curve\", fontsize=14)\n    ax1.set_xlabel('Training size (m)')\n    ax1.set_ylabel('Score')\n    ax1.grid(True)\n    ax1.legend(loc=\"best\")\n    \n    \n    # Second Estimator \n    train_sizes, train_scores, test_scores = learning_curve(\n        estimator2, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes, scoring='roc_auc')\n    train_scores_mean = np.mean(train_scores, axis=1)\n    train_scores_std = np.std(train_scores, axis=1)\n    test_scores_mean = np.mean(test_scores, axis=1)\n    test_scores_std = np.std(test_scores, axis=1)\n    \n    ax2.fill_between(train_sizes, train_scores_mean - train_scores_std, train_scores_mean + train_scores_std, alpha=0.1,\n                     color=\"#ff9124\")\n    ax2.fill_between(train_sizes, test_scores_mean - test_scores_std,\n                     test_scores_mean + test_scores_std, alpha=0.1, color=\"#2492ff\")\n    ax2.plot(train_sizes, train_scores_mean, 'o-', color=\"#ff9124\",\n             label=\"Training score\")\n    ax2.plot(train_sizes, test_scores_mean, 'o-', color=\"#2492ff\",\n             label=\"Cross-validation score\")\n    ax2.set_title(\"RandomForestClassifier Learning Curve\", fontsize=14)\n    ax2.set_xlabel('Training size (m)')\n    ax2.set_ylabel('Score')\n    ax2.grid(True)\n    ax2.legend(loc=\"best\")\n    \n    \n    # Third Estimator\n    train_sizes, train_scores, test_scores = learning_curve(\n        estimator3, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes, scoring='roc_auc')\n    train_scores_mean = np.mean(train_scores, axis=1)\n    train_scores_std = np.std(train_scores, axis=1)\n    test_scores_mean = np.mean(test_scores, axis=1)\n    test_scores_std = np.std(test_scores, axis=1)\n    ax3.fill_between(train_sizes, train_scores_mean - train_scores_std,\n                     train_scores_mean + train_scores_std, alpha=0.1,\n                     color=\"#ff9124\")\n    ax3.fill_between(train_sizes, test_scores_mean - test_scores_std,\n                     test_scores_mean + test_scores_std, alpha=0.1, color=\"#2492ff\")\n    ax3.plot(train_sizes, train_scores_mean, 'o-', color=\"#ff9124\",\n             label=\"Training score\")\n    ax3.plot(train_sizes, test_scores_mean, 'o-', color=\"#2492ff\",\n             label=\"Cross-validation score\")\n    ax3.set_title(\"CatBoostClassifier Learning Curve\", fontsize=14)\n    ax3.set_xlabel('Training size (m)')\n    ax3.set_ylabel('Score')\n    ax3.grid(True)\n    ax3.legend(loc=\"best\")\n    \n    \n    # Fourth Estimator\n    train_sizes, train_scores, test_scores = learning_curve(\n        estimator4, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes, scoring='roc_auc')\n    train_scores_mean = np.mean(train_scores, axis=1)\n    train_scores_std = np.std(train_scores, axis=1)\n    test_scores_mean = np.mean(test_scores, axis=1)\n    test_scores_std = np.std(test_scores, axis=1)\n    ax4.fill_between(train_sizes, train_scores_mean - train_scores_std,\n                     train_scores_mean + train_scores_std, alpha=0.1,\n                     color=\"#ff9124\")\n    ax4.fill_between(train_sizes, test_scores_mean - test_scores_std,\n                     test_scores_mean + test_scores_std, alpha=0.1, color=\"#2492ff\")\n    ax4.plot(train_sizes, train_scores_mean, 'o-', color=\"#ff9124\",\n             label=\"Training score\")\n    ax4.plot(train_sizes, test_scores_mean, 'o-', color=\"#2492ff\",\n             label=\"Cross-validation score\")\n    ax4.set_title(\"Decision Tree Classifier \\n Learning Curve\", fontsize=14)\n    ax4.set_xlabel('Training size (m)')\n    ax4.set_ylabel('Score')\n    ax4.grid(True)\n    ax4.legend(loc=\"best\")\n    return plt","43446495":"cvv = ShuffleSplit(n_splits=50, test_size=0.2, random_state=42)\nplot_learning_curve(GradientBoostingClassifier(), RandomForestClassifier(),\n                    CatBoostClassifier(), BaggingClassifier(DecisionTreeClassifier(), n_estimators=200), \n                    X_rus, y_rus, (0.87, 1.01), cv=cvv, n_jobs=4)","f25fb28d":"from matplotlib import pyplot","20950eb4":"roc = []\nlr = []\nfor train_idx, test_idx in sss.split(X_rus, y_rus):\n    X_train, X_test = X_rus.iloc[train_idx], X_rus.iloc[test_idx]\n    y_train, y_test = y_rus.iloc[train_idx], y_rus.iloc[test_idx]\n    ns_probs = [0 for _ in range(len(y_test))]\n\n\n    pipe1 = Pipeline(steps=[\n        ('preprocessing', ColumnTransformer(transformers=[\n            ('numeric', Pipeline(steps=[\n                ('impute', SimpleImputer(strategy='median')),\n                ('scale', RobustScaler())\n            ]), columns),\n        ])),\n        ('classifier', GradientBoostingClassifier())\n    ])\n    pipe1.fit(X_train, y_train)\n    y_predicted=pipe1.predict_proba(X_test)\n    roc.append(roc_auc_score(y_test, y_predicted[:, 1]))\n    lr.append(roc_auc_score(y_test, ns_probs))\n    fpr, tpr, thresholds = roc_curve(y_test, y_predicted[:, 1])\n\nprint('Accuracy for GradientBoostingClassifier',np.mean(roc))\nns_fpr, ns_tpr, _ = roc_curve(y_test, ns_probs)\nlr_fpr, lr_tpr, _ = roc_curve(y_test, y_predicted[:, 1])\npyplot.plot(ns_fpr, ns_tpr, linestyle='--', label='No Skill')\npyplot.plot(lr_fpr, lr_tpr, marker='.', label='GradientBoostingClassifier')\n# axis labels\npyplot.xlabel('False Positive Rate')\npyplot.ylabel('True Positive Rate')\n# show the legend\npyplot.legend()\n# show the plot\npyplot.show()\n","75b7de71":"roc = []\nlr = []\nfor train_idx, test_idx in sss.split(X_rus, y_rus):\n    X_train, X_test = X_rus.iloc[train_idx], X_rus.iloc[test_idx]\n    y_train, y_test = y_rus.iloc[train_idx], y_rus.iloc[test_idx]\n    ns_probs = [0 for _ in range(len(y_test))]\n\n\n    pipe = Pipeline(steps=[\n        ('preprocessing', ColumnTransformer(transformers=[\n            ('numeric', Pipeline(steps=[\n                ('impute', SimpleImputer(strategy='median')),\n                ('scale', RobustScaler())\n            ]), columns),\n        ])),\n        ('classifier', CatBoostClassifier(silent=True))\n    ])\n    pipe.fit(X_train, y_train)\n    y_predicted=pipe.predict_proba(X_test)\n    roc.append(roc_auc_score(y_test, y_predicted[:, 1]))\n    lr.append(roc_auc_score(y_test, ns_probs))\n    fpr, tpr, thresholds = roc_curve(y_test, y_predicted[:, 1])\n\nprint('Accuracy for CatBoostClassifier',np.mean(roc))\nns_fpr, ns_tpr, _ = roc_curve(y_test, ns_probs)\nlr_fpr, lr_tpr, _ = roc_curve(y_test, y_predicted[:, 1])\npyplot.plot(ns_fpr, ns_tpr, linestyle='--', label='No Skill')\npyplot.plot(lr_fpr, lr_tpr, marker='.', label='CatBoostClassifier')\n# axis labels\npyplot.xlabel('False Positive Rate')\npyplot.ylabel('True Positive Rate')\n# show the legend\npyplot.legend()\n# show the plot\npyplot.show()\n","60f700c7":"roc = []\nlr = []\nfor train_idx, test_idx in sss.split(X_rus, y_rus):\n    X_train, X_test = X_rus.iloc[train_idx], X_rus.iloc[test_idx]\n    y_train, y_test = y_rus.iloc[train_idx], y_rus.iloc[test_idx]\n    ns_probs = [0 for _ in range(len(y_test))]\n\n\n    pipe = Pipeline(steps=[\n        ('preprocessing', ColumnTransformer(transformers=[\n            ('numeric', Pipeline(steps=[\n                ('impute', SimpleImputer(strategy='median')),\n                ('scale', RobustScaler())\n            ]), columns),\n        ])),\n        ('classifier', RandomForestClassifier(n_estimators=200))\n    ])\n    pipe.fit(X_train, y_train)\n    y_predicted=pipe.predict_proba(X_test)\n    roc.append(roc_auc_score(y_test, y_predicted[:, 1]))\n    lr.append(roc_auc_score(y_test, ns_probs))\n    fpr, tpr, thresholds = roc_curve(y_test, y_predicted[:, 1])\n\nprint('Accuracy for RandomForestClassifier',np.mean(roc))\nns_fpr, ns_tpr, _ = roc_curve(y_test, ns_probs)\nlr_fpr, lr_tpr, _ = roc_curve(y_test, y_predicted[:, 1])\npyplot.plot(ns_fpr, ns_tpr, linestyle='--', label='No Skill')\npyplot.plot(lr_fpr, lr_tpr, marker='.', label='RandomForestClassifier')\n# axis labels\npyplot.xlabel('False Positive Rate')\npyplot.ylabel('True Positive Rate')\n# show the legend\npyplot.legend()\n# show the plot\npyplot.show()\n","d59ff4e6":"roc = []\nlr = []\nfor train_idx, test_idx in sss.split(X_rus, y_rus):\n    X_train, X_test = X_rus.iloc[train_idx], X_rus.iloc[test_idx]\n    y_train, y_test = y_rus.iloc[train_idx], y_rus.iloc[test_idx]\n    ns_probs = [0 for _ in range(len(y_test))]\n\n\n    pipe = Pipeline(steps=[\n        ('preprocessing', ColumnTransformer(transformers=[\n            ('numeric', Pipeline(steps=[\n                ('impute', SimpleImputer(strategy='median')),\n                ('scale', RobustScaler())\n            ]), columns),\n        ])),\n        ('classifier', LGBMClassifier())\n    ])\n    pipe.fit(X_train, y_train)\n    y_predicted=pipe.predict_proba(X_test)\n    roc.append(roc_auc_score(y_test, y_predicted[:, 1]))\n    lr.append(roc_auc_score(y_test, ns_probs))\n    fpr, tpr, thresholds = roc_curve(y_test, y_predicted[:, 1])\n\nprint('Accuracy for LGBMClassifier',np.mean(roc))\nns_fpr, ns_tpr, _ = roc_curve(y_test, ns_probs)\nlr_fpr, lr_tpr, _ = roc_curve(y_test, y_predicted[:, 1])\npyplot.plot(ns_fpr, ns_tpr, linestyle='--', label='No Skill')\npyplot.plot(lr_fpr, lr_tpr, marker='.', label='LGBMClassifier')\n# axis labels\npyplot.xlabel('False Positive Rate')\npyplot.ylabel('True Positive Rate')\n# show the legend\npyplot.legend()\n# show the plot\npyplot.show()\n","1c7a3ee7":"from eli5.sklearn import PermutationImportance\nfrom eli5 import show_weights","281f9793":"perm = PermutationImportance(pipe1).fit(X_test, y_test)","8e45bc3a":"show_weights(perm, feature_names=X.columns.tolist())","5982ee3a":"perm_train = PermutationImportance(pipe1).fit(X_train, y_train)\nshow_weights(perm_train, feature_names=X.columns.tolist())","9d8b4afe":"X_train, X_test, y_train, y_test = train_test_split(X_rus, y_rus)","5fdb3485":"from sklearn.metrics import confusion_matrix\n\n# Logistic Regression fitted using SMOTE technique\ny_pred = pipe1.predict(X_test)\n\ncf = confusion_matrix(y_test, y_pred)\n\nsns.heatmap(cf, annot=True,  fmt='g')","6f951289":"### Lets divide target and train columns","7ced040d":"### We will first start with imputing and I have 4 techniques I would like to try. I will compare them by predicting\n1. SimpleImputer mean\n2. SimpleImputer median\n3. IterativeImputer\n4. KNNImputer\n\n### So let's build our pipeline (We will use RandomForest for this problem)","e04e3133":"## Next thing I will do is compare different classification algorithms","75426f74":"### Now I will do data visualisation","c4447a3f":"### Hmm, It appears in some rows of this column there are mistypes. *','* is used instead of *'.'* and those rows can not be converted to float, so first we will replace *','* with *'.'*  ","2eea7c19":"* ### As we can see our data is imbalanced. We will implement \"Random Under Sampling\" which basically consists of removing data in order to have a more balanced dataset and thus avoiding our models to overfitting.\n* ### Once we determine how many instances are loan default (loan default = \"1\"), we should bring the none loan default to the same amount (assuming we want a 50\/50 ratio), this will be equivalent to 492 cases of fraud and 5013 cases of loan default (loan default = 1)","36ebf7b5":"## Now lets use dimensionality reduction on our data","5841d86a":"### Now there are 5013 1 and 0 in the dataset and this time it is balanced","c04629f3":"## Now lets try our dimensionality reduction techniques","d10375dc":"#### SimpleImputer with mean strategy performed better we continuie with it","8975e63f":"### Done!!","4ec37132":"### We had high hopes for dimensionality reduction that it will increase the accuracy, but it didn't","6285f18c":"### It is interesting that this columns consist of numbers but dtype is object we will convert it to float","10f6be99":"### Let's see our result on confusion matrix","68baa0fa":"## Now all our columns are numeric and data is balanced we can start Visualisation and Imputing","a44fd40e":"### There are interesting results in PCA RBF, Isomap and Truncated SVD. We will definately try them in the prediction","bf1917df":"### All our columns are numbers except one credit_line_utilization we will check it now","1f8e0bb6":"### Let's take a look at the target","24c368b4":"<h3>Important thing to NOTE<\/h3>\n<h3>I have tried different outlier detection and removal tehcniques(on my first EDA notebook) and unfortunately they don't work well with this data. It is better to keep this dataset as it is. So, I am not gonna spend time on them in this notebook<\/h3>\n"}}