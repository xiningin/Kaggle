{"cell_type":{"ec29a631":"code","fb035466":"code","098bf643":"code","c1ad75e4":"code","0a747ad1":"code","76296010":"code","c83cd361":"code","4da46bf1":"code","0bf985c0":"code","4b65312e":"code","317940ae":"code","4b862576":"code","b1b8ba66":"code","87db564a":"code","3c4938ba":"code","c58f6df5":"code","43014ae5":"code","39d9fa73":"code","0f22705c":"code","fdb7ce32":"code","9a19cec6":"code","4461ca83":"code","b3269453":"code","2287efb4":"code","ddf73bd8":"code","34eb443c":"code","7976e73c":"code","f902e9f0":"code","7f4fc8b0":"code","0b3f0d01":"code","c83ace67":"code","3ccc1fae":"code","d01b4c39":"code","7dc8a818":"code","40a586a6":"code","63a2bf62":"code","c87997b4":"code","1e117093":"code","5290695a":"code","9e31842c":"code","c78816a1":"code","51cada3a":"code","d5191475":"code","275e345f":"code","89060af8":"code","b4607ed7":"code","eeb0ec93":"code","7a65abc6":"code","9c7fe37b":"code","546b393e":"code","52a381e2":"code","3978ac2c":"code","0e6f8cf8":"code","a92beb81":"code","0e8e168a":"code","a5085afd":"code","5f5fbf28":"code","31fc4972":"code","63fa46bb":"code","6e541f3a":"code","26f7a0cd":"code","74b2a1f8":"code","b29327f9":"code","0137d0dd":"markdown","378333d2":"markdown","325bfece":"markdown","3a15e49c":"markdown","ec173e08":"markdown","d8ea28e5":"markdown","67d6b675":"markdown","ee7504f1":"markdown","600e0aee":"markdown","6ae6417c":"markdown","35082c2b":"markdown","c1290f9d":"markdown","23466c78":"markdown","f22ca1a6":"markdown"},"source":{"ec29a631":"!pip install autoviz geopy","fb035466":"# import libraries\nimport numpy as np\nimport pandas as pd \npd.set_option('display.max_columns', 500)\nimport joblib # export model\nfrom datetime import datetime # cek waktu proses\n\nimport category_encoders as ce # binary encoding\n\n# machine learning\nfrom sklearn import preprocessing\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn import linear_model\nfrom sklearn import metrics\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import cross_val_score\n\n# find location\nfrom geopy.geocoders import Nominatim\ngeolocator = Nominatim(user_agent=\"agent\")\n\n#automate EDA\nfrom autoviz.AutoViz_Class import AutoViz_Class\n\n# plotting\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline","098bf643":"# laod the data\ndf = pd.read_csv('..\/input\/housesalesprediction\/kc_house_data.csv')\ndf.head()","c1ad75e4":"# check tail\ndf.tail()","0a747ad1":"# check view\ndf['view'].unique() ","76296010":"# check available columns\ndf.columns","c83cd361":"# check shape\ndf.shape","4da46bf1":"# describe\ndf.describe()","0bf985c0":"# check column and its dtype\ndf.info()","4b65312e":"# check missing value\ndf.isnull().sum()","317940ae":"# check unique values\ndf.nunique()","4b862576":"# slice date to year, month, date\ndf[\"date_year\"] = df[\"date\"].str.slice(0,4).astype(int)\ndf[\"date_month\"] = df[\"date\"].str.slice(4,6).astype(int)\ndf[\"date_day\"] = df[\"date\"].str.slice(6,8).astype(int)\n\ndf.head()","b1b8ba66":"# check column uniqueness\nprint('bedrooms:', df[\"bedrooms\"].unique())\nprint('bathrooms:', df[\"bathrooms\"].unique())\nprint('floors:', df[\"floors\"].unique())\nprint('waterfront:', df[\"waterfront\"].unique())\nprint('view:', df[\"view\"].unique())\nprint('condition:', df[\"condition\"].unique())\nprint('grade:', df[\"grade\"].unique())\nprint('year_built:', df[\"yr_built\"].unique())\nprint('year_renovated:', df[\"yr_renovated\"].unique())\nprint('zipcode:', df[\"zipcode\"].unique())\nprint('date_year:', df[\"date_year\"].unique())\nprint('date_month:', df[\"date_month\"].unique())\nprint('date_day:', df[\"date_day\"].unique())","87db564a":"# generate new column house_age\ndf[\"house_age\"] = df[\"date_year\"] - df[\"yr_built\"] \ndf.head()","3c4938ba":"# its still possible, the house might be built on the next year when it's sold.\ndf[df[\"house_age\"]==-1]","c58f6df5":"# convert lat long to city using geopy\n# based on research on https:\/\/www.kingcounty.gov\/depts\/health\/codes\/cities.aspx there are roughly 39 counties in king county.\ncitytown = []\n\nstart = datetime.now()\n\nfor i in range(len(df)):\n    try:\n        try:\n            c = geolocator.reverse(df['lat'][i].astype(str)+', '+df['long'][i].astype(str)).raw['address']['city']\n        except Exception:\n            c = geolocator.reverse(df['lat'][i].astype(str)+', '+df['long'][i].astype(str)).raw['address']['town']\n    except Exception:\n        c = 'none'\n        \n    # print(c)\n    citytown.append(c)    \n    if(i%100 == 0):\n        print(i)\n\nend = datetime.now()\n\nprint('process time: ', end - start)","43014ae5":"#generate city_town column\ndf['city_town'] = pd.DataFrame(citytown)","39d9fa73":"# generate is_renovated\ndf[\"is_renovated\"] = np.where(df[\"yr_renovated\"] > 0, 1, df[\"yr_renovated\"])\ndf.head()","0f22705c":"# generate rnv_age\n# assumed renovated house = full renovated = like new house.\ndf[\"rnv_age\"] = np.where(df['yr_renovated'] > 0, df['date_year'] - df['yr_renovated'], df['yr_renovated'])\ndf[\"rnv_age\"] = np.where(df['yr_renovated'] == 0, df['house_age'], df['rnv_age'])\ndf.head()","fdb7ce32":"# check point 1 <- still not well done yet (medium rare)\ndf.to_csv('kc_data_house-mid.csv', encoding='utf-8', index=False)","9a19cec6":"# load check point 1\ndf_mid = pd.read_csv('kc_data_house-mid.csv')\ndf_mid.head()","4461ca83":"# check house_age statistic\ndf_mid[\"house_age\"].describe()","b3269453":"# check rnv_age statistic\ndf_mid[\"rnv_age\"].describe()","2287efb4":"# check yr_renovated unique values\ndf_mid[\"yr_renovated\"].unique()","ddf73bd8":"# house age\ndf_mid[\"house_age\"] = np.where((df_mid[\"house_age\"] >= -1) & (df_mid[\"house_age\"] <= 27), 1, df_mid[\"house_age\"]) #29 steps\ndf_mid[\"house_age\"] = np.where((df_mid[\"house_age\"] >= 28) & (df_mid[\"house_age\"] <= 51), 2, df_mid[\"house_age\"])\ndf_mid[\"house_age\"] = np.where((df_mid[\"house_age\"] >= 52) & (df_mid[\"house_age\"] <= 75), 3, df_mid[\"house_age\"]) \ndf_mid[\"house_age\"] = np.where((df_mid[\"house_age\"] >= 76) & (df_mid[\"house_age\"] <= 98), 4, df_mid[\"house_age\"])\ndf_mid[\"house_age\"] = np.where((df_mid[\"house_age\"] >= 99) & (df_mid[\"house_age\"] <= 122), 5, df_mid[\"house_age\"])\n\ndf_mid[\"house_age\"].head(20)","34eb443c":"# rnv age\ndf_mid[\"rnv_age\"] = np.where((df_mid[\"rnv_age\"] >= -1) & (df_mid[\"rnv_age\"] <= 27), 1, df_mid[\"rnv_age\"]) #29 steps\ndf_mid[\"rnv_age\"] = np.where((df_mid[\"rnv_age\"] >= 28) & (df_mid[\"rnv_age\"] <= 51), 2, df_mid[\"rnv_age\"])\ndf_mid[\"rnv_age\"] = np.where((df_mid[\"rnv_age\"] >= 52) & (df_mid[\"rnv_age\"] <= 75), 3, df_mid[\"rnv_age\"]) \ndf_mid[\"rnv_age\"] = np.where((df_mid[\"rnv_age\"] >= 76) & (df_mid[\"rnv_age\"] <= 98), 4, df_mid[\"rnv_age\"])\ndf_mid[\"rnv_age\"] = np.where((df_mid[\"rnv_age\"] >= 99) & (df_mid[\"rnv_age\"] <= 122), 5, df_mid[\"rnv_age\"])\n\ndf_mid[\"rnv_age\"].head(20)","7976e73c":"# check binned values\ndf_mid.head()","f902e9f0":"# binary Encoding -> zipcode, city_town\nencoder = ce.BinaryEncoder(cols=['zipcode', 'city_town'], return_df=True)\ndf_mid = encoder.fit_transform(df_mid)\ndf_mid.head()","7f4fc8b0":"# one hot encoding -> is_renovated\nis_renovated_df = pd.get_dummies(df_mid.is_renovated, prefix='is_renovated')\ndf_mid = pd.concat([df_mid, is_renovated_df], axis=1)\ndf_mid.head()","0b3f0d01":"# Selection\n# check columns\ndf_mid.columns","c83ace67":"# drop unintended column\ny = df_mid[\"price\"]\nX = df_mid.drop(['id','date','lat','long','yr_built','yr_renovated','is_renovated','price'], axis=1)","3ccc1fae":"X_columns = ['bedrooms', 'bathrooms', 'sqft_living', 'sqft_lot', 'floors',\n       'waterfront', 'view', 'condition', 'grade', 'sqft_above',\n       'sqft_basement', 'zipcode_0', 'zipcode_1', 'zipcode_2', 'zipcode_3',\n       'zipcode_4', 'zipcode_5', 'zipcode_6', 'zipcode_7', 'sqft_living15',\n       'sqft_lot15', 'date_year', 'date_month', 'date_day', 'house_age',\n       'city_town_0', 'city_town_1', 'city_town_2', 'city_town_3',\n       'city_town_4', 'city_town_5', 'city_town_6', 'rnv_age',\n       'is_renovated_0', 'is_renovated_1']","d01b4c39":"# Min Max scaler -> X\nscaler = MinMaxScaler()\nX = scaler.fit_transform(X)\nX = pd.DataFrame(X)\n\nX.columns = X_columns\nX.head()","7dc8a818":"# concat X and y\ndf_pos = pd.concat([X, y], axis=1)\ndf_pos.head()","40a586a6":"# check point 2 <- well done\ndf_pos.to_csv('kc_data_house-pos.csv', encoding='utf-8', index=False)","63a2bf62":"# load truely meaningful data\ndf_pos = pd.read_csv('kc_data_house-pos.csv')\ndf_pos.head()","c87997b4":"# check shape\ndf_pos.shape","1e117093":"# check correlation\ndf_pos.corr()","5290695a":"# check why zipcode_0 and city_town_0 = NaN\nprint('zipcode_0: ', df_pos['zipcode_0'].unique())\nprint('city_town_0: ', df_pos['city_town_0'].unique())","9e31842c":"# manual EDA -> sqft_lot (scatterplot)\nplt.scatter(df_pos['sqft_lot'], df_pos['price'])\nplt.xlabel(\"sqft_lot\")\nplt.ylabel(\"Price\")\nplt.show()","c78816a1":"# manual EDA -> bathrooms\nplt.scatter(df_pos['bathrooms'], df_pos['price'])\nplt.xlabel(\"bathrooms\")\nplt.ylabel(\"Price\")\nplt.show()","51cada3a":"# manual EDA -> sqft_living\nplt.scatter(df_pos['sqft_living'], df_pos['price'])\nplt.xlabel(\"Sqft Living\")\nplt.ylabel(\"Price\")\nplt.show()","d5191475":"# manual EDA -> grade\nplt.scatter(df_pos['grade'], df_pos['price'])\nplt.xlabel(\"Grade\")\nplt.ylabel(\"Price\")\nplt.show()","275e345f":"# auto EDA with autoviz library\nAV = AutoViz_Class()\nAV.AutoViz('kc_data_house-pos.csv', depVar='price')","89060af8":"# initialize num_feats\nnum_feats = 15","b4607ed7":"# split dependent and independent variables\ny = df_pos['price']\nX = df_pos.drop('price', axis=1)","eeb0ec93":"# 1. PEARSON CORRELATION (filter methods)\ndef cor_selector(X, y,num_feats):\n    cor_list = []\n    feature_name = X.columns.tolist()\n    # calculate the correlation with y for each feature\n    for i in X.columns.tolist():\n        cor = np.corrcoef(X[i], y)[0, 1]\n        cor_list.append(cor)\n    # replace NaN with 0\n    cor_list = [0 if np.isnan(i) else i for i in cor_list]\n    # feature name\n    cor_feature = X.iloc[:,np.argsort(np.abs(cor_list))[-num_feats:]].columns.tolist()\n    # feature selection? 0 for not select, 1 for select\n    cor_support = [True if i in cor_feature else False for i in feature_name]\n    return cor_support, cor_feature\ncor_support, cor_feature = cor_selector(X, y,num_feats)\nprint(str(len(cor_feature)), 'selected features')\n\nprint(\"pearson correlation\")\nprint(cor_feature)","7a65abc6":"# 2. CHI SQUARE FEATURES (filter methods)\nfrom sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import chi2\nfrom sklearn.preprocessing import MinMaxScaler\nX_norm = MinMaxScaler().fit_transform(X)\n\nchi_selector = SelectKBest(chi2, k=num_feats)\nchi_selector.fit(X_norm, y)\nchi_support = chi_selector.get_support()\nchi_feature = X.loc[:,chi_support].columns.tolist()\nprint(str(len(chi_feature)), 'selected features')\n\nprint(\"chi feature\")\nprint(chi_feature)","9c7fe37b":"# 3.RECURSIVE FEATURE ELIMINATION (wrapper methods)\nfrom sklearn.feature_selection import RFE\nfrom sklearn.linear_model import LogisticRegression\n\nrfe_selector = RFE(estimator=LogisticRegression(solver='lbfgs'), n_features_to_select=num_feats, step=10, verbose=5)\nrfe_selector.fit(X_norm, y)\n\nrfe_support = rfe_selector.get_support()\nrfe_feature = X.loc[:,rfe_support].columns.tolist()\nprint(str(len(rfe_feature)), 'selected features')","546b393e":"# 4. LASSO: SELECT FROM MODEL (embedded methods)\nfrom sklearn.feature_selection import SelectFromModel\nfrom sklearn.linear_model import LogisticRegression\n\nfrom sklearn.preprocessing import MinMaxScaler\nX_norm = MinMaxScaler().fit_transform(X)\n\nembeded_lr_selector = SelectFromModel(LogisticRegression(penalty='l1', solver='liblinear'), max_features=num_feats)\nembeded_lr_selector.fit(X_norm, y)\n\nembeded_lr_support = embeded_lr_selector.get_support()\nembeded_lr_feature = X.loc[:,embeded_lr_support].columns.tolist()\nprint(str(len(embeded_lr_feature)), 'selected features')\n\nprint(\"lasso model\")\nprint(embeded_lr_feature)","52a381e2":"# 5. TREE BASED SELECT FROM MODEL\nfrom sklearn.feature_selection import SelectFromModel\nfrom sklearn.ensemble import RandomForestClassifier\n\nembeded_rf_selector = SelectFromModel(RandomForestClassifier(n_estimators=10, max_depth=6), max_features=num_feats)\nembeded_rf_selector.fit(X, y)\n\nembeded_rf_support = embeded_rf_selector.get_support()\nembeded_rf_feature = X.loc[:,embeded_rf_support].columns.tolist()\nprint(str(len(embeded_rf_feature)), 'selected features')\n\n\nprint(\"random forest\")\nprint(embeded_rf_feature)","3978ac2c":"# OVERALL\npd.set_option('display.max_rows', None)\nfeature_name = X.columns.tolist()\n #put all selection together\nfeature_selection_df = pd.DataFrame({'Feature':feature_name, 'Pearson':cor_support, 'Chi-2':chi_support, 'RFE':rfe_support, 'LASSO':embeded_lr_support,\n                                    'Random Forest':embeded_rf_support})\n# count the selected times for each feature\nfeature_selection_df['Total'] = np.sum(feature_selection_df, axis=1)\n# display the top 100\nfeature_selection_df = feature_selection_df.sort_values(['Total','Feature'] , ascending=False)\nfeature_selection_df.index = range(1, len(feature_selection_df)+1)\nprint(feature_selection_df.head(num_feats))","0e6f8cf8":"# take only the selected value\nX_filtered = X[['zipcode_2', 'view', 'sqft_living15','grade','zipcode_7','zipcode_6','zipcode_5','zipcode_3','city_town_6','city_town_5', 'city_town_4', 'city_town_3', 'city_town_2','zipcode_4','waterfront']]","a92beb81":"# split into train and test dataset\n# train and test ratio => 80:20\nX_train, X_test, y_train, y_test = train_test_split(X_filtered, y, test_size=0.2, random_state=0)","0e8e168a":"print('X_train shape: ', X_train.shape)\nprint('y_train shape: ', y_train.shape)\nprint('X_test shape: ', X_test.shape)\nprint('y_test shape: ', y_test.shape)","a5085afd":"#1. Multiple Linear Regression\nfrom sklearn.linear_model import LinearRegression\n\nlr = LinearRegression()\n\nmodel_mlp = lr.fit(X_train, y_train)\ny_hat_lr = lr.predict(X_test)\ny_hat_lr","5f5fbf28":"#2. Polynomial Regression\nfrom sklearn.preprocessing import PolynomialFeatures\n\npoly = PolynomialFeatures(degree=2)\nX_poly = poly.fit_transform(X_train)\nmodel_poly = lr.fit(X_poly, y_train)\n\nX_poly_test = poly.fit_transform(X_test)\ny_hat_poly = lr.predict(X_poly_test)\ny_hat_poly","31fc4972":"# evaluate model with MSE\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import r2_score","63fa46bb":"# function for plotting\ndef DistributionPlot(RedFunction, BlueFunction, RedName, BlueName, Title):\n    width = 12\n    height = 10\n    plt.figure(figsize=(width, height))\n\n    ax1 = sns.distplot(RedFunction, hist=False, color=\"r\", label=RedName)\n    ax2 = sns.distplot(BlueFunction, hist=False, color=\"b\", label=BlueName, ax=ax1)\n\n    plt.title(Title)\n\n    plt.show()\n    plt.close()","6e541f3a":"Title = 'Distribution Plot of Multiple Linear Regression'\nDistributionPlot(y_test, y_hat_lr, \"Actual Values (Test)\", \"Predicted Values (Test)\", Title)\n\nprint(\"MSE for multiple linear regression: \", mean_squared_error(y_test, y_hat_lr))\nprint(\"r2 score for multiple linear regression: \", r2_score(y_test, y_hat_lr))","26f7a0cd":"Title = 'Distribution Plot of Polynomial Regression with degree 2'\nDistributionPlot(y_test, y_hat_poly, \"Actual Values (Test)\", \"Predicted Values (Test)\", Title)\n\nprint(\"MSE for polynomial regression (deg=2): \", mean_squared_error(y_test, y_hat_poly))\nprint(\"r2 score for polynomial regression (deg=2): \", r2_score(y_test, y_hat_poly))","74b2a1f8":"# export model to pkl format\njoblib.dump(model_poly, 'house_price_model.pkl')","b29327f9":"# prediction demo with new data\nmodel_clone = joblib.load('house_price_model.pkl')\n\nnew_data = pd.DataFrame([[0.0,0.0,0.161934,0.5,1.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0]])\n\npoly = PolynomialFeatures(degree=2)\nX_new = poly.fit_transform(new_data)\n\nmodel_clone.predict(X_new)","0137d0dd":"# Exploratory Data Analysis","378333d2":"# Feature Selection Algorithm","325bfece":"# Data Understanding","3a15e49c":"# Modeling","ec173e08":"# House Sales in King County, USA\n\n### Description\nThis dataset contains house sale prices for King County, which includes Seattle. It includes homes sold between May 2014 and May 2015.\n\nIt's a great dataset for evaluating simple regression models.","d8ea28e5":"# Binning\nGroup into 5 different categories based on house_age and rnv_age","67d6b675":"# Conclusion\n\nAs we can see, <b>2nd-degree polynomial regression<\/b> has better performance than <b>multiple linear regression<\/b>. So we prefer that model to be exported. In this notebook, we also show a mini demonstration about how the saved model can work on new data input and produced the predicted price as the final output.","ee7504f1":"# Train Test Split","600e0aee":"# References\n\n> House Sales Data\n- https:\/\/www.kaggle.com\/harlfoxem\/housesalesprediction\n\n> Burhan's Notebook\n- https:\/\/www.kaggle.com\/burhanykiyakoglu\/predicting-house-prices\n\n> Feature Information\n- https:\/\/www.slideshare.net\/PawanShivhare1\/predicting-king-county-house-prices","6ae6417c":"# Export Model","35082c2b":"# Evaluation","c1290f9d":"<!-- -->","23466c78":"# Data Preparation","f22ca1a6":"# Business Understanding\n##### Problem: \nOur client want to have a machine that can correctly predict the house price.\n\n##### Clear Questions: \n- How to predict the house price accurately?\n\n##### Analytic Approach: \nRegression ( Linear and Polynomial )\n\n##### Data Requirements \/ Features: \n- Rely on feature selection algorithm followed by EDA\n\n<img src=\".\/feature-info.jpg\">"}}