{"cell_type":{"6ce99323":"code","a435414e":"code","684d313c":"code","12288d57":"code","a361c132":"code","69b29ac1":"code","9585c625":"code","fa45704a":"code","299a407d":"code","fafad383":"code","56ba3f72":"code","c0249dab":"code","d3655580":"code","6cc284db":"code","dfd54bfc":"code","d51ec0b2":"code","ba1ae14e":"code","33aa010a":"code","c6f098f3":"code","966d2559":"code","5f085cc3":"code","87bf1281":"code","efb2b3ee":"code","9b585aa1":"markdown","20dff083":"markdown","2afe9010":"markdown","39273677":"markdown","dba56df6":"markdown","b47b0087":"markdown","99331408":"markdown","741d2234":"markdown","188947f8":"markdown","e9cf8861":"markdown","df4d02ca":"markdown","cef5c565":"markdown","b9f6b1ad":"markdown","d654fe3b":"markdown","0c7e4c9b":"markdown"},"source":{"6ce99323":"import numpy as np\nimport pandas as pd\nimport os\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\nfrom tqdm import tqdm_notebook\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import mean_absolute_error\npd.options.display.precision = 15\n\nimport time\n# Libraries\nimport numpy as np\nimport pandas as pd\npd.set_option('max_columns', None)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\nimport datetime\nimport lightgbm as lgb\nfrom scipy import stats\nfrom sklearn.model_selection import train_test_split, StratifiedKFold, KFold, cross_val_score, GridSearchCV, RepeatedStratifiedKFold\nfrom sklearn.preprocessing import StandardScaler\nimport os\nimport lightgbm as lgb\nfrom sklearn.metrics import accuracy_score, roc_auc_score\nfrom sklearn import metrics\nfrom sklearn import linear_model\nfrom tqdm import tqdm_notebook\nfrom catboost import CatBoostClassifier","a435414e":"sensor = pd.read_csv('..\/input\/sensor.csv')\nsensor.drop(['Unnamed: 0'], axis=1, inplace=True)","684d313c":"sensor.head()","12288d57":"sensor['machine_status'].value_counts()","a361c132":"sensor.drop(['sensor_15'], axis=1, inplace=True)","69b29ac1":"plt.plot(sensor.loc[sensor['machine_status'] == 'NORMAL', 'sensor_02'], label='NORMAL')\nplt.plot(sensor.loc[sensor['machine_status'] == 'BROKEN', 'sensor_02'], label='BROKEN')\nplt.plot(sensor.loc[sensor['machine_status'] == 'RECOVERING', 'sensor_02'], label='RECOVERING')\nplt.legend()","9585c625":"print(sensor.shape)\nsensor['target'] = 0\nprint(sensor.shape)\nsensor.head()","fa45704a":"sensor.loc[sensor['machine_status'] != 'NORMAL', 'target'].value_counts()","299a407d":"sensor.loc[sensor['machine_status'] != 'NORMAL', 'target'] = 1","fafad383":"sensor.loc[sensor['machine_status'] != 'NORMAL', 'target'].value_counts()","56ba3f72":"sensor['target'].value_counts()","c0249dab":"sensor.drop(['machine_status'], axis=1, inplace=True)","d3655580":"sensor.shape","6cc284db":"#before impute\nplt.figure(figsize=(12, 5))\nsns.heatmap(data=sensor.isna(),yticklabels=False,cmap='coolwarm',cbar=False)","dfd54bfc":"for col in sensor.columns[1:-1]:\n    sensor[col] = sensor[col].fillna(sensor[col].mean())","d51ec0b2":"plt.figure(figsize=(12, 5))\nsns.heatmap(data=sensor.isna(),yticklabels=False,cmap='coolwarm',cbar=False)","ba1ae14e":"X = sensor.drop(['timestamp', 'target'], axis=1)\ny = sensor['target']","33aa010a":"n_fold = 5\nfolds = StratifiedKFold(n_splits=n_fold, shuffle=True, random_state=42)","c6f098f3":"def train_model(X, y, params, folds, model_type='lgb', plot_feature_importance=False, averaging='usual', model=None):\n    oof = np.zeros(len(X))\n    scores = []\n    feature_importance = pd.DataFrame()\n    for fold_n, (train_index, valid_index) in enumerate(folds.split(X, y)):\n        print('Fold', fold_n, 'started at', time.ctime())\n        X_train, X_valid = X.loc[train_index], X.loc[valid_index]\n        y_train, y_valid = y[train_index], y[valid_index]\n        scaler = StandardScaler()\n        X_train = scaler.fit_transform(X_train)\n        X_valid = scaler.transform(X_valid)\n        \n        if model_type == 'lgb':\n            train_data = lgb.Dataset(X_train, label=y_train)\n            valid_data = lgb.Dataset(X_valid, label=y_valid)\n            \n            model = lgb.train(params,\n                    train_data,\n                    num_boost_round=1000,\n                    valid_sets = [train_data, valid_data],\n                    verbose_eval=500,\n                    early_stopping_rounds = 200)\n            \n            y_pred_valid = model.predict(X_valid)\n            \n        if model_type == 'xgb':\n            train_data = xgb.DMatrix(data=X_train, label=y_train, feature_names=X_train.columns)\n            valid_data = xgb.DMatrix(data=X_valid, label=y_valid, feature_names=X_train.columns)\n\n            watchlist = [(train_data, 'train'), (valid_data, 'valid_data')]\n            model = xgb.train(dtrain=train_data, num_boost_round=20000, evals=watchlist, early_stopping_rounds=200, verbose_eval=500, params=params)\n            y_pred_valid = model.predict(xgb.DMatrix(X_valid, feature_names=X_train.columns), ntree_limit=model.best_ntree_limit)\n        \n        if model_type == 'sklearn':\n            model = model\n            model.fit(X_train, y_train)\n            y_pred_valid = model.predict_proba(X_valid)[:, 1]\n            score = roc_auc_score(y_valid, y_pred_valid)\n            # print(f'Fold {fold_n}. AUC: {score:.4f}.')\n            # print('')\n            \n            \n        if model_type == 'glm':\n            model = sm.GLM(y_train, X_train, family=sm.families.Binomial())\n            model_results = model.fit()\n            model_results.predict(X_test)\n            y_pred_valid = model_results.predict(X_valid).reshape(-1,)\n            score = roc_auc_score(y_valid, y_pred_valid)\n            \n            \n        if model_type == 'cat':\n            model = CatBoostClassifier(iterations=20000, learning_rate=0.05, loss_function='Logloss',  eval_metric='AUC', **params)\n            model.fit(X_train, y_train, eval_set=(X_valid, y_valid), cat_features=[], use_best_model=True, verbose=False)\n\n            y_pred_valid = model.predict_proba(X_valid)[:, 1]\n            \n        oof[valid_index] = y_pred_valid.reshape(-1,)\n        scores.append(roc_auc_score(y_valid, y_pred_valid))\n        \n        if model_type == 'lgb':\n            # feature importance\n            fold_importance = pd.DataFrame()\n            fold_importance[\"feature\"] = X.columns\n            fold_importance[\"importance\"] = model.feature_importance()\n            fold_importance[\"fold\"] = fold_n + 1\n            feature_importance = pd.concat([feature_importance, fold_importance], axis=0)\n\n\n    \n    print('CV mean score: {0:.4f}, std: {1:.4f}.'.format(np.mean(scores), np.std(scores)))\n    \n    if model_type == 'lgb':\n        feature_importance[\"importance\"] \/= n_fold\n        if plot_feature_importance:\n            cols = feature_importance[[\"feature\", \"importance\"]].groupby(\"feature\").mean().sort_values(\n                by=\"importance\", ascending=False)[:50].index\n\n            best_features = feature_importance.loc[feature_importance.feature.isin(cols)]\n\n            plt.figure(figsize=(16, 12));\n            sns.barplot(x=\"importance\", y=\"feature\", data=best_features.sort_values(by=\"importance\", ascending=False));\n            plt.title('LGB Features (avg over folds)');\n        \n            return oof,  feature_importance\n        return oof,  scores\n    \n    else:\n        return oof,  scores","966d2559":"params = {'num_leaves': 8,\n         'min_data_in_leaf': 42,\n         'objective': 'binary',\n         'max_depth': 5,\n         'learning_rate': 0.01,\n         'boosting': 'gbdt',\n         'bagging_freq': 5,\n         'feature_fraction': 0.8201,\n         'bagging_seed': 11,\n         'reg_alpha': 1,\n         'reg_lambda': 4,\n         'random_state': 42,\n         'metric': 'auc',\n         'verbosity': -1,\n         'subsample': 0.81,\n         'num_threads': 4}\noof_lgb, scores = train_model(X, y, params=params, folds=folds, model_type='lgb', plot_feature_importance=True)","5f085cc3":"%%time\nmodel = linear_model.LogisticRegression(class_weight='balanced', penalty='l2', C=0.1)\noof_lr, scores = train_model(X, y, params=None, folds=folds, model_type='sklearn', model=model)","87bf1281":"oof_lr","efb2b3ee":"np.mean(scores)","9b585aa1":"## **Thank you so much Andrew for helping me on this problem, I am such a newbie to this area and have no friend who working in this area as well. I really appreciate your help**\n\n-----\n\n**My understanding**\n- Classification is used first to prove that we can classify 0 and 1, If the classification can perform well so we can think further about `time series prediction` to predict when the machine will fail again(e.g. next week, next month)\n- `machine_status`, both `recovering` and `breakdown` are in same class since they are not `normal` state.\n\n**Question**\n- Should we put `recovering` and `breakdown` in different class since `recovering` is the state after breakdown ?\n- Should we remove `recovering` from the training set since we want to predict only `normal` and `breakdown` ?\n- Should I impute missing sensor values based on class(e.g. use mean of each class for impute) ?\n- Do we need to focus on `recall` of the model just like we usually do for `cancer detection` problem. ?\n- Which metric to use for scoring, since the data has huge class imbalance {normal: 205836, broken: 7, recovering: 14477} ?\n- How to predict when the machine will fail again(e.g. ARIMA, Holt's winter, LSTM) ?, I have tried LSTM but got bad result, may be caused by target value as 0\/1","20dff083":"### add value = 1 to feature 'target'","2afe9010":"**import dependencies and apply various settings**","39273677":"### plot each machine status against sensor_02","dba56df6":"### imputed missing value with mean of the it's own series","b47b0087":"### Try to use LogisticRegression since it's binary classification, get over 99.9 AUC score","99331408":"### dropped `machine_status` feature","741d2234":"### drop sensor_15 since it's completely missing","188947f8":"### first try with LightGBM, and get auc score over 99.9","e9cf8861":"### create function to train various classifier, this function will return score of model (out of fold)","df4d02ca":"### add one new feature named 'target', this is what model will predict","cef5c565":"### import data as dataframe, drop index column","b9f6b1ad":"### show some part of data","d654fe3b":"### splitted dataset to X and Y","0c7e4c9b":"### create 5 fold for cross validation"}}