{"cell_type":{"fa099d1d":"code","c0cd7fa5":"code","13da7376":"code","5f4f3768":"code","6be633fb":"code","ef7e4c81":"code","72f12777":"code","ae68de54":"code","c5dbc0cd":"code","c6cf54ae":"code","446f352b":"code","067b6894":"code","bd144e9e":"code","984619ee":"code","e6db927a":"code","16747976":"code","ccee1c66":"code","d44ac772":"code","68a4ce63":"code","ab1b2aea":"code","4ef3a82d":"code","18945883":"code","82c9c3b9":"code","7596d035":"code","16730bc3":"code","c4af66ee":"code","41f88e14":"code","e6c59a6e":"code","bf5513ae":"code","81ec6fb2":"code","c6b101d0":"code","e60f53e8":"code","9d3a76cd":"code","627081f0":"code","f7aabc29":"code","d6bbf177":"code","496e56c3":"code","38858428":"code","5f885c7f":"code","e9e040cb":"code","ee15da6f":"code","5f9b1247":"code","ecd363af":"code","80051c9c":"code","4eaf49bc":"code","c2d50721":"code","e4c5d9d4":"code","6ac5e334":"code","c20c398f":"code","5583e56c":"code","b43d65e9":"code","0f3eeba5":"code","19353f92":"code","f1a3c1eb":"code","70158f9b":"code","a790302c":"code","9fdd5dce":"code","1f3d3821":"code","42711311":"code","d62e8bac":"code","ad9aad18":"code","78b00c07":"code","0cf659d6":"code","6d6ccd8a":"code","75c3e4f5":"code","eb38b1cf":"code","fe8d8473":"code","4f61e1fe":"code","8b479962":"code","6ec76fd5":"code","4c359743":"code","d460168e":"code","44c20b91":"code","e5e927d2":"code","7682458c":"code","097c9e9f":"code","a8a11349":"code","85a82876":"markdown","0c39031d":"markdown","9164e77d":"markdown","aca35679":"markdown","226c6bb4":"markdown","486f91e2":"markdown","45cee762":"markdown","9967e552":"markdown","1c92911f":"markdown","544fb2e0":"markdown","fa0406f3":"markdown","afe7766b":"markdown","42fd2ecf":"markdown","4f8a6d38":"markdown","c26670df":"markdown","1e7b5ccc":"markdown","0958ecb0":"markdown","07c9be5f":"markdown","b72f2df2":"markdown","88d85eb3":"markdown","80235e9e":"markdown","d52d6e02":"markdown","12b6f874":"markdown","719c0401":"markdown","3bf501ae":"markdown","e8ba10b4":"markdown","5f7c85d4":"markdown","813fbf03":"markdown","dd2c7665":"markdown","ec43184d":"markdown","614bfac9":"markdown","63503bcb":"markdown","d6641dac":"markdown","10c575a3":"markdown","f803e430":"markdown","7c6626dd":"markdown","63ee4c1b":"markdown","0852294c":"markdown","588b0626":"markdown","26a50419":"markdown","24035243":"markdown","16c0852d":"markdown","8d3c3356":"markdown","7d960940":"markdown","cc8a52b6":"markdown","de30efec":"markdown","33de9841":"markdown","ca3a4163":"markdown","61e6b152":"markdown","b2988a9a":"markdown","94907a07":"markdown","10a51d8a":"markdown","7e66a988":"markdown","1e98608c":"markdown","ee5ffacf":"markdown","a2c04891":"markdown","22cfa4c7":"markdown","74cb0ed9":"markdown","2640b710":"markdown","ecdc7453":"markdown","5529f745":"markdown","ef8d1de8":"markdown","ef54bc45":"markdown","d2c5b83c":"markdown","68b58c2f":"markdown","6a8ca549":"markdown","99e006d6":"markdown","2ae65bb4":"markdown","490ab348":"markdown","60171b21":"markdown","f78964ab":"markdown","32ec380c":"markdown","e3f894cf":"markdown","6c4c0532":"markdown","7df73521":"markdown","e121e113":"markdown","f56f7a1f":"markdown","f58db46a":"markdown","99d4e1bf":"markdown","fb40d232":"markdown","f8f6e2cc":"markdown"},"source":{"fa099d1d":"import pandas as pd\nimport numpy as np\nimport os\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n#tells Jupyter to set up Matplotlib so it uses Jupyter\u2019s own backend \n%matplotlib inline ","c0cd7fa5":"filepath = \"..\/input\/titanic\/train.csv\"\ndata = pd.read_csv(filepath)\ndata.head()","13da7376":"data.shape","5f4f3768":"data.columns","6be633fb":"data.info()","ef7e4c81":"data.describe()","72f12777":"s = data.dtypes == \"object\"\nobject_cols = data.columns[s].tolist()\nobject_cols","ae68de54":"n = (data.dtypes == \"int64\") | (data.dtypes == \"float64\")\nnumerical_cols = data.columns[n].tolist()\nnumerical_cols","c5dbc0cd":"# remove passengerId from dataset\nnumerical_cols.remove(\"PassengerId\")","c6cf54ae":"def get_missing_pct(df):\n    \"\"\"compute the percentage for missing val\"\"\"\n    missing_df = df.apply(lambda col:col.isnull().sum()).to_frame().reset_index().rename(columns={\"index\":\"column\",0:\"missing_cnt\"})\n    IsMissing = missing_df[\"missing_cnt\"]>0\n    missing_df_ = missing_df[IsMissing]\n    missing_df_[\"missing_pct\"] = missing_df[\"missing_cnt\"] \/ df.shape[0]\n    return missing_df_","446f352b":"miss_data = get_missing_pct(data)\nmiss_data","067b6894":"data[\"deck\"] = data[\"Cabin\"].str[0]\ndeck_normalized = {'A':1, 'B':2, 'C':3, 'D':4, 'E':5, 'F':6, 'G':7, 'T':8}\n\ndef encode_cabin(df):\n    df.replace({\"deck\": deck_normalized}, inplace=True)\n    df['deck'].fillna(value=9, inplace=True)        \n\nencode_cabin(data)","bd144e9e":"data_clean = data.drop(\"Cabin\",axis=1)\ndata_clean.head()","984619ee":"age_mis_data = data_clean[data_clean.Age.isnull()]\nage_mis_data.head()","e6db927a":"class_mis_data = age_mis_data.groupby(\"Pclass\",as_index=False).agg({\"PassengerId\":\"count\"}).rename(columns={\"PassengerId\":\"count\"})\nsns.barplot(data=class_mis_data,x=\"Pclass\",y=\"count\",color=\"deepskyblue\")\nsns.set_style(\"whitegrid\")\nplt.title(\"Distribution of Passengers without Age Info\")\nplt.show()","16747976":"sns.boxplot(data=data_clean,x=\"Pclass\",y=\"Age\",palette=\"Blues\")\nplt.title(\"Age Distribution acrsss Three Classes\")\nplt.show()","ccee1c66":"def fill_age(ser):\n    if np.isnan(ser.loc[\"Age\"]) and ser.loc[\"Pclass\"] == 1:\n        return 37\n    elif np.isnan(ser.loc[\"Age\"]) and ser.loc[\"Pclass\"] == 2:\n        return 29\n    elif np.isnan(ser.loc[\"Age\"]) and ser.loc[\"Pclass\"] == 3:\n        return 24\n    else:\n        return ser.loc[\"Age\"]\n\ndata_clean[\"Age\"] = data_clean.apply(lambda row:fill_age(row),axis=1)","d44ac772":"dup_data = data_clean.duplicated(subset=\"PassengerId\").astype(\"int\").sum()\ndup_data","68a4ce63":"import seaborn as sns\n\n# write function to visualize the distribution of variables\ndef plot_histograms(row,col,data,numerical_cols,color,title):\n    fig,axes = plt.subplots(row,col,figsize=(20,15),dpi=100)\n    i = 0\n    for r in range(row):\n        for c in range(col):\n            try:\n                sns.histplot(data=data[numerical_cols[i]],ax=axes[r,c],color=color)\n                text_title = title + numerical_cols[i]\n                i += 1\n                axes[r,c].set_title(text_title)\n            except IndexError:\n                pass\n\n# call the funciton\nplot_histograms(3,2,data,numerical_cols,\"deepskyblue\",\"Histogram of \")","ab1b2aea":"# let's extract these outliers to closely look at these outliers\nf_mark = data[\"Fare\"]>100\nfare_data = data[f_mark]\nfare_data.head()","4ef3a82d":"fare_data.Pclass.value_counts()","18945883":"fare_data.Sex.value_counts()","82c9c3b9":"fare_data.Age.hist(color=\"deepskyblue\")\nplt.show()","7596d035":"# get discrete columns\ndiscrete_cols = [column for column in numerical_cols if data_clean[column].nunique()<=15]\n# combine together discrete columns and categorical columns\ncategorical_columns = [object_cols[1],object_cols[4]] + discrete_cols","16730bc3":"cat_data = data[categorical_columns]\ni = 0\nfig,axes = plt.subplots(3,2,figsize=(20,15),dpi=100)\nfor r in range(3):\n    for c in range(2):        \n        col = categorical_columns[i]\n        try:\n            freq_data = cat_data[col].value_counts().reset_index().rename(columns={\"index\":col,col:\"frequency\"})\n            sns.barplot(data=freq_data,x=col,y=\"frequency\",color=\"deepskyblue\",ax=axes[r,c])\n            axes[r,c].set_title(\"Bar chart of {}\".format(col))\n            i+=1\n        except IndexError:\n            pass\nplt.show()","c4af66ee":"s = data_clean.dtypes == \"object\"\nobject_cols = data_clean.columns[s]\n# Get number of unique entries in each column with categorical data\nobject_cols_ = discrete_cols+object_cols.tolist()\nobject_nunique = list(map(lambda col: data_clean[col].nunique(), object_cols_))\nd = dict(zip(object_cols_, object_nunique))\n\n# Print number of unique entries by column, in ascending order\nsorted(d.items(), key=lambda x: x[1])","41f88e14":"data_clean = data_clean.drop([\"Ticket\"],axis=1)\ndata_clean.head()","e6c59a6e":"plt.figure(figsize=(10,5),dpi=100)\nsns.boxplot(data=data,x=\"Sex\",y=\"Fare\",palette=\"Blues\")\nplt.title(\"Boxplot of Fare by Sex\")\nplt.show()","bf5513ae":"plt.figure(figsize=(10,5),dpi=100)\nsns.boxplot(data=data,x=\"Survived\",y=\"Fare\",palette=\"Blues\")\nplt.title(\"Boxplot of Fare by Survival\")\nplt.show()","81ec6fb2":"def get_columns(data):\n    # extract cat columns\n    s = data.dtypes == \"object\"\n    object_cols = data.columns[s].tolist()\n\n    # extract num columns\n    n = (data.dtypes == \"int64\") | (data.dtypes == \"float64\")\n    numerical_cols = data.columns[n].tolist()\n    # drop target variable from numerical cols\n\n    # find out discrete cols from numerical cols\n    discrete_cols = [num_col for num_col in numerical_cols if data[num_col].nunique()<=15]\n    return object_cols,numerical_cols,discrete_cols\n\nobject_cols,numerical_cols,discrete_cols = get_columns(data_clean)","c6b101d0":"sns.catplot(data=data_clean,x=\"Age\",col=\"Survived\",kind=\"count\")\nplt.xticks(np.arange(data_clean[\"Age\"].min(),data_clean[\"Age\"].max(),10))\nplt.show()","e60f53e8":"ctab = pd.crosstab(index=data_clean[\"Pclass\"],columns=data_clean[\"Survived\"])\nsns.heatmap(ctab,annot=True,fmt=\"d\",cmap=\"Blues\")\nplt.title(\"Passengers Segmented by Survival and Pclass\")\nplt.show()","9d3a76cd":"import re,string\n\ndef extract_title(full_name):\n    \"\"\"return title from full name\"\"\"\n    given_name = full_name.split(\",\")[0]\n    surname_full = full_name.split(\",\")[1].strip()\n    title = surname_full.split(\".\")[0]\n    surname = surname_full.split(\".\")[1].strip()\n    return title\n\n\ndef replace_titles(x):\n    title=x['title']\n    if title in ['Don', \"Dona\",'Major', 'Capt', 'Jonkheer', 'Rev', 'Col',\"Sir\"]:\n        return 'Mr'\n    elif title in ['the Countess', 'Mme']:\n        return 'Mrs'\n    elif title in ['Mlle', 'Ms',\"Lady\"]:\n        return 'Miss'\n    elif title =='Dr':\n        if x['Sex']=='Male':\n            return 'Mr'\n        else:\n            return 'Mrs'\n    else:\n        return title\n    \n    \ndef create_title(data):\n    title_list=['Mrs', 'Mr', 'Master', 'Miss', 'Major', 'Rev',\"Sir\",'Dr',\n                'Ms', 'Mlle','Col', 'Capt', 'Mme', 'the Countess','Don', \n                'Jonkheer',\"Dona\"]\n    data[\"title\"] = data[\"Name\"].apply(lambda x:extract_title(x))\n    data[\"title\"] = data.apply(lambda row:replace_titles(row),axis=1)\n    return data","627081f0":"data_clean = create_title(data_clean)\ndata_clean.title.unique()","f7aabc29":"temp = pd.crosstab(index=data_clean[\"Survived\"],columns=data_clean[\"title\"]).reset_index().set_index(\"Survived\")\nsns.heatmap(temp,annot=True,fmt=\"d\",cmap=\"Blues\")\nplt.show()","d6bbf177":"# extract data of passengers from clas 1 and class 2\nmark = data_clean.Pclass != 3\ndata_class12 = data_clean[mark]\n\n# split continuous age into 6 age bins\ndata_class12[\"age_level\"] = pd.cut(data_class12.Age,6)\n\n# visualize\ntemp = data_class12.groupby([\"Survived\",\"age_level\"]).agg({\"PassengerId\":\"count\"}).reset_index().\\\nrename(columns={\"PassengerId\":\"count\"})\nsns.barplot(data=temp,x=\"age_level\",y=\"count\",hue=\"Survived\",palette=\"Blues\")\nplt.xticks(rotation=42)\nplt.title(\"Survival of Passengers from Class 1 and Class 2\")\nplt.legend(loc=\"best\")\nplt.show()","496e56c3":"data_clean[\"Is_class1_or_class2_age_less_than_14\"] = ((data_clean[\"Pclass\"] != 3) & (data_clean[\"Age\"]<=14)).astype(\"int\")","38858428":"sns.catplot(data=data_clean,x=\"SibSp\",col=\"Pclass\",kind=\"count\",hue='Survived',palette=\"Set1\")\nplt.show()","5f885c7f":"sns.catplot(data=data_clean,x=\"Embarked\",col=\"Pclass\",kind=\"count\",hue='Survived',palette=\"Set1\")\nplt.show()","e9e040cb":"sns.catplot(data=data,x=\"SibSp\",col=\"Survived\",kind=\"count\",palette=\"Blues_d\")\nplt.show()","ee15da6f":"sns.catplot(data=data_clean,x=\"Parch\",col=\"Survived\",kind=\"count\",palette=\"Blues\")\nplt.show()","5f9b1247":"data_clean[\"family_size\"] = data_clean[\"SibSp\"] + data_clean[\"Parch\"]\ncrosstab = pd.crosstab(index=data_clean['Survived'],columns=data_clean[\"family_size\"])\nsns.heatmap(crosstab,annot=True,fmt=\"d\",cmap=\"Blues\")\nplt.show()","ecd363af":"data_clean[\"Is_family_size_less_than_3\"] = ((data_clean[\"family_size\"]<=3) & ((data_clean[\"family_size\"]>0))).astype(\"int\")","80051c9c":"sns.catplot(data=data_clean,x=\"Survived\",col=\"Sex\",kind=\"count\",palette=\"Blues\")\nplt.show()","4eaf49bc":"data_clean[\"Sex\"] = data_clean[\"Sex\"].replace({\"female\":1,\"male\":0})","c2d50721":"data_clean[\"Sex_\"] =  data_clean[\"Sex\"].replace({1:\"female\",0:\"male\"})\nsns.catplot(data=data_clean,x=\"Pclass\",col='Survived',kind=\"count\",palette=\"Blues\",hue=\"Sex_\")","e4c5d9d4":"# cut fare into several data bins\nctab = pd.crosstab(pd.cut(data[\"Fare\"],bins=10),data[\"Pclass\"])\nsns.heatmap(ctab,annot=True,fmt=\"d\",cmap=\"Blues\")\nplt.title(\"Passengers Segmented by Fare and Pclass\")\nplt.show()","6ac5e334":"sns.regplot(data=data,x=\"Fare\",y=\"Age\")\nplt.title('Association between Fare and Age')\nplt.show()","c20c398f":"from string import ascii_letters\n\nsns.set_theme(style=\"white\")\n\n# extract columns\nobj,num,dis = get_columns(data_clean)\n\n# Generate a large random dataset\nrs = np.random.RandomState(33)\nd = pd.DataFrame(data=rs.normal(size=(100, 26)),\n                 columns=list(ascii_letters[26:]))\n\n# Compute the correlation matrix\ncorr = data_clean[num+dis].drop(\"PassengerId\",axis=1).corr()\n\n# Generate a mask for the upper triangle\nmask = np.triu(np.ones_like(corr, dtype=bool))\n\n# Set up the matplotlib figure\nf, ax = plt.subplots(figsize=(11, 9))\n\n# Generate a custom diverging colormap\ncmap = sns.diverging_palette(230, 20, as_cmap=True)\n\n# Draw the heatmap with the mask and correct aspect ratio\nsns.heatmap(corr, mask=mask, cmap=cmap, center=0, annot=True,\n            square=True, linewidths=.5, cbar_kws={\"shrink\": .5})\nplt.show()","5583e56c":"data_clean = data_clean.drop([\"Name\",\"PassengerId\",\"Fare\",\"SibSp\",\"Parch\",\"Sex_\"],axis=1)","b43d65e9":"from sklearn.model_selection import train_test_split\n\n# split data into features and label\nX = data_clean.drop(\"Survived\",axis=1)\ny = data_clean[\"Survived\"]\n\n# then split data into train set and test set\nX_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2,random_state=42,stratify=y)\nprint(X_train.shape,X_test.shape,y_train.shape,y_test.shape)\n\nX_train_full = X_train.copy()\nX_test_full = X_test.copy()","0f3eeba5":"# import necessary packages\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.impute import KNNImputer, SimpleImputer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import MinMaxScaler,StandardScaler, OneHotEncoder, OrdinalEncoder, LabelEncoder\nfrom sklearn.model_selection import GridSearchCV, RandomizedSearchCV, train_test_split\nfrom sklearn.metrics import accuracy_score, f1_score, confusion_matrix","19353f92":"# imputer the missing info for Embarked\nimputer = SimpleImputer(strategy=\"most_frequent\")\nX_train_full = pd.DataFrame(imputer.fit_transform(X_train_full),columns=X_train_full.columns)\nX_test_full = pd.DataFrame(imputer.fit_transform(X_test_full),columns=X_test_full.columns)\n\n# dummy variable\nX_train_full = pd.get_dummies(X_train_full,columns=[\"Embarked\",\"title\"])\nX_test_full = pd.get_dummies(X_test_full,columns=[\"Embarked\",\"title\"])","f1a3c1eb":"from sklearn.preprocessing import RobustScaler\n\nscaler = RobustScaler()\nX_train_full = pd.DataFrame(scaler.fit_transform(X_train_full),columns=X_train_full.columns)\nX_test_full = pd.DataFrame(scaler.fit_transform(X_test_full),columns=X_test_full.columns)","70158f9b":"from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC, LinearSVC\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn import metrics\nfrom sklearn.model_selection import cross_val_predict,cross_val_score\n\nseed = 42\n\nmodels = [\n    ('kNN',KNeighborsClassifier()),\n    ('SVC',SVC(random_state=seed, probability=True)),\n    ('DT',DecisionTreeClassifier(random_state=seed)),\n    ('RF',RandomForestClassifier(random_state=seed)),\n    ('GB',GradientBoostingClassifier(random_state=seed)),\n    ('GauNB',GaussianNB()),\n    ('LR',LogisticRegression(solver='liblinear',max_iter=10000))\n]","a790302c":"def fit_model(X,y,model,cv_fold,metric,plot=False):\n\n    # Fit model on trains set\n    model.fit(X,y)\n\n    # Predict training set\n    train_pred = model.predict(X)\n    train_predprob = model.predict_proba(X)[:,1]\n\n    # Cross-Validation\n    cv_score = cross_val_score(model,X,y,cv=cv_fold,scoring=metric)\n    cv_ypred = cross_val_predict(model,X,y,cv=cv_fold)\n\n    # plot confusion matrix and roc_auc curve\n    if plot:\n\n        # print accuracy\n        train_res = \"Train Accuracy: {:.4f} | \".format(metrics.accuracy_score(y,train_pred))\n        train_res += \"Train AUC Score : {:.4f}\\n\".format(metrics.roc_auc_score(y,train_pred))\n        train_res += \"CV Score: Mean - {0}|std-{1}|Min-{2}|Max-{3}\".\\\n        format(np.mean(cv_score),np.std(cv_score),np.min(cv_score),np.max(cv_score))\n\n\n        # make visualization for prediction\n        from sklearn.metrics import confusion_matrix\n        fig,axes = plt.subplots(1,2,figsize=(16,5),dpi=100)\n        fig.suptitle(\"{} Model Report\".format(model))\n        fig.text(0.5,0,train_res,ha=\"center\",va=\"center\")\n        \n        # confusion maxtrix\n        confusion=metrics.confusion_matrix(y_true=y,y_pred=cv_ypred)\n        sns.heatmap(confusion, annot=True, cmap='Blues', fmt='d',ax=axes[0])\n        axes[0].set_xlabel(\"highPunctuality_predictLogit\")\n        axes[0].set_ylabel(\"highPunctuality\") \n\n        # roc_auc curve\n        from sklearn.metrics import roc_curve\n        fpr, tpr, thresholds = roc_curve(y, cv_ypred)\n        axes[1].plot(fpr, tpr, label='ROC curve (area = %0.3f)' % np.mean(cv_score))\n        axes[1].plot([0, 1], [0, 1], 'k--')  # random predictions curve\n        axes[1].set_xlim([0.0, 1.0])\n        axes[1].set_ylim([0.0, 1.0])\n        axes[1].set_xlabel('False Positive Rate or (1 - Specifity)')\n        axes[1].set_ylabel('True Positive Rate or (Sensitivity)')\n        axes[1].set_title('Receiver Operating Characteristic')\n        axes[1].legend(loc=\"lower right\")\n        \n        print(train_res)\n    \n    return cv_score","9fdd5dce":"# iterate all selected models\nresults = []\nnames = []\nstats = []\n\nfor name,model in models:\n    res = fit_model(X_train_full,y_train,model,10,\"roc_auc\")\n    names.append(name)\n    results.append(res)\n    stats.append([np.mean(res),np.std(res)])\n\n# dataframe \nscores = pd.DataFrame(stats,index=names,columns=[\"mean\",\"std\"])\nscores = scores.sort_values(by=\"mean\",ascending=False)\nscores","1f3d3821":"# visualize the scores of each model\nfig,ax = plt.subplots(1,1,figsize=(15,10),dpi=100)\nsns.boxplot(data=results,palette=\"Set2\")\nax.set_xticklabels(scores.index)\nax.set_title(\"Performance of Each Machine Learning Algorithm\")\nplt.show()","42711311":"fit_model(X_train_full,y_train,LogisticRegression(max_iter=10000),10,\"roc_auc\",plot=True)","d62e8bac":"def cross_validation_eval(model,model_name,x,y,IsTrain=True):\n    pred_lg =cross_val_score(model,x,y,cv=10,scoring=\"roc_auc\")\n    \n    if IsTrain:\n        print(\"Train Set Accuracy: \")\n        print(\"----------------------------\")\n        print(\"{0}: {1}\".format(model_name,round(pred_lg.mean(),3)))\n    else:\n        print(\"Test Set Accuracy: \")\n        print(\"----------------------------\")\n        print(\"{0}: {1}\".format(model_name,round(pred_lg.mean(),3)))\n        \ncross_validation_eval(LogisticRegression(max_iter=10000),\"Logistic Regression\",X_train_full,y_train)","ad9aad18":"fit_model(X_train_full,y_train,GradientBoostingClassifier(random_state=42),10,\"roc_auc\",plot=True)","78b00c07":"rf_clf = RandomForestClassifier().fit(X_train_full,y_train)\ntemp = pd.DataFrame({\"Feature Name\":X_train_full.columns,\n              \"Feature Importance\":rf_clf.feature_importances_}).sort_values(by=\"Feature Importance\")\ntemp.plot(x=\"Feature Name\",y=\"Feature Importance\",kind=\"barh\",color=\"deepskyblue\")\nplt.title(\"Feature Importance of Random Forest\")\nplt.legend(loc=\"best\")\nplt.show()","0cf659d6":"from string import ascii_letters\n\nsns.set_theme(style=\"white\")\n\ncorr = pd.concat([X_train_full,y_train.reset_index().drop(\"index\",axis=1)],axis=1).corr()\n\n# Generate a large random dataset\nrs = np.random.RandomState(33)\nd = pd.DataFrame(data=rs.normal(size=(100, 26)),\n                 columns=list(ascii_letters[26:]))\n\n\n# Generate a mask for the upper triangle\nmask = np.triu(np.ones_like(corr, dtype=bool))\n\n# Set up the matplotlib figure\nf, ax = plt.subplots(figsize=(11, 9))\n\n# Generate a custom diverging colormap\ncmap = sns.diverging_palette(230, 20, as_cmap=True)\n\n# Draw the heatmap with the mask and correct aspect ratio\nsns.heatmap(corr, mask=mask, cmap=cmap, center=0, annot=True,\n            square=True, linewidths=.5, cbar_kws={\"shrink\": .5})\nplt.show()","6d6ccd8a":"X_train_full_1 = X_train_full.drop([\"Age\",\"Embarked_Q\",\"title_Master\"],axis=1)\nX_test_full_1 = X_test_full.drop([\"Age\",\"Embarked_Q\",\"title_Master\"],axis=1)","75c3e4f5":"# logsitc\nfit_model(X_train_full_1,y_train,LogisticRegression(max_iter=10000),10,\"roc_auc\",plot=True)","eb38b1cf":"# Gradient Boost\nfit_model(X_train_full_1,y_train,GradientBoostingClassifier(random_state=42),10,\"roc_auc\",plot=True)","fe8d8473":"from sklearn.model_selection import GridSearchCV\n\ngrid_test1 = {\n    \"max_iter\":list(range(1000,11000,1000)),\n    \"solver\":[\"liblinear\",\"lbfgs\",\"saga\",\"sag\"],\n    \"C\":[0.01,0.05,0.06,0.07,0.09,0.1,1,10],\n    \"penalty\":[\"l1\",\"l2\"]\n}\n\nlg_search1 = GridSearchCV(LogisticRegression(random_state=42),grid_test1,cv=10,verbose=0,scoring=\"roc_auc\")\nlg_search1.fit(X_train_full_1,y_train)\n\nprint(lg_search1.best_score_,lg_search1.best_params_)\n","4f61e1fe":"lg_rf = LogisticRegression(max_iter=10000,C=1,penalty=\"l2\",solver=\"lbfgs\")\nfit_model(X_train_full_1,y_train,lg_rf,10,\"roc_auc\",plot=True)","8b479962":"# Logistic Regression\ncross_validation_eval(LogisticRegression(max_iter=10000,C=1,penalty=\"l2\",solver=\"lbfgs\"),\n                     \"Logistic Regression\",X_test_full_1,y_test,IsTrain=False)","6ec76fd5":"cwd = os.getcwd()\ntest_path = \"..\/input\/titanic\/test.csv\"\ntest_data = pd.read_csv(test_path)\ntest_data.head()","4c359743":"test_data[\"deck\"] = test_data[\"Cabin\"].str[0]\ndeck_normalized = {'A':1, 'B':2, 'C':3, 'D':4, 'E':5, 'F':6, 'G':7, 'T':8}\n\ndef encode_cabin(df):\n    df.replace({\"deck\": deck_normalized}, inplace=True)\n    df['deck'].fillna(value=9, inplace=True)        \n\nencode_cabin(test_data)","d460168e":"# drop a series of variables\ntest_data_clean = test_data.drop([\"PassengerId\",\"Cabin\",\"Ticket\",\"Fare\"],axis=1)\ntest_data_clean[\"Sex\"] = (test_data_clean[\"Sex\"]==\"female\").astype(\"int\")","44c20b91":"# expand 1\ntest_data_clean[\"Is_class1_or_class2_age_less_than_14\"] = ((test_data_clean[\"Pclass\"] != 3) & (test_data_clean[\"Age\"]<=14)).astype(\"int\")\n\n# exapnd2\ntest_data_clean[\"family_size\"] = test_data_clean[\"SibSp\"] + test_data_clean[\"Parch\"]\ntest_data_clean[\"Is_family_size_less_than_3\"] = ((test_data_clean['family_size'] >0) & (test_data_clean[\"family_size\"]<=3)).astype(\"int\")","e5e927d2":"test_data_clean = create_title(test_data_clean)\ntest_data_clean.title.unique()","7682458c":"test_data_clean = test_data_clean.drop([\"Name\",\"SibSp\",\"Parch\",\"Age\"],axis=1)\ntest_data_clean = pd.get_dummies(test_data_clean,columns=[\"Embarked\",\"title\"])\n# then scale the dataset\ntest_data_clean = pd.DataFrame(scaler.fit_transform(test_data_clean),columns=test_data_clean.columns)\n# drop Embarked_Q\ntest_data_clean = test_data_clean.drop([\"Embarked_Q\",\"title_Master\"],axis=1)","097c9e9f":"# concat train and test\nX_full_data = pd.concat([X_train_full_1,X_test_full_1])\ny_full_data = pd.concat([y_train,y_test])","a8a11349":"# prediction\nlg_rf_full = LogisticRegression(max_iter=10000,C=1,penalty=\"l2\",solver=\"lbfgs\").\\\nfit(X_full_data,y_full_data)\nprediction_res = lg_rf_full.predict(test_data_clean)\nprediction_res = pd.DataFrame({\"PassengerId\":test_data[\"PassengerId\"],\"Survived\":prediction_res})\nprediction_res = prediction_res.set_index(\"PassengerId\")\nprediction_res.to_csv(\"log_pred_res.csv\")","85a82876":"#### Age Missing Examination\n\nTake a closer look at where these missing observations are lcoated.","0c39031d":"Yes, most of passengers without age are those from class 3. If we want to impute the age info for these passengers, we need to know what is the average\/median age for the three classes.\n\n*Question 2: What is the distribution of age across the three cabins?*","9164e77d":"**Feature Scaling**","aca35679":"The result shows that there is no duplicated record in the dataset. We move on to the examniation of outliers.","226c6bb4":"Logistic regression have the highest roc_auc score, followed by GradientBoost Classifier. But the height of GB is much less than that of logistic regression. \n\nTo determine which model performs best in the setting, I perform gridsearch to the two algorithms and use cross validation to compare them.","486f91e2":"Of these object columns, Sex, Cabin, and Embarked are categorical variables used for segmenting data and needed to be preprocessed before modeling. However, Name and Ticket seems not provide useful information for our predictive modeling.","45cee762":"**Insight:**\n\n- The difference between Survived and death narrows when the size of family increases, but the trend changes radically after size 3.\n    - With no family members, most of passengers died\n    - With one family member, the number of passenegrs perishing basically is equal to that of passengers surviving.\n    - With two or three family members, number of people surviving surpasses that of people perishing.\n- The number of people perishing begin to exceed that of people surviving when the size of family is greater than 0 but smaller or equal to 3.\n\n**Therefore, the family size, to some extent, has some subtle relations with survival. I will add one variable to indicate whether the size of family is smallter than or equal to 3 but greater than 0**","9967e552":"# Data Loading","1c92911f":"No remarkable patterns can be found when looking at survival from different ages.","544fb2e0":"The roc_auc scores of two models improve after `Age`, `Embarked_Q`, and `title_Master` are dropped.","fa0406f3":"### Inspect Duplicated Values\n\nIt is obvious that each PassengerId is the primary key for the dataset, so we can use it to check whether there is any duplicated record in the dataset.","afe7766b":"This is a typical small dataset, with only 891 observations and 12 variables.","42fd2ecf":"# Exploratory Data Analysis\n\nThere are two quesitons we need to inspect when conducting EDA on dataset:\n\n- What type of variation occurs in variables?\n- What type of covariation occurs between variables?\n____\nFramework:\n- Understand Varaibles\n    - Inspect Missing Observations\n    - Inspect Duplication\n    - Inspect Outliers\n- Variation\n- Covariation","4f8a6d38":"All of these passengers are from class 1. Now see the sex, age, survival.","c26670df":"### Pclass & Embarked\n","1e7b5ccc":"**Insights:**\n\n- In either segment, survival or die, most of passengers belong to the category of 0, indicating no association between SibSp and Survival. I will conduct feature selection to decide whether this variable should be included in model.","0958ecb0":"# Data Preprocessing & Feature Engineering\n\n- First, drop PassengerId and Name from dataset. \n- Then perform feature engineering on the dataset to process it so that we can implement machine learning algorithms.\n- For title, as Mr and Mrs are strongly associated with sex so I drop it from dataset.","07c9be5f":"**Interpretation:**\n\n- Sex is highly correlated with Survived, meaning that sex of passengers has huge impact on whether passengers can survive; In contrast, SibSp, Parch, and Age seemingly did not influence the survival of passengers. \n- Fare and Pclass is strongly correlated with each other, meaning that one of them does not provide additional new information for model if we keep them in models. So I will drop Fare from dataset.\n- Age is negatively associated with Pclass, meaning that I can infer the missing age based on Pclass.\n- The additional added variables, such as `Is_family_size_less_than_3`, is relatively correlated with survival.\n\n**Summary**\n\n- **Variation**\n    - **Missing**\n        - Cabin has more than 70% of observations being missing, and has as many as 147 unique values. So I will delete the variable.\n        - Age and Embarked have much smaller proportion of missing values, so I decided to impute them with certain values.\n    - **Distribution.** Most of variables have skewed distribution, including target variable. We have to use machine learning algorithms capable of handling with the situation to make accurate predictions.\n    - **Outlier.** Fare has a number of outliers that can make machine learning algorithms hard to identify patterns from data.\n- **Covariation**\n    - Sex is a huge influencer on survial.\n    - Pclass is correlated with fare, so fare will be deleted.\n    - Age can be inferred by Pclass","b72f2df2":"Drop `Name, SibSp, Parch, Age.`","88d85eb3":"**Finally, I decided to use Logistic Regression to make prediction.**","80235e9e":"#### Pclass and Age\n\nThrough missing checking, I have found that most of pasengers without age information center on class 3. So I decided to check the survival of passengers from class 1 and class 2 because most of they have full information.\n\n*Question: What is the survival situation for passengers from class 1 and class2. Is age an influencer on the survival? Or Are young people more likely to survive or not?*","d52d6e02":"### Age and Survial","12b6f874":"### Outliers Checking\n\nPerhaps some observations are remarkably different from others and these outliers can make machine learning algorithms hard to discover patterns behind data. We might need to consider dropping these oueliers due to logging errors.\n\n#### Inspect the difference in fare by Sex","719c0401":"**Insights**:\n\n- Obviously, passengers surviving in the disaster have higher fare than those dying.\n- Of survived passengers, one passenger, which is a remarkable outlier, even spend more than 500 on fare.\n\n","3bf501ae":"By missing values checking, I found `cabin`, `age`, and `Embarked` have observations being missing. Of these variables, `cabin` have more than 70% of observations being missing, so I will drop it. For `Age`, I decided to examine the relation between it and other variables to decide how to fill the missing information.\n\n### Age Versus Fare","e8ba10b4":"**Titanic - Machine Learning from Disaster Project**\n\n\nVariable Definition:\n- sibsp: The dataset defines family relations in this way...\n    - Sibling = brother, sister, stepbrother, stepsister\n    - Spouse = husband, wife (mistresses and fianc\u00e9s were ignored)\n\n\n**Framework**\n\n- **Data Loading.** Quickly examine the dataset to get a picture\n- **Exploratory Data Analysis.** Get a thorough understanding of dataset\n- **Data Preprocessing.**\n- **Feature Engineering.**\n- **Model Selection and Evaluation.**\n\n____","5f7c85d4":"## Logistic Regression\n\nThe logistic regression performs best but has high variance. First I perform gridsearch on it and examine the train accuracy and test accuracy. ","813fbf03":"### Correlation Coefficient","dd2c7665":"### Family Size & Survival","ec43184d":"Plot the distribution of these categorical variables","614bfac9":"## Feature Selection","63503bcb":"### SipSp & Survival","d6641dac":"Then we take a look at the correlation matrix of all variables to see the pair-wise relation.","10c575a3":"### Sex &  Survival\n\nI segmented the data by sex and count the number of male passengers who survived and the quantity of female passengers who survived. ","f803e430":"**Insights:**\n\n- Cabin has more than 70% of observations being missing from the dataset. This implies that we can consider dropping it.\n- For Age and Embarked, we can consider imputing missing values with some number or add a new column showing the location of entires of the imputed entries.","7c6626dd":"## Understand Variables\n\nWhat variables are numerical features and what variables are categorical features?","63ee4c1b":"**Insights:**\n\n- Passenger are extremely likely to perish if they are male and live in class 3.","0852294c":"Fit models again.","588b0626":"**Add one variable to represent the title of each passenger.**","26a50419":"No difference can be found between people who survived and people who failed to survive. Feature selection should be implemented to decide whether `Parch` should be included in model.","24035243":"**Interpretation:**\n\nYoung people tend to buy cheap tickets, while expensive tickets were bought by ederly people. But this plot cannot reveal any information about survival.\n\nNow examine the relations between all variables.","16c0852d":"#### Inspect the difference in fare by Survived\n\nLet's see whether survived passengers spend a lot more on Fare than passengers who failed to survive.","8d3c3356":"### P-class and Survival\n\n*Question: Whether people with high social status are more likley to survive?*","7d960940":"### Distribution of Variables\n\nPlotting the distribution of variables to find out frequent values, less-common values, and outliers.","cc8a52b6":"- Create one variable to indicate whether the family size is greater than 0 but smallter or equal to 3.\n- Create one variable to indictae whether the age of passengers is less than 14 and class of them is 1 or 2.","de30efec":"**Insights:**\n\n- Male passengers are more than female passengers\n- passengers mainly embarked from Southampton\n- Most of pasengers died in the disaster\n- Passengers in Titanic are mainly at low socio-economic status\n- Most of children travel with their nanny instead of parents\n\nNow let's take a look at the number of categories in each categorical variable. Note: if a categorical variable takes on more than 15 different values, one-hot encoding usually does not perform well on the variable. ","33de9841":"Drop corresponding variables from dataset.","ca3a4163":"**A series of information can be gained by looking at the info result:**\n\n- Several passengers did not log in their age information, leading to some missing values in age variables.\n- More than 70% of observations are missing in Cabin. The enormous amount of missing info implies we might need to drop it from dataset.\n- Numerical features include Survived(binary), Pclass(discrete), Sex(binary), Age(continuous), SibSp(binary), Parch(binary), Fare(continuous).\n- Categorical features include Cabin, Embarked.","61e6b152":"**Insights**:\n\n- The median fare of female passengers is higher than that of male passengers.\n- Male segment has more outlying observations than female segment, and more outliers can make mean value of male segment greater than that of female segment.","b2988a9a":"### Parch & Survival","94907a07":"**Insights:**\n\n- Passengers with higher than 100 fare are basically aged between 15 and 45.\n\n\nHere I proposed one assumption based on the finding and examine it in covariation section. \n\n**Assumption:**\n- Passengers with higher socio-economic status are more likely to survive\n- Pclass is associated with fare","10a51d8a":"## Variation","7e66a988":"Here most of such passengers are at upper socio-economic status, from Cherbourg(France) and Southampton(British), and survived from the disaster. It seems like that the likelihood of survival is related to people's socio-economic status.\n\nMake a bar chart to see the features of passengers spending a lot on fare.","1e98608c":"**Insights**:\n\n- The distribution of Survived is obviously skewed, with people dying half more than that people surviving. If the dataset is imbalanced, then `stratify` needs to be used when splitting dataset so we can ensure the same percentage samples each target class as the complete set.\n- Age has an approximately normal distirbution with slight positive skewness: the majority of passengers are aged between 20 and 40, and passengers who are 40 years or older are more than young people, who are less than 20 years old. We can examine the survival of young people.\n- SibSp has eight unique values, but most of observations focus on 0, 1, and 2.\n- Parch are mainly cemtered about 0, most of children travelling with only nanny. \n- Fare has severe positive skewness, with most of tickets priced from 10 to 30, and only very few passengers having fare higher than 100.\n\nWe filter out observations with higher than 100 fare, and examine any new information.","ee5ffacf":"# Model Selection\n\nI will test a range of classifiers to decide which one machine learning algorithm is worth GridSearching. And I will use roc_auc score as the core metric and cross validation to examine the performance of models.\n\nBase machine learning algorithms include:\n- Linear Models\n    - Logistic Regression\n    - Support Vector Machine\n- K-Nearest Neighbors\n- Naive Bayes\n- Tree-based Models\n    - Decision Tree Classifier\n    - Random Forest\n    - XGboost","a2c04891":"Passengers with low status are mainly centered around the range of (0,100), while only wealthy people can be found in bins of higher than 100. To some extent, the plot indicates the association between fare and pclass.","22cfa4c7":"### Fare and Pclass\n\n***Hunch: fare is highly correlated with pclass.***","74cb0ed9":"Convert Sex to binary variable","2640b710":"**Insights:**\n\n- For people who failed to survive, an obvious pattern can be found that powerful passengers are much less than people with low socio-economic status.\n- For people who survived, the number of people with high status is still greater than people in other two pclasses.\n\nThe visualization helps to confirm the survival is related to people's economic status. A person's name may also contain informaiton about people's social status, such titles prefixing their name. Here I search the meaning of a variety of common titles on WikiPedia:\n\n- *Master*. for boys and young men, or as a style for the heir to a Scottish peerage. It may also be used as a professional title, e.g. for the master of a college or the master of a merchant ship.\n- *Mr* for men, regardless of marital status, who do not have another professional or academic title. The variant Mister, with the same pronunciation, is sometimes used to give jocular or offensive emphasis, or to address a man whose name is unknown.\n- Miss: for girls, unmarried women and (in the UK) married women who continue to use their maiden name (although \"Ms\" is often preferred for the last two). In the UK, it has traditionally been used in schools to address female teachers, regardless of marital status. It is also used, without a name, to address girls or young women and (in the UK) to address female shop assistants and wait staff.\n- Mrs:  for married women who do not have another professional or academic title, an abbreviation of Mistress. The variant Missus is used in the UK to address a woman whose name is unknown. There are examples of professional women who were unmarried using the title Mrs, such as Mrs Crocombe, the Cook at Audley End House in the late 19th century.","ecdc7453":"### Typical Values\n\n- For categorical variables, I plot bar chart to indicate the most frequently occurring categorty.\n- For continuous variables, I first cut it into several data bins and then visualize them by bar charts.","5529f745":"## Grid Search\n\n### Logistic Regression","ef8d1de8":"### Inspect Missing Values\n\nCheck how many missing values each variable in the dataset has.","ef54bc45":"## Covariation\n\nI proposed an assumption in outliers section: survival of passengers is likely to be associated with socio-economic status. So the covariation description will address this issue and examine relationships between variables.","d2c5b83c":"**Feature Engineering**","68b58c2f":"Name, of course, is not related to our predictive modeling, and the unique values of Cabin and Ticket are way more than 15. So we will not apply **one-hot encoding** on them when processing them for machine learning algorithm. The two variables, I think, should be deleted.","6a8ca549":"From the result, we can find that `Age` is actually not correlated with Survival of passengers but has the highesy feature importance. So the result will be biased if we add `Age` in model. Therefore, I decided to drop `Age` from dataset and fit new models.\n\nAlso, `Embarked_Q` and `title_Master` have both low feature importance and not correlated with target variable. So the variable is also dropped from the dataset.","99e006d6":"GridSearch seems not to improve the model at all. I will fix the value of parameters.\n\nUse test set to check the accuracy of model.","2ae65bb4":"#### Sex & Pclass","490ab348":"**Interpretation:**\n\n- The oldest passenger was 80 years old.\n- There is a huge range in Fare, with maximal value being 512.3 and the minial value being 0. 0 means that certain passengers did not spend any money on travel. They should be crew of Titanic.","60171b21":"# Make Prediction Using Test Data\n\n- Preprocess Test Data\n- Logistic Regression\n- Gradient Classifier","f78964ab":"**Insights:**\n\n- For passenger from class 1, those having no spouse\/sibling are more likely to survive.\n- For passenger from class 1, those having one spouse\/sibling mostly survive.\n- From class 3, lonely passengers are much more likley to die than passengers having one or more than one SibSP.","32ec380c":"Take a quick look at the dataset","e3f894cf":"#### Pclass & SibSp\n\n*Question: what is the survival rate for passengers in different classes and having spouses or siblings.*","6c4c0532":"#### Name & Survival\n\nExtract title from the name of passenegrs.","7df73521":"**Interpretation:**\n\n- Passengers with Mr title mostly died in the disaster, only 15% of them survived, indicating that Male is more likley to die.\n- Most of passengers with Mrs title survived from the the wreckage, almost 78%, suggesting that Passengers with Mrs have higher probability of survival.\n- Approximatlely 70% of passengers with Miss title survived from the disaster.\n\n**Summary**\n\nOther than passengers with Mr title, passengers with other titles who survived are more than those dying.","e121e113":"By looking at the dataset, It is obvious that passengers without logging age are from class 3 and do not have spouses\/children and parents. Let's visualize these records so we can look at their info in detail.\n\n#### Age Versus Pclass\n\n*Question 1: Does passengers without age informaiton basically are in class 3?*","f56f7a1f":"**Interpretation:**\n\n- Richer people from top class tend to be older\n    - Passengers in first class tend to be around 37 years old.\n    - Passengers in second class tend to be around 29 years old.\n    - Passengers in the third class tend to be centered around 24 years old.\n    \nI decided to use the insights to fill missing values of age.","f58db46a":"**Interpretation:**\n\n- For male passengers, most of them failed to survive\n- For female passengers, survived passengers are more than those dying.\n- Sex is a strong indicator of survival, meaning that Sex is a good feature that can help us to predict whether a passenger can survive from the disaster.\n\n**I will encode sex variable from text to numbers, with female being 1 and male being 0, because female passengers are more likely to survive.**","99d4e1bf":"**Interpretation:**\n\n- Coming from class 1 and class 2, Passegers who are less than 14 years old basically all survived.\n- For other age bands, no obvious patterns can be found via the graph.\n\nThe insight suggests that the survival of passengers are correlated with whether they are very young and from class 1 or class2. So I decided to create one variable to indicate the conditions.","fb40d232":"## Logistic Regression Prediction using Test Data","f8f6e2cc":"PassengerId needs to be dropped when training model."}}