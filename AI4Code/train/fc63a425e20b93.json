{"cell_type":{"6d1cc426":"code","211795ce":"code","8682aba2":"code","2642a4af":"code","ce8fe2ea":"code","a6fe72b1":"code","1850e1ae":"code","01c61e4d":"code","19ae6505":"code","e3375c43":"code","180a8627":"code","be0189a6":"code","d7b596da":"code","d5c377e7":"code","313b0918":"code","635fc88e":"code","4a888c20":"code","02a0da2a":"code","3fc78587":"code","3fed1be0":"code","1bff25f9":"code","d90f68ac":"code","74599ad3":"code","1fc1008d":"code","e6057e8e":"code","55587efd":"code","9febde2a":"code","62a280c8":"code","5ef4b572":"code","dbe338ef":"code","075b2453":"code","01cee543":"code","a6e4c1f1":"code","8b54f079":"code","bc8f4b3b":"code","fa572198":"code","058c0144":"code","e7e3ed71":"code","d3eefada":"code","aa782708":"code","25709be1":"code","fbaa861f":"code","b15d13f8":"code","291a508b":"code","d79c6cb2":"code","178ba5be":"code","aaf5612c":"code","c5024fd2":"code","498c04a4":"markdown","961dc9cf":"markdown","b4dcde7c":"markdown","28fbd5cd":"markdown","5fe7dc5c":"markdown","d156e82d":"markdown","28062379":"markdown","8f6d1245":"markdown","54a55708":"markdown","f3407baa":"markdown","6ffddb27":"markdown","ec61bb9e":"markdown","5eed37ec":"markdown","2b76d363":"markdown","118b47c9":"markdown","a39dc134":"markdown","c4ebb6e9":"markdown","161109c8":"markdown","78a1371a":"markdown","7223b1e3":"markdown","1ef9fccf":"markdown","96295182":"markdown"},"source":{"6d1cc426":"import pandas as pd","211795ce":"pip install openpyxl","8682aba2":"df = pd.read_excel(\"..\/input\/hslscrape\/hasil_scrape.xlsx\",engine='openpyxl')","2642a4af":"df.columns = df.iloc[0]\ndf = df[1:]\n\ndf.head()","ce8fe2ea":"df","a6fe72b1":"df.info()","1850e1ae":"data_tokped = df[['Nama_Produk', 'Akun', 'Ulasan' ,'Rating_komentar']]\n\ndata_tokped.head()","01c61e4d":"data_tokped","19ae6505":"data_tokped['Rating_komentarsp'] = data_tokped['Rating_komentar'].str.split(' ').str[1]","e3375c43":"data_tokped['rate'] = data_tokped['Rating_komentarsp'].str.replace(r'data-testid=\"icnGivenRatingFilter','').str.replace(r'\\s+', ' ')","180a8627":"data_tokped['rate'] = data_tokped['Rating_komentarsp'].astype(str).str[-2]","be0189a6":"data_tokped","d7b596da":"data = data_tokped.drop(['Rating_komentar', 'Rating_komentarsp'], axis=1)\n\ndata.head()","d5c377e7":"data","313b0918":"import nltk","635fc88e":"nltk.download('all')","4a888c20":"data['Ulasan'] = data['Ulasan'].astype(str)","02a0da2a":"data['Ulasan'] = data['Ulasan'].str.lower()\n\n\ndata","3fc78587":"import string \nimport re \nfrom nltk.tokenize import word_tokenize \nfrom nltk.probability import FreqDist\n\ndef remove_links(text):\n    # menghapus tab, new line, ans back slice\n    text = text.replace('\\\\t',\" \").replace('\\\\n',\" \").replace('\\\\u',\" \").replace('\\\\',\"\")\n    # menghapus non ASCII (emoticon, chinese word, .etc)\n    text = text.encode('ascii', 'replace').decode('ascii')\n    # menghapus mention, link, hashtag\n    text = ' '.join(re.sub(\"([@#][A-Za-z0-9]+)|(\\w+:\\\/\\\/\\S+)\",\" \", text).split())\n    # menghapus URL\n    return text.replace(\"http:\/\/\", \" \").replace(\"https:\/\/\", \" \")\n                \ndata['Ulasan'] = data['Ulasan'].apply(remove_links)","3fed1be0":"#menghapus number\ndef remove_number(text):\n    return  re.sub(r\"\\d+\", \" \", text)\n\ndata['Ulasan'] = data['Ulasan'].apply(remove_number)","1bff25f9":"#menghapus punctuation\ndef remove_punctuation(text):\n    return text.translate(str.maketrans(\"\",\"\",string.punctuation))\n\ndata['Ulasan'] = data['Ulasan'].apply(remove_punctuation)","d90f68ac":"# menghapus single char\ndef remove_singl_char(text):\n    return re.sub(r\"\\b[a-zA-Z]\\b\", \" \", text)\n\ndata['Ulasan'] = data['Ulasan'].apply(remove_singl_char)","74599ad3":"# Tokenisasi \ndef word_tokenize_wrapper(text):\n    return word_tokenize(text)\n\ndata['Ulasan_tokenize'] = data['Ulasan'].apply(word_tokenize_wrapper)","1fc1008d":"data","e6057e8e":"# Menghitung Distibusi Persebaran Kata\ndef freqDist_wrapper(text):\n    return FreqDist(text)\n\nUlasan_fqsist = data['Ulasan_tokenize'].apply(freqDist_wrapper)\n\nprint('Frequency Tokens : \\n') \nprint(Ulasan_fqsist.head().apply(lambda x : x.most_common()))","55587efd":"slank_word_dict = {\n    \"keduakali\" : \"kedua kali\",\n    \"agak\" : \"sedikit\",\n    \"pas\" : \"saat\",\n    \"produkx\" : \"produknya\",\n    \"sukamkasih\" : \"suka makasih\",\n    \"gak\" : \"tidak\",\n    \"cpt\" : \"cepat\",\n    \"sdh\" : \"sudah\",\n    \"recommend\" : \"rekomendasi\",\n    \"bagusseller\" : \"bagus\",\n    \"bagusssss\" : \"bagus\",\n    \"bagussssss\" : \"bagus\",\n    \"baguuussss\" : \"bagus\",\n    \"cepatmakasih\": \"cepat\",\n    \"cepatmantappp\": \"cepat\",\n    \"cepatproduct\": \"cepat\",\n    \"cepatrecomended\": \"cepat\",\n    \"cepattoko\": \"cepat\",\n    \"recomended\": \"rekomendasi\",\n    \"recommended\": \"rekomendasi\",\n    \"rekomendasi\": \"rekomendasi\",\n    \"rekomended\": \"rekomendasi\",\n    }\n\ndef slank_normalized_term(document):\n    return [slank_word_dict[term] if term in slank_word_dict else term for term in document]","9febde2a":"normalizad_word = pd.read_csv(\"..\/input\/hslscrape\/kamus_alay.csv\")\n\nnormalizad_word_dict = {}\n\nfor index, row in normalizad_word.iterrows():\n    if row[0] not in normalizad_word_dict:\n        normalizad_word_dict[row[0]] = row[1] \n\ndef normalized_term(document):\n    return [normalizad_word_dict[term] if term in normalizad_word_dict else term for term in document]","62a280c8":"data['Ulasan_normalized'] = data['Ulasan_tokenize'].apply(normalized_term).apply(slank_normalized_term)","5ef4b572":"data","dbe338ef":"from nltk.corpus import stopwords","075b2453":"list_stopwords = stopwords.words('indonesian')","01cee543":"#remove stopword pada list token\ndef stopwords_removal(words):\n    return [word for word in words if word not in list_stopwords]\n\ndata['Ulasan_stop_removed'] = data['Ulasan_normalized'].apply(stopwords_removal) ","a6e4c1f1":"data","8b54f079":"pip install Sastrawi","bc8f4b3b":"pip install swifter","fa572198":"# import Sastrawi package\nfrom Sastrawi.Stemmer.StemmerFactory import StemmerFactory\nimport swifter\n\n\n# create stemmer\nfactory = StemmerFactory()\nstemmer = factory.create_stemmer()\n\n# stemmed\ndef stemmed_wrapper(term):\n    return stemmer.stem(term)\n\nterm_dict = {}\n\nfor document in data['Ulasan_stop_removed']:\n    for term in document:\n        if term not in term_dict:\n            term_dict[term] = ' '\n            \nprint(len(term_dict))\nprint(\"------------------------\")\n\nfor term in term_dict:\n    term_dict[term] = stemmed_wrapper(term)\n    print(term,\":\" ,term_dict[term])\n    \nprint(term_dict)\nprint(\"------------------------\")\n\n\n# apply stemmed term to dataframe\ndef get_stemmed_term(document):\n    return [term_dict[term] for term in document]\n\ndata['Ulasan_Stemmed'] = data['Ulasan_stop_removed'].swifter.apply(get_stemmed_term)","058c0144":"data","e7e3ed71":"data","d3eefada":"data[\"Ulasan_clean\"] = [' '.join(map(str, l)) for l in data['Ulasan_Stemmed']]","aa782708":"data","25709be1":"data.to_excel(\"tokped_text.xlsx\")","fbaa861f":"ulasan = ' '.join(str(v) for v in data['Ulasan_clean'])","b15d13f8":"tokenize_ulasan = word_tokenize(ulasan)","291a508b":"tokenize_ulasan","d79c6cb2":"fqdist = FreqDist(tokenize_ulasan)","178ba5be":"fqdist","aaf5612c":"# mencetak 15 kata paling banyak digunakan\nfqdist.most_common(15)","c5024fd2":"import matplotlib.pyplot as plt\n\n# plotting\nfqdist.plot(10,cumulative=False)\nplt.show()","498c04a4":"# Word Distribution","961dc9cf":"5. Stemming ( Mencari kata dasar dari sebuah kata\/menghapus imbuhan kata )","b4dcde7c":"Membuat variable baru yaitu data untuk menyimpan kolom yang sudah dibersihkan yaitu terdapat kolom nama produk, akun, ulasan, rating.","28fbd5cd":"Hasil scraping didapatkan ada 7 kolom yaitu kolom web-scraper-order, web-scraper-start-url, nama_produk, akun, ulasan, rating_komentar, dan nan. Data yang di scrape sebanyak 300 data dan bertipe data object.","5fe7dc5c":"Setelah proses diatas disimpan dalam kolom baru yaitu kolom ulasan_normalized dimana data teks menyisakan kata yang sudah dinormalisasi.","d156e82d":"Karena pada proses scraping tidak dapat mendapatkan nilai rating, maka saya melakukan scrape untuk rating dengan memasukkan kode HTML nya yang kemudian melakukan proses pembersihannya pada proses ini sampai dapat menghasilkan angka rating.","28062379":"# Text Preprocessing","8f6d1245":"# Proses data Rating per ulasan","54a55708":"Menyimpan output dalam bentuk excel","f3407baa":"> Hasil yang dapat dilihat dari visualisasi persebaran kata yang paling banyak muncul yaitu kata 'barang' yang merupakan kata ganti yang mendefinisikan produk yang dibeli, selanjutnya ada kata 'cepat, sesuai, bagus' ini berarti ulasan yang lebih banyak diberikan pembeli di marketplace tokopedia lebih banyak pada sentimen positif ke-3 aspek penilaian itu, lalu kata masker, kirim, beli, terima, pesan, respon, dst.","6ffddb27":"Mengubah tipe data pada kolom ulasan menjadi string agar dapat melakukan pemrosesan data teks","ec61bb9e":"3. Normalisasi ( Pada proses ini akan membersihkan kata yang berupa kata sehari-hari\/slank words yang masih berantakan agar menjadi kata indonesia yang benar)","5eed37ec":"# Import data hasil scrape","2b76d363":"2. Tokenizing (Ditahap ini akan dilakukan proses number removal, whitecase removal, punctuation removal dan word_tokenize() untuk memecah string kedalam tokens)","118b47c9":"1. Case Folding (lower)","a39dc134":"Setelah proses diatas disimpan dalam kolom baru yaitu kolom ulasan_stop_removed dimana data teks menyisakan kata bermakna.","c4ebb6e9":"4. Filtering (Stopword Removal) Pada tahap ini kita akan menggunakan stopword bahasa indonesia yang didapatkan dari library NLTK untuk filtering terhadap Dataframe.","161109c8":"Setelah proses diatas disimpan dalam kolom baru yaitu kolom ulasan_tokenize dimana data teks menyisakan kata yang sudah dipisah perkata dari kalimat.","78a1371a":"Setelah proses diatas disimpan dalam kolom baru yaitu kolom ulasan_stemmed dimana data teks menyisakan kata yang sudah di stemming.","7223b1e3":"Pada Notebook kali saya bagi menjadi 2 bagian di bagian ini saya akan melakukan pra pemrosessan data teks dari hasil scraping website Tokopedia serta analisa teks ulasan.","1ef9fccf":"Setelah proses diatas disimpan dalam kolom baru yaitu kolom ulasan_clean dimana saya menggabungkan lagi kalimat yang sebelumnya sudah ditokenisasi dan dibersihkan. ","96295182":"Pada proses ini saya mengambil data yang bisa digunakan kedalam variable baru yaitu data_tokped"}}