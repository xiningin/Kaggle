{"cell_type":{"002769c0":"code","2afa3fe6":"code","f985baeb":"code","1c558d90":"code","80112838":"code","bdc70714":"code","70b60ca6":"code","b007e688":"code","cddaab6c":"code","60ecb953":"code","de2de793":"code","7f136182":"code","34256fe4":"code","94ab24ae":"markdown","fd79c0d9":"markdown","5c5e0096":"markdown","1830f2a5":"markdown","e74f4507":"markdown","3901f306":"markdown","72fd0909":"markdown","5a1c796f":"markdown","a7d33c04":"markdown","d90d73e5":"markdown","f956f983":"markdown","329671ae":"markdown","b69fb9ba":"markdown","a176fe53":"markdown","7294ec9a":"markdown"},"source":{"002769c0":"import numpy as np\nimport pandas as pd\nfrom pandas_datareader import data\n\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neural_network import MLPClassifier\nimport xgboost as xgb\nfrom sklearn.metrics import classification_report\n\n# Suppress warnings\nimport warnings\nwarnings.filterwarnings('ignore')","2afa3fe6":"# Custom function to pull prices of stocks\ndef get_price_var(symbol):\n    '''\n    Get historical price data for a given symbol leveraging the power of pandas_datareader and Yahoo.\n    Compute the difference between first and last available time-steps in terms of Adjusted Close price.\n    \n    Input: ticker symbol\n    Output: percent price variation \n    '''\n    # read data\n    prices = data.DataReader(symbol, 'yahoo', '2019-01-01', '2019-12-31')['Adj Close']\n\n    # get all timestamps for specific lookups\n    today = prices.index[-1]\n    start = prices.index[0]\n\n    # calculate percentage price variation\n    price_var = ((prices[today] - prices[start]) \/ prices[start]) * 100\n    return price_var","f985baeb":"# Load 2019 percent price variation for all tech stocks\nPVAR = pd.read_csv('..\/input\/Example_2019_price_var.csv', index_col=0)\n\n# Load dataset with all financial indicators (referring to end of 2018)\nDATA = pd.read_csv('..\/input\/Example_DATASET.csv', index_col=0)","1c558d90":"# Divide data in train and testing splits\ntrain_split, test_split = train_test_split(DATA, test_size=0.2, random_state=1, stratify=DATA['class'])\nX_train = train_split.iloc[:, :-1].values\ny_train = train_split.iloc[:, -1].values\nX_test = test_split.iloc[:, :-1].values\ny_test = test_split.iloc[:, -1].values\n\nprint(f'Total number of samples: {DATA.shape[0]}')\nprint()\nprint(f'Number of training samples: {X_train.shape[0]}')\nprint()\nprint(f'Number of testing samples: {X_test.shape[0]}')\nprint()\nprint(f'Number of features: {X_train.shape[1]}')","80112838":"# Standardize input data\nscaler = StandardScaler()\nscaler.fit(X_train)\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.fit_transform(X_test)","bdc70714":"# Parameter grid to be tuned\ntuned_parameters = [{'kernel': ['rbf', 'linear'], 'gamma': [1e-3, 1e-4], 'C': [0.01, 0.1, 1, 10, 100]}]\n\nclf1 = GridSearchCV(SVC(random_state=1),\n                    tuned_parameters,\n                    n_jobs=4,\n                    scoring='precision_weighted',\n                    cv=5)\n\nclf1.fit(X_train, y_train)\n\nprint('Best score and parameters found on development set:')\nprint()\nprint('%0.3f for %r' % (clf1.best_score_, clf1.best_params_))\nprint()","70b60ca6":"# Parameter grid to be tuned\ntuned_parameters = {'n_estimators': [1024, 4096],\n                    'max_features': ['auto', 'sqrt'],\n                    'max_depth': [4, 6, 8],\n                    'criterion': ['gini', 'entropy']}\n\nclf2 = GridSearchCV(RandomForestClassifier(random_state=1),\n                    tuned_parameters,\n                    n_jobs=4,\n                    scoring='precision_weighted',\n                    cv=5)\n\nclf2.fit(X_train, y_train)\n\nprint('Best score and parameters found on development set:')\nprint()\nprint('%0.3f for %r' % (clf2.best_score_, clf2.best_params_))\nprint()","b007e688":"# Parameter grid to be tuned\ntuned_parameters = {'learning_rate': [0.01, 0.001],\n                    'max_depth': [4, 6, 8],\n                    'n_estimators': [512, 1024]}\n\nclf3 = GridSearchCV(xgb.XGBClassifier(random_state=1),\n                   tuned_parameters,\n                   n_jobs=4,\n                   scoring='precision_weighted', \n                   cv=5)\n\nclf3.fit(X_train, y_train)\n\nprint('Best score and parameters found on development set:')\nprint()\nprint('%0.3f for %r' % (clf3.best_score_, clf3.best_params_))\nprint()","cddaab6c":"# Parameter grid to be tuned\ntuned_parameters = {'hidden_layer_sizes': [(32,), (64,), (32, 64, 32)],\n                    'activation': ['tanh', 'relu'],\n                    'solver': ['lbfgs', 'adam']}\n\nclf4 = GridSearchCV(MLPClassifier(random_state=1, batch_size=4, early_stopping=True), \n                    tuned_parameters,\n                    n_jobs=4,\n                    scoring='precision_weighted',\n                    cv=5)\n\nclf4.fit(X_train, y_train)\n\nprint('Best score, and parameters, found on development set:')\nprint()\nprint('%0.3f for %r' % (clf4.best_score_, clf4.best_params_))\nprint()","60ecb953":"# Get 2019 price variations ONLY for the stocks in testing split\npvar_test = PVAR.loc[test_split.index.values, :]","de2de793":"# Initial investment can be $100 for each stock whose predicted class = 1\nbuy_amount = 100\n\n# In new dataframe df1, store all the information regarding each model's predicted class and relative gain\/loss in $USD\ndf1 = pd.DataFrame(y_test, index=test_split.index.values, columns=['ACTUAL']) # first column is the true class (BUY\/INGORE)\n\ndf1['SVM'] = clf1.predict(X_test) # predict class for testing dataset\ndf1['VALUE START SVM [$]'] = df1['SVM'] * buy_amount # if class = 1 --> buy $100 of that stock\ndf1['VAR SVM [$]'] = (pvar_test['2019 PRICE VAR [%]'].values \/ 100) * df1['VALUE START SVM [$]'] # compute price variation in $\ndf1['VALUE END SVM [$]'] = df1['VALUE START SVM [$]'] + df1['VAR SVM [$]'] # compute final value\n\ndf1['RF'] = clf2.predict(X_test)\ndf1['VALUE START RF [$]'] = df1['RF'] * buy_amount\ndf1['VAR RF [$]'] = (pvar_test['2019 PRICE VAR [%]'].values \/ 100) * df1['VALUE START RF [$]']\ndf1['VALUE END RF [$]'] = df1['VALUE START RF [$]'] + df1['VAR RF [$]']\n\ndf1['XGB'] = clf3.predict(X_test)\ndf1['VALUE START XGB [$]'] = df1['XGB'] * buy_amount\ndf1['VAR XGB [$]'] = (pvar_test['2019 PRICE VAR [%]'].values \/ 100) * df1['VALUE START XGB [$]']\ndf1['VALUE END XGB [$]'] = df1['VALUE START XGB [$]'] + df1['VAR XGB [$]']\n\ndf1['MLP'] = clf4.predict(X_test)\ndf1['VALUE START MLP [$]'] = df1['MLP'] * buy_amount\ndf1['VAR MLP [$]'] = (pvar_test['2019 PRICE VAR [%]'].values \/ 100) * df1['VALUE START MLP [$]']\ndf1['VALUE END MLP [$]'] = df1['VALUE START MLP [$]'] + df1['VAR MLP [$]']\n\n# Show dataframe df1\ndf1.head()","7f136182":"# Create a new, compact, dataframe in order to show gain\/loss for each model\nstart_value_svm = df1['VALUE START SVM [$]'].sum()\nfinal_value_svm = df1['VALUE END SVM [$]'].sum()\nnet_gain_svm = final_value_svm - start_value_svm\npercent_gain_svm = (net_gain_svm \/ start_value_svm) * 100\n\nstart_value_rf = df1['VALUE START RF [$]'].sum()\nfinal_value_rf = df1['VALUE END RF [$]'].sum()\nnet_gain_rf = final_value_rf - start_value_rf\npercent_gain_rf = (net_gain_rf \/ start_value_rf) * 100\n\nstart_value_xgb = df1['VALUE START XGB [$]'].sum()\nfinal_value_xgb = df1['VALUE END XGB [$]'].sum()\nnet_gain_xgb = final_value_xgb - start_value_xgb\npercent_gain_xgb = (net_gain_xgb \/ start_value_xgb) * 100\n\nstart_value_mlp = df1['VALUE START MLP [$]'].sum()\nfinal_value_mlp = df1['VALUE END MLP [$]'].sum()\nnet_gain_mlp = final_value_mlp - start_value_mlp\npercent_gain_mlp = (net_gain_mlp \/ start_value_mlp) * 100\n\npercent_gain_sp500 = get_price_var('^GSPC') # get percent gain of S&P500 index\npercent_gain_dj = get_price_var('^DJI') # get percent gain of DOW JONES index\npercent_gain_sector = PVAR['2019 PRICE VAR [%]'].mean()\n\nMODELS_COMPARISON = pd.DataFrame([start_value_svm, final_value_svm, net_gain_svm, percent_gain_svm],\n                    index=['INITIAL COST [USD]', 'FINAL VALUE [USD]', '[USD] GAIN\/LOSS', 'ROI'], columns=['SVM'])\nMODELS_COMPARISON['RF'] = [start_value_rf, final_value_rf, net_gain_rf, percent_gain_rf]\nMODELS_COMPARISON['XGB'] = [start_value_xgb, final_value_xgb, net_gain_xgb, percent_gain_xgb]\nMODELS_COMPARISON['MLP'] = [start_value_mlp, final_value_mlp, net_gain_mlp, percent_gain_mlp]\nMODELS_COMPARISON['S&P 500'] = ['', '', '', percent_gain_sp500]\nMODELS_COMPARISON['DOW JONES'] = ['', '', '', percent_gain_dj]\nMODELS_COMPARISON['TECH SECTOR'] = ['', '', '', percent_gain_sector]\n\n# Show the dataframe\nMODELS_COMPARISON","34256fe4":"print(53 * '=')\nprint(15 * ' ' + 'SUPPORT VECTOR MACHINE')\nprint(53 * '-')\nprint(classification_report(y_test, clf1.predict(X_test), target_names=['IGNORE', 'BUY']))\nprint(53 * '-')\nprint(53 * '=')\nprint(20 * ' ' + 'RANDOM FOREST')\nprint(53 * '-')\nprint(classification_report(y_test, clf2.predict(X_test), target_names=['IGNORE', 'BUY']))\nprint(53 * '-')\nprint(53 * '=')\nprint(14 * ' ' + 'EXTREME GRADIENT BOOSTING')\nprint(53 * '-')\nprint(classification_report(y_test, clf3.predict(X_test), target_names=['IGNORE', 'BUY']))\nprint(53 * '-')\nprint(53 * '=')\nprint(15 * ' ' + 'MULTI-LAYER PERCEPTRON')\nprint(53 * '-')\nprint(classification_report(y_test, clf4.predict(X_test), target_names=['IGNORE', 'BUY']))\nprint(53 * '-')","94ab24ae":"**EVALUATE THE MODELS**\n\nNow that 4 classification models have been trained, we must test them and compare their performance with respect to each other and to the benchmarks (S&P 500, DOW JONES, sector).","fd79c0d9":"**BEAT THE STOCK MARKET - THE LAZY STRATEGY**\n\nIs it possible to understand which stock will increase its value in the year *t* using only financial data available for the year *t-1*?\n\nThe objective of this kernel is to show that machine learning models can learn to recognize stocks that will grow in value after being trained on a set of financial indicators, like those that can be found in the 10-K filings.\n\n* **ABOUT THE DATA**\n\n    I already prepared and made available 2 example datasets:\n\n    1. `Example_DATASET.csv` contains a table where each row is an US stock from the technology sector and each column is a financial indicators. The last column is the class (binary, IGNORE=0, BUY=1).\n    2. `Example_2019_prive_var.cs`  contains a table where each row is an US stock from the technology sector and the column indicates the stock's percent price variation for the year 2019 (first trading day on Jan '19, last trading day on Dec '19).\n\n  I will later share a kernel where I will show how you can build these types of datasets.\n\n  From a general point of view, I will be considering the financial data collected in the `Example_DATASET.csv` as features. The data collected there are from the 10-K filings of 2018 (end of 2018). The class of each sample has been assigned by looking at the performance of the stock in the year 2019 (`Example_2019_price_var.csv`), that is:\n    1. Class = 1 if 2019 price variation is positive;\n    2. Class = 0 if 2019 price variation is negative.\n    \n  **So, this is a binary classification problem and the objective is to BUY all the stocks that will be classified as `1`, and to IGNORE all those stocks that will be classified as `0`.**\n\n* **ABOUT THE MODELS**\n\n    The machine learning models that I will be using are:\n    1. Support Vector Machine (`SVC`, scikit-learn);\n    2. Random Forest (`RandomForestClassifier`, scikit-learn);\n    3. Extreme Gradient Boosting (`xgb`, xgboost);\n    4. Multi-Layer Perceptron (`MLPClassifier`, scikit-learn).\n\n  Nothing too fancy, I am more interested in the proof-of-concept right now (if the results are satisfactory, I will implement more sofisticated neural networks via Keras or Pytorch). For what concerns the training of the models, I will be running a `GridSearchCV` with `cv=5` and `scoring='precision_weighted'`.\n    \n* **ABOUT THE LAZY WAY**\n\n    Compared to other algorithmic trading strategies, this kernel implements a **lazy strategy** which consists in buying the stocks that the machine learning models identify as buy-worthy on the first trading day of the year and selling them on the last trading day of the year. Indeed, it is widely known that most of the times buying-&-holding a stock yields higher returns compared to trying to time the market.\n    \n    As of now, this method has only been backtested with the year 2019, but I am planning to perform more backtesting with older data.\n    \n    \n* **ABOUT THE MODEL'S EVALUATION**\n\n    The evaluation of the machine learning models' performance is a two-step process:\n    \n    1. An analysis of the `classification_report` is performed in order to evaluate the models according to commonly used metrics\n    2. An analysis of the machine learning models' Return On Investment (ROI) is performed in order to evaluate their performance from a financial point of view. Furthermore, the ROI of S&P 500 index (^GSPC), Dow Jones index (^DJI) and the ROI of the Technology sector are used as benchmarks to evaluate the performance of the **lazy strategy**.\n   ","5c5e0096":"**LOAD DATA**\n\nNow, we are ready to import the required data. As explained, I have prepared the two datasets beforetime, and I will upload a kernel to shows just that. Briefly:\n\n* `PVAR` is a dataframe where each row is a stock from the US stock market Technology sector, and the column is the 2019 percent price variation for each stock.\n* `DATA` is a dataframe where each row is a stock from the US stock market Technology sector, and the columns are financial indicators that have been scraped thanks to Financial Modeling Prep API (https:\/\/financialmodelingprep.com\/developer\/docs\/). The financial indicators refer to the end of 2018. The last column of `DATA` corresponds to the binary classification of each stock, according to the rules explained above.","1830f2a5":"**TRAIN TEST SPLIT**\n\nOnce the dataset is loaded, we must split it into training and testing. 20% of the data will be used to test the ML models, note the parameter `stratify` used in order to keep the same class-ratios between training and testing splits.\nFrom the `train_split` and `test_spli` we extract both input data `X_train`, `X_test` and output target data `y_train`, `y_test`.\nA sanity check is performed after.","e74f4507":"**ML MODEL I: SUPPORT VECTOR MACHINE**\n\nThe first classification model we will run is the support vector machine. `GridSeachCV` is performed in order to tune some hyper-parameters (`kernel`, `gamma`, `C`). The required number of cross-validation sets is 5. We want to achieve maximum weighted precision.","3901f306":"We need a function that allows us to pull the historical price data for a given symbol. This will be used to compute the ROI of S&P 500 index (^GSPC) and Dow Jones index (^DJI).","72fd0909":"**ML MODEL IV: MULTI-LAYER PERCEPTRON**\n\nThe fourth classification model we will run is the multi-layer perceptron (feed-forward neural network). `GridSeachCV` is performed in order to tune some hyper-parameters (`hidden_layer_sizes`, `activation`, `solver`). The required number of cross-validation sets is 5. We want to achieve maximum weighted precision.","5a1c796f":"Let's start by importing the required libraries. This kernel leverages the power of:\n* `numpy`, `pandas` to manipulate data\n* `pandas_datareader` to pull historical price data (using Yahoo Finance)\n* `sklearn`, `xgb` for everything machine learning related","a7d33c04":"**ML MODEL II: RANDOM FOREST**\n\nThe second classification model we will run is the random forest. `GridSeachCV` is performed in order to tune some hyper-parameters (`n_estimators`, `max_features`, `max_depth`, `criterion`). The required number of cross-validation sets is 5. We want to achieve maximum weighted precision.","d90d73e5":"**ML MODEL III: EXTREME GRADIENT BOOSTING**\n\nThe third classification model we will run is the extreme gradient boosting. `GridSeachCV` is performed in order to tune some hyper-parameters (`learning_rate`, `max_depth`, `n_estimators`). The required number of cross-validation sets is 5. We want to achieve maximum weighted precision.","f956f983":"Finally, we build a compact dataframe `MODELS_COMPARISON` in which we collect the main information required to perform the comparison between the classification models and the benchmarks (S&P 500, DOW JONES, sector).\nLeveraging the dataframe `df1`, we can easily compute gains and losses for each model (`net_gain_`, `percent_gain`)\n\nSince we miss the data from the benchmarks, we quickly exploit the custom function `get_price_var` in order to get the percent price variation for both S&P 500 (^GSPC) and DOW JONES (^DJI) for the year 2019. The 2019 percent price variation for the Technology sector is computed by using `.mean()` on the whole column of the dataset `PVAR`.","329671ae":"From the dataframe `MODELS_COMPARISO`, it is possible to see that:\n\n* RF and XGB are the ML models that yield the highest ROI, 32.5% and 34.9% respectively\n* RF and XGB outperform the S&P 500 and the Tech sector by about 4-6 p.p, while they outperform the DOW JONES by more than 10 p.p.\n* MLP and SVM are closely matched, and yield an ROI of 28.3% and 27.6% respectively\n* MLP and SVM perform similarly to the S&P 500, while they both outperfom the DOW JONES\n* the SVM leads to the highest net gains, at about 3400 USD; however, it also has the highest initial investment cost at 12300 USD\n* the RF leads to the lowest net gains, at about 2570 USD; however, it also has the lowest initial investment cost at 7900 USD\n\nSo, this kernel proves that, at least as proof-of-concept, it is possible to find useful information in the 10-K filings that the publicly traded companies release. The financial information can be used to train machine learning models that learn to recognize buy-worthy stocks. With that being said, the ROI generated are only a few percent points higher that buying-and-holding the S&P 500 index.\n\nFor what concerns a more traditional comparison between the performance of the ML models implemented, it is possible to analyze the `classification_repor`.","b69fb9ba":"Looking carefully, it is possible to see that the `classification_report` confirm what has already been noted in the analysis of `MODELS_COMPARISON`. Indeed:\n\n* XGB has an excellent balance between *precision* and *recall* for the `BUY` class, which is highlighted by the highest *weighted average f1-score*\n* XGB and RF share the highest *precision* for the BUY class (80%). This allows to minimize the number of false positives, which is what makes us invest in stocks that decrease in value in year 2019\n* XGB has a slightly higher *f1-score* for the `IGNORE` class\n* SVM has the lowest *f1-score* for the `IGNORE` class (*recall* is 5%). This means that most of its predictions are `BUY` (which explains the highest initial cost of 12300 USD), and it does not recognizes the stocks to `IGNORE`\n* MLP has less *accuracy* than the SVM, but thanks to the higher *weighted average f1-score* it is able to generate the same ROI with a lower initial cost","a176fe53":"Now, we build a new dataframe `df1` in which, for each tested stock, we collect all the predicted classes from each model (it is reminded that the two classes are `0` = IGNORE, `1` = BUY).\nIf the model predicts class `1`, we proceed to buy 100 USD worth of that stock; otherwise, we ignore the stock.","7294ec9a":"**DATA STANDARDIZATION**\n\nThe next step consists in the standardization of the data. We leverage the `StandardScaler()` available from `scikit-learn`. It is important to use the same coefficients when standardizing both training and testing data: for this reason we first fit the scaler to `X_train`, and then apply it to both `X_train` and `X_test` via the method `.fit_transform()`."}}