{"cell_type":{"68d5e9a0":"code","762e66a9":"code","b003702f":"code","13be9d97":"code","29a14332":"code","358fabf8":"code","774c5dfe":"code","4cfc0102":"code","e8456a97":"code","91e28174":"code","ab279218":"code","d8a5f3a2":"code","c9f687f6":"code","f29df2eb":"code","24fce870":"code","c8a4dd98":"code","3648c488":"code","f880aa2f":"code","343b5779":"code","f196fcb6":"code","8fcc1088":"code","0d44ca44":"code","e096a378":"code","cecaeb5d":"code","89aa2e0e":"code","5d385248":"code","4405b7a2":"code","d1ccbf41":"code","02028281":"code","42a27612":"code","76d12876":"code","7e9deb80":"code","e5a3a6c1":"code","a8fd339e":"code","16f7f0e7":"markdown","ab6a58e3":"markdown","2b7aefeb":"markdown","7eb24aee":"markdown","20bfb0e6":"markdown","0371cbc8":"markdown","6cfce885":"markdown","6c665ca1":"markdown","7d985e17":"markdown","db7dab3f":"markdown","f1e5b2ce":"markdown","c622b107":"markdown","9bae1761":"markdown","07723708":"markdown","8a49ab91":"markdown","34b4b682":"markdown","153551a4":"markdown","684d0e05":"markdown","6b4bcf3c":"markdown","f8cc45ba":"markdown","ada2fe33":"markdown","d481db98":"markdown","204d56fd":"markdown","110a19b9":"markdown","a610c064":"markdown","cc26b3ea":"markdown","4c4a2f3a":"markdown","ecced921":"markdown","84d9dfa3":"markdown","f3d6cd89":"markdown","cc64ad8b":"markdown","6139dd01":"markdown","c6a339d6":"markdown","af94f7ef":"markdown","1746ecaa":"markdown","8151b944":"markdown","e4aedb5c":"markdown","82b6c639":"markdown","9ba50051":"markdown","8fb8f6f4":"markdown","87d8b136":"markdown","293a0a69":"markdown","25a4a229":"markdown","1f28910a":"markdown","724165b3":"markdown","376b9c5a":"markdown","d20c01ee":"markdown","9c49ae97":"markdown","46f26670":"markdown"},"source":{"68d5e9a0":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\nfrom sklearn.model_selection import train_test_split\n\nfrom sklearn.neighbors import KNeighborsClassifier\n\nfrom sklearn.utils import resample\nfrom scipy.stats import zscore\n!pip install imblearn\nfrom imblearn.over_sampling import SMOTE\nfrom sklearn import metrics\nfrom collections import Counter\n\nfrom sklearn.metrics import r2_score, roc_auc_score, roc_curve, average_precision_score\nfrom sklearn.metrics import confusion_matrix, accuracy_score, classification_report, plot_confusion_matrix\n\nfrom sklearn.model_selection import GridSearchCV\nimport statsmodels.api as sm\n","762e66a9":"colnames = ['P_incidence', 'P_tilt', 'L_angle', 'S_slope', 'P_radius', 'S_Degree', 'Class']\ndata1 = pd.read_csv(\"\/kaggle\/input\/biomechanical-features-of-orthopedic-patients\/column_3C_weka.csv\", names = colnames, index_col = False, header = 0)\ndata1","b003702f":"data1.info()","13be9d97":"print(data1['Class'].unique())","29a14332":"def classifier (x):\n    if x == 'Normal':\n        x = 0\n        return x\n    elif x =='Hernia':\n        x = 1\n        return x\n    else:\n        x=2\n        return x\ndata1['Class'] = data1['Class'].apply(classifier)","358fabf8":"print(data1['Class'].unique())","774c5dfe":"data1.boxplot(column = ['P_incidence', 'P_tilt', 'L_angle', 'S_slope', 'P_radius', 'S_Degree'], figsize = (15,5))","4cfc0102":"print(data1.quantile(0.04))\nprint(data1.quantile(0.96))","e8456a97":"data1[\"P_incidence\"]=np.where(data1[\"P_incidence\"]>data1[\"P_incidence\"].quantile(0.96),data1[\"P_incidence\"].quantile(0.96),data1['P_incidence'])\ndata1[\"P_tilt\"] = np.where(data1[\"P_tilt\"] < data1[\"P_tilt\"].quantile(0.04),data1[\"P_tilt\"].quantile(0.04),data1['P_tilt'])\ndata1[\"P_tilt\"] = np.where(data1[\"P_tilt\"] >data1[\"P_tilt\"].quantile(0.96),data1[\"P_tilt\"].quantile(0.96),data1['P_tilt'])\ndata1[\"L_angle\"] = np.where(data1[\"L_angle\"] > data1[\"L_angle\"].quantile(0.96),data1[\"L_angle\"].quantile(0.96),data1['L_angle'])\ndata1[\"S_slope\"] = np.where(data1[\"S_slope\"] > data1[\"S_slope\"].quantile(0.96),data1[\"S_slope\"].quantile(0.96),data1['S_slope'])\ndata1[\"P_radius\"] = np.where(data1[\"P_radius\"] < data1[\"P_radius\"].quantile(0.04),data1[\"P_radius\"].quantile(0.04),data1['P_radius'])\ndata1[\"P_radius\"] = np.where(data1[\"P_radius\"] > data1[\"P_radius\"].quantile(0.96),data1[\"P_radius\"].quantile(0.96),data1['P_radius'])\ndata1[\"S_Degree\"] = np.where(data1[\"S_Degree\"] > data1[\"S_Degree\"].quantile(0.96),data1[\"S_Degree\"].quantile(0.96),data1['S_Degree'])\ndata1.boxplot(column = ['P_incidence', 'P_tilt', 'L_angle', 'S_slope', 'P_radius', 'S_Degree'], figsize = (15,5))","91e28174":"fig, ax = plt.subplots(figsize = (20,6))\nax.set_title('Class split', color = 'red')\nsns.countplot(x = 'Class', data = data1)","ab279218":"x = data1.iloc[:,:6]\ny = data1['Class']","d8a5f3a2":"x2 = sm.add_constant(x)\nest = sm.OLS(y, x2)\nest2 = est.fit()\nprint(est2.summary())","c9f687f6":"cor =data1.corr()\ncor","f29df2eb":"f, ax = plt.subplots(figsize=(20, 20))\nsns.heatmap(cor, annot=True, cmap='cool', ax=ax)\nplt.show()","24fce870":"sns.pairplot (data=data1,vars = ['P_incidence','L_angle','P_radius','S_Degree'],  hue = 'Class', palette = 'bright')","c8a4dd98":"sns.relplot(x=\"P_radius\",y=\"S_Degree\",col='Class', data=data1, palette = 'bright')","3648c488":"sns.relplot(x=\"S_Degree\",y=\"P_incidence\",col='Class', data=data1, palette = 'warm')","f880aa2f":"sns.relplot(x=\"L_angle\",y=\"S_Degree\",col='Class', data=data1, palette = 'warm')","343b5779":"fig, ax = plt.subplots(1,6, figsize = (12,4))\nsns.histplot(data1['P_incidence'],bins = 24,kde = True, ax = ax[0])\nax[0].set_title(\"DIST OF P_incidence\")\nsns.histplot(data1['P_tilt'],bins = 24,kde = True, ax = ax[1])\nax[1].set_title(\"DIST OF P_tilt\")\nsns.histplot(data1['L_angle'],bins = 24,kde = True, ax = ax[2])\nax[2].set_title(\"DIST OF L_angle\")\nsns.histplot(data1['S_slope'],bins = 24,kde = True, ax = ax[3])\nax[3].set_title(\"DIST OF S_slope\")\nsns.histplot(data1['P_radius'],bins = 24,kde = True, ax = ax[4])\nax[4].set_title(\"DIST OF P_radius\")\nsns.histplot(data1['S_Degree'],bins = 24,kde = True, ax = ax[5])\nax[5].set_title(\"DIST OF S_Degree\")\n\nplt.tight_layout()","f196fcb6":"data1.skew()","8fcc1088":"x = data1.iloc[:,:6]\ny = data1['Class']\nxz = x.apply(zscore)\nxztrain, xztest, ytrain, ytest = train_test_split(xz, y, test_size=0.3, random_state=20)","0d44ca44":"counter = Counter (ytrain)\nprint(counter)","e096a378":"smote = SMOTE(random_state = 20)\nxtrain1, ytrain1 = smote.fit_resample(xztrain, ytrain)\nprint(xtrain1.shape)\ncounter = Counter (ytrain1)\nprint (counter)","cecaeb5d":"modelkn = KNeighborsClassifier(n_neighbors = 10)\nmodelkn.fit(xtrain1, ytrain1)","89aa2e0e":"print(\"The accuracy for train data is:\", modelkn.score(xtrain1, ytrain1))\nprint(\"The accuracy for test data is:\", modelkn.score(xztest, ytest))","5d385248":"ypred = modelkn.predict(xztest)\nprint(\"CLASSIFICATION REPORT: \\n\",classification_report(ytest,ypred))\nprint(\"CONFUSION MATRIX: \\n\",confusion_matrix(ytest,ypred))\nprint(\"CROSS TAB: \\n\", pd.crosstab(ytest, ypred, rownames=['True'], colnames=['Predicted'], margins=True))\nplot_confusion_matrix(modelkn,xztest,ytest)","4405b7a2":"mylist =np.arange(1,50)\ntrsco = []\ntesco = []\nbestk = []\nfor k in mylist:\n    modelkn = KNeighborsClassifier(n_neighbors=k)\n    modelkn.fit(xtrain1, ytrain1)\n    ypredtr = modelkn.predict(xtrain1)\n    ypredte = modelkn.predict(xztest)\n    trscores = metrics.accuracy_score(ypredtr, ytrain1)\n    tescores = metrics.accuracy_score(ypredte, ytest)\n    trsco.append(trscores)\n    tesco.append(tescores)\n    if trscores>0.85:\n        bestklist = [k,trscores,tescores]\n        bestk.append(bestklist)\n    #print('>%d,train:%0.3f,test:%0.3f' %(k,trscores,tescores))\n#print(bestk)\noptk = []\nfor x,y,z in bestk:\n    k = x\n    optk.append(x)\nprint(\"K values giving training scores more than 85% are:\", optk)","d1ccbf41":"plt.plot(mylist,trsco,'-o', label = \"Train\")\nplt.plot(mylist,tesco,'-o', label = \"Test\")\nplt.legend()\nplt.show()","02028281":"import warnings\nwarnings.filterwarnings(\"ignore\")\ngrid_params = {'n_neighbors':[5,6,7],'weights':['uniform', 'distance'],\n'leaf_size':list(range(1,20)),'algorithm':['ball_tree','kd_tree','brute'],'metric':['euclidean','manhattan']}\n\ngs = GridSearchCV(KNeighborsClassifier(), grid_params,scoring = 'recall', verbose = 1, cv = 3, n_jobs = -1)\ngs_results = gs.fit(xztest, ytest)\nprint(gs_results.best_estimator_)\nprint(gs_results.best_params_)","42a27612":"modelkn1 = KNeighborsClassifier( n_neighbors= 7, algorithm='ball_tree', leaf_size=1, metric='euclidean',weights= 'uniform')\nmodelkn1.fit(xtrain1, ytrain1)","76d12876":"print(modelkn1.score(xtrain1, ytrain1))","7e9deb80":"ypredte = modelkn1.predict(xztest)\ntescores = accuracy_score(ypredte, ytest)\ntescores","e5a3a6c1":"yproba1 = modelkn1.predict_proba(xztest)[:,1]\nyproba2 = modelkn1.predict_proba(xztest)[:,2]\nyproba12 = yproba1+yproba2\nytestnew = list()\nfor x in ytest:\n    if x == 2:\n        x = 1\n        ytestnew.append(x)\n    else:\n        x = x\n        ytestnew.append(x)\nfpr, tpr, thresholds = roc_curve(ytestnew, yproba12)\nplt.plot([0,1],[0,1])\nplt.plot(fpr,tpr, label='Knn')\nplt.xlabel('fpr')\nplt.ylabel('tpr')\nplt.title('Knn(n_neighbors=11) ROC curve')\nplt.show()\n\nfrom sklearn.metrics import roc_auc_score\nprint(\"AREA UNDER THE CURVE IS:\",roc_auc_score(ytestnew,yproba12))","a8fd339e":"print(\"CLASSIFICATION REPORT: \\n\",classification_report(ytest,ypredte))\nprint(\"CONFUSION MATRIX: \\n\",confusion_matrix(ytest,ypredte))\nprint(\"CROSS TAB: \\n\", pd.crosstab(ytest, ypredte, rownames=['True'], colnames=['Predicted'], margins=True))\nplot_confusion_matrix(modelkn1,xztest,ytest,cmap = 'cool')","16f7f0e7":"## A. Splitting the Predicting and Target variables with normalising the data. ","ab6a58e3":"## A. Designing and training a KNN Claasifier - K = 10(sqrt(105))","2b7aefeb":"2. The testing accuracy scores initially elevate to a level where it stabilises and then drops significantly.","7eb24aee":"3. We select the k values in this range where it stabilises for testing data and tune our model for better results.","20bfb0e6":"## B. Target Balancing and Train - Test Split of data. ","0371cbc8":"# 2. DATA CLEANSING ","6cfce885":"3. The model accuracy on the testing Dataset is above 70%.","6c665ca1":"## D. Automating the Task of finding the best K values:","7d985e17":"The optimal k values for the accuracy of above 85% on the training data set are listed from the above code.","db7dab3f":"Ignoring warning since the values are turning out to be Non - Finite for some iteration values","f1e5b2ce":"Applying stats model to find p values","c622b107":"## B. Treating outliers within Dataset and replacing them with appropriate values:","9bae1761":"## A.Performing detailed statistical analysis on the data","07723708":"## A. Treating the Datatypes and correcting values wherever required:","8a49ab91":"# 1.  IMPORTING AND WAREHOUSING DATA","34b4b682":"## A. CONCLUSION:","153551a4":"## AOC FOR PREDICTING THE ABNORMALITIES:","684d0e05":"## C. Displaying and explaining the Classification Report:","6b4bcf3c":"The Errors values in the above model is to the value less than 20 on a overall dataset of 217 entries which work out to be less than 10%, when compared to above 10% before tuning.","f8cc45ba":"# 6. CONCLUSION AND IMPROVISATION:","ada2fe33":"# IMPORTING NECESSARY LIBRARIES","d481db98":"The model accuracy on the testing Dataset is about 82%","204d56fd":"-->We find that the P_Radius and S_Degree emerge as winners for Significant Parameters for prdicting Class","110a19b9":"Finding Pearsons CorrelationCoefficients","a610c064":"The test data acuuracy scores have improved from 76% to above 83%.","cc26b3ea":"## E. Tuning the Paramters for best recall values:","4c4a2f3a":"The Recall values for classes 2 is about 89% when compared 84% in the earlier model.","ecced921":"We find that the class 2 is a majority class, and the other two classes are minority classes, which will be balnced by Over sampling with SMOTE Technique, Since we dont want to eliminate the target attribute by downsizing the majority class.","84d9dfa3":"The Training scores have improved from 86% to above 90%.","f3d6cd89":"Applying Pair Plots for Significant Variables to see whether the variables make the class apart ","cc64ad8b":"Normal class range of P_radius and S_Degree lies between 120 - 135 and below 10 respectievely","6139dd01":"1. Data Collection should have tried to achieve the target balancing initially itself.","c6a339d6":"# 3. DATA ANALYSIS AND VISUALISATION:","af94f7ef":"## B. IMPROVISATION:","1746ecaa":"-->From Heat Map we find that \"S-Degree\", \"L - Angle\" and \"P-Incidence\" have high correlation coefficients. ","8151b944":"2. The Recall values for classes 1 & 2 are above 80%, whereas the recall for class 0 is about 61%, which means that the Model is not biased on the majority class after balancing the dataset.","e4aedb5c":"# 5. MODEL TRAINING, TESTING AND TUNING:","82b6c639":"## B.Multivariate, Bivariate and Univariate analysis","9ba50051":"The values of P incidence, P radius and s slope are normally distributed.\nAlmost all the values are multimodal.","8fb8f6f4":"## B.Displaying the Accuracies for Train and Test Data","87d8b136":"Above plot shows that the TRaining and Test Scores converge to the same point and the following obsevations are made,","293a0a69":"# 4. DATA PRE - PROCESSING:","25a4a229":"1. The training accuracy scores continue to drop from K value of 1 and converge towards testing data scores.","1f28910a":"Normal Class Range for L angle lies within 25 - 50 whereas the values outside this range falls under abnormal class","724165b3":"Selecting the model with best parameters as above","376b9c5a":"1. The Errors values in the above model is to the value of around 20 on a overall dataset of 217 entries which work out to be around 10%.","d20c01ee":"Normal Class Range for P incidence lies within 45 - 60 whereas the values outside this range falls under abnormal class","9c49ae97":"The Selected Significant Variables definitely try to make the class apart atleast the Class 2 values and the same is evident from the below relation plots","46f26670":"Hence we Conclude that the modelkn1 is the best after parameter tuning for predicting the abnormalities in the biomechanical features for classifying against Hernia and Spondolysthesis."}}