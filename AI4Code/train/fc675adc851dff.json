{"cell_type":{"f43ace49":"code","7650d646":"code","7bd0f3ab":"code","1c01c914":"code","33a4e860":"code","59afd458":"code","36e4f321":"code","6f78304d":"code","30b84b66":"code","5d46b1fb":"code","d8154467":"code","71f01b74":"code","d70ced3f":"code","16124a35":"code","495d95e0":"code","98a8a58b":"code","11cdc33b":"code","347d21a1":"code","6a71370f":"markdown","648ce4dc":"markdown","12d8ef59":"markdown","c93dbe29":"markdown","07127020":"markdown"},"source":{"f43ace49":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom math import ceil \n%matplotlib inline\n\n\n# For ordinal encoding categorical variables, splitting data\nfrom sklearn.preprocessing import OrdinalEncoder\nfrom sklearn.model_selection import train_test_split\n\n\n# For training random forest model\nfrom xgboost import XGBRegressor\nfrom sklearn.metrics import mean_squared_error\n\nimport warnings\nwarnings.simplefilter(action='ignore', category=Warning)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","7650d646":"# Load the training data\ndf_train = pd.read_csv(\"..\/input\/30-days-of-ml\/train.csv\")\ndf_test = pd.read_csv(\"..\/input\/30-days-of-ml\/test.csv\")\n\n# Preview the data\ndf_train.head()","7bd0f3ab":"#Disturbution of target variable\n\nbins = np.arange(0, 12, 0.1)\nsns.displot(df_train.target, height = 5, aspect = 2, bins = bins);","1c01c914":"#corelation\n\nplt.figure(figsize= (20, 15))\n# Mask to hide upper-right part of plot as it is a duplicate\nmask = np.transpose(np.tril(np.ones(df_train.corr().shape)))\nsns.heatmap(df_train.corr(), annot = True, center = 0, cmap = 'RdBu', mask = mask);","33a4e860":"num_cols = [col for col in df_train.columns if 'cont' in col] \nnum_cols","59afd458":"def plot(data, cols, features_type, nrows, ncols, bins='auto', target=None, figsize=None,\n         hspace=None, wspace=None, color = None):\n    '''plot all features vs target or the distribution of features'''\n    if figsize != None:\n        plt.figure(figsize = figsize)\n    for col, plot_num in zip(cols, list(range(1, len(cols)))):\n        plt.subplot(nrows, ncols, plot_num)\n        if hspace != None or wspace != None:\n            plt.subplots_adjust(hspace = hspace, wspace = wspace)\n            \n        if features_type == 'numerical':\n            if target != None:\n                plt.scatter(data[col], data[target])\n                plt.title(col)\n            else:\n                sns.histplot(data[col], bins=bins)\n                \n        if features_type == 'categorical':\n            if target != None:\n                sns.violinplot(data=data, y=col, x=target, color=color, inner='quartile');\n            else:\n                countplot_ratio(x = col, data = data, color = color)","36e4f321":"#disturbution of numerical features\n\nn_cols = 4\nn_rows = ceil(len(num_cols)\/n_cols)\nbins = np.arange(0, 1.3, 0.02)\nplot(data=df_train, cols=num_cols, features_type='numerical', nrows=n_rows, ncols=n_cols, hspace=0.3, wspace=0.5, bins=bins,\n    figsize = (15, 15))","6f78304d":"#disturbution of continous variable vs target\n\nn_cols = 4\nn_rows = ceil(len(num_cols)\/n_cols)\nplot(data=df_train, target='target', cols=num_cols, features_type='numerical', nrows=n_rows, ncols=n_cols, hspace=0.3,\n    figsize = (15, 15))","30b84b66":"# List of categorical columns\nobject_cols = [col for col in df_train.columns if 'cat' in col]\nobject_cols","5d46b1fb":"#distrubution of categorical features\n\n# function to plot the distribution of categorical variable \n# since the countplot function show the counts of observations in each categorical bin using bars.\ndef countplot_ratio(x = None, data = None, hue = None, ax = None, color = None):\n    # plot the variable\n    ax = sns.countplot(x, data = data, hue = hue, ax = ax, color = color)\n    # names of x labels\n    ax.set_xticklabels(ax.get_xticklabels())\n    # plot title\n    ax.set_title(x + \" Distribution\")\n    # total number of data which used to get the proportion\n    total = float(len(data))\n    # for loop to iterate on the patches\n    for patch in ax.patches:\n        # get the height of the patch which represents the number of observations.\n        height = patch.get_height()\n        # Put text on each patch with the proportion of the observations\n        ax.text(patch.get_x()+patch.get_width()\/2,height+4,'{:.2f}%'.format((height\/total)*100),weight = 'bold',\n                fontsize = 12,ha = 'center')","d8154467":"n_cols = 2\nn_rows = ceil(len(object_cols)\/n_cols)\nbase_color = sns.color_palette(n_colors=2)[1]\nplot(data=df_train, cols=object_cols, features_type='categorical', nrows=n_rows, ncols=n_cols,\n     hspace=0.5, figsize = (15, 20), color='0.75')","71f01b74":"#distrbution of categorical varilable vs target\n\nn_cols = 3\nn_rows = ceil(len(object_cols)\/n_cols)\nplot(data=df_train, target='target', cols=object_cols, features_type='categorical',\n     nrows=n_rows, ncols=n_cols, hspace=0.5, figsize = (15, 20), color='0.75')","d70ced3f":"# Separate target from features\ny = df_train['target']\nfeatures = df_train.drop(['target'], axis=1)\n\n# Preview features\nfeatures.head()","16124a35":"# ordinal-encode categorical columns\nX = features.copy()\nX_test = df_test.copy()\nordinal_encoder = OrdinalEncoder()\nX[object_cols] = ordinal_encoder.fit_transform(features[object_cols])\nX_test[object_cols] = ordinal_encoder.transform(df_test[object_cols])\n\n# Preview the ordinal-encoded features\nX.head()","495d95e0":"X_train, X_valid, y_train, y_valid = train_test_split(X, y, random_state =0)","98a8a58b":"# Define the model \nmodel = XGBRegressor(n_estimators=1375, max_depth = 3,learning_rate=0.14, colsample_bytree= 0.5,\n                     subsample=0.99, random_state=1, reg_alpha = 25.4)\n\n# Train the model \nmodel.fit(X_train, y_train, early_stopping_rounds = 100, eval_set=[(X_valid, y_valid)], verbose=False)\npreds_valid = model.predict(X_valid)\nprint(mean_squared_error(y_valid, preds_valid, squared=False))","11cdc33b":"from sklearn.model_selection import GridSearchCV\ndef  Hyperparameter_tuning(params):\n        '''We use this function to get the best hyperparameters\n        Parameters\n        ----------\n        param: dict\n        A dictionary of hyperparameters names and lists of possible values of it\n        example:\n        param = { 'max_depth': [3,6,10],\n           'learning_rate': [0.01, 0.05, 0.1],\n           'n_estimators': [100, 500, 1000],\n           'colsample_bytree': [0.3, 0.7]}'''\n        params = params\n        model = XGBRegressor(n_estimators=1000, max_depth = 3,learning_rate=0.14,\n                             colsample_bytree= 0.5, subsample=0.99, random_state=1,\n                             reg_alpha = 25.4, tree_method = 'gpu_hist')\n        clf = GridSearchCV(estimator=model, \n                           param_grid=params,\n                           scoring='neg_mean_squared_error', \n                           verbose=2)\n        clf.fit(X, y)\n        print(\"Best parameters:\", clf.best_params_)\n        print(\"Lowest RMSE: \", (-clf.best_score_)**(1\/2.0))\n        \n#we set squared=False to get the root mean squared error (RMSE) on the validation data.","347d21a1":"# Use the model to generate predictions\npredictions = model.predict(X_test)\n\n# Save the predictions to a CSV file\n\n# output = pd.DataFrame({'id': X_test.index, 'target': predictions}) \n# output.to_csv('submission4.csv', index=False)\n\nsub = pd.read_csv(\"..\/input\/30-days-of-ml\/sample_submission.csv\")\nsub.target = predictions\n\nsub.to_csv('submission2.csv', index=False)\nsub.head()\n\nprint(\"Submission file created\")","6a71370f":"Target is weakly correlated with all features","648ce4dc":"# Train the model","12d8ef59":"# Prepare the data","c93dbe29":"# SUBMIT IT!!","07127020":"# **Reading the data**"}}