{"cell_type":{"5ca61ef4":"code","3e37e8f7":"code","7f95e8f3":"code","c66cdba4":"code","ffa29f76":"code","5f01d8ad":"code","8e2b3c3f":"code","52b56d05":"code","f241af57":"code","ef2a59c4":"code","374123b5":"code","5de14ed7":"code","7c1702a6":"code","50efb9d0":"code","68e66732":"code","daef16c6":"code","9f0909b4":"code","a858af4c":"code","58f51342":"code","74732810":"code","eb133c0a":"code","3677e23e":"code","03199f9a":"code","2bccc570":"code","50bf067c":"code","a5e09b8e":"code","17c9e107":"code","fd1a64a7":"code","70a8b378":"code","08d41b94":"code","1bb8f8bb":"code","6f125bac":"code","3cc21137":"code","26d5f3c0":"code","244676b1":"code","3a78fd1b":"code","d7a4a29b":"code","f1a2b9ca":"code","961ca7ac":"code","a9daac94":"code","94f84c4d":"code","ba952b3f":"code","9a6f1a3a":"code","ea98a343":"markdown","739bb067":"markdown","4a0bd7b4":"markdown","b652b741":"markdown","dfdbce5f":"markdown","0c02b06a":"markdown","ffd0b225":"markdown","4ed48b9c":"markdown","66eb07f8":"markdown","84d42730":"markdown","1e8f6ac2":"markdown","0200ddbe":"markdown","4925af34":"markdown","b7da1c6b":"markdown","16f35ff9":"markdown","530bd3ec":"markdown","eb625549":"markdown","c3bce59d":"markdown","d64e3f22":"markdown","695269cd":"markdown"},"source":{"5ca61ef4":"# Open this notebook on google colab or kaggle kernel\n# Change runtime type to GPU\n# In December 2019, tensorflow 2.0 is only available in google colab or kaggle kernel\n# ! pip install tensorflow==2.0.0","3e37e8f7":"# Import the relevant libraries\nimport os\nimport json\nimport zipfile\nimport shutil\nimport random\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\n\nfrom tensorflow.keras import layers\nfrom tensorflow.keras import Model\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.preprocessing.image import load_img\nfrom shutil import copyfile\n\nprint(tf.__version__)","7f95e8f3":"# Import the inception V3 model  \nfrom tensorflow.keras.applications.inception_v3 import InceptionV3\n\n# Clear any exiting model\ntf.keras.backend.clear_session()\n\npre_trained_model = InceptionV3(input_shape = (150,150,3), # reshape images to 150 by 150 by 3 channels\n                                include_top = False, # get straight to the CNN layer\n                                weights = 'imagenet') # use the builed-in weight pre-trained on imagent\n\n# Make all the layers in the pre-trained model non-trainable\/frozen\nfor layer in pre_trained_model.layers:\n  layer.trainable = False","c66cdba4":"# Print the InceptionV3 model summary\n# pre_trained_model.summary()","ffa29f76":"# Pick one layer from the miffde of the model\n# We well build new dense layers after this layer\n\nlast_layer = pre_trained_model.get_layer('mixed7')\nprint('last layer output shape: ', last_layer.output_shape)\nlast_output = last_layer.output","5f01d8ad":"# Define a Callback class that stops training once accuracy reaches a threshold\n\nclass myCallback(tf.keras.callbacks.Callback):\n  def on_epoch_end(self, epoch, logs={}):\n    if(logs.get('acc')>0.875):\n      print(\"\\nReached 87.5% accuracy so cancelling training!\")\n      self.model.stop_training = True","8e2b3c3f":"from tensorflow.keras.optimizers import RMSprop\n\n# Add layers after the picked last_layer\n# Note that the layers below are trainable. \n# On the other hand, the layers up to the last_output is fronzen and non-trainable.\n\n# Flatten the output layer to 1 dimension\nx = layers.Flatten()(last_output)\n# Add a fully connected layer with 1,024 hidden units and ReLU activation\nx = layers.Dense(1024, activation='relu')(x)\n# Add a dropout rate of 0.2\nx = layers.Dropout(0.2)(x)                  \n# Add a final sigmoid layer for binary classification\nx = layers.Dense(1, activation='sigmoid')(x)           \n\n# instantiate a new model \nmodel = Model(pre_trained_model.input ,x) \n\n# Compile a model\nmodel.compile(optimizer = RMSprop(lr=0.0001),  # We use RMSprop in this case\n              loss = 'binary_crossentropy',  # cats vs. dogs is a binary problem\n              metrics = ['acc']) # 'acc' stands for accuracy\n\n# See the summary of the model \n# model.summary()","52b56d05":"print(os.listdir(\"..\/input\/dogs-vs-cats\"))","f241af57":"# Delete the directory we will create if it already exists\ntry:\n    shutil.rmtree(\"\/tmp\/cat_or_dog\/\")    \nexcept:\n    pass\n\n# Create a new directory for the dataset\ntry:\n    os.mkdir(\"\/tmp\/cat_or_dog\/\")\nexcept:\n    pass","ef2a59c4":"# Unzip\u3000the training data\nlocal_zip = '..\/input\/dogs-vs-cats\/train.zip'\nzip_ref = zipfile.ZipFile(local_zip, 'r')\nzip_ref.extractall('\/tmp\/cat_or_dog')\nzip_ref.close()","374123b5":"os.listdir('\/tmp\/cat_or_dog')","5de14ed7":"# Login to kaggle \n# Join the competition where the data are stored\n# Go to My account and click \"Create new API Token\" \n# A json file will be downloaded. It can be opened in any text file\n# Set your kaggle username and key in the dictionary below \n\n\n# api_token = {\"username\":\"your_kaggle_username\",\"key\":\"your_kaggle_key\"}","7c1702a6":"# Create a .kaggle\u3000directory in the root directory \n\n\n# try:\n#     os.mkdir(\"\/root\/.kaggle\/\")\n# except:\n#     pass","50efb9d0":"# Create a json file which stores the api keys\n\n\n# with open('\/root\/.kaggle\/kaggle.json', 'w') as file:\n#     json.dump(api_token, file)","68e66732":"# Set owner permission to the json file \n\n# ! chmod 600 \/root\/.kaggle\/kaggle.json","daef16c6":"# Delete the directory we will create if it already exists\n\n# try:\n#     shutil.rmtree(\"\/tmp\/cat_or_dog\/\")    \n# except:\n#     pass\n\n# # Create a new directory for the dataset\n# try:\n#     os.mkdir(\"\/tmp\/cat_or_dog\/\")\n# except:\n#     pass","9f0909b4":"# Go to the data tab in the kaggle competition and copy the command  \n# After -p is the directory to store the data\n\n# ! kaggle competitions download -c dogs-vs-cats -p \/tmp\/cat_or_dog\/","a858af4c":"# Unzip\u3000the training data\n\n\n# local_zip = '\/tmp\/cat_or_dog\/train.zip'\n# zip_ref = zipfile.ZipFile(local_zip, 'r')\n# zip_ref.extractall('\/tmp\/cat_or_dog')\n# zip_ref.close()","58f51342":"# Cats and dogs images are stored in the same dirctory\nprint(os.listdir('\/tmp\/cat_or_dog'))\nprint(os.listdir('\/tmp\/cat_or_dog\/train')[:10])\nfiles_list = os.listdir('\/tmp\/cat_or_dog\/train')","74732810":"# Create directories for training and validation\n\nbase_train_dir = '\/tmp\/cat_or_dog\/training\/'\nbase_val_dir = '\/tmp\/cat_or_dog\/validation\/'\n\n# Delete the directory we will create if it already exists\ntry:\n    shutil.rmtree(base_train_dir)    \nexcept:\n    pass\n\ntry:\n    shutil.rmtree(base_val_dir)    \nexcept:\n    pass\n\n# Directory with our training cat\/dog pictures\ntrain_cats_dir = os.path.join(base_train_dir, 'cats\/')\ntrain_dogs_dir = os.path.join(base_train_dir, 'dogs\/')\n\n# Directory with our validation cat\/dog pictures\nval_cats_dir = os.path.join(base_val_dir, 'cats\/')\nval_dogs_dir = os.path.join(base_val_dir, 'dogs\/')\n\n\ntry:\n    os.mkdir(base_train_dir)\n    os.mkdir(base_val_dir)\n    os.mkdir(train_cats_dir)\n    os.mkdir(train_dogs_dir)\n    os.mkdir(val_cats_dir)\n    os.mkdir(val_dogs_dir)  \nexcept:\n    pass","eb133c0a":"# When using .flow_from_directory in the generator,  the \"training\" directory should have sub-directories for the classes, \"dogs\" and \"cats\"\n# Another option is to use .flow_from_dataframe. In this case, a dataframe with filename and corresponding categories is required. \nprint(os.listdir(base_train_dir))\n\n# Same for the \"validation\" directory \nprint(os.listdir(base_val_dir))","3677e23e":"# Shuffle the files before splting into training set and test set\nrandom.seed(0)\nrandom_files_list = random.sample(files_list,len(files_list))\nprint(random_files_list[:10])\nprint(len(files_list))\nprint(len(random_files_list))","03199f9a":"# Deleat empty files\n# In this dataset, there is no empty files\nSOURCE = '\/tmp\/cat_or_dog\/train\/'\n\nrandom_files_clean_list = []\n\nfor filename in random_files_list:\n    file = os.path.join(SOURCE + filename)\n    if os.path.getsize(file) > 0:\n        random_files_clean_list.append(filename)\n    else:\n        print(filename + \" is zero length, so ignoring.\")\n\n# print(random_files_clean_list)\nprint(len(random_files_list))\nprint(len(random_files_clean_list))","2bccc570":"# Split the files into training set and validation set\n\n# Set the ratio for validation set\nval_ratio = 0.3 \n\ntraining_list = random_files_clean_list[:int(len(random_files_clean_list)*(1 - val_ratio))]\nval_list = random_files_clean_list[-int(len(random_files_clean_list)*(val_ratio)):]\n\nprint(len(training_list))\nprint(len(val_list))\nprint(len(training_list) + len(val_list))","50bf067c":"# Create the training set\n\nSOURCE = '\/tmp\/cat_or_dog\/train\/' # the current directory where files are stored\n\nfor filename in training_list:\n    this_file = os.path.join(SOURCE + filename)\n    cat_destination = os.path.join(train_cats_dir + filename)\n    dog_destination = os.path.join(train_dogs_dir + filename)\n    if 'cat' in filename:\n        copyfile(this_file, cat_destination)\n    elif 'dog' in filename:\n        copyfile(this_file, dog_destination)","a5e09b8e":"train_dog_fnames = os.listdir(train_dogs_dir)\ntrain_cat_fnames = os.listdir(train_cats_dir)\n\n\nprint(os.listdir(train_cats_dir)[:5])\nprint(os.listdir(train_dogs_dir)[:5])\nprint(len(os.listdir(train_cats_dir)))\nprint(len(os.listdir(train_dogs_dir)))","17c9e107":"# Create the validation set\n\nSOURCE = '\/tmp\/cat_or_dog\/train\/' # the current directory where files are stored\n\nfor filename in val_list:\n    this_file = os.path.join(SOURCE + filename)\n    cat_destination = os.path.join(val_cats_dir + filename)\n    dog_destination = os.path.join(val_dogs_dir + filename)\n\n    \n    if 'cat' in filename:\n        copyfile(this_file, cat_destination)\n    elif 'dog' in filename:\n        copyfile(this_file, dog_destination)","fd1a64a7":"# print(os.listdir(val_cats_dir))\n# print(os.listdir(val_dogs_dir))\nprint(len(os.listdir(val_cats_dir)))\nprint(len(os.listdir(val_dogs_dir)))","70a8b378":"%matplotlib inline\n\nimport matplotlib.image as mpimg\nimport matplotlib.pyplot as plt\n\n# Parameters for our graph; we'll output images in a 4x4 configuration\nnrows = 4\nncols = 4\n\npic_index = 0 # Index for iterating over images","08d41b94":"# Set up matplotlib fig, and size it to fit 4x4 pics\n# The 8 images on the upper half are dogs, others are cats \nfig = plt.gcf()\nfig.set_size_inches(ncols*3, nrows*3)\n\npic_index+=8\n\nnext_cat_pix = [os.path.join(train_cats_dir, fname) \n                for fname in train_cat_fnames[ pic_index-8:pic_index] \n               ]\n\nnext_dog_pix = [os.path.join(train_dogs_dir, fname) \n                for fname in train_dog_fnames[ pic_index-8:pic_index]\n               ]\n\nfor i, img_path in enumerate(next_cat_pix+next_dog_pix):\n  # Set up subplot; subplot indices start at 1\n  sp = plt.subplot(nrows, ncols, i + 1)\n  sp.axis('Off') # don't show axes (or gridlines)\n\n  img = mpimg.imread(img_path)\n  plt.imshow(img)\n\nplt.show()","1bb8f8bb":"# Add our data-augmentation parameters to ImageDataGenerator\ntrain_datagen = ImageDataGenerator(rescale = 1.\/255., # rescale the images within 0-1. This yield better results when handling images. \n                                   rotation_range = 40,\n                                   width_shift_range = 0.2,\n                                   height_shift_range = 0.2,\n                                   shear_range = 0.2,\n                                   zoom_range = 0.2,\n                                   horizontal_flip = True)\n\n# Note that the validation data should not be augmented\ntest_datagen = ImageDataGenerator(rescale = 1.0 \/ 255.0)\n\n# Flow training images in batches of 20 using train_datagen generator\ntrain_generator = train_datagen.flow_from_directory(base_train_dir,\n                                                    batch_size = 20,\n                                                    class_mode = 'binary',\n                                                    target_size = (150, 150))     \n\n# Flow validation images in batches of 20 using test_datagen generator\nvalidation_generator =  test_datagen.flow_from_directory(base_val_dir,\n                                                         batch_size = 20,\n                                                         class_mode = 'binary',\n                                                         target_size = (150,150))","6f125bac":"# See the indices of the classes\n# In keras ImageDataGenerator class, classes are indexed in alphabetical order\ntrain_generator.class_indices","3cc21137":"# Fit the model the training data\n# Take the log of the training in history\ncallbacks = myCallback()\nhistory = model.fit_generator(\n    generator = train_generator, # feed the training data via the generator\n    steps_per_epoch = 8, # this is the batch size. parameters are updated per this batch size\n    epochs = 100, # number of cycles. In one epoch, the whole dataset is used once.\n    verbose = 2, # print out the logs\n    callbacks = [callbacks], # use callbacks we set before\n    validation_data = validation_generator, # feed the validation data via the generator\n    validation_steps = 50 \n)","26d5f3c0":"# Plot the accuracy history \nimport matplotlib.pyplot as plt\nacc = history.history['acc']\nval_acc = history.history['val_acc']\nloss = history.history['loss']\nval_loss = history.history['val_loss']\n\nepochs = range(len(acc))\n\nplt.plot(epochs, acc, 'r', label='Training accuracy')\nplt.plot(epochs, val_acc, 'b', label='Validation accuracy')\nplt.title('Training and validation accuracy')\nplt.legend(loc=0)\nplt.figure()\n\n\nplt.show()","244676b1":"# Inside the kaggle kernel\n# Unzip\u3000the test data\nlocal_zip = '..\/input\/dogs-vs-cats\/test1.zip'\nzip_ref = zipfile.ZipFile(local_zip, 'r')\nzip_ref.extractall('\/tmp\/cat_or_dog')\nzip_ref.close()","3a78fd1b":"# Outside the kaggle kernel\n# Unzip\u3000the test data\n# local_zip = '\/tmp\/cat_or_dog\/test1.zip'\n# zip_ref = zipfile.ZipFile(local_zip, 'r')\n# zip_ref.extractall('\/tmp\/cat_or_dog')\n# zip_ref.close()","d7a4a29b":"base_test_dir = '\/tmp\/cat_or_dog\/test1\/'\n\n# print(os.listdir(base_test_dir))","f1a2b9ca":"# To use .flow_from_dataframe. in the generator, we have to create a dataframe  \ntest_filenames = os.listdir(base_test_dir)\ntest_df = pd.DataFrame({\n    'filename': test_filenames\n})\nnb_samples = test_df.shape[0]","961ca7ac":"# Create a generator for test set\ntest_gen = ImageDataGenerator(rescale=1.\/255.0)\nbatch_size=20\n\ntest_generator = test_gen.flow_from_dataframe(\n    test_df, \n    base_test_dir, \n    x_col='filename',\n    y_col=None,\n    class_mode=None,\n    batch_size=batch_size,\n    target_size=(150, 150),\n    shuffle=False\n)","a9daac94":"# Predict the test set\npredict = model.predict_generator(test_generator, steps=np.ceil(nb_samples\/batch_size))\nthreshold = 0.5\ntest_df['category'] = np.where(predict > threshold, 1,0) # we can also ","94f84c4d":"# See sample results, # 1 = dog, 0 = cat\nsample_test = test_df.head(18)\nsample_test.head()\nplt.figure(figsize=(9, 18))\nfor index, row in sample_test.iterrows():\n    filename = row['filename']\n    category = row['category']\n    img = load_img(base_test_dir+filename, target_size=(150,150))\n    plt.subplot(6, 3, index+1)\n    plt.imshow(img)\n    plt.xlabel(filename + '(' + \"{}\".format(category) + ')' ) \nplt.tight_layout()\nplt.show()","ba952b3f":"submission_df = test_df.copy()\nsubmission_df['id'] = submission_df['filename'].str.split('.').str[0]\nsubmission_df['label'] = submission_df['category']\nsubmission_df.drop(['filename', 'category'], axis=1, inplace=True)\nsubmission_df.to_csv('\/tmp\/cat_or_dog\/submission_20191230.csv', index=False)","9a6f1a3a":"!  kaggle competitions submit -c dogs-vs-cats -f \/tmp\/cat_or_dog\/ submission_20191229.csv -m \"First submission\"","ea98a343":"So far, the model has learned nothing from the Dogs vs. Cats dataset. \n<hr>","739bb067":"<hr>","4a0bd7b4":"<hr>","b652b741":"<div style=\"font-size:14pt\" font-weight=\"bold\">Step 7. Prediction<\/div><br>\nPrediction of the unlabeled test set images is the final stage. ","dfdbce5f":"<div style=\"font-size:14pt\" font-weight=\"bold\">Step 3. Split the dataset into training set and validation set<\/div><br> \nFor details about training set and validation set, please find Andrew Ng's lecture #1 in the <a href=\"https:\/\/www.youtube.com\/watch?v=1waHlpKiNyY&list=PLkDaE6sCZn6Hn0vK8co82zjQtt3T2Nkqc&index=2&t=8s\"> link<\/a>, or #5 in the <a href=\"https:\/\/www.youtube.com\/watch?v=M3qpIzy4MQk&list=PLkDaE6sCZn6E7jZ9sN_xHwSHOdjUxUW_b&index=6&t=0s\"> link<\/a>.","0c02b06a":"<hr>","ffd0b225":"Droptout: please find lecture #6 in the <a href=\"https:\/\/www.youtube.com\/watch?v=D8PJAL-MZv8&list=PLkDaE6sCZn6Hn0vK8co82zjQtt3T2Nkqc&index=7&t=0s\"> link <\/a>.<br>\nRMSProp:  lecture #21 in the <a href=\"https:\/\/www.youtube.com\/watch?v=_e-LFe_igno&list=PLkDaE6sCZn6Hn0vK8co82zjQtt3T2Nkqc&index=22&t=0s\"> link <\/a>.","4ed48b9c":"<hr>","66eb07f8":"<div style=\"font-size:14pt\" font-weight=\"bold\">Step2.3 Load the dataset from kaggle<\/div><br> \nThe code below are the same for users outside the kaggle or inside the kaggle kernel","84d42730":"<hr>","1e8f6ac2":"<div style=\"font-size:14pt\" font-weight=\"bold\">Step2.1 Load the dataset from kaggle<\/div><br> \nIf you are in the kaggle kernel implement the followings:","0200ddbe":"<div style=\"font-size:14pt\" font-weight=\"bold\">Submit (from outside the kaggle)<\/div><br>\n<br>\nNote that we cannot submit past competiton.","4925af34":"<div style=\"font-size:14pt\" font-weight=\"bold\">Step 1. Create a model by transfer learning<\/div><br>\nTransfer learning is a technique to use parameters of a pre-trained model. In image classification, there are many models pre-trained on <a href=\"http:\/\/www.image-net.org\/\"> ImageNet<\/a>.  ImageNet is a database of images which has more than 14 million images with 20,000 categories. Several models including ResNet50 and InceptionV3 are available in <a href=\"https:\/\/keras.io\/applications\/\"> keras<\/a>. <br>\nFor theortical background of CNN(convolutional neural network) and transfer learninng, deeplearning.ai lecture videos by professor Andrew Ng are very helpful.\nPlease find lecture #1-20 in the <a href=\"https:\/\/www.youtube.com\/playlist?list=PLkDaE6sCZn6Gl29AoE31iwdVwSG-KnDzF\"> link <\/a>. ","b7da1c6b":"<div style=\"font-size:14pt\" font-weight=\"bold\">Step2.2 Load the dataset from kaggle<\/div><br> \nIf yur are o[](http:\/\/)utside the kaggle kernel, implement the following:","16f35ff9":"<hr>","530bd3ec":"<div style=\"font-size:14pt\" font-weight=\"bold\">Step 4. See sample images<\/div><br>","eb625549":"<div style=\"font-size:16pt\" font-weight=\"bold\">Dogs vs. Cats, Keras CNN with transfer learning (Inception V3)<\/div><br>\n<br> The structure of this notebook:\n<ol>\n<li>Create a model by transfer learning\n<li>Load the dataset from kaggle\n<li>Split the dataset into training set and validation set\n<li>See sample images\n<li>Data Augmentation\n<li>Training\n<li>Prediction\n<\/ol>\n<br>\nReference:<br> Thie code of this notebook is inspired by <a href=\"https:\/\/www.coursera.org\/specializations\/tensorflow-in-practice\"> TensorFlow in Practice Specialization<\/a>.<br> The theoretical background is explained in <a href=\"https:\/\/www.coursera.org\/specializations\/deep-learning\"> Deep Learning Specialization<\/a>. The lecuture videos are also available on <a href=\"https:\/\/www.youtube.com\/channel\/UCcIXc5mJsHVYTZR1maL5l9w\">youtube<\/a>. Both are offered by deeplearning.ai","c3bce59d":"<div style=\"font-size:14pt\" font-weight=\"bold\">If you are inside the kaggle kernel, Skip Step2.2 and jump tp Step 2.3<\/div><br>\n> <hr>","d64e3f22":"<div style=\"font-size:14pt\" font-weight=\"bold\">Step 5. Data Augmentation<\/div><br>\nData augmentation is a technique to artifically create training data. It rotates, shifts, zooms, shears, or horizontally flips the training images. In keras ImageDataGenerator class, the original images are unchaged. It just implements data augmentation when they are feeded into the training\/fitting.\nFor details, please find Andrew Ng's lecture #21 in the <a href=\"https:\/\/www.youtube.com\/watch?v=JI8saFjK84o&list=PLkDaE6sCZn6Gl29AoE31iwdVwSG-KnDzF&index=22&t=0s\"> link <\/a>.","695269cd":"<div style=\"font-size:14pt\" font-weight=\"bold\">Step 6. Training<\/div><br>\nWhen implementing training, it is very important to understand the concept of bias\/underfitting and variance\/overfitting. When a model has high bias, it has low performance both on the training set and the validation set. If the model has high variance, the model performs very well on the training set but has low performance on the validation set. <br><br>When the model is underfitting, there are two solutions among others. One way is to do more training or train on more epochs. If it still not work well, we have to change the model architecture. For example, if the target objects are not in the center of the images, neural networks without convolutional layers tend to have poor performance (high bias). In that case, CNN is a better choice.  <br><br>\nWhen the model is overfitting, there are muliple choices. Regularization\/add penalties, drop out, or training on fewer epochs are among them. Data augmentation is also a way to reduce overfitting. <br><br>\nBias and Variance: Please find Andrew Ng's lecture #2 in the <a href=\"https:\/\/www.youtube.com\/watch?v=SjQyLhQIXSM&list=PLkDaE6sCZn6Hn0vK8co82zjQtt3T2Nkqc&index=2\">link<\/a>.<br>\n"}}