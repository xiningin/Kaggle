{"cell_type":{"de28631f":"code","e8374590":"code","238aebfe":"code","5c77a29d":"code","15b33885":"code","6cd1b3ab":"code","50981390":"code","8a08fb78":"code","01e62f42":"code","5846fa0d":"code","9afe0058":"code","fc77a544":"code","24fa540d":"code","ab13dcec":"code","6301a0f1":"code","3aa6be8a":"code","a2e3b2ce":"code","9415dfc8":"code","513aae27":"code","aaefd94b":"code","9fd33ab6":"code","9a67d0a1":"code","7e1df091":"code","cb0fffaa":"code","488d9575":"code","2327712d":"code","e57b1fd4":"code","67f0d8a5":"code","04a4f263":"code","234a2311":"code","6b5ecef6":"code","71c5a53b":"code","025c3b93":"code","bd822e3c":"code","ab4cfdd5":"code","fec02674":"code","9a92b39b":"code","80bbdd85":"code","66b3a7f3":"code","17769b3c":"code","0ed63b89":"code","05877c37":"code","63ccbce7":"code","98e3c1ea":"code","2e196e6f":"markdown","d6098b4d":"markdown","786e3455":"markdown","59eb6a99":"markdown","e0b895e0":"markdown","8cee93c5":"markdown","0cc591b6":"markdown","b01eea9c":"markdown","30b4a44c":"markdown","743c261d":"markdown","944c3ae5":"markdown","e1598b64":"markdown","f51020bc":"markdown","b54d54c6":"markdown","2663402b":"markdown","84c27e58":"markdown","27109fd8":"markdown","7942d457":"markdown","83d41263":"markdown","98fbf04e":"markdown","5a286e63":"markdown","a167d1b0":"markdown","1bbd2998":"markdown","541a3c1b":"markdown","f7be54aa":"markdown","75210e6b":"markdown","e5c43c92":"markdown","95aee456":"markdown","d74ac867":"markdown","87974ddb":"markdown","71f1d32c":"markdown","7530c885":"markdown","e3e54378":"markdown","ac77f0bc":"markdown","343688ac":"markdown","809ff0c8":"markdown","a767a53e":"markdown","daa30c78":"markdown","e5d75625":"markdown","d0ad257d":"markdown","d28fd341":"markdown","450b15ed":"markdown","75a5a179":"markdown","2e08edbe":"markdown","9aa178a1":"markdown","2df76037":"markdown","ca72bbe2":"markdown","0566a765":"markdown","756c4d6b":"markdown","fe54fa85":"markdown","f29e2279":"markdown"},"source":{"de28631f":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","e8374590":"import pandas as pd\nfrom keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation, Conv1D, GRU, BatchNormalization\nfrom keras.layers import Bidirectional, GlobalMaxPool1D, MaxPooling1D, Add, Flatten\nfrom keras.layers import GlobalAveragePooling1D, GlobalMaxPooling1D, concatenate, SpatialDropout1D\nfrom keras.models import Model, load_model\nfrom keras import initializers, regularizers, constraints, optimizers, layers, callbacks\nfrom keras import backend as K\nfrom keras.engine import InputSpec, Layer\nfrom keras.optimizers import Adam\nfrom keras.callbacks import ModelCheckpoint, TensorBoard, Callback, EarlyStopping\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom keras.utils import plot_model\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom nltk.stem import WordNetLemmatizer\nimport nltk\nimport re","238aebfe":"posts = pd.read_csv('\/kaggle\/input\/reddit-flair-dataset\/reddit_data.csv')","5c77a29d":"posts.head()","15b33885":"posts['flair'].value_counts()","6cd1b3ab":"plt.figure(figsize=(20,8))\nsns.countplot(x='flair', data=posts)\nplt.title(\"Number of Posts of each Flair\")\nplt.xlabel('Flairs')\nplt.ylabel(\"Number of Posts\")\nplt.show()","50981390":"posts['num_comments'].sum()\/posts.shape[0]","8a08fb78":"plt.figure(figsize=(20,8))\nsns.distplot(posts[posts[\"num_comments\"] < 61][\"num_comments\"], kde=False)\nplt.grid()\nplt.title(\"Distrbution of number of Comments on the Posts\")\nplt.ylabel(\"Number of Posts\")\nplt.xlabel(\"Number of Comments\")\n\nplt.show()","01e62f42":"posts['score'].sum()\/posts.shape[0]","5846fa0d":"plt.figure(figsize=(20,8))\nsns.distplot(posts[posts[\"score\"] < 147][\"score\"], kde=False)\nplt.grid()\nplt.title(\"Distrbution of Score on the Posts\")\nplt.ylabel(\"Number of Posts\")\nplt.xlabel(\"Score\")\n\nplt.show()","9afe0058":"data_score = posts.sort_values('num_comments', ascending=False).head(15)\nplt.figure(figsize=(14,7))\nplt.title(\"Posts with highest number of Comments\")\nsns.barplot(y=data_score['title'],x=data_score['num_comments'])\nplt.xlabel('Score',fontsize=18)\nplt.ylabel(\"Post's Title\",fontsize=18)\nplt.show()","fc77a544":"data_score = posts.sort_values('score', ascending=False).head(15)\nplt.figure(figsize=(14,9))\nplt.title('Posts with highest Score')\nsns.barplot(y=data_score['title'],x=data_score['score'])\nplt.xlabel('Score',fontsize=18)\nplt.ylabel('Title',fontsize=18)\nplt.show()","24fa540d":"correlation =  posts.corr()\ncorrelation","ab13dcec":"sns.heatmap(correlation)\nplt.show()","6301a0f1":"import nltk\nnltk.download('stopwords')","3aa6be8a":"from nltk.corpus import stopwords\nstop = set(stopwords.words('english'))","a2e3b2ce":"flairs = posts['flair'].unique()\nfrom wordcloud import WordCloud, STOPWORDS \nfor flair in flairs:\n  content = \"\"\n  for i in posts[posts['flair']==flair]['title']:\n    content+=i\n  wordcloud = WordCloud(width = 800, height = 800, background_color ='white', stopwords = stop, min_font_size = 10).generate(content)\n  plt.figure(figsize = (8, 8), facecolor = None) \n  plt.imshow(wordcloud) \n  plt.title(\"WordClound for \"+flair)\n  plt.axis(\"off\")\n  plt.tight_layout(pad = 0) \n  plt.show() ","9415dfc8":"nltk.download('wordnet')\n\ndef remove_noise(text):\n\n    # Step1: Make lowercase\n    text = text.apply(lambda x: \" \".join(x.lower() for x in x.split()))\n    \n    # Step2: Remove whitespaces\n    text = text.apply(lambda x: \" \".join(x.strip() for x in x.split()))\n\n    # Step3 : Removing words inside brackets like \"[OC]\"\n    text = text.apply(lambda x: re.sub(r\"\\[.*?\\]\", \"\", x))\n\n    # Step4 : Removing everything from the data which is not alphanumeric.\n    text = text.apply(lambda x: re.sub('[^a-zA-Z0-9\\s]', '', x))\n\n    # Step5 : Lemmatization\n    lm=WordNetLemmatizer()\n    text = text.apply(lambda x: lm.lemmatize(x))\n    \n    # Step6 : Removing Stopwords\n    text = text.apply(lambda x: \" \".join(x for x in x.split() if x not in stop))\n\n    # Convert to string\n    text = text.astype(str)\n        \n    return text","513aae27":"posts['title'] = remove_noise(posts['title'])","aaefd94b":"posts['title']","9fd33ab6":"# Shuffling\nposts = posts.sample(frac=1)","9a67d0a1":"tokenizer = Tokenizer(lower=True, split=' ')\ntokenizer.fit_on_texts(posts['title'].values)","7e1df091":"sequences = tokenizer.texts_to_sequences(posts['title'].values)","cb0fffaa":"import pickle\n\n# saving\nwith open('tokenizer.pickle', 'wb') as handle:\n    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)","488d9575":"totalNumWords = [len(one_comment) for one_comment in sequences]\nplt.figure(figsize=(15, 10))\nplt.hist(totalNumWords, bins=[i for i in range(1,70, 5)])\nplt.show()","2327712d":"# Padding\nMAX_LEN = 50\npadded_sequences = pad_sequences(sequences, maxlen=MAX_LEN)","e57b1fd4":"padded_sequences","67f0d8a5":"embedding_path = '\/kaggle\/input\/fasttext-crawl-300d-2m\/crawl-300d-2M.vec'","04a4f263":"embed_size = 300\nmax_features = 30000","234a2311":"import numpy as np\ndef get_coefs(word,*arr): \n    return word, np.asarray(arr, dtype='float32')\n\nembedding_index = dict(get_coefs(*o.strip().split(\" \")) for o in open(embedding_path))\n\nword_index = tokenizer.word_index\nnb_words = min(max_features, len(word_index))\nembedding_matrix = np.zeros((nb_words + 1, embed_size))\nfor word, i in word_index.items():\n    if i >= max_features: continue\n    embedding_vector = embedding_index.get(word)\n    if embedding_vector is not None: \n        \n        # Words not found in embedding index will be all-zeros.\n        embedding_matrix[i] = embedding_vector","6b5ecef6":"embedding_matrix.shape","71c5a53b":"from sklearn.preprocessing import OneHotEncoder\nohe = OneHotEncoder(sparse=False)\ny_ohe = ohe.fit_transform(posts['flair'].values.reshape(-1, 1))","025c3b93":"y_ohe","bd822e3c":"ohe.categories_","ab4cfdd5":"import pickle\n\n# saving\nwith open('encoder.pickle', 'wb') as handle:\n    pickle.dump(ohe, handle, protocol=pickle.HIGHEST_PROTOCOL)","fec02674":"file_path = \"best_model.hdf5\"\ncheck_point = ModelCheckpoint(file_path, monitor = \"val_accuracy\", verbose = 1,\n                              save_best_only = True, mode = \"max\")","9a92b39b":"early_stop = EarlyStopping(monitor = \"val_accuracy\", mode = \"max\", patience = 25)","80bbdd85":"def build_model(lr = 0.0, lr_d = 0.0, units = 0, dr = 0.0):\n    inp = Input(shape = (50,))\n    x = Embedding(embedding_matrix.shape[0], embed_size, weights = [embedding_matrix], trainable = False)(inp)\n    x1 = SpatialDropout1D(dr)(x)\n    global history\n    x_gru = Bidirectional(GRU(units, return_sequences = True))(x1)\n    x1 = Conv1D(32, kernel_size=3, padding='valid', kernel_initializer='he_uniform')(x_gru)\n    avg_pool1_gru = GlobalAveragePooling1D()(x1)\n    max_pool1_gru = GlobalMaxPooling1D()(x1)\n    \n    x3 = Conv1D(32, kernel_size=2, padding='valid', kernel_initializer='he_uniform')(x_gru)\n    avg_pool3_gru = GlobalAveragePooling1D()(x3)\n    max_pool3_gru = GlobalMaxPooling1D()(x3)\n    \n    x_lstm = Bidirectional(LSTM(units, return_sequences = True))(x1)\n    x1 = Conv1D(32, kernel_size=3, padding='valid', kernel_initializer='he_uniform')(x_lstm)\n    avg_pool1_lstm = GlobalAveragePooling1D()(x1)\n    max_pool1_lstm = GlobalMaxPooling1D()(x1)\n    \n    x3 = Conv1D(32, kernel_size=2, padding='valid', kernel_initializer='he_uniform')(x_lstm)\n    avg_pool3_lstm = GlobalAveragePooling1D()(x3)\n    max_pool3_lstm = GlobalMaxPooling1D()(x3)\n    \n    \n    x = concatenate([avg_pool1_gru, max_pool1_gru, avg_pool3_gru, max_pool3_gru,\n                    avg_pool1_lstm, max_pool1_lstm, avg_pool3_lstm, max_pool3_lstm])\n    x = BatchNormalization()(x)\n    x = Dropout(0.2)(Dense(128,activation='relu') (x))\n    x = BatchNormalization()(x)\n    x = Dropout(0.2)(Dense(100,activation='relu') (x))\n    x = Dense(11, activation = \"sigmoid\")(x)\n    \n    model = Model(inputs = inp, outputs = x)\n    model.compile(loss = \"binary_crossentropy\", optimizer = Adam(lr = lr, decay = lr_d), metrics = [\"accuracy\"])\n    history = model.fit(padded_sequences, y_ohe, batch_size = 128, epochs = 200, validation_split=0.3, \n                        verbose = 1, callbacks = [check_point, early_stop])\n    model = load_model(file_path)\n    import matplotlib.pyplot as plt\n    print(history.history.keys())\n   \n    \n    return model","66b3a7f3":"model = build_model(lr = 1e-4, lr_d = 0, units = 128, dr = 0.5)","17769b3c":"plt.figure(figsize=(15, 10))\nplt.plot(history.history['accuracy'], label=\"acc\")\nplt.plot(history.history['val_accuracy'], label=\"val_Acc\")\nplt.grid()\nplt.legend()\nplt.show()\nplt.figure(figsize=(15, 10))\nplt.plot(history.history['loss'], label=\"loss\")\nplt.plot(history.history['val_loss'], label=\"val_loss\")\nplt.grid()\nplt.legend()\nplt.show()","0ed63b89":"plot_model(model, to_file='.\/model.png')","05877c37":"def remove_noise_test(x):\n  text = [\" \".join(x.lower() for x in x[0].split())]\n  text = [\" \".join(x.strip() for x in text[0].split())]\n  text = [\" \".join(x for x in text[0].split() if x not in stop)]\n  text = [re.sub(r\"\\[.*?\\]\", \"\", text[0])]\n  text = [re.sub('[^a-zA-Z0-9\\s]', '', text[0])]\n  lm=WordNetLemmatizer()\n  text = [lm.lemmatize(text[0])]\n  text = tokenizer.texts_to_sequences(x)\n  text = pad_sequences(text, maxlen=50, dtype='int32', value=0)\n  return text","63ccbce7":"x = [\"State Visit of Prime Minister Gandhi of India. President Reagan's Speech and Prime Minister Gandhi's Speech at Arrival Ceremony, South Lawn on July 29, 1982\"]\n\ntext = remove_noise_test(x)\n\nans = model.predict(text, batch_size=1, verbose=2)\n\nohe.categories_[0][ans[0].argmax()]","98e3c1ea":"x = [\"Holy river Ganges self-cleared during lockdown. This was shot at Triveni Ghat, Rishikesh, Uttarakhand.\"]\n\ntext = remove_noise_test(x)\n\nans = model.predict(text, batch_size=1, verbose=2)\n\nohe.categories_[0][ans[0].argmax()]","2e196e6f":"I am going to classify the posts by checking the thier titles.","d6098b4d":"Loading the dataset","786e3455":"To make each vector of same length I'll use padding.\nThat means,<br>\nSuppose the maximum length of a vector is set to 8.<br>\nWe have 3 vectors of different length.<br>\n [195, 18, 158, 365],<br>\n [1073, 787, 196, 614, 2608, 1074],<br>\n [507, 23, 419, 253],<br>\n After padding it will look like:<br>\n [0, 0, 0, 0, 195, 18, 158, 365],<br>\n [0, 0, 1073, 787, 196, 614, 2608, 1074],<br>\n [0, 0, 0, 0, 507, 23, 419, 253],<br>\n Now the lenght of all three of them is 8.","59eb6a99":"Pretrained Word Embeddings are the embeddings learned in one task that are used for solving another similar task.","e0b895e0":"before that lets make a set of stopwords which are not needed to detect the flair of a sentence.","8cee93c5":"In NLP tasks Embeding Layer would be the first hidden layer of the model.<br>\nKeras offers an Embedding layer that can be used for neural networks on text data. It requires that the input data be integer encoded, so that each word is represented by a unique integer. <br>\nThe Embedding layer is initialized with random weights and will learn an embedding for all of the words in the training dataset.<br>\nA word embedding is an approaches for representing words and documents using a dense vector representation.<br>\nHere I am not lettting the Embedding Layer to initialize it's random weights. I am using a ***TRANSFER LEARNING*** approach to train the model.","0cc591b6":"Now our sentence are converted to int vectors but they are of different length because every sentence had different number of words. The data should be structured correctly so that it can be feed into the Deep Learning model.","b01eea9c":"Initializing the pretrained word embeddings of FastText Crawl by Facebook for ***Transfer Learning***.","30b4a44c":"Removing noise from the Title feature of the dataset","743c261d":"This Embedding is 300-dimnesional.","944c3ae5":"Now I am going to convert the text data into integer vectors. First I'll split each sentence by space and then assign a number to each distinct words.<br>\nFor Example:\n\"Where are you going\" ---> [\"Where\", \"are\", \"you\", \"\"going\"] ---> [12, 43, 5, 34]","e1598b64":"Plotting the **Wordclouds** of each flair.","f51020bc":"Creating a function build_model that contains all the layers of our non-sequential fully connected Deep Learning model.","b54d54c6":"Here I am preparing the Embedding Matrix for the model by the FastText embedding.","2663402b":"Shuffling the data","84c27e58":"Checking the average number of scores on the posts in the dataset","27109fd8":"The FastText Carwl word embedding that I downloaded from Kaggle are pre-trained word embeddings trained and released by Facebook after training on 2 million words. The size of embedding is 4GB.","7942d457":"It is clearly visible that most of the posts contains atmost 30 score in the dataset.","83d41263":"Making a Histogram to check the Score Distribution for the posts in the dataset","98fbf04e":"These are all the categories in the dataset","5a286e63":"Now lets check the most occured words in the title of each flair. I am plotting the wordcloud which shows the mostly occured word with biggest font size and least occured word with smallest font size.","a167d1b0":"Let's check that more number of Comments results more score or not i.e, does these factors relates each other. For this I'll first find the correlation mactrix of columns than plot a heatmap.","1bbd2998":"Showing top 15 posts having highest number of comments","541a3c1b":"This FastText crawl embedding is trained on large datasets, saved, and then I am using it for solving other tasks. That\u2019s why pretrained word embeddings are a form of **Transfer Learning**.<br>\nTransfer learning, as the name suggests, is about transferring the learnings of one task to another. ","f7be54aa":"Making a Histogram to check the Comment Distribution for the posts in the dataset","75210e6b":"Now I am going to encode output column(flair) using One Hot Encoding. This is where the words like ASKINDIA, AMA etc is removed and a new binary variable is added for each unique value.","e5c43c92":"Here we can see that there is not any post's title having length more than 50.","95aee456":"### The final validation accracy is 94.2% and validation loss is 0.16","d74ac867":"According to the above graph most of the posts are having atmost 10 comments in the dataset","87974ddb":"### Lets start building the model","71f1d32c":"Importing all necessary libraries","7530c885":"Plotting the model to show all the layers and flow of my Neural Network.","e3e54378":"# Part II - Building a Flare Detector","ac77f0bc":"Saving the ohe object for further usage in other files.","343688ac":"The data is almost equally distributed<br>\n\n","809ff0c8":"# Part I - Exploratory Data Analysis","a767a53e":"Before making a Deep Learning model, I have to choose the more relevant words that can be used to classify easily and also I have to convert the text data into vectors so that I can feed the data to the model for training.","daa30c78":"Setting embedding size & max number of features in next code cell.","e5d75625":"Checking the average number of comments on the posts in the dataset","d0ad257d":"Vizualizing the loss and accuracy or training and validation in each epoch.","d28fd341":"Saving the tokenizer for further usage in other files.","450b15ed":"Shwoing top 15 posts having highest score","75a5a179":"I am using both GRU and LSTM layers in the model. GRU use less training parameters and therefore use less memory, execute faster and train faster than LSTM's whereas LSTM is more accurate on dataset using longer sequence. Both layers can be helpful in the NLP related task.","2e08edbe":"Making a function to remove noise i.e, unwanted data from our dataset. This function is doing following tasks: \n1. Converting each and every word or letter to lower case\n2. Removing Extra Whitespaces\n3. Removing words inside brackets like [OC], [ASKINDIA] etc.\n4. Removing every character which is non alphanumeric.\n5. Limmatizing the data.It is the process of grouping together the different inflected forms of a word so they can be analysed as a single item. Such as \"good better\" becomes \"good good\".\n6. Removing stopwords like \"if\", \"here\", \"there\" etc. which are not needed to classify the sentences.\n    By using these steps we can reduce the redundancy and the size of data so that our model get trained on a relevant data.\n","9aa178a1":"Here I am copying some real Reddit posts and cross checking the flair.","2df76037":"Initializing the file path for best model and Checkpoint to save the model whenever the validation accuarcy improves.","ca72bbe2":"check_point and early_stop are be known as model callbacks.","0566a765":"For Validation I am splitting 30% of data. That means model will train on 1534 samples, validate on 658 samples","756c4d6b":"Defining an early stop with patience=25, that means if the validation accuracy didn't improved in last 25 epochs that model will stop training.","fe54fa85":"To set the maximum length of the sentence I am going to check the distribution of length of senteces in the title feature of the dataset.","f29e2279":"Here we can see that there is a small positive correlation of 0.3 between score and number of comments. That mean there is a little bit relation between these columns but do not completly dependent upon each other."}}