{"cell_type":{"dd69cf57":"code","406c1ce4":"code","070e084a":"code","ed6053d2":"code","5a124b3d":"code","5444333a":"code","fb713e29":"code","7c17181b":"code","9dd01313":"code","63792a30":"code","26b939a8":"code","81470770":"code","aa119ffa":"code","b088d7b6":"code","482261e2":"code","7190efbe":"code","151e3ecc":"code","fb69a5cd":"code","eca79a37":"code","6d9c1a8e":"code","3a9b3428":"code","ce3c61c8":"code","e0c58451":"code","7c7b97b5":"code","17e85a97":"code","4af8450a":"code","d42b4e46":"code","e3372ad6":"code","71a48611":"code","6eaa8565":"code","6621ccc7":"code","4f106da2":"code","0fa7729a":"code","f4122583":"code","222b1234":"code","963cf812":"code","ec7e8458":"markdown","25689adc":"markdown","64e4e8ae":"markdown","7b3654ea":"markdown","b784bebe":"markdown","85a33240":"markdown","148ac231":"markdown","0a7d9e27":"markdown","86f29b22":"markdown","07c67f24":"markdown","e27621a6":"markdown","41aff759":"markdown","c257c25e":"markdown","2d3d8908":"markdown","c9345ff3":"markdown"},"source":{"dd69cf57":"import numpy as np \nimport pandas as pd \n\ndf = pd.read_csv('..\/input\/superheroes-nlp-dataset\/superheroes_nlp_dataset.csv')\ndf.head(3)","406c1ce4":"cdf = df[['creator', 'history_text']]\ncdf.head(3)","070e084a":"print('Null Values')\nprint(cdf.isnull().sum())\nprint('________________________')\nprint(cdf['creator'].value_counts())","ed6053d2":"cdf = cdf.dropna()\nprint(cdf.isnull().sum())\nprint('--------------------------------')\nprint(cdf['creator'].value_counts())","5a124b3d":"mask1 = cdf.loc[(cdf['creator'] == 'Marvel Comics' )]\nmask2 = cdf.loc[(cdf['creator'] == 'DC Comics' )]\nframes = [mask1, mask2]\ncdf = pd.concat(frames)\ncdf","5444333a":"print(cdf.isnull().sum())\nprint('--------------------------------')\nprint(cdf['creator'].value_counts())","fb713e29":"import matplotlib.pyplot as plt\nimport seaborn as sns\nsns.countplot(cdf['creator'], palette=\"plasma\")\nfig = plt.gcf()\nfig.set_size_inches(8,5)\nplt.title('Score')","7c17181b":"features = []\ntarget = []","9dd01313":"for i in cdf['history_text']:\n    features.append(i)\n    \nfor i in cdf['creator']:\n    target.append(i)\n    ","63792a30":"Cloud_Marvel = cdf['history_text'].loc[(cdf.creator == 'Marvel Comics')]\nCloud_Marvel = Cloud_Marvel.sum()\nCloud_DC = cdf['history_text'].loc[(cdf.creator == 'DC Comics')]\nCloud_DC = Cloud_DC.sum()","26b939a8":"import matplotlib.pyplot as plt\n%matplotlib inline \nfrom wordcloud import WordCloud\nwordcloud = WordCloud(max_font_size=300,background_color=\"black\").generate(Cloud_Marvel)\nplt.figure()\nplt.imshow(wordcloud, interpolation=\"bilinear\")\nplt.axis(\"off\")\nplt.show()","81470770":"import matplotlib.pyplot as plt\n%matplotlib inline \nfrom wordcloud import WordCloud\nwordcloud = WordCloud(max_font_size=300,background_color=\"black\").generate(Cloud_DC)\nplt.figure()\nplt.imshow(wordcloud, interpolation=\"bilinear\")\nplt.axis(\"off\")\nplt.show()","aa119ffa":"from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score","b088d7b6":"from nltk.corpus import stopwords  \nstopwords = set(stopwords.words('english'))  \nstopwords","482261e2":"from nltk.stem import PorterStemmer\n\nstemmer = PorterStemmer()\nanalyzer = CountVectorizer().build_analyzer()\n\ndef stemmed_words(doc):\n    return (stemmer.stem(w) for w in analyzer(doc))\n","7190efbe":"\n# Unigram and Bigram no TF , 1 & 2\n\nvect1 = CountVectorizer(stop_words = stopwords)\nvect2 = CountVectorizer(stop_words = stopwords , ngram_range = (1,2))\n\nX_t1 = vect1.fit_transform(features)\nX_t2 = vect2.fit_transform(features)\n\n# Unigram and Bigram with TF 3 & 4\n\ntf1 = TfidfVectorizer(use_idf=False, norm = \"l1\", stop_words = stopwords)\ntf2 = TfidfVectorizer(use_idf=False, norm = \"l1\", stop_words = stopwords, ngram_range = (1, 2))\n\nX_t3 = tf1.fit_transform(features)\nX_t4 = tf2.fit_transform(features)\n\n# Unigram and Bigram with TF-IDF 5 & 6\n\ntf_idf1 = TfidfVectorizer(norm=\"l1\", stop_words = stopwords)\ntf_idf2 = TfidfVectorizer(norm = \"l1\", stop_words = stopwords, ngram_range = (1, 2))\n\nX_t5 = tf_idf1.fit_transform(features)\nX_t6 = tf_idf2.fit_transform(features)\n\n# Unigram and Bigram stemmed\n\nstem_vect1 = CountVectorizer(analyzer=stemmed_words,stop_words = stopwords)\nstem_vect2 = CountVectorizer(analyzer=stemmed_words,stop_words = stopwords,ngram_range = (1,2))\n\nX_t7 = stem_vect1.fit_transform(features)\nX_t8 = stem_vect2.fit_transform(features)\n\n# Unigram and Bigram stemmed with term frequency\n\nstem_tf1 = TfidfVectorizer(analyzer=stemmed_words, use_idf=False, norm = \"l1\", stop_words = stopwords)\nstem_tf2 = TfidfVectorizer(analyzer=stemmed_words, use_idf=False, norm = \"l1\", stop_words = stopwords, ngram_range = (1, 2))\n\nX_t9 = stem_tf1.fit_transform(features)\nX_t10 = stem_tf2.fit_transform(features)\n\n# Unigram and Bigram stemmed with term frequency using IDF\nstem_tf_idf1 = TfidfVectorizer(analyzer=stemmed_words, norm=\"l1\", stop_words = stopwords)\nstem_tf_idf2 = TfidfVectorizer(analyzer=stemmed_words, norm = \"l1\", stop_words = stopwords, ngram_range = (1, 2))\n\nX_t11 = stem_tf_idf1.fit_transform(features)\nX_t12 = stem_tf_idf2.fit_transform(features)","151e3ecc":"X_train1, X_test1, y_train1, y_test1 = train_test_split(X_t1, target, test_size = .2, random_state = 42)\nX_train2, X_test2, y_train2, y_test2 = train_test_split(X_t2, target, test_size = .2, random_state = 42)\nX_train3, X_test3, y_train3, y_test3 = train_test_split(X_t3, target, test_size = .2, random_state = 42)\nX_train4, X_test4, y_train4, y_test4 = train_test_split(X_t4, target, test_size = .2, random_state = 42)\nX_train5, X_test5, y_train5, y_test5 = train_test_split(X_t5, target, test_size = .2, random_state = 42)\nX_train6, X_test6, y_train6, y_test6 = train_test_split(X_t6, target, test_size = .2, random_state = 42)\nX_train7, X_test7, y_train7, y_test7 = train_test_split(X_t7, target, test_size = .2, random_state = 42)\nX_train8, X_test8, y_train8, y_test8 = train_test_split(X_t8, target, test_size = .2, random_state = 42)\nX_train9, X_test9, y_train9, y_test9 = train_test_split(X_t9, target, test_size = .2, random_state = 42)\nX_train10, X_test10, y_train10, y_test10 = train_test_split(X_t10, target, test_size = .2, random_state = 42)\nX_train11, X_test11, y_train11, y_test11 = train_test_split(X_t11, target, test_size = .2, random_state = 42)\nX_train12, X_test12, y_train12, y_test12 = train_test_split(X_t12, target, test_size = .2, random_state = 42)\n\nX_train = [X_train1, X_train2, X_train3, X_train4, X_train5, X_train6, X_train7, X_train8, X_train9, X_train10, X_train11, X_train12]\nX_test = [X_test1, X_test2, X_test3, X_test4, X_test5, X_test6, X_test7, X_test8, X_test9, X_test10, X_test11, X_test12]\ny_train = [y_train1, y_train2, y_train3, y_train4, y_train5, y_train6, y_train7, y_train8, y_train9, y_train10, y_train11, y_train12]\ny_test = [y_test1, y_test2, y_test3, y_test4, y_test5, y_test6, y_test7, y_test8, y_test9, y_test10, y_test11, y_test12]","fb69a5cd":"from sklearn.naive_bayes import MultinomialNB\nNB = []\nfor a, b in zip(X_train, y_train):\n    NB.append(MultinomialNB().fit(a, b))","eca79a37":"NB_acc = []\n\nfor a, b, c in zip(NB, X_test, y_test):\n    predict = a.predict(b)\n    acc = accuracy_score(c, predict)\n    NB_acc.append(acc)\n    \nprint(NB_acc)","6d9c1a8e":"from sklearn.linear_model import LogisticRegression\nLR = []\nfor a, b in zip(X_train, y_train):\n    LR.append(LogisticRegression(solver = 'liblinear', random_state=42).fit(a, b))","3a9b3428":"LR_acc = []\n\nfor a, b, c in zip(LR, X_test, y_test):\n    predict = a.predict(b)\n    acc = accuracy_score(c, predict)\n    LR_acc.append(acc)\n    \nprint(LR_acc)","ce3c61c8":"import xgboost as xgb\nXGBoost = []\nfor a, b in zip(X_train, y_train):\n    XGBoost.append(xgb.XGBClassifier(objective=\"binary:logistic\", random_state=42).fit(a, b))","e0c58451":"XGBoost_acc = []\n\nfor a, b, c in zip(XGBoost, X_test, y_test):\n    predict = a.predict(b)\n    acc = accuracy_score(c, predict)\n    XGBoost_acc.append(acc)\n    \nprint(XGBoost_acc)","7c7b97b5":"from sklearn.svm import SVC\nSVM = []\nfor a, b in zip(X_train, y_train):\n    SVM.append(SVC().fit(a, b))","17e85a97":"SVM_acc = []\n\nfor a, b, c in zip(SVM, X_test, y_test):\n    predict = a.predict(b)\n    acc = accuracy_score(c, predict)\n    SVM_acc.append(acc)\n    \nprint(SVM_acc)","4af8450a":"from sklearn.ensemble import RandomForestClassifier\nForest = []\nfor a, b in zip(X_train, y_train):\n    Forest.append(RandomForestClassifier(max_depth=2, random_state=42).fit(a,b))","d42b4e46":"Forest_acc = []\n\nfor a, b, c in zip(Forest, X_test, y_test):\n    predict = a.predict(b)\n    acc = accuracy_score(c, predict)\n    Forest_acc.append(acc)\n    \nprint(Forest_acc)","e3372ad6":"from sklearn import tree as tree_model \nTree = []\nfor a, b in zip(X_train, y_train):\n    Tree.append(tree_model.DecisionTreeClassifier().fit(a, b))","71a48611":"Tree_acc = []\n\nfor a, b, c in zip(Tree, X_test, y_test):\n    predict = a.predict(b)\n    acc = accuracy_score(c, predict)\n    Tree_acc.append(acc)\n    \nprint(Tree_acc)\n","6eaa8565":"d = {\n    'MultinomialNB': NB_acc, 'Logistic Regression': LR_acc,\n    'XGBoost': XGBoost_acc, 'SVM': SVM_acc, 'Random Forest': Forest_acc,\n     'Decision Tree': Tree_acc\n    }\n\nbest_acc =pd.DataFrame(data=d)\nbest_acc.head(12)","6621ccc7":"import seaborn as sns\nax = sns.heatmap(best_acc, annot=True)","4f106da2":"DC = [\"A boy lost your parents with eight years old, he was train and later became a vigilant in his city!\",\n           \n\"A baby falls down in a farm, he was born on the planet Krypton and was given the name Kal-El at birth. As a baby, his parents sent him to Earth in a small spaceshipmoments before his planet was destroyed in a natural cataclysm.Now he resides in the fictional American city of Metropolis, where he works as a journalist for the Daily Planet.\",\n\n\"He is a vigilant in Star City, His main weapon is a bow and arrow, his favorite color is Green\",\n           \n\"Princess Diana of an all-female Amazonian race rescues US pilot Steve. Upon learning of a war, she ventures into the world of men to stop Ares, the god of war, from destroying mankind.\",\n      \n\"He is the fastest man alive, sometimes actually, when he was a kid, his mom was murdered from a yellow man, a yellow thing\"          \n          ]\n\nMarvel = [\"A wealthy American business magnate, playboy, and ingenious scientist, he suffers a severe chest injury during a kidnapping. When his captors attempt to force him to build a weapon of mass destruction, he instead creates a mechanized suit of armor to save his life and escape captivity.\",\n \"The history is simple, He is the Wakanda King\",\"His Father is Odin\",\n           \n\"He was a normal scientist falling in love for a beautiful scientist, but now when he is Angry, he become a green monster, and everybody call him HULK\",\n\"He is a good person, a good hero, an old hero, he is a captain, a leader, currently he is an avenger.\"\n     ]","0fa7729a":"\nMarvel1 = vect1.transform(Marvel)\nMarvel2 = vect2.transform(Marvel)\nMarvel3 = tf1.transform(Marvel)\nMarvel4 = tf2.transform(Marvel)\nMarvel5 = tf_idf1.transform(Marvel)\nMarvel6 = tf_idf2.transform(Marvel)\nMarvel7 = stem_vect1.transform(Marvel)\nMarvel8 = stem_vect2.transform(Marvel)\nMarvel9 = stem_tf1.transform(Marvel)\nMarvel10 = stem_tf2.transform(Marvel)\nMarvel11 = stem_tf_idf1.transform(Marvel)\nMarvel12 = stem_tf_idf2.transform(Marvel)\n\nDC1 = vect1.transform(DC)\nDC2 = vect2.transform(DC)\nDC3 = tf1.transform(DC)\nDC4 = tf2.transform(DC)\nDC5 = tf_idf1.transform(DC)\nDC6 = tf_idf2.transform(DC)\nDC7 = stem_vect1.transform(DC)\nDC8 = stem_vect2.transform(DC)\nDC9 = stem_tf1.transform(DC)\nDC10 = stem_tf2.transform(DC)\nDC11 = stem_tf_idf1.transform(DC)\nDC12 = stem_tf_idf2.transform(DC)","f4122583":"print(NB[1].predict(DC2))\nprint('-'*20)\nprint(NB[1].predict(Marvel2))","222b1234":"print(LR[1].predict(DC2))\nprint('-'*20)\nprint(LR[1].predict(Marvel2))","963cf812":"print(SVM[11].predict(DC12))\nprint('-'*20)\nprint(SVM[11].predict(Marvel12))","ec7e8458":"## Random Forest","25689adc":"# The Best","64e4e8ae":"## XG BOOST","7b3654ea":"## The better\nTesting with the better algorithm we have, MultinomialNB V2 (vectorizer with bigram)","b784bebe":"## Multinomial Naive Bayes","85a33240":"# Some tests","148ac231":"# Conclusions\n\nThe better in all tests is the MultinomialNB V2, trained with the dataset of bigrams.\n\nIn this dataset, we can see some words there is only in a histories from DC Comics, like cities, so Gotham for example is a powerful keyword that does not exist in texts from Marvel creator.\n\n\n\n","0a7d9e27":"## Stemmer","86f29b22":"## Logistic Regression","07c67f24":"## Decision Tree","e27621a6":"# Modeling\n\n> The problems is simple, then I'll use simple models.","41aff759":"## Vectorizer","c257c25e":"## Other tests","2d3d8908":"## Support Vector Machine","c9345ff3":"## Stopwords"}}