{"cell_type":{"c0029c3a":"code","b605faaa":"code","ffe46a03":"code","4f903891":"code","5cb3683a":"code","76d6bab8":"code","e5aaf3df":"code","1dd9f019":"code","d099c9c2":"code","84e5f72c":"code","5042cfb2":"code","ff58d61d":"code","7763a964":"code","7be22866":"code","51687fe7":"code","c38aa59a":"code","d4a54175":"code","ff9bf610":"code","66687db8":"code","b5afda17":"code","0cf033bd":"code","c221cf6a":"code","7456c422":"code","abd880f1":"code","d1dd8a18":"code","766baebc":"code","ca58e01b":"code","8f117d1b":"code","6beb08dc":"code","4277f77f":"code","e05efd91":"code","421c4923":"code","b509abbd":"code","f4dc37cb":"markdown","98bee91e":"markdown","7b4f4866":"markdown","3c821a22":"markdown","49712010":"markdown","cbecfd32":"markdown","e9b47206":"markdown","523ab577":"markdown","6c220ba6":"markdown","91b4842a":"markdown","2eaa72d5":"markdown","583d2e32":"markdown","2eadb68b":"markdown","fc3e8389":"markdown","4c9cda12":"markdown","930b5da1":"markdown","64149c01":"markdown","a026c564":"markdown","c8eadf94":"markdown","10872ed0":"markdown","b00cec38":"markdown","b518d596":"markdown","40cac4c5":"markdown","ca9bd2c8":"markdown"},"source":{"c0029c3a":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","b605faaa":"# Reading the data\ndf = pd.read_csv('..\/input\/weather-dataset-rattle-package\/weatherAUS.csv')\ndf","ffe46a03":"# Checking the data types\ndf.info()","4f903891":"# Changing the data types of Date to datetime\ndf['Date'] = pd.to_datetime(df['Date'])","5cb3683a":"# Checking for the missing values in the target variable\ndf['RainTomorrow'].isnull().sum()","76d6bab8":"# Droping the missing values\ndf = df.dropna(subset = ['RainTomorrow'])","e5aaf3df":"# Checking for the class imbalance\nfig = plt.figure(figsize = (10, 6))\naxis = sns.countplot(x = 'RainTomorrow', data = df);\naxis.set_title('Class Distribution for the target feature', size = 16);\n\nfor patch in axis.patches:\n    axis.text(x = patch.get_x() + patch.get_width()\/2, y = patch.get_height()\/2, \n            s = f\"{np.round(patch.get_height()\/len(df)*100, 1)}%\", \n            ha = 'center', size = 40, rotation = 0, weight = 'bold' ,color = 'white')\n    \naxis.set_xlabel('Rain Tomorrow', size = 14)\naxis.set_ylabel('Count', size = 14);","1dd9f019":"# months and days in a cyclic continuous feature.\n\ndef encode(data, col, max_val):\n    data[col + '_sin'] = np.sin(2 * np.pi * data[col]\/max_val)\n    data[col + '_cos'] = np.cos(2 * np.pi * data[col]\/max_val)\n    return data\n\ndf['month'] = df['Date'].dt.month\ndf = encode(df, 'month', 12)\n\ndf['day'] = df['Date'].dt.day\ndf = encode(df, 'day', 31)","d099c9c2":"# Let's look at the transformed features\n\nplt.style.use('ggplot')\nfig, (ax1,ax2,ax3) = plt.subplots(1,3, figsize = (12, 4), constrained_layout = True)\n\nax1 = sns.lineplot(x = 'month', y = 'day', data = df, estimator = None, ax=ax1)\nax2 = sns.scatterplot(x = 'day_sin', y = 'day_cos', data = df, ax = ax2)\nax3 = sns.scatterplot(x = 'month_sin', y = 'month_cos', data = df, ax = ax3)\n\nax1.set_title('Original Day Distribution')\nax2.set_title('Cyclic encoding of Day')\nax3.set_title('Cyclic encoding of Month')\n\nfig.suptitle('Feature Engineering for Cyclic Features', size = 16, y = 1.1);","84e5f72c":"# Splitting the data into train and test\nfrom sklearn.model_selection import train_test_split\ndf_train, df_test = train_test_split(df, train_size = 0.8, random_state  = 99, stratify = df['RainTomorrow'])","5042cfb2":"# Let's first handle missing values for catergorical data\ncategorical_col = df_train.select_dtypes('object').columns[:-1].to_list()\ndf_train[categorical_col].isnull().mean()*100","ff58d61d":"# Imputing with the mode\nfor col in categorical_col:\n    df_train[col].fillna(df_train[col].mode()[0], inplace = True)\n    df_test[col].fillna(df_train[col].mode()[0], inplace = True) # Imputing test data using train data","7763a964":"# Missing values for numeric data\nnumeric_col = df.describe().columns.to_list()\ndf_train[numeric_col].isnull().mean()*100","7be22866":"# Let's explore the features having high missing values\ncols = ['Evaporation', 'Sunshine', 'Cloud9am', 'Cloud3pm']\n\nplt.style.use('seaborn-dark')\nfig, ax = plt.subplots(4,2, figsize = (12, 8), constrained_layout = True)\n\nfor i, num_var in enumerate(cols): \n    sns.kdeplot(data = df_train, x = num_var, ax = ax[i][0],\n                fill = True, alpha = 0.6, linewidth = 1.5)\n    ax[i][0].set_ylabel(num_var)\n    ax[i][0].set_xlabel(None)\n    \n    sns.histplot(data = df_train, x = num_var, ax = ax[i][1])\n    ax[i][1].set_ylabel(None)\n    ax[i][1].set_xlabel(None)\n    \nfig.suptitle('Features having high missing values (>35%)', size = 16);","51687fe7":"# Droping the columns with high missing values (>35%) and distributed data\n# for dataframe in [df_train, df_test]:\n#     dataframe.drop(columns = ['Sunshine', 'Cloud9am', 'Cloud3pm'], axis = 1, inplace = True)\n\nfor dataframe in [df_train, df_test]:\n    for cols in ['Sunshine', 'Cloud9am', 'Cloud3pm']:\n        dataframe[cols].fillna(df_train[cols].median(), inplace = True)\n\n    dataframe['Evaporation'].fillna(df_train['Evaporation'].mean(), inplace = True)","c38aa59a":"# Removing the missing values from the remaining numerical features as they are <10%.\n# numeric_col = ['MinTemp', 'MaxTemp', 'Rainfall','WindGustSpeed', 'WindSpeed9am', 'WindSpeed3pm',\n#                'Humidity9am','Humidity3pm', 'Pressure9am', 'Pressure3pm', 'Temp9am', 'Temp3pm']\n\n# for dataframe in [df_train, df_test]:\n#     for col in numeric_col:\n#         # Imputing missing values with median based on train set\n#         dataframe[col].fillna(df_train[col].median(), inplace = True)\n\ndf_train.dropna(inplace = True)\ndf_test.dropna(inplace = True)","d4a54175":"# Checking for the correlation between the numeric features\n# Correlation between numeric variables\n\nnumeric_col = ['MinTemp', 'MaxTemp', 'Rainfall','WindGustSpeed', 'WindSpeed9am', 'WindSpeed3pm',\n               'Humidity9am','Humidity3pm', 'Pressure9am', 'Pressure3pm', 'Temp9am', 'Temp3pm',\n              'Sunshine', 'Cloud9am', 'Cloud3pm', 'Evaporation']\n\nfig=plt.figure(figsize=(16,12))\naxis=sns.heatmap(df_train[numeric_col].corr(), annot=True, linewidths=3, square=True, cmap='Blues', fmt=\".0%\")\n\naxis.set_title('Correlation between the features', fontsize=16);\naxis.set_xticklabels(numeric_col, fontsize=12)\naxis.set_yticklabels(numeric_col, fontsize=12, rotation=0);","ff9bf610":"# Droping the columns\nfor dataframe in [df_train, df_test]:\n    dataframe.drop(['Temp3pm', 'Pressure3pm', 'Temp9am'], axis = 1, inplace = True)","66687db8":"# Let's look at the outliers and the distribution of the numeric features\n\nnumeric_col = ['MinTemp', 'MaxTemp', 'Rainfall','WindGustSpeed', 'WindSpeed9am', 'WindSpeed3pm',\n               'Humidity9am','Humidity3pm', 'Pressure9am', 'Sunshine', 'Cloud9am', 'Cloud3pm', 'Evaporation']\n\nplt.style.use('seaborn')\nfig, axis = plt.subplots(13, 2, figsize = (12, 24))\nfor i, num_var in enumerate(numeric_col):\n    \n    # Checking for the outliers using boxplot\n    sns.boxplot(y = num_var, data = df_train, ax = axis[i][0], color = 'skyblue')\n    \n    # Checking for the distribution using kdeplot\n    sns.kdeplot(x = num_var, data = df_train, ax = axis[i][1], color = 'skyblue',\n               fill = True, alpha = 0.6, linewidth = 1.5)\n    \n    axis[i][0].set_ylabel(f\"{num_var}\", fontsize = 12)\n    axis[i][0].set_xlabel(None)\n    axis[i][1].set_xlabel(None)\n    axis[i][1].set_ylabel(None)\n\nfig.suptitle('Analysing Numeric Features', fontsize = 16, y = 1)\nplt.tight_layout()","b5afda17":"threshold = 0.05\nfor col in numeric_col:\n    \n    # Lower and upper threshold\n    lower_threshold = df_train[col].quantile(threshold)\n    upper_threshold = df_train[col].quantile(1-threshold)\n    \n    # Dropping the values below lower threshold and beyond upper threshold\n    df_train = df_train[(df_train[col]>=lower_threshold) & (df_train[col]<=upper_threshold)]\n    df_test = df_test[(df_test[col]>=lower_threshold) & (df_test[col]<=upper_threshold)]","0cf033bd":"# Converting 'Yes' and 'No' to '1' and '0' respectively\ndf_train['RainTomorrow'] = df_train['RainTomorrow'].map(dict({'Yes':1, 'No':0}))\ndf_test['RainTomorrow'] = df_test['RainTomorrow'].map(dict({'Yes':1, 'No':0}))","c221cf6a":"# Dropping the features not required for model\ndf_train.drop(['Date', 'day', 'month'], axis = 1 ,inplace = True)\ndf_test.drop(['Date', 'day', 'month'], axis = 1 ,inplace = True)","7456c422":"# Splitting the data into y and X\ny_train = df_train.pop('RainTomorrow')\nX_train = df_train\n\ny_test = df_test.pop('RainTomorrow')\nX_test = df_test","abd880f1":"# Now the data is ready for preprocessing, let's convert categorical variables into one hot encodings\nX_train = pd.get_dummies(X_train, drop_first = True).reset_index(drop = True)\nX_test = pd.get_dummies(X_test, drop_first = True).reset_index(drop = True)","d1dd8a18":"# Getting the categorical columns\nnumeric_col = ['MinTemp', 'MaxTemp', 'Rainfall', 'Evaporation', 'Sunshine',\n               'WindGustSpeed', 'WindSpeed9am', 'WindSpeed3pm', 'Humidity9am',\n               'Humidity3pm', 'Pressure9am', 'Cloud9am', 'Cloud3pm',\n               'month_sin', 'month_cos', 'day_sin', 'day_cos']\n\ncategorical_col = [i for i in X_train.columns if i not in numeric_col]","766baebc":"# Now the data is ready for preprocessing\nfrom sklearn.preprocessing import StandardScaler\n\nscalar = StandardScaler()\n\nX_train_scale = pd.DataFrame(scalar.fit_transform(X_train[numeric_col]), columns = numeric_col) # fit_transform on train\nX_test_scale = pd.DataFrame(scalar.transform(X_test[numeric_col]), columns = numeric_col) # only transform on test","ca58e01b":"# Creating final train and test data\nX_train_final = pd.concat([X_train_scale, X_train[categorical_col]], axis = 1)\nX_test_final = pd.concat([X_test_scale, X_test[categorical_col]], axis = 1)","8f117d1b":"from tensorflow import keras\nfrom keras.models import Sequential\nfrom keras.layers import Dense, BatchNormalization, Dropout","6beb08dc":"# Creating the ANN\nmodel = Sequential()\n\n# layers\nmodel.add(Dense(units = 1024, kernel_initializer = 'uniform', activation = 'relu', input_dim = X_train_final.shape[1]))\nmodel.add(Dense(units = 512, kernel_initializer = 'uniform', activation = 'relu'))\nmodel.add(Dropout(0.3))\nmodel.add(Dense(units = 32, kernel_initializer = 'uniform', activation = 'relu'))\nmodel.add(Dropout(0.4))\nmodel.add(Dense(units = 1, kernel_initializer = 'uniform', activation = 'sigmoid'))\n\n# Compiling the ANN\nmodel.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy', keras.metrics.AUC()])","4277f77f":"# For live plotting\nfrom IPython.display import clear_output\nclass live_accuracy(keras.callbacks.Callback):\n    plt.style.use('ggplot')\n    \n    def on_train_begin(self, logs={}):\n        self.i = 0\n        self.x = []\n        self.accuracy = []\n        self.val_accuracy = []\n        self.auc = []\n        \n        self.fig = plt.figure()\n        \n        self.logs = []\n\n    def on_epoch_end(self, epoch, logs={}):\n        \n        plt.xlim([0, epochs-1])\n        plt.ylim([0.5, 1.0])\n        plt.title('Training and Validation Accuracy', size = 16)\n        plt.xlabel('epochs')\n        plt.ylabel('accuracy')\n        \n        self.logs.append(logs)\n        self.x.append(self.i)\n        self.accuracy.append(logs.get('accuracy'))\n        self.val_accuracy.append(logs.get('val_accuracy'))\n        self.auc.append(logs.get('auc'))\n        \n        self.i += 1\n        \n        clear_output(wait=True)\n        \n        plt.plot(self.x, self.accuracy, label=\"train_accuracy\")\n        plt.plot(self.x, self.val_accuracy, label=\"val_accuracy\")\n        \n#         plt.text(x = epochs-2, y = 0.7,\n#                 s = f\"AUC : {round(self.auc[-1],2)}\",\n#                 ha = 'center', size = 14, rotation = 0, color = 'black',\n#                 bbox=dict(boxstyle=\"round,pad=1\", fc='none', ec=\"black\", lw=2))\n           \n        plt.legend()\n        plt.show();\n        \nplot_accuracy = live_accuracy()","e05efd91":"# Train the ANN\nepochs = 20\nbatch_size = 32\n\nhistory = model.fit(X_train_final, y_train, batch_size = batch_size, epochs = epochs,\n                    callbacks=[plot_accuracy], validation_split = 0.3)","421c4923":"# Model Evaluation\nfrom sklearn.metrics import confusion_matrix\ny_pred = model.predict_classes(X_test_final)\n\nmatrix = confusion_matrix(y_test, y_pred)\n\nplt.style.use('seaborn-dark')\nfig, axis1 = plt.subplots(1, 1, figsize=(10, 6), constrained_layout = True)\n\n# Threshold = 0.5\n\naxis1 = sns.heatmap(matrix, annot=True, fmt = '.0f', cbar=False, cmap='Blues',\n                    linewidths=3, square=True, ax = axis1, annot_kws={\"fontsize\":16})\naxis1.set_title(f\"Confusion Matrics\", fontsize=16, y=1.05);\naxis1.set_xlabel('Predicted', fontsize=12)\naxis1.set_ylabel('Actual', fontsize=12)\naxis1.set_xticklabels([0,1], fontsize=12 )\naxis1.set_yticklabels([0,1], fontsize=12, rotation=0);","b509abbd":"# Classification Report\nfrom sklearn.metrics import classification_report\nprint(classification_report(y_test, y_pred))","f4dc37cb":"<h2><center>  <div style=\"background-color:lightgreen;border-radius:10px; padding: 10px;\"> Model Evaluation \ud83d\udd0e<\/div><\/center><\/h2>","98bee91e":"<h3><center>  <div style=\"background-color:lightgreen;border-radius:10px; padding: 10px;\"> \ud83d\udc76 Transforming Features \ud83d\udc68<\/div><\/center><\/h3>\n\n> Feature transformation is the process of modifying your data but keeping the information. These modifications will make Machine Learning algorithms understanding easier, which will deliver better results. We will reduce repetition, improve performance, and data integrity","7b4f4866":"<h4><center>  <div style=\"background-color:skyblue;border-radius:10px; padding: 10px;\">Removing Outliers \ud83d\udc5a\ud83d\udc5a\ud83d\udc5a\ud83d\udc5a\ud83e\ude71\ud83d\udc5a<\/div><\/center><\/h4>","3c821a22":"> As the missing values are less than 10%, I will impute them with the mode.","49712010":"> Many numeric features have data points beyond IQR. I am considering a threshold of 5 percentile, for outlier removal, i.e any point beyound 95 percentile and below 5 percentile is considerd as outlier and will be removed.\n\n> The threshold of 5 percentile is choosen at random, you can very well consider other values for the threshold also.","cbecfd32":"> Now lets analyse the target variable `RainTomorrow`. I will be checking the missing values and class imbalance.","e9b47206":"<h4><center>  <div style=\"background-color:skyblue;border-radius:10px; padding: 10px;\">Removing multicollinearity \ud83e\uddd1\ud83d\udc66<\/div><\/center><\/h4>\n\n> Multicollinearity is a problem because it undermines the statistical significance of an independent variable. Other things being equal, the larger the standard error of a regression coefficient, the less likely it is that this coefficient will be statistically significant. Also it affects storage and speed.","523ab577":"<h3><center>  <div style=\"background-color:lightgreen;border-radius:10px; padding: 10px;\"> Upvote if you like it! This helps me motivate to produce more notebooks for the community \ud83d\ude0a <\/div><\/center><\/h3>","6c220ba6":"> Now I will remove the missing values from the remaining numerical features as they are <10%. One can also impute them with mean\/ median whichever is appropriate.","91b4842a":"<center><img src =\"https:\/\/kubrick.htvapps.com\/htv-prod-media.s3.amazonaws.com\/images\/nhjxygyi-1559831904.jpg?crop=1.00xw:1.00xh;0,0&resize=900:*\"><\/center>\n<h1><center>  <div style=\"background-color:lightgreen;border-radius:10px; padding: 10px;\">Introduction<\/div><\/center><\/h1>\n\n\ud83c\udf28 Motivation\n> I have been listing to the news of rain forcast since childhood and 99% of the times, they are incorrect \ud83d\ude05! I used to think that, how can the people forcating the rain be wrong so many times, is it that difficult to predict rain. This motivated me to take up this challange of forcating the rain by myself and see if I can do better than them.\n\n\ud83c\udfaf Goal\n> The goal of this notebook is to predict rain on the next day using the data science and ML skills with high accuracy.\n\n\ud83e\udd16 Artificial Neural Netwok\n> I will be predicting the rain using ANN model. One can use other classification algorithms also, but I wanted to tackle this problem with the most sofisticated algorithms we have and one of them is ANN. The focus is on getting a high accuracy and not interpreting the model or finding the feature importance, this is another reason for selecting ANN. So, let's get started!","2eaa72d5":"<h1><center>  <div style=\"background-color:lightgreen;border-radius:10px; padding: 10px;\">Reading and Cleaning the Data \ud83e\uddf9<\/div><\/center><\/h1>","583d2e32":"> `Evaporation`, `Sunshine`, `Cloud9am` and `Cloud3pm` have large missing values, so we will first look at these features and then handle the missing values for remaing numeric features.","2eadb68b":"> #### Strong correlation between\n\n`Temp3pm` and `MaxTemp`\n\n`Pressure3pm` and `Pressure9am`\n\n`Temp9am` and `MinTemp`\n\n`Temp9am` and `MaxTemp`\n\n`Temp3pm` and `Temp9am`\n\n> We will remove one of the features in each pair, to avoid multicollinearity","fc3e8389":"> The class is imbalance when the minority class has only 5-10% data. We have 22.4% data belonging to the minority class, so, there is no class imbalance. Now, let's look at other features.\n\n> I will be creating new features of `day` and `month` from `Date` column, which are cyclic in nature. If I do not do any preprocessing on them and directly feed them to ANN, the ANN can give more or less importance based on the values. Eg. days will have values from 1 to 31, so ANN thinks that value 31 is more than 1, but actually they are just days so our model can go wrong. Thus, I will be performing a transformation on these features to make them cyclic.\n\n<center><img src = \"https:\/\/www.math.hkust.edu.hk\/~machiang\/1013\/Notes\/sine_2.gif\"><\/center>\n\n> A circle is the projection of cyclic pattern. This concept is used here, to make feature transformations.","4c9cda12":"> Except `Evaporation`, all other three have distributed data, so we will impute the missing values with the median, and impute missing values for`Evaporation` with mean.","930b5da1":"<h3><center>  <div style=\"background-color:lightgreen;border-radius:10px; padding: 10px;\">Feature Engineering \ud83d\udcd0\ud83d\udccf<\/div><\/center><\/h3>","64149c01":"> `RainTomorrow` has 3267 missing values. As `RainTomorrow` is to be predicted so we can't impute for missing values. Thus, we have to drop the the rows with missing values.","a026c564":"> Before doing any preprocessing, I will split the data into train and test set. The reason for that is, we don't see the test data, so all the preprocessing should be based on the train data. If we perform the preprocessing based on test data, it means that we did some cheating \ud83d\ude1b as we looked at the test data.\n\n> I am splitting the data into train and test with ratio of 80% - 20% (randomly chosen), and chossing `stratify` which will keep the proportion of target variable equal in both train and test data.  ","c8eadf94":"<h3><center>  <div style=\"background-color:lightgreen;border-radius:10px; padding: 10px;\">Cleaning Categorical Features \ud83d\udebf<\/div><\/center><\/h3>","10872ed0":"<h3><center>  <div style=\"background-color:lightgreen;border-radius:10px; padding: 10px;\">Cleaning Numeric Features \ud83d\udebf<\/div><\/center><\/h3>\n\n> The data cleaning of numeric features involves following things:\n> - Handling missing values\n> - Removing multicollinearity\n> - Removing outliers\n\n> Let's do them one by one","b00cec38":"> The datatype of `Date` is object so I will change it to date time for easy handling of dates","b518d596":"<h1><center>  <div style=\"background-color:lightgreen;border-radius:10px; padding: 10px;\"> Creating ANN<\/div><\/center><\/h1>","40cac4c5":"#### Around 84% accurate!","ca9bd2c8":"<h4><center>  <div style=\"background-color:skyblue;border-radius:10px; padding: 10px;\">Handling Missing Values<\/div><\/center><\/h4>\n\n> Missing data present various problems. First, the absence of data reduces statistical power, which refers to the probability that the test will reject the null hypothesis when it is false. Second, the lost data can cause bias in the estimation of parameters. Third, it can reduce the representativeness of the samples."}}