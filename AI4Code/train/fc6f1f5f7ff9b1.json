{"cell_type":{"6f02be86":"code","ab570f98":"code","92323288":"code","49fd481d":"code","5b919d36":"code","7e061f96":"code","638af775":"code","4ee039bf":"code","0523abaa":"code","ca403721":"code","5d3ade45":"code","b7acd4a0":"code","c3b6a5da":"code","51c2b8c7":"code","67affe9e":"code","f144af18":"code","41170579":"code","1472ae1e":"code","ea1b86ff":"code","1fa65150":"code","a4740fdf":"code","dd89eab5":"code","feffc9dc":"code","d63fb2d5":"code","c054664e":"code","34863a16":"code","1af4c06d":"code","ec4ed004":"markdown","0597bfd6":"markdown","7e5b9c05":"markdown","fd55919e":"markdown","eca001d6":"markdown","59c2e7d8":"markdown","cab5301b":"markdown","3568b7c8":"markdown","fc60eaac":"markdown","dc9bcdc8":"markdown","427a1480":"markdown","23141f83":"markdown","59b98251":"markdown","e9807cb0":"markdown","e852ffea":"markdown","857da42e":"markdown","a93fd7b7":"markdown","79c0eaf2":"markdown","f1f18080":"markdown","c662816d":"markdown","88c1bad0":"markdown","fa4c3deb":"markdown","f91fc66f":"markdown","8c4d93ad":"markdown","b9068e3f":"markdown","478d757b":"markdown"},"source":{"6f02be86":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","ab570f98":"weatherdata = pd.read_csv('..\/input\/weather-dataset-rattle-package\/weatherAUS.csv')\nweatherdata.head(10)","92323288":"print(weatherdata.columns)\nprint(\"Shape of the dataframe: \", weatherdata.shape)","49fd481d":"weatherdata.describe()","5b919d36":"weatherdata.dtypes","7e061f96":"import seaborn as sns\nimport matplotlib.pyplot as plt\n\nsns.set(rc = {'figure.figsize':(15,8)})\ncorrplot = sns.heatmap(weatherdata.corr(), cmap = 'YlGnBu', annot = True)\n\nplt.show()","638af775":"# Count how many null values in columns containing numeric data\n\nnumeric_cols = ['MinTemp', 'MaxTemp', 'Rainfall', 'Evaporation', 'Sunshine', 'WindGustSpeed', 'WindSpeed9am', \\\n                'WindSpeed3pm', 'Humidity9am', 'Humidity3pm', 'Pressure9am', 'Pressure3pm', 'Cloud9am', 'Cloud3pm', \\\n                'Temp9am', 'Temp3pm']\nna_sum_numeric_cols = []\nfor n in numeric_cols:\n    na_sum_numeric_cols.append(weatherdata[n].isnull().sum())\n\nfor i in range(len(numeric_cols)):\n    print(\"Sum of NAs in\", numeric_cols[i], \":\", na_sum_numeric_cols[i])","4ee039bf":"fig, ax = plt.subplots(len(numeric_cols), 1, figsize = (20, 65))\nfig.suptitle(\"Distribution of Outliers\")\n\nfor n in numeric_cols:\n    sns.boxplot(x = weatherdata[n], data = weatherdata, palette = \"crest\", ax = ax[numeric_cols.index(n)], width = 0.4)\n    ax[numeric_cols.index(n)].set_title(\"\")\n\nplt.show()","0523abaa":"for n in numeric_cols:\n    weatherdata[n].fillna(value = weatherdata[n].median(), inplace = True)\n\nweatherdata.head(10)","ca403721":"for n in numeric_cols:\n    print('Amount of null values in column', n, \":\", weatherdata.shape[0] - len(weatherdata[n]))","5d3ade45":"categorical_cols = [\"WindGustDir\", \"WindDir9am\", \"WindDir3pm\"]\nna_sum_categorical_cols = []\n\n# Count the number of NAs in categorical columns\n\nfor col in categorical_cols:\n    na_sum_categorical_cols.append(weatherdata[col].isnull().sum())\n\nfor i in range(len(categorical_cols)):\n    print(\"Sum of NAs in\", categorical_cols[i], \":\", na_sum_categorical_cols[i])\n    \n# Replacing NA with 'ffill' method and check whether all NAs have been replaced\n\nfor col in categorical_cols:\n    weatherdata[col].fillna(method = 'ffill', inplace = True)\n    print('Amount of null values in column', col, \"after preprocessing is:\", \\\n          weatherdata.shape[0] - len(weatherdata[col]))","b7acd4a0":"print(\"The number of NAs in RainToday is\", weatherdata['RainToday'].isna().sum(), \\\n      \"while the total number of rows in RainToday is\", len(weatherdata['RainToday']))\nprint(\"The ratio between NAs and non-NAs is\", weatherdata['RainToday'].isna().sum()\/len(weatherdata['RainToday']))","c3b6a5da":"weatherdata.dropna(subset = [\"RainToday\"], inplace = True)\nprint(weatherdata.shape)","51c2b8c7":"print(\"The number of NAs in RainTomorrow is\", weatherdata['RainTomorrow'].isna().sum(), \\\n      \"while the total number of rows in RainToday is\", len(weatherdata['RainTomorrow']))\nprint(\"The ratio between NAs and non-NAs is\", weatherdata['RainTomorrow'].isna().sum()\/len(weatherdata['RainTomorrow']))","67affe9e":"weatherdata.dropna(subset = ['RainTomorrow'], inplace = True)\nprint(weatherdata.shape)","f144af18":"print(\"The most minimum temperature recorded is\", weatherdata['MinTemp'].min(), \"degrees Celcius\")\nprint(\"The most maximum temperature recorded is\", weatherdata['MaxTemp'].max(), \"degrees Celcius\")","41170579":"print(\"The largest amount of rainfall recorded in a day is\", weatherdata['Rainfall'].max(), \"mm\")","1472ae1e":"from sklearn import preprocessing\n\nlabel_encoder = preprocessing.LabelEncoder()\n\nfor col in categorical_cols:\n    weatherdata[col] = label_encoder.fit_transform(weatherdata[col])\n    print(weatherdata[col].unique())","ea1b86ff":"for col in ['Date', 'Location', 'RainToday', 'RainTomorrow']:\n    weatherdata[col] = label_encoder.fit_transform(weatherdata[col])\n    print(weatherdata[col].unique())","1fa65150":"weatherdata.head(10)","a4740fdf":"X = weatherdata.drop(columns = ['RainTomorrow'], axis=1)\ny = weatherdata['RainTomorrow']","dd89eab5":"from sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42)","feffc9dc":"print ('Training set:', X_train.shape,  y_train.shape)\nprint ('Test set:', X_test.shape,  y_test.shape)","d63fb2d5":"from sklearn.tree import DecisionTreeClassifier\nfrom scipy.stats import randint\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.metrics import classification_report, confusion_matrix\n\nparam_dist = {\"max_depth\": [3, None], \"max_features\": randint(1, 9), \"min_samples_leaf\": randint(1, 9), \\\n              \"criterion\": [\"gini\", \"entropy\"]}\n\n# Instantiate the classifier\nweathertree = DecisionTreeClassifier()\n\n# Instantiate the hyperparameter tuning algorithm\nweathertree_cv = RandomizedSearchCV(weathertree, param_dist, cv = 5)\n\n# Train the training data\nweathertree_cv.fit(X_train, y_train)\n\n# Print the tuned parameters and score\nprint(\"Tuned Decision Tree Parameters: {}\".format(weathertree_cv.best_params_)) \nprint(\"Best score is {}\\n\".format(weathertree_cv.best_score_))\n\n# Inspecting the accuracy of the fine-tuned model\nprint(\"Confusion matrix: \\n {}\".format(confusion_matrix(y_true=y_test, y_pred=weathertree_cv.predict(X_test))))\nprint(\"\\nClassification report: \\n \\n {}\".format(classification_report(y_test, y_pred=weathertree_cv.predict(X_test))))","c054664e":"print(\"Accuracy using Decision Tree: \", metrics.accuracy_score(y_test, weathertree_cv.predict(X_test)))","34863a16":"from sklearn.naive_bayes import GaussianNB\n\nweathernb = GaussianNB()\nweathernb.fit(X_train, y_train)\n\n# Inspecting the metrics of the model\nprint(\"Confusion matrix: \\n {}\".format(confusion_matrix(y_true=y_test, y_pred=weathernb.predict(X_test))))\nprint(\"\\nClassification report: \\n \\n {}\".format(classification_report(y_test, y_pred=weathernb.predict(X_test))))\nprint(\"\\nAccuracy using GaussianNB: \", metrics.accuracy_score(y_test, weathernb.predict(X_test)))","1af4c06d":"from sklearn.linear_model import LogisticRegression\n\nc_space = np.logspace(-5, 8, 15)\nparam_grid = {'C': c_space}\n\nweatherLR = LogisticRegression(solver = 'liblinear')\nweatherLR_cv = RandomizedSearchCV(weatherLR, param_grid, cv = 5)\n\nweatherLR_cv.fit(X_train, y_train)\n\n# Print the tuned parameters and score\nprint(\"Tuned Logistic Regression Parameters: {}\".format(weatherLR_cv.best_params_)) \nprint(\"Best score is {}\".format(weatherLR_cv.best_score_))\n\n# Inspecting the metrics of the model\nprint(\"\\nConfusion matrix: \\n {}\".format(confusion_matrix(y_true=y_test, y_pred=weatherLR_cv.predict(X_test))))\nprint(\"\\nClassification report: \\n \\n {}\".format(classification_report(y_test, y_pred=weatherLR_cv.predict(X_test))))\nprint(\"\\nAccuracy using Logistic Regression: \", metrics.accuracy_score(y_test, weatherLR_cv.predict(X_test)))","ec4ed004":"We want to make sure that there is no missing data in the columns containing numeric-type data.\nWe need to inspect how many NAs in the columns containing numeric-type data.","0597bfd6":"Loading the descriptive statistic summary of the dataframe.","7e5b9c05":"We will try classifying using Naive Bayes.","fd55919e":"Inspecting the data types of each column in the dataframe.","eca001d6":"Let's start splitting the dataset into training and test sets.","59c2e7d8":"Making sure that there is no NA value left in each column.","cab5301b":"From the above boxplots, we can find there are many numeric columns containing outliers. Hence, we want to remove the NAs by replacing the NA values with the median of each column.","3568b7c8":"Checking the shapes of training and test sets.","fc60eaac":"From the above heatmap, we can see that the most positive correlation occurs between variables MaxTemp and Temp3pm, and the most negative correlation occurs between variables Sunshine and Cloud3pm.","dc9bcdc8":"We can see that, using Decision Tree, the fine-tuned model has 80.22% accuracy.","427a1480":"From the above, we may conclude that Logistic Regression with C = 163789.37 performs better than the other two algorithms, with the accuracy of 84%.","23141f83":"List the columns of the dataframe and load the shape of the dataframe.","59b98251":"Finding the most minimum and maximum temperature in degrees Celcius.","e9807cb0":"We do the same to RainTomorrow.","e852ffea":"We will check whether the numeric columns have outliers using Seaborn boxplot, to determine how we will remove the NA values.","857da42e":"Since NA values constitute only 2.24% of all rows, we may drop the NAs in RainToday.","a93fd7b7":"From the above, we can see that, using Gaussian Naive Bayes, the accuracy is slightly better than Decision Tree at 80.30%.","79c0eaf2":"We do the same to the columns containing categorical values, namely WindGustDir, WindDir9am, and WindDir3pm. However, we will use the 'ffill' method, since we can't replace NAs with either median, mode, or mean of categorical values.","f1f18080":"We do the same for Date, Location, RainToday and Rain Tomorrow.","c662816d":"Now let's get started with training and testing data.\n\nWe notice that we have columns containing categorical data. Hence, we need to apply label encoding to \"convert\" the categorical values into numerical values.","88c1bad0":"Visualizing the correlation of each column in heatmap.","fa4c3deb":"We will try classifying using Logistic Regression.","f91fc66f":"Loading and previewing the dataset.","8c4d93ad":"We check how many NA values exist in RainToday.","b9068e3f":"We will test some machine learning algorithms, namely Decision Tree and Naive Bayes.\n\nWe start with Decision Tree first.\n\nWe will also use hyperparameter tuning using RandomizedSearchCV to find and use the best parameters, that may result in a good-performance model.","478d757b":"Finding the largest amount of rainfall recorded in a day."}}