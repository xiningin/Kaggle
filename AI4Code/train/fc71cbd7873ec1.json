{"cell_type":{"d458b71b":"code","88aa194f":"code","d32f0379":"code","07c14349":"code","e0f508b3":"code","e676c43a":"code","62d01795":"code","a0393977":"code","685f02fb":"code","6cea0b67":"code","44d9c2bb":"code","41bc1b3e":"code","13ed95c3":"code","0641ea90":"code","6d412599":"code","9c5a1a93":"code","2877f011":"code","20ef4791":"code","b20c59de":"code","dbf06c02":"code","588857cc":"code","7161b684":"code","eb68fda6":"code","45cdff58":"code","c40f1a04":"code","2bae43d3":"code","6efbc239":"code","eeb10bbc":"code","f6728688":"code","d870f4a7":"code","5c6691de":"code","10e1ae02":"code","5b6871d9":"code","41830a74":"code","791e1d18":"code","cee98ad8":"code","79aaa476":"code","a6eeeb06":"code","41644adb":"code","075e4c84":"code","2269ad97":"code","81ea084f":"code","27720612":"code","ff14d43c":"code","f73233aa":"code","ff72d94f":"code","5b13aa41":"code","f2fd6f93":"code","0faf40cf":"code","0a8eb291":"code","351521a9":"code","925c0bbc":"code","3c50d5e9":"code","d09008f8":"code","fc2998ea":"code","25eae4d6":"markdown","e5fe16ed":"markdown","e48290f1":"markdown","2d9954d2":"markdown","d964d0e3":"markdown","ed4cfe3d":"markdown","4f7f0630":"markdown","f08c30a1":"markdown","d2047ee7":"markdown","9ad96341":"markdown","d3bc46f4":"markdown","a9352f15":"markdown","196a0c06":"markdown","83423e35":"markdown","6fce01f7":"markdown","1d5ff95f":"markdown","008ac54a":"markdown","1be22b49":"markdown","b26436c7":"markdown","077a43ab":"markdown","5956f14f":"markdown","dd06ea7b":"markdown","5411b11b":"markdown","cddad49e":"markdown","23827022":"markdown","0002d838":"markdown","068a9d6d":"markdown","33a17dc2":"markdown","86062895":"markdown","d28cdf1c":"markdown","f6ee1c24":"markdown","bac1fec9":"markdown","2e03730b":"markdown","d54664f6":"markdown","5545740e":"markdown","baf85246":"markdown","685a53ed":"markdown","1d089d40":"markdown","b7f43ea9":"markdown","f7bcb344":"markdown","efd4754a":"markdown","3b6c6298":"markdown"},"source":{"d458b71b":"import pandas as pd\nimport numpy as np \nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import classification_report,confusion_matrix,f1_score,accuracy_score\nfrom sklearn.preprocessing import StandardScaler\nimport warnings\nwarnings.filterwarnings('ignore')\nimport random","88aa194f":"df = pd.read_csv('..\/input\/factors-affecting-campus-placement\/Placement_Data_Full_Class.csv')","d32f0379":"df.head()","07c14349":"df.info()","e0f508b3":"#Drop sl_no & salary columns\ndf.drop(['sl_no','salary'],axis=1,inplace=True)","e676c43a":"sns.histplot(df['status'])","62d01795":"sns.pairplot(df,hue=\"status\")","a0393977":"#Encoding label (Placed\/NotPlaces)\ndum1= pd.get_dummies(df['status'],drop_first=True)\ndf = pd.concat([df,dum1],axis=1)","685f02fb":"plt.figure(figsize=(12,6))\nsns.heatmap(df.corr(),annot=True)","6cea0b67":"#Variance Inflation Factor(For grades)\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\nN = df[['ssc_p','hsc_p','degree_p','etest_p','mba_p']]\nVIF = pd.DataFrame()\nVIF['Features']=N.columns\nVIF['VIF value']= [variance_inflation_factor(N.values,i) for i in range(len(N.columns))]\nVIF.plot(kind='bar',x='Features',y='VIF value')","44d9c2bb":"sns.pairplot(df,hue=\"gender\",diag_kind='hist',diag_kws={'multiple':'dodge'})","41bc1b3e":"#Pie Plot\nplt.subplot(1,2,1)\ndf[df['gender']=='F'].groupby('status').count()['gender'].plot.pie(autopct=\"%.1f%%\")\nplt.title('Female')\nplt.subplot(1,2,2)\ndf[df['gender']=='M'].groupby('status').count()['gender'].plot.pie(autopct=\"%.1f%%\")\nplt.title('Male')","13ed95c3":"#Box Plots (label,all degrees,hue=gender)\nL =['ssc_p','hsc_p','degree_p','etest_p','mba_p']\ni=0\nplt.figure(figsize=(24,6))\nfor j in range(len(L)):\n    plt.subplot(1,len(L),j+1)\n    \n    sns.boxplot(x='status',y=L[j],hue='gender',data=df)\n    ","0641ea90":"plt.figure(figsize=(20,6))\n#Secondary education\nplt.subplot(1,2,1)\nsns.kdeplot('ssc_p',hue='ssc_b',data=df)\nplt.title('Secondary education percentage')\n\n#Higher secondary education\n\nplt.subplot(1,2,2)\nsns.kdeplot('hsc_p',hue='hsc_b',data=df)\nplt.title('Higher secondary education')\n","6d412599":"df['specialisation'].unique()","9c5a1a93":"sns.countplot(df['specialisation'],hue='gender',data=df)","2877f011":"#Pie Plot\nplt.figure(figsize=(14,6))\nplt.subplot(2,2,1)\ndf[(df['specialisation']=='Mkt&HR')&(df['gender']=='M')].groupby(['status']).count()['mba_p'].plot.pie(autopct=\"%.1f%%\")\nplt.title('Mkt&HR Males')\nplt.subplot(2,2,2)\ndf[(df['specialisation']=='Mkt&Fin')&(df['gender']=='M')].groupby(['status']).count()['mba_p'].plot.pie(autopct=\"%.1f%%\")\nplt.title('Mkt&Fin Males')\nplt.subplot(2,2,3)\ndf[(df['specialisation']=='Mkt&HR')&(df['gender']=='F')].groupby(['status']).count()['mba_p'].plot.pie(autopct=\"%.1f%%\")\nplt.title('Mkt&HR Females')\nplt.subplot(2,2,4)\ndf[(df['specialisation']=='Mkt&Fin')&(df['gender']=='F')].groupby(['status']).count()['mba_p'].plot.pie(autopct=\"%.1f%%\")\nplt.title('Mkt&Fin Females')","20ef4791":"#Count Plot\nsns.countplot(x='workex',hue='status',data=df)","b20c59de":"#3d Plot (Work experience,Etest,Specialization,status)\nimport plotly.express as px\npx.scatter_3d(df,x='etest_p',y='specialisation',z='workex',color='status')","dbf06c02":"df.head()","588857cc":"#Missing Data (None)\ndf.isnull().sum()","7161b684":"df.info()","eb68fda6":"#Getting categorical data for encoding\nCat_Col = []\nfor l in df.columns:\n    if type(df[l].loc[1])== str:\n        Cat_Col.append(l)\nCat_Col","45cdff58":"#Since we already encoded status let's drop it from our list\nCat_Col.pop()","c40f1a04":"for l in Cat_Col : \n    df= pd.concat([df,pd.get_dummies(df[l],drop_first=True)],axis=1)","2bae43d3":"#Replacing dummy variable Others for degree_t by Comm&Mgmt for interpretability manners\ndf.columns = ['gender', 'ssc_p', 'ssc_b', 'hsc_p', 'hsc_b', 'hsc_s', 'degree_p','degree_t', 'workex', 'etest_p', 'specialisation', 'mba_p', 'status','Placed', 'M', 'Others_ssc', 'Others_hsc', 'Commerce', 'Science', 'Comm&Mgmt','Sci&Tech', 'Yes', 'Mkt&HR']\ndf['Comm&Mgmt'] = (df['Comm&Mgmt']+df['Sci&Tech']).apply(lambda x:(x+1)%2)","6efbc239":"#Splitting features and labels and dropping categorical Data which is already encoded\nX = df.drop(['gender','ssc_b','hsc_b','hsc_s','degree_t','workex','specialisation','status','Placed'],axis=1)\ny= df['Placed']","eeb10bbc":"#Standard Scaler\nscaler = StandardScaler()\n","f6728688":"#Defining a k-fold function :\n\ndef kfold_logistic(X,y,nb_fold,regularization_type='none',regularization_coefficient =1,solver_max_iter=500,display=True,fold_size=0.3):\n    random.seed(1)\n    scaler = StandardScaler()\n    \n    if regularization_type =='none' or regularization_type =='l2':\n        solver = 'lbfgs'\n    if regularization_type =='l1':\n        solver= 'liblinear'\n    mean_test_accuracy = 0\n    mean_test_f1score = 0\n    mean_train_accuracy = 0\n    mean_train_f1score = 0\n\n    for j in range(nb_fold):\n\n        #Train\/Test Split\n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=fold_size)\n        \n        scaler.fit(X_train)\n        #Scale Data\n        X_train_scaled = scaler.transform(X_train)\n        X_test_scaled = scaler.transform(X_test)\n        #Defining the model\n        logreg = LogisticRegression(max_iter=500,penalty=regularization_type,solver=solver,C=1\/regularization_coefficient)\n        #fitting model\n        logreg.fit(X_train_scaled,y_train)\n        #Predicting X_test\n        predictions = logreg.predict(X_test_scaled)\n        mean_test_accuracy += accuracy_score(y_test,predictions)\n        mean_test_f1score += f1_score(y_test,predictions)\n\n        #Predicting X_train\n    \n        #predictions on training set : \n        predictions_train = logreg.predict(X_train_scaled)\n        mean_train_accuracy += accuracy_score(y_train,predictions_train)\n        mean_train_f1score += f1_score(y_train,predictions_train)\n    \n\n    mean_test_accuracy = mean_test_accuracy\/nb_fold\n    mean_test_f1score = mean_test_f1score\/nb_fold\n    mean_train_accuracy = mean_train_accuracy\/nb_fold\n    mean_train_f1score = mean_train_f1score\/nb_fold \n    #If display = True print results\n    if display==True :\n        print('Test predictions report')\n        print('Mean Test Accuracy =',mean_test_accuracy)\n        print('Mean Test f1 score =',mean_test_f1score)\n        print('\\n')\n    \n\n        print('Train predictions report')\n        print('Mean Train Accuracy =',mean_train_accuracy)\n        print('Mean Train f1 score =',mean_train_f1score)\n    #The function returns a list containing mean accuracy and mean f1 score for test and train sets    \n    return [mean_test_accuracy,mean_train_accuracy,mean_test_f1score,mean_train_f1score]","d870f4a7":"kfold_logistic(X,y,nb_fold=100,solver_max_iter=500,display=True)","5c6691de":"#Splitting features and labels and dropping categorical Data that is already encoded\nX = df.drop(['gender','ssc_b','hsc_b','hsc_s','degree_t','workex','specialisation','status','Placed'],axis=1)\ny= df['Placed']","10e1ae02":"X.columns","5b6871d9":"#Interaction terms\nL=list(X.columns)\nfor i in range(len(L)-1):\n    for j in range(i+1,len(L)):\n        X[str(L[i]+'*'+L[j])] = X[L[i]]*X[L[j]]\n\n","41830a74":"X.info()","791e1d18":"#Scaler \nscaler = StandardScaler()","cee98ad8":"kfold_logistic(X,y,nb_fold=100)","79aaa476":"#Defining a function for picking regularization factor using K-Fold Cross Validation\n#factor_min\/max = minimum\/maximum regulariazation factor\n#factor_nb = number of regularization factors to try\n#search_type = is the way of selecting regularization factors (grid_search or random logarithmic)\ndef regularization(X,y,factor_min,factor_max,factor_nb,nb_fold,search_type = 'grid_search',regularization_type='l1',plot_display=True) :\n    random.seed(1)\n    #Initializing some host lists for results \n    TEST_acc =[]\n    TEST_F1=[]\n    TRAIN_ACC=[]\n    TRAIN_F1=[]\n    LAMBDA = []\n    \n    #Creating Regularization factor space\n    if search_type == 'random_logarithmic_search' :\n        for nb in range(factor_nb):\n            #Random logarithmic selection of regularization coefficient\n            r = (np.log10(factor_max)-np.log10(factor_min))*np.random.rand() + np.log10(factor_min)\n            lam=10**r\n            LAMBDA.append(lam)\n    if search_type == 'grid_search':\n        \n        if factor_min==0: \n            #Avoid dividing by 0 (1\/C in logistic function)\n            factor_min += 10**-8 \n        #Creating a grid    \n        LAMBDA = list(np.linspace(factor_min,factor_max,factor_nb))\n    \n    #Compute accuracy and f1 score for all space\n    for lam in LAMBDA:\n        \n        #K-Fold computation for regularization coefficient equal to lam\n        results = kfold_logistic(X,y,nb_fold=nb_fold,regularization_type=regularization_type,regularization_coefficient=lam,display=False)\n    \n    \n    \n        #Appending results  \n        TEST_acc.append(results[0])\n        TEST_F1.append(results[2])\n        TRAIN_ACC.append(results[1])\n        TRAIN_F1.append(results[3])\n    ind = TEST_acc.index(max(TEST_acc))    \n    if plot_display == True :\n        #Plot Test and Train accuray    \n        fig = plt.figure(figsize=(15,6))\n        ax = fig.add_axes([0,0,1,1])\n        ax.plot(LAMBDA,TEST_acc,'ro')\n        ax.plot(LAMBDA,TRAIN_ACC,'bo')\n        ax.plot(LAMBDA[ind],TEST_acc[ind],marker='*',ms=20,markerfacecolor='yellow',markeredgewidth=3, markeredgecolor='green')\n        ax.plot(LAMBDA[ind],TRAIN_ACC[ind],marker='*',ms=20,markerfacecolor='yellow',markeredgewidth=3, markeredgecolor='green')\n\n        print('Maximum test accuracy reached at lambda =',LAMBDA[ind])\n        print('Maximum test accuracy value =',TEST_acc[ind])\n        print('Training set accuracy value =',TRAIN_ACC[ind])\n    return [LAMBDA[ind],TEST_acc[ind],TRAIN_ACC[ind]]\n","a6eeeb06":"regularization(X,y,factor_min=0.001,factor_max=2,factor_nb=100,nb_fold=100,search_type='grid_search',regularization_type='l1')","41644adb":"regularization(X,y,factor_min=0.001,factor_max=2,factor_nb=100,nb_fold=100,search_type='grid_search',regularization_type='l2')","075e4c84":"#Add feature by feature using Test accuracy metric\n\nX_candidates = X.copy()\nX_model = pd.DataFrame()\nscore_table=[]\nfeature_table=[]\ncurrent_accuracy = 0\n\n\n\n#Redo the process for 15 times\n\nfor n in range(15):\n    \n    print('round number :',n)\n    score_table=[]\n    feature_table=[]\n    #For all features not in model\n    for feat in X_candidates.columns :  \n        #Add feature to the model\n        X_model[feat] = X_candidates[feat]\n        #Estimate test accuracy using K-Fold\n        results = kfold_logistic(X_model,y,nb_fold=100,regularization_type='l2',display=False)\n    \n        #Append result\n        mean_test_accuracy = results[0]\n        score_table.append(mean_test_accuracy)\n        feature_table.append(feat)\n        #Drop feature to prepare the entry of next one\n        X_model.drop(feat,axis=1,inplace=True)\n    #Select index of feature that yields maximum accuracy\n    index = score_table.index(max(score_table))\n    print('Winner is : ',feature_table[index])\n    print('Final test...')\n    # If best feature improves current test accuracy add it definitely to the model if not discard it and retry, one might think of implementing backward here in order to look for other alternatives\n    if score_table[index]>current_accuracy:\n        X_model[feature_table[index]] = X_candidates[feature_table[index]]\n        X_candidates.drop(feature_table[index],axis=1,inplace=True)\n        current_accuracy = score_table[index]\n        print(f'Fighter {feature_table[index]} is IN')\n    else : \n        print(f'Fighter {feature_table[index]} disqualified')","2269ad97":"#Our model \nX_model.columns","81ea084f":"#Let's apply regularization to the model we obtained\nregularization(X_model,y,factor_min=0.001,factor_max=5,factor_nb=100,nb_fold=100,search_type='grid_search',regularization_type='l2')","27720612":"\n\nX_candidates = X.copy()\nX_model = pd.DataFrame()\nlr = LogisticRegression(penalty='none',max_iter=1000)\nrandom.seed(1)\nmodels = []\nmodels_train_accuracy=[]\n\nrandom.seed(1)\n#Redo the process 15 times \n\nfor n in range(15):\n    print('round number :',n)\n    score_table=[]\n    feature_table=[]\n    #For all features not in our model\n    for feat in X_candidates.columns :       \n        X_model[feat] = X_candidates[feat]\n        #Get estimate of training accuracy using K-Fold (It's like we are getting the learning capability of our model)\n        results = kfold_logistic(X_model,y,nb_fold=100,display=False)    \n        score_table.append(results[1])\n        feature_table.append(feat)\n        X_model.drop(feat,axis=1,inplace=True)\n    #Select index of feature that yields maximum training accuracy\n    index = score_table.index(max(score_table))\n    #Add feature to our model\n    X_model[feature_table[index]] = X_candidates[feature_table[index]]\n    X_candidates.drop(feature_table[index],axis=1,inplace=True) \n    models.append(feature_table[index])\n    models_train_accuracy.append(score_table[index])\n    print('Winner is : ',feature_table[index])\n    print('Current model is : ')\n    print(models)\n    print('Current train accuracy is : ',score_table[index])\n\n    \n    \n","ff14d43c":"CV = []\nLAMBDA = []\nrandom.seed(1)\n#For all subsets compute test accuracy and lambda value for regularization using K-Folds.\nfor p in range(len(models)):\n    X_model2 = X_model[models[0:p+1]]\n    print('model1 processing...')\n    regu = regularization(X_model2,y,factor_min=0,factor_max=5,factor_nb=100,nb_fold=100,\n                          search_type='grid_search',regularization_type='l2',plot_display=False)\n    LAMBDA.append(regu[0])\n    CV.append(regu[1])\n    print(f'for model {p+1} cross validation test is equal to {regu[1]} for a value of lambda equal to {regu[0]}')","f73233aa":"#Features selected \nX_forward1 = X[['ssc_p*degree_p', 'Comm&Mgmt*Yes', 'Science*Mkt&HR', 'etest_p*M',\n       'etest_p*mba_p', 'ssc_p*Yes', 'Commerce*Yes', 'Science*Comm&Mgmt', 'Comm&Mgmt*Sci&Tech']]\n#Scaler\nscaler = StandardScaler()\n","ff72d94f":"random.seed(1)\n#Logistic regression\nlr1 = LogisticRegression(max_iter=500,C=1\/1.566343434343434)\n#Train Test split\nX_train, X_test, y_train, y_test = train_test_split(X_forward1, y, test_size=0.3)","5b13aa41":"#Scale Data\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)","f2fd6f93":"#Fit the model\nlr1.fit(X_train,y_train)\n#Predict y_test\npredictions = lr1.predict(X_test)\n#Print Results\nprint(confusion_matrix(y_test,predictions))\nprint('\\n')\nprint(classification_report(y_test,predictions))","0faf40cf":"#Coefficients\nmodel1_coefficients = pd.DataFrame({'Feature name' : X_forward1.columns , 'Coefficient value': lr1.coef_.reshape(9,)})\nmodel1_coefficients","0a8eb291":"#Features selected \nX_forward2 = X[['ssc_p*degree_p', 'Comm&Mgmt*Yes', 'mba_p', 'Science*Mkt&HR', 'degree_p*M', 'degree_p*etest_p', 'Science*Sci&Tech']]\n#Scale\nscaler = StandardScaler()","351521a9":"random.seed(1)\n#Logistic regression\nlr2 = LogisticRegression(max_iter=500,C=1\/1.364363636363636)\n#Train Test split\nX_train, X_test, y_train, y_test = train_test_split(X_forward2, y, test_size=0.3)","925c0bbc":"#Scaler Data\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)","3c50d5e9":"#Fit the model\nlr2.fit(X_train,y_train)\n#Predict y_test\npredictions = lr2.predict(X_test)\n#Print Results\nprint(confusion_matrix(y_test,predictions))\nprint('\\n')\nprint(classification_report(y_test,predictions))","d09008f8":"#Coefficients \n\nmodel2_coefficients = pd.DataFrame({'Feature name' : X_forward2.columns , 'Coefficient value': lr2.coef_.reshape(7,)})\nmodel2_coefficients","fc2998ea":"#Results might differ due to randomnes, I tried to make use of random.seed... Thanks!","25eae4d6":"### Correlation Heatmap","e5fe16ed":"# Logistic Regression","e48290f1":"## Overall Visualization (quantitative features)","2d9954d2":" ==> It appears that ssc_p, hsc_p & degree_p have significant impact on response (Status).\n \n==> Obviously some features such as ssc_p & hsc_p are highly correlated to others, let's quickly check the value of these correlations.","d964d0e3":"# Import Libraries and Data","ed4cfe3d":"### Forward Selection 1","4f7f0630":"### L2 Regularization","f08c30a1":"### Label (Placed or Not)","d2047ee7":"## Categorical data","9ad96341":"### Pair Plot (With hue=gender)","d3bc46f4":"==> Since test accuracy is quite close to train accuracy, let's first try to improve train performance","a9352f15":"### Gender","196a0c06":"==> According to the previous plots, placed students have higher grades than others, also females have better grades than males except for etest_p (Employability test percentage)","83423e35":"### Board of education","6fce01f7":" ==> Generally, we can say that features values for female and male follow the same trend, except for mba degrees and ssc_p wherein females tend to do better(According to the shape of distribution and mean value). Let's zoom in that gender effect...","1d5ff95f":"### Specialization","008ac54a":"==> It's clear that work experience plays a major role in increasing chances of employment","1be22b49":"### Regularization general function","b26436c7":"In this part we are going to get a model by selecting features that maximizes training accuracy. Our last model will be a 15 feature model. After, for all p in [1,15] we create a subset model containing first p features, then, we compare these models after regularization using test accuracy.\n","077a43ab":"# Final models : ","5956f14f":"### Work experience","dd06ea7b":"==> l1 & l2 regularization helped in improving test performance...\n\n==> Still, the effect of regularization is not that satisfying compared to the initial model (Without feature engineering and regularization). This might be due to the huge feature\/observations ratio that we have. In next section we try some forward model selection techniques","5411b11b":"Forward Selection 2 : \n    The best model according to test estimated accuracy is the one defined with : ['ssc_p*degree_p', 'Comm&Mgmt*Yes', 'mba_p', 'Science*Mkt&HR', 'degree_p*M', 'degree_p*etest_p', 'Science*Sci&Tech']\n    \n    1\/ interaction between ssc_p and degree_p\n    2\/ interaction between Work experience (Dummy variable) and Comm&Mgmt(Field of degree education)\n    3\/ mba_p\n    4\/ interaction between Science(Specialization in Higher Secondary Education) & Mkt&HR(Post graduation MBA specialization)\n    5\/ interaction between degree_p and gender\n    6\/ interaction between degree_p and etest_p\n    7\/ interaction between Science(Specialization in Higher Secondary Education) and Sci&Tech(Field of degree education)\n    \n   L2 Regularization factor value =  1.364363636363636\n   \n    Let's apply that model for a random train\/test split","cddad49e":"### Pair Plot (Whit hue = Status)","23827022":" ==> Males have a higher 'Placed' rate than females","0002d838":"Let's use k-folds in order to get accurate estimates of Accuracy and f1-score. Because we have a limited amount of data we can try some large numbers of folds.","068a9d6d":" ==> Unbalanced Data","33a17dc2":"### L1 Regularization","86062895":"### Forward Selection 2","d28cdf1c":"==> This 3d plot gives insights about the positive effect of work experience and employability test percentage on placement status in both specialization MRKT&HR and MRKT&FIN. ","f6ee1c24":"==> We overfitted Data. For next, we will try some regularizations","bac1fec9":"Forward selection 1 yield the model defined by : ['ssc_p*degree_p', 'Comm&Mgmt*Yes', 'Science*Mkt&HR', 'etest_p*M',\n       'etest_p*mba_p', 'ssc_p*Yes', 'Commerce*Yes', 'Science*Comm&Mgmt', 'Comm&Mgmt*Sci&Tech'] It containes only interaction terms :\n       \n       1\/ interaction between ssc_p and degree_p\n       2\/ interaction between Work experience (Dummy variable) and Comm&Mgmt(Field of degree education)\n       3\/ Interaction between Science(Specialization in Higher Secondary Education) & Mkt&HR(Post graduation MBA specialization)\n       4\/ interaction between etest_p and gender\n       5\/ interaction between etest_p and mba_p\n       6\/ interaction between ssc_p and work experience\n       7\/ interaction between Commerce (Specialization in Higher Secondary Education) and work experience\n       8\/ interaction between Science(Specialization in Higher Secondary Education) and Comm&Mgmt(Field of degree education)\n       9\/ interaction between Comm&Mgmt and Sci&Tech (Fields of degree education)\n \n L2 Regularization factor value = 1.566343434343434\n \n Let's apply that model for a random train\/test split","2e03730b":" ==> Distribution of grades varies between central board and others.","d54664f6":"### Variance Inflation Factor","5545740e":"==> Now we have 105 features","baf85246":"==> etest_p * gender has a positive coefficient, in contrast etest_p * mba_p coefficient is negative. This might be due to :\n    \n    1\/ females have the best mba grades but there placement ratio is inferior to male's rate \n    2\/ Males have better etest grades and better placement ratio (M refers to Male's dummy variable)\n    \n    See boxplots of grades per gender and placement status","685a53ed":" ==> Mkt&Fin is a better choice than Mkt&HR for both Females and Males in term of employability rate","1d089d40":"## Forward selection model 1","b7f43ea9":" ==> Better accuracy and f1 score...","f7bcb344":"# Data Visualization","efd4754a":"### Adding interaction features","3b6c6298":"# Preprocessing Data"}}