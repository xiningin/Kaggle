{"cell_type":{"bb5ecf85":"code","53c7eaea":"code","d4879e1a":"code","ce1b2c99":"code","51414afc":"code","4ddff8b6":"code","3c58ff10":"code","f2a2d185":"code","7703202a":"code","0b89dadd":"code","84c4db7a":"code","315009d6":"code","3d65e13c":"code","17901243":"code","ca830f86":"code","29148aff":"code","12b63366":"markdown","dab36987":"markdown","462c7986":"markdown","a132a7e3":"markdown"},"source":{"bb5ecf85":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import accuracy_score\nfrom xgboost import XGBClassifier\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\n\nimport keras\nfrom keras.models import Sequential \nfrom keras.layers import Conv2D, AveragePooling2D\nfrom keras.layers import Dense, Flatten\nfrom keras.losses import categorical_crossentropy","53c7eaea":"df_train = pd.read_csv(\"\/kaggle\/input\/digit-recognizer\/train.csv\")\ndf_test = pd.read_csv(\"\/kaggle\/input\/digit-recognizer\/test.csv\")","d4879e1a":"df_train","ce1b2c99":"df_train.label.unique()","51414afc":"from matplotlib.pyplot import imshow\n\nwidth=5\nheight=5\nrows = 2\ncols = 3\naxes=[]\n\nfig=plt.figure()\nfig.set_size_inches(8,10)\nfor i in range(rows*cols):\n    sample=np.reshape(df_train[df_train.columns[1:]].iloc[i].values\/255,(28,28))\n    axes.append(fig.add_subplot(rows,cols,i+1))\n    plt.title(\"Labeled class : {}\".format(df_train[\"label\"].iloc[i]))\n    plt.imshow(sample, 'gray')\nfig.tight_layout()\nplt.show()","4ddff8b6":"plt.figure(figsize=(8,6))\nax = sns.countplot(x='label',data=df_train)\n\nplt.title(\"Label Distribution\")\ntotal= len(df_train.label)\nfor p in ax.patches:\n    percentage = f'{100 * p.get_height() \/ total:.1f}%\\n'\n    x = p.get_x() + p.get_width() \/ 2\n    y = p.get_height()\n    ax.annotate(percentage, (x, y), ha='center', va='center')","3c58ff10":"# separate target values from df_train\ntargets = df_train.label\nfeatures = df_train.drop(\"label\",axis=1)","f2a2d185":"features ","7703202a":"features = features\/255\ndf_test = df_test\/255","0b89dadd":"X_train,X_val, y_train,y_val = train_test_split(features,targets,test_size=0.2, random_state=1)","84c4db7a":"from tensorflow.keras.utils import to_categorical\n\nnum_classes = len(np.unique(y_train))\nprint(y_train[0], end= ' => ')\ny_train = to_categorical(y_train,10)\ny_val =  to_categorical(y_val,10)\nprint(y_train[0])","315009d6":"# 784 = 28*28\nimg_rows,img_cols = 28,28\n\nX_train = X_train.values.reshape(len(X_train),img_rows,img_cols,1)\nX_val = X_val.values.reshape(len(X_val),img_rows,img_cols,1)\n\ndf_test = df_test.values.reshape(len(df_test),img_rows,img_cols,1)\n\ninput_shape = (img_rows,img_cols,1)","3d65e13c":"lenet = Sequential()\n\nlenet.add(Conv2D(6,kernel_size=(5,5),activation='relu',input_shape=input_shape,padding='same',name ='C1'))\nlenet.add(AveragePooling2D(pool_size= (2,2),strides=(1,1),padding='valid'))\nlenet.add(Conv2D(120, kernel_size=(5,5),activation='relu',name='C2'))\nlenet.add(Flatten())\nlenet.add(Dense(84,activation='relu',name='C3'))\nlenet.add(Dense(10,activation='softmax',name='Output'))\n\nlenet.compile(loss=categorical_crossentropy,optimizer='SGD',metrics = ['accuracy'])\n\nlenet.summary()\n","17901243":"batch_size=64\nepochs = 50\nhistory = lenet.fit(X_train,y_train, batch_size = batch_size,epochs = epochs, validation_data = (X_val,y_val))","ca830f86":"predictions = lenet.predict(df_test)\npredictions = [np.argmax(a) for a in predictions]\n\n","29148aff":"output = pd.read_csv(\"..\/input\/digit-recognizer\/sample_submission.csv\")\noutput['Label'] = predictions\noutput.to_csv('submission.csv',index=False)\n","12b63366":"# Explanatory Data Analysis","dab36987":"# Preprocessing the Data","462c7986":"# Making a Model and Predictions","a132a7e3":"# Background of this notebook \n\nI used XGB classifier on this notebook: [Digits-Recognizer -Simple XGB Classifier](http:\/\/https:\/\/www.kaggle.com\/satoshiss\/digits-recognizer-simple-xgb-classifier).  Next I am going to use convolutional neural networks with help of \"Deep Learning for Dummies\". I am reading the book at the moment. Hope this project helps me to understand the topic better."}}