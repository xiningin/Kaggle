{"cell_type":{"31029008":"code","3c84732d":"code","06ab7af3":"code","d0d27cfa":"code","2e9e6b06":"code","5b116a78":"code","a897ea47":"code","aa993cf7":"code","fa019f80":"code","a72b2e72":"code","74835eb5":"code","3d978f3c":"code","46f8290e":"code","c1119ce9":"code","2c161d01":"code","e68ae495":"markdown","92428c29":"markdown","218c78cc":"markdown","b7c1c228":"markdown","753b0c6f":"markdown","4c0e2ba2":"markdown","738b8d10":"markdown","bd04a48a":"markdown"},"source":{"31029008":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","3c84732d":"import re # library to clean data\nimport csv \nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nimport pandas as pd \nimport nltk# Natural Language Tool Kit\nnltk.download('stopwords')\nfrom nltk.corpus import stopwords # to remove stopword\nfrom tensorflow.keras.layers import Dense, Dropout,  GlobalAveragePooling1D, Embedding\nfrom tensorflow import keras \nimport matplotlib.pyplot as plt","06ab7af3":"vocab_size = 2000 \nembedding_dim = 64 \nmax_length = 300 \ntrunc_type = 'post' \npadding_type = 'post'\n\ntrianing_portion = 0.8\nOOV_tok = '<OoV>'","d0d27cfa":"train = pd.read_csv('..\/input\/bbc-fulltext-and-category\/bbc-text.csv')\ntrain.head()\n","2e9e6b06":"xtrain =train['text']\ny_train = train['category']\nxtrain[0], y_train[0], len(xtrain)","5b116a78":"train_size = int(len(xtrain) * trianing_portion)\n\ntrain_sentances = xtrain[:train_size]\ntrain_labels = y_train[:train_size]\n\nval_x = xtrain[train_size:]\nval_labels = y_train[train_size:]\nlen(train_sentances) , len(val_x)","a897ea47":"tokenizer = Tokenizer(num_words=vocab_size, oov_token= OOV_tok)\ntokenizer.fit_on_texts(train_sentances)\nword_index = tokenizer.word_index\n\ntrian_sequences = tokenizer.texts_to_sequences(train_sentances)\ntrain_padded = pad_sequences(trian_sequences, padding= padding_type, maxlen= max_length)\n\nprint(len(trian_sequences[0])), print(len(train_padded[0]))\nprint(len(trian_sequences[1])), print(len(train_padded[1]))\nprint(len(trian_sequences[11])), print(len(train_padded[11]))\n\n","aa993cf7":"val_sequences = tokenizer.texts_to_sequences(val_x)\nval_padded = pad_sequences(val_sequences, padding= padding_type, maxlen= max_length)\n\nprint(len(val_sequences[0])), print(len(val_padded[0]))\nprint(len(val_sequences[1])), print(len(val_padded[1]))\nprint(len(val_sequences[11])), print(len(val_padded[11]))\nval_padded.shape","fa019f80":"label_tokenizer = Tokenizer()\nlabel_tokenizer.fit_on_texts(y_train)\n\ntrain_labels_seq=np.array(label_tokenizer.texts_to_sequences(train_labels))\nval_labels_seq = np.array(label_tokenizer.texts_to_sequences(val_labels))\n\nprint(train_labels_seq[0]), print(train_labels_seq[1]), print(train_labels_seq[2])\nprint(val_labels_seq[0]), print(val_labels_seq[1]), print(val_labels_seq[2])\n\ntrain_labels_seq.shape, val_labels_seq.shape","a72b2e72":"model = keras.Sequential([         # embedding layer\n      Embedding(vocab_size, embedding_dim, input_length= max_length),\n      GlobalAveragePooling1D(),\n    \n     # Classification head\n     Dense(32, activation='relu'),Dropout(.5),\n     Dense(24, activation='relu'),Dropout(.5),\n     Dense(6, activation='softmax')    \n ]) \n","74835eb5":"model.compile(loss='sparse_categorical_crossentropy',\n              optimizer='adam',\n              metrics=['accuracy']) \n\nmodel.summary()","3d978f3c":"keras.utils.plot_model(model, show_shapes=True, dpi=48)\n\n","46f8290e":"history = model.fit(x= train_padded,y= train_labels_seq, \n                     validation_data=(val_padded,val_labels_seq), \n                     epochs=30) ","c1119ce9":"def plot_graphs(history, string ): \n    plt.plot(history.history[string])\n    plt.plot(history.history['val_' +string])\n    plt.xlabel ('Epochs')\n    plt.ylabel(string)\n    plt.legend([string, 'val_' + string])\n    plt.show()","2c161d01":"plot_graphs(history, 'accuracy' )\nplot_graphs(history,'loss' )","e68ae495":"# importing libraries ","92428c29":"# split the  data into train test","218c78cc":"# creating the model ","b7c1c228":"# ploting\nThe relation between the train, and validation loss and accuracy","753b0c6f":"# training the model \n","4c0e2ba2":"# create tokenizer \nto transform the text data into numbers ","738b8d10":"# loading the data ","bd04a48a":"# ploting the model "}}