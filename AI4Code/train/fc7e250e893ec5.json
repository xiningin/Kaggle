{"cell_type":{"dd9008cf":"code","f12d87c0":"code","ad74521d":"code","dca2162c":"code","e7bd7f59":"code","9cc01a0c":"code","1aac5e05":"code","382da065":"code","20341538":"code","c0c2de00":"code","b1b30fc8":"code","f7bb849c":"code","3da84e4f":"code","0e14f1a7":"code","a17c99b9":"code","206bb7f3":"code","584ccf69":"code","b743208a":"code","f0fe9097":"code","a1a969d9":"code","19c03f7d":"code","ecb7a7a1":"code","d31b51d5":"code","6d5ea35b":"code","53373b27":"code","8bf78057":"code","61a97308":"code","0a3049c8":"code","e89049dd":"markdown","81351f79":"markdown","8e110d82":"markdown","1dd3d659":"markdown","1bbba849":"markdown","c33cde2f":"markdown","154ca792":"markdown"},"source":{"dd9008cf":"import pandas as pd\ntrain = pd.read_csv('\/kaggle\/input\/titanic\/train.csv').set_index('PassengerId')\ntest = pd.read_csv('\/kaggle\/input\/titanic\/test.csv').set_index('PassengerId')\nsubmission = pd.read_csv('\/kaggle\/input\/titanic\/gender_submission.csv')\n\ntitanic = pd.concat([train,test], axis=0, sort=False) #Concat on the row axis\ndisplay(titanic.describe(include='all'))\ntitanic.isnull().sum()","f12d87c0":"titanic['Cabin_Missing'] = titanic.Cabin.isnull()*1.0\ntitanic.drop(['Cabin'], axis=1, inplace=True)","ad74521d":"titanic['Title'] = titanic.Name.str.split(',').str[1].str.split('.').str[0].str.strip()\ntitanic['LastName'] = titanic.Name.str.split(',').str[0]\ntitanic.drop(['Name'], axis=1 ,inplace=True)\ntitanic.LastName, _ = pd.factorize(titanic['LastName'])","dca2162c":"alone_child = titanic.query('Age < 12 & Parch == 0').index\nfor i in alone_child:\n    titanic.at[i,'Parch'] += 1 #add nanny","e7bd7f59":"def get_ticket_type(ticket):\n  ticket_list = ticket.split(' ')\n  if len(ticket_list) == 2:\n    return ticket_list[0]\n  else:\n    ticket_list = ticket.split('.')\n    return 'Normal'\n\ndef get_ticket_number(ticket):\n  ticket_list = ticket.split(' ')\n  if len(ticket_list) == 2:\n    return ticket_list[1]\n  else:\n    return ticket\n\ntitanic['Ticket_Type'] = titanic['Ticket'].apply(lambda x: get_ticket_type(x))\ntitanic['Ticket_Number'] = titanic['Ticket'].apply(lambda x: get_ticket_number(x))\nmissing_index = titanic[pd.to_numeric(titanic['Ticket_Number'], errors='coerce').isnull()].index\nmissing_index = missing_index.to_list()\ndf_ticket_type_correction = ['STON\/O2.','STON\/O2.','LINE','STON\/O2.','LINE','LINE','STON\/O2.','STON\/O2.','STON\/O2.','STON\/O2.','SC\/AH Basle','STON\/O2.','STON\/O2.','LINE','STON\/O2.',\n                             'STON\/O2.', 'STON\/O2.', 'STON\/O2.', 'STON\/O2.', 'A.\/2.']\ndf_ticket_number_correction = [3101294,2101280,0,3101275,0,0,3101293,3101289,3101269,3101274,541,3101286,3101273,0,3101292,3101285,3101288,3101291,3101268,39186]\nfor i in range(20):\n  titanic.at[missing_index[i], 'Ticket_Type'] = df_ticket_type_correction[i]\n  titanic.at[missing_index[i], 'Ticket_Number'] = df_ticket_number_correction[i]\ntitanic['Ticket_Number'] = pd.to_numeric(titanic['Ticket_Number'])\ntitanic['Ticket_Type'] = titanic['Ticket_Type'].apply(lambda x: x.replace('.','').upper())\ntitanic.drop(['Ticket'], axis=1, inplace=True)\ntitanic.drop(['Ticket_Type'], axis=1, inplace=True)\ntitanic.Ticket_Number, _ = pd.factorize(titanic['Ticket_Number'])","9cc01a0c":"titanic.isnull().sum()","1aac5e05":"print(titanic.Embarked.value_counts())\ntitanic.Embarked = titanic.Embarked.fillna('S')","382da065":"age_mean_dict = titanic.groupby(['Pclass','Title','Embarked'])['Age'].mean().to_dict()\nage_mean_dict[(3,'Ms','Q')] = titanic.groupby(['Title'])['Age'].mean().to_dict()['Ms']\nprint(age_mean_dict)\nmissing_age_index = titanic[titanic.Age.isnull()].index\nfor i in missing_age_index:\n    titanic.at[i,'Age'] = age_mean_dict[(titanic.at[i,'Pclass'], titanic.at[i,'Title'], titanic.at[i,'Embarked'])]","20341538":"missing_fare_index = titanic[titanic.Fare.isnull()].index\ntitanic.at[missing_fare_index, 'Fare'] = titanic.groupby(['Pclass','Embarked','Title'])['Age'].mean()[(3,'S','Mr')]","c0c2de00":"titanic['WomanOrChild'] = ((titanic.Title == 'Master') | (titanic.Sex == 'female'))","b1b30fc8":"titanic.Pclass = titanic.Pclass.apply(lambda x: ['Dummy','Rich','Middle','Poor'][x])\ntitanic['Family_Size'] = titanic['SibSp']+titanic['Parch']","f7bb849c":"titanic.loc[titanic.Survived.isnull(),'Survived'] = titanic.loc[titanic.Survived.isnull(),'WomanOrChild'] * 1.0\nfamily = titanic.groupby(['LastName']).Survived\nfriends = titanic.groupby(['Ticket_Number']).Survived","3da84e4f":"titanic['WomanOrBoyCount'] = family.transform(lambda s: s[titanic.WomanOrChild].fillna(0).count())\ntitanic['WomanOrBoyCount'] = titanic.mask(titanic.WomanOrChild, titanic.WomanOrBoyCount - 1, axis=0)","0e14f1a7":"titanic['WomanOrBoyCount2'] = friends.transform(lambda s: s[titanic.WomanOrChild].fillna(0).count())\ntitanic['WomanOrBoyCount2'] = titanic.mask(titanic.WomanOrChild, titanic.WomanOrBoyCount2 - 1, axis=0)\ntitanic.WomanOrChild = titanic.WomanOrChild * 1.0","a17c99b9":"titanic['Alone'] = (titanic.Family_Size == 0) * 1.0","206bb7f3":"titanic.head()","584ccf69":"titanic_onehot = pd.get_dummies(titanic, columns=['Pclass','Sex','Title','Embarked'])","b743208a":"train_cols = [col.replace('(','_').replace(']','_').replace('<','_') for col in titanic_onehot.columns]\ntitanic_onehot.columns = train_cols","f0fe9097":"train_onehot = titanic_onehot.loc[train.index]\ntest_onehot = titanic_onehot.loc[test.index]\nassert train_onehot['Survived'].isnull().sum() == 0\ntest_onehot.drop(['Survived'], axis=1, inplace=True)","a1a969d9":"!pip install rfpimp","19c03f7d":"from rfpimp import *\ntrain_X = train_onehot.drop(['Survived'], axis=1)\ntrain_y = train_onehot['Survived']\nrf = RandomForestRegressor(n_estimators=100, n_jobs=-1)\nrf.fit(train_X, train_y)\nimp = importances(rf, train_X, train_y) # permutation\nprint(imp)","ecb7a7a1":"from sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import cohen_kappa_score\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.tree.export import export_text\n\ndtree_cols=['WomanOrBoyCount2','WomanOrChild','Pclass_Poor','Pclass_Rich']\ndtree = DecisionTreeClassifier(max_depth=3).fit(train_X[dtree_cols], train_y)\nprint(cohen_kappa_score(train_y, dtree.predict(train_X[dtree_cols])))\nprint(confusion_matrix(train_y, dtree.predict(train_X[dtree_cols])))\ntree_rules = export_text(dtree, feature_names=dtree_cols, show_weights=True)\nprint(tree_rules)","d31b51d5":"dtree2_cols=['Ticket_Number','WomanOrBoyCount2','WomanOrChild']\ndtree2 = DecisionTreeClassifier(max_depth=10).fit(train_X[dtree2_cols], train_y)\nprint(cohen_kappa_score(train_y, dtree2.predict(train_X[dtree2_cols])))\nprint(confusion_matrix(train_y, dtree2.predict(train_X[dtree2_cols])))\ntree_rules = export_text(dtree2, feature_names=dtree2_cols, show_weights=True)\nprint(tree_rules)","6d5ea35b":"dtree3_cols=['LastName','WomanOrBoyCount2','WomanOrChild']\ndtree3 = DecisionTreeClassifier(max_depth=10).fit(train_X[dtree3_cols], train_y)\nprint(cohen_kappa_score(train_y, dtree3.predict(train_X[dtree3_cols])))\nprint(confusion_matrix(train_y, dtree3.predict(train_X[dtree3_cols])))\ntree_rules = export_text(dtree3, feature_names=dtree3_cols, show_weights=True)\nprint(tree_rules)","53373b27":"submission['Survived_dtree'] = dtree.predict(test_onehot[dtree_cols]).astype(int)\nsubmission['Survived_dtree2'] = dtree2.predict(test_onehot[dtree2_cols]).astype(int)\nsubmission['Survived_dtree3'] = dtree3.predict(test_onehot[dtree3_cols]).astype(int)\nsubmission['Survived_gender'] = submission['Survived']\nsubmission['Average_Survived'] = (submission['Survived_dtree'] + submission['Survived_dtree2'] + submission['Survived_dtree3'] + submission['Survived_gender'])\/4.0\nsubmission['Survived'] = (submission['Average_Survived'] >= 0.75) * 1","8bf78057":"submission = submission[['PassengerId','Survived']]","61a97308":"submission.head()","0a3049c8":"submission.to_csv('submission.csv', index=False)","e89049dd":"Let's extract the ticket number and the ticket type.","81351f79":"Children with 0 Parch is said to travel with nanny, so increase the Parch by 1 in such cases","8e110d82":"#### Missing Values Imputation!","1dd3d659":"The name column can be extracted to make two or more columns, let's make that.","1bbba849":"This notebook aims to use the ticket number in some way to do the predictions.\n\n\n### 1. Data Cleaning and Preprocessing","c33cde2f":"Model building time!\nWe will be building three models and taking the majority voting at the end.","154ca792":"On the first look, PassengerId has very high variance compared to the rest of the columns, so it is not included in our analysis.\nSecondly, cabin has 1000+ missing values so it will also be discarded from out analysis and instead we will use a derived variable cabin_missing."}}