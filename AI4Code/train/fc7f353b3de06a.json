{"cell_type":{"1c9563dc":"code","19938ec0":"code","20ed008e":"code","d543a303":"code","d1b3b4eb":"code","dd65b3de":"code","7f788ba9":"code","5d3ebfd6":"code","a3d96a03":"code","8c677bdd":"code","85777834":"code","e3796a6b":"code","0957515e":"code","330c33e5":"code","ac2263fa":"code","fe5bfbe8":"code","a70b384e":"code","31d5d070":"code","28c6dc00":"code","f97eb118":"code","23438ff8":"code","ae9edcb0":"code","7e0e11f9":"code","777acf05":"code","55353550":"code","b3cbec80":"code","a780cf57":"code","5e3603c8":"code","b998e2d8":"code","4084417e":"code","6ab782a2":"code","d95bc97a":"code","f5dc8177":"code","813fac23":"code","4c8af226":"code","39fe4908":"code","21c50308":"code","855c3944":"code","88c4939d":"code","696a3503":"code","bff7d0cd":"code","77eeee95":"code","c42d20c5":"code","f2cfac8f":"code","e2aaf6f6":"code","8dc8da3e":"code","1156dceb":"code","84fce20e":"code","68e1346e":"code","d8b9bf97":"code","31372b5e":"code","6453ae60":"code","27730276":"code","c7d7a8af":"code","391883bc":"code","cc01e026":"code","07a332b7":"code","1824b16a":"code","e9e2b774":"code","89684595":"code","498d5004":"code","cde48e8a":"code","f7d111b7":"code","77d994bb":"code","a7808b2c":"code","d1a8c017":"code","1e977cc5":"code","2361f3ae":"code","5f421aac":"code","27e40cae":"code","6fefe78c":"code","ef8a85ea":"markdown","1f46d079":"markdown","e3bc3200":"markdown","b32d919b":"markdown","9bd6429b":"markdown","1246044d":"markdown","358220b0":"markdown","6acc035c":"markdown","48a3bde9":"markdown","130ea735":"markdown","32b1c964":"markdown","b163f4be":"markdown","c215ae16":"markdown","82fb993b":"markdown","d701ea18":"markdown","17e587e7":"markdown","e71fd540":"markdown","ea0b4563":"markdown","2193c7d0":"markdown","1e66ec21":"markdown","1d8ac583":"markdown","cc32ef22":"markdown","be0771df":"markdown","a12bc72c":"markdown","2a4bb1b9":"markdown","06fcbac5":"markdown","30740f53":"markdown","ed4f5725":"markdown","f9dc5031":"markdown","74141a89":"markdown","b519e067":"markdown","6dfbbf6d":"markdown","9a065181":"markdown","9d94e68f":"markdown","2a4e82ff":"markdown","42fa980c":"markdown","a2fd0449":"markdown","d6272e68":"markdown","60b38caa":"markdown","e9dcf735":"markdown","fa2ada55":"markdown","6895e5d9":"markdown"},"source":{"1c9563dc":"import os\nimport numpy   as np \nimport pandas  as pd \nimport seaborn as sns\nfrom matplotlib import pyplot as plt \nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report, confusion_matrix\nfrom sklearn.svm import SVC, LinearSVC\nfrom sklearn.linear_model import LogisticRegressionCV\nfrom sklearn.ensemble import RandomForestClassifier\n\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","19938ec0":"for dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\ndata = pd.read_csv('..\/input\/titanic\/train.csv')\ndf_test = pd.read_csv('..\/input\/titanic\/test.csv')\nss = pd.read_csv('..\/input\/titanic\/gender_submission.csv')","20ed008e":"data.sample(5)","d543a303":"ss.sample(5)","d1b3b4eb":"data.head()","dd65b3de":"data.tail()","7f788ba9":"data.shape","5d3ebfd6":"data.columns","a3d96a03":"data.info()","8c677bdd":"data.describe()","85777834":"data.isnull().sum()","e3796a6b":"data.drop('Cabin',axis=1,inplace=True)","0957515e":"data['Embarked'].fillna(value = data['Embarked'].mode,inplace=True)\ndata['Age'].fillna(value = data['Age'].mean(),inplace = True)","330c33e5":"data.isnull().sum()","ac2263fa":"data.drop('Embarked',axis=1,inplace=True)\ndata.drop('Name',axis=1,inplace=True)","fe5bfbe8":"data.columns","a70b384e":"temp = pd.read_csv('..\/input\/titanic\/train.csv') # all extra modifications will be done on it.","31d5d070":"temp.head()","28c6dc00":"df_test.head()","f97eb118":"new_data = temp.drop('Ticket',axis=1)\ndf_test.drop('Ticket',axis=1,inplace=True)\nnew_data.head()    ","23438ff8":"mode_value = new_data['Age'].mode()\nmode_t_value = df_test['Age'].mode()\nmode_value,mode_t_value","ae9edcb0":"new_data['Age'].fillna(value = 24.0,inplace = True)\ndf_test['Age'].fillna(value = 24.0,inplace = True)","7e0e11f9":"try:\n    new_data.drop('Name',axis=1,inplace=True)\n    df_test.drop('Name',axis=1,inplace=True)\nexcept:\n    print(\"Name Already Dropped !\")\ntry:\n    new_data.drop('Cabin',axis=1,inplace=True)\n    df_test.drop('Cabin',axis=1,inplace=True)\nexcept:\n    print(\"Cabin Already Dropped !\")\n","777acf05":"new_data['Sex'] = pd.get_dummies(new_data['Sex'])\ndf_test['Sex'] = pd.get_dummies(df_test['Sex'])\nnew_data.head()","55353550":"new_data.boxplot()","b3cbec80":"fig,axes = plt.subplots(1,2,figsize=(14,5))\nsns.boxplot(new_data['Fare'],ax=axes[0]).set_title('Fare Box Plot Before',fontsize=18)\nsns.kdeplot(new_data['Fare'],ax=axes[1]).set_title('Fare Distribution Plot Before',fontsize=18)\nplt.show()","a780cf57":"Q1=data['Fare'].quantile(0.10)\nQ3=data['Fare'].quantile(0.80)\nIQR= Q3-Q1\nnew_data.loc[(new_data['Fare'] > (Q3 + 1.5*IQR)),'Fare'] = Q3\nnew_data.loc[(new_data['Fare'] < (Q1 - 1.5*IQR)),'Fare'] = Q1\n\nt_Q1=df_test['Fare'].quantile(0.10)\nt_Q3=df_test['Fare'].quantile(0.80)\nt_IQR= t_Q3-t_Q1\n\ndf_test.loc[(df_test['Fare'] > (t_Q3 + 1.5*t_IQR)),'Fare'] = t_Q3\ndf_test.loc[(df_test['Fare'] < (t_Q1 - 1.5*t_IQR)),'Fare'] = t_Q1\n\nprint(Q1,Q3)\nprint(new_data.shape)","5e3603c8":"fig,axes = plt.subplots(1,2,figsize=(14,5))\nsns.boxplot(new_data['Fare'],ax=axes[0]).set_title('Fare Box Plot After',fontsize=18)\nsns.kdeplot(new_data['Fare'],ax=axes[1]).set_title('Fare Distribution Plot After',fontsize=18)\nplt.show()","b998e2d8":"fig,axes = plt.subplots(1,2,figsize=(14,5))\nplt.suptitle('Age Plotting for Outlier Detection')\nsns.boxplot(new_data['Age'],ax=axes[0])\nsns.kdeplot(new_data['Age'],ax=axes[1])\nplt.show()","4084417e":"new_data['Age'].min(),new_data['Age'].max()","6ab782a2":"for i in new_data['Age'].values: \n    if i<1:\n        print(i)","d95bc97a":"# AGE_RANGE\nnew_data['Age_Range'] = pd.cut(new_data['Age'], bins=[0,9,18,60,100], labels=[\"Child\",\"Teenager\",\"Adult\", \"Aged\"])\ndf_test['Age_Range'] = pd.cut(df_test['Age'], bins=[0,9,18,60,100], labels=[\"Child\",\"Teenager\",\"Adult\", \"Aged\"])\nnew_data.head()","f5dc8177":"new_data = new_data.join(pd.get_dummies(new_data['Age_Range']))\ndf_test = df_test.join(pd.get_dummies(df_test['Age_Range']))\nage_range = new_data['Age_Range'].values\nnew_data.drop('Age_Range',axis=1,inplace=True)\ndf_test.drop('Age_Range',axis=1,inplace=True)\nnew_data.head()","813fac23":"embarked = pd.get_dummies(new_data['Embarked'])\nnew_data = new_data.join(embarked)\nnew_data.drop('Embarked',axis=1,inplace=True)\n\nT_embarked = pd.get_dummies(df_test['Embarked'])\ndf_test = df_test.join(T_embarked)\ndf_test.drop('Embarked',axis=1,inplace=True)\n\nnew_data.head()","4c8af226":"new_data.drop('Parch',axis=1,inplace=True)\nnew_data.drop('SibSp',axis=1,inplace=True)\n\ndf_test.drop('Parch',axis=1,inplace=True)\ndf_test.drop('SibSp',axis=1,inplace=True)","39fe4908":"Pclass = pd.get_dummies(new_data['Pclass'])\nPclass.columns=['UpperClass', 'MiddleClass','LowerClass']\nnew_data = new_data.join(Pclass)\nnew_data.drop('Pclass',axis=1,inplace=True)\n\nPclass = pd.get_dummies(df_test['Pclass'])\nPclass.columns=['UpperClass', 'MiddleClass','LowerClass']\ndf_test = df_test.join(Pclass)\ndf_test.drop('Pclass',axis=1,inplace=True)\n\nnew_data.head()","21c50308":"df_test.head()","855c3944":"temp.head() # copy of train preprocessed, will be used for visualization","88c4939d":"new_data.corr()","696a3503":"fig,axes = plt.subplots(1,2,figsize=(20,7))\nplt.suptitle('Orignal v\/s Featured Data', fontsize=18)\nsns.heatmap(temp.corr(),ax=axes[0]).set_title('Orignal Data')\nsns.heatmap(new_data.corr(),ax=axes[1]).set_title('Featured Data')\nplt.show()","bff7d0cd":"sns.pairplot(data)\nplt.show()","77eeee95":"fig,axes = plt.subplots(1,3,figsize=(15,5))\nsns.distplot(new_data['Fare'],ax=axes[0])\nsns.distplot(data['Age'],ax=axes[1])\nsns.distplot(data['Pclass'],ax=axes[2])\nplt.show()","c42d20c5":"fig,axes = plt.subplots(2,2,figsize=(16,10))\nsns.scatterplot(new_data['Age'],new_data['Fare'],ax=axes[0,0])\nsns.scatterplot(new_data['Age'],new_data['Survived'],ax=axes[0,1])\nsns.barplot(new_data['Sex'],new_data['Age'],ax=axes[1,0])\nsns.barplot(new_data['Sex'],new_data['Survived'],ax=axes[1,1])\nplt.show()","f2cfac8f":"new_data.head()","e2aaf6f6":"fig, ax = plt.subplots(1,2,figsize=(16,5))\nsns.countplot(age_range,ax=ax[0]).set_title('Count of Age Group',fontsize=16)\nsns.barplot(age_range,new_data['Survived'],ax=ax[1]).set_title('Survived v\/s Age Group',fontsize=16)","8dc8da3e":"fig, ax = plt.subplots(1,2,figsize=(16,5))\nsns.countplot(temp['Embarked'],ax=ax[0]).set_title('Count of Embarked',fontsize=16)\nsns.barplot(temp['Embarked'],new_data['Survived'],ax=ax[1]).set_title('Survived v\/s Embarked',fontsize=16)","1156dceb":"fig, ax = plt.subplots(1,2,figsize=(16,5))\nsns.countplot(temp['Pclass'],ax=ax[0]).set_title('Count of Passenger Class',fontsize=16)\nsns.barplot(temp['Pclass'],new_data['Survived'],ax=ax[1]).set_title('Survived v\/s Passenger Class',fontsize=16)\nplt.show()","84fce20e":"new_data.isnull().sum()","68e1346e":"# we have new_data for training purpose and df_test for prediction So lets create testing data.\ntrain = new_data\ntest = df_test\ntrain.head()","d8b9bf97":"test.head()","31372b5e":"X = train.drop('Survived',axis=1)\ny = train['Survived']\nX = X.iloc[:,1:]\nX.head(3)","6453ae60":"y.head(5)","27730276":"X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.10,random_state=0)\nX_train.shape,y_train.shape,X_test.shape,y_test.shape","c7d7a8af":"from sklearn.metrics import accuracy_score, confusion_matrix, classification_report","391883bc":"linear_svc = LinearSVC()\nlinear_svc.fit(X_train, y_train)\n\nsvc_pred = linear_svc.predict(X_test)\n\nlinear_svc.score(X_train, y_train)\n\nacc_linear_svc = round(linear_svc.score(X_test, y_test) * 100, 2)\nprint(round(acc_linear_svc,2,), \"%\")\nprint(confusion_matrix(y_test,svc_pred))","cc01e026":"print(classification_report(y_test,svc_pred))","07a332b7":"random_forest1 = RandomForestClassifier(criterion='entropy', n_estimators=110,min_samples_split=40,min_samples_leaf=3, max_depth=5, max_features='auto',oob_score=True,random_state=1)\nrandom_forest1.fit(X_train, y_train)\nrf_pred1 = random_forest1.predict(X_test)\n\nrandom_forest1.score(X_train, y_train)\n\nacc_rf1 = round(random_forest1.score(X_test, y_test) * 100, 2)\nprint(round(acc_rf1,2,), \"%\")\nprint(confusion_matrix(y_test,rf_pred1))","1824b16a":"print(random_forest1.oob_score_)\nprint(classification_report(y_test,rf_pred1))","e9e2b774":"LogReg = LogisticRegressionCV(cv=5)\n\nLogReg.fit(X_train,y_train)\nLR_pred = LogReg.predict(X_test)\n\nLogReg.score(X_train, y_train)\n\nacc_LR = round(LogReg.score(X_test, y_test) * 100, 2)\nprint(round(acc_LR,2,), \"%\")\nprint(confusion_matrix(y_test,LR_pred))","89684595":"print(classification_report(y_test,LR_pred))","498d5004":"from sklearn.ensemble import GradientBoostingClassifier\n\ngb = GradientBoostingClassifier()\ngb.fit(X_train, y_train)\n\n# accuracy score, confusion matrix and classification report of gradient boosting classifier\n\ngb_acc = accuracy_score(y_test, gb.predict(X_test))\n\nprint(f\"Training Accuracy of Gradient Boosting Classifier is {accuracy_score(y_train, gb.predict(X_train))}\")\nprint(f\"Test Accuracy of Gradient Boosting Classifier is {gb_acc} \\n\")\n\nprint(f\"Confusion Matrix :- \\n{confusion_matrix(y_test, gb.predict(X_test))}\\n\")","cde48e8a":"from xgboost import XGBClassifier\n\nxgb = XGBClassifier(booster = 'gbtree', learning_rate = 0.1, max_depth = 5, n_estimators = 180)\nxgb.fit(X_train, y_train)\n\n# accuracy score, confusion matrix and classification report of xgboost\n\nxgb_acc = accuracy_score(y_test, xgb.predict(X_test))\n\nprint(f\"Training Accuracy of XgBoost is {accuracy_score(y_train, xgb.predict(X_train))}\")\nprint(f\"Test Accuracy of XgBoost is {xgb_acc} \\n\")","f7d111b7":"sgb = GradientBoostingClassifier(subsample = 0.90, max_features = 0.70)\nsgb.fit(X_train, y_train)\n\n# accuracy score, confusion matrix and classification report of stochastic gradient boosting classifier\n\nsgb_acc = accuracy_score(y_test, sgb.predict(X_test))\n\nprint(f\"Training Accuracy of Stochastic Gradient Boosting is {accuracy_score(y_train, sgb.predict(X_train))}\")\nprint(f\"Test Accuracy of Stochastic Gradient Boosting is {sgb_acc} \\n\")\n\nprint(f\"Confusion Matrix :- \\n{confusion_matrix(y_test, sgb.predict(X_test))}\\n\")","77d994bb":"from sklearn.ensemble import VotingClassifier\n\nclassifiers = [('SVM',linear_svc), ('Random Forest', random_forest1), ('Logistic', LogReg),('Gradient Boost',gb),('XGBoost',xgb),(' sgb classifier',sgb)]\nvc = VotingClassifier(estimators = classifiers)\nvc.fit(X_train, y_train)","a7808b2c":"vc_acc = accuracy_score(y_test, vc.predict(X_test))\n\nprint(f\"Training Accuracy of Voting Classifier is {accuracy_score(y_train, vc.predict(X_train))}\")\nprint(f\"Test Accuracy of Voting Classifier is {vc_acc} \\n\")\n\nprint(f\"{confusion_matrix(y_test, vc.predict(X_test))}\\n\")\nprint(classification_report(y_test, vc.predict(X_test)))","d1a8c017":"test['Fare'].fillna(value=test['Fare'].mean(),inplace=True)","1e977cc5":"test.isnull().sum()","2361f3ae":"test = test.iloc[:,1:]\ntest","5f421aac":"vc_final_pred = vc.predict(test)","27e40cae":"test_csv =pd.read_csv('..\/input\/titanic\/test.csv')","6fefe78c":"final = {'PassengerId': test_csv['PassengerId'] ,'Survived':vc_final_pred}\n\nsub = pd.DataFrame (final, columns = ['PassengerId','Survived'])\nsub.to_csv('\/kaggle\/working\/vc_submission.csv', index=False)\nsub","ef8a85ea":"<h2 id=\"3.2\"> 3.2 Numerical Encoding <\/h2>","1f46d079":"<h2 id=\"5.1\"> 5.1 TRAIN-TEST SPLIT <\/h2>","e3bc3200":"<h2 id=\"3.4\">3.4 Binning<\/h2>\n<h4>The main motivation of binning is to make the model more robust and prevent overfitting, however, it has a cost to the performance.<\/h4>","b32d919b":"### AGE","9bd6429b":"#### Last 5 values of dataset","1246044d":"<h3> Let's have a look over the first 5 and last 5 rows of dataset <h3>\n<h5> Top 5 values of dataset <\/h5>","358220b0":"<h4> Performing the same task over the <b>Pclass<\/b> column<\/h4>","6acc035c":"<h2 id=\"vc\"> Voting Classifier <\/h2>","48a3bde9":"<h2 id=\"lr\"> Logistic Regression <\/h2>","130ea735":"<h4> After creating bins for <b>Age<\/b> column we can now move to <b>Embarked<\/b> column<\/h4>","32b1c964":"<h2 id=\"3.3\"> 3.3 Handling Outliers <\/h2>","b163f4be":"<h2 id=\"5.2\"> 5.2 Model Implementation <\/h1>","c215ae16":"<h4> Here it's clear that our data contains outlier and is not normalized. So lets remove these outliers by <b>Interquartile Range<\/b><\/h4> \n\n\n<h3><b>What is Interquartile Range IQR?<\/b><\/h3>\n\n<h4>IQR is used to measure variability by dividing a data set into quartiles. The data is sorted in ascending order and split into 4 equal parts. <\/h4>\n<h4>Q1, Q2, Q3 called first, second and third quartiles are the values which separate the 4 equal parts:<\/h4><br\/>\n\n<h4>Q1 represents the 25th percentile of the data.<\/h4>\n<h4>Q2 represents the 50th percentile of the data.<\/h4>\n<h4>Q3 represents the 75th percentile of the data.<\/h4> <br\/>\n<h4>And if a dataset has 2n \/ 2n+1 data points, then<\/h4>\n<h4>Q1 = median of the dataset.<\/h4>\n<h4>Q2 = median of n smallest data points.<\/h4>\n<h4>Q3 = median of n highest data points.<\/h4>\n\n<h4>IQR is the range between the first and the third quartiles namely Q1 and Q3: IQR = Q3 \u2013 Q1. The data points which fall below Q1 \u2013 1.5 IQR or above Q3 + 1.5 IQR are outliers.<\/h4>","82fb993b":"* <h3>  Printing out some random rows from dataset using \"sample()\" function<\/h3>","d701ea18":"<h2 id=\"rf\"> Random-Forest <\/h2>","17e587e7":"<h4>Dropping the unwanted columns out of the final dataset<\/h4>","e71fd540":"<h2> Final Prediction <\/h2> ","ea0b4563":"<h3> From the above boxplot we can see that some values of <b>Fare<\/b> column is far away from the normal range of other data values. This means we have to check for the outliers in this column.<\/h3>\n<h4> For this I will use 2 plotting methods, first one is <b>boxplot<\/b> of that particular column and second one is <b>kdeplot()<\/b> (used for distribution plot).<\/h4>","2193c7d0":"<h2 id=\"xgb\"> XGBoost Classifier <\/h2>","1e66ec21":"<h4> We have Cabin column with 687 out of 891 values as null so we will drop it <\/h4>","1d8ac583":"<h2 id=\"svm\"> Support-Vector Machine <\/h2>","cc32ef22":"<h4>Now we have all the outliers removed from the column <b>Fare<\/b><\/h4>","be0771df":"<h4> So here we are all sorted with null values. <\/h4>","a12bc72c":"<h1  id=\"4\"> 4. DATA VISUALIZATION <\/h1>","2a4bb1b9":"<h1> <u>Dataset Overview<\/u> \ud83d\udea2 <\/h1>\n<h4> The titanic. csv file contains data for 887 of the real Titanic passengers. Each row represents one person. \n The columns describe different attributes about the person including whether they survived (S), their age (A), their passenger-class (C), their sex (G) and the fare they paid (X).\n<\/h4>\n\n<img src= \"https:\/\/cdn-images-1.medium.com\/max\/1000\/0*1d-Zv640av5xAaL5\" \/>\n\n<h2> <u>The Challenge <\/u><\/h2>\n<h4> The sinking of the Titanic is one of the most infamous shipwrecks in history.\n<\/h4>\n<h4>On April 15, 1912, during her maiden voyage, the widely considered \u201cunsinkable\u201d RMS Titanic sank after colliding with an iceberg. Unfortunately, there weren\u2019t enough lifeboats for everyone onboard, resulting in the death of 1502 out of 2224 passengers and crew.\n<\/h4>\n<h4>While there was some element of luck involved in surviving, it seems some groups of people were more likely to survive than others.\n<\/h4><br\/>\n<h4>In this challenge, you have to build a predictive model that answers the question: \u201cwhat sorts of people were more likely to survive?\u201d using passenger data (ie name, age, gender, socio-economic class, etc).<\/h4>\n<h2><u>My Submission<\/u><\/h2>    \n <img src=\"https:\/\/cdn-images-1.medium.com\/max\/1500\/1*J76nvGf_96wKun-HtqmQ0Q.png\" \/>   \n \n<h2> <u>Steps I used in this kernel <\/u>:<\/h2> <br\/>\n<ul>\n <li><h4><a href=\"1\">Import Libraries And read Files<\/a><\/h4><\/li>\n <li><h4><a href=\"2\">Basic Insights of Data<\/a><\/h4><\/li>\n <li><h4><a href=\"3\">Feature Engineering<\/a><\/h4>\n    <ul>\n         <li><a href=\"3.1\">Imputation<\/a><\/li>\n         <li><a href=\"3.2\">Numerical Encoding<\/a><\/li>\n         <li><a href=\"3.3\">Handling Outliers<\/a><\/li>\n         <li><a href=\"3.4\">Binning<\/a><\/li>        \n    <\/ul>\n <\/li>\n <li><h4><a href=\"4\">Data Visuslization<\/a><\/h4><\/li>\n <li><h4><a href=\"5\">Modeling<\/a><\/h4>\n    <ul>\n        <li><a href=\"5.1\">TRAIN-TEST SPLIT<\/a><\/li>\n        <li><a href=\"5.2\">Model Implementation<\/a><\/li>\n    <\/ul>\n    <\/li>\n <li><h4><a href=\"6\">Saving Csv<\/a><\/h4><\/li>\n<\/ul>","06fcbac5":"<h4> Now as we can see that the columns \"Sex\" is categorical but plays an important role in predicting the final prediction. So we will convert this column to numerical column using pandas function <b>get_dummies()<\/b>.<\/h4>\n<h4> <b>pandas.get_dummies()<\/b> is used for data manipulation. It converts categorical data into dummy or indicator variables.<\/h4>","30740f53":"<h4>Statistical information about data => <\/h4>","ed4f5725":"<h4> Creating a new dataframe temp with limited columns in it, then preparing the test set. <\/h4>","f9dc5031":"* <h4>From above heatmap graphs it's clear that now we have a lot more features to process and consider for our prediction model.<\/h4>","74141a89":"<h4> So we have a age range of (0.42year - 80years)<\/h4>","b519e067":"<h4> Now we left with Embarked with 2 values as null and Age with 177 values as null so we will replace them with suitable mode and mean <\/h4>","6dfbbf6d":"<h4>Theoritical information about data => <\/h4>","9a065181":"<h1 id=\"5\"> 5. MODELING <\/h1>","9d94e68f":"<h2> Removing null\/missing values <\/h2>\n<h4> We are removing these null values because they adversely affect the performance and accuracy of any machine learning algorithm. So, removing null values from the dataset before modeling is one of the important steps in data wrangling. <\/h4>","2a4e82ff":" <h2> If you enjoyed reading this notebook. A &#128077; will motivate me to do more of this type of work.<\/h2>\n <h2> Also if there is any feedback or suggestion please let me know in the comment section.<\/h2>\n <h1>Thank You for Reading &#x1F607;<\/h1>","42fa980c":"<h1 id=\"2\"> 2. Basic insights of data <\/h1>\n<h3> In this part we will go through the shape of dataset, what columns it has and the theoritical and statistical summary of the dataset <\/h3> ","a2fd0449":"<h1 id=\"1\"> 1. Importing libraries and read files <\/h1>","d6272e68":"<h4> Now we will just replace all null values from column \"Age\" by replacing them with the mode of the whole column.(most common age) <\/h4>\n<h4> We will be using pandas function fillna() for this purpose. <\/h4>","60b38caa":"<h2 id=\"gbr\"> Gradient Boost Classifier <\/h2>","e9dcf735":"<h2 id=\"sgb\"> Stochastic gradient boosting classifier <\/h2>","fa2ada55":"<h1 id=\"6\"> 6. Save Prediction As CSV <\/h1>","6895e5d9":"<h1 id=\"3\"> 3. Feature Engineering <\/h1>\n<h2 id=\"3.1\"> 3.1 Imputation<\/h2>"}}