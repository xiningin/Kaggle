{"cell_type":{"b063720a":"code","345cd4ca":"code","394bb573":"code","ec6602f3":"code","bf65bc9f":"code","cd17cd2a":"code","6b8807af":"code","53ff69bc":"code","08397c35":"code","69b14d5c":"code","e329a19e":"code","cfd6cd86":"code","9c88aee6":"code","3dc53383":"code","184fb0e6":"markdown","bb12839a":"markdown","4a16d67f":"markdown","cc3ba11e":"markdown","06b22570":"markdown","48ec2709":"markdown","47e087d0":"markdown","aa8bb879":"markdown","f820ae87":"markdown","123904b5":"markdown","b5e3aec7":"markdown","fbf6abbf":"markdown","588af856":"markdown","cf8f2146":"markdown","a85d0a9e":"markdown","dce81bca":"markdown","6c68557c":"markdown","c118468a":"markdown","c17af116":"markdown","6a584503":"markdown"},"source":{"b063720a":"%matplotlib inline\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport scipy.stats as stats","345cd4ca":"plt.figure(figsize=(9,9))\n\ndef sigmoid(t):                          # Define the sigmoid function\n    return (1\/(1 + np.e**(-t)))    \n\nplot_range = np.arange(-6, 6, 0.1)       \n\ny_values = sigmoid(plot_range)\n\n# Plot curve\nplt.plot(plot_range,   # X-axis range\n         y_values,          # Predicted values\n         color=\"red\");","394bb573":"\ntitanic_train = pd.read_csv(\"..\/input\/train.csv\")    # Read the data\n\nchar_cabin = titanic_train[\"Cabin\"].astype(str)     # Convert cabin to str\n\nnew_Cabin = np.array([cabin[0] for cabin in char_cabin]) # Take first letter\n\ntitanic_train[\"Cabin\"] = pd.Categorical(new_Cabin)  # Save the new cabin var\n\n# Impute median Age for NA Age values\nnew_age_var = np.where(titanic_train[\"Age\"].isnull(), # Logical check\n                       28,                       # Value if check is true\n                       titanic_train[\"Age\"])     # Value if check is false\n\ntitanic_train[\"Age\"] = new_age_var \n\nnew_fare_var = np.where(titanic_train[\"Fare\"].isnull(), # Logical check\n                       50,                         # Value if check is true\n                       titanic_train[\"Fare\"])     # Value if check is false\n\ntitanic_train[\"Fare\"] = new_fare_var ","ec6602f3":"from sklearn import linear_model\nfrom sklearn import preprocessing","bf65bc9f":"# Initialize label encoder\nlabel_encoder = preprocessing.LabelEncoder()\n\n# Convert Sex variable to numeric\nencoded_sex = label_encoder.fit_transform(titanic_train[\"Sex\"])\n\n# Initialize logistic regression model\nlog_model = linear_model.LogisticRegression(solver = 'lbfgs')\n\n# Train the model\nlog_model.fit(X = pd.DataFrame(encoded_sex), \n              y = titanic_train[\"Survived\"])\n\n# Check trained model intercept\nprint(log_model.intercept_)\n\n# Check trained model coefficients\nprint(log_model.coef_)","cd17cd2a":"# Make predictions\npreds = log_model.predict_proba(X= pd.DataFrame(encoded_sex))\npreds = pd.DataFrame(preds)\npreds.columns = [\"Death_prob\", \"Survival_prob\"]\n\n# Generate table of predictions vs Sex\npd.crosstab(titanic_train[\"Sex\"], preds.loc[:, \"Survival_prob\"])","6b8807af":"# Convert more variables to numeric\nencoded_class = label_encoder.fit_transform(titanic_train[\"Pclass\"])\nencoded_cabin = label_encoder.fit_transform(titanic_train[\"Cabin\"])\n\ntrain_features = pd.DataFrame([encoded_class,\n                              encoded_cabin,\n                              encoded_sex,\n                              titanic_train[\"Age\"]]).T\n\n# Initialize logistic regression model\nlog_model = linear_model.LogisticRegression(solver = 'lbfgs')\n\n# Train the model\nlog_model.fit(X = train_features ,\n              y = titanic_train[\"Survived\"])\n\n# Check trained model intercept\nprint(log_model.intercept_)\n\n# Check trained model coefficients\nprint(log_model.coef_)","53ff69bc":"# Make predictions\npreds = log_model.predict(X= train_features)\n\n# Generate table of predictions vs actual\npd.crosstab(preds,titanic_train[\"Survived\"])","08397c35":"log_model.score(X = train_features ,\n                y = titanic_train[\"Survived\"])","69b14d5c":"from sklearn import metrics\n\n# View confusion matrix\nmetrics.confusion_matrix(y_true=titanic_train[\"Survived\"],  # True labels\n                         y_pred=preds) # Predicted labels","e329a19e":"# View summary of common classification metrics\nprint(metrics.classification_report(y_true=titanic_train[\"Survived\"],\n                                    y_pred=preds) )","cfd6cd86":"# Read and prepare test data\ntitanic_test = pd.read_csv(\"..\/input\/test.csv\")    # Read the data\n\nchar_cabin = titanic_test[\"Cabin\"].astype(str)     # Convert cabin to str\n\nnew_Cabin = np.array([cabin[0] for cabin in char_cabin]) # Take first letter\n\ntitanic_test[\"Cabin\"] = pd.Categorical(new_Cabin)  # Save the new cabin var\n\n# Impute median Age for NA Age values\nnew_age_var = np.where(titanic_test[\"Age\"].isnull(), # Logical check\n                       28,                       # Value if check is true\n                       titanic_test[\"Age\"])      # Value if check is false\n\ntitanic_test[\"Age\"] = new_age_var ","9c88aee6":"# Convert test variables to match model features\nencoded_sex = label_encoder.fit_transform(titanic_test[\"Sex\"])\nencoded_class = label_encoder.fit_transform(titanic_test[\"Pclass\"])\nencoded_cabin = label_encoder.fit_transform(titanic_test[\"Cabin\"])\n\ntest_features = pd.DataFrame([encoded_class,\n                              encoded_cabin,\n                              encoded_sex,\n                              titanic_test[\"Age\"]]).T","3dc53383":"# Make test set predictions\ntest_preds = log_model.predict(X=test_features)\n\n# Create a submission for Kaggle\nsubmission = pd.DataFrame({\"PassengerId\":titanic_test[\"PassengerId\"],\n                           \"Survived\":test_preds})\n\n# Save submission to CSV\nsubmission.to_csv(\"tutorial_logreg_submission.csv\", \n                  index=False)       # Do not save index values","184fb0e6":"*Note: Use model.predict_proba() to get the predicted class probabilities. Use model.predict() to get the predicted classes.*\n\nThe table shows that the model predicted a survival chance of roughly 19% for males and 73% for females. If we used this simple model to predict survival, we'd end up predicting that all women survived and that all men died. Let's make a more complicated model that includes a few more variables from the titanic training set:","bb12839a":"The table above shows the classes our model predicted vs. true values of the Survived variable. This table of predicted vs. actual values is known as a confusion matrix.","4a16d67f":"The confusion matrix is a common tool for assessing the results of classification. Each cell tells us something different about our predictions versus the true values. The bottom right corner indicates the True positives: people the model predicted to survive who actually did survive. The bottom left cell indicates the false positives: people for whom the model predicted survival who did not actually survive. The top left cell indicates the true negatives: people correctly identified as non survivors. Finally, the top right cell shows the false negatives: passengers the model identified as non survivors who actually did survive.\n\nWe can calculate the overall prediction accuracy from the matrix by adding the total number of correct predictions and dividing by the total number of predictions. You can also get the accuracy of a model using the scikit-learn model.score() function:","cc3ba11e":"Overall prediction accuracy is just one of many quantities you can use to assess a classification model. Oftentimes accuracy is not the best metric for assessing a model.\nConsider a model made to predict the occurrence of a disease that only occurs in 0.01% of people. A model that never predicts that anyone has the disease would be 99.99% accurate, but it also wouldn't help save lives. In this case, we might be more interested in the model's sensitivity (recall): the proportion of positive cases that the model correctly identifies as positive.\n\nRelying only on sensitivity can also be a problem. Consider a new model that predicts everyone has the disease. This new model would achieve a sensitivity score of 100% since it would correctly label everyone who has the disease as having the disease. In this case the model's precision--the proportion of positive predictions that turn out to be true positives--would be very low.\n\nWe won't discuss all the different evaluation metrics that fall out the confusion matrix, but it is a good idea to consider accuracy as well as sensitivity and precision when assessing model performance. We can view the confusion matrix, as well as various classification metrics using sklearn's metrics library:","06b22570":"The sigmoid function is bounded below by 0 and bounded above by 1. In logistic regression, the output is interpreted as a probability: the probability that an observation belongs to the second of the two categories being modeled. When the linear combination of variables produces positive numbers, the resulting probability is greater than 0.5 and when it produces negative numbers, the probability is less than 0.5.\n\nWe won't go deeper into the details behind how logistic regression works, but instead focus on how to use it in Python. The most important thing to know is that logistic regression outputs probabilities that we can use to classify observations.","48ec2709":"In the last lesson, we introduced linear regression as a predictive modeling method to estimate numeric variables. Now we turn our attention to classification: prediction tasks where the response variable is categorical instead of numeric. In this lesson we will learn how to use a common classification technique known as logistic regression and apply it to the Titanic survival data.","47e087d0":"Next, let's make class predictions using this model and then compare the predictons to the actual values:","aa8bb879":"# Logistic Regression Basics","f820ae87":"# Python for Data 28: Logistic Regression\n[back to index](https:\/\/www.kaggle.com\/hamelg\/python-for-data-analysis-index)","123904b5":"It turns out that upon submission, this logistic regression model has an accuracy score of 0.74162 which is actually worse than the accuracy of the simplistic women survive, men die model. This goes to show that adding more extra variables to a model doesn't necessarily improve performance.","b5e3aec7":"The logistic regression model coefficients look similar to the output we saw for linear regression. We can see the model produced a positive intercept value and a weight of -2.421 on gender. Let's use the model to make predictions on the test set:","fbf6abbf":"# Wrap Up","588af856":"Logistic regression is a common tool for generating class probabilities and predictions. Although logistic regression models are simple and often insufficient to fully capture relationships between variables in many predictive modeling tasks, they are a good starting point because simple models tend not to overfit the data. Next time we will explore another predictive modeling technique for classification: decision trees.","cf8f2146":"# Revisiting the Titanic","a85d0a9e":"[Logistic regression](https:\/\/en.wikipedia.org\/wiki\/Logistic_regression) is a classification method built on the same concept as linear regression. With linear regression, we take linear combination of explanatory variables plus an intercept term to arrive at a prediction. For example, last time, our simplest linear regression model was:\n\n$$ mileage = intercept + constant * CarWeight $$\n <br>\n\nLinear regression determines which constants minimize the error this linear combination produces on the input data.\nIn classification problems, the response variable is categorical. The simplest case of classification is where the response variable is binary, meaning it can only take one of two values, such as true or false. Logistic regression takes a linear combination of explanatory variables plus an intercept term just like linear regression, but then it takes the result and passes it through a \"logistic\" function. The logistic or sigmoid function used for logistic regression is defined as:\n\n$$ S(t) = \\frac{1}{1 + e^{-t}}$$\n\n<br>\n\nwhere t is the same linear combination of variables used in linear regression. The sigmoid function looks like an elongated S when plotted:\n ","dce81bca":"# The Confusion Matrix","6c68557c":"Now we are ready to use a logistic regression model to predict survival. The scikit-learn library has a logistic regression function in the learn model subfolder. Let's make a logistic regression model that only uses the Sex variable as a predictor. Before creating a model with the sex variable, we need to convert to a real number because sklearn's machine learning functions only deal with real numbers. We can convert a categorical variable like into a number using the sklearn preprocessing function LabelEncoder():","c118468a":"For the Titanic competition, accuracy is the scoring metric used to judge the competition, so we don't have to worry too much about other metrics.\n\nAs a final exercise, let's use our logistic regression model to make predictions for the Titanic test set. First, we need to load and prepare the test data using the same steps we used to prepare the training data:","c17af116":"# Next Lesson: [Python for Data 29: Decision Trees](https:\/\/www.kaggle.com\/hamelg\/python-for-data-29-decision-trees)\n[back to index](https:\/\/www.kaggle.com\/hamelg\/python-for-data-analysis-index)","6a584503":"For the remainder of the lesson we'll be working with the Titanic survival training data from Kaggle that we saw in lesson 14. We'll start by loading the data and then carrying out a few of the same preprocessing tasks we did in lesson 14:"}}