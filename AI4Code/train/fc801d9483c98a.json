{"cell_type":{"0a1895fc":"code","8993e582":"code","04fdf531":"code","53d20ba3":"code","67cca306":"code","54453212":"code","5f8bcc58":"code","ee676323":"code","d9f6e49c":"code","d8744a6b":"code","698a39f4":"code","002bb1af":"code","1b0e10df":"code","8abda29b":"code","3e230832":"code","5d2503cf":"code","cfb84a55":"code","36f62446":"code","d7e742bb":"code","87f6d3cf":"code","36629a8e":"code","f66b4be8":"code","6615992b":"code","5f313719":"code","ab2daea9":"code","0ce8e84b":"code","0713b4a4":"code","bdcccdab":"code","acbd9e0e":"code","95944455":"code","90c20a68":"code","ff1899c7":"code","267d075f":"code","66f6bc77":"code","d0e798b3":"code","f49870ff":"code","4ce30765":"code","cb7f1b30":"code","b653cc79":"code","a74b6cb0":"code","cc215d7f":"code","083f5638":"code","830b713c":"code","b7dc1154":"code","4972fe99":"code","a359a2fa":"code","d25d9f24":"code","6e2e0ab0":"code","da444a6f":"code","5efa9603":"code","37a8d826":"code","c0a4096f":"code","90a269c3":"code","a9767d9b":"code","1dc91805":"code","8c0d8e9c":"code","0f2fad63":"code","4333230f":"code","b1918f2a":"code","f0971277":"code","e7ff2f04":"code","5a3d7715":"markdown","e4c21bc2":"markdown","591a64a7":"markdown","68fd154f":"markdown","b8a869c9":"markdown","dc50a9c0":"markdown","20990286":"markdown","8264d437":"markdown","1d9987ea":"markdown","f2b08d83":"markdown","c381d023":"markdown","7147e318":"markdown","2b739fd8":"markdown","d1a30b2e":"markdown","89d9f88d":"markdown","d778913d":"markdown","545ba052":"markdown","d14c22b5":"markdown","0914320d":"markdown","c59f5ae2":"markdown","a5dcd7c8":"markdown","305533ad":"markdown","a676560e":"markdown","670a0303":"markdown","12742739":"markdown","ac1bb258":"markdown","cf6ac5d0":"markdown"},"source":{"0a1895fc":"# importing necessary libraries:\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport scipy.stats as ss\nfrom collections import Counter\nimport math \nfrom scipy import stats","8993e582":"# converting the CSV files to data frames:\n\nemp_gen = pd.read_csv(\"..\/input\/general_data.csv\") # main dataset\nemp_sur = pd.read_csv(\"..\/input\/employee_survey_data.csv\")  # employee survey data\nemp_man_sur = pd.read_csv(\"..\/input\/manager_survey_data.csv\") # manager survey data\nemp_in_time = pd.read_csv(\"..\/input\/in_time.csv\") # login time data\nemp_out_time = pd.read_csv(\"..\/input\/out_time.csv\") # logout time data\n\n# data_dictionary as a bonus:\nemp_desc = pd.read_excel(\"..\/input\/data_dictionary.xlsx\")\n\n#assigning names to each data frame:\n\nemp_sur.name = 'Employee Survey data'\nemp_gen.name = 'General data'\nemp_in_time.name = 'Log in time data'\nemp_out_time.name = 'Log out time data'\nemp_man_sur.name = 'Manager Survey data'\n","04fdf531":"emp_gen.head(5) #5 top rows of the main dataset","53d20ba3":"gen_and_surveys = [emp_gen,emp_sur,emp_man_sur]  # list of 3 'main' datasets (general + surveys). I will take care of login\/logout datasets separately\nlog_time = [emp_in_time, emp_out_time]","67cca306":"# basic info about datasets:\n\nfor dataset in gen_and_surveys:\n    print(dataset.name + ':\\n')\n    print(dataset.info())\n    print('-'*50)\n    \nfor dataset in log_time:\n    print(dataset.name + ':\\n')\n    print(dataset.info())\n    print('-'*50)","54453212":"emp_gen.describe().T # basic descriptive statistics for main dataset","5f8bcc58":"# let's see if all the values are unique:\n\nfor dataset in gen_and_surveys:\n    print(dataset.name +':')  \n    print(len(set(dataset['EmployeeID'])))  # for each dataset 'EmployeeID' column contains 4410 unique values\n    print('-'*20)  ","ee676323":"# let's see if the log_time datasets have also EmployeeID column:\nfor dataset in log_time:\n    print(dataset.name + ':\\n')\n    print(dataset.columns)\n    print('-'*50)\n\n    # we see in the output that both datasets have the same column names (1 unnamed + dates).","d9f6e49c":"for dataset in log_time:\n    print(dataset.name +':')  \n    print(len(set(dataset['Unnamed: 0'])))  # for each dataset 'EmployeeID' column contains 4410 unique values. It seems it's our desired key column\n    print('-'*20)  ","d8744a6b":"# Let's replace our unnamed columns with \"EmployeeID\"\nfor dataset in log_time:\n    dataset.rename(columns={'Unnamed: 0':'EmployeeID'}, inplace=True) #now in all 5 datasets we have common 'EmployeeID' column containing 4410 unique values.","698a39f4":"# let's make sure that in every set we have the same 'EmployeeID' values:\n\nprint(len(set(emp_sur['EmployeeID']).intersection(emp_gen['EmployeeID']).intersection(emp_in_time['EmployeeID']).intersection(emp_man_sur['EmployeeID']).intersection(emp_out_time['EmployeeID'])))\n\n# we have 4410 (all) common unique values in 'EmployeeID' column for all 5 datasets. Now, we can set the 'EmployeeID' column as an index:","002bb1af":"for dataset in log_time:\n    dataset.set_index('EmployeeID', inplace=True)\n\nfor dataset in gen_and_surveys:\n    dataset.set_index('EmployeeID', inplace=True)","1b0e10df":"# Previously we saw that both sets from our 'log_time' list contain 261 features. To be sure, I will check the number of common header names:\n\nprint(len(emp_in_time.columns.intersection(emp_out_time.columns))) # all (261) variables are common","8abda29b":"# transposing dataframes to perform calculations:\nemp_out_time_transposed = emp_out_time.T\nemp_in_time_transposed = emp_in_time.T","3e230832":"# changing format of indexes and variables\nemp_out_time_transposed.index = pd.to_datetime(emp_out_time_transposed.index)\nemp_in_time_transposed.index = pd.to_datetime(emp_in_time_transposed.index)\n\nemp_out_time_transposed = emp_out_time_transposed.apply(pd.to_datetime, errors='raise')\nemp_in_time_transposed = emp_in_time_transposed.apply(pd.to_datetime, errors='raise')","5d2503cf":"work_time =  emp_out_time_transposed.sub(emp_in_time_transposed)\nwork_time.head()","cfb84a55":"avg_work_time = work_time.mean()   # this variable will help us to create a new feature ('Overtime') a bit later\navg_work_time.head()","36f62446":"main_df = pd.concat(gen_and_surveys,1)","d7e742bb":"main_df.head(5) # checking the first observations for the new dataset","87f6d3cf":"print(round(main_df['Attrition'].value_counts(normalize = True),2))\nsns.countplot(x='Attrition',data=main_df)","36629a8e":"sns.pairplot(main_df[['Age','MonthlyIncome','DistanceFromHome','Attrition']],hue = 'Attrition')","f66b4be8":"sns.pairplot(main_df[['Age','MonthlyIncome','DistanceFromHome','Gender']],hue = 'Gender',hue_order=['Male','Female'], palette={'Male':'black','Female':'magenta'},plot_kws={'alpha':0.1},height=4)","6615992b":"print(\"The youngest employee is {} years old.\\n\\\nThe oldest employee is {} years old.\\n\\\nThe range of ages in the company: {}\".format(main_df.Age.min(), main_df.Age.max(), main_df.Age.max() - main_df.Age.min()))","5f313719":"print('Frequency of travels (in %): \\n')\nprint(round(main_df['BusinessTravel'].value_counts(normalize = True)*100,2))\nprint('\\nAttrition rate by Frequency of travels \\n')\nprint(round(main_df['BusinessTravel'][main_df['Attrition'] == 'Yes'].value_counts()\/main_df['BusinessTravel'].value_counts()*100,2))","ab2daea9":"print('Number of Employees in department (in %): \\n')\nprint(round(main_df['Department'].value_counts(normalize = True)*100,2))  \nprint('\\nAttrition rate by Department \\n')\nprint(round(main_df['Department'][main_df['Attrition'] == 'Yes'].value_counts()\/main_df['Department'].value_counts()*100,2))","0ce8e84b":"print('Number of Employees by Education Level (in %): \\n')\nprint(round(main_df['Education'].value_counts(normalize = True)*100,2).sort_index())\nprint('\\nAttrition rate by Education Level: \\n')\nprint(round(main_df['Education'][main_df['Attrition'] == 'Yes'].value_counts()\/main_df['Education'].value_counts()*100,2).sort_index())","0713b4a4":"print('Number of Employees by Education Field (in %): \\n')\nprint(round(main_df['EducationField'].value_counts(normalize = True)*100,2).sort_index())\n\nprint('\\nAttrition rate by Education Field: \\n')\nprint(round(main_df['EducationField'][main_df['Attrition'] == 'Yes'].value_counts()\/main_df['EducationField'].value_counts()*100,2).sort_index())","bdcccdab":"fig = plt.figure(figsize=(15,12))\nfig.subplots_adjust(top=0.85, wspace=0.7,hspace = 0.6)\n\nax1 = fig.add_subplot(2,2,1)\nax1.set_title(\"Percentage share of employees coming from \\ndifferent fields of educations, by department\")\nsns.heatmap(pd.crosstab(main_df.Department, main_df.EducationField, normalize = 'columns'),\n            cmap=\"coolwarm\", annot=True, cbar=False, center=0.5)\n\n\n\nax2 = fig.add_subplot(2,2,2)\nax2.set_title(\"Percentage share of employees coming from \\ndifferent fields of educations, by JobRole\")\nsns.heatmap(pd.crosstab(main_df.JobRole, main_df.EducationField, normalize = 'columns'),\n            cmap=\"coolwarm\", annot=True, cbar=False, center=0.5)\n","acbd9e0e":"print('\\nAttrition rate by Marital Status: \\n')\nprint(round(main_df['MaritalStatus'][main_df['Attrition'] == 'Yes'].value_counts()\/main_df['MaritalStatus'].value_counts()*100,2))\n(main_df['MaritalStatus'][main_df['Attrition'] == 'Yes'].value_counts()\/main_df['MaritalStatus'].value_counts()*100).plot.bar(color = 'blue')","95944455":"print('\\nAttrition rate by Total number of companies the employee has worked for: \\n')\nprint(round(main_df['NumCompaniesWorked'][main_df['Attrition'] == 'Yes'].value_counts()\/main_df['NumCompaniesWorked'].value_counts()*100,2))\n\n(main_df['NumCompaniesWorked'][main_df['Attrition'] == 'Yes'].value_counts()\/main_df['NumCompaniesWorked'].value_counts()*100).plot.bar(color = 'blue')\n\n# base to create a new feature: 4 or less companies? ","90c20a68":"print('\\Attrition rate by Salary Hike in %: \\n')\nprint(round(main_df['PercentSalaryHike'][main_df['Attrition'] == 'Yes'].value_counts()\/main_df['PercentSalaryHike'].value_counts()*100,2))\n\n(main_df['PercentSalaryHike'][main_df['Attrition'] == 'Yes'].value_counts()\/main_df['PercentSalaryHike'].value_counts()*100).plot.bar(color = 'blue')","ff1899c7":"print('Number of Employees by JobRole (in %): \\n')\nprint(round(main_df['JobRole'].value_counts(normalize = True)*100,2).sort_index())\n\nprint('\\nAttrition rate by JobRole: \\n')\nprint(round(main_df['JobRole'][main_df['Attrition'] == 'Yes'].value_counts()\/main_df['JobRole'].value_counts()*100,2).sort_index())","267d075f":"print('Number of Employees by Job Satisfaction Level(in %): \\n')\nprint(round(main_df['JobSatisfaction'].value_counts(normalize = True)*100,2).sort_index())\n\nprint('\\nAttrition rate by Job Satisfaction Level: \\n')\nprint(round(main_df['JobSatisfaction'][main_df['Attrition'] == 'Yes'].value_counts()\/main_df['JobSatisfaction'].value_counts()*100,2).sort_index())","66f6bc77":"print('Number of Employees by Work Life Balance Level(in %): \\n')\nprint(round(main_df['WorkLifeBalance'].value_counts(normalize = True)*100,2).sort_index())\n\nprint('\\nAttrition rate by Work Life Balance Level: \\n')\nprint(round(main_df['WorkLifeBalance'][main_df['Attrition'] == 'Yes'].value_counts()\/main_df['WorkLifeBalance'].value_counts()*100,2).sort_index())","d0e798b3":"fig = plt.figure(figsize=(15,12))\nfig.subplots_adjust(top=0.85, wspace=0.3,hspace = 0.3)\n\nax1 = fig.add_subplot(2,2,1)\nax1.set_title(\"Share of job satisfaction of employees by work life balance\")\nsns.heatmap(pd.crosstab(main_df.JobSatisfaction, main_df.WorkLifeBalance, normalize = True),\n            cmap=\"coolwarm\", annot=True, cbar=False)\n\n\nax2 = fig.add_subplot(2,2,2)\nax2.set_title(\"Share of job satisfaction of employees by environment satisfaction balance\")\nsns.heatmap(pd.crosstab(main_df.JobSatisfaction, main_df.EnvironmentSatisfaction, normalize = True),\n            cmap=\"coolwarm\", annot=True, cbar=False)\n\nax2 = fig.add_subplot(2,2,3)\nax2.set_title(\"Share of job involvment of employees by job satisfaction\")\nsns.heatmap(pd.crosstab(main_df.JobInvolvement, main_df.JobSatisfaction, normalize = True),\n            cmap=\"coolwarm\", annot=True, cbar=False)\n\nax2 = fig.add_subplot(2,2,4)\nax2.set_title(\"Share of job satisfaction of employees by work life balance\")\nsns.heatmap(pd.crosstab(main_df.JobInvolvement, main_df.WorkLifeBalance, normalize = True),\n            cmap=\"coolwarm\", annot=True, cbar=False)","f49870ff":"print(main_df.isnull().sum())","4ce30765":"main_df['TotalWorkingYears'].fillna(main_df.groupby(['Age'])['TotalWorkingYears'].transform('median'), inplace=True)\nmain_df['NumCompaniesWorked'].fillna(main_df.groupby(['TotalWorkingYears'])['NumCompaniesWorked'].transform('median'), inplace=True)\nmain_df[\"EnvironmentSatisfaction\"].fillna(main_df[\"EnvironmentSatisfaction\"].median(), inplace=True)\nmain_df[\"JobSatisfaction\"].fillna(main_df[\"JobSatisfaction\"].median(), inplace=True)\nmain_df[\"WorkLifeBalance\"].fillna(main_df[\"WorkLifeBalance\"].median(), inplace=True)","cb7f1b30":"main_df['Avg_time_in_company'] = main_df['TotalWorkingYears'] \/ (main_df['NumCompaniesWorked'] + 1)","b653cc79":"main_df['Overtime'] = (avg_work_time.astype('timedelta64[s]') \/ 3600)- main_df['StandardHours'] # adding average overtime in hours variable (float)","a74b6cb0":"sns.pairplot(main_df[['Avg_time_in_company','Overtime','Attrition']],hue = 'Attrition', height = 4)","cc215d7f":"unique_counts = pd.DataFrame.from_records([(col, main_df[col].nunique()) for col in main_df.columns],\n                          columns=['Variable_Name', 'Num_Unique_Vals']).sort_values(by=['Num_Unique_Vals'])\nprint('Number of unique values' +':\\n') \nprint(unique_counts)","083f5638":"# dropping unnecessary variables:\n\nmain_df.drop(['Over18', 'StandardHours', 'EmployeeCount'], axis = 1, inplace = True)","830b713c":"# Functions required to calculate the correlation\/strength-of-association of features (as a part of dython package)\n\nimport scipy.stats as ss\nfrom collections import Counter\nimport math \nfrom scipy import stats\n\n\ndef convert(data, to):\n    converted = None\n    if to == 'array':\n        if isinstance(data, np.ndarray):\n            converted = data\n        elif isinstance(data, pd.Series):\n            converted = data.values\n        elif isinstance(data, list):\n            converted = np.array(data)\n        elif isinstance(data, pd.DataFrame):\n            converted = data.as_matrix()\n    elif to == 'list':\n        if isinstance(data, list):\n            converted = data\n        elif isinstance(data, pd.Series):\n            converted = data.values.tolist()\n        elif isinstance(data, np.ndarray):\n            converted = data.tolist()\n    elif to == 'dataframe':\n        if isinstance(data, pd.DataFrame):\n            converted = data\n        elif isinstance(data, np.ndarray):\n            converted = pd.DataFrame(data)\n    else:\n        raise ValueError(\"Unknown data conversion: {}\".format(to))\n    if converted is None:\n        raise TypeError('cannot handle data conversion of type: {} to {}'.format(type(data),to))\n    else:\n        return converted\n    \ndef conditional_entropy(x, y):\n    \"\"\"\n    Calculates the conditional entropy of x given y: S(x|y)\n    Wikipedia: https:\/\/en.wikipedia.org\/wiki\/Conditional_entropy\n    :param x: list \/ NumPy ndarray \/ Pandas Series\n        A sequence of measurements\n    :param y: list \/ NumPy ndarray \/ Pandas Series\n        A sequence of measurements\n    :return: float\n    \"\"\"\n    # entropy of x given y\n    y_counter = Counter(y)\n    xy_counter = Counter(list(zip(x,y)))\n    total_occurrences = sum(y_counter.values())\n    entropy = 0.0\n    for xy in xy_counter.keys():\n        p_xy = xy_counter[xy] \/ total_occurrences\n        p_y = y_counter[xy[1]] \/ total_occurrences\n        entropy += p_xy * math.log(p_y\/p_xy)\n    return entropy\n\ndef cramers_v(x, y):\n    confusion_matrix = pd.crosstab(x,y)\n    chi2 = ss.chi2_contingency(confusion_matrix)[0]\n    n = confusion_matrix.sum().sum()\n    phi2 = chi2\/n\n    r,k = confusion_matrix.shape\n    phi2corr = max(0, phi2-((k-1)*(r-1))\/(n-1))\n    rcorr = r-((r-1)**2)\/(n-1)\n    kcorr = k-((k-1)**2)\/(n-1)\n    return np.sqrt(phi2corr\/min((kcorr-1),(rcorr-1)))\n\ndef theils_u(x, y):\n    s_xy = conditional_entropy(x,y)\n    x_counter = Counter(x)\n    total_occurrences = sum(x_counter.values())\n    p_x = list(map(lambda n: n\/total_occurrences, x_counter.values()))\n    s_x = ss.entropy(p_x)\n    if s_x == 0:\n        return 1\n    else:\n        return (s_x - s_xy) \/ s_x\n\ndef correlation_ratio(categories, measurements):\n    \"\"\"\n    Calculates the Correlation Ratio (sometimes marked by the greek letter Eta) for categorical-continuous association.\n    Answers the question - given a continuous value of a measurement, is it possible to know which category is it\n    associated with?\n    Value is in the range [0,1], where 0 means a category cannot be determined by a continuous measurement, and 1 means\n    a category can be determined with absolute certainty.\n    Wikipedia: https:\/\/en.wikipedia.org\/wiki\/Correlation_ratio\n    **Returns:** float in the range of [0,1]\n    Parameters\n    ----------\n    categories : list \/ NumPy ndarray \/ Pandas Series\n        A sequence of categorical measurements\n    measurements : list \/ NumPy ndarray \/ Pandas Series\n        A sequence of continuous measurements\n    \"\"\"\n    categories = convert(categories, 'array')\n    measurements = convert(measurements, 'array')\n    fcat, _ = pd.factorize(categories)\n    cat_num = np.max(fcat)+1\n    y_avg_array = np.zeros(cat_num)\n    n_array = np.zeros(cat_num)\n    for i in range(0,cat_num):\n        cat_measures = measurements[np.argwhere(fcat == i).flatten()]\n        n_array[i] = len(cat_measures)\n        y_avg_array[i] = np.average(cat_measures)\n    y_total_avg = np.sum(np.multiply(y_avg_array,n_array))\/np.sum(n_array)\n    numerator = np.sum(np.multiply(n_array,np.power(np.subtract(y_avg_array,y_total_avg),2)))\n    denominator = np.sum(np.power(np.subtract(measurements,y_total_avg),2))\n    if numerator == 0:\n        eta = 0.0\n    else:\n        eta = np.sqrt(numerator\/denominator)\n    return eta\n\ndef associations(dataset, nominal_columns=None, mark_columns=False, theil_u=False, plot=True,\n                          return_results = False, **kwargs):\n    \"\"\"\n    Calculate the correlation\/strength-of-association of features in data-set with both categorical (eda_tools) and\n    continuous features using:\n     - Pearson's R for continuous-continuous cases\n     - Correlation Ratio for categorical-continuous cases\n     - Cramer's V or Theil's U for categorical-categorical cases\n    :param dataset: NumPy ndarray \/ Pandas DataFrame\n        The data-set for which the features' correlation is computed\n    :param nominal_columns: string \/ list \/ NumPy ndarray\n        Names of columns of the data-set which hold categorical values. Can also be the string 'all' to state that all\n        columns are categorical, or None (default) to state none are categorical\n    :param mark_columns: Boolean (default: False)\n        if True, output's columns' names will have a suffix of '(nom)' or '(con)' based on there type (eda_tools or\n        continuous), as provided by nominal_columns\n    :param theil_u: Boolean (default: False)\n        In the case of categorical-categorical feaures, use Theil's U instead of Cramer's V\n    :param plot: Boolean (default: True)\n        If True, plot a heat-map of the correlation matrix\n    :param return_results: Boolean (default: False)\n        If True, the function will return a Pandas DataFrame of the computed associations\n    :param kwargs:\n        Arguments to be passed to used function and methods\n    :return: Pandas DataFrame\n        A DataFrame of the correlation\/strength-of-association between all features\n    \"\"\"\n\n    dataset = convert(dataset, 'dataframe')\n    columns = dataset.columns\n    if nominal_columns is None:\n        nominal_columns = list()\n    elif nominal_columns == 'all':\n        nominal_columns = columns\n    corr = pd.DataFrame(index=columns, columns=columns)\n    for i in range(0,len(columns)):\n        for j in range(i,len(columns)):\n            if i == j:\n                corr[columns[i]][columns[j]] = 1.0\n            else:\n                if columns[i] in nominal_columns:\n                    if columns[j] in nominal_columns:\n                        if theil_u:\n                            corr[columns[j]][columns[i]] = theils_u(dataset[columns[i]],dataset[columns[j]])\n                            corr[columns[i]][columns[j]] = theils_u(dataset[columns[j]],dataset[columns[i]])\n                        else:\n                            cell = cramers_v(dataset[columns[i]],dataset[columns[j]])\n                            corr[columns[i]][columns[j]] = cell\n                            corr[columns[j]][columns[i]] = cell\n                    else:\n                        cell = correlation_ratio(dataset[columns[i]], dataset[columns[j]])\n                        corr[columns[i]][columns[j]] = cell\n                        corr[columns[j]][columns[i]] = cell\n                else:\n                    if columns[j] in nominal_columns:\n                        cell = correlation_ratio(dataset[columns[j]], dataset[columns[i]])\n                        corr[columns[i]][columns[j]] = cell\n                        corr[columns[j]][columns[i]] = cell\n                    else:\n                        cell, _ = ss.pearsonr(dataset[columns[i]], dataset[columns[j]])\n                        corr[columns[i]][columns[j]] = cell\n                        corr[columns[j]][columns[i]] = cell\n    corr.fillna(value=np.nan, inplace=True)\n    if mark_columns:\n        marked_columns = ['{} (nom)'.format(col) if col in nominal_columns else '{} (con)'.format(col) for col in columns]\n        corr.columns = marked_columns\n        corr.index = marked_columns\n    if plot:\n        plt.figure(figsize=(20,20))#kwargs.get('figsize',None))\n        sns.heatmap(corr, annot=kwargs.get('annot',True), fmt=kwargs.get('fmt','.2f'), cmap='coolwarm')\n        plt.show()\n    if return_results:\n        return corr","b7dc1154":"numcols = ['Age', 'NumCompaniesWorked','Avg_time_in_company','TotalWorkingYears','YearsAtCompany','YearsWithCurrManager','YearsSinceLastPromotion','DistanceFromHome','Education',\n           'JobLevel','MonthlyIncome','PercentSalaryHike','Overtime','EnvironmentSatisfaction', 'JobSatisfaction','WorkLifeBalance','PerformanceRating','JobInvolvement']\ncatcols = ['BusinessTravel','TrainingTimesLastYear','Department','EducationField', 'Gender', 'JobRole', 'MaritalStatus','StockOptionLevel', 'Attrition']","4972fe99":"main_df_corr = main_df[numcols + catcols]\nresults = associations(main_df_corr,nominal_columns=catcols,return_results=True)","a359a2fa":"main_df['Overtime'] = np.where(main_df['Overtime']>0, 1, 0)","d25d9f24":"main_df['Attrition'] = main_df['Attrition'].map({'Yes': 1, 'No': 0})\nmain_df['Gender'] = main_df['Gender'].map({'Female': 1, 'Male': 0})\nmain_df['BusinessTravel'] = main_df['BusinessTravel'].map({'Travel_Frequently':2 , 'Travel_Rarely': 1, 'Non-Travel': 0})","6e2e0ab0":"# As we saw previously, the monthly income distribution is skewed to the right.\n# That's why, we will use pd.qcut (quantile-based) function, to devide distribution into intervals\n\nmain_df['Income_band'] = pd.qcut(main_df['MonthlyIncome'], 4)\n\nmain_df[['Income_band', 'Attrition']].groupby(['Income_band'], as_index=False).mean().sort_values(by='Income_band', ascending=True)","da444a6f":"Income_intervals = sorted(main_df['Income_band'].unique())\n   \nmain_df.loc[main_df['MonthlyIncome'] <= Income_intervals[1].left, 'MonthlyIncome'] = 0\nmain_df.loc[(main_df['MonthlyIncome'] > Income_intervals[1].left) & (main_df['MonthlyIncome'] <= Income_intervals[1].right), 'MonthlyIncome'] = 1\nmain_df.loc[(main_df['MonthlyIncome'] > Income_intervals[2].left) & (main_df['MonthlyIncome'] <= Income_intervals[2].right), 'MonthlyIncome'] = 2\nmain_df.loc[ main_df['MonthlyIncome'] > Income_intervals[2].right, 'MonthlyIncome'] = 3","5efa9603":"main_df.head()","37a8d826":"main_df.drop('Income_band', axis = 1, inplace = True)","c0a4096f":"# I am going to use dummy encoding to have numerical representation of nominal variables:\n\ndummy_cols = ['Department', 'MaritalStatus', 'EducationField', 'JobRole']\ndummy_prefix = ['Dep','M_Stat', 'Edu_Field', 'J_Role']\n\ndf_with_dummies = pd.get_dummies(main_df, columns = dummy_cols, prefix = dummy_prefix, drop_first = True)\n","90a269c3":"df_with_dummies.head(5)","a9767d9b":"# importing necessary modules\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import precision_recall_curve, average_precision_score, classification_report, confusion_matrix, roc_auc_score, roc_curve","1dc91805":"# splitting data \n\nX = df_with_dummies.drop('Attrition', axis = 1) # independent features\ny = df_with_dummies['Attrition'] # depentent variable\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state= 44)","8c0d8e9c":"# creating model and fitting training data\nlogreg = LogisticRegression(solver='liblinear', random_state = 44)\n\nlogreg.fit(X_train,y_train)","0f2fad63":"# Obtain the predictions from our logistic regression model:\ny_pred = logreg.predict(X_test)\n\n# Compute predicted probabilities: y_pred_prob\ny_pred_prob = logreg.predict_proba(X_test)[:,1]\n\n# Print the ROC curve, classification report and confusion matrix\nprint(\"ROC AUC score:\\n {} \\n\\n Classification report:\\n{}\\n\\n Confusion matrix:\\n {}\".format(roc_auc_score(y_test, y_pred_prob),\n                                                                                            classification_report(y_test, y_pred),\n                                                                                              confusion_matrix(y_test, y_pred)))","4333230f":"# ROC Curve\n\nfpr, tpr, tresholds = roc_curve(y_test,y_pred_prob)\nplt.plot([0, 1], [0, 1], 'k--')\nplt.plot(fpr, tpr)\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('ROC Curve')\nplt.show()","b1918f2a":"# Calculate average precision and the PR curve\naverage_precision = average_precision_score(y_test, y_pred)\n\n# Obtain precision and recall \nprecision, recall, _ = precision_recall_curve(y_test, y_pred)\n\n# --- plotting Precision-Recall curve -----\nfrom inspect import signature\n\n\n# In matplotlib < 1.5, plt.fill_between does not have a 'step' argument\nstep_kwargs = ({'step': 'post'}\n               if 'step' in signature(plt.fill_between).parameters\n               else {})\nplt.step(recall, precision, color='b', alpha=0.2,\n         where='post')\nplt.fill_between(recall, precision, alpha=0.2, color='b', **step_kwargs)\n\nplt.xlabel('Recall')\nplt.ylabel('Precision')\nplt.ylim([0.0, 1.05])\nplt.xlim([0.0, 1.0])\nplt.title('2-class Precision-Recall curve: avg precision={0:0.2f}'.format(average_precision))","f0971277":"\n# Setup the hyperparameter grid\n\nc_log_space = np.logspace(-5, 8)\ncl_weight = [{0:1,1:1},{0:1,1:1.5},{0:1,1:2},{0:1,1:2.5},{0:1,1:4},{0:1,1:6}, 'balanced']\n\nparam_grid = { 'C': c_log_space, 'class_weight': cl_weight}\n\n\n# Instantiate the GridSearchCV object: logreg_cv\nlogreg_cv = GridSearchCV(estimator = logreg, param_grid= param_grid, cv=5, scoring='roc_auc')\n\n# Fit it to the data\nlogreg_cv.fit(X_train,y_train)\n\n# Print the tuned parameters and score\nprint(\"Tuned Logistic Regression Parameters: {}\".format(logreg_cv.best_params_)) \nprint(\"Best score is {}\".format(logreg_cv.best_score_))\n","e7ff2f04":"# Obtain the predictions from our tuned model:\ny_pred_cv = logreg_cv.predict(X_test)\n\n# Compute predicted probabilities for tuned model: y_pred_prob_cv\ny_pred_prob_cv = logreg_cv.predict_proba(X_test)[:,1]\n\n# Print the ROC curve, classification report and confusion matrix for tuned model:\nprint(\"ROC AUC score:\\n {} \\n\\n Classification report:\\n{}\\n\\n Confusion matrix:\\n {}\".format(roc_auc_score(y_test, y_pred_prob_cv),\n                                                                                            classification_report(y_test, y_pred_cv),\n                                                                                              confusion_matrix(y_test, y_pred_cv)))","5a3d7715":"### 3.3 Removing usuless variables","e4c21bc2":"### 2.3. Exploratory Data Analysis of combined dataset","591a64a7":"Internet is full of correlation examples for continuous data (mostly Pearson's equation), ufortunatelly there are no many examples of associations between continuous-categorical or categorical-categorical features). The article https:\/\/towardsdatascience.com\/the-search-for-categorical-correlation-a1cf7f1888c9 and the 'dython' package are here to help.","68fd154f":"## 1. Importing datasets and libraries","b8a869c9":"### 3.5 Encoding variables","dc50a9c0":"There are five columns with missing values, the number of missing values is relatively low.  I am going to replace all of them with median.","20990286":"As we have filled all the missing values for \"TotalWorkingYears\" and \"NumCompaniesWorked\" variables, we can now create a new feature:","8264d437":"### 2.2 Concatenating datasets","1d9987ea":"### 4. Machine Learning","f2b08d83":"## 3. Data preprocessing","c381d023":"Let's see where do we have missing values (and how many):","7147e318":"### 3.2 Creating new variables","2b739fd8":"The model predicts 63 cases of attrition, out of which 35 employees actually have left the company. There are 28 false positives, what gives us pretty poor precision score = 0.56. Model also didn't catch 87 of employees who actually left the company, hence very low recall score = 0.29.  Let's try to improve that.","d1a30b2e":"The AUC score of tuned model is a bit higher than in the first attempt (0.8449 vs 0.8438), but the improvement is not satisfactory. Probably it's a good idea to check another tuning parameters for a model, check another alghoritms or, for example, create new 'artificial variables'.\n\nThe tuned model predicts 99 cases of attrition, out of which 50 employees actually have left the company. The 49 of false positives result in lower precision score (0.51 comparing to 0.56 in the first model). The tuned model didn't detect 'only' 72 of employees who actually left the company (first model - 87), what results in higher recall score = 0.41.\n","89d9f88d":"Many categorical variables have been already encoded. It makes the work easier, but there are still a few ordinal and nominal variables that require to be encoded later.","d778913d":"## 2. Exploratory Data Analysis","545ba052":"The 'Over18' , 'StandardHours' and 'EmployeeCount' columns have just one unique values - they are not useful for purpose of this analysis anymore. The dataset contains many variables with just a few unique values - we will treat theme as either ordinal or nominal variables. The other variables we will bin into small groups and we will treat theme as ordinal ones.","d14c22b5":"Employees from the Humar Resources department (the smallest one) were more likely to leave the company:","0914320d":"**Our goal is to predict employees' churn, that's why the 'Attrition' is our dependent variable**. Around 16% of the employees has already left the company:","c59f5ae2":"### 2.1. Basic exploratory data analysis","a5dcd7c8":"Now it's to time to use previously created variable 'avg_work_time' to build a new feature:","305533ad":"Let's see if somehow we can combine all the datasets. The 'EmployeeID' seems to be a key, as it appears in every dataset.","a676560e":"We have already pulled out the necessary information from the login \/ logout time datasets, so we don't need them anymore. Let's combine the 3 others.","670a0303":"### 3.1 Dealing with missing values","12742739":"Many of the numerical variables we could treat as categorical ones. The number of unique values for each category can tell us more. Here, we need to remember that we have created few 'artificial' values when we were replacing missing values with median (in some cases median was not an integer).","ac1bb258":"### 3.4 Correlation matrix","cf6ac5d0":"Now we are almost ready to merge all the datasets, but firstly let's explore the login\/logout datasets and pull out usefull information from there."}}