{"cell_type":{"02e043a4":"code","d6e8dc17":"code","b8855e09":"code","cca6b594":"code","674b1250":"code","4cfc7d32":"code","c47fac5c":"code","a07ac40d":"code","2b74b435":"code","8989b103":"code","6799c06b":"code","2724d0c7":"code","aba1985c":"code","7e5f4b17":"code","060f5067":"code","e0b11a61":"code","7c323a8b":"code","3cc4801e":"code","3ca95df3":"code","97a1f533":"code","ca958ac4":"markdown","c22f3982":"markdown","57f40ce9":"markdown","c0c2f091":"markdown","cb211162":"markdown","be8bd937":"markdown","ab122b1a":"markdown","e29237d7":"markdown","b0935d50":"markdown","f9ce5f40":"markdown","945b929e":"markdown","81e23eca":"markdown","af3b0dd2":"markdown","fed2e7ac":"markdown","9a9fdfa2":"markdown","7b62720f":"markdown","11e87242":"markdown","10796dec":"markdown","60dd7fb1":"markdown"},"source":{"02e043a4":"import pandas as pd\nimport numpy as np\nimport os","d6e8dc17":"# ALICE_PATH = os.path.join(\"datasets\", \"alice\")\n\n# def load_alice_data(alice_path=ALICE_PATH):\n#     csv_path_train = os.path.join(alice_path, \"train_sessions.csv\")\n#     csv_path_test = os.path.join(alice_path, \"test_sessions.csv\")\n#     return pd.read_csv(csv_path_train, index_col='session_id', parse_dates=['time1']), \\\n#             pd.read_csv(csv_path_test, index_col='session_id', parse_dates=['time1'])\n\n# df_train, df_test = load_alice_data()","b8855e09":"df_train = pd.read_csv(\"..\/input\/train_sessions.csv\", index_col='session_id', parse_dates=['time1'])\ndf_test = pd.read_csv(\"..\/input\/test_sessions.csv\", index_col='session_id', parse_dates=['time1'])","cca6b594":"df_train.head()","674b1250":"df_train = df_train.sort_values(by=\"time1\")","4cfc7d32":"df_train.info()","c47fac5c":"for i in range(2, 11):\n    df_train['time{}'.format(i)] = pd.to_datetime(df_train['time{}'.format(i)])\nfor i in range(2, 11):\n    df_test['time{}'.format(i)] = pd.to_datetime(df_test['time{}'.format(i)])","a07ac40d":"from sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.pipeline import FeatureUnion\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.preprocessing import StandardScaler","2b74b435":"class DataPreparator(BaseEstimator, TransformerMixin):\n    \"\"\"\n    Fill NaN with zero values.\n    \"\"\"\n    def fit(self, X, y=None):\n        return self\n    def transform(self, X, y=None):\n        sites = ['site%s' % i for i in range(1, 11)]\n        return X[sites].fillna(0).astype('int')","8989b103":"class ListPreparator(BaseEstimator, TransformerMixin):\n    \"\"\"\n    Prepare a CountVectorizer friendly 2D-list from data.\n    \"\"\"\n    def fit(self, X, y=None):\n        return self\n    def transform(self, X, y=None):\n        X = X.values.tolist()\n        # Convert dataframe rows to strings\n        return [\" \".join([str(site) for site in row]) for row in X]","6799c06b":"class AttributesAdder(BaseEstimator, TransformerMixin):\n    \"\"\"\n    Add new attributes to training and test set.\n    \"\"\"\n    def fit(self, X, y=None):\n        return self \n    def transform(self, X, y=None):\n        # intraday features\n        hour = X['time1'].apply(lambda ts: ts.hour)\n        morning = ((hour >= 7) & (hour <= 11)).astype('int')\n        day = ((hour >= 12) & (hour <= 18)).astype('int')\n        evening = ((hour >= 19) & (hour <= 23)).astype('int')\n        \n        # season features\n        month = X['time1'].apply(lambda ts: ts.month)\n        summer = ((month >= 6) & (month <= 8)).astype('int')\n        \n        # day of the week features\n        weekday = X['time1'].apply(lambda ts: ts.weekday()).astype('int')\n        \n        # year features\n        year = X['time1'].apply(lambda ts: ts.year).astype('int')\n        \n        X = np.c_[morning.values, day.values, evening.values, summer.values, weekday.values, year.values]\n        return X","2724d0c7":"class ScaledAttributesAdder(BaseEstimator, TransformerMixin):\n    \"\"\"\n    Add new features, that should be scaled.\n    \"\"\"\n    def fit(self, X, y=None):\n        return self\n    def transform(self, X, y=None):\n        # session time features\n        times = ['time%s' % i for i in range(1, 11)]\n        # session duration: take to the power of 1\/5 to normalize the distribution\n        session_duration = (X[times].max(axis=1) - X[times].min(axis=1)).astype('timedelta64[ms]').astype(int) ** 0.2\n        # number of sites visited in a session\n        number_of_sites = X[times].isnull().sum(axis=1).apply(lambda x: 10 - x)\n        # average time spent on one site during a session\n        time_per_site = (session_duration \/ number_of_sites) ** 0.2\n        \n        X = np.c_[session_duration.values]\n        return X","aba1985c":"vectorizer_pipeline = Pipeline([\n    (\"preparator\", DataPreparator()),\n    (\"list_preparator\", ListPreparator()),\n    (\"vectorizer\", CountVectorizer(ngram_range=(1, 3), max_features=50000))\n])\n\nattributes_pipeline = Pipeline([\n    (\"adder\", AttributesAdder())\n])\n\nscaled_attributes_pipeline = Pipeline([\n    (\"adder\", ScaledAttributesAdder()),\n    (\"scaler\", StandardScaler())\n])","7e5f4b17":"full_pipeline = FeatureUnion(transformer_list=[\n('vectorizer_pipeline', vectorizer_pipeline),\n('attributes_pipeline', attributes_pipeline),\n('scaled_attributes_pipeline', scaled_attributes_pipeline)\n])","060f5067":"X_train = full_pipeline.fit_transform(df_train)\nX_test = full_pipeline.transform(df_test)\n\ny_train = df_train[\"target\"].astype('int').values","e0b11a61":"from sklearn.model_selection import TimeSeriesSplit, cross_val_score\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.linear_model import LogisticRegression","7c323a8b":"time_split = TimeSeriesSplit(n_splits=10)\n\nlogit = LogisticRegression(C=1, random_state=42, solver='liblinear')\n\ncv_scores = cross_val_score(logit, X_train, y_train, cv=time_split, \n                        scoring='roc_auc', n_jobs=1)\n\ncv_scores.mean()","3cc4801e":"logit.fit(X_train, y_train)","3ca95df3":"def write_to_submission_file(predicted_labels, out_file,\n                             target='target', index_label=\"session_id\"):\n    predicted_df = pd.DataFrame(predicted_labels,\n                                index = np.arange(1, predicted_labels.shape[0] + 1),\n                                columns=[target])\n    predicted_df.to_csv(out_file, index_label=index_label)","97a1f533":"logit_test_pred = logit.predict_proba(X_test)[:, 1]\n\nwrite_to_submission_file(logit_test_pred, 'submit.csv') # 0.95191","ca958ac4":"**Here, in the kernel, we'll do this manually.**","c22f3982":"## Clean data","57f40ce9":"**One of the great advantages of pipelines, is that you can use classes for adding features too. You can test new features quite easilly by adding\/removing some of them from the class, and then just running the whole pipeline. **","c0c2f091":"**Finally you can combine these pipelines using ``FeatureUnion()`` class, which will merge the resulting datasets from each pipeline. **","cb211162":"**Once we've written the classes, we want to combine them into a pipeline. The ``Pipeline()`` class will call ``transform()`` methods on each one of them and return the transformed dataset, which you can pass to another pipeline as many times as you want. Here we have three separate pipelines with different purposes: ``vectorizer_pipeline`` prepares data for ``CountVectorizer()`` class, ``attributes_pipeline`` adds features and ``scaled_attributes_pipeline`` adds scaled features. **","be8bd937":"**Now let's take a look at the column data types.**","ab122b1a":"**If you're working on a local machine, it's always a good practice to write a function for data downloading. It can be as easy as the one shown below.**","e29237d7":"**As you can see, by using the pipeline template you can write fewer lines of code and understand more, what's going on in your data preparation workflow. You can test features much faster. But the greatest thing about pipelines is that you can generalize the data preparation and training process by changing the dataset you use as a the pipeline input. For example, you can use the same pipeline for transforming your train and test set. And even more: you can automate the whole process, when new data becomes available.**\n\n**Good luck with Alice!**","b0935d50":"**All you need to do at the end, is just call ``fit_transform()`` or ``transform()`` methods on the ``full_pipeline`` and pass them your original datasets.**","f9ce5f40":"## Pipeline configuration","945b929e":"## Download data","81e23eca":"**As @yorko introduced, we use time-aware cross-validation scheme.**","af3b0dd2":"**The key idea here is to write your own classes for data transformation. They must follow a pretty simple template: they should inherit from two base classes \u2013 ``BaseEstimator``, ``TransformerMixin`` \u2013 and have a ``fit()`` and ``transform()`` methods, which take the dataset (X) as input and have a y value set to None. In pipelines the ``__init__()`` method is not neccessary.**","fed2e7ac":"**It seems like only the first time column is in datetime format. Better convert the rest now, otherwise you'll have a problem with the datetime arithmetic, for example, when calculation session duration as a feature.**","9a9fdfa2":"**I'll skip EDA and feature engineering for now \u2013 there are plenty of great kernels in this competition, that cover those in depth. Instead I'll try to show you another way of preparing data and training models \u2013 using the sklearn ``Pipeline()`` class, instead of good old functions. This approach is based more on object oriented, rather than procedural programming, which in fact reduces the length of your code quite a lot.**","7b62720f":"**Finally, train your model on the whole train set and write a function for submitting results.**","11e87242":"### Hi, in this kernel we'll make a baseline submission using sklearn pipelines instead of functions. Some of the advantages:\n* Less and more readable code\n* Faster feature engineering\n* More automation","10796dec":"**First of all we should notice, that our data is time dependent. Leaving it shuffled will cause some problems later on during the cross-validation, so let's sort it right away.**","60dd7fb1":"## Cross-validation and submitting results"}}