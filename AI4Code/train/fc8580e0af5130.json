{"cell_type":{"5aa232b5":"code","4e3b31ec":"code","d7422b97":"code","34cf75a3":"code","6b44078f":"code","5ed1fa62":"code","c8bf25ae":"code","754ea9dc":"code","c52a867f":"code","da7967de":"code","c171e26a":"code","25b39d28":"code","0dbde424":"code","c02e1574":"code","755ebec6":"code","88bb6319":"code","5275a7ce":"code","09d04aa7":"code","c74df915":"code","8795ec47":"code","53b7a63a":"code","90200e61":"code","8b8765d9":"code","b1d83f34":"code","c6676d46":"code","4bb6e4dc":"code","abde29b0":"code","2f420061":"code","9d74819c":"code","a9d2fa85":"code","96fc98f0":"code","3ed44a2d":"code","dc4604c7":"code","340f3511":"code","59e00c09":"code","0d2e20c0":"code","a0793a6b":"code","b60abae3":"code","52c633ea":"code","13ac63e8":"code","ef057404":"code","dbc79463":"code","7909daad":"code","f99e491b":"code","8a00e444":"code","a8bb6116":"code","ec112674":"code","247c41ac":"code","ff9a4e56":"code","77b59e8b":"code","a44da120":"code","1169b337":"markdown","b1ca0f21":"markdown","b940a593":"markdown","abbad491":"markdown","f965f5eb":"markdown","f6fa2e20":"markdown","9ee6f80d":"markdown","c84cdbde":"markdown","a5a2b7a9":"markdown","37e9799f":"markdown","cfe8aa0c":"markdown","e8a76cfe":"markdown","a9fffb4b":"markdown","175ea89a":"markdown","fc847d49":"markdown","1725fbb1":"markdown","f854dfb1":"markdown","10b2bf83":"markdown","0477b569":"markdown","d960bc2f":"markdown","7723472d":"markdown","1b59c9be":"markdown","2de2bae0":"markdown","643cfe9e":"markdown","9624c5d9":"markdown","41eb5cc5":"markdown","c0c19c17":"markdown"},"source":{"5aa232b5":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","4e3b31ec":"# save filepath to variable for easier access\nHome_train_data_path = '\/kaggle\/input\/home-data-for-ml-course\/train.csv'\nHome_test_data_path = '\/kaggle\/input\/home-data-for-ml-course\/test.csv'\n# read the data and store data in DataFrame titled Home_data\ntrain_data = pd.read_csv(Home_train_data_path,index_col='Id')\ntest_data = pd.read_csv(Home_test_data_path, index_col='Id')\nprint(\"Train shape : \", train_data.shape)\nprint(\"Test shape : \", test_data.shape)","d7422b97":"# import the required libraries\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.model_selection import train_test_split\n\nfrom sklearn.model_selection import cross_val_score, KFold, cross_validate\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.metrics import mean_squared_error,mean_absolute_error\nfrom xgboost import XGBRegressor\nfrom lightgbm import LGBMRegressor\nfrom mlxtend.regressor import StackingCVRegressor","34cf75a3":"pd.set_option('display.max_columns', 5000)\ntrain_data.head()","6b44078f":"pd.set_option('display.max_rows', 5000)\n# columns with missing values\ntrain_data[train_data.columns[train_data.isnull().any()]].isnull().sum()","5ed1fa62":"# percentage of missing values\n(train_data[train_data.columns[train_data.isnull().any()]].isnull().sum()* 100 \/ train_data.shape[0]).sort_values(axis=0, ascending=False)","c8bf25ae":"# splitting the data into numerical and categorical.\nnumeric_col = train_data.select_dtypes(exclude=['object']).drop(['MSSubClass'], axis=1).copy()\ncat_col = train_data.select_dtypes(include=['object']).copy()\ncat_col['MSSubClass'] = train_data['MSSubClass']","754ea9dc":"#distribution of continuous numerical columns\ndisc_num_var = ['OverallQual','YearBuilt','YearRemodAdd','OverallCond','GarageYrBlt','BsmtFullBath','BsmtHalfBath','FullBath','HalfBath',\n                'BedroomAbvGr', 'KitchenAbvGr', 'TotRmsAbvGrd', 'Fireplaces', 'GarageCars', 'MoSold', 'YrSold']\ncont_num_var = []\nfor i in numeric_col.columns:\n    if i not in disc_num_var:\n        cont_num_var.append(i)\n\nfig = plt.figure(figsize=(18,16))\nfor index,col in enumerate(cont_num_var):\n    plt.subplot(6,4,index+1)\n    sns.distplot(numeric_col.loc[:,col].dropna(), kde=False)\nfig.tight_layout(pad=1.0)","c52a867f":"# checking outliers for continuous numerical columns \nfig = plt.figure(figsize=(14,15))\nfor index,col in enumerate(cont_num_var):\n    plt.subplot(6,4,index+1)\n    sns.boxplot(y=col, data=numeric_col.dropna())\nfig.tight_layout(pad=1.0)","da7967de":"#data distribution of discontinous numerical columns\nfig = plt.figure(figsize=(20,15))\nfor index,col in enumerate(disc_num_var):\n    plt.subplot(5,4,index+1)\n    sns.countplot(x=col, data=numeric_col.dropna())\nfig.tight_layout(pad=1.0)","c171e26a":"#data distribution of categorical columns\nfig = plt.figure(figsize=(18,20))\nfor index in range(len(cat_col.columns)):\n    plt.subplot(9,5,index+1)\n    sns.countplot(x=cat_col.iloc[:,index], data=cat_col.dropna())\n    plt.xticks(rotation=90)\nfig.tight_layout(pad=1.0)","25b39d28":"#correlation matrix\nplt.figure(figsize=(20,15))\nmask = numeric_col.corr() < 0.8\n\nsns.heatmap(numeric_col.corr(),annot=True,mask = mask ,linewidth=0.7,fmt='.2g',cmap='Blues')","0dbde424":"# Correlation with Target variable\nnumeric_col.corr()[['SalePrice']].sort_values(['SalePrice'],ascending = False)","c02e1574":"# Removing outliers \ntrain_data = train_data.drop(train_data[train_data['LotFrontage'] > 200].index)\ntrain_data = train_data.drop(train_data[train_data['LotArea'] > 100000].index)\ntrain_data = train_data.drop(train_data[train_data['BsmtFinSF1'] > 4000].index)\ntrain_data = train_data.drop(train_data[train_data['TotalBsmtSF'] > 5000].index)\ntrain_data = train_data.drop(train_data[train_data['GrLivArea'] > 4000].index)","755ebec6":"# combining train and test data\ny = train_data['SalePrice'].reset_index(drop=True)\nX_train = train_data.drop(['SalePrice'], axis=1)\nX_test = test_data\nX_all =  pd.concat([X_train, X_test]).reset_index(drop=True)","88bb6319":"# Removing multicollinearity variables\nX_all.drop(['GarageYrBlt','TotRmsAbvGrd','1stFlrSF','GarageArea'], axis=1, inplace=True)","5275a7ce":"# Removing variables with more than 90% missing values\nX_all.drop(['PoolQC','MiscFeature','Alley'], axis=1, inplace=True)","09d04aa7":"#Removing variables which are practically not available at the time of making predictions\nX_all.drop(['MoSold','YrSold'], axis=1, inplace=True)","c74df915":"#practically less important\nless_important = ['LotShape','LandSlope','LandContour']\nX_all.drop(less_important, axis=1, inplace=True)","8795ec47":"# Removing Categorical variables with mostly one value\ncat_col = X_train.select_dtypes(include=['object']).columns\noverfit_cat_col = []\nfor i in cat_col:\n    counts = X_train[i].value_counts()\n    zeros = counts.iloc[0]\n    if zeros \/ len(X_train) * 100 > 96:\n        overfit_cat_col.append(i)\noverfit_cat_col = list(overfit_cat_col)\nX_all.drop(overfit_cat_col, axis=1,inplace = True)","53b7a63a":"#Removing numerical variables with mostly one value\nnumeric_col = X_train.select_dtypes(exclude = ['object']).columns\noverfit_num_col = []\nfor i in numeric_col:\n    counts = X_train[i].value_counts()\n    zeros = counts.iloc[0]\n    if zeros\/len(X_train)*100 > 95:\n        overfit_num_col.append(i)\noverfit_num_col = list(overfit_num_col)\nX_all.drop(overfit_num_col,axis=1,inplace = True)","90200e61":"#changing data types of variables with incorrect datatypes\nX_all['MSSubClass'] = X_all['MSSubClass'].apply(str)","8b8765d9":"#Mapping ordinal variables\nordinal_col = ['ExterQual','ExterCond','BsmtQual', 'BsmtCond','HeatingQC','KitchenQual','GarageQual','GarageCond', 'FireplaceQu','BsmtFinType1','BsmtFinType2','Fence','BsmtExposure']\nrating_col = ['ExterQual','ExterCond','BsmtQual', 'BsmtCond','HeatingQC','KitchenQual','GarageQual','GarageCond', 'FireplaceQu']\nrating_map = {'Ex': 5,'Gd': 4, 'TA': 3, 'Fa': 2, 'Po': 1, 'NA':0}\nfintype_map = {'GLQ': 6,'ALQ': 5,'BLQ': 4,'Rec': 3,'LwQ': 2,'Unf': 1, 'NA': 0}\nexpose_map = {'Gd': 4, 'Av': 3, 'Mn': 2, 'No': 1, 'NA': 0}\nfence_map = {'GdPrv': 4,'MnPrv': 3,'GdWo': 2, 'MnWw': 1,'NA': 0}\n\n#Labeling ordinal variables\nfor col in rating_col:\n    X_all[col] = X_all[col].map(rating_map)\n    \nfin_col = ['BsmtFinType1','BsmtFinType2']\nfor col in fin_col:\n    X_all[col] = X_all[col].map(fintype_map) \n\nX_all['BsmtExposure'] = X_all['BsmtExposure'].map(expose_map)\nX_all['Fence'] = X_all['Fence'].map(fence_map)","b1d83f34":"# Missing Numerical and categorical columns\nnum_col_with_missing_values = X_all[X_all.columns[X_all.isnull().any()]].select_dtypes(exclude=['object']).columns\nCat_col_with_missing_values = X_all[X_all.columns[X_all.isnull().any()]].select_dtypes(include=['object']).columns","c6676d46":"# Making a copy of data for handling missing values\nX_all_plus = X_all.copy()\nX_all_plus = X_all.copy()","4bb6e4dc":"# Handling numerical missing values\n#creating new columns to indicate the numerical missing values\nfor col in num_col_with_missing_values:\n    X_all_plus[col + '_was_missing'] = X_all_plus[col].isnull()\n\n#filling the numerical missing values with 0\nX_all_plus[num_col_with_missing_values] = X_all_plus[num_col_with_missing_values].fillna(0)","abde29b0":"#filling missing categorical values with \"Missing\"\nX_all_plus[Cat_col_with_missing_values] = X_all_plus[Cat_col_with_missing_values].fillna(\"Missing\")","2f420061":"#interaction features\nX_all_plus['TotalLot'] = X_all_plus['LotFrontage'] + X_all_plus['LotArea']\nX_all_plus['TotalBsmtFin'] = X_all_plus['BsmtFinSF1'] + X_all_plus['BsmtFinSF2']\nX_all_plus['TotalBath'] = X_all_plus['FullBath'] + X_all_plus['HalfBath'] + X_all_plus['BsmtFullBath'] + X_all_plus['BsmtHalfBath']\nX_all_plus['TotalPorch'] = X_all_plus['OpenPorchSF'] + X_all_plus['EnclosedPorch'] + X_all_plus['ScreenPorch']+ X_all_plus['WoodDeckSF']","9d74819c":"#indicator features\nX_all_plus['fireplace_available'] = X_all_plus['Fireplaces'].apply(lambda x: 1 if x > 0 else 0)","a9d2fa85":"#removing redundant features\nredundant_features = ['LotFrontage','LotArea','BsmtFinSF1','Fireplaces','BsmtFinSF2','FullBath','HalfBath','BsmtFullBath','BsmtHalfBath','ScreenPorch','EnclosedPorch','OpenPorchSF','WoodDeckSF']\nX_all_plus.drop(redundant_features, axis=1, inplace=True)","96fc98f0":"#splitting train and test data\nX = X_all_plus.iloc[:len(y), :]\ntest = X_all_plus.iloc[len(X):, :]","3ed44a2d":"#creating training and validation datasets\ntrain_X, valid_X, train_y, valid_y = train_test_split(X, y, random_state = 42)","dc4604c7":"# Get list of categorical variables\ns = (train_X.dtypes == 'object')\nobject_cols = list(s[s].index)\n\n# Apply one-hot encoder to each column with categorical data\nOH_encoder = OneHotEncoder(handle_unknown='ignore', sparse=False)\nOH_cols_train = pd.DataFrame(OH_encoder.fit_transform(train_X[object_cols]))\nOH_cols_valid = pd.DataFrame(OH_encoder.transform(valid_X[object_cols]))\nOH_cols_test = pd.DataFrame(OH_encoder.transform(test[object_cols]))\n\n# One-hot encoding removed index; put it back\nOH_cols_train.index = train_X.index\nOH_cols_valid.index = valid_X.index\nOH_cols_test.index = test.index\n\n# Remove categorical columns (will replace with one-hot encoding)\nnum_X_train = train_X.drop(object_cols, axis=1)\nnum_X_valid = valid_X.drop(object_cols, axis=1)\nnum_X_test = test.drop(object_cols, axis=1)\n\n# Add one-hot encoded columns to numerical features\nOH_X_train = pd.concat([num_X_train, OH_cols_train], axis=1)\nOH_X_valid = pd.concat([num_X_valid, OH_cols_valid], axis=1)\nOH_X_test = pd.concat([num_X_test, OH_cols_test], axis=1)\n","340f3511":"#final features data and variable\nOH_X_final =  pd.concat([OH_X_train, OH_X_valid]).reset_index(drop=True)\ny_final  = pd.concat([train_y,valid_y]).reset_index(drop=True)","59e00c09":"def get_mae_xgb(X, y,learning_rate=0.1,n_estimators=1000,max_depth=5,min_child_weight=1,gamma=0,subsample=0.8,\n            colsample_bytree=0.8,reg_alpha=0.005,scale_pos_weight=1):\n    model = XGBRegressor(\n learning_rate =learning_rate,\n n_estimators=n_estimators,\n max_depth=max_depth,\n min_child_weight=min_child_weight,\n gamma=gamma,\n subsample=subsample,\n colsample_bytree=colsample_bytree,\n scale_pos_weight=scale_pos_weight,\nreg_alpha=reg_alpha,\n objective= 'reg:squarederror',\n    seed =42)\n    scores = -1 * cross_val_score(model,X,y,cv=5,scoring='neg_mean_absolute_error')\n    return scores.mean()","0d2e20c0":"# fine tuning learning_rate\nfor learning_rate in [0.011,0.012,0.013]:\n    my_mae = get_mae_xgb(X= OH_X_train, y=train_y,learning_rate =learning_rate)\n    print(\"optimal learning_rate: %.4f  \\t\\t Mean Absolute Error:  %.4f\" %(learning_rate, my_mae))\n    \n# fine tuning n_estimators\nfor n_estimators in range(1400,1800,100):\n    my_mae = get_mae_xgb(X= OH_X_train, y=train_y,learning_rate =0.012,n_estimators=n_estimators)\n    print(\"optimal n_estimators: %.4f  \\t\\t Mean Absolute Error:  %.4f\" %(n_estimators, my_mae))\n\n# fine tuning max_depth\nfor max_depth in range(4,7,1):\n    my_mae = get_mae_xgb(X= OH_X_train, y=train_y,learning_rate =0.012,n_estimators=1600,max_depth=max_depth)\n    print(\"optimal max_depth: %.4f  \\t\\t Mean Absolute Error:  %.4f\" %(max_depth, my_mae))\n\n# fine tuning min_child_weight\nfor min_child_weight in range(0,3,1):\n    my_mae = get_mae_xgb(X= OH_X_train, y=train_y,learning_rate =0.012,n_estimators=1600,max_depth=4,min_child_weight=min_child_weight)\n    print(\"optimal min_child_weight: %.4f  \\t\\t Mean Absolute Error:  %.4f\" %(min_child_weight, my_mae))\n\n# fine tuning gamma\nfor gamma in range(0,3,1):\n    my_mae = get_mae_xgb(X= OH_X_train, y=train_y,learning_rate =0.012,n_estimators=1600,max_depth=4\n                     ,min_child_weight=2,gamma=gamma)\n    print(\"optimal gamma: %.4f  \\t\\t Mean Absolute Error:  %.4f\" %(gamma, my_mae))\n\n# fine tuning subsample\nfor subsample in [0.74,0.75,0.76]:\n    my_mae = get_mae_xgb(X= OH_X_train, y=train_y,learning_rate =0.012,n_estimators=1600,max_depth=4\n                     ,min_child_weight=2,gamma=0,subsample=subsample)\n    print(\"optimal subsample: %.4f  \\t\\t Mean Absolute Error:  %.4f\" %(subsample, my_mae))\n    \n# fine tuning colsample_bytree\nfor colsample_bytree in [0.77,0.78,0.79]:\n    my_mae = get_mae_xgb(X= OH_X_train, y=train_y,learning_rate =0.012,n_estimators=1600,max_depth=4\n                     ,min_child_weight=2,gamma=0,subsample=0.75,colsample_bytree=colsample_bytree)\n    print(\"optimal colsample_bytree: %.4f  \\t\\t Mean Absolute Error:  %.4f\" %(colsample_bytree, my_mae))\n    \n# fine tuning scale_pos_weight\nfor scale_pos_weight in [0,1,2]:\n    my_mae = get_mae_xgb(X= OH_X_train, y=train_y,learning_rate =0.012,n_estimators=1600,max_depth=4\n                     ,min_child_weight=2,gamma=0,subsample=0.75,colsample_bytree=0.79\n                    ,scale_pos_weight=scale_pos_weight)\n    print(\"optimal scale_pos_weight: %.4f  \\t\\t Mean Absolute Error:  %.4f\" %(scale_pos_weight, my_mae))\n\n# fine tuning reg_alpha\nfor reg_alpha in [0.005,0.05,0.5,0.1]:\n    my_mae = get_mae_xgb(X= OH_X_train, y=train_y,learning_rate =0.012,n_estimators=1600,max_depth=4\n                     ,min_child_weight=2,gamma=0,subsample=0.75,colsample_bytree=0.79\n                    ,scale_pos_weight=1,reg_alpha=reg_alpha)\n    print(\"optimal reg_alpha: %.4f  \\t\\t Mean Absolute Error:  %.4f\" %(reg_alpha, my_mae))","a0793a6b":"# final xgboost model\nxgboost = XGBRegressor(\n    learning_rate=0.012,\n    n_estimators=1600,\n    max_depth=4,\n    min_child_weight=2,\n    gamma=0,\n    subsample=0.75,\n    colsample_bytree=0.79,\n    scale_pos_weight=1,\n    reg_alpha=0.005,\n    seed=42\n)","b60abae3":"def get_mae_lgb(X, y,learning_rate=0.1,n_estimators=1000,feature_fraction=0.18,num_leaves=5,max_bin=180\n            ,min_data_in_leaf=8,bagging_fraction = 0.35,bagging_freq =7):\n    model = LGBMRegressor(objective='regression',\n                          learning_rate=learning_rate,\n                         n_estimators=n_estimators,\n                         feature_fraction_seed=42,\n                         feature_fraction=feature_fraction,\n                         num_leaves=num_leaves,\n                          max_bin=max_bin,\n                          min_data_in_leaf=min_data_in_leaf,\n                         bagging_fraction=bagging_fraction,\n                         bagging_seed=42,\n                         bagging_freq=bagging_freq)\n    scores = -1 * cross_val_score(model,X,y,cv=5,scoring='neg_mean_absolute_error')\n    return scores.mean()","52c633ea":"# fine tuning learning_rate\nfor learning_rate in [0.023,0.024,0.025]:\n    my_mae = get_mae_lgb(X= OH_X_train, y=train_y,learning_rate =learning_rate)\n    print(\"optimal learning_rate: %.4f  \\t\\t Mean Absolute Error:  %.4f\" %(learning_rate, my_mae))\n\n# fine tuning n_estimators\nfor n_estimators in range(2200,2800,100):\n    my_mae = get_mae_lgb(X= OH_X_train, y=train_y,learning_rate =0.023,n_estimators=n_estimators)\n    print(\"optimal n_estimators: %.4f  \\t\\t Mean Absolute Error:  %.8f\" %(n_estimators, my_mae))\n    \n# fine tuning feature_fraction\nfor feature_fraction in [0.28,0.29,0.3,0.31,0.32]:\n    my_mae = get_mae_lgb(X= OH_X_train, y=train_y,learning_rate =0.023,n_estimators=2400,\n                     feature_fraction=feature_fraction)\n    print(\"optimal feature_fraction: %.4f  \\t\\t Mean Absolute Error:  %.4f\" %(feature_fraction, my_mae))\n    \n# fine tuning num_leaves\nfor num_leaves in range(3,8,1):\n    my_mae = get_mae_lgb(X= OH_X_train, y=train_y,learning_rate =0.023,n_estimators=2400,\n                     feature_fraction=0.3,num_leaves=num_leaves)\n    print(\"optimal num_leaves: %.4f  \\t\\t Mean Absolute Error:  %.4f\" %(num_leaves, my_mae))\n    \n# fine tuning max_bin\nfor max_bin in range(170,190,5):\n    my_mae = get_mae_lgb(X= OH_X_train, y=train_y,learning_rate =0.023,n_estimators=2400,\n                     feature_fraction=0.3,num_leaves=5,max_bin=max_bin)\n    print(\"optimal max_bin: %.4f  \\t\\t Mean Absolute Error:  %.4f\" %(max_bin, my_mae))\n    \n# fine tuning min_data_in_leaf\nfor min_data_in_leaf in range(5,10,1):\n    my_mae = get_mae_lgb(X= OH_X_train, y=train_y,learning_rate =0.023,n_estimators=2400,\n                     feature_fraction=0.3,num_leaves=5,max_bin=175,min_data_in_leaf=min_data_in_leaf)\n    print(\"optimal min_data_in_leaf: %.4f  \\t\\t Mean Absolute Error:  %.4f\" %(min_data_in_leaf, my_mae))\n    \n# fine tuning bagging_fraction\nfor bagging_fraction in [0.36,0.37,0.38]:\n    my_mae = get_mae_lgb(X= OH_X_train, y=train_y,learning_rate =0.023,n_estimators=2400,\n                     feature_fraction=0.3,num_leaves=5,max_bin=175,min_data_in_leaf=5\n                    ,bagging_fraction=bagging_fraction)\n    print(\"optimal bagging_fraction: %.4f  \\t\\t Mean Absolute Error:  %.4f\" %(bagging_fraction, my_mae))\n    \n# fine tuning bagging_freq\nfor bagging_freq in range(5,10,1):\n    my_mae = get_mae_lgb(X= OH_X_train, y=train_y,learning_rate =0.023,n_estimators=2400,\n                     feature_fraction=0.3,num_leaves=5,max_bin=175,min_data_in_leaf=5\n                    ,bagging_fraction=0.36,bagging_freq=bagging_freq)\n    print(\"optimal bagging_freq: %.4f  \\t\\t Mean Absolute Error:  %.4f\" %(bagging_freq, my_mae))","13ac63e8":"# final lightgbm model\nlightgbm = LGBMRegressor(objective='regression',\n                          learning_rate=0.023,\n                         n_estimators=2400,\n                         feature_fraction_seed=42,\n                         feature_fraction=0.3,\n                         num_leaves=5,\n                          max_bin=175,\n                          min_data_in_leaf=5,\n                         bagging_fraction=0.36,\n                         bagging_seed=42,\n                         bagging_freq=7)","ef057404":"def get_mae_gbr(X, y,learning_rate=0.01,n_estimators=1000,max_depth=5,min_samples_leaf=15):\n    model = GradientBoostingRegressor(learning_rate=learning_rate,\n                                      n_estimators=n_estimators,\n                                max_depth=max_depth,\n                                max_features='sqrt',\n                                min_samples_leaf=min_samples_leaf,\n                                loss='huber',\n                                random_state=42)\n    scores = -1 * cross_val_score(model,X,y,cv=5,scoring='neg_mean_absolute_error')\n    return scores.mean()","dbc79463":"# fine tuning learning_rate\nfor learning_rate in [0.021,0.018]:\n    my_mae = get_mae_gbr(X= OH_X_train, y=train_y,learning_rate =learning_rate)\n    print(\"optimal learning_rate: %.4f  \\t\\t Mean Absolute Error:  %.4f\" %(learning_rate, my_mae))\n    \n# fine tuning n_estimators\nfor n_estimators in range(1750,2500,250):\n    my_mae = get_mae_gbr(X= OH_X_train, y=train_y,learning_rate =0.021,n_estimators=n_estimators)\n    print(\"optimal n_estimators: %.4f  \\t\\t Mean Absolute Error:  %.4f\" %(n_estimators, my_mae))\n    \n# fine tuning max_depth\nfor max_depth in range(4,8,1):\n    my_mae = get_mae_gbr(X= OH_X_train, y=train_y,learning_rate =0.021,n_estimators=1750,max_depth=max_depth)\n    print(\"optimal max_depth: %.4f  \\t\\t Mean Absolute Error:  %.4f\" %(max_depth, my_mae))\n    \n# fine tuning min_samples_leaf\nfor min_samples_leaf in range(12,15,1):\n    my_mae = get_mae_gbr(X= OH_X_train, y=train_y,learning_rate =0.021,n_estimators=1750,max_depth=4\n                    ,min_samples_leaf=min_samples_leaf)\n    print(\"optimal min_samples_leaf: %.4f  \\t\\t Mean Absolute Error:  %.4f\" %(min_samples_leaf, my_mae))\n","7909daad":"# final gradientboosting model\ngbr = GradientBoostingRegressor(learning_rate=0.021,\n                                n_estimators=1750,\n                                max_depth=4,\n                                max_features='sqrt',\n                                min_samples_leaf=13,\n                                loss='huber',\n                                random_state=42)\n","f99e491b":"# stacking\nstacking = StackingCVRegressor(regressors=(xgboost, lightgbm,gbr),\n                                meta_regressor=gbr,\n                                use_features_in_secondary=True)\nscores = -1 * cross_val_score(stacking,OH_X_train.values,train_y.values,cv=5,scoring='neg_mean_absolute_error')\nscores.mean()","8a00e444":"# Fitting the models\nxgb_model = xgboost.fit(OH_X_train, train_y)\n\nlgb_model = lightgbm.fit(OH_X_train, train_y)\n\ngbr_model = gbr.fit(OH_X_train, train_y)\n\nstacking_model = stacking.fit(OH_X_train.values, train_y.values)","a8bb6116":"# blending the models\n# find the optimum weights by trial and error\ndef blend_models_predict(X):\n    return ((0.1 * gbr_model.predict(X)) +\n            (0.2 * xgb_model.predict(X)) +\n            (0.1 * lgb_model.predict(X)) +\n            (0.6 * stacking_model.predict(X.values)))\n\nmean_absolute_error(valid_y,blend_models_predict(OH_X_valid))","ec112674":"#final models\nxgb_model_final = xgboost.fit(OH_X_final, y_final)\n\nlgb_model_final = lightgbm.fit(OH_X_final, y_final)\n\ngbr_model_final = gbr.fit(OH_X_final, y_final)\n\nstacking_model_final = stacking.fit(OH_X_final.values, y_final.values)","247c41ac":"#final weights for blending\ndef blend_models_predict(X):\n    return ((0.35 * xgb_model_final.predict(X)) +\n            (0.1 * lgb_model_final.predict(X)) +\n            (0.1 * gbr_model_final.predict(X)) +\n            (0.45 * stacking_model_final.predict(X.values)))","ff9a4e56":"# Get predictions\npredictions = blend_models_predict(OH_X_test)\nPredictions = pd.DataFrame({'Id': test_data.index,\n                      'SalePrice': predictions})","77b59e8b":"# Defining outlier quartile ranges\nq1 = Predictions['SalePrice'].quantile(0.005)\nq2 = Predictions['SalePrice'].quantile(0.99)\n\n# Applying weights to outlier ranges to smooth them\nPredictions['SalePrice'] = Predictions['SalePrice'].apply(\n    lambda x: x if x > q1 else x * 0.77)\nPredictions['SalePrice'] = Predictions['SalePrice'].apply(lambda x: x\n                                                        if x < q2 else x * 1.1)","a44da120":"#submission\nPredictions.to_csv('submission.csv', index=False)","1169b337":"#### Handle missing data <a class=\"anchor\" id=\"Handle_missing\"><\/a>","b1ca0f21":"#### Univariate analysis of categorical data <a class=\"anchor\" id=\"univariate_cat\"><\/a>","b940a593":"#### Tuning parameters <a class=\"anchor\" id=\"parameters\"><\/a>","abbad491":"### Feature Engineering: <a class=\"anchor\" id=\"feature_eng\"><\/a>","f965f5eb":"Following variables are highly correlated:\n1. GarageYrBlt and YearBuilt\n2. TotRmsAbvGrd and GrLivArea\n3. 1stFlrSF and TotalBsmtSF\n4. GarageArea and GarageCars","f6fa2e20":"### Exploratory Data Analysis: <a class=\"anchor\" id=\"EDA\"><\/a>","9ee6f80d":"#### Bivariate analysis <a class=\"anchor\" id=\"Bivariate\"><\/a>","c84cdbde":"Utilities,Condition2,RoofMatl have only one value","a5a2b7a9":"OverallQual and GrLivArea are strongly related with sales price","37e9799f":"#### Univariate analysis of Numerical data <a class=\"anchor\" id=\"univariate_num\"><\/a>","cfe8aa0c":"#### Outliers smoothening <a class=\"anchor\" id=\"outliers\"><\/a>","e8a76cfe":"#### Redundant features <a class=\"anchor\" id=\"redundant\"><\/a>","a9fffb4b":"#### A quick look at the data <a class=\"anchor\" id=\"quick_look\"><\/a>","175ea89a":"#### Columns with missing values <a class=\"anchor\" id=\"missing_values\"><\/a>","fc847d49":"1. MSSubclass has numerical data but its a categorical feature\n2. If yearbuilt = yearremodadd then No remodelling and additions\n3. ExterQual, Extercond, BsmtQual, BsmtCond, BsmtExposure, BsmtFintype1, BsmtFintype2, HeatingQC, KitchenQual, FireplaceQu, GarageQual, GarageCond, PoolQC,  can be changed to ordinal ratings.","1725fbb1":"#### Ordinal variables <a class=\"anchor\" id=\"ordinal\"><\/a>","f854dfb1":"#### Indicator features <a class=\"anchor\" id=\"Indicator\"><\/a>","10b2bf83":"#### Interaction features <a class=\"anchor\" id=\"interaction\"><\/a>","0477b569":"Apparant Improvisation of the predictions after a certain level is only possible by doing something new rather than tuning the same approaches. so,please let me know any ideas or approaches for enhanced results which are helpful in overall learning process rather than this specific problem.\nI need help in the following:\n1. The approach I used for finding optimum weights for blending is not working perfectly, so any other better approach for the same.\n2. Any better approach for smoothening prediction outliers.\n3. I have done parameter tuning manually as it is taking lot of time for gridsearch. any other better option for parameter tuning\n\nThank you kaggle community,\n\nCharan.","d960bc2f":"7 columns (PoolArea, MiscVal, ScreenPorch,3SsnPorch, EnclosedPorch, LowQualFinSF,BsmtFinSF2) have only one value 0 and can be removed.","7723472d":"#### Remove duplicate and irrelevant observations <a class=\"anchor\" id=\"remove\"><\/a>","1b59c9be":"### Data Cleaning: <a class=\"anchor\" id=\"Data_cleaning\"><\/a>","2de2bae0":"#### One-hot encoding <a class=\"anchor\" id=\"one-hot\"><\/a>","643cfe9e":"### Table of Contents\n\n* [Exploratory Data Analysis:](#EDA)\n    * [A quick look at the data](#quick_look)\n    * [Columns with missing values](#missing_values)\n    * [Univariate analysis of Numerical data](#univariate_num)\n    * [Univariate analysis of categorical data](#univariate_cat)\n    * [Bivariate analysis](#Bivariate)\n* [Data Cleaning:](#Data_cleaning)\n    * [Remove duplicate and irrelevant observations](#remove)\n    * [Ordinal variables](#ordinal)\n    * [Handle missing data](#Handle_missing)\n* [Feature Engineering:](#feature_eng)\n    * [Interaction features](#interaction)\n    * [Indicator features](#Indicator)\n    * [Redundant features](#redundant)\n    * [One-hot encoding](#one-hot)\n* [Data Modeling:](#modeling)\n    * [Tuning parameters ](#parameters)\n    * [Blending](#blending)\n    * [Outliers smoothening](#outliers)\n* [Final thoughts:](#final)","9624c5d9":"### Final thoughts: <a class=\"anchor\" id=\"final\"><\/a>","41eb5cc5":"#### Blending <a class=\"anchor\" id=\"blending\"><\/a>","c0c19c17":"### Data Modeling: <a class=\"anchor\" id=\"modeling\"><\/a>"}}