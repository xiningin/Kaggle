{"cell_type":{"3d6ce63a":"code","52a3e769":"code","8870b1d2":"code","d6bb653f":"code","ed729ada":"code","6bc43237":"code","40ce3d1c":"code","eba7cab1":"code","6c5cbec2":"code","da059391":"markdown","bfca8436":"markdown","ee35f5b7":"markdown","fdb7a952":"markdown","67228334":"markdown","84fc2743":"markdown","2f6e57b7":"markdown","d2be3cd9":"markdown","fc7e41ac":"markdown","89e171ea":"markdown","571916a0":"markdown","feb68086":"markdown","e8630316":"markdown","858b0824":"markdown","376fc52e":"markdown","2e69cc83":"markdown"},"source":{"3d6ce63a":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport math\nimport warnings;warnings.simplefilter('ignore')\n\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","52a3e769":"#the basic implemantation sigmod fucntion with python is as shown \ndef sigmoid(z):\n    return 1 \/ (1 + np.exp(-z))","8870b1d2":"x = np.arange(-10., 10., 0.2)\nsig = sigmoid(x)\nplt.plot(sig)\nplt.show()","d6bb653f":"#the iquivalence fucntion for the above function can be summarised as follows with python\ndef loss(y):\n        return (-y * np.log(h) - (1 - y) * np.log(1 - h)).mean() ","ed729ada":" def fit(self, X, y):\n        if self.fit_intercept:\n            X = self.add_intercept(X)\n        \n        # weights initialization\n        self.theta = np.zeros(X.shape[1])\n        \n        for i in range(self.num_iter):\n            z = np.dot(X, self.theta)\n            h = self.sigmoid(z)\n            gradient = np.dot(X.T, (h - y)) \/ y.size\n            self.theta -= self.lr * (gradient + self.reg*self.theta\/y.size)","6bc43237":"class RegularizedLogisticRegression:\n    def __init__(self, lr=0.01, num_iter=100000, fit_intercept=True, verbose=False,reg=10):\n        self.lr = lr\n        self.num_iter = num_iter\n        self.fit_intercept = fit_intercept\n        self.reg = reg\n\n    def sigmoid(self,z):\n        return 1 \/ (1 + np.exp(-z))\n\n    def add_intercept(self,X):\n        intercept=np.ones((X.shape[0], 1))\n        return np.concatenate((intercept, X), axis=1)\n\n    def loss(self,h,y):\n        return (-y * np.log(h) - (1 - y) * np.log(1 - h)).mean()    \n   \n    \n    def fit(self, X, y):\n        if self.fit_intercept:\n            X = self.add_intercept(X)\n        \n        # weights initialization\n        self.theta = np.zeros(X.shape[1])\n        \n        for i in range(self.num_iter):\n            z = np.dot(X, self.theta)\n            h = self.sigmoid(z)\n            gradient = np.dot(X.T, (h - y)) \/ y.size\n            self.theta -= self.lr * (gradient + self.reg*self.theta\/y.size)\n            \n        \n    def predict_prob(self, X):\n        if self.fit_intercept:\n            X = self.add_intercept(X)\n    \n        return self.sigmoid(np.dot(X, self.theta))\n    \n    def predict(self, X, threshold):\n        return self.predict_prob(X) >= threshold","40ce3d1c":"X_train=pd.read_csv(\"\/kaggle\/input\/X_train.csv\")\ny_train=np.array(pd.read_csv(\"\/kaggle\/input\/Y_train.csv\")).flatten()\nX_test=pd.read_csv(\"\/kaggle\/input\/X_test.csv\")\n\nmodel=RegularizedLogisticRegression()\nmodel.fit(X_train,y_train)\n\n###PREDICT THE VALUES\ny_pred= [int(round(x)) for x in model.predict(X_test,.10)]\nprint(y_pred)\n\n###GET THE ACCURACY OF THE MODEL\naccuracy=((y_pred.count(0)\/31)+(y_pred.count(1)\/31))\/2\nprint(\"Accuracy of the model is:\",accuracy*100)\n\n####EXPORT DATA INTO CSV FORMAT\nsubmission=pd.DataFrame({     \n    \"Prediction\":y_pred \n})\nsubmission.to_csv(\"y_predictions\",index=False)","eba7cab1":"from sklearn.linear_model import LogisticRegression\n\n\nX=pd.read_csv(\"\/kaggle\/input\/X_train.csv\")\ny=np.array(pd.read_csv(\"\/kaggle\/input\/Y_train.csv\")).flatten()\n","6c5cbec2":"skmodel=LogisticRegression()\nskmodel.fit(X,y)\nskmodel_score=skmodel.score(X,y)\nprint(\"Sklearn score is \",skmodel_score)","da059391":"### same data with Sklearn model","bfca8436":"cost fucntion is defined by\n\\begin{align}\nj(\\theta)=\\frac{1}{m}\\sum{(-y^Tlog(h)-(1-y)^Tlog(1-h))}\n\\end{align}\n\nthe respective gradienct function is \n\\begin{align}\n\\theta_j=\\theta_j-\\alpha.\\frac{\\partial j(\\theta_0,\\theta_1,\\theta_2,......}{\\partial \\theta_j}\n\\end{align}","ee35f5b7":"Loss minimization will be achieved by adjusting the weights of the model which I have defined as fitting function in my code. The main aspect of this method is defined in gradient descent method.","fdb7a952":"##### using the logistic regression algorithm to build a model of our own","67228334":"\\begin{align}\nh=g(X,\\theta)\\\\\nj(\\theta)=\\frac{1}{m}(-y^Tlog(h)-(1-y)^Tlog(1-h))\n\\end{align}","84fc2743":"**Advantages**<br>\n\u2713 Less prone to overfitting<br>\n\u2713 Performs well on linear data<br>\n\u2713 Easier to implement, train interpret<br><br>\n**Disadvantages**<br>\n\u2713 In real data the assumption of data being linear does not exist<br>\n\u2713 Can only be used to predict discrete features<br>","2f6e57b7":"## Putting all Together","d2be3cd9":"#### Conclusion\nIn this project, I have built a logistic regression model without the third-party packages, I have tried to improve the accuracy of the model but with comparison for SKlearn module, my accuracy score was a bit low and the running time was slower. The reason might be the SKlearn uses a highly optimized solvers and grids.\nBuilding algorithms from scratch makes it easier to think about how you could extend the algorithm to fit problems in new domains.","fc7e41ac":"sigmoid function is defined as \\begin{align} g(z)=\\frac{1}{1+e^-z}\\end{align}","89e171ea":"As a machine learning engineer,we need to understand how the ML algorithm works,that is all the mathematics,matric and linear\nalgebra involved.\nin this notebook i will try to implement a logistic regression with a provided data to classify tumor as either malignant(0)\nor Benign(0)\n","571916a0":"### Gradient Descent\nGradient descent function updates the parameters of the Logistic regression specifically weights. It is achieved by looping over steepest descent defined by the inverse of the gradient","feb68086":"## what is Logistic regression\nLogistic regression is a statistical model which predicts the probability of an outcome and producing a discrete value of 0 and 1 or True and False. The model assumes that each input to the algorithm is belongs to a difference classification.\nLogistic regression works well with the help of a probability function called logistic function commonly known as sigmoid function","e8630316":"where;\ne=Euler Number which is 2.7","858b0824":"#### Objectives<br>\n\u2713 Develop a Regularized Logistic regression<br>\n\u2713 Predict the outcome from the test data as 0 or 1<br>\n\u2713 Export the predicted values\n\n#### Data cleaning<br>\nData cleaning and wrangling is the most crucial part in dealing data for machine Learning Algorithm, here we standardize that data to be used and ensure the following.<br>\n\u2713 No Null values<br>\n\u2713 There are no N\/A columns<br>\n\u2713 Convert the feature columns into integers (Machine Learning algorithms are efficient when handling numerical values)\n","376fc52e":"#### Learning rate \nThe steps while descending the slope is referred as learning rate in the regression. It\u2019s a very critical element as one can overstep and disrupt the accuracy of the model. The running time of the model depends mostly of the learning rate, which is directly proportional to the time taken to run the algorithm.\n\n#### Stopping the gradient \nGradient descent will be terminated when weight=0, or the number of iteration are set ,eg iteration =0 Weights are usually updated subtracting the derivation of (gd)*learning rate(set=0.01)","2e69cc83":"### Loss Minimizing\nThe goal of any machine Learning model to ensure ultimate optimization of the model to increase the accuracy score. This is mostly achieved through data cleaning and feature engineering.\nIn a Logistic regression, this is achieved by the following main method.<br> 1. gradient descent<br> 2.maximum likelihood<br>\nAt any step, the algorithm analysis of the Logistic regression can be measured with the following function.Which also serves as the cost funtion"}}