{"cell_type":{"9c5cb215":"code","b00634c8":"code","360372ce":"code","e7b3e359":"code","78347307":"code","d55720e3":"code","2d25d18d":"code","f7a6ac86":"code","5e3cd543":"code","abc7914d":"code","81d7b316":"code","7acd595a":"code","f003b91d":"code","57dbe0b6":"code","dc299948":"code","41aceb4d":"code","4ff7a9bf":"code","dc6b2f16":"code","0c8af149":"code","acf8914b":"code","f9a93d94":"code","10fe403c":"code","85c61d56":"code","e154f620":"code","ad24f7d0":"code","58cda8eb":"code","32bb7804":"code","7acb8af4":"code","25964cae":"code","c687940f":"code","60d7d8d8":"code","5c85b697":"code","8280f7ad":"code","1467f8c6":"code","d1de2b64":"code","4026840d":"code","77c04669":"code","9edd2b82":"code","65ed11cd":"code","416e83cb":"code","6b0def82":"code","edb9c41e":"markdown","c52fafbb":"markdown","1b24a322":"markdown","04126542":"markdown","08987e35":"markdown","bb7b0ba5":"markdown","f4a9358a":"markdown","3baf0041":"markdown","84b4ca34":"markdown","bc68d876":"markdown","c02951ae":"markdown","64770cc9":"markdown","c6dab01b":"markdown","782e1f5b":"markdown","5a89d7a7":"markdown"},"source":{"9c5cb215":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt","b00634c8":"from scipy import spatial\nfrom nltk.corpus import stopwords","360372ce":"from nltk.tokenize import word_tokenize\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.utils.np_utils import to_categorical","e7b3e359":"from keras.layers.embeddings import Embedding\nfrom keras.layers import Flatten, LSTM, Conv1D, MaxPooling1D, Dropout, Activation\nfrom keras.layers import Input, Dense\nfrom keras.models import Sequential","78347307":"from sklearn.metrics import classification_report\nfrom sklearn import preprocessing\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.manifold import TSNE","d55720e3":"train = pd.read_csv(\"..\/input\/atis-airlinetravelinformationsystem\/atis_intents_train.csv\", header=None)\ntest = pd.read_csv(\"..\/input\/atis-airlinetravelinformationsystem\/atis_intents_test.csv\", header=None)","2d25d18d":"words = set(stopwords.words(\"english\"))","f7a6ac86":"count = 0\nfor elem in iter(words):\n    count = count + 1\n    if count == 20:\n        break\n    print (elem)","5e3cd543":"train.head()","abc7914d":"test.head()","81d7b316":"train['text'] = train[1].apply(lambda x: ' '.join([word for word in x.split() if word not in (words)]))\ntest['text'] = test[1].apply(lambda x: ' '.join([word for word in x.split() if word not in (words)]))","7acd595a":"train['text'] = train['text'].str.replace('\\d+', '')\ntest['text'] = test['text'].str.replace('\\d+', '')","f003b91d":"train","57dbe0b6":"text = train['text']\nlabels = train[0]\ntest_text = test['text']\ntest_labels = test[0]","dc299948":"labels.nunique()","41aceb4d":"from keras.preprocessing.text import Tokenizer\ntok = Tokenizer()\ntok.fit_on_texts(text)\nword_index = tok.word_index","4ff7a9bf":"word_index","dc6b2f16":"max_vocab_size = len(word_index) + 1\ninput_length = 25","0c8af149":"train_data_tokens = tok.texts_to_sequences(text)\ntest_data_tokens = tok.texts_to_sequences(test_text)","acf8914b":"train_data_tokens","f9a93d94":"train_input = pad_sequences(train_data_tokens, input_length)\ntest_input = pad_sequences(test_data_tokens, input_length)","10fe403c":"train_input","85c61d56":"label_transformer = preprocessing.LabelEncoder()\nlabel_transformer.fit(labels)","e154f620":"# from sklearn.externals import joblib\n# joblib.dump(label_transformer, 'atis-airlinetravelinformationsystem\/label_encoder.pk1')\n","ad24f7d0":"labels = label_transformer.transform(labels)\ntest_labels = label_transformer.transform(test_labels)","58cda8eb":"labels","32bb7804":"labels = to_categorical(np.asarray(labels))\ntest_labels = to_categorical(np.asarray(test_labels))","7acb8af4":"labels","25964cae":"X_train, X_val, y_train, y_val = train_test_split(train_input, labels, test_size=0.2, random_state=1)","c687940f":"X_train","60d7d8d8":"embedded_dim = 300\nembedded_index = dict()\n\nwith open('..\/input\/glove42b300dtxt\/glove.42B.300d.txt', 'r', encoding='utf-8') as glove:\n    for line in glove:\n        values = line.split()\n        word = values[0]\n        vector = np.asarray(values[1:], dtype='float32')\n        embedded_index[word] = vector","5c85b697":"glove.close","8280f7ad":"embedded_matrix = np.zeros((max_vocab_size, embedded_dim))\nfor x, i in word_index.items():\n    vector = embedded_index.get(x)\n    if vector is not None:\n        embedded_matrix[i] = vector","1467f8c6":"model = Sequential()\nmodel.add(Embedding(max_vocab_size, 300, input_length=input_length, weights=[embedded_matrix], trainable=False))","d1de2b64":"model.add(Conv1D(filters=32, kernel_size=8, activation='selu'))\nmodel.add(MaxPooling1D(pool_size=2))\nmodel.add(Flatten())\nmodel.add(Dense(10, activation='selu'))\nmodel.add(Dense(8, activation='sigmoid'))","4026840d":"model.summary()","77c04669":"model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\nmodel.fit(X_train, y_train, epochs=5, verbose=2)","9edd2b82":"model.evaluate(X_val, y_val)","65ed11cd":"def acc(y_true, y_pred):\n    return np.equal(np.argmax(y_true, axis=-1), np.argmax(y_pred, axis=-1)).mean()","416e83cb":"predictions = model.predict(test_input)","6b0def82":"print(acc(test_labels, predictions))","edb9c41e":"## Train Validation Split","c52fafbb":"# Word Embeddings - Vector Representations","1b24a322":"As words and their sequence are important for NLP solutions, pixels and their order are also essential and something valubale to keep in mind while training","04126542":"## Tokenize and Padding","08987e35":"# Data Cleaning","bb7b0ba5":"Indexed each words as there are 631 chars, words are listed to 0-631","f4a9358a":"## Stopwords Corpus","3baf0041":"## One Hot Encode with LabelEncoder","84b4ca34":"Embedded Index saves the info from pretrained GloVe model which can be later used for word embedding in terms of its\napplication to our specific model. Our embedded matrix is first matrix of zeros, and then updated according to the \nour dataset-GloVe dataset comparison.","bc68d876":"# CNN for NLP task","c02951ae":"## Digits Removal \\d+","64770cc9":"# # Thanks for reading it to the end. Credits to the OpenSourceCommunity","c6dab01b":"### Tokenized each word based off of word index","782e1f5b":"### Padded each sentence (text) to the same size of 25","5a89d7a7":"# Import Libraries"}}