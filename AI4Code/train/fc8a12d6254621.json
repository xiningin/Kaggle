{"cell_type":{"e0986db2":"code","2d506f07":"code","515e863e":"code","0a6df657":"code","40516c5d":"code","352ebb0b":"code","e8d6f029":"code","e676edfe":"code","2bf0724a":"code","11fde28d":"code","6b933a92":"code","c0546009":"code","fdd44412":"code","f8b1126c":"code","460cfbf0":"code","850d2ee0":"code","624659d5":"code","8ef4335a":"code","d87daca3":"code","dc085239":"code","395963b5":"code","fa1ee65e":"code","7cacd6bf":"code","25b31068":"code","50b68779":"code","567ae45a":"code","864bc14e":"code","0e2f8ff2":"code","b0d8e9fa":"code","7d670d03":"code","be35b7c7":"code","86fb2dfb":"code","e721af4e":"code","0218ebea":"code","b5ef3b72":"code","a62042e7":"code","2fc5e79b":"code","321aed5f":"code","aef9be67":"code","10a5ce32":"code","ac03f1a3":"code","2f614468":"code","a3acdcb1":"code","2434ff12":"code","b4b95bec":"code","be597591":"code","87e89a20":"code","a68d0750":"code","83bfbece":"code","47d0df9e":"code","b38013d5":"code","82e0118e":"code","0236eeaa":"code","d13438a2":"code","0c209d2a":"code","94c776ea":"code","9027015e":"code","dd0133ef":"code","74581087":"code","36d57ea9":"code","922e2627":"markdown","da86327c":"markdown"},"source":{"e0986db2":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","2d506f07":"typeList = [\n        'outlyingScatterPlot',\n        'skewedScatterPlot',\n        'clumpyScatterPlot',\n        'sparsedScatterPlot',\n        'striatedScatterPlot',\n        'convexScatterPlot',\n        'skinnyScatterPlot',\n        'stringyScatterPlot',\n        'monotonicScatterPlot']","515e863e":"from matplotlib import pyplot as plt","0a6df657":"import json\n# with open('\/kaggle\/input\/typicalandrealwordscags\/ScagnosticsTypicalData.json') as f:\n#     typicalData = json.load(f)\n# with open('\/kaggle\/input\/typicalandrealwordscags\/ScagnosticsTypicalData1.json') as f:\n#     typicalData1 = json.load(f)    \n# with open('\/kaggle\/input\/typicalandrealwordscags\/ScagnosticsTypicalData2.json') as f:\n#     typicalData2 = json.load(f)    \n# with open('\/kaggle\/input\/typicalandrealwordscags\/RealWorldData.json') as f:\n#     realWorldData = json.load(f)\n# with open('\/kaggle\/input\/typicalandrealwordscags\/RealWorldData10.json') as f:\n#     realWorldData10 = json.load(f)","40516c5d":"# def plotSampleDataOfType(df, numPlots, scagType):\n#     ds = np.array([d['rectangularBins'] for d in df[typeList.index(scagType)]])\n#     fig, axs = plt.subplots(1, numPlots, figsize=(20, 15))\n#     counter = 0\n#     for i in np.random.choice(range(len(ds)), numPlots):\n#         axs[counter].imshow(ds[i], cmap='hot', interpolation='nearest')\n#         axs[counter].grid(False)\n#         counter += 1\n#     plt.show()","352ebb0b":"# plotSampleDataOfType(typicalData, 10, 'outlyingScatterPlot')","e8d6f029":"# plotSampleDataOfType(typicalData, 10, 'skewedScatterPlot')","e676edfe":"# plotSampleDataOfType(typicalData, 10, 'clumpyScatterPlot')","2bf0724a":"# plotSampleDataOfType(typicalData, 10, 'sparsedScatterPlot')","11fde28d":"# plotSampleDataOfType(typicalData, 10, 'striatedScatterPlot')","6b933a92":"# plotSampleDataOfType(typicalData, 10, 'convexScatterPlot')","c0546009":"# plotSampleDataOfType(typicalData, 10, 'skinnyScatterPlot')","fdd44412":"# plotSampleDataOfType(typicalData, 10, 'stringyScatterPlot')","f8b1126c":"# plotSampleDataOfType(typicalData, 10, 'monotonicScatterPlot')","460cfbf0":"numPoints = 0 # minimum number of bins\n# X_typical = []\n# y_typical = []\n# y_typical_label = []\n# for ds in typicalData:\n#     for d in ds:\n#         # filter out invalid data\n#         if not ((np.array(d['scagnostics']) > 1).any() or (np.array(d['scagnostics']) < 0).any()) and np.sum(d['rectangularBins']) >= numPoints:\n#             X_typical.append(d['rectangularBins'])\n#             y_typical.append(d['scagnostics'])\n#             y_typical_label.append([1 if tl == d['dataSource'] else 0 for tl in typeList])\n\n# X_typical1 = []\n# y_typical1 = []\n# y_typical_label1 = []\n# for ds in typicalData1:\n#     for d in ds:\n#         # filter out invalid data\n#         if not ((np.array(d['scagnostics']) > 1).any() or (np.array(d['scagnostics']) < 0).any()) and np.sum(d['rectangularBins']) >= numPoints:\n#             X_typical1.append(d['rectangularBins'])\n#             y_typical1.append(d['scagnostics'])\n#             y_typical_label1.append([1 if tl == d['dataSource'] else 0 for tl in typeList])\n\n# X_typical2 = []\n# y_typical2 = []\n# y_typical_label2 = []\n# for ds in typicalData2:\n#     for d in ds:\n#         # filter out invalid data\n#         if not ((np.array(d['scagnostics']) > 1).any() or (np.array(d['scagnostics']) < 0).any()) and np.sum(d['rectangularBins']) >= numPoints:\n#             X_typical2.append(d['rectangularBins'])\n#             y_typical2.append(d['scagnostics'])\n#             y_typical_label2.append([1 if tl == d['dataSource'] else 0 for tl in typeList])\n            \n# X_real = []\n# y_real = []\n# for ds in realWorldData:\n#     for d in ds:\n#         if not ((np.array(d['scagnostics']) > 1).any() or (np.array(d['scagnostics']) < 0).any()) and np.sum(d['rectangularBins']) >= numPoints:\n#             X_real.append(d['rectangularBins'])\n#             y_real.append(d['scagnostics'])\n\n# X_real10 = []\n# y_real10 = []\n# for ds in realWorldData10:\n#     for d in ds:\n#         if not ((np.array(d['scagnostics']) > 1).any() or (np.array(d['scagnostics']) < 0).any()) and np.sum(d['rectangularBins']) >= numPoints:\n#             X_real10.append(d['rectangularBins'])\n#             y_real10.append(d['scagnostics'])\n\n","850d2ee0":"# # convert array type.\n# X_typical = np.array(X_typical)\n# y_typical = np.array(y_typical)\n# y_typical_label = np.array(y_typical_label)\n\n# X_typical1 = np.array(X_typical1)\n# y_typical1 = np.array(y_typical1)\n# y_typical_label1 = np.array(y_typical_label1)\n\n# X_typical2 = np.array(X_typical2)\n# y_typical2 = np.array(y_typical2)\n# y_typical_label2 = np.array(y_typical_label2)\n\n# X_real = np.array(X_real)\n# y_real = np.array(y_real)\n\n# X_real10 = np.array(X_real10)\n# y_real10 = np.array(y_real10)","624659d5":"# def plotSampleData(ds, numPlots, labels = None):\n#     fig, axs = plt.subplots(1, numPlots, figsize=(20, 15))\n#     counter = 0\n#     for i in np.random.choice(range(len(ds)), numPlots):\n#         axs[counter].imshow(ds[i], cmap='hot', interpolation='nearest')\n#         axs[counter].grid(False)\n#         if labels is not None:\n#             axs[counter].title.set_text(typeList[np.argmax(labels[i])].replace('ScatterPlot', ''))\n#         counter += 1\n#     plt.show()","8ef4335a":"# numPlots = 10\n# plotSampleData(X_typical, numPlots, y_typical_label)","d87daca3":"# numPlots = 10\n# plotSampleData(X_typical1, numPlots, y_typical_label)","dc085239":"# numPlots = 10\n# plotSampleData(X_real, numPlots)","395963b5":"# numPlots = 10\n# plotSampleData(X_real10, numPlots)","fa1ee65e":"# from sklearn.model_selection import train_test_split\n# X = np.array(X_typical2)\n# y = np.array(y_typical_label2)\n# X = X.reshape(X.shape[0], X.shape[1], X.shape[2], 1)\n# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1)","7cacd6bf":"# X_real = X_real.reshape(X_real.shape[0], X_real.shape[1], X_real.shape[2], 1)","25b31068":"# # save data\n# # dump the data too.\n# import codecs, json\n# def exportNPArrayToJSON(a, fileName):\n#     b = a.tolist() # nested lists with same data, indices\n#     json.dump(b, codecs.open(fileName, 'w', encoding='utf-8')) ### this saves the array in .json format","50b68779":"# exportNPArrayToJSON(X_train, \"X_train_cls.json\")\n# exportNPArrayToJSON(X_test, \"X_test_cls.json\")\n# exportNPArrayToJSON(y_train, \"y_train_cls.json\")\n# exportNPArrayToJSON(y_test, \"y_test_cls.json\")","567ae45a":"# # X_train = X_typical.reshape(X_typical.shape[0], X_typical.shape[1], X_typical.shape[2], 1)\n# # y_train = y_typical_label\n# # X_test = X_typical1.reshape(X_typical1.shape[0], X_typical1.shape[1],X_typical1.shape[2], 1)\n# # y_test = y_typical_label1\n\n# # X_real = X_real.reshape(X_real.shape[0], X_real.shape[1], X_real.shape[2], 1)\n\n# from keras.models import Sequential\n# from keras.layers import Dense\n# from keras.layers import Flatten\n# from keras.layers import Conv2D\n# from keras.layers import MaxPooling2D\n# from keras.layers import Dropout\n# from keras.utils import plot_model\n# from keras.regularizers import l2\n# from keras.optimizers import SGD\n\n# # new classification.\n# # learned several reasons from here: https:\/\/machinelearningmastery.com\/how-to-develop-a-cnn-from-scratch-for-cifar-10-photo-classification\/\n# def create_cnn_cls():\n#     model = Sequential()\n#     # VGG Block 1\n#     model.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same', input_shape=(40, 40, 1)))\n# #     model.add(BatchNormalization())\n#     model.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n# #     model.add(BatchNormalization())\n#     model.add(MaxPooling2D((2, 2)))\n#     model.add(Dropout(0.1))\n#     # VGG Block 2\n#     model.add(Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n# #     model.add(BatchNormalization())\n#     model.add(Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n# #     model.add(BatchNormalization())\n#     model.add(MaxPooling2D((2, 2)))\n#     model.add(Dropout(0.1))\n# #     # VGG Block 3\n# #     model.add(Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n# # #     model.add(BatchNormalization())\n# #     model.add(Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n# # #     model.add(BatchNormalization())\n# #     model.add(MaxPooling2D((2, 2)))\n# #     model.add(Dropout(0.1))\n\n# # #     model.add(Conv2D(256, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n# # # #     model.add(BatchNormalization())\n# # #     model.add(Conv2D(256, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n# # # #     model.add(BatchNormalization())\n# # #     model.add(MaxPooling2D((2, 2)))\n# # #     model.add(Dropout(0.1))\n\n    \n#     model.add(Flatten())\n#     model.add(Dense(128, activation='relu', kernel_initializer='he_uniform'))\n# #     model.add(BatchNormalization())\n#     model.add(Dropout(0.4))\n    \n#     opt = SGD(lr=0.001, momentum=0.9)\n#     model.add(Dense(9, activation='softmax'))\n#     # compile model\n#     model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n#     return model\n\n# def plotSamplePrediction(ds, numPlots, model, ds_label=None):\n#     # sample data\n#     sampled_data = []\n#     sampled_labels = []\n#     for i in np.random.choice(range(len(ds)), numPlots):\n#         sampled_data.append(ds[i])\n#         if ds_label is not None:\n#             sampled_labels.append(ds_label[i])\n#     sampled_data = np.array(sampled_data)\n#     if ds_label is not None:\n#         sampled_labels = np.array(sampled_labels)\n#     # predict class\n#     predicted_labels = model.predict_classes(sampled_data)\n#     # draw\n#     fig, axs = plt.subplots(1, numPlots, figsize=(20, 15))\n#     for counter in range(numPlots):\n#         axs[counter].imshow(sampled_data[counter].reshape(sampled_data[counter].shape[0], ds[counter].shape[1]), cmap='hot', interpolation='nearest')\n#         axs[counter].grid(False)\n#         if ds_label is not None:\n#             axs[counter].title.set_text(typeList[np.argmax(sampled_labels[counter])].replace('ScatterPlot', '') + '\/' + typeList[predicted_labels[counter]].replace('ScatterPlot', ''))\n#         else:\n#             axs[counter].title.set_text(typeList[predicted_labels[counter]].replace('ScatterPlot', ''))\n#     plt.show()\n    ","864bc14e":"# from keras.callbacks import EarlyStopping\n# from keras.callbacks import ModelCheckpoint\n# mc = ModelCheckpoint('best_model_cls.h5', monitor='val_accuracy', mode='max', save_best_only=True)\n# es = EarlyStopping(monitor='val_accuracy', mode='max', patience=50)","0e2f8ff2":"# model = create_cnn_cls()\n# # model = create_dense_model()\n# plot_model(model=model, to_file='model.png', show_shapes=True, show_layer_names=True)","b0d8e9fa":"# history = model.fit(X_train, y_train, validation_split=0.33, epochs=500, callbacks=[mc, es])","7d670d03":"# plt.plot(history.history['accuracy'])\n# plt.plot(history.history['val_accuracy'])\n# plt.title('Train\/test accuracy')\n# plt.xlabel('epochs')\n# plt.ylabel('accuracy')","be35b7c7":"# ret = model.evaluate(X_test, y_test)\n# print(rt)","86fb2dfb":"# plotSamplePrediction(X_test, 10, model, y_test)","e721af4e":"# plotSamplePrediction(X_real, 10, model)","0218ebea":"# #For real-life data.\n# from sklearn.model_selection import train_test_split\n# X_train_test = X_real.reshape(X_real.shape[0], X_real.shape[1], X_real.shape[2], 1)\n# y_train_test = y_real\n# X_train, X_test, y_train, y_test = train_test_split(X_train_test, y_train_test, test_size=0.333)\n\n# # For real-life + augmented data.\n# X_train = X_real10.reshape(X_real10.shape[0], X_real10.shape[1], X_real10.shape[2], 1)\n# y_train = y_real10\n# X_test = X_real.reshape(X_real.shape[0], X_real.shape[1], X_real.shape[2], 1)\n# y_test = y_real\n\n# # For typical data\n# X_train = X_typical.reshape(X_typical.shape[0], X_typical.shape[1], X_typical.shape[2], 1)\n# y_train = y_typical\n# X_test = X_typical1.reshape(X_typical1.shape[0], X_typical1.shape[1],X_typical1.shape[2], 1)\n# y_test = y_typical1\n\n# For both real and typical\n# from sklearn.model_selection import train_test_split\n# X = np.concatenate([X_real10, X_typical2])\n# y = np.concatenate([y_real10, y_typical2])\n# X = X.reshape(X.shape[0], X.shape[1], X.shape[2], 1)\n# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1)","b5ef3b72":"# To be consistent we keep the train\/validation\/test splits and re-train with different models.\n# We will use the ones without filtering ther number of points as of experience models learns well and discard these scatter plots from learning too. As we train with the data filtered out the points => we get the same model with the same result.\nimport json\nwith open('\/kaggle\/input\/typicalandrealwordscags\/X_train.json') as f:\n    X_train = json.load(f)\nwith open('\/kaggle\/input\/typicalandrealwordscags\/y_train.json') as f:\n    y_train = json.load(f)\nwith open('\/kaggle\/input\/typicalandrealwordscags\/X_test.json') as f:\n    X_test = json.load(f)\nwith open('\/kaggle\/input\/typicalandrealwordscags\/y_test.json') as f:\n    y_test = json.load(f)","a62042e7":"# len(X_train)","2fc5e79b":"# len(X_test)","321aed5f":"X_train = np.array(X_train)\ny_train = np.array(y_train)\nX_test = np.array(X_test)\ny_test = np.array(y_test)","aef9be67":"y_train_df = pd.DataFrame(y_train)\nscagnosticScores = [\"outlyingScore\", \"skewedScore\", \"clumpyScore\", \"sparseScore\", \"striatedScore\", \"convexScore\", \"skinnyScore\", \"stringyScore\", \"monotonicScore\"]\ny_train_df.columns = scagnosticScores\n# !pip install pandas-profiling\nimport pandas_profiling\ny_train_df.profile_report(style={'full_width': True})","10a5ce32":"y_test_df = pd.DataFrame(y_test)\nscagnosticScores = [\"outlyingScore\", \"skewedScore\", \"clumpyScore\", \"sparseScore\", \"striatedScore\", \"convexScore\", \"skinnyScore\", \"stringyScore\", \"monotonicScore\"]\ny_test_df.columns = scagnosticScores\n# !pip install pandas-profiling\nimport pandas_profiling\ny_test_df.profile_report(style={'full_width': True})","ac03f1a3":"from keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.layers import Flatten\nfrom keras.layers import Conv2D\nfrom keras.layers import MaxPooling2D\nfrom keras.layers import Dropout\nfrom keras.utils import plot_model\nfrom keras.regularizers import l2\nfrom keras.optimizers import SGD\nfrom keras.layers import BatchNormalization\n","2f614468":"# learned several reasons from here: https:\/\/machinelearningmastery.com\/how-to-develop-a-cnn-from-scratch-for-cifar-10-photo-classification\/\ndef create_cnn_model():\n    model = Sequential()\n    # VGG Block 1\n    model.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same', input_shape=(40, 40, 1)))\n#     model.add(BatchNormalization())\n    model.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n#     model.add(BatchNormalization())\n    model.add(MaxPooling2D((2, 2)))\n    model.add(Dropout(0.1))\n    # VGG Block 2\n    model.add(Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n#     model.add(BatchNormalization())\n    model.add(Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n#     model.add(BatchNormalization())\n    model.add(MaxPooling2D((2, 2)))\n    model.add(Dropout(0.1))\n    # VGG Block 3\n#     model.add(Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n# #     model.add(BatchNormalization())\n#     model.add(Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n# #     model.add(BatchNormalization())\n#     model.add(MaxPooling2D((2, 2)))\n# #     model.add(Dropout(0.1))\n\n# #     model.add(Conv2D(256, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n# # #     model.add(BatchNormalization())\n# #     model.add(Conv2D(256, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n# # #     model.add(BatchNormalization())\n# #     model.add(MaxPooling2D((2, 2)))\n# #     model.add(Dropout(0.1))\n\n    \n    model.add(Flatten())\n    model.add(Dense(128, activation='relu', kernel_initializer='he_uniform'))\n#     model.add(BatchNormalization())\n    model.add(Dropout(0.1))\n    \n    opt = SGD(lr=0.001, momentum=0.9)\n    model.add(Dense(9, activation='relu'))\n    # compile model\n    model.compile(optimizer=opt, loss='mse')\n    return model","a3acdcb1":"model = create_cnn_model()\nplot_model(model=model, to_file='model.png', show_shapes=True, show_layer_names=True)","2434ff12":"from keras.callbacks import EarlyStopping\nfrom keras.callbacks import ModelCheckpoint\nmc = ModelCheckpoint('best_model.h5', monitor='val_loss', mode='min', save_best_only=True)\nes = EarlyStopping(monitor='val_loss', mode='min', patience=50)","b4b95bec":"history = model.fit(X_train, y_train, validation_split=0.33, epochs=500, callbacks=[mc, es])","be597591":"plt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('Training\/validation loss')\nplt.ylabel('Loss')\nplt.xlabel('Epoch')\nplt.legend(['training loss', 'validation loss'], loc='upper left')","87e89a20":"# predict on the real data.\nprint(model.evaluate(X_test, y_test))","a68d0750":"y_predicted = model.predict(X_test)","83bfbece":"def plotResult(i):\n    print(typeList[i])\n    plt.figure(figsize=(15, 5))\n    sortOrder = np.argsort(y_test[:, i])\n    sortPredicted = np.array([y_predicted[:, i][idx] for idx in sortOrder])\n    plt.scatter(np.arange(len(y_test)), sorted(y_test[:,i]), label='actual')\n    plt.scatter(np.arange(len(y_predicted)), sortPredicted, label='predicted')\n    plt.legend()\n    plt.show()","47d0df9e":"plotResult(0)","b38013d5":"plotResult(1)","82e0118e":"plotResult(2)","0236eeaa":"plotResult(3)","d13438a2":"plotResult(4)","0c209d2a":"plotResult(5)","94c776ea":"plotResult(6)","9027015e":"plotResult(7)","dd0133ef":"plotResult(8)","74581087":"# # dump the data too.\n# import codecs, json\n# def exportNPArrayToJSON(a, fileName):\n#     b = a.tolist() # nested lists with same data, indices\n#     json.dump(b, codecs.open(fileName, 'w', encoding='utf-8')) ### this saves the array in .json format","36d57ea9":"# exportNPArrayToJSON(X_train, \"X_train.json\")\n# exportNPArrayToJSON(y_train, \"y_train.json\")\n# exportNPArrayToJSON(X_test, \"X_test.json\")\n# exportNPArrayToJSON(y_test, \"y_test.json\")","922e2627":"# Prediction section","da86327c":"# Classification section"}}