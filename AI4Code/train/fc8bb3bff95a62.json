{"cell_type":{"3f968c19":"code","1468b7fc":"code","31b9ca06":"code","1fce7216":"code","22388f2e":"code","285e179e":"code","9f6207c2":"code","d0ced26b":"code","a8d00355":"code","b5674bd9":"code","349e166e":"code","f060ba6e":"code","bbb36b8f":"code","9f4ce090":"code","80e06bb7":"code","2be15806":"code","c8abd7be":"code","2b160f8f":"markdown","41fb5c51":"markdown","9d319df4":"markdown","cdd22af6":"markdown","81391d6b":"markdown","676d8745":"markdown"},"source":{"3f968c19":"# IDEA: Maybe one round with full backbone fine tuning. ","1468b7fc":"# !pip install -q tensorflow-ranking\n# import tensorflow_ranking as tfr","31b9ca06":"SUBMISSION = True\n\n## Model Architecture ## \n# FEATURE_HIDDEN_LAYERS = [256, 64, 16, 4]\n# FINAL_HIDDEN_LAYERS = [4, 1]\n# HIDDEN_DROPOUT = 0.10\n\n## Model Inputs ## \n# LIST_FEATURES = ['old_lb827', 'new_lb777', 'ruddit_lb785', 'fold3_lb820', 'fold3_old_lb805']\nLIST_FEATURES = ['old_lb827', 'new_lb777', 'ruddit_lb785']\nLIST_FEATURE_DIMS = [768, 768, 768, 1024, 1024]\nMAX_SCORER_LENGTH = 96\n\nUNIT_FEATURES = ['tfidf_lb864']\nMODEL_INPUTS = UNIT_FEATURES + LIST_FEATURES\n\n## Model Training ##\nBATCH_SIZE = 8","1fce7216":"import tensorflow_addons as tfa\nimport tensorflow as tf\n\nimport transformers\nimport tokenizers\nimport datasets\n\nfrom functools import partial\nfrom tqdm.auto import tqdm\nfrom pathlib import Path\nimport tensorflow as tf\nfrom time import time\nimport pandas as pd\nimport numpy as np\nimport joblib    \nimport re\n\n# Enable Mixed Precision, JIT Compilation & set random seed\ndef _enable_mixed_precision(): \n    from tensorflow.keras.mixed_precision import experimental as mixed_precision\n    policy = mixed_precision.Policy('mixed_float16')\n    mixed_precision.set_policy(policy)\n\n# _enable_mixed_precision()\ntf.config.optimizer.set_jit(True)\ntf.random.set_seed(12)\n\n# Cache Paths\nBACKBONES_DIR = Path('..\/input\/toxic-internet-deep-model-backbones')\nWTS = Path('..\/input\/toxic-monster-model-internet')","22388f2e":"TOXIC_FEATURES = ['severe_toxic', 'identity_hate', 'threat', 'toxic', 'insult', 'obscene']","285e179e":"old = pd.read_csv('..\/input\/toxic-public-dataframes\/old_pseudo_label.csv')\ndf = pd.read_csv('..\/input\/toxic-dataframes\/valid.csv')\ndfc = pd.read_csv('..\/input\/toxic-dataframes\/comments.csv')\n\ndf = df[df.more_toxic.isin(old.comment_text) & df.less_toxic.isin(old.comment_text)]\ndfc = dfc[dfc.comment_text.isin(df.more_toxic) | dfc.comment_text.isin(df.less_toxic)] \n\nif SUBMISSION: \n    sub = pd.read_csv('..\/input\/jigsaw-toxic-severity-rating\/comments_to_score.csv')\n    sub['comment_text'] = sub.text\nelse: \n    sub = pd.read_csv('..\/input\/toxic-dataframes\/test_comments.csv')\n    sub['text'] = sub.comment_text\n    sub = sub.drop_duplicates('comment_text')\n    \n# Map Feature Values\nold_dict = old.set_index('comment_text').to_dict()\nfor feat in TOXIC_FEATURES: \n    sub[feat] = sub.comment_text.map(old_dict[feat])\n    df[f'MT_{feat}'] = df.more_toxic.map(old_dict[feat])\n    df[f'LT_{feat}'] = df.less_toxic.map(old_dict[feat])\n    \ndf","9f6207c2":"def dataset_to_test_ds(dataset): \n    'Processed huggingface dataset to tensorflow dataset'\n    dataset.set_format(type='numpy')\n    input_ids_ds = tf.data.Dataset.from_tensor_slices(dataset['input_ids'].astype(np.int32))\n    attention_mask_ds = tf.data.Dataset.from_tensor_slices(dataset['attention_mask'].astype(np.int32))\n    ds = tf.data.Dataset.zip((input_ids_ds, attention_mask_ds))\n    ds = tf.data.Dataset.zip((ds, ds))\n    return ds.batch(1024).prefetch(tf.data.AUTOTUNE)\n\ndef load_tokenizer_and_backbone(folder): \n    print('Loading tokenizer and backbone from', folder)\n    tokenizer = transformers.AutoTokenizer.from_pretrained(str(folder))\n    with tf.device('\/device:GPU:0'): \n        backbone = transformers.TFAutoModel.from_pretrained(str(folder))\n    return tokenizer, backbone","d0ced26b":"%%time\n# 8 min\n\ndef build_scorer_model(backbone): \n    input_ids = tf.keras.Input((MAX_SCORER_LENGTH,), dtype=tf.int32)\n    attention_mask = tf.keras.Input((MAX_SCORER_LENGTH,), dtype=tf.int32)\n    \n    backbone_outputs = backbone(\n        input_ids=input_ids, \n        attention_mask=attention_mask, \n        return_dict=True,\n    )\n    x = backbone_outputs.pooler_output\n    score_layer = tf.keras.layers.Dense(1, activation='sigmoid')\n    return tf.keras.Model(inputs=[input_ids, attention_mask], outputs=score_layer(x))\n\nwith tf.device('\/device:GPU:0'): \n    robertaB_tokenizer, robertaB_backbone = load_tokenizer_and_backbone(BACKBONES_DIR\/'roberta_base')\n    robertaL_tokenizer, robertaL_backbone = load_tokenizer_and_backbone(BACKBONES_DIR\/'roberta_large')\n\nraw_dataset = datasets.Dataset.from_pandas(dfc)\nprocessed_dataset = raw_dataset.map(\n    lambda ex: robertaB_tokenizer(ex['comment_text'], max_length=MAX_SCORER_LENGTH, padding='max_length', truncation=True), \n    batched=True, num_proc=4,\n)\ndf_ds = dataset_to_test_ds(processed_dataset) \nraw_dataset = datasets.Dataset.from_pandas(sub)\nprocessed_dataset = raw_dataset.map(\n    lambda ex: robertaB_tokenizer(ex['text'], max_length=MAX_SCORER_LENGTH, padding='max_length', truncation=True), \n    batched=True, num_proc=4,\n)\nsub_ds = dataset_to_test_ds(processed_dataset) \n\ndef predict(backbone, ds):\n    preds = backbone.predict(ds, verbose=1).pooler_output.astype(np.float32)\n    return np.squeeze(preds)\n\nwith tf.device('\/device:GPU:0'): \n    model = build_scorer_model(robertaB_backbone)\n    \n    model.load_weights(WTS\/'old_pseudo_label.h5')\n    dfc_old_lb827 = predict(robertaB_backbone, df_ds)\n    sub_old_lb827 = predict(robertaB_backbone, sub_ds)\n    \n    model.load_weights(WTS\/'2019val737_robertab_scorer.h5')\n    dfc_new_lb777 = predict(robertaB_backbone, df_ds)\n    sub_new_lb777 = predict(robertaB_backbone, sub_ds)\n    \n    model.load_weights(WTS\/'ruddit_val731_robertab.h5')\n    dfc_ruddit_lb785 = predict(robertaB_backbone, df_ds)\n    sub_ruddit_lb785 = predict(robertaB_backbone, sub_ds)\n    \n    import gc\n    del robertaB_backbone; gc.collect()\n#     model = build_scorer_model(robertaL_backbone)\n    \n#     model.load_weights(WTS\/'comp_only_fold3_val737_robertal_sentiment_10thDec.h5')\n#     dfc_fold3_lb820 = predict(robertaL_backbone, df_ds)\n#     sub_fold3_lb820 = predict(robertaL_backbone, sub_ds)\n    \n#     model.load_weights(WTS\/'fold3_old_then_comp_loss571_val750_ep1_roberta_sent_12thDec.h5')\n#     dfc_fold3_old_lb805 = predict(robertaL_backbone, df_ds)\n#     sub_fold3_old_lb805 = predict(robertaL_backbone, sub_ds)\n    \npipeline = joblib.load('..\/input\/toxic-dataframes\/pipeline_lb864.pkl')\ndfc['tfidf_lb864'] = pipeline.predict(dfc.comment_text)\nsub['tfidf_lb864'] = pipeline.predict(sub.text)","a8d00355":"dfc_comment_to_i = {c: i for i, c in enumerate(dfc.comment_text.values)}\nsub_comment_to_i = {c: i for i, c in enumerate(sub.text.values)}\n\nA, B = df.copy(), df.copy()\n\nA['A_comment'], A['B_comment'] = A.more_toxic, A.less_toxic\nB['A_comment'], B['B_comment'] = B.less_toxic, B.more_toxic\ndfc_dict = dfc.set_index('comment_text').to_dict()\nfor feat in UNIT_FEATURES: \n    df[f'MT_{feat}'] = df.more_toxic.map(dfc_dict[feat])\n    df[f'LT_{feat}'] = df.less_toxic.map(dfc_dict[feat])\n    \n    A[f'A_{feat}'], A[f'B_{feat}'], A['y'] = df[f'MT_{feat}'], df[f'LT_{feat}'], 0.0\n    B[f'A_{feat}'], B[f'B_{feat}'], B['y'] = df[f'LT_{feat}'], df[f'MT_{feat}'], 1.0\n    \n\ndf_temp = pd.concat([A, B])\ndf_temp['A_i'] = df_temp.A_comment.map(dfc_comment_to_i)\ndf_temp['B_i'] = df_temp.B_comment.map(dfc_comment_to_i)\ntrain, valid = df_temp[df_temp.fold!=3], df_temp[df_temp.fold==3]","b5674bd9":"%%time\ndef df_to_tfds(df, is_train=False): \n    col_to_values = {\n        'old_lb827': dfc_old_lb827,\n        'new_lb777': dfc_new_lb777,\n        'ruddit_lb785': dfc_ruddit_lb785,\n        # 'fold3_lb820': dfc_fold3_lb820,\n        # 'fold3_old_lb805': dfc_fold3_old_lb805,\n    }\n    inputs = []\n    for feat in UNIT_FEATURES: \n        inputs.append(tf.data.Dataset.from_tensor_slices(df[f'A_{feat}'].values))\n    for feat in LIST_FEATURES: \n        x = col_to_values[feat][df.A_i]\n        inputs.append(tf.data.Dataset.from_tensor_slices(x))\n    for feat in UNIT_FEATURES: \n        inputs.append(tf.data.Dataset.from_tensor_slices(df[f'B_{feat}'].values))\n    for feat in LIST_FEATURES:\n        x = col_to_values[feat][df.B_i]\n        inputs.append(tf.data.Dataset.from_tensor_slices(x))\n    input_ds = tf.data.Dataset.zip(tuple(inputs))\n    \n    label_ds = tf.data.Dataset.from_tensor_slices(df.y.values)\n    ds = tf.data.Dataset.zip((input_ds, label_ds))\n    if is_train: \n        ds = ds.shuffle(len(df), reshuffle_each_iteration=True).repeat()\n    ds = ds.batch(BATCH_SIZE)\n    if not is_train: \n        ds = ds.cache()\n    steps = len(df)\/\/BATCH_SIZE\n    return ds.prefetch(tf.data.AUTOTUNE), steps\n\n\ndef test_df_to_tfds(df): \n    col_to_values = {\n        'old_lb827': sub_old_lb827,\n        'new_lb777': sub_new_lb777,\n        'ruddit_lb785': sub_ruddit_lb785,\n        # 'fold3_lb820': sub_fold3_lb820,\n        # 'fold3_old_lb805': sub_fold3_old_lb805,\n    }\n    \n    inputs = []\n    for feat in UNIT_FEATURES: \n        inputs.append(tf.data.Dataset.from_tensor_slices(df[feat].values))\n    for feat in LIST_FEATURES: \n        x = col_to_values[feat]\n        inputs.append(tf.data.Dataset.from_tensor_slices(x))\n    ds = tf.data.Dataset.zip(tuple(inputs))\n    ds = tf.data.Dataset.zip((ds, ds))\n    ds = ds.batch(1024)\n    return ds.prefetch(tf.data.AUTOTUNE)\n\n\ntrain_ds, train_steps = df_to_tfds(train, is_train=True)\nvalid_ds, valid_steps = df_to_tfds(valid, is_train=False)\ntest_ds = test_df_to_tfds(sub)","349e166e":"def build_hidden_layer(hidden_layer_units, hidden_dropout, name='hidden_layer'): \n    layers = []\n    for units in hidden_layer_units: \n        layers.append(tf.keras.layers.Dropout(hidden_dropout))\n        layers.append(tf.keras.layers.Dense(\n            units, \n            activation=tfa.activations.mish, \n        ))\n    return tf.keras.Sequential(layers, name=name)\n\ndef build_model(): \n    A_unit_inputs, B_unit_inputs = [], []\n    A_unit_embeddings, B_unit_embeddings = [], []\n    for unit_feat in UNIT_FEATURES:\n        A_unit_input = tf.keras.Input(shape=(1,), dtype=tf.float32, name=f'A_{unit_feat}')\n        B_unit_input = tf.keras.Input(shape=(1,), dtype=tf.float32, name=f'B_{unit_feat}')\n        \n        A_unit_inputs.append(A_unit_input)\n        B_unit_inputs.append(B_unit_input)\n        \n        U = tf.keras.layers.Dense(1, activation=tfa.activations.mish, name=unit_feat)\n        A_unit_embeddings.append(U(A_unit_input))\n        B_unit_embeddings.append(U(B_unit_input))\n        \n    \n    A_list_inputs, B_list_inputs = [], []\n    A_list_embeddings, B_list_embeddings = [], []\n    for list_feat, dim in zip(LIST_FEATURES, LIST_FEATURE_DIMS):\n        A_list_input = tf.keras.Input(shape=(dim,), dtype=tf.float32, name=f'A_{list_feat}')\n        B_list_input = tf.keras.Input(shape=(dim,), dtype=tf.float32, name=f'B_{list_feat}')\n        \n        A_list_inputs.append(A_list_input)\n        B_list_inputs.append(B_list_input)\n        \n        L = build_hidden_layer(LIST_FEATURE_HIDDEN_LAYERS, HIDDEN_DROPOUT, name=list_feat)\n        A_list_embeddings.append(L(A_list_input))\n        B_list_embeddings.append(L(B_list_input))\n\n    A_x = tf.concat(A_unit_embeddings+A_list_embeddings, axis=-1)\n    B_x = tf.concat(B_unit_embeddings+B_list_embeddings, axis=-1)\n    x = tf.concat([A_x, B_x], axis=-1)\n    \n    H = build_hidden_layer(FINAL_HIDDEN_LAYERS, HIDDEN_DROPOUT, name='final_hidden_layer')\n    x = H(x)\n    x = tf.keras.layers.Dense(1, activation='sigmoid', name='y')(x)\n    return tf.keras.Model(A_unit_inputs+A_list_inputs+B_unit_inputs+B_list_inputs, outputs=x)\n\ndef optimizer_factory(lr):\n    optimizer = tf.keras.optimizers.Adam(learning_rate=lr)\n    optimizer = tfa.optimizers.SWA(optimizer)\n    return optimizer\n\ndef loss_fn(y_true, y_pred): \n    y_true, y_pred = tf.cast(y_true, tf.float32), tf.cast(y_pred, tf.float32)\n    \n    # return tfr.keras.losses.PairwiseHingeLoss()(y_true, y_pred)\n    return tf.keras.losses.BinaryCrossentropy()(y_true, y_pred)\n    # return tfr.keras.losses.PairwiseLogisticLoss(temperature=1.0)(y_true, y_pred)\n\ndef model_compile(): \n    optimizer = optimizer_factory(1e-2)\n    model.compile(\n        optimizer=optimizer, \n        loss=loss_fn, \n        metrics='accuracy', \n        steps_per_execution=1024, \n    )    \n\nLIST_FEATURE_HIDDEN_LAYERS = [16]\nFINAL_HIDDEN_LAYERS = [256, 64, 32, 16, 4, 2]\nHIDDEN_DROPOUT = 0.25\n\n\nwith tf.device('\/device:GPU:0'): \n    model = build_model()\n    model_compile()\n    \nreduce_lr_on_plateau = tf.keras.callbacks.ReduceLROnPlateau(\n    monitor='val_accuracy', factor=0.5, patience=5, verbose=1, mode='max', min_lr=1e-6, \n)\naccuracy_checkpoint = tf.keras.callbacks.ModelCheckpoint(\n    'checkpoint_acc.h5', monitor='val_accuracy', mode='max', save_weights_only=True, save_best_only=True, verbose=1\n)\n\nhistory = model.fit(\n    train_ds, steps_per_epoch=train_steps, epochs=100, \n    validation_data=valid_ds, validation_steps=valid_steps, \n    callbacks=[reduce_lr_on_plateau, accuracy_checkpoint]\n)","f060ba6e":"# 0.744 without [4], None (No Fine Tuned Layers)","bbb36b8f":"def extract_features(model): \n    unit_inputs, unit_embeddings = [], []\n    for unit_feat in UNIT_FEATURES: \n        unit_input = tf.keras.Input(shape=(1,), dtype=tf.float32, name=f'{unit_feat}_input')\n        unit_embeddings.append(model.get_layer(unit_feat)(unit_input))\n        unit_inputs.append(unit_input)\n        \n    list_inputs, list_embeddings = [], []\n    for list_feat, dim in zip(LIST_FEATURES, LIST_FEATURE_DIMS): \n        list_input = tf.keras.Input(shape=(dim,), dtype=tf.float32, name=f'{list_feat}_input')\n        L = model.get_layer(list_feat)\n        list_embeddings.append(L(list_input))\n        list_inputs.append(list_input)\n\n    x = tf.concat(unit_embeddings+list_embeddings, axis=-1)\n    return tf.keras.Model(unit_inputs+list_inputs, outputs=x)\n\n\nwith tf.device('\/device:GPU:0'): \n    model.load_weights('checkpoint_acc.h5')\n    feature_model = extract_features(model)\n    test_x = feature_model.predict(test_ds, verbose=1)","9f4ce090":"def scorer_model(model): \n    x_dim = len(UNIT_FEATURES) + len(LIST_FEATURES)*LIST_FEATURE_HIDDEN_LAYERS[-1]\n    A_x = tf.keras.Input(shape=(x_dim,), dtype=tf.float32, name='A_x')\n    B_x = tf.keras.Input(shape=(x_dim,), dtype=tf.float32, name='B_x')\n    x = tf.concat([A_x, B_x], axis=-1)\n    \n    H = model.get_layer('final_hidden_layer')\n    x = H(x)\n    score_layer = model.get_layer('y')\n    x = score_layer(x)\n    return tf.keras.Model(inputs=[A_x, B_x], outputs=x)\n\nwith tf.device('\/device:GPU:0'): \n    scorer = scorer_model(model)","80e06bb7":"SPEED_UP = 2\n\ndef build_test_ds(i):\n    B_x = test_x[np.arange(0, len(sub), SPEED_UP)]\n    A_x = np.stack([test_x[i] for _ in range(len(B_x))])\n    \n    A_ds = tf.data.Dataset.from_tensor_slices(A_x)\n    B_ds = tf.data.Dataset.from_tensor_slices(B_x)\n    ds = tf.data.Dataset.zip((A_ds, B_ds))\n    ds = tf.data.Dataset.zip((ds, A_ds))\n    ds = ds.batch(4096*4)\n    return ds.prefetch(tf.data.AUTOTUNE)\n\nsub_scores = []\nfor i in tqdm(range(len(sub))): \n    test_ds = build_test_ds(i)\n    scores = scorer.predict(test_ds, verbose=0)\n    sub_scores.append(scores.mean())\n    \nsub['score'] = sub_scores","2be15806":"sub.score = -sub.score\nsub.score = sub.score.rank(method='first')\nsub[['comment_id', 'score']].to_csv('submission.csv', index=False)\n\nsub.sort_values(by='score')","c8abd7be":"# sub.sort_values(by='tfidf_lb864')","2b160f8f":"## Inference\n---","41fb5c51":"## \u2692 Data Factory\n---","9d319df4":"## Embedding Extractor\n---","cdd22af6":"### Notebook Imports & Setup","81391d6b":"# Toxic: TF Ranking\n---\n","676d8745":"## Train Cross Encoder"}}