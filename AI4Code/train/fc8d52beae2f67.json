{"cell_type":{"b06a36bb":"code","3d976e13":"code","e55b1e12":"code","bfb5808f":"code","8551aa0a":"code","017aad16":"code","e4f937d4":"code","66005ecf":"code","b9e90e0f":"code","c8b5fa6b":"code","0feca3be":"code","4035ee5a":"code","01d8825b":"code","7d443fbf":"code","2a251b2c":"code","ffc04e2c":"code","4903f30b":"code","7ac3ac8f":"code","fcc226cd":"markdown","b9ba63aa":"markdown","e8343c0e":"markdown","c8ac4c40":"markdown","59ca1f10":"markdown","24bf41b6":"markdown","9c6fcb0b":"markdown","ea7fb5bf":"markdown","48967df7":"markdown","d26f25c5":"markdown","071a0e60":"markdown","ca0aa17f":"markdown","1bf5d9c3":"markdown","7543061a":"markdown"},"source":{"b06a36bb":"# Libraries\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport pylab as pl\nimport matplotlib.pyplot as plt\nimport scipy.optimize as opt\nfrom sklearn import preprocessing\nfrom sklearn.model_selection import train_test_split, KFold, cross_val_score, RepeatedStratifiedKFold\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import confusion_matrix, classification_report, log_loss\nfrom sklearn.metrics import plot_confusion_matrix\nfrom sklearn.metrics import accuracy_score\n%matplotlib inline\n\nfrom collections import Counter\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","3d976e13":"rpt = pd.read_csv('..\/input\/heart-disease-prediction-using-logistic-regression\/framingham.csv')\nrpt[20:41]","e55b1e12":"# Checkig datatypes of each column and shape\nprint(rpt.dtypes, '\\n')\nprint(\"Shape: \", rpt.shape)\n\n# Checking for NaN and correcting\nprint(\"NaN exists: \", rpt.isnull().values.any())\nprint(\"NaN count: \", rpt.isnull().values.sum())\nprint(\"NaN count for each attriue:\")\nprint(rpt.isnull().sum())\nprint(\"\\n\")\n\n# Dropping all rows with any NaN\nrpt.dropna(axis = 0, inplace = True)\nprint(\"New Shape after dropping rows: \", rpt.shape)\nprint(\"\\n\")","bfb5808f":"# Depictong Correlations\nrpt.corr()","8551aa0a":"# Creating X and y\nX = np.asarray(rpt[['male', 'age', 'cigsPerDay', 'prevalentStroke', 'prevalentHyp', 'diabetes', 'sysBP', 'diaBP', 'BMI', 'heartRate', 'glucose']])\ny = np.asarray(rpt['TenYearCHD']) #do not do [['TenYearCHD']] it will give shape (2924,1) instead of needed (2924,)\n\n# Normalizing X\nX = preprocessing.StandardScaler().fit_transform(X)\nprint(\"Shape X:\", X.shape)\nprint(\"Shape y:\", y.shape)","017aad16":"def visualizeIn2D(X, y):\n    '''\n    Visualizing Data in 2 dim using features: sysBP, diaBP. \n    '''\n    # Class labels in a dict\n    counter = Counter(y)\n    print(counter)\n    \n    plt.figure(figsize = (15, 10))\n    for label, _ in counter.items():\n        row = np.where(y == label)[0]\n        plt.scatter(X[row, 0], X[row, 1], label=str(label))\n    plt.legend()\n    plt.show()","e4f937d4":"# Calculating number of classes\nNumOfClasses = len(rpt.groupby('TenYearCHD').size().values)\n\n#Creating new X1 because X is already ndarray and normalized\nX1 = np.asarray(rpt[['sysBP', 'diaBP']])\n\n# Visualize\nvisualizeIn2D(X1, y)","66005ecf":"def Ktimes_train_LR(LR, kf, X, y):\n    '''\n    Since we are not using train_test_split where say 80% was used for training and 20% was used for testing,\n    in KFold our training is done on k-1 sets and testing is done on 1 set. So if n_sets equals 5, then 4 sets\n    (or 80%) are used for training and 1 set (or 20%) is used for testing.\n    '''\n    scores = list()\n    \n    for train_index, test_index in kf.split(X):\n        \n        # Splitting Data into train test\n        X_train, X_test = X[train_index], X[test_index]\n        y_train, y_test = y[train_index], y[test_index]\n        \n        # Train\n        LR.fit(X_train, y_train)\n        \n        # Predicting\n        y_hat = LR.predict(X_test)\n        y_hat_prob = LR.predict_proba(X_test)\n        \n        # Scoring\n        scores.append(accuracy_score(y_test, y_hat))\n        \n    '''\n    The y_hat and y_hat_proba returned below hold the values from the \"LAST\" iteration for KFold training.\n    '''\n    report = (X_train, X_test, y_train, y_test, y_hat, y_hat_prob, scores)\n    return report\n\n\n# Init the main Logistic model - the lesser the C, the greater the regularization \nLR = LogisticRegression(C = 0.0000001, solver = 'liblinear')\n\n# Init KF\nkf = KFold(n_splits = 5, random_state = 7)\n\n# Call for K times training and predicting\n(X_train, X_test, y_train, y_test, y_hat, y_hat_prob, scores) = Ktimes_train_LR(LR, kf, X, y)\n\n# Priting the jaccard_scores\nprint(\"Scores: \", scores)","b9e90e0f":"'''\nThe above block of code and these 3 lines of code below do exactly the same work, except that the we do not\nhave access to (or do not need) X_train, X_test, y_train, y_test, y_hat, y_hat_prob. We just get back scores.\nOf course, we can always split the data using other functions like ShuffleSplit or train_test_split to get\naccess of X_train, X_test and so on.\n'''\nkfold = KFold(n_splits = 5, random_state = 7)\ncv_result = cross_val_score(LR, X, y, cv = kfold, scoring = 'accuracy')\nprint(cv_result)","c8b5fa6b":"# Plotting the confusion matrix\nconfusion_matrix(y_test, y_hat, labels = [1, 0])\nplot_confusion_matrix(LR, X_test, y_test, labels = [1, 0], cmap=plt.cm.Blues) #test acc\nplot_confusion_matrix(LR, X_train, y_train, labels = [1, 0], cmap=plt.cm.Blues) #train acc","0feca3be":"# Classification Report\nprint(\"\\t\\t\\t *TEST REPORT*\")\nprint(classification_report(y_test, y_hat)) #test acc\nprint('\\n')\nprint(\"\\t\\t\\t *TRAIN REPORT*\")\nprint(classification_report(y_train, LR.predict(X_train))) #train acc\n\n# Log-loss\nprint(\"LogLoss: \", log_loss(y_hat, y_hat_prob))","4035ee5a":"# Checking Label Imballance\nprint(\"Label Imballance:\")\nprint(rpt.groupby('TenYearCHD').size())\nprint(\"\\n\")\n\n# Checking Gender Discrepancy \nprint(\"Gender Distribution:\")\nprint(rpt.groupby('male').size())","01d8825b":"# Import Additional Libraries\nfrom imblearn.pipeline import Pipeline\nfrom imblearn.over_sampling import SMOTE\nfrom imblearn.under_sampling import RandomUnderSampler","7d443fbf":"# Defining Pipeline\nover = SMOTE(sampling_strategy = 0.4, k_neighbors = 3, random_state = 7)\nunder = RandomUnderSampler(sampling_strategy = .55)\nsteps = [('o', over), ('u', under), ('model', LR)]\npipe1 = Pipeline(steps = steps)\n\n# Training and Evaluating\ncv_result = cross_val_score(pipe1, X, y, cv = kfold, scoring = 'accuracy')\nprint(\"Mean Accuracy: \", np.mean(cv_result))\nprint('\\n')\n\npipe = Pipeline(steps =[('o', over), ('u', under)])\nX2, y1 = pipe.fit_resample(X1, y)\ncounter = Counter(y1)\nprint(counter)\n\n# Call for K times training and predicting\n(X_train, X_test, y_train, y_test, y_hat, y_hat_prob, scores) = Ktimes_train_LR(pipe1, kf, X, y)\nplot_confusion_matrix(pipe1, X_test, y_test, labels = [1, 0], cmap=plt.cm.Blues) #test acc\nvisualizeIn2D(X2, y1)","2a251b2c":"print(classification_report(y_test, y_hat)) #test acc","ffc04e2c":"from sklearn.preprocessing import binarize\n\ncm2 = 0\ny_hat2 = binarize(y_hat_prob,0.4999856)[:,1]\ncm2 = confusion_matrix(y_test,y_hat2)\nprint ('With',0.4999856,'threshold the Confusion Matrix is ','\\n',cm2,)\nprint(classification_report(y_test, y_hat2)) ","4903f30b":"from sklearn.metrics import roc_curve, roc_auc_score\nfpr, tpr, _ = roc_curve(y_test, y_hat_prob[:,1])\nplt.plot(fpr,tpr)\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.0])\nplt.title('Heart Disease Predictor (ROC curve)')\nplt.xlabel('False positive rate')\nplt.ylabel('True positive rate')\nplt.grid(True)","7ac3ac8f":"print(\"Area under curve: \", roc_auc_score(y_test,y_hat_prob[:,1]))","fcc226cd":"### From the new report above, it is clear that:\n* ### Our f1_score for true postive (1) has increased from 0.31 to 0.35. \n* ### This has come at a cost of compromizing f1_Score for true negative (0) from 0.88 to 0.78\n* ### But our True Positive is still very poor, we care for it more than True Negartive Rate. ","b9ba63aa":"# Heart Disease Classification using Logistic Regression\n## We use logistic regression for this problem because we don't just need class labels but also the confidence percentage behind those predicted class labels. Our current problem exactly fits these needs.","e8343c0e":"### Since the logloss error is high (i.e. close to 69%) and f1_scores of accuracy for label 1 are very low (i.e. close to 30% compared to label 0's 88%), we need to do some more preprocessing on the dataset. The technique we will use is called SMOTE (discussed later).\n","c8ac4c40":"### From the new report, it is clear that:\n* ### Our f1_score for true postive (1) has further increased from 0.35 to 0.38. \n* ### We need more data to improve this result.","59ca1f10":"# Visualization","24bf41b6":"# Preprocessing","9c6fcb0b":"# Applying SMOTE","ea7fb5bf":"### Finally, our Area Under Curve (AUC) shows that if we want to increase \"true positie rate\" (detecting patients who truly have a heart disease) we also end having large \"false positive rate\" which is problematic (many more patients who are actually healthy end up getting diagnosed with a heart disease). This scenario makes it infeasible for deployment in a real-world setting at the moment. More data and less label imballance can help here.","48967df7":"# Build Logistic Regression Model","d26f25c5":"# Tinkering with threshold value","071a0e60":"# How Smote Works?\n### SMOTE picks K-Nearest Neighbours (usually 5) at random in the feature space and draws a line between any two points from KNNs. Finally it picks, a new sample at a point along that line.\n\n>  *The simplest approach involves duplicating examples in the minority class, although these examples don\u2019t add any new information to the model. Instead, new examples can be synthesized from the existing examples. This is a type of data augmentation for the minority class and is referred to as the Synthetic Minority Oversampling Technique, or SMOTE for short.\" ~[Jason Brownlee](http:\/\/https:\/\/machinelearningmastery.com\/smote-oversampling-for-imbalanced-classification\/)*","ca0aa17f":"# Load the dataset","1bf5d9c3":"# Model Evaluation","7543061a":"### It seems that our data has a lot of label imballance, cases for label 0 are 3000 vs 550 for label 1. This means that our model learns features of label 0 too well but for label 1 not so much. So what's the solution? \n\n### We can do many things. \n\n1. ### Add synthetic cases for mnority class to our dataset\n2. ### Reduce size of minority class by throwing out some cases\n3. ### Do both task 1 and 2 *(SMOTE Technique)*\n4. ### Since we are using Logistic Regression, change the threshold for y_hat_prob"}}