{"cell_type":{"bd5e98b3":"code","f3aadc1d":"code","2e26f942":"code","484680f8":"code","349b14db":"code","141adfc3":"code","b509d736":"code","df01d760":"code","c469ada0":"code","6ece3e2c":"code","9a208b91":"code","ed922eb4":"code","f0c84d38":"code","99c14b40":"code","367d70e2":"code","b82170dd":"code","a379c24e":"code","a8126e77":"code","da4ddfd0":"code","ec3224e1":"code","eab37fca":"code","51775fe9":"code","b31dd472":"code","78ff6a47":"code","42738f2f":"code","41d5b845":"code","8420d2ed":"code","4f5ba635":"code","4a1bfded":"code","4a2bc0b2":"code","dfd2cc94":"code","3dca88fe":"markdown","1c458fad":"markdown","b97e71f8":"markdown","8211d4dc":"markdown","8bcd65b5":"markdown","6f37cc07":"markdown","7d10a6f0":"markdown","07d63d30":"markdown","5a833e6a":"markdown","c4dbb540":"markdown","f4b6799b":"markdown","95618439":"markdown","88c6fe1a":"markdown","2ef557e2":"markdown","edbd6e5f":"markdown","b7d21aae":"markdown","0bc46cc3":"markdown","a8e3291c":"markdown","d5ddbfa6":"markdown","b411bbe6":"markdown","0407dd84":"markdown","0d9d59dc":"markdown","05bd8b7b":"markdown","34257a2b":"markdown","1eaa9d55":"markdown","71d1ec64":"markdown","86bc9190":"markdown","d02c69ec":"markdown","00ff18ca":"markdown"},"source":{"bd5e98b3":"import pandas as pd, sklearn, numpy as np, os\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.gaussian_process import GaussianProcessClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.ensemble import VotingClassifier\nfrom sklearn.metrics import accuracy_score, precision_score, make_scorer","f3aadc1d":"data_folder = '..\/input'\ntrain = pd.read_csv(os.path.join(data_folder, 'train.csv'))\ntest = pd.read_csv(os.path.join(data_folder, 'test.csv'))\nn_train, m_train = train.shape","2e26f942":"train.head()","484680f8":"train.info()","349b14db":"train.describe()","141adfc3":"from sklearn.model_selection import train_test_split\n\ndef drop_unused_columns(df):\n    return df.drop(['PassengerId', 'Cabin', 'Ticket', 'Embarked'], axis=1)\n\ndef to_features_and_labels(df):\n    y = df['Survived'].values\n    X = drop_unused_columns(df)\n    X = X.drop('Survived', axis=1)\n    return X, y\n\nX_train_val, y_train_val = to_features_and_labels(train) # All data with labels, to be split into train and val\nX_test = drop_unused_columns(test)\n\n# Split the available training data into training set (used for choosing the best model) \n# and validation set (used for estimating the generalization error, could also be called \"hold-out\" set)\nX_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=0.20, random_state=42)\nX_train.head()","b509d736":"from sklearn.base import BaseEstimator, TransformerMixin\nclass DataFrameColumnMapper(BaseEstimator, TransformerMixin):\n    def __init__(self, column_name, mapping_func, new_column_name=None):\n        self.column_name = column_name\n        self.mapping_func = mapping_func\n        self.new_column_name = new_column_name if new_column_name is not None else self.column_name\n    def fit(self, X, y=None):\n        # Nothing to do here\n        return self\n    def transform(self, X):\n        transformed_column = X.transform({self.column_name: self.mapping_func})\n        Y = X.copy()\n        Y = Y.assign(**{self.new_column_name: transformed_column})\n        if self.column_name != self.new_column_name:\n            Y = Y.drop(self.column_name, axis=1)\n        return Y\n\n# Return a lambda function that extracts title from the full name, this allows instantiating the pattern only once\ndef extract_title():\n    import re\n    pattern = re.compile(', (\\w*)')\n    return lambda name: pattern.search(name).group(1)\n\n# Example usage and output \ndf = DataFrameColumnMapper(column_name='Name', mapping_func=extract_title(), new_column_name='Title').fit_transform(X_train)\ndf.head()","df01d760":"df['Title'].value_counts()[1:10]","c469ada0":"class CategoricalTruncator(BaseEstimator, TransformerMixin):\n    def __init__(self, column_name, n_values_to_keep=5):\n        self.column_name = column_name\n        self.n_values_to_keep = n_values_to_keep\n        self.values = None\n    def fit(self, X, y=None):\n        # Here we must ensure that the test set is transformed similarly in the later phase and that the same values are kept\n        self.values = list(X[self.column_name].value_counts()[:self.n_values_to_keep].keys())\n        return self\n    def transform(self, X):\n        transform = lambda x: x if x in self.values else 'Other'\n        y = X.transform({self.column_name: transform})\n        return X.assign(**{self.column_name: y})\n\n# Print title counts\ntitle_counts = CategoricalTruncator('Title', n_values_to_keep=3).fit_transform(df)['Title'].value_counts()\ntitle_counts","6ece3e2c":"from sklearn.pipeline import Pipeline\n\npipeline = Pipeline([\n    ('name_to_title', DataFrameColumnMapper(column_name='Name', mapping_func=extract_title(), new_column_name='Title')),\n    ('truncate_titles', CategoricalTruncator('Title', n_values_to_keep=3))\n])\n\ndf = pipeline.fit_transform(X_train)\ndf.head(10)","9a208b91":"class ImputerByReference(BaseEstimator, TransformerMixin):\n    def __init__(self, column_to_impute, column_ref):\n        self.column_to_impute = column_to_impute\n        self.column_ref = column_ref\n        # TODO Allow specifying the aggregation function\n        # self.impute_func = np.median if impute_type == 'median' or impute_type is None else np.mean\n    def fit(self, X, y=None):\n        # Pick columns of interest\n        df = X.loc[:, [self.column_to_impute, self.column_ref]]\n        # Dictionary containing mean per group\n        self.value_per_group = df.groupby(self.column_ref).median().to_dict()[self.column_to_impute]\n        return self\n    def transform(self, X):\n        def transform(row):\n            row_copy = row.copy()\n            if pd.isnull(row_copy.at[self.column_to_impute]):\n                row_copy.at[self.column_to_impute] = self.value_per_group[row_copy.at[self.column_ref]]\n            return row_copy\n        return X.apply(transform, axis=1)\n\n# Example output\nImputerByReference('Age', 'Title').fit_transform(df).head(10)","ed922eb4":"pipeline = Pipeline([\n    ('name_to_title', DataFrameColumnMapper(column_name='Name', mapping_func=extract_title(), new_column_name='Title')),\n    ('truncate_titles', CategoricalTruncator('Title', n_values_to_keep=3)),\n    ('impute_ages_by_title', ImputerByReference('Age', 'Title'))\n])\n\ndf = pipeline.fit_transform(X_train)\ndf.info()","f0c84d38":"class CategoricalToOneHotEncoder(BaseEstimator, TransformerMixin):\n    def __init__(self, columns=None):\n        self.columns = columns\n    def fit(self, X, y=None):\n        # Pick all categorical attributes if no columns to transform were specified\n        if self.columns is None:\n            self.columns = X.select_dtypes(exclude='number')\n        \n        # Keep track of which categorical attributes are assigned to which integer. This is important \n        # when transforming the test set.\n        mappings = {}\n        \n        for col in self.columns:\n            labels, uniques = X.loc[:, col].factorize() # Assigns unique integers for all categories\n            int_and_cat = list(enumerate(uniques))\n            cat_and_int = [(x[1], x[0]) for x in int_and_cat]\n            mappings[col] = {'int_to_cat': dict(int_and_cat), 'cat_to_int': dict(cat_and_int)}\n    \n        self.mappings = mappings\n        return self\n\n    def transform(self, X):\n        Y = X.copy()\n        for col in self.columns:\n            transformed_col = Y.loc[:, col].transform(lambda x: self.mappings[col]['cat_to_int'][x])\n            for key, val in self.mappings[col]['cat_to_int'].items():\n                one_hot = (transformed_col == val) + 0 # Cast boolean to int by adding zero\n                Y = Y.assign(**{'{}_{}'.format(col, key): one_hot})\n            Y = Y.drop(col, axis=1)\n        return Y\n    \n# Example output    \nCategoricalToOneHotEncoder().fit_transform(df).head()   ","99c14b40":"from sklearn.preprocessing import MinMaxScaler\nfrom sklearn.impute import SimpleImputer\n\nclass DataFrameToValuesTransformer(BaseEstimator, TransformerMixin):\n    def __init__(self):\n        pass\n    def fit(self, X, y=None):\n        # Remember the order of attributes before converting to NumPy\n        self.attribute_order = list(X)\n        return self\n    def transform(self, X):\n        return X.loc[:, self.attribute_order].values\n\ndef build_preprocessing_pipeline():\n    return Pipeline([\n        ('name_to_title', DataFrameColumnMapper(column_name='Name', mapping_func=extract_title(), new_column_name='Title')),\n        ('truncate_titles', CategoricalTruncator(column_name='Title', n_values_to_keep=3)),\n        ('impute_ages_by_title', ImputerByReference(column_to_impute='Age', column_ref='Title')),\n        ('encode_categorical_onehot', CategoricalToOneHotEncoder()),\n        ('encode_pclass_onehot', CategoricalToOneHotEncoder(columns=['Pclass'])),\n        ('to_numpy', DataFrameToValuesTransformer()),\n        ('imputer', SimpleImputer(strategy='median')), # Test set has one missing fare\n        ('scaler', MinMaxScaler())\n    ])\n\nX_train_prepared = build_preprocessing_pipeline().fit_transform(X_train)\nprint('Prepared training data: {} samples, {} features'.format(*X_train_prepared.shape))","367d70e2":"def build_pipeline(classifier=None):\n    preprocessing_pipeline = build_preprocessing_pipeline()\n    return Pipeline([\n        ('preprocessing', preprocessing_pipeline),\n        ('classifier', classifier) # Expected to be filled by grid search\n    ])\n\n\ndef build_grid_search(pipeline, param_grid):\n    return GridSearchCV(pipeline, param_grid, cv=5, return_train_score=True, refit='accuracy',\n                        scoring={ 'accuracy': make_scorer(accuracy_score),\n                                  'precision': make_scorer(precision_score)\n                                },\n                        verbose=1)\n\ndef pretty_cv_results(cv_results, \n                      sort_by='rank_test_accuracy',\n                      sort_ascending=True,\n                      n_rows=5):\n    df = pd.DataFrame(cv_results)\n    cols_of_interest = [key for key in df.keys() if key.startswith('param_') \n                        or key.startswith('mean_train') \n                        or key.startswith('mean_test_')\n                        or key.startswith('rank')]\n    return df.loc[:, cols_of_interest].sort_values(by=sort_by, ascending=sort_ascending).head(n_rows)\n\ndef run_grid_search(grid_search):\n    grid_search.fit(X_train, y_train)\n    print('Best test score accuracy is:', grid_search.best_score_)\n    return pretty_cv_results(grid_search.cv_results_)","b82170dd":"param_grid = [\n    { 'preprocessing__truncate_titles__n_values_to_keep': [3, 4, 5],\n      'classifier': [SGDClassifier(loss='log', tol=None, random_state=42)],\n      'classifier__alpha': np.logspace(-5, -3, 3),\n      'classifier__penalty': ['l2'],\n      'classifier__max_iter': [20],\n    }\n]\nlog_grid_search = build_grid_search(pipeline=build_pipeline(), param_grid=param_grid)\nlinear_cv = run_grid_search(grid_search=log_grid_search)","a379c24e":"linear_cv","a8126e77":"param_grid = [\n    { 'preprocessing__truncate_titles__n_values_to_keep': [5],\n      'classifier': [RandomForestClassifier(random_state=42)],\n      'classifier__n_estimators': [10, 30, 100],\n      'classifier__max_features': range(4, 14, 3)\n    }\n]\nrf_grid_search = build_grid_search(pipeline=build_pipeline(), param_grid=param_grid)\nrf_cv_results = run_grid_search(grid_search=rf_grid_search)\nrf_cv_results","da4ddfd0":"param_grid = [\n    { \n        'preprocessing__truncate_titles__n_values_to_keep': [5],\n        'classifier': [ SVC(random_state=42, probability=True) ], # Probability to use in voting later\n        'classifier__C': np.logspace(-1, 1, 3),\n        'classifier__kernel': ['linear', 'poly', 'rbf'],\n        'classifier__gamma': ['auto', 'scale']\n    }\n]\n\n\nsvm_grid_search = build_grid_search(pipeline=build_pipeline(), param_grid=param_grid)\nsvm_cv_results = run_grid_search(grid_search=svm_grid_search)","ec3224e1":"svm_cv_results","eab37fca":"from sklearn.gaussian_process.kernels import RBF, Matern\n\nparam_grid = [\n    { \n        'preprocessing__truncate_titles__n_values_to_keep': [5],\n        'classifier': [ GaussianProcessClassifier() ], \n        'classifier__kernel': [1.0*RBF(1.0), 1.0*Matern(1.0)]\n    }\n]\n\ngp_grid_search = build_grid_search(pipeline=build_pipeline(), param_grid=param_grid)\ngp_cv_results = run_grid_search(grid_search=gp_grid_search)\n","51775fe9":"gp_cv_results","b31dd472":"from sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.tree import DecisionTreeClassifier\n\nparam_grid = [\n    { \n        'preprocessing__truncate_titles__n_values_to_keep': [5],\n        'classifier': [ AdaBoostClassifier(random_state=42) ],\n        'classifier__n_estimators': [50, 100],\n        'classifier__learning_rate': np.logspace(-1, 1, 3),\n        'classifier__base_estimator': [\n            DecisionTreeClassifier(max_depth=1),\n            DecisionTreeClassifier(max_depth=2)\n        ],\n        # 'classifier__base_estimator__max_depth': [1, 2]\n    }\n]\n\nada_grid_search = build_grid_search(pipeline=build_pipeline(), param_grid=param_grid)\nada_cv_results = run_grid_search(grid_search=ada_grid_search)","78ff6a47":"ada_cv_results","42738f2f":"param_grid = [\n    { \n        'preprocessing__truncate_titles__n_values_to_keep': [5],\n        'classifier': [ GradientBoostingClassifier(random_state=42) ],\n        'classifier__loss': ['deviance'],\n        'classifier__n_estimators': [50, 100],\n        'classifier__max_features': [7, 13],\n        'classifier__max_depth': [3, 5],\n        'classifier__min_samples_leaf': [1],\n        'classifier__min_samples_split': [2]\n    }\n]\n\ngb_grid_search = build_grid_search(pipeline=build_pipeline(), param_grid=param_grid)\ngb_cv_results = run_grid_search(grid_search=gb_grid_search)","41d5b845":"gb_cv_results","8420d2ed":"gb_grid_search.best_estimator_.score(X_val, y_val)","4f5ba635":"voting_estimators = [\n    # ('logistic', log_grid_search),\n    # ('rf', rf_grid_search),\n    ('svc', svm_grid_search),\n    ('gp', gp_grid_search),\n    # ('ada', ada_grid_search),\n    ('gb', gb_grid_search),\n]\n\nestimators_with_names = [(name, grid_search.best_estimator_) for name, grid_search in voting_estimators]\n\nvoting_classifier = VotingClassifier(estimators=estimators_with_names,\n                                     voting='soft')\n\nvoting_classifier.fit(X_train, y_train)\nvoting_classifier.score(X_val, y_val)\n# cross_val_score(voting_classifier, X_train_val, y_train_val, cv=5)","4a1bfded":"voting_classifier.fit(X_train_val, y_train_val)","4a2bc0b2":"def get_predictions(estimator):\n    predictions = estimator.predict(X_test)\n    indices = test.loc[:, 'PassengerId']\n    as_dict = [{'PassengerId': index, 'Survived': prediction} for index, prediction in zip(indices, predictions)]\n    return pd.DataFrame.from_dict(as_dict)\n\npredictions = get_predictions(voting_classifier)","dfd2cc94":"submission_folder = '.'\ndest_file = os.path.join(submission_folder, 'submission.csv')\npredictions.to_csv(dest_file, index=False)","3dca88fe":"Important things to note here are that \n- The dataset is slightly skewed as only 38% survived, which could be important to take into account in cross-validation\n- Some values in the `Fare` column are zero\n- As noted above, many `Age`s are missing.","1c458fad":"## Voting classifier\nCreate a voting classifier from the best estimators and check the generalization accuracy for heldout data `X_val`","b97e71f8":"Before moving to trying different algorithms and optimizing hyperparameters, we define a few helper functions that are hopefully self-explanatory.","8211d4dc":"## [Gradient boosting](http:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.ensemble.GradientBoostingClassifier.html)","8bcd65b5":"Note that we could drop either `Sex_male` or `Sex_female` without losing any data, but we'll leave that for now. Now that all values are imputed and all columns are numerical, we finally define a transformer for converting the DataFrame to a NumPy matrix and build the  full data preparation pipeline. We also include `MinMaxScaler` as last preprocessing step as some algorithms are sensitive to variations in scale. We also add a simple imputer, as test set has one missing `Fare` value.","6f37cc07":"Trying different algorithms is now straightforward. Choose the parameters to vary and run the grid search with cross validation to find both the best preprocessing pipeline and classifier.","7d10a6f0":"Take a peek at the columns:","07d63d30":"`train.info()` shows that many values in `Age` and `Cabin` columns are missing:","5a833e6a":"## [Random forest](http:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.ensemble.RandomForestClassifier.html)","c4dbb540":"## [AdaBoost](http:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.ensemble.AdaBoostClassifier.html#sklearn.ensemble.AdaBoostClassifier)","f4b6799b":"To get more insights, check statistical summary for numerical attributes using `train.describe()`:","95618439":"Let's take a look at the transformed names:","88c6fe1a":"First import the required libraries:","2ef557e2":"## [SVM](http:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.svm.SVC.html)","edbd6e5f":"It seems that only the, say, five most frequent would be useful for imputing ages, so let us write a transformer that transforms the less frequent fields of a categorical attribute as \"Other\". The number of classes to keep is included as a constructor argument that can be optimized using cross-validation.","b7d21aae":"Looks good! Now we one-hot encode all categorical attributes using again a generic transformer. Note that the one-hot encoding would get us into trouble if we were encoding columns with many different values (like column `Ticket`), but we do not worry about that here. The first step is convert all categorical attributes to numerical ones by factorizing to integers (`Mrs.` is 0, `Mr.` is 1, for example.):","0bc46cc3":"The full pipeline so far is below. Let us use `.info()` to check that no values are missing from the transformed data:","a8e3291c":"This kernel illustrates the use of [scikit-learn Pipelines](http:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.pipeline.Pipeline.html) for data transformation. Custom transformers written here are easily reusable for other projects and they also enable including any data transformation parameters in the hyperparameter optimization.","d5ddbfa6":"First write a [scikit-learn transformer](http:\/\/scikit-learn.org\/stable\/data_transforms.html) for converting name to title. You may think that there's a lot of overhead involved in writing such classes (and you're right), but the transformer classes are highly reusable and therefore save a lot of time in future projects.","b411bbe6":"Now write a generic imputer that uses values in a given column (\"Title\" in our case) to impute missing values for a numeric column (\"Age\") with the median value for the group in question.","0407dd84":"## Prepare the submission","0d9d59dc":"## [Logistic classifier](http:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.linear_model.SGDClassifier.html#sklearn.linear_model.SGDClassifier)","05bd8b7b":"## Train voting classifier with all data available","34257a2b":"## [Gaussian process classifier](http:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.gaussian_process.GaussianProcessClassifier.html#sklearn.gaussian_process.GaussianProcessClassifier)","1eaa9d55":"As there are lots of data exploration kernels out there, I will skip any further data exploration here and focus on the actual data transformation pipelines. Data exploration shows that `Age` is an important indicator for survival and the missing values should be imputed. The data processing steps carried out below are:\n1. Split the dataset into features and labels. To simplify the kernel, we drop `Ticket`, `Cabin`, `Embarked`, and of course `PassengerId` from the feature set.\n2. Extract the title from the `Name` field and use it as a feature to impute the missing values. For example, title `Mr.` says that the person in question was not a child.\n3. One-hot encode all categorical attributes\n4. Convert the dataframe to numpy matrix.","71d1ec64":"# Reusable data transformations using scikit-learn pipelines and hyperparameter optimization","86bc9190":"First we split the dataset into features and labels and give them common names `X_train` and `y_train`. Note that these are still dataframes, conversion to NumPy is done just before feeding the inputs to prediction algorithms.A Also note that we split the given training set into training and validation sets. The validation set works as a hold-out set that can be used for estimating the generalization error after hyperparameter optimization. The algorithm used in the final submission use, of course, all training data available.","d02c69ec":"Let us see what we have done so far by putting the transformers together in a pipeline:","00ff18ca":"Load the datasets. Note that for simplicity, we'll keep the label field `Survived` in the `train` and `test` dataframes until actually starting transformations and predictions."}}