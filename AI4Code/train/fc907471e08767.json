{"cell_type":{"40fa3a79":"code","8b96ad79":"code","2ddae784":"code","a962e887":"code","043e0f06":"code","6dbdf0a7":"code","78f051cf":"code","7fca9219":"code","9d41e81b":"code","d8f45697":"code","3b9188e6":"code","462fd42e":"code","a84deecf":"code","c7df6fa0":"code","0826e4b9":"code","9ebfff33":"code","f40ebfba":"code","dc9d0a42":"code","a01e24dd":"markdown","8308d85e":"markdown","be9c47ac":"markdown","b16e63cd":"markdown","6d96953b":"markdown","9a1cf7c5":"markdown","380255d6":"markdown","8079dd96":"markdown","f9f6a846":"markdown","36655162":"markdown","e70c93a8":"markdown","28e3d9f0":"markdown","c506ee81":"markdown","ee8265d1":"markdown","cfc93f1e":"markdown","7dffa079":"markdown","afceb159":"markdown","a2580f3b":"markdown","addc63f6":"markdown","75502232":"markdown","cf4fd588":"markdown","e851a762":"markdown","69323014":"markdown","c617470c":"markdown"},"source":{"40fa3a79":"# Data Processing\nimport numpy as np \nimport pandas as pd \n\n\n# Data Visualization\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\n\n\n# Modeling\nfrom sklearn.model_selection import train_test_split\n\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import RandomForestClassifier\n\n\n# Hyperparameter Tuning\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.model_selection import GridSearchCV","8b96ad79":"# Loading the data\ndf_train = pd.read_csv(\"\/kaggle\/input\/titanic\/train.csv\")\ndf_test = pd.read_csv(\"\/kaggle\/input\/titanic\/test.csv\")\n\n\n# Handling NaN values\ndf_train['Age'] = df_train['Age'].fillna(df_train['Age'].mean())\ndf_test['Age'] = df_test['Age'].fillna(df_test['Age'].mean())\n\ndf_train['Cabin'] = df_train['Cabin'].fillna(\"Missing\")\ndf_test['Cabin'] = df_test['Cabin'].fillna(\"Missing\")\n\ndf_train = df_train.dropna()\n\ndf_test['Fare'] = df_test['Fare'].fillna(df_test['Fare'].mean())\n\n# Cleaning the data\ndf_train = df_train.drop(columns=['Name'], axis=1)\ndf_test = df_test.drop(columns=['Name'], axis=1)\n\nsex_mapping = {\n    'male': 0,\n    'female': 1\n}\ndf_train.loc[:, \"Sex\"] = df_train['Sex'].map(sex_mapping)\ndf_test.loc[:, \"Sex\"] = df_test['Sex'].map(sex_mapping)\n\ndf_train = df_train.drop(columns=['Ticket'], axis=1)\ndf_test = df_test.drop(columns=['Ticket'], axis=1)\n\ndf_train = df_train.drop(columns=['Cabin'], axis=1)\ndf_test = df_test.drop(columns=['Cabin'], axis=1)\n\ndf_train = pd.get_dummies(df_train, prefix_sep=\"__\",\n                              columns=['Embarked'])\ndf_test = pd.get_dummies(df_test, prefix_sep=\"__\",\n                              columns=['Embarked'])","2ddae784":"df_train.head()","a962e887":"df_test.head()","043e0f06":"# Everything except the target variable\nX = df_train.drop(\"Survived\", axis=1)\n\n# Target variable\ny = df_train['Survived'].values","6dbdf0a7":"# Random seed for reproducibility\nnp.random.seed(42)\n\n# Splitting the data into train & test set\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2)","78f051cf":"# Setting up KNeighborsClassifier()\nknn = KNeighborsClassifier()\nknn.fit(X_train, y_train)\nknn.score(X_test, y_test)","7fca9219":"# Setting up RandomForestClassifier()\nrfc = RandomForestClassifier()\nrfc.fit(X_train, y_train)\nrfc.score(X_test, y_test)","9d41e81b":"# List of train scores\ntrain_scores = []\n\n# List of test scores\ntest_scores = []\n\n# List of different values for n_neighbors\nneighbors = range(1, 51) # 1 to 50\n\n# Setting up the classifier\nknn = KNeighborsClassifier()\n\n# Loop through different neighbors values\nfor i in neighbors:\n    knn.set_params(n_neighbors = i) # set neighbors value\n    \n    # Fitting the algorithm\n    knn.fit(X_train, y_train)\n    \n    # Append the training scores\n    train_scores.append(knn.score(X_train, y_train))\n    \n    # Append the test scores\n    test_scores.append(knn.score(X_test, y_test))","d8f45697":"# Plotting the Train and Test scores\nplt.figure(figsize=(20,10))\nplt.plot(neighbors, train_scores, label=\"Train score\")\nplt.plot(neighbors, test_scores, label=\"Test score\")\nplt.xticks(np.arange(1, 51, 1))\nplt.xlabel(\"Number of neighbors\")\nplt.ylabel(\"Model score\")\nplt.legend()\n\n\nprint(f\"Maximum KNN score on the test data: {max(test_scores)*100:.2f}%\")\nprint(f\"Number of Neighbours with Maximum KNN: {test_scores.index(max(test_scores)) + 1}\")","3b9188e6":"# Setting up dictionary with RandomForestClassifier hyperparameters\nrfc_rs_grid = {\"n_estimators\": np.arange(10, 1000, 50),\n               \"max_depth\": [None, 3, 5, 10],\n               \"min_samples_split\": np.arange(2, 20, 2),\n               \"min_samples_leaf\": np.arange(1, 20, 2)}","462fd42e":"# Import RandomizedSearchCV\nfrom sklearn.model_selection import RandomizedSearchCV\n\n# Setting random seed\nnp.random.seed(42)\n\n# Setting random hyperparameter search for RandomForestClassifier\nrs_rfc = RandomizedSearchCV(RandomForestClassifier(),\n                           param_distributions=rfc_rs_grid,\n                           cv=5,\n                           n_iter=20,\n                           verbose=True)\n\n# Fitting random hyperparameter search model\nrs_rfc.fit(X_train, y_train);","a84deecf":"# Finding the best parameters\nrs_rfc.best_params_","c7df6fa0":"# Evaluate the model\nrs_rfc.score(X_test, y_test)","0826e4b9":"# Setting up dictionary with RandomForestClassifier hyperparameters\nrfc_gs_grid = {\"n_estimators\": np.arange(10, 1010, 100),\n               \"max_depth\": [None, 5, 10]}","9ebfff33":"# Import GridSearchCV\nfrom sklearn.model_selection import GridSearchCV\n\n\n# Setting grid hyperparameter search for RandomForestClassifier\ngs_rfc = GridSearchCV(RandomForestClassifier(),\n                          param_grid=rfc_gs_grid,\n                          cv=5,\n                          verbose=True)\n\n# Fitting grid hyperparameter search model\ngs_rfc.fit(X_train, y_train);","f40ebfba":"# Finding the best parameters\ngs_rfc.best_params_","dc9d0a42":"# Evaluate the model\ngs_rfc.score(X_test, y_test)","a01e24dd":"# Preparing the data\n\nI would like to skip right to the point where we have prepared data without desciption.\nI am using the same approach as in this [Kernal](https:\/\/www.kaggle.com\/dietzschdaniel\/my-simplistic-titanic-approach-0-79665).","8308d85e":"First, we again have to define the dictionary. This time we only use `n_estimators` and `max_depth` since this approach takes a lot of time to compute.","be9c47ac":"## Tuning hyperparameters by hand","b16e63cd":"### About the dataset:\n\n**Context:** \n> The competition is simple: use machine learning to create a model that predicts which passengers survived the Titanic shipwreck.\n\n**About the Data:**\n\n<ul>\n    <li>survival:\tSurvival<\/li>\n        <ul>\n            <li>0 = No<\/li>\n            <li>1 = Yes <\/li>\n        <\/ul>\n    <li>pclass: A proxy for socio-economic status (SES)<\/li>\n        <ul>\n            <li>1 = 1st (Upper)<\/li>\n            <li>2 = 2nd (Middle)<\/li>\n            <li>3 = 3rd (Lower)<\/li>\n        <\/ul>\n    <li>sex: Sex<\/li>\n        <ul>\n            <li>0 = male<\/li>\n            <li>1 = female <\/li>\n        <\/ul>\n    <li>age: Age in years. Age is fractional if less than 1. If the age is estimated, is it in the form of xx.5<\/li>\n    <li>sibsp: # of siblings \/ spouses aboard the Titanic. The dataset defines family relations in this way:<\/li>\n        <ul>\n            <li>Sibling = brother, sister, stepbrother, stepsister<\/li>\n            <li>Spouse = husband, wife (mistresses and fianc\u00e9s were ignored)<\/li>\n        <\/ul>\n    <li>parch: # of parents \/ children aboard the Titanic. The dataset defines family relations in this way:<\/li>\n        <ul>\n            <li>Parent = mother, father<\/li>\n            <li>Child = daughter, son, stepdaughter, stepson<\/li>\n            <li>Some children travelled only with a nanny, therefore parch=0 for them.<\/li>\n        <\/ul>\n    <li>fare: Passenger fare<\/li>\n    <li>embarked: Port of Embarkation<\/li>\n        <ul>\n            <li>C = Cherbourg<\/li>\n            <li>Q = Queenstown<\/li>\n            <li>S = Southampton<\/li>\n        <\/ul>\n<\/ul> \n","6d96953b":"**If you liked this notebook or found it helpful in any way, feel free to leave an upvote - That will keep me motivated :)**\n\n**If you have any suggestions for improvement, leave a comment :)**","9a1cf7c5":"<h1 style=\"text-align:center\">Introduction to Hyperparameter Tuning <\/h1>","380255d6":"First, we have to define a dictionary of hyperparameters we would like to check:","8079dd96":"`RandomizedSearchCV()` uses a randomized search through hyperparameters.   \n\nIn this example we use the `RandomForestClassifier()`.","f9f6a846":"### Preparing the data for Modeling","36655162":"Later I will be using `KNeighborsClassifier()` and `RandomForestClassifier()`.\nLet's check out the scores of `KNeighborsClassifier()` and `RandomForestClassifier()` ***without*** hyperparameter tuning:","e70c93a8":"Let's plot the train and test scores for the different neighbour values:","28e3d9f0":"# Hyperparameter Tuning","c506ee81":"We were able to improve the score!","ee8265d1":"# Imports","cfc93f1e":"One way is of course to tune the hyperparameters by hand.\n\nIn this example, we tune the hyperparameter `n_neighbors` of the `KNeighborsClassifier()` by looping through different `n_neighbors` values and comparing their score.","7dffa079":"### About the Notebook:\n\n**In this notebook, I want to address different approaches to hyperparameter tuning using the example of the titanic dataset.**","afceb159":"## Tuning hyperparameters using ***RandomizedSearchCV()***","a2580f3b":"In this case we reached the same score as without the hyperparameter tuning. We could try a different dictionary to improve the score. However, due to the long time to compute and since this is just a demonstration, I will leave it like this.","addc63f6":"`GridSearchCV()` uses exhaustive search over specified parameter values for an estimator.  \n\nIn this example we use the `RandomForestClassifier()`.","75502232":"The Maximum KNN score on the test data is reached with `n_neighbors = 29` with a score of `69.66%`.","cf4fd588":"## Tuning hyperparameters using ***GridSearchCV()***","e851a762":"Now let's set up the hyperparameter search for the `RandomForestClassifier()` using `RandomizedSearchCV()`.","69323014":"Our data now looks as follows:","c617470c":"<div style=\"text-align:center;\"><img src=\"https:\/\/images.unsplash.com\/photo-1503792453751-9dffb431aa63?ixlib=rb-1.2.1&ixid=eyJhcHBfaWQiOjEyMDd9&auto=format&fit=crop&w=1636&q=80\" \/><\/div>"}}