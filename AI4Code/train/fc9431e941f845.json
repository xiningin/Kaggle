{"cell_type":{"0d0ecb94":"code","26c8ffbb":"code","b7d401b4":"code","1ab2abf9":"code","f860d327":"code","f497878f":"code","c8ac68dd":"code","dd877d67":"code","f744f116":"code","917ebab3":"code","70801616":"code","d50b4bbd":"code","1d3d5496":"code","6c05006c":"code","89c2ccbe":"code","8543dc4f":"code","a054072c":"code","5c1ccf70":"code","8eb871ba":"code","a77973bd":"code","2abc9a89":"code","329f25bc":"code","759a3400":"markdown","a3110429":"markdown","44f5817e":"markdown","e6803cc1":"markdown","8d1aac10":"markdown","4962be47":"markdown","1cdb75ba":"markdown","3cb993cd":"markdown","9aa52832":"markdown"},"source":{"0d0ecb94":"import warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\nimport pandas as pd\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import*\nimport matplotlib.pyplot as plt\n\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.feature_selection import VarianceThreshold\n\nfrom sklearn.linear_model import*\nfrom sklearn.preprocessing import*\nfrom sklearn.ensemble import*\nfrom sklearn.neighbors import*\n\nfrom xgboost import XGBRegressor","26c8ffbb":"df = pd.read_csv(\"\/kaggle\/input\/nmims-m2-batch-1\/train.csv\")\ntest = pd.read_csv(\"\/kaggle\/input\/nmims-m2-batch-1\/test.csv\")","b7d401b4":"df.info()","1ab2abf9":"if df.isnull().sum().any() == False:\n    print(\"No missing values\")\nelse:\n    print(\"Missing values present\")","f860d327":"print(\"number of duplicate rows: \", df.duplicated().sum())","f497878f":"df.describe()","c8ac68dd":"plt.figure()\nplt.title(\"Avocado Average Price by Type\")\nsns.barplot(x=\"type\",y=\"AveragePrice\",data= df)\nplt.show()","dd877d67":"df_conventional = df[df['type'] == 'conventional']\ndf_organic = df[df['type'] == 'organic']\nf, ax = plt.subplots(nrows=1, ncols=1, figsize=(18, 4))\nsns.distplot(df_conventional['AveragePrice'])\nsns.distplot(df_organic['AveragePrice'])\nplt.show()","f744f116":"X=df.drop('AveragePrice',1)\ny=df['AveragePrice']\nX_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.20,random_state=42)","917ebab3":"scaler=LabelEncoder()\nfor col in X_train.columns:\n    if df[col].dtype=='object':\n        X_train[col]=scaler.fit_transform(X_train[col])\n        X_test[col]=scaler.transform(X_test[col])\n        test[col]=scaler.transform(test[col])","70801616":"scaler=VarianceThreshold(0.1)\nX_train=scaler.fit_transform(X_train)\nX_test=scaler.transform(X_test)\ntest = scaler.transform(test)","d50b4bbd":"scaler=StandardScaler()\nX_train=scaler.fit_transform(X_train)\nX_test=scaler.transform(X_test)\ntest = scaler.transform(test)","1d3d5496":"from sklearn.ensemble import ExtraTreesRegressor\netr = ExtraTreesRegressor(random_state=42)\netr.fit(X_train,y_train)\n\ny_tr1_etr=etr.predict(X_train)\ny_pr_etr=etr.predict(X_test)\n\nprint('train data accuracy :',etr.score(X_train,y_train))\nprint('test data accuracy :',etr.score(X_test,y_test))\nprint('loss of train data :',mean_squared_error(y_train,y_tr1_etr))\nprint('loss of test data :',mean_squared_error(y_test,y_pr_etr))","6c05006c":"plt.scatter(x=y_test,y=y_pr_etr)","89c2ccbe":"# The hyperparameters were found using GridSerchCV, since the run time was too long I didnt include it here\nxgb = XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,\n             importance_type='gain', interaction_constraints='',\n             learning_rate=0.1, max_delta_step=0, max_depth=9,\n             min_child_weight=1, monotone_constraints='()',\n             n_estimators=180, n_jobs=4, num_parallel_tree=1, random_state=0,\n             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n             tree_method='exact', validate_parameters=1, verbosity=None)\nxgb.fit(X_train, y_train)\n\ny_tr2=xgb.predict(X_train)\ny_pr2=xgb.predict(X_test)\n\nprint('train data accuracy :',xgb.score(X_train,y_train))\nprint('test data accuracy :',xgb.score(X_test,y_test))\nprint('loss of train data :',mean_squared_error(y_train,y_tr2))\nprint('loss of test data :',mean_squared_error(y_test,y_pr2))","8543dc4f":"plt.scatter(x=y_test,y=y_pr2)","a054072c":"price_preds_etr = etr.predict(test)\nprice_preds_xgb = xgb.predict(test)","5c1ccf70":"sub_etr = pd.read_csv('..\/input\/sample-sbm\/sample_submission.csv')\nsub_etr['AveragePrice'] = price_preds_etr\nsub_etr.to_csv('submission_etr.csv', index=False)","8eb871ba":"sub_xgb = pd.read_csv('..\/input\/sample-sbm\/sample_submission.csv')\nsub_xgb['AveragePrice'] = price_preds_xgb\nsub_xgb.to_csv('submission_xgb.csv', index=False)","a77973bd":"weight_model_paths_pairs = [\n    (.575, '..\/input\/best-subs\/submission_xgb.csv'),\n    (.425, '..\/input\/best-subs\/submission_etr.csv')\n]","2abc9a89":"targets = ['AveragePrice']\n\nsubmissions = []\nfor weight, path in weight_model_paths_pairs:\n    df = pd.read_csv(path)\n    df[targets] = df[targets] * weight\n    submissions.append(df)\n\nensembled_output = pd.concat(submissions).groupby(['id']).sum().reset_index()","329f25bc":"ensembled_output.to_csv('submission_ensemble_575_425.csv', index=False)","759a3400":"# Exploratory Data Analysis","a3110429":"No missing values, no duplicates and no outliers\n\n(Thank god) \n","44f5817e":"# Importing Libraries","e6803cc1":"On submitting both the files, we got 0.084 MSE on xgboost and 0.090 on ETR. So we ensembled the results with weightage given according to the results obtained on the submissions","8d1aac10":"# Ensembling","4962be47":"# Predictions","1cdb75ba":"# The Data","3cb993cd":"# Scaling and Preprocessing","9aa52832":"# Training"}}