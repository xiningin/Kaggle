{"cell_type":{"457fa1da":"code","9412db4c":"code","7bf8f1d9":"code","69857657":"code","422b4f18":"code","0def51e1":"code","04676141":"code","209d3fed":"code","b9fb71f7":"code","01fdc8f5":"code","4b77143b":"code","efe78de8":"code","d1dd1fb3":"code","0b1e42d0":"code","fded028a":"code","79f31e66":"code","67b17828":"code","a1171232":"code","b53bf5bc":"code","4771b353":"code","616794e8":"code","e8d77f70":"code","d744014b":"code","27830fca":"markdown","8ba85571":"markdown","1b53ff23":"markdown","15c5ff5b":"markdown"},"source":{"457fa1da":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","9412db4c":"import logging\nimport math\nfrom io import BytesIO\nfrom zipfile import ZipFile\nimport pandas as pd\nimport numpy as np\nimport numpy.random as rnd\nfrom sklearn.preprocessing import LabelBinarizer\nfrom PIL import Image, ImageFilter\nfrom keras.models import Sequential\nfrom keras.layers.normalization import BatchNormalization\nfrom keras.layers.convolutional import Conv2D\nfrom keras.layers.convolutional import MaxPooling2D\nfrom keras.layers.core import Activation\nfrom keras.layers.core import Flatten\nfrom keras.layers.core import Dense","7bf8f1d9":"#configuration data \nimage_height = 128\/\/1\nimage_width = 2*176\/\/1\ntraining_image_count = 576\ntesting_image_count = 384\nclasses_count = 11","69857657":"data_root_path = '..\/input\/bijli-wala-project\/'\nsub = pd.read_csv(\"..\/input\/bijli-wala-project\/submission_format.csv\")\nsub.to_csv('submission_results.csv', index=False)\nsub.head(3)","422b4f18":"training_image_data_file_path = data_root_path + 'image_train.data'\ntraining_labels_data_file_path = data_root_path + 'image_train_labels.csv'\ntesting_data_file_path = data_root_path + 'image_test.data'\n\ntesting_submission_file_path = data_root_path + 'submission_format.csv'\nsubmission_results_file_path =  '\/kaggle\/working\/' + 'submission_results.csv'","0def51e1":"def compose_train_image(p_img1, p_img2) :\n    #Stacks images horizontally (i.e. one afer another on width axis)\n    img_merge_data = np.hstack([np.asarray(p_img1), np.asarray(p_img2)])\n    img_merge = Image.fromarray( img_merge_data )\n        \n    return img_merge","04676141":"def get_image_data(p_image) :\n    #Generates image data from the received image object\n    width, height = p_image.size\n    data = np.asarray(p_image).reshape(height*width)\n    \n    return data","209d3fed":"def create_trainining_images_data_file(p_input_data_file_path, p_training_data_file_path):\n    training_labels_file_path = 'train_labels.csv'\n    labels = None\n\n    with open(p_training_data_file_path, 'w+b') as data_file :\n        with ZipFile(p_input_data_file_path) as data_zip:\n            with data_zip.open(training_labels_file_path) as train_labels_file:\n                content = train_labels_file.read()\n                with BytesIO(content) as io_content:\n                    train_labels = pd.read_csv(io_content)\n                    max_count = train_labels.shape[0]    \n                    labels = np.zeros(max_count)\n                    count = 0\n\n                    for _, row in train_labels.iterrows() :\n                        with data_zip.open('train\/' + str(row[\"id\"]) + \"_c.png\") as c_file :\n                            with BytesIO(c_file.read()) as input_buffer:\n                                c_image = Image.open(input_buffer).convert(\"L\")\n\n                        with data_zip.open('train\/' + str(row[\"id\"]) + \"_v.png\") as v_file :\n                            with BytesIO(v_file.read()) as input_buffer:\n                                v_image = Image.open(input_buffer).convert(\"L\")\n\n                        image_data = get_image_data(compose_train_image(c_image, v_image))\n                        labels[count] = row[\"appliance\"]\n                        data_file.write(image_data)\n\n                        count = count + 1       \n\n    return labels[:count]","b9fb71f7":"def create_training_labels(p_labels, p_labels_data_file_path) :\n    classes = pd.DataFrame(p_labels.astype(int))\n    classes.to_csv(p_labels_data_file_path, header=None)\n\n    return","01fdc8f5":"def create_testing_images_data_file(p_input_data_file_path, p_testing_data_file_path):\n    submission_format_file_path = 'submission_format.csv'\n\n    with open(p_testing_data_file_path, 'w+b') as data_file :\n        with ZipFile(p_input_data_file_path) as data_zip:\n            with data_zip.open(submission_format_file_path) as submission_format_file:\n                content = submission_format_file.read()\n                with BytesIO(content) as io_content:\n                    submission_indexes = pd.read_csv(io_content)\n                    count = 0\n\n                    for _, row in submission_indexes.iterrows() :\n                        with data_zip.open('test\/' + str(row[\"id\"]) + \"_c.png\") as c_file :\n                            with BytesIO(c_file.read()) as input_buffer:\n                                c_image = Image.open(input_buffer).convert(\"L\")\n\n                        with data_zip.open('test\/' + str(row[\"id\"]) + \"_v.png\") as v_file :\n                            with BytesIO(v_file.read()) as input_buffer:\n                                v_image = Image.open(input_buffer).convert(\"L\")\n\n                        image_data = get_image_data(compose_train_image(c_image, v_image))\n                        data_file.write(image_data)\n\n                        count = count + 1       \n\n    return count","4b77143b":"def create_testing_submission(p_input_data_file_path, p_testing_submission_file_path) :\n    submission_format_file_path = 'submission_format.csv'\n\n    with ZipFile(p_input_data_file_path) as data_zip:\n        with data_zip.open(submission_format_file_path) as submission_format_file:\n            content = submission_format_file.read()\n            with BytesIO(content) as io_content:\n                submission_indexes = pd.read_csv(io_content)\n                submission_indexes.to_csv(p_testing_submission_file_path, index=False)\n\n    return","efe78de8":"def main() :\n    logging.basicConfig(level=logging.INFO)\n    \n    #create training data\n    logging.info('Creating training data ...')\n    training_labels  = create_trainining_images_data_file(input_data_file_path, training_image_data_file_path)\n    create_training_labels(training_labels, training_labels_data_file_path)\n    logging.info(\"Processed training images count: %d\" % training_labels.shape[0])\n    logging.info('Creating training data DONE')\n\n    logging.info('Creating testing data ...')\n    testing_count = create_testing_images_data_file(input_data_file_path, testing_data_file_path)\n    create_testing_submission(input_data_file_path, testing_submission_file_path)\n    logging.info(\"Processed testing images count: %d\" % testing_count)\n    logging.info('Creating testing data DONE')","d1dd1fb3":"def read_image(p_image_data_file_path, p_position, p_image_width, p_image_height) :\n    with open(p_image_data_file_path, \"rb\") as image_file :\n        image_file.seek(p_position * p_image_height* p_image_width)\n        data = image_file.read(p_image_height * p_image_width)\n        data_b = np.frombuffer(data, dtype=np.uint8)\n\n    return np.asarray(data_b)","0b1e42d0":"def process_images(p_images, p_image_width, p_image_height) :\n\n    #reshape according to inputs accepted by a Conv2d layer\n    processed_images = p_images.reshape(p_images.shape[0], p_image_height, p_image_width, 1)\n\n    #data normalization to max value (0-255 grayscale values)\n    processed_images = (processed_images * 1.0) \/255\n \n    return processed_images","fded028a":"def read_labels(p_labels_file_path) :\n  \n    labels = pd.read_csv(p_labels_file_path, header= None)\n    labels.columns = [\"id\", \"label\"]\n  \n    return labels","79f31e66":"def process_labels(p_labels) :\n    processed_labels = LabelBinarizer().fit_transform(p_labels)\n    \n    return processed_labels","67b17828":"def generate_train_set(\n    p_image_training_data_file_path, \n    p_labels_file_path, \n    p_train_set_size, \n    p_image_width, \n    p_image_height\n) :\n    labels = read_labels(p_labels_file_path)\n    \n    labels_batch = np.zeros(p_train_set_size)\n    labels_batch = labels[\"label\"][0:p_train_set_size].values\n\n    images_batch = []\n  \n    for i in range(0, p_train_set_size) :\n        image_data = read_image(p_image_training_data_file_path, i, p_image_width, p_image_height)\n        images_batch.append(image_data.reshape(p_image_height, p_image_width))\n  \n    train_labels_processed = process_labels(labels_batch)\n  \n    train_images_processed = process_images(np.array(images_batch), p_image_width, p_image_height)\n  \n    return train_labels_processed, train_images_processed","a1171232":"def generate_test_set(\n    p_test_image_data_file_path, \n    p_test_set_size, \n    p_image_width, \n    p_image_height\n) :\n    images_batch = []\n\n    for i in range(0, p_test_set_size) :\n        image_data = read_image(p_test_image_data_file_path, i, p_image_width, p_image_height)\n        images_batch.append(image_data.reshape(p_image_height, p_image_width))\n\n    test_images_processed = process_images(np.array(images_batch), p_image_width, p_image_height)\n\n    return test_images_processed ","b53bf5bc":"def create_model(p_image_width, p_image_height, p_num_classes) :\n    input_shape = (p_image_height, p_image_width, 1)\n\n    #we will use a sequential model for training \n    model = Sequential()\n\t\n    #CONV 3x3x32 => RELU => NORMALIZATION => MAX POOL 3x3 block\n    model.add(Conv2D(32, (3, 3), padding=\"same\", input_shape=input_shape))\n    model.add(Activation(\"relu\"))\n    model.add(BatchNormalization(axis=-1))\n    model.add(MaxPooling2D(pool_size=(3, 3)))\n\n    #CONV 3x3x64 => RELU => NORMALIZATION => MAX POOL 2x2 block\n    model.add(Conv2D(64, (3, 3), padding=\"same\"))\n    model.add(Activation(\"relu\"))\n    model.add(BatchNormalization(axis=-1))\n    model.add(MaxPooling2D(pool_size=(2, 2)))\n\n    #CONV 3x3x128 => RELU => NORMALIZATION => MAX POOL 2x2 block\n    model.add(Conv2D(128, (3, 3), padding=\"same\"))\n    model.add(Activation(\"relu\"))\n    model.add(BatchNormalization(axis=-1))\n    model.add(MaxPooling2D(pool_size=(2, 2)))\n\n    #FLATTEN => DENSE 1024 => RELU => NORMALIZATION block\n    model.add(Flatten())\n    model.add(Dense(1024))\n    model.add(Activation(\"relu\"))\n    model.add(BatchNormalization())\n\n    #final DENSE => SOFTMAX block for multi-label classification\n    model.add(Dense(p_num_classes))\n    model.add(Activation(\"softmax\"))\n\n    #using categorical_crossentropy loss function with adam optimizer\n    model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n\n    return model","4771b353":"def train_model(\n    p_model, \n    p_training_image_data, \n    p_trainging_labels, \n    p_batch_size = 64, \n    p_epochs_to_train = 50, \n    p_verbose_level = 2\n) : \n    p_model.fit(\n        x = p_training_image_data, \n        y = p_trainging_labels, \n        batch_size = p_batch_size, \n        epochs = p_epochs_to_train,\n        shuffle = True,\n        verbose = p_verbose_level    \n    )\n    \n    return p_model","616794e8":"def predict_labels(p_model, p_test_image_data, p_batch_size = 32) :\n    labels = p_model.predict_classes(p_test_image_data, p_batch_size)\n  \n    return labels\n","e8d77f70":"def write_results(\n    p_testing_submission_file_path, \n    p_submission_results_file_path, \n    p_results\n) :\n    submission_structure = pd.read_csv(p_testing_submission_file_path)\n    submission_structure['appliance'] = p_results\n    submission_structure.to_csv(p_submission_results_file_path, index=False)","d744014b":"def main():\n    logging.basicConfig(level=logging.INFO)\n    \n    #prepare training data\n    logging.info('Reading training data ...')\n    train_labels, train_images = generate_train_set(\n        training_image_data_file_path, \n        training_labels_data_file_path, \n        training_image_count, \n        image_width, \n        image_height\n    )\n    logging.info('Reading training data DONE')\n    \n    #create and train model\n    logging.info('Creating model ...')\n    model = create_model (image_width, image_height, classes_count)\n    logging.info('Creating model DONE')\n\n    logging.info('Training model ... ')\n    model = train_model(model, train_images, train_labels, p_epochs_to_train = 50)\n    logging.info('Training model DONE')\n    \n    #create test data\n    logging.info('Reading testing data ...')\n    test_images = generate_test_set(\n      testing_data_file_path, \n      testing_image_count, \n      image_width, \n      image_height\n    )\n    logging.info('Reading testing data DONE')\n    \n    #predict labels for test data\n    logging.info('Predicting test data classes ...')\n    result = predict_labels(model, test_images)\n    logging.info('Predicting test data classes DONE')\n    \n    #write results\n    logging.info('Writing results ...')\n    write_results(\n        testing_submission_file_path, \n        submission_results_file_path, \n        result\n    )\n    logging.info('Writing results DONE')\n\nif __name__ == '__main__':\n    main()","27830fca":"**DAT264x: Microsoft Professional Capstone : Artificial Intelligence - Dec 2019 **","8ba85571":"<img src=\"https:\/\/i.ibb.co\/3fWvrXQ\/Certificate.jpg\" width=\"1000px\">","1b53ff23":"*I used 'bijli-wala-project' which in Hindi means 'That project with electricity thingy' which was the only project at that time........ But wanted to make it fun and happy ;-)*","15c5ff5b":"The best I could restore for the archived website..... :-(\n\n\n**DAT264x: Identifying Appliances from Energy Use Spectrograms.\nHosted By Microsoft **\n\n* According to a 2017 report, the U.S. Energy Information Administration projects a 28% increase in world energy consumption by 2040. And the energy sector is a major contributor to climate change. For example, energy production and use accounts for more than 84% of U.S. greenhouse gas emissions.\n* Increasing the efficiency of energy consumption has benefits for consumers, providers, and the environment. With an increasing number of IoT devices coming online in the energy sector, there is more and more data that can be used to monitor and track energy consumption. Ultimately, this type of data can be used to provide consumers and businesses with recommendations on ways to save energy, lower costs, and help the planet.\n* In this challenge, you will use standard AI tools to identify 11 different types of appliances from their electric signatures, quantified by current and voltage measurements.\n* This plug load dataset contains current and voltage measurements sampled at 30 kHz from 11 different appliance types present in more than 60 households in Pittsburgh, Pennsylvania.  Plus load refers to the energy used by products that are powered by means of an ordinary AC plug (i.e., plugged into an outlet). \n* For each appliance, plug load measurements were post-processed to extract a three-second-long window of measurements of current and voltage. For some observations, the window contains both the startup transient state (turning the appliance on) as well as the steady-state operation (once the appliance is running). \n* For others, the window only contains the steady-state operation. The observations were then transformed into two spectrograms, one for current, and one for voltage.\n*  A spectrogram is a visual representation of the various frequencies of sound as they vary with time. The x-axis represents time (3 seconds in our case), and the y-axis represents frequency (measured in Hz). The colors indicate the amplitude of a particular frequency at a particular time (i.e., how loud it is). \n* We're measuring amplitude in decibels,with 0 being the loudest, and -80 being the softest. So in the example spectrogram below, lower frequencies are louder than higher frequencies. \n\n*Our spectrograms tend to have horizontal lines given that we are capturing appliances in their steady-state. In other words, the amplitudes of various frequencies are fairly constant over time.*\n\nReport: https:\/\/www.eia.gov\/todayinenergy\/detail.php?id=32912\n\nUnder the hood, this process:\n* Takes the [Fourier transform](http:\/\/betterexplained.com\/articles\/an-interactive-guide-to-the-fourier-transform\/) of a windowed excerpt of the raw signal, in order to decompose the signal into its consistuent frequencies.\n* To learn more about Fourier transforms, check out this awesome tutorial by 3Blue1Brown: [But what is the Fourier Transform](https:\/\/www.youtube.com\/watch?v=spUNpyF58BY). (PS:- Really reallt helpful)\n* Maps the powers of the spectrum onto the [mel scale](https:\/\/en.wikipedia.org\/wiki\/Mel_scale). The mel scale is a perceptual scale where pitches are judged to be equal in distance from one another based on the human ear. \n* Takes the logs of the power (amplitude squared) at each of the mel frequencies to [convert to decibel units](https:\/\/librosa.github.io\/librosa\/generated\/librosa.core.power_to_db.html). \n* Plots and saves the resulting image. \n* There is a lot of useful information encoded in these spectrograms. Now it's time to use your deep learning skills to parse out which patterns correspond to which types of appliances.\n"}}