{"cell_type":{"028ae7b2":"code","1b19a303":"code","5d798aff":"code","98d07127":"code","08138439":"code","7c1258e1":"code","7fef1214":"code","29225568":"code","4028c330":"code","a97b4676":"code","40709a84":"code","7fa7619f":"code","bb96ae20":"code","572de6ab":"code","c2e66dce":"code","6012a9ec":"code","4203ccb4":"code","efc494cd":"code","1b6928e3":"code","76ccd43c":"code","0189ba22":"code","0ccb7dfc":"code","97691bd7":"code","90028bd9":"code","f4afcc9b":"code","054a816a":"code","6f5ea9cb":"code","8b0a3caf":"code","3b1c2a0e":"code","8ecb378a":"code","6b87a22e":"code","9d36ea7e":"code","411edb55":"code","4ca0b02b":"code","342fb643":"code","f1e160d0":"code","adcc735b":"code","fbd1d5b4":"code","bc9ae770":"code","8e24ca5c":"code","4b0033ec":"code","5340da82":"code","ce1d18ab":"code","3963ebc9":"code","a509e5ea":"code","01b491b8":"code","45b597f2":"code","d80ae7ba":"code","9163fe4f":"code","24220a31":"code","de222cc0":"code","ce34d214":"code","a83e7c93":"code","aad54b71":"code","5568a9d7":"code","8f27351d":"code","5643e816":"code","01947f07":"markdown","0272afc8":"markdown","48576d10":"markdown","e3081aa0":"markdown","e3e2f349":"markdown","f760ef62":"markdown","04f43c3a":"markdown","ef5c08f0":"markdown","853cf0eb":"markdown","56e439a2":"markdown","877a86b4":"markdown","fe7ac9b1":"markdown","1b4b77ce":"markdown","29c47a6c":"markdown","b7277081":"markdown","86064da1":"markdown","432cfa67":"markdown","cc406b28":"markdown","652285e5":"markdown","e8b16892":"markdown","7559a524":"markdown","b1ac6183":"markdown","2af1a631":"markdown","8ffe4cb0":"markdown","63367006":"markdown","bc9cb39a":"markdown","6951587b":"markdown","ae8c4ac1":"markdown","ded046fb":"markdown","63bca440":"markdown","a675c0b5":"markdown","a2ee1d0b":"markdown","047c6568":"markdown"},"source":{"028ae7b2":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt","1b19a303":"datapath = \"..\/input\/kc1_data.txt\"","5d798aff":"df=pd.read_csv(datapath,sep=\",\",header=None,\n                 names=['log','v(g)','ev(g)','iv(g)'\n                        ,'n','v','l','d','i','e','b'\n                        ,'t','10Code','10Comment','10Blank'\n                        ,'10CodeAndComment','uniq_op','uniq_Opnd'\n                        ,'total_op','total_Opnd','branchCount'\n                        ,'problems'],encoding = 'latin')","98d07127":"df.head()","08138439":"df.columns","7c1258e1":"df.dtypes","7fef1214":"df.shape","29225568":"df.isnull().sum()","4028c330":"X =df.drop([\"problems\"],axis=1)","a97b4676":"X.head()","40709a84":"y = df[[\"problems\"]]","7fa7619f":"y.head()","bb96ae20":"from sklearn.model_selection import train_test_split","572de6ab":"x_train,x_test,y_train,y_test=train_test_split(X,y,test_size=0.3,random_state=1)","c2e66dce":"x_train.shape  , y_train.shape","6012a9ec":"x_test.shape , y_test.shape","4203ccb4":"from sklearn.svm import SVC\nfrom sklearn.metrics import accuracy_score","efc494cd":"svc_model = SVC()","1b6928e3":"svc_model.fit(x_train,y_train)","76ccd43c":"svc_pred = svc_model.predict(x_test)","0189ba22":"svc_score = accuracy_score(svc_pred,y_test)*100","0ccb7dfc":"svc_score","97691bd7":"from sklearn.naive_bayes import GaussianNB","90028bd9":"naive_bayes_model = GaussianNB()","f4afcc9b":"naive_bayes_model.fit(x_train,y_train)","054a816a":"naive_bayes_pred = naive_bayes_model.predict(x_test)","6f5ea9cb":"naive_bayes_score = accuracy_score(naive_bayes_pred,y_test)*100","8b0a3caf":"naive_bayes_score","3b1c2a0e":"from sklearn.cross_validation import cross_val_score\nfrom sklearn.cross_validation import KFold","8ecb378a":"k_fold = KFold(len(df), n_folds=10, shuffle=True, random_state=0)","6b87a22e":"svc_cv_model = SVC()","9d36ea7e":"svc_cv_score = cross_val_score(svc_cv_model,X,y,cv=k_fold,scoring = 'accuracy')*100","411edb55":"svc_cv_score","4ca0b02b":"svc_cv_score.mean()","342fb643":"naive_bayes_cv_model = GaussianNB()","f1e160d0":"naive_bayes_cv_score = cross_val_score(naive_bayes_cv_model,X,y,cv=k_fold,scoring = 'accuracy')*100","adcc735b":"naive_bayes_cv_score","fbd1d5b4":"naive_bayes_cv_score.mean()","bc9ae770":"naive_bayes_cv_model.fit(X,y)","8e24ca5c":"naive_bayes_cv_pred = naive_bayes_cv_model.predict(X)","4b0033ec":"naive_bayes_cv_score = accuracy_score(naive_bayes_cv_pred,y)*100","5340da82":"naive_bayes_cv_score","ce1d18ab":"from sklearn.tree import DecisionTreeClassifier","3963ebc9":"tree_model = DecisionTreeClassifier()","a509e5ea":"tree_cv_score = cross_val_score(tree_model,X,y,cv=k_fold,scoring = 'accuracy')*100","01b491b8":"tree_cv_score","45b597f2":"tree_cv_score.mean()","d80ae7ba":"from sklearn.linear_model import LogisticRegression","9163fe4f":"logistic_model = LogisticRegression()","24220a31":"logistic_cv_score = cross_val_score(logistic_model,X,y,cv=k_fold,scoring = 'accuracy')*100","de222cc0":"logistic_cv_score","ce34d214":"logistic_cv_score.mean()","a83e7c93":"from sklearn.neighbors import KNeighborsClassifier","aad54b71":"k_range = range(1,26)\nscores = []\nfor k in k_range :\n    KNN = KNeighborsClassifier(n_neighbors=k)\n    KNN.fit(x_train,y_train)\n    pred = KNN.predict(x_test)\n    scores.append(accuracy_score(pred,y_test)*100)\n    \nprint(pd.DataFrame(scores))","5568a9d7":"plt.plot(k_range,scores)\nplt.xlabel(\"K for KNN\")\nplt.ylabel(\"Testing scores\")\nplt.show()","8f27351d":"k_range = range(1,26)\nscores = []\nfor k in k_range :\n    KNN = KNeighborsClassifier(n_neighbors=k)\n    KNN_cv_score = cross_val_score(KNN,X,y,cv=k_fold,scoring = 'accuracy')*100\n    cv_score = scores.append(KNN_cv_score)\n    \nprint(pd.DataFrame(scores))","5643e816":"KNN_cv_score.mean()","01947f07":"# Size of test data","0272afc8":"We can see there are three format of data types:\n\nflaot\n\nint \n\nbool","48576d10":"# Applying Naive Bayes Classifier with K fold cross validation","e3081aa0":"KNN cross validation mean score","e3e2f349":"After applying k fold cross validatin our predictions are almost 82% accurate.","f760ef62":" 86% accuracy","04f43c3a":"with cross validion KNN score is 85%","ef5c08f0":"# Decision Tree","853cf0eb":"So our predictions are almost 85% accurate, i.e. we have identified 85% of the problems correctly for our SVM classifier model.","56e439a2":"set the value of k = 1 to 26","877a86b4":"After applying Naive Bayes our predictions are almost 83% accurate, i.e. we have identified 83% of the problems correctly for our Naive Bayes classifier model","fe7ac9b1":"We have 21 independent variables and 1 target variable, i.e. problems in the dataset.","1b4b77ce":"# Logistic Regression","29c47a6c":"# Applying Naive Bayes Classifier","b7277081":"# Implement KNN","86064da1":"# Read the data set and set appropriate name each column","432cfa67":"KNN score for train, test split method.","cc406b28":"No missing value.","652285e5":"# Set data path","e8b16892":"grapch of KNN for each K's value how testing score change","7559a524":"# Print the data types","b1ac6183":"# Applying SVM Classifier with K fold cross validation","2af1a631":"# Understanding the Data","8ffe4cb0":"# Shape of the data","63367006":"# Import required package","bc9cb39a":"In this section, we will look at the structure of the dataset. Firstly, we will check the features present in our data and then we will look at their data types.","6951587b":"# Split the data","ae8c4ac1":"# Applying SVM Classifier","ded046fb":"The dataset has been divided into training and validation part.\n\n70% data will use for train the model and rest of the 30% data will use for test the model.","63bca440":"After applying k fold cross validatin our predictions are almost 85% accurate, i.e. we have identified 85% of the problems correctly for our SVM classifier model.","a675c0b5":"# Missing value imputation","a2ee1d0b":"# KNN with k fold Cross Validation.","047c6568":"# Size of train data"}}