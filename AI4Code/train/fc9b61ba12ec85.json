{"cell_type":{"6168b800":"code","c231a7f1":"code","9ad6402f":"code","b8384560":"code","207b7f58":"code","2da4b3e2":"code","b825ac35":"code","d8780aa7":"code","2b32ffed":"code","f5f0c59a":"code","ca692253":"code","6a72266d":"code","ed586474":"code","50919373":"code","603fd2cc":"code","eefa9356":"code","01fa4864":"code","bd80c387":"code","67ace6f3":"code","8a0af929":"code","0438953b":"code","ce385352":"code","94814855":"code","6aaa23d4":"markdown","130b42c8":"markdown","f6a315db":"markdown","8655fd69":"markdown","83c7f793":"markdown","9a2404c9":"markdown","bebadd1a":"markdown","bbd2217c":"markdown","1831f262":"markdown"},"source":{"6168b800":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","c231a7f1":"# # Importing the libraries\n\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Conv2D\nfrom tensorflow.keras.layers import MaxPooling2D\nfrom tensorflow.keras.layers import Flatten\nfrom tensorflow.keras.layers import LeakyReLU\nfrom tensorflow.keras.layers import Dense\nimport numpy as np\nfrom matplotlib import pyplot as plt\n","9ad6402f":"classifier=Sequential()\n# 1st conv layer \nclassifier.add(Conv2D(6,(4,4),input_shape = (64, 64, 3),activation = 'relu'))\nclassifier.add(LeakyReLU(alpha=0.1))\n\n# 2nd conv layer \nclassifier.add(Conv2D(6,(2,2),activation = 'relu'))\n\n# pooling \nclassifier.add(MaxPooling2D(2,2))\n\n# Flattening \nclassifier.add(Flatten())\n\n#Dense layer wuth 128 neurons \nclassifier.add(Dense(units=128,activation='relu'))\n\n# output layer\nclassifier.add(Dense(units=4,activation='softmax'))\n\n# Model comiplation \nclassifier.compile(optimizer='adam',loss = 'categorical_crossentropy',metrics = ['accuracy'])\n","b8384560":"from tensorflow.keras.preprocessing.image import ImageDataGenerator\n\n#data preprocessing \n\n# training data\ntrain_datagen=ImageDataGenerator(rescale=1.\/255,shear_range=0.2,zoom_range=0.2,horizontal_flip=True)\n\n# validation data \nvalidation_datagen=ImageDataGenerator(rescale=1.\/255,shear_range=0.2,zoom_range=0.2,horizontal_flip=True)\n\n# test data \ntest_datagen=ImageDataGenerator(rescale=1.\/255)\n\n# reading data \ntraining_set = train_datagen.flow_from_directory('\/kaggle\/input\/cotton-disease-dataset\/Cotton Disease\/train',target_size = (64, 64),batch_size = 16,class_mode = 'categorical')\n\nvalidation_set = validation_datagen.flow_from_directory('\/kaggle\/input\/cotton-disease-dataset\/Cotton Disease\/val',target_size = (64, 64),batch_size = 16,class_mode = 'categorical')\n\ntest_set = test_datagen.flow_from_directory('\/kaggle\/input\/cotton-disease-dataset\/Cotton Disease\/test',target_size = (64, 64),batch_size = 16,class_mode = 'categorical')\n\n# train model \nmodel1 = classifier.fit_generator(training_set,steps_per_epoch = 100,\n                                  epochs = 3,\n                                  validation_data = validation_set,\n                                  validation_steps = 20)\n\n#save model \nclassifier.save(\"Cotton_Disease_Pred_keras.h5\")\nprint(\"Saved model\")","207b7f58":"from tensorflow.keras.models import load_model\nfrom tensorflow.keras.preprocessing import image\ntest_image = image.load_img(r'\/kaggle\/input\/cotton-disease-dataset\/Cotton Disease\/test\/diseased cotton plant\/dd (16)_iaip.jpg', target_size = (64, 64))\ntest_image = image.img_to_array(test_image)\ntest_image = np.expand_dims(test_image, axis = 0)\nmodel1=load_model('Cotton_Disease_Pred_keras.h5')\nresult = model.predict(test_image)\ntraining_set.class_indices\nif result[0][3] == 1:\n    prediction = 'fresh cotton plant'\n    print(prediction)\nelif result[0][2] == 1:\n    prediction = 'fresh cotton leaf'\n    print(prediction)\nelif result[0][1] == 1:\n    prediction='diseased cotton plant'\n    print(prediction)\nelif result[0][0] == 1:\n    prediction = 'diseased cotton leaf'\n    print(prediction)\nelse:\n    prediction = 'i am unable to predict '\n    print(prediction)\n\n# actual image    \nimg = plt.imread('\/kaggle\/input\/cotton-disease-dataset\/Cotton Disease\/test\/diseased cotton plant\/dd (16)_iaip.jpg')\nplt.imshow(img)","2da4b3e2":"from tensorflow.keras.applications.inception_v3 import InceptionV3, preprocess_input\nfrom tensorflow.keras.layers import Flatten, Dense\nfrom tensorflow.keras.models import Model\nimport matplotlib.pyplot as plt\nfrom glob import glob","b825ac35":"# re-size all the images to this\nIMAGE_SIZE = [224, 224]","d8780aa7":" conv_base = InceptionV3(input_shape=IMAGE_SIZE + [3], weights='imagenet', include_top=False)","2b32ffed":"# don't train existing weights\nfor layer in conv_base.layers:\n    layer.trainable = False","f5f0c59a":"# output classes\nfolders = glob('..\/input\/cotton-disease-dataset\/Cotton Disease\/train\/*')\nfolders","ca692253":"# flatten layers \nx = Flatten()(conv_base.output)","6a72266d":"# add dense layers \nprediction = Dense(len(folders), activation='softmax')(x)\n# model \nmodel = Model(inputs=conv_base.input, outputs=prediction)","ed586474":"# model summary, number of trainable parameters \nmodel.summary ()","50919373":"# compile and fit model on trainig data \nmodel.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\n","603fd2cc":"# Make sure you provide the same target size as initialied for the image size\ntraining_set = train_datagen.flow_from_directory('..\/input\/cotton-disease-dataset\/Cotton Disease\/train',\n                                                 target_size = (224, 224),\n                                                 batch_size = 32,\n                                                 class_mode = 'categorical')","eefa9356":"validation_set = validation_datagen.flow_from_directory('\/kaggle\/input\/cotton-disease-dataset\/Cotton Disease\/val',\n                                                        target_size = (224, 224),\n                                                        batch_size = 16,\n                                                        class_mode = 'categorical')","01fa4864":"test_set = test_datagen.flow_from_directory('..\/input\/cotton-disease-dataset\/Cotton Disease\/test',\n                                            target_size = (224, 224),\n                                            batch_size = 32,\n                                            class_mode = 'categorical')","bd80c387":"# fit \n# Run the cell. It will take some time to execute\nrunnning = model.fit_generator(training_set,validation_data=validation_set,epochs=9,\n                               steps_per_epoch=len(training_set),\n                               validation_steps=len(validation_set)\n)","67ace6f3":"# plot the loss\nplt.plot(runnning.history['loss'], label='train loss')\nplt.plot(runnning.history['val_loss'], label='val loss')\nplt.legend()\nplt.show()\nplt.savefig('LossVal_loss')\n\n# plot the accuracy\nplt.plot(runnning.history['accuracy'], label='train acc')\nplt.plot(runnning.history['val_accuracy'], label='val acc')\nplt.legend()\nplt.show()\nplt.savefig('AccVal_acc')","8a0af929":"model.save('inceptionv3.h5')","0438953b":"model_prediction = model.predict(test_set)\nmodel_prediction","ce385352":"model_prediction = np.argmax(model_prediction, axis =1 )\nmodel_prediction","94814855":"# make single prediction\ntest_image = image.load_img(r'\/kaggle\/input\/cotton-disease-dataset\/Cotton Disease\/test\/diseased cotton plant\/dd (16)_iaip.jpg', \n                            target_size = (224, 224))\ntest_image = image.img_to_array(test_image)\ntest_image = np.expand_dims(test_image, axis = 0)\nmodel=load_model('inceptionv3.h5')\nresult = model.predict(test_image)\ntraining_set.class_indices\nif result[0][3] == 1:\n    prediction = 'fresh cotton plant'\n    print(prediction)\nelif result[0][2] == 1:\n    prediction = 'fresh cotton leaf'\n    print(prediction)\nelif result[0][1] == 1:\n    prediction='diseased cotton plant'\n    print(prediction)\nelif result[0][0] == 1:\n    prediction = 'diseased cotton leaf'\n    print(prediction)\nelse:\n    prediction = 'i am unable to predict '\n    print(prediction)\n\n# actual image    \nimg = plt.imread('\/kaggle\/input\/cotton-disease-dataset\/Cotton Disease\/test\/diseased cotton plant\/dd (16)_iaip.jpg')\nplt.imshow(img)","6aaa23d4":"# InceptionV3 Model ","130b42c8":"# CNN model","f6a315db":"# part3: new prediction","8655fd69":"# Part 3 Make new prediction","83c7f793":"# part1: training","9a2404c9":"# Part 1:  Build model ","bebadd1a":"# Part 2: Training  model ","bbd2217c":"Models used: \n#1 CNN model \n#2 Inception V3 model \n","1831f262":"# part2: performance of model "}}