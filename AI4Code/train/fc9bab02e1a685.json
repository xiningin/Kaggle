{"cell_type":{"3b7c9009":"code","9efe397e":"code","5947b687":"code","9e814790":"code","c14e5758":"code","bcb8d436":"code","9385760c":"code","45a6fa3c":"code","53286828":"code","f3bf96c9":"code","e12c9368":"code","499538a0":"code","15548535":"code","bb85fecd":"code","8cc7833a":"code","62fbba62":"code","41c38619":"code","ea43c79a":"code","928d151d":"code","c1cf8f7a":"code","4e657300":"code","69cc4c2e":"code","09d04d5e":"code","1dae083b":"code","9a7f64e2":"code","01a72f81":"code","dcad3639":"code","f32506ff":"code","79296578":"code","4b07bd23":"code","0bf4c74c":"code","ccbe0214":"code","7aba6048":"code","bfceec96":"code","870e419d":"code","d0e27480":"code","c6200b13":"code","41782f9b":"code","c3f5d6fe":"code","85337b24":"code","93472ec0":"markdown","5c8277a9":"markdown","3afd2b1c":"markdown","d4852e6d":"markdown","b580cae7":"markdown","a6b962b0":"markdown","e035ee87":"markdown","7d3e7227":"markdown"},"source":{"3b7c9009":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n%matplotlib inline\nplt.figure(figsize=(16,5))\n","9efe397e":"# load train and test data\ntrain_data = pd.read_csv(\"..\/input\/titanic\/train.csv\", index_col='PassengerId')\ntest_data = pd.read_csv(\"..\/input\/titanic\/test.csv\", index_col='PassengerId')\n","5947b687":"# see columns names, types and missing values and head of test_data\nprint(train_data.info())\nprint(\"___________________________________\")\ntrain_data.head()","9e814790":"# print description of numerical data\ntrain_data.describe()","c14e5758":"# print number of missing values in train and test dataFrame\nprint(train_data.isnull().sum())\nprint(\"________________________\")\nprint(test_data.isnull().sum())\n","bcb8d436":"# plot heatmap with numeric features\nplt.figure(figsize=(16,5))\nsns.heatmap(data=train_data.corr(), vmin=-1, vmax=1, cmap='YlGnBu', annot=True)\nplt.show()","9385760c":"#PClass plot barplot\n\nsns.barplot(x=\"Pclass\", y=\"Survived\", data=train_data)\nplt.show()\n\n# print values\ntrain_data[['Pclass', 'Survived']].groupby(['Pclass'], as_index=False).mean().sort_values(\n    by='Survived', ascending=False)","45a6fa3c":"# SEX plot barplot\n\nsns.barplot(x=\"Sex\", y=\"Survived\", data=train_data)\nplt.show()\n\n# print values\ntrain_data[['Sex', 'Survived']].groupby(['Sex'], as_index=False).mean().sort_values(\n    by='Survived', ascending=False)","53286828":"# SibSp plot barplot\n\nsns.barplot(x=\"SibSp\", y=\"Survived\", data=train_data)\nplt.show()\n\n# print values\ntrain_data[['SibSp', 'Survived']].groupby(['SibSp'], as_index=False).mean().sort_values(\n    by='Survived', ascending=False)\n","f3bf96c9":"# Parch plot barplot\n\nsns.barplot(x=\"Parch\", y=\"Survived\", data=train_data)\nplt.show()\n\n# print values\ntrain_data[['Parch', 'Survived']].groupby(['Parch'], as_index=False).mean().sort_values(\n    by='Survived', ascending=False)","e12c9368":"# Embarked plot barplot\n\nsns.barplot(x=\"Embarked\", y=\"Survived\", data=train_data)\n\n# print values\ntrain_data[['Embarked', 'Survived']].groupby(['Embarked'], as_index=False).mean().sort_values(\n    by='Survived', ascending=False)","499538a0":"#sort the ages into logical categories\n\nbins = [-1, 0, 5, 12, 60, np.inf]\nlabels = ['Unknown', 'Baby', 'Child', 'Teenager-Adult', 'Senior']\nfor df in [train_data, test_data]:\n    df[\"Age\"] = df[\"Age\"].fillna(-0.5)\n    df['AgeGroup'] = pd.cut(df[\"Age\"], bins, labels = labels)\n\n#draw a bar plot of Age vs. survival\nsns.barplot(x=\"AgeGroup\", y=\"Survived\", data=train_data)\nplt.xticks(np.linspace(0,6,7), labels, rotation=45, ha=\"right\")\nplt.xlim(-0.6,4.6)\nplt.show()\n\n# print values\ntrain_data[['AgeGroup', 'Survived']].groupby(['AgeGroup'], as_index=False).mean().sort_values(\n    by='Survived', ascending=False)","15548535":"#sort the ages into logical categories\n\nbins = [-1, 8, 15, 30, np.inf]\nlabels = ['<8', '8-15', '15-31', '>31']\nfor df in [train_data, test_data]:\n    df[\"Fare\"] = df[\"Fare\"].fillna(-0.5)\n    df['FareGroup'] = pd.cut(df[\"Fare\"], bins, labels = labels)\n\n#draw a bar plot of Age vs. survival\nsns.barplot(x=\"FareGroup\", y=\"Survived\", data=train_data)\nplt.xticks(np.linspace(0,5,6), labels, rotation=45, ha=\"right\")\nplt.xlim(-0.6,3.6)\nplt.show()\n\n# print values\ntrain_data[['FareGroup', 'Survived']].groupby(['FareGroup'], as_index=False).mean().sort_values(\n    by='Survived', ascending=False)","bb85fecd":"# create Name Title\n\nfor df in [train_data, test_data]:\n    df['Title'] = df.Name.str.extract(' ([A-Za-z]+)\\.', expand=False)\n    df['Title'] = df['Title'].replace(['Lady', 'Countess','Capt', 'Col',\\\n                                             'Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare')\n    df['Title'] = df['Title'].replace('Mlle', 'Miss')\n    df['Title'] = df['Title'].replace('Ms', 'Miss')\n    df['Title'] = df['Title'].replace('Mme', 'Mrs')\n\ntrain_data[['Title', 'Survived']].groupby(['Title'], as_index=False).mean()","8cc7833a":"# join SibSp and Parch as FamilySize\n\nfor df in [train_data, test_data]:\n    df['FamilySize'] = (df['SibSp'] + df['Parch'] + 1)\n    df.loc[df['FamilySize'] > 4, 'FamilySize'] = 5\n\ntrain_data[['FamilySize', 'Survived']].groupby(['FamilySize'], as_index=False).mean().sort_values(\n        by='Survived', ascending=False)","62fbba62":"# fill missing Embarked with mode\n\nfor df in [train_data, test_data]:\n    df['Embarked'] = df['Embarked'].fillna(df['Embarked'].mode()[0])","41c38619":"# map each Sex value to a numerical value\n\nsex_mapping = {\"male\": 0, \"female\": 1}\nfor df in [train_data, test_data]:\n    df['Sex'] = df['Sex'].map(sex_mapping).astype(int)","ea43c79a":"# map each Age value to a numerical value and fill missing values\n\nage_group_mapping = {'Baby' : 0, 'Child' : 1, 'Teenager-Adult' : 2, 'Senior' : 3}\nfor df in [train_data, test_data]:\n    # Fill missing values based on Title    \n    df['AgeGroup'] = df['AgeGroup'].replace(['Unknown'], [None])\n    mr_age = df[df[\"Title\"] == \"Mr\"][\"AgeGroup\"].mode()[0] \n    miss_age = df[df[\"Title\"] == \"Miss\"][\"AgeGroup\"].mode()[0]\n    mrs_age = df[df[\"Title\"] == \"Mrs\"][\"AgeGroup\"].mode()[0] \n    master_age = df[df[\"Title\"] == \"Master\"][\"AgeGroup\"].mode()[0]\n    rare_age = df[df[\"Title\"] == \"Rare\"][\"AgeGroup\"].mode()[0]\n    title_age_mapping = {\"Mr\": mr_age, \"Miss\": miss_age, \"Mrs\": mrs_age, \"Master\": master_age, \"Rare\": rare_age}\n    df['AgeGroup'].fillna(df['Title'].map(title_age_mapping), inplace=True)\n    \n    # map strings to int\n    df['AgeGroup'] = df['AgeGroup'].map(age_group_mapping).astype('int')\n","928d151d":"# map each FareGroup value to a numerical value\n\nfare_mapping = {'<8' : 0, '8-15' : 1, '15-31' : 2, '>31' : 3}\nfor df in [train_data, test_data]:\n    df['FareGroup'] = df['FareGroup'].map(fare_mapping).astype('int')","c1cf8f7a":"# create CabinBool feature, that show if the passanger have or not a Cabin\n\nfor df in [train_data, test_data]:\n    df[\"CabinBool\"] = df[\"Cabin\"].notnull().astype('bool')","4e657300":"# drop unused data\n\nfor df in [train_data, test_data]:\n    df.drop(['Name'], axis = 1, inplace=True)\n    df.drop(['SibSp'], axis = 1, inplace=True)\n    df.drop(['Parch'], axis = 1, inplace=True)\n    df.drop(['Age'], axis = 1, inplace=True)\n    df.drop(['Cabin'], axis = 1, inplace=True)\n    df.drop(['Fare'], axis = 1, inplace=True)\n    df.drop(['Ticket'], axis = 1, inplace=True)","69cc4c2e":"# Checking the data in train and test DataFrames.\n\ntrain_data.info()\nprint(\"********************************************\")\ntest_data.info()","09d04d5e":"# prepare data to be used in the models\n\nfrom sklearn.model_selection import cross_val_score\ntrain_data = pd.get_dummies(train_data, columns=['Embarked', 'Title'], drop_first=True)\ny = train_data[\"Survived\"]\nX = train_data.drop(['Survived'], axis = 1)","1dae083b":"# Gaussian Naive Bayes\n\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.metrics import accuracy_score\n\ngaussian = GaussianNB()\nscores = cross_val_score(gaussian, X, y, cv=5)\nacc_gaussian = round(scores.mean() * 100, 2)\nprint(acc_gaussian)","9a7f64e2":"# Logistic Regression\n\nfrom sklearn.linear_model import LogisticRegression\n\nlogreg = LogisticRegression(solver='lbfgs')\nscores = cross_val_score(logreg, X, y, cv=5)\nacc_logreg = round(scores.mean() * 100, 2)\nprint(acc_logreg)","01a72f81":"# Support Vector Machines\n\nfrom sklearn.svm import SVC\n\nsvc = SVC(gamma='auto')\nscores = cross_val_score(svc, X, y, cv=5)\nacc_svc = round(scores.mean() * 100, 2)\nprint(acc_svc)","dcad3639":"# Linear SVC\n\nfrom sklearn.svm import LinearSVC\n\nlinear_svc = LinearSVC(max_iter=3000)\nscores = cross_val_score(linear_svc, X, y, cv=5)\nacc_linear_svc = round(scores.mean() * 100, 2)\nprint(acc_linear_svc)","f32506ff":"# Perceptron\n\nfrom sklearn.linear_model import Perceptron\n\nperceptron = Perceptron()\nscores = cross_val_score(perceptron, X, y, cv=5)\nacc_perceptron = round(scores.mean() * 100, 2)\nprint(acc_perceptron)","79296578":"#Decision Tree\n\nfrom sklearn.tree import DecisionTreeClassifier\n\ndecisiontree = DecisionTreeClassifier()\nscores = cross_val_score(decisiontree, X, y, cv=5)\nacc_decisiontree = round(scores.mean() * 100, 2)\nprint(acc_decisiontree)","4b07bd23":"# Random Forest\n\nfrom sklearn.ensemble import RandomForestClassifier\n\nrandomforest = RandomForestClassifier(max_depth=4, n_estimators=600)\nscores = cross_val_score(randomforest, X, y, cv=5)\nacc_randomforest = round(scores.mean() * 100, 2)\nprint(acc_randomforest)","0bf4c74c":"# KNN or k-Nearest Neighbors\n\nfrom sklearn.neighbors import KNeighborsClassifier\n\nknn = KNeighborsClassifier(n_neighbors = 5)\nscores = cross_val_score(knn, X, y, cv=5)\nacc_knn = round(scores.mean() * 100, 2)\nprint(acc_knn)","ccbe0214":"# Stochastic Gradient Descent\n\nfrom sklearn.linear_model import SGDClassifier\n\nsgd = SGDClassifier()\nscores = cross_val_score(sgd, X, y, cv=5)\nacc_sgd = round(scores.mean() * 100, 2)\nprint(acc_sgd)","7aba6048":"# Gradient Boosting Classifier\n\nfrom sklearn.ensemble import GradientBoostingClassifier\n\ngbk = GradientBoostingClassifier(n_estimators=500, learning_rate=0.11)\nscores = cross_val_score(gbk, X, y, cv=5)\nacc_gbk = round(scores.mean() * 100, 2)\nprint(acc_gbk)","bfceec96":"# Comparison of Models\n\nmodels = pd.DataFrame({\n    'Model': ['Support Vector Machines', 'KNN', 'Logistic Regression', \n              'Random Forest', 'Naive Bayes', 'Perceptron', 'Linear SVC', \n              'Decision Tree', 'Stochastic Gradient Descent', 'Gradient Boosting Classifier'],\n    'Score': [acc_svc, acc_knn, acc_logreg, acc_randomforest, acc_gaussian, acc_perceptron, \n              acc_linear_svc, acc_decisiontree, acc_sgd, acc_gbk]})\nmodels = models.sort_values(by='Score', ascending=False)\n\nprint(models)","870e419d":"randomforest.fit(X, y)\ndfFit = pd.DataFrame(randomforest.feature_importances_, train_data.drop(['Survived'], axis = 1).columns, \n                     columns=['Coefficient']).sort_values('Coefficient') \ndfFit.sort_values(by='Coefficient', ascending=False)","d0e27480":"# print classification report\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=123, test_size=0.2)\nrandomforest = RandomForestClassifier(max_depth=4, n_estimators=600)\nrandomforest.fit(X_train, y_train)\ny_pred = randomforest.predict(X_test)\n\nprint(classification_report(y_test,y_pred))","c6200b13":"# plot confusion matrix\n\nfrom sklearn.metrics import confusion_matrix\n\ncm = confusion_matrix(y_test,y_pred)\ncmNorm = [[cm[0,0]\/(cm[0,0]+cm[0,1]), cm[0,1]\/(cm[0,0]+cm[0,1])],\n         [cm[1,0]\/(cm[1,0]+cm[1,1]), cm[1,1]\/(cm[1,0]+cm[1,1])]]\ndf_cm = pd.DataFrame(cmNorm, index=['Real True', 'Real False'], columns=['Predict True', 'Predict False'])\nplt.figure(figsize = (6,3))\nplt.title(\"Normalized Confusion Matrix\")\nsns.heatmap(df_cm, annot=True, vmin=0, vmax=1, cmap='binary', fmt = \".3f\")\nplt.show()","41782f9b":"# plot roc_curve and auc\n\nfrom sklearn.metrics import roc_curve, auc\n\ny_pred_proba = randomforest.predict_proba(X_test)\ny_pred_proba = y_pred_proba[:, 1] \nFPR, TPR, _ = roc_curve(y_test, y_pred_proba)\nROC_AUC = auc(FPR, TPR)\nprint (\"Area Under ROC Curve (AUC):\", ROC_AUC)\n\nplt.figure(figsize =[8,7])\nplt.plot(FPR, TPR, label= 'ROC curve(area = %0.2f)'%ROC_AUC, linewidth= 4)\nplt.plot([0,1],[0,1], 'k--', linewidth = 4)\nplt.xlim([0.0,1.0])\nplt.ylim([0.0,1.0])\nplt.xlabel('False Positive Rate', fontsize = 18)\nplt.ylabel('True Positive Rate', fontsize = 18)\nplt.title('ROC for Titanic survivors', fontsize= 18)\nplt.show()","c3f5d6fe":"# plot precision_recall_curve\n\nfrom sklearn.metrics import precision_recall_curve\n\nprecision, recall, _ = precision_recall_curve(y_test, y_pred_proba)\nPR_AUC = auc(recall, precision)\n\nplt.figure(figsize=[8,7])\nplt.plot(recall, precision, label='PR curve (area = %0.2f)' % PR_AUC, linewidth=4)\nplt.xlabel('Recall', fontsize=18)\nplt.ylabel('Precision', fontsize=18)\nplt.title('Precision Recall Curve for Titanic survivors', fontsize=18)\nplt.legend(loc=\"lower right\")\nplt.show()","85337b24":"X_test = pd.get_dummies(test_data, columns=['Embarked', 'Title'], drop_first=True)\n\nrandomforest = RandomForestClassifier(max_depth=4, n_estimators=600)\nrandomforest.fit(X, y)\npredictions = randomforest.predict(X_test)\n\noutput = pd.DataFrame({'PassengerId': X_test.index.values, 'Survived': predictions})\noutput.to_csv('submission.csv', index=False)\nprint(\"Submission successfully\")","93472ec0":"# Fitting and comparing Models\n","5c8277a9":"# Clean and arrange data\n","3afd2b1c":"# Read and Explore Data","d4852e6d":"Support Vector Machines is the one of the best models (83.28%)","b580cae7":"# Validating the Model\n","a6b962b0":"# Data Analysis and Visualization","e035ee87":"# Import Libraries","7d3e7227":"# Creating Submission File\n"}}