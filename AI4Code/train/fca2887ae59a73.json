{"cell_type":{"ca6eb2dd":"code","9da8e417":"code","cef88940":"code","421c89b6":"code","8d49587b":"code","48dd0a92":"code","0d3605a6":"code","d14ac515":"code","756e4f65":"code","f4afc211":"code","5fbd3f9c":"code","02346401":"code","54df8e20":"code","93c067b5":"code","352a3ba9":"code","8402bc1b":"code","5bcfebbc":"code","a89c0bad":"code","a8a24f30":"code","5350b51f":"code","7e3c8e31":"code","eaa6aa2b":"code","131f7a0f":"code","458b0ab2":"code","9aa791b5":"code","001cc1d2":"code","7aabe314":"code","2472eb64":"code","2d565836":"code","022e716a":"code","2bcee9ea":"code","39050991":"code","9de8e0ea":"code","9675da18":"code","1318d6e2":"code","24199afc":"code","9bb86892":"code","2a56cc2c":"code","a95eb2cd":"code","7a06c3eb":"code","f3ce8d96":"code","ec112876":"code","9ec228ed":"code","e37eba41":"code","fed3afb6":"code","90e8d81d":"code","1c2076f0":"code","075ae167":"code","75c9a6cd":"code","f1a85264":"code","c468e908":"code","70af46c8":"code","7c0e2878":"code","e3d72c77":"code","d28c7821":"code","1f623cb4":"code","c01b8d94":"code","9f3facff":"code","3aadf94a":"markdown","750122e1":"markdown","f350a28d":"markdown","ce359ee4":"markdown","8589a0e9":"markdown","ad2d7df5":"markdown","d46bd016":"markdown"},"source":{"ca6eb2dd":"from datetime import datetime\nfrom time import time\n\nimport numpy as np\nimport pandas as opd\n\nfrom tqdm import tqdm\n\nimport matplotlib\nimport matplotlib.pyplot as plt\nfrom matplotlib.lines import Line2D\n\nfrom plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\nimport plotly as py\nimport plotly.graph_objs as go\nfrom plotly.subplots import make_subplots\n\ninit_notebook_mode(connected=True)\n\nimport warnings\nimport fastai\nfrom fastai.vision import *\nfrom fastai.callbacks import *\nfrom fastai.callbacks.hooks import *\nfrom fastai.utils.mem import *","9da8e417":"from fastai.utils.show_install import show_install; show_install()","cef88940":"!nvidia-smi","421c89b6":"def fmt_now():\n    return datetime.today().strftime('%Y%m%d-%H%M%S')","8d49587b":"path = Path('..\/input')\npath.ls()","48dd0a92":"path_img = path\/'train_images'\n\nfnames_train = get_image_files(path_img)\nfor f in fnames_train[:3]:\n    print(f)\nprint('\\nTotal number of training images: {}'.format(len(fnames_train)))","0d3605a6":"path_test = path\/'test_images'\n\nfnames_test = get_image_files(path_test)\nfor f in fnames_test[:3]:\n    print(f)\nprint('\\nTotal number of test images: {}'.format(len(fnames_test)))","d14ac515":"img_f = fnames_train[2]\nimg = open_image(img_f)\nimg.show(figsize=(10, 10))","756e4f65":"def split_img_label(img_lbl):\n    \"\"\"Return image and label from filename \"\"\"\n    s = img_lbl.split(\"_\")\n    assert len(s) == 2\n    return s[0], s[1]","f4afc211":"train = pd.read_csv(f'{path}\/train.csv')\ntrain['Image'] = train['Image_Label'].apply(lambda img_lbl: split_img_label(img_lbl)[0])\ntrain['Label'] = train['Image_Label'].apply(lambda img_lbl: split_img_label(img_lbl)[1])\ndel train['Image_Label']\ntrain.head(5)","5fbd3f9c":"train_with_mask = train.dropna(subset=['EncodedPixels'])\n\n#colors = ['gold', 'mediumturquoise', 'darkorange', 'lightgreen']\n\nfig = go.Figure(data=[go.Pie(labels=train_with_mask['Label'].value_counts().index, \n                             values=train_with_mask['Label'].value_counts().values)])\n\nfig.update_traces(hoverinfo=\"label+percent+name\")\n\nfig.update_layout(height=600, width=900, title = 'Class distribution')\n\nfig.show()","02346401":"class_counts = train.dropna(subset=['EncodedPixels']).groupby('Image')['Label'].nunique()\n\nfig = go.Figure()\n\nfig.add_trace(\n    go.Histogram(\n        x = class_counts,\n        xbins=dict(\n        start=0.5,\n        end=4.5,\n        size=1\n        ),\n    )\n)\n\nfig.update_layout(height=450, width=900, title = 'Distribution of no. labels per image')\n\nfig.update_layout(\n    xaxis_title_text='No. Image Class Labels', # xaxis label\n    yaxis_title_text='Count', # yaxis label\n    bargap=0.2, # gap between bars of adjacent location coordinates\n    bargroupgap=0.1 # gap between bars of the same location coordinates\n)\n\nfig.update_xaxes(tickvals=[1, 2, 3, 4])\n\nfig.show()","54df8e20":"train = train.pivot(index='Image', columns='Label', values='EncodedPixels')\nassert len(train) == len(fnames_train)\ntrain.head()","93c067b5":"def show_img_fn(fname, figsize=(10, 10)):\n    img = open_image(fname)\n    img.show(figsize=figsize)","352a3ba9":"def show_img_info(fname):\n    show_img_fn(path_img\/fname)\n    display(train.loc[[fname]])","8402bc1b":"unusual_imgs = [\"1588d4c.jpg\", \"c0306e5.jpg\", \"c26c635.jpg\", \"fa645da.jpg\", \"41f92e5.jpg\", \"e5f2f24.jpg\"]","5bcfebbc":"for fname in unusual_imgs:\n    img = open_image(path_img\/fname)\n    img.show(figsize=(5, 5), title=fname)     ","a89c0bad":"train_img_dims = (1400, 2100)","a8a24f30":"def rle_to_mask(rle, shape):\n    mask_img = open_mask_rle(rle, shape)\n    mask = mask_img.px.permute(0, 2, 1)\n    return mask","5350b51f":"def mask_to_rle(mask):\n    \"\"\" Convert binary 'mask' to RLE string \"\"\"\n    return rle_encode(mask.numpy().T)","7e3c8e31":"def test_mask_rle():\n    \"\"\" test case for mask RLE encode\/decode\"\"\"\n    mask_rle = train.iloc[0]['Fish']\n    mask = rle_to_mask(mask_rle, train_img_dims)\n    mask_rle_enc = mask_to_rle(mask)\n    assert mask_rle_enc == mask_rle\n    \n    print(mask.shape)\n    Image(mask).show()\n    \ntest_mask_rle()","eaa6aa2b":"item_list = (SegmentationItemList\n            .from_df(df=train.reset_index(), path=path_img, cols='Image')\n             #.use_partial_data(sample_pct=0.1)\n            .split_by_rand_pct(0.2)\n            )","131f7a0f":"class MultiLabelImageSegment(ImageSegment):\n    \"\"\"Store overlapping masks in separate image channels\"\"\"\n\n    def show(self, ax:plt.Axes=None, figsize:tuple=(3,3), title:Optional[str]=None, hide_axis:bool=True,\n        cmap:str='tab20', alpha:float=0.5, class_names=None, **kwargs):\n        \"Show the masks on `ax`.\"\n             \n        # put all masks into a single channel\n        flat_masks = self.px[0:1, :, :].clone()\n        for idx in range(1, self.shape[0]): # shape CxHxW\n            mask = self.px[idx:idx+1, :, :] # slice tensor to a single mask channel\n            # use powers of two for class codes to keep them distinguishable after sum \n            flat_masks += mask * 2**idx\n        \n        # use same color normalization in image and legend\n        norm = matplotlib.colors.Normalize(vmin=0, vmax=2**self.shape[0]-1)\n        ax = show_image(Image(flat_masks), ax=ax, hide_axis=hide_axis, cmap=cmap, norm=norm,\n                        figsize=figsize, interpolation='nearest', alpha=alpha, **kwargs)\n        \n     # custom legend, see https:\/\/matplotlib.org\/3.1.1\/gallery\/text_labels_and_annotations\/custom_legends.html\n        cm = matplotlib.cm.get_cmap(cmap)\n        legend_elements = []\n        for idx in range(self.shape[0]):\n            c = 2**idx\n            label = class_names[idx] if class_names is not None else f\"class {idx}\"\n            line = Line2D([0], [0], color=cm(norm(c)), label=label, lw=4)\n            legend_elements.append(line)\n        ax.legend(handles=legend_elements)\n        \n        # debug info\n        # ax.text(10, 10, f\"px={self.px.size()}\", {\"color\": \"white\"})\n        \n        if title: ax.set_title(title)\n\n    def reconstruct(self, t:Tensor): \n        return MultiClassImageSegment(t)\n        ","458b0ab2":"# source: https:\/\/forums.fast.ai\/t\/unet-how-to-get-4-channel-output\/54674\/4\ndef bce_logits_floatify(input, target, reduction='mean'):\n    return F.binary_cross_entropy_with_logits(input, target.float(), reduction=reduction)","9aa791b5":"class MultiLabelSegmentationLabelList(SegmentationLabelList):\n    \"\"\"Return a single image segment with all classes\"\"\"\n    # adapted from https:\/\/forums.fast.ai\/t\/how-to-load-multiple-classes-of-rle-strings-from-csv-severstal-steel-competition\/51445\/2\n    \n    def __init__(self, items:Iterator, src_img_size=None, classes:Collection=None, **kwargs):\n        super().__init__(items=items, classes=classes, **kwargs)\n        self.loss_func = bce_logits_floatify\n        self.src_img_size = src_img_size\n        # add attributes to copy by new() \n        self.copy_new += [\"src_img_size\"]\n    \n    def open(self, rles):        \n        # load mask at full resolution\n        masks = torch.zeros((len(self.classes), *self.src_img_size)) # shape CxHxW\n        for i, rle in enumerate(rles):\n            if isinstance(rle, str):  # filter out NaNs\n                masks[i] = rle_to_mask(rle, self.src_img_size)\n        return MultiLabelImageSegment(masks)\n    \n    def analyze_pred(self, pred, thresh:float=0.0):\n        # binarize masks\n        return (pred > thresh).float()\n    \n    \n    def reconstruct(self, t:Tensor): \n        return MultiLabelImageSegment(t)","001cc1d2":"class_names = [\"Fish\", \"Flower\", \"Gravel\", \"Sugar\"]","7aabe314":"def get_masks_rle(img):\n    \"\"\"Get RLE-encoded masks for this image\"\"\"\n    img = img.split(\"\/\")[-1]  # get filename only\n    return train.loc[img, class_names].to_list()","2472eb64":"img_size = (84, 132)  # use multiple of 4\nimg_size","2d565836":"classes = [0, 1, 2, 3] # no need for a \"void\" class: if a pixel isn't in any mask, it is not labelled\nitem_list = item_list.label_from_func(func=get_masks_rle, label_cls=MultiLabelSegmentationLabelList, \n                                      classes=classes, src_img_size=train_img_dims)","022e716a":"item_list = item_list.add_test_folder(path_test, label=\"\")","2bcee9ea":"\nbatch_size = 8\n\n# TODO add data augmentation\ntfms = ([], [])\n# tfms = get_transforms()\n\nitem_list = item_list.transform(tfms, tfm_y=True, size=img_size)","39050991":"data = (item_list\n        .databunch(bs=batch_size)\n        .normalize(imagenet_stats) # use same stats as pretrained model\n       )  \nassert data.test_ds is not None","9de8e0ea":"data.show_batch(2, figsize=(15, 10), class_names=class_names)","9675da18":"def dice_metric(pred, targs, threshold=0):\n    pred = (pred > threshold).float()\n    targs = targs.float()  # make sure target is float too\n    return 2.0 * (pred*targs).sum() \/ ((pred+targs).sum() + 1.0)","1318d6e2":"metrics = [dice_metric]\n\ncallback_fns = [\n    # update a graph of learner stats and metrics after each epoch\n    ShowGraph,\n\n    # save model at every metric improvement\n    partial(SaveModelCallback, every='improvement', monitor='dice_metric', name=f\"{fmt_now()}_unet_resnet18_stage1_best\"),\n    \n    # stop training if metric no longer improve\n    partial(EarlyStoppingCallback, monitor='dice_metric', min_delta=0.01, patience=2),\n]\n\nlearn = unet_learner(data, models.resnet18, metrics=metrics, wd=1e-2, callback_fns=callback_fns)\nlearn.model_dir = \"\/kaggle\/working\/\"  # point to writable directory","24199afc":"learn.loss_func","9bb86892":"learn.summary()","2a56cc2c":"learn.lr_find()\nlearn.recorder.plot()","a95eb2cd":"learn.fit_one_cycle(15, max_lr=1e-4)","7a06c3eb":"learn.recorder.plot_metrics()","f3ce8d96":"learn.save(f\"{fmt_now()}_unet_resnet18_stage1\", return_path=True)","ec112876":"!ls -lth {learn.model_dir}","9ec228ed":"learn.unfreeze()","e37eba41":"learn.lr_find()\nlearn.recorder.plot()","fed3afb6":"learn.fit_one_cycle(15, max_lr=slice(5e-7, 5e-6))","90e8d81d":"learn.save(f\"{fmt_now()}_unet_resnet18_stage2\", return_path=True)","1c2076f0":"img_size = (336, 528)\nitem_list = item_list.label_from_func(func=get_masks_rle, label_cls=MultiLabelSegmentationLabelList, \n                                      classes=classes, src_img_size=train_img_dims)\nitem_list = item_list.add_test_folder(path_test, label=\"\")\nbatch_size = 8\ntfms = ([], [])\nitem_list = item_list.transform(tfms, tfm_y=True, size=img_size)","075ae167":"data = (item_list\n        .databunch(bs=batch_size)\n        .normalize(imagenet_stats) # use same stats as pretrained model\n       )  \nassert data.test_ds is not None","75c9a6cd":"data.show_batch(2, figsize=(15, 10), class_names=class_names)","f1a85264":"learn.data = data\ndata.train_ds[0][0].shape","c468e908":"learn.freeze()","70af46c8":"learn.lr_find()\nlearn.recorder.plot()","7c0e2878":"lr = 2e-4","e3d72c77":"learn.fit_one_cycle(5, slice(lr))","d28c7821":"lr=5e-5","1f623cb4":"learn.fit_one_cycle(5, slice(lr))","c01b8d94":"learn.save(f\"{fmt_now()}_unet_resnet18_stage3\", return_path=True)","9f3facff":"learn.show_results(imgsize=8, class_names=class_names)","3aadf94a":"## Load images","750122e1":"### Convert masks from\/to RLE","f350a28d":"## Training","ce359ee4":"## Further tuning with larger images","8589a0e9":"## EDA","ad2d7df5":"### Broken images","d46bd016":"### unfreeze and differential learing rate"}}