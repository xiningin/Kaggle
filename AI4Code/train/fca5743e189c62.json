{"cell_type":{"ca0dc33a":"code","4f42b44b":"code","f19d242d":"code","f81bf82b":"code","015ae3df":"code","8cbd536f":"code","5d024469":"code","5f31fe80":"code","ced6e319":"code","d38b267c":"code","9c1e3c0d":"code","70d4e85e":"code","0b1dbea5":"code","5e60c5d2":"code","a9edda65":"code","149e0d22":"code","f18ab5fc":"code","7739219b":"code","2f0f242e":"code","98e2ccec":"code","b8739f8f":"code","2c2745a8":"code","78c2e836":"code","f4c9420c":"code","e58bf161":"code","c1d8e58c":"code","b74b17af":"code","b3dce238":"code","249a1e42":"code","5ea3eb70":"code","015d94b6":"code","437967f3":"code","9b69a746":"code","156eb417":"markdown","de1c694e":"markdown","7979e029":"markdown"},"source":{"ca0dc33a":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\n# import os\n# for dirname, _, filenames in os.walk('\/kaggle\/input'):\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","4f42b44b":"traindf = pd.read_csv(\"\/kaggle\/input\/plant-pathology-2020-fgvc7\/train.csv\")\ntestdf = pd.read_csv(\"\/kaggle\/input\/plant-pathology-2020-fgvc7\/test.csv\")\nsubmissiondf = pd.read_csv(\"\/kaggle\/input\/plant-pathology-2020-fgvc7\/sample_submission.csv\")","f19d242d":"traindf.head()","f81bf82b":"import matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly.express as px","015ae3df":"sns.countplot(traindf['healthy'])","8cbd536f":"sns.countplot(traindf['rust'])","5d024469":"sns.countplot(traindf['multiple_diseases'])","5f31fe80":"sns.countplot(traindf['scab'])","ced6e319":"testdf.head()","d38b267c":"# Loading train images \nimport os\nimport cv2\nimport glob\nimg_size = 224\npath = \"\/kaggle\/input\/plant-pathology-2020-fgvc7\/images\/\"\n\ntestimages = []\ntrainimages = []\n\nfor img in traindf['image_id']:\n    imgpath = os.path.join(path,img) + \".jpg\" \n    IMAGE = cv2.imread(imgpath)\n    IMAGE=cv2.resize(IMAGE,(img_size,img_size),interpolation=cv2.INTER_AREA)\n    trainimages.append(IMAGE)\n    \nfor img in testdf['image_id']:\n    imgpath = os.path.join(path,img) + \".jpg\"\n    IMAGE = cv2.imread(imgpath)\n    IMAGE=cv2.resize(IMAGE,(img_size,img_size),interpolation=cv2.INTER_AREA)\n    testimages.append(IMAGE)   \n","9c1e3c0d":"len(trainimages) , len(testimages)","70d4e85e":"fig,ax = plt.subplots(1,4,figsize=(15,15))\nfor i in range(4):\n    ax[i].imshow(trainimages[i])","0b1dbea5":"fig,ax = plt.subplots(1,4,figsize=(15,15))\nfor i in range(4):\n    ax[i].imshow(testimages[i])","5e60c5d2":"# creating X and Y data for training\n\nfrom keras.preprocessing.image import img_to_array\n\nX = np.ndarray(shape=(len(trainimages),img_size,img_size,3),dtype = np.float32)\ni = 0\nfor img in trainimages:\n    X[i] = img_to_array(img)\n    X[i] = trainimages[i]\n    i += 1\nX = X\/255.0","a9edda65":"y = traindf.drop(columns=['image_id']) # take rest 4 columns\ny = np.array(y.values)\ny","149e0d22":"X.shape,y.shape","f18ab5fc":"# similary for final testing data\nX_for_testing = np.ndarray(shape=(len(testimages),img_size,img_size,3),dtype = np.float32)\ni = 0\nfor img in testimages:\n    X_for_testing[i] = img_to_array(img)\n    X_for_testing[i] = testimages[i]\n    i += 1\nX_for_testing = X_for_testing\/255.0\n","7739219b":"X_for_testing.shape","2f0f242e":"# DATA SPLITSSSSS\nfrom sklearn.model_selection import train_test_split\n\nX_train,X_val,y_train,y_val = train_test_split(X,y,test_size=0.2,random_state=42)\n\nprint(X_train.shape,X_val.shape,y_train.shape,y_val.shape)","98e2ccec":"# HANDLING UNEQUAL DATASET USING SMOTE \n\nfrom imblearn.over_sampling import SMOTE\n\nsmote = SMOTE(random_state=42)\n\nX_train,y_train = smote.fit_resample(X_train.reshape((-1,img_size*img_size*3)),y_train)\n\nX_train = X_train.reshape((-1,img_size,img_size,3))\n\nX_train.shape,y_train.shape,y_train.sum(axis=0)","b8739f8f":"print(X_train.shape,X_val.shape,y_train.shape,y_val.shape)","2c2745a8":"import tensorflow as tf\nfrom keras.applications.vgg16 import VGG16\nfrom keras.applications.vgg16 import preprocess_input\nfrom tensorflow import keras \nfrom tensorflow.keras.models import Model\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.regularizers import l1,l2\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.optimizers import SGD, Adam, RMSprop, Adagrad, Nadam, Adadelta, Adamax\nfrom tensorflow.keras.layers import Dropout , BatchNormalization , Flatten , MaxPool2D,MaxPooling2D , Activation , Dense , Conv2D , InputLayer","78c2e836":"datagen = ImageDataGenerator(rotation_range=45,\n                             shear_range=.25,\n                              zoom_range=.25,\n                              width_shift_range=.25,\n                              height_shift_range=.25,\n                              rescale=1\/255,\n                              brightness_range=[.5,1.5],\n                              horizontal_flip=True,\n                              vertical_flip=True,\n                              fill_mode='nearest')","f4c9420c":"from keras.callbacks import ReduceLROnPlateau\nfrom keras.callbacks import EarlyStopping\n\nIMAGE_SIZE = [224, 224]\nvgg = VGG16(input_shape=IMAGE_SIZE + [3], weights='imagenet', include_top=False)\nfor layer in vgg.layers:\n  layer.trainable = False\nx = Flatten()(vgg.output)\nx = Dense(256, activation='relu')(x)\nprediction = Dense(4, activation='softmax')(x)\nmodel = Model(inputs=vgg.input, outputs=prediction)\nprint(model.summary())\nmodel.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\n\nlr=ReduceLROnPlateau(monitor='val_accuracy',factor=.5,patience=10,min_lr=.000001,verbose=1)\nes=EarlyStopping(monitor='val_loss', patience=20)\ncallbacks = [lr,es]","e58bf161":"batch_size = 32\nepochs = 200\n\nvgghistory = model.fit_generator(datagen.flow(X_train,y_train,batch_size=batch_size),epochs=epochs,\n                                callbacks=callbacks,\n                                steps_per_epoch = X_train.shape[0]\/\/batch_size,\n                                verbose=1,\n                                validation_data = datagen.flow(X_val,y_val,batch_size=batch_size),\n                                validation_steps = X_val.shape[0]\/\/batch_size)\n","c1d8e58c":"img_size=224\nreg = 0.0005\n\nmodel = Sequential()\n\nmodel.add(Conv2D(32, kernel_size=(5,5),activation='relu', input_shape=(img_size, img_size, 3), kernel_regularizer=l2(reg)))\nmodel.add(BatchNormalization(axis=-1,center=True,scale=False))\nmodel.add(Conv2D(128, kernel_size=(5,5),activation='relu', kernel_regularizer=l2(reg)))\nmodel.add(BatchNormalization(axis=-1,center=True,scale=False))\nmodel.add(MaxPooling2D(pool_size=(2,2), padding='SAME'))\nmodel.add(Dropout(.25))\n\nmodel.add(Conv2D(32, kernel_size=(3,3),activation='relu', kernel_regularizer=l2(reg)))\nmodel.add(BatchNormalization(axis=-1,center=True,scale=False))\nmodel.add(Conv2D(128, kernel_size=(3,3),activation='relu',kernel_regularizer=l2(reg)))\nmodel.add(BatchNormalization(axis=-1,center=True,scale=False))\nmodel.add(MaxPooling2D(pool_size=(2,2), padding='SAME'))\nmodel.add(Dropout(.25))\n\nmodel.add(Conv2D(128, kernel_size=(5,5),activation='relu', kernel_regularizer=l2(reg)))\nmodel.add(BatchNormalization(axis=-1,center=True,scale=False))\nmodel.add(Conv2D(512, kernel_size=(5,5),activation='relu',kernel_regularizer=l2(reg)))\nmodel.add(BatchNormalization(axis=-1,center=True,scale=False))\nmodel.add(MaxPooling2D(pool_size=(2,2), padding='SAME'))\nmodel.add(Dropout(.25))\n\nmodel.add(Conv2D(128, kernel_size=(3,3),activation='relu',kernel_regularizer=l2(reg)))\nmodel.add(BatchNormalization(axis=-1,center=True,scale=False))\nmodel.add(Conv2D(512, kernel_size=(3,3),activation='relu',kernel_regularizer=l2(reg)))\nmodel.add(BatchNormalization(axis=-1,center=True,scale=False))\nmodel.add(MaxPooling2D(pool_size=(2,2), padding='SAME'))\nmodel.add(Dropout(.25))\n\nmodel.add(Flatten())\nmodel.add(Dense(300,activation='relu'))\nmodel.add(BatchNormalization(axis=-1,center=True,scale=False))\nmodel.add(Dropout(.25))\nmodel.add(Dense(200,activation='relu'))\nmodel.add(BatchNormalization(axis=-1,center=True,scale=False))\nmodel.add(Dropout(.25))\nmodel.add(Dense(100,activation='relu'))\nmodel.add(BatchNormalization(axis=-1,center=True,scale=False))\nmodel.add(Dropout(.25))\nmodel.add(Dense(4,activation='softmax'))\n\nmodel.summary()","b74b17af":"from keras.callbacks import ReduceLROnPlateau\nfrom keras.callbacks import EarlyStopping\nbatch_size = 32\nepochs = 200\n\nmodel.compile(optimizer='adam',\n              loss='categorical_crossentropy',\n              metrics=['accuracy']\n              )\n\nlr=ReduceLROnPlateau(monitor='val_accuracy',factor=.5,patience=10,min_lr=.000001,verbose=1)\nes=EarlyStopping(monitor='val_loss', patience=20)\ncallbacks = [lr,es]\n\nhistory = model.fit_generator(datagen.flow(X_train,y_train,batch_size=batch_size),epochs=epochs,\n                                callbacks=callbacks,\n                                steps_per_epoch = X_train.shape[0]\/\/batch_size,\n                                verbose=1,\n                                validation_data = datagen.flow(X_val,y_val,batch_size=batch_size),\n                                validation_steps = X_val.shape[0]\/\/batch_size)","b3dce238":"import plotly.express as px\n\nhist = history.history\npx.line(\n    hist, x=range(1, len(hist['loss'])+1), y=['accuracy', 'val_accuracy'], \n    title='Model Accuracy', labels={'x': 'Epoch', 'value': 'Accuracy'}\n)","249a1e42":"px.line(\n    hist, x=range(1, len(hist['loss'])+1), y=['loss', 'val_loss'], \n    title='Model Loss', labels={'x': 'Epoch', 'value': 'Loss'}\n)","5ea3eb70":"pred = model.predict(X_for_testing).argmax(axis=0)\npred","015d94b6":"testids = testdf['image_id']","437967f3":"pred = model.predict(X_for_testing)\nprint(pred)\nres = pd.DataFrame()\nres['image_id'] = testids\nres['healthy'] = pred[:, 0]\nres['multiple_diseases'] = pred[:, 1]\nres['rust'] = pred[:, 2]\nres['scab'] = pred[:, 3]\nres.to_csv('submission.csv', index=False)\nres.head(10)","9b69a746":"model.save(\"plantpathology.h5\")","156eb417":"# VGG16 MODEL","de1c694e":"# Lets try with custom model -\n\nI have used the model used in this notebook - https:\/\/www.kaggle.com\/nightwolfbrooks\/data-augmentation-and-keras-cnn\n\n","7979e029":"# Image Data Augmentation"}}