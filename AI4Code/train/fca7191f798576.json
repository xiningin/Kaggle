{"cell_type":{"0b9aa40d":"code","71d0d2b8":"code","5d2e2cb1":"code","95f4918e":"code","c4a95023":"code","632c12fb":"code","919b98c6":"code","b2eab061":"code","08313dcd":"code","5971b1ce":"code","ac99efac":"code","d630c81a":"code","01be7f27":"code","d9cbd9b1":"code","9ce19e65":"code","5b47cc4d":"code","c4ea8030":"code","97dd4d1f":"code","391310a5":"code","af764f2f":"code","9acfab30":"code","29a0025f":"code","0c6d5c26":"code","803a7080":"markdown","15aa9134":"markdown","ec65f0f4":"markdown"},"source":{"0b9aa40d":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\n# data analysis and wrangling\nimport pandas as pd\nimport numpy as np\nimport random as rnd\nimport copy\n\n# visualization\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n# machine learning\nimport xgboost as xgb\nfrom xgboost import XGBClassifier, XGBRegressor\n# from xgboost import plot_importance, plot_tree\nfrom sklearn.model_selection import GridSearchCV\n\n","71d0d2b8":"class data_preprocessing:\n    def __init__(self,data):\n        self.data = data\n        self.null_data = []\n        self.droped_data = []\n        self.num_data=[]\n        self.cat_data = []\n#         null check\n    def null_check(self):\n        total = self.data.isnull().sum().sort_values(ascending=False)\n        percent = (self.data.isnull().sum()\/self.data.isnull().count()).sort_values(ascending=False)\n        self.null_data = pd.concat([total,percent], axis = 1, keys = ['Total', 'Percent'])\n        print(self.null_data)\n        print(self.data.isnull().sum().max())\n        return self.null_data\n    def drop_null_data(self,n):\n        self.data = self.data.drop((self.null_data[self.null_data['Total']>n]).index, 1)\n        print(self.data.isnull().sum().max())\n        return self.data\n#   drop data\n    def drop_data(self,names):\n        self.data.drop(names, axis=1,inplace=True)\n#     numerical,categorical data\n    def decollate_cat_data(self):\n        self.cat_data = self.data.select_dtypes(include = [\"object\"]).columns\n        return self.cat_data\n    def decollate_num_data(self):\n        self.num_data = self.data[self.data.select_dtypes(exclude = [\"object\"]).columns]\n        return self.num_data\n    def decollate_data(self):\n        self.cat_data = self.data.select_dtypes(include = [\"object\"]).columns\n        self.num_data = self.data[self.data.select_dtypes(exclude = [\"object\"]).columns]\n#         print(self.data.select_dtypes(include = [\"object\"]).columns)\n#         print(self.data.select_dtypes(exclude=[\"object\"]).columns)\n        print(\"cat_data, num_data\")\n        return self.cat_data , self.num_data\n#     LabelEncoder\n    def label_encoder(self):\n        pass\n#     data scaling\n    def standard_scaling(self,row):\n        scaler = StandardScaler()\n        scaler.fit(self.num_data[row])\n        scaling_result = scaler.transform(self.num_data[row])\n        return scaling_result\n    def robust_scaling(self,row):\n        scaler = RonustScaler()\n        scaler.fit(self.num_data[row])\n        scaling_result = scaler.transform(self.num_data[row])\n        return scaling_result\n    def min_max_scaling(self,row):\n        scaler = MinMaxScaler()\n        scaler.fit(self.num_data[row])\n        scaling_result = scaler.transform(self.num_data[row])\n        return scaling_result\n    def nomalizer(self,row):\n        scaler = Nomalizer()\n        scaler.fit(self.num_data[row])\n        scaling_result = scaler.transform(self.num_data[row])\n        return scaling_result\n#     one-hot incoder and concat data\n    def getdummies_and_concat(self):\n        self.decollate_data()\n        self.cat_data = pd.get_dummies(self.data[self.cat_data], prefix = self.cat_data)\n        print(self.cat_data)\n        self.data = pd.concat([self.cat_data,self.num_data],axis=1)\n#         return self.data","5d2e2cb1":"train = data_preprocessing(pd.read_csv(\"..\/input\/titanic\/train.csv\"))\ntest = data_preprocessing(pd.read_csv(\"..\/input\/titanic\/test.csv\"))\ngender_submission = pd.read_csv(\"..\/input\/titanic\/gender_submission.csv\")","95f4918e":"train.null_check()\ntest.null_check()","c4a95023":"train.drop_data([\"Name\",\"Cabin\",\"Ticket\",\"PassengerId\"])\ntest.drop_data([\"Name\",\"Cabin\",\"Ticket\",\"PassengerId\"])","632c12fb":"train.data[\"Embarked\"].fillna(('S'), inplace= True)\ntrain.data['Fare'].fillna(np.min(train.data['Fare']), inplace=True)","919b98c6":"test.data[\"Embarked\"].fillna(('S'), inplace= True)\ntest.data['Fare'].fillna(np.min(test.data['Fare']), inplace=True)","b2eab061":"age_avg = train.data['Age'].mean()\nage_std = train.data['Age'].std()\ntrain.data['Age'].fillna(np.random.randint(age_avg - age_std,age_avg + age_std), inplace=True)","08313dcd":"age_avg = test.data['Age'].mean()\nage_std = test.data['Age'].std()\ntest.data['Age'].fillna(np.random.randint(age_avg - age_std,age_avg + age_std), inplace=True)","5971b1ce":"train.decollate_data()","ac99efac":"train.getdummies_and_concat()\ntest.getdummies_and_concat()","d630c81a":"X_train = train.data.drop('Survived',axis=1)\ny_train = train.data['Survived']\nX_train , y_train","01be7f27":"# model = XGBClassifier()","d9cbd9b1":"# parameters = {\n#               'objective':['reg:linear'],\n#               'learning_rate': [0.1,0.07], #so called `eta` value\n#               'max_depth': [4,7],\n#               'min_child_weight': [1,4],\n#               'silent': [1],\n#               'subsample': [0.5],\n#               'colsample_bytree': [0.5],\n#               'n_estimators': [300,400,500]\n# }","9ce19e65":"# grid = GridSearchCV(model,parameters, cv = 2,n_jobs = 5,verbose=True)","5b47cc4d":"# grid.fit(X_train, y_train)","c4ea8030":"# print(grid.best_score_)\n# print(grid.best_params_)\n# 'colsample_bytree': 0.5, 'learning_rate': 0.07, 'max_depth': 4, 'min_child_weight': 4, 'n_estimators': 300, 'objective': 'reg:linear', 'silent': 1, 'subsample': 0.5","97dd4d1f":"model = XGBClassifier(learning_rate = 0.07,\n                      max_depth=4,\n                      n_estimators = 300,\n                      gamma=1,\n                      silent= 1,\n                      colsample_bytree= 0.5,\n                      min_child_weight= 4,\n                      subsample=0.5\n                     )","391310a5":"model.fit(X_train, y_train, \n             verbose=False)","af764f2f":"y_pred = model.predict(test.data)\ny_pred","9acfab30":"sub=pd.read_csv(\"..\/input\/titanic\/gender_submission.csv\")","29a0025f":"sub['Survived'] = list(map(int, y_pred))","0c6d5c26":"sub.to_csv('submission.csv', index=False)","803a7080":"survived","15aa9134":"Training","ec65f0f4":"hyperparameters tuning"}}