{"cell_type":{"2b802c2a":"code","a41f87a3":"code","c9785383":"code","9bfd9584":"code","218195fb":"code","6e755d8a":"code","ca20b2df":"code","dffdac0a":"code","065b112c":"code","531d7639":"code","5123bb24":"code","27b7f65a":"code","6c30f30e":"code","b3e6bd78":"code","3ab3f317":"code","5ca4f71c":"code","135ba2a8":"code","bbd9f80e":"code","bc4680c6":"code","99ffeeb9":"code","67902dd7":"code","b38e9a05":"code","6f6e8b4e":"code","68751ed0":"code","c88e008b":"code","9e4c9331":"code","c2c511dc":"code","33efa10c":"code","fc790cb4":"code","13f68db1":"code","f8dd8fe5":"code","fedc04af":"code","0a5f214a":"code","cdc9b2bd":"code","704e50a2":"code","722cb100":"code","21e5874e":"code","2e4bb6bb":"code","d0729fc4":"code","05b44c8e":"code","729ecf2a":"code","15b002e2":"code","eb3ae76e":"code","7c7c308d":"code","4b679281":"code","16f15c16":"code","9659c5a1":"code","855af29e":"code","0ce2d0fd":"code","7e3a7380":"code","14edeb38":"markdown","2364c6ee":"markdown","105a8e2f":"markdown","fa848452":"markdown","2c10f16d":"markdown","b8432061":"markdown","3687a248":"markdown","8fb61023":"markdown","39ecbce2":"markdown","769ca55e":"markdown","8dbb4ce1":"markdown","7d87bb2f":"markdown","79b0e5ac":"markdown","88e12008":"markdown","cbdd3dd0":"markdown","06ba8ac2":"markdown","95bd04d2":"markdown","bdbe0cbe":"markdown","593e3b48":"markdown","28938f90":"markdown","81d1266d":"markdown","30a8f9c8":"markdown","38b97a80":"markdown","7f8e31d1":"markdown","2e2b7ff5":"markdown","cddd9a9e":"markdown","d50b1fb7":"markdown","0f74dacf":"markdown","0c4405c2":"markdown","5a4c838c":"markdown","242d4dfd":"markdown","abcc7201":"markdown","a3268a9d":"markdown","d99b790f":"markdown","4acf67b5":"markdown","1c213e80":"markdown","1692fa5d":"markdown","62dc175c":"markdown","b86643fc":"markdown","6c983981":"markdown","7a6923c9":"markdown"},"source":{"2b802c2a":"# import needed packages\nimport math\nimport pandas as pd\nimport numpy as np\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.model_selection import cross_val_score, GridSearchCV\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import ParameterGrid\nimport sklearn.metrics as metrics\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline","a41f87a3":"# load datasets\ntrain = pd.read_csv('..\/input\/titanic\/train.csv')\ntest = pd.read_csv('..\/input\/titanic\/test.csv')","c9785383":"train.describe()","9bfd9584":"# check for NaN values\ntrain.isnull().sum()","218195fb":"# make a copy of our dataset\ntrain1 = train.copy()\n\n# drop columns\ntrain1.drop(columns=['Name', 'Cabin', 'PassengerId'], inplace=True)","6e755d8a":"# drop rows with NaN\ntrain1.dropna(inplace=True)","ca20b2df":"# convert string columns to type int\n# we will put this into a function since we will be using this later on\ndef string_to_int(dataset):\n    string_cols = dataset.select_dtypes(['object']).columns\n    dataset[string_cols].nunique()\n    dataset[string_cols] = dataset[string_cols].astype('category').apply(lambda x: x.cat.codes)\n    \n# call function\nstring_to_int(train1)","dffdac0a":"train1","065b112c":"# splitting dataset into training and validation data\nXdata = train1.drop(columns = ['Survived'])\nydata = train1['Survived']\nrd_seed = 333 # data splitted randomly with this seed\nXtrain, Xval, ytrain, yval = train_test_split(Xdata, ydata, test_size=0.25, random_state=rd_seed)","531d7639":"# chosen range of hyperparameters for Decision Tree Classifier\nparam_grid = {\n    'max_depth': range(1,100), \n    'criterion': ['entropy', 'gini']\n}\n# using sklearn's ParameterGrid to make a grid of parameter combinations\nparam_comb = ParameterGrid(param_grid)","5123bb24":"# iterate each parameter combination and fit model\nval_acc = []\ntrain_acc = []\nfor params in param_comb:\n    dt = DecisionTreeClassifier(**params)\n    dt.fit(Xtrain, ytrain)\n    train_acc.append(metrics.accuracy_score(ytrain, dt.predict(Xtrain)))\n    val_acc.append(metrics.accuracy_score(yval, dt.predict(Xval)))","27b7f65a":"# plotting how accurate our model is with each hyperparameter combination\nplt.figure(figsize=(20,6))\nplt.plot(train_acc,'or-')\nplt.plot(val_acc,'ob-')\nplt.xlabel('hyperparametr index')\nplt.ylabel('accuracy')\nplt.legend(['train', 'validation'])\nbest_params = param_comb[np.argmax(val_acc)]\nbest_params","6c30f30e":"print(param_comb[5]) # printing hyperparameters with index 5\nprint(param_comb[105]) # printing hyperparameters with index 105","b3e6bd78":"# chosen range of hyperparameters for Random Forest Classifier\nparam_grid = {\n    'max_depth': range(1, 30), \n    'n_estimators': range(1, 500, 25),\n}\n# using sklearn's ParameterGrid\nparam_comb = ParameterGrid(param_grid)","3ab3f317":"# iterate each parameter combination and fit model\nval_acc = []\ntrain_acc = []\nfor params in param_comb:\n    dt = RandomForestClassifier(**params)\n    dt.fit(Xtrain, ytrain)\n    train_acc.append(metrics.accuracy_score(ytrain, dt.predict(Xtrain)))\n    val_acc.append(metrics.accuracy_score(yval, dt.predict(Xval)))","5ca4f71c":"# plotting how accurate our model is with each hyperparameter combination\nplt.figure(figsize=(20,6))\nplt.plot(train_acc,'or-')\nplt.plot(val_acc,'ob-')\nplt.xlabel('hyperparametr index')\nplt.ylabel('accuracy')\nplt.legend(['train', 'validation'])\nbest_params = param_comb[np.argmax(val_acc)]\nbest_params","135ba2a8":"print(param_comb[0]) # printing hyperparameters with index 0\nprint(param_comb[1]) # printing hyperparameters with index 1\nprint(param_comb[200]) # printing hyperparameters with index 1","bbd9f80e":"# chosen range of hyperparameters for AdaBoost Classifier\nparam_grid = {\n    'n_estimators': range(1, 1000, 25),\n    'algorithm': ['SAMME', 'SAMME.R'],\n}\n# using sklearn's ParameterGrid\nparam_comb = ParameterGrid(param_grid)","bc4680c6":"# iterate each parameter combination and fit model\nval_acc = []\ntrain_acc = []\nfor params in param_comb:\n    dt = AdaBoostClassifier(**params)\n    dt.fit(Xtrain, ytrain)\n    train_acc.append(metrics.accuracy_score(ytrain, dt.predict(Xtrain)))\n    val_acc.append(metrics.accuracy_score(yval, dt.predict(Xval)))","99ffeeb9":"# plotting how accurate our model is with each hyperparameter combination\nplt.figure(figsize=(20,6))\nplt.plot(train_acc,'or-')\nplt.plot(val_acc,'ob-')\nplt.xlabel('hyperparametr index')\nplt.ylabel('accuracy')\nplt.legend(['train', 'validation'])\nbest_params = param_comb[np.argmax(val_acc)]\nbest_params","67902dd7":"print(param_comb[0]) # printing hyperparameters with index 0\nprint(param_comb[50]) # printing hyperparameters with index 1\nprint(param_comb[75]) # printing hyperparameters with index 75","b38e9a05":"def rank_classifiers(data):\n    rd_seed = 333\n    # specify the data we want to predict\n    Xdata = data.drop(columns = ['Survived'])\n    ydata = data['Survived']\n    # ready Classifiers and their distinctive range of parameters for\n    # hyperparameter tuning with cross-validation using GridSearchCV\n    pipeline = Pipeline([\n        ('clf', DecisionTreeClassifier()) # placeholder classifier\n    ])\n    # narrowed down hyperparameters\n    parameters = [\n        {\n            'clf': (DecisionTreeClassifier(),),\n            'clf__max_depth': range(1, 15), \n            'clf__criterion': ['gini', 'entropy'],\n        }, {\n            'clf': (RandomForestClassifier(),),\n            'clf__max_depth': range(1, 10), \n            'clf__n_estimators': range(25, 500, 25),\n        }, {\n            'clf': (AdaBoostClassifier(),),\n            'clf__n_estimators': range(1, 250, 10),\n            'clf__algorithm': ['SAMME', 'SAMME.R'],\n        }\n    ]\n    # run GridSearchCV with training data to determine the best \n    # classifier with the best parameters while cross-validating them\n    # thus maximizing fairness of score rankings\n    clf = GridSearchCV(pipeline, parameters, cv=5, iid=False, n_jobs=-1)\n    clf.fit(Xdata, ydata)\n    print('accuracy score (train): {0:.6f}'.format(clf.best_score_))\n    # now lets see how well it predicts our testing data\n    return clf","6f6e8b4e":"clf = rank_classifiers(train1)","68751ed0":"print(clf.best_params_)","c88e008b":"# print sum of missing values for each column\nfor dataset in [train, test]:\n    print(dataset.isna().sum())","9e4c9331":"import re\ndeck = {\"A\": 1, \"B\": 2, \"C\": 3, \"D\": 4, \"E\": 5, \"F\": 6, \"G\": 7, \"U\": 8}\n\nfor dataset in [train ,test]:\n    dataset['Cabin'] = dataset['Cabin'].fillna(\"U0\")\n    dataset['Deck'] = dataset['Cabin'].map(lambda x: re.compile(\"([a-zA-Z]+)\").search(x).group())\n    # we can now drop the cabin feature\n    dataset.drop(columns=['Cabin'], inplace=True)","c2c511dc":"for dataset in [train, test]:\n    mean = dataset[\"Age\"].mean()\n    std = dataset[\"Age\"].std()\n    is_null = dataset[\"Age\"].isnull().sum()\n    # create an array of random numbers given the range and length\n    rand_ages_for_nan = np.random.randint(mean - std, mean + std, size = is_null)\n    # fill NaN values in the column age with random values generated\n    ages_slice = dataset[\"Age\"].copy()\n    ages_slice[np.isnan(ages_slice)] = rand_ages_for_nan\n    dataset[\"Age\"] = ages_slice.astype(int)","33efa10c":"# describe data to find the most frequent one\nfor dataset in [train, test]:\n    print(dataset.Embarked.describe())","fc790cb4":"# fill missing values with the frequent value\nfor dataset in [train, test]:\n    dataset.Embarked.fillna('S', inplace=True)","13f68db1":"test.Fare.fillna(test.Fare.mean(), inplace=True)","f8dd8fe5":"# final check of missing values\nfor dataset in [train, test]:\n    print(dataset.isna().sum())","fedc04af":"train.drop(columns=['PassengerId'], inplace=True)","0a5f214a":"for dataset in [train, test]:\n    dataset['Title'] = dataset.Name.str.extract(' ([A-z]+\\.)', expand=False)\n    # print added Titles\n    print(dataset.Title.value_counts())\n","cdc9b2bd":"for dataset in [train, test]:\n    dataset.drop(columns=['Name'], inplace=True)","704e50a2":"for dataset in [train, test]:\n    # create relatives feature by summing up sibsp and parch\n    dataset['Relatives'] = dataset.SibSp + dataset.Parch\n    # create not_alone feature, where it is 1 if # of relatives is 0\n    dataset.loc[dataset['Relatives'] > 0, 'Not_alone'] = 0\n    dataset.loc[dataset['Relatives'] == 0, 'Not_alone'] = 1","722cb100":"for dataset in [train, test]:\n    string_to_int(dataset)","21e5874e":"train.describe()","2e4bb6bb":"test.describe()","d0729fc4":"for dataset in [train, test]:\n    dataset.loc[dataset.Age > 65, 'Age_cat'] = 6\n    dataset.loc[dataset.Age <= 65, 'Age_cat'] = 5\n    dataset.loc[dataset.Age <= 48, 'Age_cat'] = 4\n    dataset.loc[dataset.Age <= 35, 'Age_cat'] = 3\n    dataset.loc[dataset.Age <= 24, 'Age_cat'] = 2\n    dataset.loc[dataset.Age <= 13, 'Age_cat'] = 1\n    dataset.loc[dataset.Age <= 1, 'Age_cat'] = 0\n    # drop age column\n    dataset.drop(columns=['Age'], inplace=True)","05b44c8e":"for dataset in [train, test]:\n    dataset.Fare = pd.qcut(dataset.Fare, 6, labels=[0,1,2,3,4,5]).astype(int)","729ecf2a":"for dataset in [train, test]:\n    dataset.drop(columns=['Ticket'], inplace=True)","15b002e2":"train.describe()","eb3ae76e":"test.describe()","7c7c308d":"plt.subplots(figsize = (15,10))\nsns.heatmap(train.corr(), annot=True,cmap=\"RdYlGn_r\")\nplt.title(\"Feature Correlations\", fontsize = 18)","4b679281":"clf = rank_classifiers(train)","16f15c16":"print(clf.best_params_)","9659c5a1":"importances = pd.DataFrame({'feature':train.drop(columns=['Survived']).columns,'importance':np.round(clf.best_params_['clf'].feature_importances_,3)})\nimportances = importances.sort_values('importance',ascending=False).set_index('feature')\nimportances.head(15)\nimportances.plot.bar()","855af29e":"# extract PassengerId\nid_col = test.PassengerId\n# predict survival\ntest = test.drop(columns=['PassengerId'])","0ce2d0fd":"# create submission dataframe\nsubmission = pd.DataFrame({\n    'PassengerId': id_col.values,\n    'Survived': clf.predict(test)\n})","7e3a7380":"# save submission\nsubmission.to_csv('submission.csv', index=False)","14edeb38":"Dropping rows with NaN from this point on drops only maximum of 179 rows, which is kind of ok for starters.","2364c6ee":"As we can see, most of our features are categorized (small number of unique values). The only features left to categorize are: **Age**, **Ticket** and **Fare**.","105a8e2f":"#### Random Forest Classifier","fa848452":"**Ticket** feature has too many unique values, creating too many categories. It would be hard to make it a useful feature so we will drop it","2c10f16d":"## 4.3 Converting string features\nLets see what columns are of type *string*. Keep in mind that in **pandas**, type *string* is in the class *object*.\n\nWe can use our predefined function from before.","b8432061":"And there is our winner: **RandomForestClassifier**,\nWith parameters: **max_depth = 1** and **n_estimators= 275**, with an accuracy score of: **0.94** for training data and **0.75** for testing data.\n\nLet's see what happens if we clean our data a little better this time.","3687a248":"# 3. Building models\nFor this work, we will be using 3 ML classifiers: **DecisionTreeClassifier**, **RandomForestClassifier** and **AdaBoostClassifier**, ranking them by their accuracy scores while watching how this ranking chages as we clean our dataset more precisely.","8fb61023":"In contraty to the **Decision Tree Classifier**, the hyperparamemter **n_estimators** with a higher value is overfitting our model (starting at around **250** for the SAMME algorithm). The same happens for the **SAMME.R** algorithm in a more severe manner.","39ecbce2":"### 3.1.1 Narrowing down hyperparameter ranges for each classifier","769ca55e":"Let's categorize **Age** as follows:\n- Infants(0): x <= 1,\n- Children(1): 1 < x <= 13,\n- Youth(2): 13 < x <= 24,\n- Young_adults(3): 24 < x <= 35,\n- Adults(4): 35 < x <= 48,\n- Older_adults(5) 48 < x <= 65,\n- Seniors(6) 65 < x.","8dbb4ce1":"Let's check:","7d87bb2f":"## 3.1 Hyperparameter tuning and cross-validation\n\nAs rankings can be very close, lets do cross-validation and tune hyperparameters for each of them to maximize the fairness. For that, I've decided to use sklearn's **[GridSearchCV](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.model_selection.GridSearchCV.html#sklearn.model_selection.GridSearchCV.fit)**.\n\nSince **GridSearchCV** is an exhaustive search (it's really slow) and we plan on calling our **GridSearchCV** many times, let's first try narrowing down our hyperparameter ranges as much as possible.","79b0e5ac":"For **Fare** feature, we cannot easily categorize values with our real-world knowledge. For this case, we will use **[pandas.qcut](https:\/\/pandas.pydata.org\/pandas-docs\/version\/0.23.4\/generated\/pandas.qcut.html)**, which discretizes variables into equal-sized buckets.","88e12008":"#### Decision Tree Classifier","cbdd3dd0":"Our scores are significantly higher this time at **0.830**. Best classifier is still **RandomForestClassifier** with parameters: **max_depth = 5** and **n_estimators = 375**.","06ba8ac2":"# 1. Get to know the dataset\nLets get some general information about our dataset","95bd04d2":"# 5. Correlation matrix\nWe can plot the correlation matrix to see how relevant are our new features.","bdbe0cbe":"So about **38%** of the passengers survived the titanic in this dataset.\nAges seem to range from **0.42** to **80**. \nWe can also see that there are missing data as the row **count** indicates.","593e3b48":"There is only one missing value left in the **Fare** column of test data so we will just use the mean for that.","28938f90":"What seems to be causing large accuracy dips in our graph are low values of **n_estimators** and from a certain value (in this case **25**), it's slowly converging to the maximum accuracy it can achieve with other given hyperparameters (in this case **max_depth**).\nAs for **max_depth**, the graph indicates that it's value of **10** and greater is overfitting our model.","81d1266d":"As we can see from our graph, comparing **gini** and **entropy** criterions can be beneficial.\nAs for the **max_depth** hyperparameter however, the accuracy on validation dataset doesn't get better after the value 15. As a matter of fact, it seems to be heavily overfitting (when validation score doesn't get better and train score is close to 100%).","30a8f9c8":"# 8. Submission\nAll that is left to do is to fit our model on the test data and submit our predictions.","38b97a80":"## 4.2 Creating new features based on other features","7f8e31d1":"Final check:","2e2b7ff5":"# 7. Hyperparameter importance\nThe **RandomForestClassifier** model allows us to plot out the importance of hyperparameters. ","cddd9a9e":"## 4.4 Categorizing continuous data","d50b1fb7":"**Age**: Generate random ages. Generated ages will be around the mean of ages, with the maximum difference of stadart deviation (std).\n\nAnother idea is to fill the missing values using another ML model. We will do that later and see if it has any impact on the prediction of survivability.","0f74dacf":"**Cabin:**\nAfter further investigation, it seems that the letter in the cabin number represents a deck. We can thus extract each letter and create a new feature for every passanger.","0c4405c2":"**Age** and **Fare** colums data need to be categorized so our Classifiers can work better. That will be for later though, since we want to try fitting badly cleaned data and see how our models will perform.","5a4c838c":"#### AdaBoost Classifier","242d4dfd":"**Name**: We can extract **Titles** from the **Name** column and create a new feature.","abcc7201":"### 3.1.2 GridSearchCV\n<a id='gridsearchcv'><\/a>\nUsing our aproximations of relevant hyperparameters from the previous section, we can now define our function with **GridSearchCV** more optimally.\n\nWe will also put it in a function to call later on.","a3268a9d":"Column **Embarked** has only two missing values, which can be easily replaced by the most frequent value. Missing values in the column **Age** can potentionally be replaced with the mean age of all passangers. The **Cabin** column has many missing values and needs further investigation.","d99b790f":"# 6. Train classifiers on better cleaned data\nWe can now use our function **[rank_classifiers](#gridsearchcv)** with **GridSearchCV**, which we have defined earlier in this work.","4acf67b5":"# 4. Cleaning data try 2\n\n## 4.1 Missing data","1c213e80":"This work is a walkthrough of the most basic Data Science workflow practices.\n\nIn this notebook we will:\n- Clean the dataset and feature engineer,\n- Build predictive models and rank their performace,\n- Tune hyperparameters and cross-validate.\n\nWe will test out each practice, observe how it enhances prediction accuracy and make sense of them.","1692fa5d":"Firsty, lets drop the column **PassengerId** since it does not corelate with survivability for obvious reasons.\n\nNote: We will only drop the **PassengerId** column for train dataset, since we will be training our model on this dataset. We will keep the **PassengerId** on the test dataset since the submission requires it.","62dc175c":"**Sibsp and Parch**: These features together mean the number of **Relatives** on board. We can also create another feature called **Not_alone** out of this aswell.","b86643fc":"Apart from the **title**, which we saved as a new feature, there doesn't seem to be any more value in the **name** column, thus we can drop that.","6c983981":"# 2. Cleaning data\nFirst, lets try poorly cleaning our data and see how our models will perform.\n\nColumns **Name**, **Cabin** and **PassengerId** don't seem to have much corelation to the survivabilty at first glance, let's just drop them.\n\nWe don't have to change our **test** dataset since we will do a more throughout cleaning later on, this is only to showcase how important data engineering is and how it can affect our prediction accuracy.","7a6923c9":"**Embarked**: This column has only two missing values, which we can just fill with the most common value."}}