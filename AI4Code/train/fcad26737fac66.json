{"cell_type":{"3f9b28ec":"code","eaba91b0":"code","7ac01d6b":"code","335b2e9b":"code","4ea9969a":"code","95221ac5":"code","b823e096":"code","436a80fc":"code","8c002f30":"code","4576ba23":"code","adf876fb":"code","e917a3da":"code","0c9e00b9":"code","6ca413e1":"code","7e32e413":"code","f4cfb09a":"code","859139fd":"code","48b76886":"code","cac904d3":"code","dfb88994":"code","9a4f4627":"code","b41ded2d":"code","c260a88d":"code","42053427":"code","2250fda2":"code","e3c8c45d":"code","fbb4a9c1":"code","3dd1ca7c":"code","ad0edfb2":"code","d1b5d7d9":"code","b5e6ee23":"code","b6c9c6fc":"code","60c36f58":"code","43ffa519":"code","d2f4e4c2":"code","1e583a30":"code","742e2b28":"code","fa143382":"code","13b4e0ba":"code","cf69d05e":"code","3a796645":"code","6cee1b77":"markdown","bbbbb635":"markdown","6e5df916":"markdown","e0e1c358":"markdown","0fe6f3a3":"markdown","29fb2873":"markdown","6401833f":"markdown","b6c38900":"markdown","d0ae4f1f":"markdown","79e45456":"markdown","dbd02cfe":"markdown","dd74112c":"markdown","44fd07eb":"markdown","8f6ecd6d":"markdown","70aa8cc5":"markdown","21fde1d9":"markdown","5df76b52":"markdown","8bf3c864":"markdown","feb99d29":"markdown","9ff13710":"markdown","a7e2dcab":"markdown","3837910f":"markdown","fe783ea2":"markdown","22d16b02":"markdown","2ab32565":"markdown","38c5d4bb":"markdown","15e6a250":"markdown","da7ce0ce":"markdown","6dc87c3d":"markdown","b6f55112":"markdown","274853fd":"markdown","af5bf1c1":"markdown","eb2947c5":"markdown","1b08b468":"markdown","b0d1c356":"markdown","65ce995b":"markdown","38b6d764":"markdown","1e5c4e44":"markdown","5cb1b550":"markdown"},"source":{"3f9b28ec":"# importing librarys\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport seaborn as sns\nsns.set(style=\"darkgrid\")\n\nfrom scipy import stats\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import RandomForestRegressor\n\n\n\n\n","eaba91b0":"# Laoding daily data set\nbike_day = pd.read_csv('..\/input\/day.csv')","7ac01d6b":"# expand number of columns\npd.set_option('display.max_columns', 30)\n# Viewing day data\nbike_day.head()","335b2e9b":"# View shape of sets\nprint(bike_day.dtypes)\n","4ea9969a":"# Renaming Columns\n\nbike_day.rename(columns={'instant':'rec_id',\n                        'dteday':'datetime',\n                        'weathersit':'weather',\n                        'mnth':'month',\n                        'hum':'humidity',\n                        'cnt':'total_cnt'},inplace=True)\n\nimport calendar\nfrom datetime import datetime\n\n# Creating new Columns\n\nbike_day[\"weekday\"] = bike_day.datetime.apply(lambda dateString : calendar.day_name[datetime.strptime(dateString,\"%Y-%m-%d\").weekday()])\nbike_day[\"month\"] = bike_day.datetime.apply(lambda dateString : calendar.month_name[datetime.strptime(dateString,\"%Y-%m-%d\").month])\nbike_day[\"season_num\"] = bike_day.season \nbike_day[\"season\"] = bike_day.season.map({1: 'Winter', 2 : 'Spring', 3 : 'Summer', 4 : 'Fall' })\nbike_day[\"weather_num\"] = bike_day.weather\nbike_day[\"weather\"] = bike_day.weather.map({1: \" Clear + Few clouds + Partly cloudy + Partly cloudy\",\\\n                                        2 : \" Mist + Cloudy, Mist + Broken clouds, Mist + Few clouds, Mist \", \\\n                                        3 : \" Light Snow, Light Rain + Thunderstorm + Scattered clouds, Light Rain + Scattered clouds\", \\\n                                        4 :\" Heavy Rain + Ice Pallets + Thunderstorm + Mist, Snow + Fog \" })\n\nbike_day['weekday_num'] = bike_day.weekday.map({'Monday': 1, 'Tuesday':2, 'Wednesday':3, 'Thursday':4, 'Friday':5, 'Saturday':6, 'Sunday': 7})\nbike_day['month_num'] = bike_day.month.map({'January': 1, 'February': 2, 'March': 3, 'April': 4, 'May': 5, 'June' : 6, 'July': 7, 'August': 8, 'September': 9, 'October': 10, 'November': 11, 'December': 12})\n\n\n\n# defining categorical variables\n\nbike_day['season'] = bike_day.season.astype('category')\nbike_day['holiday'] = bike_day.holiday.astype('category')\nbike_day['weekday'] = bike_day.weekday.astype('category')\nbike_day['weather'] = bike_day.weather.astype('category')\nbike_day['month'] = bike_day.month.astype('category')\nbike_day['workingday'] = bike_day.workingday.astype('category')\n\n\n","95221ac5":"bike_day.head()","b823e096":"# checking data set once more \n\nprint(bike_day.dtypes)","436a80fc":"bike_day.isnull().any()","8c002f30":"fig, axes = plt.subplots(nrows=2,ncols=2)\nfig.set_size_inches(12, 10)\nsns.boxplot(data=bike_day,y=\"total_cnt\",x=\"month\",orient=\"v\",ax=axes[0][0])\nsns.boxplot(data=bike_day,y=\"total_cnt\",x=\"season\",orient=\"v\",ax=axes[0][1])\nsns.boxplot(data=bike_day,y=\"total_cnt\",x=\"weekday\",orient=\"v\",ax=axes[1][0])\nsns.boxplot(data=bike_day,y=\"total_cnt\",x=\"workingday\",orient=\"v\",ax=axes[1][1])\n\naxes[0][0].set(ylabel='Count',title=\"Total Count vs. Month\")\naxes[0][1].set(xlabel='Season', ylabel='Count',title=\"Total Count vs. Season\")\naxes[1][0].set(xlabel='Weekday', ylabel='Count',title=\"Total Count vs. Weekday\")\naxes[1][1].set(xlabel='Working Day', ylabel='Count',title=\"Total Count vs. Working Day\")","4576ba23":"bike_day_WithoutOutliers = bike_day[np.abs(bike_day[\"total_cnt\"]-bike_day[\"total_cnt\"].mean())<=(3*bike_day[\"total_cnt\"].std())] \n\nprint (\"Shape Of The Before Ouliers: \",bike_day.shape)\nprint (\"Shape Of The After Ouliers: \",bike_day_WithoutOutliers.shape)","adf876fb":"corrMatt = bike_day[['temp','atemp','windspeed','humidity','registered','casual','total_cnt']].corr()\n\nmask = np.array(corrMatt)\n# Turning the lower-triangle of the array to false\nmask[np.tril_indices_from(mask)] = False\nfig,ax = plt.subplots()\nsns.heatmap(corrMatt, \n            mask=mask,\n            vmax=0.7, \n            square=True,\n            annot=True,\n            cmap=\"YlGnBu\")\nfig.set_size_inches(8,10)","e917a3da":"fig, axes = plt.subplots(nrows=2,ncols=3)\nfig.set_size_inches(20, 13)\nsns.scatterplot(data=bike_day,y=\"total_cnt\",x=\"temp\",ax=axes[0][0])\nsns.scatterplot(data=bike_day,y=\"total_cnt\",x=\"humidity\",ax=axes[0][1])\nsns.scatterplot(data=bike_day,y=\"total_cnt\",x=\"windspeed\",ax=axes[0][2])\nsns.scatterplot(data=bike_day,y=\"total_cnt\",x=\"month\",ax=axes[1][0])\nsns.barplot(data=bike_day,y=\"total_cnt\",x=\"season\",ax=axes[1][1])\nsns.scatterplot(data=bike_day,y=\"total_cnt\",x=\"weather_num\",ax=axes[1][2])\n\n\n\naxes[0][0].set(xlabel='Temp Norm.',ylabel='Count',title=\"Total Count vs. Temp\")\naxes[0][1].set(xlabel='Humidity', ylabel='Count',title=\"Total Count vs. Humidity\")\naxes[0][2].set(xlabel='Windspeed', ylabel='Count',title=\"Total Count vs. Windspeed\")\naxes[1][0].set(xlabel=' ', ylabel='Count',title=\"Total Count vs. Weekday\")\naxes[1][1].set(xlabel=' ', ylabel='Count',title=\"Total Count vs. Working Day\")\naxes[1][2].set(xlabel='Weather Condition', ylabel='Count',title=\"Total Count vs. Working Day\")","0c9e00b9":"fig,axes = plt.subplots(ncols=2,nrows=1)\nfig.set_size_inches(12, 8)\nsns.distplot(bike_day[\"total_cnt\"],ax=axes[0]) # was macht distplot?\nstats.probplot(bike_day[\"total_cnt\"], dist='norm', fit=True, plot=axes[1])\n#sns.distplot(np.log(bike_day_WithoutOutliers[\"total_cnt\"]),ax=axes[1][0])\n#stats.probplot(np.log1p(bike_day_WithoutOutliers[\"total_cnt\"]), dist='norm', fit=True, plot=axes[1][1])","6ca413e1":"import statsmodels.api as sm\n\n#fit the model\nregressors = bike_day[['temp','atemp', 'windspeed', 'humidity']] \nreg_const = sm.add_constant(regressors)\nmod = sm.OLS(bike_day['total_cnt'], reg_const)\nres = mod.fit()\n#print the summary\nprint(res.summary())","7e32e413":"import statsmodels.formula.api as smf\n\n#fit the model\nmixed = smf.mixedlm(\"total_cnt ~ atemp+windspeed+humidity\", bike_day, groups = bike_day['month_num'],re_formula='~windspeed+humidity')\nmixed_fit = mixed.fit()\n#print the summary\nprint(mixed_fit.summary())","f4cfb09a":"# load data\nbike_hour = pd.read_csv('..\/input\/hour.csv')","859139fd":"# Renaming Columns\n\nbike_hour.rename(columns={'instant':'rec_id',\n                        'dteday':'datetime',\n                        'weathersit':'weather',\n                        'mnth':'month',\n                        'hr':'hour',\n                        'hum':'humidity',\n                        'cnt':'total_cnt'},inplace=True)\n\nimport calendar\nfrom datetime import datetime\n\n# Creating new Columns\n\nbike_hour[\"weekday\"] = bike_hour.datetime.apply(lambda dateString : calendar.day_name[datetime.strptime(dateString,\"%Y-%m-%d\").weekday()])\nbike_hour[\"month\"] = bike_hour.datetime.apply(lambda dateString : calendar.month_name[datetime.strptime(dateString,\"%Y-%m-%d\").month])\nbike_hour[\"season_num\"] = bike_hour.season\nbike_hour[\"season\"] = bike_hour.season.map({1: 'Winter', 2 : 'Spring', 3 : 'Summer', 4 : 'Fall' })\nbike_hour[\"weather_num\"] = bike_hour.weather\nbike_hour[\"weather\"] = bike_hour.weather.map({1: \" Clear + Few clouds + Partly cloudy + Partly cloudy\",\\\n                                        2 : \" Mist + Cloudy, Mist + Broken clouds, Mist + Few clouds, Mist \", \\\n                                        3 : \" Light Snow, Light Rain + Thunderstorm + Scattered clouds, Light Rain + Scattered clouds\", \\\n                                        4 :\" Heavy Rain + Ice Pallets + Thunderstorm + Mist, Snow + Fog \" })\n\n\n\nbike_hour['weekday_num'] = bike_hour.weekday.map({'Monday': 1, 'Tuesday':2, 'Wednesday':3, 'Thursday':4, 'Friday':5, 'Saturday':6, 'Sunday': 7})\nbike_hour['month_num'] = bike_hour.month.map({'January': 1, 'February': 2, 'March': 3, 'April': 4, 'May': 5, 'June' : 6, 'July': 7, 'August': 8, 'September': 9, 'October': 10, 'November': 11, 'December': 12})\n\n\n\n# defining categorical variables\n\nbike_hour['season'] = bike_hour.season.astype('category')\nbike_hour['holiday'] = bike_hour.holiday.astype('category')\nbike_hour['weekday'] = bike_hour.weekday.astype('category')\nbike_hour['weather'] = bike_hour.weather.astype('category')\nbike_hour['month'] = bike_hour.month.astype('category')\nbike_hour['workingday'] = bike_hour.workingday.astype('category')\n\n\n\n","48b76886":"bike_hour.head()","cac904d3":"bike_hour.isnull().any()","dfb88994":"fig, axes = plt.subplots(nrows=2,ncols=2)\nfig.set_size_inches(12, 10)\nsns.boxplot(data=bike_hour,y=\"total_cnt\",x=\"month\",orient=\"v\",ax=axes[0][0])\nsns.boxplot(data=bike_hour,y=\"total_cnt\",x=\"season\",orient=\"v\",ax=axes[0][1])\nsns.boxplot(data=bike_hour,y=\"total_cnt\",x=\"weekday\",orient=\"v\",ax=axes[1][0])\nsns.boxplot(data=bike_hour,y=\"total_cnt\",x=\"workingday\",orient=\"v\",ax=axes[1][1])\n\naxes[0][0].set(ylabel='Count',title=\"Total Count vs. Month\")\naxes[0][1].set(xlabel='Season', ylabel='Count',title=\"Total Count vs. Season\")\naxes[1][0].set(xlabel='Weekday', ylabel='Count',title=\"Total Count vs. Weekday\")\naxes[1][1].set(xlabel='Working Day', ylabel='Count',title=\"Total Count vs. Working Day\")","9a4f4627":"bike_hour_WithoutOutliers = bike_hour[np.abs(bike_hour[\"total_cnt\"]-bike_hour[\"total_cnt\"].mean())<=(3*bike_hour[\"total_cnt\"].std())] \n\n# how many outliers are removed?\nprint (\"Shape Of The Before Ouliers: \",bike_hour.shape)\nprint (\"Shape Of The After Ouliers: \",bike_hour_WithoutOutliers.shape)","b41ded2d":"fig,axes = plt.subplots(ncols=2,nrows=2)\nfig.set_size_inches(12, 10)\nsns.distplot(np.log(bike_hour[\"total_cnt\"]),ax=axes[0][0])\nstats.probplot(bike_hour[\"total_cnt\"], dist='norm', fit=True, plot=axes[0][1])\nsns.distplot(np.log(bike_hour_WithoutOutliers[\"total_cnt\"]),ax=axes[1][0])\nstats.probplot((bike_hour_WithoutOutliers[\"total_cnt\"]), dist='norm', fit=True, plot=axes[1][1])\n\naxes[0][0].set(xlabel='log(Count)',title=\"With Outliers\")\naxes[1][0].set(xlabel='log(Count)',title=\"Without Outliers\")","c260a88d":"# renaming bike_hour_WithoutOutliers to bike_hour\nbike_hour = bike_hour_WithoutOutliers\nprint (\"Shape Of Data after Cleaning: \",bike_hour.shape)","42053427":"fig,(ax1,ax2,ax3)= plt.subplots(nrows=3)\nfig.set_size_inches(8,15)\nsortOrder = [\"January\",\"February\",\"March\",\"April\",\"May\",\"June\",\"July\",\"August\",\"September\",\"October\",\"November\",\"December\"]\nhueOrder = [\"Sunday\",\"Monday\",\"Tuesday\",\"Wednesday\",\"Thursday\",\"Friday\",\"Saturday\"]\n\n# Plotting average count vs. month\nmonth_h_Aggregated = pd.DataFrame(bike_hour.groupby(\"month\")[\"total_cnt\"].mean()).reset_index()\nmonth_h_Sorted = month_h_Aggregated.sort_values(by=\"total_cnt\",ascending=False)\nsns.barplot(data=month_h_Sorted,x=\"month\",y=\"total_cnt\",ax=ax1,order=sortOrder)\nax1.set(xlabel='Month', ylabel='Count',title=\"Count vs. Month\")\n\n# Plotting average count vs. season\nhour_h_Aggregated = pd.DataFrame(bike_hour.groupby([\"hour\",\"season\"],sort=True)[\"total_cnt\"].mean()).reset_index()\nsns.pointplot(x=hour_h_Aggregated[\"hour\"], y=hour_h_Aggregated[\"total_cnt\"],hue=hour_h_Aggregated[\"season\"], data=hour_h_Aggregated, join=True,ax=ax2)\nax2.set(xlabel='Hour Of The Day', ylabel='Count',title=\"Hourly Count vs. Season\",label='big')\n\n# Plotting average count vs. weekday\nweekday_h_Aggregated = pd.DataFrame(bike_hour.groupby([\"hour\",\"weekday\"],sort=True)[\"total_cnt\"].mean()).reset_index()\nsns.pointplot(x=weekday_h_Aggregated[\"hour\"], y=weekday_h_Aggregated[\"total_cnt\"],hue=weekday_h_Aggregated[\"weekday\"], data=weekday_h_Aggregated, join=True,ax=ax3)\nax3.set(xlabel='Hour', ylabel='Count',title=\"Count vs. Weekday\",label='big')\n\n","2250fda2":"# Create X by defining features \nfeatures = ['month','weather','temp','windspeed','season','humidity']\nX_hour = bike_hour[features]\n\n# Define y\ny_hour = bike_hour.total_cnt","e3c8c45d":"# Split into training, test and validaion set\n\nX_hour_train, X_hour_test, y_hour_train, y_hour_test  = train_test_split(X_hour, y_hour, test_size=0.2, random_state=1)\n\nX_hour_train, X_hour_val, y_hour_train, y_hour_val = train_test_split(X_hour_train, y_hour_train, test_size=0.25, random_state=1)","fbb4a9c1":"# checking for categorical variables\ns = (X_hour_train.dtypes == 'category')\nobject_cols_h = list(s[s].index)\n\nprint(\"Categorical variables:\")\nprint(object_cols_h)","3dd1ca7c":"# Encoding Categoricals using One Hot Encoding\nfrom sklearn.preprocessing import OneHotEncoder\n\n# Apply one-hot encoder to each column with categorical data\nOH_encoder = OneHotEncoder(handle_unknown='ignore', sparse=False) \nOH_cols_train_h = pd.DataFrame(OH_encoder.fit_transform(X_hour_train[object_cols_h])) #training set\nOH_cols_valid_h = pd.DataFrame(OH_encoder.transform(X_hour_val[object_cols_h])) # validation set\nOH_cols_test_h = pd.DataFrame(OH_encoder.transform(X_hour_test[object_cols_h])) # validation set\nOH_cols_X_h = pd.DataFrame(OH_encoder.transform(X_hour[object_cols_h])) #complete set of X\n\n\n# One-hot encoding removed index; put it back\nOH_cols_train_h.index = X_hour_train.index\nOH_cols_valid_h.index = X_hour_val.index\nOH_cols_test_h.index = X_hour_test.index\nOH_cols_X_h.index = X_hour.index\n\n# Remove categorical columns (will replace with one-hot encoding)\nnum_train_X_h = X_hour_train.drop(object_cols_h, axis=1)\nnum_val_X_h = X_hour_val.drop(object_cols_h, axis=1)\nnum_test_X_h = X_hour_test.drop(object_cols_h, axis=1)\nnum_X_h = X_hour.drop(object_cols_h, axis=1)\n\n\n\n# Add one-hot encoded columns to numerical features\nOH_train_X_h = pd.concat([num_train_X_h, OH_cols_train_h], axis=1)\nOH_val_X_h = pd.concat([num_val_X_h, OH_cols_valid_h], axis=1)\nOH_test_X_h = pd.concat([num_test_X_h, OH_cols_test_h], axis=1)\nOH_X_h = pd.concat([num_X_h, OH_cols_X_h], axis=1) ","ad0edfb2":"OH_X_h.head()","d1b5d7d9":"#predictions using the Random Forest Regressor\n\n#Define the model. Set random_state to 1\nrf_model = RandomForestRegressor(n_estimators=100,random_state=1)\nrf_model.fit(OH_train_X_h, y_hour_train)\nrf_val_predictions = rf_model.predict(OH_val_X_h)\nrf_val_mae = mean_absolute_error(rf_val_predictions, y_hour_val)\n\nprint(\"Validation MAE for Random Forest Model: {:,.0f}\".format(rf_val_mae))\n","b5e6ee23":"#apply model on test set:\n\nrf_model.fit(OH_test_X_h, y_hour_test)\n\ntest_preds_h = rf_model.predict(OH_test_X_h)\nerrors_h = abs(y_hour_test-test_preds_h)\n\n# Calculate mean absolute percentage error (MAPE)\nMAPE_h = 100 * (errors_h \/ test_preds_h)\n\n\n# Calculate and display accuracy\naccuracy = 100 - np.mean(MAPE_h)\nprint(\"Accuracy: {:,.2f}%\".format(accuracy))\n\n\n","b6c9c6fc":"# apply model on all data\n\nrf_model.fit(OH_X_h, y_hour)\n\npreds_all_h = rf_model.predict(OH_X_h)\nerrors_all_h = abs(y_hour-preds_all_h)\n\n# Calculate mean absolute percentage error (MAPE)\nMAPE_all_h = 100 * (errors_all_h \/ preds_all_h)\n\n\n# Calculate and display accuracy\naccuracy_all_h = round(100 - np.mean(MAPE_all_h),2)\nprint(\"Accuracy: {:,.2f}%\".format(accuracy_all_h))\n\n\n","60c36f58":"# Create X by defining features \nfeatures_d = ['month','weather','temp','windspeed','season','humidity']\nX_day = bike_day[features_d]\n\n# Define y\ny_day = bike_day.total_cnt\n\n\n# Split into training, test and validaion set\n\nX_day_train, X_day_test, y_day_train, y_day_test  = train_test_split(X_day, y_day, test_size=0.2, random_state=1)\n\nX_day_train, X_day_val, y_day_train, y_day_val = train_test_split(X_day_train, y_day_train, test_size=0.25, random_state=1)","43ffa519":"X_day_train.head()","d2f4e4c2":"X_day_train.isnull().any()","1e583a30":"# checking for categorical variables\ns = (X_day_train.dtypes == 'category')\nobject_cols_d = list(s[s].index)\n\nprint(\"Categorical variables:\")\nprint(object_cols_d)","742e2b28":"# Encoding Categoricals using One Hot Encoding\n\n\n# Apply one-hot encoder to each column with categorical data\nOH_encoder_d = OneHotEncoder(handle_unknown='ignore', sparse=False) \nOH_cols_train_d = pd.DataFrame(OH_encoder_d.fit_transform(X_day_train[object_cols_d])) #training set\nOH_cols_valid_d = pd.DataFrame(OH_encoder_d.transform(X_day_val[object_cols_d])) # validation set\nOH_cols_test_d = pd.DataFrame(OH_encoder_d.transform(X_day_test[object_cols_d])) # validation set\nOH_cols_X_d = pd.DataFrame(OH_encoder_d.transform(X_day[object_cols_d])) #complete set of X\n\n\n# One-hot encoding removed index; put it back\nOH_cols_train_d.index = X_day_train.index\nOH_cols_valid_d.index = X_day_val.index\nOH_cols_test_d.index = X_day_test.index\nOH_cols_X_d.index = X_day.index\n\n# Remove categorical columns (will replace with one-hot encoding)\nnum_train_X_d = X_day_train.drop(object_cols_d, axis=1)\nnum_val_X_d = X_day_val.drop(object_cols_d, axis=1)\nnum_test_X_d = X_day_test.drop(object_cols_d, axis=1)\nnum_X_d = X_day.drop(object_cols_d, axis=1)\n\n\n\n# Add one-hot encoded columns to numerical features\nOH_train_X_d = pd.concat([num_train_X_d, OH_cols_train_d], axis=1)\nOH_val_X_d = pd.concat([num_val_X_d, OH_cols_valid_d], axis=1)\nOH_test_X_d = pd.concat([num_test_X_d, OH_cols_test_d], axis=1)\nOH_X_d = pd.concat([num_X_d, OH_cols_X_d], axis=1) ","fa143382":"#predictions using the Random Forest Model without specifying leaf nodes\n\n#Define the model. Set random_state to 1\nrf_model_d = RandomForestRegressor(n_estimators=100, random_state=1)\nrf_model_d.fit(OH_train_X_d, y_day_train)\nrf_val_predictions_d = rf_model_d.predict(OH_val_X_d)\nrf_val_mae_d = mean_absolute_error(rf_val_predictions_d, y_day_val)\n\nprint(\"Validation MAE for Random Forest Model: {:,.0f}\".format(rf_val_mae_d))\n\n","13b4e0ba":"#apply model on test set:\n\nrf_model_d.fit(OH_test_X_d, y_day_test)\n\ntest_preds_d = rf_model_d.predict(OH_test_X_d)\nerrors_d = abs(y_day_test-test_preds_d)\n\n# Calculate mean absolute percentage error (MAPE)\nMAPE_d = 100 * (errors_d \/ test_preds_d)\n\n\n# Calculate and display accuracy\naccuracy_d = 100 - np.mean(MAPE_d)\nprint(\"Accuracy: {:,.2f}%\".format(accuracy_d))","cf69d05e":"#apply model on complete data set:\n\nmodel = rf_model_d.fit(OH_X_d, y_day)\n\npreds_all_d = rf_model_d.predict(OH_X_d)\nerrors_all_d = abs(y_day-preds_all_d)\n\n# Calculate mean absolute percentage error (MAPE)\nMAPE_all_d = 100 * (errors_all_d \/ preds_all_d)\n\n\n# Calculate and display accuracy\naccuracy_all_d = 100 - np.mean(MAPE_all_d)\nprint(\"Accuracy: {:,.2f}%\".format(accuracy_all_d))","3a796645":"feat_imp = model.feature_importances_\n\ndf = pd.DataFrame(feat_imp)\n\ndf['ind'] = np.arange(len(feat_imp))\n\ndf['feat_imp'] = df[0]\n\ndf['labels'] = df.ind.map({0: 'temp', 1 : 'windspeed', 2 : 'humidity', 3 : 'Jan',  4 : 'Feb' , 5 : 'Mar', 6 : 'Apr' , 7 : 'May', 8 : 'Jun' , 9 : 'Jul', 10 : 'Aug', 11 : 'Sep', 12 : 'Oct', 13 : 'Nov', 14 : 'Dec' , 15 : 'Weat1', 16 : 'Weat2', 17 : 'Weat3', 18 : 'Sea1', 19 : 'Sea2', 20 : 'Sea3', 21 : 'Sea4',})\nprint(df.head())\n\nwidth = 0.1\nind = np.arange(len(feat_imp))\nax = df['feat_imp'].plot(kind='bar', xticks=df.index)\nax.set_xticklabels(df['labels'])\n\n\nplt.title('Feature Importance')\nplt.xlabel('Relative importance')\nplt.ylabel('feature')\n","6cee1b77":"### 5.1 Checking for Outliers","bbbbb635":"### 5.3 Plotting rest of data\n","6e5df916":"**This check is needed, as most of the machine learning models are tailor-made for working with Normally distributed data sets.**\n- Data seems to be following a normal distribution and is not skewed.","e0e1c358":"1. Predications based on hourly data were only to 65% accurate, wheares predications based on daily data were to 90% accurate. From the business perspective, an hourly perdication is not very useful, because the time frame is too short to react to, for instance, a drastic increase in the count, or defective bikes that need tp be replaced. Here daily data suffice. \n\n\n2. This data can be applied for anomaly detecion in the city: by analysing less common weather conditions (e.g. condition 4) combined with the information on the date, one can derive anomalies or uncommon events in the city. This essentially means, filtering the days where the count was significantly lower than average and filtering by less common weather conditions or windspeed > 0.6. Remove days that are weekends or holidays. Apply a search engine to find more about this date.\n\nOne can also analyse the outliers to understand what happend and why it happend in case its related to a rare event in the city.\n\n\n3. In addition to weather information, location coordinates might help indentify other significant patterns, such as how long is the drive on average for different days, seasons etc. Which locations need more bikes than provided or accumalte more bikes than needed to be used? What are key locations to place bikes at and which ones are important for effective distribution?   \n","0fe6f3a3":"There are no missing data points","29fb2873":"**Viewing data types**","6401833f":"### 4.1 Linear Regression Model: total count vs. temp, atemp, windspeed, humidity\n\nIs there a linear dependence between total count and the features temp, atemp, windspeed, humidity?","b6c38900":"### 2.1 Checking for missing values","d0ae4f1f":"## 3. Visulaizing Data: Plotting Features vs. Daily Count","79e45456":"- coefficient = beta (slope)\n- const. coeff = beta_0 (intercept)\n- std error = diviation from average\n- P-value (derived from t-value = how insiginificant is my value)\n\n- R-squared << 1, this means either no linearity or random effects in data + huge error (residual not 0)\n- There is a significance in the const. value because P = 0, the same goes for humidity and windspeed. ","dbd02cfe":"Positive correlations are displayed in blue and negaive correlations in yellow. \nTo visually understand correlations, plotting each feature with respect to total count can be helpful for trend recognition. ","dd74112c":"### 3.2 Removing outliers \n\nIf there were and any outliers this is how to remove them:","44fd07eb":"## 5. Loading and Visulaizing Data","8f6ecd6d":"# Analysing Hourly Data","70aa8cc5":"There is a clear monthly trend (upper graph): during colder months the count is lower compared to warmer months. This is also seen in the seasonal category (center graph). On average the rental behaviour during spring, summer and fall is the same. A significant decrease is observed in winter. The hourly trend shows that peak times are between 7-9 am, when people drive to work and 4-7 pm when people drive home from work. There is also a small increase durring lunch time (12-13 am). The lower plot shows the hourly distribution at different week days. \nWorking days mirror the trend seen in the central plot. Weekend days on the other hand show an interesting effect: there aren't clear peak times during the weekend. Instead the average usage is equal for a large hourly interval (approx. from 10 am - 6 pm) and decreases outside of this interval. \n\nOverall it can be concluded that bike usage is at its maximum during working days and peak hours. On weekends the usage on average is noramlly distrbuted. Seasonal effects generally play a role, however only in winter months. \n","21fde1d9":"### 5.2 Is the data normally dirstributed? ","5df76b52":"# Conclusion and Outlook","8bf3c864":"### 6.1 Predications from hourly data using Random Forest","feb99d29":"## 6 Feature Engineering","9ff13710":"### 3.1 Checking for outliers \n\n- plotting boxplots","a7e2dcab":"**Bike sharing data from Washington DC downoaded from UCI Machine Learning Repository**\n\nThe goal is to predict the usage or total count from the daily and hourly data. \nThis notebook describes the steps taken to clean and analyse the provided datasets. \n\nKey questions to be answered with the help of the analysis:\n\n- Is an hourly predication rational and necessary?\n- Is the daily predication enough?\n- Can we spot anomalies based on uncommon weather conditions? \n- What are other helpful features that can be collected and what can they be used for?","3837910f":"### 6.3 Feature Importance (only for daily model)","fe783ea2":"As seen from the correllation analysis, the features temp, windspeed and humidity are important features to be used for predication. Suprisingly, also the extreme seansons winter and summer turned out to be significant. Less significant, (however still important) are the months. ","22d16b02":"## 1. Importing libraries and reading data sets","2ab32565":"**No outliers found in the daily data set. In the case of outliers, see 3.2 Removing outliers**","38c5d4bb":"### 3.3 Checking for correlations between count and numerical features\n\nA fast way of identifying correlations between features is with a correlation matrix:","15e6a250":"## 2. Cleaning Daily Data\n\n- Columns will be renamed. \n- \"season\",\"month\",\"holiday\",\"workingday\" and \"weather\" should be of \"categorical\" data type.In the following these are converted.\n- We also keep the numerical category for mixed linear model analysis\n- Note: mistake in Read_me.txt in the asignment of the seasons: Winter =1, Spring = 2, Summer = 3; Fall = 4;","da7ce0ce":"**Removing the outliers here is ok, as it amount to only 1.4% of the total data set.\nThis step needs to be carefully considered, because valuable information can be contained in outliers. Anomalies can be gained from outlier information.** ","6dc87c3d":"**1. Create a matrix with values = False and NaN = True**\n\n**2. Find the true values**\n\n**3. Substitute with average (imputing)**\n","b6f55112":"There are no missing values","274853fd":"### 4.2 Mixed Linear Model: Total count vs. atemp, windspeed, humidity with random effect of month\n\nA more sophisticated model to allow both fixed and random effects. Here random effects within and between months are considered, as the month group gives a finer data increment (no. of groups = 12), compared to season and weather (no. of groups = 4) and the groups are on average of the same sizes. This is not the case for instance when considering weather conditions, as less data is available on weather condition 4 for example. It would result in a higher std. error.\n\nIn addition, only atemp is taken into consideration. ","af5bf1c1":"# Analysing Bike-Sharing Data","eb2947c5":"#### Is the data normaly distributed?","1b08b468":"## 4. Linear mixed model for daily data","b0d1c356":"**There aren't any outliers that were removed**","65ce995b":"**Compared to the daily data, the hourly data set is not entirely normally distributed and is left skewd. Caution when using ML models for predictions.**","38b6d764":"**There are several outliers, that will be removed.**","1e5c4e44":"- with temp as a feature,  model doesnt converge. Therefore atemp is used here\n- All fixed values (coef.) strongly significant as seen from P>z = 0\n- Confidence intervals contain no 0 values (different from 0)\n- Group, windspeed and humidity var are variances. \n- Random effects can be extracted from co-variances (Cov)\n- Correlations can be extracted from co-var\/var matrix: this shows weak correlations (negative var) between group and windspeed, group and humidity, group and windspeed+humidity, where the group is month. This is further indicated by the difference to the coef. \n\n\n","5cb1b550":"### 6.2 Predications from daily data using Random Forest"}}