{"cell_type":{"e0391de8":"code","be6b560d":"code","b78448db":"code","c8282ee6":"code","589408dd":"code","c93f741f":"code","9f2f61a7":"code","0512b29c":"code","08bbf173":"code","2d3f5dbc":"code","e6fd943b":"code","de7c9bdb":"code","83e2f682":"markdown","f26f5295":"markdown","d4e7391e":"markdown","ab63aeca":"markdown","e81a8f3b":"markdown","3bcd093f":"markdown","f6155f2d":"markdown","00ce967a":"markdown","55b757d0":"markdown","d9f204e2":"markdown"},"source":{"e0391de8":"import numpy as np\nimport matplotlib.pyplot as plt\n\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.preprocessing import PolynomialFeatures","be6b560d":"# Generate training samples\nx_train = np.random.rand(100,1)\ny_train = - x_train + 3 * (x_train ** 2) - 2 * (x_train ** 3) + 2 + np.random.rand(100,1) * 0.1\n\n# Generate some outlier points in the dataset \nx_train_noise = np.random.rand(10,1)\ny_train_noise = - x_train_noise + 3 * (x_train_noise ** 2) - 2 * (x_train_noise ** 3) + 2 \\\n                + np.random.rand(10,1) * 0.5\n\n# Combine 'normal' points and 'outlier' points to a single training set\nx_train = np.concatenate((x_train, x_train_noise), axis=0)\ny_train = np.concatenate((y_train, y_train_noise), axis=0)\n\n# Generate test samples\nx_test = np.random.rand(20,1)\ny_test = - x_test + 3 * (x_test ** 2) - 2 * (x_test ** 3) + 2 + np.random.rand(20,1) * 0.1","b78448db":"# Plot training samples\nplt.scatter(x_train,y_train, label='Training samples')\nplt.scatter(x_test,y_test, label='Test samples')\nplt.xlabel('x', fontsize=18)\nplt.ylabel('y', rotation=0, fontsize=18)\nplt.legend()","c8282ee6":"# Generate polynomial features\npolynomial_features= PolynomialFeatures(degree=1)\nx_train_poly = polynomial_features.fit_transform(x_train)[:,1:]\nx_test_poly = polynomial_features.fit_transform(x_test)[:,1:]\n\n# Create linear regression model\nmodel = LinearRegression()\n\n# fit model to polynomial data\nmodel.fit(x_train_poly, y_train)\n\n# print fitted model\nprint('Coef:', model.coef_, 'Intercept:', model.intercept_)","589408dd":"print('Train score:', mean_squared_error(model.predict(x_train_poly), y_train))\nprint('Test score:', mean_squared_error(model.predict(x_test_poly), y_test))","c93f741f":"idx = np.argsort(x_train, axis=0)[:,0]\nplt.plot(x_train[idx], model.predict(x_train_poly)[idx], 'r', label='Fitting line')\nplt.scatter(x_train,y_train, label='Training samples')\nplt.scatter(x_test,y_test, label='Test samples')\nplt.legend()","9f2f61a7":"# Generate polynomial features\npolynomial_features= PolynomialFeatures(degree=3)\nx_train_poly = polynomial_features.fit_transform(x_train)[:,1:]\nx_test_poly = polynomial_features.fit_transform(x_test)[:,1:]\n\n# Create linear regression model\nmodel = LinearRegression()\n\n# fit model to polynomial data\nmodel.fit(x_train_poly, y_train)\n\n# print fitted model\nprint('Coef:', model.coef_, 'Intercept:', model.intercept_)","0512b29c":"print('Train score:', mean_squared_error(model.predict(x_train_poly), y_train))\nprint('Test score:', mean_squared_error(model.predict(x_test_poly), y_test))","08bbf173":"idx = np.argsort(x_train, axis=0)[:,0]\nplt.plot(x_train[idx], model.predict(x_train_poly)[idx], 'r', label='Fitting line')\nplt.scatter(x_train,y_train, label='Training samples')\nplt.scatter(x_test,y_test, label='Test samples')\nplt.legend()","2d3f5dbc":"# Generate polynomial features\npolynomial_features= PolynomialFeatures(degree=30)\nx_train_poly = polynomial_features.fit_transform(x_train)[:,1:]\nx_test_poly = polynomial_features.fit_transform(x_test)[:,1:]\n\n# Create linear regression model\nmodel = LinearRegression()\n\n# fit model to polynomial data\nmodel.fit(x_train_poly, y_train)\n\n# print fitted model\nprint('Coef:', model.coef_, 'Intercept:', model.intercept_)","e6fd943b":"print('Train score:', mean_squared_error(model.predict(x_train_poly), y_train))\nprint('Test score:', mean_squared_error(model.predict(x_test_poly), y_test))","de7c9bdb":"idx = np.argsort(x_train, axis=0)[:,0]\nplt.plot(x_train[idx], model.predict(x_train_poly)[idx], 'r', label='Fitting line')\nplt.scatter(x_train,y_train, label='Training samples')\nplt.scatter(x_test,y_test, label='Test samples')\nplt.legend()","83e2f682":"### Okay it's not ;)","f26f5295":"### Let's plot the fitting line we have just trained.","d4e7391e":"### Firstly, we will need some data to start with.","ab63aeca":"### Or it's just higher degree is *always* better? Let's see what we got with degree 30 features.","e81a8f3b":"As usual, we will import stuffs we are going to use.","3bcd093f":"Err... What the !@#$ fitting line is this?","f6155f2d":"### Let's see what we have here.","00ce967a":"### Let's try some simple linear regression model without any high degree polynomial (or we can say degree = 1)","55b757d0":"Okay so we can see we will need a polynomial line which describe:\n\\begin{equation} y = \\theta_0.x + \\theta_1.x^2 + \\theta_2.x^3 + \\theta_3 \\end{equation} \n\nWhich is still a linear combination of *thetas* but use higher degree of *x* as features. So we can use the **PolynomialFeatures()** function from sklearn those features.","d9f204e2":"### Okay-ish. But I bet the degree 3 polynomial will work better."}}