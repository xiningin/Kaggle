{"cell_type":{"63556770":"code","e969dcc6":"code","e66e6057":"code","16734a15":"code","85891390":"code","c745257f":"code","fc985c05":"code","b3ebf04a":"code","edf9657b":"code","afa4eff2":"code","146b2553":"code","e2240874":"code","948373f9":"code","7dbad08b":"code","b455cd1a":"code","9806d993":"code","0a75d97b":"code","9ef6f9f3":"code","32023dd0":"code","9fd51ce6":"code","5a3a3fe8":"code","1c10dc54":"code","172b6727":"code","62c712b4":"code","9d57c3e9":"code","3384b965":"code","96826b31":"code","c5d7182b":"code","4d41b990":"code","e21d6a7a":"code","62ac5f93":"code","0dc5b1f7":"code","1c328ee8":"code","6e148375":"code","86383c4b":"code","470bd403":"code","6a2c50d2":"code","7d293751":"code","a5eeeb09":"code","119e7e5c":"code","7ccc963f":"code","f6cb4c7b":"code","315e56c8":"code","371bd023":"code","e55bad95":"code","d64fab2a":"code","a5deca56":"code","6f4af026":"code","e9788853":"code","b7a01cd8":"code","72ac873f":"code","f0e05476":"code","6e02591f":"code","242688f9":"code","106a35a7":"code","a8bdadc2":"code","5c58627a":"code","04b09b25":"code","5285a3ad":"code","ddaa8808":"code","96bac8c0":"code","c9dc2499":"code","9fb453db":"code","12d6436e":"markdown","42145897":"markdown","3fbb344d":"markdown","23eb8916":"markdown","f76fc54d":"markdown","349ce8c4":"markdown","927f3cf8":"markdown","bd0aba6f":"markdown","373d11f9":"markdown","8a299525":"markdown","b7d80ab1":"markdown","3bb92a23":"markdown","c8495e22":"markdown","0c4fe554":"markdown","14d5a5d0":"markdown","1512790c":"markdown","51a0e91f":"markdown","29938fca":"markdown","0e73aafb":"markdown","1fb3d4c5":"markdown","6f199443":"markdown","b58e9a7e":"markdown","32370a90":"markdown","e76a4fc6":"markdown","2007f7fe":"markdown","2d26dbc0":"markdown"},"source":{"63556770":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","e969dcc6":"## Performance \n### Hyper parameter tuning:\n\n# xgboost : 0.9460779708783467\n# RandomForestClassifier : 0.8956317519962423\n\n\n### UP SAMPLING and One Hot encoding\n\n# Catboost : 0.9428839830906529\n# LightGBM : 0.9457022076092062\n\n\n### No Up sampling and LabelEncoding\n\n# Catboost : 0.9081063340991139\n# LightGBM : 0.9043321299638989\n\n\n### Voting Classifier ( Logistic Regression, KNN, Decision Tree Classifier)\n\n## Accuracy : 0.9389384687646782","e66e6057":"import matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.decomposition import PCA","16734a15":"df = pd.read_csv(r'\/kaggle\/input\/bank-marketing\/bank-additional-full.csv', sep=';')","85891390":"df.drop_duplicates(keep=False, inplace=True) ","c745257f":"columns = df.columns.tolist()\nnumerical = [feature for feature in columns if df[feature].dtypes!='O']\ncategorical = [feature for feature in columns if feature not in numerical]\npolychotomus = [feature for feature in categorical if feature not in ['default','housing','loan','contact','y']]","fc985c05":"for feature in columns:\n    df[feature] = np.where(df[feature]=='unknown',np.nan,df[feature])","b3ebf04a":"## Missing values in Categorical\n\ndef Missing(df,columns):\n    missing = []\n    for feature in columns:\n        missing_val = np.round(df[feature].isna().sum()\/len(df), 3)*100\n        missing.append([feature, missing_val])\n    miss_df = pd.DataFrame(missing, columns=['Feature', '% Missing'])\n    miss_df = miss_df[miss_df['% Missing'] != 0]\n    return miss_df","edf9657b":"Missing(df, columns)","afa4eff2":"# Missing values in Numerical\n\ncount_999 = []\nfor feature in numerical:\n    if (999 in df[feature].unique()):\n        count_val = np.round((df[feature].value_counts()[999]\/len(df[feature])),4)*100\n        count_999.append([feature, count_val])\n    else:\n        pass\npd.DataFrame(count_999, columns=['Feature', '% Missing'])","146b2553":"df.dropna(axis=0,inplace=True)","e2240874":"df.drop(['pdays','default'],axis=1, inplace=True)\ncolumns.remove('pdays')\ncolumns.remove('default')","948373f9":"numerical = [feature for feature in columns if df[feature].dtypes!='O']\ncategorical = [feature for feature in columns if feature not in numerical]\npolychotomus = [feature for feature in categorical if feature not in ['housing','loan','contact','y']]","7dbad08b":"def unique(df, column):\n    unique_vals = []\n    for feature in column:\n        val1 = df[feature].nunique()\n        unique_vals.append([feature, val1])\n    unique_df = pd.DataFrame(unique_vals, columns=['Feature', 'No. Unique'])\n\n    return unique_df","b455cd1a":"## Numerical Variables Describe\ndef describe_num(df,numerical):\n    vals = []\n    for feature in numerical:\n        feat_des = [feature, df[feature].nunique(),np.round(df[feature].mean(),2), min(df[feature]),\n                    np.quantile(df[feature],0.25),np.quantile(df[feature],0.5),np.quantile(df[feature],0.75),\n                    max(df[feature])]\n        vals.append(feat_des)\n    desc_num = pd.DataFrame(vals, columns=['Feature','No. Unique','Mean','Min','Q1','Q2','Q3','Max'])\n    return desc_num","9806d993":"describe_num(df,numerical)","0a75d97b":"unique(df, categorical)","9ef6f9f3":"ax = sns.countplot(df['y'])\nfor p in ax.patches:\n        ax.annotate('%{:.1f}'.format(np.round(p.get_height()\/len(df),3)), (p.get_x()+0.3, p.get_height()+100))","32023dd0":"## Duration among the people who have and haven't agreed to the plan\n\nplt.figure(figsize=(10,6))\nsns.distplot(df[df['y']=='yes']['duration'])\nsns.distplot(df[df['y']=='no']['duration'])\nplt.legend(labels=['Yes','No'])\nplt.show()","9fd51ce6":"plt.figure(figsize=(15,6))\n\n\nplt.subplot(1,2,1)\nsns.distplot(df[df['y']=='yes']['campaign'])\nsns.distplot(df[df['y']=='no']['campaign'])\nplt.title('Campaign vs target')\nplt.legend(labels=['Yes','No'])\n\nplt.subplot(1,2,2)\nsns.distplot(df[df['y']=='yes']['campaign'])\nsns.distplot(df[df['y']=='no']['campaign'])\nplt.title('Zoom to Peak')\nplt.xlim([0,20])\nplt.ylim([0.2,1.2])\nplt.legend(labels=['Yes','No'])\n\nplt.show()","5a3a3fe8":"## Month \n\nplt.figure(figsize=(10,5))\nsns.countplot(df['month'], order=['mar','apr','may','jun','jul','aug','sep','oct','nov','dec'], hue=df['y'])\nplt.show()","1c10dc54":"# Age Bin\n\ndef feat_creat(df):\n    ## Age Binn\n    age_bin = []\n    for val in df['age']:\n        if (val <= 32):\n            age_bin.append(1)\n        elif (val>32 and val<=38):\n            age_bin.append(2)\n        elif (val>38 and val<=47):\n            age_bin.append(3)\n        else:\n            age_bin.append(4)\n    df['Age_Bin'] = age_bin","172b6727":"## Principal Component Analysis\n\npca = PCA(n_components=1)\n\npca.fit(df[numerical])\nl1 = pca.transform(df[numerical])\ndf['PCA'] = l1\n\nfeat_creat(df)","62c712b4":"## Housing, Loan : Dichotomus (yes, no)\n\nmap_dichot = {'yes':1,'no':0}\nfor feature in ['housing','loan','y']:\n    df[feature] = df[feature].map(map_dichot)\n\n## Contact : Dichotomus (Telephone, cellular)\ndf['contact'] = df['contact'].map({'telephone':1,'cellular':0})\n","9d57c3e9":"## df1 = label Encoding\n## df2 = one hot encoding\n\ndf1 = df.copy()\ndf2 = df.copy()","3384b965":"## Label Encoding Polychotomus Features\n\nfrom sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()","96826b31":"for feature in polychotomus:\n    df1[feature] = le.fit_transform(df1[feature])","c5d7182b":"## One Hot Encoding Categorical Features\n\ndf2 = pd.get_dummies(df, columns=polychotomus, drop_first=True)","4d41b990":"## Label Encoded variables\n\nx1 = df1.drop(['y'],axis=1)\ny1 = df1['y']","e21d6a7a":"## One Hot Encoded Variables\n\nx2 = df2.drop(['y'],axis=1)\ny2 = df2['y']","62ac5f93":"from imblearn.over_sampling import SMOTE\n\nSMOTE_OBJ = SMOTE()\nxu,yu = SMOTE_OBJ.fit_sample(x2,y2)### Imp note hereee ","0dc5b1f7":"from sklearn.ensemble import RandomForestClassifier\nfrom catboost import CatBoostClassifier\nfrom xgboost.sklearn import XGBClassifier\nfrom sklearn.neighbors import KNeighborsClassifier \nfrom lightgbm import LGBMClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import VotingClassifier\n\n\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.model_selection import train_test_split\nimport pickle\nfrom sklearn.metrics import classification_report","1c328ee8":"### UP SAMPLING TRAIN TEST SPLIT\n\nx_train,x_test,y_train,y_test = train_test_split(xu,yu,test_size=0.2,random_state=42)","6e148375":"## XGB\n\nxgb = XGBClassifier()\n\nparam_grid = dict(learning_rate = [0.05, 0.1, 0.15, 0.2, 0.25, 0.3],\n                  boosting_type=['gbdt','dart','goss','rf'],\n                 n_estimators=[100, 200, 350, 400])\nxgb_ran = RandomizedSearchCV(xgb, param_grid, cv=5, n_jobs=-1)","86383c4b":"xgb_ran.fit(x_train, y_train)","470bd403":"xgb_ran.score(x_test, y_test)","6a2c50d2":"print(xgb_ran.best_params_)","7d293751":"pred_xgb = xgb_ran.predict(x_test)","a5eeeb09":"print(classification_report(y_test, pred_xgb))","119e7e5c":"## RFC\n\nrandom_grid = {'n_estimators': [100,200,400,300],\n               'max_features': ['auto','sqrt'],\n               'max_depth': [1, 3, 5],\n               'min_samples_split': [5,10,15]}\n\nrfc_ran = RandomizedSearchCV(estimator = RandomForestClassifier(), param_distributions = random_grid, \n                               n_iter = 7, cv = 5, random_state=42, n_jobs = -1)","7ccc963f":"rfc_ran.fit(x_train, y_train)","f6cb4c7b":"print(rfc_ran.best_params_)","315e56c8":"rfc_ran.score(x_test, y_test)","371bd023":"pred_rfc = rfc_ran.predict(x_test)\nprint(classification_report(y_test, pred_rfc))","e55bad95":"## cat boost upsampling\n\ncat = CatBoostClassifier()\ncat.fit(x_train,y_train)","d64fab2a":"cat.score(x_test, y_test)","a5deca56":"pred_cat = cat.predict(x_test)\nprint(classification_report(y_test, pred_cat))","6f4af026":"lgb = LGBMClassifier()\nlgb.fit(x_train, y_train)","e9788853":"lgb.score(x_test, y_test)","b7a01cd8":"pred_lgb = lgb.predict(x_test)\nprint(classification_report(y_test, pred_lgb))","72ac873f":"lgb_featimp = pd.DataFrame(lgb.feature_importances_, index=x_train.columns).sort_values(by=0,ascending=False)","f0e05476":"## Feature importance LGBM\n\nplt.figure(figsize=(10,12))\nsort_lgb = lgb.feature_importances_.argsort()\nplt.barh(x_train.columns[sort_lgb], lgb.feature_importances_[sort_lgb])\nplt.show()","6e02591f":"estimator = [] \nestimator.append(('LR',  \n                  LogisticRegression(max_iter = 10000))) \nestimator.append(('KNN', KNeighborsClassifier(n_neighbors=5, metric='minkowski', p=2 ))) \nestimator.append(('DTC', DecisionTreeClassifier())) ","242688f9":"vot_soft = VotingClassifier(estimators = estimator, voting ='soft') ","106a35a7":"vot_soft.fit(x_train, y_train)","a8bdadc2":"vot_soft.score(x_test, y_test)","5c58627a":"pred_vot_soft = vot_soft.predict(x_test)\nprint(classification_report(y_test, pred_vot_soft))","04b09b25":"\nx1_train,x1_test,y1_train,y1_test = train_test_split(x1,y1,test_size=0.2,random_state=42)","5285a3ad":"cat.fit(x1_train, y1_train)","ddaa8808":"print(\"Accuracy : \", cat.score(x1_test, y1_test),\"\\n\")\nprint(classification_report(y1_test, cat.predict(x1_test)))\n","96bac8c0":"lgb.fit(x1_train, y1_train)","c9dc2499":"print(\"Accuracy : \", lgb.score(x1_test, y1_test),\"\\n\")\nprint(classification_report(y1_test, lgb.predict(x1_test)))","9fb453db":"### Hyper parameter tuning:\n\n# xgboost : 0.9460779708783467\n# RandomForestClassifier : 0.8956317519962423\n\n\n### UP SAMPLING and One Hot encoding\n\n# Catboost : 0.9428839830906529\n# LightGBM : 0.9457022076092062\n\n\n### No Up sampling and LabelEncoding\n\n# Catboost : 0.9081063340991139\n# LightGBM : 0.9043321299638989\n\n\n### Voting Classifier ( Logistic Regression, KNN, Decision Tree Classifier)\n\n## Accuracy : 0.9389384687646782","12d6436e":"# Bank Marketing ","42145897":"## Exploratory Data Analysis ","3fbb344d":"#### XGBoost ","23eb8916":"#### Catboost ","f76fc54d":"## Feature Engineering","349ce8c4":"## Preprocessing  ","927f3cf8":"### Unique Values  ","bd0aba6f":"### Encoding ","373d11f9":"## Modelling ","8a299525":"### No upsampling technique used and Label Encoded","b7d80ab1":"#### Catboost  ","3bb92a23":"The Dataset contains missing values in categorical varaibles as \"Unknown\" and in numerical variables as \"999\" ","c8495e22":"### Feature Creation ","0c4fe554":"### Random Forest  and XGBoost Hyperparameter tuning ","14d5a5d0":"Clients who have agreed for a term deposit plan tend to have a larger duration of conversation. Probably to get to know more about the details of the term deposit plan. On the other hand, the clients who are not interested donot attend the call for a long duration","1512790c":"Most Employees have been contacted in May followed by July, August and then June. The percent of people agreeing to a Term deposit plan are also significantly large as compared in other months. ","51a0e91f":"pdays and default have 96.32 and 20.9 percent missing values respectively hence need to drop these two columns. Also the rest of the columns having missing values, the rows have been dropped.","29938fca":"### Upsampling and One Hot Encoding","0e73aafb":"#### LGBM ","1fb3d4c5":"The aim here is to classify whether a client will subscribe to a term deposit plan or not. (Binary Classification)\nDataset url: https:\/\/www.kaggle.com\/henriqueyamahata\/bank-marketing","6f199443":"### Missing Values ( Unknown, 999 ) ","b58e9a7e":"Dataset is highly imbalanced 9:1","32370a90":"### Upsampling using Smote ","e76a4fc6":"#### LGBM","2007f7fe":"#### Random Forest ","2d26dbc0":"#### Voting Classifier "}}