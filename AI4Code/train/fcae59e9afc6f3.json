{"cell_type":{"7454869d":"code","abbeccae":"code","aa5d7d2f":"code","1601976f":"code","85a83b6b":"markdown","f68467cd":"markdown","7f1f1bf9":"markdown"},"source":{"7454869d":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nimport seaborn as sns\nimport scipy.stats as stats\nimport matplotlib.pyplot as plt\nfrom statsmodels.api import  qqplot\n\nfrom sklearn import preprocessing as preprocessing\nfrom sklearn.linear_model import LinearRegression, RidgeCV, LassoCV, Ridge\nfrom sklearn.metrics import mean_absolute_error\nimport types\n\n\nfrom scipy.stats import shapiro, normaltest, anderson\nfrom sklearn.pipeline import Pipeline, FeatureUnion\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.compose import TransformedTargetRegressor\nfrom sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin\nfrom sklearn.model_selection import GridSearchCV\n\ndef load_data(path):\n    return pd.read_csv(path+'train.csv', index_col='Id'), pd.read_csv(path+'test.csv', index_col='Id')\n\ndef init_sns():\n    # set seaborn style dark\n    sns.set_style('dark')\n    \ndef qq_plot(x, **kwargs):\n    qqplot(x, ax=plt.gca(), **kwargs)\n\ndef cat_cols(df):\n    \"\"\"return categorical columns\n    \"\"\"\n    return [cname for cname in df.columns if\n                    df[cname].dtype == \"O\"]\n\ndef num_cols(df):\n    \"\"\"return numerical columns\n\n        total columns = categorical columns + numerical columns\n    \"\"\"\n    return df.columns[~df.columns.isin(cat_cols(df))]\n\ndef get_cols_with_(x, data):\n    \"\"\" return columns of DataFrame 'data', which contains str 'x'\n    \"\"\"\n    cols = []\n    for c in data.columns:\n        if x in c:\n            cols.append(c)\n    return cols\n\ndef save(index, preds):\n    # Save test predictions to file\n    output = pd.DataFrame({'Id': index,\n                           'SalePrice': preds})\n    output.to_csv('submission.csv', index=False)\n    \ndef nomality_tests(data):\n    \"\"\"\n    return shpiro test result(s1, p1),\n    D\u2019Agostino\u2019s K^2 Test result(s2,p2), \n    Anderson-Darling Test result\n    in tuple\n    \"\"\"\n    s1, p1 = shapiro(data)\n    s2, p2 = normaltest(data)\n    result = anderson(data)\n    return (s1, p1), (s2, p2), result\n\ndef linear_regression(data):\n    lr = LinearRegression()\n    x_train, y_train = data[n_feats], data['SalePrice']\n    lr.fit(x_train, y_train)\n    return lr\n\ndef fill_garageyrblt(data, c):\n    rows = data['GarageYrBlt'].isnull()\n    data.loc[rows, 'GarageYrBlt']= \\\n    data.loc[rows, 'YearBuilt']\n    \ndef miss_val_handler(data):\n    \n    def _handle_(data, c, v):\n        if isinstance(v, types.FunctionType):\n            v(data, c)\n        else:\n            null_r = data[c].isnull()\n            if null_r.sum() == 0: return\n            data.loc[null_r, c] = v        \n\n    for c in MISS_VAL:\n        v = MISS_VAL[c]\n        if isinstance(v, tuple):\n            for e in v:\n                _handle_(data, c, e)\n        else:\n            _handle_(data, c, v)\n            \n    return data\n            \ndef BsmtExposure_no(data,c):\n    #data.loc[949, 'BsmtExposure']='No'\n    pass\n\ndef median_impute(data, c):\n    null_r = data[c].isnull()\n    if null_r.sum() == 0: return\n    data.loc[null_r, c] = data[c].median()\n    \ndef most_freq_impute(data, c):\n    null_r = data[c].isnull()\n    if null_r.sum() == 0: return\n    data.loc[null_r, c] = data[c].mode()[0]\n    \n# load data to TRAIN_DATA, TEST_DATA\nPATH = '\/kaggle\/input\/home-data-for-ml-course\/'\nTRAIN_DATA, TEST_DATA = load_data(PATH)\n\n## Chosen numeric features\nN_FEATS = ['BedroomAbvGr', 'GrLivArea', 'LowQualFinSF', 'YearBuilt', 'FullBath',\n           'EnclosedPorch', '1stFlrSF', 'BsmtFinSF2', 'OverallQual', 'YearRemodAdd',\n           'BsmtUnfSF', 'LotArea', 'KitchenAbvGr', 'Fireplaces', 'OverallCond',\n           'TotRmsAbvGrd', 'PoolArea', 'LotFrontage', 'OpenPorchSF', 'BsmtFinSF1',\n           'GarageYrBlt', 'TotalBsmtSF', 'GarageCars', '3SsnPorch', 'BsmtHalfBath',\n           'WoodDeckSF', '2ndFlrSF', 'GarageArea', 'BsmtFullBath', 'MiscVal', 'ScreenPorch',\n           'HalfBath', 'MasVnrArea', 'YrSold']\n\n## Chosen categrical featrues\nC_FEATS = ['MSZoning', 'Street', 'Alley', 'LotShape', 'LandContour', 'Utilities',\n           'LotConfig', 'LandSlope', 'Neighborhood', 'Condition1', 'Condition2', \n           'BldgType','HouseStyle', 'RoofStyle', 'RoofMatl', 'Exterior1st',\n           'Exterior2nd', 'MasVnrType', 'ExterQual', 'ExterCond', 'Foundation',\n           'BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2',\n           'Heating', 'HeatingQC', 'CentralAir', 'Electrical', 'KitchenQual',\n           'Functional', 'FireplaceQu', 'GarageType', 'GarageFinish', 'GarageQual',\n           'GarageCond', 'PavedDrive', 'PoolQC', 'Fence', 'MiscFeature', 'SaleType',\n           'SaleCondition', 'MoSold', 'MSSubClass']\n\n## Doc of Values for missing value to fill.\n## where key is column name and value to fill missing value\nMISS_VAL = {'PoolQC': 'NA',\n            'FireplaceQu': 'NA',\n            'GarageCond': 'NA',\n            'GarageType': 'NA',\n            'GarageFinish': 'NA',\n            'GarageQual': 'NA',\n            'BsmtExposure': ('NA', BsmtExposure_no),\n            'BsmtFinType2': 'NA',\n            'BsmtFinType1': 'NA',\n            'BsmtQual': 'NA',\n            'BsmtCond': 'NA',\n            'MasVnrType': 'None',\n            'Electrical': 'SBrkr',\n            'MasVnrArea': 0,\n            'LotFrontage': median_impute,\n            'GarageYrBlt': fill_garageyrblt,\n            'BsmtFullBath':0,\n            'BsmtHalfBath':0,\n            'TotalBsmtSF': 0,\n            'BsmtFinSF1': 0,\n            'BsmtUnfSF': 0, \n            'BsmtFinSF2': 0,\n            'Exterior1st': most_freq_impute,\n            'Exterior2nd': most_freq_impute,\n            'MSZoning': most_freq_impute,\n            'Functional': most_freq_impute,\n            'Utilities': most_freq_impute,\n            'KitchenQual': most_freq_impute,\n            'SaleType': most_freq_impute,\n            'GarageArea': 0,\n            'GarageCars': 0}\n\n## Ordinal features\nORDINAL = ['ExterQual','BsmtQual', 'KitchenQual',\n           'GarageQual', 'HeatingQC', 'PoolQC',\n           'GarageType', 'GarageFinish', 'GarageCond',\n           'BsmtCond', 'BsmtExposure', 'BsmtFinType1', \n           'BsmtFinType2', 'FireplaceQu', 'Electrical',\n           'LotShape', 'Functional', 'ExterCond',\n           'Utilities', 'PavedDrive']\n\n## Features to be one-hot encoded\nONE_HOT = ['LandSlope', 'LotConfig', 'Foundation', 'MSSubClass', 'SaleType', 'MSZoning',\n           'Neighborhood', 'MoSold', 'Condition1', 'Exterior2nd', 'BldgType', 'RoofStyle',\n           'HouseStyle', 'CentralAir', 'Heating', 'MasVnrType', 'LandContour', 'Street',\n           'Exterior1st', 'Condition2', 'SaleCondition', 'RoofMatl']\n\nQT = preprocessing.QuantileTransformer(output_distribution='normal', random_state=0)\nOHE =  preprocessing.OneHotEncoder(dtype='int', drop='first')\nORDE = preprocessing.OrdinalEncoder()\nCT = ColumnTransformer([('ohe', OHE, ONE_HOT),\n                        ('orde', ORDE, ORDINAL),\n                        ('qt', QT, N_FEATS)],\n                       remainder='drop')\n\nFEATS = N_FEATS + ORDINAL + ONE_HOT\n\nREGRESSOR = TransformedTargetRegressor(regressor=LinearRegression(),\n                                       transformer=QT)\n\nMISS_VAL_IMPUTER = preprocessing.FunctionTransformer(miss_val_handler, validate=False)\n\nPIPELINE = Pipeline(memory=None,\n                    steps=[('imputer', MISS_VAL_IMPUTER),\n                           ('col_trans', CT),\n                           ('linear_reg', REGRESSOR)],\n                    verbose=True)\n\ndef ols_regression(x_train, x_test):\n    X_train = x_train.copy()\n    X_test = x_test.copy()\n\n    # test + train data merge\n    merged = pd.concat([X_train, X_test], sort=True)\n    miss_val_handler(merged)\n    # missing value handle\n    X_train[949, 'BsmtExposure'] = 'No'\n\n    PIPELINE.fit(merged.loc[:1460,:].copy(), merged.loc[:1460,'SalePrice'].copy())\n    return mean_absolute_error(X_train['SalePrice'],\n                              PIPELINE.predict(merged.loc[:1460,:].copy()))\n\nMSSUBCLASS_OHE =  preprocessing.OneHotEncoder(dtype='int', handle_unknown='ignore')\n\nCT2 = ColumnTransformer([('mssubclass_ohe', MSSUBCLASS_OHE, ['MSSubClass']),\n                        ('ohe', OHE, list(set(ONE_HOT) - set(['MSSubClass']))),\n                        ('orde', ORDE, ORDINAL),\n                        ('qt', QT, N_FEATS)],\n                       remainder='drop')\n\nRIDGE_REGRESSOR = TransformedTargetRegressor(\n    regressor=RidgeCV(alphas=[1e-6, 1e-3, 1e-2, 1e-1, 1], scoring='neg_mean_absolute_error'),\n    transformer=QT)\n\nRIDGE_PIPELINE = Pipeline(memory=None,\n                    steps=[('imputer', MISS_VAL_IMPUTER),\n                           ('transformer', CT2),\n                           ('regressor', RIDGE_REGRESSOR)],\n                    verbose=True)\n\ndef ridge_regression(x_train, x_test, save_test=False):\n    X_train = x_train.copy()\n    X_test = x_test.copy()\n\n    # test + train data merge\n    merged = pd.concat([X_train, X_test], sort=True)\n    miss_val_handler(merged)\n    # missing value handle\n    X_train[949, 'BsmtExposure'] = 'No'\n\n    RIDGE_PIPELINE.fit(merged.loc[:1460,:].copy(), merged.loc[:1460,'SalePrice'].copy())\n    \n        \n    if save_test:\n        preds = RIDGE_PIPELINE.predict(merged.loc[1461:,:].copy())\n        save(X_test.index, preds)\n        \n    return mean_absolute_error(X_train['SalePrice'],\n                              RIDGE_PIPELINE.predict(merged.loc[:1460,:].copy()))\n\n\nclass HPLassoCV(BaseEstimator, RegressorMixin):\n    \n    def __init__(self, regressor=None, transformer=None, imputer=None):\n        if regressor is None:\n            self.regressor = LassoCV(cv=10, random_state=0)\n        else: self.regressor = regressor\n        \n        if imputer is None:\n            self.imputer = preprocessing.FunctionTransformer(miss_val_handler, validate=False)\n        else:\n            self.imputer = imputer\n        \n        if transformer is None:\n            self.transformer = ColumnTransformer([('mssubclass_ohe', MSSUBCLASS_OHE, ['MSSubClass']),\n                                                  ('ohe', OHE, list(set(ONE_HOT) - set(['MSSubClass']))),\n                                                  ('orde', ORDE, ORDINAL),\n                                                  ('qt', QT, N_FEATS)],\n                                                 remainder='drop')\n        else:\n            self.transformer = transformer\n            \n        self.pipeline = Pipeline(memory=None,\n                                 steps=[('imputer',self.imputer),\n                                        ('transformer', self.transformer),\n                                        ('regressor', self.regressor)],\n                                 verbose=True)\n        \n    def fit(self, x_train, y_train):\n        X_train = x_train.copy()\n        Y_train = y_train.copy()\n        self.pipeline.fit(X_train, Y_train)\n        return self\n    \n    def predict(self, x_test):\n        X_test = x_test.copy()\n        return self.pipeline.predict(x_test)\n    \n    def path(self, x_train, y_train):\n        return self.regressor.path(self.pipeline[:-1].fit_transform(x_train, y_train), y_train)\n    \n    @property\n    def alpha_(self):\n        return self.regressor.alpha_","abbeccae":"class HPRidge(BaseEstimator, RegressorMixin):\n    \n    def __init__(self, \n                 regressor=None,\n                 transformer=None,\n                 imputer=None, \n                 param_grid=None,\n                 alpha=1.0, \n                 copy_X=True, \n                 fit_intercept=True, \n                 max_iter=None,\n                 normalize=False,\n                 random_state=None,\n                 solver='auto',\n                 tol=0.001):\n        \n        if regressor is None:\n            self.regressor = Ridge(alpha=alpha, copy_X=copy_X, fit_intercept=fit_intercept,\n                                   max_iter=max_iter, normalize=normalize, random_state=random_state,\n                                   solver=solver, tol=tol)\n        else: self.regressor = regressor\n        \n        if imputer is None:\n            self.imputer = preprocessing.FunctionTransformer(miss_val_handler, validate=False)\n        else:\n            self.imputer = imputer\n        \n        if transformer is None:\n            self.transformer = ColumnTransformer([('mssubclass_ohe', MSSUBCLASS_OHE, ['MSSubClass']),\n                                                  ('ohe', OHE, list(set(ONE_HOT) - set(['MSSubClass']))),\n                                                  ('orde', ORDE, ORDINAL),\n                                                  ('qt', QT, N_FEATS)],\n                                                 remainder='drop')\n        else:\n            self.transformer = transformer\n            \n        self.preprocessing = Pipeline(memory=None,\n                                 steps=[('imputer',self.imputer),\n                                        ('transformer', self.transformer)],\n                                 verbose=True)\n        self.gridsearch = GridSearchCV(self.regressor, param_grid, cv=5)\n    def fit(self, x_train, y_train):\n        X_train = x_train.copy()\n        Y_train = y_train.copy()\n        return self.gridsearch.fit(self.preprocessing.fit_transform(X_train), Y_train)\n\n    @property\n    def alpha_(self):\n        return self.regressor.alpha_\n    \n    @property\n    def cv_results_(self):\n        return self.gridsearch.cv_results_\n    \n    @property\n    def best_params_(self):\n        return self.gridsearch.best_params_\n    \n    @property\n    def best_estimator_(self):\n        return self.gridsearch.best_estimator_\n    \nparam_grid = [{'alpha':[1e-6, 1e-3, 1e-2, 1e-1, 1, 10, 20]}]\ngs = HPRidge(param_grid=param_grid)\ngs.fit(TRAIN_DATA.drop('SalePrice', axis=1), TRAIN_DATA['SalePrice'])","aa5d7d2f":"gs.cv_results_","1601976f":"gs.best_params_","85a83b6b":"# Goal\n\nApply grid search to ridge regression\n\n# Data load and utilities","f68467cd":"# Search what?\n\nRidge regression's $\\alpha$ is the searching target","7f1f1bf9":"## best parameter $\\alpha$"}}