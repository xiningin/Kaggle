{"cell_type":{"a752df40":"code","cc1f8e6e":"code","8f5349ba":"code","963ac1c7":"code","a90603aa":"code","7b343977":"code","40eb9cfa":"code","794570fb":"code","84b50c07":"code","3fecccf6":"code","bfb7eec1":"code","30f9e139":"code","9afe6120":"code","dbbd6abf":"code","f84b18b5":"code","17f5f4a2":"code","69da5dad":"code","401f035d":"code","b53d3017":"code","146b3868":"code","7c4a1456":"code","7b494cf8":"code","48319edb":"code","2fe0f103":"code","7241eaca":"code","6c965674":"code","70a4af39":"code","ef740f11":"code","c938a994":"code","a1227b26":"code","33bf26b3":"code","fdc9b04e":"code","cec2d036":"code","6b9d8966":"code","86423f6d":"code","2ed2c587":"code","1cdf6537":"code","e255d97e":"code","8e132dfa":"code","7a2a9b3c":"code","df69abf3":"code","b6b4facf":"code","05f8d5df":"code","dec0f78b":"code","bd7a41f8":"code","177305d7":"code","bed06eae":"code","064daa53":"code","27a2f107":"code","e447be7c":"code","b1293f5a":"code","732ef579":"code","5098c666":"code","d0a1e98b":"code","6b61b496":"code","c1daa0e1":"code","5e9f464b":"code","a8b45393":"code","2129e0c9":"code","3ad06c24":"code","8351f503":"code","60183c3f":"code","7b90706c":"code","499d887b":"code","75170b8e":"code","27ec66f9":"code","26486f66":"code","c9b1a286":"code","5d1cd967":"code","60c100c1":"code","cbb3681a":"code","c36aa668":"code","70527857":"code","292a6646":"code","7e023fdf":"code","67d21555":"code","dfdc8176":"code","eab3c343":"code","f52688f3":"code","57298cd0":"code","a7916740":"code","76b13bde":"code","b93d6020":"code","3306c248":"code","c49e44ad":"code","29af3036":"code","95f0f7be":"code","02cd5444":"code","d46986cc":"code","4f53659a":"code","d90e8406":"code","96392b0b":"code","95b0528a":"code","8f07b6a6":"code","b34fbad2":"code","40f99bb5":"code","c674999d":"code","66907e0c":"code","32b45c51":"code","7174b477":"code","aee4f149":"code","a060e209":"code","755928b3":"code","2e3a4f71":"code","6af07250":"code","cbcc4920":"code","dbb7a6c9":"code","6579534a":"code","86940239":"code","ffd49419":"code","be883f16":"code","e706c71b":"code","12a37bc8":"code","b5c483cd":"code","c7b7f3d9":"code","46863585":"code","dfff7cb7":"code","28310e7e":"code","68688837":"code","421018dd":"code","f9bab78a":"code","e163cecd":"code","0c7d9695":"code","ecd9c724":"code","b9aa56b0":"code","feb7eb40":"code","811a14fe":"code","dc24ea0a":"code","19b3620f":"code","7b89756a":"code","2068b760":"code","839c1895":"code","223aa760":"markdown","ae77ccbd":"markdown","6618378c":"markdown","131b3c23":"markdown","eed54066":"markdown","d61b8d63":"markdown","20c3d12f":"markdown","85fb85ae":"markdown","eed77a58":"markdown","9dbd4c37":"markdown","25e4d862":"markdown","4491a74a":"markdown","3d1a6c88":"markdown","6a139871":"markdown","77a84255":"markdown","40d6f4c9":"markdown","d95ccec5":"markdown","6ac6fe13":"markdown","fe1fac49":"markdown","af360913":"markdown","43c9d699":"markdown","661cf5c9":"markdown","58959426":"markdown","2c5c0f61":"markdown","6c60840e":"markdown","28f40365":"markdown","6497cbbf":"markdown","ed22b9c7":"markdown"},"source":{"a752df40":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport scipy.stats as st","cc1f8e6e":"df=pd.read_csv('..\/input\/car-sales-regression\/car_sales-1.csv',index_col=0)\ndf.head(5)","8f5349ba":"df.shape","963ac1c7":"df.info()","a90603aa":"df.isnull().sum()","7b343977":"#pd.set_option('display.width', 1000)\npd.set_option('display.max_columns', 27)","40eb9cfa":"df.head()","794570fb":"cor=df.corr()","84b50c07":"plt.figure(figsize=(15,8))\nsns.heatmap(cor,annot=True,cmap='seismic')","3fecccf6":"cor_target = abs(cor[\"Price\"])\nrelevant_features = cor_target[cor_target>0.5]\nrelevant_features","bfb7eec1":"def airbags(x):\n    if x=='None':\n        x=0\n    elif x=='Driver only':\n        x=1\n    elif x=='Driver & Passenger':\n        x=2\n    return x\ndf.AirBags=df.AirBags.apply(airbags)","30f9e139":"df.head()","9afe6120":"df.DriveTrain.value_counts()","dbbd6abf":"def x(a):\n    if a=='Front':\n        a=0\n    elif a=='Rear':\n        a=1\n    elif a=='4WD':\n        a=2\n    return a\ndf.DriveTrain=df.DriveTrain.apply(x)","f84b18b5":"df.head()","17f5f4a2":"round(pd.DataFrame(df.groupby('Manufacturer')['Price'].mean()),0)","69da5dad":"df.Cylinders.value_counts()","401f035d":"df.Cylinders=df.Cylinders.replace('rotary',10)","b53d3017":"df.head()","146b3868":"df['Man.trans.avail'].value_counts()","7c4a1456":"df[['Man.trans.avail','Price']].head(20)","7b494cf8":"def a(x):\n    if x=='Yes':\n        x=0\n    elif x=='No':\n        x=1\n    return x\ndf['Man.trans.avail']=df['Man.trans.avail'].apply(a)","48319edb":"df.head()","2fe0f103":"mfg = pd.DataFrame(df.groupby('Manufacturer')['Price'].mean()) \nmfg.rename(columns ={\"Price\": \"brand_price\"}, inplace = True)\nprint(mfg.head(10))\ndf = pd.merge(mfg,df,on='Manufacturer')","7241eaca":"df.head()","6c965674":"#round(pd.DataFrame(df.groupby('Type')['Price'].mean()),0)\n#Since it is not clear from average price,we will go for ohe instead of le","70a4af39":"from sklearn.impute import SimpleImputer","ef740f11":"imp = SimpleImputer(missing_values=np.nan, strategy='mean')","c938a994":"df.isnull().sum()","a1227b26":"df['Rear.seat.room']=df.groupby('Type')['Rear.seat.room'].transform(lambda x:x.fillna(x.mean()))","33bf26b3":"df.head()","fdc9b04e":"df.groupby('Type')['Luggage.room'].mean()","cec2d036":"df.groupby('Type')['Passengers'].mean()","6b9d8966":"df['Luggage.room']=df['Luggage.room'].fillna(15)\n#Since mean of midsize is 15","86423f6d":"df.head()","2ed2c587":"df.groupby('Origin')['Price'].mean()","1cdf6537":"df['Origin']=pd.get_dummies(df['Origin'],drop_first=True)\n#converted string to num","e255d97e":"df.head()","8e132dfa":"df.drop(['Manufacturer','Model','Min.Price','Max.Price','Make'],axis=1,inplace=True)","7a2a9b3c":"df.head()","df69abf3":"df.head()","b6b4facf":"df.Cylinders.value_counts()","05f8d5df":"df['Cylinders']=pd.to_numeric(df['Cylinders'],errors='coerce')","dec0f78b":"df.Type.value_counts()","bd7a41f8":"df.Cylinders.value_counts()","177305d7":"df=pd.get_dummies(df,drop_first=True)","bed06eae":"df.head()","064daa53":"df.shape","27a2f107":"from sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\nfrom sklearn.pipeline import Pipeline\nfrom scipy.stats import randint as sp_randint\nfrom scipy.stats import uniform as sp_uniform\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.svm import SVR\nfrom sklearn.ensemble import AdaBoostRegressor\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.linear_model import SGDRegressor\nfrom tpot import TPOTRegressor\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.metrics import r2_score","e447be7c":"X = df.drop('Price', axis=1)\ny= df['Price']","b1293f5a":"X_train,X_test,y_train,y_test=train_test_split(X,y, test_size=0.3, random_state=0)","732ef579":"from sklearn.model_selection import cross_val_score","5098c666":"from sklearn.linear_model import LinearRegression\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.model_selection import KFold\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.svm import SVR","d0a1e98b":"lr=LinearRegression()\nknn=KNeighborsRegressor()\nrf=RandomForestRegressor()\nsvr=SVR()","6b61b496":"models=[]\nmodels.append(('MVLR',lr))\nmodels.append(('KNN',knn))\nmodels.append(('RF',rf))\nmodels.append(('SVR',svr))","c1daa0e1":"results=[]\nnames=[]\nfor name,model in models:\n    kfold=KFold(shuffle=True,n_splits=3,random_state=0)\n    cv_results=cross_val_score(model,X,y,cv=kfold,scoring='neg_mean_squared_error')\n    results.append(np.sqrt(np.abs(cv_results)))\n    names.append(name)\n    print(\"%s: %f (%f)\"%(name,np.mean(np.sqrt(np.abs(cv_results))),np.var(np.sqrt(np.abs(cv_results)),ddof=1)))","5e9f464b":"# cleaned data backup in df1\ndf1 = df.copy()","a8b45393":"from sklearn.preprocessing import StandardScaler\nsc=StandardScaler()\nscaled=pd.DataFrame(sc.fit_transform(df),columns=df.columns)","2129e0c9":"X = df.drop('Price', axis=1)\ny= df['Price']\nfrom sklearn.linear_model import LinearRegression\n\nlin_reg = LinearRegression()\nlin_reg.fit(X, y)\n\nprint(f'Coefficients: {lin_reg.coef_}')\nprint(f'Intercept: {lin_reg.intercept_}')\nprint(f'R^2 score: {lin_reg.score(X, y)}')","3ad06c24":"from sklearn.model_selection import train_test_split\nX_train, X_test , y_train, y_test = train_test_split(X,y, test_size = 0.30, random_state = 1)\nprint(X_train.shape)\nprint(X_test.shape)\nprint(y_test.shape)","8351f503":"lin_reg = LinearRegression()\nmodel = lin_reg.fit(X_train,y_train)\nprint(f'R^2 score for train: {lin_reg.score(X_train, y_train)}')\nprint(f'R^2 score for test: {lin_reg.score(X_test, y_test)}')","60183c3f":"import warnings \nwarnings.filterwarnings('ignore')\nimport statsmodels.api as sm\n\nX_constant = sm.add_constant(X)\nlin_reg = sm.OLS(y,X_constant).fit()\nlin_reg.summary()","7b90706c":"from scipy import stats\nprint(stats.jarque_bera(lin_reg.resid))","499d887b":"import seaborn as sns\n\nsns.distplot(lin_reg.resid)","75170b8e":"%matplotlib inline\n%config InlineBackend.figure_format ='retina'\nimport seaborn as sns \nimport matplotlib.pyplot as plt\nimport statsmodels.stats.api as sms\nsns.set_style('darkgrid')\nsns.mpl.rcParams['figure.figsize'] = (15.0, 9.0)\n\ndef linearity_test(model, y):\n    '''\n    Function for visually inspecting the assumption of linearity in a linear regression model.\n    It plots observed vs. predicted values and residuals vs. predicted values.\n    \n    Args:\n    * model - fitted OLS model from statsmodels\n    * y - observed values\n    '''\n    fitted_vals = model.predict()\n    resids = model.resid\n\n    fig, ax = plt.subplots(1,2)\n    \n    sns.regplot(x=fitted_vals, y=y, lowess=True, ax=ax[0], line_kws={'color': 'red'})\n    ax[0].set_title('Observed vs. Predicted Values', fontsize=16)\n    ax[0].set(xlabel='Predicted', ylabel='Observed')\n\n    sns.regplot(x=fitted_vals, y=resids, lowess=True, ax=ax[1], line_kws={'color': 'red'})\n    ax[1].set_title('Residuals vs. Predicted Values', fontsize=16)\n    ax[1].set(xlabel='Predicted', ylabel='Residuals')\n    \nlinearity_test(lin_reg, y)  ","27ec66f9":"import statsmodels.api as sm\nsm.stats.diagnostic.linear_rainbow(res=lin_reg, frac=0.5)","26486f66":"import scipy.stats as stats\nimport pylab\nfrom statsmodels.graphics.gofplots import ProbPlot\nst_residual = lin_reg.get_influence().resid_studentized_internal\nstats.probplot(st_residual, dist=\"norm\", plot = pylab)\nplt.show()","c9b1a286":"lin_reg.resid.mean()","5d1cd967":"from statsmodels.compat import lzip\nimport numpy as np\nfrom statsmodels.compat import lzip\n%matplotlib inline\n%config InlineBackend.figure_format ='retina'\nimport seaborn as sns \nimport matplotlib.pyplot as plt\nimport statsmodels.stats.api as sms\nsns.set_style('darkgrid')\nsns.mpl.rcParams['figure.figsize'] = (15.0, 9.0)\n\nmodel = lin_reg\nfitted_vals = model.predict()\nresids = model.resid\nresids_standardized = model.get_influence().resid_studentized_internal\nfig, ax = plt.subplots(1,2)\n\nsns.regplot(x=fitted_vals, y=resids, lowess=True, ax=ax[0], line_kws={'color': 'red'})\nax[0].set_title('Residuals vs Fitted', fontsize=16)\nax[0].set(xlabel='Fitted Values', ylabel='Residuals')\nsns.regplot(x=fitted_vals, y=np.sqrt(np.abs(resids_standardized)), lowess=True, ax=ax[1], line_kws={'color': 'red'})\nax[1].set_title('Scale-Location', fontsize=16)\nax[1].set(xlabel='Fitted Values', ylabel='sqrt(abs(Residuals))')\n\nname = ['F statistic', 'p-value']\ntest = sms.het_goldfeldquandt(model.resid, model.model.exog)\nlzip(name, test)","60c100c1":"import statsmodels.tsa.api as smt\n\nacf = smt.graphics.plot_acf(lin_reg.resid, lags=20,alpha=0.05)\nacf.show()","cbb3681a":"from statsmodels.stats.outliers_influence import variance_inflation_factor\n\nvif = [variance_inflation_factor(X_constant.values, i) for i in range(X_constant.shape[1])]\npd.DataFrame({'vif': vif[1:]}, index=X.columns).T","c36aa668":"cor=df.corr()","70527857":"sns.heatmap(cor,annot=True)","292a6646":"cor=abs(cor['Price'])","7e023fdf":"main_features=cor[cor>0.5]\nmain_features","67d21555":"df2=df.copy()\ndf2 = df2.transform(lambda x: x**0.5)\ndf2.head()","dfdc8176":"# Apply and check improvement by square root of data\nX = df2.drop('Price',axis=1)\ny = df2.Price\nX_constant = sm.add_constant(X)\nmodel = sm.OLS(y, X_constant).fit()\npredictions = model.predict(X_constant)\nmodel.summary()","eab3c343":"df3=df.copy()\ndf3=df3.transform(lambda x:x**2)\ndf3.head()","f52688f3":"X =df3.drop('Price',axis=1)\ny =df3.Price\nX_constant = sm.add_constant(X)\nmodel = sm.OLS(y, X_constant).fit()\npredictions = model.predict(X_constant)\nmodel.summary()","57298cd0":"### We can clearly see that after doing square transformation r2 value gets decreased and vice versa for square root","a7916740":"dfbox=df.copy()","76b13bde":"dfbox.skew()","b93d6020":"l=[]\nfor i in dfbox.columns:\n    if ((dfbox[i].skew()<0.1) or (dfbox[i].skew()>0.2) and (i!='Price')):\n        l.append(i)\nl","3306c248":"for i in dfbox.columns:\n    if i in l:\n        dfbox[i]=list(st.boxcox(dfbox[i]+1)[0])\ndfbox.skew()","c49e44ad":"X =dfbox.drop('Price',axis=1)\ny =dfbox.Price\nX_constant = sm.add_constant(X)\nmodel = sm.OLS(y, X_constant).fit()\npredictions = model.predict(X_constant)\nmodel.summary()","29af3036":"df4=df.copy()\ndf4=df4.transform(lambda x:x**0.4)\ndf4.head()","95f0f7be":"X =df4.drop('Price',axis=1)\ny =df4.Price\nX_constant = sm.add_constant(X)\nmodel = sm.OLS(y, X_constant).fit()\npredictions = model.predict(X_constant)\nmodel.summary()","02cd5444":"X =df2.drop('Price',axis=1)\ny =df2.Price\nX_constant = sm.add_constant(X)\nmodel = sm.OLS(y, X_constant).fit()\npredictions = model.predict(X_constant)\nmodel.summary()","d46986cc":"df5=df.copy()\ndf5=df5.transform(lambda x:x**0.01)\ndf5.head()","4f53659a":"X =df5.drop('Price',axis=1)\ny =df5.Price\nX_constant = sm.add_constant(X)\nmodel = sm.OLS(y, X_constant).fit()\npredictions = model.predict(X_constant)\nmodel.summary()","d90e8406":"import statsmodels.api as sm\n%matplotlib inline\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.feature_selection import RFE\nfrom sklearn.linear_model import RidgeCV, LassoCV, Ridge, Lasso","96392b0b":"#Adding constant column of ones, mandatory for sm.OLS model\nX_1 = sm.add_constant(X)\n#Fitting sm.OLS model\nmodel = sm.OLS(y,X_1).fit()\nmodel.pvalues","95b0528a":"#Backward Elimination\ncols = list(X.columns)\npmax = 1\nwhile (len(cols)>0):\n    p= []\n    X_1 = X[cols]\n    X_1 = sm.add_constant(X_1)\n    model = sm.OLS(y,X_1).fit()\n    p = pd.Series(model.pvalues.values[1:],index = cols)      \n    pmax = max(p)\n    feature_with_p_max = p.idxmax()\n    if(pmax>0.05):\n        cols.remove(feature_with_p_max)\n    else:\n        break\nselected_features_BE = cols\nprint(selected_features_BE)","8f07b6a6":"model = LinearRegression()","b34fbad2":"#Initializing RFE model\nrfe = RFE(model, 10)","40f99bb5":"#Transforming data using RFE\nX_rfe = rfe.fit_transform(X,y)  \n#Fitting the data to model\nmodel.fit(X_rfe,y)\nprint(rfe.support_)\nprint(rfe.ranking_)","c674999d":"X.columns","66907e0c":"#no of features\nnof_list=np.arange(1,27)            \nhigh_score=0\n#Variable to store the optimum features\nnof=0           \nscore_list =[]\nfor n in range(len(nof_list)):\n    X_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.3, random_state = 0)\n    model = LinearRegression()\n    rfe = RFE(model,nof_list[n])\n    X_train_rfe = rfe.fit_transform(X_train,y_train)\n    X_test_rfe = rfe.transform(X_test)\n    model.fit(X_train_rfe,y_train)\n    score = model.score(X_test_rfe,y_test)\n    score_list.append(score)\n    if(score>high_score):\n        high_score = score\n        nof = nof_list[n]\nprint(\"Optimum number of features: %d\" %nof)\nprint(\"Score with %d features: %f\" % (nof, high_score))","32b45c51":"cols = list(X.columns)\nmodel = LinearRegression()\n#Initializing RFE model\nrfe = RFE(model, 8)             \n#Transforming data using RFE\nX_rfe = rfe.fit_transform(X,y)  \n#Fitting the data to model\nmodel.fit(X_rfe,y)              \ntemp = pd.Series(rfe.support_,index = cols)\nselected_features_rfe = temp[temp==True].index\nprint(selected_features_rfe)","7174b477":"reg = LassoCV()\nreg.fit(X, y)\nprint(\"Best alpha using built-in LassoCV: %f\" % reg.alpha_)\nprint(\"Best score using built-in LassoCV: %f\" %reg.score(X,y))\ncoef = pd.Series(reg.coef_, index = X.columns)","aee4f149":"print(\"Lasso picked \" + str(sum(coef != 0)) + \" variables and eliminated the other \" +  str(sum(coef == 0)) + \" variables\")","a060e209":"imp_coef = coef.sort_values()\nimport matplotlib\nmatplotlib.rcParams['figure.figsize'] = (8.0, 10.0)\nimp_coef.plot(kind = \"barh\")\nplt.title(\"Feature importance using Lasso Model\")","755928b3":"dforig=df2.copy()","2e3a4f71":"## Building of simple OLS model.\nX = df2.drop('Price',1)\ny = df2.Price\nX_constant = sm.add_constant(X)\nmodel = sm.OLS(y, X_constant).fit()\npredictions = model.predict(X_constant)\nmodel.summary()","6af07250":"### calculating the vif values as multicollinearity exists (as stated by warning 2)\n\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\n\n[variance_inflation_factor(X.values, j) for j in range(1, X.shape[1])]","cbcc4920":"# removing collinear variables\n# function definition\n\ndef calculate_vif(x):\n    thresh = 5.0\n    output = pd.DataFrame()\n    k = x.shape[1]\n    vif = [variance_inflation_factor(x.values, j) for j in range(x.shape[1])]\n    for i in range(1,k):\n        print(\"Iteration no.\")\n        print(i)\n        print(vif)\n        a = np.argmax(vif)\n        print(\"Max VIF is for variable no.:\")\n        print(a)\n        if vif[a] <= thresh :\n            break\n        if i == 1 :          \n            output = x.drop(x.columns[a], axis = 1)\n            vif = [variance_inflation_factor(output.values, j) for j in range(output.shape[1])]\n        elif i > 1 :\n            output = output.drop(output.columns[a],axis = 1)\n            vif = [variance_inflation_factor(output.values, j) for j in range(output.shape[1])]\n    return(output)","dbb7a6c9":"## passing X to the function so that the multicollinearity gets removed.\ntrain_out = calculate_vif(X)","6579534a":"## includes only the relevant features\ntrain_out.head()","86940239":"lr = LinearRegression()\nlr.fit(X_train, y_train)","ffd49419":"# higher the alpha value, more restriction on the coefficients; \n# low alpha > more generalization, coefficients are barely\nrr = Ridge(alpha=0.01) \n# restricted and in this case linear and ridge regression resembles\nrr.fit(X_train, y_train)","be883f16":"rr100 = Ridge(alpha=100) #  comparison with alpha value\nrr100.fit(X_train, y_train)","e706c71b":"train_score=lr.score(X_train, y_train)\ntest_score=lr.score(X_test, y_test)","12a37bc8":"Ridge_train_score = rr.score(X_train,y_train)\nRidge_test_score = rr.score(X_test, y_test)","b5c483cd":"Ridge_train_score100 = rr100.score(X_train,y_train)\nRidge_test_score100 = rr100.score(X_test, y_test)","c7b7f3d9":"print(\"linear regression train score:\", train_score)\nprint(\"linear regression test score:\", test_score)\nprint(\"ridge regression train score low alpha:\", Ridge_train_score)\nprint(\"ridge regression test score low alpha:\", Ridge_test_score)\nprint(\"ridge regression train score high alpha:\", Ridge_train_score100)\nprint(\"ridge regression test score high alpha:\", Ridge_test_score100)","46863585":"from sklearn.linear_model import Lasso\n","dfff7cb7":"lasso = Lasso()\nlasso.fit(X_train,y_train)\ntrain_score=lasso.score(X_train,y_train)\ntest_score=lasso.score(X_test,y_test)\ncoeff_used = np.sum(lasso.coef_!=0)","28310e7e":"print(\"training score:\"), train_score \nprint(\"test score: \"), test_score\nprint(\"number of features used: \"), coeff_used","68688837":"lasso001 = Lasso(alpha=0.01, max_iter=10e5)\nlasso001.fit(X_train,y_train)","421018dd":"train_score001=lasso001.score(X_train,y_train)\ntest_score001=lasso001.score(X_test,y_test)\ncoeff_used001 = np.sum(lasso001.coef_!=0)","f9bab78a":"print(\"training score for alpha=0.01:\"), train_score001 \nprint(\"test score for alpha =0.01: \"), test_score001\nprint(\"number of features used: for alpha =0.01:\"), coeff_used001","e163cecd":"lasso00001 = Lasso(alpha=0.0001, max_iter=10e5)\nlasso00001.fit(X_train,y_train)","0c7d9695":"train_score00001=lasso00001.score(X_train,y_train)\ntest_score00001=lasso00001.score(X_test,y_test)\ncoeff_used00001 = np.sum(lasso00001.coef_!=0)","ecd9c724":"print(\"training score for alpha=0.0001:\"), train_score00001 \nprint(\"test score for alpha =0.0001: \"), test_score00001\nprint(\"number of features used: for alpha =0.0001:\"), coeff_used00001","b9aa56b0":"lr = LinearRegression()\nlr.fit(X_train,y_train)\nlr_train_score=lr.score(X_train,y_train)\nlr_test_score=lr.score(X_test,y_test)","feb7eb40":"print(\"LR training score:\"), lr_train_score \nprint(\"LR test score: \"), lr_test_score","811a14fe":"# Let's perform a cross-validation to find the best combination of alpha and l1_ratio\nfrom sklearn.linear_model import ElasticNetCV, ElasticNet\nfrom sklearn.metrics import r2_score\n\ncv_model = ElasticNetCV(l1_ratio=[.1, .5, .7, .9, .95, .99, .995, 1], eps=0.001, n_alphas=100, fit_intercept=True, \n                        normalize=True, precompute='auto', max_iter=2000, tol=0.0001, cv=5, \n                        copy_X=True, verbose=0, n_jobs=-1, positive=False, random_state=None, selection='cyclic')","dc24ea0a":"cv_model.fit(X_train, y_train)","19b3620f":"print('Optimal alpha: %.8f'%cv_model.alpha_)\nprint('Optimal l1_ratio: %.3f'%cv_model.l1_ratio_)\nprint('Number of iterations %d'%cv_model.n_iter_)","7b89756a":"# train model with best parameters from CV\nmodel = ElasticNet(l1_ratio=cv_model.l1_ratio_, alpha = cv_model.alpha_, max_iter=cv_model.n_iter_, fit_intercept=True, normalize = True)\nmodel.fit(X_train, y_train)","2068b760":"print(r2_score(y_train, model.predict(X_train))) # training data performance","839c1895":"print(r2_score(y_test, model.predict(X_test))) # test data performance","223aa760":"### Standard Scaling:","ae77ccbd":"## Raw linear regression model","6618378c":"## Assumptions For linear Regression","131b3c23":"### Imputing null values","eed54066":"### Importing the necessary libraries:","d61b8d63":"### 3.Homoscedasticity","20c3d12f":"### 4. Autocorrelation","85fb85ae":"### Reading the dataset","eed77a58":"### Backward Elimination","9dbd4c37":"### 5. Multicollinearity","25e4d862":"Build a regression model in order to predict the price of the cars based on 26 features provided in the dataset","4491a74a":"### RFE","3d1a6c88":"### Lasso:","6a139871":"### Splitting Dataset into X and y:","77a84255":"### Ridge:","40d6f4c9":"### Problem Statement:","d95ccec5":"#### 1. Normality","6ac6fe13":"## Regularization","fe1fac49":"### Checking for null values:","af360913":"#### 2.Linearity","43c9d699":"### VIF","661cf5c9":"### Elastic Net","58959426":"### Variables correlation with target variable:","2c5c0f61":"### Lasso Feature selection","6c60840e":"### Transformation:","28f40365":"Since there is null values for luggage room in whole van category,so we are checking it by the number of passengers in diff type","6497cbbf":"## Feature Selection","ed22b9c7":"#### Rainbow Test"}}