{"cell_type":{"e635d269":"code","984d1d48":"code","8afa3ac3":"code","b3526e61":"code","cadab5e3":"code","9c9f523b":"code","f909ca93":"code","221689c5":"code","c1119368":"code","8f186c66":"code","3ba4ced2":"code","e327b9af":"code","3df1695d":"code","05b8b169":"code","d22ca274":"code","c61ad0fd":"code","b236119d":"code","805c76d7":"code","53f02f90":"code","dbb37161":"code","fe814fea":"code","67bf6491":"code","3a27220a":"code","3d1db6b1":"code","96095b65":"code","5bb7bce1":"code","9e75c847":"code","f2fca821":"code","9fa1082e":"code","6a5973dd":"code","438ca900":"code","b6d4a858":"code","8cd1164c":"code","cdd2dd9e":"code","fe3eb312":"code","7266983f":"code","f68542d7":"code","5bf3965e":"code","1dfc7577":"code","28c738b5":"code","46e65a02":"code","1bced88a":"code","c1591a6a":"code","ed333abd":"code","d07d7243":"code","2dbc037d":"code","31b6326f":"code","b26d7826":"code","0271092a":"code","79a758d7":"code","7087320c":"code","7a349e19":"code","922bd24b":"code","2ae4f665":"markdown","4e7e29dc":"markdown","b26f881e":"markdown","f5a50dd3":"markdown","7a174686":"markdown","69c7acd8":"markdown","f31a735b":"markdown","bd4ca122":"markdown","41f37884":"markdown","2e077f3b":"markdown","962c1257":"markdown","2b745eee":"markdown","9e8780b5":"markdown"},"source":{"e635d269":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","984d1d48":"#!pip install glob2","8afa3ac3":"import matplotlib.pyplot as plt\nimport seaborn as sns\nimport glob\nfrom sklearn.feature_selection import chi2;\nfrom scipy import stats","b3526e61":"\n\n\npath = r'\/kaggle\/input\/learnplatform-covid19-impact-on-digital-learning\/engagement_data' # use your path\nall_files = glob.glob(path + \"\/*.csv\")\n\nli = []\n\nfor filename in all_files:\n    df = pd.read_csv(filename, index_col=None, header=0)\n    district_id = filename.split('\/')[5].split('.')[0]\n    df['district_id'] = district_id\n    li.append(df)\n\ndf_eng = pd.concat(li, axis=0, ignore_index=True)","cadab5e3":"#df_eng.to_csv('df_enga.csv')\ndf_eng","9c9f523b":"df_eng.dtypes","f909ca93":"os.chdir(\"\/kaggle\/input\/learnplatform-covid19-impact-on-digital-learning\")\ndf_dist = pd.read_csv(\"districts_info.csv\")","221689c5":"df_dist","c1119368":"df_dist.dtypes","8f186c66":"df_prod = pd.read_csv(\"products_info.csv\")\ndf_prod","3ba4ced2":"df_prod.dtypes","e327b9af":"# how many missing values exist or better still what is the % of missing values in the dataset?\ndef percent_missing(df):\n\n    # Calculate total number of cells in dataframe\n    totalCells = np.product(df.shape)\n\n    # Count number of missing values per column\n    missingCount = df.isnull().sum()\n\n    # Calculate total number of missing values\n    totalMissing = missingCount.sum()\n\n    # Calculate percentage of missing values\n    print(\"The Data  contains\", round(((totalMissing\/totalCells) * 100), 2), \"%\", \"missing values.\")\n\n","3df1695d":"percent_missing(df_eng)","05b8b169":"percent_missing(df_prod)","d22ca274":"percent_missing(df_dist)","c61ad0fd":"# Function to calculate missing values by column\ndef missing_values_table(df):\n    # Total missing values\n    mis_val = df.isnull().sum()\n\n    # Percentage of missing values\n    mis_val_percent = 100 * df.isnull().sum() \/ len(df)\n\n    # dtype of missing values\n    mis_val_dtype = df.dtypes\n\n    # Make a table with the results\n    mis_val_table = pd.concat([mis_val, mis_val_percent, mis_val_dtype], axis=1)\n\n    # Rename the columns\n    mis_val_table_ren_columns = mis_val_table.rename(\n    columns = {0 : 'Missing Values', 1 : '% of Total Values', 2: 'Dtype'})\n\n    # Sort the table by percentage of missing descending\n    mis_val_table_ren_columns = mis_val_table_ren_columns[\n        mis_val_table_ren_columns.iloc[:,1] != 0].sort_values(\n    '% of Total Values', ascending=False).round(1)\n\n    # Print some summary information\n    print (\"Your selected dataframe has \" + str(df.shape[1]) + \" columns.\\n\"      \n        \"There are \" + str(mis_val_table_ren_columns.shape[0]) +\n          \" columns that have missing values.\")\n\n    # Return the dataframe with missing information\n    return mis_val_table_ren_columns\n","b236119d":"missing_values_table(df_eng)","805c76d7":"missing_values_table(df_dist)","53f02f90":"missing_values_table(df_prod)","dbb37161":"#fix the missing values\ndef fix_missing_ffill(df, col):\n    df[col] = df[col].fillna(method='ffill')\n    return df[col]\n\ndef fix_missing_bfill(df, col):\n    df[col] = df[col].fillna(method='bfill')\n    return df[col]\n","fe814fea":"clean_eng = df_eng.copy()\n\nclean_eng['engagement_index'] = clean_eng['engagement_index'].fillna(clean_eng['engagement_index'].mean())\nclean_eng['pct_access'] = clean_eng['pct_access'].fillna(clean_eng['pct_access'].mean())\nclean_eng['lp_id'] = fix_missing_ffill(clean_eng, 'lp_id')\n\n\n","67bf6491":"missing_values_table(clean_eng)","3a27220a":"clean_dist = df_dist.copy()\n#remove columns with 30% of missing values\nclean_dist = clean_dist.drop(['pp_total_raw','pct_free\/reduced','county_connections_ratio'],axis=1)\nclean_dist['state'] = fix_missing_ffill(clean_dist, 'state')\n\nclean_dist['locale'] = fix_missing_bfill(clean_dist, 'locale')\nclean_dist['pct_black\/hispanic'] = fix_missing_bfill(clean_dist, 'pct_black\/hispanic')\n\n","3d1db6b1":"missing_values_table(clean_dist)","96095b65":"clean_prod = df_prod.copy()\nclean_prod['Sector(s)'] = fix_missing_ffill(clean_prod, 'Sector(s)')\n\nclean_prod['Primary Essential Function'] = fix_missing_ffill(clean_prod, 'Primary Essential Function')\nclean_prod['Provider\/Company Name'] = fix_missing_ffill(clean_prod, 'Provider\/Company Name')","5bb7bce1":"missing_values_table(clean_prod)","9e75c847":"clean_eng['district_id'] =clean_eng['district_id'].astype(int)\n\nclean_eng.dtypes","f2fca821":"df_merge1 = pd.merge(clean_eng, clean_dist, on=\"district_id\")","9fa1082e":"df_merge1","6a5973dd":"missing_values_table(df_merge1)","438ca900":"#Change the name of the column\nclean_prod = clean_prod.rename(columns={'LP ID': 'lp_id'})\nclean_prod['lp_id'] =clean_prod['lp_id'].astype('float')\nclean_prod.head(3)","b6d4a858":"df_merge2 = pd.merge( clean_prod,df_merge1, on=\"lp_id\")\n","8cd1164c":"missing_values_table(df_merge2)","cdd2dd9e":"df_merge2","fe3eb312":"df_merge2.dtypes","7266983f":"df_merge2.describe()","f68542d7":"mode1= df_merge2.mode()\nprint(mode1)","5bf3965e":"def plot_count(df:pd.DataFrame, column:str) -> None:\n    plt.figure(figsize=(12, 7))\n    sns.countplot(data=df, x=column)\n    plt.title(f'Distribution of {column}', size=20, fontweight='bold')\n    plt.show()","1dfc7577":"plot_count(df_merge2, \"Sector(s)\")","28c738b5":"df_merge2.columns","46e65a02":"plot_count(df_merge2, \"pct_black\/hispanic\")","1bced88a":"#splitting date column into day_name,month,weekdef features_create(data):\ndef features_create(data): \n    data['year']=data['time'].dt.year\n    data['month']=data['time'].dt.month\n    data['day_name']=data['time'].dt.day_name()\n    return data\n","c1591a6a":"df_merge2['time'] = pd.to_datetime(df_merge2['time'])\n","ed333abd":"features_create(df_merge2)","d07d7243":"plot_count(df_merge2, \"day_name\")","2dbc037d":"plot_count(df_merge2, \"month\")","31b6326f":"plt.figure(figsize=(14,10))\nfor i, column in enumerate(df_merge2[['Sector(s)']]):\n    data=df_merge2[column].value_counts().sort_values(ascending=False)\n   # plt.subplot(1,2,i+1)\n    sns.barplot(x=data, y=data.index);","b26d7826":"plt.figure(figsize=(14,10))\nfor i, column in enumerate(df_merge2[['pct_black\/hispanic']]):\n    data=df_merge2[column].value_counts().sort_values(ascending=False)\n   # plt.subplot(1,2,i+1)\n    sns.barplot(x=data, y=data.index)","0271092a":"plt.figure(figsize=(14,10))\nfor i, column in enumerate(df_merge2[['state']]):\n    data=df_merge2[column].value_counts().sort_values(ascending=False)\n   # plt.subplot(1,2,i+1)\n    sns.barplot(x=data, y=data.index)","79a758d7":"plt.figure(figsize=(10,70))\nfor i, column in enumerate(df_merge2[['Provider\/Company Name']]):\n    data=df_merge2[column].value_counts().sort_values(ascending=False)\n   # plt.subplot(1,2,i+1)\n    sns.barplot(x=data, y=data.index)","7087320c":"plt.figure(figsize=(10,25))\nfor i, column in enumerate(df_merge2[['Primary Essential Function']]):\n    data=df_merge2[column].value_counts().sort_values(ascending=False)\n   # plt.subplot(1,2,i+1)\n    sns.barplot(x=data, y=data.index)","7a349e19":"plt.figure(figsize=(10,8))\nfor i, column in enumerate(df_merge2[['locale']]):\n    data=df_merge2[column].value_counts().sort_values(ascending=False)\n   # plt.subplot(1,2,i+1)\n    sns.barplot(x=data, y=data.index)","922bd24b":"plt.figure(figsize=(10,80))\nfor i, column in enumerate(df_merge2[['Product Name']]):\n    data=df_merge2[column].value_counts().sort_values(ascending=False)\n   # plt.subplot(1,2,i+1)\n    sns.barplot(x=data, y=data.index)","2ae4f665":"### Clean data","4e7e29dc":"#### Univariate analysis","b26f881e":" reference \n \n   Google docs has high engagement ","f5a50dd3":" the 3 states Connecticus,Utah,Massachussettes are the high engaged  states","7a174686":"<B> DIGITAL LEARNING <B>\n    \n<b>Problem Statement<b>\n    \nThe COVID-19 Pandemic has disrupted learning for more than 56 million students in the United States. In the Spring of 2020, most states and local governments across the U.S. closed educational institutions to stop the spread of the virus. In response, schools and teachers have attempted to reach students remotely through distance learning tools and digital platforms. Until today, concerns of the exacaberting digital divide and long-term learning loss among America\u2019s most vulnerable learners continue to grow.\n    \n<b>Business Need<b>\n    \nWhat is the state of digital learning in 2020? And how does the engagement of digital learning relate to factors such as district demographics, broadband access, and state\/national level policies and events?\n\n<b>Basic information<b>\n    \n    \n<b>Engagement data<b>\n    \nThe engagement data are aggregated at school district level, and each file in the folder engagement_data represents data from one school district. The 4-digit file name represents district_id which can be used to link to district information in district_info.csv. The lp_id can be used to link to product information in product_info.csv.\n\n<b>Name :<\/b>Description\n    \n<b>time :<\/b>date in \"YYYY-MM-DD\"\n    \n<b>lp_id:<\/b>The unique identifier of the product\n    \n<b>pct_access:<\/b>Percentage of students in the district have at least one page-load event of a given product and on a given day\n    \n<b>engagement_index:<\/b>Total page-load events per one thousand students of a given product and on a given day\n    \n    \n <b>District information data<b>\n    \nThe district file districts_info.csv includes information about the characteristics of school districts, including data from NCES (2018-19), FCC (Dec 2018), and Edunomics Lab. In this data set, we removed the identifiable information about the school districts. We also used an open source tool ARX (Prasser et al. 2020) to transform several data fields and reduce the risks of re-identification. For data generalization purposes some data points are released with a range where the actual value falls under. Additionally, there are many missing data marked as 'NaN' indicating that the data was suppressed to maximize anonymization of the dataset.\n\n<b>Name<\/b>:Description\n    \n<b>district_id:<\/b>The unique identifier of the school district\n    \n<b>state:<\/b>The state where the district resides in\n    \n<b>locale:<\/b>NCES locale classification that categorizes U.S. territory into four types of areas: City, Suburban, Town, and Rural. See Locale Boundaries User's Manual for more information.\n    \n<b>pct_black\/hispanic:<\/b>Percentage of students in the districts identified as Black or Hispanic based on 2018-19 NCES data\n    \n<b>pct_free\/reduced:<\/b>Percentage of students in the districts eligible for free or reduced-price lunch based on 2018-19 NCES data\n    \n<b>countyconnectionsratio:<\/b>ratio (residential fixed high-speed connections over 200 kbps in at least one direction\/households) based on the county level data from FCC From 477 (December 2018 version). See FCC data for more information.\n    \n<b>pptotalraw:<\/b>Per-pupil total expenditure (sum of local and federal expenditure) from Edunomics Lab's National Education Resource Database on Schools (NERD$) project. The expenditure data are school-by-school, and we use the median value to represent the expenditure of a given school district.\n    \n<b>Product information data<\/b>\n\nThe product file products_info.csv includes information about the characteristics of the top 372 products with most users in 2020. The categories listed in this file are part of LearnPlatform's product taxonomy. Data were labeled by our team. Some products may not have labels due to being duplicate, lack of accurate url or other reasons.\n\n<b>Name:<\/b>Description\n\n<b>LP ID:<\/b>The unique identifier of the product\n\n<b>URL:<\/b>Web Link to the specific product\n\n<b>Product Name:<\/b>Name of the specific product\n\n<b>Provider\/Company Name:<\/b>Name of the product provider\n\n<b>Sector(s):<\/b>Sector of education where the product is used\n\n<b>Primary Essential Function:<\/b>The basic function of the product. There are two layers of labels here. \nProducts are first labeled as one of these three categories: LC = Learning & Curriculum, CM = Classroom Management, and SDO = School & District Operations. Each of these categories have multiple sub-categories with which the products were labeled\n    \n <b>Objectives<\/b>\n \nUncover trends in digital learning\n\nVisualize the trends of digital connectivity and engagement in 2020\n\nUnderstand and measure the scope and impact of the pandemic on digital learning\n\nHow does student engagement with different types of education technology change over the course of the pandemic?\n\nHow does student engagement with online learning platforms relate to different geography? Demographic context (e.g., race\/ethnicity, ESL, learning disability)? Learning context? Socioeconomic status?\nDo certain state interventions, practices or policies (e.g., stimulus, reopening, eviction moratorium) correlate with the increase or decrease online engagement?\n    ","69c7acd8":"## Data exploration","f31a735b":"## Data prepocessing","bd4ca122":"### Merge data","41f37884":" Google LLC was the the provider company  that offered services significantly","2e077f3b":" reference \n \n  from the graph above we see  LC DIgital learning platform was most engaged in this case ","962c1257":"The most important thing is this part is to know how to deal with missin data. Here we decide to  remove columns from the dataset  which contains more than 30% of missing values and fix those which is less than 30% by using methode such  **forward fill**,  **backward fill**, **fill by the mode, mean etc....**\n","2b745eee":"<b>Setup<b>\n    \n<b>Load libraries<\/b>","9e8780b5":"<b>Now which column(s) has missing values<\/b>\n"}}