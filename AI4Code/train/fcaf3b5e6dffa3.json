{"cell_type":{"6e024fe0":"code","8e75c337":"code","c93e3f4f":"code","efa54b12":"code","337dbb3b":"code","92fe0a9f":"code","dbf14b08":"code","ccd543f9":"code","929849a1":"code","4b627596":"code","0a15aaf3":"code","4eb66577":"code","4e5c51b8":"code","d68c7713":"code","b6b687e2":"code","c56229f7":"code","e9ff199b":"code","36b7b3de":"code","1b323ac1":"code","1154e820":"code","f476a0e5":"code","68e36db5":"code","33ab5ef6":"code","7ed37e65":"code","659f04df":"code","3dfa4959":"code","c282e0a6":"code","2c177c24":"code","7afdfaad":"code","4a04e3a1":"code","3522f4a2":"code","50d13b6d":"code","876c8e9b":"code","9d912a6d":"code","a97331b0":"code","f9cced80":"code","d756bd6d":"code","267d0df3":"code","e2338c2c":"code","74e93083":"code","badde077":"code","2bea98ae":"code","5d5b027b":"code","3d97e41f":"code","74051463":"code","85333bb6":"code","ee2e79f3":"code","a968110f":"code","1c5981f0":"code","9b7b8a01":"markdown","8a4ec9cd":"markdown","cdde9ca2":"markdown","0fbc2646":"markdown","3387e65d":"markdown","f9e9185e":"markdown","6acdb273":"markdown","158f16a0":"markdown","8c6042aa":"markdown","94677a12":"markdown","3a202558":"markdown","4106c13a":"markdown","62a1d723":"markdown","12123ccb":"markdown","97daccb9":"markdown","cf71fd86":"markdown","59858d90":"markdown","aeb8415f":"markdown","616489c6":"markdown","3ee09a1e":"markdown","34b771d0":"markdown","f677ee59":"markdown","68f7bb8d":"markdown","49bf2990":"markdown","ad55c514":"markdown","8206ce24":"markdown","2dc82280":"markdown","f9bb71e3":"markdown","1d519e7b":"markdown","659bd614":"markdown","a93ec96c":"markdown","5d803b57":"markdown","6178323b":"markdown","0e81335e":"markdown","6faf462a":"markdown","4ef6b9cf":"markdown","d2101273":"markdown","75e4f0cc":"markdown","a5ec571d":"markdown","a8f5c936":"markdown","405634b7":"markdown","a53a6cfb":"markdown","575602ad":"markdown","338fb7f1":"markdown","12a4d217":"markdown","e1d02653":"markdown","7d1d16c4":"markdown","49508f5a":"markdown","e08329fa":"markdown","f0f24b04":"markdown","855e2305":"markdown","e5e0def3":"markdown"},"source":{"6e024fe0":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\nX_train = pd.read_csv('..\/input\/volcanoes_train\/train_images.csv')\ny_train = pd.read_csv('..\/input\/volcanoes_train\/train_labels.csv')\nX_test = pd.read_csv('..\/input\/volcanoes_test\/test_images.csv')\ny_test = pd.read_csv('..\/input\/volcanoes_test\/test_labels.csv')","8e75c337":"X_train.head()","c93e3f4f":"y_train.head()","efa54b12":"print(\"X_train ->\",X_train.shape,\"\\ny_train ->\",y_train.shape,\"\\nX_test ->\",X_test.shape,\"\\ny_test ->\",y_test.shape)","337dbb3b":"def XFix(X):\n    X.loc[-1] = X.columns.values.astype(float).astype(int)  # adding column names as a new row\n    X.index = X.index + 1  # shifting index\n    X.sort_index(inplace=True)  # sorting the index\n\nXFix(X_train)\nXFix(X_test)","92fe0a9f":"print(\"X_train ->\",X_train.shape,\"\\ny_train ->\",y_train.shape,\"\\nX_test ->\",X_test.shape,\"\\ny_test ->\",y_test.shape)","dbf14b08":"import plotly\nfrom plotly.offline import iplot\nplotly.offline.init_notebook_mode(connected=True)\nimport plotly.graph_objs as go\n\ny_trainVolcanoCount = y_train[y_train[\"Volcano?\"] == 1].shape[0]\ny_testVolcanoCount = y_test[y_test[\"Volcano?\"] == 1].shape[0]\n\ny_trainNotVolcanoCount = y_train[y_train[\"Volcano?\"] == 0].shape[0]\ny_testNotVolcanoCount = y_test[y_test[\"Volcano?\"] == 0].shape[0]\n\nx = ['Train', 'Test']\ny = [y_trainVolcanoCount, y_testVolcanoCount]\ny2 = [y_trainNotVolcanoCount, y_testNotVolcanoCount]\n\ntrace1 = go.Bar(\n    x=x,\n    y=y,\n    textposition = 'auto',\n    name = 'Volcano',\n    marker=dict(\n        color='rgb(255,120,0)',\n        line=dict(\n            color='rgb(8,48,107)',\n            width=1.5),\n        ),\n    opacity=0.6\n)\n\ntrace2 = go.Bar(\n    x=x,\n    y=y2,\n    textposition = 'auto',\n    name = 'Not Volcano',\n    marker=dict(\n        color='rgb(20,20,20)',\n        line=dict(\n            color='rgb(8,48,107)',\n            width=1.5),\n        ),\n    opacity=0.6\n)\n\ndata = [trace1,trace2]\n\nlayout = go.Layout(\n    title='Train and Test Data Volcanoes Distribution',\n)\n\nfig = go.Figure(data=data, layout=layout)\n\niplot(fig)","ccd543f9":"y_trainTypeDefinitelyCount = y_train[y_train[\"Type\"] == 1].shape[0]\ny_testTypeDefinitelyCount = y_test[y_test[\"Type\"] == 1].shape[0]\n\ny_trainTypeProbablyCount = y_train[y_train[\"Type\"] == 2].shape[0]\ny_testTypeProbablyCount = y_test[y_test[\"Type\"] == 2].shape[0]\n\ny_trainTypePossiblyCount = y_train[y_train[\"Type\"] == 3].shape[0]\ny_testTypePossiblyCount = y_test[y_test[\"Type\"] == 3].shape[0]\n\ny_trainTypePitCount = y_train[y_train[\"Type\"] == 4].shape[0]\ny_testTypePitCount = y_test[y_test[\"Type\"] == 4].shape[0]\n\nx = ['Train', 'Test']\ny = [y_trainTypeDefinitelyCount, y_testTypeDefinitelyCount]\ny2 = [y_trainTypeProbablyCount, y_testTypeProbablyCount]\ny3 = [y_trainTypePossiblyCount, y_testTypePossiblyCount]\ny4 = [y_trainTypePitCount, y_testTypePitCount]\n\ntrace1 = go.Bar(\n    x=x,\n    y=y,\n    textposition = 'auto',\n    name = 'Definitely Volcano',\n    marker=dict(\n        color='rgb(255,0,0)',\n        line=dict(\n            color='rgb(0,0,0)',\n            width=1.5),\n        ),\n    opacity=0.9\n)\n\ntrace2 = go.Bar(\n    x=x,\n    y=y2,\n    textposition = 'auto',\n    name = 'Probably Volcano',\n    marker=dict(\n        color='rgb(255,100,0)',\n        line=dict(\n            color='rgb(100,100,100)',\n            width=1.5),\n        ),\n    opacity=0.7\n)\n\ntrace3 = go.Bar(\n    x=x,\n    y=y3,\n    textposition = 'auto',\n    name = 'Possibly Volcano',\n    marker=dict(\n        color='rgb(255,240,0)',\n        line=dict(\n            color='rgb(200,200,200)',\n            width=1.5),\n        ),\n    opacity=0.5\n)\n\ntrace4 = go.Bar(\n    x=x,\n    y=y4,\n    textposition = 'auto',\n    name = 'Only Pit is Visible',\n    marker=dict(\n        color='rgb(200,200,200)',\n        line=dict(\n            color='rgb(255,255,255)',\n            width=1.5),\n        ),\n    opacity=0.4\n)\n\ndata = [trace1,trace2,trace3,trace4]\n\nlayout = go.Layout(\n    title='Train and Test Data Volcanoes Type Distribution',\n)\n\nfig = go.Figure(data=data, layout=layout)\n\niplot(fig)","929849a1":"y_trainNumberVolcanoesOnelyCount = y_train[y_train[\"Number Volcanoes\"] == 1].shape[0]\ny_testNumberVolcanoesOneCount = y_test[y_test[\"Number Volcanoes\"] == 1].shape[0]\n\ny_trainNumberVolcanoesTwoCount = y_train[y_train[\"Number Volcanoes\"] == 2].shape[0]\ny_testNumberVolcanoesTwoCount = y_test[y_test[\"Number Volcanoes\"] == 2].shape[0]\n\ny_trainNumberVolcanoesThreeCount = y_train[y_train[\"Number Volcanoes\"] == 3].shape[0]\ny_testNumberVolcanoesThreeCount = y_test[y_test[\"Number Volcanoes\"] == 3].shape[0]\n\ny_trainNumberVolcanoesFourCount = y_train[y_train[\"Number Volcanoes\"] >= 4].shape[0]\ny_testNumberVolcanoesFourCount = y_test[y_test[\"Number Volcanoes\"] >= 4].shape[0]\n\nx = ['Train', 'Test']\ny = [y_trainNumberVolcanoesOnelyCount, y_testNumberVolcanoesOneCount]\ny2 = [y_trainNumberVolcanoesTwoCount, y_testNumberVolcanoesTwoCount]\ny3 = [y_trainNumberVolcanoesThreeCount, y_testNumberVolcanoesThreeCount]\ny4 = [y_trainNumberVolcanoesFourCount, y_testNumberVolcanoesFourCount]\n\ntrace1 = go.Bar(\n    x=x,\n    y=y,\n    textposition = 'auto',\n    name = 'One Volcano',\n    marker=dict(\n        color='rgb(212,123,231)',\n        line=dict(\n            color='rgb(0,0,0)',\n            width=1.5),\n        ),\n    opacity=0.9\n)\n\ntrace2 = go.Bar(\n    x=x,\n    y=y2,\n    textposition = 'auto',\n    name = 'Two Volcano',\n    marker=dict(\n        color='rgb(13,22,11)',\n        line=dict(\n            color='rgb(100,100,100)',\n            width=1.5),\n        ),\n    opacity=0.7\n)\n\ntrace3 = go.Bar(\n    x=x,\n    y=y3,\n    textposition = 'auto',\n    name = 'Three Volcano',\n    marker=dict(\n        color='rgb(0,240,0)',\n        line=dict(\n            color='rgb(200,200,200)',\n            width=1.5),\n        ),\n    opacity=0.5\n)\n\ntrace4 = go.Bar(\n    x=x,\n    y=y4,\n    textposition = 'auto',\n    name = 'Four or More Volcano',\n    marker=dict(\n        color='rgb(1,1,244)',\n        line=dict(\n            color='rgb(255,255,255)',\n            width=1.5),\n        ),\n    opacity=0.4\n)\n\ndata = [trace1,trace2,trace3,trace4]\n\nlayout = go.Layout(\n    title='Train and Test Data Volcanoes Type Distribution',\n)\n\nfig = go.Figure(data=data, layout=layout)\n\niplot(fig)","4b627596":"y_train_copy = y_train.copy(deep=True)\nX_train_copy = X_train.copy(deep=True)","0a15aaf3":"X_train_copy.shape","4eb66577":"X_train_copy = np.resize(X_train_copy, (7000, 110, 110))","4e5c51b8":"y_train_copy.loc[y_train_copy[\"Type\"] == 1, 'Type'] = \"Definitely Volcano\"\ny_train_copy.loc[y_train_copy[\"Type\"] == 2, 'Type'] = \"Probably Volcano\"\ny_train_copy.loc[y_train_copy[\"Type\"] == 3, 'Type'] = \"Possibly Volcano\"\ny_train_copy.loc[y_train_copy[\"Type\"] == 4, 'Type'] = \"Only Pit is Visible\"\n\ny_train_copy.loc[y_train_copy[\"Volcano?\"] == 0, 'Volcano?'] = \"Not Volcano\"\ny_train_copy.loc[y_train_copy[\"Volcano?\"] == 1, 'Volcano?'] = \"Volcano\"\n\ny_train_copy.loc[y_train_copy[\"Number Volcanoes\"] == 1, 'Number Volcanoes'] = \"One Volcano\"\ny_train_copy.loc[y_train_copy[\"Number Volcanoes\"] == 2, 'Number Volcanoes'] = \"Two Volcano\"\ny_train_copy.loc[y_train_copy[\"Number Volcanoes\"] == 3, 'Number Volcanoes'] = \"Three or More Volcano\"\ny_train_copy.loc[y_train_copy[\"Number Volcanoes\"] == 4, 'Number Volcanoes'] = \"Three or More Volcano\"\ny_train_copy.loc[y_train_copy[\"Number Volcanoes\"] == 5, 'Number Volcanoes'] = \"Three or More Volcano\"","d68c7713":"from matplotlib import pyplot as plt\n\ndef draw_images(feature):\n    featureTypes = (y_train_copy.groupby([feature])[feature].nunique()).index\n    f, ax = plt.subplots(len(featureTypes)*2, 5, figsize=(12,6*len(featureTypes)))\n    for i in range(len(featureTypes)*2):\n        typ = featureTypes[int(i\/2)]\n        sample = y_train_copy[y_train_copy[feature]==typ].sample(5)\n        for j in range(5):\n            imageIndex = sample.iloc[j].name\n            image = X_train_copy[imageIndex]\n            ax[i, j].imshow(image, resample=True)\n            ax[i, j].set_title(typ, fontsize=13)\n    plt.tight_layout()\n    plt.show()","b6b687e2":"draw_images(\"Volcano?\")","c56229f7":"draw_images(\"Type\")","e9ff199b":"draw_images(\"Number Volcanoes\")","36b7b3de":"from sklearn.model_selection import train_test_split\nX_test, X_val, y_test, y_val = train_test_split(X_test, y_test, test_size=0.5, random_state=22)","1b323ac1":"print(\"After split train data for valitadion data.\")\nprint(\"%70   X_train ->\",X_train.shape,\"\\n      y_train ->\",y_train.shape,\"\\n\\n%15   X_test ->\",X_test.shape,\"\\n      y_test ->\",y_test.shape,\"\\n\\n%15   X_val ->\",X_val.shape,\"\\n      y_val ->\",y_val.shape)","1154e820":"print(\"Before resize\")\nprint(\"%70   X_train ->\",X_train.shape,\"\\n      y_train ->\",y_train.shape,\"\\n\\n%15   X_test ->\",X_test.shape,\"\\n      y_test ->\",y_test.shape,\"\\n\\n%15   X_val ->\",X_val.shape,\"\\n      y_val ->\",y_val.shape)\n\nX_test = np.resize(X_test, (1367, 110, 110, 1))\nX_train = np.resize(X_train, (7000, 110, 110, 1))\nX_val = np.resize(X_val, (1367, 110, 110, 1))\n\nprint(\"\\nAfter resize\")\nprint(\"%70   X_train ->\",X_train.shape,\"\\n      y_train ->\",y_train.shape,\"\\n\\n%15   X_test ->\",X_test.shape,\"\\n      y_test ->\",y_test.shape,\"\\n\\n%15   X_val ->\",X_val.shape,\"\\n      y_val ->\",y_val.shape)","f476a0e5":"y_train = y_train[\"Volcano?\"]\ny_test = y_test[\"Volcano?\"]\ny_val = y_val[\"Volcano?\"]","68e36db5":"X_train = X_train \/ 255.0\nX_test = X_test \/ 255.0\nX_val = X_val \/ 255.0","33ab5ef6":"from keras.utils.np_utils import to_categorical # convert to one-hot-encoding\ny_train = to_categorical(y_train, num_classes = 2)\ny_test = to_categorical(y_test, num_classes = 2)\ny_val = to_categorical(y_val, num_classes = 2)","7ed37e65":"from keras import optimizers\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Conv2D, Flatten, MaxPool2D, Dropout\nfrom keras.optimizers import Adam\n\nmodelA = Sequential()\nmodelA.add(Conv2D(filters = 8, kernel_size = (5,5), activation ='relu', input_shape = (110,110,1)))\nmodelA.add(MaxPool2D(pool_size=(2,2)))\nmodelA.add(Conv2D(filters = 16, kernel_size = (3,3), activation ='relu'))\nmodelA.add(MaxPool2D(pool_size=(2,2)))\nmodelA.add(Flatten())\nmodelA.add(Dense(y_train.shape[1], activation = \"sigmoid\"))","659f04df":"modelA.summary()","3dfa4959":"modelA.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])","c282e0a6":"modelA_trained = modelA.fit(X_train, y_train, batch_size=64, epochs = 5, validation_data = (X_val,y_val))","2c177c24":"def trainingResultsGraph(model, modelCode):\n    trace0 = go.Scatter(\n        x = model.epoch,\n        y = model.history['loss'],\n        mode = 'lines',\n        name = 'loss',\n        line=dict(color='aquamarine')\n    )\n\n    trace1 = go.Scatter(\n        x = model.epoch,\n        y = model.history['val_loss'],\n        mode = 'lines',\n        name = 'val_loss',\n        line=dict(color='darkred', dash='dash')\n    )\n\n    trace2 = go.Scatter(\n        x = model.epoch,\n        y = model.history['acc'],\n        mode = 'lines',\n        name = 'acc',\n        line=dict(color='violet')\n    )\n\n    trace3 = go.Scatter(\n        x = model.epoch,\n        y = model.history['val_acc'],\n        mode = 'lines',\n        name = 'val_acc',\n        line=dict(color='aqua', dash='dash')\n    )\n\n    updatemenus = list([\n        dict(type=\"buttons\",\n             active=-1,\n             buttons=list([\n                dict(label = 'Acc Graph',\n                     method = 'update',\n                     args = [{'visible': [False, False, True, True]},\n                             {'title': 'Trained Model'+modelCode+' training and validation accuracy'}]),\n                dict(label = 'Loss Graph',\n                     method = 'update',\n                     args = [{'visible': [True, True, False, False]},\n                             {'title': 'Trained Model'+modelCode+' training and validation loss'}]),\n                dict(label = 'Both',\n                     method = 'update',\n                     args = [{'visible': [True, True, True, True]},\n                             {'title': 'Trained Model'+modelCode+' training and validation values'}])\n            ]),\n        )\n    ])\n\n    data = [trace0, trace1, trace2, trace3]\n    layout = dict(title='Trained Model'+modelCode+' training and validation values',\n                  xaxis = dict(title = 'Epochs'),\n                  updatemenus=updatemenus)\n\n    fig = dict(data=data, layout=layout)\n\n    iplot(fig, filename='lossGraph')\n    \ntrainingResultsGraph(modelA_trained, 'A')","7afdfaad":"scoreA = modelA.evaluate(X_test, y_test)\nprint('Test Loss ->', scoreA[0])\nprint('Test Accuracy ->', scoreA[1])","4a04e3a1":"# We need to flatten(resize 1D array) back our images then sum every 10 pixels.\n# If sum equals to 0 there is corruption in the image.\n# There is an important problem here If the image started to corrupt from the upper side my perfect :\/ \n# method will not work (Please comment if you have better)\n# because every 110th pixel will be black in this situation. \n# So I use range's third parameter and jump every 119 pixel in loop\n\n#range([start], stop[, step])\n    # *start: Starting number of the sequence.\n    # *stop: Generate numbers up to, but not including this number.\n    # *step: Difference between each number in the sequence.\n\ndef corruptedImages(data):\n    corruptedImagesIndex = []\n    for index, image in enumerate(np.resize(data, (data.shape[0], 12100))): # resize (7000, 110, 110, 1) to (7000,12100)\n        sum = 0;\n        for pixelIndex in range(0,len(image)):\n            sum += image[pixelIndex]\n            if pixelIndex == 10:\n                break\n        if sum == 0:\n            corruptedImagesIndex.append(index)\n        else:\n            sum = 0\n\n    for index, image in enumerate(np.resize(data, (data.shape[0], 12100))): # resize (7000, 110, 110, 1) to (7000,12100)\n        sum = 0;\n        for pixelIndex in range(0,len(image),110):\n            sum += image[pixelIndex]\n            if pixelIndex == 10:\n                break\n        if sum == 0 and index not in corruptedImagesIndex:\n            corruptedImagesIndex.append(index)\n        else:\n            sum = 0\n    return corruptedImagesIndex","3522f4a2":"print(\"There are \"+str(len(corruptedImages(X_train_copy)))+\" corrupted images in train set.\")","50d13b6d":"trainCorruptedList = corruptedImages(X_train_copy)\n\nimport random\nrandom.shuffle(trainCorruptedList)\n\nf, axarr = plt.subplots(5,5,figsize=(15,15))\nfor i in range(5):\n    for j in range(5):\n        axarr[i,j].imshow(X_train_copy[trainCorruptedList[i*5+j]])","876c8e9b":"X_train = pd.read_csv('..\/input\/volcanoes_train\/train_images.csv')\ny_train = pd.read_csv('..\/input\/volcanoes_train\/train_labels.csv')\nX_test = pd.read_csv('..\/input\/volcanoes_test\/test_images.csv')\ny_test = pd.read_csv('..\/input\/volcanoes_test\/test_labels.csv')\n\n# Fix the column names\nXFix(X_train)\nXFix(X_test)","9d912a6d":"def deleteCorrupted(X, y, corruptedIndexList):\n    for i in corruptedIndexList:\n        X.drop(i, inplace=True) # Drop the corrupted\n        y.drop(i, inplace=True) \n    \n    X.reset_index(inplace=True) # Reconstruct the index\n    X.drop(['index'], axis=1, inplace=True) # Delete old index column\n    #Because they are pandas.series we don't need to deal with index\n    #Because we use inplace=True we don't need to return something\n    \ntrainCorruptedList = corruptedImages(X_train)\ntestCorruptedList = corruptedImages(X_test)\n\ndeleteCorrupted(X_train, y_train, trainCorruptedList)\ndeleteCorrupted(X_test, y_test, testCorruptedList)","a97331b0":"print(\"X_train ->\",X_train.shape,\"\\ny_train ->\",y_train.shape,\"\\nX_test ->\",X_test.shape,\"\\ny_test ->\",y_test.shape)","f9cced80":"# Get our label\ny_train = y_train[\"Volcano?\"]\ny_test = y_test[\"Volcano?\"]\n\n# Normalize\nX_train = X_train \/ 255.0\nX_test = X_test \/ 255.0\n\n# One Hot Label encoding\ny_train = to_categorical(y_train, num_classes = 2)\ny_test = to_categorical(y_test, num_classes = 2)\n\n# Spliting\nX_test, X_val, y_test, y_val = train_test_split(X_test, y_test, test_size=0.5, random_state=22)\n\n# Lastly resizing\nX_test = np.resize(X_test, (X_test.shape[0], 110, 110, 1))\nX_train = np.resize(X_train, (X_train.shape[0], 110, 110, 1))\nX_val = np.resize(X_val, (X_val.shape[0], 110, 110, 1))","d756bd6d":"print(\"%70   X_train ->\",X_train.shape,\"\\n      y_train ->\",y_train.shape,\"\\n\\n%15   X_test ->\",X_test.shape,\"\\n      y_test ->\",y_test.shape,\"\\n\\n%15   X_val ->\",X_val.shape,\"\\n      y_val ->\",y_val.shape)","267d0df3":"modelB = Sequential()\nmodelB.add(Conv2D(filters = 8, kernel_size = (5,5),padding = 'same',activation ='relu', input_shape = (110,110,1)))\nmodelB.add(MaxPool2D(pool_size=(2,2)))\nmodelB.add(Dropout(0.5))\nmodelB.add(Conv2D(filters = 16, kernel_size = (3,3),padding = 'same',activation ='relu'))\nmodelB.add(MaxPool2D(pool_size=(2,2)))\nmodelB.add(Dropout(0.5))\nmodelB.add(Flatten())\nmodelB.add(Dense(y_train.shape[1], activation = \"sigmoid\"))","e2338c2c":"modelB.summary()","74e93083":"modelB.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\nmodelB_trained = modelB.fit(X_train, y_train, batch_size=64, epochs = 10, validation_data = (X_val,y_val))","badde077":"trainingResultsGraph(modelB_trained, 'B')","2bea98ae":"scoreB = modelB.evaluate(X_test, y_test)\nprint('Test Loss ->', scoreB[0])\nprint('Test Accuracy ->', scoreB[1])","5d5b027b":"from keras.preprocessing.image import ImageDataGenerator\n\ndataGenerator = ImageDataGenerator(\n        featurewise_center=False,\n        samplewise_center=False,\n        featurewise_std_normalization=False,\n        samplewise_std_normalization=False,\n        zca_whitening=False,\n        rotation_range=90,\n        zoom_range = 0.1, \n        width_shift_range=0.1,\n        height_shift_range=0.1, \n        horizontal_flip=False,\n        vertical_flip=False)\ndataGenerator.fit(X_train)","3d97e41f":"modelC = Sequential()\nmodelC.add(Conv2D(filters = 2, kernel_size = (3,3),padding = 'same',activation ='relu', input_shape = (110,110,1)))\nmodelC.add(Conv2D(filters = 4, kernel_size = (3,3),padding = 'same',activation ='relu'))\nmodelC.add(Conv2D(filters = 8, kernel_size = (5,5),padding = 'same',activation ='relu'))\nmodelC.add(MaxPool2D(pool_size=(2,2)))\nmodelC.add(Dropout(0.5))\nmodelC.add(Conv2D(filters = 16, kernel_size = (5,5),padding = 'same',activation ='relu'))\nmodelC.add(MaxPool2D(pool_size=(2,2)))\nmodelC.add(Conv2D(filters = 24, kernel_size = (7,7),padding = 'same',activation ='relu'))\nmodelC.add(Dropout(0.5))\nmodelC.add(Flatten())\nmodelC.add(Dense(y_train.shape[1], activation = \"sigmoid\"))","74051463":"modelC.summary()","85333bb6":"modelC.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\nmodelC_trained = modelC.fit_generator(dataGenerator.flow(X_train, y_train, batch_size=64), epochs=50, validation_data=[X_val, y_val], steps_per_epoch=X_train.shape[0] \/\/ 64)","ee2e79f3":"trainingResultsGraph(modelC_trained, 'C')","a968110f":"scoreC = modelC.evaluate(X_test, y_test)\nprint('Test Loss ->', scoreC[0])\nprint('Test Accuracy ->', scoreC[1])","1c5981f0":"x = ['Model A', 'Model B', 'Model C']\ny = [scoreA[0], scoreB[0], scoreC[0]]\ny2 = [scoreA[1], scoreB[1], scoreC[1]]\n\ntrace1 = go.Bar(\n    x=x,\n    y=y,\n    textposition = 'auto',\n    name = 'Test Loss',\n    marker=dict(\n        color='rgb(212,123,231)',\n        line=dict(\n            color='rgb(155,123,244)',\n            width=1.5),\n        ),\n    opacity=0.7\n)\n\ntrace2 = go.Bar(\n    x=x,\n    y=y2,\n    textposition = 'auto',\n    name = 'Test Acc',\n    marker=dict(\n        color='rgb(13,22,11)',\n        line=dict(\n            color='rgb(200,100,240)',\n            width=1.5),\n        ),\n    opacity=0.7\n)\n\ndata = [trace1,trace2]\n\nlayout = go.Layout(\n    title='Models Performance Graph',\n)\n\nfig = go.Figure(data=data, layout=layout)\n\niplot(fig)","9b7b8a01":"### b) More Layers and New Hyperparams [^](#0) <a id=\"14\"><\/a> <br>\n\nFinally, we can start to build the last model. In this model, we will change some hyperparameters and add some new layers.","8a4ec9cd":"Okay, let's compile our model and train with image data generator.","cdde9ca2":"### b) Volcanoes With Graphs [^](#0) <a id=\"2\"><\/a> <br>\nIn this part, we can try to analyze our data's features with our precious graph libraries :)\n\nWe can check out the distribution of data with some colored graphs.  I will use plotly for it.","0fbc2646":"In this model we will use:\n\n* Convolutional layer, with 2 filters of dimmension 3,3\n* Convolutional layer, with 3 filters of dimmension 3,3\n* Convolutional layer, with 8 filters of dimmension 5,5\n* Maxpoll layer, with pool size 2,2\n* Convolutional layer, with 16 filters of dimmension 5,5\n* Dropout layer, wtih 0.5 drop ratio\n* Maxpoll layer, with pool size 2,2\n* Convolutional layer, with 24 filters of dimmension 7,7\n* Dropout layer, wtih 0.5 drop ratio\n* Flatten layer to transform images 2D to 1D\n* Dense layer with sigmoid activation\n\nthis layers.","3387e65d":"Test loss is not bad  it's even better than I expected. But I think we can improve it.","f9e9185e":"Let's create a new model with some new things like ImageDataGenerator. It's provided by Keras and it does some image processing like shifting, rotation and whitening so it increases our data amount like magic.\n\nTo add this tool first, we need to import its libraries then we need to set its params. For more info [click!](https:\/\/machinelearningmastery.com\/image-augmentation-deep-learning-keras\/)","6acdb273":"Most of the images just include one volcano.\n\nWe have this information so far:\n* Only %15 of images belongs to volcanoes.\n* Even that %15 is labeled as volcanoes just %10 of them is definitely volcanoes.\n* Most of the images just contain one volcano.\n\nFrom now on we can check that the images.","158f16a0":"## 2-Print Our Images [^](#0) <a id=\"4\"><\/a> <br>\n\nWe work with graphs to understand more efficiently our data but we still don't know how Magellan's pictures look. To do this:\nFirstly, I will deep copy the data to protect from changes.","8c6042aa":"Lastly we transform the data to make it ready for keras ","94677a12":"We have some labels in here. \nAccording to the dataset information first column tell us that the image has contain valcanoes or not.\n\nType column has 4 different value these are:\n*Type: 1= definitely a volcano,2 =probably, 3= possibly, 4= only a pit is visible*\n\nRadius: is the radius of the volcan in the center of the image, in pixels.\n\nNumber Volcanoes: The number of volcanoes in the image.**","3a202558":"Okay we can succesfully found all of them. Now we will load and fix our data again then delete corrupted images.","4106c13a":"## 1-Load and Exploring the Data [^](#0) <a id=\"1\"><\/a> <br>\nWe have a lot of images which taken by Magellan spacecraft from Venus. Some of them belong to Venus volcanoes and our main task is the classification of them.\n\nOkay let's start with load the data.","62a1d723":"<img src=\"https:\/\/upload.wikimedia.org\/wikipedia\/commons\/a\/a4\/Eruption_1954_Kilauea_Volcano.jpg\" width=\"500\" height=\"500\" \/>\n\n# **Finding Volcanoes with CNN** <a id=\"0\"><\/a> <br>\n\n### Hello in this kernel we will try to classify the Magellan's Venus images and find the volcanoes with CNN.\n#### **Content:**\n* 1-[Load and Exploring the Data](#1)\n    * a-[Fixing the Data](#2)\n    * b-[Volcanoes With Graphs](#3)\n* 2-[Print the Images](#4)\n* 3-[The First Step to the CNN](#5)\n    * a-[Prepare the Data for the CNN](#6)\n    * b-[Building the First Model](#7)\n    * c-[Train Our First Model](#8)\n* 4-[Clearing the Data and Adding Dropouts](#9)\n    * a-[Clearing the Data From Blank Images](#10)\n    * b-[Building Improved Model With Dropouts](#11)\n* 5-[Improving the Accuracy](#12)\n    * a-[Image Data Generator](#13)\n    * b-[More Layers and New Hyperparams](#14)\n* 6-[Conclusion](#15)\n","12123ccb":"Yes new result is much better than old ones.\n\nAnd also val_loss and training loss is very close to each other so there is no overfitting problem. So there is one last problem remains that we need to solve. The accuracy. Can we increase it?\n","97daccb9":"Okay let's apply them.","cf71fd86":"** Volcano?**","59858d90":"### b) Building and Training Improved Model With Dropouts [^](#0) <a id=\"11\"><\/a> <br>\n\nWe cleaned our data and now we can start to build a new model this time we will add dropout layers.\n\nDropout layers will reduce the overfitting. They cut randomly the connections between nodes in layers.\n\n<img src=\"https:\/\/cdn-images-1.medium.com\/max\/1044\/1*iWQzxhVlvadk6VAJjsgXgg.png\" width=\"500\" height=\"500\" \/>","aeb8415f":"We cleared the data but our data's shape is changed. Let's look them.","616489c6":"Second, we need to resize our X_test too like X_train we did before. And we need to add one more dimension both of them beacuse keras want it to know the color chanels of image.","3ee09a1e":"Now we solve the problem let's continue.","34b771d0":"The values and graphs show us validation_loss is same as training_loss. If valitadion_loss too higher than training_loss this means that there is an overfitting problem. It learns training data too much :) and optimize itself just for training data so the weights not worked on validation data and won't work on test data. But in this model suprisingly there is no overfitting problem occur.","f677ee59":"** Number Volcanoes **","68f7bb8d":"Four, we need to normalize our pixels from 0-255 to 0-1. But why we do this? \n\nThe goal of normalization is to change the values of numeric columns in the dataset to use a common scale, without distorting differences in the ranges of values or losing information.","49bf2990":"In conclusion, we tried to improve our results step by step and we achieve more than 0.9 accuracy in test data. \n\nMy observations are:\n* If you add dropout increase epoch because dropouts reduce and cut the momentum of training and you need more epoch to gain old results.\n* Adding layer with a very high number of neurons not good. They can stuck loss rate at the same value for every epoch.\n* Adding Image Data Generator also has the same effect with dropouts.\n* I don't expect this acc rate. CNN works very nice for this mission.\n\nThis kernel is one my first kernel about CNN and there would be some mistakes.\nPlease comment if you see them or if you have a better idea about code.\n\nThanks for reading.","ad55c514":"### c) Train Our First Model [^](#0) <a id=\"8\"><\/a> <br>\nAfter building and compiling the model we can start to train our model.\n\nBatch size will be 64 and epoch will be 5.","8206ce24":"Resize the numpy arrays in here. I just translate the 7000 images 1D to 2D","2dc82280":"And we plot our result graph:","f9bb71e3":"## 4-Clearing the Data and Adding Dropouts[^](#0) <a id=\"9\"><\/a> <br>\nWe tried to train our model it's fine but we can make it better. But how?\n\n### a) Clearing the Data From Blank Images [^](#0) <a id=\"10\"><\/a> <br>\nRemeber that we have some blank\/black images in our data. We need to get rid of them to make our model better.\n\nWe can detect the images which are completely black however the detection of the partly corrupted images will be more difficult.\n\nI have an idea to do this to find black pixel series in all images and delete them if the series' length passed the 10 pixels.","1d519e7b":"## 6-Conclusion [^](#0) <a id=\"15\"><\/a> <br>","659bd614":"Then I change the numerical values with their string equivalents.","a93ec96c":"Okay, we are close to building the best model. After 12 days from the beginning for me.\n\n### a) Image Data Generator [^](#0) <a id=\"13\"><\/a> <br>","5d803b57":"Let's, compile our model. I will use adam optimizer to optimize loss function and use categorical_crossentropy for calculating loss function.","6178323b":"The graph shows us the train and test data balanced but it is sad that we have just 140 images that definitely volcano from nearly 10000 images.\n\nAnd now I will plot the *Number Volcanoes* column that will gives us the number of volcano in one image.","0e81335e":"## 3-The First Step to the CNN [^](#0) <a id=\"5\"><\/a> <br>\n\nAfter printing the images we can start to build our model But firstly we need to clean and prepare our data.\n\n### a) Prepare the Data for the CNN [^](#0)<a id=\"6\"><\/a> <br>\nFirst, we need to split our test data to test and validation data. We will use %50 of test data as validation data. \nWe use the validation data to understand that is there any overfitting problem in our model before testing with test data.","6faf462a":"### b) Building the First Model [^](#0)<a id=\"7\"><\/a> <br>\n\nOkay now we can build our first CNN model. I will try to build very basic one for the first step.\n\nIn a CNN model we use Convolutional layers and Maxpool layers then we add flatten layer to transform the image 2D to 1D. After that we get lot of pixels and we give all of them as a feature to our DL model.\n\nPlease look the image below","4ef6b9cf":"So we have 6999 flatten images that contain 12100 pixels in train set and 2733 in the test set.  So %28 of the data is test. The ratio is good so I will leave it same.\n\nBut we have some big problem here. For train data, we have 6999 images but in labels, we have 7000 labels so there must be one missing image.\n### a) Fixing the Data [^](#0) <a id=\"2\"><\/a> <br>\n\nActually, if you checked above the X_train.head part you will see clearly that the column names are our missing image's pixels. The problem is the indexing of the X_train and X_test and we must reindex this. \n\nThere is also one more thing that we must solve. The column names are floating points and I don't know why :\/ But we can turn them into an integer to use as pixels.","d2101273":"We delete corrupted images here","75e4f0cc":"** Type **","a5ec571d":"We printed our images with their categories. I saw some images has black pixels and we need to clean them for better results but we don't do it now. Instead, we're going to start building the first CNN model.","a8f5c936":"## 5-Improving the Accuracy[^](#0) <a id=\"12\"><\/a> <br>","405634b7":"Then we check again the shape of X_train.","a53a6cfb":"And last we need to change our data's shape again but this time we just change our labels. See the pic below. We name this operation as One Hot Label Encoding. One hot encoding allows the representation of categorical data to be more expressive and Keras likes it.\n![p1](https:\/\/i.imgur.com\/mtimFxh.png)","575602ad":"So how many samples do we have?","338fb7f1":"Third, we need to drop the *Type, Radius, Number Volcanoes* columns from output data. Remember that we just want to label the images that volcano or not.","12a4d217":"I will print some of them randomly","e1d02653":"We can clearly see that our data is unbalanced. I will leave it same for now.\n\nThen let's plot another graph that shows us the Type column distribution.","7d1d16c4":"Now we are ready to print them. I will print images categorically. To do this I use a printer function.","49508f5a":"In this image, we have 3 feature in Input Layer. If we classify the apple and orange we can think this 3 feature as weight, colour, and volume. Colour is an important feature besides the volume or weight when we classify apple and orange.\n\nIn this CNN model, we have 11664 (for the first model) pixels as a feature as an output of CNN layers. And the output is just a volcano or not.\n\nBut how are CNN layers working? Look here-> [DATAI's CNN Tutorial](https:\/\/www.kaggle.com\/kanncaa1\/convolutional-neural-network-cnn-tutorial) \n\nWhy we use CNN? Because CNN is better when we classify images and it finds the important features itself. So actually we don't have to use CNN layers to classify.","e08329fa":"![dl](https:\/\/upload.wikimedia.org\/wikipedia\/commons\/thumb\/4\/46\/Colored_neural_network.svg\/280px-Colored_neural_network.svg.png)","f0f24b04":"We added two dropout layers. The model ready for compiling and training. In this model I also change the epoch and make it 10.","855e2305":"In this *Sequential* model:\n* Convolutional layer, with 8 filters of dimmension 5,5\n* Maxpoll layer, with pool size 2,2\n* Convolutional layer, with 16 filters of dimmension 3,3\n* Maxpoll layer, with pool size 2,2\n* Flatten layer to transform images 2D to 1D\n* Dense layer with sigmoid activation beacuse it's the last layer.","e5e0def3":"![](http:\/\/i63.tinypic.com\/jt3zpg.png)"}}