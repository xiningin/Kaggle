{"cell_type":{"f53e3ac5":"code","d91a170c":"code","b4e17f46":"code","c9023359":"code","8f57e566":"code","e8acea20":"code","ce7cca97":"code","ca3bad2f":"code","d86234ee":"code","25fbdcaa":"code","019928fa":"code","e8b9b480":"markdown","9df2a539":"markdown","3144bd68":"markdown","32d715c0":"markdown","be416c7d":"markdown","b8f5e191":"markdown","c774135c":"markdown","f6118f4b":"markdown","c4bb8ff2":"markdown","64a1a259":"markdown","3a56a240":"markdown","d297833f":"markdown","3f789103":"markdown"},"source":{"f53e3ac5":"import pandas as pd\n\nfrom tabulate import tabulate\n\nfrom sklearn.datasets import load_boston\nfrom sklearn.datasets import load_diabetes\nfrom sklearn.datasets import make_regression\nfrom sklearn.model_selection import train_test_split\n\n# we import the three main functions from the utility script for scoring, training and prediction\nfrom quick_regression import score_models\nfrom quick_regression import train_models\nfrom quick_regression import predict_from_models\n\nBASE = \"\/kaggle\/input\"","d91a170c":"df = pd.read_csv(f\"{BASE}\/sample-data\/mpg.csv\")\n# score_models() just expects your training data as a Pandas dataframe and the column name of the target variable\n# the function prints out scoring values (\"r2\" by default) and processing times per classifier\nscores_mpg = score_models(df, \"mpg\")","b4e17f46":"# the utility script returns a dataframe with a sorted list of scores of 14 classifiers\nprint(tabulate(scores_mpg, showindex=False, floatfmt=\".3f\", headers=\"keys\"))","c9023359":"scores_mpg = score_models(df=df, \n                          target_name=\"mpg\", \n                          sample_size=None, \n                          impute_strategy=\"mean\", \n                          scoring_metric=\"r2\", \n                          log_x=False,\n                          log_y=False, \n                          verbose=True,\n                         )","8f57e566":"# the diamonds data set has more than 50k samples which would take a while to crossvalidate on 14 classifiers\n# we therefore reduce to 1000 samples\ndf = pd.read_csv(f\"{BASE}\/sample-data\/diamonds.csv\")\nscores_diamonds = score_models(df, \"price\", sample_size=1000, verbose=False)\nprint()\nprint(tabulate(scores_diamonds, showindex=False, floatfmt=\".3f\", headers=\"keys\"))","e8acea20":"df = pd.read_csv(f\"{BASE}\/house-prices-advanced-regression-techniques\/train.csv\")\nscores_ames = score_models(df, \"SalePrice\", verbose=False)\nprint(tabulate(scores_ames, showindex=False, floatfmt=\".3f\", headers=\"keys\"))\nprint()\n\n# now trying with log transformed target variable y\nscores_ames = score_models(df, \"SalePrice\", log_y=True, verbose=False)\nprint(tabulate(scores_ames, showindex=False, floatfmt=\".3f\", headers=\"keys\"))\nprint()\n\n# now trying with log transformed predictive variables\nscores_ames = score_models(df, \"SalePrice\", log_x=True, log_y=True, verbose=False)\nprint(tabulate(scores_ames, showindex=False, floatfmt=\".3f\", headers=\"keys\"))\nprint()","ce7cca97":"pipelines = train_models(df, \"SalePrice\", log_y=True)\n\ndf_test = pd.read_csv(f\"{BASE}\/house-prices-advanced-regression-techniques\/test.csv\")\npredictions = predict_from_models(df_test, pipelines)\npredictions.head()","ca3bad2f":"df = pd.read_csv(f\"{BASE}\/tmdb-box-office-prediction\/train.csv\")\nbaseline_tmdb = score_models(df, \"revenue\", 1000)","d86234ee":"from sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import cross_val_score","25fbdcaa":"df = pd.read_csv(f\"{BASE}\/house-prices-advanced-regression-techniques\/train.csv\")\nX = df.select_dtypes(\"number\").drop(\"SalePrice\", axis=1)\ny = df.SalePrice\n\n# using the convenience function make_pipeline() to build a whole data pipeline in just one line of code\npipe = make_pipeline(SimpleImputer(), RobustScaler(), LinearRegression())\nprint(f\"The R2 score is: {cross_val_score(pipe, X, y).mean():.4f}\")","019928fa":"num_cols = df.drop(\"SalePrice\", axis=1).select_dtypes(\"number\").columns\ncat_cols = df.select_dtypes(\"object\").columns\n\n# we instantiate a first Pipeline, that processes our numerical values\nnumeric_transformer = Pipeline(steps=[\n        ('imputer', SimpleImputer()),\n        ('scaler', RobustScaler())])\n\n# the same we do for categorical data\ncategorical_transformer = Pipeline(steps=[\n        ('imputer', SimpleImputer(strategy='most_frequent', fill_value='missing')),\n        ('onehot', OneHotEncoder(handle_unknown='ignore'))])\n    \n# a ColumnTransformer combines the two created pipelines\n# each tranformer gets the proper features according to \u00abnum_cols\u00bb and \u00abcat_cols\u00bb\npreprocessor = ColumnTransformer(\n        transformers=[\n            ('num', numeric_transformer, num_cols),\n            ('cat', categorical_transformer, cat_cols)])\n\npipe = Pipeline(steps=[('preprocessor', preprocessor), ('classifier', LinearRegression())])\n\nX = df.drop(\"SalePrice\", axis=1)\ny = df.SalePrice\nprint(f\"The R2 score is: {cross_val_score(pipe, X, y).mean():.4f}\")\n","e8b9b480":"In just a couple of seconds we get a first impression how several algorithms perform!","9df2a539":"<font color=\"darkred\">**The script simply abstracts all this away and in addition takes care of instantiating the classifiers, crossvalidation, training and prediction.**","3144bd68":"We can log transform the target variable to see if it improves scoring. We can also log transform all the numerical predictive variables.","32d715c0":"# Demo Notebook for quick_regression utility script","be416c7d":"We can tune the scoring by providing several parameters. These are the defaults. ","b8f5e191":"Now on to training and prediction... With just one more line we train all classifiers on the full training set. The function returns the fitted scikit Pipelines.\n\nWe can use these in the next step to predict from the test data. Just be aware: The first column are the predictions from the DummyRegressor. This will very likely spoil your result... \ud83d\ude09","c774135c":"If we have a larger dataset we can e.g. score on a subsample of our data to speed up execution.","f6118f4b":"### References\n\n[Alexis' Kaggle Tutorial](https:\/\/www.kaggle.com\/alexisbcook\/pipelines)<br>\n[Dan Becker's Pipeline Tutorial](https:\/\/www.kaggle.com\/dansbecker\/pipelines)<br>\n[Using the Column Transformer](https:\/\/scikit-learn.org\/stable\/auto_examples\/compose\/plot_column_transformer_mixed_types.html)<br>","c4bb8ff2":"### <font color=\"darkred\">What if we could score 12+ common regression classifiers on our data with just one line of code? \n\nFor the Ames Housing Price competition I experimented with a scikit Pipeline to automically clean and prepare the data, handle numerical and categorical values and crossvalidate on the most common classifiers being used by fellow Kagglers. See the complete notebook here:\n\nhttps:\/\/www.kaggle.com\/chmaxx\/sklearn-pipeline-playground-for-10-classifiers\n\nThis experimental playground I extended to this small utility script that can be used on any training data for a regression problem:\n\nhttps:\/\/www.kaggle.com\/chmaxx\/quick-regression","64a1a259":"Let's try some more Kaggle datasets out of the box and see what happens.","3a56a240":"The same we can setup with two pipeline branches for numerical and categorical data.","d297833f":"Let's try the script on a first regression problem: Predicting miles per gallon from car data. The data is [taken from seaborn demo data here](https:\/\/github.com\/mwaskom\/seaborn-data).","3f789103":"## Behind the scenes\nAt the core of the util script I used scikit-learn's pipeline class. This allows to chain arbitrary transformers with a final estimator. Let's look at a simple example. "}}