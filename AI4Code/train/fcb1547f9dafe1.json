{"cell_type":{"94f83668":"code","d49b2279":"code","7f447d90":"code","e892ae64":"code","a9e338b7":"code","690c7f42":"code","e69ac1f0":"code","724d7053":"code","fca60c20":"code","f42deb2f":"code","c83f3051":"code","f5520b33":"code","7b0a86be":"code","e44d4000":"code","986f3e29":"code","633af208":"code","3aaec3ac":"code","ca4f99e6":"code","80e51c6b":"code","78ad707f":"code","2a2b3154":"code","c6ca2ea3":"code","730bc00c":"markdown","2d6d359d":"markdown","83f0a3e5":"markdown","deede8cf":"markdown","58db7837":"markdown","2d18df77":"markdown","abfbb707":"markdown","5cb35191":"markdown","f2fe7ab1":"markdown","90d60e82":"markdown","6c880cf6":"markdown","ec45dd8a":"markdown","32721516":"markdown","3c66991d":"markdown","d3a2a0ba":"markdown","4c4adc8b":"markdown","d862e3e0":"markdown","0d06ab6d":"markdown","73a066de":"markdown","02d8cf35":"markdown"},"source":{"94f83668":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\n\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.manifold import TSNE\nfrom sklearn.preprocessing import StandardScaler\nimport umap\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n\nimport keras\nfrom keras.datasets import mnist\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Flatten\nfrom keras.layers import Conv2D, MaxPooling2D, BatchNormalization\nfrom keras import backend as K\nfrom keras.utils.vis_utils import plot_model\nimport matplotlib.pyplot as plt\nimport os\nimport tensorflow as tf\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","d49b2279":"RANDOM_STATE = 2021\nimg_rows, img_cols = 5, 10","7f447d90":"train = pd.read_csv(\"..\/input\/tabular-playground-series-may-2021\/train.csv\", index_col = 'id')\ntest = pd.read_csv(\"..\/input\/tabular-playground-series-may-2021\/test.csv\", index_col = 'id')","e892ae64":"# Duplicates in dataset? This is noise ... kill them ....\n# I find it thanks @omarvivas: https:\/\/www.kaggle.com\/c\/tabular-playground-series-may-2021\/discussion\/236561\n\ntrain = train[~train.drop('target', axis = 1).duplicated()]\ntrain.shape","a9e338b7":"X = pd.DataFrame(train.drop(\"target\", axis = 1))\nlencoder = LabelEncoder()\ny = pd.DataFrame(lencoder.fit_transform(train['target']), columns=['target'])","690c7f42":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state= RANDOM_STATE)","e69ac1f0":"X_train = np.array(X_train, dtype= np.float32) \nX_train = X_train.reshape(X_train.shape[0], img_rows, img_cols, 1)\ny_train = np.array(y_train)\n\nX_test = np.array(X_test, dtype= np.float32) \nX_test = X_test.reshape(X_test.shape[0], img_rows, img_cols, 1)\ny_test = np.array(y_test)\n\ntest = np.array(test, dtype= np.float32)\ntest = test.reshape(test.shape[0], img_rows, img_cols, 1)\n\nprint(f'X_train shape: {X_train.shape}')\nprint(f'X_test shape: {y_train.shape}')\nprint(f'X_test shape: {X_test.shape}')\nprint(f'X_test shape: {y_test.shape}')\n\nX_train = X_train.astype('float32')\nX_test = X_test.astype('float32')\ntest = test.astype('float32')\n\nX_train \/= 255\nX_test \/= 255\ntest \/= 255","724d7053":"num = 25\nimages = X_train[:num]\nlabels = y_train[:num]\n\nnum_row = 5\nnum_col = 5\n\n\nfig, axes = plt.subplots(num_row, num_col, figsize=(3*num_col,4*num_row))\nfor i in range(num):\n    ax = axes[i\/\/num_col, i%num_col]\n    ax.imshow(images[i], cmap='gray')\n    ax.set_title('Label: {}'.format(labels[i]))\nplt.tight_layout()\nplt.show()","fca60c20":"# Lets look on TSNE  \ntrain_sub = train.sample(10000, random_state= RANDOM_STATE)\nmodel = TSNE(n_components=2, random_state=0, perplexity= 50, n_iter=3000)\ntsne_data = model.fit_transform(StandardScaler().fit_transform(train_sub.drop('target', axis = 1).astype(float)))\ntsne_data = np.vstack((tsne_data.T, train_sub.target)).T\n\ntsne_df = pd.DataFrame(data=tsne_data, columns=(\"D1\", \"D2\", \"target\"))\n\nsns.FacetGrid(tsne_df, hue=\"target\", height=6).map(plt.scatter, 'D1', 'D2').add_legend()\nplt.title('Perplexity= 50, n_iter=3000')\nplt.show()","f42deb2f":"train_sub = train.sample(10000, random_state= RANDOM_STATE)\nlda_data = LDA(n_components=3).fit_transform(train_sub.drop(columns='target'),train_sub.target)\nplt.figure(figsize=(10,10))\nsns.scatterplot(x = lda_data[:, 0], y = lda_data[:, 1], hue = 'target', data=train_sub)","c83f3051":"train_sub = train.sample(10000, random_state= RANDOM_STATE)\nembedding = umap.UMAP(random_state = RANDOM_STATE ,n_components=3).fit_transform(train_sub.drop(columns='target').to_numpy())\nplt.figure(figsize=(10,10))\nsns.scatterplot(x = embedding[:, 0], y = embedding[:, 1], hue='target', data=train_sub)","f5520b33":"batch_size = 128\nnum_classes = 4\nepochs = 50\n\ninput_shape = (img_rows, img_cols, 1)","7b0a86be":"y_train = keras.utils.to_categorical(y_train, num_classes)\ny_test = keras.utils.to_categorical(y_test, num_classes)","e44d4000":"# standard model ... nothing special ... \n\nmodel = Sequential()\nx = Conv2D(256, kernel_size=(2, 2), padding='same', activation='relu', input_shape=input_shape)\nmodel.add(x)\nmodel.add(Conv2D(128, (3, 3), padding='same', activation='relu'))\nmodel.add(Conv2D(64, (2, 2), padding='same', activation = 'relu'))\nmodel.add(Conv2D(32, (2, 2), padding='same', activation = 'relu'))\nmodel.add(Flatten())\nmodel.add(Dense(128, activation='relu'))\nmodel.add(Dense(63, activation='relu'))\nmodel.add(Dense(32, activation='relu'))\nmodel.add(Dense(num_classes, activation='softmax'))\n\nmodel.compile(loss=keras.losses.categorical_crossentropy,\n              optimizer=keras.optimizers.Adadelta(),\n              metrics=['accuracy'])","986f3e29":" earlystop = tf.keras.callbacks.EarlyStopping(\n        monitor='val_accuracy',\n        mode='max',\n        patience=10, \n        verbose=1\n    )\n\nhistory = model.fit(X_train, y_train,\n                    batch_size=batch_size,\n                    epochs=epochs,\n                    verbose=1,\n                    validation_data=(X_test, y_test),\n                    callbacks = earlystop)","633af208":"plt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()","3aaec3ac":"score = model.evaluate(X_test, y_test, verbose=0)\nprint(\"Accuracy on test set: \",score[1])","ca4f99e6":"filters, biases = x.get_weights()\nconv_weight = filters[:,:,0,:]\n\n# Check the shape of first Conv2D layer\nprint(f'First conv2D shape: {filters.shape}')\nprint(f'First conv2D output size: {x.output.shape} \\n')\n\nplt.figure(figsize = (20,10))\nprint(\"First 16 filters of conv2D layer\")\nfor i in range(1,17):\n    plt.subplot(4,4,i)\n    plt.imshow(conv_weight[:,:,i], interpolation='nearest', cmap='gray', aspect='auto')\n\nplt.show()","80e51c6b":"from numpy import expand_dims\nfrom keras.models import Model\n\n# I take one example from test dataset\nimg = expand_dims(X_test[0], axis=0)\n\n# Then hijacked output from first layer\nmodel_first2D = Model(inputs=model.inputs, outputs=x.output)\n\n# Made prediction of first sample\nfeature_maps = model_first2D.predict(img)\n\n# Plot all (32) images from our conv2D layer \nplt.figure(figsize = (40,20))\nsquare = 8\nix = 1\nfor _ in range(4):\n    for _ in range(square):\n        ax = plt.subplot(square, square, ix)\n        plt.imshow(feature_maps[0, :, :, ix-1], cmap='gray', interpolation='nearest')\n        ix += 1\nplt.show()","78ad707f":"preds = model.predict_proba(test)","2a2b3154":"sub = pd.read_csv(\"\/kaggle\/input\/tabular-playground-series-may-2021\/sample_submission.csv\")\n\npredictions_df = pd.DataFrame(preds, columns = [\"Class_1\", \"Class_2\", \"Class_3\", \"Class_4\"])\npredictions_df['id'] = sub['id']","c6ca2ea3":"predictions_df.to_csv(\"conv_net_submission.csv\", index = False)","730bc00c":"### A. FILTERS - first conv layer ","2d6d359d":"## 2B. TSNE","83f0a3e5":"# 3. SUBMIT","deede8cf":"## 2A. CONVERT TO CATEGORICAL","58db7837":"## 2C. LDA","2d18df77":"## 2B. DEFINE MODEL ARCHITECTURE","abfbb707":"## 2D. UMAP","5cb35191":"I love making AI fun. This is for fun ... or just this is not fun and you can find something cool with this experiment. I appreciate comments, feedback or any improvements for this notebook.","f2fe7ab1":"# 2. Neural Network (Keras 2DConv)\nI take just simple NN architecture from cool MNIST dataset. You can just train this network as you can. ** Just play and have fun !!!","90d60e82":"## 2F. HACK THE MODEL","6c880cf6":"## 2A. Show images","ec45dd8a":"### B. OUTPUTS\nHow input looks like after filter application  ... ","32721516":"## 2C. TRAIN ","3c66991d":"## 2D. VISUALIZE TRAINING LOSS","d3a2a0ba":"# 2. VISUALIZE DATA (IMAGES)","4c4adc8b":"## 2F. PREDICT","d862e3e0":"# 1. LOAD DATA AND TRANSFORM FOR NN","0d06ab6d":"# [TPS-05] What if we assume that data are images ... Can we use convolution for this challange?","73a066de":"## 2E. MODEL EVALUATE","02d8cf35":"# 0. PREPARE"}}