{"cell_type":{"38836280":"code","5f071b0b":"code","d50af7e4":"code","1d863c1d":"code","efaf314a":"code","a77e4af9":"code","4fce80bf":"code","e480a3cd":"code","f17ed80f":"code","10a43ecd":"code","beb66296":"code","389a0525":"code","60f896fb":"code","9af8a037":"code","ca8cf63c":"code","f6c34759":"code","cc5e67e3":"code","70bde7b5":"code","8b2459b4":"code","d821e8bb":"markdown","b77dd932":"markdown","51dfc1f0":"markdown","8b58dfbe":"markdown","68049d1e":"markdown","24021d8a":"markdown","3b098abb":"markdown","87a5fda7":"markdown","0cb45ea1":"markdown","cee0094d":"markdown","207b45b0":"markdown"},"source":{"38836280":"# Import numpy, pandas, and matplotlib using the standard aliases.\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# Import the following tools from sklearn: \n#     Pipeline, SimpleImputer, ColumnTransformer, OneHotEncoder, StandardScaler\n#     LogisticRegression, DecisionTreeClassifier, RandomForestClassifier, GridSearchCV\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.preprocessing import StandardScaler\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import GridSearchCV\n\n# Import joblib\nimport joblib","5f071b0b":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","d50af7e4":"# Load the training data into a DataFrame named 'train'. \n# Print the shape of the resulting DataFrame. \n# You do not need the test data in this notebook. \ntrain = pd.read_csv('\/kaggle\/input\/titanic\/train.csv')\n\nprint(train.shape)","1d863c1d":"# Display the head of the train DataFrame. \ntrain.head()","efaf314a":"train.count()","a77e4af9":"# Calculate and print the number of number of missing values in each column.\ntrain.isnull().sum().sort_values(ascending=False)","4fce80bf":"# Display a DataFrame showing the proportion of observations with each \n# possible of the target variable (which is Survived). \n(train.Survived.value_counts()\/len(train)).to_frame()","e480a3cd":"# We will start with some feature engineering. \n\n# Add a new column named 'FamSize' to the DataFrame. \n# This should be the sum of the 'SibSp' and 'Parch' columns. \n\ntrain['FamSize'] = train['SibSp'] + train['Parch']\n\n# We will use the function below to determine the deck letter for each passenger:\ndef set_deck(cabin):\n    if str(cabin) == 'nan':\n        return 'Missing'\n    return cabin[0]\n\n# Use the map() method of the train DataFrame to apply the function above \n# to the 'Cabin' column. Store the results in a new column named 'Deck'. \ntrain['Deck'] = train['Cabin'].map(set_deck)\ntrain.head()","f17ed80f":"# Create a list of numberical feature names. Use the following features: 'Age', 'FamSize', 'Fare'\nnum_features = ['Age','FamSize','Fare']\n\n# Create a list of categorical feature names. Use the following features: 'Sex', 'Pclass', 'Deck', 'Embarked'\ncat_features = ['Sex','Pclass','Deck','Embarked']\n\n# Combine the two previous lists into one list named 'features'\nfeatures = num_features + cat_features\n\n# Create a Pipeline object for processing the numerical features. \n# This pipeline should consist of a SimpleImputer and a StandardScaler\nnum_transformer = Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='mean')),\n    ('scaler', StandardScaler())])\n\n# Create a Pipeline object for processing the categorical features. \n# This pipeline should consist of a SimpleImputer and a OneHotEncoder\ncat_transformer = Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='constant', fill_value='Missing')),\n    ('onehot', OneHotEncoder(handle_unknown='ignore'))])\n\n# Create a ColumnTransformer object that combines the two pipelines created above. \n# Name this ColumnTransformer 'preprocessor'\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('num', num_transformer, num_features),\n        ('cat', cat_transformer, cat_features)])","10a43ecd":"features","beb66296":"# Fit the preprocessor to the training data, selecting only the columns in the 'features' list. \n# Store the array created in the previous step into a variable named 'X_train'.\npreprocessor.fit(train[features])\nX_train = preprocessor.transform(train[features])\ny_train = train['Survived']\n\n# Create a variable named 'y_train' that contains the training labels.\n\n# Print the shapes of X_train and y_train.\nprint('X_train shape:', X_train.shape)\nprint('y_train shape:', y_train.shape)","389a0525":"# Select a range of parameter values of C. \n# You might need to experiment with this to find a good value for C\n# Update the code below to perform 10-Fold cross-validation. \n# Set the scoring parameter below to 'accuracy'\n\n%time \n\nlr_clf = LogisticRegression(max_iter=1000, solver='saga', penalty='elasticnet')\n\nlr_parameters = {\n    'l1_ratio':[0, 0.5, 1],\n    'C': [0.001,.009,0.01,.09,1,5,10,25]\n}\n\nlr_grid = GridSearchCV(lr_clf, lr_parameters, cv=10, refit='True', n_jobs=-1, verbose=10, scoring='accuracy')\nlr_grid.fit(X_train, y_train)\n\nlr_model = lr_grid.best_estimator_\n\nprint('Best Parameters:', lr_grid.best_params_)\nprint('Best CV Score:  ', lr_grid.best_score_)\nprint('Training Acc:   ', lr_model.score(X_train, y_train))","60f896fb":"# Run this cell without any changes to view the CV results.\n\nlr_summary = pd.DataFrame(lr_grid.cv_results_['params'])\nlr_summary['cv_score'] = lr_grid.cv_results_['mean_test_score']\n\nfor r in lr_parameters['l1_ratio']:\n    temp = lr_summary.query(f'l1_ratio == {r}')\n    plt.plot(temp.C, temp.cv_score, label=r)\nplt.xscale('log')\nplt.ylim([0.75, 0.82])\nplt.xlabel('Regularization Parameter (C)')\nplt.ylabel('CV Score')\nplt.legend(title='L1 Ratio', loc='lower right')\nplt.grid()\nplt.show()\n\nprint(lr_summary.to_string(index=False))","9af8a037":"# Select values to consider for the max_depth and min_samples_leaf hyperparameters.\n# You might need to experiment to find a good range of parameter values. \n# Update the code below to perform 10-Fold cross-validation. \n# Set the scoring parameter below to 'accuracy'\n\n%time \n\ndt_clf = DecisionTreeClassifier(random_state=1)\n\ndt_parameters = {\n    'max_depth': range(2,30),\n    'min_samples_leaf': range(1,20)\n}\n\ndt_grid = GridSearchCV(dt_clf, dt_parameters, cv=10, refit='True', n_jobs=-1, verbose=0, scoring='accuracy')\ndt_grid.fit(X_train, y_train)\n\ndt_model = dt_grid.best_estimator_\n\nprint('Best Parameters:', dt_grid.best_params_)\nprint('Best CV Score:  ', dt_grid.best_score_)\nprint('Training Acc:   ', dt_model.score(X_train, y_train))","ca8cf63c":"# Run this cell without any changes to view the CV results.\n\ndt_summary = pd.DataFrame(dt_grid.cv_results_['params'])\ndt_summary['cv_score'] = dt_grid.cv_results_['mean_test_score']\n\nfor ms in dt_parameters['min_samples_leaf']:\n    temp = dt_summary.query(f'min_samples_leaf == {ms}')\n    plt.plot(temp.max_depth, temp.cv_score, label=ms)\nplt.xlabel('Maximum Depth')\nplt.ylabel('CV Score')\nplt.legend(title='Min Samples')\nplt.grid()\nplt.show()\n\nprint(dt_summary.to_string(index=False))","f6c34759":"# Select a number of trees to use in your random forest.\n# Select values to consider for the max_depth and min_samples_leaf hyperparameters.\n# You might need to experiment to find a good range of parameter values. \n# Update the code below to perform 10-Fold cross-validation. \n# Set the scoring parameter below to 'accuracy'\n\n%time \n\nrf_clf = RandomForestClassifier(random_state=1, n_estimators=100)\n\nrf_parameters = {\n    'max_depth': range(2, 30),\n    'min_samples_leaf': range(1,20)\n}\n\nrf_grid = GridSearchCV(rf_clf, rf_parameters, cv=10, refit='True', n_jobs=-1, verbose=0, scoring='accuracy')\nrf_grid.fit(X_train, y_train)\n\nrf_model = rf_grid.best_estimator_\n\nprint('Best Parameters:', rf_grid.best_params_)\nprint('Best CV Score:  ', rf_grid.best_score_)\nprint('Training Acc:   ', rf_model.score(X_train, y_train))","cc5e67e3":"# Run this cell without any changes to view the CV results.\n\nrf_summary = pd.DataFrame(rf_grid.cv_results_['params'])\nrf_summary['cv_score'] = rf_grid.cv_results_['mean_test_score']\n\nfor ms in rf_parameters['min_samples_leaf']:\n    temp = rf_summary.query(f'min_samples_leaf == {ms}')\n    plt.plot(temp.max_depth, temp.cv_score, label=ms)\nplt.xlabel('Maximum Depth')\nplt.ylabel('CV Score')\nplt.legend(title='Min Samples')\nplt.grid()\nplt.show()\n\nprint(rf_summary.to_string(index=False))","70bde7b5":"# Save your pipeline to a file. \n# Determine the best model found above and save that to a file. \n# Download both files to your local device and then upload them as a Kaggle dataset. \n\nfinal_model = RandomForestClassifier(n_estimators=100, max_depth=21, min_samples_leaf = 2,random_state=42)\nfinal_model.fit(X_train, y_train)\n\nprint(f'Training Accuracy for Final Model:  {round(final_model.score(X_train, y_train),4)}')","8b2459b4":"joblib.dump(preprocessor, 'Preprocessor.joblib')\njoblib.dump(final_model, 'Final_Model.joblib')","d821e8bb":"# Check for Missing Values","b77dd932":"# Decicion Trees","51dfc1f0":"# Save Pipeline and Model","8b58dfbe":"# Titanic dataset analysis","68049d1e":"#### Loading libraries and dependencies.","24021d8a":"# Model Selection","3b098abb":"## Logistic Regression","87a5fda7":"# Check Label Distribution","0cb45ea1":"### Exploratory Data Analysis","cee0094d":"# Random Forests","207b45b0":"# Preprocessing"}}