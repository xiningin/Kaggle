{"cell_type":{"d6dc06e3":"code","ebb4fd57":"code","550b4282":"code","b334441d":"code","c3c994ba":"code","ad7b57c4":"code","27eb489e":"code","ac6e2ddc":"code","374a0ff5":"code","b71f48fd":"code","613121fa":"code","840dc09a":"code","f7d6657f":"code","b267a836":"code","71f6cdeb":"code","b05960ea":"code","989c836b":"code","2c2e6b57":"code","603b9bbe":"code","ce0628b1":"code","a54e2b37":"code","09a06482":"code","d201aa05":"code","263ccfad":"code","9f4f7647":"code","312a42a3":"code","46ef2eb0":"code","13e5a99a":"code","5a53d3e6":"code","ae92ac50":"code","422a422d":"code","1abd9a9c":"code","9fd1c799":"code","c76cf855":"markdown","89cd4949":"markdown","4d0c8f7a":"markdown","eac0593e":"markdown","58fdfe33":"markdown","fc23086a":"markdown","73c2a3f5":"markdown","8010c976":"markdown","5465fab8":"markdown","2607aed6":"markdown","83bd6274":"markdown","b6ae296b":"markdown","f872c061":"markdown"},"source":{"d6dc06e3":"#load Libraries\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","ebb4fd57":"from sklearn.preprocessing import MultiLabelBinarizer\nfrom sklearn.metrics import accuracy_score,hamming_loss\nfrom skmultilearn.problem_transform import BinaryRelevance\nfrom sklearn.naive_bayes import GaussianNB \nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.multioutput import ClassifierChain, MultiOutputClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier, VotingClassifier, AdaBoostClassifier\nfrom sklearn.model_selection import cross_validate, train_test_split, ShuffleSplit, GridSearchCV, learning_curve\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import classification_report\nfrom keras.models import Sequential\nfrom keras.layers import Dense,Conv1D, MaxPooling1D, Flatten, Dropout,AveragePooling1D\nfrom sklearn.metrics import accuracy_score\nfrom keras.utils.vis_utils import plot_model\nimport statistics\nimport warnings\nwarnings.filterwarnings(\"ignore\")","550b4282":"traindf=pd.read_csv('\/kaggle\/input\/multi-label-tweets-feature-extraction-lexicon-3\/vad_hlex_lex_doc2vec_freq_bi_tweets_train.csv')\ndevdf=pd.read_csv('\/kaggle\/input\/multi-label-tweets-feature-extraction-lexicon-3\/vad_hlex_lex_doc2vec_freq_bi_tweets_dev.csv')\ntestdf=pd.read_csv('\/kaggle\/input\/multi-label-tweets-feature-extraction-lexicon-3\/vad_hlex_lex_doc2vec_freq_bi_tweets_test.csv')\ntraindf.columns.values","b334441d":"feature_vector = [\"freq_anger\",\"bi_anger\",\"doc2vec_anger\",\"lex_Anger\",\"hlex_anger\", \"freq_anticipation\",\"bi_anticipation\",\"doc2vec_anticipation\", \"lex_Anticipation\", \"hlex_anticipation\", \"freq_disgust\",\"bi_disgust\",\"doc2vec_disgust\",\"lex_Disgust\", \"hlex_disgust\", \"freq_fear\",\"bi_fear\",\"doc2vec_fear\", \"lex_Fear\", \"hlex_fear\", \"freq_joy\",\"bi_joy\",\"doc2vec_joy\", \"lex_Joy\",\"hlex_joy\", \"freq_love\", \"bi_love\", \"doc2vec_love\", \"freq_optimism\", \"bi_optimism\", \"doc2vec_optimism\", \"freq_pessimism\", \"bi_pessimism\", \"doc2vec_pessimism\", \"freq_sadness\",\"bi_sadness\",\"doc2vec_sadness\", \"lex_Sadness\", \"hlex_sadness\", \"freq_surprise\",\"bi_surprise\",\"doc2vec_surprise\",\"lex_Surprise\", \"hlex_surprise\", \"freq_trust\",\"bi_trust\",\"doc2vec_trust\", \"lex_Trust\", \"hlex_trust\", \"lex_Positive\",\"lex_Negative\",\"V\",\"D\",\"A\"]\ntrain_x = traindf[feature_vector]\ndev_x = devdf[feature_vector]\ntest_x = testdf[feature_vector]\n#feature vector length\nlen(train_x.columns)","c3c994ba":"#define emotion class\nem=[\"anger\",\"anticipation\",\"disgust\",\"fear\",\"joy\",\"love\",\"optimism\",\"pessimism\",\"sadness\",\"surprise\",\"trust\"]","ad7b57c4":"#reformate emotion classes\ndef labelling_classes(df):\n    arr=[]\n    for i,val in enumerate(df.iterrows()):\n        lbl=[]\n        for e in em:\n            if df.iloc[i][e]==1:\n                lbl.append(e)\n        arr.append(lbl)\n    return arr\n   \n#define classes \ntrain_y=traindf[em]\ntrain_y[\"classes\"]=np.array(labelling_classes(traindf))\n\ndev_y=devdf[em]\ndev_y[\"classes\"]=np.array(labelling_classes(devdf))\n","27eb489e":"#split data for training and testing\n#shuffle to apply random shuffle on data splitting\ncv = ShuffleSplit( n_splits=5, test_size=0.33)","ac6e2ddc":"\nmlb = MultiLabelBinarizer(classes=(\"anger\",\"anticipation\",\"disgust\",\"fear\",\"joy\",\"love\",\"optimism\",\"pessimism\",\"sadness\",\"surprise\",\"trust\"))\ny_enc = mlb.fit_transform(train_y[\"classes\"])\n\nmlb = MultiLabelBinarizer(classes=(\"anger\",\"anticipation\",\"disgust\",\"fear\",\"joy\",\"love\",\"optimism\",\"pessimism\",\"sadness\",\"surprise\",\"trust\"))\nydev_enc = mlb.fit_transform(dev_y[\"classes\"])\n ","374a0ff5":"#hamming score \ndef hamming_score(y_true, y_pred, normalize=True, sample_weight=None):\n    '''\n    Compute the Hamming score (a.k.a. label-based accuracy) for the multi-label case\n    http:\/\/stackoverflow.com\/q\/32239577\/395857\n    '''\n    acc_list = []\n    for i in range(y_true.shape[0]):\n        set_true = set( np.where(y_true[i])[0] )\n        set_pred = set( np.where(y_pred[i])[0] )\n        tmp_a = None\n        if len(set_true) == 0 and len(set_pred) == 0:\n            tmp_a = 1\n        else:\n            tmp_a = len(set_true.intersection(set_pred))\/float(len(set_true.union(set_pred)) )\n        acc_list.append(tmp_a)\n    return np.mean(acc_list)","b71f48fd":"def plot_learning_curve(estimator, title, X, y, ylim=None, cv=None,n_jobs=1, train_sizes=np.linspace(.1, 1.0, 5)):\n    \n    plt.figure()\n    plt.title(title)\n    if ylim is not None:\n        plt.ylim(*ylim)\n    plt.xlabel(\"Training examples\")\n    plt.ylabel(\"Score\")\n    train_sizes, train_scores, test_scores = learning_curve(\n        estimator, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes)\n    train_scores_mean = np.mean(train_scores, axis=1)\n    train_scores_std = np.std(train_scores, axis=1)\n    test_scores_mean = np.mean(test_scores, axis=1)\n    test_scores_std = np.std(test_scores, axis=1)\n    plt.grid()\n\n    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n                     train_scores_mean + train_scores_std, alpha=0.1,\n                     color=\"r\")\n    plt.fill_between(train_sizes, test_scores_mean - test_scores_std,\n                     test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n    plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n             label=\"Training score\")\n    plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n             label=\"Cross-validation score\")\n\n    plt.legend(loc=\"best\")\n    return plt","613121fa":"def test_submit(test_predict,file_name):\n    test_submission = pd.read_csv('\/kaggle\/input\/semeval-2018-task-ec\/2018-E-c-En-test.txt',sep=\"\\t\",encoding=\"utf-8\")\n    i = 0\n    for e in em:\n        test_submission[e]= test_predict[:,i]\n        i = i + 1\n    test_submission.to_csv(file_name + \".txt\",sep=\"\\t\",encoding=\"utf-8\",index=False,header=True)","840dc09a":"from matplotlib import pyplot\ndef return_counts(df,x):\n    total=0\n    for e in em:\n        total = total + len(df[df[e]==x])\n    return total\n    \ndef plot_test_res(df,predict):\n    i=0\n    for e in em:\n        df[e]=predict[:,i]\n        i = i +1\n    \n    ax,fig=pyplot.subplots(len(em)+1, 1,figsize=(15,25))\n    i=0\n    for e in em:\n        i=i+1\n        pyplot.subplot(len(em)+1,1,i)\n        df[e].value_counts().plot.bar(title=e)\n    \n    ones = return_counts(df,1)\n    print(\"ones\",ones)\n    zeros = return_counts(df,0)\n    print(\"zeros\",zeros)\n    pyplot.subplot(len(em)+1,1,len(em)+1)\n    pyplot.bar([\"1\",\"0\"],[ones,zeros])\n    \n    pyplot.show()    ","f7d6657f":"#Guassian Naive Bayes\ncv = ShuffleSplit( n_splits=3, test_size=0.34)\n\nGnb = BinaryRelevance(classifier = GaussianNB())\nparam_values = {'classifier__var_smoothing':[1e-10,1e-9]}  \nGS_gnb = GridSearchCV(Gnb, param_grid=param_values, cv=cv)\nGS_gnb.fit(train_x,y_enc)\nprint(\"best estimator parameters\",GS_gnb.best_estimator_)\n\nprint(\"training data evaluation\")\ny_pred=GS_gnb.best_estimator_.predict(train_x)\nscore=hamming_score(y_enc,y_pred.toarray())\nloss=hamming_loss(y_enc,y_pred.toarray())\nprint(\"hamming score\",score)\nprint(\"hamming loss\",loss)\nprint(classification_report(y_enc,y_pred))\n\nprint(\"development data evaluation\")\ny_pred=GS_gnb.best_estimator_.predict(dev_x)\nscore=hamming_score(ydev_enc,y_pred.toarray())\nloss=hamming_loss(ydev_enc,y_pred.toarray())\nprint(\"hamming score\",score)\nprint(\"hamming loss\",loss)\nprint(classification_report(ydev_enc,y_pred))\n\nprint(\"plot learning curve\")\nplot_learning_curve(GS_gnb.best_estimator_, \"Guassian Bayes\", train_x,y_enc, cv=cv)","b267a836":"#test data\ntest_pred=GS_gnb.best_estimator_.predict(test_x).toarray()\nplot_test_res(testdf, test_pred)","71f6cdeb":"#Support vector machine\ncv = ShuffleSplit( n_splits=3, test_size=0.34)\n\nsvm = BinaryRelevance(classifier = SVC(probability=True))\nparam_values = {'classifier__gamma': [1, 0.01]}  \nGS_svm = GridSearchCV(svm, param_grid=param_values, cv=cv)\nGS_svm.fit(train_x,y_enc)\nprint(\"best estimator parameters\",GS_svm.best_estimator_)\n\nprint(\"training data evaluation\")\ny_pred=GS_svm.best_estimator_.predict(train_x)\nscore=hamming_score(y_enc,y_pred.toarray())\nloss=hamming_loss(y_enc,y_pred.toarray())\nprint(\"hamming score\",score)\nprint(\"hamming loss\",loss)\nprint(classification_report(y_enc,y_pred))\n\nprint(\"development data evaluation\")\ny_pred=GS_svm.best_estimator_.predict(dev_x)\nscore=hamming_score(ydev_enc,y_pred.toarray())\nloss=hamming_loss(ydev_enc,y_pred.toarray())\nprint(\"hamming score\",score)\nprint(\"hamming loss\",loss)\nprint(classification_report(ydev_enc,y_pred))\n\nprint(\"plot learning curve\")\nplot_learning_curve(GS_svm.best_estimator_, \"SVM\", train_x,y_enc, cv=cv)","b05960ea":"#test data\ntest_pred=GS_svm.best_estimator_.predict(test_x).toarray()\nplot_test_res(testdf, test_pred)","989c836b":"#K nearest neigbor\ncv = ShuffleSplit( n_splits=3, test_size=0.34)\n\nknn = BinaryRelevance(classifier = KNeighborsClassifier())\nparam_values = {'classifier__n_neighbors':[5,7,9,11]}  \nGS_knn = GridSearchCV(knn, param_grid=param_values, cv=cv)\nGS_knn.fit(train_x,y_enc)\nprint(\"best estimator parameters\",GS_knn.best_estimator_)\n\nprint(\"training data evaluation\")\ny_pred=GS_knn.best_estimator_.predict(train_x)\nscore=hamming_score(y_enc,y_pred.toarray())\nloss=hamming_loss(y_enc,y_pred.toarray())\nprint(\"hamming score\",score)\nprint(\"hamming loss\",loss)\nprint(classification_report(y_enc,y_pred))\n\nprint(\"development data evaluation\")\ny_pred=GS_knn.best_estimator_.predict(dev_x)\nscore=hamming_score(ydev_enc,y_pred.toarray())\nloss=hamming_loss(ydev_enc,y_pred.toarray())\nprint(\"hamming score\",score)\nprint(\"hamming loss\",loss)\nprint(classification_report(ydev_enc,y_pred))\n\nprint(\"plot learning curve\")\nplot_learning_curve(GS_knn.best_estimator_, \"KNN\", train_x,y_enc, cv=cv)","2c2e6b57":"#test data\ntest_pred=GS_knn.best_estimator_.predict(test_x).toarray()\nplot_test_res(testdf, test_pred)","603b9bbe":"#logistic Regression\ncv = ShuffleSplit( n_splits=3, test_size=0.34)\n\nlr=BinaryRelevance(classifier = LogisticRegression())\nparam_values = {'classifier__penalty': ['l1','l2'], 'classifier__C': [0.001,0.01,1,10,30,50]}\nGS_lr = GridSearchCV(lr, param_grid=param_values, cv=cv)\nGS_lr.fit(train_x,y_enc)\nprint(\"best estimator parameters\",GS_lr.best_estimator_)\n\nprint(\"training data evaluation\")\ny_pred=GS_lr.best_estimator_.predict(train_x)\nscore=hamming_score(y_enc,y_pred.toarray())\nloss=hamming_loss(y_enc,y_pred.toarray())\nprint(\"hamming score\",score)\nprint(\"hamming loss\",loss)\nprint(classification_report(y_enc,y_pred))\n\nprint(\"development data evaluation\")\ny_pred=GS_lr.best_estimator_.predict(dev_x)\nscore=hamming_score(ydev_enc,y_pred.toarray())\nloss=hamming_loss(ydev_enc,y_pred.toarray())\nprint(\"hamming score\",score)\nprint(\"hamming loss\",loss)\nprint(classification_report(ydev_enc,y_pred))\n\nprint(\"plot learning curve\")\nplot_learning_curve(GS_knn.best_estimator_, \"Logistic Regression (BR)\", train_x,y_enc, cv=cv)","ce0628b1":"#test data\ntest_pred=GS_lr.best_estimator_.predict(test_x).toarray()\nplot_test_res(testdf, test_pred)","a54e2b37":"#MLP(Shallow)\ncv = ShuffleSplit( n_splits=3, test_size=0.34)\n\nfrom sklearn.neural_network import MLPClassifier\nmlp = MLPClassifier()\nparam_values = {'max_iter': [30,50,100], 'hidden_layer_sizes': [(32),(32,16),(16),(8),(16,32)]}\nGS_mlp = GridSearchCV(mlp, param_grid=param_values, cv=cv)\nGS_mlp.fit(train_x, y_enc)\nprint(\"best estimator parameters\",GS_mlp.best_estimator_)\n\nprint(\"training data evaluation\")\ny_pred=GS_mlp.best_estimator_.predict(train_x)\nscore=hamming_score(y_enc,y_pred)\nloss=hamming_loss(y_enc,y_pred)\nprint(\"hamming score\",score)\nprint(\"hamming loss\",loss)\nprint(classification_report(y_enc,y_pred))\n\nprint(\"development data evaluation\")\ny_pred=GS_mlp.best_estimator_.predict(dev_x)\nscore=hamming_score(ydev_enc,y_pred)\nloss=hamming_loss(ydev_enc,y_pred)\nprint(\"hamming score\",score)\nprint(\"hamming loss\",loss)\nprint(classification_report(ydev_enc,y_pred))\n\nprint(\"plot learning curve\")\nplot_learning_curve(GS_mlp.best_estimator_, \"MLP (Shallow)\", train_x,y_enc, cv=cv)","09a06482":"#test data\ntest_pred=GS_mlp.best_estimator_.predict(test_x)\nplot_test_res(testdf, test_pred)","d201aa05":"dl_model = Sequential()\ndl_model.add(Dense(54))\ndl_model.add(Dense(32,activation='relu'))\ndl_model.add(Dense(16,activation='relu'))\ndl_model.add(Dense(11,activation='sigmoid'))\ndl_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\nhistory = dl_model.fit(train_x, y_enc,validation_split=0.34,shuffle=True,epochs=100,batch_size=100,verbose=0)\nprint(\"best estimator parameters\",dl_model.summary())\n\nprint(\"training data evaluation\")\ny_pred=dl_model.predict(train_x)\ny_pred = y_pred.round()\nscore=hamming_score(y_enc,y_pred)\nloss=hamming_loss(y_enc,y_pred)\nprint(\"hamming score\",score)\nprint(\"hamming loss\",loss)\nprint(classification_report(y_enc,y_pred))\n\nprint(\"development data evaluation\")\ny_pred=dl_model.predict(dev_x)\ny_pred = y_pred.round()\nloss=hamming_loss(ydev_enc,y_pred)\nprint(\"hamming score\",score)\nprint(\"hamming loss\",loss)\nprint(classification_report(ydev_enc,y_pred))\n\nprint(\"plot learning curve\")\n#  \"Accuracy\"\nplt.plot(history.history['accuracy'])\nplt.plot(history.history['val_accuracy'])\nplt.title('model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'validation'], loc='upper left')\nplt.show()\n# \"Loss\"\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'validation'], loc='upper left')\nplt.show()","263ccfad":"#test data\ntest_pred=dl_model.predict(test_x).round()\nplot_test_res(testdf, test_pred)","9f4f7647":"\ntrain_2x=np.expand_dims(train_x,axis=2)\ndev_2x=np.expand_dims(dev_x,axis=2)\ntest_2x=np.expand_dims(test_x,axis=2)\n\nn_timesteps,n_features=train_2x.shape[1],train_2x.shape[2]\n\ncnn_dl_model = Sequential()\ncnn_dl_model.add(Conv1D(filters=11,kernel_size=2,activation='relu', input_shape=(n_timesteps,n_features)))\ncnn_dl_model.add(AveragePooling1D(pool_size=2))\ncnn_dl_model.add(Flatten())\n\ncnn_dl_model.add(Dense(128,activation='relu'))\ncnn_dl_model.add(Dense(64,activation='relu'))\ncnn_dl_model.add(Dense(32,activation='relu'))\ncnn_dl_model.add(Dense(16,activation='relu'))\ncnn_dl_model.add(Dense(11,activation='sigmoid'))\ncnn_dl_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n\nhistory = cnn_dl_model.fit(train_2x, y_enc,validation_split=0.34,shuffle=True,epochs=100,batch_size=100,verbose=0)\ncnn_dl_model.summary()\n\nprint(\"training data evaluation\")\ny_pred=cnn_dl_model.predict(train_2x)\ny_pred = y_pred.round()\nscore=hamming_score(y_enc,y_pred)\nloss=hamming_loss(y_enc,y_pred)\nprint(\"hamming score\",score)\nprint(\"hamming loss\",loss)\nprint(classification_report(y_enc,y_pred))\n\nprint(\"development data evaluation\")\ny_pred=cnn_dl_model.predict(dev_2x)\ny_pred = y_pred.round()\nloss=hamming_loss(ydev_enc,y_pred)\nprint(\"hamming score\",score)\nprint(\"hamming loss\",loss)\nprint(classification_report(ydev_enc,y_pred))\n\nprint(\"plot learning curve\")\n#  \"Accuracy\"\nplt.plot(history.history['accuracy'])\nplt.plot(history.history['val_accuracy'])\nplt.title('model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'validation'], loc='upper left')\nplt.show()\n# \"Loss\"\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'validation'], loc='upper left')\nplt.show()","312a42a3":"#test data\ntest_pred=cnn_dl_model.predict(test_2x).round()\nplot_test_res(testdf, test_pred)","46ef2eb0":"def avg_predict(X,x2d):\n    nb_pred = GS_gnb.predict_proba(X)\n    knn_pred = GS_knn.predict_proba(X)\n    lr_pred = GS_lr.predict_proba(X)\n    svm_pred = GS_svm.predict_proba(X)\n    mlp_pred = GS_mlp.predict_proba(X)\n    dl_pred = dl_model.predict(X)\n    cnn_dl_pred = cnn_dl_model.predict(x2d)\n    avg_pred=(nb_pred.toarray()+knn_pred.toarray()+lr_pred.toarray()+svm_pred.toarray()+mlp_pred + dl_pred + cnn_dl_pred)\/7\n    avg_pred[avg_pred>=0.5]=1\n    avg_pred[avg_pred<0.5]=0\n    \n    return avg_pred","13e5a99a":"def voting_predict(X,x2d):\n    nb_pred = GS_gnb.predict(X).toarray()\n    knn_pred = GS_knn.predict(X).toarray()\n    lr_pred = GS_lr.predict(X).toarray()\n    svm_pred = GS_svm.predict(X).toarray()\n    mlp_pred = GS_mlp.predict(X)\n    dl_pred = dl_model.predict(X).round()\n    cnn_dl_pred = cnn_dl_model.predict(x2d).round()\n    voting_pred=[]\n    arr=[]\n    \n    for row in range(0,X.shape[0]):\n        arr=[]\n        for column in range(0,len(em)):\n            arr.append(statistics.mode([nb_pred[row,column],knn_pred[row,column],lr_pred[row,column],svm_pred[row,column],mlp_pred[row,column],\n                                      dl_pred[row,column],cnn_dl_pred[row,column]]))\n        voting_pred.append(arr)\n    voting_pred=np.array(voting_pred,dtype=float)\n    \n    return voting_pred","5a53d3e6":"#training data voting\ntrain_predict = voting_predict(train_x,train_2x)\n\nprint(\"Voting ensemble technique hamming score:\",hamming_score(y_enc,train_predict))\nprint(\"Voting ensemble technique hamming loss:\",hamming_loss(y_enc,train_predict))\nprint(\"Voting ensemble classification matrix:\",classification_report(y_enc,train_predict))\n\n#training data average\ntrain_predict = avg_predict(train_x,train_2x)\n\nprint(\"Avg ensemble technique hamming score:\",hamming_score(y_enc,train_predict))\nprint(\"Avg ensemble technique hamming loss:\",hamming_loss(y_enc,train_predict))\nprint(\"Avg ensemble classification matrix:\",classification_report(y_enc,train_predict))","ae92ac50":"#development data voting\ndev_predict = voting_predict(dev_x,dev_2x)\n\nprint(\"Voting ensemble technique hamming score:\",hamming_score(ydev_enc,dev_predict))\nprint(\"Voting ensemble technique hamming loss:\",hamming_loss(ydev_enc,dev_predict))\nprint(\"Voting ensemble classification matrix:\",classification_report(ydev_enc,dev_predict))\n\n#development data average\ndev_predict = avg_predict(dev_x,dev_2x)\n\nprint(\"Avg ensemble technique hamming score:\",hamming_score(ydev_enc,dev_predict))\nprint(\"Avg ensemble technique hamming loss:\",hamming_loss(ydev_enc,dev_predict))\nprint(\"Avg ensemble classification matrix:\",classification_report(ydev_enc,dev_predict))","422a422d":"#test data - voting ensemble\ntest_predict = voting_predict(test_x,test_2x)\ntest_submit(test_predict,\"voting\")\nplot_test_res(testdf, test_predict)","1abd9a9c":"#test data - average ensemble\ntest_predict=avg_predict(test_x,test_2x)\nplot_test_res(testdf, test_predict)\ntest_submit(test_predict,\"avg\")","9fd1c799":"#naive bayes showed best results with full feature vector to classify tweets between 11 emotion classes\n#testing data(nb)\ntest_predict = GS_gnb.best_estimator_.predict(test_x).toarray()\ntest_submit(test_predict,\"nb\")","c76cf855":"Tweets are multi label classified into eleven emotions: (anger,anticipation, disgust, fear, joy, love, optimism, pessimism, sadness, surprise, trust)","89cd4949":"limited feature vector \nfeature_vector = [\"freq_anger\",\"bi_anger\",\"doc2vec_anger\",\n               \"freq_anticipation\",\"bi_anticipation\",\"doc2vec_anticipation\", \n               \"freq_disgust\",\"bi_disgust\",\"doc2vec_disgust\",\n               \"freq_fear\",\"bi_fear\",\"doc2vec_fear\", \n               \"freq_joy\",\"bi_joy\",\"doc2vec_joy\",  \n               \"freq_love\", \"bi_love\", \"doc2vec_love\",\n               \"freq_optimism\", \"bi_optimism\", \"doc2vec_optimism\",\n               \"freq_pessimism\", \"bi_pessimism\", \"doc2vec_pessimism\",\n               \"freq_sadness\",\"bi_sadness\",\"doc2vec_sadness\", \n               \"freq_surprise\",\"bi_surprise\",\"doc2vec_surprise\",\n               \"freq_trust\",\"bi_trust\",\"doc2vec_trust\",\n                  \"lex_Positive\",\"lex_Negative\",\"V\",\"D\",\"A\"]","4d0c8f7a":"# Voting and Average Ensemble Techniques","eac0593e":"# Problem Transformation Techniques","58fdfe33":"# SemEval2018 TaskE-c Multi emotion tweets.","fc23086a":"# Model Building & Training","73c2a3f5":"This notebook utilize the extracted features and apply different algorithms to build predictive classification models.","8010c976":"* These tweets were previously preprocessed in https:\/\/www.kaggle.com\/sohaelshafey\/multi-label-classification-tweets-preprocessing\n* Then, Feature extraction using unigram and bigram and expressed in https:\/\/www.kaggle.com\/sohaelshafey\/tweets-feature-extraction-unigram-bigram\n* Then, Feature extraction using cosine similarity and doc2vec in https:\/\/www.kaggle.com\/sohaelshafey\/multi-label-tweets-feature-extraction-doc2vec\n* Then, Feature extraction using NRC lexicons in https:\/\/www.kaggle.com\/sohaelshafey\/multi-label-tweets-feature-extraction-lexicon , https:\/\/www.kaggle.com\/sohaelshafey\/multi-label-tweets-feature-extraction-lexicon2 , and https:\/\/www.kaggle.com\/sohaelshafey\/multi-label-tweets-feature-extraction-lexicon-3","5465fab8":"Transforms a multi-label classification problem with L labels into L single-label separate binary classification problems using the same base classifier provided in the constructor. The prediction output is the union of all per label classifiers","2607aed6":"Full feature vector\nfeature_vector = [\"freq_anger\",\"bi_anger\",\"doc2vec_anger\",\"lex_Anger\",\"hlex_anger\",\n               \"freq_anticipation\",\"bi_anticipation\",\"doc2vec_anticipation\", \"lex_Anticipation\", \"hlex_anticipation\",\n               \"freq_disgust\",\"bi_disgust\",\"doc2vec_disgust\",\"lex_Disgust\", \"hlex_disgust\",\n               \"freq_fear\",\"bi_fear\",\"doc2vec_fear\", \"lex_Fear\", \"hlex_fear\",\n               \"freq_joy\",\"bi_joy\",\"doc2vec_joy\",  \"lex_Joy\",\"hlex_joy\",\n               \"freq_love\", \"bi_love\", \"doc2vec_love\",\n               \"freq_optimism\", \"bi_optimism\", \"doc2vec_optimism\",\n               \"freq_pessimism\", \"bi_pessimism\", \"doc2vec_pessimism\",\n               \"freq_sadness\",\"bi_sadness\",\"doc2vec_sadness\", \"lex_Sadness\",  \"hlex_sadness\",\n               \"freq_surprise\",\"bi_surprise\",\"doc2vec_surprise\",\"lex_Surprise\", \"hlex_surprise\",\n               \"freq_trust\",\"bi_trust\",\"doc2vec_trust\", \"lex_Trust\", \"hlex_trust\",\n                  \"lex_Positive\",\"lex_Negative\",\"V\",\"D\",\"A\"]\n","83bd6274":"# Load Data","b6ae296b":"# Define Model features","f872c061":"**Model Input**\n* tweets feature vector consists of : n-gram features, doc2vec features, VAD lexicon,  and negative and positive lexicon features\n**Model Output**\n* multi labelled 11 emotion class"}}