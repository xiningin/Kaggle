{"cell_type":{"492b9e6a":"code","4510ac12":"code","203436cd":"code","9fc5ea9d":"code","2751393a":"code","6eb16622":"code","38aea035":"code","25a44a47":"code","cc8629e8":"code","78200ecf":"code","bd3af415":"code","979045a0":"code","6395e48c":"code","85ea4a64":"code","6d7a6bc7":"code","2b77c085":"code","02dbcd37":"code","38fad6ec":"code","a356b358":"code","b40aeda5":"code","cc32532d":"code","a219ae47":"code","557871d0":"code","c5e1a844":"markdown","cd48caa4":"markdown","30284de5":"markdown","474881fe":"markdown","738e264e":"markdown","2b8a862a":"markdown","931597bf":"markdown","fcfcd950":"markdown"},"source":{"492b9e6a":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nimport os\n'''\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n'''\n\n# Any results you write to the current directory are saved as output.\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset,DataLoader\nfrom torchvision import transforms,models\nfrom tqdm import tqdm_notebook as tqdm","4510ac12":"train = pd.read_csv('\/kaggle\/input\/bengaliai-cv19\/train.csv')\ndata0 = pd.read_feather('\/kaggle\/usr\/lib\/resize_and_load_with_feather_format_much_faster\/train_data_0.feather')\ndata1 = pd.read_feather('\/kaggle\/usr\/lib\/resize_and_load_with_feather_format_much_faster\/train_data_1.feather')\ndata2 = pd.read_feather('\/kaggle\/usr\/lib\/resize_and_load_with_feather_format_much_faster\/train_data_2.feather')\ndata3 = pd.read_feather('\/kaggle\/usr\/lib\/resize_and_load_with_feather_format_much_faster\/train_data_3.feather')","203436cd":"ls \/kaggle\/usr\/lib\/resize_and_load_with_feather_format_much_faster\/","9fc5ea9d":"data_full = pd.concat([data0,data1,data2,data3],ignore_index=True)","2751393a":"class GraphemeDataset(Dataset):\n    def __init__(self,df,label,_type='train'):\n        self.df = df\n        self.label = label\n    def __len__(self):\n        return len(self.df)\n    def __getitem__(self,idx):\n        label1 = self.label.vowel_diacritic.values[idx]\n        label2 = self.label.grapheme_root.values[idx]\n        label3 = self.label.consonant_diacritic.values[idx]\n        image = self.df.iloc[idx][1:].values.reshape(64,64).astype(np.float)\n        return image,label1,label2,label3","6eb16622":"class ResidualBlock(nn.Module):\n    def __init__(self,in_channels,out_channels,stride=1,kernel_size=3,padding=1,bias=False):\n        super(ResidualBlock,self).__init__()\n        self.cnn1 =nn.Sequential(\n            nn.Conv2d(in_channels,out_channels,kernel_size,stride,padding,bias=False),\n            nn.BatchNorm2d(out_channels),\n            nn.ReLU(True)\n        )\n        self.cnn2 = nn.Sequential(\n            nn.Conv2d(out_channels,out_channels,kernel_size,1,padding,bias=False),\n            nn.BatchNorm2d(out_channels)\n        )\n        if stride != 1 or in_channels != out_channels:\n            self.shortcut = nn.Sequential(\n                nn.Conv2d(in_channels,out_channels,kernel_size=1,stride=stride,bias=False),\n                nn.BatchNorm2d(out_channels)\n            )\n        else:\n            self.shortcut = nn.Sequential()\n            \n    def forward(self,x):\n        residual = x\n        x = self.cnn1(x)\n        x = self.cnn2(x)\n        x += self.shortcut(residual)\n        x = nn.ReLU(True)(x)\n        return x","38aea035":"class ResNet18(nn.Module):\n    def __init__(self):\n        super(ResNet18,self).__init__()\n        \n        self.block1 = nn.Sequential(\n            nn.Conv2d(1,64,kernel_size=2,stride=2,padding=3,bias=False),\n            nn.BatchNorm2d(64),\n            nn.ReLU(True)\n        )\n        \n        self.block2 = nn.Sequential(\n            nn.MaxPool2d(1,1),\n            ResidualBlock(64,64),\n            ResidualBlock(64,64,2)\n        )\n        \n        self.block3 = nn.Sequential(\n            ResidualBlock(64,128),\n            ResidualBlock(128,128,2)\n        )\n        \n        self.block4 = nn.Sequential(\n            ResidualBlock(128,256),\n            ResidualBlock(256,256,2)\n        )\n        self.block5 = nn.Sequential(\n            ResidualBlock(256,512),\n            ResidualBlock(512,512,2)\n        )\n        \n        self.avgpool = nn.AvgPool2d(2)\n        # vowel_diacritic\n        self.fc1 = nn.Linear(512,11)\n        # grapheme_root\n        self.fc2 = nn.Linear(512,168)\n        # consonant_diacritic\n        self.fc3 = nn.Linear(512,7)\n        \n    def forward(self,x):\n        x = self.block1(x)\n        x = self.block2(x)\n        x = self.block3(x)\n        x = self.block4(x)\n        x = self.block5(x)\n        x = self.avgpool(x)\n        x = x.view(x.size(0),-1)\n        x1 = self.fc1(x)\n        x2 = self.fc2(x)\n        x3 = self.fc3(x)\n        return x1,x2,x3","25a44a47":"class ResNet34(nn.Module):\n    def __init__(self):\n        super(ResNet34,self).__init__()\n        \n        self.block1 = nn.Sequential(\n            nn.Conv2d(1,64,kernel_size=2,stride=2,padding=3,bias=False),\n            nn.BatchNorm2d(64),\n            nn.ReLU(True)\n        )\n        \n        self.block2 = nn.Sequential(\n            nn.MaxPool2d(1,1),\n            ResidualBlock(64,64),\n            ResidualBlock(64,64,2)\n        )\n        \n        self.block3 = nn.Sequential(\n            ResidualBlock(64,128),\n            ResidualBlock(128,128,2)\n        )\n        \n        self.block4 = nn.Sequential(\n            ResidualBlock(128,256),\n            ResidualBlock(256,256,2)\n        )\n        self.block5 = nn.Sequential(\n            ResidualBlock(256,512),\n            ResidualBlock(512,512,2)\n        )\n        \n        self.avgpool = nn.AvgPool2d(2)\n        # vowel_diacritic\n        self.fc1 = nn.Linear(512,11)\n        # grapheme_root\n        self.fc2 = nn.Linear(512,168)\n        # consonant_diacritic\n        self.fc3 = nn.Linear(512,7)\n        \n    def forward(self,x):\n        x = self.block1(x)\n        x = self.block2(x)\n        x = self.block3(x)\n        x = self.block4(x)\n        x = self.block5(x)\n        x = self.avgpool(x)\n        x = x.view(x.size(0),-1)\n        x1 = self.fc1(x)\n        x2 = self.fc2(x)\n        x3 = self.fc3(x)\n        return x1,x2,x3","cc8629e8":"#!pip install torchsummary\n#from torchsummary import summary","78200ecf":"device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")","bd3af415":"#model_resnet34 = ResNet34().to(device)\n#summary(model_resnet34, (1, 64, 64))","979045a0":"model = ResNet34().to(device)\noptimizer = optimizer = torch.optim.Adam(model.parameters(), lr=4e-4)\n#scheduler = torch.optim.lr_scheduler.CyclicLR(optimizer, base_lr=1e-4, max_lr=0.05)\ncriterion = nn.CrossEntropyLoss()\nbatch_size=32","6395e48c":"epochs = 50 # original 50\nmodel.train()\nlosses = []\naccs = []\nfor epoch in range(epochs):\n    reduced_index =train.groupby(['grapheme_root', 'vowel_diacritic', 'consonant_diacritic']).apply(lambda x: x.sample(5)).image_id.values\n    reduced_train = train.loc[train.image_id.isin(reduced_index)]\n    reduced_data = data_full.loc[data_full.image_id.isin(reduced_index)]\n    train_image = GraphemeDataset(reduced_data,reduced_train)\n    train_loader = torch.utils.data.DataLoader(train_image,batch_size=batch_size,shuffle=True)\n    \n    print('epochs {}\/{} '.format(epoch+1,epochs))\n    running_loss = 0.0\n    running_acc = 0.0\n    for idx, (inputs,labels1,labels2,labels3) in tqdm(enumerate(train_loader),total=len(train_loader)):\n        inputs = inputs.to(device)\n        labels1 = labels1.to(device)\n        labels2 = labels2.to(device)\n        labels3 = labels3.to(device)\n        \n        optimizer.zero_grad()\n        outputs1,outputs2,outputs3 = model(inputs.unsqueeze(1).float())\n        loss1 = criterion(outputs1,labels1)\n        loss2 = criterion(outputs2,labels2)\n        loss3 = criterion(outputs3,labels3)\n        running_loss += loss1+loss2+loss3\n        running_acc += (outputs1.argmax(1)==labels1).float().mean()\n        running_acc += (outputs2.argmax(1)==labels2).float().mean()\n        running_acc += (outputs3.argmax(1)==labels3).float().mean()\n        (loss1+loss2+loss3).backward()\n        optimizer.step()\n    #scheduler.step()\n    losses.append(running_loss\/len(train_loader))\n    accs.append(running_acc\/(len(train_loader)*3))\n    print('acc : {:.2f}%'.format(running_acc\/(len(train_loader)*3)))\n    print('loss : {:.4f}'.format(running_loss\/len(train_loader)))\ntorch.save(model.state_dict(), 'resnet34_50epochs_saved_weights.pth')","85ea4a64":"import matplotlib.pyplot as plt\nfig,ax = plt.subplots(1,2,figsize=(15,5))\nax[0].plot(losses)\nax[0].set_title('loss')\nax[1].plot(accs)\nax[1].set_title('acc')","6d7a6bc7":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.\nimport cv2\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset,DataLoader\nfrom torchvision import transforms,models\nfrom tqdm import tqdm_notebook as tqdm","2b77c085":"test = pd.read_csv('\/kaggle\/input\/bengaliai-cv19\/test.csv')","02dbcd37":"class GraphemeDataset(Dataset):\n    def __init__(self,df,_type='train'):\n        self.df = df\n    def __len__(self):\n        return len(self.df)\n    def __getitem__(self,idx):\n        image = self.df.iloc[idx][1:].values.reshape(64,64).astype(float)\n        return image","38fad6ec":"device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\nmodel = ResNet34().to(device)\nmodel.load_state_dict(torch.load('\/kaggle\/input\/trained400\/resnet34_400epochs_saved_weights.pth'))","a356b358":"def Resize(df,size=64):\n    resized = {} \n    df = df.set_index('image_id')\n    for i in tqdm(range(df.shape[0])):\n        image = cv2.resize(df.loc[df.index[i]].values.reshape(137,236),(size,size))\n        resized[df.index[i]] = image.reshape(-1)\n    resized = pd.DataFrame(resized).T.reset_index()\n    resized.columns = resized.columns.astype(str)\n    resized.rename(columns={'index':'image_id'},inplace=True)\n    return resized","b40aeda5":"model.eval()\ntest_data = ['test_image_data_0.parquet','test_image_data_1.parquet','test_image_data_2.parquet','test_image_data_3.parquet']\npredictions = []\nbatch_size=1\nfor fname in test_data:\n    data = pd.read_parquet(f'\/kaggle\/input\/bengaliai-cv19\/{fname}')\n    data = Resize(data)\n    test_image = GraphemeDataset(data)\n    test_loader = torch.utils.data.DataLoader(test_image,batch_size=1,shuffle=False)\n    with torch.no_grad():\n        for idx, (inputs) in tqdm(enumerate(test_loader),total=len(test_loader)):\n            inputs.to(device)\n            \n            outputs1,outputs2,outputs3 = model(inputs.unsqueeze(1).float().cuda())\n            predictions.append(outputs3.argmax(1).cpu().detach().numpy())\n            predictions.append(outputs2.argmax(1).cpu().detach().numpy())\n            predictions.append(outputs1.argmax(1).cpu().detach().numpy())","cc32532d":"submission = pd.read_csv('\/kaggle\/input\/bengaliai-cv19\/sample_submission.csv')","a219ae47":"submission.target = np.hstack(predictions)\nsubmission.head(10)","557871d0":"submission.to_csv('submission.csv',index=False)","c5e1a844":"## Remarks\n\n- See previous version for torch summary as submission must not have internet enabled","cd48caa4":"# Save Results","30284de5":"## Training Model\n\n- We use ResNet34 and train for 50 epochs","474881fe":"# Part 2","738e264e":"## ResNet34 Model","2b8a862a":"## ResNet-18 Model","931597bf":"# ResNet-34 PyTorch Starter Kit\n\n- Updates:\n  - added result for training with 400 epochs\n\n\n- References (ResNet):\n  - https:\/\/github.com\/pytorch\/vision\/blob\/master\/torchvision\/models\/resnet.py\n  - https:\/\/arxiv.org\/pdf\/1512.03385.pdf\n  \n  \n- Acknowledgements:\n  - Original kernels: https:\/\/www.kaggle.com\/hanjoonchoe\/grapheme-resnet-18-n-l-inference-lb-0-8566 and https:\/\/www.kaggle.com\/hanjoonchoe\/grapheme-resnet-18-naive-learning-3\n  \n  \n- Kindly upvote the kernel if you found it helpful, including the original author's!","fcfcd950":"# Part 1"}}