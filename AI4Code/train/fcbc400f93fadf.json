{"cell_type":{"ca639312":"code","163dc906":"code","c4f2e785":"code","24ed89b6":"code","69449b8c":"code","a5f227e0":"code","bc3d2ce5":"code","a6c06ab3":"code","dfeae9fb":"code","a9851e01":"code","c7cd1d56":"code","3be74bec":"code","a21f7df9":"code","79ae9600":"markdown","c63af7cd":"markdown","c65d0644":"markdown","1b789746":"markdown","cf591888":"markdown","0db29f2f":"markdown","28ce24b7":"markdown","feb0bb38":"markdown"},"source":{"ca639312":"import numpy as np \nimport pandas as pd \n\ndebug = False","163dc906":"score_threshold = 4\nread_file = open('\/kaggle\/input\/netflix-prize-data\/combined_data_1.txt')\nmovie_table = []  # A list of dictionaries with the same set of keys: \n                  # Movie_id, list of users who liked it, number of likes.\nuser_dict = {}  # key=user_id, value = movies the user liked.\ntrain_list = []  # A list of (moive, user) pairs.\nexception = 0\nfor line in read_file:\n    l = line.rstrip(':\\n').split(',')\n    if len(l)==1:\n        if debug and l[0]=='100':\n            break\n        movie_table += [\n            {'movie': 'm' + l[0],\n             'user_like': [],\n             'like_count': 0\n            }\n        ]\n    elif len(l)==3:\n        user_id, score, _ = l\n        if int(score)>=score_threshold:\n            movie_table[-1]['user_like']+=['u' + user_id]\n            movie_table[-1]['like_count']+=1\n            train_list += [\n                [movie_table[-1]['movie'], 'u' + user_id]\n            ]\n            if ('u'+user_id) in user_dict:\n                user_dict['u' + user_id] += [movie_table[-1]['movie']]\n            else:\n                user_dict['u' + user_id] = [movie_table[-1]['movie']]\n    else:\n        exception += 1\nread_file.close()\nprint(exception)","c4f2e785":"from gensim.models import Word2Vec\n\nemb_dim = 4  # Only 4k movies.  We can try to increase this up to 10 but won't need too many dimensions.\nmodel = Word2Vec(sentences=train_list, vector_size=emb_dim, \n                 window=2, min_count=1, workers=4)\n\ndef derive_user_vec(user, model, mapping):\n    \"\"\"\n    (str, func, dict) -> np.array\n    Maps a user into the vector sum of all movies they like.\n    \"\"\"\n    movie_list = [\n        model.wv[m] \/ np.linalg.norm(model.wv[m])\n        for m in mapping[user]\n    ]\n    user_vec = sum(movie_list)\n    user_vec = user_vec\/np.linalg.norm(user_vec)\n    return user_vec\n\nuser_vec_dict = {u: derive_user_vec(u, model, mapping=user_dict) for u in user_dict.keys()}","24ed89b6":"sim_thres = 0.001  # Cosine distance larger than (1-this) means too close.\narea_power = 1  # A point too close to n other points are discounted by (n+1)**(-area_power)\n                # area_power=1 leads to a good approximation of area covered by points of sim_thres radius. \ndef diversity(user_list, model, user_vec_dict, batch_size=10000):\n    \"\"\"\n    (list, func, dict, int) -> float\n    Calculate the diversity score (number of different types of users),\n    approximated by the size of area spanned by the vectors in the user_list.\n    The batch mode prevents out-of-memory problem.\n    \"\"\"\n    if user_list == []:\n        return 0\n    vec_list = [ user_vec_dict[u]\n        for u in user_list\n    ]\n    l = len(vec_list)\n    vec_list = np.array(vec_list)\n    batch = 0\n    total = 0\n    while (batch+1)*batch_size < l:\n        v_list = vec_list[batch*batch_size:(batch+1)*batch_size]\n        cross = np.matmul(v_list, vec_list.transpose())\n        overlaps = (cross>(1-sim_thres)).sum(axis=1)\n        total += (1\/overlaps**(area_power)).sum()\n        batch += 1\n    v_list = vec_list[batch*batch_size:]\n    cross = np.matmul(v_list, vec_list.transpose())\n    overlaps = (cross>(1-sim_thres)).sum(axis=1)\n    total += (1\/overlaps**(area_power)).sum()\n    return total","69449b8c":"for m in movie_table:\n    d = diversity(m['user_like'], model, user_vec_dict)\n    m['diversity'] = d\n    #print(m['movie'], m['like_count'], m['diversity'])","a5f227e0":"from sklearn.metrics import confusion_matrix\nimport seaborn as sn\nimport matplotlib.pyplot as plt\n\nmovie_df = pd.DataFrame(movie_table, columns=['movie', 'like_count', 'diversity'])\nmovie_df['like_decile'] = pd.qcut(movie_df['like_count'],10, labels=False)\nmovie_df['dvst_decile'] = pd.qcut(movie_df['diversity'],10, labels=False)\n\ncm = confusion_matrix(movie_df['like_decile'], movie_df['dvst_decile'])\nplt.figure(figsize = (10,7))\nsn.heatmap(cm, annot=True, fmt='g')","bc3d2ce5":"movie_df.groupby('like_decile').mean()[['like_count', 'diversity']]","a6c06ab3":"import random\n\nrobot_count = 1000\ntarget_count = 10\nother_count = 0\nif debug:\n    robot_count = 5\n    target_count = 3\n    other_count = 3\ntargets = movie_df.loc[movie_df['like_decile']==2] \\\n    .loc[movie_df['dvst_decile']==2]['movie'][:target_count].to_list()\nrobots = ['r'+str(i) for i in range(robot_count)]\nrobot_dict = {}\nmovie_extra_likes = {}\nfor r in robots:\n    like_list = targets + [m['movie'] for m in random.sample(movie_table, other_count)\n                          if m['movie'] not in targets]\n    robot_dict[r] = like_list\n    for m in like_list:\n        if m in movie_extra_likes.keys():\n            movie_extra_likes[m] += [r]\n        else:\n            movie_extra_likes[m] = [r]\n        \n\nhacked_user_dict = {**robot_dict, **user_dict}\nhacked_train_list = train_list + [(m, r) for r in robots for m in robot_dict[r]]","dfeae9fb":"hacked_model = Word2Vec(\n    sentences=hacked_train_list, \n    vector_size=emb_dim, window=2, min_count=1, workers=4)\n\nhacked_user_vec_dict = {\n    u: derive_user_vec(u, hacked_model, mapping=hacked_user_dict) \n    for u in hacked_user_dict.keys()\n}","a9851e01":"for m in movie_table:\n    if m['movie'] in movie_extra_likes:\n        d = diversity(m['user_like']+movie_extra_likes[m['movie']], \n            hacked_model, user_vec_dict=hacked_user_vec_dict)\n        m['hacked_like_count'] = m['like_count'] + len(movie_extra_likes[m['movie']])\n        if debug:\n            print('hacked!')\n    else:\n        d = diversity(m['user_like'], hacked_model, user_vec_dict=hacked_user_vec_dict)\n        m['hacked_like_count'] = m['like_count']\n    m['hacked_diversity'] = d","c7cd1d56":"movie_df = pd.DataFrame(\n    movie_table, \n    columns=['movie', 'like_count', 'diversity', 'hacked_like_count', 'hacked_diversity']\n)\nmovie_df['like_decile'] = pd.qcut(movie_df['like_count'],10, labels=False)\nmovie_df['dvst_decile'] = pd.qcut(movie_df['diversity'],10, labels=False)\nmovie_df['hacked_like_decile'] = pd.qcut(movie_df['hacked_like_count'],10, labels=False)\nmovie_df['hacked_dvst_decile'] = pd.qcut(movie_df['hacked_diversity'],10, labels=False)\nmovie_df.loc[movie_df['movie'].isin(targets)].head(target_count)","3be74bec":"cm_like = confusion_matrix(movie_df['hacked_like_decile'], movie_df['like_decile'])\nplt.figure(figsize = (10,7))\nsn.heatmap(cm_like, annot=True, fmt='g')","a21f7df9":"cm_dvst = confusion_matrix(movie_df['hacked_dvst_decile'], movie_df['dvst_decile'])\nplt.figure(figsize = (10,7))\nsn.heatmap(cm_dvst, annot=True, fmt='g')","79ae9600":"### Result Two: When they are not correlated, diversity metric can be better.\nNext, we are going to artificially add something to the data to create a situation in which the two metrics differ.  This can also represent a realistic situation in which someone tries to hack the Netflix system by creating 1000 accounts and use them to boost the score of 10 particular movies.\n\nFor easier visualization, we are going to choose 10 movies which are originally from decile 2 in both metrics, which means they are pretty bad.  According to the above table, adding 1000 likes should boost their popularity metric to the 7th decile.  \n\nWe will see that the diversity metric, on the other hand, is less affected by such change. It is not hard to understand since these 1000 accounts behave very similarly.  Therefore, their contribution to the diversity metric suffers more penalty than the accounts from regular people.\n\nIf the only thing the fake accounts do is to like those 10 target movies, one might think it is too trivial for the diversity metric to work (it's almost mathematically guaranteed by our design).  Therefore, we add random behavior to those fake accounts.  In addition to always liking the 10 target movies, each of them will also randomly choose about 200 other movies to like.  We can see that the diversit metric is still very resilient to this attack.","c63af7cd":"# Introduction:\nWe will try to define a \"diversity metric\" as a comparison to the conventional \"popularity metric\".\n#### Popularity: How many people like it?  How many followers do you have?  How many user watched this?\n#### Diversity: Instead of how many people, we want to count \"how many **different types** of people\".\nWe are not going to define \"types\" with self-reported, explicit categories, such as gender, race, etc.\nWe are going to define type in an abstract way derived from the user behavior.\n# Data:\nWe will be using the famous Netflix competition data which consists of users giving scores 1-5 to movies.  In this demo, we will only use 1\/4 of the original data (the first file).  We will also simplify it by taking only scores 4 and 5.  We will treat a score of 4 or 5 as \"this user likes this movie.\"  We can then define the straightforward popularity metric for each movie as how many likes.  It serves as a good benchmark to compare with the diversity metric we are going to define.\n# Goal:\nWe would like to show that the diversity metric can be a candidate to replace the popularity metric.  \n#### Usually, they are highly correlated anyway.\n#### They are not correlated in special cases.  For example, when someone is trying to hack the system.  We will show that in such a case, the diversity metric is more robust. \nSince this is only one demo, we shouldn't draw the conclusion just yet.  We only wish to trigger more interests and encourage more people to look deeper into this space.","c65d0644":"### Diversity definition.\nAfter vectorization, we can treat every user as a point on an n-dimensional unit sphere.  The usual popularity metric is simply counting how many points there are.\nFor the diversity metric, we will first picture each point as having a finite size.  We then count the total area they cover.  If 2 points are too close to each other, their area overlaps, so their contribution need to get a discount.  In this definition, there is one obvious parameter we need to specify, that is \"how close counts as too close\".\n#### Similarity threshold: When the cosine distance between 2 points is larger than 1-x, they are too close and will suffer discount.  We are using a discrete definition for simplicity.  But one can easily generalize this to a continuous definition with a similar property.\nThere is implicitly a second parameter, which we call the area power.  When 2 points are too close, we will discount both of them by 1\/2.  When n points are too close together, all of them will be discounted by 1\/n.  This is a special case of a discount of n^(-power) where power=1.  We use the natural choice of power=1, but in principle it can be fine-tuned to observe different behavior.\n","1b789746":"Of course, this result is directly dependent on our choice of similarity threshold.  In the limit which the similarity threshold is very small, then no points will be too close to each other, and the diversity metric becomes identical to the popularity metric.  When the threshold is too large, then every point becomes similar to one another, and every movie gets the same diversity score independent of its popularity score.  We have chosen the similarity threshold to be somewhere in the middle such that the two metrics are obviously correlated, but can be different.","cf591888":"### Vectorization method.\nWe vectorize the movies and users with the help of Word2Vec package.  The training data is basically pairs of (movie, user) whenever a user likes a movie.  In the NLP terms, it's a list of 2-word-sentences.\nThere are far more users than movies, and many users only liked one or a few movies.  Since the vectors are initialized randomly and the training has to be conservative to avoid over-fitting, the movie vectors will be better trained than the user vectors.  In other words, 2 movies liked by the same set of users will have very similar vectors.  However, 2 users who liked the same set of movies can still be quite far away.\n\nFor our purpose, we would like to build on a more precise notion of similarity between users.  Therefore, we are not using the Word2Vec user vectors directly.  We will use the movie vectors, and further transform every user into the sum of the vectors of the movies they liked. Note that movie vectors are normalized before the sum, and the resulting vector is also normalized since in the end we only care about cosine distance.","0db29f2f":"We can also take a look at the average of both metrics by the popularity decile.","28ce24b7":"First we read the data and turns it into a few lists and dictionaries for later steps.","feb0bb38":"### Result One: Popularity and Diversity are highly correlated.\nWe visualize this fact by putting both metric into deciles, and show the confusion metric between their deciles.  We can clearly see that the confusion metric is concentrated along the diagonal line.  In other words, a movie with a popularity score between 70%-80% most likely also has a diveristy score between 70%-80%, and rarely outside the 60%-90% window.  The reverse is also true."}}