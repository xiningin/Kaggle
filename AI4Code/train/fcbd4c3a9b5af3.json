{"cell_type":{"f91e293d":"code","f01ed1b5":"code","6c1c3d93":"code","9fc5cda4":"code","c2398b03":"code","c946746a":"code","1e519e6e":"code","b38c7734":"code","e714ddff":"code","da0a76f1":"code","aaf4b0e0":"code","97aac25f":"code","0a6c791f":"code","a7a93215":"code","56f30ccc":"code","15bf39a8":"code","8bf59201":"code","89e3dc3a":"code","d348b2c9":"code","b0bbb3a1":"code","6a0a0665":"code","9565c625":"code","7a5860b2":"code","60ed7328":"code","a7ac50d6":"code","15a38158":"code","09bfa3f0":"code","c14f8973":"code","0d19d131":"code","46944fdc":"code","b29beadb":"code","31749ae2":"code","fd6014aa":"code","24abb2d8":"code","c17a13bc":"markdown","0e4a2635":"markdown","e5d199db":"markdown","e81eb211":"markdown","4a0fe4dd":"markdown","4bb5188a":"markdown","a0038371":"markdown","a71fde82":"markdown","e98cd17e":"markdown","366580c4":"markdown","dd53af08":"markdown","6789e71e":"markdown","faacad59":"markdown","d13d5c67":"markdown","89f4a788":"markdown","7586cbf0":"markdown","e2ed427c":"markdown","4c50c2c1":"markdown","4aecd3bf":"markdown","b703fae9":"markdown","480b48c3":"markdown"},"source":{"f91e293d":"# data analysis and wrangling\nimport pandas as pd\nimport numpy as np\nimport random as rnd\nfrom sklearn.utils import shuffle\nfrom sklearn.preprocessing import Imputer\nfrom sklearn.preprocessing import StandardScaler\nimport pickle\nimport gzip\n\n# visualization\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n# machine learning\nfrom sklearn.linear_model import LogisticRegression","f01ed1b5":"train_df = pd.read_csv('..\/input\/train.csv')\ntest_df = pd.read_csv('..\/input\/test.csv')","6c1c3d93":"train_df.shape","9fc5cda4":"#checking for duplicates\ntrain_df.drop_duplicates()\ntrain_df.shape","c2398b03":"# Checking number of rows and columns in test data\ntest_df.shape","c946746a":"# preview the data\ntrain_df.head(10)","1e519e6e":"# Target varibale distribution\nsns.countplot(x=\"target\", data=train_df)","b38c7734":"# Handling imbalanced dataset by undersampling\ndesired_apriori=0.10\n\n# Get the indices per target value\nidx_0 = train_df[train_df.target == 0].index\nidx_1 = train_df[train_df.target == 1].index\n\n# Get original number of records per target value\nrecords_0 = len(train_df.loc[idx_0])\nrecords_1 = len(train_df.loc[idx_1])\n\n# Calculate the undersampling rate and resulting number of records with target=0\nundersampling_rate = ((1-desired_apriori)*records_1)\/(records_0*desired_apriori)\nundersampled_records_0 = int(undersampling_rate*records_0)\n\n# Randomly select records with target=0 to get at the desired a priori\nundersampled_idx = shuffle(idx_0, random_state=37, n_samples=undersampled_records_0)\n\n# Construct list with remaining indices\nidx_list = list(undersampled_idx) + list(idx_1)\n\n# Return undersample data frame\ntrain_df = train_df.loc[idx_list].reset_index(drop=True)\n# Random shuffle\ntrain_df = train_df.sample(frac=1).reset_index(drop=True)\ntrain_df.shape","e714ddff":"# Frequency Distribution of each binary variable\nbin_col = [col for col in train_df.columns if '_bin' in col]\nfor feature in bin_col:\n    print (train_df[feature].value_counts())","da0a76f1":"# Dropping ps_ind_09_bin, ps_ind_10_bin, ps_ind_11_bin and ps_ind_12_bin as they are completely dominated by zeros\ntrain_df = train_df.drop(['ps_ind_10_bin', 'ps_ind_11_bin', 'ps_ind_12_bin','ps_ind_13_bin'], axis=1)\ntest_df = test_df.drop(['ps_ind_10_bin', 'ps_ind_11_bin', 'ps_ind_12_bin','ps_ind_13_bin'], axis=1)","aaf4b0e0":"# checking for missing values\nfor feature in train_df.columns:\n    missings = train_df[train_df[feature] == -1][feature].count()\n    if missings > 0:\n        print (str(feature)+\"\\t\\t\"+str(missings))","97aac25f":"# Dropping ps_car_03_cat and ps_car_05_cat as they have a large proportion of records with missing values\ntrain_df = train_df.drop(['ps_car_03_cat', 'ps_car_05_cat'], axis=1)\ntest_df = test_df.drop(['ps_car_03_cat', 'ps_car_05_cat'], axis=1)","0a6c791f":"# Replacing missing values of features other than categorical\n# Imputing with the mean or mode\nmean_imp = Imputer(missing_values=-1, strategy='mean', axis=0)\nmode_imp = Imputer(missing_values=-1, strategy='most_frequent', axis=0)\ntrain_df['ps_reg_03'] = mean_imp.fit_transform(train_df[['ps_reg_03']]).ravel()\ntrain_df['ps_car_14'] = mean_imp.fit_transform(train_df[['ps_car_14']]).ravel()\ntrain_df['ps_car_11'] = mode_imp.fit_transform(train_df[['ps_car_11']]).ravel()","a7a93215":"train_df.dtypes","56f30ccc":"train_float = train_df.select_dtypes(include=['float64'])\n# correlation matrix for float features\ncolormap = plt.cm.magma\nplt.figure(figsize=(16,12))\nplt.title('Pearson correlation of continuous features', y=1.05, size=15)\nsns.heatmap(train_float.corr(),linewidths=0.1,vmax=1.0, square=True, \n            cmap=colormap, linecolor='white', annot=True)","15bf39a8":"# plotting ps_ind_02_cat against target variable\nsns.barplot(x=\"ps_ind_02_cat\", y=\"target\", data=train_df)","8bf59201":"# plotting ps_ind_04_cat against target variable\nsns.barplot(x=\"ps_ind_04_cat\", y=\"target\", data=train_df)","89e3dc3a":"# plotting ps_ind_05_cat against target variable\nsns.barplot(x=\"ps_ind_05_cat\", y=\"target\", data=train_df)","d348b2c9":"# plotting ps_car_01_cat against target variable\nsns.barplot(x=\"ps_car_01_cat\", y=\"target\", data=train_df)","b0bbb3a1":"# plotting ps_car_02_cat against target variable\nsns.barplot(x=\"ps_car_02_cat\", y=\"target\", data=train_df)","6a0a0665":"# plotting ps_car_04_cat against target variable\nsns.barplot(x=\"ps_car_04_cat\", y=\"target\", data=train_df)","9565c625":"# plotting ps_car_06_cat against target variable\nsns.barplot(x=\"ps_car_06_cat\", y=\"target\", data=train_df)","7a5860b2":"# plotting ps_car_07_cat against target variable\nsns.barplot(x=\"ps_car_07_cat\", y=\"target\", data=train_df)","60ed7328":"# plotting ps_car_08_cat against target variable\nsns.barplot(x=\"ps_car_08_cat\", y=\"target\", data=train_df)","a7ac50d6":"# plotting ps_car_09_cat against target variable\nsns.barplot(x=\"ps_car_09_cat\", y=\"target\", data=train_df)","15a38158":"# plotting ps_car_10_cat against target variable\nsns.barplot(x=\"ps_car_10_cat\", y=\"target\", data=train_df)","09bfa3f0":"# plotting ps_car_11_cat against target variable\nsns.barplot(x=\"ps_car_11_cat\", y=\"target\", data=train_df)","c14f8973":"# Dropping irrevalent feature\ntrain_df = train_df.drop(['ps_car_10_cat'], axis=1)\ntest_df = test_df.drop(['ps_car_10_cat'], axis=1)","0d19d131":"# checking for missing values in test data\nfor feature in test_df.columns:\n    missings = test_df[test_df[feature] == -1][feature].count()\n    if missings > 0:\n        print (str(feature)+\"\\t\\t\"+str(missings))","46944fdc":"# Replacing missing values of features other than categorical\n# Imputing with the mean or mode\nmean_imp = Imputer(missing_values=-1, strategy='mean', axis=0)\nmode_imp = Imputer(missing_values=-1, strategy='most_frequent', axis=0)\ntest_df['ps_reg_03'] = mean_imp.fit_transform(test_df[['ps_reg_03']]).ravel()\ntest_df['ps_car_14'] = mean_imp.fit_transform(test_df[['ps_car_14']]).ravel()\ntest_df['ps_car_11'] = mode_imp.fit_transform(test_df[['ps_car_11']]).ravel()","b29beadb":"X_train = train_df.drop([\"target\",\"id\"], axis=1)\nY_train = train_df[\"target\"]\nX_test  = test_df.drop(\"id\", axis=1).copy()\nX_train.shape, Y_train.shape, X_test.shape","31749ae2":"# Feature Scaling\nsc = StandardScaler()\nX_train = sc.fit_transform(X_train)\nX_test = sc.transform(X_test)","fd6014aa":"# Logistic Regression\nlogreg = LogisticRegression()\nlogreg.fit(X_train, Y_train)\n# calculating the coefficient of the features\ncoeff_df = pd.DataFrame(train_df.columns.delete(0))\ncoeff_df.columns = ['Feature']\ncoeff_df[\"Correlation\"] = pd.Series(logreg.coef_[0])\ncoeff_df.sort_values(by='Correlation', ascending=False)","24abb2d8":"# Dropping less important feature from each pair of highly correlated feature  \ntrain_df = train_df.drop([\"ps_reg_03\",\"ps_car_12\",\"ps_car_15\"], axis=1)\ntest_df = test_df.drop([\"ps_reg_03\", \"ps_car_12\", \"ps_car_15\"], axis=1)\nX_train = train_df.drop([\"target\",\"id\"], axis=1)\nY_train = train_df[\"target\"]\nX_test  = test_df.drop(\"id\", axis=1).copy()\n# Feature Scaling\nsc = StandardScaler()\nX_train = sc.fit_transform(X_train)\nX_test = sc.transform(X_test)\nX_train.shape, Y_train.shape, X_test.shape","c17a13bc":"We create a correlation matrix for float features.","0e4a2635":"We drop two features due to high proportion of missing records.","e5d199db":"We will now check for missing values.","e81eb211":"Next, let's see the distribution of target classes in the training data.","4a0fe4dd":"Next, let's plot the categorical features against the target variable","4bb5188a":"There are no duplicates in the training dataset. So, we don't have to deal with that.","a0038371":"Next, we use logistic regression to determine feature importance.","a71fde82":"We will now handle missing values in test data in the same way as training data.","e98cd17e":"The test data has *58 *features i.e., the test data has all the features as training data except for the target variable which we have to predict.","366580c4":"Four binary features - *ps_ind_09_bin, ps_ind_10_bin, ps_ind_11_bin and ps_ind_12_bin* are completely dominated by zeroes. So, we will drop them.","dd53af08":"We will do feature scaling using standardization.","6789e71e":"For other features, we replace the missing value by meam or mode except categorical features. For categorical features we treat *-1 (missing value)* as an *additional category value*.","faacad59":"Next, we will move to feature selection. First, we will check the distribution of different *binary features*.","d13d5c67":"We are now ready to fit different models.","89f4a788":"My kernel is highly inspired by the top kernels for this project.","7586cbf0":"As, we can see the dataset is highly imbalanced. We wil now perform *undersampling* to deal with it. We choose *undersampling* as the training dataset is huge. ","e2ed427c":"Pair of higly correlated features: (ps_reg_02, ps_reg_03) (ps_car_12, ps_car_13) (ps_car_12, ps_car_14) (ps_car_13, ps_car_15)","4c50c2c1":"After fitting models like logistic regression, k-nearest neighbour, perceptron, gaussian Naive Bayes, decision tree, random forest, XGB classifier and artificial neural network and tuning the parameters of different models, I observed that the tuned **artificial neural network**  outperformed all other models in cross-validation. I used **three** hidden layers with **256** hidden nodes in each. I used **dropout** in each layer. I have used a batch size of **64** and number_of_epochs=**800**. ","4aecd3bf":"The Training data consists of *59* columns and *595212* rows.","b703fae9":"Hello everyone, I am relatively new to data science. I am describing the steps that I took for feature engineering in this project. Any feedback  will be highly appreciated.","480b48c3":"Will treat the higly correlated features later while fitting models"}}