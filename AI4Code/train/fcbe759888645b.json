{"cell_type":{"3cf732ae":"code","fab31211":"code","903f2a4d":"code","5798519a":"code","023ae2fe":"code","c4959e7e":"code","2fc76a94":"code","b2bd80ed":"code","c4a9e276":"code","50c65ce9":"code","1fb5ec09":"code","b36012d0":"code","cbb22e64":"code","c53a5886":"code","da8948fe":"markdown","eb05719e":"markdown","0bfba9f5":"markdown","0dfa1fad":"markdown","e0bf15c2":"markdown","4024134b":"markdown","d9014526":"markdown","dfa9b879":"markdown","e30241ea":"markdown","4bf3f8ab":"markdown","b146addc":"markdown","268f2b3c":"markdown","4cdf541d":"markdown","431bb78a":"markdown","e31bc8e0":"markdown"},"source":{"3cf732ae":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport re\nimport os\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, confusion_matrix, precision_recall_curve\nfrom catboost import Pool, CatBoostClassifier\n\nfrom scipy.stats import pearsonr, chi2_contingency\nfrom itertools import combinations\nfrom statsmodels.stats.proportion import proportion_confint\nfrom imblearn.under_sampling import RandomUnderSampler","fab31211":"# Read all the data\nall_data = pd.read_csv(\n    '..\/input\/lending-club\/accepted_2007_to_2018q4.csv\/accepted_2007_to_2018Q4.csv',\n    parse_dates=['issue_d'], infer_datetime_format=True)\n# Only process data in a 1 year period from beginning of 2018 to beginning of 2019\nall_data = all_data[(all_data.issue_d >= '2017-01-01 00:00:00') & (all_data.issue_d < '2019-01-01 00:00:00')]\nall_data.head()","903f2a4d":"# Read in the LCDataDictionary.xls file which contains all the features available to investors and their descriptions\ndataset_fields = pd.read_excel('..\/input\/lending-club-loan-data\/LCDataDictionary.xlsx',sheet_name=1)\n\n# Read in the features file\ndataset_features = dataset_fields['BrowseNotesFile'].dropna().values\n\n# Reformat the feature names to match the ones in all_data\ndataset_features = [re.sub('(?<![0-9_])(?=[A-Z0-9])', '_', x).lower().strip() for x in dataset_features]\n\n# There are some features in both datasets that mean the same thing are different words\n# are used for them. Change those feautre names accordingly\nold_name = ['is_inc_v', 'mths_since_most_recent_inq', 'mths_since_oldest_il_open',\n         'mths_since_recent_loan_delinq', 'verified_status_joint']\nnew_name = ['verification_status', 'mths_since_recent_inq', 'mo_sin_old_il_acct',\n           'mths_since_recent_bc_dlq', 'verification_status_joint']\n\ndataset_features = np.setdiff1d(dataset_features, old_name)\ndataset_features = np.append(dataset_features, new_name)\n\n# Remove all the feauture columns from all_data that are not part of dataset_features\nall_data_feaures = all_data.columns.values\navailble_features = np.intersect1d(dataset_features, all_data_feaures)\navailble_features = np.append(availble_features, ['loan_status']);\ndata_with_available_features = all_data[availble_features].copy()\n\n# Get info regarding the dataframe to confirm appropriate features are removed\ndata_with_available_features.info()","5798519a":"# Change appropriate columns to float type and remove rows where string cannot be converted to float\ndata_with_available_features['emp_length'] = data_with_available_features['emp_length'].replace({'< 1 year': '0 years', '10+ years': '11 years'})\ndata_with_available_features['emp_length'] = data_with_available_features['emp_length'].str.extract('(\\d+)').astype('float')\n\n# Change appropriate columns to datetime type\ndata_with_available_features['earliest_cr_line'] = pd.to_datetime(data_with_available_features['earliest_cr_line'], infer_datetime_format=True)\ndata_with_available_features['sec_app_earliest_cr_line'] = pd.to_datetime(data_with_available_features['sec_app_earliest_cr_line'], infer_datetime_format=True)\n\n# Remove columns with no values\ndata_with_available_features = data_with_available_features.drop(['desc', 'member_id', 'id'], axis=1, errors='ignore')\n\n# Get info regarding the dataframe to confirm appropriate features are removed\ndata_with_available_features.info()","023ae2fe":"# Define columns that need to be filled with an empty value, max, or min value\ncolumns_to_fill_empty = ['emp_title', 'verification_status_joint']\ncolumns_to_fill_max = ['bc_open_to_buy', 'mo_sin_old_il_acct', 'mths_since_last_delinq',\n            'mths_since_last_major_derog', 'mths_since_last_record',\n            'mths_since_rcnt_il', 'mths_since_recent_bc', 'mths_since_recent_bc_dlq',\n            'mths_since_recent_inq', 'mths_since_recent_revol_delinq',\n            'pct_tl_nvr_dlq','sec_app_mths_since_last_major_derog']\ncolumns_to_fill_min = np.setdiff1d(data_with_available_features.columns.values, np.append(columns_to_fill_empty, columns_to_fill_max))\n\ndata_with_available_features[columns_to_fill_empty] = data_with_available_features[columns_to_fill_empty].fillna('')\ndata_with_available_features[columns_to_fill_max] = data_with_available_features[columns_to_fill_max].fillna(data_with_available_features[columns_to_fill_max].max())\ndata_with_available_features[columns_to_fill_min] = data_with_available_features[columns_to_fill_min].fillna(data_with_available_features[columns_to_fill_min].min())","c4959e7e":"# Remove all features that only hold one value or all unqiue values\ndata_with_available_features = data_with_available_features.drop(['num_tl_120dpd_2m', 'url', 'emp_title'], axis=1, errors='ignore')\n\n# Calculate the correlation between all pairs of numeric features\n# Pearson's R correlation coefficient was used\nfeatures_with_num_type = data_with_available_features.select_dtypes('number').columns.values\ncombination_of_features_with_num_type = np.array(list(combinations(features_with_num_type, 2)))\ncorrelation_of_features_with_num_type = np.array([])\nfor comb in combination_of_features_with_num_type:\n    corr = pearsonr(data_with_available_features[comb[0]], data_with_available_features[comb[1]])[0]\n    correlation_of_features_with_num_type = np.append(correlation_of_features_with_num_type, corr)\n\n# Drop the first one in the pair with coefficient >= 0.9\nhigh_corr_num = combination_of_features_with_num_type[np.abs(correlation_of_features_with_num_type) >= 0.9]\ndata_with_available_features = data_with_available_features.drop(np.unique(high_corr_num[:, 0]), axis=1, errors='ignore')\n\n# Calculate the correlation between all pairs of categorical features\n# Cramer's V correlation coefficient was used\ncat_feat = data_with_available_features.select_dtypes('object').columns.values\ncomb_cat_feat = np.array(list(combinations(cat_feat, 2)))\ncorr_cat_feat = np.array([])\nfor comb in comb_cat_feat:\n    table = pd.pivot_table(data_with_available_features, values='loan_amnt', index=comb[0], columns=comb[1], aggfunc='count').fillna(0)\n    corr = np.sqrt(chi2_contingency(table)[0] \/ (table.values.sum() * (np.min(table.shape) - 1) ) )\n    corr_cat_feat = np.append(corr_cat_feat, corr)\n\n# Drop the second one in the pair with coefficient >= 0.9\nhigh_corr_cat = comb_cat_feat[corr_cat_feat >= 0.9]\ndata_with_available_features = data_with_available_features.drop(np.unique(high_corr_cat[:, 1]), axis=1, errors='ignore')","2fc76a94":"# Create the expected_output dataframe\nexpected_output = data_with_available_features['loan_status'].copy()\nexpected_output = expected_output.isin(['Current', 'Fully Paid', 'In Grace Period']).astype('int')","b2bd80ed":"undersampler = RandomUnderSampler(sampling_strategy='majority')\n","c4a9e276":"# remove the int_rate field since its highly correlated with the grading field and loan_status since we saved it in expected_output\ndataset = data_with_available_features.drop(['int_rate', 'loan_status'], axis=1, errors='ignore')\nexpected_results = expected_output[dataset.index]\n\n# Split the dataset and the expected_result set into training and testing\ndataset_train, dataset_test, expected_results_train, expected_results_test = train_test_split(dataset, expected_results, stratify=expected_results, random_state=0)\n\n# Further split the train datasets into training and validation dataset\ndataset_train, dataset_validate, expected_results_train, expected_results_validate = train_test_split(dataset_train, expected_results_train, stratify=expected_results_train, random_state=0)","50c65ce9":"# Transform the datasets into CatBoost objects\ncat_feat_ind = (dataset_train.dtypes == 'object').nonzero()[0]\npool_train = Pool(dataset_train, expected_results_train, cat_features=cat_feat_ind)\npool_val = Pool(dataset_validate, expected_results_validate, cat_features=cat_feat_ind)\npool_test = Pool(dataset_test, expected_results_test, cat_features=cat_feat_ind)\n\n# Build the model and fit the data\nn = expected_results_train.value_counts()\nmodel = CatBoostClassifier(learning_rate=0.03,\n                           iterations=1000,\n                           early_stopping_rounds=100,\n                           class_weights=[1, n[0] \/ n[1]],\n                           verbose=False,\n                           random_state=0)\nmodel.fit(pool_train, eval_set=pool_val, plot=True);","1fb5ec09":"# Predict the test dataset with the model\nactual_results = model.predict(pool_test)\n\n# Get the acuracy, precision (false positives), and recall (false negatives)\nacc_test = accuracy_score(expected_results_test, actual_results)\nprec_test = precision_score(expected_results_test, actual_results)\nrec_test = recall_score(expected_results_test, actual_results)\nprint(f'''Accuracy (test): {acc_test:.3f}\nPrecision (test): {prec_test:.3f}\nRecall (test): {rec_test:.3f}''')\n\n# Generate the confusion matrix\ncm = confusion_matrix(expected_results_test, actual_results)\nax = sns.heatmap(cm, cmap='viridis_r', annot=True, fmt='d', square=True)\nax.set_xlabel('Predicted')\nax.set_ylabel('True');","b36012d0":"# Plot the precision (false positives) and the recall (false negatvies). Want to maximize this graph for the precision.\nexpected_output_proba_validate = model.predict_proba(pool_val)[:, 1]\np_val, r_val, t_val = precision_recall_curve(expected_results_validate, expected_output_proba_validate)\nplt.plot(r_val, p_val)\nplt.ylabel('Precision');\nplt.xlabel('Recall')","cbb22e64":"# precision of 1 results in recall of 0 which is too low\nmax_precision = p_val[p_val != 1].max()\n\n# Find the threshold at which precision is maximized\nt_all = np.insert(t_val, 0, 0)\nt_adj_val = t_all[p_val == max_precision]\nexpected_results_adj_validate = (expected_output_proba_validate > t_adj_val).astype(int)\np_adj_val = precision_score(expected_results_validate, expected_results_adj_validate)\nprint(f'Adjusted precision (validation): {p_adj_val:.3f}')","c53a5886":"expected_results_proba_test = model.predict_proba(pool_test)[:, 1]\nexpected_results_adj_test = (expected_results_proba_test > t_adj_val).astype(int)\na_adj_test = accuracy_score(expected_results_test, expected_results_adj_test)\np_adj_test = precision_score(expected_results_test, expected_results_adj_test)\nr_adj_test = recall_score(expected_results_test, expected_results_test)\nprint(f'''Adjusted Accuracy (test): {a_adj_test:.3f}\nAdjusted Precision (test): {p_adj_test:.3f}\nAdjusted Recall (test): {r_adj_test:.3f}''')\n\ncm_test = confusion_matrix(expected_results_test, expected_results_adj_test)\nax = sns.heatmap(cm_test, cmap='viridis_r', annot=True, fmt='d', square=True)\nax.set_xlabel('Predicted')\nax.set_ylabel('True');","da8948fe":"Optimize for max precision, hence minimum false positives. ","eb05719e":"Read the whole lending club dataset. There are 151 columns but some are not needed.","0bfba9f5":"# Decision Trees","0dfa1fad":"Change the field values of some columns depending on if the missing value should be replaced with a empty, max, or min value.","e0bf15c2":"Predict the test dataset with the model without any optimizations.","4024134b":"We only want the features that are available to investors, ie. feautures known before the loan.\nA list of all feautres known to investors before the loan is provided in  LCDataDictionary.xls.\nRemove all columns from all_data that are not in this file and save in a new dataset. Should be 109 columns after filtering.","d9014526":"Delete all feautres with a very high correlation coefficient to improve model accuracies. ","dfa9b879":"Create new dataframe with 1 column which is the expected_output dataframe. 1 means loan is success, 0 means its not. Current, Fully Paid, and In Grace Period loan statuses will count as success.","e30241ea":"Plot the precision (false positives) and the recall (false negatvies). Want to maximize this graph fro the precision.","4bf3f8ab":"Since this is loan data, we want to minimize the number of false postivies, number of times a bad loan was given a approval, as they result in the investor losing money. False negatives, number of times a good loan was turned down, means the investor made the wrong decision, but doesn't end up losing any money. Hence, lets optimize for false positives to improve the model.","b146addc":"Partition the data into training, validation, and testing.","268f2b3c":"Create the model and fit the data.","4cdf541d":"# Data Preprocessing","431bb78a":"Change the types of some columns for the model and remove columns with no values.","e31bc8e0":"Rerun model with specified threshold"}}