{"cell_type":{"febc67f6":"code","10c6c818":"code","8d8ef416":"code","45b1ce31":"code","647c3930":"code","75967e0d":"code","fe8f4ef7":"code","477336f3":"code","09be24fc":"code","baf34f77":"code","d0e2e7d5":"code","dc2b3559":"code","688dbaed":"code","4ad5596c":"code","15345103":"code","a67158a0":"code","aba0e993":"code","7e629362":"code","42dc3cd2":"code","c5adad3e":"code","f5c79b4e":"code","483cb1f2":"code","9a9c8296":"code","109659e8":"markdown","a29e18dc":"markdown","a9526c41":"markdown","c584a219":"markdown","3e27ad26":"markdown","87683c1a":"markdown","0a018dc7":"markdown","77dad541":"markdown","aa1db425":"markdown","bc4a1232":"markdown","cbbb4e7b":"markdown","5198acd9":"markdown","ddfd68f9":"markdown","a54bbf0d":"markdown"},"source":{"febc67f6":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\npd.set_option('display.max_columns', None) # Setting pandas to display a N number of columns\npd.set_option('display.max_rows', 10) # Setting pandas to display a N number rows\npd.set_option('display.width', 1000) # Setting pandas dataframe display width to N\n\nimport pandas_profiling ","10c6c818":"train = pd.read_csv('..\/input\/titanic\/train.csv')\ntest = pd.read_csv('..\/input\/titanic\/test.csv')\ndataset = [train, test]\n\n# Let's get a summary of our datasets\n\nprint('Entries in training set: ', len(train), '\\nEntries in testing set: ',len(test))\n\nfor df in dataset:\n    print(df.isna().sum())\n\n# A combination of training and test dataset would be helpful in data analysis\n\ntrain_test_comb = pd.concat([train, test], axis=0)","8d8ef416":"print('\\nSurvived Class Counts in Training Set:')\nsns.countplot(x='Survived', data=train)\nplt.show()","45b1ce31":"# Visualizing the 'Age' distribution among the passengers\nsns.distplot(train_test_comb['Age'])\n\n# Replace 'Age' column missing values with mean\nfor df in dataset:\n    df['Age'].fillna(df['Age'].mean(), inplace=True)\n    df['Fare'].fillna(df['Fare'].mean(), inplace=True)\n    \n# Drop the 'Cabin' column\nfor df in dataset:\n    df.drop(['Cabin'], axis=1, inplace=True)  \n    \n    \n# Replace 'Embarked' column missing values with 'S'\nfor df in dataset:\n    df['Embarked'].fillna('S', inplace=True)","647c3930":"# Create new 'Familysize' and 'Title' column\n\nfor df in dataset:\n    df['Familysize'] = df['SibSp']+df['Parch']\n    df['Title'] = df['Name'].str.split(', ', expand=True)[1].str.split('.', expand=True)[0]\n\n    # Replace the titles that has less than 20 ocurrences with 'Misc'\n    title_names = (df['Title'].value_counts()> 10) #this will create a true false series with title name as index\n    \n    df['Title'] = df['Title'].apply(lambda x: x if title_names.loc[x] == True else 'Misc')\n\n    \n# Let's do this again as we have new features\ntrain_test_comb = pd.concat([train, test], axis=0)\n\nprint(f\"Titles: {list(train['Title'].unique())}\")\nprint(train_test_comb['Title'].value_counts())","75967e0d":"# Some barplots for the discrete features sex, embarked, passengerclass and title, with respect to survival\n\nfig, ax= plt.subplots(1,2, figsize=(16,6))\nsns.barplot(x = 'Sex', y = 'Survived', ax = ax[0], data=train)\nsns.barplot(x = 'Embarked', y = 'Survived', ax = ax[1], data=train)\n\nfig, ax= plt.subplots(1,2, figsize=(16,6))\nsns.barplot(x = 'Pclass', y = 'Survived', ax = ax[0], data=train)\nsns.barplot(x = 'Title', y = 'Survived', ax = ax[1], data=train)","fe8f4ef7":"# Boxplots for continuos features like age, fare and familysize\n\nfig, ax= plt.subplots(1,3, figsize=(25,10))\nsns.boxplot(x='Fare', orient='v', meanline = False, ax= ax[0], data=train)\nsns.boxplot(x='Age', orient='v', meanline = False, ax= ax[1], data=train)\nsns.boxplot(x='Familysize', orient='v', meanline = False, ax= ax[2], data=train)","477336f3":"# Histogram of age, fare and familysize by Survival\n\nfig, ax= plt.subplots(1, 3, figsize=(25, 8))\nsns.distplot( train[train['Survived']==1]['Age'] , kde=False, label='Survived', color='g', ax=ax[0])\nsns.distplot( train[train['Survived']==0]['Age'] , kde=False, label='Didn\\'t survive', color='r', ax=ax[0])\n\nsns.distplot(train[train['Survived']==1]['Fare'], kde=False, label='Survived', color = 'g', ax=ax[1])\nsns.distplot(train[train['Survived']==0]['Fare'], kde=False, label='Didn\\'t survive', color = 'r', ax=ax[1])\n\nsns.distplot(train[train['Survived']==1]['Familysize'], kde=False, label='Survived', color = 'g', ax=ax[2])\nsns.distplot(train[train['Survived']==0]['Familysize'], kde=False, label='Didn\\'t survive', color = 'r', ax=ax[2])\n\nplt.legend(prop={'size': 18})","09be24fc":"fig, ax = plt.subplots(1, 2,figsize=(16,6))\n\nsns.pointplot(x=\"Embarked\", y=\"Survived\", hue=\"Sex\", data=train, linestyles=[\"-\", \"--\"], ax = ax[0])\nsns.pointplot(x=\"Pclass\", y=\"Survived\", hue=\"Sex\", data=train, linestyles=[\"-\", \"--\"], ax = ax[1])","baf34f77":"for df in dataset:\n    df['Fare_cat'] = pd.qcut(df['Fare'], q=4, labels=(1,2,3,4))\n    df['Age_cat'] = pd.qcut(df['Age'], q=4, labels=(1,2,3,4))\n    \n    # lambda function to change the values of 'Familysize'\n    df['Familysize'] = df['Familysize'].apply(lambda x: 'Alone' if x==0 else('Small' if x>0 and x<5 else('Medium' if x>=5 and x<7 else 'Large')))","d0e2e7d5":"fig, ax = plt.subplots(1, 3,figsize=(18,6) )   \n\nsns.pointplot(x = 'Fare_cat', y = 'Survived',  data=train, ax = ax[0])\nsns.pointplot(x = 'Age_cat', y = 'Survived',  data=train, ax = ax[1])\nsns.pointplot(x = 'Familysize', y = 'Survived', data=train, ax = ax[2])","dc2b3559":"# Convert categorical dtypes into numerical dtypes\nfor df in dataset:\n    # Convert category dtypes to integers\n    df['Age_cat'] = df['Age_cat'].astype(np.int32)\n    df['Fare_cat'] = df['Fare_cat'].astype(np.int32)\n    \n    # Replace string values with integer values\n    df.Title.replace({'Mr':1, 'Mrs':2, 'Miss':3, 'Master':4, 'Misc':5}, inplace=True)\n    df.Sex.replace({'female':0, 'male': 1}, inplace=True)\n    df.Embarked.replace({'S':1, 'C':2, 'Q':3}, inplace=True)","688dbaed":"# One-hot encoding\nfrom sklearn.preprocessing import OneHotEncoder, LabelEncoder, StandardScaler\n\nfeatures = ['Age_cat', 'Fare_cat', 'Pclass', 'Sex', 'Embarked', 'Title', 'Familysize']\nencoded_fearures = []\n\nfor df in dataset:\n  for feature in features:\n    encoded = OneHotEncoder().fit_transform(df[feature].values.reshape(-1, 1)).toarray()\n    n = df[feature].nunique()\n    cols = [f'{feature}_{n}' for n in range(1, n + 1)]\n    encoded_df = pd.DataFrame(encoded, columns=cols)\n    encoded_df.index = df.index\n    encoded_fearures.append(encoded_df)\n\ntrain_one = pd.concat([train, *encoded_fearures[:7]], axis=1)\ntest_one = pd.concat([test, *encoded_fearures[7:]], axis=1)\n\ndataset = [train_one, test_one]","4ad5596c":"train_one","15345103":"for df in dataset:\n    print(df.columns)","a67158a0":"for df in dataset:\n  df.drop(['PassengerId', 'Pclass', 'Name', 'Sex', 'Age', 'SibSp', 'Parch', \n           'Ticket', 'Fare', 'Embarked', 'Familysize', 'Title', 'Fare_cat', 'Age_cat' ], axis=1, inplace=True)","aba0e993":"from sklearn.model_selection import train_test_split\n\nfeatures = [x for x in train_one.columns if x!='Survived']\n\nx = train_one[features].to_numpy()\ny = train_one['Survived'].to_numpy()\n\nx_train, x_val, y_train, y_val = train_test_split(x, y, train_size = int(0.95*len(train_one)), shuffle=False ,random_state=1400)\n\nprint(x_train.shape, y_train.shape, x_val.shape, y_val.shape)","7e629362":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import confusion_matrix, classification_report\n\nclf = RandomForestClassifier(criterion='gini', \n                        n_estimators=300,\n                        max_depth=4,\n                        min_samples_split=4,\n                        min_samples_leaf=7,\n                        max_features='auto',\n                        oob_score=True,\n                        random_state=1400,\n                        n_jobs=-1)\n\nclf.fit(x_train, y_train)\n\ny_pred = clf.predict(x_val)\n\ncm = confusion_matrix(y_val, y_pred)\nprint(cm)\nprint(classification_report(y_val, y_pred))","42dc3cd2":"# Prediction using RandomForest Classifier\ntest_data = test_one[features].to_numpy()\n\nprediction_clf = clf.predict(test_data)\nprint(len(prediction_clf))\n\noutput = pd.DataFrame({'PassengerId': test['PassengerId'], 'Survived': prediction_clf})\n# output.to_csv('\/kaggle\/working\/my_submission.csv', index=False)","c5adad3e":"from xgboost import XGBClassifier\n\nxgb = XGBClassifier(learning_rate=0.01, n_estimators=3000,\n                       max_depth=3, min_child_weight=0,\n                       gamma=0, subsample=0.7,\n                       colsample_bytree=0.7, nthread=-1, seed=27,\n                       reg_alpha=0.00006, random_state=42)\n\n\nxgb.fit(x, y)\n\ny_pred = xgb.predict(x_val)\n\ncm = confusion_matrix(y_val, y_pred)\nprint(cm)\nprint(classification_report(y_val, y_pred))","f5c79b4e":"# Prediction using RandomForest Classifier\ntest_data = test_one[features].to_numpy()\n\nprediction_xgb = xgb.predict(test_data)\nprint(len(prediction_xgb))\n\noutput = pd.DataFrame({'PassengerId': test['PassengerId'], 'Survived': prediction_xgb})\n# output.to_csv('\/kaggle\/working\/my_submission.csv', index=False)","483cb1f2":"# Using Tensorflow Neural Network\nimport tensorflow as tf\nimport tensorflow.keras as keras\n\nseed = 1400\n\ntf.random.set_seed(seed)\nmy_init = keras.initializers.glorot_uniform(seed=seed)\n\nmodel = keras.models.Sequential()\nmodel.add(keras.layers.Input(shape=(x.shape[1],)))\nmodel.add(keras.layers.Dense(360, activation='selu', kernel_initializer=my_init))\nmodel.add(keras.layers.Dropout(0.2))\nmodel.add(keras.layers.Dense(360, activation='selu', kernel_initializer=my_init))\nmodel.add(keras.layers.Dropout(0.2))\nmodel.add(keras.layers.Dense(360, activation='selu', kernel_initializer=my_init))\nmodel.add(keras.layers.Dense(1, activation='sigmoid'))\n# model.summary()\n\nmodel.compile(optimizer='adam', loss = keras.losses.BinaryCrossentropy(), metrics=['accuracy'])\n\nearly_stopping = keras.callbacks.EarlyStopping(monitor='accuracy', patience=3, mode='max', restore_best_weights=True)\nreduce_lr = keras.callbacks.ReduceLROnPlateau(monitor='accuracy', factor=0.1, patience=3, mode='max', min_lr=0)\n\nmodel.fit(x, y, epochs = 50, batch_size = 4, callbacks=[reduce_lr, early_stopping], verbose = 1)","9a9c8296":"target_col =[]\n\ntest_data = test_one[features].to_numpy()\nprediction_nn = model.predict(test_data)\n\nfor i in prediction_nn:\n  target_col.append(int(round(i[0])))\n\noutput = pd.DataFrame({'PassengerId': test['PassengerId'], 'Survived': target_col})\noutput.to_csv('my_submission.csv', index=False)","109659e8":"# XGBoost","a29e18dc":"# Get Rid of Redundant Data\n\nLook at the number of columns we have! Let's drop allll the redundant ones, and we will only have the one hot encoded columns.\n","a9526c41":"# Random Forest Classifier","c584a219":"# What to Do With The Null Values!\nLet's work with null values first. As we can see from the info table of both datasets that we have null values present in 4 columns: 'Age', 'Cabin', 'Embarked' & 'Fare'. \n\n* Dropping the 'Age' column wouldn't be wise cause, we can guess that age played an important role in a passenger's survival possibility. Let's see what we can do with for the missing values. See that, the age data for all the 1391 passengers is **normally distributed** (which is what we would expect), so we can **replace the missing values with the mean of the normal distribution**.\n* As 'Cabin''s **null value percentage is higher than 50%** we'll simply drop it.\n* 'Fare' has only 1 missing value, replace it with the median of fare.\n* 'Embarked' has 2 missing values we will simply replace them with 'S' (Southampton)\n","3e27ad26":"# The End\n\nMy neural network had a slightly better accuracy than the random forest classifier because, I put in more effort in tuning my neural network than tuning the random forest,that's all. You could get this accuracy from the random forest too with a little bit of fine-tuning.\n\nPerformance of a machine learning model depends on two major tasks. \n* Data analysis and preprocessing &\n* Optimization of your model things like hyperparameter optimaization, weight initialization, seeding etc. \n\nDeeper analysis of data will get you more accuracy. I have seen some notebooks where they also made use of the 'Cabin' and 'Ticket' features that we dropped as redundant. So you could play around with it more to explore more!\n\nThat's all guys. Thank you for giving it a read!\nAny kind of constructive criticism will be appreciated. Sayonara!","87683c1a":"# Nowww Time For Some Plots & Charts!","0a018dc7":"# Data Formatting i.e. Discretization, Datatype Coversion\n\nWe will convert the continuous features into discrete categories for the sake of our model to classify the data with a higher accuracy. \n\n* We will use pandas' quantile-based discretization function [pandas.qcut()](https:\/\/pandas.pydata.org\/pandas-docs\/stable\/reference\/api\/pandas.qcut.html) (see the documentation for further reading) to discretize 'Age' & 'Fare' into 4 quantile based buckets.\n\n* We will categorize the familysize into 4 categories:\n> Alone (family size= 0), Small (familysize = 1,2,3,4), Medium (familysize = 5,6), Large (familysize = larger than 6)\n\nLet's make some plots again!","77dad541":"# One Hot Encoding\n\nAhem, why do we need one hot encoding? \n\nOne hot encoding allows the representation of categorical data to be more expressive hence the model just works better!\n\nTo have a better understanding head over here: [https:\/\/bambielli.com\/til\/2018-02-11-one-hot-encoding\/](https:\/\/bambielli.com\/til\/2018-02-11-one-hot-encoding\/)","aa1db425":"# It's Time For The Machine To Learn!\n\nNow that we have gone through all the data anaysis and preprocessing steps, we are now going to train our model!\n\nHere I am gonna use two types of model:\nA **random forest classifier** and then a **neural network**. Now I know that you don't really need to call in the big gun 'neural network' here. But meh, I'll just do it. You always have more hyperparametrs and optimizations available in a neural network so it's fun to work with!\n\nLet's first split the dataset into 95% training and 5% validation data. \nWe have only 891 entries, that's a very small dataset so we will keep only 5% (45 entries) for validation.","bc4a1232":"# Neural Network","cbbb4e7b":"Visualization of data makes it very easy for us to analyse the data. We can understand the relation between a feature and the target from these plots. \n\nLike from the **barplot of sex vs. survived** we can see that around 75% women survived the disaster whereas only around 20% of the men survived. \n\nFrom the **histogram of the familysize by survival** we see that, passengers whose familysize was 0, meaning alone passengers had a very low survival rate.\n\nThe pointplots show how does embarked and class factor with sex & survival.","5198acd9":"# Titanic: Getting inside of the **top 10%** and the key to that is **EDA** \n\nNotebook written by Arnab Saha\n\nI would like to thank LD Freeman for his great notebook ['A Data Science Framework: To Achieve 99% Accuracy'](https:\/\/www.kaggle.com\/ldfreeman3\/a-data-science-framework-to-achieve-99-accuracy). This notebook provides a great insight on how to analyze and process and your data. ","ddfd68f9":"# Feature Engineering & Data Exploration\n\nLet's import our necessary libraries for data analysis and get started.","a54bbf0d":"# Create New Feature Columns\nNow, notice in the dataset there are 2 columns named **'SibSp' (no. of siblings and spouses) and 'Parch' (no. of parents and children)**. These contain family information, how many members are there in a family excluding the passenger. Instead of using two seperate columns let's create a new column **'Familysize'** by adding the total numbers of sibsp and parch.\n\n\nThe name column can seem like a a feature that won't be necessary because obviously, you won't decide if a person will survive a disaster depending on his name!\nThat just won't make sense.\n\nBUT BUT BUT wait! The title can be useful right? Like see some passengers hold the 'Master' title. Don't you think they would be a little bit privileged to get a seat on the life boat than other passengers?\nLet's extract the titles from the name column, make a new column title and let's find out if survival really depends on the title!\n\nWe will split the namestrings and extract the titles and make a new column. The column will have these values: \n> ['Mr', 'Mrs', 'Miss', 'Master', 'Don', 'Rev', 'Dr', 'Mme', 'Ms', 'Major', 'Lady', 'Sir', 'Mlle', 'Col', 'Capt', 'the Countess', 'Jonkheer']\n\nAn then we will replace the titles having a lower frequency say, has less than 20 ocurrences, with 'Misc'. This way our data will look a lot cleaner as we have fewer categories to work with.\n"}}