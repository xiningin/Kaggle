{"cell_type":{"e277ba09":"code","444dbc47":"markdown"},"source":{"e277ba09":"from sklearn.preprocessing import StandardScaler\nfrom sklearn import ensemble, gaussian_process, linear_model, naive_bayes, neighbors, svm, tree, discriminant_analysis, model_selection\nfrom xgboost import XGBClassifier \nimport pandas as pd\nimport numpy as np\nimport kaggle\n\n\n# Custom imputer that handles numerical and categorical values\nfrom sklearn.base import TransformerMixin\nclass CustomImputer(TransformerMixin):\n\n    def __init__(self):\n        \"\"\"Impute missing values.\n\n        Columns of dtype object are imputed with the most frequent value \n        in column.\n\n        Columns of other types are imputed with median of column.\n\n        \"\"\"\n    def fit(self, X, y=None):\n\n        self.fill = pd.Series([X[c].value_counts().index[0]\n            if X[c].dtype == np.dtype('O') else X[c].median() for c in X],\n            index=X.columns)\n\n        return self\n\n    def transform(self, X, y=None):\n        return X.fillna(self.fill)\n\n# Gather data\ntrain_data = pd.read_csv(\"\/kaggle\/input\/titanic\/train.csv\")\ntest_data = pd.read_csv(\"\/kaggle\/input\/titanic\/test.csv\")\ntest_copy = test_data.copy()\n\n# Create family feature\ntrain_data['FamilySize'] = train_data['SibSp'] + train_data['Parch']\ny = train_data['Survived']\ntest_data['FamilySize'] = test_data['SibSp'] + test_data['Parch']\ntrain_data['IsAlone'] = train_data['FamilySize'] == 0\ntest_data['IsAlone'] = test_data['FamilySize'] == 0\n\n# Extract title\ndef extract_title(train_data):\n    name_list = train_data['Name']\n    title_list = [name.split(\", \")[1].split(\".\")[0] for name in name_list]\n    title_dict = {\n        \"Mr\": \"Mr\",\n        \"Mrs\": \"Mrs\",\n        \"Miss\": \"Miss\",\n        \"Master\": \"Master\",\n        \"Don\": \"Noble\",\n        \"Dona\": \"Noble\",\n        \"Rev\": \"Rev\",\n        \"Dr\": \"Dr\",\n        \"Mme\": \"Noble\",\n        \"Ms\": \"Miss\",\n        \"Major\": \"Military\",\n        \"Lady\": \"Noble\",\n        \"Sir\": \"Noble\",\n        \"Mlle\": \"Noble\",\n        \"Col\": \"Military\",\n        \"Capt\": \"Military\",\n        \"the Countess\": \"Noble\",\n        \"Jonkheer\": \"Noble\"\n    }\n    title_feature = [title_dict[key] for key in title_list]\n    return title_feature\ntrain_data['Title'] = extract_title(train_data)\ntest_data['Title'] = extract_title(test_data)\n\n\n# Remove unwanted features\nunwanted_features = ['PassengerId', \"Cabin\", 'Ticket','Name']\ntrain_data.drop(columns=unwanted_features, inplace=True)\ntrain_data.drop(columns=\"Survived\", inplace=True)\ntest_data.drop(columns=unwanted_features, inplace=True)\n\n# Impute median missing values for both numerical and categorical features\nCI = CustomImputer()\ntrain_data = CI.fit_transform(train_data)\ntest_data = CI.transform(test_data)\n\n# Define categorical variables\ncat_vars = ['Sex', 'Pclass', 'Embarked', 'Title']\nnum_vars = [var for var in train_data.columns if var not in cat_vars]\n\n# Transform to dummy variables\ntrain_data = pd.get_dummies(train_data, columns=cat_vars, drop_first=True) \ntest_data = pd.get_dummies(test_data, columns=cat_vars, drop_first=True)\n\n# Rescale the data\nscaler = StandardScaler()\ntrain_data[train_data.columns] = scaler.fit_transform(train_data)\ntest_data[test_data.columns] = scaler.transform(test_data)\n\n#MLA_predict\n\nvote_est = [\n    #Ensemble Methods: http:\/\/scikit-learn.org\/stable\/modules\/ensemble.html\n    ('ada', ensemble.AdaBoostClassifier()),\n    ('bc', ensemble.BaggingClassifier()),\n    ('etc',ensemble.ExtraTreesClassifier()),\n    ('gbc', ensemble.GradientBoostingClassifier()),\n    ('rfc', ensemble.RandomForestClassifier()),\n\n    #Gaussian Processes: http:\/\/scikit-learn.org\/stable\/modules\/gaussian_process.html#gaussian-process-classification-gpc\n    ('gpc', gaussian_process.GaussianProcessClassifier()),\n    \n    #GLM: http:\/\/scikit-learn.org\/stable\/modules\/linear_model.html#logistic-regression\n    ('lr', linear_model.LogisticRegressionCV()),\n    \n    #Navies Bayes: http:\/\/scikit-learn.org\/stable\/modules\/naive_bayes.html\n    ('bnb', naive_bayes.BernoulliNB()),\n    ('gnb', naive_bayes.GaussianNB()),\n    \n    #Nearest Neighbor: http:\/\/scikit-learn.org\/stable\/modules\/neighbors.html\n    ('knn', neighbors.KNeighborsClassifier()),\n    \n    #SVM: http:\/\/scikit-learn.org\/stable\/modules\/svm.html\n    ('svc', svm.SVC(probability=True)),\n    \n    #xgboost: http:\/\/xgboost.readthedocs.io\/en\/latest\/model.html\n   ('xgb', XGBClassifier())\n\n]\n\ngrid_n_estimator = [10, 50, 100, 300]\ngrid_ratio = [.1, .25, .5, .75, 1.0]\ngrid_learn = [.01, .03, .05, .1, .25]\ngrid_max_depth = [2, 4, 6, 8, 10, None]\ngrid_min_samples = [5, 10, .03, .05, .10]\ngrid_criterion = ['gini', 'entropy']\ngrid_bool = [True, False]\ngrid_seed = [0]\n\n\ngrid_param = [\n            [{\n            #AdaBoostClassifier - http:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.ensemble.AdaBoostClassifier.html\n            'n_estimators': grid_n_estimator, #default=50\n            'learning_rate': grid_learn, #default=1\n            #'algorithm': ['SAMME', 'SAMME.R'], #default=\u2019SAMME.R\n            'random_state': grid_seed\n            }],\n       \n    \n            [{\n            #BaggingClassifier - http:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.ensemble.BaggingClassifier.html#sklearn.ensemble.BaggingClassifier\n            'n_estimators': grid_n_estimator, #default=10\n            'max_samples': grid_ratio, #default=1.0\n            'random_state': grid_seed\n             }],\n\n    \n            [{\n            #ExtraTreesClassifier - http:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.ensemble.ExtraTreesClassifier.html#sklearn.ensemble.ExtraTreesClassifier\n            'n_estimators': grid_n_estimator, #default=10\n            'criterion': grid_criterion, #default=\u201dgini\u201d\n            'max_depth': grid_max_depth, #default=None\n            'random_state': grid_seed\n             }],\n\n\n            [{\n            #GradientBoostingClassifier - http:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.ensemble.GradientBoostingClassifier.html#sklearn.ensemble.GradientBoostingClassifier\n            #'loss': ['deviance', 'exponential'], #default=\u2019deviance\u2019\n            'learning_rate': [.05], #default=0.1 -- 12\/31\/17 set to reduce runtime -- The best parameter for GradientBoostingClassifier is {'learning_rate': 0.05, 'max_depth': 2, 'n_estimators': 300, 'random_state': 0} with a runtime of 264.45 seconds.\n            'n_estimators': [300], #default=100 -- 12\/31\/17 set to reduce runtime -- The best parameter for GradientBoostingClassifier is {'learning_rate': 0.05, 'max_depth': 2, 'n_estimators': 300, 'random_state': 0} with a runtime of 264.45 seconds.\n            #'criterion': ['friedman_mse', 'mse', 'mae'], #default=\u201dfriedman_mse\u201d\n            'max_depth': grid_max_depth, #default=3   \n            'random_state': grid_seed\n             }],\n\n    \n            [{\n            #RandomForestClassifier - http:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.ensemble.RandomForestClassifier.html#sklearn.ensemble.RandomForestClassifier\n            'n_estimators': grid_n_estimator, #default=10\n            'criterion': grid_criterion, #default=\u201dgini\u201d\n            'max_depth': grid_max_depth, #default=None\n            'oob_score': [False], #default=False -- 12\/31\/17 set to reduce runtime -- The best parameter for RandomForestClassifier is {'criterion': 'entropy', 'max_depth': 6, 'n_estimators': 100, 'oob_score': True, 'random_state': 0} with a runtime of 146.35 seconds.\n            'random_state': grid_seed\n             }],\n    \n            [{    \n            #GaussianProcessClassifier\n            'max_iter_predict': grid_n_estimator, #default: 100\n            'random_state': grid_seed\n            }],\n        \n    \n            [{\n            #LogisticRegressionCV - http:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.linear_model.LogisticRegressionCV.html#sklearn.linear_model.LogisticRegressionCV\n            'fit_intercept': grid_bool, #default: True\n            #'penalty': ['l1','l2'],\n            'solver': ['liblinear'], #default: lbfgs\n            'random_state': grid_seed\n             }],\n            \n    \n            [{\n            #BernoulliNB - http:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.naive_bayes.BernoulliNB.html#sklearn.naive_bayes.BernoulliNB\n            'alpha': grid_ratio, #default: 1.0\n             }],\n    \n    \n            #GaussianNB - \n            [{}],\n    \n            [{\n            #KNeighborsClassifier - http:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.neighbors.KNeighborsClassifier.html#sklearn.neighbors.KNeighborsClassifier\n            'n_neighbors': [1,2,3,4,5,6,7], #default: 5\n            'weights': ['uniform', 'distance'], #default = \u2018uniform\u2019\n            'algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute']\n            }],\n            \n    \n            [{\n            #SVC - http:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.svm.SVC.html#sklearn.svm.SVC\n            #http:\/\/blog.hackerearth.com\/simple-tutorial-svm-parameter-tuning-python-r\n            #'kernel': ['linear', 'poly', 'rbf', 'sigmoid'],\n            'C': [1,2,3,4,5], #default=1.0\n            'gamma': grid_ratio, #edfault: auto\n            'decision_function_shape': ['ovo', 'ovr'], #default:ovr\n            'probability': [True],\n            'random_state': grid_seed\n             }],\n\n    \n            [{\n            #XGBClassifier - http:\/\/xgboost.readthedocs.io\/en\/latest\/parameter.html\n            'learning_rate': grid_learn, #default: .3\n            'max_depth': [1,2,4,6,8,10], #default 2\n            'n_estimators': grid_n_estimator, \n            'seed': grid_seed  \n             }]   \n        ]\nimport time\nfrom sklearn.model_selection import GridSearchCV, cross_val_score\nstart_total = time.perf_counter() #https:\/\/docs.python.org\/3\/library\/time.html#time.perf_counter\ncv_split = model_selection.ShuffleSplit(n_splits = 10, test_size = .3, train_size = .6)\nfor clf, param in zip (vote_est, grid_param): #https:\/\/docs.python.org\/3\/library\/functions.html#zip\n\n    #print(clf[1]) #vote_est is a list of tuples, index 0 is the name and index 1 is the algorithm\n    #print(param)\n    \n    \n    start = time.perf_counter()        \n    best_search = model_selection.GridSearchCV(estimator = clf[1], param_grid = param, cv = cv_split, scoring = 'roc_auc')\n    best_search.fit(train_data, y)\n    run = time.perf_counter() - start\n\n    best_param = best_search.best_params_\n    print('The best parameter for {} is {} with a runtime of {:.2f} seconds.'.format(clf[1].__class__.__name__, best_param, run))\n    clf[1].set_params(**best_param) \n\n\nrun_total = time.perf_counter() - start_total\nprint('Total optimization time was {:.2f} minutes.'.format(run_total\/60))\n\nprint('-'*10)\nfrom sklearn.ensemble import VotingClassifier\nVCensemble = VotingClassifier(estimators=vote_est, voting=\"hard\")\nscores = cross_val_score(VCensemble, train_data, y, cv=10, scoring='accuracy')\nprint(scores.mean(), scores.std())\n    \n# Make submission\nVCensemble.fit(train_data, y)\npredictions = VCensemble.predict(test_data)\noutput = pd.DataFrame({'PassengerId': test_copy.PassengerId, 'Survived': predictions})\noutput.to_csv('title_soft_submission.csv', index=False)\nprint(\"Your submission was successfully saved!\")","444dbc47":"The goal for this competition was to predict who would survive from the titanic given certain features about the passengers, like age, title, city of origin, gender, class, ticket number, etc. \n\nThe workflow I followed was:\n1. Clean up the data by extracting titles from the passengers' names and adding it as a new feature, as well as adding a new family feature that accounts for passengers traveling in groups.\n2. Clean up the data by removing some features that likely had little meaning or had so few entries it was unhelpful, such as name, ticket number, passenger id number.\n3. Clean up the data by imputing missing values (median for numerical, mode for categorical).\n4. Transformed the categorical features into one-hot encoding.\n5. Scaled the numerical features using a standard scaler.\n6. Performed a hyperparameter search (using grid search + cross validation) on multiple classifier machine learning algorithms\n7. Created an ensemble model using a voting estimator that combined the multiple classifiers together. \n8. As an estimation of model accuracy, I used cross validation with 10 folds on the training dataset and achieved a mean+std accuracy of 0.83+0.04."}}