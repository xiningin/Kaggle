{"cell_type":{"7961374b":"code","b85586d8":"code","8ed93c5b":"code","a0b34f5f":"code","0b7f694a":"code","84bc72d8":"code","e9d25785":"code","406c6eba":"code","5d293060":"code","35fcd4ce":"code","a7cf63ba":"code","68e5cb37":"code","954abfaa":"code","28637f96":"code","9fc63051":"code","4a23bb76":"code","6936b0b8":"code","f0607236":"code","fa47e683":"code","a91335bc":"code","d9e153f5":"code","037ca864":"code","71e3af3a":"code","aaa6098a":"code","39e43c41":"code","83623e5a":"code","f7055ab9":"markdown","455b3544":"markdown","31c224b8":"markdown","0fe7ef59":"markdown","8489757b":"markdown","e4bb3046":"markdown","f8f02ea5":"markdown","7f138aaa":"markdown","186fa585":"markdown","16d03929":"markdown","781af377":"markdown","dc4e8000":"markdown","7c8fd317":"markdown","9eb5ef58":"markdown","3126e259":"markdown","f91ef5f8":"markdown","f08ed325":"markdown","b373178f":"markdown","0a8d5d37":"markdown","4d18614c":"markdown","cc4e7b5b":"markdown"},"source":{"7961374b":"import pandas as pd\nimport numpy as np\nfrom scipy.stats import skew\nfrom scipy.special import boxcox1p, boxcox\nfrom scipy.stats import boxcox_normmax\n\nfrom sklearn.model_selection import train_test_split\n\nfrom sklearn.ensemble import RandomForestClassifier\nfrom xgboost import XGBClassifier\nimport xgboost as xgb\n\n# custom class to preprocess features\nfrom sklearn.base import BaseEstimator, TransformerMixin  \n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\nfrom yellowbrick.classifier import ConfusionMatrix, ClassificationReport\n\nimport shap\n# load JS visualization code to notebook\nshap.initjs()  ","b85586d8":"train = pd.read_csv(\"..\/input\/learn-together\/train.csv\", index_col='Id')\ntest = pd.read_csv(\"..\/input\/learn-together\/test.csv\", index_col='Id')\nTarget = 'Cover_Type'","8ed93c5b":"# quick check for missing values\ntrain.isnull().sum()","a0b34f5f":"#Ref : https:\/\/www.kaggle.com\/vprokopev\/mean-likelihood-encodings-a-comprehensive-study\n\ndef describe_dataset(data):\n    datatype = []  # added by Sid\n    ncats = []\n    ncats10 = []\n    ncats100 = []\n    nsamples_median = []\n    X_col_names = list(data.columns)\n    #X_col_names.remove(target_col)\n    print('Number of samples: ', data.shape[0])\n    for col in X_col_names:\n        datatype.append(data.dtypes[col])\n        counts = data.groupby([col])[col].count()\n        ncats.append(len(counts))\n        ncats10.append(len(counts[counts<10]))\n        ncats100.append(len(counts[counts<100]))\n        nsamples_median.append(counts.median())\n    data_review_df = pd.DataFrame({'Column':X_col_names, 'DType':datatype, 'Number of categories':ncats, \n                                   'Categories with < 10 samples':ncats10,\n                                   'Categories with < 100 samples':ncats100,\n                                   'Median samples in category':nsamples_median})\n    data_review_df = data_review_df.loc[:, ['Column', 'DType', 'Number of categories',\n                                             'Median samples in category',\n                                             'Categories with < 10 samples',\n                                             'Categories with < 100 samples']]\n    return data_review_df.sort_values(by=['Number of categories'], ascending=False)","0b7f694a":"## check for hi cardinality feat\ndescribe_dataset(train)\n","84bc72d8":"#Check Target classes \ntrain[Target].value_counts()","e9d25785":"all_cols = [train.columns.values]\nall_cols","406c6eba":"num_cols = ['Elevation', 'Aspect', 'Slope', 'Horizontal_Distance_To_Hydrology', 'Vertical_Distance_To_Hydrology', 'Horizontal_Distance_To_Roadways',\n              'Hillshade_9am', 'Hillshade_Noon', 'Hillshade_3pm', 'Horizontal_Distance_To_Fire_Points' ]\n\ncat_cols = [col for col in train.columns if col not in num_cols]\ncat_cols.remove(\"Cover_Type\")\ncat_cols","5d293060":"len(num_cols), len(cat_cols)","35fcd4ce":"# Feat Engr\n\nclass MyBoxCox(BaseEstimator, TransformerMixin):\n    def __init__(self):\n        self.cols_to_skew = num_cols\n        self.skew_features = []\n        self.lam = []\n        self.feat_min = np.nan\n        \n    def fit(self, X, y=None):\n        self.skew_features = X[self.cols_to_skew].apply(lambda x: skew(x)).sort_values(ascending=False)\n        self.high_skew = self.skew_features[self.skew_features > 0.5]  #t\/f large -ve skew gets strange results\n        self.skew_index = self.high_skew.index\n        \n        for i in self.skew_index:\n            self.feat_min = X[i].min()\n            if self.feat_min > 0 :\n                self.lam.append( boxcox_normmax(X[i]) )\n            else :\n                self.lam.append( boxcox_normmax(X[i] + 1.0 - self.feat_min) ) #min is 1, can use boxcox\n     \n        self.lam = np.asarray(self.lam)\n                \n        return self\n    \n    def transform(self, X, y=None):\n        df = X.copy()\n        if self.skew_index is None:\n            raise ValueError('ERROR !! skew index is None !!')\n        \n        for idx, i in enumerate(self.skew_index) :\n            self.feat_min = df[i].min()\n            if self.feat_min > 0 :\n                df[i] = boxcox(df[i], self.lam[idx])\n            else :\n                df[i] = boxcox(df[i] + 1.0 - self.feat_min, self.lam[idx])   #min is 1, can use boxcox\n        \n        return df\n        \n\n\n","a7cf63ba":"bc_tf = MyBoxCox()\nbc_tf.fit(train)\nz1=bc_tf.transform(train)\n#z3=bc_tf.transform(test)","68e5cb37":"plt.figure(figsize=(15,6))\nsns.distplot(a=train[Target], label=\"Cover Type\", kde=False) \nplt.title(\"Distribution of Trees Cover Type\")\nplt.legend()  # force legend\n","954abfaa":"train.hist(figsize=(20,30));","28637f96":"f = pd.melt(train, value_vars=num_cols)\ng = sns.FacetGrid(f, col=\"variable\",  col_wrap=5, sharex=False, sharey=False)\ng = g.map(sns.distplot, \"value\", kde=False)","9fc63051":"f = pd.melt(z1, value_vars=num_cols)\ng = sns.FacetGrid(f, col=\"variable\",  col_wrap=5, sharex=False, sharey=False)\ng = g.map(sns.distplot, \"value\", kde=False)","4a23bb76":"colormap = plt.cm.RdBu\nplt.figure(figsize=(50,35))\nplt.title('Pearson Correlation of Features', y=1.05, size=50)\nsns.heatmap(train.corr(),linewidths=0.1, vmax=1.0, square=True, cmap=colormap, linecolor='white', annot=True)","6936b0b8":"X = pd.read_csv(\"..\/input\/learn-together\/train.csv\", index_col='Id')\nX_test = pd.read_csv(\"..\/input\/learn-together\/test.csv\", index_col='Id')\n\ny = X.Cover_Type\nX = X.drop([Target], axis=1)\n","f0607236":"# Keep a small valid set for SHAP plot\nX_train, X_valid, y_train, y_valid = train_test_split(X, y, train_size=0.9 )  #no random seed\n\nxgb_model = XGBClassifier(n_estimators=3000, learning_rate=0.05,  random_state=0, )\nxgb_model.fit(X_train, y_train, early_stopping_rounds=50, eval_set=[(X_valid, y_valid)])","fa47e683":"from xgboost import plot_importance\nfrom matplotlib import pyplot\nplot_importance(xgb_model)\nplt.rcParams[\"figure.figsize\"] = (24, 20)\npyplot.show()","a91335bc":"fig, ax = plt.subplots()\ncm = ConfusionMatrix(xgb_model,  ax=ax)\n\n#Fit fits the passed model. This is unnecessary if you pass the visualizer a pre-fitted model\n#cm.fit(X_train, y_train)\n\ncm.score(X_valid, y_valid)\ncm.poof()","d9e153f5":"fig, ax = plt.subplots()\nvisualizer = ClassificationReport(xgb_model,  ax=ax)\nax.grid(False)\n\n#visualizer.fit(X_train, y_train)  # Fit the training data to the visualizer\nvisualizer.score(X_valid, y_valid) \ng = visualizer.poof() ","037ca864":"# explain the model's predictions using SHAP values\n# For non tree models, use shap.KernelExplainer() instead of shap.TreeExplainer(), but computation time may be LONG\nexplainer = shap.TreeExplainer(xgb_model)\nshap_values = explainer.shap_values(X_valid)\nshap.summary_plot(shap_values, X_valid)","71e3af3a":"# shap_values[0 to 6][0 to n_rows] --> 7 Tgt classes, n rows\n\nexplainer.expected_value # mean value of each class","aaa6098a":"shap.force_plot(explainer.expected_value[1], shap_values[1], X_valid) # index [0 to 6] refers to Target Class, select '1' here  ","39e43c41":"shap.dependence_plot(\"Elevation\", shap_values[0], X_valid)","83623e5a":"shap.dependence_plot(\"Aspect\", shap_values[0], X_valid, interaction_index=\"Hillshade_3pm\")","f7055ab9":"Define numerical & categorical features --> since all columns are type 'int64'","455b3544":"# My first public Kernel...comments are welcome !\n# My main aim is to quickly: \n1) Run some fast stats through the data\n\n2) Plot charts to see how data looks like \n\n3) Run a fast simulation without any feature preprocessing --> Tree algo like Random Forest, xgboost comes to mind ","31c224b8":"# SHAP summary plot above is similar to xgboost internal \"feature importance\" plot\n# However, we can use force_plot or dependence plot to drill down influence of each feature, or each data sample (row) on the Target ","0fe7ef59":"# Train data before Boxcox transform","8489757b":"# Explain entire dataset\nYou can use pulldown menu on chart to filter & select patterns !\n\nBlue means negative contribution to Target, while red is positive contribution","e4bb3046":"# Check if data has hi-cardinality (ie many categories)","f8f02ea5":"# Can also check how \"Aspect\" & \"Hillshade_3pm\" affect Target","7f138aaa":"# What is SHAP plot ?\nRef: \n\n1) https:\/\/towardsdatascience.com\/interpretable-machine-learning-with-xgboost-9ec80d148d27\n\n2) https:\/\/github.com\/slundberg\/shap\n\nBasically, SHAP values can explain the output of any machine learning model by using a simpler model to approximate the exact model.\nAny model means it is not just restricted to tree classifiers, but non-tree models like SVM. \nHowever, computation time is fastest for Tree based models","186fa585":"# Import Data","16d03929":"Apply Boxcox transformation to skewed numerical features\n\nNote: This is not necessary for Tree classifiers, but useful for linear classifiers like Logistic Regression or Support Vector Machine","781af377":"# Note: Expected value near 0.5 means Class is difficult to separate","dc4e8000":"# Confusion Matrix\nFrom plots below, it is clear some Target Classes can be separated easily, while other Classes need more work\n\nThis is where SHAP plots come in","7c8fd317":"# Use xgboost internal feature importance plot","9eb5ef58":"# Import Packages","3126e259":"# Run xgboost for pipe cleaning --> get a baseline","f91ef5f8":"# Check how feature \"Elevation\" affects Target","f08ed325":"# Note: '0' refers to Class 1, '1' refers to Class 2, and so on...","b373178f":"# Quick & fast way to see all data","0a8d5d37":"# Correlation Matrix","4d18614c":"# Histograms for Target ","cc4e7b5b":"# Train data after Boxcox transform (numerical features with skew > 0.5 )"}}