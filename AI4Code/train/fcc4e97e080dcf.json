{"cell_type":{"aaf0497a":"code","529bfeae":"code","28a9e829":"code","275dcd73":"code","ac8f242e":"code","e4ea01d5":"code","e75aa931":"code","d11ec106":"code","ce00de39":"code","eb1dd8f9":"code","3b0bd4d7":"code","a2b22af3":"code","3308218a":"code","80788ce6":"code","39fe48c4":"code","329c28e8":"code","34bc0d09":"code","bcdda3df":"code","3a04380d":"code","1416766b":"code","56ea0da4":"markdown"},"source":{"aaf0497a":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","529bfeae":"import pandas as pd, numpy as np\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer","28a9e829":"train = pd.read_csv('..\/input\/train.csv')\ntest = pd.read_csv('..\/input\/test.csv')\nsubm = pd.read_csv('..\/input\/sample_submission.csv')","275dcd73":"train.head()","ac8f242e":"train['question_text'][0]","e4ea01d5":"lens = train.question_text.str.len()\nlens.mean(), lens.std(), lens.max()","e75aa931":"lens.hist();","d11ec106":"len(train),len(test)","ce00de39":"train['question_text'].fillna(\"unknown\", inplace=True)\ntest['question_text'].fillna(\"unknown\", inplace=True)","eb1dd8f9":"import re, string\nre_tok = re.compile(f'([{string.punctuation}\u201c\u201d\u00a8\u00ab\u00bb\u00ae\u00b4\u00b7\u00ba\u00bd\u00be\u00bf\u00a1\u00a7\u00a3\u20a4\u2018\u2019])')\ndef tokenize(s): return re_tok.sub(r' \\1 ', s).split()","3b0bd4d7":"n = train.shape[0]\nvec = TfidfVectorizer(ngram_range=(1,2), tokenizer=tokenize,\n               min_df=3, max_df=0.9, strip_accents='unicode', use_idf=1,\n               smooth_idf=1, sublinear_tf=1 )\ntrn_term_doc = vec.fit_transform(train['question_text'])\ntest_term_doc = vec.transform(test['question_text'])","a2b22af3":"def pr(y_i, y):\n    p = x[y==y_i].sum(0)\n    return (p+1) \/ ((y==y_i).sum()+1)","3308218a":"x = trn_term_doc\ntest_x = test_term_doc","80788ce6":"def get_mdl(y):\n    y = y.values\n    r = np.log(pr(1,y) \/ pr(0,y))\n    m = LogisticRegression(C=4, dual=True)\n    x_nb = x.multiply(r)\n    return m.fit(x_nb, y), r","39fe48c4":"m,r = get_mdl(train['target'])\n","329c28e8":"preds = m.predict_proba(test_x.multiply(r))","34bc0d09":"preds = preds[:,1] ","bcdda3df":"thresholds = np.linspace(0, 1, 1000)\nscore = 0.0\ntest_threshold=0.5\nbest_threshold=np.zeros(1)\nbest_val = np.zeros(1)\n\nfor threshold in thresholds:\n    test_threshold = threshold\n    max_val = np.max(val_pred[:,0])\n    val_predict = (val_pred[:,0] > test_threshold)\n    score = f1_score(y_val, val_predict)\n    if score > best_val:\n        best_threshold = threshold\n        best_val = score\n\nprint(\"Threshold %0.6f, F1: %0.6f\" % (best_threshold,best_val))\ntest_threshold = best_threshold\n\nprint(\"Best threshold: \")\nprint(best_threshold)\nprint(\"Best f1:\")\nprint(best_val)","3a04380d":"y_te = (preds > best_threshold).astype(np.int)","1416766b":"submit_df = pd.DataFrame({\"qid\": test[\"qid\"], \"prediction\": y_te})\nsubmit_df.to_csv(\"submission.csv\", index=False)","56ea0da4":"All stolen from https:\/\/www.kaggle.com\/jhoward\/nb-svm-strong-linear-baseline"}}