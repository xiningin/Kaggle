{"cell_type":{"8e5ec5ad":"code","7507cc31":"code","89c02c19":"code","0c27cc08":"code","e584b171":"code","14fe2e78":"code","0c0fc968":"code","da7d0efa":"code","8bf3ba37":"code","03da6ac0":"code","d62ac173":"code","122d7042":"code","8f66fb66":"code","8bd37db5":"code","a59bc001":"code","d62b7a82":"code","cb3499ac":"code","e93659a6":"code","85964b6b":"code","2cee9143":"code","c0d36018":"code","2e158ea2":"code","cef8d920":"code","82b647b8":"markdown","b354d403":"markdown","7a4f7938":"markdown","be2ba679":"markdown","f7eb8990":"markdown","a27f6e71":"markdown","10643e73":"markdown","ff766cc9":"markdown","995143aa":"markdown","f944d11e":"markdown","73e605be":"markdown","bdc14746":"markdown","8a914904":"markdown","817375cf":"markdown","e5ebfaaf":"markdown","b1e1d2c0":"markdown"},"source":{"8e5ec5ad":"import numpy as np \nimport pandas as pd \nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\ndf = pd.read_csv('..\/input\/chocolate-ratings\/chocolate_ratings.csv')\nmakerdf = pd.read_csv('..\/input\/chocolate-ratings\/chocolate_makers.csv')\ndf.info()","7507cc31":"df.head()","89c02c19":"df = df.assign(Cocoa_Num = lambda row: row[\"Cocoa Percent\"].replace(\"%\", \"\", regex=True).astype('float'))\ndf = df.drop(\"Cocoa Percent\", axis=1)\ndf.head()","0c27cc08":"_temp = df[\"Country of Bean Origin\"]\ndf[\"country_bean_codes\"] = _temp.astype('category').cat.codes\ndf.head()","e584b171":"_temp = df[\"Ingredients\"]\ndf[\"Ingredients_codes\"] = _temp.astype('category').cat.codes\ndf.head()","14fe2e78":"_temp = df[\"Ingredients\"]\ndf[\"ingredient_codes\"] = _temp.astype('category').cat.codes\ndf = df.drop(\"Ingredients\", axis=1)\ndf.head()","0c0fc968":"characteristics = []\nchar_dict = {}\nfor index, row in df.iterrows():\n    _chars = row[\"Most Memorable Characteristics\"].split(\",\")\n    for _c in _chars:\n        if _c.strip().lower() in char_dict.keys():\n            char_dict[_c.strip().lower()] += 1\n        else:\n            char_dict[_c.strip().lower()] = 1","da7d0efa":"char_dict = { k: v for k, v in char_dict.items() if v >= 20 }","8bf3ba37":"df.loc[1][\"REF\"]\ndf_copy = df.copy()\n\nzeroes = []\nfor index, row in df.iterrows():\n    zeroes.append(0)\n\nfor k in char_dict.keys():\n    df_copy[k] = zeroes \n\nfor index, row in df.iterrows():\n    _chars = row[\"Most Memorable Characteristics\"].split(\",\")\n    for _c in _chars:\n        if _c.strip().lower() in char_dict.keys():\n            df_copy.loc[index,_c.strip().lower()] = 1\n\ndf_copy.describe()","03da6ac0":"import matplotlib.pyplot as plt\nimport seaborn as sns\n\ndataset = df_copy\ny = dataset[\"Rating\"]\nsns.displot(y)","d62ac173":"def Rating_rank(rating):\n        if 3.5 <= rating:\n            return 3\n        elif 3.0 <= rating <3.5:\n            return 2\n        else:\n            return 1\n\ndataset[\"rating_code\"] = dataset.loc[:,'Rating'].apply(Rating_rank)\ny = dataset[\"rating_code\"]\nsns.displot(y)","122d7042":"from sklearn.model_selection import train_test_split\nfrom sklearn import preprocessing\nfrom imblearn.over_sampling import RandomOverSampler\n\nX = dataset.select_dtypes(exclude=\"object\").copy()\nX.drop([\"rating_code\"], axis=1, inplace=True)\nX.drop([\"Rating\"], axis=1, inplace=True)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42) \n\nscaler = preprocessing.StandardScaler().fit(X_train)\nX_train = scaler.transform(X_train.values)\nX_test = scaler.transform(X_test.values)","8f66fb66":"from sklearn.tree import DecisionTreeClassifier\nclf = DecisionTreeClassifier()\nclf.fit(X_train, y_train)\nprint(clf.score(X_train, y_train))\nprint(clf.score(X_test, y_test))","8bd37db5":"from sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(max_depth=5, n_estimators=10, max_features=1)\nclf.fit(X_train, y_train)\nprint(clf.score(X_train, y_train))\nprint(clf.score(X_test, y_test))","a59bc001":"from sklearn.ensemble import AdaBoostClassifier\nclf = AdaBoostClassifier(n_estimators=500)\nclf.fit(X_train, y_train)\nprint(clf.score(X_train, y_train))\nprint(clf.score(X_test, y_test))","d62b7a82":"from sklearn.ensemble import ExtraTreesClassifier\nclf = ExtraTreesClassifier()\nclf.fit(X_train, y_train)\nprint(clf.score(X_train, y_train))\nprint(clf.score(X_test, y_test))","cb3499ac":"from sklearn.neighbors import KNeighborsClassifier\nclf = KNeighborsClassifier( n_neighbors = 10, weights = 'distance')\nclf.fit(X_train, y_train)\nprint(clf.score(X_train, y_train))\nprint(clf.score(X_test, y_test))","e93659a6":"from sklearn.ensemble import GradientBoostingClassifier\nclf = GradientBoostingClassifier()\nclf.fit(X_train, y_train)\nprint(clf.score(X_train, y_train))\nprint(clf.score(X_test, y_test))","85964b6b":"from sklearn.linear_model import LogisticRegression\nclf = LogisticRegression(solver=\"liblinear\")\nclf.fit(X_train, y_train)\nprint(clf.score(X_train, y_train))\nprint(clf.score(X_test, y_test))","2cee9143":"!pip install pycaret","c0d36018":"from pycaret.classification import *\n\ndataset = dataset.drop([\"Rating\"], axis=1)\n\nsetup(data = dataset.select_dtypes(exclude=\"object\").copy(), \n             target = 'rating_code',\n             numeric_imputation = 'mean',\n             silent = True, normalize = True, session_id=42)\nprint(\"done\")\ntop3 = compare_models(exclude = ['catboost','xgboost','lightgbm'], n_select=3)","2e158ea2":"evaluate_model(top3[0])","cef8d920":"from sklearn.linear_model import RidgeClassifier\nclf = RidgeClassifier(alpha=1,class_weight=None,copy_X=True,max_iter=None,normalize=False,random_state=5809,tol=0.001,solver=\"auto\")\nclf.fit(X_train, y_train)\nprint(clf.score(X_train, y_train))\nprint(clf.score(X_test, y_test))","82b647b8":"# Scale Data and Split X","b354d403":"# Process Data Create Y\n\nY will be the class column. Let's see what the balance is like.\n","7a4f7938":"Cocoa Percent can be changed to to a numerical value","be2ba679":"Bean origin can be converted into categorical codes","f7eb8990":"# Load Data","a27f6e71":"Hmm a lot of objects not the most auspicious start for Exploratory Data Analysis. Often it is better to have floats or ints. Let's see if there is anything that can be improved.","10643e73":"Looks like we are starting to do better now. There is a wide table with a lot of features to draw upon.","ff766cc9":"This shows we don't have enough samples for some codes. Let's improve that. ","995143aa":"# Conclusion\n\nA Ridge Classifier was the best approach found at 54.9% accuracy on the test set. ","f944d11e":"I will get rid of then characteristics that rarely occur","73e605be":"# Attempt Classification","bdc14746":"I'm going to stop trying to hand roll and instead use Pycaret for some insight.","8a914904":"Now I will split up the characteristics so I can make seperate columns for every one","817375cf":"# Introduction\n\nThis is an interesting data set though right at the start I am concerned about the information available to predict the Rating of the chocolate. I am interested to look at the data and see what solution I find though. This is really the joy of machine learning for me, finding an approach to something I've never seen before.","e5ebfaaf":"Looks like a Ridge Classifier is the best approach. Let's do a like for like comparison.","b1e1d2c0":"Reducing to three categories may seem very severe but the dataset has unfortunately got very few data points at ends of the spectrum. To balance it this would seem the best approach. "}}