{"cell_type":{"c2a393b9":"code","c95e9381":"code","1a78c0b3":"code","b56e8c33":"code","8c8ff30f":"code","e24387e5":"code","b23a45e0":"code","3a364b58":"code","b331e574":"code","110a0b19":"code","fb29c6b6":"code","bd97a569":"code","bae0ff0c":"code","bbbc8f95":"code","772de46f":"code","d628f46c":"code","6a993f15":"code","79c247dc":"code","c0a74a4d":"code","c9c8c84b":"code","91c36379":"code","bf968ce4":"code","82dec32b":"code","498939f6":"markdown","292ded12":"markdown","7a8e1bb5":"markdown","faccfccd":"markdown","45f97fd7":"markdown","84b37d4b":"markdown","02004568":"markdown","db0f2235":"markdown","3b87c2cf":"markdown","5c82ef65":"markdown"},"source":{"c2a393b9":"# Install:\n# Kaggle environments.\n!git clone https:\/\/github.com\/Kaggle\/kaggle-environments.git\n!cd kaggle-environments && pip install .\n\n# GFootball environment.\n!apt-get update -y\n!apt-get install -y libsdl2-gfx-dev libsdl2-ttf-dev\n\n# Make sure that the Branch in git clone and in wget call matches !!\n!git clone -b v2.3 https:\/\/github.com\/google-research\/football.git\n!mkdir -p football\/third_party\/gfootball_engine\/lib\n\n!wget https:\/\/storage.googleapis.com\/gfootball\/prebuilt_gameplayfootball_v2.3.so -O football\/third_party\/gfootball_engine\/lib\/prebuilt_gameplayfootball.so\n!cd football && GFOOTBALL_USE_PREBUILT_SO=1 pip3 install .","c95e9381":"!pip install pfrl==0.1.0","1a78c0b3":"import os\nimport cv2\nimport sys\nimport glob \nimport random\nimport imageio\nimport pathlib\nimport collections\nfrom collections import deque\nimport numpy as np\nimport argparse\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set()\n%matplotlib inline\n\nfrom gym import spaces\nfrom tqdm import tqdm\nfrom logging import getLogger, StreamHandler, FileHandler, DEBUG, INFO\nfrom typing import Union, Callable, List, Tuple, Iterable, Any, Dict\nfrom dataclasses import dataclass\nfrom IPython.display import Image, display\nsns.set()\n\n\n# PyTorch\nimport pfrl\nfrom pfrl.agents import CategoricalDoubleDQN\nfrom pfrl import experiments\nfrom pfrl import explorers\nfrom pfrl import nn as pnn\nfrom pfrl import utils\nfrom pfrl import replay_buffers\nfrom pfrl.wrappers import atari_wrappers\nfrom pfrl.q_functions import DistributionalDuelingDQN\n\nimport torch\nfrom torch import nn\n\n# Env\nimport gym\nimport gfootball\nimport gfootball.env as football_env\nfrom gfootball.env import observation_preprocessing","b56e8c33":"# Check we can use GPU\nprint(torch.cuda.is_available())\n\n# set gpu id\nif torch.cuda.is_available(): \n    # NOTE: it is not number of gpu but id which start from 0\n    gpu = 0\nelse:\n    # cpu=>-1\n    gpu = -1","8c8ff30f":"# set logger\ndef logger_config():\n    logger = getLogger(__name__)\n    handler = StreamHandler()\n    handler.setLevel(\"DEBUG\")\n    logger.setLevel(\"DEBUG\")\n    logger.addHandler(handler)\n    logger.propagate = False\n\n    filepath = '.\/result.log'\n    file_handler = FileHandler(filepath)\n    logger.addHandler(file_handler)\n    return logger\n\nlogger = logger_config()","e24387e5":"# fixed random seed\n# but this is NOT enough to fix the result of rewards.Please tell me the reason.\ndef seed_everything(seed=1234):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    utils.set_random_seed(seed)  # for PFRL\n    \n# Set a random seed used in PFRL.\nseed = 5046\nseed_everything(seed)\n\n# Set different random seeds for train and test envs.\ntrain_seed = seed\ntest_seed = 2 ** 31 - 1 - seed","b23a45e0":"# wrapper for env(resize and transpose channel order)\nclass TransEnv(gym.ObservationWrapper):\n    def __init__(self, env, channel_order=\"hwc\"):\n\n        gym.ObservationWrapper.__init__(self, env)\n        self.height = 84\n        self.width = 84\n        self.ch = env.observation_space.shape[2]\n        shape = {\n            \"hwc\": (self.height, self.width, self.ch),\n            \"chw\": (self.ch, self.height, self.width),\n        }\n        self.observation_space = spaces.Box(\n            low=0, high=255, shape=shape[channel_order], dtype=np.uint8\n        )\n        \n\n    def observation(self, frame):\n        frame = cv2.resize(frame, (self.width, self.height), interpolation=cv2.INTER_AREA)\n        return frame.reshape(self.observation_space.low.shape)","3a364b58":"def make_env(test):\n    # Use different random seeds for train and test envs\n    env_seed = test_seed if test else train_seed\n    \n    # env = gym.make('GFootball-11_vs_11_kaggle-SMM-v0')\n    env = football_env.create_environment(\n      env_name='11_vs_11_easy_stochastic',  # easy mode\n      stacked=False,\n      representation='extracted',  # SMM\n      rewards='scoring, checkpoints',\n      write_goal_dumps=False,\n      write_full_episode_dumps=False,\n      render=False,\n      write_video=False,\n      dump_frequency=1,\n      logdir='.\/',\n      extra_players=None,\n      number_of_left_players_agent_controls=1,\n      number_of_right_players_agent_controls=0\n    )\n    env = TransEnv(env, channel_order=\"chw\")\n\n    env.seed(int(env_seed))\n    if test:\n        # Randomize actions like epsilon-greedy in evaluation as well\n        env = pfrl.wrappers.RandomizeAction(env, random_fraction=0.0)\n    return env\n\nenv = make_env(test=False)\neval_env = make_env(test=True)","b331e574":"print('observation space:', env.observation_space.low.shape)\nprint('action space:', env.action_space)","110a0b19":"env.reset()\naction = env.action_space.sample()\nobs, r, done, info = env.step(action)\nprint('next observation:', obs.shape)\nprint('reward:', r)\nprint('done:', done)\nprint('info:', info)","fb29c6b6":"obs_n_channels = env.observation_space.low.shape[0]\nn_actions = env.action_space.n\nprint(\"obs_n_channels: \", obs_n_channels)\nprint(\"n_actions: \", n_actions)\n\n# params based the original paper\nn_atoms = 51\nv_max = 10\nv_min = -10\nq_func = DistributionalDuelingDQN(n_actions, n_atoms, v_min, v_max, obs_n_channels)\nprint(q_func)","bd97a569":"# Noisy nets\npnn.to_factorized_noisy(q_func, sigma_scale=0.5)\n\n# Turn off explorer\nexplorer = explorers.Greedy()\n\n# Use the same hyper parameters as https:\/\/arxiv.org\/abs\/1710.02298\nopt = torch.optim.Adam(q_func.parameters(), 6.25e-5, eps=1.5 * 10 ** -4)\n\n# Prioritized Replay\n# Anneal beta from beta0 to 1 throughout training\nupdate_interval = 4\nbetasteps = 5 * 10 ** 7 \/ update_interval\nrbuf = replay_buffers.PrioritizedReplayBuffer(\n        10 ** 5,  # Default value is 10 ** 6 but it is too large in this notebook. I chose 10 ** 5.\n        alpha=0.5,\n        beta0=0.4,\n        betasteps=betasteps,\n        num_steps=3,\n        normalize_by_max=\"memory\",\n    )\n\n\ndef phi(x):\n    # Feature extractor\n    return np.asarray(x, dtype=np.float32) \/ 255","bae0ff0c":"agent = CategoricalDoubleDQN(\n        q_func,\n        opt,\n        rbuf,\n        gpu=gpu,  \n        gamma=0.99,\n        explorer=explorer,\n        minibatch_size=32,\n        replay_start_size=2 * 10 ** 4,\n        target_update_interval=32000,\n        update_interval=update_interval,\n        batch_accumulator=\"mean\",\n        phi=phi,\n    )","bbbc8f95":"# if you have a pretrained model, agent can load pretrained weight. \nuse_pretrained = False\npretrained_path = None\nif use_pretrained:\n    agent.load(pretrained_path)","772de46f":"%%time\nexperiments.train_agent_with_evaluation(\n    agent=agent,\n    env=env,\n    steps=100000,\n    eval_n_steps=None,\n    eval_n_episodes=1,\n    eval_interval=3000,\n    outdir=\".\/kaggle_simulations\/agent\",\n    save_best_so_far_agent=True,\n    eval_env=eval_env,\n    logger=logger\n)","d628f46c":"import csv\n\ndef text_csv_converter(datas):\n    file_csv = datas.replace(\"txt\", \"csv\")\n    with open(datas) as rf:\n        with open(file_csv, \"w\") as wf:\n            readfile = rf.readlines()\n            for read_text in readfile:\n                read_text = read_text.split()\n                writer = csv.writer(wf, delimiter=',')\n                writer.writerow(read_text)\n\nfilename = \".\/kaggle_simulations\/agent\/scores.txt\"\ntext_csv_converter(filename)","6a993f15":"!ls -la .\/kaggle_simulations\/agent","79c247dc":"import pandas as pd\nscores = pd.read_csv(\".\/kaggle_simulations\/agent\/scores.csv\")\nscores.head()","c0a74a4d":"# visualize reward each episodes\nfig = plt.figure(figsize=(15, 5))\nax1 = fig.add_subplot(121)\nax2 = fig.add_subplot(122)\nax1.set_title(\"median reward\")\nax2.set_title(\"average loss\")\nsns.lineplot(x=\"episodes\", y=\"median\", data=scores, ax=ax1)\nsns.lineplot(x=\"episodes\", y=\"average_loss\", data=scores,ax=ax2)\nplt.show()","c9c8c84b":"%%writefile .\/kaggle_simulations\/agent\/main.py\nimport cv2\nimport collections\nimport gym\nimport numpy as np\nimport os\nimport sys\nimport torch\n\nfrom gfootball.env import observation_preprocessing\nfrom gfootball.env import wrappers\n\n# PFRL\nimport pfrl\nfrom pfrl.agents import CategoricalDoubleDQN\nfrom pfrl import experiments\nfrom pfrl import explorers\nfrom pfrl import nn as pnn\nfrom pfrl import utils\nfrom pfrl import replay_buffers\nfrom pfrl.q_functions import DistributionalDuelingDQN\n\n\ndef phi(x):\n    # Feature extractor\n    return np.asarray(x, dtype=np.float32) \/ 255\n\ndef make_model():\n    global device\n    # Q_function\n    n_atoms = 51\n    v_max = 10\n    v_min = -10\n    obs_n_channels = 4\n    n_actions = 19\n    q_func = DistributionalDuelingDQN(n_actions, n_atoms, v_min, v_max, obs_n_channels)\n\n    # Noisy nets\n    pnn.to_factorized_noisy(q_func, sigma_scale=0.5)\n\n    # Turn off explorer\n    explorer = explorers.Greedy()\n\n    # Use the same hyper parameters as https:\/\/arxiv.org\/abs\/1710.02298\n    opt = torch.optim.Adam(q_func.parameters(), 6.25e-5, eps=1.5 * 10 ** -4)\n\n    # Prioritized Replay\n    # Anneal beta from beta0 to 1 throughout training\n    update_interval = 4\n    betasteps = 5 * 10 ** 7 \/ update_interval\n    rbuf = replay_buffers.PrioritizedReplayBuffer(\n            10 ** 6,\n            alpha=0.5,\n            beta0=0.4,\n            betasteps=betasteps,\n            num_steps=3,\n            normalize_by_max=\"memory\",\n        )\n\n\n    # prepare agent\n    model = CategoricalDoubleDQN(\n            q_func,\n            opt,\n            rbuf,\n            gpu=-1,  \n            gamma=0.99,\n            explorer=explorer,\n            minibatch_size=32,\n            replay_start_size=2 * 10 ** 4,\n            target_update_interval=32000,\n            update_interval=update_interval,\n            batch_accumulator=\"mean\",\n            phi=phi,\n        )\n    \n    model.load(\".\/kaggle_simulations\/agent\/100000_finish\")\n    # model.load(\".\/kaggle_simulations\/agent\/best\")\n    return model.model.to(device)\n\n\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\nmodel = make_model()\n\ndef agent(obs):\n    global device\n    global model\n    \n    # Get observations for the first (and only one) player we control.\n    obs = obs['players_raw'][0]\n    # Agent we trained uses Super Mini Map (SMM) representation.\n    # See https:\/\/github.com\/google-research\/seed_rl\/blob\/master\/football\/env.py for details.\n    obs = observation_preprocessing.generate_smm([obs])[0]\n    # preprocess for obs\n    obs = cv2.resize(obs, (84,84))           # resize\n    obs = np.transpose(obs, [2,0,1])         # transpose to chw\n    obs = torch.tensor(obs).float()          # to tensor\n    obs = torch.unsqueeze(obs,0).to(device)  # add batch\n\n    action = model(obs)\n    action = action.greedy_actions.cpu().numpy()\n    return list(action)","91c36379":"from kaggle_environments import make\nfrom kaggle_simulations.agent import main\nenv = make(\"football\", configuration={\"save_video\": True, \"scenario_name\": \"11_vs_11_kaggle\", \"running_in_notebook\": True})\nobs = env.state[0][\"observation\"]\naction = main.agent(obs)\nprint(action)","bf968ce4":"from kaggle_environments import make\nenv = make(\"football\", configuration={\"save_video\": True, \"scenario_name\": \"11_vs_11_kaggle\", \"running_in_notebook\": True}, debug=True)\noutput = env.run([\".\/kaggle_simulations\/agent\/main.py\", \"run_right\"])[-1]\nprint('Left player: action = %s, reward = %s, status = %s, info = %s' % (output[0][\"action\"], output[0]['reward'], output[0]['status'], output[0]['info']))\nprint('Right player: action = %s, reward = %s, status = %s, info = %s' % (output[1][\"action\"], output[1]['reward'], output[1]['status'], output[1]['info']))\nenv.render(mode=\"human\", width=800, height=600)","82dec32b":"# Prepare a submision package containing trained model and the main execution logic.\n!cd .\/kaggle_simulations\/agent && tar -czvf \/kaggle\/working\/submit.tar.gz main.py best","498939f6":"It looks like I can create a submit agent, but I failed to submit by error.  \nPlease teach me any idea. Thank you!  ","292ded12":"## Environment","7a8e1bb5":"## Config","faccfccd":"## Submission","45f97fd7":"## Train","84b37d4b":"### check submission agent","02004568":"## About\nIn this notebook, I will introduce [Rainbow](https:\/\/arxiv.org\/abs\/1710.02298) and [PFRL](https:\/\/github.com\/pfnet\/pfrl ).  \n### Rainbow\nRainbow is an Reinforcement Learning(RL) algorithm that extends the DQN. It has performed well in Atari games (benchmarking for RL) **only single GPU**. Modern high-performance RL algorithms(Ape-X, R2D2, etc) are mainly distributed RL method that use multiple distributed environments, and I guess distributed RL is effective approach in this competition. But these are little difficult to run well on the kaggle notebook and google colab environment because these approach need massively distributed computing resource. So, I try to use Rainbow-DQN in this notebook.  \n\nRainbow consists of the following seven elements.\n- DQN\n- Double D-learnig\n- Prioritized replay\n- Dueling networks\n- Multi-step learning\n- Distributional RL\n- Noisy nets\n\nPlease click [here](https:\/\/arxiv.org\/abs\/1710.02298) for details.\nI will write these components by PFRL.\n\n### PFRL\n<div align=\"center\"><img src=\"https:\/\/raw.githubusercontent.com\/pfnet\/pfrl\/master\/assets\/PFRL.png\" width=30%\/><\/div>\nPFRL is a deep reinforcement learning library that implements various state-of-the-art deep reinforcement algorithms in Python using PyTorch.  \n\nMost of the published notebooks are written in keras(TensorFlow). But there are many people who would like to use PyTorch. So, I propose PFRL. In this notebook, there is not much PyTorch-specific code since I use existing modules. But we can rewrite by PyTorch in detail if you want. \n  \nPlease check [here](https:\/\/github.com\/pfnet\/pfrl) for details. \n\n","db0f2235":"In this notebook, I set 100K steps(spend about 1 hour).  \nBut it is not enough to improve the agent. To improve the agent, we will need to try set high value step, high value replay buffers and consider using distributed RL to reduce computation time.","3b87c2cf":"## Model","5c82ef65":"## Install"}}