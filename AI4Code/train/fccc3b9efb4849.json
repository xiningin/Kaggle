{"cell_type":{"945f71c5":"code","dce37443":"code","08f7ae91":"code","401642a3":"code","b987122b":"code","55645dc3":"code","7d1a3465":"code","3d2a6ba0":"code","f7bc4a80":"code","5f896859":"code","700a2c47":"code","a898b827":"code","9c667550":"code","b6f445eb":"code","099ad761":"code","dd143696":"code","19e8126e":"code","f9e53cde":"code","484a5bc0":"code","403b053c":"code","e4cc7da7":"code","a7fcb94e":"code","e7aba277":"code","ae83fdef":"code","e38da1d1":"code","9443406e":"code","5b35dd02":"code","f3d19b1f":"code","a0767e46":"code","60e4aac2":"code","13ed6d3a":"code","7522f59e":"code","7d2b6ee5":"code","e7c0231b":"code","e503cf3e":"code","e2ae7fe9":"code","6cf37268":"code","c062bd7b":"code","79f8ab1a":"code","7dfde64f":"code","b91efb07":"code","55954468":"code","cb445641":"code","2b44fbb6":"code","132ff68d":"code","42ab815d":"code","4b6b4a4a":"code","94c37145":"code","b334d954":"code","01d7523a":"code","7d5d1762":"code","e09ee6ba":"code","de7d49b5":"code","42f5afb8":"code","c0194190":"code","9f7776a7":"code","c14005ed":"code","1a181785":"code","056d8aa6":"code","959a5f9f":"code","7c900919":"code","599cf172":"code","165ef425":"code","814122eb":"code","c86dd4da":"code","861d8505":"code","69ea6eac":"code","9dcc8244":"code","b55ae325":"code","b0fd1390":"code","01817863":"code","a66868bc":"code","c25c148c":"code","a7581c66":"code","b9484aef":"code","db270d05":"code","b228e8f1":"code","2b6bf7f7":"code","8b4fbcad":"code","8c28dfe3":"code","319d6106":"code","24e8acbd":"code","f0209772":"code","1a50107f":"code","2bcc0ec2":"code","677b0c06":"code","c6041da1":"code","fdc412e9":"code","668a73cb":"markdown","2b24e048":"markdown","0399cff7":"markdown","83ce6363":"markdown","bcb30b3d":"markdown","f913d061":"markdown","0bcaa680":"markdown","e385d6c4":"markdown","4bb13241":"markdown","45c8ebd1":"markdown","00564761":"markdown","3fe0868c":"markdown","94d4f66f":"markdown","00ea4fbf":"markdown","ec7d8490":"markdown","1d918d1c":"markdown","bd5d771a":"markdown","cc30dc8f":"markdown","ac33ee9e":"markdown","8972ac5d":"markdown","c3f46eb2":"markdown","7a8787ae":"markdown","389ba7a4":"markdown","97cd3b16":"markdown","25e9bbe6":"markdown","be30b975":"markdown","bb8f6125":"markdown","6b57029a":"markdown","1f635b0e":"markdown","7a86c1f8":"markdown","9b734667":"markdown","03fe7e2c":"markdown","49e34074":"markdown","b7fd97a2":"markdown","29ee873e":"markdown","0369010a":"markdown","fe820965":"markdown","6f371fdb":"markdown","590e8c2c":"markdown","ae91dad2":"markdown","21665101":"markdown","8d480658":"markdown","870ada98":"markdown","7888f783":"markdown","b17765c8":"markdown"},"source":{"945f71c5":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport xgboost as xgb\nimport sklearn\nimport os\nprint(os.listdir(\"..\/input\"))","dce37443":"for i in [np,pd,sns,xgb, sklearn]:\n    print(i.__version__)","08f7ae91":"import subprocess\nfrom IPython.display import Image\nfrom collections import Counter\nfrom sklearn.datasets import make_classification\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import log_loss, accuracy_score\n\nfrom sklearn.tree import DecisionTreeClassifier, export_graphviz\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\n\nseed = 104","401642a3":"X,y= make_classification(n_samples=1000, n_features=20, n_informative=8, n_redundant=3, n_repeated=2, random_state=seed)\n\nX_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.2, random_state=seed)","b987122b":"pd.DataFrame(X_train).head(3)","55645dc3":"pd.Series(y_train).head(3)","7d1a3465":"print(\"Train label distribution:\")\nprint(Counter(y_train))\n\nprint(\"\\nTest label distribution:\")\nprint(Counter(y_test))","3d2a6ba0":"decision_tree = DecisionTreeClassifier(random_state=seed).fit(X_train, y_train)                # TRAINING THE CLASSIFIER MODEL\n\ndecision_tree_y_pred = decision_tree.predict(X_test)                                           # PREDICT THE OUTPUT\ndecision_tree_y_pred_prob = decision_tree.predict_proba(X_test)\n\ndecision_tree_accuracy = accuracy_score(y_test, decision_tree_y_pred)\ndecision_tree_logloss = log_loss(y_test, decision_tree_y_pred_prob)\n\nprint(\"== Decision Tree ==\")\nprint(\"Accuracy: {0:.2f}\".format(decision_tree_accuracy))\nprint(\"Log loss: {0:.2f}\".format(decision_tree_logloss))\nprint(\"Number of nodes created: {}\".format(decision_tree.tree_.node_count))","f7bc4a80":"\nprint('True labels:')\nprint(y_test[:35,])\nprint('\\nPredicted labels:')\nprint(decision_tree_y_pred[:35,])\nprint('\\nPredicted probabilities:')\nprint(decision_tree_y_pred_prob[:5,])\n","5f896859":"adaboost = AdaBoostClassifier(base_estimator=DecisionTreeClassifier(max_depth=1), algorithm='SAMME', n_estimators=1000, random_state=seed)\n\nadaboost.fit(X_train, y_train)\n\nadaboost_y_pred = adaboost.predict(X_test)\nadaboost_y_pred_proba = adaboost.predict_proba(X_test)\n\nadaboost_accuracy = accuracy_score(y_test, adaboost_y_pred)\nadaboost_logloss = log_loss(y_test, adaboost_y_pred_proba)\n\nprint(\"== AdaBoost ==\")\nprint(\"Accuracy: {0:.2f}\".format(adaboost_accuracy))\nprint(\"Log loss: {0:.2f}\".format(adaboost_logloss))","700a2c47":"print('True labels:')\nprint(y_test[:25,])\nprint('\\nPredicted labels:')\nprint(adaboost_y_pred[:25,])\nprint('\\nPredicted probabilities:')\nprint(adaboost_y_pred_proba[:5,])","a898b827":"print(\"Error: {0:.2f}\".format(adaboost.estimator_errors_[0]))\nprint(\"Tree importance: {0:.2f}\".format(adaboost.estimator_weights_[0]))","9c667550":"gbc = GradientBoostingClassifier(max_depth=1, n_estimators=1000, warm_start=True, random_state=seed)\n\ngbc.fit(X_train, y_train)\n\ngbc_y_pred = gbc.predict(X_test)\ngbc_y_pred_proba = gbc.predict_proba(X_test)\n\ngbc_accuracy = accuracy_score(y_test, gbc_y_pred)\ngbc_logloss = log_loss(y_test, gbc_y_pred_proba)\n\nprint(\"== Gradient Boosting ==\")\nprint(\"Accuracy: {0:.2f}\".format(gbc_accuracy))\nprint(\"Log loss: {0:.2f}\".format(gbc_logloss))","b6f445eb":"print('True labels:')\nprint(y_test[:25,])\nprint('\\nPredicted labels:')\nprint(gbc_y_pred[:25,])\nprint('\\nPredicted probabilities:')\nprint(gbc_y_pred_proba[:5,])","099ad761":"dtrain = xgb.DMatrix('..\/input\/agaricus_train.txt')\ndtest = xgb.DMatrix('..\/input\/agaricus_test.txt')","dd143696":"print([w for w in dir(xgb) if not w.startswith( \"_\")])","19e8126e":"print(\"Train dataset contains {0} rows and {1} columns\".format(dtrain.num_row(), dtrain.num_col()))\nprint(\"Test dataset contains {0} rows and {1} columns\".format(dtest.num_row(), dtest.num_col()))","f9e53cde":"print(\"Train possible labels: \", np.unique(dtrain.get_label()))\nprint(\"\\nTest possible labels: \", np.unique(dtest.get_label()))","484a5bc0":"params = {\n    'objective':'binary:logistic',\n    'max_depth':2,\n    'silent':1,\n    'eta':1\n}\n\nnum_rounds = 5","403b053c":"bst = xgb.train(params=params, dtrain=dtrain, num_boost_round=num_rounds)","e4cc7da7":"watchlist = [(dtest,'test'), (dtrain,'train')]\nbst = xgb.train(params, dtrain, num_rounds, watchlist)","a7fcb94e":"preds_prob = bst.predict(dtest)\npreds_prob","e7aba277":"labels = dtest.get_label()\npreds = preds_prob > 0.5 # threshold\ncorrect = 0\n\nfor i in range(len(preds)):\n    if (labels[i] == preds[i]):\n        correct += 1\n\nprint('Predicted correctly: {0}\/{1}'.format(correct, len(preds)))\nprint('Error: {0:.4f}'.format(1-correct\/len(preds)))","ae83fdef":"dtrain = xgb.DMatrix('..\/input\/agaricus_train.txt')\ndtest = xgb.DMatrix('..\/input\/agaricus_test.txt')","e38da1d1":"params = {\n    'objective':'binary:logistic',\n    'max_depth':1,\n    'silent':1,\n    'eta':0.5\n}\n\nnum_rounds = 5","9443406e":"watchlist  = [(dtest,'test'), (dtrain,'train')] # native interface only\nbst = xgb.train(params, dtrain, num_rounds, watchlist)","5b35dd02":"trees_dump = bst.get_dump(fmap='..\/input\/featmap.txt', with_stats=True)\n\nfor tree in trees_dump:\n    print(tree)","f3d19b1f":"xgb.plot_importance(bst, importance_type='gain', xlabel='Gain')","a0767e46":"xgb.plot_importance(bst)","60e4aac2":"importances = bst.get_fscore()\nimportances","13ed6d3a":"# create df\nimportance_df = pd.DataFrame({'Splits': list(importances.values()),'Feature': list(importances.keys())})\nimportance_df.sort_values(by='Splits', inplace=True)\nimportance_df.plot(kind='barh', x='Feature', figsize=(8,6), color='orange')","7522f59e":"from sklearn.model_selection import validation_curve\nfrom sklearn.datasets import load_svmlight_file\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.datasets import make_classification\nfrom xgboost.sklearn import XGBClassifier\nfrom scipy.sparse import vstack\n\nseed = 123\nnp.random.seed(seed)","7d2b6ee5":"print([w for w in dir(xgb.sklearn) if not w.startswith('_')])","e7c0231b":"#X, y = make_classification(n_samples=1000, n_features=20, n_informative=8, n_redundant=3, n_repeated=2, random_state=seed)\nseed = 2024\nX, y = make_classification(n_samples=10000, n_features=30, n_informative=10, n_redundant=5, n_repeated=3, random_state=seed)","e503cf3e":"skf = StratifiedKFold(n_splits=10, shuffle=True, random_state=seed)\ncv = skf.split(X,y)\ncv ","e2ae7fe9":"default_params = {\n    'objective': 'binary:logistic',\n    'max_depth': 1,\n    'learning_rate': 0.3,\n    'silent': 1.0\n}\n\nn_estimators_range = np.linspace(1, 200, 10).astype('int')\n\ntrain_scores, test_scores = validation_curve(\n    XGBClassifier(**default_params),\n    X, y,\n    param_name = 'n_estimators',\n    param_range = n_estimators_range,\n    cv=cv,\n    scoring='accuracy'\n)","6cf37268":"train_scores_mean = np.mean(train_scores, axis=1)\ntrain_scores_std = np.std(train_scores, axis=1)\ntest_scores_mean = np.mean(test_scores, axis=1)\ntest_scores_std = np.std(test_scores, axis=1)\n\nfig = plt.figure(figsize=(10, 6), dpi=100)\n\nplt.title(\"Validation Curve with XGBoost (eta = 0.3)\")\nplt.xlabel(\"number of trees\")\nplt.ylabel(\"Accuracy\")\nplt.ylim(0.7, 1.1)\n\nplt.plot(n_estimators_range,\n             train_scores_mean,\n             label=\"Training score\",\n             color=\"r\")\n\nplt.plot(n_estimators_range,\n             test_scores_mean, \n             label=\"Cross-validation score\",\n             color=\"g\")\n\nplt.fill_between(n_estimators_range, \n                 train_scores_mean - train_scores_std,\n                 train_scores_mean + train_scores_std, \n                 alpha=0.2, color=\"r\")\n\nplt.fill_between(n_estimators_range,\n                 test_scores_mean - test_scores_std,\n                 test_scores_mean + test_scores_std,\n                 alpha=0.2, color=\"g\")\n\nplt.axhline(y=1, color='k', ls='dashed')\n\nplt.legend(loc=\"best\")\nplt.show()\n\ni = np.argmax(test_scores_mean)\nprint(\"Best cross-validation result ({0:.2f}) obtained for {1} trees\".format(test_scores_mean[i], n_estimators_range[i]))","c062bd7b":"default_params = {\n    'objective': 'binary:logistic',\n    'max_depth': 2, # changed\n    'learning_rate': 0.3,\n    'silent': 1.0,\n    'colsample_bytree': 0.6, # added\n    'subsample': 0.7 # added\n}\n\nn_estimators_range = np.linspace(1, 200, 10).astype('int')\n\ntrain_scores, test_scores = validation_curve(\n    XGBClassifier(**default_params),\n    X, y,\n    param_name = 'n_estimators',\n    param_range = n_estimators_range,\n    cv=cv,\n    scoring='accuracy'\n)","79f8ab1a":"train_scores_mean = np.mean(train_scores, axis=1)\ntrain_scores_std = np.std(train_scores, axis=1)\ntest_scores_mean = np.mean(test_scores, axis=1)\ntest_scores_std = np.std(test_scores, axis=1)\n\nfig = plt.figure(figsize=(10, 6), dpi=100)\n\nplt.title(\"Validation Curve with XGBoost (eta = 0.3)\")\nplt.xlabel(\"number of trees\")\nplt.ylabel(\"Accuracy\")\nplt.ylim(0.7, 1.1)\n\nplt.plot(n_estimators_range,\n             train_scores_mean,\n             label=\"Training score\",\n             color=\"r\")\n\nplt.plot(n_estimators_range,\n             test_scores_mean, \n             label=\"Cross-validation score\",\n             color=\"g\")\n\nplt.fill_between(n_estimators_range, \n                 train_scores_mean - train_scores_std,\n                 train_scores_mean + train_scores_std, \n                 alpha=0.2, color=\"r\")\n\nplt.fill_between(n_estimators_range,\n                 test_scores_mean - test_scores_std,\n                 test_scores_mean + test_scores_std,\n                 alpha=0.2, color=\"g\")\n\nplt.axhline(y=1, color='k', ls='dashed')\n\nplt.legend(loc=\"best\")\nplt.show()\n\ni = np.argmax(test_scores_mean)\nprint(\"Best cross-validation result ({0:.2f}) obtained for {1} trees\".format(test_scores_mean[i], n_estimators_range[i]))","7dfde64f":"from xgboost.sklearn import XGBClassifier\n\nfrom sklearn.model_selection import GridSearchCV, RandomizedSearchCV\nfrom sklearn.datasets import make_classification\nfrom sklearn.model_selection import StratifiedKFold\n\nfrom scipy.stats import randint, uniform\n\n# reproducibility\nseed = 342\nnp.random.seed(seed)","b91efb07":"seed = 2024\nX, y = make_classification(n_samples=1000, n_features=30, n_informative=10,\n                           n_redundant=5, n_repeated=3, random_state=seed)\nskf = StratifiedKFold(n_splits=10, shuffle=True, random_state=seed)\ncv = skf.split(X,y)","55954468":"print(X.shape)\nprint(y.shape)\nprint(X[:2,:])","cb445641":"type(train_index)","2b44fbb6":"params_grid = {\n    'max_depth': [1, 2, 3],\n    'n_estimators': [5, 10, 25, 50],\n    'learning_rate': np.linspace(1e-16, 1, 3)\n}","132ff68d":"params_fixed = {\n    'objective': 'binary:logistic',\n    'silent': 1\n}","42ab815d":"bst_grid = GridSearchCV(estimator=XGBClassifier(**params_fixed, seed=seed),param_grid=params_grid,cv=cv,scoring='accuracy')","4b6b4a4a":"bst_grid.fit(X, y)","94c37145":"bst_grid.cv_results_","b334d954":"print(\"Best accuracy obtained: {0}\".format(bst_grid.best_score_))\nprint(\"Parameters:\")\nfor key, value in bst_grid.best_params_.items():\n    print(\"\\t{}: {}\".format(key, value))","01d7523a":"params_dist_grid = {\n    'max_depth': [1, 2, 3, 4],\n    'gamma': [0, 0.5, 1],\n    'n_estimators': randint(1, 1001), # uniform discrete random distribution\n    'learning_rate': uniform(), # gaussian distribution\n    'subsample': uniform(), # gaussian distribution\n    'colsample_bytree': uniform() # gaussian distribution\n}","7d5d1762":"skf = StratifiedKFold(n_splits=10, shuffle=True, random_state=seed)\ncv = skf.split(X,y)\n\nrs_grid = RandomizedSearchCV(\n    estimator=XGBClassifier(**params_fixed, seed=seed),\n    param_distributions=params_dist_grid,\n    n_iter=10,\n    cv=cv,\n    scoring='accuracy',\n    random_state=seed\n)","e09ee6ba":"rs_grid.fit(X, y)","de7d49b5":"print([w for w in dir(rs_grid) if not w.startswith('_')])","42f5afb8":"print(rs_grid.best_estimator_)\nprint(rs_grid.best_params_)\nprint(rs_grid.best_score_)\n#print(rs_grid.score)","c0194190":"params = {\n    'objective':'binary:logistic',\n    'max_depth':1,\n    'silent':1,\n    'eta':0.5\n}\n\nnum_rounds = 5\nwatchlist  = [(dtest,'test'), (dtrain,'train')]\n\nbst = xgb.train(params, dtrain, num_rounds, watchlist)","9f7776a7":"params['eval_metric'] = 'logloss'\nbst = xgb.train(params, dtrain, num_rounds, watchlist)","c14005ed":"params['eval_metric'] = ['logloss', 'auc']\nbst = xgb.train(params, dtrain, num_rounds, watchlist)","1a181785":"# custom evaluation metric\ndef misclassified(pred_probs, dtrain):\n    labels = dtrain.get_label() # obtain true labels\n    preds = pred_probs > 0.5 # obtain predicted values\n    return 'misclassified', np.sum(labels != preds)","056d8aa6":"bst = xgb.train(params, dtrain, num_rounds, watchlist, feval=misclassified, maximize=False)","959a5f9f":"evals_result = {}\nbst = xgb.train(params, dtrain, num_rounds, watchlist, feval=misclassified, maximize=False, evals_result=evals_result)","7c900919":"from pprint import pprint\npprint(evals_result)","599cf172":"params['eval_metric'] = 'error'\nnum_rounds = 1500\n\nbst = xgb.train(params, dtrain, num_rounds, watchlist, early_stopping_rounds=10)","165ef425":"print(\"Booster best train score: {}\".format(bst.best_score))\nprint(\"Booster best iteration: {}\".format(bst.best_iteration))\nprint(\"Booster best number of trees limit: {}\".format(bst.best_ntree_limit))","814122eb":"num_rounds = 10 # how many estimators\nhist = xgb.cv(params, dtrain, num_rounds, nfold=10, metrics={'error'}, seed=seed)\nhist","c86dd4da":"# create valid dataset\nnp.random.seed(seed)\n\ndata_v = np.random.rand(10,5) # 10 entities, each contains 5 features\ndata_v","861d8505":"# add some missing values\ndata_m = np.copy(data_v)\n\ndata_m[2, 3] = np.nan\ndata_m[0, 1] = np.nan\ndata_m[0, 2] = np.nan\ndata_m[1, 0] = np.nan\ndata_m[4, 4] = np.nan\ndata_m[7, 2] = np.nan\ndata_m[9, 1] = np.nan\n\ndata_m","69ea6eac":"np.random.seed(seed)\n\nlabel = np.random.randint(2, size=10) # binary target\nlabel","9dcc8244":"# specify general training parameters\nparams = {\n    'objective':'binary:logistic',\n    'max_depth':1,\n    'silent':1,\n    'eta':0.5\n}\n\nnum_rounds = 5","b55ae325":"dtrain_v = xgb.DMatrix(data_v, label=label)\n\nxgb.cv(params, dtrain_v, num_rounds, seed=seed)\n","b0fd1390":"dtrain_m = xgb.DMatrix(data_m, label=label, missing=np.nan)\n\nxgb.cv(params, dtrain_m, num_rounds, seed=seed)","01817863":"params = {\n    'objective': 'binary:logistic',\n    'max_depth': 1,\n    'learning_rate': 0.5,\n    'silent': 1.0,\n    'n_estimators': 5\n}\n\nclf = XGBClassifier(**params)\nclf","a66868bc":"from sklearn.model_selection import cross_val_score\ncross_val_score(clf, data_v, label, cv=2, scoring='accuracy')","c25c148c":"cross_val_score(clf, data_m, label, cv=2, scoring='accuracy')","a7581c66":"from sklearn.datasets import make_classification\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score\nfrom sklearn.model_selection import train_test_split\n\n# reproducibility\nseed = 123","b9484aef":"X, y = make_classification(\n    n_samples=200,\n    n_features=5,\n    n_informative=3,\n    n_classes=2,\n    weights=[.9, .1],\n    shuffle=True,\n    random_state=seed\n)\n\nprint('There are {} positive instances.'.format(y.sum()))","db270d05":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, stratify=y, random_state=seed)\n\nprint('Total number of postivie train instances: {}'.format(y_train.sum()))\nprint('Total number of positive test instances: {}'.format(y_test.sum()))","b228e8f1":"dtrain = xgb.DMatrix(X_train, label=y_train)\ndtest = xgb.DMatrix(X_test)","2b6bf7f7":"params = {\n    'objective':'binary:logistic',\n    'max_depth':1,\n    'silent':1,\n    'eta':1\n}\n\nnum_rounds = 15","8b4fbcad":"bst = xgb.train(params, dtrain, num_rounds)\ny_test_preds = (bst.predict(dtest) > 0.5).astype('int')\n","8c28dfe3":"pd.crosstab(\n    pd.Series(y_test, name='Actual'),\n    pd.Series(y_test_preds, name='Predicted'),\n    margins=True\n)","319d6106":"print('Accuracy: {0:.2f}'.format(accuracy_score(y_test, y_test_preds)))\nprint('Precision: {0:.2f}'.format(precision_score(y_test, y_test_preds)))\nprint('Recall: {0:.2f}'.format(recall_score(y_test, y_test_preds)))","24e8acbd":"weights = np.zeros(len(y_train))\nweights[y_train == 0] = 1\nweights[y_train == 1] = 5\n\ndtrain = xgb.DMatrix(X_train, label=y_train, weight=weights) # weights added\ndtest = xgb.DMatrix(X_test)","f0209772":"bst = xgb.train(params, dtrain, num_rounds)\ny_test_preds = (bst.predict(dtest) > 0.5).astype('int')\n\npd.crosstab(\n    pd.Series(y_test, name='Actual'),\n    pd.Series(y_test_preds, name='Predicted'),\n    margins=True\n)","1a50107f":"print('Accuracy: {0:.2f}'.format(accuracy_score(y_test, y_test_preds)))\nprint('Precision: {0:.2f}'.format(precision_score(y_test, y_test_preds)))\nprint('Recall: {0:.2f}'.format(recall_score(y_test, y_test_preds)))","2bcc0ec2":"dtrain = xgb.DMatrix(X_train, label=y_train)\ndtest = xgb.DMatrix(X_test)","677b0c06":"train_labels = dtrain.get_label()\n\nratio = float(np.sum(train_labels == 0)) \/ np.sum(train_labels == 1)\nparams['scale_pos_weight'] = ratio","c6041da1":"bst = xgb.train(params, dtrain, num_rounds)\ny_test_preds = (bst.predict(dtest) > 0.5).astype('int')\n\npd.crosstab(\n    pd.Series(y_test, name='Actual'),\n    pd.Series(y_test_preds, name='Predicted'),\n    margins=True\n)","fdc412e9":"print('Accuracy: {0:.2f}'.format(accuracy_score(y_test, y_test_preds)))\nprint('Precision: {0:.2f}'.format(precision_score(y_test, y_test_preds)))\nprint('Recall: {0:.2f}'.format(recall_score(y_test, y_test_preds)))","668a73cb":"In the experiment first we will create a valid DMatrix (with all values), see if it works ok, and then repeat the process with lacking one.\n\n","2b24e048":"## Gradient Boosted Trees\nLet's construct a gradient boosted tree consiting of 1000 trees where each successive one will be created with gradient optimization. Again we are going to leave most parameters with their default values, specifiy only maximum depth of the tree to 1 (again decision stumps), and setting warm start for more intelligent computations.","0399cff7":"Assume that we will create 15 decision tree stumps, solving binary classification problem, where each next one will be train very aggressively.\n\nThese parameters will also be used in consecutive examples.","83ce6363":"# Single Decision Tree\nThis code will create a single decision tree, fit it using training data and evaluate the results using test sample.","bcb30b3d":"## Grid Search \nIn grid-search we start by defining a dictionary holding possible parameter values we want to test. All combinations will be evaluted.","f913d061":"Now, we can look at all obtained scores, and try to manually see what matters and what not. A quick glance looks that the largeer n_estimators then the accuracy is higher.","0bcaa680":"Also keep in mind that train() will return a model from the last iteration, not the best one.\n\n","e385d6c4":"## Prepare Data\nIn all examples we will be dealing with binary classification. Generate 20 dimensional artificial dataset with 1000 samples, where 8 features holding information, 3 are redundant and 2 repeated.","4bb13241":"It looks like the algorithm works also with missing values.\n\nIn XGBoost chooses a soft way to handle missing values.\n\nWhen using a feature with missing values to do splitting, XGBoost will assign a direction to the missing values instead of a numerical value.\n\nSpecifically, XGBoost guides all the data points with missing values to the left and right respectively, then choose the direction with a higher gain with regard to the objective.\n\n### Sklearn wrapper\nThe following section shows how to validate the same behaviour using Sklearn interface.\n\nBegin with defining parameters and creating an estimator object.","45c8ebd1":"For each split we are getting the following details:\n\n<ol>which feature was used to make split,<\/ol>\n<ol>possible choices to make (branches)<\/ol>\n<ol>gain which is the actual improvement in accuracy brough by that feature. The idea is that before adding a new split on a feature X to the branch there was some wrongly classified elements, after adding the split on this feature, there are two new branches, and each of these branch is more accurate (one branch saying if your observation is on this branch then it should be classified as 1, and the other branch saying the exact opposite),<\/ol>\n<ol>cover measuring the relative quantity of observations concerned by that feature<\/ol>\n\n## Plotting\nHopefully there are better ways to figure out which features really matter. We can use built-in function plot_importance that will create a plot presenting most important features due to some criterias. We will analyze the impact of each feature for all splits and all trees and visualize results.\n\nSee which feature provided the most gain:","00564761":"Notice that:\n\n<ol>by default we get a pandas data frame object (can be changed with as_pandas param),<\/ol>\n<ol>metrics are passed as an argument (muliple values are allowed),<\/ol>\n<ol>we can use own evaluation metrics (param feval and maximize),<\/ol>\n<ol>we can use early stopping feature (param early_stopping_rounds)<\/ol>","3fe0868c":"## Creating custom evaluation metric\nIn order to create our own evaluation metric, the only thing needed to do is to create a method taking two arguments - predicted probabilities and DMatrix object holding training data.\n\nIn this example our classification metric will simply count the number of misclassified examples assuming that classes are positive. You can change this threshold if you want more certainty.\n\nThe algorithm is getting better when the number of misclassified examples is getting lower. Remember to also set the argument maximize=False while training.","94d4f66f":"You can see that even though the params dictionary is holding eval_metric key these values are being ignored and overwritten by feval.\n\n## Extracting the evaluation results\nYou can get evaluation scores by declaring a dictionary for holding values and passing it as a parameter for evals_result argument.","00ea4fbf":"Train the booster and make predictions.","ec7d8490":"## Early stopping\nThere is a nice optimization trick when fitting multiple trees.\n\nYou can train the model until the validation score stops improving. Validation error needs to decrease at least every early_stopping_rounds to continue training. This approach results in simpler model, because the lowest number of trees will be found (simplicity).\n\nIn the following example a total number of 1500 trees is to be created, but we are telling it to stop if the validation score does not improve for last ten iterations.","1d918d1c":"## Bias-variance Trade-off","bd5d771a":"Calculate the ratio between both classes and assign it to a parameter.","cc30dc8f":"We can see two things:\n\n1. the log loss score is not very promising (due to the fact that leaves in decision tree outputs either 0 or 1 as probability which is heaviliy penalized in case of errors, but the accuracy score is quite decent,\n2. the tree is complicated (large number of nodes)\n\nYou can inspect first few predicted outputs, and see that only 2 instances out of 5 were classified correctly.","ac33ee9e":"You can also use multiple evaluation metrics at one time","8972ac5d":"Show the validation curve plot\n\n","c3f46eb2":"Create a GridSearchCV estimator. We will be looking for combination giving the best accuracy.","7a8787ae":" ## Evaluation Metric\n What is already available?\nThere are already some predefined metrics availabe. You can use them as the input for the eval_metric parameter while training the model.\n\n<ol>rmse - root mean square error,<\/ol>\n<ol>mae - mean absolute error,<\/ol>\n<ol>logloss - negative log-likelihood<\/ol>\n<ol>error - binary classification error rate. It is calculated as #(wrong cases)\/#(all cases). Treat predicted values with probability $p &gt; 0.5$ as positive,<\/ol>\n<ol>merror - multiclass classification error rate. It is calculated as #(wrong cases)\/#(all cases),<\/ol>\n<ol>auc - area under curve,<\/ol>\n<ol>ndcg - normalized discounted cumulative gain,<\/ol>\n<ol>map - mean average precision<\/ol>\nBy default an error metric will be used.","389ba7a4":"## Handling Imbalanced Datasets","97cd3b16":"Add a dictionary to fixed parameters","25e9bbe6":"### Filter them manually to get the best combination","be30b975":"The obtained results are obviously the best of all presented algorithm. We have obtained most accurate algorithm giving more sensible predictions about class probabilities.\n\n","bb8f6125":"## Specify training parameters\nLet's make the following assuptions and adjust algorithm parameters to it:\n\n<ol>we are dealing with binary classification problem ('objective':'binary:logistic'),<\/ol>\n<ol>we want shallow single trees with no more than 2 levels ('max_depth':2),<\/ol>\n<ol>we don't any oupout ('silent':1),<\/ol>\n<ol>we want algorithm to learn fast and aggressively ('eta':1),<\/ol>\n<ol>we want to iterate only 5 rounds<\/ol>","6b57029a":"Intuitively we know that the foucs should be on finding positive samples. First results are very promising (94% accuracy - wow), but deeper analysis show that the results are biased towards majority class - we are very poor at predicting the actual label of positive instances. That is called an accuracy paradox.\n\n## Custom weights\nTry to explicitly tell the algorithm what important using relative instance weights. Let's specify that positive instances have 5x more weight and add this information while creating DMatrix.","1f635b0e":"### Native interface\nIn this case we will check how does the native interface handles missing data. Begin with specifing default parameters","7a86c1f8":"## Make Predicitons","9b734667":"When using early_stopping_rounds parameter resulting model will have 3 additional fields - bst.best_score, bst.best_iteration and bst.best_ntree_limit.","03fe7e2c":"## Cross validating results\nNative package provides an option for cross-validating results (but not as sophisticated as Sklearn package). The next input shows a basic execution. Notice that we are passing only single DMatrix, so it would be good to merge train and test into one object to have more training samples.","49e34074":"You see that we made a trade-off here. We are now able to better classify the minority class, but the overall accuracy and precision decreased. Test multiple weights combinations and see which one works best.\n\n## Use scale_pos_weight parameter\nYou can automate the process of assigning weights manually by calculating the proportion between negative and positive instances and setting it to scale_pos_weight parameter.\n\nLet's reinitialize datasets.","b7fd97a2":"## Using XGBoost","29ee873e":"Initialize RandomizedSearchCV **to randomly pick 10 combinations of parameters**. With this approach you can easily control the number of tested models.","0369010a":"# Adaboost\nBelow we are creating a AdaBoost classifier running on 1000 iterations (1000 trees created). Also we are growing decision node up to first split (they are called decision stumps). We are also going to use SAMME algorithm which is inteneded to work with discrete data (output from base_estimator is 0 or 1)","fe820965":"## Dealing with missing values","6f371fdb":"## Hyper-parameter Tuning","590e8c2c":"Before running the calculations notice that $3*4*3*10=360$ models will be created to test all combinations. You should always have rough estimations about what is going to happen","ae91dad2":"Looking for best parameters is an iterative process. You should start with coarsed-granularity and move to to more detailed values.\n\n## Randomized Grid-Search\nWhen the number of parameters and their values is getting big traditional grid-search approach quickly becomes ineffective. A possible solution might be to randomly pick certain parameters from their distribution. While it's not an exhaustive solution, it's worth giving a shot.\n\nCreate a parameters distribution dictionary:","21665101":"## Baseline model\nIn this approach try to completely ignore the fact that classed are imbalanced and see how it will perform. Create DMatrix for train and test data.","8d480658":"The output obviously doesn't make sense, because the data is completely random.\n\nWhen creating DMatrix holding missing values we have to explicitly tell what denotes that it's missing. Sometimes it might be 0, 999 or others. In our case it's Numpy's NAN. Add missing argument to DMatrix constructor to handle it.","870ada98":"## Spotting most important features","7888f783":"You can see that scalling weight by using scale_pos_weights in this case gives better results that doing it manually. We are now able to perfectly classify all posivie classes (focusing on the real problem). On the other hand the classifier sometimes makes a mistake by wrongly classifing the negative case into positive (producing so called false positives).","b17765c8":"We will divide into 10 stratified folds (the same distibution of labels in each fold) for testing"}}