{"cell_type":{"941a0a46":"code","7080234a":"code","27bdb3b4":"code","a9dfa165":"code","2363c5f7":"code","bbafc9d5":"code","7b0157c3":"code","08eb20b8":"code","55e10858":"code","aa2e1c45":"code","57db2df7":"code","f81d6247":"code","4f9137cd":"code","84a5059c":"code","a3e74a56":"code","99014dfe":"code","5e08c2d2":"code","7a6908b1":"code","f07aeaa7":"code","cf98b6c8":"code","6731f31e":"code","40f0ddfc":"code","0c2eef96":"code","cb977678":"code","7d713d43":"code","a0ba39a7":"code","410238fc":"code","ac0a688c":"code","b8fbd4ca":"code","b4e1ecc8":"code","6b0aa558":"code","9436c2c9":"code","1a36f70d":"code","5b6478ac":"code","0115b143":"code","795e9c19":"code","fe2951ff":"code","5c8284a6":"code","255c57c3":"code","7880ee4e":"code","a9a2bc7f":"code","4d38c9b4":"code","c3899a90":"code","45cdc06a":"code","53251ebe":"code","ae155295":"code","77fb0d42":"code","c7846ce0":"code","3101078f":"code","c9dd86e2":"code","2a1b314b":"code","41c7ff62":"code","d3e35c4e":"code","a9ca0ee2":"code","42cf0a3d":"code","ff25ba70":"code","8406fe78":"code","605c237b":"code","d8a7e886":"code","aa3ad375":"code","63e53e98":"code","bb382158":"code","05d95c55":"code","bebbcbd4":"code","c003c0fa":"code","c58105eb":"code","a76f1f0d":"markdown","734bb2c5":"markdown","00803380":"markdown","4b931c72":"markdown","88574931":"markdown","8cb70da4":"markdown","f16161f3":"markdown","09e0fb60":"markdown","91313cae":"markdown","329c4614":"markdown","266466ba":"markdown","5090084e":"markdown","2a5a7477":"markdown","98377268":"markdown","ad7657d8":"markdown","d27a6871":"markdown","76cdd938":"markdown","a5f00dfa":"markdown","9678c01c":"markdown","4458a438":"markdown","a7707747":"markdown","f7504a9a":"markdown","f44884e2":"markdown","5a158d93":"markdown","c2f2aa60":"markdown","48f67c6b":"markdown","cb8d06a6":"markdown","603aad96":"markdown","ae86b1ca":"markdown","b0bc4feb":"markdown","8552637e":"markdown","c2b4eae9":"markdown","ea3a6191":"markdown","2f5a3c49":"markdown"},"source":{"941a0a46":"%%javascript\nMathJax.Hub.Config({\n    TeX: { equationNumbers: { autoNumber: \"AMS\" } }\n});","7080234a":"import requests\nimport time\n\nimport xml.etree.ElementTree as ET\nimport pandas as pd\nimport numpy as np\nimport math\nimport matplotlib.pyplot as plt\nimport re\nimport glob\nimport random\nimport seaborn as sns\nimport string\n\nfrom IPython.display import clear_output\n\n# Hide warnings\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# http:\/\/www.nltk.org\/howto\/wordnet.html\n\nfrom nltk.corpus import wordnet as wn\nfrom nltk.corpus import stopwords\nfrom nltk.wsd import lesk","27bdb3b4":"# Location of test\/train data files on local computer, data downloaded directly from Stanford source[2]\n#test_dir = '\/Users\/philiposborne\/Documents\/Written Notes\/Learning Notes\/IMDB Reviews\/IMDB Data\/test'\n#train_dir = '\/Users\/philiposborne\/Documents\/Written Notes\/Learning Notes\/IMDB Reviews\/IMDB Data\/train'\n\ndata = pd.read_csv('..\/input\/imdb_master.csv',encoding=\"latin-1\")\n","a9dfa165":"# Select only training data\ndata = data[data['type']=='train'].reset_index(drop=True)","2363c5f7":"print('Number of comments in data:', len(data))\n\ndata = data[0:1000]\n\nprint('Number of comments left in data after removal:', len(data))","bbafc9d5":"train_data = data","7b0157c3":"# Data import written as a function:\n# Replace test and train dir with correct path for file saved on local computer\n# Data files are downloaded from reference link above where main file name is changed to IMDB Data\n\n# This function converts the raw files form the original Stanford source into csv files.\n\"\"\"\ndef IMDB_to_csv(directory):    \n    data = pd.DataFrame()\n    \n    for filename in glob.glob(str(directory)+'\/neg\/*.txt'):\n        with open(filename, 'r',  encoding=\"utf8\") as f:\n            content = f.readlines()\n            content_table = pd.DataFrame({'id':filename.split('_')[0].split('\/')[-1],'rating':filename.split('_')[1].split('.')[0],'pol':'neg', 'text':content})\n        data = data.append(content_table)\n        \n    for filename in glob.glob(str(directory)+'\/pos\/*.txt'):\n        with open(filename, 'r',  encoding=\"utf8\") as f:\n            content = f.readlines()\n            content_table = pd.DataFrame({'id':filename.split('_')[0].split('\/')[-1],'rating':filename.split('_')[1].split('.')[0],'pol':'pos', 'text':content})\n        data = data.append(content_table)\n    data = data.sort_values(['pol','id'])\n    data = data.reset_index(drop=True)\n    #data['rating_norm'] = (data['rating'] - data['rating'].min())\/( data['rating'].max() - data['rating'].min() )\n\n    return(data)\n\ntrain_data = IMDB_to_csv(train_dir)\n\"\"\"","08eb20b8":"train_data.columns = ['id', 'dataset', 'text', 'pol','file']\ntrain_data.head()","55e10858":"train_data['text'][0].split('.')","aa2e1c45":"train_data['text'][0].split('.')[0]","57db2df7":"len(train_data['text'][0].split('.'))","f81d6247":"train_data['text'][0].split('.')[8]","4f9137cd":"train_data_sent = pd.DataFrame()\n\nstart_time = time.time()\nfor index in train_data.index:\n    data_row = train_data.iloc[index,:]\n\n    for sent_id in range(0,len(data_row['text'].split('.'))-1):\n        sentence = data_row['text'].split('.')[sent_id]\n        # Form a row in a dataframe for this setence that captures the words and keeps ids and polarity scores\n        # We must pass an arbitrary index which we then reset to show unique numbers\n        sentence_row = pd.DataFrame({\n                                     'id':data_row['id'],\n                                     'pol':data_row['pol'],\n                                     'sent_id':sent_id,\n                                     'sentence':sentence}, index = [index]) \n        \n        # Form full table that has rows for all sentences\n        train_data_sent = train_data_sent.append(sentence_row)\n    \n    \n    # Outputs progress of main loop, see:\n    clear_output(wait=True)\n    print('Proportion of comments completed:', np.round(index\/len(train_data),4)*100,'%')\n    \nend_time = time.time()\nprint('Total run time = ', np.round(end_time-start_time,2)\/60, ' minutes')\n# Reset index so that each index value is a unique number\ntrain_data_sent = train_data_sent.reset_index(drop=True)\n        ","84a5059c":"train_data_sent.head()","a3e74a56":"train_data_sent['sentence_clean'] = train_data_sent['sentence'].str.replace('[{}]'.format(string.punctuation), '')\ntrain_data_sent['sentence_clean'] = train_data_sent['sentence_clean'].str.lower()\n\ntrain_data_sent['sentence_clean'] = '<s ' + train_data_sent['sentence_clean']\ntrain_data_sent['sentence_clean'] = train_data_sent['sentence_clean'] + ' \/s>'\n\ntrain_data_sent.head()","99014dfe":"text = train_data_sent['sentence_clean']\ntext_list = \" \".join(map(str, text))\ntext_list[0:100]","5e08c2d2":"word_list = pd.DataFrame({'words':text.str.split(' ', expand = True).stack().unique()})","7a6908b1":"word_count_table = pd.DataFrame()\nfor n,word in enumerate(word_list['words']):\n    # Create a list of just the word we are interested in, we use regular expressions so that part of words do not count\n    # e.g. 'ear' would be counted in each appearance of the word 'year'\n    word_count = len(re.findall(' ' + word + ' ', text_list))  \n    word_count_table = word_count_table.append(pd.DataFrame({'count':word_count}, index=[n]))\n    \n    clear_output(wait=True)\n    print('Proportion of words completed:', np.round(n\/len(word_list),4)*100,'%')\n\nword_list['count'] = word_count_table['count']\n# Remove the count for the start and end of sentence notation so \n# that these do not inflate the other probabilities\nword_list['count'] = np.where(word_list['words'] == '<s' , 0,\n                     np.where(word_list['words'] == '\/s>', 0,\n                     word_list['count']))","f07aeaa7":"word_list['prob'] = word_list['count']\/sum(word_list['count'])\nword_list.head()","cf98b6c8":"unigram_table = pd.DataFrame()\n\nstart_time = time.time()\n# Loop through each sentence\n# REMOVE ROW LIMIT FOR FULL RUN\nfor index in train_data_sent[0:200].index:\n    data_row = train_data_sent.iloc[index,:]\n\n    sent_probs = pd.DataFrame()\n    # Go through each word in the sentence, lookup the probability of the word and \n    # then find the mulitplicitive product of all probabilities in the sentence.\n    for n,word in enumerate(data_row['sentence_clean']):\n        sent_probs = sent_probs.append(pd.DataFrame({'prob':word_list[ word_list['words']==word]['prob']}, index = [n]))\n    unigram = sent_probs['prob'].prod(axis=0)\n    \n    # Create a list of unigram calculation for each sentence\n    unigram_table = unigram_table.append(pd.DataFrame({'unigram':unigram},index = [index]))\n    \n    clear_output(wait=True)\n    print('Proportion of sentences completed:', np.round(index\/len(train_data_sent),4)*100,'%')\n        \nend_time = time.time()\nprint('Total run time = ', np.round(end_time-start_time,2)\/60, ' minutes')\n\ntrain_data_sent['unigram'] = unigram_table['unigram']","6731f31e":"base_time = end_time-start_time","40f0ddfc":"unigram_table.head(10)","0c2eef96":"unigram_table_log = pd.DataFrame()\n\nstart_time_log = time.time()\n# Loop through each sentence\n# REMOVE ROW LIMIT FOR FULL RUN\nfor index in train_data_sent[0:200].index:\n    data_row = train_data_sent.iloc[index,:]\n\n    sent_probs = pd.DataFrame()\n    # Go through each word in the sentence, lookup the probability of the word and \n    # then find the mulitplicitive product of all probabilities in the sentence.\n    for n,word in enumerate(data_row['sentence_clean']):\n        log_prob = np.log10(word_list[ word_list['words']==word]['prob'])\n        sent_probs = sent_probs.append(pd.DataFrame({'log_prob':log_prob}, index = [n]))\n        \n    unigram_log = sum(sent_probs['log_prob'])\n    \n    # Create a list of unigram calculation for each sentence\n    unigram_table_log = unigram_table.append(pd.DataFrame({'unigram_log':unigram_log},index = [index]))\n                                         \n    clear_output(wait=True)\n    print('Proportion of sentences completed:', np.round(index\/len(train_data_sent),4)*100,'%')\n                                                                   \nend_time_log = time.time()\nprint('Total run time = ', np.round(end_time_log-start_time_log,2)\/60, ' minutes')\n\n#train_data_sent['unigram_log'] = unigram_table_log['unigram_log']","cb977678":"log_time = end_time_log - start_time_log","7d713d43":"print('The log base 10 method takes approximately ', np.round((log_time)\/base_time,4)*100, '% of the time of the orginal calculation.')","a0ba39a7":"word_1 = 'to'\nword_2 = 'a'\n\nprob_word_1 = word_list[word_list['words'] == word_1]['prob'].iloc[0]\nprob_word_2 = word_list[word_list['words'] == word_2]['prob'].iloc[0]\n\nunigram_prob = prob_word_1*prob_word_2\n\nprint('The unigram probability of the word \"a\" occuring given the word \"to\" was the previous word is: ', np.round(unigram_prob,10))","410238fc":"word_1 = ' ' + str('to') + ' '\nword_2 = str('a') + ' '\n\nbigram_prob = len(re.findall(word_1 + word_2, text_list)) \/ len(re.findall(word_1, text_list)) \n\nprint('The probability of the word \"a\" occuring given the word \"to\" was the previous word is: ', np.round(bigram_prob,5))","ac0a688c":"word_1 = ' ' + str('has') + ' '\nword_2 = str('a') + ' '\n\nbigram_prob = len(re.findall(word_1 + word_2, text_list)) \/ len(re.findall(word_1, text_list)) \n\nprint('The probability of the word \"a\" occuring given the word \"has\" was the previous word is: ', np.round(bigram_prob,5))","b8fbd4ca":"W_W_Matrix = pd.DataFrame({'words': word_list['words']})\n\nstart_time = time.time()\n\n\n# Add limits to number of columns\/rows so this doesn't run for ages\ncolumn_lim = 1000\n#column_lim = len(W_W_Matrix)\nrow_lim = 10\n#row_lim = len(W_W_Matrix)\n\nfor r, column in enumerate(W_W_Matrix['words'][0:column_lim]):\n    \n    prob_table = pd.DataFrame()\n    for i, row in enumerate(W_W_Matrix['words'][0:row_lim]):\n\n        word_1 = ' ' + str(row) + ' '\n        word_2 = str(column) + ' '\n\n        if len(re.findall(word_1, text_list)) == 0:\n            prob = pd.DataFrame({'prob':[0]}, index=[i])\n        else:\n            prob = pd.DataFrame({'prob':[len(re.findall(word_1 + word_2, text_list)) \/ len(re.findall(word_1, text_list)) ]}, index=[i])\n        \n        prob_table = prob_table.append(prob)\n    W_W_Matrix[str(column)] = prob_table['prob']\n    \n    # Outputs progress of main loop, see:\n    clear_output(wait=True)\n    print('Proportion of column words completed:', np.round(r\/len(W_W_Matrix[0:column_lim]),2)*100,'%')\n    \nend_time = time.time()\nprint('Total run time = ', np.round(end_time-start_time,2)\/60, ' minutes')\n","b4e1ecc8":"W_W_Matrix[W_W_Matrix['a'] >= 0]","6b0aa558":"for i in range(0,row_lim):\n    plt.bar(W_W_Matrix.iloc[i,1:].sort_values(ascending=False)[1:10].index,W_W_Matrix.iloc[i,1:].sort_values(ascending=False)[1:10].values)\n    plt.title('Most Common Words that Follow the word: ' +str(W_W_Matrix.iloc[i,0]))\n    plt.show()","9436c2c9":"W_W_Matrix = pd.DataFrame({'words': word_list['words']})\n\nstart_time = time.time()\n\ntext_list = \" \".join(map(str, text))\n\n# Increasing these take significant time to run but provide more realistic sentences\nnum_sentences = 1\nsentence_word_limit = 1\n\n#extract start and end of sentence notation so that they are always included\nsentence_forms = W_W_Matrix[(W_W_Matrix['words']=='<s') | (W_W_Matrix['words']=='\/s>')]['words']\n\nsentences_output = pd.DataFrame()\nfor sample in range(0,num_sentences):\n    \n    sentence = pd.DataFrame()\n    \n    for i in range(0,sentence_word_limit):\n        # if this is the first word, fix it to be start of sentence notation else take output of previous iteration\n        if (i==0):\n            current_word = str('<s')\n        # Randomly select first word after sentence start\n        elif (i==1):\n            current_word = str(W_W_Matrix[(W_W_Matrix['words']!='<s') ]['words'].sample(1, axis=0).iloc[0])\n        else:\n            current_word = next_word\n        \n        sentence['word_'+str(i)] = [current_word]\n        # if we have reached end of sentence, add this sentence to output table and break loop to start new sentence\n        if (current_word==str('\/s>')):\n            sentences_output = sentences_output.append(sentence)\n            break   \n        else:\n            \n            prob_table = pd.DataFrame()\n\n            # randomly select other words form rest of list\n            for r, column in enumerate(W_W_Matrix[(W_W_Matrix['words']!='<s') ]['words'].reset_index(drop=True)):\n\n                next_words = str(column)\n\n\n\n                if len(re.findall(' ' + current_word + ' ', text_list)) == 0:\n                    prob = pd.DataFrame({'word':str(column),\n                        'prob':[0]}, index=[i])\n                else:\n                    prob = pd.DataFrame({'word':str(column),\n                        'prob':[len(re.findall(' ' + current_word + ' ' + next_words + ' ', text_list)) \/ len(re.findall(' ' + current_word + ' ', text_list)) ]}, index=[i])\n\n                prob_table = prob_table.append(prob)\n                # We can reduce the probability of the sentence ending so that we return longer sentences\n                reduce_end_prob = 0.5\n                prob_table['prob'] = np.where(prob_table['word']=='\/s>', prob_table['prob']*reduce_end_prob,prob_table['prob'])\n                # next word is most probable of this given the current word\n                next_word = prob_table[prob_table['prob'] == max(prob_table['prob'])]['word'].reset_index(drop=True).iloc[0]\n                \n                # Outputs progress of main loop:\n                clear_output(wait=True)\n                print(\"Sentence number: \",sample+1)\n                print(\"Words completed in current sentence:\",i+1)\n                print('Proportion of column words completed:', np.round(r\/len(W_W_Matrix),2)*100,'%')\n\n        \nend_time = time.time()\nprint('Total run time = ', np.round(end_time-start_time,2)\/60, ' minutes')\n","1a36f70d":"for i in range(1,num_sentences):\n    print('Sentence ',i,':',sentences_output.iloc[i].values)","5b6478ac":"word_1 = str('movie')\nword_2 = str('\/s>') \n\nbigram_prob = len(re.findall(' ' + word_1 + ' ' + word_2 + ' ', text_list)) \/ len(re.findall(' ' + word_1 + ' ', text_list))\n\n\nprint('The probability of the word \"',word_2,'\" occuring given the word \"',word_1,'\" was the previous word is: ', (bigram_prob))","0115b143":"word_1 = str('to')\nword_2 = str('a') \nword_3 = str('movie')\n\ntrigram_prob = (len(re.findall(' ' + word_1 + ' ' + word_2 + ' ' + word_3 + ' ', text_list)) \/ \n                    len(re.findall(' ' + word_1 + ' ' + word_2, text_list)))\n\n\nprint('The probability of the word \"',word_3,'\" occuring given the word \"',word_1,'\" and \"',word_2,'\" were the previous two words is: ', (trigram_prob))","795e9c19":"word_1 = str('to')\nword_2 = str('a') \nword_3 = str('film')\n\ntrigram_prob = (len(re.findall(' ' + word_1 + ' ' + word_2 + ' ' + word_3 + ' ', text_list)) \/ \n                    len(re.findall(' ' + word_1 + ' ' + word_2, text_list)))\n\n\nprint('The probability of the word \"',word_3,'\" occuring given the word \"',word_1,'\" and \"',word_2,'\" were the previous two words is: ', (trigram_prob))","fe2951ff":"p = [0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1]\nH = [-(p*np.log2(p) + (1-p)*np.log(1-p)) for p in p]\n# Replace nan output with 0 \nH = [0 if math.isnan(x) else x for x in H]\n\nplt.plot(p,H)\nplt.xlim([-0.05,1.05])\nplt.ylim([-0.05,1])\nplt.xlabel('Probability of Heads (p)')\nplt.ylabel('Entropy (H(p))')\nplt.title('The Entropy of a Bias Coin as \\n the Probabilitiy of Heads Varies')","5c8284a6":"sent_1 = text.iloc[0]\nsent_2 = text.iloc[1]\n\nprint('Sentence 1', sent_1)\nprint('--.--.--.--.--.--.--.--')\nprint('Sentence 2', sent_2)","255c57c3":"data_prob = word_list[['words','count','prob']]\ndata_prob.head()","7880ee4e":"def entropy(sentence, data_prob):\n    entropy_table = pd.DataFrame()\n    for n,word in enumerate(sentence.split(' ')):\n        # log2(0) provide nan so return 0 instead\n        if ((data_prob[data_prob['words']==word]['prob'].iloc[0]) == 0):\n            entropy = 0\n        else:\n            prob = data_prob[data_prob['words']==word]['prob'].iloc[0]\n            entropy = prob*np.log2(prob)\n        entropy_table = entropy_table.append(pd.DataFrame({'word':word,\n                                                            'entropy':entropy}, index = [n]))\n    phrase_entropy = -1*sum(entropy_table['entropy'])\n    return(phrase_entropy)","a9a2bc7f":"sent_1_entropy = entropy(sent_1,data_prob)\nsent_2_entropy = entropy(sent_2,data_prob)\n\nprint('Sentence 1: ', sent_1)\nprint('Sentence 1 entropy: ', np.round(sent_1_entropy,5))\nprint('Per-word Sentence 1 entropy: ', np.round(sent_1_entropy\/len(sent_1.split(' ')),5))\n\nprint('--.--.--.--.--.--.--.--')\nprint('Sentence 2: ', sent_2)\nprint('Sentence 2 entropy: ', np.round(sent_2_entropy,5))\nprint('Per-word Sentence 2 entropy: ', np.round(sent_2_entropy\/len(sent_2.split(' ')),5))\n","4d38c9b4":"sent_1_perplex = 2**sent_1_entropy\nsent_2_perplex = 2**sent_2_entropy\n\nprint('Sentence 1: ', sent_1)\nprint('Sentence 1 entropy: ', np.round(sent_1_entropy,5))\nprint('Per-word Sentence 1 entropy: ', np.round(sent_1_entropy\/len(sent_1.split(' ')),5))\nprint('Sentence 1 Perplexity: ', sent_1_perplex)\n\nprint('--.--.--.--.--.--.--.--')\nprint('Sentence 2: ', sent_2)\nprint('Sentence 2 entropy: ', np.round(sent_2_entropy,5))\nprint('Per-word Sentence 2 entropy: ', np.round(sent_2_entropy\/len(sent_2.split(' ')),5))\nprint('Sentence 2 Perplexity: ', sent_2_perplex)\n","c3899a90":"sent_1_prob = (1\/sent_1_perplex)**len(sent_1.split(' '))\nsent_2_prob = (1\/sent_2_perplex)**len(sent_2.split(' '))\n\nprint('Sentence 1 Probability: ', '%0.10f' % sent_1_prob)\nprint('Sentence 2 Probability: ', '%0.10f' % sent_2_prob  )","45cdc06a":"train_data_sent.head()","53251ebe":"corpus = train_data_sent['sentence_clean'][:int(np.round(len(train_data_sent)*0.9,0))]\ntest = train_data_sent['sentence_clean'][int(np.round(len(train_data_sent)*0.9,0))+1:]\n\ncorpus_list = \" \".join(map(str, corpus))\ntest_list = \" \".join(map(str, test))\n","ae155295":"# Corpus word probabilities\ncorpus_word_list = pd.DataFrame({'words':corpus.str.split(' ', expand = True).stack().unique()})\ncorpus_word_count_table = pd.DataFrame()\nfor n,word in enumerate(corpus_word_list['words']):\n    # Create a list of just the word we are interested in, we use regular expressions so that part of words do not count\n    # e.g. 'ear' would be counted in each appearance of the word 'year'\n    corpus_word_count = len(re.findall(' ' + word + ' ', corpus_list))  \n    corpus_word_count_table = corpus_word_count_table.append(pd.DataFrame({'count':corpus_word_count}, index=[n]))\n    \n    clear_output(wait=True)\n    print('Proportion of words completed:', np.round(n\/len(corpus_word_list),4)*100,'%')\n\ncorpus_word_list['count'] = corpus_word_count_table['count']\n# Remove the count for the start and end of sentence notation so \n# that these do not inflate the other probabilities\ncorpus_word_list['count'] = np.where(corpus_word_list['words'] == '<s' , 0,\n                     np.where(corpus_word_list['words'] == '\/s>', 0,\n                     corpus_word_list['count']))\ncorpus_word_list['prob'] = corpus_word_list['count']\/sum(corpus_word_list['count'])\n\n\n\n","77fb0d42":"corpus_word_list.head()","c7846ce0":"# Test set word probabilities\ntest_word_list = pd.DataFrame({'words':test.str.split(' ', expand = True).stack().unique()})\ntest_word_count_table = pd.DataFrame()\nfor n,word in enumerate(test_word_list['words']):\n    # Create a list of just the word we are interested in, we use regular expressions so that part of words do not count\n    # e.g. 'ear' would be counted in each appearance of the word 'year'\n    test_word_count = len(re.findall(' ' + word + ' ', test_list))  \n    test_word_count_table = test_word_count_table.append(pd.DataFrame({'count':test_word_count}, index=[n]))\n    \n    clear_output(wait=True)\n    print('Proportion of words completed:', np.round(n\/len(test_word_list),4)*100,'%')\n\ntest_word_list['count'] = test_word_count_table['count']\n# Remove the count for the start and end of sentence notation so \n# that these do not inflate the other probabilities\ntest_word_list['count'] = np.where(test_word_list['words'] == '<s' , 0,\n                     np.where(test_word_list['words'] == '\/s>', 0,\n                     test_word_list['count']))\ntest_word_list['prob'] = test_word_list['count']\/sum(test_word_list['count'])\n\n","3101078f":"test_word_list.head()","c9dd86e2":"# Merge corpus counts to test set and replace missing values with 0\ntest_word_list_2 = test_word_list.merge(corpus_word_list[['words','count']], how='left', on = 'words')\ntest_word_list_2['count_y'].fillna(0, inplace=True)\n\ntest_word_list_2.head()","2a1b314b":"print('Percentage of words in test set that are not contained in corpus',len(test_word_list_2[test_word_list_2['count_y']==0])\/len(test_word_list_2)*100,'%')","41c7ff62":"# Extract missing words from training set\nmissing_words = test_word_list_2[test_word_list_2['count_y']==0]\nmissing_words = missing_words[(missing_words['words']!='<s')&(missing_words['words']!='\/s>')]\nmissing_words = missing_words[['words']]\nmissing_words['count'] = 0\nmissing_words['prob'] = 0\nmissing_words.head()","d3e35c4e":"# Add missing words onto end of corpus word list and apply laplace +1 smoothing\ncorpus_word_list_fixed = corpus_word_list.append(missing_words)\n\ncorpus_word_list_fixed['count+1'] = corpus_word_list_fixed['count']+1\ncorpus_word_list_fixed['prob+1'] = corpus_word_list_fixed['count+1']\/sum(corpus_word_list_fixed['count+1'])\ncorpus_word_list_fixed.head()","a9ca0ee2":"# Plot distribution before and after Laplace +1 Smoothing\nsns.distplot(corpus_word_list_fixed[corpus_word_list_fixed['count']<=15]['count'], label='Before')\nsns.distplot(corpus_word_list_fixed[corpus_word_list_fixed['count']<=15]['count+1'], label='After +1 Smoothing')\n\nplt.legend()\nplt.title('Distribution of Word Counts Before\/After \\n Laplace +1 Smoothing')\nplt.xlabel('Word Count Bin')\nplt.ylabel('Count')\nplt.show()","42cf0a3d":"corpus.head()","ff25ba70":"corpus_2 = corpus[:int(np.round(len(corpus)*0.9,0))]\nhold_out = corpus[int(np.round(len(corpus)*0.9,0))+1:]\n\ncorpus_2_list = \" \".join(map(str, corpus_2))\nhold_out_list = \" \".join(map(str, hold_out))\n","8406fe78":"# Test set word probabilities\nhold_out_word_list = pd.DataFrame({'words':hold_out.str.split(' ', expand = True).stack().unique()})\nhold_out_word_count_table = pd.DataFrame()\nfor n,word in enumerate(hold_out_word_list['words']):\n    # Create a list of just the word we are interested in, we use regular expressions so that part of words do not count\n    # e.g. 'ear' would be counted in each appearance of the word 'year'\n    hold_out_word_count = len(re.findall(' ' + word + ' ', hold_out_list))  \n    hold_out_word_count_table = hold_out_word_count_table.append(pd.DataFrame({'count':hold_out_word_count}, index=[n]))\n    \n    clear_output(wait=True)\n    print('Proportion of words completed:', np.round(n\/len(hold_out_word_list),4)*100,'%')\n\nhold_out_word_list['count'] = hold_out_word_count_table['count']\n# Remove the count for the start and end of sentence notation so \n# that these do not inflate the other probabilities\nhold_out_word_list['count'] = np.where(hold_out_word_list['words'] == '<s' , 0,\n                     np.where(hold_out_word_list['words'] == '\/s>', 0,\n                     hold_out_word_list['count']))\nhold_out_word_list['prob'] = hold_out_word_list['count']\/sum(hold_out_word_list['count'])\n\n","605c237b":"hold_out_Matrix = pd.DataFrame({'words': hold_out_word_list['words']})\n\nstart_time = time.time()\n\n\n# Add limits to number of columns\/rows so this doesn't run for ages\ncolumn_lim = 100\n#column_lim = len(W_W_Matrix)\nrow_lim = 10\n#row_lim = len(W_W_Matrix)\n\nfor r, column in enumerate(hold_out_Matrix['words'][0:column_lim]):\n    \n    prob_table = pd.DataFrame()\n    for i, row in enumerate(hold_out_Matrix['words'][0:row_lim]):\n\n        word_1 = ' ' + str(row) + ' '\n        word_2 = str(column) + ' '\n\n        if len(re.findall(word_1, hold_out_list)) == 0:\n            prob = pd.DataFrame({'prob':[0]}, index=[i])\n        else:\n            prob = pd.DataFrame({'prob':[len(re.findall(word_1 + word_2, hold_out_list)) \/ len(re.findall(word_1, hold_out_list)) ]}, index=[i])\n        \n        prob_table = prob_table.append(prob)\n    hold_out_Matrix[str(column)] = prob_table['prob']\n    \n    # Outputs progress of main loop, see:\n    clear_output(wait=True)\n    print('Proportion of column words completed:', np.round(r\/len(hold_out_Matrix[0:column_lim]),2)*100,'%')\n    \nend_time = time.time()\nprint('Total run time = ', np.round(end_time-start_time,2)\/60, ' minutes')\n","d8a7e886":"hold_out_Matrix.head(20)","aa3ad375":"hold_out_Matrix['words'] = hold_out_word_list['words']","63e53e98":"lambda_1 = 0.5\nlambda_2 = 0.5\n\n# Create copy so we dont have to re-calculate original\nhold_out_Matrix_2 = hold_out_Matrix.copy()\nhold_out_Matrix_2 = hold_out_Matrix_2.dropna()\n# Extract 'words' column \nhold_out_Matrix_3 = pd.DataFrame({'words':hold_out_Matrix_2.iloc[:,0]})\nhold_out_Matrix_2 = hold_out_Matrix_2.iloc[:,1:]\n\n# Multiply bigrams by lambda 1\nhold_out_Matrix_2 = lambda_1*hold_out_Matrix_2\n\nfor n,column in enumerate(list(hold_out_Matrix_2)):\n    column_prob = hold_out_word_list[hold_out_word_list['words']==column]['prob'].iloc[0]\n    column_prob = lambda_2*column_prob\n    \n    hold_out_Matrix_3[str(column)] = hold_out_Matrix_2[column] + column_prob\n    \n    # Outputs progress of main loop, see:\n    clear_output(wait=True)\n    print('Proportion of column words completed:', np.round(n\/len(list(hold_out_Matrix_2)),2)*100,'%')\n    \n# Sum probabilities of matrix (remove word column from calculation)\ntotal_prob = hold_out_Matrix_3.iloc[:,1:].values.sum()\n","bb382158":"print(total_prob)","05d95c55":"output_table = pd.DataFrame()\n\nfor x in range(0,11):\n    lambda_1 = x\/10\n    lambda_2 = 1-lambda_1\n        \n    # Create copy so we dont have to re-calculate original\n    hold_out_Matrix_2 = hold_out_Matrix.copy()\n    hold_out_Matrix_2 = hold_out_Matrix_2.dropna()\n    # Extract 'words' column \n    hold_out_Matrix_3 = pd.DataFrame({'words':hold_out_Matrix_2.iloc[:,0]})\n    hold_out_Matrix_2 = hold_out_Matrix_2.iloc[:,1:]\n\n    # Multiply bigrams by lambda 1\n    hold_out_Matrix_2 = lambda_1*hold_out_Matrix_2\n\n    for n,column in enumerate(list(hold_out_Matrix_2)):\n        column_prob = hold_out_word_list[hold_out_word_list['words']==column]['prob'].iloc[0]\n        column_prob = lambda_2*column_prob\n\n        hold_out_Matrix_3[str(column)] = hold_out_Matrix_2[column] + column_prob\n\n        # Outputs progress of main loop, see:\n        clear_output(wait=True)\n        print('Current lambda 1 value:', np.round(lambda_1,2))\n        print('Current lambda 2 value:', np.round(lambda_2,2)) \n        print('Proportion of column words completed:', np.round(n\/len(list(hold_out_Matrix_2)),2)*100,'%')\n\n\n    # Sum probabilities of matrix (remove word column from calculation)\n    total_prob = hold_out_Matrix_3.iloc[:,1:].values.sum()\n    output_table = output_table.append(pd.DataFrame({'lambda_1':lambda_1,\n                                                     'lambda_2':lambda_2,\n                                                     'total_prob':total_prob}, index = [x]))","bebbcbd4":"output_table.head()","c003c0fa":"output_table['lambda_1_2'] = 'L1:' + output_table['lambda_1'].astype(str) +' \/L2: '+ output_table['lambda_2'].astype(str)\n        \nplt.bar(output_table['lambda_1_2'], output_table['total_prob'])\nplt.title(\"Total Probability of Hold-out Set after \\n Applying Simple Interpolation\")\nplt.xlabel('Lambda 1 and Lambda 2 Parameters')\nplt.xticks(output_table['lambda_1_2'], rotation='vertical')\nplt.xlabel('Total Probability')\nplt.show()","c58105eb":"optimal_lambda_1 = output_table[output_table['total_prob']==max(output_table['total_prob'])].iloc[0]['lambda_1']\noptimal_lambda_2 = output_table[output_table['total_prob']==max(output_table['total_prob'])].iloc[0]['lambda_2']\n\nprint(\"Optimal Lambda 1 = \", optimal_lambda_1)\nprint(\"Optimal Lambda 2 = \", optimal_lambda_2)","a76f1f0d":"#### Find the occurence of each word and use this to find the probability of occurence of each word","734bb2c5":"# Language Models (LMs)\n\n#### Jan 2019\n#### Philip Osborne \n#### version 1.0\n\n\nSee the corresponding Medium post for a full write-up with complete examples. Some of these code cells have been reduced in complexity to save run time which effects the output.\n\nhttps:\/\/medium.com\/@philiposbornedata\/learning-nlp-language-models-with-real-data-cdff04c51c25\n\n--------------------","00803380":"### We reduce the number of rows in our training corpus for learning purposes as the models take a substantial amout of time otherwise.","4b931c72":"#### The matrix defines the probability of the column given the row (i.e. P(column_header|row_header)).\n\n#### Therefore, we add the probability of the column word (as the second word, to each. ","88574931":"---\n---\n# Part 4\n\n## Selecting the Language Model to Use\n\nWe have introduced the first three LMs (unigram, bigram and trigram) but which is best to use?\n\nTrigrams are generally provide better outputs than bigrams and bigrams provide better outputs than unigrams but as we increase the complexity the computation time becomes increasingly large. Furthermore, the amount of data available decreases as we increase n (i.e. there will be far fewer next words available in a 10-gram than a bigram model).\n\n## XI. Back-off Method: Use trigrams (or higher n model) if there is good evidence to, else use bigrams (or other simpler n-gram model).\n\n## XII. Interpolation: Use a mixture of n-gram models\n\n### Defn: Simple Interpolation:\n\n\\begin{equation}\n    P(w_3|w_1,w_2) = \\lambda_1 P(w3|w_1,w_2) + \\lambda_2 P(w_3|w_2) + \\lambda_3 P(w_3)\n\\end{equation}\n\nwhere $\\sum_{i} \\lambda_i = 1$.\n\n### Defn: Contidional Context Interpolation:\n\n\\begin{equation}\n    P(w_3|w_1,w_2) = \\lambda_1 (w_{1}^{2})P(w3|w_1,w_2) + \\lambda_2 (w_{1}^{2})P(w_3|w_2) + \\lambda_3 (w_{1}^{2})P(w_3)\n\\end{equation}\n\n\n### Calculating $\\lambda$s:\n\nUsing a held-out subset of the corpus (validation set), find $\\lambda$s that maximise the probability of the held out data:\n\n\\begin{equation}\n    P(w_1,w_2,...,w_n|M(\\lambda_1,\\lambda_2,...,\\lambda_k)) = \\sum_i log P_{M(\\lambda_1,\\lambda_2,...,\\lambda_k)}(w_i|w_{i-1})\n\\end{equation}\n\nWhere unknown words are assigned an unknown word token '<Ukn'.\n\n\n### Small Interpolation Example\n\nSay we are given the following corpus:\n\n- <s I am Sam \/s>\n- <s Sam I am \/s>\n- <s I am Sam \/s>\n- <s I do not like green eggs and Sam \/s>\n\nUsing linear interpolation smoothing with a bigram and unigram model with $\\lambda_1 = \\frac{1}{2}$ and $\\lambda_2 = \\frac{1}{2}$, what is $P(Sam|am)$? (note: include '<s' and '\/s>' in calculations)\n\nUsing the following equation:\n\n\\begin{equation}\n    P(w_2|w_1) = lambda_1 P(w_2|w1) + lambda_2 P(w2)\n\\end{equation}\n\nWe have in our case:\n\n\\begin{equation}\n    P(Sam|am) = \\frac{1}{2} P(Sam|am) + \\frac{1}{2} P(Sam)\n\\end{equation}\n\nwhere \n\n\\begin{equation}\n    P(Sam|am) = \\frac{count(am, Sam)}{count(am)} = \\frac{2}{3}\n\\end{equation}\n\nand\n\n\\begin{equation}\n    P(Sam) = \\frac{count(Sam)}{Total \\ num \\ words} = \\frac{4}{25}\n\\end{equation}\n\nTherefore,\n\n\\begin{equation}\n    P(Sam|am) = \\frac{1}{2}*\\frac{2}{3} + \\frac{1}{2}*\\frac{4}{25} \\approx 0.413\n\\end{equation}\n\n","8cb70da4":"---\n## VII. Entropy of Language\n\n### 1. Entropy of a sequence of words:\n\n\\begin{equation}\n    H(w_1 w_2 ... w_n) = - \\sum_{w_1...wn} P(w_1 ... w_n) log_2 P(w_1 ... w_n)\n\\end{equation}\n\n### 2. The per-word entropy rate of a sequence of words\n\n\\begin{equation}\n    \\frac{1}{n} H(w_1 w_2 ... w_n) = \\frac{-1}{n} \\sum_{w1...wn} P(w_1 ... w_n) log_2 P(w_1 ... w_n),2)\n\\end{equation}\n\n### 3. Entropy of a language $L = \\{w_1 ... w_n | 1 < n < \\infty\\}$:\n\n\\begin{equation}\n    H(L) = - \\lim_{n\\to\\infty} \\frac{1}{n} H(w_1 ... w_n)\n\\end{equation}\n\n\n### Defn: Cross Entropy\n\nThe cross entropy, H(p,m), of a true distribution **p** and a model distribution **m** is defined as:\n\n\\begin{equation}\n    H(p,m) = - \\sum_{x} p(x) log_2 m(x)\n\\end{equation}\n\nThe lower the cross entropy is the closer it is to the true distribution.\n\n### Defn: Cross Entropy of a Sequence of Words\n\n\\begin{equation}\n    H(p,m) = - \\lim_{n\\to\\infty} \\frac{1}{n} \\sum_{w1...wn} p(w_1 ... w_n) log_2 m(w_1 ... w_n)\n\\end{equation}\n\n---\n## VIII. Perplexity and Entropy\n\n\\begin{equation}\nPP(w_1 ... w_N) = 2^{H(w_1 ... w_N)}\n\\end{equation}\n\n\n","f16161f3":"and likewise, if we change the previous word to 'has':","09e0fb60":"# Example with Real Data\n\n\n### Import Packages and IMDB Movie Review Data [2]\n\n\n[2] http:\/\/ai.stanford.edu\/~amaas\/data\/sentiment\/","91313cae":"### X. Futher Smoothing Methods\n\nLaplace +1 smoothing is used in text classification and domains where the number of zeros isn't large. However, it is not often used for n-grams, some better smothing methods for n-grams are:\n\n- Add-k Laplace Smoothing\n- Good-Turing\n- Kenser-Ney\n- Witten-Bell\n","329c4614":"---\n---\n# Part 2\n---\n## V. Training and Testing the Language Models (LMs)\n\nThe corpus used to train our LMs will impact the output predictions. Therefore we need to introduce a methodology for evaluating how well our trained LMs perform. The best trained LM is the one that can correctly predict the next word of setences in an unseen test set.\n\nThis can be time consuming, to build multiple LMs for comparison could take hours to compute. Therefore, we introduce the intrinsic evaluation method of **perplexity**. In short perplexity is a measure of how well a probability distribution or probability model predicts a sample. [3]\n\n### Defn: Perplexity\n\nPerplexity is the inverse probability of the test set normalised by the number of words, more specifically can be defined by the following equation:\n\n\\begin{equation}\n    PP(W) = P(w_1 w_2 ... w_N)^{\\frac{-1}{N}}\n\\end{equation}\n\ne.g. Suppose a sentence consists of random digits [0-9], what is the perplexit of this sentence by a model that asigns an equal probability (i.e. $P = 1\/10$) to each digit?\n\n\\begin{equation}\n    PP(W) = (\\frac{1}{10}*\\frac{1}{10}*...*\\frac{1}{10})^{\\frac{-1}{10}} = (\\frac{1}{10}^{10} )^{\\frac{-1}{10}} = \\frac{1}{10}^{-1} = 10\n\\end{equation}\n\n\n--\n## VI. Entropy in Mechanics and Information Theory\n\nIn mechanics, Boltsmann defined the entropy of a system is related to the natural log of the number of microstates:\n\n\\begin{equation}\n    S = klnW \\ where \\ k = 1.3x10^{-23}\n\\end{equation}\n\nW is the number of microstates and is calculated by:\n\n\\begin{equation}\n    W = n^x\n\\end{equation}\n\nwhere n is the number of positions and x is the number of molecules.\n\n\nIn Information Theory, entropy (denoted $H(X)$) of a random variable X is the expected log probabiltiy:\n\n\\begin{equation}\n    H(X) = - \\sum P(x)log_2 P(x)\n\\end{equation}\n\nand is a measure of uncertainty. [4]\n\n#### In other words, entropy is the number of possible states that a system can be.\n\n### Entropy of a bias coin toss\n\nSay we have the probabilities of heads and tails in a coin toss defined by:\n\n- $P(heads) = p$\n- $P(tails) = 1-p$\n\nThen the entropy of this is:\n\n\\begin{equation}\n    H(X) = - \\sum P(x)log_2 P(x) = -[plog_2 p + (1-p)log_2 (1-p)]\n\\end{equation}\n\nIf the coin is fair, i.e. p = 0.5, then we have:\n\n\\begin{equation}\n    H(X) = -[0.5log_2 0.5 + (1-0.5)log_2 (1-0.5)] = -[-0.5-0.5] = 1\n\\end{equation}\n\nThe full entropy distibution over varying bias probabilities is shown below.\n\n\n[3] https:\/\/en.wikipedia.org\/wiki\/Perplexity\n[4] https:\/\/en.wikipedia.org\/wiki\/Entropy_(information_theory)","266466ba":"### Likewise, we can calculate the perplexity of each sentence:","5090084e":"#### Therefore, trigram phrase 'to a movie' is used more commonly than 'to a film' and is the choice our algorithm would take when forming sentences.\n\nThe code to systemtically find the most likely next word and form sentences with trigrams can be repeated following the previous bigram computations. ","2a5a7477":"So, for example, if we wanted to find the P(a|to) we first count the occurances of (to,a) and divide this by the count of (to):\n","98377268":"## Data Pre-processing\n\n### 1. Convert full text comment into individual sentences","ad7657d8":"We can use logarithm space as addition is faster computationally than multiplications. \n\nAs by log rules: $log(P1xP2xP3) = log(P1) + log(P2) + log(P3)$","d27a6871":"### Unigram Model (k=1): $P(w_1 w_2 ... w_n) \\approx \\prod_{i} P(w_i)$\n\nTo apply this we can simply lookup the probability of each word in the sentence and then calculate the multiplicative product of these probabilities.\n\n**Note for this notebook, I have reduced the number of items considered so that it runs in good time**\n","76cdd938":"We can repeat this calculation for all word pairs to find the most likely word to follow the given word. This of course takes an exceptional amount of time and it may be better to compute this for words as required rather than attempting to do it for all. ","a5f00dfa":"These can be used to form sentences, if we calculate the bigram probabilities 'on the fly' we can find the next most likely word for the given word. To ensure that different sentences are form, we vary the selected colmuns randomly.\n\nFixing the start and end of the sentence to be the respective <s and \/s> notations, we form the following:\n\n**NOTE I have made the word cap very small as this takes a significant time to run (2 hours for 5 words) but an example output is shown.**\n","9678c01c":"# Introduction\n\n\n# Part 1\n--- \n## I. Probabilistic Language Models\n\n### Example uses\n- **Machine Translation:** P(**high** winds tonight) > P(**large** winds tonight)\n- **Spell Correction:** P(...about fifteen **minutes** from...) > P(...about fifteen **mineuts** from...)\n- **Speech Recognition:** P(I saw a van) >> P(eyes awe of an)\n\n### Aim of Language Models\nThe goal of paobabilistic language modelling is to calculate the probability of a sentence of sequence of words: \n\n\\begin{equation}\n    P(W) = P(w_1,w_2,w_3,...,w_n)\n\\end{equation}\n\nand can be used to find the probability of the next word in the sequence:\n\n\\begin{equation}\n    P(w_5|w_1,w_2,w_3,w_4)\n\\end{equation}\n\na model that computes either of these is called a **language model (LM)**.\n\n---\n## II. Initial Method for Calcualting Probabilities\n\n### Defn: Conditional Probability\nLet A and B be two events with $P(B) \\neq 0$, the conditional probability of A given B is:\n\n\\begin{equation}\n    P(A|B) = \\frac{P(A,B)}{P(B)}\n\\end{equation}\n\n### Defn: Chain Rule\n\\begin{equation}\n    P(x_1,x_2,...,x_n) = P(x_1)P(x_2|x_1)...P(x_n|x_1,...,x_{n-1})\n\\end{equation}\n\n#### The Chain Rule applied to compute the join probability of words in a sentence:\n\\begin{equation}\n    P(w_1 w_2 ... w_n) = \\prod_{i} P(w_i|w_1 w_2 ... w_{i-1})\n\\end{equation}\n\ne.g. \n\nP(\"its water is so transparent\") =\n    \n    P(its)xP(water|its) \n    x P(is|its water) \n    x P(so|its water is) \n    x P(transparent|its water is so)\n\n#### Can we estimate this by simply counting and dividing the results by the following?\n\\begin{equation}\n    P(transparent|its \\ water \\ is \\ so) = \\frac{count(its \\ water \\ is \\ so \\ transparent)}{count(its \\ water \\ is \\ so)}  \n\\end{equation}\n#### NO! Far to many possible sentences that would need to be calculated, we would never have enough data to achieve this.\n\n\n---\n## III. Methods using the Markov Assumption\n\n### Defn: Markov Property\n\n*A stochastic process has the Markov property if the conditional probability distribution of future states of the process (conditional on both past and present states) depends only upon the present state, not on the sequence of events that preceded it. A process with this property is called a Markov process.* [1]\n\nIn other words, the probability of the next word can be estimated given only the previous $k$ number of words.\n\ne.g. \n\n\\begin{equation}\n    P(transparent|its \\ water \\ is \\ so) \\approx P(transparent|so) \n\\end{equation}\nor\n\n\n\\begin{equation}\n    P(transparent|its \\ water \\ is \\ so) \\approx P(transparent|is \\ so) \n\\end{equation}\n\n#### General Equation for the Markov Assumption\n\n\n\\begin{equation}\n    P(w_i|w_1 w_2 ... w_{i-1}) \\approx P(w_i|w_{i-k} ... w_{i-1})\n\\end{equation}\nwhere k is the number of words in the 'state' to be defined by the user.\n\n\n### Unigram Model (k=1)\n\n\\begin{equation}\n    P(w_1 w_2 ... w_n) \\approx \\prod_{i} P(w_i)\n\\end{equation}\n\n### Bigram Model (k=2)\n\n\\begin{equation}\n    P(w_i|w_1 w_2 ... w_{i-1}) \\approx P(w_i|w_{i-1})\n\\end{equation}\n\n\n---\n## IV. N-gram Models (k=n)\n\nThe previous two equations can be extended to compute trigrams, 4-grams, 5-grams, etc. In general, this is an insufficient model of language because **sentences often have long distance dependencies**. For example, the subject of the sentence may be at the start whilst our next word to be predicited occurs more than 10 words later.\n\n### Estimating Bigram Probabilities using the Maximum Likelihood Estimate\n\\begin{equation}\n    P(w_i|w_{i-1}) = \\frac{count(w_{i-1},w_i)}{count(w_{i-1})}\n\\end{equation}\n\n#### Small Example\n\nThree sentences:\n\n- <s I am Sam \/s>\n- <s Sam I am \/s>\n- <s I do not like green eggs and ham \/s>\n\n\\begin{equation}\nP(I|<s) = \\frac{count(<s,I)}{count(<s)} = \\frac{2}{3}\n\\end{equation}\n\n\n\\begin{equation}\nP(am|I) = \\frac{count(I,am)}{count(I)} = \\frac{2}{3}\n\\end{equation}\n\n\n\n[1] https:\/\/en.wikipedia.org\/wiki\/Markov_property\n\n","4458a438":"### Tri-grams and beyond!\n\nIf we continue the estimation equation for trigrams, we have the following:\n\n\\begin{equation}\n    P(x_3|x_1,x_2) \\approx \\frac{count(x_1,x_2,x_3)}{count(x_1,x_2)}\n\\end{equation}","a7707747":"#### To simplify the process we remove all grammer and lowercase all words in the sentences.\n\n#### We also add the string '<s' and '\/s>' to the start and end of each sentence respectively so that we can find which words start and complete the sentences. ","f7504a9a":"First, let us create a dummy training corpus and test set from the original data:","f44884e2":"#### And can reference individual setences via indexing. We can also find the total number of sentences for each comment but it appears that the last setence is always blank. Therefore, when we use this in our loop, we reduce the upper index bound by 1 to account for this.\n","5a158d93":"#### Using these, we break down our full commments into individual sentences. I have also introduced some methods for tracking and timing the progress of our for loops, for more info on these see the following:\n\n[PhilipOsborneData.com link](https:\/\/www.philiposbornedata.com\/2018\/06\/28\/the-simplest-cleanest-method-for-tracking-a-for-loops-progress-and-expected-run-time-in-python-notebooks\/)\n\n[Medium.com link](https:\/\/towardsdatascience.com\/the-simplest-cleanest-method-for-tracking-a-for-loops-progress-and-expected-run-time-in-python-972675392b3)\n    ","c2f2aa60":"Which we can use to find the probability of the whole sentence as:\n    \n\\begin{equation}\n    PP(W) = P(w_1 w_2 ... w_N)^{\\frac{-1}{N}}\n\\end{equation}\n","48f67c6b":"---\n---","cb8d06a6":"#### The unigram model can be similarly used to find the estimated probability of two (or more words) occuring in sequence.\n\nFor example, we can compute the probability of words 'to' and 'a' occuring:","603aad96":"Sample output sentence:\n\nSentence  0 : ['<s' 'root' 'for' 'the' 'movie' '\/s>']\n\nand we can manually explore each probability, for example:\n","ae86b1ca":"#### We can break the full text down into its individual sentences using the following:","b0bc4feb":"#### Enable equation numbering in jupyter notebook for improved readability:","8552637e":"---\n---\n\n# Part 3\n---\n## Challenges in Fitting LMs\n\nDue to the output of LMs is dependent on the training corpus, N-grams only work well if the training corpus is similar to the testing dataset and we risk overfitting in training. \n\nAs with any machine learning method, we would like results that are generalisable to new information. \n\nEven harder is how we deal with words that do not even appear in training but are in the test data. \n\n## IX. Dealing with Zero Counts in Training: Laplace +1 Smoothing\n\nTo deal with words that are unseen in training we can introduce add-one smoothing. To do this, we simply add one to the count of each word.\n\nThis shifts the distribution slightly and is often used in text classification and domains where the number of zeros isn't large. However, this is not often used for n-grams, instead we use more complex methods. \n\n\n","c2b4eae9":"### Bigram Model (k=2): $P(w_i|w_1 w_2 ... w_{i-1}) \\approx P(w_i|w_{i-1})$\n\nApplying this is somewhat more complex, first we find the co-occurances of each word into a word-word matrix. The counts are normalised by the counts of the previous word: \n\n\\begin{equation}\nP(w_i|w_{i-1}) \\approx \\frac{count(w_{i-1},w_i)}{count(w_{i-1})}\n\\end{equation}\n","ea3a6191":"#### Exhaustively applying method to find the optimal Lambda parameters that maximise the total probability of the hold-out subset.\n\nNote because lambda_1 + lambda_2 = 1 then we can define lambda_2 w.r.t. the chosen lambda_1 value.","2f5a3c49":"### Interpolation Example with IMDB Data\n\nSay we start with the corpus defined in the previous part, we again take a small subset of this as the 'hold-out' set."}}