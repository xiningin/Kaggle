{"cell_type":{"dbc9d118":"code","a1099e55":"code","c67b26ac":"code","9b9f5eb4":"code","da055a97":"code","af0dbb04":"code","b2155d1b":"code","689b36fd":"code","cf32f83c":"code","80245917":"code","015a7dc1":"code","58907fa1":"code","9c6f6753":"code","b188068d":"code","0a6fcb99":"code","67b2158e":"code","ce9cff4e":"code","ad481c40":"code","195d62f2":"code","dd4f1df5":"code","49941135":"code","423cc9f4":"code","f0f38c1c":"code","54ffbb59":"code","abb3b04c":"code","dbf672a9":"code","54926a28":"code","240eb78f":"code","083ee0f9":"code","5f68a281":"code","e0bbea41":"code","bffb13a2":"code","e886a608":"code","74707099":"code","eef4886c":"code","1e154c5b":"code","24ccc887":"code","06c038be":"code","91f8c7a0":"code","2c9649c7":"code","5fe192dd":"code","5b73ea26":"code","bc48ff4b":"code","e20569f1":"code","887221fb":"code","84902444":"code","155118da":"markdown","eb81f09a":"markdown","2c40409f":"markdown","3d35f04b":"markdown","dc2627bc":"markdown","e8615700":"markdown","5bba675f":"markdown","e0807615":"markdown","51682d05":"markdown","b9c2bc19":"markdown","ac6b69af":"markdown","877cf64e":"markdown","98cd60f2":"markdown","00a660d6":"markdown","9d02e644":"markdown","e93c6f8e":"markdown","91363635":"markdown","959ae275":"markdown","df97ae05":"markdown"},"source":{"dbc9d118":"#importing the required libraries\n\nimport numpy as np \nimport pandas as pd\n\n\n#importing the data from heart.csv\n\ndata=pd.read_csv(\"\/kaggle\/input\/heart-disease-dataset\/heart.csv\")\n\n\n#exploring the head to have an outlook of the data\n\ndata.head()","a1099e55":"#checking for null values \n\ndata.isnull().sum()","c67b26ac":"#looking at the shape\n\ndata.shape","9b9f5eb4":"#looking at the data types \n\ndata.dtypes","da055a97":"#looking at the unique value of each column\n\ndata.nunique()","af0dbb04":"#converting all the below columns into categories for easier processing\n\nfor x in [\"cp\",\"sex\",\"fbs\",\"exang\",\"restecg\",\"slope\",\"ca\",\"thal\"]:\n    data[x]=data[x].astype(\"category\")\n\ndata.dtypes","b2155d1b":"#getting an overview of the data \n\ndata.describe()","689b36fd":"#plotting count of genders in the data\n\nimport seaborn as sns\nsns.countplot(data.sex)","cf32f83c":"#plotting the distribution of age in the data\n\n\nimport plotly.figure_factory as ff\nff.create_distplot([data.age],[\"Age\"])","80245917":"#plotting the distribution of age with respect to genders\n\nff.create_distplot([data[data.sex==0].age,data[data.sex==1].age],[\"Age of Females\",\"Age of Males\"])","015a7dc1":"# grouping data by chest types to see if any chest pain type can attribute heart disease the most\n\nimport plotly.express as px\npx.bar(data.groupby(\"cp\").sum().reset_index()[[\"cp\",\"target\"]],x=\"cp\",y=\"target\",color=\"cp\",title=\"Count Plot of Heart Disease with Respect to Chest Pain\")","58907fa1":"# Finding the average resting blood pressure between the ones with heart disease and the ones without it. \n\ndata.groupby(\"target\").mean()[\"trestbps\"]","9c6f6753":"# Difference in Average Cholesterol between ones with heart disease and ones without it.\n\ndata.groupby(\"target\").mean()[\"chol\"]","b188068d":"# Finding maximum average heart rate between ones with heart disease and ones without it\n\ndata.groupby(\"target\").mean()[\"thalach\"]","0a6fcb99":"#plotting the distribution of maximum heart rate in ones with heart disease and ones without.\n\nff.create_distplot([data[data.target==0].thalach,data[data.target==1].thalach],[\"Max. Heart rate of ones without heart disease\",\"Max. Heart rate of ones with heart disease\"])","67b2158e":"# plotting the correlation heatmap \n\nimport matplotlib.pyplot as plt\nplt.figure(figsize=(12,10))\nsns.heatmap(data.corr())","ce9cff4e":"\n#grouping by target to check mean \n\ndata.groupby(\"target\").mean()[\"oldpeak\"]","ad481c40":"#Lets build the scatter matrix to see the relationship between the independent variables, to see if any are correlated\n\n\nfrom pandas.plotting import scatter_matrix\nscatter_matrix(data, figsize=(16, 16), diagonal='kde')\nplt.show()","195d62f2":"#converting target to category\n\ndata.target=data.target.astype(\"category\")","dd4f1df5":"#finding the highest correlated independent variables\n\nfor x in data.corr().columns:\n    print(np.abs(data.corr())[x].nlargest(2))","49941135":"# Making a comparing Pie chart between sex of heart disease patients and those without heart diseases\n\nimport plotly.graph_objects as go\nfrom plotly.subplots import make_subplots\n\nlabels = [\"Male\",\"Female\"]\n\n# Create subplots: use 'domain' type for Pie subplot\nfig = make_subplots(rows=1, cols=2, specs=[[{'type':'domain'}, {'type':'domain'}]])\nfig.add_trace(go.Pie(labels=labels, values=[57,43], name=\"Heart Disease\"),\n              1, 1)\nfig.add_trace(go.Pie(labels=labels, values=[82.8,17.2], name=\"No Heart Disease\"),\n              1, 2)\n\n# Use `hole` to create a donut-like pie chart\nfig.update_traces(hole=.4, hoverinfo=\"label+percent+name\")\n\nfig.update_layout(\n    title_text=\"Comparision of sex between Heart Disease patients and without it\",\n    # Add annotations in the center of the donut pies.\n    annotations=[dict(text='Disease', x=0.18, y=0.5, font_size=18, showarrow=False),\n                 dict(text='No Disease', x=0.84, y=0.5, font_size=18, showarrow=False)])\nfig.show()","423cc9f4":"# Making a comparing Pie chart of excercise induced angiana of heart disease patients and those without heart disease\n\nimport plotly.graph_objects as go\nfrom plotly.subplots import make_subplots\n\nlabels = [\"No Excercise induced angiana\",\"Having Excercise induced angiana\"]\n\n# Create subplots: use 'domain' type for Pie subplot\nfig = make_subplots(rows=1, cols=2, specs=[[{'type':'domain'}, {'type':'domain'}]])\nfig.add_trace(go.Pie(labels=labels, values=[45.1,54.9], name=\"Heart Disease\"),\n              1, 1)\nfig.add_trace(go.Pie(labels=labels, values=[86.5,13.5], name=\"No Heart Disease\"),\n              1, 2)\n\n# Use `hole` to create a donut-like pie chart\nfig.update_traces(hole=.4, hoverinfo=\"label+percent+name\")\n\nfig.update_layout(\n    title_text=\"Comparision of Excercise induced angiana between Heart Disease patients and without it\",\n    # Add annotations in the center of the donut pies.\n    annotations=[dict(text='Disease', x=0.16, y=0.5, font_size=18, showarrow=False),\n                 dict(text='No Disease', x=0.86, y=0.5, font_size=18, showarrow=False)])\nfig.show()","f0f38c1c":"# Making a comparing Pie chart of chest pain of heart disease patients and those without heart disease\n\nimport plotly.graph_objects as go\nfrom plotly.subplots import make_subplots\n\nlabels = [\"cp0\",\"cp1\",\"cp2\",\"cp3\"]\n\n# Create subplots: use 'domain' type for Pie subplot\nfig = make_subplots(rows=1, cols=2, specs=[[{'type':'domain'}, {'type':'domain'}]])\nfig.add_trace(go.Pie(labels=labels, values=[75.2,6.61,13,5.21], name=\"Heart Disease\"),\n              1, 1)\nfig.add_trace(go.Pie(labels=labels, values=[23.2,25.5,41.6,9.7], name=\"No Heart Disease\"),\n              1, 2)\n\n# Use `hole` to create a donut-like pie chart\nfig.update_traces(hole=.4, hoverinfo=\"label+percent+name\")\n\nfig.update_layout(\n    title_text=\"Comparision of chest pain values in Heart Disease patients and without it\",\n    # Add annotations in the center of the donut pies.\n    annotations=[dict(text='Disease', x=0.18, y=0.5, font_size=18, showarrow=False),\n                 dict(text='No Disease', x=0.84, y=0.5, font_size=18, showarrow=False)])\nfig.show()","54ffbb59":"# We will seperate the Independent variable and target variable \n\nX=data.iloc[:,:-1]\n\ny=data.target","abb3b04c":"# Let's Split the data into training and test set\n\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test=train_test_split(X,y,test_size=0.3,random_state=23)","dbf672a9":"#checking the shapes\n\nprint(X_train.shape,X_test.shape)","54926a28":"#checking the head to see if we need to normalise or dummify\n\nX_train.head()","240eb78f":"#splitting the data into numerical and categorical data types so that we can either standardise or dummify\n\nX_train_num=X_train.select_dtypes(include=[\"int64\",\"float64\"])\nX_test_num=X_test.select_dtypes(include=[\"int64\",\"float64\"])\nX_train_cat=X_train.select_dtypes(include=[\"category\"])\nX_test_cat=X_test.select_dtypes(include=[\"category\"])\nprint(X_train_num.columns,X_train_cat.columns)","083ee0f9":"#standardising numerical variables \n\nfrom sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nscaler.fit(X_train_num)\nX_train_num_stan=pd.DataFrame(scaler.transform(X_train_num),columns=X_train_num.columns,index=X_train_num.index)\nX_test_num_stan=pd.DataFrame(scaler.transform(X_test_num),columns=X_test_num.columns,index=X_test_num.index)\nX_test_num_stan","5f68a281":"# dummifying Categorical Variables\n\nX_train_cat_dum=pd.get_dummies(X_train_cat)\nX_test_cat_dum=pd.get_dummies(X_test_cat)","e0bbea41":"# Merging the numerical and categorical dataframes\n\nX_train_final=pd.concat([X_train_num_stan,X_train_cat_dum],axis=1)\nX_test_final=pd.concat([X_test_num_stan,X_test_cat_dum],axis=1)","bffb13a2":"# We will build a Regression model using statsmodels API, this gives us a deep description into the model, the fit,\n# the residual information, etc. However, Since this is a very old library, it has some restrictions\n\nimport statsmodels.api as sm\n\n#defining the model \nlr=sm.Logit(y_train,X_train_final).fit()\n\n#let's look at the summary\n\nprint(lr.summary())","e886a608":"# Getting the predictions from the training dataset \ntrain_predictions=lr.predict(X_train_final)","74707099":"# Compute ROC curve and ROC area for each class\n\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import roc_curve, auc\n\nfpr = dict()\ntpr = dict()\nroc_auc = dict()\nfpr, tpr, _ = roc_curve(y_train, train_predictions)\nroc_auc = auc(fpr, tpr)","eef4886c":"#plotting the roc curve \n\nplt.figure()\nlw = 2\nplt.plot(fpr, tpr, color='darkorange',\n         lw=lw, label='ROC curve (area = %0.2f)' % roc_auc)\nplt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver operating characteristic example')\nplt.legend(loc=\"lower right\")\nplt.show()","1e154c5b":"# Getting the test predictions \n\ntest_predictions=lr.predict(X_test_final)\ntest_predictions=[0 if x<0.15 else 1 for x in test_predictions]","24ccc887":"# Now lets get the metrics\n\n#classification Metrics\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import recall_score\nfrom sklearn.metrics import precision_score\n\nprint(\"Accuracy for Test set:\")\nprint(accuracy_score(y_test,test_predictions))\n\nprint(\"Recall for Test set:\")\nprint(recall_score(y_test,test_predictions,pos_label=1))\n\nprint(\"Precision for Test set:\")\nprint(precision_score(y_test,test_predictions,pos_label=1))","06c038be":"# let's build a Decision Tree classifier to see how it does compared to Logistic Regression\n\nfrom sklearn.tree import DecisionTreeClassifier\nDTC = DecisionTreeClassifier(random_state=0)\nDTC.fit(X_train_final,y_train)\ntrain_predictions=DTC.predict(X_train_final)\ntest_predictions=DTC.predict(X_test_final)\n","91f8c7a0":"# Now lets get the metrics for the test set\n\nprint(\"Accuracy for Test set:\")\nprint(accuracy_score(y_test,test_predictions))\n\nprint(\"Recall for Test set:\")\nprint(recall_score(y_test,test_predictions,pos_label=1))\n\nprint(\"Precision for Test set:\")\nprint(precision_score(y_test,test_predictions,pos_label=1))","2c9649c7":"#Let's do some cross validation\nfrom sklearn.model_selection import GridSearchCV\n\n# Use a grid over parameters of interest\nparam_grid = {\n     'criterion': [\"gini\",\"entropy\"]\n}\n\n \nCV_DTC = GridSearchCV(estimator=DTC, param_grid=param_grid, cv= 5)\nCV_DTC.fit(X=X_train_final, y=y_train)\nprint(CV_DTC.best_estimator_)\ntest_predictions=CV_DTC.predict(X_test_final)","5fe192dd":"# Now lets get the metrics for the test set after cross validation\n\nprint(\"Accuracy for Test set:\")\nprint(accuracy_score(y_test,test_predictions))\n\nprint(\"Recall for Test set:\")\nprint(recall_score(y_test,test_predictions,pos_label=1))\n\nprint(\"Precision for Test set:\")\nprint(precision_score(y_test,test_predictions,pos_label=1))","5b73ea26":"#XGBoost\nfrom xgboost import XGBClassifier\nXGB_model = XGBClassifier(max_depth=6,learning_rate=0.05)\nXGB_model.fit(X_train_final, y_train)\ntest_predictions=XGB_model.predict(X_test_final)\n","bc48ff4b":"# Now lets get the metrics for the test set after cross validation\n\nprint(\"Accuracy for Test set:\")\nprint(accuracy_score(y_test,test_predictions))\n\nprint(\"Recall for Test set:\")\nprint(recall_score(y_test,test_predictions,pos_label=1))\n\nprint(\"Precision for Test set:\")\nprint(precision_score(y_test,test_predictions,pos_label=1))","e20569f1":"fig = px.bar(y=[96.8,98.1,100],x=[\"Logistic Regression\",\"Decision Tree\", \"XGBoost\"],color=[\"Logistic Regression\",\"Decision Tree\", \"XGBoost\"],title=\"Comparision of Recall for Classification Models\",labels={\"x\":\"Model Name\",\"y\":\"Percentage\"})\nfig.show()","887221fb":"#let's get the feature importances in XGBoost\nsorted_idx = XGB_model.feature_importances_.argsort()\nplt.figure(figsize=(6, 8),dpi=70)\nplt.barh(X_test_final.columns[sorted_idx], XGB_model.feature_importances_[sorted_idx])\n\nplt.xlabel(\"Xgboost Feature Importance\")\nplt.show()","84902444":"#end","155118da":"<center><h4> Looking at the metrics, this is good model, but there is a lot of scope for optmisation <\/h4><\/center>","eb81f09a":"## <a id=\"3\">Model Building<\/a> ","2c40409f":"<center><h4>We can see that more than half the cases occured in cases where the type of chest pain is 2<h4><\/center>","3d35f04b":"<center><h4> From the above model we can see that the resting blood pressure(trestbps), maximum heart rate(thalach) and the oldpeak have the most significance. The Adjusted R-squared above is 0.58. This means it can only explain 58% of the residuals. <\/h4><\/center>","dc2627bc":"<center><h4>In the above Figure we can see the difference in the distribution of Maximum Heart Rate for the ones with Heart Disease, Their Heart seems to be working much harder during high intensity activities compared to resting rate. <\/h4><\/center>","e8615700":"\n## <a id=\"2\">Data Preprocessing<\/a>","5bba675f":"## <a id =\"7\">Results <\/a>","e0807615":"<center><h4>We have brought the recall up to 98.1% for the test set, Let's see if we can get it even higher<\/h4><center>","51682d05":"### <a id =\"5\">Decision Tree Classifier <\/a>","b9c2bc19":"<center><h4>In the above chart we can see that thal variable which show whether its a reversible defect has the highest importance, followed by the chest pains, so that is what we saw earlier in the visualizations earlier, its not usually possible to acheive 100% results but here the data seems to be highly correlated.  <\/h4><\/center>","ac6b69af":"<center><h4>Here we can see there is a major difference between the ones with heart disease and the ones without it. <\/h4><\/center>","877cf64e":"<center><h4>Here, we can see that XGBoost can explain 100% of the data and has a recall of 100% on the test data, on data which the model has never seen before<\/h4><\/center>","98cd60f2":"<b>Looking at the above ROC curve, the threshold i decided to put for the test predictions is 0.15, since we need the best recall possible. Ideally we dont want any false negetives since we want to test for the heart disease. If there is a false positive, the doctor can go in depth to look into it. But if it goes unnoticed a life is at risk <b>","00a660d6":"### <a id =\"6\">XGBoost Classifier <\/a>","9d02e644":"<center><h1> Heart Disease Prediction <\/h1><\/center>\n\n### We are going to use the following attributes to predict:\n\n1. age\n2. sex - category\n3. chest pain type (4 values)\n4. resting blood pressure\n5. serum cholestoral in mg\/dl\n6. fasting blood sugar > 120 mg\/dl\n7. resting electrocardiographic results (values 0,1,2) \n8. maximum heart rate achieved\n9. exercise induced angina\n10. oldpeak = ST depression induced by exercise relative to rest\n11. the slope of the peak exercise ST segment\n12. number of major vessels (0-3) colored by flourosopy\n13. thal: 0 = normal; 1 = fixed defect; 2 = reversable defect\n\n### Contents of the Notebook:\n#### [1. Importing and Exploratory Data Analysis](#1)\n#### [2. Data Pre processing ](#2)\n#### [3. Model Building](#3) \n##### [3.1 Logistic Regression](#4)\n##### [3.2 Decision Tree](#5)\n##### [3.3 XGBoost](#6)\n#### [4. Results](#7)","e93c6f8e":"### <a id =\"4\">Logistic Regression <\/a>","91363635":"Although we have got less attributes, we can see how each are mostly indepent from others. But there are some noticable relations in the data. \n\n* When you see the graph between age and cholesterol, it shows and increasing trend. \n* Similarly, the maximum heartrate to age shows a decreasing trend. \n* The oldpeak (ST Depression induced by excercise, Basically an abnormality in the ECG relative to rest) is increasing in magnitude as age increases. \n\nThis is a good healthy data with not much correlation upon independent variables. The maximum correlation found here is -0.34 and thats between thalach (max heart rate) variable and oldpeak","959ae275":"<center><h4>From the above we can see how complete the data is and how the XGBoost model can explain the test set with 100% accuracy and 100% percentage recall, but here we lose the explainability since we dont exactly know the coefficients like we do in logistic regression. However, we can get the feature importances using XGBoost<\/h4><\/center>","df97ae05":"## <a id=1>Importing and Exploratory Data Analysis<\/a>"}}