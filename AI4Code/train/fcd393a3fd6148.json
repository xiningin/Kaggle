{"cell_type":{"bb69163c":"code","f189c9d4":"code","94d2d444":"code","c54f28ed":"code","9d76189b":"code","7d0dbce7":"code","ed66c7b1":"code","a7ddf91c":"code","6b231279":"code","5bbc4991":"code","656c66d2":"code","04f6467b":"code","592d7770":"code","0e6566dd":"code","d88f23a1":"code","cf8f2788":"code","6d444bf1":"code","e7a09667":"markdown","f97f3990":"markdown","8f902c61":"markdown","ef836b36":"markdown","39a6b305":"markdown","abc39d25":"markdown","2825e46d":"markdown","02809fe5":"markdown","9471e4cc":"markdown","f85b6d42":"markdown","5d58a761":"markdown"},"source":{"bb69163c":"!pip install alibi","f189c9d4":"import tensorflow as tf\ntf.get_logger().setLevel(40)\ntf.compat.v1.disable_v2_behavior() \nfrom tensorflow.keras.layers import Dense, Input\nfrom tensorflow.keras.models import Model, load_model\nfrom tensorflow.keras.utils import to_categorical\n\nimport matplotlib\n%matplotlib inline\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport os\nimport pandas as pd\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom alibi.explainers import CEM\n\nprint('TF version: ', tf.__version__)\nprint('Eager execution enabled: ', tf.executing_eagerly()) # False","94d2d444":"dataset = pd.read_csv('..\/input\/heart-disease-cleveland-uci\/heart_cleveland_upload.csv')\n# To display the top 5 rows\ndataset.head(5)","c54f28ed":"heart = dataset.copy()","9d76189b":"target = 'condition'\nfeature_names = list(heart.columns)\nfeature_names.remove(target)","7d0dbce7":"y = heart.pop('condition')","ed66c7b1":"heart = (heart - heart.mean(axis=0)) \/ heart.std(axis=0)","a7ddf91c":"X_train, X_test, y_train, y_test = train_test_split(heart, y, test_size=0.2, random_state=33)\nx_train=X_train.to_numpy()\nx_test=X_test.to_numpy()\ny_train = to_categorical(y_train)\ny_test = to_categorical(y_test)","6b231279":"def nn_model():\n    x_in = Input(shape=(13,))\n    x = Dense(40, activation='relu')(x_in)\n    x = Dense(40, activation='relu')(x)\n    x_out = Dense(2, activation='softmax')(x)\n    nn = Model(inputs=x_in, outputs=x_out)\n    nn.compile(loss='categorical_crossentropy', optimizer='sgd', metrics=['accuracy'])\n    return nn","5bbc4991":"nn = nn_model()\nnn.summary()\nnn.fit(X_train, y_train, batch_size=64, epochs=500, verbose=0)\nnn.save('nn_heart.h5', save_format='h5')","656c66d2":"idx = 1\nX = x_test[idx].reshape((1,) + x_test[idx].shape)\nprint('Prediction on instance to be explained: {}'.format([np.argmax(nn.predict(X))]))\nprint('Prediction probabilities for each class on the instance: {}'.format(nn.predict(X)))","04f6467b":"mode = 'PN' \nshape = (1,) + x_train.shape[1:]  \nkappa = .2 \n            \n            \nbeta = .1  \nc_init = 10. \nc_steps = 10\nmax_iterations = 1000  \nfeature_range = (x_train.min(axis=0).reshape(shape)-.1,  \n                 x_train.max(axis=0).reshape(shape)+.1)  \nclip = (-1000.,1000.)  \nlr_init = 1e-2  ","592d7770":"lr = load_model('nn_heart.h5')\n\n# initialize CEM explainer and explain instance\ncem = CEM(lr, mode, shape, kappa=kappa, beta=beta, feature_range=feature_range,\n          max_iterations=max_iterations, c_init=c_init, c_steps=c_steps,\n          learning_rate_init=lr_init, clip=clip)\ncem.fit(x_train, no_info_type='median')  \nexplanation = cem.explain(X, verbose=False)","0e6566dd":"print('Feature names: {}'.format(feature_names))\nprint('Original instance: {}'.format(explanation.X))\nprint('Predicted class: {}'.format([explanation.X_pred]))\nprint('Pertinent negative: {}'.format(explanation.PN))\nprint('Predicted class: {}'.format([explanation.PN_pred]))","d88f23a1":"mode = 'PP'","cf8f2788":"# define model\nlr = load_model('nn_heart.h5')\n\n# initialize CEM explainer and explain instance\ncem = CEM(lr, mode, shape, kappa=kappa, beta=beta, feature_range=feature_range,\n          max_iterations=max_iterations, c_init=c_init, c_steps=c_steps,\n          learning_rate_init=lr_init, clip=clip)\ncem.fit(x_train, no_info_type='median')\nexplanation = cem.explain(X, verbose=False)","6d444bf1":"print('Original instance: {}'.format(explanation.X))\nprint('Predicted class: {}'.format([explanation.X_pred]))\nprint('Pertinent positive: {}'.format(explanation.PP))\nprint('Predicted class: {}'.format([explanation.PP_pred]))","e7a09667":"Training the model:","f97f3990":"Generating contrastive explaination for pertinent negative:","8f902c61":"Here, \n\n*   mode : 'PN' (Pertinent Negative) or 'PP' (Pertinent Positive)\n*   shape : Shape of the current instance. As CEM is applicable for single explanations, we take 1.\n*   kappa, beta, gamma, c_init, c_steps are all mathematical terms for calculating loss\n*   max_iterations : the total no. of loss optimization steps for each value of c\n*   feature_range : global or feature wise minimum and maximum values for the changed instance\n*   clip : minimum and maximum gradient values\n*   lr_init : initial learning rate \n\n","ef836b36":"Consider the second instance of testing data.","39a6b305":"# Contrastive Explanations Method(CEM) applied to Heart dataset\n","abc39d25":"Reading the  dataset:","2825e46d":"The original prediction class is 0, since it has a greater prediction probability.","02809fe5":"The above result shows that the predicted class remains same on applying PP. The CEM values generated, close to 0, should be compulsorily and minimally present in order to get the same original class 0 as predicted class.","9471e4cc":"Generating pertinent positive:","f85b6d42":"The above result clearly shows that the pertinent negative method pushes the prediction to get a prediction different from the original prediction which is 0 to 1 in this case.\n\nThe CEM values in array which are different from the original one change the prediction class. Some of them are cp, ca, thal. Thus, it can be concluded that changes in these features should necessarily be absent to retain the original prediction as 0 as they are responsible for flipping the prediction class.","5d58a761":"Contrastive Explanation Method, abbreviated as CEM, is a XAI Method which can give local explanations for a black box model. This method is applicable for classification datasets. CEM gives two kinds of explanations: \n\nPertinent Positives (PP): For a PP, the method finds the features that should be minimally and sufficiently present (e.g. important pixels in an image) to predict the same class as on the original instance.  PP works similarly to Anchors.\n\n\nPertinent Negatives (PN): PN\u2019s on the other hand identify what features should be minimally and necessarily absent from the instance to be explained in order to maintain the original prediction class. The aim of PN\u2019s is not to provide a full set of characteristics that should be absent in the explained instance, but to identify a minimal set of features that is enough to differentiate it from the nearest different class. PN works similarly to Counterfactuals.\n\n\n\n\n"}}