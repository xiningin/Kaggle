{"cell_type":{"41c5e2a0":"code","48be0022":"code","b36ce429":"code","179ec5f6":"code","05eaf69e":"code","052212c7":"code","c10c3feb":"code","41ef9176":"code","ccb2ff5c":"code","9a5e40b8":"code","708065bb":"code","8097a4c0":"code","cc24e7d6":"code","922eff8c":"code","4f5f572c":"code","71d668e8":"code","708a44b4":"code","7ab2073f":"code","f574e262":"code","55ae2287":"code","3db0692a":"code","9e7c1144":"code","b4c02997":"code","ed2737d0":"code","e589d67b":"code","642f9104":"code","e51ba3e8":"code","903c13f4":"code","a0df52b4":"code","c9c0087b":"code","6adec857":"code","34ba2165":"code","02a1c187":"code","5b65419a":"code","1b7f65ea":"code","bb006135":"code","d7f04ae5":"code","de8b81fd":"code","6caf4480":"code","471fff97":"code","94e3021d":"code","4d2ffc67":"code","5140f406":"code","20a56984":"code","e88c49bb":"code","9c63c2c4":"code","550763d9":"code","a2bc4253":"code","d808d416":"code","129cd597":"code","6f0ced0e":"code","07916ab2":"code","0c744f13":"code","53e76cc6":"code","9994a327":"code","c5c4508d":"code","f33809cd":"code","c2beee6a":"code","aa594cd0":"code","b1223f67":"code","c807833e":"code","98472545":"code","6bdf9fc1":"code","64871019":"code","c648d47e":"code","24c60f5e":"code","4f5ab946":"code","4a304b10":"code","404b0480":"code","f2be7b38":"code","2502e6d4":"markdown","99d0f52b":"markdown","4702cc4c":"markdown","39e65164":"markdown","60ca5759":"markdown","025144c2":"markdown","e8d87c1e":"markdown","415cfbcf":"markdown","f5728e79":"markdown","18786a29":"markdown","06448160":"markdown","97554897":"markdown","4ed7f185":"markdown","07565982":"markdown","2ef5d977":"markdown","dc5c1a9d":"markdown","afb6dbc8":"markdown","e8803a16":"markdown","649d09ad":"markdown","99496273":"markdown","e55789cc":"markdown","4159ff16":"markdown","039efdeb":"markdown","9f1cd758":"markdown","dd5fcb35":"markdown","04d76b19":"markdown","ab2e90ff":"markdown"},"source":{"41c5e2a0":"#import the relevant libraries\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport pickle\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import train_test_split","48be0022":"# read the dataset\n\nred_wine_data = pd.read_csv('..\/input\/red-and-white-wine-quality-datasets\/winequality-red.csv', delimiter =';')\nred_wine_data.head()","b36ce429":"# 1 Shape\nred_wine_data.shape","179ec5f6":"# 2 Datatypes\nred_wine_data.dtypes","05eaf69e":"# 3 Missing values\nred_wine_data.info()","052212c7":"# 4 Identify zero variance columns\nfor col in red_wine_data:\n    print(col, red_wine_data[col].value_counts().count())","c10c3feb":"# 5. Range of numbers in each column\nfor i in red_wine_data.columns:\n    print(\"Range of {}: minimum {} & maximum {} \" .format(i, red_wine_data[i].min(), red_wine_data[i].max()))# 5. Range of numbers in each column\nfor i in red_wine_data.columns:\n    print(\"Range of {}: minimum {} & maximum {} \" .format(i, red_wine_data[i].min(), red_wine_data[i].max()))","41ef9176":"# 6 Relationship between features & the target\n# check how the quality is influenced by fixed acidity\nsns.barplot(x=red_wine_data['quality'], y=red_wine_data['fixed acidity'])","ccb2ff5c":"# check how the quality is influenced by volatile acidity\nsns.barplot(x=red_wine_data['quality'], y=red_wine_data['volatile acidity'])","9a5e40b8":"# check how the quality is influenced by citric acid\nsns.barplot(x=red_wine_data['quality'], y=red_wine_data['citric acid'])","708065bb":"# check how the quality is influenced by residual sugar\nsns.barplot(x=red_wine_data['quality'], y=red_wine_data['residual sugar'])","8097a4c0":"# check how the quality is influenced by chlorides\nsns.barplot(x=red_wine_data['quality'], y=red_wine_data['chlorides'])","cc24e7d6":"# check how the quality is influenced by free sulfur dioxide\nsns.barplot(x=red_wine_data['quality'], y=red_wine_data['free sulfur dioxide'])","922eff8c":"# check how the quality is influenced by total sulfur dioxide\nsns.barplot(x=red_wine_data['quality'], y=red_wine_data['total sulfur dioxide'])","4f5f572c":"# check how the quality is influenced by density\nsns.barplot(x=red_wine_data['quality'], y=red_wine_data['density'])","71d668e8":"# check how the quality is influenced by pH\nsns.barplot(x=red_wine_data['quality'], y=red_wine_data['pH'])","708a44b4":"# check how the quality is influenced by sulphates\nsns.barplot(x=red_wine_data['quality'], y=red_wine_data['sulphates'])","7ab2073f":"# check how the quality is influenced by alcohol\nsns.barplot(x=red_wine_data['quality'], y=red_wine_data['alcohol'])","f574e262":"# 6 Relationship between features & the target - Correlation matrix\nred_wine_data.corr()","55ae2287":"plt.figure(figsize=(16, 6))\nsns.heatmap(red_wine_data.corr(), annot=True);","3db0692a":"#7. Target: Check for discrete values\nred_wine_data['quality'].value_counts()","9e7c1144":"# create the \"features and target\" data sets\nX = red_wine_data.drop('quality',axis=1)\ny = red_wine_data['quality']\n\n# split the features and target data sets into train and test sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nprint('Model 1 train\/test shapes:')\nprint(X_train.shape, X_test.shape, y_train.shape, y_test.shape)","b4c02997":"# create and fit a linear regression model\nlm_red_wine1 = LinearRegression()\nred_model1 = lm_red_wine1.fit(X_train, y_train)\n\n# computing yhat (ie train_predictions) using X (ie train_features)\ntrain_predictions = lm_red_wine1.predict(X_train)\ntrain_prediction = [int(round(x,0)) for x in train_predictions]","ed2737d0":"# simple function to compare actual and predicted values\ndef compare_prediction(y, yhat):\n    comp_matrix = pd.DataFrame(zip(y_train,train_prediction), columns = ['Actual', 'Predicted'])\n    comp_matrix['Err'] = abs(comp_matrix['Actual']-comp_matrix['Predicted'])\n    comp_matrix['PctErr'] = comp_matrix['Err']\/comp_matrix['Actual'] * 100\n    mean_value = np.mean(comp_matrix['PctErr'])\n    return comp_matrix, mean_value","e589d67b":"# compare actual and predicted values\ncomp_matrix, mean = compare_prediction(y, train_prediction)\nprint(\"Model 1 prediction comparison and mean error:\", comp_matrix, mean)\n\naccuracy1 = round((100-mean),2)\nprint('Model1 accuracy =', accuracy1)","642f9104":"### 3. Dimensionality reduction\/Drop the identified columns\nlst = ['pH', 'free sulfur dioxide', 'residual sugar']\nred_wine_data.drop(lst, axis =1, inplace = True)","e51ba3e8":"# create the \"features and target\" data sets\nX = red_wine_data.drop('quality',axis=1)\ny = red_wine_data['quality']\n\n# split the features and target data sets into train and test sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nprint('Model 2 train\/test shapes:')\nprint(X_train.shape, X_test.shape, y_train.shape, y_test.shape)","903c13f4":"# create and fit a linear regression model\nlm_red_wine2 = LinearRegression()\nred_model2 = lm_red_wine2.fit(X_train, y_train)\n\n# computing yhat (ie train_predictions) using X (ie train_features)\ntrain_predictions = lm_red_wine2.predict(X_train)\ntrain_prediction = [int(round(x,0)) for x in train_predictions] ","a0df52b4":"# compare actual and predicted values\ncomp_matrix, mean = compare_prediction(y, train_prediction)\nprint(\"Model 2 prediction comparison and mean error:\", comp_matrix, mean)\n\naccuracy2 = round((100-mean),2)\nprint(\"Model 2 accuracy =\", accuracy2)","c9c0087b":"### 3. Dimensionality reduction\/Drop the identified columns\nlst = ['fixed acidity', 'citric acid', 'chlorides', 'total sulfur dioxide', 'density']\nred_wine_data.drop(lst, axis =1, inplace = True)","6adec857":"# create the \"features and target\" data sets\nX = red_wine_data.drop('quality',axis=1)\ny = red_wine_data['quality']\n\n# split the features and target data sets into train and test sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nprint('Model 2 train\/test shapes:')\nprint(X_train.shape, X_test.shape, y_train.shape, y_test.shape)","34ba2165":"# create and fit a linear regression model\nlm_red_wine3 = LinearRegression()\nred_model3 = lm_red_wine3.fit(X_train, y_train)\n\n# computing yhat (ie train_predictions) using X (ie train_features)\ntrain_predictions = lm_red_wine3.predict(X_train)\ntrain_prediction = [int(round(x,0)) for x in train_predictions]","02a1c187":"# compare actual and predicted values\ncomp_matrix, mean = compare_prediction(y, train_prediction)\nprint(\"Model 3 prediction comparison and mean error:\", comp_matrix, mean)\n\naccuracy3 = round((100-mean),2)\nprint(\"Model 3 accuracy =\", accuracy3)","5b65419a":"print(\"Model1 Accuracy {}\".format(accuracy1))\nprint(\"Model2 Accuracy {}\".format(accuracy2))\nprint(\"Model3 Accuracy {}\".format(accuracy3))","1b7f65ea":"# save red_model3 as per analysis\nmodel_file = open('red_wine_model.pkl', 'wb')\npickle.dump(obj=red_model3, file=model_file)\nmodel_file.close()","bb006135":"# reload the model from disk and check if it is saved properly.\nmodel_file = open('red_wine_model.pkl', 'rb')\nlr_model = pickle.load(model_file)\nmodel_file.close()\nprint(lr_model)","d7f04ae5":"# Read the white wine quality dataset\nwhite_wine_data = pd.read_csv('..\/input\/red-and-white-wine-quality-datasets\/winequality-white.csv', delimiter = ';')\nwhite_wine_data.head()","de8b81fd":"# 1 Shape\nwhite_wine_data.shape","6caf4480":"# 2 Datatypes\nwhite_wine_data.dtypes","471fff97":"# 3 Missing values\nwhite_wine_data.info()","94e3021d":"#4 zero variance column needs to be removed. \nfor col in white_wine_data:\n    print(col, white_wine_data[col].value_counts().count())","4d2ffc67":"# 5. Range of numbers in each column\nfor i in white_wine_data.columns:\n    print(\"Range of {}: minimum {} & maximum {} \" .format(i, white_wine_data[i].min(), white_wine_data[i].max()))","5140f406":"# 6 Relationship between features & the target\n# check how the quality is influenced by fixed acidity\nsns.barplot(x=white_wine_data['quality'], y=white_wine_data['fixed acidity'])","20a56984":"# check how the quality is influenced by volatile acidity\nsns.barplot(x=white_wine_data['quality'], y=white_wine_data['volatile acidity'])","e88c49bb":"# check how the quality is influenced by citric acid\nsns.barplot(x=white_wine_data['quality'], y=white_wine_data['citric acid'])","9c63c2c4":"# check how the quality is influenced by residual sugar\nsns.barplot(x=white_wine_data['quality'], y=white_wine_data['residual sugar'])","550763d9":"# check how the quality is influenced by chlorides\nsns.barplot(x=white_wine_data['quality'], y=white_wine_data['chlorides'])","a2bc4253":"# check how the quality is influenced by free sulfur dioxide\nsns.barplot(x=white_wine_data['quality'], y=white_wine_data['free sulfur dioxide'])","d808d416":"# check how the quality is influenced by total sulfur dioxide\nsns.barplot(x=white_wine_data['quality'], y=white_wine_data['total sulfur dioxide'])","129cd597":"# check how the quality is influenced by density\nsns.barplot(x=white_wine_data['quality'], y=white_wine_data['density'])","6f0ced0e":"# check how the quality is influenced by pH\nsns.barplot(x=white_wine_data['quality'], y=white_wine_data['pH'])","07916ab2":"# check how the quality is influenced by sulphates\nsns.barplot(x=white_wine_data['quality'], y=white_wine_data['sulphates'])","0c744f13":"# check how the quality is influenced by alcohol\nsns.barplot(x=white_wine_data['quality'], y=white_wine_data['alcohol'])","53e76cc6":"# 6 Relationship between features & the target\nwhite_wine_data.corr()","9994a327":"plt.figure(figsize=(16, 6))\nsns.heatmap(white_wine_data.corr(), annot=True);","c5c4508d":"#7. Target: Check for discrete values\nwhite_wine_data['quality'].value_counts()","f33809cd":"# create the \"features and target\" data sets\nX = white_wine_data.drop('quality',axis=1)\ny = white_wine_data['quality']\n\n# split the features and target data sets into train and test sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nprint('White Wine Model 1 train\/test shapes:')\nprint(X_train.shape, X_test.shape, y_train.shape, y_test.shape)","c2beee6a":"# create and fit a linear regression model\nlm_white_wine1 = LinearRegression()\nwhite_model1 = lm_white_wine1.fit(X_train, y_train)\n\n# computing yhat (ie train_predictions) using X (ie train_features)\ntrain_predictions = lm_white_wine1.predict(X_train)\ntrain_prediction = [int(round(x,0)) for x in train_predictions]","aa594cd0":"# compare actual and predicted values\ncomp_matrix, mean = compare_prediction(y, train_prediction)\nprint(\"White Wine Model 1 prediction comparison and mean error:\", comp_matrix, mean)\n\naccuracy1 = round((100-mean),2)\nprint(\"White Wine Model 1 accuracy =\", accuracy1)","b1223f67":"### 3. Dimensionality reduction\/Drop the identified columns\nlst = ['fixed acidity', 'volatile acidity', 'citric acid','residual sugar',\n       'total sulfur dioxide', 'free sulfur dioxide','pH','sulphates']\nwhite_wine_data.drop(lst, axis=1,inplace=True)","c807833e":"# create the \"features and target\" data sets\nX = white_wine_data.drop('quality',axis=1)\ny = white_wine_data['quality']\n\n# split the features and target data sets into train and test sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nprint('White Wine Model 2 train\/test shapes:')\nprint(X_train.shape, X_test.shape, y_train.shape, y_test.shape)","98472545":"# create and fit a linear regression model\nlm_white_wine2 = LinearRegression()\nwhite_model2 = lm_white_wine2.fit(X_train, y_train)\n\n# computing yhat (ie train_predictions) using X (ie train_features)\ntrain_predictions = lm_white_wine2.predict(X_train)\ntrain_prediction = [int(round(x,0)) for x in train_predictions]","6bdf9fc1":"# compare actual and predicted values\ncomp_matrix, mean = compare_prediction(y, train_prediction)\nprint(\"White Wine Model 2 prediction comparison and mean error:\", comp_matrix, mean)\n\naccuracy2 = round((100-mean),2)\nprint(\"White Wine Model 2 accuracy =\", accuracy2)","64871019":"### 3. Dimensionality reduction\/Drop the identified columns\nlst = ['chlorides', 'density']\nwhite_wine_data.drop(lst, axis=1,inplace=True)","c648d47e":"# create the \"features and target\" data sets\nX = white_wine_data.drop('quality',axis=1)\ny = white_wine_data['quality']\n\n# split the features and target data sets into train and test sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nprint('White Wine Model 3 train\/test shapes:')\nprint(X_train.shape, X_test.shape, y_train.shape, y_test.shape)","24c60f5e":"# create and fit a linear regression model\nlm_white_wine3 = LinearRegression()\nwhite_model3 = lm_white_wine3.fit(X_train, y_train)\n\n# computing yhat (ie train_predictions) using X (ie train_features)\ntrain_predictions = lm_white_wine3.predict(X_train)\ntrain_prediction = [int(round(x,0)) for x in train_predictions]","4f5ab946":"# compare actual and predicted values\ncomp_matrix, mean = compare_prediction(y, train_prediction)\nprint(\"White Wine Model 3 prediction comparison and mean error:\", comp_matrix, mean)\n\naccuracy3 = round((100-mean),2)\nprint(\"White Wine Model 3 accuracy =\", accuracy3)","4a304b10":"print(\"Model1 Accuracy {}\".format(accuracy1))\nprint(\"Model2 Accuracy {}\".format(accuracy2))\nprint(\"Model3 Accuracy {}\".format(accuracy3))","404b0480":"# save white_model1 as per analysis\nmodel_file = open('white_wine_model.pkl','wb')\npickle.dump(white_model1, model_file)\nmodel_file.close()","f2be7b38":"# reload the model from disk and check if it is saved properly\nmodel_file = open('white_wine_model.pkl', 'rb')\nlr_model = pickle.load(model_file)\nmodel_file.close()\nprint(lr_model)","2502e6d4":"## Section 1 - Red Wine Analysis, Model Creation and Saving","99d0f52b":"## EDA\/Preprocessing\nRefer to the EDA steps in the red wine analysis","4702cc4c":"### Sanity Check","39e65164":"#### Choose the best model for deployment.\n\nSince we are getting almost the same accuracy across models, we are going for model3 as it has least number of features.","60ca5759":"### Develop an ML Model as per the Pipeline\nRefer to the corresponding red wine section","025144c2":"#### Checklist of STANDARD EDA items\n\n1. Strategy for missing data\n    1. Action: No missing data, no action to be taken\n    \n    \n2. Convert categorical to numeric\n    1. Action: No Categorical data, no action to be taken\n    \n    \n3. Dimensionality reduction\/Drop the identified columns\n    1. Action: Drop identified columns in Insights 6A and 6B\n    \n    \n4. Check for Outliers, Normalize data in columns to fit a range (*Optional*)\n    1. Action: As per Insights 5A there are no Outliers\n\n#### Approach:\nWe will follow a 3-step approach as outlined below:\n\nStep 1:\n1. First, we will process the complete dataset without dropping any columns.\n2. We will build the ML model with the complete data, test and validate the predictions.\n\nStep 2:\n1. As per Insights 6A, we will drop the columns that show very weak correlations. These columns are - `pH`, `free sulfur dioxide`, `residual sugar`\n2. The dataset will thus have 9 features (including target)\n3. We will build the ML model with the remaining data, test and validate the predictions\n\nStep 3:\n1. As per Insights 6B, we will next drop the columns that show weak correlations. These columns are - `fixed acidity`, `citric acid`, `chlorides`, `total sulfur dioxide`, `density`\n2. The dataset will thus have 4 features (including target)\n3. We will build the ML model with the remaining data, test and validate the predictions\n\nStep 4:\n1. Compare the Accuracy of all the three models developed\n2. Choose the best model for deployment\n","e8d87c1e":"# Welcome to this kaggle notebook!\n\nThis notebook is intended for beginners of machine learning. I have used a machine learning pipeline to build a linear regression model to predict the quality of red and white wine given their various characteristics.\n\nI have also created a simple Flask web application for model deployment. You can deploy this in your local machine or in Heroku (the Procfile, requirements.txt are also provided).\n\nThe complete codebase is available in github at https:\/\/github.com\/saigeethachandrashekar\/wine_quality.git","415cfbcf":"#### Step 3: Drop columns showing weak correlations (0.2 - 0.4)\n1. Drop columns `chlorides` and `density`\n2. Build the ML model, test and validate the predictions.\n\nNote: we will be left with just 1 column - i.e., `alcohol` - this doesn't make sense, but let's continue nonetheless.","f5728e79":"#### Step 4:\n\nCompare the Accuracy of all the three models developed\n","18786a29":"# Wine Quality Model Deployment\nSo far, we have after analyzed the given datasets, computed the accuracies of various models and chosen an appropriate model. The next step is to deploy these models in production.\n\nFor this we will be developing a Flask web application. This application will load both the red and white wine models,  allow users to enter various physiochemical inputs and obtain wine quality predictions. This web application cannot be run on Kaggle - you need to run it on your local system or on a platform like Heroku.\n\nComplete code has been uploaded onto github at https:\/\/github.com\/saigeethachandrashekar\/wine_quality.\n\nPlease clone the repo - this contains both the datasets, the code required for building and saving the model on to your local system. Code for a Flask app is provided for deploying the models on your local machine. The app can also be deployed on Heroku - the requirements.txt and Procfile are also provided for this.\n\n","06448160":"## Section 2 - White Wine Analysis, Model Creation and Saving\nWe will follow the same steps as we did for Red Wine analysis.","97554897":"### Red Wine: Model Saving","4ed7f185":"### EDA\/Preprocessing\n_(Based on the insights from the sanity check, we can now determine how to process the data.)_","07565982":"### Sanity Check\n\n1. Shape and data sufficiency: Check if there are sufficient rows of data for an ML problem\n2. Datatypes: Check whether all the columns in the given dataset is numeric\n3. Missing Values: Check whether there are missing values\n4. Zero-variance: Check if there are any zero variance column in the dataset\n5. Range of numbers in each column: Check if the column values within the dataset are in the same magnitude\n6. Correlation: Check correlation between feature columns & target\n7. Target: Check for discrete values","2ef5d977":"### Insights \/ Sanity Check Conclusions\n\n1. **Shape and data sufficiency: Check if there are sufficient rows of data for an ML problem**\n    1. **INSIGHT:** Shape of the data is (1599, 12). i.e., dataset contains ~1600 observations, which is much greater than number of columns (12). Hence we can apply ML techniques rather than statistical rule-based approach.\n\n\n2. **Datatypes: Check whether all the columns in the given dataset is numeric**\n    1. **INSIGHT:** `Dtype` indicates that all columns are numeric\n    \n\n3. **Missing Values: Check whether there are missing values**\n    1. **INSIGHT:** `Non-Null Count` indicates there are no missing values in the dataset\n\n\n4. **Zero-variance: Check if there are any zero variance column in the dataset**\n    1. **INSIGHT:** No zero-variance columns found in the dataset\n\n\n5. **Range of numbers in each column: Check if the column values within the dataset are in the same magnitude**\n    1. **INSIGHT:** Each column has numbers within the same magnitude or plottable in a graph\n\n\n6. **Correlation: Check correlation between feature columns & target**\n    1. **INSIGHT:** The columns `pH`, `free sulfur dioxide`, `residual sugar` have very weak correlation (0.00 - 0.20)\n    2. **INSIGHT:** The columns `fixed acidity`, `citric acid`, `chlorides`, `total sulfur dioxide`, `density` have weak correlation (0.20 - 0.40)\n    3. ***Note:*** *The barplots and correlation heatmap complement each other and reveal these findings\n    4. ***Note:*** *absolute values of correlations were considered*\n\n\n7. **Other Observations:**\n    1. **INSIGHT:** Since (a) the target is given (b) target is continuous (number between 0..10), we can conclude that this is a supervised linear regression problem\n    2. **INSIGHT:** The target variable, i.e., `quality` has discrete values which indicates that this can be solved using classification methods also. However we will continue with Linear Regression in this exercise.\n","dc5c1a9d":"#### Step 4:\n\nCompare the Accuracy of all the three models developed","afb6dbc8":"### Insights \/ Sanity Check Conclusions\n\n\n1. **Shape and data sufficiency: Check if there are sufficient rows of data for an ML problem**\n    1. **INSIGHT:** Shape of the data is (4898, 12), which is much greater than number of columns (12). Hence we can apply ML techniques rather than statistical rule-based approach.\n\n\n2. **Datatypes: Check whether all the columns in the given dataset is numeric**\n    1. **INSIGHT:** `Dtype` indicates that all columns are numeric\n    \n\n3. **Missing Values: Check whether there are missing values**\n    1. **INSIGHT:** `Non-Null Count` indicates there are no missing values in the dataset\n\n\n4. **Zero-variance: Check if there are any zero variance column in the dataset**\n    1. **INSIGHT:** No zero-variance columns found in the dataset\n\n\n5. **Range of numbers in each column: Check if the column values within the dataset are in the same magnitude**\n    1. **INSIGHT:** Each column has numbers within the same magnitude\n\n\n6. **Correlation: Check correlation between feature columns & target**\n\n    1. **INSIGHT:** The columns `fixed acidity`, `volatile acidity`, `citric acid`,`residual sugar`, `total sulphur dioxide`, `free sulphur dioxide`,`ph`,`sulphates` have very weak correlation (0.00 - 0.20)\n    2. **INSIGHT:** The columns `chlorides`, `density` have weak correlation (0.20 - 0.40)\n    3. ***Note:*** *The barplots and correlation heatmap complement each other and reveal these findings\n    4. ***Note:*** *absolute values of correlations were considered*\n\n7. **Other Observations:**\n    1. **INSIGHT:** Since (a) the target is given (b) target is continuous (number between 0..10), we can conclude that this is a supervised linear regression problem\n    2. **INSIGHT:** The target variable, i.e., `quality` has discrete values which indicates that this can be solved using classification methods also. However we will continue with Linear Regression in this exercise.\n","e8803a16":"#### Step 3: Drop columns showing weak correlations (0.2 - 0.4)\n1. Drop columns fixed acidity, citric acid, chlorides, total sulfur dioxide, density.\n2. Build the ML model, test and validate the predictions.","649d09ad":"### Checklist of STANDARD EDA items\n\n1. Strategy for missing data\n    1. Action: No missing data, no action to be taken\n    \n    \n2. Convert categorical to numeric\n    1. Action: No Categorical data, no action to be taken\n    \n    \n3. Dimensionality reduction\/Drop the identified columns\n    1. Action: Drop identified columns in Insights 6A and 6B\n    \n    \n4. Check for Outliers, Normalize data in columns to fit a range (*Optional*)\n    1. Action: As per Insights 5A there are no Outliers\n\n### White Wine Analysis Approach After Insights:\nWe will follow a 3-step approach as outlined below:\n\nStep 1:\n1. First, we will process the complete dataset without dropping any columns.\n2. We will build the ML model with the complete data, test and validate the predictions.\n\nStep 2:\n1. As per Insights 6A, we will drop the columns that show very weak correlations. These columns are - `pH`, `free sulfur dioxide`, `residual sugar`\n2. The dataset will thus have 9 features (including target)\n3. We will build the ML model with the remaining data, test and validate the predictions\n\nStep 3:\n1. As per Insights 6B, we will next drop the columns that show weak correlations. These columns are - `fixed acidity`, `citric acid`, `chlorides`, `total sulfur dioxide`, `density`\n2. The dataset will thus have 4 features (including target)\n3. We will build the ML model with the remaining data, test and validate the predictions\n\nStep 4:\n1. Compare the Accuracy of all the three models developed\n2. Choose the best model for deployment\n","99496273":"#### Step 1:\nBuild the ML model with the complete data, test and validate the predictions.","e55789cc":"#### Step 2: Drop columns showing very weak correlations (0.0 - 0.2)\n1. Drop pH, free sulfur dioxide, residual sugar columns.\n2. Build the ML model, test and validate the predictions.","4159ff16":"### Develop an ML Model for Red Wine\n\nDeveloping an ML model involves a number of steps.\n\nLet us adopt the following Machine Learning Pipeline:\n\n1. Sanity Check \n2. EDA\/Preprocessing\n3. Feature Engineering\n4. Model Building\n5. Model Saving\n6. Model Deployment - this is covered under the `deployment` folder\n\nOnce the model is deployed, the pipeline extends to include the below steps:\n\n7. Model in Production\n8. Observe model behaviour\n9. Obtain updated datasets\n10. Redo steps 1..9 if required\n\n_Note: these extended steps are not covered in this exercise_","039efdeb":"#### Choose the best model for deployment.\nModel 3 predicts the quality using just 1 column - i.e., `alcohol` - this doesn't make sense. We will be conservative and choose Model 1 for the way forward.","9f1cd758":"## Approach for Wine Quality Model Development\n\nWe have covered both red and white wine quality predictions in this kernel. We have analyzed them separately and have developed models for each dataset adopting a standard ML pipeline. We have also covered model deployment briefly at the end of the kernel.\n\n### How the code is organized\n\nThe code is provided in 2 sections:\n\n1. **Section 1:** Red Wine Analysis, Model Creation and Saving\n2. **Section 2:** White Wine Analysis, Model Creation and Saving\n3. Deployment information and github link\n","dd5fcb35":"#### Step 2: Drop columns showing very weak correlations (0.0 - 0.2)\n1. drop the columns `fixed acidity`, `volatile acidity`, `citric acid`,`residual sugar`, `total sulfur dioxide`, `free sulfur dioxide`,`pH`,`sulphates`\n2. Build the ML model, test and validate the predictions.","04d76b19":"#### Step 1:\nBuild the ML model with the complete data, test and validate the predictions.","ab2e90ff":"### White Wine Model Saving"}}