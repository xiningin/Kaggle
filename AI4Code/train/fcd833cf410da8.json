{"cell_type":{"97786f91":"code","3edb0093":"code","357fe03f":"code","5a3aa7a4":"code","16c868f2":"code","b9c995bc":"code","1b9b5e04":"code","6ceb59d9":"code","18b2ea30":"code","de88916c":"code","5913fddf":"code","70080c62":"code","6acf499a":"code","e92c949b":"code","b95beef0":"code","767052b0":"code","3c833d0d":"code","e88a5cde":"code","dcc2d480":"code","5630c106":"code","bea1aed8":"code","03a5ceae":"code","58a2b564":"code","9023a6cf":"code","9b494bf0":"code","e43bfe4f":"code","b2257c84":"code","0bd6972c":"code","9e69943c":"code","50a935d4":"code","8ac0ef75":"code","5c6c57ea":"code","6dad1aee":"code","7f966c46":"markdown","276788ed":"markdown","4313a537":"markdown","16a6af17":"markdown","a64f007d":"markdown","c9ab5beb":"markdown","0ab4afa5":"markdown","a74e441e":"markdown","7ee75db7":"markdown","2d5b030d":"markdown","6d3ff5e3":"markdown","120e656b":"markdown","e3483124":"markdown","421ea65a":"markdown","a26b2a36":"markdown","87ed5f9a":"markdown"},"source":{"97786f91":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt #plotting\nimport seaborn as sns #good visualizing\nimport os\nimport warnings\nwarnings.filterwarnings('ignore')\nprint(os.listdir(\"..\/input\"))\ndata = pd.read_csv('..\/input\/winequality-red.csv')\ndata.columns = data.columns.str.replace(' ','_')\ndata.info()","3edb0093":"data.describe()","357fe03f":"#correlation map view\ndata.corr() \nf, ax = plt.subplots(figsize = (10,10))\nsns.heatmap(data.corr(), annot = True, linewidths=.5, fmt = \".2f\", ax=ax)\nplt.show()","5a3aa7a4":"fig, axes = plt.subplots(11,11, figsize=(50,50))\nfor i in range(11):\n    for j in range(11):\n        axes[i, j].scatter(data.iloc[:,i], data.iloc[:,j], c = data.quality)\n        axes[i,j].set_xlabel(data.columns[i])\n        axes[i,j].set_ylabel(data.columns[j])\n        axes[i,j].legend(data.quality)\nplt.show()","16c868f2":"g = sns.pairplot(data, hue=\"quality\")","b9c995bc":"#How many wine quality number is realted with how many unique wines\n#print(data['quality'].value_counts())\nsns.barplot(data['quality'].unique(),data['quality'].value_counts())\nplt.xlabel(\"Quality Rankings\")\nplt.ylabel(\"Number of Red Wine\")\nplt.title(\"Distribution of Red Wine Quality Ratings\")\nplt.show()","1b9b5e04":"print(data['quality'].value_counts())","6ceb59d9":"#Check the outliers for each feature with respect to output value\nfig, ax1 = plt.subplots(4,3, figsize=(22,16))\nk = 0\nfor i in range(4):\n    for j in range(3):\n        if k != 11:\n            sns.boxplot('quality',data.iloc[:,k], data=data, ax = ax1[i][j])\n            k += 1\nplt.show()","18b2ea30":"#Check the outliers for each feature with respect to output value\nfig, ax1 = plt.subplots(4,3, figsize=(22,16))\nk = 0\nfor i in range(4):\n    for j in range(3):\n        if k != 11:\n            sns.barplot('quality',data.iloc[:,k], data=data, ax = ax1[i][j])\n            k += 1\nplt.show()","de88916c":"#Fucntion Part\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import learning_curve\nfrom sklearn.model_selection import ShuffleSplit\nimport graphviz  \nfrom sklearn.externals.six import StringIO\nfrom IPython.display import Image \n\n#Normalization ==> x_norm = (x - mean)\/std \n#it gives for each value the same value intervals means between 0-1\ndef normalization(X):\n    mean = np.mean(X)\n    std = np.std(X)\n    X_t = (X - mean)\/std\n    return X_t\n\n#Train and Test splitting of data     \ndef train_test(X_t, y):\n    x_train, x_test, y_train, y_test = train_test_split(X_t, y, test_size = 0.3, random_state = 42)\n    print(\"Train:\",len(x_train), \" - Test:\", len(x_test))\n    return x_train, x_test, y_train, y_test\n\ndef grid_search(name_clf, clf, x_train, x_test, y_train, y_test):\n    if name_clf == 'Logistic_Regression':\n        # Logistic Regression \n        log_reg_params = {\"penalty\": ['l1', 'l2'], 'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000]}\n        grid_log_reg = GridSearchCV(LogisticRegression(), log_reg_params)\n        grid_log_reg.fit(x_train, y_train)\n        # We automatically get the logistic regression with the best parameters.\n        log_reg = grid_log_reg.best_estimator_\n        print(\"Best Parameters for Logistic Regression: \", grid_log_reg.best_estimator_)\n        print(\"Best Score for Logistic Regression: \", grid_log_reg.best_score_)\n        print(\"------------------------------------------\")\n        return log_reg\n    \n    elif name_clf == 'SVM':\n        # Support Vector Classifier\n        svc_params = {'C':[0.0001, 0.001, 0.01, 0.1, 1.0, 10.0, 100.0, 1000.0],\n                      'kernel': ['rbf', 'poly', 'sigmoid', 'linear']}\n        grid_svc = GridSearchCV(SVC(), svc_params)\n        grid_svc.fit(x_train, y_train)\n        # SVC best estimator\n        svc = grid_svc.best_estimator_\n        print(\"Best Parameters for SVM: \", grid_svc.best_estimator_)\n        print(\"Best Score for SVM: \", grid_svc.best_score_)\n        print(\"------------------------------------------\")\n        return svc\n    \n    elif name_clf == 'Decision_Tree':\n        # DecisionTree Classifier\n        tree_params = {\"criterion\": [\"gini\", \"entropy\"], \"max_depth\": list(range(2,30,1)), \n                  \"min_samples_leaf\": list(range(5,20,1))}\n        grid_tree = GridSearchCV(DecisionTreeClassifier(), tree_params)\n        grid_tree.fit(x_train, y_train)\n        # tree best estimator\n        tree_clf = grid_tree.best_estimator_\n        print(\"Best Parameters for Decision Tree: \", grid_tree.best_estimator_)\n        print(\"Best Score for Decision Tree: \", grid_tree.best_score_)\n        print(\"------------------------------------------\")\n        \n        #FEATURE IMPORTANCE FOR DECISION TREE\n        importnce = tree_clf.feature_importances_\n        plt.figure(figsize=(10,10))\n        plt.title(\"Feature Importances of Decision Tree\")\n        plt.barh(X_t.columns, importnce, align=\"center\")\n        \n        return tree_clf\n    \n    elif name_clf == 'Random_Forest':\n        forest_params = {\"bootstrap\":[True, False], \"max_depth\": list(range(2,10,1)), \n                  \"min_samples_leaf\": list(range(5,20,1))}\n        grid_forest = GridSearchCV(RandomForestClassifier(), forest_params)\n        grid_forest.fit(x_train, y_train)\n        # forest best estimator\n        forest_clf = grid_forest.best_estimator_\n        print(\"Best Parameters for Random Forest: \", grid_forest.best_estimator_)\n        print(\"Best Score for Random Forest: \", grid_forest.best_score_)\n        print(\"------------------------------------------\")\n        \n        #FEATURE IMPORTANCE FOR DECISION TREE\n        importnce = forest_clf.feature_importances_\n        plt.figure(figsize=(10,10))\n        plt.title(\"Feature Importances of Random Forest\")\n        plt.barh(X_t.columns, importnce, align=\"center\")\n        \n        return forest_clf\n    \ndef plot_learning_curve(estimator,title, X, y, ylim=None, cv=None, n_jobs=None,\n                        train_sizes=np.linspace(.1, 1.0, 5)):\n    plt.figure()\n    plt.title(title)\n    if ylim is not None:\n        plt.ylim(*ylim)\n    plt.xlabel(\"Training examples\")\n    plt.ylabel(\"Score\")\n    train_sizes, train_scores, test_scores = learning_curve(estimator, X, y, cv=cv, \n                                                            n_jobs=n_jobs, train_sizes=train_sizes)\n    train_scores_mean = np.mean(train_scores, axis=1)\n    train_scores_std = np.std(train_scores, axis=1)\n    test_scores_mean = np.mean(test_scores, axis=1)\n    test_scores_std = np.std(test_scores, axis=1)\n    plt.grid()\n\n    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n                     train_scores_mean + train_scores_std, alpha=0.1,\n                     color=\"r\")\n    plt.fill_between(train_sizes, test_scores_mean - test_scores_std,\n                     test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n    plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n             label=\"Training score\")\n    plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n             label=\"Cross-validation score\")\n\n    plt.legend(loc=\"best\")\n    return plt\n\n#Create applying classification funciton\ndef apply_classification(name_clf, clf, x_train, x_test, y_train, y_test):\n    #Find the best parameters and get the classification with the best parameters as return valu of grid search\n    grid_clf = grid_search(name_clf, clf, x_train, x_test, y_train, y_test)\n    \n    #Plotting the learning curve\n    # score curves, each time with 30% data randomly selected as a validation set.\n    cv = ShuffleSplit(n_splits=10, test_size=0.3, random_state=42)\n    plot_learning_curve(grid_clf, name_clf, x_train, y_train, \n                    ylim=(0.1, 1.01), cv=cv, n_jobs=4)\n    \n    #Apply cross validation to estimate the skills of models with 10 split with using best parameters\n    scores = cross_val_score(grid_clf, x_train, y_train, cv=10)\n    print(\"Mean Accuracy of Cross Validation: %\", round(scores.mean()*100,2))\n    print(\"Std of Accuracy of Cross Validation: %\", round(scores.std()*100))\n    print(\"------------------------------------------\")\n    \n    #Predict the test data as selected classifier\n    clf_prediction = grid_clf.predict(x_test)\n    clf1_accuracy = sum(y_test == clf_prediction)\/len(y_test)\n    print(\"Accuracy of\",name_clf,\":\",clf1_accuracy*100)\n    \n    #print confusion matrix and accuracy score before best parameters\n    clf1_conf_matrix = confusion_matrix(y_test, clf_prediction)\n    print(\"Confusion matrix of\",name_clf,\":\\n\", clf1_conf_matrix)\n    print(\"==========================================\")\n    return grid_clf","5913fddf":"#Now seperate the dataset as response variable and feature variabes\nX = data.drop(['quality'], axis = 1)\n#y = pd.DataFrame(data['value'])\ny = data['quality']","70080c62":"#Normalization\nX_t = normalization(X)\nprint(\"X_t:\", X_t.shape)\n\n#Train and Test splitting of data \nx_train, x_test, y_train, y_test = train_test(X_t, y)","6acf499a":"from sklearn.linear_model import LogisticRegression\n\nlr = LogisticRegression()\napply_classification('Logistic_Regression', lr, x_train, x_test, y_train, y_test)","e92c949b":"from sklearn.svm import SVC\n\nsvm = SVC()\napply_classification('SVM', svm, x_train, x_test, y_train, y_test)","b95beef0":"from sklearn.tree import DecisionTreeClassifier, export_graphviz\nfrom sklearn import tree\n\ndt = DecisionTreeClassifier()\ndt_clf = apply_classification('Decision_Tree', dt, x_train, x_test, y_train, y_test)","767052b0":"#Plot the decision tree \ndot_data = export_graphviz(dt_clf, out_file=None, filled=True, rounded=True,special_characters=True)\ngraph = graphviz.Source(dot_data)\ngraph","3c833d0d":"from sklearn.ensemble import RandomForestClassifier\n\nrf = RandomForestClassifier(n_estimators=100)\napply_classification('Random_Forest', rf, x_train, x_test, y_train, y_test)","e88a5cde":"#Add a new feature according to mean of the quality\n#Good wine represented by 1, bad wine represented by 0\ndata['value'] = \"\"\ndata['value'] = [1 if each > 5 else 0 for each in data['quality']]\n\nprint(\"Good Wine Class:\",data[data['value'] == 1].shape)\nprint(\"Bad Wine Class:\",data[data['value'] == 0].shape)","dcc2d480":"#Check the outliers for each feature with respect to output value\nfig, ax1 = plt.subplots(4,3, figsize=(22,16))\nk = 0\nfor i in range(4):\n    for j in range(3):\n        if k != 11:\n            sns.boxplot('value',data.iloc[:,k], data=data, ax = ax1[i][j])\n            k += 1\nplt.show()","5630c106":"#Categorical distribution plots:\nfig, ax1 = plt.subplots(4,3, figsize=(22,16))\nk = 0\nfor i in range(4):\n    for j in range(3):\n        if k != 11:\n            sns.barplot(x=\"value\",y=data.iloc[:,k],hue = 'value', data=data, ax = ax1[i][j])\n            k += 1\nplt.show()","bea1aed8":"fig, axes = plt.subplots(11,11, figsize=(50,50))\nfor i in range(11):\n    for j in range(11):\n        axes[i, j].scatter(data.iloc[:,i], data.iloc[:,j], c = data.value)\n        axes[i,j].set_xlabel(data.columns[i])\n        axes[i,j].set_ylabel(data.columns[j])\n        axes[i,j].legend(data.value)\nplt.show()","03a5ceae":"#Now seperate the dataset as response variable and feature variabes\nXb = data.drop(['quality','value'], axis = 1)\n#y = pd.DataFrame(data['value'])\nyb = data['value']","58a2b564":"#Normalization\nXb_t = normalization(Xb)\nprint(\"X_t:\", Xb_t.shape)\n\n#Train and Test splitting of data \nxb_train, xb_test, yb_train, yb_test = train_test(Xb_t, yb)","9023a6cf":"lrb = LogisticRegression()\napply_classification('Logistic_Regression', lrb, xb_train, xb_test, yb_train, yb_test)\n\nsvmb = SVC()\napply_classification('SVM', svmb, xb_train, xb_test, yb_train, yb_test)\n\ndtb = DecisionTreeClassifier()\ndtb_clf = apply_classification('Decision_Tree', dtb, xb_train, xb_test, yb_train, yb_test)\n\nrfb = RandomForestClassifier(n_estimators=100)\napply_classification('Random_Forest', rfb, xb_train, xb_test, yb_train, yb_test)","9b494bf0":"#Plot the decision tree \ndot_data = export_graphviz(dtb_clf, out_file=None, filled=True, rounded=True,special_characters=True)\ngraph = graphviz.Source(dot_data)\ngraph","e43bfe4f":"#Add a new feature according to mean of the quality\n#Good wine represented by 2, average 1, and bad wine represented by 0\ndata['value'] = \"\"\ndata['value'] = [2 if each > 6 else 1 if ((each > 4) and (each < 7)) else 0 for each in data['quality']]\n\nprint(\"Good Wine Class:\",data[data['value'] == 2].shape)\nprint(\"Average Wine Class:\",data[data['value'] == 1].shape)\nprint(\"Bad Wine Class:\",data[data['value'] == 0].shape)","b2257c84":"#Check the outliers for each feature with respect to output value\nfig, ax1 = plt.subplots(4,3, figsize=(22,16))\nk = 0\nfor i in range(4):\n    for j in range(3):\n        if k != 11:\n            sns.boxplot('value',data.iloc[:,k], data=data, ax = ax1[i][j])\n            k += 1\nplt.show()","0bd6972c":"#Categorical distribution plots:\nfig, ax1 = plt.subplots(4,3, figsize=(22,16))\nk = 0\nfor i in range(4):\n    for j in range(3):\n        if k != 11:\n            sns.barplot(x=\"value\",y=data.iloc[:,k],hue = 'value', data=data, ax = ax1[i][j])\n            k += 1\nplt.show()","9e69943c":"fig, axes = plt.subplots(11,11, figsize=(50,50))\nfor i in range(11):\n    for j in range(11):\n        axes[i, j].scatter(data.iloc[:,i], data.iloc[:,j], c = data.value)\n        axes[i,j].set_xlabel(data.columns[i])\n        axes[i,j].set_ylabel(data.columns[j])\n        axes[i,j].legend(data.value)\nplt.show()","50a935d4":"#Now seperate the dataset as response variable and feature variabes\nX3 = data.drop(['quality','value'], axis = 1)\n#y = pd.DataFrame(data['value'])\ny3 = data['value']","8ac0ef75":"#Normalization\nX3_t = normalization(X3)\nprint(\"X_t:\", X3_t.shape)\n\n#Train and Test splitting of data \nx3_train, x3_test, y3_train, y3_test = train_test(X3_t, y3)","5c6c57ea":"lr3 = LogisticRegression()\napply_classification('Logistic_Regression', lr3, x3_train, x3_test, y3_train, y3_test)\n\nsvm3 = SVC()\napply_classification('SVM', svm3, x3_train, x3_test, y3_train, y3_test)\n\ndt3 = DecisionTreeClassifier()\ndt3_clf = apply_classification('Decision_Tree', dt3, x3_train, x3_test, y3_train, y3_test)\n\nrf3 = RandomForestClassifier(n_estimators=100)\napply_classification('Random_Forest', rf3, x3_train, x3_test, y3_train, y3_test)","6dad1aee":"#Plot the decision tree \ndot_data = export_graphviz(dt3_clf, out_file=None, filled=True, rounded=True,special_characters=True)\ngraph = graphviz.Source(dot_data)\ngraph","7f966c46":"**2. SVM**\n\nSVM Support Vector Machine is a discriminative classifier by a separating hyperplane and supervised learning technique for Machine Learning. Differently from the unsupervised learning algorithms, there is a dataset belongs to different classes(labels). Data is trained with those class labels and then it is predicted with test data set then calculate accuracy how the algorithm predicts test data correctly. In other words, separating dataset into labeled training and test(categorize) dataset with labeled data, it could be better with work with as binary classification.\nIn more detail, SVM uses margin and hyperplane instead of line to separate data into two or more different class. In order to separate classes, it can be drawn many different lines but by choosing best line it is considered that margin should be maximum in between support vectors which are the closest points with different classes. While SVM algorithm is working, it follows two rules which are firstly classify correctly, then increase the margins in hyperplane. Additionally, SVM is useful for non-linear classification by increasing dimension then find a new line classification in increased dimension.\nThe C parameter tells the SVM optimization how many misclassifying point of each training example. For large values of C, the optimization will choose a smaller-margin hyperplane if that hyperplane does a better job of getting all the training points classified correctly. Conversely, a very small value of C will cause the optimizer to look for a larger-margin separating hyperplane, even if that hyperplane misclassifies more points. For very tiny values of C, there are misclassified examples, often even if training data is linearly separable.","276788ed":"**3. Decision Tree**\n\nTree models where the target variable can take a discrete set of values are called classification trees; in these tree structures, leaves represent class labels and branches represent conjunctions of features that lead to those class labels. The standard deviation is used to calculate the homogeneity of a numerical sample. After each standard deviation calculations, standard deviation reduction is used to classify dataset. The standard deviation reduction is based on the decrease in standard deviation after a dataset is split on an attribute. Constructing a decision tree is all about finding attribute that returns the highest standard deviation reduction.\nA decision tree is drawn upside down with its root at the top involves partitioning the data into subsets that contain instances with similar values (homogenous), then on the middle there are condition\/internal node based on the tree split into branches\/edges. The end of the branch that doesn\u2019t split anymore it is the decision\/leaf tree, means that they are the last classification nodes(qualities). The base algorithm of the decision tree; recursive binary splitting. In this procedure, all the features are considered and different split points are tried and tested using a cost function. The split with the best cost (or lowest cost) is selected. The cost function is used to understand how model split and predict the split dataset classifications.","4313a537":"**Ordinal Dataset Analyze**\n\ngrid_search, cross validation, feature selection for noise, try with taking important features then look what happens... the purpose to get good accuracy classification.","16a6af17":"**Data Introduction**\n\nIn this dataset there are specifically red wine variants of Poteguese \"Vinho Verde\" wine.  Due to privacy and logistic issues, only physicochemical (inputs) and sensory (the output) variables are available (e.g. there is no data about grape types, wine brand, wine selling price, etc.).\nFor more information, read [Cortez et al., 2009].\n\nIn the data set, there 1599 different wine as row data and 12 features as columns. Furthermore, there is no nun value to deal with it and all values are numeric means that input values are float and only output value is integer.\n\n","a64f007d":"**3 Categorical Analysis**","c9ab5beb":"**Apply all previous classification algorithms**","0ab4afa5":"According to figure in above;\n* Quality has a (+)positive relationship  between alcohol \n* Quality has a (-)negative weak relationship  between volitile_acidicity\n* Quality has almost no relationship  between residual_sugar, free_sulfur_dioxide, and pH.(corr =~ 0)\n\n* Alcohol  has a (+)positive relationship  between quality and weakly pH\n* Alcohol  has a (-)negative relationship  between density\n* Alcohol  has almost no relationship  between fixed_acidicity, residual_sugar, free_sulfur_dioxide, sulphates\n\n* Volitile_acidicity has a weak (+)positive relationship  between pH.\n* Volitile_acidicity has a strong (-)negative relationship  between citric_acid\n* Volitile_acidicity has weak (-)negative relationship  between fixed_acidicity and sulphates\n* Volitile_acidicity has almost no relationship between residual_sugar, chlorides, free_sulfur_dioxide, total_sulfur_dioxide, density\n\n* Density has (+)positive relationship between fixed_acidicity\n* Density has (-)negative relationship between density\n* Density has almost no relationship between volitile_acidicity, free_sulfur_dioxide, total_sulfur_dioxide\n\n* Citric_acid has (+)positive relationship between fixed_acidicity\n* Citric_acid has (-)negative relationship between volitile_acidicity, pH\n* Citric_acid has almost no relationship between residual_sugar, free_sulfur_dioxide, total_sulfur_dioxide\n\nIt seems very hard to analyze dataset like that, therefore, it is better to go deep down analyzing with some other visualizations.","a74e441e":"**Data visualization**\n\nThis data set has many different features and it is important to understand relationship between these in order to analyze dataset better. For that reason, correlation map helps to understand these relations in a single representation. Correlation map is made by calculating the covariance of each features with respect to others, then each covariance value is divided by standard deviation of each variables and get results between -1, 0, 1.\n\n-1 means: There is a negative relationship between dependent and independent variables .\n\n0 means: There is no relationship between dependent and independent variables .\n\n1 means: There is a positive relationship between dependent and independent variables .\nAccording to these information, it can be made a good analyze about dataset and columns.","7ee75db7":"**4. Random Forest**\n\nRandom forest --> baging pf the decision tree\nRandom forests construct many individual decision trees at training and it uses the simplicity of decision trees with flexibility resulting in improvement the accuracy. Predictions from all trees are pooled to make the final prediction; the mode of the classes for classification or the mean prediction for regression. As they use a collection of results to make a final decision.\nRandom forest algorithm contains many variables, and many categorical variables with a large number of class labels. It gives results using data sets that show a loss or unbalanced distribution. When new trees are added into the random forest, algorithm updates itself with decreasing the loss by eliminating noises.\nBootstraping the sample data(creating some mini sample dataset with less variable), then calcuating with regression to the gini then pick the highly correlated, therefore first 2-3 split will be the same because of the central limit theorem because the variance of the sum is decrreasing","2d5b030d":"The measures of central tendency and variability or distribution are some commonly used measures to define the data set. The measures used to define the central tendency are mean, median and mode. The standard deviations (or variance) are the minimum and maximum values of variables. The table above is a summary of some statistical measures for each numeric predictor of the dataset: \n**count** indicates the number of records for each attribute that corresponds to the number of wines. \n**mean** indicates the average value around which each group of attributes is attested.\n**std** indicates the standard deviation of each attribute group, from which we can guess the degree of data dispersion around the average.\n**max** and min indicate the attribute that I assume the highest and lowest value for each attribute group.","6d3ff5e3":"**Binary dataset Analyze**\n\nThis dataset can be proper for classification and regression techniques because the data is multivariate, contains numeric values as an input and output value is ordered and not balanced means that there are much more normal wines than excellent or poor ones. As it seems from the figure in above, data is not balanced and the ranges are between 3-8 score. For this reason, it is more convenient to start with making data balanced.","120e656b":"**Classificaiton and Cross Validation**\n\n    1. Logistic Regression\n    2. SVM\n    3. Decision Tree\n    4. Random Forest","e3483124":"**1. Logistic Regression**\n\nThe logistic regression is a predictive analysis of statistical method. Logistic regression is analyzing a dataset in which there are one or more independent variables that determine an outcome. The outcome is measured with a dichotomous variable (in which there are only two possible outcomes). It is used to predict a binary outcome (1 \/ 0, Yes \/ No, True \/ False) given a set of independent variables. Logistic regression is a classification algorithm used to assign observations to a discrete set of classes. For this data set, firstly using ordinal logistic regression then it is better to using binary logistic regression with modified dataset.\nFormulation of the logistic regression:\nP= 1\/(1+ e^(-(b_0+ b_1 x)) )","421ea65a":"**Red Wine Quality Classification**\n\nIn this analysis of the work, it will be determined which physiochemical properties make red wine 'good!' by using some machine learning techniques. Content of the paper is starting with introduction of the data set, and some data visualization. Then some classification and one regression technique will be applied to the data and with cross validation accuracy will be evaluated. Lastly, the best estimator will be decided according to cross validation score and conculusion.\n\n-Introduction\n\n-Data Introduction(Data upload and some enhancements)\n\n-Data visualization\n\n-Classification and Cross Validation Results\n\n    1. Logistic Regression\n    2. SVM\n    3. Decision Tree\n    4. Random Forest\n    \n -Best Classifier according to classfication accuraies \n \n -Conculusion","a26b2a36":"**Conclusion**\n\n\nFor this work, it was aimed that the analyzing which psychochemical are more related with wine quality and which approach is good for prediction of wine quality better. After all work, it is obvious that working with binary classification is more better the predict good or bad wines. During this research, four important machine learning techniques was used;\n\n--> Logistic regression\n\n--> Support Vector Machine \n\n--> Decision Tree\n\n--> Random Forest\n\nFrom all algorithms, it was obvious that for this dataset, SVM and then Random Forest algorithm gave the best model and accuracy means that those algorithms predict correctly test data. If someone wants to analyze similar data like that it is better to work SVM or Random Forest. Hence, those algorithms variances are found better with high margin terminology, therefore with multiclass analysis, those algorithms will give the best accuracy.\nAfter the analysis of this dataset, some features have more effect to deciding quality of the wine, there are some insights about the criteria about wine quality, you can compare just looking the some psychochemical on the label of wines;\n*Should be higher;*\n\n** Alcohol is the most important feature to decide quality of the wine. If the alcohol percentage is high enough, it means that quality of the wine should be better\n\n** Sulphates is another selecting criteria for good wines, with high percentage sulphates wine quality is increasing\n\n** Citric Acid is another selecting criteria, it should be higher to decide more better wine\n\n*Should be lower;*\n\n** Volatile Acidity should be less in the good wine\n\n** Sulfur dioxide is another effect to decreasing wine quality and also it causes head ache therefore if\nthere is less sulfur dioxide in wine, it should be selected\n\n** Chlorides value has very less effect to quality of the wine but again it is obvious more value of it\ncauses bad quality of the wine\n\nAdditionally, for marketing point of view, if a customer wants to buy a wine just looking with some psychochemical values can decide what s\/he needs to buy. Of course, brand and price feature was evaluated on this research, therefore, it is not a good analysis for saying \u201cit is good wine\u201d. However, it can give some idea for the people who do not have more knowledge about wine for selecting the good wine maybe for just dinner or gift for friends!","87ed5f9a":"Now my data seems with the value dimension as balanced because the count of those two classes seems very equal with each other Good wine dataset equals to 855, and bad wine dataset equals to 744. Then I can modify quality column values with by value column values"}}