{"cell_type":{"54c718ee":"code","053ecf4b":"code","2d083a88":"code","a38effcf":"code","a1700188":"code","cf49c5d3":"code","665a771e":"code","b10c7efc":"code","4ce92963":"code","fa6f5754":"code","852a0d56":"code","ed5236f0":"code","19a734cc":"code","c7cd7644":"code","379c8552":"code","2f297b30":"code","8575ec3e":"code","49049686":"code","3f78a6db":"code","1b54a7ea":"code","7619c07b":"code","731fb1d8":"code","3a03a1ed":"code","9db549a5":"code","18d1093e":"code","7070dcc9":"code","2301b290":"code","350e4944":"code","71ec5c1f":"code","2c6b7ab2":"code","2c432707":"code","b0e01f27":"code","b73bcc80":"code","f7456519":"code","9baa91e0":"code","7e4251be":"code","5dc923ce":"code","3ba58335":"code","fabbb4b2":"code","f77e3b39":"code","37aa3808":"code","71de2e99":"code","57227e17":"code","a31696c1":"code","a6ebb582":"code","92e93b46":"code","5e7b5817":"code","4704e538":"code","8844604c":"code","5e5b3caf":"code","6b8195ab":"markdown","8f402365":"markdown","13f4a2ce":"markdown","acf7dea1":"markdown","ff3b1bbf":"markdown","db7c7c8c":"markdown","b01682ae":"markdown","c9cd7ae5":"markdown","0b9ebb9d":"markdown","9185202e":"markdown","fc7b9a85":"markdown","f84d0d3e":"markdown","a8c8b3c2":"markdown","7cffcc37":"markdown","9d0345e2":"markdown","f2967345":"markdown","14b65be1":"markdown","5f9669f0":"markdown","5f382fde":"markdown","47f97f81":"markdown","66261387":"markdown","96099855":"markdown","d0a0b8f9":"markdown","049c83bd":"markdown","ca73062c":"markdown","1cdfa382":"markdown","55f85b12":"markdown","b74e927d":"markdown","319bbf3e":"markdown","663d1c35":"markdown","5e8ce136":"markdown","93f77a25":"markdown","59d35e58":"markdown","3fdc9555":"markdown","a6c2fa2b":"markdown","06288990":"markdown","3b90d24e":"markdown","633139ba":"markdown","81d6fcd8":"markdown"},"source":{"54c718ee":"# Import necessary libraries\nimport numpy as np\nimport pandas as pd\nimport re\nimport sklearn\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom pandas.api.types import is_string_dtype\nfrom pandas.api.types import is_numeric_dtype\nfrom sklearn.ensemble import RandomForestClassifier, VotingClassifier\nfrom sklearn.preprocessing import OneHotEncoder, LabelEncoder, StandardScaler\nfrom sklearn.metrics import roc_curve, auc, f1_score\nfrom sklearn.model_selection import train_test_split, StratifiedKFold, GridSearchCV, RandomizedSearchCV\nfrom sklearn.linear_model import LogisticRegression\nimport joblib\n\n%matplotlib inline","053ecf4b":"df = pd.read_csv('..\/input\/telco-customer-churn\/WA_Fn-UseC_-Telco-Customer-Churn.csv')\ndf.shape","2d083a88":"print(df.info())","a38effcf":"def print_df_unique_vals(df):\n    for col in df.columns:\n        unique_vals = df[col].unique()\n        if len(unique_vals) < 10:\n            print(\"Unique values for column {}: {}\".format(col, unique_vals))\n        else:\n            if is_string_dtype(df[col]):\n                print(\"column {} has values string type\".format(col))\n            elif is_numeric_dtype(df[col]):\n                print(\"column {} is numerical\".format(col))","a1700188":"print_df_unique_vals(df)","cf49c5d3":"dec_reg_exp = r'^[+-]{0,1}((\\d*\\.)|\\d*)\\d+$'\nabnormal_total_charges = df[~df.TotalCharges.str.contains(dec_reg_exp)]\nabnormal_total_charges","665a771e":"df = df[df.TotalCharges.str.contains(dec_reg_exp)]\ndf['TotalCharges'] = df['TotalCharges'].astype(float)","b10c7efc":"def display_missing(df):    \n    for col in df.columns.tolist():          \n        print('{} column missing values: {}'.format(col, df[col].isnull().sum()))\n    print('\\n')\n    \n\ndisplay_missing(df)","4ce92963":"# Summary statistics:\ncont_features = [\"tenure\", \"MonthlyCharges\", \"TotalCharges\"]\ndf_num = df[cont_features]\ndf_num.describe()","fa6f5754":"Q1 =df_num.quantile(0.25)\nQ3 = df_num.quantile(0.75)\nIQR = Q3 - Q1\nIQR\n((df_num < (Q1 - 1.5 * IQR)) |(df_num > (Q3 + 1.5 * IQR))).any()","852a0d56":"churn = df['Churn'] == 'Yes'","ed5236f0":"def plot_dist_num_cols_target(df, cont_features, target, target_label):\n    fig, axs = plt.subplots(ncols=1, nrows=len(cont_features), figsize=(20, 20))\n    plt.subplots_adjust(right=1.5)\n    for i, feature in enumerate(cont_features):    \n        sns.distplot(df[~target][feature], label='Not {}'.format(target_label), hist=True, color='#e74c3c', ax=axs[i])\n        sns.distplot(df[target][feature], label='{}'.format(target_label), hist=True, color='#2ecc71', ax=axs[i])\n        \n        axs[i].set_xlabel('')\n        axs[i].set_xlabel('')\n        \n        for j in range(len(cont_features)):        \n            axs[j].tick_params(axis='x', labelsize=15)\n            axs[j].tick_params(axis='y', labelsize=15)\n\n        axs[i].legend(loc='upper right', prop={'size': 10})\n        axs[i].legend(loc='upper right', prop={'size': 10})\n        axs[i].set_title('Distribution of {} in {}'.format(target_label, feature), size=20, y=1.05)\n\n    plt.tight_layout(pad=5)\n    plt.savefig('numerical_attributes.png')\n    plt.show()","19a734cc":"plot_dist_num_cols_target(df, cont_features, churn, 'Churned')","c7cd7644":"def plot_dist_num_cols_target(df, cont_features):\n    fig, axs = plt.subplots(ncols=1, nrows=len(cont_features), figsize=(20, 20))\n    plt.subplots_adjust(right=1.5)\n    for i, feature in enumerate(cont_features):    \n        sns.distplot(df[feature], label='{}'.format(feature), hist=False, color='#e74c3c', ax=axs[i])\n        \n        axs[i].set_xlabel('')\n        axs[i].set_xlabel('')\n        \n        for j in range(len(cont_features)):        \n            axs[j].tick_params(axis='x', labelsize=15)\n            axs[j].tick_params(axis='y', labelsize=15)\n\n        axs[i].legend(loc='upper right', prop={'size': 10})\n        axs[i].legend(loc='upper right', prop={'size': 10})\n        axs[i].set_title('Distribution of {} feature'.format(feature), size=20, y=1.05)\n\n    plt.tight_layout(pad=5)\n    plt.savefig('distr_plots_numerical_attributes.png')\n    plt.show()","379c8552":"plot_dist_num_cols_target(df, cont_features)","2f297b30":"cat_features = ['gender', 'SeniorCitizen', 'Partner', 'Dependents', 'PhoneService', 'MultipleLines', 'InternetService', \n                'OnlineSecurity', 'OnlineBackup', 'DeviceProtection', 'TechSupport', 'StreamingTV', 'StreamingMovies',\n               'Contract', 'PaperlessBilling', 'PaymentMethod']","8575ec3e":"def plot_bar_cat_cols(df, cat_features, target_label, hue):\n    fig, axs = plt.subplots(ncols=2, nrows=8, figsize=(50, 50))\n    plt.subplots_adjust(right=1.5, top=1.25)\n    for i, feature in enumerate(cat_features, 1):    \n        plt.subplot(8, 2, i)\n        sns.countplot(x=feature, hue=hue, data=df)\n        plt.xlabel('{}'.format(feature), size=30, labelpad=15)\n        plt.ylabel('Customer Count', size=30, labelpad=15)    \n        plt.tick_params(axis='x', labelsize=30)\n        plt.tick_params(axis='y', labelsize=30)\n\n        plt.legend(['Not {}'.format(target_label), '{}'.format(target_label)], loc='upper right', prop={'size': 25})\n        plt.title('Count of {} in {} Feature'.format(target_label, feature), size=40, y=1.05)\n\n    plt.tight_layout(h_pad=5)\n    plt.savefig('cat_attributes_bar.png')\n    plt.show()","49049686":"plot_bar_cat_cols(df, cat_features, 'Churned', 'Churn')","3f78a6db":"sns.countplot(x=\"Churn\", data=df)","1b54a7ea":"df['MonthlyCharges'] = pd.qcut(df['MonthlyCharges'], 10)\n\nfig, axs = plt.subplots(figsize=(22, 9))\nsns.countplot(x='MonthlyCharges', hue='Churn', data=df)\n\nplt.xlabel('MonthlyCharges', size=15, labelpad=20)\nplt.ylabel('Customer count', size=15, labelpad=20)\nplt.tick_params(axis='x', labelsize=15)\nplt.tick_params(axis='y', labelsize=15)\n\nplt.legend(['Not churned', 'churned'], loc='upper right', prop={'size': 15})\nplt.title('Churn Counts in {} Feature'.format('MonthlyCharges'), size=15, y=1.05)\nplt.savefig('churned_v_not_churned.png')\nplt.show()","7619c07b":"non_numeric_features = ['gender', 'Partner', 'Dependents', 'PhoneService', 'MultipleLines', 'InternetService', 'OnlineSecurity',\n                       'OnlineBackup', 'DeviceProtection', 'TechSupport', 'StreamingTV', 'StreamingMovies', 'Contract', \n                       'PaperlessBilling', 'PaymentMethod', 'MonthlyCharges', 'Churn']\n\nfor feature in non_numeric_features:        \n    df[feature] = LabelEncoder().fit_transform(df[feature])","731fb1d8":"df.head()","3a03a1ed":"print(df.info())","9db549a5":"cat_features = ['MultipleLines', 'InternetService', 'OnlineSecurity',\n                'OnlineBackup', 'DeviceProtection', 'TechSupport', 'StreamingTV', 'StreamingMovies', 'Contract', \n                'PaymentMethod']\nencoded_features = []\n\nfor feature in cat_features:\n    encoded_feat = OneHotEncoder().fit_transform(df[feature].values.reshape(-1, 1)).toarray()\n    n = df[feature].nunique()\n    cols = ['{}_{}'.format(feature, n) for n in range(1, n + 1)]\n    encoded_df = pd.DataFrame(encoded_feat, columns=cols)\n    encoded_df.index = df.index\n    encoded_features.append(encoded_df)","18d1093e":"len(encoded_features)","7070dcc9":"df = pd.concat([df, *encoded_features], axis=1)","2301b290":"df.columns","350e4944":"df2 = df.copy()","71ec5c1f":"drop_cols = ['customerID', 'MultipleLines', 'InternetService', 'OnlineSecurity',\n            'OnlineBackup', 'DeviceProtection', 'TechSupport', \n            'StreamingTV', 'StreamingMovies', 'Contract', \n            'PaymentMethod']\n\ndf.drop(columns=drop_cols, inplace=True)","2c6b7ab2":"X = df.drop(columns=['Churn']).values\ny = df[\"Churn\"].values\n\nseed = 42\nx_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.2,  stratify=y, random_state=seed)\nprint('X_train shape: {}'.format(x_train.shape))\nprint('X_test shape: {}'.format(x_test.shape))","2c432707":"skf = StratifiedKFold(n_splits=5)\nfprs, tprs, scores, val_auc_scores = [], [], [], []\nfor train_index, valid_index in skf.split(x_train, y_train):\n    x_pseudo_train, x_pseudo_valid = x_train[train_index], x_train[valid_index]\n    y_pseudo_train, y_pseudo_valid = y_train[train_index], y_train[valid_index]\n    ss = StandardScaler()\n    x_pseudo_train_scaled = ss.fit_transform(x_pseudo_train)\n    x_pseudo_valid_scaled = ss.transform(x_pseudo_valid)\n    lr = LogisticRegression()  # Using default parameters.\n    lr.fit(x_pseudo_train_scaled, y_pseudo_train)\n    y_pred_train_probs = lr.predict_proba(x_pseudo_train_scaled)[:, 1]\n    y_pred_valid_probs = lr.predict_proba(x_pseudo_valid_scaled)[:, 1]\n    trn_fpr, trn_tpr, trn_thresholds = roc_curve(y_pseudo_train, \n                                                 y_pred_train_probs)\n    trn_auc_score = auc(trn_fpr, trn_tpr)\n    val_fpr, val_tpr, val_thresholds = roc_curve(y_pseudo_valid, \n                                                 y_pred_valid_probs)\n    val_auc_score = auc(val_fpr, val_tpr)\n    val_auc_scores.append(val_auc_score)\n    scores.append((trn_auc_score, val_auc_score))\n    fprs.append(val_fpr)\n    tprs.append(val_tpr)","b0e01f27":"average_val_auc = np.mean(val_auc_scores)\nprint(\"Average Validation AUC score: {}\".format(average_val_auc))","b73bcc80":"def plot_roc_curve(fprs, tprs):\n    \n    tprs_interp = []\n    aucs = []\n    mean_fpr = np.linspace(0, 1, 100)\n    f, ax = plt.subplots(figsize=(15, 15))\n    \n    # Plotting ROC for each fold and computing AUC scores\n    for i, (fpr, tpr) in enumerate(zip(fprs, tprs), 1):\n        tprs_interp.append(np.interp(mean_fpr, fpr, tpr))\n        tprs_interp[-1][0] = 0.0\n        roc_auc = auc(fpr, tpr)\n        aucs.append(roc_auc)\n        ax.plot(fpr, tpr, lw=1, alpha=0.3, label='ROC Fold {} (AUC = {:.3f})'.format(i, roc_auc))\n        \n    # Plotting ROC for random guessing\n    plt.plot([0, 1], [0, 1], linestyle='--', lw=2, color='r', alpha=0.8, label='Random Guessing')\n    \n    mean_tpr = np.mean(tprs_interp, axis=0)\n    mean_tpr[-1] = 1.0\n    mean_auc = auc(mean_fpr, mean_tpr)\n    std_auc = np.std(aucs)\n    \n    # Plotting the mean ROC\n    ax.plot(mean_fpr, mean_tpr, color='b', label='Mean ROC (AUC = {:.3f} $\\pm$ {:.3f})'.format(mean_auc, std_auc), lw=2, alpha=0.8)\n    \n    # Plotting the standard deviation around the mean ROC Curve\n    std_tpr = np.std(tprs_interp, axis=0)\n    tprs_upper = np.minimum(mean_tpr + std_tpr, 1)\n    tprs_lower = np.maximum(mean_tpr - std_tpr, 0)\n    ax.fill_between(mean_fpr, tprs_lower, tprs_upper, color='grey', alpha=.2, label='$\\pm$ 1 std. dev.')\n    \n    ax.set_xlabel('False Positive Rate', size=15, labelpad=20)\n    ax.set_ylabel('True Positive Rate', size=15, labelpad=20)\n    ax.tick_params(axis='x', labelsize=15)\n    ax.tick_params(axis='y', labelsize=15)\n    ax.set_xlim([-0.05, 1.05])\n    ax.set_ylim([-0.05, 1.05])\n\n    ax.set_title('ROC Curves of Folds', size=20, y=1.02)\n    ax.legend(loc='lower right', prop={'size': 13})\n    plt.savefig('roc_curve.png')\n    plt.show()\n\nplot_roc_curve(fprs, tprs)","f7456519":"ss = StandardScaler()\nx_train_scaled = ss.fit_transform(x_train)\nx_test_scaled = ss.transform(x_test)\n\n# Applying logistic regression classifier\nlr = LogisticRegression()  # Using default parameters.\nlr.fit(x_train_scaled, y_train)  # training the model with X_train, y_train\n\n# Generate Confusion Matrix\ny_pred = lr.predict(x_test_scaled)                # Make predictions on test set\ny_pred = pd.Series(y_pred)\ny_test = pd.Series(y_test)\npd.crosstab(y_pred, y_test, rownames=['Predicted'], colnames=['True'], margins=True)","9baa91e0":"def print_summary_stats(y_pred, y_test):\n    TP = sum((y_pred == y_test) & (y_pred == 1))            # No. of True Positives\n    FN = sum((y_pred != y_test) & (y_pred == 0))            # No. of False Negatives\n    P = TP + FN                                             # Total No. of Positives\n    TN = sum((y_pred == y_test) & (y_pred == 0))            # No. of True Negatives\n    FP = sum((y_pred != y_test) & (y_pred == 1))\n    N = TN + FP\n\n    print(\"Sensitivity: {}\".format(TP \/ P))                 # True Positive \/ Positive (Sensitivity)\n    print(\"Specificity: {}\".format(TN \/ N))                 # TN \/ N (Specificity)\n    print(\"Precision: {}\".format(TP \/ (TP + FP)))           # (TP\/ (TP+FP)) Precision\n    print(\"True Negative Rate: {}\".format(TN \/ (TN + FN)))  # (TN \/ (TN+FN))\n    print(\"Overall Accuracy: {}\".format(sum(y_pred == y_test)\/len(y_test)))","7e4251be":"print_summary_stats(y_pred, y_test)","5dc923ce":"MODEL_DIR = '\/kaggle\/working\/'\njoblib.dump(ss, MODEL_DIR + 'scaler.pkl')\njoblib.dump(lr, MODEL_DIR + 'lr_model.pkl')","3ba58335":"feats = df.drop(columns=['Churn'])\nnum_features = len(feats.columns)\n\ncoef_info = []\nfor i in range(num_features):\n    coef = lr.coef_[0][i]\n    coef_info.append((feats.columns[i], coef))","fabbb4b2":"sorted(coef_info, key=lambda x: abs(x[1]), reverse=True)[:10]","f77e3b39":"param_grid = { \n    'n_estimators': [200, 500, 1000],\n    'max_features': ['auto', 'sqrt', 'log2'],\n    'max_depth' : [4,5,6,7,8],\n    'criterion' :['gini', 'entropy']\n}","37aa3808":"rfc = RandomForestClassifier(random_state=seed)\ncv_rfc = RandomizedSearchCV(estimator=rfc, param_distributions=param_grid, cv= 5, n_jobs=-1)\ncv_rfc.fit(x_train_scaled, y_train)","71de2e99":"cv_rfc.best_params_","57227e17":"skf = StratifiedKFold(n_splits=5)\nfprs, tprs, scores, val_auc_scores = [], [], [], []\nfor train_index, valid_index in skf.split(x_train, y_train):\n    x_pseudo_train, x_pseudo_valid = x_train[train_index], x_train[valid_index]\n    y_pseudo_train, y_pseudo_valid = y_train[train_index], y_train[valid_index]\n    ss = StandardScaler()\n    x_pseudo_train_scaled = ss.fit_transform(x_pseudo_train)\n    x_pseudo_valid_scaled = ss.transform(x_pseudo_valid)\n    rf = RandomForestClassifier(**cv_rfc.best_params_)  # Using default parameters.\n    rf.fit(x_pseudo_train_scaled, y_pseudo_train)\n    y_pred_train_probs = rf.predict_proba(x_pseudo_train_scaled)[:, 1]\n    y_pred_valid_probs = rf.predict_proba(x_pseudo_valid_scaled)[:, 1]\n    trn_fpr, trn_tpr, trn_thresholds = roc_curve(y_pseudo_train, \n                                                 y_pred_train_probs)\n    trn_auc_score = auc(trn_fpr, trn_tpr)\n    val_fpr, val_tpr, val_thresholds = roc_curve(y_pseudo_valid, \n                                                 y_pred_valid_probs)\n    val_auc_score = auc(val_fpr, val_tpr)\n    val_auc_scores.append(val_auc_score)\n    scores.append((trn_auc_score, val_auc_score))\n    fprs.append(val_fpr)\n    tprs.append(val_tpr)","a31696c1":"average_val_auc = np.mean(val_auc_scores)\nprint(\"Average Validation AUC score: {}\".format(average_val_auc))","a6ebb582":"plot_roc_curve(fprs, tprs)","92e93b46":"# Applying random forest classifier\nrf = RandomForestClassifier(**cv_rfc.best_params_)  # Using CV grid search\nrf.fit(x_train_scaled, y_train)  # training the model with X_train, y_train\n\n# Generate Confusion Matrix\ny_pred = rf.predict(x_test_scaled)                # Make predictions on test set\ny_pred = pd.Series(y_pred)\ny_test = pd.Series(y_test)\npd.crosstab(y_pred, y_test, rownames=['Predicted'], colnames=['True'], margins=True)","5e7b5817":"print_summary_stats(y_pred, y_test)","4704e538":"joblib.dump(rf, MODEL_DIR + 'rf_model.pkl')","8844604c":"from sklearn.ensemble import VotingClassifier\nclf1 = RandomForestClassifier(**cv_rfc.best_params_)\nclf2 = LogisticRegression()\neclf1 = VotingClassifier(estimators=[('rf', clf1), ('lr', clf2)], voting='soft')\neclf1.fit(x_train_scaled, y_train)\npredictions = eclf1.predict(x_test_scaled)","5e5b3caf":"print_summary_stats(predictions, y_test)","6b8195ab":"* Tenure looks to be separable according to churned and not churned customers. The higher the tenure, the less likely it is that customers will be churned.\n\n* Similarly, for TotalCharges, the higher the TotalCharges, the less likely it is that customers will be churned.\n\n* For MonthlyCharges, it is uneven. There are groups in the middle that have churned customers outnumbering customers that are not churned. There are also groups in the middle that have customers that are not churned outnumering customers that are churned. To account for these groups, let's bin MonthlyCharges\n\n<a href=\"#top\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\" title=\"go to Colors\">Go to TOC<\/a>","8f402365":"<a id=\"logreg\"><\/a>\n## 3.2 Logistic Regression Baseline\n\n* We run stratifiedKFolds cross validation to check cross-validation results\n\n* The logistic regression model has a difficult time predicting churn. \n* The sensitivity is about 57%, meaning it is only about to predict 57% of churned customers correct out of all of the possible churned customers in our test dataset.\n\n\n<a href=\"#top\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\" title=\"go to Colors\">Go to TOC<\/a>","13f4a2ce":"1 way we can check for outliers is to use the interquartile range. The IQR is a measure of statistical dispersion, being equal to the difference between 75th and 25th percentiles. From the results below, it seems that there are no outliers. ","acf7dea1":"<a id=\"feateng\"><\/a>\n## 2. Features Engineering & Transformation\n\n<a href=\"#top\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\" title=\"go to Colors\">Go to TOC<\/a>","ff3b1bbf":"Print summary statistics:","db7c7c8c":"<a id=concl><\/a>\n## 4. Conclusion","b01682ae":"<a id=\"ena\"><\/a>\n## 1.4 Exploring Numerical Attributes\n\nThe visualizations below show the distribution plot of the numerical attributes, split by Churn and non-Churn\n\n* For tenure: The longer the tenure, the more non-Churn customers outweigh Churn customers. The converse holds true, the shorter the tenure, the more Churn customers outweigh non-Churn customers. This fits the intuition as the longer the customer stick wit the Telco company, the less likely he\/she is to switch to another Telco \n\n\n* For MonthlyCharges: The less the charge, the more non-Churn customers outweigh Churn customers. The converse holds true, the higher the charge, the more Churn customers outweigh non-Churn customers. This fits the intuition that the more the customer is charged per month, the more likely it is that the customer will switch to another Telco.\n\n\n* For TotalCharges: The more the charge, the more non-Churn customers outweigh Churn customers. The converse holds true, the lower the charge, the more Churn customers outweigh non-Churn customers. This could mean that the total lifetime charge for that customer by the Telco is lower for customers that switch as compared to those customers that don't switch\n\n<a href=\"#top\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\" title=\"go to Colors\">Go to TOC<\/a>","c9cd7ae5":"In short, as a result of this exercise, we have accomplished 2 things\n\n1. Identify customers with most important attributes that are likely to churn. (See [Logistic Regression Feature Importance](#lrfi)). The telco can target customers with such characteristics with strategic pricing or promotional packages. Such segmentation will help optimize the telco's efforts to retain customers\n\n2. Train a model to predict churn based on customer characteristics. This is important in targeting new customers as well. If the telco knows that the customer will likely churn based on these characteristics, it would not be wise to reach out to these customers as they may renege or switch telco soon.\n\n<a href=\"#top\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\" title=\"go to Colors\">Go to TOC<\/a>","0b9ebb9d":"<a id=lrfi><\/a>\n## 3.2.1 Logistic Regression Feature Importance\n\n* tenure is the most important explanatory feature, with people having longer tenure being less likely to be churned (negative coef)\n\n* TotalCharges is second most important, the higher the customer is charged to date, the more likely the customer will churn\n\n* People with 2 year contracts are also less likely to be churned, which makes intuitive business sense, given they may have to pay a higher penalty if they renage\n\n* People with month-to-month contract are more likely to be churned\n\n* People with DSL are less likely to be churned while people with Fibre optic are more likely to be churned. This is surprising and could be perhaps due to fibre optic being more popular nowadays and hence many companies have promotions to switch to a newer fibre optic service.\n\n* Monthly charges is surprising as people with higher Monthly charge are less likely to switch according to the negative coefficient. These could mean that people are willing to pay for more premium services monthly as long as those services are worthwhile\n\n* PaperlessBilling increases convenience, which fits the intuition that people with paperless bills ar e less likely to churn\n\n* People with no online security are more likely to churn which would make sense as they might go for services with online security, changing their current plan\/contract. The telco can target these people by proving promotions to these people to upgrade their security\n\n* People with automatic bank transfers as payment, are more likely to switch\n\n<a href=\"#top\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\" title=\"go to Colors\">Go to TOC<\/a>","9185202e":"<a id=\"intro\"><\/a>\n# 0. Introduction\n\n<a href=\"#top\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\" title=\"go to Colors\">Go to TOC<\/a>","fc7b9a85":"<a id=\"ohe\"><\/a>\n## 2.2 One-Hot Encoding Categorical Features\n\n* The categorical features are one-hot encoded (except for binary variables). \n\n* MonthlyCharges is not one-hot encoded as it is an ordinal variable\n\n<a href=\"#top\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\" title=\"go to Colors\">Go to TOC<\/a>","f84d0d3e":"Save scaler and logistic baseline model:","a8c8b3c2":"<a id=\"loading-data\"><\/a>\n\n## 0.2 Loading the dataset\n\nThere are 7,043 observations (rows) with 21 attributes (columns) in the dataset\n\n<a href=\"#top\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\" title=\"go to Colors\">Go to TOC<\/a>","7cffcc37":"Train-test split\n\nData is split into 80% training data and 20% test data\n\n<a href=\"#top\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\" title=\"go to Colors\">Go to TOC<\/a>","9d0345e2":"Let's take a look at count distribution of the dependent variable","f2967345":"![image.png](attachment:image.png)","14b65be1":"Next steps?\n\nWe can:\n\n1. Try Oversampling Techniques to improve modelling accuracy, given that the target variable is imbalanced (ie. SMOTE).\n\n2. Improve Predictive Accuracy by using a more advanced model (i.e. XGBoost)\n3. More In-depth customer segmentation, design of promotional package to retain existing customers and attract new customers","5f9669f0":"Average Validation score is ~0.85, not bad","5f382fde":"<a id=\"eca\"><\/a>\n## 1.5 Exploring Categorical Attributes\n\n* For all classes of categorical variables, for all the classes, it looks like customers that are not churned outnumber customers that are churned.\n\n* For gender, males and females have almost equal number of churned and non-churned customers\n\n* For other categorical variables, there is at least 1 class with number of churned customers outnumbering the number of non-churned customers\n\n* On first glance, a simple classification model likely is not good enough as categorical variables are not highly separable to churned and non-churned customers.\n\n* Target variable has uneven distribution, with number of customers not churned, greatly outnumbering number of churned customers in total by more than double\n\n<a href=\"#top\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\" title=\"go to Colors\">Go to TOC<\/a>","47f97f81":"<a id=\"libraries\"><\/a>\n## 0.1 Libraries\n\n<a href=\"#top\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\" title=\"go to Colors\">Go to TOC<\/a>","66261387":"<a id=\"top\"><\/a>\n\n<div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n<h3 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\"  role=\"tab\" aria-controls=\"home\">Table of content<\/h3>\n    \n[0. Introduction](#intro)\n* [0.1 Libraries](#libraries)\n* [0.2 Loading the Dataset](#loading-data)\n    \n[1. Exploratory Data Analysis](#data-exploration)\n* [1.1 Overview](#overview)\n* [1.2 Dealing with Missing Values](#missing-vals)\n* [1.3 Dealing with Outliers](#outliers)\n* [1.4 Exploring Numerical Attributes](#ena)\n* [1.5 Exploring Categorical Attributes](#eca)\n    \n[2. Features Engineering & Transformation](#feateng)\n* [2.1 Binning Continuous Variables](#bin)\n* [2.2 Label Encoding Non-Numeric Features](#le)\n* [2.3 One-Hot Encoding Categorical Features](#ohe)\n* [2.4 Dropping Unrelated Columns](#drop)\n    \n[3. Model Training](#model)\n* [3.1 Train-Test Split](#train-test)\n* [3.2 Logistic Regression Baseline](#logreg)\n  * [3.2.1 Logistic Regression Feature Importance](#lrfi)\n* [3.3 Random Forest](#randfor)\n* [3.4 Model Ensembling](#ens)\n    \n[4. Conclusion](#concl)\n","96099855":"**Fare**\n\n* It can be seen that, the higher the MonthlyCharges, the higher the proportion of customers that are churned compared to customers that are not churned. \n\n* The group at the highest extreme (102.645, 118.75] is unusual, where churned customers are comparable in proportion to groups in the middle. \n\n* This grup is captured when we bin as compared to not binning","d0a0b8f9":"<a id=\"bin\"><\/a>\n## 2.1 Binning Continuous Variables","049c83bd":"The visualizations below show the distribution plot of the numerical attributes","ca73062c":"<a id=\"randfor\"><\/a>\n## 3.3 Random Forest\n\n* The AUC score and ROC curve is pretty similar to the LogisticRegression model results earlier\n* In terms of true positives, Random Forest fared worse than LogisticRegression with a sensitivity score of ~0.505\n* Moreover, LogisticRegression has a higher accuracy than RandomForest (~80% as compared to 79.4%)\n\n<a href=\"#top\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\" title=\"go to Colors\">Go to TOC<\/a>","1cdfa382":"Plotting the Receiver Operator Curve will illustrate the diagnostic ability of the model. The dotted red straight line illustrates performance of a 50-50 random guess. The curve do not lie below this line, indicating the model's predictive power is better than random (in fact, significantly better)","55f85b12":"There is something off about TotalCharges. On inspection, total charges should be numerical but yet that column is showing as a string type. Let's find out why that is the case using regex","b74e927d":"<a id=\"model\"><\/a>\n## 3. Model Training\n\n2 models are trained to predict churn given customer characteristics\n\n1. Logistic Regression (Baseline)\n2. Random Forest\n\n<a href=\"#top\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\" title=\"go to Colors\">Go to TOC<\/a>","319bbf3e":"<a id=\"ens\"><\/a>\n## 3.4 Model Ensembling\n\n* Overall accuracy did not improve. Results obtained from LogisticRegression were still the best with accuracy above 80%. Ensembling only achieved an accuracy ~79.8%\n\n<a href=\"#top\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\" title=\"go to Colors\">Go to TOC<\/a>","663d1c35":"<a id=\"drop\"><\/a>\n## 2.3 Dropping Unrelated Columns\n\nDrop columns that are unrelated (customerID) and columns where we generate one-hot encoded variables earlier\n\n<a href=\"#top\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\" title=\"go to Colors\">Go to TOC<\/a>","5e8ce136":"<a id=\"outliers\"><\/a>\n## 1.3 Dealing with Outliers\nOutliers need to be checked for too as they may skew a model's prediction\n\n\n<a href=\"#top\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\" title=\"go to Colors\">Go to TOC<\/a>","93f77a25":"* gender, Partner, Dependents, PhoneService, MultipleLines, InternetService, OnlineSecurity, OnlineBackup, DeviceProtection, TechSupport, StreamingTV, StreamingMovies, Contract, PaperlessBilling, PaymentMethod, Churn are object type and MonthlyCharges is categorical. \n\n* They are converted to numerical type with LabelEncoder.\n\n<a href=\"#top\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\" title=\"go to Colors\">Go to TOC<\/a>","59d35e58":"<a id=\"le\"><\/a>\n## 2.2 Label Encoding Non-Numeric Features","3fdc9555":"The TotalCharges for these observations need to be imputed or dropped\n\nThe percentage of blank values is about 0.15%, it is an insignificant portion of the entire dataset\n\nLet's drop these observations","a6c2fa2b":"<a id=\"missing-vals\"><\/a>\n## 1.2 Dealing with Missing Values\n\nChecking if there are columns with missing values is paramount. It is fortunate that the data has no missing value and requires no imputation\n\n<a href=\"#top\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\" title=\"go to Colors\">Go to TOC<\/a>","06288990":"<a id=\"overview\"><\/a>\n## 1.1 Overview\n\nThere are 2 types of data, categorical and numerical\n\n* ```customerID``` represents each customer and do not have an effect on churn\n* ```gender``` is binary (Male\/Female)\n* ```SeniorCitizen``` is binary\n    * 1 - Senior Citizen\n    * 0 - Others\n* ```Partner``` is binary\n    * Yes - customer has a partner\n    * No - customer do not have a partner\n* ```Dependents``` is binary\n    * Yes - customer has dependents\n    * No - customer do not have dependents\n* ```tenure``` is numerical\n    * Represents how long the customer have been using the service\n* ```PhoneService``` is binary\n    * Yes - customer has phone service with company\n    * no - customer do not have phone service with company\n* ```MultipleLines``` is categorical\n    * Yes - customer has multiple line subscriptions\n    * No - customer has only 1 line subscription\n    * No phone service - customer do not have phone service with company\n* ```InternetService``` is categorical\n    * DSL\n    * Fiber optic\n    * No - customer do not have internet service with company\n* ```OnlineSecurity``` is categorical\n    * Yes - customer has online security with company\n    * No - customer do not have online security with company\n    * No internet service - customer do not have internet service with company\n* ```OnlineBackup```, ```DeviceProtection```, ```TechSupport```, ```StreamingTV```, ```StreamingMovies``` are similar to ```OnlineSecurity``` (all categorical with same possible values)\n\n* ```Contract``` is categorical\n    * Customers contract arrangement, can be either of these: ['Month-to-month' 'One year' 'Two year']\n* ```PaperlessBilling``` is binary\n    * How customers are billed (either electronic ('Yes') or through a paper bill ('No'))\n* ```PaymentMethod``` is categorical (self-explanatory), can be either of these: ['Electronic check' 'Mailed check' 'Bank transfer (automatic)', 'Credit card (automatic)']\n* Finally, we have ```MonthlyCharges```, ```TotalCharges``` and ```Churn```\n\n<a href=\"#top\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\" title=\"go to Colors\">Go to TOC<\/a>","3b90d24e":"tldr: The goal is to minimize number of customers switching to competitors or discontinuing their contracts.\n\nIn order to do so, a model can be trained on an existing dataset, which can internalize and weigh important attributes of customers\nwho will likely be loyal.\n\nThe model would be able to predict customers who will likely be churn and attention should be paid to target customers with\nthese attributes, through pricing\/ promotions\n\nThis kernel has 3 main parts: Exploratory Data Analysis, Feature Engineering and Model Training","633139ba":"<a id=\"train-test\"><\/a>\n## 3.1 Train-Test Split","81d6fcd8":"<a id=\"data-exploration\"><\/a>\n## 1. Exploratory Data Analysis\n\n<a href=\"#top\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\" title=\"go to Colors\">Go to TOC<\/a>"}}