{"cell_type":{"166c9eb3":"code","bedf7f2a":"code","141810aa":"code","4be245aa":"code","380455a2":"code","bd13a12f":"code","d5d03ee7":"code","e477335b":"code","015d2372":"code","5e66a4a9":"code","52084ade":"code","b102c746":"code","dae751ee":"code","08bf0928":"code","e5a6c391":"code","bcc5083a":"code","98cce82e":"code","9af5b8b8":"code","ad06978a":"code","ecc97485":"code","774601ab":"code","48ace6b3":"code","54a186e7":"code","5c25b2b9":"code","c4ab673a":"code","c581f2eb":"code","cbfd8733":"code","16156be7":"code","bfa495cb":"code","8f2bb4f8":"code","3e4a6827":"code","a0be8870":"code","fdd6934b":"code","1a2f9090":"code","24f68e38":"code","2bbca165":"code","1d2cc1be":"code","7cfa3b0b":"code","c23bf2da":"code","4f2ad36b":"code","69f31926":"code","49fe0934":"code","2e68359f":"code","e3066b7c":"code","53700211":"code","0abf79a2":"code","937674c5":"code","e1434ea1":"code","5cafdb01":"code","4ca9b94f":"code","36844df5":"code","7c4efbaf":"code","3728b2fd":"code","755a6fb5":"code","bf780e6c":"code","277f73bd":"code","ade8456d":"code","afd9898e":"code","864a481f":"code","f8814308":"code","ac190652":"code","05c3b529":"code","2254ba3f":"code","c037583e":"code","741f1305":"code","a07f6ae1":"code","92f88c8a":"code","762f566b":"code","dafd6382":"markdown","29eeb831":"markdown","27e2b768":"markdown","1a45c441":"markdown","3c1ca7b9":"markdown","e382c5c6":"markdown","4de381b6":"markdown","f576465c":"markdown","b072fc3e":"markdown","629fe983":"markdown","05b8949d":"markdown","b180f540":"markdown","54ee6a63":"markdown","e30ea067":"markdown","b28d3091":"markdown"},"source":{"166c9eb3":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","bedf7f2a":"df = pd.read_csv(\"\/kaggle\/input\/water-potability\/water_potability.csv\")","141810aa":"%matplotlib inline\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nmpl.rc('axes', labelsize=14)\nmpl.rc('xtick', labelsize=12)\nmpl.rc('ytick', labelsize=12)","4be245aa":"df.info()","380455a2":"df.shape","bd13a12f":"df.describe(include = \"all\")","d5d03ee7":"def describe_dataframe(df=pd.DataFrame()):\n    print(\"\\n\\n\")\n    print(\"*\"*30)\n    print(\"Data Description\")\n    print(\"*\"*30)\n    \n    print(\"Number of rows::\",df.shape[0])\n    print(\"Number of columns::\",df.shape[1])\n    print(\"\\n\")\n    \n    print(\"Column Names::\",df.columns.values.tolist())\n    print(\"\\n\")\n    \n    print(\"Columns with Missing Values::\",df.columns[df.isnull().any()].tolist())\n    print(\"\\n\")\n    ","e477335b":"describe_dataframe(df)","015d2372":"df.isnull().sum()\/len(df)","5e66a4a9":"df.hist(bins=50, figsize=(20,15))\nplt.show()","52084ade":"df['Potability'].value_counts(dropna=False)","b102c746":"!pip install ptitprince","dae751ee":"from ptitprince import PtitPrince as pt\nimport seaborn as sns","08bf0928":"dx = df.columns\ndy = df.Potability\n","e5a6c391":"ort=\"h\"; pal = sns.color_palette(n_colors=1)\n\nfor x in dx:\n    pal = \"Set2\"\n    f, ax = plt.subplots(figsize=(12, 5))\n\n    ax=pt.half_violinplot( x = x, y = dy, data = df, palette = pal, bw = .2, cut = 0.,\n                      scale = \"area\", width = .6, inner = None, orient = ort)\n    ax=sns.stripplot( x = x, y = dy, data = df, palette = pal, edgecolor = \"white\",\n                 size = 3, jitter = 1, zorder = 0, orient = ort)\n    ax=sns.boxplot( x = x, y = dy, data = df, color = \"black\", width = .15, zorder = 10,\\\n            showcaps = True, boxprops = {'facecolor':'none', \"zorder\":10},\\\n            showfliers=True, whiskerprops = {'linewidth':2, \"zorder\":10},\\\n               saturation = 1, orient = ort)\n    plt.title(x)\n\n","bcc5083a":"skew = pd.Series(df.skew(),name=\"skew\")\nkurtosis = pd.Series(df.kurtosis(),name=\"kurtosis\")\npd.concat([skew,kurtosis],axis =1)","98cce82e":"from sklearn.model_selection import StratifiedShuffleSplit","9af5b8b8":"split = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\nfor train_index, test_index in split.split(df, df[\"Potability\"]):\n    strat_train_set = df.loc[train_index]\n    strat_test_set = df.loc[test_index]","ad06978a":"df['Potability'].value_counts()\/len(df)","ecc97485":"strat_test_set['Potability'].value_counts()\/len(strat_test_set)","774601ab":"df_copy =strat_train_set.copy()","48ace6b3":"#to_impute = ['ph','Sulfate','Trihalomethanes']\n#df_copy[to_impute]=df_copy[to_impute].fillna((df_copy[to_impute].median()))\n\n# Better to handle it in Sklearn pipeline ","54a186e7":"fig, ax = plt.subplots(figsize=(10,10)) \nsns.heatmap(df_copy.corr(),annot = True,fmt='.1g',cmap= 'coolwarm',linewidths=1, linecolor='black')","5c25b2b9":"import scipy.stats as stats","c4ab673a":"x = ['ph', 'Hardness', 'Solids', 'Chloramines', 'Sulfate', 'Conductivity',\n       'Organic_carbon', 'Trihalomethanes', 'Turbidity']","c581f2eb":"from sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\n\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.linear_model import SGDClassifier","cbfd8733":"df_copy.columns","16156be7":"train_x = df_copy.drop(\"Potability\",axis = 1)\ntrain_Y = df_copy[\"Potability\"].copy()","bfa495cb":"test_x = strat_test_set.drop(\"Potability\",axis = 1)\ntest_Y = strat_test_set[\"Potability\"].copy()","8f2bb4f8":"full_pipeline = Pipeline([\n                    ('imputer', SimpleImputer(strategy='median')),\n                    ('std_scaler',StandardScaler())\n])","3e4a6827":"train_final = full_pipeline.fit_transform(train_x)","a0be8870":"train_final[0]","fdd6934b":"test_final = full_pipeline.fit_transform(test_x)","1a2f9090":"x_imputed_train = pd.DataFrame(train_final, columns = train_x.columns)","24f68e38":"train_Y = train_Y.to_frame()","2bbca165":"x_imputed_train.describe().T","1d2cc1be":"train_Y.columns","7cfa3b0b":"df_train = x_imputed_train.merge(train_Y, left_index=True, right_index=True, how='inner')","c23bf2da":"for i in x:\n    corr = stats.pointbiserialr(x_imputed_train[i], train_Y['Potability'])\n    print(corr)","4f2ad36b":"!pip install ppscore","69f31926":"import ppscore as pps","49fe0934":"pps.matrix(df_train)","2e68359f":"def heatmap(df):\n    df = df[['x', 'y', 'ppscore']].pivot(columns='x', index='y', values='ppscore')\n    ax = sns.heatmap(df, vmin=0, vmax=1, cmap=\"Blues\", linewidths=0.5, annot=True)\n    ax.set_title(\"PPS matrix\")\n    ax.set_xlabel(\"feature\")\n    ax.set_ylabel(\"target\")\n    return ax","e3066b7c":"matrix = pps.matrix(df_train)","53700211":"heatmap(matrix)","0abf79a2":"# Models from Scikit-Learn\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import RandomForestClassifier\n\n# Model Evaluations\nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.model_selection import RandomizedSearchCV, GridSearchCV\nfrom sklearn.metrics import confusion_matrix, classification_report\nfrom sklearn.metrics import precision_score, recall_score, f1_score\nfrom sklearn.metrics import plot_roc_curve","937674c5":"X_train = x_imputed_train\nX_test  = test_final\ny_train = train_Y\ny_test  = test_Y","e1434ea1":"# Put models in a dictionary\nmodels = {\"Logistic Regression\": LogisticRegression(),\n          \"KNN\": KNeighborsClassifier(),\n          \"Random Forest\": RandomForestClassifier()}\n\n# Create a function to fit and score models\ndef fit_and_score(models, X_train, X_test, y_train, y_test):\n    \"\"\"\n    Fits and evaluates given machine learning models.\n    models : a dict of differetn Scikit-Learn machine learning models\n    X_train : training data (no labels)\n    X_test : testing data (no labels)\n    y_train : training labels\n    y_test : test labels\n    \"\"\"\n    # Set random seed\n    np.random.seed(42)\n    # Make a dictionary to keep model scores\n    model_scores = {}\n    # Loop through models\n    for name, model in models.items():\n        # Fit the model to the data\n        model.fit(X_train, y_train)\n        # Evaluate the model and append its score to model_scores\n        model_scores[name] = model.score(X_test, y_test)\n    return model_scores","5cafdb01":"model_scores = fit_and_score(models=models,\n                             X_train=X_train,\n                             X_test=X_test,\n                             y_train=y_train,\n                             y_test=y_test)\n\nmodel_scores","4ca9b94f":"model_compare = pd.DataFrame(model_scores, index=[\"accuracy\"])\nmodel_compare.T.plot.bar();","36844df5":"# Create a hyperparameter grid for LogisticRegression\nlog_reg_grid = {\"C\": np.logspace(-4, 4, 20),\n                \"solver\": [\"liblinear\"]}\n\n# Create a hyperparameter grid for RandomForestClassifier\nrf_grid = {\"n_estimators\": np.arange(10, 1000, 50),\n           \"max_depth\": [None, 3, 5, 10],\n           \"min_samples_split\": np.arange(2, 20, 2),\n           \"min_samples_leaf\": np.arange(1, 20, 2)}","7c4efbaf":"# Tune LogisticRegression\n\nnp.random.seed(42)\n\n# Setup random hyperparameter search for LogisticRegression\nrs_log_reg = RandomizedSearchCV(LogisticRegression(),\n                                param_distributions=log_reg_grid,\n                                cv=5,\n                                n_iter=20,\n                                verbose=True)\n\n# Fit random hyperparameter search model for LogisticRegression\nrs_log_reg.fit(X_train, y_train)","3728b2fd":"rs_log_reg.best_params_\n","755a6fb5":"rs_log_reg.score(X_test, y_test)\n","bf780e6c":"# Setup random seed\nnp.random.seed(42)\n\n# Setup random hyperparameter search for RandomForestClassifier\nrs_rf = RandomizedSearchCV(RandomForestClassifier(), \n                           param_distributions=rf_grid,\n                           cv=5,\n                           n_iter=20,\n                           verbose=True)\n\n# Fit random hyperparameter search model for RandomForestClassifier()\nrs_rf.fit(X_train, y_train)","277f73bd":"\n# Find the best hyperparameters\nrs_rf.best_params_","ade8456d":"\n# Evaluate the randomized search RandomForestClassifier model\nrs_rf.score(X_test, y_test)","afd9898e":"rs_rf.score(X_test, y_test)","864a481f":"# Make predictions with tuned model\ny_preds = rs_rf.predict(X_test)","f8814308":"\n# Plot ROC curve and calculate and calculate AUC metric\nplot_roc_curve(rs_rf, X_test, y_test)","ac190652":"print(confusion_matrix(y_test, y_preds))\n","05c3b529":"print(classification_report(y_test, y_preds))\n","2254ba3f":"importance = rs_rf.best_estimator_.feature_importances_\n# summarize feature importance\n","c037583e":"a = train_x.columns","741f1305":"b = zip(a,importance)","a07f6ae1":"mapped = set(b)\nprint(mapped)","92f88c8a":"f = pd.DataFrame(mapped,columns=['Feature','Importance'])\n","762f566b":"f.plot(x ='Feature', y='Importance', kind = 'bar')","dafd6382":"# Interpreting Skewness & Kurtosis\n\nIf the skewness is between -0.5 and 0.5, the data are fairly symmetrical\nIf the skewness is between -1 and \u2013 0.5 or between 0.5 and 1, the data are moderately skewed\nIf the skewness is less than -1 or greater than 1, the data are highly skewed\n\nA normal distribution has kurtosis exactly 3. Any distribution with kurtosis \u22483 (excess \u22480) is called mesokurtic.\nA distribution with kurtosis <3 is called platykurtic. Compared to a normal distribution, its tails are shorter and thinner, and often its central peak is lower and broader.\nA distribution with kurtosis >3 is called leptokurtic. Compared to a normal distribution, its tails are longer and fatter, and often its central peak is higher and sharper\n","29eeb831":"# Time to split the dataset to train & test","27e2b768":"# Find the missing values and impute them","1a45c441":"# Overall it seems like the predictors that are available are very weak to predict the Water Potability","3c1ca7b9":"Let's do it for RF as well","e382c5c6":"Now we've got a baseline model... and we know a model's first predictions aren't always what we should based our next steps off. What should we do?\n\nLet's look at the following:\n\nHypyterparameter tuning\nFeature importance\nConfusion matrix\nCross-validation\nPrecision\nRecall\nF1 score\nClassification report\nROC curve\nArea under the curve (AUC)","4de381b6":"# Predictive Power Score\nPredictive Power Score is another interesting way to analyse the relationship between variables. It is touted to be better than correlations","f576465c":"#Modelling, Hyperparameter Tuning and Feature importances","b072fc3e":"# A \u2018raincloud\u2019 plot, which combines boxplots, raw jittered data, and a split-half violin. It is much more inituitive to understand the distribution of the data","629fe983":"# Describing the dataset","05b8949d":"# Create a new dataset post transformation to do some EDA","b180f540":"# EDA Starts","54ee6a63":"# Point Biserial correlation is appropriate for Continuous vs Categorical value","e30ea067":"First, let's check the correlation matrix. The correlation levels between the IVs are very low","b28d3091":"# Imputation to be done only after split"}}