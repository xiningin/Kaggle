{"cell_type":{"f74a7f13":"code","a501255c":"code","f485c336":"code","c07585a3":"code","fa8d562a":"code","250d834e":"code","d4bce34c":"code","25c8a3aa":"code","b2cd50b0":"code","54ed9ccc":"code","e6ebdf5b":"code","7a5ea253":"code","043275b7":"code","45769c17":"code","e818a100":"code","1d11d6a0":"code","f9853b75":"code","13cfe579":"code","86388c38":"code","01051395":"code","88240641":"code","cce7c4ec":"code","0ef448dd":"code","6faa0c25":"code","e6cabcb2":"code","ffb543ea":"code","7b5f846a":"code","5875aaf7":"code","062208bb":"code","1830e0f5":"markdown","f9d8db78":"markdown","ec303257":"markdown","a655d5ef":"markdown","3708b0c7":"markdown","8721ad6f":"markdown","af97e313":"markdown","55c7b406":"markdown","316813fa":"markdown","a515c09c":"markdown","b6f5d6a0":"markdown","f9dc214e":"markdown","344e8402":"markdown","37ce426a":"markdown","b0a19e97":"markdown"},"source":{"f74a7f13":"!pip install -U vega_datasets notebook vega","a501255c":"import numpy as np\nimport pandas as pd\nimport os\n\nimport matplotlib.pyplot as plt\nfrom tqdm import tqdm_notebook\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.svm import NuSVR, SVR\nfrom sklearn.metrics import roc_auc_score\nimport datetime\nimport random\npd.options.display.precision = 15\n\nimport lightgbm as lgb\nimport xgboost as xgb\nimport time\nimport datetime\nfrom catboost import CatBoostRegressor\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import StratifiedKFold, KFold, RepeatedKFold, GroupKFold, GridSearchCV, train_test_split, TimeSeriesSplit\nfrom sklearn import linear_model\nfrom sklearn import tree\nimport gc\nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport eli5\nimport shap\nfrom IPython.display import HTML\nimport json\nimport altair as alt\n\nimport networkx as nx\nimport matplotlib.pyplot as plt\n%matplotlib inline\n# alt.renderers.enable('notebook')\n\n# %env JOBLIB_TEMP_FOLDER=\/tmp","f485c336":"train = pd.read_pickle(\"..\/input\/fraud-detection-forum-supcom-2019\/train.pkl\")\ntest = pd.read_pickle(\"..\/input\/fraud-detection-forum-supcom-2019\/test.pkl\")","c07585a3":"print(f'Train dataset has {train.shape[0]} rows and {train.shape[1]} columns.')\nprint(f'Test dataset has {test.shape[0]} rows and {test.shape[1]} columns.')","fa8d562a":"train.head()","250d834e":"print(f'There are {train.isnull().any().sum()} columns in train dataset with missing values.')\nprint(f'There are {test.isnull().any().sum()} columns in test dataset with missing values.')","d4bce34c":"one_value_cols = [col for col in train.columns if train[col].nunique() <= 1]\none_value_cols_test = [col for col in test.columns if test[col].nunique() <= 1]\nprint(f'There are {len(one_value_cols)} columns in train dataset with one unique value.')\nprint(f'There are {len(one_value_cols_test)} columns in test dataset with one unique value.')","25c8a3aa":"for i in range(1,10): \n    print(f\"The most counted values of id_0{i}\")\n    print(train[f'id_0{i}'].value_counts(dropna=False, normalize=True).head()*100)\n    print(\"\\n\\n\")\nfor i in range(10,38): \n    print(f\"The most counted values of id_{i}\")\n    print(train[f'id_{i}'].value_counts(dropna=False, normalize=True).head())\n    print(\"\\n\\n\")","b2cd50b0":"for i in range(1,10): \n    if train[f'id_0{i}'].dtypes!=\"O\":\n        plt.figure();\n        plt.hist(train[f'id_0{i}']);\n        plt.title(f'Distribution of id_0{i} variable');\nfor i in range(10,38): \n    if train[f'id_{i}'].dtypes!=\"O\":\n        plt.figure();\n        plt.hist(train[f'id_{i}']);\n        plt.title(f'Distribution of id_{i} variable');","54ed9ccc":"import os\nimport time\nimport datetime\nimport json\nimport gc\nfrom numba import jit\n\nimport numpy as np\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom tqdm import tqdm_notebook\n\nimport lightgbm as lgb\nimport xgboost as xgb\nfrom catboost import CatBoostRegressor, CatBoostClassifier\nfrom sklearn import metrics\n\nfrom itertools import product\n\nimport altair as alt\nfrom altair.vega import v5\nfrom IPython.display import HTML\n\n# using ideas from this kernel: https:\/\/www.kaggle.com\/notslush\/altair-visualization-2018-stackoverflow-survey\ndef prepare_altair():\n    \"\"\"\n    Helper function to prepare altair for working.\n    \"\"\"\n\n    vega_url = 'https:\/\/cdn.jsdelivr.net\/npm\/vega@' + v5.SCHEMA_VERSION\n    vega_lib_url = 'https:\/\/cdn.jsdelivr.net\/npm\/vega-lib'\n    vega_lite_url = 'https:\/\/cdn.jsdelivr.net\/npm\/vega-lite@' + alt.SCHEMA_VERSION\n    vega_embed_url = 'https:\/\/cdn.jsdelivr.net\/npm\/vega-embed@3'\n    noext = \"?noext\"\n    \n    paths = {\n        'vega': vega_url + noext,\n        'vega-lib': vega_lib_url + noext,\n        'vega-lite': vega_lite_url + noext,\n        'vega-embed': vega_embed_url + noext\n    }\n    \n    workaround = f\"\"\"    requirejs.config({{\n        baseUrl: 'https:\/\/cdn.jsdelivr.net\/npm\/',\n        paths: {paths}\n    }});\n    \"\"\"\n    \n    return workaround\n    \n\ndef add_autoincrement(render_func):\n    # Keep track of unique <div\/> IDs\n    cache = {}\n    def wrapped(chart, id=\"vega-chart\", autoincrement=True):\n        if autoincrement:\n            if id in cache:\n                counter = 1 + cache[id]\n                cache[id] = counter\n            else:\n                cache[id] = 0\n            actual_id = id if cache[id] == 0 else id + '-' + str(cache[id])\n        else:\n            if id not in cache:\n                cache[id] = 0\n            actual_id = id\n        return render_func(chart, id=actual_id)\n    # Cache will stay outside and \n    return wrapped\n\n@add_autoincrement\ndef render(chart, id=\"vega-chart\"):\n    \"\"\"\n    Helper function to plot altair visualizations.\n    \"\"\"\n    chart_str = \"\"\"\n    <div id=\"{id}\"><\/div><script>\n    require([\"vega-embed\"], function(vg_embed) {{\n        const spec = {chart};     \n        vg_embed(\"#{id}\", spec, {{defaultStyle: true}}).catch(console.warn);\n        console.log(\"anything?\");\n    }});\n    console.log(\"really...anything?\");\n    <\/script>\n    \"\"\"\n    return HTML(\n        chart_str.format(\n            id=id,\n            chart=json.dumps(chart) if isinstance(chart, dict) else chart.to_json(indent=None)\n        )\n    )","e6ebdf5b":"charts = {}\nfor i in ['id_30', 'id_31', 'id_33', 'DeviceType', 'DeviceInfo']:\n    feature_count = train[i].value_counts(dropna=False)[:40].reset_index().rename(columns={i: 'count', 'index': i})\n    chart = alt.Chart(feature_count).mark_bar().encode(\n                x=alt.X(f\"{i}:N\", axis=alt.Axis(title=i)),\n                y=alt.Y('count:Q', axis=alt.Axis(title='Count')),\n                tooltip=[i, 'count']\n            ).properties(title=f\"Counts of {i}\", width=800)\n    charts[i] = chart\n\nrender(charts['id_30'] & charts['id_31'] & charts['id_33'] & charts['DeviceType'] & charts['DeviceInfo'])","7a5ea253":"plt.hist(train['TransactionDT'], label='train');\nplt.hist(test['TransactionDT'], label='test');\nplt.legend();\nplt.title('Distribution of transactiond dates');","043275b7":"train.isFraud.hist()","45769c17":"def seed_everything(seed=0):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)","e818a100":"SEED = 42\nseed_everything(SEED)\nTARGET = 'isFraud'\nSTART_DATE = datetime.datetime.strptime('2017-11-30', '%Y-%m-%d')","1d11d6a0":"def frequency_encoding(train_df, test_df, columns, self_encoding=False):\n    for col in tqdm_notebook(columns):\n        temp_df = pd.concat([train_df[[col]], test_df[[col]]])\n        fq_encode = temp_df[col].value_counts(dropna=False).to_dict()\n        if self_encoding:\n            train_df[col] = train_df[col].map(fq_encode)\n            test_df[col]  = test_df[col].map(fq_encode)            \n        else:\n            train_df[col+'_fq_enc'] = train_df[col].map(fq_encode)\n            test_df[col+'_fq_enc']  = test_df[col].map(fq_encode)\n    return train_df, test_df","f9853b75":"def timeblock_frequency_encoding(train_df, test_df, periods, columns, \n                                 with_proportions=True, only_proportions=False):\n    for period in periods:\n        for col in tqdm_notebook(columns):\n            new_col = col +'_'+ period\n            train_df[new_col] = train_df[col].astype(str)+'_'+train_df[period].astype(str)\n            test_df[new_col]  = test_df[col].astype(str)+'_'+test_df[period].astype(str)\n\n            temp_df = pd.concat([train_df[[new_col]], test_df[[new_col]]])\n            fq_encode = temp_df[new_col].value_counts().to_dict()\n\n            train_df[new_col] = train_df[new_col].map(fq_encode)\n            test_df[new_col]  = test_df[new_col].map(fq_encode)\n            \n            if only_proportions:\n                train_df[new_col] = train_df[new_col]\/train_df[period+'_total']\n                test_df[new_col]  = test_df[new_col]\/test_df[period+'_total']\n            if with_proportions:\n                train_df[new_col+'_proportions'] = train_df[new_col]\/train_df[period+'_total']\n                test_df[new_col+'_proportions']  = test_df[new_col]\/test_df[period+'_total']\n\n    return train_df, test_df","13cfe579":"from pandas.tseries.holiday import USFederalHolidayCalendar as calendar\ndates_range = pd.date_range(start='2017-10-01', end='2019-01-01')\nus_holidays = calendar().holidays(start=dates_range.min(), end=dates_range.max())\n\n# Let's add temporary \"time variables\" for aggregations\n# and add normal \"time variables\"\nfor df in [train, test]:\n    \n    # Temporary variables for aggregation\n    df['DT'] = df['TransactionDT'].apply(lambda x: (START_DATE + datetime.timedelta(seconds = x)))\n    df['DT_M'] = ((df['DT'].dt.year-2017)*12 + df['DT'].dt.month).astype(np.int8)\n    df['DT_W'] = ((df['DT'].dt.year-2017)*52 + df['DT'].dt.weekofyear).astype(np.int8)\n    df['DT_D'] = ((df['DT'].dt.year-2017)*365 + df['DT'].dt.dayofyear).astype(np.int16)\n    \n    df['DT_hour'] = (df['DT'].dt.hour).astype(np.int8)\n    df['DT_day_week'] = (df['DT'].dt.dayofweek).astype(np.int8)\n    df['DT_day_month'] = (df['DT'].dt.day).astype(np.int8)\n        \n    # Possible solo feature\n    df['is_december'] = df['DT'].dt.month\n    df['is_december'] = (df['is_december']==12).astype(np.int8)\n\n    # Holidays\n    df['is_holiday'] = (df['DT'].dt.date.astype('datetime64').isin(us_holidays)).astype(np.int8)\n\n    \n# Total transactions per timeblock\nfor col in ['DT_M','DT_W','DT_D']:\n    temp_df = pd.concat([train[[col]], test[[col]]])\n    fq_encode = temp_df[col].value_counts().to_dict()\n            \n    train[col+'_total'] = train[col].map(fq_encode)\n    test[col+'_total']  = test[col].map(fq_encode)\n    \n","86388c38":"for df in [train, test]:\n    df['bank_type'] = df['card3'].astype(str) +'_'+ df['card5'].astype(str)","01051395":"# from final features. But we can use it for aggregations.\ntrain['uid'] = train['card1'].astype(str)+'_'+train['card2'].astype(str)\ntest['uid'] = test['card1'].astype(str)+'_'+test['card2'].astype(str)\n\ntrain['uid2'] = train['uid'].astype(str)+'_'+train['card3'].astype(str)+'_'+train['card5'].astype(str)\ntest['uid2'] = test['uid'].astype(str)+'_'+test['card3'].astype(str)+'_'+test['card5'].astype(str)\n\ntrain['uid3'] = train['uid2'].astype(str)+'_'+train['addr1'].astype(str)+'_'+train['addr2'].astype(str)\ntest['uid3'] = test['uid2'].astype(str)+'_'+test['addr1'].astype(str)+'_'+test['addr2'].astype(str)\n\ntrain['uid4'] = train['uid3'].astype(str)+'_'+train['P_emaildomain'].astype(str)\ntest['uid4'] = test['uid3'].astype(str)+'_'+test['P_emaildomain'].astype(str)\n\ntrain['uid5'] = train['uid3'].astype(str)+'_'+train['R_emaildomain'].astype(str)\ntest['uid5'] = test['uid3'].astype(str)+'_'+test['R_emaildomain'].astype(str)\n\n# Add values remove list\nnew_columns = ['uid','uid2','uid3','uid4','uid5']\n\nprint('#'*10)\nprint('Most common uIds:')\nfor col in new_columns:\n    print('#'*10, col)\n    print(train[col].value_counts()[:10])\n\n# Do Global frequency encoding \n# i_cols = ['card1','card2','card3','card5'] + new_columns\n","88240641":"cat_features=[]\ncontinuous_features=[]\nfor col in train.columns:\n    if train[col].dtypes ==\"O\":\n        cat_features.append(col)\n    else:\n        if col != \"isFraud\":\n            continuous_features.append(col)","cce7c4ec":"periods = ['DT_M','DT_W','DT_D']\ntrain, test = frequency_encoding(train, test, cat_features, self_encoding=True)\nprint(\"Freq encoding is done\")\ntrain, test = timeblock_frequency_encoding(train, test, periods, cat_features, \n                                 with_proportions=False, only_proportions=True)\nprint(\"Freq encoding based on time is done\")","0ef448dd":"train.dtypes","6faa0c25":"EXCLU_COLUMNS = [\"isFraud\",'TransactionID',\"DT\"]\ncols= [col for col in train.columns if col not in EXCLU_COLUMNS]","e6cabcb2":"train.fillna(-1,inplace=True)\ntest.fillna(-1,inplace=True)\ndepth_tree=12","ffb543ea":"gc.collect()","7b5f846a":"from sklearn.tree import DecisionTreeClassifier\n\nkf = KFold(n_splits = 5, random_state = 5168, shuffle = False)\noof_train_Decision_Tree= np.zeros(train.shape[0])\nfor i,(train_index, val_index) in enumerate(kf.split(train)):\n    X_train, X_val = train[cols].iloc[train_index], train[cols].iloc[val_index]\n    y_train, y_val = train.isFraud[train_index], train.isFraud[val_index]\n    \n    clf = DecisionTreeClassifier(max_depth=depth_tree)\n    print(f\"Training of the {i+1}th decision tree has started : \")\n    clf.fit(X_train, y_train)\n    oof_train_Decision_Tree[val_index] =clf.predict_proba(X_val)[:,1]\n    \n    \n    val_score = roc_auc_score(y_val, oof_train_Decision_Tree[val_index])\n    print(f\"CV on the {i+1}th fold = {val_score}\")\nprint(f\"AUC after training all the folds = {roc_auc_score(train.isFraud, oof_train_Decision_Tree)}\")\n    ","5875aaf7":"from sklearn.ensemble import RandomForestClassifier\n\nkf = KFold(n_splits = 5, random_state = 5168, shuffle = False)\noof_train_RF= np.zeros(train.shape[0])\nfor i,(train_index, val_index) in enumerate(kf.split(train)):\n    X_train, X_val = train[cols].iloc[train_index], train[cols].iloc[val_index]\n    y_train, y_val = train.isFraud[train_index], train.isFraud[val_index]\n    \n    clf = RandomForestClassifier(n_estimators=50,max_depth=depth_tree,n_jobs=4)\n    print(f\"Training of the {i+1}th random forest has started : \")\n    clf.fit(X_train, y_train)\n    oof_train_RF[val_index] =clf.predict_proba(X_val)[:,1]\n    \n    \n    val_score = roc_auc_score(y_val, oof_train_RF[val_index])\n    print(f\"CV on the {i+1}th fold = {val_score}\")\nprint(f\"AUC after training all the folds = {roc_auc_score(train.isFraud, oof_train_RF)}\")\n    \n\n\n","062208bb":"import lightgbm as lgb\n\nkf = KFold(n_splits = 5, random_state = 5168, shuffle = False)\noof_train_LGB= np.zeros(train.shape[0])\nfor i,(train_index, val_index) in enumerate(kf.split(train)):\n    X_train, X_val = train[cols].iloc[train_index], train[cols].iloc[val_index]\n    y_train, y_val = train.isFraud[train_index], train.isFraud[val_index]\n    lgb_params = {\n                'objective':'binary',\n                'boosting_type':'gbdt',\n                'metric':'auc',\n                'n_jobs':-1,\n                'learning_rate':0.1,\n                'max_depth':depth_tree,\n                'n_estimators':1000,\n                'seed': 0,\n                'early_stopping_rounds':100, \n            }\n    clf = lgb.LGBMClassifier(**lgb_params)\n    print(f\"Training of the {i+1}th fold has started : \")\n    clf.fit(\n        X_train, y_train,\n        eval_set=[(X_train, y_train),(X_val, y_val)],\n        verbose=50,\n        early_stopping_rounds=100)\n    oof_train_LGB[val_index] = clf.predict_proba(X_val,num_iteration=clf.best_iteration_)[:,1]\n    \n    \n    val_score = roc_auc_score(y_val, oof_train_LGB[val_index])\n    print(f\"CV on the {i+1}th fold = {val_score}\")\nprint(f\"AUC after training all the folds = {roc_auc_score(train.isFraud, oof_train_LGB)}\")\n    \n\n\n","1830e0f5":"Fraud is a billion-dollar business and it is increasing every year. The PwC global economic crime survey of 2018 found that half (49 percent) of the 7,200 companies they surveyed had experienced fraud of some kind. This is an increase from the PwC 2016 study in which slightly more than a third of organizations surveyed (36%) had experienced economic crime.  \nThis problem is a binary classification problem - i.e. our target variable is a binary attribute (Is the user making the click fraudlent or not?) and our goal is to classify users into \"fraudlent\" or \"not fraudlent\" as well as possible.","f9d8db78":"Installing notebook vega for the visualization ","ec303257":"<font size=\"6\">**Introduction**<\/font>","a655d5ef":"* TransactionDT: timedelta from a given reference datetime (not an actual timestamp)\n* TransactionAMT: transaction payment amount in USD\n* ProductCD: product code, the product for each transaction\n* card1 - card6: payment card information, such as card type, card category, issue bank, country, etc.\n* addr: address\n* dist: distance\n* P_ and (R__) emaildomain: purchaser and recipient email domain\n* C1-C14: counting, such as how many addresses are found to be associated with the payment card, etc. The actual meaning is masked.\n* D1-D15: timedelta, such as days between previous transaction, etc.\n* M1-M9: match, such as names on card and address, etc.\n* Vxxx: Vesta engineered rich features, including ranking, counting, and other entity relations.","3708b0c7":"<font size=\"6\">**EDA - Exploratory Data Analysis**<\/font>","8721ad6f":"LightGBM","af97e313":"Most of columns have missing data, which is normal in real world. Also there are columns with one unique value (or all missing). There are a lot of continuous variables and some categorical. Let's have a closer look at them.","55c7b406":"AUC score only depends on how well you well you can separate the two classes. In practice, this means that only the order of your predictions matter, as a result of this, any rescaling done to your model's output probabilities will have no effect on your score.","316813fa":"Random Forest","a515c09c":"<font size=\"6\">**Modeling**<\/font>","b6f5d6a0":"Decision Tree","f9dc214e":"<font size=\"6\">**Feature Engineering**<\/font>","344e8402":"<font size=\"8\">**Data Science Workshop - Forum SUPCOM 2019**<\/font>","37ce426a":"<font size=\"6\">**Metric**<\/font>","b0a19e97":"<font size=\"6\">**Libraries Importing**<\/font>"}}