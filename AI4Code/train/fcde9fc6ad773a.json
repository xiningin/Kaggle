{"cell_type":{"66d789c8":"code","785efdf7":"code","cb936433":"code","0b9ada04":"code","2154379d":"code","b873d4e8":"code","7a07c70f":"code","64811f6b":"code","6b4c281f":"markdown","c591dcba":"markdown","b5ee90cd":"markdown","ccd10ae2":"markdown","f18f7c60":"markdown","c032452d":"markdown","029a8c80":"markdown","abace2e4":"markdown","3886ce15":"markdown","080aeacd":"markdown","f8551bfb":"markdown"},"source":{"66d789c8":"try:\n  # %tensorflow_version only exists in Colab.\n  %tensorflow_version 2.x\nexcept Exception:\n    pass\n  \nimport tensorflow as tf\nimport tensorflow_datasets as tfds\n\nimport numpy as np\nimport matplotlib.pyplot as plt","785efdf7":"def map_image(image, label):\n    '''Normalizes and flattens the image. Returns image as input and label.'''\n    image = tf.cast(image, dtype=tf.float32)\n    image = image \/ 255.0\n    image = tf.reshape(image, shape=(784,))\n    return image, image","cb936433":"# Load the train and test sets from TFDS\n\nBATCH_SIZE = 128\nSHUFFLE_BUFFER_SIZE = 1024\n\ntrain_dataset = tfds.load('mnist', as_supervised=True, split=\"train\")\ntrain_dataset = train_dataset.map(map_image)\ntrain_dataset = train_dataset.shuffle(SHUFFLE_BUFFER_SIZE).batch(BATCH_SIZE).repeat()\n\ntest_dataset = tfds.load('mnist', as_supervised=True, split=\"test\")\ntest_dataset = test_dataset.map(map_image)\ntest_dataset = test_dataset.batch(BATCH_SIZE).repeat()","0b9ada04":"def simple_autoencoder(inputs):\n    '''Builds the encoder and decoder using Dense layers.'''\n    encoder = tf.keras.layers.Dense(units=32, activation='relu')(inputs)\n    decoder = tf.keras.layers.Dense(units=784, activation='sigmoid')(encoder)\n  \n    return encoder, decoder\n\n# set the input shape\ninputs =  tf.keras.layers.Input(shape=(784,))\n\n# get the encoder and decoder output\nencoder_output, decoder_output = simple_autoencoder(inputs)\n\n# setup the encoder because you will visualize its output later\nencoder_model = tf.keras.Model(inputs=inputs, outputs=encoder_output)\n\n# setup the autoencoder\nautoencoder_model = tf.keras.Model(inputs=inputs, outputs=decoder_output)","2154379d":"autoencoder_model.compile(\n    optimizer=tf.keras.optimizers.Adam(), \n    loss='binary_crossentropy')","b873d4e8":"train_steps = 60000 \/\/ BATCH_SIZE\nsimple_auto_history = autoencoder_model.fit(train_dataset, steps_per_epoch=train_steps, epochs=50)","7a07c70f":"def display_one_row(disp_images, offset, shape=(28, 28)):\n    '''Display sample outputs in one row.'''\n    for idx, test_image in enumerate(disp_images):\n        plt.subplot(3, 10, offset + idx + 1)\n        plt.xticks([])\n        plt.yticks([])\n        test_image = np.reshape(test_image, shape)\n        plt.imshow(test_image, cmap='gray')\n\n\ndef display_results(disp_input_images, disp_encoded, disp_predicted, enc_shape=(8,4)):\n    '''Displays the input, encoded, and decoded output values.'''\n    plt.figure(figsize=(15, 5))\n    display_one_row(disp_input_images, 0, shape=(28,28,))\n    display_one_row(disp_encoded, 10, shape=enc_shape)\n    display_one_row(disp_predicted, 20, shape=(28,28,))","64811f6b":"# take 1 batch of the dataset\ntest_dataset = test_dataset.take(1)\n\n# take the input images and put them in a list\noutput_samples = []\nfor input_image, image in tfds.as_numpy(test_dataset):\n      output_samples = input_image\n\n# pick 10 random numbers to be used as indices to the list above\nidxs = np.random.choice(BATCH_SIZE, size=10)\n\n# get the encoder output\nencoded_predicted = encoder_model.predict(test_dataset)\n\n# get a prediction for the test batch\nsimple_predicted = autoencoder_model.predict(test_dataset)\n\n# display the 10 samples, encodings and decoded values!\ndisplay_results(output_samples[idxs], encoded_predicted[idxs], simple_predicted[idxs])","6b4c281f":"![Autoencoders3.jpg](attachment:968e2200-abee-4870-b4b5-1cd28e3a06fc.jpg)","c591dcba":"You will load the MNIST data from TFDS into train and test sets. Let's first define a preprocessing function for normalizing and flattening the images. Since we'll be training an autoencoder, this will return `image, image` because the input will also be the target or label while training.\n","b5ee90cd":"## Train the Model","ccd10ae2":"You will now build a simple autoencoder to ingest the data. Like before, the encoder will compress the input and reconstructs it in the decoder output.","f18f7c60":"## Compile the Model","c032452d":"You will setup the model for training. You can use binary crossentropy to measure the loss between pixel values that range from 0 (black) to 1 (white).","029a8c80":"## Display sample results\n\nYou can now visualize the results. The utility functions below will help in plotting the encoded and decoded values.","abace2e4":"## MNIST Autoencoder\n\nYou will now work on an autoencoder that works on the [MNIST dataset](https:\/\/www.tensorflow.org\/datasets\/catalog\/mnist). This will encode the inputs to lower resolution images. The decoder should then be able to generate the original input from this compressed representation.","3886ce15":"## Imports","080aeacd":"## Build the Model","f8551bfb":"## Prepare the Dataset"}}