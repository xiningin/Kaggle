{"cell_type":{"ce5dc8ea":"code","f60a9c76":"code","d94d7cee":"code","4447af63":"code","00c2bd30":"code","ae5df87b":"code","eeb409c7":"code","363fb7c3":"code","a8e01303":"code","00419850":"code","1935f1a7":"code","0edfaca8":"code","1cdf9a45":"code","c6a34817":"code","e12a5f64":"markdown","2ad56aa6":"markdown","03900140":"markdown","bd944199":"markdown","b0a66c61":"markdown","4360c23a":"markdown","c2364f2d":"markdown","0e00796d":"markdown","27b7dad9":"markdown","ee7efd93":"markdown","72a11938":"markdown"},"source":{"ce5dc8ea":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\n# import os\n# for dirname, _, filenames in os.walk('\/kaggle\/input'):\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))\n\n# # Any results you write to the current directory are saved as output.\n\nimport csv\nimport os.path\nimport datetime\nimport copy\nfrom datetime import datetime\nimport matplotlib.pyplot as plt\nimport copy\nimport itertools","f60a9c76":"cwd = os.getcwd()\n\nif os.path.isdir('COVID-19'):\n  !git -C .\/COVID-19 pull\nelse:\n  !git clone https:\/\/github.com\/CSSEGISandData\/COVID-19.git\n\n","d94d7cee":"if not os.path.isdir('output_files'):\n  !mkdir output_files\nelse:\n  print('The output_file is already created.')\n\ndb_path = 'COVID-19\/csse_covid_19_data\/csse_covid_19_time_series'\nout_path = 'output_files\/'\nfiles =['time_series_covid19_confirmed_global.csv', 'time_series_covid19_deaths_global.csv']\nformat_str = '%m\/%d\/%y' # The format\n\ndata =[]\ndata_daily = []\ndates = []\nlocations = []\nline_cnt = 0\nmarker = itertools.cycle((',', '+', '.', 'o', '*')) ","4447af63":"line_cnt = 0\ndate_cnt = 0\nwith open(os.path.join(db_path,files[0]),'r') as fi1,\\\n        open(os.path.join(db_path,files[1]),'r') as fi3, open(os.path.join(out_path,'output.csv'),'w', newline='') as ou,\\\n                open(os.path.join(out_path,'output_daily.csv'),'w', newline='') as ou1:\n  f1 = csv.reader(fi1)\n  f3 = csv.reader(fi3)\n  writer = csv.writer(ou)\n  writer1 = csv.writer(ou1)\n  for line in zip(f1,f3):\n    line_cnt += 1\n    if line_cnt == 1: #header line\n      for i in range (len(line[0][4:])):\n        dates.append(datetime.strptime(line[0][4+i], format_str))\n        date_cnt += 1\n      writer.writerow(['Date', 'location', 'Province\/State', 'Country', 'Lat', 'Long', 'Total Confirmed', 'Total Death'])\n      writer1.writerow(['Date', 'location', 'Province\/State', 'Country', 'Lat', 'Long', 'Daily New Confirmed', 'Daily New Death'])\n    else:\n      loc = '%s - %s' % (line[0][1], line[0][0]) if line[0][0] != '' else line[0][1] \n      locations.append(loc)\n      for i in range(date_cnt):\n        if line[0][4+i] == '' or line[1][4+i]=='':\n          # If the data for a particular location is not available, repeat the last day's data\n          data.append([dates[i].strftime(\"%Y-%m-%d\"),loc,line[0][0],line[0][1],line[0][2],line[0][3],line[0][4+i-1],line[1][4+i - 1]])\n        else: \n          data.append([dates[i].strftime(\"%Y-%m-%d\"),loc,line[0][0],line[0][1],line[0][2],line[0][3],line[0][4+i],line[1][4+i]])\n      \n      for i in range(1, date_cnt):\n        if not (line[0][4+i] == '' or line[1][4+i]==''):\n          data_daily.append([dates[i].strftime(\"%Y-%m-%d\"),loc,line[0][0],line[0][1],line[0][2],line[0][3],int(line[0][4+i]) - int(line[0][3+i]),int(line[1][4+i])-int(line[1][3+i])])\n\n\n  writer.writerows(data)\n  writer1.writerows(data_daily)","00c2bd30":"data = pd.read_csv(os.path.join(out_path,'output.csv'))\n\nN_death = 100\n\ndate_N_Death = dict()\n\nfor loc in locations:\n  tmp = data.loc[(data['location'] ==loc) & (data['Total Death'] > N_death)]['Date']\n  if len(tmp) != 0:\n    date_N_Death.update({loc: min(tmp)})\nprint('Date countries past %d Deaths'%N_death)\nprint(date_N_Death)\n","ae5df87b":"data_daily = pd.read_csv(os.path.join(out_path,'output_daily.csv'))\ndate_N_Confirmed = dict()\n\nN_confirmed = 500\n\nfor loc in locations:\n  tmp = data_daily.loc[(data_daily['location'] ==loc) & (data_daily['Daily New Confirmed'] > N_confirmed)]['Date']\n  if len(tmp) != 0:\n    date_N_Confirmed.update({loc: min(tmp)})\n\nprint('Date countries past %d Confirmed cases'%N_confirmed)\nprint(date_N_Confirmed)","eeb409c7":"fig=plt.figure(figsize=(20, 5), dpi= 80, facecolor='w', edgecolor='k')\ntbl_N_death = pd.DataFrame(columns=data.columns)\nfor loc, date in date_N_Death.items():\n  tbl_tmp = copy.deepcopy(data.loc[(data['location'] == loc) & (data['Date']>=date)])\n  for index, row in tbl_tmp.iterrows():\n  # #   # tbl_tmp.iloc['Date', index] = datetime.strptime(tbl_tmp['Date', index], format_str) - datetime.strptime(date, format_str)\n    tbl_tmp.loc[index, 'Date'] = (datetime.strptime(row['Date'], \"%Y-%m-%d\") - datetime.strptime(date, \"%Y-%m-%d\")).days\n  plt.plot('Date','Total Death',marker=next(marker),data=tbl_tmp,label=loc)\n  tbl_N_death = tbl_N_death.append(tbl_tmp)\nplt.legend()\nplt.gca().set_xlim(left=0)\nplt.gca().set_ylim(bottom=0)\nplt.grid(True)\nplt.title(\"Total Death after Countries Reach Total %d Death\"%N_death)\nplt.xlabel(\"Days afther total %d-Death\"%N_death)\nplt.ylabel(\"Total Death by Conuntry\/Region\")\nplt.show()\n","363fb7c3":"day = ['Day%d' % i for i in range(100)]\n# day = ['Location'] + day\nfig=plt.figure(figsize=(20, 5), dpi= 80, facecolor='w', edgecolor='k')\ntbl_N_confirmed = pd.DataFrame(columns=data.columns)\nfor loc, date in date_N_Confirmed.items():\n  tbl_tmp = copy.deepcopy(data.loc[(data['location'] == loc) & (data['Date']>=date)])\n  for index, row in tbl_tmp.iterrows():\n  # #   # tbl_tmp.iloc['Date', index] = datetime.strptime(tbl_tmp['Date', index], format_str) - datetime.strptime(date, format_str)\n    tbl_tmp.loc[index, 'Date'] = (datetime.strptime(row['Date'], \"%Y-%m-%d\") - datetime.strptime(date, \"%Y-%m-%d\")).days\n  plt.plot('Date','Total Confirmed',marker=next(marker), data=tbl_tmp,label=loc)\n  tbl_N_confirmed = tbl_N_confirmed.append(tbl_tmp)\nplt.legend()\nplt.gca().set_xlim(left=0)\nplt.gca().set_ylim(bottom=0)\nplt.grid(True)\nplt.title(\"Total Confirmed Cases After Countries Reach %d New Daily Confirmed Cases\"%N_confirmed)\nplt.xlabel(\"Days afther %d New Daily Confirmed Cases\"%N_confirmed)\nplt.ylabel(\"Total Death by Conuntry\/Region\")\nplt.show()","a8e01303":"data = pd.read_csv(os.path.join(out_path,'output.csv'))\ndata_lastDay = copy.deepcopy(data[data['Date'] == max(data['Date'])])\nmax_size = max(data_lastDay['Total Confirmed'])\ntmp_size = np.array(data_lastDay['Total Confirmed'])\nfor i in range(len(tmp_size)):\n  val = tmp_size[i]\n  size = 200* val\/max_size\n  size = 10 if size <1 and size > 0 else size\n  tmp_size[i] = size\ndata_lastDay.insert(len(data_lastDay.columns), 'size', tmp_size)","00419850":"import plotly.express as px\n\nfig = px.scatter_mapbox(data_lastDay, lat=\"Lat\", lon=\"Long\", \\\n                        size='size', color='Total Confirmed',\\\n                        hover_name=\"location\", hover_data=[\"Total Confirmed\", \"Total Death\"],\n                        color_discrete_sequence=[\"fuchsia\"], zoom=2, height=600)\nfig.update_layout(title='Total Confirmed', mapbox_style=\"open-street-map\",margin={\"r\":0,\"t\":0,\"l\":0,\"b\":0})\nfig.show()","1935f1a7":"data = pd.read_csv(os.path.join(out_path,'output.csv'))\ndata_lastDay = copy.deepcopy(data[data['Date'] == max(data['Date'])])\nmax_size = max(data_lastDay['Total Death'])\ntmp_size = np.array(data_lastDay['Total Death'])\nfor i in range(len(tmp_size)):\n  val = tmp_size[i]\n  size = 200* val\/max_size\n  size = 10 if size <1 and size > 0 else size\n  tmp_size[i] = size\ndata_lastDay.insert(len(data_lastDay.columns), 'size', tmp_size)\n\nfig = px.scatter_mapbox(data_lastDay, lat=\"Lat\", lon=\"Long\", \\\n                        size='size', color='Total Death',\\\n                        hover_name=\"location\", hover_data=[\"Total Confirmed\", \"Total Death\"],\n                        color_discrete_sequence=[\"fuchsia\"], zoom=2, height=600)\nfig.update_layout(title='Total Death', mapbox_style=\"open-street-map\",margin={\"r\":0,\"t\":0,\"l\":0,\"b\":0})\n\n\nfig.update_layout(margin={\"r\":0,\"t\":0,\"l\":0,\"b\":0})\nfig.show()","0edfaca8":"%%html\n<iframe width=\"1200\" height=\"600\" src=\"https:\/\/app.powerbi.com\/view?r=eyJrIjoiNjUyMWI2MTYtZTk4OS00NGJjLTg3MzUtMDdkYTE3NTBlNzY4IiwidCI6IjZlOTM4MWQwLWI3NWQtNDE2Yi05ZjA2LTVhZTRhMDVhNGY2NCIsImMiOjl9\" frameborder=\"0\" allowFullScreen=\"true\"><\/iframe>","1cdf9a45":"%%html\n<iframe width=\"1200\" height=\"600\" src=\"https:\/\/app.powerbi.com\/view?r=eyJrIjoiNjQwZWM4MmEtY2QxNi00YTIyLTk5OTItNTUyYzI0YWFjODE5IiwidCI6IjZlOTM4MWQwLWI3NWQtNDE2Yi05ZjA2LTVhZTRhMDVhNGY2NCIsImMiOjl9\" frameborder=\"0\" allowFullScreen=\"true\"><\/iframe>","c6a34817":"%%html\n<iframe width=\"1200\" height=\"600\" src=\"https:\/\/app.powerbi.com\/view?r=eyJrIjoiNjQwZWM4MmEtY2QxNi00YTIyLTk5OTItNTUyYzI0YWFjODE5IiwidCI6IjZlOTM4MWQwLWI3NWQtNDE2Yi05ZjA2LTVhZTRhMDVhNGY2NCIsImMiOjl9\" frameborder=\"0\" allowFullScreen=\"true\"><\/iframe>","e12a5f64":"# Goegraphical (Country-Based) Study on COVID-19 Based on JHU CSSE Data\n\n## Objectives:\nThe aim is to study the offcial reports classified based on contries, dates, and confirmed\/recovered\/death cases to observe the spreading rate of COVID-19. This objective is achieved by visualisation of data in various forms including but not limiting to \"Days after N-th Death\" and \"Days after N-th Daily New COnfirmed Cases\".\n\n## Interoduction:\nThis notebook provided by ZTSIN analysis the geographical data regarding COVID-19 based on the Johns Hopkins University Center for Systems Science and Engineering (JHU CSSE). This dataset is available: [Github](https:\/\/github.com\/CSSEGISandData\/COVID-19.git) and updates daily. A copy of data has been already uploaded to Kaggle dataset collection.\n\n**Disclaimer: The data is provided by JHU CSSE and the author have no claims or responsibilty regarding the publicly availble data.**\n\n## Results\n![image.png](attachment:image.png)","2ad56aa6":"# Cloning the Git repository\nCheck if the dataset is available on the dist:\nclone if it is not there, update if the clone vesion updated.","03900140":"# Check Our Interactive Analysis\nThese analysis are done by Power BI.\n","bd944199":"# Create Output Directory and Refences\n* Create a directory to save the new CSV files \n* Create lookup arrays (e.g. filenames)","b0a66c61":"## Total Confirmed\nThe size of circle represent the relative number of confimed cases.","4360c23a":"## Total Death\nThe size of circle represent the relative number of death cases.","c2364f2d":"# Including Libray\nlet start with loading the required libraries.","0e00796d":"# Preparing Data to Show on the Map\nAs the reported cases change drasticly from a country to another one, we are normalising them to a range [0,100] for better visualization on the map.","27b7dad9":"# Notes:\nThis notebook clone the repository from github to present the latest data. Please make sure the notebook has access to internet using the setting.","ee7efd93":"# Reading Files and Merge data\nThe reported total confirmed, recoverd, and death cases are presented in three seperate files per day. In the following, we are reading these three files and merge data data to a new CSV file called \"output.csv\". In addition, the daily number of cases (i.e. the new cases in each day, which is the difference of the reported number day to day) is calculated and sved in \"output_daily.csv\".","72a11938":"# Visualisaiton on The Map\nThe following map summerizes the data in a Map. By hovering the mouse over each point, the total number of confimed, recoverd and death cases can be observed."}}