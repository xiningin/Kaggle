{"cell_type":{"e6622cf6":"code","5de96601":"code","c9882a55":"code","8b7ed305":"code","3919d735":"code","655bfc87":"code","5f930f3c":"code","1181aeb9":"code","98215d79":"code","86c41df7":"code","22dc6381":"code","8dc3cd27":"code","13cb6a3e":"code","6f0070b1":"code","1a66e06c":"code","59a03a97":"code","09e025c7":"code","46ad9adb":"code","64f2ac35":"code","fc876e01":"code","e113dd50":"code","80ac7871":"code","818dba1c":"code","cb547783":"markdown","fdac6c71":"markdown","a7ae3113":"markdown","9e195f78":"markdown","84f2dc33":"markdown","0d661165":"markdown","81108e58":"markdown","e3dad4a7":"markdown","dd613154":"markdown","a95dacdd":"markdown","1210e2eb":"markdown","38e875f5":"markdown","50a50581":"markdown","43fc0dc7":"markdown","4c83ea48":"markdown","a17050f2":"markdown"},"source":{"e6622cf6":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","5de96601":"df = pd.read_csv('\/kaggle\/input\/ecommerce-purchase-history-from-jewelry-store\/jewelry.csv')\nprint(df.columns)\ndf.head()","c9882a55":"# Calculate total revenue\ndf['price'].sum()","8b7ed305":"dataset = df.drop(columns=['product_id', 'quantity', 'category_id', 'category_code', 'brand', 'gender', 'color', 'metal', 'gem'])\ndataset.head()","3919d735":"print(\"Number of elements: \", len(dataset))\nprint(\"Number of unique orders: \", len(dataset['order_id'].unique()))","655bfc87":"dataset.describe()","5f930f3c":"dataset['event_time'] = dataset['event_time'].str.slice(0, -4)\ndataset['event_time'] = (pd.DatetimeIndex(dataset['event_time']).astype(np.int64) \/\/ 10**9) * 1000\ndataset.head()","1181aeb9":"def f(x, y):\n    foo = dataset[(dataset['user_id'] == y) & (dataset['event_time'] < x)]\n    return len(foo)\n    \nprev_orders = [f(x, y) for x, y in zip(dataset['event_time'], dataset['user_id'])]\nprev_orders[-100:]","98215d79":"# Add new column to dataset\ndataset['prev_orders'] = prev_orders\n# Delete old column from dataset\ndataset = dataset.drop(columns=['order_id'])\ndataset.head()","86c41df7":"from random import random\nl = [0] * len(dataset['price'])\nfor i in range(len(l)):\n    if random() < 0.05:\n        l[i] = 1\n\ndataset['lost'] = l\nprint(dataset['lost'].value_counts())\ndataset.head()","22dc6381":"def g(x):\n    return max(20, x*0.1)\n    \ninsurance = [g(x) for x in dataset['price']]\ninsurance[-10:]","8dc3cd27":"dataset['insurance'] = insurance\ndataset = dataset.drop(columns=['event_time', 'user_id'])\ndataset.head()","13cb6a3e":"from sklearn.model_selection import train_test_split\n\ndataset = dataset.dropna()\ntrain, test = train_test_split(dataset, test_size=0.2)\nprint(train.head())\nprint(len(train))\nprint(test.head())\nprint(len(test))","6f0070b1":"x_train = train.iloc[:,[0, 1]]\nx_test = test.iloc[:,[0, 1]]\ny_train = train.iloc[:, 2]\ny_test = test.iloc[:, 2]\ni_train = train.iloc[:, 3]\ni_test = test.iloc[:, 3]","1a66e06c":"x_train.isnull().sum()","59a03a97":"y_train.value_counts()","09e025c7":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.calibration import CalibratedClassifierCV\n\n\nclf = LogisticRegression(class_weight='balanced')\ncalibrated_clf = CalibratedClassifierCV(base_estimator=clf, cv=3, method='isotonic')\ncalibrated_clf.fit(x_train, y_train.values.ravel())\ny_pred = calibrated_clf.predict_proba(x_test)[:, 1]\nroc_auc_score(y_test, y_pred)","46ad9adb":"y_test_pred = pd.DataFrame(y_pred, columns=['prediction'])\ny_test_pred.head()","64f2ac35":"x_final = x_test\npred = [x[0] for x in y_test_pred.values.tolist()]\nx_final['prediction'] = pred\nx_final[-10:]","fc876e01":"E_x = x_final['price'] * x_final['prediction']\nins = [g(x) for x in x_final['price']]\nE_x","e113dd50":"x_final['E_x'] = E_x\nx_final['ins'] = ins\nx_final['lost'] = y_test\nx_final","80ac7871":"x_final['price'].sum()","818dba1c":"print(\"Total cost (insuring everything): \", x_final['ins'].sum())\nprint(\"Total cost (insuring everything where insurance cost is less than actual cost): \",\n     x_final['ins'][x_final['price']>x_final['ins']].sum()\n      +x_final['price'][(x_final['price']<x_final['ins'])&(x_final['lost']==1)].sum())\nprint(\"Total cost (insuring according to model): \",\n     x_final['ins'][x_final['E_x']>x_final['ins']].sum()\n      +x_final['price'][(x_final['E_x']<x_final['ins'])&(x_final['lost']==1)].sum())","cb547783":"# Inference\nFrom the above, we can see that by using the model, we can easily save **$457253.78**, which is a pretty decent value.","fdac6c71":"Now, we calculate the expenses we have in different cases.","a7ae3113":"# The Dataset\n\nFirst, let's load the datset into memory, and list out the columns. Next, we inspect it by calling `df.head()`","9e195f78":"As we can see clearly here, the dataset is very clearly unbalanced. So, there is a very large possibility that the model does not \"learn\" anything, but rather always guesses \"0\" (do not insure). This is bad for us, though. Hence, we attempt to balance this out by means of calibration. This is achieved through the cross validation calibrated classifier found in scikit learn, and has been implemented below. \n\nFor the model, we are using **logistic regression**, with **auc score** as our evaluation metric.","84f2dc33":"As we can see, the number of unique orders < total number of rows in the dataset. This is because there are instances of some orders having multiple products in them.\n\n**For simplicity sake, we will be assuming that each order gets shipped in a seperate box, and hence each box will have to be insured seperately.**","0d661165":"Now unfortunately for us, the dataset does not contain data as to whether the customer recieved their order successfully or not. Hence, we generate fictional data for this using the following assumptions:\n\n    - Each package has a 5% chance of getting stolen\n    - This probability is random, and not dependent on any other parameters.\n\nThese assumptions are satisfied by using the `random` module in python.","81108e58":"# Overview\nThe issue of missing packages has plagued the postal and shipping industry for as long as one can remember. This is especially costly (in terms of both money and reputation) in the modern day and age, where e commerce companies regularly ship out valuable goods to their paying customers.\n\nThrough the course of this notebook, I intend to showcase the potential for using logistic regression to save costs in an e commerce company by building a model to identify the parcels to apply for insurance for. \n\nIn this Hypothetical example, we managed to save **$457253.78** in insurance costs for 22,581 parcels.\n\n### Note:\n1. Insurance rates have been defined as **insurance_cost = max($20, 10% the cost of the order)** for this example.\n\n2. We are assuming that a random 5% of all parcels get stolen. Hence, we **calibrate the model** to account for this unbalance.\n\n3. Total reveune generated from the 22,581 parcels stands at **$6531261.43**\n\n**By : [Naimish Mani B](https:\/\/www.linkedin.com\/in\/naimish-balaji-a6182b180\/)**","e3dad4a7":"Next, we convert all timestamps from the current format to Unix time, to make comparisions easier down the line.","dd613154":"Next, we need to calculate the insurance cost for each order. That is calculated according to the following definition:\n\n**insurance_cost = max($20, 10% the cost of the order)**\n\nThis is implemented below.","a95dacdd":"Now that we have the whole dataset ready, we split it up into train and test sets using sklearn.","1210e2eb":"# Concluding Remarks\n\nThere are a couple of things to note, here. For starters, we have used a generated dataset, and results like the one derived above may \/ may not be actually visible on real world datasets. Also, the model could be improved by adding the following features into consideration:\n\n- Zip code to where the parcel is being delivered\n- Medium of shipment (air, freight, cargo might have it's own set of effects on this)\n- Company used to facilitate the shipment","38e875f5":"# Introducton\n\nThere are 3 cases wherein the customer can claim to have not recieved a package they had paid for: \n\n1. Package was stolen from the customer before they could recieve it.\n2. Package was lost in transit.\n3. The customer is lying to get a refund.\n\nRegardless of the case, usually the company offers a refund to the customer. But at the end of the day, it amounts to revenue lost from the company's side. To minimise the losses from the company's end, the e commerce can opt to insure the packages that are being shipped out.\n\nBut again, insurance usually comes at a fixed cost. Hence, for young startups that have just started out and are struggling to maintain good cashflow, insuring all packages might not be feasible. Hence, they might want to selectively insure only certain packages.\n\n---","50a50581":"Now, we calculate the number of previous times the customer has made purchases on the site. We will be using this value as an input to the model later.","43fc0dc7":"Next, we add this column to the dataset and get rid of the 'order_id' column, as we don't need it anymore.","4c83ea48":"For the task at hand, we'll only be requiring the following columns:\n   - event_time\n   - order_id\n   - user_id\n   - price\n\nHence, we drop the other columns directly.","a17050f2":"Although the AUC value looks bad, it is exactly what is to be expected, since we had randomly initialised the labels."}}