{"cell_type":{"fedac03e":"code","0cb3273a":"code","bcf68e1c":"code","602e236d":"code","6f0d1d4f":"code","7c1cceca":"code","e09a0186":"code","8c3ce69a":"code","b79e9eb1":"code","289601d1":"code","d3d5b947":"code","ce1ea696":"code","4f261ad0":"code","76e3deab":"code","36193ba6":"code","ac984c32":"code","54810038":"code","174e4146":"code","3f7eede3":"code","d088794b":"code","7136d7e5":"code","dd2953a0":"code","2d57dd6f":"code","0a484018":"code","1738094d":"code","69d6b092":"code","3f952f5e":"code","5a7015cb":"code","9d1c56af":"code","18a9a796":"code","02c13ed2":"markdown","d10cc54a":"markdown","013198a1":"markdown","3a8fd4ac":"markdown","801d7746":"markdown","36726271":"markdown","4333c5ab":"markdown","20807929":"markdown","cf47038e":"markdown","07e71501":"markdown","b041f066":"markdown","2c105c08":"markdown","523605fc":"markdown","c659a57c":"markdown","ce750f75":"markdown","b92c8a66":"markdown"},"source":{"fedac03e":"# Load packages\nimport urllib\nimport warnings\n\nimport numpy as np \nimport matplotlib.pyplot as plt\nimport tensorflow as tf\n\nfrom scipy.io import loadmat\n\nfrom skimage.io import imread\nfrom skimage.transform import resize\n\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.applications.imagenet_utils import preprocess_input\nfrom tensorflow.keras.applications.resnet50 import ResNet50\nfrom tensorflow.keras.layers import Convolution2D\nfrom tensorflow.keras.models import Model\n\n# Set seed for the session\nnp.random.seed(42)","0cb3273a":"# Load a pre-trained ResNet50 model\n# We use include_top=False for now\nbase_model = ResNet50(include_top=False)\n\nprint(f'Shape of the output of the model: {base_model.output_shape}.')","bcf68e1c":"print(base_model.summary())","602e236d":"# Get the last layer of the ResNet model\nres5c = base_model.layers[-1]","6f0d1d4f":"print(f'Type of res5c: {type(res5c)}, Output shape of res5c: {res5c.output_shape}.')","7c1cceca":"# Define the custom Softmax layer\nclass SoftmaxMap(layers.Layer):\n    def __init__(self, axis=-1, **kwargs):\n        self.axis = axis\n        super(SoftmaxMap, self).__init__(**kwargs)\n        \n    def build(self, input_shape):\n        pass\n    \n    def call(self, x, mask=None):\n        \"\"\"This function is very similar to the regular Softmax but\n        we accept x.shape == (batch_size, w, h, n_classes)\n        which is not the case in Keras by default.\n        Note also that we substract the logits by their maximum to\n        make the softmax numerically stable.\n        \"\"\"\n        e = tf.exp(x - tf.math.reduce_max(x, axis=self.axis, keepdims=True))\n        s = tf.math.reduce_sum(e, axis=self.axis, keepdims=True)\n        return e \/ s\n    \n    def get_output_shape_for(self, input_shape):\n        return input_shape","e09a0186":"n_samples, w, h, n_classes = 10, 3, 4, 5\nrandom_data = np.random.randn(n_samples, w, h, n_classes).astype('float32')\n\nprint(f'Shape of random_data: {random_data.shape}.')","8c3ce69a":"random_data[0].sum(axis=-1)","b79e9eb1":"softmaxMap = SoftmaxMap()\nsoftmax_mapped_data = softmaxMap(random_data).numpy()\n\nprint(f'Shape of softmax_mapped_data: {softmax_mapped_data.shape}.')","289601d1":"softmax_mapped_data[0].sum(axis=-1)","d3d5b947":"np.alltrue(random_data[0].argmax(axis=-1) == softmax_mapped_data[0].argmax(axis=-1))","ce1ea696":"# Define a Fully Convolutional ResNet\ninput_tensor = base_model.layers[0].input\n\n# Take the output of the last layer of the ConvNet model\noutput_tensor = base_model.layers[-1].output\n\n# A 1x1 convolution, with 1000 output channels, one per class\noutput_tensor = Convolution2D(1000, (1, 1), name='Conv1000')(output_tensor)\n\n# Softmax on last axis of tensor to normalize the class predictions in each spatial area\noutput_tensor = SoftmaxMap(axis=-1)(output_tensor)\n\n# Define model\nfully_conv_resnet = Model(inputs=input_tensor, outputs=output_tensor)","4f261ad0":"prediction_maps = fully_conv_resnet(np.random.randn(1, 200, 300, 3)).numpy()\nprint(f'Shape of the predictions: {prediction_maps.shape}.')","76e3deab":"prediction_maps.sum(axis=-1)","36193ba6":"# Load weights and biases\ncomplete_model = ResNet50(include_top=True)\nW = complete_model.layers[-1].get_weights()[0]\nB = complete_model.layers[-1].get_weights()[1]\n\nlast_layer = fully_conv_resnet.layers[-2]\n\nprint(f'Shape of the weights of the last layer from the ResNet50 model: {W.shape}.')\nprint(f'Shape of the weights of the last convolutional layer: {last_layer.get_weights()[0].shape}.')","ac984c32":"# Reshape the weights\nW_reshaped = W.reshape((1, 1, 2048, 1000))\n\n# Set the convolution layer weights\nlast_layer.set_weights([W_reshaped, B])","54810038":"def forward_pass_resize(img_path, img_size):\n    img_raw = imread(img_path)\n    img = resize(img_raw, img_size, mode='reflect', preserve_range=True)\n    img = preprocess_input(img[np.newaxis])\n    print(f'Shape of the raw image: {img_raw.shape}.')\n    print(f'Shape of the reshaped image: {img.shape}.')\n    \n    prediction_map = fully_conv_resnet(img).numpy()\n    return prediction_map","174e4146":"IMG_URL = 'https:\/\/upload.wikimedia.org\/wikipedia\/commons\/thumb\/1\/16\/Female_Black_Labrador_Retriever.jpg\/1280px-Female_Black_Labrador_Retriever.jpg'\nurllib.request.urlretrieve(IMG_URL, 'dog.jpg')\noutput = forward_pass_resize('dog.jpg', (800, 600))\nprint(f'Shape of the prediction map: {output.shape}.')","3f7eede3":"# Get synset data\nSYNSET_DAT = 'https:\/\/github.com\/m2dsupsdlclass\/lectures-labs\/raw\/master\/labs\/05_conv_nets_2\/data\/meta_clsloc.mat'\nurllib.request.urlretrieve(SYNSET_DAT, 'synset_dat.mat')","d088794b":"# Load synsets\nsynsets = loadmat('synset_dat.mat')['synsets'][0]\nsynsets_imagenet_sorted = sorted([(int(s[0]), str(s[1][0])) for s in synsets[:1000]], key=lambda v:v[1])\n\ncorr = {}\nfor j in range(1000):\n    corr[synsets_imagenet_sorted[j][0]] = j\n\ncorr_inv = {}\nfor j in range(1, 1001):\n    corr_inv[corr[j]] = j\n\ndef depthfirstsearch(id_, out=None):\n    if out is None:\n        out = []\n    if isinstance(id_, int):\n        pass\n    else:\n        id_ = next(int(s[0]) for s in synsets if s[1][0] == id_)\n        \n    out.append(id_)\n    children = synsets[id_ - 1][5][0]\n    for c in children:\n        depthfirstsearch(int(c), out)\n    return out\n\ndef synset_to_dfs_ids(synset):\n    ids = [x for x in depthfirstsearch(synset) if x <= 1000]\n    ids = [corr[x] for x in ids]\n    return ids\n\ndef id_to_words(id_):\n    return synsets[corr_inv[id_] - 1][2][0]","7136d7e5":"synset_dog = 'n02084071'\nidx = synset_to_dfs_ids(synset_dog)\nprint(f'Number of dog classes ids: {len(idx)}.')","dd2953a0":"def build_heatmap(img_path, synset, size):\n    \"\"\"Build a heatmap \n    :param img_path: path of the input image, str\n    :param synset: synset to find in the image, str\n    :param size: size of the reshaped image, tuple\n    \"\"\"\n    prediction_map = forward_pass_resize(img_path, size)\n    \n    class_ids = synset_to_dfs_ids(synset)\n    class_ids = np.array([id_ for id_ in class_ids if id_ is not None])\n    \n    each_dog_proba_map = prediction_map[0, :, :, class_ids]\n    # This style of indexing a tensor by an other array has the following shape effect:\n    # (H, W, 1000) indexed by (118) => (118, H, W)\n    any_dog_proba_map = each_dog_proba_map.sum(axis=0)\n    return any_dog_proba_map","2d57dd6f":"def display_img_heatmap(img_path, heatmap):\n    \"\"\"Display the image and the heatmap side by side\n    \"\"\"\n    img = imread(img_path)\n    \n    plt.figure(figsize=(12, 8))\n    \n    plt.subplot(1, 2, 1)\n    plt.imshow(img)\n    plt.title(f'Image {img.shape}')\n    plt.axis('off')\n    \n    plt.subplot(1, 2, 2)\n    plt.imshow(heatmap, interpolation='nearest', cmap='viridis')\n    plt.title(f'Heatmap {heatmap.shape}')\n    plt.axis('off')","0a484018":"# (200, 320)\nheatmap_200x320 = build_heatmap('dog.jpg', synset_dog, (200, 320))\ndisplay_img_heatmap('dog.jpg', heatmap_200x320)","1738094d":"# (400, 640)\nheatmap_400x640 = build_heatmap('dog.jpg', synset_dog, (400, 640))\ndisplay_img_heatmap('dog.jpg', heatmap_400x640)","69d6b092":"# (800, 1280)\nheatmap_800x1280 = build_heatmap('dog.jpg', synset_dog, (800, 1280))\ndisplay_img_heatmap('dog.jpg', heatmap_800x1280)","3f952f5e":"# (1600, 2560)\nheatmap_1600x2560 = build_heatmap('dog.jpg', synset_dog, (1600, 2560))\ndisplay_img_heatmap('dog.jpg', heatmap_1600x2560)","5a7015cb":"# We resize each of the heatmap to the larger one.\nheatmap_200x320_r = resize(heatmap_200x320, (50, 80), mode='reflect',\n                           preserve_range=True, anti_aliasing=True)\nheatmap_400x640_r = resize(heatmap_400x640, (50, 80), mode='reflect',\n                           preserve_range=True, anti_aliasing=True)\nheatmap_800x1280_r = resize(heatmap_800x1280, (50, 80), mode='reflect',\n                            preserve_range=True, anti_aliasing=True)","9d1c56af":"# Arithmetic average\nheatmap = (heatmap_200x320_r + heatmap_400x640_r + heatmap_800x1280_r + heatmap_1600x2560) \/ 4\ndisplay_img_heatmap('dog.jpg', heatmap)","18a9a796":"# Geometric average\nheatmap = np.power(heatmap_200x320_r * heatmap_400x640_r * heatmap_800x1280_r * heatmap_1600x2560, 0.25)\ndisplay_img_heatmap('dog.jpg', heatmap)","02c13ed2":"## A forward pass\n\nThe following function is used to test the network. It resizes the input to a given size, then uses `model.predict` to compute the output.","d10cc54a":"Let's check that we can use this layer to normalize the classes probabilities of some random spatial predictions.","013198a1":"### Combining the heatmaps\n\nBy combining the heatmaps at different scales, we obtain a much better information about the location of the dog. The idea is to resize the heatmaps to a similar shape, and then average them.  We may look at different average computation.","3a8fd4ac":"## Unsupervised heatmap of the class *dog*\n\nThe following function builds a heatmap from a forward pass. It sums the representation for all ids corresponding to a synset.","801d7746":"The last dimension now approximately sum to one, we can therefore be used as class probabilities (or parameters for a multinouli distribution).","36726271":"# Fully Convolutional Neural Networks\n\nThis notebook is based on the Deep Learning course from the Master Datascience Paris Saclay. Materials of the course can be found [here](https:\/\/github.com\/m2dsupsdlclass\/lectures-labs).\n\n**Goals**\n* Load a CNN model pre-trained on ImageNet\n* Transform the network into a Fully Convolutional Network\n* Apply the network to perform weak segmentation on images","4333c5ab":"A $1\\times 1$ convolution applies a Dense layer to each spatial grid location. We can use random data to check that it's possible to run a forward pass on a random RGB image.","20807929":"Note that the highest activated channel for each spatial location is still the same before and after the softmax map. The ranking of the activations is preserved as softmax is a monotonic function (when considered element-wise).","cf47038e":"We observe that the two first heatmaps gave coarser segmentations than the other ones (make sense because of the different discretization). However, the two last heatmaps has small artifacts outside of the dog area. They encode more local, texture level information about the dog ,while lower resolutions will encode more semantic information about the full object. So, we may combine them to obtain a better result.","07e71501":"The output shape results to the region selected in the image. The class probabilities of each region\/area of the output map should sum to one.","b041f066":"Let's create a `SoftmaxMap` function from the layer and process our test data.","2c105c08":"## Fully convolutional ResNet\n\n* Out of the `res5c` residual block, the ResNet outputs a tensor of shape $W \\times H \\times 2048$.\n* For the default ImageNet input, $224 \\times 224$, the output size is $7 \\times 7 \\times 2048$.\n\n### Regular ResNet layers\n\nThe regular ResNet head after the base model is as follows:\n    \n    x = base_model.output\n    x = GlobalAveragePooling2D()(x)\n    x = Dense(1000)(x)\n    x = Softmax()(x)\n   \nThe full definition of the ResNet50 model is [here](https:\/\/github.com\/keras-team\/keras-applications\/blob\/master\/keras_applications\/resnet50.py).\n\n### Our version\n\n* We want to retrieve the labels information, which is stored in the Dense layer. We will load these weights afterwards.\n* We will change the Dense layer to a Convolution2D layer to keep spatial information, to output a $W \\times H \\times 1000$.\n* We can use a kernel size of $(1, 1)$ for that new Convolution2D layer to pass the spatial organization of the previous layer unchanged, *pointwise convolution*.\n* We want to apply a softmax only on the last dimension so as to preserve the $W \\times H$ spatial information.\n\n### A custom Softmax\n\nWe build the following custom Softmax layer to apply a softmax only on the last dimension of a tensor.","523605fc":"## Finding dog-related classes\n\nImageNet uses an ontology of concepts, from which classes are derived. A synset corresponds to a node in the ontology. For example, all species of dogs are children of the synset [n02084071](http:\/\/image-net.org\/synset?wnid=n02084071) (dog, domestic dog, canis familiaris).","c659a57c":"## Loading dense weights\n\nThe weights and bias of the last Dense layer of ResNet 50 are provided [here](https:\/\/www.kaggle.com\/keras\/resnet50?select=imagenet_class_index.json). Our last layer is norw a $1\\times 1$ convolutional layer instead of a fully connected layer.","ce750f75":"The shape of the convolution kernel, we want to apply to replace the Dense layer, should be $(1, 1)$. We want the output to preserve the spatial dimensions but ouput $1000$ channels (one channel perclass). And after applying SoftmaxMap, the results are normalize as per-class probabilities.","b92c8a66":"Because those predictions are random, if we sum accross the classes dimensions, we get random values instead of class probabilities that would need to sum to 1."}}