{"cell_type":{"a01035fb":"code","add8647d":"code","afbfd869":"code","e8b27886":"code","8f302848":"code","2fb25159":"code","1f78bad7":"code","72903d04":"code","09bedf17":"code","c23bb849":"code","55c53e6d":"code","040cb256":"code","3b9367fe":"code","bfeec53e":"code","ae4f6548":"code","6bfd07d8":"code","4d7675a1":"code","b8a1ac01":"code","0b59af05":"code","fc432c22":"code","4ec2ecba":"markdown","ad6c98e4":"markdown","a660cef4":"markdown","5781c838":"markdown","de3c140a":"markdown","fda746a3":"markdown","f2d41800":"markdown","333939f9":"markdown","b56cb7d6":"markdown","5630355d":"markdown","ff9d746c":"markdown","4fdc6e96":"markdown","9bdf2f81":"markdown","da1b168d":"markdown","8c6248ae":"markdown","a56056f1":"markdown","aa27f47c":"markdown","a4ced9b2":"markdown","045717e9":"markdown","70273d6e":"markdown","2b211472":"markdown","d8e7f09f":"markdown","cd8d6fff":"markdown","df0ece92":"markdown","6c0c2245":"markdown","788d9fca":"markdown","331141ea":"markdown","264e5e0d":"markdown","a34507a8":"markdown","46af5b04":"markdown","38893493":"markdown","bf08f481":"markdown","ce14342c":"markdown","0f55f64e":"markdown","a11a06df":"markdown","6ccfb58e":"markdown","24d24292":"markdown","afccc798":"markdown","ab96b958":"markdown","efc7b4c5":"markdown","3fbc7bb1":"markdown","a86a9a2a":"markdown","e316c582":"markdown","5543086d":"markdown","77a29a4e":"markdown","675dbc5f":"markdown","06c80520":"markdown","d2e01ed1":"markdown","b6e71fa1":"markdown","7d0e21eb":"markdown","b2f0ac61":"markdown","9b5cbc64":"markdown"},"source":{"a01035fb":"import numpy as np\n\nimport csv\n\nimport torch\ntorch.set_printoptions(edgeitems=2, precision=2, linewidth=75)","add8647d":"wine_path = \"\/kaggle\/input\/winequality-white.csv\"\nwineq_numpy = np.loadtxt(wine_path, dtype=np.float32, delimiter=\";\", skiprows=1)\n\nwineq_numpy","afbfd869":"col_list = next(csv.reader(open(wine_path), delimiter=';'))\n\nwineq_numpy.shape, col_list","e8b27886":"wineq = torch.from_numpy(wineq_numpy)\n\nwineq.shape, wineq.dtype","8f302848":"data = wineq[:, :-1]  # Selects all rows and all columns except the last\ndata, data.shape","2fb25159":"target = wineq[:, -1]  # Selects all rows and the last column\ntarget, target.shape","1f78bad7":"target = wineq[:, -1].long()\ntarget","72903d04":"target_onehot = torch.zeros(target.shape[0], 10)\n\ntarget_onehot.scatter_(1, target.unsqueeze(1), 1.0)","09bedf17":"target_unsqueezed = target.unsqueeze(1)\ntarget_unsqueezed","c23bb849":"data_mean = torch.mean(data, dim=0)\ndata_mean","55c53e6d":"data_var = torch.var(data, dim=0)\ndata_var","040cb256":"data_normalized = (data - data_mean) \/ torch.sqrt(data_var)\ndata_normalized","3b9367fe":"bad_indexes = target <= 3  # PyTorch also provides comparison functions, \n                           # here torch.le(target, 3), but using operators\n                           # seems to be a good standard.\nbad_indexes.shape, bad_indexes.dtype, bad_indexes.sum()","bfeec53e":"bad_data = data[bad_indexes]\nbad_data.shape","ae4f6548":"bad_data = data[target <= 3]\nmid_data = data[(target > 3) & (target < 7)]  # For Boolean NumPy arrays and\n                                              # PyTorch tensors, the & operator\n                                              # does a logical \u201cand\u201d operation.\ngood_data = data[target >= 7]","6bfd07d8":"bad_mean = torch.mean(bad_data, dim=0)\nmid_mean = torch.mean(mid_data, dim=0)\ngood_mean = torch.mean(good_data, dim=0)","4d7675a1":"for i, args in enumerate(zip(col_list, bad_mean, mid_mean, good_mean)):\n    print('{:2} {:20} {:6.2f} {:6.2f} {:6.2f}'.format(i, *args))","b8a1ac01":"total_sulfur_threshold = 141.83\ntotal_sulfur_data = data[:,6]\npredicted_indexes = torch.lt(total_sulfur_data, total_sulfur_threshold)\n\npredicted_indexes.shape, predicted_indexes.dtype, predicted_indexes.sum()","0b59af05":"actual_indexes = target > 5\n\nactual_indexes.shape, actual_indexes.dtype, actual_indexes.sum()","fc432c22":"n_matches = torch.sum(actual_indexes & predicted_indexes).item()\nn_predicted = torch.sum(predicted_indexes).item()\nn_actual = torch.sum(actual_indexes).item()\n\nn_matches, n_matches \/ n_predicted, n_matches \/ n_actual","4ec2ecba":"## Finding thresholds","ad6c98e4":"Let\u2019s see what scatter_ does. First, we notice that its name ends with an underscore.\nAs you learned in the previous chapter, this is a convention in PyTorch that indicates\nthe method will not return a new tensor, but will instead modify the tensor in place.\nThe arguments for scatter_ are as follows:\n* The dimension along which the following two arguments are specified\n* A column tensor indicating the indices of the elements to scatter\n* A tensor containing the elements to scatter or a single scalar to scatter (1, in\nthis case)","a660cef4":"The call to unsqueeze adds a singleton dimension, from a 1D tensor of 4,898 elements\nto a 2D tensor of size (4,898 \u00d7 1), without changing its contents\u2014no extra elements\nare added; we just decided to use an extra index to access the elements. That is, we\naccess the first element of target as target[0] and the first element of its\nunsqueezed counterpart as target_unsqueezed[0,0] .","5781c838":"Finally, categorical values have neither ordering nor numerical meaning to their values. These are often just enumerations of possibilities assigned arbitrary numbers. Assigning water to 1, coffee to 2, soda to 3, and milk to 4 is a good example. There\u2019s no real logic to placing water first and milk last; they simply need distinct values to differentiate them. We could assign coffee to 10 and milk to \u20133, and there would be no significant change (though assigning values in the range 0.. N \u2013 1 will have advantages for one-hot encoding and the embeddings we\u2019ll discuss in section 4.5.4.) Because the numerical values bear no meaning, they are said to be on a nominal scale.","de3c140a":"## Loading a wine data tensor","fda746a3":"Note that the new bad_data tensor has 20 rows, the same as the number of rows with True in the bad_indexes tensor. It retains all 11 columns. Now we can start to get information about wines grouped into good, middling, and bad categories. Let\u2019s take the .mean() of each column:","f2d41800":"If targets were string labels, like wine color, assigning an integer number to each string\nwould let us follow the same approach.","333939f9":"The third option is the most time- and memory-efficient. However, we\u2019ll avoid introducing an additional library in our learning trajectory just because we need to load a file. Since we already introduced NumPy in the previous section, and PyTorch has excellent NumPy interoperability, we\u2019ll go with that. Let\u2019s load our file and turn the resulting NumPy array into a PyTorch tensor (code\/p1ch4\/3_tabular_wine.ipynb).","b56cb7d6":"PyTorch allows us to use class indices directly as targets while training neural networks. However, if we wanted to use the score as a categorical input to the network, we would have to transform it to a one-hot-encoded tensor.","5630355d":"PyTorch tensors, on the other hand, are homogeneous. Information in PyTorch is\ntypically encoded as a number, typically floating-point (though integer types and\nBoolean are supported as well). This numeric encoding is deliberate, since neural\nnetworks are mathematical entities that take real numbers as inputs and produce real\nnumbers as output through successive application of matrix multiplications and\nnonlinear functions.","ff9d746c":"Our first job as deep learning practitioners is to encode heterogeneous, real-world\ndata into a tensor of floating-point numbers, ready for consumption by a neural network. A large number of tabular datasets are freely available on the internet; see, for\ninstance, https:\/\/github.com\/caesar0301\/awesome-public-datasets. Let\u2019s start with\nsomething fun: wine! The Wine Quality dataset is a freely available table containing\nchemical characterizations of samples of vinho verde, a wine from north Portugal,\ntogether with a sensory quality score. The dataset for white wines can be downloaded\nhere: http:\/\/mng.bz\/90Ol. For convenience, we also created a copy of the dataset on\nthe Deep Learning with PyTorch Git repository, under data\/p1ch4\/tabular-wine.","4fdc6e96":"Since there are about 500 more actually good wines than our threshold predicted, we already have hard evidence that it\u2019s not perfect. Now we need to see how well our predictions line up with the actual rankings. We will perform a logical \u201cand\u201d between our prediction indexes and the actual good indexes (remember that each is just an array of zeros and ones) and use that intersection of wines-in-agreement to determine how well we did:","9bdf2f81":"![image.png](attachment:image.png)","da1b168d":"## When to categorize","8c6248ae":"In this case, dim=0 indicates that the reduction is performed along dimension 0. At this point, we can normalize the data by subtracting the mean and dividing by the standard deviation, which helps with the learning process (we\u2019ll discuss this in more detail in chapter 5, in section 5.4.4):","a56056f1":"Here we just prescribe what the type of the 2D array should be (32-bit floating-point), the delimiter used to separate values in each row, and the fact that the first line should not be read since it contains the column names. Let\u2019s check that all the data has been read","aa27f47c":"## One-hot encoding","a4ced9b2":"https:\/\/github.com\/deep-learning-with-pytorch\/dlwpt-code","045717e9":"Indeed, a simple neural network would overcome all of these limitations, as would a lot of other basic machine learning methods. We\u2019ll have the tools to tackle this problem after the next two chapters, once we have learned how to build our first neural network from scratch. We will also revisit how to better grade our results in chapter 12. Let\u2019s move on to other data types for now.","70273d6e":"The other approach is to build a one-hot encoding of the scores: that is, encode each of the 10 scores in a vector of 10 elements, with all elements set to 0 but one, at a different index for each score. This way, a score of 1 could be mapped onto the vector (1,0,0,0,0,0,0,0,0,0) , a score of 5 onto (0,0,0,0,1,0,0,0,0,0) , and so on. Note that the fact that the score corresponds to the index of the nonzero element is purely incidental: we could shuffle the assignment, and nothing would change from a classification standpoint.","2b211472":"This means our threshold implies that just over half of all the wines are going to be high quality. Next, we\u2019ll need to get the indexes of the actually good wines:","d8e7f09f":"We should be aware of three different kinds of numerical values as we attempt to make sense of our data. The first kind is continuous values. These are the most intuitive when represented as numbers. They are strictly ordered, and a difference between various values has a strict meaning. Stating that package A is 2 kilograms heavier than package B, or that package B came from 100 miles farther away than A has a fixed meaning, regardless of whether package A is 3 kilograms or 10, or if B came from 200 miles away or 2,000. If you\u2019re counting or measuring something with units, it\u2019s probably a continuous value. The literature actually divides continuous values further: in the previous examples, it makes sense to say something is twice as heavy or three times farther away, so those values are said to be on a ratio scale. The time of day, on the other hand, does have the notion of difference, but it is not reasonable to claim that 6:00 is twice as late as 3:00; so time of day only offers an interval scale.","cd8d6fff":"Next we have ordinal values. The strict ordering we have with continuous values remains, but the fixed relationship between values no longer applies. A good example of this is ordering a small, medium, or large drink, with small mapped to the value 1, medium 2, and large 3. The large drink is bigger than the medium, in the same way that 3 is bigger than 2, but it doesn\u2019t tell us anything about how much bigger. If we were to convert our 1, 2, and 3 to the actual volumes (say, 8, 12, and 24 fluid ounces), then they would switch to being interval values. It\u2019s important to remember that we can\u2019t \u201cdo math\u201d on the values outside of ordering them; trying to average large = 3 and small = 1 does not result in a medium drink!","df0ece92":"Next, let\u2019s start to look at the data with an eye to seeing if there is an easy way to tell good and bad wines apart at a glance. First, we\u2019re going to determine which rows in target correspond to a score less than or equal to 3:","6c0c2245":"* https:\/\/www.kaggle.com\/dmisky\/dlwpt-p1ch2-dog-detection\n\n* https:\/\/www.kaggle.com\/dmisky\/dlwpt-p1ch2-gan-horse-zebra\n\n* https:\/\/www.kaggle.com\/dmisky\/dlwpt-p1ch3-tensors\n\n* https:\/\/www.kaggle.com\/dmisky\/dlwpt-p1ch4-working-with-images\n\n* https:\/\/www.kaggle.com\/dmisky\/dlwpt-p1ch4-3d-images-volumetric-data","788d9fca":"All data from book **Deep Learning with PyTorch** https:\/\/pytorch.org\/deep-learning-with-pytorch","331141ea":"At first we are going to assume there\u2019s no meaning to the order in which samples\nappear in the table: such a table is a collection of independent samples, unlike a time\nseries, for instance, in which samples are related by a time dimension.","264e5e0d":"The file contains a comma-separated collection of values organized in 12 columnsmpreceded by a header line containing the column names. The first 11 columns contain values of chemical variables, and the last column contains the sensory quality score from 0 (very bad) to 10 (excellent). These are the column names in the order they appear in the dataset:","a34507a8":"# Representing tabular data","46af5b04":"It looks like we\u2019re on to something here: at first glance, the bad wines seem to have higher total sulfur dioxide, among other differences. We could use a threshold on total sulfur dioxide as a crude criterion for discriminating good wines from bad ones. Let\u2019s get the indexes where the total sulfur dioxide column is below the midpoint we calculated earlier, like so:","38893493":"The simplest form of data we\u2019ll encounter on a machine learning job is sitting in a\nspreadsheet, CSV file, or database. Whatever the medium, it\u2019s a table containing one\nrow per sample (or record), where columns contain one piece of information about\nour sample.","bf08f481":"and proceed to convert the NumPy array to a PyTorch tensor:","ce14342c":"* fixed acidity\n* volatile acidity\n* citric acid\n* residual sugar\n* chlorides\n* free sulfur dioxide\n* total sulfur dioxide\n* density\n* pH\n* sulphates\n* alcohol\n* quality","0f55f64e":"We got around 2,000 wines right! Since we predicted 2,700 wines, this gives us a 74% chance that if we predict a wine to be high quality, it actually is. Unfortunately, there are 3,200 good wines, and we only identified 61% of them. Well, we got what we signed up for; that\u2019s barely better than random! Of course, this is all very naive: we know for sure that multiple variables contribute to wine quality, and the relationships between the values of these variables and the outcome (which could be the actual score, rather than a binarized version of it) is likely more complicated than a simple threshold on a single value.","a11a06df":"**Continuous, ordinal, and categorical values**\n","6ccfb58e":"Columns may contain numerical values, like temperatures at specific locations; or\nlabels, like a string expressing an attribute of the sample, like \u201cblue.\u201d Therefore, tabu-\nlar data is typically not homogeneous: different columns don\u2019t have the same type. We\nmight have a column showing the weight of apples and another encoding their color\nin a label.","24d24292":"Now we have seen ways to deal with both continuous and categorical data. You may wonder what the deal is with the ordinal case discussed in the earlier sidebar. There is no general recipe for it; most commonly, such data is either treated as categorical (losing the ordering part, and hoping that maybe our model will pick it up during training if we only have a few categories) or continuous (introducing an arbitrary notion of distance). We will do the latter for the weather situation in figure 4.5. We summarize our data mapping in a small flow chart in figure 4.4.","afccc798":"Note that only 20 of the bad_indexes entries are set to True ! By using a feature in PyTorch called advanced indexing, we can use a tensor with data type torch.bool to index the data tensor. This will essentially filter data to be only items (or rows) corresponding to True in the indexing tensor. The bad_indexes tensor has the same shape as target , with values of False or True depending on the outcome of the comparison between our threshold and each element in the original target tensor:","ab96b958":"In other words, the previous invocation reads, \u201cFor each row, take the index of the target label (which coincides with the score in our case) and use it as the column index to set the value 1.0.\u201d The end result is a tensor encoding categorical information.","efc7b4c5":"If we want to transform the target tensor in a tensor of labels, we have two options, depending on the strategy or what we use the categorical data for. One is simply to treat labels as an integer vector of scores:","3fbc7bb1":"There\u2019s a marked difference between the two approaches. Keeping wine quality scores in an integer vector of scores induces an ordering on the scores\u2014which might be totally appropriate in this case, since a score of 1 is lower than a score of 4. It also induces some sort of distance between scores: that is, the distance between 1 and 3 is the same as the distance between 2 and 4. If this holds for our quantity, then great. If, on the other hand, scores are purely discrete, like grape variety, one-hot encoding will be a much better fit, as there\u2019s no implied ordering or distance. One-hot encoding is also appropriate for quantitative scores when fractional values in between integer scores, like 2.4, make no sense for the application\u2014for when the score is either this or that.","a86a9a2a":"Let\u2019s go back to our data tensor, containing the 11 variables associated with the chemical analysis. We can use the functions in the PyTorch Tensor API to manipulate our data in tensor form. Let\u2019s first obtain the mean and standard deviations for each column:","e316c582":"![image.png](attachment:image.png)","5543086d":"We could treat the score as a continuous variable, keep it as a real number, and perform a regression task, or treat it as a label and try to guess the label from the chemical analysis in a classification task. In both approaches, we will typically remove the score from the tensor of input data and keep it in a separate tensor, so that we can use the score as the ground truth without it being input to our model:","77a29a4e":"## Representing scores","675dbc5f":"The second argument of scatter_ , the index tensor, is required to have the same\nnumber of dimensions as the tensor we scatter into. Since target_onehot has two\ndimensions (4,898 \u00d7 10), we need to add an extra dummy dimension to target using\nunsqueeze :","06c80520":"A possible machine learning task on this dataset is predicting the quality score from chemical characterization alone. Don\u2019t worry, though; machine learning is not going to kill wine tasting anytime soon. We have to get the training data from somewhere! As we can see in figure 4.3, we\u2019re hoping to find a relationship between one of the chemical columns in our data and the quality column. Here, we\u2019re expecting to see quality increase as sulfur decreases.","d2e01ed1":"At this point, we have a floating-point torch.Tensor containing all the columns, including the last, which refers to the quality score.","b6e71fa1":"* The csv module that ships with Python\n* NumPy\n* Pandas","7d0e21eb":"As a starting point for a more in-depth discussion, refer to https:\/\/en.wikipedia.org\/wiki\/Level_of_measurement.","b2f0ac61":"## Using a real-world dataset","9b5cbc64":"We can achieve one-hot encoding using the scatter_ method, which fills the ten-\nsor with values from a source tensor along the indices provided as arguments:"}}