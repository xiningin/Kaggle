{"cell_type":{"d10642ac":"code","adaf84df":"code","06f10e43":"code","13599300":"code","4ca508e7":"code","68b319ef":"code","899280c6":"code","ee4afbcb":"code","046c69a9":"code","cb537475":"code","13983153":"code","5eb24074":"code","1d5db3f7":"code","8187e8df":"code","16f8e0b2":"code","5af42a4f":"code","ab509ab5":"code","af2c17e7":"code","fc7474d1":"code","b359bb66":"code","93ac886c":"code","050c9cd0":"code","4f13e4d8":"code","9c4ab750":"markdown","859fa4bf":"markdown","86329776":"markdown","b5d03d05":"markdown","717310a3":"markdown","cef36112":"markdown"},"source":{"d10642ac":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","adaf84df":"from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\nfrom sklearn.decomposition import NMF, LatentDirichletAllocation\nimport h2o\nfrom h2o.automl import H2OAutoML\nfrom h2o.estimators.word2vec import H2OWord2vecEstimator\nimport gc\nfrom fuzzywuzzy import fuzz","06f10e43":"df = pd.read_csv(\"..\/input\/train.csv\")\ndf.head()","13599300":"df_toxic = df[df[\"target\"]>0.50]\nprint(df_toxic.shape)\ndf_toxic.head()","4ca508e7":"documents_toxic = df_toxic[\"comment_text\"]\ndocuments_toxic[0:10]\n\ndel df_toxic\ngc.collect()","68b319ef":"df_nontoxic = df[df[\"target\"]<0.50].sample(frac=0.075, replace=False, random_state=1331)\nprint(df_nontoxic.shape)\ndf_nontoxic.head()","899280c6":"documents_nontoxic = df[\"comment_text\"]\ndocuments_nontoxic[0:10]\n\ndel df_nontoxic\ngc.collect()","ee4afbcb":"def myfeaturing(documents,column_tag=\"_run1_\",no_features=1000, no_topics=20, max_df=0.95, min_df=2,\n                alpha=0.1, l1_ratio=0.5, max_iter=5, learning_offset=50):\n    # NMF is able to use tf-idf\n    tfidf_vectorizer = TfidfVectorizer(max_df=max_df, min_df=min_df, max_features=no_features, stop_words='english')\n    tfidf = tfidf_vectorizer.fit_transform(documents)\n    tfidf_feature_names = tfidf_vectorizer.get_feature_names()\n    \n    # LDA can only use raw term counts for LDA because it is a probabilistic graphical model\n    tf_vectorizer = CountVectorizer(max_df=max_df, min_df=min_df, max_features=no_features, stop_words='english')\n    tf = tf_vectorizer.fit_transform(documents)\n    tf_feature_names = tf_vectorizer.get_feature_names()\n    \n    # Run NMF\n    nmf = NMF(n_components=no_topics, random_state=1, alpha=alpha, l1_ratio=l1_ratio, init='nndsvd').fit(tfidf)\n\n    # Run LDA\n    lda = LatentDirichletAllocation(n_topics=no_topics, max_iter=max_iter, \n                                    learning_method='online', learning_offset=learning_offset,random_state=0).fit(tf)\n    \n    # display top 10 words for top 10 topics\n    def display_topics(model, feature_names, no_top_words=10):\n        for topic_idx, topic in enumerate(model.components_):\n            print(\"Topic %d:\" % (topic_idx))\n            print(\" \".join([feature_names[i]\n                            for i in topic.argsort()[:-no_top_words - 1:-1]]))\n            if topic_idx > 9:\n                break\n    \n    display_topics(nmf, tfidf_feature_names)\n    display_topics(lda, tf_feature_names)\n    \n    # small tfidf\n    tfidf_max_features=20\n    word_vectorizer = TfidfVectorizer(sublinear_tf=True,strip_accents='unicode',analyzer='word',\n                                      token_pattern=r'\\w{1,}',stop_words='english',ngram_range=(1, 1),\n                                      max_features=tfidf_max_features)\n    word_vectorizer.fit(documents)\n    \n    # scoring\n    \n    def myscoring(df):\n        documents2 = df[\"comment_text\"]\n\n        tftran = tf_vectorizer.transform(documents2)\n        out2 = lda.transform(tftran)\n\n        tfidftran = tfidf_vectorizer.transform(documents2)\n        out3 = nmf.transform(tfidftran)\n\n        mycols = range(1,no_topics+1)\n        out2pd = pd.DataFrame(data=out2, columns=[\"lda\" + column_tag + str(mycol) for mycol in mycols])\n        out3pd = pd.DataFrame(data=out3, columns=[\"nmf\" + column_tag + str(mycol) for mycol in mycols])\n        \n        tfidf = word_vectorizer.transform(df['comment_text'])\n        tfidf_df = pd.DataFrame(tfidf.toarray(), columns=word_vectorizer.get_feature_names())\n        \n        being_absurd = \"Well could give address? I would happy send guy ilk house, grab stuff right front face run away it. You know scared want stuff yourself. Perhaps could invite scumbag dinner wine. Share smokes him. For love God, happened us?\"\n        political_rant = \"Another opinionated non-headline again, could well leave trumps name say much-i.e. reason think play out? But oh no, fit adns mission every article anything regarding next administration (or republicans matter) MUST negative tone. What garbage.\"\n        name_calling = \"You need drunk moron.\"\n        political_slight = \"He wants Obama's black privilege Hillary's ability rig elections.\"\n        black_insensitive = \"There 3 white soldiers killed well. I've heard read nothing next kin news media. Here clue...... While I've tragedies surely empathize widow, pattern life. For past 30-35 years, I see black American's complaining everything.....but never black black crime. They 'leaders' always complaining others, never take responsibility actions. Not media. It happens job. It happens schools. On buses. Every perceived slight 'I get respect'.....unless black person them. I mid-60's civil rights marcher. That equal rights. For 50 years I've watched blacks get breaks affirmative action media coverage. Meanwhile, I white co-workers passed promotions, allowed fraction black employees get away daily. I'm empathized out.\"\n\n        # create function for comparing string to other strings\n        def my_str_compare(x):\n            return fuzz.token_sort_ratio(x,my_string)\n\n        # add features to training set\n        my_string = being_absurd\n        df['being_absurd'] = df['comment_text'].apply(my_str_compare)\n        my_string = political_rant\n        df['political_rant'] = df['comment_text'].apply(my_str_compare)\n        my_string = name_calling\n        df['name_calling'] = df['comment_text'].apply(my_str_compare)\n        my_string = political_slight\n        df['political_slight'] = df['comment_text'].apply(my_str_compare)\n        my_string = black_insensitive\n        df['black_insensitive'] = df['comment_text'].apply(my_str_compare) \n        \n        df_fuzzy = df[['being_absurd','political_rant','name_calling','political_slight','black_insensitive']]\n        \n        frames = [out2pd,out3pd,tfidf_df,df_fuzzy]\n        df = pd.concat(frames,axis=1)\n        \n        del out2pd, out3pd, out2, out3, frames, tfidf_df, tfidf\n        \n        return df\n    \n    df = pd.read_csv(\"..\/input\/train.csv\")\n    df_train = myscoring(df)\n    df = pd.read_csv(\"..\/input\/test.csv\")\n    df_test = myscoring(df)\n    \n    \n    del df \n    gc.collect()\n    \n    return df_train, df_test","046c69a9":"df_train1, df_test1 = myfeaturing(documents=documents_toxic, column_tag=\"_run1_\")\n\ndel documents_toxic\ngc.collect()","cb537475":"frames = [df[\"target\"],df_train1]\ndf_train = pd.concat(frames,axis=1)\ndf_train.head()","13983153":"del frames, df_train1\ngc.collect()","5eb24074":"h2o.init()","1d5db3f7":"train = h2o.H2OFrame(df_train)\ntrain.head()","8187e8df":"# Identify predictors and response\nx = train.columns\ny = \"target\"\nx.remove(y)","16f8e0b2":"# Run AutoML for 30 base models (limited to 1 hour max runtime by default)\naml = H2OAutoML(max_models=20, seed=1331)\naml.train(x=x, y=y, training_frame=train)","5af42a4f":"# View the AutoML Leaderboard\nlb = aml.leaderboard\nlb.head(rows=lb.nrows)  # Print all rows instead of default (10 rows)","ab509ab5":"# The leader model is stored here\naml.leader","af2c17e7":"test = h2o.H2OFrame(df_test1)","fc7474d1":"prediction = aml.leader.predict(test)","b359bb66":"# create submission DataFrame\nt_df = pd.read_csv(\"..\/input\/test.csv\")\nsubmission = pd.DataFrame(t_df['id'])\nsubmission.head()","93ac886c":"pd_pred = prediction.as_data_frame()\npd_pred.head()","050c9cd0":"submission['prediction'] = pd_pred\nsubmission.head()","4f13e4d8":"submission.to_csv('submission.csv',index=False)","9c4ab750":"# Clustering","859fa4bf":"# H2O","86329776":"# Submission","b5d03d05":"# Imports","717310a3":"# Get Data","cef36112":"# References:\n\nhttps:\/\/medium.com\/mlreview\/topic-modeling-with-scikit-learn-e80d33668730\n\nhttps:\/\/github.com\/h2oai\/h2o-3\/blob\/master\/h2o-py\/demos\/word2vec_craigslistjobtitles.ipynb\n\nhttp:\/\/docs.h2o.ai\/h2o\/latest-stable\/h2o-docs\/automl.html\n\nhttps:\/\/stackoverflow.com\/questions\/46971969\/conversion-of-pandas-dataframe-to-h2o-frame-efficiently\n\nhttps:\/\/github.com\/h2oai\/h2o-meetups\/blob\/master\/2016_10_06_TrumpTweets_Meetup\/python-nlp\/Python-TF-IDF.ipynb\n"}}