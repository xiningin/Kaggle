{"cell_type":{"d2320abf":"code","0f9f50e8":"code","d6db2aa6":"code","aefde5c0":"code","439e9e31":"code","fa21afd1":"code","f72f3c3c":"code","8b8fd8ec":"code","c41fb158":"code","7937c314":"code","2947e618":"code","d851c53b":"code","a8e18492":"code","346dfd1b":"code","888c8b0e":"code","ae28ca6c":"code","99a3708e":"code","c60d8992":"code","6e612554":"code","b4a1c595":"code","cea307f4":"code","4bee08cf":"code","742e6406":"code","3e13ba07":"code","6ea248c5":"code","56f02c37":"code","0a8be247":"code","009a7e1c":"code","2bab8a4d":"code","a84aa8af":"code","3bedcd4b":"code","b65dbac3":"code","76353554":"code","11e3a3e5":"code","1f8554c7":"code","9f5f4c18":"code","59253967":"code","fb989322":"code","9aae181f":"code","4b7c8a37":"code","faee572e":"code","60a84573":"code","17c035b8":"code","1d9ebdbf":"code","dc9a95fd":"code","d43ce2c0":"code","f23f639a":"code","e930c7a2":"code","528e932d":"code","60e91411":"code","4b3a1734":"code","a6a311b8":"code","869f39b1":"code","98bad015":"code","cc5c9305":"code","eaf761df":"code","95dd4748":"code","ebe1c0c7":"code","5053bf4c":"code","7d7883b1":"markdown","802162be":"markdown","10f40d38":"markdown","6a8c6b38":"markdown","31dcd61a":"markdown","f0d8dd08":"markdown","1f991632":"markdown","0fb4c0b3":"markdown","5c1736c9":"markdown","d35f086d":"markdown","0def3d01":"markdown","deba18ad":"markdown","703cf567":"markdown","cf1bb9ed":"markdown","c34f7e23":"markdown","52b88bb1":"markdown","1128a21b":"markdown","627e0206":"markdown","1108160b":"markdown","e51085a9":"markdown","36449f14":"markdown","50cfd936":"markdown","0fd7d797":"markdown","065b68d4":"markdown","75c9ce41":"markdown","4151c814":"markdown","baea66a3":"markdown","cd61dc03":"markdown","73fffbed":"markdown","bc234d68":"markdown","6851c048":"markdown","5baf4b43":"markdown","16accf30":"markdown","f303a5f6":"markdown","626541be":"markdown","2f45f3b0":"markdown","58b83c80":"markdown","65709516":"markdown","87196955":"markdown","1b267d66":"markdown","56df6827":"markdown","480bdcd7":"markdown","5a2a0afb":"markdown","e1a7a895":"markdown","011e29ba":"markdown","3f39a853":"markdown","be6e14c1":"markdown","9b397f58":"markdown","7213d754":"markdown","6d7bf4db":"markdown","c7500243":"markdown","97c397d5":"markdown","a55b9c2c":"markdown","ba2c5095":"markdown","eb66a7d6":"markdown","a238fa73":"markdown","d5e42886":"markdown","c71c7178":"markdown"},"source":{"d2320abf":"#!pip install comet_ml\n\n#from comet_ml import Experiment\n\n#experiment = Experiment(api_key='PPWu2N8uFtsj5dnmQzCNSLBVE', project_name='team-2-dbn-classifiers')","0f9f50e8":"# Required packages\n#import os\n#for dirname, _, filenames in os.walk('\/kaggle\/input'):\n#    for filename in filenames:\n#        print(os.path.join(dirname, filename))\nimport pickle\n#import comet_ml\n\n# Import Libraries for Data Visualisation\nimport seaborn as sns\nfrom scipy import stats\nimport matplotlib.pyplot as plt\nfrom wordcloud import WordCloud, STOPWORDS\nfrom matplotlib.colors import ListedColormap\n%matplotlib inline\n\n\n\n# Import libraries for Natural Languge Processing\nimport re\nimport nltk\nimport numpy as np\nimport pandas as pd\nfrom nltk.corpus import stopwords\nfrom html.parser import HTMLParser\nfrom nltk.stem import PorterStemmer\nfrom nltk.tokenize import word_tokenize\nnltk.download('stopwords')\nnltk.download('punkt')\n\n# Import library for Feature Extraction\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.feature_extraction.text import CountVectorizer\n\n# Import library for train test split\nfrom sklearn.model_selection import train_test_split\n\n# Import Machine Learning algorithms\nfrom sklearn.svm import SVC\nfrom sklearn import metrics\nfrom sklearn.svm import LinearSVC\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import GridSearchCV, RandomizedSearchCV\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.model_selection import StratifiedKFold, cross_val_score\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\nfrom sklearn.ensemble import VotingClassifier\n\n# Import libraries for model evaluation\nfrom sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n\n# Import warnings\nimport warnings\nwarnings.filterwarnings('ignore', category=FutureWarning)\nwarnings.filterwarnings('ignore', category=DeprecationWarning)\n","d6db2aa6":"# Importing the train & test data sets\ntrain_df = pd.read_csv('..\/input\/climate-change-belief-analysis\/train.csv')\ntest_df = pd.read_csv('..\/input\/climate-change-belief-analysis\/test.csv')","aefde5c0":"# View the first 5 rows of the train data\ntrain_df.head()","439e9e31":"# View the first 5 rows of the test data\ntest_df.head()","fa21afd1":"# Let us inspect the dimensions of the data sets.\nprint('train dim:', train_df.shape, 'test dim:', test_df.shape)\ntrain_df.iloc[0:2]","f72f3c3c":"# Create class distribution dataframe\nclass_dist = pd.DataFrame(list(train_df['sentiment'].value_counts()),\n                          index=['Pro', 'News', 'Neutral', 'Anti'],\n                          columns=['Count'])\n\n# Plot class distribution\nsns.set(style=\"whitegrid\")\nsns.barplot(x=class_dist.index, y=class_dist.Count, \n           palette=\"Blues_d\")\nplt.title('Class Distributions')","8b8fd8ec":"# A count of each class of sentiment in the train data set\nclass_dist","c41fb158":"# Extract tweets from dataframe & creating a new dataframe with only tweets\nraw_tweets = train_df['message']\n\n# View top 5 rows of raw_tweets\nraw_tweets[3010:3015]","7937c314":"# Replace email addresses with 'email'\ntweets = raw_tweets.str.replace(r'^.+@[^\\.].*\\.[a-z]{2,}$',\n                                 'emailaddress')\n\n# Remove unicode strings\ntweets = tweets.str.replace(r'(\\\\u[0-9A-Fa-f]+)', '')\ntweets = tweets.str.replace(r'[^\\x00-\\x7f]', '')\n\n# Convert any url to URL\ntweets = tweets.str.replace('((www\\.[^\\s]+)|(https?:\/\/[^\\s]+))',\n                            'website')\n\n# Remove special characters, numbers, punctuation\ntweets = tweets.str.replace('[^a-zA-Z#@]+',' ')\n\n# Remove additional white spaces\ntweets = tweets.str.replace(r'\\s+', ' ')\ntweets = tweets.str.replace(r'^\\s+|\\s+?$', '')\n\n# Change words to lower case\ntweets = tweets.str.lower()\n\n# Remove stop words from tweets\nstop_words = set(stopwords.words('english'))\n\ntweets = tweets.apply(lambda x: ' '.join(\n    word for word in x.split() if word not in stop_words))","2947e618":"# Viewing the changed message column\ntweets[:5]","d851c53b":"# Creating a dataframe from the cleaned up message column\ntweets_df = pd.DataFrame(tweets)\n\n# Add sentiment column to the tweets dataframe\ntweets_df['sentiment'] = train_df['sentiment']\n\n# View the top 5 rows of tweets\ntweets_df.head()","a8e18492":"# Create a Corpus for every class sentiment \nnews_tweets = ' '.join([text for text in tweets_df['message']\n                        [tweets_df['sentiment'] == 2]])\npro_tweets = ' '.join([text for text in tweets_df['message']\n                       [tweets_df['sentiment'] == 1]])\nneutral_tweets = ' '.join([text for text in tweets_df['message']\n                           [tweets_df['sentiment'] == 0]])\nanti_tweets = ' '.join([text for text in tweets_df['message']\n                        [tweets_df['sentiment'] == -1]])","346dfd1b":"# Visualising each sentiment class according to the count of words\nfull_title = ['Most Popular words for News tweets',\n              'Most popular words for Pro tweets',\n              'Most popular words for Neutral tweets',\n              'Most popular words for Anti tweets']\n\ntweet_list = [news_tweets, pro_tweets,\n              neutral_tweets, anti_tweets]\n\nplt.rcParams['figure.figsize'] = [50, 5]\n\nfor i, sent in enumerate(tweet_list):\n    plt.subplot(1, 4, i + 1)\n    freq_dist = nltk.FreqDist(sent.split(' '))\n    df = pd.DataFrame({'Word': list(freq_dist.keys()),\n                      'Count' : list(freq_dist.values())})\n\n    df = df.nlargest(columns='Count', n=15)\n\n    ax = sns.barplot(data=df, y='Word', x='Count')\n    plt.title(full_title[i])\n    plt.show()","888c8b0e":"# Create word clouds of the most common words in each sentiment class\nwc = WordCloud(width=800, height=500, \n               background_color='white', colormap='Dark2',\n               max_font_size=150, random_state=42)\n\nplt.rcParams['figure.figsize'] = [20, 15]\n\n# Create subplots \nfor i in range(0, len(tweet_list)):\n    wc.generate(tweet_list[1])\n    \n    plt.subplot(2, 2, i + 1)\n    plt.imshow(wc, interpolation='bilinear')\n    plt.axis(\"off\")\n    plt.title(full_title[i])\n    \nplt.show()","ae28ca6c":"# Creating a function to extract handles from tweets\ndef extract_handles(x):\n    \"\"\" This function finds handles in a \n        tweet and returns them as a list\"\"\"\n    handles = []\n    for i in x:\n        h = re.findall(r'@(\\w+)', i)\n        handles.append(h)\n        \n    return handles","99a3708e":"# Extracting handles from tweets\nnews_h = extract_handles(tweets_df['message']\n                              [tweets_df['sentiment'] == 2])\npro_h = extract_handles(tweets_df['message']\n                          [tweets_df['sentiment'] == 1])\nneutral_h = extract_handles(tweets_df['message']\n                              [tweets_df['sentiment'] == 0])\nanti_h = extract_handles(tweets_df['message']\n                          [tweets_df['sentiment'] == -1])\n\n# Unnesting list\nhandles = [sum(news_h, []), sum(pro_h, []), sum(neutral_h, []),\n           sum(anti_h, [])]\n\n# Visualising the Handles\nfull_title = ['Impact of Handles on the News sentiment',\n              'Impact of Handles on the Pro sentiment',\n              'Impact of Handles on the Neutral sentiment',\n              'Impact of Handles on the Anti sentiment']\n\nplt.rcParams['figure.figsize'] = [50, 5]\n\nfor i, sent in enumerate(handles):\n    plt.subplot(1, 4, i + 1)\n    freq_dist = nltk.FreqDist(sent)\n    df = pd.DataFrame({'Handle': list(freq_dist.keys()),\n                      'Count' : list(freq_dist.values())})\n\n    df = df.nlargest(columns='Count', n=15)\n\n    ax = sns.barplot(data=df, y='Handle', x='Count')\n    plt.title(full_title[i])\n    plt.show()","c60d8992":"# Creating a function to extract hashtags from tweets\ndef extract_hashtags(x):\n    \"\"\" The following function finds hashtags on a \n        tweet and returns them as a list\"\"\"\n    hashtags = []\n    for i in x:\n        ht = re.findall(r'#(\\w+)', i)\n        hashtags.append(ht)\n        \n    return hashtags","6e612554":"# Extracting hashtags from tweets\nnews_ht = extract_hashtags(tweets_df['message']\n                              [tweets_df['sentiment'] == 2])\npro_ht = extract_hashtags(tweets_df['message']\n                          [tweets_df['sentiment'] == 1])\nneutral_ht = extract_hashtags(tweets_df['message']\n                              [tweets_df['sentiment'] == 0])\nanti_ht = extract_hashtags(tweets_df['message']\n                          [tweets_df['sentiment'] == -1])\n\n# Unnesting list\nhashtags = [sum(news_ht, []), sum(pro_ht, []),\n            sum(neutral_ht, []),sum(anti_ht, [])]\n\n# Visualising the Hashtags\nfull_title = ['Impact of Hashtags on the News sentiment',\n              'Impact of Hashtags on the Pro sentiment',\n              'Impact of Hashtags on the Neutral sentiment',\n              'Impact of Hashtags on the Anti sentiment']\n\nplt.rcParams['figure.figsize'] = [50, 5]\n\nfor i, sent in enumerate(hashtags):\n    plt.subplot(1, 4, i + 1)\n    freq_dist = nltk.FreqDist(sent)\n    df = pd.DataFrame({'Hashtag': list(freq_dist.keys()),\n                      'Count' : list(freq_dist.values())})\n\n    df = df.nlargest(columns='Count', n=15)\n\n    ax = sns.barplot(data=df, y='Hashtag', x='Count')\n    plt.title(full_title[i])\n    plt.show()","b4a1c595":"# Function to extract hashtags - not based on Sentiment\ndef extract_hashtags(tweet):\n    '''Provides a tweet and extract hashtags from it'''\n    hashtags_only = []\n    if len(re.findall(\"(#[^#\\s]+)\", tweet)) > 0:\n        hashtags_only.append(re.findall(\"(#[^#\\s]+)\",\n                                        tweet))\n    else:\n        hashtags_only.append([\"0\"])\n    return hashtags_only[0]","cea307f4":"# Creating a dataframe to store the hashtags & visualise them\nhashtag_df = train_df.copy()\nhashtag_df['tweet_hashtags'] = hashtag_df['message'].map(extract_hashtags)","4bee08cf":"# Now we will extract the hashtags to a list\nall_hashtags = hashtag_df['tweet_hashtags'].tolist()\n\n# Next we observe that our all_hashtags is a list of lists...lets change that\ncleaned_hashtags = []\nfor i in all_hashtags:\n    for j in i:\n            cleaned_hashtags.append(j)\n\n# Convert cleaned_hashtags to a series and count the most frequent occuring\ncleaned_hashtag_series = pd.Series(cleaned_hashtags)\nhashtag_counts = cleaned_hashtag_series.value_counts()","742e6406":"# Changing the series to an ndarray\nhashes = cleaned_hashtag_series.values\nhashes = hashes.tolist()\n\n# Convert list to one string with all the words\nhashes_words = \" \".join(hashes)\n\n# Generate the wordcloud. The max_words argument controls the number of words on the cloud\nwordcloud = WordCloud(width=1000, height=300,relative_scaling=1.0\n                      ,background_color='white', colormap='Dark2',\n                      max_words=100).generate(hashes_words)\n\nplt.figure(figsize=(20,10))\nplt.imshow(wordcloud)\nplt.axis(\"off\")\nplt.tight_layout(pad=0)\nplt.show()","3e13ba07":"# Select all duplicate rows based on the message column\nmessage = tweets_df[tweets_df.\n                    duplicated(['message',\n                                'sentiment'],\n                               keep=False)]\n\n# View top 5 duplicate rows\nmessage.head()","6ea248c5":"# Create class distribution dataframe for the duplicate tweets\nclass_dist = pd.DataFrame(list(message['sentiment'].value_counts()), \n                 index=['Pro', 'News', 'Neutral', 'Anti'],\n                          columns=['Count'])\n\n# Plot class distribution\nclass_dis_figure = plt.figure(figsize=(10,8))\nsns.set(style=\"whitegrid\")\nsns.barplot(x=class_dist.index, y=class_dist.Count, \n           palette=\"Blues_d\",)\nplt.title('Class Distributions for Duplicate tweets')","56f02c37":"# View a count of class distribution for duplicate tweets\nclass_dist","0a8be247":"# Merging both the data sets\ncombine_df = train_df.append(test_df, \n                             ignore_index=True,\n                             sort=False)\ncombine_df.head(3)","009a7e1c":"# Combine (Merged) Data Set Information\nprint('Combine Data Set Info - Total Rows | Total Columns | Total Null Values')\nprint(combine_df.info())","2bab8a4d":"html_parser = HTMLParser()\n# Created a new columns i.e. clean_tweet contains the same tweets but cleaned version\ncombine_df['clean_message'] = combine_df['message'].apply(lambda x: html_parser.unescape(x))\ncombine_df.head(3)","a84aa8af":"combine_df['clean_message'] = combine_df['clean_message'].apply(lambda x: x.lower())\ncombine_df.head(3)","3bedcd4b":"# Creating token for the clean tweets\ncombine_df['tweet_token'] = combine_df['clean_message'].apply(lambda x: word_tokenize(x))\n\n## Fully formated tweets & there tokens\ncombine_df.head(3)","b65dbac3":"# Importing stop words from NLTK corpus for english language\nstop_words = set(stopwords.words('english'))","76353554":"# Created new columns of tokens without stop words\ncombine_df['tweet_token_filtered'] = combine_df['tweet_token'\n                                               ].apply(lambda x:[word for word in x if not word in stop_words])\n\n# Tokenized columns with stop words and without stop words\ncombine_df[['tweet_token', 'tweet_token_filtered']].head(3)","11e3a3e5":"# Created one more columns tweet_stemmed\nstemming = PorterStemmer()\ncombine_df['tweet_stemmed'] = combine_df['tweet_token_filtered'].apply(lambda x: ' '.join([stemming.stem(i) for i in x]))\ncombine_df.head(3)","1f8554c7":"# Splitting the data into train & test\ntrain_stemmed = combine_df[:15819]\ntest_stemmed = combine_df[15819:].drop(['sentiment'], axis=1)","9f5f4c18":"# Setting the parameters for the Vectorizer\n\nvectorizer = CountVectorizer(analyzer = 'word', \n                             tokenizer = None, \n                             preprocessor = None, \n                             stop_words = None, \n                             max_features = 180000,\n                             min_df = 1,\n                             ngram_range = (1,2)\n                            )","59253967":"# Building a Pipeline for word vectorization\npipe = Pipeline( [('vect', vectorizer)] )","fb989322":"# Pickle pipeline for Streamlit App\n# Save the Linear SVC model to disk\nfilename = 'pipeline.pickle'\npickle.dump(pipe, open(filename, 'wb'))","9aae181f":"# Fitting & transforming the data\ntrain_vect = pipe.fit_transform(train_stemmed['tweet_stemmed'])\ntest_vect = pipe.transform(test_stemmed['tweet_stemmed'])","4b7c8a37":"# Inspecting the shape of our vectorized data\nprint('train dim:', train_vect.shape, 'test dim:', test_vect.shape)","faee572e":"# Splitting the data into train & test sets\nX_train, X_test, y_train, y_test = train_test_split(train_vect, train_df['sentiment'],\n                                                    test_size = 0.2, random_state=42,\n                                                    stratify=train_df['sentiment'])","60a84573":"# These are the different Classification models we will train our data on\n# Creating a list of names so we can print metrics for the entire list at once\nnames = ['Logistic Regression'\n         ,'Nearest Neighbors'\n         ,'Linear SVC'\n         , 'RBF SVC'\n         , 'Linear SVM'\n         , 'Decision Tree'\n         , 'Random Forest'\n         ,  'AdaBoost'\n]","17c035b8":"# These are the different Classification models we will train our data on\n# Creating a list of names so we can print metrics for the entire list at once\nclassifiers = [\n    LogisticRegression()\n    , KNeighborsClassifier(3)\n    , SVC(kernel=\"linear\", C=0.025)\n    , SVC(gamma=2, C=1)\n    , LinearSVC()\n    , DecisionTreeClassifier(max_depth=5)\n    , RandomForestClassifier(max_depth=5, n_estimators=10, max_features=1)\n    , AdaBoostClassifier()\n]","1d9ebdbf":"# Training of models, scoring & appending to results list\nresults = []\n\n# Creating dictionaries to store evaluation results\nmodels = {}\nconfusion = {}\nclass_report = {}\n\n# This will calculate the total training time of each model\nfor name, clf in zip(names, classifiers):\n    print ('Fitting {:s} model...'.format(name))\n    run_time = %timeit -q -o clf.fit(X_train, y_train)\n\n# This will show us the status of the training\n    print ('... predicting')\n    y_pred = clf.predict(X_train)\n    y_pred_test = clf.predict(X_test)\n\n# This will show us the status of the scoring\n    print ('... scoring')\n    \n# These are our evaluation measures\n    accuracy  = metrics.accuracy_score(y_train, y_pred)\n    precision = metrics.precision_score(y_train, y_pred, average='weighted')\n    recall    = metrics.recall_score(y_train, y_pred, average='weighted')\n\n    f1        = metrics.f1_score(y_train, y_pred, average='weighted')\n    f1_test   = metrics.f1_score(y_test, y_pred_test, average='weighted')\n\n# Save the results to dictionaries\n    models[name] = clf\n    confusion[name] = metrics.confusion_matrix(y_train, y_pred)\n    class_report[name] = metrics.classification_report(y_train, y_pred)\n\n    results.append([name, accuracy, precision,\n                    recall, f1, f1_test, run_time.best])\n\n# Creating a dataframe of the results to view easily\nresults = pd.DataFrame(results, columns=['Classifier', 'Accuracy',\n                                         'Precision', 'Recall', 'F1 Train',\n                                         'F1 Test', 'Train Time'])\nresults.set_index('Classifier', inplace= True)","dc9a95fd":"# Viewing the results in a dataframe\nresults.sort_values('F1 Test', ascending=False)","d43ce2c0":"# Viewing the results on a bar chart\nfig, ax = plt.subplots(1, 2, figsize=(10, 5))\nresults.sort_values('F1 Train', ascending=False, inplace=True)\nresults.plot(y=['F1 Test'], kind='bar', ax=ax[0],\n             xlim=[0,1.1], ylim=[0.15,0.92])\nresults.plot(y='Train Time', kind='bar', ax=ax[1])","f23f639a":"# Setting the number of folds\n\nkfold = StratifiedKFold( n_splits = 150)","e930c7a2":"# Linear SVC model\n#Set the parameters according to the first run output (C = 0.09)\nparam_grid2 = {'C': [0.01, 0.03, 0.06, 0.09]}\n\n# Grid Search on the Linear SVC model\ngs_sv = GridSearchCV(LinearSVC(), param_grid = [param_grid2], \n                     verbose = 1, cv = kfold, n_jobs = -1)\n\n# Fitting the model to our data\ngs_sv.fit(X_train, y_train)\n\n# Find the best estimator\ngs_sv_best = gs_sv.best_estimator_\n\n# Print the best parameters\nprint(gs_sv.best_params_)\n\n# Print the best score\nprint(gs_sv.best_score_)","528e932d":"# Building Linear SVC model with best parameters\nlr_svc = LinearSVC(C=0.06)\n\n# Training the model on the whole training dataset\nlr_svc.fit(train_vect, train_df['sentiment'])\n\n# Print the best parameters\nprint(gs_sv.best_params_)\n\n# Print the best score\nprint(gs_sv.best_score_)","60e91411":"# Logistic Regression model\nlr = LogisticRegression(class_weight='balanced')\n'''The balanced class weight automatically balances data based on the label frequency'''\n\n#Set the parameters according to the first run output (C = 0.09)\nlr2_param = {\n    'multi_class':[('ovr')],\n    'penalty':['l2'],\n    'dual':[False],\n    'C':[5]   \n    }\n\n# Grid Search on the Logistic Regression model\nlr_CV = GridSearchCV(lr, param_grid=[lr2_param],\n                     cv=kfold, n_jobs=-1, verbose=1)\n\n#  Fitting the model to our data\nlr_CV.fit(train_vect, train_df['sentiment'])\n\n# Print the best parameters\nprint(lr_CV.best_params_)\n\n# Find the best estimator\nlogi_best = lr_CV.best_estimator_\n\n# Print the best score\nprint(lr_CV.best_score_)","4b3a1734":"# Building Logistic Regression model with best parameters\nlg_reg = LogisticRegression(class_weight='balanced', multi_class=('ovr'), \n                            penalty='l2', dual=False, C=5)\n\n# Training the model on the whole training dataset\nlg_reg.fit(train_vect, train_df['sentiment'])","a6a311b8":"# create the ensemble model\nestimators = {('linearSVC', lr_svc), ('logistic', lg_reg)}\nensemble = VotingClassifier(estimators)\nensemble.fit(train_vect, train_df['sentiment'])\n\n# Checking model perfomance\nprint('accuracy: {}'.format(metrics.accuracy_score(y_train, y_pred)))\nprint('precision: {}'.format(metrics.precision_score(y_train, y_pred, average='weighted')))\nprint('recall: {}'.format(metrics.recall_score(y_train, y_pred, average='weighted')))\nprint('f1_score: {}'.format(metrics.f1_score(y_train, y_pred, average='weighted')))","869f39b1":"# Dictionaries for the data logging\n#metrics = {'f1': f1,\n          #'recall': recall,\n          #'precision': precision}","98bad015":"# Log our parameters and results\n#experiment.log_metrics(metrics)","cc5c9305":"# Making predictions on the test data set\nsubmission1 = gs_sv.predict(test_vect)","eaf761df":"# Appending the prediction results to the test set, in a new column called 'sentiment'\noutput = pd.DataFrame( data = {'tweetid': test_df['tweetid'],\n                               'sentiment': submission1 })\n\n# Creating a csv file\noutput.to_csv('submission_final.csv', index = False, quoting = 3)","95dd4748":"# Save the Linear SVC model to disk\nfilename = 'lin_svc_model.pickle'\npickle.dump(gs_sv, open(filename, 'wb'))","ebe1c0c7":"# Save the Logistic Regression model to disk\nfilename = 'log_reg_model.pickle'\npickle.dump(lr_CV, open(filename, 'wb'))","5053bf4c":"# Save the Linear SVC model to disk\nfilename = 'lin_svc_model.pickle'\npickle.dump(gs_sv, open(filename, 'wb'))","7d7883b1":"Support Vector Machines which are supervised learning models with associated learning algorithms that analyze data used for classification and regression analysis. Given a set of training samples, each marked as belonging to one or the other of **two categories**, an SVM training algorithm builds a model that assigns new test samples to one category or the other, making it a non-probabilistic binary linear classifier. Reference [Wikipedia](https:\/\/en.wikipedia.org\/wiki\/Support_vector_machine).\n\nNote that the model generates a confidence score which is higher than Logistics Regression model.","802162be":"<a id=\"6.4\"><\/a>\n## 6.4 Word Tokenization:\nNLTK provides a function called word_tokenize() for splitting strings into tokens (nominally words). It splits tokens based on white space and punctuation. For example, commas and periods are taken as separate tokens.","10f40d38":"<a id=\"12\"><\/a>\n# 12. Exporting the different models\nPickle is the standard way of serializing objects in Python. You can use the pickle operation to serialize your machine learning algorithms and save the serialized format to a file. Later you can load this file to deserialize your model and use it to make new predictions.","6a8c6b38":"<a id=\"6.2\"><\/a>\n## 6.2 Converting Hypertext Markup Language(HTML) entities:\nAn HTMLParser instance is fed HTML data and calls handler methods when start tags, end tags, text, comments, and other markup elements are encountered. Removal of words in our data like '&amp', '&lt' (which are basically used in HTML).","31dcd61a":"<a id=\"1.2\"><\/a>\n## 1.2 **Problem Landscape:**\n\n###  **Data**:\n\n>    The collection of this data was funded by a Canada Foundation for Innovation JELF Grant to Chris Bauch, University of Waterloo. The dataset aggregates tweets pertaining to climate change collected between Apr 27, 2015 and Feb 21, 2018. In total, 43943 tweets were collected. Each tweet is labelled as one of the following classes:\n\n| Name | Description         \n| :-: | :-------------\n| 2 | **News:** the tweet link to factual news about climate change\n| 1 | **Pro:** the tweet supports the belief of man-made climate change\n| 0 | **Neutral:** the tweet neither supports nor refutes the belief of man-made climate change\n| -1 | **Anti:** the tweet does not believe in man-made climate change\n\n**Variable definitions:**\n\n- ***sentiment***: Sentiment of tweet\n\n- ***message***: Tweet body\n\n- ***tweetid***: Twitter unique id\n","f0d8dd08":"**Notes:**\n\n***Observations***:\n\n* The RBF & Adaboost models took long the longest to train but didn't perform well on the F1 test.\n\n* All other models were trained relatively fast in comparison to the RBF & Adaboost.\n\n* From the results table & bar chart, we can see the models that perform best are the Logistic Regression & Linear Support Vector Machines(SVM). We will now try & improve on these two models. ","1f991632":"## 5.4 **Exploring the impact of Hashtags on the sentiment classes:**\nPeople use the hashtag symbol (#) before a relevant keyword or phrase in their Tweet to categorize those Tweets and help them show more easily in Twitter search. Clicking or tapping on a hashtagged word in any message shows you other Tweets that include that hashtag. Hashtags can be included anywhere in a Tweet.","0fb4c0b3":"# Table of Contents\n\n* [1 Project Overview](#1)\n    - [1.1 Problem Statement](#1.1)\n    - [1.2 Problem Landscape](#1.2)\n* [2. Imports](#2)\n* [3. Importing the dataset](#3)\n* [4. Exploratory Data Analysis](#4)\n* [5 Distribution of target variable](#5)\n    - [5.1 Text Cleaning](#5.1)\n* [6 Data Preprocessing](#6)\n    - [6.1 Merge  datasets](#6.1)\n    - [6.2 Converting html entities](#6.2)\n    - [6.3 Changing Tweets into lowercase](#6.3)\n    - [6.4 Word Tokenization](#6.4)\n    - [6.5 Removing Stop Words](#6.5)\n    - [6.6 Word Stemming](#6.6)\n* [7 Feature Engineering](#7)\n    - [7.1 Split data into Train & Test sets](#7.1)\n    - [7.2 CountVectorizer](#7.2)\n    - [7.3 Building the pipeline](#7.3)\n* [8. Modeling](#8)\n    - [8.1 Train-test split](#8.1)\n    - [8.2 Model training](#8.2)\n    - [8.3 Tuning the chosen models](#8.3)\n* [9. Concluding Remarks](#9)\n* [10. Logging to Comet](#10)\n* [11. Generating a submission file](#11)\n* [12. Exporting the different models](#12)\n    \n\n","5c1736c9":"<a id=\"8\"><\/a>\n# 8. Modeling","d35f086d":"<a id=\"2\"><\/a>\n\n# **2. Imports**","0def3d01":"<a id=\"3\"><\/a>\n# **3. Importing the dataset**\nThe following is the imported Data for Training the Machine Learning Model\n1. Train.csv:  Dataset that contains all the variables that should be used to train the model\n\n2. Test.csv :  Dataset that contains variables that will be used to test the model","deba18ad":" ## 5.2 Visualising a corpus for each sentiment class:","703cf567":"<a id=\"11\"><\/a>\n# 11. Generating a submission file\nNow that best our model is tuned, we can make predictions on the unseen test data, generate a submission file & submit to Kaggle.","cf1bb9ed":"**Notes:**\n\n***Observations***:\n* The tuned model that performs better is the Logistic Regression, with a score of over 76%, however, the SVC model performs better on unseen data on Kaggle.\n\n***Insights***:\n* We found that increasing the number of kfolds gives us a greater accuracy, however, this also increases the processing time. \n\n* The difference between a kfold of 5 & 150 is between 2 & approximately only 0.4%.\n\n* One must keep processing time versus accuracy in mind when creating a model & weight up the trade off between processing time & accuracy.","c34f7e23":"**Notes:**\n\n***Observations***:\n* The most popular News handles are actual news broadcaster accounts.\n\n* The most popular Pro handles seem to be celebrity accounts & news accounts.\n\n* Trump features most for most popular Anti & Neutral tweets. \n\n***Insights***:\n* Celebrities have been raising climate change as an issue to increase public awareness.\n\n* Most popular trends handles are American, as most Twitter users reside in the United States of America, 64.2%. [Click here](http:\/\/statista.com\/statistics\/242606\/number-of-active-twitter-users-in-selected-countries)\n\n* It is interesting that Fox News, does not appear in the positive sentiment. The company is usually under scrutiny from the public as they are perceived to support Republican policies in the United States of America.","52b88bb1":"**Notes:**\n\n***Observations***:\n* The most popular \n\n***Insights***:\n* ","1128a21b":"## Logistic Regression Model","627e0206":"<a id=\"8.2\"><\/a>\n## 8.2 Model training:","1108160b":" <a id=\"5.1\"><\/a>\n## **5.1 Text Cleaning:**","e51085a9":"Now we are ready to train a model and predict the required solution. There are 60+ predictive modelling algorithms to choose from. We must understand the type of problem and solution requirement to narrow down to a select few models which we can evaluate. Our problem is a classification and regression problem. We want to identify relationship between output (sentiment positive or negative) with other variables. We are also perfoming a category of machine learning which is called supervised learning as we are training our model with a given dataset. With these two criteria - Supervised Learning plus Classification and Regression, we can narrow down our choice of models to a few. These include:\n\n- Logistic Regression\n- Nearest Neighbors\n- Linear SVC\n- RBF SVC\n- Linear SVM\n- Decision Tree\n- Random Forrest\n- AdaBoost\n- XGBoost\n","36449f14":"<a id=\"8.3\"><\/a>\n## 8.3 Tuning the chosen models","50cfd936":"PorterStemmer uses Suffix Stripping to produce stems. Notice how the PorterStemmer is giving the root (stem) of the word \"researchers\" by simply removing the 'ers' after researchers. This is a suffix added to research to make it plural. But if you look at 'evidence'it is stemmed to 'evid' because **PorterStemmer algorithm does not follow linguistics rather a set of 05 rules for different cases that are applied in phases (step by step) to generate stems**. This is the reason why PorterStemmer does not often generate stems that are actual English words. It does not keep a lookup table for actual stems of the word but applies algorithmic rules to generate stems.","0fd7d797":"## Linear SVC Model","065b68d4":"<a id=\"10\"><\/a>\n# 10. Logging to Comet \nThe following lines of code logs our experiment to Comet, for efficiency and smooth running of the notebook we have commented it out since the experiment was logged on Jupyter notebook.","75c9ce41":"### Using the Regex library to clean the raw tweets dataframe:\nRegular expressions (called REs, or regexes, or regex patterns) are essentially a tiny, highly specialized programming language embedded inside Python and made available through the re module. Using this little language, you specify the rules for the set of possible strings that you want to match; this set might contain English sentences, or e-mail addresses, or TeX commands, or anything you like. You can then ask questions such as \u201cDoes this string match the pattern?\u201d, or \u201cIs there a match for the pattern anywhere in this string?\u201d. You can also use REs to modify a string or to split it apart in various ways. For more information on regular expressions, [Click here](http:\/\/docs.python.org\/3\/howto\/regex.html).","4151c814":"<a id=\"9\"><\/a>\n# 9. Concluding Remarks\nOver the years,the use of social media platforms  such as Twitter has played a important role in  capturing public opinion for various topics which is vital for different organisations. Classifying the sentiments of tweets which pertains to climate change sheds light to different topics and issues associated with climate change. It also allows one to get a grasp of the main concerns of the public. Organisations can gain powerful insights for marketing purposes by using social media & sentiment analysis.","baea66a3":"**Notes:**\n\n***Observations***:\n* The most popular words in all 4 classes are Climate Change, Global Warming.\n\n***Insights***:\n* There is some unnecessary data. The words http, https, website, co & RT are prominent, but will not assist us in our classification.\n\n\n* In fact, the top 5 words in all classes are the same, except for the News class, in which the word 'Trump' features prominently.","cd61dc03":"<a id=\"7.3\"><\/a>\n## 7.3 Building the pipeline:\nIt's always a good practice to make a pipeline of transformation for your data, it will make the process of data transformation really easy and reusable. We will implement a pipeline for transforming our tweets to something that our ML models can digest (vectors).","73fffbed":"<a id=\"6.5\"><\/a>\n## 6.5 Removing Stop Words:\nWhat we can see is that stop words are the most used, but in fact they don't help us determine if a tweet is happy\/sad, however, they are consuming memory and they are making the learning process slower, so we really need to get rid of them.","bc234d68":"## 5.5 **Duplicate Tweets:**\nTwitter allows a user to retweet, or RT another users tweets. We see RT as a popular word in the above visuals. This is great for creating trends, but not useful for sentiment analysis. Now we will remove the duplicates to get a clearer picture of our data set.","6851c048":"<a id=\"7\"><\/a>\n# 7. Feature Engineering","5baf4b43":"#### Now we will clean up the Hashtags, to get a clearer picture of the most prominent ones","16accf30":"**Notes:**\n\n* There are no missing values in the data.\n\n* Class distribution is skewed to the 'Pro' sentiment.\n\n* The sum of tweets relating to News, Neutral & Anti is less than half of the total tweets.","f303a5f6":"<a id=\"4\"><\/a>\n\n# **4. Exploratory Data Analysis**\nThe following section provides an overview of the given data. We looks at  some key inights of the raw data where we look at sentiment distribution of the data. This has been illustrated using graphs.  We also visualise word frequency of messages of the unprocessed train and test data.","626541be":"In order to visualise the tweets we have to clean the data.","2f45f3b0":"## Importing Comet\nThe following code imports the notebook to the Comet as an experiment.","58b83c80":"<a id=\"1\"><\/a>\n##  Introduction:\nMany companies are built around lessening one\u2019s environmental impact or carbon footprint. They offer products and services that are environmentally friendly and sustainable, in line with their values and ideals. They would like to determine how people perceive climate change and whether or not they believe it is a real threat. This would add to their market research efforts in gauging how their product\/service may be received.\n\nOur goal within this notebook is creating a Machine Learning model that is able to classify whether or not a person believes in climate change, based on their novel tweet data.\n\nProviding an accurate and robust solution to this task gives companies access to a broad base of consumer sentiment, spanning multiple demographic and geographic categories - thus increasing their insights and informing future marketing strategies.","65709516":"Now we will create a corpus for each sentiment class. In linguistics, a corpus is a large and structured set of texts. In corpus linguistics, they are used to do statistical analysis and hypothesis testing, checking occurrences or validating linguistic rules within a specific language territory.","87196955":"#### Now that our data preprocessing is complete, let's prepare it for machine learning.","1b267d66":"#### To improve our models, we will use Cross-validation & Grid Search methods\n\n* Cross-validation is a resampling procedure used to evaluate machine learning models on a limited data sample. The procedure has a single parameter called k that refers to the number of groups that a given data sample is to be split into. As such, the procedure is often called k-fold cross-validation. When a specific value for k is chosen, it may be used in place of k in the reference to the model, such as k=10 becoming 10-fold cross-validation. Cross-validation is primarily used in applied machine learning to estimate the skill of a machine learning model on unseen data. That is, to use a limited sample in order to estimate how the model is expected to perform in general when used to make predictions on data not used during the training of the model.\n\n\n* Grid-searching is the process of scanning the data to configure optimal parameters for a given model. Grid-Search will build a model on each parameter combination possible.","56df6827":"# 1. Project Overview\n## Predict an individual\u2019s belief in climate change based on historical tweet data\n### Explore Data Science Academy - Team 2 - Classifiers","480bdcd7":"<a id=\"6.1\"><\/a>\n## 6.1 Merge the train & test datasets:\nThis will allow us to preprocess the train & test data sets simultaneously.","5a2a0afb":"Next we model using Logistic Regression which is a useful model to run early in the workflow. Logistic regression measures the relationship between the categorical dependent variable (feature) and one or more independent variables (features) by estimating probabilities using a logistic function, which is the cumulative logistic distribution. Reference [Wikipedia](https:\/\/en.wikipedia.org\/wiki\/Logistic_regression).\n","e1a7a895":"Our Linear SVC model gives us a score of around 75%. Let's check the score of the Logistic Regression model.","011e29ba":"<a id=\"6\"><\/a>\n# 6 Data Preprocessing","3f39a853":"<a id=\"7.1\"><\/a>\n## **7.1 Split data into Train & Test sets:**\nNow that our Preprocessing is complete, we can separate the data back into the train & test sets.","be6e14c1":"**Notes:**\n\n***Observations***:\n* We can see the top hashtags more clearly now. \n\n***Insights***:\n* The top 3 hashtags are: \n    1. climate\n    2. climatechange\n    3. BeforeTheFlood\n\n\n\n* The hashtags also give us an indication of the time the data was gathered, i.e. the documentary called 'Before the Flood', where actor Leonardo DiCaprio meets with scientists, activists and world leaders to discuss the dangers of climate change and possible solutions was released in October 2016.","9b397f58":"<a id=\"1.1\"><\/a>\n## **1.1 Problem Statement:**\nBuild a Natural Language Processing model to classify whether or not a person believes in climate change, based on their novel tweet data.","7213d754":"We can now visualise the duplicate tweets:","6d7bf4db":"<a id=\"5\"><\/a>\n# 5. Distribution of the Target variable","c7500243":"## Combine Model Predictions Into Ensemble Predictions\nUsing a Voting classifier simply means building multiple models (typically of differing types) and simple statistics (like calculating the mean) are used to combine predictions. It works by first creating two or more standalone models from your training dataset. A Voting Classifier can then be used to wrap your models and average the predictions of the sub-models when asked to make predictions for new data.[click here](https:\/\/machinelearningmastery.com\/ensemble-machine-learning-algorithms-python-scikit-learn\/)","97c397d5":"<a id=\"6.6\"><\/a>\n## 6.6 Word Stemming:\nThere are words that have the same meaning, but written in a different manner, sometimes in the plural and sometimes with a suffix (ing, es ...), this will make our model think that they are different words and also make our vocabulary bigger (waste of memory and time for the learning process). The solution is to reduce those words with the same root, this is called stemming. Stemming refers to the removal of suffices, like \u201cing\u201d, \u201cly\u201d, \u201cs\u201d, etc. by a simple rule-based approach.","a55b9c2c":"<a id=\"8.1\"><\/a>\n## 8.1 Train-test split:\nWe split the data into train and test sets for for model prediction. To ensure  that the data is split correctly taking into account the imbalace of the classes, we will set the 'stratify' parameter of sklearn's train_test_split function. For more infomation [click here.](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.model_selection.train_test_split.html)","ba2c5095":"**Notes:**\n\nLooking at the top 5 rows of the data we can observe and make the following assumptions:\n\n***Observations***:\n\n* Some tweets contain twitter handles (e.g @RawStory), numbers (e.g year 2016), hashtags (e.g #TodayinMaker# WIRED) and re-tweets (RT).\n\n* Some tweets contain names of ogarnisations, continents and countries. \n\n* New lines are represented by '\\n' in the tweet string.\n\n***Assumptions***:\n\n* The tweets may contain URLs.\n\n* The tweets may contain percetages, money symbols and emoticons.","eb66a7d6":"<a id=\"7.2\"><\/a>\n## 7.2 CountVectorizer:\nThe CountVectorizer provides a simple way to both tokenize a collection of text documents and build a vocabulary of known words, but also to encode new documents using that vocabulary. An encoded vector is returned with a length of the entire vocabulary and an integer count for the number of times each word appeared in the document.","a238fa73":"<a id=\"6.3\"><\/a>\n## 6.3 Changing all the Tweets into lowercase:\nThe following changes all the messeges in the 'clean_message' column of the dataframe into lowercase. ","d5e42886":"**Notes:**\n\n***Observations***:\n* We can see that the top 5 hashtags are generally similar, although there seem to be words here that are irrelevant, eg. single letters like 'd', 'p' & words like 'maga' & 'tcot' ","c71c7178":"## 5.3 **Exploring the impact of Handles on the sentiment classes:**\nA Twitter handle is the username that appears at the end of your unique Twitter URL. Twitter handles appear after the @ sign in your profile URL and it must be unique to your account. A Twitter name, on the other hand, is simply there to help people find the company they're looking for."}}