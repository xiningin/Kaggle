{"cell_type":{"7fbe4e58":"code","d74929fd":"code","5b67180a":"code","2e8fdb4d":"code","005579b7":"code","8ace8f51":"code","6882d7b3":"code","5dc5f4cd":"code","a3d3e1de":"code","bb6e3090":"code","a4471fe9":"code","d1c6a859":"code","d9045f3d":"markdown","faef182b":"markdown","5dd314c0":"markdown","43abb707":"markdown","6538fbab":"markdown","eec8c18c":"markdown","777ffef0":"markdown","156aa45d":"markdown"},"source":{"7fbe4e58":"import numpy as np\nimport pandas as pd\nimport random","d74929fd":"comp1=pd.read_csv('..\/input\/jigsaw-multilingual-toxic-comment-classification\/jigsaw-toxic-comment-train.csv')","5b67180a":"comp1[['id','comment_text','toxic','severe_toxic']].head()","2e8fdb4d":"comp2=pd.read_csv('..\/input\/jigsaw-multilingual-toxic-comment-classification\/jigsaw-unintended-bias-train.csv')","005579b7":"comp2[['id','comment_text','toxic','severe_toxicity']].head()","8ace8f51":"print(comp1.shape)\nprint(comp2.shape)","6882d7b3":"comp1.toxic.value_counts()","5dc5f4cd":"comp1.severe_toxic.value_counts()","a3d3e1de":"less_toxic=[]\nmore_toxic=[]\nnon_toxic=comp1[comp1.toxic==0].reset_index(drop=True)\ntoxic=comp1[comp1.toxic==1].reset_index(drop=True)\nfor i in range(len(non_toxic)):\n    less_toxic.append(non_toxic.loc[i,'comment_text'])\n    j=random.randint(0,len(toxic)-1)\n    more_toxic.append(toxic.loc[j,'comment_text'])","bb6e3090":"train=pd.DataFrame()\ntrain['less_toxic']=less_toxic\ntrain['more_toxic']=more_toxic","a4471fe9":"train.head()","d1c6a859":"train.to_csv('train.csv',index=False)","d9045f3d":"# Idea","faef182b":"**Evaluation**\n\nSubmissions are evaluated on Average Agreement with Annotators. For the ground truth, annotators were shown two comments and asked to identify which of the two was more toxic. Pairs of comments can be, and often are, rated by more than one annotator, and may have been ordered differently by different annotators.\n\nFor each of the approximately 200,000 pair ratings in the ground truth test data, we use your predicted toxicity score to rank the comment pair. The pair receives a 1 if this ranking matches the annotator ranking, or 0 if it does not match.\n\nThe final score is the average across all the pair evaluations.","5dd314c0":"**Training data**\n\nNote, there is no training data for this competition. You can refer to previous Jigsaw competitions for data that might be useful to train models. But note that the task of previous competitions has been to predict the probability that a comment was toxic, rather than the degree or severity of a comment's toxicity.\n\n* Toxic Comment Classification Challenge\n* Jigsaw Unintended Bias in Toxicity Classification\n* Jigsaw Multilingual Toxic Comment Classification\n\nWhile we don't include training data, we do provide a set of paired toxicity rankings that can be used to validate models.","43abb707":"**Objective**\n\nIn this competition you will be ranking comments in order of severity of toxicity. You are given a list of comments, and each comment should be scored according to their relative toxicity. Comments with a higher degree of toxicity should receive a higher numerical value compared to comments with a lower degree of toxicity.","6538fbab":"**As we know there is no training dataset provided for this competition, this notebook shows you how to use previous Jigsaw competitions datasets to create a dataset for this competition. Don't just use the output file since what I've done at the end is just a basic idea. One must understand the idea so that you can create a dataset on your own depending on how many times particular comment can be repeated.**","eec8c18c":"* For now let's just consider the dataset from the very first Jigsaw competition and see how we can use that to create a train dataset for the current competition.","777ffef0":"# Overview","156aa45d":"* We can create a dataset to train our models in this competition in the format of validation_data.csv\n* There are many ways one can create a training dataset by considering the non toxic comments i.e. toxicity==0 as less toxic ones and toxic comments i.e. toxicity==1 as more toxic ones. \n* And we can also use the severe_toxic feature to compare comments whose toxicity==1 and divide the ones whose severe_toxicity==0 as less_toxic and ones whose severe_toxity==1 as more_toxic. All of this only for the dataset from the first Jigsaw competition. We have the 2nd competition dataset which is almost 10 times that of first.\n* If we consider all the possibilities we can create a dataset of size ****(idk, you calculate).\n* So for now I've created it this way. For each non toxic comment(202165 of those) I've chosen a toxic comment(from 21384 comments) randomly. The length of train.csv will be 202165\n"}}