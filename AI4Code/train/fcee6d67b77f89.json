{"cell_type":{"86c8c24c":"code","7be08ad0":"code","d5832a4b":"code","de7ca44f":"code","584bdf20":"code","8cca8c83":"code","e7435b84":"code","ff19f0d3":"code","bac5ad3c":"code","bbeec338":"code","ac13e632":"code","393ab4c8":"code","d3033cfa":"code","571c5d80":"code","1ea1c8e0":"code","83af2d75":"code","d2baf709":"code","2fa8699c":"code","c6b07383":"code","7e2cbad2":"code","76f0176d":"code","6b443391":"code","b0bd8ccc":"code","71277317":"code","e5cf8840":"code","bde44739":"code","10c8c96b":"code","f808c8ed":"code","a70829e8":"code","5b8a6676":"code","b36c93f7":"code","d338bef9":"code","39bfde41":"code","d618f1de":"code","0758648b":"code","d7d33897":"code","14f28386":"code","877489d5":"code","1332f527":"code","c8138139":"code","d7fa1604":"code","946b9c6f":"code","17cdc6ae":"code","6c08fd97":"code","b269cd4f":"code","4849e4d3":"code","1c6509ea":"code","850429c7":"code","8bec4775":"code","1573b585":"code","4a083ac3":"code","c7b4474b":"code","2a3d13d8":"code","5b09d6d5":"code","f9ba043b":"code","9daefc66":"code","de234ac5":"code","c3e59b3d":"code","83ce9a10":"code","68391a63":"code","a7e6877c":"markdown","52869455":"markdown","a979497d":"markdown","ab61e618":"markdown","7e8fffd7":"markdown","3c7aabbd":"markdown","ce10c2d4":"markdown","2607d143":"markdown","6309a4ec":"markdown","867c36b5":"markdown","3441a4f8":"markdown","89a4924d":"markdown","ff2700aa":"markdown","07636dbb":"markdown","8c8f5c5f":"markdown","028494ad":"markdown","820b12ff":"markdown","6af796ae":"markdown","5f7253b4":"markdown","cae79fa8":"markdown","b05f842e":"markdown","9ca96258":"markdown","6729f00f":"markdown","d68b6267":"markdown","706392e1":"markdown","12a33eff":"markdown","c4022720":"markdown","a05f59b3":"markdown","97b880dd":"markdown","ffc42942":"markdown","4853fdd0":"markdown","d5ff8886":"markdown","409058da":"markdown","81c404b9":"markdown","a77b6967":"markdown","e01a672a":"markdown","71e117c9":"markdown"},"source":{"86c8c24c":"import numpy as np\nimport pandas as pd\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nimport tensorflow as tf\n\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.layers.experimental import preprocessing","7be08ad0":"# URL for getting the dataset\nurl_link = 'http:\/\/archive.ics.uci.edu\/ml\/machine-learning-databases\/auto-mpg\/auto-mpg.data'\ncol_names = ['MPG', 'Cylinders', 'Displacement', 'Horsepower', 'Weight',\n                'Acceleration', 'Model Year', 'Origin']\n\ndata = pd.read_csv(url_link, names=col_names,\n                          na_values='?', comment='\\t',\n                          sep=' ', skipinitialspace=True)\n","d5832a4b":"dataset = data.copy()\ndataset.head()","de7ca44f":"# check for null values \ndataset.isnull().sum()","584bdf20":"dataset['Horsepower'].describe","8cca8c83":"dataset.shape","e7435b84":"# Let us drop the null values from dataset \ndataset = dataset.dropna()","ff19f0d3":"dataset.shape","bac5ad3c":"dataset.info()","bbeec338":"dataset['Origin'].unique()","ac13e632":"dataset['Origin'] = dataset['Origin'].map({1: 'USA', 2: 'Europe', 3: 'Japan'})","393ab4c8":"dataset = pd.get_dummies(dataset, columns=['Origin'], prefix='', prefix_sep='')\ndataset.head()","d3033cfa":"train_data = dataset.sample(frac=0.8, random_state=0)\ntest_data = dataset.drop(train_data.index)","571c5d80":"train_data.shape,test_data.shape","1ea1c8e0":"sns.pairplot(train_data, diag_kind='kde')","83af2d75":"plt.figure(figsize=(12,10))\nsns.heatmap(train_data.corr(),annot=True)","d2baf709":"train_data.describe().T","2fa8699c":"train_features = train_data.copy()\ntest_features = test_data.copy()\n\ntrain_target = train_features.pop('MPG')\ntest_target = test_features.pop('MPG')\n","c6b07383":"train_data.describe().transpose()[['mean', 'std']]","7e2cbad2":"normalize = preprocessing.Normalization()","76f0176d":"normalize.adapt(np.array(train_features))\n# This calculates the mean and variance, and stores them in the layer.","6b443391":"print(normalize.mean.numpy())","b0bd8ccc":"# Following gives an idea on how the normalization works \nfirst = np.array(train_features[:1])\n\nwith np.printoptions(precision=2, suppress=True):\n  print('First example:', first)\n  print()\n  print('Normalized:', normalize(first).numpy())","71277317":"hp = np.array(train_features['Horsepower'])\n\nhp_normalize = preprocessing.Normalization(input_shape=[1,])\nhp_normalize.adapt(hp)","e5cf8840":"model_hp = tf.keras.Sequential([\n    hp_normalize,\n    layers.Dense(units=1)\n])\nmodel_hp.summary()","bde44739":"model_hp.predict(hp[:10])","10c8c96b":"model_hp.compile(\n    optimizer=tf.optimizers.Adam(learning_rate=0.1),\n    loss='mean_absolute_error')","f808c8ed":"history = model_hp.fit(\n    train_features['Horsepower'], train_target,\n    epochs=100,\n    # suppress logging\n    verbose=0,\n    # Calculate validation results on 20% of the training data\n    validation_split = 0.2)","a70829e8":"hist = pd.DataFrame(history.history)\nhist['epoch'] = history.epoch\nhist.head()","5b8a6676":"def plot_loss(history):\n  plt.plot(history.history['loss'], label='loss')\n  plt.plot(history.history['val_loss'], label='val_loss')\n  plt.ylim([0, 10])\n  plt.xlabel('Epoch')\n  plt.ylabel('Error [MPG]')\n  plt.legend()\n  plt.grid(True)","b36c93f7":"plot_loss(history)","d338bef9":"test_results = {}\n\ntest_results['Linear Model - Single Input Variable Horsepower'] = model_hp.evaluate(\n    test_features['Horsepower'],\n    test_target, verbose=0)","39bfde41":"x = tf.linspace(0.0, 250, 251)\ny = model_hp.predict(x)","d618f1de":"def plot_horsepower(x, y):\n  plt.scatter(train_features['Horsepower'], train_target, label='Data')\n  plt.plot(x, y, color='k', label='Predictions')\n  plt.xlabel('Horsepower')\n  plt.ylabel('MPG')\n  plt.legend()","0758648b":"plot_horsepower(x,y)","d7d33897":"model_linear = tf.keras.Sequential([\n    normalize,\n    layers.Dense(units=1)\n])","14f28386":"model_linear.predict(train_features[:10])","877489d5":"model_linear.layers[1].kernel","1332f527":"model_linear.compile(\n    optimizer=tf.optimizers.Adam(learning_rate=0.1),\n    loss='mean_absolute_error')","c8138139":"history = model_linear.fit(\n    train_features, train_target, \n    epochs=100,\n    # suppress logging\n    verbose=0,\n    # Calculate validation results on 20% of the training data\n    validation_split = 0.2)","d7fa1604":"plot_loss(history)","946b9c6f":"test_results['Linear Model - all Input Variables'] = model_linear.evaluate(\n    test_features, test_target, verbose=0)","17cdc6ae":"def dnn_b_c_model(norm):\n  model = keras.Sequential([\n      norm,\n      layers.Dense(64, activation='relu'),\n      layers.Dense(64, activation='relu'),\n      layers.Dense(1)\n  ])\n\n  model.compile(loss='mean_absolute_error',\n                optimizer=tf.keras.optimizers.Adam(0.001))\n  return model","6c08fd97":"model_dnn_hp = dnn_b_c_model(hp_normalize)","b269cd4f":"model_dnn_hp.summary()","4849e4d3":"history = model_dnn_hp.fit(\n    train_features['Horsepower'], train_target,\n    validation_split=0.2,\n    verbose=0, epochs=100)","1c6509ea":"plot_loss(history)","850429c7":"x = tf.linspace(0.0, 250, 251)\ny = model_dnn_hp.predict(x)","8bec4775":"plot_horsepower(x, y)","1573b585":"test_results['DNN Model - Single Input Variable Horsepower'] = model_dnn_hp.evaluate(\n    test_features['Horsepower'], test_target,\n    verbose=0)","4a083ac3":"model_dnn = dnn_b_c_model(normalize)\nmodel_dnn.summary()","c7b4474b":"history = model_dnn.fit(\n    train_features, train_target,\n    validation_split=0.2,\n    verbose=0, epochs=100)","2a3d13d8":"plot_loss(history)","5b09d6d5":"test_results['DNN Model - all Input Variables'] = model_dnn.evaluate(test_features, test_target, verbose=0)","f9ba043b":"pd.DataFrame(test_results, index=['Mean absolute error [MPG]']).T","9daefc66":"test_predictions = model_dnn.predict(test_features).flatten()\n\na = plt.axes(aspect='equal')\nplt.scatter(test_target, test_predictions)\nplt.xlabel('True Values [MPG]')\nplt.ylabel('Predictions [MPG]')\nlims = [0, 50]\nplt.xlim(lims)\nplt.ylim(lims)\n_ = plt.plot(lims, lims)","de234ac5":"error = test_predictions - test_target\nplt.hist(error, bins=25)\nplt.xlabel('Prediction Error [MPG]')\n_ = plt.ylabel('Count')","c3e59b3d":"model_dnn.save('model_dnn')","83ce9a10":"reloaded = tf.keras.models.load_model('model_dnn')\n\ntest_results['reloaded'] = reloaded.evaluate(\n    test_features, test_target, verbose=0)","68391a63":"pd.DataFrame(test_results, index=['Mean absolute error [MPG]']).T","a7e6877c":"### Step 1: DNN with one input\/variable Horsepower ","52869455":"Post normalize we can do a .adapt it to the data ","a979497d":"#### Origin column as per the description is a Categorical column and represents country of Origin. I will convert the data using one hot encoding ","ab61e618":"Training","7e8fffd7":"## We are all set to build a model. \n\n### Step 1:-  use a single input to predict MPG in this case we will take Horse power\n\nNormalize the input variable Horsepower","3c7aabbd":"It is good practice to normalize features that use different scales and ranges.\n\nOne reason this is important is because the features are multiplied by the model weights. So the scale of the outputs and the scale of the gradients are affected by the scale of the inputs.\n\nAlthough a model might converge without feature normalization, normalization makes training much more stable.\n\n** Credit Google **","ce10c2d4":"Let me do a split of the data between train and test \n\nI will use the test set for Model Evaluation","2607d143":"Stage 4 : fit the model to train the data ","6309a4ec":"The preprocessing.Normalization layer is a clean and simple way to build that preprocessing into your model.\nFirst Step is to create a layer","867c36b5":"Collect the results on the test set, for later:","3441a4f8":"If you plot the predictions as a function of Horsepower, you'll see how this model takes advantage of the nonlinearity provided by the hidden layers:","89a4924d":"Make predictions\n\nFinally, predict have a look at the errors made by the model when making predictions on the test set:","ff2700aa":"Stage 2 : Compile the model \n\nStage 3 : Optimize ","07636dbb":"Stage 1: Build Model ","8c8f5c5f":"Let us save the model save or later use:\nLet us try to re-load model it shows exactly same outputs","028494ad":"Amazing model!!! it's doing a good job, predicts reasonably well.\n\nLet us take a look at the error distribution:","820b12ff":"Collect the results on the test set:","6af796ae":"Using all the inputs achieves a much lower training and validation error than the horsepower model:\n\nCollect the results on the test set, for later:\n\n","5f7253b4":"## Regression - Predict Fuel Efficiency using DNN with classic AUTO MPG dataset\n\nIts a regression use case of prediction of contunious value. I am using classic Auto MPG Dataset to build a model for prediction of fuel efficiency. This dataset belongs to late 70s and early 80s. Some of the key features include: cylinders, displacement, horsepower, and weight.","cae79fa8":"Let us seperate the target variable from train and test dataset","b05f842e":"Let us look at the heatmap ","9ca96258":"## Performance \n\nNow that all the models are trained check the test-set performance and see how they did:","6729f00f":"### Step 2: DNN with all input\/variable ","d68b6267":"## Summary\n\nThis notebook gave insights and a few techniques to handle a regression problem. Key takeaways:\n\n1. Mean Squared Error (MSE) and Mean Absolute Error (MAE) are common loss functions used for regression problems.\n\n2. Mean Absolute Error is less sensitive to outliers. Different loss functions are used for classification problems.\n\n3. Similarly, evaluation metrics used for regression differ from classification.\n\n4. When numeric input data features have values with different ranges, each feature should be scaled independently to the same range.","706392e1":"We can see there are 6 records in Horsepower which have null values","12a33eff":"This model does slightly better than the linear-horsepower model.","c4022720":"## DNN Regression\n\nSo far we have implemented in step1 and step 2 linear models for single and multiple inputs.\n\nNow we will look at implenenting single-input and multiple-input DNN models. \n\nThe code will remain same except the model is expanded to include  \"hidden\" non-linear layers. \n\nThe normalization layer.\nTwo hidden, nonlinear, Dense layers using the relu nonlinearity.\nA linear single-output layer.\nBoth will use the same training procedure so the compile method is included in the build_and_compile_model function below.\n\n","a05f59b3":"## Please provide your comments for this notebook........Thanks so much........","97b880dd":"Stage 1 Build the model ","ffc42942":"### Data set is available on UCI ML Reposisitory https:\/\/archive.ics.uci.edu\/ml\/.\n\nGet the data","4853fdd0":"Visualize the model's training progress using the stats stored in the history object.","d5ff8886":"### Normalize the data or scale the data","409058da":"The above model will predict MPG from Horsepower ","81c404b9":"Stage 2 : Compile the model \n\nStage 3 : Optimize ","a77b6967":"Since this is a single variable regression it's easy to look at the model's predictions as a function of the input:","e01a672a":"### Step 2:-  use multiple input to predict MPG \n\nNormalize the whole of the input data, we have done this exercise early","71e117c9":"Let us do a correlation using pairplot to look at the skewness"}}