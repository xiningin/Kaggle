{"cell_type":{"f28ef3ab":"code","7fa16c1e":"code","702e56f3":"code","7e0a85fb":"code","f20b5090":"code","04336c5d":"code","6b7e5bd0":"code","5ebb7770":"code","597fa5b7":"code","4358c40f":"code","10a77528":"code","7e1cb045":"code","092fd153":"code","e95d4e32":"code","dd2e1d25":"code","9bf5d0b6":"code","28298c61":"code","665f7d8a":"code","eccc7768":"code","9c369c35":"code","08cf86f6":"code","5d85dc8f":"code","adc693e6":"code","dc6fc316":"code","4b999190":"code","722e98b9":"code","82587f46":"code","beacaead":"code","3e56181d":"code","7090c8ad":"code","6ca88694":"code","487efac9":"code","fad4d6dd":"code","e5a6ff14":"code","d53da900":"code","716aaa95":"code","525de211":"code","db812dc5":"code","b64a7fd5":"code","7a4074f8":"code","950a13aa":"code","440f01fc":"code","764c700e":"code","7845119e":"code","6080d247":"code","dd8d425c":"code","e8a943d9":"markdown","05809e34":"markdown","0f1aad80":"markdown","53c2b451":"markdown","77f9c39d":"markdown","ae81f396":"markdown","b48f10c1":"markdown","363954b7":"markdown","5cf6c11b":"markdown","20db6eb2":"markdown","9916eab1":"markdown","007282af":"markdown","330f85b3":"markdown","2c9c8c62":"markdown","003566fb":"markdown","0136ad64":"markdown","193ade19":"markdown","783c040b":"markdown","1fc80b86":"markdown","6e65c509":"markdown","57d706f1":"markdown","8637e1dc":"markdown","091ceacc":"markdown","ee8d13c4":"markdown","028f349c":"markdown"},"source":{"f28ef3ab":"# importing basic packages\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O\nimport datetime # manipulating date formats\n# Viz\nimport matplotlib.pyplot as plt # basic plotting\nimport seaborn as sns # for prettier plots\n%matplotlib inline\n\n# settings\nimport warnings\nwarnings.filterwarnings(\"ignore\")","7fa16c1e":"# reading data\nfeatures=pd.read_csv(\"..\/input\/walmart-recruiting-store-sales-forecasting\/features.csv.zip\")\nstores=pd.read_csv(\"..\/input\/walmart-recruiting-store-sales-forecasting\/stores.csv\")\ntrain=pd.read_csv(\"..\/input\/walmart-recruiting-store-sales-forecasting\/train.csv.zip\")\ntest=pd.read_csv(\"..\/input\/walmart-recruiting-store-sales-forecasting\/test.csv.zip\")","702e56f3":"df_names=['features','stores','train','test']\ndf_list=[features,stores,train,test]\nfor i in range(4):\n    print('--'*15)\n    print(f'Dataframe {df_names[i]} has {df_list[i].shape[0]} rows and {df_list[i].shape[1]} columns.')\n    print('--'*15)\n    display(df_list[i].head(5))","7e0a85fb":"# checking time duration of records\nfor i in [0,2,3]:\n    print(f'Dataframe {df_names[i]} contains data from {df_list[i].Date.min()} to {df_list[i].Date.max()}.\\n')","f20b5090":"# checking missing values\nfor i in range(4):\n    print(f'Dataframe {df_names[i]} has missing values.\\n') if (df_list[i].isna().sum().any()==True) else print(f'Dataframe {df_names[i]} does not have missing values.')","04336c5d":"import missingno as msno\nmsno.bar(features,figsize=(15, 5),fontsize=15,color='orange');","6b7e5bd0":"print('Percentages of missing values in features dataframe.')\n(100*features.isna().sum()\/features.shape[0]).sort_values()","5ebb7770":"# pie chart\nlabels = stores.Type.value_counts().index.tolist()\nsizes = stores.Type.value_counts().values.tolist()\nexplode = (0.05, 0.02, 0)\nplt.figure(figsize=(5,5))\nplt.pie(sizes, explode=explode, labels=labels, autopct='%1.1f%%', startangle=60,\n        textprops={'fontsize': 18},colors=['#f538cc','#fa5282','#facc69'])\nplt.title('Different types of stores');","597fa5b7":"ax = sns.countplot(stores.Type ,facecolor=(0,0,0,0),linewidth=10,\n                   edgecolor=sns.color_palette(\"spring\", 3))\nfor p in ax.patches:\n    ax.annotate(f'Number of\\n stores:\\n {p.get_height()}', (p.get_x() + p.get_width() \/ 2., p.get_height()-4),\n               ha = 'center', va = 'center', xytext = (0, 10), textcoords = 'offset points',fontsize=12);","4358c40f":"plt.figure(figsize=(15,5))\nsns.barplot(x='Store',y='Size',data=stores,order=stores.sort_values('Size')['Store'].tolist())\nplt.title('Sizes of all the stores.',fontsize=15)\nplt.tight_layout();","10a77528":"sns.set_style('whitegrid')\nsns.boxplot(x='Type',y='Size',data=stores,palette='spring')\nplt.title('Type vs Size',fontsize=15);","7e1cb045":"# combining train\/test and stores and features dataframes for further analysis\n\ntrain_expanded = train.merge(features, how='inner',on=['Store','Date','IsHoliday']).sort_values(by=\n                            ['Store','Dept','Date']).reset_index(drop=True)\ntrain_expanded = train_expanded.merge(stores, how='inner', on=['Store'])\n\ntest_expanded = test.merge(features, how='inner',on=['Store','Date','IsHoliday']).sort_values(by=\n                            ['Store','Dept','Date']).reset_index(drop=True)\ntest_expanded = test_expanded.merge(stores, how='inner', on=['Store'])\n\n# converting dtype of date column\ntrain_expanded['Date'] = pd.to_datetime(train_expanded['Date'])\ntest_expanded['Date'] = pd.to_datetime(test_expanded['Date'])","092fd153":"train_expanded.head(3)","e95d4e32":"plt.figure(figsize=(15,3))\ntrain_expanded.groupby('Date')['Weekly_Sales'].mean().plot()\nplt.title('Average weekly Sales of the company across all stores in given timeframe', fontsize=18)\nplt.ylabel('Sales', fontsize=16)\nplt.xlabel('Date', fontsize=16);","dd2e1d25":"# creating 3 new features from date column\n\nfor df in [train_expanded,test_expanded]:\n    df['Week'] = df['Date'].dt.week\n    df['Month'] = df['Date'].dt.month\n    df['Year'] = df['Date'].dt.year\n\nplt.figure(figsize=(15,3))\ntrain_expanded[train_expanded['Year']==2010].groupby('Month').mean()['Weekly_Sales'].plot()\ntrain_expanded[train_expanded['Year']==2011].groupby('Month').mean()['Weekly_Sales'].plot()\ntrain_expanded[train_expanded['Year']==2012].groupby('Month').mean()['Weekly_Sales'].plot()\nplt.title('Average weekly Sales of the company in each year', fontsize=18)\nplt.legend(['2010', '2011', '2012'], loc='best', fontsize=16)\nplt.ylabel('Sales', fontsize=16)\nplt.xlabel('Months', fontsize=16);","9bf5d0b6":"plt.figure(figsize=(15,3))\ntrain_expanded[train_expanded['Type']=='A'].groupby('Month').mean()['Weekly_Sales'].plot()\ntrain_expanded[train_expanded['Type']=='B'].groupby('Month').mean()['Weekly_Sales'].plot()\ntrain_expanded[train_expanded['Type']=='C'].groupby('Month').mean()['Weekly_Sales'].plot()\nplt.title('Average weekly Sales of the company by type of the store', fontsize=18)\nplt.legend(['Type A', 'Type B', 'Type C'], loc='best', fontsize=16)\nplt.ylabel('Sales', fontsize=16)\nplt.xlabel('Months', fontsize=16);","28298c61":"def av_sales_plotter(str):\n    plt.figure(figsize=(20,5))\n    train_expanded.groupby(str).mean()['Weekly_Sales'].sort_values().plot(kind='bar',color='#b7f28a')\n    plt.title(f'Average Sales of each {str} in given timeframe.', fontsize=18)\n    plt.ylabel('Sales', fontsize=16)\n    plt.xlabel(str, fontsize=16)\n    plt.tick_params(axis='x', labelsize=14)\n    \nav_sales_plotter('Store')","665f7d8a":"av_sales_plotter('Dept')","eccc7768":"print('IsHoliday vs Weekly_Sales')\nplt.figure(figsize=(15,5))\nplt.subplot(1,2,1)\nsns.stripplot(y=train_expanded['Weekly_Sales'],x=train_expanded['IsHoliday'])\nplt.subplot(1,2,2)\nsns.violinplot(y=train_expanded['Weekly_Sales'],x=train_expanded['IsHoliday']);","9c369c35":"print('Type vs Weekly_Sales')\nplt.figure(figsize=(15,5))\nplt.subplot(1,2,1)\nsns.stripplot(y=train_expanded['Weekly_Sales'],x=train_expanded['Type'])\nplt.subplot(1,2,2)\nsns.boxenplot(y=train_expanded['Weekly_Sales'],x=train_expanded['Type']);","08cf86f6":"print('Weekly sales vs size')\nsns.jointplot(train_expanded['Weekly_Sales'],train_expanded['Size']);","5d85dc8f":"train_expanded[['Date', 'Temperature', 'Fuel_Price', 'CPI', 'Unemployment']].plot(x='Date', subplots=True, figsize=(20,15));","adc693e6":"sns.set_palette(\"summer\")\nsns.pairplot(train_expanded[['Weekly_Sales', 'Temperature', 'Fuel_Price', 'CPI', 'Unemployment']],\n             corner=True,diag_kind=\"kde\");","dc6fc316":"train_expanded.dropna().corr()['Weekly_Sales'].abs().sort_values()[:-1].plot(kind='bar');","4b999190":"plt.figure(figsize=(15,10))\nsns.heatmap(train_expanded.corr(),annot=True,cmap='summer');","722e98b9":"# importing relevant libraries\n\nfrom sklearn.model_selection import train_test_split,KFold,cross_val_score\nfrom sklearn.preprocessing import StandardScaler\nfrom xgboost import XGBRegressor\nfrom sklearn.metrics import mean_absolute_error,r2_score,mean_squared_error\nfrom sklearn.ensemble import RandomForestRegressor\n# from sklearn.linear_model import LinearRegression\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.svm import SVR\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.model_selection import GridSearchCV,RandomizedSearchCV","82587f46":"# preprocessing\n\ndef preprocessor(df):\n    # label-encoding\n    df['IsHoliday'] = df['IsHoliday'].astype('str').map({'True':0,'False':1})\n    df.Type = df.Type.map({'A':2,'B':1,'C':0})\n    # deleting less important features\n    return df.drop(['Date','Year','MarkDown1','MarkDown2','MarkDown4','MarkDown3','MarkDown5','CPI',\n             'Unemployment','Temperature','Fuel_Price'],axis=1)\n\ntrain_preprocessed = preprocessor(train_expanded)\ntest_preprocessed = preprocessor(test_expanded)","beacaead":"# splitting data into 2 parts\n\ny = train_preprocessed[\"Weekly_Sales\"]\nX = train_preprocessed.drop(['Weekly_Sales'],axis=1)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state = 101)\n\n# function for displaying scores\n\ndef score_calc(predictions):\n    scores = pd.DataFrame([mean_absolute_error(y_test, predictions)\n#                           ,mean_squared_error(y_test, predictions)\n                          ,r2_score(y_test, predictions)],columns=['score'],index=['MAE','R2 score'])\n    return scores\n\n# function for building model\n\ndef run_model(model,name):\n    \n    name = model()\n    name.fit(X_train, y_train)\n    preds = name.predict(X_test)\n    try:\n        feat_imp = name.feature_importances_\n        plt.bar(X_train.columns,feat_imp,color='green')\n        plt.title('Feature Importance graph')\n        plt.xticks(rotation=45)\n    except:\n        pass\n    return score_calc(preds)","3e56181d":"train_preprocessed.head()","7090c8ad":"run_model(DecisionTreeRegressor,'dtree')","6ca88694":"run_model(RandomForestRegressor,'rfc')","487efac9":"run_model(XGBRegressor,'xgb')","fad4d6dd":"run_model(KNeighborsRegressor,'knn')","e5a6ff14":"# scaling size column\nscaler=StandardScaler()\nscaler.fit(train_preprocessed['Size'].values.reshape(-1,1))\nX_train['Size'] = scaler.transform(X_train['Size'].values.reshape(-1, 1))\nX_test['Size'] = scaler.transform(X_test['Size'].values.reshape(-1, 1))","d53da900":"run_model(DecisionTreeRegressor,'dtree')","716aaa95":"# run_model(XGBRegressor,'xgb')","525de211":"# run_model(RandomForestRegressor,'rfc')","db812dc5":"X_train = X_train[['Store','Dept','Size']]\nX_test = X_test[['Store','Dept','Size']]","b64a7fd5":"run_model(DecisionTreeRegressor,'dtree')","7a4074f8":"# I tried one hot encoding week feature but score did not improve so I dropped the idea.\n\n# dummies=pd.get_dummies(train_preprocessed.Week.astype(str),drop_first=True,prefix='week')\n# dum_week = pd.concat([train_preprocessed,dummies],axis=1)\n# dum_week.drop('Week',axis=1,inplace=True)","950a13aa":"# option A\nfrom sklearn.model_selection import cross_val_score\nscore = cross_val_score(RandomForestRegressor(), X, y, cv=4)\nprint(f\"Average 4-Fold CV Score: {score.mean().round(4)}\")","440f01fc":"random_grid = {'n_estimators': [50,60,70],\n               'max_features': [3,4],\n               'max_depth': [25,30,35],\n               'min_samples_split': [3,4],\n              'min_samples_leaf':[1,2]}\n\nrf_grid = RandomizedSearchCV(RandomForestRegressor(),\n                        random_grid,\n                        cv = 4,\n                        n_jobs = 5,\n                        verbose=True)\n\nrf_grid.fit(X,y)\n\nprint(rf_grid.best_score_)\nprint(rf_grid.best_params_)","764c700e":"# parameters = {'learning_rate': [.03, 0.05, .07],\n#               'max_depth': [6,7,8,9],\n#               'n_estimators': [500,700]}\n\n# xgb_grid = GridSearchCV(XGBRegressor(),\n#                         parameters,\n#                         cv = 3,\n#                         n_jobs = 5,\n#                         verbose=True)\n\n# xgb_grid.fit(X,y)\n\n# print(xgb_grid.best_score_)\n# print(xgb_grid.best_params_)","7845119e":"# option B - cross-validation using kfold\n\npredictor_train_scale = train_preprocessed.drop('Weekly_Sales',axis=1)\npredictor_test_scale = test_preprocessed\ntarget_train = train_preprocessed.Weekly_Sales\n\nkf=KFold(n_splits=4,shuffle=True)\n\npreds_3   = list()\ny_pred_3  = []\nr2_score_ = []\nmae=[]\n\n# Applying model on each fold and calculating mean of score\nfor i,(train_idx,val_idx) in enumerate(kf.split(predictor_train_scale)):    \n    \n    X_train, y_train = predictor_train_scale.iloc[train_idx,:], target_train.iloc[train_idx]    \n    X_val, y_val = predictor_train_scale.iloc[val_idx, :], target_train.iloc[val_idx]\n   \n    print('\\nFold: {}\\n'.format(i+1))\n    rf = RandomForestRegressor()\n    rf.fit(X_train, y_train)\n\n    r2 = r2_score(y_val,rf.predict(X_val))\n    mae_ = mean_absolute_error(y_val,rf.predict(X_val))\n    r2_score_.append(r2)\n    mae.append(mae_)\n    preds_3.append(rf.predict(predictor_test_scale[predictor_test_scale.columns]))\n    \ny_pred_final_3 = np.mean(preds_3,axis=0)    \n\nprint('R2 - CV Score: {}'.format((sum(r2_score_)\/4)),'\\n')\nprint('MAE Score: {}'.format((sum(mae)\/4)),'\\n')\nprint(\"Score : \",r2_score_)","6080d247":"sns.set_style('darkgrid')\nplt.figure(figsize=(15,5))\ntest_expanded['Weekly_Sales'] = y_pred_final_3\ntrain_expanded.groupby('Date')['Weekly_Sales'].mean().plot()\ntest_expanded.groupby('Date')['Weekly_Sales'].mean().plot(color='orange')\nplt.legend(['Actual', 'Predicted'], loc='best', fontsize=16)\nplt.ylabel('Sales', fontsize=16);","dd8d425c":"submission = pd.read_csv('..\/input\/walmart-recruiting-store-sales-forecasting\/sampleSubmission.csv.zip')\nsubmission['Weekly_Sales'] = y_pred_final_3\nsubmission.to_csv('results_rf_cv_tuned.csv',index=False)","e8a943d9":"### Cross-validation","05809e34":"Here cv score is much higher. I was not able to figure out why. If you know please tell in comments section.","0f1aad80":"<b> Approach 3: Using only Store, Dept and Size columns<\/b>","53c2b451":"Highest sales events were recorded in the type B stores.","77f9c39d":"So, cv scored increased after hyper-parameter tuning.","ae81f396":"Almost half of the stores are of type A. Type C stores are least in number.","b48f10c1":"I will label encode the IsHoliday and Type feature. From EDA, I concluded to drop Year, CPI, unemployment, temperature and fuel price. I am also dropping all the 'markdown' columns.","363954b7":"Highest sales events were recorded in the special holiday week.","5cf6c11b":"### ASSIGNMENT\n\n<b>Problem Statement:<\/b>\nYou are provided with historical sales data for 45 stores of a Retail chain located in different\nregions. Each store contains a number of departments, and you are tasked with predicting the\ndepartment-wide sales for each store.\n\nThe data is provided in 4 different CSVs.\n\n","20db6eb2":"In above heatmap, correlation between different columns can be checked.","9916eab1":"There are broadly 3 types of stores: small-sized, medium-sized and large-sized. This numerical variable can be converted into categorical variable using pd.cut function but first let's check relation between size and type of the stores.","007282af":"As expected, temperature has high seasonality. Week and month column can effectively cover its effect.\n\nFuel price and CPI show an upward trend and unemployment shows downward trend.\n\nLet's explore effect of these feature with weekly sales.","330f85b3":"Sales of the company see rise during the end of the year. Possible reasons could be any tradition or festival in that company.\n\nType A stores seem to have comparitively high sales. But this can be due to difference in number of stores of different type. Also, we can notice that sales of type C are constant over the year.","2c9c8c62":"<b> Approach 2: Standardizing size column<\/b>\n\nHere, we can see tree based models do not need feature scaling.","003566fb":"Dize and Dept are most correlated with the target variable.","0136ad64":"No specific pattern.","193ade19":"<b> Approach 1: Taking all columns<\/b>","783c040b":"### Hyper parameter tuning","1fc80b86":"There is no particular relationship between these features and target variable.\n\nAlso, distribution of target variable is highly skewed. That's why, I will not go with linear regression.","6e65c509":"Though we don't have any knowledge about how stores were divided into these types, we can see from the graph that it covers the effect of size column.","57d706f1":"## Pre-processing and modelling","8637e1dc":"### Making final predictions","091ceacc":"Around 10 departments have lowest sales. The company can further analyse as to what are the possible reasons and how it can be improved.","ee8d13c4":"pLEASE PROVIDE FEEDBACK SO THAT I CAN IMPROVE!!","028f349c":"All the markdown<sub>i<\/sub> columns have more than 50% missing values. These are anonymized data related to promotional markdowns that the retail chain is running. MarkDown data is only available after Nov 2011. So, it's quite difficult to choose best imputation technique. If correlation of these columns is not strong with target variable, I will drop them. Let's do EDA first.\n\n### EDA"}}