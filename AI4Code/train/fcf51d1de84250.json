{"cell_type":{"da57faf9":"code","a8fad48e":"code","75caadb1":"code","3b7fe7ff":"code","dc75967a":"code","63e913fe":"code","58654c04":"code","f55ebf0c":"code","6c951e0f":"code","80ba2dca":"code","a87318b0":"code","57c4145c":"code","6891c30b":"code","fe3fc744":"code","dc514639":"code","e33acb91":"markdown","8d4cf861":"markdown","f4b05133":"markdown","0ddf30f8":"markdown"},"source":{"da57faf9":"import numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt \nimport seaborn as sns \nimport os \nimport time \nimport json \nimport requests \nfrom tqdm import tqdm \nimport wandb \nfrom wandb.keras import WandbCallback \nfrom kaggle_secrets import UserSecretsClient \nimport random \nfrom typing import Tuple \nimport gc \n\nfrom sklearn.model_selection import GroupKFold\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.preprocessing import RobustScaler \n\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras.models import Sequential \nfrom tensorflow.keras import layers , Model\nfrom tensorflow.keras.layers import MultiHeadAttention, Input, Dropout, Dense, Conv1D, LayerNormalization\nfrom tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\nfrom tensorflow.keras.callbacks import LearningRateScheduler, ReduceLROnPlateau\nfrom tensorflow.keras.optimizers.schedules import ExponentialDecay\nfrom tensorflow.keras.callbacks import Callback\nimport tensorflow.keras.backend as K\n# from keras_pos_embd import PositionEmbedding\n\ndef seed_everything(seed=42):\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    random.seed(seed)\n    tf.random.set_seed(seed)\n    \nseed_everything()\npd.set_option(\"display.max_columns\", None)","a8fad48e":"config = dict(\n    competition = \"ventilator\", \n    infra = \"kaggle\", \n    train = True, \n    type = \"train\", \n    debug = False, \n    inference = True, \n    \n    model_name = \"transformer\", \n    frame_word = \"tensorflow\", \n    device = \"tpu\", \n    n_fold = 5, \n    early_stopping_rounds = 30, \n    batch_size = 1024, \n    epoch = 530, \n    verbose = 100, \n    seed = 42 \n)\n\nparams = {\n    \"input_size\": (80, 73),\n    \"hidden_dim\": 128, \n    \"head_size\": 256, \n    \"num_heads\": 12, \n    \"ff_dim\": 4, \n    \"num_transformer_blocks\": 4, \n    \"mlp_units\": [128], \n    \"dropout\": 0.2, \n    \"mlp_dropout\": 0 , \n}","75caadb1":"user_secrets = UserSecretsClient()\nurl = user_secrets.get_secret(\"WEB_HOOK_URL\") \n\nuser_secrets = UserSecretsClient()\napi = user_secrets.get_secret(\"wandb_api\")\n\n\ndef setup_db():\n    wandb.login(key=api)\n    run = wandb.init(\n        project = config[\"competition\"], \n        name = config[\"model_name\"], \n        config = config, \n        group = config[\"model_name\"], \n        job_type = config[\"type\"]\n    )\n    return run\n\ndef slack(txt):\n    requests.post(url, data=json.dumps({\n        \"username\": \"kaggle\", \n        \"text\": txt \n    }))","3b7fe7ff":"if config[\"debug\"]:\n    train = pd.read_csv(\"..\/input\/ventilator-pressure-prediction\/train.csv\", nrows=80*100)\n    test = pd.read_csv(\"..\/input\/ventilator-pressure-prediction\/test.csv\", nrows=80*100)\nelse:\n    train = pd.read_csv(\"..\/input\/ventilator-pressure-prediction\/train.csv\")\n    test = pd.read_csv(\"..\/input\/ventilator-pressure-prediction\/test.csv\")\n\nsort = np.sort(train.pressure.unique())\nPRESSURE_MIN = sort[0]\nPRESSURE_MAX = sort[-1]\nPRESSURE_STEP = sort[1] - sort[0]","dc75967a":"def reduce_mem_usage(train_data):\n    start_mem = train_data.memory_usage().sum() \/ 1024**2\n    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n    for col in train_data.columns:\n        col_type = train_data[col].dtype\n\n        if col_type != object:\n            c_min = train_data[col].min()\n            c_max = train_data[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    train_data[col] = train_data[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    train_data[col] = train_data[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    train_data[col] = train_data[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    train_data[col] = train_data[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    train_data[col] = train_data[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    train_data[col] = train_data[col].astype(np.float32)\n                else:\n                    train_data[col] = train_data[col].astype(np.float64)\n        else:\n            train_data[col] = train_data[col].astype('category')\n\n    end_mem = train_data.memory_usage().sum() \/ 1024**2\n    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) \/ start_mem))\n\n    return train_data","63e913fe":"train = reduce_mem_usage(train)\ntest = reduce_mem_usage(test)","58654c04":"def lag_feature(df) -> pd.DataFrame:\n    df[\"area\"] = df.time_step * df.u_in \n    df[\"area\"] = df.groupby(\"breath_id\")[\"area\"].cumsum()\n    \n    df[\"u_in_cumsum\"] = df.groupby(\"breath_id\")[\"u_in\"].cumsum()\n    \n    for i in range(4):\n        df[\"u_in_\"+f\"lag{i+1}\"] = df.groupby(\"breath_id\")[\"u_in\"].shift(i+1).fillna(0)\n        df[\"u_out_\"+f\"lag{i+1}\"] = df.groupby(\"breath_id\")[\"u_out\"].shift(i+1).fillna(0)\n\n        df[\"u_in_\"+f\"back{i+1}\"] = df.groupby(\"breath_id\")[\"u_in\"].shift((-1)*(i+1)).fillna(0)\n        df[\"u_out_\"+f\"back{i+1}\"] = df.groupby(\"breath_id\")[\"u_out\"].shift((-1)*(i+1)).fillna(0)\n\n    df[\"u_out_rolling_10\"] = df.groupby(\"breath_id\")[\"u_out\"].rolling(window=10).mean().reset_index(drop=True).fillna(0)\n    df[\"u_in_rolling_10\"] = df.groupby(\"breath_id\")[\"u_in\"].rolling(window=10).mean().reset_index(drop=True).fillna(0)\n    \n    df[\"u_in_max\"] = df.groupby(\"breath_id\")[\"u_in\"].transform(\"max\")\n    df[\"u_in_min\"] = df.groupby(\"breath_id\")[\"u_in\"].transform(\"min\")\n    df[\"u_in_mean\"] = df.groupby(\"breath_id\")[\"u_in\"].transform(\"mean\")\n    df[\"u_out_max\"] = df.groupby(\"breath_id\")[\"u_out\"].transform(\"max\")\n    df[\"u_out_min\"] = df.groupby(\"breath_id\")[\"u_out\"].transform(\"min\")\n    df[\"u_out_mean\"] = df.groupby(\"breath_id\")[\"u_out\"].transform(\"mean\")\n    \n    df[\"u_in_first\"] = df.groupby(\"breath_id\")[\"u_in\"].transform(\"first\")\n    df[\"u_in_last\"] = df.groupby(\"breath_id\")[\"u_in\"].transform(\"last\")\n    \n    for i in range(4):\n        df[\"u_in\"+f\"_diff{i+1}\"] = df[\"u_in\"] - df[f\"u_in_lag{i+1}\"]\n        df[\"u_in\"+f\"_diff_back{i+1}\"] = df[\"u_in\"] - df[f\"u_in_back{i+1}\"]\n\n        df[\"u_out\"+f\"_diff{i+1}\"] = df[\"u_out\"] - df[f\"u_out_lag{i+1}\"]\n        df[\"u_out\"+f\"_diff_back{i+1}\"] = df[\"u_out\"] - df[f\"u_out_back{i+1}\"]\n\n    df[\"u_in_diff_max\"] = df[\"u_in_max\"] - df[\"u_in\"]\n    df[\"u_in_diff_min\"] = df[\"u_in_min\"] - df[\"u_in\"]\n    df[\"u_in_diff_mean\"] = df[\"u_in_mean\"] - df[\"u_in\"]\n    \n    df[\"cross\"] = df[\"u_in\"] * df[\"u_out\"]\n    df[\"cross2\"] = df[\"time_step\"] * df[\"u_out\"]\n    \n    df[\"time_class\"] = df.groupby(\"breath_id\").cumcount()\n    df[\"R\"] = df.R.astype(str)\n    df[\"C\"] = df.C.astype(str)\n    df[\"R_C\"] = df.R + \"_\" + df.C \n    gc.collect()\n    return df\n\ndef group_feature(train, test) -> Tuple[pd.DataFrame, pd.DataFrame]:\n    # time_class x u_in\n    time_grp = train.groupby(\"time_class\").mean().loc[:, [\"u_in\"]]\n    time_grp = time_grp.rename(columns={\"u_in\": \"u_in_time_class\"})\n    train = pd.merge(train, time_grp, how=\"left\", left_on=\"time_class\", right_index=True)\n    test = pd.merge(test, time_grp, how=\"left\", left_on=\"time_class\", right_index=True)\n    del time_grp \n    gc.collect()\n    \n    print(1)\n    \n    # R x u_in \n    r = train.groupby(\"R\").mean().loc[:, [\"u_in\"]]\n    r = r.rename(columns={\"u_in\": \"u_in_r_mean\"})\n    train = pd.merge(train, r, how=\"left\", left_on=\"R\", right_index=True)\n    test = pd.merge(test, r, how=\"left\", left_on=\"R\", right_index=True)\n    del r \n    gc.collect()\n\n    \n    # c x u_in \n    c = train.groupby(\"C\").mean().loc[:, [\"u_in\"]]\n    c = c.rename(columns={\"u_in\": \"u_in_c_mean\"})\n    train = pd.merge(train, c, how=\"left\", left_on=\"C\", right_index=True)\n    test = pd.merge(test, c, how=\"left\", left_on=\"C\", right_index=True)\n    del c \n    gc.collect()\n    \n    print(2)\n\n    # r_c x u_in \n    rc = train.groupby(\"R_C\").mean().loc[:, [\"u_in\"]]\n    rc = rc.rename(columns={\"u_in\": \"u_in_rc_mean\"})\n    train = pd.merge(train, rc, how=\"left\", left_on=\"R_C\", right_index=True)\n    test = pd.merge(test, rc, how=\"left\", left_on=\"R_C\", right_index=True)\n    del rc \n    gc.collect()\n    \n    print(3)\n\n    # r_c, time_class x u_in \n    rc = train.groupby([\"R_C\", \"time_class\"]).mean().loc[:, [\"u_in\"]]\n    rc = rc.rename(columns={\"u_in\": \"u_in_rc_time_mean\"})\n    train = pd.merge(train, rc, how=\"left\", left_on=[\"R_C\", \"time_class\"], right_index=True)\n    test = pd.merge(test, rc, how=\"left\", left_on=[\"R_C\", \"time_class\"], right_index=True)\n    del rc \n    gc.collect()\n    \n    print(4)\n    \n    # get dummmies object\n    last_train_shape = train.shape[0]\n    y = train.pressure.values.ravel()\n    df = pd.concat([train.drop(\"pressure\", axis=1), test])\n    df = pd.get_dummies(data=df, columns=[\"R\", \"C\", \"R_C\"])\n    train, test = df.iloc[:last_train_shape, :], df.iloc[last_train_shape:, :]\n    del df \n    train[\"pressure\"] = y \n    del y \n    gc.collect()\n    return train, test ","f55ebf0c":"%%time \n\ntrain = lag_feature(train)\ntest = lag_feature(test)","6c951e0f":"%%time \n\ntrain, test = group_feature(train, test)","80ba2dca":"train = reduce_mem_usage(train)\ntest = reduce_mem_usage(test)","a87318b0":"len(train.drop([\"id\", \"breath_id\", \"pressure\"], axis=1).columns) # use train features = model input size ","57c4145c":"@tf.custom_gradient\ndef round_with_gradients(x):\n    def grad(dy):\n        return dy\n    return tf.round(x), grad\n\nclass ScaleLayer(tf.keras.layers.Layer):\n    def __init__(self):\n        super(ScaleLayer, self).__init__()\n        self.min = tf.constant(PRESSURE_MIN, dtype=np.float32)\n        self.max = tf.constant(PRESSURE_MAX, dtype=np.float32)\n        self.step = tf.constant(PRESSURE_STEP, dtype=np.float32)\n\n    def call(self, inputs):\n        steps = tf.math.divide(tf.math.add(inputs, -self.min), self.step)\n        int_steps = round_with_gradients(steps)\n        rescaled_steps = tf.math.add(tf.math.multiply(int_steps, self.step), self.min)\n        clipped = tf.clip_by_value(rescaled_steps, self.min, self.max)\n        return clipped\n    \n    \nclass PositionalEncoding(tf.keras.layers.Layer):\n    def __init__(self):\n        super(PositionalEncoding, self).__init__()\n        position = 80 \n        emb_dim = params[\"hidden_dim\"]\n        self.pos_encoding = self._positional_encoding(position, emb_dim)\n        \n    def _get_angles(self, position, i, emb_dim):\n        \"\"\"\n        assign position, i and emb_dim to the expression of the angle of positional encoding formulae\n        outputs: shape=(position.shape[0], i.shape[1])\n        \"\"\"\n        denominator = 1 \/ tf.pow(10000, (2 * (i \/\/ 2)) \/ tf.cast(emb_dim, tf.float32))\n        return position * denominator\n\n    def _positional_encoding(self, sentence_length, emb_dim):\n        \"\"\"\n        inputs:\n        sentence_length: int\n        emb_dim: int\n        \n        outputs:\n        output: shape=(1, sentence_length, emb_dim), float32\n        \"\"\"\n        # \u8a08\u7b97\u3092\u52b9\u7387\u5316\u3059\u308b\u305f\u3081\u306bposition\u3068i\u3092\u884c\u5217\u306b\u3057\u3066angle\u8a08\u7b97\u3092\u884c\u5217\u306e\u7a4d\u3067\u4e00\u5ea6\u306b\u5b9f\u884c\u3059\u308b\n        angle = self._get_angles(\n            position=tf.expand_dims(tf.range(sentence_length, dtype=tf.float32), -1),\n            i=tf.expand_dims(tf.range(emb_dim, dtype=tf.float32), 0),\n            emb_dim=emb_dim\n        )\n        \n        # \u30a4\u30f3\u30c7\u30c3\u30af\u30b9\u304c\u5076\u6570\u306e\u3082\u306e\u306f\u30b5\u30a4\u30f3\u95a2\u6570\u306b\u9069\u5fdc\n        sine = tf.math.sin(angle[:, 0::2])\n        # \u30a4\u30f3\u30c7\u30c3\u30af\u30b9\u304c\u5947\u6570\u306e\u3082\u306e\u306f\u30b3\u30b5\u30a4\u30f3\u95a2\u6570\u306b\u9069\u5fdc\n        cos = tf.math.cos(angle[:, 1::2])\n        \n        pos_encoding = tf.concat([sine, cos], axis=-1)\n        pos_encoding = tf.expand_dims(pos_encoding, 0)\n        return tf.cast(pos_encoding, tf.float32)\n    \n    def call(self, inputs):\n        \"\"\"\n        inputs: shape=(batch, sentence_length, emb_dim)\n        \"\"\"\n        return inputs + self.pos_encoding[:, :tf.shape(inputs)[1], :]\n    \n    \ndef transformer_encoder(inputs, head_size, num_heads, ff_dim, dropout=0):\n    # Normalization and Attention\n    x = LayerNormalization(epsilon=1e-6)(inputs)\n    x = MultiHeadAttention(\n        key_dim=head_size, num_heads=num_heads, dropout=dropout\n    )(x, x)\n    x = layers.Dropout(dropout)(x)\n    res = x + inputs\n\n    # Feed Forward Part\n    x = LayerNormalization(epsilon=1e-6)(res)\n    x = Conv1D(filters=ff_dim, kernel_size=1, activation=\"relu\")(x)\n    x = Dropout(dropout)(x)\n    x = Conv1D(filters=inputs.shape[-1], kernel_size=1)(x)\n    return x + res\n\n\ndef base_model(\n    input_shape=params[\"input_size\"], \n    head_size=params[\"head_size\"],\n    hidden_dim = params[\"hidden_dim\"],\n    num_heads=params[\"num_heads\"],\n    ff_dim=params[\"ff_dim\"],\n    num_transformer_blocks=params[\"num_transformer_blocks\"], \n    mlp_units=params[\"mlp_units\"], \n    dropout=params[\"dropout\"],\n    mlp_dropout=params[\"mlp_dropout\"]\n):\n    inputs = Input(shape=input_shape)\n    x = inputs\n    x = Dense(hidden_dim, activation=\"relu\")(x)\n    x = PositionalEncoding()(x)\n    for _ in range(num_transformer_blocks):\n        x = transformer_encoder(x, head_size, num_heads, ff_dim, dropout)\n\n    for dim in mlp_units:\n        x = Dense(dim, activation=\"selu\")(x)\n        x = Dropout(mlp_dropout)(x)\n    outputs = Dense(1)(x)\n    output = ScaleLayer()(x)\n    return Model(inputs, outputs)\n\ndef build_model():\n    model = base_model()\n    model.compile(loss=\"mae\", optimizer=\"adam\")\n    return model \n\nmodel = build_model()\nmodel.summary()","6891c30b":"if config[\"debug\"] is not True and config[\"device\"] == \"tpu\":\n    # detect and init the TPU\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver.connect()\n\n    # instantiate a distribution strategy\n    tpu_strategy = tf.distribute.experimental.TPUStrategy(tpu)\n\n\ndef mae(corr, pred):\n    return np.mean(np.abs(corr - pred))\n\n\ndef scaler(tr, va, te):\n    RS = RobustScaler()\n    return RS.fit_transform(tr), RS.transform(va), RS.transform(te)\n\n\ndef callback_tools(fold) -> Tuple[object, object, object, object]:\n    lr = ReduceLROnPlateau(monitor=\"val_loss\", factor=0.5, patience=10, verbose=1)\n    es = EarlyStopping(monitor=\"val_loss\", patience=60, verbose=1, \n                           mode=\"min\", restore_best_weights=True)\n    os.makedirs(\"models\", exist_ok=True)\n    checkpoint_filepath = f\"models\/{fold}.hdf5\"\n    sv = keras.callbacks.ModelCheckpoint(\n            checkpoint_filepath, monitor='val_loss', verbose=1, save_best_only=True,\n            save_weights_only=False, mode='auto', save_freq='epoch',\n            options=None\n    )\n    wb = WandbCallback(log_weights=True)\n    return lr, es, sv, wb \n\n\ndef submit(pred, name):\n    sub = pd.read_csv(\"..\/input\/ventilator-pressure-prediction\/sample_submission.csv\")\n    sub[\"pressure\"] = pred \n    sub.to_csv(f\"submission_lstm_{name}.csv\", index=False)\n    del sub \n    \n    \ndef viz_predict(corr, pred):\n    plt.figure(figsize=(15, 6))\n    \n    plt.subplot(121)\n    sns.histplot(corr)\n    plt.title(\"Label\")\n    \n    plt.subplot(122)\n    sns.histplot(pred)\n    plt.title(\"Predict\")\n    \n    plt.show()\n    \n\ndef train_fn(train, test):\n    \n    with tpu_strategy.scope():\n\n        predict_val, val_idx, predict_test = [], [], []\n        kf = GroupKFold(n_splits=2 if config[\"debug\"] else config[\"n_fold\"])\n\n        for fold, (tr, va) in enumerate(kf.split(train, train.pressure, train.breath_id)):\n            print(f\"=====================fold: {fold+1}==========================\")\n            x_train, x_val = train.iloc[tr].drop([\"id\", \"pressure\", \"breath_id\"], axis=1), train.iloc[va].drop([\"id\", \"pressure\", \"breath_id\"], axis=1)\n            y_train, y_val = train.iloc[tr][\"pressure\"], train.iloc[va][\"pressure\"]\n            use_col = x_train.columns \n            x_test = test[use_col]\n\n            # scaler \n            x_train, x_val, x_test = scaler(x_train, x_val, x_test)\n\n            # transformer batch shape \n            x_train = x_train.reshape(-1, 80, params[\"input_size\"][1])\n            x_val = x_val.reshape(-1, 80, params[\"input_size\"][1]) \n            x_test = x_test.reshape(-1, 80, params[\"input_size\"][1])        \n            y_train = y_train.values.reshape(-1, 80, 1)\n            y_val = y_val.values.reshape(-1, 80, 1)        \n\n            # set up tools \n            run = setup_db()\n            model = build_model()\n            lr, es, sv, ws = callback_tools(fold)\n            wandb.config.fold = fold \n\n            model.fit(x_train,\n                     y_train,\n                     validation_data=(x_val, y_val),\n                     callbacks=[lr, es, sv, ws],\n                     epochs=1 if config[\"debug\"] else config[\"epoch\"],\n                     batch_size=config[\"batch_size\"])\n\n            pred_v = model.predict(x_val, batch_size=config[\"batch_size\"], verbose=config[\"verbose\"]).squeeze().reshape(-1, 1).squeeze()\n            pred_t = model.predict(x_test, batch_size=config[\"batch_size\"], verbose=config[\"verbose\"]).squeeze().reshape(-1, 1).squeeze()\n\n            predict_val.append(pred_v)\n            predict_test.append(pred_t)        \n            val_idx.append(va)\n\n            print(f\"fold: {fold+1} | mae: {mae(y_val.squeeze().reshape(-1, 1).squeeze(), pred_v)}\")\n\n            del x_train, x_val, x_test, model \n            gc.collect()\n\n        predict_val = np.concatenate(predict_val)\n        val_idx = np.concatenate(val_idx)\n        val_idx = np.argsort(val_idx)\n        predict_val = predict_val[val_idx]\n\n        print(\"##############################################################\")\n        print(f\"CV SCORE: {mae(train.pressure.values.ravel(), predict_val)}\")\n        print(\"##############################################################\")\n\n        predict_test_mean = np.mean(predict_test, 0)\n        predict_test_median = np.median(predict_test, 0)\n\n        predict_test_mean_clip =(np.round(predict_test_mean - PRESSURE_MIN)\/ PRESSURE_STEP) * PRESSURE_STEP + PRESSURE_MIN\n        predict_test_mean_clip = np.clip(predict_test_mean_clip, PRESSURE_MIN, PRESSURE_MAX)\n        predict_test_median_clip =(np.round(predict_test_median - PRESSURE_MIN)\/ PRESSURE_STEP) * PRESSURE_STEP + PRESSURE_MIN\n        predict_test_median_clip = np.clip(predict_test_median_clip, PRESSURE_MIN, PRESSURE_MAX)\n\n        # submit \n        if config[\"debug\"] is not True:\n            submit(predict_test_mean, \"mean\")\n            submit(predict_test_median, \"median\")\n            submit(predict_test_mean_clip, \"mean_clip\")\n            submit(predict_test_median_clip, \"median_clip\")\n\n        gc.collect()\n        slack(\"Transfomer model done.\")\n        return predict_val ","fe3fc744":"pred_v = train_fn(train, test)","dc514639":"viz_predict(pred_v, train.pressure.values.ravel())","e33acb91":"# Transformer Model ","8d4cf861":"# Train ","f4b05133":"# Config ","0ddf30f8":"# Feature engineering "}}