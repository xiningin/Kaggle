{"cell_type":{"a2ee4784":"code","dae6b850":"code","73dc2940":"code","c967b5be":"code","e5c5101d":"code","e364d42b":"code","42db842e":"code","4bef0adf":"code","01a32ba9":"code","b071f054":"code","f4a55438":"code","574f07b7":"code","f0c9d886":"code","2a42e8bd":"code","687a6e65":"code","dae57252":"code","663613ec":"code","e7fc7db7":"code","d89e7b42":"code","0485e320":"code","aca3301d":"code","be1a47a5":"code","70669b28":"code","cd2ec71a":"code","eb069d46":"code","ec2efdd9":"code","4858c760":"markdown","eb8002f6":"markdown","02050252":"markdown","d358bd19":"markdown","199b21c7":"markdown","c8369aad":"markdown","e312469a":"markdown","8d2180bc":"markdown","940cbb90":"markdown","ae3167ce":"markdown","f64242aa":"markdown","0ec7b9e1":"markdown"},"source":{"a2ee4784":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","dae6b850":"import numpy as np\nimport json\nimport torch\nfrom torch import nn, Tensor\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nfrom tqdm import tqdm\nfrom torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\nfrom typing import Optional\nimport matplotlib.pyplot as plt\n\nfrom transformers import AutoTokenizer, AutoModel, AutoModelForCausalLM","73dc2940":"class PairsDataset(Dataset):\n    \n    def __init__(self, file_path, sep='\\t'):\n        super().__init__()\n        \n        self.file_path = file_path\n        self.sep = sep\n        \n        self.data = self.load_data()\n        \n    def load_data(self):\n        \n        data = list()\n        \n        with open(self.file_path) as file_object:\n            for line in file_object:\n                data.append(line.strip().split(self.sep))\n                \n        return data\n    \n    def __len__(self):\n        return len(self.data)\n    \n    def __getitem__(self, index):\n        \n        return self.data[index]","c967b5be":"class Collater:\n    \n    def __init__(self, \n                 tokenizer_name='sberbank-ai\/rugpt3small_based_on_gpt2', \n                 max_length=24):\n        \n        # \u0431\u0443\u0434\u0435\u043c \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u044c \u043f\u0440\u0435\u0434\u043e\u0431\u0443\u0447\u0435\u043d\u043d\u044b\u0439 \u0442\u043e\u043a\u0435\u043d\u0438\u0437\u0430\u0442\u043e\u0440\n        self.tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)\n        # \u0443\u0441\u0442\u0430\u043d\u0430\u0432\u043b\u0438\u0432\u0430\u0435\u043c \u043f\u0430\u0434 \u0442\u043e\u043a\u0435\u043d, \u043e\u043d \u0431\u0443\u0434\u0435\u0442 \u0438\u043c\u0435\u0442\u044c \u0438\u043d\u0434\u0435\u043a\u0441 \"\u0434\u043b\u0438\u043d\u0430 \u0441\u043b\u043e\u0432\u0430\u0440\u044f + 1\"\n        self.tokenizer.pad_token = '<pad>'\n        self.max_length = max_length\n        \n    def tokenize(self, texts):\n        tokenized = self.tokenizer(texts, padding=True, \n                                   truncation=True,\n                                   max_length=self.max_length, \n                                   return_tensors='pt')\n        # \u0437\u0430 \u043f\u0430\u0440\u0443 \u043c\u0438\u043d\u0443\u0442 \u044f \u043d\u0435 \u043d\u0430\u0448\u0435\u043b \u043a\u0430\u043a \u0432\u0441\u0442\u0430\u0432\u0438\u0442\u044c \u043f\u0430\u0434 \u043d\u0430 \u043d\u0443\u0436\u043d\u043e\u0435 \u043c\u043d\u0435 \u043c\u0435\u0441\u0442\u043e, \u043f\u043e\u044d\u0442\u043e\u043c\u0443 \u043f\u0440\u0438\u0431\u0435\u0433\u043d\u0443\u043b \u043a \u0442\u0430\u043a\u043e\u043c\u0443 \u0445\u0430\u043a\u0443\n        tokenized['input_ids'] = tokenized['input_ids'] * tokenized['attention_mask']\n        return tokenized\n    \n    def __call__(self, batch):\n        \n        questions, responses = list(), list()\n        \n        for question, response in batch:\n            questions.append(question)\n            responses.append(response)\n            \n        tokenized_questions = self.tokenize(questions)\n        tokenized_responses = self.tokenize(responses)\n        \n        return tokenized_questions, tokenized_responses","e5c5101d":"def embedding_masking(x: Tensor,\n                      pad_mask: Tensor,\n                      value: float = 0.) -> Tensor:\n    x = x.masked_fill((~(pad_mask.bool())).unsqueeze(-1), value)\n    return x\n\n\nclass GlobalMaskedPooling(nn.Module):\n\n    POOLING_TYPES = ('mean', 'max')\n\n    def __init__(self,\n                 pooling_type: str = 'mean',\n                 dim: int = 1,\n                 normalize: bool = False,\n                 length_scaling: bool = True,\n                 scaling_square_root: bool = True):\n        super().__init__()\n\n        self.pooling_type = pooling_type\n        self.dim = dim\n\n        self.normalize = normalize\n        self.length_scaling = length_scaling\n        self.scaling_square_root = scaling_square_root\n\n        if self.pooling_type == 'max':\n            self.mask_value = -float('inf')\n        else:\n            self.mask_value = 0.\n\n        if self.pooling_type not in self.POOLING_TYPES:\n            raise ValueError(f'Available types: {\", \".join(self.POOLING_TYPES)}')\n\n    def forward(self, x: Tensor, pad_mask: Tensor) -> Tensor:\n        lengths = pad_mask.sum(self.dim).float()\n\n        x = embedding_masking(x=x, pad_mask=pad_mask, value=self.mask_value)\n\n        if self.pooling_type == 'mean':\n            scaling = x.size(self.dim) \/ lengths\n        else:\n            scaling = torch.ones(x.size(0))\n\n        if self.length_scaling:\n            lengths_factor = lengths\n            if self.scaling_square_root:\n                lengths_factor = lengths_factor ** 0.5\n            scaling \/= lengths_factor\n\n        scaling = scaling.masked_fill(lengths == 0, 1.).unsqueeze(-1)\n\n        if self.pooling_type == 'mean':\n            x = x.mean(self.dim)\n        else:\n            x, _ = x.max(self.dim)\n\n        x *= scaling\n\n        if self.normalize:\n            x = F.normalize(x)\n\n        return x","e364d42b":"class LSTMBiEncoder(nn.Module):\n    \n    def __init__(self, \n                 embedding_matrix, \n                 num_layers=3, \n                 model_dim=512, \n                 dropout=0.25, \n                 freeze_embeddings=True):\n        super().__init__()\n\n        self.embedding_layer = nn.Embedding.from_pretrained(embeddings=embedding_matrix, \n                                                            freeze=freeze_embeddings,\n                                                            padding_idx=0)\n        \n        self.rnn = nn.LSTM(input_size=embedding_matrix.size(1), \n                           hidden_size=model_dim, \n                           num_layers=num_layers,\n                           batch_first=True,\n                           dropout=dropout, \n                           bidirectional=True)\n        \n        self.rnn_projection = nn.Linear(model_dim * 2, model_dim)\n        \n        self.pooling = GlobalMaskedPooling()\n        \n        self.question_encoder = self.get_head(model_dim, dropout)\n        self.response_encoder = self.get_head(model_dim, dropout)\n        \n    def get_head(self, model_dim, dropout):\n        return nn.Sequential(\n            nn.BatchNorm1d(model_dim), \n            nn.Dropout(dropout),\n            nn.Linear(model_dim, model_dim),\n            nn.ReLU(),\n            nn.BatchNorm1d(model_dim), \n            nn.Dropout(dropout),\n            nn.Linear(model_dim, model_dim),\n            nn.ReLU(),\n            nn.BatchNorm1d(model_dim), \n            nn.Dropout(dropout),\n            nn.Linear(model_dim, model_dim)\n        )\n    \n    def backbone_encoding(self, batch):\n        \n        x = self.embedding_layer(batch['input_ids'])\n        \n        pad_mask = batch['attention_mask']\n        \n        x = pack_padded_sequence(x, \n                                 lengths=pad_mask.sum(dim=-1).cpu(),\n                                 batch_first=True, \n                                 enforce_sorted=False)\n        \n        x, _ = self.rnn(x)\n        \n        x, _ = pad_packed_sequence(x, batch_first=True)\n        \n        x = self.pooling(x, pad_mask)\n        \n        x = self.rnn_projection(x)\n        \n        return x\n    \n    def question_encoding(self, batch):\n        return self.question_encoder(self.backbone_encoding(batch))\n    \n    def response_encoding(self, batch):\n        return self.response_encoder(self.backbone_encoding(batch))\n        \n    def forward(self, question, response):\n        \n        question_embeddings = self.question_encoding(question)\n        response_embeddings = self.response_encoding(response)\n        \n        return question_embeddings, response_embeddings","42db842e":"class SoftmaxLoss(nn.Module):\n    \n    def __init__(self):\n        super().__init__()\n        \n        self.criterion = nn.CrossEntropyLoss()\n        \n    def forward(self, question_embeddings, response_embeddings):\n        \n        similarity_matrix = question_embeddings @ response_embeddings.T\n        \n        targets = torch.arange(similarity_matrix.size(0)).to(question_embeddings.device)\n        \n        loss = self.criterion(similarity_matrix, targets) + self.criterion(similarity_matrix.T, targets)\n        \n        return loss","4bef0adf":"# \u0444\u0443\u043d\u043a\u0446\u0438\u044f, \u043a\u043e\u0442\u043e\u0440\u0430\u044f \u043f\u0435\u0440\u0435\u043a\u0438\u0434\u044b\u0432\u0430\u0435\u0442 \u0441\u043f\u0438\u0441\u043e\u043a \u0438\u0437 \u0442\u0435\u043d\u0437\u043e\u0440\u043e\u0432 \u043d\u0430 \u043d\u0443\u0436\u043d\u044b\u0439 \u0434\u0435\u0432\u0430\u0439\u0441\ndef batch_to_device(batch, device):\n    for key, value in batch.items():\n        batch[key] = value.to(device)\n    return batch","01a32ba9":"pretrained_model = 'sberbank-ai\/rugpt3small_based_on_gpt2'","b071f054":"train_dataset = PairsDataset('\/kaggle\/input\/chit-chat-encoders\/train.txt')\ncollater = Collater(tokenizer_name=pretrained_model)\n# \u0434\u0430, \u0443 \u043d\u0430\u0441 \u0431\u043e\u043b\u044c\u0448\u043e\u0439 \u0431\u0430\u0442\u0447 \u0441\u0430\u0439\u0437\ntrain_loader = DataLoader(train_dataset, batch_size=512, shuffle=True, collate_fn=collater)","f4a55438":"# \u043e\u0442 gpt \u043d\u0430\u043c \u043f\u043e\u0442\u0440\u0435\u0431\u0443\u0435\u0442\u0441\u044f \u0442\u043e\u043b\u044c\u043a\u043e \u043c\u0430\u0442\u0440\u0438\u0446\u0430 \u044d\u043c\u0431\u0435\u0434\u0434\u0438\u043d\u0433\u043e\u0432\ngpt_model = AutoModelForCausalLM.from_pretrained(pretrained_model)\nembedding_matrix = gpt_model.transformer.wte.weight.detach()\nembedding_matrix[0] = torch.zeros(embedding_matrix.size(1))\n\n# \u0441\u0434\u0435\u043b\u0430\u0435\u043c \u043c\u043e\u0434\u0435\u043b\u044c\nmodel = LSTMBiEncoder(embedding_matrix)","574f07b7":"device = torch.device('cuda')","f0c9d886":"model.to(device)","2a42e8bd":"# \u0437\u0430\u0434\u0430\u0434\u0438\u043c \u043e\u043f\u0442\u0438\u043c\u0438\u0437\u0430\u0442\u043e\u0440\noptimizer = torch.optim.AdamW(model.parameters())","687a6e65":"# \u0437\u0430\u0434\u0430\u0434\u0438\u043c \u043b\u043e\u0441\u0441\ncriterion = SoftmaxLoss()","dae57252":"n_epochs = 3","663613ec":"for n_epoch in range(1, n_epochs + 1):\n    \n    progress_bar = tqdm(total=len(train_loader), desc=f'Epoch {n_epoch}')\n    \n    losses = list()\n    \n    for n_iter, (question, response) in enumerate(train_loader):\n        \n        question = batch_to_device(question, device)\n        response = batch_to_device(response, device)\n        \n        question_embeddings, response_embeddings = model(question, response)\n        \n        loss = criterion(question_embeddings, response_embeddings)\n        \n        optimizer.zero_grad()\n        loss.backward()\n        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=3.)\n        optimizer.step()\n        \n        losses.append(loss.item())\n        \n        progress_bar.update()\n        progress_bar.set_postfix(loss=np.mean(losses[-100:]))\n        \n    torch.save(model.state_dict(), f'lstm_bi_encoder_state_dict_{n_epoch}.pth')\n        \n    progress_bar.close()","e7fc7db7":"plt.figure(figsize=(20, 10))\nplt.plot(losses)\nplt.title('Loss Values Dynamic')\nplt.xlabel('Step')\nplt.ylabel('Value');","d89e7b42":"class Encoder:\n    \n    def __init__(self, collater, model, batch_size=512, device=None):\n        super().__init__()\n        \n        self.collater = collater\n        self.model = model\n        \n        self.batch_size = batch_size\n        self.device = torch.device('cpu') if device is None else device\n        \n        self.model.to(self.device)\n        self.model.eval()\n        \n    def encoding(self, texts, is_questions=True):\n        \n        tokenized = self.collater.tokenize(texts)\n        \n        tokenized = batch_to_device(tokenized, device=self.device)\n        \n        if is_questions:\n            embeddings = self.model.question_encoding(tokenized)\n        else:\n            embeddings = self.model.response_encoding(tokenized)\n            \n        embeddings = F.normalize(embeddings).detach().cpu()\n            \n        return embeddings\n    \n    def __call__(self, texts, is_questions=True):\n        \n        if len(texts) <= self.batch_size:\n            return self.encoding(texts, is_questions=is_questions)\n        \n        embeddings = list()\n        \n        for batch_index in range(0, len(texts), self.batch_size):\n            batch = texts[batch_index:batch_index + self.batch_size]\n            embeddings.append(self.encoding(batch, is_questions=is_questions))\n            \n        embeddings = torch.cat(embeddings)\n            \n        return embeddings","0485e320":"encoder = Encoder(collater, model, batch_size=4096, device=device)","aca3301d":"data = list()\n\nwith open('\/kaggle\/input\/chit-chat-encoders\/valid.jsonl') as file_object:\n    for line in file_object:\n        data.append(json.loads(line.strip()))","be1a47a5":"unique_responses = set()\n\nfor sample in data:\n    unique_responses.update(sample['candidates'])\n    \nunique_responses = list(unique_responses)","70669b28":"unique_responses_mapper = {text: index for index, text in enumerate(unique_responses)}","cd2ec71a":"questions_embeddings = encoder([sample['question'] for sample in data])\nresponses_embeddings = encoder(unique_responses, is_questions=False)","eb069d46":"prediction = list()\n\nfor i_sample in range(len(data)):\n    question_emb = questions_embeddings[i_sample].unsqueeze(0)\n    candidates_emb = torch.stack(\n        [responses_embeddings[unique_responses_mapper[data[i_sample]['candidates'][i_cand]]]\n         for i_cand in range(len(data[i_sample]['candidates']))])\n    similarity = question_emb @ candidates_emb.T\n    prediction.append(similarity.argmax().item())","ec2efdd9":"with open('submission.csv', 'w') as file_object:\n    file_object.write('Id,Category\\n')\n    for n, p in enumerate(prediction):\n        file_object.write(f'{n},{p}\\n')","4858c760":"### \u041c\u043e\u0434\u0443\u043b\u044c, \u043a\u043e\u0442\u043e\u0440\u044b\u0439 \u0434\u0435\u043b\u0430\u0435\u0442 `max\/mean` \u043f\u0443\u043b\u0438\u043d\u0433 \u0441 \u0443\u0447\u0435\u0442\u043e\u043c \u043f\u0430\u0434\u043e\u0432\n\u0412\u0435\u0434\u044c \u0443\u0441\u0440\u0435\u0434\u043d\u0438\u0442\u044c\n[1, 2, 3, 0, 0]  \n\u041d\u0435 \u0432\u0441\u0435 \u0440\u0430\u0432\u043d\u043e, \u0447\u0442\u043e \u0443\u0441\u0440\u0435\u0434\u043d\u0438\u0442\u044c\n[1, 2, 3]  \n\u041f\u0430\u0434\u044b \u043d\u0430\u043c \u0443\u0447\u0438\u0442\u044b\u0432\u0430\u0442\u044c \u043d\u0435 \u043d\u0443\u0436\u043d\u043e","eb8002f6":"## \u0421\u043e\u0431\u0435\u0440\u0435\u043c \u0438\u0442\u043e\u0433\u043e\u0432\u044b\u0435 \u043f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u0430\u043d\u0438\u044f \u0432 \u0444\u0430\u0439\u043b","02050252":"## \u0421\u0434\u0435\u043b\u0430\u0435\u043c \u0434\u0430\u0442\u0430\u0441\u0435\u0442\n\u0412 \u044d\u0442\u043e\u043c \u0437\u0430\u0434\u0430\u043d\u0438\u0438 \u043c\u043d\u0435 \u0443\u0434\u043e\u0431\u043d\u0435\u0435 \u0437\u0430\u0434\u0430\u0442\u044c \u043f\u0443\u0442\u044c \u043a \u0444\u0430\u0439\u043b\u0443, \u0430 \u0432\u043d\u0443\u0442\u0440\u0438 \u043f\u0443\u0441\u0442\u044c \u0431\u0443\u0434\u0435\u0442 \u0447\u0442\u0435\u043d\u0438\u0435 \u0444\u0430\u0439\u043b\u0430. \u041a \u0442\u043e\u043c\u0443 \u0436\u0435 \u043f\u0443\u0441\u0442\u044c \u0434\u0430\u0442\u0430\u0441\u0435\u0442 \u043e\u0442\u0434\u0430\u0435\u0442 \u043f\u0430\u0440\u044b \u0441\u0442\u0440\u043e\u043a, \u0442\u043e\u043a\u0435\u043d\u0438\u0437\u0438\u0440\u0443\u044e \u044f \u0438\u0445 \u043f\u043e\u0442\u043e\u043c.","d358bd19":"## \u0421\u043e\u0431\u0435\u0440\u0435\u043c \u0443\u043d\u0438\u043a\u0430\u043b\u044c\u043d\u044b\u0435 \u043e\u0442\u0432\u0435\u0442\u044b\n\u0427\u0442\u043e\u0431\u044b \u0431\u044b\u0441\u0442\u0440\u0435\u0435 \u043f\u043e\u0441\u0447\u0438\u0442\u0430\u0442\u044c \u044d\u043c\u0431\u0435\u0434\u0434\u0438\u043d\u0433\u0438","199b21c7":"## \u0421\u0434\u0435\u043b\u0430\u0435\u043c \u043c\u043e\u0434\u0435\u043b\u044c","c8369aad":"## \u041f\u043e\u043b\u0443\u0447\u0438\u043c \u043f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u0430\u043d\u0438\u044f\n\u0418\u043d\u0434\u0435\u043a\u0441 \u0441\u0430\u043c\u043e\u0433\u043e \u0431\u043b\u0438\u0437\u043a\u043e\u0433\u043e \u043e\u0442\u0432\u0435\u0442\u0430 \u0438\u0437 \u043a\u0430\u043d\u0434\u0438\u0434\u0430\u0442\u043e\u0432","e312469a":"## \u041c\u043e\u0434\u0443\u043b\u044c \u0434\u043b\u044f \u043c\u043e\u0434\u0435\u043b\u0438\n\u0412\u043d\u0443\u0442\u0440\u0438 \u0443 \u043d\u0430\u0441 \u0435\u0441\u0442\u044c \u043d\u0430\u0448 backbone - \u044d\u0442\u043e lstm. \u042d\u043c\u0431\u0435\u0434\u0434\u0438\u043d\u0433\u0438 \u0431\u0443\u0434\u0435\u043c \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u044c \u043f\u0440\u0435\u0434\u043e\u0431\u0443\u0447\u0435\u043d\u043d\u044b\u0435, \u043e\u0442 gpt (\u0442\u043e\u0439 \u0432\u0435\u0440\u0441\u0438\u0438, \u043a\u043e\u0442\u043e\u0440\u0443\u044e \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u043b\u0438 \u0434\u043b\u044f \u0442\u043e\u043a\u0435\u043d\u0438\u0437\u0430\u0442\u043e\u0440\u0430).","8d2180bc":"## \u0421\u0434\u0435\u043b\u0430\u0435\u043c \u043b\u043e\u0441\u0441","940cbb90":"## \u0414\u0430\u0432\u0430\u0439\u0442\u0435 \u0441\u0434\u0435\u043b\u0430\u0435\u043c \u0442\u043e\u043a\u0435\u043d\u0438\u0437\u0430\u0442\u043e\u0440\n\u041f\u0443\u0441\u0442\u044c \u0443 \u043d\u0435\u0433\u043e \u0431\u0443\u0434\u0435\u0442 \u043c\u0435\u0442\u043e\u0434\u044b `tokenize` \u0434\u043b\u044f \u0442\u043e\u0433\u043e, \u0447\u0442\u043e\u0431\u044b \u043c\u044b \u043c\u043e\u0433\u043b\u0438 \u043f\u043e\u0434\u0430\u0442\u044c \u0441\u043f\u0438\u0441\u043e\u043a \u0442\u0435\u043a\u0441\u0442\u043e\u0432 \u0438 \u043f\u043e\u043b\u0443\u0447\u0438\u0442\u044c \u043d\u0443\u0436\u043d\u0443\u044e \u043d\u0430\u043c \u0438\u043d\u0444\u043e\u0440\u043c\u0430\u0446\u0438\u044e (\u0438\u043d\u0434\u0435\u043a\u0441\u044b \u0441\u043b\u043e\u0432, \u043f\u0430\u0434 \u043c\u0430\u0441\u043a\u0443) \u0438 `collate` \u0434\u043b\u044f \u0442\u043e\u0433\u043e, \u0447\u0442\u043e\u0431\u044b \u043e\u0431\u0440\u0430\u0431\u0430\u0442\u044b\u0432\u0430\u0442\u044c \u0431\u0430\u0442\u0447\u0438, \u044d\u0442\u0443 \u0444\u0443\u043d\u043a\u0446\u0438\u044f \u043d\u0443\u0436\u043d\u0430 \u0434\u043b\u044f `collate_fn` \u0432 `DataLoader`.","ae3167ce":"## \u0412\u0435\u043a\u0442\u043e\u0440\u0438\u0437\u0443\u0435\u043c \u0442\u0435\u043a\u0441\u0442\u044b","f64242aa":"## \u0421\u0434\u0435\u043b\u0430\u0435\u043c \u0443\u0434\u043e\u0431\u043d\u0435\u0435 \u0438\u043d\u0444\u0435\u0440\u0435\u043d\u0441\n\u0422\u0430\u043a\u043e\u0439 \u043a\u043b\u0430\u0441\u0441, \u043a\u0443\u0434\u0430 \u043f\u043e\u0434\u0430\u0435\u0448\u044c `collater` \u0438 `model` \u0438 \u043f\u043e \u0432\u044b\u0437\u043e\u0432\u0443 \u044d\u0442\u043e\u0433\u043e \u043a\u043b\u0430\u0441\u0441\u0430 \u043c\u043e\u0436\u043d\u043e \u0432\u0435\u043a\u0442\u043e\u0440\u0438\u0437\u043e\u0432\u0430\u0442\u044c \u0442\u0435\u043a\u0441\u0442\u044b.","0ec7b9e1":"## \u041e\u0431\u0443\u0447\u0435\u043d\u0438\u0435\n\u0414\u043e\u0431\u0430\u0432\u044c\u0442\u0435 \u0435\u0449\u0435 \u0432\u0430\u043b\u0438\u0434\u0430\u0446\u0438\u044e, \u0447\u0442\u043e\u0431\u044b \u043d\u0435 \u043f\u0435\u0440\u0435\u043e\u0431\u0443\u0447\u0438\u0442\u044c\u0441\u044f"}}