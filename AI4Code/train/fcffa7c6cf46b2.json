{"cell_type":{"a73c637e":"code","ac716a74":"code","fbd28328":"code","dbcdab52":"code","737f47a0":"code","1b557d5e":"code","a2d12f66":"code","9f57d9f3":"code","0f66525b":"code","27d3e916":"code","4002fcb8":"code","c9fb7641":"code","c9ccbc4c":"code","5a95346c":"code","20dd90a8":"code","68d20efa":"code","ec1e085b":"code","78188de2":"code","6201d30c":"code","e9c41c26":"code","bb5f6e5e":"code","7f341108":"code","cedd5185":"code","d2171f30":"code","b0f4cc36":"code","ed38b02f":"code","0d3eaa43":"code","b308ec3c":"code","545f806b":"code","71fe2acd":"code","62f51ced":"code","5a93095b":"code","6504abfc":"code","4789a826":"code","39de1454":"code","bbe9f8e5":"code","823c05e0":"code","c7c1eef8":"code","e6b79dc9":"code","93abe008":"code","39b783df":"code","4577de08":"code","1a1e45a0":"code","4778e887":"code","84320de5":"code","536b60d3":"code","fb5c34a1":"code","f6e5130e":"code","44b9e6a5":"code","3b8d2331":"code","2aef239b":"code","e669e36e":"code","657b26aa":"code","246a0c35":"code","e3bfa072":"code","1383e863":"code","6dc56a11":"code","f62b0258":"code","5908b9cd":"code","5cfe4016":"markdown","c3a98055":"markdown","f0e4b2b7":"markdown","e253db2c":"markdown","b7d91bdf":"markdown","157bb5ff":"markdown","70ea419b":"markdown","e7b8bc16":"markdown","57e674be":"markdown","5506dbd0":"markdown","ab83412d":"markdown","2d40e9b4":"markdown","38889fdd":"markdown","e90ba1b0":"markdown","4310f063":"markdown","e0bcf834":"markdown","5ec71270":"markdown","a54c5c9a":"markdown","1f5ef8c3":"markdown","5c35bfd4":"markdown","dfbb2683":"markdown","957a1258":"markdown","9f85994d":"markdown","25a7d5d3":"markdown","ac276219":"markdown","a7053d67":"markdown","c58c3e87":"markdown","59372b65":"markdown","aaffb1ef":"markdown","65a5763e":"markdown","03164c43":"markdown","ac5fb4d2":"markdown","ae07e12a":"markdown","261ba40b":"markdown","e3d74ed7":"markdown","d95cc64d":"markdown","d4282f96":"markdown"},"source":{"a73c637e":"import pandas as pd\nimport numpy as np\nimport os","ac716a74":"path = r'..\/input\/'","fbd28328":"anime = 'anime.csv'\nratings = 'rating.csv'","dbcdab52":"anime_path = os.path.join(path, anime)\nratings_path = os.path.join(path,ratings)","737f47a0":"df_anime = pd.read_csv(anime_path)\ndf_ratings = pd.read_csv(ratings_path) ","1b557d5e":"df_anime.shape","a2d12f66":"df_ratings.shape","9f57d9f3":"df_ratings.info()","0f66525b":"df_anime.info()","27d3e916":"n_users = df_ratings.user_id.unique().shape[0]\nn_users","4002fcb8":"n_items = df_ratings.anime_id.unique().shape[0]\nn_items","c9fb7641":"print(len(set(df_anime.anime_id)))","c9ccbc4c":"df_ratings.groupby('user_id').rating.count()","5a95346c":"df_anime[df_anime['genre'].isnull()].sort_values('members', ascending = False).sample(10)","20dd90a8":"df_anime[df_anime['type'].isnull()].sort_values('members', ascending = False).sample(10)","68d20efa":"df_anime[df_anime['rating'].isnull()].sort_values('members', ascending = False).sample(10)","ec1e085b":"df_anime = df_anime.replace('Unknown', np.nan)\ndf_anime = df_anime.dropna(how = 'all')\ndf_anime['type'] = df_anime['type'].fillna('TV')\ndf_anime['episodes'] = df_anime['episodes'].map(lambda x:np.nan if pd.isnull(x) else int(x))\ndf_ratings = df_ratings.replace(-1, np.nan)","78188de2":"df_anime[df_anime['anime_id']==841]","6201d30c":"from matplotlib import pyplot as plt\nimport seaborn as sns; sns.set(style = 'white', palette = 'muted')","e9c41c26":"sns.pairplot(data=df_anime[['type','rating','episodes','members']].dropna(),hue='type')","bb5f6e5e":"%matplotlib inline\nplt.hist(df_anime['rating'].fillna(0))","7f341108":"df_anime['rating'] = df_anime['rating'].fillna(df_anime.rating.median())","cedd5185":"plt.hist(df_anime['rating'])","d2171f30":"pd.DataFrame(df_ratings.groupby('rating').user_id.count()).reset_index()","b0f4cc36":"sns.boxplot(data = df_anime, y = 'rating', x='type')","ed38b02f":"plt.hist(df_ratings.groupby(['anime_id'])['anime_id'].count())","0d3eaa43":"sns.scatterplot( x = df_anime['episodes'], y= df_anime['rating'])","b308ec3c":"full_df = pd.merge(df_anime, df_ratings, how = 'right', on ='anime_id', suffixes = ['_avg', '_user'])\nfull_df.rename(columns = {'rating_user':'user_rating', 'rating_avg':'avg_rating'}, inplace = True)","545f806b":"full_df.sample(10)","71fe2acd":"df_col = full_df[['user_id', 'name', 'user_rating']]\ndf_col.head()","62f51ced":"df_genres_list = df_anime['genre'].str.get_dummies(sep = ', ')","5a93095b":"corr = df_genres_list.corr()\n\nmask = np.zeros_like(corr, dtype=np.bool)\nmask[np.triu_indices_from(mask)] = True\n\nf, ax = plt.subplots(figsize=(11, 9))\n\ncmap = sns.diverging_palette(220, 10, as_cmap=True)\n\nsns.heatmap(corr, mask=mask, cmap=cmap, vmax=1, center=0,\n            square=True, linewidths=.5, cbar_kws={\"shrink\": .5})","6504abfc":"df_genres_list.sample(10)","4789a826":"df_types_list = pd.get_dummies(df_anime[[\"type\"]])\ndf_types_list.sample(10)","39de1454":"df_types_list.sample(10)","bbe9f8e5":"df_feat = df_anime[['members','rating','episodes']]","823c05e0":"df_features = pd.concat([df_feat,df_genres_list, df_types_list], axis = 1).fillna(0)","c7c1eef8":"df_anime[df_anime['anime_id']==5114]","e6b79dc9":"def get_nombre_from_index(index):\n    return df_anime[df_anime.index == index]['name'].values[0]\ndef get_id_from_nombre(name):\n    return df_anime[df_anime.name == name]['anime_id'].values[0]\ndef get_index_from_id(anime_id):\n    return df_anime[df_anime.anime_id == anime_id].index.values[0]","93abe008":"#Obtendremos el promedio de las valoraciones que el usuario ha dado a las series para determinar si le gustan, y le recomendaremos series similares a sus favoritas o mejor valoradas.. \ndef get_user_top_list(user):\n    df_user = df_ratings[df_ratings['user_id']==user]\n    df_rated = df_user.dropna(how = 'any')\n    avg =  df_rated.rating.mean() \n    df_toplist = df_rated[df_rated['rating']>= avg].sort_values('rating', ascending = False).head(10)\n    return list(df_toplist['anime_id'])\ndef get_user_viewed_list(user):\n    return list(df_ratings[df_ratings['user_id']==user]['anime_id'])","39b783df":"from sklearn.neighbors import NearestNeighbors\nfrom sklearn.preprocessing import MaxAbsScaler","4577de08":"mas = MaxAbsScaler()\ndf_features2 = mas.fit_transform(df_features)","1a1e45a0":"k = 11","4778e887":"neighbors_content = NearestNeighbors(n_neighbors = k, algorithm = 'ball_tree')","84320de5":"neighbors_content.fit(df_features2)","536b60d3":"distances, indices = neighbors_content.kneighbors(df_features2)","fb5c34a1":"distances.shape","f6e5130e":"indices.shape","44b9e6a5":"from numpy import random","3b8d2331":"series = np,random.randint(0,len(indices))\nprint(series[1])\nname = get_nombre_from_index(series[1])\nprint(name)","2aef239b":"aid = get_id_from_nombre(name)","e669e36e":"ind = get_index_from_id(aid)","657b26aa":"anime = ind\nlist(indices[anime,1:11])","246a0c35":"def get_recommendations(aid):\n    anime =  get_index_from_id(aid)\n    test = list(indices[anime,1:11])\n    nb = []\n    for i in test:\n        a_name = get_nombre_from_index(i)\n        nb.append(a_name)\n    return nb","e3bfa072":"get_user_top_list(73509)","1383e863":"get_recommendations(23283)","6dc56a11":"def get_n_recommends(user, n):\n    vistas = list(get_user_viewed_list(user))\n    liked = list(get_user_top_list(user))\n    lista = []\n    for i in liked:\n        ani = pd.Series(get_recommendations(i))\n        recs = np.setdiff1d(ani, vistas) \n        lista.extend(recs)\n        if(len(lista) > n):\n            lista = lista[n:]\n            break\n    return lista","f62b0258":"get_n_recommends(10,5)","5908b9cd":"get_n_recommends(73509, 10)","5cfe4016":"* Creamos el dataset de entrenamiento final para contenido","c3a98055":"* Extraemos un subset para el filtro colaborativo","f0e4b2b7":"## Representaciones Matriciales","e253db2c":"* Extraemos las series que el usuario ya haya visto","b7d91bdf":"* Validamos que el dataset haya quedado bien construido","157bb5ff":"* Tomamos una vision general del comportamiento de las columnas del dataset de anime","70ea419b":"* Buscamos los valores nulos en genero, tipo y ratings","e7b8bc16":"* Se usa k = K+1 siendo K el numero de recomendaciones que se desea obtener, ya que la primera siempre es el mismo dato\n* Como tenemos variables dummy binarias vs variables con valor muy alto (episodios, miembros) usaremos la biblioteca MaxAbsScaler para convertir dichos valores en una distribucion 0-1. Equivale a normalizar con funcion Z","57e674be":"* Veamos especificamente cuantas valoraciones de cada tipo existen","5506dbd0":"### Observamos que el archivo de ratings tiene todas sus entradas en orden, segun el readme, y por que los valores fueron llenados con -1 en caso de no existir valoracion para el usuario.","ab83412d":"* En caso de existir menos series recomendadas ya que el usuario ha visto muy pocas no se rellenara con contenido relevante. Puede considerarse una posible mejora.","2d40e9b4":"* Procedemos a llenar los valores vacios con valores que podamos trabajar, en el caso de genero y episodios, no podemos llenar los valores faltantes por que no conocemos el dato, por lo que lo dejamos en NaN. \n* En caso de tipo, dada la baja cantidad de series, consideraremos estas como TV. \n* En el caso de rating, veremos si el sistema lo incluye a pesar de estar vacio, ya sea llenandolo con una medida o calculandolo.","38889fdd":"* Extraeremos una serie al azar para evaluar el modelo y ver como se comporta","e90ba1b0":" * Y tambien cuantos usuarios han visto cada serie","4310f063":"* De acuerdo a una busqueda rapida en Google, y el readme de este dataset que es del 2016, las series sin rating no han sido evaluadas ya que o se encuentran aun en emision o no han sido lanzadas, por lo que trataremos que nuestro sistema tambien sea capaz de recomendarlas. ","e0bcf834":"* Para modelar la recomendacion extraemos los usuarios con mas valoraciones como prueba (usaremos el 42635)","5ec71270":"* Y probamos para un usuario cualquiera de acuerdo a la lista anterior","a54c5c9a":"* Notamos que no varia la distribucion de los ratings por lo que la funcion sigma no se vera mayormente afectada, sin embargo esto entregara mayor precision al sistema","1f5ef8c3":"## Para el filtro de Contenido","5c35bfd4":"* A partir de este analisis sabremos que existen 73515 usuarios quienes evaluaron 11200 Animes distintos de la lista de 12294 animes unicos","dfbb2683":"* Por ultimo recomendamos series hasta que encontremos n series que el usuario no ha visto","957a1258":"### Veremos la comparacion entre la recomendacion y la serie","9f85994d":"* Otro dato curioso, tambien observamos que a mayor cantidad de episodios que tenga una serie, tienden a tener un score parecido, o un comportamiento relativamente lineal, por lo que se espera que el sistema asocie las series largas con otras series largas","25a7d5d3":"## Conclusiones\n\n* El modelo es capaz de relacionar series en base a su contenido, recomendando contenido que comparte los generos y ratings del mismo. Sera necesario medir como se comporta con outliers via LightFM WARP method.\n\n* En vista de la brevedad del tiempo de construccion, se plantean las siguientes mejoras posibles:\n\n    * Realizar clustering the usuarios para mejorar recomendaciones\n    * Extraer un ponderado de filtro colaborativo y filtro de contenido para sacar un score final (R. Lineal o Random Forest)\n    * Realizar una prediccion estimada de las series que no poseen rating de acuerdo a su contenido.\n    * Utilizar sigmoides en tensorflow. \n    * Se pueden utilizar sugerencias aleatorias en vez de una toplist\n    * Se puede normalizar el score de las series para sugerir algunas que puedan estar bajo el promedio","ac276219":"## Modelado por KNN (Contenido)","a7053d67":"* A partir de este grafico podemos concluir que si queremos reducir los generos podriamos remapear 'Space' y 'Mecha' como Sci-Fi, junto algunas otras categorias, sin embargo por mantener la logica bayesiana no sera del alcance de este modelo","c58c3e87":"* Esto crea un sesgo en el sistema de recomendacion, ya que este constantemente recomendara series que hayan sido mas vistas sobre las que menos, veremos como podemos resolver esta situacion.","59372b65":"* Tenemos que el 80% de las evaluaciones se concentran sobre el valor de 4. Reemplazando el -1 por 0 no cambia esta distribucion.\n* Veamos si se mantiene al separarlos por tipo","aaffb1ef":"* Consideraremos el valor promedio de las series que el usuario haya evaluado","65a5763e":"* Al analizar los datos de ratings, podemos observar que estos tienen una distribucion leptocurtica (datos agrupados en el centro) distribuida entre 3 y 9, con algunos outliers observables. Veremos si esto se da de acuerdo a la mayor cantidad de gente que ha visto una determinada serie, u otra variable, pero sera necesario normalizar esta variable\n\n* Esto significa que si vamos a llenar los valores faltantes sera necesario llenarlos usando la mediana y no el promedio","03164c43":"# Sistema de recomendacion de Anime\n* Construido por razorhedge (github) para Option utilizando el dataset de kaggle: \n* https:\/\/www.kaggle.com\/CooperUnion\/anime-recommendations-database","ac5fb4d2":"### En el caso del dataset de series, encontramos que existen 62 entradas sin genero, 25 sin tipo, y 230 sin rating.","ae07e12a":"* Llenamos los valores con la mediana para que el analisis sin rating no pese tanto","261ba40b":"* Creamos funciones de Apoyo","e3d74ed7":"## Exploracion de Datos\n* En este caso, se conoce los tama\u00f1os de los dataset por informacion previa, por lo que no se necesita utilizar mayor exploracion de tama\u00f1o, sin embargo, se incluira por efectos de estandarizacion. ","d95cc64d":"* Unimos los datasets para extraer subsets de entrenamiento","d4282f96":"* En general vemos que la distribucion por tipo es mas extensa de acuerdo a si es serie o OVA, por lo que se priorizara la categorizacion por genero."}}