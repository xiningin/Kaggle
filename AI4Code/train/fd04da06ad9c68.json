{"cell_type":{"686fc6ae":"code","7d70ddf7":"code","6f3ec072":"code","365e4797":"code","15ed9277":"code","a22e11eb":"code","384c9e19":"code","5c389a69":"code","42c15dc3":"code","4a884022":"code","760c5c52":"code","1c7a8253":"code","7c120751":"code","f6a7bef9":"code","404109f8":"code","785bc5ab":"code","9e4d10c7":"code","de618611":"code","044d479f":"code","8222c2b9":"code","b14673e3":"code","86a88039":"code","c44c1144":"code","db04bee7":"code","fa9fe7e1":"code","9489f31c":"code","59d7f370":"code","281e2351":"code","d220be10":"code","ee01bf84":"code","ea96171b":"code","b1d1cf9c":"code","deeb4d88":"code","d4490a8b":"code","311546f3":"code","250b5b1a":"code","f03d4b70":"code","aa6ddf62":"code","720773d0":"code","4bc89295":"code","2fa405ee":"code","b34c4e94":"code","f56ab768":"code","342d7cf4":"code","2034bbbb":"code","19290df0":"code","8eec08a7":"code","4245820f":"code","f8b0419f":"code","21110015":"code","8aa17146":"code","2783fc8c":"code","8f3a8024":"code","5b1297b1":"code","f7915370":"code","b4dc43d5":"code","4ef208de":"code","5f5c025d":"code","e132198b":"code","1b9bebcf":"code","d5fec93c":"code","21e2f03a":"code","aa21bcd9":"code","a5db82d4":"markdown","32b8edbf":"markdown","e44c1071":"markdown","d32b0ef0":"markdown","30e799b4":"markdown","3dd916c5":"markdown","7e78cc17":"markdown"},"source":{"686fc6ae":"import pandas as pd \nimport numpy as np \nimport matplotlib.pyplot as plt \nimport seaborn as sns \n\n#Import Library ","7d70ddf7":"df =  pd.read_csv('..\/input\/bus-breakdown-and-delays.csv')\n #Import dataset ","6f3ec072":"df.head() #target variable isn't a number, will need to do regex to extract relevant information ","365e4797":"df.info() #See quick summary of data- 328k total rows, but some columns have missingg values ","15ed9277":"df.isna().sum()\/df['School_Year'].count()","a22e11eb":"sns.heatmap(df.isnull()) #See distribution of missing data \nplt.figsize = (5,2.5)\nplt.tight_layout()\nplt.title('Distribution of Missing Data by Variable ')\n\n#Incident number has high number of NAs ","384c9e19":"df = df.drop(['Incident_Number'], axis = 1) #Drop incident number, most of column is missing\ndf.info()","5c389a69":"df_clean = df.dropna() #Drop remainaing NAs\ndf_clean.info()","42c15dc3":"df_clean.head(100)","4a884022":"df_clean['Delay'] = df_clean['How_Long_Delayed'].str.extract('(\\d+)') #Extract digits from string column \ndf_clean.head() #Check if regex worked- Yes!","760c5c52":"df_clean[df_clean['Delay'].isnull()] #Check if data is null\n#We see that there's question marks or other irregularaties- lets drop this data \n","1c7a8253":"df_clean = df_clean.dropna() #Drop new NAs \ndf_clean.isnull().sum() #Check that no NAs are left ","7c120751":"df_clean['Delay'] =  pd.to_numeric(df_clean['Delay']) #Convert string to integer ","f6a7bef9":"df_clean = df_clean.drop(['How_Long_Delayed'], axis = 1) #Drop original column ","404109f8":"reasons = pd.pivot_table(df_clean, index = 'Reason', values = 'Delay', aggfunc = [np.mean, np.max,np.size]).sort_values(by = \n                                                                                                    ('mean', 'Delay'), \n                                                                                        ascending = False)\nplt.figure(figsize = (5,50))\nreasons.plot(kind = 'bar', y = ('mean','Delay'), color = 'lightblue')\nplt.title('Average Delay in Minutes')\nplt.legend().remove()\nplt.xticks(rotation = 80)\n\n#See size distribution by reason\nplt.figure(figsize = (5,50))\nreasons.plot(kind = 'bar', y = ('size','Delay'), color = 'lightblue')\nplt.title('Number of Delays')\nplt.legend().remove()\nplt.xticks(rotation = 80)\n","785bc5ab":"sns.boxplot( x = df_clean['Delay'])\n#Looks like two clear outliers- 1 around 50,000 and the other around 200,000 Let's remove \n","9e4d10c7":"df_exoutliers = df_clean[df_clean['Delay'] < 50000]\nsns.boxplot(x = df_exoutliers['Delay']) #Check if we need to remove further outliers ","de618611":"df_clean.head()","044d479f":"df_clean['Route_Number'].value_counts()","8222c2b9":"pd.pivot_table(df_clean, index = 'Route_Number', values = 'Delay', aggfunc = [np.mean,np.size]).sort_values(by = \n                                                                                                           ('size','Delay'), \n                                                                                                           ascending = False).head(6)","b14673e3":"routes = ['1','2','3','5','4','6']\ntop_routes = df_clean[df_clean['Route_Number'].isin(routes)] #Filter to see cases where route is top 6 in # of delays","86a88039":"routes_pivot = pd.pivot_table(top_routes, index = 'Route_Number', values = 'Delay', aggfunc = [np.mean,np.size])\nroutes_pivot.head(6)","c44c1144":"df_clean.head()","db04bee7":"df_clean['Bus_Company_Name'].value_counts()","fa9fe7e1":"#First Let's remove unnecessary features, checking 1 by 1 \n\n#School Year- is it relevant? \ndf_clean['School_Year'].value_counts().plot(kind = 'bar')\nplt.xticks(rotation = 75) #Make Data cleaner to read \n\n#See an increasing trend year on year in quantity-let's investigate if there's any significant deviations in delay by year\n","9489f31c":"#Let's first see average delay, across the dataset \ndf_clean['Delay'].mean() #Around 29 mins is the average delay time ","59d7f370":"pd.pivot_table(df_clean, index = 'School_Year', values = 'Delay', aggfunc = np.mean).plot(kind = 'bar')\nplt.legend().remove() #Get rid of legend \nplt.title('Average Delay by Year')\nplt.xticks(rotation = 75) #Make easier to read \n\n#Doesn't look any year is terribly far off from another but also not congruent- will keep for now ","281e2351":"df_clean.head() #Let's check what the data looked like again ","d220be10":"df_clean['Busbreakdown_ID'].value_counts() #Data seems like no noise, we'll drop ","ee01bf84":"df_clean = df_clean.drop(['Busbreakdown_ID'], axis = 1)\ndf_clean.head()","ea96171b":"bus_num = pd.pivot_table(df_clean, index = 'Bus_No', values = 'Delay',aggfunc = np.size).sort_values(by = 'Delay', \n                                                                                                    ascending = False)\nbus_num\n\n#Create pivot to see number of delays by bus number- we see that a lot have only have 1. \n#Instead of one hot encoding, let's just convert to digits \n","b1d1cf9c":"df_clean['Bus_Number'] = df_clean['Bus_No'].str.extract('(\\d+)') #Extract digits from string column \n\ndf_clean['Bus_Number'] =  pd.to_numeric(df_clean['Bus_Number']) #Convert string to integer \ndf_clean.isnull().sum() #We now have some more NAs- let's do a quick investigation \n","deeb4d88":"df_clean[df_clean['Bus_Number'].isnull()] #Looks like noisy data, will drop \ndf_clean = df_clean.dropna()\ndf_clean = df_clean.drop(['Bus_No'], axis = 1) #Drop original column ","d4490a8b":"df_clean.head() #Look familiar? ","311546f3":"df_clean.corr() #Let's look at the current correlation across features","250b5b1a":"df_clean['Run_Type'].value_counts().plot(kind = 'bar') \nplt.title('Trip distribution ')\nplt.xticks(rotation = 75) #Data heavily weighted towards Special Ed AM in terms of quantity \n","f03d4b70":"df_clean.head()","aa6ddf62":"df_clean.nunique() #See number of unique values per feature ","720773d0":"from sklearn.model_selection import train_test_split\n\ny = df_clean['Delay'] #store target variable\ndf_model = df_clean.drop(['Delay'], axis = 1)\nX = df_model[['Run_Type','Reason','Boro','Number_Of_Students_On_The_Bus','Breakdown_or_Running_Late',\n             'School_Age_or_PreK']] #store some basic features\n","4bc89295":"dummy_df = pd.get_dummies(X) #Convert data to dummies to enable modeling\n","2fa405ee":"print(dummy_df.shape) \nprint(y.shape)\n\n#Check that number of rows match number of labels (target) ","b34c4e94":"#Split Data into test, train \nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test,y_train, y_test = train_test_split(dummy_df,y,test_size = .2, random_state = 40) #Split into 20% test data","f56ab768":"from sklearn.ensemble import RandomForestRegressor \nmodel = RandomForestRegressor(n_estimators = 100) #create model \nmodel.fit(X_train,y_train) #Run model on training set","342d7cf4":"predictions = model.predict(X_test) #Predict on testing set","2034bbbb":"from sklearn import metrics \n\nprint('MAE:', metrics.mean_absolute_error(y_test,predictions)) #We see that average error is 12.92 mins- compared to the naive guess of 28 mins","19290df0":"df_clean.head()","8eec08a7":"from sklearn.model_selection import train_test_split\n\ny = df_clean['Delay'] #store target variable\nX = df_model[['School_Year','Run_Type','Reason','Boro','Bus_Company_Name','Number_Of_Students_On_The_Bus','Breakdown_or_Running_Late',\n             'School_Age_or_PreK']] #Added bus company name\/school year features\ndummy_df = pd.get_dummies(X) #Convert data to dummies to enable modeling\nprint(dummy_df.shape)\nprint(y.shape)\n\n#Shape of both datasets is matching, ok to proceed to next step ","4245820f":"#Split Data into test, train \nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test,y_train, y_test = train_test_split(dummy_df,y,test_size = .2, random_state = 40) #Split into 20% test data","f8b0419f":"#Run model again \n\nfrom sklearn.ensemble import RandomForestRegressor \nmodel = RandomForestRegressor(n_estimators = 150) #create model \nmodel.fit(X_train,y_train) #Run model on training set","21110015":"from sklearn import metrics \n\npredictions = model.predict(X_test) #Predict on testing set\nprint('MAE:', metrics.mean_absolute_error(y_test,predictions)) #We see that average error is 11.13 mins- down from the last model!","8aa17146":"feature_values = pd.DataFrame(model.feature_importances_,\n                              index = X_train.columns,\n                              columns = ['importance']).sort_values('importance',\n                                                                    ascending=False)\nfeature_values.head(8).plot(kind = 'bar', color = 'lightgreen')\nplt.xticks(rotation = 85)\nplt.title('Feature Importance')\n","2783fc8c":"df_clean['Size'] = df_clean.groupby('Schools_Serviced')['Delay'].transform(len) #Create a column to see count of Bus\ndf_clean.head()\n\n","8f3a8024":"df_clean['Size'].nunique() #365 unique values for Schools Serviced","5b1297b1":"df_clean[df_clean['Size'] > 500]['Size'].nunique() #Cut down number of unique schools serviced to 58 by filtering value count","f7915370":"df_clean['Schools_Serviced2'] = np.where(df_clean['Size'] > 500,df_clean['Schools_Serviced'], 'other')\n","b4dc43d5":"\ny = df_clean['Delay'] #store target variable\nX = df_clean[['School_Year','Run_Type','Reason','Boro','Bus_Company_Name','Number_Of_Students_On_The_Bus','Breakdown_or_Running_Late',\n             'School_Age_or_PreK','Schools_Serviced2']] #added additional feature\ndummy_df = pd.get_dummies(X) #look familiar? \nX_train, X_test,y_train, y_test = train_test_split(dummy_df,y,test_size = .2, random_state = 40) #Split into 20% test data\n","4ef208de":"model = RandomForestRegressor(n_estimators = 150) #create model \nmodel.fit(X_train,y_train) #Run model on training set","5f5c025d":"predictions = model.predict(X_test) #Predict on testing set\nprint('MAE:', metrics.mean_absolute_error(y_test,predictions)) #Down to 10.99- can we get to single digits? ","e132198b":"pd.pivot_table(df_clean, index = 'Has_Contractor_Notified_Schools', values = 'Delay', aggfunc = np.mean)\n","1b9bebcf":"pd.pivot_table(df_clean, index = 'Has_Contractor_Notified_Parents', values = 'Delay', aggfunc = np.mean)","d5fec93c":"y = df_clean['Delay'] #store target variable\nX = df_clean[['School_Year','Run_Type','Reason','Boro','Bus_Company_Name','Number_Of_Students_On_The_Bus','Breakdown_or_Running_Late',\n             'School_Age_or_PreK','Schools_Serviced2', 'Has_Contractor_Notified_Parents','Has_Contractor_Notified_Schools']] \n    #added additional features related to contractors \ndummy_df = pd.get_dummies(X) #look familiar? \nX_train, X_test,y_train, y_test = train_test_split(dummy_df,y,test_size = .2, random_state = 40) #Split into 20% test data","21e2f03a":"model = RandomForestRegressor(n_estimators = 150) #create model \nmodel.fit(X_train,y_train) #Run model on training set","aa21bcd9":"predictions = model.predict(X_test) #Predict on testing set\nprint('MAE:', metrics.mean_absolute_error(y_test,predictions)) ","a5db82d4":"Let's take a look at the importance of the features. ","32b8edbf":"Re-run Model ","e44c1071":"Before we do more data wrangling, let's do some EDA! ","d32b0ef0":"**Machine Learning Model Prep **","30e799b4":"Let's dive into the contractor notification features ","3dd916c5":"Let's change the model up by adding more features and check out the impact ","7e78cc17":"Let's work with the Schools Serviced Feature "}}