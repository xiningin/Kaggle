{"cell_type":{"3e1d8955":"code","b70482b5":"code","d5c9b10f":"code","69a2eea9":"code","89953e00":"code","7b5179c3":"code","439432ca":"code","047e9031":"code","8f154364":"code","dedd7f7f":"code","b1df8f84":"code","1532c20a":"code","b5517989":"code","74922a60":"code","0ed05378":"code","cf1ee623":"code","897d66b2":"code","6d33a4ea":"code","e9d173c7":"code","d4b6e51b":"code","5885074c":"code","74e92405":"code","8c60a213":"code","cfa267a1":"code","5449a76d":"code","bd42dee6":"code","37b7f37c":"code","707fcd09":"code","a4cf084a":"code","ddf3872b":"code","8809cbb3":"code","03d500ce":"code","e4f7a570":"code","8b61da31":"markdown","be5d0768":"markdown","989a8ae0":"markdown","620db80b":"markdown","9a73f22c":"markdown","3ed20824":"markdown"},"source":{"3e1d8955":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt","b70482b5":"# !pip3 install -q -U keras-tuner","d5c9b10f":"import kerastuner as kt\n\nfrom nltk.corpus import stopwords \nfrom nltk.tokenize import word_tokenize \nstop_words = set(stopwords.words('english')) \n  \nimport tensorflow as tf\nfrom tensorflow.keras.preprocessing.text import Tokenizer \nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\n\nfrom sklearn.model_selection import train_test_split\n","69a2eea9":"vocab_size = 2_000\nembedding_dim = 100\nmax_length = 1403","89953e00":"train_df = pd.read_csv(\"..\/input\/jigsaw-toxic-comment-classification-challenge\/train.csv.zip\")\nprint(train_df.shape)\ntrain_df[:3]","7b5179c3":"test_df = pd.read_csv(\"..\/input\/jigsaw-toxic-comment-classification-challenge\/test.csv.zip\")\nprint(test_df.shape)\ntest_df[:3]","439432ca":"target_cols = np.array(['toxic','severe_toxic',\n                        'obscene', 'threat',\n                        'insult', 'identity_hate'])","047e9031":"# remove stop words\ndef remove_stopwords(sent):\n    word_tokens = word_tokenize(sent) \n    filtered_sentence = [w for w in word_tokens if not w in stop_words]\n    return \" \".join(filtered_sentence)","8f154364":"# train_df.comment_text=train_df.comment_text.apply(remove_stopwords)\n# test_df.comment_text=test_df.comment_text.apply(remove_stopwords)","dedd7f7f":"def concat_labels(row):\n    label_idx = np.where(row)[0]\n    # print(label_idx, len(label_idx), row.index)\n    \n    if len(label_idx)>0:\n        return \" \".join(row.index[label_idx].tolist())\n    else:\n        return \"none\"\n\ndef onehot_labels(row):\n        return row.astype(\"int\").values.reshape(1, -1)\n    \n# (train_df[target_cols]>0)[:12].apply(concat_labels,1)\n(train_df[target_cols]>0)[:12].apply(onehot_labels,1)","b1df8f84":"idx_x, idx_y = np.where(train_df[target_cols]>0)\nlen(idx_x)","1532c20a":"sentences = train_df[\"comment_text\"].tolist()","b5517989":"test_sentences = test_df[\"comment_text\"].tolist()","74922a60":"# labels = (train_df[target_cols]>0).apply(concat_labels,1)\nlabels = (train_df[target_cols]>0).apply(onehot_labels,1)\nlabels_npy = np.concatenate(labels.values, axis=0)","0ed05378":"tokenizer = Tokenizer(oov_token = \"<OOV>\", num_words=vocab_size)\ntokenizer.fit_on_texts(sentences)\nword_index = tokenizer.word_index\nprint(len(word_index))","cf1ee623":"pd.Series(word_index)","897d66b2":"sequences = tokenizer.texts_to_sequences(sentences) # Your Code Here\npadded = pad_sequences(sequences, padding = 'post')  # Your Code here\nprint(padded[0])\nprint(padded.shape)","6d33a4ea":"test_sequences = tokenizer.texts_to_sequences(test_sentences) # Your Code Here\ntest_padded = pad_sequences(test_sequences, padding = 'post', maxlen=max_length)  # Your Code here\nprint(test_padded[0])\nprint(test_padded.shape)","e9d173c7":"print(len(padded[0]))\nprint(len(test_padded[0]))","d4b6e51b":"# v1 labels\n# label_tokenizer = Tokenizer()\n# label_tokenizer.fit_on_texts(labels)\n# label_word_index = label_tokenizer.word_index\n# label_seq = label_tokenizer.texts_to_sequences(labels)\n# print(label_seq[:5])\n# print(label_word_index)","5885074c":"tf.keras.backend.clear_session()\nmodel = tf.keras.Sequential([\n# YOUR CODE HERE\n    tf.keras.layers.Input(max_length),\n    tf.keras.layers.Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=max_length),\n#     tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64, return_sequences=False)),    \n    tf.keras.layers.Conv1D(64, 3, padding='valid', strides=1, activation='relu'),\n    tf.keras.layers.Conv1D(64, 3, padding='same', strides=1, activation='relu'),\n    tf.keras.layers.MaxPooling1D(2),\n    \n    tf.keras.layers.Conv1D(64, 3, padding='valid', strides=1, activation='relu'),\n    tf.keras.layers.Conv1D(64, 3, padding='same', strides=1, activation='relu'),\n    tf.keras.layers.GlobalMaxPooling1D(),\n    \n    tf.keras.layers.Dropout(0.2),\n    tf.keras.layers.Flatten(), \n    tf.keras.layers.Dense(32, activation = 'relu'),\n    tf.keras.layers.Dropout(0.2),\n    tf.keras.layers.Dense(6, activation = 'sigmoid')\n    \n])\nmodel.compile(loss='binary_crossentropy', optimizer='adam',metrics=['mae', tf.keras.metrics.AUC()])\nmodel.summary()","74e92405":"np.random.seed(1291)\nX_train, X_test, y_train, y_test = train_test_split(padded, labels_npy, test_size=0.2)","8c60a213":"callbacks=[\n    tf.keras.callbacks.EarlyStopping(monitor=\"val_auc\", patience=5, mode=\"min\", restore_best_weights=True, verbose=1),\n    tf.keras.callbacks.ModelCheckpoint(filepath=\"best_model-{epoch:02d}-{val_auc:.4f}.hdf5\", save_best_only=True, verbose=1, monitor=\"val_auc\")\n] \n","cfa267a1":"history = model.fit(X_train, y_train, epochs=15, batch_size=120, validation_split=0.2, callbacks=callbacks)","5449a76d":"evel_loss, evel_mae, evel_auc = model.evaluate(x=X_test, y=y_test)","bd42dee6":"def plot_learning_curve(history):\n        # plt.plot(history.epoch, history.history[\"auc\"], \".:\")\n        # plt.plot(history.epoch, history.history[\"val_auc\"], \".:\")\n\n        plt.plot(history.epoch, history.history[\"loss\"], \".:\", label=\"loss\")\n        plt.plot(history.epoch, history.history[\"val_loss\"], \".:\", label=\"val_loss\")\n        plt.legend()\n\nplot_learning_curve(history)\nval_auc = history.history[\"val_auc\"][-1]","37b7f37c":"val_auc, evel_auc, min(val_auc,evel_auc)-abs(val_auc - evel_auc)","707fcd09":"test_pred = model.predict(test_padded)","a4cf084a":"test_pred","ddf3872b":"# (test_pred>0.5).astype(\"int\")","8809cbb3":"pd.concat([test_df[\"id\"], pd.DataFrame(test_pred, columns=target_cols)], 1).to_csv(\"sub.csv\", index=False)","03d500ce":"# !kaggle competitions submit -f sub.csv -m \"\" jigsaw-toxic-comment-classification-challenge","e4f7a570":"# !kaggle competitions submissions jigsaw-toxic-comment-classification-challenge","8b61da31":"### Sentence","be5d0768":"# prediction for submission","989a8ae0":"```\npublicScore  privateScore\n0.93708      0.93811\n```","620db80b":"# Model","9a73f22c":"# Tokenize","3ed20824":"### Label"}}