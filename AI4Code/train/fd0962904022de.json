{"cell_type":{"7f83a026":"code","a386a918":"code","182f2658":"code","d582c558":"code","adaa05d7":"code","c0fde15b":"code","ffa0c5aa":"code","a7190507":"code","ee27cc69":"code","23ebfa4b":"code","f03272bd":"code","698ae517":"code","ec57b532":"code","35b84c92":"code","a91a7e87":"code","c1d45c47":"code","ed3100bc":"code","0bad794c":"code","d435e0bc":"code","453b617e":"code","af03e6f3":"code","deaad957":"code","24996acc":"code","d207d44d":"code","8e125221":"code","4f2af2c9":"markdown","128145d8":"markdown","7aeaf679":"markdown","0f959859":"markdown","d6621bf7":"markdown","75a35e8e":"markdown","05c414cc":"markdown","7313fca6":"markdown","ede49d2b":"markdown","b4759598":"markdown","1e131108":"markdown","cbae93ce":"markdown","450c7091":"markdown","94fb01f6":"markdown","99c45435":"markdown","8bf32fef":"markdown","d782f7be":"markdown","2ea51aff":"markdown","8d786b7f":"markdown","3704474c":"markdown","427a85ff":"markdown"},"source":{"7f83a026":"# Computational imports\nimport numpy as np   # Library for n-dimensional arrays\nimport pandas as pd  # Library for dataframes (structured data)\n\n# Helper imports\nimport os \nimport warnings\nimport pandas_datareader as web\nimport datetime as dt\n\n# ML\/DL imports\nfrom keras.models import Sequential\nfrom keras.layers import LSTM, Dense, Dropout, RepeatVector, TimeDistributed\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n\n# Plotting imports\nimport matplotlib.pyplot as plt\nimport matplotlib.dates as dates\nimport plotly.express as px\nimport plotly.graph_objects as go\nfrom plotly.subplots import make_subplots\nfrom plotly.offline import init_notebook_mode, iplot\n\n%matplotlib inline\nwarnings.filterwarnings(\"ignore\")\ninit_notebook_mode(connected=True)\n\n# Set seeds to make the experiment more reproducible.\nfrom numpy.random import seed\nseed(1)","a386a918":"# split a univariate sequence into samples\ndef split_sequence(data, days_past, days_future):\n    X, y = list(), list()\n    \n    for i in range(len(data)):        \n        # find the end of this pattern\n        end_ix = i + days_past\n        out_end_ix = end_ix + days_future\n        \n        # check if we are beyond the sequence\n        if out_end_ix > len(data):\n            break\n            \n        # gather input and output parts of the pattern\n        seq_x, seq_y = data[i:end_ix], data[end_ix:out_end_ix]\n        X.append(seq_x)\n        y.append(seq_y)\n        \n    return np.array(X), np.array(y)","182f2658":"file_name = '..\/input\/demand-forecasting-kernels-only\/train.csv'\ndataset = pd.read_csv(file_name, parse_dates=['date'])\n\ndataset.drop(labels=['store', 'item'], axis=1, inplace = True)","d582c558":"dataset.head()","adaa05d7":"dataset.describe()","c0fde15b":"print('Min date from train set: %s' % dataset.date.min().date())\nprint('Max date from train set: %s' % dataset.date.max().date())","ffa0c5aa":"dataset.sales.isnull().sum()","a7190507":"training_data = dataset[(dataset['date'] <= '2016')]\ntest_data = dataset[(dataset['date'] >= '2017')]\n\ntraining_data.sort_values('date', inplace=True)\ntraining_data = training_data.groupby('date')\ntraining_data = training_data.agg({'sales':'mean'})\ntraining_data.colums = ['sales']\n\ntest_data.sort_values('date', inplace=True)\ntest_data = test_data.groupby('date')\ntest_data = test_data.agg({'sales':'mean'})\ntest_data.colums = ['sales']\n\nprint(len(training_data))\nprint(len(test_data))","ee27cc69":"training_data.head()","23ebfa4b":"fig  = make_subplots()\nfig.add_trace(go.Scatter(x=training_data.index,y=training_data.sales,name=\"Zoom\"))\nfig.update_layout(autosize=True,width=900,height=500,title_text=\"Sales\")\nfig.update_xaxes(title_text=\"year\")\nfig.update_yaxes(title_text=\"prices\")\nfig.show()","f03272bd":"fig  = make_subplots()\nfig.add_trace(go.Scatter(x=training_data.index,y=training_data.sales,name=\"Train\"))\nfig.add_trace(go.Scatter(x=test_data.index,y=test_data.sales,name=\"Test\"))\nfig.update_layout(autosize=False,width=900,height=500,title_text=\"Sales\")\nfig.update_xaxes(title_text=\"day\")\nfig.update_yaxes(title_text=\"prices\")\nfig.show()","698ae517":"scaler = MinMaxScaler(feature_range=(0,1))\nscaled_tain_data = scaler.fit_transform(training_data.values.reshape(-1,1))\nscaled_tain_data.shape","ec57b532":"scaler = MinMaxScaler(feature_range=(0,1))\nscaled_test_data = scaler.fit_transform(test_data.values.reshape(-1,1))\nscaled_test_data.shape","35b84c92":"days_past = 30   # use past 30 time steps to predict\ndays_future = 5  # predict 5 time steps in the future\n\nx_train, y_train = split_sequence(scaled_tain_data, days_past, days_future)\nx_test, y_test = split_sequence(scaled_test_data, days_past, days_future)","a91a7e87":"print(x_train.shape)\nprint(x_test.shape)","c1d45c47":"x_train = x_train.reshape((x_train.shape[0], x_train.shape[1], 1))\ny_train = y_train.reshape((y_train.shape[0], y_train.shape[1], 1))\n\nx_test = x_test.reshape((x_test.shape[0], x_test.shape[1], 1))\ny_test = y_test.reshape((y_test.shape[0], y_test.shape[1], 1))","ed3100bc":"print(x_train.shape)\nprint(x_test.shape)","0bad794c":"def encoder_decoder_model():\n    \n    # Use Keras sequential model\n    model = Sequential()\n    \n    # Encoder LSTM layer with Dropout regularisation; Set return_sequences to False since we are feeding last output to decoder layer\n    model.add(LSTM(100, activation='relu',input_shape = (days_past,1)))\n    model.add(Dropout(0.2))\n    \n    # The fixed-length output of the encoder is repeated, once for each required time step in the output sequence with the RepeatVector wrapper\n    model.add(RepeatVector(days_future))\n    \n    # Decoder LSTM layer with Dropout regularisation; Set return_sequences to True to feed each output time step to a Dense layer\n    model.add(LSTM(100, activation='relu', return_sequences=True))\n    model.add(Dropout(0.2))\n    \n    # Same dense layer is repeated for each output timestep with the TimeDistributed wrapper\n    model.add(TimeDistributed(Dense(units=1, activation = \"linear\")))\n    \n    return model","d435e0bc":"model = encoder_decoder_model()\nmodel.summary()","453b617e":"model.compile(optimizer='adam', loss='mean_squared_error', metrics = ['accuracy'])","af03e6f3":"checkpointer = ModelCheckpoint(filepath = '.\/checkpoint.hdf5', verbose = 0, save_best_only = True)\nhis=model.fit(x_train,y_train,epochs=50,callbacks=[checkpointer], verbose=0)","deaad957":"plt.plot(his.history['loss'])\nplt.plot(his.history['accuracy'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['loss','accuracy'])\nplt.show()","24996acc":"# Predicting the prices\npredicted_sales = model.predict(x_test)","d207d44d":"# We upse numpy squeeze to transform (samples, timesteps, number of features) into (samples, timesteps), since that is what inverse_transform expects\ny_test = scaler.inverse_transform(np.squeeze(y_test))\npredicted_sales = scaler.inverse_transform(np.squeeze(predicted_sales))\n\n\n# We flatten the 2 dimensional array so we can plot it with matplotlib\ny_test = y_test.flatten()\npredicted_sales = predicted_sales.flatten()","8e125221":"plt.plot(y_test, color='black', label=f\"Actual Sales\")\nplt.plot(predicted_sales, color= 'green', label=\"Predicted Sales\")\nplt.title(\"Sales vs Predicted Sales\")\nplt.xlabel(\"Days in test period\")\nplt.ylabel(\"Price\")\nplt.legend()\nplt.show()","4f2af2c9":"# Final Remarks\nI hope this notebook helped you a bit. If so, please upvote the notebook :)!","128145d8":"# Test Visualization\nLet's conclude this notebook with some good ole data visualization. We'll start by flattening the 2D array into an 1D array. This is done for ease of plotting.","7aeaf679":"# Introduction \n`V1.0.1 - 2021-10-18`\n### Who am I\nJust a fellow Kaggle learner. I was creating this Notebook as practice and thought it could be useful to some others \n### Who is this for\nThis Notebook is for people that learn from examples. Forget the boring lectures and follow along for some fun\/instructive time :)\n### What can I learn here\nYou learn all the basics needed to create a rudimentary LSTM Encoder-Decoder Network. I go over a multitude of steps with explanations. Hopefully with these building blocks,\nyou can go ahead and build much more complex models.\n\n### Thins to remember\n+ Please Upvote\/Like the Notebook so other people can learn from it\n+ Feel free to give any recommendations\/changes. \n+ I will be continuously updating the notebook. Look forward to many more upcoming changes in the future.","0f959859":"# Transforming the data into a time-series appropriate problem\nWe will now have to rpepare the data in a form that is appropriate to be fed into a RNN\/LSTM network. The key is to seperate the data into past sequence (Day 0-29) that is going to be used to predict the future sequence (Day 30-34) for example.","d6621bf7":"# Testing Time\nNow that we have validated that the model does pretty well on our training data, we can move to some more serious stuff... TEST DATA\nWhy is this important you might ask? Well if you score plenty of goals in your practices (good job I guess), but none on the real game... there must be something wrong.\n\nThat is why we need test data to confirm that our model does well on unseen data.","75a35e8e":"# Scaling the training data\nHere we will use MinMaxScaler from the Sci-kit learn library to standardize and scale our input data between 0-1.","05c414cc":"# Time period of the dataset\nLet's see what is the start and end of the dataset","7313fca6":"# Visualizing and Plotting our Data\nIt is very important to plot and visualize the data to get a good grasp of what is happening. For this, I have decided to use plotly, but you can use any other visualizaiton library such as seaborn and matplotlib. We are using plotly graph objects since we will need to create subplots later.","ede49d2b":"# Splitting the data set in training and test sets\nThis part is important since we will be using the training set to train the model and the test set to check if the model does well on unseen inputs.\n\nWe are going to use only a small subset of the data due to training time. In competitons, use as much data that there is avaialable to botrain a stronger model that is good at generalizing. ","b4759598":"This dataset is pretty easy, no missing data... less work for us :)","1e131108":"# Reading and Preparing the Data\nLet's start by preparing our data. We will store it in a dataframe. We are only interested in the sales number.","cbae93ce":"Now to plot the training and test data on the same plot. Notice the lag between the training data and test data. This is done on purpose to ensure that the test data isn't too strongly correlated to the training data.","450c7091":"Let us now visualize the loss and accuracy to see how the training went.","94fb01f6":"Now we set our compiler and our optimatization mechanism. We will be using the Adam optimazation method since it is widely used and performs much better than regular gradient descent.","99c45435":"The LSTM model expects the input to have shape: (Samples, Timesteps, Number of feautures). In our case, we have an univariate RNN problem, therefore Number of feautures = 1","8bf32fef":"We see that the loss decreaes and is lower than 0.2 MSE. This shows that our model is indeed doing well on the training data.","d782f7be":"# Helper Functions\nThese functions will help us prepare our data to be fed into the model","2ea51aff":"# Training\/Fitting time\nWe can finally train our model with our training data. Let's see how it does.","8d786b7f":"# Imports\nFirst let us start by importing the relevant libraries that we need.","3704474c":"# Checking for NaN (missing values) in the dataframe\nThere is multiple ways of doing this, I have chosen do it with the following method:","427a85ff":"# Creating the LSTM Network\nWe are going to be creating a three layer LSTM Network with a dense layer at the end. We are using dropout as a regularisation method to combat overffiting."}}