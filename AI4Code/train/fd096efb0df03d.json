{"cell_type":{"6ef00daa":"code","a1708102":"code","da556291":"code","a7a91c8f":"code","1946c940":"code","7f52b17f":"code","a11146b3":"code","398f89cd":"code","acb21383":"code","59f3304c":"code","1a3c11bb":"code","7d419653":"code","aae48ab3":"code","17fb4eb1":"code","e5db5287":"code","01936e07":"code","dde09822":"code","4f49b460":"code","0ef98b12":"code","79d4fcd0":"code","3106c8c6":"code","189e7de9":"code","357a87bc":"code","09495ad5":"code","f571819f":"code","f39849ba":"code","c9bb1298":"code","83ea7fbd":"code","26a4ca23":"code","02ab1100":"code","e4877674":"code","7ff98f91":"code","a4bc0fcf":"code","e1d5d352":"code","052ec0f9":"code","5d6338cb":"code","e99c0e48":"code","fe9de84d":"code","60212ffe":"code","a42eaf7e":"code","2e5ebe54":"code","45910269":"code","0efee4b3":"code","74242c21":"code","7eedc266":"code","c9830573":"code","7b0a5d5d":"code","6885a975":"code","d8221b9f":"code","63a8e044":"code","5d1832e2":"code","a05b9f00":"code","dd86c314":"markdown","c9a25bb0":"markdown","d79850ce":"markdown","e2fd8fe3":"markdown","22ca2f4e":"markdown","e0efca3e":"markdown","8c4018af":"markdown","c6fee35d":"markdown","0fc2e437":"markdown","d1c775ef":"markdown","1788069b":"markdown","ed360c8c":"markdown","78d085e6":"markdown","4a4fa7d8":"markdown","c6f99ec9":"markdown","a64ba480":"markdown","2d2b4432":"markdown","3dbefb2b":"markdown","65a27aea":"markdown","08b07ff2":"markdown","2c90790e":"markdown","f753ce79":"markdown","d1cde321":"markdown","630b8a24":"markdown","1776b649":"markdown","aef67153":"markdown","1dad4ec5":"markdown","65247ffd":"markdown","93bbe7a4":"markdown","19d25ba1":"markdown","0dc756d6":"markdown","db20c1ca":"markdown","99b046e0":"markdown","7bd8d5b4":"markdown","9f374263":"markdown","c386032c":"markdown","5c328611":"markdown","0a6b7958":"markdown"},"source":{"6ef00daa":"from sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\nimport numpy as np\nimport pandas as pd\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.ensemble import RandomForestClassifier, VotingClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier\nimport lightgbm as lgb\nfrom imblearn.under_sampling import RandomUnderSampler\nfrom imblearn.over_sampling import SMOTE\nfrom sklearn.metrics import auc, precision_recall_curve\nfrom collections import Counter\nfrom sklearn.utils import shuffle\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import precision_recall_curve\nfrom imblearn.pipeline import Pipeline\nfrom numpy import mean\nfrom sklearn.datasets import make_classification\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import RepeatedStratifiedKFold\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn import metrics\nfrom sklearn.model_selection import train_test_split, RandomizedSearchCV\nfrom sklearn.model_selection import StratifiedShuffleSplit\nfrom imblearn.under_sampling import NearMiss\nfrom imblearn.pipeline import make_pipeline as imbalanced_make_pipeline\nfrom sklearn.model_selection import StratifiedShuffleSplit\n","a1708102":"import pandas as pd\ndata = pd.read_csv('\/kaggle\/input\/GiveMeSomeCredit\/cs-training.csv',index_col=0)\ntest = pd.read_csv('\/kaggle\/input\/GiveMeSomeCredit\/cs-test.csv',index_col=0)\ntest = test.drop(columns=['SeriousDlqin2yrs'])\ntest_index = test.index","da556291":"data.head()","a7a91c8f":"data.isnull().sum()","1946c940":"data.NumberOfDependents.fillna(data.NumberOfDependents.mean(),inplace=True)\ndata.MonthlyIncome.fillna(data.MonthlyIncome.mean(),inplace=True)\ntest.NumberOfDependents.fillna(test.NumberOfDependents.mean(),inplace=True)\ntest.MonthlyIncome.fillna(test.MonthlyIncome.mean(),inplace=True)","7f52b17f":"print('No Frauds', round(data['SeriousDlqin2yrs'].value_counts()[0]\/len(data) * 100,2), '% of the dataset')\nprint('Frauds', round(data['SeriousDlqin2yrs'].value_counts()[1]\/len(data) * 100,2), '% of the dataset')","a11146b3":"sns.countplot('SeriousDlqin2yrs', data=data, palette=[\"#0101DF\", \"#DF0101\"])\n# plt.title('Class Distributions \\n (0: No Fraud || 1: Fraud)', fontsize=12)","398f89cd":"original_Xtrain, original_Xtest, original_ytrain, original_ytest = train_test_split(data, data['SeriousDlqin2yrs'], test_size=0.2, random_state=42)\n\ntrain_unique_label, train_counts_label = np.unique(original_ytrain, return_counts=True)\ntest_unique_label, test_counts_label = np.unique(original_ytest, return_counts=True)\n\nprint('Label Distributions: \\n')\n\nprint('distribution of original_ytrain ')\nprint(train_counts_label\/ len(original_ytrain))\n\nprint('distribution of original_ytest')\nprint(test_counts_label\/ len(original_ytest))","acb21383":"data = data.sample(frac=1)\nfraud = data.loc[data['SeriousDlqin2yrs'] == 1]\n\nno_fraud = data.loc[data['SeriousDlqin2yrs'] == 0][:len(fraud)]\n\nsub_sample = pd.concat([fraud,no_fraud])\nsub_sample = sub_sample.sample(frac=1,random_state = 42)","59f3304c":"colors = [\"#0101DF\", \"#DF0101\"]","1a3c11bb":"sns.countplot('SeriousDlqin2yrs', data=sub_sample)\nplt.title('Class Distributions \\n (0: No Fraud || 1: Fraud)', fontsize=12)","7d419653":"sub_sample.head()","aae48ab3":"sub_sample.isnull().sum().max()","17fb4eb1":"colormap = plt.cm.RdBu\nplt.figure(figsize=(14,12))\nplt.title('Pearson Correlation of Features', y=1.05, size=15)\nsns.heatmap(sub_sample.astype(float).corr(),linewidths=0.1,vmax=1.0, \n            square=True, cmap=colormap, linecolor='white', annot=True)","e5db5287":"sub_sample.columns","01936e07":"f, axes = plt.subplots(ncols=2, nrows=3, figsize=(20,15))\n\n# Negative Correlations with our Class (The lower our feature value the more likely it will be a fraud transaction)\nsns.boxplot(x=\"SeriousDlqin2yrs\", y=\"age\", data=sub_sample, palette=colors, ax=axes[0][0])\naxes[0][0].set_title('age vs Class Negative Correlation')\n\nsns.boxplot(x=\"SeriousDlqin2yrs\", y=\"MonthlyIncome\", data=sub_sample, palette=colors, ax=axes[0][1])\naxes[0][1].set_title('MonthlyIncome vs Class Negative Correlation')\n\n\nsns.boxplot(x=\"SeriousDlqin2yrs\", y=\"NumberOfOpenCreditLinesAndLoans\", data=sub_sample, palette=colors, ax=axes[1][0])\naxes[1][0].set_title('NumberOfOpenCreditLinesAndLoans vs Class Negative Correlation')\n\nsns.boxplot(x=\"SeriousDlqin2yrs\", y=\"DebtRatio\", data=sub_sample, palette=colors, ax=axes[1][1])\naxes[1][1].set_title('DebtRatio vs Class Negative Correlation')\n\nsns.boxplot(x=\"SeriousDlqin2yrs\", y=\"NumberRealEstateLoansOrLines\", data=sub_sample, palette=colors, ax=axes[2][0])\naxes[2][0].set_title('NumberRealEstateLoansOrLines vs Class Negative Correlation')\n\n\nsns.boxplot(x=\"SeriousDlqin2yrs\", y=\"NumberRealEstateLoansOrLines\", data=sub_sample, palette=colors, ax=axes[2][1])\naxes[2][1].set_title('NumberRealEstateLoansOrLines vs Class Negative Correlation')\n\nplt.show()","dde09822":"f, axes = plt.subplots(ncols=2,nrows=2,figsize=(15,10))\n\n# Negative Correlations with our Class (The lower our feature value the more likely it will be a fraud transaction)\nsns.boxplot(x=\"SeriousDlqin2yrs\", y=\"NumberOfTime30-59DaysPastDueNotWorse\", data=sub_sample, palette=colors, ax=axes[0][0])\naxes[0][0].set_title('NumberOfTime30-59DaysPastDueNotWorse vs Class Negative Correlation')\n\nsns.boxplot(x=\"SeriousDlqin2yrs\", y=\"NumberOfTimes90DaysLate\", data=sub_sample, palette=colors, ax=axes[0][1])\naxes[0][1].set_title('NumberOfTimes90DaysLate vs Class Negative Correlation')\n\nsns.boxplot(x=\"SeriousDlqin2yrs\", y=\"NumberOfTime60-89DaysPastDueNotWorse\", data=sub_sample, palette=colors, ax=axes[1][0])\naxes[1][0].set_title('NumberOfTime60-89DaysPastDueNotWorse vs Class Negative Correlation')\n\n\nsns.boxplot(x=\"SeriousDlqin2yrs\", y=\"NumberOfDependents\", data=sub_sample, palette=colors, ax=axes[1][1])\naxes[1][1].set_title('NumberOfDependents vs Class Negative Correlation')\n\nplt.show()","4f49b460":"scale2 = ['DebtRatio','NumberOfTimes90DaysLate','DebtRatio','MonthlyIncome']\nfor col in scale2:\n    sub_sample[col] = sub_sample[col].map(lambda x:np.log(x) if x>0 else 0 )","0ef98b12":"scale2 = ['DebtRatio','NumberOfTimes90DaysLate','DebtRatio','MonthlyIncome']\nfor col in scale2:\n    test[col] = test[col].map(lambda x:np.log(x) if x>0 else 0 )","79d4fcd0":"def detection_outlier(df,cols):\n    \n    outlier_indices = np.array([])\n    \n    for col in cols:\n        Q1 = np.percentile(df[col],25)\n        Q3 = np.percentile(df[col],75)\n        IQR = Q3 - Q1\n        outlier_step = 1.5 * IQR\n        outlier_list_col = df[(df[col] < Q1 - outlier_step) | (df[col] > Q3 + outlier_step )].index\n        outlier_indices = np.append(outlier_indices,outlier_list_col.values)         \n    return np.unique(outlier_indices)","3106c8c6":"special_column = ['NumberOfDependents','NumberOfTime30-59DaysPastDueNotWorse','NumberOfTime60-89DaysPastDueNotWorse','age']\nindex_outlier_sub_sample = detection_outlier(sub_sample,special_column)","189e7de9":"index_outlier_sub_sample.shape","357a87bc":"x_sub_sample = sub_sample.drop(index_outlier_sub_sample, axis = 0).reset_index(drop=True)\ny_sub_sample = sub_sample['SeriousDlqin2yrs']","09495ad5":"f, axes = plt.subplots(ncols=2, nrows=2, figsize=(15,10))\n\n# Negative Correlations with our Class (The lower our feature value the more likely it will be a fraud transaction)\nsns.boxplot(x=\"SeriousDlqin2yrs\", y=\"age\", data=x_sub_sample, palette=colors, ax=axes[0][0])\naxes[0][0].set_title('age vs Class Negative Correlation')\n\nsns.boxplot(x=\"SeriousDlqin2yrs\", y=\"NumberOfDependents\", data=x_sub_sample, palette=colors, ax=axes[0][1])\naxes[0][1].set_title('NumberOfDependents vs Class Negative Correlation')\n\nsns.boxplot(x=\"SeriousDlqin2yrs\", y=\"NumberOfTime30-59DaysPastDueNotWorse\", data=x_sub_sample, palette=colors, ax=axes[1][0])\naxes[1][0].set_title('NumberOfTime30-59DaysPastDueNotWorse vs Class Negative Correlation')\n\nsns.boxplot(x=\"SeriousDlqin2yrs\", y=\"NumberOfTime60-89DaysPastDueNotWorse\", data=x_sub_sample, palette=colors, ax=axes[1][1])\naxes[1][1].set_title('NumberOfTime60-89DaysPastDueNotWorse vs Class Negative Correlation')\n\nplt.show()","f571819f":"index_outlier = detection_outlier(original_Xtrain,special_column)","f39849ba":"scale2 = ['DebtRatio','NumberOfTimes90DaysLate','DebtRatio','MonthlyIncome']\nfor col in scale2:\n    original_Xtrain[col] = original_Xtrain[col].map(lambda x:np.log(x) if x>0 else 0 )","c9bb1298":"x_sub_sample = sub_sample.drop(columns=['SeriousDlqin2yrs'])\ny_sub_sample = sub_sample['SeriousDlqin2yrs']\n\noriginal_Xtrain = original_Xtrain.drop(columns=['SeriousDlqin2yrs'], axis = 1).reset_index(drop=True)\noriginal_Xtest  = original_Xtest.drop(columns=['SeriousDlqin2yrs'], axis = 1).reset_index(drop=True)\n\nprivate_test    = test.values\noriginal_Xtrain = original_Xtrain.values \noriginal_Xtest =  original_Xtest.values\noriginal_ytrain = original_ytrain.values\noriginal_ytest = original_ytest.values","83ea7fbd":"num_round = 5\nkfold = StratifiedKFold(n_splits=num_round, random_state=None, shuffle=False)\nid =0\nsplit_data ={}\nfor train,test in kfold.split(original_Xtrain,original_ytrain):\n    split_data['train'+str(id)] = train\n    split_data['valid'+str(id)] = test\n    id+=1","26a4ca23":"clssifers ={\n    \"LogisticRegression\" : LogisticRegression(max_iter = 10000),\n    \"RandomForestClassifier\" : RandomForestClassifier(),\n    \"GaussianNB\": GaussianNB(),\n    \"KNearest\": KNeighborsClassifier(),\n    \"DecisionTreeClassifier\": DecisionTreeClassifier()\n}","02ab1100":"for key,clf in clssifers.items():\n    clf.fit(x_sub_sample,y_sub_sample)\n    score = cross_val_score(clf,x_sub_sample,y_sub_sample,cv = 5,scoring='roc_auc')\n    print(key,'score',round(score.mean(),2))\n    ","e4877674":"def find_best_parameter(clf,parameter,X_train,y_train):\n    best_model = GridSearchCV(clf,parameter).fit(X_train,y_train)\n    return best_model.best_estimator_","7ff98f91":"# Use GridSearchCV to find the best parameters.\nfrom sklearn.model_selection import GridSearchCV\n\n\n# Logistic Regression \nlog_reg_params = {'C': [0.01, 0.1, 1]}\nlog_clf = find_best_parameter(LogisticRegression(max_iter = 10000), log_reg_params,x_sub_sample,y_sub_sample)\n\n#Knn n_neighbors must odd number\nknears_params = {\"n_neighbors\": list(range(1,5,2)), 'algorithm': ['ball_tree', 'kd_tree']}\n\nknn_clf = find_best_parameter(KNeighborsClassifier(), knears_params,x_sub_sample,y_sub_sample)\n\n","a4bc0fcf":"# save new parameter\nclssifers['KNearest'] = knn_clf  \nclssifers['LogisticRegression'] = log_clf  ","e1d5d352":"X_nearmiss, y_nearmiss = NearMiss().fit_sample(original_Xtrain, original_ytrain)\nprint('NearMiss Label Distribution: {}'.format(Counter(y_nearmiss)))\np = {} # save result\nfor key,clf in clssifers.items():\n    pres = 0.0\n    averager_pre =[]\n    id = 0\n    for id in range(0,num_round):\n        # get index train and valid\n        train_id = split_data['train'+str(id)]\n        valid_id = split_data['valid'+str(id)]\n        id+=1\n        # fit model\n        undersample_pipeline    = imbalanced_make_pipeline(NearMiss(sampling_strategy='majority'), clf) \n        undersample_model       = undersample_pipeline.fit(original_Xtrain[train_id], original_ytrain[train_id])\n        \n        # calculate score\n        result  = undersample_model.predict(original_Xtrain[valid_id])\n        pres += result\/num_round\n    for value in pres:\n        if value>.5:\n            averager_pre.append(1)\n        else:\n            averager_pre.append(0)\n    p[key] = metrics.confusion_matrix(averager_pre,original_ytrain[valid_id])","052ec0f9":"import matplotlib.pyplot as plt\nimport numpy as np\nimport itertools\n\ndef plot_confusion_matrix(cm,\n                          target_names,\n                          title='Confusion matrix',\n                          cmap=None,\n                          normalize=True):\n\n\n    accuracy = np.trace(cm) \/ float(np.sum(cm))\n    misclass = 1 - accuracy\n\n    if cmap is None:\n        cmap = plt.get_cmap('Blues')\n\n    plt.figure(figsize=(8, 6))\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n\n    if target_names is not None:\n        tick_marks = np.arange(len(target_names))\n        plt.xticks(tick_marks, target_names, rotation=45)\n        plt.yticks(tick_marks, target_names)\n\n    if normalize:\n        cm = cm.astype('float') \/ cm.sum(axis=1)[:, np.newaxis]\n\n\n    thresh = cm.max() \/ 1.5 if normalize else cm.max() \/ 2\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        if normalize:\n            plt.text(j, i, \"{:0.4f}\".format(cm[i, j]),\n                     horizontalalignment=\"center\",\n                     color=\"white\" if cm[i, j] > thresh else \"black\")\n        else:\n            plt.text(j, i, \"{:,}\".format(cm[i, j]),\n                     horizontalalignment=\"center\",\n                     color=\"white\" if cm[i, j] > thresh else \"black\")\n\n\n    plt.tight_layout()\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label\\naccuracy={:0.4f}; misclass={:0.4f}'.format(accuracy, misclass))\n    plt.show()","5d6338cb":"plot_confusion_matrix(p['LogisticRegression'],['non-Fraud','Fraud'])","e99c0e48":"plot_confusion_matrix(p['RandomForestClassifier'],['non-Fraud','Fraud'])","fe9de84d":"plot_confusion_matrix(p['GaussianNB'],['non-Fraud','Fraud'])","60212ffe":"def transform_average_result(pres):\n    averager_pre = []\n    for value in pres:\n        if value>.5:\n            averager_pre.append(1)\n        else:\n            averager_pre.append(0)\n    return averager_pre","a42eaf7e":"\n# Implementing SMOTE Technique \n# Cross Validating the right way\n\nresult_valid  = {} # save result\nresult_orginal = {}\nprivate_test_result = {}\nfor key,clf in clssifers.items():\n    average_original_test = 0.0\n    average_valid_test = 0.0\n    average_private_test = 0.0\n    for id in range(0,num_round):\n        # get index train and valid\n        train_id = split_data['train'+str(id)]\n        valid_id = split_data['valid'+str(id)]\n        # apply Smote() \n        pipeline = imbalanced_make_pipeline(SMOTE(sampling_strategy='minority'),RandomForestClassifier()) # SMOTE happens during Cross Validation not before..\n        model = pipeline.fit(original_Xtrain[train_id], original_ytrain[train_id])\n        \n        # predict \n        score_valid_test = model.predict(original_Xtrain[valid_id])\n        score_original_test = model.predict(original_Xtest)\n        score_private_test = model.predict_proba(private_test)\n        \n        ## average 5 round\n        average_valid_test     += score_valid_test\/num_round\n        average_original_test  += score_original_test\/num_round\n        average_private_test   += score_private_test\/num_round\n    \n    # transform probability to class\n    valid  = transform_average_result(average_valid_test)\n    orginal = transform_average_result(average_original_test)\n    \n    ## save result in cross validation\n    result_valid[key] = metrics.confusion_matrix(valid,original_ytrain[valid_id])\n    result_orginal[key] = metrics.confusion_matrix(orginal,original_ytest)\n    \n    ## The result probability predict private test save to dict private_test_result\n    private_test_result[key] = average_private_test\n","2e5ebe54":"plot_confusion_matrix(result_orginal['LogisticRegression'],['non-Fraud','Fraud'])","45910269":"plot_confusion_matrix(result_orginal['KNearest'],['non-Fraud','Fraud'])","0efee4b3":"print(\"private test with knn algorithm \")\nprivate_test_result['KNearest'][:,1]","74242c21":"print(\"private test with Logistic Regression algorithm\")\nprivate_test_result['LogisticRegression'][:,1]","7eedc266":"result = pd.DataFrame({'Id': test_index,'Probability': private_test_result['LogisticRegression'][:,1]})\nresult.to_csv('result.csv',index=0)","c9830573":"params = {\n    'objective'         : 'binary',    \n    'metric'            : 'auc', \n    'nthread'           : 4,\n    'learning_rate'     : 0.01, \n    'num_leaves'        : 23,\n    'feature_fraction'  : 0.106,\n    'bagging_fraction'  : 0.825,\n    'max_depth'         : -1,\n    'lambda_l1'         : 0.2,\n    'lambda_l2'         : 2.7,\n    'min_split_gain'    : 0.007,\n}","7b0a5d5d":"\npreds_valid = 0.0\npreds_origin_test = 0.0\npreds_private_test = 0.0\naccuracy_valid = []\nf1_valid = []\nfor id in range(0,num_round):\n    ## get index train && test set \n    train_id = split_data['train'+str(id)]\n    valid_id = split_data['valid'+str(id)]\n\n    ## build model\n    lgb_train = lgb.Dataset(original_Xtrain[train_id], original_ytrain[train_id])\n    \n    lgb_eval  = lgb.Dataset(original_Xtrain[valid_id], original_ytrain[valid_id], reference = lgb_train)\n    \n    model = lgb.train(params,\n                lgb_train,\n                num_boost_round = 99999,  \n                early_stopping_rounds = 800,\n                verbose_eval = False,\n                valid_sets = [lgb_train, lgb_eval])\n    \n    # valid test\n    pred_valid = model.predict(original_Xtrain[valid_id])\n    \n    trans_class = transform_average_result(pred_valid)\n    accuracy_valid.append(metrics.accuracy_score(trans_class,original_ytrain[valid_id]))\n    f1_valid.append(metrics.f1_score(trans_class,original_ytrain[valid_id]))\n    \n    # apply original test\n    pred_origin_test = model.predict(original_Xtest)\n    preds_origin_test += pred_origin_test\/num_round\n    \n    # apply private test\n    pred_private_test = model.predict(private_test)\n    preds_private_test += pred_private_test\/num_round\n","6885a975":"for value in accuracy_valid:\n    print(\"probability transcation fall in Fraud: \",value)","d8221b9f":"for value in f1_valid:\n    print(\"probability transcation fall in Fraud: \",value)","63a8e044":"cm = metrics.confusion_matrix(transform_average_result(preds_origin_test),original_ytest)\nplot_confusion_matrix(cm,['non-Fraud','Fraud'])","5d1832e2":"preds_private_test","a05b9f00":"result = pd.DataFrame({'Id': test_index,'Probability': preds_private_test})\nresult.to_csv('result.csv',index=0)","dd86c314":"- Holding indicates of the train test and valid test compare each other algorithm","c9a25bb0":"- The result original_test on Knn and LogisticRegression approx 90% accuracy. We observed the results in the confusion matrix below.","d79850ce":"## Replace mising value <a id=\"replace\"><\/a>\n- To avoid changing the standard deviation, I will use the mean of each column to fill in the missing values","e2fd8fe3":"- Apply the method to original train. ","22ca2f4e":"<h1 align=\"center\"> Credit Scoring  <\/h1>\n\nFirst of all, Thank you for your interest in my notebook. If you feel useful please upvote for me. Thank you for your consideration.\n\n# Introduction\n>  \" *A credit score is a numerical expression based on a level analysis of a person's credit files, to represent the creditworthiness of an individual. A credit score is primarily based on a credit report, information typically sourced from credit bureaus.Lenders, such as banks and credit card companies, use credit scores to evaluate the potential risk posed by lending money to consumers and to mitigate losses due to bad debt. Lenders use credit scores to determine who qualifies for a loan, at what interest rate, and what credit limits. Lenders also use credit scores to determine which customers are likely to bring in the most revenue. The use of credit or identity scoring prior to authorizing access or granting credit is an implementation of a trusted system.Credit scoring is not limited to banks. Other organizations, such as mobile phone companies, insurance companies, landlords, and government departments employ the same techniques. Digital finance companies such as online lenders also use alternative data sources to calculate the creditworthiness of borrowers.* \" [1]\n\nOverview of my solution\n![image.png](attachment:image.png)\n# Outline\n\n# [I. Preprocess](#preprocess)\n\n   * [1,1. Replace mising value](#replace)\n   * [1.2. Split data](#split)\n   * [1.3. Robust transform](#robust)\n   \n# [II. Visualization](#visual)\n   * [2.1. Heat map](#corre) \n   * [2.2. Boxplot](#boxplot)\n\n# [III. OUTLIER-DETECTION](#out)\n    \n   * [3.1. Why do we use Tukey's method ?](#turkey)\n   * [3.2. TUKEY\u2019S METHOD](#wturkey)\n   \n# [IV. Algorithm](#algo)\n\n   * [4.1. Classifiers (Knn, Logistic regression, Naive Bayes, Random Forest, Decision Tree)](#class)\n   * [4.2. Random Undersampling](#under)\n   * [4.3. Random Oversampling](#over)\n   * [4.4. Lightgbm](#light)\n[1]: https:\/\/en.wikipedia.org\/wiki\/Credit_score\n","e0efca3e":"- Calculate score: average of accuracy and f1 score on validation test.\n- Look at the result, I think f1_valid lower because of my model wrong some case in the class Fraud. \n- The result 5 rounds in validation test.","8c4018af":"Summary: \n- I get approx 91% accuracy and miss approx 0.08% class Fraud, The result is better than undersampling.\n- I submit a private test with two algorithms Knn and logistic receive 80% and 81% roc_auc score, respectively. ","c6fee35d":"- In recent years, Lightgbm is a famous model and is used in many Kaggle competitions. And I decide to apply for the problems. \n- I use 5-Cross-validation on original_Xtrain. I tested it on a valid-test and Original-test result has 94% accuracy. I submit results get an 85% private test and 84% public test. We can see more information confusion matrix below.","0fc2e437":"## 3.1. Why do we use Tukey's method ? <a class=\"anchor\" id=\"turkey\" ><\/a> \n   \n   * The dataset is very large(~250000 rows),\n   * Data are heavily skewed in a some feature. \n   * Tukey\u2019s method is applicable to skewed or non mound-shaped data since it makes no distributional assumptions and it does not depend on the mean or standard deviation.\n\n## 3.2. TUKEY\u2019S METHOD <a class=\"anchor\" id=\"wturkey\" ><\/a>\n   ![](https:\/\/upload.wikimedia.org\/wikipedia\/commons\/thumb\/a\/a9\/Empirical_Rule.PNG\/350px-Empirical_Rule.PNG)   \n    \n   * The plot shows that about 68%,95%, and 99.7% of the data from a normal distribution are within 1, 2, and 3 standard deviations of the mean, respectively[3].\n   * Tukey method depend on a mean or standard deviation of the data. \n   * The plot from [wikimedia commons][4] shows for us Tukey's method work into the data.\n![](https:\/\/upload.wikimedia.org\/wikipedia\/commons\/thumb\/1\/1a\/Boxplot_vs_PDF.svg\/550px-Boxplot_vs_PDF.svg.png)    \n   * The lower quartile (Q1) is the 25th percentile, and the upper quartile (Q3) is the 75th percentile of the data.\n   * The IQR (Inter Quartile Range) is the distance between the lower (Q1) and upper (Q3) quartiles.\n   * Inner fences are located at a distance 1.5 IQR below Q1 and above Q3 [Q1-1.5 IQR,Q3+1.5IQR].\n   * Further information can be found in [book][3].\n   \n[1]: https:\/\/en.wikipedia.org\/wiki\/Medcouple\n[2]: https:\/\/github.com\/tks1998\/statistical-function-and-algorithm-ML-\/blob\/master\/medcople.py\n[3]: http:\/\/d-scholarship-dev.library.pitt.edu\/7948\n[4]: https:\/\/commons.wikimedia.org\/wiki\/File:Boxplot_vs_PDF.svg\n[5]: https:\/\/www.statisticshowto.com\/probability-and-statistics\/skewed-distribution\/        ","d1c775ef":"- In my function, I remove data on this important feature based on Turkey's method","1788069b":"# III. OUTLIER-DETECTION <a id=\"out\"><\/a>","ed360c8c":"- Testing model in the Original Test receives ~ 93% accuracy and wrong 6.36% class.","78d085e6":"# Preprocessing <a id=\"preprocess\"><\/a>","4a4fa7d8":"- Undersampling method during Cross Validation\n- Randomly delete examples in the majority class. Random undersampling involves randomly selecting examples from the majority class and deleting them from the training dataset. The puporse to balance data after that fit it to my model.\n- The results are average k fold validation on valid test, original test.","c6f99ec9":"Notice: The goal we want to predict is **SeriousDlqin2yrs**","a64ba480":"- Submit private test I receive 85.5% roc_auc score. ","2d2b4432":"## 2.2. Boxplot <a id=\"boxplot\"><\/a> ","3dbefb2b":"-  With pandas building  function, We can see that there are 29731 missing value in 'MonthlyIncome' and 3924 missing in 'NumberOfTime60-89DaysPastDueNotWorse'","65a27aea":"- Split train-test has the same percent of each label.","08b07ff2":"- The dataset are used in this notebook was heavily imbalanced","2c90790e":"## Split data <a id=\"split\"><\/a>\n- Sub-sampling: Take a part of the data with the same label (same ratio Fraud and no Fraud) to understand data. Because of the data heavily imbalanced so correlation gets the wrong value for each other. \n- Cross-validation of data to evaluation my model.","f753ce79":"- Now, I create sub-samples from the data with the balancing in the label to observe the data correlation","d1cde321":"## 4.1 Classifiers (Knn, Logistic regression, Naive Bayes, Random Forest, Decision Tree)<a id=\"class\"><\/a>","630b8a24":"# 4.4. Lightgbm <a id=\"light\"><\/a>","1776b649":"## 4.3. Oversampling <a id=\"over\"><\/a>","aef67153":"# II. Visualization <a id=\"visual\"><\/a>","1dad4ec5":"# Summary:\n- The main problems we dealing is imbalanced data.\n- Create sub-sampling for understanding data.\n- Oversampling and Undersampling are famous algorithms apply to imbalanced data. But We more carefully apply it with cross-validation\n- Lightgbm is a strong algorithm, but We need to find the best parameter to high performance it takes a long time. \n\n# References: \n1. https:\/\/en.wikipedia.org\/wiki\/Credit_score \n2. https:\/\/www.kaggle.com\/janiobachmann\/credit-fraud-dealing-with-imbalanced-datasets\n3. https:\/\/www.statisticshowto.com\/probability-and-statistics\/skewed-distribution\/    \n4. https:\/\/scikit-learn.org\/stable\/\n5. https:\/\/www.marcoaltini.com\/blog\/dealing-with-imbalanced-data-undersampling-oversampling-and-proper-cross-validation\n","65247ffd":"* Oversampling method during Cross-Validation\n- Notice: We must split the train and test after that using an oversample. Unless We get overfit because the oversampling method creates the value based on my data. When I separate data, data generated from the oversampling method can appear in both training and testing.\n### The Wrong Way:\n  <img src=\"https:\/\/www.marcoaltini.com\/uploads\/1\/3\/2\/3\/13234002\/2639934.jpg?401\"><br>\n### The right ways:\n  <img src=\"https:\/\/www.marcoaltini.com\/uploads\/1\/3\/2\/3\/13234002\/9101820.jpg?372\"> <br>","93bbe7a4":"-  According to these boxplot, some features like: 'NumberOfDependents','NumberOfTime30-59DaysPastDueNotWorse','NumberOfTime60-89DaysPastDueNotWorse','age' have the same distribution on the class so that My model hardly for identify these class based on this feature. But It has a large value so it impacts other important features. I apply a log transform on it.\n\n- According to these boxplot, some features like: 'NumberOfDependents','NumberOfTime30-59DaysPastDueNotWorse','NumberOfTime60-89DaysPastDueNotWorse','age' are important to class.\n- The important features have more information to identify the class of the class, It is important for my model. So I applied a Turkey method to remove outlier from important feature.","19d25ba1":"## 2.1. Heatmap <a id=\"corre\"><\/a>","0dc756d6":"Summary : \n- The result is an average of 5 fold. \n- In this case, I implement undersampling to get bad results with classifiers. I think the undersampling method lost a lot of information from data and my preprocessing not good for the undersampling algorithm. ","db20c1ca":"## Why do we create sub-sample before visualization ?\n- we see that the my data was heavily imbalanced! \n- Using the original dataset will cause the following issues:\n\n    ** Overfitting **: My model will assume that in most cases there are no frauds. Leading to the parameter is not optimize.\n    \n    ** Wrong Correlations**: The majority class heavily impacts the class. we are not able to see the true correlations between the classes and features.","99b046e0":"# IV. Algorithm <a id=\"algo\"><\/a>","7bd8d5b4":"- **Negative Correlations:** age, MonthlyIncome, NumberOfOpenCreditLinesAndLoans, If the features has lower these value then more likely the end result will be a fraud transaction.\n- **Possitive Correlations** NumberOfTime30-59DaysPastDueNotWorse, NumberOfTimes90DaysLate, NumberOfTime60-89DaysPastDueNotWorse, NumberOfDependents, If that feature has higher these value then more likely the end result will be a fraud transaction.","9f374263":"- Apply to private test","c386032c":"- Now, I'm training sub-sampling, this purpose method find the best parameter to apply to orther algorithm","5c328611":"## 4.2. Undersampling <a id=\"under\"><\/a>","0a6b7958":"I use GridSearchCV for sklearn to find the best parameter."}}