{"cell_type":{"17f7c120":"code","6de599dd":"code","4f0ee1b6":"code","580f2d94":"code","61d5cc7a":"code","96e74117":"code","4cb71da8":"code","bd05afa4":"code","65bbd093":"code","a2901f0d":"code","7b7d0f78":"code","18096427":"code","f95c3d1f":"code","47e7bb08":"code","47f1f08a":"code","fc6fd74d":"code","c23b73a9":"code","2a89af28":"code","3c25d47e":"code","1bc10ef2":"code","e1501e0d":"code","2435dd58":"code","76378048":"code","0e7c9a38":"code","39ba7e21":"code","a0242542":"code","af29aa9f":"code","e92927a1":"code","d57493bf":"code","9b217c3d":"code","e3676e96":"code","a418ddf9":"code","ebbba8af":"code","a1b07c03":"code","d7b871df":"code","76b2a3b9":"markdown","07dd592a":"markdown","040b665c":"markdown","0c08b570":"markdown","7d9bbf6a":"markdown","d895bdb2":"markdown","a4d53255":"markdown","ea4f588c":"markdown","88627c40":"markdown","f0475344":"markdown","6ba3f316":"markdown","c8412804":"markdown","1b62a8ca":"markdown","0af6eb88":"markdown","4092fc15":"markdown","dbc62aeb":"markdown","596c34e6":"markdown","701be1ff":"markdown"},"source":{"17f7c120":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport math as m\n\ndf = pd.read_csv('\/kaggle\/input\/kaggle-survey-2020\/kaggle_survey_2020_responses.csv')","6de599dd":"# Remove first row\ndf = df.iloc[1:]\ndf = df.reset_index(drop = True)\n\n# Change first column to integer data type\n# the first row made this column a mixed data type column causing it to take alot of memory\ndf = df.astype({'Time from Start to Finish (seconds)': 'int32'})","4f0ee1b6":"# Remove first column (optional)\n# df = df.iloc[:,1:]","580f2d94":"# Drop rows were Q5 is empty\n# Very ineficient way (looping through and dropping takes a long time, better to select)\n# Drop rows where question 5 is empty\n#q5_drop_count = 0\n#for i in range(len(df)):\n    #if pd.isna(df.at[i, 'Q5']):\n        #q5_drop_count += 1\n        #df = df.drop(i)\n# Reset index\n#df = df.reset_index(drop = True)\n# Display rows dropped\n#print('rows dropped because q5 was empty: ' + str(q5_drop_count))","61d5cc7a":"# Select all data where Q5 contains an asnwer \ndf = df[df['Q5'].notna()]\nlen(df)","96e74117":"# Select only US participants\ndf = df.loc[(df['Q3'] == 'United States of America')]\ndf = df.reset_index(drop = True)","4cb71da8":"# Num of rows, will be used later to check process\nlen(df)","bd05afa4":"# Question 7\n# Check current count of 'None' \ndf['Q7_Part_12'].value_counts()","65bbd093":"# Inefficient way to change question 7 part 12 to none\n# Question 7\n# If all parts of question 7 are empty, assume none, so input \"None\" option\n#cols = ['Q7_Part_1', 'Q7_Part_2', 'Q7_Part_3', 'Q7_Part_4', 'Q7_Part_5', 'Q7_Part_6',\n               #'Q7_Part_7', 'Q7_Part_8', 'Q7_Part_9', 'Q7_Part_10', 'Q7_Part_11', 'Q7_Part_12' ,'Q7_OTHER']\n#for i in range(len(df)):\n    #check = []\n    #for x in cols:\n        #check_ele = pd.isnull(df.at[i,x]) # true if there is a value\n        #check.append(check_ele)\n    #if sum(check) == len(cols):\n        #df.at[i,'Q7_Part_12'] = 'None'\n\n# See new count of 'None'\n#df['Q7_Part_12'].value_counts()","a2901f0d":"# change question 7 to no programming experience to none if all are na\ndf['Q7_Part_12'] = np.where((df['Q7_Part_1'].isnull() & df['Q7_Part_2'].isnull() & df['Q7_Part_3'].isnull() \n                       & df['Q7_Part_4'].isnull() & df['Q7_Part_5'].isnull() & df['Q7_Part_6'].isnull() \n                       & df['Q7_Part_7'].isnull() & df['Q7_Part_8'].isnull() & df['Q7_Part_9'].isnull() \n                       & df['Q7_Part_10'].isnull() & df['Q7_Part_11'].isnull() & df['Q7_OTHER'].isnull()),\n                      'None',\n                      None) \n\n# create column to check for programming experience\ndf['Programming Experience'] = np.where((df['Q7_Part_12'] != 'None'), True, False)\ndf['Programming Experience'].value_counts()","7b7d0f78":"# change question 29A to no database experience to none if all are na\ndf['Q29_A_Part_17'] = np.where((df['Q29_A_Part_1'].isnull() & df['Q29_A_Part_2'].isnull() & df['Q29_A_Part_3'].isnull() \n                             & df['Q29_A_Part_4'].isnull() & df['Q29_A_Part_5'].isnull() & df['Q29_A_Part_6'].isnull() \n                             & df['Q29_A_Part_7'].isnull() & df['Q29_A_Part_8'].isnull() & df['Q29_A_Part_9'].isnull() \n                             & df['Q29_A_Part_10'].isnull() & df['Q29_A_Part_11'].isnull() & df['Q29_A_Part_12'].isnull()\n                             & df['Q29_A_Part_13'].isnull() & df['Q29_A_Part_14'].isnull() & df['Q29_A_Part_15'].isnull()\n                             & df['Q29_A_Part_16'].isnull() & df['Q29_A_OTHER'].isnull()),\n                            'None',\n                            None) \n\n# create column to check for programming experience\ndf['Database Experience'] = np.where((df['Q29_A_Part_17'] != 'None'), True, False)\n\ndf['Database Experience'].value_counts()","18096427":"# change question 31A to no business intelligence experience to none if all are na\ndf['Q31_A_Part_14'] = np.where((df['Q31_A_Part_1'].isnull() & df['Q31_A_Part_2'].isnull() & df['Q31_A_Part_3'].isnull() \n                             & df['Q31_A_Part_4'].isnull() & df['Q31_A_Part_5'].isnull() & df['Q31_A_Part_6'].isnull() \n                             & df['Q31_A_Part_7'].isnull() & df['Q31_A_Part_8'].isnull() & df['Q31_A_Part_9'].isnull() \n                             & df['Q31_A_Part_10'].isnull() & df['Q31_A_Part_11'].isnull() & df['Q31_A_Part_12'].isnull()\n                             & df['Q31_A_Part_13'].isnull() & df['Q31_A_OTHER'].isnull()),\n                            'None',\n                            None) \n\n# create column to check for programming experience\ndf['Business Intelligence Experience'] = np.where((df['Q31_A_Part_14'] != 'None'), True, False)\n\ndf['Business Intelligence Experience'].value_counts()","f95c3d1f":"# Make all NaN values as string 'N\/A' for easier programming later\ndf = df.fillna('N\/A')","47e7bb08":"# Create new dataframe with all 3 of skills\n# These dataframes will be helpful later\ndf_tri = df.loc[((df['Programming Experience'] == True ) & (df['Database Experience'] == True) \n                    & (df['Business Intelligence Experience'] == True))]","47f1f08a":"# Create new dataframe with 2 of skills\ndf_two = df.loc[((df['Programming Experience'] == True ) & (df['Database Experience'] == True) \n                    & (df['Business Intelligence Experience'] == False))\n                \n               | ((df['Programming Experience'] == True ) & (df['Database Experience'] == False) \n                    & (df['Business Intelligence Experience'] == True))\n                \n               | ((df['Programming Experience'] == False ) & (df['Database Experience'] == True) \n                    & (df['Business Intelligence Experience'] == True))]","fc6fd74d":"# Create new dataframe with only 1 of skills\ndf_one = df.loc[((df['Programming Experience'] == True ) & (df['Database Experience'] == False) \n                    & (df['Business Intelligence Experience'] == False))\n                \n               | ((df['Programming Experience'] == False ) & (df['Database Experience'] == True) \n                    & (df['Business Intelligence Experience'] == False))\n                \n               | ((df['Programming Experience'] == False ) & (df['Database Experience'] == False) \n                    & (df['Business Intelligence Experience'] == True))]","c23b73a9":"# Create new dataframe with 0 of skills\ndf_zero = df.loc[((df['Programming Experience'] == False ) & (df['Database Experience'] == False) \n                    & (df['Business Intelligence Experience'] == False))]","2a89af28":"# Check dataframe creations, should be len of df\nlen(df_tri) + len(df_two) + len(df_one) + len(df_zero)","3c25d47e":"# Add new column counting number of skills\n# Note python doesnt like this method -> df_tri['Count of Skills'] = 3\n\ndf_tri.insert(0, 'Count of Skills', 3)\ndf_two.insert(0, 'Count of Skills', 2)\ndf_one.insert(0, 'Count of Skills', 1)\ndf_zero.insert(0, 'Count of Skills', 0)","1bc10ef2":"# Put all dataframes together\nnewdf = pd.concat([df_tri, df_two, df_one, df_zero])\nnewdf = newdf.fillna('N\/A')","e1501e0d":"# Percentage plot\npercentage_group = newdf.groupby(['Count of Skills'])['Q1'].count()\npercentage_group = percentage_group \/ sum(percentage_group)\npercentage_group = percentage_group.reset_index().rename(columns = { 'Q1': 'Percentage'})\n\n# add another axis\ndata_crosstab = pd.crosstab(newdf['Count of Skills'], newdf['Q5'], margins = True)\npercentage_group.insert(0, 'Percent Data Scientists', 0)\ncounts_list = list(newdf['Count of Skills'].unique())\n\nfor i in counts_list:\n    ds = data_crosstab.at[i, 'Data Scientist'] \/ data_crosstab.at[i, 'All']\n    percentage_group.loc[i, 'Percent Data Scientists'] = ds","2435dd58":"# Figure 1 \n#sns.axes_style('dark')\n\n#Create combo chart\nfig, ax1 = plt.subplots(figsize=(10,6))\ncolor = 'tab:green'\n\n#bar plot creation\nax1.set_title('Figure 1: Proportion by Number of Skills', fontsize=16, weight='bold', pad=10, size=20)\nax1.set_xlabel('', fontsize=16)\nax1.set_ylabel('', fontsize=16, color = color)\nax1 = sns.barplot(x='Count of Skills', y='Percentage', data = percentage_group, palette='viridis')\nax1.tick_params(axis='y')\nax1.grid(False)\nax1.set(ylim=(0, .75))\n\n# change y axis\nvals = ax1.get_yticks()\nax1.set_yticklabels(['{:,.0%}'.format(x) for x in vals])\n\n# annotate bar plots\nfor p in ax1.patches:\n    ax1.annotate(format(p.get_height(), '.2%'),\n                   (p.get_x() + p.get_width() \/ 2., p.get_height()), \n                   ha = 'center', va = 'center', \n                   xytext = (0, 9), \n                   textcoords = 'offset points')\n    \n#format(p.get_height(), '.2f')\n#specify we want to share the same x-axis\nax2 = ax1.twinx()\ncolor = 'tab:red'\n\n#line plot creation\nax2.set_ylabel('', fontsize=16, color=color)\nax2 = sns.lineplot(x='Count of Skills', y='Percent Data Scientists', data = percentage_group, sort=False, color=color)\nax2.tick_params(axis='y', color=color)\nax2.grid(False)\n\n# change y axis\nvals = ax2.get_yticks()\nax2.set_yticklabels(['{:,.0%}'.format(x) for x in vals])\n\n#show plot\nplt.show()","76378048":"# Percentage plot\n# take percent_group from before\npercentage_group.insert(0, 'Q7_Part_12', 0)\npercentage_group.insert(0, 'Q29_A_Part_17', 0)\npercentage_group.insert(0, 'Q31_A_Part_14', 0)","0e7c9a38":"question_list = [ 'Q7_Part_12', 'Q29_A_Part_17', 'Q31_A_Part_14' ]\ncounts_list = [3,2,1,0]\n\nfor j in question_list:\n     \n    data_crosstab = pd.crosstab(newdf['Count of Skills'], newdf[j], margins = True)\n    \n    for i in counts_list:\n        ds = data_crosstab.at[i, 'None'] \/ data_crosstab.at[i, 'All']\n        ds = 1 - ds # inverse to get proportion of having this skill\n        percentage_group.loc[i, j] = ds","39ba7e21":"percentage_group2 = percentage_group.rename(columns={'Q7_Part_12': \"Programming\", \n                                 'Q29_A_Part_17': \"Big Data Tools\", \n                                 'Q31_A_Part_14': 'Business Intelligence Tools'})\npercentage_group2 = percentage_group2.drop(columns=['Percentage', 'Percent Data Scientists'])\n#percentage_group2","a0242542":"reformatted = pd.melt(percentage_group2, id_vars=\"Count of Skills\", var_name = 'Skills', value_name=\"Percentage\")\n#reformatted","af29aa9f":"#plot settings\nfig, (ax0, ax1, ax2, ax3) = plt.subplots(1,4,figsize=(16,3))\nfig.subplots_adjust(hspace=0.1, wspace=0.05)\n\n#barplots\ndf0 = reformatted.loc[(reformatted['Count of Skills'] == 0)]\ndf1 = reformatted.loc[(reformatted['Count of Skills'] == 1)]\ndf2 = reformatted.loc[(reformatted['Count of Skills'] == 2)]\ndf3 = reformatted.loc[(reformatted['Count of Skills'] == 3)]\n\n#=========\n# 0 skills\nsns.barplot(x = 'Skills', y = 'Percentage' , data = df0, ax = ax0, palette='viridis')\nax0.set_xticklabels(ax0.get_xticklabels(), rotation=45, horizontalalignment='right')\nax0.set(ylim=(0, 1.2))\nvals = ax3.get_yticks()\nax0.set_yticklabels(['{:,.0%}'.format(x) for x in vals])\nax0.title.set_text('Skill Count = 0')\n\n\n#=========\n# 1 skills\nsns.barplot(x = 'Skills', y = 'Percentage' , data = df1, ax = ax1, palette='viridis')\nax1.set_xticklabels(ax1.get_xticklabels(), rotation=45, horizontalalignment='right')\n\n# sub plot title\nax1.title.set_text('Skill Count = 1')\n\n# y axis\nax1.set(ylim=(0, 1.2))\n\n# remove y axis\nax1.set_ylabel('')\nax1.set_yticklabels([])\n\n# annotate bar plots\nfor p in ax1.patches:\n    ax1.annotate(format(p.get_height(), '.2%'),\n                   (p.get_x() + p.get_width() \/ 2., p.get_height()), \n                   ha = 'center', va = 'center', \n                   xytext = (0, 9), \n                   textcoords = 'offset points')\n\n#=========\n# 2 skills\nsns.barplot(x = 'Skills', y = 'Percentage' , data = df2, ax = ax2, palette='viridis')\nax2.set_xticklabels(ax2.get_xticklabels(), rotation=45, horizontalalignment='right')\n\n# sub plot title\nax2.title.set_text('Skill Count = 2')\n\n# y axis\nax2.set(ylim=(0, 1.2))\n\n# remove y axis\nax2.set_ylabel('')\nax2.set_yticklabels([])\n\n# annotate bar plots\nfor p in ax2.patches:\n    ax2.annotate(format(p.get_height(), '.2%'),\n                   (p.get_x() + p.get_width() \/ 2., p.get_height()), \n                   ha = 'center', va = 'center', \n                   xytext = (0, 9), \n                   textcoords = 'offset points')\n    \n#=========\n# 3 skills\nsns.barplot(x = 'Skills', y = 'Percentage', data = df3, ax = ax3, palette='viridis')\nax3.set_xticklabels(ax3.get_xticklabels(), rotation=45, horizontalalignment='right')\n\n# sub plot title\nax3.title.set_text('Skill Count = 3')\n\n# y axis\nax3.set(ylim=(0, 1.2))\n\n# remove y axis\nax3.set_ylabel('')\nax3.set_yticklabels([])\n\n# annotate bar plots\nfor p in ax3.patches:\n    ax3.annotate(format(p.get_height(), '.2%'),\n                   (p.get_x() + p.get_width() \/ 2., p.get_height()), \n                   ha = 'center', va = 'center', \n                   xytext = (0, 9), \n                   textcoords = 'offset points')\n\nplt.text(0.85, 1.25, 'Figure 2: Percent of Trifecta Skills Obtained by Number of Skill Group',weight='bold',\n         horizontalalignment='center',fontsize=20,transform = ax1.transAxes)","e92927a1":"# df : the dataframe that contains predicted clusters (null values should be string '0')\n# x_category : takes one string of column name (column should be in df)\n# y_category : takes one string of cluster column name (this should be in df) (values should be integers)\n# size : takes (x,y) format for size of heatmap\n# x_sort : takes list of custom sorted categories for x axis\n# y_sort : takes list of custom sorted categories for y axis\n# title : takes string for title\n\ndef probability_heatmap_single_col(df, x_category, y_category, x_sort, y_sort, size, title):\n    #df = df.fillna('N\/A')\n    # Intialize\n    intialize_df = pd.DataFrame(columns = [y_category, x_category, 'probability'])\n    y_values = sorted(df[y_category].unique())\n    x_values = sorted(df[x_category].unique()) \n\n    #x_values.remove('N\/A')\n    \n    data_crosstab = pd.crosstab(df[y_category], df[x_category], margins = True)\n    #data_crosstab = data_crosstab.drop('0', axis = 1)\n\n    # calculations\n    for j in x_values:   \n        for i in y_values:\n            prob = data_crosstab.at[i, j] \/ data_crosstab.at[i, 'All']\n            intialize_df = intialize_df.append({y_category : i, x_category : j, 'probability':prob}, \n                                               ignore_index=True)\n            \n    #pivot map for seaborn heatmap\n    pivot_df = intialize_df.pivot(index = y_category, columns = x_category, values = 'probability')\n    \n    # custom sorting Y-axis\n    pivot_df.index = pd.CategoricalIndex(pivot_df.index, categories= y_sort)\n    pivot_df.sort_index(level=0, inplace=True)\n        \n    # custom sorting X-axis\n    pivot_df = pivot_df[x_sort]\n\n    #set correlation graph size\n    sns.set(font_scale=1)\n    plt.figure(figsize = size)\n\n    #correlation graph settings\n    ax = sns.heatmap(pivot_df, vmin=0., vmax=1,cmap='viridis', fmt='.2f', annot = True,square = False,\n                     linewidths = .5, cbar = True)\n\n    ax.set_xticklabels(ax.get_xticklabels(), rotation=45, horizontalalignment='right')\n\n    # set title\n    plt.title(title, fontsize = 20, weight='bold', pad=10, size=20) \n    # show\n    plt.show()","d57493bf":"probability_heatmap_single_col(df = newdf, x_category = 'Q5' , y_category = 'Count of Skills', \n                               x_sort = ['Other', 'Currently not employed', 'Student', \n                                         'Product\/Project Manager', 'Statistician', 'Business Analyst', \n                                         'DBA\/Database Engineer', 'Data Engineer', 'Machine Learning Engineer', \n                                         'Research Scientist','Software Engineer','Data Analyst',\n                                         'Data Scientist'],\n                               y_sort= [3, 2, 1, 0], size = (20,5), title = 'Figure 3: Job Roles Proportion')","9b217c3d":"# df : the dataframe that contains predicted clusters (null values should be string '0')\n# columns : takes list of column names (columns should be in df)\n# y_category : takes one string of cluster column name (this should be in df) (values should be integers)\n# size : takes (x,y) format for size of heatmap\n# title : takes string for title\n\ndef probability_heatmap_multi_col(df,columns, y_category, x_sort, optional_size = (15,10), title = ''):\n    intialize_df = pd.DataFrame(columns = [y_category, ' ', 'probability'])\n    clusters_list = sorted(df[y_category].unique())\n    \n    for j in columns:\n        data_crosstab = pd.crosstab(df[y_category], df[j], margins = True)\n        \n        column_value = list(df[j].unique())\n        column_value.remove('N\/A')\n        column_value = column_value[0]\n        \n        for i in clusters_list:\n            prob = data_crosstab.at[i, column_value] \/ data_crosstab.at[i, 'All']\n            \n            intialize_df = intialize_df.append({y_category : i, ' ' : column_value, 'probability':prob}, ignore_index=True)\n            \n    #pivot map for seaborn heatmap\n    pivot_df = intialize_df.pivot(index = y_category, columns = ' ', values = 'probability')\n    \n    # custom sorting Y-axis\n    pivot_df.index = pd.CategoricalIndex(pivot_df.index, categories= [3, 2, 1, 0])\n    pivot_df.sort_index(level=0, inplace=True)\n    \n    # custom sorting X-axis\n    pivot_df = pivot_df[x_sort]\n\n    #set correlation graph size\n    sns.set(font_scale=1)\n    plt.figure(figsize = (optional_size))\n\n    #correlation graph settings\n    ax = sns.heatmap(pivot_df, vmin=0., vmax=1,cmap='viridis', fmt='.2f', annot = True,square = False,\n                     linewidths = .5, cbar = True)\n    ax.set_xticklabels(ax.get_xticklabels(), rotation=45, horizontalalignment='right')\n\n    # set title\n    plt.title(title, fontsize = 20, weight='bold', pad=10, size=20) \n    # show\n    plt.show()","e3676e96":"Q23_cols = ['Q23_Part_1', 'Q23_Part_2', 'Q23_Part_3', 'Q23_Part_4', 'Q23_Part_5', 'Q23_Part_6',\n           'Q23_Part_7', 'Q23_OTHER']\nx_sort = ['Analyze and understand data to influence product or business decisions', \n          'Build and\/or run the data infrastructure that my business uses for storing, analyzing, and operationalizing data',\n          'Build prototypes to explore applying machine learning to new areas', \n          'Experimentation and iteration to improve existing ML models', \n          'Build and\/or run a machine learning service that operationally improves my product or workflows', \n          'Do research that advances the state of the art of machine learning', \n          'None of these activities are an important part of my role at work',\n         'Other']\nnewdf = newdf.fillna('N\/A')\n\nprobability_heatmap_multi_col(df = newdf, columns = Q23_cols , x_sort = x_sort, y_category = 'Count of Skills', optional_size = (10,5), \n                               title = 'Figure 4: Job Description Proportion')","a418ddf9":"newdf = newdf.fillna('N\/A')\nprobability_heatmap_single_col(df = newdf, x_category = 'Q24' , y_category = 'Count of Skills', \n                               x_sort = ['N\/A','$0-999','1,000-1,999','2,000-2,999','3,000-3,999','4,000-4,999',\n                                         '5,000-7,499', '7,500-9,999', '10,000-14,999', '15,000-19,999',\n                                         '20,000-24,999', '25,000-29,999','30,000-39,999', \n                                         '40,000-49,999','50,000-59,999','60,000-69,999','70,000-79,999',\n                                         '80,000-89,999', '90,000-99,999','100,000-124,999', \n                                         '125,000-149,999','150,000-199,999','200,000-249,999',\n                                         '250,000-299,999','300,000-500,000', '> $500,000'],\n                               y_sort = [3, 2, 1, 0],\n                               size = (25,5), \n                               title = 'Figure 5: Salary Proportion')","ebbba8af":"#plot settings\nfig, axs = plt.subplots(2,2,figsize=(15,15),sharex='col', sharey='row')\nfig.subplots_adjust(hspace=0.15, wspace=0.15)\n(ax0, ax1), (ax2, ax3) = axs\n\n#===================\n# Skill 0 count point\n\n# Aggregate counts as percentages\ngrouped_count_0 = df_zero.groupby(by = ['Q1']).count().reset_index(drop = False)[['Q1', 'Count of Skills']]\ngrouped_count_0['Count of Skills'] = grouped_count_0['Count of Skills'] \/ sum(grouped_count_0['Count of Skills'])\ngrouped_count_0 = grouped_count_0.rename(columns={'Count of Skills': 'Percent'})\npercent_df_0 = grouped_count_0\npercent_df_0\n\n# plot options\nsns.barplot(x = 'Q1', y = 'Percent', data = percent_df_0, order = sorted(percent_df_0['Q1'].unique()), ax = ax0,\n            palette='viridis')\nax0.set_xticklabels(ax0.get_xticklabels(), rotation=45, horizontalalignment='right')\nax0.set(ylim=(0, .3))\nax0.set_xlabel('')\nvals = ax0.get_yticks()\nax0.set_yticklabels(['{:,.0%}'.format(x) for x in vals])\nax0.title.set_text('Skill Count = 0')\n\n\n# annotate bar plots\nfor p in ax0.patches:\n    ax0.annotate(format(p.get_height(), '.2%'),\n                   (p.get_x() + p.get_width() \/ 2., p.get_height()), \n                   ha = 'center', va = 'center', \n                   xytext = (0, 9), \n                   textcoords = 'offset points')\n\n#===================\n# Skill 1 count point\n\n# Aggregate counts as percentages\ngrouped_count_1 = df_one.groupby(by = ['Q1']).count().reset_index(drop = False)[['Q1', 'Count of Skills']]\ngrouped_count_1['Count of Skills'] = grouped_count_1['Count of Skills'] \/ sum(grouped_count_1['Count of Skills'])\ngrouped_count_1 = grouped_count_1.rename(columns={'Count of Skills': 'Percent'})\npercent_df_1 = grouped_count_1\npercent_df_1\n\n# plot options\nsns.barplot(x = 'Q1', y = 'Percent', data = percent_df_1, order = sorted(percent_df_1['Q1'].unique()), ax = ax1,\n            palette='viridis')\nax1.set_xticklabels(ax1.get_xticklabels(), rotation=45, horizontalalignment='right')\nax1.set(ylim=(0, .3))\nax1.set_ylabel('')\nax1.set_xlabel('')\nax1.title.set_text('Skill Count = 1')\n\n# annotate bar plots\nfor p in ax1.patches:\n    ax1.annotate(format(p.get_height(), '.2%'),\n                   (p.get_x() + p.get_width() \/ 2., p.get_height()), \n                   ha = 'center', va = 'center', \n                   xytext = (0, 9), \n                   textcoords = 'offset points')\n    \n#===================    \n# Skill 2 count point\n\n# Aggregate counts as percentages\ngrouped_count_2 = df_two.groupby(by = ['Q1']).count().reset_index(drop = False)[['Q1', 'Count of Skills']]\ngrouped_count_2['Count of Skills'] = grouped_count_2['Count of Skills'] \/ sum(grouped_count_2['Count of Skills'])\ngrouped_count_2 = grouped_count_2.rename(columns={'Count of Skills': 'Percent'})\npercent_df_2 = grouped_count_2\npercent_df_2\n\n# plot options\nsns.barplot(x = 'Q1', y = 'Percent', data = percent_df_2, order = sorted(percent_df_2['Q1'].unique()), ax = ax2,\n            palette='viridis')\nax2.set_xticklabels(ax2.get_xticklabels(), rotation=45, horizontalalignment='right')\nax2.set(ylim=(0, .3))\nvals = ax2.get_yticks()\nax2.set_yticklabels(['{:,.0%}'.format(x) for x in vals])\nax2.title.set_text('Skill Count = 2')\n\n# annotate bar plots\nfor p in ax2.patches:\n    ax2.annotate(format(p.get_height(), '.2%'),\n                   (p.get_x() + p.get_width() \/ 2., p.get_height()), \n                   ha = 'center', va = 'center', \n                   xytext = (0, 9), \n                   textcoords = 'offset points')\n    \n#===================\n# Skill 3 bar plot\n\n# Aggregate counts as percentages\ngrouped_count_3 = df_tri.groupby(by = ['Q1']).count().reset_index(drop = False)[['Q1', 'Count of Skills']]\ngrouped_count_3['Count of Skills'] = grouped_count_3['Count of Skills'] \/ sum(grouped_count_3['Count of Skills'])\ngrouped_count_3 = grouped_count_3.rename(columns={'Count of Skills': 'Percent'})\npercent_df_3 = grouped_count_3\npercent_df_3\n\n# plot options\nsns.barplot(x = 'Q1', y = 'Percent', data = percent_df_3, order = sorted(percent_df_3['Q1'].unique()), ax = ax3,\n            palette='viridis')\nax3.set_xticklabels(ax3.get_xticklabels(), rotation=45, horizontalalignment='right')\nax3.set(ylim=(0, .3))\nax3.set_ylabel('')\nax3.title.set_text('Skill Count = 3')\n\n# annotate bar plots\nfor p in ax3.patches:\n    ax3.annotate(format(p.get_height(), '.2%'),\n                   (p.get_x() + p.get_width() \/ 2., p.get_height()), \n                   ha = 'center', va = 'center', \n                   xytext = (0, 9), \n                   textcoords = 'offset points')\n\n# title\nplt.text(1.05, 1.15, 'Figure 6: Age Percent Distribution by Number of Skill Group',weight='bold',\n         horizontalalignment='center',fontsize=20,transform = ax0.transAxes)","a1b07c03":"# get percentages df pivoted\ndfs = [df_zero, df_one, df_two, df_tri]\ncount_order = [0, 1, 2, 3]\n\ndf_pivoted = pd.DataFrame(columns = ['Skill Count', 'Labels', 'Proportion', 'Percent'])\n\nfor (df, count) in zip(dfs, count_order): \n    data_crosstab = pd.crosstab(df['Count of Skills'], df['Q2'], margins = True)\n    labels = list(df['Q2'].unique())\n    for label in labels:\n        prob = data_crosstab.at[count, label ] \/ data_crosstab.at[count, 'All']\n        new_row = {'Skill Count' : count, 'Labels' : label, 'Proportion' : prob, 'Percent' : round(prob * 100,2)}\n        df_pivoted = df_pivoted.append(new_row ,ignore_index = True)    \n\ndf_pie_0 = df_pivoted.loc[(df_pivoted['Skill Count'] == 0 )]\ndf_pie_0 = df_pie_0.sort_values(by = 'Proportion', ascending = True).reset_index(drop = True)\n\ndf_pie_1 = df_pivoted.loc[(df_pivoted['Skill Count'] == 1 )]\ndf_pie_1 = df_pie_1.sort_values( by = 'Proportion', ascending = True).reset_index(drop = True)\n\ndf_pie_2 = df_pivoted.loc[(df_pivoted['Skill Count'] == 2 )]\ndf_pie_2 = df_pie_2.sort_values( by = 'Proportion', ascending = True).reset_index(drop = True)\n\ndf_pie_3 = df_pivoted.loc[(df_pivoted['Skill Count'] == 3 )]\ndf_pie_3 = df_pie_3.sort_values( by = 'Proportion', ascending = True).reset_index(drop = True)\n\n#plot settings\nfig, axs = plt.subplots(2,2,figsize=(15,15))\nfig.subplots_adjust(hspace=0.15, wspace=0.15)\n(ax0, ax1), (ax2, ax3) = axs\n\n# color mapping labels\nimport matplotlib\n\ncmap = matplotlib.cm.get_cmap('viridis')\n\nc1 = cmap(0)\nc2 = cmap(.25)\nc3 = cmap(.50)\nc4 = cmap(.75)\nc5 = cmap(.99)\n\ncolours={'Man': c1, \n        'Nonbinary':c2,\n        'Woman' : c5,\n        'Prefer not to say' : c4,\n        'Prefer to self-describe' : c3}\n\n# ========================\n# ax0\nlabels = df_pie_0['Labels']\npercentages = df_pie_0['Percent']\nexplode=([.01] * len(labels))\n\n\nax0.pie(percentages, explode=explode, labels=labels, colors=[colours[key] for key in labels], \n        autopct='%1.0f%%', \n       shadow=False, startangle=180,   \n       pctdistance=1.2,labeldistance=1.4)\nax0.set_title(\"Skill Count = 0\",fontsize=15, weight='bold')\n\n# ========================\n# ax1\nlabels = df_pie_1['Labels']\npercentages = df_pie_1['Percent']\nexplode=([.01] * len(labels))\n\n\nax1.pie(percentages, explode=explode, labels=labels,  colors=[colours[key] for key in labels],\n        autopct='%1.0f%%', \n       shadow=False, startangle=180,   \n       pctdistance=1.2,labeldistance=1.4)\nax1.set_title(\"Skill Count = 1\",fontsize=15, weight='bold')\nax1.legend(frameon=True, bbox_to_anchor=(1.5,0.8))\n\n# ========================\n# ax2\nlabels = df_pie_2['Labels']\npercentages = df_pie_2['Percent']\nexplode=([.01] * len(labels))\n\nax2.pie(percentages, explode=explode, labels=labels, colors=[colours[key] for key in labels],\n        autopct='%1.0f%%', \n       shadow=False, startangle=180,   \n       pctdistance=1.2,labeldistance=1.4)\nax2.set_title(\"Skill Count = 2\",fontsize=15, weight='bold')\n\n# ========================\n# ax3\nlabels = list(df_pie_3['Labels'])\npercentages = list(df_pie_3['Percent'])\nexplode=([.01] * len(labels))\n\nax3.pie(percentages, explode=explode, labels=labels,  \n        autopct='%1.0f%%', \n       shadow=False, startangle=180,   \n       pctdistance=1.2,labeldistance=1.4,colors=[colours[key] for key in labels])\nax3.set_title(\"Skill Count = 3\", fontsize=15, weight='bold')\n\n\n# Title\nplt.text(1.05, 1.15, 'Figure 7: Gender Distribution by Number of Skill Group',weight='bold',\n         horizontalalignment='center',fontsize=20,transform = ax0.transAxes)","d7b871df":"#plot settings\nfig, axs = plt.subplots(2,2,figsize=(15,15),sharex='col', sharey='row')\nfig.subplots_adjust(hspace=0.15, wspace=0.15)\n(ax0, ax1), (ax2, ax3) = axs\n\norder = ['Bachelor\u2019s degree', 'Master\u2019s degree', 'Doctoral degree', \n         'Some college\/university study without earning a bachelor\u2019s degree', 'Professional degree', \n         'No formal education past high school']\n#===================\n# Skill 0 count point\n\n# Aggregate counts as percentages\ngrouped_count_0 = df_zero.groupby(by = ['Q4']).count().reset_index(drop = False)[['Q4', 'Count of Skills']]\ngrouped_count_0['Count of Skills'] = grouped_count_0['Count of Skills'] \/ sum(grouped_count_0['Count of Skills'])\ngrouped_count_0 = grouped_count_0.rename(columns={'Count of Skills': 'Percent'})\npercent_df_0 = grouped_count_0\npercent_df_0\n\n# plot options\nsns.barplot(x = 'Q4', y = 'Percent', data = percent_df_0, order = order, ax = ax0,\n            palette='viridis')\nax0.set_xticklabels(ax0.get_xticklabels(), rotation=45, horizontalalignment='right')\nax0.set(ylim=(0, .7))\nax0.set_xlabel('')\nvals = ax0.get_yticks()\nax0.set_yticklabels(['{:,.0%}'.format(x) for x in vals])\nax0.title.set_text('Skill Count = 0')\n\n\n# annotate bar plots\nfor p in ax0.patches:\n    ax0.annotate(format(p.get_height(), '.2%'),\n                   (p.get_x() + p.get_width() \/ 2., p.get_height()), \n                   ha = 'center', va = 'center', \n                   xytext = (0, 9), \n                   textcoords = 'offset points')\n\n#===================\n# Skill 1 count point\n\n# Aggregate counts as percentages\ngrouped_count_1 = df_one.groupby(by = ['Q4']).count().reset_index(drop = False)[['Q4', 'Count of Skills']]\ngrouped_count_1['Count of Skills'] = grouped_count_1['Count of Skills'] \/ sum(grouped_count_1['Count of Skills'])\ngrouped_count_1 = grouped_count_1.rename(columns={'Count of Skills': 'Percent'})\npercent_df_1 = grouped_count_1\npercent_df_1\n\n# plot options\nsns.barplot(x = 'Q4', y = 'Percent', data = percent_df_1, order = order, ax = ax1,\n            palette='viridis')\nax1.set_xticklabels(ax1.get_xticklabels(), rotation=45, horizontalalignment='right')\nax1.set(ylim=(0, .7))\nax1.set_ylabel('')\nax1.set_xlabel('')\nax1.title.set_text('Skill Count = 1')\n\n# annotate bar plots\nfor p in ax1.patches:\n    ax1.annotate(format(p.get_height(), '.2%'),\n                   (p.get_x() + p.get_width() \/ 2., p.get_height()), \n                   ha = 'center', va = 'center', \n                   xytext = (0, 9), \n                   textcoords = 'offset points')\n    \n#===================    \n# Skill 2 count point\n\n# Aggregate counts as percentages\ngrouped_count_2 = df_two.groupby(by = ['Q4']).count().reset_index(drop = False)[['Q4', 'Count of Skills']]\ngrouped_count_2['Count of Skills'] = grouped_count_2['Count of Skills'] \/ sum(grouped_count_2['Count of Skills'])\ngrouped_count_2 = grouped_count_2.rename(columns={'Count of Skills': 'Percent'})\npercent_df_2 = grouped_count_2\npercent_df_2\n\n# plot options\nsns.barplot(x = 'Q4', y = 'Percent', data = percent_df_2, order = order, ax = ax2,\n            palette='viridis')\nax2.set_xticklabels(ax2.get_xticklabels(), rotation=45, horizontalalignment='right')\nax2.set(ylim=(0, .7))\nvals = ax2.get_yticks()\nax2.set_yticklabels(['{:,.0%}'.format(x) for x in vals])\nax2.title.set_text('Skill Count = 2')\n\n# annotate bar plots\nfor p in ax2.patches:\n    ax2.annotate(format(p.get_height(), '.2%'),\n                   (p.get_x() + p.get_width() \/ 2., p.get_height()), \n                   ha = 'center', va = 'center', \n                   xytext = (0, 9), \n                   textcoords = 'offset points')\n    \n#===================\n# Skill 3 bar plot\n\n# Aggregate counts as percentages\ngrouped_count_3 = df_tri.groupby(by = ['Q4']).count().reset_index(drop = False)[['Q4', 'Count of Skills']]\ngrouped_count_3['Count of Skills'] = grouped_count_3['Count of Skills'] \/ sum(grouped_count_3['Count of Skills'])\ngrouped_count_3 = grouped_count_3.rename(columns={'Count of Skills': 'Percent'})\npercent_df_3 = grouped_count_3\npercent_df_3\n\n# plot options\nsns.barplot(x = 'Q4', y = 'Percent', data = percent_df_3, order = order, ax = ax3,\n            palette='viridis')\nax3.set_xticklabels(ax3.get_xticklabels(), rotation=45, horizontalalignment='right')\nax3.set(ylim=(0, .7))\nax3.set_ylabel('')\nax3.title.set_text('Skill Count = 3')\n\n# annotate bar plots\nfor p in ax3.patches:\n    ax3.annotate(format(p.get_height(), '.2%'),\n                   (p.get_x() + p.get_width() \/ 2., p.get_height()), \n                   ha = 'center', va = 'center', \n                   xytext = (0, 9), \n                   textcoords = 'offset points')\n\n# title\nplt.text(1.05, 1.15, 'Figure 8: Education Percent Distribution by Number of Skill Group',weight='bold',\n         horizontalalignment='center',fontsize=20,transform = ax0.transAxes)","76b2a3b9":"Ok, what about education of the skill groups? Let's compare the education levels across the age groups with Figure 8. We can ee that high skill count groups tend to have a master's degree as their highest or currently actively pursueing that degree. There is a link here, participants with more skills have a high education. ","07dd592a":"## Table of Contents","040b665c":"Now we'll take a look at the gender proportions of each skill group. From Figure 7 we can see that men represent more than the majority in all skill groups. However, men generally make up a smaller percentage as the skill count goes down but skill count 2 and 3 are rougly similar percentages at 81% and 80% respectively. \n\nFrom what we've learned so far, we know that participants with a low skill count tend to be younger and are students, and participants with higher skill counts tend to be older and are professionals. So its interesting to see here that women make up a a larger portion at the lower skill count group than the higher skill count group. It is possible that more women are getting more interested in data science. Women have been historically underrepresented in STEM areas of study. We can also infer that women studying in STEM is more promeninent in this day and age than previous years. ","0c08b570":"    Objective\nI am currently on winter break from my masters program and I wanted to to brush up on my data science skills in preparation for applying for jobs. Upon looking at some job postings, I noticed a pattern: most data science related positions require skills in Python, SQL, and Tableau. I wondered, do you really need all three of these for a data science job? What if you only have one, two, three, or even none of these skills? How do these skills relate to other positions suchs as data analysts and business analysts? This is my quest: to understand the impact of having the \"trifecta\" of data science skills. \n\nIt's necessary to point out that the skills are not mutually exclusively in terms of analytics accomplished and purpose. Python can also query data just like SQL and also produce vibrant visualizations like Tableau. SQL is also a programming language but it specializes in querying and storing data and is not as multi-faceted as Python. \n\nNote: I will be referencing these skills as the \"trifecta\" because of how prevalant I've heard about them in school, job postings, and fellow students. \n\n    Short Answer:\n\nThe answer to my question of do I really need to know Python\/SQL\/Tableau is... yes. The distinction mostly occurs when comparing participants with a low count of skills (0 or 1 of the skills) to high count of skills (2 or 3 of the skills). Participants with a high count, generally have more of a likelihood of having a data science role, and a higher salary. \n\n    Background\nI have left my engineering role in pursuit of something more technical, data orientated, and where reasoning is powered by science. I started in January and have obtained all three skills in by May, although by all means not an expert in each. I was able to work on an internship that required heavy usage of SQL during the summer. So I have a feel for what it is like to have one of the trifecta skills. At these point, I consider myself just crossing the line from begineer to intermediate. Since data science roles are scarce, I hope to start out with a job related to data, most likely a business or data analyst, then move to data science. ","7d9bbf6a":"First, we'll take a simple look at the participants and what percent of them have a certain amount of skills. We'll also compare to that group's percent of people who responded as being a data scientist. This will give us a general idea of what is to follow. \n\nFrom the plot, we can see that almost half of the participants have one of the skills. As the number of trifecta skills increase, the percent of that group consisting of data scientists also increases. This is interesting, now we have an idea that number of the trifecta skills correlate to being a data scientist.","d895bdb2":"1. [Introduction](#intro)\n2. [Data Clean and Prep](#clean)\n3. [Part 1: Analysis](#part1)\n4. [Part 2: Demographics](#part2)","a4d53255":"## Part 2: Demographics <a id = 'part2'><\/a>","ea4f588c":"Okay, last heatmap. Now let's take a look at salary levels per count of the trifecta skills. Count of trifecta skills is on the y-axis and salary is on the x-axis, ordered by dollar value. This is a very intereting plot. Almost half of low count of trifecta skills ( 1 to 0) do not answer the question. It is possible that participants are very confidential about their salary. Another possiblity is that participants have the option to skip this question and did not answer because they did not have a salary. For the high count of trifecta skills (2 or 3), participants tend to concentrate around the salary ranges of 100,000 to 199,999.","88627c40":"## Data Clean and Prep <a id = 'clean'><\/a>","f0475344":"## Introduction <a id = 'intro'><\/a>","6ba3f316":"## Part 1: Analysis <a id = 'part1'><\/a>","c8412804":"I suspect the group with one skill will most likely have programming skills. Let's take a quick look at that at Figure 2. From the figure, we can see that participants with one of the trifecta skills have an overwelmingly proportion of the skills in programming. As noted early in this analysis, skills are not mutally exclusive. Programming can also accomplish similar functions of the big data and business intelligence tools. It's possible that people with one skill can be compensating the others by programming. \n\nLooking at the \"Skill Count = 2\" subplot, we can see that most participants with two skills tend to have experience in programming and big data tools.","1b62a8ca":"Let's compare the count of trifecta skills to job roles (Question 5). Heatmaps are my favorite and we will be using quite a bit of heatmaps. The y-axis will be denoting the number of trifecta skills. The x-axis will be the job titles, ordered by level of relation to data scientists. On the left will be non-related titles such as \"Other\" and \"Currently not employed\". Going to the right we have roles that are subjectively ordered by level of data science involved, with \"Data Scientist\" at the right most.\n\nFrom the Figure 3 below, we can decipher two insights from this plot.\n\n1) Participants with low count of skills ( 1 or 0) tend to be Students, Other, and Currently not employed.\n\n2) Participants with high number of skills (2 or 3) tend to be Software Engineers, Data Analysts, and Data Scientists. \n\nSo, if you have two or three skills, it is more likely you will be a data scientist.","0af6eb88":"We have answered the main question of this notebook in Part 1: Analysis. Part 2 will be a bonus section where I explore more qualities of the different skill groups. \n\nThe question for this part is: \"Who are these people\"? \n\nLet's take a look at the demographics. First up, age. We can plot separate bar graphs for each skill group and see the age distribution. \n\n    Insight from Figure 6 below:\n\n1) We can see that participants with one of the skills, tend to concentrate around the age group of 25-29 and have a skewed normal distribution. \n\n2) Participants with all three of the skills have a more even distribution when compared to the skill group of 1. \n","4092fc15":"Before I begin to explain my reasoning, we need to prep the data, and make some assumptions.\n\n    Prep work:\n1) I will only be only selecting participants from the United States of America. I understand that there can be slight nuances going from country to country; salaries will need to be adjusted,  and job roles can vary. For simplicity, I will be concentrating on the United states of America, althought all the analysis can be easily redone for another country. \n\n2) I will drop all participants who have left question 5 (choosing title similar to current role) empty. Having an answer to this question is paramount to my analysis. \n\n3) It is helpful to drop the first column in the dataset (time used to answer survey) to greatly reduce the cpu usage. The row contains multiple datatypes so it uses alot of memory. It's also not necessary for my analysis and to prove my point. Alternatively, we can drop the first row first then change the data type of the first column to be integer. \n\n    Assumptions:\n1) Knowing Python will be expanded to having programming experience. Question 7 will be deciding question if the participant has programming experience or not. All programming experience will be grouped together. So, having R or Matlab or Python skills will be considered the same as having programming experience.\n\n2) Having SQL skills will be expanded to having big data product experience. Question 29 will be the deciding question for having experience in big data products. SQL is a big data tool. So having at least 1 skill in the question will count towards having experience in big data. \n\n3) Having Tableau skills will be expanded to having business intelligence experience. Questions 31 will be the deciding question for having experience in business intelligence tools. Tableau is a business intelligence tool so having any other BI experience will deemed equal experience.\n\n4) For questions with the possiblility to select more than one answer choice, if they are all empty, then I assume the participant has none of those items. The answer choice will be changed to 'None'.","dbc62aeb":"    Conclusion\n\nWhat have we learned from our analysis? Here is a recap of what we learned from the plots we made:\n\n    Details:\n\nFigure 2:\nFor participants with \"Count of Skill = 1\", the participant is 99.65% chance of having that one skill in programming.\nFor participants with \"Count of Skill = 2\", the participant is 98.98% chance of having one skill in programming and a 86.26% propability of having the other skill in a big data tool. \n\nFigure 3:\nWhen comparing the proportion of Data Scientists for Count of Skill = 1, 2, 3, we have the proportions of 0.11, 0.26, 0.33 respectively. \n\nFigure 4:\nWhen comparing the job details between different skill count groups we generally see an increase in activities related to data, such as analyzing and understanding data to influence product or business decisions, increase as skills increase. For machine learning activities, such as building prototypes to explore applying machine learning to new areas, the greatest difference in proportions occurs whn comparing skill count group 1 to 2. \n\nFigure 5:\nWhen comparing the proportion of salary ranges from 100,000 to 199,999 for Count of Skill = 1, 2, 3 we have proportions of 0.17, 0.50, and 0.51, respectively.\n\n\n    Main Take-away:\n\nIf you want a higher chance of having a job title of Data Scientist, to perform job activities most relevant to data science, and to make a salary of 100,000 to 199,000, you have a higher chance if you have all three of the trifecta data science skills: programming, big data tools, and business intelligence tools.\n","596c34e6":"Job titles can be sometimes ambigous and not related to the type of work you do. Some of us have been there before. Let's suplement this by looking at question 23, data science work involved. Now, let's take a look at the count of trifecta skills to job activities related to data science. I've ordered the x-axis from left most being data orientated functions, the middle being machine learning functions, and the right being non-related. For this graph, keep in mind that participants can have more than one of the choices chosen in the x-axis. \n\nThere are two notable insights from this heatmap.\n\n1) Participants with a high count (2 or 3) of the trifecta skills tend to have more job acitivites related to data science. This includes activities such as analyzing data to influence decisions, building machine learning prototypes, and building data structures or storing, analyzing, and opertionalizing data. \n\n2) Participants with a low count (1 or 0) of the trifecta skills tend to job activities less related to data science, especially in the options related to ML. \n\nSo, if you have two or three of the skills, it is more likely you will be doing job activities related to data science. Are you starting to see a pattern yet? ","701be1ff":"# Do you really need to know Python\/SQL\/Tableau to become a Data Scientist?"}}