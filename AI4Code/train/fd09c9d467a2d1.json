{"cell_type":{"72947141":"code","0d2ae2f7":"code","f896b7ee":"code","31d1bfbe":"code","7ae55e33":"code","0322424c":"code","da7c09ae":"code","0b6e4815":"code","2179d11c":"code","5a05bf8c":"code","feb91713":"code","101e8e6f":"code","cc53d298":"code","3ea97478":"markdown","62ffad2f":"markdown","87a94ba0":"markdown","d58d8e54":"markdown","32ac421a":"markdown","864ee867":"markdown","75f36929":"markdown","16f736b1":"markdown","c1597e1d":"markdown"},"source":{"72947141":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","0d2ae2f7":"dataset = pd.read_csv(\"..\/input\/student-performance\/student-mat.csv\", sep=';')\ndataset.describe()","f896b7ee":"from sklearn.model_selection import train_test_split\n\nX = dataset.iloc[:,:-3]\ny = dataset.iloc[:,-1:]\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=2)","31d1bfbe":"X_train = pd.get_dummies(X_train, columns=['sex','address','address','famsize','school','Pstatus','Mjob','Fjob','reason','guardian','schoolsup','famsup','paid','activities','nursery','higher','internet','romantic'])\nX_train.dtypes\nX_test = pd.get_dummies(X_test, columns=['sex','address','address','famsize','school','Pstatus','Mjob','Fjob','reason','guardian','schoolsup','famsup','paid','activities','nursery','higher','internet','romantic'])","7ae55e33":"from sklearn import linear_model\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import cross_val_score\n\nregressor = linear_model.LinearRegression()\ndef fit_model(model, X_train, y_train):\n    \"\"\"\n    Convenient way of fitting the model and displaying the results\n    \"\"\"\n    regressor.fit(X_train, y_train)\n    print(\"train score:\" + str(regressor.score(X_train,y_train)))\n    predictions = regressor.predict(X_train)\n    print(\"train MSE: \" + str(mean_squared_error(predictions,y_train)))\n    print(\"test score:\" + str(regressor.score(X_test,y_test)))\n    predictions = regressor.predict(X_test)\n    print(\"test MSE: \" + str(mean_squared_error(predictions,y_test)))\n\ndef rmse_cv(model):\n    rmse= np.sqrt(-cross_val_score(model, X_train, y_train, scoring=\"neg_mean_squared_error\", cv = 3))\n    return(rmse)\n\nfit_model(regressor, X_train, y_train)","0322424c":"mean = sum(y_train.values) \/ len(y_train.values)\npredictions = []\nfor i in range(0, len(y_train)):\n    predictions.append(mean)\n\nprint(\"baseline train MSE: \" + str(mean_squared_error(predictions,y_train)))\nmean = sum(y_test.values) \/ len(y_test.values)\npredictions = []\nfor i in range(0, len(y_test)):\n    predictions.append(mean)\nprint(\"baseline test MSE: \" + str(mean_squared_error(predictions,y_test)))","da7c09ae":"from sklearn.ensemble import GradientBoostingRegressor\nregressor = GradientBoostingRegressor(n_estimators = 150)\nfit_model(regressor, X_train, y_train)","0b6e4815":"import warnings\nfrom sklearn.exceptions import DataConversionWarning\nwarnings.filterwarnings(action='ignore', category=DataConversionWarning)\n\n\nn_estimators = [150, 170 , 200, 400, 500]\ncv_rmse_gb = [rmse_cv(GradientBoostingRegressor(n_estimators = n_estimator)).mean() \n            for n_estimator in n_estimators]\nprint(cv_rmse_gb)","2179d11c":"from sklearn.ensemble import BaggingRegressor\nn_estimators = [170, 200, 250, 300]\ncv_rmse_br = [rmse_cv(BaggingRegressor(n_estimators = n_estimator)).mean() \n            for n_estimator in n_estimators]\nprint(cv_rmse_gb)","5a05bf8c":"from sklearn.ensemble import BaggingRegressor\nregressor = BaggingRegressor(n_estimators = 150)\nfit_model(regressor, X_train, y_train)","feb91713":"!pip install imblearn","101e8e6f":"!pip install delayed","cc53d298":"from imblearn.over_sampling import RandomOverSampler\n\nsampler = RandomOverSampler(random_state=0)\nX_res, y_res = sampler.fit_resample(X_train.values, y_train)\nregressor = BaggingRegressor(n_estimators = 150)\nfit_model(regressor, X_res, y_res)\nprint(np.sqrt(-cross_val_score(regressor, X_res, y_res, scoring=\"neg_mean_squared_error\", cv = 5)))","3ea97478":"Create Dummies","62ffad2f":"Let's try again with synthetic data","87a94ba0":"Linear regression time","d58d8e54":"Compare to baseline which will be predicting the mean each time","32ac421a":"Load Dataset","864ee867":"Split train test cutting out G1 and G2 as they are previous gradings that give too much information","75f36929":"So we are doing better than baseline but still nothing special so far. Lets see if we can do better.","16f736b1":"Let's try a bagging regressor","c1597e1d":"Let's try and optimise this a little"}}