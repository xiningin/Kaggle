{"cell_type":{"1b53af8c":"code","91350807":"code","43416db5":"code","3624dc81":"code","9482ead6":"code","1f1987a4":"code","3f6e5f52":"code","2536dd48":"code","b0a6e8c7":"code","835c41de":"markdown","351476d2":"markdown","2df3c107":"markdown","2be541c5":"markdown","29a38f15":"markdown","ddab3043":"markdown","1b443b10":"markdown","ef897001":"markdown","327db7d8":"markdown"},"source":{"1b53af8c":"%matplotlib inline\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport os\nimport warnings\nimport itertools\nfrom sklearn.model_selection import StratifiedKFold, StratifiedShuffleSplit\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import confusion_matrix, precision_recall_fscore_support\nfrom sklearn.preprocessing import normalize\n\nplt.rcParams.update({'figure.max_open_warning': 0})\nwarnings.simplefilter(action='ignore', category=FutureWarning)\n\nprint(os.listdir(\"..\/input\"))","91350807":"cc = pd.read_csv('..\/input\/creditcard.csv')","43416db5":"cc.head()","3624dc81":"# Split the column names into features and target\nfeature_columns = cc.columns[:-1].values\ntarget_column = cc.columns[-1:].values[0]","9482ead6":"plt.figure(figsize=(16,8))\nax = sns.countplot(x=target_column, data=cc)","1f1987a4":"cc.groupby(by='Class')['Class'].count()","3f6e5f52":"fig, axs = plt.subplots(feature_columns.size,1, figsize=(15, 6*feature_columns.size))\nfig.subplots_adjust(hspace = .5, wspace=.001)\n\naxs = axs.ravel()\n\nfor i,feature in enumerate(feature_columns):\n    c0_data = cc[cc['Class'] == 0][feature]\n    c1_data = cc[cc['Class'] == 1][feature]\n    axs[i].set_title(feature)\n    sns.kdeplot(c0_data, legend=False, color='g', ax=axs[i])\n    sns.rugplot(c0_data, color='g', ax=axs[i])\n    sns.kdeplot(c1_data, legend=False, color='r', ax=axs[i])\n    sns.rugplot(c1_data, color='r', ax=axs[i])","2536dd48":"def plot_confusion_matrix(cm, classes,\n                          title='Confusion matrix',\n                          cmap=plt.cm.Blues):\n\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=45)\n    plt.yticks(tick_marks, classes)\n\n    fmt = '.2f' if normalize else 'd'\n    thresh = cm.max() \/ 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, format(cm[i, j], fmt),\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    plt.tight_layout()\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n    plt.show()","b0a6e8c7":"# Just use some selected columns as features\nX = cc[['V10', 'V11','V12']].values\ny = cc[target_column].values\n\n#Lets split the dataset into a \nskf = StratifiedShuffleSplit(n_splits=1, train_size=0.5, test_size=0.5)\n\nfor i, (train_index, test_index) in enumerate(skf.split(X, y)):\n    X_train, X_test = X[train_index], X[test_index]\n    y_train, y_test = y[train_index], y[test_index]\n    \n    #print(\"Fold {0} Train Target: {1} Test Target: {2}\".format(i, np.unique(y_train, return_counts=True), np.unique(y_test, return_counts=True)))\n    \n    clf = LogisticRegression(random_state=0).fit(X_train, y_train)\n    preds = clf.predict(X_test)    \n    \n    cm = confusion_matrix(y_test,preds)\n    scores = precision_recall_fscore_support(y_test,preds)\n    print(\"UNBALANCED\")\n    \n    print(\"Precision: {0} \".format(scores[0]))\n    print(\"Recall: {0} \".format(scores[1]))\n    print(\"F1: {0} \".format(scores[2]))\n    plot_confusion_matrix(cm, ['correct','fraudulent'], title='LogisticRegression with default settings')\n    \n    \n    print('-'*50)\n    clf_balanced = LogisticRegression(random_state=0, class_weight='balanced').fit(X_train, y_train)\n    preds = clf_balanced.predict(X_test)\n    \n    cm_balanced = confusion_matrix(y_test,preds)\n    scores_balanced = precision_recall_fscore_support(y_test,preds)\n    print(\"BALANCED\")\n    print(\"Precision: {0} \".format(scores_balanced[0]))\n    print(\"Recall: {0} \".format(scores_balanced[1]))\n    print(\"F1: {0} \".format(scores_balanced[2]))\n    plot_confusion_matrix(cm_balanced, ['correct','fraudulent'], title='LogisticRegression with balanced classes')\n    \n    \n    clf_weights = LogisticRegression(random_state=0, class_weight={0: 0.001, 1: 0.999}).fit(X_train, y_train)\n    preds = clf_weights.predict(X_test)\n    \n    cm_weights = confusion_matrix(y_test,preds)\n    scores_weights = precision_recall_fscore_support(y_test,preds)\n    print(\"BALANCED\")\n    print(\"Precision: {0} \".format(scores_weights[0]))\n    print(\"Recall: {0} \".format(scores_weights[1]))\n    print(\"F1: {0} \".format(scores_weights[2]))\n    plot_confusion_matrix(cm_weights, ['correct','fraudulent'], title='LogisticRegression with custom class weights')","835c41de":"## 2. Now lets check the distribution of the target variable","351476d2":"## Findings\n- With balanced classes and custom class weights, we can improve the prediction of fraudulent transactions.\n- But this goes hand in hand with more correct transactions falsely classified as fraudulent","2df3c107":"## 1.  Setup environment and load the dataset","2be541c5":"## Findings\n\n- :-) What a surprise, the target variable is very unevenly distributed.","29a38f15":"## 4. Plot the distribution per class for each feature\n\n### Todos \/ Goals\n- I would like to determine whether there are features that can well divide the data set into the given classes.","ddab3043":"## 3. What should be the goal of a classifier\n\n- Well, the goal should be to predict the fraud as best as possible.\n- The goal should not be to predict correct transcations as good as possible. Because this is easy.  \nWe don't need a classifier for that. We can simply classify all transactions as correct and would have a accuracy of $\\frac{284315}{284315 + 492}$ (over 99%)","1b443b10":"## Findings\n\n- Hmm. Seems to be difficult, to separate the dataset by probabilities","ef897001":"## 5. Train a classifier\n\n### Todos \/ Goals:\n- Now I want to train a simple linear classifier. The goal is to compare the results with and without weighted classes.\n- I don't want to use all features, so I selected V10, V11 and V12 for further use","327db7d8":"# Handling imbalanced data #1\n\nInspired by this great [post](https:\/\/towardsdatascience.com\/handling-imbalanced-datasets-in-machine-learning-7a0e84220f28), I choosed this dataset to play a little bit with imbalanced data"}}