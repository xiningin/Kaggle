{"cell_type":{"0055f276":"code","e6ef18b1":"code","b2d919f7":"code","5c3f3886":"code","19c83c20":"code","034489ca":"code","4f0eb27f":"code","a57f5cea":"code","14ebfa3c":"code","35f6afac":"code","cab0cf27":"code","1e33d0d1":"code","349b68f3":"code","8a8fb30e":"code","8abbda9e":"code","6d5c7646":"code","07ab0067":"code","faa88769":"code","98410c4a":"code","2f4d52af":"code","494a5dcf":"code","0ca08774":"markdown","1a75d750":"markdown","83353265":"markdown","3e10f842":"markdown","6d95ef10":"markdown","7294a802":"markdown","4497d579":"markdown","63da8d27":"markdown"},"source":{"0055f276":"import pandas as pd \nimport numpy as np\npd.plotting.register_matplotlib_converters()\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nimport optuna\nfrom sklearn.model_selection import train_test_split, KFold, StratifiedKFold\nfrom lightgbm import LGBMClassifier\nfrom sklearn.metrics import roc_auc_score","e6ef18b1":"train = pd.read_csv('..\/input\/tabular-playground-series-mar-2021\/train.csv')\ntest = pd.read_csv('..\/input\/tabular-playground-series-mar-2021\/test.csv')","b2d919f7":"train.isnull().sum().values.sum()","5c3f3886":"test.isnull().sum().values.sum()","19c83c20":"train.head()","034489ca":"from sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\nfor i in range(19):\n    le.fit(list(train['cat'+str(i)])+list(test['cat'+str(i)]))\n    train['cat'+str(i)] = le.transform(train['cat'+str(i)])\n    test['cat'+str(i)] = le.transform(test['cat'+str(i)])","4f0eb27f":"X = train.iloc[:,1:-1].values\ny = train.iloc[:,-1].values\nX_test = test.iloc[:,1:]","a57f5cea":"X_train, X_dev, y_train, y_dev = train_test_split(X, y, test_size=0.15,random_state=42)","14ebfa3c":"lg = LGBMClassifier()\nlg.fit(X_train,y_train)\ny_pred_l = lg.predict_proba(X_dev)[:,1]\nroc_auc_score(y_dev,y_pred_l)","35f6afac":"def fun(trial,data=X,target=y):\n    \n    train_x, test_x, train_y, test_y = train_test_split(data, target, test_size=0.2,random_state=42)\n    param = {\n        'reg_alpha': trial.suggest_loguniform('reg_alpha', 1e-5, 10.0),\n        'reg_lambda': trial.suggest_loguniform('reg_lambda', 1e-5, 10.0),\n        'colsample_bytree': trial.suggest_uniform('colsample_bytree', 0.1, 0.9),\n        'subsample': trial.suggest_uniform('subsample', 0,1),\n        'learning_rate': trial.suggest_uniform('learning_rate', 0, 0.1 ),\n        'max_depth': trial.suggest_int('max_depth', 10,100),\n        'num_leaves' : trial.suggest_int('num_leaves', 1, 1000),\n        'min_child_samples': trial.suggest_int('min_child_samples', 1, 300),\n        'min_child_weight' : trial.suggest_loguniform('min_child_weight' , 1e-5 , 1),\n        'cat_smooth' : trial.suggest_int('cat_smooth', 1, 100),\n        'cat_l2': trial.suggest_int('cat_l2',1,20),\n        'min_data_per_group': trial.suggest_int('min_data_per_group', 50, 200),\n        \n        'metric': 'auc', \n        'random_state': 2021,\n        'n_estimators': 10000,\n        \n    }\n    model = LGBMClassifier(**param)  \n    \n    model.fit(train_x,train_y,eval_set=[(test_x,test_y)],early_stopping_rounds=200,verbose=False)\n    \n    preds = model.predict_proba(test_x)[:,1]\n    \n    auc = roc_auc_score(test_y, preds)\n    \n    return auc","cab0cf27":"study = optuna.create_study(direction='maximize')\nstudy.optimize(fun, n_trials=30)\nprint('Number of finished trials:', len(study.trials))\nprint('Best trial:', study.best_trial.params)","1e33d0d1":"#plot_optimization_histor: shows the scores from all trials as well as the best score so far at each point.\noptuna.visualization.plot_optimization_history(study)","349b68f3":"#plot_parallel_coordinate: interactively visualizes the hyperparameters and scores\noptuna.visualization.plot_parallel_coordinate(study)","8a8fb30e":"# plot_slice: shows the evolution of the search. You can see where in the hyperparameter space your search\n# went and which parts of the space were explored more.\noptuna.visualization.plot_slice(study)","8abbda9e":"#Visualize parameter importances.\noptuna.visualization.plot_param_importances(study)","6d5c7646":"best_params = study.best_params\nbest_params['n_estimators'] = 10000\nbest_params['cat_feature'] = [i for i in range(19)]\nbest_params['random_state'] = 2021\nbest_params['metric'] = 'auc'","07ab0067":"columns = [col for col in train.columns if col not in ['id','target'] ]","faa88769":"preds = np.zeros(X_test.shape[0])\nkf = StratifiedKFold(n_splits = 10 , random_state = 7 , shuffle = True)\nauc =[]\nn=0\n\nfor tr_idx, test_idx in kf.split(train[columns], train['target']):\n    \n    X_tr, X_val = train[columns].iloc[tr_idx], train[columns].iloc[test_idx]\n    y_tr, y_val = train['target'].iloc[tr_idx], train['target'].iloc[test_idx]\n    \n    model = LGBMClassifier(**best_params)\n    \n    model.fit(X_tr,y_tr,eval_set=[(X_val,y_val)],early_stopping_rounds=500,verbose=False)\n    \n    preds+=model.predict_proba(X_test)[:,1]\/kf.n_splits\n    auc.append(roc_auc_score(y_val, model.predict_proba(X_val)[:,1]))\n    print(n+1,auc[n])\n    n+=1","98410c4a":"np.mean(auc)","2f4d52af":"submission = pd.DataFrame({'id':test['id'],'target':preds})\nsubmission.to_csv('submit.csv',index=False)","494a5dcf":"submission.head()","0ca08774":"No null values!!","1a75d750":"# Hyperparameter tuning using Optuna","83353265":"### Same code with little variation in parameters and folds from my other notebook, Pleas check out that too [TPS March Lgbm with Optuna](https:\/\/www.kaggle.com\/nishantdhingra\/tps-march-lgbm-with-optuna)","3e10f842":"* Binary Classification problem based on real life data\n* We have to predict probabilities and the metric is ROC-AUC\n* This is a sample notebook which gives beginner approach to the data and hyperparameter tuning using Optuna\n\n\nI'm a beginner in this field too, Please give suggestions to improove the score!!\n","6d95ef10":"## Best Parameters found by Optuna","7294a802":"## Make Predictions","4497d579":"# Please upvote the notebook if it helped in any way! \n\n## Have a nice day :)","63da8d27":"# Baseline Model"}}