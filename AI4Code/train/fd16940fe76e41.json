{"cell_type":{"b0580f5d":"code","0e23256a":"code","729f0236":"code","ef70fc54":"code","1afab7ec":"code","4725a5a2":"code","28809fe4":"code","afaa6a0d":"code","7ddc5294":"code","811ffd1c":"code","b687d078":"code","39dc55fb":"code","5b51c990":"code","f9ef801e":"code","e7894500":"code","70ee21b5":"code","c1945648":"markdown","d9c74e12":"markdown","c5a34444":"markdown","5f5a5d87":"markdown","6575b06c":"markdown","36c3b032":"markdown","175cbacd":"markdown","0d2568d9":"markdown","aa7ce887":"markdown"},"source":{"b0580f5d":"import json\nimport os\nfrom PIL import Image\nfrom glob import glob\nfrom zipfile import ZipFile\nimport pandas as pd\nfrom keras.optimizers import Adam\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.callbacks import ModelCheckpoint, Callback, EarlyStopping\nfrom keras.models import Model, Input, load_model\nfrom keras.layers import Dense, GlobalAveragePooling2D\nfrom keras.applications.mobilenetv2 import MobileNetV2\nfrom keras import backend as K\nfrom matplotlib import pyplot as plt\nfrom matplotlib.image import imread\nimport cv2\nimport numpy as np","0e23256a":"def process_csv(dataframe: pd.DataFrame, image_column_name: str,\n                label_column_name: str,\n                folder_with_images: str) -> pd.DataFrame:\n    \"\"\"This function process Pandas DataFrame, which contains image filenames\n    and their corresponding labels.\n\n    Args:\n        dataframe: Pandas DataFrame object. It should consist of 2 columns\n        image_column_name: The name of the column containing the image\n            filenames\n        label_column_name: The name of the column containing the image\n            labels\n        folder_with_images: Folder with images\n\n    Returns:\n        dataframe: processed DataFrame with full paths to images\n    \"\"\"\n    dataframe[image_column_name] = dataframe[image_column_name].apply(\n        lambda x: f\"{folder_with_images}{x}.png\")\n    dataframe[label_column_name] = dataframe[label_column_name].astype('str')\n    return dataframe","729f0236":"train_datagen = ImageDataGenerator(rescale=1. \/ 255,\n                                   rotation_range=15,\n                                   width_shift_range=0.1,\n                                   height_shift_range=0.1,\n                                   shear_range=0.01,\n                                   zoom_range=[0.9, 1.25],\n                                   horizontal_flip=True,\n                                   vertical_flip=True,\n                                   fill_mode='reflect',\n                                   data_format='channels_last',\n                                   brightness_range=[0.5, 1.5],\n                                   validation_split=0.3)","ef70fc54":"train_csv = pd.read_csv(\"\/kaggle\/input\/aptos2019-blindness-detection\/train.csv\")\ntrain_csv = process_csv(\n    dataframe=train_csv,\n    image_column_name=\"id_code\",\n    label_column_name=\"diagnosis\",\n    folder_with_images=\"\/kaggle\/input\/aptos2019-blindness-detection\/train_images\/\")\n\ntrain_generator = train_datagen.flow_from_dataframe(\n    dataframe=train_csv, x_col=\"id_code\", y_col=\"diagnosis\", subset=\"training\",\n    batch_size=16, target_size=(224, 224))\nval_generator = train_datagen.flow_from_dataframe(\n    dataframe=train_csv, x_col=\"id_code\", y_col=\"diagnosis\",\n    subset=\"validation\", batch_size=16, target_size=(224, 224))","1afab7ec":"from keras.models import Model, Input\nfrom keras.layers import Dense, GlobalAveragePooling2D\nfrom keras.applications.mobilenetv2 import MobileNetV2\nfrom keras.applications.vgg16 import VGG16\nfrom keras.applications.inception_v3 import InceptionV3\nfrom keras.layers import Average, Input\n\n\ndef create_model():\n    input_tensor = Input((224, 224, 3))\n    outputs = []\n    \n    mobilenet_model = MobileNetV2(weights=None, input_shape=(224, 224, 3),include_top=False, alpha=1.4)                          \n    mobilenet_model.load_weights(\"\/kaggle\/input\/mobilenet-v2-keras-weights\/mobilenet_v2_weights_tf_dim_ordering_tf_kernels_1.4_224_no_top.h5\")\n    \n    VGG16_model = VGG16(weights=None, input_shape = (224,224,3), include_top=False)\n    VGG16_model.load_weights(\"\/kaggle\/input\/keras-pretrained-models\/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\")\n    \n    #InceptionV3_model = InceptionV3(weights=None, input_shape=(224,224,3),include_top = False)\n    #InceptionV3_model.load_weights(\"\/kaggle\/input\/keras-pretrained-models\/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5\")\n    \n    pretrained_models = [\n        mobilenet_model,VGG16_model\n    ]\n    for model in pretrained_models:\n        curr_output = model(input_tensor)\n        curr_output = GlobalAveragePooling2D()(curr_output)\n        curr_output = Dense(1024, activation=\"relu\")(curr_output)\n        outputs.append(curr_output)\n    output_tensor = Average()(outputs)\n    output_tensor = Dense(5, activation=\"softmax\")(output_tensor)\n\n    model = Model(input_tensor, output_tensor)\n    return model","4725a5a2":"class LRFinder(Callback):\n\n    def __init__(self, start_learning_rate, multiplier):\n        self.multiplier = multiplier\n        self.start_learning_rate = start_learning_rate\n        self.curr_train_step_number = 1\n        self.learning_rate = start_learning_rate\n        self.all_learning_rates = []\n        self.all_loss_values = []\n        super().__init__()\n\n    def on_batch_end(self, batch, logs=None):\n        self.learning_rate *= self.multiplier\n        K.set_value(self.model.optimizer.lr, self.learning_rate)\n        self.curr_train_step_number += 1\n        self.all_learning_rates.append(self.learning_rate)\n        self.all_loss_values.append(logs.get('loss'))","28809fe4":"start_lr = 1e-10\nend_lr = 1","afaa6a0d":"lrfinder_generator = ImageDataGenerator(rescale=1.\/255, validation_split=0).flow_from_dataframe(\n    dataframe=train_csv, x_col=\"id_code\", y_col=\"diagnosis\", subset=\"training\",\n    batch_size=8, target_size=(224, 224))\nlrfinder_callback = LRFinder(1e-10, multiplier=(end_lr \/ start_lr) ** (1 \/ len(lrfinder_generator))) \nlrfinder_model = create_model()\nlrfinder_model.compile(optimizer=Adam(start_lr),\n                       loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])","7ddc5294":"lrfinder_model.fit_generator(generator=lrfinder_generator,\n                             steps_per_epoch=len(lrfinder_generator),\n                             epochs=1,\n                             callbacks=[lrfinder_callback])","811ffd1c":"def exponentionaly_weighted_average(data: list, beta: float) -> list:\n    processed_data = [data[0]]\n    for value in data[1:]:\n        processed_data.append(beta * value + (1 - beta) * processed_data[-1])\n    return processed_data","b687d078":"beta_values = [0.9, 0.5, 0.3, 0.1, 0.05]\nlosses = lrfinder_callback.all_loss_values\nlearning_rates = lrfinder_callback.all_learning_rates","39dc55fb":"plt.figure(1, figsize=(10, 5))\nplt.xscale('log')\nplt.xticks([10**(-i) for i in range(11)])\nplt.plot(learning_rates, losses)\nplt.xlabel(\"Learning rate (log scale, without EWMA)\")\nplt.ylabel(\"Loss\")\nplt.show()\n\nfor idx, beta_value in enumerate(beta_values):\n    plt.figure(idx + 2, figsize=(10, 5))\n    plt.xscale('log')\n    plt.xticks([10**(-i) for i in range(11)])\n    plt.plot(learning_rates, exponentionaly_weighted_average(losses, beta_value))\n    plt.xlabel(f\"Learning rate (log scale, with EWMA, beta={beta_value})\")\n    plt.ylabel(\"Loss\")\n    plt.show()","5b51c990":"#callbacks = [\n#    ModelCheckpoint(\n#        \"best_weights.h5\",\n#        monitor='val_acc',\n#        verbose=1, save_best_only=True),\n#    EarlyStopping(monitor='val_acc', patience=5)\n#]\ncallbacks = [ModelCheckpoint(\"best_weights.h5\", monitor='acc', verbose=1, save_best_only=True, save_weights_only=False, mode='auto', period=1)]","f9ef801e":"model = create_model()\nmodel.compile(optimizer=Adam(1e-4),\n              loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\nmodel.fit_generator(generator=train_generator,\n                    steps_per_epoch=len(train_generator),\n                    validation_data=val_generator,\n                    validation_steps=len(val_generator),\n                    epochs=50,\n                    callbacks=callbacks)","e7894500":"model = load_model(\"best_weights.h5\")","70ee21b5":"test_csv = pd.read_csv(\"\/kaggle\/input\/aptos2019-blindness-detection\/test.csv\")\npredicted_csv = pd.DataFrame(columns=[\"id_code\", \"diagnosis\"])\n\nfor id_code in test_csv[\"id_code\"]:\n    filename = f\"\/kaggle\/input\/aptos2019-blindness-detection\/test_images\/{id_code}.png\"\n    img = imread(filename)\n    img = cv2.resize(img, dsize=(224, 224), interpolation=cv2.INTER_CUBIC)\n    img = np.expand_dims(img, 0)\n    prediction = int(np.argmax(model.predict(img)[0]))\n    predicted_csv = predicted_csv.append(\n        {'id_code':id_code ,\"diagnosis\": prediction}, ignore_index=True)\n\nwith open(\"submission.csv\", \"w\") as f:\n    f.write(predicted_csv.to_csv(index=False))","c1945648":"To determine the optimal loss function, we display the values on a graph using the log scale on the axis with a learning rate. Also for averaging the result we use exponentionaly weighted average ( EWAi=\u03b2\u2217\u03b8i+(1\u2212\u03b2)\u2217EWAi\u22121 )","d9c74e12":"Testing the neural network model","c5a34444":"Loading a dataset into the ImageDataGenerator","5f5a5d87":"Creating a class for LRFinder","6575b06c":"Based on the plots, we determine that the optimal value for learning speed is 10^-4. Now we can begin to train the model","36c3b032":"Neural network training using LRFinder to determine the optimal learning rate","175cbacd":"Creating training and validation generators","0d2568d9":"Work with the neural network model\n\nCreating a neural network mode","aa7ce887":"imports"}}