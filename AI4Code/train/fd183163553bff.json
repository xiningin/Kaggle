{"cell_type":{"31b032eb":"code","c58d879e":"code","aa915688":"code","7600037a":"code","23583d60":"code","95351666":"code","2e3f4f18":"code","1ed1a11b":"code","2926138a":"code","b04100ee":"code","0924086b":"code","e73f1f3a":"markdown","d014f1de":"markdown","1041b039":"markdown","2f77612c":"markdown"},"source":{"31b032eb":"import torch\nfrom torch import nn\nfrom torch.optim import Adam\nfrom torch.nn import functional as F\nfrom torch.utils.data import Dataset, DataLoader\nimport os \nimport cv2\nimport matplotlib.pyplot as plt\nfrom tqdm.notebook import tqdm\nimport random\nimport numpy as np\n\n%matplotlib inline","c58d879e":"def random_noise(batch_size, channels, side_size):\n    return torch.randn(batch_size, channels, side_size, side_size).cuda()\n\ndef imagewide_average(x):\n    return x.mean(dim=(-1, -2))","aa915688":"def visualise(imgs, rows=2):\n    imgs = (imgs.transpose(1, 3) + 1) \/ 2\n    imgs = torch.cat([imgs[i::rows] for i in range(rows)], dim=1)\n    cols = len(imgs)\n    imgs = (torch.cat(list(imgs), dim=1)).cpu().numpy()[:, :, ::-1]\n    plt.figure(figsize=(cols*1.5, rows*1.5))\n    plt.imshow(imgs)\n    plt.axis('off')\n    plt.tight_layout()\n    plt.show()","7600037a":"class CatDataset(Dataset):\n    def __init__(self, path_to_dataset=\"..\/input\/cat-136\/cat_136\", size=64):\n        self.photo_names = os.listdir(path_to_dataset)\n        self.path_base = path_to_dataset\n        self.size = size\n    \n    def __getitem__(self, index):\n        path = self.path_base + \"\/\" + self.photo_names[index]\n        img = cv2.imread(path) # 136 x 136\n        crop_rate = 8\n        x_crop = random.randint(0, crop_rate)\n        y_crop = random.randint(0, crop_rate)\n        img = img[x_crop:136 - crop_rate + x_crop, y_crop:136 - crop_rate + y_crop]\n        img = cv2.resize(img, (self.size, self.size), interpolation=cv2.INTER_CUBIC)\n        return 2 * torch.tensor(img).float().transpose(0, 2) \/ 255. - 1\n    \n    def __len__(self):\n        return len(self.photo_names)","23583d60":"dataset = CatDataset()\nvisualise(torch.cat([dataset[i].unsqueeze(0) for i in [3, 15, 182, 592, 394, 2941]], dim=0))","95351666":"class Generator(nn.Module):\n    def __init__(self, start_size=2, latent_channels=32, start_channels=1024, upsamplings=6):\n        super().__init__()\n        self.start_size = start_size\n        self.latent_channels = latent_channels\n        layers = [nn.Conv2d(latent_channels, start_channels, kernel_size=1, stride=1, padding=0)]\n        last_in = start_channels\n        for _ in range(upsamplings):\n            layers += [\n                nn.ConvTranspose2d(last_in, last_in \/\/ 2, kernel_size=4, stride=2, padding=1),\n                nn.BatchNorm2d(last_in \/\/ 2),\n                nn.ReLU()\n            ]\n            last_in = last_in \/\/ 2\n        layers += [\n            nn.Conv2d(last_in, 3, kernel_size=1, stride=1, padding=0),\n            nn.Tanh()\n        ]\n        self.model = nn.Sequential(*layers)\n    \n    def forward(self, batch_size: int):\n        x = random_noise(batch_size, self.latent_channels, self.start_size)\n        return self.model(x) # torch.Tensor batch_size x 3 x (start_size * 2**upsamplings) x (start_size * 2**upsamplings)","2e3f4f18":"class Discriminator(nn.Module):\n    def __init__(self, image_size, downsamplings=6, start_channels=8):\n        super().__init__()\n        layers = [nn.Conv2d(3, start_channels, kernel_size=1, stride=1, padding=0)]\n        last_in = start_channels\n        for _ in range(downsamplings):\n            layers += [\n                nn.Conv2d(last_in, last_in * 2, kernel_size=3, stride=2, padding=1),\n                nn.BatchNorm2d(last_in * 2),\n                nn.ReLU()\n            ]\n            last_in = last_in * 2\n        image_size = image_size \/\/ 2 ** downsamplings\n        layers += [\n            nn.Flatten(),\n            nn.Linear(image_size * image_size * last_in, 1),\n            nn.Sigmoid()\n        ]\n        self.model = nn.Sequential(*layers)\n        \n    def forward(self, x):\n        return self.model(x) # torch.Tensor batch_size x 1","1ed1a11b":"def train_gan():\n    generator = Generator()\n    discriminator = Discriminator(image_size=128)\n    epochs = 120\n    visualise_every = 10\n    batch_size = 8\n    generator.cuda()\n    discriminator.cuda()\n\n    gen_optim = Adam(generator.parameters(), lr=1e-4, betas=(0.5, 0.999))\n    disc_optim = Adam(discriminator.parameters(), lr=1e-4, betas=(0.5, 0.999))\n\n    dataset = CatDataset(size=128)\n\n    for ep in range(epochs):\n        dataloader = DataLoader(dataset, shuffle=True, batch_size=batch_size)\n        total_batches = 0\n        gen_loss_avg = 0\n        disc_loss_avg = 0\n\n        for i, batch in tqdm(enumerate(dataloader), total=(len(dataset) + batch_size) \/\/ batch_size):\n            if len(batch) < batch_size:\n                continue\n            total_batches += 1\n            # Positive update\n            batch = batch.cuda()\n            pred = discriminator(batch)\n            loss = F.binary_cross_entropy(pred, torch.ones_like(pred))\n            disc_optim.zero_grad()\n            loss.backward()\n            disc_optim.step()\n            disc_loss_avg += loss.item()\n\n            # Negative update\n            batch = generator(batch_size).detach()\n            pred = discriminator(batch)\n            loss = F.binary_cross_entropy(pred, torch.zeros_like(pred))\n            disc_optim.zero_grad()\n            loss.backward()\n            disc_optim.step()\n            disc_loss_avg += loss.item()\n\n            # Generator update\n            batch = generator(batch_size)\n            pred = discriminator(batch)\n            loss = F.binary_cross_entropy(pred, torch.ones_like(pred))\n            gen_optim.zero_grad()\n            loss.backward()\n            gen_optim.step()\n            gen_loss_avg += loss.item()\n            \n        if (ep + 1) % visualise_every == 0:\n            with torch.no_grad():\n                visualise(generator(6), rows=2)\n\n        print(f\"Epoch {ep+1} | Discriminator loss: {disc_loss_avg \/ total_batches} | Generator loss: {gen_loss_avg \/ total_batches}\")","2926138a":"train_gan()","b04100ee":"class VAE(nn.Module):\n    def __init__(self, img_size, downsamplings=7, latent_size=512, down_channels=8, up_channels=16):\n        super().__init__()\n        self.latent_size = latent_size\n        encoder_layers = [nn.Conv2d(3, down_channels, kernel_size=1, stride=1, padding=0)]\n        last_in = down_channels\n        for _ in range(downsamplings):\n            encoder_layers += [\n                nn.Conv2d(last_in, last_in * 2, kernel_size=3, stride=2, padding=1),\n                nn.BatchNorm2d(last_in * 2),\n                nn.ReLU()\n            ]\n            last_in = last_in * 2\n        encoder_layers += [nn.Conv2d(last_in, 2 * latent_size, kernel_size=1, stride=1, padding=0)]\n        self.encoder = nn.Sequential(*encoder_layers)\n\n        decoder_layers = [nn.Conv2d(latent_size, up_channels * 2 ** downsamplings, kernel_size=1, stride=1, padding=0)]\n        last_in = up_channels * 2 ** downsamplings\n        for _ in range(downsamplings):\n            decoder_layers += [\n                nn.ConvTranspose2d(last_in, last_in \/\/ 2, kernel_size=4, stride=2, padding=1),\n                nn.BatchNorm2d(last_in \/\/ 2),\n                nn.ReLU()\n            ]\n            last_in = last_in \/\/ 2\n        decoder_layers += [\n            nn.Conv2d(up_channels, 3, kernel_size=1, stride=1, padding=0),\n            nn.Tanh()\n        ]\n        self.decoder = nn.Sequential(*decoder_layers)\n        \n    def forward(self, x):\n        x = self.encoder(x)\n        mu = x[:, :self.latent_size, :, :]\n        sigma = torch.exp(x[:, self.latent_size: , :, :])\n        z = mu + random_noise(x.shape[0], self.latent_size, 1) * sigma\n        KL = 0.5 * (torch.sum(sigma*sigma, dim=1) + torch.sum(mu*mu, dim=1) - torch.sum((sigma*sigma).log(), dim=1) - sigma.shape[1])\n        return self.decode(z), KL\n    \n    def encode(self, x):\n        x = self.encoder(x)\n        mu = x[:, :self.latent_size, :, :]\n        sigma = torch.exp(x[:, self.latent_size: , :, :])\n        return mu + random_noise(x.shape[0], self.latent_size, 1) * sigma\n    \n    def decode(self, z):\n        return self.decoder(z)","0924086b":"def train_vae():\n    vae = VAE(img_size=128)\n    vae.cuda()\n\n    epochs = 201\n    batch_size = 8\n    vae_optim = Adam(vae.parameters(), lr=1e-4)\n\n    dataset = CatDataset(size=128)\n\n    test_imgs_1 = torch.cat([dataset[i].unsqueeze(0) for i in (0, 34, 76, 1509)])\n    test_imgs_2 = torch.cat([dataset[i].unsqueeze(0) for i in (734, 123, 512, 3634)])\n\n    for ep in range(epochs):\n        dataloader = DataLoader(dataset, shuffle=True, batch_size=batch_size)\n        total_batches = 0\n        rec_loss_avg = 0\n        kld_loss_avg = 0\n\n        if ep % 10 == 0:\n            with torch.no_grad():\n                z_1 = vae.encode(test_imgs_1.cuda())\n                z_2 = vae.encode(test_imgs_2.cuda())\n                x_int = []\n                for i in range(9):\n                    z = (i * z_1 + (8 - i) * z_2) \/ 8\n                    x_int.append(vae.decode(z))\n                x_int = torch.cat(x_int)\n                visualise(x_int, rows=len(test_imgs_1))\n                z_rand = torch.randn_like(z_1)\n                x_int = vae.decode(z_rand)\n                visualise(x_int, rows=len(test_imgs_1)\/\/2)\n\n        for i, batch in tqdm(enumerate(dataloader), total=(len(dataset) + batch_size) \/\/ batch_size):\n            if len(batch) < batch_size:\n                continue\n            total_batches += 1\n            x = batch.cuda()\n            x_rec, kld = vae(x)\n            img_elems = float(np.prod(list(batch.size())))\n            kld_loss = kld.sum() \/ batch_size\n            rec_loss = ((x_rec - x)**2).sum() \/ batch_size\n            loss = rec_loss + 0.1 * kld_loss # https:\/\/openreview.net\/forum?id=Sy2fzU9gl\n            vae_optim.zero_grad()\n            loss.backward()\n            vae_optim.step()\n            kld_loss_avg += kld_loss.item()\n            rec_loss_avg += rec_loss.item()\n\n        print(f\"Epoch {ep+1} | Reconstruction loss: {rec_loss_avg \/ total_batches} | KLD loss: {kld_loss_avg \/ total_batches}\")\n\ntrain_vae()","e73f1f3a":"### \u0417\u0430\u0434\u0430\u043d\u0438\u0435 1 (2 \u0431\u0430\u043b\u043b\u0430)\n\u0414\u043b\u044f \u043d\u0430\u0447\u0430\u043b\u0430 \u0440\u0435\u0430\u043b\u0438\u0437\u0443\u0435\u043c \u0433\u0435\u043d\u0435\u0440\u0430\u0442\u043e\u0440 \u0434\u043b\u044f \u043d\u0430\u0448\u0435\u0433\u043e DCGAN. \u041f\u0440\u0435\u0434\u043b\u0430\u0433\u0430\u0435\u0442\u0441\u044f \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u044c \u0441\u043b\u0435\u0434\u0443\u044e\u0449\u0443\u044e \u0430\u0440\u0445\u0438\u0442\u0435\u043a\u0442\u0443\u0440\u0443:\n\n![](imgs\/DCGAN.png)\n\n\u0414\u043b\u044f \u0435\u0435 \u0440\u0435\u0430\u043b\u0438\u0437\u0430\u0446\u0438\u0438 \u0432\u0430\u043c \u043f\u043e\u0442\u0440\u0435\u0431\u0443\u044e\u0442\u0441\u044f \u043c\u043e\u0434\u0443\u043b\u0438 `nn.BatchNorm2d`, `nn.Conv2d`, `nn.ConvTranspose2D`, `nn.ReLU`, \u0430 \u0442\u0430\u043a\u0436\u0435 \u0444\u0443\u043d\u043a\u0446\u0438\u044f `F.interpolate`.\n\n#### \u041c\u0435\u0442\u043e\u0434\u044b\n* `__init__` - \u043f\u0440\u0438\u043d\u0438\u043c\u0430\u0435\u0442 \u043d\u0430 \u0432\u0445\u043e\u0434 `start_size`, `latent_channels`, `start_channels` \u0438 `upsamplings`. \u041f\u0435\u0440\u0432\u044b\u0435 \u0434\u0432\u0430 \u0430\u0440\u0433\u0443\u043c\u0435\u043d\u0442\u0430 \u043e\u0442\u0432\u0435\u0447\u0430\u044e\u0442 \u0437\u0430 \u0440\u0430\u0437\u043c\u0435\u0440 \u0441\u043b\u0443\u0447\u0430\u0439\u043d\u043e\u0433\u043e \u0448\u0443\u043c\u0430, \u0438\u0437 \u043a\u043e\u0442\u043e\u0440\u043e\u0433\u043e \u0432 \u043f\u043e\u0441\u043b\u0435\u0434\u0441\u0442\u0432\u0438\u0438 \u0431\u0443\u0434\u0435\u0442 \u0441\u0433\u0435\u043d\u0435\u0440\u0438\u0440\u043e\u0432\u0430\u043d\u0430 \u043a\u0430\u0440\u0442\u0438\u043d\u043a\u0430. `start_channels` \u043e\u0442\u0432\u0435\u0447\u0430\u0435\u0442 \u0437\u0430 \u0442\u043e, \u0441\u043a\u043e\u043b\u044c\u043a\u043e \u043a\u0430\u043d\u0430\u043b\u043e\u0432 \u0434\u043e\u043b\u0436\u043d\u043e \u0431\u044b\u0442\u044c \u0432 \u043a\u0430\u0440\u0442\u0438\u043d\u043a\u0435 \u043f\u0435\u0440\u0435\u0434 \u0442\u0435\u043c, \u043a\u0430\u043a \u043a \u043d\u0435\u0439 \u0431\u0443\u0434\u0443\u0442 \u043f\u0440\u0438\u043c\u0435\u043d\u0435\u043d\u044b upsampling \u0431\u043b\u043e\u043a\u0438. `upsamplings` - \u044d\u0442\u043e \u043a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u043e upsampling \u0431\u043b\u043e\u043a\u043e\u0432, \u043a\u043e\u0442\u043e\u0440\u044b\u0435 \u0434\u043e\u043b\u0436\u043d\u044b \u0431\u044b\u0442\u044c \u043f\u0440\u0438\u043c\u0435\u043d\u0435\u043d\u044b \u043a \u043a\u0430\u0440\u0442\u0438\u043d\u043a\u0435. \u0412 \u043a\u0430\u0436\u0434\u043e\u043c \u0442\u0430\u043a\u043e\u043c \u043b\u043e\u043a\u0435 \u043a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u043e \u043a\u0430\u043d\u0430\u043b\u043e\u0432 \u0443\u043c\u0435\u043d\u044c\u0448\u0430\u0435\u0442\u0441\u044f \u0432 \u0434\u0432\u0430 \u0440\u0430\u0437\u0430.\n\n\n* `forward` - \u043f\u0440\u0438\u043d\u0438\u043c\u0430\u0435\u0442 \u043d\u0430 \u0432\u0445\u043e\u0434 `batch_size`, \u0433\u0435\u043d\u0435\u0440\u0438\u0440\u0443\u0435\u0442 `batch_size` \u043a\u0430\u0440\u0442\u0438\u043d\u043e\u043a \u0438\u0437 \u0441\u043b\u0443\u0447\u0430\u0439\u043d\u043e\u0433\u043e \u0448\u0443\u043c\u0430.","d014f1de":"### \u0417\u0430\u0434\u0430\u043d\u0438\u0435 2 (2 \u0431\u0430\u043b\u043b\u0430)\n\u0414\u043b\u044f \u043d\u0430\u0447\u0430\u043b\u0430 \u0440\u0435\u0430\u043b\u0438\u0437\u0443\u0435\u043c \u0434\u0438\u0441\u043a\u0440\u0438\u043c\u0438\u043d\u0430\u0442\u043e\u0440 \u0434\u043b\u044f \u043d\u0430\u0448\u0435\u0433\u043e DCGAN. \u041f\u0440\u0435\u0434\u043b\u0430\u0433\u0430\u0435\u0442\u0441\u044f \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u044c \u0441\u043b\u0435\u0434\u0443\u044e\u0449\u0443\u044e \u0430\u0440\u0445\u0438\u0442\u0435\u043a\u0442\u0443\u0440\u0443:\n\n![](imgs\/Disc_DCGAN.png)\n\n\u0414\u043b\u044f \u0435\u0435 \u0440\u0435\u0430\u043b\u0438\u0437\u0430\u0446\u0438\u0438 \u0432\u0430\u043c \u043f\u043e\u0442\u0440\u0435\u0431\u0443\u044e\u0442\u0441\u044f \u043c\u043e\u0434\u0443\u043b\u0438 `nn.BatchNorm2d`, `nn.Conv2d`, `nn.ReLU` \u0438 `nn.Sigmoid`.\n\n#### \u041c\u0435\u0442\u043e\u0434\u044b\n* `__init__` - \u043f\u0440\u0438\u043d\u0438\u043c\u0430\u0435\u0442 \u043d\u0430 \u0432\u0445\u043e\u0434 `start_channels` \u0438 `downsamplings`. `start_channels` \u043e\u043f\u0440\u0435\u0434\u0435\u043b\u044f\u0435\u0442 \u043a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u043e \u043a\u0430\u043d\u0430\u043b\u043e\u0432, \u043a\u043e\u0442\u043e\u0440\u044b\u0435 \u0434\u043e\u043b\u0436\u043d\u044b \u0431\u044b\u0442\u044c \u0432 \u0438\u0437\u043e\u0431\u0440\u0430\u0436\u0435\u043d\u0438\u0438 \u043f\u0435\u0440\u0435\u0434 \u043f\u0440\u0438\u043c\u0435\u043d\u0435\u043d\u0438\u0435\u043c downsampling \u0431\u043b\u043e\u043a\u043e\u0432.\n\n\n* `forward` - \u043f\u0440\u0438\u043d\u0438\u043c\u0430\u0435\u0442 \u043d\u0430 \u0432\u0445\u043e\u0434 `x` - \u0442\u0435\u043d\u0437\u043e\u0440 \u0441 \u043a\u0430\u0440\u0442\u0438\u043d\u043a\u0430\u043c\u0438. \u0412\u043e\u0437\u0432\u0440\u0430\u0449\u0430\u0435\u0442 \u0432\u0435\u043a\u0442\u043e\u0440 \u0441 \u0440\u0430\u0437\u043c\u0435\u0440\u043d\u043e\u0441\u0442\u044c\u044e `batch_size`.","1041b039":"### \u0417\u0430\u0434\u0430\u043d\u0438\u0435 3 (5 \u0431\u0430\u043b\u043b\u043e\u0432)\n\u0422\u0435\u043f\u0435\u0440\u044c \u043f\u043e\u0441\u043c\u043e\u0442\u0440\u0438\u043c \u043d\u0430 \u0434\u0440\u0443\u0433\u0443\u044e \u043c\u043e\u0434\u0435\u043b\u044c: Variational Autoencoder. \u0412 \u043e\u0442\u043b\u0438\u0447\u0438\u0438 \u043e\u0442 GAN, \u0432 \u043a\u043e\u0442\u043e\u0440\u043e\u043c \u0433\u0435\u043d\u0435\u0440\u0430\u0442\u043e\u0440 \u043f\u044b\u0442\u0430\u0435\u0442\u0441\u044f \u0441\u0435\u0431\u044f \u043e\u0431\u043c\u0430\u043d\u0443\u0442\u044c \u0434\u0438\u0441\u043a\u0440\u0438\u043c\u0438\u043d\u0430\u0442\u043e\u0440, \u0430 \u0434\u0438\u0441\u043a\u0440\u0438\u043c\u0438\u043d\u0430\u0442\u043e\u0440 \u0441\u0442\u0430\u0440\u0430\u0435\u0442\u0441\u044f \u043d\u0435 \u0431\u044b\u0442\u044c \u043e\u0431\u043c\u0430\u043d\u0443\u0442\u044b\u043c, VAE \u0440\u0435\u0448\u0430\u0435\u0442 \u0437\u0430\u0434\u0430\u0447\u0443 \u0440\u0435\u043a\u043e\u043d\u0441\u0442\u0440\u0443\u043a\u0446\u0438\u0438 \u044d\u043b\u0435\u043c\u0435\u043d\u0442\u0430 \u043c\u043d\u043e\u0436\u0435\u0441\u0442\u0432\u0430 X \u0441 \u043f\u0440\u0438\u043c\u0435\u043d\u0435\u043d\u0438\u0435\u043c \u0440\u0435\u0433\u0443\u043b\u044f\u0440\u0438\u0437\u0430\u0446\u0438\u0438 \u0432 \u043b\u0430\u0442\u0435\u043d\u0442\u043d\u043e\u043c \u043f\u0440\u043e\u0441\u0442\u0440\u0430\u043d\u0441\u0442\u0432\u0435. \n\n\u041f\u043e\u043b\u043d\u043e\u0441\u0442\u044c\u044e \u0430\u0440\u0445\u0438\u0442\u0435\u043a\u0442\u0443\u0440\u0430 \u0432\u044b\u0433\u043b\u044f\u0434\u0438\u0442 \u0442\u0430\u043a:\n![](imgs\/VAE.png)\n\n\u0418\u0437 \u043d\u0435\u0435 \u043c\u043e\u0436\u043d\u043e \u0432\u044b\u0434\u0435\u043b\u0438\u0442\u044c \u0434\u0432\u0435 \u0447\u0430\u0441\u0442\u0438: Encoder (\u043f\u043e \u0438\u0437\u043e\u0431\u0440\u0430\u0436\u0435\u043d\u0438\u044e \u0432\u043e\u0437\u0432\u0440\u0430\u0449\u0430\u0435\u0442 mu \u0438 sigma) \u0438 Decoder (\u043f\u043e \u0441\u043b\u0443\u0447\u0430\u0439\u043d\u043e\u043c\u0443 \u0448\u0443\u043c\u0443 \u0432\u043e\u0441\u0441\u0442\u0430\u043d\u0430\u0432\u043b\u0438\u0432\u0430\u0435\u0442 \u0438\u0437\u043e\u0431\u0440\u0430\u0436\u0435\u043d\u0438\u0435). \u041d\u0430 \u0432\u044b\u0441\u043e\u043a\u043e\u043c \u0443\u0440\u043e\u0432\u043d\u0435 VAE \u043c\u043e\u0436\u043d\u043e \u043f\u0440\u0435\u0434\u0441\u0442\u0430\u0432\u0438\u0442\u044c \u0442\u0430\u043a:\n\n![](imgs\/VAE_highlevel.png)\n\n\u0412 \u0434\u0430\u043d\u043d\u043e\u043c \u0437\u0430\u0434\u0430\u043d\u0438\u0438 \u0432\u0430\u043c \u043d\u0435\u043e\u0431\u0445\u043e\u0434\u0438\u043c\u043e \u0440\u0435\u0430\u043b\u0438\u0437\u043e\u0432\u0430\u0442\u044c \u043f\u043e\u043b\u043d\u0443\u044e \u0430\u0440\u0445\u0438\u0442\u0435\u043a\u0442\u0443\u0440\u0443 VAE.\n\n#### \u041c\u0435\u0442\u043e\u0434\u044b\n* `__init__` - \u043f\u0440\u0438\u043d\u0438\u043c\u0430\u0435\u0442 \u043d\u0430 \u0432\u0445\u043e\u0434 `img_size`, `downsamplings`, `latent_size`, `linear_hidden_size`, `down_channels` \u0438 `up_channels`. `img_size` - \u0440\u0430\u0437\u043c\u0435\u0440 \u0441\u0442\u043e\u0440\u043e\u043d\u044b \u0432\u0445\u043e\u0434\u043d\u043e\u0433\u043e \u0438\u0437\u043e\u0431\u0440\u0430\u0436\u0435\u043d\u0438\u044f. `downsamplings` - \u043a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u043e downsampling (\u0438 upsampling) \u0431\u043b\u043e\u043a\u043e\u0432. `latent_size` - \u0440\u0430\u0437\u043c\u0435\u0440 \u043b\u0430\u0442\u0435\u043d\u0442\u043d\u043e\u0433\u043e \u043f\u0440\u043e\u0441\u0442\u0440\u0430\u043d\u0441\u0442\u0432\u0430, \u0432 \u043a\u043e\u0442\u043e\u0440\u043e\u043c \u0432 \u043a\u043e\u0442\u043e\u0440\u044b\u0439 \u0431\u0443\u0434\u0435\u0442 \u0437\u0430\u043a\u043e\u0434\u0438\u0440\u043e\u0432\u0430\u043d\u0430 \u043a\u0430\u0440\u0442\u0438\u043d\u043a\u0430. `linear_hidden_size` \u043a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u043e \u043d\u0435\u0439\u0440\u043e\u043d\u043e\u0432 \u043d\u0430 \u0441\u043a\u0440\u044b\u0442\u043e\u043c \u0441\u043b\u043e\u0435 \u043f\u043e\u043b\u043d\u043e\u0441\u0432\u044f\u0437\u043d\u043e\u0439 \u0441\u0435\u0442\u0438 \u0432 \u043a\u043e\u043d\u0446\u0435 encoder'\u0430. \u0414\u043b\u044f \u043f\u043e\u043b\u043d\u043e\u0441\u0432\u044f\u0437\u043d\u043e\u0439 \u0441\u0435\u0442\u0438 decoder'\u0430 \u044d\u0442\u043e \u0447\u0438\u0441\u043b\u043e \u0441\u0442\u043e\u0438\u0442 \u0443\u043c\u043d\u043e\u0436\u0438\u0442\u044c \u043d\u0430 2. `down_channels` - \u043a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u043e \u043a\u0430\u043d\u0430\u043b\u043e\u0432, \u0432 \u043a\u043e\u0442\u043e\u0440\u043e\u0435 \u0431\u0443\u0434\u0435\u0442 \u043f\u0440\u0435\u043e\u0431\u0440\u0430\u0437\u043e\u0432\u0430\u043d\u043e \u0442\u0440\u0435\u0445\u0446\u0432\u0435\u0442\u043d\u043e\u0435 \u0438\u0437\u043e\u0431\u0440\u0430\u0436\u0435\u043d\u0438\u0435 \u043f\u0435\u0440\u0435\u0434 \u043f\u0440\u0438\u043c\u0435\u043d\u0435\u043d\u0438\u0435\u043c `downsampling` \u0431\u043b\u043e\u043a\u043e\u0432. `up_channels` - \u043a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u043e \u043a\u0430\u043d\u0430\u043b\u043e\u0432, \u043a\u043e\u0442\u043e\u0440\u043e\u0435 \u0434\u043e\u043b\u0436\u043d\u043e \u043f\u043e\u043b\u0443\u0447\u0438\u0442\u044c\u0441\u044f \u043f\u043e\u0441\u043b\u0435 \u043f\u0440\u0438\u043c\u0435\u043d\u0435\u043d\u0438\u044f \u0432\u0441\u0435\u0445 upsampling \u0431\u043b\u043e\u043a\u043e\u0432.\n\n* `forward` - \u043f\u0440\u0438\u043d\u0438\u043c\u0430\u0435\u0442 \u043d\u0430 \u0432\u0445\u043e\u0434 `x`. \u0421\u0447\u0438\u0442\u0430\u0435\u0442 \u0440\u0430\u0441\u043f\u0440\u0435\u0434\u0435\u043b\u0435\u043d\u0438\u0435 $N(\\mu, \\sigma^2)$ \u0438 \u0432\u0435\u043a\u0442\u043e\u0440 $z \\sim N(\\mu, \\sigma^2)$. \u0412\u043e\u0437\u0432\u0440\u0430\u0449\u0430\u0435\u0442 $x'$ - \u0432\u043e\u0441\u0441\u0442\u0430\u043d\u043e\u0432\u043b\u0435\u043d\u043d\u0443\u044e \u0438\u0437 \u0432\u0435\u043a\u0442\u043e\u0440\u0430 $z$ \u043a\u0430\u0440\u0442\u0438\u043d\u043a\u0443 \u0438 $D_{KL}(N(\\mu, \\sigma^2), N(0, 1)) = 0.5 \\cdot (\\sigma^2 + \\mu^2 - \\log \\sigma^2 - 1)$.\n\n* `encode` - \u043f\u0440\u0438\u043d\u0438\u043c\u0430\u0435\u0442 \u043d\u0430 \u0432\u0445\u043e\u0434 `x`. \u0412\u043e\u0437\u0432\u0440\u0430\u0449\u0430\u0435\u0442 \u0432\u0435\u043a\u0442\u043e\u0440 \u0438\u0437 \u0440\u0430\u0441\u043f\u0440\u0435\u0434\u0435\u043b\u0435\u043d\u0438\u044f $N(\\mu, \\sigma^2)$.\n\n* `decode` - \u043f\u0440\u0438\u043d\u0438\u043c\u0430\u0435\u0442 \u043d\u0430 \u0432\u0445\u043e\u0434 `z`. \u0412\u043e\u0437\u0432\u0440\u0430\u0449\u0430\u0435\u0442 \u0432\u043e\u0441\u0441\u0442\u0430\u043d\u043e\u0432\u043b\u0435\u043d\u043d\u0443\u044e \u043f\u043e \u0432\u0435\u043a\u0442\u043e\u0440\u0443 \u043a\u0430\u0440\u0442\u0438\u043d\u043a\u0443.\n\n\n#### \u0415\u0441\u043b\u0438 \u0445\u043e\u0447\u0435\u0442\u0441\u044f \u0443\u043b\u0443\u0447\u0448\u0438\u0442\u044c \u043a\u0430\u0447\u0435\u0441\u0442\u0432\u043e\nhttps:\/\/arxiv.org\/pdf\/1906.00446.pdf","2f77612c":"# This cat does not exist\n__\u0421\u0443\u043c\u043c\u0430\u0440\u043d\u043e\u0435 \u043a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u043e \u0431\u0430\u043b\u043b\u043e\u0432: 10__\n\n__\u0420\u0435\u0448\u0435\u043d\u0438\u0435 \u043e\u0442\u043f\u0440\u0430\u0432\u043b\u044f\u0442\u044c \u043d\u0430 `ml.course.practice@gmail.com`__\n\n__\u0422\u0435\u043c\u0430 \u043f\u0438\u0441\u044c\u043c\u0430: `[HSE][ML][HW06] <\u0424\u0418\u041e>`, \u0433\u0434\u0435 \u0432\u043c\u0435\u0441\u0442\u043e `<\u0424\u0418\u041e>` \u0443\u043a\u0430\u0437\u0430\u043d\u044b \u0444\u0430\u043c\u0438\u043b\u0438\u044f \u0438 \u0438\u043c\u044f__\n\n\u0426\u0435\u043b\u044c \u044d\u0442\u043e\u0433\u043e \u0437\u0430\u0434\u0430\u043d\u0438\u044f - \u0441\u043e\u0437\u0434\u0430\u0442\u044c \u043a\u043e\u0442\u043e\u0432, \u043a\u043e\u0442\u043e\u0440\u044b\u0445 \u043d\u0435 \u0441\u0443\u0449\u0435\u0441\u0442\u0432\u0443\u0435\u0442. \u0412 \u0445\u043e\u0434\u0435 \u0434\u0430\u043d\u043d\u043e\u0433\u043e \u0437\u0430\u0434\u0430\u043d\u0438\u044f \u0432\u044b \u043e\u0431\u0443\u0447\u0438\u0442\u0435 DCGAN \u0438 VAE, \u043a\u043e\u0442\u043e\u0440\u044b\u0435 \u044f\u0432\u043b\u044f\u044e\u0442\u0441\u044f \u043e\u0434\u043d\u0438\u043c\u0438 \u0438\u0437 \u043f\u0435\u0440\u0432\u044b\u0445 \u0433\u0435\u043d\u0435\u0440\u0430\u0442\u0438\u0432\u043d\u044b\u0445 \u043c\u043e\u0434\u0435\u043b\u0435\u0439. \u0414\u043b\u044f \u044d\u0442\u043e\u0433\u043e \u0437\u0430\u0434\u0430\u043d\u0438\u044f \u0432\u0430\u043c \u043d\u0430\u0432\u0435\u0440\u043d\u044f\u043a\u0430 \u043f\u043e\u0442\u0440\u0435\u0431\u0443\u0435\u0442\u0441\u044f GPU \u0441 CUDA, \u043f\u043e\u044d\u0442\u043e\u043c\u0443 \u0440\u0435\u043a\u043e\u043c\u0435\u043d\u0434\u0443\u0435\u0442\u0441\u044f \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u044c Google Colab."}}