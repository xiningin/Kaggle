{"cell_type":{"e1c0bf13":"code","48adf6fc":"code","29213d7b":"code","c95f1192":"code","03dd3271":"code","c0257921":"code","25c82d6a":"code","1f216089":"code","62de00bf":"code","50b7d8da":"code","94743673":"code","0d97f3d3":"code","0de2311f":"markdown","79a1d00e":"markdown","f32d46ad":"markdown","8f635c2e":"markdown","2e719a2f":"markdown"},"source":{"e1c0bf13":"# Imports\n\nimport os\nimport numpy as np\nimport pandas as pd\nimport random\n\nfrom transformers import AutoConfig, AutoModel, AutoTokenizer, AdamW, get_linear_schedule_with_warmup, logging\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, TensorDataset, SequentialSampler, RandomSampler, DataLoader\n\nfrom tqdm.notebook import tqdm\n\nimport gc; gc.enable()\nfrom IPython.display import clear_output\n\nfrom sklearn.model_selection import StratifiedKFold\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nsns.set_style('whitegrid')\nlogging.set_verbosity_error()","48adf6fc":"# Definitions\n\nINPUT_DIR = '..\/input\/commonlitreadabilityprize'\nMODEL_DIR = '..\/input\/roberta-transformers-pytorch\/roberta-large'\n\nHIDDEN_SIZE = 1024\nNUM_HIDDEN_LAYERS = 24\nLAYER_START = 4   # for WeightedLayerPoolingModel\n\nHIDDEN_DIM_FC = 128    # for AttentionPooling\n\nMAX_LENGTH = 300\nLR = 2e-5\nEPS = 1e-8\n\nSEED = 42\n\nNUM_FOLDS = 5\n# SEEDS = [113, 71, 17, 43, 37]\n\nSEEDS = [113, 71, 17, 43]\n\nEPOCHS = 5\nTRAIN_BATCH_SIZE = 8\nVAL_BATCH_SIZE = 32\n\nDEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')","29213d7b":"# Utilities\n\n# Utility classes and functions\n\nclass ContinuousStratifiedKFold(StratifiedKFold):\n    def split(selfself, x, y, groups=None):\n        num_bins = int(np.floor(1 + np.log2(len(y))))\n        bins = pd.cut(y, bins=num_bins, labels=False)\n        return super().split(x, bins, groups)\n    \ndef seed_everything(seed):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = True\n    \ndef get_data_loaders(data, fold):\n    \n    x_train = data.loc[data.fold != fold, 'excerpt'].tolist()\n    y_train = data.loc[data.fold != fold, 'target'].values\n    x_val = data.loc[data.fold == fold, 'excerpt'].tolist()\n    y_val = data.loc[data.fold == fold, 'target'].values\n    \n    tokenizer = AutoTokenizer.from_pretrained(MODEL_DIR)\n    \n    encoded_train = tokenizer.batch_encode_plus(\n        x_train, \n        add_special_tokens=True, \n        return_attention_mask=True, \n        padding='max_length', \n        truncation=True,\n        max_length=MAX_LENGTH, \n        return_tensors='pt'\n    )\n    \n    encoded_val = tokenizer.batch_encode_plus(\n        x_val, \n        add_special_tokens=True, \n        return_attention_mask=True, \n        padding='max_length', \n        truncation=True,\n        max_length=MAX_LENGTH, \n        return_tensors='pt'\n    )\n    \n    dataset_train = TensorDataset(\n        encoded_train['input_ids'],\n        encoded_train['attention_mask'],\n        torch.tensor(y_train)\n    )\n    \n    dataset_val = TensorDataset(\n        encoded_val['input_ids'],\n        encoded_val['attention_mask'],\n        torch.tensor(y_val)\n    )\n    \n    dataloader_train = DataLoader(\n        dataset_train,\n        sampler = RandomSampler(dataset_train),\n        batch_size=TRAIN_BATCH_SIZE\n    )\n\n    dataloader_val = DataLoader(\n        dataset_val,\n        sampler = SequentialSampler(dataset_val),\n        batch_size=VAL_BATCH_SIZE\n    )\n\n    return dataloader_train, dataloader_val","c95f1192":"# Model definitions\n\nclass PoolerModel(nn.Module):\n    \n    def __init__(self, model_name):\n        super().__init__()\n        \n        config = AutoConfig.from_pretrained(model_name)\n        config.update({'num_labels':1})\n        self.model = AutoModel.from_pretrained(model_name, config)\n        self.linear = nn.Linear(HIDDEN_SIZE, 1)\n        self.loss = nn.MSELoss()\n        \n    def forward(self, input_ids, attention_mask, labels=None):\n        \n        outputs = self.model(input_ids, attention_mask)\n        pooler_output = outputs[1]\n        logits = self.linear(pooler_output)\n        \n        preds = logits.squeeze(-1).squeeze(-1)\n        \n        if labels is not None:\n            loss = self.loss(preds.view(-1).float(), labels.view(-1).float())\n            return loss\n        else:\n            return preds\n        \n        \nclass LastLayerCLSModel(nn.Module):\n    \n    def __init__(self, model_name):\n        super().__init__()\n        \n        config = AutoConfig.from_pretrained(model_name)\n        self.model = AutoModel.from_pretrained(model_name, config=config)\n        self.linear = nn.Linear(HIDDEN_SIZE, 1)\n        self.loss = nn.MSELoss()\n        \n    def forward(self, input_ids, attention_mask, labels=None):\n        \n        outputs = self.model(input_ids, attention_mask)\n        last_hidden_state = outputs[0]\n        cls_embeddings = last_hidden_state[:,0]\n        logits = self.linear(cls_embeddings)\n        \n        preds = logits.squeeze(-1).squeeze(-1)\n        \n        if labels is not None:\n            loss = self.loss(preds.view(-1).float(), labels.view(-1).float())\n            return loss\n        else:\n            return preds\n        \n        \nclass MeanPoolingModel(nn.Module):\n    \n    def __init__(self, model_name):\n        super().__init__()\n        \n        config = AutoConfig.from_pretrained(model_name)\n        self.model = AutoModel.from_pretrained(model_name, config=config)\n        self.linear = nn.Linear(HIDDEN_SIZE, 1)\n        self.loss = nn.MSELoss()\n        \n    def forward(self, input_ids, attention_mask, labels=None):\n        \n        outputs = self.model(input_ids, attention_mask)\n        last_hidden_state = outputs[0]\n        input_mask_expanded = attention_mask.unsqueeze(-1).expand(last_hidden_state.size()).float()\n        sum_embeddings = torch.sum(last_hidden_state * input_mask_expanded, 1)\n        sum_mask = input_mask_expanded.sum(1)\n        sum_mask = torch.clamp(sum_mask, min=1e-9)\n        mean_embeddings = sum_embeddings \/ sum_mask\n        logits = self.linear(mean_embeddings)\n        \n        preds = logits.squeeze(-1).squeeze(-1)\n        \n        if labels is not None:\n            loss = self.loss(preds.view(-1).float(), labels.view(-1).float())\n            return loss\n        else:\n            return preds\n        \n        \nclass MaxPoolingModel(nn.Module):\n    \n    def __init__(self, model_name):\n        super().__init__()\n        \n        config = AutoConfig.from_pretrained(model_name)\n        self.model = AutoModel.from_pretrained(model_name, config=config)\n        self.linear = nn.Linear(HIDDEN_SIZE, 1)\n        self.loss = nn.MSELoss()\n        \n    def forward(self, input_ids, attention_mask, labels=None):\n        \n        outputs = self.model(input_ids, attention_mask)\n        last_hidden_state = outputs[0]\n        input_mask_expanded = attention_mask.unsqueeze(-1).expand(last_hidden_state.size()).float()\n        last_hidden_state[input_mask_expanded == 0] = -1e9   # large negative value\n        max_embeddings, _ = torch.max(last_hidden_state, 1)\n        logits = self.linear(max_embeddings)\n        \n        preds = logits.squeeze(-1).squeeze(-1)\n        \n        if labels is not None:\n            loss = self.loss(preds.view(-1).float(), labels.view(-1).float())\n            return loss\n        else:\n            return preds\n        \n        \nclass SecondToLastLayerCLSModel(nn.Module):\n    \n    def __init__(self, model_name):\n        super().__init__()\n        \n        config = AutoConfig.from_pretrained(model_name)\n        config.update({'output_hidden_states':True})\n        self.model = AutoModel.from_pretrained(model_name, config=config)\n        self.linear = nn.Linear(HIDDEN_SIZE, 1)\n        self.loss = nn.MSELoss()\n        \n    def forward(self, input_ids, attention_mask, labels=None):\n        \n        outputs = self.model(input_ids, attention_mask)\n        all_hidden_states = torch.stack(outputs[2])\n        second_to_last_layer = 23\n        cls_embeddings = all_hidden_states[second_to_last_layer,:,0]\n        logits = self.linear(cls_embeddings)\n        \n        preds = logits.squeeze(-1).squeeze(-1)\n        \n        if labels is not None:\n            loss = self.loss(preds.view(-1).float(), labels.view(-1).float())\n            return loss\n        else:\n            return preds\n        \nclass ConcatenateLastFourModel(nn.Module):\n    \n    def __init__(self, model_name):\n        super().__init__()\n        \n        config = AutoConfig.from_pretrained(model_name)\n        config.update({'output_hidden_states':True})\n        self.model = AutoModel.from_pretrained(model_name, config=config)\n        self.linear = nn.Linear(4*HIDDEN_SIZE, 1)\n        self.loss = nn.MSELoss()\n        \n    def forward(self, input_ids, attention_mask, labels=None):\n        \n        outputs = self.model(input_ids, attention_mask)\n        all_hidden_states = torch.stack(outputs[2])\n        concatenate_pooling = torch.cat(\n            (all_hidden_states[-1], all_hidden_states[-2], all_hidden_states[-3], all_hidden_states[-4]), -1\n        )\n        concatenate_pooling = concatenate_pooling[:,0]\n        logits = self.linear(concatenate_pooling)\n        \n        preds = logits.squeeze(-1).squeeze(-1)\n        \n        if labels is not None:\n            loss = self.loss(preds.view(-1).float(), labels.view(-1).float())\n            return loss\n        else:\n            return preds\n        \n        \nclass WeightedLayerPooling(nn.Module):\n    def __init__(self, num_hidden_layers, layer_start: int = 4, layer_weights = None):\n        super(WeightedLayerPooling, self).__init__()\n        self.layer_start = layer_start\n        self.num_hidden_layers = num_hidden_layers\n        self.layer_weights = layer_weights if layer_weights is not None \\\n            else nn.Parameter(\n                torch.tensor([1] * (num_hidden_layers+1 - layer_start), dtype=torch.float)\n            )\n\n    def forward(self, all_hidden_states):\n        all_layer_embedding = all_hidden_states[self.layer_start:, :, :, :]\n        weight_factor = self.layer_weights.unsqueeze(-1).unsqueeze(-1).unsqueeze(-1).expand(all_layer_embedding.size())\n        weighted_average = (weight_factor*all_layer_embedding).sum(dim=0) \/ self.layer_weights.sum()\n        return weighted_average\n    \nclass WeightedLayerPoolingModel(nn.Module):\n    def __init__(self, model_name):\n        super().__init__()\n        \n        config = AutoConfig.from_pretrained(model_name)\n        config.update({'output_hidden_states':True})\n        self.model = AutoModel.from_pretrained(model_name, config=config)\n        self.pooling = WeightedLayerPooling(NUM_HIDDEN_LAYERS, \n                                      layer_start=LAYER_START,\n                                      layer_weights=None)\n        self.layer_norm = nn.LayerNorm(HIDDEN_SIZE)\n        self.linear = nn.Linear(HIDDEN_SIZE, 1)\n        self.loss = nn.MSELoss()\n        \n    def forward(self, input_ids, attention_mask, labels=None):\n        \n        outputs = self.model(input_ids, attention_mask)\n        all_hidden_states = torch.stack(outputs[2])\n        \n        weighted_pooling_embeddings = self.pooling(all_hidden_states)\n        weighted_pooling_embeddings = weighted_pooling_embeddings[:,0]\n        \n        norm_embeddings = self.layer_norm(weighted_pooling_embeddings)\n        logits = self.linear(norm_embeddings)\n        \n        preds = logits.squeeze(-1).squeeze(-1)\n        \n        if labels is not None:\n            loss = self.loss(preds.view(-1).float(), labels.view(-1).float())\n            return loss\n        else:\n            return preds\n        \nclass AttentionPooling(nn.Module):\n    def __init__(self, num_layers, hidden_size, hiddendim_fc):\n        super(AttentionPooling, self).__init__()\n        self.num_hidden_layers = num_layers\n        self.hidden_size = hidden_size\n        self.hiddendim_fc = hiddendim_fc\n        self.dropout = nn.Dropout(0.1)\n\n        q_t = np.random.normal(loc=0.0, scale=0.1, size=(1, self.hidden_size))\n        self.q = nn.Parameter(torch.from_numpy(q_t)).float().to(DEVICE)\n        w_ht = np.random.normal(loc=0.0, scale=0.1, size=(self.hidden_size, self.hiddendim_fc))\n        self.w_h = nn.Parameter(torch.from_numpy(w_ht)).float().to(DEVICE)\n\n    def forward(self, all_hidden_states):\n        hidden_states = torch.stack([all_hidden_states[layer_i][:, 0].squeeze()\n                                     for layer_i in range(1, self.num_hidden_layers+1)], dim=-1)\n        hidden_states = hidden_states.view(-1, self.num_hidden_layers, self.hidden_size)\n        out = self.attention(hidden_states)\n        out = self.dropout(out)\n        return out\n\n    def attention(self, h):\n        v = torch.matmul(self.q, h.transpose(-2, -1)).squeeze(1)\n        v = F.softmax(v, -1)\n        v_temp = torch.matmul(v.unsqueeze(1), h).transpose(-2, -1)\n        v = torch.matmul(self.w_h.transpose(1, 0), v_temp).squeeze(2)\n        return v\n\nclass AttentionPoolingModel(nn.Module):\n    def __init__(self, model_name):\n        super().__init__()\n        \n        config = AutoConfig.from_pretrained(model_name)\n        config.update({'output_hidden_states':True})\n        self.model = AutoModel.from_pretrained(model_name, config=config)\n        self.pooler = AttentionPooling(NUM_HIDDEN_LAYERS, HIDDEN_SIZE, HIDDEN_DIM_FC)\n        \n        self.linear = nn.Linear(HIDDEN_DIM_FC, 1)\n        self.loss = nn.MSELoss()\n        \n    def forward(self, input_ids, attention_mask, labels=None):\n        \n        outputs = self.model(input_ids, attention_mask)\n        all_hidden_states = torch.stack(outputs[2])\n        \n        attention_pooling_embeddings = self.pooler(all_hidden_states)\n        \n        logits = self.linear(attention_pooling_embeddings)\n        \n        preds = logits.squeeze(-1).squeeze(-1)\n        \n        if labels is not None:\n            loss = self.loss(preds.view(-1).float(), labels.view(-1).float())\n            return loss\n        else:\n            return preds\n        ","03dd3271":"# Read competition data\ndata = pd.read_csv(os.path.join(INPUT_DIR, 'train.csv'))\n\n# Create stratified folds\nkf = ContinuousStratifiedKFold(n_splits=5, shuffle=True, random_state=SEED)\n\nfor f, (t_, v_) in enumerate(kf.split(data, data.target)):\n    data.loc[v_, 'fold'] = f\n    \ndata['fold'] = data['fold'].astype(int)","c0257921":"def evaluate(model, val_dataloader):\n\n    model.eval()\n    \n    loss_val_total = 0\n    \n    for batch in val_dataloader:\n        \n        batch = tuple(b.to(DEVICE) for b in batch)\n        \n        inputs = {'input_ids':      batch[0],\n                  'attention_mask': batch[1],\n                  'labels':         batch[2],\n                 }\n\n        with torch.no_grad():        \n            loss = model(**inputs)\n            \n        loss_val_total += loss.item()\n\n    loss_val_avg = loss_val_total\/len(val_dataloader) \n            \n    return loss_val_avg\n\n\ndef train(model, train_dataloader, val_dataloader):\n    \n    optimizer = AdamW(model.parameters(), lr = LR, eps = EPS)\n\n    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, \n                                                num_training_steps=len(train_dataloader) * EPOCHS)\n    best_val_loss = 1\n    best_model = None\n    \n    model.train()                               \n    for epoch in range(EPOCHS):\n    \n        loss_train_total = 0\n        for batch in tqdm(train_dataloader):\n    \n            model.zero_grad()\n            batch = tuple(b.to(DEVICE) for b in batch)\n            inputs = {\n                'input_ids': batch[0],\n                'attention_mask': batch[1],\n                'labels': batch[2]\n            }\n        \n            loss = model(**inputs)\n            loss_train_total += loss.item()\n        \n            loss.backward()\n            optimizer.step()\n            scheduler.step()\n        \n        loss_train_avg = loss_train_total \/ len(train_dataloader)\n        loss_val_avg = evaluate(model, val_dataloader)\n        print(f'epoch:{epoch+1}\/{EPOCHS} train loss={loss_train_avg}  val loss={loss_val_avg}')\n   \n        if loss_val_avg < best_val_loss:\n            best_val_loss = loss_val_avg\n            best_model = model\n                       \n    return best_val_loss, best_model ","25c82d6a":"TRAINING=False\n\nif TRAINING: \n\n    for i, seed in enumerate(SEEDS):\n\n        print(f'********* seed({i}) = {seed} ***********')\n\n        for fold in range(NUM_FOLDS):\n            print(f'*** fold = {fold} ***')\n            seed_everything(seed)\n            train_dataloader, val_dataloader = get_data_loaders(data, fold)\n\n            # Change the instantiated model by LastLayerCLSModel, PoolingModel, etc. \n            model = MeanPoolingModel(MODEL_DIR)\n            model.to(DEVICE)\n\n            loss, best_model = train(model, train_dataloader, val_dataloader)\n\n            model_path = f\"model_{seed + 1}_{fold + 1}.pth\"\n            torch.save(best_model.state_dict(), model_path)\n\n            del model, best_model        \n            gc.collect()","1f216089":"test = pd.read_csv(os.path.join(INPUT_DIR, 'test.csv'))\ntest.head(2)","62de00bf":"TEST_BATCH_SIZE = 1\n\ndef get_test_loader(data):\n\n    x_test = data.excerpt.tolist()\n    \n    tokenizer = AutoTokenizer.from_pretrained(MODEL_DIR)\n\n    encoded_test = tokenizer.batch_encode_plus(\n        x_test, \n        add_special_tokens=True, \n        return_attention_mask=True, \n        padding='max_length', \n        truncation=True,\n        max_length=MAX_LENGTH, \n        return_tensors='pt'\n    )\n\n    dataset_test = TensorDataset(\n        encoded_test['input_ids'],\n        encoded_test['attention_mask']\n    )\n\n    dataloader_test = DataLoader(\n        dataset_test,\n        sampler = SequentialSampler(dataset_test),\n        batch_size=TEST_BATCH_SIZE\n    )\n    \n    return dataloader_test\n\ntest_dataloader = get_test_loader(test)","50b7d8da":"CHECKPOINT_DIR1 = '..\/input\/clrp-mean-pooling\/'\nCHECKPOINT_DIR2 = '..\/input\/clrp-mean-pooling-seeds-17-43\/'\n\nall_predictions = [] \nfor seed in SEEDS:\n    \n    fold_predictions = []\n    \n    for fold in tqdm(range(NUM_FOLDS)):\n        \n        model_path = f\"model_{seed + 1}_{fold + 1}.pth\" \n        print(f\"\\nUsing {model_path}\")\n        \n        if seed in [113, 71]:\n            model_path = CHECKPOINT_DIR1 + f\"model_{seed + 1}_{fold + 1}.pth\"\n            \n        if seed in [17, 43]:\n            model_path = CHECKPOINT_DIR2 + f\"model_{seed + 1}_{fold + 1}.pth\"            \n            \n        model = MeanPoolingModel(MODEL_DIR)\n        model.load_state_dict(torch.load(model_path)) \n        model.to(DEVICE)\n        model.eval()\n\n        predictions = []\n        for batch in test_dataloader:\n\n            batch = tuple(b.to(DEVICE) for b in batch)\n\n            inputs = {'input_ids':      batch[0],\n                      'attention_mask': batch[1],\n                      'labels':         None,\n                     }\n\n     \n            preds = model(**inputs).item()\n            predictions.append(preds)\n            \n        del model \n        gc.collect()\n            \n        fold_predictions.append(predictions)\n    all_predictions.append(np.mean(fold_predictions, axis=0).tolist())\n    \nmodel_predictions = np.mean(all_predictions,axis=0)","94743673":"submit = pd.read_csv(os.path.join(INPUT_DIR, 'sample_submission.csv'))\nsubmit.target = model_predictions\nsubmit","0d97f3d3":"submit.to_csv('submission.csv',index=False)","0de2311f":"<h2>Read data and create folds<\/h2>\n\nFor the results to be reproducible, I create the same folds I'm using in the competition.","79a1d00e":"<h2>Training and evaluation<\/h2>","f32d46ad":"<h1>Which representations are best?<\/h1>\n\nIn the first place, thanks to [torch](https:\/\/www.kaggle.com\/rhtsingh) and his awesome notebook [Utilizing Transformer Representations Efficiently](https:\/\/www.kaggle.com\/rhtsingh\/utilizing-transformer-representations-efficiently), on which this notebook is based.\n\nI'm new to transformers, and I was trying to understand how to use them properly. In his notebook, torch covers most of the common ways of using transformers' embeddings for fine-tuning a downstream task. It helped me a lot to understand transformers.\n\nBut a question remained after digesting torch's notebook: which representation(s) is(are) the best for a given task, in particular for the competition task of assessing readability of a text? So I decided to run some experiments to help me find out, if not the definitive answer, at least a way of discarding some of the representations and concentrating on a few of them for further investigation.\n\nThis notebook is the result of these experiments. In them, I've tested some of the model on the competition's task. All the experiments have been conducted under the same conditions:\n<ul>\n    <li>RobertaLarge has been used as the base model for all architectures.<\/li>\n    <li>The bare models has been used, as proposed by torch, without further architecture modifications like layer normalization, dropout, etc.<\/li>\n    <li>Linear schedule without warmup has been used for all the runs.<\/li>\n    <li>Each experiment has been run for 5 different seeds, 5 folds. The best loss of 5 epochs is taken (25 values for each model).<\/li>\n    <li>A constant learning rate of 2e-5 (no layer-wise learning rate decay), eps of 1e-8 and max_size of 300 are used .<\/li>\n<\/ul>\n\nFor each model, a 95% confident interval of the mean of the RMSE has been plotted. The results, which (hopefully) should be reproducible using this notebook (change the instantiated model in the training cell), are summarized in this image:\n\n<a href='https:\/\/i.postimg.cc\/4N2mFXq8\/transformer-representations.png'>Results<\/a>\n\n![image.png](attachment:d646bf40-7010-444b-9667-006240506e0b.png)\n\nIf you find any bugs in this notebook, please let me know. ","8f635c2e":"<h2>PREDICTIONS<\/h2>","2e719a2f":"<h2>Model definitions<\/h2>"}}