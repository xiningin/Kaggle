{"cell_type":{"b91940a8":"code","ef072d50":"code","407cc731":"code","ced53913":"code","eaddb71e":"code","6e4daf3b":"code","a3350667":"code","a68094dc":"code","2135fff5":"code","e62c13d6":"code","f151259d":"code","348ce60e":"code","df8dff50":"code","deedbee2":"markdown","54e94764":"markdown","73e2001d":"markdown","f2369a2f":"markdown","d658c471":"markdown","6f26bef3":"markdown","8e88fb22":"markdown","49dd0335":"markdown","29abb43c":"markdown","60a6659d":"markdown","df2bd912":"markdown","21f4f755":"markdown","f775cf6e":"markdown","238cc7a7":"markdown","0361065f":"markdown"},"source":{"b91940a8":"import re\nimport pickle\nimport os\nimport logging\nimport json\nimport pickle\nfrom gensim import corpora\nimport numpy as np\n\nlogging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\nDATA_DIR = '..\/input\/vntcdata'\nDATA_DIR_TRAIN = os.path.join(DATA_DIR, 'train_dict.json')\nDATA_DIR_TEST = os.path.join(DATA_DIR, 'test_dict.json')\n\ndef read_txt(file_path):\n    with open(file_path, 'r', encoding = 'utf-16-le') as f:\n        s = f.read().lower()\n        s = re.sub('\\ufeff+|\\n+', ' ', s)\n    return s\n\ndef read_json(file_path):\n    with open(file_path, 'r', encoding = 'utf-16-le') as f:\n        data = json.load(f)\n    return data\n\ndef save_json(obj, file_name):\n    with open(file_name, 'w', encoding = 'utf-16-le') as f:\n        json.dump(obj, f)\n        \ndef read_stopwords(file_path):\n    with open(file_path, 'r', encoding='utf8') as f:\n        stopwords = set([w.strip().replace(' ', '_') for w in f.readlines()])\n    return stopwords\n\ndef save_pickle(obj, file_path):\n    with open(file_path, 'wb') as outfile:\n        pickle.dump(obj, outfile, pickle.HIGHEST_PROTOCOL)\n        \ndef load_pickle(file_path):\n    with open(file_path, 'rb') as outfile:\n        obj = pickle.load(outfile)\n    return obj\n\nTOPIC_LIST = [list(read_json('..\/input\/10topicvntc\/topic.json'))]\ntopic_dict = corpora.Dictionary(TOPIC_LIST)\nTOPIC = topic_dict.token2id\nTOPIC","ef072d50":"def read_data(folder_path):\n    topics = os.listdir(folder_path)\n    data = {'file_names':[], 'topic_ids':[],'contents': []}\n    for topic in topics:\n        folder_path_topic = os.path.join(folder_path, topic)\n        file_names = os.listdir(folder_path_topic)\n        data['file_names'] += file_names\n        data['topic_ids'] += [TOPIC[topic]]*len(file_names)\n        contents = [read_txt(os.path.join(folder_path_topic, file_name)) for file_name in file_names]\n        data['contents'] += contents\n    return data\n\n# train_dict = read_data(DATA_DIR_TRAIN)\n# save_json(train_dict, 'data\/train.json')\ntrain_dict = read_json(DATA_DIR_TRAIN)\n# test_dict = read_data(DATA_DIR_TEST)\n# save_json(test_dict, 'data\/test.json')\n# test_dict = read_json(DATA_DIR_TEST)","407cc731":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics import classification_report\nfrom keras.models import Model\nfrom keras.layers import LSTM, Activation, Dense, Dropout, Input, Embedding\nfrom keras.optimizers import RMSprop, Adam\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing import sequence\nfrom keras.utils import to_categorical\nfrom keras.callbacks import EarlyStopping\n%matplotlib inline\n\n# Ki\u1ec3m tra ph\u00e2n ph\u1ed1i s\u1ed1 l\u01b0\u1ee3ng b\u00e0i b\u00e1o c\u1ee7a m\u1ed7i topic_ids\nsns.countplot(train_dict['topic_ids'])\nplt.xlabel('Label')\nplt.title('Number of each topics')","ced53913":"X = train_dict['contents']\ny = train_dict['topic_ids']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.5, stratify = y)","eaddb71e":"sns.countplot(y_train)\nplt.xlabel('Label')\nplt.title('Number of each topics')","6e4daf3b":"sns.countplot(y_test)\nplt.xlabel('Label')\nplt.title('Number of each topics')","a3350667":"max_words = 3000\nmax_len = 150\ntok = Tokenizer(num_words = max_words)\ntok.fit_on_texts(X_train)\nsequences = tok.texts_to_sequences(X_train)\nsequences_matrix = sequence.pad_sequences(sequences,maxlen = max_len)\nprint('sequences[2] length: ', len(sequences[2]))\nprint('sequences length: ', len(sequences))\nprint('sequences_matrix shape: ', sequences_matrix.shape)\nprint('sequences_matrix first row: ', sequences_matrix[1, :])","a68094dc":"def RNN():\n    inputs = Input(name='inputs',shape=[max_len])\n    layer = Embedding(max_words,50,input_length=max_len)(inputs)\n    # Embedding (input_dim: size of vocabolary, \n    # output_dim: dimension of dense embedding, \n    # input_length: length of input sequence)\n    layer = LSTM(64)(layer)\n    layer = Dense(256,name='FC1')(layer)\n    layer = Activation('relu')(layer)\n    layer = Dense(128,name='FC2')(layer)\n    layer = Activation('relu')(layer)\n    layer = Dropout(0.5)(layer)\n    layer = Dense(10,name='out_layer')(layer)\n    layer = Activation('softmax')(layer)\n    model = Model(inputs=inputs,outputs=layer)\n    return model","2135fff5":"model = RNN()\nmodel.summary()","e62c13d6":"model.compile(loss = 'sparse_categorical_crossentropy', \\\n              optimizer = RMSprop(), metrics = ['accuracy'])","f151259d":"model.fit(sequences_matrix, y_train, batch_size = 10, epochs = 10,\n          validation_split = 0.2, callbacks = \\\n          [EarlyStopping(monitor = 'val_loss', min_delta = 0.01)])","348ce60e":"test_sequences = tok.texts_to_sequences(X_test)\ntest_sequences_matrix = sequence.pad_sequences(test_sequences,maxlen=max_len)\naccr = model.evaluate(test_sequences_matrix,y_test)\nprint('Test set\\n  Loss: {:0.3f}\\n  Accuracy: {:0.3f}'.format(accr[0],accr[1]))","df8dff50":"y_pred = model.predict_classes(test_sequences_matrix)\nprint(classification_report(y_pred, y_test))","deedbee2":"Nh\u01b0 v\u1eady k\u1ebft qu\u1ea3 d\u1ef1 b\u00e1o tr\u00ean t\u1eadp test cho th\u1ea5y m\u00f4 h\u00ecnh ph\u00e2n lo\u1ea1i c\u00f3 m\u1ee9c \u0111\u1ed9 ch\u00ednh x\u00e1c kh\u00f4ng qu\u00e1 cao nh\u01b0ng \u0111\u00e3 v\u01b0\u1ee3t m\u1ed9t s\u1ed1 m\u00f4 h\u00ecnh ph\u00e2n lo\u1ea1i truy\u1ec1n th\u1ed1ng c\u1ee7a Machine Learning.","54e94764":"\u0110\u00e1nh gi\u00e1 m\u1ee9c \u0111\u1ed9 ch\u00ednh x\u00e1c c\u1ee7a model tr\u00ean t\u1eadp test.","73e2001d":"L\u1ef1a ch\u1ecdn h\u00e0m loss function l\u00e0 `sparse_categorical_crossentropy` do bi\u1ebfn m\u1ee5c ti\u00eau $y$ l\u00e0 bi\u1ebfn th\u1ee9 b\u1eadc. Metric \u0111\u01b0\u1ee3c d\u00f9ng \u0111\u1ec3 \u0111o l\u01b0\u1eddng k\u1ebft qu\u1ea3 l\u00e0 \u0111\u1ed9 ch\u00ednh x\u00e1c `accuracy`. Ph\u01b0\u01a1ng ph\u00e1p gradient descent \u0111\u01b0\u1ee3c s\u1eed d\u1ee5ng \u0111\u1ec3 c\u1eadp nh\u1eadt nghi\u1ec7m l\u00e0 [RMSprop](https:\/\/towardsdatascience.com\/a-look-at-gradient-descent-and-rmsprop-optimizers-f77d483ef08b), m\u1ed9t ph\u01b0\u01a1ng ph\u00e1p c\u1eadp nh\u1eadt nghi\u1ec7m d\u1ef1a tr\u00ean momentum v\u00e0 c\u00f3 \u0111i\u1ec1u ch\u1ec9nh learning rate.","f2369a2f":"## 3.2. Word embedding.\n\n\u1ede b\u01b0\u1edbc n\u00e0y ta s\u1ebd l\u1ea5y ra 2000 t\u1eeb c\u00f3 t\u1ea7n xu\u1ea5t xu\u1ea5t hi\u1ec7n nhi\u1ec1u nh\u1ea5t trong to\u00e0n b\u1ed9 b\u1ed9 v\u0103n b\u1ea3n (*corpus*). M\u1ed7i m\u1ed9t t\u1eeb s\u1ebd \u0111\u01b0\u1ee3c th\u1ec3 hi\u1ec7n b\u1eb1ng index t\u1ea7n xu\u1ea5t xu\u1ea5t hi\u1ec7n c\u1ee7a n\u00f3 trong 2000 t\u1eeb. C\u00e1c document s\u1ebd \u0111\u01b0\u1ee3c bi\u1ec3u di\u1ec5n b\u1eb1ng index c\u1ee7a t\u1eeb. \n\nH\u00e0m Tokenizer(num_words = 2000) s\u1ebd \u0111\u01b0\u1ee3c s\u1eed d\u1ee5ng \u0111\u1ec3 l\u1ecdc 2000 t\u1eeb v\u00e0 \u0111\u00e1nh index cho c\u00e1c t\u1eeb t\u1ea7n su\u1ea5t cao. C\u00e1c t\u1eeb c\u00f2n l\u1ea1i s\u1ebd \u0111\u01b0\u1ee3c b\u1ecf qua v\u00e0 coi nh\u01b0 c\u00f3 index = 0. Sau khi \u00e1p d\u1ee5ng h\u00e0m `texts_to_sequences()` c\u1ee7a tokenizer, list c\u00e1c t\u1eeb v\u1ef1ng c\u1ee7a m\u1ed7i m\u1ed9t document s\u1ebd \u0111\u01b0\u1ee3c bi\u1ebfn \u0111\u1ed5i th\u00e0nh c\u00e1c sequences l\u00e0 nh\u1eefng list c\u00e1c s\u1ed1. Do c\u00e1c list n\u00e0y c\u00f3 \u0111\u1ed9 d\u00e0i kh\u00e1c bi\u1ec7t nhau n\u00ean kh\u00f4ng th\u1ec3 \u0111\u01b0a v\u00e0o \u0111\u01b0\u1ee3c m\u00f4 h\u00ecnh m\u1ea1ng neural. Do \u0111\u00f3 \u0111\u00f2i h\u1ecfi ta ph\u1ea3i \u00e1p d\u1ee5ng h\u00e0m `sequence.pad_sequences()` bi\u1ebfn c\u00e1c list th\u00e0nh nh\u1eefng vector c\u00f3 \u0111\u1ed9 d\u00e0i b\u1eb1ng nhau b\u1eb1ng c\u00e1ch th\u00eam v\u00e0o b\u00ean ph\u1ea3i c\u1ee7a list c\u00e1c gi\u00e1 tr\u1ecb 0 n\u1ebfu s\u1ed1 ph\u1ea7n t\u1eed nh\u1ecf h\u01a1n \u0111\u1ed9 d\u00e0i c\u1ee7a vector v\u00e0 c\u1eaft ng\u1eafn nh\u1eefng list c\u00f3 \u0111\u1ed9 d\u00e0i l\u1edbn h\u01a1n \u0111\u1ed9 d\u00e0i cho ph\u00e9p c\u1ee7a vector.","d658c471":"Ph\u00e2n chia t\u1eadp train, test b\u1eb1ng h\u00e0m `train_test_split()` v\u1edbi ph\u00e2n ph\u1ed1i c\u1ee7a t\u1ef7 l\u1ec7 m\u1eabu theo c\u00e1c topics \u0111\u01b0\u1ee3c gi\u1eef nguy\u00ean tr\u00ean c\u1ea3 t\u1eadp train v\u00e0 test.","6f26bef3":"## 3.4. D\u1ef1 b\u00e1o.\n\nD\u1ef1a tr\u00ean m\u00f4 h\u00ecnh v\u1eeba hu\u1ea5n luy\u1ec7n, ch\u00fang ta s\u1ebd d\u1ef1 b\u00e1o d\u1ef1a tr\u00ean t\u1eadp test v\u00e0 t\u00ednh to\u00e1n c\u00e1c gi\u00e1 tr\u1ecb \u0111o l\u01b0\u1eddng accuracy c\u1ee7a model tr\u00ean t\u1eadp test nh\u01b0 Recal, Precision, F1-score, Accuracy.","8e88fb22":"Load d\u1eef li\u1ec7u train v\u00e0 test.","49dd0335":"Ki\u1ec3m tra s\u1ed1 l\u01b0\u1ee3ng ph\u00e2n ph\u1ed1i theo topics c\u1ee7a train","29abb43c":"Bi\u1ec3u di\u1ec5n c\u00e1c document b\u1eb1ng index c\u1ee7a 3000 t\u1eeb v\u1ef1ng l\u1edbn nh\u1ea5t v\u00e0 padding c\u00e1c list \u0111\u1ec3 ch\u00fang c\u00f3 \u0111\u1ed9 d\u00e0i b\u1eb1ng nhau v\u00e0 b\u1eb1ng 150 ph\u1ea7n t\u1eed.","60a6659d":"Hu\u1ea9n luy\u1ec7n m\u00f4 h\u00ecnh v\u1edbi k\u00edch th\u01b0\u1edbc batch_size = 30, s\u1ed1 l\u01b0\u1ee3ng epochs = 10. \u1ede \u0111\u00e2y ch\u00fang ta s\u1eed d\u1ee5ng k\u0129 thu\u1eadt l\u1ea5y 20% m\u1eabu l\u00e0m k\u1ebft qu\u1ea3 validation. Gi\u00e1 tr\u1ecb accuracy \u0111\u01b0\u1ee3c th\u1ec3 hi\u1ec7n \u1edf log ch\u00ednh l\u00e0 gi\u00e1 tr\u1ecb accuracy tr\u00ean m\u1eabu validation. Ph\u01b0\u01a1ng ph\u00e1p Earlystopping \u0111\u01b0\u1ee3c s\u1eed d\u1ee5ng \u0111\u1ec3 gi\u1ea3m thi\u1ec1u overfitting. Khi gi\u00e1 tr\u1ecb loss function tr\u00ean t\u1eadp validation kh\u00f4ng c\u00f2n ti\u1ebfp t\u1ee5c gi\u1ea3m sau c\u00e1c step v\u00e0 n\u1ebfu ng\u01b0\u1ee1ng ch\u00eanh l\u1ec7ch c\u1ee7a loss function so v\u1edbi gi\u00e1 tr\u1ecb li\u1ec1n tr\u01b0\u1edbc l\u00e0 0.0001 th\u00ec s\u1ebd d\u1eebng thu\u1eadt to\u00e1n.","df2bd912":"# 1. Gi\u1edbi thi\u1ec7u.\n\nNg\u00e0y nay v\u1edbi s\u1ef1 b\u00f9ng n\u1ed5 c\u1ee7a internet, m\u1ea1ng x\u00e3 h\u1ed9i, c\u00e1c n\u1ec1n t\u1ea3ng online, d\u1eef li\u1ec7u ng\u00e0y c\u00e0ng tr\u1edf n\u00ean phong ph\u00fa v\u00e0 \u0111a d\u1ea1ng. \u0110\u1ee9ng tr\u01b0\u1edbc ngu\u1ed3n d\u1eef li\u1ec7u v\u00f4 t\u1eadn \u0111\u00f3, c\u00e1c doanh nghi\u1ec7p th\u01b0\u1eddng g\u1eb7p kh\u00f3 kh\u0103n, l\u00fang t\u00fang trong kh\u00e2u \u0111\u1eb7t b\u00e0i to\u00e1n, t\u1ed5 ch\u1ee9c khai th\u00e1c d\u1eef li\u1ec7u sao cho \u0111\u1ea1t hi\u1ec7u qu\u1ea3, th\u00fac \u0111\u1ea9y t\u1eadp kh\u00e1ch h\u00e0ng, th\u00fac \u0111\u1ea9y kinh doanh, v\u00e0 \u0111a d\u1ea1ng h\u00f3a s\u1ea3n ph\u1ea9m. Nh\u1edd c\u00e1c thu\u1eadt to\u00e1n ph\u00e2n l\u1edbp c\u1ee7a AI v\u00e0 s\u1ee9c m\u1ea1nh t\u00ednh to\u00e1n c\u1ee7a m\u00e1y t\u00ednh ng\u00e0y \u0111\u01b0\u1ee3c t\u0103ng c\u01b0\u1eddng, doanh nghi\u1ec7p \u0111\u00e3 c\u00f3 th\u1ec3 x\u1eed l\u00fd \u0111\u01b0\u1ee3c r\u1ea5t nhi\u1ec1u nh\u1eefng b\u00e0i to\u00e1n kh\u00f3 trong kinh doanh. M\u1ed9t trong nh\u1eefng b\u00e0i to\u00e1n \u0111\u00f3 l\u00e0 ph\u00e2n lo\u1ea1i v\u0103n b\u1ea3n, m\u1ed9t l\u1edbp b\u00e0i to\u00e1n li\u00ean quan \u0111\u1ebfn x\u1eed l\u00fd ng\u00f4n ng\u1eef t\u1ef1 nhi\u00ean (*NLP*) hi\u1ec7n \u0111ang \u0111\u01b0\u1ee3c \u1ee9ng d\u1ee5ng r\u1ed9ng r\u00e3i v\u00e0 \u0111\u1ed3ng th\u1eddi mang l\u1ea1i hi\u1ec3u qu\u1ea3 cao cho doanh nghi\u1ec7p trong ph\u00e2n t\u00edch h\u00e0nh vi kh\u00e1ch h\u00e0ng, s\u1ea3n ph\u1ea9m. D\u1ef1a tr\u00ean c\u00e1c thu\u1eadt to\u00e1n v\u1ec1 x\u1eed l\u00fd ng\u00f4n ng\u1eef nh\u01b0 nh\u00fang t\u1eeb (*word embedding*), \u0111\u00e1nh tr\u1ecdng s\u1ed1 Tfidf (*term frequence - inverse document frequence*), c\u00e1c c\u1ea5u tr\u00fac m\u1ea1ng n\u01a1 ron truy h\u1ed3i (*recurrent neural network*), m\u1ea1ng \u0111\u1ecbnh tr\u00ed nh\u1edb ng\u1eafn h\u1ea1n \u0111\u1ecbnh h\u01b0\u1edbng d\u00e0i h\u1ea1n (*long - short term memmory*) v\u00e0 c\u00e1c c\u1ea5u tr\u00fac m\u1ea1ng t\u00edch ch\u1eadp (*covolutional neural network*), doanh nghi\u1ec7p c\u00f3 th\u1ec3 gi\u1ea3i quy\u1ebft \u0111\u01b0\u1ee3c c\u00e1c t\u00e1c v\u1ee5 h\u1ecdc m\u00e1y v\u1edbi \u0111\u1ed9 ch\u00ednh x\u00e1c t\u01b0\u01a1ng \u0111\u01b0\u01a1ng ho\u1eb7c th\u1eadm ch\u00ed l\u00e0 h\u01a1n c\u1ea3 con ng\u01b0\u1eddi trong nh\u01b0:\n\n* Ph\u00e2n lo\u1ea1i email spam.\n* X\u00e1c \u0111\u1ecbnh c\u1ea3m x\u00fac ng\u01b0\u1eddi d\u00f9ng l\u00e0 ti\u00eau c\u1ef1c\/t\u00edch c\u1ef1c \u0111\u1ed1i v\u1edbi m\u1ed9t m\u1eb7t h\u00e0ng.\n* Ph\u00e2n lo\u1ea1i n\u1ed9i dung v\u0103n b\u1ea3n theo ch\u1ee7 \u0111\u1ec1: Ch\u1eb3ng h\u1ea1n ph\u00e2n lo\u1ea1i h\u1ea1ng m\u1ee5c b\u00e1o ch\u00ed, ph\u00e2n lo\u1ea1i n\u1ed9i dung s\u1ea3n ph\u1ea9m,....\n* T\u00ecm hi\u1ec3u ph\u00e2n b\u1ed1 c\u1ee7a ch\u1ee7 \u0111\u1ec1 \u0111\u01b0\u1ee3c n\u00f3i \u0111\u1ebfn trong m\u1ed9t \u0111o\u1ea1n v\u0103n.\n* X\u00e1c \u0111\u1ecbnh c\u00e1c th\u1ef1c th\u1ec3 trong m\u1ed9t \u0111o\u1ea1n h\u1ed9i tho\u1ea1i.\n* Tagging c\u00e1c th\u00e0nh ph\u1ea7n c\u1ee7a c\u00e2u.\n* X\u00e2y d\u1ef1ng c\u00e1c h\u1ec7 th\u1ed1ng chatbot t\u1ef1 \u0111\u1ed9ng.\n* C\u00e1c thu\u1eadt to\u00e1n GAN t\u1ef1 \u0111\u1ed9ng sinh v\u0103n b\u1ea3n nh\u1eb1m ph\u1ee5c v\u1ee5 cho vi\u1ebft b\u00e0i, vi\u1ebft tin.\n* Thu\u1eadt to\u00e1n \u1ee9ng d\u1ee5ng trong d\u1ecbch m\u00e1y.\n* Nh\u1eadn di\u1ec7n gi\u1ecdng n\u00f3i.\n\nTrong b\u00e0i vi\u1ebft n\u00e0y m\u00ecnh s\u1ebd gi\u1edbi thi\u1ec7u \u0111\u1ebfn c\u00e1c b\u1ea1n l\u1edbp b\u00e0i to\u00e1n \u0111\u01b0\u1ee3c \u1ee9ng d\u1ee5ng nhi\u1ec1u nh\u1ea5t v\u00e0 c\u01a1 b\u1ea3n nh\u1ea5t trong NLP \u0111\u00f3 l\u00e0 ph\u00e2n lo\u1ea1i n\u1ed9i dung v\u0103n b\u1ea3n theo ch\u1ee7 \u0111\u1ec1. \u0110\u00e2y l\u00e0 b\u00e0i to\u00e1n h\u1ecdc c\u00f3 gi\u00e1m s\u00e1t \u0111\u01b0\u1ee3c x\u00e2y d\u1ef1ng tr\u00ean t\u1eadp d\u1eef li\u1ec7u g\u1ed3m 10 topics thu\u1ed9c l\u0129nh v\u1ef1c b\u00e1o ch\u00ed nh\u01b0: Ch\u00ednh tr\u1ecb x\u00e3 h\u1ed9i, v\u0103n h\u00f3a, th\u1ec3 thao, du l\u1ecbch,.... B\u1ed9 d\u1eef li\u1ec7u c\u00f3 t\u00ean l\u00e0 [VNTC](https:\/\/github.com\/duyvuleo\/VNTC\/tree\/master\/Data\/10Topics\/Ver1.1) \u0111\u01b0\u1ee3c thu th\u1eadp b\u1edfi m\u1ed9t nh\u00f3m c\u00e1c anh\/ch\u1ecb l\u00e0m trong x\u1eed l\u00fd ng\u00f4n ng\u1eef t\u1ef1 nhi\u00ean. \n\n# 2. B\u00e0i to\u00e1n.\n\nC\u00f3 r\u1ea5t nhi\u1ec1u c\u00e1c ph\u01b0\u01a1ng ph\u00e1p \u0111\u1ec3 gi\u1ea3i quy\u1ebft b\u00e0i to\u00e1n ph\u00e2n lo\u1ea1i v\u0103n b\u1ea3n c\u00f3 th\u1ec3 k\u1ec3 \u0111\u1ebfn nh\u01b0: C\u00e1c thu\u1eadt to\u00e1n machine learning c\u01a1 b\u1ea3n thu\u1ed9c l\u1edbp lazy learning v\u00e0 non-parametric nh\u01b0 kNN, thu\u1eadt to\u00e1n d\u1ef1a tr\u00ean bi\u00ean ph\u00e2n chia \u0111\u1ed9 r\u1ed9ng l\u1edbn nh\u1ea5t SVM v\u00e0 c\u00e1c thu\u1eadt to\u00e1n deep learing s\u1eed d\u1ee5ng c\u00e1c ki\u1ebfn tr\u00fac m\u1ea1ng neural network d\u1ef1a tr\u00ean qu\u00e1 tr\u00ecnh lan truy\u1ec1n thu\u1eadn v\u00e0 lan truy\u1ec1n ng\u01b0\u1ee3c. Trong b\u00e0i n\u00e0y m\u00ecnh s\u1ebd h\u01b0\u1edbng d\u1eabn c\u00e1c b\u1ea1n gi\u1ea3i quy\u1ebft b\u00e0i to\u00e1n th\u00f4ng qua m\u1ea1ng neural network. Nh\u01b0 h\u1ea7u h\u1ebft c\u00e1c b\u00e0i to\u00e1n ph\u00e2n lo\u1ea1i s\u1eed d\u1ee5ng m\u1ea1ng neural, \u0111\u1ec3 th\u1ef1c hi\u1ec7n m\u1ed9t b\u00e0i to\u00e1n v\u1ec1 x\u1eed l\u00fd ng\u00f4n ng\u1eef t\u1ef1 nhi\u00ean th\u00f4ng th\u01b0\u1eddng ch\u00fang ta s\u1ebd ph\u1ea3i tr\u1ea3i qua c\u00e1c b\u01b0\u1edbc c\u01a1 b\u1ea3n sau \u0111\u00e2y:\n\n1. Thu th\u1eadp v\u00e0 t\u1ed5ng h\u1ee3p v\u00e0 x\u1eed l\u00fd d\u1eef li\u1ec7u.\n2. Th\u1ef1c hi\u1ec7n word embedding cho c\u00e1c t\u1eeb trong c\u00e2u.\n3. X\u00e2y d\u1ef1ng ki\u1ebfn tr\u1ee5c m\u1ea1ng neural.\n4. X\u00e2y d\u1ef1ng h\u00e0m loss function v\u00e0 thu\u1eadt to\u00e1n gradient descent ph\u00f9 h\u1ee3p.\n5. Hu\u1ea5n luy\u1ec7n m\u00f4 h\u00ecnh tr\u00ean t\u1eadp train.\n6. \u0110\u00e1nh gi\u00e1 m\u00f4 h\u00ecnh d\u1ef1 b\u00e1o tr\u00ean t\u1eadp test.\n\nB\u01b0\u1edbc 2 nh\u1eb1m m\u1ee5c \u0111\u00edch l\u01b0\u1ee3ng h\u00f3a \u0111\u01b0\u1ee3c c\u00e1c t\u1eeb ng\u1eef. Thay v\u00ec s\u1eed d\u1ee5ng d\u1eef li\u1ec7u l\u00e0 text, ch\u00fang ta th\u01b0\u1eddng s\u1ebd ph\u1ea3i g\u00e1n cho m\u1ed7i m\u1ed9t t\u1eeb b\u1eb1ng 1 \u0111\u1ea1i l\u01b0\u1ee3ng scalar th\u1ec3 hi\u1ec7n gi\u00e1 tr\u1ecb c\u1ee7a t\u1eeb. \u0110\u1ed1i v\u1edbi c\u00e1c ph\u01b0\u01a1ng ph\u00e1p machine learning c\u01a1 b\u1ea3n do ph\u00e2n lo\u1ea1i c\u00e1c v\u0103n b\u1ea3n ch\u1ec9 c\u1ea7n d\u1ef1a tr\u00ean kho\u1ea3ng c\u00e1ch gi\u1eefa c\u00e1c vector n\u00ean vi\u1ec7c g\u00e1n t\u1eeb b\u1eb1ng 1 \u0111\u1ea1i l\u01b0\u1ee3ng scalar m\u00e0 kh\u00f4ng c\u1ea7n ph\u1ea3i x\u1eed l\u00fd g\u00ec th\u00eam. \u0110\u1ed1i v\u1edbi m\u00f4 h\u00ecnh deep learning m\u1ed9t layer embedding s\u1ebd \u0111\u01b0\u1ee3c th\u00eam m\u1edbi ngay sau layer input nh\u1eb1m bi\u1ebfn d\u1eef li\u1ec7u ma tr\u1eadn 2 chi\u1ec1u th\u00e0nh m\u1ed9t kh\u1ed1i tensor 3 chi\u1ec1u. Layer embedding n\u00e0y cho ph\u00e9p bi\u1ec3u di\u1ec5n m\u1ed7i m\u1ed9t t\u1eeb b\u1edfi m\u1ed9t vector. C\u00e1c ph\u1ea7n t\u1eed c\u1ee7a m\u1ed9t vector t\u1eeb s\u1ebd ch\u00ednh l\u00e0 c\u00e1c tham s\u1ed1 c\u1ea7n t\u1ed1i \u01b0u c\u1ee7a m\u00f4 h\u00ecnh. C\u00e1c vector c\u1ea7n tu\u00e2n theo gi\u1ea3 thuy\u1ebft v\u1ec1 ph\u00e2n ph\u1ed1i (*distributional hypothesis*) trong NLP \u0111\u00f3 l\u00e0 c\u00e1c t\u1eeb c\u00f3 ngh\u0129a c\u00e0ng gi\u1ed1ng nhau th\u00ec gi\u00e1 tr\u1ecb kho\u1ea3ng c\u00e1ch c\u1ee7a c\u00e1c vectors c\u00e0ng s\u00e1t nhau.\n\n\u1ede b\u01b0\u1edbc 3 ch\u00fang ta c\u1ea7n thi\u1ebft k\u1ebf ra m\u1ed9t ki\u1ebfn tr\u00fac m\u1ea1ng neuron g\u1ed3m nhi\u1ec1u layer trong \u0111\u00f3 c\u00f3 input layer, c\u00e1c hidden layers, v\u00e0 output layers. T\u1ea1i input layer th\u01b0\u1eddng c\u00f3 d\u1eef li\u1ec7u \u0111\u1ea7u v\u00e0o l\u00e0 m\u1ed9t tensor nhi\u1ec1u chi\u1ec1u \u0111\u1ea1i di\u1ec7n cho n\u1ed9i dung c\u1ee7a c\u00e1c \u0111o\u1ea1n v\u0103n \u0111\u00e3 \u0111\u01b0\u1ee3c word embedding \u1edf b\u01b0\u1edbc 2. M\u1ed9t \u0111o\u1ea1n v\u0103n s\u1ebd \u0111\u01b0\u1ee3c m\u00e3 h\u00f3a b\u1edfi th\u00f4ng th\u01b0\u1eddng l\u00e0 m\u1ed9t tensor 3D. Trong \u0111\u00f3 m\u1ed7i c\u00e2u v\u0103n s\u1ebd \u0111\u1ea1i di\u1ec7n b\u1edfi ma tr\u1eadn c\u00e1c t\u1eeb. M\u00e0 m\u1ed7i m\u1ed9t t\u1eeb s\u1ebd t\u01b0\u01a1ng \u1ee9ng v\u1edbi m\u1ed9t vector. C\u00e1c layer ti\u1ebfp theo ch\u00fang ta th\u01b0\u1eddng d\u00f9ng l\u00e0 LSTM v\u00e0 ti\u1ebfp \u0111\u1ebfn l\u00e0 c\u00e1c fully connected layer. Gi\u1eefa c\u00e1c layers c\u00f3 th\u1ec3 s\u1eed d\u1ee5ng c\u00e1c h\u00e0m k\u00edch ho\u1ea1t relu nh\u1eb1m bi\u1ebfn \u0111\u1ed5i d\u1eef li\u1ec7u phi tuy\u1ebfn qua nhi\u1ec1u t\u1ea7ng \u1ea9n v\u00e0 \u0111\u01b0a ra k\u1ebft qu\u1ea3 cu\u1ed1i c\u00f9ng \u1edf layer output. Layer output \u0111\u01b0\u1ee3c thi\u1ebft k\u1ebf c\u00f3 s\u1ed1 unit b\u1eb1ng s\u1ed1 class v\u00e0 h\u00e0m activation l\u00e0 softmax \u0111\u01b0\u1ee3c \u00e1p d\u1ee5ng \u0111\u1ec3 t\u00ednh x\u00e1c xu\u1ea5t (trong tr\u01b0\u1eddng h\u1ee3p binary classification ch\u1ec9 c\u1ea7n 1 unit output, v\u00e0 activation l\u00e0 sigmoid).\n\n\u1ede b\u01b0\u1edbc 4 khi x\u00e2y d\u1ef1ng b\u00e0i to\u00e1n ph\u00e2n l\u1edbp nh\u1ecb ph\u00e2n, h\u00e0m loss function n\u00ean \u0111\u01b0\u1ee3c l\u1ef1a ch\u1ecdn l\u00e0 binary_crossentropy. \u0110\u1ed1i v\u1edbi c\u00e1c b\u00e0i to\u00e1n ph\u00e2n lo\u1ea1i nhi\u1ec1u h\u01a1n 2 l\u1edbp ch\u00fang ta c\u00f3 th\u1ec3 s\u1eed d\u1ee5ng h\u00e0m loss function categorical_crossentropy. Bi\u1ec3u di\u1ec5n c\u1ee7a y c\u00f3 th\u1ec3 d\u01b0\u1edbi d\u1ea1ng s\u1ed1 c\u00f3 gi\u00e1 tr\u1ecb d\u1ea1ng th\u1ee9 b\u1eadc, ch\u1eb3ng h\u1ea1n n\u1ebfu b\u00e0i to\u00e1n ph\u00e2n lo\u1ea1i 3 l\u1edbp th\u00ec $y \\in \\{1,2,3\\}$ ho\u1eb7c m\u1ed9t one-hot matrix c\u00f3 s\u1ed1 chi\u1ec1u b\u1eb1ng s\u1ed1 l\u1edbp d\u1ea1ng \n\n$$\\begin{bmatrix}1 & 0 & 0 \\\\\n0 & 1 & 0 \\\\\n& \\dots & \\\\ \n0 & 1 & 0 \\\\\\end{bmatrix}$$ \n\n\nTrong keras v\u00e0 tensorflow \u0111\u1ed1i v\u1edbi bi\u1ec3u di\u1ec5n d\u1ea1ng th\u1ee9 b\u1eadc ta s\u1ebd s\u1eed d\u1ee5ng h\u00e0m loss function l\u00e0 `sparse_categorical_crossentropy` v\u00e0 tr\u01b0\u1eddng h\u1ee3p one-hot matrix s\u1ebd l\u00e0 `categorical_crossentropy`. Ta s\u1ebd s\u1eed d\u1ee5ng c\u00e1c thu\u1eadt to\u00e1n gradient descent th\u00f4ng d\u1ee5ng nh\u01b0 RMSprop, Adam, Ada, .... Trong tr\u01b0\u1eddng h\u1ee3p c\u00f3 s\u1eed d\u1ee5ng layer LSTM th\u00ec thu\u1eadt to\u00e1n RMSprop th\u01b0\u1eddng \u0111\u01b0\u1ee3c s\u1eed d\u1ee5ng v\u00ec n\u00f3 mang l\u1ea1i k\u1ebft qu\u1ea3 t\u1ed1t h\u01a1n so v\u1edbi c\u00e1c thu\u1eadt to\u00e1n kh\u00e1c. \n\nSau \u0111\u00e2y ta s\u1ebd ti\u1ebfn h\u00e0nh x\u00e2y d\u1ef1ng m\u00f4 h\u00ecnh.\n\n# 3. X\u00e2y d\u1ef1ng m\u00f4 h\u00ecnh.\n## 3.1. Chu\u1ea9n b\u1ecb d\u1eef li\u1ec7u.\n\n\u1ede b\u01b0\u1edbc n\u00e0y ta c\u1ea7n truy c\u1eadp v\u00e0o t\u1eebng file d\u1eef li\u1ec7u trong m\u1ed7i folder topics \u0111\u1ec3 \u0111\u1ecdc n\u1ed9i dung v\u00e0 l\u01b0u v\u00e0o m\u1ed9t \u0111\u1ecbnh d\u1ea1ng json. Qu\u00e1 tr\u00ecnh th\u1ef1c hi\u1ec7n kh\u00e1 l\u00e2u n\u00ean m\u00ecnh \u0111\u00e3 l\u00e0m s\u1eb5n b\u01b0\u1edbc n\u00e0y v\u00e0 l\u01b0u v\u00e0o 2 file ch\u00ednh l\u00e0 `train_dict.json` v\u00e0 `test_dict.json` t\u01b0\u01a1ng \u1ee9ng v\u1edbi t\u1eadp train v\u00e0 test. M\u1ed7i file \u0111\u1ec1u g\u1ed3m 3 tr\u01b0\u1eddng d\u1eef li\u1ec7u. Trong \u0111\u00f3:\n\n* topic_ids: nh\u00e3n th\u1ee9 b\u1eadc t\u01b0\u01a1ng \u1ee9ng v\u1edbi ch\u1ee7 \u0111\u1ec1 c\u1ee7a b\u00e0i b\u00e1o.\n* contents: N\u1ed9i dung c\u00e1c b\u00e0i b\u00e1o.\n* file_names: T\u00ean file.\n\nB\u00ean d\u01b0\u1edbi l\u00e0 c\u00e1c b\u01b0\u1edbc \u0111\u1ecdc d\u1eef li\u1ec7u `train_dict.json`, `test_dict.json` v\u00e0 g\u00e1n nh\u00e3n cho c\u00e1c TOPIC.","21f4f755":"Ki\u1ec3m tra s\u1ed1 l\u01b0\u1ee3ng ph\u00e2n ph\u1ed1i theo topics c\u1ee7a test","f775cf6e":"## 4. M\u1edf r\u1ed9ng.\n\nNh\u01b0 v\u1eady ta \u0111\u00e3 k\u1ebft th\u00fac b\u00e0i th\u1ef1c h\u00e0nh v\u1ec1 x\u00e2y d\u1ef1ng m\u00f4 h\u00ecnh ph\u00e2n lo\u1ea1i v\u0103n b\u1ea3n d\u1ef1a tr\u00ean thu\u1eadt to\u00e1n LSTM. Qua b\u00e0i h\u01b0\u1edbng d\u1eabn n\u00e0y c\u00e1c b\u1ea1n n\u1eafm \u0111\u01b0\u1ee3c:\n\n1. K\u0129 thu\u1eadt x\u1eed l\u00fd embedding d\u1eef li\u1ec7u v\u0103n b\u1ea3n.\n2. C\u00e1c thi\u1ebft k\u1ebf m\u1ed9t m\u1ea1ng neural network LSTM.\n3. L\u1ef1a ch\u1ecdn h\u00e0m loss function v\u00e0 ph\u01b0\u01a1ng ph\u00e1p gradient descent.\n4. \u0110\u00e1nh gi\u00e1 c\u00e1c th\u00f4ng s\u1ed1 c\u1ee7a m\u00f4 h\u00ecnh d\u1ef1 b\u00e1o th\u00f4ng qua h\u00e0m classification_report.\n\nTuy nhi\u00ean k\u1ebft qu\u1ea3 n\u00e0y \u0111\u00e3 ph\u1ea3i l\u00e0 k\u1ebft qu\u1ea3 t\u1ed1t nh\u1ea5t? \u0110i\u1ec1u g\u00ec s\u1ebd x\u1ea3y ra n\u1ebfu nh\u01b0 ta:\n\n1. Th\u00eam c\u00e1c layers LSTM v\u00e0o model.\n2. Thay \u0111\u1ed5i ph\u01b0\u01a1ng ph\u00e1p word embedding theo index th\u1ee9 t\u1ef1 v\u1ec1 frequences c\u1ee7a t\u1eeb b\u1eb1ng c\u00e1c thu\u1eadt to\u00e1n \u0111\u00e1nh tr\u1ecdng s\u1ed1 d\u1ef1a tr\u00ean t\u1ea7n su\u1ea5t xu\u1ea5t hi\u1ec7n c\u1ee7a t\u1eeb nh\u01b0 Tfidf (*Term frequence - inverse document frequence*).\n3. T\u0103ng batch_size c\u1ee7a m\u00f4 h\u00ecnh hu\u1ea5n luy\u1ec7n. \n4. Thay \u0111\u1ed5i s\u1ed1 l\u01b0\u1ee3ng unit \u1edf m\u1ed7i layer trung gian.\n\nC\u00e2u tr\u1ea3 l\u1eddi xin d\u00e0nh cho b\u1ea1n \u0111\u1ecdc.","238cc7a7":"## 3.3. X\u00e2y d\u1ef1ng m\u1ea1ng neural network\n\nCh\u00fang ta s\u1ebd x\u00e2y d\u1ef1ng m\u1ea1ng neural network g\u1ed3m c\u00e1c layer nh\u01b0 sau:","0361065f":"Trong \u0111\u00f3:\n    \n* Input: Khai b\u00e1o layer \u0111\u1ea7u v\u00e0o. Layer n\u00e0y c\u00f3 shape b\u1eb1ng v\u1edbi \u0111\u1ed9 d\u00e0i vector sau khi padding = 150.\n* Embedding: Th\u1ef1c hi\u1ec7n ph\u00e9p nh\u00fang t\u1eeb. \u1ede layer n\u00e0y ch\u00fang ta c\u1ea7n khai b\u00e1o l\u1ea7n l\u01b0\u1ee3t c\u00e1c tham s\u1ed1:\n    1. input_dim: K\u00edch th\u01b0\u1edbc c\u1ee7a c\u00e1c t\u1eeb v\u1ef1ng v\u00e0 ch\u00ednh b\u1eb1ng `maximun_index + 1`\n    2. output_dim: Ch\u00ednh l\u00e0 k\u00edch th\u01b0\u1edbc c\u1ee7a embedding c\u1ee7a m\u1ed9t t\u1eeb trong c\u00e2u. N\u1ebfu ta mu\u1ed1n bi\u1ec3u di\u1ec5n m\u1ed7i m\u1ed9t t\u1eeb b\u1edfi m\u1ed9t vector c\u00f3 \u0111\u1ed9 d\u00e0i l\u00e0 50 th\u00ec khai b\u00e1o output_dim = 50.\n    3. input_length: \u0110\u1ed9 d\u00e0i c\u1ee7a m\u1ed9t v\u0103n b\u1ea3n sau khi \u0111\u01b0\u1ee3c padding v\u00e0 ch\u00ednh l\u00e0 150.\n* LSTM: Layer long-short term memory v\u1edbi s\u1ed1 unit l\u00e0 64, s\u1ed1 l\u01b0\u1ee3ng unit s\u1ebd \u0111\u1ecbnh h\u00ecnh k\u00edch th\u01b0\u1edbc ma tr\u1eadn \u1edf layer Dense k\u1ebf ti\u1ebfp. V\u1ec1 b\u1ea3n ch\u1ea5t layer long-short term memory l\u00e0 m\u1ed9t c\u1ea5u tr\u00fac m\u1ea1ng c\u00f3 kh\u1ea3 n\u0103ng tu\u1ea7n ho\u00e0n. T\u1eeb m\u1ed9t \u0111\u1ea7u v\u00e0o $x_{t}$ th\u00f4ng qua nh\u1eefng bi\u1ebfn \u0111\u1ed5i tuy\u1ebfn t\u00ednh v\u00e0 phi tuy\u1ebfn, m\u1ea1ng s\u1ebd t\u1ea1o ra \u0111\u1ea7u ra l\u00e0 m\u1ed9t nh\u00e2n t\u1ed1 \u1ea9n $h(t)$ v\u00e0 nh\u00e2n t\u1ed1 n\u00e0y ti\u1ebfp t\u1ee5c \u0111\u01b0\u1ee3c s\u1eed d\u1ee5ng l\u00e0 nguy\u00ean li\u1ec7u \u0111\u1ea7u v\u00e0o cho m\u1ed9t c\u1ea5u tr\u00fac m\u1ea1ng ti\u1ebfp theo. D\u1ea1ng tr\u1ea3i d\u00e0i t\u1ed5ng qu\u00e1t c\u1ee7a LSTM c\u00f3 d\u1ea1ng nh\u01b0 b\u00ean d\u01b0\u1edbi:\n\n![LSTM tr\u1ea3i d\u00e0i](https:\/\/cdn-images-1.medium.com\/max\/1000\/1*xTKE0g6XNMLM8IQ4aFdP0w.png)\n\n\u1ede \u0111\u00e2y $A$ t\u01b0\u1ee3ng tr\u01b0ng cho m\u1ed9t c\u1ea5u tr\u00fac m\u1ea1ng tu\u1ea7n ho\u00e0n c\u00f3 kh\u1ea3 n\u0103ng thu nh\u1eadn th\u00f4ng tin t\u1eeb tr\u1ea1ng th\u00e1i tr\u01b0\u1edbc \u0111\u00f3. Chi ti\u1ebft v\u1ec1 c\u1ea5u tr\u00fac m\u1ea1ng LSTM s\u1ebd kh\u00f4ng \u0111i qu\u00e1 s\u00e2u trong b\u00e0i vi\u1ebft n\u00e0y. B\u1ea1n \u0111\u1ecdc c\u00f3 th\u1ec3 tham kh\u1ea3o th\u00eam t\u00e0i li\u1ec7u gi\u1ea3i th\u00edch v\u1ec1 [LSTM](http:\/\/colah.github.io\/posts\/2015-08-Understanding-LSTMs\/)\n\nTh\u00f4ng th\u01b0\u1eddng Layer LSTM \u0111\u01b0\u1ee3c s\u1eed d\u1ee5ng ngay sau layer embedding nh\u01b0 s\u01a1 \u0111\u1ed3 sau:\n\n![LSTM layer](https:\/\/cdn-images-1.medium.com\/max\/1000\/1*GsT2fwE_39fgmtC735924Q.png)\n\n* Dense: Layer fully connected v\u1edbi s\u1ed1 unit l\u00e0 256. \n* Dropout: Layer \u0111\u01b0\u1ee3c th\u00eam v\u00e0o v\u1edbi x\u00e1c xu\u1ea5t t\u1eaft m\u1edf c\u00e1c units c\u1ee7a layer tr\u01b0\u1edbc l\u00e0 0.5 nh\u1eb1m m\u1ee5c \u0111\u00edch gi\u1ea3m overfiting.\n* Dense: Layer fully connected k\u1ebft n\u1ed1i v\u1edbi 10 units \u0111\u1ea7u ra. M\u1ed7i unit t\u01b0\u1ee3ng tr\u01b0ng cho m\u1ed9t class topic.\n* Activation: Layer k\u00edch ho\u1ea1t \u0111\u01b0\u1ee3c s\u1eed d\u1ee5ng \u0111\u1ec3 t\u00ednh x\u00e1c xu\u1ea5t d\u1ef1a tr\u00ean h\u00e0m softmax cho m\u1ed7i class topic.\n\nTa c\u00f3 th\u1ec3 xem ki\u1ebfn tr\u00fac c\u1ee7a model th\u00f4ng qua h\u00e0m `summary()` \u1edf b\u00ean d\u01b0\u1edbi."}}