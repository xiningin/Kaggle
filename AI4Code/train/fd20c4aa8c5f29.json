{"cell_type":{"14a943cc":"code","a7a93850":"code","0253670a":"code","72e6dae1":"code","6b875185":"code","8a7ee8b2":"code","faeb16e5":"code","d3cc7595":"code","398807dc":"code","3fc85acb":"code","641d4fb6":"code","fe6f1f74":"code","d312a5f2":"code","567f46ea":"code","3927f60c":"code","f62ad51c":"code","e71ab040":"code","553e4f83":"code","28768a30":"code","eb06c2fd":"code","23191322":"code","a4b71d0b":"code","9497f8ce":"code","faa606c9":"code","5db8c83b":"markdown","c00257d9":"markdown","dfe46e10":"markdown","515aa8c4":"markdown"},"source":{"14a943cc":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","a7a93850":"#Load the dataset with pandas command\ndbdt = pd.read_csv('..\/input\/pima-indians-diabetes-database\/diabetes.csv')\n\n#Print the first 5 rows of the dataframe.\ndbdt.head()","0253670a":"#print the information on the dataframe providing the details like all columns, their datatypes\ndbdt.info()\n\n#print the statistics of the data frame\ndbdt.describe()","72e6dae1":"#create a copy if the dataframe\ndbdt_cp=dbdt.copy(deep=True)\n\n#copy the named columns data again back in their place after replacing zeros with null values \ndbdt_cp[['Glucose','BloodPressure','SkinThickness','Insulin','BMI']]=dbdt_cp[['Glucose','BloodPressure','SkinThickness','Insulin','BMI']].replace(0,np.NaN)","6b875185":"#replace the null values in the named column cells with respective mean values\n\ndbdt_cp['Glucose'].fillna(dbdt_cp['Glucose'].mean(),inplace=True)\ndbdt_cp['BloodPressure'].fillna(dbdt_cp['BloodPressure'].mean(),inplace=True)\ndbdt_cp['SkinThickness'].fillna(dbdt_cp['SkinThickness'].mean(),inplace=True)\ndbdt_cp['Insulin'].fillna(dbdt_cp['Insulin'].mean(),inplace=True)\ndbdt_cp['BMI'].fillna(dbdt_cp['BMI'].mean(),inplace=True)\n\n#print the statistics after updating the values\ndbdt_cp.describe()","8a7ee8b2":"#prints the pair plot between all the features and labels. Pair plot between features to features or features to label indicates if there exisists a correlation. Example: There is a correlation between skin thinkness and BMI. It could be because they both are affected by another features.  \np=sns.pairplot(dbdt_cp, hue = 'Outcome')","faeb16e5":"#plot the heatmap showing the correlation between the features and label. This indicates varied degree of correlation. 1 being the perfect correlation and 0 being no correlation at all. Example BMI to skin thickness indiates a correlation of 0.54. Diagonal values 1 can be ignored as its teh correlation between the same variables.\nplt.figure(figsize=(12,10))\np=sns.heatmap(dbdt_cp.corr(), annot=True,cmap ='RdYlGn')","d3cc7595":"#Leaving the outcome column all others are scaled back to same axis using (x-u\/sigma) formula. This will help make the features comparable\nfrom sklearn.preprocessing import StandardScaler\nsc_X = StandardScaler()\n\n#X is created from the features column. This represents all the input values.\nX =  pd.DataFrame(sc_X.fit_transform(dbdt_cp.drop([\"Outcome\"],axis = 1),),\n        columns=['Pregnancies', 'Glucose', 'BloodPressure', 'SkinThickness', 'Insulin',\n       'BMI', 'DiabetesPedigreeFunction', 'Age'])\n\n#y is created from the label column. This represents the output value.\ny=dbdt_cp[\"Outcome\"]","398807dc":"#importing train_test_split\nfrom sklearn.model_selection import train_test_split\n\n#Divide the data into test and training sets including features and labels. Test data size is 33% and Training data is 67%. \nX_train,X_test,y_train,y_test = train_test_split(X,y,test_size=1\/3,random_state=42, stratify=y)","3fc85acb":"#Print and check the top 5 rows of train and test data features\nX_train.head()\nX_test.head()","641d4fb6":"#Print and check the top 5 rows of train and test data label\ny_train.head()\ny_test.head()","fe6f1f74":"from sklearn.neighbors import KNeighborsClassifier\n\ntest_scores = []\ntrain_scores = []\n\nfor i in range(1,20):\n\n    knn = KNeighborsClassifier(i)\n    knn.fit(X_train,y_train)\n    \n    train_scores.append(knn.score(X_train,y_train))\n    test_scores.append(knn.score(X_test,y_test))","d312a5f2":"## checking for the best score among training data scores and printing the same.\nmax_train_score = max(train_scores)\ntrain_scores_ind = [i for i, v in enumerate(train_scores) if v == max_train_score]\nprint('Max train score {} % and k = {}'.format(max_train_score*100,list(map(lambda x: x+1, train_scores_ind))))","567f46ea":"## checking for the best score among test data scores and printing the same.\nmax_test_score = max(test_scores)\ntest_scores_ind = [i for i, v in enumerate(test_scores) if v == max_test_score]\nprint('Max test score {} % and k = {}'.format(max_test_score*100,list(map(lambda x: x+1, test_scores_ind))))","3927f60c":"#Plotting the test and train data based scores for KNN selection. It shows how the accuracy values are varying. We can observe the max scores here.\nplt.figure(figsize=(12,5))\np = sns.lineplot(range(1,20),train_scores,marker='*',label='Train Score')\np = sns.lineplot(range(1,20),test_scores,marker='o',label='Test Score')","f62ad51c":"#Setup a knn classifier with k neighbors. Here we can enter the KNN number and read the test score.\nknn = KNeighborsClassifier(11)\n\nknn.fit(X_train,y_train)\nknn.score(X_test,y_test)","e71ab040":"#import confusion_matrix. Confusion matrix shows the accuracy, \nfrom sklearn.metrics import confusion_matrix\n#let us get the predictions using the classifier we had fit above\ny_pred = knn.predict(X_test)\nconfusion_matrix(y_test,y_pred)\npd.crosstab(y_test, y_pred, rownames=['True'], colnames=['Predicted'], margins=True)","553e4f83":"y_pred = knn.predict(X_test)\nfrom sklearn import metrics\ncnf_matrix = metrics.confusion_matrix(y_test, y_pred)\np = sns.heatmap(pd.DataFrame(cnf_matrix), annot=True, cmap=\"YlGnBu\" ,fmt='g')\nplt.title('Confusion matrix', y=1.1)\nplt.ylabel('Actual label')\nplt.xlabel('Predicted label')","28768a30":"#import classification_report\nfrom sklearn.metrics import classification_report\nprint(classification_report(y_test,y_pred))","eb06c2fd":"#import GridSearchCV and we are doing a grid search of best KNN option. This is another option to explore. different CV value here shows different n options as the solution.\nfrom sklearn.model_selection import GridSearchCV\n#In case of classifier like knn the parameter to be tuned is n_neighbors\nparam_grid = {'n_neighbors':np.arange(1,15)}\nknn = KNeighborsClassifier()\nknn_cv= GridSearchCV(knn,param_grid,cv=3)\nknn_cv.fit(X,y)\n\nprint(\"Best Score:\" + str(knn_cv.best_score_))\nprint(\"Best Parameters: \" + str(knn_cv.best_params_))","23191322":"#Import Logistic regression and try that as solutition for the problem. \nfrom sklearn.linear_model import LogisticRegression\nclf = LogisticRegression(random_state=0).fit(X_train, y_train)","a4b71d0b":"print(clf.score(X_test, y_test))\nprint(clf.score(X_train, y_train))","9497f8ce":"#Here y-pred provides us with the array of the predicted label values vased on the Logistic regress. We can use thsi to compare with actual y-values for test data.\ny_pred = clf.predict(X_test)\nprint(y_pred)","faa606c9":"#Here we implement support vector machine algorithm. SVM provides is the option to implement a linear or a polynomial\/ sigmoid or Gaussian fits to classify the data. \n#This could be helpful in some cases based on the specific conditions. Polynomial also has options to set various degress of orders for the equations. \n#In this case however, KNN seems to prvide the best solution for teh problem.\nfrom sklearn.svm import SVC\n#svclassifier = SVC(kernel='poly',degree=10)\nsvclassifier = SVC(kernel='sigmoid')#we kernel can be linear, gausian(rbf), sigmoid or non-linear('poly') with various orders\/degrees of polynominals\nsvclassifier.fit(X_train, y_train)\ny_pred = svclassifier.predict(X_test)\nprint(svclassifier.score(X_test,y_test))\nprint(svclassifier.score(X_train,y_train))\nfrom sklearn.metrics import classification_report, confusion_matrix\nprint(confusion_matrix(y_test,y_pred))\nprint(classification_report(y_test,y_pred))","5db8c83b":"Logistic Regression comes up with a lower accuracy than the KNN=11 option. Indicates that the KNN is teh best fit of these two. In the next steps we will try other classifier algorithms as well.","c00257d9":"#Features Glucose, BloodPressure, SkinThickness, Insulin and BMI cannot have 0 as the minimum value in them.This indicates that the values are missing and have been replaced by 0. Before we can process the data, we need to replace these 0 values with valid numbers. Here we will replace the numbers with the column means since that doesnt alter the data statistics for the columns. ","dfe46e10":"# #Implementation of KNN classifier algorithm. \n*I am considering upto 20 Neighbours. So code is checking the score for each of these and saving it as part of an array which is being checked for the best score in the next steps. *****","515aa8c4":"**Here we tabulate the confusion matrix. Confusion Matrix gives is the accuracy, precision (psitive prediction), Sensitivity\/Recall. Accuracy gives the ratio of total correct predictions to total values. Precision gives the the proportion of positive cases that were correctly identified. Recall the proportion of actual positive cases which are correctly identified. F1-Score is the harmonic mean of precision and recall.****"}}