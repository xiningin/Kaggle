{"cell_type":{"ae3f5396":"code","bbd28af7":"code","102471e0":"code","a1b7f15d":"code","f35bc08b":"code","a3fe9083":"code","a4e88ebc":"code","23cfd20b":"code","08185776":"markdown","4d6814e5":"markdown","e69a74ad":"markdown","66f67315":"markdown","4771d415":"markdown"},"source":{"ae3f5396":"#Import\nimport numpy as np\nimport pandas as pd\nimport itertools\nimport tensorflow as tf\nnp.random.seed(1239)","bbd28af7":"def sigmoid(x):\n    return 1\/(1+np.exp(-x))","102471e0":"#First we generate the test data\n\n#The synthetic question:\nsynthetic_questions = np.arange(-1.9, 3.1, 1)\nsynthetic_students = np.arange(0,2,0.1)\nsynthetic_logits = synthetic_students.reshape(-1,1) - synthetic_questions.reshape(1,-1)\nsynthetic_probs = sigmoid(synthetic_logits)\nsynthetic_data = (synthetic_probs > np.random.rand(synthetic_probs.shape[0],synthetic_probs.shape[1])).astype('float')","a1b7f15d":"\nsynthetic_data","f35bc08b":"data_shape = synthetic_data.shape\nlearning_rate = 0.1\ntf.reset_default_graph()\nX = tf.placeholder(dtype='float' ,shape=data_shape, name=\"X\")\nalpha = tf.Variable(initial_value=np.zeros((data_shape[0],1)), name=\"alpha\", dtype='float')\ndelta = tf.Variable(initial_value=np.zeros((1,data_shape[1])), name=\"delta\", dtype='float')\nlog_likelihood = tf.reduce_sum(X * tf.log(tf.sigmoid(alpha-delta)) + (1-X) * tf.log(1-tf.sigmoid(alpha-delta)))\ncost = -log_likelihood\noptimizer = tf.train.GradientDescentOptimizer(learning_rate)\ntraining_op = optimizer.minimize(cost)","a3fe9083":"init = tf.global_variables_initializer()\nn_epochs = 4000\n\n\nwith tf.Session() as sess:\n    sess.run(init)\n\n    for epoch in range(n_epochs):\n        if epoch % 1000 == 0:\n            print(\"Epoch\", epoch, \"Cost =\", cost.eval(feed_dict={X: synthetic_data}))\n        sess.run(training_op, feed_dict={X: synthetic_data})\n    \n    best_alpha = alpha.eval()\n    best_delta = delta.eval()","a4e88ebc":"best_alpha","23cfd20b":"best_delta","08185776":"It got the questions in the right order, and the students are also roughly in the right order, but are affected by chance.\n\nOne of the improvements of this model would be to add priors for $\\alpha$ and $\\delta$, which will cause regularization and the smoothing of both student ability scores and the question difficulty score.","4d6814e5":"The log likelihood is then:\n\n$$L \\equiv log(X|\\alpha, \\beta) = \\sum_{q,s} { x_{qs} log \\sigma(\\alpha_s - \\delta_q) + \n(1 - x_{qs}) log (1 - \\sigma(\\alpha_s - \\delta_q))}$$\n","e69a74ad":"# The Rasch Model #\n\nThis notebook demonstrates implementation of the Rasch model in TensorFlow. All theoretical parts are taken from an excellent textbook \"Bayesian Reasoning and Machine Learning\" by David Barber. The Rasch Model is covered in chapter 22 of the book.","66f67315":"And the partial derivatives are:\n\n$$\\frac{\\partial L}{\\partial \\alpha_s} = \\sum_{q=1}^Q(x_{qs} - \\sigma(\\alpha_s - \\delta_q))$$\n\n$$\\frac{\\partial L}{\\partial \\delta_q} = - \\sum_{s=1}^S(x_{qs} - \\sigma(\\alpha_s - \\delta_q))$$\n\nBut since we are going to use TensorFlow, it will calculate the derivatives automatically, so these are just for the information","4771d415":"Consider an exam in which student $s$ answers question $q$ either correctly $x_{qs} = 1$ or incorrectly $x_{qs} = 0$.\nFor a set of $N$ students and $Q$ questions, the performance of all students is given in the $Q \\times N$ binary\nmatrix $X$. Based on this data alone we wish to evaluate the ability of each student, and at the same time estimate difficulty of each question. To learn both, we assign the probability that a student $s$ gets a question $q$ correct based on the student's latent ability $\\alpha_s$ and the latent difficulty of the question $\\delta_q$:\n\n$$p(x_{qs} = 1|\\alpha, \\delta) = \\sigma(\\alpha_s -\\delta_q)$$\nWhere $\\sigma$ is sigmoid function.\n\nMaking the i.i.d. assumption, the likelihood of the data $X$ under this model is:\n\n$$p(X|\\alpha, \\delta) = \\prod_{s=1}^S\\prod_{q=1}^Q \\sigma(\\alpha_s-\\delta_q)^{x_{qs}} (1-\\sigma(\\alpha_s-\\delta_q))^{1-x_{qs}}$$"}}