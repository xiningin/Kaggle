{"cell_type":{"0f997c90":"code","25d1b936":"code","2b46ae1d":"code","ce7d2237":"code","8cb84d53":"code","f4f094d2":"code","1e4bf678":"code","3fd7e80d":"markdown","252b1528":"markdown","18822701":"markdown","699051d1":"markdown","9c40e4d0":"markdown","ffc06c27":"markdown","43fdcf95":"markdown","b26d8b45":"markdown"},"source":{"0f997c90":"import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\n\nimport warnings \nwarnings.filterwarnings('ignore')\n\n# Read Data\ndata = pd.read_csv('..\/input\/melb_data.csv')\ncols_to_use = ['Rooms', 'Distance', 'Landsize', 'BuildingArea', 'YearBuilt']\nX = np.array(data[cols_to_use])\ny = data.Price\ntrain_X, val_X, train_y, val_y = train_test_split(X, y)\n","25d1b936":"from xgboost import XGBRegressor\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import Imputer\n\nmy_pipeline = Pipeline([('imputer', Imputer()), ('xgbrg', XGBRegressor())])","2b46ae1d":"from sklearn.model_selection import GridSearchCV\n\nparam_grid = {\n    \"xgbrg__n_estimators\": [10, 50, 100, 500],\n    \"xgbrg__learning_rate\": [0.1, 0.5, 1],\n}\n\nfit_params = {\"xgbrg__eval_set\": [(val_X, val_y)], \n              \"xgbrg__early_stopping_rounds\": 10, \n              \"xgbrg__verbose\": False} \n\nsearchCV = GridSearchCV(my_pipeline, cv=5, param_grid=param_grid, fit_params=fit_params)\nsearchCV.fit(train_X, train_y)  ","ce7d2237":"searchCV.best_params_ ","8cb84d53":"searchCV.cv_results_['mean_train_score']","f4f094d2":"searchCV.cv_results_['mean_test_score']","1e4bf678":"searchCV.cv_results_['mean_train_score'].mean(), searchCV.cv_results_['mean_test_score'].mean()","3fd7e80d":"We use `Imputer` to fill in missing values, followed by a `XGBRegressor` to make predictions.  These can be bundled together in a pipeline as shown below.","252b1528":"The above code is similar to the [Pipeline tutorial](https:\/\/www.kaggle.com\/dansbecker\/pipelines) except that here we use `Pipeline` instead of `make_pipeline` because we want to have a name for every step in our pipeline so that we can call on a step and set parameters.","18822701":"As explained in [Learning to Use XGBoost tutorial](https:\/\/www.kaggle.com\/dansbecker\/learning-to-use-xgboost), the number of trees in XGBoost models, that is `n_estimators`, are tuned by using `early_stopping_rounds`. The early stopping is decided by checking the prediction of the trained models on a validation set, and hence it is required that we pass an `eval_set` alongside the `early_stopping_rounds` in the `fit_params`.","699051d1":"# GridSearchCV for hyper-parameters tuning XGBoost models using Pipelines ","9c40e4d0":"### Building a pipeline","ffc06c27":"`GridSearchCV` is a cross validation technique for tuning a model. Similar to `cross_val_score`, it uses the cross validation process explained in the [Cross-Validation tutorial](https:\/\/www.kaggle.com\/dansbecker\/cross-validation). However, the main objective of `GridSearchCV` is to find the most optimal parameters. We pass on a parameters' dictionary to the function and the function compares the cross-validation score for each combination of parameters in the dictionary and returns the set of best parameters. The tutorial specifically covers `XGBBoost` since they are very sensitive to hyperparameters' tuning and here we demonstrate how to use early stopping rounds in `GridSearchCV`.\n\n\nWe won't focus on the data loading. For now, you can imagine you are at a point where you already have train_X, val_X, train_y, and val_y. ","43fdcf95":"More to come!","b26d8b45":"### GridSearchCV for tuning the model\n\nHere we use `GridSearchCV` on our pipeline and train it on our training set. We are using 5-fold cross validation by passing the argument `cv=5` in the `GridSearchCV`. The `param_grid` is the dictionary for the values of parameters that we want to compare. Using `GridSearchCV` on a pipeline is very similar to use it on a regressor\/classifier, except that we add the name of the regression (here `xgbrg__`) in front of the parameters' names in the `param_grid`. "}}