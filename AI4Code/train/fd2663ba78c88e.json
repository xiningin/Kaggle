{"cell_type":{"3c5eaa98":"code","c54c3225":"code","d7fc4a86":"code","955cca10":"code","2713a359":"code","69bf22fa":"code","c4f88b4b":"code","82dc0aee":"code","c88188cc":"code","e83296f5":"code","fb114c61":"code","eec14077":"code","3ae8f5a4":"code","3a9ad04f":"code","e0f8025f":"code","3f1ca9ec":"code","9b482236":"code","ad5a1c87":"code","f17b7311":"code","05d784a5":"code","99390f40":"code","336a2104":"code","8f86b0e5":"code","72f42111":"code","4ba04d09":"code","cc0a851d":"code","e0ef9391":"code","aecda884":"code","57693ae1":"code","0ad4eb38":"code","ad61d128":"code","0a3454db":"code","6bed4585":"code","f6b9ecc6":"code","b5e1677f":"markdown","329de6bd":"markdown","4939da33":"markdown","f841bf34":"markdown","f41b2991":"markdown","b4ea6135":"markdown","bcd82a0e":"markdown","a8c43932":"markdown","78098265":"markdown","2b3dc146":"markdown","24054ba5":"markdown","bdf8ac11":"markdown","009cf9de":"markdown","3bd73d19":"markdown"},"source":{"3c5eaa98":"import tweepy\nfrom textblob import TextBlob","c54c3225":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport re\nimport nltk\nnltk.download('stopwords')\nfrom nltk.corpus import stopwords\nfrom nltk.stem.porter import PorterStemmer\nfrom wordcloud import WordCloud\nimport  json\nfrom collections import Counter","d7fc4a86":"df = pd.read_csv('..\/input\/pfizer-vaccine-tweets\/vaccination_tweets.csv')","955cca10":"df.replace(False,0,inplace=True)\ndf.replace(True,1,inplace=True)","2713a359":"df.sample(2)","69bf22fa":"df = df[['id','text','user_verified']]\ndf.head()","c4f88b4b":"def remove_pattern(text,pattern):\n    \n    # finding the pattern\n    r = re.findall(pattern ,text)\n    \n    for i in r:\n        text = re.sub(i,'',text)\n        \n    return text    ","82dc0aee":"df['tidy_tweets'] = np.vectorize(remove_pattern)(df['text'],'@[\\w]*')\n\ndf.head(10)","c88188cc":"df['tidy_tweets'] = df['tidy_tweets'].str.replace('[^a-zA-Z#]',' ')\ndf.head(10)","e83296f5":"df['tidy_tweets'] = df['tidy_tweets'].apply(lambda x: ' '.join([w for w in x.split() if len(w)>3]))\ndf.head(10)","fb114c61":"tokined_tweet = df['tidy_tweets'].apply(lambda x: x.split())\ntokined_tweet.head(10)","eec14077":"from nltk import PorterStemmer\n\nps = PorterStemmer()\n\ntokenized_tweet = tokined_tweet.apply(lambda x: [ps.stem(i) for i in x])\n\ntokenized_tweet.head(10)","3ae8f5a4":"for i in range(len(tokenized_tweet)):\n    tokenized_tweet[i] = ''.join(tokenized_tweet[i])\n    \n    \ndf['tidy_tweets'] = tokenized_tweet\ndf.head()","3a9ad04f":"from wordcloud import WordCloud, ImageColorGenerator\nfrom PIL import Image\nimport urllib\nimport requests","e0f8025f":"non_verified_user = ''.join(text for text in df['tidy_tweets'][combine['user_verified']==0])","3f1ca9ec":"# combine the image with dataset\nMask = np.array(Image.open(requests.get('http:\/\/clipart-library.com\/image_gallery2\/Twitter-PNG-Image.png', stream=True).raw))\n\nimage_colors = ImageColorGenerator(Mask)\n\nwc = WordCloud(background_color='black', height=1500, width=5000, mask=Mask).generate(non_verified_user)","9b482236":"plt.figure(figsize=(10,20))\n\nplt.imshow(wc.recolor(color_func=image_colors), interpolation='hamming')\n\nplt.axis('off')\nplt.show()","ad5a1c87":"verified_user = ''.join(text for text in df['tidy_tweets'][combine['user_verified']==1])","f17b7311":"# combine the image with dataset\nMask = np.array(Image.open(requests.get('http:\/\/clipart-library.com\/image_gallery2\/Twitter-PNG-Image.png', stream=True).raw))\n\nimage_colors = ImageColorGenerator(Mask)\n\nwc = WordCloud(background_color='black', height=1500, width=4000, mask=Mask).generate(verified_user)","05d784a5":"plt.figure(figsize=(10,20))\n\nplt.imshow(wc.recolor(color_func=image_colors), interpolation='hamming')\n\nplt.axis('off')\nplt.show()","99390f40":"def Hastags_Extract(x):\n    hashtags = []\n    \n    for i in x:\n        ht = re.findall(r'#(\\w+)',i)\n        hashtags.append(ht)\n        \n    return hashtags    ","336a2104":"ht_positive = Hastags_Extract(df['tidy_tweets'][df['user_verified']==1])\n\nht_positive","8f86b0e5":"ht_positive_unnest = sum(ht_positive,[])","72f42111":"ht_positive_unnest","4ba04d09":"ht_negative = Hastags_Extract(df['tidy_tweets'][df['user_verified']==0])\n\nht_negative","cc0a851d":"ht_negative_unnest = sum(ht_negative,[])\nht_negative_unnest\n","e0ef9391":"word_freq_positive = nltk.FreqDist(ht_positive_unnest)\n\nword_freq_positive","aecda884":"df_positive  = pd.DataFrame({'Hashtags':list(word_freq_positive.keys()),'Count':list(word_freq_positive.values())})\ndf_positive.head(10)","57693ae1":"import seaborn as sns\ndf_positive_plot = df_positive.nlargest(20,columns='Count')\n\nsns.barplot(data = df_positive_plot, y='Hashtags',x='Count')\nsns.despine()","0ad4eb38":"word_freq_negative = nltk.FreqDist(ht_negative_unnest)\n\nword_freq_negative\n\ndf_negative  = pd.DataFrame({'Hashtags':list(word_freq_negative.keys()),'Count':list(word_freq_negative.values())})\ndf_negative.head(10)","ad61d128":"df_negative_plot = df_negative.nlargest(20,columns='Count')\n\nsns.barplot(data = df_negative_plot, y='Hashtags',x='Count')\nsns.despine()","0a3454db":"from sklearn.feature_extraction.text import CountVectorizer\n\nbow_vectorizer = CountVectorizer(max_df = 0.90, min_df = 2, max_features=325, stop_words='english')\n\nbow = bow_vectorizer.fit_transform(combine['tidy_tweets'])\ndf_bow = pd.DataFrame(bow.todense())\ndf_bow","6bed4585":"train_bow = bow\ntrain_bow.todense()","f6b9ecc6":"# Refrences\n# Towards Data Science Newsletter\n","b5e1677f":"# Barplot for the 20 most frequent words used for hashtags","329de6bd":"# Impact of Hashtag ","4939da33":"# Removing Twitter Handles(@User)","f841bf34":"# Similarly for non_verified_user","f41b2991":"# Stemming\n\nStemmming is a rule based process of stripping the suffixes('ing','es' etc.)from a word\n\nfi=or example - 'play','player','played','plays','playing'are different variation of 'play'","b4ea6135":"# Removing Short Words ","bcd82a0e":"# Using CountVectorizer","a8c43932":"*Switching these tokens back together*","78098265":"# Counting Frequency of words by verified_user","2b3dc146":"# Tokenization\n*Tokenization is the process of splitting a string of text into tokens*","24054ba5":"# List Unnestting","bdf8ac11":"# A nested list of all hastags from non_verified_user","009cf9de":"# Dataframe of most frequently words by verified_user","3bd73d19":"# Removing Punctuation, Numbers, and Special Characters"}}