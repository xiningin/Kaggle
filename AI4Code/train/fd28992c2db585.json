{"cell_type":{"ac1d62d7":"code","dfcf3fbc":"code","9f1dbbbe":"code","8c256be3":"code","2f4fbfbf":"code","640caae6":"code","e5e9cf62":"code","c3be6923":"code","35f53360":"code","07a04294":"code","ccf333c8":"code","2c1d6f82":"code","6c2fa3bb":"code","df303a32":"code","fa7d35d1":"code","95a369fa":"code","35dc5713":"code","1b03b9c9":"code","fededb19":"code","4e682172":"code","7a8e4366":"code","0127821e":"code","e3c62746":"code","b3694fad":"code","e910ae39":"code","7eb9843b":"code","c939cb64":"code","72e03734":"code","268f413a":"code","9c668db9":"code","aed76059":"code","59125b4b":"code","eb16c504":"code","9964e31c":"code","e4cbb9ed":"code","aafdb9f0":"code","e06091db":"code","46ba932b":"code","8ed56b92":"code","b121c541":"code","d5b50fc4":"code","ad3a9cad":"code","db357284":"code","065a58ae":"code","dc672e21":"code","e2d07074":"code","7404da27":"code","64ed572f":"code","158c7e0b":"code","b4609c8d":"code","3dd08edf":"code","bb1b0453":"markdown","3da168b5":"markdown","42d6fe3b":"markdown","7fb0947b":"markdown","9edc450f":"markdown","1068cdfc":"markdown","f186a3b3":"markdown","98c8204e":"markdown","cb4d9fbd":"markdown","e785a94a":"markdown","c87f7c7d":"markdown","d0686873":"markdown","50dddbf0":"markdown","1e86c98c":"markdown","cdb11915":"markdown","b8dff919":"markdown","bd100def":"markdown","d0cb37d0":"markdown","c863a5cd":"markdown"},"source":{"ac1d62d7":"%pip install 'git+https:\/\/github.com\/facebookresearch\/detectron2.git'\n#!pip install detectron2 -f https:\/\/dl.fbaipublicfiles.com\/detectron2\/wheels\/cu110\/torch1.7\/index.html\n#!python -m pip install detectron2==0.3 -f https:\/\/dl.fbaipublicfiles.com\/detectron2\/wheels\/cu110\/torch1.7\/index.html","dfcf3fbc":"#!pip install albumentations==0.5.2 pyyaml==5.1","9f1dbbbe":"!mkdir models logs configs ","8c256be3":"!nvidia-smi","2f4fbfbf":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom typing import List, Optional, Union\nfrom detectron2.data.transforms import Transform as T\nimport torch\nimport torchvision\nimport albumentations as A\n#from albumentations.augmentations.transforms import Sharpen\n#from albumentations.pytorch.transforms import ToTensorV2\n\n\nimport detectron2\nfrom detectron2.data.datasets import register_coco_instances\nfrom detectron2.data import transforms as T\nfrom detectron2 import model_zoo\nfrom detectron2.engine import DefaultPredictor\nfrom detectron2.config import get_cfg\nfrom detectron2.utils.visualizer import Visualizer\nfrom detectron2.data import MetadataCatalog, DatasetCatalog, DatasetMapper, build_detection_test_loader , build_detection_train_loader\nfrom detectron2.engine import DefaultTrainer\nfrom detectron2.evaluation import COCOEvaluator, inference_on_dataset\nfrom detectron2.data import detection_utils as utils\nfrom detectron2.config import configurable\nfrom detectron2.engine.hooks import EvalHook\nfrom detectron2.modeling import build_model\nfrom detectron2.checkpoint import DetectionCheckpointer\nfrom detectron2.utils.logger import setup_logger\nsetup_logger()\n\nfrom sklearn.model_selection import KFold,StratifiedKFold,StratifiedShuffleSplit,GroupKFold\n\nimport matplotlib.pyplot as plt\nimport cv2\nimport json\nimport io\nimport os\nimport copy\nimport random\nfrom IPython.display import FileLink, FileLinks\nimport yaml\nfrom abc import ABC,ABCMeta, abstractmethod","640caae6":"class Config(metaclass=ABCMeta):\n    general={\n        \"DATASET_PATH\" : \"..\/input\/tacotrashdataset\",\n        \"LOGS_PATH\" : \"logs\",\n        \"MODELS_PATH\" : \"models\",\n        \"CONFIG_PATH\" : \"configs\",\n        \"seed\" : 42,\n        \"n_folds\" : 5,\n        \"tool\" : \"detectron2\",\n        \"experiment_id\" : \"24-03-2021\",\n        \"category\" : \"super_category\",\n        \"augmentations\" : True,\n        \"TTA\" : False\n        \n    }\n    model={\n        \"base_lr\" : 0.001,\n        \"num_classes\" : 29, #29 if super category 60 if normal category \n        \"model_name\" : \"faster_rcnn_R_101_FPN_3x\",\n        \"batchsize_per_image\" : 1024,\n        \"images_per_batch\" : 4,\n        \"epochs\" : 9,\n    }\n\n    @staticmethod\n    def dump(config,path=\"config.yaml\"):\n        stream = open(path, 'w')\n        yaml.dump(config,stream)\n        stream.close()\n    \n    @staticmethod\n    def load(path=\"config.yaml\"):\n        stream = open(path, 'r')\n        config=yaml.load(stream)\n        stream.close()\n        return config\n\nglobal_config=Config()\nc=global_config\ndef inject_config(funct):\n    def function_wrapper(*args,**kwargs):\n        return funct(global_config,*args,**kwargs)  \n    return function_wrapper\n    ","e5e9cf62":"Config.dump(global_config.general,os.path.join(global_config.general[\"CONFIG_PATH\"],\"global_config.yaml\"))\nConfig.dump(global_config.model,os.path.join(global_config.general[\"CONFIG_PATH\"],\"model_config.yaml\"))","c3be6923":"@inject_config\ndef seed_all(config):\n    seed_value=config.general[\"seed\"]\n    random.seed(seed_value) # Python\n    np.random.seed(seed_value) # cpu vars\n    torch.manual_seed(seed_value) # cpu  vars\n    os.environ['PYTHONHASHSEED'] = str(seed_value)\n    \n    if torch.cuda.is_available(): \n        torch.cuda.manual_seed(seed_value)\n        torch.cuda.manual_seed_all(seed_value) # gpu vars\n        torch.backends.cudnn.deterministic = True  #needed\n        torch.backends.cudnn.benchmark = False","35f53360":"annot=json.load(open(os.path.join(global_config.general[\"DATASET_PATH\"],\"data\/annotations.json\")))\nannot[\"annotations\"][308][\"id\"]=0\nannot[\"annotations\"][4039][\"id\"]=2197","07a04294":"annot_to_delete=[]\nfor idx,annotation in enumerate(annot[\"annotations\"]):\n    if (annotation[\"bbox\"][0]<0 or annotation[\"bbox\"][1]<0 or\n        annotation[\"bbox\"][2]<0 or annotation[\"bbox\"][3]<0):\n        annot_to_delete.append(idx)\nfor pos,idx in enumerate(annot_to_delete):\n    del annot[\"annotations\"][idx-pos]\n","ccf333c8":"json.dump(annot,open(\"new_annotations.json\",\"w\"))","2c1d6f82":"categories={ annotation[\"id\"] : annotation[\"name\"] for annotation in annot[\"categories\"]}\nsuper_categories={ annotation[\"id\"] : annotation[\"supercategory\"] for annotation in annot[\"categories\"]}","6c2fa3bb":"annot_df=pd.DataFrame(annot[\"annotations\"])\nimages_df=pd.DataFrame(annot[\"images\"])","df303a32":"images_df.describe()","fa7d35d1":"\"\"\"\nfor code testing\n\"\"\"\n#annot_df=annot_df.head(200)","95a369fa":"annot_df[\"category\"]=annot_df[\"category_id\"].apply(lambda value : categories[value])\nannot_df[\"super_category\"]=annot_df[\"category_id\"].apply(lambda value : super_categories[value])\nsuper_category_to_index={value : key for key,value in enumerate(annot_df[\"super_category\"].unique())}\nannot_df[\"super_category_id\"]=annot_df[\"super_category\"].apply(lambda value : super_category_to_index[value])\nannot_df[\"normal_category_id\"]=annot_df[\"category_id\"]\nannot_df[\"normal_category\"]=annot_df[\"category\"]\nif c.general[\"category\"] != \"normal_category\":\n    annot_df[\"category_id\"]=annot_df[\"super_category_id\"]\n    annot_df[\"category\"]=annot_df[\"super_category\"]\n    annot_cat=annot_df.groupby(\"category_id\")[[\"category_id\",\"category\",\"super_category\"]].first()\n    annot_cat.columns=[\"id\",\"name\",\"supercategory\"]\n    annot[\"categories\"]=annot_cat.to_dict(\"records\")\n\n                       ","35dc5713":"annot_df.head()","1b03b9c9":"annot_df[\"category\"].value_counts().head(10).plot(kind=\"bar\",title=\"annotations category distribution\")","fededb19":"annot_df[\"super_category\"].value_counts().head(10).plot(kind=\"bar\",title=\"annotations super category distribution\")","4e682172":"@inject_config\ndef kfold_split(config):\n    seed_all()\n    annot_df[\"folds\"]=-1\n    kf = GroupKFold(n_splits=config.general[\"n_folds\"])\n    #kf = StratifiedShuffleSplit(n_splits=Config.n_folds,test_size=Config.test_size, random_state=Config.random_state, )\n    #kf = StratifiedShuffleSplit(n_splits=10,test_size=0.1, random_state=Config.random_state, )\n\n    for fold, (_, val_index) in enumerate(kf.split(annot_df,groups=annot_df[\"image_id\"])):\n            annot_df.loc[val_index, \"folds\"] = fold","7a8e4366":"kfold_split()","0127821e":"annot_df[annot_df[\"folds\"]==0][\"category\"].value_counts().head(10).plot(kind=\"bar\",title=\"Test annotations category distribution\")","e3c62746":"annot_df[annot_df[\"folds\"]!=0][\"category\"].value_counts().head(10).plot(kind=\"bar\",title=\"Train annotations category distribution\")","b3694fad":"annot_df[annot_df[\"folds\"]==0][\"super_category\"].value_counts().head(10).plot(kind=\"bar\",title=\"Test annotations super category distribution\")","e910ae39":"annot_df[annot_df[\"folds\"]==0][\"super_category\"].value_counts().head(10).plot(kind=\"bar\",title=\"Train annotations super category distribution\")","7eb9843b":"@inject_config\ndef register_dataset(config,fold):\n    train_dataset_name=f\"my_dataset_train_{fold}\"\n    test_dataset_name=f\"my_dataset_test_{fold}\"\n    train_dataset_file=f\"my_dataset_train_{fold}.json\"\n    test_dataset_file=f\"my_dataset_test_{fold}.json\"\n    \n    train_annot_df=annot_df[annot_df[\"folds\"]!=fold]\n    test_annot_df=annot_df[annot_df[\"folds\"]==fold]\n    train_annot_df=train_annot_df.drop([\"normal_category\",\"normal_category_id\"],axis=1)\n    test_annot_df=test_annot_df.drop([\"normal_category\",\"normal_category_id\"],axis=1)\n\n    train_images_df=images_df[images_df[\"id\"].apply(lambda i:True if i in list(train_annot_df[\"image_id\"].unique()) else False)]\n    test_images_df=images_df[images_df[\"id\"].apply(lambda i:True if i in list(test_annot_df[\"image_id\"].unique()) else False)]\n    \n    train_annot=annot.copy()\n    test_annot=annot.copy()\n    \n    train_annot[\"annotations\"]=train_annot_df.reset_index(drop=True).to_dict(\"records\")\n    train_annot[\"images\"]=train_images_df.reset_index(drop=True).to_dict(\"records\")\n    test_annot[\"annotations\"]=test_annot_df.reset_index(drop=True).to_dict(\"records\")\n    test_annot[\"images\"]=test_images_df.reset_index(drop=True).to_dict(\"records\")\n    \n    json.dump(train_annot,open(train_dataset_file,\"w\"))\n    json.dump(test_annot,open(test_dataset_file,\"w\"))\n    \n    if train_dataset_name in DatasetCatalog.list():\n        DatasetCatalog.remove(train_dataset_name)\n        MetadataCatalog.remove(train_dataset_name)\n    if test_dataset_name in DatasetCatalog.list():\n        DatasetCatalog.remove(test_dataset_name)\n        MetadataCatalog.remove(test_dataset_name)\n        \n    register_coco_instances(train_dataset_name, {}, train_dataset_file, os.path.join(config.general[\"DATASET_PATH\"],\"data\"))\n    register_coco_instances(test_dataset_name, {}, test_dataset_file, os.path.join(config.general[\"DATASET_PATH\"],\"data\"))\n","c939cb64":"def mono_to_color(X, mean=None, std=None, norm_max=None, norm_min=None, eps=1e-6,**kwargs):\n    # Stack X as [X,X,X]\n    #print(X.shape)\n    #X = np.squeeze(np.stack([X, X, X], axis=-2),axis=-1)\n    X=X.transpose(1, 0, 2)\n    mean = mean or X.mean()\n    std = std or X.std()\n    Xstd = (X - mean) \/ (std + eps)\n    _min, _max = Xstd.min(), Xstd.max()\n    norm_max = norm_max or _max\n    norm_min = norm_min or _min\n    if (_max - _min) > eps:\n        # Normalize to [0, 255]\n        V = Xstd\n        V[V < norm_min] = norm_min\n        V[V > norm_max] = norm_max\n        V = 255 * (V - norm_min) \/ (norm_max - norm_min)\n        V = V.astype(np.uint8)\n    else:\n        # Just zero\n        V = np.zeros_like(Xstd, dtype=np.uint8)\n    return V\n  \n\nclass ToColor:\n    def __init__(self,\n                  mean=None,\n                  std=None):\n        self.mean=mean\n        self.std = std\n        \n\n    def __call__(self, image,**kwargs):\n        return mono_to_color(image,\n                            self.mean,\n                            self.std,\n                            )","72e03734":"def get_train_transforms():\n    return A.Compose(\n        [\n            #A.RandomSizedCrop(min_max_height=(1024, 1024), height=1024, width=1024, p=0.2),\n            A.OneOf([\n                A.HueSaturationValue(hue_shift_limit=0.4, sat_shift_limit= 0.4, \n                                     val_shift_limit=0.4, p=0.8),\n                A.RandomBrightnessContrast(brightness_limit=0.4, \n                                           contrast_limit=0.4, p=0.8),\n            ],p=0.8),\n            A.Rotate (limit=15, interpolation=1, border_mode=4, value=None, mask_value=None, p=0.8),\n            \n            #A.transforms.Sharpen (alpha=(0.2, 0.5), lightness=(0.5, 1.0), p=0.5),\n            A.HorizontalFlip(p=0.5),\n            A.VerticalFlip(p=0.5),\n            A.RandomResizedCrop (1500, 1500, scale=(0.8, 0.8), ratio=(0.75, 1.3333333333333333), interpolation=1, always_apply=False, p=0.3),\n            A.Resize(height=1500, width=1500, p=1.0),\n            A.CLAHE(p=0.7),\n            #A.Lambda(ToColor(),p=1),\n            #A.Cutout(num_holes=16, max_h_size=64, max_w_size=64, fill_value=0, p=0.1),\n            \n        ], \n        p=1.0, \n        bbox_params=A.BboxParams(\n            format='coco',\n            min_area=0.5, \n            min_visibility=0.5,\n            label_fields=['category_id']\n        )\n    )\n\ndef get_valid_transforms():\n    return A.Compose(\n        [\n            \n            A.Resize(height=1500, width=1500, p=1.0),\n            A.CLAHE(p=1),\n            #A.Normalize(p=1),\n            #A.Lambda(ToColor(),p=1),\n            \n        ], \n        p=1.0, \n        bbox_params=A.BboxParams(\n            format='coco',\n            min_area=0.5, \n            min_visibility=0.5,\n            label_fields=['category_id']\n        )\n    )\n\ndef get_transforms(train=True):\n    if (train):\n        return get_train_transforms()\n    return get_valid_transforms()\nalbu_transformations=get_transforms(train=True)\n#albu_transformations=None","268f413a":"class PersonalMapper (detectron2.data.DatasetMapper):\n    def __call__(self, dataset_dict):\n        \"\"\"\n        Args:\n            dataset_dict (dict): Metadata of one image, in Detectron2 Dataset format.\n\n        Returns:\n            dict: a format that builtin models in detectron2 accept\n        \"\"\"\n        dataset_dict = copy.deepcopy(dataset_dict)  # it will be modified by code below\n        # USER: Write your own image loading if it's not from a file\n        image = utils.read_image(dataset_dict[\"file_name\"], format=self.image_format)\n        #utils.check_image_size(dataset_dict, image)\n\n        \n        \n        ##### ADDED PART\n\n        #print(\"dataset dict : \",dataset_dict)\n\n        annos = [\n            obj for obj in dataset_dict[\"annotations\"]\n        ]\n        annos_bbox = [\n            obj[\"bbox\"] for obj in dataset_dict[\"annotations\"]\n        ]\n        annos_categroy_id = [\n            obj[\"category_id\"] for obj in dataset_dict.pop(\"annotations\")\n        ]\n        \n        if albu_transformations is not None:\n            transform_list=get_transforms(self.is_train)\n            image=cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n            transform_result=transform_list(image=image,bboxes=annos_bbox,category_id=annos_categroy_id)\n            image=cv2.cvtColor(transform_result[\"image\"], cv2.COLOR_RGB2BGR)\n            annos=[annos[i] for i in range(len(transform_result[\"bboxes\"]))]\n            for i in range(len(annos)):\n                annos[i][\"bbox\"]=list(transform_result[\"bboxes\"][i])\n                annos[i][\"category_id\"]=transform_result[\"category_id\"][i]\n        \n        dataset_dict[\"annotations\"]=annos\n        \n        \n        ##### ADDED PART\n        \n        # USER: Remove if you don't do semantic\/panoptic segmentation.\n        if \"sem_seg_file_name\" in dataset_dict:\n            sem_seg_gt = utils.read_image(dataset_dict.pop(\"sem_seg_file_name\"), \"L\").squeeze(2)\n        else:\n            sem_seg_gt = None\n\n        aug_input = T.AugInput(image, sem_seg=sem_seg_gt)\n        transforms = self.augmentations(aug_input)\n        image, sem_seg_gt = aug_input.image, aug_input.sem_seg\n\n        image_shape = image.shape[:2]  # h, w\n        # Pytorch's dataloader is efficient on torch.Tensor due to shared-memory,\n        # but not efficient on large generic data structures due to the use of pickle & mp.Queue.\n        # Therefore it's important to use torch.Tensor.\n        dataset_dict[\"image\"] = torch.as_tensor(np.ascontiguousarray(image.transpose(2, 0, 1)))\n        if sem_seg_gt is not None:\n            dataset_dict[\"sem_seg\"] = torch.as_tensor(sem_seg_gt.astype(\"long\"))\n\n        # USER: Remove if you don't use pre-computed proposals.\n        # Most users would not need this feature.\n        if self.proposal_topk is not None:\n            utils.transform_proposals(\n                dataset_dict, image_shape, transforms, proposal_topk=self.proposal_topk\n            )\n\n        if not self.is_train:\n            # USER: Modify this if you want to keep them for some reason.\n            dataset_dict.pop(\"annotations\", None)\n            dataset_dict.pop(\"sem_seg_file_name\", None)\n            return dataset_dict\n\n        if \"annotations\" in dataset_dict:\n            # USER: Modify this if you want to keep them for some reason.\n            for anno in dataset_dict[\"annotations\"]:\n                if not self.use_instance_mask:\n                    anno.pop(\"segmentation\", None)\n                if not self.use_keypoint:\n                    anno.pop(\"keypoints\", None)\n\n            # USER: Implement additional transformations if you have other types of data\n            annos = [\n                utils.transform_instance_annotations(\n                    obj, transforms, image_shape, keypoint_hflip_indices=self.keypoint_hflip_indices\n                )\n                for obj in dataset_dict.pop(\"annotations\")\n                if obj.get(\"iscrowd\", 0) == 0\n            ]\n            instances = utils.annotations_to_instances(\n                annos, image_shape, mask_format=self.instance_mask_format\n            )\n\n            # After transforms such as cropping are applied, the bounding box may no longer\n            # tightly bound the object. As an example, imagine a triangle object\n            # [(0,0), (2,0), (0,2)] cropped by a box [(1,0),(2,2)] (XYXY format). The tight\n            # bounding box of the cropped triangle should be [(1,0),(2,1)], which is not equal to\n            # the intersection of original bounding box and the cropping box.\n            if self.recompute_boxes:\n                instances.gt_boxes = instances.gt_masks.get_bounding_boxes()\n            dataset_dict[\"instances\"] = utils.filter_empty_instances(instances)\n        return dataset_dict","9c668db9":"def show_rand_image(train_ds,meta_ds):\n    \"\"\"\n    Can't find solution to show image with augs\n    \"\"\"\n    mapper=PersonalMapper(cfg,is_train=True, augmentations=[])\n    plt.figure(figsize=(16,20))\n    for d in random.sample(ds, 1):\n        d=mapper(d)\n        d[\"annotations\"]=[]\n        #print(\"d : \",d[\"instances\"].get_fields())\n\n        for bbox,classes in zip(d[\"instances\"].get_fields()[\"gt_boxes\"],\n                        d[\"instances\"].get_fields()[\"gt_classes\"]):\n            #print(\"bbox : \",bbox.numpy())\n            instance={\n                'iscrowd': 0,\n                'bbox': list(bbox.numpy()),\n                'bbox_mode': detectron2.structures.BoxMode.XYXY_ABS,\n                'category_id' : classes\n            }\n            d[\"annotations\"].append(instance)\n        #print(\"annotations : \",d[\"annotations\"])\n        img = d[\"image\"]\n        visualizer = Visualizer(img.numpy().transpose(1, 2, 0), metadata=meta, scale=0.5)\n        out = visualizer.draw_dataset_dict(d)\n        plt.imshow(out.get_image()[:, :, ::-1])","aed76059":"#show_rand_image(ds,meta)","59125b4b":"meta_df=pd.read_csv(\"..\/input\/tacotrashdataset\/meta_df.csv\")","eb16c504":"register_coco_instances(\"my_dataset\", {}, \"new_annotations.json\", os.path.join(global_config.general[\"DATASET_PATH\"],\"data\"))","9964e31c":"ds=DatasetCatalog.get(\"my_dataset\")\nmeta=MetadataCatalog.get(\"my_dataset\")","e4cbb9ed":"plt.figure(figsize=(16,20))\nfor d in random.sample(ds, 1):\n    #print(\"d : \",d)\n    img = cv2.imread(d[\"file_name\"])\n    visualizer = Visualizer(img[:, :, ::-1], metadata=meta, scale=0.5)\n    out = visualizer.draw_dataset_dict(d)\n    plt.imshow(out.get_image()[:, :, ::-1])","aafdb9f0":"#show_rand_image(ds,meta)","e06091db":"class PersonalTrainer (detectron2.engine.defaults.DefaultTrainer):\n    \n    def __init__(self, cfg , config=c):\n        super().__init__(cfg)\n        self.metric=0\n        self.checkpointer.save_dir=config.general[\"MODELS_PATH\"]\n\n        \n    def build_hooks(self):\n        hooks = super().build_hooks()\n        def save_best_model():\n            \n            metric=self.test(self.cfg, self.model)[\"bbox\"][\"AP50\"]\n            if(metric>self.metric):\n                self.metric=metric\n                self.checkpointer.save(\"best_model\") # it will add .pth alone\n                \n        steps_per_epoch=annot_df.shape[0]\/\/global_config.model[\"images_per_batch\"]\n        model_checkpointer=EvalHook(steps_per_epoch, save_best_model)\n        hooks.insert(-1,model_checkpointer)\n        return hooks\n    \n    @classmethod\n    def build_train_loader(cls, cfg):\n        \n        #return build_detection_train_loader(cfg,mapper=DatasetMapper(cfg,is_train=True,))\n        return build_detection_train_loader(cfg,mapper=PersonalMapper(cfg,is_train=True,augmentations=[]))\n    @classmethod\n    def build_test_loader(cls, cfg, dataset_name):\n        \n        #return build_detection_test_loader( cfg,dataset_name,mapper=DatasetMapper(cfg,is_train=False,))\n        return build_detection_test_loader( cfg,dataset_name,mapper=PersonalMapper(cfg,is_train=False,augmentations=[]))\n\n    \n    @classmethod\n    def build_evaluator(cls, cfg, dataset_name):\n        return COCOEvaluator(dataset_name, (\"bbox\",), False, output_dir=None)","46ba932b":"@inject_config\ndef get_config(config,fold=0):\n    steps_per_epoch=annot_df.shape[0]\/\/config.model[\"images_per_batch\"]\n    train_dataset_name=f\"my_dataset_train_{fold}\"\n    test_dataset_name=f\"my_dataset_test_{fold}\"\n    cfg = get_cfg()\n    cfg.MODEL.DEVICE='cuda' if torch.cuda.is_available() else 'cpu'\n    cfg.merge_from_file(model_zoo.get_config_file(f\"COCO-Detection\/{config.model['model_name']}.yaml\"))\n    cfg.DATASETS.TRAIN = (train_dataset_name,)\n    cfg.DATASETS.TEST = (test_dataset_name,)\n    cfg.DATALOADER.NUM_WORKERS = 4\n    cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(f\"COCO-Detection\/{config.model['model_name']}.yaml\")  # Let training initialize from model zoo\n    cfg.SOLVER.IMS_PER_BATCH = config.model[\"images_per_batch\"]\n    #cfg.SOLVER.IMS_PER_BATCH = 100\n    cfg.SOLVER.BASE_LR = config.model[\"base_lr\"]  # pick a good LR\n    cfg.SOLVER.MAX_ITER = steps_per_epoch*config.model[\"epochs\"]  # 300 iterations seems good enough for this toy dataset; you will need to train longer for a practical dataset\n    #cfg.SOLVER.MAX_ITER = 100\n    cfg.SOLVER.STEPS = (steps_per_epoch*8,)\n    cfg.MODEL.ROI_HEADS.BATCH_SIZE_PER_IMAGE = config.model[\"batchsize_per_image\"]   # faster, and good enough for this toy dataset (default: 512)\n    cfg.MODEL.ROI_HEADS.NUM_CLASSES = config.model[\"num_classes\"]  # only has one class (ballon). (see https:\/\/detectron2.readthedocs.io\/tutorials\/datasets.html#update-the-config-for-new-datasets)\n    cfg.TEST.EVAL_PERIOD=0\n    #cfg.TEST.EVAL_PERIOD=steps_per_epoch\n    cfg.OUTPUT_DIR = config.general[\"LOGS_PATH\"]\n    #cfg.TEST.EVALUATION_PERIOD=95\n    cfg.OUTPUT_DIR_BEST=f'{config.general[\"LOGS_PATH\"]}'\n    cfg.SOLVER.AMP.ENABLED = True\n    cfg.MODEL.WEIGHTS = \"..\/input\/trash-detection-output\/models\/best_model.pth\"\n\n    cfg.SEED = config.general[\"seed\"]\n\n    os.makedirs(cfg.OUTPUT_DIR, exist_ok=True)\n    os.makedirs(cfg.OUTPUT_DIR_BEST, exist_ok=True)\n    return cfg\n","8ed56b92":"cfg=get_config()","b121c541":"@inject_config\ndef train(config,fold):\n    seed_all()\n    register_dataset(fold)\n    cfg=get_config(fold)\n    #trainer = DefaultTrainer(cfg)\n    trainer = PersonalTrainer(cfg) \n    trainer.resume_or_load(resume=True)\n    trainer.evaluator = COCOEvaluator(f\"my_dataset_test_{fold}\", (\"bbox\",), False, output_dir=None)\n    trainer.train()\n    ","d5b50fc4":"train(0)","ad3a9cad":"with open(os.path.join(global_config.general[\"CONFIG_PATH\"],\"detectron_config.yaml\"),\"w\") as f:\n    f.write(get_config().dump())","db357284":"%ls logs","065a58ae":"%ls models","dc672e21":"%ls configs","e2d07074":"# Look at training curves in tensorboard:\n#%load_ext tensorboard\n#%tensorboard --logdir output","7404da27":"metrics={}","64ed572f":"cfg=get_config()\n#cfg.merge_from_file(model_zoo.get_config_file(\"COCO-Detection\/faster_rcnn_R_101_FPN_3x.yaml\"))\ncfg.MODEL.WEIGHTS = os.path.join(global_config.general[\"MODELS_PATH\"], \"best_model.pth\")  # path to the model we just trained\n#cfg.MODEL.WEIGHTS = os.path.join(cfg.OUTPUT_DIR, \"best_model.pth.pth\")  # path to the model we just trained\n#cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.5   # set a custom testing threshold\nmodel = build_model(cfg)\nm=DetectionCheckpointer(model).load(cfg.MODEL.WEIGHTS)","158c7e0b":"evaluator = COCOEvaluator(\"my_dataset_test_0\", (\"bbox\",), False, output_dir=global_config.general[\"LOGS_PATH\"])\nval_loader = build_detection_test_loader(cfg, \"my_dataset_test_0\")\ntrain_metric=inference_on_dataset(model, val_loader, evaluator)\nmetrics[\"train_metric\"]=train_metric","b4609c8d":"evaluator = COCOEvaluator(\"my_dataset_train_0\", (\"bbox\",), False, output_dir=global_config.general[\"LOGS_PATH\"])\nval_loader = build_detection_test_loader(cfg, \"my_dataset_train_0\")\nvalid_metric=inference_on_dataset(model, val_loader, evaluator)\nmetrics[\"valid_metric\"]=valid_metric","3dd08edf":"Config.dump(metrics,os.path.join(global_config.general[\"LOGS_PATH\"],\"metrics.yaml\"))","bb1b0453":"# ENV","3da168b5":"# Build Trainer","42d6fe3b":"# Config","7fb0947b":"# Imports and utils","9edc450f":"## Categories and Super categories dict","1068cdfc":"# Prepare config params","f186a3b3":"# SEED","98c8204e":"# Register Dataset","cb4d9fbd":"## VIZ","e785a94a":"# Prepare Output","c87f7c7d":"# Annotation Preprocess and Viz","d0686873":"## Preprocess","50dddbf0":"# Prepare trainer","1e86c98c":"# FIX annotation duplicated ids and negative bboxes\n\nrepeated annotations idx \n\n308 => 0\n\n4039  =>2197\n","cdb11915":"## delete negative BBOX","b8dff919":"# Test Dataset","bd100def":"# STATS and VIZ","d0cb37d0":"# Kfold","c863a5cd":"# Preprocess and augmentations "}}