{"cell_type":{"669e221e":"code","2cb1c545":"code","78e2a3ca":"code","b541b264":"code","53dc15b6":"code","db508eff":"code","2833fe6e":"code","21c8f6a0":"code","f316da95":"code","ee53fe47":"code","0374fe18":"code","c89d17f3":"code","8ef02433":"code","6cce9ea9":"code","e9f4435e":"markdown","f766d756":"markdown","ce37b878":"markdown","f13d0e3b":"markdown","4a123228":"markdown","2c5d3b16":"markdown","53e8f33f":"markdown","2bc13382":"markdown","53e5d732":"markdown","1624832b":"markdown","5f3e7b24":"markdown"},"source":{"669e221e":"!pip install -q monai\n!pip install -q git+https:\/\/github.com\/ildoonet\/pytorch-gradual-warmup-lr.git","2cb1c545":"import os\nimport sys\n# os.environ['CUDA_VISIBLE_DEVICES'] = '0,1' # specify GPUs locally\n\n# libraries\nimport time\nimport numpy as np\nimport pandas as pd\nimport cv2\nimport PIL.Image\nimport matplotlib.pyplot as plt\n%matplotlib inline\nfrom tqdm import tqdm as tqdm\nfrom sklearn.metrics import cohen_kappa_score, confusion_matrix\nfrom sklearn.model_selection import train_test_split, StratifiedKFold\nimport random\n\nimport torch\nfrom torch.utils.data import TensorDataset, DataLoader,Dataset\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torchvision\nimport torchvision.transforms as transforms\nimport torch.optim as optim\nfrom torch.optim import lr_scheduler\nfrom torch.utils.data.sampler import SubsetRandomSampler, RandomSampler, SequentialSampler\nfrom torch.optim.lr_scheduler import StepLR, ReduceLROnPlateau, CosineAnnealingLR\nfrom warmup_scheduler import GradualWarmupScheduler\nfrom sklearn.metrics import roc_auc_score\nimport albumentations\n\nimport monai\nfrom monai.data import NiftiDataset\nfrom monai.transforms import AddChannel, Compose, RandRotate90, Resize, ScaleIntensity, ToTensor\n\n# from apex import amp # I cannot install apex in Kagggle notebook\n\ndevice = torch.device('cuda')\n\ndef set_seed(seed=0):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)  \n    torch.cuda.manual_seed(seed)  \n    torch.cuda.manual_seed_all(seed)  \n    torch.backends.cudnn.deterministic = True\n    \nset_seed(0)","78e2a3ca":"DEBUG = True\n\nkernel_type = 'monai3d_160_3ch_1e-5_20ep_aug'\n\nimage_size = 160\nuse_amp = False\ndata_dir = '..\/input\/rsna-str-pe-detection-jpeg-256\/train-jpegs'\nnum_workers = 4\ninit_lr = 1e-5\nout_dim = 9\nfreeze_epo = 0\nwarmup_epo = 1\ncosine_epo = 2 if DEBUG else 19\nn_epochs = freeze_epo + warmup_epo + cosine_epo","b541b264":"target_cols = [\n        'negative_exam_for_pe', # exam level\n        'rv_lv_ratio_gte_1', # exam level\n        'rv_lv_ratio_lt_1', # exam level\n        'leftsided_pe', # exam level\n        'chronic_pe', # exam level\n        'rightsided_pe', # exam level\n        'acute_and_chronic_pe', # exam level\n        'central_pe', # exam level\n        'indeterminate' # exam level\n    ]","53dc15b6":"df = pd.read_csv('..\/input\/rsna-str-pulmonary-embolism-detection\/train.csv')\n\nfrom sklearn.model_selection import GroupKFold\nnp.random.seed(0)\ngroup_kfold = GroupKFold(n_splits=5)\nprint(group_kfold)\n\ndf['fold'] = -1\nfor i, (_, val_index) in enumerate(group_kfold.split(df, groups=df.StudyInstanceUID)):\n    df.loc[val_index, 'fold'] = i\n\ndf.fold.value_counts()","db508eff":"df_study = df.drop_duplicates('StudyInstanceUID')[['StudyInstanceUID','SeriesInstanceUID','fold']+target_cols]\nif DEBUG:\n    df_study = df_study.head(600)","2833fe6e":"from glob import glob\nfrom monai.transforms import LoadNifti, Randomizable, apply_transform\nfrom monai.transforms import AddChannel, Compose, RandRotate90, Resize, ScaleIntensity, ToTensor, RandAffine\nfrom monai.utils import get_seed\n\nclass RSNADataset3D(torch.utils.data.Dataset, Randomizable):\n    def __init__(self, csv, mode, transform=None):\n\n        self.csv = csv.reset_index()\n        self.mode = mode\n        self.transform = transform\n\n    def __len__(self):\n        return self.csv.shape[0]\n    \n    def randomize(self) -> None:\n        MAX_SEED = np.iinfo(np.uint32).max + 1\n        self._seed = self.R.randint(MAX_SEED, dtype=\"uint32\")    \n\n    def __getitem__(self, index):\n        self.randomize()\n        row = self.csv.iloc[index]\n        jpg_lst = sorted(glob(os.path.join(data_dir, row.StudyInstanceUID, row.SeriesInstanceUID, '*.jpg')))\n        img_lst = [cv2.imread(jpg)[:,:,::-1] for jpg in jpg_lst] \n        img = np.stack([image.astype(np.float32) for image in img_lst], axis=2).transpose(3,0,1,2)\n\n        if self.transform is not None:\n            if isinstance(self.transform, Randomizable):\n                self.transform.set_random_state(seed=self._seed)\n            img = apply_transform(self.transform, img)\n\n        if self.mode == 'test':\n            return img\n        else:\n            return img, torch.tensor(row[target_cols]).float()","21c8f6a0":"train_transforms = Compose([ScaleIntensity(), \n                            Resize((image_size, image_size, image_size)), \n                            RandAffine( \n                                      prob=0.5,\n                                      translate_range=(5, 5, 5),\n                                      rotate_range=(np.pi * 4, np.pi * 4, np.pi * 4),\n                                      scale_range=(0.15, 0.15, 0.15),\n                                      padding_mode='border'),\n                            ToTensor()])\nval_transforms = Compose([ScaleIntensity(), Resize((image_size, image_size, image_size)), ToTensor()])","f316da95":"dataset_show = RSNADataset3D(df_study.head(5), 'train', transform=val_transforms)\ndataset_show_aug = RSNADataset3D(df_study.head(5), 'train', transform=train_transforms)\nfrom pylab import rcParams\nrcParams['figure.figsize'] = 20,5\nfor i in range(5):\n    f, axarr = plt.subplots(1,6)\n    img, label = dataset_show[i]\n    for j in range(6):        \n        if j<=2: axarr[j].imshow(img.numpy().transpose(1,2,3,0).mean(axis=j))\n        elif j==3: axarr[j].imshow(img.numpy().transpose(1,2,3,0)[image_size\/\/2,:,:])\n        elif j==4: axarr[j].imshow(img.numpy().transpose(1,2,3,0)[:,image_size\/\/2,:])\n        elif j==5: axarr[j].imshow(img.numpy().transpose(1,2,3,0)[:,:,image_size\/\/2])\n        axarr[j].set_title(f\"Orig {i}\")\n    f, axarr = plt.subplots(1,6)\n    img, label = dataset_show_aug[i]    \n    for j in range(6):        \n        if j<=2: axarr[j].imshow(img.numpy().transpose(1,2,3,0).mean(axis=j))\n        elif j==3: axarr[j].imshow(img.numpy().transpose(1,2,3,0)[image_size\/\/2,:,:])\n        elif j==4: axarr[j].imshow(img.numpy().transpose(1,2,3,0)[:,image_size\/\/2,:])\n        elif j==5: axarr[j].imshow(img.numpy().transpose(1,2,3,0)[:,:,image_size\/\/2])\n        axarr[j].set_title(f\"Aug {i}\")\n    plt.show()","ee53fe47":"bce = nn.BCEWithLogitsLoss()\ndef criterion(logits, target): \n    loss = bce(logits.view(-1), target.view(-1))\n    return loss","0374fe18":"def train_epoch(model, loader, optimizer):\n\n    model.train()\n    train_loss = []\n    bar = tqdm(loader)\n    for (data, target) in bar:\n        data, target = data.to(device), target.to(device)\n        optimizer.zero_grad()\n        logits = model(data)       \n        loss = criterion(logits, target)\n\n        if not use_amp:\n            loss.backward()\n        else:\n            with amp.scale_loss(loss, optimizer) as scaled_loss:\n                scaled_loss.backward()\n\n        optimizer.step()\n\n        loss_np = loss.detach().cpu().numpy()\n        train_loss.append(loss_np)\n        smooth_loss = sum(train_loss[-100:]) \/ min(len(train_loss), 100)\n        bar.set_description('loss: %.5f, smth: %.5f' % (loss_np, smooth_loss))\n    return train_loss\n\n\ndef val_epoch(model, loader, is_ext=None, n_test=1, get_output=False):\n\n    model.eval()\n    val_loss = []\n    LOGITS = []\n    TARGETS = []\n\n    with torch.no_grad():\n        for (data, target) in tqdm(loader):\n            data, target = data.to(device), target.to(device)\n            logits = model(data)\n            LOGITS.append(logits.detach().cpu())\n            TARGETS.append(target.detach().cpu())\n\n    val_loss = criterion(torch.cat(LOGITS), torch.cat(TARGETS)).numpy()\n    PROBS = torch.sigmoid(torch.cat(LOGITS)).numpy().squeeze()    \n    LOGITS = torch.cat(LOGITS).numpy()\n    TARGETS = torch.cat(TARGETS).numpy()\n    \n    if get_output:\n        return LOGITS, PROBS, TARGETS\n    else:\n        acc = (PROBS.round() == TARGETS).mean() * 100.\n        auc = roc_auc_score(TARGETS, LOGITS)\n        return float(val_loss), acc, auc","c89d17f3":"class GradualWarmupSchedulerV2(GradualWarmupScheduler):\n    def __init__(self, optimizer, multiplier, total_epoch, after_scheduler=None):\n        super(GradualWarmupSchedulerV2, self).__init__(optimizer, multiplier, total_epoch, after_scheduler)\n    def get_lr(self):\n        if self.last_epoch > self.total_epoch:\n            if self.after_scheduler:\n                if not self.finished:\n                    self.after_scheduler.base_lrs = [base_lr * self.multiplier for base_lr in self.base_lrs]\n                    self.finished = True\n                return self.after_scheduler.get_lr()\n            return [base_lr * self.multiplier for base_lr in self.base_lrs]\n        if self.multiplier == 1.0:\n            return [base_lr * (float(self.last_epoch) \/ self.total_epoch) for base_lr in self.base_lrs]\n        else:\n            return [base_lr * ((self.multiplier - 1.) * self.last_epoch \/ self.total_epoch + 1.) for base_lr in self.base_lrs]","8ef02433":"def run(fold):\n    df_train = df_study[(df_study['fold'] != fold)]\n    df_valid = df_study[(df_study['fold'] == fold)]\n\n    dataset_train = RSNADataset3D(df_train, 'train', transform=train_transforms)\n    dataset_valid = RSNADataset3D(df_valid, 'val', transform=val_transforms)\n    train_loader = torch.utils.data.DataLoader(dataset_train, batch_size=8, sampler=RandomSampler(dataset_train), num_workers=num_workers)\n    valid_loader = torch.utils.data.DataLoader(dataset_valid, batch_size=8, num_workers=num_workers)\n\n    model = monai.networks.nets.densenet.densenet121(spatial_dims=3, in_channels=3, out_channels=out_dim).to(device)\n\n    val_loss_best = 1000\n    model_file = f'{kernel_type}_best_fold{fold}.pth'\n\n    optimizer = optim.Adam(model.parameters(), lr=init_lr)\n    if use_amp:\n        model, optimizer = amp.initialize(model, optimizer, opt_level=\"O1\")\n#     if len(os.environ['CUDA_VISIBLE_DEVICES'].split(',')) > 1:\n#         model = nn.DataParallel(model)         \n        \n    scheduler_cosine = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, cosine_epo)\n    scheduler_warmup = GradualWarmupSchedulerV2(optimizer, multiplier=10, total_epoch=warmup_epo, after_scheduler=scheduler_cosine)\n\n    print(len(dataset_train), len(dataset_valid))\n\n    for epoch in range(1, n_epochs+1):\n        print(time.ctime(), 'Epoch:', epoch)\n        scheduler_warmup.step(epoch-1)\n\n        train_loss = train_epoch(model, train_loader, optimizer)\n        val_loss, acc, auc = val_epoch(model, valid_loader)\n    \n        content = time.ctime() + ' ' + f'Fold {fold}, Epoch {epoch}, lr: {optimizer.param_groups[0][\"lr\"]:.7f}, train loss: {np.mean(train_loss):.5f}, valid loss: {(val_loss):.5f}, acc: {(acc):.4f}, auc: {(auc):.6f}'\n        print(content)\n        with open(f'log_{kernel_type}.txt', 'a') as appender:\n            appender.write(content + '\\n')             \n            \n        if val_loss < val_loss_best:\n            print('val_loss_best ({:.6f} --> {:.6f}).  Saving model ...'.format(val_loss_best, val_loss))\n            torch.save(model.state_dict(), model_file)\n            val_loss_best = val_loss\n\n    torch.save(model.state_dict(), f'{kernel_type}_model_fold{fold}.pth')","6cce9ea9":"run(fold=0)","e9f4435e":"## 3D dataset","f766d756":"We use the preprocessed data by Dr. Ian Pan https:\/\/www.kaggle.com\/vaillant\/rsna-str-pe-detection-jpeg-256\n1. read all the jpgs in the same study\n2. stack them in order to make it a 3-dimensional array\n3. resize into 160x160x160x3 cube (with 3 channels)","ce37b878":"Please leave a comment for any question.","f13d0e3b":"## loss function","4a123228":"## training","2c5d3b16":"## Inference and submitting","53e8f33f":"Visualize some studies before and after augmentations. In the plots below, each study has two rows. Top row is original; bottom row is after augmentations.\n\nEach row has 6 images: first three are the mean in three direction's view of the 3-D array (front\/back, side, top\/bottom); next three are the middle image in the three directions.","2bc13382":"see https:\/\/www.kaggle.com\/boliu0\/monai-3d-cnn-inference\/\n\nModel trained locally is linked in the inference kernel.","53e5d732":"## setup","1624832b":"Note: there are some differences between code in this notebook and my local code due to Kaggle notebook hardware limitation.\n* I use apex for mixed precision locally, but I don't use it here as I cannot install `apex`\n* I use 2 GPUs (32G each) and 32 CPU cores locally with batch size 48, here I can only fit in batch size 8\n* I use DEBUG mode in the notebook for illustration. In DEBUG mode I only train for 3 epochs using first 600 studies. Locally I train 20 epochs using all 7000 studies by setting `DEBUG = False` in the beginning\n\nIn my local setup, valid loss can go down to 0.306","5f3e7b24":"There are only 7000 data in train set, so we use heavy augmentations."}}