{"cell_type":{"67e12122":"code","e84eced0":"code","b08bf989":"code","b7d88525":"code","79ed23c5":"code","1f970d73":"code","33c8c219":"code","286df7bb":"code","e24ea18c":"code","43845413":"code","f6e8042a":"code","a9139c26":"code","da185537":"code","76762a2c":"code","73f02033":"code","2deeb451":"code","c405aa63":"code","a8424f0a":"code","f7894a35":"code","67f82ace":"code","fb1e9fd6":"code","02a5529e":"code","a170b0dc":"code","22855d95":"code","8f0a6f9b":"code","e145b69b":"code","594e8b6c":"code","6b8c53bf":"code","cc9a04e3":"code","b11e56ab":"code","1010d736":"code","c43a26f4":"code","f34f2585":"code","17a48a3a":"code","37511a8d":"markdown","17526224":"markdown","3c613b09":"markdown","994693a5":"markdown","6a2f351c":"markdown","4f92fffa":"markdown","b3d9cd34":"markdown","760d686e":"markdown","695467fd":"markdown","a92a9715":"markdown","cd685a1d":"markdown","ce6504d0":"markdown","12b17e65":"markdown","648738d2":"markdown","1578f03c":"markdown","743f643f":"markdown","7ec5c19b":"markdown","6867cbf3":"markdown","395cc3b5":"markdown","c7c01caa":"markdown","cdba4011":"markdown","2a678db8":"markdown"},"source":{"67e12122":"import numpy as np\nimport pandas as pd\nfrom tqdm import tqdm\n\nimport re\nimport string\n\nimport nltk\nfrom nltk.corpus import stopwords\nstop_words = stopwords.words('english')\n\n\nimport tensorflow as tf\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Embedding\nfrom tensorflow.keras.initializers import Constant\nfrom tensorflow.keras.callbacks import EarlyStopping\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix,classification_report\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns","e84eced0":"df = pd.read_csv('\/kaggle\/input\/nlp-getting-started\/train.csv')\ndf.head()","b08bf989":"ax = sns.countplot(x=\"target\", data=df)","b7d88525":"def clean_text(text):\n    \n    sw = stopwords.words('english')\n    \n    text = re.sub(r\"[^a-zA-Z?.!,\u00bf]+\", \" \", text) # replacing everything with space except (a-z, A-Z, \".\", \"?\", \"!\", \",\")\n\n    text = re.sub(r\"http\\S+\", \"\",text) #Removing URLs \n    #text = re.sub(r\"http\", \"\",text)\n    \n    html=re.compile(r'<.*?>') \n    \n    text = html.sub(r'',text) #Removing html tags\n    \n    punctuations = '@#!?+&*[]-%.:\/();$=><|{}^' + \"'`\" + '_'\n    for p in punctuations:\n        text = text.replace(p,'') #Removing punctuations\n        \n    text = [word.lower() for word in text.split() if word.lower() not in sw]\n    \n    text = \" \".join(text) #removing stopwords\n    \n    emoji_pattern = re.compile(\"[\"\n                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                           u\"\\U00002702-\\U000027B0\"\n                           u\"\\U000024C2-\\U0001F251\"\n                           \"]+\", flags=re.UNICODE)\n    text = emoji_pattern.sub(r'', text) #Removing emojis\n    \n    return text\n","79ed23c5":"mispell_dict = {\"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \"could've\": \"could have\", \"couldn't\": \"could not\", \"didn't\": \"did not\",  \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \"hasn't\": \"has not\", \"haven't\": \"have not\", \"he'd\": \"he would\",\"he'll\": \"he will\", \"he's\": \"he is\", \"how'd\": \"how did\", \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\",  \"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\", \"I'll've\": \"I will have\",\"I'm\": \"I am\", \"I've\": \"I have\", \"i'd\": \"i would\", \"i'd've\": \"i would have\", \"i'll\": \"i will\",  \"i'll've\": \"i will have\",\"i'm\": \"i am\", \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\", \"it'd've\": \"it would have\", \"it'll\": \"it will\", \"it'll've\": \"it will have\",\"it's\": \"it is\", \"let's\": \"let us\", \"ma'am\": \"madam\", \"mayn't\": \"may not\", \"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\", \"must've\": \"must have\", \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \"needn't\": \"need not\", \"needn't've\": \"need not have\",\"o'clock\": \"of the clock\", \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\", \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\", \"she'd\": \"she would\", \"she'd've\": \"she would have\", \"she'll\": \"she will\", \"she'll've\": \"she will have\", \"she's\": \"she is\", \"should've\": \"should have\", \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\"so's\": \"so as\", \"this's\": \"this is\",\"that'd\": \"that would\", \"that'd've\": \"that would have\", \"that's\": \"that is\", \"there'd\": \"there would\", \"there'd've\": \"there would have\", \"there's\": \"there is\", \"here's\": \"here is\",\"they'd\": \"they would\", \"they'd've\": \"they would have\", \"they'll\": \"they will\", \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\", \"to've\": \"to have\", \"wasn't\": \"was not\", \"we'd\": \"we would\", \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\", \"we're\": \"we are\", \"we've\": \"we have\", \"weren't\": \"were not\", \"what'll\": \"what will\", \"what'll've\": \"what will have\", \"what're\": \"what are\",  \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\", \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\", \"where've\": \"where have\", \"who'll\": \"who will\", \"who'll've\": \"who will have\", \"who's\": \"who is\", \"who've\": \"who have\", \"why's\": \"why is\", \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\", \"won't've\": \"will not have\", \"would've\": \"would have\", \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\", \"y'all\": \"you all\", \"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\", \"you'll've\": \"you will have\", \"you're\": \"you are\", \"you've\": \"you have\", 'colour': 'color', 'centre': 'center', 'favourite': 'favorite', 'travelling': 'traveling', 'counselling': 'counseling', 'theatre': 'theater', 'cancelled': 'canceled', 'labour': 'labor', 'organisation': 'organization', 'wwii': 'world war 2', 'citicise': 'criticize', 'youtu ': 'youtube ', 'Qoura': 'Quora', 'sallary': 'salary', 'Whta': 'What', 'narcisist': 'narcissist', 'howdo': 'how do', 'whatare': 'what are', 'howcan': 'how can', 'howmuch': 'how much', 'howmany': 'how many', 'whydo': 'why do', 'doI': 'do I', 'theBest': 'the best', 'howdoes': 'how does', 'mastrubation': 'masturbation', 'mastrubate': 'masturbate', \"mastrubating\": 'masturbating', 'pennis': 'penis', 'Etherium': 'Ethereum', 'narcissit': 'narcissist', 'bigdata': 'big data', '2k17': '2017', '2k18': '2018', 'qouta': 'quota', 'exboyfriend': 'ex boyfriend', 'airhostess': 'air hostess', \"whst\": 'what', 'watsapp': 'whatsapp', 'demonitisation': 'demonetization', 'demonitization': 'demonetization', 'demonetisation': 'demonetization'}","1f970d73":"def _get_mispell(mispell_dict):\n    mispell_re = re.compile('(%s)' % '|'.join(mispell_dict.keys()))\n    return mispell_dict, mispell_re\n\nmispellings, mispellings_re = _get_mispell(mispell_dict)\ndef replace_misspell(text):\n    def replace(match):\n        return mispellings[match.group(0)]\n    return mispellings_re.sub(replace, text)","33c8c219":"df['text'] = df['text'].str.lower()\ndf['text'] = df['text'].apply(lambda x: clean_text(x))\ndf['text'] = df['text'].apply(lambda x: replace_misspell(x))\n\n \ndf.head()","286df7bb":"x_train,x_test,y_train,y_test=train_test_split(df['text'].values,df['target'].values,test_size=0.2,random_state=123)\nx_train,x_val,y_train,y_val=train_test_split(x_train,y_train,test_size=0.1,random_state=123)","e24ea18c":"tokenizer = Tokenizer(num_words=None)\ntokenizer.fit_on_texts(x_train)\n\nword_index = tokenizer.word_index\nnum_words = len(tokenizer.word_index) + 1","43845413":"train_sequences = tokenizer.texts_to_sequences(x_train)\ntrain_padded = tf.keras.preprocessing.sequence.pad_sequences(train_sequences,padding='post')\n\nmax_len = len(train_padded[0])\n\nvalidation_sequences = tokenizer.texts_to_sequences(x_val)\nvalidation_padded = tf.keras.preprocessing.sequence.pad_sequences(validation_sequences,padding='post',maxlen=max_len)\n\ntest_sequences = tokenizer.texts_to_sequences(x_test)\ntest_padded = tf.keras.preprocessing.sequence.pad_sequences(test_sequences,padding='post',maxlen=max_len)","f6e8042a":"model = tf.keras.Sequential([\ntf.keras.layers.Embedding(num_words, 100,mask_zero=True),\n#tf.keras.layers.LSTM(64),\ntf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64,recurrent_dropout=0.1)),\ntf.keras.layers.Dense(64, activation='relu'),\ntf.keras.layers.Dense(1, activation='sigmoid')\n])\n\nmodel.compile(loss='binary_crossentropy',optimizer=tf.keras.optimizers.Adam(1e-4),metrics=['accuracy'])","a9139c26":"model.summary()","da185537":"num_epochs = 30","76762a2c":"history = model.fit(train_padded, y_train, epochs=num_epochs,validation_data=(validation_padded, y_val), verbose=2)","73f02033":"y_pred = model.predict_classes(test_padded)","2deeb451":"cnf_matrix = confusion_matrix(y_test,y_pred)\ngroup_names = ['TN','FP','FN','TP']\ngroup_counts = [\"{0:0.0f}\".format(value) for value in cnf_matrix.flatten()]\nlabels = [f\"{v1}\\n{v2}\" for v1, v2 in zip(group_names,group_counts)]\nlabels = np.asarray(labels).reshape(2,2)\nsns.heatmap(cnf_matrix, annot=labels, fmt='', cmap='Blues');","c405aa63":"print(classification_report(y_test,y_pred))","a8424f0a":"#history.history['accuracy']\nplt.plot(history.history['accuracy'])\nplt.plot(history.history['val_accuracy'])\nplt.title('Model Performance')\nplt.ylabel('Accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'val'], loc='upper right')\nplt.box(False)\nplt.show()","f7894a35":"embedding_dict={}\nwith open('..\/input\/glove6b100dtxt\/glove.6B.100d.txt','r') as f:\n    for line in f:\n        values=line.split()\n        word = values[0]\n        vectors=np.asarray(values[1:],'float32')\n        embedding_dict[word]=vectors\nf.close()","67f82ace":"words_na = []\nembedding_matrix = np.zeros((num_words,100))\nword_index = tokenizer.word_index\nfor word,i in tqdm(word_index.items()):\n    if i > num_words:\n        continue\n    emb_vec=embedding_dict.get(word)\n    \n    if emb_vec is None:\n        \n        \n        words_na.append(word)\n    \n    elif emb_vec is not None:\n        embedding_matrix[i]=emb_vec","fb1e9fd6":"print(\"Out of vocabulory words:\",len(words_na))","02a5529e":"print(\"% of Out of vocabulory words:\",(len(words_na)\/num_words)*100)","a170b0dc":"model2 = tf.keras.Sequential([\ntf.keras.layers.Embedding(num_words,100,embeddings_initializer=Constant(embedding_matrix),trainable=False),\n#tf.keras.layers.LSTM(64),\ntf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64)),\ntf.keras.layers.Dense(64, activation='relu'),\ntf.keras.layers.Dense(1, activation='sigmoid')\n])\n\nmodel2.compile(loss='binary_crossentropy',optimizer=tf.keras.optimizers.Adam(1e-4),metrics=['accuracy'])","22855d95":"model2.summary()","8f0a6f9b":"earlystop = EarlyStopping(monitor='val_loss', min_delta=0, patience=5, verbose=0,restore_best_weights=True, mode='auto')\nhistory = model2.fit(train_padded, y_train, epochs=num_epochs,validation_data=(validation_padded, y_val), verbose=2,callbacks=[earlystop])","e145b69b":"y_pred = model2.predict_classes(test_padded)","594e8b6c":"cnf_matrix = confusion_matrix(y_test,y_pred)\ngroup_names = ['TN','FP','FN','TP']\ngroup_counts = [\"{0:0.0f}\".format(value) for value in cnf_matrix.flatten()]\nlabels = [f\"{v1}\\n{v2}\" for v1, v2 in zip(group_names,group_counts)]\nlabels = np.asarray(labels).reshape(2,2)\nsns.heatmap(cnf_matrix, annot=labels, fmt='', cmap='Blues');","6b8c53bf":"print(classification_report(y_test,y_pred))","cc9a04e3":"epochs = range(1,len(history.history['loss'])+1)\nplt.plot(epochs,history.history['accuracy'])\nplt.plot(epochs,history.history['val_accuracy'])\nplt.title('Model Performance')\nplt.ylabel('Accuracy')\nplt.xlabel('epoch')\nplt.xticks(epochs)\nplt.legend(['train', 'val'], loc='upper right')\nplt.box(False)\nplt.show()","b11e56ab":"df_sub = pd.read_csv('..\/input\/nlp-getting-started\/test.csv')","1010d736":"df_sub['text'] = df_sub['text'].str.lower()\ndf_sub['text'] = df_sub['text'].apply(lambda x: clean_text(x))\ndf_sub['text'] = df_sub['text'].apply(lambda x: replace_misspell(x))\n\n \ndf_sub.head()","c43a26f4":"X_sub =  df_sub['text'].values\nsub_sequences = tokenizer.texts_to_sequences(X_sub)\nsub_padded = tf.keras.preprocessing.sequence.pad_sequences(sub_sequences,padding='post',maxlen=max_len)","f34f2585":"y_pred = model2.predict_classes(sub_padded)\ndf_output = pd.DataFrame()\ndf_output['id'] = df_sub['id']\ndf_output['target'] =[x.item() for x in list(y_pred)]","17a48a3a":"df_output.to_csv('submission.csv',index=False)","37511a8d":"We can observe we are not using the best model. We can use tensorflow callback method to ensure we get the best model.","17526224":"We can see the improvement in accuracy.","3c613b09":"we will load glove embeddings, pretrained on wikepedia data.","994693a5":"# Tokenization\n\nReferences\n1. https:\/\/www.analyticsvidhya.com\/blog\/2020\/05\/what-is-tokenization-nlp\/\n2. https:\/\/www.analyticsvidhya.com\/blog\/2019\/07\/how-get-started-nlp-6-unique-ways-perform-tokenization\/\n\n\n\nTokenization is a common task in NLP. This is the process of spliting of sentences to basic input unit. Ultimate aim of tokenization is to built a vocabulary. Vocabulary is the unique set of all the tokens. Tokenization can be be performed on:\n\n* character level\n* subword level\n* word level\n\n**Word Tokenization** is the most commonly used tokenization algorithm, this is because of the availabity of pretrained word embeddings. Pretrained word embeddings are trained on a large volume of corpous, hence large vocabulary.\n\nA problem with word tokenization is that the presence of **OOV**(Out of vocabulary) words. OOV words refer to the new words which are encountered at testing. These new words do not exist in the vocabulary. Hence, these methods fail in handling OOV words. One way to handle OOV words is to form the vocabulary with the Top K Frequent Words and replace the rare words in training data with unknown tokens (UNK). This helps the model to learn the representation of OOV words in terms of UNK tokens. So, during test time, any word that is not present in the vocabulary will be mapped to a UNK token. This is how we can tackle the problem of OOV in word tokenizers. The problem with this approach is that the entire information of the word is lost as we are mapping OOV to UNK tokens. The structure of the word might be helpful in representing the word accurately. And another issue is that every OOV word gets the same representation.\n\n**character Tokenization** splits a piece of text into a set of characters. Character level tokenization will have fixed vocabulary (26 characters + digits + special characters). The inputs increases verymuch and it will be challenging to learn the meaning of words from the character level tokens.\n\n**Subword Tokenization** splits the piece of text into subwords (or n-gram characters). Transformed based models rely on Subword Tokenization algorithms for preparing vocabulary. There are different types of subword tokenization such as **BPE (Byte Pair Encoding)** .","6a2f351c":"Now, we will create an embedding matrix.","4f92fffa":"We are using custom functions to perform the following tasks. Cleaning up the data for modeling should be carried out carefully and with the help of subject matter experts, if possible. This cleaning is done completely based on observation, and can not be considered as a generic preprocessing step for all the NLP tasks. This preprocessing function ensures:","b3d9cd34":"# Submission","760d686e":"# Problem statement\n\nThe aim is to develop an algorithm to predict whether a tweet is about a real disaster or not.","695467fd":"## Using Pre-trained Word Embeddings","a92a9715":"# End Notes\n\nThis notebook is a beginer level approach for solving a NLP task. The model is not perfect, and there is scope for experiments like modifying model architecture,performing hyper parameter tuning, using different word embeddings etc. Also, there are other models, including transformer based models like BERT,that can be used for this task.","cd685a1d":"Now,Like any supervised machine learning problem,we are spliting the dataset to train,test and validation sets.","ce6504d0":"We will be using word level tokenization, Using **Tensorflow Keras Tokenizer API**. <br> \n\nRef - https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/keras\/preprocessing\/text\/Tokenizer","12b17e65":"## Embedding layer","648738d2":"We bulit dictionary based on tokens in training set. We can perform a variety of preprocessing using the tokenizer class, without writing custom functions. We will analyse the model performance based on this limited vocabulary, with OOV tokens in test and validation sets.","1578f03c":"* Removing urls from tweet\n* Removing html tags\n* Removing punctuations\n* Removing stopwords\n* Removing emoji\n* Replacing misspelled words (For using pretrained word embeddings)","743f643f":"## Model Using tf.Keras","7ec5c19b":"# Data Clensing","6867cbf3":"Reference - https:\/\/www.kaggle.com\/mlwhiz\/textcnn-pytorch-and-keras","395cc3b5":"### Converting text to sequences","c7c01caa":"There is no class imbalance in the distribution of target variable.","cdba4011":"Transfers one-hot word vectors to dense vectors. One way is to use pre-defined embedding layers such as word2vec or glove, which are calculated on a huge amount of data such as Wikepedia. Another way is to calculate the embedding layer while training. Here the embedding vectors change after each timestep of training. For calculating training embedding, we can either use pre-defined embedding values initially or train from scratch.\n\nWe will built our first model training on the sequences we generated earlier.","2a678db8":"Tokenizer class maps the tokens to unique numbers. In a sequential learning problem, every sentence is inputed as sequence of numbers. The prebuilt function 'texts_to_sequences' in tensorflow helps to convert sentences to sequence of numbers. We all know that sentences can be of variable length. We use padding, to make sentence constant length. We add 0's either before or after every sequence, to match the maximum sequence length. "}}