{"cell_type":{"6797eed6":"code","e5e2140d":"code","5e816307":"code","f1d0c880":"code","be8d7618":"code","b238947a":"code","cce70c5c":"code","3ed4df07":"code","6c213f4c":"code","5334d9ba":"code","337a0d62":"code","c681744d":"code","1bced313":"code","6efdd5b5":"markdown","d9f05894":"markdown","f6ce6a0e":"markdown","0097b217":"markdown","7feef8f1":"markdown","bf75e67d":"markdown","b7471782":"markdown","c8f1408a":"markdown"},"source":{"6797eed6":"# Disable warnings in Anaconda\nimport warnings\nimport numpy as np\nwarnings.filterwarnings('ignore')\n\n%matplotlib inline\nfrom matplotlib import pyplot as plt\nplt.style.use('ggplot')\nplt.rcParams['figure.figsize'] = 10, 6\n%config InlineBackend.figure_format = 'retina' \n\nimport seaborn as sns\nfrom sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\nfrom sklearn.ensemble import BaggingClassifier, BaggingRegressor\nfrom sklearn.tree import DecisionTreeRegressor, DecisionTreeClassifier\nfrom sklearn.datasets import make_circles\nfrom sklearn.model_selection import train_test_split","e5e2140d":"n_train = 150        \nn_test = 1000       \nnoise = 0.1\n\n# Generate data\ndef f(x):\n    x = x.ravel()\n    return np.exp(-x ** 2) + 1.5 * np.exp(-(x - 2) ** 2)\n\ndef generate(n_samples, noise):\n    X = np.random.rand(n_samples) * 10 - 5\n    X = np.sort(X).ravel()\n    y = np.exp(-X ** 2) + 1.5 * np.exp(-(X - 2) ** 2)\\\n        + np.random.normal(0.0, noise, n_samples)\n    X = X.reshape((n_samples, 1))\n\n    return X, y\n\nX_train, y_train = generate(n_samples=n_train, noise=noise)\nX_test, y_test = generate(n_samples=n_test, noise=noise)\n\n# One decision tree regressor\ndtree = DecisionTreeRegressor().fit(X_train, y_train)\nd_predict = dtree.predict(X_test)\n\nplt.figure(figsize=(10, 6))\nplt.plot(X_test, f(X_test), \"b\")\nplt.scatter(X_train, y_train, c=\"b\", s=20)\nplt.plot(X_test, d_predict, \"g\", lw=2)\nplt.xlim([-5, 5])\nplt.title(\"Decision tree, MSE = %.2f\" \n          % np.sum((y_test - d_predict) ** 2))\n\n# Bagging with a decision tree regressor\nbdt = BaggingRegressor(DecisionTreeRegressor()).fit(X_train, y_train)\nbdt_predict = bdt.predict(X_test)\n\nplt.figure(figsize=(10, 6))\nplt.plot(X_test, f(X_test), \"b\")\nplt.scatter(X_train, y_train, c=\"b\", s=20)\nplt.plot(X_test, bdt_predict, \"y\", lw=2)\nplt.xlim([-5, 5])\nplt.title(\"Bagging for decision trees, MSE = %.2f\" % np.sum((y_test - bdt_predict) ** 2));\n\n# Random Forest\nrf = RandomForestRegressor(n_estimators=10).fit(X_train, y_train)\nrf_predict = rf.predict(X_test)\n\nplt.figure(figsize=(10, 6))\nplt.plot(X_test, f(X_test), \"b\")\nplt.scatter(X_train, y_train, c=\"b\", s=20)\nplt.plot(X_test, rf_predict, \"r\", lw=2)\nplt.xlim([-5, 5])\nplt.title(\"Random forest, MSE = %.2f\" % np.sum((y_test - rf_predict) ** 2));","5e816307":"np.random.seed(42)\nX, y = make_circles(n_samples=500, factor=0.1, noise=0.35, random_state=42)\nX_train_circles, X_test_circles, y_train_circles, y_test_circles = \\\n    train_test_split(X, y, test_size=0.2)\n\ndtree = DecisionTreeClassifier(random_state=42)\ndtree.fit(X_train_circles, y_train_circles)\n\nx_range = np.linspace(X.min(), X.max(), 100)\nxx1, xx2 = np.meshgrid(x_range, x_range)\ny_hat = dtree.predict(np.c_[xx1.ravel(), xx2.ravel()])\ny_hat = y_hat.reshape(xx1.shape)\nplt.contourf(xx1, xx2, y_hat, alpha=0.2)\nplt.scatter(X[:,0], X[:,1], c=y, cmap='viridis', alpha=.7)\nplt.title(\"Decision tree\")\nplt.show()\n\nb_dtree = BaggingClassifier(DecisionTreeClassifier(), \n                            n_estimators=300, random_state=42)\nb_dtree.fit(X_train_circles, y_train_circles)\n\nx_range = np.linspace(X.min(), X.max(), 100)\nxx1, xx2 = np.meshgrid(x_range, x_range)\ny_hat = b_dtree.predict(np.c_[xx1.ravel(), xx2.ravel()])\ny_hat = y_hat.reshape(xx1.shape)\nplt.contourf(xx1, xx2, y_hat, alpha=0.2)\nplt.scatter(X[:,0], X[:,1], c=y, cmap='viridis', alpha=.7)\nplt.title(\"Bagging (decision trees)\")\nplt.show()\n\nrf = RandomForestClassifier(n_estimators=300, random_state=42)\nrf.fit(X_train_circles, y_train_circles)\n\nx_range = np.linspace(X.min(), X.max(), 100)\nxx1, xx2 = np.meshgrid(x_range, x_range)\ny_hat = rf.predict(np.c_[xx1.ravel(), xx2.ravel()])\ny_hat = y_hat.reshape(xx1.shape)\nplt.contourf(xx1, xx2, y_hat, alpha=0.2)\nplt.scatter(X[:,0], X[:,1], c=y, cmap='viridis', alpha=.7)\nplt.title(\"Random forest\")\nplt.show()","f1d0c880":"import pandas as pd\nfrom sklearn.model_selection import cross_val_score, StratifiedKFold, GridSearchCV\nfrom sklearn.metrics import accuracy_score\n\n# Load data\ndf = pd.read_csv(\"..\/input\/mlcourse\/telecom_churn.csv\")\n\n# Choose the numeric features\ncols = []\nfor i in df.columns:\n    if (df[i].dtype == \"float64\") or (df[i].dtype == 'int64'):\n        cols.append(i)\n        \n# Divide the dataset into the input and target\nX, y = df[cols].copy(), np.asarray(df[\"Churn\"],dtype='int8')\n\n# Initialize a stratified split of our dataset for the validation process\nskf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n\n# Initialize the classifier with the default parameters \nrfc = RandomForestClassifier(random_state=42, n_jobs=-1)\n\n# Train it on the training set\nresults = cross_val_score(rfc, X, y, cv=skf)\n\n# Evaluate the accuracy on the test set\nprint(\"CV accuracy score: {:.2f}%\".format(results.mean() * 100))","be8d7618":"# Initialize the validation\nskf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n\n# Create lists to save the values of accuracy on training and test sets\ntrain_acc = []\ntest_acc = []\ntemp_train_acc = []\ntemp_test_acc = []\ntrees_grid = [5, 10, 15, 20, 30, 50, 75, 100]\n\nfor ntrees in trees_grid:\n    rfc = RandomForestClassifier(n_estimators=ntrees, random_state=42, n_jobs=-1)\n    temp_train_acc = []\n    temp_test_acc = []\n    for train_index, test_index in skf.split(X, y):\n        X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n        y_train, y_test = y[train_index], y[test_index]\n        rfc.fit(X_train, y_train)\n        temp_train_acc.append(rfc.score(X_train, y_train))\n        temp_test_acc.append(rfc.score(X_test, y_test))\n    train_acc.append(temp_train_acc)\n    test_acc.append(temp_test_acc)\n    \ntrain_acc, test_acc = np.asarray(train_acc), np.asarray(test_acc)\nprint(\"Best CV accuracy is {:.2f}% with {} trees\".format(max(test_acc.mean(axis=1))*100, \n                                                        trees_grid[np.argmax(test_acc.mean(axis=1))]))","b238947a":"plt.style.use('ggplot')\n\nfig, ax = plt.subplots(figsize=(8, 4))\nax.plot(trees_grid, train_acc.mean(axis=1), alpha=0.5, color='blue', label='train')\nax.plot(trees_grid, test_acc.mean(axis=1), alpha=0.5, color='red', label='cv')\nax.fill_between(trees_grid, test_acc.mean(axis=1) - test_acc.std(axis=1), \n                test_acc.mean(axis=1) + test_acc.std(axis=1), color='#888888', alpha=0.4)\nax.fill_between(trees_grid, test_acc.mean(axis=1) - 2*test_acc.std(axis=1), \n                test_acc.mean(axis=1) + 2*test_acc.std(axis=1), color='#888888', alpha=0.2)\nax.legend(loc='best')\nax.set_ylim([0.88,1.02])\nax.set_ylabel(\"Accuracy\")\nax.set_xlabel(\"N_estimators\");","cce70c5c":"# Create lists to save accuracy values on the training and test sets\ntrain_acc = []\ntest_acc = []\ntemp_train_acc = []\ntemp_test_acc = []\nmax_depth_grid = [3, 5, 7, 9, 11, 13, 15, 17, 20, 22, 24]\n\nfor max_depth in max_depth_grid:\n    rfc = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1, max_depth=max_depth)\n    temp_train_acc = []\n    temp_test_acc = []\n    for train_index, test_index in skf.split(X, y):\n        X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n        y_train, y_test = y[train_index], y[test_index]\n        rfc.fit(X_train, y_train)\n        temp_train_acc.append(rfc.score(X_train, y_train))\n        temp_test_acc.append(rfc.score(X_test, y_test))\n    train_acc.append(temp_train_acc)\n    test_acc.append(temp_test_acc)\n    \ntrain_acc, test_acc = np.asarray(train_acc), np.asarray(test_acc)\nprint(\"Best CV accuracy is {:.2f}% with {} max_depth\".format(max(test_acc.mean(axis=1))*100, \n                                                        max_depth_grid[np.argmax(test_acc.mean(axis=1))]))\n\nfig, ax = plt.subplots(figsize=(8, 4))\nax.plot(max_depth_grid, train_acc.mean(axis=1), alpha=0.5, color='blue', label='train')\nax.plot(max_depth_grid, test_acc.mean(axis=1), alpha=0.5, color='red', label='cv')\nax.fill_between(max_depth_grid, test_acc.mean(axis=1) - test_acc.std(axis=1), \n                test_acc.mean(axis=1) + test_acc.std(axis=1), color='#888888', alpha=0.4)\nax.fill_between(max_depth_grid, test_acc.mean(axis=1) - 2*test_acc.std(axis=1), \n                test_acc.mean(axis=1) + 2*test_acc.std(axis=1), color='#888888', alpha=0.2)\nax.legend(loc='best')\nax.set_ylim([0.88,1.02])\nax.set_ylabel(\"Accuracy\")\nax.set_xlabel(\"Max_depth\");","3ed4df07":"# Create lists to save accuracy values on the training and test sets\ntrain_acc = []\ntest_acc = []\ntemp_train_acc = []\ntemp_test_acc = []\nmin_samples_leaf_grid = [1, 3, 5, 7, 9, 11, 13, 15, 17, 20, 22, 24]\n\nfor min_samples_leaf in min_samples_leaf_grid:\n    rfc = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1, \n                                 min_samples_leaf=min_samples_leaf)\n    temp_train_acc = []\n    temp_test_acc = []\n    for train_index, test_index in skf.split(X, y):\n        X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n        y_train, y_test = y[train_index], y[test_index]\n        rfc.fit(X_train, y_train)\n        temp_train_acc.append(rfc.score(X_train, y_train))\n        temp_test_acc.append(rfc.score(X_test, y_test))\n    train_acc.append(temp_train_acc)\n    test_acc.append(temp_test_acc)\n    \ntrain_acc, test_acc = np.asarray(train_acc), np.asarray(test_acc)\nprint(\"Best CV accuracy is {:.2f}% with {} min_samples_leaf\".format(max(test_acc.mean(axis=1))*100, \n                                                        min_samples_leaf_grid[np.argmax(test_acc.mean(axis=1))]))","6c213f4c":"fig, ax = plt.subplots(figsize=(8, 4))\nax.plot(min_samples_leaf_grid, train_acc.mean(axis=1), alpha=0.5, color='blue', label='train')\nax.plot(min_samples_leaf_grid, test_acc.mean(axis=1), alpha=0.5, color='red', label='cv')\nax.fill_between(min_samples_leaf_grid, test_acc.mean(axis=1) - test_acc.std(axis=1), \n                test_acc.mean(axis=1) + test_acc.std(axis=1), color='#888888', alpha=0.4)\nax.fill_between(min_samples_leaf_grid, test_acc.mean(axis=1) - 2*test_acc.std(axis=1), \n                test_acc.mean(axis=1) + 2*test_acc.std(axis=1), color='#888888', alpha=0.2)\nax.legend(loc='best')\nax.set_ylim([0.88,1.02])\nax.set_ylabel(\"Accuracy\")\nax.set_xlabel(\"Min_samples_leaf\");","5334d9ba":"# Create lists to save accuracy values on the training and test sets\ntrain_acc = []\ntest_acc = []\ntemp_train_acc = []\ntemp_test_acc = []\nmax_features_grid = [2, 4, 6, 8, 10, 12, 14, 16]\n\nfor max_features in max_features_grid:\n    rfc = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1, \n                                 max_features=max_features)\n    temp_train_acc = []\n    temp_test_acc = []\n    for train_index, test_index in skf.split(X, y):\n        X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n        y_train, y_test = y[train_index], y[test_index]\n        rfc.fit(X_train, y_train)\n        temp_train_acc.append(rfc.score(X_train, y_train))\n        temp_test_acc.append(rfc.score(X_test, y_test))\n    train_acc.append(temp_train_acc)\n    test_acc.append(temp_test_acc)\n    \ntrain_acc, test_acc = np.asarray(train_acc), np.asarray(test_acc)\nprint(\"Best CV accuracy is {:.2f}% with {} max_features\".format(max(test_acc.mean(axis=1))*100, \n                                                        max_features_grid[np.argmax(test_acc.mean(axis=1))]))\n\nfig, ax = plt.subplots(figsize=(8, 4))\nax.plot(max_features_grid, train_acc.mean(axis=1), alpha=0.5, color='blue', label='train')\nax.plot(max_features_grid, test_acc.mean(axis=1), alpha=0.5, color='red', label='cv')\nax.fill_between(max_features_grid, test_acc.mean(axis=1) - test_acc.std(axis=1), \n                test_acc.mean(axis=1) + test_acc.std(axis=1), color='#888888', alpha=0.4)\nax.fill_between(max_features_grid, test_acc.mean(axis=1) - 2*test_acc.std(axis=1), \n                test_acc.mean(axis=1) + 2*test_acc.std(axis=1), color='#888888', alpha=0.2)\nax.legend(loc='best')\nax.set_ylim([0.88,1.02])\nax.set_ylabel(\"Accuracy\")\nax.set_xlabel(\"Max_features\");","337a0d62":"# Initialize the set of parameters for exhaustive search and fit \nparameters = {'max_features': [4, 7, 10, 13], \n              'min_samples_leaf': [1, 3, 5, 7], \n              'max_depth': [5, 10, 15, 20]}\nrfc = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)\ngcv = GridSearchCV(rfc, parameters, n_jobs=-1, cv=skf, verbose=1)\ngcv.fit(X, y)","c681744d":"gcv.best_params_, gcv.best_score_","1bced313":"#Source : https:\/\/github.com\/Yorko\/mlcourse.ai\/blob\/master\/jupyter_english\/topic05_ensembles_random_forests\/topic5_part2_random_forest.ipynb","6efdd5b5":"The figures above show that the decision boundary of the decision tree is quite jagged and has a lot of acute angles that suggest overfitting and a weak ability to generalize. We would have trouble making reliable predictions on new test data. In contrast, the bagging algorithm has a rather smooth boundary and has no obvious signs of overfitting.\n\nNow, let's investigate some parameters which can help us increase the model accuracy.","d9f05894":"\n\nIn our case, the optimal number of features is equal to 10. This is the value at which the best result is achieved.\n\nWe have seen how the learning curves change with different values of the basic parameters. Now, let's use GridSearch to find the optimal parameters for our example:\n","f6ce6a0e":"\n\nParameter max_depth copes well with the regularization of our model and it does not overfit as badly as before. The model accuracy has increased slightly.\n\nAnother important parameter worth tuning is min_samples_leaf. It also contributes to regularization.\n","0097b217":"\n\nIn this case, we do not see an improvement in accuracy on the validation set, but we significantly reduce the overfitting down to 2% while keeping the accuracy at about 92%.\n\nLet's consider the parameter max_features. For classification, the value $\\large \\sqrt{d}$ (the total number of features) is typically used as the default choice. Let's check whether it would be optimal to use 4 features in our case:\n","7feef8f1":"\n\nAs we can see from our graphs and the MSE values above, a random forest of 10 trees achieves a better result than a single decision tree and is comparable to bagging with 10 trees. The main difference between random forests and bagging is that, in a random forest, the best feature for a split is selected from a random subset of the available features while, in bagging, all features are considered for the next best split.\n\nWe can also look at the advantages of random forests and bagging in classification problems:\n","bf75e67d":"# ****Practice with random forests in a real problem****\n\nIn this example we will look at predicting customer churn. This is a classification problem, so we will use accuracy for model evaluation.\n\nFirst, let's build a simple classifier which we will use as a baseline. For the sake of simplicity, we will use only numeric features.","b7471782":"\n\nAs you can see, when a certain number of trees is reached, our accuracy on the test set is very close to the asymptote. You can decide by yourself which value would be the optimal number of trees for your problem.\n\nThe figures also show that we achieved 100% accuracy on the training set, which tells us that we overfit. In order to avoid overfitting, we need to add regularization parameters to our model.\n\nWe will start with the maximum depth of trees max_depth and fix the number of trees at 100:\n","c8f1408a":"We have accuracy equal to 91.18%. Now, let's try to improve this result, and take a look at the behavior of the learning curves when we change the basic parameters.\n\nLet's start with the number of trees:\n"}}