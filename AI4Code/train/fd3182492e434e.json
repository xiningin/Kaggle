{"cell_type":{"1307cb04":"code","7be0971f":"code","c3d3481d":"code","8247a5cc":"code","903625a8":"code","f0ee778a":"code","72fd6f0e":"code","9214e120":"code","5a404b4c":"code","3384c619":"code","067f2e5e":"code","3eda4ecc":"code","0e4ea05e":"code","8448e3c0":"code","2706f2d0":"code","edb5cece":"code","9fa5ff61":"code","33395c19":"code","d77105bc":"code","ec1e954c":"code","9ee521e3":"code","c9732c3a":"code","e971ffc8":"code","79dcd4d0":"code","436c1080":"code","60ecebaf":"code","3e21beb9":"code","065740a1":"code","37c37d89":"code","0aad9242":"code","eef8093f":"code","55ceeec3":"code","b2751a34":"code","26020705":"code","8b18dca2":"code","9b57429a":"code","2dbf7632":"code","6d28f758":"code","31765d4d":"code","c57bd52e":"code","9d140c2b":"code","f560d824":"code","98784d4b":"code","73e92586":"code","0a9f86aa":"code","5d91f080":"code","92b74775":"code","ba1e366a":"code","682b4a79":"code","24e82187":"code","4cee8f0c":"markdown","6b509e87":"markdown","4360bec3":"markdown","123e91de":"markdown","92a49854":"markdown","720a68e7":"markdown","21c07ff7":"markdown","f54e5974":"markdown","6d241b03":"markdown","bf5dc347":"markdown","4fba2bbd":"markdown","70c20d56":"markdown","315d4f3b":"markdown","e7b16129":"markdown","f1eb82a7":"markdown","6bceba5e":"markdown","32ced1cf":"markdown","d109ed3c":"markdown","d8939351":"markdown","008938fa":"markdown","4c6dafb6":"markdown","dca4d2b4":"markdown","b15307f6":"markdown","1167df16":"markdown","dc4bf2d2":"markdown","e9d3651e":"markdown","d5c2bb12":"markdown","a42d4dd7":"markdown","525b7fd1":"markdown","b70b4595":"markdown","154756b1":"markdown"},"source":{"1307cb04":"import sys\nimport numpy as np\nimport pandas as pd\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport sklearn as sk\nimport nltk\n#nltk.download('stopwords')\n#nltk.download('wordnet')\n\nprint('*'*50)\nprint('Numpy Version     : ', np.__version__)\nprint('Pandas Version    : ', pd.__version__)\nprint('Matplotlib Version: ', mpl.__version__)\nprint('Seaborn Version   : ', sns.__version__)\nprint('SKLearn Version   : ', sk.__version__)\nprint('NLTK Version      : ', nltk.__version__)\nprint('*'*50)","7be0971f":"#seaborn options\nsns.set_style('white')\n\n#pandas options\npd.options.display.max_rows = 100\npd.options.display.max_columns = 100\n\n#Reproducibility!\nseed = 42\nnum_folds = 10\nv_size = 0.2\nmetric = 'accuracy'\n\n#Random seeds\nnp.random.seed(seed)","c3d3481d":"raw_twcs = pd.read_csv('..\/input\/twcs\/twcs.csv',encoding='utf-8')","8247a5cc":"raw_twcs.sample(10)","903625a8":"raw_twcs.shape","f0ee778a":"raw_twcs.info()","72fd6f0e":"def missingData(data):\n    '''\n    @author steno\n    Check the missing data in the features.\n    This function returns  dataframe with the missing values.\n    '''\n    total = data.isnull().sum().sort_values(ascending = False)\n    percent = (data.isnull().sum()\/data.isnull().count()*100).sort_values(ascending = False)\n    md = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\n    md = md[md[\"Percent\"] > 0]\n    sns.set(style = 'darkgrid')\n    plt.figure(figsize = (8, 4))\n    plt.xticks(rotation='90')\n    sns.barplot(md.index, md[\"Percent\"],color=\"g\",alpha=0.8)\n    plt.xlabel('Features', fontsize=15)\n    plt.ylabel('Percent of missing values', fontsize=15)\n    plt.title('Percent missing data by feature', fontsize=15)\n    return md","9214e120":"missingData(raw_twcs)","5a404b4c":"#first inbound = Richiesta iniziale di un cliente\nfirst_inbound = raw_twcs[pd.isnull(raw_twcs.in_response_to_tweet_id) & raw_twcs.inbound]\n\ninbOutb = pd.merge(first_inbound, raw_twcs, left_on='tweet_id', \n                                  right_on='in_response_to_tweet_id').sample(frac=1)\n\n# Filter to only outbound replies (from companies)\ninbOutb = inbOutb[inbOutb.inbound_y ^ True]","3384c619":"inbOutb.shape","067f2e5e":"missingData(inbOutb)","3eda4ecc":"inbOutb.columns","0e4ea05e":"toDrop = ['tweet_id_x', 'inbound_x','response_tweet_id_x', 'in_response_to_tweet_id_x', \n          'tweet_id_y', 'inbound_y','response_tweet_id_y', 'in_response_to_tweet_id_y']","8448e3c0":"inbOutb.drop(toDrop, axis=1, inplace=True)\nprint('inbOutb shape: ', inbOutb.shape)","2706f2d0":"inbOutb.sample(5)","edb5cece":"plt.figure(figsize=(20, 10))\nsns.set_style('white')\nsns.countplot(x='author_id_y', data=inbOutb )\nplt.xticks(rotation = 90)\nplt.title(\"Distributions of Number of Companies' Replies \", fontsize = 20)\nplt.show()","9fa5ff61":"inbOutb.info()","33395c19":"inbOutb.shape","d77105bc":"def remove_uppercase(text):\n    text_lowercase = ' '.join(x.lower() for x in text.split())# It will discard all uppercases\n    return text_lowercase","ec1e954c":"inbOutb['text_x_clean'] = inbOutb['text_x'].apply(lambda x: remove_uppercase(x))\ninbOutb['text_y_clean'] = inbOutb['text_y'].apply(lambda x: remove_uppercase(x))\n#in modo da poter rimuovere i nomi delle compagnie\ninbOutb['author_id_y'] = inbOutb['author_id_y'].apply(lambda x: remove_uppercase(x)) ","9ee521e3":"inbOutb.head(3)","c9732c3a":"import string\nstring.punctuation","e971ffc8":"#Function to remove Punctuation\ndef remove_punct(text):\n    text_nopunct = \"\".join([char for char in text if char not in string.punctuation])# It will discard all punctuations\n    return text_nopunct","79dcd4d0":"inbOutb['text_x_clean'] = inbOutb['text_x_clean'].apply(lambda x: remove_punct(x))\ninbOutb['text_y_clean'] = inbOutb['text_y_clean'].apply(lambda x: remove_punct(x))","436c1080":"inbOutb.head(3)","60ecebaf":"#usernames = inbOutb['author_id_x'].unique()\ncompanies = inbOutb['author_id_y'].unique()","3e21beb9":"# gli username dei clienti sono numeri\ninbOutb['text_x_clean'] = inbOutb['text_x_clean'].str.replace('\\d+', '')\ninbOutb['text_y_clean'] = inbOutb['text_y_clean'].str.replace('\\d+', '')","065740a1":"inbOutb['text_x_clean'] = inbOutb['text_x_clean'].str.replace('|'.join(companies), '')\ninbOutb['text_y_clean'] = inbOutb['text_y_clean'].str.replace('|'.join(companies), '')","37c37d89":"inbOutb.head(3)","0aad9242":"inbOutb.shape","eef8093f":"#common worlds\n\nfreqX = pd.Series(' '.join(inbOutb['text_x_clean']).split()).value_counts()[:10]\nfreqY = pd.Series(' '.join(inbOutb['text_y_clean']).split()).value_counts()[:10]\nprint('FREQ X: \\n',freqX,'\\nFREQ Y: \\n', freqY)","55ceeec3":"#removing them\nfreqX = list(freqX.index)\nfreqY = list(freqY.index)\ninbOutb['text_x_clean'] = inbOutb['text_x_clean'].apply(lambda x: \" \".join(x for x in x.split() if x not in freqX))\ninbOutb['text_y_clean'] = inbOutb['text_y_clean'].apply(lambda x: \" \".join(x for x in x.split() if x not in freqY))","b2751a34":"rareX = pd.Series(' '.join(inbOutb['text_x_clean']).split()).value_counts()[-100:]\nrareY = pd.Series(' '.join(inbOutb['text_y_clean']).split()).value_counts()[-100:]\nprint('RARE X: \\n',rareX,'\\nRARE Y: \\n', rareY)","26020705":"#removing them\nrareX = list(rareX.index)\nrareY = list(rareY.index)\ninbOutb['text_x_clean'] = inbOutb['text_x_clean'].apply(lambda x: \" \".join(x for x in x.split() if x not in rareX))\ninbOutb['text_y_clean'] = inbOutb['text_y_clean'].apply(lambda x: \" \".join(x for x in x.split() if x not in rareY))","8b18dca2":"import re\n\n# Function to Tokenize words\ndef tokenize(text):\n    tokens = re.split('\\W+', text) #W+ means that either a word character (A-Za-z0-9_) or a dash (-) can go there.\n    return tokens","9b57429a":"inbOutb['text_x_tokenized'] = inbOutb['text_x_clean'].apply(lambda x: tokenize(x.lower())) \ninbOutb['text_y_tokenized'] = inbOutb['text_y_clean'].apply(lambda x: tokenize(x.lower()))\n#We convert to lower as Python is case-sensitive. ","2dbf7632":"inbOutb.head(3)","6d28f758":"import nltk\n\nstopword = nltk.corpus.stopwords.words('english') # All English Stopwords\n\n# Function to remove Stopwords\ndef remove_stopwords(tokenized_list):\n    text = [word for word in tokenized_list if word not in stopword]# To remove all stopwords\n    return text\n\ninbOutb['text_x_tokenized'] = inbOutb['text_x_tokenized'].apply(lambda x: remove_stopwords(x))\ninbOutb['text_y_tokenized'] = inbOutb['text_y_tokenized'].apply(lambda x: remove_stopwords(x))","31765d4d":"inbOutb.head(3)","c57bd52e":"#ps = nltk.PorterStemmer()\n\n#def stemming(tokenized_text):\n#    text = [ps.stem(word) for word in tokenized_text]\n#    return text","9d140c2b":"#inbOutb['text_x_stemmed'] = inbOutb['text_x_nostop'].apply(lambda x: stemming(x))\n#inbOutb['text_y_stemmed'] = inbOutb['text_y_nostop'].apply(lambda x: stemming(x))","f560d824":"wn = nltk.WordNetLemmatizer()\n\ndef lemmatizing(tokenized_text):\n    text = [wn.lemmatize(word) for word in tokenized_text]\n    return text","98784d4b":"inbOutb['text_x_lemmatized'] = inbOutb['text_x_tokenized'].apply(lambda x: lemmatizing(x))\ninbOutb['text_y_lemmatized'] = inbOutb['text_y_tokenized'].apply(lambda x: lemmatizing(x))","73e92586":"inbOutb.head(3)","0a9f86aa":"#inbOutb.to_csv('inbOutb.csv')","5d91f080":"## Load the library with the CountVectorizer method\nfrom sklearn.feature_extraction.text import CountVectorizer","92b74775":"questions = inbOutb['text_x_clean'].dropna()\nq = np.array(questions)","ba1e366a":"# Initialise the count vectorizer with the English stop words\ncountV = CountVectorizer(stop_words='english',\n                         max_features=10000)\n\n# Fit and transform the processed titles\nbagQuestions = countV.fit_transform(q)\n\nprint('BOW Questions: ',bagQuestions.shape)","682b4a79":"words = countV.get_feature_names()\ntotal_counts = np.zeros(len(words))\nfor t in bagQuestions:\n    total_counts += t.toarray()[0]\n    \ncount_dict = (zip(words, total_counts))\ncount_dict = sorted(count_dict, key=lambda x:x[1], reverse=True)\nd = dict(count_dict)","24e82187":"from wordcloud import WordCloud\n\nsns.set_style('white')\nplt.figure(figsize=(15,15))\nwc = WordCloud(background_color=\"white\",width=1000,height=1000, max_words=50,\n               relative_scaling=0.5,normalize_plurals=False).generate_from_frequencies(d)\nplt.title('WordCloud')\nplt.imshow(wc)\nplt.show()","4cee8f0c":"## 3.8 Stemming\n#### Stemming helps reduce a word to its stem form. It often makes sense to treat related words in the same way. It removes suffices, like \u201cing\u201d, \u201cly\u201d, \u201cs\u201d, etc. by a simple rule-based approach.\n#### It reduces the corpus of words but often the actual words get neglected. eg: Entitling,Entitled->Entitl","6b509e87":"## 3.9 Lemmatizing","4360bec3":"## 1.4 Checking Missing data","123e91de":"## 1.3 First data exploration","92a49854":"### The dataset is a csv, where each line is a tweet. Each conversation includes at least one customer request and one company reply. It is composed as follows:\n\n|feature|description|\n|-------|-----------|\n|id|A unique, anonymized ID for the Tweet. Referenced by response_tweet_id and in_response_to_tweet_id.|\n|id_author|A unique, anonymized user ID. @s in the dataset have been replaced with their associated anonymized user ID.|\n|inbound|Whether the tweet is \"inbound\" to a company doing customer support on Twitter. This feature is useful when re-organizing data for training conversational models.|\n|created_at|Date and time when the tweet was sent.|\n|text|Tweet content. Sensitive information like phone numbers and email addresses are replaced with mask values like __email__.response_tweet_id |\n|response_tweet_id|IDs of tweets that are responses to this tweet, comma-separated.|\n|in_response_to_tweet_id|IDs of the tweet is in response to, if any|\n\n### Some considerations:\n- The inbound feature is important because it allows you to distinguish between company and customer.\n- Obviously the 'text' function is the main source of information.\n- I will apply natural language processing techniques. In my opinion, it is possible to address the problem in two ways:\n    - Catalog customer tweets and identify the most discussed topics;\n    - Create a model that adapts to the tweets of the companies and try to answer a specific topic of a customer (to do)","720a68e7":"# Count Vectorizer","21c07ff7":"## Drop useless features","f54e5974":"#### Lemmatizing derives the canonical form (\u2018lemma\u2019) of a word. i.e the root form. It is better than stemming as it uses a dictionary-based approach i.e a morphological analysis to the root word.eg: Entitling, Entitled->Entitle\n#### In Short, Stemming is typically faster as it simply chops off the end of the word, without understanding the context of the word. Lemmatizing is slower and more accurate as it takes an informed analysis with the context of the word in mind.","6d241b03":"#### At this point we use the typical techniques of Natural Language Processing to take full advantage of the texts.","bf5dc347":"## 1.2 Get the data","4fba2bbd":"# Wordcloud with the most common words in customer requests","70c20d56":"## 3.5 Checking Most Rare Worlds","315d4f3b":"# 0. Setup","e7b16129":"## 0.1 Libraries used in the project","f1eb82a7":"# 3. Text Processing","6bceba5e":"## 3.4 Checking Most Common Worlds\nPreviously, we just removed commonly occurring words in a general sense. We can also remove commonly occurring words from our text data First, let\u2019s check the 10 most frequently occurring words in our text data then take call to remove or retain.","32ced1cf":"## 1.1 Notes on the project","d109ed3c":"## 3.3 Removing usernames \n- rimuovere nomi utenti, compagnie da text_x_clean\n- rimuovere nomi utenti, compagnie da text_y_clean","d8939351":"## This is my first NLP project. The functions are defined in the specific section, so don't worry if you don't find them immediately before they are used. I accept all your criticisms,and, if you liked the kernel, please rate it. This will give me the motivation to improve myself. Thank you.\n### P.S. This is an alpha version**\n","008938fa":"# 1. Project Start!","4c6dafb6":"# 2. Feature Extraction\nNow I create another dataset containing a client's initial request and the company's response.\n","dca4d2b4":"#### Now the dataset is doubled in size (features), as each line contains:\n- a customer request (text_x)\n- a reply from the company (text_y)\n- Related related features\n\n#### From which we can easily verify that:\n- the 'inbound_x' feature is always True;\n- the 'inbound_y' feature is always False;","b15307f6":"## 3.1 Lower case","1167df16":"## 0.2 Basic setup","dc4bf2d2":"#### Referring to this dataset it is possible to observe that:\n- The tweets refer to 108 different companies;\n- AmazonHelp, AppleSupport and Uber are services with multiple tweets;\n- The dataset is full of companies with few interactions (needs more information).\n\n#### However:\n- Tweets related to Amazon may be less discriminating;\n- For a better analysis it is necessary to categorize the companies to select more specific tweets to the relative services;","e9d3651e":"## 3.6 Tokenizing\n#### Tokenizing separates text into units such as sentences or words. It gives structure to previously unstructured text.","d5c2bb12":"## 2.3 Companies","a42d4dd7":"## The project ends here at least for the moment. Later I will implement techniques to improve the analysis. If you liked the kernel give me an upvote, if you have any advice it is welcome. Thank you.","525b7fd1":"## 3.2 Remove Puntuaction\n#### Punctuation can provide grammatical context to a sentence which supports our understanding. But for our vectorizer which counts the number of words and not the context, it does not add value, so we remove all special characters. eg: How are you?->How are you.","b70b4595":"## 3.7 Remove StopWords\n#### Stopwords are common words that will likely appear in any text. They don\u2019t tell us much about our data so we remove them. eg: silver or lead is fine for me-> silver, lead, fine.","154756b1":"#### The 'in_response_to_tweet_id_x' feature is totally composed of NaN, as the client's message is not a response to any tweets, because it is the beginning of the conversation. So this feature will be dropped."}}