{"cell_type":{"2f937cc5":"code","1e3000e7":"code","c32a1125":"code","b13b212e":"code","671a88ba":"code","827376e8":"code","5463857c":"code","6f2c4cff":"code","4a03c9e7":"code","af750a76":"code","29e4cc2e":"code","8de11800":"code","a912221c":"code","29019c1d":"code","c1eeac90":"code","455d27b3":"code","fac8206e":"code","2b8723fa":"code","f7b6ea41":"code","367ed935":"code","ed77f399":"code","e9bd53bb":"code","a0baf8d3":"code","0cc693f2":"code","fc5f2617":"code","c4b66374":"code","75eda814":"code","f33596b0":"code","45639b59":"code","71279e31":"code","c26215d5":"code","189e8653":"code","9cfcf732":"code","e3238709":"code","b1e39889":"code","560a3f56":"code","495433fd":"code","2fc43123":"code","81238fc4":"code","c80c277d":"code","19146fe1":"code","f5b8b47c":"code","e134cfec":"code","56ccb36c":"code","96010005":"code","ba2fb86a":"code","2e2eedff":"code","77e47c45":"code","db35cbc2":"markdown","a1725030":"markdown","d5650810":"markdown","c01dc667":"markdown","3eb0a173":"markdown","8cce2940":"markdown","80c1f0ef":"markdown","25a73c6b":"markdown","1a85ca96":"markdown","b12c6d04":"markdown","4aab09b4":"markdown","b8097d7d":"markdown","44d99c15":"markdown","b5a4b5ed":"markdown","bcd0285c":"markdown","b1089f77":"markdown","5ff63083":"markdown","f20e2385":"markdown"},"source":{"2f937cc5":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","1e3000e7":"summary = pd.read_csv('\/kaggle\/input\/news-summary\/news_summary.csv', encoding='iso-8859-1')\nraw = pd.read_csv('\/kaggle\/input\/news-summary\/news_summary_more.csv', encoding='iso-8859-1')","c32a1125":"pre1 =  raw.iloc[:,0:2].copy()\n# pre1['head + text'] = pre1['headlines'].str.cat(pre1['text'], sep =\" \") \n\npre2 = summary.iloc[:,0:6].copy()\npre2['text'] = pre2['author'].str.cat(pre2['date'].str.cat(pre2['read_more'].str.cat(pre2['text'].str.cat(pre2['ctext'], sep = \" \"), sep =\" \"),sep= \" \"), sep = \" \")","b13b212e":"pre = pd.DataFrame()\npre['text'] = pd.concat([pre1['text'], pre2['text']], ignore_index=True)\npre['summary'] = pd.concat([pre1['headlines'],pre2['headlines']],ignore_index = True)","671a88ba":"pre.head(2)","827376e8":"#LSTM with Attention\n#pip install keras-self-attention\n\npre['text'][:10]\n\n","5463857c":"import re\n\n#Removes non-alphabetic characters:\ndef text_strip(column):\n    for row in column:\n        \n        #ORDER OF REGEX IS VERY VERY IMPORTANT!!!!!!\n        \n        row=re.sub(\"(\\\\t)\", ' ', str(row)).lower() #remove escape charecters\n        row=re.sub(\"(\\\\r)\", ' ', str(row)).lower() \n        row=re.sub(\"(\\\\n)\", ' ', str(row)).lower()\n        \n        row=re.sub(\"(__+)\", ' ', str(row)).lower()   #remove _ if it occors more than one time consecutively\n        row=re.sub(\"(--+)\", ' ', str(row)).lower()   #remove - if it occors more than one time consecutively\n        row=re.sub(\"(~~+)\", ' ', str(row)).lower()   #remove ~ if it occors more than one time consecutively\n        row=re.sub(\"(\\+\\++)\", ' ', str(row)).lower()   #remove + if it occors more than one time consecutively\n        row=re.sub(\"(\\.\\.+)\", ' ', str(row)).lower()   #remove . if it occors more than one time consecutively\n        \n        row=re.sub(r\"[<>()|&\u00a9\u00f8\\[\\]\\'\\\",;?~*!]\", ' ', str(row)).lower() #remove <>()|&\u00a9\u00f8\"',;?~*!\n        \n        row=re.sub(\"(mailto:)\", ' ', str(row)).lower() #remove mailto:\n        row=re.sub(r\"(\\\\x9\\d)\", ' ', str(row)).lower() #remove \\x9* in text\n        row=re.sub(\"([iI][nN][cC]\\d+)\", 'INC_NUM', str(row)).lower() #replace INC nums to INC_NUM\n        row=re.sub(\"([cC][mM]\\d+)|([cC][hH][gG]\\d+)\", 'CM_NUM', str(row)).lower() #replace CM# and CHG# to CM_NUM\n        \n        \n        row=re.sub(\"(\\.\\s+)\", ' ', str(row)).lower() #remove full stop at end of words(not between)\n        row=re.sub(\"(\\-\\s+)\", ' ', str(row)).lower() #remove - at end of words(not between)\n        row=re.sub(\"(\\:\\s+)\", ' ', str(row)).lower() #remove : at end of words(not between)\n        \n        row=re.sub(\"(\\s+.\\s+)\", ' ', str(row)).lower() #remove any single charecters hanging between 2 spaces\n        \n        #Replace any url as such https:\/\/abc.xyz.net\/browse\/sdf-5327 ====> abc.xyz.net\n        try:\n            url = re.search(r'((https*:\\\/*)([^\\\/\\s]+))(.[^\\s]+)', str(row))\n            repl_url = url.group(3)\n            row = re.sub(r'((https*:\\\/*)([^\\\/\\s]+))(.[^\\s]+)',repl_url, str(row))\n        except:\n            pass #there might be emails with no url in them\n        \n\n        \n        row = re.sub(\"(\\s+)\",' ',str(row)).lower() #remove multiple spaces\n        \n        #Should always be last\n        row=re.sub(\"(\\s+.\\s+)\", ' ', str(row)).lower() #remove any single charecters hanging between 2 spaces\n\n        \n        \n        yield row\n\n\n","6f2c4cff":"brief_cleaning1 = text_strip(pre['text'])\nbrief_cleaning2 = text_strip(pre['summary'])","4a03c9e7":"from time import time\nimport spacy\nnlp = spacy.load('en', disable=['ner', 'parser']) # disabling Named Entity Recognition for speed\n\n#Taking advantage of spaCy .pipe() method to speed-up the cleaning process:\n#If data loss seems to be happening(i.e len(text) = 50 instead of 75 etc etc) in this cell , decrease the batch_size parametre \n\nt = time()\n\n#Batch the data points into 5000 and run on all cores for faster preprocessing\ntext = [str(doc) for doc in nlp.pipe(brief_cleaning1, batch_size=5000, n_threads=-1)]\n\n#Takes 7-8 mins\nprint('Time to clean up everything: {} mins'.format(round((time() - t) \/ 60, 2)))","af750a76":"#Taking advantage of spaCy .pipe() method to speed-up the cleaning process:\n\n\nt = time()\n\n#Batch the data points into 5000 and run on all cores for faster preprocessing\nsummary = ['_START_ '+ str(doc) + ' _END_' for doc in nlp.pipe(brief_cleaning2, batch_size=5000, n_threads=-1)]\n\n#Takes 7-8 mins\nprint('Time to clean up everything: {} mins'.format(round((time() - t) \/ 60, 2)))","29e4cc2e":"text[0]","8de11800":"summary[0]","a912221c":"pre['cleaned_text'] = pd.Series(text)\npre['cleaned_summary'] = pd.Series(summary)","29019c1d":"text_count = []\nsummary_count = []","c1eeac90":"for sent in pre['cleaned_text']:\n    text_count.append(len(sent.split()))\nfor sent in pre['cleaned_summary']:\n    summary_count.append(len(sent.split()))","455d27b3":"graph_df= pd.DataFrame()\ngraph_df['text']=text_count\ngraph_df['summary']=summary_count","fac8206e":"import matplotlib.pyplot as plt\n\ngraph_df.hist(bins = 5)\nplt.show()","2b8723fa":"#Check how much % of summary have 0-15 words\ncnt=0\nfor i in pre['cleaned_summary']:\n    if(len(i.split())<=15):\n        cnt=cnt+1\nprint(cnt\/len(pre['cleaned_summary']))","f7b6ea41":"#Check how much % of text have 0-70 words\ncnt=0\nfor i in pre['cleaned_text']:\n    if(len(i.split())<=100):\n        cnt=cnt+1\nprint(cnt\/len(pre['cleaned_text']))","367ed935":"#Model to summarize the text between 0-15 words for Summary and 0-100 words for Text\nmax_text_len=100\nmax_summary_len=15","ed77f399":"#Select the Summaries and Text between max len defined above\n\ncleaned_text =np.array(pre['cleaned_text'])\ncleaned_summary=np.array(pre['cleaned_summary'])\n\nshort_text=[]\nshort_summary=[]\n\nfor i in range(len(cleaned_text)):\n    if(len(cleaned_summary[i].split())<=max_summary_len and len(cleaned_text[i].split())<=max_text_len):\n        short_text.append(cleaned_text[i])\n        short_summary.append(cleaned_summary[i])\n        \npost_pre=pd.DataFrame({'text':short_text,'summary':short_summary})","e9bd53bb":"post_pre.head(2)","a0baf8d3":"#Add sostok and eostok at \npost_pre['summary'] = post_pre['summary'].apply(lambda x : 'sostok '+ x + ' eostok')\n","0cc693f2":"post_pre.head(2)","fc5f2617":"from sklearn.model_selection import train_test_split\nx_tr,x_val,y_tr,y_val=train_test_split(np.array(post_pre['text']),np.array(post_pre['summary']),test_size=0.1,random_state=0,shuffle=True)","c4b66374":"#Lets tokenize the text to get the vocab count , you can use Spacy here also\n\nfrom keras.preprocessing.text import Tokenizer \nfrom keras.preprocessing.sequence import pad_sequences\n\n#prepare a tokenizer for reviews on training data\nx_tokenizer = Tokenizer() \nx_tokenizer.fit_on_texts(list(x_tr))","75eda814":"thresh=4\n\ncnt=0\ntot_cnt=0\nfreq=0\ntot_freq=0\n\nfor key,value in x_tokenizer.word_counts.items():\n    tot_cnt=tot_cnt+1\n    tot_freq=tot_freq+value\n    if(value<thresh):\n        cnt=cnt+1\n        freq=freq+value\n    \nprint(\"% of rare words in vocabulary:\",(cnt\/tot_cnt)*100)\nprint(\"Total Coverage of rare words:\",(freq\/tot_freq)*100)","f33596b0":"\n#prepare a tokenizer for reviews on training data\nx_tokenizer = Tokenizer(num_words=tot_cnt-cnt) \nx_tokenizer.fit_on_texts(list(x_tr))\n\n#convert text sequences into integer sequences (i.e one-hot encodeing all the words)\nx_tr_seq    =   x_tokenizer.texts_to_sequences(x_tr) \nx_val_seq   =   x_tokenizer.texts_to_sequences(x_val)\n\n#padding zero upto maximum length\nx_tr    =   pad_sequences(x_tr_seq,  maxlen=max_text_len, padding='post')\nx_val   =   pad_sequences(x_val_seq, maxlen=max_text_len, padding='post')\n\n#size of vocabulary ( +1 for padding token)\nx_voc   =  x_tokenizer.num_words + 1\n\nprint(\"Size of vocabulary in X = {}\".format(x_voc))","45639b59":"#prepare a tokenizer for reviews on training data\ny_tokenizer = Tokenizer()   \ny_tokenizer.fit_on_texts(list(y_tr))","71279e31":"thresh=6\n\ncnt=0\ntot_cnt=0\nfreq=0\ntot_freq=0\n\nfor key,value in y_tokenizer.word_counts.items():\n    tot_cnt=tot_cnt+1\n    tot_freq=tot_freq+value\n    if(value<thresh):\n        cnt=cnt+1\n        freq=freq+value\n    \nprint(\"% of rare words in vocabulary:\",(cnt\/tot_cnt)*100)\nprint(\"Total Coverage of rare words:\",(freq\/tot_freq)*100)","c26215d5":"#prepare a tokenizer for reviews on training data\ny_tokenizer = Tokenizer(num_words=tot_cnt-cnt) \ny_tokenizer.fit_on_texts(list(y_tr))\n\n#convert text sequences into integer sequences (i.e one hot encode the text in Y)\ny_tr_seq    =   y_tokenizer.texts_to_sequences(y_tr) \ny_val_seq   =   y_tokenizer.texts_to_sequences(y_val) \n\n#padding zero upto maximum length\ny_tr    =   pad_sequences(y_tr_seq, maxlen=max_summary_len, padding='post')\ny_val   =   pad_sequences(y_val_seq, maxlen=max_summary_len, padding='post')\n\n#size of vocabulary\ny_voc  =   y_tokenizer.num_words +1\nprint(\"Size of vocabulary in Y = {}\".format(y_voc))","189e8653":"ind=[]\nfor i in range(len(y_tr)):\n    cnt=0\n    for j in y_tr[i]:\n        if j!=0:\n            cnt=cnt+1\n    if(cnt==2):\n        ind.append(i)\n\ny_tr=np.delete(y_tr,ind, axis=0)\nx_tr=np.delete(x_tr,ind, axis=0)","9cfcf732":"ind=[]\nfor i in range(len(y_val)):\n    cnt=0\n    for j in y_val[i]:\n        if j!=0:\n            cnt=cnt+1\n    if(cnt==2):\n        ind.append(i)\n\ny_val=np.delete(y_val,ind, axis=0)\nx_val=np.delete(x_val,ind, axis=0)","e3238709":"from keras import backend as K \nimport gensim\nfrom numpy import *\nimport numpy as np\nimport pandas as pd \nimport re\nfrom bs4 import BeautifulSoup\nfrom keras.preprocessing.text import Tokenizer \nfrom keras.preprocessing.sequence import pad_sequences\nfrom nltk.corpus import stopwords\nfrom tensorflow.keras.layers import Input, LSTM, Embedding, Dense, Concatenate, TimeDistributed\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.callbacks import EarlyStopping\nimport warnings\npd.set_option(\"display.max_colwidth\", 200)\nwarnings.filterwarnings(\"ignore\")\n\nprint(\"Size of vocabulary from the w2v model = {}\".format(x_voc))\n\nK.clear_session()\n\nlatent_dim = 300\nembedding_dim=200\n\n# Encoder\nencoder_inputs = Input(shape=(max_text_len,))\n\n#embedding layer\nenc_emb =  Embedding(x_voc, embedding_dim,trainable=True)(encoder_inputs)\n\n#encoder lstm 1\nencoder_lstm1 = LSTM(latent_dim,return_sequences=True,return_state=True,dropout=0.4,recurrent_dropout=0.4)\nencoder_output1, state_h1, state_c1 = encoder_lstm1(enc_emb)\n\n#encoder lstm 2\nencoder_lstm2 = LSTM(latent_dim,return_sequences=True,return_state=True,dropout=0.4,recurrent_dropout=0.4)\nencoder_output2, state_h2, state_c2 = encoder_lstm2(encoder_output1)\n\n#encoder lstm 3\nencoder_lstm3=LSTM(latent_dim, return_state=True, return_sequences=True,dropout=0.4,recurrent_dropout=0.4)\nencoder_outputs, state_h, state_c= encoder_lstm3(encoder_output2)\n\n# Set up the decoder, using `encoder_states` as initial state.\ndecoder_inputs = Input(shape=(None,))\n\n#embedding layer\ndec_emb_layer = Embedding(y_voc, embedding_dim,trainable=True)\ndec_emb = dec_emb_layer(decoder_inputs)\n\ndecoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True,dropout=0.4,recurrent_dropout=0.2)\ndecoder_outputs,decoder_fwd_state, decoder_back_state = decoder_lstm(dec_emb,initial_state=[state_h, state_c])\n\n#dense layer\ndecoder_dense =  TimeDistributed(Dense(y_voc, activation='softmax'))\ndecoder_outputs = decoder_dense(decoder_outputs)\n\n# Define the model \nmodel = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n\nmodel.summary()\n","b1e39889":"model.compile(optimizer='rmsprop', loss='sparse_categorical_crossentropy')\n","560a3f56":"es = EarlyStopping(monitor='val_loss', mode='min', verbose=1,patience=2)\n","495433fd":"history=model.fit([x_tr,y_tr[:,:-1]], y_tr.reshape(y_tr.shape[0],y_tr.shape[1], 1)[:,1:] ,epochs=50,callbacks=[es],batch_size=128, validation_data=([x_val,y_val[:,:-1]], y_val.reshape(y_val.shape[0],y_val.shape[1], 1)[:,1:]))\n","2fc43123":"from matplotlib import pyplot\npyplot.plot(history.history['loss'], label='train')\npyplot.plot(history.history['val_loss'], label='test')\npyplot.legend()\npyplot.show()","81238fc4":"reverse_target_word_index=y_tokenizer.index_word\nreverse_source_word_index=x_tokenizer.index_word\ntarget_word_index=y_tokenizer.word_index","c80c277d":"# Encode the input sequence to get the feature vector\nencoder_model = Model(inputs=encoder_inputs,outputs=[encoder_outputs, state_h, state_c])\n\n# Decoder setup\n# Below tensors will hold the states of the previous time step\ndecoder_state_input_h = Input(shape=(latent_dim,))\ndecoder_state_input_c = Input(shape=(latent_dim,))\ndecoder_hidden_state_input = Input(shape=(max_text_len,latent_dim))\n\n# Get the embeddings of the decoder sequence\ndec_emb2= dec_emb_layer(decoder_inputs) \n# To predict the next word in the sequence, set the initial states to the states from the previous time step\ndecoder_outputs2, state_h2, state_c2 = decoder_lstm(dec_emb2, initial_state=[decoder_state_input_h, decoder_state_input_c])\n\n# A dense softmax layer to generate prob dist. over the target vocabulary\ndecoder_outputs2 = decoder_dense(decoder_outputs2) \n\n# Final decoder model\ndecoder_model = Model(\n    [decoder_inputs] + [decoder_hidden_state_input,decoder_state_input_h, decoder_state_input_c],\n    [decoder_outputs2] + [state_h2, state_c2])","19146fe1":"def decode_sequence(input_seq):\n    # Encode the input as state vectors.\n    e_out, e_h, e_c = encoder_model.predict(input_seq)\n    \n    # Generate empty target sequence of length 1.\n    target_seq = np.zeros((1,1))\n    \n    # Populate the first word of target sequence with the start word.\n    target_seq[0, 0] = target_word_index['sostok']\n\n    stop_condition = False\n    decoded_sentence = ''\n    while not stop_condition:\n      \n        output_tokens, h, c = decoder_model.predict([target_seq] + [e_out, e_h, e_c])\n\n        # Sample a token\n        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n        sampled_token = reverse_target_word_index[sampled_token_index]\n        \n        if(sampled_token!='eostok'):\n            decoded_sentence += ' '+sampled_token\n\n        # Exit condition: either hit max length or find stop word.\n        if (sampled_token == 'eostok'  or len(decoded_sentence.split()) >= (max_summary_len-1)):\n            stop_condition = True\n\n        # Update the target sequence (of length 1).\n        target_seq = np.zeros((1,1))\n        target_seq[0, 0] = sampled_token_index\n\n        # Update internal states\n        e_h, e_c = h, c\n\n    return decoded_sentence","f5b8b47c":"def seq2summary(input_seq):\n    newString=''\n    for i in input_seq:\n        if((i!=0 and i!=target_word_index['sostok']) and i!=target_word_index['eostok']):\n            newString=newString+reverse_target_word_index[i]+' '\n    return newString\n\ndef seq2text(input_seq):\n    newString=''\n    for i in input_seq:\n        if(i!=0):\n            newString=newString+reverse_source_word_index[i]+' '\n    return newString","e134cfec":"for i in range(0,100):\n    print(\"Review:\",seq2text(x_tr[i]))\n    print(\"Original summary:\",seq2summary(y_tr[i]))\n    print(\"Predicted summary:\",decode_sequence(x_tr[i].reshape(1,max_text_len)))\n    print(\"\\n\")","56ccb36c":"for i in range(0,100):\n    print(\"Review:\",seq2text(x_val[i]))\n    print(\"Original summary:\",seq2summary(y_val[i]))\n    print(\"Predicted summary:\",decode_sequence(x_val[i].reshape(1,max_text_len)))\n    print(\"\\n\")","96010005":"test_art = \"\"\" \"\"\"","ba2fb86a":"\n#prepare a tokenizer for reviews on training data\nx_tokenizer = Tokenizer(num_words=tot_cnt-cnt) \nx_tokenizer.fit_on_texts(list(test_art))\n\n#convert text sequences into integer sequences (i.e one-hot encodeing all the words)\ntest_art_seq    =   x_tokenizer.texts_to_sequences(test_art) \n\n#padding zero upto maximum length\ntest_art    =   pad_sequences(test_art_seq,  maxlen=max_text_len, padding='post')","2e2eedff":"print(test_art.shape)\nprint(test_art)","77e47c45":"for i in range(0,100):\n    print(\"Review:\",seq2text(test_art[i]))\n    print(\"Original summary:\",seq2summary(test_art[i]))\n    print(\"Predicted summary:\",decode_sequence(test_art[i].reshape(1,max_text_len)))\n    print(\"\\n\")","db35cbc2":"**We are defining a function below which is the implementation of the inference process**","a1725030":"> **Perform Data Cleansing**","d5650810":"**Visualize the model learning**","c01dc667":"**Let us define the functions to convert an integer sequence to a word sequence for summary as well as the reviews:**\n","3eb0a173":"**Next, let\u2019s build the dictionary to convert the index to word for target and source vocabulary:**","8cce2940":"**Vectorizing the news article now**\n* \n**Note that during vectorization , if the word was not present in the bag of words which were converted to tokens , it returns the word vector as 0**","80c1f0ef":"**RARE WORD ANALYSIS FOR X i.e 'text'**\n* tot_cnt gives the size of vocabulary (which means every unique words in the text)\n\n* cnt gives me the no. of rare words whose count falls below threshold\n\n* tot_cnt - cnt gives me the top most common words","25a73c6b":"We will now remove \"Summary\" i.e Y (both train and val) which has only _START_ and _END_","1a85ca96":"Split the data to TRAIN and VALIDATION sets","b12c6d04":"**Run the model over Validation Set**","4aab09b4":"**Start fitting the model with the data**","b8097d7d":"**RARE WORD ANALYSIS FOR Y i.e 'summary'**\n* tot_cnt gives the size of vocabulary (which means every unique words in the text)\n\n* cnt gives me the no. of rare words whose count falls below threshold\n\n* tot_cnt - cnt gives me the top most common words","44d99c15":"**Seq2Seq LSTM Modelling**\n![final.jpg](attachment:final.jpg)","b5a4b5ed":"**Run the model over the data to see the results**","bcd0285c":"**Resources - **\n1. https:\/\/www.analyticsvidhya.com\/blog\/2019\/06\/comprehensive-guide-text-summarization-using-deep-learning-python\/\n2. https:\/\/www.analyticsvidhya.com\/blog\/2018\/11\/introduction-text-summarization-textrank-python\/\n3. https:\/\/towardsdatascience.com\/understand-text-summarization-and-create-your-own-summarizer-in-python-b26a9f09fc70\n4. https:\/\/github.com\/aravindpai\/How-to-build-own-text-summarizer-using-deep-learning","b1089f77":"**Run the model over our own test article**","5ff63083":"**SEQ2SEQ MODEL BUILDING **","f20e2385":"**Run the model over vectorized text**"}}