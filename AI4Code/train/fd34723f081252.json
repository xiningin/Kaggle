{"cell_type":{"5765c9bf":"code","047875b0":"code","fa142323":"code","0a460624":"code","610595cc":"code","6541a929":"code","b72c8ccb":"code","955c8b13":"code","abf4e55f":"code","6341074c":"code","5bfa6a7d":"code","d9129716":"code","6fc0d320":"code","ff5ce56d":"code","74c442bd":"code","96be2815":"code","3ff51d0d":"code","c8b071de":"code","f42c0b25":"code","c1b03ad3":"code","c8d986ca":"code","d0aa5f10":"code","21a94477":"code","475795a8":"code","09cd394d":"code","e10531c5":"markdown","6a25afc0":"markdown","bf555a20":"markdown","83e4654e":"markdown","ca48c5c4":"markdown","807d0fad":"markdown"},"source":{"5765c9bf":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\n\nimport matplotlib.pyplot as plt\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nfrom scipy import stats\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport seaborn as sns\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","047875b0":"from sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import precision_score\nfrom sklearn.metrics import recall_score\nfrom sklearn.metrics import f1_score\nfrom sklearn.metrics import roc_curve, auc\nfrom sklearn.preprocessing import label_binarize\nfrom sklearn.linear_model import RidgeClassifier\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.gaussian_process import GaussianProcessClassifier\nfrom sklearn.gaussian_process.kernels import RBF\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import precision_recall_curve\nfrom sklearn.metrics import average_precision_score\nfrom sklearn.linear_model import RidgeClassifierCV\nfrom sklearn import preprocessing","fa142323":"train_data=pd.read_csv(r\"..\/input\/bank-train.csv\")\ntest_data=pd.read_csv(r\"..\/input\/bank-test.csv\")","0a460624":"y_train=train_data[\"y\"]\nX_train=train_data.drop(\"y\",axis=1)\ncutoff=X_train.shape[0]\nX=X_train.append(test_data)\nX.drop(\"duration\",axis=1,inplace=True)","610595cc":"X_dummies=pd.get_dummies(X,drop_first=True)","6541a929":"X=X_dummies.iloc[:cutoff]\nX_valid=X_dummies.iloc[cutoff:]\ny=y_train\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)","b72c8ccb":"base_clf = LogisticRegression(random_state=0).fit(X_train, y_train)\ny_pred=base_clf.predict(X_test)\ny_score = base_clf.fit(X_train, y_train).decision_function(X_test)\n# Compute ROC curve and ROC area for each class\nfpr, tpr, _ = roc_curve(y_test, y_score)\nroc_auc = auc(fpr, tpr)\nC=confusion_matrix(y_test,y_pred)\nsns.heatmap(C \/ C.astype(np.float).sum(axis=1))\nplt.title(\"Confusion Matrix Normalized\")","955c8b13":"def print_classfiction_metrics(testy,yhat_classes):\n    # accuracy: (tp + tn) \/ (p + n)\n    accuracy = accuracy_score(testy, yhat_classes)\n    print('Accuracy: %f' % accuracy)\n    # precision tp \/ (tp + fp)\n    precision = precision_score(testy, yhat_classes)\n    print('Precision: %f' % precision)\n    # recall: tp \/ (tp + fn)\n    recall = recall_score(testy, yhat_classes)\n    print('Recall: %f' % recall)\n    # f1: 2 tp \/ (2 tp + fp + fn)\n    f1 = f1_score(testy, yhat_classes)\n    print('F1 score: %f' % f1)\n    \n    ","abf4e55f":"y_test.value_counts(normalize=True)","6341074c":"print_classfiction_metrics(y_test,y_pred)\nprint(\"Area Under ROC Curve:\", roc_auc)","5bfa6a7d":"y_train=train_data[\"y\"]\nX_train=train_data.drop(\"y\",axis=1)\ncutoff=X_train.shape[0]\nX=X_train.append(test_data)\nX.drop(\"duration\",axis=1,inplace=True)","d9129716":"X_transform=X.copy()\nX_transform[\"age\"]=np.log(X_transform[\"age\"])\n#Transform the campaign varaible \nX_transform[\"campaign\"]=np.log(X_transform[\"campaign\"])\n#Transform pdays to a category: \nbins=[0,7,14,21,28,1000]\nX_transform[\"pdays\"]=pd.cut(X_transform[\"pdays\"],bins, labels=[\"OneWeek\", \"TwoWeek\", \"ThreeWeek\",\"FourWeek\",\"NoPreviousCampaign\"],right=False)","6fc0d320":"X_transform=pd.get_dummies(X_transform,drop_first=True)","ff5ce56d":"X_transform.head(1)","74c442bd":"numerics=[\"age\",\"campaign\",\"previous\",\"emp.var.rate\",\"cons.price.idx\",\"cons.conf.idx\",\"euribor3m\",\"nr.employed\"]\nX_transform[numerics] = preprocessing.scale(X_transform[numerics])","96be2815":"X_transform.head(1)","3ff51d0d":"X_transform.shape","c8b071de":"X=X_transform.iloc[:cutoff]\ny=y_train\nX_valid=X_transform.iloc[cutoff:]\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)","f42c0b25":"y = label_binarize(y, classes=[0, 1])\nn_classes = y.shape[1]\nbase_clf = LogisticRegression(random_state=0).fit(X_train, y_train)\ny_pred=base_clf.predict(X_test)\ny_score = base_clf.fit(X_train, y_train).decision_function(X_test)\n# Compute ROC curve and ROC area for each class\nfpr, tpr, _ = roc_curve(y_test, y_score)\nroc_auc = auc(fpr, tpr)\nC=confusion_matrix(y_test,y_pred)\nsns.heatmap(C \/ C.astype(np.float).sum(axis=1))\nplt.title(\"Confusion Matrix Normalized\")","c1b03ad3":"y_test.value_counts(normalize=True)","c8d986ca":"print_classfiction_metrics(y_test,y_pred)\nprint(\"Area Under ROC Curve:\", roc_auc)","d0aa5f10":"#Recall is kind of low. the model is not good at covering\n#the successful phone calls. ","21a94477":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\nrg_base_clf = RidgeClassifier().fit(X_train, y_train)\ny_score = rg_base_clf.fit(X_train, y_train).decision_function(X_test)\ny_pred=rg_base_clf.predict(X_test)\nfpr, tpr, _ = roc_curve(y_test, y_score)\nroc_auc = auc(fpr, tpr)\nprint_classfiction_metrics(y_test,y_pred)\nprint(\"Area Under ROC Curve\",roc_auc)","475795a8":"result=rg_base_clf.predict(X_valid)\nsubmission = pd.concat([test_data[\"id\"], pd.Series(result)], axis = 1)\nsubmission.columns = ['id', 'Predicted']\nsubmission.to_csv('submission.csv', index=False)","09cd394d":"#Are Under curve also dropped ","e10531c5":"names = [\"Nearest Neighbors\", \n         \"Decision Tree\", \"Random Forest\", \"Neural Net\", \"AdaBoost\",\n         \"Naive Bayes\", \"QDA\"]\n\nclassifiers = [\n    KNeighborsClassifier(3),\n    DecisionTreeClassifier(max_depth=5),\n    RandomForestClassifier(max_depth=5, n_estimators=10, max_features=1),\n    MLPClassifier(alpha=1, max_iter=1000),\n    AdaBoostClassifier(),\n    GaussianNB(),\n    QuadraticDiscriminantAnalysis()]\n\n\n    # iterate over classifiers\nfor name, clf in zip(names, classifiers):\n    if name==\"Nearest Neighbors\":\n        X_new = StandardScaler().fit_transform(X)\n        X_train, X_test, y_train, y_test = train_test_split(X_new, y, test_size=0.33, random_state=42)\n    else: \n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n    clf.fit(X_train, y_train.ravel())\n    if hasattr(clf, \"decision_function\"):\n        y_score = clf.decision_function(X_test)\n    else:\n        y_score = clf.predict_proba(X_test)[:, 1]\n    y_pred=clf.predict(X_test)\n    fpr, tpr, _ = roc_curve(y_test.ravel(), y_score)\n    roc_auc = auc(fpr, tpr)\n    print_classfiction_metrics(y_test.ravel(),y_pred)\n    plt.plot(fpr, tpr,lw=2, label='ROC curve (area = {}) for {}'.format(roc_auc,name) )\n  \nplt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver Operating Characteristic Curve')\nplt.legend(loc=\"upper left\", bbox_to_anchor=(1.05,1.05))\nplt.show()\n","6a25afc0":"### Baseline ","bf555a20":"* ### Other Moddels Using Transformed Datasets","83e4654e":"### Explore Training Dataset - Transformation","ca48c5c4":"This base model result in 0.88668 accuracy on final dataset. ","807d0fad":"### Tune Ada Model "}}