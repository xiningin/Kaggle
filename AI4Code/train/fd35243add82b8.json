{"cell_type":{"c5f343da":"code","5ae8c235":"code","095995a0":"code","92b0a085":"code","1b9fd0fc":"code","d3cb1381":"code","385f9578":"code","ddc55788":"code","670b6ddf":"code","2d441e15":"markdown"},"source":{"c5f343da":"!pip install --upgrade pip\n!pip uninstall -y allennlp\n!pip install transformers==4.1.1 typer\n!pip install -U pytorch-lightning\n!pip install https:\/\/github.com\/veritable-tech\/pytorch-lightning-spells\/archive\/master.zip","5ae8c235":"!mkdir -p \/src\/finetuning-t5\n!git clone https:\/\/github.com\/ceshine\/finetuning-t5.git -b master \/src\/finetuning-t5","095995a0":"%cd \/src\/finetuning-t5\/mnli","92b0a085":"!mkdir -p data\/kaggle\n!cp -r \/kaggle\/input\/contradictory-my-dear-watson\/* data\/kaggle\n!ls data\/kaggle\/","1b9fd0fc":"!mkdir -p data\/mt5-base-mnli\n!cp -r \/kaggle\/input\/mt5-base-mnli-pretrained\/mt5-base_best\/* data\/mt5-base-mnli\n!ls data\/mt5-base-mnli\/","d3cb1381":"!mkdir -p cache\/kaggle\/\n!python preprocess\/preprocess_kaggle.py\n!python preprocess\/tokenize_dataset.py kaggle --tokenizer-name data\/mt5-base-mnli","385f9578":"!SEED=9923 python train.py --t5-model data\/mt5-base-mnli --batch-size 64 --grad-accu 1 \\\n        --epochs 2 --lr 1e-4 --disable-progress-bar --dataset kaggle \\\n        --max-len 128 --freeze-embeddings --valid-frequency 0.25 ","ddc55788":"!ls cache\/\n!mv cache\/tb_logs \/kaggle\/working\n!mv cache\/mt5-base-mnli_best \/kaggle\/working","670b6ddf":"!python kaggle_inference.py \/kaggle\/working\/mt5-base-mnli_best\n!cp submission.csv \/kaggle\/working","2d441e15":"[Training code is from this repo of mine](https:\/\/github.com\/ceshine\/finetuning-t5\/tree\/mt5-classifier-single-token\/mnli). Currently my fine-tunes mT5 models still underperforms comparing to the BERT and XLM models.\n\nThe mT5-base model was [trained on the MNLI dataset (Enlgish-only)](https:\/\/www.kaggle.com\/ceshine\/preprocess-and-finetune-mt5?scriptVersionId=52689872). The result is slightly better than [the zero-shot one](https:\/\/www.kaggle.com\/ceshine\/mt5-base-mnli-zero-shot).\n\nThe embedding matrix in the encoder is frozen when training.\n\nUpdate in version 8: Use AdaFactor optimizer; update the maxlen trucation scheme; use the Sortish sampler. Train all weights."}}