{"cell_type":{"49664587":"code","123874fa":"code","45dc5fd2":"code","ba614fe2":"code","bee6479f":"code","541459c4":"code","0792ba37":"code","7f658d15":"code","7bfbeffd":"code","4e53ecf4":"code","281e4d3a":"code","1d49dbda":"code","78266d26":"code","41d92395":"code","932682ee":"code","e8f91ccd":"code","7051ce49":"code","b0dcb365":"code","786f9547":"code","2c14e785":"code","c43a5bc9":"code","a61251a1":"code","ec9af5ae":"code","5862418e":"code","2c33bcf1":"code","75c2e15b":"code","7ed4c264":"code","314c4afe":"code","c4ec86dd":"code","e2c811a5":"code","5327999b":"code","3dab9adc":"code","9399116f":"code","59a38a81":"code","63adf235":"markdown","3b23d148":"markdown","127899fa":"markdown","912d903e":"markdown","e7fe1d40":"markdown","dc2e6749":"markdown","4a80d080":"markdown","c7186255":"markdown","462679cb":"markdown","e977fbf2":"markdown","7ff3f870":"markdown","b67bb1fb":"markdown","00179791":"markdown","a4cd124c":"markdown","1ad5f7f8":"markdown","c999d9bf":"markdown","0b9d7958":"markdown","bd69f37a":"markdown","f61ff505":"markdown","6a83db71":"markdown","6f93fb18":"markdown","8581fe48":"markdown","e530080e":"markdown","71b8b5a6":"markdown","5d038660":"markdown","2dc08bbc":"markdown","b9e56722":"markdown","76f01d6e":"markdown","16b5c7e9":"markdown","761df66c":"markdown","fb72d59c":"markdown","9e553f6b":"markdown","1f04932d":"markdown","3a3b3ee2":"markdown"},"source":{"49664587":"#Here we install the package. For me it's been a nightmare to install rdkit into Kaggle's environment. \n#But wonderful Kaggle's technical support helped me to find the way.\n\n!conda install -y -c rdkit rdkit;\n!pip install pandas==0.23.0","123874fa":"%matplotlib inline\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport warnings\nwarnings.filterwarnings(\"ignore\")","45dc5fd2":"#Let's load the data and look at them\ndf= pd.read_csv('..\/input\/mlchem\/logP_dataset.csv', names=['smiles', 'logP'])\ndf.head()","ba614fe2":"#Importing Chem module\nfrom rdkit import Chem \n\n#Method transforms smiles strings to mol rdkit object\ndf['mol'] = df['smiles'].apply(lambda x: Chem.MolFromSmiles(x)) \n\n#Now let's see what we've got\nprint(type(df['mol'][0]))","bee6479f":"from rdkit.Chem import Draw\nmols = df['mol'][:20]\n\n#MolsToGridImage allows to paint a number of molecules at a time\nDraw.MolsToGridImage(mols, molsPerRow=5, useSVG=True, legends=list(df['smiles'][:20].values))","541459c4":"# AddHs function adds H atoms to a MOL (as Hs in SMILES are usualy ignored)\n# GetNumAtoms() method returns a general nubmer of all atoms in a molecule\n# GetNumHeavyAtoms() method returns a nubmer of all atoms in a molecule with molecular weight > 1\n\n\ndf['mol'] = df['mol'].apply(lambda x: Chem.AddHs(x))\ndf['num_of_atoms'] = df['mol'].apply(lambda x: x.GetNumAtoms())\ndf['num_of_heavy_atoms'] = df['mol'].apply(lambda x: x.GetNumHeavyAtoms())","0792ba37":"import seaborn as sns\nsns.jointplot(df.num_of_atoms, df.logP)\nplt.show()","7f658d15":"# First we need to settle the pattern.\nc_patt = Chem.MolFromSmiles('C')\n\n# Now let's implement GetSubstructMatches() method\nprint(df['mol'][0].GetSubstructMatches(c_patt))","7bfbeffd":"#We're going to settle the function that searches patterns and use it for a list of most common atoms only\ndef number_of_atoms(atom_list, df):\n    for i in atom_list:\n        df['num_of_{}_atoms'.format(i)] = df['mol'].apply(lambda x: len(x.GetSubstructMatches(Chem.MolFromSmiles(i))))\n\nnumber_of_atoms(['C','O', 'N', 'Cl'], df)","4e53ecf4":"sns.pairplot(df[['num_of_atoms','num_of_C_atoms','num_of_N_atoms', 'num_of_O_atoms', 'logP']], diag_kind='kde', kind='reg', markers='+')\nplt.show()","281e4d3a":"from sklearn.linear_model import RidgeCV\nfrom sklearn.model_selection import train_test_split\n\n#Leave only features columns\ntrain_df = df.drop(columns=['smiles', 'mol', 'logP'])\ny = df['logP'].values\n\nprint(train_df.columns)\n\n#Perform a train-test split. We'll use 10% of the data to evaluate the model while training on 90%\n\nX_train, X_test, y_train, y_test = train_test_split(train_df, y, test_size=.1, random_state=1)\n","1d49dbda":"from sklearn.metrics import mean_absolute_error, mean_squared_error\ndef evaluation(model, X_test, y_test):\n    prediction = model.predict(X_test)\n    mae = mean_absolute_error(y_test, prediction)\n    mse = mean_squared_error(y_test, prediction)\n    \n    plt.figure(figsize=(15, 10))\n    plt.plot(prediction[:300], \"red\", label=\"prediction\", linewidth=1.0)\n    plt.plot(y_test[:300], 'green', label=\"actual\", linewidth=1.0)\n    plt.legend()\n    plt.ylabel('logP')\n    plt.title(\"MAE {}, MSE {}\".format(round(mae, 4), round(mse, 4)))\n    plt.show()\n    \n    print('MAE score:', round(mae, 4))\n    print('MSE score:', round(mse,4))","78266d26":"#Train the model\nridge = RidgeCV(cv=5)\nridge.fit(X_train, y_train)\n#Evaluate results\nevaluation(ridge, X_test, y_test)","41d92395":"atp = Chem.MolFromSmiles('C1=NC2=C(C(=N1)N)N=CN2[C@H]3[C@@H]([C@@H]([C@H](O3)COP(=O)(O)OP(=O)(O)OP(=O)(O)O)O)O')\n\n# Getting number of rings with specified number of backbones\nprint('Number of rings with 1 backbone:', atp.GetRingInfo().NumAtomRings(1))\nprint('Number of rings with 2 backbones:', atp.GetRingInfo().NumAtomRings(2))","932682ee":"m = Chem.MolFromSmiles('C(=O)C(=N)CCl')\n#Iterating through atoms to get atom symbols and explicit valencies \nfor atom in m.GetAtoms():\n    print('Atom:', atom.GetSymbol(), 'Valence:', atom.GetExplicitValence())","e8f91ccd":"from rdkit.Chem import Descriptors\ndf['tpsa'] = df['mol'].apply(lambda x: Descriptors.TPSA(x))\ndf['mol_w'] = df['mol'].apply(lambda x: Descriptors.ExactMolWt(x))\ndf['num_valence_electrons'] = df['mol'].apply(lambda x: Descriptors.NumValenceElectrons(x))\ndf['num_heteroatoms'] = df['mol'].apply(lambda x: Descriptors.NumHeteroatoms(x))","7051ce49":"train_df = df.drop(columns=['smiles', 'mol', 'logP'])\ny = df['logP'].values\n\nprint(train_df.columns)\n\n#Perform a train-test split. We'll use 10% of the data to evaluate the model while training on 90%\n\nX_train, X_test, y_train, y_test = train_test_split(train_df, y, test_size=.1, random_state=1)","b0dcb365":"#Train the model\nridge = RidgeCV(cv=5)\nridge.fit(X_train, y_train)\n#Evaluate results and plot predictions\nevaluation(ridge, X_test, y_test)","786f9547":"#Installing a package\n!pip install git+https:\/\/github.com\/samoturk\/mol2vec;","2c14e785":"#Load the dataset and extract target values\nmdf= pd.read_csv('..\/input\/mlchem\/logP_dataset.csv', names=['smiles', \n                                           'target'])\ntarget = mdf['target']\nmdf.drop(columns='target',inplace=True)","c43a5bc9":"#Transforming SMILES to MOL\nmdf['mol'] = mdf['smiles'].apply(lambda x: Chem.MolFromSmiles(x))\n","a61251a1":"#Loading pre-trained model via word2vec\nfrom gensim.models import word2vec\nmodel = word2vec.Word2Vec.load('..\/input\/mlchem\/model_300dim.pkl')","ec9af5ae":"from mol2vec.features import mol2alt_sentence, mol2sentence, MolSentence, DfVec, sentences2vec\nfrom gensim.models import word2vec\nprint('Molecular sentence:', mol2alt_sentence(mdf['mol'][1], radius=1))\nprint('\\nMolSentence object:', MolSentence(mol2alt_sentence(mdf['mol'][1], radius=1)))\nprint('\\nDfVec object:',DfVec(sentences2vec(MolSentence(mol2alt_sentence(mdf['mol'][1], radius=1)), model, unseen='UNK')))","5862418e":"#Constructing sentences\nmdf['sentence'] = mdf.apply(lambda x: MolSentence(mol2alt_sentence(x['mol'], 1)), axis=1)\n\n#Extracting embeddings to a numpy.array\n#Note that we always should mark unseen='UNK' in sentence2vec() so that model is taught how to handle unknown substructures\nmdf['mol2vec'] = [DfVec(x) for x in sentences2vec(mdf['sentence'], model, unseen='UNK')]\nX = np.array([x.vec for x in mdf['mol2vec']])\ny = target.values\n\nX.shape","2c33bcf1":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.1, random_state=1)\nridge = RidgeCV(cv=5)\nridge.fit(X_train, y_train)\nevaluation(ridge, X_test, y_test)","75c2e15b":"mdf = pd.DataFrame(X)\nnew_df = pd.concat((mdf, train_df), axis=1)","7ed4c264":"X_train, X_test, y_train, y_test = train_test_split(new_df, y, test_size=.1, random_state=1)\nridge = RidgeCV(cv=5)\nridge.fit(X_train, y_train)\nevaluation(ridge, X_test, y_test)","314c4afe":"import warnings\nwarnings.filterwarnings(\"ignore\")\n#Read the data\nhiv = pd.read_csv('..\/input\/mlchem\/HIV.csv')\nhiv.head()","c4ec86dd":"#Let's look at the target values count\nsns.countplot(data = hiv, x='HIV_active', orient='v')\nplt.ylabel('HIM active')\nplt.xlabel('Count of values')\nplt.show()","e2c811a5":"#Transform SMILES to MOL\nhiv['mol'] = hiv['smiles'].apply(lambda x: Chem.MolFromSmiles(x)) \n\n#Extract descriptors\nhiv['tpsa'] = hiv['mol'].apply(lambda x: Descriptors.TPSA(x))\nhiv['mol_w'] = hiv['mol'].apply(lambda x: Descriptors.ExactMolWt(x))\nhiv['num_valence_electrons'] = hiv['mol'].apply(lambda x: Descriptors.NumValenceElectrons(x))\nhiv['num_heteroatoms'] = hiv['mol'].apply(lambda x: Descriptors.NumHeteroatoms(x))","5327999b":"y = hiv.HIV_active.values\nX = hiv.drop(columns=['smiles', 'activity','HIV_active', 'mol'])\n\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.20, random_state=1)","3dab9adc":"from sklearn.metrics import auc, roc_curve\ndef evaluation_class(model, X_test, y_test):\n    prediction = model.predict_proba(X_test)\n    preds = model.predict_proba(X_test)[:,1]\n    fpr, tpr, threshold = roc_curve(y_test, preds)\n    roc_auc = auc(fpr, tpr)\n    \n    plt.title('ROC Curve')\n    plt.plot(fpr, tpr, 'g', label = 'AUC = %0.3f' % roc_auc)\n    plt.legend(loc = 'lower right')\n    plt.plot([0, 1], [0, 1],'r--')\n    plt.xlim([0, 1])\n    plt.ylim([0, 1])\n    plt.ylabel('True Positive Rate')\n    plt.xlabel('False Positive Rate')\n    plt.show()\n    \n    print('ROC AUC score:', round(roc_auc, 4))","9399116f":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.preprocessing import StandardScaler\n\nX_train = StandardScaler().fit_transform(X_train)\nX_test = StandardScaler().fit_transform(X_test)\n\nlr = LogisticRegression()\nlr.fit(X_train, y_train)\n\nevaluation_class(lr, X_test, y_test)","59a38a81":"#Constructing sentences\nhiv['sentence'] = hiv.apply(lambda x: MolSentence(mol2alt_sentence(x['mol'], 1)), axis=1)\n\n#Extracting embeddings to a numpy.array\n#Note that we always should mark unseen='UNK' in sentence2vec() so that model is taught how to handle unknown substructures\nhiv['mol2vec'] = [DfVec(x) for x in sentences2vec(hiv['sentence'], model, unseen='UNK')]\nX_mol = np.array([x.vec for x in hiv['mol2vec']])\nX_mol = pd.DataFrame(X_mol)\n\n#Concatenating matrices of features\nnew_hiv = pd.concat((X, X_mol), axis=1)\n\nX_train, X_test, y_train, y_test = train_test_split(new_hiv, y, test_size=.20, random_state=1)\n\nX_train = StandardScaler().fit_transform(X_train)\nX_test = StandardScaler().fit_transform(X_test)\n\nlr = LogisticRegression()\nlr.fit(X_train, y_train)\n\n\nevaluation_class(lr, X_test, y_test)","63adf235":"Once again we need to transform SMILES representations to MOL.","3b23d148":"Notice that MOLs are represented as special rdkit.Chem items of a corresponding C++ class.\n\n","127899fa":"Now we'll load a pre-trained mol2vec model. It's trained with radius=1 for Morgan fingerprints to yield 300 dimensional embeddings.","912d903e":"We'll start from scratch with initial dataset.","e7fe1d40":"As you see the only information we have here is SMILES representation of molecular formulas. But RDkit is able to work with MOL\nrepresentations. And it's actually nice to know RDkit still provides an opportunity to transform SMILES to MOL. Let's check what we can do.   ","dc2e6749":"# RDkit for working with SMILES and MOL representations\n\nRDKit is a collection of cheminformatics and machine learning tools written in C++ and Python.\nWhich is more important, it allows to work with many representations of chemical data and has a power to extract almost each chemical descriptor from the data you have. Check the docs for deeper understanding.\n\nIn this tutorial we'll try to implement ML to a propety prediction task. RDkit is the one to help at the first step.","4a80d080":"We notice great class disbalance. Now let's repeat the same approaches.\nAt the first step - descriptors from RDkit.","c7186255":"Note that you can also extract some ring information or iterate through each molecule's atoms and bonds with RDkit. Methods ***GetRingInfo()***, ***GetAtoms()*** and ***GetBonds()*** yield corresponding generators over rings and atoms in molecule. \n\nSome toy examples:","462679cb":"We'll work with the data on lipophilicity of small organic molecules. \n\n**Lipophilicity** is a physical property of a substance that refers to an ability of a chemical compound to dissolve in lipids, oils and generally in non-polar solvents. That's a fundamental property that plays great part in biochemical and technological behaviour of a substance. It's usually evaluated by a distribution coefficent P -  the ratio of concentrations of a particular compound in a mixture of two immiscible phases at equilibrium (in our case water and octanol). The greater P values refer to greater lipophilicity. Usually lipophilicity is presented as log10P like in our dataset. \n\nA thing important to mention is that it takes much time to perform an experiment to measure the corresponding P value, because it must be an exact and reproducible procedure repeated at least 3-5 times and each time you have to wait till the system's equilibrium is reached.\n\n*Chemical insights: from theory P value must turn great when a molecule is large and doesn't have many polar atoms or atomic groups (such containing O, N, P, S, Br, Cl, F an etc.) in it.*","e977fbf2":"# mol2vec - learning vector representations of molecular subctructures\n\nFrom package description 'Mol2vec is an unsupervised machine learning approach to obtain high dimensional embeddings of chemical substructures. It is based on word2vec algorithm and if word2vec learns such embeddings that vectors representing similar words group together, so should Mol2vec learn substructure embeddings where vectors of chemically related substructures end up close in vector space'. \n\nIn other words mol2vec takes inspiration in one of the most widespread NLP technique.\nWithout going deep in detaials we can describe the process as following: \n\n**1.** A molecule is divided into substructures of a fixed radius (usually encoding distance to other atoms in bonds, i.e. radius = 1 represents a single heavy atom or a group of closest atoms around a heavy atom), substructures are encoded via [Morgan fingerprints](https:\/\/docs.chemaxon.com\/display\/docs\/Extended+Connectivity+Fingerprint+ECFP) with the help of RDkit. Substructures reflect words in 'chemical sentences'.\n\n**2.** These encoded substructures are fed to Word2vec yielding vector representations of substructures in a number of dimensions according to their occurance together.\n\n**3.** Summing up substructure vectors we get vector representations of whole molecules.\n\n\nThe main idea behind is that we can use vector representations of molecules in different dimensions as features in supervised learning, supposing similar molecules will have close vector representations and will have close target values. \n\nAnother wonderful point is that mol2vec provides a pre-trained model which has learned 20M different compounds with radius=1 for Morgan fingerprints.","7ff3f870":"**Visualization of molecules**\n\nRDkit provides visualization of MOLs with ***rdkit.Chem.Draw*** module. To visualize a set of molecules you should pass a set of corresponging MOLs:","b67bb1fb":"**Offtop: particular atoms, bonds and rings**","00179791":"Let's check the improvement of the model perfomance with new features.","a4cd124c":"Train the model and evaluate results. As an evaluation metric we can choose MAE or MSE. Evaluation function plots a number of first predictions (by default - 300).","1ad5f7f8":"Looking at the bottom plots we notice some linear dependence of logP on numbers of particular atoms. We also see normal distribution of the target variable which values lie approximately from -5 to 5. Let's build our first model.\nWe'll count on ridge regression.","c999d9bf":"**Descriptors**\n\n***rdkit.Chem.Descriptors*** provides a number of general molecular descriptors that can also be used to featurize a molecule. Most of the descriptors are straightforward to use from Python.\n\nUsing this package we can add some useful features to our model:\n- rdkit.Chem.Descriptors.TPSA() - the surface sum over all polar atoms or molecules also including their attached hydrogen atoms;\n- rdkit.Chem.Descriptors.ExactMolWt() - exact molecural weight;\n- rdkit.Chem.Descriptors.NumValenceElectrons() - number of valence electrons (may illustrate general electronic density)\n- rdkit.Chem.Descriptors.NumHeteroatoms() - general number of non-carbon atoms.","0b9d7958":"**Numbers of atoms**\n\nSince size of a molecule can be approximated by a number of atoms in it, let's extract corresponding values from MOL.\nRDkit provides ***GetNumAtoms()*** and ***GetNumHeavyAtoms()*** methods for that task.","bd69f37a":"And we got the best among our results. This one is actually a rather good result even for computional chemistry.","f61ff505":"The method returns a tuple of tuples of positions of corresponding patterns. To extract the number of matches we need to take the length of a corresponding tuple of tuples.","6a83db71":"# Introduction\n\nAs far as I've already found out, data science approaches can actually be implemented everywhere. The only limitation is data that you have (or you don't). Since chemistry is known to be a rather exact science (because we do have laws and numbers in it, still we don't have enough principles that are mathematically strict) it must have data to be analyzed. You might understand that in real-world chemistry we can find many examples of the things that are seeking to have a good data scientist's look at.\nI'll tell the simplest ever example: two main general tasks that many chemistry researchers face almost everyday are predicting physical, chemical, biochemical (or some other desirable) properties of a molecule (or a substance when we talk about physical properties) by it's general descriptors - chemical formula and 3D structure - and predicting chemical formula and 3D structure for an unknown yet molecule or substance with desirable properties respectively. They are called QSPR (quantitative structure-property relationship) and QSAR (quantitative structure-activity relationship) problems. From the history of the problem we can see many different approaches based on some chemical and mathematical logic, some quantum mechanics approaches and etc. The main disadvantage of all these methods is that they're really computationally hard (and also not always accurate). And now doesn't the problem sound familiar? What if don't need to wait for weeks to find values of some properties that we need? Let's find it out.\n\n# Representation of chemical data\nLet's consider a case where we want to implement some machine learning in chemistry research. The first issue we're goind to face is how to represent the data in the way that keeps much information.\nWhen it comes to chemical formulas, in chemoinformatics there're several general ways of representation.\n\n**SMILES (Simplified Molecular Input Line Entry System)**\n\n\nThis is the simplest way to reflect a molecule. The idea behind is to use simple line notations for chemical formulas that are based on some rules. Atoms of chemical elements are represented by chemical symbols in capital letter, hydrogen is usually ignored. Single bonds are not displayed; for double, triple and quadruple bonds we shall use *'='*, *'#'*, *'$'* respectively. Atoms that are bonded must stand nearby. Ring structures are written by breaking each ring at an arbitrary point (although some choices will lead to a more legible SMILES than others) to make a 'straight non-ring' structure (as if it wasn't a ring) and adding numerical ring closure labels to show connectivity between non-adjacent atoms. Aromacity is commonly illustrated by writing the constituent B, C, N, O, P and S atoms in lower-case forms b, c, n, o, p and s, respectively. To represent side chains of atomic gpoups branches are used. \n\nA simple example:\n\n<img src=\"http:\/\/www.daylight.com\/dayhtml\/doc\/theory\/theory4.gif\"\/>\n\nA fact that usually you cand find SMILES representation of a molecule in any database of chemical information makes the thing simple. But also you can write it by yourself, if needed, which is definetely a huge advantage.\n\nPros:\n- Easy to find or to write by yourelf.\n- Illustrates some main chemical concepts such as atomic map, bond structure, can represent stereochemical properties (isomerism).\n- Can be further processed as a string data, if needed.\n\nCons:\n- Doesn't give information upon space structure of the molecule (2D, but mostly 3D).\n- Different formulas may be written for the same molecule.\n\n**MDL Molfile**\n\n\nIt's a file format that keeps information about the atoms, bonds, connectivity and coordinates of a molecule.\n\nTypical MOL consists of some header information, the Connection Table containing atom info, then bond connections and types, followed by sections for more complex information. MOL representations can either hold information about 2D or 3D structure of the molecule. This format is so widely spread that most cheminformatics software systems are able to read it.\n\nAn example of how it looks like:\n\n<img src=\"https:\/\/chem.libretexts.org\/@api\/deki\/files\/112699\/MOL_FILE_1.png?revision=1&size=bestfit&width=777&height=601\"\/>\n\nPros:\n- Acceptable by most chemoinformatics software.\n- Represents space structure (2D or 3D)\n- Invariant for particular molecule.\n\nCons:\n- Huge size.\n- Can't be perfomed by yourself.\n\n**Coloumb matrices and other quantum descriptors**\n\n\nThe Coulomb matrix as well as some custom quntum approaches comes from idea that molecular properties can be calculated from the Schr\u00f6dinger equation, which takes the Hamiltonian operator as its input. The Hamiltonian operator for an isolated molecule can be specified by the atom coordinates and nuclear charges. \nIn fact, there're many custom ways to represent a molecule with the help of quantum chemistry. The only limitation is the presense of such data.\n\nPros:\n- Mathemathicaly and chemically strict representations.\n\nCons:\n- Hard to compute.\n- Seeks relative data.\n\n**Custom set of descriptors**\n\nOne can actually represent the inforation by creating suitable descriptors that are belivied to be helpful - bond count, atom count, presence of particular groups of atoms an etc.\nSuch descriptors may be exctracted from SMILES strings, MOL representations or performed manually.\n\nPros:\n- Extracting suitable information based on chemical logic.\n\nCons:\n- Hard to produce manually.\n- Needs some other representation for computer processing.\n- May not represent a molecule correctly.","6f93fb18":"# Preface\n\nWhile considering which topic to pick to write a tutorial I suddenly understood that there's no better way to show an interesting thing than to describe something you're truly aware of. Alright, I got a higher education in chemistry, then actually went crazy on a thing called data science hoping that one day I'll be able to put two of them together. That's why I've actually entered mlcourse.ai. And I suppose the time has come. In this tutorial I'll try to show some insights on how can some of Python packages and machine learning be easily and efficiently applied in chemistry without going deep to the strange and contradictory world of deep neural networks for chemistry and drug development.\n\n<img src=\"https:\/\/as1.ftcdn.net\/jpg\/01\/62\/02\/60\/500_F_162026016_NbhvWZvqo7SLke6dhEdNw6yPyuedR8Ae.jpg\"\/>","8581fe48":"# Conclusion\n\nWe've tried a number of approaches to handle chemical data and built rather good predictive models for physical and biological properties of chemical matter with wonderful ***RDkit*** and ***mol2vec*** packages. I suppose, further feature construction can bring an even better result, but still it's shown that we are able to predict important general properties rather fast and easy, without constructing great neural networks or spending weeks on quantum calculations, which can actually be implemented in a lab if an approximated evaluation of the target is needed. Another important outcome is the fact that these approaches are suitable for both regression and classification tasks (in classification task we dealed to beat a MoleculeNet benchmark). Also I'd like to mention that ***RDkit*** allows to do many useful things that are not shown in the kernel, thus, are yet to research, if needed. The docs can be found [here](https:\/\/www.rdkit.org\/docs\/) and [here](https:\/\/mol2vec.readthedocs.io\/en\/latest\/). \n\nWill ***mol2vec*** approach be useful in a QSAR problem solving is a question to research. But still it's a good thing to start with when it comes to ML in chemistry.\n\nThanks for your attention!\n\n**References**:\n - Kaggle Datasets: LogP of Chemical Structures. https:\/\/www.kaggle.com\/matthewmasters\/chemical-structure-and-logp\n - Jaeger, S., Fulle, S., & Turk, S. (2018). Mol2vec: Unsupervised machine learning approach with chemical intuition. Journal of chemical information and modeling, 58(1), 27-35. URL = {http:\/\/dx.doi.org\/10.1021\/acs.jcim.7b00616}, eprint = {http:\/\/dx.doi.org\/10.1021\/acs.jcim.7b00616}\n - AIDS Antiviral Screen Data. https:\/\/wiki.nci.nih.gov\/display\/NCIDTPdata\/AIDS+Antiviral+Screen+Data","e530080e":"Let's take look at how number of atoms is connected with target variable:","71b8b5a6":"Importing necessary functions. \n\n***mol2alt_sentence()*** constructs a so-called 'molecular sentence' with desired Morgan fingerprints' radius (uses RDkit backend) where 'words' are unique substructure identifiers; ***MolSentence()*** is an internal wrapper function; ***sentences2vec()*** generates molecular embeddings with the help of the trained model; ***DfVec()*** is an internal wrapper for embeddings' generator (attribute .vec yields aggregated vectors).  ","5d038660":"Now let's implement those functions above and construct a feature matrix of vector representations.","2dc08bbc":"And finally let's add molecular embeddings with mol2vec.","b9e56722":"**Summimg up**\n\nRDkit is a wonderful tool to work with chemical data, especially represented as SMILES strings or in MOL format. We've seen some basic general abilities of the package but some other powerful tools are yet to be found in the docs.\n\nTrying to improve our model perfomance at the next step we'll take a look at a copletely different way to featurize molecules. ","76f01d6e":"# SIMPLE ML IN CHEMISTRY RESEARCH: RDKIT AND MOL2VEC ","16b5c7e9":"Train and evaluate the model.","761df66c":"Let's see the perfomance of the model with vector representations.","fb72d59c":"# Classification task\nWe'll take another task to prove the skills. This time it's going to be a classification problem. We'll use a dataset from [MoleculeNet](http:\/\/moleculenet.ai\/datasets-1) database - a benchmark for computional chemistry. There's a number of datasets for classification, so I've picked the one that is not so large - HIV dataset. From description: \"The HIV dataset was introduced by the Drug Therapeutics Program (DTP) AIDS Antiviral Screen, which tested the ability to inhibit HIV replication for over 40,000 compounds. Screening results were evaluated and placed into three categories: confirmed inactive (CI),confirmed active (CA) and confirmed moderately active (CM). We further combine the latter two labels, making it a classification task between inactive (CI) and active (CA and CM)\". \n\nIn the dataset once again we can find SMILES representations and two target columns, but only 'HIV_active' is used as a benchmark.\nA benchmark for Logistic Regression for this task [is ROC AUC = 0.782](http:\/\/moleculenet.ai\/latest-results).\nActually I haven't dealed to find any information about how was this result achieved. The only information I could find is that 10 % of the data were used to validate and 10 % were used as test data. To make it fair we'll use 20 % of the data for the test set.","9e553f6b":"Wow! Vector representations alone absolutely outscored features built with chemical logic. mol2vec is a really powerful tool!\n\nNow let's put embeddings together with already constructed features to see the result.","1f04932d":"No clear dependence is observed so we need some more features to be exctracted.\nThe next obvious step is to count numbers of the most common atoms.\nRDkit supports subpattern search represented by ***GetSubstructMatches()*** method. It takes a MOL of a substructure pattern as an argument. So you can futher extract occurance of each pattern you'd like.","3a3b3ee2":"That's an absolutely outstanding result when comparing to best results on MoleculeNet. Even though we were unable to reproduce the split, still results show that we are very close to or even better than best results published on MoleculeNet. Incredible!"}}