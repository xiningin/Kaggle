{"cell_type":{"360c0629":"code","3d95d372":"code","44dfa892":"code","3e128e59":"code","cbdf4545":"code","9397bbf8":"code","1bfd7ee8":"code","f27e9729":"code","43ae3554":"code","7ea3f6a4":"code","089fbffe":"code","0931ae37":"code","7abff9e2":"code","1dd8c99b":"code","852423b8":"code","14837d6f":"code","cd4ca115":"code","693e2b1e":"code","a32d5045":"code","9314378b":"code","3cef3d39":"code","301dbc59":"code","031ee285":"code","f7738e2a":"code","4b4700ad":"code","a1721d74":"code","8c17302a":"code","577563aa":"code","88a14ab8":"code","92b7760c":"code","38e0a298":"code","7595ab4e":"code","d8fac773":"code","3633332a":"code","12ade977":"code","0c3ec14b":"code","e2b7e6fa":"code","0699f73f":"code","de75b3a2":"code","75fdbcd2":"code","3fd7377c":"code","b2c42b61":"code","80fca9b9":"code","2a0024c1":"code","9af9c14e":"code","38ac6dd4":"code","c653855e":"code","2fb34081":"code","19901230":"code","11a56c50":"code","3d3903d8":"code","c388c10b":"code","42efa469":"code","0142a6c1":"code","4bc5211c":"code","b8a27532":"markdown","641d0ebe":"markdown","9b2e8359":"markdown","b29f4266":"markdown","9a471b46":"markdown","111f169b":"markdown","4d6122f0":"markdown","e05271ca":"markdown","c9825bab":"markdown","9e0bcc9f":"markdown","0f34d789":"markdown","65c57ec3":"markdown","9770d416":"markdown","4d65ab63":"markdown","e9a31168":"markdown","c3f31e11":"markdown","fa5dc836":"markdown","3c8aa0da":"markdown","05b5f032":"markdown","ae967414":"markdown","26a4a42b":"markdown","99c3361f":"markdown","8dcaff53":"markdown","0b270ed5":"markdown","250fa682":"markdown","e366aa8f":"markdown","0439d249":"markdown","183f4eb0":"markdown","27b503f4":"markdown","2cdd5434":"markdown","0643b738":"markdown","f9ed6548":"markdown","9d3ba61b":"markdown","10d601f6":"markdown","8b158e70":"markdown","dcd004ce":"markdown","2c377076":"markdown"},"source":{"360c0629":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","3d95d372":"# let's import some libraries to explore the data\n\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings(\"ignore\")","44dfa892":"# let's load the input files into dataframes\n\ntrain = pd.read_csv('..\/input\/home-data-for-ml-course\/train.csv')\ntest = pd.read_csv('..\/input\/home-data-for-ml-course\/test.csv')\nsample_submission = pd.read_csv('..\/input\/home-data-for-ml-course\/sample_submission.csv')\n\ntrain_df = train.copy() # let's create a copy for exploration purpose \n\n# let's check if the train data and test data have the same features\n\nall([i in test.columns for i in train.drop('SalePrice',axis=1).columns]) ","3e128e59":"train_df.head() # to inspect the top rows","cbdf4545":"train_df.shape # to see the number of rows and columns","9397bbf8":"train_df[train_df.duplicated()] # to check for duplicate rows","1bfd7ee8":"train_df.info() # to get more information about the data","f27e9729":"# let's create a list of columns to be converted to the category data type\n\nto_be_converted = train_df.select_dtypes(exclude='number').columns.to_list() + ['MSSubClass',\n                                                                                'YearBuilt', \n                                                                                'YearRemodAdd', \n                                                                                'YrSold',\n                                                                                'MoSold',\n                                                                                'OverallQual',\n                                                                               'OverallCond',\n                                                                                'GarageYrBlt']\n                                                                                                \nfor col in to_be_converted:\n    train_df[col] = train_df[col].astype('category')\n\ntrain_df.info() # check to see the changes","43ae3554":"# the Id column can be dropped since pandas already provided an index column\n\ntrain_df.drop('Id', axis=1, inplace=True)","7ea3f6a4":"# Let's seperate numeric columns from non-numeric ones\n\nnum_cols = train_df.select_dtypes(include='number').columns.to_list()\ncat_cols = train_df.select_dtypes(exclude='number').columns.to_list()\n\nprint(f'Number of numeric columns: {len(num_cols)}')\nprint(f'Number of categorical columns: {len(cat_cols)}')","089fbffe":"train_df[num_cols].describe() # to get more information on the numerical data","0931ae37":"train_df.hist(bins=50,figsize=(15,20));","7abff9e2":"from pandas.plotting import scatter_matrix\n\nnum_attr = ['SalePrice', 'LotFrontage', 'LotArea', 'BsmtFinSF1', 'BsmtUnfSF', \n         'TotalBsmtSF', 'GrLivArea', 'GarageArea']\n\nscatter_matrix(train_df[num_attr], figsize=(20,15));","1dd8c99b":"cat_attr = ['MSSubClass',\n 'MSZoning',\n 'Street',\n 'LandSlope',\n 'Neighborhood',\n 'BldgType',\n 'HouseStyle',\n 'OverallCond',\n 'YearBuilt',\n 'YearRemodAdd',\n 'ExterQual',\n 'ExterCond',\n 'CentralAir',\n 'KitchenQual',\n 'Functional',\n 'GarageCond',\n 'Fence',\n 'MoSold',\n 'YrSold',\n 'SaleType',\n 'SaleCondition']\n\nfig, ax = plt.subplots(7,3, figsize=(15,30), sharey='row')\nfor col in cat_attr:\n    index = cat_attr.index(col)\n    sns.boxplot(x=col, y='SalePrice', data=train_df, ax=ax[index\/\/3,index%3])","852423b8":"expensive = train_df.groupby('Neighborhood').mean().SalePrice.sort_values(ascending=False)\nplt.figure(figsize=(10,10))\nsns.barplot(y=expensive.index, x=expensive.values, order=expensive.index)\nplt.xlabel('SalePrice')\nplt.title('Average Prices across Neighborhoods');","14837d6f":"fig,ax = plt.subplots(1,2, figsize=(10,5), sharey='row')\nplt.title('SalePrices across different House Types and Styles')\nsns.boxplot('BldgType', 'SalePrice', data=train_df, ax=ax[0])\nsns.boxplot('HouseStyle','SalePrice',data=train_df, ax=ax[1]);","cd4ca115":"sns.scatterplot(x='GarageArea',y='SalePrice',hue='GarageCond', data=train_df)\nplt.legend(loc=(0.1,0.64));","693e2b1e":"train_df.columns","a32d5045":"# to drop the target leakage features\n\ntrain_df.drop(['MoSold', 'YrSold', 'SaleType', 'SaleCondition'],axis=1,inplace=True)\n\ntrain_df.columns","9314378b":"# let's see the correlation values of the features with SalePrice\n\ntrain_df.corr().SalePrice.sort_values(ascending=False)","3cef3d39":"plt.figure(figsize=(10,10))\nhigh_corr = train_df.corr()\nsns.heatmap(high_corr);","301dbc59":"print(len(train_df.columns))\ntrain_df.drop(['1stFlrSF','GarageArea','TotRmsAbvGrd'], axis=1, inplace=True)\nprint(len(train_df.columns))","031ee285":"# let's take a look at columns with missing values\n\ntrain_df.info()","f7738e2a":"train_df.drop(['Alley', 'FireplaceQu', 'PoolQC', 'Fence', 'MiscFeature'],axis=1,inplace=True)\n\n# to seperate numerical features from categorical ones\n\nnum_cols = train_df.drop('SalePrice',axis=1).select_dtypes(include='number').columns.to_list()\ncat_cols = train_df.drop('SalePrice',axis=1).select_dtypes(exclude='number').columns.to_list()\n\ntrain_df.info()","4b4700ad":"from sklearn.model_selection import train_test_split\n\nX = train.drop('SalePrice',axis=1)\ny = train.SalePrice\n\nX_train, X_val, y_train, y_val = train_test_split(X,y,test_size=0.25,random_state=42)","a1721d74":"print(len(X_train),len(X_val)) # to check the size of train set and validation set","8c17302a":"# to import necessary libraries\n\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler\nfrom category_encoders import OrdinalEncoder, HashingEncoder\nfrom sklearn.pipeline import Pipeline","577563aa":"# without pipeline\n\nnum_imputed = SimpleImputer(strategy='median').fit_transform(X_train[num_cols]) # to impute numerical data\nnum_imputed_scaled = StandardScaler().fit_transform(num_imputed) # to scale numerical data\nnum_imputed_scaled","88a14ab8":"# with pipeline\n\nnum_pipeline = Pipeline([('impute',SimpleImputer(strategy='median')),\n                         ('scale',StandardScaler())])\n\n(num_pipeline.fit_transform(X_train[num_cols]) == num_imputed_scaled).all() # to test pipeline","92b7760c":"to_dummy = ['MSZoning','Street','LotShape','LandContour','Utilities','LotConfig','LandSlope',\n            'Condition1', 'Condition2', 'BldgType','HouseStyle','RoofStyle', 'RoofMatl',\n            'MasVnrType','Foundation','Heating', 'CentralAir','Electrical','GarageType','PavedDrive']\n\nto_ordinal = ['ExterQual','ExterCond','BsmtQual','BsmtCond','BsmtExposure','BsmtFinType1',\n              'BsmtFinType2','HeatingQC','KitchenQual','Functional','GarageFinish',\n              'GarageQual','GarageCond']\n\nto_hash = ['Neighborhood','Exterior1st','Exterior2nd']\n\nothers = [col for col in cat_cols if col not in (to_dummy+to_ordinal+to_hash)]\n\nlen(cat_cols) == len(to_dummy+to_ordinal+to_hash+others) # to check if all cat columns were included","38e0a298":"po_to_ex = {'Po':1,'Fa':2, 'TA':3, 'Gd':4, 'Ex':5}\nbsmt_exp = {'No':0, 'Mn':1, 'Av':2, 'Gd':3}\nbsmt_type = {'Unf':0, 'LwQ':1, 'Rec':2, 'BLQ':3, 'ALQ':4, 'GLQ':5}\nfunctional = {'Sal':0, 'Sel':1, 'Maj1':2, 'Maj2':2, 'Mod':3, 'Min1':4,'Min2':4,'Typ':5}\ngrg_fnsh = {'Unf':1, 'RFn':1, 'Fin':2}","7595ab4e":"po_to_ex_cols = ['ExterQual','ExterCond','BsmtQual','BsmtCond','HeatingQC','KitchenQual','GarageQual','GarageCond']\n\npo_to_ex_maps = [{'col':col,'mapping':po_to_ex} for col in po_to_ex_cols]\nother_maps = [{'col':'BsmtExposure','mapping':bsmt_exp},{'col':'BsmtFinType1','mapping':bsmt_type},\n          {'col':'BsmtFinType2','mapping':bsmt_type},{'col':'Functional','mapping':functional},\n          {'col':'GarageFinish','mapping':grg_fnsh}]\nmapping = po_to_ex_maps + other_maps\n\nlen(mapping) == len(to_ordinal) # to check that all ordinal columns have been considered","d8fac773":"# without pipeline\n\ncat_imputed = SimpleImputer(strategy='most_frequent').fit_transform(X_train[cat_cols]) # to impute the data\nimputed_df = pd.DataFrame(cat_imputed,columns=cat_cols) # to reconstruct the dataframe\n\ndummy_data = OneHotEncoder(handle_unknown='ignore',sparse=False).fit_transform(imputed_df[to_dummy]) # to create dummy variables\n\nordinal_data = OrdinalEncoder(mapping=mapping).fit_transform(imputed_df[to_ordinal]) # to map ordinal data\n\nhashed_data = HashingEncoder().fit_transform(imputed_df[to_hash]) # to create hashed variables\n\nother_data = imputed_df[others].values  # to create an array of other categories\n\ncat_imputed_encoded = np.c_[dummy_data,ordinal_data,hashed_data,other_data] # to concatenate all categorical data\ncat_imputed_encoded","3633332a":"class EncodeCategories():\n    cat_cols = cat_cols\n    to_dummy = to_dummy\n    to_ordinal = to_ordinal\n    to_hash = to_hash\n    others = others\n    \n    def __init__(self,n_components=8): # The n_components attributes was included for the purpose of hyperparameter tuning\n        self.n_components = n_components\n        self.one_hot = OneHotEncoder(handle_unknown='ignore',sparse=False)\n        self.ordinal = OrdinalEncoder(mapping=mapping)\n        self.hash = HashingEncoder(n_components=n_components)\n    \n    def fit(self,X,y=None):\n        df = pd.DataFrame(X,columns=cat_cols)  # to turn the input array to dataframe\n        \n        # fit the data to their respective encoders\n        \n        self.one_hot.fit(df[to_dummy])\n        self.ordinal.fit(df[to_ordinal])\n        self.hash.fit(df[to_hash])\n        \n        return self # return the fitted object\n    \n    def transform(self,X,y=None):\n        df = pd.DataFrame(X,columns=self.cat_cols)  # to turn the input array to dataframe\n        \n        # to create a dataframe of dummy variables \n        dummies = pd.DataFrame(self.one_hot.transform(df[to_dummy]),columns=self.one_hot.get_feature_names(to_dummy))\n        \n        # to map ordinal data and create dataframe\n        ordinals = pd.DataFrame(self.ordinal.transform(df[to_ordinal]),columns=to_ordinal)\n        \n        # to create a dataframe hashed variables\n        hashes = pd.DataFrame(self.hash.transform(df[to_hash]),columns=self.hash.get_feature_names())\n        \n        other = df[others] # to create a dataframe of other categories\n        \n        transformed_df = pd.concat([dummies,ordinals,hashes,other],axis=1)  # to concatenate all categories\n        \n        return transformed_df # to return a dataframe of the transformed data\n    \n    def fit_transform(self,X,y=None):\n        df = pd.DataFrame(X,columns=cat_cols)  # to turn the input array to dataframe\n        \n        # fit the data to their respective encoders\n        \n        self.one_hot.fit(df[to_dummy])\n        self.ordinal.fit(df[to_ordinal])\n        self.hash.fit(df[to_hash])\n        \n        # to create a dataframe of dummy variables \n        dummies = pd.DataFrame(self.one_hot.transform(df[to_dummy]),columns=self.one_hot.get_feature_names(to_dummy))\n        \n        # to map ordinal data and create dataframe\n        ordinals = pd.DataFrame(self.ordinal.transform(df[to_ordinal]),columns=to_ordinal)\n        \n        # to create a dataframe hashed variables\n        hashes = pd.DataFrame(self.hash.transform(df[to_hash]),columns=self.hash.get_feature_names())\n        \n        other = df[others] # to create a dataframe of other categories\n        \n        transformed_df = pd.concat([dummies,ordinals,hashes,other],axis=1)  # to concatenate all categories\n        \n        self.features = transformed_df.columns.to_list() # to store the column names\n        \n        return transformed_df  # to return a dataframe of the transformed data","12ade977":"# with pipeline\n\ncat_pipeline = Pipeline([('impute',SimpleImputer(strategy='most_frequent')),\n                        ('encode',EncodeCategories())])\n\n(cat_pipeline.fit_transform(X_train[cat_cols]).values == cat_imputed_encoded).all() # to test pipeline","0c3ec14b":"preprocessor = ColumnTransformer([('num',num_pipeline,num_cols),('cat',cat_pipeline,cat_cols)])\n\n# distance based algorithms are sensitive to scaling. We'll create another pipeline to only handle missing values for numerical data\nrnd_preprocessor = ColumnTransformer([('num',SimpleImputer(strategy='median'),num_cols),('cat',cat_pipeline,cat_cols)])\n\n(preprocessor.fit_transform(X_train) == np.c_[num_imputed_scaled,cat_imputed_encoded]).all() # to test single pipeline","e2b7e6fa":"sns.boxplot(y);","0699f73f":"train_dropped = train.drop(y[y>700000].index)  # to drop outliers\n\n# to recreate the matrix of features and target variable vector\nX = train_dropped.drop('SalePrice',axis=1)\ny = train_dropped.SalePrice\n\nX_train, X_val, y_train, y_val = train_test_split(X,y,test_size=0.25,random_state=42)","de75b3a2":"# to import the necessary algorithms\n\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import RandomForestRegressor","75fdbcd2":"# to create the models\n\nlinear_reg = LinearRegression()\ndecision_tree = DecisionTreeRegressor(random_state=42)  # random state parameter is to ensure we get the same reults each time we run the code\nrnd_forest = RandomForestRegressor(random_state=42)","3fd7377c":"# to import necessary libraries for evaluation purposes\n\nfrom sklearn.metrics import mean_squared_error as mse","b2c42b61":"# to train and evaluate the models\nmodels = [linear_reg, decision_tree, rnd_forest]\n\n# to fit and transform input data\nX_train_prepared = preprocessor.fit_transform(X_train)\nX_val_prepared = preprocessor.transform(X_val)\n\n# to fit the models\nfitted_models = [model.fit(X_train_prepared,y_train) for model in models]\n\n# to make predictions\ntrain_preds = [fitted_model.predict(X_train_prepared) for fitted_model in fitted_models]\n\nval_preds = [fitted_model.predict(X_val_prepared) for fitted_model in fitted_models]\n\n\n# to evaluate performance\ntrain_scores = [(mse(preds,y_train))**0.5 for preds in train_preds]\nval_scores = [(mse(preds,y_val))**0.5 for preds in val_preds]\n\n# to create a dictionary of the performance of the models\nzipped_scores = zip(train_scores,val_scores)\nscores = {model[0]:model[1] for model in zip(models,zipped_scores)}\nscores","80fca9b9":"# let's define a function to get training and validation scores\ndef get_scores(model):\n    training_instances = []\n    train_scores = []\n    val_scores = []\n    for i in range(1,len(X_train_prepared)+1):\n        training_instances.append(i)\n        model = model\n        model.fit(X_train_prepared[:i],y_train[:i])\n        train_scores.append(mse(model.predict(X_train_prepared[:i]),y_train[:i])**0.5)\n        val_scores.append(mse(model.predict(X_val_prepared[:i]),y_val[:i])**0.5)\n    return training_instances, train_scores, val_scores\n\n# let's define a function to plot learning curve\ndef plot_scores(training_instances,train_scores,val_scores):\n    sns.lineplot(x=training_instances,y=train_scores,label='train')\n    sns.lineplot(x=training_instances,y=val_scores, label='validation')\n    plt.xlabel('Training Instances')\n    plt.ylabel('Scores')\n    plt.legend()\n    plt.show()","2a0024c1":"inst,train_scores,val_scores = get_scores(DecisionTreeRegressor(random_state=42))  # get scores\nplot_scores(inst,train_scores,val_scores)  # plot curve","9af9c14e":"X_prepared = preprocessor.fit_transform(X)  # to transform the feature matrix","38ac6dd4":"X_prepared.shape","c653855e":"# import cross validation\n\nfrom sklearn.model_selection import cross_val_score\n\n# define a function to display the scores\n\ndef display_scores(model):\n    scores = cross_val_score(model, X_prepared, y,\n                            scoring=\"neg_mean_squared_error\", cv=5)\n    rmse_scores = np.sqrt(-scores)\n    print(model)\n    print(\"Scores:\", rmse_scores)\n    print(\"Mean:\", rmse_scores.mean())\n    print(\"Standard deviation:\", rmse_scores.std())\n    print('\\n')","2fb34081":"models = [LinearRegression(),DecisionTreeRegressor(random_state=42),RandomForestRegressor(random_state=42)]\n\nfor model in models:\n    display_scores(model)","19901230":"from sklearn.model_selection import GridSearchCV\nparam_grid = [\n {'bootstrap': [False,True],'n_estimators': [30,60,90,100,110,120,130,140], 'max_features':  [2,10,20,30,40,50,60]},\n  ]\nforest_reg = RandomForestRegressor()\ngrid_search = GridSearchCV(forest_reg, param_grid, cv=5,\n                            scoring='neg_mean_squared_error', return_train_score=True)\ngrid_search.fit(X_prepared, y)","11a56c50":"grid_search.best_estimator_","3d3903d8":"cvres = grid_search.cv_results_\nfor mean_score, params in zip(cvres[\"mean_test_score\"], cvres[\"params\"]):\n    print(np.sqrt(-mean_score), params)","c388c10b":"categories = cat_pipeline.named_steps['encode'].features  # to get the transformd categories\n\nfeature_names = num_cols+categories\n\nimportances = sorted(zip(grid_search.best_estimator_.feature_importances_,feature_names),reverse=True) # to sort the importances","42efa469":"# let's plot the top 25 features\n\nx = [importance[0] for importance in importances[:25]]  # to get the x-axis values\ny = [importance[1] for importance in importances[:25]]  #to get the y-axis values\n\nplt.figure(figsize=(10,15))\nsns.barplot(x=x,y=y);","0142a6c1":"# to create a model wth the best parameters\nmodel = grid_search.best_estimator_\n\n# to bundle the preprocessor with the model\nfull_pipeline = Pipeline([('preprocessor',preprocessor),('model',model)])\n\n# to get predictions of the test data\ntest_pred = full_pipeline.predict(test)","4bc5211c":"# to create the predictions dataframe\nsubmission_df = pd.DataFrame({'Id':test.Id,'SalePrice':test_pred})\n\n# to save the predictions dataframe to a file\noutput = submission_df.to_csv('submission.csv',index=False)","b8a27532":"We can tune the hyperparameters of the best performing model to improve it performance","641d0ebe":"There seems to be lots of outliers but the two points above 700,000 are too far away from the rest. We should remove them","9b2e8359":"The Living Area, Basement area and garage-related features are highly correlated with the sale price. Let's see the heatmap for the all the correlation values ","b29f4266":"We'll create a list of columns to use the **po_to_ex** mapping and specify the mapping for other columns. We'll then create our final mapping list","9a471b46":"Yet again, Random forest is the best performer. However, this validation score is far from the training score. Reducing the noise in the data might improve the score","111f169b":"SalePrice, the target variable, seems to be okay for now (minimum value is **above** zero). Let's see how the house prices are distributed","4d6122f0":"## Next Steps\n\n* Let's see the average house prices across different neighborhoods\n* We should also consider a boxplot of **HouseStyle** against **SalePrice**\n* Finally, we'll see the relationship between **SalePrice**, **GarageArea**, and **GarageCond**","e05271ca":"## Data Leakage\n\nThere are signs of **Target Leakage**. Some predictors like **MoSold**, **YrSold**, **SaleType**, and **SaleCondition** include information that will be unavailable at the time we make predictions.","c9825bab":"# **Hyperparameter Tuning** ","9e0bcc9f":"So far we've explored the input features. Now, let's see check the target variable for possible outliers","0f34d789":"# **Data Pre-processing**","65c57ec3":"## Decision Tree Regressor Learning Curve","9770d416":"Everything seems right. We can now bundle the preprocessing steps into a single pipeline","4d65ab63":"Apart from **LotArea** and **BsmtUnfSF**, which do not have obvious patterns, other attributes are linearly related to **SalePrice**\n\nLet's also see how **SalePrice** changes across some categorical features","e9a31168":"Adding more training data would reduce overfitting as the two curves converge. We should train the models on the full train set and use a more reliable evaluation metric, cross validation.","c3f31e11":"The linear regressor underfits while the decision tree overfits very badly. The random forest model also overfits but not as much as  the decision tree. Let's plot the learning curve for the decision tree model","fa5dc836":"One common feature among **1Fam**, **TwnhsE**, **2Story** and **2.5Fin** houses is privacy. For example, occupants of End-unit town houses enjoy more quietness than those in Inside-unit town houses","3c8aa0da":"Let's build a custom transformer to handle all the encoding techniques","05b5f032":"* **OrdinalEncoder** has a mapping parameter that maps attributes to values \n\n* The mapping parameter takes a list of dictionaries for every column in the data\n\n* Let's create the mapping dictionaries. Some columns have similar categories so, we do not have to create seperate dictionaries for them","ae967414":"Firstly, there's a spike at **zero** in some of the distributions. It occurs that many houses in the dataset do not have a basement and pool, among other features.\n\nSecondly, the attributes have different scales. We'll have to do feature scaling before feeding them to our model. This would help our model converge faster\n\nFinally, many histograms are tail heavy. We might have to transform them later to have more bell-shaped distributions\n\nLet's handpick some promising features to see how they affect **SalePrice**","26a4a42b":"As shown above, there's no discernible relationship between **GarageCond** and **SalePrice**. Buyers seem to be more concerned with the size of the garage than its condition. However, they appear to pay less for poor looking garages","99c3361f":"## Few things to note\n\n1. Houses located in sparsely populated areas cost more. **FV** which stands for _Floating Village Residential_ houses, which are built on water, cost more on an average than houses built on land\n\n2. Houses located on paved streets are valued more than those on gravel streets\n\n3. Houses in some neighborhoods clearly cost more than others\n\n4. The condition of a home clearly affects its value. However, home buyers might not consider spending a couple more bucks on houses above average condition\n\n5. Recently built\/remodelled houses cost more than old houses\n\n6. Kitchen and Exterior quality and condition are some of the factors buyers consider when buying a house. Houses with poor exteriors are valued the lowest\n\n7. Unlike GarageArea, excellent garage condition does not guarantee high **SalePrice**. We should see how houses with poor garage condition and large area are valued\n\n8. Newly constructed houses tend to be sold for higher prices even if they were partially completed when assessed by buyers. ","8dcaff53":"Let's have a look at the columns once again","0b270ed5":"## Imputation, Scaling and Encoding\n\nThis section will involve building pipelines for numerical and categorical columns. For numerical columns, we'll impute and scale while for categorical columns, we'll impute and encode","250fa682":"There are so many missing values in **Alley**, **FirePlaceQu**, **PoolQc**, **Fence**, **MiscFeature**. They should be dropped","e366aa8f":"1. Categorical features appear to be of the **_object_** data type. Some categorical features like **YearBuilt** are also of the **_integer_** data type. We''ll have to convert them to the **_category_** data type\n \n2. There are lots of missing values in the **Alley, FireplaceQu, PoolQC, Fence** and **MiscFeature** columns. We'll take care of that later\n  ","0439d249":"We'll fill numerical columns with the median and categorical columns with the most frequent value\n\nBefore fitting the data, we should create our matrix of features, target variable vector and holdout a validation set to evaluate our model performance","183f4eb0":"We now have a feel of our data. Let's prepare our data for machine learning models\n\nIn this section we'll:\n* Identify and remove irrelevant and redundant features\n* Handle Missing Values\n* Encode Categorical Variables\n* Scale and Transform the data\n* Build pipelines to automate the pre-processing stage","27b503f4":"# **Data Exploration and Cleaning**","2cdd5434":"## Handling Missing Values","0643b738":"Let's impute and encode the categorical columns without a pipeline ","f9ed6548":"## Outliers Detection","9d3ba61b":"## Multicollinearity\n\n**TotalBsmtSF** and **1stFlrSF** are highly correlated. We'll drop **1stFlrSF** since **TotalBsmtSF** has a higher correlation value with **SalePrice**\n\nSame thing goes for **GarageCars** and **GarageArea**. We'll drop **GarageArea**\n\nWe'll also drop **TotalRmsAbvGr**\n","10d601f6":"Numerical pipeline works well. Let's do the same for categorical pipeline\n\n\n## Creating Categorical Pipeline\n\nWe'll like to impute all the columns and apply different encoding techniques. Let's create 4 lists : **to_dummy**, **to_ordinal**, **to_hash** and **others**\n\n* **to_dummy**: contains columns that would be one-hot encoded. This columns have few categories\n* **to_ordinal**: contains columns with ordered categories\n* **to_hash**: contains columns that have to many categories to be one-hot-encoded\n* **others**: contains columns **not** to be encoded","8b158e70":"# **Output**","dcd004ce":"# **Model Building**\n\n* We'll train our preprocessed data on the Linear Regressor, Decision Tree Regressor and Random Forest Regressor\n\n* We'll also tune the hyperparameters of the best performing model and train it on the whole training set","2c377076":"## Creating Numerical Pipeline"}}