{"cell_type":{"25f8e8f5":"code","664ce4c5":"code","fd136f30":"code","e0127def":"code","9a984abd":"code","1742207f":"code","f6152c1f":"code","872b549b":"markdown","e8d7f6ae":"markdown","db7537ea":"markdown","3ce269bd":"markdown","430f52b3":"markdown","6fd14d4e":"markdown","a024f281":"markdown","297ba6bb":"markdown"},"source":{"25f8e8f5":"# Basic lib \nimport numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# PCA\nfrom sklearn.decomposition import PCA\n\n#ignore warning messages \nimport warnings\nwarnings.filterwarnings('ignore') \n\nsns.set()","664ce4c5":"# Get the train and test data \ntrain = pd.read_csv(\"..\/input\/train.csv\")\n# test = pd.read_csv(\"..\/input\/train.csv\")\n\n# Splite data the X - Our data , and y - the prdict label\nX = train.drop('label',axis = 1)\ny = train['label'].astype('category')\n\nX.head()","fd136f30":"dataset_7 = X[y == 7].reset_index().drop(\"index\",axis = 1)\n\nplt.figure(figsize = (10,8))\nrow, colums = 3, 3\nfor i in range(9):  \n    plt.subplot(colums, row, i+1)\n    plt.imshow(dataset_7.iloc[i].values.reshape(28,28),interpolation='nearest', cmap='Greys')\nplt.show()\n","e0127def":"n_components = 5 # the components number can change to every number that you want.\n\n# Create PCA with components number\npca = PCA(n_components = n_components)\n# fit transform with PCA on dataset\npca_dataset_7 = pca.fit_transform(dataset_7)\n# inverse transform back to regular dataset \ninverse_transform_dataset_7 = pca.inverse_transform(pca_dataset_7)\n\nprint(\"dataset_7 shape\",dataset_7.shape)\nprint(\"pca_dataset_7 shape\",pca_dataset_7.shape)\nprint(\"inverse_transform_dataset_7 shape\",inverse_transform_dataset_7.shape)","9a984abd":"# Check the diffrent between X and the inverse_transform_X\n# (X-inverse_transform_X)**2) = MSE for any pixel, and we make sum() for get image MSE.\nMSE_score = ((dataset_7-inverse_transform_dataset_7)**2).sum(axis=1)\n\nMSE_score.head()","1742207f":"MSE_max_scores = MSE_score.nlargest(9).index\n\nplt.figure(figsize = (10,8))\nrow, colums = 3, 3\nfor i in range(9):  \n    plt.subplot(colums, row, i+1)\n    plt.imshow(dataset_7.iloc[MSE_max_scores[i]].values.reshape(28,28),interpolation='nearest', cmap='Greys')\nplt.show()","f6152c1f":"plt.figure(figsize = (10,8))\nrow, colums = 5, 10\n    \nfor number in range(10):\n    # Get the current number\n    dataset = pd.DataFrame(X[(y == number)].reset_index().drop(\"index\",axis = 1))\n    # Create PCA with components number\n    pca = PCA(n_components = n_components)\n    # fit transform with PCA on dataset\n    pca_dataset = pca.fit_transform(dataset)\n    # inverse transform back to regular dataset \n    inverse_transform_dataset = pca.inverse_transform(pca_dataset)\n    MSE_score = ((dataset-inverse_transform_dataset)**2).sum(axis=1)\n    MSE_worst = MSE_score.nlargest(5).index # get max\n    for number2 in range(0,5):\n        plt.subplot(colums, row, (number2+(number*5))+ 1)\n        plt.imshow(dataset.iloc[MSE_worst[number2]].values.reshape(28,28),interpolation='nearest', cmap='Greys')\nplt.show()","872b549b":"We take the observations with the max MSE's and plot it","e8d7f6ae":"And this is it! we found our outliers","db7537ea":"now we have inverse_transform_dataset_7 and dataset_7, we can check the diffrent between them by MSE","3ce269bd":"We can make the same thing with any number","430f52b3":"Let start only with the number 7,\nand plot the samples","6fd14d4e":"What is PCA? - Principal component analysis\n\nPrincipal component analysis (PCA) is a statistical procedure that uses an orthogonal transformation to convert a set of observations of possibly correlated variables (entities each of which takes on various numerical values) into a set of values of linearly uncorrelated - You can read more in wikipedia: [LINK](https:\/\/en.wikipedia.org\/wiki\/Principal_component_analysis)\n\n\nPCA reduction the data to a lower dimensional space. [sklearn link](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.decomposition.PCA.html)\n\nWe can ues the PCA to reduce to a lower dimensional and then use inverse_transform to reconstruction back, \nWe can change how mush the reconstruction data is diffrent from the regular data (with MSE) and find out how mush any observation is diffrent from all the data.\nThe observation with max MSE will be candidate to bar outlier.\n","a024f281":"Any image is with 784 pixels (28X28).","297ba6bb":"This 7 images is the most diffrent 7 in the dataset, and we can see that they are really not look like a 7"}}