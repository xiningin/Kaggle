{"cell_type":{"f1e98960":"code","095822a3":"code","dd6c61ed":"code","448172fc":"code","d8d9f178":"code","b0f343a6":"code","896e8523":"code","aeb3c53e":"code","64823f29":"code","b2418bfe":"code","792305f4":"code","0b2eface":"code","32dad063":"code","6f8ff340":"code","8a07efbf":"code","04ebf8ad":"code","92fdc411":"code","165eaa00":"code","9229a465":"markdown","9edc09ba":"markdown","509042df":"markdown","0756183d":"markdown","18286d14":"markdown","eb73105c":"markdown","a8bc3674":"markdown","23997de9":"markdown","a8627028":"markdown","c2fbc6f7":"markdown"},"source":{"f1e98960":"#Basics\nimport numpy as np\nimport pandas as pd\nimport progressbar\n\n#Sklearn\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn import metrics\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\n#NLP\nimport nltk\nimport re\nimport string\nfrom nltk.stem import WordNetLemmatizer \nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize \nstop_words = set(stopwords.words(\"english\")) \nlemmatizer = WordNetLemmatizer() \n\n#Others\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nimport sys\nsys.setrecursionlimit(5000)\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","095822a3":"TRAIN_DATA_PATH = \"\/kaggle\/input\/jigsaw-toxic-comment-classification-challenge\/train.csv\"\nTEST_DATA_PATH = \"\/kaggle\/input\/jigsaw-toxic-severity-rating\/comments_to_score.csv\"\nVALID_DATA_PATH = \"..\/input\/jigsaw-toxic-severity-rating\/validation_data.csv\"\nSAMPLE_SUBMISSION = \"\/kaggle\/input\/jigsaw-toxic-severity-rating\/sample_submission.csv\"","dd6c61ed":"df_train = pd.read_csv(TRAIN_DATA_PATH)\ndf_validation_data = pd.read_csv(VALID_DATA_PATH)\ndf_sample_submission = pd.read_csv(SAMPLE_SUBMISSION)","448172fc":"def clean_text(text):\n#will replace the html characters with \" \"\n    text=re.sub('<.*?>', ' ', text)  \n    #To remove the punctuations\n    text = text.translate(str.maketrans(' ',' ',string.punctuation))\n    #will consider only alphabets and numerics\n    text = re.sub('[^a-zA-Z]',' ',text)  \n    #will replace newline with space\n    text = re.sub(\"\\n\",\" \",text)\n    #will convert to lower case\n    text = text.lower()\n    # will split and join the words\n    text=' '.join(text.split())\n    return text\n\ndef stopwords(input_text, stop_words):\n    word_tokens = word_tokenize(input_text) \n    output_text = [w for w in word_tokens if not w in stop_words]\n    output = [] \n    for w in word_tokens: \n        if w not in stop_words:\n            output.append(w)\n            \n    text = ' '.join(output)\n    return text","d8d9f178":"unrelevant_words = ['wiki','wikipedia','page']\n#Clean step 1, 2 and 3\ndf_train['comment_text'] = df_train['comment_text'].apply(lambda x: ''.join([w for w in clean_text(x) if w not in unrelevant_words]))\n\n#Clean Step 4\ndf_train['comment_text'] = df_train['comment_text'].apply(lambda x: ''.join([w for w in stopwords(x,stop_words)]))\n\n#Clean Step 5\ndf_train['comment_text'] = df_train['comment_text'].apply(lambda x: ''.join([w for w in lemmatizer.lemmatize(x)]))\n\ndf_train.head(3)","b0f343a6":"#Clean step 1, 2 and 3\ndf_validation_data['less_toxic'] = df_validation_data['less_toxic'].apply(lambda x: ''.join([w for w in clean_text(x) if w not in unrelevant_words]))\ndf_validation_data['more_toxic'] = df_validation_data['more_toxic'].apply(lambda x: ''.join([w for w in clean_text(x) if w not in unrelevant_words]))\n\n#Clean Step 4\ndf_validation_data['less_toxic'] = df_validation_data['less_toxic'].apply(lambda x: ''.join([w for w in stopwords(x,stop_words)]))\ndf_validation_data['more_toxic'] = df_validation_data['more_toxic'].apply(lambda x: ''.join([w for w in stopwords(x,stop_words)]))\n\n#Clean Step 5\ndf_validation_data['less_toxic'] = df_validation_data['less_toxic'].apply(lambda x: ''.join([w for w in lemmatizer.lemmatize(x)]))\ndf_validation_data['more_toxic'] = df_validation_data['more_toxic'].apply(lambda x: ''.join([w for w in lemmatizer.lemmatize(x)]))\n\ndf_validation_data.head(3)","896e8523":"# Create a score that messure how much toxic is a comment\ncat_mtpl = {'obscene': 0.10, 'toxic': 0.20, 'threat': 0.5, \n            'insult': 0.62, 'severe_toxic': 0.7, 'identity_hate': 1}\n\nfor category in cat_mtpl:\n    df_train[category] = df_train[category] * cat_mtpl[category]\n\ndf_train['score'] = df_train.loc[:, 'toxic':'identity_hate'].sum(axis=1)","aeb3c53e":"#Clases aren't balanced, that's whya a balance is needed\n#Balance the clases\nn_samples_toxic = len(df_train[df_train['score'] != 0])\nn_samples_normal = len(df_train) - n_samples_toxic\n\nidx_to_drop = df_train[df_train['score'] == 0].index[n_samples_toxic\/\/5:]\ndf_train = df_train.drop(idx_to_drop)\n\nprint(f'Reduced number of neutral text samples from {n_samples_normal} to {n_samples_toxic\/\/5}.')\nprint(f'Total number of training samples: {len(df_train)}')\n","64823f29":"#Feature engineering for creating \ndf_tragets = pd.DataFrame(pd.unique(df_train['score'].values), columns=['target_value']).sort_values(by='target_value', ascending = True).reset_index(drop=True)\nTHRESHOLD = df_tragets['target_value'].quantile(q=0.2)\ndf_train['sentiment'] = df_train['score'].map(lambda x: 1 if x < THRESHOLD else 2 if x < THRESHOLD*2 else 3 if x < THRESHOLD*3 else 4 if x < THRESHOLD*4 else 5)\n\ndf_train = df_train[['comment_text','sentiment']].reset_index(drop=True)\ndf_train","b2418bfe":"tf_idf_vect = TfidfVectorizer(analyzer='word',stop_words= 'english')\nX = tf_idf_vect.fit_transform(df_train['comment_text'])\nX","792305f4":"model = MultinomialNB()\nmodel.fit(X, df_train['sentiment'])","0b2eface":"X_less_toxic = tf_idf_vect.transform(df_validation_data['less_toxic'])\nX_more_toxic = tf_idf_vect.transform(df_validation_data['more_toxic'])","32dad063":"p1 = model.predict_proba(X_less_toxic)\np2 = model.predict_proba(X_more_toxic)","6f8ff340":"(p1[:, 1] < p2[:, 1]).mean()","8a07efbf":"df_test = pd.read_csv(TEST_DATA_PATH)\n\n#Clean step 1, 2 and 3\ndf_test['text'] = df_test['text'].apply(lambda x: ''.join([w for w in clean_text(x) if w not in unrelevant_words]))\n\n#Clean Step 4\ndf_test['text'] = df_test['text'].apply(lambda x: ''.join([w for w in stopwords(x,stop_words)]))\n\n#Clean Step 5\ndf_test['text'] = df_test['text'].apply(lambda x: ''.join([w for w in lemmatizer.lemmatize(x)]))\n\ndf_test.head(3)","04ebf8ad":"X_test = tf_idf_vect.transform(df_test['text'])\np3 = model.predict_proba(X_test)\n\ndf_test['score'] = p3[:, 1]\ndf_test = df_test[['comment_id','score']]","92fdc411":"df_test","165eaa00":"df_test.to_csv('.\/submission.csv', index=False)","9229a465":"# Naive Bayes\n\nNaive Bayes is an approach selected becaouse of its well balance between fast modeling and competitive results.\n\nThe same as tf idf, just type this concept and lot of knwoladge will come to your eyes.","9edc09ba":"Hater comments are arround us, every time in every social netwrok. Thansk to [Jigsaw](https:\/\/www.kaggle.com\/c\/jigsaw-toxic-severity-rating) for pomoving this kind of competition in order to has a clear internet improving our moderation methods.\n\nIn this notebook you will an approach using Navie Bayes classifier because this is a very simple model with a strong stadistic point of view.\n\nI always try to create the best notebooks as possible, and i hope you reciebe it as well. <b>So please if you find it interesting, UPVOTE my kernel<\/b>\nyou probably know how much positive impact does it have to our work as data scientist\n\n","509042df":"<h2>Upload Datasets<\/h2>\n\nNote that the train data is not provided in this competition, but jigsaw provided information about sentiment in texts in another competition before.","0756183d":"<h2>Imports<\/h2>","18286d14":"<div style=\"font-weight: bold;font-size:40px;background-color: #A0E8FF;padding:0 5%; color:#2F4F4F\">Toxic Comments<\/div>\n\n<center><img src=\"https:\/\/www.institutotraficoonline.com\/wp-content\/uploads\/2021\/02\/haters-instituto-trafico-online-ito-roberto-gamboa.jpg\" width=400><\/center>\n","eb73105c":"# Scoring training data\n\nThe data provided in this contest does not include training data, so it is important to find another data source. This needs to be transformed into a score to create a toxicity scale.\n\nAlso, clases are desbalanced, thats why some taks need to be done.","a8bc3674":"## Validate\n\nThis is very usefull because is a valid approach about the performance of this model.","23997de9":"# Predictions and load submission.csv","a8627028":"<h2>Clean text<\/h2>\n\nThe most important step in machine learnign process, is to clean de data because \"Garbage in, garbage out\".\n\n\nAs this [post recomend](https:\/\/medium.com\/analytics-vidhya\/text-cleaning-in-natural-language-processing-nlp-bea2c27035a6), this clean data will include:\n\n1. Removing HTML characters,ASCII\n2. Convert Text to Lowercase\n3. Remove Punctuation's\n4. Remove Stop words\n5. Stemming\n\n1, 2 and 3 are included in the clean_text function. On the other hand 4 and 5 both are included in stopword function","c2fbc6f7":"## TF-IDF\n\n[Term frequency \u2013 Inverse document frequency](https:\/\/es.wikipedia.org\/wiki\/Tf-idf) is one of the most popular techniques used in the land of NLP to represent words by numbers. Thats just for one simple reason: **COMPUTERS ONLY UNDERSTAND NUMBERS.**\n\nIf you want to dive into this concept, just type tfidf on google search to find a watherfalls of posts about this topics."}}