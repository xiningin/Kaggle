{"cell_type":{"264f343a":"code","d5c39680":"code","3526d3c5":"code","7127e5ba":"code","adc9b8f4":"code","46d8de32":"code","ebfdc547":"code","61f12d34":"code","dce2d8df":"code","893bcff4":"code","6fff2e09":"code","9391dfb0":"code","5b614507":"code","e9501306":"code","50f6785f":"code","bc94e18b":"code","ac9c88c5":"code","01b66640":"code","7ffd1b23":"code","7b3fa289":"code","106607c7":"code","59c38015":"code","d80a134b":"code","468315b9":"code","01bf46a1":"code","2d331852":"code","3ffd2d6f":"code","282ddc45":"code","65378f64":"code","cff79f42":"code","865b20d1":"code","8e12e3c9":"code","6bf20e01":"code","f8c7b893":"code","f12b2fab":"code","96fb77db":"code","6b27c259":"code","aab42450":"code","a07001de":"code","dca67b64":"code","39b44ab8":"code","c2ebafef":"code","11e54067":"code","7aad427f":"markdown","fd713bfc":"markdown","87bc4a2d":"markdown","a537d7df":"markdown"},"source":{"264f343a":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport plotly.express as px\nimport seaborn as sns\nfrom catboost import CatBoostClassifier","d5c39680":"df = pd.read_csv('\/kaggle\/input\/song-popularity-prediction\/train.csv')\ndf_test = pd.read_csv('\/kaggle\/input\/song-popularity-prediction\/test.csv')\ndf.head(10)","3526d3c5":"df.shape","7127e5ba":"df_test.shape","adc9b8f4":"df.describe()","46d8de32":"df.astype('object').describe()","ebfdc547":"#null value count \nprint(\"Null value percentage in train set%\")\nprint('-'*30)\nprint(df.isna().sum()\/df.shape[0] * 100)\nprint('-'*30)\n\nprint(\"Null value percentage in test set%\")\nprint('-'*30)\nprint((df_test.isna().sum()\/df_test.shape[0]) * 100)\nprint('-'*30)","61f12d34":"print(f\"Total missing values:\\nTrain set :{sum(df.isna().sum())\/(df.shape[0]*df.shape[1])*100}\\nTest set :{sum(df_test.isna().sum())\/(df_test.shape[0]*df_test.shape[1])*100}\")","dce2d8df":"null_train = df.isna().sum()\/df.shape[0] * 100\nnull_test = df_test.isna().sum()\/df_test.shape[0] * 100","893bcff4":"fig = px.bar(y=df_test.columns,x=null_test,orientation = 'h', barmode='relative',width = 1000,\n             title = \"Missing values % : Train VS Test set\")\nfig.add_bar(name='train set',y=df.columns,x=null_train,orientation='h')\nfig.show()","6fff2e09":"df.shape[0]*14","9391dfb0":"FEATURES = [col for col in df.columns if col not in ['id', 'song_popularity']]\ndata = pd.concat([df[FEATURES], df_test[FEATURES]], axis=0)\n\ncat_features = [col for col in FEATURES if df[col].nunique() < 15]\ncont_features = [col for col in FEATURES if df[col].nunique() >= 15]\n\n\nprint(f'Total number of features: {len(FEATURES)}')\nprint(f'\\033[92mNumber of categorical features: {len(cat_features)}')\nprint(f'\\033[96mNumber of continuos features: {len(cont_features)}')\n\nplt.pie([len(cat_features), len(cont_features)], \n        labels=['Categorical', 'Continuos'],\n        colors=['#DE3163', '#58D68D'],\n        textprops={'fontsize': 13},\n        autopct='%1.1f%%')\nplt.show()","5b614507":"\n\nncols = 5\nnrows = int(len(cont_features) \/ ncols + (len(FEATURES) % ncols > 0))-1\n\nfig, axes = plt.subplots(nrows, ncols, figsize=(18, 10), facecolor='#EAEAF2')\n\nfor r in range(nrows):\n    for c in range(ncols):\n        col = cont_features[r*ncols+c]\n        sns.histplot(x= df[col], ax=axes[r, c], color='cyan', label='Train data' , fill =True , kde = True)\n        sns.histplot(x=df_test[col], ax=axes[r, c], color='pink', label='Test data', fill =True, kde = True)\n        axes[r,c].legend()\n        axes[r, c].set_ylabel('')\n        axes[r, c].set_xlabel(col, fontsize=8)\n        axes[r, c].tick_params(labelsize=5, width=0.5)\n        axes[r, c].xaxis.offsetText.set_fontsize(4)\n        axes[r, c].yaxis.offsetText.set_fontsize(4)\nplt.title(\"Distribution of Continuos variables in the data\")\nplt.show()\n\n","e9501306":"if len(cat_features) == 0 :\n    print(\"No Categorical features\")\nelse:\n    ncols = 3\n    nrows = 1\n\n    fig, axes = plt.subplots(nrows, ncols, figsize=(18, 5))\n    plt.title(\"Distribution of categorical variables\")\n    for r in range(nrows):\n        for c in range(ncols):\n            col = cat_features[c]\n            sns.countplot( x =df[col],ax = axes[c] ,color = \"blue\", label='Train data')\n            sns.countplot( x = df_test[col],ax = axes[c] ,color = \"cyan\", label='Test data')\n            axes[c].legend()\n            axes[c].set_ylabel('')\n            axes[c].set_xlabel(col, fontsize=20)\n            axes[c].tick_params(labelsize=10, width=0.5)\n            axes[c].xaxis.offsetText.set_fontsize(4)\n            axes[c].yaxis.offsetText.set_fontsize(4)\n    plt.show()","50f6785f":"df.song_popularity.value_counts()","bc94e18b":"px.pie(df, df.song_popularity.unique(), values = df.song_popularity.value_counts(), title = 'Target class distribution', width = 1000)","ac9c88c5":"features_ = data.columns\n\nX_train = df[features_]\n\n\nX_test = df_test[features_]\nX_train.shape,X_test.shape","01b66640":"y_train = df['song_popularity']\ny_train.shape\ny_train","7ffd1b23":"X_train","7b3fa289":"X_train.instrumentalness = np.log1p(X_train.instrumentalness)\nX_train.instrumentalness.replace([np.inf, -np.inf], np.nan, inplace=True)\nX_train.instrumentalness.isna().sum()","106607c7":"X_test.instrumentalness = np.log1p(X_test.instrumentalness)\nX_test.instrumentalness.replace([np.inf, -np.inf], np.nan, inplace=True)\nX_test.instrumentalness.isna().sum()","59c38015":"X_test.instrumentalness.isna().sum()","d80a134b":"from sklearn.experimental import enable_iterative_imputer\nfrom sklearn.impute import IterativeImputer  \n\n\nimputer = IterativeImputer(random_state = 10,max_iter= 10, initial_strategy= 'median') \n\nX_train = pd.DataFrame(imputer.fit_transform(X_train),columns = X_train.columns)\nX_test = pd.DataFrame(imputer.fit_transform(X_test),columns = X_train.columns) \n\n\n\n","468315b9":"X_train.isnull().sum()","01bf46a1":"from xgboost import XGBClassifier\nfrom sklearn.model_selection import StratifiedKFold \nimport warnings\nwarnings.filterwarnings('ignore')\nimport time\nfrom sklearn.metrics import roc_auc_score","2d331852":"from sklearn.model_selection import StratifiedKFold \nimport warnings\nwarnings.filterwarnings('ignore')\nimport time\nfrom sklearn.metrics import roc_auc_score\n\n\n\nxgb_score = []\nfinal_pred = []\nskf= StratifiedKFold(n_splits=10, shuffle=True, random_state=10)\nfor fold,(train_indicies, valid_indicies) in enumerate(skf.split(X_train,y_train)):\n    print(10*\"=\", f\"Fold={fold+1}\", 10*\"=\")\n    start_time = time.time()\n    x_train,x_valid = X_train.iloc[train_indicies],X_train.iloc[valid_indicies]\n    y_train1,y_valid = y_train.iloc[train_indicies],y_train.iloc[valid_indicies] \n\n    model = XGBClassifier(n_estimators=50,max_depth = 4,booster='gbtree')\n    model.fit(x_train,y_train1,verbose = 0,\n            eval_set = [(x_valid, y_valid)],\n            eval_metric = 'auc',\n            early_stopping_rounds = 300) \n  \n    preds_valid = model.predict_proba(x_valid)[:, 1]\n    auc = roc_auc_score(y_valid,  preds_valid)\n    xgb_score.append(auc)\n    run_time = time.time() - start_time\n\n\n    print(f\"Fold={fold+1}, AUC score: {auc:.2f}, Run Time: {run_time:.2f}s\")\n    test_preds = model.predict_proba(X_test[FEATURES])[:, 1]\n    final_pred.append(test_preds)\n  \n","3ffd2d6f":"np.array(xgb_score).mean()","282ddc45":"final_pred\n\nsubmit = pd.DataFrame(df_test['id'])\nans = np.mean(np.stack(final_pred), axis = 0)\nsubmit['song_popularity'] = ans\nsubmit.head()","65378f64":"submit.to_csv('submit_this_file.csv',index=False)","cff79f42":"from sklearn.model_selection import RandomizedSearchCV","865b20d1":"model = CatBoostClassifier()\nparameters = {'depth':[i for i in range(2,8,2)], 'learning_rate':[0.06, 0.07, 0.075, 0.08]}\nn_folds = 5\ngrid_model = RandomizedSearchCV(estimator=model, param_distributions=parameters, cv=n_folds, scoring='roc_auc',\n                                return_train_score=True)\ngrid_model.fit(X_train, y_train)","8e12e3c9":"grid_model.best_params_","6bf20e01":"grid_model.best_score_","f8c7b893":"cat_params = {\n            'depth' : 2,\n            'grow_policy' : 'SymmetricTree',\n            'learning_rate' : 0.08,\n            'iterations' : 1000,\n            'loss_function' : 'CrossEntropy',\n            'eval_metric' : 'AUC',\n            'use_best_model' : True,\n            'early_stopping_rounds' : 100,\n            'task_type' : 'CPU',\n            'thread_count' : 4,\n            'verbose' : False\n        }\n        \n\n\ncat_score = []\nfinal_pred = []\nskf= StratifiedKFold(n_splits=10, shuffle=True, random_state=10)\nfor fold,(train_indicies, valid_indicies) in enumerate(skf.split(X_train,y_train)):\n    print(10*\"=\", f\"Fold={fold+1}\", 10*\"=\")\n    start_time = time.time()\n    x_train,x_valid = X_train.iloc[train_indicies],X_train.iloc[valid_indicies]\n    y_train1,y_valid = y_train.iloc[train_indicies],y_train.iloc[valid_indicies] \n\n    model = CatBoostClassifier(**cat_params) \n    model.fit(x_train,y_train1,verbose = 0,\n            eval_set = [(x_valid, y_valid)]) \n  \n    preds_valid = model.predict_proba(x_valid)[:, 1]\n    auc = roc_auc_score(y_valid,  preds_valid)\n    cat_score.append(auc)\n    run_time = time.time() - start_time\n\n\n    print(f\"Fold={fold+1}, AUC score: {auc:.2f}, Run Time: {run_time:.2f}s\")\n    test_preds = model.predict_proba(X_test[FEATURES])[:, 1]\n    final_pred.append(test_preds)\n  \n","f12b2fab":"np.array(cat_score).mean()\n\n# 5780690942749","96fb77db":"cat_score","6b27c259":"final_pred\n\nsubmit = pd.DataFrame(df_test['id'])\nans = np.mean(np.stack(final_pred), axis = 0)\nsubmit['song_popularity'] = ans\nsubmit.head()","aab42450":"submit.to_csv('submit_this_file.csv',index=False)","a07001de":"check = pd.DataFrame(df['instrumentalness'].apply(np.log))\ncheck['target'] = df.song_popularity\n\nsns.histplot(data = check,x='instrumentalness',hue = 'target')","dca67b64":"df.energy.shape","39b44ab8":"len(np.log(df.instrumentalness.values))","c2ebafef":"sns.scatterplot(x=np.log(df.instrumentalness.values), y = df.loudness , hue = df.song_popularity)","11e54067":"sns.scatterplot(x=df.instrumentalness.values, y = df.loudness , hue = df.song_popularity)","7aad427f":"key points : \n* overall missing values are around 5% across the dataset\n* not all features have missing values ","fd713bfc":"Initial discoveries \n* Various scales can be seen some falling under 0-1, some going very small and very high \n* Many null values(NA) can bee seen lot in the sample data itself in various columns  ","87bc4a2d":"Import necessary libraries ","a537d7df":"* Class imbalance can be noted, not heavy but maybe considered later"}}