{"cell_type":{"8d511898":"code","a150e391":"code","f5103c19":"code","8746121f":"code","77fad154":"code","0c0dac6c":"code","4d82708e":"code","4c6e7192":"code","6a428715":"code","be381ea3":"code","4688bba3":"code","5bc7dcc9":"code","9fa26cc4":"code","7ffe34d5":"code","01c6dbf0":"code","7a1a0102":"code","cc5d88b9":"code","dbb7363a":"code","6f47ecfb":"code","3fc1f5d2":"code","84500ff2":"code","da11ef23":"code","76b00175":"code","89249f60":"code","ef71137f":"code","05d224a0":"code","3dd6b1a2":"code","52fe5e3d":"code","02958b4f":"markdown","a8bc25a1":"markdown","d7b92990":"markdown","d221d4f1":"markdown","33f7116f":"markdown","0d5b61c7":"markdown","4e7035b9":"markdown","b52ddd8a":"markdown","00abc553":"markdown","0a7ec951":"markdown","a13ea454":"markdown","09b3150d":"markdown","15d8099e":"markdown","291a249c":"markdown","3e478f73":"markdown","e39f754d":"markdown"},"source":{"8d511898":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings('ignore')\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, roc_auc_score, recall_score, precision_score, confusion_matrix, f1_score, precision_recall_curve, roc_curve, plot_roc_curve, plot_precision_recall_curve\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\n\nfrom lightgbm import LGBMClassifier, plot_importance","a150e391":"# Load data\ndata = pd.read_csv('..\/input\/employees-evaluation-for-promotion\/employee_promotion.csv')\ndata","f5103c19":"# Check whether any columns contain NaN or Null values\ndata.isnull().sum()","8746121f":"# We have lots of data, specifically over 10000\n# Therefore, I decided to drop rows contain NaN or Null\ndata.dropna(axis=0, inplace=True)\ndata.reset_index(drop=True, inplace=True)\ndata","77fad154":"# 'emplye_id' column is unnecessary so, I gonna drop for it\ndata.drop(columns=['employee_id'], inplace=True)\ndata","0c0dac6c":"# Convert 'gender' column into binary values\ndata['gender_new'] = pd.Series()\ndata.loc[data['gender'] == 'f', 'gender_new'] = 0\ndata.loc[data['gender'] == 'm', 'gender_new'] = 1\ndata = data.astype({'gender_new' : 'int'})\ndata.drop(columns=['gender'], inplace=True)\ndata","4d82708e":"# Check distributions of features for training by pie charts\nfig, axs = plt.subplots(nrows=3, ncols=4, figsize=(40, 20))\n\nfor i, feature in enumerate(data.columns):\n    row = int(i\/4)\n    col = i%4\n    pd.value_counts(data.iloc[:, i]).plot.pie(autopct=\"%.1f%%\", ax=axs[row][col])\n\nplt.suptitle('Distribution of features')\nplt.tight_layout()","4c6e7192":"# Check distributions of features contain numbers by distplot\ncolumns = ['gender_new', 'no_of_trainings', 'age', 'previous_year_rating', 'length_of_service', 'awards_won', 'avg_training_score', 'is_promoted']\nfig, axs = plt.subplots(nrows=2, ncols=4, figsize=(20, 10))\n\nfor i, feature in enumerate(data[columns]):\n    row = int(i\/4)\n    col = i%4\n    sns.distplot(data[columns].iloc[:, i], ax=axs[row][col])\n\nplt.suptitle('Distirbution of features')\nplt.tight_layout","6a428715":"data","be381ea3":"# Log Transformation\nage_log = np.log1p(data['age'])\nservice_log = np.log1p(data['length_of_service'])\nscore_log = np.log1p(data['avg_training_score'])\n\ndata.insert(6, 'age_log', age_log)\ndata.insert(9, 'length_of_service_log', service_log)\ndata.insert(12, 'avg_training_score_log', score_log)\n\ndata","4688bba3":"# Check distributions of log converted columns\nlog_columns = ['age_log', 'length_of_service_log', 'avg_training_score_log']\n\nfig, (ax1, ax2, ax3) = plt.subplots(nrows=1, ncols=3, figsize=(20, 10))\n\nsns.distplot(data['age_log'], ax=ax1)\nax1.set_title('Distribution of age_log')\nsns.distplot(data['length_of_service_log'], ax=ax2)\nax2.set_title('Distribution of length_of_service_log')\nsns.distplot(data['avg_training_score_log'], ax=ax3)\nax3.set_title('Distribution of avg_training_score_log')\n\nplt.suptitle('Distribution of log converted features', fontweight='bold')\nplt.tight_layout()\nplt.show()","5bc7dcc9":"# Correlation Heatmap\nplt.figure(figsize=(10, 8))\nplt.title('Correlation of features')\nsns.heatmap(data.corr(), annot=True, linewidths=.5, cmap=\"YlGnBu\")","9fa26cc4":"# Get One-Hot encoded DataFrame\ndata_oh = pd.get_dummies(data)\ndata_oh","7ffe34d5":"# Scaling of features\nfeatures = np.array(data_oh.columns).reshape(-1, 1)\n\nfor feature in features:\n    scaler = StandardScaler()\n    scaler.fit(data_oh[feature])\n    data_oh[feature] = scaler.transform(data_oh[feature])\n\ndata_oh","01c6dbf0":"# Define features and label for training\ntrain_features = data_oh.drop(columns=['is_promoted'], inplace=False)\ntrain_label = data_oh['is_promoted'].astype(int)","7a1a0102":"# Split datasets\nX_train, X_test, y_train, y_test = train_test_split(train_features, train_label, test_size=0.2, random_state=11)\n\nprint('Shape of X_train: ', X_train.shape)\nprint('Shape of X_test: ', X_test.shape)\nprint('Shape of y_train: ', y_train.shape)\nprint('Shape of y_test: ', y_test.shape)","cc5d88b9":"# Utility Function\ndef get_clf_eval(y_test, pred=None, pred_proba=None):\n    confusion = confusion_matrix(y_test, pred)\n    accuracy = accuracy_score(y_test, pred)\n    precision = precision_score(y_test, pred, average=\"macro\")\n    recall = recall_score(y_test, pred, average=\"macro\")\n    f1 = f1_score(y_test, pred, average=\"macro\")\n    roc_auc = roc_auc_score(y_test, pred_proba, average=\"macro\")\n    print('Confusion Matrix')\n    print(confusion)\n    print('Accuracy: {0:.4f}, Precision: {1:.4f}, Recall {2:.4f}, F1: {3:.4f}, AUC:{4:.4f}'.format(accuracy, precision, recall, f1, roc_auc))","dbb7363a":"# Process fitting, prediction and evalution by Logistic Regression\n# Create Estimator CLass\ndt_clf = DecisionTreeClassifier()\nlr_clf = LogisticRegression()\nrf_clf = RandomForestClassifier()\n\n# Fitting\ndt_clf.fit(X_train, y_train)\nlr_clf.fit(X_train, y_train)\nrf_clf.fit(X_train, y_train)\n\n# Prediction\ndt_pred = dt_clf.predict(X_test)\nlr_pred = lr_clf.predict(X_test)\nrf_pred = rf_clf.predict(X_test)\n\n# Pred_Proba\ndt_pred_proba = dt_clf.predict_proba(X_test)[:, 1]\nlr_pred_proba = lr_clf.predict_proba(X_test)[:, 1]\nrf_pred_proba = rf_clf.predict_proba(X_test)[:, 1]\n\n# Evaluation\nget_clf_eval(y_test, dt_pred, dt_pred_proba)\nget_clf_eval(y_test, lr_pred, lr_pred_proba)\nget_clf_eval(y_test, rf_pred, rf_pred_proba)","6f47ecfb":"# Plot Precision-Recall curve\nplot_precision_recall_curve(dt_clf, X_test, y_test)\nplot_precision_recall_curve(lr_clf, X_test, y_test)\nplot_precision_recall_curve(rf_clf, X_test, y_test)\nplt.show()","3fc1f5d2":"# Plot ROC curve\nplot_roc_curve(dt_clf, X_test, y_test)\nplot_roc_curve(lr_clf, X_test, y_test)\nplot_roc_curve(rf_clf, X_test, y_test)\nplt.show()","84500ff2":"# Create estimator and process fitting, prediction and evaluation for model after applying SMOTE\nlgbm_clf = LGBMClassifier(n_estimators=1000, num_leaves=64, n_jobs=-1, boost_from_average=False)\n\nlgbm_clf.fit(X_train, y_train)\nlgbm_preds_over = lgbm_clf.predict(X_test)\nlgbm_pred_proba = lgbm_clf.predict_proba(X_test)[:, 1]\n\nget_clf_eval(y_test, lgbm_preds_over, lgbm_pred_proba)\nplot_roc_curve(lgbm_clf, X_test, y_test)","da11ef23":"# Create estimator and process fitting, prediction and evaluation for model\nlgbm_wrapper = LGBMClassifier(n_estimators=400, num_leaves=64, n_jobs=-1, boost_from_average=False)\n\nevals = [(X_test, y_test)]\nlgbm_wrapper.fit(X_train, y_train, early_stopping_rounds=1200, eval_metric='logloss', eval_set=evals, verbose=True)\npreds = lgbm_wrapper.predict(X_test)\npred_proba = lgbm_wrapper.predict_proba(X_test)[:, 1]\nget_clf_eval(y_test, preds, pred_proba)","76b00175":"# Plot Feature importance\nfig, ax = plt.subplots(figsize=(10, 12))\nplot_importance(lgbm_wrapper, ax=ax)","89249f60":"# Create individual ML model\nknn_clf = KNeighborsClassifier(n_neighbors=4)\nrf_clf = RandomForestClassifier(n_estimators=100, random_state=11)\ndt_clf = DecisionTreeClassifier()\nada_clf = AdaBoostClassifier(n_estimators=100)\n\n# Create the model which will be fitted by dataset Stacking processed\nlr_final = LogisticRegression(C=10)","ef71137f":"# Fitting each models\nknn_clf.fit(X_train, y_train)\nrf_clf.fit(X_train, y_train)\ndt_clf.fit(X_train, y_train)\nada_clf.fit(X_train, y_train)","05d224a0":"# Predict each models and predict them\n\nknn_pred = knn_clf.predict(X_test)\nrf_pred = rf_clf.predict(X_test)\ndt_pred = dt_clf.predict(X_test)\nada_pred = ada_clf.predict(X_test)\n\nprint('Accuracy Score of KNN: {0:.4f}'.format(accuracy_score(y_test, knn_pred)))\nprint('Accuracy Score of RandomForestClassifier: {0:.4f}'.format(accuracy_score(y_test, rf_pred)))\nprint('Accuracy Score of DeicisionTreeClassifier: {0:.4f}'.format(accuracy_score(y_test, dt_pred)))\nprint('Accuracy Score of AdaBoostClassifier: {0:.4f}'.format(accuracy_score(y_test, ada_pred)))","3dd6b1a2":"# Combine preds to one ndarray\npred = np.array([knn_pred, rf_pred, dt_pred, ada_pred])\nprint(pred.shape)\n\n# Transponse 'pred' in order to convert as Feature\npred = np.transpose(pred)\nprint(pred.shape)","52fe5e3d":"# Fit, Predict, Evaluate for final model\nlr_final.fit(pred, y_test)\nfinal = lr_final.predict(pred)\n\nprint('Accuracy Score of Final Model: {0:.4f}'.format(accuracy_score(y_test, final)))","02958b4f":"# Import libraries and data","a8bc25a1":"# Data Visualization","d7b92990":"## Comment\n'age_log' became nearly normal distributed but other features didn't  \nLog transformation is one of the most powerful strategies of preparing data for training","d221d4f1":"## Comment\nFinally, we got preprocessed DataFrame named 'data'.  \nIt has no NaN or Null values, therefore it is good to go for machine learning.","33f7116f":"## Comment\nAs I anticipated, 'age', 'length of service' and 'avg_training_score' was highly correlated  \nSo, maybe I can try decomposition for those features (To be Continued)","0d5b61c7":"### Comment\nLightGBM is also another powerful model which runs by Boosting(Boot Strapping)  \nIt is more lighter than GradientBoostingClassifier(literally)  \nSpecifically, comparing to basic estimators, AUC score was the highest","4e7035b9":"## Stacking Ensemble","b52ddd8a":"## Basic Estimators","00abc553":"### Comment\nAs you can see above, LogisticRegression and RandomForestClassifier are both basic models but really powerful","0a7ec951":"## LightGBM","a13ea454":"## Dataset Overview\n|        Column        |                                         Descriptions                                        |\n|:--------------------:|:-------------------------------------------------------------------------------------------:|\n| employee_id          | Unique ID for the employee                                                                  |\n| department           | Department of employee                                                                      |\n| region               | Region of employment(unordered)                                                             |\n| education            | Education level                                                                             |\n| gender               | Gender of Employee                                                                          |\n| recruitment_channel  | Channel of recruitment for employee                                                         |\n| no_of_trainings      | no of other trainings completed in the previous year on soft skills, technical skills, etc. |\n| age                  | Age of Employee                                                                             |\n| previous_year_rating | Employee Rating for the previous year                                                       |\n| length_of_service    | Length of service in years                                                                  |\n| awards_won           | if awards won during the previous year then 1 else 0                                        |\n| avg_training_score   | Average score in current training evaluations                                               |\n| is_promoted          | Recommended for promotion                                                                   |","09b3150d":"# Conclusion\n**Accuracy Score of Final Model: 0.9430**\n\n1. Through trraining basic estimators, LightGBM and ensemble models, I could try lots of estimators for this datset.  \n2. Especially, the label for training was binary (which is 0 or 1) so, the evaluation score seems much higher than those from multi-classification.  \n3. Also, ensemble and nearest-neighbor model became ditinguished, therefore, visualzing scatter plot could be another good way for analyzing data.\n\nThanks for reviewing my notebooks  \nIf you'd like it, please give me upvotes and leave comments  \nAny questions or comments are always welcome","15d8099e":"# Classification","291a249c":"# Data Preprocessing","3e478f73":"## Comment\nAs you can see above, none of columns have normal distribution which is proper for training  \nTherefore, we can try converting following columns for normal distribution: 'age', 'length_of_service' and 'avg_training_score' ","e39f754d":"# Split Datasets"}}