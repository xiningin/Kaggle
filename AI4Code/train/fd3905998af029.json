{"cell_type":{"a469a099":"code","9c929d39":"code","8c61f02d":"code","8d6fa11b":"code","fc6c0aa2":"code","fbc7720a":"code","2a774a6a":"code","a1387668":"code","27a2e974":"code","73f1af99":"code","3e51c5bc":"code","e82dc16e":"code","3aec2a1b":"code","8c976a4b":"code","48e9fc74":"code","a7e1076f":"code","e0ac8f72":"code","f5461461":"code","4c9e12c9":"code","9b9025c5":"code","84d9db40":"code","12dd47e9":"code","5016ca6d":"markdown","03045dd3":"markdown","a7131d64":"markdown","3f7196e3":"markdown","3575cb63":"markdown","61422c1d":"markdown","9c84091e":"markdown","5274e00c":"markdown","b5756d6a":"markdown","b9865cc4":"markdown","788bf689":"markdown","c2de0514":"markdown"},"source":{"a469a099":"import os\nimport numpy as np\nimport cv2\nfrom glob import glob\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\n\nfrom tensorflow.keras.layers import *\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.applications import MobileNetV2\nfrom tensorflow.keras.applications.resnet50 import ResNet50\nfrom tensorflow.keras.applications.inception_resnet_v2 import InceptionResNetV2\n# from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\nfrom tensorflow.keras.metrics import Recall, Precision","9c929d39":"print(\"TensorFlow Version: \", tf.__version__)","8c61f02d":"np.random.seed(42)\ntf.random.set_seed(42)","8d6fa11b":"IMAGE_SIZE = 256\nEPOCHS = 800\nBATCH = 64\nLR = 1e-4\n\nPATH = \"..\/input\/semantic-segmentation-of-people\/\"","fc6c0aa2":"def load_data(path, split=0.1):\n    images = sorted(glob(os.path.join(path, \"Ground_Truth\/*\")))\n    masks = sorted(glob(os.path.join(path, \"Training_Images\/*\")))\n\n    total_size = len(images)\n    valid_size = int(split * total_size)\n    test_size = int(split * total_size)\n\n    train_x, valid_x = train_test_split(images, test_size=valid_size, random_state=42)\n    train_y, valid_y = train_test_split(masks, test_size=valid_size, random_state=42)\n\n    train_x, test_x = train_test_split(train_x, test_size=test_size, random_state=42)\n    train_y, test_y = train_test_split(train_y, test_size=test_size, random_state=42)\n\n    return (train_x, train_y), (valid_x, valid_y), (test_x, test_y)","fbc7720a":"def read_image(path):\n    path = path.decode()\n    x = cv2.imread(path, cv2.IMREAD_COLOR)\n    x = cv2.resize(x, (IMAGE_SIZE, IMAGE_SIZE))\n    x = x\/255.0\n    return x\n\ndef read_mask(path):\n    path = path.decode()\n    x = cv2.imread(path, cv2.IMREAD_GRAYSCALE)\n    x = cv2.resize(x, (IMAGE_SIZE, IMAGE_SIZE))\n    x = x\/255.0\n    x = np.expand_dims(x, axis=-1)\n    return x","2a774a6a":"def tf_parse(x, y):\n    def _parse(x, y):\n        x = read_image(x)\n        y = read_mask(y)\n        return x, y\n\n    x, y = tf.numpy_function(_parse, [x, y], [tf.float64, tf.float64])\n    x.set_shape([IMAGE_SIZE, IMAGE_SIZE, 3])\n    y.set_shape([IMAGE_SIZE, IMAGE_SIZE, 1])\n    return x, y\n\ndef tf_dataset(x, y, batch=8):\n    dataset = tf.data.Dataset.from_tensor_slices((x, y))\n    dataset = dataset.map(tf_parse)\n    dataset = dataset.batch(batch)\n    dataset = dataset.repeat()\n    return dataset","a1387668":"(train_x, train_y), (valid_x, valid_y), (test_x, test_y) = load_data(PATH)\n\nprint(\"Training data: \", len(train_x))\nprint(\"Validation data: \", len(valid_x))\nprint(\"Testing data: \", len(test_x))","27a2e974":"def read_and_rgb(x):\n    x = cv2.imread(x)\n    x = cv2.cvtColor(x, cv2.COLOR_BGR2RGB)\n    return x","73f1af99":"fig = plt.figure(figsize=(15, 15))\na = fig.add_subplot(1, 4, 1)\nimgplot = plt.imshow(read_and_rgb(train_x[0]))\n\na = fig.add_subplot(1, 4, 2)\nimgplot = plt.imshow(read_and_rgb(train_x[1]))\nimgplot.set_clim(0.0, 0.7)\n\na = fig.add_subplot(1, 4, 3)\nimgplot = plt.imshow(read_and_rgb(train_x[2]))\nimgplot.set_clim(0.0, 1.4)\n\na = fig.add_subplot(1, 4, 4)\nimgplot = plt.imshow(read_and_rgb(train_x[3]))\nimgplot.set_clim(0.0, 2.1)\n\nfig = plt.figure(figsize=(15, 15))\na = fig.add_subplot(1, 4, 1)\nimgplot = plt.imshow(read_and_rgb(train_y[0]))\n\na = fig.add_subplot(1, 4, 2)\nimgplot = plt.imshow(read_and_rgb(train_y[1]))\nimgplot.set_clim(0.0, 0.7)\n\na = fig.add_subplot(1, 4, 3)\nimgplot = plt.imshow(read_and_rgb(train_y[2]))\nimgplot.set_clim(0.0, 1.4)\n\na = fig.add_subplot(1, 4, 4)\nimgplot = plt.imshow(read_and_rgb(train_y[3]))\nimgplot.set_clim(0.0, 1.4)","3e51c5bc":"def model():\n    inputs = Input(shape = (IMAGE_SIZE, IMAGE_SIZE, 3), name = \"input_image\")\n    \n    encoder = MobileNetV2(\n        input_tensor = inputs, \n        weights = \"imagenet\", \n        include_top = False, \n        alpha = 0.35\n    )\n    \n    skip_connection_names = [\n        \"input_image\", \n        \"block_1_expand_relu\", \n        \"block_3_expand_relu\", \n        \"block_6_expand_relu\"\n    ]\n    encoder_output = encoder.get_layer(\"block_13_expand_relu\").output\n    \n    f = [16, 32, 48, 64]\n    x = encoder_output\n    for i in range(1, len(skip_connection_names)+1, 1):\n        x_skip = encoder.get_layer(skip_connection_names[-i]).output\n        x = UpSampling2D((2, 2))(x)\n        x = Concatenate()([x, x_skip])\n        \n        x = Conv2D(f[-i], (3, 3), padding = \"same\")(x)\n        x = BatchNormalization()(x)\n        x = Activation(\"relu\")(x)\n        \n        x = Conv2D(f[-i], (3, 3), padding = \"same\")(x)\n        x = BatchNormalization()(x)\n        x = Activation(\"relu\")(x)\n        \n    x = Conv2D(1, (1, 1), padding = \"same\")(x)\n    x = Activation(\"sigmoid\")(x)\n    \n    model = Model(inputs, x)\n    return model","e82dc16e":"model = model()\nmodel.summary()","3aec2a1b":"smooth = 1e-15\ndef dice_coef(y_true, y_pred):\n    y_true = tf.keras.layers.Flatten()(y_true)\n    y_pred = tf.keras.layers.Flatten()(y_pred)\n    intersection = tf.reduce_sum(y_true * y_pred)\n    return (2. * intersection + smooth) \/ (tf.reduce_sum(y_true) + tf.reduce_sum(y_pred) + smooth)\n\ndef dice_loss(y_true, y_pred):\n    return 1.0 - dice_coef(y_true, y_pred)","8c976a4b":"train_dataset = tf_dataset(train_x, train_y, batch=BATCH)\nvalid_dataset = tf_dataset(valid_x, valid_y, batch=BATCH)","48e9fc74":"opt = tf.keras.optimizers.Nadam(LR)\nmetrics = [dice_coef, Recall(), Precision()]\nmodel.compile(\n    loss = dice_loss, \n    optimizer = opt, \n    metrics = metrics\n)","a7e1076f":"# callbacks = [\n#     ReduceLROnPlateau(\n#         monitor = 'val_loss', \n#         factor = 0.1, \n#         patience = 4\n#     ),\n#     EarlyStopping(\n#         monitor = 'val_loss', \n#         patience = 10, \n#         restore_best_weights = False\n#     )\n# ]","e0ac8f72":"train_steps = len(train_x)\/\/BATCH\nvalid_steps = len(valid_x)\/\/BATCH\n\nif len(train_x) % BATCH != 0:\n    train_steps += 1\nif len(valid_x) % BATCH != 0:\n    valid_steps += 1\n\nhistory = model.fit(\n    train_dataset,\n    validation_data = valid_dataset,\n    epochs = EPOCHS,\n    steps_per_epoch = train_steps,\n    validation_steps = valid_steps,\n#     callbacks = callbacks\n)","f5461461":"train_loss = history.history[\"loss\"]\nvalid_loss = history.history[\"val_loss\"]\n\nepochs = range(len(train_loss)) \n\nplt.plot(epochs, train_loss)\nplt.plot(epochs, valid_loss)\nplt.legend([\"Training Loss\", \"Validation Loss\"])\nplt.title(\"Training and Validation Loss\")","4c9e12c9":"test_dataset = tf_dataset(test_x, test_y, batch = BATCH)\n\ntest_steps = (len(test_x)\/\/BATCH)\nif len(test_x) % BATCH != 0:\n    test_steps += 1\n\nmodel.evaluate(test_dataset, steps = test_steps)","9b9025c5":"def read_image(path):\n    x = cv2.imread(path, cv2.IMREAD_COLOR)\n    x = cv2.cvtColor(x, cv2.COLOR_BGR2RGB)\n    x = cv2.resize(x, (IMAGE_SIZE, IMAGE_SIZE))\n    x = x\/255.0\n    return x\n\ndef read_mask(path):\n    x = cv2.imread(path, cv2.IMREAD_GRAYSCALE)\n    x = cv2.resize(x, (IMAGE_SIZE, IMAGE_SIZE))\n    x = np.expand_dims(x, axis=-1)\n    x = x\/255.0\n    return x","84d9db40":"def mask_parse(mask):\n    mask = np.squeeze(mask)\n    mask = [mask, mask, mask]\n    mask = np.transpose(mask, (1, 2, 0))\n    return mask","12dd47e9":"for i, (x, y) in enumerate(zip(test_x[:10], test_y[:10])):\n    x = read_image(x)\n    y = read_mask(y)\n    y_pred = model.predict(np.expand_dims(x, axis=0))[0] > 0.5\n    h, w, _ = x.shape\n    white_line = np.ones((h, 10, 3))\n\n    all_images = [\n        x, white_line,\n        mask_parse(y), white_line,\n        mask_parse(y_pred)\n    ]\n    image = np.concatenate(all_images, axis=1)\n    \n    fig = plt.figure(figsize=(12, 12))\n    a = fig.add_subplot(1, 1, 1)\n    imgplot = plt.imshow(image)","5016ca6d":"## Dataset","03045dd3":"## Metrics","a7131d64":"## Training","3f7196e3":"## Evaluation","3575cb63":"Building the tf.data pipeline for training, validation and testing dataset.","61422c1d":"Seeding","9c84091e":"## Results","5274e00c":"Reading the images and masks","b5756d6a":"## Model","b9865cc4":"## Plot error","788bf689":"## Hyperparameters","c2de0514":"## Importing Libraries and Functions"}}