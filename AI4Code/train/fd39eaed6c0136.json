{"cell_type":{"ceb8bd4a":"code","468f762b":"code","ac74044a":"code","eda4f92b":"code","b9c7c8f1":"code","a5d87ba8":"code","dbf16fd7":"code","db9b5b73":"code","b754b5d9":"code","5011d736":"code","683bc55c":"code","121de0c3":"code","4bdb2373":"code","8dbdfeee":"code","f9417a34":"code","af21b8a7":"code","f5652ea0":"markdown","26641b1e":"markdown","bb3403c5":"markdown","149c97a1":"markdown","74ba8964":"markdown","2a87df5d":"markdown","ea7a3f6d":"markdown","ae408f03":"markdown","826e8eef":"markdown","6214c3e1":"markdown","8d5fff09":"markdown","43ea395f":"markdown","bb041a3a":"markdown","5f6c4722":"markdown","a1be2298":"markdown"},"source":{"ceb8bd4a":"#imports\nimport os, pickle, string, tqdm\nimport numpy as np\nimport pandas as pd\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics import accuracy_score, confusion_matrix, ConfusionMatrixDisplay\nfrom matplotlib import pyplot as plt","468f762b":"#read dataset\n\ndata_dir = '..\/input\/speech-sentiment-dataset\/NLP dataset'\ntrain_file = 'twitter_training.csv'\n\ntrain_data_df = pd.read_csv(os.path.join(data_dir, train_file))\ntrain_data_df.head()","ac74044a":"def arrange_df(df):\n    df.index = range(0, len(df))\n    df.columns = ['Tweet_ID', 'Entity', 'Sentiment', 'Tweet Content']\n    return df\n\ntrain_data_df = arrange_df(train_data_df)\ntrain_data_df.head()","eda4f92b":"train_data_df.isna().sum()","b9c7c8f1":"class SpeechPreprocessing:\n    \n    def __init__(self):\n        self.punctuations = string.punctuation\n        self.word2idx = {'<pad>':0, '<unk>':1}\n        self.idx2word = {0:'<pad>', 1:'<unk>'}\n        self.words = []\n        self.word_idx = []\n        self.max_len = 0\n        \n    def add_sentence(self, sentence):\n        sentence = self.process_sent(sentence)\n        sent_len = len(sentence)\n        if sent_len > self.max_len:\n            self.max_len = sent_len\n        for i in sentence:\n            self.add_word(i)\n            \n    def add_word(self, word):\n        word = self.remove_punc(word)\n        if word not in self.word2idx:\n            word_idx = 0 if len(self.idx2word) == 0 else max(self.idx2word)+1\n            self.word2idx[word] = word_idx\n            self.idx2word[word_idx] = word\n            self.words.append(word)\n            self.word_idx.append(word_idx)\n        else:\n            pass\n        \n    def process_sent(self, sentence):\n        sentence = sentence.lower()\n        sentence = sentence.replace('<unk>', ' <unk>')\n        sentence = sentence.split(' ')\n        return sentence\n    \n    def remove_punc(self, word):\n        if word == '<unk>':\n            return word\n        for i in word:\n            if i in self.punctuations:\n                word = word.replace(i, '')\n        return word","a5d87ba8":"text = SpeechPreprocessing()\n\nfor sentence in tqdm.tqdm(train_data_df['Tweet Content']):\n    sentence = str(sentence)\n    text.add_sentence(sentence)","dbf16fd7":"#length of the longest sentence\ntext.max_len","db9b5b73":"def sentence2vec(sentence, max_len=text.max_len):\n    vector = []\n    sentence = text.process_sent(sentence)\n    for word in sentence:\n        word = text.remove_punc(word)\n        if word not in text.word2idx:\n            vector.append(text.word2idx['<unk>'])\n            continue\n        vector.append(text.word2idx[word])\n    if len(vector) != max_len:\n        for i in range(max_len-len(vector)):\n            vector.append(text.word2idx['<pad>'])\n    return vector","b754b5d9":"def Xy_split(df):\n    features, labels = [], []\n    for sentence, sentiment in tqdm.tqdm(zip(df['Tweet Content'], df['Sentiment'])):\n        sentence = str(sentence)\n        features.append(sentence2vec(sentence))\n        labels.append(sentiment)\n        \n    return features, labels","5011d736":"train_features, train_labels = Xy_split(train_data_df)\n\ntrain_features = np.array(train_features)\n\nlabel_encoder = LabelEncoder()\nlabel_encoder.fit(train_labels)\ntrain_labels = label_encoder.transform(train_labels)\n\n(unique, counts) = np.unique(train_labels, return_counts=True)\nfrequencies = np.asarray((unique, counts)).T\nfrequencies","683bc55c":"#view shape of training feature and label\nprint(f'features shape: {train_features.shape}\\nlabel shape: {train_labels.shape}')\n\ntrain_features","121de0c3":"#training process\nclassifier = DecisionTreeClassifier()\nclassifier = classifier.fit(train_features, train_labels)\n\npred = classifier.predict(train_features)\n\naccuracy = accuracy_score(train_labels, pred)\ncm = confusion_matrix(train_labels, pred)\ndisplay_cm = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=classifier.classes_)\ndisplay_cm.plot()\nplt.title('Confusion matrix for training set')\nplt.show()\n\nprint(f'\\n\\ntraining accuracy of model: {round(accuracy*100, 4)}%')","4bdb2373":"#load and standardize testing data\ntest_file = 'twitter_validation.csv'\ntest_data_df = pd.read_csv(os.path.join(data_dir, test_file))\n\ntest_data_df = arrange_df(test_data_df)\ntest_data_df.head()","8dbdfeee":"test_features, test_labels = Xy_split(test_data_df)\n\ntest_features, test_labels = np.array(test_features), label_encoder.transform(test_labels)","f9417a34":"#testing_process\npred = classifier.predict(test_features)\n\naccuracy = accuracy_score(test_labels, pred)\ncm = confusion_matrix(test_labels, pred)\ndisplay_cm = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=classifier.classes_)\ndisplay_cm.plot()\nplt.title('Confusion matrix for testing set')\nplt.show()\n\nprint(f'\\n\\ntesting accuracy of model: {round(accuracy*100, 4)}%')","af21b8a7":"working_dir = '.\/Models'\n\nif not os.path.isdir(working_dir):\n    os.mkdir(working_dir)\n\npickle.dump(label_encoder, open(os.path.join(working_dir, 'label_encoder.sav'), 'wb'))\npickle.dump(classifier, open(os.path.join(working_dir, 'DT_model.sav'), 'wb'))\n\nprint('models saved successfully.')","f5652ea0":"<h3>Load and Arrange Training Dataset<\/h3>","26641b1e":"<h3>Testing Process<\/h3>\n\nIn this process, inference is run on the model with a set of data it has not encountered before.<br>\nSimilarly, the accuracy and the confusion matrix are visualized","bb3403c5":"<h3>Training Process<\/h3>","149c97a1":"The train_features is converted to a 2D numpy array for processing.<br>\nThe train_labels is encoded with the sklearn 'LabelEncoder' so that each class of sentiment is given a unique numerical identifier.<br>\n\nA 'fit()' method is called to fit on the labels and the 'transform()' method is called to transform the classes into unique numbers corresponding to each class.<br>\n\nVisit <a href='https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.preprocessing.LabelEncoder.html'>here<\/a> for more info on the LabelEncoder.","74ba8964":"A predefined Decision Tree Classifier algorithm from the sklearn library is used to define the classifier model.<br>\nOn the high level, the model is fed the training features and labels via the 'fit()' method, to learn a pattern so it can make predictions via the 'predict()' method.<br>\n\nVisit <a href='https:\/\/scikit-learn.org\/stable\/modules\/tree.html'>here<\/a> for more info on the working principle of the decision tree algorithm.<br>\nAfter training, the accuracy is outputed alongside a graphical visualization of the confusion matrix.\n\nThe confusion matrix shows the correctly predicted classes and the wrongly predicted ones.","2a87df5d":"This process add words of sentences in the training dataset to the word2idx and idx2word dictionary attributes of the SpeechPreprocessing() instance","ea7a3f6d":"The 'sentence2vec()' function takes in a sentence and vectorizes it.<br>\nThe words of sentence are converted into their corresponding indexes for modeling.<br>\nThe sentence is them padded so that it is equal to the length of the longest sentence.","ae408f03":"<h3>Loading and Arranging Testing Data<\/h3>","826e8eef":"Checking for missing values","6214c3e1":"The 'SpeechPreprocessing' class assign unique words to unique indexes and vice versa with word2idx and idx2word as corresponding attributes as well as other attributes corresponding to length of the longest sentence, list of all unique words and list of all unique indexes.","8d5fff09":"The 'Xy_split()' function splits the training data into relevant features (Tweet Content) and label (Sentiment) for modeling.","43ea395f":"<h3>Data Preprocessing<\/h3>","bb041a3a":"<h1>Speech Sentiment Classification with  Decision Tree Classifier<\/h1>\n\nIn this project, a machine learning algorithm known as the decision tree is used to model a speech sentiment dataset compiled from twitter social media so as to classify the sentiment of several statements.","5f6c4722":"the 'arrange_df()' function adds a new valid index and column names to the dataframe","a1be2298":"<h3>Saving The Model<\/h3>\n\nThe label encoder and the decision tree model are serialized into sav format and saved into a pickle file for future use.\n"}}