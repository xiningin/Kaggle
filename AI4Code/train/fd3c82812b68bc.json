{"cell_type":{"98240584":"code","10a2e552":"code","743b9e3b":"code","de6ac90a":"code","972f1b8a":"code","92b9253b":"code","73ae8f21":"code","d6eb33a9":"code","82398af5":"code","2136867e":"code","f1af9871":"code","f80c9273":"code","570b8e5f":"code","6b06a81e":"code","088b7f6b":"code","78ac7996":"code","1cb68ac8":"code","0dc1d703":"code","a96fc27a":"code","b108f65f":"code","e99436d8":"code","be8e26e8":"markdown","ce2bf328":"markdown","cde7207d":"markdown","05318077":"markdown","7db3913f":"markdown","35b7aac8":"markdown","ae18747c":"markdown","83bbb035":"markdown","3fe168c7":"markdown","a4a9efb5":"markdown","c9105ebd":"markdown","c1d7a909":"markdown","60807e44":"markdown","f71b8363":"markdown","786f78e6":"markdown","9fb84317":"markdown","0f5e6e92":"markdown","2d07319b":"markdown"},"source":{"98240584":"import pandas as pd\n\nfrom sklearn.model_selection import train_test_split\n\ntrain_csv_path = '\/kaggle\/input\/nlp-getting-started\/train.csv'\ntrain_df = pd.read_csv(train_csv_path)\n\nall_texts = train_df['text'].values.tolist()\nall_labels = train_df['target'].values.tolist()\n\nprint(\"Tweets are loaded. Total # of tweets: {}.\".format(len(all_texts)))\nprint(\"# of labels:\")\nprint(train_df['target'].value_counts())","10a2e552":"frequent_tweets = {}\nfor t, l in zip(all_texts, all_labels):\n    if all_texts.count(t) > 2:\n        frequent_tweets[t] = [l] if t not in frequent_tweets else frequent_tweets[t] + [l]\n        \nprint(\"The number of tweeets which appear multiple times: {}\"\n      .format(len(frequent_tweets.keys())))     \n\nprint(\"Tweets which have inconsistent labeling:\")\nprint()\n\nfor t, ls in frequent_tweets.items():\n    if not all(element == ls[0] for element in ls):\n        print(t)\n        print(ls)","743b9e3b":"should_be_real = [\".POTUS #StrategicPatience is a strategy for #Genocide; refugees; IDP Internally displaced people; horror; etc. https:\/\/t.co\/rqWuoy1fm4\",\n                 \"#foodscare #offers2go #NestleIndia slips into loss after #Magginoodle #ban unsafe and hazardous for #humanconsumption\",\n                 \"CLEARED:incident with injury:I-495  inner loop Exit 31 - MD 97\/Georgia Ave Silver Spring\"]\n\nshould_not_be_real = [\"He came to a land which was engulfed in tribal war and turned it into a land of peace i.e. Madinah. #ProphetMuhammad #islam\",\n                     \"Who is bringing the tornadoes and floods. Who is bringing the climate change. God is after America He is plaguing her\",\n                      \"The Prophet (peace be upon him) said 'Save yourself from Hellfire even if it is by giving half a date in charity.'\",\n                     \"Hellfire is surrounded by desires so be careful and don\u0089\u00db\u00aat let your desires control you! #Afterlife\",\n                     \"#Allah describes piling up #wealth thinking it would last #forever as the description of the people of #Hellfire in Surah Humaza. #Reflect\",\n                     \"that horrible sinking feeling when you\u0089\u00db\u00aave been at home on your phone for a while and you realise its been on 3G this whole time\",\n                     \"To fight bioterrorism sir.\"]\n\n\ndef fix_labels(tweets_to_fix, correct_label):\n    for i, (tweet, label) in enumerate(zip(all_texts, all_labels)):\n        if any(tweet.startswith(t) for t in tweets_to_fix):\n            all_labels[i] = correct_label\n\n        \nfix_labels(should_be_real, 1)\nfix_labels(should_not_be_real, 0)\n\nprint(\"Relabeled {} tweets in total\".format(len(should_be_real) + len(should_not_be_real)))","de6ac90a":"\ntrain_texts, val_texts, train_labels, val_labels = train_test_split(\n    all_texts, all_labels,\n    stratify = train_df['target']\n)\n\nprint('Train data is read and split into training and validation sets.')\nprint('Size of train data (# of entries): {}'.format(len(train_texts)))\nprint('Size of validation data (# of entries): {}'.format(len(val_texts)))","972f1b8a":"import re\nimport string\n\n\ndef remove_urls(tweet):\n    return re.sub(r\"http(s?):\/\/[\\S]+\", '', tweet)\n\ndef remove_at_links(tweet):\n    return re.sub(r\"\\B(@)\\S+\", '', tweet)\n\ndef remove_non_ascii_chars(tweet):\n    ascii_chars = set(string.printable)\n    for c in tweet:\n        if c not in ascii_chars:\n            tweet = tweet.replace(c,'')\n    return tweet\n\ndef fix_ax_nots(tweet):\n    tweet = tweet.replace(\" dont \", \" do not \")\n    tweet = tweet.replace(\" don't \", \" do not \")\n    tweet = tweet.replace(\" doesnt \", \" does not \")\n    tweet = tweet.replace(\" doesn't \", \" does not \")\n    tweet = tweet.replace(\" wont \", \" will not \")\n    tweet = tweet.replace(\" won't \", \" will not \")\n    tweet = tweet.replace(\" cant \", \" cannot \")\n    tweet = tweet.replace(\" can't \", \" cannot \")\n    tweet = tweet.replace(\" couldnt \", \" could not \")\n    tweet = tweet.replace(\" couldn't \", \" could not \")\n    tweet = tweet.replace(\" shouldnt \", \" should not \")\n    tweet = tweet.replace(\" shouldn't \", \" should not \")\n    tweet = tweet.replace(\" wouldnt \", \" would not \")\n    tweet = tweet.replace(\" wouldn't \", \" would not \")\n    tweet = tweet.replace(\" mustnt \", \" must not \")\n    tweet = tweet.replace(\" mustn't \", \" must not \")\n    \n    return tweet\n\ndef fix_personal_pronouns_and_verb(tweet):\n    tweet = tweet.replace(\" im \", \" i am \")\n    tweet = tweet.replace(\" youre \", \" you are\")\n    tweet = tweet.replace(\" hes \", \" he is\") # ? he's can be he has as well\n    tweet = tweet.replace(\" shes \", \" she is\")\n    # we are -> we're -> were  ---- were is a valid word\n    tweet = tweet.replace(\" theyre \", \" they are\")\n    \n    tweet = tweet.replace(\" ive \", \" i have \")\n    tweet = tweet.replace(\" youve \", \" you have \")\n    tweet = tweet.replace(\" weve \", \" we have \")\n    tweet = tweet.replace(\" theyve \", \" they have \")\n    \n    tweet = tweet.replace(\" youll \", \" you will \")\n    tweet = tweet.replace(\" theyll \", \" they will \")\n    \n    return tweet\n\ndef fix_special_chars(tweet):\n    tweet = tweet.replace(\"&amp;\", \" and \")\n    # tweet = tweet.replace(\"--&gt;\", \"\")\n    return tweet\n        \n\ndef clean_tweet(tweet):\n    tweet = remove_urls(tweet)\n    tweet = remove_at_links(tweet)\n    tweet = remove_non_ascii_chars(tweet)\n    tweet = fix_special_chars(tweet)\n    tweet = fix_ax_nots(tweet)\n    tweet = fix_personal_pronouns_and_verb(tweet)\n        \n    return tweet\n\n\ncleaned_train_texts = [clean_tweet(tweet) for tweet in train_texts]\nprint(\"Train tweets cleaned.\")\ncleaned_val_texts = [clean_tweet(tweet) for tweet in val_texts]\nprint(\"Validation tweets cleaned.\")","92b9253b":"from transformers import AutoTokenizer\n\nmodel_name = 'bert-base-cased'\n\ntokenizer = AutoTokenizer.from_pretrained(model_name)\n\ntrain_encodings = tokenizer(cleaned_train_texts, truncation=True, padding=True)\nval_encodings = tokenizer(cleaned_val_texts, truncation=True, padding=True)\n\nprint('Train & validation texts encoded')","73ae8f21":"import torch\n\nclass TweetDataset(torch.utils.data.Dataset):\n    \"\"\"\n    Class to store the tweet data as PyTorch Dataset\n    \"\"\"\n    \n    def __init__(self, encodings, labels):\n        self.encodings = encodings\n        self.labels = labels\n        \n    def __getitem__(self, idx):\n        # an encoding can have keys such as input_ids and attention_mask\n        # item is a dictionary which has the same keys as the encoding has\n        # and the values are the idxth value of the corresponding key (in PyTorch's tensor format)\n        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n        item['labels'] = torch.tensor(self.labels[idx])\n        return item\n    \n    def __len__(self):\n        return len(self.labels)\n        \n\nprint(TweetDataset.__doc__)","d6eb33a9":"# device (turn on GPU acceleration for faster execution)\ndevice = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\nprint(\"Device used: {}.\".format(device))","82398af5":"from torch import nn\nfrom transformers import BertModel\n\nin_features = 768 # it's 768 because that's the size of the output provided by the underlying BERT model\n\nclass BertWithCustomNNClassifier(nn.Module):\n    \"\"\"\n    A pre-trained BERT model with a custom classifier.\n    The classifier is a neural network implemented in this class.\n    \"\"\"\n    \n    def __init__(self, linear_size):\n        super(BertWithCustomNNClassifier, self).__init__()\n        self.bert = BertModel.from_pretrained(model_name)\n        self.dropout1 = nn.Dropout()\n        self.linear1 = nn.Linear(in_features=in_features, out_features=linear_size)\n        self.batch_norm1 = nn.BatchNorm1d(num_features=linear_size)\n        self.dropout2 = nn.Dropout(p=0.8)\n        self.linear2 = nn.Linear(in_features=linear_size, out_features=1)\n        self.batch_norm2 = nn.BatchNorm1d(num_features=1)\n        self.sigmoid = nn.Sigmoid()\n        \n    def forward(self, tokens, attention_mask):\n        bert_output = self.bert(input_ids=tokens, attention_mask=attention_mask)\n        x = self.dropout1(bert_output[1])\n        x = self.linear1(x)\n        x = self.dropout2(x)\n        x = self.batch_norm1(x)\n        x = self.linear2(x)\n        x = self.batch_norm2(x)\n        return self.sigmoid(x)\n        \n    def freeze_bert(self):\n        \"\"\"\n        Freezes the parameters of BERT so when BertWithCustomNNClassifier is trained\n        only the wieghts of the custom classifier are modified.\n        \"\"\"\n        for param in self.bert.named_parameters():\n            param[1].requires_grad=False\n    \n    def unfreeze_bert(self):\n        \"\"\"\n        Unfreezes the parameters of BERT so when BertWithCustomNNClassifier is trained\n        both the wieghts of the custom classifier and of the underlying BERT are modified.\n        \"\"\"\n        for param in self.bert.named_parameters():\n            param[1].requires_grad=True\n\n            \nprint(BertWithCustomNNClassifier.__doc__)","2136867e":"import numpy as np\nfrom sklearn.metrics import f1_score, accuracy_score\n\ndef eval_prediction(y_batch_actual, y_batch_predicted):\n    \"\"\"Return batches of accuracy and f1 scores.\"\"\"\n    y_batch_actual_np = y_batch_actual.cpu().detach().numpy()\n    y_batch_predicted_np = np.round(y_batch_predicted.cpu().detach().numpy())\n    \n    acc = accuracy_score(y_true=y_batch_actual_np, y_pred=y_batch_predicted_np)\n    f1 = f1_score(y_true=y_batch_actual_np, y_pred=y_batch_predicted_np, average='weighted')\n    \n    return acc, f1\n\nprint(eval_prediction.__doc__)","f1af9871":"# parameters\nnum_of_epochs = 24\nlearning_rate = 27e-6\nbatch_size = 16\nhidden_layers = 8\n\nprint(\"Epochs: {}\".format(num_of_epochs))\nprint(\"Learning rate: {:.6f}\".format(learning_rate))\nprint(\"Batch size: {}\".format(batch_size))\nprint(\"The number of hidden layers in the custom head: {}\".format(hidden_layers))","f80c9273":"model = BertWithCustomNNClassifier(linear_size=hidden_layers)\nmodel.to(device)","570b8e5f":"from transformers import AdamW\n\n# optimizer\noptimizer = AdamW(model.parameters(), lr=learning_rate)\nprint('Initialized optimizer.')","6b06a81e":"# loss function\nloss_fn = nn.BCELoss()\nprint('Initialized loss function.')","088b7f6b":"from torch.utils.data import DataLoader\n\n# Dataset & dataloader\ntrain_dataset = TweetDataset(train_encodings, train_labels)\nval_dataset = TweetDataset(val_encodings, val_labels)\ntrain_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True)\nprint('Created train & val datasets.')","78ac7996":"def training_step(dataloader, model, optimizer, loss_fn, if_freeze_bert):\n    \"\"\"Method to train the model\"\"\"\n    \n    model.train()\n    model.freeze_bert() if if_freeze_bert else model.unfreeze_bert()\n      \n    epoch_loss = 0\n    size = len(dataloader.dataset)\n \n    for i, batch in enumerate(dataloader):        \n        input_ids = batch['input_ids'].to(device)\n        attention_mask = batch['attention_mask'].to(device)\n        labels = batch['labels'].to(device)\n    \n        outputs = torch.flatten(model(tokens=input_ids, attention_mask=attention_mask))\n                        \n        optimizer.zero_grad()\n        loss = loss_fn(outputs, labels.float())\n        epoch_loss += loss.item()\n        loss.backward()\n        optimizer.step()\n\n            \nprint(training_step.__doc__)","1cb68ac8":"def validation_step(dataloader, model, loss_fn):\n    \"\"\"Method to test the model's accuracy and loss on the validation set\"\"\"\n    \n    model.eval()\n    model.freeze_bert()\n    \n    size = len(dataloader)\n    f1, acc = 0, 0\n    \n    with torch.no_grad():\n        for batch in dataloader:\n            X = batch['input_ids'].to(device)\n            attention_mask = batch['attention_mask'].to(device)\n            y = batch['labels'].to(device)\n                  \n            pred = model(tokens=X, attention_mask=attention_mask)\n            \n            acc_batch, f1_batch = eval_prediction(y.float(), pred)                        \n            acc += acc_batch\n            f1 += f1_batch\n\n        acc = acc\/size\n        f1 = f1\/size\n                \n    return acc, f1\n        \nprint(validation_step.__doc__)","0dc1d703":"from tqdm.auto import tqdm\n\ntqdm.pandas()\n\nbest_acc, best_f1 = 0, 0\npath = '.\/best_model.pt'\nif_freeze_bert = False\n\nfor i in tqdm(range(num_of_epochs)):\n    print(\"Epoch: #{}\".format(i+1))\n\n    if i < 5:\n        if_freeze_bert = False\n        print(\"Bert is not freezed\")\n    else:\n        if_freeze_bert = True\n        print(\"Bert is freezed\")\n    \n    training_step(train_loader, model,optimizer, loss_fn, if_freeze_bert)\n    train_acc, train_f1 = validation_step(train_loader, model, loss_fn)\n    val_acc, val_f1 = validation_step(val_loader, model, loss_fn)\n    \n    print(\"Training results: \")\n    print(\"Acc: {:.3f}, f1: {:.3f}\".format(train_acc, train_f1))\n    \n    print(\"Validation results: \")\n    print(\"Acc: {:.3f}, f1: {:.3f}\".format(val_acc, val_f1))\n    \n    if val_acc > best_acc:\n        best_acc = val_acc    \n        torch.save(model, path)","a96fc27a":"test_data = pd.read_csv('..\/input\/nlp-getting-started\/test.csv')\nsample_submission = pd.read_csv('..\/input\/nlp-getting-started\/sample_submission.csv')\n\n\nclean_test_texts = [clean_tweet(tweet) for tweet in test_data['text'].values.tolist()]\ntest_encodings = tokenizer(clean_test_texts,\n                           truncation=True, padding=True,\n                           return_tensors='pt').to(device)\n\nprint(\"Encodings are ready.\")","b108f65f":"model = torch.load(path)\nmodel.eval()\nwith torch.no_grad():\n    predictions = model(tokens=test_encodings['input_ids'], attention_mask=test_encodings['attention_mask'])\n    \nbinary_predictions = np.round(predictions.cpu().detach().numpy()).astype(int).flatten()\n    \nprint(\"Predictions are ready.\")","e99436d8":"sample_submission['target'] = binary_predictions\nsample_submission.to_csv('submission.csv', index=False)\nprint('Predictions are saved to submission.csv.')","be8e26e8":"<a id=\"processing_data_train_test_split\"><\/a>\n## Train-test split:\n[[back to top]](#toc)\n\nNote that *train_test_split*'s *stratify* parameter is used to preserve the ratio of 0 and 1 labels in the splits. It is necessary because there are 4342 non-disaster and 3271 disaster tweets.","ce2bf328":"<a id=\"pytorch_setup_training_step\"><\/a>\n## Training step\n[[back to top]](#toc)","cde7207d":"<a id=\"processing_data_fix_labels\"><\/a>\n## Fix labels\n[[back to top]](#toc)\n\nAs it turns out there are couple of tweets which occurs multiple times. Among those there are some whose labels aren't consistent throughout the occurrences.","05318077":"<a id=\"processing_data_data_cleaning\"><\/a>\n## Data cleaning\n[[back to top]](#toc)\n\nThe most obvious step to take is to remove URLs as they are most likely just noise.\n\nAn additional consideration to take into account is that Hugging Face's tokenizer employs *subword tokenization* as detailed in their summary [here](https:\/\/huggingface.co\/transformers\/tokenizer_summary.html). It essentialy means that if the tokenizer encounters a word which is unknown to it the word gets splitted into multiple tokens. Each new token gets the '##' prefix. For example: \"annoyingly\" becomes \"annoying\" + \"##ly\". Now it is easy to figure out which words are unknown to the model (just by searching for the '##' prefix) and thus gain ideas what sort of cleaning might worth implementing.\n\nIn this implementation URLs, @ links, non ascii characters are completely removed, the negation of some of the auxiliary verbs are fixed (eg.: shouldnt -> should not) and some of the personal pronouns (eg.: im -> i am)","7db3913f":"The amount of tweets with inconsistent labeling seems reasonably low so they can be fixed by hand. (*Note*: One could argue that deleting tweets with inconsistent labeling would be a better practice because modifing the input like that is an overreach, but for the sake of the example I go with releballing.)","35b7aac8":"<a id=\"make_predictions_load_model\"><\/a>\n## Load best model & make predictions\n[[back to top]](#toc)","ae18747c":"<a id=\"make_predictions_save_results\"><\/a>\n## Save results\n[[back to top]](#toc)","83bbb035":"<a id=\"introduction\"><\/a>\n# Introduction\n\nThis notebook describes how to solve the classification of disaster tweets using one of Hugging Face's BERT models with a custom classifier head (the custom classifier being a neural network) using PyTorch. I used Hugging Face's related [tutorial](https:\/\/huggingface.co\/transformers\/custom_datasets.html), [PyTorch QuickStart](https:\/\/pytorch.org\/tutorials\/beginner\/basics\/quickstart_tutorial.html). I also rely on fellow kaggler Milan Kalkenings' excellent [notebook](https:\/\/www.kaggle.com\/milankalkenings\/customize-bert-top-10-pytorch?scriptVersionId=65806223).\n\n(Turn on GPU to make this notebook run faster)","3fe168c7":"<a id=\"pytorch_setup_metrics\"><\/a>\n## Metrics\n[[back to top]](#toc)\n\nMeasuring f1 score & accuracy based on [this](https:\/\/www.kaggle.com\/milankalkenings\/bert-is-the-word-add-custom-layers-to-bert#Train-the-Model-and-evaluate-it-on-the-Validation-Set).","a4a9efb5":"<a id=\"pytorch_setup_custom_model\"><\/a>\n## Custom model\n[[back to top]](#toc)\n\nA pre-trained BERT model with custom classifier. Based on [this notebook](https:\/\/www.kaggle.com\/milankalkenings\/bert-is-the-word-add-custom-layers-to-bert).\n\nThe custom model consists of a pre-trained BERT model (a model which holds a semantical representation of English) and on the top of the BERT model there is a custom neural network which is trained to the spcific task (tweet classification in this case). Therefore, it seems to be reasonable to have *freeze_bert* and *unfreeze_bert* methods apart from the mandatory *\\__init__* and *forward*. Having this two additional methods makes it possible to sort of train the underlying BERT model and the custom classifier separately. (So train BERT and the custom head together, freeze BERT and then train the custom head on the classification task based on the previously trained BERT). The idea of freezing & unfreezing was taken from Milan Kalkenings' [notebook](https:\/\/www.kaggle.com\/milankalkenings\/customize-bert-top-10-pytorch?scriptVersionId=65806223).","c9105ebd":"<a id=\"pytorch_setup_params_model_opt\"><\/a>\n## Parameters, model, optimizer, loss function and data loaders\n[[back to top]](#toc)","c1d7a909":"<a id=\"toc\"><\/a>\n# Table of contents\n\n* [Introduction](#introduction)\n* [Processing data](#processing_data)\n    - [Load](#processing_data_load)\n    - [Fix labels](#processing_data_fix_labels)\n    - [Train-test split](#processing_data_train_test_split)\n    - [Data cleaning](#processing_data_data_cleaning)\n    - [Tokenization](#processing_data_tokenization)\n* [PyTorch setup](#pytorch_setup)\n    - [Custom dataset](#pytorch_setup_custom_dataset)\n    - [Custom model](#pytorch_setup_custom_model)\n    - [Metrics](#pytorch_setup_metrics)\n    - [Parameters, model, optimizer, loss function and data loaders](#pytorch_setup_params_model_opt)\n    - [Training step](#pytorch_setup_training_step)\n    - [Validation step](#pytorch_setup_validation_step)\n    - [Training loop](#pytorch_setup_training_loop)\n* [Make predictions](#make_predictions)\n    - [Read & preprocess test data](#make_predictions_test_data)\n    - [Load best model & make predictions](#make_predictions_load_model)\n    - [Save results](#make_predictions_save_results)","60807e44":"<a id=\"processing_data\"><\/a>\n# Processing data\n[[back to top]](#toc)\n\n<a id=\"processing_data_load\"><\/a>\n## Load\n[[back to top]](#toc)\n\nLoad data using Pandas:","f71b8363":"<a id=\"make_predictions\"><\/a>\n# Make predictions\n[[back to top]](#toc)\n\n<a id=\"make_predictions_test_data\"><\/a>\n## Read & preprocess test data\n[[back to top]](#toc)","786f78e6":"<a id=\"pytorch_setup_training_loop\"><\/a>\n## Training loop\n[[back to top]](#toc)\n\nThe training of the underlying BERT model is freezed after the 5th iteration (there is room for experimenting with the freezing pattern though). After that the training & validation takes place. (I validate on the training set too just to see how the model evolves). What counts is the accuracy on the validation set. If the current model outperforms the best model so far (based on the accuracy on the validation set) it gets saved. And thus the best performing model is selected to make predictions.","9fb84317":"<a id=\"pytorch_setup\"><\/a>\n# PyTorch setup\n[[back to top]](#toc)\n\n<a id=\"pytorch_setup_custom_dataset\"><\/a>\n## Custom dataset\n[[back to top]](#toc)\n\nPyTorch uses *datasets* and *dataloaders* to handle data (see their introductionary tutorial [here](https:\/\/pytorch.org\/tutorials\/beginner\/basics\/data_tutorial.html)). It means that in order to make the handling of tweets straightforward a custom dataset has to be defined. (Named *TweetDataset* in this code)\n\nA *dataset* is a data structure which makes it easy to iterate through the data in training and testing loops, therefore it needs to implement three methods of its base class (which is *torch.utils.data.Dataset*): *\\__init__* (to initialize the dataset with the data), *\\__len__* (to get the number of items in the dataset) and *\\__getitem__* (to return the ith element of the dataset).","0f5e6e92":"<a id=\"pytorch_setup_validation_step\"><\/a>\n## Validation step\n[[back to top]](#toc)\n\nBased on the PyTorch tutorial's [related article](https:\/\/pytorch.org\/tutorials\/beginner\/basics\/optimization_tutorial.html).","2d07319b":"<a id=\"processing_data_tokenization\"><\/a>\n# Tokenization\n[[back to top]](#toc)\n\nThe next code snippet tokenizes the tweets using the model's own tokenizer. Each model has its own associated tokenizer in Hugging Face as it is detailed [here](https:\/\/huggingface.co\/transformers\/preprocessing.html).\n\nThe tokenizer's *truncation=True* setting ensures that the sequence of tokens is truncated if the sequence is longer than the maximal input length acceptable by the model.\n*padding=True* ensures that each sentence is padded to the longest sentence of the batch.\n\nSee [the documentation](https:\/\/huggingface.co\/transformers\/main_classes\/tokenizer.html) for detailed description."}}