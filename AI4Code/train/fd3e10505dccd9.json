{"cell_type":{"c4531b3c":"code","06f75e4e":"code","77cd2359":"code","9a1198cb":"code","a4755129":"code","70bbf074":"code","f6f03940":"code","b78de596":"code","57aff77b":"code","e4a40cfe":"code","e90134ce":"code","7d66bb3e":"code","c530d0a7":"code","68975c28":"code","cdff190f":"code","b1b3baae":"markdown","63d3ff67":"markdown","a5a9f13c":"markdown","543e549a":"markdown","48feffc0":"markdown","d5314d50":"markdown","afbb9396":"markdown","63b00a45":"markdown","960dc316":"markdown"},"source":{"c4531b3c":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","06f75e4e":"data = pd.read_csv(\"\/kaggle\/input\/pima-indians-diabetes-database\/diabetes.csv\")\ndata.info()","77cd2359":"data.head()","9a1198cb":"# Dependent variables, class label for train dataset\ny=data[\"Outcome\"].values  #array\ny","a4755129":"# Independent variables, traindataset\nx_data = data.drop([\"Outcome\"],axis=1)\nx_data.head()","70bbf074":"# Normalization Dataset\n# normalized data = (X - X_M\u0130N) \/ (X_MAX - X_M\u0130N)\nx = (x_data - np.min(x_data)) \/ (np.max(x_data) - np.min(x_data))\nx.head()","f6f03940":"from sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(x,y,test_size=0.2,random_state=42)\n\nprint(x_train.shape)\nprint(x_test.shape)\nprint(y_train.shape)\nprint(y_test.shape)\n\n# Feature ve onlara ait de\u011ferlerin yeri de\u011fi\u015ftirildi.\nx_train = x_train.T\nx_test = x_test.T\ny_train = y_train.T\ny_test = y_test.T\n\nprint(\"Changed of Features and Values place.\")\n\nprint(x_train.shape)\nprint(x_test.shape)\nprint(y_train.shape)\nprint(y_test.shape)","b78de596":"# Initialize\n# w = Create matrix array and all array values are 0.01\n# np.exp = exponent method in numpy library\ndef initialize_weights_and_bias(dimension):\n    #initialize\n    w = np.full((dimension,1),0.01) \n    b=0.0\n    return w,b\n\ndef sigmoid(z):\n    y_head = 1 \/ (1 + np.exp(-z))\n    return y_head","57aff77b":"def forward_backward_propagation(w,b,x_train,y_train):\n    # forward propagation\n    z = np.dot(w.T,x_train) + b\n    y_head = sigmoid(z)\n    loss =  -(1 - y_train) * np.log(1 - y_head) -y_train * np.log(y_head)\n    cost = (np.sum(loss)) \/ x_train.shape[1]\n    \n    #backward propagation\n    derivative_weight = (np.dot(x_train, ((y_head - y_train).T))) \/ x_train.shape[1] #derivative based on weight\n    derivative_bias = np.sum(y_head - y_train ) \/ x_train.shape[1] #derivative based on bias\n    \n    #weight and bias are derivates kept in dictionary(gradients)\n    gradients = {\"derivative_weight\": derivative_weight, \"derivative_bias\":derivative_bias}\n    \n    return cost,gradients","e4a40cfe":"# Updating(learning) parameters\ndef update(w, b, x_train, y_train, learning_rate,number_of_iterarion):\n    cost_list = []\n    cost_list2 = []\n    index = []\n    # updating(learning) parameters is number_of_iterarion times\n    for i in range(number_of_iterarion):\n        # make forward and backward propagation and find cost and gradients\n        cost,gradients = forward_backward_propagation(w,b,x_train,y_train)\n        cost_list.append(cost)\n        # lets update\n        w = w - learning_rate * gradients[\"derivative_weight\"]\n        b = b - learning_rate * gradients[\"derivative_bias\"]\n        if i % 10 == 0:\n            cost_list2.append(cost)\n            index.append(i)\n            print (\"Cost after iteration %i: %f\" %(i, cost))\n    # we update(learn) parameters weights and bias\n    parameters = {\"weight\": w,\"bias\": b}\n    plt.plot(index,cost_list2)\n    plt.xticks(index,rotation='vertical')\n    plt.xlabel(\"Number of Iterarion\")\n    plt.ylabel(\"Cost\")\n    plt.show()\n    return parameters, gradients, cost_list\n#parameters, gradients, cost_list = update(w, b, x_train, y_train, learning_rate = 0.009,number_of_iterarion = 200)","e90134ce":"# Predict Method\n\ndef predict(w,b,x_test):\n    z=sigmoid(np.dot(w.T,x_test) + b)\n    Y_prediction = np.zeros((1,x_test.shape[1]))\n    \n    # if z is bigger than 0.5, our prediction is sign one (y_head=1),\n    # if z is smaller than 0.5, our prediction is sign zero (y_head=0),\n    \n    for i in range(z.shape[1]):\n        if z[0,i]<= 0.5:\n            Y_prediction[0,i] = 0\n        else:\n            Y_prediction[0,i] = 1\n\n    return Y_prediction\n    ","7d66bb3e":"def logistic_regression(x_train, y_train, x_test, y_test, learning_rate ,  num_iterations):\n    \n    #initialize\n    dimension =  x_train.shape[0]  # that is 4096\n    w,b = initialize_weights_and_bias(dimension)\n    # do not change learning rate\n    \n    #update method for forward and backward propagation\n    parameters, gradients, cost_list = update(w, b, x_train, y_train, learning_rate,num_iterations)\n    \n    y_prediction_test = predict(parameters[\"weight\"],parameters[\"bias\"],x_test)\n    \n\n    # Print train\/test Errors\n    print(\"test accuracy: {} %\".format(100 - np.mean(np.abs(y_prediction_test - y_test)) * 100))\n    ","c530d0a7":"#test\nlogistic_regression(x_train, y_train, x_test, y_test,learning_rate = 5, num_iterations = 300)","68975c28":"from sklearn.linear_model import LogisticRegression\nlr = LogisticRegression()\nlr.fit(x_train.T,y_train.T)\nprint(\"Test Accuracy : {}\".format(lr.score(x_test.T,y_test.T)))","cdff190f":"from sklearn import linear_model\nlogreg = linear_model.LogisticRegression(random_state = 42,max_iter= 150)\nprint(\"test accuracy: {} \".format(logreg.fit(x_train.T, y_train.T).score(x_test.T, y_test.T)))\nprint(\"train accuracy: {} \".format(logreg.fit(x_train.T, y_train.T).score(x_train.T, y_train.T)))","b1b3baae":"<a id=\"6\"><\/a>\n# Predict Method","63d3ff67":"<a id=\"2\"><\/a>\n# Train Test Split Data\n    Train Test Split data==> 80% of data set for Train, 20% of data set for Test\n    train_test_split method'unun i\u00e7ine x ve y e\u011fitim i\u00e7in verildi.\n    test_size=0.2 parametresi ilede veri setinin %20'sini test olarak ayr\u0131ld\u0131.\n    random_state parametresi ile \"42\" say\u0131s\u0131n\u0131 id olarak tut ve ayn\u0131 i\u015flem  birdaha yap\u0131l\u0131rsa:\n    ayn\u0131 b\u00f6l\u00fcmleme i\u015flemini yaparak ayn\u0131 sonu\u00e7lara ula\u015fmam\u0131z\u0131 bize sa\u011fla.\n    bu method sonucunda olu\u015facak resultlar\u0131da belirtilen de\u011fi\u015fkenlere aktar.\n    x'in %80 x_train , x'in %20'si x_test ; y'nin %80'i y_train, y'nin %20'si y_test\n","a5a9f13c":"<a id=\"3\"><\/a>\n# Parameter Initialize and Sigmoid Function\n\n    Dimension Parameter: Features count ==> dimension = 8\n    \n    Sigmoid Function : f(x) = 1 \/ ( 1 + (e ^ -x)\n    Initialize weight = 0.01 for each data\n    Initialize bias = 0","543e549a":"<a id=\"8\"><\/a>\n# Logistic Regression with Sklearn Library","48feffc0":"<a id=\"1\"><\/a><br>\n# Data Reading and Data Pre-Processing","d5314d50":"<a id=\"4\"><\/a>\n# Forward and Back Propagation Function\n\n*  **z = bias + px1*w1 + px2*w2 + ... + pxn*wn **\n*  **loss function = -(1 - y) * log(1- y_head) - y * log(y_head)**\n*  **cost function = sum(loss value) \/ train dataset sample count**\n   \n         Her bir weightin, kendisine ait her bir x_train ile \u00e7arp\u0131lmas\u0131 gerekir.\n         \n         \u00d6rne\u011fin:\n         \n         x_train,y_train = \u00d6\u011frenme veri seti, w=weightler ==> (30,1) * (30,455) matrixlerinin \u00e7arpmas\u0131 olamaz.\n         \n             1. matrix'in s\u00fctunu ile 2. matrix'in sat\u0131r\u0131 birbirine uymal\u0131d\u0131r => (1,30) * (30,455) olmal\u0131d\u0131r.\n             \n         Bu i\u015flem sonucunda da (1,455) lik bir matrix elde edilir. ==> np.dot(w.T,x_train,y_train)\n         \n         Forward - Backward \u0130\u015fleminde yap\u0131lacaklar:\n         \n      *forward i\u00e7in:*\n      \n           1.) x_train de\u011ferleri ile a\u011f\u0131rl\u0131klar\u0131(weights) \u00e7arp ve bias ekle.\n           2.) y_head de\u011ferini sigmoid function ile hesapla.\n           3.) loss function formul\u00fcnden yola \u00e7\u0131karak loss de\u011ferini hesapla\n           4.) cost functionu hesapla => sum(loss) \/ sample_count\n\n      *Backward i\u00e7in:*\n      \n           1.)backward i\u015fleminde gerekli weighte g\u00f6re t\u00fcrev al.\n           2.)backward i\u015fleminde gerekli bias'a g\u00f6re t\u00fcrev al\n           cost ve gradients(derivative_weight, derivative_bias) return et.","afbb9396":"# INTRODUCTION\n* Bu kernelde amac\u0131m\u0131z veri setini kullanarak bir hastan\u0131n diyabet olup olmad\u0131\u011f\u0131n\u0131 tahmin etmektir. Tahmin etme i\u015flemini Logistic Regression ile yaparak 0 veya 1 s\u0131n\u0131f etiketlerine ula\u015f\u0131ca\u011f\u0131z.\n\n        Bu kenelde, pima-indians-diabetes data seti ile Linear Learning ger\u00e7ekle\u015ftirilmi\u015ftir.\n        Features:\n        Pregnancies: Gebelik Say\u0131s\u0131\n        Glucose: Oral glukoz tolerans testinde glikoz konsantrasyonu de\u011feri.\n        BloogPressure: Kan De\u011feri(mm Hg)\n        SkinThickness: Cilt Kal\u0131nl\u0131\u011f\u0131(mm)\n        Insulin: 2 saatlik serum insulini(mu U\/ml)\n        BMI:  V\u00fccut K\u00fctle indeksi\n        DiabetesPedigreeFunction: Diyabet soya\u011fac\u0131 i\u015flevi\n        Age: Ya\u015f\n        Outcome: Diyabet olup olmamas\u0131 1 veya 0\n\n  \n\n<br>\n1.) [Data Reading and Data Pre-Processing](#1)<br>\n2.) [Train Test Split Data](#2)<br>\n3.) [Parameter Initialize and Sigmoid Function](#3)<br>\n4.) [Forward and Back Propagation Function](#4)<br>\n5.) [Update Function for Parameters (Weight,Bias)](#5)<br>\n6.) [Predict Method](#6)<br>\n7.) [Logistic Regression (test main)](#7)<br>\n8.) [Logistic Regression with Sklearn Library](#8)<br>\n\n","63b00a45":"<a id=\"5\"><\/a>\n# Update Function\n\n     Update i\u015flemi arka arkaya forward propagation ve backward propagatin i\u015fleminin n defa yap\u0131lmas\u0131 i\u015flemidir.\n     Bu nedenle parametrelerimiz:\n         g\u00fcncellenecek weightler             : w                    => parameter 1\n         g\u00fcncellenecek biaslar               : b                    => parameter 2\n         forward i\u00e7in x_train input1         : x_train              => inputs features values\n         forward i\u00e7in y_train input_label    : y_train              => inputs class labels\n         slope i\u00e7in learning_rate de\u011feri     : learning_rate        => hyper parameter 1\n         forward ve backward tekrar say\u0131s\u0131   : number_of_iteration  => hyper parameter 2\n     Not: Gradients, weight ve bias'\u0131n t\u00fcrevlerini tutar.\n     costlar\u0131 tutar\u0131z \u00e7\u00fcnk\u00fc nuber_of_itearation say\u0131s\u0131n\u0131 belirlemek i\u00e7in.\n     learning_rate fazla veya az olursa \u00f6\u011frenme i\u015flemi kazaya u\u011frayabilir.\n     cost2'nin pek bir i\u015flevi yoktur sadece her 10 ad\u0131mda bir costlar\u0131 tutuyoruz \n     Bunun sebebi costlar\u0131 10'ar ad\u0131mda bir plot ettirmektir.","960dc316":"<a id=\"7\"><\/a>\n# Logistic Regression (test main)"}}