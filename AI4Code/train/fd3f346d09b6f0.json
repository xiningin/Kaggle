{"cell_type":{"c3673b00":"code","d43c4ca0":"code","188733c9":"code","aec385d1":"code","0612b704":"code","55a50b04":"code","1602efb9":"code","8d1471aa":"code","381309b5":"code","f7ede1c8":"code","f8350545":"code","229efc47":"code","50f2a1c7":"code","d783c6f1":"code","64c02e33":"code","efbc757a":"code","f428ab1b":"code","7ac9d083":"code","054d9e8a":"code","b4bbb144":"code","de3b4381":"code","101a04a4":"code","061a0bc3":"code","5c8cbc90":"code","3ac4760a":"code","0b5da7cd":"code","32e5de6e":"code","d27889e4":"code","91bd52b2":"markdown","1bd18503":"markdown","34e22c7f":"markdown","43587c97":"markdown","49433db0":"markdown","4e8e9b5a":"markdown"},"source":{"c3673b00":"import pandas as pd\nimport numpy as np\nfrom mlxtend.frequent_patterns import apriori, association_rules","d43c4ca0":"df = pd.read_csv('..\/input\/groceries-dataset\/Groceries_dataset.csv', parse_dates=[1]) ","188733c9":"df.dtypes","aec385d1":"df.isna().sum()","0612b704":"df","55a50b04":"# First, get a list of all items. Easiest way to do this is by dummyfying the item description column\nitems_dummies = pd.get_dummies(df['itemDescription'])","1602efb9":"# Baskets will be a list of items bought together\nbaskets = df.groupby(['Date', 'Member_number']).agg(lambda x: ','.join(x).split(','))['itemDescription'].values","8d1471aa":"baskets, len(baskets)","381309b5":"df = df.join(items_dummies)","f7ede1c8":"df","f8350545":"df.drop('itemDescription', axis=1, inplace=True)","229efc47":"df = df.groupby(['Date', 'Member_number']).sum() ","50f2a1c7":"df.head()","d783c6f1":"df['basket'] = baskets\ndf.head(1) # Added the basket column to the dataset","64c02e33":"# Check if the calculation is ok\n(df.sum(axis=1) != df['basket'].apply(len)).sum() # Perfect","efbc757a":"# There are samples where there are more than one of the same item in the basket (eg. {milk, milk})\n# We need to only keep 1\nlen(np.where(df.drop('basket', axis=1)>1)[0])","f428ab1b":"for i in df.drop('basket', axis=1):\n    df[i] = df[i].map(lambda x: 1 if x >1 else x)","7ac9d083":"df","054d9e8a":"len(np.where(df.drop('basket', axis=1)>1)[0])","b4bbb144":"df['UHT-milk'].sum()\/len(df) # An example of calculating support for an item","de3b4381":"# Drop the item list from the dataframe. No longer needed, since we have verified that the encoding is correct.\ndf.drop('basket', axis=1, inplace=True)","101a04a4":"# Lets keep the support threshold at 0.1%\nsupports = apriori(df, min_support=1e-3, use_colnames=True)","061a0bc3":"# Down to 69 items. How many associations can we have?\nsupports","5c8cbc90":"# Number of itemsets which have over 1 item in the basket\nsupports[supports['itemsets'].map(lambda x: len(x)>1)] ","3ac4760a":"associations = association_rules(supports, metric='lift', min_threshold=1)","0b5da7cd":"# Antecendent support is support for the antecedent, i.e. before adding the new item\n# Consequent support is the support of the consequent, i.e. the new item (in the row 127, the consequent support\n# is the same as the support of sausage)\nassociations = associations.sort_values('confidence', ascending=False)\n","32e5de6e":"associations.head(10)","d27889e4":"associations.sort_values('lift', ascending=False).head(10)","91bd52b2":"The support is the probability of an item (or itemset) being bought. Companies usually ignore promoting or working on products that have low support, since there is no point promoting a product that in any case hardly sells. ","1bd18503":"The format of the data is the id, data and the item bought (i.e. the item description column). \nSince the itemDescription column has only 1 item, we first group it by the id and the date, to create itemsets (also known as baskets).\n\nThe issue with this grouping is that we cannot be sure that the items are bought during the same visit, or doing multiple visits in the same day. However, since there is no other way, we assume that no one visited the store more than once a day","34e22c7f":"The table below is sorted by lift. This is a better metric to use while creating association rules, since it tells you how much more a likely customer will buy an item given that he already has the other items in his basket.\n\nFor the first two examples, a customer is twice as likely to buy yogurt and milk if he\/she has already bought sausages, or sausages if he\/she has already bought yogurt and milk.\n\nAn interesting association here is between citrus fruit and specialty chocolate (lift of 1.65) and between tropical fruit and flour (lift of 1.61). However, since the dataset is limited (only ~14000 baskets) and the support for the new rules is extremely low, this could be due to chance as well. ","43587c97":"Since the dataset is in one column, we need to group it based on date and member number\nWe will have to make the assumption that one member does not visit the store twice in one day, since we only have date of visit, not the time.","49433db0":"The table below is sorted by confidence. The topmost association {Yogurt, Sausage} -> Whole Milk has a confidence of 0.25, i.e. 25% of all baskets containing yogurt and sausage also contains whole milk.\n\nSimilarly, 21% of items containing {Rolls\/Buns, Sausage} also contains Whole Milk. \n\nWhole milk is highly prevalent in this table, since it's individual support is 15.7%, i.e. 15.7% of all people who buy from the store buy milk.","4e8e9b5a":"It is good to have a threshold metric for association rules.\nThe two main options are \"confidence\" and \"lift\".\n\nConfidence is the proportion of all baskets of the selected antecendent itemset that also contains the consequent item. For eg, if 70% of all baskets with {Egg, Cheese} also contains {Milk}, then the confidence in the rule is 70%. \n\nLift is the influence the antecedent itemset has on the consequent item. For instance, if 70% of baskets with {Egg, Cheese} contain {Milk}, but overall, 80% of all itemsets contain milk, then the lift is 70%\/80% = 0.875. This means that although the confidence in the rule is high, the probability of the customer buying milk if he\/she has egg and cheese in the basket actually decreases.\n\nUsing confidence as a pruning metric may work in certain scenarios, but in general, it is always better to use lift.\nThe minimum threshold should be over 1. However, since the data we have is extremely limited, I have gone with a threshold of 1."}}