{"cell_type":{"ab10f56c":"code","752cb2aa":"code","a90d25b2":"code","f683f17b":"code","53087d97":"code","2cc80cbb":"code","7412bb13":"code","c8971a0f":"code","7b1e8eb9":"code","2188dbee":"code","2a286bc9":"code","6f79d1cb":"code","c0941151":"code","6f603de7":"code","a7bebd0a":"code","675d0a68":"code","e9fbcffe":"code","4201999f":"code","6ff44904":"code","32ba25ac":"code","66f4119c":"code","45ff475e":"code","18be6b42":"code","325d61f8":"code","966056aa":"code","10a7fabc":"code","174bb3bb":"code","3df08797":"code","b2f6c104":"code","6a4e5f53":"code","9a37352d":"code","43ca8f2f":"code","bc82ae46":"code","b1d782cc":"code","6d1cd486":"code","66bd125c":"code","acea8b07":"code","0a0bf24a":"code","5dcccad2":"code","75bac01a":"code","802c805f":"code","48dd506d":"code","0c7b000e":"markdown","dd253cfa":"markdown","4af24f11":"markdown","02fd07a9":"markdown","d89bcfab":"markdown","2ecd4530":"markdown","fe94f827":"markdown","6bd31443":"markdown","e21165cf":"markdown","8eeebc51":"markdown","697ab832":"markdown","26e83616":"markdown","c4a1e456":"markdown","cb87d9a9":"markdown","6b7f7658":"markdown","c1ab3c89":"markdown","aeeeda26":"markdown","4add8a46":"markdown","ae368cd2":"markdown","7d684d41":"markdown","6baf5927":"markdown","786541a7":"markdown","d9ae933d":"markdown","d9b7e7a4":"markdown","96f366fb":"markdown","4349354a":"markdown","bf8c7a9c":"markdown","09d3ef8c":"markdown","5a5c37f3":"markdown","4a26b103":"markdown","3c1132fa":"markdown","9c5e877f":"markdown","79788f28":"markdown","e32d5b08":"markdown","5b6d739d":"markdown","44dc2196":"markdown","9640ef24":"markdown","c395aa07":"markdown","542ba5fc":"markdown","482a7d1d":"markdown"},"source":{"ab10f56c":"import pandas as pd\nimport numpy as np\n\nmeta = pd.read_csv('..\/input\/birdclef-2021\/train_metadata.csv')","752cb2aa":"meta.head(5)","a90d25b2":"meta.shape","f683f17b":"meta.describe()","53087d97":"meta.isnull().sum()","2cc80cbb":"meta.info()","7412bb13":"import plotly.express as px\nspecies = meta['primary_label'].value_counts()\nfig = px.bar(species, x=species.index, y='primary_label', labels=dict(x=\"Species\", y=\"Count\"),title = \"Bird Species Count\")\nfig.show()","c8971a0f":"print(len(species))","7b1e8eb9":"cnt = 0\nfor i in species:\n    if i > 300:\n        cnt += 1\nprint(cnt)","2188dbee":"meta['secondary_labels'].value_counts()","2a286bc9":"meta['author'].nunique()","6f79d1cb":"meta_exp = meta.groupby(['primary_label','author']).size()\nmeta_exp","c0941151":"# Code adapted from: https:\/\/www.kaggle.com\/andradaolteanu\/birdcall-recognition-eda-and-audio-fe\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport descartes\nimport geopandas as gpd\nfrom shapely.geometry import Point, Polygon\n\n# SHP file\nworld_map = gpd.read_file(\"..\/input\/world-shapefile\/world_shapefile.shp\")\n\n# Coordinate reference system\ncrs = {\"init\" : \"epsg:4326\"}\n\n# Lat and Long need to be of type float, not object\nspecies_list = ['norcar', 'houspa', 'wesblu', 'banana']\ndata = meta[meta['primary_label'].isin(species_list)]\ndata[\"latitude\"] = data[\"latitude\"].astype(float)\ndata[\"longitude\"] = data[\"longitude\"].astype(float)\n\n# Create geometry\ngeometry = [Point(xy) for xy in zip(data[\"longitude\"], data[\"latitude\"])]\n\n# Geo Dataframe\ngeo_df = gpd.GeoDataFrame(data, crs=crs, geometry=geometry)\n\n# Create ID for species\nspecies_id = geo_df[\"primary_label\"].value_counts().reset_index()\nspecies_id.insert(0, 'ID', range(0, 0 + len(species_id)))\n\nspecies_id.columns = [\"ID\", \"primary_label\", \"count\"]\n\n# Add ID to geo_df\ngeo_df = pd.merge(geo_df, species_id, how=\"left\", on=\"primary_label\")\n\n# === PLOT ===\nfig, ax = plt.subplots(figsize = (16, 10))\nworld_map.plot(ax=ax, alpha=0.4, color=\"grey\")\n\npalette = iter(sns.hls_palette(len(species_id)))\nfor i in range(len(species_list)):\n    geo_df[geo_df[\"ID\"] == i].plot(ax=ax, \n                                   markersize=20, \n                                   color=next(palette), \n                                   marker=\"o\", \n                                   label = species_id['primary_label'].values[i]);\n    \nax.legend()","6f603de7":"meta['year'] = meta['date'].apply(lambda x: x.split('-')[0])\nmeta['month'] = meta['date'].apply(lambda x: x.split('-')[1])\nmeta['day_of_month'] = meta['date'].apply(lambda x: x.split('-')[2])\nmeta.head(3)","a7bebd0a":"patterns = pd.DataFrame()\npatterns['time'] = pd.to_datetime(meta['time'], errors='coerce')\npatterns = patterns.dropna(subset=['time']).reset_index().drop('index',axis=1)\npatterns.time = patterns['time'].dt.hour.astype('int')","675d0a68":"patterns.time","e9fbcffe":"patterns_type = []\nfor t in patterns.time:\n    if((t>4) and (t<12)):\n        patterns_type.append(0)\n    elif((t>=12) and (t<16)):\n        patterns_type.append(1)\n    elif((t>=16) and (t<19)):\n        patterns_type.append(2)\n    else:\n        patterns_type.append(3)","4201999f":"patterns['type'] = patterns_type","6ff44904":"patterns","32ba25ac":"b_types = patterns['type'].value_counts()\nb_types","66f4119c":"fig = px.bar(patterns, x=b_types.index, y=b_types, title = \"Bird Call Time Distribution Graph\",)\nfig.show()","45ff475e":"print(\"Minimum Rating:\",min(meta['rating']))\nprint(\"Maximum Rating:\",max(meta['rating']))\n","18be6b42":"ratings = meta['rating'].value_counts()\nfig = px.bar(ratings, x=ratings.index, y='rating', title = \"Rating Count\")\nfig.show()","325d61f8":"meta['type'].value_counts()","966056aa":"type_edit = meta['type'].apply(lambda x: x.replace('[', ''))\ntype_edit = type_edit.apply(lambda x: x.replace(']', ''))\ntype_edit = type_edit.apply(lambda x: x.split(',')).reset_index().explode(\"type\")","10a7fabc":"type_edit['type'].value_counts()","174bb3bb":"top_10 = list(type_edit['type'].value_counts().head(10).reset_index()['index'])\ntop_10_freq = list(type_edit['type'].value_counts().head(10))\ndata = type_edit[type_edit['type'].isin(top_10)]\nfig = px.bar(type_edit, x=top_10, y=top_10_freq, title = \"Top 10 Call Types\")\nfig.show()","3df08797":"train = pd.read_csv('..\/input\/birdclef-2021\/train_soundscape_labels.csv')\ntest = pd.read_csv('..\/input\/birdclef-2021\/test.csv')","b2f6c104":"train","6a4e5f53":"train.info()","9a37352d":"sites = train['site'].value_counts()\nfig = px.bar(train, x=sites.index, y=sites, title = \"Site Distribution\")\nfig.show()","43ca8f2f":"types_bird = train['birds'].value_counts()\nfig = px.bar(train, x=types_bird.index, y=types_bird)\nfig.show()","bc82ae46":"test","b1d782cc":"test.info()","6d1cd486":"import os\ntrain_short_audio_path = '..\/input\/birdclef-2021\/train_short_audio'\naudio_count = []\nbird_species = []\nfor i in os.listdir(train_short_audio_path):\n    bird_species.append(i)\n    audio_cnt = len(os.listdir(train_short_audio_path + os.sep + f\"{i}\"))\n    audio_count.append(audio_cnt)\n","66bd125c":"fig = px.bar(x=bird_species, y=audio_count, title = \" Audio Count Distribution\")\nfig.show()","acea8b07":"#Reading Random File\n\nimport librosa\naudio_file = '..\/input\/birdclef-2021\/train_short_audio\/astfly\/XC118723.ogg'\nx , sr = librosa.load(audio_file)","0a0bf24a":"import IPython.display as play\nplay.Audio(audio_file)","5dcccad2":"import matplotlib.pyplot as plt\nimport librosa.display\nplt.figure(figsize=(14, 5))\nlibrosa.display.waveplot(x, sr=sr)","75bac01a":"X = librosa.stft(x)\nXdb = librosa.amplitude_to_db(abs(X))\nplt.figure(figsize=(14, 5))\nlibrosa.display.specshow(Xdb, sr=sr, x_axis='time', y_axis='hz')\nplt.colorbar()","802c805f":"fs=10\nmfccs = librosa.feature.mfcc(x, sr=fs)\nprint(mfccs.shape)\nplt.figure(figsize=(15, 7))\nlibrosa.display.specshow(mfccs, sr=sr, x_axis='time')","48dd506d":"hop_length=12\nchromagram = librosa.feature.chroma_stft(x, sr=sr, hop_length=hop_length)\nplt.figure(figsize=(15, 5))\nlibrosa.display.specshow(chromagram, x_axis='time', y_axis='chroma', hop_length=hop_length, cmap='coolwarm')","0c7b000e":"\n\n\n\n\n\n---","dd253cfa":"Recent advances in machine listening have improved acoustic data collection. However, it remains a challenge to generate analysis outputs with high precision and recall. The majority of data is unexamined due to a lack of effective tools for efficient and reliable extraction of the signals of interests (e.g., bird calls).\n\nIn this competition, you\u2019ll automate the acoustic identification of birds in soundscape recordings. You'll examine an acoustic dataset to build detectors and classifiers to extract the signals of interest (bird calls). Innovative solutions will be able to do so efficiently and reliably.\n\n### **Task:-**\nYour challenge in this competition is to identify which birds are calling in long recordings, given training data generated in meaningfully different contexts. For each row_id\/time window, you need to provide a space delimited list of the set of unique birds that made a call beginning or ending in that time window. If there are no bird calls in a time window, use the code nocall.","4af24f11":"#### <font color = 'orange'>Waveplots<\/font>\n* Waveplots let us know the loudness of the audio at a given time.\n* librosa.display.waveplot is used to plot waveform of amplitude vs time","02fd07a9":"### <font color = 'red'>Rating<\/font>","d89bcfab":"## ***Train Metadata***\n* It specifies the audible species for each recording.\n* Consists of information like Primary Label, Secondary Label, Type, Location, Time & Date, Rating etc.","2ecd4530":"### **Now, Let's explore each column**","fe94f827":"# Data Description","6bd31443":"### <font color = 'red'>Author<\/font>","e21165cf":"* It contain recordings of 397 different primary labels(species).\n* As we can see from the graph, it is highly imbalanced training data kind of 'Multitailed Classification'.\n* Out of 397 species only 39 species have label count of more than 300","8eeebc51":"Both Sites have equal distribution.","697ab832":"### <font color = 'red'>Secondary Labels<\/font>\n* It contains list of eBird codes (i.e., primary labels) that recordists annotated.\n* Can be used for Multi-label training.","26e83616":"### <font color ='red'>Date<\/font>","c4a1e456":"#### <font color = 'orange'>Chromagram<\/font>\n* Chromagram closely relates to the twelve different pitch classes. \n* Chroma-based features, which are also referred to as \u201cpitch class profiles\u201d.\n* One main property of chroma features is that they capture harmonic and melodic characteristics of music, while being    robust to changes in timbre and instrumentation.","cb87d9a9":"\n#### **train_short_audio -**\nThe bulk of the training data consists of short recordings of individual bird calls generously uploaded by users of xenocanto.org. These files have been downsampled to 32 kHz where applicable to match the test set audio and converted to the ogg format. The training data should have nearly all relevant files; we expect there is no benefit to looking for more on xenocanto.org.\n\n#### **train_soundscapes -** \nAudio files that are quite comparable to the test set. They are all roughly ten minutes long and in the ogg format. The test set also has soundscapes from the two recording locations represented here.\n\n#### **test_soundscapes -**\nWhen you submit a notebook, the test_soundscapes directory will be populated with approximately 80 recordings to be used for scoring. These will be roughly 10 minutes long and in ogg audio format. The file names include the date the recording was taken, which can be especially useful for identifying migratory birds.\n\nThis folder also contains text files with the name and approximate coordinates of the recording location plus a csv with the set of dates the test set soundscapes were recorded.\n\n#### **test.csv -** Only the first three rows are available for download; the full test.csv is in the hidden test set.\n\n**row_id:** ID code for the row.\n\n**site:** Site ID.\n\n**seconds:** the second ending the time window\n\n**audio_id:** ID code for the audio file.\n\n#### **train_metadata.csv -** A wide range of metadata is provided for the training data. The most directly relevant fields are:\n\n**primary_label:** a code for the bird species. You can review detailed information about the bird codes by appending the code to https:\/\/ebird.org\/species\/, such as https:\/\/ebird.org\/species\/amecro for the American Crow.\n\n**recodist:** the user who provided the recording.\n\n**latitude & longitude:** coordinates for where the recording was taken. Some bird species may have local call 'dialects,' so you may want to seek geographic diversity in your training data.\n\n**date:** while some bird calls can be made year round, such as an alarm call, some are restricted to a specific season. You may want to seek temporal diversity in your training data.\n\n**filename:** the name of the associated audio file.\n\n#### **train_soundscape_labels.csv -**\n\n**row_id:** ID code for the row.\n\n**site:** Site ID.\n\n**seconds:** the second ending the time window\n\n**audio_id:** ID code for the audio file.\n\n**birds:** space delimited list of any bird songs present in the 5 second window. The label nocall means that no call occurred.\n\n#### **sample_submission.csv -** A properly formed sample submission file. Only the first three rows are public, the remainder will be provided to your notebook as part of the hidden test set.\n\n**row_id**\n\n**birds:** space delimited list of any bird songs present in the 5 second window. If there are no bird calls, use the label nocall.\n","6b7f7658":"# **Let's Understand the Data**","c1ab3c89":"## **Train & Test Data**","aeeeda26":"### <font color='red'>Primary Label<\/font>\n* A code for the bird species. \n* You can review detailed information about the bird codes by appending the code to https:\/\/ebird.org\/species\/, such as https:\/\/ebird.org\/species\/amecro for the American Crow.\n\n","4add8a46":"### <font color = 'green'>**Refrences**<\/font>\n\n* https:\/\/www.kaggle.com\/andradaolteanu\/birdcall-recognition-eda-and-audio-fe\n* https:\/\/www.kaggle.com\/stefankahl\/birdclef2021-exploring-the-data","ae368cd2":"So most of the recordings have rating greater than 3.0 whcih is good. However, there are around 3.3k recordings which have 0 rating.","7d684d41":"![\"](https:\/\/storage.googleapis.com\/kaggle-competitions\/kaggle\/25954\/logos\/header.png?t=2021-03-19-18-32-57)\n> # Complete EDA\ud83d\udd0e\ud83d\udcca\ud83d\udcc8 for BirdCLEF 2021 - Birdcall Identification Challenge\n","6baf5927":"***","786541a7":"There is quite large 5 seconds window in recordings where there is no call present.","d9ae933d":"\n# Competition Overview","d9b7e7a4":"* Around in 41358 rows, secondary label is not present.\n* Majority of recordings do not have an annotation of background species. \n* Yet, it is highly likely that most of them actually contain one or more additional species. ","96f366fb":"while some bird calls can be made year round, such as an alarm call, some are restricted to a specific season. You may want to seek temporal diversity in your training data.","4349354a":"> ### **If you find this notebook useful, do give me an upvote.\ud83d\udc4d**","bf8c7a9c":"***","09d3ef8c":"Total 2129 authors are there.","5a5c37f3":"The audio signal is a three-dimensional signal in which three axes represent time, amplitude and frequency.\nThe data provided of audio cannot be understood by the models directly to convert them into an understandable format feature extraction is used.\nlibrosa.display is used to display the audio files in different formats such as wave plot, spectrogram, or colormap etc. Waveplots let us know the loudness of the audio at a given time.\n\n","4a26b103":"## If you find this notebook useful, do give me an upvote \ud83d\udc4d.\n\n## This notebook will be updated frequently so keep checking for further developments.\n\n## In case of any doubts reach out to me on [LinkedIn](https:\/\/www.linkedin.com\/in\/rajendra-sarpal-rs465\/).","3c1132fa":"### <font color = 'red'>Location<\/font>","9c5e877f":"### *Librosa vs Scipy*\nI have chosen 'Librosa' because It normalizes the data while reading\/loading audio file in the range 1 and -1 where as 'scipy' doesn't.","79788f28":"Here,\n\n0 : Morning\n\n1 : Afternoon\n\n2 : Evening\n\n3 : Night","e32d5b08":"Only the first three rows are available ; the full test.csv is in the hidden test set.","5b6d739d":"---","44dc2196":"#### <font color = 'orange'>MFCC<\/font>\n* This feature is one of the most important method to extract a feature of an audio signal and is used majorly whenever working on audio signals. \n* The mel frequency cepstral coefficients (MFCCs) of a signal are a small set of features (usually about 10\u201320) which concisely describe the overall shape of a spectral envelope.\n* .mfcc() is used to calculate mfccs of a signal.\n* By printing the shape of mfccs you get how many mfccs are calculated on how many frames. The first value represents the number of mfccs calculated and another value represents a number of frames available.","9640ef24":"Location data might be a good feature since It is poosible that certain bird species are from particular regions only.\nFor example, here :\nThe Bananaquit (banana) seems to only occur in Central and South America.\nHouse Sparrow (houspa) has occurrences around the globe.","c395aa07":"\n#### <font color = 'orange'>Spectrogram<\/font>\n* A spectrogram is a visual representation of the spectrum of frequencies of sound or other signals as they vary with time. \n* It\u2019s a representation of frequencies changing with respect to time for given music signals.\n* .stft() converts data into short term Fourier transform(STFT) so that we can know the amplitude of given frequency at a given time.\n* .specshow() is used to display spectogram.","542ba5fc":"# Let's Explore Audio Data","482a7d1d":"---"}}