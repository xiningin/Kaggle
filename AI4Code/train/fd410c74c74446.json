{"cell_type":{"ab5063f5":"code","1fc22317":"code","1920e278":"code","899da1f2":"code","f92b01ea":"code","4dbe8264":"code","5ec8ccd1":"code","83a83bd3":"code","507fbde7":"code","928033ad":"code","96e6244c":"code","d3981307":"code","294d7d2f":"code","aff75a18":"code","f665146b":"code","1cb30ebe":"code","a60f30a5":"code","0d9c78a0":"code","eb9e63a3":"code","49d5ab0e":"code","f495fae8":"code","1196be89":"code","57ea8679":"code","3d3aa718":"code","f8c90197":"code","fa10cc77":"code","95f7c01b":"code","ee946b54":"code","4484ed0f":"code","9a69ce07":"code","a2712497":"code","f863c55e":"code","d60f195d":"code","adb35142":"code","80fc6774":"code","dde1653c":"code","ac17f266":"code","07ce805c":"code","cfa1a1b6":"code","d520c40a":"code","10d8ecfb":"code","4404b7e7":"code","f17d3b3f":"code","c4608a37":"code","89e65204":"code","356f2ee9":"code","d332cf56":"code","42fbdc26":"code","f1ddb07d":"code","74ac4b49":"code","4617abf2":"code","80fde086":"code","9c2c32ab":"code","20a9f82f":"code","c28358f9":"code","686663eb":"code","db262fe7":"code","37ce064d":"code","3dee85c9":"code","adf3fcfb":"code","852c5b55":"code","4b8e64f6":"code","6a7491ac":"code","9133c5bb":"code","da3adaaa":"code","e5ff4480":"code","6f8a3806":"code","f0c34592":"code","dbe5acbc":"code","0e8f1329":"code","c6ced54b":"markdown","5bfa214f":"markdown","ac0dc7df":"markdown","d78e5290":"markdown","d630a26f":"markdown"},"source":{"ab5063f5":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.linear_model import LogisticRegression, LogisticRegressionCV, Lasso, lars_path\nfrom sklearn.naive_bayes import MultinomialNB,GaussianNB,BernoulliNB\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier,VotingClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split, KFold, cross_val_score, GridSearchCV, RandomizedSearchCV\nfrom sklearn.metrics import confusion_matrix, precision_score, recall_score, precision_recall_curve,f1_score, roc_auc_score, roc_curve, log_loss,classification_report\n\nfrom ipywidgets import interactive\n\npd.set_option('display.max_columns', 500)\npd.set_option('display.max_rows', 500)\nimport re\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n%matplotlib inline\n%pylab inline\nimport os\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.svm import LinearSVC\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.manifold import TSNE\nfrom pprint import pprint\nfrom xgboost import XGBRegressor\nfrom gensim.models import Phrases, LdaModel\nfrom gensim.corpora import Dictionary\nimport nltk\nfrom nltk.stem import WordNetLemmatizer, SnowballStemmer\nfrom nltk.tokenize import RegexpTokenizer\nfrom nltk.corpus import brown\nfrom nltk import FreqDist\nfrom wordcloud import WordCloud \nfrom collections import OrderedDict","1fc22317":"import tensorflow as tf\nfrom IPython.display import display, Image\nfrom pandas import get_dummies\n# Config the matlotlib backend as plotting inline in IPython\n%matplotlib inline\n\nfrom sklearn.model_selection import train_test_split","1920e278":"train = pd.read_csv('\/content\/drive\/MyDrive\/PICT\/News\/train (2).csv')\ntrain.drop('ids', axis=1, inplace=True)\ntrain.head()","899da1f2":"train.info()","f92b01ea":"sample = pd.read_csv('\/content\/drive\/MyDrive\/PICT\/News\/sample_submission_lvl3.csv')\nsample.head()","4dbe8264":"for i,row in enumerate(train['article']):\n\n  words = re.split(r'\\W+', row)\n  train['article'][i] = words","5ec8ccd1":"import string\ntable = str.maketrans('', '', string.punctuation)","83a83bd3":"for i,row in enumerate(train['article']):\n\n  train['article'][i] = [w.translate(table) for w in row]","507fbde7":"for i,row in enumerate(train['article']):\n\n  train['article'][i] = [word.lower() for word in row]","928033ad":"import nltk","96e6244c":"for i,row in enumerate(train['article']):\n\n  train['article'][i] = [word for word in row if word.isalpha()]","d3981307":"import nltk\nnltk.download('stopwords')\n\nfrom nltk.corpus import stopwords\nstop_words = stopwords.words('english')\nprint(stop_words)","294d7d2f":"for i,row in enumerate(train['article']):\n\n  train['article'][i] = [w for w in row if not w in stop_words]","aff75a18":"from nltk.stem.porter import PorterStemmer\nporter = PorterStemmer()","f665146b":"for i,row in enumerate(train['article']):\n\n  train['article'][i] = [porter.stem(word) for word in row]","1cb30ebe":"tr","a60f30a5":"train['article']","0d9c78a0":"temp = train\nx_t = train","eb9e63a3":"import nltk\nfrom nltk.corpus import stopwords\nfrom nltk.stem.snowball import SnowballStemmer\nimport re\nimport sys\nimport warnings\n\nif not sys.warnoptions:\n    warnings.simplefilter(\"ignore\")\ndef cleanHtml(sentence):\n    cleanr = re.compile('<.*?>')\n    cleantext = re.sub(cleanr, ' ', str(sentence))\n    return cleantext\ndef cleanPunc(sentence): #function to clean the word of any punctuation or special characters\n    cleaned = re.sub(r'[?|!|\\'|\"|#]',r'',sentence)\n    cleaned = re.sub(r'[.|,|)|(|\\|\/]',r' ',cleaned)\n    cleaned = cleaned.strip()\n    cleaned = cleaned.replace(\"\\n\",\" \")\n    return cleaned\ndef keepAlpha(sentence):\n    alpha_sent = \"\"\n    for word in sentence.split():\n        alpha_word = re.sub('[^a-z A-Z]+', ' ', word)\n        alpha_sent += alpha_word\n        alpha_sent += \" \"\n    alpha_sent = alpha_sent.strip()\n    return alpha_sent","49d5ab0e":"temp['article'] = temp['article'].str.lower()\ntemp['article'] = temp['article'].apply(cleanHtml)\ntemp['article'] = temp['article'].apply(cleanPunc)\ntemp['article'] = temp['article'].apply(keepAlpha)","f495fae8":"temp.head()","1196be89":"import nltk\nnltk.download('stopwords')","57ea8679":"stop_words = set(stopwords.words('english'))\nstop_words.update(['zero','one','two','three','four','five','six','seven','eight','nine','ten','may','also','across','among','beside','however','yet','within'])\nre_stop_words = re.compile(r\"\\b(\" + \"|\".join(stop_words) + \")\\\\W\", re.I)\ndef removeStopWords(sentence):\n    global re_stop_words\n    return re_stop_words.sub(\" \", sentence)","3d3aa718":"temp['article'] = temp['article'].apply(removeStopWords)","f8c90197":"temp.head()","fa10cc77":"stemmer = SnowballStemmer(\"english\")\ndef stemming(sentence):\n    stemSentence = \"\"\n    for word in sentence.split():\n        stem = stemmer.stem(word)\n        stemSentence += stem\n        stemSentence += \" \"\n    stemSentence = stemSentence.strip()\n    return stemSentence","95f7c01b":"temp['article'] = temp['article'].apply(stemming)\ntemp.head()","ee946b54":"x = temp['article']\ny = temp.drop('article', axis=1)","4484ed0f":"from sklearn.feature_extraction.text import TfidfVectorizer\nvectorizer = TfidfVectorizer(strip_accents='unicode', analyzer='word', ngram_range=(1, 3), norm='l1', max_features=8000)\nvectorizer.fit(x)","9a69ce07":"x = vectorizer.transform(x)","a2712497":"from sklearn.model_selection import train_test_split\nX_train_val, x_test, y_train_val, y_test = train_test_split(x, y, random_state=42, test_size=0.15, shuffle=True)","f863c55e":"labels = ['advertisement', 'conflict_of_interest', 'fan_point_of_view', 'press_release']","d60f195d":"train['advertisement'].value_counts()","adb35142":"train['press_release'].value_counts()","80fc6774":"train['conflict_of_interest'].value_counts()","dde1653c":"train['fan_point_of_view'].value_counts()","ac17f266":"x_test.shape","07ce805c":"y_test.shape","cfa1a1b6":"ftr = \"advertisement\"","d520c40a":"kf = KFold(n_splits=5, shuffle=True, random_state = 71)\nparams_rf = {'max_depth': 25,\n         'min_samples_leaf': 1,\n         'min_samples_split': 2,\n         'n_estimators': 1200,\n         'random_state': 42}","10d8ecfb":"import lightgbm as lgb\n\nparams_lgb ={'colsample_bytree': 0.85, \n         'max_depth': 15, \n         'min_split_gain': 0.1, \n         'n_estimators': 200, \n         'num_leaves': 50, \n         'reg_alpha': 1.2, \n         'reg_lambda': 1.2, \n         'subsample': 0.95, \n         'subsample_freq': 20}","4404b7e7":"!pip install catboost","f17d3b3f":"from catboost import CatBoostClassifier\nparams_cb ={}","c4608a37":"params_xgb ={}","89e65204":"Mean_AUC_KNN_CV = np.mean(cross_val_score(RandomForestClassifier(**params_rf), X_train_val, y_train_val[ftr], cv=kf, scoring='roc_auc'))\nMean_Precision_KNN_CV = np.mean(cross_val_score(RandomForestClassifier(**params_rf), X_train_val, y_train_val[ftr], cv=kf, scoring='precision'))\nMean_Recall_KNN_CV = np.mean(cross_val_score(RandomForestClassifier(**params_rf), X_train_val, y_train_val[ftr], cv=kf, scoring='recall'))\n\nprint(\"accuracy {}  :  precision {}  :  recall {}\".format(Mean_AUC_KNN_CV, Mean_Precision_KNN_CV, \n                                                          Mean_Recall_KNN_CV))","356f2ee9":"Mean_AUC_KNN_CV = np.mean(cross_val_score(lgb.LGBMClassifier(**params_lgb), X_train_val, y_train_val, cv=kf, scoring='roc_auc'))\nMean_Precision_KNN_CV = np.mean(cross_val_score(lgb.LGBMClassifier(**params_lgb), X_train_val, y_train_val, cv=kf, scoring='precision'))\nMean_Recall_KNN_CV = np.mean(cross_val_score(lgb.LGBMClassifier(**params_lgb), X_train_val, y_train_val, cv=kf, scoring='recall'))\n\nprint(\"accuracy {}  :  precision {}  :  recall {}\".format(Mean_AUC_KNN_CV, Mean_Precision_KNN_CV, \n                                                          Mean_Recall_KNN_CV))","d332cf56":"Mean_AUC_KNN_CV = np.mean(cross_val_score(CatBoostClassifier(**params_cb), X_train_val, y_train_val, cv=kf, scoring='roc_auc'))\nMean_Precision_KNN_CV = np.mean(cross_val_score(CatBoostClassifier(**params_cb), X_train_val, y_train_val, cv=kf, scoring='precision'))\nMean_Recall_KNN_CV = np.mean(cross_val_score(CatBoostClassifier(**params_cb), X_train_val, y_train_val, cv=kf, scoring='recall'))\n\nprint(\"accuracy {}  :  precision {}  :  recall {}\".format(Mean_AUC_KNN_CV, Mean_Precision_KNN_CV, \n                                                          Mean_Recall_KNN_CV))","42fbdc26":"Mean_AUC_KNN_CV = np.mean(cross_val_score(xgb.XGBClassifier(**params_xgb), X_train_val, y_train_val, cv=kf, scoring='roc_auc'))\nMean_Precision_KNN_CV = np.mean(cross_val_score(xgb.XGBClassifier(**params_xgb), X_train_val, y_train_val, cv=kf, scoring='precision'))\nMean_Recall_KNN_CV = np.mean(cross_val_score(xgb.XGBClassifier(**params_xgb), X_train_val, y_train_val, cv=kf, scoring='recall'))\n\nprint(\"accuracy {}  :  precision {}  :  recall {}\".format(Mean_AUC_KNN_CV, Mean_Precision_KNN_CV, \n                                                          Mean_Recall_KNN_CV))","f1ddb07d":"import xgboost as xgb","74ac4b49":"Log_Model = RandomForestClassifier(**params_rf)\nKNN_Model = lgb.LGBMClassifier(**params_lgb)\n#NB_Model = CatBoostClassifier(**params_cb)\nTree_Model = xgb.XGBClassifier(**params_xgb)\n\nmodel_list = [Log_Model, KNN_Model, Tree_Model]\nmodel_names = [\"log\", \"knn\", \"tree_model\"]\nmodel = list(zip(model_names, model_list))","4617abf2":"ftr = \"advertisement\"","80fde086":"Mean_AUC_ensemble_CV = np.mean(cross_val_score(VotingClassifier(estimators=model,voting='soft',n_jobs=-1), X_train_val, y_train_val[ftr], cv=kf, scoring='roc_auc'))\nMean_AUC_ensemble_CV","9c2c32ab":"Mean_Precision_ensemble_CV = np.mean(cross_val_score(VotingClassifier(estimators=model,voting='soft',n_jobs=-1), X_train_val, y_train_val[ftr], cv=kf, scoring='precision'))\nMean_Precision_ensemble_CV","20a9f82f":"Mean_Recall_ensemble_CV = np.mean(cross_val_score(VotingClassifier(estimators=model,voting='soft',n_jobs=-1), X_train_val, y_train_val[ftr], cv=kf, scoring='recall'))\nMean_Recall_ensemble_CV","c28358f9":"ftr = \"press_release\"","686663eb":"tr = KNeighborsClassifier()\ntr = tr.fit(x, y[ftr])","db262fe7":"from sklearn.metrics import accuracy_score\nprediction = tr.predict(x_test)\naccuracy_score(y_test[ftr], prediction)","37ce064d":"prediction = tr.predict(test)\n\np = 0\n\nfor i in prediction:\n  if i==0:\n    p += 1\np","3dee85c9":"sample[ftr] = prediction","adf3fcfb":"sample[ftr] = prediction","852c5b55":"sample.head()","4b8e64f6":"len(sample)","6a7491ac":"sample.to_csv('\/content\/drive\/MyDrive\/PICT\/News\/res10.csv', index=False)","9133c5bb":"test = pd.read_csv('\/content\/drive\/MyDrive\/PICT\/News\/test (3).csv')\ntest.drop('ids', inplace=True, axis=1)\ntest.head()","da3adaaa":"test['article'] = test['article'].str.lower()\ntest['article'] = test['article'].apply(cleanHtml)\ntest['article'] = test['article'].apply(cleanPunc)\ntest['article'] = test['article'].apply(keepAlpha)","e5ff4480":"test['article'] = test['article'].apply(removeStopWords)\ntest['article'] = test['article'].apply(stemming)","6f8a3806":"test = vectorizer.transform(test['article'])","f0c34592":"train.head()","dbe5acbc":"len(train)","0e8f1329":"len(test)","c6ced54b":"# **OPTION 2**","5bfa214f":"# Credits to dhiraj jha, this is his notebook.","ac0dc7df":"# pre - processing","d78e5290":"# Pre-Processing","d630a26f":"# **OPTION 1**"}}