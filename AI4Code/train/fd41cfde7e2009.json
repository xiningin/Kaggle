{"cell_type":{"55499a4c":"code","cad289a8":"code","f8215684":"code","bf4ae1d5":"code","93b70c4c":"code","5ea61fac":"code","8840a3e6":"code","f6a61860":"code","ce4e3b98":"code","d7159922":"code","6d74a6b1":"code","661fb878":"code","0052d72f":"code","d70d31ea":"code","54d2a2b0":"code","d082f8f9":"code","ecfb6ae4":"code","fcbe97d6":"code","96fceb24":"code","e0d5d46a":"code","bf9f27c1":"code","d4c2ac4a":"code","f5c9fe2f":"code","69ef71e1":"code","6ce11e27":"code","15cc6cb0":"code","6be6aa4c":"code","1e0d64d7":"code","0adffef6":"code","88cefdd6":"code","4ce5683f":"code","a5efed47":"code","f4dfa67b":"code","cfbc617f":"code","b35a6b40":"code","0b9347a1":"code","90332fc0":"code","6345616c":"code","6f912204":"code","70a9f7dc":"code","2a89073a":"code","368b4322":"code","9622683b":"code","9bad74d9":"code","12fec4eb":"code","3661225f":"code","c64cfd27":"code","3c3db3e2":"code","2e5c18c5":"code","3a47041b":"code","ae8692b2":"code","a812d0a7":"code","e9b22ab7":"code","4749acf7":"code","c0a20d6e":"code","e0d9ad83":"code","750a216a":"code","a69962a3":"code","5d6cf6bb":"code","c1f35f9f":"code","874f2151":"code","b9d5f96b":"code","aca79ac3":"code","0d684d4f":"code","fc30738e":"code","c046b50b":"code","55194f62":"code","2c42ed52":"code","b606d883":"code","1de5256a":"code","ff363b20":"code","dc8d9306":"code","fb1ad4de":"markdown","fcab2f29":"markdown","6c8cb071":"markdown","16bb42ab":"markdown","c37b309a":"markdown","27cb981d":"markdown","3a42a767":"markdown","eaa0198d":"markdown","de438cec":"markdown","800dc62d":"markdown","d8b083a1":"markdown","825c3248":"markdown","95f464b8":"markdown","d63298e6":"markdown","27919aac":"markdown","166757da":"markdown","877e1998":"markdown","7bcaa14e":"markdown"},"source":{"55499a4c":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","cad289a8":"# import tensorflow as tf\n# p = tf.config.experimental.list_physical_devices('GPU')\n# tf.config.experimental.set_visible_devices(p[0], 'GPU')","f8215684":"train = pd.read_csv(\"\/kaggle\/input\/data-scientist-salary\/train.csv\")\ntest = pd.read_csv(\"\/kaggle\/input\/data-scientist-salary\/test.csv\")","bf4ae1d5":"train.head()","93b70c4c":"train.shape, test.shape","5ea61fac":"train.nunique()","8840a3e6":"train.info()","f6a61860":"train.isna().sum()","ce4e3b98":"train = train.dropna(subset = [\"key_skills\"])\ndf_train = train[['key_skills', 'job_desig', 'job_description', 'location', 'job_type', 'experience','salary']]\ndf_test = test[['key_skills', 'job_desig', 'job_description', 'job_type', 'experience', 'location']]","d7159922":"df_train.head()","6d74a6b1":"import re\n\ndef clean_skills(skl):\n    skills = str(skl).lower()\n    skills = re.sub('\\...','',skills)\n    skills = re.sub(',','',skills)\n    skills = re.sub(r'\\s+', ' ', skills)\n    return skills\n\ndf_train['skills_cleaned'] = df_train['key_skills'].apply(clean_skills)\ndf_test['skills_cleaned'] = df_test['key_skills'].apply(clean_skills)","661fb878":"df_train.head()","0052d72f":"train.job_description.fillna('missing',inplace = True)\ntest['job_description'].fillna('missing', inplace=True)\n\ndef clean_job_desc(job):\n    job_desc = str(job).lower()\n    job_desc = re.sub(r'[^a-z]', ' ', job_desc)\n    job_desc = re.sub(r'\\s+', ' ', job_desc)\n    return job_desc\n\ndf_train['job_desc_cleaned'] = df_train['job_description'].apply(clean_job_desc)\ndf_test['job_desc_cleaned'] = df_test['job_description'].apply(clean_job_desc)","d70d31ea":"df_train.head()","54d2a2b0":"\ndef clean_location(loc):\n    location = loc.lower()\n    location = re.sub(r'[^a-z]', ' ', location)\n    location = re.sub(r'\\s+', ' ', location)\n    return location\n\ndf_train['loc_cleaned'] = df_train['location'].apply(clean_location)\ndf_test['loc_cleaned'] = df_test['location'].apply(clean_location)","d082f8f9":"train['job_type'].fillna('missingjobtype', inplace=True)\ntrain['job_type'].replace('Analytics', 'analytics', inplace=True)\ntrain['job_type'].replace('Analytic', 'analytics', inplace=True)\ntrain['job_type'].replace('ANALYTICS', 'analytics', inplace=True)\ntrain['job_type'].replace('analytic', 'analytics', inplace=True)\n\ntest['job_type'].fillna('missingjobtype', inplace=True)\ntest['job_type'].replace('Analytics', 'analytics', inplace=True)\ntest['job_type'].replace('Analytic', 'analytics', inplace=True)\ntest['job_type'].replace('ANALYTICS', 'analytics', inplace=True)\ntest['job_type'].replace('analytic', 'analytics', inplace=True)\n\ndf_train['job_type_cleaned'] = train['job_type'] \ndf_test['job_type_cleaned'] = test['job_type']","ecfb6ae4":"df_train.head()","fcbe97d6":"df_train.isna().sum()","96fceb24":"df_train.head()","e0d5d46a":"def min_exp(val):\n    exp = re.sub('-',' ',val)\n    exp = exp.split(\" \")\n    exp = int(exp[0])\n    return exp\n    \ndef max_exp(val):\n    exp = re.sub('-',' ',val)\n    exp = exp.split(' ')\n    exp = int(exp[1])\n    return exp\n    \ndf_train['min_exp'] = df_train['experience'].apply(lambda x : min_exp(x))\ndf_train['max_exp'] = df_train['experience'].apply(lambda x : max_exp(x))\n\ndf_test['min_exp'] = df_test['experience'].apply(lambda x : min_exp(x))\ndf_test['max_exp'] = df_test['experience'].apply(lambda x : max_exp(x))\n        ","bf9f27c1":"df_train.head()","d4c2ac4a":"def clean_job_desig(desig):\n    job_desig = desig.lower()\n    job_desig = re.sub(r'[^a-z]', ' ', job_desig)\n    job_desig = re.sub(r'\\s+', ' ', job_desig)\n    return job_desig\n\ndf_train['desig_cleaned'] = df_train['job_desig'].apply(clean_job_desig)\ndf_test['desig_cleaned'] = df_test['job_desig'].apply(clean_job_desig)","f5c9fe2f":"df_train['merged'] = (df_train['desig_cleaned'] + ' ' + df_train['job_desc_cleaned'] + ' ' + df_train['skills_cleaned']\n                      + ' ' + df_train['job_type_cleaned'])\n\ndf_test['merged'] = (df_test['desig_cleaned'] + ' ' + df_test['job_desc_cleaned'] + ' ' + df_test['skills_cleaned']\n                     + ' ' + df_test['job_type_cleaned'])","69ef71e1":"df_train.head()","6ce11e27":"data_train  = df_train[['merged', 'loc_cleaned', 'min_exp', 'max_exp']] \ndata_test = df_test[['merged', 'loc_cleaned', 'min_exp', 'max_exp']] ","15cc6cb0":"data_train.head()","6be6aa4c":"data_test.head()","1e0d64d7":"data_train = data_train.rename(columns = {'merged':'emp_info'},inplace = False)","0adffef6":"data_test = data_test.rename(columns = {'merged':'emp_info'},inplace = False)","88cefdd6":"def min_sal(sal):\n    val = str(sal).split(\"to\")\n    return val[0]\ndef max_sal(sal):\n    val = str(sal).split(\"to\")\n    return val[1]\n\ntarget = pd.DataFrame()\ntarget[\"min_sal\"] = df_train[\"salary\"].apply(lambda x: min_sal(x))\ntarget[\"max_sal\"] = df_train[\"salary\"].apply(lambda x: max_sal(x))\ntarget1 = target.min_sal\ntarget2 = target.max_sal","4ce5683f":"target.head()","a5efed47":"import matplotlib.pyplot as plt\nimport seaborn as sns\n\n","f4dfa67b":"def get_ax(rows = 1,cols = 2,size = 7):\n    fig, ax = plt.subplots(rows, cols, figsize=(size*cols, size*rows))\n    return fig,ax","cfbc617f":"fig,ax = get_ax()\nsns.distplot(data_train[\"emp_info\"].str.len(),ax = ax[0])\nsns.distplot(data_test[\"emp_info\"].str.len(),ax = ax[1])","b35a6b40":"data_train.nunique()","0b9347a1":"fig,ax = get_ax()\n\nsns.distplot(data_train.min_exp,ax = ax[0])\nsns.distplot(data_train.max_exp,ax = ax[0])\n\n\nsns.distplot(data_test.min_exp,ax = ax[1])\nsns.distplot(data_test.max_exp,ax = ax[1])","90332fc0":"sns.distplot(data_train.max_exp-data_train.min_exp)","6345616c":"\nfrom wordcloud import WordCloud\ndef wordcloud(data):\n    wordcloud = WordCloud(background_color = 'Black',\n                         max_words = 50,\n                         max_font_size = 40,\n                         scale = 5,\n                         random_state = 5).generate(str(data))\n    fig = plt.figure(1, figsize=(10,10))\n    plt.imshow(wordcloud)\n    plt.axis(\"off\")\n    plt.show()\nwordcloud(data_train[\"emp_info\"]) ","6f912204":"from wordcloud import WordCloud\ndef wordcloud(data):\n    wordcloud = WordCloud(background_color = 'Black',\n                         max_words = 50,\n                         max_font_size = 40,\n                         scale = 5,\n                         random_state = 5).generate(str(data))\n    fig = plt.figure(1, figsize=(10,10))\n    plt.imshow(wordcloud)\n    plt.axis(\"off\")\n    plt.show()\nwordcloud(data_test[\"emp_info\"]) ","70a9f7dc":"data_train.head()","2a89073a":"from sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\ntrain['salary'] = le.fit_transform(train['salary'])","368b4322":"from sklearn.model_selection import train_test_split\n\nX_train, X_cv, y_train, y_cv = train_test_split(\n    data_train,train['salary'], test_size=0.20, \n    stratify=train['salary'], random_state=75)","9622683b":"print('No. of sample texts X_train: ', len(X_train))\nprint('No. of sample texts X_cv   : ', len(X_cv))\n","9bad74d9":"X_train_merged = X_train['emp_info']\nX_train_loc = X_train['loc_cleaned']\n\nX_cv_merged = X_cv['emp_info']\nX_cv_loc = X_cv['loc_cleaned']","12fec4eb":"from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\ntf1 = TfidfVectorizer(min_df=3, token_pattern=r'\\w{3,}', ngram_range=(1,3), max_df=0.9)\ntf2 = TfidfVectorizer(min_df=2, token_pattern=r'\\w{3,}')\n\nX_train_merged = tf1.fit_transform(X_train_merged)\nX_train_loc = tf2.fit_transform(X_train_loc)\n\nX_cv_merged = tf1.transform(X_cv_merged)\nX_cv_loc = tf2.transform(X_cv_loc)\n# X_cv_merged","3661225f":"from scipy import sparse\nfrom sklearn.preprocessing import StandardScaler\n\nsc1 = StandardScaler()\nX_train_MinExp = sc1.fit_transform(np.array(X_train['min_exp']).reshape(-1,1))\nX_cv_MinExp = sc1.transform(np.array(X_cv['min_exp']).reshape(-1,1))\nX_train_MinExp = sparse.csr_matrix(X_train_MinExp)\nX_cv_MinExp = sparse.csr_matrix(X_cv_MinExp)\n\nsc2 = StandardScaler()\nX_train_MaxExp = sc2.fit_transform(np.array(X_train['max_exp']).reshape(-1,1))\nX_cv_MaxExp = sc2.transform(np.array(X_cv['max_exp']).reshape(-1,1))\nX_train_MaxExp = sparse.csr_matrix(X_train_MaxExp)\nX_cv_MaxExp = sparse.csr_matrix(X_cv_MaxExp)","c64cfd27":"from scipy.sparse import hstack, csr_matrix\n\nmerged_train = hstack((X_train_merged, X_train_loc, X_train_MinExp, X_train_MaxExp))\nmerged_cv  = hstack((X_cv_merged, X_cv_loc, X_cv_MinExp, X_cv_MaxExp))","3c3db3e2":"merged_train.shape, merged_cv.shape\n","2e5c18c5":"from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n","3a47041b":"import lightgbm as lgb\ntrain_data = lgb.Dataset(merged_train, label=y_train)\ntest_data = lgb.Dataset(merged_cv, label=y_cv)","ae8692b2":"param = {'objective': 'multiclass',\n         'num_iterations': 80,\n         'learning_rate': 0.04,  \n         'num_leaves': 23,\n         'max_depth': 7, \n         'min_data_in_leaf': 28, \n         'max_bin': 10, \n         'min_data_in_bin': 3,   \n         'num_class': 6,\n         'metric': 'multi_logloss'\n         }","a812d0a7":"lgbm = lgb.train(params=param,\n                 train_set=train_data,\n                 num_boost_round=100,\n                 valid_sets=[test_data])\n\ny_pred_class = lgbm.predict(merged_cv)","e9b22ab7":"X_train_merged = data_train['emp_info']\nX_train_loc = data_train['loc_cleaned']\n\nX_test_merged = data_test['emp_info']\nX_test_loc = data_test['loc_cleaned']\n\ny_train = train['salary']","4749acf7":"tf1 = TfidfVectorizer(min_df=3, token_pattern=r'\\w{3,}', ngram_range=(1,3))\ntf2 = TfidfVectorizer(min_df=2, token_pattern=r'\\w{3,}')\n\nX_train_merged = tf1.fit_transform(X_train_merged)\nX_train_loc = tf2.fit_transform(X_train_loc)\n\nX_test_merged = tf1.transform(X_test_merged)\nX_test_loc = tf2.transform(X_test_loc)","c0a20d6e":"from scipy import sparse\nfrom sklearn.preprocessing import StandardScaler\n\nsc1 = StandardScaler()\nX_train_MinExp = sc1.fit_transform(np.array(df_train['min_exp']).reshape(-1,1))\nX_test_MinExp = sc1.transform(np.array(df_test['min_exp']).reshape(-1,1))\nX_train_MinExp = sparse.csr_matrix(X_train_MinExp)\nX_test_MinExp = sparse.csr_matrix(X_test_MinExp)\n\nsc2 = StandardScaler()\nX_train_MaxExp = sc2.fit_transform(np.array(df_train['max_exp']).reshape(-1,1))\nX_test_MaxExp = sc2.transform(np.array(df_test['max_exp']).reshape(-1,1))\nX_train_MaxExp = sparse.csr_matrix(X_train_MaxExp)\nX_test_MaxExp = sparse.csr_matrix(X_test_MaxExp)","e0d9ad83":"merged_train = hstack((X_train_merged, X_train_loc, X_train_MinExp, X_train_MaxExp))\nmerged_test  = hstack((X_test_merged, X_test_loc, X_test_MinExp, X_test_MaxExp))","750a216a":"import lightgbm as lgb\ntrain_data = lgb.Dataset(merged_train, label=y_train)\n\nparam = {'objective': 'multiclass',\n         'num_iterations': 80,\n         'learning_rate': 0.04, \n         'num_leaves': 23,\n         'max_depth': 7, \n         'min_data_in_leaf': 28, \n         'max_bin': 10, \n         'min_data_in_bin': 3,   \n         'num_class': 6,\n         'metric': 'multi_logloss'\n         }\n\nlgbm = lgb.train(params=param, \n                 train_set=train_data)\n\npredictions = lgbm.predict(merged_test)\n\ny_pred_class = []\nfor x in predictions:\n    y_pred_class.append(np.argmax(x))\n\ny_pred_class = le.inverse_transform(y_pred_class)","a69962a3":"df_sub = pd.DataFrame(data=y_pred_class, columns=['salary'])\n","5d6cf6bb":"df_sub","c1f35f9f":"df_sub.to_csv(\"sub.csv\",index = False)","874f2151":"def min_sal(sal):\n    val = str(sal).split(\"to\")\n    return val[0]\ndef max_sal(sal):\n    val = str(sal).split(\"to\")\n    return val[1]\n\nminsal = df_sub[\"salary\"].apply(lambda x: min_sal(x))\nmax_sal = df_sub[\"salary\"].apply(lambda x: max_sal(x))\n","b9d5f96b":"X = pd.DataFrame({\"min_sal\":minsal,\n                  \"max_sal\":max_sal})","aca79ac3":"fig,ax = plt.subplots(1,1 ,figsize = (14,8))\nsns.distplot(minsal)\nsns.distplot(max_sal)","0d684d4f":"test1 = pd.read_csv(\"\/kaggle\/input\/data-scientist-salary\/test.csv\")","fc30738e":"final = pd.concat([test1,X],axis=1)","c046b50b":"final.head()","55194f62":"def min_exp(val):\n    exp = re.sub('-',' ',val)\n    exp = exp.split(\" \")\n    exp = int(exp[0])\n    return exp\n    \ndef max_exp(val):\n    exp = re.sub('-',' ',val)\n    exp = exp.split(' ')\n    exp = int(exp[1])\n    return exp\n    \nfinal['min_exp'] = final['experience'].apply(lambda x : min_exp(x))\nfinal['max_exp'] = final['experience'].apply(lambda x : max_exp(x))\n\n# df_test['min_exp'] = df_test['experience'].apply(lambda x : min_exp(x))\n# df_test['max_exp'] = df_test['experience'].apply(lambda x : max_exp(x))\n        ","2c42ed52":"final.head()","b606d883":"labels = [\"min_exp\",\"max_exp\",\"min_sal\",\"max_sal\"]\nsns.pairplot(final[labels])","1de5256a":"# def min_exp(val):\n#     exp = re.sub('-',' ',val)\n#     exp = exp.split(\" \")\n#     exp = int(exp[0])\n#     return exp\n    \n# def max_exp(val):\n#     exp = re.sub('-',' ',val)\n#     exp = exp.split(' ')\n#     exp = int(exp[1])\n#     return exp\ncol = final.loc[: , \"min_sal\":\"max_sal\"]\nfinal['salary_mean'] = col.mean(axis=1)\n\ncols = final.loc[: , \"min_exp\":\"max_exp\"]\nfinal['exp_mean'] = cols.mean(axis=1)\n","ff363b20":"final.head()","dc8d9306":"sns.scatterplot(x = final[\"exp_mean\"],y = final[\"salary_mean\"])","fb1ad4de":"You can apply any Hyperparamter tuning tech to obtain the optimal parameters but I have chosen according to my past tuning parameters...Do try to model with different set of params.","fcab2f29":"Cleaning Job_description column feature","6c8cb071":"# EDA","16bb42ab":"Distribution of the min experience and maximum experience. ","c37b309a":"Transforming the Jobs column with consistent values having same meaning.","27cb981d":"Filling nan values and cleaning Job Description column(Removing punctuations,stopwords,lowercasing text,etc)","3a42a767":"Cleaning locations columns","eaa0198d":"Making a new aggregate column with clean feature values with text.","de438cec":"Cleaning key_skills column(Removing punctuations,stopwords,lowercasing text)","800dc62d":"**PREDICTING SALARIES OF DATA SCIENTISTS**\n* Data scientist is the sexiest job in the world. How many times have you heard that? Analytics India Annual Salary Study which aims to understand a wide range of trends data science says that the median analytics salary in India for the year 2017 is INR 12.7 Lakhs across all experience level and skill sets. So given the job description and other key information can you predict the range of salary of the job posting? What kind of factors influence the salary of a data scientist? The study also says that in the world of analytics, Mumbai is the highest paymaster at almost 13.3 Lakhs per annum, followed by Bengaluru at 12.5 Lakhs. The industry of the data scientist can also influence the salary. Telecom industry pays the highest median salaries to its analytics professionals at 18.6 Lakhs. What are you waiting for, solve the problem by predicting how much a data scientist or analytics professional will be paid by analysing the data given. Bonus Tip: You can analyse the data and get key insights for your career as well. The best data scientists and machine learning engineers will be given awesome prizes at the end of hackathon. Share this hackathon with a colleague who may be interested in mining the dataset for insights and make great predictions. Data The dataset is based on salary and job postings in India across the internet. The train and the test data consists of attributes mentioned below. The rows of train dataset has rich amount of information regarding the job posting such as name of the designation and key skills required for the job. The training data and test data comprise of 19802 samples and of 6601 samples each. This is a dataset which has been collected over some time to gather relevant analytics jobs posting over the years. Features Name of the company (Encoded) Years of experience Job description Job designation Job Type Key skills Location Salary in Rupees Lakhs(To be predicted) Problem Statement Based on the given attributes and salary information, build a robust machine learning model that predicts the salary range of the salary post.\n","d8b083a1":"Extracting minimum and maximum experience from the Experience column","825c3248":"Creating separate column values to encode them accordingly.","95f464b8":"Number of nan values in the feature columns","d63298e6":"# EDA AND FEATURE ENGINEERING","27919aac":"In this kernel I will be standardizing only the experience columns but please try to standardize all the numerical features and convert them into sparsematrix then stack them horizontally as proceeded further for better results.","166757da":"Final Feature Selection to obtain the final dataframe which will be used for modelling.","877e1998":"Encoding text column(merged) using TF-IDF vectorizer","7bcaa14e":"# MODELLING"}}