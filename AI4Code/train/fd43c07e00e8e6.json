{"cell_type":{"e0b7a1ad":"code","e2b98cff":"code","7e8b81d6":"code","c2592aad":"code","a8e1f218":"code","af941dfb":"code","e058432b":"code","d634b193":"code","9017d8f6":"code","b2d3c9ba":"code","7a2ea39f":"code","e6ef8660":"code","0d5df843":"code","63e770dc":"code","296ede2e":"code","ad8beec0":"code","c8cbb359":"code","61f6d080":"code","a082b7c7":"code","70832e75":"code","19819718":"code","45887423":"code","0a1b5aa4":"code","41414074":"code","c9370641":"code","5505c39e":"code","adaa1506":"code","04f16343":"code","a6e19c94":"code","22368091":"code","73291ec4":"code","a6a4c1ba":"code","07ba4c81":"code","ecbdd33b":"code","f16be979":"code","a73d4223":"code","d017bb79":"code","9554120f":"code","373c8c85":"code","4ec52d0e":"code","8b862795":"code","7c7476da":"code","c74976fc":"markdown","d634e95e":"markdown","926ff371":"markdown","6435ece4":"markdown","2747329c":"markdown","638c6c8e":"markdown","8c168c95":"markdown","c43013f0":"markdown","ca5c09f3":"markdown","07491e70":"markdown","c5182d4d":"markdown","60a51d65":"markdown","f1b94409":"markdown","372c2ffd":"markdown","8f3603e1":"markdown"},"source":{"e0b7a1ad":"!pip install sadedegel","e2b98cff":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nimport itertools\n\nfrom collections import Counter, defaultdict\nimport nltk\nstop_word_list = nltk.corpus.stopwords.words('turkish')\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly.express as px\nfrom matplotlib.ticker import MaxNLocator\nimport matplotlib.gridspec as gridspec\nimport matplotlib.patches as mpatches\n\nfrom tqdm import tqdm\nfrom tqdm._tqdm_notebook import tqdm_notebook\ntqdm_notebook.pandas()\n\n\nplt.rcParams['ytick.labelsize'] = 18\nplt.style.use('ggplot')","7e8b81d6":"from sadedegel.extension.sklearn import TfidfVectorizer, Text2Doc, HashVectorizer\nfrom sadedegel.bblock.word_tokenizer_helper import puncts\nfrom sadedegel.bblock.util import tr_lower\nfrom sadedegel import Doc, Token","c2592aad":"import sklearn\nfrom sklearn.utils import shuffle\nfrom sklearn.metrics import accuracy_score, f1_score,make_scorer,classification_report\nfrom lightgbm import LGBMClassifier\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.ensemble import RandomForestClassifier\nfrom scipy.sparse import csr_matrix\nfrom sklearn.manifold import TSNE\nfrom sklearn.decomposition import PCA\nfrom sklearn.feature_extraction.text import CountVectorizer\n\nfrom functools import partial\nimport optuna\n\nimport string\nimport re\n\n\nfrom nltk.probability import FreqDist\nfrom wordcloud import WordCloud, ImageColorGenerator\nimport random","a8e1f218":"df = pd.read_csv('..\/input\/tweet-sentiment-tr\/sadedegel_tweet_sentiment.csv',names=['id','document','sentiment'], header=0)\ndf = df[['document','sentiment']]","af941dfb":"diag = df.sentiment.value_counts()\nfig = px.pie(diag,\n             values='sentiment',\n             names=diag.index,             \n             hole=.4,title=\"Distribution of the Sentiments\",color_discrete_sequence=[\"green\", \"red\"],)\nfig.update_traces(textinfo='percent+label', pull=0.05)\nfig.show()\n\n","e058432b":"# Creating a new feature for the visualization.\n\ndf['Character Count'] = df['document'].apply(lambda x: len(str(x)))\n\n\ndef plot_dist3(df, feature, title, color):\n    # Creating a customized chart. and giving in figsize and everything.\n    fig = plt.figure(constrained_layout=True, figsize=(18, 8))\n    # Creating a grid of 3 cols and 3 rows.\n    grid = gridspec.GridSpec(ncols=3, nrows=2, figure=fig)\n\n    # Customizing the histogram grid.\n    ax1 = fig.add_subplot(grid[0, :2])\n    # Set the title.\n    ax1.set_title('Histogram')\n    # plot the histogram.\n    sns.distplot(df.loc[:, feature],\n                 hist=True,\n                 kde=True,\n                 ax=ax1,\n                 color=color)\n    ax1.set(ylabel='Frequency')\n    ax1.xaxis.set_major_locator(MaxNLocator(nbins=20))\n\n    # Customizing the ecdf_plot.\n    ax2 = fig.add_subplot(grid[1, :2])\n    # Set the title.\n    ax2.set_title('Empirical CDF')\n    # Plotting the ecdf_Plot.\n    sns.distplot(df.loc[:, feature],\n                 ax=ax2,\n                 kde_kws={'cumulative': True},\n                 hist_kws={'cumulative': True},\n                 color=color)\n    ax2.xaxis.set_major_locator(MaxNLocator(nbins=20))\n    ax2.set(ylabel='Cumulative Probability')\n\n    # Customizing the Box Plot.\n    ax3 = fig.add_subplot(grid[:, 2])\n    # Set title.\n    ax3.set_title('Box Plot')\n    # Plotting the box plot.\n    sns.boxplot(y=feature, data=df, orient='v', ax=ax3, color=color)\n    ax3.yaxis.set_major_locator(MaxNLocator(nbins=25))\n\n    plt.suptitle(f'{title}', fontsize=24)","d634b193":"plot_dist3(df[df['sentiment'] == 'POSITIVE'], 'Character Count',\n           'Character Distribution of Positive Tweets','Green')","9017d8f6":"plot_dist3(df[df['sentiment'] == 'NEGATIVE'], 'Character Count',\n           'Character Distribution of Negative Tweets', 'Red')","b2d3c9ba":"def plot_word_number_histogram(textneg, textpos):\n    \n    \"\"\"A function for comparing word counts\"\"\"\n\n    fig, axes = plt.subplots(figsize=(18, 6), sharey=True)\n    sns.kdeplot(textneg.str.split().map(lambda x: len(x)),shade=True, color='Red')\n    sns.kdeplot(textpos.str.split().map(lambda x: len(x)),shade=True, color='Green')\n    \n    plt.xlabel('Word Count')\n    plt.ylabel('Frequency')\n    plt.legend(['Negative','Positive'])\n    fig.suptitle('Words Per Tweet', fontsize=24, va='baseline')\n    \n    fig.tight_layout()","7a2ea39f":"plot_word_number_histogram(df[df['sentiment'] == 'NEGATIVE']['document'],\n                           df[df['sentiment'] == 'POSITIVE']['document'])","e6ef8660":"def remove_digit(text):    \n    return re.sub(r'\\d+', '', text)\n\ndef remove_URL(text):\n    url = re.compile(r'https?:\/\/\\S+|www\\.\\S+')\n    return url.sub(r'', text)\n\n\ndef remove_emoji(text):\n    emoji_pattern = re.compile(\n        '['\n        u'\\U0001F600-\\U0001F64F'  # emoticons\n        u'\\U0001F300-\\U0001F5FF'  # symbols & pictographs\n        u'\\U0001F680-\\U0001F6FF'  # transport & map symbols\n        u'\\U0001F1E0-\\U0001F1FF'  # flags (iOS)\n        u'\\U00002702-\\U000027B0'\n        u'\\U000024C2-\\U0001F251'\n        ']+',\n        flags=re.UNICODE)\n    return emoji_pattern.sub(r'', text)\n\n\ndef remove_html(text):\n    html = re.compile(r'<.*?>|&([a-z0-9]+|#[0-9]{1,6}|#x[0-9a-f]{1,6});')\n    return re.sub(html, '', text)\n\n\ndef remove_punct(text):\n    table = str.maketrans('', '', string.punctuation)\n    return text.translate(table)\n\n# Applying helper functions\n\ndf['document'] = df['document'].progress_apply(lambda x: remove_URL(x))\ndf['document'] = df['document'].progress_apply(lambda x: remove_emoji(x))\ndf['document'] = df['document'].progress_apply(lambda x: remove_digit(x))\ndf[\"document\"] = df[\"document\"].progress_apply(lambda x: \" \".join(x.split()))","0d5df843":"corpus = []\ndf_pos = df[df['sentiment']=='POSITIVE']\n\nfor text in tqdm(df_pos['document'].values):\n    d = Doc(str(text))\n    t = [i.tokens for i in d]\n    corpus.append(list(itertools.chain.from_iterable(t)))\ncorpus=list(itertools.chain.from_iterable(corpus))\n\ncorpus_pos=[tr_lower(str(token)) for token in corpus if str(token)]\ncorpus_pos = [token for token in corpus_pos if token not in stop_word_list+['bir','kadar','var','yok','bi','mi']]\n\n\ncounter = Counter(corpus_pos)\nmost = counter.most_common()\nx, y = [], []\nfor word, count in most[:15]:\n    x.append(word)\n    y.append(count)\n\n    \nfig, ax = plt.subplots( figsize=(22, 8))\nsns.barplot(x=y, y=x, palette='Greens_r')\n\nplt.suptitle('Total Positive Word Counts', fontsize='large')\nplt.tight_layout()","63e770dc":"corpus = []\ndf_pos = df[df['sentiment']=='NEGATIVE']\n\nfor text in tqdm(df_pos['document'].values):\n    d = Doc(str(text))\n    t = [i.tokens for i in d]\n    corpus.append(list(itertools.chain.from_iterable(t)))\ncorpus=list(itertools.chain.from_iterable(corpus))\n\ncorpus_neg=[tr_lower(str(token)) for token in corpus if str(token)]\ncorpus_neg = [token for token in corpus_neg if token not in stop_word_list+['bir','kadar','var','yok','bi','mi']]\n\n\n\ncounter = Counter(corpus_neg)\nmost = counter.most_common()\nx, y = [], []\nfor word, count in most[:15]:\n    x.append(word)\n    y.append(count)\n\n    \nfig, ax = plt.subplots(figsize=(22, 8))\nsns.barplot(x=y, y=x, palette='Reds_r')\n\nlabels = [item.get_text() for item in ax.get_yticklabels()]\n\nfor i in range(15):\n    if i in[3,5,7,8,14]:\n        labels[i] = 'Censored :)'\n    else:\n        pass\n\n\nax.set_yticklabels(labels)\n\nplt.suptitle('Total Negative Word Counts', fontsize='large')\nplt.tight_layout()","296ede2e":"lis = [\n    df[df['sentiment'] == 'NEGATIVE']['document'],\n    df[df['sentiment'] == 'POSITIVE']['document'],\n]\ndef ngrams(n, title):\n    \"\"\"A Function to plot most common ngrams\"\"\"\n    fig, axes = plt.subplots(1, 2, figsize=(18, 8))\n    axes = axes.flatten()\n    sent = 0\n    for i, j in zip(lis, axes):\n\n        new = i.str.split(' ')\n        new = new.values.tolist()\n        corpus = [word for i in new for word in i]\n\n        def _get_top_ngram(corpus, n=None):\n            #getting top ngrams\n            vec = CountVectorizer(ngram_range=(n, n),\n                                  max_df=0.9,\n                                  strip_accents='unicode').fit(corpus)\n            bag_of_words = vec.transform(corpus)\n            sum_words = bag_of_words.sum(axis=0)\n            words_freq = [(word, sum_words[0, idx])\n                          for word, idx in vec.vocabulary_.items()]\n            words_freq = sorted(words_freq, key=lambda x: x[1], reverse=True)\n            return words_freq[:15]\n\n        top_n_bigrams = _get_top_ngram(i, n)[:15]\n        x, y = map(list, zip(*top_n_bigrams))\n        if sent == 0:\n            sns.barplot(x=y, y=x, palette='Reds_r', ax=j)\n            sent += 1\n        else:\n            sns.barplot(x=y, y=x, palette='Greens_r', ax=j)\n        \n        axes[0].set_title('NEGATIVE')\n        axes[1].set_title('POSITIVE')\n        axes[0].set_xlabel('Count')\n        axes[0].set_ylabel('Words')\n        axes[1].set_xlabel('Count')\n        axes[1].set_ylabel('Words')\n        \n        labels = [item.get_text() for item in axes[0].get_yticklabels()]\n        if n==3:\n            for i in range(15):\n                if i in[0,1,2,3]:\n                    pass\n                else:\n                    labels[i] = 'Censored :)'\n        else:\n            for i in range(15):\n                if i in[2,4,6,8,9,11]:\n                    pass\n                else:\n                    labels[i] = 'Censored :)'\n\n        axes[0].set_yticklabels(labels)\n\n        fig.suptitle(title, fontsize=24, va='baseline')\n        plt.tight_layout()","ad8beec0":"ngrams(2, 'Most Common Bigrams')","c8cbb359":"ngrams(3, 'Most Common Trigrams')","61f6d080":"# Make worldcloud\ndef plot_wordcloud(text, title, title_size):\n    all_tweets = str(FreqDist(text).most_common(150))\n    \n\n    wordcloud = WordCloud(\n                          max_words=1500,\n                          max_font_size=350, random_state=42,\n                          width=2000, height=1000,\n                          colormap = \"twilight\")\n    wordcloud.generate(all_tweets)\n\n    # Plot\n    plt.figure(figsize = (16, 8))\n    plt.imshow(wordcloud, interpolation='bilinear')\n    plt.title(title,\n              fontdict={\n                  'size': title_size,\n                  'verticalalignment': 'bottom'\n              })\n    plt.axis(\"off\")\n    plt.show()","a082b7c7":"plot_wordcloud(corpus_pos,\n               'POSITIVE WORD CLOUD',\n               title_size=30)","70832e75":"plot_wordcloud(corpus_neg,\n               'NEGATIVE WORD CLOUD',\n               title_size=30)","19819718":"pipeline=Pipeline(\n        [('text2doc', Text2Doc('icu')),\n         ('hash', HashVectorizer(n_features=1063288,alternate_sign=True)),\n         ('logreg',LogisticRegression(C=1.001))\n         ]\n    )","45887423":"X_train, X_test, y_train, y_test = train_test_split(df.document, df.sentiment, test_size=0.2, random_state=42, stratify=df.sentiment)","0a1b5aa4":"txt = Text2Doc('icu')\ndocs = txt.fit_transform(X_train)","41414074":"tfidf = TfidfVectorizer()\nvector=tfidf.fit_transform(docs)\nvector_df=pd.DataFrame.sparse.from_spmatrix(vector)","c9370641":"vector_df.sample(5, random_state=42)","5505c39e":"pca = PCA(3)\ng=pca.fit_transform(vector_df.values)","adaa1506":"import plotly.express as px\nfig = px.scatter_3d(x=g[:,0],y=g[:,1],z=g[:,2],\n              labels=y_train.values,color=y_train.values,size=np.abs(vector_df.sum(axis=1)).values, opacity=0.8)\n\nfig.update_layout(margin=dict(l=0, r=0, b=0, t=0))\nfig.show()","04f16343":"itemindex = np.where((g[:, 0] >= 10)|(g[:, 1] >= 10)|(g[:, 2] >= 5))\ng=np.delete(g, itemindex, axis=0)\ny_train.drop(y_train.index[itemindex], inplace=True)\nX_train.drop(X_train.index[itemindex], inplace=True)\nvector_df.drop(vector_df.index[itemindex], inplace=True)\n\nfig = px.scatter_3d(x=g[:,0],y=g[:,1],z=g[:,2],\n              labels=y_train.values,color=y_train.values,size=np.abs(vector_df.sum(axis=1)).values, opacity=0.8)\n\nfig.update_layout(margin=dict(l=0, r=0, b=0, t=0))\nfig.show()","a6e19c94":"y_train=y_train.replace(['NEGATIVE','POSITIVE'],[0,1])\ny_test=y_test.replace(['NEGATIVE','POSITIVE'],[0,1])","22368091":"pipeline.fit(X_train, y_train)","73291ec4":"y_pred = pipeline.predict(X_test)","a6a4c1ba":"y_test=y_test.replace([0,1],['NEGATIVE','POSITIVE'])\ny_pred=pd.Series(y_pred)\ny_pred=y_pred.replace([0,1], ['NEGATIVE','POSITIVE'])","07ba4c81":"print(classification_report(y_pred, y_test))","ecbdd33b":"def objective_partial(trial, data=X_train[:1500], label=y_train[:1500]):\n    \n    vectorizer_name = trial.suggest_categorical('vectorizer', ['TfidfVectorizer', 'HashVectorizer'])\n    classifier_name = trial.suggest_categorical('classifier', ['SVC', 'LogReg'])\n    \n    if vectorizer_name == 'TfidfVectorizer':\n        tf_m = trial.suggest_categorical(\"tf_method\", [\"binary\", \"raw\", \"freq\", \"log_norm\"])\n        idf_m = trial.suggest_categorical(\"idf_method\", [\"smooth\", \"probabilistic\"])\n        tf_p = trial.suggest_categorical(\"drop_punct\", [True, False])\n        tf_l = trial.suggest_categorical(\"lowercase\", [True, False])\n        vectorizer = TfidfVectorizer(tf_method=tf_m, idf_method=idf_m, drop_punct=tf_p,\n                                lowercase=tf_l, show_progress=False)\n    else:\n        alternate_sign = trial.suggest_categorical(\"alternate_sign\", [True, False])\n        n_features = trial.suggest_int('n_features', 500000,2000000, 1000)\n        vectorizer = HashVectorizer(n_features=n_features,alternate_sign=alternate_sign)\n        \n    t = Text2Doc(tokenizer='icu')\n    n= t.transform(data, label)\n    X = vectorizer.transform(n)\n    y = label\n\n    if classifier_name == 'LogReg':\n        c_log = trial.suggest_float('c_log', 1e-4,20)\n        fit_intercept = trial.suggest_categorical(\"fit_intercept\", [True, False])\n        classifier = LogisticRegression(C=c_log, fit_intercept = fit_intercept, random_state=42)\n    else:\n        c_svm = trial.suggest_float('c_svm', 1e-4,15)\n        shrinking = trial.suggest_categorical(\"shrinking\", [True, False])\n        classifier = SVC(C=c_svm, shrinking = shrinking, random_state=42)\n\n    \n    \n    f1_macro = make_scorer(f1_score, average=\"macro\")\n    score = sklearn.model_selection.cross_val_score(classifier, X, y, n_jobs=-1, cv=3,\n                                                        scoring=f1_macro)\n    return score.mean()\n\ndef driver():\n    objective = partial(objective_partial)\n    study = optuna.create_study(direction=\"maximize\")\n    study.optimize(objective, n_trials=5)    \n    for key, value in study.best_trial.params.items():\n        print(f\"    {key}: {value}\")\n","f16be979":"if __name__ == \"__main__\":\n    driver()","a73d4223":"bert_df = pd.read_csv('..\/input\/tweet-sentiment-tr\/tweet_sentiment_embeddings.csv')\nbert_df.head()","d017bb79":"X_train, X_test, y_train, y_test = train_test_split(bert_df.iloc[:,1:], df.sentiment, test_size=0.2, random_state=42, stratify=df.sentiment)","9554120f":"y_train=y_train.replace(['NEGATIVE','POSITIVE'],[0,1])\ny_test=y_test.replace(['NEGATIVE','POSITIVE'],[0,1])","373c8c85":"y_train.drop(y_train.index[itemindex], inplace=True)\nX_train.drop(X_train.index[itemindex], inplace=True)","4ec52d0e":"logreg = LogisticRegression()\nlogreg.fit(X_train,y_train)","8b862795":"bert_pred = logreg.predict(X_test)","7c7476da":"print(classification_report(bert_pred, y_test))","c74976fc":" <header>\n  <h1>Thank you for joining us for this demo! :)<\/h1>\n<\/header> \n\n<img src=\"https:\/\/i.pinimg.com\/originals\/fd\/6b\/8e\/fd6b8e8e5cc2e65dd4c2d792696f0fc8.jpg\">\n<div style=\"clear:left\">\n\n","d634e95e":"\n# Train-Test Splitting\n\nIn this part we're taking 20% of the data for testing how is our model performing on a data which it has never seen before so we can test our model's generalization capability","926ff371":"# Visualizing Most Common Words Based on the Sentiment\n\nIn this part we count the tokens and visualize the most occuring ones in our total corpus. It's a good demo for showing our text preprocessing results visually...\n","6435ece4":"# Basic Text Preprocessing\n\nIn this part we're going to remove digits, url's, emojis, html tags and punctuations for cleaner texts before we tokenize and vectorize them. We also removed extra blank spaces we noticed while inspecting the text itself. Then we shuffle the rows for adding extra randomness...\n","2747329c":"\n# Training and Making Predictions with the Model\n\nHere we fit our pipeline on the preprocessed text and let the model train\/learn from them. At the end we make predictions based on test data and check our model's performance on several metrics.\n","638c6c8e":"\n# Hyperparameter Optimization Using Optuna\n\nIn this part we're going to set some search space for Optuna trials. Optuna going to use bayesian algorithms to reduce this space for us based on previous results. In each trial it learns something and uses that knowledge for next step...\n","8c168c95":"# **Introduction**\n\nIn this notebook we're going to follow usual NLP workflow using some popular libraries including **Sadedegel**, adapted on smart cities approach. Big cities comes with big problems including huge amount of raw text data which nearly impossible or too expensive to analyze by humans. But with the help of machine learning we can get some meaningful results to solve these problems.\n\n### In this notebook we're going to see:\n- Reading and inspecting text datasets,\n- Exploring the data\n- Visualizing patterns about the data\n- Text preprocessing\n- Wordclouds\n- Preparing the data for machine learning algorithms with these steps;\n * Tokenizing text using sadedegel's 'ICU' tokenizer.\n * Vectorizing tokens using sadedegel's 'HashVectorizer' and 'TfidfVectorizer'\n- Train-Test splitting\n- Dimensionality reduction and 3D scatterplots\n- Outlier removing\n- Supervised classification training by using logistic regression\n- Hyperparameter optimization by using bayesian optimization (Optuna)\n- Simple example with BERT embeddings","c43013f0":"# Distribution of the Labels\n\nHere we inspect our target variables and visalize them using pie chart by plotly package. We have pretty balanced data which is good...\n","ca5c09f3":"# Installing and Loading Packages\n\nHere we install sadedegel package using pip installer then load some needed packages which we're going to use in the process. Also setting some global options...\n","07491e70":"\n# Creating the Pipeline for Machine Learning\n\nPipeline is basically adding some machine learning steps back to back for creating merged and more codified versions of the sequenced steps. This giving us clearer and more automated ways of creating machine learning steps.\n\nOur pipeline consists of three steps:\n\n- Tokenizing text using sadedegel's 'ICU' tokenizer.\n- Vectorizing the tokens using 'HashVectorizer'\n- Feeding these values into LogisticRegression algorithm to train and predict targets.\n\nNote: We set our model hyperparameters using Optuna which we're going to cover it at the end of this notebook.\n","c5182d4d":"# Bert Embeddings and a Simple Model","60a51d65":"# Loading the Data\n\nHere we load the .csv file using pandas then select text and label columns. \n","f1b94409":"\n# Word Clouds\n\nIn next part we're going to create cool looking word clouds for another representation of bigram counts visually.\n","372c2ffd":"# Meta Features\n\nThese features may or may not be effective on machine learning models but explorating them gives data scientists an idea to where to begin...","8f3603e1":"\n# Side Work for Showing Tf-Idf Vectorizer and Using it for Outlier Detection\n\nIn this part we're not going to use our main pipeline but TfidfVectorizer for exemplary purposes. Here we get tf-idf vectors of our text and reduce dimension of our matrix ~64k to 3... This process can be used for several purposes including visualization and outlier detection.\n"}}