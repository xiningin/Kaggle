{"cell_type":{"ecb911d0":"code","44b03627":"code","834570b7":"code","168abd9c":"code","5ee97cc5":"code","19ef4618":"code","69ed1183":"code","4753c6b8":"code","274470c8":"code","c2b88c01":"code","dcbcfe7a":"code","2db3e853":"code","9b0e1372":"code","8a648353":"code","b3ba284f":"markdown","14abb457":"markdown","1733861c":"markdown","aac20406":"markdown","ee51b8a1":"markdown","8a7c46ab":"markdown","968e1a59":"markdown","607d6be6":"markdown","3a82c241":"markdown"},"source":{"ecb911d0":"import numpy as np \nimport pandas as pd\nfrom sklearn import *\nimport lightgbm as lgb\n\ntrain = pd.read_csv('\/kaggle\/input\/liverpool-ion-switching\/train.csv')\ntest = pd.read_csv('\/kaggle\/input\/liverpool-ion-switching\/test.csv')\ntrain.shape, test.shape","44b03627":"from functools import partial\nimport scipy as sp\nclass OptimizedRounder(object):\n    \"\"\"\n    An optimizer for rounding thresholds\n    to maximize F1 (Macro) score\n    # https:\/\/www.kaggle.com\/naveenasaithambi\/optimizedrounder-improved\n    \"\"\"\n    def __init__(self):\n        self.coef_ = 0\n\n    def _f1_loss(self, coef, X, y):\n        \"\"\"\n        Get loss according to\n        using current coefficients\n        \n        :param coef: A list of coefficients that will be used for rounding\n        :param X: The raw predictions\n        :param y: The ground truth labels\n        \"\"\"\n        X_p = pd.cut(X, [-np.inf] + list(np.sort(coef)) + [np.inf], labels = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10])\n\n        return -metrics.f1_score(y, X_p, average = 'macro')\n\n    def fit(self, X, y):\n        \"\"\"\n        Optimize rounding thresholds\n        \n        :param X: The raw predictions\n        :param y: The ground truth labels\n        \"\"\"\n        loss_partial = partial(self._f1_loss, X=X, y=y)\n        initial_coef = [0.5, 1.5, 2.5, 3.5, 4.5, 5.5, 6.5, 7.5, 8.5, 9.5]\n        self.coef_ = sp.optimize.minimize(loss_partial, initial_coef, method='nelder-mead')\n\n    def predict(self, X, coef):\n        \"\"\"\n        Make predictions with specified thresholds\n        \n        :param X: The raw predictions\n        :param coef: A list of coefficients that will be used for rounding\n        \"\"\"\n        return pd.cut(X, [-np.inf] + list(np.sort(coef)) + [np.inf], labels = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10])\n\n\n    def coefficients(self):\n        \"\"\"\n        Return the optimized coefficients\n        \"\"\"\n        return self.coef_['x']","834570b7":"def optimize_prediction(prediction, coefficients):\n    prediction[prediction <= coefficients[0]] = 0\n    prediction[np.where(np.logical_and(prediction > coefficients[0], prediction <= coefficients[1]))] = 1\n    prediction[np.where(np.logical_and(prediction > coefficients[1], prediction <= coefficients[2]))] = 2\n    prediction[np.where(np.logical_and(prediction > coefficients[2], prediction <= coefficients[3]))] = 3\n    prediction[np.where(np.logical_and(prediction > coefficients[3], prediction <= coefficients[4]))] = 4\n    prediction[np.where(np.logical_and(prediction > coefficients[4], prediction <= coefficients[5]))] = 5\n    prediction[np.where(np.logical_and(prediction > coefficients[5], prediction <= coefficients[6]))] = 6\n    prediction[np.where(np.logical_and(prediction > coefficients[6], prediction <= coefficients[7]))] = 7\n    prediction[np.where(np.logical_and(prediction > coefficients[7], prediction <= coefficients[8]))] = 8\n    prediction[np.where(np.logical_and(prediction > coefficients[8], prediction <= coefficients[9]))] = 9\n    prediction[prediction > coefficients[9]] = 10\n    \n    return prediction","168abd9c":"def features(df):\n    df = df.sort_values(by=['time']).reset_index(drop=True)\n    df.index = ((df.time * 10_000) - 1).values\n    df['batch'] = df.index \/\/ 50_000\n    df['batch_index'] = df.index  - (df.batch * 50_000)\n    df['batch_slices'] = df['batch_index']  \/\/ 5_000\n    df['batch_slices2'] = df.apply(lambda r: '_'.join([str(r['batch']).zfill(3), str(r['batch_slices']).zfill(3)]), axis=1)\n    \n    for c in ['batch','batch_slices2']:\n        d = {}\n        d['mean'+c] = df.groupby([c])['signal'].mean()\n        d['median'+c] = df.groupby([c])['signal'].median()\n        d['max'+c] = df.groupby([c])['signal'].max()\n        d['min'+c] = df.groupby([c])['signal'].min()\n        d['std'+c] = df.groupby([c])['signal'].std()\n        d['mean_abs_chg'+c] = df.groupby([c])['signal'].apply(lambda x: np.mean(np.abs(np.diff(x))))\n        d['abs_max'+c] = df.groupby([c])['signal'].apply(lambda x: np.max(np.abs(x)))\n        d['abs_min'+c] = df.groupby([c])['signal'].apply(lambda x: np.min(np.abs(x)))\n        for v in d:\n            df[v] = df[c].map(d[v].to_dict())\n        df['range'+c] = df['max'+c] - df['min'+c]\n        df['maxtomin'+c] = df['max'+c] \/ df['min'+c]\n        df['abs_avg'+c] = (df['abs_min'+c] + df['abs_max'+c]) \/ 2\n    \n    #add shifts\n    df['signal_shift_+1'] = [0,] + list(df['signal'].values[:-1])\n    df['signal_shift_-1'] = list(df['signal'].values[1:]) + [0]\n    for i in df[df['batch_index']==0].index:\n        df['signal_shift_+1'][i] = np.nan\n    for i in df[df['batch_index']==49999].index:\n        df['signal_shift_-1'][i] = np.nan\n\n    for c in [c1 for c1 in df.columns if c1 not in ['time', 'signal', 'open_channels', 'batch', 'batch_index', 'batch_slices', 'batch_slices2']]:\n        df[c+'_msignal'] = df[c] - df['signal']\n        \n    return df\n\ntrain = features(train)\ntest = features(test)","5ee97cc5":"col = [c for c in train.columns if c not in ['time', 'open_channels', 'batch', 'batch_index', 'batch_slices', 'batch_slices2']]\nx1, x2, y1, y2 = model_selection.train_test_split(train[col], train['open_channels'], test_size=0.3, random_state=7)","19ef4618":"def lgb_Metric(preds, dtrain):\n    labels = dtrain.get_label()\n    preds = np.round(np.clip(preds, 0, 10)).astype(int)\n    score = metrics.f1_score(labels, preds, average='macro')\n    return ('F1 Macro', score, True)\n \nparams = {'learning_rate': 0.8, 'max_depth': 7, 'num_leaves':2**7+1, 'metric': 'rmse', 'random_state': 7, 'n_jobs':-1} \nmodel = lgb.train(params, lgb.Dataset(x1, y1), 2000,  lgb.Dataset(x2, y2), verbose_eval=50, early_stopping_rounds=100, feval=lgb_Metric)\ntrain_preds_lgb = model.predict(train[col], num_iteration=model.best_iteration)\npreds_lgb = model.predict(test[col], num_iteration=model.best_iteration)","69ed1183":"optR_lgb = OptimizedRounder()\noptR_lgb.fit(train_preds_lgb.reshape(-1,), train['open_channels'])\ncoefficients_lgb = optR_lgb.coefficients()\nprint(coefficients_lgb)","4753c6b8":"test['open_channels'] = optimize_prediction(preds_lgb,coefficients_lgb).astype(int)\ntest[['time','open_channels']].to_csv('submission_lgb.csv', index=False, float_format='%.4f')","274470c8":"from catboost import Pool,CatBoostRegressor\n\n# Initialize CatBoostRegressor\nmodel = CatBoostRegressor(task_type = \"CPU\",\n                          iterations=1000,\n                          learning_rate=0.1,\n                          random_seed = 42,\n                          depth=2,\n                         )\n# Fit model\nmodel.fit(x1, y1)\n# Get predictions\ntrain_preds_cat = model.predict(train[col])\npreds_catb = model.predict(test[col])","c2b88c01":"optR_cat = OptimizedRounder()\noptR_cat.fit(train_preds_cat.reshape(-1,), train['open_channels'])\ncoefficients_cat = optR_cat.coefficients()\nprint(coefficients_cat)","dcbcfe7a":"test['open_channels'] = optimize_prediction(preds_catb, coefficients_cat).astype(int)\ntest[['time','open_channels']].to_csv('submission_cat.csv', index=False, float_format='%.4f')","2db3e853":"train_preds_comb = 0.75 * train_preds_lgb + 0.25 * train_preds_cat\noptR = OptimizedRounder()\noptR.fit(train_preds_comb.reshape(-1,), train['open_channels'])\ncoefficients = optR.coefficients()\nprint(coefficients)","9b0e1372":"preds_comb = 0.75 * preds_lgb + 0.25 * preds_catb","8a648353":"\ntest['open_channels'] = optimize_prediction(preds_comb, coefficients).astype(int)\ntest[['time','open_channels']].to_csv('submission.csv', index=False, float_format='%.4f')\n","b3ba284f":"#### Optimize Rounder Coefficients on Ensembled training predictions and training dataset ","14abb457":"#### Optimize Rounder Coefficients on LGB training predictions and training dataset ","1733861c":"### Light GBM model","aac20406":"#### Optimize Rounder Coefficients on Catboost training predictions and training dataset ","ee51b8a1":"### Create features for train and test","8a7c46ab":"### CatBoost model","968e1a59":"Credit to [AlexFocus](https:\/\/www.kaggle.com\/alexfocus) and [jazivxt](https:\/\/www.kaggle.com\/jazivxt) for original kernels","607d6be6":"\n![liverpol](https:\/\/storage.googleapis.com\/kaggle-competitions\/kaggle\/18045\/logos\/thumb76_76.png?t=2020-02-21-18-37-27)\n\nIf you like this notebook please let here your upvote!\n\n### Install libraries and load the Data","3a82c241":"### Ensemble predictions"}}