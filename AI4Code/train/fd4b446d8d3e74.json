{"cell_type":{"3588167d":"code","84c52f44":"code","fccbe617":"code","774d0cd4":"code","35ef1f25":"code","347d0c95":"code","f77b0c00":"code","af28b9ca":"code","111b2f07":"code","6a9aed3f":"code","65fe89e9":"code","d9f3ab06":"code","c95ebf0d":"code","147e7320":"code","9f6db4e4":"code","9cee7f5d":"code","97e084c3":"code","6268ac7f":"code","2785c024":"code","600bc0ce":"code","b4219ea6":"code","5be5319a":"code","099ef0ab":"code","f437979e":"code","97322889":"code","bca6857b":"code","74cde1b2":"code","74878205":"code","488a90d5":"code","b7fe96ca":"code","3c5542a0":"code","6cdba38e":"code","d253ce6c":"code","1663ea54":"code","dc3db6dc":"code","da1685a3":"markdown","ba60da00":"markdown","e9880708":"markdown","88f2c4e2":"markdown","7bc0b13b":"markdown","179f59b3":"markdown","6f244bba":"markdown","ff529b65":"markdown","bd65d8b0":"markdown","f4c80e45":"markdown","b0eb01b1":"markdown","3c1b3748":"markdown"},"source":{"3588167d":"import numpy as np\nimport pandas as pd\n\nimport keras.layers as layers\nfrom keras.models import Sequential\nfrom sklearn.preprocessing import Normalizer\nimport tensorflow as tf\n\nimport seaborn as sns\nsns.set_theme(palette='magma')\nimport matplotlib.pyplot as plt","84c52f44":"train = pd.read_csv(\"\/kaggle\/input\/titanic\/train.csv\")\ntest = pd.read_csv(\"\/kaggle\/input\/titanic\/test.csv\")\ntrain.head()","fccbe617":"train.info()","774d0cd4":"train.dropna(subset=['Embarked'], how='all', inplace=True)\ntrain = train.drop(['PassengerId'], axis=1)","35ef1f25":"for x in train.columns:\n    print(f\"{x}\\n{train[x].unique()[:10]}\")","347d0c95":"f, axes = plt.subplots(1, 3, figsize=(25, 8))\nsns.barplot(x=\"Sex\", y=\"Survived\", hue=\"Pclass\", data=train, ax=axes[0])\nsns.barplot(x=\"Pclass\", y=\"Survived\", data=train, ax=axes[1]);\nsns.barplot(x=\"Pclass\", y=\"Fare\", data=train, ax=axes[2]);","f77b0c00":"f, axes = plt.subplots(1, 4, figsize=(25, 8))\nsns.histplot(x=\"Age\", hue=\"Survived\", data=train, ax=axes[0]);\nsns.histplot(x=\"Fare\", bins=50, hue=\"Survived\", data=train, ax=axes[1]);\nsns.histplot(x=\"Parch\", hue=\"Survived\", data=train, ax=axes[2]);\nsns.histplot(x=\"SibSp\", bins=50, hue=\"Survived\", data=train, ax=axes[3]);","af28b9ca":"train.drop(train[train[\"Fare\"]>500].index, inplace=True)\ntrain[\"Fare\"].describe()\n# max fare drops to 263 when values above 500 are dropped, 3 rows","111b2f07":"# Columns \"Cabin\", \"Ticket\", \"Name\" comparatively have some complex logistics, many missing values and possibly a lot of extractable data\n# Age has lot of missing values\n# Fare column might prove of significance to get values for the cabin\ncols = [\"Fare\", \"Cabin\", \"Ticket\", \"Name\", \"Age\"]\ntrain[cols][:5]","6a9aed3f":"print(train[train[\"Cabin\"]!=np.nan].values[:200])\ntrain[\"Ticket\"].value_counts() #tickets with most family members, largest family being of 7 members","65fe89e9":"train[train[\"Ticket\"] == \"CA. 2343\"] # The CA. 2343 family, no one survived.","d9f3ab06":"# extracting titles of individuals to make use of the names along with getting some idea for the age by inference\ndef nameExtract(x):\n    x = x.lower().split(\",\")[1].split(\".\")[0].replace(\" \", \"\")\n    return x\n    \ntrain[\"Title\"] = train[\"Name\"].apply(lambda x:nameExtract(x))\ntest[\"Title\"] = test[\"Name\"].apply(lambda x:nameExtract(x))","c95ebf0d":"f, axes = plt.subplots(2, 1, figsize=(25, 10), sharex=True)\nsns.histplot(x=\"Title\", hue=\"Survived\", data=train, ax=axes[0]);\nsns.barplot(x=\"Title\", y=\"Age\", data=train, ax=axes[1]);","147e7320":"train[\"Title\"].unique()","9f6db4e4":"for x in train[\"Title\"].unique():\n    train[train[\"Title\"]==x] = train[train[\"Title\"]==x].fillna(train[train[\"Title\"]==x].Age.mean())\n    \nfor x in test[\"Title\"].unique():\n    test[test[\"Title\"]==x] = test[test[\"Title\"]==x].fillna(test[test[\"Title\"]==x].Age.mean())\n\nf, axes = plt.subplots(1, 1, figsize=(25, 8), sharex=True)\nsns.barplot(x=\"Title\", y=\"Age\", data=train, ax=axes);","9cee7f5d":"test = test.fillna(40)","97e084c3":"def cabinExtract(x):\n    try:\n        x = [n.lower() for n in x if n.isalpha()][0]\n    except:\n        return np.nan\n    return x\n\ntrain[\"CabinLetter\"] = train[\"Cabin\"].apply(lambda x:cabinExtract(x))","6268ac7f":"# bins for plotting\ntrain[\"FareBin10\"] = train[\"Fare\"].apply(lambda x:round(x\/10)*10) # creating bins of 10 for fare\ntrain[\"AgeBin5\"] = train[\"Age\"].apply(lambda x:round(x\/5)*5) # creating bings of 5 for age\ntest[\"FareBin10\"] = test[\"Fare\"].apply(lambda x:round(x\/10)*10)\ntest[\"AgeBin5\"] = test[\"Age\"].apply(lambda x:round(x\/5)*5)","2785c024":"# We get much simpler information to work with, constituting of 8 symbols\ntrain[\"CabinLetter\"].value_counts()","600bc0ce":"f, axes = plt.subplots(2, 2, figsize=(25, 8))\nsns.histplot(x=\"CabinLetter\", hue=\"Pclass\", data=train, ax=axes[0,0]);\nsns.barplot(x=\"CabinLetter\", y=\"Fare\", hue=\"Pclass\", data=train, ax=axes[0,1]);\nsns.histplot(x=\"CabinLetter\", hue=\"Sex\", data=train, ax=axes[1,0]);\nsns.barplot(x=\"CabinLetter\", y=\"Fare\", hue=\"Survived\", data=train, ax=axes[1,1]);","b4219ea6":"cols = ['Pclass', 'Sex', 'SibSp', 'Parch', 'Embarked', 'Fare', 'Age', 'Title'] # 'Title'\ncat_cols = ['Pclass', 'Sex', 'SibSp', 'Parch', 'Embarked', 'Title'] # 'Title'\nnum_cols= ['Fare', 'Age']\ntraining = train[cols]#.astype(\"Float16\")\ntesting = test[cols]\nlabel = train[\"Survived\"].values.astype(\"float16\")","5be5319a":"combined = pd.concat([training, testing]).astype(\"object\")\ncombined.info()","099ef0ab":"transformer = Normalizer()\ntransformed = pd.DataFrame()\ntransformed[[\"Age\", \"Fare\"]]=transformer.fit_transform(combined[[\"Age\", \"Fare\"]])","f437979e":"\ncombined = pd.get_dummies(combined[cat_cols])\ncombined[[\"Age\", \"Fare\"]]=transformed[[\"Age\", \"Fare\"]].astype(\"float16\")\nprint(combined.shape)\ncombined.info()","97322889":"'''\nCATEGORICAL_COLUMNS = cols[:6]\nNUMERIC_COLUMNS = cols[6:]\n\nfeature_columns = []\nfor feature_name in CATEGORICAL_COLUMNS:\n  vocabulary = training[feature_name].unique()\n  feature_columns.append(tf.feature_column.categorical_column_with_vocabulary_list(feature_name, vocabulary))\n\nfor feature_name in NUMERIC_COLUMNS:\n  feature_columns.append(tf.feature_column.numeric_column(feature_name, dtype=tf.float32))\n\nlabel = train[\"Survived\"].values.astype(\"float16\")\n\ndef make_input_fn(data_df, label_df, num_epochs=10, shuffle=True, batch_size=32):\n  def input_function():\n    ds = tf.data.Dataset.from_tensor_slices((dict(training), label))\n    if shuffle:\n      ds = ds.shuffle(10)\n    ds = ds.batch(batch_size).repeat(num_epochs)\n    return ds\n  return input_function\n\ntrain_input_fn = make_input_fn(training, label)\neval_input_fn = make_input_fn(training, label, num_epochs=1, shuffle=False)\n\nlinear_est = tf.estimator.DNNLinearCombinedClassifier(linear_feature_columns=feature_columns)\nlinear_est.train(train_input_fn)\nresult = linear_est.evaluate(eval_input_fn)\nprint(result)\n''';","bca6857b":"print(training.shape)\nprint(testing.shape)","74cde1b2":"training = combined[:886]\ntesting = combined[886:]\nprint(training.shape)\nprint(testing.shape)","74878205":"model = Sequential()\nmodel.add(layers.Input(43,))\nmodel.add(layers.Dense(50,))\nmodel.add(layers.Dense(50,))\nmodel.add(layers.Dense(50,))\nmodel.add(layers.Dense(50,))\nmodel.add(layers.Dense(50,))\nmodel.add(layers.Dense(50,))\nmodel.add(layers.Dense(50,))\nmodel.add(layers.Dense(50,))\nmodel.add(layers.Dense(50,))\nmodel.add(layers.Dense(50,))\nmodel.add(layers.Dense(50,))\nmodel.add(layers.Dense(50,))\nmodel.add(layers.Dense(50,))\nmodel.add(layers.Dense(50,))\nmodel.add(layers.Dense(50,))\nmodel.add(layers.Dense(50,))\nmodel.add(layers.Dense(50,))\nmodel.add(layers.Dense(50,))\nmodel.add(layers.Dense(50,))\nmodel.add(layers.Dense(50,))\nmodel.add(layers.Dense(1,))\n\nmodel.compile(\n    optimizer=tf.keras.optimizers.Adamax(\n        learning_rate=0.0005),\n    loss=tf.keras.losses.BinaryCrossentropy(\n        from_logits=False,\n        label_smoothing=0.0,\n        reduction=\"auto\"),\n    metrics=['accuracy'])","488a90d5":"history = model.fit(training, label, batch_size=128, epochs=200, verbose=False)\nplt.plot(history.history['accuracy'])\n\nscore = model.evaluate(training, label, verbose=0)\nprint(\"Test loss:\", score[0])\nprint(\"Test accuracy:\", score[1])","b7fe96ca":"preds = model.predict(testing)","3c5542a0":"out=[]\nfor idx, x in enumerate(preds):\n    out.append(round(preds[idx][0]))","6cdba38e":"pd.read_csv(\"..\/input\/titanic\/gender_submission.csv\").head()","d253ce6c":"test[\"Survived\"] = out\nout = test[[\"PassengerId\", \"Survived\"]]","1663ea54":"out.head()","dc3db6dc":"out.to_csv(\".\/out.csv\", index = False)","da1685a3":"- Plotting the simplified cabin letters lebelled with the Pclass they associate with shows a clear indication for most of the letters, (c, a, b, t) all fall purely under Pclass 1, CabinLetter (e, g, d, f) have mixed Pclasses with (e, d) mostly with Pclass 1. (g, f) have no Pclass 1 members. (g) is purely Pclass 3.\n- CabinLetter (g) only constitues of women in the available data points, (a) constitutes of almost all males, rest Letters have a similar Sex distribution.\n\n- For now lets say we can save our Cabin column and more on without much guilt.","ba60da00":"- Applying normalization to values of both training and testing sets\n- Getting dummies for categorical columns using pandas for both train and test sets","e9880708":"# Modelling and Predictions","88f2c4e2":"# EDA","7bc0b13b":"- From the above charts what we infer is that females had a much higher chance of survival over men, and individuals from Pclass 1 had much higher survival rate than the other two classes, which is the highest class tier aboard.\n\n- Females in Pclass 1 and Pclass 2 have similar survival rates, and the highest among all other counter parts, whereas men of only Pclass 1 have some chance of surviving, Men of Pclass 2 and Pclass 3 have the lowest chances of survival, almost approximately 1\/8th of Pclass 1 women. \n\n- Pclass 1 costs significantly more than Pclass 2 and Pclass 3, with the average ticket fare being more than triple of the next.","179f59b3":"- Cabin not only has a vast majority of values missing but also shows repeating cabin numbers and multiple cabin allocations on single entries, this will be a complex situation to tackle. Each cabin entry has an alphbet value followed by an integer, this can be associated with pClass and maybe something can be inferred from the ticket number they reside with to reach optimal way to engineer missing values. Some entries do have a combination of different chars ('F G73') though that will be a minor problem to suffice.\n\n- For the Age and Ticket numeric variable, binning will be suitable, after engineering a methodology to fill in the nan values of age.\n\n- Indexing by the ticket values which were repeated we can get insight into the families that were onboard the titanic with the same ticket numbers. Interestingly, looking at the largest families onboard we see that there are no survivors, whereas earlier from the charts having more family equated a higher rate of survival.","6f244bba":"On a quick glance most of the classes seem intact with no missing values, except for \"Age\" which has a few missing counterparts, \"Cabin\" where majority of the data is missing and \"Embarked\" with two empty rows which can be dropped.","ff529b65":"Getting a slice of all unique values in each column shows us what is the data like and how can the feature engineering be address for each specific column. The ones with few distinct categorical differences can be one hot encoded, such as SipSp (siblings and spouse), Sex (M F), Embarked (Port of boarding), Pclass (Ticket class) etc. whilst the broader categorical features can be refined down and extracted from to create new and potetially useful features.","bd65d8b0":"All missing age values have been replaced with the means age of their respective title distribution of the Age, thus we see the graph not change proportions.","f4c80e45":"- Individuals below the age of 10 show the highest survival ration with more survivors by count than dead and almost none above the the age of 60 show any chance at survivng.\n\n- Going by the fares, the lower tiers tickets below a fare of 20 have the worst survival ratio with only around 1\/3rd of the population making it, fares above 50 have a significantly better ratio with more survivors than casualities.\n\n- Individuals with no parents or children (Parch) have a equally likely chance of making it or not, whereas having anny relative makes the chance much better, same way around for having siblings or spouse (SibSp) shows a increase in chances of survival proportinal to count.\n\n- The price range is very broad with some extreme outliers above 500, these(3) have been removed to improve distribution.","b0eb01b1":"- Looking closely at the survival differences, it is very much more apparent that the ratio of men (mr)* is a lot higher than others, but the ratio of survivability is also the worst. Despite being almost 2 times more men onboard, the number of survivors is half than that on women.\n\n- Women (mrs, miss) show a high survival ratio with more survivors than dead, with adult women (mrs) showing comparatively more survivors.\n\n- Now that we have the titles, deciphering the average bin of ages based on the titles of their non-missing counterparts we can get a decent extimation for filling out the values of the missing.","3c1b3748":" # Feature Engineering"}}