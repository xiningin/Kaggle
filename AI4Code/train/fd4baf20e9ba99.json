{"cell_type":{"d3a24947":"code","f6cb7fba":"code","54c986b8":"code","b0244152":"code","d861b451":"code","ee86b8aa":"code","95495814":"code","e320c152":"code","d30faa0f":"code","5ef0430d":"code","6bbd8941":"code","b20cbe69":"markdown"},"source":{"d3a24947":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport csv\nimport os\ncsv_file = os.listdir(\"..\/input\")\nprint(csv_file)\n\n# Any results you write to the current directory are saved as output.","f6cb7fba":"# Read input file\n\nfeatures = []\nnames = []\nwith open('..\/input\/musicfeatures\/data.csv','rt')as f:\n    data = csv.reader(f)\n    for row in data:\n        features.append(np.array(row[1:]))\nfeatures = np.array(features)\n\nkey = features[0]\nfeatures = features[1:,:]\nlabels = features[:,-1]\ncategories = list(set(labels))\n\nfeatures = features[:,:-1]\nkey = key[:-1]\n\nprint('Features -', features.shape, key)\nprint('Categories -', categories, len(categories))\n\nlabels_temp = []\nfor i in range(len(categories)):\n    for label in labels:\n        if label == categories[i]:\n            labels_temp.append(i)\nlabels = np.array(labels_temp)","54c986b8":"# Convert string features to number\nfeatures = features.astype(np.float)\nprint(features[0])","b0244152":"# PCA\nfrom sklearn.decomposition import PCA\n\nfeatures_copy = features.copy()\nfor i in range(len(key)):\n    Xi = features[:,i]\n    features_copy[:,i] = (Xi-np.mean(Xi))\/np.std(Xi)\n    \npca = PCA(n_components = 22)\nPC = pca.fit_transform(features_copy)\nprint(pca.explained_variance_ratio_)\ncumulated = []\ncumulated_score = 0\nfor val in pca.explained_variance_ratio_:\n    cumulated_score += val\n    cumulated.append(cumulated_score)\nprint(cumulated)\nprint(PC.shape)","d861b451":"from sklearn.utils import shuffle\n\nX, Y = shuffle(PC, labels)\nprint(X.shape, Y.shape)\n\ntrain_split = 1\n\nX_Train = X[:int(1000*train_split), :]\nY_Train = Y[:int(1000*train_split)]\nX_Val = X[int(1000*train_split):, :]\nY_Val = Y[int(1000*train_split):]\n\nprint(X_Train.shape, Y_Train.shape, X_Val.shape, Y_Val.shape)","ee86b8aa":"from keras.layers import Input, Dense, BatchNormalization, Activation, Dropout \nfrom keras.models import Model\nfrom keras import optimizers\n\n\nfrom numpy.random import seed\nseed(2019)\n\nfrom tensorflow import set_random_seed\nset_random_seed(2019)\n\ninputs = Input(shape=(22,))\n\nx = Dense(32)(inputs)\nx = BatchNormalization()(x)\n#x = Dropout(0.5)(x)\nx = Activation('relu')(x)\n\nx = Dense(64)(x)\nx = BatchNormalization()(x)\n#x = Dropout(0.5)(x)\nx = Activation('relu')(x)\n\nx = Dense(64)(x)\nx = BatchNormalization()(x)\nx = Dropout(0.5)(x)\nx = Activation('relu')(x)\n\nx = Dense(32)(x)\nx = BatchNormalization()(x)\n#x = Dropout(0.5)(x)\nx = Activation('relu')(x)\n\npredictions = Dense(10, activation='softmax')(x)\n\nmodel = Model(inputs=inputs, outputs=predictions)\nopt = optimizers.SGD(lr=0.01)\n\nmodel.compile(optimizer=opt,\n              loss='sparse_categorical_crossentropy',\n              metrics=['accuracy'])\n\nhistory = model.fit(np.array(X_Train), np.array(Y_Train), batch_size = 32, epochs = 250, validation_split = 0.2)","95495814":"from matplotlib import pyplot as plt\n\nplt.plot(history.history['acc'])\nplt.plot(history.history['loss'])\nplt.title('Training')\nplt.ylabel('Score')\nplt.xlabel('Epoch')\nplt.legend(['Accuracy', 'Loss'], loc='upper left')\nplt.show()","e320c152":"from matplotlib import pyplot as plt\n\nplt.plot(history.history['val_acc'])\nplt.plot(history.history['val_loss'])\nplt.title('Validation')\nplt.ylabel('Score')\nplt.xlabel('Epoch')\nplt.legend(['Accuracy', 'Loss'], loc='upper left')\nplt.show()","d30faa0f":"# Read test data\n\nfeatures = []\nnames = []\noffsets = []\nwith open('..\/input\/test-songs\/features.csv','rt')as f:\n    data = csv.reader(f)\n    for row in data:\n        features.append(np.array(row))\n        names.append(row[0])\n        offsets.append(row[-1])\n        \nfeatures = np.array(features)\nfeatures = features[1:,1:-1].astype(np.float)\nnames = np.array(names[1:])\noffsets = np.array(offsets[1:])\n\nprint('Test Data Shape -', features.shape)","5ef0430d":"# Predict on features with trained model\nfrom scipy import stats\n\nfeatures_copy = features.copy()\nfor i in range(len(key)):\n    Xi = features[:,i]\n    features_copy[:,i] = (Xi-np.mean(Xi))\/np.std(Xi)\n\nsong_names = list(set(names))\nsong_groups = {song_name:[] for song_name in song_names}\nsong_pred = {song_name:[] for song_name in song_names}\nsong_genre = {song_name:[] for song_name in song_names}\npred_count = {song_name:{category:0 for category in categories} for song_name in song_names}\n\nfor i in range(len(names)):\n    song_groups[names[i]].append(features_copy[i])\n\nfor song_name in song_names:\n    features = song_groups[song_name]\n    for feature in features:\n        feature = pca.transform(np.reshape(feature, (1, feature.shape[0])))\n        pred = model.predict(feature)\n        idx = np.argmax(pred)\n        final_pred = categories[idx]\n        song_pred[song_name].append(final_pred)\n    for pred_temp in song_pred[song_name]:\n        pred_count[song_name][pred_temp] += 1\n\nprint(pred_count)","6bbd8941":"for song_name in song_names:\n    stats = pred_count[song_name]\n    song_genre[song_name] = max(stats, key=stats.get)\n    \nprint(song_genre)","b20cbe69":"**TRAINING COMPLETE**"}}