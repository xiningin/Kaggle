{"cell_type":{"71b5f024":"code","b2683086":"code","9c753d4e":"code","aa2a62b5":"code","18e05a77":"code","6ecac9e5":"code","e4df1a39":"code","30828660":"code","c0be50b8":"code","cfcb87e2":"code","4948d1d7":"code","e6cb132d":"code","043166d5":"code","42a34cab":"code","6ecdaeeb":"code","5fc488a5":"code","fdac611c":"code","d9bbf80b":"code","02e69cdf":"code","897e6429":"code","e351eadd":"code","edc8a0ef":"code","a1baebc5":"code","b0829e3a":"code","92ab0796":"code","2e92c5da":"code","06cf2332":"code","e8d6ce8b":"code","2b065c18":"code","31291493":"code","5420ce68":"code","0af5d065":"code","50e589ab":"code","96215209":"code","9c5e7938":"code","422c5fb7":"code","1eb19ef7":"code","24877693":"code","2fcafbd5":"code","395ac330":"code","13fc7a0c":"code","640191ec":"code","6e378dd5":"code","931f34c5":"code","1e7944e0":"code","00d62ee8":"code","1eca02b4":"code","97be37b7":"code","212529b5":"markdown","e163e225":"markdown","6cb3408f":"markdown","cc91c09a":"markdown","c17864df":"markdown","f7b8d40f":"markdown","fbd237f1":"markdown"},"source":{"71b5f024":"import pandas as pd","b2683086":"#Reading the data file\ndf = pd.read_csv('..\/input\/crop-recommendation-dataset\/Crop_recommendation.csv')\ndf.head()","9c753d4e":"import seaborn as sns\nfrom matplotlib import pyplot as plt\n%matplotlib inline","aa2a62b5":"df.shape","18e05a77":"df.info()","6ecac9e5":"df.describe()","e4df1a39":"df.isnull().sum()","30828660":"#To understand the columns \nsns.barplot(data=df)","c0be50b8":"plt.figure(figsize=(10,8))\nplt.title(\"Temperature relation with crops\")\nsns.barplot(y=\"temperature\",x=\"label\", data=df,palette='bright')\nplt.xticks(rotation='vertical')\nplt.ylabel(\"Temperature\")\nplt.xlabel(\"crops\")\nplt.figure(figsize=(10,8))\nplt.title(\"humidity relation with crops\")\nsns.barplot(y=\"humidity\",x=\"label\", data=df,palette=\"colorblind\")\nplt.xticks(rotation='vertical')\nplt.ylabel(\"humidity\")\nplt.xlabel(\"crops\")\nplt.figure(figsize=(10,10))\nplt.title(\"ph relation with crops\")\nsns.barplot(y=\"ph\",x=\"label\", data=df,palette=\"cubehelix\")\nplt.xticks(rotation='vertical')\nplt.ylabel(\"ph\")\nplt.xlabel(\"crops\")","cfcb87e2":"  plt.figure(figsize=(8,6))\n  plt.title(\"Temperature and pH effect values for crops\")\n  sns.scatterplot(data=df, x=\"temperature\", y=\"label\", hue=\"ph\")\n  plt.ylabel(\"Crops\")","4948d1d7":"plt.figure(figsize=(8,5))\nsns.heatmap(df.corr(), annot =True, fmt= '.0%')","e6cb132d":"from sklearn.model_selection import train_test_split \nfrom sklearn.preprocessing import LabelEncoder","043166d5":"#Encoding label(i.e string format) to integer for training and testing model\nlabel_encode = LabelEncoder()\n\ndf['label'] = label_encode.fit_transform(df['label'])\ncrop_category = {index : label for index, label in enumerate(label_encode.classes_)}\ncrop_category","42a34cab":"X = df.drop('label', axis = 1)\ny = df['label']","6ecdaeeb":"X_train, X_test, y_train, y_test = train_test_split(X, y, train_size = 0.8)","5fc488a5":"import tensorflow as tf\nimport warnings\nwarnings.filterwarnings(\"ignore\")","fdac611c":"a1 = tf.keras.Input(shape = (7, ))\na2= tf.keras.layers.Dense(64, activation = 'relu')(a1)\na3 = tf.keras.layers.Dense(22, activation = 'softmax')(a2)\n\nmodel = tf.keras.Model(a1,a3)","d9bbf80b":"#Compiling the keras model\nmodel.compile(optimizer = 'adam',loss = 'sparse_categorical_crossentropy', metrics = 'accuracy')","02e69cdf":"#fitting the model \nhistory = model.fit(X_train,y_train,validation_split = 0.2,batch_size = 128,epochs = 100)","897e6429":"# Accuracy plot of the model\nimport matplotlib.pyplot as plt\n\nplt.plot(history.history['val_accuracy'],label='valdation accuracy')\nplt.plot(history.history['accuracy'],label='accuracy')\nplt.title('Model Accuracy')\nplt.xlabel('epochs')\nplt.ylabel('accuracy')\nplt.grid(True)\nplt.legend(loc='upper right')\nplt.show()","e351eadd":"#Loss plot of the model\nplt.figure(figsize = (8,5))\n\nplt.plot(history.history['loss'], label = 'loss')\nplt.plot(history.history['val_loss'], label = 'Validation loss')\nplt.title('Model Loss')\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.grid(True)\nplt.legend(loc='upper right')\nplt.show()","edc8a0ef":"#Classification report of keras model\nimport numpy as np\nfrom sklearn.metrics import classification_report\n\npredkeras = model.predict(X_test, batch_size = 128, verbose = 1)\npred = np.argmax(predkeras,axis = 1)\nprint(classification_report(y_test,pred))","a1baebc5":"#Evaluating the loss and accuracy of the model \nloss, accuracy= model.evaluate(X_test, y_test)\nprint(\"Accuracy of the keras model :\",accuracy*100)","b0829e3a":"acc = []\nmodel = []","92ab0796":"acc.append(accuracy)\nmodel.append('predkeras')","2e92c5da":"from sklearn.metrics import classification_report\nfrom sklearn import metrics","06cf2332":"#Feature selecion\nfeatures = df[['N', 'P','K','temperature', 'humidity', 'ph', 'rainfall']]\ntarget = df['label']","e8d6ce8b":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(features,target,test_size = 0.2,random_state =2)","2b065c18":"from sklearn.tree import DecisionTreeClassifier\n\ndecisiontree = DecisionTreeClassifier()\nd = decisiontree.fit(X_train,y_train)\n\npredicted_values = decisiontree.predict(X_test)\n\nacc_dt = metrics.accuracy_score(y_test, predicted_values)\nacc.append(acc_dt)\nmodel.append('decisiontree')\nprint(\" Accuracy of Decision Tree is: \", acc_dt*100)","31291493":"print(classification_report(y_test,predicted_values))","5420ce68":"#import libraries for data visualization\nimport plotly.graph_objects as go\nimport plotly.express as px\nfrom plotly.subplots import make_subplots\n\nfrom sklearn.metrics import classification_report\nfrom sklearn import metrics\nfrom sklearn import tree\nfrom sklearn.model_selection import cross_val_score\nimport warnings\nwarnings.filterwarnings('ignore')","0af5d065":"#creating a confusion matrix for predicted and actual values\nfrom sklearn.metrics import confusion_matrix\n\ncm_dt = confusion_matrix(y_test,predicted_values)\n\nf, ax = plt.subplots(figsize=(10,8))\nsns.heatmap(cm_dt, annot=True, linewidth=0.5, fmt=\".0f\",  cmap='plasma', ax = ax)\nplt.xlabel(\"Predicted\")\nplt.ylabel(\"Actual\")\nplt.title('Predicted vs actual')\nplt.show()","50e589ab":"#Implementing Random forest algorithm\nfrom sklearn.ensemble import RandomForestClassifier\n\nrandomforest = RandomForestClassifier(n_estimators=30, random_state=0)\nrandomforest.fit(X_train,y_train)\n\npredicted_values = randomforest.predict(X_test)\n\nx = metrics.accuracy_score(y_test, predicted_values)\nacc.append(x)\nmodel.append('Random Forest')\nprint(\"Accuracy of Random Forest is : \", x*100)\n","96215209":"print(classification_report(y_test,predicted_values))","9c5e7938":"#Print Train Accuracy\nprint(\"Training accuracy = \",randomforest.score(X_train,y_train))\n#Print Test Accuracy\nprint(\"Testing accuracy = \",randomforest.score(X_test,y_test))","422c5fb7":"#Displaying confusion matrix of predicted and actual values\ny_pred = randomforest.predict(X_test)\ny_true = y_test\n\nfrom sklearn.metrics import confusion_matrix\n\ncm_rf = confusion_matrix(y_true,y_pred)\n\nf, ax = plt.subplots(figsize=(10,8))\nsns.heatmap(cm_rf, annot=True, linewidth=0.5, fmt=\".0f\",  cmap='viridis', ax = ax)\nplt.xlabel(\"Predicted\")\nplt.ylabel(\"Actual\")\nplt.title('Predicted vs actual')\nplt.show()\n","1eb19ef7":"#Implementing mean accuracy of the model\nmean_accrf = np.zeros(20)\nfor i in range(1,21):\n    #Train Model and Predict  \n    rf = RandomForestClassifier(n_estimators= i).fit(X_train,y_train)\n    meanpredrf = rf.predict(X_test)\n    mean_accrf[i-1] = metrics.accuracy_score(y_test, meanpredrf)\n\nmean_accrf","24877693":"#Graph representing number of estimators with accuracy\nimport numpy as np\nloc = np.arange(1,21,step=1.0)\nplt.figure(figsize = (10, 6))\nplt.plot(range(1,21), mean_accrf)\nplt.xticks(loc)\nplt.xlabel('Number of estimators ')\nplt.ylabel('Accuracy')\nplt.show()","2fcafbd5":"# Create a confusion matrix  \nfrom sklearn.metrics import confusion_matrix\nimport numpy as np\ncm = confusion_matrix(y_test, meanpredrf) #mean accuracy matrix\n\n# Plot heatmap for the confusion matrix:\nplt.figure(figsize = (10,8))\nsns.heatmap(cm, cmap='copper', annot = True, fmt = 'g')\nplt.xlabel(\" Mean Predicted\")\nplt.ylabel(\"Mean Actual\")\nplt.title('Mean Predicted vs Mean actual')","395ac330":"#Scatter plot to demonstrate temperature and rainfall and different training data with testing data\nplt.figure(figsize=(10,8))\nplt.title(\"Training Temperature and rainfall \")\nsns.scatterplot(y=\"temperature\",x=\"rainfall\", data=X_train)\nplt.xticks(rotation='vertical')\nplt.ylabel(\"Temperature\")\nplt.xlabel(\"crops\")\nplt.figure(figsize=(10,8))\nplt.title(\"Testing Temperature and rainfall\")\nsns.scatterplot(y=\"temperature\",x=\"rainfall\", data=X_test)\nplt.xticks(rotation='vertical')\nplt.ylabel(\"Temperature\")\nplt.xlabel(\"crops\")","13fc7a0c":"#K-Nearest Neighbors\n\nfrom sklearn.neighbors import KNeighborsClassifier\nknn = KNeighborsClassifier()\n\nknn.fit(X_train,y_train)\n\npredicted_values = knn.predict(X_test)\n\nacc_knn = metrics.accuracy_score(y_test, predicted_values)\nacc.append(acc_knn)\nmodel.append('K Nearest Neighbours')\nprint(\"Accuracy of KNN is :  \", acc_knn*100)\n","640191ec":"print(classification_report(y_test,predicted_values))","6e378dd5":"import matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nimport plotly.graph_objects as go\nimport plotly.express as px\nfrom plotly.subplots import make_subplots\nfrom sklearn.metrics import confusion_matrix\n\nknncm = confusion_matrix(y_test,predicted_values) #Confusion Matrix for Predicted values\n\nf1,ax1 = plt.subplots(figsize=(8,5))\nsns.heatmap(knncm, annot=True, linewidth=0.5, fmt=\".0f\",cmap='PuBu', ax = ax1)\nplt.xlabel('Predicted')\nplt.ylabel('Actual')\nplt.title('Predicted vs actual')\nplt.show()","931f34c5":"import numpy as np\nmean_accuracy = np.zeros(20)\nfor i in range(1,21):\n    #Train Model and Predict  \n    knn = KNeighborsClassifier(n_neighbors = i).fit(X_train,y_train)\n    meanpred= knn.predict(X_test)\n    mean_accuracy[i-1] = metrics.accuracy_score(y_test, meanpred)\nmean_accuracy","1e7944e0":"#Graph between number of neighbors and accuracy\nimport numpy as np\nloc = np.arange(1,21,step=1.0)\nplt.figure(figsize = (10, 6))\nplt.plot(range(1,21), mean_accuracy)\nplt.xticks(loc)\nplt.xlabel(' Number of Neighbors ')\nplt.ylabel(' Accuracy ')\nplt.show()","00d62ee8":"# Create a confusion matrix  \nfrom sklearn.metrics import confusion_matrix\nimport numpy as np\nplt.figure(figsize = (10,8))\ncm = confusion_matrix(y_test, meanpred) #Confusion matrix for Mean Predicted values\nplt.xlabel('Mean Predicted')\nplt.ylabel('Mean Actual')\nplt.title('Mean Predicted vs Mean actual')\n# Plot heatmap for the confusion matrix:\n\nsns.heatmap(cm, cmap='YlOrRd', annot = True, fmt = 'g')\\","1eca02b4":"#Accuracy comparision of various model \nplt.figure(figsize=[8,5],dpi = 100, facecolor='white')\nplt.title('Accuracy Comparison')\nplt.xlabel('Accuracy')\nplt.ylabel('ML Algorithms')\nsns.barplot(x = acc,y = model,palette='bright')\nplt.savefig('plot.png', dpi=300, bbox_inches='tight')\n\n","97be37b7":"#Random Forest proves to have higher accuracy compared to other models","212529b5":"Exploring and visualizing Data ","e163e225":"Featuring the data\n","6cb3408f":"Decision Tree","cc91c09a":"DNN USING KERAS MODEL","c17864df":"Plotting different factors with crops","f7b8d40f":"K-Nearest Neighbors","fbd237f1":"Random Forest Algorithm"}}