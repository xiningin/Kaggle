{"cell_type":{"a3aadbe9":"code","83bf5329":"code","c70875ac":"code","cfb0be6c":"code","5d2a2da9":"code","81a22ce5":"code","c4b08207":"code","fd8559dc":"code","4d7b5bf7":"code","a0997f00":"code","c0d05102":"code","967198aa":"code","9aa1c8eb":"code","947f6675":"code","287c75c3":"code","036ffd92":"code","7f60eae5":"markdown"},"source":{"a3aadbe9":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport os\nfrom tensorflow.keras.layers import *\nfrom tensorflow.keras.preprocessing.text import *\nfrom tensorflow.keras.preprocessing.sequence import *\nfrom tensorflow.keras.models import *\nimport tensorflow.keras.backend as k\nfrom tensorflow.keras.optimizers import *\nfrom sklearn.model_selection import train_test_split,StratifiedKFold\nfrom tensorflow.keras.callbacks import *\nfrom nltk.corpus import *\nfrom nltk.stem import *\nimport string\nfrom sklearn.preprocessing import *\nfrom tqdm import tqdm","83bf5329":"train_df=pd.read_csv('..\/input\/commonlitreadabilityprize\/train.csv')\ntest_df=pd.read_csv('..\/input\/commonlitreadabilityprize\/test.csv')\nsample_sub=pd.read_csv('..\/input\/commonlitreadabilityprize\/sample_submission.csv')","c70875ac":"#Load Word Embeddings\nembedding_path='..\/input\/glove840b300dtxt\/glove.840B.300d.txt'\nembedding_dict={}\nembd_file=open(embedding_path,'r',errors = 'ignore',encoding='utf8')\nfor line in tqdm(embd_file):\n    values=line.split(' ')\n    word=values[0]\n    coef=np.asarray(values[1:],dtype='float32')\n    embedding_dict[word]=coef\nembd_file.close()","cfb0be6c":"#Clean Text\nsp=stopwords.words('english')\nlm=WordNetLemmatizer()\n\ndef clean_text(df):\n    #Remove punctuation\n    print('Cleaning Punctuations')\n    cleaned_text=[txt.translate(str.maketrans('','',string.punctuation)) for txt in df['excerpt']]\n\n    print('Cleaning numbers')\n    cleaned_text=[' '.join([i for i in txt.lower().split() if i.isalpha()]) for txt in cleaned_text]\n\n    print('Cleaning Stopwords')\n    cleaned_text=[' '.join(i for i in txt.split() if i not in sp) for txt in cleaned_text]\n    \n    #Normalize Word\n    print('Word Normalizing')\n    cleaned_text=[' '.join(lm.lemmatize(i) for i in txt.split()) for txt in cleaned_text]\n    \n    return cleaned_text","5d2a2da9":"train_cleaned=clean_text(train_df)","81a22ce5":"maxlen_=500\nmax_words=20000\nprint('Word Tokenization and Transforming')\ntokenizer=Tokenizer(num_words=max_words)\ntokenizer.fit_on_texts(train_cleaned)\nsequences=tokenizer.texts_to_sequences(train_cleaned)\ntrain_data_preped=pad_sequences(sequences,maxlen=maxlen_)\nword_index=tokenizer.word_index\nprint('Tokenization Done!')","c4b08207":"embedding_matrix=np.zeros((max_words,300))\nprint('Loading Embedding Matrix..\\n')\nfor word,ix in tqdm(word_index.items()):\n    if ix<max_words:\n        embed_vec=embedding_dict.get(word)\n        if embed_vec is not None:\n            embedding_matrix[ix]=embed_vec","fd8559dc":"#Split Dataset\nX_train,X_val,y_train,y_val=train_test_split(train_data_preped,train_df['target'],test_size=0.15)\nprint('Size of Train: ',X_train.shape)\nprint('Size of Validation: ',X_val.shape)","4d7b5bf7":"inp=Input(maxlen_)\nx=Embedding(max_words,300)(inp)\nx=Bidirectional(LSTM(256,return_sequences=True))(x)\n\nx=Conv1D(16,5,strides=2,padding='same')(x)\nx=Activation('relu')(x)\n\nx=Conv1D(32,3,strides=2,padding='same')(x)\nx=Activation('relu')(x)\n\nx=Conv1D(64,3,strides=4,padding='same')(x)\nx=Activation('relu')(x)\n\nx=Conv1D(128,3,strides=4,padding='same')(x)\nx=Activation('relu')(x)\n\nx=GRU(256)(x)\nx=Dense(128,activation='relu')(x)\nout=Dense(1)(x)\nmodel=Model(inp,out)\nmodel.summary()","a0997f00":"model.layers[1].set_weights([embedding_matrix])\nmodel.layers[1].trainable=True","c0d05102":"def rmse(y_true, y_pred):\n        return k.sqrt(k.mean(k.square(y_pred - y_true))) \n    \nmodel.compile(loss=rmse,optimizer=RMSprop(0.001))\n\n#Callbacks\nrop=ReduceLROnPlateau(min_lr=0.00000001,patience=10)\nmc=ModelCheckpoint('model.h5',save_freq='epoch')","967198aa":"history=model.fit(X_train,y_train,batch_size=128,epochs=300,validation_data=(X_val,y_val),\n                  callbacks=[mc])","9aa1c8eb":"import matplotlib.pyplot as plt\nloss = history.history['loss']\nval_loss = history.history['val_loss']\nepochs = range(1, len(loss) + 1)\nplt.figure(figsize=(10,5))\nplt.plot(epochs, loss, 'b', color='red', label='Training loss')\nplt.plot(epochs, val_loss, 'b',color='blue', label='Validation loss')\nplt.title('Training and validation loss')\nplt.legend()\nplt.grid()\nplt.show()","947f6675":"claned_test=clean_text(test_df)","287c75c3":"print('Word Tokenization and Transforming of Test data')\ntokenizer=Tokenizer(num_words=max_words)\ntokenizer.fit_on_texts(claned_test)\nsequences=tokenizer.texts_to_sequences(claned_test)\ntest_data_preped=pad_sequences(sequences,maxlen=maxlen_)\nword_index=tokenizer.word_index\nprint('Tokenization Done!')","036ffd92":"model=load_model('model.h5',custom_objects={'rmse': rmse})\npreds=model.predict(test_data_preped)\nsample_sub['target']=preds\nsample_sub.to_csv('submission.csv',index=False)","7f60eae5":"**Test Data**"}}