{"cell_type":{"690b3840":"code","84f70ff5":"code","4f62a1db":"code","1b43cc3c":"code","04cd221c":"code","cc869f64":"code","552c612b":"code","77d608bf":"code","a049c5b9":"code","8814b5c9":"code","e431c7ae":"code","7a0c2110":"code","70cc3869":"code","4034fda4":"code","14d4a8f3":"code","e21a1829":"code","54e05ba3":"code","14066436":"code","20265c81":"code","b4179076":"code","ab142569":"code","9da37a84":"code","e077f856":"code","a7b0da71":"code","bc1d820b":"code","1a878911":"code","40c30b68":"code","77fa1353":"code","ae9f0353":"code","f53c374f":"code","5e1e2140":"code","3e941243":"code","7df10283":"code","afb1a4fd":"code","1946e613":"code","23b28921":"code","df444134":"code","f9d9473b":"code","605b068f":"code","2280e4e7":"code","6bc33b65":"code","1a0a76c8":"code","3564394c":"code","f41e9a56":"code","72a4ee53":"code","c0088757":"code","869744d9":"code","266180e7":"markdown","2ebeba8b":"markdown","60fa6939":"markdown","9284f32c":"markdown","2d7a9c1e":"markdown","d3023323":"markdown","c11b98e9":"markdown","0bdc8d03":"markdown","1eec876f":"markdown","114f0266":"markdown","166c172c":"markdown","25640c0c":"markdown","24790569":"markdown","f00e21ef":"markdown","3afbc27c":"markdown","5815a9a5":"markdown","4f33fb8a":"markdown","f5ebae38":"markdown","7a16daee":"markdown"},"source":{"690b3840":"# Reference link:\n# https:\/\/www.kaggle.com\/gordotron85\/future-sales-xgboost-top-3\n# https:\/\/www.kaggle.com\/szhou42\/predict-future-sales-top-11-solution\n# https:\/\/www.kaggle.com\/pavansanagapati\/14-simple-tips-to-save-ram-memory-for-1-gb-dataset","84f70ff5":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport xgboost as xgb\nimport time\n\nfrom math import sqrt\nfrom numpy import loadtxt\nfrom itertools import product\nfrom tqdm import tqdm\nfrom sklearn import preprocessing\nfrom xgboost import plot_tree\nfrom matplotlib import pyplot\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import KFold\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.preprocessing import StandardScaler\n\nimport gc\n\nkernel_with_output = False # use to contro the code\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input\/'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","4f62a1db":"print(\"Loading data ... ...\")\n\nsales_train = pd.read_csv('\/kaggle\/input\/competitive-data-science-predict-future-sales\/sales_train.csv')\nitems = pd.read_csv('\/kaggle\/input\/competitive-data-science-predict-future-sales\/items.csv')\nshops = pd.read_csv('\/kaggle\/input\/competitive-data-science-predict-future-sales\/shops.csv')\nitem_categories = pd.read_csv('\/kaggle\/input\/competitive-data-science-predict-future-sales\/item_categories.csv')\ntest = pd.read_csv('\/kaggle\/input\/competitive-data-science-predict-future-sales\/test.csv')\nsample_submission = pd.read_csv('\/kaggle\/input\/competitive-data-science-predict-future-sales\/sample_submission.csv')\n\nprint('Done.')","1b43cc3c":"# Remove duplicate of train data\nfrom sklearn.preprocessing import LabelEncoder\n\nprint('Data processing ...')\n\nsubset = ['date','date_block_num','shop_id','item_id','item_cnt_day']\nprint(sales_train.duplicated(subset=subset).value_counts())\nsales_train.drop_duplicates(subset=subset, inplace=True)\n\n\n# Drop outlier\nsales_train.drop(sales_train[sales_train['item_price']>300000].index, inplace=True)\nsales_train.drop(sales_train[sales_train['item_cnt_day']>1000].index, inplace=True)\n\nsales_train = sales_train[sales_train.item_price > 0].reset_index(drop = True)\n\nsales_train.loc[sales_train.item_cnt_day < 1, \"item_cnt_day\"] = 0\n\n'''\n# clean shop data\n# \u042f\u043a\u0443\u0442\u0441\u043a \u041e\u0440\u0434\u0436\u043e\u043d\u0438\u043a\u0438\u0434\u0437\u0435, 56\nsales_train.loc[sales_train.shop_id == 0, 'shop_id'] = 57\ntest.loc[test.shop_id == 0, 'shop_id'] = 57\n# \u042f\u043a\u0443\u0442\u0441\u043a \u0422\u0426 \"\u0426\u0435\u043d\u0442\u0440\u0430\u043b\u044c\u043d\u044b\u0439\"\nsales_train.loc[sales_train.shop_id == 1, 'shop_id'] = 58\ntest.loc[test.shop_id == 1, 'shop_id'] = 58\n# \u0416\u0443\u043a\u043e\u0432\u0441\u043a\u0438\u0439 \u0443\u043b. \u0427\u043a\u0430\u043b\u043e\u0432\u0430 39\u043c\u00b2\nsales_train.loc[sales_train.shop_id == 10, 'shop_id'] = 11\ntest.loc[test.shop_id == 10, 'shop_id'] = 11\n\nprint('Done.')\n'''","04cd221c":"# drop shops&items not in test data: this doesn't help\n# test_shops = test.shop_id.unique()\n# test_items = test.item_id.unique()\n\n# sales_train = sales_train[sales_train.shop_id.isin(test_shops)]\n# sales_train = sales_train[sales_train.item_id.isin(test_items)]\n\n# print('train:', sales_train.shape)","cc869f64":"print(\"Creating grid ... ...\")\n\n# Create grid of all combinations of shops\/items from that month\ngrid = []\nfor block_num in sales_train['date_block_num'].unique():\n    cur_shops = sales_train[sales_train['date_block_num']==block_num]['shop_id'].unique()\n    cur_items = sales_train[sales_train['date_block_num']==block_num]['item_id'].unique()\n    grid.append(np.array(list(product(*[cur_shops, cur_items, [block_num]])),dtype='int32'))\n    \nindex_cols = ['shop_id', 'item_id', 'date_block_num']\ngrid = pd.DataFrame(np.vstack(grid), columns = index_cols,dtype=np.int32)\n\nprint('Done.')","552c612b":"print(\"Aggregating and merging ... ...\")\n\n# trim the daily sales: why 20?\nsales_train['item_cnt_day'] = sales_train['item_cnt_day'].clip(0,20)\n\n# Group trian data; and aggregate daily sales (sum) and daily price (mean)\ngroups = sales_train.groupby(['shop_id', 'item_id', 'date_block_num'])\n\ntrainset = groups.agg({'item_cnt_day':'sum', 'item_price':'mean'}).reset_index()\n# Rename the column names to make more sense\ntrainset = trainset.rename(columns = {'item_cnt_day' : 'item_cnt_month'})\n\n# trim the monthl sales: again, why 20?\ntrainset['item_cnt_month'] = trainset['item_cnt_month'].clip(0,20)\n\n\n# merge the grid and trainset\ntrainset = pd.merge(grid,trainset,how='left',on=index_cols)\ntrainset.item_cnt_month = trainset.item_cnt_month.fillna(0)\n\n# merge category id\ntrainset = pd.merge(trainset, items[['item_id', 'item_category_id']], on = 'item_id')\n\nprint(\"Writing trainset to csv ... ... \")\ntrainset.to_csv('trainset_with_grid.csv')\n\nprint('Done.')","77d608bf":"# free some memory\ndel grid\ndel sales_train\ndel groups\ndel trainset\n\ngc.collect()","a049c5b9":"# test_size_mb = trainset.memory_usage().sum() \/ 1024 \/ 1024\n# print(\"Test memory size: %.2f MB\" % test_size_mb)","8814b5c9":"# some variables;\n# Set seeds and options\nnp.random.seed(10)\npd.set_option('display.max_rows', 231)\npd.set_option('display.max_columns', 100)\n\n# Feature engineering list\nnew_features = []\n# enable_feature_idea = [True, True, True, True, True, True, True, True, True, True]\n\n# Some parameters(maybe add more periods, score will be better) [1,2,3,12]\nlookback_range = [1,2,3,4,5,6,7,8,9,10,11,12]\n\ntqdm.pandas()","e431c7ae":"# Load train data; convert data type; take the useful base features;\nprint('Loading train data ...')\n\n# Load data\ntrainset = pd.read_csv('\/kaggle\/working\/trainset_with_grid.csv')\n\nitems = pd.read_csv('\/kaggle\/input\/competitive-data-science-predict-future-sales\/items.csv')\nshops = pd.read_csv('\/kaggle\/input\/competitive-data-science-predict-future-sales\/shops.csv')\n\n# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n# Clean shop data\n\n# \u042f\u043a\u0443\u0442\u0441\u043a \u041e\u0440\u0434\u0436\u043e\u043d\u0438\u043a\u0438\u0434\u0437\u0435, 56\ntrainset.loc[trainset.shop_id == 0, 'shop_id'] = 57\n# \u042f\u043a\u0443\u0442\u0441\u043a \u0422\u0426 \"\u0426\u0435\u043d\u0442\u0440\u0430\u043b\u044c\u043d\u044b\u0439\"\ntrainset.loc[trainset.shop_id == 1, 'shop_id'] = 58\n# \u0416\u0443\u043a\u043e\u0432\u0441\u043a\u0438\u0439 \u0443\u043b. \u0427\u043a\u0430\u043b\u043e\u0432\u0430 39\u043c\u00b2\ntrainset.loc[trainset.shop_id == 10, 'shop_id'] = 11\n\n\n# Clean\/Add shop city and category\nshops.loc[ shops.shop_name == '\u0421\u0435\u0440\u0433\u0438\u0435\u0432 \u041f\u043e\u0441\u0430\u0434 \u0422\u0426 \"7\u042f\"',\"shop_name\" ] = '\u0421\u0435\u0440\u0433\u0438\u0435\u0432\u041f\u043e\u0441\u0430\u0434 \u0422\u0426 \"7\u042f\"'\n\ncity = shops.shop_name.apply(lambda x: str.replace(x, '!', '')).apply(lambda x: x.split(' ')[0])\nshops['city'] = pd.Categorical(city).codes\n\nshops[\"category\"] = shops.shop_name.str.split(\" \").map( lambda x: x[1] )\nshops[\"shop_category\"] = LabelEncoder().fit_transform( shops.category )\n\nshops.loc[shops.city == \"!\u042f\u043a\u0443\u0442\u0441\u043a\", \"city\"] = \"\u042f\u043a\u0443\u0442\u0441\u043a\"\n\n# Only keep shop category if there are 5 or more shops of that category, the rest are grouped as \"other\".\ncategory = []\nfor cat in shops.category.unique():\n    if len(shops[shops.category == cat]) >= 5:\n        category.append(cat)\nshops.category = shops.category.apply( lambda x: x if (x in category) else \"other\" )\n\nfrom sklearn.preprocessing import LabelEncoder\nshops[\"shop_category\"] = LabelEncoder().fit_transform( shops.category )\n\nshops = shops[[\"shop_id\", \"shop_category\", \"city\"]]\n\n# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n# Add revenue to train data;\ntrainset[\"revenue\"] = trainset[\"item_cnt_month\"] * trainset[\"item_price\"]\n\n\n# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n# Add city info to train data;\ntrainset = pd.merge(trainset, shops[['shop_id', 'city']], on = 'shop_id')\n\n# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\nprint('Converting data type ...')\n\ntrainset['shop_id'] = trainset['shop_id'].astype('int16')\ntrainset['item_id'] = trainset['item_id'].astype('int16')\ntrainset['date_block_num'] = trainset['date_block_num'].astype('int16')\ntrainset['item_cnt_month'] = trainset['item_cnt_month'].astype('int16')\ntrainset['item_price'] = trainset['item_price'].astype('float32')\ntrainset['item_category_id'] = trainset['item_category_id'].astype('int16')\ntrainset['revenue'] = trainset['revenue'].astype('float32')\ntrainset['city'] = trainset['city'].astype('int16')\n\n# Take the data with feature set\n# And choose data within months (?)\nstart_month = 0\nend_month = 33\n\ntrainset = trainset[['shop_id', 'item_id', 'item_category_id', 'date_block_num', 'item_price', 'item_cnt_month', 'revenue', 'city']]\n\ntrainset = trainset[(trainset.date_block_num >= start_month) & (trainset.date_block_num <= end_month)]\n\nprint('Done.')","7a0c2110":"trainset.head()","70cc3869":"# Load test data; merge test data;\nprint('Loading test set...')\n\ntest_dataset = loadtxt('\/kaggle\/input\/competitive-data-science-predict-future-sales\/test.csv', delimiter=\",\" ,skiprows=1, usecols = (1,2), dtype=int)\ntestset = pd.DataFrame(test_dataset, columns = ['shop_id', 'item_id'])\n\n\n# Merge test data\nprint('Merging with other datasets...')\n\n# Get item category id into test_df\ntestset = testset.merge(items[['item_id', 'item_category_id']], on = 'item_id', how = 'left')\n\ntestset['date_block_num'] = 34\n    \n# Make testset contains same column as trainset so we can concatenate them row-wise\ntestset['item_cnt_month'] = -1\n\n# Merge city info;\ntestset = pd.merge(testset,shops[['shop_id','city']],how='left',on='shop_id')\n\n# \u042f\u043a\u0443\u0442\u0441\u043a \u041e\u0440\u0434\u0436\u043e\u043d\u0438\u043a\u0438\u0434\u0437\u0435, 56\ntestset.loc[testset.shop_id == 0, 'shop_id'] = 57\n# \u042f\u043a\u0443\u0442\u0441\u043a \u0422\u0426 \"\u0426\u0435\u043d\u0442\u0440\u0430\u043b\u044c\u043d\u044b\u0439\"\ntestset.loc[testset.shop_id == 1, 'shop_id'] = 58\n# \u0416\u0443\u043a\u043e\u0432\u0441\u043a\u0438\u0439 \u0443\u043b. \u0427\u043a\u0430\u043b\u043e\u0432\u0430 39\u043c\u00b2\ntestset.loc[testset.shop_id == 10, 'shop_id'] = 11\n\nprint('Done.')","4034fda4":"# Combine train\/test data: concatenate;\n# Notice: test data has NaN in column item_price;\ntrain_test_set = pd.concat([trainset, testset], axis = 0) \n\n# Check the length of data;\nlen(trainset)+len(testset) == len(train_test_set)","14d4a8f3":"# free memory\ndel trainset\ndel testset\ndel test_dataset\n\ngc.collect()","e21a1829":"item_cat = pd.read_csv('\/kaggle\/input\/competitive-data-science-predict-future-sales\/item_categories.csv')\n\n# Let's use the upper level item category names;\nitem_cat.head()","54e05ba3":"# Translate the item categoty group names to english;\nl_cat = list(item_cat.item_category_name)\n\nfor ind in range(0,1):\n    l_cat[ind] = 'PC Headsets \/ Headphones'\nfor ind in range(1,8):\n    l_cat[ind] = 'Access'\nl_cat[8] = 'Tickets (figure)'\nl_cat[9] = 'Delivery of goods'\nfor ind in range(10,18):\n    l_cat[ind] = 'Consoles'\nfor ind in range(18,25):\n    l_cat[ind] = 'Consoles Games'\nl_cat[25] = 'Accessories for games'\nfor ind in range(26,28):\n    l_cat[ind] = 'phone games'\nfor ind in range(28,32):\n    l_cat[ind] = 'CD games'\nfor ind in range(32,37):\n    l_cat[ind] = 'Card'\nfor ind in range(37,43):\n    l_cat[ind] = 'Movie'\nfor ind in range(43,55):\n    l_cat[ind] = 'Books'\nfor ind in range(55,61):\n    l_cat[ind] = 'Music'\nfor ind in range(61,73):\n    l_cat[ind] = 'Gifts'\nfor ind in range(73,79):\n    l_cat[ind] = 'Soft'\nfor ind in range(79,81):\n    l_cat[ind] = 'Office'\nfor ind in range(81,83):\n    l_cat[ind] = 'Clean'\nl_cat[83] = 'Elements of a food'\n\n\n# Encode the information\nlb = preprocessing.LabelEncoder()\n\n\n# Now add item categoty group names and id;\nitem_cat['item_category_id_fix'] = lb.fit_transform(l_cat)\nitem_cat['item_category_name_fix'] = l_cat\n\n","14066436":"# Merge info to train\/test\ntrain_test_set = train_test_set.merge(item_cat[['item_category_id', 'item_category_id_fix']], on = 'item_category_id', how = 'left')\n\n\n# Drop item category id; rename column;\n_ = train_test_set.drop(['item_category_id'],axis=1, inplace=True)\ntrain_test_set.rename(columns = {'item_category_id_fix':'item_category_id'}, inplace = True)\n\n# Drop item_cat columns redundant info;\n_ = item_cat.drop(['item_category_id'],axis=1, inplace=True)\n_ = item_cat.drop(['item_category_name'],axis=1, inplace=True)\n\n# Rename and drop ducplicated info;\nitem_cat.rename(columns = {'item_category_id_fix':'item_category_id'}, inplace = True)\nitem_cat.rename(columns = {'item_category_name_fix':'item_category_name'}, inplace = True)\nitem_cat = item_cat.drop_duplicates()\nitem_cat.index = np.arange(0, len(item_cat))","20265c81":"# So far, train and test are in same df: because we did the item category group info, and also\n# will add new features (especially lag info)","b4179076":"# Convert data type to save memory;\ntrain_test_set['shop_id'] = train_test_set['shop_id'].astype('int16')\ntrain_test_set['item_id'] = train_test_set['item_id'].astype('int16')\ntrain_test_set['date_block_num'] = train_test_set['date_block_num'].astype('int16')\ntrain_test_set['item_price'] = train_test_set['item_price'].astype('float32')\ntrain_test_set['item_cnt_month'] = train_test_set['item_cnt_month'].astype('int16')\ntrain_test_set['revenue'] = train_test_set['revenue'].astype('float32')\ntrain_test_set['city'] = train_test_set['city'].astype('int16')\ntrain_test_set.info()","ab142569":"item_cat.head()","9da37a84":"del lb\ngc.collect()","e077f856":"\n# Add prev month sales as new features;\nlookback_range = [1,2,3]\n\nfor diff in tqdm(lookback_range):\n    feature_name = 'prev_shopitem_sales_' + str(diff)\n    \n    trainset2 = train_test_set.copy()\n    trainset2.loc[:, 'date_block_num'] += diff\n    \n    trainset2.rename(columns={'item_cnt_month': feature_name}, inplace=True)\n    \n    train_test_set = train_test_set.merge(trainset2[['shop_id', 'item_id', 'date_block_num', feature_name]], on = ['shop_id', 'item_id', 'date_block_num'], how = 'left')\n    train_test_set[feature_name] = train_test_set[feature_name].fillna(0)\n    \n    new_features.append(feature_name)\n    \n    # free memory\n    del trainset2\n    \n# Save some memory\ntrain_test_set[new_features] = train_test_set[new_features].astype('int16')\ngc.collect()\n","a7b0da71":"'''\n# YG: try add mean, max, min of last 12 months sales;\n\ntrain_test_set['sales_mean'] = train_test_set[new_features].mean()\ntrain_test_set['sales_max'] = train_test_set[new_features].max()\ntrain_test_set['sales_min'] = train_test_set[new_features].min()\n\ntrain_test_set['sales_mean'] = train_test_set['sales_mean'].fillna(0)\ntrain_test_set['sales_min'] = train_test_set['sales_min'].fillna(0)\ntrain_test_set['sales_max'] = train_test_set['sales_max'].fillna(0)\n\ntrain_test_set['sales_mean'] = train_test_set['sales_mean'].astype('int16')\ntrain_test_set['sales_min'] = train_test_set['sales_min'].astype('int16')\ntrain_test_set['sales_max'] = train_test_set['sales_max'].astype('int16')\n\nnew_features.append('sales_mean')\nnew_features.append('sales_max')\nnew_features.append('sales_min')\n'''","bc1d820b":"\nlookback_range = [1,2,3]\n\ngroups = train_test_set.groupby(by = ['item_id', 'date_block_num'])\n\nfor diff in tqdm(lookback_range):\n    feature_name = 'prev_item_sales_' + str(diff)\n            \n    result = groups.agg({'item_cnt_month':'mean'})\n    result = result.reset_index()\n    result.loc[:, 'date_block_num'] += diff\n    result.rename(columns={'item_cnt_month': feature_name}, inplace=True)\n    \n    train_test_set = train_test_set.merge(result, on = ['item_id', 'date_block_num'], how = 'left')\n    train_test_set[feature_name] = train_test_set[feature_name].fillna(0)\n    \n    # YG\n    train_test_set[feature_name] = train_test_set[feature_name].astype('int16')\n    \n    new_features.append(feature_name)    \n    \n    del result\n    \n# sae some memory\ndel groups\ngc.collect()\n","1a878911":"'''\nlookback_range = [1,2,3]\n\nprint('Adding shop\/item lagged prices features ...')\n\ngroups = train_test_set.groupby(by = ['shop_id', 'item_id', 'date_block_num'])\n\nfor diff in tqdm(lookback_range):\n    feature_name = 'prev_shopitem_price_' + str(diff)\n    result = groups.agg({'item_price':'mean'})\n    result = result.reset_index()\n    result.loc[:, 'date_block_num'] += diff\n    result.rename(columns={'item_price': feature_name}, inplace=True)\n    \n    train_test_set = train_test_set.merge(result, on = ['shop_id', 'item_id', 'date_block_num'], how = 'left')\n    train_test_set[feature_name] = train_test_set[feature_name].fillna(0)\n    new_features.append(feature_name)        \n    \n    del result\n    \n# Save some memory\ndel groups\ngc.collect()\n\nprint('Done.')\n'''","40c30b68":"lookback_range = [1,2,3]\n\n# Add prev month price as new features;\n\nfor diff in tqdm(lookback_range):\n    feature_name = 'prev_shopitem_price_' + str(diff)\n    \n    trainset2 = train_test_set.copy()\n    trainset2.loc[:, 'date_block_num'] += diff\n    \n    trainset2.rename(columns={'item_price': feature_name}, inplace=True)\n    \n    train_test_set = train_test_set.merge(trainset2[['shop_id', 'item_id', 'date_block_num', feature_name]], on = ['shop_id', 'item_id', 'date_block_num'], how = 'left')\n    train_test_set[feature_name] = train_test_set[feature_name].fillna(0)\n    \n    new_features.append(feature_name)\n        \n    # free memory\n    del trainset2\n\ngc.collect()\n","77fa1353":"lookback_range = [1,2,3]\n\nprint('Adding item lagged price features ... ')\n\ngroups = train_test_set.groupby(by = ['item_id', 'date_block_num'])\n        \nfor diff in tqdm(lookback_range):\n    feature_name = 'prev_item_price_' + str(diff)\n    \n    result = groups.agg({'item_price':'mean'})\n    result = result.reset_index()\n    result.loc[:, 'date_block_num'] += diff\n    result.rename(columns={'item_price': feature_name}, inplace=True)\n    \n    train_test_set = train_test_set.merge(result, on = ['item_id', 'date_block_num'], how = 'left')\n    train_test_set[feature_name] = train_test_set[feature_name].fillna(0)\n    \n    # YG\n    train_test_set[feature_name] = train_test_set[feature_name].astype('float32')\n    \n    new_features.append(feature_name)    \n    \n    del result\n    \n# Save some memory\ndel groups\ngc.collect()\n\nprint('Done.')\n","ae9f0353":"\nprint('Adding # of month from last sale feature ...')\n\nlookback_range = [1,2,3,4,5,6,7,8,9,10,11,12]\n\ndef create_last_sale_shop_item(row):\n    for diff in range(1,33+1):\n        feature_name = '_prev_shopitem_sales_' + str(diff)\n        if row[feature_name] != 0.0:\n            return diff\n    return np.nan\n\nlookback_range = list(range(1, 33 + 1))\n\nfor diff in tqdm(lookback_range):\n    feature_name = '_prev_shopitem_sales_' + str(diff)\n    trainset2 = train_test_set.copy()\n    trainset2.loc[:, 'date_block_num'] += diff\n    trainset2.rename(columns={'item_cnt_month': feature_name}, inplace=True)\n    train_test_set = train_test_set.merge(trainset2[['shop_id', 'item_id', 'date_block_num', feature_name]], on = ['shop_id', 'item_id', 'date_block_num'], how = 'left')\n    train_test_set[feature_name] = train_test_set[feature_name].fillna(0)\n    \n    train_test_set[feature_name] = train_test_set[feature_name].astype('int16')\n    del trainset2\n\ntrain_test_set.loc[:, 'last_sale_shop_item'] = train_test_set.progress_apply (lambda row: create_last_sale_shop_item(row),axis=1)\nnew_features.append('last_sale_shop_item')\n\ngc.collect()\n\nprint('Done.')\n","f53c374f":"'''\nprint('Add text features ...')\n\nitems_subset = items[['item_id', 'item_name']]\nfeature_count = 25\n\ntfidf = TfidfVectorizer(max_features=feature_count)\nitems_df_item_name_text_features = pd.DataFrame(tfidf.fit_transform(items_subset['item_name']).toarray())\n\ncols = items_df_item_name_text_features.columns\nfor i in range(feature_count):\n\n    feature_name = 'item_name_tfidf_' + str(i)\n    items_subset[feature_name] = items_df_item_name_text_features[cols[i]]\n    \n    # YG\n    items_subset[feature_name] = items_subset[feature_name].astype('int16')\n    \n    new_features.append(feature_name)\n\n    \nitems_subset.drop('item_name', axis = 1, inplace = True)\ntrain_test_set = train_test_set.merge(items_subset, on = 'item_id', how = 'left')\n\n# Save some memory\ndel items_subset\ndel items_df_item_name_text_features\n\ngc.collect()\n\nprint('Done.')\n'''","5e1e2140":"group = train_test_set.groupby( [\"date_block_num\",\"shop_id\"] ).agg({\"revenue\": [\"sum\"] })\ngroup.columns = [\"date_shop_revenue\"]\ngroup.reset_index(inplace = True)\n\ntrain_test_set = train_test_set.merge( group , on = [\"date_block_num\", \"shop_id\"], how = \"left\" )\ntrain_test_set['date_shop_revenue'] = train_test_set['date_shop_revenue'].astype(np.float32)\n\ngroup = group.groupby([\"shop_id\"]).agg({ \"date_block_num\":[\"mean\"] })\ngroup.columns = [\"shop_avg_revenue\"]\ngroup.reset_index(inplace = True )\n\ntrain_test_set = train_test_set.merge( group, on = [\"shop_id\"], how = \"left\" )\ntrain_test_set[\"shop_avg_revenue\"] = train_test_set.shop_avg_revenue.astype(np.float32)\ntrain_test_set[\"delta_revenue\"] = (train_test_set['date_shop_revenue'] - train_test_set['shop_avg_revenue']) \/ train_test_set['shop_avg_revenue']\ntrain_test_set[\"delta_revenue\"] = train_test_set[\"delta_revenue\"]. astype(np.float32)\n\n# shift 1 month\nfeature_name = 'delta_revenue_lag_1'\ntrainset2 = train_test_set.copy()\ntrainset2.loc[:, 'date_block_num'] += 1\ntrainset2.rename(columns={'delta_revenue': feature_name}, inplace=True)\n\ntrain_test_set = train_test_set.merge(trainset2[['shop_id', 'item_id', 'date_block_num', feature_name]], on = ['shop_id', 'item_id', 'date_block_num'], how = 'left')\ntrain_test_set[feature_name] = train_test_set[feature_name].fillna(0)\n    \ntrain_test_set[feature_name] = train_test_set[feature_name].astype('float32')\ntrain_test_set.drop( [\"date_shop_revenue\", \"shop_avg_revenue\", \"delta_revenue\"] ,axis = 1, inplace = True)\n\n# Scale the revenue\ntrain_test_set[feature_name] = (train_test_set[feature_name] - train_test_set[feature_name].mean()) \/ (train_test_set[feature_name].max()-train_test_set[feature_name].min())\n\ndel trainset2\ndel group\ngc.collect()\n\nnew_features.append(feature_name)","3e941243":"# Fill nan first\ntrain_test_set = train_test_set.fillna(0)\ntrain_test_set.isnull().values.any()\n\n# Let's use the last month as validation;\n\nbaseline_features = ['shop_id', 'item_id', 'item_category_id', 'date_block_num', 'city'] +  new_features + ['item_cnt_month']","7df10283":"# ~~~~~~~~~~~~~~~~~~~~~ Test scaling ~~~~~~~~~~~~~~~~~~~~~~\n# scaler = StandardScaler()\n# train_test_set[new_features] = scaler.fit_transform(train_test_set[new_features])","afb1a4fd":"\n# Set up train\/val\/test data;\n\nprint('Set up train\/val\/test data ...')\n\n# Clipping to range 0-20\ntrain_test_set['item_cnt_month'] = train_test_set.item_cnt_month.fillna(0).clip(0,20)\n\n# train: want rows with date_block_num from 0 to 31\ntrain_time_range_lo = (train_test_set['date_block_num'] >= 0)\ntrain_time_range_hi =  (train_test_set['date_block_num'] <= 32)\n\n# val: want rows with date_block_num from 33\nvalidation_time =  (train_test_set['date_block_num'] == 33)\n\n# test: want rows with date_block_num from 34\ntest_time =  (train_test_set['date_block_num'] == 34)\n\n\n# Retrieve rows for train set, val set, test set\ncv_trainset = train_test_set[train_time_range_lo & train_time_range_hi]\n\ncv_valset = train_test_set[validation_time]\ncv_trainset = cv_trainset[baseline_features]\ncv_valset = cv_valset[baseline_features]\n\ntestset = train_test_set[test_time]\ntestset = testset[baseline_features]\n\nprint('Done.')\n","1946e613":"# free memory\ndel train_test_set\ndel train_time_range_lo\ndel train_time_range_hi\ndel validation_time\ndel test_time\n\ngc.collect()","23b28921":"#https:\/\/scikit-learn.org\/stable\/auto_examples\/feature_selection\/plot_select_from_model_diabetes.html#sphx-glr-auto-examples-feature-selection-plot-select-from-model-diabetes-py\n#https:\/\/scikit-learn.org\/stable\/modules\/feature_selection.html\n#https:\/\/towardsdatascience.com\/feature-selection-using-regularisation-a3678b71e499","df444134":"'''\n# Load library\n# from sklearn.linear_model import Lasso, LogisticRegression\nfrom sklearn.feature_selection import SelectFromModel\nfrom sklearn.linear_model import LassoCV\nfrom sklearn import linear_model\n'''","f9d9473b":"# Take part of data to get most important features; (memory limit)\n# cv_trainset = cv_trainset[cv_trainset['date_block_num']>15]\n# cv_trainset['date_block_num'].unique()","605b068f":"'''\n# Construct train set and targets;\ncv_trainset_vals = cv_trainset.values\n# del cv_trainset\n\ntrainx = cv_trainset_vals[:, 0:len(baseline_features) - 1]\ntrainy = cv_trainset_vals[:, len(baseline_features) - 1]\n\ndel cv_trainset_vals\n\n\n# Method 1\n# Rank the importance of feature;\nclf = LassoCV(alphas=[.005]).fit(trainx, trainy)\nimportance = np.abs(clf.coef_)\n\nprint(importance)\n\nfigure = plt.figure()\nplt.plot(importance)\nplt.show()\n\n# Method 2\n# Use Lasso to select features;\nclf = linear_model.Lasso(alpha=0.005).fit(trainx, trainy)\n\nmodel = SelectFromModel(clf, prefit=True)\n\n# Visualize which features are selected\nmodel.get_support()\n\n# See the names of selected features\n# selected_feat = cv_trainset.columns[(model.get_support())]\n\n'''","2280e4e7":"# Prepare numpy arrays for training\/val\/test\n# cv_trainset_vals = cv_trainset.values.astype(int)\n\ncv_trainset_vals = cv_trainset.values\ntrainx = cv_trainset_vals[:, 0:len(baseline_features) - 1]\ntrainy = cv_trainset_vals[:, len(baseline_features) - 1]\n\ndel cv_trainset\ndel cv_trainset_vals","6bc33b65":"# cv_valset_vals = cv_valset.values.astype(int)\n\ncv_valset_vals = cv_valset.values\nvalx = cv_valset_vals[:, 0:len(baseline_features) - 1]\nvaly = cv_valset_vals[:, len(baseline_features) - 1]\n\ndel cv_valset\ndel cv_valset_vals","1a0a76c8":"# testset_vals = testset.values.astype(int)\n\ntestset_vals = testset.values\ntestx = testset_vals[:, 0:len(baseline_features) - 1]\n\ndel testset\ndel testset_vals","3564394c":"gc.collect()","f41e9a56":"# Fitting the model\nprint('Fitting...')\nmodel = xgb.XGBRegressor(\n    max_depth = 11, \n    min_child_weight=0.5, \n    subsample = 1, \n    eta = 0.3, \n    num_round = 1000, \n    seed = 1, \n    nthread = 16)\n\nmodel.fit(trainx, trainy, eval_metric='rmse')","72a4ee53":"# Use validation data to test\npreds = model.predict(valx)\n# Clipping to range 0-20\npreds = np.clip(preds, 0,20)\nprint('val set rmse: ', sqrt(mean_squared_error(valy, preds)))","c0088757":"# predict test data and generate submission file\npreds = model.predict(testx)\n    \n# Clipping to range 0-20\npreds = np.clip(preds, 0,20)\ndf = pd.DataFrame(preds, columns = ['item_cnt_month'])\ndf['ID'] = df.index\ndf = df.set_index('ID')\n\ndf.to_csv('test_preds.csv')\nprint('test predictions written to file')","869744d9":"from xgboost import plot_importance\n\ndef plot_features(booster, figsize):    \n    fig, ax = plt.subplots(1,1,figsize=figsize)\n    return plot_importance(booster=booster, ax=ax)\n\nplot_features(model, (10,14))","266180e7":"# **Data Loading**","2ebeba8b":"New feature: test 6 \n\nUse text info;","60fa6939":"Select feature and Organize data","9284f32c":"# **Train\/Test Data Setup**","2d7a9c1e":"New feature: test 3\n\nUse lagged 12 month price -- YG","d3023323":"New  feature: test 4 \n \nUse lagged 12 month item price","c11b98e9":"# **XGBoost Model**","0bdc8d03":"New feature: test 1\n\nUse lagged 12 month item sales;","1eec876f":"New feature: test 0\n\nUse lagged 12 month shop\/item sales;","114f0266":"new feature: test 7 \n\ndelta revenue","166c172c":"# **Preprocessing**","25640c0c":"# **Feature Engineering**","24790569":"Predict","f00e21ef":"New feature: test 5 \n\nUse Number of month from last sale of shop\/item","3afbc27c":"# **Add New Features**","5815a9a5":"# **Feature Selection**","4f33fb8a":"# **Feature Importance Analysis**","f5ebae38":"New feature: test 2\n\nUse lagged shop\/item price","7a16daee":"# **Create grid, aggregate data**"}}