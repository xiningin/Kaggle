{"cell_type":{"de834d13":"code","04ef6a94":"code","01260ffd":"code","b0dfb324":"code","ba3822c4":"code","c39053ae":"code","065315fd":"code","3dbb3870":"markdown","65d6a39f":"markdown","dfc18d6c":"markdown","a6789648":"markdown","7f94f0e1":"markdown","a5e63032":"markdown","0c48df63":"markdown","d7d20634":"markdown","f454fe39":"markdown"},"source":{"de834d13":"import numpy as np \nimport pandas as pd \n\ndf = pd.read_csv(\"..\/input\/wine-quality-dataset\/WineQT.csv\")\ndf.dtypes","04ef6a94":"df.describe()","01260ffd":"from sklearn import preprocessing\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import PolynomialFeatures\n\nX = df.iloc[:,:-2]\ny = df.iloc[:,-2]\n\ntrans = PolynomialFeatures(degree=2)\nX_poly = trans.fit_transform(X)\n\nX_train, X_test, y_train, y_test = train_test_split(X_poly, y, test_size=0.1, random_state=0) \n\nscaler = preprocessing.StandardScaler().fit(X_poly)\nX_poly_scaled = scaler.transform(X_poly) \n\nscaler = preprocessing.StandardScaler().fit(X_train)\nX_train = scaler.transform(X_train)\nX_test = scaler.transform(X_test)\n\nscaler_y = preprocessing.StandardScaler().fit(y_train.values.reshape(-1, 1))\ny_train_scaled = scaler_y.transform(y_train.values.reshape(-1, 1))\ny_test_scaled = scaler_y.transform(y_test.values.reshape(-1, 1))","b0dfb324":"from sklearn import linear_model\n\nclf = linear_model.LinearRegression()\nclf.fit(X_train,y_train)\nprint(clf.score(X_train,y_train))\nprint(clf.score(X_test,y_test))","ba3822c4":"clf = linear_model.LinearRegression()\nclf.fit(X,y)\ncdf = pd.concat([pd.DataFrame(X.columns),pd.DataFrame(np.transpose(clf.coef_))], axis = 1)\ncdf = cdf.set_axis(['Name', 'Value'], axis=1, inplace=False)\ncdf = cdf.sort_values(by=['Value'], ascending=False)\ncdf","c39053ae":"from sklearn.ensemble import RandomForestRegressor, AdaBoostRegressor, ExtraTreesRegressor, BaggingRegressor\nfrom sklearn import svm\n\ndef fit_and_score(model, name):\n    model.fit(X_train, y_train)\n    print(name + \" train score: \" + str(model.score(X_train,y_train)))\n    print(name + \" test score: \" + str(model.score(X_test,y_test)))\n    \nfit_and_score(RandomForestRegressor(random_state=0), \"RandomForestRegressor\")\nfit_and_score(ExtraTreesRegressor(random_state=0), \"ExtraTreesRegressor\")\nfit_and_score(AdaBoostRegressor(random_state=0), \"AdaBoostRegressor\")\nfit_and_score(BaggingRegressor(random_state=0), \"BaggingRegressor\")\nfit_and_score(svm.SVR(), \"SVR\")\n","065315fd":"from mlxtend.regressor import StackingCVRegressor\nfrom sklearn.model_selection import cross_val_score\n\nrfr = RandomForestRegressor(random_state=0)\netr = ExtraTreesRegressor(random_state=0)\nbgr = BaggingRegressor(random_state=0)\nstack = StackingCVRegressor(regressors=(rfr, etr, bgr),\n                            meta_regressor=etr,\n                            random_state=0)\n\nprint('5-fold cross validation scores:\\n')\n\nfor clf, label in zip([rfr, etr, bgr, stack], ['Random Forest', \n                                                'ExtraTreesRegressor', \n                                                'BaggingRegressor', \n                                                'Stacked']):\n    scores = cross_val_score(clf, X_poly_scaled, y, cv=4)\n    print(\"R^2 Score: %0.2f (+\/- %0.2f) [%s]\" % (\n        scores.mean(), scores.std(), label))","3dbb3870":"# Datatypes look good now let's look at a description of the dataset","65d6a39f":"# Load the dataset and examine the datatypes","dfc18d6c":"# Interesting results. Let's look at a few different regressors","a6789648":"# Everything is looking good. Time to process the data into a test train split and perform scaling\n\nI'm also going to create polynomial features in case there are interactions within the dataset","7f94f0e1":"# ","a5e63032":"# These scores aren't impressive let's take a look at the coefficients on the raw data\n\nWe will sort the coefficient in descending order and look at what is most predictive","0c48df63":"So these R2 scores are indicative of a low level of learning in the attempted regressors so far. I'm not afraid to say this is indicative of a need to come back at the data from a new angle and attempt new things. We are getting overfitting on the training examples when train in the holdout of the test train split and this is showing poor results on cross validation. ","d7d20634":"# Let's apply linear regression to this problem","f454fe39":"# Let's check cross validation scores and try out a stacking Regressor\n\nWe are seeing a serious difference on test score to training score which is indicative of a problem. \nTime to look at how these perform in cross validation. "}}