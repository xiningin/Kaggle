{"cell_type":{"fa62544f":"code","34829315":"code","fc84ba9d":"code","f38cae33":"code","13835477":"code","3f37703d":"code","b3fa7edc":"code","a4c5914d":"code","bc82de17":"code","b9515f57":"code","ac915a6b":"code","60ef4d22":"code","0c755c47":"code","26eab278":"code","2682d0a0":"code","801bb8a7":"code","b299e1a6":"code","81e479bc":"code","4ee62ba2":"code","0beb0d26":"code","b0dc2039":"code","a1ce8264":"code","3a75975d":"markdown","2443bf40":"markdown","08d2b8d3":"markdown","d7502827":"markdown","b8b87a0c":"markdown","7cf5a1f7":"markdown"},"source":{"fa62544f":"!wget --quiet https:\/\/raw.githubusercontent.com\/tensorflow\/models\/master\/official\/nlp\/bert\/tokenization.py","34829315":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nimport tensorflow as tf\nfrom tensorflow.keras.layers import Dense, Input\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.callbacks import ModelCheckpoint\nimport tensorflow_hub as hub\nimport tokenization\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","fc84ba9d":"data = pd.read_csv('\/kaggle\/input\/steam-reviews-dataset\/steam_reviews.csv')\ndata.head()","f38cae33":"data.describe()","13835477":"data.recommendation.value_counts()","3f37703d":"sizes = [data.recommendation.value_counts()[0], data.recommendation.value_counts()[1]]\nlabels = ['Recommended', 'Not Recommended']\n\nexplode = (0, 0.1)\nfig1, ax1 = plt.subplots()\nax1.set_title('Games recommendation')\nax1.pie(sizes, explode=explode, labels=labels, autopct='%1.1f%%',\n        shadow=True, startangle=90)\n\nax1.axis('equal')  \nplt.tight_layout()\nplt.show()","b3fa7edc":"data['hour_played_reviews'] = data.groupby('hour_played')['hour_played'].transform('count')\nx = data.hour_played\ny = data['hour_played_reviews']\nfig = plt.figure(figsize = (13,8))\nax = fig.add_axes([0.1, 0.1, 0.8, 0.8])\nax.scatter(x,y)\nax.set_title('Dependence of the number of ratings on the duration of the game')\nax.set_xlabel('Hours played')\nax.set_ylabel('Number of reviews')","a4c5914d":"top_reviewed_games = data.title.value_counts()\nprint('Top 10 reviewed games:\\n\\n{}'.format(data.title.value_counts()[:10]))","bc82de17":"data = data.assign(y = (data.recommendation == 'Recommended').astype(int))\ndata.head(3)","b9515f57":"print(len(data)\/2)\ndata_cut = data[0:25000] # We will use just a small portion of data \ndata_cut.tail(1)         # because BERT with a full data size will work for a very long time","ac915a6b":"data_cut.review = [str(x) for x in data_cut.review.values] # So that there are no problems in the tokenizer","60ef4d22":"from sklearn.model_selection import train_test_split\nX = data_cut.review\ny = data_cut.y\nX_train, X_test, y_train, y_test = train_test_split(X, y,random_state = 42, test_size=0.50)\nfor each in [y_train, y_test]:\n    print(f\"y fraction = {each.mean():.4f}\")","0c755c47":"print('Train : {}, Test: {}'.format(len(X_train),len(X_test)))","26eab278":"#X_test = X_test[:-2] # if it's not equal\n#y_test = y_test[:-2]\n#X_train = X_train[:-1]\n#y_train = y_train[:-1]\nprint('\\n train X: {} \\n train y: {} \\n Val X: {} \\n val y: {}'.format(len(X_train),len(y_train),len(X_test),len(y_test)))","2682d0a0":"%%time\nmodule_url = \"https:\/\/tfhub.dev\/tensorflow\/bert_en_uncased_L-24_H-1024_A-16\/1\"\nbert_layer = hub.KerasLayer(module_url, trainable=True)","801bb8a7":"vocab_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()\ndo_lower_case = bert_layer.resolved_object.do_lower_case.numpy()\ntokenizer = tokenization.FullTokenizer(vocab_file, do_lower_case)","b299e1a6":"def bert_encode(input_text, tokenizer, max_len = 512):\n    token_input = [] \n    mask_input = []\n    seg_input = []\n    \n    for text in input_text:\n        text = tokenizer.tokenize(text)\n        text = text[:max_len-2]\n        input_sequence = [\"[CLS]\"] + text + [\"[SEP]\"]\n        pad_len = max_len - len(input_sequence)\n        \n        tokens = tokenizer.convert_tokens_to_ids(input_sequence)      \n        token_input.append(tokens + [0]*pad_len)\n        mask_input.append([1]*len(input_sequence) + [0]*pad_len)\n        seg_input.append([0] * max_len)\n        \n    return np.array(token_input), np.array(mask_input), np.array(seg_input)","81e479bc":"def build_model(bert_layer, max_len = 512):\n    input_word_ids = Input(shape=(max_len, ),dtype = tf.int32,name = 'input_words_ids')\n    input_mask = Input(shape=(max_len,), dtype=tf.int32, name=\"input_mask\")\n    segment_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"segment_ids\")\n    \n    _, sequence_output = bert_layer([input_word_ids, input_mask, segment_ids])\n    clf_output = sequence_output[:, 0, :]\n    out = Dense(1, activation='sigmoid')(clf_output)\n    \n    model = Model(inputs=[input_word_ids, input_mask, segment_ids], outputs=out)\n    model.compile(Adam(lr=2e-6), loss='binary_crossentropy', metrics=['accuracy'])\n    \n    return model","4ee62ba2":"%%time\ntrain_input = bert_encode(X_train.values, tokenizer, max_len=160)\ntest_input = bert_encode(X_test.values, tokenizer, max_len=160)\ntrain_labels = y_train.values","0beb0d26":"model = build_model(bert_layer, max_len=160)\nmodel.summary()","b0dc2039":"%%time\ntrain_history = model.fit(\n    train_input, train_labels,\n    validation_split=0.2,\n    epochs=3,\n    batch_size=16\n)\n\nmodel.save('model.h5')","a1ce8264":"prediction = model.predict(test_input)\npreds = []\nfor x in prediction:\n    preds.append(int(x.round()))\n\nfrom sklearn.metrics import accuracy_score\nprint(\"Accuracy: \", accuracy_score(preds, y_test.values))","3a75975d":"We check how identical we got the parts. If the difference is big, you need to try mixing the data with another method. However, now we have a very good ratio.","2443bf40":"# Steam Reviews Classifier with BERT\nHere I\u2019ll try to show how BERT handles the classification of reviews in Steam. For the BERT part, I will use [xhlulu](https:\/\/www.kaggle.com\/xhlulu) code.","08d2b8d3":"Not bad for lazy model without data cleaning","d7502827":"The graph looks ugly, but we can see here that a greater number of reviews are made by players who played only a few hours, and very few reviews where players played for really long.","b8b87a0c":"Data contain reviews from Steam's best selling games as February 2019","7cf5a1f7":"Most reviews are positive"}}