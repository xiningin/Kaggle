{"cell_type":{"2c22babf":"code","c7cc3925":"code","c0af398e":"code","e1c45191":"code","c834e37c":"code","b0f326be":"code","a7e1f966":"code","848e1026":"code","d4f2fbb5":"code","bbf9840b":"code","a22f74e0":"code","acca7f8b":"code","2fade64d":"code","203ce251":"code","df136454":"code","0567aa64":"code","fa842a42":"code","29d7997b":"code","85f04cdf":"code","fbf9a13d":"code","011f99b9":"code","09407780":"code","b7fe5626":"code","1501d413":"code","3ef381f4":"code","c3cd6208":"code","0fb7fd25":"code","926f7d18":"code","0653bf0b":"code","7e7e5861":"code","d207d8e3":"code","84543d1c":"code","859043ad":"code","0baf00b2":"code","f27c0004":"code","cda5f34f":"code","9905258e":"code","669f2ac9":"code","0c897918":"code","9da17db1":"code","bf876c68":"code","37b66a99":"code","33812295":"code","ef5c2a17":"code","f92f4bf5":"code","c59a7ba5":"code","e35c51e2":"markdown","01416e7f":"markdown","b14e8fa2":"markdown","7c8a6501":"markdown","9d56bc95":"markdown","f87e86b9":"markdown","2af8e057":"markdown","9d4a113a":"markdown","b235f43c":"markdown","da5c3e72":"markdown","650ef445":"markdown","5abeb923":"markdown","70e4c43b":"markdown","b12ccd63":"markdown","34c7ab1c":"markdown","daf727b1":"markdown","78140b3b":"markdown","7f0b4838":"markdown","e72a2699":"markdown","cd3b3368":"markdown","8b94d0cb":"markdown","6a227813":"markdown","2536eff5":"markdown","aa3ddc10":"markdown","6b0b81b0":"markdown","baea5760":"markdown","629c90d5":"markdown"},"source":{"2c22babf":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","c7cc3925":"import pandas as pd \nimport numpy as np \nimport matplotlib.pyplot as plt\nimport seaborn as sns ","c0af398e":"df = pd.read_csv('\/kaggle\/input\/diamonds\/diamonds.csv')\ndf.head()","e1c45191":"df.drop('Unnamed: 0',axis=1, inplace=True)\ndf = df.reindex(columns=[\"carat\", \"cut\", \"color\", \"clarity\", \"depth\", \"table\", \"x\", \"y\", \"z\", \"price\"])","c834e37c":"len(df)","b0f326be":"df.describe()","a7e1f966":"# check for missing values \ndf.isnull().any()","848e1026":"sns.distplot(df['price'])","d4f2fbb5":"# Skewness \nprint(\"The skewness of the Price in the dataset is {}\".format(df['price'].skew()))","bbf9840b":"# Transforming the target variable\ntarget = np.log(df['price'])\nprint(\"Skewness: {}\".format(target.skew()))\nsns.distplot(target)","a22f74e0":"df['carat'].hist()","acca7f8b":"df['cut'].unique()","2fade64d":"sns.countplot(x='cut', data=df)","203ce251":"df['color'].unique()","df136454":"sns.countplot(x='color', data=df)","0567aa64":"df['clarity'].unique()","fa842a42":"sns.countplot(df['clarity'])","29d7997b":"fig, ax = plt.subplots(2, figsize=(10,10))\ndf['depth'].hist(ax=ax[0])\ndf['table'].hist(ax=ax[1])\nax[0].set_title(\"Distribution of depth\")\nax[1].set_title(\"Distribution of table\")","85f04cdf":"fig, ax = plt.subplots(3, figsize=(10,10))\ndf['x'].hist(ax=ax[0])\ndf['y'].hist(ax=ax[1])\ndf['z'].hist(ax=ax[2])\nax[0].set_title(\"Distribution of x\")\nax[1].set_title(\"Distribution of y\")\nax[2].set_title(\"Distribution of z\")","fbf9a13d":"df['price'].hist()","011f99b9":"# Using Pearson Correlation \nplt.figure(figsize=(12,10))\ncor = df.corr()\nsns.heatmap(cor, annot=True,cmap=plt.cm.Reds)\nplt.show()","09407780":"# correlation with output variable \ncor_target = abs(cor[\"price\"])\n\n# Selecting highly correlated features \nrelevent_features = cor_target[cor_target>0.5]\nrelevent_features","b7fe5626":"df.drop(['depth', 'table'], axis=1, inplace=True)","1501d413":"df.head()","3ef381f4":"# Encoding the categorical data \n# Encoding the independent variables\ndummy_cut = pd.get_dummies(df['cut'],drop_first=True)   # drop_first to avoid the dummy variable trap\ndf = pd.concat([df, dummy_cut], axis=1)\ndf = df.drop('cut',axis=1)\ndf.head()","c3cd6208":"dummy_color = pd.get_dummies(df['color'], drop_first=True)   \ndf = pd.concat([df, dummy_color], axis=1)\ndf = df.drop('color',axis=1)\ndf.head()","0fb7fd25":"dummy_clarity = pd.get_dummies(df['clarity'], drop_first=True)\ndf = pd.concat([df, dummy_clarity], axis=1)\ndf = df.drop('clarity', axis=1)\ndf.head()","926f7d18":"order = df.columns.to_list()\norder","0653bf0b":"order = ['carat',\n 'x',\n 'y',\n 'z',\n 'Good',\n 'Ideal',\n 'Premium',\n 'Very Good',\n 'E',\n 'F',\n 'G',\n 'H',\n 'I',\n 'J',\n 'IF',\n 'SI1',\n 'SI2',\n 'VS1',\n 'VS2',\n 'VVS1',\n 'VVS2',\n  'price']","7e7e5861":"df = df[order]","d207d8e3":"df.head()","84543d1c":"X = df.iloc[:,:-1].values\ny = df.iloc[:,21].values","859043ad":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2, random_state=0)","0baf00b2":"from sklearn.linear_model import LinearRegression\nfrom sklearn import model_selection\nfrom sklearn.metrics import r2_score, mean_squared_error\n\nregressor = LinearRegression()\nregressor.fit(X_train, y_train)","f27c0004":"# making predictions\ny_pred = regressor.predict(X_test)","cda5f34f":"y_pred","9905258e":"mlr_score = regressor.score(X_test, y_test)","669f2ac9":"from sklearn import preprocessing, svm\n\nX_svm = X.copy()\nX_svm = preprocessing.scale(X_svm)\n\nX_svm_train, X_svm_test, y_svm_train, y_svm_test = train_test_split(X_svm, y, test_size=0.2, random_state=0)\n","0c897918":"clf = svm.SVR(kernel='linear')\nclf.fit(X_svm_train, y_svm_train)\n","9da17db1":"svr_score = clf.score(X_svm_test,y_svm_test)","bf876c68":"from sklearn.tree import DecisionTreeRegressor\nregressor_dt = DecisionTreeRegressor(random_state=0)\nregressor_dt.fit(X_train, y_train)","37b66a99":"regressor_dt.predict(X_test)","33812295":"dt_score = regressor_dt.score(X_test, y_test)","ef5c2a17":"from sklearn.ensemble import RandomForestRegressor\nregressor_rf = RandomForestRegressor(n_estimators=100, random_state=0)\nregressor_rf.fit(X_train, y_train)","f92f4bf5":"rf_score = regressor_rf.score(X_test, y_test)","c59a7ba5":"print('Multiple Linear Regression accuracy:', mlr_score)\nprint('SVR score: ', svr_score)\nprint('Decision Tree Regression score: ', dt_score)\nprint('Random Forest Regression score: ', rf_score)","e35c51e2":"We can infer that majority of the cuts are of \"Ideal\" or \"Premium\" type, whereas there are very few \"Fair\" cuts in the data.","01416e7f":"Let us look at the distribution of the target variable","b14e8fa2":"The dataset contains the prices and other attributes of almost 54,000 diamonds. The columns are as follows:\n\n - carat weight of the diamond (0.2--5.01)\n\n - cut quality of the cut (Fair, Good, Very Good, Premium, Ideal)\n\n - color diamond colour, from J (worst) to D (best)\n\n - clarity a measurement of how clear the diamond is (I1 (worst), SI2, SI1, VS2, VS1, VVS2, VVS1, IF (best))\n\n - x length in mm (0--10.74)\n\n - y width in mm (0--58.9)\n\n - z depth in mm (0--31.8)\n\n - depth total depth percentage = z \/ mean(x, y) = 2 * z \/ (x + y) (43--79)\n\n - table width of top of diamond relative to widest point (43--95)\n\n - price (dependent variable)\n\n We will use regression methods to model the price according to the different features.","7c8a6501":"### Decision Tree Regression ","9d56bc95":"#### Carat","f87e86b9":"### EDA","2af8e057":"We can conclude that the Random Forest Regression model performed the best with an accuracy of 97.4%","9d4a113a":"We see that most of the diamond carats range from 0.2-1.2\n","b235f43c":"### Splitting the data into training and test sets ","da5c3e72":"#### Cut","650ef445":"#### Depth and Table","5abeb923":"Let us now log-transform this variable and see if the distribution can get any more closer to normal ","70e4c43b":"### Best Model","b12ccd63":"### Importing the libraries and dataset","34c7ab1c":"#### x,y,z","daf727b1":"### Support Vector Regression","78140b3b":"In this notebook, we will attempt to predict the price of diamonds after analysing the effect of different physical variables that influence the price. We will use different regression techniques to model the price and evaluate their performance.\n","7f0b4838":"We see that the target variable is right-skewed. We can take the log transform this variable so that it becomes normally distributed. A normally distributed target variable helps in better modelling the relationship of the target variable with the independent variables.","e72a2699":"Here, we can infer that most of the diamonds have claritites of 'SI1' or 'VS2'\n","cd3b3368":"## REGRESSION","8b94d0cb":"#### Price","6a227813":"### Feature Selection","2536eff5":"### Random Forest Regression\n","aa3ddc10":"#### Clarity ","6b0b81b0":"#### Color","baea5760":"### Multiple Linear Regression","629c90d5":"Let us now examine each of the independent variables"}}