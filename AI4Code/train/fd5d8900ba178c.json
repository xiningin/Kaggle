{"cell_type":{"825b3501":"code","0afd4676":"code","65e53443":"code","5bad707e":"code","66629889":"code","5e7e99a5":"code","7f53eac0":"code","ea4b078a":"code","15ede069":"code","39e0e1d2":"code","76093e13":"code","98c72509":"code","9829dd7c":"code","32c19b9e":"code","48d80e0e":"code","cdf26932":"code","d955a3ac":"code","cc3a9d4e":"code","1bba2727":"code","9a4bfcf8":"code","728de104":"code","ffd3ce07":"code","d0da8872":"code","8cfd1922":"code","d3cb7988":"code","af096788":"code","590d5648":"code","35edf452":"code","3c1bfe2e":"code","46b82c90":"code","0d6d571d":"code","7bd54b88":"code","51d5f09c":"code","2f82a504":"code","80e9f9c6":"code","c17bbbe6":"code","b8c83d82":"code","3b96b78a":"code","9887197d":"code","2405858d":"code","ec894f72":"code","c9dcb446":"code","b6a5939d":"code","402cdf68":"code","615ae1b0":"code","873cda27":"code","4df90d4a":"code","6823f32f":"code","81bb9cbc":"markdown","b0739fda":"markdown","eed47026":"markdown","a442cb25":"markdown","e59ab3ef":"markdown","2d211bca":"markdown","0ce3902f":"markdown","ac97d71f":"markdown","a3ba7e49":"markdown","477ce330":"markdown","6af1e390":"markdown","32a259fa":"markdown","847d7c7a":"markdown","ca67e3ec":"markdown","125e40bd":"markdown","6e3ac9a1":"markdown","05687b81":"markdown","c559aee1":"markdown","63c5243b":"markdown","495e9f6a":"markdown","8e491517":"markdown","7696840b":"markdown","a1454719":"markdown","38a1ac29":"markdown","ea8e9402":"markdown","05946b22":"markdown"},"source":{"825b3501":"import numpy as np \nimport pandas as pd \n\n%matplotlib inline\nimport matplotlib.pylab as plt\nimport seaborn as sns\n\nimport ast\n\nfrom sklearn.decomposition import PCA\nfrom sklearn.cluster import KMeans\nfrom scipy.spatial.distance import cdist\n\nfrom wordcloud import WordCloud","0afd4676":"!python -m spacy download en_core_web_md\nimport spacy\nnlp =  spacy.load('en_core_web_md')","65e53443":"df_metadata = pd.read_csv('..\/input\/ted-talks\/ted_main.csv')\ndf_transcripts = pd.read_csv('..\/input\/ted-talks\/transcripts.csv') ","5bad707e":"df_metadata.head()","66629889":"print(\"There are {} talks, with {} duplicate titles, and {} speakers that have more than one talk.\".format(\n        len(df_metadata),\n        df_metadata.duplicated('title').sum(),\n        df_metadata.duplicated('main_speaker').sum()\n     ))","5e7e99a5":"df_metadata['number_tags'] = df_metadata.tags.apply(lambda x: len(ast.literal_eval(x)))\n\nfig, ax = plt.subplots(1,1,figsize=(16,3))\nsns.countplot(x='number_tags',data=df_metadata,ax=ax)","7f53eac0":"def flatten_list_of_list_of_words(l):\n    \"\"\"The tags column returns a list of lists of strings, this function just turns it to a single list.\n    It is a bit annoying cause python usually loops on chars not words, but I found this dark magic here:\n    https:\/\/stackoverflow.com\/questions\/52981376\/flatten-list-of-list-of-strings\n    \"\"\"\n    return [inner for item in df_metadata.tags for inner in ast.literal_eval(item)] \n\ndf_all_tags = pd.Series(flatten_list_of_list_of_words(df_metadata.tags))","ea4b078a":"###(\n# Just to make the issue clear:\ntags_test = df_metadata.tags\ntags = flatten_list_of_list_of_words(df_metadata.tags)\nprint(tags[0],'vs',tags_test[0])\nprint(len(np.unique(tags)),'vs',len(np.unique(tags_test)))\n###)","15ede069":"df_all_tags.value_counts().sort_values(ascending=False).head(10)","39e0e1d2":"text = (' ').join(df_all_tags)\n\nplt.figure(figsize=(20,6))\nwc = WordCloud(background_color=\"white\",width=1800,height=600).generate(text)\nplt.imshow(wc, interpolation='bilinear')\nplt.axis(\"off\");","76093e13":"fig, ax = plt.subplots(1,1,figsize=(16,3))\nsns.violinplot(df_all_tags.value_counts(),ax=ax,color='C1')\nax.set_xlabel('Number of times a tag was used')\n\nq25,q50,q75 = np.quantile(df_all_tags.value_counts(),[0.25,0.50,0.75])\n#ax.axvline(q25,color='k')\n#ax.axvline(q50,color='k')\n#ax.axvline(q75,color='k');\n\nprint('25% of the tags are used less than {} times in the entire data set, 50% {} and 75% {} times)'.format(q25,q50,q75))","98c72509":"df_tags = df_all_tags.value_counts().rename_axis('tags').reset_index(name='counts')\ndf_tags['has_vector'] = df_tags.tags.apply(lambda x: nlp(x).has_vector)\ndf_tags['word_vector'] = df_tags.tags.apply(lambda x: nlp(x).vector)\ndf_tags.head()","9829dd7c":"df_tags.has_vector.value_counts()","32c19b9e":"flat_list = [item for sublist in df_tags.word_vector for item in sublist]\nX = np.reshape(flat_list,(len(df_tags),-1))\nX.shape","48d80e0e":"print((df_tags.word_vector[42] - X[42]).sum())","cdf26932":"pca = PCA(n_components=5)\nX_PCA = pca.fit_transform(X)\nprint(pca.explained_variance_ratio_)","d955a3ac":"# I just coppied this from here: https:\/\/pythonprogramminglanguage.com\/kmeans-elbow-method\/\n\ndistortions = []\nK = range(1,50,2)\nfor k in K:\n    kmeanModel = KMeans(n_clusters=k).fit(X_PCA)\n    distortions.append(sum(np.min(cdist(X_PCA, kmeanModel.cluster_centers_, 'euclidean'), axis=1)) \/ X_PCA.shape[0])\n\n# Plot the elbow\nplt.plot(K, distortions, 'bx-')\nplt.xlabel('k')\nplt.ylabel('Distortion')\nplt.title('The Elbow Method showing the optimal k')\nplt.show()","cc3a9d4e":"distortions = []\nK = range(1,50,2)\nfor k in K:\n    kmeanModel = KMeans(n_clusters=k).fit(X)\n    distortions.append(sum(np.min(cdist(X, kmeanModel.cluster_centers_, 'euclidean'), axis=1)) \/ X.shape[0])\n\n# Plot the elbow\nplt.plot(K, distortions, 'bx-')\nplt.xlabel('k')\nplt.ylabel('Distortion')\nplt.title('The Elbow Method showing the optimal k')\nplt.show()","1bba2727":"df_all_tags","9a4bfcf8":"df_metadata.head()","728de104":"len(df_all_tags.unique())","ffd3ce07":"from tqdm import tqdm\nfrom itertools import combinations","d0da8872":"def connect_videos_with_same_tag(df):\n    links = []\n    for tag in tqdm(df_all_tags.unique()):\n        #videos_with_tag = df.title[[i for i,tag_list in enumerate(df.tags) if tag in tag_list]]\n        #all_comb_2_videos = list(combinations(videos_with_tag, 2))\n        idx_videos_with_tag = [i for i,tag_list in enumerate(df.tags) if tag in tag_list]\n        all_comb_2_videos = list(combinations(idx_videos_with_tag, 2))\n        links.append(all_comb_2_videos)\n        \n    flat_links = [item for sublist in links for item in sublist]\n    df_links = pd.DataFrame({'TED1':np.transpose(flat_links)[0],'TED2':np.transpose(flat_links)[1]})\n    \n    return df_links\n\n\ndef get_links_weight(df):\n    # This is literally coppied from the medium article. Check the original: \n    # https:\/\/towardsdatascience.com\/building-a-social-network-from-the-news-using-graph-theory-by-marcell-ferencz-9155d314e77f\n    df_links = df.groupby(['TED1', 'TED2']).size().reset_index()\n    df_links.rename(columns={0: 'weight'}, inplace=True)\n    df_links = df_links[df_links['weight'] > 1]\n    df_links.reset_index(drop=True, inplace=True)\n    df_links.sort_values('weight', ascending=False)\n    return df_links\n\ndef add_title_for_human_readability(df_links,df):\n    df_links['Title1'] = [df.iloc[idx_ted].title for idx_ted in tqdm(df_links.TED1)]\n    df_links['Title2'] = [df.iloc[idx_ted].title for idx_ted in tqdm(df_links.TED2)]\n    return df_links\n\ndf_links = get_links_weight(connect_videos_with_same_tag(df_metadata))\ndf_links = add_title_for_human_readability(df_links,df_metadata)","8cfd1922":"df_links.sample(10).head(10)","d3cb7988":"import networkx as nx","af096788":"df_plot = df_links[df_links['weight']>10]\ndf_plot.reset_index(inplace=True, drop=True)\nG_plot = nx.Graph()\nfor link in tqdm(df_plot.index):\n    G_plot.add_edge(str(df_plot.iloc[link]['TED1']),\n                    str(df_plot.iloc[link]['TED2']),\n                    weight=df_plot.iloc[link]['weight'])","590d5648":"pos = nx.kamada_kawai_layout(G_plot)\nnodes = G_plot.nodes()\n\nfig, axs = plt.subplots(1, 1, figsize=(15,20))\nel = nx.draw_networkx_edges(G_plot, pos, alpha=0.1, ax=axs)\nnl = nx.draw_networkx_nodes(G_plot, pos, nodelist=nodes, node_color='#FAA6FF', with_labels=True, node_size=50, ax=axs)\nll = nx.draw_networkx_labels(G_plot, pos, font_size=10, font_family='sans-serif')","35edf452":"pos = nx.spring_layout(G_plot)\nnodes = G_plot.nodes()\n\nfig, axs = plt.subplots(1, 1, figsize=(15,20))\nel = nx.draw_networkx_edges(G_plot, pos, alpha=0.1, ax=axs)\nnl = nx.draw_networkx_nodes(G_plot, pos, nodelist=nodes, node_color='#FAA6FF', with_labels=True, node_size=50, ax=axs)\nll = nx.draw_networkx_labels(G_plot, pos, font_size=10, font_family='sans-serif')","3c1bfe2e":"most_common_tags = df_all_tags.value_counts().sort_values(ascending=False).rename_axis('tag',axis=0).reset_index()['tag'][:50].tolist()\nmost_common_tags","46b82c90":"labels = []\nfor tag in most_common_tags:\n    labels.append([1 if tag in tags else 0 for tags in df_metadata.tags])\n    \nlabels = np.transpose(labels)\nlabels.shape","0d6d571d":"from sklearn.feature_extraction.text import TfidfVectorizer\n\nvectorizer = TfidfVectorizer(stop_words='english',strip_accents='ascii',ngram_range=(1,1),max_features=5000)\nunigrams = vectorizer.fit_transform(df_metadata.description)\nprint(unigrams.toarray().shape)\n\nvectorizer = TfidfVectorizer(stop_words='english',strip_accents='ascii',ngram_range=(2,2),max_features=5000)\nbigrams = vectorizer.fit_transform(df_metadata.description)\nprint(bigrams.toarray().shape)\n\nvecs = df_metadata.description.apply(lambda x: nlp(x).vector)\nvecs = np.array([i for i in vecs]) # this probably can be done better\nprint(vecs.shape)","7bd54b88":"from sklearn.model_selection import train_test_split\n\nX_train_uni, X_test_uni, y_train_uni, y_test_uni = train_test_split(unigrams, labels, test_size=0.33, random_state=42)\nX_train_bi, X_test_bi, y_train_bi, y_test_bi     = train_test_split(bigrams, labels, test_size=0.33, random_state=42)\nX_train_vec, X_test_vec, y_train_vec, y_test_vec = train_test_split(vecs, labels, test_size=0.33, random_state=42)","51d5f09c":"import tensorflow as tf\nfrom keras import metrics\n\nmodel = tf.keras.models.Sequential([\n  tf.keras.layers.Flatten(input_shape=(5000,)),\n  tf.keras.layers.Dense(128, activation='relu'),\n  tf.keras.layers.Dropout(0.2),\n  tf.keras.layers.Dense(128, activation='relu'),\n  tf.keras.layers.Dropout(0.2),\n  tf.keras.layers.Dense(64, activation='relu'),\n  tf.keras.layers.Dropout(0.2),\n  tf.keras.layers.Dense(50,activation='softmax')\n])\n\nmodel.compile(optimizer='adam',\n              loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),\n              metrics=['accuracy',metrics.categorical_accuracy])\n\nmodel.fit(X_train_uni.toarray(), y_train_uni, epochs=20)","2f82a504":"model.evaluate(X_test_uni.toarray(),  y_test_uni, verbose=2)","80e9f9c6":"model_bi = tf.keras.models.Sequential([\n  tf.keras.layers.Flatten(input_shape=(5000,)),\n  tf.keras.layers.Dense(128, activation='relu'),\n  tf.keras.layers.Dropout(0.2),\n  tf.keras.layers.Dense(128, activation='relu'),\n  tf.keras.layers.Dropout(0.2),\n  tf.keras.layers.Dense(64, activation='relu'),\n  tf.keras.layers.Dropout(0.2),\n  tf.keras.layers.Dense(50,activation='softmax')\n])\n\nmodel_bi.compile(optimizer='adam',\n              loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),\n              metrics=['accuracy',metrics.categorical_accuracy])\n\nmodel_bi.fit(X_train_bi.toarray(), y_train_bi, epochs=20)","c17bbbe6":"model_bi.evaluate(X_test_bi.toarray(),  y_test_bi, verbose=2)","b8c83d82":"model_vec = tf.keras.models.Sequential([\n  tf.keras.layers.Flatten(input_shape=(300,)),\n  tf.keras.layers.Dense(128, activation='relu'),\n  tf.keras.layers.Dropout(0.2),\n  tf.keras.layers.Dense(128, activation='relu'),\n  tf.keras.layers.Dropout(0.2),\n  tf.keras.layers.Dense(64, activation='relu'),\n  tf.keras.layers.Dropout(0.2),\n  tf.keras.layers.Dense(50,activation='softmax')\n])\n\nmodel_vec.compile(optimizer='adam',\n              loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),\n              metrics=['accuracy',metrics.categorical_accuracy])\n\nmodel_vec.fit(X_train_vec, y_train_vec, epochs=20)","3b96b78a":"model_vec.evaluate(X_test_vec,  y_test_vec, verbose=2)","9887197d":"TO BE CONTINUED","2405858d":"def count_POS(text,pos='VERB',max_words=None):\n    ''' Other maybe interestung POS: VERB, NOUN, PROPN\n    '''\n    doc = nlp(text)\n    words = [token.lemma_ for token in doc if token.pos_ == pos]\n    df_sorted_words =  pd.Series(words).value_counts().sort_values(ascending=False)\n    df_sorted_words = df_sorted_words.rename_axis('words').reset_index().rename({0:'count'},axis=1)\n    return df_sorted_words.words.to_list()[:None]\n\ndf_metadata['verbs_in_title'] = df_metadata.title.apply(lambda x: count_POS(x,pos='VERB'))\ndf_metadata['nouns_in_title'] = df_metadata.title.apply(lambda x: count_POS(x,pos='NOUN'))\ndf_metadata['proper_nouns_in_title'] = df_metadata.title.apply(lambda x: count_POS(x,pos='PROPN'))","ec894f72":"df_metadata[['title','verbs_in_title','nouns_in_title','proper_nouns_in_title']].sample(10).head(10)","c9dcb446":"df_metadata['verbs_in_description'] = df_metadata.description.apply(lambda x: count_POS(x,pos='VERB',max_words=None))\ndf_metadata['nouns_in_description'] = df_metadata.description.apply(lambda x: count_POS(x,pos='NOUN',max_words=None))\ndf_metadata['proper_nouns_in_description'] = df_metadata.description.apply(lambda x: count_POS(x,pos='PROPN',max_words=None))","b6a5939d":"df_metadata[['description','verbs_in_description','nouns_in_description','proper_nouns_in_description']].sample(10).head(10)","402cdf68":"df_metadata[['title','verbs_in_title','verbs_in_description']].head()","615ae1b0":"def similarity_lists(list_title,list_description):\n    return nlp(' '.join(list_title)).similarity(' '.join(list_description))\n\ndf_metadata['similarity_verbs'] = df_metadata.apply(lambda row: similarity_lists(row['verbs_title'],row['verbs_description']),axis=1)\ndf_metadata['similarity_nouns'] = df_metadata.apply(lambda row: similarity_lists(row['nouns_title'],row['nouns_description']),axis=1)","873cda27":"df_metadata['similarity_title_desc']","4df90d4a":"TO DO \n\nput all verbs in the ground form - DONE\nmaybe td-idf is useful, to see what words are more unique to each description\nthink of a way to relate the two, it is list vs list -- WITH SIMILARITY\n\nsee how to generate news headers from news\n","6823f32f":"def count_most_common_words(text,include_stop_words=True,max_return=None):\n    \n    doc = nlp(text)\n    \n    if include_stop_words:\n        words = [w.text for w in doc if w.is_alpha]\n    else:\n        words = [w.text for w in doc if w.is_alpha and not w.is_stop]\n        \n    df = pd.Series(words).value_counts().sort_values(ascending=False)\n    df = df.rename_axis('word').reset_index().rename({0:'count'},axis=1)\n\n    if max_return is None:\n        return df\n    \n    else:\n        return df[:max_return].to_list()\n\ndef count_most_common_POS(text,pos='VERB'):\n    ''' Other maybe interestung POS: VERB, NOUN, PROPN\n    '''\n    doc = nlp(text)\n    words = [w for token in doc if token.pos_ == pos]\n    return pd.Series(words).value_counts().sort_values(ascending=False)\n\n\ndf_test = count_most_common_words(df_metadata.description[0],include_stop_words=False)","81bb9cbc":"Quick look of the verbs and nouns","b0739fda":"Let's focus on:\n\n* tags.\n* description\n* (mabe title? (same as name, except that name also has the name of the speaker))\n\nBut keep in mind that there are other super interesting columns, for example: stuff related with popularity (ratings, comments) and talks that a particular talk is related to. ","eed47026":"### Tags analysis II: can we predict the tags using the description?","a442cb25":"Since there are too many tags (about 400) the first step is to reduce this to about 50, by just keeping the most common ones.","e59ab3ef":"Actually, we might skip the PCA all together","2d211bca":"For the K-means, we have to chose a number of classes before running the algorith. A way to try to come up with the optimal number of classes (k) is using the elbow-method (k vs mean distance of the points to the centroid to which they belong to).","0ce3902f":"Most talks have 4-7 tags.","ac97d71f":"All worlds exist in the vocab","a3ba7e49":"**Very simple model**\n\n*unigrams*","477ce330":"Make input data for the model:\n    \n    version 1: td-idf unigrams\n    version 2: td-idf bigrams\n    version 3: GloVe vectors using spacy\n  ","6af1e390":"**Bigrams**","32a259fa":"Quick check to make sure I didn't swap the dimentions of the X matrix...","847d7c7a":"## 1.3 Titles\n\nI'll take a bit more of a manual approach here and use something quite handy that spacy has:\n\n* [Part-of-speach tagging POS](https:\/\/spacy.io\/api\/annotation#pos-tagging) that says if a word is a verb, noun, adjective, etc\n* [Named Entities Recognition](https:\/\/spacy.io\/api\/annotation#named-entities) that recognises some words as locations, people, events etc\n\nAnd I'll just list the verbs, nouns, and proper nouns in the titles","ca67e3ec":"## 1.2 Descriptions\n\nInstead of just listing the most common words, I'll also have a look at the most common verbs and nouns. Hopefully, they'll be related with the ones on the titles.","125e40bd":"What are the most popular tags?","6e3ac9a1":"# 1 Data Exploration","05687b81":"## 1.3 Similarities between title and description","c559aee1":"Mostly a useless plot, but that people seem to enjoy, cause it's kinda of cool:","63c5243b":"### Tags analysis I: K-means\n\nCan we group this tags into broader groups (and end up with just a couple of tags)? \n\n   1. Take a pre-trainned embedding (from spacy, not sure what is the model, but it has a dimmention of 96, not 300 like Glove).\n   2. Use PCA\/t-sne\/... to diminish the dimensions of the vectors from the embeddings.\n   3. See if we can group it using k-means.","495e9f6a":"### Tags analysis II: Network analysis\n\nHere I'm basing this work in this post about analysing a [entities relations in news](https:\/\/towardsdatascience.com\/building-a-social-network-from-the-news-using-graph-theory-by-marcell-ferencz-9155d314e77f).\n\nThe original articule says:\n\n> The fundamental premise behind building our social network will be two-fold and quite simple:\n> 1. If two people are mentioned in the same article, they are friends.\n> 2. The more articles mention the same two people, the closer they are as friends.\n\n\nWhich I'm going to addapt for this example by:\n\n1. If two TED talks have the same tag, these talks are related (aka friends, but that's weird for talks...)\n2. The more tags talks have in common, the stronger their relation is (which is still kind of weird for talks)\n\nNotice here that I'm ignoring the actual content of the tags.","8e491517":"Is there a group of tags that are way more used than others (that we might ignore)? ","7696840b":"It doesn't seem that there is a neat and smallish number of clusters of worlds, that would maybe point towards the same topic (e.g. childreen, education, play vs technology, computers, ai...). So **k-means doesn't seem to be that interesting to cluster ted talks**.","a1454719":"**PCA**\n\nI randomly choose to keep 5 components.","38a1ac29":"**Glove vectors**","ea8e9402":"## 1.1 Tags","05946b22":"I'll use the cosine similarity to compare the word vectors of verbs\/nouns in the title to the ones in the description, just to have an idea of how close they are."}}