{"cell_type":{"32dcce3f":"code","3a28193d":"code","d9127d68":"code","72beb91f":"code","eb980498":"code","962f7cd3":"code","e839eb5c":"code","39dc789d":"code","8fecc205":"code","f3f48714":"code","1b15eee0":"code","eab7f7ac":"code","45ba8786":"code","e39e6dca":"code","5e6b0758":"code","c84dfe52":"code","06c48491":"code","8105f266":"code","4354f969":"code","83c65649":"code","72cadeaf":"code","9e0bf9cd":"code","47234762":"code","12164363":"code","24f79ab0":"code","3a8fcc48":"code","3f19f4c4":"code","962e0df1":"code","3ab5e8f9":"code","a9ee97b5":"code","be67637e":"code","3be751d8":"code","1d7f6058":"code","f5d395f2":"code","aeb9b6d6":"code","12cd438e":"code","63f55322":"code","9ff5e1d6":"code","7e357269":"code","58eef104":"code","02655075":"code","365f8226":"code","ec61a0c4":"code","ffb32b27":"code","ebf80dbc":"code","bfa18da6":"code","5d723b28":"code","8bd28d2f":"code","eb664f4e":"code","217414b2":"code","29049c7f":"code","3f2d861f":"code","a9f379f3":"code","5152de56":"code","78a408b0":"code","8ba65ed6":"code","a5092d8c":"code","3632bbb9":"code","738db71f":"code","85a2f8a6":"code","1f9a4f2e":"code","b5439dc3":"code","363e415d":"code","263c294d":"code","dc50becb":"code","c7cb68a8":"code","35b64357":"code","aa7db831":"code","384c7480":"code","eccb0859":"code","ee8fa488":"code","586a7d3f":"code","6f0d5401":"code","e2df3656":"code","3411b727":"code","90170540":"code","35651d5b":"code","fc64392a":"code","5085b3de":"code","b9ec58fd":"code","54cac08e":"code","2bffc932":"code","c571b168":"code","38de9bf5":"code","001eb256":"code","15a56a99":"code","a3fdfa96":"code","5449ab08":"code","5d858906":"code","2ad6b4c5":"code","7055bd9f":"code","c464e555":"code","df1996a2":"markdown","b90c9359":"markdown","bcea25e5":"markdown","61f8a097":"markdown","f5202f66":"markdown","4223168a":"markdown","9087fbcf":"markdown","23d83739":"markdown","9718d65c":"markdown","c40d3272":"markdown","a77612dc":"markdown","8c353dd8":"markdown","ad9c8f40":"markdown","dc30dc38":"markdown","1e62c44e":"markdown","3b812462":"markdown","26279053":"markdown","d40cfb6a":"markdown","5f4c59f9":"markdown","6fde7439":"markdown","48ae4183":"markdown","d9b5becf":"markdown","96381cb8":"markdown","b71ecb75":"markdown","1d95e1b1":"markdown","29754fcf":"markdown","ad3b7f50":"markdown","a19de783":"markdown","18a73f03":"markdown","fda26fdc":"markdown","4ea10d8f":"markdown","7ac7a3d6":"markdown","e69ca348":"markdown","c94345a9":"markdown","3a871596":"markdown","8679ef58":"markdown","d2668abe":"markdown","79e34859":"markdown","47c1fa4b":"markdown","7a573e8e":"markdown","eca2a296":"markdown","ba049bfc":"markdown","86e3cbf1":"markdown","c06909be":"markdown","8ef4212c":"markdown","6b719300":"markdown","60224c05":"markdown","7311f7ba":"markdown","9c14a108":"markdown","4da2ea56":"markdown","f3c364bf":"markdown","58d69be4":"markdown","f06077bd":"markdown","96fc9dde":"markdown","756d23fe":"markdown","17c978c8":"markdown","e9be63d1":"markdown","50bad153":"markdown","973433de":"markdown","a9380393":"markdown","83c6703c":"markdown","7192ecd8":"markdown","b17578af":"markdown","b654e321":"markdown"},"source":{"32dcce3f":"# Library import\nfrom numpy import array\nfrom keras.models import Sequential, Model\nfrom keras.layers.core import Activation, Dense\nfrom keras.layers import Flatten, LSTM, Input, Bidirectional\nfrom keras.layers import RepeatVector, TimeDistributed\nfrom keras.utils import plot_model\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nEPOCHS = 200","3a28193d":"X = list()\nY = list()\nX = [x+1 for x in range(20)]\nY = [y * 15 for y in X]\n\nprint(f'X: {X}')\nprint(f'Y: {Y}')","d9127d68":"# since it's one-to-one model, so timesteps = features = 1, So\n# (samples, timesteps, features)\nX = array(X).reshape(20, 1, 1)","72beb91f":"model = Sequential()\n# LSTM input shape should be (time_steps, features)\nmodel.add(LSTM(50, activation='relu', input_shape=(1, 1)))\n# output\nmodel.add(Dense(1))\nmodel.compile(optimizer='adam', loss='mse')\n\nplot_model(model, show_shapes=True)","eb980498":"model.fit(X, np.asarray(Y), epochs=EPOCHS, validation_split=0.2, batch_size=5)","962f7cd3":"# Test set prediction\ntest_input = array([30])\ntest_input = test_input.reshape((1, 1, 1))\ntest_output = model.predict(test_input, verbose=0)\nprint(f'Actual:{30*15}, Prediction:{test_output}')","e839eb5c":"model = Sequential()\n# We specify an additional layer of LSTM which returns the sequence of outputs for next layer\nmodel.add(LSTM(50, activation='relu', return_sequences=True, input_shape=(1, 1)))\n# LSTM input shape should be (time_steps, features)\nmodel.add(LSTM(50, activation='relu'))\nmodel.add(Dense(1))\nmodel.compile(optimizer='adam', loss='mse')\n\nplot_model(model, show_shapes=True)","39dc789d":"model.fit(X, np.asarray(Y), epochs=EPOCHS, validation_split=0.2, verbose=1, batch_size=5)","8fecc205":"test_output = model.predict(test_input, verbose=0)\nprint(f'Actual:{30*15}, Prediction:{test_output}')","f3f48714":"nums = 25\n\nX1 = list()\nX2 = list()\nX = list()\nY = list()\n\nX1 = [(x+1)*2 for x in range(25)]\nX2 = [(x+1)*3 for x in range(25)]\nY = [x1*x2 for x1,x2 in zip(X1,X2)]\n\nprint(X1)\nprint(X2)\nprint(Y)","1b15eee0":"X = np.column_stack((X1, X2))\nprint(X)","eab7f7ac":"# (sample, timesteps, features=2)\nX = array(X).reshape(25, 1, 2)","45ba8786":"model = Sequential()\n# LSTM input shape should be (time_steps, features)\nmodel.add(LSTM(80, activation='relu', input_shape=(1, 2)))\nmodel.add(Dense(10, activation='relu'))\nmodel.add(Dense(1))\nmodel.compile(optimizer='adam', loss='mse')\nplot_model(model, show_shapes=True)","e39e6dca":"model.fit(X, np.asanyarray(Y), epochs=EPOCHS, validation_split=0.2, batch_size=5)","5e6b0758":"test_input = array([55,80])\ntest_input = test_input.reshape((1, 1, 2))\ntest_output = model.predict(test_input, verbose=0)\nprint(f'Actual:{55*80}, Prediction:{test_output}')","c84dfe52":"model = Sequential()\nmodel.add(LSTM(200, activation='relu', return_sequences=True, input_shape=(1, 2)))\nmodel.add(LSTM(100, activation='relu', return_sequences=True))\nmodel.add(LSTM(50, activation='relu', return_sequences=True))\nmodel.add(LSTM(25, activation='relu'))\nmodel.add(Dense(20, activation='relu'))\nmodel.add(Dense(10, activation='relu'))\nmodel.add(Dense(1))\nmodel.compile(optimizer='adam', loss='mse')\nplot_model(model, show_shapes=True)","06c48491":"model.fit(X, np.asanyarray(Y), epochs=EPOCHS, validation_split=0.2, verbose=1, batch_size=3)","8105f266":"test_output = model.predict(test_input, verbose=0)\nprint(f'Actual:{55*80}, Prediction:{test_output}')","4354f969":"X = np.array([x+1 for x in range(45)])\nprint(X)","83c65649":"X = X.reshape(15,3,1)\nprint(X)","72cadeaf":"Y = list()\nfor x in X:\n    Y.append(x.sum())\n\nY = np.array(Y)\nprint(Y)","9e0bf9cd":"model = Sequential()\nmodel.add(LSTM(50, activation='relu', input_shape=(3, 1)))\nmodel.add(Dense(1))\nmodel.compile(optimizer='adam', loss='mse')\nplot_model(model, show_shapes=True)","47234762":"model.fit(X, Y, epochs=EPOCHS, validation_split=0.2, verbose=1)","12164363":"test_input = array([50,51,52])\ntest_input = test_input.reshape((1, 3, 1))\ntest_output = model.predict(test_input, verbose=0)\nprint(f'Actual:{50+51+52}, Prediction:{test_output}')","24f79ab0":"model = Sequential()\nmodel.add(LSTM(200, activation='relu', return_sequences=True, input_shape=(3, 1)))\nmodel.add(LSTM(100, activation='relu', return_sequences=True))\nmodel.add(LSTM(50, activation='relu', return_sequences=True))\nmodel.add(LSTM(25, activation='relu'))\nmodel.add(Dense(20, activation='relu'))\nmodel.add(Dense(10, activation='relu'))\nmodel.add(Dense(1))\nmodel.compile(optimizer='adam', loss='mse')\nplot_model(model, show_shapes=True)","3a8fcc48":"model.fit(X, Y, epochs=EPOCHS, validation_split=0.2, verbose=1)","3f19f4c4":"test_output = model.predict(test_input, verbose=0)\nprint(f'Actual:{50+51+52}, Prediction:{test_output}')","962e0df1":"model = Sequential()\nmodel.add(Bidirectional(LSTM(50, activation='relu'), input_shape=(3, 1)))\nmodel.add(Dense(1))\nmodel.compile(optimizer='adam', loss='mse')\nplot_model(model, show_shapes=True)","3ab5e8f9":"model.fit(X, Y, epochs=EPOCHS, validation_split=0.2, verbose=1)","a9ee97b5":"test_output = model.predict(test_input, verbose=0)\nprint(f'Actual:{50+51+52}, Prediction:{test_output}')","be67637e":"X1 = np.array([x+3 for x in range(0, 135, 3)])\nprint(X1)\n\nX2 = np.array([x+5 for x in range(0, 225, 5)])\nprint(X2)","3be751d8":"X = np.column_stack((X1, X2))\nprint(X)","1d7f6058":"X = array(X).reshape(15, 3, 2)\nprint(X)","f5d395f2":"Y = list()\nfor x in X:\n    Y.append(x[2].sum())\n\nY = np.array(Y)\nprint(Y)\n","aeb9b6d6":"model = Sequential()\nmodel.add(LSTM(50, activation='relu', input_shape=(3, 2)))\nmodel.add(Dense(1))\nmodel.compile(optimizer='adam', loss='mse')\nplot_model(model, show_shapes=True)","12cd438e":"model.fit(X, Y, epochs=EPOCHS, validation_split=0.2, verbose=1)","63f55322":"test_input = array([[8, 51],\n                    [11,56],\n                    [14,61]])\n\ntest_input = test_input.reshape((1, 3, 2))\ntest_output = model.predict(test_input, verbose=0)\nprint(f'Actual:{14+61}, Prediction:{test_output}')","9ff5e1d6":"model = Sequential()\nmodel.add(LSTM(200, activation='relu', return_sequences=True, input_shape=(3, 2)))\nmodel.add(LSTM(100, activation='relu', return_sequences=True))\nmodel.add(LSTM(50, activation='relu', return_sequences=True))\nmodel.add(LSTM(25, activation='relu'))\nmodel.add(Dense(20, activation='relu'))\nmodel.add(Dense(10, activation='relu'))\nmodel.add(Dense(1))\nmodel.compile(optimizer='adam', loss='mse')\nplot_model(model, show_shapes=True)","7e357269":"model.fit(X, Y, epochs=EPOCHS, validation_split=0.2, verbose=1)","58eef104":"test_output = model.predict(test_input, verbose=0)\nprint(f'Actual:{14+61}, Prediction:{test_output}')","02655075":"model = Sequential()\nmodel.add(Bidirectional(LSTM(50, activation='relu'), input_shape=(3, 2)))\nmodel.add(Dense(1))\nmodel.compile(optimizer='adam', loss='mse')\nplot_model(model, show_shapes=True)","365f8226":"model.fit(X, Y, epochs=EPOCHS, validation_split=0.2, verbose=1)","ec61a0c4":"test_output = model.predict(test_input, verbose=0)\nprint(f'Actual:{14+61}, Prediction:{test_output}')","ffb32b27":"Y = list()\nfor x in X:\n    new_item = list()\n    new_item.append(x[2][0]+3)\n    new_item.append(x[2][1]+5)\n    Y.append(new_item)\n\nY = np.array(Y)\nprint(Y)","ebf80dbc":"model = Sequential()\nmodel.add(LSTM(50, activation='relu', input_shape=(3, 2)))\nmodel.add(Dense(2))\nmodel.compile(optimizer='adam', loss='mse')\nplot_model(model, show_shapes=True)","bfa18da6":"model.fit(X, Y, epochs=EPOCHS, validation_split=0.2, verbose=1)","5d723b28":"test_input = array([[20,34],\n                    [23,39],\n                    [26,44]])\n\ntest_input = test_input.reshape((1, 3, 2))\ntest_output = model.predict(test_input, verbose=0)\nprint(f'Actual:{[26+3,44+5]}, Prediction:{test_output}')","8bd28d2f":"model = Sequential()\nmodel.add(LSTM(100, activation='relu', return_sequences=True, input_shape=(3, 2)))\nmodel.add(LSTM(50, activation='relu', return_sequences=True))\nmodel.add(LSTM(25, activation='relu'))\nmodel.add(Dense(10, activation='relu'))\nmodel.add(Dense(2))\nmodel.compile(optimizer='adam', loss='mse')\nplot_model(model, show_shapes=True)","eb664f4e":"model.fit(X, Y, epochs=EPOCHS, validation_split=0.2, verbose=1)","217414b2":"test_output = model.predict(test_input, verbose=0)\nprint(f'Actual:{[26+3,44+5]}, Prediction:{test_output}')","29049c7f":"model = Sequential()\nmodel.add(Bidirectional(LSTM(50, activation='relu'), input_shape=(3, 2)))\nmodel.add(Dense(2))\nmodel.compile(optimizer='adam', loss='mse')\nplot_model(model, show_shapes=True)","3f2d861f":"model.fit(X, Y, epochs=EPOCHS, validation_split=0.2, verbose=1)","a9f379f3":"test_output = model.predict(test_input, verbose=0)\nprint(f'Actual:{[26+3,44+5]}, Prediction:{test_output}')","5152de56":"X = list()\nY = list()\nX = [x+3 for x in range(-2, 43, 3)]\n\nfor i in X:\n    output_vector = list()\n    output_vector.append(i+1)\n    output_vector.append(i+2)\n    Y.append(output_vector)\n\nprint(X)\nprint(Y)","78a408b0":"X = np.array(X).reshape(15, 1, 1)\nY = np.array(Y)","8ba65ed6":"model = Sequential()\nmodel.add(LSTM(50, activation='relu', input_shape=(1, 1)))\nmodel.add(Dense(2))\nmodel.compile(optimizer='adam', loss='mse')\nplot_model(model, show_shapes=True)","a5092d8c":"model.fit(X, Y, epochs=EPOCHS, validation_split=0.2, verbose=1, batch_size=3)","3632bbb9":"test_input = array([10])\ntest_input = test_input.reshape((1, 1, 1))\ntest_output = model.predict(test_input, verbose=0)\nprint(f'Actual:{[10+1, 10+2]}, Prediction:{test_output}')","738db71f":"model = Sequential()\nmodel.add(LSTM(50, activation='relu', return_sequences=True, input_shape=(1, 1)))\nmodel.add(LSTM(50, activation='relu'))\nmodel.add(Dense(2))\nmodel.compile(optimizer='adam', loss='mse')\nplot_model(model, show_shapes=True)","85a2f8a6":"model.fit(X, Y, epochs=EPOCHS, validation_split=0.2, verbose=1, batch_size=3)","1f9a4f2e":"test_output = model.predict(test_input, verbose=0)\nprint(f'Actual:{[10+1, 10+2]}, Prediction:{test_output}')","b5439dc3":"model = Sequential()\nmodel.add(Bidirectional(LSTM(50, activation='relu'), input_shape=(1, 1)))\nmodel.add(Dense(2))\nmodel.compile(optimizer='adam', loss='mse')\nplot_model(model, show_shapes=True)","363e415d":"model.fit(X, Y, epochs=EPOCHS, validation_split=0.2, verbose=1, batch_size=3)","263c294d":"test_output = model.predict(test_input, verbose=0)\nprint(f'Actual:{[10+1, 10+2]}, Prediction:{test_output}')","dc50becb":"nums = 25\n\nX1 = list()\nX2 = list()\nX = list()\nY = list()\n\nX1 = [(x+1)*2 for x in range(25)]\nX2 = [(x+1)*3 for x in range(25)]\n\nfor x1, x2 in zip(X1, X2):\n    output_vector = list()\n    output_vector.append(x1+1)\n    output_vector.append(x2+1)\n    Y.append(output_vector)\n\nX = np.column_stack((X1, X2))\nprint(X)","c7cb68a8":"X = np.array(X).reshape(25, 1, 2)\nY = np.array(Y)\nY.shape","35b64357":"model = Sequential()\nmodel.add(LSTM(50, activation='relu', input_shape=(1, 2)))\nmodel.add(Dense(2))\nmodel.compile(optimizer='adam', loss='mse')\nplot_model(model, show_shapes=True)","aa7db831":"model.fit(X, Y, epochs=EPOCHS, validation_split=0.2, verbose=1, batch_size=3)","384c7480":"test_input = array([40, 60])\ntest_input = test_input.reshape((1, 1, 2))\ntest_output = model.predict(test_input, verbose=0)\nprint(f'Actual:{40+1, 60+1}, Prediction:{test_output}')","eccb0859":"model = Sequential()\nmodel.add(LSTM(50, activation='relu', return_sequences=True, input_shape=(1, 2)))\nmodel.add(LSTM(50, activation='relu'))\nmodel.add(Dense(2))\nmodel.compile(optimizer='adam', loss='mse')\nplot_model(model, show_shapes=True)","ee8fa488":"model.fit(X, Y, epochs=EPOCHS, validation_split=0.2, verbose=1, batch_size=3)","586a7d3f":"test_input = array([40, 60])\ntest_input = test_input.reshape((1, 1, 2))\ntest_output = model.predict(test_input, verbose=0)\nprint(f'Actual:{40+1, 60+1}, Prediction:{test_output}')","6f0d5401":"model = Sequential()\nmodel.add(Bidirectional(LSTM(50, activation='relu'), input_shape=(1, 2)))\nmodel.add(Dense(2))\nmodel.compile(optimizer='adam', loss='mse')\nplot_model(model, show_shapes=True)","e2df3656":"model.fit(X, Y, epochs=EPOCHS, validation_split=0.2, verbose=1, batch_size=3)","3411b727":"test_output = model.predict(test_input, verbose=0)\nprint(f'Actual:{40+1, 60+1}, Prediction:{test_output}')","90170540":"X = list()\nY = list()\nX = [x for x in range(5, 301, 5)]\nY = [y for y in range(20, 316, 5)]\n\nX = np.array(X).reshape(20, 3, 1)\nY = np.array(Y).reshape(20, 3, 1)","35651d5b":"print(f'X: {X[:3]}')\nprint(f'Y: {Y[:3]}')","fc64392a":"model = Sequential()\n\n# encoder layer\nmodel.add(LSTM(100, activation='relu', input_shape=(3, 1)))\n\n# repeat vector\nmodel.add(RepeatVector(3))\n\n# decoder layer\nmodel.add(LSTM(100, activation='relu', return_sequences=True))\n\nmodel.add(TimeDistributed(Dense(1)))\nmodel.compile(optimizer='adam', loss='mse')\nplot_model(model, show_shapes=True)","5085b3de":"model.fit(X, Y, epochs=EPOCHS, validation_split=0.2, verbose=1, batch_size=3)","b9ec58fd":"test_input = array([300, 305, 310])\ntest_input = test_input.reshape((1, 3, 1))\ntest_output = model.predict(test_input, verbose=0)\nprint(f'Actual:{[315, 320, 325]}, Prediction:{test_output}')","54cac08e":"model = Sequential()\nmodel.add(Bidirectional(LSTM(100, activation='relu'), input_shape=(3, 1)))\nmodel.add(RepeatVector(3))\nmodel.add(Bidirectional(LSTM(100, activation='relu', return_sequences=True)))\nmodel.add(TimeDistributed(Dense(1)))\nmodel.compile(optimizer='adam', loss='mse')\nplot_model(model, show_shapes=True)","2bffc932":"model.fit(X, Y, epochs=EPOCHS, validation_split=0.2, verbose=1, batch_size=3)","c571b168":"test_output = model.predict(test_input, verbose=0)\nprint(f'Actual:{[315, 320, 325]}, Prediction:{test_output}')","38de9bf5":"X = list()\nY = list()\nX1 = [x1 for x1 in range(5, 301, 5)]\nX2 = [x2 for x2 in range(20, 316, 5)]\nY = [y for y in range(35, 331, 5)]\n\nX = np.column_stack((X1, X2))","001eb256":"X = np.array(X).reshape(20, 3, 2)\nY = np.array(Y).reshape(20, 3, 1)","15a56a99":"print(f'X: {X[:3]}')\nprint(f'Y: {Y[:3]}')","a3fdfa96":"model = Sequential()\nmodel.add(LSTM(100, activation='relu', input_shape=(3, 2)))\nmodel.add(RepeatVector(3))\nmodel.add(LSTM(100, activation='relu', return_sequences=True))\nmodel.add(TimeDistributed(Dense(1)))\nmodel.compile(optimizer='adam', loss='mse')\nplot_model(model, show_shapes=True)","5449ab08":"model.fit(X, Y, epochs=EPOCHS, validation_split=0.2, verbose=1, batch_size=3)","5d858906":"X1 = [300, 305, 310]\nX2 = [315, 320, 325]\n\ntest_input = np.column_stack((X1, X2))\n\ntest_input = test_input.reshape((1, 3, 2))\nprint(f'test_input: {test_input}')\ntest_output = model.predict(test_input, verbose=0)\nprint(f'Actual:{330, 335, 340}, Prediction:{test_output}')","2ad6b4c5":"model = Sequential()\nmodel.add(Bidirectional(LSTM(100, activation='relu'), input_shape=(3, 2)))\nmodel.add(RepeatVector(3))\nmodel.add(Bidirectional(LSTM(100, activation='relu', return_sequences=True)))\nmodel.add(TimeDistributed(Dense(1)))\nmodel.compile(optimizer='adam', loss='mse')\nplot_model(model, show_shapes=True)","7055bd9f":"model.fit(X, Y, epochs=EPOCHS, validation_split=0.2, verbose=1, batch_size=3)","c464e555":"test_output = model.predict(test_input, verbose=0)\nprint(f'Actual:{330, 335, 340}, Prediction:{test_output}')","df1996a2":"#### <a id='4biii'>iii. Solution via Bidirectional LSTM<\/a>","b90c9359":"#### <a id='5aii'>ii. Solution via Bidirectional LSTM<\/a>","bcea25e5":"Till now we have predicted single values based on multiple features values from different time-steps. There is another case of many-to-one sequences where you want to predict one value for each feature in the time-step. For instance, the dataset we used in this section has three time-steps and each time-step has two features. We may want to predict individual value for each feature series. The following example makes it clear, suppose we have the following input:\ni.e. \n<br>for input - \n<br>[[[  3   5] <br>\n  [  6  10] <br>\n  [  9  15]]\n  \nIn the output, we want one time-step with two features as shown below:<br>\n[12, 20]<br><br>\nYou can see the first value in the output is a continuation of the first series and the second value is the continuation of the second series. We can solve such problems by simply changing the number of neurons in the output dense layer to the number of features values that we want in the output. However, first we need to update our output vector Y. The input vector will remain the same:\n\n","61f8a097":"## <a id='3'>3. Multiple features in a single output<\/a>\n\n![Multiple-Feature-Sequence-Prediction-Model.png](attachment:Multiple-Feature-Sequence-Prediction-Model.png)\n\nAs we were explaining earlier, we can also provide multiple features in a single timestep, it simply is a multivariate input, which can produce a multivariate output if we need, completely depends on problem and requirement.","f5202f66":"In the previous sections we saw how to solve one-to-one sequence problems with LSTM. In a one-to-one sequence problem, each sample consists of single time-step of one or multiple features. In Many-to-One Networks, we can provide input data where each sample contains one or more than one timesteps with one or more than one features\/timestep.\n\nText classification is a prime example of many-to-one sequence problems where we have an input sequence of words and we want to predict a single output tag.","4223168a":"#### <a id='5ai'>i. Solution via Stacked LSTM<\/a>","9087fbcf":"### <a id='1a'>a. One-to-One Sequence Problems with a Single Feature<\/a>","23d83739":"Simple LSTM is producing better predictions, may be because biderictional is overaly complicated in this case.","9718d65c":"#### <a id='1aii'>ii. Solution via Stacked LSTM<\/a>","c40d3272":"Well, it's much better than simple LSTM's output.. slight overforecast though.","a77612dc":"#### <a id='2bi'>i. Solution via Simple LSTM<\/a>","8c353dd8":"Predicted value is no where close to our Actual.","ad9c8f40":"## <a id='2'>2. Many-to-One Sequence Problems<\/a>\n\n![Many-to-One-Sequence-Prediction-Model.png](attachment:Many-to-One-Sequence-Prediction-Model.png)","dc30dc38":"Simple LSTM does a best job here.","1e62c44e":"#### <a id='2bii'>ii. Solution via Stacked LSTM<\/a>","3b812462":"#### Creating the Dataset\n\nSamples - 15<br>\nX1 - Multiple of 3 till 135 (15*3)<br>\nX2 - Multiple of 5 till 225 (15*5)<br>\nY - sum of featires at 3rd timesteps of each sample","26279053":"For begineers, it's always a challange to defind a correct paradigm for sequence models while solving a sequence problem. Let's first understand what do we mean by Sequence problems: in a layman's term - *data science problems where we take a single or multiple series of input and generate a single or series of output, are know as sequence problems.* Common examples are: Machine Translation, Text generation, Voice recognition, Image caption generation, Univariate and Multivariate Timeseries forecasting, etc. \n\nHere we are going to look into the the ways to build sequence models using LSTM to frame a correct solution for a given sequence problem. A simple sequence problem can be solved using a simple One-to-One Sequence model, on ther other hand, a sophisticated problem will require a hybrid sequence model.\n\nSince we are not solving any specific problem here, but are understanding how to emply various sequence models, we will focus more and more on framing input and output data, and creating various LSTM network models. To keep things simple and easy to understand, we'll be generating a sample data in each of the subproblems.\n\nBelow are major kinds of sequence models, we use them based on the requirement of our problems. A really simple problem can be solved using a simple One-to-One sequence model, and a sophisticated problem may required a hybrid solution, which is nothing but a collection of these paradigms.","d40cfb6a":"#### Creating the Dataset\nSamples - 25<br>\nX1 - multiple of 2 <br>\nX2 - multiple of 3<br>\nY - [X1+1, X2+1]","5f4c59f9":"## Content:\n0. <a href='#0' target='_self'>Difference between Sample, Timestep, and Feature<\/a><br><br>\n1. <a href='#1' target='_self'>One-to-One Sequence Problems<\/a><br>\n    a. <a href='#1a' target='_self'>One-to-One Sequence Problems with a Single Feature<\/a><br>\n    i. <a href='#1ai' target='_self'>Solution via Simple LSTM<\/a><br>\n    ii. <a href='#1aii' target='_self'>Solution via Stacked LSTM<\/a><br><br>\n    b. <a href='#1b' target='_self'>One-to-One Sequence Problems with Multiple Features<\/a><br>\n    i. <a href='#1bi' target='_self'>Solution via simple LSTM<\/a><br>\n    ii. <a href='#1bii' target='_self'>Solution via Stacked LSTM<\/a><br><br>\n2. <a href='#2' target='_self'>Many-to-One Sequence Problems<\/a><br>\n    a. <a href='#2a' target='_self'>Many-to-One Sequence Problems with a Single Feature<\/a><br>\n    i. <a href='#2ai' target='_self'>Solution via Simple LSTM<\/a><br>\n    ii. <a href='#2aii' target='_self'>Solution via Stacked LSTM<\/a><br>\n    iii. <a href='#2aiii' target='_self'>Solution via Bidirectional LSTM<\/a><br><br>\n    b. <a href='#2b' target='_self'>Many-to-one Sequence Problems with Multiple Features<\/a><br>\n    i. <a href='#2bi' target='_self'>Solution via Simple LSTM<\/a><br>\n    ii. <a href='#2bii' target='_self'>Solution via Stacked LSTM<\/a><br>\n    iii. <a href='#2biii' target='_self'>Solution via Bidirectional LSTM<\/a><br><br>\n3. <a href='#3' target='_self'>Multiple feature in a single output<\/a><br>\n    a. <a href='#3a' target='_self'>Simple LSTM Model<\/a><br>\n    b. <a href='#3b' target='_self'>Stacked LSTM<\/a><br>\n    c. <a href='#3c' target='_self'>Bidirectional LSTM<\/a><br><br>\n4. <a href='#4' target='_self'>One-to-Many Sequence Problems<\/a><br>\n\ta. <a href='#4a' target='_self'>One-to-Many Sequence Problems with a Single Feature<\/a><br>\n\ti. <a href='#4ai' target='_self'>Solution via Simple LSTM<\/a><br>\n\tii. <a href='#4aii' target='_self'>Solution via Stacked LSTM<\/a><br>\n\tiii. <a href='#4aiii' target='_self'>Solution via Bidirectional LSTM<\/a><br><br>\n\tb. <a href='#4b' target='_self'>One-to-Many Sequence Problems with Multiple Features<\/a><br>\n\ti. <a href='#4bi' target='_self'>Solution via Simple LSTM<\/a><br>\n\tii. <a href='#4bii' target='_self'>Solution via Stacked LSTM<\/a><br>\n\tiii. <a href='#4biii' target='_self'>Solution via Bidirectional LSTM<\/a><br><br>\n5. <a href='#5' target='_self'>Many-to-Many Sequence Problems<\/a><br>\n\ta. <a href='#5a' target='_self'>Many-to-Many Sequence Problems with Single Feature<\/a><br>\n\ti. <a href='#5ai' target='_self'>Solution via Stacked LSTM<\/a><br>\n\tii. <a href='#5aii' target='_self'>Solution via Bidirectional LSTM<\/a><br><br>\n\tb. <a href='#5b' target='_self'>Many-to-Many Sequence Problems with Multiple Features<\/a><br>\n\ti. <a href='#5bi' target='_self'>Solution via Stacked LSTM<\/a><br>\n    ii. <a href='#5bii' target='_self'>Solution via Bidirectional LSTM<\/a>","6fde7439":"### <a id='1b'>b. One-to-One Sequence Problems with Multiple Features<\/a>","48ae4183":"### <a id='5b'>b. Many-to-Many Sequence Problems with Multiple Features<\/a>","d9b5becf":"#### <a id='4aiii'>iii. Solution via Bidirectional LSTM<\/a>","96381cb8":"#### <a id='4bii'>ii. Solution via Stacked LSTM<\/a>","b71ecb75":"It looks better than Simple LSTM.","1d95e1b1":"### <a id='3c'>c. Bidirectional LSTM<\/a>[](http:\/\/)","29754fcf":"### <a id='3b'>b. Stacked LSTM<\/a>","ad3b7f50":"#### Creating the Dataset\nX - our train set contains 20 samples values starting from 1 to 20.<br>\nY - X*15","a19de783":"### <a id='4a'>a. One-to-Many Sequence Problems with a Single Feature<\/a>","18a73f03":"prediction is not much good, let's see stacked LSTM's output.","fda26fdc":"#### <a id='1ai'>i. Solution via Simple LSTM<\/a>","4ea10d8f":"All three models are producing values close to actuals.","7ac7a3d6":"## <a id='1'>1. One-to-One Sequence Problems<\/a>\n\n![One-to-One-Sequence-Prediction-Model-Over-Time.png](attachment:One-to-One-Sequence-Prediction-Model-Over-Time.png)\n\nAs name implies, here we provide a single timestep\/sample as inputs and in return we get a single prediction\/sample. One really important point needs to be understood here is that a single timestep can have a single or multiple features in both input as well as in outputs, as per our needs. We will explain this point more with supporting examples later in this notebook.\n\nTypical example of a one-to-one sequence problems is the case where you have an image and you want to predict a single label for the image.\n","e69ca348":"#### <a id='2ai'>i. Solution via Simple LSTM<\/a>","c94345a9":"### <a id='4b'>b. One-to-Many Sequence Problems with Multiple Features<\/a>\nThis one is slightly confusing. Here input sequence 1 timestep\/sample and each timestep has multiple features. In output layer, we are producing multiple timesteps for each sample.","3a871596":"#### <a id='2biii'>iii. Solution via Bidirectional LSTM<\/a>","8679ef58":"As we were talking earlier, we can have multiple features in a single or many timesteps, let's see how","d2668abe":"#### <a id='5bi'>i. Solution via Stacked LSTM<\/a>","79e34859":"Stacked LSTM produced a best result for this. Simple LSTM and Bidirectional both are under forecasting.","47c1fa4b":"#### <a id='4aii'>ii. Solution via Stacked LSTM<\/a>","7a573e8e":"#### <a id='2aii'>ii. Solution via Stacked LSTM<\/a>","eca2a296":"<a id='5bii'>ii. Solution via Stacked LSTM<\/a>","ba049bfc":"it's close.","86e3cbf1":"#### <a id='1bi'>i. Solution via simple LSTM<\/a>","c06909be":"### <a id='3a'>a. Simple LSTM Model<\/a>","8ef4212c":"It's and improvement, but still not good enough.","6b719300":"## <a id='4'>4. One-to-Many Sequence Problems<\/a>\n\n![One-to-Many-Sequence-Prediction-Model.png](attachment:One-to-Many-Sequence-Prediction-Model.png)\nAs we get from name, here input data contains one timestep\/input sample and output can contain many timesteps, predicted in response to single sample. To get a vector output we only have to specify the sdesired size of output vector in our output Dense layer, that's it.\n\nA typical example is an image and its corresponding description. This model can also be used for image captioning where one image is provided as input and a sequence of words are generated as output. Additionaly we can use it to get Timeseries forecast of next n-days.","60224c05":"### <a id='5a'>a. Many-to-Many Sequence Problems with Single Feature<\/a>\nJust like previous variants, even here, in encoders and decoder multiple features can be incorporated if required.","7311f7ba":"input vector remains same.<br>\n<br>\nLet's now train our simple, stacked and bidirectional LSTM networks on our dataset. The following script trains a simple LSTM:","9c14a108":"### <a id='2a'>a. Many-to-One Sequence Problems with a Single Feature<\/a>","4da2ea56":"#### Creating the Dataset\nSamples - 20 <br>\nX - Each sample contains 3 timesteps, which is nothing but a multiple of 5.\nY - Ouput for each sample is nothing but next 3 consecutive multiples of 5 where trains samples last timestep ends.","f3c364bf":"Not an improvement over Stacked LSTM","58d69be4":"#### Creating the Dataset\nSamples  - 20\nX1 - Multiples of 5 starting from 5\nX2 - Multiples of 5 starting from 20\nY - Next 3 timesteps after second feature values in each sample.","f06077bd":"## <a id='5'>5. Many-to-Many Sequence Problems<\/a>\n\n![Many-to-Many-Sequence-Prediction-Model-1.png](attachment:Many-to-Many-Sequence-Prediction-Model-1.png)\n\nIn few sequence problems, we want to be able to provide many sequence of inputs and in return get many sequence of outputs i.e. multiple timesteps\/sample. As usual, here also we have a capability of keeping 1 or more number of features in input and output stages. Best example of such problems is Machine Translation and Multivariate Timeseries Forecasting.\n\nNote that, this is kind of sequence problems where concept of Encoder and Decoder comes into the picture. Encoder and Decoder both can be build using LSTMs or other RNN models, the only thing which remains constant is the job that Encoder and Decoder performs. The first layer works as an encoder layer and encodes the input sequence. The decoder is also an LSTM layer, which accepts three inputs: the encoded sequence from the encoder LSTM, the previous hidden state, and the current input.\n\nDuring training the actual output at each time-step is used to train the encoder-decoder model. While making predictions, the encoder output, the current hidden state, and the previous output is used as input to make prediction at each time-step.","96fc9dde":"This is it for now, I hope, after this it should be easy to approach any sequence problem. \n\nPlease take a look at below references if you need, they've really helped me in getting a good confidence on using these Sequential models:\nhttps:\/\/machinelearningmastery.com\/models-sequence-prediction-recurrent-neural-networks\/ <br>\nhttps:\/\/machinelearningmastery.com\/sequence-prediction\/ <br>\nhttps:\/\/machinelearningmastery.com\/lstms-with-python\/ <br>\nhttps:\/\/stackabuse.com\/solving-sequence-problems-with-lstm-in-keras\/ <br>\nhttps:\/\/stackabuse.com\/solving-sequence-problems-with-lstm-in-keras-part-2\/\n\nFeel free to post your questions\/sugestions and upvote if you find it helpful. Later, we'll apply this learning in solving few interesting NLP and Timeseries problems, till then. Happy Learning ;)","756d23fe":"## <a id='0'>0. Difference between Sample, Timestep, and Feature<\/a>\n\nBefore we begin, it's really important that we understand the meaning and difference of samples, timesteps, and features because we are going to refer to this terminologies very frequently. Please refer to below images for clarity:\n\n![sample%20timestep%20feature%202.png](attachment:sample%20timestep%20feature%202.png)\n\n- Collection of features is a single timestep and collection of timesteps is a sample.\n![sample%20timestep%20feature%201.png](attachment:sample%20timestep%20feature%201.png)","17c978c8":"### <a id='2b'>b. Many-to-one Sequence Problems with Multiple Features<\/a>","e9be63d1":"#### Creating the Dataset\nSamples - 15<br>\nX - items starting from 1 in a gap of 3<br>\nY - Vector [X+1, X+2]","50bad153":"#### <a id='1bii'>ii. Solution via Stacked LSTM<\/a>","973433de":"#### <a id='4bi'>i. Solution via Simple LSTM<\/a>","a9380393":"#### <a id='2aiii'>iii. Solution via Bidirectional LSTM<\/a>","83c6703c":"It's really close and is better than Simple LSTM.","7192ecd8":"#### Creating the Dataset\nsamples - 15<br>\nX - continious number from 1-45, where per sample contains 3 continious values in its timesteps.<br>\nY - sum of all timesteps\/sample","b17578af":"#### Creating the Dataset\nSamples - 25<br>\nX1 - multiple of 2 till 50<br>\nX2 - multiple of 3 till 75<br>\nY - X1*X2","b654e321":"#### <a id='4ai'>i. Solution via Simple LSTM<\/a>"}}