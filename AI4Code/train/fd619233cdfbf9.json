{"cell_type":{"6262a1d9":"code","c07af14e":"code","4bd7236e":"code","a8304a4b":"code","028b57c2":"code","65c78a53":"code","327e9935":"code","4b82bba2":"code","0e352d65":"code","544222b8":"code","ee0e3fff":"code","dab57b2f":"code","0c1b0ec0":"code","baf20e67":"code","3204587c":"code","3f315451":"code","e8379ba4":"code","41bf2523":"code","219347bd":"code","cfd904da":"markdown","1009ad19":"markdown","a9b296bb":"markdown","93518585":"markdown","836967f2":"markdown","82b5684c":"markdown","5f21d753":"markdown","c6dfc4b0":"markdown","4c81dc2c":"markdown","7c2f2a3b":"markdown","412644c8":"markdown","be697d6f":"markdown","714138dc":"markdown","4dafe81e":"markdown","7f4b8d14":"markdown","c7e62567":"markdown","5b25b98d":"markdown"},"source":{"6262a1d9":"import sys, os, random\nimport glob, string\nimport numpy as np\nimport pandas as pd\nfrom nltk.tokenize import sent_tokenize, word_tokenize \nimport warnings \nfrom copy import deepcopy\nimport kmapper as km\nimport sklearn\nfrom plotly.offline import init_notebook_mode, iplot\nimport igraph as ig\nfrom matplotlib import pyplot as plt\n  \nwarnings.filterwarnings(action = 'ignore') \n  \nimport gensim \nfrom gensim.models import Word2Vec \n\nimport tensorflow as tf\nfrom tensorflow.keras.preprocessing.text import Tokenizer","c07af14e":"#r = \"..\/input\/large-metal-lyrics-archive-228k-songs\/metal_lyrics\"\nroot=\"..\/input\"\nalphabet_list = list(string.ascii_lowercase)\nNAMES = []\n\nfor char in alphabet_list:\n    path= os.path.join(root, \"large-metal-lyrics-archive-228k-songs\/metal_lyrics\", char)\n    #print(list(os.listdir(path)))\n    for x in list(os.listdir(path)):\n        NAMES.append(x)","4bd7236e":"print(\"We have information about \", len(NAMES) ,\"bands\")\n\nprint(\"Status of Carnifex presence in dataset is \", (\"CARNIFEX\" in NAMES))","a8304a4b":"def get_tokens(name):\n    BAND_NAME = name\n    BASE_PATH =  '..\/input\/large-metal-lyrics-archive-228k-songs\/metal_lyrics'\n    lyrics_files = glob.glob(os.path.join(BASE_PATH, BAND_NAME.lower()[0], BAND_NAME,'*','*.txt'))\n    \n# generating corpus\n    corpus = []\n    for lyric in lyrics_files:\n        with open(lyric) as f: \n            corpus.append(f.read())\n\n    # cleaning up the lyrics\n    corpus = '\\n'.join([x.lower() for x in corpus])       # join all songs to form on big corpus\n    corpus = corpus.split('\\n')                           # split the lines\n    corpus = [x for x in corpus if not x.startswith('[')] # removing comments starting with [\n    corpus = [x for x in corpus if x != '']               # removing empty items\n    return(corpus)","028b57c2":"get_tokens('CARNIFEX')","65c78a53":"connector_words =['of','and','the','do','or','so','to','on','in','this','for','.',',','a','an', 'it','is', 'in',\"n't\",\"'ll\",\"'s\",\"'m\", \"'ve\",\"'\" ,'x2','2x','x4','4x',']','[', 'chorus','thecakeofpoison', ':','?','10','x','2','4']\ncarnifex_lyrics=get_tokens('CARNIFEX')\ndata = [] \n  \nfor i in carnifex_lyrics: \n    t = [] \n      \n    # tokenize the sentence into words \n    for j in word_tokenize(i): \n         if j not in connector_words:\n            t.append(j.lower())\n    data.append(t) \nmodel1 = gensim.models.Word2Vec(data, min_count = 1,  \n                              size = 100, window = 5)","327e9935":"data","4b82bba2":"\nprint(\"Cosine similarity between 'death' \" + \n             \"and 'blood' - CBOW : \", \n    model1.similarity('death', 'blood')) \n      \nprint(\"Cosine similarity between 'death' \" +\n                \"and 'possessed' - CBOW : \", \n     model1.similarity('death', 'possessed'))\n\nmodel1.most_similar(positive=['blood'], topn=10)","0e352d65":"# Create Skip Gram model \nmodel2 = gensim.models.Word2Vec(data, min_count = 1, size = 100, \n                                             window = 5, sg = 1) \n  \n#Print results \nprint(\"Cosine similarity between 'death' \" + \n               \"and 'blood' - CBOW : \", \n   model2.similarity('death', 'blood')) \nprint(\"Cosine similarity between 'death' \" +\n                 \"and 'possessed' - CBOW : \", \n     model2.similarity('death', 'possessed')) \nmodel2.most_similar(positive=['blood'], topn=10)","544222b8":"dictt= model1.wv.vocab\n#list(dictt)\ndictt\nwords = []\nfqcys = []\nwlen = []\nfor w in set(list(dictt)):\n    words.append(w)\n    wlen.append(len(w))\n    fqcys.append(dictt[w].count)","ee0e3fff":"frequencies = pd.DataFrame({'Word': words,'Frequency':fqcys})\n#frequencies.sort_values('Freq',axis=0, ascending=False)\nfsort = frequencies.iloc[np.argsort(frequencies.Frequency),:]\nfsort.tail(n=20)\nvocab_length = len(list(dictt))\n\nprint(\"Carnifex used \", vocab_length, \" unique words\")\nprint(\"The ratio of unique words to total words used is \", vocab_length\/np.sum(fqcys))\nprint(\"The most frequently occurring words were: \\n\" ,fsort.tail(n=20))\n#np.mean(wlen)","dab57b2f":"num_words = len(list(dictt))\npairwise =np.zeros((num_words, num_words))\n\nfor i in range(num_words):\n    word1 = list(dictt)[i]\n    for j in range(num_words):\n        word2=list(dictt)[j]\n        pairwise[i,j] = model1.similarity(word1,word2)\n\n","0c1b0ec0":"X = pd.DataFrame(pairwise,columns=list(dictt))\nX.index=list(dictt)\nX.values","baf20e67":"\n\nnp.random.seed(1234)\n\nmapper = km.KeplerMapper(verbose=0)\nlens = mapper.fit_transform(pairwise, projection=sklearn.manifold.TSNE(random_state=1234), scaler=None)","3204587c":"def get_cluster_summary(player_list, average_mean, average_std, dataset, columns):\n    # Compare players against the average and list the attributes that are above and below the average\n\n    cluster_mean = np.mean(dataset.iloc[player_list].values, axis=0)\n    diff = cluster_mean - average_mean\n    std_m = np.sqrt((cluster_mean - average_mean) ** 2) \/ average_std\n\n    stats = sorted(zip(columns, cluster_mean, average_mean, diff, std_m), key=lambda x: x[4], reverse=True)\n    above_stats = [a[0] + ': ' + f'{a[1]:.2f}' for a in stats if a[3] > 0]\n    below_stats = [a[0] + ': ' + f'{a[1]:.2f}' for a in stats if a[3] < 0]\n    below_stats.reverse()\n\n    # Create a string summary for the tooltips\n    cluster_summary = 'Above Mean:<br>' + '<br>'.join(above_stats[:5]) + \\\n                      '<br><br>Below Mean:<br>' + '<br>'.join(below_stats[-5:])\n\n    return cluster_summary\n\ndef make_igraph_plot(graph, data, X, player_names, layout, mean_list, std_dev_list, title, line_color='rgb(200,200,200)'):\n    # Extract node information for the plot\n    div = '<br>-------<br>'\n    node_list = []\n    cluster_sizes = []\n    avg_points = []\n    tooltip = []\n    for node in graph['nodes']:\n        node_list.append(node)\n        players = graph['nodes'][node]\n        cluster_sizes.append(2 * int(np.log(len(players) + 1) + 1))\n        avg_points.append(np.average([data.iloc[i, np.where(df.columns=='Points')[0][0]-2] for i in players]))\n        node_info = node + div + '<br>'.join([player_names[i] for i in players]) \n        #+ div + \\\n                    #get_cluster_summary(players, mean_list, std_dev_list, X, X.columns)\n        tooltip += tuple([node_info])\n        #print(avg_points)\n    # Add the edges to a list for passing into iGraph:\n    edge_list = []\n    for node in graph['links']:\n        for nbr in graph['links'][node]:\n            # Need to base everything on indices for igraph\n            edge_list.append((node_list.index(node), node_list.index(nbr)))\n\n    # Make the igraph plot\n    g = ig.Graph(len(node_list))\n    g.add_edges(edge_list)\n\n    links = g.get_edgelist()\n    plot_layout = g.layout(layout)\n\n    n = len(plot_layout)\n    x_nodes = [plot_layout[k][0] for k in range(n)]  # x-coordinates of nodes\n    y_nodes = [plot_layout[k][1] for k in range(n)]  # y-coordinates of nodes\n\n    x_edges = []\n    y_edges = []\n    for e in links:\n        x_edges.extend([plot_layout[e[0]][0], plot_layout[e[1]][0], None])\n        y_edges.extend([plot_layout[e[0]][1], plot_layout[e[1]][1], None])\n\n    edges_trace = dict(type='scatter', x=x_edges, y=y_edges, mode='lines', line=dict(color=line_color, width=0.3),\n                       hoverinfo='none')\n\n    nodes_trace = dict(type='scatter', x=x_nodes, y=y_nodes, mode='markers', opacity=0.8,\n                       marker=dict(symbol='circle-dot', colorscale='Viridis', showscale=True, reversescale=True,\n                                   color=avg_points, size=cluster_sizes,\n                                   line=dict(color=line_color, width=0.3),\n                                   colorbar=dict(thickness=20, ticklen=4)),\n                       text=tooltip, hoverinfo='text')\n\n    axis = dict(showline=False, zeroline=False, showgrid=False, showticklabels=False, title='')\n\n    layout = dict(title=title, font=dict(size=12), showlegend=False, autosize=False, width=700, height=700,\n                  xaxis=dict(axis), yaxis=dict(axis), hovermode='closest', plot_bgcolor='rgb(200,200,200)')\n\n    iplot(dict(data=[edges_trace, nodes_trace], layout=layout))","3f315451":"# Create the graph of the nerve of the corresponding pullback\ngraph = mapper.map(lens, X.values, clusterer=sklearn.cluster.KMeans(n_clusters=2, random_state=1234))","e8379ba4":"np.mean(frequencies.Frequency)","41bf2523":"frqs = np.log(frequencies.Frequency - np.mean(frequencies.Frequency))\nf = (frqs -np.mean(frqs))\/(np.max(np.abs(frqs))*np.std(frqs))\nplt.hist(f)","219347bd":"\nnames=list(dictt)\nmeans = np.mean(X.values, axis=0)\nstd_dev = np.std(X.values, axis=0)\ndf = deepcopy(X)\ndf['Identifier'] = list(dictt)\n#df = df.insert(0, 'Identifier', list(dictt))\ntype(df)\ndf['Points'] = f\n\nmake_igraph_plot(graph, df, X, names, 'kk', means, std_dev, title='Carnifex Lyric Similarity Data', line_color='rgb(20,20,20)')","cfd904da":"In this next part, we flatten the data, filtering out stop words that we don't care about, like \"the\" and \"and\". Then we use Word2Vec to encode the data as a matrix of zeroes and 1s, with each row a line, and collumns are each unique word in the entire Carnifex corpus. Ones correspond to word presence in that string (aka line of a song). Word2vec also builds a model with which we can examine the similarities of each word in the corpus, based on how frequently they occur together.","1009ad19":"Now, we use two helpful TDA functions from [this excellent notebook](https:\/\/www.kaggle.com\/marylenab\/topological-analysis-of-premier-league-players\/edit)","a9b296bb":"Now we apply the persistent homology principle to acquire a 2D cluster map","93518585":"**Step 2: Data Preprocessing**\n\nNext, we acquire all the band names that we have. While this step is not technically necessary, it is useful for checking whether we have lyrics info for a particular band","836967f2":"We might be interested in how frequently overall each word occurs. Here, we flatten the vocabulary dictionary into a list, and then extract the frequencies.","82b5684c":"Standardize word frequencies before plotting","5f21d753":"**Step 1:** Read in libraries","c6dfc4b0":"Now we can compile this information into a dataframe and look at the most frequently occuring words.","4c81dc2c":"Now, we check how many bands we have, and whether or not Carnifex is one of them.","7c2f2a3b":"Next we build a skip Gram model, also with Word2Vec.","412644c8":"This is code mainly taken from another notebook, which you can access [here](https:\/\/www.kaggle.com\/sumitkant\/generate-metal-lyrics-using-keras). Basically, get_tokens takes a band name, finds it in the dataset, and then returns a list of lists, where the sublists are strings of lyrics. Each string is a line.","be697d6f":"Voila! Here, colour represents mean word frequency in that cluster. Bigger nodes mean more words in that cluster and edges between nodes indicate that those two node clusters share at least one word.","714138dc":"Now we look at some lyrics from Carnifex:","4dafe81e":"Now we convert to a pandas DataFrame for ease of access:","7f4b8d14":"# Metal Lyrics Analysis\n*by MaryLena Bleile*\n\nIn this notebook, I provide a framework for analyzing lyric similarity clustering via Topological Data Analysis. As an example, we will perform the analysis on one of my favourite bands, Carnifex, though it is simple to run the analysis on other bands' lyrical content as well.","c7e62567":"Now we get to the interesting part. We compute all pairwise differences between cosine similarities of words, which will eventually be used as the input for the Topological Data Analysis algorithm, using persistent homology.","5b25b98d":"**Step 3: Modelling**\nBefore we apply the persistent homology analysis, we project the feature set onto 2D space for better eventual visualization. This is facilitated through the t-SNE algorithim. "}}