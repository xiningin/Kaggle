{"cell_type":{"4667cfe5":"code","70a22360":"code","f8b7dde7":"code","80b106f5":"code","257962f2":"code","b1d6605b":"code","4552c2ac":"code","64471bf1":"code","f527ecba":"code","2e0a1ef5":"code","e19550bd":"code","55acdd23":"code","88e7bd4e":"code","a4e81159":"code","cfb1d5e9":"code","41b8e8d2":"code","2e721adf":"code","0d6858ba":"code","8a280a1d":"code","3f606d7c":"code","849f04f1":"code","5d647783":"code","5ee786f3":"code","e2f7a10a":"code","02b599df":"code","c1162f66":"code","9d6b8b18":"markdown","5d42090f":"markdown","0f8a027b":"markdown","9e54e7f4":"markdown","5449b8ca":"markdown","25c094bb":"markdown","7460e0a5":"markdown","e0b07385":"markdown","0c6cff51":"markdown","a042dddb":"markdown","1d2a462c":"markdown","2df1874f":"markdown","3504b63c":"markdown","7d5c78ab":"markdown","bfce6f45":"markdown"},"source":{"4667cfe5":"import pandas as pd, numpy as np, os, gc, matplotlib.pyplot as plt, seaborn as sb, re, warnings, calendar, sys\nfrom numpy import arange\nget_ipython().run_line_magic('matplotlib', 'inline')\nwarnings.filterwarnings('ignore'); np.set_printoptions(suppress=True); pd.options.mode.chained_assignment = None\npd.set_option('display.float_format', lambda x: '%.3f' % x)\nglobal directory; directory = '..\/input'\n\ndef files(): return os.listdir(directory)\n\ndef read_clean(data):\n    data.columns = [str(x.lower().strip().replace(' ','_')) for x in data.columns]\n    seen = {}; columns = []; i = 0\n    for i,x in enumerate(data.columns):\n        if x in seen: columns.append(x+'_{}'.format(i))\n        else: columns.append(x)\n        seen[x] = None\n        \n    for x in data.columns[data.count()\/len(data) < 0.0001]: del data[x];\n    gc.collect();\n    try: data = data.replace({'':np.nan,' ':np.nan});\n    except: pass;\n    \n    if len(data) < 10000: l = len(data);\n    else: l = 10000;\n    sample = data.sample(l);size = len(sample);\n    \n    for x in sample.columns:\n        ints = pd.to_numeric(sample[x], downcast = 'integer', errors = 'coerce')\n        if ints.count()\/size > 0.97:\n            minimum = ints.min()\n            if minimum > 0: data[x] = pd.to_numeric(data[x], downcast = 'unsigned', errors = 'coerce')\n            else: data[x] = pd.to_numeric(data[x], downcast = 'integer', errors = 'coerce')\n        else:\n            floats = pd.to_numeric(sample[x], downcast = 'float', errors = 'coerce')\n            if floats.count()\/size > 0.97: data[x] = pd.to_numeric(data[x], downcast = 'float', errors = 'coerce')\n            else:\n                dates = pd.to_datetime(sample[x], errors = 'coerce')\n                if dates.count()\/size > 0.97: data[x] = pd.to_datetime(data[x], errors = 'coerce')\n    return data.reset_index(drop = True)\n\ndef read(x):\n    '''Kaggle Reading in CSV files.\n    Just type read('file.csv'), and you'll get back a Table.'''\n    \n    file = '{}\/{}'.format(directory,x)\n    try:     data = pd.read_csv(file)\n    except:  data = pd.read_csv(file, encoding = 'latin-1')\n    return read_clean(data)\n\ndef tally(column, minimum = 0, top = None, graph = False, percent = False, multiple = False, lowercase = False, min_count = 1, method = 'count'):\n    '''Provides a tally count of all values in a COLUMN.\n        1. minimum  =  (>0)          Least count of item to show.\n        2. top      =  (-1,>0)       Only show top N objects\n        3. graph    =  (False,True)  Show bar graph of results\n        4. percent  =  (False,>0)    Instead of showing counts, show percentages of total count\n        \n       multiple = False\/True.\n       If True, counts and tallies objects in list of lists (Count Vectorizer)\n       \n       lowercase = True \/ False.\n       If True, lowers all text firsrt. So A == a\n       \n       min_count >= 1\n       If a column sum for tag has less than min_count, discard whole column\n       \n       method == count | tfidf\n       Can choose normal bag of words (count) or tfidf (Term Document Frequency)\n    '''\n    if multiple == False:\n        counts = column.value_counts().astype('uint')\n        counts = counts[counts >= minimum][:top]\n        counts = pd.DataFrame(counts).reset_index()\n        counts.columns = [column.name, 'tally']\n        if percent: \n            counts['tally'] \/= counts['tally'].sum()\/100\n            counts['tally'] = counts['tally']\n        if graph:\n            C = counts[::-1]\n            C.plot.barh(x = column.name, y = 'tally', legend = False); plt.show();\n        return counts\n    else:\n        column = column.fillna('<NAN>')\n        if type(column.iloc[0]) != list: column = column.apply(lambda x: [x])\n\n        if method == 'count':\n            from sklearn.feature_extraction.text import CountVectorizer\n            counter = CountVectorizer(lowercase = lowercase, tokenizer = lambda x: x, dtype = np.uint32, min_df = min_count)\n        else:\n            from sklearn.feature_extraction.text import TfidfVectorizer\n            counter = TfidfVectorizer(lowercase = lowercase, tokenizer = lambda x: x, dtype = np.float32, min_df = min_count)\n        counter.fit(column)\n        counts = pd.DataFrame(counter.transform(column).toarray())\n        if column.name is None: column.name = 'text'\n        counts.columns = [column.name+'_('+str(x)+')' for x in counter.get_feature_names()]\n        return counts\n    \n    \ndef describe(data):\n    '''Provides an overview of your data\n        1. dtype    =  Column type\n        2. missing% =  % of the column that is missing\n        3. nunique  =  Number of unique values in column\n        4. top3     =  Top 3 most occuring items\n        5. min      =  Minimum value. If not a number column, then empty\n        6. mean     =  Average value. If not a number column, then empty\n        7. median   =  Middle value. So sort all numbers, and get middle. If not a number column, then empty\n        8. max      =  Maximum value. If not a number column, then empty\n        9. sample   =  Random 2 elements\n        10. name    =  Column Name\n    '''\n    dtypes = dtype(data)\n    length = len(data)\n    missing = ((length - data.count())\/length*100)\n    \n    N = [];    most3 = []\n    for dt,col in zip(dtypes,data.columns):\n        if dt != 'datetime':\n            U = data[col].value_counts()\n            N.append(len(U))\n            if U.values[0] > 1: most3.append(U.index[:3].tolist())\n            else: most3.append([]);\n        else: N.append(0); most3.append([]);\n            \n    df = pd.concat([dtypes, missing], 1)\n    df.columns = ['dtype','missing%']\n    df['nunique'] = N; df['top3'] = most3\n    \n    numbers = list(data.columns[df['dtype'].isin(('uint','int','float'))])\n    df['min'] = data.min()\n    df['mean'] = data[numbers].mean()\n    df['median'] = data[numbers].median()\n    df['max'] = data.max()\n    df['sample'] = data.apply(lambda x : x.sample(2).values.tolist())\n    df['name'] = list(data.columns)\n    return df.sort_values(['missing%', 'nunique', 'dtype'], ascending = [False, False, True]).reset_index(drop = True)\n\n\ndef Checker(x):\n    if type(x) is pd.DataFrame: return 0\n    elif type(x) is pd.Series: return 1\n    else: return -1\n\ndef columns(data): return list(data.columns)\ndef rows(data): return list(data.index)\ndef index(data): return list(data.index)\ndef head(data, n = 10): return data.head(n)\ndef tail(data, n = 10): return data.tail(n)\ndef sample(data, n = 10): return data.sample(n)\n\ndef dtype(data):\n    what = Checker(data)\n    if what == 0:\n        dtypes = data.dtypes.astype('str')\n        dtypes = dtypes.str.split(r'\\d').str[0]\n    else:\n        dtypes = str(data.dtypes)\n        dtypes = re.split(r'\\d', dtypes)[0]\n    return dtypes\n\ndef mean(data):\n    what = Checker(data)\n    _dt = ('uint','int','float')\n    if what == 0:\n        dtypes = dtype(data)\n        numbers = data.columns[dtypes.isin(_dt)]\n        return data[numbers].mean()\n    elif what == 1:\n        dtypes = dtype(data)\n        if dtypes in _dt: return data.mean()\n        else: return np.nan\n    else:\n        try:     return np.nanmean(data)\n        except:  return np.nan\n        \ndef std(data):\n    what = Checker(data)\n    _dt = ('uint','int','float')\n    if what == 0:\n        dtypes = dtype(data)\n        numbers = data.columns[dtypes.isin(_dt)]\n        return data[numbers].std()\n    elif what == 1:\n        dtypes = dtype(data)\n        if dtypes in _dt: return data.std()\n        else: return np.nan\n    else:\n        try:     return np.nanstd(data)\n        except:  return np.nan\n        \ndef var(data):\n    what = Checker(data)\n    _dt = ('uint','int','float')\n    if what == 0:\n        dtypes = dtype(data)\n        numbers = data.columns[dtypes.isin(_dt)]\n        return data[numbers].var()\n    elif what == 1:\n        dtypes = dtype(data)\n        if dtypes in _dt: return data.var()\n        else: return np.nan\n    else:\n        try:     return np.nanvar(data)\n        except:  return np.nan\n        \ndef log(data):\n    what = Checker(data)\n    _dt = ('uint','int','float')\n    if what == 0:\n        dtypes = dtype(data)\n        numbers = data.columns[dtypes.isin(_dt)]\n        x = np.log(data[numbers])\n        x[np.isinf(x)] = np.nan\n        return pd.Series(x)\n    elif what == 1:\n        dtypes = dtype(data)\n        if dtypes in _dt:\n            x = np.log(data)\n            x[np.isinf(x)] = np.nan\n            return x\n        else: return np.nan\n    else:\n        try:\n            x = np.log(data)\n            x[np.isinf(x)] = np.nan\n            return x\n        except:  return np.nan\n        \ndef median(data):\n    what = Checker(data)\n    _dt = ('uint','int','float')\n    if what == 0:\n        dtypes = dtype(data)\n        numbers = data.columns[dtypes.isin(_dt)]\n        return data[numbers].median()\n    elif what == 1:\n        dtypes = dtype(data)\n        if dtypes in _dt: return data.median()\n        else: return np.nan\n    else:\n        try:     return np.nanmedian(data)\n        except:  return np.nan\n        \ndef minimum(data):\n    what = Checker(data)\n    if what == 0:      return data.min()\n    elif what == 1:    return data.min()\n    else:              return np.min(data)\n        \ndef maximum(data):\n    what = Checker(data)\n    if what == 0:      return data.max()\n    elif what == 1:    return data.max()\n    else:              return np.max(data)\n    \ndef missing(data):\n    what = Checker(data)\n    if what >= 0:      return pd.isnull(data)\n    else:              return np.isnan(data)\n    \ndef count(data):\n    what = Checker(data)\n    if what >= 0:      return data.count()\n    else:              return len(data)\n    \ndef nunique(data):\n    what = Checker(data)\n    if what >= 0:      return data.nunique()\n    else:              return len(np.unique(data))\n    \ndef unique(data):\n    if type(data) is pd.DataFrame:\n        uniques = []\n        for x in data.columns:\n            uniques.append(data[x].unique())\n        df = pd.Series(uniques)\n        df.index = data.columns\n        return df\n    elif type(data) is pd.Series: return data.unique()\n    else:              return np.unique(data)\n    \ndef total(data):\n    what = Checker(data)\n    _dt = ('uint','int','float')\n    if what == 0:\n        dtypes = dtype(data)\n        numbers = data.columns[dtypes.isin(_dt)]\n        return data[numbers].sum()\n    elif what == 1:\n        dtypes = dtype(data)\n        if dtypes in _dt: return data.sum()\n        else: return np.nan\n    else:\n        try:     return np.nansum(data)\n        except:  return np.nan\n        \ndef time_number(date): return hours(date)+minutes(date)\/60+seconds(date)\/60**2\ndef hours_minutes(date): return hours(date)+minutes(date)\/60\ndef hours(date): return date.dt.hour\ndef minutes(date): return date.dt.minute\ndef seconds(date): return date.dt.second\ndef month(date): return date.dt.month\ndef year(date): return date.dt.year\ndef day(date): return date.dt.day\ndef weekday(date): return date.dt.weekday\ndef leap_year(date): return year(date).apply(calendar.isleap)\ndef date_number(date): return year(date)+month(date)\/12+day(date)\/(365+leap_year(date)*1)\ndef year_month(date): return year(date)+month(date)\/12\n\ndef hcat(*columns):\n    cols = []\n    for c in columns:\n        if c is None: continue;\n        if type(c) in (list, tuple): \n            for i in c:\n                if type(i) not in (pd.DataFrame, pd.Series): cols.append(pd.Series(i))\n                else: cols.append(i)\n        elif type(c) not in (pd.DataFrame, pd.Series): cols.append(pd.Series(c))\n        else: cols.append(c)\n    out = pd.concat(cols, 1)\n    columns = []\n    seen = {}\n    for i,x in enumerate(out.columns):\n        if x not in seen:\n            columns.append(x)\n            seen[x] = 0\n        else:\n            columns.append(x+f'_{i}')\n    out.columns = columns\n    return out\n\ndef vcat(*columns):\n    cols = []\n    for c in columns:\n        if c is None: continue;\n        if type(c) in (list, tuple): \n            for i in c:\n                if type(i) not in (pd.DataFrame, pd.Series): cols.append(pd.Series(i))\n                else: cols.append(i)\n        elif type(c) not in (pd.DataFrame, pd.Series): cols.append(pd.Series(c))\n        else: cols.append(c)\n    return pd.concat(cols, 0)\n\ndef melt(data, columns):\n    '''Converts a dataset into long form'''\n    return data.melt(id_vars = columns)\n    \ndef tabulate(*columns, method = 'count'):\n    '''Splits columns into chunks, and counts the occurences in each group.\n        Remember - tabulate works on the LAST column passed.\n        Options:\n            1. count            = Pure Count in group\n            2. count_percent    = Percentage of Count in group\n            3. mean             = Mean in group\n            4. median           = Median in group\n            5. max              = Max in group\n            6. min              = Min in group\n            7. sum_percent      = Percentage of Sum in group\n        Eg:\n            Apple | 1\n            ---------\n            Orange| 3\n            ---------\n            Apple | 2\n            ---------\n        Becomes:\n            Apple | 1 | 1\n            -------------\n                  | 2 | 1\n            -------------\n            Orange| 3 | 1\n        \n        NOTE --------\n            method can be a list of multiple options.\n    '''\n    if type(method) in (list, tuple):\n        xs = []\n        for x in method:\n            g = tabulate(*columns, method = x)\n            xs.append(g)\n        xs = hcat(xs)\n        xs = xs.T.drop_duplicates().T\n        return read_clean(xs)        \n    else:\n        def percent(series):\n            counts = series.count()\n            return counts.sum()\n\n        data = hcat(*columns)\n        columns = data.columns.tolist()\n\n        if method in ('count', 'count_percent'):\n            groups = data.groupby(data.columns.tolist()).apply(lambda x: x[data.columns[-1]].count())\n\n            if method == 'count_percent':\n                groups = groups.reset_index()\n                groups.columns = list(groups.columns[:-1])+['Group_Count']\n                right = data.groupby(columns[:-1]).count().reset_index()\n                right.columns = list(right.columns[:-1])+['Group_Sum']\n\n                groups = pd.merge(left = groups, right = right, left_on = columns[:-1], right_on = columns[:-1])\n                groups['Percent%'] = groups['Group_Count']\/groups['Group_Sum']*100\n                groups = groups[columns+['Percent%']]\n                return groups\n\n        elif method == 'mean': groups = data.groupby(data.columns.tolist()[:-1]).apply(lambda x: x[data.columns[-1]].mean())\n        elif method == 'median': groups = data.groupby(data.columns.tolist()[:-1]).apply(lambda x: x[data.columns[-1]].median())\n        elif method == 'max': groups = data.groupby(data.columns.tolist()[:-1]).apply(lambda x: x[data.columns[-1]].max())\n        elif method == 'min': groups = data.groupby(data.columns.tolist()[:-1]).apply(lambda x: x[data.columns[-1]].min())\n        elif method == 'sum_percent':\n            groups = data.groupby(data.columns.tolist()[:-1]).apply(lambda x: x[data.columns[-1]].sum()).reset_index()\n            groups.columns = list(groups.columns[:-1])+['Group_Count']\n            right = data.groupby(columns[:-1]).sum().reset_index()\n            right.columns = list(right.columns[:-1])+['Group_Sum']\n\n            groups = pd.merge(left = groups, right = right, left_on = columns[:-1], right_on = columns[:-1])\n            groups['Sum%'] = groups['Group_Count']\/groups['Group_Sum']*100\n            groups = groups[cols+['Sum%']]\n            return groups\n        else:\n            print('Method does not exist. Please choose count, count_percent, mean, median, max, min, sum_percent.'); return None;\n        #except: print('Method = {}'.format(method)+' cannot work on Object, Non-Numerical data. Choose count.'); return None;\n\n        groups = pd.DataFrame(groups)\n        groups.columns = [method]\n        groups.reset_index(inplace = True)\n        return groups\n\n\ndef sort(data, by = None, how = 'ascending', inplace = False):\n    ''' how can be 'ascending' or 'descending' or 'a' or 'd'\n    It can also be a list for each sorted column.\n    '''\n    replacer = {'ascending':True,'a':True,'descending':False,'d':False}\n    if by is None and type(data) is pd.Series:\n        try:    x = replacer[how]\n        except: print(\"how can be 'ascending' or 'descending' or 'a' or 'd'\"); return None;\n        return data.sort_values(ascending = x, inplace = inplace)\n    elif type(how) is not list:\n        try:    how = replacer[how]\n        except: print(\"how can be 'ascending' or 'descending' or 'a' or 'd'\"); return None;\n    else:\n        for x in how: \n            try:    x = replacer[x]\n            except: print(\"how can be 'ascending' or 'descending' or 'a' or 'd'\"); return None;\n    return data.sort_values(by, ascending = how, inplace = inplace)\n\ndef keep(data, what, inplace = False):\n    '''Keeps data in a column if it's wanted.\n    Everything else is filled with NANs'''\n    if type(what) not in (list,tuple,np.array,np.ndarray): what = [what]\n    need = data.isin(what)\n    if inplace: \n        df = data\n        df.loc[~need] = np.nan\n    else: \n        df = data.copy()\n        df.loc[~need] = np.nan\n        return df\n\ndef remove(data, what, inplace = False):\n    '''Deletes data in a column if it's not wanted.\n    Everything else is filled with NANs'''\n    if type(what) not in (list,tuple): what = [what]\n    need = data.isin(what)\n    if inplace: \n        df = data\n        df.loc[need] = np.nan\n    else: \n        df = data.copy()\n        df.loc[need] = np.nan\n        return df\n    \n    \ndef ternary(data, condition, true, false = np.nan, inplace = False):\n    '''C style ternary operator on column.\n    Condition executes on column, and if true, is filled with some value.\n    If false, then replaced with other value. Default false is NAN.'''\n    try:\n        execute = 'data {}'.format(condition)\n        series = eval(execute)\n        try: series = series.map({True:true, False:false})\n        except: series = series.replace({True:true, False:false})\n        return series\n    except: print('Ternary accepts conditions where strings must be enclosed.\\nSo == USD not allowed. == \"USD\" allowed.'); return False;\n\n    \ndef locate(data, column):\n    '''Use ternary to get result and then filter with notnull'''\n    if dtype(column) == 'bool': return data.loc[column]\n    return data.loc[column.notnull()]\n    \ndef query(data, column = None, condition = None):\n    '''Querying data based on conditions'''\n    def Q(data, column, condition):\n        if column is not None:\n            if type(condition) in (np.array, np.ndarray, list, tuple, set):\n                cond = keep(data[column], tuple(condition))\n                cond = (cond.notnull())\n            else: cond = ternary(data[column], condition, True, False)\n            return data.loc[cond]\n        else:\n            if type(condition) in (np.array, np.ndarray, list, tuple, set):\n                cond = keep(data, tuple(condition))\n            else: cond = ternary(data, condition, True, False)\n            return data.loc[cond]\n    try:\n        return Q(data, column, condition)\n    except:\n        condition = condition.replace('=','==')\n        return Q(data, column, condition)\n        \ndef keep_top(x, n = 5):\n    '''Keeps top n (after tallying) in a column'''\n    df = keep(x, tally(x)[x.name][:n].values)\n    return df\n\ndef keep_bot(x, n = 5):\n    '''Keeps bottom n (after tallying) in a column'''\n    df = keep(x, tally(x)[x.name][:-n].values)\n    return df\n\n\ndef remove_outlier(x, method = 'iqr', range = 1.5):\n    '''Removes outliers in column with methods:\n        1. mean     =    meean+range (normally 3.5)\n        2. median   =    median+range (normally 3.5)\n        3. iqr      =    iqr+range (normally 1.5)\n    '''\n    i = x.copy()\n    if method == 'iqr':\n        first = np.nanpercentile(x, 0.25)\n        third = np.nanpercentile(x, 0.75)\n        iqr = third-first\n        i[(i > third+iqr*range) | (i < first-iqr*range)] = np.nan\n    else:\n        if method == 'mean': mu = np.nanmean(x)\n        else: mu = np.nanmedian(x)\n        std = np.nanstd(x)\n        i[(i > mu+std*range) | (i < mu-std*range)] = np.nan\n    return i\n\n\ndef cut(x, bins = 5, method = 'range'):\n    '''Cut continuous column into parts.\n        Method options:\n            1. range\n            2. quantile (number of quantile cuts)'''\n    if method == 'range': return pd.cut(x, bins = bins, duplicates = 'drop')\n    else: return pd.qcut(x, q = bins, duplicates = 'drop')\n    \n    \ndef plot(x, y = None, colour = None, column = None, data = None, size = 5, top = 10, wrap = 4, \n         subset = 5000, method = 'mean', quantile = True, bins = 10,\n         style = 'lineplot', logx = False, logy = False, logc = False, power = 1):\n    '''Plotting function using seaborn and matplotlib\n        Options:\n        x, y, colour, column, subset, style, method\n        \n        Plot styles:\n            1. boxplot\n            2. barplot\n            3. tallyplot (counting number of appearances)\n            4. violinplot (boxplot just fancier)\n            5. lineplot (mean line plot)\n            6. histogram\n            7. scatterplot (X, Y must be numeric --> dates will be converted)\n            8. bivariate (X, Y must be numeric --> dates will be converted)\n            9. heatmap (X, Y will be converted into categorical automatically --> bins)\n            10. regplot (X, Y must be numeric --> dates will be converted)\n    '''\n    if type(x) in (np.array,np.ndarray): x = pd.Series(x); x.name = 'x';\n    if type(y) in (np.array,np.ndarray): y = pd.Series(y); y.name = 'y';\n    if type(column) in (np.array,np.ndarray): column = pd.Series(column); column.name = 'column';\n    if type(colour) in (np.array,np.ndarray): colour = pd.Series(colour); colour.name = 'colour';\n        \n    if type(x) == pd.Series: \n        data = pd.DataFrame(x); x = x.name\n        if type(x) is not str:\n            data.columns = [str(x)]\n            x = str(x)\n    if method == 'mean': estimator = np.nanmean\n    elif method == 'median': estimator = np.nanmedian\n    elif method == 'min': estimator = np.min\n    elif method == 'max': estimator = np.max\n    else: print('Wrong method. Allowed = mean, median, min, max'); return False;\n    #----------------------------------------------------------\n    sb.set(rc={'figure.figsize':(size*1.75,size)})\n    dtypes = {'x':None,'y':None,'c':None,'col':None}\n    names = {'x':None,'y':None,'c':None,'col':None}\n    xlim = None\n    #----------------------------------------------------------\n    if data is not None:\n        if type(x) is str: x = data[x];\n        if type(y) is str: y = data[y]; \n        if type(colour) is str: colour = data[colour]; \n        if type(column) is str: column = data[column]; \n    if type(x) is str: print('Please specify data.'); return False;\n    #----------------------------------------------------------\n    if x is not None:\n        dtypes['x'] = dtype(x); names['x'] = x.name\n        if dtypes['x'] == 'object': x = keep_top(x, n = top)\n        elif dtypes['x'] == 'datetime': x = date_number(x)\n        if logx and dtype(x) != 'object': x = log(x)\n    if y is not None: \n        dtypes['y'] = dtype(y); names['y'] = y.name\n        if dtypes['y'] == 'object': y = keep_top(y, n = top)\n        elif dtypes['y'] == 'datetime': y = date_number(y)\n        if logy and dtype(y) != 'object': y = log(y)\n    if colour is not None:\n        dtypes['c'] = dtype(colour); names['c'] = colour.name\n        if dtypes['c'] == 'object': colour = keep_top(colour, n = top)\n        elif dtypes['c'] == 'datetime': colour = date_number(colour)\n        if logc and dtype(colour) != 'object': colour = log(colour)\n    if column is not None:\n        dtypes['col'] = dtype(column); names['col'] = column.name\n        if dtypes['col'] == 'object': column = keep_top(column, n = top)\n        elif dtypes['col'] == 'datetime': column = date_number(column)\n    #----------------------------------------------------------\n    df = hcat(x, y, colour, column)\n    if subset > len(df): subset = len(df)\n    df = sample(df, subset)\n    #----------------------------------------------------------\n    if column is not None:\n        if dtype(df[names['col']]) not in ('object', 'uint',' int') and nunique(df[names['col']]) > top: \n            if quantile: df[names['col']] = cut(df[names['col']], bins = bins, method = 'quantile')\n            else: df[names['col']] = cut(df[names['col']], bins = bins, method = 'range')\n    \n    try: df.sort_values(names['y'], inplace = True);\n    except: pass;\n    #----------------------------------------------------------\n    replace = {'boxplot':'box', 'barplot':'bar', 'tallyplot':'count', 'violinplot':'violin', \n               'lineplot': 'point', 'histogram':'lv'}\n    \n    if style == 'histogram' and y is None:\n        plot = sb.distplot(df[names['x']].loc[df[names['x']].notnull()], bins = bins)\n    elif style == 'lineplot' and y is None:\n        plot = plt.plot(df[names['x']]);\n        plt.show(); return;\n    elif style == 'barplot' and y is None:\n        plot = df.sort_values(names['x']).plot.bar();\n        plt.show(); return;\n    elif style in replace.keys():\n        if dtype(df[names['x']]) not in ('object', 'uint',' int') and nunique(df[names['x']]) > top: \n            if quantile: df[names['x']] = cut(df[names['x']], bins = bins, method = 'quantile')\n            else: df[names['x']] = cut(df[names['x']], bins = bins, method = 'range')\n        \n        if names['col'] is not None:\n            plot = sb.factorplot(x = names['x'], y = names['y'], hue = names['c'], data = df, kind = replace[style], col = names['col'],\n                             n_boot = 1, size = size, estimator = estimator, col_wrap = wrap)\n        else:\n            plot = sb.factorplot(x = names['x'], y = names['y'], hue = names['c'], data = df, kind = replace[style], col = names['col'],\n                             n_boot = 1, size = size, estimator = estimator)\n            \n        for ax in plot.axes.flatten(): \n            for tick in ax.get_xticklabels(): \n                tick.set(rotation=90)\n    \n    elif style == 'heatmap':\n        if dtype(df[names['x']]) != 'object'and nunique(df[names['x']]) > top:\n            if quantile: df[names['x']] = cut(df[names['x']], bins = bins, method = 'quantile')\n            else: df[names['x']] = cut(df[names['x']], bins = bins, method = 'range')\n                \n        if dtype(df[names['y']]) != 'object'and nunique(df[names['y']]) > top:\n            if quantile: df[names['y']] = cut(df[names['y']], bins = bins, method = 'quantile')\n            else: df[names['y']] = cut(df[names['y']], bins = bins, method = 'range')     \n\n        df = tabulate(df[names['x']], df[names['y']]).pivot(index = names['x'], columns = names['y'], values = 'count')\n        plot = sb.heatmap(df, cmap=\"YlGnBu\")\n\n        \n    elif dtype(df[names['x']]) == 'object' or dtype(df[names['y']]) == 'object':\n            print('{} can only take X = number and Y = number.'.format(style)); return False;\n        \n    elif style  in ('regplot', 'scatterplot'):\n        if column is None: col_wrap = None\n        else: col_wrap = wrap\n        if style == 'regplot': reg = True\n        else: reg = False\n        \n        plot = sb.lmplot(x = names['x'], y = names['y'], hue = names['c'], data = df, col = names['col'],\n                             n_boot = 2, size = size, ci = None, scatter_kws={\"s\": 50,'alpha':0.5},\n                        col_wrap = col_wrap, truncate = True, fit_reg = reg, order = power)\n        plot.set_xticklabels(rotation=90)\n        \n    elif style == 'bivariate':\n        plot = sb.jointplot(x = names['x'], y = names['y'], data = df, dropna = True, size = size, kind = 'reg',\n                           scatter_kws={\"s\": 50,'alpha':0.5}, space = 0)\n    plt.show()\n    \n    \ndef match_pattern(x, pattern, mode = 'find'):\n    '''Regex pattern finds in data and returns only match\n        \\d = digits\n        \\l = lowercase alphabet\n        \\p = uppercase alphabet\n        \\a = all alphabet\n        \\s = symbols and punctuation\n        \\e = end of sentence\n        \n        Modes =\n            1. find:   True\/False if find or not\n            2. keep:   Output original string if match, else NAN\n            3. match:  Output only the matches in the string, else NAN\n        '''\n    pattern = pattern.replace('\\d','[0-9]').replace('\\l','[a-z]').replace('\\p','[A-Z]').replace('\\a','[a-zA-Z]')\\\n                .replace('\\s','[^0-9a-zA-Z]').replace('\\e', '(?:\\s|$)')\n    if dtype(x) != 'object': print('Data is not string. Convert first'); return False;\n\n    regex = re.compile(r'{}'.format(pattern))\n    \n    def patternFind(i):\n        try: j = re.match(regex, i).group(); return True\n        except: return False;\n    def patternKeep(i):\n        try: j = re.match(regex, i).group(); return i\n        except: return np.nan;\n    def patternMatch(i):\n        try: j = re.match(regex, i).group(); return j\n        except: return np.nan;\n    \n    if mode == 'find':        return x.apply(patternFind)\n    elif mode == 'keep':      return x.apply(patternKeep)\n    elif mode == 'match':     return x.apply(patternMatch)\n    \n    \ndef split(x, pattern):\n    '''Regex pattern finds in data and returns match. Then, it is splitted accordingly.\n        \\d = digits\n        \\l = lowercase alphabet\n        \\p = uppercase alphabet\n        \\a = all alphabet\n        \\s = symbols and punctuation\n        \\e = end of sentence\n        \\S = most symbols including spaces but not apostrophes\n        '''\n    pattern2 = pattern.replace('\\d','[0-9]').replace('\\l','[a-z]').replace('\\p','[A-Z]').replace('\\a','[a-zA-Z]')\\\n                .replace('\\s','[^0-9a-zA-Z]').replace('\\e', '(?:\\s|$)').replace('\\S','[.!, \"\\(\\)\\?\\*\\&\\^%$#@:\/\\\\_;\\+\\-\\\u2026]')\n    \n    if dtype(x) != 'object': print('Data is not string. Convert first'); return False;\n    \n    regex = re.compile(r'{}'.format(pattern2))\n    try: return x.str.split(pattern2)\n    except: return x.apply(lambda i: re.split(regex, i))\n    \ndef replace(x, pattern, with_ = None):\n    '''Regex pattern finds in data and returns match. Then, it is replaced accordingly.\n        \\d = digits\n        \\l = lowercase alphabet\n        \\p = uppercase alphabet\n        \\a = all alphabet\n        \\s = symbols and punctuation\n        \\e = end of sentence\n        '''\n    if type(pattern) is list:\n        d = {}\n        for l in pattern: d[l[0]] = l[1]\n        try:\n            return x.replace(d)\n        except:\n            return x.astype('str').replace(d)\n            \n    pattern2 = pattern.replace('\\d','[0-9]').replace('\\l','[a-z]').replace('\\p','[A-Z]').replace('\\a','[a-zA-Z]')\\\n                .replace('\\s','[^0-9a-zA-Z]').replace('\\e', '(?:\\s|$)')\n    \n    if dtype(x) != 'object': print('Data is not string. Convert first'); return False;\n    \n    regex = re.compile(r'{}'.format(pattern2))\n    try: return x.str.replace(pattern2, with_)\n    except: return x.apply(lambda i: re.sub(regex, with_, i))\n    \ndef remove(x, what):\n    return replace(x, what, '')\n    \ndef notnull(data, loc = None):\n    '''Returns the items that are not null in a column \/ dataframe'''\n    if loc is not None:\n        return data.loc[loc.notnull()]\n    else:\n        return data.loc[data.notnull().sum(1) == data.shape[1]]\n    \n    \ndef exclude(data, col):\n    '''Only returns a dataframe where the columns in col are not included'''\n    if type(col) is str: col = [col]\n    columns = list(data.columns)\n    leave = list(set(columns) - set(col))\n    return data[leave]\n\n################### -----------------------------------------------------------------#######################\n#Recommendation Systems\ndef pivot(index, columns, values):\n    '''Creates a table where rows = users, columns = items, and cells = values \/ ratings'''\n    from scipy.sparse import dok_matrix\n    S = dok_matrix((nunique(index), nunique(columns)), dtype=np.float32)\n    \n    mins = np.abs(np.min(values))+1\n    indexM = {}\n    for i,x in enumerate(unique(index)): indexM[x] = i;\n    columnsM = {}\n    for i,x in enumerate(unique(columns)): columnsM[x] = i;\n        \n    for i,c,v in zip(index, columns, values+mins): S[indexM[i],columnsM[c]] = v;\n    \n    S = S.toarray(); S[S == 0] = np.nan; S -= mins\n    S = pd.DataFrame(S)\n    S.index = indexM.keys(); S.columns = columnsM.keys();\n    return S\n\ndef row_operation(data, method = 'sum'):\n    '''Apply a function to a row\n        Allowed functions:\n            1. sum\n            2. median\n            3. mean\n            4. max\n            5. min\n            6. count\n            7. count_percent\n            8. sum_percent\n            9. mean_zero         (Mean but zeroes arent counted)\n            10. count_zero       (Count but zeroes arent counted)\n        Own functions are also allowed.\n    '''\n    if method in ('sum','median','mean','max','min','count'):\n        x = eval('data.{}(1)'.format(method))\n    elif method in ('count_percent', 'sum_percent'):\n        x = eval('data.{}(1)'.format(method.split('_')[0]))\n        x \/= x.sum()\n        x *= 100\n    elif method in ('mean_zero', 'count_zero'):\n        df = data.copy()\n        df[df == 0] = np.nan\n        x = eval('df.{}(1)'.format(method.split('_')[0]))\n    else: return data.apply(method, axis = 1)\n    x.name = 'row_operation'\n    return x\n\n\ndef col_operation(data, method = 'sum'):\n    '''Apply a function to a column\n        Allowed functions:\n            1. sum\n            2. median\n            3. mean\n            4. max\n            5. min\n            6. count\n            7. count_percent\n            8. sum_percent\n            9. mean_zero         (Mean but zeroes arent counted)\n            10. count_zero       (Count but zeroes arent counted)\n        Own functions are also allowed.\n        '''\n    if method in ('sum','median','mean','max','min','count'):\n        x = eval('data.{}(0)'.format(method))\n    elif method in ('count_percent', 'sum_percent'):\n        x = eval('data.{}(0)'.format(method.split('_')[0]))\n        x \/= x.sum()\n        x *= 100\n    elif method in ('mean_zero', 'count_zero'):\n        df = data.copy()\n        df[df == 0] = np.nan\n        x = eval('df.{}(0)'.format(method.split('_')[0]))\n    else: return data.apply(method, axis = 0)\n    x.name = 'col_operation'\n    return x\n\n    \ndef random(obj, n = 1, p = None):\n    if p is not None:\n        if type(p) is pd.Series: p = p.values\n        if p.sum() > 2: p \/= 100\n    return list(np.random.choice(obj, size = n, replace = False, p = p))\n\ndef row(data, n):\n    return data.iloc[n:n+1]\n\ndef distances(source, target):\n    '''Returns all distances between target and source (L2)'''\n    Y = np.tile(target.values, (source.shape[0],1))\n    nans = np.isnan(Y)\n    X = source.values; X[np.isnan(X)] = 0;\n    Y[nans] = 0;\n    diff = X - Y;\n    diff[nans] = 0;\n    d = np.linalg.norm(diff, axis = 1)\n    j = pd.Series(d)\n    j.index = source.index\n    return j\n\n################### -----------------------------------------------------------------#######################\n#Natural Language Processing & Machine Learning\n\ndef multiply(left, right):\n    ''' Multiplies 2 tables or columns together.\n        Will do automatic type casting'''\n\n    if len(left.shape) == 1:\n        try: return left.values.reshape(-1,1)*right\n        except: return left.reshape(-1,1)*right\n    elif len(right.shape) == 1:\n        try: return right.values.reshape(-1,1)*left\n        except: return right.reshape(-1,1)*left\n    else:\n        return left*right\n    \n    \ndef clean(data, missing = 'mean', remove_id = True):\n    '''Cleans entire dataset.\n    1. missing =\n        mean, max, median, min\n        Fills all missing values with column mean\/median etc\n\n    2. remove_id = True\/False\n        Checks data to see if theres an ID column.\n        Removes it (not perfect)\n    '''\n    x = data[data.columns[dtype(data) != 'object']].copy()\n    for c in x.columns[x.count()!=len(x)]:\n        x[c] = eval('x[c].fillna(x[c].{}())'.format(missing))\n    if remove_id:\n        for c in x.columns[(dtype(x) == 'int')|(dtype(x) == 'uint')]:\n            if x[c].min() >= 0:\n                j = (x[c] - x[c].min()).sort_values().diff().sum()\n                if j <= 1.001*len(x) and j >= len(x)-1: x.pop(c);\n    return x\n\n\ndef scale(data):\n    columns = data.columns\n    from sklearn.preprocessing import StandardScaler\n    scaler = StandardScaler().fit(data)\n    X = pd.DataFrame(scaler.transform(data))\n    X.columns = columns\n    return X\n\nfrom sklearn.base import BaseEstimator, RegressorMixin, ClassifierMixin\n# from keras.models import Sequential, load_model\nfrom sklearn.linear_model import LassoLarsIC\nfrom sklearn.linear_model import Ridge\nfrom sklearn.utils import class_weight\n# from keras.layers import Dense, Activation, GaussianNoise, BatchNormalization, Dropout\n# from keras.initializers import glorot_normal\n# from keras.callbacks import *\n# from keras.optimizers import Nadam, SGD\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error, accuracy_score\n\nclass LinearModel(BaseEstimator, RegressorMixin):\n\n    def __init__(self, lasso = False, scale = True, logistic = False, layers = 0, activation = 'tanh', epochs = 50,\n                    time = None, shift = 1, test_size = 0.2, early_stopping = 7, lr = 0.1, fast = False, random_state = 0, impact = 1):\n        self.scale = scale; self.logistic = logistic; self.lasso = lasso; self.layers = layers;\n        assert activation in ['tanh','relu','sigmoid','linear']\n        assert shift > 0;\n        self.activation = activation; self.epochs = epochs; self.time = time; self.shift = shift\n        if logistic: self.model = Sequential()\n        elif lasso: self.model = LassoLarsIC()\n        else: self.model = Ridge()\n        self.mapping = {}; self.test_size = test_size; self.early_stopping = early_stopping\n        self.lr = lr; self.fast = fast; self.random_state = random_state; self.impact = impact\n\n    def fit(self, X, Y):\n        print('Model now fitting...')\n        X = self._process_X(X.copy())\n        X = self._time_transform(X)\n        \n        self.uniques, self.columns = X.apply(self._range_unique), list(X.columns)\n        if self.scale: X, self.means, self.stds = self._scaler(X)\n        else:\n            A = np.array([np.nan for x in range(len(self.columns))])\n            self.means, self.stds = A, A\n        self.uniques_scale = X.apply(self._range_unique)\n        \n        if self.logistic:\n            Y = self._process_Y(Y)\n            if self.fast: self._fit_sklearn(X, Y)\n            else: self._fit_keras(X, Y)\n        else:\n            try: \n                if self.layers == 0: self._fit_sklearn(X, Y)\n                else: \n                    self.out = 1\n                    if Y.min() >= 0:\n                        if Y.max() <= 1: self.activ = 'sigmoid'\n                        else: self.activ = 'relu'\n                    elif Y.min() >= -1 and Y.max() <= 1:\n                        self.activ = 'tanh'\n                    else: self.activ = 'linear'\n                    self.loss = 'mse'\n                    self._fit_keras(X, Y)\n            except: \n                print('Y is not numeric. Choose logistic = True for classification'); return None\n\n        self._store_coefficients()\n        self._df = self._store_analysis(X, self.impact)\n        print('Model finished fitting')\n\n        \n    def predict(self, X):\n        X = self._process_X(X.copy())\n        X = self._time_transform(X)\n        X = self._transform(X)\n        if self.logistic:\n            prob = self.model.predict(X)\n            if self.fast == False:\n                if self.activ == 'sigmoid': prob = prob.round().astype('int').flatten()\n                else: prob = prob.argmax(1).astype('int').flatten()\n                prob = pd.Series(prob).replace(self.mapping)\n            return prob\n        else: return self.model.predict(X).flatten()\n\n\n    def predict_proba(self, X):\n        if self.logistic:\n            X = self._process_X(X.copy())\n            X = self._time_transform(X)\n            X = self._transform(X)\n            if self.fast == False: prob = self.model.predict(X)\n            else: prob = self.model.predict_proba(X)\n            return prob\n        else: print('Predict Probabilities only works for logisitc models.'); return None;\n\n        \n    def coefficients(self, plot = False, top = None):\n        df = self.coef\n        if self.layers == 0:\n            if top is not None: df = df[:top]\n            if plot:\n                df = df.fillna('')\n                if self.fast:\n                    if len(self.model.classes_) > 2: df = df.style.bar(subset = [x for x in df.columns if 'Y=(' in x], align='mid', color=['#d65f5f', '#5fba7d'])\n                    else: df = df.style.bar(subset = ['Coefficient'], align='mid', color=['#d65f5f', '#5fba7d'])\n                else:\n                    if len(self.mapping) > 2: df = df.style.bar(subset = [x for x in df.columns if 'Y=(' in x], align='mid', color=['#d65f5f', '#5fba7d'])\n                    else: df = df.style.bar(subset = ['Coefficient'], align='mid', color=['#d65f5f', '#5fba7d'])\n        return df\n    \n    \n    def plot(self, predictions, real_Y):\n        if self.logistic:\n            from sklearn.metrics import confusion_matrix\n            conf = pd.DataFrame(confusion_matrix(real_Y, predictions))\n            try: \n                if self.fast:\n                    conf.index = [f'True({x})' for x in self.model.classes_]\n                    conf.columns = [f'{x}' for x in self.model.classes_]\n                else:\n                    conf.index = [f'True({x}\/{i})' for i,x in zip(self.mapping.keys(), self.mapping.values())]\n                    conf.columns = [f'{x}\/{i}' for i,x in zip(self.mapping.keys(), self.mapping.values())]\n            except: \n                conf.index = [f'True({x})' for x in range(nunique(real_Y))]\n            conf = conf.divide(conf.sum(1), axis = 'index')*100\n            return sb.heatmap(conf, cmap=\"YlGnBu\", vmin = 0, vmax = 100, annot = True)\n        else:\n            return plot(x = predictions, y = real_Y, style = 'regplot')\n    \n    \n    def score(self, predictions, real_Y):\n        if self.logistic:\n            from sklearn.metrics import matthews_corrcoef\n            coef = matthews_corrcoef(real_Y, predictions)\n            if np.abs(coef) < 0.3: print('Model is not good. Score is between (-0.3 and 0.3). A score larger than 0.3 is good, or smaller than -0.3 is good.') \n            else: print('Model is good.')\n            return coef\n        else:\n            from sklearn.metrics import mean_squared_error\n            error = np.abs(np.sqrt(mean_squared_error(real_Y, predictions))\/np.mean(real_Y))\n            if error > 0.4: print('Model is not good. Score is larger than 40%. Smaller than 40% relative error is good.')\n            else: print('Model is good.')\n            return error\n    \n    \n    def analyse(self, column = None, plot = False, top = 20, impact = 1):\n        if self.layers == 0:\n            df = self._df.round(2)\n            if self.logistic:\n                if column is not None: df = df.loc[column]\n                else: df = df[:top]\n                def color_negative_red(val):\n                    color = 'lightgreen' if 'Add' in val else 'pink'\n                    return 'background-color: %s' % color\n\n                def highlight_max(s):\n                    is_max = s == s.max()\n                    return ['color: lightgreen' if v else '' for v in is_max]\n\n                if plot:\n                    df = df.fillna('')\n                    if self.activ == 'sigmoid':\n                        df = df.style.bar(align = 'mid', width = 75, color = ['gray'])\\\n                                    .applymap(color_negative_red, subset = ['Effect'])\n                    else:\n                        df = df.style.bar(align = 'mid', width = 75, color = ['gray'])\\\n                                    .applymap(color_negative_red, subset = ['Effect'])\\\n                                    .apply(highlight_max, subset = df.columns[2:], axis = 1)\n            else:\n                if column is not None: df = pd.DataFrame(df.loc[[column]])\n                else: df = df[:top]\n                if plot:\n                    cols = list(df.columns); cols.remove('If Stays'); cols.remove('Change if Removed')\n                    df[cols] = df[cols].fillna('')\n                    def color_negative_red(val):\n                        if val == True: color = 'cyan'\n                        elif val == False: color = 'pink'\n                        else: color = ''\n                        return 'background-color: %s' % color\n\n                    df = df.style.bar(subset = ['Coefficient','If Stays','Change if Removed','Best Addon',\n                                               'Worst Reduced'], align='mid', color=['#d65f5f', '#5fba7d'])\\\n                            .applymap(color_negative_red, subset = ['Stay'])\\\n                            .bar(subset = ['Best Contrib','Worst Contrib'], align='mid', color=['pink', 'cyan'])\n            return df\n        else:\n            print(\"Can't analyse since it's a neural network. I can only give you the model layout and loss graphs\")\n            print(self.model.summary()); history = self.history\n            plt.plot(history.history['loss']); plt.plot(history.history['val_loss'])\n            plt.title('model loss'); plt.ylabel('loss');plt.xlabel('epoch')\n            plt.legend(['train', 'test'], loc='upper left')\n            plt.show()\n        \n        \n    def degrees(self, prediction, real_Y):\n        '''The closer the degree of the fit line to 45*, the better!'''\n        if not self.logistic:\n            from sklearn.linear_model import Ridge as modeller\n            models = modeller().fit(prediction.reshape(-1,1),real_Y)\n            deg = np.round((np.arctan(models.coef_[0]))\/np.pi*180, 3)\n            if deg <= 50 and deg > 45: print('Prediction seems good, but probably overpredicting')\n            elif deg > 50: print(\"Prediction doesn't seem good. It's overpredicting\")\n            elif deg == 45: print(\"Prediction looks ideal! It's quite smooth\")\n            elif deg <= 45 and deg > 40: print(\"Prediction seems good, but probably underpredicting\")\n            else: print(\"Prediction doesn't seem good. It's underpredicting\")\n            return deg\n        else: print('Model is not regression. Use score instead'); return None;\n        \n        \n    def _process_X(self, X):\n        try: X.shape[1]\n        except: X = X.reshape(-1,1)\n        if type(X) is not pd.DataFrame: X = pd.DataFrame(X)\n        try: X = X[self.columns]\n        except: pass\n        return X\n\n\n    def _process_Y(self, Y):\n        if type(Y) is not pd.Series: Y = pd.Series(Y)\n        if self.fast == False:\n            n = nunique(Y); Y = Y.astype('category')\n            self.mapping = dict(enumerate(Y.cat.categories))\n            self.reverse_mapping = dict(zip(self.mapping.values(), self.mapping.keys()))\n            Y = Y.cat.codes\n\n            class_weights = class_weight.compute_class_weight('balanced', list(self.mapping.keys()), Y)\n            self.class_weights = dict(enumerate(class_weights))\n\n            if n == 2:\n                self.activ, self.loss, self.out = 'sigmoid', 'binary_crossentropy', 1\n            else:\n                self.activ, self.loss = 'softmax', 'categorical_crossentropy'\n                Y = pd.get_dummies(Y); self.out = Y.shape[1]\n        else:\n            if len(np.unique(Y)) > 2: self.activ, self.loss = 'softmax', 'categorical_crossentropy'\n            else: self.activ, self.loss, self.out = 'sigmoid', 'binary_crossentropy', 1\n        return Y\n    \n    \n    def _time_transform(self, X):\n        if self.time is not None:\n            X.sort_values(self.time, inplace = True)\n            alls = [X]\n            for s in range(1,self.shift+1):\n                ss = X.shift(s); ss.columns = [x+f'({-s})' for x in ss.columns]\n                alls.append(ss)\n            X = pd.concat(alls, 1)\n            X.fillna(method = 'backfill', inplace = True); X.sort_index(inplace = True)\n        return X\n    \n        \n    def _store_coefficients(self):\n        if self.logistic:\n            if self.layers == 0:\n                coefs = pd.DataFrame(self.coef_)\n                if self.fast: \n                    if len(self.model.classes_) > 2: coefs.columns, coefs.index = [f'Y=({x})' for x in self.model.classes_], self.columns\n                    else: coefs.columns, coefs.index = ['Coefficient'], self.columns\n                else: \n                    if len(self.mapping) > 2: coefs.columns, coefs.index = [f'Y=({x})' for x in self.mapping.values()], self.columns\n                    else: coefs.columns, coefs.index = ['Coefficient'], self.columns\n                coefs['Abs'] = np.abs(coefs).sum(1)\n                coefs['Mean'], coefs['Std'], coefs['Range'], coefs['Scale'] = self.means, self.stds, self.uniques, self.uniques_scale\n                coefs.sort_values('Abs', inplace = True, ascending = False); coefs.pop('Abs');\n                self.coef = coefs\n            else: self.coef = self.coef_\n        else:\n            if self.layers == 0:\n                df = pd.DataFrame({'Coefficient':self.coef_ , 'Abs' : np.abs(self.coef_),\n                                    'Mean':self.means, 'Std':self.stds, 'Range':self.uniques, 'Scale':self.uniques_scale})\n                df.index = self.columns; df.sort_values('Abs', ascending = False, inplace = True)\n                df.pop('Abs');\n                self.coef = df\n            else: self.coef = self.coef_\n                \n\n    def _store_analysis(self, X, impact = 1):\n        if self.logistic:\n            if self.layers == 0:\n                coefs = pd.DataFrame(self.coef_)\n                if self.activ == 'sigmoid':\n                    if self.fast: col = 'Probability (Y={})'.format(self.model.classes_[1])\n                    else: col = 'Probability (Y={})'.format(max(list(self.mapping.values())))\n                    coefs.columns = [col]\n                    coefs.index = self.columns\n                    exponential = np.exp(impact*coefs + self.bias_)\n                    exponential = exponential.divide(exponential + 1)*100\n                    exponential['Effect'] = f'Add {impact}'\n\n                    neg_exponential = np.exp(-impact*coefs + self.bias_)\n                    neg_exponential = neg_exponential.divide(neg_exponential + 1)*100\n                    neg_exponential['Effect'] = f'Minus {impact}'\n\n                    coefs = pd.concat([exponential, neg_exponential]).round(2)\n                    coefs.reset_index(inplace = True); coefs.columns = ['Column']+list(coefs.columns[1:])\n                    coefs.sort_values(col, ascending = False, inplace = True)\n                else:\n                    if self.fast: coefs.columns, coefs.index = [f'Y=({x})' for x in self.model.classes_], self.columns\n                    else: coefs.columns, coefs.index = [f'Y=({x})' for x in self.mapping.values()], self.columns\n                    exponential = np.exp(impact*coefs + self.bias_)\n                    exponential = exponential.divide(exponential.sum(1), axis = 0)*100\n                    exponential['Effect'] = f'Add {impact}'\n\n                    neg_exponential = np.exp(-impact*coefs + self.bias_)\n                    neg_exponential = neg_exponential.divide(neg_exponential.sum(1), axis = 0)*100\n                    neg_exponential['Effect'] = f'Minus {impact}'\n\n                    coefs = pd.concat([exponential, neg_exponential])\n                    coefs.reset_index(inplace = True)\n                    coefs.columns = ['Column']+list(coefs.columns[1:])\n                    coefs = coefs[['Column','Effect']+list(coefs.columns)[1:-1]].round(2)\n\n                    coefs['Max'] = coefs.max(1); coefs.sort_values('Max', ascending = False, inplace = True); del coefs['Max'];\n                return coefs\n            else: return None\n        else:\n            if self.layers == 0:\n                full = X*self.coef_\n                transformed = full.sum(1) + self.bias_\n                selects, unselects, worst, best, W, B, L, original_G, original_B, overall = [],[],[],[],[],[],[],[],[],[]\n\n                for i, (col, mu) in enumerate(zip(self.columns, self.means)):\n                    if np.isnan(mu):\n                        cond = (X[col]!=0)\n                        select = transformed.loc[cond]\n                        unselect = transformed.loc[~cond]\n                        selects.append(select.mean())\n                        unselects.append(unselect.mean())\n\n                        original = X.loc[cond].mean(0)\n                        d = full.loc[cond].mean(0)\n                        dx = full.loc[~cond].mean(0)\n\n                        d = pd.DataFrame({col: d, 'Abs': np.abs(d)}).sort_values('Abs', ascending = False)[col]\n                        s = (d.index == col)\n                        d = d.loc[~s].sort_values(ascending = False)\n                        first = d.index[0]; end = d.index[-1]\n                        best.append(first)\n                        B.append(d[0]-dx.loc[first])\n                        worst.append(d.index[-1])\n                        W.append(d[-1]-dx.loc[end])\n                        L.append(len(select))\n\n                        original_G.append(original.loc[first])\n                        original_B.append(original.loc[end])\n                    else:\n                        selects.append(np.nan); unselects.append(np.nan); L.append(np.nan)\n\n                        gt = (full.gt(full[col], axis = 'index')*full)\n                        gt[gt == 0] = np.nan; gt_means = gt.mean(0).sort_values(ascending = False)\n                        changes = gt.subtract(full[col], axis = 'index').mean(0)\n                        b = gt_means.index[0]; b_add = changes.loc[b]; b_contrib = gt_means.iloc[0]\n                        best.append(b); B.append(b_add); original_G.append(b_contrib)\n\n                        lt = (full.lt(full[col], axis = 'index')*full)\n                        lt[lt == 0] = np.nan; lt_means = lt.mean(0).sort_values(ascending = True)\n                        changes = lt.subtract(full[col], axis = 'index').mean(0)\n                        w = lt_means.index[0]; w_add = changes.loc[w]; w_contrib = lt_means.iloc[0]\n                        worst.append(w); W.append(w_add); original_B.append(w_contrib)\n\n\n                df = pd.DataFrame({'Coefficient':self.coef_, 'N':L,'If Stays':selects, 'Removed':unselects, 'Change if Removed': 0, 'Stay' : 0,\n                                  'Best Combo':best, 'Best Addon':B,'Best Contrib':original_G,'Worst Combo':worst, 'Worst Reduced':W, 'Worst Contrib':original_B})\n\n                df['Change if Removed'] = df['Removed'] - df['If Stays']\n                df['Stay'] = (df['Change if Removed'] < 0); df['Abs'] = np.abs(df['Change if Removed'])\n                df.loc[df['N'].isnull(), 'Stay'] = np.nan\n                df['Abs_Coef'] = np.abs(df['Coefficient'])\n                df.index = self.columns\n                df.sort_values(['Abs','Abs_Coef'], ascending = [False,False], inplace = True)\n                df.pop('Abs'); df.pop('Removed'); df.pop('Abs_Coef');\n                return df\n            else: return None\n\n    def _fit_keras(self, X, Y):\n        self.model.add(GaussianNoise(0.01, input_shape = (X.shape[1],)))\n        \n        for l in range(self.layers):\n            self.model.add(Dense(X.shape[1], kernel_initializer = glorot_normal(seed = 0)))\n            self.model.add(Activation(self.activation))\n            self.model.add(BatchNormalization())\n            self.model.add(Dropout(0.15))\n            self.model.add(GaussianNoise(0.01))\n            \n        self.model.add(Dense(self.out, kernel_initializer = glorot_normal(seed = 0)))\n        self.model.add(Activation(self.activ))\n    \n        earlyStopping = EarlyStopping(monitor = 'val_loss', patience = int(self.early_stopping*(self.layers\/2+1)), verbose = 0, mode = 'min')\n        reduce_lr_loss = ReduceLROnPlateau(monitor = 'val_loss', factor = 0.1, patience = int(1*(self.layers\/2+1)), verbose = 0, epsilon = 1e-4, mode = 'min')\n        cycle = CyclicLR(base_lr = 0.0005, max_lr = self.lr, step_size = 2000, mode = 'exp_range')\n        checkpoint = ModelCheckpoint('Best_Model.hdf5', save_best_only = True)\n        \n        self.metrics = ['acc']        \n        if not self.logistic: self.class_weights = None; self.metrics = None\n        \n        self.model.compile(optimizer = Nadam(), loss = self.loss, metrics = self.metrics)\n\n        if len(X) < 100: bs = 10\n        elif len(X) < 200: bs = 20\n        elif len(X) < 300: bs = 30\n        else: bs = 32\n\n        self.history = self.model.fit(X, Y, epochs = self.epochs, batch_size = bs, verbose = 2, validation_split = self.test_size, shuffle = True,\n                    callbacks = [earlyStopping, TerminateOnNaN(), reduce_lr_loss, cycle, checkpoint], \n                   class_weight = self.class_weights)\n        self.model = load_model('Best_Model.hdf5')\n        if self.layers == 0: self.coef_, self.bias_ = self.model.get_weights()\n        else: self.coef_ = self.model.get_weights()\n        self.lr = cycle\n        \n        \n    def _fit_sklearn(self, X, Y):\n        x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size = self.test_size, random_state = self.random_state)\n\n        if self.logistic: \n            if len(np.unique(Y)) > 2: \n                self.model = LogisticRegression(n_jobs = -1, class_weight = 'balanced', \n                                               multi_class = 'multinomial', solver = 'saga', tol = 0.1,\n                                              max_iter = int((self.epochs)\/3))\n            else:\n                self.model = LogisticRegression(n_jobs = -1, class_weight = 'balanced', \n                                               multi_class = 'ovr', solver = 'saga', tol = 0.1,\n                                              max_iter = int((self.epochs)\/3))\n            self.scorer_ = accuracy_score\n        else: self.scorer_ = mean_squared_error\n            \n        self.model.fit(x_train, y_train)\n        self.coef_, self.bias_ = self.model.coef_.T, self.model.intercept_.T\n\n        if self.logistic: text = 'accuracy'\n        else: text = 'error'\n        print('Training {} = {}'.format(text, self.scorer_(y_train, self.model.predict(x_train))*100))\n        print('Testing {} = {}'.format(text, self.scorer_(y_test, self.model.predict(x_test))*100))\n        \n\n\n    def _range_unique(self, x):\n        s = x.sort_values(ascending = True).values\n\n        mins, maxs = np.round(s[0], 2), np.round(s[-1], 2)\n        length = len(s)\/4\n        qtr1, qtr3 = np.round(s[int(length)], 2), np.round(s[int(3*length)], 2)\n        return sorted(set([mins, qtr1, qtr3, maxs]))\n\n\n    def _scaler(self, X):\n        result = []; means = []; stds = []\n        \n        for col in X.columns:\n            df = X[col]\n            if df.nunique() == 2 and df.min() == 0 and df.max() == 1:\n                result.append(df); means.append(np.nan); stds.append(np.nan)\n            else:\n                mu, std = df.mean(), df.std()\n                means.append(mu); stds.append(std)\n                result.append((df-mu)\/std)\n        return pd.concat(result, 1), np.array(means), np.array(stds)\n\n\n    def _transform(self, X):\n        if self.scale:\n            final = []\n            for col, mu, std in zip(self.columns, self.means, self.stds):\n                if np.isnan(mu): final.append(X[col])\n                else: final.append((X[col]-mu)\/std)\n            X = pd.concat(final, 1)\n        return X\n\n        \n##----NATURAL LANG PROCESSING\ndef lower(x):\n    if type(x) is pd.Series: return x.str.lower()\n    else: return [y.lower() for y in x]\n\ndef upper(x):\n    if type(x) is pd.Series: return x.str.upper()\n    else: return [y.upper() for y in x]\n    \ndef remove_space(x):\n    '''Removes duplicate spaces'''\n    return x.str.replace('[\\s]{2,}', ' ')\n\ndef keep_length(x, length = 2):\n    '''Removes objects in lists spaces'''\n    return x.apply(lambda x: [y for y in x if len(re.sub(r'[0-9a-zA-Z\\']','',y)) > 0 or len(y) > length])\n\ndef clean_up(x):\n    '''Deletes \\n \\r'''\n    return x.str.replace('\\n',' ').str.replace('\\r',' ').str.lstrip().str.rstrip()\n\ndef clean_text(x, method = 'basic'):\n    '''Cleans a column of text. 2 methods exist:\n    1. basic:\n        1. Lowercase FIRST letter ONLY (Apple == apple) (APPLE == aPPLE --> symbolizes caps)\n        1. Remove all digits (**replace**)\n        2. Clean up some messy strings using (**clean_up**)\n        3. Reduce spaces to just 1 (3 spaces == 1 space) (**remove_space**)\n        4. Split by space OR punctuation OR any symbols (**split**)\n        5. Ignore words that are 2 letters or shorter (a, by, am, I, ...) (**keep_length**)\n    \n    2. complex:\n        1. Does all basic INCLUDING:\n            1. Places :), :(, other smileys as separate entities.\n            2. Removes all digit like, number like, email\/url like objects as well and puts them into another variable.\n    '''\n    if method == 'basic':\n        callback = lambda pat: pat.group(0).lower()\n        lowered = x.str.replace(r'([A-Z])([^A-Z\\s\\-!@#$%\\^&\\*\\(\\):\\\"\\,\\.])', callback)\n        cleaned = clean_up(lowered)\n        no_digits = cleaned.str.replace(r'[0-9]+',' ')\n        space_fixed = remove_space(no_digits)\n        splitted = split(space_fixed, '\\S')\n        \n        k = []\n        for i in splitted:\n            j = []\n            try:\n                for y in i:\n                    try:\n                        if len(re.sub(r'[0-9a-zA-Z\\']','',y)) > 0 or len(y) > 2:\n                            j.append(y)\n                    except: pass\n                k.append(j)\n            except: k.append([])\n        x = pd.Series(k)\n    return x\n\nimport statsmodels.api as sm\nfrom scipy import stats\nstats.chisqprob = lambda chisq, df: stats.chi2.sf(chisq, df)\n\ndef Results(results, columns, y = None):\n    '''take the result of an statsmodel results table and transforms it into a dataframe'''\n    pvals = results.pvalues\n    coeff = results.params\n    significant = {3:'@@@',2:'@@',1:'@',0:'X',4:'@@@@',5:'@@@@@'}\n\n    if y is not None:\n        #conf_lower = results.conf_int()[0]\n        #conf_higher = results.conf_int()[1]\n        names = np.array(sorted(np.unique(y))[1:])\n        if 'float' in str(names.dtype):\n            names = names.astype(int)\n        pvals = pd.DataFrame(pvals)\n        pvals.columns = [f'(Y={x})-P>|z|' for x in names]\n        coeff = pd.DataFrame(coeff)\n        coeff.columns = [f'(Y={x})-coef' for x in names]\n        #conf_lower = pd.DataFrame(conf_lower)\n        #conf_lower.columns = [f'(Y={x})-[0.025' for x in names]\n        #conf_higher = pd.DataFrame(conf_higher)\n        #conf_higher.columns = [f'0.975]-(Y={x})' for x in names]\n        \n        very_sig = ((pvals <= 0.01) * 3).max(1)\n        sig = (((pvals > 0.01) & (pvals <= 0.05)) * 2).max(1)\n        kinda = (((pvals > 0.05) & (pvals <= 0.1)) * 1).max(1)\n        final = very_sig + sig + kinda\n        final.loc[final > 4] = 5\n        final.name = 'imp'\n        codes = final.replace(significant)\n        codes.name = 'sig'\n        \n        results_df = pd.concat([coeff, pvals, final, codes], 1).sort_values('imp', ascending = False)\n        results_df.pop('imp');\n    else:\n        #conf_lower = results.conf_int().T[0]\n        #conf_higher = results.conf_int().T[1]\n        \n        check = pd.Series(pvals)\n        very_sig = (check <= 0.01) * 3\n        sig = ((check > 0.01) & (check <= 0.05)) * 2\n        kinda = ((check > 0.05) & (check <= 0.1)) * 1\n        final = very_sig + sig + kinda\n\n        results_df = pd.DataFrame({\"coef\":coeff,\n                                   \"P>|z|\":pvals,\n                                   \"sig\":final.replace(significant),\n                                   #\"[0.025\":conf_lower,\n                                   #\"0.975]\":conf_higher\n                                    })\n        #results_df['Columns'] = columns\n        results_df = results_df[['coef','P>|z|','sig']].sort_values('P>|z|')\n    return results_df\n\n\n\nclass Inference(BaseEstimator, RegressorMixin, ClassifierMixin):\n\n\tdef __init__(self, logistic = False):\n\t\tself.logistic = logistic\n\t\treturn None\n\n\tdef fit(self, X, y):\n\t\tif type(X) is not pd.DataFrame:\n\t\t\tX = pd.DataFrame(X)\n\n\t\tif self.logistic:\n\t\t\tif len(np.unique(y)) == 2:\n\t\t\t\tmodel = sm.Logit(y, X)\n\t\t\t\tresults = model.fit(maxiter = 200)\n\t\t\t\tself.coef = Results(results, X.columns)\n\t\t\telse:\n\t\t\t\tmodel = sm.MNLogit(y, X)\n\t\t\t\tresults = model.fit(maxiter = 200)\n\t\t\t\tself.coef = Results(results, X.columns, y)\n\t\telse:\n\t\t\tmodel = sm.OLS(y, X)\n\t\t\tresults = model.fit()\n\t\t\tself.coef = Results(results, X.columns)\n\t\tself.model = results\n\t\treturn self\n\n\tdef predict(self, X):\n\t\treturn self.model.predict(X)\n\n\tdef coefficients(self):\n\t\treturn self.coef\n","70a22360":"#read pokemon data and save it as dataframe\nfile = files()\npokemon_file = file[0]\npokemon = read(pokemon_file)\npokemon","f8b7dde7":"#get rid of unwantted columns\ncolumns_to_remove = ['#','name','total']\ntry:\n    y = pokemon['total']\n    pokemon.drop(columns_to_remove,axis = 1, inplace = True)\nexcept:\n    pass","80b106f5":"pokemon","257962f2":"pokemon.columns","b1d6605b":"type_1 = tally(pokemon['type_1'],multiple = True,min_count = 3)\ntype_1","4552c2ac":"type_2 = tally(pokemon['type_2'],multiple = True,min_count = 3)\ntype_2","64471bf1":"legendary = tally(pokemon['legendary'],multiple = True,min_count = 3)\nlegendary","f527ecba":"generation = tally(pokemon['generation'],multiple = True,min_count = 3)\ngeneration","2e0a1ef5":"plot(x = pokemon['type_2'],y = y,colour = pokemon['type_1'],style = 'lineplot')","e19550bd":"defense_t = ternary(pokemon['defense'], '>60', 1, 0)\ndefense_t.name = 'defense_t'\ndefense_t","55acdd23":"hp_t = ternary(pokemon['hp'],'>=60',-1,1)\nhp_t.name = 'hp_t'\nhp_t","88e7bd4e":"speed_t = ternary(pokemon['speed'],'<=100',0,1)\nspeed_t.name = 'speed_t'\nspeed_t","a4e81159":"clean_pokemon = clean(pokemon)\nclean_pokemon = exclude(clean_pokemon,'legendary')\nclean_pokemon","cfb1d5e9":"conc_pokemon = pd.concat([clean_pokemon,type_1,type_2,legendary,generation,defense_t,hp_t,speed_t],axis = 1, sort = False)\nconc_pokemon","41b8e8d2":"model = Inference()","2e721adf":"model.fit(conc_pokemon, y)","0d6858ba":"model.coefficients()","8a280a1d":"y_pred = model.predict(conc_pokemon)","3f606d7c":"plot(y_pred, y, style = 'regplot', power = 1)","849f04f1":"model.score(conc_pokemon,y)","5d647783":"help(model.score)","5ee786f3":"new_pokemon = conc_pokemon[['speed','defense','sp._atk','attack','hp','sp._def']]\n#,'legendary_(False)','type_1_(Dark)','generation_(6)','type_2_(Ice)','generation_(3)','type_2_(Poison)']]\nnew_model = Inference()\nnew_model.fit(new_pokemon,y)","e2f7a10a":"new_model.coefficients()","02b599df":"y_new_pred = new_model.predict(new_pokemon)\nplot(y_new_pred, y, style = 'regplot', power = 1)","c1162f66":"new_model.score(new_pokemon,y)","9d6b8b18":"Note this is VERSION 2. There are 2 VERSIONS.\n\n<h1> THIS IS VERSION TWO <\/h1>\n\n<h1> THIS IS VERSION TWO <\/h1>\n\n<h1> Do VERSION 1 if your ZID IS ODD (500201, 500303, 500607, 508085, 513909 etc [if your ZID ENDS in 1, 3, 5, 7, 9] <\/h1>\n\n<h1> Do VERSION 2 if your ZID [ENDS IN 0, 2, 4, 6, 8] <\/h1>","5d42090f":"<h2> Question 2 [1.5 marks] <\/h2>\n\nUsing the following columns:\n1. Type_1\n2. Type_2\n3. Legendary\n4. Generation\n\nUse TALLY and perform BAG OF WORDS on all 4 columns. Save all 4 to 4 separate variables.\n\nIE:\na = tally(...)\nb = tally(...)\n...\n\nALSO, only perform BAG OF WORDS if the count(each word) >= 3\n\nThis means within TALLY, keep only >=3 count","0f8a027b":"<h2> Question 1 [0.5 marks] <\/h2>\nRead the Pokemon data in.\nEXCLUDE COLUMNS from the data including:\n1. #\n2. name\n3. total.\n\nMake a new variable Y.\n\nY = the column TOTAL in data","9e54e7f4":"<h2> Question 8 [2 marks] <\/h2>\n\nLooking at the COEFFICIENTS and SIGNIFANCE columns, KEEP ONLY the GOOD columns.\n\nThen, run another INFERENCE model like in Question 7 on this NEW data, and verify if the MODEL IS GOOD OR BAD.\n\nWrite 100 words or less whether the model is WORSE OR BETTER than the one in Question 7, for both INTERPRETABLE purposes, and USEFULNESS.","5449b8ca":"<h2> Question 3 [0.5 marks] <\/h2>\n\nPLOT a LINEPLOT x = Type_2, y = Y (from Question 1), and COLOUR = Type_1. The data is from Question 1 after you excluded the columns","25c094bb":"<h1> MARK5826 Assignment 2 Version 2 <\/h1>\n\n<h1> You are now using POKEMON data!! Total marks is 20 (tableau = 10, python = 10), but is scaled down to 13. <\/h1>\n\n\nNote this is VERSION 2. There are 2 VERSIONS.\n\n<h1> THIS IS VERSION TWO <\/h1>\n\n<h1> THIS IS VERSION TWO <\/h1>\n\n<h1> Do VERSION 1 if your ZID IS ODD (500201, 500303, 500607, 508085, 513909 etc [if your ZID ENDS in 1, 3, 5, 7, 9] <\/h1>\n\n<h1> Do VERSION 2 if your ZID [ENDS IN 0, 2, 4, 6, 8] <\/h1>","7460e0a5":"New model is even better than the old one.\nI not only get rid of the insignificant features and also significant features with 0 coefficient that have no impact on the predictive model. The new model turns out to be the same good as the previous one. We still got a y=x plot, all the coefficient of variables are significant and R square = 1. However,  the new model with fewer variables(features) is easier to interpret and easier to apply to do furthur prediction.","e0b07385":"<h2> Question 5 [1 marks] <\/h2>\n\nCLEAN the DATA from Question 1.\n\nEXCLUDE the column LEGENDARY\n\nCOMBINE all the datas \/ new variables into 1 large data (combine all data from Question 1,2,3,4,5)","0c6cff51":"Model is BETTER, since removal of insignificant columns did not cause the predictions to diverge. Also BETTER for usefulness, since fewer columns showcase easier interpretation and is easier to present.","a042dddb":"The model is amazingly good. From the plot of y_true and y_pred, you can see all the values lies in the line y = x. It seems like every y_true =  y_pred which means our model is able to predict the correct y for every X.  To confirm it, we call the model.score function to get the R square,  the coefficient of determination. It turns out to be 1 and proves numerically the model is so good that it got every prediction right.","1d2a462c":"<h2> Question 9 [1.5 marks] <\/h2>\n\nINTERPRET your MODEL. Write 100 - 200 words EXPLAINING your NEW MODEL's's (from question 9) coefficients, PVALUES and SIGNIFANCE. Which columns are good, and how do they contribute to the prediction of the column TOTAL (Y)?","2df1874f":"All the variables remained in the new model have coefficient 1. They all have a very small p-value which means they are highly signficant. Besides removing the insignificant variables,  I get rid of the signficant values with coefficient 0 as well from the previous model, considering the coefficients 0 will not affect the value of the right hand side of the equation.  From the significance, now every column in the new X are good. Now we can see clearly from the equation of the new model, y(total) is just the simple sum of all the variables 'speed','defense','sp._atk','attack','hp','sp._def '. So the equation goes like Total = speed + defense + sp._atk  + attack +  hp + sp._def.  This equation can be applied to predict the column of Total. ","3504b63c":"<h2> Question 4 [1 marks] <\/h2>\n\nUsing TERNARY on the following columns:\n1. DEFENSE\n2. HP\n2. SPEED\n\nWhere you do:\n\n1. DEFENCE >60, letting TRUE = 1 and FALSE = 0\n2. HP >=60, letting TRUE = -1, FALSE = 1\n3. SPEED <= 100, letting TRUE = 0 and FALSE = 1\n\nSave all 3 to new variables.\n","7d5c78ab":"<h2> Question 6 [1 marks] <\/h2>\n\nCreate a INFERENCE MODEL.\n\nFIT the model on Y with the combined data.\n\nShow the COEFFICIENTS.","bfce6f45":"<h2> Question 7 [1 marks] <\/h2>\n\nUsing PLOT, plot X = predictions from model and Y = true Y. Choose REGPLOT, with POWER = 1.\n\nIn UNDER 100 words, explain if this model is GOOD or BAD."}}