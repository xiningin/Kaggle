{"cell_type":{"7c8bda9d":"code","9a991b20":"code","99198a46":"code","03a8a316":"code","29952794":"code","ae8149ff":"code","946875db":"code","a8d9b258":"code","4e62dfec":"code","db66178c":"code","7dacd52f":"code","b671effe":"code","465e1282":"code","588d4622":"code","b870b1be":"code","ec86e70c":"code","3c89c7d5":"code","ae53a4c1":"code","294522bd":"code","fddff85e":"code","fbe5aa55":"code","f0b71f87":"code","cd387008":"code","9b3911c1":"code","0330b03b":"code","f03909e5":"code","e7db9783":"markdown","3df2274a":"markdown","1b73baba":"markdown","db811ac2":"markdown","e921d423":"markdown","48c45d6b":"markdown","45eabf32":"markdown","6f8fa5d3":"markdown","21269dc2":"markdown","e2eb1bf0":"markdown","2fdc1261":"markdown","13d3b54f":"markdown","4fd764cc":"markdown","abd7fa3b":"markdown","263a52dc":"markdown","bc2afb11":"markdown","204321a3":"markdown","ec09c0ce":"markdown","e9850bc3":"markdown","46ed70a3":"markdown"},"source":{"7c8bda9d":"import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split","9a991b20":"train = pd.read_csv('..\/input\/tabular-playground-series-sep-2021\/train.csv')\ntest = pd.read_csv('..\/input\/tabular-playground-series-sep-2021\/test.csv')\nsample_submission = pd.read_csv('..\/input\/tabular-playground-series-sep-2021\/sample_solution.csv')","99198a46":"train.head()","03a8a316":"test.head()","29952794":"train.shape, test.shape","ae8149ff":"train['n_nan'] = train.isnull().sum(axis=1)\ntest['n_nan'] = test.isnull().sum(axis=1)","946875db":"FEATURES = train.columns[:-2]\nfor col in FEATURES:\n    avg_val_train = train[col].mean()\n    avg_val_test = test[col].mean()\n    train[col].fillna(avg_val_train, inplace=True)\n    test[col].fillna(avg_val_test, inplace=True)","a8d9b258":"X = train.drop(['claim'], axis=1)\ny = train['claim']","4e62dfec":"x_train, x_val, y_train, y_val = train_test_split(X, y, test_size=0.1, random_state=0)","db66178c":"from sklearn.preprocessing import MinMaxScaler\nscaler_x = MinMaxScaler()","7dacd52f":"scaler_x.fit(x_train)\nx_train = scaler_x.transform(x_train)\nx_val = scaler_x.transform(x_val)\nx_test = scaler_x.transform(test)","b671effe":"!pip install scikit-learn-intelex --progress-bar off >> \/tmp\/pip_sklearnex.log","465e1282":"from sklearnex import patch_sklearn\npatch_sklearn()","588d4622":"from sklearn.linear_model import Ridge\nfrom sklearn.metrics import roc_auc_score\nimport optuna","b870b1be":"def objective_ridge(trial):\n    params ={\n        'alpha': trial.suggest_float('alpha', 0.0, 2.0),\n    }\n    model = Ridge(**params).fit(x_train, y_train)\n    y_pred = model.predict(x_val)\n    loss = roc_auc_score(y_val, y_pred)\n    return loss","ec86e70c":"study = optuna.create_study(sampler=optuna.samplers.TPESampler(seed=123),\n                            direction=\"maximize\",\n                            pruner=optuna.pruners.HyperbandPruner())","3c89c7d5":"%%time\nstudy.optimize(objective_ridge, n_trials=100)","ae53a4c1":"full_x = np.concatenate((x_train, x_val), axis=0)\nfull_y = np.concatenate((y_train, y_val), axis=0)","294522bd":"%%time\nfinal_model = Ridge(**study.best_params).fit(full_x, full_y)","fddff85e":"y_pred = final_model.predict(x_test)","fbe5aa55":"sample_submission['claim'] = y_pred\nsample_submission.to_csv('submission.csv', index=False)\nsample_submission.head(10)","f0b71f87":"from sklearnex import unpatch_sklearn\nunpatch_sklearn()","cd387008":"from sklearn.linear_model import Ridge","9b3911c1":"study = optuna.create_study(sampler=optuna.samplers.TPESampler(seed=123),\n                            direction=\"maximize\",\n                            pruner=optuna.pruners.HyperbandPruner())","0330b03b":"%%time\nstudy.optimize(objective_ridge, n_trials=100)","f03909e5":"%%time\nfinal_model = Ridge(**study.best_params).fit(full_x, full_y)","e7db9783":"<big>Let's look at the data.<\/big>","3df2274a":"<big><strong>Select parameters<\/strong><\/big>","1b73baba":"# Conclusions\n<big>We can see that using only one classical machine learning algorithm may give you a pretty hight accuracy score. We also use well-known libraries Scikit-learn and Optuna, as well as the increasingly popular library Intel\u00ae Extension for Scikit-learn. Noted that Intel\u00ae Extension for Scikit-learn gives you opportunities to:<\/big>\n\n* <big>Use your Scikit-learn code for training and inference without modification.<\/big>\n* <big>Speed up selection of parameters <strong>from 2 minutes to 46 seconds.<\/strong><\/big>\n* <big>Get predictions of the similar quality.<\/big>","db811ac2":"<big>Patch original scikit-learn.<\/big>","e921d423":"<h2>Importing data<\/h2>","48c45d6b":"# Preprocessing\n\n<big>Let's add a feature with the number of missing values in each line.<\/big>","45eabf32":"<big><strong>Training the model with the selected parameters.<\/strong><\/big>","6f8fa5d3":"<big>For classical machine learning algorithms, we often use the most popular Python library, Scikit-learn. With Scikit-learn you can fit models and search for optimal parameters, but\u202fit\u202fsometimes works for hours.<\/big><br><br>\n\n<big>I want to show you how to use Scikit-learn library and get the results faster without changing the code. To do this, we will make use of another Python library, \u202f<a href='https:\/\/github.com\/intel\/scikit-learn-intelex'>Intel\u00ae Extension for Scikit-learn*<\/a>.<\/big><br><br>\n\n<big>I will show you how to <strong>speed up your kernel more than 2 times<\/strong> without changing your code!<\/big>","21269dc2":"<big>Normalize the data.<\/big>","e2eb1bf0":"<h2>Installing Intel(R) Extension for Scikit-learn<\/h2>\n\n<big>Use Intel\u00ae Extension for Scikit-learn* for fast compute Scikit-learn estimators.<\/big>","2fdc1261":"<big>Let's see the execution time without patch.<\/big>","13d3b54f":"<big>Let's see the execution time with Intel(R) Extension for Scikit-learn.<\/big>","4fd764cc":"# Using optuna to select parameters for Ridge algorithm\n<big>In Ridge regression, we add a penalty term which is equal to the square of the coefficient. The L2 term is equal to the square of the magnitude of the coefficients. We also add a coefficient  lambda  to control that penalty term.<\/big><br><br>\n<big>We adjust hyperparameters for the best result.<\/big><br><br>\n\n<big>Parameter that we select:<\/big><br>\n<big>* <code>alpha<\/code> - Regularization parameter. Regularization improves the solution and reduces the variance of estimates.<br> <\/big>\n","abd7fa3b":"<big>Let\u2019s run the same code with original scikit-learn and compare its execution time with the execution time of the patched by Intel(R) Extension for Scikit-learn.<\/big>","263a52dc":"<big>Fill missing feathures with mean value.<\/big>","bc2afb11":"<big>Split the data into train and test sets.<\/big>","204321a3":"<big><strong>Prediction.<\/strong><\/big>","ec09c0ce":"<big><strong>Select parameters<\/strong><\/big>","e9850bc3":"<big>Save the results in 'submission.csv'.<\/big>","46ed70a3":"# Now we use the same algorithms with original scikit-learn"}}