{"cell_type":{"67c04800":"code","fc72f134":"code","931706af":"code","82e864d9":"code","eba0d599":"code","86c23cd9":"code","1c5e867a":"code","ad2f3213":"code","af15669f":"code","3596844d":"code","0f93a2ed":"code","eeffc5e7":"code","56fc60c6":"code","554172d2":"code","0bfc2ebd":"code","4d18a0be":"code","a6f653f2":"code","601e6cc0":"code","f6b017d9":"code","9f28fa1c":"code","f704f25d":"code","3cdc8f8c":"code","7712e519":"code","bda2760a":"code","4720ecb9":"code","c80a92fe":"code","5477f92c":"code","7c20c8df":"code","ecce7dfe":"code","9a80c08f":"code","f8c40c1c":"code","d5247dc0":"code","fcaf5868":"code","4b6b8419":"code","9c64ddcc":"code","0e1ed77a":"code","d9f1e872":"code","3a103eed":"code","b1acb8ae":"code","6d9c1cc3":"code","da524922":"code","19194269":"code","8bde5c19":"code","fe84fe8b":"code","29bb51ce":"code","1186ad3f":"code","3273fce2":"code","65c2c618":"code","24d98dd4":"code","898bf080":"code","a43bd216":"code","acf948e5":"code","46ef3de5":"code","9f430fa3":"code","1531cce8":"code","8cbfb145":"code","f9e982e2":"code","e03ddd8c":"code","39a557b4":"code","73c76779":"code","a0fa73d5":"code","72d4d772":"code","9ab1db70":"code","5ebbbd4e":"code","aa105a39":"code","c251bb72":"code","307084b1":"code","a12a3998":"code","2c5a1ee8":"code","7d64ebca":"code","4b7b4fb6":"code","04f6905a":"code","3bf2e05d":"code","383e286c":"code","2f21205e":"code","6480103d":"code","47f1218c":"code","b3f1a8c7":"code","6a7a2f1c":"code","b749e7ba":"code","89a145d1":"code","df950723":"code","00fe37c3":"code","b9fcf38a":"code","a447c345":"code","8eb09fd9":"code","d473cbea":"code","5a5dfece":"code","7174a64c":"code","e7fdc9b4":"code","0dd84326":"markdown","75018522":"markdown","da257651":"markdown","b4d941fc":"markdown","4f9df01d":"markdown","d5a67d63":"markdown","820a1f49":"markdown","43cb2c1d":"markdown","5616cd5b":"markdown","fee5460e":"markdown","9ab3fee4":"markdown","a5a8d7c3":"markdown","4fcf0112":"markdown","d158fe26":"markdown","468a0795":"markdown","1585a3e1":"markdown","a54536d3":"markdown","8c95143c":"markdown","45b8e904":"markdown","91138981":"markdown","2b6d310b":"markdown","12068c4a":"markdown","e5103579":"markdown","0696a3f1":"markdown","f569b3a1":"markdown","74a0ef9b":"markdown","7da3b3e0":"markdown","c5365f0f":"markdown","8974adb3":"markdown","2d64dc5a":"markdown","07691b5c":"markdown"},"source":{"67c04800":"# Importing all the necessary packages\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n","fc72f134":"# Creating a dataframe \ndf=pd.read_csv(\"..\/input\/dean-de-cock-house-prices\/train.csv\",low_memory=False)\ndf.info()","931706af":"# Checking for missing values\ndf.isna().sum()","82e864d9":"# Creating a copy of the dataframe so the edits doesn't affect the original dataframe\ndf_tmp=df.copy()","eba0d599":"df_tmp.head().T","86c23cd9":"df_tmp.LotFrontage","1c5e867a":"# Checking the columns that contain strings\nfor label,content in df_tmp.items():\n    if pd.api.types.is_string_dtype(content):\n        print(label)","ad2f3213":"# Converting all the string data features into pandas categorical features\nfor label,content in df_tmp.items():\n    if pd.api.types.is_string_dtype(content):\n        df_tmp[label]=content.astype(\"category\").cat.as_ordered()\n        ","af15669f":"df_tmp.info()","3596844d":"df_tmp.MiscFeature.cat.categories","0f93a2ed":"df_tmp.MiscFeature.cat.codes","eeffc5e7":"for label,content in df_tmp.items():\n    if pd.api.types.is_numeric_dtype(content):\n        if pd.isnull(content).sum():\n            print(label)","56fc60c6":"# Fill the numeric columns with median as median is so robust than mean\nfor label,content in df_tmp.items():\n    if pd.api.types.is_numeric_dtype(content):\n        if pd.isnull(content).sum():\n            #Adding a binary column to tell if the data was missing\n            df_tmp[label+\"_is_missing\"]=pd.isnull(content)\n            #Filling the missing values with the median\n            df_tmp[label]=content.fillna(content.median())\n            ","554172d2":"for label,content in df_tmp.items():\n    if pd.api.types.is_numeric_dtype(content):\n        if pd.isnull(content).sum():\n            print(label)","0bfc2ebd":"for label,content in df_tmp.items():\n    if not pd.api.types.is_numeric_dtype(content):\n        if pd.isnull(content).sum():\n            print(label)","4d18a0be":"\nfor label,content in df_tmp.items():\n    if not pd.api.types.is_numeric_dtype(content):\n            #Adding a column to let us know what we are missing\n            df_tmp[label+\"_is_missing\"]=pd.isnull(content)\n            #Turn categories into number and add +1 to the codes\n            df_tmp[label]=pd.Categorical(content).codes+1\n            #Check the note above to understand why I'm using codes+1","a6f653f2":"# Checking to make sure that all the missing values in categories are filled\nfor label,content in df_tmp.items():\n    if not pd.api.types.is_numeric_dtype(content):\n            if pd.isnull(content).sum():\n                print(label)","601e6cc0":"# Checking the categorical codes of MiscFeature column\npd.Categorical(df_tmp[\"MiscFeature\"]).codes","f6b017d9":"df_tmp.isna().sum()","9f28fa1c":"#Building our baseline model\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.model_selection import train_test_split","f704f25d":"df_tmp.info()","3cdc8f8c":"%%time\nmodel=RandomForestRegressor(n_jobs=-1,random_state=69)\n\n\nX=df_tmp.drop(\"SalePrice\",axis=1)\ny=df_tmp[\"SalePrice\"]\n#Fit the model\nmodel.fit(X,y)\n","7712e519":"model.score(X,y)","bda2760a":"from sklearn.metrics import mean_squared_error\ndef rmsle(y_true,y_preds):\n    return np.sqrt(mean_squared_error(y_true,y_preds))\ndef show_scores(model,X,y):\n    y_preds=model.predict(X)\n    scores={\"Root Mean Squared Error\":rmsle(y,y_preds)}\n    return scores","4720ecb9":"show_scores(model,X,y)","c80a92fe":"%%time \n\nfrom sklearn.model_selection import RandomizedSearchCV\n# Tuning the hyper parameters of the model\nrs_grid={\n    \"n_estimators\":np.arange(10,100,10),\n    \"max_depth\":[None,3,5,10],\n    \"min_samples_split\":np.arange(2,20,2),\n    \"min_samples_leaf\":np.arange(1,20,2),\n    \"max_features\":[0.5,1,\"sqrt\",\"auto\"]\n}\n\nrs_model=RandomizedSearchCV(RandomForestRegressor(),param_distributions=rs_grid,n_iter=20,cv=5,verbose=True,random_state=69)\n\nrs_model.fit(X,y)","5477f92c":"rs_model.best_params_","7c20c8df":"ideal_model=RandomForestRegressor(n_estimators=60,min_samples_split=8,min_samples_leaf=1,max_features=0.5,max_depth=10,n_jobs=-1,random_state=69)","ecce7dfe":"ideal_model.fit(X,y)","9a80c08f":"X.shape","f8c40c1c":"test_df=pd.read_csv(\"..\/input\/dean-de-cock-house-prices\/test.csv\")\ntest_df","d5247dc0":"test_df.shape","fcaf5868":"# Checking for the missing numeric features\nfor label,content in test_df.items():\n    if pd.api.types.is_numeric_dtype(content):\n        if pd.isnull(content).sum():\n            print(label)","4b6b8419":"test_df.info()","9c64ddcc":"#Checking for the non numeric features\nfor label,content in test_df.items():\n    if not pd.api.types.is_numeric_dtype(content):\n        if pd.isnull(content).sum():\n            print(label)","0e1ed77a":"# Filling the numeric feature with median\nfor label,content in test_df.items():\n    if pd.api.types.is_numeric_dtype(content):\n        if pd.isnull(content).sum():\n            #Add a binary column to know which row is missing\n            test_df[label+\"_is_missing\"]=pd.isnull(content)\n            #Filling the missing content with median\n            test_df[label]=content.fillna(content.median())","d9f1e872":"for label,items in test_df.items():\n    if pd.api.types.is_numeric_dtype(content):\n        if pd.isnull(content):\n            print(label)\n            ","3a103eed":"#Convert the non numeric features into pandas categorical features\nfor label,content in test_df.items():\n    if not pd.api.types.is_numeric_dtype(content):\n        test_df[label+\"_is_missing\"]=pd.isnull(content)\n        test_df[label]=pd.Categorical(content).codes+1","b1acb8ae":"for label,content in test_df.items():\n    if not pd.api.types.is_numeric_dtype(content):\n        if pd.isnull(content).sum():\n            print(label)","6d9c1cc3":"test_df.shape","da524922":"missing=set(test_df.columns)-set(X.columns)\nmissing=list(missing)","19194269":"set(test_df.columns)-set(X.columns)","8bde5c19":"for i in range(len(missing)):\n    X[str(missing[i])]=False","fe84fe8b":"set(X.columns)-set(test_df.columns)","29bb51ce":"test_df.columns","1186ad3f":"ideal_model.fit(X,y)","3273fce2":"y_preds=ideal_model.predict(test_df)","65c2c618":"y_preds.max()\n","24d98dd4":"X.shape,test_df.shape,y.shape","898bf080":"# Making the dataframe in the format that kaggle asks for\nsub_df=pd.DataFrame()\nsub_df[\"Id\"]=test_df[\"Id\"]\nsub_df[\"SalePrice\"]=y_preds","a43bd216":"sub_df.head()","acf948e5":"sub_df.to_csv(\"\/kaggle\/working\/subfile.csv\",index=False)","46ef3de5":"from catboost import CatBoostRegressor","9f430fa3":"cat =CatBoostRegressor(random_state=69)\ncat.fit(X,y)","1531cce8":"#Predicting the test data with catboost\ncat_preds=cat.predict(test_df)","8cbfb145":"cat_mean=cat_preds.mean()","f9e982e2":"# Saving the baseline's prediction mean\nrandomf_mean=y_preds.mean()","e03ddd8c":"# Checking how well our catboost's predictions are\nrandomf_mean-cat_mean","39a557b4":"boosted_csv=pd.DataFrame()\nboosted_csv[\"Id\"]=test_df[\"Id\"]\nboosted_csv[\"SalePrice\"]=cat_preds","73c76779":"boosted_csv.head()","a0fa73d5":"boosted_csv.to_csv(\"\/kaggle\/working\/boosted.csv\",index=False)","72d4d772":"cat_mean","9ab1db70":" model_CBR = CatBoostRegressor()","5ebbbd4e":"parameters = {'depth'         : [6,8,10],\n                  'learning_rate' : [0.01, 0.05, 0.1],\n                  'iterations'    : [30, 50]\n                 }\n","aa105a39":"from sklearn.model_selection import GridSearchCV\ngrid = GridSearchCV(estimator=model_CBR, param_grid = parameters, cv = 2, n_jobs=-1)","c251bb72":"grid.fit(X,y)","307084b1":"grid.best_score_","a12a3998":"grid.best_params_","2c5a1ee8":"tuned_cat=CatBoostRegressor(depth=6,iterations=100, learning_rate= 0.1)","7d64ebca":"tuned_cat.fit(X,y)","4b7b4fb6":"tuned_cat_predicts=tuned_cat.predict(test_df)","04f6905a":"tuned_mean=tuned_cat_predicts.mean()","3bf2e05d":"tuned_mean","383e286c":"import xgboost as xgb","2f21205e":"xg_reg = xgb.XGBRegressor(objective ='reg:linear', colsample_bytree = 0.3, learning_rate = 0.1,\n                max_depth = 5, alpha = 10, n_estimators = 10)","6480103d":"data_dmatrix = xgb.DMatrix(data=X,label=y)","47f1218c":"\ntrain_x = X.values\ntest_x = test_df.values","b3f1a8c7":"xg_reg.fit(train_x,y)","6a7a2f1c":"xg_predicts=xg_reg.predict(test_x)","b749e7ba":"xg_predicts.mean()","89a145d1":"xgboost_csv=pd.DataFrame()\nxgboost_csv[\"Id\"]=test_df[\"Id\"]\nxgboost_csv[\"SalePrice\"]=xg_predicts","df950723":"xgboost_csv.head()","00fe37c3":"boosted_csv.head()","b9fcf38a":"xgboost_csv.to_csv(\"\/kaggle\/working\/xgboosted.csv\",index=False)","a447c345":"xgb1 = xgb.XGBRegressor()\nparameters = {'nthread':[4], #when use hyperthread, xgboost may become slower\n              'objective':['reg:linear'],\n              'learning_rate': [.03, 0.05, .07], #so called `eta` value\n              'max_depth': [5, 6, 7],\n              'min_child_weight': [4],\n              'subsample': [0.7],\n              'colsample_bytree': [0.7],\n              'n_estimators': [500]}\n\nxgb_grid = GridSearchCV(xgb1,\n                        parameters,\n                        cv = 2,\n                        n_jobs = -1,\n                        verbose=True)\n\n","8eb09fd9":"X_mat=X.values\ntest_mat=test_df.values","d473cbea":"xgb_grid.fit(X_mat,\n         y)","5a5dfece":"xgbtuned_predictions=xgb_grid.predict(test_mat)","7174a64c":"print(xgb_grid.best_score_)\nprint(xgb_grid.best_params_)","e7fdc9b4":"xgbtuned_predictions.mean()","0dd84326":"## Keeping trying with different parameters and different Algorithms Because Each one differs from one other.The main tidbit is that if you are focusing on accuracy the model has to be trained in more data ","75018522":"## Filling the missing values in the df_tmp data","da257651":"### Sometimes the training dataframe doesn't have the same attributes like the test dataframe so checking if there are any","b4d941fc":"Now Test dataframe has same attributes as Training dataframe","4f9df01d":"### Coverting the strings features into pandas categorical features","d5a67d63":"## House Prices Prediction\n\n### Competition Link:\nhttps:\/\/www.kaggle.com\/c\/house-prices-advanced-regression-techniques\n\n### Problem Statement:\nTo predict the sale prices of the houses given the features of the location and the house\n\n### Data \nDataset is called Ames Housing dataset compiled by dean de cock in kaggle\n\n**Link**:https:\/\/www.kaggle.com\/c\/house-prices-advanced-regression-techniques\/data\n\n### Evaluation\nThe Kaggle vealuates our submission on the basis of Root-Mean-Squared-Error (RMSE)\n\n**Link**:https:\/\/www.kaggle.com\/c\/house-prices-advanced-regression-techniques\/overview\/evaluation\n\n### Features\nTo know more about the data there is a data dictionary available which was also compiled by Dean de Cockunder the data fields heading\n\n**Link**:https:\/\/www.kaggle.com\/c\/house-prices-advanced-regression-techniques\/data\n","820a1f49":"# Step 1:The Baseline predictions\n\n## Building the Model","43cb2c1d":"These above attributes are not in the test dataframe","5616cd5b":"## This predictions was not better than the old xgboost predictions so I would recommend to stick with the older xgboost model","fee5460e":"### Since Checking the Tuned catboost performance was not satisfying now going on with Xgboost","9ab3fee4":"## Getting the data ready","a5a8d7c3":"## My Baseline model's rank in the leaderboard\n\n![oie_UlJHYwFTM8cC.png](attachment:oie_UlJHYwFTM8cC.png)","4fcf0112":"### In my case I'm checking predictions mean because to see how each predictions differ from each other","d158fe26":"### Building a evaluation function (RMSE) ","468a0795":"# Step 2: Trying gradient boosting algorithm","1585a3e1":"### Filling the above attributes in our test dataframe","a54536d3":"### Training the model with the best parameters","8c95143c":"### Creating a dataframe for the testdata","45b8e904":"## Hyperparameter tuning using RandomizedSearch CV","91138981":"### Tuning the Catboost Regressor","2b6d310b":"# Steps to rank up in a leaderboard\n\n## Step 1: Make predictions first with a baseline model\n\n## Step 2: Trying with different Algorithms(Gradient Boosting Algorithm in our case)\n\n## Step 3: Try with different hyperparameters ","12068c4a":"### Preparing a dataframe for submission","e5103579":"### Making predictions on the test dataset","0696a3f1":"### Checking for missing values in the numeric columns","f569b3a1":"### **Note:** As we saw before cat.codes has -1 values due to the missing values but our model doesn't accept negative values so cat.codes+1 will make it 0","74a0ef9b":"### XG boost accepts the data as in the form of matrix ","7da3b3e0":"### As the above for loop did not print any output it is clear that all the numerical columns are filled ","c5365f0f":"#### **Note:**\n I'm converting the data into pandas categories and pandas cat codes because I'm using Gradient Boosting Algorithm (CatBoost).It is not recommended in CatBoost Documentation to use OneHotEncoding","8974adb3":"### Checking with the different set of tuning parameters","2d64dc5a":"## My Gradient Boosting Model's rank in the leaderboard\n\n![oie_IpbZIMwGwMit.png](attachment:oie_IpbZIMwGwMit.png)","07691b5c":"### Preprocessing the dataframe in the same format as training data"}}