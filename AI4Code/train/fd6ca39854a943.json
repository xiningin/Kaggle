{"cell_type":{"30896018":"code","56b211c3":"code","a4f76e06":"code","7b9b181e":"code","10d6366c":"code","3cff9ccc":"code","7e56dd5a":"code","50e10235":"code","517c3f51":"code","3a29ac9c":"code","e72c1113":"code","eae44e01":"code","de5fffcd":"code","4b180211":"code","4774072c":"code","9c31089e":"code","fa1ffce4":"code","f4c847d2":"code","6de79b06":"code","e207a68a":"code","5bd73808":"code","61f325f0":"code","83557b41":"code","74b1386a":"code","3ecaa1df":"code","4262c63e":"code","e1dd8d67":"markdown","249294e5":"markdown","9ea33175":"markdown","fd3505f3":"markdown","b7e8ce94":"markdown","d553a9f7":"markdown","9e4aa08e":"markdown","be891c5d":"markdown","a0ddf047":"markdown","781f03f1":"markdown","43430783":"markdown","da6c6b57":"markdown","ba375a0a":"markdown","64faa691":"markdown","abffaae4":"markdown","50c6f414":"markdown","a9d80285":"markdown"},"source":{"30896018":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pylab as plt\nimport seaborn as sns\nimport random\nimport os, sys\n\nimport warnings\n\nif not sys.warnoptions:\n    warnings.simplefilter(\"ignore\")\n\n!ls ..\/input\/","56b211c3":"# Read in datasets\ntrain = pd.read_csv('..\/input\/train.csv')\ntest = pd.read_csv('..\/input\/test.csv')\nss = pd.read_csv('..\/input\/sample_submission.csv')","a4f76e06":"print('Train shape {}'.format(train.shape))\nprint('Test shape {}'.format(test.shape))","7b9b181e":"train.groupby('target').count()['id'].plot(kind='barh', title='Target Distribution', figsize=(15, 5))\nplt.show()","10d6366c":"train['target'].mean() * 100","3cff9ccc":"random.seed(5)\nfor x in range(0, 5):\n    random_feature = random.randint(1,299)\n    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15 ,3))\n    train[str(random_feature)].plot(kind='hist', ax=ax1, bins=20, title='Feature {}: Train set'.format(random_feature))\n    test[str(random_feature)].plot(kind='hist', ax=ax2, bins=20, title='Feature {}: Test set'.format(random_feature))\n    plt.show()","7e56dd5a":"sns.pairplot(train, vars=['target', '0','1','2','3','4'], hue='target')\nplt.show()","50e10235":"from sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.linear_model import LogisticRegression\n\ntrain_X = train.drop(['id','target'], axis=1).as_matrix()\ntrain_y = train['target'].values\ntest_X = test.drop(['id'], axis=1).as_matrix()","517c3f51":"clf = KNeighborsClassifier(n_neighbors=5)\nclf.fit(train_X, train_y)\ntest['target'] = clf.predict(test_X)\ntest[['id','target']].to_csv('submission_KNeighborsClassifier.csv', index=False)\ntest = test.drop('target', axis=1) # drop the target column","3a29ac9c":"clf = DecisionTreeClassifier()\nclf.fit(train_X, train_y)\n# Delete the old prediction\ntest['target'] = clf.predict(test_X)\ntest[['id','target']].to_csv('submission_DecisionTreeClassifier.csv', index=False)\ntest = test.drop('target', axis=1) # drop the target column","e72c1113":"clf = RandomForestClassifier()\nclf.fit(train_X, train_y)\ntest['target'] = clf.predict(test_X)\ntest[['id','target']].to_csv('submission_RandomForestClassifier.csv', index=False)\ntest = test.drop('target', axis=1) # drop the target column","eae44e01":"clf = AdaBoostClassifier()\nclf.fit(train_X, train_y)\ntest['target'] = clf.predict(test_X)\ntest[['id','target']].to_csv('submission_AdaBoostClassifier.csv', index=False)\ntest = test.drop('target', axis=1) # drop the target column","de5fffcd":"clf = GaussianNB()\nclf.fit(train_X, train_y)\ntest['target'] = clf.predict(test_X)\ntest[['id','target']].to_csv('submission_GaussianNB.csv', index=False)\ntest = test.drop('target', axis=1) # drop the target column","4b180211":"import xgboost as xgb\nclf = xgb.XGBClassifier()\nclf.fit(train_X, train_y)\ntest['target'] = clf.predict(test_X)\ntest[['id','target']].to_csv('submission_XGBClassifier.csv', index=False)\ntest = test.drop('target', axis=1) # drop the target column","4774072c":"clf = LogisticRegression(class_weight='balanced', penalty='l1', C=1.0, solver='liblinear')\nclf.fit(train_X, train_y)\ntest['target'] = clf.predict(test_X)\ntest[['id','target']].to_csv('submission_LogisticRegression.csv', index=False)\ntest = test.drop('target', axis=1) # drop the target column","9c31089e":"from sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import ShuffleSplit\n\nfor model_type in [KNeighborsClassifier, DecisionTreeClassifier, RandomForestClassifier,\n                   AdaBoostClassifier, xgb.XGBClassifier, LogisticRegression]:\n    clf = model_type()\n    kfold = KFold(n_splits=5, shuffle=True)\n    cv = ShuffleSplit(n_splits=100, test_size=0.3, random_state=0)\n    scores = cross_val_score(clf, train_X, train_y, cv=cv, scoring='roc_auc')\n    print(\"Print {} Accuracy: {} (+\/- {})\".format(model_type.__name__, scores.mean(), scores.std() * 2))","fa1ffce4":"from sklearn.model_selection import RandomizedSearchCV\nfrom scipy.stats import uniform\n\nclf = LogisticRegression(class_weight='balanced', solver='liblinear')\n\n# Search through these optino\npenalty = ['l1', 'l2']\nC = uniform(loc=0, scale=4)\nhyperparameters = dict(C=C, penalty=penalty)\n\nrand_cv = RandomizedSearchCV(clf, hyperparameters, random_state=1, n_iter=100, cv=5, verbose=0, n_jobs=-1, scoring='roc_auc')\nbest_model = rand_cv.fit(train_X, train_y)\n\nprint('Best Penalty:', best_model.best_estimator_.get_params()['penalty'])\nprint('Best C:', best_model.best_estimator_.get_params()['C'])","f4c847d2":"cv_results = pd.DataFrame()\ncv_results['params'] = best_model.cv_results_['params']\ncv_results['mean_test_score'] = best_model.cv_results_['mean_test_score']\ncv_results['std_test_score'] = best_model.cv_results_['std_test_score']\ncv_results['rank_test_score'] = best_model.cv_results_['rank_test_score']\ncv_results = pd.concat([cv_results.drop('params', axis=1), pd.DataFrame(cv_results['params'].tolist())], axis=1)\ncv_results.sort_values('rank_test_score').head()","6de79b06":"cv_results['penalty_color'] = cv_results.apply(lambda x: 1 if x['penalty'] == 'l1' else 0, axis=1)\ncv_results[['mean_test_score','C']].plot.scatter(x='mean_test_score', y='C', c=cv_results['penalty_color'], colormap='viridis')\nplt.show()","e207a68a":"# Search through these optino\nC = [0.001, 0.01, 0.02, 0.05, 0.1, 0.12, 0.13, 0.15, 0.16, 0.17, 0.178, 0.179, 0.175, 0.2, 0.3]\nhyperparameters = dict(C=C)\n\nclf = LogisticRegression(solver='liblinear', class_weight='balanced', penalty='l1')\nrand_cv = RandomizedSearchCV(clf, hyperparameters, random_state=1, n_iter=100, cv=5, verbose=0, n_jobs=-1, scoring='roc_auc')\nbest_model = rand_cv.fit(train_X, train_y)\n\nprint('Best Penalty:', best_model.best_estimator_.get_params()['penalty'])\nprint('Best C:', best_model.best_estimator_.get_params()['C'])\nprint('Best Score: {}'.format(best_model.best_score_))","5bd73808":"test['target'] = best_model.predict(test_X)\ntest[['id','target']].to_csv('submission_LogisticRegression_randomCV.csv', index=False)\ntest = test.drop('target', axis=1) # drop the target column","61f325f0":"# Use predict proba\ntest['target'] = best_model.predict_proba(test_X)[:,1]\ntest[['id','target']].to_csv('submission_LogisticRegression_randomCV_proba.csv', index=False)\ntest = test.drop('target', axis=1) # drop the target column","83557b41":"clf = xgb.XGBClassifier(learning_rate=0.02, n_estimators=600, objective='binary:logistic')\n# A parameter grid for XGBoost\nparams = {\n        'min_child_weight': [1, 5, 10],\n        'gamma': [0.5, 1, 1.5, 2, 5],\n        'subsample': [0.6, 0.8, 1.0],\n        'colsample_bytree': [0.6, 0.8, 1.0],\n        'max_depth': [3, 4, 5]\n        }\nrand_cv = RandomizedSearchCV(clf, params, random_state=1, n_iter=20, cv=5, verbose=0, n_jobs=-1, scoring='roc_auc')\nbest_model = rand_cv.fit(train_X, train_y)","74b1386a":"best_model.best_score_","3ecaa1df":"test['target'] = best_model.predict(test_X)\ntest[['id','target']].to_csv('submission_XGBClassifier_randomCV.csv', index=False)\ntest = test.drop('target', axis=1) # drop the target column\n\n# Use predict proba\ntest['target'] = best_model.predict_proba(test_X)[:,1]\ntest[['id','target']].to_csv('submission_XGBClassifier_randomCV_proba.csv', index=False)\ntest = test.drop('target', axis=1) # drop the target column","4262c63e":"clf = LogisticRegression(class_weight='balanced', penalty='l1', C=0.01, solver='liblinear').fit(train_X, train_y)","e1dd8d67":"## AdaBoostClassifier - Public LB 0.638","249294e5":"## KNeighborsClassifier - Public LB 0.549","9ea33175":"## DecisionTreeClassifier - Public LB 0.558","fd3505f3":"# Logistic Regression","b7e8ce94":"# First Look of the Don't Overfit II Data","d553a9f7":"# See correlation between first 4 features and target","9e4aa08e":"## Focused Gridsearch\n- Use only l1\n- Smaller C values","be891c5d":"# Cross Validation\nNow lets use some cross validation techniques to validate our scores for each model type. Cross validation allows us to train multiple models on the training data by splitting differently each time it trains the model.","a0ddf047":"# Walkthrough of SKlearn Classification Algs (Not worrying about overfitting yet)\n- Lets not worry about overfitting yet and try out some classification algs","781f03f1":"## Naive Bayes - Public LB 0.611","43430783":"## Logistic Regression - Public LB ???","da6c6b57":"## XGBoost - Public LB ???","ba375a0a":"# Parameter Tuning using RandomizedSearchCV\n- Next we want to do some parameter tuning on our best model.","64faa691":"## RandomForestClassifier - Public LB 0.574","abffaae4":"## 64-36 split of target in training set - will that hold true for test set?","50c6f414":"# XGBClassifier with RandomizedSearchCV","a9d80285":"# Plot distribution of features in train vs test\n- Picked 5 random features\n- There are a lot of features (300)"}}