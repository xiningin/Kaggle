{"cell_type":{"94584084":"code","9d566ea4":"code","7000706e":"code","f5c1324b":"code","79e6e444":"code","9d535355":"code","0bfdc757":"code","f6602e21":"code","be621a10":"code","563cf899":"code","336ea71f":"code","787b0334":"code","8253afb8":"code","fca1cf58":"code","04918cd6":"code","f5fb8446":"code","4df27445":"code","d3cc47e2":"code","bda11db7":"code","e8400363":"code","871a4ffb":"code","dba88973":"code","9f0a8f2c":"code","0d97f5e5":"code","80123bae":"code","8b1d703e":"code","fbe357b6":"code","e339482e":"code","e5d26df2":"code","5f35e935":"code","23e50c7c":"code","d17cd351":"code","67b2ee6e":"code","99310197":"code","be855f23":"code","a518d768":"code","daccfd51":"code","6ff67414":"code","44de6440":"code","1246b447":"code","be5e4e78":"code","5936ed07":"code","73431af6":"code","97019915":"code","15df29d7":"code","ebeca36e":"code","1421835a":"code","7c51edaa":"code","2caa8786":"code","8d3403b2":"code","1512f813":"code","f8813088":"code","15399dba":"markdown","a7bfe478":"markdown","52e03139":"markdown","5daad318":"markdown"},"source":{"94584084":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport re\nimport string\nimport heapq\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelBinarizer\nimport nltk\nimport keras\nfrom keras.models import Sequential \nfrom keras.preprocessing.text import one_hot\nfrom keras.layers import Dense,Dropout,LSTM,Embedding\nfrom keras.preprocessing.sequence import pad_sequences","9d566ea4":"data = pd.read_csv('..\/input\/twitter-airline-sentiment\/Tweets.csv')\ndata.head()","7000706e":"len(data)","f5c1324b":"data.isna().sum()","79e6e444":"plt.style.use('seaborn')\nsns.countplot(data=data,x='airline_sentiment')","9d535355":"data.columns","0bfdc757":"plt.style.use('dark_background')\nsns.countplot(data=data,x='airline')","f6602e21":"sns.set(rc={'figure.figsize':(25,15)})\nsns.countplot(data=data,x='negativereason')","be621a10":"data = data[['text','airline_sentiment']]\ndata.head()","563cf899":"data['text'][5]","336ea71f":"def remove_stopwords(inp_text):\n    stop = nltk.corpus.stopwords.words('english')\n    punc = string.punctuation\n    stop.append(punc)\n    whitelist = [\"n't\", \"not\", \"no\"]\n    clean_words = []\n    words = nltk.word_tokenize(inp_text)\n    for word in words:\n        if word not in stop or word not in whitelist and len(word)>1:\n            clean_words.append(word)\n    return \" \".join(clean_words)","787b0334":"remove_stopwords(data['text'][5])","8253afb8":"def remove_mentions(input_text):\n        return re.sub(r'@ \\w+', '', input_text)","fca1cf58":"data.text = data.text.apply(remove_stopwords).apply(remove_mentions)\ndata.head()","04918cd6":"word2count = {}\n\nfor i in range(len(data['text'])):\n    words = nltk.word_tokenize(data['text'][i])\n    \n    for word in words:\n        if word not in word2count.keys():\n            word2count[word] = 1\n        else:\n            word2count[word] += 1","f5fb8446":"word2count","4df27445":"print(\"Vocabluray of our corpus is: {}\".format(len(word2count)))","d3cc47e2":"word_freq = heapq.nlargest(10000,word2count,key=word2count.get)\nword_freq[:15]","bda11db7":"vocab_size = len(word_freq)","e8400363":"onehot_text = []\nfor sentences in data['text']:\n    Z = one_hot(sentences,vocab_size)\n    onehot_text.append(Z)","871a4ffb":"onehot_text[:5]","dba88973":"length = 20\nembedded_sents = pad_sequences(onehot_text,padding='pre',maxlen=length)","9f0a8f2c":"embedded_sents[:5]","0d97f5e5":"labels = data['airline_sentiment']\nlabels","80123bae":"lb = LabelBinarizer()","8b1d703e":"labels = lb.fit_transform(labels)\nlabels","fbe357b6":"X = embedded_sents\ny = labels","e339482e":"X = np.asarray(X)\ny = np.asarray(y)","e5d26df2":"len(X)","5f35e935":"len(y)","23e50c7c":"X_train = X[:13000]\ny_train = y[:13000]","d17cd351":"X_test = X[13000:]\ny_test = y[13000:]","67b2ee6e":"X_train.shape","99310197":"y_train.shape","be855f23":"X_test.shape","a518d768":"y_test.shape","daccfd51":"X_training,X_valid,y_training,y_valid = train_test_split(X_train,y_train,test_size=0.2)","6ff67414":"X_training.shape","44de6440":"X_valid.shape","1246b447":"model = Sequential()","be5e4e78":"model.add(Dense(512,activation='relu',input_shape=(20,)))\nmodel.add(Dropout(0.2))\nmodel.add(Dense(256,activation='relu'))\nmodel.add(Dropout(0.2))\nmodel.add(Dense(128,activation='relu'))\nmodel.add(Dropout(0.2))\nmodel.add(Dense(3,activation='softmax'))","5936ed07":"model.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy'])","73431af6":"history = model.fit(X_training,y_training,validation_data=(X_valid,y_valid),epochs=100,batch_size=64)","97019915":"plt.style.use('dark_background')\nfig, ax = plt.subplots(2, 2, figsize=(10, 10))\nsns.lineplot(x=np.arange(1, 101), y=history.history.get('loss'), ax=ax[0, 0])\nsns.lineplot(x=np.arange(1, 101), y=history.history.get('accuracy'), ax=ax[0, 1])\nsns.lineplot(x=np.arange(1, 101), y=history.history.get('val_loss'), ax=ax[1, 0])\nsns.lineplot(x=np.arange(1, 101), y=history.history.get('val_accuracy'), ax=ax[1, 1])\nax[0, 0].set_title('Training Loss vs Epochs')\nax[0, 1].set_title('Training Accuracy vs Epochs')\nax[1, 0].set_title('Validation Loss vs Epochs')\nax[1, 1].set_title('Validation Accuracy vs Epochs')\nplt.suptitle('Traditional Deep learning model',size=16)\nplt.show()","15df29d7":"embedding_feature_vectors = 40\nmodel1 = Sequential()\nmodel1.add(Embedding(vocab_size,embedding_feature_vectors,input_length=length))\nmodel1.add(Dropout(0.2))\nmodel1.add(LSTM(200,dropout=0.2,recurrent_dropout=0.3))\nmodel.add(Dropout(0.2))\nmodel.add(Dense(100, activation='relu'))\nmodel.add(Dropout(0.2))\nmodel1.add(Dense(3,activation='softmax'))","ebeca36e":"model1.compile(loss='binary_crossentropy',optimizer=keras.optimizers.Adadelta(),metrics=['accuracy'])","1421835a":"history1 = model1.fit(X_training,y_training,validation_data=(X_valid,y_valid),epochs=10,batch_size=32,verbose=2)","7c51edaa":"plt.style.use('dark_background')\nfig, ax = plt.subplots(2, 2, figsize=(10, 10))\nsns.lineplot(x=np.arange(1, 11), y=history1.history.get('loss'), ax=ax[0, 0])\nsns.lineplot(x=np.arange(1, 11), y=history1.history.get('accuracy'), ax=ax[0, 1])\nsns.lineplot(x=np.arange(1, 11), y=history1.history.get('val_loss'), ax=ax[1, 0])\nsns.lineplot(x=np.arange(1, 11), y=history1.history.get('val_accuracy'), ax=ax[1, 1])\nax[0, 0].set_title('Training Loss vs Epochs')\nax[0, 1].set_title('Training Accuracy vs Epochs')\nax[1, 0].set_title('Validation Loss vs Epochs')\nax[1, 1].set_title('Validation Accuracy vs Epochs')\nplt.suptitle('LSTM RNN model',size=16)\nplt.show()","2caa8786":"score_dl = model.evaluate(X_test,y_test)\nscore_dl","8d3403b2":"print(\"Accuracy of Traditional DL model: {}\".format(score_dl[1]*100))","1512f813":"score_lstm = model1.evaluate(X_test,y_test)\nscore_lstm","f8813088":"print(\"Accuracy of LSTM RNN model: {}\".format(score_lstm[1]*100))","15399dba":"LSTM RNN Model","a7bfe478":"Test accurcy using Traditional DL Model","52e03139":"Traditional Deep Learning Model","5daad318":"Test Accuracy of LSTM RNN Model"}}