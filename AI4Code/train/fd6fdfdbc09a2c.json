{"cell_type":{"8fa5751c":"code","e0ee83ef":"code","0a1b4865":"code","79fde360":"code","9f2e9f16":"code","0727444f":"code","6e1ab590":"code","2f9f6e78":"code","84ae8622":"code","85da02aa":"code","80c4b6d8":"code","9679fde9":"code","5922f1cf":"code","de07594e":"code","389afc72":"markdown","137b0268":"markdown","b6491089":"markdown","e7df2a1b":"markdown","bba12dac":"markdown","7957cc5c":"markdown","88008db4":"markdown","ac0748f5":"markdown","b81f08f5":"markdown"},"source":{"8fa5751c":"\nimport optuna\nfrom optuna.samplers import TPESampler\n\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.metrics import mean_squared_error\nimport matplotlib.pyplot as plt\nimport warnings\n\n\n# imports \nimport numpy as np\nimport pandas as pd \nimport random,os\n\nfrom sklearn.preprocessing import LabelEncoder\nfrom catboost import CatBoostRegressor\nfrom sklearn.model_selection import train_test_split\n\n\nwarnings.simplefilter(action=\"ignore\", category=FutureWarning)\nwarnings.simplefilter(action=\"ignore\", category=pd.core.common.SettingWithCopyWarning)\nimport lightgbm\n\n#variables \nTRAIN_PATH = \"..\/input\/widsdatathon2022\/train.csv\"\nTEST_PATH = \"..\/input\/widsdatathon2022\/test.csv\"\nSAMPLE_SUBMISSION_PATH = \"..\/input\/widsdatathon2022\/sample_solution.csv\"\nSUBMISSION_PATH = \"submission.csv\"\n\nID = \"id\"\nTARGET = \"site_eui\"\n\nTEST_SIZE = 0.2\n\nSEED = 2022\ndef seed_everything(seed=SEED):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n\nseed_everything()\n\n\n \n#load\ntrain = pd.read_csv(TRAIN_PATH)\ntest = pd.read_csv(TEST_PATH)\n\n#preprocess\nstr_list = [] \nnum_list = []\nfor colname, colvalue in train.iteritems():\n    if type(colvalue[1]) == str:\n        str_list.append(colname)\n    else:\n        num_list.append(colname)\n        \nfor col in str_list:\n    encoder = LabelEncoder()\n    encoder.fit(train[col])\n    train[col] = encoder.transform(train[col])\n\n    for label in np.unique(test[col]):\n        if label not in encoder.classes_: \n            encoder.classes_ = np.append(encoder.classes_, label) \n    test[col] = encoder.transform(test[col])","e0ee83ef":"# \u8a55\u4fa1 Evaluation method \ndef calculate_scores(true, pred):\n    scores = np.sqrt(mean_squared_error(true, pred))\n    return scores\n","0a1b4865":"X = train.drop([ID,TARGET],axis=1)\ny = train[TARGET]\nmodel = lightgbm.LGBMRegressor()\nmodel.fit(X, y)\n\n#submission\n#X_test = test.drop([ID],axis=1)\n#pred_test = model.predict(X_test)\n#sub = pd.DataFrame(test['id'],columns={'id'})\n#sub[TARGET] = pred_test\n#sub.to_csv('submission.csv', index=False)\n#sub.head()","79fde360":"lightgbm.plot_importance(model, figsize = (20, 60))","9f2e9f16":"X = train.drop([ID,TARGET],axis =1)\ny = train[TARGET]\n\n\nMODEL_MAX_DEPTH = 12\nMODEL_TASK_TYPE = 'GPU'#'GPU'\nMODEL_RL = 0.025\nMODEL_EVAL_METRIC ='RMSE'\nMODEL_LOSS_FUNCTION = 'RMSE'\nMODEL_ESR = 10\nMODEL_VERBOSE = 1000\nMODEL_ITERATIONS = 28000\n\nmodel = CatBoostRegressor(\n    verbose=MODEL_VERBOSE,\n    early_stopping_rounds=MODEL_ESR,\n    random_seed=SEED,\n    max_depth=MODEL_MAX_DEPTH,\n    task_type=MODEL_TASK_TYPE,\n    learning_rate=MODEL_RL,\n    iterations=MODEL_ITERATIONS,\n    loss_function=MODEL_LOSS_FUNCTION,\n    eval_metric= MODEL_EVAL_METRIC\n)\n\n\n#model.fit(X, y)\n\n#submission\n#X_test = test.drop([ID],axis=1)\n#pred_test = model.predict(X_test)\n#sub = pd.DataFrame(test['id'],columns={'id'})\n#sub[TARGET] = pred_test\n#sub.to_csv('submission.csv', index=False)\n#sub.head()","0727444f":"test.isnull().sum()","6e1ab590":"train.drop(['direction_max_wind_speed','direction_peak_wind_speed','max_wind_speed','days_with_fog'],axis=1,inplace=True)\ntest.drop(['direction_max_wind_speed','direction_peak_wind_speed','max_wind_speed','days_with_fog'],axis=1,inplace=True)","2f9f6e78":"#X = train.drop([ID,TARGET],axis =1)\n#y = train[TARGET]\n\n\n#model.fit(X, y)\n\n#submission\n#X_test = test.drop([ID],axis=1)\n#pred_test = model.predict(X_test)\n#sub = pd.DataFrame(test['id'],columns={'id'})\n#sub[TARGET] = pred_test\n#sub.to_csv('submission.csv', index=False)\n#sub.head()","84ae8622":"temp_list=[i for i in train.columns if 'temp' in i if i!='avg_temp']\ntemp_list_2=[i for i in temp_list if ('january' not in i)&('july' not in i)]","85da02aa":"train.drop(temp_list_2,axis=1,inplace=True)\ntest.drop(temp_list_2,axis=1,inplace=True)\n","80c4b6d8":"X = train.drop([ID,TARGET],axis =1)\ny = train[TARGET]\n\n\nMODEL_MAX_DEPTH = 12\nMODEL_TASK_TYPE = 'GPU'#'GPU'\nMODEL_RL = 0.025\nMODEL_EVAL_METRIC ='RMSE'\nMODEL_LOSS_FUNCTION = 'RMSE'\nMODEL_ESR = 10\nMODEL_VERBOSE = 1000\nMODEL_ITERATIONS = 28000\n\nmodel = CatBoostRegressor(\n    verbose=MODEL_VERBOSE,\n    early_stopping_rounds=MODEL_ESR,\n    random_seed=SEED,\n    max_depth=MODEL_MAX_DEPTH,\n    task_type=MODEL_TASK_TYPE,\n    learning_rate=MODEL_RL,\n    iterations=MODEL_ITERATIONS,\n    loss_function=MODEL_LOSS_FUNCTION,\n    eval_metric= MODEL_EVAL_METRIC\n)\n\n\nmodel.fit(X, y)","9679fde9":"X_test = test.drop([ID],axis=1)\npred_test = model.predict(X_test)\n\nsub = pd.read_csv(SAMPLE_SUBMISSION_PATH)\nsub[TARGET] = pred_test\nsub.to_csv(SUBMISSION_PATH,index=False)\nsub.head()","5922f1cf":"#x_train, x_valid, y_train, y_valid = train_test_split(X, y)\n\n#from sklearn.model_selection import KFold\n\n#models = []\n\n#K_fold = KFold(n_splits=5, shuffle=True,  random_state=42)\n\n\n\n#for train_cv_no, eval_cv_no in K_fold.split(X, y):\n    # iloc\u3067\u53d6\u308a\u51fa\u3059\u884c\u3092\u6307\u5b9a\n#    X_train_cv = X.iloc[train_cv_no]\n#    y_train_cv = y.iloc[train_cv_no]\n#    X_eval_cv = X.iloc[eval_cv_no]\n#    y_eval_cv = y.iloc[eval_cv_no]\n    \n\n    # \u5b66\u7fd2\n#    evaluation_results = {}                                     # \u5b66\u7fd2\u306e\u7d4c\u904e\u3092\u4fdd\u5b58\u3059\u308b\u7bb1\n\n#    model.fit(X_train_cv, y_train_cv)\n#    pred = model.predict(X_eval_cv)\n\n    # Accuracy \u3092\u8a08\u7b97\u3059\u308b\n#    score = calculate_scores(y_eval_cv, pred)\n    #print('score:', score)\n    \n    # \u5b66\u7fd2\u304c\u7d42\u308f\u3063\u305f\u30e2\u30c7\u30eb\u3092\u30ea\u30b9\u30c8\u306b\u5165\u308c\u3066\u304a\u304f\n    #models.append(model) \n    \n    \n","de07594e":"#X_test = test.drop([ID],axis=1)\n#test_pred=pd.DataFrame()\n#test_pred['id']=test['id'].copy()\n#for fold_, model in enumerate(models):\n    # test\u3092\u4e88\u6e2c\n    #pred_ = model.predict(X_test) \n    # test\u306e\u4e88\u6e2c\u3092\u4fdd\u5b58\n    #test_pred['fold_'+str(fold_)] = pred_\n\n#sub = pd.read_csv(SAMPLE_SUBMISSION_PATH)\n#test_pred[TARGET] = (test_pred['fold_0']+test_pred['fold_1']+test_pred['fold_2']+test_pred['fold_3']+test_pred['fold_4'])\/5\n#sub=test_pred[['id',TARGET]]\n#sub.to_csv(SUBMISSION_PATH,index=False)\n#sub.head()","389afc72":"## Drop features \/ Score = 34.336","137b0268":"I refferd to the following notebook to change the model to catboost.\nIt got 34.999 score.\n\n\u30e2\u30c7\u30eb\u3092catboost\u306b\u5909\u66f4\u3059\u308b\u305f\u3081\u306b\u3001\u6b21\u306e\u30ce\u30fc\u30c8\u30d6\u30c3\u30af\u3092\u53c2\u7167\u3057\u307e\u3057\u305f\u3002\n34.999\u3067\u3057\u305f\u3002\n\n\nreflection:[WiDS 2022 CATBOOST RMSE](https:\/\/www.kaggle.com\/rhythmcam\/wids-2022-catboost-rmse)","b6491089":"### Build Catboost model\/Score=34.999","e7df2a1b":"When I saw the test datasets, last 4 columns are almost missing.\n\nI dropped 4 columns, and score improved to 34.395.\n\n\u30c6\u30b9\u30c8\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u3092\u898b\u305f\u3068\u304d\u3001\u6700\u5f8c\u306e4\u5217\u304c\u307b\u3068\u3093\u3069\u6b20\u843d\u3057\u3066\u3044\u307e\u3059\u3002\n\n4\u5217\u3092\u843d\u3068\u3057\u3001\u30b9\u30b3\u30a2\u304c34.395\u306b\u5411\u4e0a\u3057\u307e\u3057\u305f\u3002","bba12dac":"## Drop more features \/ Score = 31.826\n","7957cc5c":"reference\n\n* [WiDS 2022 CATBOOST RMSE](https:\/\/www.kaggle.com\/rhythmcam\/wids-2022-catboost-rmse)\n\nSimple EDA for bigginer's:\n* [WiDS 2022 Simple EDA(JP\/EN)About Features](https:\/\/www.kaggle.com\/ycca1018\/wids-2022-simple-eda-jp-en-about-features)","88008db4":"If we reduce the features more, the score may increase.\nI decided to erase the temperature data other than January and July.\nThen the score became 31.71.\n\n\u6a5f\u80fd\u3092\u3055\u3089\u306b\u6e1b\u3089\u3059\u3068\u3001\u30b9\u30b3\u30a2\u304c\u4e0a\u304c\u308b\u53ef\u80fd\u6027\u304c\u3042\u308a\u307e\u3059\u3002\n1\u6708\u30687\u6708\u4ee5\u5916\u306e\u6c17\u6e29\u30c7\u30fc\u30bf\u3092\u6d88\u53bb\u3059\u308b\u3053\u3068\u306b\u3057\u307e\u3057\u305f\u3002\n\u305d\u306e\u5f8c\u3001\u30b9\u30b3\u30a2\u306f31.71\u306b\u306a\u308a\u307e\u3057\u305f\u3002\n\n","ac0748f5":"### imports & variables & load & preprocess","b81f08f5":"At first,\u3000I made the simple LGBM model.\nScore got 41.126 and check feature importance.\n\n\u6700\u521d\u306f\u3001\u30b7\u30f3\u30d7\u30eb\u306aLGBM\u30e2\u30c7\u30eb\u3092\u4f5c\u308a\u307e\u3057\u305f\u3002\n\u30b9\u30b3\u30a2\u306f41.126\u306b\u306a\u308a\u3001\u7279\u5fb4\u306e\u91cd\u8981\u6027\u3092\u78ba\u8a8d\u3057\u307e\u3059\u3002"}}