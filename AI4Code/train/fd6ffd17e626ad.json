{"cell_type":{"a399d410":"code","08cf195e":"code","1d2261a6":"code","dc6f9e49":"code","9f1bbbcc":"code","15de5a70":"code","144c5c31":"code","a3d7c764":"code","bb84be9a":"code","0212ca17":"code","e29d84a5":"code","a3d9ffab":"code","46ceb605":"code","473371b6":"code","909fefb5":"code","e01425a7":"code","bc469509":"code","ee415e7a":"code","2f4d6594":"markdown","68f74e90":"markdown","82e12978":"markdown","d8c8530c":"markdown","d7b745f4":"markdown","95b43afe":"markdown","86b51876":"markdown","4057b719":"markdown","0931e5d1":"markdown","ee65c916":"markdown","3d9d9112":"markdown","72d92433":"markdown","d4df0b0a":"markdown","092c792e":"markdown","3a7ae381":"markdown","8aa7ea77":"markdown","81337bfd":"markdown","0cc18780":"markdown"},"source":{"a399d410":"from tensorflow.keras.datasets import imdb\n# Loading the dataset.\n(train_data, train_labels),(test_data, test_labels) = imdb.load_data(num_words=10000)","08cf195e":"# Let's take a look at shapes of datasets\nprint(f\"Shapes of train dataset: {train_data.shape}\")\nprint(f\"Shapes of train labels: {train_labels.shape}\")\nprint(f\"Shapes of test dataset: {test_data.shape}\")\nprint(f\"Shapes of test labels: {test_labels.shape}\")","1d2261a6":"# Let's see the data of the row with index 0.\nprint(f\"First row of train dataset: {train_data[0]}\")","dc6f9e49":"# Let's see the label of the row with index 0.\nprint(f\"First label: {train_labels[0]}\")","9f1bbbcc":"# Creating a function to turn data into multi-hot encode.\nimport numpy as np\ndef vectorize_sequences(sequences, dimension=10000):\n    # Creating an all-zero matrix of shape\n    results = np.zeros((len(sequences), dimension))\n    for i, sequence in enumerate(sequences):\n        for j in sequence:\n            # Seting specific indices of results[i] to 1s\n            results[i, j] = 1. \n    return results","15de5a70":"# Vectorized traning data and test data:\nx_train = vectorize_sequences(train_data)\nx_test = vectorize_sequences(test_data) ","144c5c31":"print(f\"First row : {x_train[0]}\")","a3d7c764":"# I am going to vectorize the labels. \ny_train = np.asarray(train_labels).astype(\"float32\")\ny_test = np.asarray(test_labels).astype(\"float32\")","bb84be9a":"print(f\"The labels of train dataset: {y_train}\") \nprint(f\"The labels of test dataset: {y_test}\")","0212ca17":"from tensorflow import keras \nfrom tensorflow.keras import layers\n\nmodel = keras.Sequential([\n    layers.Dense(16, activation = \"relu\"),\n    layers.Dense(16, activation = \"relu\"),\n    layers.Dense(1,activation = \"relu\")\n])","e29d84a5":"model.compile(optimizer=\"rmsprop\",\n              loss=\"binary_crossentropy\",\n              metrics=[\"accuracy\"])","a3d9ffab":"# Creating validation set:\nx_val = x_train[:10000]\npartial_x_train = x_train[10000:]\ny_val = y_train[:10000]\npartial_y_train = y_train[10000:]","46ceb605":"history = model.fit(partial_x_train, \n                   partial_y_train,\n                   epochs = 20,\n                   batch_size = 512,\n                   validation_data = (x_val, y_val))","473371b6":"import matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set_style('darkgrid')\nplt.figure(figsize = (16, 10))\nhistory_dict = history.history\nloss_values = history_dict[\"loss\"]\nval_loss_values = history_dict[\"val_loss\"]\nepochs = range(1, len(loss_values) + 1)\nplt.plot(epochs, loss_values, \"bo\", label=\"Training loss\")  \nplt.plot(epochs, val_loss_values, \"b\", label=\"Validation loss\") \nplt.title(\"Training and validation loss\")\nplt.xlabel(\"Epochs\")\nplt.ylabel(\"Loss\")\nplt.legend()\nplt.show()","909fefb5":"plt.clf()     \nplt.figure(figsize = (16, 10))\nacc = history_dict[\"accuracy\"]\nval_acc = history_dict[\"val_accuracy\"]\nplt.plot(epochs, acc, \"bo\", label=\"Training acc\")\nplt.plot(epochs, val_acc, \"b\", label=\"Validation acc\")\nplt.title(\"Training and validation accuracy\")\nplt.xlabel(\"Epochs\")\nplt.ylabel(\"Accuracy\")\nplt.legend()\nplt.show()","e01425a7":"# Building a new model:\nmodel = keras.Sequential([\n    layers.Dense(16, activation=\"relu\"),\n    layers.Dense(16, activation=\"relu\"),\n    layers.Dense(1, activation=\"sigmoid\")\n])\nmodel.compile(optimizer=\"rmsprop\",\n              loss=\"binary_crossentropy\",\n              metrics=[\"accuracy\"])\nmodel.fit(x_train, y_train, epochs=4, batch_size=512)","bc469509":"# Evaluating the model:\nresults = model.evaluate(x_test, y_test)\nprint(f\"Test loss : {results[0]}\")\nprint(f\"Test accuracy : {results[1]}\")","ee415e7a":"model.predict(x_test)","2f4d6594":"## <span style=\"color:Orange\">Predicting New Data<\/span>\n\n### Let's predict the test data using our model.","68f74e90":"## <span style = \"color:Orange\"> Resource<\/span>","82e12978":"## <span style = \"color: Orange\"> Building the Model <\/span>\n\n### To build the model, I will use two intermediate layers with 16 units and an output layer for scaler prediction. Let me set relu activation for intermediate layers and the a sigmoid activation for output layer. \n\n### Note that a relu (rectified linear unit) is a function meant to zero out negative values, whereas a sigmoid \u201csquashes\u201d arbitrary values into the [0, 1] interval, outputting something that can be interpreted as a probability.","d8c8530c":"### Note that loss and accuracy seem to peak at the fourth epoch. A overfitting problem arises when a model that performs well on training data does not perform well on data it has never seen before\n\n### To prevent overfitting you could stop training after epochs. Let's traing a new model for four epochs.","d7b745f4":"## <span style = \"color: Orange\"> Creating Validation Dataset <\/span> \n\n### We use the training data to train the model, but we use the validation set to evaluate the model. Now let's create validation set.","95b43afe":"# <span style=\"color:OrangeRed\">Classifying Movie Reviews with Keras<\/span> ","86b51876":"### Here, 0 stands for negative and 1 stands for positive.","4057b719":"### Finally, we need to choose a loss function and an optimizer. Because we have a binary classfication problem, it's best to use the binary_crossentropy loss. I will use rmsprop optimizer which is a usually a good default choice for any problem. I will also set accuracy to monito during training.","0931e5d1":"## <span style=\"color:Orange\">Plotting the training and validation accuracy<\/span> \n","ee65c916":"## <span style = \"color: Orange\"> Traning the model <\/span>","3d9d9112":"![](https:\/\/images.unsplash.com\/photo-1536440136628-849c177e76a1?ixid=MnwxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8&ixlib=rb-1.2.1&auto=format&fit=crop&w=1025&q=80)","72d92433":"### Two-class classification is one of the most common kinds of machine learning problems. In this example, I will show you how to classify movie reviews as positive or negative.\n\n## <span style = \"color:Orange\">IMDB Dataset <\/span>\n\n### I am going to use IMDB dataset in this example. This dataset contains a set of 50,000 highly polarized reviecs from the Internet Movie Database. The dataset are split into 25,000 reviews for training and 25,000 reviews for testing. \n\n## <span style = \"color : Orange\">Loading the Dataset<\/span>\n\n### First, let me load the dataset using Keras. I only want to use the top 10,000 most frequently occuring words in the training data.","d4df0b0a":"### [Chollet, F. 2021, Deep Learning with Python](https:\/\/www.manning.com\/books\/deep-learning-with-python-second-edition)","092c792e":"### To feed lists of integers into a neural network, you have to turn your lists into tensors.","3a7ae381":"## <span style = \"color : Orange\"> Plotting the training and validation loss <\/span>","8aa7ea77":"## <span style=\"color:Orange\"> Preparing the Data <\/span>","81337bfd":"### Here, numbers stand for words","0cc18780":"### Don\u2019t forget to follow us on [YouTube](https:\/\/youtube.com\/c\/tirendazakademi) \ud83c\udf9e, [GitHub](https:\/\/github.com\/tirendazacademy) \ud83c\udf31, [Twitter](https:\/\/twitter.com\/@tirendazacademy) \ud83d\ude0e, [LinkedIn](https:\/\/www.linkedin.com\/in\/tirendaz-academy) \ud83d\udc4d"}}