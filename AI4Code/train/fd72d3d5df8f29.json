{"cell_type":{"21a1f74e":"code","b579ac88":"code","41d71885":"code","1ead9900":"code","98884ea2":"code","3bb9fab9":"code","b6921a1f":"code","8e700bcb":"code","40026107":"code","498e6487":"code","d834d748":"code","5651ab24":"code","0e1b1caa":"code","ffdddce8":"code","dc80efd2":"code","03af9688":"code","f72a6833":"code","cf39e5d1":"code","0f504dbf":"code","c2c319c6":"code","6aeccc18":"code","6b4f1f4e":"code","f0a1567d":"code","a2d48fa9":"code","c6afe386":"code","e9a17156":"code","2d537d4d":"markdown","06103014":"markdown","8f425534":"markdown","f6b4dcaf":"markdown","8a200f30":"markdown","18e17783":"markdown","428e8677":"markdown","a511598d":"markdown","8a6f544d":"markdown","9ec11fda":"markdown","5e53ac34":"markdown","793b1af4":"markdown","568d8d50":"markdown","a45139ec":"markdown","807c8c60":"markdown","8f904c24":"markdown","38562817":"markdown","7f4b250b":"markdown","fe4313fb":"markdown","c2a5a128":"markdown"},"source":{"21a1f74e":"import pandas as pd\nfrom sklearn.model_selection import train_test_split  \nfrom sklearn import metrics\nfrom sklearn.preprocessing import LabelEncoder\nfrom IPython.display import Audio, display\nfrom sklearn.ensemble import RandomForestRegressor\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nimport re\nfrom datetime import datetime\nimport time\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","b579ac88":"# notification - just to make the life easier and more relax\nsound = 'https:\/\/freesound.org\/data\/previews\/29\/29589_215874-lq.mp3'\n\ndef allDone():\n    display(Audio(url=sound, autoplay=True))","41d71885":"\nfile_path = '\/kaggle\/input\/sf-dst-restaurant-rating\/'\n\ntrain_set = pd.read_csv(file_path + 'main_task.csv')\n\n# add additional dataset & aling datafields with the initial one\nadditional_train_set = pd.read_csv('..\/input\/tripadvisor-competition-additional-dataset\/additional_dataset.csv')\nadditional_train_set['Restaurant_id'] = additional_train_set['Unnamed: 0']\nadditional_train_set.drop(['Unnamed: 0', 'Name'], axis=1, inplace=True)\n\ntrain_set = pd.concat([train_set, additional_train_set])\n\ntest_set = pd.read_csv(file_path + 'kaggle_task.csv')\n\ntrain_set['sample'] = 1  # mark the train set\ntest_set['sample'] = 0  # mark the test set\ntest_set['Rating'] = 0  # define the absent field for the test set as well\n\ndf = train_set.append(test_set, sort=False).reset_index(drop=True)\ndf.dropna(subset=['Rating', 'Reviews', 'Ranking'], axis=0, inplace=True)\n\nSolution = pd.DataFrame()  #DataFrame for the Kaggle Data Collection\nSolution['Restaurant_id'] = df.query('sample == 0')[\n    'Restaurant_id']  # keep IDs as the Kaggle dataset solution requirements","1ead9900":"\ndef features_proccessing(df):\n\n    df = df.rename(\n        columns={\n            'Number of Reviews': 'Number_of_Reviews',\n            'Price Range': 'Price_Range',\n            'Cuisine Style': 'Cuisine_Style'\n        })\n    # add \"some IDs\"\n    df['some ID'] = df['ID_TA'].apply(lambda x: float(x[1:]))\n    df['some ID 2'] = df['URL_TA'].apply(lambda x: float(x[20:26]))\n    df.drop(['URL_TA', 'ID_TA'], axis=1, inplace=True)\n\n    # Restaturant\u0443 Chain Network\n    restaurant_chain = set()\n    for chain in df['Restaurant_id']:\n        #print(chain)\n        restaurant_chain.update(str(chain))\n\n    def find_item(cell):\n        #print(cell)\n        if item in str(cell):\n            return 1\n        return 0\n\n    for item in restaurant_chain:\n        df['Network'] = df['Restaurant_id'].apply(find_item)\n\n\n# a new feature \"Country Code\"\n    city_country = {\n        'London': 'UK',\n        'Paris': 'France',\n        'Madrid': 'Spain',\n        'Barcelona': 'Spain',\n        'Berlin': 'Germany',\n        'Milan': 'Italy',\n        'Rome': 'Italy',\n        'Prague': 'Czech',\n        'Lisbon': 'Portugalia',\n        'Vienna': 'Austria',\n        'Amsterdam': 'Nederlands',\n        'Brussels': '144784 ',\n        'Hamburg': 'Germany',\n        'Munich': 'Germany',\n        'Lyon': 'France',\n        'Stockholm': 'Sweden',\n        'Budapest': 'Hungary',\n        'Warsaw': 'Poland',\n        'Dublin': 'Ireland',\n        'Copenhagen': 'Denmark',\n        'Athens': 'Greece',\n        'Edinburgh': 'Schotland',\n        'Zurich': 'Switzerland',\n        'Oporto': 'Portugalia',\n        'Geneva': 'Switzerland',\n        'Krakow': 'Poland',\n        'Oslo': 'Norway',\n        'Helsinki': 'Finland',\n        'Bratislava': 'Slovakia',\n        'Luxembourg': 'Luxembourg',\n        'Ljubljana': 'Slovenija'\n    }\n\n    df['County Code'] = df['City'].map(city_country)\n\n    le = LabelEncoder()\n\n    le = LabelEncoder()\n    le.fit(df['County Code'])\n    df['County Code'] = le.transform(df['County Code'])\n\n    # feature 'City' as a numerical feature\n    le.fit(df['City'])\n    df['City Code'] = le.transform(df['City'])\n\n    # a dummy variable \"Capital\"\n    capitals = [\n        'London', 'Paris', 'Madrid', 'Berlin', 'Rome', 'Prague', 'Lisbon',\n        'Vienna', 'Amsterdam', 'Brussels', 'Stockholm', 'Budapest', 'Warsaw',\n        'Dublin', 'Copenhagen', 'Athens', 'Oslo', 'Helsinki', 'Bratislava',\n        'Luxembourg', 'Ljubljana', 'Edinburgh'\n    ]\n    df['Capital'] = df['City'].apply(lambda x: 1 if x in capitals else 0)\n\n    # new feature \"quantity of restaurants per City\"\n    df['Rests per City'] = df['City'].map(\n        df.groupby(['City'])['Restaurant_id'].count().to_dict())\n    df.drop(['Restaurant_id'], axis=1, inplace=True)\n\n    #new feature \"Rests per capita\", \"Population\"\n    population = {\n        'Paris': 2190327,\n        'Stockholm': 961609,\n        'London': 8908081,\n        'Berlin': 3644826,\n        'Munich': 1456039,\n        'Oporto': 237591,\n        'Milan': 1378689,\n        'Bratislava': 432864,\n        'Vienna': 1821582,\n        'Rome': 4355725,\n        'Barcelona': 1620343,\n        'Madrid': 3223334,\n        'Dublin': 1173179,\n        'Brussels': 179277,\n        'Zurich': 428737,\n        'Warsaw': 1758143,\n        'Budapest': 1752286,\n        'Copenhagen': 615993,\n        'Amsterdam': 857713,\n        'Lyon': 506615,\n        'Hamburg': 1841179,\n        'Lisbon': 505526,\n        'Prague': 1301132,\n        'Oslo': 673469,\n        'Helsinki': 643272,\n        'Edinburgh': 488100,\n        'Geneva': 200548,\n        'Ljubljana': 284355,\n        'Athens': 664046,\n        'Luxembourg': 115227,\n        'Krakow': 769498\n    }\n\n    df['Population'] = df['City'].map(population)\n    df['Rests per Capita'] = df['Rests per City'] \/ df['Population']\n\n    #a new feature 'relative ranking'\n    df['Relative Ranking'] = df['Ranking'] \/ df['Rests per City']\n\n    # a dummy variable \"Pricing\" based on price range \"$\", \"$$ - $$$\", \"$$$\"\n    price_range = {'$': 0, '$$ - $$$': 1, '$$$$': 2}\n\n    df['Pricing'] = df['Price_Range'].map(price_range)\n    df['Pricing'].fillna(df['Pricing'].mean(), inplace=True)\n\n    df['Price_Unknown'] = df['Price_Range'].isna().astype('int32')\n\n    del df['Price_Range']\n\n    # a new feature \"quantity of reviews per a City\"\n    df['Number_of_Reviews'].fillna(df['Number_of_Reviews'].mean(), inplace=True)\n    df['fb per City'] = df['Number_of_Reviews'].map(\n        df.groupby(['Number_of_Reviews'\n                    ])['Number_of_Reviews'].count().to_dict())\n\n    #df['fb \/ Rest \/ Person'] = df['Number_of_Reviews'] \/ df['Rests per Capita'] - excluded as MAE is better without this feature\n\n    # a dummy variable \"Population and Tourists flow\"\n    population_tourists = list(\n        open('..\/input\/tripadvisor-competition-additional-dataset\/population_and_tourists.txt', 'r'))\n\n    population_dict = {}\n    tourists_dict = {}\n\n    for city in population_tourists:\n        data = city.rstrip().split(' ')\n        population_dict.update({data[0]: data[1]})\n        tourists_dict.update({data[0]: data[2]})\n\n    def data_update_population(city):\n        try:\n            city = str(city)\n            return float(population_dict.get(city))\n        except:\n            pass\n\n    def data_update_tourists(city):\n        try:\n            city = str(city)\n            return float(tourists_dict.get(city))\n        except:\n            pass\n\n    df['population'] = df['City'].apply(data_update_population)\n    df['tourists'] = df['City'].apply(data_update_tourists)\n\n    df[['tourists', 'population']] = df[['tourists', 'population']].fillna(0)\n    df[['tourists', 'population']] = df[['tourists',\n                                         'population']].astype('int64')\n\n    del df['City']\n\n    # a dummy variables \"cuisines\"\n    df['Cuisine Qntty'] = df['Cuisine_Style'].str[2:-2].str.split(\n        \"', '\").str.len().fillna(1)\n\n    del df['Cuisine_Style']\n\n    #split reviews (feedback (fb)) by review_content and by date of review\n    reviews = df['Reviews'].str[3:-3].str.split(', ').dropna(0)\n\n    def fill_up_new_column(serie, column):\n        serie = serie[3:-3].split(', ')\n        try:\n            if len(serie) == 4:\n                return serie[column]\n            elif len(serie) == 2:\n                if column == 1 and column == 2:\n                    return serie[column]\n                elif column == 3 and column == 4:\n                    return ''\n            else:\n                return ''\n        except:\n            return ''\n\n    df['fb1_content'] = df['Reviews'].apply(fill_up_new_column, column=0)\n    df['fb1_date'] = df['Reviews'].apply(\n        fill_up_new_column,\n        column=2)  #data contains more than 2 feedbacks at some point\n    df['fb2_content'] = df['Reviews'].apply(fill_up_new_column, column=1)\n    df['fb2_date'] = df['Reviews'].apply(fill_up_new_column, column=3)\n\n    #cleanup the date info\n    df['fb1_date'] = df['fb1_date'].apply(\n        lambda x: re.findall(r'\\d\\d\/\\d\\d\/\\d{4}', str(x)))\n    df['fb2_date'] = df['fb2_date'].apply(\n        lambda x: re.findall(r'\\d\\d\/\\d\\d\/\\d{4}', str(x)))\n\n    # a new dummy 'no feedback'\n    df['fb_none'] = df['Reviews'].apply(\n        lambda x: 0 if bool(re.search('\\w+', x)) == True else 1)\n\n    df.drop(['Reviews'], axis=1, inplace=True)\n\n    # a dummy \"days since the last review\"\n    df[['fb1_date', 'fb2_date']] = df[['fb1_date',\n                                       'fb2_date']].fillna('01\/01\/2000')\n    today = '03\/04\/2020'\n    today = datetime.strptime(today, '%m\/%d\/%Y')\n\n    def days_count(date):\n        try:\n            date = str(date)[2:-2]\n            date = datetime.strptime(date, '%m\/%d\/%Y')\n            time_delta = (today - date).days\n        except:\n            return 7368  # count empty datacells as 7368 days (since 01\/01\/2000) ... kind of 'a long period'\n        return time_delta\n\n    df['days fb1'] = df['fb1_date'].apply(days_count)\n    df['days fb2'] = df['fb2_date'].apply(days_count)\n    df.drop(['fb1_date', 'fb2_date'], axis=1, inplace=True)\n\n    # add dummy \"days between reviews\"\n    df['review_gap'] = df['days fb1'] - df['days fb2']\n\n    # add dummy variables \"feedback tonality\"\n    # (!)attention(!) there is a mistake in the code as a few tests showed\n    # didn't clean up the code intentionally\n\n    #negative_adj.txt & negative_adj.txt contain a list of positive\/negative words \n    \n    f = open('..\/input\/tripadvisor-competition-additional-dataset\/negative_adj.txt', 'r')\n    negative_adj = []\n    for line in f:\n        negative_adj.append(line[:-1].lower())\n\n    f = open('..\/input\/tripadvisor-competition-additional-dataset\/negative_adj.txt', 'r')\n    positive_adj = []\n    for line in f:\n        positive_adj.append(line[:-1].lower())\n\n    df['fb1_content'] = df['fb1_content'].fillna('no data')\n    df['fb2_content'] = df['fb2_content'].fillna('no data')\n\n    def find_item(feedback, word):\n        if feedback.find(word) > 0:\n            return 1\n        else:\n            return 0\n\n    for word in positive_adj:\n        df['fb1_positive'] = df['fb1_content'].apply(find_item, word=word)\n\n    for word in negative_adj:\n        df['fb1_negative'] = df['fb1_content'].apply(find_item, word=word)\n\n    for word in positive_adj:\n        df['fb2_positive'] = df['fb2_content'].apply(find_item, word=word)\n\n    for word in negative_adj:\n        df['fb2_negative'] = df['fb2_content'].apply(find_item, word=word)\n\n    df.drop(['fb1_content', 'fb2_content'], axis=1, inplace=True)\n\n    df = df.drop(\n        ['Network', 'Capital'], axis=1\n    )  #features were droped as a few experiments showed that MAE was better without them\n\n    return df","98884ea2":"# rating - the fuction aligns the final model resul to the Trip Advisor scale, which is multiple to 0.25 \ndef rating(prediction):\n        if prediction < 0.25:\n            return 0\n        elif 0.25 < prediction <= 0.75:\n            return 0.5\n        elif 0.75 < prediction <= 1.25:\n            return 1\n        elif 1.25 <prediction <= 1.75:\n            return 1.5\n        elif 1.75 < prediction <= 2.25:\n            return 2\n        elif 2.25 < prediction <= 2.75:\n            return 2.5\n        elif 2.75 < prediction <= 3.25:\n            return 3\n        elif 3.25 < prediction <= 3.75:\n            return 3.5\n        elif 3.75 < prediction <= 4.25:\n            return 4\n        elif 4.25 < prediction <= 4.75:\n            return 4.5\n        else:\n            return 5","3bb9fab9":"def baseline_model(df_model):\n    X = df_model.query('sample == 1').drop(['sample','Rating'], axis=1)\n    y = df_model.query('sample == 1')['Rating']\n\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state = 42)\n\n    model = RandomForestRegressor(n_estimators=100, verbose=1, n_jobs=-1, random_state=42)\n    model.fit(X_train, y_train)\n\n    y_pred = model.predict(X_test)\n\n    for i in range(y_pred.size):\n        y_pred[i]=rating(y_pred[i])\n\n    mae = round(metrics.mean_absolute_error(y_test, y_pred),4)\n    print('MAE:', mae)\n\n    #plot the most valuable features\n    plt.rcParams['figure.figsize'] = (5,5)\n    feat_importances = pd.Series(model.feature_importances_, index=X.columns)\n    feat_importances.nlargest(25).plot(kind='barh')\n    \n    return","b6921a1f":"df_model = features_proccessing(df)\nbaseline_model(df_model)\nallDone()","8e700bcb":"import seaborn as sns\nimport numpy as np\n\nsns.set(style=\"white\")\ncorr = df_model.corr()\n\n# Generate a mask for the upper triangle\nmask = np.triu(np.ones_like(corr, dtype=np.bool))\n\n# Generate a custom diverging colormap\ncmap = sns.diverging_palette(220, 10, as_cmap=True)\n\n# Set up the matplotlib figure\nf, ax = plt.subplots(figsize=(8, 8))\n\nsns.heatmap(corr, mask=mask, cmap=cmap, vmax=.3, center=0,\n            square=True, linewidths=.5, cbar_kws={\"shrink\": .5})","40026107":"df_temp = df_model.drop('Cuisine Qntty', axis = 1)\nbaseline_model(df_temp)","498e6487":"columns = list(df_model.columns)\ncolumns.remove('Rating')","d834d748":"# add poly features\nfrom sklearn.preprocessing import PolynomialFeatures\n\npf = PolynomialFeatures(2)\npoly_features = pf.fit_transform(df_model[columns])","5651ab24":"%%time\ndf1 = pd.concat([pd.DataFrame(poly_features), df_model], axis =1)\nbaseline_model(df1)\nallDone()","0e1b1caa":"df['all_reviews'] = df['Reviews']\ndf['all_reviews'].replace('[[], []]', 'empty', inplace=True)\n\ndef clean_feedback(content):\n    pattern = r'[^a-zA-Z ]'\n    content = re.sub(pattern,'',(str(content).lower()))\n    return content\n\ndf['all_reviews'] = df['all_reviews'].apply(clean_feedback)","ffdddce8":"# assumption: all comments with rating < negative_cutoff are negative (label = 0)\n# all comments with rating >= positive_cutoff are positive (label = 1)\n\nnegative_cutoff = 3.0\npositive_cutoff = 4.0\n\ndf_content = pd.DataFrame()\ndf_content = df.query('sample == 1')[['Rating','all_reviews']]\ndf_content['feedback'] = None\ndf_content.reset_index(inplace = True)","dc80efd2":"for i in range (len(df_content)):\n    if df_content.all_reviews.loc[i] != 'empty':\n        if df_content.Rating.loc[i] >= positive_cutoff:\n            df_content.feedback.loc[i] = 1\n        elif df_content.Rating.loc[i] < negative_cutoff:\n            df_content.feedback.loc[i] = 0\n        else:\n            df_content.feedback.loc[i] = None\n    else:\n        df_content.feedback.loc[i] = None","03af9688":"df_content.feedback.hist(figsize = (2,2))","f72a6833":"df_content.feedback = df_content.feedback.apply(lambda x: 0 if x == None else x)\ndf_content.feedback.hist(figsize = (2,2))\ndf_content = df_content.query('all_reviews != \"empty\"')\ndf_content.drop(['Rating'], axis = 1, inplace = True)","cf39e5d1":"from sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n\n#stop_words = list(ENGLISH_STOP_WORDS) \n#vect.vocabulary_","0f504dbf":"from sklearn.pipeline import Pipeline\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import f1_score\nfrom sklearn.feature_extraction.text import CountVectorizer\n\nX = df_content.all_reviews\ny = df_content.feedback\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n\ncfm = confusion_matrix\n\ntext_clf = Pipeline([\n                     ('CV', CountVectorizer(min_df=2)),\n                     ('rf', RandomForestRegressor(n_jobs = -1, n_estimators=70))\n                     ])\n \ntext_clf.fit(list(X_train), y_train.array)","c2c319c6":"y_pred = []\nfor x in X_test:\n    y_pred.append(text_clf.predict([x])[0])\n    \ny_pred = pd.Series(y_pred)\nallDone()","6aeccc18":"y_pred_adj = []\ntreshhold = 0.54122\n\nfor y in y_pred:\n    y_pred_adj.append(1 if y > treshhold else 0)","6b4f1f4e":"from sklearn.metrics import f1_score\n\ncnf_matrix = metrics.confusion_matrix(y_test, y_pred_adj, normalize='all')\nprint(f1_score (y_test, y_pred_adj, average='macro'))\n\nfig, ax = plt.subplots(figsize=(3,3)) \nsns.heatmap(cnf_matrix, annot=True)","f0a1567d":"# optimize f1_score based on treshhold:\n\nf1_thresh = {}\n\nfor threshhold in range (100, 900, 5):\n    y_pred_adj = []\n    \n    for y in y_pred:\n        y_pred_adj.append(1 if y > threshhold\/1000 else 0)\n        \n    #cnf_matrix = metrics.confusion_matrix(y_test, y_pred_adj, normalize='all')\n    f1 = f1_score (y_test, y_pred_adj, average='macro')\n    f1_thresh[threshhold\/1000] = f1\n\nprint(f'threshhold \/ f1:  {max(f1_thresh, key=f1_thresh.get)} \/ {max(f1_thresh.values())}')","a2d48fa9":"# find optimized confusion_matrix\n\ny_pred_adj = []\nfor y in y_pred:\n    y_pred_adj.append(1 if y > max(f1_thresh, key=f1_thresh.get) else 0)\n\nfig, ax = plt.subplots(figsize=(3,3)) \ncnf_matrix = metrics.confusion_matrix(y_test, y_pred_adj, normalize='all')\nsns.heatmap(cnf_matrix, annot=True)","c6afe386":"# find with TRUE POSITIVE focus \n\nf1_thresh = {}\n\nfor threshhold in range (1, 900, 5):\n    y_pred_adj = []\n    \n    for y in y_pred:\n        y_pred_adj.append(1 if y > threshhold\/1000 else 0)\n        \n    #cnf_matrix = metrics.confusion_matrix(y_test, y_pred_adj, normalize='all')\n    cnf_matrix = metrics.confusion_matrix(y_test, y_pred_adj, normalize='all')\n    f1_thresh[threshhold\/1000] = cnf_matrix[1,1]\n\nprint(f'threshhold \/ f1:  {max(f1_thresh, key=f1_thresh.get)} \/ {max(f1_thresh.values())}')\nfig, ax = plt.subplots(figsize=(3,3)) \ncnf_matrix = metrics.confusion_matrix(y_test, y_pred_adj, normalize='all')\nsns.heatmap(cnf_matrix, annot=True)","e9a17156":"# train model on all dataset without \"train\", \"test\" subsets \n\n# df_model = features_proccessing(df)\n\n\nX = df_model.query('sample == 1').drop(['sample','Rating'], axis=1)\ny = df_model.query('sample == 1')['Rating']\n\nmodel = RandomForestRegressor(n_estimators=100, verbose=1, n_jobs=-1, random_state=42)\nmodel.fit(X, y)\n\nX_kaggle = df_model.query('sample == 0').drop(['sample','Rating'], axis=1)\ny_kaggle = model.predict(X_kaggle)\n\nfor i in range(y_kaggle.size):\n    y_kaggle[i]=rating(y_kaggle[i])\n\nSolution['Rating'] = y_kaggle\n\nSolution.to_csv('solution14-27-04-20.csv', index = False)\nallDone()","2d537d4d":"## libs & drivers import","06103014":"Kaggle result is 0.20025","8f425534":"### ( FAIL ) Polynom features","f6b4dcaf":"## Dataset Upload & Setup","8a200f30":"An attempt to convert the text feedback into a feature. The most promising action but the current version failed due to the overfitting.","18e17783":"### ( FAIL ) attemp to drop some features , which showed mutual correlation","428e8677":"# TripAdviser Restaurant Rating","a511598d":"<b>Business Goal:  <\/b> to predict the Tripadvisor Restaurant rating based on a historical dataset<br><br>\n\n<b>General Comments:<\/b><br>\n1) the initial train dataset showed MAE 0.1688<br>\n2) an additional dataset (100k+ lines) has been used to improve the score  <br>\n3) a few attempts to improve the score was applies but failed (see the relevant marks below) <br>\n4) seems like the primitive text analysis ('Reviews' datafield), which was introduced in the <i>def features_proccessing()<\/i>  didn't work well. Plus a mistake in the code was found. \n\n<b>Further improvements:<\/b> 1) I would play with the \"Reviews\" datafield as it was shown dramatic improvement on the initial dataset (MAE = 0.12+) but the model was overfitted as the Kaggle set test showed<br>\n\nIf you have questions or comments be free to message me.","8a6f544d":"there was as improvement on the tests dataset but the Kaggle dataset showed a worse result","9ec11fda":"this section introduces the baseline modeling","5e53ac34":"as there is a huge disbalance, we consider only 'positive' as a feature as a start","793b1af4":"#### ( FAIL ) as 'Cuisine Qntty' has a corr with 'Relative Ranking' & 'Price_Unknown', let's drop 'Cuisine Quantity' to check the accuracy","568d8d50":"### Feedback text analysis","a45139ec":"## Kaggle data file","807c8c60":"### \"positive\" feature optimization based on the threshhold ","8f904c24":"## Baseline Model","38562817":"- as the result, based on f1_score, the threshhold shall be 0.605<br>\n- next time GridSearchCV should be applied","7f4b250b":"all features processing was packed into a function to ease the further experiments with the datasets\n\n","fe4313fb":"### ( FAIL ) Text Feedback Features\n","c2a5a128":"#### the features correlation matrix"}}