{"cell_type":{"382d7665":"code","90cda2bf":"code","a06ea5fe":"code","8b8ffc62":"code","a7b9d977":"code","6af199e1":"code","c755ae96":"code","a03fef7f":"code","52436996":"code","78a5685f":"code","25cb0071":"code","1533a419":"code","506282c4":"code","38f6c55c":"code","a1408067":"code","c5012ce0":"code","d7b8a0cc":"code","869fc80e":"code","72c39f41":"code","f4c7a595":"code","78bb2ec7":"code","398e4451":"code","b035662c":"code","13486a15":"code","33c5d67a":"code","f4b9cb56":"code","22f418cc":"code","84d57f57":"code","2a144381":"code","ab3fe19e":"code","0303b8b6":"code","c8a854cf":"code","38aff942":"code","e38dbb39":"code","278f27f7":"code","6c1ddd41":"code","2278d8d4":"code","f4654936":"code","397cb5ab":"code","63178407":"code","a97685fa":"code","c3ab7590":"code","1b9dca81":"code","0701dccf":"code","41db6b4b":"code","93e69f49":"code","66eceb14":"code","90713a49":"code","994202a0":"code","39bb0af2":"code","577cfce2":"code","bfc7e175":"code","8b36ca74":"code","49da4f76":"code","466c6583":"code","407ba89d":"code","4a9861c3":"code","f54530e4":"code","4a006e80":"code","9ddafc54":"code","2c3e0573":"code","7784aba7":"markdown","801275d2":"markdown","f2de99fe":"markdown","314fa874":"markdown","298a8c8d":"markdown","4e142d6d":"markdown","1217a689":"markdown","5e4294e2":"markdown","a7a3cb58":"markdown","8955dfef":"markdown","c7611152":"markdown","01f026f5":"markdown","5c17854a":"markdown","59251de0":"markdown","411e66d7":"markdown","46777cf9":"markdown","5f695b2b":"markdown","400287ac":"markdown","536625b4":"markdown","0dce6dd5":"markdown","f46bf856":"markdown","a65875e5":"markdown","6d8cac47":"markdown","caefcd05":"markdown","47c00316":"markdown","7d2918c9":"markdown","e2b8dd54":"markdown","0ae39fbe":"markdown","13af0bf4":"markdown","9ddec15d":"markdown","8209c959":"markdown"},"source":{"382d7665":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nplt.style.use('ggplot')\nimport warnings\nwarnings.filterwarnings('ignore')\n\n\nfrom tabulate import tabulate\n\nimport nltk        # Imports the natural language toolkit\nfrom nltk.corpus import stopwords\n\npd.set_option(\"display.max_colwidth\", 100)","90cda2bf":"train_df = pd.read_parquet('..\/input\/kaggle-pog-series-s01e01\/train.parquet')\ntest_df = pd.read_parquet('..\/input\/kaggle-pog-series-s01e01\/test.parquet')\n# ss = pd.read_csv('..\/input\/kaggle-pog-series-s01e01\/sample_submission.csv')\n\n# train_df = pd.read_parquet('train.parquet')\n# test_df = pd.read_parquet('test.parquet')","a06ea5fe":"train_df.head(3)","8b8ffc62":"train_df['ratings_disabled'].value_counts()","a7b9d977":"train_df.groupby('ratings_disabled')['target'].mean()","6af199e1":"train_df=train_df[~train_df['ratings_disabled']]","c755ae96":"#https:\/\/stackoverflow.com\/questions\/29177498\/python-pandas-replace-nan-in-one-column-with-value-from-corresponding-row-of-sec\ntrain_df.loc[train_df['description'].isnull(),'description'] = train_df['title']","a03fef7f":"#https:\/\/stackoverflow.com\/questions\/19377969\/combine-two-columns-of-text-in-pandas-dataframe\ntrain_df['description_2'] = train_df[['title','description']].agg(' '.join, axis=1)\ntrain_df[['title','description','description_2']].head(15)","52436996":"#below only used to clean text for word length count\n\ndef text_process_1(text):\n    #text=text.decode('utf-8')\n    \n    # Replace the xa0 with a space\n    #text=text.replace('xa0',' ')\n    # Replace the \\xa0 with a space\n    #text=text.replace('\\xa0',' ')\n    # Replace the \\n\\n with a space\n    text=text.replace('\\n\\n',' ')\n    # Replace the \\n with a space\n    text=text.replace('\\n',' ')\n    # Replace apostrophes with nothing\n    text=text.replace('\\'','')\n    # Replace http with a space\n    text=text.replace('http','')\n    \n    text=text.replace('#',' ')\n    text=text.replace('!',' ')\n    \n    text=text.replace('\\r',' ')\n    \n    text=text.replace('vlogs',' video logs')\n    \n    \n    tokens = nltk.word_tokenize(text)\n    \n    cleaned_words=[w.lower() for w in tokens if w.isalnum()]\n    #remove any words that are actually digits. \n    no_integers = [x for x in cleaned_words if not (x.isdigit() or x[0] == '-' and x[1:].isdigit())]\n    no_integers = ' '.join(no_integers)\n    \n    return no_integers","78a5685f":"import time\nt_start = time.time()\n\ntrain_df['clean']=train_df['description_2'].apply(lambda x: text_process_1(x))\ntrain_df['wd_len']=train_df['clean'].apply(lambda x: len(nltk.word_tokenize(x)))\n\n\n\nt_stop = time.time()\nprint('Time elapsed: {:.3f} seconds'.format(t_stop - t_start))","25cb0071":"def text_cleaner(df_input):    \n    df_input['clean_text']=df_input['description_2'].str.lower()\n\n    df_input['clean_text']=df_input['clean_text'].str.replace('\\n\\n',' ')\n    df_input['clean_text']=df_input['clean_text'].str.replace('\\n',' ')\n    df_input['clean_text']=df_input['clean_text'].str.replace('\\s',' ')\n    df_input['clean_text']=df_input['clean_text'].str.replace('xa0',' ')\n    df_input['clean_text']=df_input['clean_text'].str.replace('\\xa0',' ')\n\n    df_input['clean_text']=df_input['clean_text'].str.replace('http',' ')\n    #df_input['clean_text']=df_input['clean_text'].str.replace('utc',' ')\n    df_input['clean_text']=df_input['clean_text'].str.replace('www',' ')\n    df_input['clean_text']=df_input['clean_text'].str.replace(r'\\.com',' ')\n    df_input['clean_text']=df_input['clean_text'].str.replace('-',' ')\n    df_input['clean_text']=df_input['clean_text'].str.replace('=',' ')\n    df_input['clean_text']=df_input['clean_text'].str.replace(':',' ')\n    df_input['clean_text']=df_input['clean_text'].str.replace(',',' ')\n    df_input['clean_text']=df_input['clean_text'].str.replace('*',' ')\n\n\n\n    df_input['clean_text']=df_input['clean_text'].str.replace('!','! ') \n    #df_input['clean_text']=df_input['clean_text'].str.replace('\\\"',' ')\n\n    df_input['clean_text']=df_input['clean_text'].str.replace('#',' ')\n    df_input['clean_text']=df_input['clean_text'].str.replace('!',' ')\n    df_input['clean_text']=df_input['clean_text'].str.replace('\\r',' ')\n    df_input['clean_text']=df_input['clean_text'].str.replace('vlogs',' video logs')\n    df_input['clean_text']=df_input['clean_text'].str.replace('lmao',' laughing my ass off')\n    \n    df_input['clean_text']=df_input['clean_text'].str.replace('pga',' professional golf association ')\n    df_input['clean_text']=df_input['clean_text'].str.replace('nba',' national basketball association ')\n    df_input['clean_text']=df_input['clean_text'].str.replace('mlb',' major league baseball ')\n    df_input['clean_text']=df_input['clean_text'].str.replace('nhl',' national hockey league ')\n    df_input['clean_text']=df_input['clean_text'].str.replace('nfl',' national football league ')\n    \n    #iPadOS\n    #iOS etc\n\n        # specific contractions\n    df_input['clean_text']=df_input['clean_text'].str.replace(r\"won\\'t\", \"will not\")\n    df_input['clean_text']=df_input['clean_text'].str.replace(r\"can\\'t\", \"can not\")\n\n        # general contractions\n    df_input['clean_text']=df_input['clean_text'].str.replace(r\"n\\'t\", \" not\")\n    df_input['clean_text']=df_input['clean_text'].str.replace(r\"\\'re\", \" are\")\n    df_input['clean_text']=df_input['clean_text'].str.replace(r\"\\'s\", \" is\")\n    df_input['clean_text']=df_input['clean_text'].str.replace(r\"\\'d\", \" would\")\n    df_input['clean_text']=df_input['clean_text'].str.replace(r\"\\'ll\", \" will\")\n    df_input['clean_text']=df_input['clean_text'].str.replace(r\"\\'t\", \" not\")\n    df_input['clean_text']=df_input['clean_text'].str.replace(r\"\\'ve\", \" have\")\n    df_input['clean_text']=df_input['clean_text'].str.replace(r\"\\'m\", \" am\")\n    \n    \n    df_input['clean_text']=df_input['clean_text'].str.replace(r\"won\\\u2019t\", \"will not\")\n    df_input['clean_text']=df_input['clean_text'].str.replace(r\"can\\\u2019t\", \"can not\")\n\n        # general contractions\n    df_input['clean_text']=df_input['clean_text'].str.replace(r\"n\\\u2019t\", \" not\")\n    df_input['clean_text']=df_input['clean_text'].str.replace(r\"\\\u2019re\", \" are\")\n    df_input['clean_text']=df_input['clean_text'].str.replace(r\"\\\u2019s\", \" is\")\n    df_input['clean_text']=df_input['clean_text'].str.replace(r\"\\\u2019d\", \" would\")\n    df_input['clean_text']=df_input['clean_text'].str.replace(r\"\\\u2019ll\", \" will\")\n    df_input['clean_text']=df_input['clean_text'].str.replace(r\"\\\u2019t\", \" not\")\n    df_input['clean_text']=df_input['clean_text'].str.replace(r\"\\\u2019ve\", \" have\")\n    df_input['clean_text']=df_input['clean_text'].str.replace(r\"\\\u2019m\", \" am\")\n    \n    # Replace apostrophes with nothing\n    df_input['clean_text']=df_input['clean_text'].str.replace('\\'','')\n\n    df_input['clean_text']=df_input['clean_text'].str.replace(r'[^a-zA-Z0-9]',' ')\n    df_input['clean_text']=df_input['clean_text'].str.lower()\n\n    df_input['clean_text'] = df_input['clean_text'].str.replace(r'\\s\\s+', ' ')\n    df_input['clean_text'] = df_input['clean_text'].str.replace('\\s', ' ')\n\n    return df_input","1533a419":"import time\nt_start = time.time()\n\ntrain_df_2=train_df[(train_df['wd_len']<=200) & (train_df['wd_len']>5)].copy()\ntrain_df_2=train_df_2.reset_index()\ntrain_df_2.pop('index')\n\ntrain_df_2=text_cleaner(train_df_2)\n\n\nt_stop = time.time()\nprint('Time elapsed: {:.3f} seconds'.format(t_stop - t_start))","506282c4":"#https:\/\/stackoverflow.com\/questions\/31726643\/how-to-plot-in-multiple-subplots\n#https:\/\/stackoverflow.com\/questions\/63895392\/seaborn-is-not-plotting-within-defined-subplots\n\nplt.subplot(1,2, 1)\nax = sns.histplot(data=train_df, x='wd_len')\n#ax = sns.displot(data=train_df, x='target', height=8, aspect=1.5)#, bins=30)\n\n#plt.show()\n\nplt.subplot(1,2, 2)\nax = sns.histplot(data=train_df, x='target')\n#ax = sns.displot(data=train_df, x='wd_len', height=8, aspect=1.5)#, bins=30)\n#plt.show()\nplt.gcf().set_size_inches(25,5)\n","38f6c55c":"train_df.loc[train_df['target']<.1,'target_cat'] = 'point_1'\ntrain_df.loc[(train_df['target']>=.1)&(train_df['target']<.2),'target_cat'] = 'point_2'\ntrain_df.loc[(train_df['target']>=.2)&(train_df['target']<.3),'target_cat'] = 'point_3'\ntrain_df.loc[(train_df['target']>=.3)&(train_df['target']<.5),'target_cat'] = 'point_4'","a1408067":"plt.subplot(1,2,1)\nax = sns.histplot(data=train_df, x='wd_len', hue='target_cat')\nax = sns.displot(data=train_df, x='wd_len', hue='target_cat',height=8, aspect=1.5)#, bins=30)\n\nplt.subplot(1,2,2)\nax = sns.histplot(data=train_df[train_df['wd_len']<=600], x='wd_len', hue='target_cat')\n#ax = sns.displot(data=train_df[train_df['wd_len']<=600], x='wd_len', hue='target_cat',height=8, aspect=1.5,multiple=\"stack\")#, bins=30)\nplt.gcf().set_size_inches(35,5)","c5012ce0":"import seaborn as sns, numpy as np\nax = sns.displot(data=train_df[train_df['wd_len']<=600], x='wd_len', hue='target_cat',height=8, aspect=1.5,multiple=\"stack\")#, bins=30)","d7b8a0cc":"import numpy as np # linear algebra\nimport os, sys\n\nfrom gensim.models import Word2Vec, KeyedVectors\nfrom gensim.test.utils import datapath\n\nimport time\nt_start = time.time()\n\n\n# Get path to file\nvector_size = 300\n\n\nglove_file = '..\/input\/fasttext-crawl-300d-2m\/crawl-300d-2M.vec'\n#glove_file = 'crawl-300d-2M.vec'\n\n# Load with gensim\nmodel = KeyedVectors.load_word2vec_format(glove_file)\n\nt_stop = time.time()\nprint('Time elapsed: {:.3f} seconds'.format(t_stop - t_start))","869fc80e":"min_cols=train_df_2.groupby('video_id').min()\nmin_cols=min_cols.reset_index()\nmin_dt=min_cols[['video_id','trending_date']].copy()\nmin_dt.columns=['video_id', 'min_trending_dt']\n\ntrain_df_4=train_df_2.merge(min_dt, how='inner', on='video_id')\ntrain_df_5=train_df_4[(train_df_4['trending_date']==train_df_4['min_trending_dt'])].copy()\ntrain_df_5=train_df_5.reset_index()","72c39f41":"#average first 30 words in the sentence\ndef sent_vectorizer_1(text):\n    n=len(text)\n    text_1=text[0:n]\n    vectors = [model[x] for x in text_1.split(' ') if x in model.key_to_index]\n    #sent_vector=sum(vectors)\/len(vectors)\n    if len(vectors) != 0:\n        sent_vector=sum(vectors)\/len(vectors)\n    elif len(text) !=0:\n        sent_vector=sum(vectors)\/len(text)\n    else:\n        sent_vector=sum(vectors)\/1\n    return sent_vector","f4c7a595":"X=train_df_5['clean_text']\ny=train_df_5['target']","78bb2ec7":"import time\nt_start = time.time()\n\ndf_glove=pd.DataFrame(X)\ndf_glove.columns=['text']\ndf_glove['sent_vec_1']=df_glove['text'].apply(lambda x: sent_vectorizer_1(x))\n\n\nt_stop = time.time()\nprint('Time elapsed: {:.3f} seconds'.format(t_stop - t_start))","398e4451":"df_glove","b035662c":"import time\nt_start = time.time()\n\nX_glove=df_glove['sent_vec_1']\n\n\nddd=pd.DataFrame()\n\n\nfor idx, row in enumerate(X_glove):\n    #print((roww))\n    #print(list(roww))\n    row_df=pd.Series(X_glove[idx]).to_frame()\n    row_df_2=row_df.T\n    ddd=pd.concat([ddd, row_df_2])\n    #row_df= pd.DataFrame(roww)\n#     row_df_2=row_df.T\n#     ddd=pd.concat([ddd, row_df_2])\n    \nt_stop = time.time()\nprint('Time elapsed: {:.3f} seconds'.format(t_stop - t_start))","13486a15":"df_glove_avg_2=ddd.reset_index()\ndf_glove_avg_2.pop('index')\nmerged_df=df_glove_avg_2.merge(y,left_index=True, right_index=True)","33c5d67a":"#delete problematic rows\nmerged_df[merged_df[0]==0]","f4b9cb56":"merged_df_2=merged_df[~(merged_df[0]==0)]","22f418cc":"import time\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport matplotlib.cm as cm\n%matplotlib inline\n\nimport keras\nimport tensorflow\n\nimport tensorflow.keras\nfrom tensorflow.keras.constraints import MaxNorm\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.optimizers import SGD\nfrom keras.utils import np_utils\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.layers import Dense, Activation, Flatten, Dropout, BatchNormalization\nfrom tensorflow.keras.layers import Conv2D, MaxPooling2D\nfrom tensorflow.keras.datasets import cifar10\nfrom tensorflow.keras import regularizers\nfrom tensorflow.keras.callbacks import LearningRateScheduler\nfrom tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, TensorBoard\nimport numpy as np\n\nfrom keras.layers.convolutional import Conv1D, MaxPooling1D\n\n\nimport keras.utils\nfrom keras import utils as np_utils\n\nfrom keras.utils import np_utils\n\n\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense\nfrom tensorflow.keras.layers import LSTM\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.metrics import mean_squared_error\n\nfrom keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation, GRU, Convolution1D, Flatten, LeakyReLU\nfrom keras.layers import Bidirectional, GlobalMaxPool1D, GlobalAveragePooling1D, MaxPooling1D, GlobalMaxPooling1D\nfrom keras.layers import SpatialDropout1D, MaxPooling1D, Bidirectional, GRU, concatenate\n\n\n# import necessary tools and models \nimport seaborn as sns\nimport matplotlib.pyplot as plt \n\nimport sklearn.model_selection as cv\nfrom sklearn.model_selection import cross_val_score\nimport numpy as np","84d57f57":"import seaborn as sns, numpy as np\nax = sns.distplot(merged_df_2['target'])","2a144381":"from sklearn.preprocessing import PowerTransformer\nfrom sklearn.preprocessing import QuantileTransformer\n\ntrans_y = QuantileTransformer(output_distribution=\"uniform\")\n#trans_y = PowerTransformer()\n#trans_y = MinMaxScaler()\n\n\ny_2 = trans_y.fit_transform(merged_df_2['target'].values.reshape(-1, 1))","ab3fe19e":"ax_2 = sns.distplot(y_2)","0303b8b6":"X=merged_df_2.loc[:, merged_df_2.columns!='target']","c8a854cf":"# Regular Toxic - separate train - test data \nX_train_g200, X_test_g200, y_train_g200, y_test_g200 = cv.train_test_split(X, y_2,test_size=0.2)\n\n#get number of columns in training data\nn_cols = X_train_g200.shape[1]\n#n_cols = X_train_tfidf.shape[1]","38aff942":"X_train_g200.shape","e38dbb39":"#https:\/\/datascience.stackexchange.com\/questions\/72351\/how-to-prevent-vanishing-gradient-or-exploding-gradient\n#https:\/\/stackoverflow.com\/questions\/54011173\/what-is-the-default-weight-initializer-in-keras\nDL_model = keras.models.Sequential([\n#    keras.layers.BatchNormalization(),\n    keras.layers.Dense(256, activation=\"relu\",input_shape=(n_cols,)),\n    keras.layers.Dropout(0.3),\n    keras.layers.Dense(128, activation=\"relu\",input_shape=(n_cols,)),\n    keras.layers.Dropout(0.3),\n    keras.layers.BatchNormalization(),\n    keras.layers.Dense(64, activation=\"relu\"),\n    keras.layers.Dropout(0.3),\n    keras.layers.Dense(10, activation=\"relu\"),\n    keras.layers.Dropout(0.3),\n    keras.layers.Dense(1)])\n    #keras.layers.Dense(1, activation=\"relu\")])\n    #])","278f27f7":"# Fit model\nepochs = 200\nbatch_size=32","6c1ddd41":"DL_model.compile(loss=keras.losses.MeanAbsoluteError(),\n              optimizer=tensorflow.keras.optimizers.Adadelta())","2278d8d4":"# my_callbacks = [\n#     EarlyStopping(monitor='val_loss', min_delta=0, patience=500,restore_best_weights=True),\n#     ModelCheckpoint(filepath='model.{epoch:02d}-{val_loss:.4f}.h5',save_best_only=True,),\n#     TensorBoard(log_dir='logs'),\n# ]\n\nmy_callbacks = [\n    EarlyStopping(monitor='val_loss', min_delta=0, patience=300,restore_best_weights=True)#,\n    #ModelCheckpoint(filepath='model.{epoch:02d}-{val_loss:.4f}.h5',save_best_only=True,),\n    #TensorBoard(log_dir='logs'),\n]","f4654936":"history = DL_model.fit(X_train_g200, y_train_g200,\n                       #epochs=100, batch_size=32,\n                       epochs=8000, batch_size=32,\n                       validation_split=0.2, callbacks=my_callbacks,shuffle=True,\n                      )","397cb5ab":"DL_model.save('best_model.h5')","63178407":"plt.figure(221)\n\n\nplt.figure(1)\nplt.plot(history.history['loss'],'r')\nplt.plot(history.history['val_loss'],'g')\nplt.xticks(np.arange(0, 21000, 1000))\nplt.rcParams['figure.figsize'] = (25,5)\nplt.xlabel(\"Num of Epochs\")\nplt.ylabel(\"Loss\")\nplt.title(\"Training Loss vs Validation Loss\")\nplt.legend(['train','validation'])\n\nplt.gcf().set_size_inches(15,5)\nplt.show()","a97685fa":"#testing\n#500 epochs produced loss: 0.2461\n#500 more epochs produced loss: 0.2025\nscores = DL_model.evaluate(X_test_g200, y_test_g200, batch_size=32, verbose=1)","c3ab7590":"Y_train_pred=DL_model.predict(X)\nY_train_pred_2=trans_y.inverse_transform(Y_train_pred)\nY_train_pred_2=pd.DataFrame(Y_train_pred_2)\nY_train_pred_2.columns=['Y_train_pred']","1b9dca81":"v_id_lookup=train_df_5[['video_id','target']].merge(merged_df,left_index=True, right_index=True)\nv_id_lookup=v_id_lookup.iloc[:,[0,1,2]]\nv_id_lookup.columns=['video_id','target','w2v']\n\n#need to remove the problematic row merged_df_2=merged_df[~(merged_df[0]==0)]\nv_id_lookup=v_id_lookup[~(v_id_lookup['w2v']==0)]\nv_id_lookup=v_id_lookup.reset_index()\nv_id_lookup.pop('index')\nv_id_lookup","0701dccf":"Y_train_pred_2","41db6b4b":"training_submit_df_3=pd.concat([v_id_lookup, Y_train_pred_2], axis=1)","93e69f49":"training_submit_df_3.to_csv('training_submit_df.csv', index=False)","66eceb14":"#test_df[test_df['description'].isnull()]","90713a49":"test_df.loc[test_df['description'].isnull(),'description'] = test_df['title']","994202a0":"#https:\/\/stackoverflow.com\/questions\/19377969\/combine-two-columns-of-text-in-pandas-dataframe\ntest_df['description_2'] = test_df[['title','description']].agg(' '.join, axis=1)\ntest_df[['title','description','description_2']].head(15)","39bb0af2":"import time\nt_start = time.time()\n\ntest_df=text_cleaner(test_df)\n\n\nt_stop = time.time()\nprint('Time elapsed: {:.3f} seconds'.format(t_stop - t_start))","577cfce2":"pred_X=test_df['clean_text']","bfc7e175":"import time\nt_start = time.time()\n\ndf_glove_test=pd.DataFrame(pred_X)\ndf_glove_test.columns=['text']\ndf_glove_test['sent_vec_1']=df_glove_test['text'].apply(lambda x: sent_vectorizer_1(x))\n\n\nt_stop = time.time()\nprint('Time elapsed: {:.3f} seconds'.format(t_stop - t_start))","8b36ca74":"import time\nt_start = time.time()\n\nX_glove_test=df_glove_test['sent_vec_1']\n\n\nddd=pd.DataFrame()\n\n\nfor idx, row in enumerate(X_glove_test):\n    #print((roww))\n    #print(list(roww))\n    row_df=pd.Series(X_glove_test[idx]).to_frame()\n    row_df_2=row_df.T\n    ddd=pd.concat([ddd, row_df_2])\n    #row_df= pd.DataFrame(roww)\n#     row_df_2=row_df.T\n#     ddd=pd.concat([ddd, row_df_2])\n    \nt_stop = time.time()\nprint('Time elapsed: {:.3f} seconds'.format(t_stop - t_start))","49da4f76":"df_glove_avg_2_test=ddd.reset_index()\ndf_glove_avg_2_test.pop('index')\nmerged_df_test=df_glove_avg_2_test.merge(test_df['id'],left_index=True, right_index=True)","466c6583":"merged_df_test[merged_df_test[0]==0]","407ba89d":"merged_df_test_2=merged_df_test[~(merged_df_test[0]==0)]","4a9861c3":"pred_X_2=merged_df_test_2.loc[:, merged_df_test_2.columns!='id']\nY_test_pred=DL_model.predict(pred_X_2)","f54530e4":"#Merge on indices, the test ids onto predicted values\nY_test_pred=pd.DataFrame(Y_test_pred)\n#Y_test_pred\npred_test_ids=pd.DataFrame(merged_df_test_2['id'])\npred_test_ids=pred_test_ids.reset_index()\npred_test_ids.pop('index')\n#pred_test_ids\npred_y_df=pd.concat([pred_test_ids, Y_test_pred], axis=1)\nall_test_ids=pd.DataFrame(merged_df_test['id'])\nsubmit_df=all_test_ids.merge(pred_y_df,how='left', on='id')\nsubmit_df.columns=['id','target']","4a006e80":"submit_df_2=submit_df.fillna(submit_df.mean())\nsubmit_df_2","9ddafc54":"target_2=submit_df_2['target'].values.reshape(-1,1)\ntarget_3=trans_y.inverse_transform(target_2)\ntarget_4=pd.Series(target_3.flatten())\n\nsubmit_df_3=pd.concat([submit_df_2, target_4], axis=1)\nsubmit_df_3.pop('target')\nsubmit_df_3.columns=['id','target']","2c3e0573":"submit_df_3.to_csv('submission.csv', index=False)","7784aba7":"### Scale Target Variable to Prevent Vanishing Gradients","801275d2":"### Test Data - Create Sentence Vector for each Description by Averaging the Word Vectors","f2de99fe":"### Split into Train and Test sets","314fa874":"#### Predict value using trained model, then join video_id back onto predicted values.","298a8c8d":"#### Concatenate the Video's Title Before and After the Video Description","4e142d6d":"#### Inverse Transform","1217a689":"# Predict on Training set and save to csv","5e4294e2":"#### Concatenate the Video's Title Before and After the Video Description","a7a3cb58":"### EDA - Word Length and Target Ratio Distribution","8955dfef":"#### If there exists Null Values in Video Description, replace Description field with Video's Title","c7611152":"### Test Data - Creates Data Frame where 1 sentence is 1 Row, with 300 columns","01f026f5":"### Evaluate Test Set","5c17854a":"### Import Deep Learning Libraries","59251de0":"## Compile and Train Neural Network","411e66d7":"### Delete Problematic Rows\n\n#### 6 Problematic Rows - Predicted Target will be Mean of all Predicted Targets","46777cf9":"# Submission Data Set","5f695b2b":"#### In Test Set - 409 out of 5800 videos have Null Descriptions","400287ac":"#### If there exists Null Values in Video Description, replace Description field with Video's Title","536625b4":"# Video Description Word Embeddings and NNs","0dce6dd5":"### delete problematic rows","f46bf856":"## Load Pre-trained Glove and Word2Vec embeddings","a65875e5":"## Create Sentence Vector for each Description by Averaging the Word Vectors","6d8cac47":"#### Fill Nan Values (those video descriptions not found in Fasttext Dictionary) with Mean Prediction","caefcd05":"### Further EDA - Word Length Distribution by Video's Target Ratio ","47c00316":"### Creates Data Frame where 1 sentence is 1 Row, with 300 columns","7d2918c9":"### Want Initial View Count - Only Select Video's with first trending date. ","e2b8dd54":"### Appears as if theres a higher proportion of High View Count Ratio videos the lower the word count\n\n- Filter for those descriptions less than 100 words","0ae39fbe":"## Clean Video Descriptions and Perform EDA","13af0bf4":"#### Create Submission Data Frame","9ddec15d":"### Test Data - Clean Text","8209c959":"### Plot the loss curve"}}