{"cell_type":{"d99e84b6":"code","fc803660":"code","8a95f099":"code","70487e33":"code","2cbdcae5":"code","9cc5ead2":"code","b15e4930":"code","3f59c0bd":"code","32d42cac":"code","093dfc27":"code","1ed08cd0":"code","127b3292":"code","61b9aa96":"markdown"},"source":{"d99e84b6":"# Install W&B for experiment tracking and Visualizations\n!pip install --upgrade -q wandb\n\n# Install pycbc to do easy preprocessing of the data\n# Thanks AlexNitz for intruducing this library\n!pip -q install pycbc","fc803660":"# General imports\nimport os\nimport gc \nimport glob\nimport numpy as np\nimport pandas as pd\nfrom PIL import Image\nimport seaborn as sns\nfrom tqdm import tqdm\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n# Deeplearning import\nimport tensorflow as tf\nprint(f'TensorFlow version: {tf.__version__}')\n\n# PyCBC imports\nimport pylab\nimport pycbc.types\n\n# Multiprocessing \nfrom multiprocessing import Pool\nfrom multiprocessing import cpu_count\n\n# W&B imports\nimport wandb\nfrom wandb.keras import WandbCallback","8a95f099":"TRAIN_PATH = '..\/input\/g2net-gravitational-wave-detection\/train\/'\ntrain_files = glob.glob(TRAIN_PATH+'*\/*\/*\/*')","70487e33":"len(train_files)","2cbdcae5":"train_files[0]","9cc5ead2":"train_df = pd.read_csv('..\/input\/g2net-gravitational-wave-detection\/training_labels.csv')\nprint(f'Size of training_labels.csv: {len(train_df)}')\ntrain_df.head()","b15e4930":"os.makedirs('train_cqt', exist_ok=True)","3f59c0bd":"def get_constant_q_transform(file_names):\n    esp=1e-6\n    normalize=True\n    \n    for file_name in file_names:\n        example_id = file_name.split('\/')[-1].split('.')[0]\n\n        # load the specific 2s sample\n        data = np.load(file_name)\n\n        channels = []\n        for i in range(3):\n            # convert the data to a TimeSeries instance\n            ts = pycbc.types.TimeSeries(data[i, :], epoch=0, delta_t=1.0\/2048) \n\n            # whiten the data (i.e. normalize the noise power at different frequencies)\n            ts = ts.whiten(0.125, 0.125)\n\n            # calculate the qtransform\n            time, freq, power = ts.qtransform(.002, logfsteps=100, qrange=(10, 10), frange=(20, 512))\n\n            # normalize and scale to 0-255\n            if normalize:\n                mean = power.mean()\n                std = power.std()\n\n                power = (power - mean) \/ (std + esp)\n                _min, _max = power.min(), power.max()\n\n                power[power < _min] = _min\n                power[power > _max] = _max\n                power = 255 * (power - _min) \/ (_max - _min)\n                power = power.astype(np.uint8)\n\n            channels.append(np.flip(power, 0))\n\n        vstacked = np.vstack(channels)\n\n        im = Image.fromarray(vstacked)\n        im.save(f\"train_cqt\/{example_id}.png\")","32d42cac":"def chunk(l, n):\n    # loop over the list in n-sized chunks\n    for i in range(0, len(l), n):\n        # yield the current n-sized chunk to the calling function\n        yield l[i: i + n]","093dfc27":"procs = cpu_count()\nprint(procs)\nprocIDs = list(range(0, procs))\n# grab the paths to the input images, then determine the number\n# of images each process will handle\nnumImagesPerProc = len(train_files) \/ float(procs)\nnumImagesPerProc = int(np.ceil(numImagesPerProc))\n# chunk the image paths into N (approximately) equal sets, one\n# set of image paths for each individual process\nchunkedPaths = list(chunk(train_files, numImagesPerProc))","1ed08cd0":"print(\"[INFO] launching pool using {} processes...\".format(procs))\npool = Pool(processes=procs)\npool.map(get_constant_q_transform, chunkedPaths)","127b3292":"# close the pool and wait for all processes to finish\nprint(\"[INFO] waiting for processes to finish...\")\npool.close()\npool.join()\nprint(\"[INFO] multiprocessing complete\")","61b9aa96":"This kernel creates Constant Q Transformed (CQT) image dataset. Thanks to [AlexNitz](https:\/\/www.kaggle.com\/alexnitz) for sharing his [PyCBC: Making Images](https:\/\/www.kaggle.com\/alexnitz\/pycbc-making-images) kernel that helped me to create this dataset. \n\nThe dataset is created by stacking the CQT vertically (`np.vstack`).\n\n### [Find the Dataset here $\\rightarrow$](http:\/\/wandb.me\/g2net-cqt)\n\n### [Visualize data interatively using W&B Tables $\\rightarrow$](http:\/\/wandb.me\/cqt-wandb-table-viz)\n\nLearn more about W&B Tables [here](http:\/\/wandb.me\/better-tables)."}}