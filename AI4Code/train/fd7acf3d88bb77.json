{"cell_type":{"d3691fd4":"code","49961f6f":"code","0ba0a447":"code","dbd8e114":"code","aaff5580":"code","45edcbd4":"code","b0ec71a5":"code","2ea8303f":"code","2e951268":"code","4768aff4":"code","6b2fbee7":"code","9cacfa23":"code","c9d45b53":"code","d87e8899":"code","c22360ef":"code","ece57b28":"code","8190c03a":"code","b5ac07c9":"code","c27cc894":"code","8e5ef880":"code","86a1cbff":"code","1383cdc8":"code","a782c232":"code","c97dc4d9":"code","98a1d04c":"code","1879ec5e":"code","ec3c2372":"code","29956b1f":"markdown","8697d99c":"markdown","bfaed7c1":"markdown"},"source":{"d3691fd4":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set()","49961f6f":"#veri setlerini y\u00fckleme\nx= np.load(\"..\/input\/sign-language-digits-dataset\/X.npy\")\ny= np.load(\"..\/input\/sign-language-digits-dataset\/Y.npy\")","0ba0a447":"#datadan \u00f6rnekler g\u00f6sterme\n\n\nimg_size=64\nplt.subplot(2,2,1)\nplt.imshow(x[260].reshape(img_size, img_size))\nplt.axis(\"off\")\nplt.colorbar()\nplt.subplot(2,2,2)\nplt.imshow(x[1500].reshape(img_size, img_size))\nplt.axis(\"off\")\nplt.colorbar()\nplt.subplot(2,2,3)\nplt.imshow(x[950].reshape(img_size, img_size))\nplt.axis(\"off\")\nplt.colorbar()\nplt.subplot(2,2,4)\nplt.imshow(x[400].reshape(img_size, img_size))\nplt.axis(\"off\")\nplt.colorbar()","dbd8e114":"X = np.concatenate((x[204:409], x[822:1027] ), axis=0)\nz=np.zeros(205)\no=np.ones(205)\nY=np.concatenate((z,o), axis=0).reshape(X.shape[0],1)\nprint(\"X shape: \" , X.shape)\nprint(\"Y shape: \" , Y.shape)","aaff5580":"from sklearn.model_selection import train_test_split\nX_train, X_test, Y_train, Y_test= train_test_split(X,Y, test_size=0.15, random_state=42)\nnumber_of_train=X_train.shape[0]\nnumber_of_test=X_test.shape[0]\n\n\nprint(number_of_train)\nprint(number_of_test)","45edcbd4":"X_train_flatten = X_train.reshape(number_of_train,X_train.shape[1]*X_train.shape[2])\nX_test_flatten = X_test.reshape(number_of_test,X_test.shape[1]*X_test.shape[2])\nprint(\"X train flatten\",X_train_flatten.shape)\nprint(\"X test flatten\",X_test_flatten.shape)","b0ec71a5":"x_train= X_train_flatten.T\nx_test=X_test_flatten.T\ny_train=Y_train.T\ny_test=Y_test.T\n\nprint(\"x train: \",x_train.shape)\nprint(\"x test: \",x_test.shape)\nprint(\"y train: \",y_train.shape)\nprint(\"y test: \",y_test.shape)","2ea8303f":"#initializing parameters\n\ndef initialize_weights_and_bias (dimension):\n    w= np.full((dimension,1),0.01)  #np.full --> Verilen \u015fekil ve t\u00fcrde fill_value (0.01) ile doldurulmu\u015f yeni bir dizi d\u00f6nd\u00fcr\u00fcr. yani pixel say\u0131s\u0131 kadar 0.01 de\u011ferli array olu\u015ftur\n    b=0.0\n    return w, b\n","2e951268":"#Sigmoid Function\n# z= np.dot(w.T, x_train)+b\n\ndef sigmoid(z):\n    y_head=1\/(1+np.exp(-z))\n    \n    return y_head   # tahmin edilen de\u011fer  ----- y_head= sigmoid(z)","4768aff4":"# Forward Propagation\n# find z = w.Tx+ b\n# y_head= sigmodi(z)\n# loss(error)= loss( y,y_head)\n# cost= sum(loss)\n\ndef forward_propagation(w,b,x_train,y_train):\n    z= np.dot(w.T, x_train)+b\n    y_head= sigmoid(z)   #propabilistiv 0-1 aras\u0131nda\n    loss= -y_tain*np.log(y_head)-(1-y_train)*np.log(1-y_head)  #loss function\n    cost= (np.sum(loss))\/x_train.shape[1]   #x_train.shape[1] ----x_traindeki \u00f6rnek say\u0131s\u0131\n    return cost","6b2fbee7":"# forward and Backward Propagation\n\ndef forward_backward_propagation(w,b,x_train,y_train):\n    # Forward Propagation\n    z= np.dot(w.T, x_train)+b\n    y_head= sigmoid(z)   #propabilistiv 0-1 aras\u0131nda\n    loss= -y_train*np.log(y_head)-(1-y_train)*np.log(1-y_head)  #loss function\n    cost= (np.sum(loss))\/x_train.shape[1]   #x_train.shape[1] ----x_traindeki \u00f6rnek say\u0131s\u0131\n    \n    # Backward Propagation\n    \n    derivative_weight= (np.dot(x_train,((y_head-y_train).T)))\/x_train.shape[1]  # x_trian.sahape[1] is scaling\n    derivative_bias= (np.sum(y_head-y_train))\/ x_train.shape[1]  # x_trian.sahape[1] is scaling\n    gradients= {\"derivative_weight\": derivative_weight, \"derivative_bias\": derivative_bias}\n    \n    return cost, gradients\n","9cacfa23":"# Uptading (Learning) Paramaters\n\ndef update( w, b, x_train, y_train, learning_rate, number_of_iteration):\n    cost_list  = []\n    cost_list2 = []\n    index = []\n    \n    for i in range( number_of_iteration):\n        #cost ve gradientsleri bul\n        cost, gradients = forward_backward_propagation(w,b,x_train, y_train)\n        cost_list.append(cost)\n        \n        #g\u00fcncelle (uptade)\n        w= w-learning_rate*gradients[\"derivative_weight\"]\n        \n        b= b-learning_rate*gradients[\"derivative_bias\"]\n        \n        if i % 10 ==0:    # 10 iterasyonda 1 costlar\u0131 yazd\u0131r.\n            cost_list2.append(cost)\n            index.append(i)\n            print(\"Cost after iteration %i: %f\" % (i, cost))\n            \n    #weight ve biaslar\u0131 g\u00fcncelle\n    \n    parameters= {\"weight\":w, \"bias\":b}\n    plt.plot(index, cost_list2)\n    plt.xticks(index,rotation=\"vertical\")\n    plt.xlabel(\"Number of iteration\")\n    plt.ylabel(\"Cost\")\n    plt.show()\n    return parameters,gradients, cost_list","c9d45b53":"#prediction\n\ndef predict (w,b,x_test):\n    y_head= sigmoid(np.dot(w.T, x_test)+b)\n    Y_prediction=np.zeros((1,x_test.shape[1]))\n    \n    for i in range(y_head.shape[1]):\n        if y_head[0,i]<=0.5:\n            Y_prediction[0,i]= 0\n        else:\n            Y_prediction[0,i]= 1\n\n    return Y_prediction","d87e8899":"#Logistic Regression\n\ndef logistic_regression(x_train, y_train, x_test, y_test, learning_rate, num_iterations):\n    dimension= x_train.shape[0] #pixel say\u0131s\u0131\n    w,b= initialize_weights_and_bias (dimension)\n    \n    parameters,gradients, cost_list= update(w,b, x_train, y_train, learning_rate, num_iterations)\n    \n    y_prediction_test= predict(parameters[\"weight\"], parameters[\"bias\"], x_test)\n    y_prediction_train= predict(parameters[\"weight\"], parameters[\"bias\"], x_train)\n    \n    print(\"train accuracy: {} %\".format(100-np.mean(np.abs(y_prediction_train- y_train))*100))  #np.abs-- mutlak de\u011fer\n    print(\"test accuracy: {} %\".format(100-np.mean(np.abs(y_prediction_test- y_test))*100))\n    \nlogistic_regression(x_train, y_train, x_test, y_test, learning_rate=0.01, num_iterations=150)","c22360ef":"from sklearn import linear_model\nlogreg = linear_model.LogisticRegression(random_state = 42,max_iter= 150, solver='liblinear' )  #'str' object has no attribute 'decode' hatas\u0131n\u0131 almamak i\u00e7in \"Solver\"\nprint(\"test accuracy: {} \".format(logreg.fit(x_train.T, y_train.T).score(x_test.T, y_test.T)))\nprint(\"train accuracy: {} \".format(logreg.fit(x_train.T, y_train.T).score(x_train.T, y_train.T)))\n","ece57b28":"#intialize parameters and layer size\n\ndef initialize_parameters_and_layer_sizes_NN(x_train,y_train):\n    parameters= {\"weight1\": np.random.randn(3, x_train.shape[0]) * 0.1,   #gizli katmanda 3 tane node oldu\u011fu i\u00e7in (3,4096)\n                 \"bias1\": np.zeros((3,1)),\n                 \"weight2\": np.random.randn(y_train.shape[0],3) * 0.1,    # (1,3)\n                 \"bias2\": np.zeros((y_train.shape[0],1))\n                }\n    return parameters","8190c03a":"# Forward Progation\n\ndef forward_propagation_NN(x_train, parameters):\n    Z1= np.dot(parameters[\"weight1\"],x_train)+ parameters[\"bias1\"]\n    A1= np.tanh(Z1)\n    Z2= np.dot(parameters[\"weight2\"],A1)+ parameters[\"bias2\"]\n    A2= sigmoid(Z2)\n    \n    cache={\"Z1\":Z1,\n           \"A1\":A1,\n           \"Z2\":Z2,\n           \"A2\":A2}\n    \n    return A2, cache","b5ac07c9":"# Cost function\n\ndef compute_cost_NN(A2, Y, parameters):   \n    logprobs= np.multiply(np.log(A2),Y)     # -Toplam(yi*ln(y'i))\n    cost= -np.sum(logprobs)\/Y.shape[1]\n    return cost","c27cc894":"# Backward propagation\n\ndef backward_propagation_NN(parameters, cache, X,Y):\n    dZ2= cache[\"A2\"]-Y\n    dW2= np.dot(dZ2, cache[\"A1\"].T)\/X.shape[1]\n    db2= np.sum( dZ2, axis=1, keepdims=True)\/X.shape[1]\n    dZ1= np.dot(parameters[\"weight2\"].T, dZ2)*(1 - np.power(cache[\"A1\"],2))\n    dW1= np.dot(dZ1,X.T)\/X.shape[1]\n    db1= np.sum(dZ1, axis = 1, keepdims=True)\/X.shape[1]\n    grads = {\"dweight1\": dW1,\n             \"dbias1\":db1,\n             \"dweight2\": dW2,\n             \"dbias2\": db2\n            }\n    \n    return grads","8e5ef880":"#uptade parameters\n\ndef uptade_parameters_NN(parameters, grads, learning_rate):\n    \n    parameters={\"weight1\": parameters[\"weight1\"]-learning_rate*grads[\"dweight1\"],\n                \"bias1\": parameters[\"bias1\"]-learning_rate*grads[\"dbias1\"],\n                \"weight2\": parameters[\"weight2\"]- learning_rate*grads[\"dweight2\"],\n                \"bias2\": parameters[\"bias2\"]- learning_rate*grads[\"dbias2\"]\n                \n    }\n    return parameters\n\n","86a1cbff":"#prediction\n\ndef predict_NN(parameters, x_test):\n    A2, cache = forward_propagation_NN(x_test,parameters)\n    Y_prediction= np.zeros((1, x_test.shape[1]))\n    \n    for i in range (A2.shape[1]):\n        if A2[0,i]<= 0.5:\n            Y_prediction[0,i]= 0\n        else:\n            Y_prediction[0,i]=1\n    return Y_prediction\n","1383cdc8":"# ANN Model\n\ndef two_layer_neural_network(x_train, y_train, x_test, y_test, num_iteration):\n    cost_list=[]\n    index_list=[]\n    \n    parameters= initialize_parameters_and_layer_sizes_NN(x_train,y_train)  #ba\u015flang\u0131\u00e7 parametreleri\n    \n    for i in range(0, num_iteration):\n        \n        A2, cache= forward_propagation_NN(x_train, parameters)  # ileri besleme\n        \n        cost= compute_cost_NN(A2, y_train, parameters)          # cost function\n        \n        grads= backward_propagation_NN(parameters, cache, x_train, y_train)  # geri yay\u0131l\u0131m \n        \n        parameters= uptade_parameters_NN(parameters, grads,learning_rate=0.01 )  #parametre g\u00fcncelle\n\n        \n        if i % 100==0:\n            cost_list.append(cost)  # her y\u00fcz tekrarda listeye cost functionu ekle\n            index_list.append(i)\n            \n            print(\"Cost after iteration %i: %f\" %(i,cost))\n            \n        \n            \n    plt.plot(index_list, cost_list)\n    plt.xticks(index_list, rotation=\"vertical\")\n    plt.xlabel(\"Number of Iteration\")\n    plt.ylabel(\"Cost\")\n    plt.show()\n    \n\n        \n    #predict\n    y_prediction_test = predict_NN(parameters, x_test)\n    y_prediction_train= predict_NN(parameters, x_train)\n        \n    #print train\/test Errors\n        \n    print(\"train accuracy:{} %\".format(100- np.mean(np.abs(y_prediction_train- y_train)* 100)))  #np.abs - mutlak de\u011fer\n    print(\"test accuracy:{} %\".format(100-np.mean(np.abs(y_prediction_test- y_test)* 100)))  # np.mean- aritmetik ortalama\n        \n    return parameters\n    \nparameters= two_layer_neural_network(x_train, y_train, x_test, y_test, num_iteration=2500)","a782c232":"#deneme\nnum_iteration=10000\ncost_list=[]\nindex_list=[]\n    \nparameters= initialize_parameters_and_layer_sizes_NN(x_train,y_train)  #ba\u015flang\u0131\u00e7 parametreleri\n    \nfor i in range(0, num_iteration):\n        \n    A2, cache= forward_propagation_NN(x_train, parameters)  # ileri besleme\n        \n    cost= compute_cost_NN(A2, y_train, parameters)          # cost function\n        \n    grads= backward_propagation_NN(parameters, cache, x_train, y_train)  # geri yay\u0131l\u0131m \n        \n    parameters= uptade_parameters_NN(parameters, grads,learning_rate=0.01)  #parametre g\u00fcncelle\n\n        \n    if i % 500==0:\n        cost_list.append(cost)  # her y\u00fcz tekrarda listeye cost functionu ekle\n        index_list.append(i)                \n  \n       \n#predict\ny_prediction_test = predict_NN(parameters, x_test)\ny_prediction_train= predict_NN(parameters, x_train)\n        \n#print train\/test Errors\n        \nprint(\"train accuracy:{} %\".format(100- np.mean(np.abs(y_prediction_train- y_train)* 100)))  #np.abs - mutlak de\u011fer\nprint(\"test accuracy:{} %\".format(100-np.mean(np.abs(y_prediction_test- y_test)* 100)))  # np.mean- aritmetik ortalama","c97dc4d9":"#deneme\nnum_iteration=10000\ncost_list2=[]\nindex_list=[]\n    \nparameters= initialize_parameters_and_layer_sizes_NN(x_train,y_train)  #ba\u015flang\u0131\u00e7 parametreleri\n    \nfor i in range(0, num_iteration):\n        \n    A2, cache= forward_propagation_NN(x_train, parameters)  # ileri besleme\n        \n    cost= compute_cost_NN(A2, y_train, parameters)          # cost function\n        \n    grads= backward_propagation_NN(parameters, cache, x_train, y_train)  # geri yay\u0131l\u0131m \n        \n    parameters= uptade_parameters_NN(parameters, grads,learning_rate=0.02)  #parametre g\u00fcncelle\n\n        \n    if i % 500==0:\n        cost_list2.append(cost)  # her y\u00fcz tekrarda listeye cost functionu ekle\n        index_list.append(i)                \n  \n       \n#predict\ny_prediction_test = predict_NN(parameters, x_test)\ny_prediction_train= predict_NN(parameters, x_train)\n        \n#print train\/test Errors\n        \nprint(\"train accuracy:{} %\".format(100- np.mean(np.abs(y_prediction_train- y_train)* 100)))  #np.abs - mutlak de\u011fer\nprint(\"test accuracy:{} %\".format(100-np.mean(np.abs(y_prediction_test- y_test)* 100)))  # np.mean- aritmetik ortalama","98a1d04c":"#deneme\nplt.plot(index_list, cost_list, c=\"green\") #0.01\nplt.plot(index_list, cost_list2) ##0.02\nplt.legend()\nplt.xticks(index_list, rotation=\"vertical\")\nplt.xlabel(\"Number of Iteration\")\nplt.ylabel(\"Cost\")\nplt.show()","1879ec5e":"#reshaping\n\nx_train, x_test, y_train, y_test = x_train.T, x_test.T, y_train.T, y_test.T","ec3c2372":"# Evaluating the ANN\n\nfrom tensorflow.keras.wrappers.scikit_learn import KerasClassifier\nfrom sklearn.model_selection import cross_val_score\nfrom tensorflow.keras.models import Sequential # initialize neural network library\nfrom tensorflow.keras.layers import Dense # build our layers library\ndef build_classifier():\n\n    classifier = Sequential() # initialize neural network\n    classifier.add(Dense(units = 8, kernel_initializer = 'uniform', activation = 'relu', input_dim = x_train.shape[1]))\n    classifier.add(Dense(units = 4, kernel_initializer = 'uniform', activation = 'relu'))\n    classifier.add(Dense(units = 1, kernel_initializer = 'uniform', activation = 'sigmoid'))\n    classifier.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n    return classifier\nclassifier = KerasClassifier(build_fn = build_classifier, epochs = 100)\naccuracies = cross_val_score(estimator = classifier, X = x_train, y = y_train, cv = 3)\nmean = accuracies.mean()\nvariance = accuracies.std()\nprint(\"Accuracy mean: \"+ str(mean))\nprint(\"Accuracy variance: \"+ str(variance))","29956b1f":"# ARTIFICIAL NEURAL NETWORK\n","8697d99c":"# L-Layer Neural Network\n\n**Keras ile ANN uygulamas\u0131**","bfaed7c1":"iki farkl\u0131 \u00f6\u011frenme oran\u0131 ile grafileri \u00e7izdirme"}}