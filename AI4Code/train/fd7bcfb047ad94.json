{"cell_type":{"f76cb7ef":"code","f2092dca":"code","17759bf6":"code","d9f178f1":"code","997cdca9":"code","f3cbf4b2":"code","8e535c12":"code","e4a792b2":"code","2e9ec471":"code","a32925d1":"code","efd42073":"code","b68615ca":"code","662f652c":"code","22e8c53c":"code","e4f1d4ab":"code","1dc7255d":"code","8ec29d99":"code","278a027b":"code","eb6581a8":"code","a4b04a6f":"code","5dc2215e":"code","086fc760":"code","2c70002a":"code","fc7ac073":"code","0ee5f15f":"code","26259916":"code","9e217d7e":"code","65cb0ff8":"code","0a68f9fc":"code","7f1df073":"code","0f0acd0a":"code","ace2e68c":"code","9ce438c5":"code","8de60389":"code","51ad0265":"code","5ae113b5":"code","0539ae38":"code","14a3ec18":"code","54013780":"code","febaa1e0":"code","8778fdd9":"code","a7eab5be":"code","2a6cd98d":"code","f4f3c1b4":"markdown","e33daaae":"markdown","6ac23853":"markdown","28521b42":"markdown","db4a60d6":"markdown","d038810f":"markdown","1e0605b5":"markdown","b7cf2982":"markdown","52c4c537":"markdown","72826237":"markdown","5c499d23":"markdown","bd427e82":"markdown","7a7d4e29":"markdown","1732beb6":"markdown","ea09f8eb":"markdown","36b9131c":"markdown","3443456e":"markdown","5b4beb6e":"markdown","b8803214":"markdown","c186e82d":"markdown","03463d55":"markdown","5a74e10a":"markdown"},"source":{"f76cb7ef":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\nfrom __future__ import unicode_literals\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport re\nfrom collections import Counter\nfrom tqdm import tqdm\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport nltk\n# NLTK Stop words\nnltk.download('stopwords')\nfrom nltk.corpus import stopwords\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer\n\nsid = SentimentIntensityAnalyzer()\n\nstop_words = stopwords.words('english')\nstop_words.extend(['from', 'subject', 're', 'edu', 'use'])\n\n\n# Gensim\nimport gensim\nimport gensim.corpora as corpora\nfrom gensim.utils import simple_preprocess\nfrom gensim.models import CoherenceModel, HdpModel, LdaModel\n\n\nimport spacy\ntry:\n    from spacymoji import Emoji\nexcept:\n    !pip install spacymoji\n    from spacymoji import Emoji\n\nnlp = spacy.load(\"en_core_web_sm\", disable=['parser'])\nemoji = Emoji(nlp, merge_spans=False)\nnlp.add_pipe(emoji, first=True)\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        \n\n#pd.options.plotting.backend = \"plotly\"\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n# Plotting tools\nimport pyLDAvis\nimport pyLDAvis.gensim\npyLDAvis.enable_notebook()\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","f2092dca":"## Time Features\n\ndef get_timefeatures(df: pd.DataFrame, col:str):\n    from pandas.api.types import is_datetime64_ns_dtype as is_datetime\n    from pandas.api.types import is_float_dtype as is_float\n    \n    if is_datetime(df[col]):\n        df[col+'_date'] = df[col].dt.date\n        df[col+'_doy'] = df[col].dt.day_of_year\n        df[col+'_dow'] = df[col].dt.day_of_week\n        df[col+'_time'] = df[col].dt.time\n        df[col+'_hour'] = df[col].dt.hour\n        df[col+'_week'] = df[col].dt.week\n        df[col+'_day'] = df[col+'_doy'] - df[col+'_doy'].min()\n        df[col+'_floorT'] = df[col].dt.floor('Min')\n        df[col+'_floorH'] = df[col].dt.floor('H')\n        return [col,col+'_date',col+'_doy', col+'_dow',\n                col+'_time',col+'_hour',col+'_week',col+'_day', col+'_floorT', col+'_floorH']\n    elif is_float(df[col]):\n        df[col+'_age'] = df[col].max() - df[col]\n        return [col, col+'_age']\n    \n## Feature Engineering\n\ndef remove_outliers(df: pd.DataFrame):\n    try:\n        return df[df.timestamp_doy<273]\n    except:\n        print('It is not possible to filter on day of year column.')\n        return df\ndef VaderSentiment(text: str):\n        d = sid.polarity_scores(text)\n        return d['pos'],d['neu'],d['neg'],d['compound']\ndef NumbersFromText(txt: str):\n    try:\n        doc = nlp(txt)\n        words = [token.text for token in doc if token.is_alpha]\n        emojis = [token.text for token in doc if token._.is_emoji]\n        return len(words), len(emojis)\n    except:\n        return 0,0\n\ndef get_TextFeatures(df: pd.DataFrame):\n    df['full_text'] = df['title'] + ' ' + df['body'].fillna('')\n    df['text_lenght'] = df['full_text'].apply(lambda x: len(x))\n    p = df.full_text.transform(VaderSentiment).to_list()\n    df[['VS_pos','VS_neu','VS_neg','compound']] = pd.DataFrame(p, columns=['VS_pos','VS_neu','VS_neg','compound'])\n    df['sentiment'] = df['compound'].apply(lambda c: 'pos' if c>0.1 else 'neg' if c<-0.1 else 'neu')\n    df[['num_of_words','num_of_emojis']] = pd.DataFrame(df.title.apply(lambda x: NumbersFromText(x)).to_list(), columns=['num_of_words','num_of_emojis'])\n\ndef extract_emoji(sent):\n    doc = nlp(sent)\n    res = ' '.join([token.text for token in doc if token._.is_emoji])\n    return res.strip()\n\n## Text Preprocessing\n\ndef remove_url(text):\n    return re.sub(r'^https?:\\\/\\\/.*[\\r\\n]*', '', text, flags=re.MULTILINE)\ndef remove_newline(text):\n    return re.sub('\\s+', ' ', text)\ndef remove_squote(text):\n    return re.sub(\"\\'\", \"\", text)\n\ndef sent_to_words(texts):\n    texts = [remove_url(text) for text in texts]\n    texts = [remove_newline(text) for text in texts]\n    texts = [remove_squote(text) for text in texts]\n    for sent in texts:\n        yield([token.lemma_.lower() for token in nlp(sent) if (token._.is_emoji)|(token.is_alpha)|(token.is_digit)])\n#    for sent in texts:\n#        yield(gensim.utils.simple_preprocess(str(sent), deacc=True))\n\ndef remove_stopwords(texts):\n    return [[word for word in text if word not in stop_words] for text in texts]\n#[[word for word in doc if word not in stop_words] for doc in texts]\n\ndef make_bigrams(texts):\n    return [bigram_mod[doc] for doc in texts]\n\ndef make_trigrams(texts):\n    return [trigram_mod[bigram_mod[doc]] for doc in texts]\n\ndef lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV','PROPN'], is_emoji=True):\n    \"\"\"https:\/\/spacy.io\/api\/annotation\"\"\"\n    texts_out = []\n    item = []\n    for sent in texts:\n        doc = nlp(\" \".join(sent)) \n        texts_out.append([token.lemma_ for token in doc if (token.pos_ in allowed_postags)|(is_emoji&token._.is_emoji)])\n    return texts_out\n\n# equivalent to build_corpus but optimized with spacy\ndef preprocess(texts: pd.Series, allowed_postags:list):\n    docs = list(nlp.pipe(texts))\n    data_words = [[token.lemma_.lower() for token in doc if (\n        (token._.is_emoji)|(token.is_alpha)|(token.is_digit)|(token.is_stop==False)\n    )&(token.pos_ in allowed_postags)]for doc in docs]\n    #data_words = remove_stopwords(data_words)\n    return data_words\n\ndef build_corpus(texts:pd.Series, dictionary, allowed_postags = ['NOUN', 'ADJ', 'VERB', 'ADV','PROPN']):\n    docs = list(nlp.pipe(texts))\n    data_words = [[token.lemma_.lower() for token in doc if (\n        (token._.is_emoji)|(token.is_alpha)|(token.is_digit)|(token.is_stop==False)\n    )&(token.pos_ in allowed_postags)]for doc in docs]\n    \n    data_words = make_bigrams(data_words)\n    corpus = [id2word.doc2bow(text) for text in data_words]\n    return corpus\n\n### other utils\n\n\n# deprecated thanks to MultiLabelBinarizer\n\ndef in_set(stock:object, set_stock:set):\n    if type(stock)==list:\n        v = []\n        for s in stock:\n            if s in set_stock:\n                v.append(1)\n            else:\n                v.append(0)\n        return v\n    elif type(stock)==str:\n        if stock in set_stock:\n            return 1\n        else:\n            return 0","17759bf6":"def compute_coherence_values(dictionary, corpus, texts, limit, start=2, step=3):\n    \"\"\"\n    Compute c_v coherence for various number of topics\n\n    Parameters:\n    ----------\n    dictionary : Gensim dictionary\n    corpus : Gensim corpus\n    texts : List of input texts\n    limit : Max num of topics\n\n    Returns:\n    -------\n    model_list : List of LDA topic models\n    coherence_values : Coherence values corresponding to the LDA model with respective number of topics\n    \"\"\"\n    coherence_values = []\n    model_list = []\n    for num_topics in range(start, limit, step):\n        model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n                                           id2word=id2word,\n                                           num_topics=num_topics,\n                                               alpha='auto',\n                                               eta='auto')#learn asymmetric priors\n        model_list.append(model)\n        coherencemodel = CoherenceModel(model=model, texts=texts, dictionary=dictionary, coherence='c_v')\n        coherence_values.append(coherencemodel.get_coherence())\n\n    return model_list, coherence_values","d9f178f1":"path = '\/kaggle\/input\/reddit-wallstreetsbets-posts\/reddit_wsb.csv'\ndf = pd.read_csv(path, parse_dates = ['timestamp'], infer_datetime_format=True)\nprint(df.info())","997cdca9":"import gc\n\ngc.collect()\n\nget_TextFeatures(df)\n\ntimefeatures = []\n\ntimefeatures.extend(get_timefeatures(df,'timestamp'))\n\nkeyfeatures=['timestamp_day','timestamp','full_text','url','score','comms_num']","f3cbf4b2":"df.head(10)","8e535c12":"df.describe(include='all')","e4a792b2":"df[timefeatures].describe(include='all')","2e9ec471":"a = df[df.timestamp_doy==273][['full_text','timestamp','url']].values[0]\nprint('Posted at ',a[1])\n\nprint(a[0])\n\nprint('URL: ', a[2])","a32925d1":"df = remove_outliers(df)\n\ntimefeatures.extend(get_timefeatures(df,'created'))\nprint('Records in reddit_wsb goes from ',df.timestamp.min(),' to ',df.timestamp.max())\nprint('There are ',df.shape[0],'records.')","efd42073":"fig,ax = plt.subplots(nrows=1, ncols=3, figsize=(20,5))\nax[0].set_yscale('log')\nax[1].set_yscale('log')\nax[2].set_yscale('log')\ndf.hist(column=['score'], bins = 25, ax = ax[0])\ndf.hist(column=['comms_num'], bins = 25, ax = ax[1])\ndf.hist(column=['text_lenght'], bins = 25, ax = ax[2])","b68615ca":"nrows = 3\nncols = 2\n\nfig,ax = plt.subplots(nrows = nrows, ncols = ncols, figsize = (30,10))\nsns.boxplot(data = df, x='timestamp_doy', y='score', ax = ax[0,0])\nsns.boxplot(data = df, x='timestamp_hour', y='score', ax = ax[0,1])\nsns.boxplot(data = df, x='timestamp_doy', y='comms_num', ax = ax[1,0])\nsns.boxplot(data = df, x='timestamp_hour', y='comms_num', ax = ax[1,1])\nsns.boxplot(data = df, x='timestamp_doy', y='text_lenght', ax = ax[2,0])\nsns.boxplot(data = df, x='timestamp_hour', y='text_lenght', ax = ax[2,1])\n\nfor i in range(nrows):\n    for j in range(ncols):\n        ax[i,j].set_yscale('log')\n\nfig.tight_layout()","662f652c":"df.sort_values(by='score', ascending = False).head(5)[keyfeatures]","22e8c53c":"df.sort_values(by='comms_num', ascending = False).head(5)[keyfeatures]","e4f1d4ab":"df_series = df.groupby(['timestamp_doy','sentiment']).agg(\n    {'id':'count','score':'median','comms_num':'median','created_age':'median','VS_pos':'median','VS_neg':'median','compound':'median'}\n).reset_index()\ndf_series=df_series[df_series.sentiment!='neu']\n\n\nnrows = 2\nncols = 3\n\nfig,ax = plt.subplots(nrows = nrows, ncols = ncols, figsize = (20,8))\n\nsns.lineplot(data = df_series,x='timestamp_doy',y='id',hue='sentiment', ax = ax[0,0])\nsns.lineplot(data = df_series,x='timestamp_doy',y='score',hue='sentiment', ax = ax[0,1])\nsns.lineplot(data = df_series,x='timestamp_doy',y='comms_num',hue='sentiment',  ax = ax[0,2])\nsns.lineplot(data = df_series,x='timestamp_doy',y='VS_pos',hue='sentiment',  ax = ax[1,0])\nsns.lineplot(data = df_series,x='timestamp_doy',y='VS_neg',hue='sentiment',  ax = ax[1,1])\nsns.lineplot(data = df_series,x='timestamp_doy',y='compound',hue='sentiment',  ax = ax[1,2])\n\nfor i in range(ncols):\n    for j in range(nrows):\n        for label in ax[j,i].get_xticklabels():\n            label.set_rotation(45)\nfig.tight_layout()","1dc7255d":"%%time\n\ndef is_stock(token):\n    return (token.is_lower|token.is_upper)&(token.pos_=='PROPN')&(\n    1<len(token.text)<=4)&(token._.is_emoji==False)&(token.ent_type_ in ['','ORG','PERSON'])&(token.is_stop==False)\n\ndocs = list(nlp.pipe(df.full_text))\nstock_docs = [[token.text for token in doc if is_stock(token)] for doc in tqdm(docs)]\ndel docs","8ec29d99":"n_head= 20\nstock_count = pd.DataFrame.from_dict(\n    dict(Counter(\n        [stock for stock_list in map(set, stock_docs) for stock in stock_list]\n    )), orient='index'\n).reset_index()\nstock_count.rename(columns={'index':'stock',0:'count'},inplace=True)\nstock_count = stock_count[stock_count['count']>10]\n\n#fig,ax=plt.subplots()\nstock_count.sort_values(by='count',ascending=False)[:n_head].plot.bar(x='stock',y='count',rot=0, figsize=(15,6))","278a027b":"def filter_by_list(stdocs:list, sl:list):\n    return [[stock.upper() for stock in los if stock in sl] for los in stdocs]\n\nstock_list = stock_count[stock_count['count']>30].stock.to_list()\n\ndf['stocks'] = list(map(set,filter_by_list(stock_docs, stock_list)))","eb6581a8":"from sklearn.preprocessing import MultiLabelBinarizer\n\ngc.collect()\n\nstock_trend = df['stocks'].copy()\n\nmlb = MultiLabelBinarizer()\nmatrix = mlb.fit_transform(stock_trend)\nstock_list = mlb.classes_\nstock_trend = pd.DataFrame(matrix, columns=mlb.classes_)\nstock_trend = pd.concat([stock_trend, df[['timestamp','created','id','score','comms_num','compound']]],axis = 1).dropna()\n\ndel mlb, matrix\n\n\nstocks_dict = {'score':'sum','compound':'median','comms_num':'sum','flg':'sum'}\n\nget_timefeatures(stock_trend,'timestamp')\n\nstock_trend = stock_trend.melt(id_vars=['timestamp','id','score','compound',\n                                        'comms_num','timestamp_date','timestamp_doy'],\n                 value_vars=stock_list,\n                 var_name='stock',\n                 value_name='flg'\n                )\n\nfalse_stock = ['\/U','HOLD','WSB','MOON','BUY','HOLD']\n\nstock_trend = stock_trend[(stock_trend.flg>0)&(stock_trend.stock.isin(false_stock)==False)]\nstock_trend = stock_trend.groupby(['timestamp_date','timestamp_doy','stock']).agg(stocks_dict).reset_index()","a4b04a6f":"import plotly.express as px\n\nfig = px.line(stock_trend, x='timestamp_date',y='flg',\n              color='stock', hover_name='stock'\n             )\nfig.update_xaxes(\n        tickangle = 45,\n        title_text = \"Date\")\nfig.update_yaxes(\n        title_text = \"Number of Posts\")\nfig.show()\n\n#stock_trend.plot.line(x='timestamp_date',y='flg', color='stock', rot=45, figsize=(15,6))","5dc2215e":"import plotly.express as px\n\nfig = px.line(stock_trend, x='timestamp_date',y='score',\n              color='stock', hover_name='stock'\n             )\nfig.update_xaxes(\n        tickangle = 45,\n        title_text = \"Date\")\nfig.update_yaxes(\n        title_text = \"Score Level\")\nfig.show()\n\n","086fc760":"import plotly.express as px\n\nfig = px.line(stock_trend, x='timestamp_date',y='comms_num',\n              color='stock', hover_name='stock'\n             )\nfig.update_xaxes(\n        tickangle = 45,\n        title_text = \"Date\")\nfig.update_yaxes(\n        title_text = \"Overall Number of Comments\")\nfig.show()\n\n","2c70002a":"df['emojis'] = df.title.transform(extract_emoji)\nc = Counter([emoji for sent in df.emojis.to_list() for emoji in set(sent) if emoji!=' '])\n#most_common_emojis","fc7ac073":"most_common_emojis = pd.DataFrame.from_dict(\n    dict(c.most_common()), orient='index', columns=['count']\n)\nmost_common_emojis = most_common_emojis[most_common_emojis['count']>1].index[:7].to_list()\nmost_common_emojis","0ee5f15f":"def emoji_mood(emojis: str):\n    x = Counter([emoji for emoji in emojis.replace(' ','') if emoji in most_common_emojis]).most_common(1)\n    if len(x)>=1:\n        return x[0][0]\n    else:\n        return ''\n\n\ndataset = df[['timestamp','title','emojis','sentiment','comms_num','compound']][df['emojis']!='']\ndataset['emoji_mood'] = dataset.emojis.apply(emoji_mood)\ndataset = dataset[dataset['emoji_mood']!='']\n_ = get_timefeatures(dataset,'timestamp')\n#dataset.head(5)","26259916":"lda_model = LdaModel.load('..\/input\/yolo-topic-modeling\/Models\/lda_model')\nid2word = gensim.corpora.Dictionary.load('..\/input\/yolo-topic-modeling\/Dictionary\/dictionary')\nbigram = gensim.models.Phrases.load('..\/input\/yolo-topic-modeling\/bigram')\nbigram_mod = gensim.models.phrases.Phraser.load('..\/input\/yolo-topic-modeling\/Models\/bigram_mod')","9e217d7e":"%%time\ncorpus = build_corpus(df.full_text, id2word)","65cb0ff8":"%%time\ndef format_topics_sentences(texts,ldamodel=lda_model, corpus=corpus):\n    # Init output\n    sent_topics_df = pd.DataFrame()\n\n    # Get main topic in each document\n    for i, row in enumerate(ldamodel[corpus]):\n        row = sorted(row, key=lambda x: x[1], reverse=True)\n        # Get the Dominant topic, Perc Contribution and Keywords for each document\n        for j, (topic_num, prop_topic) in enumerate(row):\n            if j == 0:  # => dominant topic\n                wp = ldamodel.show_topic(topic_num, topn=7)\n                topic_keywords = \", \".join([word for word, prop in wp])\n                sent_topics_df = sent_topics_df.append(pd.Series([int(topic_num), round(prop_topic,4), topic_keywords]), ignore_index=True)\n            else:\n                break\n    sent_topics_df.columns = ['Dominant_Topic', 'Perc_Contribution', 'Topic_Keywords']\n\n    # Add original text to the end of the output\n    contents = pd.Series(texts)\n    sent_topics_df = pd.concat([sent_topics_df, contents], axis=1)\n    return(sent_topics_df)\n\n\ndf_topic_sents_keywords = format_topics_sentences(\n    texts=df.full_text,\n    ldamodel=lda_model, \n    corpus=corpus)\n\n# Format\ndf_dominant_topic = df_topic_sents_keywords.reset_index()\ndf_dominant_topic.columns = ['Document_No', 'Dominant_Topic', 'Topic_Perc_Contrib', 'Keywords', 'Text']\n\n# Show\ndf_dominant_topic.head(10)","0a68f9fc":"%%time\n# Group top 5 sentences under each topic\nsent_topics_sorteddf = pd.DataFrame()\n\nsent_topics_outdf_grpd = df_topic_sents_keywords.groupby('Dominant_Topic')\n\nfor i, grp in sent_topics_outdf_grpd:\n    sent_topics_sorteddf = pd.concat([sent_topics_sorteddf,\n                                      grp.sort_values(['Perc_Contribution'], \n                                                      ascending=[0]).head(1)], \n                                            axis=0)\n\n# Reset Index    \nsent_topics_sorteddf.reset_index(drop=True, inplace=True)\n\n# Format\nsent_topics_sorteddf.columns = ['Topic_Num', \"Topic_Perc_Contrib\", \"Keywords\", \"Text\"]\n\n# Show\nsent_topics_sorteddf","7f1df073":"%%time\n# Number of Documents for Each Topic\ntopic_counts = df_topic_sents_keywords['Dominant_Topic'].value_counts()\n\n# Percentage of Documents for Each Topic\ntopic_contribution = round(topic_counts\/topic_counts.sum(), 4)\n\n# Topic Number and Keywords\ntopic_num_keywords = df_topic_sents_keywords[\n    ['Dominant_Topic', 'Topic_Keywords']\n].drop_duplicates()\n\n# Concatenate Column wise\ndf_dominant_topics = pd.concat([topic_num_keywords.sort_values(by='Dominant_Topic').reset_index() , \n                                topic_counts.sort_index()\n                                , topic_contribution.sort_index()\n                               ], axis=1).drop(columns='index')\n\n# Change Column names\ndf_dominant_topics.columns = ['Dominant_Topic', 'Topic_Keywords', 'Num_Documents', 'Perc_Documents']\n\n# Show\ndf_dominant_topics.sort_values(by='Num_Documents', ascending=False)","0f0acd0a":"topics_docs= list(lda_model.get_document_topics(corpus, minimum_probability=0.0))\nlda_topics = [sorted(topics, key=lambda x: x[1], reverse=True)[0] for topics in topics_docs]\ndf = pd.concat([df,pd.DataFrame(lda_topics, columns=['lda_topic','lda_topic_prob'])], axis = 1)\ndel topics_docs, lda_topics","ace2e68c":"for num in range(lda_model.num_topics):\n    print('\\n')\n    print('###### Topic no.',num)\n    print('\\n')\n    for i,row in df[(df.lda_topic==num)&(df.lda_topic_prob)>0.95].sort_values(by='lda_topic_prob', ascending=False).head(3).iterrows():\n        print('At: ', row.timestamp)\n        print('Text: ',row.full_text)\n        print('Url :', row.url)\n        print('DataFrame index: ', i)\n        print('\\n')","9ce438c5":"map_dict = {\n    5:'YOLO',\n    0:'Market Thoughts',\n    1:'Market Analysis (?)',\n    2:'Brokerage Account',\n    3:'Trading Discussion (?)',\n    4:'General (?)'\n}\n\ndf.lda_topic = df.lda_topic.map(map_dict)","8de60389":"fig, ax = plt.subplots(nrows=3, ncols=1, figsize=(20,20))\nsns.lineplot(data=df, x='timestamp_doy',y='compound', hue='lda_topic', ax = ax[0])\nsns.lineplot(data=df, x='timestamp_doy',y='score', hue='lda_topic', ax = ax[1])\nsns.lineplot(data=df, x='timestamp_doy',y='comms_num', hue='lda_topic', ax = ax[2])\nplt.show()","51ad0265":"from sklearn.preprocessing import PowerTransformer\nfrom sklearn.pipeline import Pipeline\n\ndata = df.copy()\nfeatures = ['score','comms_num','created_age','compound']\n\npt = PowerTransformer()\ndata[features] = pd.DataFrame(\n    pt.fit_transform(\n        data[features])\n)","5ae113b5":"from sklearn.cluster import KMeans\n\nsns.set_theme(style=\"ticks\")\n\nn_clusters = 3\n\n\ndata.dropna(inplace=True)\n\nkmeans = KMeans(n_clusters=n_clusters, random_state=0).fit(data[features])\ndata['cluster'] = kmeans.predict(data[features])\nsns.pairplot(data=data[features+['cluster']], hue='cluster')\ndata[features] = pt.inverse_transform(data[features])\nplt.show()","0539ae38":"for i in range(n_clusters):\n    for text in data[data.cluster==i].full_text.head(1):\n        print('Cluster : ',i)\n        print('\\n')\n        print(text)\n        print('\\n')\n","14a3ec18":"from pandas_datareader import data\n\ntickers = ['GME']\nstart_date = str(df.timestamp_date.min() + pd.DateOffset(-20))\nend_date = str(df.timestamp_date.max())\nall_weekdays = pd.date_range(start=start_date, end=end_date, freq='B')\n\npanel_data = data.DataReader(tickers, 'yahoo', start_date, end_date)\npanel_data = panel_data.reindex(all_weekdays).fillna(method='ffill')\n\noutput_path = '.\/GME_20210110.csv'\npanel_data.to_csv(output_path)\npanel_data['Close'].plot.line()","54013780":"panel_data.head(10)","febaa1e0":"try:\n    from GoogleNews import GoogleNews\nexcept:\n    !pip install GoogleNews\n    from GoogleNews import GoogleNews","8778fdd9":"googlenews = GoogleNews()\ngooglenews.set_lang('en')\nstart_date = (df.timestamp_date.min() + pd.DateOffset(0)).strftime('%d\/%m\/%y')\nend_date = (df.timestamp_date.min()).strftime('%d\/%m\/%y')\ngooglenews.set_time_range(start_date,end_date)\ngooglenews.set_encode('utf-8')","a7eab5be":"googlenews.get_news('GME')","2a6cd98d":"news = pd.DataFrame.from_records(googlenews.results())\nnews.to_csv('.\/news.csv')\nnews.head(10)\n","f4f3c1b4":"The dataset contains posts from Reddit starting from 29th January 2021. Be aware that data don't start before GME stock hype. Moreover, this dataset is regularly updated and total number of rows increases. \n\nThe first date is an \"outlier\" that I've decided to remove from this analysis. ","e33daaae":"# Questions\n\nWe need to understand what data we have since some points are still not clear.\n- ~~Which is the time range?~~\n- ~~Number of posts?~~\n- Score and number of comments distribution and trend\n- ~~Do we have posts evolution alogn time?~~","6ac23853":"## Stock Analysis\n\nIs it possible to detect stock trends from WallStreetBets? In this subsection we try to understand if we can retrieve this information from our dataset.","28521b42":"# Utils","db4a60d6":"To Do:\n- Can we find data on minute level?","d038810f":"# Get Financial Historical Data ","1e0605b5":"# Sentiment Analysis on Full Text\n\n- ~~LDA Topic Modeling~~\n- ~~Topic Interpretability~~\n- ~~Sentiment Analysis based on Emoji... WIP~~\n\nOne of the most interesting things of Reddit posts is the strong presence of emojis. \nEmojis are ideograms and smilies used in message and provide their own meaning which can be that can be really different from the original one. In this section I will try to detect topics from reddits text and understand how to use emojis to detect emotions and\/or topics.","b7cf2982":"## Clustering","52c4c537":"Here is top 5 reddit posts by score.","72826237":"###\u00a0To Do:\n- Try another tecnique\n- Look optimal number of clusters","5c499d23":"Here I will show the the most used emojis in posts' titles.\nIf you have read something on WallStreetBets, these have a real clear meaning:\n- \ud83d\ude80 usually refer to \"Going to the moon\" and refers to the fact that a stock grows\n- \ud83d\udc8e, together with \ud83d\ude4c, means \"Diamonds Hands\". It measure how likely a trader is to sell if the price drops or if they make a modest profit. Diamonds means they will not drop. Otherwise they have \"paper\" hands.\n- \ud83e\udd8d,= \"apes together strongs\" and seems to derive directly from Rise of Planet of Apes. The \"apes\" are the retail investors that, if they are united, they could be strong enough to outlast those short on the stock. \n- \u270b and hands-related emojis could be related to diamond and paper hands or other supportive messages (?)\n\nEmojis could be used together or multiple times. In my opinion, in this case they should be considered as regular words.","bd427e82":"The dataset has a total of 8 columns. From [an official page on GitHub of the API Wrapper](https:\/\/github.com\/reddit-archive\/reddit\/wiki\/JSON) we have the following:\n- **title**: the title of the link. may contain newlines for some reason\n- **score**:the net-score of the link. note: A submission's score is simply the number of upvotes minus the number of downvotes. If five users like the submission and three users don't it will have a score of 2. Please note that the vote numbers are not \"real\" numbers, they have been \"fuzzed\" to prevent spam bots etc. So taking the above example, if five users upvoted the submission, and three users downvote it, the upvote\/downvote numbers may say 23 upvotes and 21 downvotes, or 12 upvotes, and 10 downvotes. The points score is correct, but the vote totals are \"fuzzed\".\n- **id**: this item's identifier, e.g. \"8xwlg\"\n- **url**: the link of this post. the permalink if this is a self-post\n- **comms_num**: the number of comments that belong to this link. includes removed comments.\n- **created**: the time of creation in local epoch-second format\n- **body**:  the raw text. this is the unformatted text which includes the raw markup characters such as ** for bold. <, >, and & are escaped.\n- **timestamp**: datetime about the related activity ","7a7d4e29":"## Topic Interpretability","1732beb6":"Let's see what happend along the days. We show number of reddits, median score or number of comments by hour.","ea09f8eb":"# Explorative Data Analysis","36b9131c":"# Get News Data","3443456e":"Here is top 5 reddit posts by number of comments.","5b4beb6e":"Can we find some relationship between day\/hour and reddit score, number of comments or lenght of texts? In the following we show box plot on log scale grouped by day of year or hour. ","b8803214":"The most commont \"stocks\" in our dataset are the following. Be aware of two things:\n- we count in how many documents a stock is cited in\n- the chosen method detect stocks **but** also common words. I've still not thought about how to refine this point yet. ","c186e82d":"## LDA Topic Modeling","03463d55":"To Do:\n- ~~improve visualization (with plotly?)~~\n- correlation between stocks? - To do in another Notebook or last section\n- ~~correlation between stocks, number of post, number of comments, overall score and median compound~~\n- ~~define engagement~~\n- animated scatterplot score vs comms_num","5a74e10a":"### Add features from LDA model"}}