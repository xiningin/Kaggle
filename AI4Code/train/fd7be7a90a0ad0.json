{"cell_type":{"05ba6b95":"code","aed790f9":"code","861d53c6":"code","6aacd12a":"code","aebb06b9":"code","564a977a":"code","b7d10b30":"code","e9aff939":"code","35b76cd8":"code","cb3df879":"code","a56f9456":"markdown","e122bb64":"markdown","36645af0":"markdown","114674c6":"markdown","9cb56a58":"markdown","c93a2a53":"markdown","8d968a66":"markdown","5fad3bfc":"markdown"},"source":{"05ba6b95":"import sys\nsys.path.append(\"\/kaggle\/input\/multilabel-stratification\")  # https:\/\/github.com\/trent-b\/iterative-stratification\nfrom iterstrat.ml_stratifiers import MultilabelStratifiedKFold","aed790f9":"import torch\nfrom torch import nn\nfrom torch.utils import data\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader\nimport pandas as pd\nfrom pathlib import Path\nfrom statistics import mean, stdev\nfrom tqdm.auto import tqdm\nfrom functools import reduce","861d53c6":"data_dir = Path(\"\/kaggle\/input\/lish-moa\")","6aacd12a":"D_feats=pd.read_csv(data_dir \/ \"train_features.csv\", index_col=\"sig_id\")\ndisplay(D_feats.head(2))\n\nD_targets=pd.read_csv(data_dir \/ \"train_targets_scored.csv\", index_col=\"sig_id\")\ndisplay(D_targets.head(2))\n\nD_feats_test=pd.read_csv(data_dir \/ \"test_features.csv\", index_col=\"sig_id\")","aebb06b9":"def process(df, select_trt=True):\n    if select_trt:\n        df=df.query(\"cp_type=='trt_cp'\")\n        \n    df=df.drop(columns=\"cp_type\")    \n        \n    df[\"cp_dose\"]=df[\"cp_dose\"].map({\"D1\":0, \"D2\":1})\n    df[\"cp_time\"]=df[\"cp_time\"]\/df[\"cp_time\"].max()\n\n    df=df.astype(\"float32\")\n    \n    return df\n    \nX=process(D_feats, select_trt=False)\ndisplay(X.head(2))\n\nY=D_targets.loc[X.index].astype(\"float32\")\n\nXt=process(D_feats_test)\ndisplay(Xt.head(2))","564a977a":"n_splits = 5\nsplitter_random_seed = 123\n\nfold_splitter = MultilabelStratifiedKFold(n_splits=n_splits, shuffle=True, random_state=splitter_random_seed)\nfolds = pd.Series(-1, index=X.index)\nfor fold_idx, (train_idx, val_idx) in enumerate(fold_splitter.split(X, Y)):\n    folds.iloc[val_idx]=fold_idx","b7d10b30":"class FoldTensorDatasets:\n    def __init__(self, X, Y, fold):\n        self.X = X\n        self.Y = Y\n        self._fold = fold\n        self._folds = sorted(fold.unique())\n\n    def fold(self, fold_idx):\n        train_idx = self._fold.loc[lambda x: x != fold_idx].index\n        val_idx = self._fold.loc[lambda x: x == fold_idx].index\n\n        train_dataset = data.TensorDataset(\n            torch.tensor(self.X.loc[train_idx].values, dtype=torch.float),\n            torch.tensor(self.Y.loc[train_idx].values, dtype=torch.float),\n        )\n\n        val_dataset = data.TensorDataset(\n            torch.tensor(self.X.loc[val_idx].values, dtype=torch.float),\n            torch.tensor(self.Y.loc[val_idx].values, dtype=torch.float),\n        )\n        \n        return train_dataset, val_dataset\n\n    def folds(self):\n        for fold_idx in self._folds:\n            yield self.fold(fold_idx)\n\n\ndataset = FoldTensorDatasets(X, Y, folds)","e9aff939":"max_epochs=25\nhidden_size1 = 200\nhidden_size2 = 400\ntrain_batch_size = 128\nval_batch_size=1024\n\n##\n\nnum_feats = X.shape[1]\nnum_targets = Y.shape[1]\n    \n   \nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        \n        self.net = nn.Sequential(\n            nn.BatchNorm1d(num_feats),\n            nn.Dropout(0.2),\n            nn.utils.weight_norm(nn.Linear(num_feats, hidden_size1)),\n            nn.ReLU(),\n            #\n            nn.BatchNorm1d(hidden_size1),\n            nn.Dropout(0.5),\n            nn.utils.weight_norm(nn.Linear(hidden_size1, hidden_size2)),\n            nn.ReLU(),\n            #\n            nn.BatchNorm1d(hidden_size2),\n            nn.Dropout(0.5),\n            nn.utils.weight_norm(nn.Linear(hidden_size2, num_targets)),\n        )\n        \n    def forward(self, x):\n        return self.net(x)\n    \n    def infer(self, x):\n        return self.net(x).sigmoid()\n    ","35b76cd8":"fold_val_losses = []\nmodel_filenames = []\ndevice=\"cuda\"\n\nfor fold_num, (train_data, val_data) in tqdm(enumerate(dataset.folds(), 1), total=n_splits):\n    print(f\"Fold {fold_num}\")\n    \n    train_loader = DataLoader(train_data, batch_size=train_batch_size)\n    val_loader = DataLoader(val_data, batch_size=val_batch_size)\n    \n    model = Model().to(device)\n    \n    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n\n    scheduler = torch.optim.lr_scheduler.OneCycleLR(\n        optimizer=optimizer,\n        pct_start=0.1,\n        div_factor=1e3,\n        max_lr=1e-2,\n        epochs=max_epochs,\n        steps_per_epoch=len(train_loader),\n    )\n    \n    for epoch in range(max_epochs):\n        model.train()\n        for x, y in train_loader:\n            x=x.to(device)\n            y=y.to(device)\n            \n            optimizer.zero_grad()\n            out = model(x)\n            loss = F.binary_cross_entropy_with_logits(out, y)\n            loss.backward()\n            optimizer.step()\n            scheduler.step()\n            \n        model.eval()\n        val_losses=[]\n        with torch.no_grad():\n            for x, y in val_loader:\n                x=x.to(device)\n                y=y.to(device)\n                \n                out = model(x)\n                loss = F.binary_cross_entropy_with_logits(out, y)\n                val_losses.append(loss.item())\n        \n        mean_val_loss=mean(val_losses)\n        print(f\"[e{epoch:02}] Last Train Loss: {loss.item():.4f}, Val Loss {mean_val_loss:.4f}\")\n        \n    model_filename =  f\"model_fold{fold_num}.pth\"\n    torch.save(model.state_dict(), model_filename)\n    model_filenames.append(model_filename)\n    \n    fold_val_losses.append(mean_val_loss)\n    \nprint(f\"Avg. fold val. loss: {mean(fold_val_losses):.5f}+-{stdev(fold_val_losses):.5f}\")\nprint(\"TRAINING DONE\")\n%ls *.pth","cb3df879":"device=\"cuda\"\n\nmodel = Model().to(device)\n\nfold_preds = []\n\nfor model_filename in model_filenames:\n    model.load_state_dict(torch.load(model_filename))\n    model.eval()\n    \n    preds=[]\n    \n    for i in range(0, len(Xt), val_batch_size):\n        X_batch=Xt.iloc[i:i+val_batch_size]\n\n        cur_preds=model.infer(torch.tensor(X_batch.values).to(device))\n\n        preds.append(pd.DataFrame(cur_preds.cpu().detach().numpy(), index=X_batch.index, columns=Y.columns))\n        \n    fold_preds.append(pd.concat(preds))\n    \nYt = reduce(pd.DataFrame.add, fold_preds) \/ len(fold_preds)\n\nYt = Yt.reindex(D_feats_test.index).fillna(0)\nYt.to_csv(\"submission.csv\", float_format=\"%.4e\")\nYt.head()","a56f9456":"# Fold definition","e122bb64":"# Predict","36645af0":"# Prepare data","114674c6":"Here is a succinct version of a basic neural network. The architecture is currently taken from another kernel. There are so many similar notebooks, so it's hard to tell where it originates from.\n\nMost of the notebooks felt long and slightly messy. So I wrote a short version of it.\n\nI was quite disappointed from Pytorch Lightning, as despite it's shiny presentation and version 1.0 it's buggy under the hood and the documentation is incomplete. Something goes very wrong during training and it's puzzling why a self-written loop performs much better.","9cb56a58":"# Load raw data","c93a2a53":"# Model definition","8d968a66":"# Training","5fad3bfc":"That's all folks."}}