{"cell_type":{"3bcfef3a":"code","738c1baf":"code","0829c646":"code","cd47235e":"code","846bd592":"code","1b8e9091":"code","fdf772ca":"code","fb0e05f7":"code","f620a0ec":"code","af0986e8":"code","c0492c0d":"markdown","2d6ea377":"markdown","fbd3bb4d":"markdown","d7006346":"markdown","9a4d39fe":"markdown","20b5cb2a":"markdown","3edda234":"markdown","27f14d3b":"markdown","f3208f3a":"markdown","5740547b":"markdown","8b0713e5":"markdown"},"source":{"3bcfef3a":"import matplotlib.pyplot as plt\n\nimport numpy as np\nimport pandas as pd\n\nimport scipy\nimport seaborn as sns\n\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import cross_val_score","738c1baf":"#Reading the datasets\ndata_v1 = pd.read_csv(\"\/kaggle\/input\/graduate-admissions\/Admission_Predict_Ver1.1.csv\")\ndata_v0 = pd.read_csv(\"\/kaggle\/input\/graduate-admissions\/Admission_Predict.csv\")\ndata = pd.concat([data_v1, data_v0])\n\nprint(data.shape)\n\ndata.head()","0829c646":"data = data.drop_duplicates()\ndata.shape","cd47235e":"#Removing the serial number column as it adds no correlation to any columns\ndata = data.drop(columns = [\"Serial No.\"])\n\n#The column \"Chance of Admit\" has a trailing space which is removed\ndata = data.rename(columns={\"Chance of Admit \": \"Chance of Admit\"})\n\ndata.head()","846bd592":"def get_training_data(df):\n    \"\"\"\n    This function splits the data into X and y variables and returns them\n    \"\"\"\n    X = df.drop(columns = [\"University Rating\", \"Chance of Admit\"])\n    y = df[\"Chance of Admit\"]\n    \n    return X, y","1b8e9091":"def train_model(university_rating):\n    \"\"\"\n    1. Takes the subset only for one university rating\n    2. Invokes the get_training_data function,\n    3. Fits a linear regression model\n    4. Cross validates it\n    5. Returns the model object and the metrics for cross validation\n    \"\"\"\n    #Filtering for one university fromt the data dataframe\n    df = data[data[\"University Rating\"] == university_rating]\n    print(df.shape)\n    \n    #Splitting into X and y for regression\n    X, y = get_training_data(df)\n    \n    regressor = LinearRegression()\n    regressor.fit(X, y)\n    \n    metric = cross_val_score(regressor, X, y, cv = 5)\n    \n    return regressor, metric","fdf772ca":"university_ratings = data[\"University Rating\"].unique()\n\nuniversity_recommendations = {}\n\nfor u in university_ratings:\n    regressor, metric = train_model(u)\n    university_recommendations[\"University ranking \" + str(u)] = {'model': regressor, 'metric': metric}","fb0e05f7":"university_recommendations","f620a0ec":"test = data.sample(20)\ntest = test.drop(columns = [\"Chance of Admit\", \"University Rating\"])\ntest.head()","af0986e8":"predictions = {}\n\nfor uni in university_recommendations.keys():\n    model = university_recommendations[uni][\"model\"]\n    \n    predictions[uni] = model.predict(test)\n    \npred = pd.DataFrame(predictions)\npred.head(10)","c0492c0d":"# Importing and exploring the datasets\nFor importing the packages, I just like to put them in alphabetical order of the package, so that it is easy to manage and review if needed","2d6ea377":"# The approach\n\nFor working this from a recommendation perspective, we train 5 regression model, each for one of the university rank from 1 to 5. The first regression model will compute a score, which is the probability of the student getting admitted into the university with university rank 1, the second regression model will compute a score of the student getting admitted into the university with rank 2 and so on.\n\nFinally all these scores will be compared against each other and in unison will recommend the university rank which the student has the best possiblity of getting admitted to.\n\nFor this purpouse we use the other features given below:\n1. GRE Score\n2. TOEFL Score\n3. SOP\n4. LOR\n5. CGPA\n6. Research\n\nAnd we will use the variable Chance of Admit as the Y variable.\n\nWe will use the variable University Rating to take out data corresponding to each of the universities and then train 5 independent model corresponding all the 5 university ranking\n\nFor the purpouse of prediction, for one single student record, we will make the prediction out of all the 5 models and will compare the scores with each other to make the best recommended university ranking for the student.","fbd3bb4d":"The cross validation summary does not look good!","d7006346":"# Getting recommendation from the model\n\nWe will prepare a test data with random 20 observations to see how good the model performs!","9a4d39fe":"As you can see, the above dataframe represents each student's propensity to get admitted into the university of that rank!","20b5cb2a":"Turns out there are no extra records between both the datasets, whatever records were there in the V1.1 was also present in the first dataset.","3edda234":"Now let us train the 5 models and save the models to a object","27f14d3b":"# Introduction:\n\nAfter working on the college admissions challenge as a prediction model to predict the propensity of the student getting admitted into a particular college, I wanted to take a different approach and see, which rank college the student has the best possiblity of getting admitted into.\n\nI have already performed EDA in the first notebook that I published on this dataset. Please take a look at [this notebook](https:\/\/www.kaggle.com\/gireeshs\/eda-and-predicting-graduate-admission) and upvote if you like it!\n\nIn this notebook, I look at this data from a recommendation perspective to see which rank college the student has the best shot of getting admitted to given his other features.\n\nThe sections in this notebook are:\n1. Introduction\n2. The approach\n3. Importing and exploring the datasets\n4. Model building\n5. Getting recommendation from the model\n6. Conclusion","f3208f3a":"# Model building","5740547b":"Since there were two files, I did not want to leave out any records, so I imported both the datasets, concatenated into a single dataset and then dropped duplicates. ","8b0713e5":"# Conclusion:\n\nThe limitation of this technique on this dataset is not having enough data. There were only 500 records and it was not evenly distributed among all the universities!\n\nBut generally this technique can seen to be working with a large amount of data. Because of the limitation of the data, I did not split the notebook into test and train sets. But a validation score such as MAP or MAR (Mean absolute precision or Mean absolute recall) could have been incorporated if there were train and test sets.\n\nPlease let me know about your feedback on this notebook in the comments section below! And upvote this notebook if you found it interesting!"}}