{"cell_type":{"9f7d0067":"code","09e4ce26":"code","903c6375":"code","146cc4bc":"code","bf35f59d":"code","2811929c":"code","66d14361":"code","ed75f711":"code","2ec8fcf0":"code","9a5eeacd":"code","c71bd082":"code","b91b4f89":"code","a1b23af1":"code","4bc61d0c":"code","385d3b61":"code","f0c9086e":"code","c489ff32":"code","fb021894":"code","296668dc":"code","a6aceded":"code","f628a4ec":"code","556d3387":"code","f73279f0":"code","c5275128":"code","9d152658":"code","b077d7ef":"code","4b919566":"code","e4632a75":"code","c3bc7f4d":"code","8449e910":"code","bb95ef2f":"code","5c7e2145":"code","10874a3d":"code","cd1ca75d":"code","f896c5b9":"code","46094c3f":"code","6713fb54":"code","b4f01aeb":"code","6ec24e76":"code","34fead3e":"code","99708e59":"code","3e02d07c":"code","9e0b23f2":"code","ef293e9b":"code","5007d809":"code","1c9ce81e":"code","1794f02d":"code","d25d5f8f":"code","d801ecbe":"code","022d2002":"code","23729c7c":"code","552b0342":"code","23bdfef2":"code","2435ed91":"code","152c8b36":"code","43799461":"code","9ffa267f":"code","f27e06e9":"code","115a6760":"code","9f80e498":"code","c549643f":"code","0729324b":"code","bb3c170c":"code","d675d9c7":"code","f02e057f":"code","85de0c5b":"code","1d168cad":"code","7e7e9c2a":"code","4a32939f":"code","0f0aef00":"code","11ba12bb":"code","246d00fb":"code","09d88d87":"code","d0c05ce4":"code","11ca8e9b":"code","7cf5ec81":"code","9771b2d9":"code","68596419":"code","1de2a47d":"code","2c95f56a":"code","b8192153":"code","81b2af79":"code","45cb351e":"code","52e527df":"code","74727355":"code","5ae858ce":"code","df8ef8b1":"code","a232e322":"code","cdd6531e":"code","4ad93b54":"code","fb4f3b47":"code","a31a78db":"code","de24d75b":"code","16acb591":"code","835b49dd":"code","66732775":"code","60cff8c8":"code","8da5dbbe":"code","e3732bb5":"code","09e6e1ed":"code","fbd0de03":"code","c09a3d88":"code","516b6716":"code","b1044583":"code","67b8e97f":"markdown","4406fd39":"markdown","1c78658b":"markdown","bcf4e936":"markdown","8c4c3780":"markdown","4705c43b":"markdown","421bea33":"markdown","42bf7a4b":"markdown","50ec922c":"markdown"},"source":{"9f7d0067":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","09e4ce26":"import matplotlib.pyplot as plt\nimport seaborn as sns","903c6375":"df = pd.read_csv('\/kaggle\/input\/creditcardfraud\/creditcard.csv')\ndf","146cc4bc":"df.shape","bf35f59d":"df.columns","2811929c":"df.describe().transpose()","66d14361":"df['Class'].value_counts()\n","ed75f711":"sns.countplot(df['Class'])","2ec8fcf0":"df.isnull().sum()","9a5eeacd":"#There are no missing values","c71bd082":"df['Time']\/3600\n#looks like the time of transaction is provided in seconds when converted to hours turns \n# out to be for 48 hours of distribution","b91b4f89":"df['Time'] = df['Time']\/3600\ndf","a1b23af1":"#creating time bins and seeing if they form a pattern","4bc61d0c":"df_time = df['Time']\nbins = [0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48] \n# \u2013 Use min and max and other values  to initiate breaks in age required \nlabels = ['0-1', '1-2', '2-3', '3-4', '4-5', '5-6','6-7', '7-8', '8-9','9-10', '10-11', '11-12', '12-13', '13-14', '14-15','15-16', '16-17', '17-18','18-19','19-20','20-21', '21-22', '22-23', '23-24', '24-25','25-26', '26-27', '27-28','28-29','29-30','30-31', '31-32', '32-33', '33-34', '34-35','35-36', '36-37', '37-38','38-39','39-40','40-41', '41-42', '42-43', '43-44', '44-45','45-46', '46-47', '47-48']\n#\u2013 Define the labels, labels should always be one less in number compared to bins\ndf['Timerange'] = pd.cut(df_time, bins, labels = labels,include_lowest = True)\ndf","385d3b61":"df_Fraud = df[df['Class']==1]\ndf_Fraud['Timerange'].value_counts().plot(kind='bar')","f0c9086e":"sns.countplot(df_Fraud['Timerange'])\nplt.xticks(rotation=90)","c489ff32":"#there is no pattern in timerange seen for more frauds in a particular time range but we shall take the feature as a category variable\n#also replacing all values in the range 24-48 hour range to 0-24 range to accomodate cyclicity if at all present","fb021894":"cleanup_nums = {\"Timerange\": {\"47-48\": \"23-24\",\"46-47\": \"22-23\",\"45-46\": \"21-22\",\"44-45\": \"20-21\",\"43-44\": \"19-20\",\"42-43\": \"18-19\",\"41-42\": \"17-18\",\"40-41\": \"16-17\",\"39-40\": \"15-16\",\"38-39\": \"14-15\",\"37-38\": \"13-14\",\"36-37\": \"12-13\",\"35-36\": \"11-12\",\"34-35\": \"10-11\",\"33-34\": \"9-10\",\"32-33\": \"8-9\",\"31-32\": \"7-8\",\"30-31\": \"6-7\",\"29-30\": \"5-6\",\"28-29\": \"4-5\",\"27-28\": \"3-4\",\"26-27\": \"2-3\",\"25-26\": \"1-2\",\"24-25\": \"0-1\"}}","296668dc":"df = df.replace(cleanup_nums)\ndf","a6aceded":"df['Timerange'].nunique()","f628a4ec":"df = df.drop('Time',axis=1)","556d3387":"#checking details for ","f73279f0":"df_fraud_modified = df[df['Class']==1]\nsns.countplot(df_fraud_modified['Timerange'])\nplt.xticks(rotation=90)","c5275128":"#distribution doesn't generalize into any particular pattern\n# will check same details for amount data\n\nf, (ax1,ax2) = plt.subplots(2,1, figsize=(15, 6))\nsns.distplot(df['Amount'], color='tab:blue',ax=ax1)\nsns.distplot(df_fraud_modified['Amount'], color='tab:red',ax=ax2)","9d152658":"df['Amount'].describe()\n#sns.distplot(df_fraud_modified['Amount'], color='tab:red',ax=ax2)","b077d7ef":"df_fraud_modified['Amount'].describe()","4b919566":"sns.scatterplot(df_fraud_modified['Timerange'],df_fraud_modified['Amount'])\nplt.xticks(rotation=90)","e4632a75":"f, (ax1,ax2,ax3,ax4,ax5,ax6,ax7,ax8,ax9,ax10,ax11,ax12,ax13,ax14,ax15,ax16,ax17,ax18,ax19,ax20,ax21,ax22,ax23,ax24,ax25,ax26,ax27,ax28) = plt.subplots(28,1, figsize=(15, 50))\nsns.distplot(df['V1'], color='tab:blue',ax=ax1)\nsns.distplot(df_fraud_modified['V1'], color='tab:red',ax=ax1,axlabel='V1')\nsns.distplot(df['V2'], color='tab:blue',ax=ax2)\nsns.distplot(df_fraud_modified['V2'], color='tab:red',ax=ax2)\nsns.distplot(df['V3'], color='tab:blue',ax=ax3)\nsns.distplot(df_fraud_modified['V3'], color='tab:red',ax=ax3)\nsns.distplot(df['V4'], color='tab:blue',ax=ax4)\nsns.distplot(df_fraud_modified['V4'], color='tab:red',ax=ax4)\nsns.distplot(df['V5'], color='tab:blue',ax=ax5)\nsns.distplot(df_fraud_modified['V5'], color='tab:red',ax=ax5)\nsns.distplot(df['V6'], color='tab:blue',ax=ax6)\nsns.distplot(df_fraud_modified['V6'], color='tab:red',ax=ax6)\nsns.distplot(df['V7'], color='tab:blue',ax=ax7)\nsns.distplot(df_fraud_modified['V7'], color='tab:red',ax=ax7)\nsns.distplot(df['V8'], color='tab:blue',ax=ax8)\nsns.distplot(df_fraud_modified['V8'], color='tab:red',ax=ax8)\nsns.distplot(df['V9'], color='tab:blue',ax=ax9)\nsns.distplot(df_fraud_modified['V9'], color='tab:red',ax=ax9)\nsns.distplot(df['V10'], color='tab:blue',ax=ax10)\nsns.distplot(df_fraud_modified['V10'], color='tab:red',ax=ax10)\nsns.distplot(df['V11'], color='tab:blue',ax=ax11)\nsns.distplot(df_fraud_modified['V11'], color='tab:red',ax=ax11)\nsns.distplot(df['V12'], color='tab:blue',ax=ax12)\nsns.distplot(df_fraud_modified['V12'], color='tab:red',ax=ax12)\nsns.distplot(df['V13'], color='tab:blue',ax=ax13)\nsns.distplot(df_fraud_modified['V13'], color='tab:red',ax=ax13)\nsns.distplot(df['V14'], color='tab:blue',ax=ax14)\nsns.distplot(df_fraud_modified['V14'], color='tab:red',ax=ax14)\nsns.distplot(df['V15'], color='tab:blue',ax=ax15)\nsns.distplot(df_fraud_modified['V15'], color='tab:red',ax=ax15)\nsns.distplot(df['V16'], color='tab:blue',ax=ax16)\nsns.distplot(df_fraud_modified['V16'], color='tab:red',ax=ax16)\nsns.distplot(df['V17'], color='tab:blue',ax=ax17)\nsns.distplot(df_fraud_modified['V17'], color='tab:red',ax=ax17)\nsns.distplot(df['V18'], color='tab:blue',ax=ax18)\nsns.distplot(df_fraud_modified['V18'], color='tab:red',ax=ax18)\nsns.distplot(df['V19'], color='tab:blue',ax=ax19)\nsns.distplot(df_fraud_modified['V19'], color='tab:red',ax=ax19)\nsns.distplot(df['V20'], color='tab:blue',ax=ax20)\nsns.distplot(df_fraud_modified['V20'], color='tab:red',ax=ax20)\nsns.distplot(df['V21'], color='tab:blue',ax=ax21)\nsns.distplot(df_fraud_modified['V21'], color='tab:red',ax=ax21)\nsns.distplot(df['V22'], color='tab:blue',ax=ax22)\nsns.distplot(df_fraud_modified['V22'], color='tab:red',ax=ax22)\nsns.distplot(df['V23'], color='tab:blue',ax=ax23)\nsns.distplot(df_fraud_modified['V23'], color='tab:red',ax=ax23)\nsns.distplot(df['V24'], color='tab:blue',ax=ax24)\nsns.distplot(df_fraud_modified['V24'], color='tab:red',ax=ax24)\nsns.distplot(df['V25'], color='tab:blue',ax=ax25)\nsns.distplot(df_fraud_modified['V25'], color='tab:red',ax=ax25)\nsns.distplot(df['V26'], color='tab:blue',ax=ax26)\nsns.distplot(df_fraud_modified['V26'], color='tab:red',ax=ax26)\nsns.distplot(df['V27'], color='tab:blue',ax=ax27)\nsns.distplot(df_fraud_modified['V27'], color='tab:red',ax=ax27)\nsns.distplot(df['V28'], color='tab:blue',ax=ax28)\nsns.distplot(df_fraud_modified['V28'], color='tab:red',ax=ax28)","c3bc7f4d":"X = df.drop(['Class','Timerange'],axis=1)\ny=df['Class']","8449e910":"#from sklearn.ensemble import RandomForestClassifier\n#rfc = RandomForestClassifier(n_estimators=50,random_state=0)\n#rfc.fit(X,y)\n#pd.Series(rfc.feature_importances_,index=X.columns).sort_values(ascending=False)","bb95ef2f":"from statsmodels.stats.outliers_influence import variance_inflation_factor\n\n# VIF dataframe\n\nvif_data = pd.DataFrame()\nvif_data['feature'] = X.columns\n\n#vif_data\n# calculating VIF for each feature\nvif_data[\"VIF\"] = [variance_inflation_factor(X.values, i) for i in range(len(X.columns))]\n  \nprint(vif_data)","5c7e2145":"sns.heatmap(df.corr())","10874a3d":"df_US_nonFraud = df[df['Class']==0].sample(frac = 1)\ndf_US_nonFraud = df_US_nonFraud[:492]\ndf_US_Fraud = df[df['Class']==1]","cd1ca75d":"df_US = pd.concat([df_US_nonFraud,df_US_Fraud])\ndf_US = df_US.sample(frac = 1)\ndf_US","f896c5b9":"df_US['Class']","46094c3f":"#sns.clustermap(df_US.corr())\ncorrelation_matrix = df.corr()\nsns.heatmap(correlation_matrix,xticklabels=True, yticklabels=True)\nplt.figure(figsize=(15,20))\nplt.show()","6713fb54":"correlation_matrix_US = df_US.corr()\nsns.heatmap(correlation_matrix_US,xticklabels=True, yticklabels=True)\nplt.figure(figsize=(25,25))\nplt.show()","b4f01aeb":"#vif_data_US = pd.DataFrame()\n#vif_data_US['feature'] = X.columns\n\n#vif_data\n# calculating VIF for each feature\n#vif_data_US[\"VIF\"] = [variance_inflation_factor(X.values, i) for i in range(len(X.columns))]\n#vif_data_US","6ec24e76":"#normal split\nfrom sklearn.model_selection import train_test_split\ny.shape\n#X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=1\/3,random_state=42, stratify=y)","34fead3e":"#not one hot encoding Timerange and not including in features as no pattern observed on the same\nX_US = df_US.drop(['Class','Timerange'],axis=1)\ny_US=df_US['Class']\nX_train_US,X_test_US,y_train_US,y_test_US = train_test_split(X_US,y_US,test_size=0.33,random_state=42,stratify=y_US)\n#while spitting this stratify was to be put as equal to y_US and not y which was causing delays.","99708e59":"#scaling value of Amount feature in undersampled dataset\nfrom sklearn.preprocessing import RobustScaler\nrob_scaler = RobustScaler() \nX_train_US['scaled_Amount'] = rob_scaler.fit_transform(X_train_US[['Amount']])\nX_test_US['scaled_Amount'] = rob_scaler.transform(X_test_US[['Amount']])\n#X_train_US.drop('Amount', axis=1, inplace=True)\n#y_train_US.drop('Amount', inplace=True)","3e02d07c":"results = pd.DataFrame(columns = ['LR', 'SVM-Linear', 'SVM-RGF', 'Decision Trees'], index = ['Accuracy Score','Recall Score','Precision Score','F1 Score','ROC-AUC','Train_score','Test_score','CV_score'])\n#results.index.values[0] = \"Accuracy Score\"\n#results.index.values[1] = \"Recall Score\"\nresults","9e0b23f2":"from sklearn.linear_model import LogisticRegression\n#from sklearn.metrics import (confusion_matrix, precision_recall_curve, auc, roc_curve, recall_score,precision_score,accuracy_score,\n                            # classification_report, f1_score, average_precision_score, precision_recall_fscore_support,roc_auc_score)\n\nfrom sklearn.model_selection import cross_val_score \n#model = LogisticRegression(solver='lbfgs', max_iter=1000)\n#lr_US = model.fit(X_train_US,y_train_US)\n#lr_US_pred = model.predict(X_test_US)\n\nfrom sklearn.model_selection import KFold \n#kfold = KFold(n_splits=5) \n#validation_score = cross_val_score(model,X_train_US,y_train_US,cv=kfold)\n\n#print(classification_report(y_test_US,lr_US_pred))\n#print(\"\")\n#print(confusion_matrix(y_test_US,lr_US_pred))","ef293e9b":"#results.iloc[0,0] = accuracy_score(y_test_US, lr_US_pred) # saving precision score for Logistic regression, make sure to name the y_pred different for different algorithms\n#results.iloc[1,0] = recall_score(y_test_US, lr_US_pred)\n#results.iloc[2,0] = precision_score(y_test_US, lr_US_pred)\n#results.iloc[3,0] = f1_score(y_test_US, lr_US_pred)\n#results.iloc[4,0] = roc_auc_score(y_test_US, lr_US_pred) \n#results.iloc[5,0] = model.score(X_train_US, y_train_US) \n#results.iloc[6,0] = model.score(X_test_US, y_test_US) \n#results.iloc[7,0] = validation_score.mean() \n\n#results","5007d809":"from sklearn.metrics import (confusion_matrix, precision_recall_curve, auc, roc_curve, recall_score,precision_score,accuracy_score,\n                             classification_report, f1_score, average_precision_score, precision_recall_fscore_support,roc_auc_score)","1c9ce81e":"from sklearn.svm import SVC\n#model = SVC(kernel = 'linear')\n#SVM_linear_US = model.fit(X_train_US,y_train_US)\n#SVM_linear_US_pred = model.predict(X_test_US)\n\n#from sklearn.model_selection import KFold \n#kfold = KFold(n_splits=5) \n#validation_score = cross_val_score(model,X_train_US,y_train_US,cv=kfold)\n\n#print(classification_report(y_test_US,SVM_linear_US_pred))\n#print(\"\")\n#print(confusion_matrix(y_test_US,SVM_linear_US_pred))","1794f02d":"#results.iloc[0,1] = accuracy_score(y_test_US, SVM_linear_US_pred) # saving precision score for Logistic regression, make sure to name the y_pred different for different algorithms\n#results.iloc[1,1] = recall_score(y_test_US, SVM_linear_US_pred)\n#results.iloc[2,1] = precision_score(y_test_US, SVM_linear_US_pred)\n#results.iloc[3,1] = f1_score(y_test_US, SVM_linear_US_pred)\n#results.iloc[4,1] = roc_auc_score(y_test_US, SVM_linear_US_pred) \n#results.iloc[5,1] = model.score(X_train_US, y_train_US) \n#results.iloc[6,1] = model.score(X_test_US, y_test_US) \n#results.iloc[7,1] = validation_score.mean() \n\n#results","d25d5f8f":"#from sklearn.svm import SVC\n#model = SVC(kernel = 'rbf')\n#SVM_rbf_US = model.fit(X_train_US,y_train_US)\n#SVM_rbf_US_pred = model.predict(X_test_US)\n\n#from sklearn.model_selection import KFold \n#kfold = KFold(n_splits=5) \n#validation_score = cross_val_score(model,X_train_US,y_train_US,cv=kfold)\n\n#print(classification_report(y_test_US,SVM_rbf_US_pred))\n#print(\"\")\n#print(confusion_matrix(y_test_US,SVM_rbf_US_pred))","d801ecbe":"#results.iloc[0,2] = accuracy_score(y_test_US, SVM_rbf_US_pred) # saving precision score for Logistic regression, make sure to name the y_pred different for different algorithms\n#results.iloc[1,2] = recall_score(y_test_US, SVM_rbf_US_pred)\n#results.iloc[2,2] = precision_score(y_test_US, SVM_rbf_US_pred)\n#results.iloc[3,2] = f1_score(y_test_US, SVM_rbf_US_pred)\n#results.iloc[4,2] = roc_auc_score(y_test_US, SVM_rbf_US_pred) \n#results.iloc[5,2] = model.score(X_train_US, y_train_US) \n#results.iloc[6,2] = model.score(X_test_US, y_test_US) \n#results.iloc[7,2] = validation_score.mean()\n\n#results","022d2002":"from sklearn import tree\n#model = tree.DecisionTreeClassifier()\n#DT_US = model.fit(X_train_US,y_train_US)\n#DT_US_pred = model.predict(X_test_US)\n\n#from sklearn.model_selection import KFold \n#kfold = KFold(n_splits=5) \n#validation_score = cross_val_score(model,X_train_US,y_train_US,cv=kfold)\n\n#print(classification_report(y_test_US,DT_US_pred))\n#print(\"\")\n#print(confusion_matrix(y_test_US,DT_US_pred))","23729c7c":"#results.iloc[0,3] = accuracy_score(y_test_US, DT_US_pred) # saving precision score for Logistic regression, make sure to name the y_pred different for different algorithms\n#results.iloc[1,3] = recall_score(y_test_US, DT_US_pred)\n#results.iloc[2,3] = precision_score(y_test_US, DT_US_pred)\n#results.iloc[3,3] = f1_score(y_test_US, DT_US_pred)\n#results.iloc[4,3] = roc_auc_score(y_test_US, DT_US_pred) \n#results.iloc[5,3] = model.score(X_train_US, y_train_US) \n#results.iloc[6,3] = model.score(X_test_US, y_test_US) \n#results.iloc[7,3] = validation_score.mean()\n\n#results","552b0342":"#since the above data is giving very high scores representing good models, since the dataset was very small due to undersampling we \n#will do better test by separating the test data in normal split and then undersamling the original train data to create models","23bdfef2":"df","2435ed91":"df_original = df.drop('Timerange',axis=1)","152c8b36":"X_original = df_original.drop('Class',axis=1)\ny_original = df_original['Class']","43799461":"from sklearn.model_selection import train_test_split\nX_original_train,X_original_test,y_original_train,y_original_test = train_test_split(X_original,y_original,test_size=1\/3,random_state=42, stratify=y_original)","9ffa267f":"y_original_test.value_counts()","f27e06e9":"from sklearn.preprocessing import RobustScaler\nrob_scaler = RobustScaler()\nX_original_train['scaled_Amount'] = rob_scaler.fit_transform(X_original_train[['Amount']])\nX_original_test['scaled_Amount'] = rob_scaler.transform(X_original_test[['Amount']])\nX_original_train.drop('Amount', axis=1, inplace=True)\nX_original_test.drop('Amount', axis=1, inplace=True)","115a6760":"X_original_test_Re = X_original_test\nX_original_test_Re","9f80e498":"results_originalstest = pd.DataFrame(columns = ['LR', 'SVM-Linear', 'SVM-RGF', 'Decision Trees','NaiveBayes','knn'], index = ['Accuracy Score','Recall Score','Precision Score','F1 Score','ROC-AUC','Train_score','Test_score','CV_score'])\nresults_originalstest","c549643f":"#concatting and splitting again the original function X_original_train and y_original_train. We will create model on this and then\n#test on original\n\noriginal_train = pd.concat([X_original_train,y_original_train],axis=1)\noriginal_train = original_train.sample(frac = 1)\noriginal_train.shape","0729324b":"#sampling 1000 randomly sampled majority class\noriginal_majorityclass = original_train[original_train['Class']==0].sample(frac = 1)\noriginal_majorityclass = original_majorityclass[:3000]\noriginal_minorityclass = original_train[original_train['Class']==1]\n#original_train_US = pd.concat([original_majorityclass,original_minorityclass])\n#original_train_US = original_train_US.sample(frac = 1)\n#X_original_train_US = original_train_US.drop('Class',axis=1)\n#y_original_train_US = original_train_US['Class']","bb3c170c":"original_train_US = pd.concat([original_majorityclass,original_minorityclass])\noriginal_train_US = original_train_US.sample(frac = 1)\noriginal_train_US","d675d9c7":"X_original_train_US = original_train_US.drop('Class',axis=1)\ny_original_train_US = original_train_US['Class']","f02e057f":"model = LogisticRegression(solver='lbfgs', max_iter=1000)\nlr_sampled_US = model.fit(X_original_train_US,y_original_train_US)\n#lr_sampled_US_pred = model.predict(X_test_original_train_US)\n\nfrom sklearn.model_selection import KFold \nkfold = KFold(n_splits=5) \nvalidation_score = cross_val_score(model,X_original_train_US,y_original_train_US,cv=kfold)\n\nlr_originaltest_pred = model.predict(X_original_test)\n\nprint(classification_report(y_original_test,lr_originaltest_pred))\nprint(\"\")\nprint(confusion_matrix(y_original_test,lr_originaltest_pred))\n","85de0c5b":"results_originalstest.iloc[0,0] = accuracy_score(y_original_test, lr_originaltest_pred) # saving precision score for Logistic regression, make sure to name the y_pred different for different algorithms\nresults_originalstest.iloc[1,0] = recall_score(y_original_test, lr_originaltest_pred)\nresults_originalstest.iloc[2,0] = precision_score(y_original_test, lr_originaltest_pred)\nresults_originalstest.iloc[3,0] = f1_score(y_original_test, lr_originaltest_pred)\nresults_originalstest.iloc[4,0] = roc_auc_score(y_original_test, lr_originaltest_pred) \nresults_originalstest.iloc[5,0] = model.score(X_original_train_US, y_original_train_US) \nresults_originalstest.iloc[6,0] = model.score(X_original_test, y_original_test) \nresults_originalstest.iloc[7,0] = validation_score.mean() \n\nresults_originalstest","1d168cad":"model = SVC(kernel = 'linear')\nSVM_linear_sampled_US = model.fit(X_original_train_US,y_original_train_US)\n\nfrom sklearn.model_selection import KFold \nkfold = KFold(n_splits=5) \nvalidation_score = cross_val_score(model,X_original_train_US,y_original_train_US,cv=kfold)\n\nSVM_linear_originaltest_pred = model.predict(X_original_test)\n\nprint(classification_report(y_original_test,SVM_linear_originaltest_pred))\nprint(\"\")\nprint(confusion_matrix(y_original_test,SVM_linear_originaltest_pred))","7e7e9c2a":"results_originalstest.iloc[0,1] = accuracy_score(y_original_test, SVM_linear_originaltest_pred) # saving precision score for Logistic regression, make sure to name the y_pred different for different algorithms\nresults_originalstest.iloc[1,1] = recall_score(y_original_test, SVM_linear_originaltest_pred)\nresults_originalstest.iloc[2,1] = precision_score(y_original_test, SVM_linear_originaltest_pred)\nresults_originalstest.iloc[3,1] = f1_score(y_original_test, SVM_linear_originaltest_pred)\nresults_originalstest.iloc[4,1] = roc_auc_score(y_original_test, SVM_linear_originaltest_pred) \nresults_originalstest.iloc[5,1] = model.score(X_original_train_US, y_original_train_US) \nresults_originalstest.iloc[6,1] = model.score(X_original_test, y_original_test) \nresults_originalstest.iloc[7,1] = validation_score.mean() \n\nresults_originalstest","4a32939f":"model = SVC(kernel = 'rbf')\nSVM_rbf_sampled_US = model.fit(X_original_train_US,y_original_train_US)\n\nfrom sklearn.model_selection import KFold \nkfold = KFold(n_splits=5) \nvalidation_score = cross_val_score(model,X_original_train_US,y_original_train_US,cv=kfold)\n\nSVM_rbf_originaltest_pred = model.predict(X_original_test)\n\nprint(classification_report(y_original_test,SVM_rbf_originaltest_pred))\nprint(\"\")\nprint(confusion_matrix(y_original_test,SVM_rbf_originaltest_pred))","0f0aef00":"results_originalstest.iloc[0,2] = accuracy_score(y_original_test, SVM_rbf_originaltest_pred) # saving precision score for Logistic regression, make sure to name the y_pred different for different algorithms\nresults_originalstest.iloc[1,2] = recall_score(y_original_test, SVM_rbf_originaltest_pred)\nresults_originalstest.iloc[2,2] = precision_score(y_original_test, SVM_rbf_originaltest_pred)\nresults_originalstest.iloc[3,2] = f1_score(y_original_test, SVM_rbf_originaltest_pred)\nresults_originalstest.iloc[4,2] = roc_auc_score(y_original_test, SVM_rbf_originaltest_pred) \nresults_originalstest.iloc[5,2] = model.score(X_original_train_US, y_original_train_US) \nresults_originalstest.iloc[6,2] = model.score(X_original_test, y_original_test) \nresults_originalstest.iloc[7,2] = validation_score.mean() \n\nresults_originalstest","11ba12bb":"model = tree.DecisionTreeClassifier()\nDT_sampled_US = model.fit(X_original_train_US,y_original_train_US)\n\nfrom sklearn.model_selection import KFold \nkfold = KFold(n_splits=5) \nvalidation_score = cross_val_score(model,X_original_train_US,y_original_train_US,cv=kfold)\n\nDT_originaltest_pred = model.predict(X_original_test)\n\nprint(classification_report(y_original_test,DT_originaltest_pred))\nprint(\"\")\nprint(confusion_matrix(y_original_test,DT_originaltest_pred))","246d00fb":"results_originalstest.iloc[0,3] = accuracy_score(y_original_test, DT_originaltest_pred) # saving precision score for Logistic regression, make sure to name the y_pred different for different algorithms\nresults_originalstest.iloc[1,3] = recall_score(y_original_test, DT_originaltest_pred)\nresults_originalstest.iloc[2,3] = precision_score(y_original_test, DT_originaltest_pred)\nresults_originalstest.iloc[3,3] = f1_score(y_original_test, DT_originaltest_pred)\nresults_originalstest.iloc[4,3] = roc_auc_score(y_original_test, DT_originaltest_pred) \nresults_originalstest.iloc[5,3] = model.score(X_original_train_US, y_original_train_US) \nresults_originalstest.iloc[6,3] = model.score(X_original_test, y_original_test) \nresults_originalstest.iloc[7,3] = validation_score.mean() \n\nresults_originalstest","09d88d87":"from sklearn.naive_bayes import GaussianNB\nmodel = GaussianNB()\nGB_sampled_US = model.fit(X_original_train_US,y_original_train_US)\n\nfrom sklearn.model_selection import KFold \nkfold = KFold(n_splits=5) \nvalidation_score = cross_val_score(model,X_original_train_US,y_original_train_US,cv=kfold)\n\nGB_originaltest_pred = model.predict(X_original_test)\n\nprint(classification_report(y_original_test,GB_originaltest_pred))\nprint(\"\")\nprint(confusion_matrix(y_original_test,GB_originaltest_pred))","d0c05ce4":"results_originalstest.iloc[0,4] = accuracy_score(y_original_test, GB_originaltest_pred) # saving precision score for Logistic regression, make sure to name the y_pred different for different algorithms\nresults_originalstest.iloc[1,4] = recall_score(y_original_test, GB_originaltest_pred)\nresults_originalstest.iloc[2,4] = precision_score(y_original_test, GB_originaltest_pred)\nresults_originalstest.iloc[3,4] = f1_score(y_original_test, GB_originaltest_pred)\nresults_originalstest.iloc[4,4] = roc_auc_score(y_original_test, GB_originaltest_pred) \nresults_originalstest.iloc[5,4] = model.score(X_original_train_US, y_original_train_US) \nresults_originalstest.iloc[6,4] = model.score(X_original_test, y_original_test) \nresults_originalstest.iloc[7,4] = validation_score.mean() \n\nresults_originalstest","11ca8e9b":"#import pandas as pd\nfrom sklearn.neighbors import KNeighborsClassifier\nk_values_metrics_3000 = pd.DataFrame()\n\noriginal_majorityclass = original_train[original_train['Class']==0].sample(frac = 1)\noriginal_majorityclass = original_majorityclass[:3000]\noriginal_minorityclass = original_train[original_train['Class']==1]\noriginal_train_US = pd.concat([original_majorityclass,original_minorityclass])\noriginal_train_US = original_train_US.sample(frac = 1)\nX_original_train_US = original_train_US.drop('Class',axis=1)\ny_original_train_US = original_train_US['Class']\n\nfor i in list(range(1,21,4)):\n    model=KNeighborsClassifier(n_neighbors=i) \n    model.fit(X_original_train_US,y_original_train_US)\n    KNN_originaltest_pred= model.predict(X_original_test)\n    k_values_metrics_3000.loc[0,i] = accuracy_score(y_original_test, KNN_originaltest_pred)\n    k_values_metrics_3000.loc[1,i] = recall_score(y_original_test, KNN_originaltest_pred)\n    k_values_metrics_3000.loc[2,i] = precision_score(y_original_test, KNN_originaltest_pred)\n    k_values_metrics_3000.loc[3,i] = f1_score(y_original_test, KNN_originaltest_pred)\n\n\nprint(k_values_metrics_3000.set_index([pd.Index(['Accuracy', 'Recall', 'Precision','f1'])]).transpose())","7cf5ec81":"#we shall look at k-neighbors with highest values for recall, shall also consider precision and f1 score as very low precision means \n#more false negatives meaning more actual negatives(non-fradulent) classified as positives and transactions declined  \n#at k-value 39  \n#Accuracy 0.999168  \n#Recall 0.792683   \n#Precision 0.742857 \n#f-1 score 0.766962\n\n#Resampling majority dataset with multiple majority class population instead of current 3000 and checking again","9771b2d9":"original_majorityclass = original_train[original_train['Class']==0].sample(frac = 1)\noriginal_majorityclass = original_majorityclass[:1000]\noriginal_minorityclass = original_train[original_train['Class']==1]\noriginal_train_US = pd.concat([original_majorityclass,original_minorityclass])\noriginal_train_US = original_train_US.sample(frac = 1)\nX_original_train_US = original_train_US.drop('Class',axis=1)\ny_original_train_US = original_train_US['Class']\n\nk_values_metrics_1000 = pd.DataFrame()\nfor i in list(range(1,43,4)):\n    model=KNeighborsClassifier(n_neighbors=i) \n    model.fit(X_original_train_US,y_original_train_US)\n    KNN_originaltest_pred= model.predict(X_original_test)\n    k_values_metrics_1000.loc[0,i] = accuracy_score(y_original_test, KNN_originaltest_pred)\n    k_values_metrics_1000.loc[1,i] = recall_score(y_original_test, KNN_originaltest_pred)\n    k_values_metrics_1000.loc[2,i] = precision_score(y_original_test, KNN_originaltest_pred)\n    k_values_metrics_1000.loc[3,i] = f1_score(y_original_test, KNN_originaltest_pred)\n\nprint(k_values_metrics_1000.set_index([pd.Index(['Accuracy', 'Recall', 'Precision','f1'])]).transpose())\n","68596419":"original_majorityclass = original_train[original_train['Class']==0].sample(frac = 1)\noriginal_majorityclass = original_majorityclass[:6000]\noriginal_minorityclass = original_train[original_train['Class']==1]\noriginal_train_US = pd.concat([original_majorityclass,original_minorityclass])\noriginal_train_US = original_train_US.sample(frac = 1)\nX_original_train_US = original_train_US.drop('Class',axis=1)\ny_original_train_US = original_train_US['Class']\n\nk_values_metrics_6000 = pd.DataFrame()\nfor i in list(range(1,21,4)):\n    model=KNeighborsClassifier(n_neighbors=i) \n    model.fit(X_original_train_US,y_original_train_US)\n    KNN_originaltest_pred= model.predict(X_original_test)\n    k_values_metrics_6000.loc[0,i] = accuracy_score(y_original_test, KNN_originaltest_pred)\n    k_values_metrics_6000.loc[1,i] = recall_score(y_original_test, KNN_originaltest_pred)\n    k_values_metrics_6000.loc[2,i] = precision_score(y_original_test, KNN_originaltest_pred)\n    k_values_metrics_6000.loc[3,i] = f1_score(y_original_test, KNN_originaltest_pred)\n\nprint(k_values_metrics_6000.set_index([pd.Index(['Accuracy', 'Recall', 'Precision','f1'])]).transpose())","1de2a47d":"original_majorityclass = original_train[original_train['Class']==0].sample(frac = 1)\noriginal_majorityclass = original_majorityclass[:9000]\noriginal_minorityclass = original_train[original_train['Class']==1]\noriginal_train_US = pd.concat([original_majorityclass,original_minorityclass])\noriginal_train_US = original_train_US.sample(frac = 1)\nX_original_train_US = original_train_US.drop('Class',axis=1)\ny_original_train_US = original_train_US['Class']\n\nk_values_metrics_9000 = pd.DataFrame()\nfor i in list(range(1,9,2)):\n    model=KNeighborsClassifier(n_neighbors=i) \n    model.fit(X_original_train_US,y_original_train_US)\n    KNN_originaltest_pred= model.predict(X_original_test)\n    k_values_metrics_9000.loc[0,i] = accuracy_score(y_original_test, KNN_originaltest_pred)\n    k_values_metrics_9000.loc[1,i] = recall_score(y_original_test, KNN_originaltest_pred)\n    k_values_metrics_9000.loc[2,i] = precision_score(y_original_test, KNN_originaltest_pred)\n    k_values_metrics_9000.loc[3,i] = f1_score(y_original_test, KNN_originaltest_pred)\n\nprint(k_values_metrics_9000.set_index([pd.Index(['Accuracy', 'Recall', 'Precision','f1'])]).transpose())","2c95f56a":"original_majorityclass = original_train[original_train['Class']==0].sample(frac = 1)\noriginal_majorityclass = original_majorityclass[:11000]\noriginal_minorityclass = original_train[original_train['Class']==1]\noriginal_train_US = pd.concat([original_majorityclass,original_minorityclass])\noriginal_train_US = original_train_US.sample(frac = 1)\nX_original_train_US = original_train_US.drop('Class',axis=1)\ny_original_train_US = original_train_US['Class']\n\nk_values_metrics_11000 = pd.DataFrame()\nfor i in list(range(1,5)):\n    model=KNeighborsClassifier(n_neighbors=i) \n    model.fit(X_original_train_US,y_original_train_US)\n    KNN_originaltest_pred= model.predict(X_original_test_Re)\n    k_values_metrics_11000.loc[0,i] = accuracy_score(y_original_test, KNN_originaltest_pred)\n    k_values_metrics_11000.loc[1,i] = recall_score(y_original_test, KNN_originaltest_pred)\n    k_values_metrics_11000.loc[2,i] = precision_score(y_original_test, KNN_originaltest_pred)\n    k_values_metrics_11000.loc[3,i] = f1_score(y_original_test, KNN_originaltest_pred)\n\nprint(k_values_metrics_11000.set_index([pd.Index(['Accuracy', 'Recall', 'Precision','f1'])]).transpose())","b8192153":"X_original_test.shape","81b2af79":"# for 11000 majority class recall and f1 score taper at n=3, fairly high recall and f1 score\n# for 9000 majority class recall and f1 score taper n=3,5, fairly high recall and f1 score though not consistently\n# For 6000 majority class recall and f1 score taper at n=19\n# for 3000 majority class recall and f1 score taper at n=39\n# for 1000 majority class recall and f1 score taper at n=41","45cb351e":"LR_values_metrics = pd.DataFrame()","52e527df":"for i in list(range(1000,21000,2000)):\n    original_majorityclass = original_train[original_train['Class']==0].sample(frac = 1)\n    original_minorityclass = original_train[original_train['Class']==1]\n    original_majorityclass = original_majorityclass[:i]\n    original_train_US = pd.concat([original_majorityclass,original_minorityclass])\n    original_train_US = original_train_US.sample(frac = 1)\n    X_original_train_US = original_train_US.drop('Class',axis=1)\n    y_original_train_US = original_train_US['Class']\n    model=LogisticRegression()\n    model.fit(X_original_train_US,y_original_train_US)\n    LR_originaltest_pred= model.predict(X_original_test)\n    LR_values_metrics.loc[0,i] = accuracy_score(y_original_test, LR_originaltest_pred)\n    LR_values_metrics.loc[1,i] = recall_score(y_original_test, LR_originaltest_pred)\n    LR_values_metrics.loc[2,i] = precision_score(y_original_test, LR_originaltest_pred)\n    LR_values_metrics.loc[3,i] = f1_score(y_original_test, LR_originaltest_pred)\n    \nprint(LR_values_metrics.set_index([pd.Index(['Accuracy', 'Recall', 'Precision','f1'])]).transpose())\n# On multiple executions for above we observe to be receiving best recall and stabilized f1 score with majority class samples at 11000 ","74727355":"SVM_Linear_values_metrics = pd.DataFrame()\n\nfor i in list(range(1000,21000,2000)):\n    original_majorityclass = original_train[original_train['Class']==0].sample(frac = 1)\n    original_minorityclass = original_train[original_train['Class']==1]\n    original_majorityclass = original_majorityclass[:i]\n    original_train_US = pd.concat([original_majorityclass,original_minorityclass])\n    original_train_US = original_train_US.sample(frac = 1)\n    X_original_train_US = original_train_US.drop('Class',axis=1)\n    y_original_train_US = original_train_US['Class']\n    model = SVC(kernel = 'linear')\n    model.fit(X_original_train_US,y_original_train_US)\n    SVM_linear_originaltest_pred= model.predict(X_original_test)\n    SVM_Linear_values_metrics.loc[0,i] = accuracy_score(y_original_test, SVM_linear_originaltest_pred)\n    SVM_Linear_values_metrics.loc[1,i] = recall_score(y_original_test, SVM_linear_originaltest_pred)\n    SVM_Linear_values_metrics.loc[2,i] = precision_score(y_original_test, SVM_linear_originaltest_pred)\n    SVM_Linear_values_metrics.loc[3,i] = f1_score(y_original_test, SVM_linear_originaltest_pred)\n    \nprint(SVM_Linear_values_metrics.set_index([pd.Index(['Accuracy', 'Recall', 'Precision','f1'])]).transpose())\n# On multiple executions for above we observe to be receiving best recall and constant f1 score with majority class samples at 9000(recall)\/11000(f1 score) ","5ae858ce":"SVM_rbf_values_metrics = pd.DataFrame()\n\nfor i in list(range(1000,21000,2000)):\n    original_majorityclass = original_train[original_train['Class']==0].sample(frac = 1)\n    original_minorityclass = original_train[original_train['Class']==1]\n    original_majorityclass = original_majorityclass[:i]\n    original_train_US = pd.concat([original_majorityclass,original_minorityclass])\n    original_train_US = original_train_US.sample(frac = 1)\n    X_original_train_US = original_train_US.drop('Class',axis=1)\n    y_original_train_US = original_train_US['Class']\n    model = SVC(kernel = 'rbf')\n    model.fit(X_original_train_US,y_original_train_US)\n    SVM_rbf_originaltest_pred= model.predict(X_original_test)\n    SVM_rbf_values_metrics.loc[0,i] = accuracy_score(y_original_test, SVM_rbf_originaltest_pred)\n    SVM_rbf_values_metrics.loc[1,i] = recall_score(y_original_test, SVM_rbf_originaltest_pred)\n    SVM_rbf_values_metrics.loc[2,i] = precision_score(y_original_test, SVM_rbf_originaltest_pred)\n    SVM_rbf_values_metrics.loc[3,i] = f1_score(y_original_test, SVM_rbf_originaltest_pred)\n    \nprint(SVM_rbf_values_metrics.set_index([pd.Index(['Accuracy', 'Recall', 'Precision','f1'])]).transpose())\n# On multiple executions for above we observe to be receiving best recall and stabilized f1 score with majority class samples at 5000 ","df8ef8b1":"NaiveBayes_values_metrics = pd.DataFrame()\n\nfor i in list(range(1000,21000,2000)):\n    original_majorityclass = original_train[original_train['Class']==0].sample(frac = 1)\n    original_minorityclass = original_train[original_train['Class']==1]\n    original_majorityclass = original_majorityclass[:i]\n    original_train_US = pd.concat([original_majorityclass,original_minorityclass])\n    original_train_US = original_train_US.sample(frac = 1)\n    X_original_train_US = original_train_US.drop('Class',axis=1)\n    y_original_train_US = original_train_US['Class']\n    model = GaussianNB()\n    model.fit(X_original_train_US,y_original_train_US)\n    NB_originaltest_pred= model.predict(X_original_test)\n    NaiveBayes_values_metrics.loc[0,i] = accuracy_score(y_original_test, NB_originaltest_pred)\n    NaiveBayes_values_metrics.loc[1,i] = recall_score(y_original_test, NB_originaltest_pred)\n    NaiveBayes_values_metrics.loc[2,i] = precision_score(y_original_test, NB_originaltest_pred)\n    NaiveBayes_values_metrics.loc[3,i] = f1_score(y_original_test, NB_originaltest_pred)\n    \nprint(NaiveBayes_values_metrics.set_index([pd.Index(['Accuracy', 'Recall', 'Precision','f1'])]).transpose())\n# On multiple executions for above we observe we should not evaluate Naive Bayes model due to low scoring ","a232e322":"k_FE_values_metrics_11000 = pd.DataFrame()","cdd6531e":"#We shall check for LR,Knn,SVM Linear at 11000 majority and SVM-rbf at majority class>5000 if hyperparameters improve Recall and f1 scores\n# Before that checking on  LR,Knn if retaining important features(as discovered during feature engineering) improve recall and f1 scores\n#Shall retain features - V17,V14 ,V12 , V10  ,V18  ,V11  V16  V4   V7   V9   V26\noriginal_majorityclass = original_train[original_train['Class']==0].sample(frac = 1)\noriginal_majorityclass = original_majorityclass[:11000]\noriginal_minorityclass = original_train[original_train['Class']==1]\noriginal_train_US = pd.concat([original_majorityclass,original_minorityclass])\noriginal_train_US = original_train_US.sample(frac = 1)\nX_original_train_US = original_train_US.drop('Class',axis=1)\ny_original_train_US = original_train_US['Class']\nX_original_train_US = X_original_train_US[['V17','V14','V12' , 'V10'  ,'V18'  ,'V11', 'V16',  'V4', 'V7','V9','V26']]\nX_original_test = X_original_test[['V17','V14','V12' , 'V10'  ,'V18'  ,'V11', 'V16',  'V4', 'V7','V9','V26']]\n#this might be changing the fixed X_original_test permanently \nk_FE_values_metrics_11000 = pd.DataFrame()\nfor i in list(range(1,15,2)):\n    model=KNeighborsClassifier(n_neighbors=i) \n    model.fit(X_original_train_US,y_original_train_US)\n    KNN_FE_originaltest_pred= model.predict(X_original_test)\n    k_FE_values_metrics_11000.loc[0,i] = accuracy_score(y_original_test, KNN_FE_originaltest_pred)\n    k_FE_values_metrics_11000.loc[1,i] = recall_score(y_original_test, KNN_FE_originaltest_pred)\n    k_FE_values_metrics_11000.loc[2,i] = precision_score(y_original_test, KNN_FE_originaltest_pred)\n    k_FE_values_metrics_11000.loc[3,i] = f1_score(y_original_test, KNN_FE_originaltest_pred)\n\nprint(k_FE_values_metrics_11000.set_index([pd.Index(['Accuracy', 'Recall', 'Precision','f1'])]).transpose())\n#no significant improvements for Knn through feature engineering, will check the same for Logisitc regression","4ad93b54":"LR_FE_values_metrics = pd.DataFrame()","fb4f3b47":"for i in list(range(1000,21000,2000)):\n    original_majorityclass = original_train[original_train['Class']==0].sample(frac = 1)\n    original_minorityclass = original_train[original_train['Class']==1]\n    original_majorityclass = original_majorityclass[:i]\n    original_train_US = pd.concat([original_majorityclass,original_minorityclass])\n    original_train_US = original_train_US.sample(frac = 1)\n    X_original_train_US = original_train_US.drop('Class',axis=1)\n    y_original_train_US = original_train_US['Class']\n    X_original_train_US = X_original_train_US[['V17','V14','V12' , 'V10'  ,'V18'  ,'V11', 'V16',  'V4', 'V7','V9','V26']]\n    X_original_test = X_original_test[['V17','V14','V12' , 'V10'  ,'V18'  ,'V11', 'V16',  'V4', 'V7','V9','V26']]\n    #this might be changing the fixed X_original_test permanently \n    model=LogisticRegression()\n    model.fit(X_original_train_US,y_original_train_US)\n    LR_FE_originaltest_pred= model.predict(X_original_test)\n    LR_FE_values_metrics.loc[0,i] = accuracy_score(y_original_test, LR_FE_originaltest_pred)\n    LR_FE_values_metrics.loc[1,i] = recall_score(y_original_test, LR_FE_originaltest_pred)\n    LR_FE_values_metrics.loc[2,i] = precision_score(y_original_test, LR_FE_originaltest_pred)\n    LR_FE_values_metrics.loc[3,i] = f1_score(y_original_test, LR_FE_originaltest_pred)\n    \nprint(LR_FE_values_metrics.set_index([pd.Index(['Accuracy', 'Recall', 'Precision','f1'])]).transpose())\n#definite improvement in f1 score(by 5-6%) but nothing on recall, i.e imporvement in precision score, will check for SVM Linear and rbf as well","a31a78db":"SVM_FE_Linear_values_metrics = pd.DataFrame()\n\nfor i in list(range(1000,21000,2000)):\n    original_majorityclass = original_train[original_train['Class']==0].sample(frac = 1)\n    original_minorityclass = original_train[original_train['Class']==1]\n    original_majorityclass = original_majorityclass[:i]\n    original_train_US = pd.concat([original_majorityclass,original_minorityclass])\n    original_train_US = original_train_US.sample(frac = 1)\n    X_original_train_US = original_train_US.drop('Class',axis=1)\n    y_original_train_US = original_train_US['Class']\n    X_original_train_US = X_original_train_US[['V17','V14','V12' , 'V10'  ,'V18'  ,'V11', 'V16',  'V4', 'V7','V9','V26']]\n    X_original_test = X_original_test[['V17','V14','V12' , 'V10'  ,'V18'  ,'V11', 'V16',  'V4', 'V7','V9','V26']]\n    model = SVC(kernel = 'linear')\n    model.fit(X_original_train_US,y_original_train_US)\n    SVM_FE_linear_originaltest_pred= model.predict(X_original_test)\n    SVM_FE_Linear_values_metrics.loc[0,i] = accuracy_score(y_original_test, SVM_FE_linear_originaltest_pred)\n    SVM_FE_Linear_values_metrics.loc[1,i] = recall_score(y_original_test, SVM_FE_linear_originaltest_pred)\n    SVM_FE_Linear_values_metrics.loc[2,i] = precision_score(y_original_test, SVM_FE_linear_originaltest_pred)\n    SVM_FE_Linear_values_metrics.loc[3,i] = f1_score(y_original_test, SVM_FE_linear_originaltest_pred)\n    \nprint(SVM_FE_Linear_values_metrics.set_index([pd.Index(['Accuracy', 'Recall', 'Precision','f1'])]).transpose())\n#no significant improvements for SVM through feature engineering","de24d75b":"#Checking hyperparameter tuning improves the overall recall - LR(with feature engineering), Knn(original dataset) and then SVM's(original dataset)\nlog_reg_params = {\"penalty\": ['l1', 'l2'], 'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000]}\n\nfrom sklearn.model_selection import GridSearchCV\n\noriginal_majorityclass = original_train[original_train['Class']==0].sample(frac = 1)\noriginal_minorityclass = original_train[original_train['Class']==1]\noriginal_majorityclass = original_majorityclass[:11000]\noriginal_train_US = pd.concat([original_majorityclass,original_minorityclass])\noriginal_train_US = original_train_US.sample(frac = 1)\nX_original_train_US = original_train_US.drop('Class',axis=1)\ny_original_train_US = original_train_US['Class']\nX_original_train_US = X_original_train_US[['V17','V14','V12' , 'V10'  ,'V18'  ,'V11', 'V16',  'V4', 'V7','V9','V26']]\nX_original_test = X_original_test[['V17','V14','V12' , 'V10'  ,'V18'  ,'V11', 'V16',  'V4', 'V7','V9','V26']]\n\ngrid_log_reg = GridSearchCV(LogisticRegression(), log_reg_params)\ngrid_log_reg.fit(X_original_train_US, y_original_train_US)\n# We automatically get the logistic regression with the best parameters.\nlog_reg = grid_log_reg.best_estimator_\n\n\nlog_reg","16acb591":"#X_original_train_US = X_original_train_US[['V17','V14','V12' , 'V10'  ,'V18'  ,'V11', 'V16',  'V4', 'V7','V9','V26']]\n#X_original_test = X_original_test[['V17','V14','V12' , 'V10'  ,'V18'  ,'V11', 'V16',  'V4', 'V7','V9','V26']]","835b49dd":"LR_FE_HP_values_metrics = pd.DataFrame()\n#calculating outputs for best hyperparameters for Logistic regression\nmodel=LogisticRegression(C=0.1)\nmodel.fit(X_original_train_US,y_original_train_US)\nLR_FE_HP_originaltest_pred= model.predict(X_original_test)\nLR_FE_HP_values_metrics.loc[0,11000] = accuracy_score(y_original_test, LR_FE_HP_originaltest_pred)\nLR_FE_HP_values_metrics.loc[1,11000] = recall_score(y_original_test, LR_FE_HP_originaltest_pred)\nLR_FE_HP_values_metrics.loc[2,11000] = precision_score(y_original_test, LR_FE_HP_originaltest_pred)\nLR_FE_HP_values_metrics.loc[3,11000] = f1_score(y_original_test, LR_FE_HP_originaltest_pred)\n    \nprint(LR_FE_HP_values_metrics.set_index([pd.Index(['Accuracy', 'Recall', 'Precision','f1'])]).transpose())\nprint('')\nprint(classification_report(y_original_test, LR_FE_HP_originaltest_pred))\nprint('')\nprint(confusion_matrix(y_original_test, LR_FE_HP_originaltest_pred))","66732775":"# 81%  best recall score  and 75 % f1 score for feature engineer and hyperparameter tuned(c=0.1) at 11000 majority  class\n##Tuning hyperparamter for knn without Feature engineered dataset\nknears_params = {\"n_neighbors\": list(range(2,5,1)), 'algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute']}\n\noriginal_majorityclass = original_train[original_train['Class']==0].sample(frac = 1)\noriginal_minorityclass = original_train[original_train['Class']==1]\noriginal_majorityclass = original_majorityclass[:11000]\noriginal_train_US = pd.concat([original_majorityclass,original_minorityclass])\noriginal_train_US = original_train_US.sample(frac = 1)\nX_original_train_US = original_train_US.drop('Class',axis=1)\ny_original_train_US = original_train_US['Class']\n\ngrid_knears = GridSearchCV(KNeighborsClassifier(), knears_params)\ngrid_knears.fit(X_original_train_US,y_original_train_US)\n# KNears best estimator\nknears_neighbors = grid_knears.best_estimator_\n\nknears_neighbors\n#no hyperpameters bring significat change. Best recall = 83.5% and f1 score 67% from before at nearest neighbors=3","60cff8c8":"svc_params = {'C': [0.5, 0.7, 0.9, 1]}\n\noriginal_majorityclass = original_train[original_train['Class']==0].sample(frac = 1)\noriginal_minorityclass = original_train[original_train['Class']==1]\noriginal_majorityclass = original_majorityclass[:9000]\noriginal_train_US = pd.concat([original_majorityclass,original_minorityclass])\noriginal_train_US = original_train_US.sample(frac = 1)\nX_original_train_US = original_train_US.drop('Class',axis=1)\ny_original_train_US = original_train_US['Class']\n\ngrid_svc = GridSearchCV(SVC(), svc_params)\ngrid_svc.fit(X_original_train_US, y_original_train_US)\n# SVC best estimator\nsvc = grid_svc.best_estimator_\nsvc","8da5dbbe":"#Since we are testing this on non engineered dataset will have to take test dataset that has not been feature engineered,\n#i,e X_original_test_Re\n\nSVM_HP_Linear_values_metrics=pd.DataFrame()\nmodel = SVC(C=0.5, kernel='linear')\nmodel.fit(X_original_train_US,y_original_train_US)\nSVM_HP_linear_originaltest_pred= model.predict(X_original_test_Re)\nSVM_HP_Linear_values_metrics.loc[0,1] = accuracy_score(y_original_test, SVM_HP_linear_originaltest_pred)\nSVM_HP_Linear_values_metrics.loc[1,1] = recall_score(y_original_test, SVM_HP_linear_originaltest_pred)\nSVM_HP_Linear_values_metrics.loc[2,1] = precision_score(y_original_test, SVM_HP_linear_originaltest_pred)\nSVM_HP_Linear_values_metrics.loc[3,1] = f1_score(y_original_test, SVM_HP_linear_originaltest_pred)\n\n\nprint(SVM_HP_Linear_values_metrics.set_index([pd.Index(['Accuracy', 'Recall', 'Precision','f1'])]).transpose())\nprint(classification_report(y_original_test, SVM_HP_linear_originaltest_pred))\nprint('')\nprint(confusion_matrix(y_original_test, SVM_HP_linear_originaltest_pred))","e3732bb5":"svc_rbf_params = {'C': [0.5, 0.7, 0.9, 1], 'gamma': [0.001, 0.01, 0.1, 1,10,100]}\n\noriginal_majorityclass = original_train[original_train['Class']==0].sample(frac = 1)\noriginal_minorityclass = original_train[original_train['Class']==1]\noriginal_majorityclass = original_majorityclass[:5000]\noriginal_train_US = pd.concat([original_majorityclass,original_minorityclass])\noriginal_train_US = original_train_US.sample(frac = 1)\nX_original_train_US = original_train_US.drop('Class',axis=1)\ny_original_train_US = original_train_US['Class']\n\ngrid_svc = GridSearchCV(SVC(kernel = 'rbf'), svc_rbf_params)\ngrid_svc.fit(X_original_train_US, y_original_train_US)\n# SVC best estimator\nsvc_rbf = grid_svc.best_estimator_\nsvc_rbf","09e6e1ed":"\nSVM_HP_rbf_values_metrics=pd.DataFrame()\nmodel = SVC(C=1,gamma=0.01, kernel='rbf')\nmodel.fit(X_original_train_US,y_original_train_US)\nSVM_HP_rbf_originaltest_pred= model.predict(X_original_test_Re)\nSVM_HP_rbf_values_metrics.loc[0,1] = accuracy_score(y_original_test, SVM_HP_rbf_originaltest_pred)\nSVM_HP_rbf_values_metrics.loc[1,1] = recall_score(y_original_test, SVM_HP_rbf_originaltest_pred)\nSVM_HP_rbf_values_metrics.loc[2,1] = precision_score(y_original_test, SVM_HP_rbf_originaltest_pred)\nSVM_HP_rbf_values_metrics.loc[3,1] = f1_score(y_original_test, SVM_HP_rbf_originaltest_pred)\n\n\nprint(SVM_HP_rbf_values_metrics.set_index([pd.Index(['Accuracy', 'Recall', 'Precision','f1'])]).transpose())\nprint(classification_report(y_original_test, SVM_HP_rbf_originaltest_pred))\nprint('')\nprint(confusion_matrix(y_original_test, SVM_HP_rbf_originaltest_pred))","fbd0de03":"#Concluding model is able to generalize best at majority samples with 9000-11000 and minority original\n#and knn appears to be the best fitting at n=3 gives 3   Accuracy 0.998905  Recall 0.841463  Precision 0.638889  and f1 score 0.726316 \n#implying highest recall and better f1 score ","c09a3d88":"knn_params = {'leaf_size': list(range(1,25)), 'n_neighbors' : list(range(1,9,2)),'p':[1,2],'algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute']}\noriginal_majorityclass = original_train[original_train['Class']==0].sample(frac = 1)\noriginal_minorityclass = original_train[original_train['Class']==1]\noriginal_majorityclass = original_majorityclass[:11000]\noriginal_train_US = pd.concat([original_majorityclass,original_minorityclass])\noriginal_train_US = original_train_US.sample(frac = 1)\nX_original_train_US = original_train_US.drop('Class',axis=1)\ny_original_train_US = original_train_US['Class']\n\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.model_selection import GridSearchCV\n\nmodel=KNeighborsClassifier() \n#model.fit(X_original_train_US,y_original_train_US)\ngrid_knn = GridSearchCV(KNeighborsClassifier(), knn_params)\ngrid_knn.fit(X_original_train_US, y_original_train_US)\n# grid best estimator\nknn_best = grid_knn.best_estimator_\nknn_best","516b6716":"knn_HP_values_metrics=pd.DataFrame()\nmodel = KNeighborsClassifier(leaf_size=2, n_neighbors=3, p=1)\nmodel.fit(X_original_train_US,y_original_train_US)\nknn_HP_originaltest_pred= model.predict(X_original_test)\nknn_HP_values_metrics.loc[0,1] = accuracy_score(y_original_test, knn_HP_originaltest_pred)\nknn_HP_values_metrics.loc[1,1] = recall_score(y_original_test, knn_HP_originaltest_pred)\nknn_HP_values_metrics.loc[2,1] = precision_score(y_original_test, knn_HP_originaltest_pred)\nknn_HP_values_metrics.loc[3,1] = f1_score(y_original_test, knn_HP_originaltest_pred)\nknn_HP_values_metrics.loc[4,1] = model.score(X_original_train_US, y_original_train_US) \nknn_HP_values_metrics.loc[5,1] = model.score(X_original_test, y_original_test)\n\nprint(knn_HP_values_metrics.set_index([pd.Index(['Accuracy', 'Recall', 'Precision','f1','TrainingScore','TestScore'])]).transpose())\nprint(classification_report(y_original_test, knn_HP_originaltest_pred))\nprint('')\nprint(confusion_matrix(y_original_test, knn_HP_originaltest_pred))","b1044583":"#SO knn(n=3) generalizes well and provides highest recall and decent f1 score for undersampled data(tapers at undersampled \n# majority samples=11000) ","67b8e97f":"*There are no multocollinear features and this might also be due to high imbalance in our dataset*****. We will check the collinearity as well as correlation in the correlation matrix","4406fd39":"V1 to V28 - Features are already scaled, while amount and time require scaling","1c78658b":"Dataset is heavily unbalanced","bcf4e936":"As can be seen the fraud values do not comprise of very high values","8c4c3780":"Correlation with Class variable\n\nPositively correlated: V2, V4, V11, V19\nNegatively correlated: V3,V9,V10,V12,V14,V16,V17,V18\n\nDistribution based variable feature selection: V1,V2,V3,V4, V6,V7, V8,V9,V10,V12, V14, V16,V17,V18,V19,V21\n\nrfc- Feature importance - V17,V14 ,V12 , V10  ,V18  ,V11  V16  V4   V7   V9   V26  V21  V6   V5   V15\n\nShall retain features - V17,V14 ,V12 , V10  ,V18  ,V11  V16  V4   V7   V9   V26","4705c43b":"No coliinearity in Undersampled data as well,except for in Amount but we will not skip the Amount feature","421bea33":"Plan:\n1. Check for missing values, convert features to relevant feature types(here converting time feature to category)\n2. Check for individual graphs and relationship with dependent variable\n3. Feature engineer to identify important features and drop others(that are not useful or are multicollinear) - Use Random forest, distribution check, correlation matrix(as this is very biased dataset use undersampled dataset to get clear understanding of the correlation matrix) and VIF\n4. Create undersample dataset\n5. Create oversample dataset (later if needed) \n6. Split normal dataset(use stratified split), scale amount and time features\n7. Split undersample dataset(undersample majority class and stratified split), scale amount and time features\n8. Train models on undersample dataset for different techniques(use k-fold split technique), check for training score\n9. Test model on undersample dataset,check for overfitting and underfitting by comparing training and test scores, find recall score, confusion matrix, f1 score, ROC-AUC and accuracy score\n10. Train undersampled models after hyper-parameter tuning and find the best paramters\n11. Train models on normal dataset for different techniques(use k-fold split technique), check for training score\n12. Test model on normal dataset,check for overfitting and underfitting by comparing training and test scores, find recall score, confusion matrix, f1 score, ROC-AUC and accuracy score","42bf7a4b":"From the above distributions checking that we can drop features that do not show significance for different classes: V5, V8, V13, V15, 'V28','V27','V26','V25','V24','V23','V22','V20'.\nWill check with random forest feature importance if above claim is supported.\nWe shall retain V1,V2,V3,V4, V6,V7, V8,V9,V10,V12, V14, V16,V17,V18,V19,V21","50ec922c":"Correlation matrix is not very clear as it couldn't generalize on the details due to unbalanced dataset"}}