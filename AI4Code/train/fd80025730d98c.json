{"cell_type":{"332be9b4":"code","7ff8e93d":"code","f03690fd":"code","6c608529":"code","90171a71":"code","2386f201":"code","0b2d0d51":"code","9f8609f6":"code","acd83912":"code","227a70e4":"code","eb2a0bfc":"code","30b18d2d":"code","36c02451":"code","deab8538":"code","52c5a364":"code","9a619e43":"code","5ea58a24":"code","3588eee2":"code","7fef968b":"markdown","ac84b2e9":"markdown","5dcccd91":"markdown","58a097f3":"markdown","3e90c0c8":"markdown","7b9e3cdb":"markdown","10b640a6":"markdown","b1e271e0":"markdown","9b6da1ea":"markdown","de42c83a":"markdown","cea9ac37":"markdown","a79f8120":"markdown","65046f7c":"markdown","36d8ab30":"markdown"},"source":{"332be9b4":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\n        \n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","7ff8e93d":"import cv2\nimport tensorflow as tf\nfrom tqdm import tqdm\nimport os\nimport numpy as np\n\ntrain_dir = \"..\/input\/asl-alphabet\/asl_alphabet_train\/asl_alphabet_train\/\"\ntest_dir =  \"..\/input\/asl-alphabet\/asl_alphabet_test\/asl_alphabet_test\/\"\nIMG_SIZE = 50\nlabels_map = {'A':0,'B':1,'C': 2, 'D': 3, 'E':4,'F':5,'G':6, 'H': 7, 'I':8, 'J':9,'K':10,'L':11, 'M': 12, 'N': 13, 'O':14, \n                'P':15,'Q':16, 'R': 17, 'S': 18, 'T':19, 'U':20,'V':21, 'W': 22, 'X': 23, 'Y':24, 'Z':25, \n                'del': 26, 'nothing': 27,'space':28}","f03690fd":"def create_train_data():\n    x_train = []\n    y_train = []\n    for folder_name in os.listdir(train_dir):\n        label = labels_map[folder_name]\n        for image_filename in tqdm(os.listdir(train_dir + folder_name)):\n            path = os.path.join(train_dir,folder_name,image_filename)\n            img = cv2.resize(cv2.imread(path, cv2.IMREAD_GRAYSCALE),(IMG_SIZE, IMG_SIZE ))\n            x_train.append(np.array(img))\n            y_train.append(np.array(label))\n    print(\"Done creating train data\")\n    return x_train, y_train\n","6c608529":"def create_test_data():\n    x_test = []\n    y_test = []\n    for folder_name in os.listdir(test_dir):\n        label = folder_name.replace(\"_test.jpg\",\"\")\n        label = labels_map[label]\n        path = os.path.join(test_dir,folder_name)\n        img = cv2.resize(cv2.imread(path, cv2.IMREAD_GRAYSCALE),(IMG_SIZE, IMG_SIZE ))\n        x_test.append(np.array(img))\n        y_test.append(np.array(label))\n    print(\"Done creating test data\")\n    return x_test,y_test","90171a71":"x_train, y_train= create_train_data()  \nx_test,y_test = create_test_data()","2386f201":"num_features = 2500\nnum_classes = 29\n\nx_train, x_test = np.array(x_train, np.float32), np.array(x_test, np.float32)\nx_train, x_test = x_train.reshape([-1, num_features]), x_test.reshape([-1, num_features])\nx_train, x_test = x_train \/ 255., x_test \/ 255.","0b2d0d51":"%matplotlib inline\nimport matplotlib.pyplot as plt\n\ndef display_image(num):\n    label = y_train[num]\n    plt.title('Label: %d' % (label))\n    image = x_train[num].reshape([IMG_SIZE,IMG_SIZE])\n    plt.imshow(image, cmap=plt.get_cmap('gray_r'))\n    plt.show()\ndisplay_image(5)","9f8609f6":"# Training parameters.\nlearning_rate = 0.001\ntraining_steps = 5000\nbatch_size = 250\ndisplay_step = 500\n\n# Network parameters.\nn_hidden =  300# Number of neurons.","acd83912":"# Use tf.data API to shuffle and batch data.\ntrain_data = tf.data.Dataset.from_tensor_slices((x_train, y_train))\ntrain_data = train_data.repeat().shuffle(87000).batch(batch_size).prefetch(1)","227a70e4":"# Store layers weight & bias\n\n# A random value generator to initialize weights initially\nrandom_normal = tf.initializers.RandomNormal()\n\nweights = {\n    'h1': tf.Variable(random_normal([num_features, n_hidden])),\n    'h2': tf.Variable(random_normal([n_hidden, n_hidden])),\n    'out': tf.Variable(random_normal([n_hidden, num_classes]))\n}\nbiases = {\n    'b': tf.Variable(tf.zeros([n_hidden])),\n    'out': tf.Variable(tf.zeros([num_classes]))\n}","eb2a0bfc":"def neural_nets(input_data):\n    hidden_layer1 = tf.add(tf.matmul(input_data,weights['h1']),biases['b'])\n    hidden_layer1 = tf.nn.sigmoid(hidden_layer1)\n    \n    hidden_layer2 = tf.add(tf.matmul(hidden_layer1,weights['h2']),biases['b'])\n    hidden_layer2 = tf.nn.sigmoid(hidden_layer2)\n    \n    out_layer = tf.add(tf.matmul(hidden_layer1,weights['out']),biases['out'])\n    \n    return tf.nn.softmax(out_layer)","30b18d2d":"def cross_entropy(y_pred, y_true):\n    # Encode label to a one hot vector.\n    y_true = tf.one_hot(y_true, depth=num_classes)\n    # Clip prediction values to avoid log(0) error.\n    y_pred = tf.clip_by_value(y_pred, 1e-9, 1.)\n    # Compute cross-entropy.\n    return tf.reduce_mean(-tf.reduce_sum(y_true * tf.math.log(y_pred)))","36c02451":"optimizer = tf.keras.optimizers.SGD(learning_rate)\n\ndef run_optimization(x, y):\n    # Wrap computation inside a GradientTape for automatic differentiation.\n    with tf.GradientTape() as g:\n        pred = neural_nets(x)\n        loss = cross_entropy(pred, y)\n        \n    # Variables to update, i.e. trainable variables.\n    trainable_variables = list(weights.values()) + list(biases.values())\n\n    # Compute gradients.\n    gradients = g.gradient(loss, trainable_variables)\n    \n    # Update W and b following gradients.\n    optimizer.apply_gradients(zip(gradients, trainable_variables))","deab8538":"def accuracy(y_pred, y_true):\n    # Predicted class is the index of highest score in prediction vector (i.e. argmax).\n    #print(\"argmax:\",tf.argmax(y_pred,1))\n    #print(\"cast\",tf.cast(y_true, tf.int64))\n    correct_prediction = tf.equal(tf.argmax(y_pred, 1), tf.cast(y_true, tf.int64))\n    return tf.reduce_mean(tf.cast(correct_prediction, tf.float32), axis=-1)","52c5a364":"# Run training for the given number of steps.\nfor step, (batch_x, batch_y) in enumerate(train_data.take(training_steps), 1):\n    # Run the optimization to update W and b values.\n    run_optimization(batch_x, batch_y)\n    \n    if step % display_step == 0:\n        pred = neural_nets(batch_x)\n        loss = cross_entropy(pred, batch_y)\n        acc = accuracy(pred, batch_y)\n        print(\"Training epoch: %i, Loss: %f, Accuracy: %f\" % (step, loss, acc))","9a619e43":"# Test model on validation set.\npred = neural_nets(x_test)\nprint(\"Test Accuracy: %f\" % accuracy(pred, y_test))","5ea58a24":"def get_key(val):\n    for key, value in labels_map.items(): \n         if val == value: \n            return key ","3588eee2":"n_images = 28\npredictions = neural_nets(x_test)\nfor i in range(n_images):\n    model_prediction = np.argmax(predictions.numpy()[i])\n    plt.imshow(np.reshape(x_test[i], [50, 50]), cmap='gray_r')\n    plt.show()\n    print(\"Original Labels: %s\" % get_key(y_test[i]))\n    print(\"Model prediction: %s\" % get_key(model_prediction))","7fef968b":"## Defining our loss function and SGD Optimizer\n>   <h4>\ud83d\udcdd COMMENTS<\/h4>\n>    <li>The loss function for measuring the progress in gradient descent: <b>cross entropy<\/b>. It uses a logrithamic scale that penalizes incorrect classification more than the ones that are close.\n>    <li> Finally we set up the <b>stocashtic gradient descent optimizer<\/b>. The gradient tape is a TensorFlow API that does <b>reverse mode auto differentiation<\/b>. It's the new way of setting up neural nets from scratch in Tensorflow 2.\n>   <li> We define a function to measure the accuracy of the neural network. It does this by selecting the class with the highest probability and matching it to the true class.\n","ac84b2e9":"## Results","5dcccd91":"## Determining training and network parameters\n >   <h4>\ud83d\udcdd COMMENTS<\/h4>\n > <li>These parameters or \"hyperparameters\" are ones we have to experiment with to improve upon the <b>accuracy<\/b> of the neural network. A little tweak could result in a huge difference!\n    >  <li> <b>learning rate<\/b> - controls how much to change the model in response to the estimated error each time the model weights are updated. \n    >  <li> <b> training steps<\/b> - number of epochs\n    >  <li> <b> batch size<\/b> - small chunks for each iteration of training\n    >  <li><b> n hidden<\/b> - number of neurons in the hidden layer\n >  <li> We use tf.data API to shuffle the data and divide it into batches\n","58a097f3":"## Constructing the neural network\n>   <h4>\ud83d\udcdd COMMENTS<\/h4>\n>   <li>We start by initializing weights randomly for each layer in the neural network. We use the RandomNormal API to do so.\n>   <li> We can use bias to allow the activation function to be shifted to the left or right, to better fit the data. Bias makes it easier for the neural networks to fire. The biases are initialized to zero and are learnt during training.\n>   <li> The neural network consist of 2 hidden layers, with 300 neurons in each layer and one output layer followed by a <b>softmax<\/b> function that converts the weights of the neural network into probabilities for each class. Softmax helps turn the weights into probabilities and make it a classification problem.","3e90c0c8":"> <h4> \ud83d\udcca My insight<\/h4>\n> <li>The highest accuracy I have got is <b>1.0<\/b> meaning all the images have been correctly classified in the test set.\nThis might change each time we train the model because each time we start with different random weights and biases.\n> <li> We can tweak the hyperparameters and the layers and neurons in the network to try out different topologies to see if they give better results.\n","7b9e3cdb":"## Validating the model","10b640a6":"Don't forget to <b>upvote<\/b> if you found this notebook insightful!","b1e271e0":"## Objective \u2692\n> In this notebook, we will interpret ASL images using TensorFlow. Things we will cover:\n> <h4>1. Create the train and test data\n> <h4>2. Determine training and network parameters\n> <h4>3. Construct the neural network\n> <h4>4. Define our loss function and SGD Optimizer\n> <h4>5. Train the neural network \n> <h4>6. Validate the model\n> <h4>7. Results<\/h4>\n> <p>I will also be giving my insight every step of the way! Let's dive in!<\/p>\n\n## What is TensorFlow? \n![68747470733a2f2f7777772e74656e736f72666c6f772e6f72672f696d616765732f74665f6c6f676f5f686f72697a6f6e74616c2e706e67.png](attachment:68747470733a2f2f7777772e74656e736f72666c6f772e6f72672f696d616765732f74665f6c6f676f5f686f72697a6f6e74616c2e706e67.png)\n> <li>Before we get into the programming bit, let us establish what <b>TensorFlow<\/b> is -  It is a Python library that allows you to create neural networks with many layers. It is free and open source and was developed by Google.\n> <li>Although there are neural networks that are specifically well suited for image recognition, we actually don't need to go there for this relatively simple task.\n> <li>There are libraries like Keras and TFLearn that run on top of TensorFlow.These are higher level API's to TensorFlow\n  but for this dataset, we can do without any of these libraries. \n    \n \n","9b6da1ea":"![1200px-American_Sign_Language_ASL.svg%20%281%29.png](attachment:1200px-American_Sign_Language_ASL.svg%20%281%29.png)","de42c83a":"## Create the train and test data\n>    <h4>\ud83d\udcdd COMMENTS<\/h4>\n>    Before we can work on the data, let's start off by importing the necessary libraries.\n>    <li> <b>cv2<\/b> - We use cv2 to load image from a specified file and also resize it to the desired pixel size\n>    <li> <b>tensorflow<\/b> - This library will help us construct and train a neural network that will do the classification for us\n>    <li> <b>tqdm<\/b> - a smart progress meter\n>    <li> The training set contains 87,000 images which are 200x200 pixels.There are 29 classes, of which 26 are for the letters A-Z and 3 classes for SPACE, DELETE and NOTHING.\n>    <li> While reading the images, I have resized it to 50x 50 pixels. The labels_map dictionary maps each of the 29 classes to a corresponding number\n","cea9ac37":"## Training the neural network \n>   <h4>\ud83d\udcdd COMMENTS<\/h4>\n    > <li> Now that we have everything set up, let's try to run it!\n    > <li> We train the model in 4000 epochs or training steps. At each step we run the optimization function on a small chunk of training data 250 records, in our case\n    > <li> Every 100 steps we display the current <b>Loss function and Accuracy<\/b> of the model","a79f8120":">   <h4>\ud83d\udcdd COMMENTS<\/h4>\n>   <li>The training images are therefore a tensor of shape [87,000, 25000]. The training labels are a one-dimensional tensor of 87,000 labels that range from 0 to 28.\n>   <li>  Let's get a feel of what the images look like compressed to 50 x 50 pixels and in gray scale. Remember we have compressed it from 200 x 200 to 50 x 50. Looking at the image below it is a bit pixellated; but still looks good!  ","65046f7c":"> <h4>\ud83d\udccaMy insight<\/h4>\n    ><li>Let's take a look at some of the test images and see just how good or bad our model is, compared to what your own brain can do. \n    ><li> Our model has done excellent job in classifying the images to the right classes. And we are very happy with this simple neural network for now, it seems to do it's job. In future notebooks, we will dive deeper into more sofisticated neural networks for harder problems. We will also make use of the in-built API's Python provides like Keras and TFLearn. <\/li>","36d8ab30":">    <h4>\ud83d\udcdd COMMENTS<\/h4>\n>  <li> The number of features are 25000 [50 x50 pixels] i.e considering each pixel as a feature for the image.We reshape the images into 1D arrays of 2500 pixels. Each one of those values will be an input node into our deep neural network. We also normalize the inputs to be a value between 0 and 1 (inclusive)\n> <li>The number of class are 29, which means there will be 29 neurons in the output layer each representing an output class A-Z, SPACE, NOTHING or DELETE "}}