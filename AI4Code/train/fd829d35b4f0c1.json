{"cell_type":{"be007a95":"code","31dc34f6":"code","3716b5ba":"code","1421e261":"code","1efd7eb7":"code","39c905c7":"code","0069bd61":"code","83625751":"code","82c19731":"code","38fba446":"code","8af28a9b":"code","fc1e73aa":"code","74efce26":"code","accb4581":"code","e585ec4b":"code","59b1f7e4":"code","d76ef35f":"code","b6076a82":"code","ddc3a23d":"code","7cd26c69":"code","286d2154":"code","84e698c7":"markdown","3146bec7":"markdown","ec5784b2":"markdown","8bcef21b":"markdown","294c6967":"markdown","185ddacb":"markdown","558601f0":"markdown","53f07ece":"markdown","7b399d82":"markdown","4d435ab0":"markdown","0921f10d":"markdown","e70e5d28":"markdown","6bdc3912":"markdown","079eb3e1":"markdown"},"source":{"be007a95":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport plotly.express as px\n\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder\nfrom sklearn.metrics import classification_report, confusion_matrix\nfrom sklearn.model_selection import train_test_split\n\nfrom xgboost import XGBClassifier,XGBRFClassifier\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier","31dc34f6":"from mlxtend.preprocessing import TransactionEncoder\nfrom mlxtend.frequent_patterns import apriori\nfrom mlxtend.frequent_patterns import association_rules","3716b5ba":"sns.set_style('whitegrid')","1421e261":"df = pd.read_csv('\/kaggle\/input\/mushroom-classification\/mushrooms.csv')\ndf['class'].replace(['e','p'],['Edible','Poisonous'],inplace=True)\ndf.head()","1efd7eb7":"df.shape","39c905c7":"df.info()","0069bd61":"labels = df['class'].value_counts().index\nsizes = df['class'].value_counts().values\n\nplt.figure(figsize=(10,5))\nplt.pie(x=sizes,autopct='%1.1f%%',explode=(0.1,0),shadow=True, textprops={'color':\"gray\"}, \nstartangle=90,colors=[\"teal\",\"darkkhaki\"],frame=True,pctdistance=1.2,labeldistance=0)\nplt.axis('equal')\nplt.legend(labels)\nplt.title(\"Classes\".upper(),fontsize=20)\nplt.xticks([])\nplt.yticks([])\nplt.show()\nprint('Figure 1: Percentages of Mushroom Classes')","83625751":"plt.figure(figsize=(10,28))\nfor i,j in zip(df.iloc[:,1:].columns,range(1,23)):\n    plt.subplot(11,2,j)\n    sns.countplot(x=i, data=df, palette=\"twilight\", edgecolor=\"black\")\nplt.tight_layout()\nplt.show()\n\nprint('Figure 2: Counter Cards for Attributes')","82c19731":"plt.figure(figsize=(15,40))\nfor i,j in zip(df.iloc[:,1:].columns,range(1,23)):\n    plt.subplot(11,2,j)\n    df.groupby(i)['class'].value_counts().plot(kind=\"barh\",edgecolor=\"black\",color=\"teal\")\n    plt.xlabel(\"Value Counts\")\nplt.tight_layout()\nplt.show()\nprint('Figure 3: Query Cards for Attributes')","38fba446":"X = df.drop('class', axis=1)\ny = df['class']\n\nX_encoded = pd.get_dummies(df,prefix_sep=\"_\")\ny_encoded = LabelEncoder().fit_transform(y)\n\nX_scaled = StandardScaler().fit_transform(X_encoded)\n\nX_train, X_test, y_train, y_test = train_test_split(X_scaled, y_encoded, test_size=0.30, random_state=38)\nclasses = ['p','e']","8af28a9b":"import warnings\nwarnings.simplefilter(action='ignore', category=FutureWarning)\nwarnings.simplefilter(action='ignore', category=RuntimeWarning)\n\nfrom yellowbrick.classifier import ROCAUC,ConfusionMatrix\nfrom sklearn.metrics import accuracy_score\n\ndef Performance(model):\n    global X_train,y_train,X_test,X_train,classes\n    \n    print(\"REPORT:\")\n    print(classification_report(y_test,model.predict(X_test)))\n    \n    visualizer = ROCAUC(model, classes=classes)\n    visualizer.fit(X_train, y_train)      \n    visualizer.score(X_test, y_test)        \n    visualizer.show();\n\n    plt.figure(figsize=(3,3))\n    cm = ConfusionMatrix(model, classes=classes)\n    cm.fit(X_train, y_train)\n    cm.score(X_test, y_test)\n    plt.xticks(rotation=0)\n    cm.show();","fc1e73aa":"from sklearn.model_selection import cross_val_score\ndef CrossValidationScore(model_list):\n    global X_scaled,y_encoded\n    \n    mean_cross_val_score = []\n    model_name           = []\n    \n    for model in model_list:\n        model_name.append(type(model).__name__)\n        \n    for i in model_list:\n        scores = cross_val_score(i, X_scaled, y_encoded, cv=5)\n        mean_cross_val_score.append(scores.mean())\n        \n    cvs = pd.DataFrame({\"Model Name\":model_name,\"CVS\":mean_cross_val_score})\n    return cvs.style.background_gradient(\"Greens\")","74efce26":"mlp = MLPClassifier(hidden_layer_sizes=(64,128,64),activation=\"relu\",max_iter=500,solver=\"adam\")\nmlp.fit(X_train,y_train)\n\nPerformance(mlp)","accb4581":"rfc = RandomForestClassifier(n_estimators=150)\nrfc.fit(X_train,y_train)\n\nPerformance(rfc)","e585ec4b":"gbc = GradientBoostingClassifier()\ngbc.fit(X_train,y_train)\n\nPerformance(gbc)","59b1f7e4":"xrfc = XGBRFClassifier()\nxrfc.fit(X_train,y_train)\n\nPerformance(xrfc)","d76ef35f":"xgbc = XGBClassifier()\nxgbc.fit(X_train,y_train)\n\nPerformance(xgbc)","b6076a82":"model_list = [mlp, gbc, rfc, xrfc, xgbc]\nCrossValidationScore(model_list)","ddc3a23d":"df_ap = pd.get_dummies(df,prefix_sep=\"_\")\ndf_ap.head()","7cd26c69":"df1 =  apriori(df_ap, min_support=0.80, use_colnames = True, verbose=1)\ndf1.style.background_gradient(\"Greens\")","286d2154":"association_rules(df1, metric = \"support\", min_threshold = 0.9).style.background_gradient(\"gist_earth_r\")","84e698c7":"The Apriori algorithm was proposed by Agrawal and Srikant in 1994. Apriori is designed to operate on databases containing transactions (for example, collections of items bought by customers, or details of a website frequentation or IP addresses). Other algorithms are designed for finding association rules in data having no transactions (Winepi and Minepi), or having no timestamps (DNA sequencing). Each transaction is seen as a set of items (an itemset). Given a threshold C, the Apriori algorithm identifies the item sets which are subsets of at least C transactions in the database.\n\nApriori uses a \"bottom up\" approach, where frequent subsets are extended one item at a time (a step known as candidate generation), and groups of candidates are tested against the data. The algorithm terminates when no further successful extensions are found.\n\nApriori uses breadth-first search and a Hash tree structure to count candidate item sets efficiently. It generates candidate item sets of length k from item sets of length k-1. Then it prunes the candidates which have an infrequent sub pattern. According to the downward closure lemma, the candidate set contains all frequent k-length item sets. After that, it scans the transaction database to determine frequent item sets among the candidates.\n<center><img src=\"https:\/\/wikimedia.org\/api\/rest_v1\/media\/math\/render\/svg\/549fa6a5f46897d137b5d704ef7f30b6ba36d4de\"><\/center>","3146bec7":"<a id=\"t3.2\"><\/a>\n## 3.2 Functions for models","ec5784b2":"<a id=\"t4.\"><\/a>\n## 4. Association Rules","8bcef21b":"<a id=\"t3.1\"><\/a>\n## 3.1 Split data for train and test","294c6967":"<a id=\"t3.3\"><\/a>\n## 3.3 Models","185ddacb":"Association rule learning is a rule-based machine learning method for discovering interesting relations between variables in large databases. It is intended to identify strong rules discovered in databases using some measures of interestingness.\n\nBased on the concept of strong rules, Rakesh Agrawal, Tomasz Imieli\u0144ski and Arun Swami introduced association rules for discovering regularities between products in large-scale transaction data recorded by point-of-sale (POS) systems in supermarkets. For example, the rule {onion,potatoes}=>{burger} found in the sales data of a supermarket would indicate that if a customer buys onions and potatoes together, they are likely to also buy hamburger meat. Such information can be used as the basis for decisions about marketing activities such as, e.g., promotional pricing or product placements.\n\nIn addition to the above example from market basket analysis association rules are employed today in many application areas including Web usage mining, intrusion detection, continuous production, and bioinformatics. In contrast with sequence mining, association rule learning typically does not consider the order of items either within a transaction or across transactions.","558601f0":"<a id=\"t4.1\"><\/a>\n## 4.1 Apriori Algorithm","53f07ece":"<h1><center>Association Rules And Classification:Mushrooms Dataset<\/center><\/h1>\n\n<center><img src=\"https:\/\/www.uncovercolorado.com\/wp-content\/uploads\/2020\/04\/Amanita-Muscaria-CO-1600x800-1.jpg\"><\/center>","7b399d82":"<a id=\"t3.\"><\/a>\n# 3. Classification","4d435ab0":"<a id=\"t1.\"><\/a>\n# 1. Import data and python packages","0921f10d":"Which features are most indicative of a poisonous mushroom? Must use query chart for answer to the question.","e70e5d28":"<a id=\"t2.\"><\/a>\n# 2. Data visualization and Analysis","6bdc3912":"The number of poisonous and edible mushrooms is almost half.","079eb3e1":"# Introduction\nA mushroom or toadstool is the fleshy, spore-bearing fruiting body of a fungus, typically produced above ground, on soil, or on its food source.\n\nThe standard for the name \"mushroom\" is the cultivated white button mushroom, Agaricus bisporus; hence the word \"mushroom\" is most often applied to those fungi (Basidiomycota, Agaricomycetes) that have a stem (stipe), a cap (pileus), and gills (lamellae, sing. lamella) on the underside of the cap. \"Mushroom\" also describes a variety of other gilled fungi, with or without stems, therefore the term is used to describe the fleshy fruiting bodies of some Ascomycota. These gills produce microscopic spores that help the fungus spread across the ground or its occupant surface.\n\nForms deviating from the standard morphology usually have more specific names, such as \"bolete\", \"puffball\", \"stinkhorn\", and \"morel\", and gilled mushrooms themselves are often called \"agarics\" in reference to their similarity to Agaricus or their order Agaricales. By extension, the term \"mushroom\" can also refer to either the entire fungus when in culture, the thallus (called a mycelium) of species forming the fruiting bodies called mushrooms, or the species itself.\nContext\nAlthough this dataset was originally contributed to the UCI Machine Learning repository nearly 30 years ago, mushroom hunting (otherwise known as \"shrooming\") is enjoying new peaks in popularity. Learn which features spell certain death and which are most palatable in this dataset of mushroom characteristics. And how certain can your model be?\n\n## Content\nThis dataset includes descriptions of hypothetical samples corresponding to 23 species of gilled mushrooms in the Agaricus and Lepiota Family Mushroom drawn from The Audubon Society Field Guide to North American Mushrooms (1981). Each species is identified as definitely edible, definitely poisonous, or of unknown edibility and not recommended. This latter class was combined with the poisonous one. The Guide clearly states that there is no simple rule for determining the edibility of a mushroom; no rule like \"leaflets three, let it be'' for Poisonous Oak and Ivy.\n\n* Time period: Donated to UCI ML 27 April 1987\n## Inspiration\n* What types of machine learning models perform best on this dataset?\n\n* Which features are most indicative of a poisonous mushroom?\n\n## Acknowledgements\nThis dataset was originally donated to the UCI Machine Learning repository. You can learn more about past research using the data here.\n\n## About this file\n\n* Attribute Information: (classes: edible=e, poisonous=p)\n\n* cap-shape: bell=b,conical=c,convex=x,flat=f, knobbed=k,sunken=s\n\n* cap-surface: fibrous=f,grooves=g,scaly=y,smooth=s\n\n* cap-color: brown=n,buff=b,cinnamon=c,gray=g,green=r,pink=p,purple=u,red=e,white=w,yellow=y\n\n* bruises: bruises=t,no=f\n\n* odor: almond=a,anise=l,creosote=c,fishy=y,foul=f,musty=m,none=n,pungent=p,spicy=s\n\n* gill-attachment: attached=a,descending=d,free=f,notched=n\n\n* gill-spacing: close=c,crowded=w,distant=d\n\n* gill-size: broad=b,narrow=n\n\n* gill-color: black=k,brown=n,buff=b,chocolate=h,gray=g, green=r,orange=o,pink=p,purple=u,red=e,white=w,yellow=y\n\n* stalk-shape: enlarging=e,tapering=t\n\n* stalk-root: bulbous=b,club=c,cup=u,equal=e,rhizomorphs=z,rooted=r,missing=?\n\n* stalk-surface-above-ring: fibrous=f,scaly=y,silky=k,smooth=s\n\n* stalk-surface-below-ring: fibrous=f,scaly=y,silky=k,smooth=s\n\n* stalk-color-above-ring: brown=n,buff=b,cinnamon=c,gray=g,orange=o,pink=p,red=e,white=w,yellow=y\n\n* stalk-color-below-ring: brown=n,buff=b,cinnamon=c,gray=g,orange=o,pink=p,red=e,white=w,yellow=y\n\n* veil-type: partial=p,universal=u\n\n* veil-color: brown=n,orange=o,white=w,yellow=y\n\n* ring-number: none=n,one=o,two=t\n\n* ring-type: cobwebby=c,evanescent=e,flaring=f,large=l,none=n,pendant=p,sheathing=s,zone=z\n\n* spore-print-color: black=k,brown=n,buff=b,chocolate=h,green=r,orange=o,purple=u,white=w,yellow=y\n\n* population: abundant=a,clustered=c,numerous=n,scattered=s,several=v,solitary=y\n\n* habitat: grasses=g,leaves=l,meadows=m,paths=p,urban=u,waste=w,woods=d\n\n## Contents of notebook\n\n1. [Import data and python packages](#t1.)\n    * Import packages\n    * Import data\n    * Data shape and info\n2. [Data visualization and Analysis](#t2.)\n    * Pie Chart\n    * Count plots\n    * Query Charts\n3. [Classification](#t3.)\n\n    3.1 [Split data for train and test](#t3.1)\n    \n    3.2 [Functions for models](#t3.2)\n    \n    3.3 [Models](#t3.3)\n      * MLP Classifier\n      * Random Forest Classifier\n      * Gradient Boosting Classifier\n      * XGB Classifier\n      * Cross Validation Score\n4. [Association Rules](#t4.)\n\n      4.1[Apriori Algorithm](#t4.1)"}}