{"cell_type":{"d8162058":"code","263764ba":"code","11842a3b":"code","f355a803":"code","015311af":"code","7c6e8f53":"code","ca6cf5b5":"code","6f3dbe37":"code","96f7a96a":"code","34fd1b0e":"code","6e51567b":"code","33942a30":"code","62027854":"code","f8c0a06c":"code","0ed500a5":"code","62208bd2":"code","5369c1f8":"code","13986724":"code","795919b9":"code","2e86a4b7":"code","f1e46a87":"code","b811e108":"code","dba13086":"code","84f407ef":"code","9b105cfe":"code","1d222107":"code","20ad53d6":"code","bd101d35":"code","70a56297":"code","4b6494f4":"code","a538d500":"code","2863a962":"code","143fe7d6":"code","8343dd70":"code","28cbffaa":"code","4973b5ee":"markdown","0a3165e2":"markdown","0c1b2425":"markdown","879f1b1c":"markdown","eabdb6a2":"markdown","418803b3":"markdown","e550edde":"markdown","aec1f672":"markdown","ed410e79":"markdown","3cb9eacd":"markdown","f3a2aeb6":"markdown","ba0dee98":"markdown","2dc35019":"markdown","e579c185":"markdown","f704a7d5":"markdown","b6ed905e":"markdown","e2a163db":"markdown"},"source":{"d8162058":"import numpy as np\nimport tensorflow as tf\nfrom tensorflow import keras\nimport pandas as pd\nfrom pickle import dump, load\nimport re\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.utils import to_categorical\nfrom keras.layers.merge import add\nfrom keras.models import Model, load_model,Sequential\nfrom keras.layers import Input, Dense, LSTM, Embedding, Dropout\n\nfrom keras.applications.vgg16 import VGG16,preprocess_input\nfrom keras.preprocessing.image import load_img,img_to_array\nfrom keras.models import Model\nfrom glob import glob\n\n","263764ba":"# # detect and init the TPU\n# tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n# tf.config.experimental_connect_to_cluster(tpu)\n# tf.tpu.experimental.initialize_tpu_system(tpu)\n\n# # instantiate a distribution strategy\n# tpu_strategy = tf.distribute.experimental.TPUStrategy(tpu)\n","11842a3b":"img_path=\"..\/input\/flickr8k\/Images\/\"\nimages = glob(img_path+'*.jpg')\nlen(images)","f355a803":"\n\nmodel=VGG16()\nmodel.layers.pop()\nmodel = Model(inputs=model.inputs, outputs=model.layers[-2].output)\nmodel.summary()\n","015311af":"\n# features = {}\n\n# for i in images:\n#     img=load_img(i,target_size=(224, 224,3))\n#     img=img_to_array(img)\n#     img = np.expand_dims(img, axis=0)\n#     img=preprocess_input(img)\n#     feature = model.predict(img)\n# #     feature=feature.reshape((-1,))\n    \n#     img_id=i.replace(\"..\/input\/flickr8k\/Images\/\",\"\")\n    \n#     features[img_id]=feature\n\n# dump(features, open(\"features.p\",\"wb\"))\n    ","7c6e8f53":"features=load(open(\"..\/input\/image-captioning-models\/features.p\",\"rb\"))","ca6cf5b5":"# import os\n# from IPython.display import FileLink\n\n# os.chdir(r'.\/')\n# FileLink(r'vgg16.h5')","6f3dbe37":"captions_txt = open(\"..\/input\/flickr8k\/captions.txt\", 'rb').read().decode('utf-8')\nimg_cap_corpus=captions_txt.split('\\n')\nimg_cap_corpus.pop(0)","96f7a96a":"img_cap_dic={}\nfor line in img_cap_corpus:   \n    img_cap = line.split(',')\n    \n    if(len(img_cap)>=2):\n    \n        img_id=img_cap[0]\n        caption=img_cap[1]\n\n\n        if img_id not in img_cap_dic:\n            img_cap_dic[img_id] = [caption]\n        else:\n            img_cap_dic[img_id].append(caption)\n","34fd1b0e":"def preprocessed(txt):\n    modified = txt.lower()    \n    tokens=modified.split(' ')\n    \n    for wd in tokens:\n        if len(wd)<2:\n            tokens.remove(wd)\n    modified=\" \".join(tokens)\n    modified = 'startofseq ' + modified + ' endofseq'\n    return modified","6e51567b":"for k,caps in img_cap_dic.items():\n    for cap in caps:\n        img_cap_dic[k][caps.index(cap)] = preprocessed(cap)\n","33942a30":"max_length=0\nfor caps in img_cap_dic.values():\n    for cap in caps:\n        if max_length<len(cap.split(\" \")):\n            max_length=len(cap.split(\" \"))\n\nmax_length            ","62027854":"\ntokenizer=Tokenizer(num_words=None,filters='!\"#$%&()*+,-.\/:;<=>?@[\\\\]^_`{|}~\\t\\n',oov_token='<oov>')\n\nfor k,caps in img_cap_dic.items():\n    tokenizer.fit_on_texts(caps)\n    \n    \nvocab_size=len(tokenizer.word_index)+1\n        \ndump(tokenizer, open('tokenizer.p', 'wb'))\n\n\n\n","f8c0a06c":"# import os\n# os.chdir(\"\/kaggle\/working\")\n# os.listdir() ","0ed500a5":"# tokenizer=load(open(\".\/tokenizer.p\",\"rb\"))\n# vocab_size=len(tokenizer.word_index)+1","62208bd2":"# sequence={}\n# for k,caps in img_cap_dic.items():\n#         sequence[k]=tokenizer.texts_to_sequences(caps)\n    ","5369c1f8":"def data_generator(descriptions, features, tokenizer, max_length,batch):\n    X1, X2, y = list(), list(), list()\n    n=0\n    \n    while 1:\n        for key, desc_list in descriptions.items():\n            n+=1\n            #retrieve photo features\n            feature = features[key][0]            \n            \n            # walk through each description for the image\n            for desc in desc_list:\n                # encode the sequence\n                seq = tokenizer.texts_to_sequences([desc])[0]\n                # split one sequence into multiple X,y pairs\n                for i in range(1, len(seq)):\n                    # split into input and output pair\n                    in_seq, out_seq = seq[:i], seq[i]\n                    # pad input sequence\n                    in_seq = pad_sequences([in_seq], maxlen=max_length)[0]\n                    # encode output sequence\n                    out_seq = to_categorical([out_seq], num_classes=vocab_size)[0]\n                    # store\n                    X1.append(feature)\n                    X2.append(in_seq)\n                    y.append(out_seq)\n\n            \n             # yield the batch data\n            if n==batch:\n                yield ([np.array(X1), np.array(X2)], np.array(y))\n                X1, X2, y = list(), list(), list()\n                n=0\n            \n\n    ","13986724":"\n# embeddings_index = {} \n# golve_path ='..\/input\/glove6b200d\/glove.6B.200d.txt'\n# glove = open(golve_path, 'r', encoding = 'utf-8').read() \n\n","795919b9":"# for line in glove.split(\"\\n\"): \n#     values = line.split(\" \") \n#     word = values[0] \n#     indices = np.asarray(values[1: ], dtype = 'float32') \n#     embeddings_index[word] = indices \n  \n","2e86a4b7":"# emb_dim = 200\n# emb_matrix = np.zeros((vocab_size, emb_dim)) \n# for word, i in .tokenizer.word_index.items(): \n#     emb_vec = embeddings_index.get(word) \n#     if emb_vec is not None: \n#         emb_matrix[i] = emb_vec \n# emb_matrix.shape","f1e46a87":"\ninputs1 = Input(shape=(4096,))\nfe1 = Dropout(0.5)(inputs1)\nfe2 = Dense(256, activation='relu')(fe1)\ninputs2 = Input(shape=(max_length,))\nse1 = Embedding(vocab_size, 256, mask_zero=True)(inputs2)\nse2 = Dropout(0.5)(se1)\nse3 = LSTM(256)(se2)\ndecoder1 = add([fe2, se3])\ndecoder2 = Dense(256, activation='relu')(decoder1)\noutputs = Dense(vocab_size, activation='softmax')(decoder2)\nmodel = Model(inputs=[inputs1, inputs2], outputs=outputs)","b811e108":"model.compile(loss='categorical_crossentropy', optimizer='adam')","dba13086":"# import random\n\n# test = 30\n# training = 70\n\n# key_list = list(img_cap_dic.keys())   \n\n# test_key_count = int((len(key_list) \/ 100) * test) \n# test_keys = [random.choice(key_list) for ele in range(test_key_count)] \n# train_keys = [ele for ele in key_list if ele not in test_keys] \n \n# testing_dict = dict((key, img_cap_dic[key]) for key in test_keys  \n#                                         if key in img_cap_dic) \n\n# training_dict = dict((key, img_cap_dic[key]) for key in train_keys  \n#                                         if key in img_cap_dic) \n\n  \n# testing_features = dict((key, features[key]) for key in test_keys  \n#                                         if key in features)  \n# training_features = dict((key, features[key]) for key in train_keys  \n#                                         if key in features) \n","84f407ef":"tf.config.run_functions_eagerly(True)","9b105cfe":"\nepochs = 5\n\nnumber_pics_per_bath = 3\nsteps = len(img_cap_dic)\/\/number_pics_per_bath","1d222107":"\nfor i in range(epochs):\n    generator = data_generator(img_cap_dic, features, tokenizer, max_length, number_pics_per_bath)\n    model.fit_generator(generator, epochs=1, steps_per_epoch=steps,\n                        verbose=1)\n#     model.save(\"icg\"+str(i) + '.h5')","20ad53d6":"model.optimizer.lr = 0.0001\nepochs = 5\nnumber_pics_per_bath = 6\nsteps = len(img_cap_dic)\/\/number_pics_per_bath","bd101d35":"for i in range(epochs):\n    generator = data_generator(img_cap_dic, features, tokenizer, max_length, number_pics_per_bath)\n    model.fit_generator(generator, epochs=1, steps_per_epoch=steps, verbose=1)\n#     model.save(\"icgm\"+str(i) + '.h5')\n    ","70a56297":" model.save(\"icg.h5\")","4b6494f4":"epochs =1\nsteps = len(img_cap_dic)\nfor i in range(epochs):\n    generator = data_generator(img_cap_dic,features, tokenizer, max_length)\n    icgmodel.fit(generator, epochs=1, steps_per_epoch= steps, verbose=1)\n","a538d500":"# icgmodel.save('icmm.h5')","2863a962":"# icgmodel=load_model('..\/input\/image-captioning-models\/final.h5')","143fe7d6":"# def generate_desc(model, tokenizer, photo, max_length):\n#     # seed the generation process\n#     in_text = 'startseq'\n#     # iterate over the whole length of the sequence\n#     for i in range(max_length):\n#         # integer encode input sequence\n#         sequence = tokenizer.texts_to_sequences([in_text])[0]\n#         # pad input\n#         sequence = pad_sequences([sequence], maxlen=max_length)\n#         # predict next word\n#         yhat = model.predict([photo,sequence], verbose=0)\n#         # convert probability to integer\n#         yhat = argmax(yhat)\n#         # map integer to word\n#         word = word_for_id(yhat, tokenizer)\n#         # stop if we cannot map the word\n#         if word is None:\n#             break\n#         # append as input for generating the next word\n#         in_text += ' ' + word\n#         # stop if we predict the end of the sequence\n#         if word == 'endseq':\n#             break\n#     return in_text\n\n# #evaluate the skill of the model\n\n# def evaluate_model(model, descriptions, photos, tokenizer, max_length):\n#     actual, predicted = list(), list()\n#     # step over the whole set\n#     for key, desc_list in img_cap_dic.items():\n#     # generate description\n#         yhat = generate_desc(model, tokenizer, photos[key], max_length)\n#     # store actual and predicted\n#     references = [d.split() for d in desc_list]\n#     actual.append(references)\n#     predicted.append(yhat.split())\n#     # calculate BLEU score\n#     print('BLEU-1: %f' % corpus_bleu(actual, predicted, weights=(1.0, 0, 0, 0)))\n#     print('BLEU-2: %f' % corpus_bleu(actual, predicted, weights=(0.5, 0.5, 0, 0)))\n#     print('BLEU-3: %f' % corpus_bleu(actual, predicted, weights=(0.3, 0.3, 0.3, 0)))\n#     print('BLEU-4: %f' % corpus_bleu(actual, predicted, weights=(0.25, 0.25, 0.25, 0.25)))\n ","8343dd70":"# # load the model\n# model = load_model('..\/input\/image-captioning-models\/final.h5')\n# evaluate model\n# evaluate_model(model, testing_dict, testing_features, tokenizer, max_length)","28cbffaa":"import numpy as np\nfrom PIL import Image\nimport matplotlib.pyplot as plt\n\n\nmax_length=36\nimg_path = \"..\/input\/image-captioning-models\/Lionel-Messi-2018.png\"\n\ndef extract_features(filename, model):\n        try:\n            image = Image.open(filename)\n        except:\n            print(\"ERROR: Couldn't open image! Make sure the image path and extension is correct\")\n        if image.mode != \"RGB\":\n            image = image.convert(\"RGB\")\n        image = image.resize((224, 224))\n        image = img_to_array(image)\n        image = np.expand_dims(image, axis=0)\n        image = preprocess_input(image)\n        feature = model.predict(image)\n        return feature\n    \n    \ndef word_for_id(integer, tokenizer):\n    for word, index in tokenizer.word_index.items():\n        if index == integer:\n            return word\n    return None\n\ndef generate_desc(model, tokenizer, photo, max_length):\n    in_text = 'startofseq'\n    for i in range(max_length):\n        sequence = tokenizer.texts_to_sequences([in_text])[0]\n        sequence = pad_sequences([sequence], maxlen=max_length)\n        \n        pred = model.predict([photo,sequence], verbose=0)\n        pred = np.argmax(pred)\n                \n        word = word_for_id(pred, tokenizer)\n        if word is None:\n            break\n        in_text += ' ' + word\n        if word == 'endofseq':\n            break\n    return in_text\n\n\n# tokenizer = load(open(\"..\/input\/image-captioning-models\/tokenizer.p\",\"rb\"))\n# model = load_model('..\/input\/image-captioning-models\/final.h5')\n\nvgmodel=VGG16()\nvgmodel.layers.pop()\nvgmodel = Model(inputs=vgmodel.inputs, outputs=vgmodel.layers[-2].output)\n\nphoto = extract_features(img_path, vgmodel)\nimg = Image.open(img_path)\ndescription = generate_desc(model, tokenizer, photo, max_length)\n\nprint(description.replace('startofseq', '').replace('endofseq', ''))\nplt.imshow(img)\n","4973b5ee":"### clean text and tokenize\n","0a3165e2":"### Tokenize","0c1b2425":"# Pre-processing","879f1b1c":"### create dictionary of image and its captions","eabdb6a2":"# Predict","418803b3":"# Test","e550edde":"# load glove vectors for embedding layer ","aec1f672":"## Text pre-processing","ed410e79":"### read caption file","3cb9eacd":"## Image pre-processing","f3a2aeb6":"# MODEL","ba0dee98":"# Train","2dc35019":"### Calculating max length of caption","e579c185":"# Extracting image Features","f704a7d5":"# Split data","b6ed905e":"# Data generator","e2a163db":"## Pad the sequence"}}