{"cell_type":{"1994d1f9":"code","df49f03f":"code","f2658fb5":"code","503e6337":"code","e2d88240":"code","fa73fb6e":"code","201434d0":"code","26aa0f82":"code","9811286c":"code","83a98b56":"code","bf68cfe0":"code","90b9ab34":"code","a9e372d6":"code","0b78c3a5":"code","3c4e491b":"code","977d256d":"code","599106ee":"code","bfcc9ea9":"code","d955905d":"code","22bc55f1":"code","693b21e3":"code","68d77b39":"code","d4fcfa34":"code","f5929639":"code","7e4cb0c6":"code","3cef3760":"code","7c6d0c01":"code","1eb2391c":"code","ded23428":"code","ca18243b":"code","2939996b":"code","a3d997ec":"code","872127d6":"code","4f188f33":"code","c994a707":"code","878d6131":"code","f0aa74b6":"code","ace8df59":"code","bd4d7dad":"code","7ffdddbe":"code","adb2e1cf":"code","7323ef95":"code","3500b7f7":"code","842a5e66":"code","5395c2af":"code","c901a87f":"code","79cb5d53":"code","16b0d411":"code","13d31598":"code","a87834ad":"code","86672fff":"code","e3412fd5":"code","6236afa5":"code","8e1dc27c":"code","f443dd6f":"code","a605cd04":"code","add8a34c":"code","e62efd99":"code","6b7a0f31":"code","3d77d35b":"code","9dd6a8c4":"code","d03abe64":"code","9f29513a":"code","42b1d4dc":"code","6de456f4":"code","9bed429d":"code","996cb301":"code","5cd886af":"code","5848626e":"code","631488c0":"code","770c6a58":"code","923edeed":"code","ba04b6e8":"code","ccab4e72":"markdown","5ddc5d14":"markdown","fb03474d":"markdown","e0f74da7":"markdown","2b2bdba0":"markdown","689cf734":"markdown","65341c47":"markdown","8026791a":"markdown","c50e8359":"markdown","2757beba":"markdown","d22773a5":"markdown","2793a372":"markdown","aecf8b76":"markdown"},"source":{"1994d1f9":"# To stop  displaying warning messages in output\nimport warnings\nwarnings.filterwarnings('ignore')\n# To  collect garbage (delete files)\nimport gc\nimport pandas as pd\nimport numpy as np\nfrom fbprophet import Prophet\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n# for basic math operations like sqrt\nimport math\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport folium\nimport gpxpy.geo\nfrom datetime import datetime\nimport time\nimport seaborn as sns\nimport os\nimport xgboost as xgb\nimport matplotlib\nmatplotlib.use('nbagg')\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.linear_model import SGDRegressor\nfrom sklearn.cluster import MiniBatchKMeans, KMeans\nimport warnings\nwarnings.simplefilter('ignore')\nfrom math import sin, cos, sqrt, atan2, radians","df49f03f":"%matplotlib inline","f2658fb5":"input_data = pd.read_csv(\"..\/input\/rapido-rides\/ct_rr.csv\")\ninput_data.shape","503e6337":"print(\"Data size before removing: \",input_data.shape)\n\n# Check duplicated rows in train set\ndf = input_data[input_data.duplicated()]  # checks duplicate rows considering all columns\nprint(\"Number of duplicate observations: \", len(df))\ndel df\ngc.collect();\n\n#Dropping duplicates and keeping first occurence only\ninput_data.drop_duplicates(keep = 'first', inplace = True)\n\nprint(\"Data size after removing: \",input_data.shape)","e2d88240":"input_data.head()","fa73fb6e":"print(\"Number of unique customers: \" ,input_data[\"number\"].nunique()) #number of distinct customers = 1.7 lakhs","201434d0":"# new data frame with split value columns \nnew = input_data[\"ts\"].str.split(\" \", n = 1, expand = True) \n  \n# making separate first name column from new data frame \ninput_data[\"raw_date\"]= new[0] \n  \n# making separate last name column from new data frame \ninput_data[\"raw_time\"]= new[1] \n\ninput_data.head()","26aa0f82":"# new data frame with split value columns \nnew = input_data[\"raw_date\"].str.split(\"-\", n = 2, expand = True) \n  \n# making separate first name column from new data frame \ninput_data[\"year\"]= new[0] \n  \n# making separate last name column from new data frame \ninput_data[\"month\"]= new[1] \n\n# making separate last name column from new data frame \ninput_data[\"date\"]= new[2] \n\ninput_data.head()","9811286c":"# new data frame with split value columns \nnew = input_data[\"raw_time\"].str.split(\":\", n = 2, expand = True) \n  \n# making separate first name column from new data frame \ninput_data[\"hour\"]= new[0]\n#24:00 time system\n  \n# making separate last name column from new data frame \ninput_data[\"minute\"]= new[1] \n\ninput_data.head()","83a98b56":"#removing cols which are not reqd.\ndata = input_data.copy()\ndata.drop([\"raw_date\",\"raw_time\",\"number\",\"minute\"],axis=1, inplace=True)\ndata.head()\n\ndel input_data\ngc.collect();","bf68cfe0":"print(\"Is there any missing value? \",data.isna().sum().sum()>0)","90b9ab34":"def distance(pick_lat, pick_lng, drop_lat, drop_lng):\n    \n    # approximate radius of earth in km\n    R = 6373.0\n    \n    s_lat = pick_lat*np.pi\/180.0                      \n    s_lng = np.deg2rad(pick_lng)     \n    e_lat = np.deg2rad(drop_lat)                       \n    e_lng = np.deg2rad(drop_lng)  \n    \n    d = np.sin((e_lat - s_lat)\/2)**2 + np.cos(s_lat)*np.cos(e_lat) * np.sin((e_lng - s_lng)\/2)**2\n    \n    return round(2 * R * np.arcsin(np.sqrt(d)),1) ","a9e372d6":"data[\"distance\"] = data.apply(lambda x: distance(x.pick_lat, x.pick_lng, x.drop_lat, x.drop_lng), axis=1)\ndata.head(3)","0b78c3a5":"#assuming avg. bike speed of 35km\/hrs, we can calculate time in min.\navg_speed = 35\/60 #speed in km\/minutes\ndata[\"ride_minutes\"] = data[\"distance\"].apply(lambda x: round(x\/avg_speed,0))\n\nprint(\"Maximum ride distance covered in Km: \", data.distance.max())\nprint(\"Minimum ride distance covered in Km: \", data.distance.min())\nprint(\"Maximum ride time in mins: \",data.ride_minutes.max())\nprint(\"Minimum ride time in mins: \", data.ride_minutes.min())","3c4e491b":"plt.figure(figsize = (12,8))\nsns.kdeplot(data[\"ride_minutes\"].values, shade = True, cumulative = False)\nplt.tick_params(labelsize = 20)\nplt.xlabel(\"Trip Duration\", fontsize = 20)\nplt.title(\"PDF of Trip Duration\", fontsize = 20)\nplt.show()","977d256d":"data = data[(data.pick_lng <90) & (data.drop_lng <90) & (data.pick_lng >66) & (data.drop_lng >66) &\n       (data.pick_lat <40) & (data.drop_lat <40) & (data.pick_lat >8) & (data.drop_lat >8)]\n\ndata.shape","599106ee":"#after removing above outliers based on lat and long\n\nprint(\"Maximum ride distance covered in Km: \", data.distance.max())\nprint(\"Minimum ride distance covered in Km: \", data.distance.min())\nprint(\"Maximum ride time in mins: \",data.ride_minutes.max())\nprint(\"Minimum ride time in mins: \", data.ride_minutes.min())","bfcc9ea9":"plt.figure(figsize = (12,8))\nsns.kdeplot(data[\"ride_minutes\"].values, shade = True, cumulative = False)\nplt.tick_params(labelsize = 20)\nplt.xlabel(\"Trip Duration\", fontsize = 20)\nplt.title(\"PDF of Trip Duration\", fontsize = 20)\nplt.show()","d955905d":"coord = data[[\"pick_lat\", \"pick_lng\"]].values\nregions = MiniBatchKMeans(n_clusters = 30, batch_size = 10000).fit(coord)\ndata[\"pickup_cluster\"] = regions.predict(data[[\"pick_lat\", \"pick_lng\"]])","22bc55f1":"centerOfRegions = regions.cluster_centers_\nnoOfClusters = len(centerOfRegions)\nm = folium.Map(location = [12.972442, 77.580643], tiles = \"Stamen Toner\")\n\nfor i in range(noOfClusters):\n    folium.Marker([centerOfRegions[i][0], centerOfRegions[i][1]], popup = (str(np.round(centerOfRegions[i][0], 2))+\", \"+str(np.round(centerOfRegions[i][1], 2)))).add_to(m)\nm","693b21e3":"coord = data[[\"drop_lat\", \"drop_lng\"]].values\nregions = MiniBatchKMeans(n_clusters = 30, batch_size = 10000).fit(coord)\ndata[\"drop_cluster\"] = regions.predict(data[[\"drop_lat\", \"drop_lng\"]])\n\ncenterOfRegions = regions.cluster_centers_\nnoOfClusters = len(centerOfRegions)\nm = folium.Map(location = [12.972442, 77.580643], tiles = \"Stamen Toner\")\n\nfor i in range(noOfClusters):\n    folium.Marker([centerOfRegions[i][0], centerOfRegions[i][1]], popup = (str(np.round(centerOfRegions[i][0], 2))+\", \"+str(np.round(centerOfRegions[i][1], 2)))).add_to(m)\nm","68d77b39":"%matplotlib inline\nplt.scatter(x=data['pick_lng'], y=data['pick_lat'])\nplt.show()","d4fcfa34":"plt.scatter(x=data['drop_lng'], y=data['drop_lat'])\nplt.show()","f5929639":"sns.countplot(x=\"hour\", data=data)  # plot counts of variabe in bars form","7e4cb0c6":"data.boxplot('distance')","3cef3760":"sns.countplot(x=\"date\", data=data) ","7c6d0c01":"data.head()","1eb2391c":"data.year.value_counts().plot(kind='bar').get_figure()\nplt.show()","ded23428":"# Feature Engineering\ndef create_day_series(df):\n    \n    # Grouping by Date\/Time to calculate number of trips\n    day_df = pd.Series(df.groupby(['ts']).size())\n    # setting Date\/Time as index\n    day_df.index = pd.DatetimeIndex(day_df.index)\n    # Resampling to daily trips\n    day_df = day_df.resample('1D').apply(np.sum)\n    \n    return day_df\n\nday_df = create_day_series(data)\nday_df.head()","ca18243b":"day_df.shape","2939996b":"from random import randrange\nfrom pandas import Series\nfrom statsmodels.tsa.seasonal import seasonal_decompose\nresult = seasonal_decompose(day_df, model='additive', freq=7)\nresult.plot()\nplt.show()","a3d997ec":"from statsmodels.tsa.holtwinters import ExponentialSmoothing\ntrain, test = day_df.iloc[0:300], day_df.iloc[300:]\nmodel = ExponentialSmoothing(day_df, seasonal='add', seasonal_periods=7).fit()\npred = model.predict(start=test.index[0], end=test.index[-1])\nplt.figure(figsize=(15,9))\nplt.plot(train.index, train, label='Train')\nplt.plot(test.index, test, label='Test')\nplt.plot(pred.index, pred, label='Holt-Winters')\nplt.legend(loc='best')\nplt.show()","872127d6":"def mean_absolute_percentage_error(y_true, y_pred): \n    y_true, y_pred = np.array(y_true), np.array(y_pred)\n    return np.mean(np.abs((y_true - y_pred) \/ y_true)) * 100","4f188f33":"mae = mean_absolute_error(test, pred)\nprint('MAE: %.3f' % mae)","c994a707":"mape = mean_absolute_percentage_error(test, pred)\nprint('MAPE: %.3f' % mape)","878d6131":"# plot the time series\nplt.figure(figsize=(12,8))\nday_df.plot()\nplt.show()","f0aa74b6":"# prepare expected column names\ndf = pd.DataFrame()\ndf['ds'] = day_df.index\ndf['y'] = day_df.values","ace8df59":"## Plot the Time series data\nfig, ax = plt.subplots(figsize=(20,7))\na = sns.lineplot(x=\"ds\", y=\"y\", data=df)\na.set_title(\"Daily Ride Data\",fontsize=15)\nplt.show()","bd4d7dad":"from fbprophet import Prophet\n# define the model\nmodel = Prophet()\n# fit the model\nmodel.fit(df)","7ffdddbe":"# Plot the components of the model\nfig = model.plot_components(forecast)\nplt.show()","adb2e1cf":"# create test dataset, remove last 60 days\ntrain = df.drop(df.index[:-60])\nprint(train.tail())","7323ef95":"train.shape","3500b7f7":"train['ds'] = pd.to_datetime(train['ds'])","842a5e66":"forecast = model.predict(train)","5395c2af":"# summarize the forecast\nprint(forecast[['ds', 'yhat', 'yhat_lower', 'yhat_upper']].head())\n","c901a87f":"# plot forecast\nmodel.plot(forecast)\nplt.legend(loc='best')\nplt.show()","79cb5d53":"# Plot the components of the model\nfig = model.plot_components(forecast)","16b0d411":"forecast.shape","13d31598":"# calculate MAE between expected and predicted values for december\ny_true = df['y'][-60:].values\ny_pred = forecast['yhat'].values\nmae = mean_absolute_error(y_true, y_pred)\nprint('MAE: %.3f' % mae)","a87834ad":"mape = mean_absolute_percentage_error(y_true, y_pred)\nprint('MAPE: %.3f' % mape)","86672fff":"# plot expected vs actual\n%matplotlib inline\nplt.figure(figsize=(12,8))\nplt.plot(y_true, label='Actual')\nplt.plot(y_pred, label='Predicted')\nplt.legend()\nplt.show()","e3412fd5":"import holidays","6236afa5":"holiday = pd.DataFrame([])\nfor date, name in sorted(holidays.India(years=[2018,2019]).items()):\n    holiday = holiday.append(pd.DataFrame({'ds': date, 'holiday': \"IN-Holidays\"}, index=[0]), ignore_index=True)\nholiday['ds'] = pd.to_datetime(holiday['ds'], format='%Y-%m-%d', errors='ignore')","8e1dc27c":"holiday.head()","f443dd6f":"# Setup and train model with holidays\nmodel_with_holidays = Prophet(holidays=holiday)\nmodel_with_holidays.fit(df)","a605cd04":"forecast_new = model_with_holidays.predict(train)\nforecast_new[['ds', 'yhat', 'yhat_lower', 'yhat_upper']].tail(7)","add8a34c":"# plot forecast\nmodel.plot(forecast_new)\nplt.legend(loc='best')\nplt.show()","e62efd99":"# Plot the components of the model\nfig = model_with_holidays.plot_components(forecast_new)","6b7a0f31":"# calculate MAE between expected and predicted values for december\ny_true = df['y'][-60:].values\ny_pred = forecast_new['yhat'].values\nmae = mean_absolute_error(y_true, y_pred)\nprint('MAE: %.3f' % mae)","3d77d35b":"mape = mean_absolute_percentage_error(y_true, y_pred)\nprint('MAPE: %.3f' % mape)","9dd6a8c4":"# plot expected vs actual\n%matplotlib inline\nplt.figure(figsize=(12,8))\nplt.plot(y_true, label='Actual')\nplt.plot(y_pred, label='Predicted')\nplt.legend()\nplt.show()","d03abe64":"from sklearn.model_selection import ParameterGrid\nparams_grid = {'seasonality_mode':('multiplicative','additive'),\n               'changepoint_prior_scale':[0.1,0.2,0.3,0.4,0.5],\n              'holidays_prior_scale':[0.1,0.2,0.3,0.4,0.5],\n              'n_changepoints' : [100,150,200]}\ngrid = ParameterGrid(params_grid)\ncnt = 0\nfor p in grid:\n    cnt = cnt+1\n\nprint('Total Possible Models',cnt)","9f29513a":"import random","42b1d4dc":"strt='2018-01-01'\nend='2019-12-31'\nmodel_parameters = pd.DataFrame(columns = ['MAPE','Parameters'])\nfor p in grid:\n    test = pd.DataFrame()\n    print(p)\n    random.seed(0)\n    train_model =Prophet(changepoint_prior_scale = p['changepoint_prior_scale'],\n                         holidays_prior_scale = p['holidays_prior_scale'],\n                         n_changepoints = p['n_changepoints'],\n                         seasonality_mode = p['seasonality_mode'],\n                         weekly_seasonality=True,\n                         daily_seasonality = True,\n                         yearly_seasonality = True,\n                         holidays=holiday, \n                         interval_width=0.95)\n    train_model.add_country_holidays(country_name='US')\n    train_model.fit(df)\n    train_forecast = train_model.make_future_dataframe(periods=366, freq='D',include_history = False)\n    train_forecast = train_model.predict(train_forecast)\n    test=train_forecast[['ds','yhat']]\n    Actual = df[(df['ds']>strt) & (df['ds']<=end)]\n    MAPE = mean_absolute_percentage_error(Actual['y'],abs(test['yhat']))\n    print('Mean Absolute Percentage Error(MAPE)------------------------------------',MAPE)\n    model_parameters = model_parameters.append({'MAPE':MAPE,'Parameters':p},ignore_index=True)","6de456f4":"parameters = model_parameters.sort_values(by=['MAPE'])\nparameters = parameters.reset_index(drop=True)\nparameters.head()","9bed429d":"parameters['Parameters'][0]","996cb301":"# Setup and train model with holidays\nfinal_model = Prophet(holidays=holiday,\n                      changepoint_prior_scale= 0.5,\n                      holidays_prior_scale = 0.4,\n                      n_changepoints = 200,\n                      seasonality_mode = 'additive',\n                      weekly_seasonality=True,\n                      daily_seasonality = True,\n                      yearly_seasonality = True)\nfinal_model.add_country_holidays(country_name='India')\nfinal_model.fit(df)","5cd886af":"forecast_final = final_model.predict(train)\nforecast_final[['ds', 'yhat', 'yhat_lower', 'yhat_upper']].tail(7)","5848626e":"# plot forecast\nfinal_model.plot(forecast_final)\nplt.legend(loc='best')\nplt.show()","631488c0":"# Plot the components of the model\nfig = final_model.plot_components(forecast_final)","770c6a58":"# calculate MAE between expected and predicted values for december\ny_true = df['y'][-60:].values\ny_pred = forecast_final['yhat'].values\nmae = mean_absolute_error(y_true, y_pred)\nprint('MAE: %.3f' % mae)","923edeed":"mape = mean_absolute_percentage_error(y_true, y_pred)\nprint('MAPE: %.3f' % mape)","ba04b6e8":"# plot expected vs actual\n%matplotlib inline\nplt.figure(figsize=(12,8))\nplt.plot(y_true, label='Actual')\nplt.plot(y_pred, label='Predicted')\nplt.legend()\nplt.show()","ccab4e72":"We can clearly see the trend in rides over time and a monthly\/yearly seasonal pattern to the ride counts. These are patterns we expect the forecast model to take into account.","5ddc5d14":"Trend and Seasonality analysis","fb03474d":"Assuming the data given to us is for india geography only. It will also help to remove outliers that might lie in entering data for lat and long for India geography. Therefore, we can remove invalid latitude and longitudes For India, longitude: (66, 90), latitude: (8, 40)","e0f74da7":"## Fbprophet","2b2bdba0":"## Manually Evaluate Forecast Model","689cf734":"The number of customers are 1.7 lakhs. Therefore,it won't be practically feasible to predict forecast for each customer. So, we can drop that column.\n\nConverting timestamp to month number, year number etc","65341c47":"## Prophet Model Tuning","8026791a":"## Adding Holidays to the model\n\nWe will use holidays library to get the INDIAN holidays","c50e8359":"From the above plot it is clear that there are both trend and seasonality in the data","2757beba":"## Time Series predictions using Holt-Winters\n\nHolt-Winters is a model of time series behavior. Forecasting always requires a model, and Holt-Winters is a way to model three aspects of the time series: a typical value (average), a slope (trend) over time, and a cyclical repeating pattern (seasonality). Holt-Winters uses exponential smoothing to encode lots of values from the past and use them to predict \u201ctypical\u201d values for the present and future","d22773a5":"Out of total 150 models, we will select the parameters which has the least MAPE to train the final model and predict the next 2 months rides\n\nBest Parameters on which the model has the least MAPE is:","2793a372":"*After adding holidays we get some boost in MAE\/MAPE*","aecf8b76":"## HyperParameter Tuning using ParameterGrid"}}