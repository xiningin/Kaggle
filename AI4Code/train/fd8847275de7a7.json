{"cell_type":{"17cca887":"code","c4672d00":"code","6820ac4b":"code","f6b47af3":"code","c2ecbf2c":"code","d9a5de17":"code","3d3437c9":"code","b10a7c28":"code","de220e80":"code","913a7a80":"code","7f26be27":"code","bde34c04":"code","ffa62967":"code","6372ef5b":"code","e73eaca3":"code","5a6d0c97":"code","5b3bc521":"code","c566ef53":"markdown","009b9249":"markdown","0a9654d7":"markdown","7b66df8c":"markdown","6832f22c":"markdown","ae226cf6":"markdown","af87d6e2":"markdown","a839e653":"markdown","a152c0db":"markdown","bf994db8":"markdown","b9075009":"markdown","392328e0":"markdown","2006837a":"markdown","f2b29b58":"markdown","3b2f4df9":"markdown","8a3e283e":"markdown","718b88d0":"markdown","f2c3604c":"markdown","123a2310":"markdown","201dcf44":"markdown","fba431b8":"markdown","cbcc53e9":"markdown","c7c4019b":"markdown","1bea0e25":"markdown","d08ac8ae":"markdown","abeba7e2":"markdown","40e1d971":"markdown","1c3d7180":"markdown","9f957e5e":"markdown","86624a36":"markdown","dbd2f414":"markdown","f68f538e":"markdown","56a66fb6":"markdown","3c1822f4":"markdown","1c6960d7":"markdown","55778663":"markdown","941084c4":"markdown","8dda6130":"markdown"},"source":{"17cca887":"!pip install selenium                 #install selenium \n!apt-get update -y                    # update \n!apt install chromium-chromedriver -y #install chrome driver\n!cp \/usr\/lib\/chromium-browser\/chromedriver \/usr\/bin\n!pip install beautifulsoup4           #install beautiful soup\nimport pandas as pd\nimport sys\nimport time\nsys.path.insert(0,'\/usr\/lib\/chromium-browser\/chromedriver')\n","c4672d00":"from selenium import webdriver                    #import webDriver\nchrome_options = webdriver.ChromeOptions()\nchrome_options.add_argument('--headless')        #remove this for easy debbuing on your laptop \/pc\nchrome_options.add_argument('--no-sandbox')                             \nchrome_options.add_argument('--disable-dev-shm-usage')\nwd = webdriver.Chrome('chromedriver',chrome_options=chrome_options)\n\n\nurl = \"https:\/\/www.amazon.in\/\"            #url \nwd.get(url)                               ","6820ac4b":"html = wd.page_source\nhtml #show the html code of the document","f6b47af3":"from bs4 import BeautifulSoup as bs\nsoup = bs(html)\nprint(soup.prettify()) #also show the html code of the document ;but this is readable","c2ecbf2c":"#the search bar on amazon has the\n#id attribute as twotabsearchtextbox\n#find_element_by_id finds the element with the \n#given id, we can use it to interact with the element\nelement = wd.find_element_by_id(\"twotabsearchtextbox\")","d9a5de17":"from selenium.webdriver.common.keys import Keys\nelement.send_keys(\"Laptop\")   #same as typing Laptop into the amazon search bar\nelement.send_keys(Keys.ENTER) #same as hitting enter\ntime.sleep(10)","3d3437c9":"#html = wd.page_source\n#html\n#soup = bs(html)\n#print(soup.prettify())","b10a7c28":"#The bellow code finds all the div elements in the webpage with the class attribute as 'sg-col-inner'\nelements = wd.find_elements_by_class_name('sg-col-inner')","de220e80":"names =  []\nprices = []\nlinks =  []\nfrom selenium.common.exceptions import NoSuchElementException\nfor element in elements:\n    try: #more abut xpath in the following cell\n        name = element.find_element_by_xpath(\".\/\/span[@class = 'a-size-medium a-color-base a-text-normal']\").text\n        price = element.find_element_by_xpath(\".\/\/span[@class = 'a-price-whole']\").text\n        link = element.find_element_by_xpath(\".\/\/a[@class = 'a-size-base a-link-normal a-text-normal']\").get_attribute('href')\n        prices.append(price)\n        names.append(name)\n        links.append(link)\n    except:\n        NoSuchElementException          \n        \n# some elements might have the same class name as the parent class name for the emements \n# in that case if we don't use try catch we might get NoSuchElementException\n#many webpages these days re use the CSS these days ","913a7a80":"dfL = pd.DataFrame(zip(names, prices ,links), columns=['ItemName','Price','Href'])\ndfL","7f26be27":"def add_data(a,b,c):\n    elements = wd.find_elements_by_class_name('sg-col-inner')\n    for element in elements:\n        try:\n            name = element.find_element_by_xpath(\".\/\/span[@class = 'a-size-medium a-color-base a-text-normal']\").text\n            price = element.find_element_by_xpath(\".\/\/span[@class = 'a-price-whole']\").text\n            link = element.find_element_by_xpath(\".\/\/a[@class = 'a-size-base a-link-normal a-text-normal']\").get_attribute('href')\n            b.append(price)\n            a.append(name)\n            c.append(link)\n        except:\n            NoSuchElementException  \n    return a,b,c","bde34c04":"c = -1\nimport time\nwhile c!=0:\n    try:\n        wd.find_element_by_xpath(\"\/\/li[@class = 'a-last']\/a\").click()\n        time.sleep(10)   #sometime the webpage dosen't load in time and this can raise NoSuchElementException\n        names, prices ,links = add_data(names, prices ,links)\n    except NoSuchElementException:\n        print(\"end of Search Results\")\n        c = 0","ffa62967":"dfNew = pd.DataFrame(zip(names, prices ,links), columns=['ItemName','Price','Href'])","6372ef5b":"dfNew.drop_duplicates(inplace = True) #drop duplicates ","e73eaca3":"dfNew.to_csv('out.csv', index=False)","5a6d0c97":"ls","5b3bc521":"dfNew","c566ef53":"Primarily it is for automating web applications for testing purposes, but is certainly not limited to just that.\n\nBoring web-based administration tasks can (and should) also be automated as well.\n    - Selenium dev","009b9249":"![image.png](attachment:fcdbf011-c589-4ddd-a044-f9f268dec33a.png)","0a9654d7":"The above function takes in the arguments as our original lists, appends the new data into them and returns the lists back","7b66df8c":"<a href=\"https:\/\/imgbb.com\/\"><img src=\"https:\/\/i.ibb.co\/ZMM53Z1\/2.png\" alt=\"2\" border=\"0\"><\/a><br \/><a target='_blank' href='https:\/\/imgbb.com\/'>url photo editor<\/a><br \/>","6832f22c":"<a href=\"https:\/\/imgbb.com\/\"><img src=\"https:\/\/i.ibb.co\/FD5c2BN\/Screenshot-2021-05-16-140925.png\" alt=\"Screenshot-2021-05-16-140925\" border=\"0\"><\/a>","ae226cf6":"We will be using the following in built functions to navigate the webpage \nI will leave the link for documentation for better understanding\n1. element = driver.find_element_by_id(\"some_id\")\n2. element = driver.find_element_by_xpath(\"some xPath)\n3. link to documentation - https:\/\/selenium-python.readthedocs.io\/navigating.html","af87d6e2":"Note - Relative Xpath is preferred over absolute Xpath , since a small change in the web-page can render the absolute Xpath useless","a839e653":"Now that can be a little confusing , so to simplify Xpath is used to find web elements such as tags , buttons etc , using an XML epression for example \nXpath=\/\/tagname[@attribute='value']\nThe breakdown of the above expression can be as follows \n1.     \/\/ - Select or search the whole web-page\n2.     tagname -  Name of the HTML tag ;example - span , li , ul\n3.     @attribute - an attribute useful to identify the element or group of elements ;example -class, id ,etc \n4.     value      - value of the attribute ; example class = \"my_class , id = \"my_id\"","a152c0db":"\"\/\/li[@class = 'a-last']\/a\") - Find the element a(a tag is used for redirecting users to another webpage ) whose parent is a li(tag name of the parent ) with the class as \"a-last\"","bf994db8":"More tutorials on Xpath - \n1. https:\/\/www.w3schools.com\/xml\/xpath_syntax.asp\n2. https:\/\/www.tutorialspoint.com\/xpath\/index.htm","b9075009":"The above HTML code is pretty hard to read so we will convert it to a soup object and pretiffy it.This is done to find the relevant tags","392328e0":"The above code clicks on the next button while it is present in the webpage,and calls the get data function","2006837a":"![image.png](attachment:0fc68d4a-d88e-4607-bf50-a26355f29e17.png)","f2b29b58":"In this notebook we will see an example of web scrapping and use it to list out the laptop prices listed on Amazon\n","3b2f4df9":"![image.png](attachment:d951bf8a-a18b-4ee5-8fd1-b25b7e912e91.png)","8a3e283e":"Now we have searched for laptop using selenium ,\nnow to view the html cde and extract information\n","718b88d0":"Now we inspect the webpage to obtain the container element of \nall the listed items on amazon\n\n![image.png](attachment:a019ee44-b138-46b9-a37d-29f022ea7fe1.png)","f2c3604c":"An alternative and an user friendly approach is to use inspect element to obtain  the nessary information \n![t1.png](attachment:41a91d27-a66a-4caa-901e-f860dec44316.png)","123a2310":"Another method would be using beautiful soup , i will try to update the notebook on a later date","201dcf44":" Web scraping, web harvesting, or web data extraction is data scraping used for extracting data from websites     - Wikipedia","fba431b8":"Now that we have the element we can use other functions to interact with it\n1. element.send_keys(\"some text\") -> sends the keys as input to the element\n2. element.click() -> clicks on the element","cbcc53e9":"Now we simply loop this statement - what this does is simple ,it simply keeps checking the document for this tag and keeps pressing it ; this will keep redirecting us to the secnd page , the third one etc until we reach the end of the document","c7c4019b":"**Selenium automates browsers. That's it!**\nNow how cool does that sound !\n1.     With just a few steps you can automate Movie Bokings\n2.     Automate your grocery shopping\n3.     Create custom testing Scripts \n4.     Automate just about anything that requires interaction with the internet ","1bea0e25":"A small Note - The above code snippet can be copied and used for google collab as well","d08ac8ae":"The bellow function is simply the same code above i used for navigating the document and finding relevant information","abeba7e2":"Prerequisites\n1. We will be using basic python and HTML , but however there is no need to know these languages \n2. It will help a little to know basic CSS ","40e1d971":"Some of our results might be repeated , this can happen when the same product is listed twice on Amazon or due to bad Xpath\n","1c3d7180":"However those are the results present only in the first page,\nWe now use selenium to navigate to the next page  and view the results there\n","9f957e5e":"wd.get(url) - This line of code opens the url in a new chrome window ","86624a36":"<a href=\"https:\/\/imgbb.com\/\"><img src=\"https:\/\/i.ibb.co\/kQWkKK9\/1.png\" alt=\"1\" border=\"0\"><\/a><br \/><a target='_blank' href='https:\/\/imgbb.com\/'>0 images<\/a><br \/>","dbd2f414":"Now to convert data to a pandas dataframe","f68f538e":"Now to convert our data to a dataframe for better understanding ","56a66fb6":"XPath in Selenium is an XML path used for navigation through the HTML structure of the page. It is a syntax or language for finding any element on a web page using XML path expression. XPath can be used for both HTML and XML documents to find the location of any element on a webpage using HTML DOM structure.\n\n    -https:\/\/www.guru99.com\/xpath-selenium.html#:~:text=XPath%20in%20Selenium%20is%20an,webpage%20using%20HTML%20DOM%20structure.","3c1822f4":"Now convert the data to a csv file","1c6960d7":"the url is now opened and we can now use the wd variable to navigate through the amazon website","55778663":"Now to simplify the term element -  An element can be a tag, property, or anything\n    ","941084c4":"Since the class name changed , the element is no longer present and we will simply come out from the loop","8dda6130":"Now that we have installed the required packages it's time to import them and scrape some information from amazon "}}