{"cell_type":{"99334b0b":"code","95cd31c6":"code","a20029ca":"code","28fe3b15":"code","aa0da0e3":"code","a8db6982":"code","49a1e557":"code","f6ad4c64":"code","fdbf24de":"code","21b67afd":"code","11fb1e2d":"code","09d489a1":"code","80521c5b":"code","3b0064e0":"code","549f3cf7":"code","178da353":"code","7872e13f":"code","79bd1357":"code","3d394b81":"code","ccbbbc01":"code","594103ae":"code","a86a0338":"code","16732eb4":"code","4cb190a5":"code","e7e39c04":"code","8e52dd3d":"code","de9eed0b":"code","d06b367d":"code","7c060b51":"code","0ed37ed5":"code","dd1b6ad9":"code","14f872e9":"code","e27b0503":"code","0450337c":"code","e7f3e7b6":"code","1b09052b":"code","12b95d72":"code","4eb44ff0":"code","1c0e1712":"code","424c2938":"code","810273cb":"code","6cd2522b":"code","85a74226":"code","e26812c0":"code","bd407489":"code","fe1035f6":"code","f1b2b468":"code","f55e41f3":"code","aa5d79f9":"code","ddd02cb9":"code","cc05689e":"code","ec50e7a9":"code","763694af":"code","61fe96c3":"code","6957faea":"code","1d6fac61":"code","23d366e9":"code","5c6baee3":"code","2e94661f":"code","c6d5c132":"code","541912af":"code","9967dc16":"code","33f247d9":"code","e526b9c6":"code","62c0c0f5":"code","83713236":"code","3834c298":"code","b055c7dc":"code","2e110348":"code","e73f02b9":"code","ee4cfda0":"code","ae6f1858":"code","c5301672":"code","c14b5968":"code","abae755f":"code","52c77727":"code","10c035ba":"code","4ef1bf8f":"code","4749aaa4":"code","acfeb497":"code","4626fff9":"code","3fa853ec":"code","4a516e51":"markdown","ec045bce":"markdown","45f211de":"markdown","1db48527":"markdown","961f3348":"markdown","de8975cd":"markdown","b4196363":"markdown","34358ce6":"markdown","e2a7fe08":"markdown","64ef2b00":"markdown","cde5ac7a":"markdown","9dd83af2":"markdown","45826470":"markdown","2c585fe6":"markdown","58bf0309":"markdown","5cd46afe":"markdown","434c74b8":"markdown","9ec976fe":"markdown","d28ae547":"markdown","55b60301":"markdown","dd803ca7":"markdown","158bb02f":"markdown","e632172b":"markdown","6d2bfdb2":"markdown","0cca7eb5":"markdown","6a173dc8":"markdown","4073d5a5":"markdown","d2e83ff3":"markdown","e77ccef9":"markdown","81787ec3":"markdown","70bc4c23":"markdown","54814e79":"markdown","c44f1ed4":"markdown","abf404e5":"markdown","1148d1ed":"markdown","f0872986":"markdown","262b933b":"markdown","1397b071":"markdown","989be36a":"markdown","482de6da":"markdown","21231749":"markdown","69c4503b":"markdown","93e7639d":"markdown","1110cca2":"markdown","37949ab2":"markdown","311daf62":"markdown","098f979a":"markdown","89091a0c":"markdown","193989c7":"markdown","e046901b":"markdown","8810c225":"markdown"},"source":{"99334b0b":"import pandas as pd\nimport numpy as np\n\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, StackingClassifier, BaggingClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler\nfrom sklearn.impute import KNNImputer\nfrom collections import defaultdict\n\nfrom sklearn.metrics import roc_auc_score, roc_auc_score\nfrom itertools import product\n\nfrom tqdm import tqdm\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","95cd31c6":"df = pd.read_csv('..\/input\/cirrhosis-prediction-dataset\/cirrhosis.csv')\ndf.head()","a20029ca":"df.info()","28fe3b15":"df['Drug'].value_counts()","aa0da0e3":"df['Drug'] = df['Drug'].fillna('NotParticipated')\ndf","a8db6982":"def encode_with_nan(data):\n    \"\"\"\n        Encode cat columns in df skipping nan. 'fit_transform' in LabelEncoder()\n    \"\"\"\n    cat_cols = data.dtypes\n    cat_cols = list(cat_cols[cat_cols == 'object'].index)\n    \n    decoder = dict.fromkeys(cat_cols, dict())\n    \n    for col in cat_cols:\n        vals = list(data[col].unique())\n        if np.nan in vals:\n            vals.remove(np.nan)\n        \n        d = dict.fromkeys(vals, None)\n        for val in enumerate(vals):\n            d[val[1]] = val[0]\n            \n        decoder[col] = d\n        data[col] = data[col].apply(lambda x: d[x] if x in d.keys() else x)\n    return data, decoder\n\n\ndef decode_cat(data, decoder):\n    \"\"\"\n        Decode cat columns in df skipping nan. 'inverse_transform' in LabelEncoder()\n    \"\"\"\n    cat_cols = list(decoder.keys())\n    \n    for col in cat_cols:\n        vals = list(data[col].unique())\n        if np.nan in vals:\n            vals.remove(np.nan)\n        \n        keys = list(decoder[col].keys())\n        data[col] = data[col].apply(lambda x: keys[list(decoder[col].values()).index(x)] if x in decoder[col].values() else x)\n    return data","49a1e557":"df, decoder = encode_with_nan(df)\ndecoder","f6ad4c64":"def knn_imputer2df(data, n_neighbors=4, weights='distance'):\n    \"\"\"\n        Replaces nan values with prediction by KNN. Can be customed (n_neighbours, weights)\n    \"\"\"\n    df_column_names = list(data.columns)\n    \n    imputer = KNNImputer(n_neighbors=n_neighbors, weights=weights)\n    data = imputer.fit_transform(data)\n    return pd.DataFrame(data=data, columns=df_column_names)","fdbf24de":"df = knn_imputer2df(df)\ndf","21b67afd":"df.describe()","11fb1e2d":"def round_encoded_cat_features(data, decoder):\n    columns = list(decoder.keys())\n    data.loc[:, columns] = data.loc[:, columns].apply(round)\n    data['Stage'] = data['Stage'].apply(round)\n    return data","09d489a1":"temp_df = df.copy()\ntemp_df = round_encoded_cat_features(temp_df, decoder)\ntemp_df","80521c5b":"temp_df = decode_cat(temp_df, decoder)\ntemp_df","3b0064e0":"plt.figure(figsize=(12, 8))\nplt.title('Cirrhosis stage and treatment')\nsns.boxplot(x=temp_df['Drug'], y=temp_df['Stage']);","549f3cf7":"plt.figure(figsize=(12, 8))\nplt.title('Days spent in observation and stage')\nsns.scatterplot(x=temp_df['N_Days'], y=temp_df['Stage'], hue=temp_df['Drug']);","178da353":"temp_df['Sex'].value_counts()","7872e13f":"plt.figure(figsize=(12, 8))\nplt.title('Cirrhosis stage and Sex')\nsns.boxplot(x=temp_df['Sex'], y=temp_df['Stage']);","79bd1357":"plt.figure(figsize=(12, 8))\nplt.title('Cirrhosis stage and patient status')\nsns.boxplot(x=temp_df['Status'], y=temp_df['Stage'], hue=temp_df['Drug']);","3d394b81":"plt.figure(figsize=(12, 8))\nplt.title('Cirrhosis stage and age')\nsns.scatterplot(x=temp_df['Age'] \/ 365, y=temp_df['Stage'], hue=temp_df['Drug']);","ccbbbc01":"temp_df['Age'].mean() \/ 365","594103ae":"temp_df['Stage'].value_counts()","a86a0338":"diseases = list(temp_df.iloc[:,6:-1].columns)\n\nfor d in diseases:\n    if temp_df[d].dtype == 'object':\n        plt.figure(figsize=(8,4))\n        name = 'Cirrhosis stage and ' + str(d)\n        plt.title(name)\n        sns.boxplot(x=temp_df[d], y=temp_df['Stage'], hue=temp_df['Drug']);\n    else:\n        plt.figure(figsize=(8,4))\n        name = 'Cirrhosis stage and ' + str(d)\n        plt.title(name)\n        sns.scatterplot(x=temp_df[d], y=temp_df['Stage'], hue=temp_df['Drug']);","16732eb4":"plt.figure(figsize=(12,8))\nplt.title('Feature corr')\nsns.heatmap(df.corr());","4cb190a5":"#df = df.drop('ID', axis=1)","e7e39c04":" for col in list(temp_df.iloc[:,1:].columns):\n        plt.figure(figsize=(8, 4))\n        name = str(col) + ' Distribution'\n        plt.title(name)\n        sns.histplot(temp_df[col]);","8e52dd3d":"df","de9eed0b":"columns = ['Bilirubin', 'Cholesterol', 'Copper', 'Alk_Phos', 'SGOT', 'Tryglicerides', 'Platelets', 'Prothrombin']\nfor col in columns:\n    temp_df[col] = temp_df[col].apply(lambda x: np.log(x + 1))\n    df[col] = df[col].apply(lambda x: np.log(x + 1))","d06b367d":" for col in list(temp_df.iloc[:,1:].columns):\n        plt.figure(figsize=(8, 4))\n        name = str(col) + ' Distribution'\n        plt.title(name)\n        sns.histplot(temp_df[col]);","7c060b51":"def remove_outliers(df, column_list):\n    for col in column_list:\n        Q1 = np.quantile(df[col], 0.25)\n        Q3 = np.quantile(df[col], 0.75)\n        IQR = Q3 - Q1\n        \n        drop_outliers = [x for x in df[col] if (\n        (x > Q1 - 1.5 * IQR) & (x < Q3 + 1.5 * IQR))]\n        df = df.loc[df[col].isin(drop_outliers)]\n    return df","0ed37ed5":"X, y = df.drop('Stage', axis=1), df['Stage']\ny = y.apply(int)\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)\nX.shape","dd1b6ad9":"X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.33, random_state=17)","14f872e9":"def add_performance_to_df(df_name, name_model, model, train_X, train_y, test_X, test_y):\n    adder = {'model' : '', 'train_roc_auc_score_ovo': '', 'train_roc_auc_score_ovr': '',\n             'test_roc_auc_score_ovo': '', 'test_roc_auc_score_ovr': ''}\n    \n    train_proba_predictions = model.predict_proba(train_X)\n    test_proba_predictions = model.predict_proba(test_X)\n    \n    adder['model'] = name_model\n    adder['train_roc_auc_score_ovo'] = roc_auc_score(y_true=train_y, y_score = train_proba_predictions, average='macro', multi_class='ovo')\n    adder['train_roc_auc_score_ovr'] = roc_auc_score(y_true=train_y, y_score = train_proba_predictions, average='macro', multi_class='ovr')\n    adder['test_roc_auc_score_ovo'] = roc_auc_score(y_true=test_y, y_score = test_proba_predictions, average='macro', multi_class='ovo')\n    adder['test_roc_auc_score_ovr'] = roc_auc_score(y_true=test_y, y_score = test_proba_predictions, average='macro', multi_class='ovr')\n    \n    \n    return df_name.append(adder, ignore_index=True)\n\n\ndef get_models_performance(models, X_train, y_train, X_test, y_test):\n    cols = ['model', 'train_roc_auc_score_ovo', 'train_roc_auc_score_ovr', \n            'test_roc_auc_score_ovo', 'test_roc_auc_score_ovr']\n    \n    model_performance = pd.DataFrame(columns=cols)\n\n    for key in models:\n        model_performance = add_performance_to_df(model_performance, key, models[key],\n                                                  X_train, y_train, X_test, y_test)\n    return model_performance","e27b0503":"def data_prep(outliers_treatment=None, drop_id=False, floating_cat_features_encoding=False):\n    data = pd.read_csv('..\/input\/cirrhosis-prediction-dataset\/cirrhosis.csv')\n    data['Drug'] = data['Drug'].fillna('NotParticipated')\n    data, decoder = encode_with_nan(data)\n    data = knn_imputer2df(data)\n    \n    if not floating_cat_features_encoding:\n        data = round_encoded_cat_features(data, decoder)\n    \n    if outliers_treatment == 'log':\n        columns = ['Bilirubin', 'Cholesterol', 'Copper', 'Alk_Phos', 'SGOT', 'Tryglicerides', 'Platelets', 'Prothrombin']\n        for col in columns:\n            data[col] = data[col].apply(lambda x: np.log(x + 1))\n    \n    if outliers_treatment == 'IQR':\n        columns = ['Bilirubin', 'Cholesterol', 'Copper', 'Alk_Phos', 'SGOT', 'Tryglicerides', 'Platelets', 'Prothrombin']\n        data = remove_outliers(data, columns)\n        \n    \n    if drop_id:\n        data = data.drop('ID', axis=1)\n        \n    X, y = data.drop('Stage', axis=1), data['Stage']\n    y = y.apply(int)\n    scaler = StandardScaler()\n    X_scaled = scaler.fit_transform(X)\n    \n    return X_scaled, y","0450337c":"def pipeline(outliers_treatment=None, drop_id=False, floating_cat_features_encoding=False):\n    X_scaled, y = data_prep(outliers_treatment=outliers_treatment, drop_id=drop_id, floating_cat_features_encoding=floating_cat_features_encoding)\n    print('X shape: ', X_scaled.shape, ' y shape: ', y.shape)\n    \n    \n    X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.33, random_state=17)\n    \n    # lr\n    lr = LogisticRegression(random_state=17)\n    lr.fit(X_train, y_train)\n    # ----------------------------------\n\n    # lr_gcv\n    lr_params = {'solver': ['sag', 'saga', 'liblinear'],\n                 'C': np.logspace(-3, 1, 5),\n                 'penalty': ['l1', 'l2', 'elasticnet'],\n                 'class_weight': ['balanced', None]\n                }\n\n    lr_gcv = GridSearchCV(estimator=LogisticRegression(random_state=17), param_grid=lr_params, scoring='roc_auc_ovo', cv=5, verbose=True, n_jobs=-1)\n    lr_gcv.fit(X_train, y_train)\n    best_lr = lr_gcv.best_estimator_\n    # -----------------------------\n    \n    # knn\n    knn = KNeighborsClassifier()\n    knn.fit(X_train, y_train)\n    # --------------------------\n    \n    # knn_gcv\n    knn_params = {'n_neighbors': range(2, 8),\n              'weights': ['uniform', 'distance'],\n              'algorithm': ['ball_tree', 'kd_tree'], \n              'p': range(1, 5)}\n\n    knn_gcv = GridSearchCV(estimator=KNeighborsClassifier(), param_grid=knn_params,  scoring='roc_auc_ovo', cv=5, verbose=True, n_jobs=-1)\n    knn_gcv.fit(X_train, y_train)\n    best_knn = knn_gcv.best_estimator_\n    # --------------------\n    \n    # svm\n    svm = SVC(probability=True, random_state=17)\n    svm.fit(X_train, y_train)\n    # --------------------\n    \n    # svm_gcv\n    svm_params = {'C': np.logspace(-4, 1, 6),\n              'kernel': ['linear', 'poly', 'rbf', 'sigmoid'],\n              'class_weight': ['balanced', None]}\n\n    svm_gcv = GridSearchCV(estimator=SVC(random_state=17, probability=True), param_grid=svm_params, scoring='roc_auc_ovo', cv=5, verbose=True, n_jobs=-1)\n    svm_gcv.fit(X_train, y_train)\n    best_svm = svm_gcv.best_estimator_\n    # -----------------------\n\n    # rf\n    rf = RandomForestClassifier(random_state=17)\n    rf.fit(X_train, y_train)\n    # ------------------------------------\n\n    # rf_gcv\n    rf_params = {'n_estimators': range(20, 120, 20),\n                 'criterion': ['gini', 'entropy'],\n                 'min_samples_split': range(2, 8, 2),\n                 'min_samples_leaf': range(1, 5),\n                 'class_weight': ['balanced', 'balanced_subsample', None]}\n\n\n    rf_gcv = GridSearchCV(estimator=RandomForestClassifier(random_state=17), param_grid=rf_params, scoring='roc_auc_ovo', cv=5, verbose=True, n_jobs=-1)\n    rf_gcv.fit(X_train, y_train)\n    best_rf = rf_gcv.best_estimator_\n    # ----------------------------\n\n    # gb\n    gb = GradientBoostingClassifier(random_state=17)\n    gb.fit(X_train, y_train)\n    # ----------------------------\n\n    # gb_gcv\n    gb_params = {'loss': ['deviance', 'exponential'],\n                 'learning_rate': np.logspace(-4, 1, 4),\n                 'n_estimators': range(60, 160, 20),\n                 'min_samples_split': range(2, 8, 2),\n                 'min_samples_leaf': range(1, 5),\n                 'max_features': ['sqrt', 'log2']}\n\n\n    gb_gcv = GridSearchCV(estimator=GradientBoostingClassifier(random_state=17), param_grid=gb_params, scoring='roc_auc_ovo', cv=5, verbose=True, n_jobs=-1)\n    gb_gcv.fit(X_train, y_train)\n    best_gb = gb_gcv.best_estimator_\n\n    \n    lr = {'LogReg': lr}\n    best_lr = {'LogReg_CV': best_lr}\n    knn = {'KNN': knn}\n    best_knn = {'KNN-CV': best_knn}\n    svm = {'SVM': svm}\n    best_svm = {'SVM-CV': best_svm}\n    rf = {'RandomForest': rf}\n    best_rf = {'RandomForest_CV': best_rf}\n    gb = {'GradBoost': gb}\n    best_gb =  {'GradBoost_CV': best_gb}\n    \n    models = {**lr, **best_lr, **knn, **best_knn, **svm, **best_svm, **rf, **best_rf, **gb, **best_gb}\n    \n    print('making perform...')\n    model_performance = get_models_performance(models=models, X_train=X_train, y_train=y_train, X_test=X_test, y_test=y_test)\n    print('Perform ready')\n    return model_performance, models","e7f3e7b6":"outliers = ['log', 'IQR', None]\ndrop_id = [False, True]\nfcfe = [False, True]\ncombinations = list(product(outliers, drop_id, fcfe))\ncombinations","1b09052b":"perf_n_models = []\nfor combo in tqdm(combinations):\n    print(combo)\n    perf_n_models.append(pipeline(*combo))","12b95d72":"# ('log', False, False)\nperf_n_models[0][0]","4eb44ff0":"perf_n_models[0][0].describe()","1c0e1712":"#  ('log', False, True),\nperf_n_models[1][0]","424c2938":"perf_n_models[1][0].describe()","810273cb":"# ('log', True, False),\nperf_n_models[2][0]","6cd2522b":"perf_n_models[2][0].describe()","85a74226":"# ('log', True, True),\nperf_n_models[3][0]","e26812c0":"perf_n_models[3][0].describe()","bd407489":"#('IQR', False, False),\nperf_n_models[4][0]\n","fe1035f6":"perf_n_models[4][0].describe()","f1b2b468":"#('IQR', False, True),\nperf_n_models[5][0]","f55e41f3":"perf_n_models[5][0].describe()","aa5d79f9":"#('IQR', True, False),\nperf_n_models[6][0]\n","ddd02cb9":"perf_n_models[6][0].describe()","cc05689e":"#('IQR', True, True),\nperf_n_models[7][0]","ec50e7a9":"perf_n_models[7][0].describe()","763694af":"#(None, False, False),\nperf_n_models[8][0]","61fe96c3":"perf_n_models[8][0].describe()","6957faea":"#(None, False, True),\nperf_n_models[9][0]","1d6fac61":"perf_n_models[9][0].describe()","23d366e9":"#(None, True, False),\nperf_n_models[10][0]","5c6baee3":"perf_n_models[10][0].describe()","2e94661f":"#(None, True, True)]\nperf_n_models[11][0]","c6d5c132":"perf_n_models[11][0].describe()","541912af":"# ('log', False, True) mean: 0.700372, 0.730750;  min: 0.611770, 0.629183; max: 0.748286, 0.779441\n# (None, True, True)   mean: 0.702960, 0.733629;  min: 0.650990, 0.662221; max: 0.762657, 0.781433","9967dc16":"def df_for_blend(models, models_name, tr_X, te_X, tr_y, te_y):\n    tr_blend_df = pd.DataFrame(columns=models_name) \n    te_blend_df = pd.DataFrame(columns=models_name)\n    for model, name in zip(models, models_name):\n        model.fit(tr_X, tr_y)\n        train_pred = model.predict(tr_X)\n        test_pred = model.predict(te_X)\n        tr_blend_df[name] = train_pred\n        te_blend_df[name] = test_pred\n    return tr_blend_df, te_blend_df\n\n\ndef blend_clf(tr_blend_df, te_blend_df, tr_y, te_y):\n    lr = LogisticRegression(solver='liblinear', random_state=17)\n    lr.fit(tr_blend_df, tr_y)\n    return lr","33f247d9":"first_approach = perf_n_models[1][0].copy() # ('log', False, True)\nfirst_approach","e526b9c6":"X1_scaled, y1 = data_prep('log', False, True)\nX1_train, X1_test, y1_train, y1_test = train_test_split(X1_scaled, y1, test_size=0.33, random_state=17)","62c0c0f5":"models = [perf_n_models[1][1]['LogReg_CV'], perf_n_models[1][1]['KNN-CV'],\n          perf_n_models[1][1]['SVM'], perf_n_models[1][1]['RandomForest'], perf_n_models[1][1]['GradBoost_CV']]\n\nmodel_names = ['LogReg_CV', 'KNN-CV', 'SVM', 'RandomForest', 'GradBoost_CV']\n\ntr_blend_df, te_blend_df = df_for_blend(models=models, models_name=model_names,\n                                        tr_X=X1_train, te_X=X1_test, tr_y=y1_train, te_y=y1_test)\n\ntr_blend_df\n","83713236":"plt.figure(figsize=(12, 8))\nsns.heatmap(tr_blend_df.corr());","3834c298":"tr_blend_df, te_blend_df = tr_blend_df.drop('GradBoost_CV', axis=1), te_blend_df.drop('GradBoost_CV', axis=1)","b055c7dc":"blend = blend_clf(tr_blend_df, te_blend_df, y1_train, y1_test)\nfirst_approach = add_performance_to_df(perf_n_models[1][0], 'BlendingClassifier', blend, tr_blend_df, y1_train, te_blend_df, y1_test)\nfirst_approach","2e110348":"second_approach = perf_n_models[11][0] #(None, True, True)]\nsecond_approach","e73f02b9":"X2_scaled, y2 = data_prep(None, True, True)\nX2_train, X2_test, y2_train, y2_test = train_test_split(X2_scaled, y2, test_size=0.33, random_state=17)","ee4cfda0":"models = [perf_n_models[11][1]['LogReg'], perf_n_models[11][1]['KNN-CV'],\n          perf_n_models[11][1]['SVM'], perf_n_models[11][1]['RandomForest_CV'], perf_n_models[11][1]['GradBoost_CV']]\n\nmodel_names = ['LogReg', 'KNN-CV', 'SVM', 'RandomForest_CV', 'GradBoost_CV']\n\ntr_blend_df, te_blend_df = df_for_blend(models=models, models_name=model_names,\n                                        tr_X=X2_train, te_X=X2_test, tr_y=y2_train, te_y=y2_test)\n\ntr_blend_df\n","ae6f1858":"plt.figure(figsize=(12, 8))\nsns.heatmap(tr_blend_df.corr());","c5301672":"tr_blend_df, te_blend_df = tr_blend_df.drop('GradBoost_CV', axis=1), te_blend_df.drop('GradBoost_CV', axis=1)","c14b5968":"blend = blend_clf(tr_blend_df, te_blend_df, y2_train, y2_test)\nsecond_approach = add_performance_to_df(second_approach, 'BlendingClassifier', blend, tr_blend_df, y2_train, te_blend_df, y2_test)\nsecond_approach","abae755f":"def stacking_clf(models, model_names, tr_X, tr_y):\n    level0 = list()\n    for model, name in zip(models, model_names):\n        level0.append((name, model))\n        \n    level1 = LogisticRegression(C=0.001, class_weight='balanced', random_state=17, solver='liblinear')\n    stcl = StackingClassifier(estimators=level0, final_estimator=level1, cv=5, stack_method='predict_proba', passthrough=True, n_jobs=-1)\n    stcl.fit(tr_X, tr_y)\n    return stcl","52c77727":"model_names, models = list(perf_n_models[1][1].keys()), list(perf_n_models[1][1].values()),\nstacking = stacking_clf(models, model_names, X1_train, y1_train)\nfirst_approach = add_performance_to_df(first_approach, 'StackingClassifier', stacking, X1_train, y1_train, X1_test, y1_test)\nfirst_approach","10c035ba":"model_names, models = list(perf_n_models[11][1].keys()), list(perf_n_models[11][1].values()),\nstacking = stacking_clf(models, model_names, X2_train, y2_train)\nsecond_approach = add_performance_to_df(second_approach, 'StackingClassifier', stacking, X2_train, y2_train, X2_test, y2_test)\nsecond_approach","4ef1bf8f":"bagg = BaggingClassifier(base_estimator=perf_n_models[1][1]['GradBoost_CV'], random_state=17, bootstrap=True)\nbagg.fit(X1_train, y1_train)\nfirst_approach = add_performance_to_df(first_approach, 'BaggingClassifier(GradBoost)', bagg, X1_train, y1_train, X1_test, y1_test)\nfirst_approach","4749aaa4":"bagg = BaggingClassifier(base_estimator=perf_n_models[11][1]['RandomForest_CV'], random_state=17, bootstrap=True)\nbagg.fit(X2_train, y2_train)\nsecond_approach = add_performance_to_df(second_approach, 'BaggingClassifier(RandomForest_CV)', bagg, X2_train, y2_train, X2_test, y2_test)\nsecond_approach","acfeb497":"best_first_ap_model = perf_n_models[1][1]['GradBoost_CV']\nthe_best_second_ap_model = perf_n_models[11][1]['RandomForest_CV']","4626fff9":"plt.figure(figsize=(12, 8))\n\n\nrf_indices = np.argsort(the_best_second_ap_model.feature_importances_)[::-1]\n\n\nax2 = sns.barplot(y=df.drop('ID', axis=1).columns[rf_indices], x = best_first_ap_model.feature_importances_[rf_indices] , orient='h')\nax2.set_xlabel('Importance', fontsize=12)\nax2.set_ylabel('Features', fontsize=12)\nax2.set_title('Second approach RF-CV (The best!)');\n\n","3fa853ec":"plt.figure(figsize=(12, 8))\ngb_indices = np.argsort(best_first_ap_model.feature_importances_)[::-1]\nax1 = sns.barplot(y=df.columns[gb_indices], x = best_first_ap_model.feature_importances_[gb_indices] , orient='h')\nax1.set_xlabel('Importance', fontsize=12)\nax1.set_ylabel('Features', fontsize=12)\nax1.set_title('First approach GB-CV (Not the best)')","4a516e51":"**Functions that make stats for models performance**","ec045bce":"### first approach","45f211de":"So we faced problem of making decision which approach is better.  \n1. We can treat outliers performing log transform, deleting observation using quantiles or just leave them;  \n2. We can drop or leave *'ID'* column; \n3. We can use floating encoding for categorical features or just round them.  \n","1db48527":"* Stacking-type ensemble where the meta-model is trained on out-of-fold predictions made during k-fold cross-validation","961f3348":"**In second approach gradboosting also highly corr**","de8975cd":"***So one can observe relations between features and target. It might be some outliers in bilirium, chole., copper, alk_ph columns.***","b4196363":"**So now we will try to increase models performance of two different approaches:**  \n1. Log features transformation + with ID column + discrete encoding for categorical features\n2. No outliers treatment + whithout ID column + discrete encoding for categorical features","34358ce6":"Gradient booosting has 1.0 corr with each model. So lets delete it.","e2a7fe08":"### second approach","64ef2b00":"*First inspection: Those who  use Placebo tend to have illness progression*","cde5ac7a":"# Solving missing values problem ","9dd83af2":"*ID column can be dropped, but let`s leave it*","45826470":"* Are models correlated?","2c585fe6":"# Scaling","58bf0309":"*Cross-Validation* will be using scoring 'roc_auc_ovo' and 5 fields","5cd46afe":"## Categorical values encoding","434c74b8":"**So none of ensemble model helped to increase score**","9ec976fe":"### So the best score has RF_CV without any tratment of outliers, whithout ID column and with discrete encoding for categorical features","d28ae547":"### First approach","55b60301":"*not informative (disbalanced)*","dd803ca7":"## KNN-Imputer","158bb02f":"Let`s try each combination of possible data preparation while training models. It will take some time, but results may surprise.","e632172b":"### Second approach","6d2bfdb2":"* **Here one can see that category columns in past was fill in by floating numbers.**\n* Lets leave it as it is. this values can be interpreted as proximity to 'floor' and 'round' existing values. If this treatment will decrease performance we will round this observations.","0cca7eb5":"1. We ll test any possible combinations of preprocess steps and compare models results. Then will choose best approach of data prep","6a173dc8":"## Stacking ","4073d5a5":"**As we can see Blending does not improve score**","d2e83ff3":"## Blending","e77ccef9":"# Handling Outliers","81787ec3":"**models**:\n1. LogReg;\n2. LogReg with cross-validation;\n3. KNN;\n4. KNN with cross-validation;\n5. SVM;\n6. SVM with cross-validation;\n7. RandomForest;\n8. RandomForest with cross-validation;\n9. GradientBoosting;\n10. GradientBoosting with cross-validation.","70bc4c23":"**Bagging became as good as a simple gradient boosting**","54814e79":"*not informative plot*","c44f1ed4":"# EDA","abf404e5":"**Again stacking does not change a thing**","1148d1ed":"### Second Approach","f0872986":"**Log features transform**","262b933b":"# Building Ensemble Classifiers","1397b071":"1. Decode DataFrame to represent relationships clearly  \n2. Analysys and plots","989be36a":"## Bagging","482de6da":"* There NaN values in both: categorical columns and numerical\n* Before solving NaN-values-problem we will encode cat. variables. But there will be used custom encoder wich allows to encode, skipping NaN values and perform encoding under a group of columns.\n* To fill in empty values, we will use imputation based on KNN alg. This kind of imputation solves supervised learning problem for each column with missing values, considering such col as target.","21231749":"Let`s look at descriptive statistics ","69c4503b":"# Feature importance","93e7639d":"**Here stacking shows mid score**","1110cca2":"* Stacking-type ensemble where the meta-model is trained on predictions made on a holdout dataset.","37949ab2":"## Finding Optimal Approach","311daf62":"*STATUS????*","098f979a":"**Attribute Information**\n1) ID: unique identifier  \n2) N_Days: number of days between registration and the earlier of death, transplantation, or study analysis time in July 1986  \n3) Status: status of the patient C (censored), CL (censored due to liver tx), or D (death)  \n4) Drug: type of drug D-penicillamine or placebo  \n5) Age: age in [days]  \n6) Sex: M (male) or F (female)  \n7) Ascites: presence of ascites N (No) or Y (Yes)  \n8) Hepatomegaly: presence of hepatomegaly N (No) or Y (Yes)  \n9) Spiders: presence of spiders N (No) or Y (Yes)  \n10) Edema: presence of edema N (no edema and no diuretic therapy for edema), S (edema present without diuretics, or edema resolved by diuretics), or Y (edema despite diuretic therapy)  \n11) Bilirubin: serum bilirubin in [mg\/dl]  \n12) Cholesterol: serum cholesterol in [mg\/dl]  \n13) Albumin: albumin in [gm\/dl]  \n14) Copper: urine copper in [ug\/day]  \n15) Alk_Phos: alkaline phosphatase in [U\/liter]  \n16) SGOT: SGOT in [U\/ml]  \n17) Triglycerides: triglicerides in [mg\/dl]  \n18) Platelets: platelets per cubic [ml\/1000]  \n19) Prothrombin: prothrombin time in seconds [s]  \n20) Stage: histologic stage of disease (1, 2, 3, or 4)  ","89091a0c":"### First approach","193989c7":"* First of all, dataframe is relatively small. *So we should try to get as much as it possible from each record in given table*.   \n* Those observations that have missing values in 'Drug' column - *group that allowed to gather their anamnesis but **refused** to participate in drug test*.  So this group of people have label *'NotParticipated'* in *'Drug'* column.  ","e046901b":"**Here also blending performs very bad**","8810c225":"# Models"}}