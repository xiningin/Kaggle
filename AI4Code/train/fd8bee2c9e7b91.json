{"cell_type":{"81b3f876":"code","36fd4a94":"code","e0a617c5":"code","d360972a":"code","46f04133":"code","8e17b759":"code","45ae4344":"code","d13d9790":"code","05d9922f":"code","d1015aad":"code","bf7be088":"code","dc7483c2":"code","6cbf48d6":"code","ddd038a8":"code","8623ad4f":"code","c030268a":"code","bfd2fe3c":"code","5c3fad43":"code","cf5e11ca":"code","66555933":"code","1180a151":"code","2b3eb4ba":"code","6b4853bd":"code","47d00abd":"code","2691337a":"code","0fee9982":"code","f18d17d6":"code","4dfaff6d":"code","efb13fef":"code","f461eaab":"code","9f5c6275":"code","b679b9a8":"code","6aaece3a":"code","5a8cadef":"code","267e7316":"code","68a4250b":"code","9e4a1c7e":"code","90271bf1":"code","de7ff880":"code","8ac4d630":"code","d490857a":"code","6a7d0430":"code","2ac26f96":"code","4a6d625b":"code","4c646de8":"code","55c2d6cb":"code","a8efc6d8":"code","bb45eb58":"code","c3d5840f":"code","ec5d4ff7":"code","9df690df":"code","76978d7d":"code","2b18d851":"code","b1c97961":"code","633365e9":"code","c9404f23":"code","9faf178a":"code","9ae9d358":"code","70f3fce2":"code","2365fdeb":"code","a67456b5":"code","d7b3ce45":"code","f1f0a561":"code","a5cbf7e0":"code","3a52106a":"code","fbf730a0":"code","1501ceb3":"code","5f82ebb8":"code","a2d37801":"code","4617586b":"code","9b0aad75":"markdown","e03ad8da":"markdown","276c36e5":"markdown","687b1637":"markdown","f7b72e71":"markdown","44d801e7":"markdown","af2db12d":"markdown","9c5dfb61":"markdown","c0900b10":"markdown","1ca5f14e":"markdown","a19e542b":"markdown","6ddbd864":"markdown","92e73bb7":"markdown","cf1e0995":"markdown","5cc51a9f":"markdown","9dec35dd":"markdown","6b86da83":"markdown","f4269133":"markdown","e932ea5b":"markdown"},"source":{"81b3f876":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","36fd4a94":"train_data = pd.read_csv(\"\/kaggle\/input\/titanic\/train.csv\")\ntrain_data['Source'] = 'Train'\ntrain_data.head()","e0a617c5":"test_data = pd.read_csv(\"\/kaggle\/input\/titanic\/test.csv\")\ntest_data['Source'] = 'Test'\ntest_data.head()","d360972a":"combine = pd.concat([train_data, test_data], ignore_index = True, sort=False)\ncombine.head() # This is to print top 5 records.","46f04133":"combine.tail() # This is to print last 5 records","8e17b759":"# As mentioned in Competation, that the Submission file should have 2 columns such as \"PassengerId\" and \"Survived\". So we can have PassengerId as our Index in all the 3 data frames, and the Survived for submission file will be predicted later.\ntrain_data.set_index(['PassengerId'], inplace = True)\ntest_data.set_index(['PassengerId'], inplace = True)\ncombine.set_index(['PassengerId'], inplace = True)","45ae4344":"train_data.head()","d13d9790":"# Checking the Columns.\nprint(train_data.columns.values)\nprint('-'*50)\nprint(train_data.columns.values)\nprint('-'*50)\nprint(combine.columns.values)","05d9922f":"# Checking the data-type of DF's.\ntrain_data.info()\nprint('_'*40)\ntest_data.info()","d1015aad":"# Alternate way of checking the size of data frames.\nprint(train_data.shape)\nprint('_'*20)\nprint(test_data.shape) # Here we do not have column with name Survived.\nprint('_'*20)\nprint(combine.shape)","bf7be088":"# Lets see the status of our data.\ntrain_data.describe()","dc7483c2":"train_data.describe(include=['O'])","6cbf48d6":"# visualization\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline","ddd038a8":"# Lets see the Correlation of each variable to Target Variable using HeatMap.\npearson_corr = train_data.corr(method ='pearson') \n\nkendall_corr = train_data.corr(method ='kendall') ","8623ad4f":"#plt.subplot(221)\nsns.heatmap(pearson_corr, annot=True) # heatmap(pearson_corr, cmap=\"YlGnBu\") # Just to have different colors.\n\n#plt.subplot(222)\n#sns.heatmap(kendall_corr, cmap = 'cubehelix') # heatmap(kendall_corr, cmap=\"YlGnBu\") # Just to have different colors.\n\nplt.show()","c030268a":"sns.heatmap(kendall_corr, annot=True, cmap = 'cubehelix') # heatmap(kendall_corr, cmap=\"YlGnBu\") # Just to have different colors.\nplt.show()","bfd2fe3c":"train_data.isnull().sum()","5c3fad43":"test_data.isnull().sum()","cf5e11ca":"# Another way of identifying Null or Missing Values\nnull_value_stats = train_data.isnull().sum(axis=0)\nnull_value_stats[null_value_stats != 0]","66555933":"# Another way of identifying Null or Missing Values\ntotal = train_data.isnull().sum().sort_values(ascending=False)\npercent_1 = train_data.isnull().sum()\/train_data.isnull().count()*100\npercent_2 = (round(percent_1, 1)).sort_values(ascending=False)\nmissing_data = pd.concat([total, percent_2], axis=1, keys=['Total', '%'])\nmissing_data.head(5)","1180a151":"train_data['Age'].describe()","2b3eb4ba":"train_data['Age'] = train_data['Age'].fillna(train_data['Age'].mean())\ntest_data['Age'] = test_data['Age'].fillna(test_data['Age'].mean())\ncombine['Age'] = combine['Age'].fillna(combine['Age'].mean())","6b4853bd":"train_data['Age'].describe()","47d00abd":"train_data['Embarked'].describe()","2691337a":"print(train_data['Embarked'].mode())\nprint(combine['Embarked'].mode())","0fee9982":"# Seems \"S\" is the most common value for Embarked. Thus replacing the NaN with the same ie Mode.\ntrain_data['Embarked'] = train_data['Embarked'].fillna('S')\ncombine['Embarked'] = combine['Embarked'].fillna('S')","f18d17d6":"train_data['Embarked'].describe()","4dfaff6d":"train_data['Embarked'].isnull().sum(axis=0)","efb13fef":"sns.countplot(x='Survived', hue='Sex', data=train_data)","f461eaab":"# From here onwards, we will be using the combine data frame.\nsns.barplot(x=\"Embarked\", y=\"Survived\", hue=\"Sex\", data=combine);","9f5c6275":"g = sns.FacetGrid(train_data, col='Survived')\ng.map(plt.hist, 'Age', bins=20)","b679b9a8":"grid = sns.FacetGrid(train_data, col='Survived', row='Pclass', size=2.2, aspect=1.6)\ngrid.map(plt.hist, 'Age', alpha=.5, bins=20)\ngrid.add_legend();","6aaece3a":"sns.barplot(x='Pclass', y='Survived', data=train_data)","5a8cadef":"train_data.info()","267e7316":"train_data['Sex']","68a4250b":"#genders = {\"male\": 0, \"female\": 1}\n#for dataset in combine:\n#    dataset['Sex']= dataset['Sex'].map( genders ).astype(int)\n#\n#combine.head()\n\ntrain_data['Sex'] = train_data.Sex.apply(lambda x:0 if x == 'female' else 1)\ntest_data['Sex'] = test_data.Sex.apply(lambda x:0 if x == 'female' else 1)","9e4a1c7e":"train_data.head()","90271bf1":"train_data.groupby('Embarked').size()","de7ff880":"train_data['Embarked'] = train_data.Embarked.apply(lambda x:0 if x == 'S' else (1 if x == 'C' else 2 ))\ntest_data['Embarked'] = test_data.Embarked.apply(lambda x:0 if x == 'S' else (1 if x == 'C' else 2 ))","8ac4d630":"train_data.head()","d490857a":"train_data.groupby('Embarked').size()","6a7d0430":"# Group by Survived.\ntrain_data.groupby('Survived').mean()","2ac26f96":"# Group by Sex.\ntrain_data.groupby('Sex').mean()","4a6d625b":"# Defining Feature and Target.\nfeatures = [\"Pclass\", \"Sex\", \"SibSp\", \"Parch\", \"Age\", \"Embarked\"]\ntarget = train_data[\"Survived\"]","4c646de8":"train_data[features]","55c2d6cb":"# split the train_data into 2 DF's aka X_train, X_test, y_train, y_test.\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(train_data[features], target, test_size=0.2)\n\nprint (X_train.shape, y_train.shape)\nprint (X_test.shape, y_test.shape)","a8efc6d8":"# test_data \nX_test_df  = test_data[features].copy()","bb45eb58":"# machine learning\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC, LinearSVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.linear_model import Perceptron\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.tree import DecisionTreeClassifier","c3d5840f":"# Logistic Regression\n\nlogreg = LogisticRegression()\nlogreg.fit(X_train, y_train)\nY_pred_lr = logreg.predict(X_test)\n#print(Y_pred_lr)\nacc_log = round(logreg.score(X_train, y_train) * 100, 2)\nprint(\"Accuracy (LogisticRegression)\", acc_log)","ec5d4ff7":"# Predicting on test_data\nY_pred_test_df = logreg.predict(X_test_df)\nY_pred_test_df ","9df690df":"# Support Vector Machines\n\nsvc = SVC()\nsvc.fit(X_train, y_train)\nY_pred_svc = svc.predict(X_test)\nacc_svc = round(svc.score(X_train, y_train) * 100, 2)\nprint(\"Accuracy (Support Vector Machines)\", acc_svc)","76978d7d":"# KNN\nknn = KNeighborsClassifier(n_neighbors = 3)\nknn.fit(X_train, y_train)\nY_pred_knn = knn.predict(X_test)\nacc_knn = round(knn.score(X_train, y_train) * 100, 2)\nprint(\"Accuracy (KNN)\", acc_knn)","2b18d851":"# Gaussian Naive Bayes\n\ngaussian = GaussianNB()\ngaussian.fit(X_train, y_train)\nY_pred_gnb = gaussian.predict(X_test)\nacc_gaussian = round(gaussian.score(X_train, y_train) * 100, 2)\nprint(\"Accuracy (Gaussian Naive Bayes)\", acc_gaussian)","b1c97961":"# Perceptron\n\nperceptron = Perceptron()\nperceptron.fit(X_train, y_train)\nY_pred_per = perceptron.predict(X_test)\nacc_perceptron = round(perceptron.score(X_train, y_train) * 100, 2)\nprint(\"Accuracy (Perceptron)\", acc_perceptron)","633365e9":"# Linear SVC\n\nlinear_svc = LinearSVC()\nlinear_svc.fit(X_train, y_train)\nY_pred_lsvc = linear_svc.predict(X_test)\nacc_linear_svc = round(linear_svc.score(X_train, y_train) * 100, 2)\nprint(\"Accuracy (Linear SVC)\", acc_linear_svc)","c9404f23":"# Stochastic Gradient Descent\n\nsgd = SGDClassifier()\nsgd.fit(X_train, y_train)\nY_pred_sgc = sgd.predict(X_test)\nacc_sgd = round(sgd.score(X_train, y_train) * 100, 2)\nprint(\"Accuracy (Stochastic Gradient Descent)\", acc_sgd)","9faf178a":"# Decision Tree\n\ndecision_tree = DecisionTreeClassifier()\ndecision_tree.fit(X_train, y_train)\nY_pred_dt = decision_tree.predict(X_test)\nacc_decision_tree = round(decision_tree.score(X_train, y_train) * 100, 2)\nprint(\"Accuracy (Decision Tree)\", acc_decision_tree)","9ae9d358":"# Random Forest\n\nrandom_forest = RandomForestClassifier(n_estimators=100)\nrandom_forest.fit(X_train, y_train)\nY_pred_rf = random_forest.predict(X_test)\nacc_random_forest = round(random_forest.score(X_train, y_train) * 100, 2)\nprint(\"Accuracy (random Forest)\", acc_random_forest)","70f3fce2":"modelling_score = pd.DataFrame({\n    'Model': ['Support Vector Machines', 'KNN', 'Logistic Regression', \n              'Random Forest', 'Naive Bayes', 'Perceptron', \n              'Stochastic Gradient Decent', 'Linear SVC', \n              'Decision Tree'],\n    'Score': [acc_svc, acc_knn, acc_log, \n              acc_random_forest, acc_gaussian, acc_perceptron, \n              acc_sgd, acc_linear_svc, acc_decision_tree]})\n","2365fdeb":"modelling_score.sort_values(by='Score', ascending=False)","a67456b5":"X_test_df.shape","d7b3ce45":"# Predicting on actual test_data\nY_pred_test_df = random_forest.predict(X_test_df)\nY_pred_test_df ","f1f0a561":"X_test_df.head()","a5cbf7e0":"PassengerId = X_test_df.index","3a52106a":"PassengerId.shape","fbf730a0":"Y_pred_test_df.shape","1501ceb3":"submission = pd.DataFrame( { 'PassengerId': PassengerId , 'Survived': Y_pred_test_df } )","5f82ebb8":"print(\"Submission File Shape \",submission.shape)\nsubmission.head()\n","a2d37801":"submission.to_csv( '\/kaggle\/working\/titanic_prediction_submission.csv' , index = False )","4617586b":"#submission = pd.DataFrame({\n#        \"PassengerId\": X_test_df.index,\n#        \"Survived\": Y_pred_test_df\n#    })\n# submission.to_csv('..\/output\/titanic_prediction_submission.csv', index=False)","9b0aad75":"From the records \/ data above, we can say which columns data is categorical or numerical and others....\n\nCategorical: Survived, Sex, and Embarked. \n\nOrdinal    : Pclass.\n\nContinous (numeical): Age, Fare. \n\nDiscrete (numeical) : SibSp, Parch.\n\nAlphaNumeric (Others) : Ticket, Cabin.\n\nString (Others)       : Name.\n\nWe may not be interseted in Ticket, Cabin, Name fields \/ columns while doing the prediction.","e03ad8da":"# Analysing the data","276c36e5":"Onservation from above results :  \nMissing Values for Age fiels in Train Data Frame is 177; and 86 in Test Data Frame. Also for Cabin, 687 missing values in Train DF and 327 in Test DF; Embarked has 2 missing values in Train DF; Fare column has 1 missing value only in Test DF. Will clean this data.\nCabin column has highest count and it is more than 50%... and hard to fill or clean it. So we will not be using it. Similar with Fare from Test DF, its jst a single missing entry, so we can live without it.\nFinally we will be handling Age and Embarked missing values.","687b1637":"# Steps completed till now.\n1) Corellation - looks good. We already checked this using HeatMap. But still look with a combination of 2-3 columns.\n2) Filling Null values - We seen that Age and Cabin has some NaN values.. Already corrected\n3) Will check if any new feature need to introduce or not.\n4) Converting Classification features into Numerical. ","f7b72e71":"Decided to go with Random Forest.. as the Score with it is the highest.","44d801e7":"# Sex Vs Survived","af2db12d":"# Modeling","9c5dfb61":"Observations from above graph.\n\nPclass=3 had most passengers, however most did not survive.  \nInfant passengers in all class mostly survived.  \nMost passengers in Pclass=1 and 3 survived.  \nPclass varies in terms of Age distribution of passengers.","c0900b10":"Outcome of above result :\nNames are unique across the train dataset (count=unique=891)\nSex variable as two possible values with 65% male (top=male, freq=577\/count=891).\nCabin values have several dupicates across samples. Alternatively several passengers shared a cabin.\nEmbarked takes three possible values. S port used by most passengers (top=S)\nTicket feature has high ratio (22%) of duplicate values (unique=681).","1ca5f14e":"Observations from above graph.\n\nInfants (Age <=4) had high survival rate.\nAll the Oldest passengers (Age = 80) survived.\nLarge number of 15-30 year olds did not survive.\nMost passengers are in 15-35 age range.","a19e542b":"# Identifying best Model from above","6ddbd864":"# Variable Description\n\nSurvived: Survived (1) or died (0)\n\nPclass: Passenger's class\n\nName: Passenger's name\n\nSex: Passenger's sex\n\nAge: Passenger's age\n\nSibSp: Number of siblings\/spouses aboard\n\nParch: Number of parents\/children aboard\n\nTicket: Ticket number\n\nFare: Fare\n\nCabin: Cabin\n\nEmbarked: Port of embarkation","92e73bb7":"# Converting a categorical feature into Integer.","cf1e0995":"# Visualizing Data","5cc51a9f":"# Finding Missing Values","9dec35dd":"From above data, it seems we do have 891 records in train_df with 13 columns. And 418 records with 12 columns (Survived is missing).\nAlso there are some null or blank or NaN values are also there. This is identified based on the count of each records.\nSay in train dataframe, total records are 891... where as Age and Cabin has less records, which meaning remaining are NaN (missing values). Will see the NaN values later.","6b86da83":"Outcome from above heat map of Correlation between variables.\nOne thing that that the Pearson Correlation plot can tell us is that there are not too many features strongly correlated with one another. This is good from a point of view of feeding these features into your learning model because this means that there isn't much redundant or superfluous data in our training set and we are happy that each feature carries with it some unique information.","f4269133":"# Submission","e932ea5b":"# Load Titanic Data-Set\nThe very first step is to load the train and test data-frame and also labelling it with new field 'Source'. This is just to have remember which data is from train, as next we will be combining both the DF's for further analysis on data to get more insights."}}