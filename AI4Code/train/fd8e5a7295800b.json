{"cell_type":{"8d6def9c":"code","17d4e2f5":"code","d475bbc5":"code","b0d835bc":"code","fd7bc930":"code","f5ada4f3":"code","a15ac220":"code","c795eb34":"code","000e6238":"code","25d12913":"code","53f4f19f":"code","e69b6a91":"code","3a5e08c8":"code","fb28aef7":"code","65a2b646":"code","e412dcb9":"code","99a29d41":"code","6c78f49c":"code","aea4635b":"code","6efcf039":"code","b4741d9a":"code","aa1d3928":"code","bb47a22b":"code","3f07da0f":"code","9b516908":"code","720166c0":"code","55cdc5ac":"code","5ad6b25e":"code","ad8a273c":"code","ea0a359e":"code","64369091":"code","e750eb5c":"code","ee9ed7ba":"code","06aa155b":"code","564594f5":"code","71634e72":"code","2d47b7fc":"code","ef348294":"code","83d3aec6":"code","85104b32":"code","2c54389b":"code","e205220b":"code","7edbb6cf":"code","20a8cb49":"code","21033f7d":"code","aeb658e3":"code","cd2111bf":"code","6e342615":"code","ff02e671":"code","e0dbdef2":"code","f185c927":"code","3d87ae5e":"code","c4d10084":"code","4777b588":"code","a46e05bd":"code","1eaf2269":"code","1132b636":"code","33f4e8d3":"code","9d9b3266":"code","7db6f289":"code","d4e4fffe":"code","18409699":"code","66da591c":"code","007470cd":"code","5f17df84":"code","937edb06":"code","82927bd8":"markdown","262ec667":"markdown","af03cee7":"markdown","c1dd59a1":"markdown","46974906":"markdown","519e5680":"markdown","638ac374":"markdown","7cf76e84":"markdown","c6933fd1":"markdown","6b710870":"markdown","74a1e5a4":"markdown","d82bde73":"markdown","7ceb85af":"markdown","f01d1f6f":"markdown","a49511af":"markdown","ce892eb8":"markdown","dcf4c553":"markdown","7e1c56b1":"markdown","e9db7a15":"markdown","198a886e":"markdown","ff06ffd5":"markdown","b9f99eaa":"markdown","1d3135ef":"markdown","94288a6b":"markdown","c9ab50ff":"markdown","f6fd79b4":"markdown","03ce98ac":"markdown"},"source":{"8d6def9c":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline","17d4e2f5":"# Import data with pandas\ndf = pd.read_csv('..\/input\/brazilian-cities\/BRAZIL_CITIES.csv',delimiter=';')\n# Select our columns based in 2010 measures\ndesired_cols = list(df.columns[0:16]) + list(df.columns[19:25])\ndf = df[desired_cols]","d475bbc5":"# Preview\ndf.head(3)","b0d835bc":"# Plot a Brazil map based in LAT\/LON\n## remove zero values\nmask1= df[\"LONG\"] != 0\nmask2 = df[\"LAT\"] !=0 \nmask3 = df['CAPITAL'] ==1\n \n## use the scatter function\nplt.figure(figsize=(10,10))\nplt.title(\"Cities Latitude and Longitude\")\nplt.xlabel(\"Longitude\")\nplt.ylabel(\"Latitude\")\nplt.scatter(df[mask1&mask2&mask3][\"LONG\"], df[mask1&mask2&mask3][\"LAT\"], s=20, alpha=1, label='Capital city')\nplt.scatter(df[mask1&mask2&~mask3][\"LONG\"], df[mask1&mask2&~mask3][\"LAT\"], s=1, alpha=1, label='Other')\nplt.legend()\nplt.show()","fd7bc930":"# Loop into all samples to create the categories\nfor i in range(0, len(df)):\n    # Category 1\n    if df.loc[i,'IDHM'] >= 0.0 and df.loc[i,'IDHM'] < 0.500:\n        df.loc[i,'IDHM_CAT'] = 'C1'\n    # Category 2\n    elif df.loc[i,'IDHM'] >= 0.500 and df.loc[i,'IDHM'] < 0.600:\n        df.loc[i,'IDHM_CAT'] = 'C2'\n    # Category 3\n    elif df.loc[i,'IDHM'] >= 0.600 and df.loc[i,'IDHM'] < 0.700:\n        df.loc[i,'IDHM_CAT'] = 'C3'\n    # Category 4\n    elif df.loc[i,'IDHM'] >= 0.700 and df.loc[i,'IDHM'] < 0.800:\n        df.loc[i,'IDHM_CAT'] = 'C4'\n    # Category 5\n    elif df.loc[i,'IDHM'] >= 0.800 and df.loc[i,'IDHM'] <= 1:\n        df.loc[i,'IDHM_CAT'] = 'C5'","f5ada4f3":"# Barplot\nsns.countplot(x=\"IDHM_CAT\", data=df, order=['C1','C2','C3','C4','C5']);","a15ac220":"# Loop into all samples to create the categories more balanced\nfor i in range(0, len(df)):\n    # Category 1 and 2\n    if df.loc[i,'IDHM'] >= 0.0 and df.loc[i,'IDHM'] < 0.600:\n        df.loc[i,'IDHM_CAT2'] = 'C1-C2'\n    # Category 3\n    elif df.loc[i,'IDHM'] >= 0.600 and df.loc[i,'IDHM'] < 0.700:\n        df.loc[i,'IDHM_CAT2'] = 'C3'\n    # Category 4\n    elif df.loc[i,'IDHM'] >= 0.700 and df.loc[i,'IDHM'] <= 1:\n        df.loc[i,'IDHM_CAT2'] = 'C4-C5'","c795eb34":"# Barplot\nsns.countplot(x=\"IDHM_CAT2\", data=df, order=['C1-C2','C3','C4-C5']);","000e6238":"# See how many NaN values do I have\ndf.isnull().sum()","25d12913":"# How many rows do I have now?\nsb = len(df)\nsb","53f4f19f":"# Drop any row that contains a single NaN value\ndf.dropna(axis=0, inplace=True)","e69b6a91":"# Reset index\ndf.index = range(0,len(df))","3a5e08c8":"# How many rows do I have after cleaning my dataset?\nsa = len(df)\nsa","fb28aef7":"# What I have lost?\nsb - sa","65a2b646":"# Separate y categorical (targets)\ny = df[['IDHM_CAT2']]\nyReal = df[['IDHM_CAT']]\n# Separate y numeric\nyNum = df[['IDHM']]\n# Separate x (predictors)\nx = df[['IBGE_RES_POP', 'IBGE_RES_POP_BRAS','IBGE_RES_POP_ESTR', 'IBGE_DU', 'IBGE_DU_URBAN', 'IBGE_DU_RURAL', 'IBGE_POP', \n        'IBGE_1', 'IBGE_1-4', 'IBGE_5-9', 'IBGE_10-14', 'IBGE_15-59', 'IBGE_60+']]\n# Separate a (analysis)\na = df[['CITY', 'STATE', 'CAPITAL', 'LONG', 'LAT']]","e412dcb9":"# Import sklearn preprocessing library\nfrom sklearn.preprocessing import StandardScaler","99a29d41":"# Create MinMaxScaler object\nnormData = StandardScaler()","6c78f49c":"# Scale our x set by applying a fit and transform\nnd_x = normData.fit_transform(x)","aea4635b":"# Transform the results into a dataframe as was the original\nx = pd.DataFrame(nd_x, index=x.index, columns=x.columns)","6efcf039":"# Spearman correlation\nmc = x.corr(method='spearman')","b4741d9a":"# Generate a mask for the upper triangle\ntriangle_mask = np.zeros_like(mc, dtype=np.bool)\ntriangle_mask[np.triu_indices_from(triangle_mask)] = True\n\n# Plot\nplt.figure(figsize = (15,15))\nsns.heatmap(data = mc, linewidths=.1, linecolor='black', vmin = -1, vmax = 1, mask = triangle_mask, annot = True,\n            cbar_kws={\"ticks\":[-1,-0.8,-0.6,0,0.6,0.8,1]});","aa1d3928":"# Import the libraries to FS and score function (metric)\nfrom sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import mutual_info_classif","bb47a22b":"# FS process for only two predictors variables\nFS2 = SelectKBest(score_func=mutual_info_classif, k=2);","3f07da0f":"# Fit and Transform\nx_fs2 = FS2.fit_transform(x, y.values.ravel());","9b516908":"# The chosen 2 variables\nfs2_cols = x.columns[FS2.get_support()]\nfs2_cols","720166c0":"# FS process for three predictors variables\nFS3 = SelectKBest(score_func=mutual_info_classif, k=3);","55cdc5ac":"# Fit and Transform\nx_fs3 = FS3.fit_transform(x, y.values.ravel());","5ad6b25e":"# The chosen 3 variables\nfs3_cols = x.columns[FS3.get_support()]\nfs3_cols","ad8a273c":"# Import our favorite library\nfrom sklearn.cluster import KMeans","ea0a359e":"# Create our data variable\ntrain = x[fs2_cols]","64369091":"# Elbow Method for K-Means\ndistortions = []\nfor i in range(1, 11):\n    km = KMeans(n_clusters=i, init='k-means++', n_init=10, max_iter=300, tol=1e-04, random_state=1206)\n    km.fit(train)\n    distortions.append(km.inertia_)","e750eb5c":"# plot Elbow\nplt.plot(range(1, 11), distortions, marker='o')\nplt.xlabel('Number of clusters')\nplt.ylabel('Distortion')\nplt.show()","ee9ed7ba":"# Creating KMeans model with 3 clusters as was propose\nkm3 = KMeans(n_clusters=3, init='k-means++', n_init=10, max_iter=300, tol=1e-04, random_state=1206)","06aa155b":"# Fit\/Predict KMeans 3 Cluster\npred2D = km3.fit_predict(train);","564594f5":"# Create auxiliar dataframe for plot\ndata = pd.concat([train, y], axis=1)","71634e72":"# Create a column for predicted clusters from KMeans\nfor i in range(0,len(data)):\n    if pred2D[i] == 0:\n        data.loc[i,'PREDICT'] = 'Cluster 1'\n    elif pred2D[i] == 1:\n        data.loc[i,'PREDICT'] = 'Cluster 2'\n    elif pred2D[i] == 2:\n        data.loc[i,'PREDICT'] = 'Cluster 3'","2d47b7fc":"# Plot 2D\nfig, ax = plt.subplots(1,2, figsize=(20,10));\nsns.scatterplot(x='IBGE_RES_POP_ESTR', y='IBGE_DU_URBAN', hue=\"PREDICT\", data=data, ax=ax[0], sizes=0.1);\nsns.scatterplot(x='IBGE_RES_POP_ESTR', y='IBGE_DU_URBAN', hue=\"IDHM_CAT2\", data=data, ax=ax[1]);","ef348294":"# Create our data variable\ntrain = x[fs3_cols]","83d3aec6":"# Elbow Method for K-Means\ndistortions = []\nfor i in range(1, 11):\n    km = KMeans(n_clusters=i, init='k-means++', n_init=10, max_iter=300, tol=1e-04, random_state=1206)\n    km.fit(train)\n    distortions.append(km.inertia_)\n# plot Elbow\nplt.plot(range(1, 11), distortions, marker='o')\nplt.xlabel('Number of clusters')\nplt.ylabel('Distortion')\nplt.show()","85104b32":"# Creating KMeans model with 3 clusters as was propose\nkm3 = KMeans(n_clusters=3, init='k-means++', n_init=10, max_iter=300, tol=1e-04, random_state=1206)","2c54389b":"# Fit\/Predict KMeans 3 Cluster\npred3D = km3.fit_predict(train);","e205220b":"# Create auxiliar dataframe for plot\ndata = pd.concat([train, y], axis=1)","7edbb6cf":"# Create a column for predicted clusters from KMeans\nfor i in range(0,len(data)):\n    if pred2D[i] == 0:\n        data.loc[i,'PREDICT'] = 'Cluster 1'\n        #data.loc[i,'COLOR'] = 'blue'\n    elif pred2D[i] == 1:\n        data.loc[i,'PREDICT'] = 'Cluster 2'\n        #data.loc[i,'COLOR'] = 'green'\n    elif pred2D[i] == 2:\n        data.loc[i,'PREDICT'] = 'Cluster 3'\n        #data.loc[i,'COLOR'] = 'orange'","20a8cb49":"# Import library for 3D plot\nimport plotly.express as px","21033f7d":"# Plot 3D - Original Variables Cluster\npx.scatter_3d(data, x='IBGE_DU_URBAN', y='IBGE_RES_POP_ESTR', z='IBGE_60+', color='IDHM_CAT2')","aeb658e3":"# Plot 3D - Original Variables Cluster\npx.scatter_3d(data, x='IBGE_DU_URBAN', y='IBGE_RES_POP_ESTR', z='IBGE_60+', color='PREDICT')\n\n## other way to plot\n# Library for 3D\n#from mpl_toolkits.mplot3d import Axes3D\n# Plot 3D\n#plotting = plt.figure(figsize=(15,15)).gca(projection='3d');\n#plotting.scatter(data['IBGE_DU_URBAN'], data['IBGE_RES_POP_ESTR'], data['IBGE_60+'],c=data['COLOR']);\n#plotting.set_xlabel('IBGE_DU_URBAN');\n#plotting.set_ylabel('IBGE_RES_POP_ESTR');\n#plotting.set_zlabel('IBGE_60+');","cd2111bf":"# Import our model\nfrom sklearn.decomposition import PCA","6e342615":"# Create model\npca = PCA(n_components=2)","ff02e671":"# Fit and Transform X to PC dimension\npc = pca.fit_transform(x)","e0dbdef2":"# See explained variance\npca.explained_variance_ratio_","f185c927":"# Create a Dataframe for PC with target var\nxPC = pd.DataFrame(data = pc, columns = ['PC1', 'PC2'])\n# Join with target categories\nxPC = pd.concat([xPC, y], axis = 1)","3d87ae5e":"# Plot 2D\nplt.figure(figsize=(20,10));\nsns.scatterplot(x=\"PC2\", y=\"PC1\", hue=\"IDHM_CAT2\", data=xPC);","c4d10084":"# Create model\npca = PCA(n_components=3)","4777b588":"# Fit and Transform X to PC dimension\npc = pca.fit_transform(x)","a46e05bd":"# Create a Dataframe for PC with target var\nxPC = pd.DataFrame(data = pc, columns = ['PC1', 'PC2', 'PC3'])\n# Join with target categories\nxPC = pd.concat([xPC, y], axis = 1)","1eaf2269":"# Plot 3D\npx.scatter_3d(xPC, x='PC1', y='PC2', z='PC3', color='IDHM_CAT2')","1132b636":"# See explained variance\npca.explained_variance_ratio_","33f4e8d3":"# Import library for TSNE\nfrom sklearn.manifold import TSNE","9d9b3266":"# Create tSNE model\ntsne = TSNE(n_components=2)","7db6f289":"# Fit and Transform x\nx_emb = tsne.fit_transform(x)","d4e4fffe":"# Turn X embeddeb into a dataframe to a easy plot\nx_emb = pd.DataFrame(data = x_emb, columns = ['XE1', 'XE2'])\n# Join with target categories\nx_emb = pd.concat([x_emb, y], axis = 1)","18409699":"# Plot 2D data\nsns.scatterplot(x='XE1', y='XE2', hue=\"IDHM_CAT2\", data=x_emb);","66da591c":"# Create tSNE model\ntsne = TSNE(n_components=3)","007470cd":"# Fit and Transform x\nx_emb = tsne.fit_transform(x)","5f17df84":"# Turn X embedded into a dataframe to an easy plot\nx_emb = pd.DataFrame(data = x_emb, columns = ['XE1', 'XE2', 'XE3'])\n# Join with target categories\nx_emb = pd.concat([x_emb, y], axis = 1)","937edb06":"# Plot 3D\npx.scatter_3d(x_emb, x='XE1', y='XE2', z='XE3', color='IDHM_CAT2')","82927bd8":"Now, using three variables shows that distortion did not changed at all.","262ec667":"## 5 - Data Standardization\nBecause our predictors data have different ranges, I will apply a standardization using *sklearn* to get a comparable range for our variables. \n\nThis is important because most of the clustering methods are based in *distance metrics* such as KMeans and PCA. So an unique range it's necessary to make all variables comparable, avoiding those mistakes:\n\n**|X - Y| = 100000**\n\n**|Y - Z| = 0.1**\n\nBased only in a scale difference.\n\n**PS**: I could use Normalization method, but this way is sensible to outliers so I will stick to standardization.","af03cee7":"## tSNE with 3 components (3D plot)","c1dd59a1":"# 5 - Correlation Analysis\nBefore we jump in our models creation, I will do a correlation analysis into our X set. This analysis will help see correlated information between our variables and could help to improve our results later.","46974906":"### 6.3 - KMeans for two variables","519e5680":"As you can see, our categories are desbalanced for **C1** and   **C5** and this could be a problem for our models. To future analysis, I will create another column, now mixing together **C1** with **C2** and **C4** with **C5**.","638ac374":"# 2010 Analysis with Clustering methods\n\n## Goal\nMy objective with this notebook is to analyze brazilian cities data related to 2010 measurements using the follows clustering methods:\n- KMeans Clustering\n- PCA (Principal Components Analysis)\n- t-SNE\n\nFor this purpose, I want to understand a specific target variable: **IDHM**. The main idea is to answer this question:\n### \"Based in 2010 numeric measurements, it's possible to clusterize our data into IDHM categories?\"\n\nThe IDHM 2010 categories mencioned are listed bellow:\n![IDHM2010](https:\/\/user-images.githubusercontent.com\/32513366\/71258991-2843b500-2316-11ea-9ba2-46541647ad5a.PNG)\n\nWhere, from left to the right, IDHM have those categories:\n- C1: from **0** to **0,499**\n- C2: from **0,500** to **0,599**\n- C3: from **0,600** to **0,699**\n- C4: from **0,700** to **0,799**\n- C5: from **0,800** to **1**\n\nFor more info, acess [here](http:\/\/www.atlasbrasil.org.br\/2013\/pt\/o_atlas\/idhm\/). So now let's begin!\n\n## Common Libraries","7cf76e84":"# 3 - Missing data treament\nFor this notebook, I will **only** work with complete samples and focus in my clustering analysis. So I will remove any sample that contains a missing value.","c6933fd1":"Apparently by the elbow method (way to choose the K optimal), 3 clusters could be a good decision using 2 variables.","6b710870":"So, after I cleaned my dataset I lost 83 samples. When I choose such a simple strategy, I assume that those samples will not be meaningful for my study. Other ways for Data imputation could be applied, but I chose not to do so since the loss was quite insignificant (1.5% of my data was lost).","74a1e5a4":"Through this plot we can see that KMeans with two features, based on my pipeline, was succesfull to clusterize our data, but the real data doesn't show to a clear clusterization. Therefore the clusters created by KMeans can lead to mistakes. We will see if with 3 variables there can be a clearer grouping.\n\n### 6.4 - KMeans for three variables","d82bde73":"Now we have a more balanced class for IDHM. To simplify our life, I will work with the second kind of categories that is more balanced.","7ceb85af":"## 7.1 - Using two Principal Components","f01d1f6f":"So, through this plot we can see that using PCA with two components was unsuccessful. Now, let's see if a third dimension could improve the results.\n\n### 7.2 - Using three Principal Components","a49511af":"As is possible to see, our explained variance is very low. That could be a bad indicator for PCA in this dataset.","ce892eb8":"Through the heatmap analysis, we can see that, indeed, there is a high correlation in our predictors set.","dcf4c553":"This plot was based on other [notebook](https:\/\/www.kaggle.com\/fjab76\/brazilian-cities-a-simple-exploration) for this same dataset. Through this figure, it is possible to see that the **data density** is close to the coastal region in southern, southeastern and northeast areas.\n\n# 2 - Creating IDHM Categories\nUsing the categories presented before, I will create manually IDHM categories for the data","7e1c56b1":"# 8 - tSNE\n\nAfter the lost of KMeans and PCA, let's use a t-SNE to see if it is possible to clusterize our data based on 2010 measures.","e9db7a15":"# Conclusions\nWith this notebook, we could see that not every machine learning story can end with a happy ending. Based in the question, we did not had sucess to apply correct clusters for IDHM. That is tons of possibilities for this failing:\n- The hyperparameters was not set correctly for any method\n- The correlated nature from predictors set could be real problem\n- My feature selection for KMeans was not the best (most related with the first reason)\n- The 2010 measurements used as predictors could not extract any information from IDHM\n\nUsing my experience, I could say that the third reason is correct. Sometimes the data can not give to us the desired answer. Besides that, it is important to remember that the methods used here are **Unsupervised** methods, meaning that they were blinded to our target (IDHM categories).\n\n**PS**: The first possibility still reasonable.\n\nSo, answering the beginning question:\n\n### No, the 2010 numeric measurements do not hold enough (or any) knowledge about IDHM categories, making impossible to clusterize.","198a886e":"Again, our PCA model has not succeed to visualize and separate in proper clusters. The explained variance could be a indicator for this mistake.","ff06ffd5":"# 6 - K-Means clustering\n\nThe main idea to use KMeans here, is to visualize clusters based on our IDHM categories. **However**, we have a high dimensional data in predictors set (13 variables) which makes it impossible to create a reasonable plot (the maximum is a 3D plot). So to this task I will implement a feature selection before the model creation to choose the best variables to create the clusters and visualize. \n\n### 6.1 - Feature selection with SKlearn for two variables","b9f99eaa":"So, even for tSNE we did not had success to answer the question propose at *Goal*.","1d3135ef":"# 4 - Split X and Y\nIn this short part, I will split our data into X (predictors) and Y (target).","94288a6b":"# 1 - Import and Select the data\nFor this analysis,as was said I will use the numeric columns related to measures of 2010 in Brazil cities. ","c9ab50ff":"## tSNE with 2 components (2D plot)","f6fd79b4":"### 6.2 - Feature selection with SKlearn for three variables","03ce98ac":"So, based on those plots it is possible to see that KMeans creates succesfully the 3 clusters, but analyzing the real clusters with the data we can see that KMeans would probably predict wrong, most because the difference between IDHM categories are very blur.\n\nBy this, we can say that KMeans was unsucessful for this task.\n\n# 7 - PCA\nTo use PCA, is not necessary a Feature selection (for a higher dimensional data it is recommended)."}}