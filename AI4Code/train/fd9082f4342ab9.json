{"cell_type":{"b4d119f4":"code","18447b71":"code","0a876f05":"code","8d16de56":"code","e4f0f745":"code","a8dcb890":"code","64442a52":"code","d6cff881":"code","c1286521":"code","5827929b":"code","8dd1c66e":"code","482ccff6":"code","6865566a":"code","90c3683d":"code","3cdc6acc":"code","648307f7":"code","203ba978":"code","79797911":"code","c7b1542f":"code","d127c1ce":"code","5bee28a7":"code","836f3185":"code","ef444d83":"code","a5125e22":"code","1633b2eb":"code","a133cf16":"code","ce92784d":"code","c80c07dd":"code","665eede9":"code","190c3bab":"code","4699baed":"code","bc9f5645":"code","c8d311bf":"code","5ef982d3":"code","2f9f835c":"code","7e0f7454":"code","550f57b8":"code","083b133a":"code","3fba2e59":"code","344f216d":"code","aa51940e":"code","e2f8cac6":"code","5cd0f868":"code","90f4120c":"code","23270e2e":"code","30470717":"code","4c7e44f6":"code","4cfa2ad1":"code","4361fcd5":"code","a68f3808":"code","29dd7e12":"code","f4ffd3ec":"code","246ec0eb":"code","7638f746":"code","b552e42f":"code","6f174205":"code","a37dbf30":"code","ae56992a":"code","9787781d":"code","9907ddb6":"code","1ff8fdf7":"code","294dd6a3":"code","3cac953b":"code","86787fed":"code","ba375475":"code","6c146db5":"code","b6fe136b":"code","6436c29c":"code","0ba7b466":"code","18fb143c":"code","f3ae0cca":"code","3e5ce812":"code","4c44db6b":"code","1b02874f":"markdown","3e67a271":"markdown","9dd6ae36":"markdown","805d99dc":"markdown","a198fe82":"markdown","50bf40f4":"markdown","c37da009":"markdown","e3e0c781":"markdown","04e23ce8":"markdown","1197291a":"markdown","19a5946c":"markdown","642a1dc1":"markdown","c9c4b70c":"markdown","1bf07734":"markdown","26be5998":"markdown","fb844a2c":"markdown","d22c937d":"markdown","112dd919":"markdown","629e697b":"markdown","0cbbc3ad":"markdown","85284dc2":"markdown","13cb52ed":"markdown","fbfb54b8":"markdown","52bcba2a":"markdown","920da5d1":"markdown","5803bffb":"markdown","97e10988":"markdown","c304e252":"markdown","14b89ef6":"markdown","348df6c4":"markdown","09f544b1":"markdown","f8067d05":"markdown","780c13fd":"markdown","7f9970fa":"markdown","47f751a1":"markdown","d8b58971":"markdown","246720b5":"markdown","51937bdf":"markdown","fd758565":"markdown","c9456190":"markdown","2195dc00":"markdown","14994c98":"markdown","dec466c4":"markdown","c68af253":"markdown","e540f4c4":"markdown","c416f360":"markdown","e2a3c3e3":"markdown","8cfff7b1":"markdown","0dedcaee":"markdown","906aadfd":"markdown","10b2bbde":"markdown","2ad9561e":"markdown","a32d9b07":"markdown","963c48f0":"markdown","741f5d1e":"markdown","92563f7f":"markdown"},"source":{"b4d119f4":"from mlxtend.plotting import plot_decision_regions\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly as py\nfrom plotly.offline import init_notebook_mode, iplot, plot\nimport plotly.graph_objs as go\ninit_notebook_mode(connected=True)\nimport warnings\nwarnings.filterwarnings('ignore')\n%matplotlib inline\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","18447b71":"#data read\ndata = pd.read_csv(\"\/kaggle\/input\/pima-indians-diabetes-database\/diabetes.csv\")","0a876f05":"data.shape # sample,feature","8d16de56":"data.info()","e4f0f745":"\ndata.head() #first 5 data","a8dcb890":"data.describe()","64442a52":"data.describe().T # Changed rows and columns. (T)","d6cff881":"data.isnull().sum()","c1286521":"print(\"Number of rows with Glucose == 0 => {}\".format((data.Glucose==0).sum()))\nprint(\"Number of rows with BloodPressure == 0 => {}\".format((data.BloodPressure==0).sum()))\nprint(\"Number of rows with SkinThickness == 0 => {}\".format((data.SkinThickness==0).sum()))\nprint(\"Number of rows with Insulin == 0 => {}\".format((data.Insulin==0).sum()))\nprint(\"Number of rows with BMI == 0 => {}\".format((data.BMI==0).sum()))","5827929b":"#datay\u0131 bozmadan kopyalad\u0131k ve ba\u011fl\u0131l\u0131\u011f\u0131 kald\u0131rd\u0131k.\ndata_copy = data.copy(deep=True) \n\n#Featurelerdeki  0 valueleri Nan value yapal\u0131m.\ndata_copy[['Glucose','BloodPressure','SkinThickness','Insulin','BMI']]= data_copy[['Glucose','BloodPressure','SkinThickness','Insulin','BMI']].replace(0,np.NaN)\n\nprint(data_copy.isnull().sum())","8dd1c66e":"data_copy.hist(figsize=(15,15))\nplt.show()","482ccff6":"#Featurelerdeki missing valueleri mean ve median de\u011ferleri ile doldurduk.\n#We filled missing values in features with mean and median values\n\ndata_copy.BloodPressure.fillna(data_copy.BloodPressure.mean(),inplace=True)\ndata_copy.Glucose.fillna(data_copy.Glucose.mean(),inplace=True)\ndata_copy.SkinThickness.fillna(data_copy.SkinThickness.median(),inplace=True)\ndata_copy.Insulin.fillna(data_copy.Insulin.median(),inplace=True)\ndata_copy.BMI.fillna(data_copy.BMI.median(),inplace=True)","6865566a":"data_copy.describe().T","90c3683d":"data_copy.hist(figsize=(15,15))\nplt.show()","3cdc6acc":"data_copy.head()","648307f7":"# Data Normalize\n#Y\u00f6ntem 1(Medhod 1)\n#from sklearn.preprocessing import MinMaxScaler\n#norm = MinMaxScaler()\n#x = pd.DataFrame(norm.fit_transform(data_copy.drop(['Outcome'],axis=1)),\n#                 columns=['Pregnancies', 'Glucose', 'BloodPressure', 'SkinThickness', 'Insulin',\n#       'BMI', 'DiabetesPedigreeFunction', 'Age'])\n#x.head()\n\n#Y\u00f6ntem 2(Method 2)\ny=  data_copy['Outcome'].values.reshape(-1,1)  #Dependent Features (Class)\nx_data =data_copy.drop(['Outcome'],axis=1)     #Independent Features\nx = (x_data - np.min(x_data)) \/ (np.max(x_data) - np.min(x_data))   #normalized\nx.head()","203ba978":"from sklearn.model_selection import train_test_split\n\nx_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.3,random_state=29)\nx_train.head()","79797911":"y_train[:5]","c7b1542f":"from sklearn.neighbors import KNeighborsClassifier\ntrain_score_list=[]\ntest_score_list=[]\n\nfor each in range (2,31):\n    knn=KNeighborsClassifier(n_neighbors=each)\n    knn.fit(x_train,y_train)\n    test_score_list.append(knn.score(x_test,y_test)*100)\n    train_score_list.append(knn.score(x_train,y_train)*100)\n\n    \nprint(\"Best accuracy(test) is {:.3f} % with K = {}\".format(np.max(test_score_list),test_score_list.index(np.max(test_score_list))+ 2))\nprint(\"Best accuracy(train) is {:.3f}% with K = {}\".format(np.max(train_score_list),train_score_list.index(np.max(train_score_list))+2))\n","d127c1ce":"import plotly.graph_objs as go\n\narange=np.arange(2,31)\ntrace1=go.Scatter(\n    x=arange,\n    y=train_score_list,\n    mode=\"lines + markers\",\n    name=\"Train_Score\",\n    marker=dict(color = 'rgba(16, 112, 2, 0.8)'),\n    \n)\ntrace2=go.Scatter(\n    x=arange,\n    y=test_score_list,\n    mode=\"lines + markers\",\n    name=\"Test_Score\",\n    marker=dict(color = 'rgba(80, 26, 80, 0.8)'),\n)\ndata=[trace1,trace2]\nlayout = dict(title = '-value VS Accuracy',\n              xaxis= dict(title= 'K Value',ticklen= 5,zeroline= False),\n              yaxis= dict(title= 'Score',ticklen= 5,zeroline= False)\n             )\nfig = dict(data = data, layout = layout)\niplot(fig)","5bee28a7":"# import confusion_matrix\nfrom sklearn.metrics import confusion_matrix\n\nk_value =test_score_list.index(np.max(test_score_list))+ 2   # K paramer\nknn2 = KNeighborsClassifier(n_neighbors=k_value)\nknn2.fit(x_train,y_train)\ny_pred = knn2.predict(x_test)\ny_true = y_test\n\n# 0 value is negative\n# 1 value is positive\ncmatrix = confusion_matrix(y_true,y_pred,labels=[0,1])\n\nf,ax = plt.subplots(figsize=(8,4))\nsns.heatmap(cmatrix,annot=True,linewidths=0.5,cbar=True,linecolor=\"white\",fmt='.0f',ax=ax)\nplt.title(\"Model's Confusion Matrix\")\nplt.xlabel(\"y_predict (Predicted Label)\")\nplt.ylabel(\"y_true (Actual Label)\")\nplt.show()\n\n#1=\"Pozitif\"\n#0=\"Negatif\"\n\nprint('True negative = ', cmatrix[0][0])\nprint('False positive = ', cmatrix[0][1])\nprint('False negative = ', cmatrix[1][0])\nprint('True positive = ', cmatrix[1][1])\n","836f3185":"y_true","ef444d83":"from sklearn.metrics import classification_report\nprint(classification_report(y_true,y_pred))","a5125e22":"from sklearn import tree\ndt = tree.DecisionTreeClassifier(random_state=1,criterion='entropy') # criterion default = gini\ndt.fit(x_train,y_train)\n\nprint(\"Train Score: \",dt.score(x_train,y_train))\nprint(\"Test Score: \",dt.score(x_test,y_test))\n\nprint(\"\\n\\nDecision Tree default parameters : \",dt.get_params)\n","1633b2eb":"from sklearn import tree\ndt = tree.DecisionTreeClassifier(random_state=1,criterion='gini')\ndt.fit(x_train,y_train)\n\nprint(\"Train Score: \",dt.score(x_train,y_train))\nprint(\"Test Score: \",dt.score(x_test,y_test))\n","a133cf16":"# Criterion gini visualization\nimport graphviz \nfrom graphviz import Source\nplt.figure(figsize=(40,20))  \n_ = tree.plot_tree(dt, feature_names = x.columns, \n             filled=True, fontsize=10, rounded = True)\nplt.savefig('diabetes.png')\nplt.show()","ce92784d":"#confusion matrix for Criterion gini\ny_pred = dt.predict(x_test)\ny_true = y_test\n\n# 0 value is negative\n# 1 value is positive\ncmatrix2 = confusion_matrix(y_true,y_pred,labels=[0,1])\n\nf,ax = plt.subplots(figsize=(8,4))\nsns.heatmap(cmatrix2,annot=True,linewidths=0.5,cbar=True,linecolor=\"white\",fmt='.0f',ax=ax)\nplt.title(\"Model's Confusion Matrix\")\nplt.xlabel(\"y_predict (Predicted Label)\")\nplt.ylabel(\"y_true (Actual Label)\")\nplt.show()\n\n#1=\"Pozitif\"\n#0=\"Negatif\"\n\nprint('True negative = ', cmatrix2[0][0])\nprint('False positive = ', cmatrix2[0][1])\nprint('False negative = ', cmatrix2[1][0])\nprint('True positive = ', cmatrix2[1][1])","c80c07dd":"from sklearn.metrics import classification_report\nprint(classification_report(y_true,y_pred))","665eede9":"from sklearn.ensemble import RandomForestClassifier\n\n#*n_estimators: number of tree models to be used\n#max_depth: The maximum depth of the tree. If None, \n#then nodes are expanded until all leaves are pure or until all leaves contain less than min_samples_split samples.\n\nrf = RandomForestClassifier(n_estimators=10,max_depth= 5, random_state=2, criterion=\"gini\")\nrf.fit(x_train,y_train)\n\nprint(\"Random Forest Score: \",rf.score(x_test,y_test))\n\nprint(\"Random Forest Parameters: \",rf.get_params)\n# n_ estimators default = 100\n","190c3bab":"pd.concat((pd.DataFrame(x_train.columns, columns = ['variable']), \n           pd.DataFrame(rf.feature_importances_, columns = ['importance'])), \n          axis = 1).sort_values(by='importance', ascending = False)","4699baed":"score_estimators=[]\nfor i in range(0,len(rf.estimators_)):\n    estimator=rf.estimators_[i].score(x_test,y_test)\n    print(\"{}. estimator score is {}\".format(i+1,estimator))\n    score_estimators.append(estimator)\n    \nprint(\"\\nBest Accuracy(test) is {:.3f}% with estimator number = {}\".format(np.max(score_estimators)*100,score_estimators.index(np.max(score_estimators))+1))","bc9f5645":"# decision tree structure according to best estimator\n\nbest_estimator_number = score_estimators.index(np.max(score_estimators))+1\nmodel_estimator = rf.estimators_[best_estimator_number]\n\nplt.figure(figsize=(18,10))  \n_ = tree.plot_tree(model_estimator, feature_names = x.columns, \n             filled=True, fontsize=10, rounded = True)\nplt.savefig('diabetes.png')\nplt.show()","c8d311bf":"import timeit\nstart = timeit.default_timer()\ntrain_score_list=[]\ntest_score_list=[]\n\nfor i in range(1,101):\n    rf2 = RandomForestClassifier(n_estimators=i,max_depth= 5, random_state=2, criterion=\"gini\")\n    rf2.fit(x_train,y_train)\n    train_score_list.append(rf2.score(x_test,y_test))\n    test_score_list.append(rf2.score(x_train,y_train))\n    \nplt.figure(figsize=(12,5))\np1=sns.lineplot(x=range(1,101),y=train_score_list,color='red',label=\"Train Scores\")\np1=sns.lineplot(x=range(1,101),y=test_score_list,color='lime',label=\"Test Scores\")\nplt.legend()\nplt.title('N-Estimator vs Accuracy')\nplt.xlabel(\"Estimators\")\nplt.ylabel(\"Accuracy\")\nplt.show()\n\nstop = timeit.default_timer()\nprint('Run Time: ', stop - start) \nprint(\"\\nBest Accuracy(test): {:.3f} with n_estimators: {} \".format(np.max(test_score_list)*100,test_score_list.index(np.max(test_score_list))+1))\nprint(\"\\nBest Accuracy(train): {:.3f} with n_estimators: {} \".format(np.max(train_score_list)*100,train_score_list.index(np.max(train_score_list))+1))\nbest_n_estimators_parameter = test_score_list.index(np.max(test_score_list))+1","5ef982d3":"rf3 = RandomForestClassifier(n_estimators=best_n_estimators_parameter,max_depth= 5, random_state=2, criterion=\"gini\")\nrf3.fit(x_train,y_train)\ny_true = y_test\ny_pred = rf3.predict(x_test)","2f9f835c":"# 0 value is negative\n# 1 value is positive\nfrom sklearn.metrics import confusion_matrix\ncmatrix3 = confusion_matrix(y_true,y_pred,labels=[0,1])\n\nf,ax = plt.subplots(figsize=(8,4))\nsns.heatmap(cmatrix3,annot=True,linewidths=0.5,cbar=True,linecolor=\"white\",fmt='.0f',ax=ax)\nplt.title(\"Model's Confusion Matrix\")\nplt.xlabel(\"y_predict (Predicted Label)\")\nplt.ylabel(\"y_true (Actual Label)\")\nplt.show()\n\n#1=\"Pozitif\"\n#0=\"Negatif\"\n\nprint('True negative = ', cmatrix3[0][0])\nprint('False positive = ', cmatrix3[0][1])\nprint('False negative = ', cmatrix3[1][0])\nprint('True positive = ', cmatrix3[1][1])","7e0f7454":"from sklearn.metrics import classification_report\nprint(classification_report(y_true,y_pred))","550f57b8":"from sklearn.svm import SVC\nc_range=np.arange(0.1,1.5,0.1)\n\nfor i in c_range:\n    svm_linear = SVC(kernel=\"linear\",C=i,random_state=29)\n    svm_linear.fit(x_train,y_train)\n    print(\"accuracy of svm(linear): {:.4f} with C(cost) parameter: {:.2f}\".format(svm_linear.score(x_test,y_test),i))\n\n\n","083b133a":"#confusion matrix for Criterion gini\ny_pred = svm_linear.predict(x_test)\ny_true = y_test\n\n# 0 value is negative\n# 1 value is positive\ncmatrix_linear = confusion_matrix(y_true,y_pred,labels=[0,1])\n\nf,ax = plt.subplots(figsize=(8,4))\nsns.heatmap(cmatrix_linear,annot=True,linewidths=0.5,cbar=True,linecolor=\"white\",fmt='.0f',ax=ax)\nplt.title(\"Model's Confusion Matrix\")\nplt.xlabel(\"y_predict (Predicted Label)\")\nplt.ylabel(\"y_true (Actual Label)\")\nplt.show()\n\n#1=\"Pozitif\"\n#0=\"Negatif\"\n\nprint('True negative = ', cmatrix_linear[0][0])\nprint('False positive = ', cmatrix_linear[0][1])\nprint('False negative = ', cmatrix_linear[1][0])\nprint('True positive = ', cmatrix_linear[1][1])\n\n\n\n\n\nfrom sklearn.metrics import classification_report\nprint(classification_report(y_true,y_pred))","3fba2e59":"from sklearn.datasets import make_moons\n# create spiral dataset\nX, Y = make_moons(n_samples=1000, noise=0.15, random_state=42)\nplt.figure(figsize=(10,5))\nax = plt.axes()\nax.scatter(X[:,0],X[:,1],c=Y);\nplt.xlabel(\"1\");\nplt.ylabel(\"2\");","344f216d":"X=(X - np.min(X)) \/ (np.max(X) - np.min(X))\nfrom sklearn.svm import SVC\nsvm_poly = SVC(kernel=\"poly\",C=1.0,degree=3,random_state=29)\nsvm_poly.fit(X,Y)\n\nplt.figure(figsize=(15,5))\nplot_decision_regions(X,Y,svm_poly)\nplt.show()","aa51940e":"from sklearn.svm import SVC\nsvm_polynomial = SVC(kernel=\"poly\",C=1.0,degree=2.0,gamma='scale')\nsvm_polynomial.fit(x_train,y_train)\nprint(\"Accuracy of svm(poly)%: \",svm_polynomial.score(x_test,y_test)*100)\nprint(\"\\n\\nOther Default Parameters: \",svm_polynomial.get_params())\n","e2f8cac6":"#confusion matrix for Criterion gini\ny_pred = svm_polynomial.predict(x_test)\ny_true = y_test\n\n# 0 value is negative\n# 1 value is positive\ncmatrix_polynomial = confusion_matrix(y_true,y_pred,labels=[0,1])\n\nf,ax = plt.subplots(figsize=(8,4))\nsns.heatmap(cmatrix_polynomial,annot=True,linewidths=0.5,cbar=True,linecolor=\"white\",fmt='.0f',ax=ax)\nplt.title(\"Model's Confusion Matrix\")\nplt.xlabel(\"y_predict (Predicted Label)\")\nplt.ylabel(\"y_true (Actual Label)\")\nplt.show()\n\n#1=\"Pozitif\"\n#0=\"Negatif\"\n\nprint('True negative = ', cmatrix_polynomial[0][0])\nprint('False positive = ', cmatrix_polynomial[0][1])\nprint('False negative = ', cmatrix_polynomial[1][0])\nprint('True positive = ', cmatrix_polynomial[1][1])\nprint(\"\\n\\n\")\n\n\n\n\n\nfrom sklearn.metrics import classification_report\nprint(classification_report(y_true,y_pred))","5cd0f868":"from sklearn.datasets.samples_generator import make_circles\nX,Y = make_circles(90, factor=0.2, noise=0.1) \n#noise = standard deviation of Gaussian noise added in data. \n#factor = scale factor between the two circles\nplt.figure(figsize=(8,5))\nplt.scatter(X[:,0],X[:,1], c=Y, s=50, cmap='seismic')\nplt.show()","90f4120c":"X=(X - np.min(X)) \/ (np.max(X) - np.min(X))\nfrom sklearn.svm import SVC\nimport time\n\ngamma_values=[0.1,1,10,100]\nfor i in gamma_values:\n    start=time.time()\n    svm_gausian = SVC(kernel=\"rbf\",C=1.0,degree=3,gamma=i,random_state=29)\n    svm_gausian.fit(X,Y)\n    finish=time.time()\n\n    plt.figure(figsize=(15,5))\n    plt.title(\"Gamma = {}, Time Cost= {:.4f}\".format(i,(finish-start)))\n    plt.xlabel(\"x feature\")\n    plt.ylabel(\"y feature\")\n    plot_decision_regions(X,Y,svm_gausian)\n    plt.show()","23270e2e":"from sklearn.svm import SVC\nsvm_rbf = SVC(kernel=\"rbf\",C=0.5,gamma=\"scale\")\nsvm_rbf.fit(x_train,y_train)\nprint(\"Accuracy of svm(poly): %\",svm_rbf.score(x_test,y_test)*100)\nprint(svm_rbf.get_params())","30470717":"#confusion matrix for Criterion gini\ny_pred = svm_rbf.predict(x_test)\ny_true = y_test\n\n# 0 value is negative\n# 1 value is positive\ncmatrix_rbf = confusion_matrix(y_true,y_pred,labels=[0,1])\n\nf,ax = plt.subplots(figsize=(8,4))\nsns.heatmap(cmatrix_rbf,annot=True,linewidths=0.5,cbar=True,linecolor=\"white\",fmt='.0f',ax=ax)\nplt.title(\"Model's Confusion Matrix\")\nplt.xlabel(\"y_predict (Predicted Label)\")\nplt.ylabel(\"y_true (Actual Label)\")\nplt.show()\n\n#1=\"Pozitif\"\n#0=\"Negatif\"\n\nprint('True negative = ', cmatrix_rbf[0][0])\nprint('False positive = ', cmatrix_rbf[0][1])\nprint('False negative = ', cmatrix_rbf[1][0])\nprint('True positive = ', cmatrix_rbf[1][1])\nprint(\"\\n\\n\")\n\n\n\n\nfrom sklearn.metrics import classification_report\nprint(classification_report(y_true,y_pred))","4c7e44f6":"from sklearn.svm import SVC\n\nsvm_sigmoid = SVC(kernel=\"sigmoid\",gamma=0.5)\nsvm_sigmoid.fit(x_train,y_train)\nprint(\"Accuracy of svm(sigmoid): %\",svm_sigmoid.score(x_test,y_test)*100)","4cfa2ad1":"y_pred = svm_sigmoid.predict(x_test)\ny_true = y_test\n\n# 0 value is negative\n# 1 value is positive\ncmatrix_sigmoid  = confusion_matrix(y_true,y_pred,labels=[0,1])\n\nf,ax = plt.subplots(figsize=(8,4))\nsns.heatmap(cmatrix_sigmoid,annot=True,linewidths=0.5,cbar=True,linecolor=\"white\",fmt='.0f',ax=ax)\nplt.title(\"Model's Confusion Matrix\")\nplt.xlabel(\"y_predict (Predicted Label)\")\nplt.ylabel(\"y_true (Actual Label)\")\nplt.show()\n\n#1=\"Pozitif\"\n#0=\"Negatif\"\n\nprint('True negative = ', cmatrix_sigmoid[0][0])\nprint('False positive = ', cmatrix_sigmoid[0][1])\nprint('False negative = ', cmatrix_sigmoid[1][0])\nprint('True positive = ', cmatrix_sigmoid[1][1])\nprint(\"\\n\\n\")\n\n\n\n\nfrom sklearn.metrics import classification_report\nprint(classification_report(y_true,y_pred))","4361fcd5":"from sklearn.naive_bayes import GaussianNB\nnb = GaussianNB()\nnb.fit(x_train, y_train)\nprint(\"print accuracy of naive bayes: % \",nb.score(x_test, y_test)*100)\nprint(svm_rbf.get_params())","a68f3808":"#confusion matrix for Criterion gini\ny_pred = nb.predict(x_test)\ny_true = y_test\n\n# 0 value is negative\n# 1 value is positive\ncmatrix_nb = confusion_matrix(y_true,y_pred,labels=[0,1])\n\nf,ax = plt.subplots(figsize=(8,4))\nsns.heatmap(cmatrix_nb,annot=True,linewidths=0.5,cbar=True,linecolor=\"white\",fmt='.0f',ax=ax)\nplt.title(\"Model's Confusion Matrix\")\nplt.xlabel(\"y_predict (Predicted Label)\")\nplt.ylabel(\"y_true (Actual Label)\")\nplt.show()\n\n#1=\"Pozitif\"\n#0=\"Negatif\"\n\nprint('True negative = ', cmatrix_nb[0][0])\nprint('False positive = ', cmatrix_nb[0][1])\nprint('False negative = ', cmatrix_nb[1][0])\nprint('True positive = ', cmatrix_nb[1][1])\nprint(\"\\n\\n\")\n\n\n\n\nfrom sklearn.metrics import classification_report\nprint(classification_report(y_true,y_pred))","29dd7e12":"from sklearn.linear_model import LogisticRegression\nlr = LogisticRegression()\nlr.fit(x_train,y_train)\nprint(\"Test Accuracy : %{}\".format(lr.score(x_test,y_test)*100))","f4ffd3ec":"from sklearn import linear_model\ntrain_score_list=[]\ntest_score_list=[]\narange=np.arange(1,20)\nfor each in range (1,len(arange)+1):\n    logreg = linear_model.LogisticRegression(random_state = 29,max_iter= each)\n    logreg.fit(x_train,y_train)\n    train_score_list.append(logreg.score(x_train,y_train))\n    test_score_list.append(logreg.score(x_test,y_test))    \nprint(\"Best accuracy(test) is {} with itereration n. = {}\".format(np.max(test_score_list),test_score_list.index(np.max(test_score_list))+ 1))\nprint(\"Best accuracy(train) is {} with iteration n. = {}\".format(np.max(train_score_list),train_score_list.index(np.max(train_score_list))+ 1))\nbest_max_iter_parameter = test_score_list.index(np.max(test_score_list))+ 1","246ec0eb":"logreg2 = linear_model.LogisticRegression(random_state = 29,max_iter= best_max_iter_parameter)\nlogreg2.fit(x_train,y_train)\ny_pred = logreg2.predict(x_test)\ny_true = y_test\n\nprint(\"Score: \",logreg2.score(x_train,y_train))","7638f746":"# 0 value is negative\n# 1 value is positive\ncmatrix_logreg = confusion_matrix(y_true,y_pred,labels=[0,1])\n\nf,ax = plt.subplots(figsize=(8,4))\nsns.heatmap(cmatrix_logreg,annot=True,linewidths=0.5,cbar=True,linecolor=\"white\",fmt='.0f',ax=ax)\nplt.title(\"Model's Confusion Matrix\")\nplt.xlabel(\"y_predict (Predicted Label)\")\nplt.ylabel(\"y_true (Actual Label)\")\nplt.show()\n\n#1=\"Pozitif\"\n#0=\"Negatif\"\n\nprint('True negative = ', cmatrix_logreg[0][0])\nprint('False positive = ', cmatrix_logreg[0][1])\nprint('False negative = ', cmatrix_logreg[1][0])\nprint('True positive = ', cmatrix_logreg[1][1])\n\nprint(\"\\n\\n\")\n\n\n\nfrom sklearn.metrics import classification_report\nprint(classification_report(y_true,y_pred))","b552e42f":"#import library for parameter tuning\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import cross_validate","6f174205":"x.shape #ALL DATASET drop('Outcome')","a37dbf30":"y.shape #Outcome","ae56992a":"knn = KNeighborsClassifier(68)\nknn.fit(x_train,y_train)\n\n\nresults = cross_validate(knn, x, y, cv=5,scoring='accuracy',return_train_score=True)\nprint(\"Cv:5, Test Score: {}\".format(results['test_score']))\nprint(\"Cv:5, Train Score: {}\".format(results['train_score']))\n\naccuracy = cross_val_score(knn,x,y,cv=5)\nprint(\"Accuracy: %0.2f (+\/- %0.2f)\" % (accuracy.mean(),accuracy.std() * 2))\n\nf1 = cross_val_score(knn,x,y,cv=5,scoring='f1')\nprint('F1 Score : ',f1)\n\nmse = cross_val_score(knn,x,y,cv=5,scoring='neg_mean_squared_error')\nprint('Negative Mean Squared Error: ', mse)\n\n\nprint(\"Cv = 5, recall = \",cross_val_score(knn, x, y, scoring='recall'))\nprint(\"Cv = 5, precision = \",cross_val_score(knn, x, y, scoring='precision'))\n\n","9787781d":"grid = {'n_neighbors': np.arange(1,100)}\nknn = KNeighborsClassifier()\nknn_cv = GridSearchCV(knn, grid, cv=5) # cross validation (cv) default = 5\nknn_cv.fit(x_train,y_train)# Fit\n\n# Print hyperparameter\n\nprint(\"Tuned hyperparameter k: {}\".format(knn_cv.best_params_)) \nprint(\"Best score: {}\".format(knn_cv.best_score_))","9907ddb6":"from sklearn.tree import DecisionTreeClassifier\ndtree = DecisionTreeClassifier(criterion='gini',max_depth=3)\ndtree.fit(x,y)\ndtree_result = cross_validate(dtree,x, y, cv=5,scoring='accuracy',return_train_score=True)\nprint(\"Cv:5, Test Score: {}\".format(dtree_result['test_score']))\nprint(\"Cv:5, Train Score: {}\".format(dtree_result['train_score']))\n\n\naccuracy = cross_val_score(dtree,x,y,cv=5)\nprint(\"Accuracy: %0.2f (+\/- %0.2f)\" % (accuracy.mean(),accuracy.std() * 2))\n\nf1 = cross_val_score(dtree,x,y,cv=5,scoring='f1')\nprint('F1 Score : ',f1)\n\nmse = cross_val_score(dtree,x,y,cv=5,scoring='neg_mean_squared_error')\nprint('Negative Mean Squared Error: ', mse)\nprint(\"Cv = 5, recall = \",cross_val_score(knn, x, y, scoring='recall'))\nprint(\"Cv = 5, precision = \",cross_val_score(knn, x, y, scoring='precision'))","1ff8fdf7":"from sklearn.tree import DecisionTreeClassifier\n\n#create a dictionary of all values we want to test\nparam_grid = {'criterion':['gini', 'entropy'],'max_depth': list(range(3,15))}\n# decision tree model\ndtree_model=DecisionTreeClassifier(random_state=29)\n#use gridsearch to test all values\ndtree_gscv = GridSearchCV(dtree_model, param_grid)# cv default = 5 = 5-fold cv\n #fit model to data\ndtree_gscv.fit(x_train, y_train)\nprint(\"Tuned hyperparameters Criterion: {}, Max_depth: {}\".format\n      (dtree_gscv.best_params_['criterion'],dtree_gscv.best_params_['max_depth'])) \nprint(\"Best score: {}\".format(dtree_gscv.best_score_))","294dd6a3":"rftree = RandomForestClassifier(criterion='entropy',max_depth=4,n_estimators=100,max_features='auto',random_state=1)\nrftree.fit(x,y)\nrftree_result = cross_validate(rftree,x, y, cv=5,scoring='accuracy',return_train_score=True)\nprint(\"Cv:5, Test Score: {}\".format(rftree_result['test_score']))\nprint(\"Cv:5, Train Score: {}\".format(rftree_result['train_score']))\n\n\naccuracy = cross_val_score(rftree,x,y,cv=5)\nprint(\"Accuracy: %0.2f (+\/- %0.2f)\" % (accuracy.mean(),accuracy.std() * 2))\n\nf1 = cross_val_score(rftree,x,y,cv=5,scoring='f1')\nprint('F1 Score : ',f1)\n\nmse = cross_val_score(rftree,x,y,cv=5,scoring='neg_mean_squared_error')\nprint('Negative Mean Squared Error: ', mse)\nprint(\"Cv = 5, recall = \",cross_val_score(rftree, x, y, scoring='recall'))\nprint(\"Cv = 5, precision = \",cross_val_score(rftree, x, y, scoring='precision'))","3cac953b":"from sklearn.ensemble import RandomForestClassifier\nparam_grid = { \n    'n_estimators': [10,50,100],\n    'max_features': ['auto', 'sqrt', 'log2'],\n    'max_depth' : [4,5,6,7],\n    'criterion' :['gini', 'entropy']\n}\nrfc =RandomForestClassifier(random_state=29)\nrfc_gscv=GridSearchCV(estimator=rfc, param_grid=param_grid) # cv default = 5\nrfc_gscv.fit(x_train,y_train)\n\nprint(\"Tuned best parameters for random forest: \",rfc_gscv.best_params_ ) \nprint(\"Best score: {}\".format(rfc_gscv.best_score_))\n\n                       \n# OUTPUT\n#Tuned best parameters for random forest:  {'criterion': 'entropy', 'max_depth': 4, 'max_features': 'auto', 'n_estimators': 100}\n#Best score: 0.780356524749048                     \n                     ","86787fed":"lin_svc = SVC(kernel='linear',C=10)\nlin_svc.fit(x,y)\nlin_svc_result = cross_validate(lin_svc,x, y, cv=5,scoring='accuracy',return_train_score=True)\nprint(\"Cv:5, Test Score: {}\".format(lin_svc_result['test_score']))\nprint(\"Cv:5, Train Score: {}\".format(lin_svc_result['train_score']))\n\n\naccuracy = cross_val_score(lin_svc,x,y,cv=5)\nprint(\"Accuracy: %0.2f (+\/- %0.2f)\" % (accuracy.mean(),accuracy.std() * 2))\n\nf1 = cross_val_score(lin_svc,x,y,cv=5,scoring='f1')\nprint('F1 Score : ',f1)\n\nmse = cross_val_score(lin_svc,x,y,cv=5,scoring='neg_mean_squared_error')\nprint('Negative Mean Squared Error: ', mse)\nprint(\"Cv = 5, recall = \",cross_val_score(lin_svc, x, y, scoring='recall'))\nprint(\"Cv = 5, precision = \",cross_val_score(lin_svc, x, y, scoring='precision'))","ba375475":"from sklearn.svm import SVC\nparam_grid = { 'C' : [0.001, 0.01, 0.1, 1, 10, 20]}\nsvm_linear = SVC(kernel=\"linear\")\nsvm_gscv0 = GridSearchCV(svm_linear,param_grid=param_grid)\nsvm_gscv0.fit(x_train,y_train)\n\nprint(\"Tuned best parameters for kernel linear svm: \",svm_gscv0.best_params_ ) \nprint(\"Best score: {}\".format(svm_gscv0.best_score_))\n","6c146db5":"rbf_svc = SVC(kernel='rbf',C=100,gamma=0.1)\nrbf_svc.fit(x,y)\nrbf_svc_result = cross_validate(rbf_svc,x, y, cv=5,scoring='accuracy',return_train_score=True)\nprint(\"Cv:5, Test Score: {}\".format(rbf_svc_result['test_score']))\nprint(\"Cv:5, Train Score: {}\".format(rbf_svc_result['train_score']))\n\n\naccuracy = cross_val_score(rbf_svc,x,y,cv=5)\nprint(\"Accuracy: %0.2f (+\/- %0.2f)\" % (accuracy.mean(),accuracy.std() * 2))\n\nf1 = cross_val_score(rbf_svc,x,y,cv=5,scoring='f1')\nprint('F1 Score : ',f1)\n\nmse = cross_val_score(rbf_svc,x,y,cv=5,scoring='neg_mean_squared_error')\nprint('Negative Mean Squared Error: ', mse)\nprint(\"Cv = 5, recall = \",cross_val_score(rbf_svc, x, y, scoring='recall'))\nprint(\"Cv = 5, precision = \",cross_val_score(rbf_svc, x, y, scoring='precision'))","b6fe136b":"from sklearn.svm import SVC\nparam_grid = { \n   'C': np.logspace(-3, 2, 6), 'gamma': np.logspace(-3, 2, 6)\n}\nsvm_rbf = SVC(kernel=\"rbf\")\nsvm_gscv = GridSearchCV(svm_rbf,param_grid) #cv default = 5\nsvm_gscv.fit(x_train,y_train)\n\nprint(\"Tuned best parameters for kernel rbf svm: \",svm_gscv.best_params_ ) \nprint(\"Best score: {}\".format(svm_gscv.best_score_))","6436c29c":"poly_svc = SVC(kernel='poly',C=1,degree=3)\npoly_svc.fit(x,y)\npoly_svc_result = cross_validate(poly_svc,x, y, cv=5,scoring='accuracy',return_train_score=True)\nprint(\"Cv:5, Test Score: {}\".format(poly_svc_result['test_score']))\nprint(\"Cv:5, Train Score: {}\".format(poly_svc_result['train_score']))\n\n\naccuracy = cross_val_score(poly_svc,x,y,cv=5)\nprint(\"Accuracy: %0.2f (+\/- %0.2f)\" % (accuracy.mean(),accuracy.std() * 2))\n\nf1 = cross_val_score(poly_svc,x,y,cv=5,scoring='f1')\nprint('F1 Score : ',f1)\n\nmse = cross_val_score(poly_svc,x,y,cv=5,scoring='neg_mean_squared_error')\nprint('Negative Mean Squared Error: ', mse)\nprint(\"Cv = 5, recall = \",cross_val_score(poly_svc, x, y, scoring='recall'))\nprint(\"Cv = 5, precision = \",cross_val_score(poly_svc, x, y, scoring='precision'))","0ba7b466":"### from sklearn.svm import SVC\nparam_grid = { \n    'C' : [0.001, 0.01, 0.1, 1, 10, 20],\n    'degree':list(range(1,5))\n}\nsvm_poly = SVC(kernel=\"poly\")\nsvm_gscv3 = GridSearchCV(svm_poly,param_grid) #cv default = 5\nsvm_gscv3.fit(x_train,y_train)\n\nprint(\"Tuned best parameters for kernel poly svm: \",svm_gscv3.best_params_ ) \nprint(\"Best score: {}\".format(svm_gscv3.best_score_))","18fb143c":"sigmoid_svc = SVC(kernel='sigmoid',gamma=0.5)\nsigmoid_svc.fit(x,y)\nsigmoid_svc_result = cross_validate(sigmoid_svc,x, y, cv=5,scoring='accuracy',return_train_score=True)\nprint(\"Cv:5, Test Score: {}\".format(sigmoid_svc_result['test_score']))\nprint(\"Cv:5, Train Score: {}\".format(sigmoid_svc_result['train_score']))\n\n\naccuracy = cross_val_score(sigmoid_svc,x,y,cv=5)\nprint(\"Accuracy: %0.2f (+\/- %0.2f)\" % (accuracy.mean(),accuracy.std() * 2))\n\nf1 = cross_val_score(sigmoid_svc,x,y,cv=5,scoring='f1')\nprint('F1 Score : ',f1)\n\nmse = cross_val_score(sigmoid_svc,x,y,cv=5,scoring='neg_mean_squared_error')\nprint('Negative Mean Squared Error: ', mse)\nprint(\"Cv = 5, recall = \",cross_val_score(sigmoid_svc, x, y, scoring='recall'))\nprint(\"Cv = 5, precision = \",cross_val_score(sigmoid_svc, x, y, scoring='precision'))","f3ae0cca":"from sklearn.svm import SVC\nparam_grid = { \n   'gamma': np.arange(0,1,0.1),\n    'coef0': np.logspace(-3, 2, 10)\n}\nsvm_sigmoid = SVC(kernel=\"sigmoid\")\nsvm_gscv4 = GridSearchCV(svm_sigmoid,param_grid) #cv default = 5\nsvm_gscv4.fit(x_train,y_train)\n\nprint(\"Tuned best parameters for kernel poly svm: \",svm_gscv4.best_params_ ) \nprint(\"Best score: {}\".format(svm_gscv4.best_score_))\n\n#Output\n#Tuned best parameters for kernel poly svm:  {'coef0': 0.003593813663804626, 'gamma': 0.5}\n#Best score: 0.7747317410868813\n","3e5ce812":"classifier = ['Knn','Decision Tree(Gini)','Random Forest','Svm(Linear)','Svm(Rbf)','Svm(Poly)','Svm(Sigmoid)']\naccuracy_score=[77.7,77.1,78,77.1,76.9,77.1,77.5]\nconclusionf =dict(Model=classifier,Accuracy_Score=accuracy_score)\nconclusionf = pd.DataFrame(conclusionf)\nconclusionf","4c44db6b":"arange=np.arange(2,31)\ntrace1=go.Scatter(\n    x=conclusionf.Model,\n    y=conclusionf.Accuracy_Score,\n    mode=\"lines + markers\",\n    name=\"Accuracy_Score-For CV 5\",\n    marker=dict(color = 'rgba(16, 0, 2, 0.8)'),\n    \n)\ndata=trace1\nlayout = dict(title = '-Model VS Accuracy',\n              xaxis= dict(title= 'Model',ticklen= 5,zeroline= False),\n              yaxis= dict(title= 'Accuracy_Score',ticklen= 5,zeroline= False)\n             )\nfig = dict(data = data, layout = layout)\niplot(fig)","1b02874f":"<a id=4.8><\/a>\n# 4.8. Parameter Tuning for SVM (kernel=\"rbf\")","3e67a271":"<b> Decision Tree Some Parameters <\/b>\n\n- criterion{\u201cgini\u201d, \u201centropy\u201d}, default=\u201dgini\u201d<br>\nThe function to measure the quality of a split. Supported criteria are \u201cgini\u201d for the Gini impurity and \u201centropy\u201d for the information gain.\n\n- splitter{\u201cbest\u201d, \u201crandom\u201d}, default=\u201dbest\u201d<br>\nThe strategy used to choose the split at each node. Supported strategies are \u201cbest\u201d to choose the best split and \u201crandom\u201d to choose the best random split.\n\n- max_depth: int, default=None<br>\nThe maximum depth of the tree. If None, then nodes are expanded until all leaves are pure or until all leaves contain less than min_samples_split samples.\n\n- min_samples_leafint or float, default=1 <br>\nThe minimum number of samples required to be at a leaf node. A split point at any depth will only be considered if it leaves at least min_samples_leaf training samples in each of the left and right branches. This may have the effect of smoothing the model, especially in regression.\n\nIf int, then consider min_samples_leaf as the minimum number.\n\nIf float, then min_samples_leaf is a fraction and ceil(min_samples_leaf * n_samples) are the minimum number of samples for each node.\n\n[For Reference](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.tree.DecisionTreeClassifier.html)","9dd6ae36":"\n### RBF SVM Performance Analysis","805d99dc":"\n        missing value does not appear in the data.\n        It consists of 768 sample and 9 features. \n        The class label of the dataset is the feature Outcome.\n        The forecasting process we will do will be on Outcome.","a198fe82":"<a id=1.1><\/a>\n## 1.1. Exploratory Data Analysis (EDA)","50bf40f4":"<a id=4.7><\/a>\n# 4.7. Parameter Tuning for SVM (kernel=\"linear\")","c37da009":"## Kernel Trick (None Linear SVM)\n\n\n-It aims to find the most appropriate decision line by adding z-axis as a third axis (3rd dimension) in the classification of the Svm graphic linearly.\n\n-Svm verileri do\u011frusal olarak s\u0131n\u0131fland\u0131ramad\u0131\u011f\u0131nda \u00fc\u00e7\u00fcnc\u00fc bir eksen (3.boyut) olarak z ekseni ekleyerek en optimal karar do\u011frusunu bulmay\u0131 hedefler.\n\n<center>![Kernel Trick](https:\/\/i.hizliresim.com\/mSa98Y.png)<\/center>\n","e3e0c781":"<a id='1'><\/a>\n# 1. Data Reading and Data Pre-Processing\n\n\n\n This phase includes operations such as reading the data set, checking the missing value, checking the type of properties, the properties of the records in the data set, clearing the missing values \u200b\u200bin the data set, and several visualizations to better interpret the data set.\n\n--------------\n                                                                            TR\n---------\nBu a\u015fama veri setinin okunmas\u0131, kay\u0131p de\u011fer kontrol\u00fc, \u00f6zelliklerin t\u00fcr kontrol\u00fc, veri seti i\u00e7erisindeki kay\u0131tlar\u0131n \u00f6zellikleri, veri setinde bulunan kay\u0131p de\u011ferlerin temizlenmesi ve veri setini daha iyi yorumlamak i\u00e7in birka\u00e7 g\u00f6rselle\u015ftirme gibi i\u015flemler i\u00e7ermektedir.","04e23ce8":"### Naive Bayes Model Performace Analysis","1197291a":"* Formula of Min-Max Normalization \n\n![Min-Max Normalization](https:\/\/miro.medium.com\/max\/506\/1*Dl3P3Rrzto258X0Ales9Xw.png)","19a5946c":"<a id=4.6><\/a>\n# 4.6. Parameter Tuning for Random Forest\n","642a1dc1":"<a id=1.2><\/a>\n## 1.2. Data Cleaning\n\n* Missing Value significantly affects analysis results and success rates of models. These lost values \u200b\u200bmust be eliminated.\n* Some methods I can count to eliminate lost values \u200b\u200bare:\n   * With a constant value for all null values.\n   * completely remove the relevant sheet from the dataset\n   * With the mean value of the corresponding feature\n   * With the median value of the relevant fetur\n   * Estimation of lost values \u200b\u200bcan be provided by establishing a regression or decision tree model for missing values.\n* By looking at the histogram of each featurer, we tried to conclude how to replace null values.\n* Since the distribution of BloodPressure and Glucose feathers seemed \"Normal\" and close to normal, I chose to fill each null value with the mean value of the relevant featurer.\n* I chose to fill the median values \u200b\u200bof the related feathers for the Null values \u200b\u200bof the other feathers.\n    \n------------------------------------------------------------------------------------------------------------------------------------------------\n\n                                                                TR\n* Kay\u0131p de\u011ferler (Missing Value), analiz sonu\u00e7lar\u0131n\u0131 ve modellerin ba\u015far\u0131 oranlar\u0131n\u0131 ciddi miktarda etkiler. Bu kay\u0131p de\u011ferlerin giderilmesi gerekir.\n* Kay\u0131p de\u011ferleri gidermek i\u00e7in sayabilece\u011fim baz\u0131 y\u00f6ntemler \u015funlard\u0131r:\n  *  B\u00fct\u00fcn null de\u011ferleri i\u00e7in bir sabit de\u011fer ile.\n  *  ilgili samplenin tamamen veri setinden \u00e7\u0131kar\u0131lmas\u0131\n  *  \u0130lgili Featurenin mean de\u011feri ile\n  *  \u0130lgili feturenin median de\u011feri ile\n  *   Kay\u0131p de\u011ferler i\u00e7in bir regresyon yada karar a\u011fac\u0131 modeli kurularak kay\u0131p de\u011ferlerin tahmini sa\u011flanabilir. \n* Her bir featurenin histogram\u0131na bakarak, null de\u011ferleri nas\u0131l replace edece\u011fimiz hakk\u0131nda bir kan\u0131ya varmaya \u00e7al\u0131\u015ft\u0131k\n* BloodPressure ve Glucose featurelerinin da\u011f\u0131l\u0131m\u0131 \"Normal\" ve normale yak\u0131n g\u00f6r\u00fcnd\u00fc\u011f\u00fc i\u00e7in her bir null de\u011feri ilgili featurenin mean de\u011feri ile doldurmay\u0131 tercih ettim.\n* Di\u011fer featurelerin Null de\u011ferleri i\u00e7in ilgili featurelerin median de\u011ferleri doldurmay\u0131 tercih ettim. ","c9c4b70c":"<a id=3.4><\/a>\n# 3.4. Classification with Random Forest\n*Random Forests combine multiple decision tree models to convey their predictions to a decision mechanism\nand this decision mechanism is determined by the majority of votes and the final classification result. *<br>\n<center> ![](https:\/\/i.hizliresim.com\/2NnVpM.png) <\/center> \n\n1. The whole data set is divided into two parts according to the determined rate (train and test). When dividing into pieces, the pieces that will correspond to the percentiles are randomly divided. There are now two data sets available: the Learning data set and the Test data set.\n2. Learning data sets are randomly divided into n sub-set for N decision tree models.\n3. N decision tree models perform the learning process with the learning data assigned to them.\n4. N decision tree models are tested using the test data set and N prediction results are obtained.\n5. These estimation results conveyed to the decision making mechanism are subject to the majority of votes.\n6. The result determined by the majority of votes from the decision making mechanism is the final result. Classification process is completed\n\n----------------------------------------------------------------------------------------------------\n                                                                TR\n----------------------------------------------------------------------------\n\n*Rassal Ormanlar birden fazla karar a\u011fac\u0131 modelinin biraraya gelerek yapt\u0131klar\u0131 tahminleri bir karar mekanizmas\u0131na iletmesi\nve bu karar mekanizmas\u0131nda oy \u00e7oklu\u011fu ile nihai s\u0131n\u0131fland\u0131rma sonucunun belirlenmesi \u015feklinde ger\u00e7ekle\u015fir.*\n\n\n1. T\u00fcm veri seti belirlenen orana g\u00f6re (\u00f6\u011frenme ve test) iki par\u00e7aya b\u00f6l\u00fcn\u00fcr. Par\u00e7aya b\u00f6l\u00fcn\u00fcrken y\u00fczdelik dilimlere denk gelecek par\u00e7alar rastgele b\u00f6l\u00fcmlenir. Art\u0131k elde iki veri seti bulunmaktad\u0131r: \u00d6\u011frenme veri seti ve Test veri seti.\n2. \u00d6\u011frenme veri setleri N adet karar a\u011fac\u0131 modeli i\u00e7in n adet alt par\u00e7aya yine rastgele olacak \u015fekilde b\u00f6l\u00fcn\u00fcr.\n3. N adet karar a\u011fac\u0131 modelleri kendilerine atanan \u00f6\u011frenme verileri ile \u00f6\u011frenme i\u015flemini ger\u00e7ekle\u015ftirir.\n4. Test veri seti kullan\u0131larak N adet karar a\u011fac\u0131 modeli test edilir ve N adet tahmin sonucu elde edilir.\n5. Karar verme mekanizmas\u0131na iletilen bu tahmin sonu\u00e7lar\u0131, oy \u00e7oklu\u011funa tabi tutulur.\n6. Karar verme mekanizmas\u0131ndan oy \u00e7oklu\u011fu ile belirlenen sonu\u00e7 nihai sonu\u00e7tur. S\u0131n\u0131fland\u0131rma i\u015flemi tamamlan\u0131r.\n\n","1bf07734":"<a id=1.3><\/a>\n## 1.3. Data Normalization (X={0...1})\n    \n    The distance between the values \u200b\u200bin the data is important. Many different values \u200b\u200bcan destroy added values.\n    Very small or very large values \u200b\u200bare accepted as outliers. It affects system performance.\n    Considering two different features, one can contain values \u200b\u200bfrom 1 to 10, while the other feature can contain values \u200b\u200bfrom 10,000 and above.\n    In this case, the feature with large values \u200b\u200bcan manipulate small-value feaures and decrease the number of floors.\n    This causes poor performance.\n    By normalizing all the data, it is possible to draw, reduce, convert to the range of 0 to 1.\n    \n    -------------------------------------------------------------------------------------------------------------------------------------------\n                                                                TR\n    \n    Veri i\u00e7erisindeki de\u011ferlerin birbirlerine uzakl\u0131\u011f\u0131 \u00f6nemlidir. Birbirinden \u00e7ok farkl\u0131 de\u011ferler katma de\u011ferlerini yok edebilir.\n    \u00c7ok k\u00fc\u00e7\u00fck veya \u00e7ok b\u00fcy\u00fck de\u011ferler ayk\u0131r\u0131 de\u011ferler olarak kab\u00fcl edilir. Sistem performans\u0131n\u0131 etkilemesi s\u00f6z konusudur.\n    \u0130ki farkl\u0131 featureyi d\u00fc\u015f\u00fcn\u00fcrsek birisi 1 ila 10 aras\u0131nda de\u011ferler bar\u0131nd\u0131r\u0131rken, di\u011fer feature 10.000 ve yukar\u0131s\u0131 de\u011fer bar\u0131nd\u0131rabilir.\n    Bu durumda b\u00fcy\u00fck de\u011ferlerin yer ald\u0131\u011f\u0131 feature k\u00fc\u00e7\u00fck de\u011ferli feaureyi manipule edebilir, kat say\u0131s\u0131n\u0131 d\u00fc\u015f\u00fcrebilir. \n    Bu durum performans d\u00fc\u015f\u00fckl\u00fc\u011f\u00fcne yol a\u00e7ar.\n    B\u00fct\u00fcn verileri normalize ederek 0 ila 1 aral\u0131\u011f\u0131na \u00e7ekmek,indirgemek,d\u00f6n\u00fc\u015ft\u00fcrmek m\u00fcmk\u00fcnd\u00fcr.\n    \n    A = [5,10,100,250,50] bir feature d\u00fc\u015f\u00fcnelim.\n    A featuresini normalize etmek istedi\u011fimiz izleyece\u011fimiz form\u00fcl a\u015fa\u011f\u0131dad\u0131r.\n    A[i] Normalize de\u011fer = (ger\u00e7ek de\u011fer) - (featurenin minimum de\u011feri)         \/  (featurenin maksimum de\u011feri)-(featurenin minimum de\u011feri)\n    A[0] = (5-5) \/ 250-5 = 0\n    A[1] = (10-5) \/ (250-5) = 0.020\n    A[2] = (100-5) \/ (250-5) = 0.380\n    A[3] = (250-5) \/ (250-5) = 1\n    A[4] = (50-5) \/ (25-5) = 0.14\n    \n    A(norm)= [0,0.020,0.38,1,0.14]\n    ","26be5998":"### Rbf (Gaussian) Support Vector Machine with Sklearn","fb844a2c":"<a id=2><\/a>\n# 2. Train Test Split\n\n    The train_test_split method is a method in the model_selection module.\n    Two data sets are required to be used in the development and testing of models.\n    These datasets can be derived from the dataset that they are intended to use in the development of the model or they may be a different dataset.\n    With the train_test_split method, a data set can be separated into a train and test data set at the desired rate.\n    X, which includes the previously separated samples, y, which indicates which classes the sample belongs to, is given as a parameter in this method.\n    According to the percentage given to the test_size parameter, this method performs partitioning.\n    \n    x_train: x data to be used in education (70%)  y_train: y data to be used in education (class labels of x_train)\n    x_test: x data to be used in the test (30%)    y_test: y data to be used in the test (class labels of x_test)\n    \n    test_size = 0.3: We said that 30% of the data set is used for training and 70% for the test.\n    \n    The random_state parameter is: The measurement of the performance of the models requires constant processing with the same data.\n    Otherwise, the result for any model will give a different result when the same model is run again. Because train_test_split\n    The method takes the percentile given to the test_size parameter from different parts of the data set each time.\n    Here is the random_state parameter that will make this partitioning process from a fixed part.\n    The value given to this parameter is not important. This method stores the parameter value in its memory and when the model is run,\n    value is the random_stte parameter. If the parameter matches, it processes with the same data set.\n\n------------------------------------------------------------------------------------------------------------------------\n\n    train_test_split methodu model_selection modul\u00fc i\u00e7erisinde yer alan bir methoddur.\n    Modellerin geli\u015ftirilmesinde ve test edilmesinde kullan\u0131lmak \u00fczere iki veri seti gerekir.\n    Bu veri setleri modelin geli\u015ftirilmesinde kullanmas\u0131 hedeflenen veri setinden t\u00fcretilebilece\u011fi gibi farkl\u0131 bir veri seti de olabilir.\n    train_test_split methodu ile bir veri seti istenilen oranda train ve test veri seti olmak \u00fczere ayr\u0131labilir.\n    Daha \u00f6nceden ayr\u0131lm\u0131\u015f \u00f6rneklemleri i\u00e7eren x, \u00f6rneklemin hangi s\u0131n\u0131flara ait oldu\u011funu ifade eden y, bu methoda parametre olarak verilir.\n    test_size parametresine verilen y\u00fczdeli\u011fe g\u00f6re bu method b\u00f6l\u00fcmleme i\u015flemi yapar.\n    \n    x_train: E\u011fitimde kullan\u0131lacak x verisi (%70)         y_train: E\u011fitimde kullan\u0131lacak y verisi (x_train'in s\u0131n\u0131f etiketleri)\n    x_test : Testte kullan\u0131lacak x verisi (%30)           y_test: Testte kullan\u0131lacak y verisi (x_test'in s\u0131n\u0131f etiketleri)\n    \n    test_size =0.3 : Veri setinin %30'unu test i\u00e7n %70 ini e\u011fitim i\u00e7in kullan demi\u015f olduk.\n    \n    random_state parametresi ise: Modellerin performanslar\u0131n\u0131n \u00f6l\u00e7\u00fcm\u00fcnde sabit olarak ayn\u0131 veriler ile i\u015flem yap\u0131lmas\u0131 gerekir.\n    Aksi taktirde herhangi bir model i\u00e7in al\u0131nan sonu\u00e7 tekrar ayn\u0131 model ko\u015fturuldu\u011funda farkl\u0131 sonucu verecektir. \u00c7\u00fcnk\u00fc train_test_split\n    methodu her seferinde test_size parametresine verilen y\u00fczdelik b\u00f6l\u00fcm\u00fc, veri setinin farkl\u0131 k\u0131s\u0131mlar\u0131ndan almaktad\u0131r.\n    \u0130\u015fte bu b\u00f6l\u00fcmleme i\u015flemini sabit bir k\u0131s\u0131mdan yap\u0131lmas\u0131n\u0131 sa\u011flayacak olan ise random_state parametresidir.\n    Bu parametreye verilen de\u011ferin \u00f6nemi yoktur. Bu method parametre de\u011ferini haf\u0131zas\u0131nda saklar ve model ko\u015fturuldu\u011funda ilk bakt\u0131\u011f\u0131\n    de\u011fer random_stte parametresidir. Parametrenin e\u015fle\u015fmesi halinde ayn\u0131 veri seti ile i\u015flem yapar.","d22c937d":"## Decision Tree Performance Analysis\n\n","112dd919":"\n- In reality, the number of patients without diabetes is 149. He estimated 16 of them as diabetes, and 133 of them correctly predicted.\n- The number of patients with diabetes is 82. He predicted 48 correctly, 34 incorrectly (not diabetic)\n- The rate of knowing those with diabetes is higher than the rate of knowing non-patients.\n- So they actually have a higher rate than they know 0 and 0, actually 1 and 1\n-------------------------------------------------------------------------------------------------------\n- 149 diyabet olmayan hastan\u0131n 133'\u00fcn\u00fc do\u011fru bilmi\u015f 16's\u0131n\u0131 yanl\u0131\u015f bilmi\u015f yani diyabetmi\u015f gibi bulmu\u015f.\n- 82 diyabet hastas\u0131n\u0131n 34 tanesini diyabet de\u011fil olarak bilmi\u015f ve 48 tanesini do\u011fru bilmi\u015f.\n- Yap\u0131lacak \u00e7\u0131kar\u0131m \u015fudur ki diyabet hastas\u0131 olanlar\u0131 bilme oran\u0131, hasta olmayanlar\u0131 bilme oran\u0131na g\u00f6re daha y\u00fcksektir.\n- Yani ger\u00e7ekte 0 olup 0 bildikleri, ger\u00e7ekte 1 olup 1 bildiklerinden daha y\u00fcksek orana sahiptir","629e697b":"<a id=3.5.3><\/a>\n## 3.5.3. Gaussian Rbf (Radial Basis Function) Kernel\n\n    - Gamma is the main parameter.\n                                             Parameter Gamma and C\n    - It is generally used in the sample space and it has a spiral distribution.\n    - Since the data is not distributed linearly, then it is impossible to draw a logical decision line (maximum margin).\n    - It is possible to create a decision line (max margin) by adding a new dimension (z) to a 2D dataset.\n    - The gamma parameter used in Rbf kernel is directly proportional to the euclidean distance. That is, the closer the two vectors, the lower the gamma            value.\n    - But lowering Gamma too much may make the process generalizing, that is, giving bad results,\n    - Keeping it high may cause excessive curve fitting and increase processing cost.\n    - Gamma should be kept neither too little nor too much. \n    - Because the best, most optimal decision line(margin classifier) is the equidistant and max decision line on both sides.\n    \n    C is the slack parameter for SVM, regardless of kernel chosen, it has nothing to do with the kernel. It determines the tradeoff between a wide margin and classifier error. Large C means you are allowing little slack and the model will fit tighter to your data, small C means you are allowing a lot of slack and the model will have more error on the training set, but be less sensitive to noise.\n  \n--------------------------------------------------------------------------------------------------------------------------------------\n                                               Gamma ve C parametreleri\n    - Genelde \u00f6rneklem uzay\u0131ndaki verilerin spiral \u015feklinde da\u011f\u0131l\u0131ma sahip oldu\u011fu durumlarda kullan\u0131l\u0131r.\n    - Verilerin do\u011frusal olarak da\u011f\u0131lmad\u0131\u011f\u0131 durumlarda veriler i\u00e7in mant\u0131kl\u0131 bir karar do\u011frusu (max margin) \u00e7izmek imkans\u0131zd\u0131r.\n    - Bu durumlarda a\u015fa\u011f\u0131daki \u00f6rnekte g\u00f6r\u00fcld\u00fc\u011f\u00fc gibi 2 boyutlu bir veri k\u00fcmesine yeni bir boyut(z) ekleyerek, karar do\u011frusu (max margin) olu\u015fturmak olas\u0131d\u0131r.\n    - Rbf kernel'da kullan\u0131lan gamma parametresi, \u00f6klid uzakl\u0131\u011f\u0131 ile do\u011fru orant\u0131l\u0131d\u0131r. Yani iki vekt\u00f6r nekadar yak\u0131nsa gamma de\u011feri o kadar d\u00fc\u015fecektir.\n    - Fakat Gamman\u0131n fazla d\u00fc\u015f\u00fcr\u00fclmesi i\u015flemin genelleme yapmas\u0131n\u0131 yani k\u00f6t\u00fc sonu\u00e7 vermesini sa\u011flayabilece\u011fi gibi,\n    - y\u00fcksek tutulmas\u0131da e\u011fri uydurmada a\u015f\u0131r\u0131ya sebebiyet verebilir ve i\u015flem maliyetini artt\u0131r\u0131r.\n    - Gamma ne az, ne de \u00e7ok tutulmal\u0131d\u0131r. \u00c7\u00fcnk\u00fc en iyi, en optimal karar \u00e7izgisi iki tarafada e\u015fit uzakl\u0131kta ve maksimum olan karar \u00e7izgisidir.\n    \n    C, se\u00e7ilen \u00e7ekirde\u011fe bak\u0131lmaks\u0131z\u0131n, SVM'nin gev\u015fek parametresidir, \u00e7ekirdek ile ilgisi yoktur. Geni\u015f bir kenar bo\u015flu\u011fu ve s\u0131n\u0131fland\u0131r\u0131c\u0131 hatas\u0131 aras\u0131ndaki dengeyi belirler. B\u00fcy\u00fck C, biraz gev\u015fekli\u011fe izin verdi\u011finiz anlam\u0131na gelir ve model verilerinize daha s\u0131k\u0131 s\u0131\u011far,\n    k\u00fc\u00e7\u00fck C, \u00e7ok fazla gev\u015fekli\u011fe izin verdi\u011finiz anlam\u0131na gelir ve modelin e\u011fitim setinde daha fazla hatas\u0131 olacakt\u0131r, ancak g\u00fcr\u00fclt\u00fcye daha az duyarl\u0131 olacakt\u0131r.\n\n    \n- [For Reference](https:\/\/scikit-learn.org\/stable\/auto_examples\/svm\/plot_rbf_parameters.html)\n\n","0cbbc3ad":"\n# Parameter Tuning for SVM","85284dc2":"<a id=3.3><\/a>\n# 3.3. Classification with Decision Tree\n\n* Each attribute is represented by a node.\nThe branches and leaves are the tree structure.\nThe last structure is called \"leaf\", the top structure is called \"root\" and the structures between them are called \"branch\".\n\n* Algorithms such as Twoing and Gini have been developed for Classification and Regression trees.\nMany algorithms have been developed by Qinlan for classification processes with decision trees.\nSome of these are the ID3 and C4.5 algorithms.\n\n* The measure of uncertainty in a system is called \"entropy\". Let S be a resource. Let's say that this resource can produce n messages: {m1, m2, ... mn}.\nAll messages are independent of each other and the probability that mi messages are produced is pi. The entropy H (S) of the S source that produces the messages with probability distribution P = {p1, p2 ... pn} is as follows.\n\n![Entropi](https:\/\/i.hizliresim.com\/lHKfya.png)\n\n\n### Entropy, Feature Selection and Gain criteria?\n\n* If we consider the values \u200b\u200bthat the class variable (T) can take in a data set, let's assume that the record set is {C1, C2, C3..CK}.\nIn other words, there are k types of values \u200b\u200b(class label, class category) in the class variable.\nIn this case, the Entropy of the T class is calculated as follows.\n\n* First, P (T), that is, the probability distribution of classes must be calculated.\n\n![olasl\u0131k da\u011f\u0131l\u0131m\u0131](https:\/\/i.hizliresim.com\/GKLial.png)\n\n- examle\n\n        risk = { blue,blue,blue,purple,blue,purple,purple,blue,blue,purple}\n        \n        Probability distribution for these attribute values: C1 = blue, C2 = purple.\n            [C1] = 6\n            [C2] = 4\n            probabilities are calculated as p1 = 6\/10, p2 = 4\/10.\n\n            So the probability distribution = P (risk) = (6\/10, 4\/10).\n\n            Entropy => H (risk) = - [6 \/ 10.log2.6 \/ 10 + 4 \/ 10.log2.4 \/ 10] = 0.97.\n            \n** Selecting Attributes for Branching and Boiler Criterion **\n\n* If T, which expresses the target attribute, is divided into T1, T2 .. Tn al clusters depending on the value of an X attribute without a target attribute (i.e. not a class attribute),\nThe information required to determine the class of an element of T is considered the weighted average of the information required to determine the class of an element of Ti.\nSo, depending on this definition, the information required to determine the class of an element of T is calculated as follows.\n\n![](https:\/\/i.hizliresim.com\/n69rYU.png)\n<br>\n\n* In order to measure the information obtained as a result of dividing the T set by X test, the expression called \"gain criterion\" is used.\nThe gain criterion is calculated as follows.\n\n** Gain (X, T) = H (T) - H (X, T) **\n\n** It is aimed to maximize (maximize) the value of earnings while making the separation process in decision trees.\nThe X attribute is selected, which provides the highest information gain, that is, to maximize the gain. **\n\n\n        ------------------------------------------------------------------------------------------------------------------------------\n<center> TR <\/center><br>\n\n\n\n* Her bir \u00f6znitelik bir d\u00fc\u011f\u00fcm taraf\u0131ndan temsil edilir.\nDallar ve yapraklar a\u011fa\u00e7 yap\u0131s\u0131n\u0131n elemanlar\u0131d\u0131r.\nEn son yap\u0131 \"yaprak\" olarak, en \u00fcst yap\u0131\"k\u00f6k\" ve bunlar\u0131n aras\u0131nda kalan yap\u0131lar ise \"dal\" olarak isimlendirilir.\n\n* S\u0131n\u0131fland\u0131rma ve Regresyon a\u011fa\u00e7lar\u0131 konusunda Twoing, Gini gibi algoritmalar geli\u015ftirilmi\u015ftir.\nKarar a\u011fa\u00e7lar\u0131 ile s\u0131n\u0131fland\u0131rma i\u015flemleri i\u00e7in Qinlan taraf\u0131ndan bir\u00e7ok algoritma geli\u015ftirilmi\u015ftir.\nBunlar aras\u0131nda yer alan ID3 ve C4.5 algoritmalar\u0131ndan baz\u0131lar\u0131d\u0131r.\n\n* Bir sistemdeki belirsizli\u011fin \u00f6l.\u00fcs\u00fcne \"entropi\" ad\u0131 verilir. S bir kaynak olsun. Bu kayna\u011f\u0131n {m1,m2,...mn} olmak \u00fczere n mesaj \u00fcretebildi\u011fini varsayal\u0131m. \nT\u00fcm mesajlar birbirinden ba\u011f\u0131ms\u0131zd\u0131r ve mi mesajlar\u0131n\u0131n \u00fcretilme olas\u0131l\u0131klar\u0131 pi'dir. P={p1,p2...pn} olas\u0131l\u0131k da\u011f\u0131l\u0131m\u0131na sahip mesajlar\u0131 \u00fcreten S kayna\u011f\u0131n entropisi H(S) \u015fu \u015fekildedir.\n\n![Entropi](https:\/\/i.hizliresim.com\/lHKfya.png)\n\n### Entropi, \u00d6znitelik Se\u00e7ilmesi ve Kazan\u00e7 \u00f6l\u00e7\u00fct\u00fc ?\n\n* Bir veri setinde s\u0131n\u0131f de\u011fi\u015fkeninin (T) alabilece\u011fi de\u011ferleri d\u00fc\u015f\u00fcn\u00fcrsek,kay\u0131t k\u00fcmesinin {C1,C2,C3..CK} \u015feklinde oldu\u011funu varsayal\u0131m.\nYani s\u0131n\u0131f de\u011fi\u015fkeninde k t\u00fcrde de\u011fer (class label, class category) mevcut olsun.\nBu durumda T s\u0131n\u0131f\u0131n\u0131n Entropisi \u015fu \u015fekilde hesaplan\u0131r.\n\n* \u0130lk olarak P(T) yani s\u0131n\u0131flar\u0131n olas\u0131l\u0131k da\u011f\u0131l\u0131m\u0131n\u0131n hesaplanmas\u0131 gerekir.\n\n![olasl\u0131k da\u011f\u0131l\u0131m\u0131](https:\/\/i.hizliresim.com\/GKLial.png)\n\n* [Ci] ifadesi Ci k\u00fcmesindeki elemanlar\u0131n say\u0131s\u0131n\u0131 vermektedir. Burada \u00f6rne\u011fin Pi = |Ci| \/ |T| olas\u0131l\u0131\u011f\u0131n\u0131 ifade etmektedir.\nO halde ba\u015fka bir deyi\u015fle T i\u00e7n ortalama bilgi miktar\u0131 yani entropi \u015fu \u015fekilde ifade edilir.\n\n![entropi T](https:\/\/i.hizliresim.com\/ebEAUJ.png)\n\n\u00f6rnek\n\n    risk = { mavi,mavi,mavi,mor,mavi,mor,mor,mavi,mavi,mor}\n    \n    Bu \u00f6znitelik de\u011ferleri i\u00e7in olas\u0131l\u0131klar da\u011f\u0131l\u0131m\u0131: C1=Mavi, C2=Mor olsun.\n    [C1] = 6\n    [C2] = 4\n    oldu\u011funa g\u00f6re, olas\u0131l\u0131klar p1=6\/10, p2=4\/10 bi\u00e7iminde hesaplan\u0131r.\n    \n    O halde olas\u0131l\u0131klar da\u011f\u0131l\u0131m\u0131 = P(risk) = (6\/10, 4\/10).\n    \n    Entropi =>  H(risk) = -[ 6\/10.log2.6\/10  +  4\/10.log2.4\/10] = 0.97.\n\n** Dallanma i\u00e7in \u00d6zniteliklerin Se\u00e7ilmesi ve Kazan \u00d6l\u00e7\u00fct\u00fc **\n\n* Hedef niteli\u011fini ifade eden T, hedef niteli\u011fi olmayan (yani s\u0131n\u0131f niteli\u011fi olmayan) bir X niteli\u011finin de\u011ferine ba\u011fl\u0131 olarak T1,T2 .. Tn al k\u00fcmelerine ayr\u0131l\u0131rsa,\nT nin bir eleman\u0131n\u0131n s\u0131n\u0131f\u0131n\u0131 belirlemek i\u00e7in gerekli bilgi Ti nin bir eleman\u0131n\u0131n s\u0131n\u0131f\u0131n\u0131n belirlenmesinde gerekli olan bilginin a\u011f\u0131rl\u0131kl\u0131 ortalamas\u0131 olarak kabul edilir.\nO halde, bu tan\u0131ma ba\u011fl\u0131 olarak T nin bir eleman\u0131n\u0131n s\u0131n\u0131f\u0131n\u0131 belirlemek i\u00e7in gerekli bilgi \u015fu \u015fekilde hesaplan\u0131r.\n\n![](https:\/\/i.hizliresim.com\/n69rYU.png)\n<br><br>\n\n* T k\u00fcmesinin X testine g\u00f6re b\u00f6l\u00fcnmesi sonucunda elde edilen bilgiyi \u00f6l\u00e7mek i\u00e7in \"kazan\u00e7 \u00f6l\u00e7\u00fct\u00fc\" ad\u0131 verilen ifadeye ba\u015fvurulur.\nKazan\u00e7 \u00f6l\u00e7\u00fct\u00fc \u015fu \u015fekilde hesaplan\u0131r.\n\nKazan\u00e7(X,T) = H(T) - H(X,T)\n\n** Karar a\u011fa\u00e7lar\u0131nda ay\u0131rma i\u015flemi yap\u0131l\u0131rken kazan\u00e7 de\u011ferini en \u00e7oklama (maximize) ama\u00e7lan\u0131r.\nEn y\u00fcksek bilgi kazanc\u0131n\u0131 sa\u011flayan, yani kazan\u0131 maksimize edebilecek X \u00f6zniteli\u011fi se\u00e7ilir. **<br><br><br>\n\n\n\n<center>![](https:\/\/i.hizliresim.com\/347M52.png)<\/center>","13cb52ed":"<a id=5><\/a>\n# Conclusion\n\n** For Cross Validation = 5**\n\n","fbfb54b8":"    Featurelerde yer alan \"0\" de\u011ferlerini \"NAN (null)\" olarak kab\u00fcl ettik.\n    0 de\u011ferleri null olarak replace edildi.\n\n    0 values in features are accepted as null values \u200b\u200band 0 values \u200b\u200bare replaced with null","52bcba2a":"<b> Precision <\/b>\n\n- 80% of what we estimate as \"not diabetic\" is not really sick.                                        ==> p0\n- 75% of what we estimate as \"diabetic\" is really sick.                                                ==> p1\n\n        NOTE: It allows us to think carefully before calling someone sick.\n\n- We actually estimated 89% of those who were not sick. In other words, we said 89% of them \"not sick\".==> R0\n- We actually estimated 59% of those who were sick. So we said 59% of you are \"sick\". ==> R1\n\n<b> Recall <\/b>\n\n        NOTE: It is more important to correctly detect some anomaly cases than to produce false alarms. In other words, false negative is more critical than false positivity. Rather than being able to detect someone with cancer and cause death, it is more acceptable to make a false estimate and call him to the hospital.\n        \n<b>F1 SCORE:<\/b>\n\n- As you can see, recall and precision have two important metrics and a trade-off between them. To cope with this, the F1-score is used. The F1-score uses the harmonic average instead of the arithmetic mean to punish extreme situations.        \n\n--------------------------------------------------------------------------------------\n--------------------------------------------------------------------------------------\n<b> TR <\/b>\n\n<b> Precision (Kesinlik) <\/b>\n\n- \"Diyabet hastas\u0131 de\u011fil\" olarak tahmin ettiklerimizin %80'i ger\u00e7ekten hasta de\u011fil.                     ==>p0\n- \"Diyabet hastas\u0131\" olarak tahmin ettiklerimizin %75'i ger\u00e7ekten hasta.                                 ==>p1\n\n         NOT:Birine hasta demeden \u00f6nce iyice d\u00fc\u015f\u00fcn\u00fcp ta\u015f\u0131nmam\u0131z\u0131 sa\u011fl\u0131yor.\n         \n<b>  Recision <\/b>\n\n- Ger\u00e7ekte hasta olmayanlar\u0131n %89'unu do\u011fru tahmin ettik. Yani %89'una \"hasta de\u011fil\" dedik.             ==>R0\n- Ger\u00e7ekte hasta olanlar\u0131n %59'unu do\u011fru tahmin ettik. Yani %59'una \"hastas\u0131n\" dedik.                   ==>R1\n\n        NOT :Baz\u0131 anomali vakalar\u0131n\u0131 do\u011fru tespit etmek yanl\u0131\u015f alarm \u00fcretmekten daha \u00f6nemli. Di\u011fer bir deyi\u015fle false negative false positiveden daha kritik. Kanserli birini tespit edemeyip \u00f6l\u00fcm\u00fcne neden olmaktansa kanser olmayan biri i\u00e7in yanl\u0131\u015f tahmin yap\u0131p onu hastaneye \u00e7a\u011f\u0131rmak daha kabul edilebilir.\n\n\n<b>F1 SCORE:<\/b>\n\n\n- As you can see, recall and precision have two important metrics and a trade-off between them. To cope with this, the F1-score is used. The F1-score uses the harmonic average instead of the arithmetic mean to punish extreme situations.\n------------------------------------------------------------------------------------------------------\n- G\u00f6rd\u00fc\u011f\u00fcn\u00fcz gibi recall ve precision iki \u00f6nemli metrik ve aralar\u0131nda bir trade-off var. Bununla ba\u015f edebilmek i\u00e7in F1-skoru kullan\u0131l\u0131yor. F1-skoru ekstrem durumlar\u0131 cezaland\u0131rmak i\u00e7in aritmetik ortalama yerine harmonik ortalamay\u0131 kullan\u0131yor.\n","920da5d1":"<a id=4.10><\/a>\n# 4.10 Parameter Tuning for SVM (kernel=\"sigmoid\")","5803bffb":"### Linear svm performance analysis","97e10988":"<a id=4.5><\/a>\n# 4.5. Parameter Tuning for Decision Tree","c304e252":"## describe()\n\n\n* How many records belong to each attribute (count),\n* Mean value of the records in each attribute (mean),\n* standard deviations, i.e. divergence value of each value,\n* It is the method that outputs 25%, 50% (median), 75% percentiles (percentile).\n\n* 25% percentile, 25% and less of the values \u200b\u200bin Pregnancies have a value of 1 and below. It allows us to comment.\n* Similarly, 50% and less of the Glucose features of the dataset have a value of 117 or less. It also corresponds to the median value of the data set with 50% percentile. In other words, it is the value that divides the data set into two in a ordered manner.\n* With the same logic, 75% and less of the records of the BloodPressure feature have 32 and lower values.\n-------------------------------------------------------------------------------------\n*     Her bir \u00f6zniteli\u011fe ait ka\u00e7 kay\u0131t yer almakta (count),\n*     Her bir \u00f6znitelikte yer alan kay\u0131tlar\u0131n ortalama de\u011feri(mean),\n*     standart sapmalar\u0131, yani her bir de\u011ferin ortalamadan uzakla\u015fma de\u011feri,\n*     %25,%50(median),%75 lik y\u00fczdelikler(percentile) gibi \u00e7\u0131kt\u0131lar\u0131 veren methoddur.\n\n*     %25 percentile, Pregnancies deki de\u011ferlerin %25 ve daha az\u0131 1 de\u011feri ve a\u015fa\u011f\u0131s\u0131na sahiptir. Yorumunu yapmam\u0131z\u0131 sa\u011flar.\n*     Ayn\u0131 \u015fekilde veri setinin Glucose featuresinin %50 ve daha az\u0131 117 ve daha az de\u011fere sahiptir. Ayn\u0131 zamanda %50 percentile veri setinin ortanca de\u011ferine(median) denk gelmektedir. Yani veri setini s\u0131ralanm\u0131\u015f bir bi\u00e7imde ikiye ay\u0131ran de\u011ferdir.\n*     Yine ayn\u0131 mant\u0131kla BloodPressure featuresine ait kay\u0131tlar\u0131n %75 ve daha az\u0131 32 ve a\u015fa\u011f\u0131 de\u011fere sahiptir.\n","14b89ef6":"### Best Estimator","348df6c4":"<a id=3.5><\/a>\n# 3.5. Classification with Support Vector Machine\n1. Linear Support Vector Machine\n       \n        Linear SVM can be used in the linearly distributed sample space. \n        In datasets that are not distributed as truths what will we do?\n        In this case, SVMs cannot draw a linear hyper plane. \"Core Cheats\" are used.\n        The kernel method greatly increases machine learning in the nonlinear machine.\n        -----------------------------------------TR---------------------------------------------------     \n        Do\u011frusal olarak da\u011f\u0131lan \u00f6rneklem uzay\u0131nda Linear SVM kullan\u0131labilir fakat. \n        Do\u011frusalar olarak da\u011f\u0131lmayan veri k\u00fcmelerinde ne yapaca\u011f\u0131z ? \n        Bu durumda SVMler do\u011frusal bir hiper d\u00fczlem \u00e7izemez. Bu nedenle \u00e7ekirdek numaras\u0131 olarak adland\u0131r\u0131lan \"Kernel Trick\" ler kullan\u0131l\u0131r.\n        \u00c7ekirdek y\u00f6ntemi, do\u011frusal olmayan verilerde makine \u00f6\u011frenimini y\u00fcksek oranda art\u0131rmaktad\u0131r.\n\n2.  Kernel Trick (None Linear SVM) <br>\n\n    2.1  (Kernel=Polynomial) Support Vector Machine <br>\n    2.2. (Kernel=Rbf) Support Vector Machine (Gaussian Kernel(Radial Basis Function))<br>\n    2.3. (Kernel=sigmoid) Support Vector Machine <br>\n\n\n* Default Parameters: {C(cost):1.0, kernel:rbf, degree(for poly):3, gamma(for rbf,poly,sigmoid)\n* [For More Details and Reference](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.svm.SVC.html)","09f544b1":"## Polynomial SVM Perfrmance Analysis","f8067d05":"<b> Importance coefficients of features","780c13fd":"### Logistic Regression Model Performance Analysis\n","7f9970fa":"<a id=4.2><\/a>\n## 4.2. Cross Validation\n\n\n- It includes methods used to test the reliability of models. Performs test of model performance.\nThe difference from other methods takes the data set as a whole and the model is tested by separating the data according to certain parameters as training and test, crossing the separated parts.\n                                                                          -  TR -\n\n- Modellerin g\u00fcvenilirli\u011finin s\u0131nanmas\u0131ndan kullan\u0131lan y\u00f6ntemler bar\u0131nd\u0131r\u0131r. Model ba\u015far\u0131m\u0131n\u0131n s\u0131nanmas\u0131n\u0131 ger\u00e7ekle\u015ftirir.\nDi\u011fer y\u00f6ntemlerden fark\u0131 veri setini b\u00fct\u00fcn olarak al\u0131r ve belirli parametrelere g\u00f6re veriyi e\u011fitim ve test olarak ay\u0131rarak, ayr\u0131lan par\u00e7alar\u0131 \u00e7aprazlanarak modelin s\u0131nanmas\u0131 ger\u00e7ekle\u015ftirilir.\n\n\n## 4.3. K-Fold Cv\n\n- It treats the data set as a whole to test the success and divides the data into k equal parts according to the given k parameter. Data corresponding to the divided pieces are randomly assigned from the data set.\n- The first part of K pieces is separated for testing, the rest is reserved for training, the model is tested.\n- In the second stage, the second part is separated for testing and the remaining legs are used for training and the model is tested.\n- This process continues until the last piece and k points are obtained for the model. The average of these scores gives the performance score of the model.\n\n                                                                           - TR -\n                                                                           \n- Ba\u015far\u0131n\u0131n s\u0131nanmas\u0131 i\u00e7in veri k\u00fcmesini bir b\u00fct\u00fc olarak ele al\u0131r ve verilen k parametresine g\u00f6re veriyi k adet e\u015fit par\u00e7aya b\u00f6ler. B\u00f6l\u00fcnen e\u015fit par\u00e7alara denk gelecek veriler, veri seti i\u00e7erisinden rastgele olarak atan\u0131r.\n- K adet par\u00e7an\u0131n s\u0131ras\u0131 ile 1. par\u00e7as\u0131 test i\u00e7in ayr\u0131l\u0131r, gerisi e\u011fitim i\u00e7in ayr\u0131l\u0131r, model test edilir.\n- \u0130kinci a\u015famada 2. par\u00e7as\u0131 test i\u00e7in ayr\u0131l\u0131r kalan pa\u00e7alar e\u011fitim i\u00e7in kullan\u0131l\u0131r ve model test edilir.\n- En sondaki par\u00e7aya kadar bu i\u015flem devam eder ve modele ait k adet puan elde edilir. Bu puanlar\u0131n ortalamas\u0131 modelin ba\u015far\u0131m puan\u0131n\u0131 verir.\n\n![Cross Validation](https:\/\/i.hizliresim.com\/xspPaZ.png)\n\n<a id=4.3><\/a>\n## 4.4.  LOOCV (Leave One Out Cv)\n\n- Each of the data in the data set is separated as an individual test, the remainder is used for training.\nAt each stage, test data is asked to the trained model and scoring occurs.\n- The number of records in the data set is formed and the average of these points gives the performance score of the model.\n\n                                                                            - TR -\n\n- Veri setinde bulunan verilerin her biri tek tek test olarak ayr\u0131l\u0131r, kalanlar e\u011fitim i\u00e7in kullanl\u0131r.\nHer a\u015famada test verisi, e\u011fitilmi\u015f modele sorulur ve puanlamalar olu\u015fur. \n- Veri setindeki kay\u0131t say\u0131s\u0131 kadar puan olu\u015fur ve bu puanlar\u0131n ortalamas\u0131 modelin ba\u015far\u0131m puan\u0131n\u0131 verir.\n\n![LOOCV](https:\/\/i.hizliresim.com\/3v7ohc.png)\n","47f751a1":"<a id=4.9><\/a>\n# 4.9. Parameter Tuning for SVM (kernel=\"poly\")","d8b58971":"### Sigmoid SVM Performance Analysis","246720b5":"<a id=3><\/a>\n# 3. Machine Learning Classifiers and Model Performance Analysis\n","51937bdf":"<a id=3.5.2><\/a>\n## 3.5.2. Polynomial Support Vector Machine\n\n    - gamma parameter default value is 1\/(n_features * x_train.var())\n    - if gamma parameter value is \"auto\", it is calculated as 1\/n_features \n    -Polynomial: (gamma*u'*v + coef0)^degree (using libsvm's nomenclature) Degree is the main parameter here, but you can also vary gamma & coef0 to make the kernel non-symmetric.\n   \n   \n    - Polinom: (gamma * u '* v + coef0) ^ degree (libsvm terminolojisini kullanarak) Derecesi burada ana parametredir ancak \u00e7ekirde\u011fi simetrik olmayan yapmak i\u00e7in gamma & coef0'\u0131 da de\u011fi\u015ftirebilirsiniz.\n\n\n","fd758565":"## 3.2. Knn Model Performance Analysis","c9456190":"### decision tree structure according to best estimator","2195dc00":"<a id=3.2><\/a>\n# 3.2. Classification with KNN (K Nearest Neighborhood)\n\n**KNN Algorithm Steps:**\n1. Determine the K parameter.\n2. Calculate the distance between the point (x_test) to which the class is to be determined and all points (x_train).\n3. Sort the calculated distances and choose the smallest k.\n4. Look at the class labels of the selected k observations, the most repeated class category is the new class category.\n------------------------------------------------------------------------------------------------------------------------\n**KNN Algoritmas\u0131 Ad\u0131mlar\u0131:**\n1. K parametresini belirle.\n2. S\u0131n\u0131f\u0131 belirlenmesi istenen nokta (x_test) ile t\u00fcm noktalar(x_train) aras\u0131ndaki uzakl\u0131\u011f\u0131 hesapla.\n3. Hesaplanan uzakl\u0131klar\u0131 s\u0131rala ve en k\u00fc\u00e7\u00fck k tanesini se\u00e7.\n4. Se\u00e7ilen k tane g\u00f6zlemin s\u0131n\u0131f kategorilerine (class label) bak, en \u00e7ok tekrarlanan s\u0131n\u0131f kategorisi yeni s\u0131n\u0131f kategorisidir.\n\n\n<center>![](https:\/\/res.cloudinary.com\/dyd911kmh\/image\/upload\/f_auto,q_auto:best\/v1531424125\/KNN_final1_ibdm8a.png)<\/center>","14994c98":"<a id=3.1><\/a>\n## 3.1. Confusion Matrix and Classification Report\n    \n* Confusion matrix is used to summarize the examination of the classification algorithm\n* Unlike MSE, MAE, RMAE techniques, detailed error matrix is obtained and analyzed better.\n\n* Well, it is a performance measurement for machine learning classification problem where output can be two or more classes. It is a table with 4 different combinations of predicted and actual values.    \n\n*Kar\u0131\u015f\u0131kl\u0131k Matrisi, herhangi bir s\u0131n\u0131fland\u0131rma algoritmas\u0131n\u0131n performans\u0131n\u0131 \u00f6zetlemek i\u00e7in kullan\u0131lan bir tekniktir.\n*MSE,MAE,RMAE gibi tekniklerden farkl\u0131 olarak detayl\u0131 hata matrisi elde edilir ve daha iyi analiz ger\u00e7ekle\u015ftirilir. \n\n![](https:\/\/i.hizliresim.com\/3c7NdN.png)\n\n\n    True Positive: You predicted positive and it\u2019s true.\n    True Positive: You predicted negative and it\u2019s true.\n    False Positive(Type 1 Error): You predicted positive and it\u2019s false.\n    False Negative(Type 2 Error): You predicted negative and it\u2019s false.\n\n** If 0 negative 1 positive inside **:\n\n<center>![](https:\/\/i.hizliresim.com\/noxyWA.png)<\/center>\n\n**Recall, Precision, Acc?**\n\n- Acc: Accuracy rate\n- Recall: How many were known in the test? (Actually, the rate of correctly identifying those who have Diabetes?)\n- Precision: How many of the existing elements did he know? (How many of our diabetes patients are sick?)\n----------------------------------------------\n- Acc : Do\u011fruluk oran\u0131\n- Recall: Ger\u00e7ekle\u015fen testte, ka\u00e7ta ka\u00e7\u0131 bilindi ?     (Ger\u00e7ekte Diyabet Hasta olanlar\u0131 do\u011fru tespit etme oran\u0131 ?)\n- Precision: Var olan elemanlarda ka\u00e7ta ka\u00e7\u0131n\u0131 bildi ? (Diyabet hastas\u0131 dediklerimizin ger\u00e7ekten ka\u00e7\u0131 hasta ?)\n\n\n**Why use Confusion Matrix ?**\n\n\n    Classification successful high accuracy.\n    It is among the data set. In the dataset,\n    All 99000 cats were correctly estimated but none of the 1000 dogs were correctly predicted, so the accuracy was 99%.\n    But as he seems, he has not guessed any dog correctly!\n\n    S\u0131n\u0131fland\u0131rma sonucunda y\u00fcksek ba\u015far\u0131 oran\u0131 her zaman ba\u015far\u0131l\u0131 bir modeli do\u011frulamaz.\n    Veri seti i\u00e7erisinde verilerin da\u011f\u0131l\u0131m\u0131 da \u00f6nemlidir. \u00d6rne\u011fin kedi ve k\u00f6peklerin s\u0131n\u0131f etiketi oldu\u011fu bir veri setinde,\n    99000 kedinin hepsi do\u011fru tahmin edilmi\u015f fakat 1000 k\u00f6pe\u011fin hi\u00e7biri do\u011fru tahmin edilmemi\u015f bu durumda ba\u015far\u0131 oran\u0131 %99 \u00e7\u0131kabilir.\n    Fakat g\u00f6r\u00fcnd\u00fc\u011f\u00fc gibi hi\u00e7 bir k\u00f6pe\u011fi do\u011fru tahmin edememi\u015f !\n\n<center>![](https:\/\/i.hizliresim.com\/eLEhbN.png)<\/center>\n\n\n**Classification Report**\n\n\n![](https:\/\/i.hizliresim.com\/QRECj7.png)\n\n\n![](https:\/\/i.hizliresim.com\/LRn3AR.png)\n\n\n*  **For Reference : https:\/\/towardsdatascience.com\/understanding-confusion-matrix-a9ad42dcfd62** \n*  **https:\/\/www.udemy.com\/user\/datai-team\/**","dec466c4":"## Result Visualization\n","c68af253":"## Result Visualization","e540f4c4":"<a id=4><\/a>\n# 4. HYPERPARAMETER TUNING and CROSS VALIDATION\n<a id=4.1><\/a>\n## 4.1. Hyper Parameter\n\nIn machine learning algorithms, some parameters are referred to as hyper parameters. These are the parameters that developers should choose the best result oriented. <br>\nIn other words, unlike other parameters, it is not known which of the values \u200b\u200bto be given to these parameters is the most optimal.\n\nTo give an example of hyper parameters:\n     * \"K\" parameter in Knn algorithm.\n     * Alpha parameter in Ridge and Lasso regression.\n     * Coefficients in linear regression (bias, weight)\n     * Learning_rate and number_of_iteration (forward and backward repeats) for Logistic Regression\n     * Max_depth, max_features parameters for Random Forest.\n     * C and Gamma parameters for rbf kernel svm.\n     * Degree parameter for poly kernel svm.\n     * N_estimator, max_depth ... for Random forest\netc...\n\nWhat will be the most optimal, best parameters mentioned for the above algorithms? This problem is a model\nthey are very effective for their performance. <br>\nWith GridSearchCv, all combinations of hyper parameters are implemented and give us the best parameters.\n\n\n\n                                                       TR\n\nMakine \u00f6\u011frenmesi algoritmalar\u0131nda baz\u0131 parametreler hiper parametre (hyper parameter) olarak ge\u00e7er. Bu parametrelerin geli\u015ftiriciler taraf\u0131ndan en iyi sonu\u00e7 odakl\u0131 se\u00e7mesi gereken parametrelerdir.<br>\nYani di\u011fer parametrelerden farkl\u0131 olarak bu parametrelere verilecek de\u011ferlerin en optimali hangisidir bilinmemektedir, geli\u015ftiricilerin deneyerek bulmas\u0131 gerekir.\n\nHiper parametrelere \u00f6rnek verecek olursak:\n     * Knn algoritmas\u0131nda \"k\" parametresi.\n     * Ridge ve Lasso regression'da alpha parametresi.\n     * Do\u011frusal regresyonda katsay\u0131lar (bias,weight)\n     * Logistic Regression i\u00e7in learning_rate ve number_of_iteration (forward ve backward tekrar say\u0131s\u0131)\n     * Random Forest i\u00e7in max_depth, max_features parametreleri.\n     * Rbf kernel svm i\u00e7in C ve Gama parametreleri.\n     * Poly kernel svm i\u00e7in degree parametresi.\n     * Random forest i\u00e7in n_estimator, max_depth...\nvb...\n\nYukar\u0131daki algoritmalar i\u00e7in bahsedilen en optimal, en iyi parametreler neler olacak?. Bu problem bir modelin\nperformans\u0131 i\u00e7in \u00e7ok etkilidirler.<br>\nGridSearchCv ile t\u00fcm hiper parametreleri kombinasyonlar\u0131 ger\u00e7eklenir ve en iyi parametreleri bize verir.\n\n\n\n\n     ","c416f360":"<a id=3.7><\/a>\n# 3.7. Classification with Logistic Regression\n\n- for detailed information : [My Detailed Logistic Regression Kernel](https:\/\/www.kaggle.com\/kursatkaragoz29\/pima-indians-diabetes-db-to-logistic-regression#Data-Reading-and-Data-Pre-Processing)","e2a3c3e3":"    \n    When we examine the data.desribe () output, it is seen that the minimum values \u200b\u200bof each feature are 0.\n    This may be a coincidence, but it is likely that null data is entered as 0. What we need to do is find the number of zeros in each attribute and fill in these 0 values \u200b\u200bwith another value (mean, median, etc.) depending on the number of numbers.\n    \n    Obviously, when we look at the percentages (percentile (25.50)), a large number of 0s appear in feathers.\n    We have to fix them.\n    --------------------------------------------------TR-------------------------------------------\n    data.desribe() \u00e7\u0131kt\u0131s\u0131n\u0131 inceledi\u011fimizde her featurenin minimum de\u011ferlerinin 0 oldu\u011fu g\u00f6r\u00fclmektedir.\n    Bunun bir tesad\u00fcf olmas\u0131 y\u00fcksek olabilece\u011fi gibi null verilerin 0 olarak girilmi\u015f olmas\u0131da muhtemeldir. Yapmam\u0131z gereken i\u015flem her bir \u00f6znitelikteki s\u0131f\u0131r say\u0131s\u0131n\u0131 bulup gerekti\u011fi taktirde, say\u0131n\u0131n \u00e7oklu\u011funa g\u00f6re bu 0 de\u011ferlerini ba\u015fka bir de\u011ferle (mean,median vb.) doldurmam\u0131zd\u0131r.\n    \n    A\u00e7\u0131kcas\u0131 y\u00fczdeliklere bakt\u0131\u011f\u0131m\u0131zda(percentile(25,50)) featurelerde fazla say\u0131da 0 g\u00f6r\u00fcnmektedir.\n    Bunlar\u0131 d\u00fczeltmeliyiz.","8cfff7b1":"<a id=3.6><\/a>\n# 3.6. Classification with Naive Bayes\n\n- The Naive Bayes classification algorithm performs a learning, possibly with the data provided to it.\n- The learning result is for guessing, it calculates according to its values \u200b\u200bas it was created during the learning phase and makes an estimate.\n------------------------------------------------------\n- Naive Bayes s\u0131n\u0131fland\u0131rma algoritmas\u0131, ona sunulan veriler ile olas\u0131l\u0131ksak olarak bir \u00f6\u011frenme ger\u00e7ekle\u015ftirir.\n- \u00d6\u011frenme sonucu, tahmin edilmesi istenen test verilerini \u00f6\u011frenme a\u015famas\u0131nda olu\u015fturmu\u015f oldu\u011fu olas\u0131l\u0131k de\u011ferlerine g\u00f6re hesaplar ve bir tahminde bulunur.\n\n\n![](https:\/\/i.hizliresim.com\/SKvKZl.png)\n![](https:\/\/i.hizliresim.com\/ZJBgPx.png)\n\n\n        Let's consider a data set with deneyim and Maas.\n        Purple colors represent physics, red colors represent mathematics. (6 maths, 5 physics (fiz))\n        Is our problem math or physics that will be added to the sample?\n\n        Calculating as possible:\n        P (Mathematics | X) => probability that the x point is math? required for:\n\n        1. P (Math) => Probability of mathematics (Except Black Point (x)) = 6\/11\n        2. P (X) ==> Probability of points within the similarity range in X = 4\/11 (3 purple + 1 red) \/ total data\n        3. P (X | Math) => Probability of x according to mathematics within similarity range = 3\/6\n\n        P (Mathematics | X) = 3\/6 * 6\/11 \/ 4\/11 = 75%\n        p (Fiz | X) = 1\/5 * 5\/11 \/ 4\/11 = 25%\n\n        Result => Blackhead = Math\n        \n------------------------------------------------------------------------------------------------------------------\n                                    TR\n        Deneyim ve Maas featurelerinden olu\u015fan bir veri setini  d\u00fc\u015f\u00fcnelim.\n        Mor renkler fizik, k\u0131rm\u0131z\u0131 renkler matemati\u011fi temsil eder. ( 6 tane matematik, 5 tane fizik)\n        Problemimiz \u00f6rnekleme yeni kat\u0131lacak olan de\u011fer (siyah nokta) matematik mi yoksa fizik mi ?\n        \n        Olas\u0131l\u0131ksak olarak hesaplanmas\u0131:\n        P(Math|X) => x noktas\u0131n\u0131n matematik olma olas\u0131l\u0131\u011f\u0131 ? i\u00e7in ad\u0131mlar:\n        \n            1. P(Math) => Matemati\u011fin olma olas\u0131l\u0131\u011f\u0131 (Siyah Nokta(x) Hari\u00e7) = 6\/11\n            2. P(X) ==>   X'in belirlenen benzerlik aral\u0131\u011f\u0131 i\u00e7indeki noktalardan olma olas\u0131l\u0131\u011f\u0131  =4\/11    (3 mor + 1 k\u0131rm\u0131z\u0131) \/ toplam data\n            3. P(X|MATH) => Benzerlik aral\u0131\u011f\u0131 i\u00e7inde matemati\u011fe g\u00f6re x'in olma olas\u0131l\u0131\u011f\u0131 = 3\/6   \n            \n            P(Math|X) =  3\/6 * 6\/11  \/ 4\/11  = %75\n            p(Fiz|X)  =  1\/5 * 5\/11  \/ 4\/11  = %25\n            \n            Sonu\u00e7 => Siyah Nokta = Math\n       \n        \n        \n        \n        \n [For Reference: DATAITEAM - Machine Learning](https:\/\/www.udemy.com\/course\/machine-learning-ve-python-adan-zye-makine-ogrenmesi-4\/)\n        \n\n\n\n","0dedcaee":"<a id=3.5.1><\/a>\n## 3.5.1. Linear Support Vector Machine\n\n<center>![SVM](https:\/\/i.hizliresim.com\/ZQX62g.png)<\/center>\n\n- For Reference [Data Iteam-Machine Learning](https:\/\/www.udemy.com\/course\/machine-learning-ve-python-adan-zye-makine-ogrenmesi-4\/)\n\n- Linear SVM is parameterless classifier. Parameter C, as with all kernels, is used only as a loose parameter to determine the imbalance in the classifier error.\n\n- Linear SVM parametresiz bir svm s\u0131n\u0131fland\u0131r\u0131c\u0131s\u0131d\u0131r. C parametresi b\u00fct\u00fcn \u00e7ekirdeklerde oldu\u011fu gibi sadece s\u0131n\u0131fland\u0131r\u0131c\u0131 hatas\u0131ndaki dengesizli\u011fi belirlemek i\u00e7in gev\u015fek parametre olarak kullan\u0131l\u0131r.\n","906aadfd":"<a id=4.4><\/a>\n# 4.4. Parameter Tuning for Knn","10b2bbde":"## 3.5.2. (Polynomial) support vector machine with sklearn","2ad9561e":"## Random Forest Performance Analysis","a32d9b07":"<a id=\"1\"><\/a>\n# INTRODUCTION\n\n- Our aim in this kernel is to estimate whether a patient has diabetes using the data set. By guessing with Logistic Regression, we will reach 0 or 1 class labels.\n\n\n    Bu kenelde, pima-indians-diabetes data seti ile baz\u0131 s\u0131n\u0131fand\u0131rma algoritmalar\u0131 ger\u00e7ekle\u015ftirilmi\u015ftir.\n    Pregnancies: Gebelik Say\u0131s\u0131\n    Glucose: Oral glukoz tolerans testinde glikoz konsantrasyonu de\u011feri.\n    BloogPressure: Kan De\u011feri(mm Hg)\n    SkinThickness: Cilt Kal\u0131nl\u0131\u011f\u0131(mm)\n    Insulin: 2 saatlik serum insulini(mu U\/ml)\n    BMI:  V\u00fccut K\u00fctle indeksi\n    DiabetesPedigreeFunction: Diyabet soya\u011fac\u0131 i\u015flevi\n    Age: Ya\u015f\n    Outcome: Diyabet olup olmamas\u0131 1 veya 0\n    \n<b> Content: <\/b>\n\n1. [Data Reading and Data Pre-Processing](#1) <br>\n    1.1. [Exploratory Data Analysis (EDA)](#1.1) <br>\n    1.2. [Data Cleaning](#1.2) <br> \n    1.3.  [Data Normalization](#1.3) <br>\n2. [Train Test Split Data](#2) <br>\n3. [Machine Learning Classifiers and Model Performance Analysis](#3)<br>\n    3.1. [Confusion Matrix and Classification Report ?](#3.1) <br>\n    3.2. [K Neighhbors Classifier](#3.2) <br>\n    3.3. [Decision Tree Classifier](#3.3) <br>\n    3.4. [Random Forest Classifier](#3.4) <br>\n    3.5. [Support Vector Machine Classifier](#3.5) <br>\n    \n    *3.5.1. [Linear SVM](#3.5.1) <br>\n    *3.5.2. [Polynomial Kernel SVM](#3.5.2) <br>\n    *3.5.3. [Gaussian (Rbf) Kernel SVM](#3.5.3) <br>\n    *3.5.4. [Sigmoid Kernel SVM](#3.5.4)    \n    \n   3.6. [Naive Bayes Classifier](#3.6) <br>\n   3.7. [Logistic Regression Classifier](#3.7) - For More (https:\/\/www.kaggle.com\/kursatkaragoz29\/pima-indians-diabetes-db-to-logistic-regression) <br>\n4. [Hyperparameter Tuning and Cross Validation](#4)<br>\n    4.1. [Hyper Parameter](#4.1)<br>\n    4.2. [Cross Validation - K Fold Cv](#4.2)<br>\n    4.3. [Cross Validation - LOO CV](#4.3)<br>\n    4.4. [Grid Search - CV - KNN](#4.4)<br>\n    4.5. [Grid Search - CV - Decision Tree](#4.5)<br>\n    4.6. [Grid Search - CV - Random Forest](#4.6)<br>\n    4.7. [Grid Search - CV - Linear SVM](#4.7)<br>\n    4.8. [Grid Search - CV - Rbf (Gaussian) SVM](#4.8)<br>\n    4.9. [Grid Search - CV - Polynomial SVM](#4.9)<br>\n    4.10. [Grid Search - CV - Sigmoid SVM](#4.10)<br>\n5. [Conclusion](#5)\n\n","963c48f0":"<a id=3.5.4><\/a>\n## 3.5.4. Sigmoid Support Vector Machine\n\n- Gamma and Coef0 \u00f6nemli rol oynar.\n- Gamma ana parametredir.","741f5d1e":"## Result Visualization","92563f7f":"### Let's understand svm from the visual (Where to use polynomial)"}}