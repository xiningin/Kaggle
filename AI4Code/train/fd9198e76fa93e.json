{"cell_type":{"77404921":"code","76ef790b":"code","967a094c":"code","ea9dfb2e":"code","dd7aac8c":"code","55212ecd":"code","ad51d783":"code","d58a667e":"code","e1f35f9b":"code","1d6f4b98":"code","ab384373":"code","6a7f5238":"code","33d9b7a1":"code","e3b42611":"code","c3e75c1e":"code","c2c39c3c":"code","a7538654":"code","55e480cc":"code","8a29d11f":"code","6f3f0bf1":"code","e3ebd956":"code","2f8f53f2":"code","b02d26de":"code","20e77f8f":"code","ed46593d":"code","5efb58b0":"code","6d95144d":"markdown","dd575bb9":"markdown","a64789c5":"markdown","c45e95a2":"markdown","4097a9a7":"markdown","e456a78d":"markdown","b5606931":"markdown","62a6df4b":"markdown","1da1b735":"markdown","c9dec87f":"markdown","599b4af8":"markdown","4ccacd3a":"markdown","ba003d73":"markdown","d3a0ec85":"markdown","b43a549d":"markdown","fa352c16":"markdown","292c0740":"markdown"},"source":{"77404921":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt","76ef790b":"data=pd.read_csv('..\/input\/creditcardfraud\/creditcard.csv')\ndata.tail(10)","967a094c":"data.shape\nb=data.shape[0]#we need this later","ea9dfb2e":"data.columns\n#data.shape","dd7aac8c":"data.info()","55212ecd":"data.drop_duplicates(subset=data.columns.values[:-1], keep='first',inplace=True)\nprint(b-data.shape[0],\" duplicated Rows has been removed\")","ad51d783":"data.shape","d58a667e":"sns.countplot(x='Class',data=data)","e1f35f9b":"data.Class.value_counts()","1d6f4b98":"g=sns.FacetGrid(data,col='Class')\ng.map(plt.hist,'Time', bins=20)","ab384373":"g=sns.FacetGrid(data,col='Class')\ng.map(plt.hist,'Amount', bins=20)","6a7f5238":"#sns.pairplot(data)","33d9b7a1":"plt.subplots(figsize=(20,20))\nsns.heatmap(data.corr(), vmax=.8 , square=True,annot=True,fmt='.2f')","e3b42611":"data.corr().nlargest(31,'Class')['Class']","c3e75c1e":"def feature_dist(df0,df1,label0,label1,features):\n    plt.figure()\n    fig,ax=plt.subplots(6,5,figsize=(30,45))\n    i=0\n    for ft in features:\n        i+=1\n        plt.subplot(6,5,i)\n        # plt.figure()\n        sns.distplot(df0[ft], hist=False,label=label0)\n        sns.distplot(df1[ft], hist=False,label=label1)\n        plt.xlabel(ft, fontsize=11)\n        #locs, labels = plt.xticks()\n        plt.tick_params(axis='x', labelsize=9)\n        plt.tick_params(axis='y', labelsize=9)\n    plt.show()\n\nt0 = data.loc[data['Class'] == 0]\nt1 = data.loc[data['Class'] == 1]\nfeatures = data.columns.values[:30]\nfeature_dist(t0,t1 ,'Normal', 'Busted', features)","c2c39c3c":"def showboxplot(df,features):\n    melted=[]\n    plt.figure()\n    fig,ax=plt.subplots(5,6,figsize=(30,20))\n    i=0\n    for n in features:\n        melted.insert(i,pd.melt(df,id_vars = \"Class\",value_vars = [n]))\n        i+=1\n    for s in np.arange(1,len(melted)):\n        plt.subplot(5,6,s)\n        sns.boxplot(x = \"variable\", y = \"value\", hue=\"Class\",data= melted[s-1])\n    plt.show()\n\n\nshowboxplot(data,data.columns.values[:-1])\n\n","a7538654":"X=data.drop(['Class'],axis=1)\ny=data['Class']","55e480cc":"from sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split","8a29d11f":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=40, shuffle =True)","6f3f0bf1":"#we combine the train data here for the function of removing outliers \nX_train['Class']=y_train","e3ebd956":"\ndef Remove_Outliers(df,features):\n    \n    \n    Positive_df = df[df[\"Class\"] == 1]#1\n    Negative_df = df[df[\"Class\"] == 0]#0\n    before=df.shape[0]\n\n    for n in features:\n        \n        desc1 = Positive_df[n].describe()\n        lower_bound1 = desc1[4] - 1.5*(desc1[6]-desc1[4])\n        upper_bound1 = desc1[6] + 1.5*(desc1[6]-desc1[4])\n        \n        desc0 = Negative_df[n].describe()\n        lower_bound0 = desc0[4] - 1.5*(desc0[6]-desc0[4])\n        upper_bound0 = desc0[6] + 1.5*(desc0[6]-desc0[4])\n\n        df=df.drop(df[(((df[n]<lower_bound1) | (df[n]>upper_bound1))\n                      &\n                      (df['Class']==1))\n                      |\n                      (((df[n]<lower_bound0) | (df[n]>upper_bound0))\n                      &\n                      (df['Class']== 0))].index)\n\n    after=df.shape[0]\n    print(\"number of deleted outiers :\",before-after)\n    return df\n\n\na=Remove_Outliers(X_train,X_train.columns.values[:-1])\nX_train=a.iloc[:,:-1]\ny_train=a.iloc[:,-1]","2f8f53f2":"def showboxplot(df,features):\n    melted=[]\n    plt.figure()\n    fig,ax=plt.subplots(5,6,figsize=(30,20))\n    i=0\n    for n in features:\n        melted.insert(i,pd.melt(df,id_vars = \"Class\",value_vars = [n]))\n        i+=1\n    #print(melted[29])\n    # print(len(melted))\n    #print(np.arange(len(melted)+1))\n    for s in np.arange(1,len(melted)):\n        plt.subplot(5,6,s)\n        sns.boxplot(x = \"variable\", y = \"value\", hue=\"Class\",data= melted[s-1])\n    plt.show()\n\n\nshowboxplot(a,a.columns.values[:-1])\n\n","b02d26de":"from sklearn.preprocessing import StandardScaler\n\nX_train=StandardScaler().fit_transform(X_train)\nX_test=StandardScaler().fit_transform(X_test)","20e77f8f":"from sklearn.linear_model import LogisticRegression \nfrom sklearn.metrics import accuracy_score, confusion_matrix","ed46593d":"#logistic Regression\n\nlogreg = LogisticRegression()\nlogreg.fit(X_train, y_train)\ny_pred = logreg.predict(X_test)\n\n\nTrain_acc_log = round(logreg.score(X_train, y_train) * 100, 3)\nTest_acc_log = round(logreg.score(X_test, y_test) * 100, 3)\nacc_logreg=round(accuracy_score(y_test, y_pred)*100,3)\n\nprint(\"Score : \",Test_acc_log)\n","5efb58b0":"sns.heatmap(confusion_matrix(y_test , y_pred), center=True,annot=True,fmt='.1f')","6d95144d":"That's huge number it may be half of the data we have but it's okay !\nlet's see now the boxplot again","dd575bb9":"**Let's predict**","a64789c5":"Let's see what target data hide from us .","c45e95a2":"Before we move forward let's see if there any Null variable","4097a9a7":"**Before the trip begin**\n\nIf you like my work, please hit upvote since it will keep me motivated","e456a78d":"check other notebooks here\n[https:\/\/www.kaggle.com\/abdilatifssou\/notebooks](http:\/\/)","b5606931":"That's all.\n\nAgain if you have any question leave it in comment section \n,and dont forget the UPVOTE \nthanks","62a6df4b":"Now let's move to removing outliers section \nbut before let see if there any usng boxplot ","1da1b735":"As u can see a lot of outliers and it will effect our study if we dont remove them \nif you dont know what is an Outlier here's an example explain it :\nA value that \"lies outside\" (is much smaller or larger than) most of the other values in a set of data. For example in the scores 25,29,3,32,85,33,27,28 both 3 and 85 are \"outliers\".\nwe'arnt going to remove them until we split the data so it will be removed just in train data not test data .","c9dec87f":"GOOD! all the dataframe contains non-null and the only categorical features is the target, even that we should check for duplicated rows and remove them to make the data clean. ","599b4af8":"We notice that most of the features are correlated with target data 'Class', so no need to drop any feature.","4ccacd3a":"At this kernel we're going to analyse dataframe we have, and after removing the outliers and duplicated rows and at the end we're going to use LG to predict.","ba003d73":"But before let's not forget to transform the train data and test data to make the algorithm work faster ","d3a0ec85":"Better than before \nGreat!!","b43a549d":"**Visualisation**","fa352c16":"As you see there just a few fraud transaction in our data, it may effect the result of the accuracy but we are going to avoid that as much as we can.","292c0740":"Here is a function which show us every feature's distribution "}}