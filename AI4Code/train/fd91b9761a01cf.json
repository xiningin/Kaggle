{"cell_type":{"5817b187":"code","6291f950":"code","3c542356":"code","eacd6996":"code","5d3dbe6b":"code","0b45ddeb":"code","f4ab77a5":"code","bcf7d54d":"code","15a2548a":"code","8e59b6c4":"code","ec2c5efb":"code","da7d1c2e":"code","74c7eb45":"code","21ff26ef":"code","f7429dca":"code","265b3b32":"code","1d78b3d2":"code","2499a105":"code","f8f30024":"code","4a29743b":"code","96f6c424":"code","31658239":"code","3cfcb491":"code","5898de16":"code","3af4e052":"code","9e6e9e66":"code","a400f1a3":"code","db658a99":"code","3f57cf80":"code","a635bd93":"code","c212963e":"code","cdb4d542":"code","1aa48d0e":"code","b566921c":"code","f44ee4b4":"code","2021d49d":"code","f8805da6":"code","5cb09ae8":"code","7f8f8af9":"code","c9e0293f":"code","30601040":"code","e5ce8a1f":"code","547607bb":"code","8836e5c0":"code","79faadc6":"code","c76874c2":"code","8d16f431":"code","8f974edf":"code","57d4ccd5":"code","763dea73":"code","bdaf4675":"code","546a1183":"code","4dd67bb4":"code","4940bb0c":"code","75cfd9bb":"code","ce7a7f03":"code","f04f558e":"code","e03c06bd":"code","3c8a02b5":"code","186462c8":"code","7cf1dc1a":"code","8b4b8c70":"code","7b2e1797":"code","b8459880":"code","713608fb":"code","0636e7c7":"code","e5a67dfd":"code","0a7927b1":"code","f2ab8ed5":"code","f8e4709e":"code","e0ccc1ff":"code","15923dd3":"code","db07815d":"code","9dda799c":"markdown","4148fffc":"markdown","3196228d":"markdown","762454d4":"markdown","da2aa7a2":"markdown","756818bf":"markdown","92ff3caa":"markdown","29a175b5":"markdown","f7e47745":"markdown","b67e41f4":"markdown","8a0841e5":"markdown","3a7f820d":"markdown","d916425a":"markdown","3da51431":"markdown","f0c24e72":"markdown","c97614a5":"markdown","83120b47":"markdown","9fcef68a":"markdown","9eb677e1":"markdown","2912fc95":"markdown","14b4286d":"markdown","698eedbb":"markdown","3701e683":"markdown","97cf24d9":"markdown","6b022d1f":"markdown","2b240f47":"markdown","839c6a63":"markdown","5573c2f9":"markdown","9dc6d057":"markdown","be7cba43":"markdown","d8524ac5":"markdown"},"source":{"5817b187":"# Data Analysis\nimport pandas as pd\nimport numpy as np\n\n# Data Visualization\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport yellowbrick\nfrom yellowbrick.model_selection import FeatureImportances\nfrom yellowbrick.features import RadViz\n\n\n# Text Processing\nimport re\nimport itertools\nimport spacy\nimport string\nfrom spacy.lang.en import English\nfrom spacy.lang.en.stop_words import STOP_WORDS\nimport en_core_web_sm\nfrom collections import Counter\n\n# Machine Learning packages\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nimport umap\nimport sklearn.cluster as cluster\n\n# Ignore noise warning\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport pickle as pkl\nfrom scipy import sparse\nfrom numpy import asarray\nfrom numpy import savetxt\n\n# Fix imbalance\nfrom imblearn.under_sampling import InstanceHardnessThreshold\n\n# Model training and evaluation\nfrom sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold, cross_val_score\n\n#Metrics\nfrom sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error, accuracy_score, balanced_accuracy_score\nfrom sklearn.metrics import precision_score, recall_score, f1_score, multilabel_confusion_matrix, confusion_matrix\nfrom sklearn.metrics import classification_report\n#Models\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\nfrom sklearn.neural_network import MLPClassifier","6291f950":"result_svd_vec_types  = pd.read_csv(\"..\/input\/2-mbti-preprocessing\/result_svd_vec_types.csv\")\nresult_svd_vec_types.drop([\"Unnamed: 0\"], axis=1, inplace=True)","3c542356":"result_svd_vec_types.head()","eacd6996":"result_svd_vec_types.shape","5d3dbe6b":"X = result_svd_vec_types.drop([\"type\",\"enfj\", \"enfp\", \"entj\", \"entp\", \"esfj\", \"esfp\", \"estj\", \"estp\",\"infj\", \"infp\", \"intj\",\n                               \"intp\", \"isfj\", \"isfp\", \"istj\", \"istp\"], axis=1).values\ny = result_svd_vec_types[\"type\"].values","0b45ddeb":"X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, test_size=0.2)\nprint ((X_train.shape),(y_train.shape),(X_test.shape),(y_test.shape))","f4ab77a5":"def baseline_report(model, X_train, X_test, y_train, y_test, name):\n    strat_k_fold = StratifiedKFold(n_splits=5, shuffle=True)\n    model.fit(X_train, y_train)\n    accuracy     = np.mean(cross_val_score(model, X_train, y_train, cv=strat_k_fold, scoring='accuracy', n_jobs=-1))\n    precision    = np.mean(cross_val_score(model, X_train, y_train, cv=strat_k_fold, scoring='precision_weighted', n_jobs=-1))\n    recall       = np.mean(cross_val_score(model, X_train, y_train, cv=strat_k_fold, scoring='recall_weighted', n_jobs=-1))\n    f1score      = np.mean(cross_val_score(model, X_train, y_train, cv=strat_k_fold, scoring='f1_weighted', n_jobs=-1))\n    y_pred = model.predict(X_test)\n    mcm = multilabel_confusion_matrix(y_test, y_pred)\n    tn = mcm[:, 0, 0]\n    tp = mcm[:, 1, 1]\n    fn = mcm[:, 1, 0]\n    fp = mcm[:, 0, 1]\n    specificities = tn \/ (tn+fp)\n    specificity = (specificities.sum())\/ 16\n\n    df_model = pd.DataFrame({'model'        : [name],\n                             'accuracy'     : [accuracy],\n                             'precision'    : [precision],\n                             'recall'       : [recall],\n                             'f1score'      : [f1score],\n                             'specificity'  : [specificity]\n                            })   \n    return df_model","bcf7d54d":"models = {'gnb': GaussianNB(),\n          'logit': LogisticRegression(),\n          'knn': KNeighborsClassifier(),\n          'decisiontree': DecisionTreeClassifier(),\n          'randomforest': RandomForestClassifier(),\n          'xgboost': GradientBoostingClassifier(),\n          'MLPC': MLPClassifier()\n         }","15a2548a":"#raise SystemExit(\"Here it comes a very consuming memory process that takes about 45 minutes\")","8e59b6c4":"# Evaluation of models\nmodels_svd = pd.concat([baseline_report(model, X_train, X_test, y_train, y_test, name) for (name, model) in models.items()])\nmodels_svd.sort_values(by='f1score', axis=0, ascending=False, inplace=True)\nmodels_svd.to_csv(\"models_svd.csv\")\nmodels_svd","ec2c5efb":"xgboost = GradientBoostingClassifier().fit(X_train, y_train)\n\nfig, ax = plt.subplots(figsize=(10,20))\nviz = FeatureImportances(xgboost)\nviz.fit(X, y)\nviz.show()\nviz.show(outpath=\"feature_importance_types.png\")\nsns.set_context(\"talk\")\nplt.show()","da7d1c2e":"def sampling_k_elements(group, k=39):\n    if len(group) < k:\n        return group\n    return group.sample(k)\n\nbalanced_svd = result_svd_vec_types.groupby(\"type\").apply(sampling_k_elements).reset_index(drop=True)","74c7eb45":"X = balanced_svd.drop([\"type\",\"enfj\", \"enfp\", \"entj\", \"entp\", \"esfj\", \"esfp\", \"estj\", \"estp\",\"infj\", \"infp\", \"intj\",\n                               \"intp\", \"isfj\", \"isfp\", \"istj\", \"istp\"], axis=1).values\ny = balanced_svd[\"type\"].values","21ff26ef":"X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, test_size=0.2)\nprint ((X_train.shape),(y_train.shape),(X_test.shape),(y_test.shape))","f7429dca":"def baseline_report(model, X_train, X_test, y_train, y_test, name):\n    strat_k_fold = StratifiedKFold(n_splits=5, shuffle=True)\n    model.fit(X_train, y_train)\n    accuracy     = np.mean(cross_val_score(model, X_train, y_train, cv=strat_k_fold, scoring='accuracy', n_jobs=-1))\n    precision    = np.mean(cross_val_score(model, X_train, y_train, cv=strat_k_fold, scoring='precision_weighted', n_jobs=-1))\n    recall       = np.mean(cross_val_score(model, X_train, y_train, cv=strat_k_fold, scoring='recall_weighted', n_jobs=-1))\n    f1score      = np.mean(cross_val_score(model, X_train, y_train, cv=strat_k_fold, scoring='f1_weighted', n_jobs=-1))\n    y_pred = model.predict(X_test)\n    mcm = multilabel_confusion_matrix(y_test, y_pred)\n    tn = mcm[:, 0, 0]\n    tp = mcm[:, 1, 1]\n    fn = mcm[:, 1, 0]\n    fp = mcm[:, 0, 1]\n    specificities = tn \/ (tn+fp)\n    specificity = (specificities.sum())\/ 16\n\n    df_model = pd.DataFrame({'model'        : [name],\n                             'accuracy'     : [accuracy],\n                             'precision'    : [precision],\n                             'recall'       : [recall],\n                             'f1score'      : [f1score],\n                             'specificity': [specificity]\n                            })   \n    return df_model","265b3b32":"models = {'gnb': GaussianNB(),\n          'logit': LogisticRegression(),\n          'knn': KNeighborsClassifier(),\n          'decisiontree': DecisionTreeClassifier(),\n          'randomforest': RandomForestClassifier(),\n          'xgboost': GradientBoostingClassifier(),\n          'MLPC': MLPClassifier()\n         }","1d78b3d2":"# Evaluation of models\nmodels_svd_resampled = pd.concat([baseline_report(model, X_train, X_test, y_train, y_test, name) for (name, model) in models.items()])\nmodels_svd_resampled.sort_values(by='f1score', axis=0, ascending=False, inplace=True)\nmodels_svd_resampled.to_csv(\"models_svd_resampled.csv\")\nmodels_svd_resampled","2499a105":"result_umap_types  = pd.read_csv(\"..\/input\/2-mbti-preprocessing\/result_umap_types.csv\")\nresult_umap_types.drop([\"Unnamed: 0\"], axis=1, inplace=True)","f8f30024":"result_umap_types.head()","4a29743b":"result_umap_types.shape","96f6c424":"X = result_umap_types.drop([\"type\",\"enfj\", \"enfp\", \"entj\", \"entp\", \"esfj\", \"esfp\", \"estj\", \"estp\",\"infj\", \"infp\", \"intj\",\n                               \"intp\", \"isfj\", \"isfp\", \"istj\", \"istp\"], axis=1).values\ny = result_umap_types[\"type\"].values","31658239":"X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, test_size=0.2)\nprint ((X_train.shape),(y_train.shape),(X_test.shape),(y_test.shape))","3cfcb491":"def baseline_report(model, X_train, X_test, y_train, y_test, name):\n    strat_k_fold = StratifiedKFold(n_splits=5, shuffle=True)\n    model.fit(X_train, y_train)\n    accuracy     = np.mean(cross_val_score(model, X_train, y_train, cv=strat_k_fold, scoring='accuracy', n_jobs=-1))\n    precision    = np.mean(cross_val_score(model, X_train, y_train, cv=strat_k_fold, scoring='precision_weighted', n_jobs=-1))\n    recall       = np.mean(cross_val_score(model, X_train, y_train, cv=strat_k_fold, scoring='recall_weighted', n_jobs=-1))\n    f1score      = np.mean(cross_val_score(model, X_train, y_train, cv=strat_k_fold, scoring='f1_weighted', n_jobs=-1))\n    y_pred = model.predict(X_test)\n    mcm = multilabel_confusion_matrix(y_test, y_pred)\n    tn = mcm[:, 0, 0]\n    tp = mcm[:, 1, 1]\n    fn = mcm[:, 1, 0]\n    fp = mcm[:, 0, 1]\n    specificities = tn \/ (tn+fp)\n    specificity = (specificities.sum())\/ 16\n\n    df_model = pd.DataFrame({'model'        : [name],\n                             'accuracy'     : [accuracy],\n                             'precision'    : [precision],\n                             'recall'       : [recall],\n                             'f1score'      : [f1score],\n                             'specificity': [specificity]\n                            })   \n    return df_model","5898de16":"models = {'gnb': GaussianNB(),\n          'logit': LogisticRegression(),\n          'knn': KNeighborsClassifier(),\n          'decisiontree': DecisionTreeClassifier(),\n          'randomforest': RandomForestClassifier(),\n          'xgboost': GradientBoostingClassifier(),\n          'MLPC': MLPClassifier()\n         }","3af4e052":"# Evaluation of models\nmodels_umap = pd.concat([baseline_report(model, X_train, X_test, y_train, y_test, name) for (name, model) in models.items()])\nmodels_umap.sort_values(by='f1score', axis=0, ascending=False, inplace=True)\nmodels_umap.to_csv(\"models_umap.csv\")\nmodels_umap","9e6e9e66":"def sampling_k_elements(group, k=39):\n    if len(group) < k:\n        return group\n    return group.sample(k)\n\nbalanced_umap = result_umap_types.groupby(\"type\").apply(sampling_k_elements).reset_index(drop=True)","a400f1a3":"X = balanced_umap.drop([\"type\",\"enfj\", \"enfp\", \"entj\", \"entp\", \"esfj\", \"esfp\", \"estj\", \"estp\",\"infj\", \"infp\", \"intj\",\n                               \"intp\", \"isfj\", \"isfp\", \"istj\", \"istp\"], axis=1).values\ny = balanced_umap[\"type\"].values","db658a99":"X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, test_size=0.2)\nprint ((X_train.shape),(y_train.shape),(X_test.shape),(y_test.shape))","3f57cf80":"def baseline_report(model, X_train, X_test, y_train, y_test, name):\n    strat_k_fold = StratifiedKFold(n_splits=5, shuffle=True)\n    model.fit(X_train, y_train)\n    accuracy     = np.mean(cross_val_score(model, X_train, y_train, cv=strat_k_fold, scoring='accuracy', n_jobs=-1))\n    precision    = np.mean(cross_val_score(model, X_train, y_train, cv=strat_k_fold, scoring='precision_weighted', n_jobs=-1))\n    recall       = np.mean(cross_val_score(model, X_train, y_train, cv=strat_k_fold, scoring='recall_weighted', n_jobs=-1))\n    f1score      = np.mean(cross_val_score(model, X_train, y_train, cv=strat_k_fold, scoring='f1_weighted', n_jobs=-1))\n    y_pred = model.predict(X_test)\n    mcm = multilabel_confusion_matrix(y_test, y_pred)\n    tn = mcm[:, 0, 0]\n    tp = mcm[:, 1, 1]\n    fn = mcm[:, 1, 0]\n    fp = mcm[:, 0, 1]\n    specificities = tn \/ (tn+fp)\n    specificity = (specificities.sum())\/ 16\n\n    df_model = pd.DataFrame({'model'        : [name],\n                             'accuracy'     : [accuracy],\n                             'precision'    : [precision],\n                             'recall'       : [recall],\n                             'f1score'      : [f1score],\n                             'specificity': [specificity]\n                            })   \n    return df_model","a635bd93":"models = {'gnb': GaussianNB(),\n          'logit': LogisticRegression(),\n          'knn': KNeighborsClassifier(),\n          'decisiontree': DecisionTreeClassifier(),\n          'randomforest': RandomForestClassifier(),\n          'xgboost': GradientBoostingClassifier(),\n          'MLPC': MLPClassifier()\n         }","c212963e":"# Evaluation of models\nmodels_umap_resampled = pd.concat([baseline_report(model, X_train, X_test, y_train, y_test, name) for (name, model) in models.items()])\nmodels_umap_resampled.sort_values(by='f1score', axis=0, ascending=False, inplace=True)\nmodels_umap_resampled.to_csv(\"models_umap_resampled.csv\")\nmodels_umap_resampled","cdb4d542":"result_umap_svd_types  = pd.read_csv(\"..\/input\/2-mbti-preprocessing\/result_umap_svd_types.csv\")\nresult_umap_svd_types.drop([\"Unnamed: 0\"], axis=1, inplace=True)","1aa48d0e":"result_umap_svd_types.head()","b566921c":"result_umap_svd_types.shape","f44ee4b4":"X = result_umap_svd_types.drop([\"type\",\"enfj\", \"enfp\", \"entj\", \"entp\", \"esfj\", \"esfp\", \"estj\", \"estp\",\"infj\", \"infp\", \"intj\",\n                               \"intp\", \"isfj\", \"isfp\", \"istj\", \"istp\"], axis=1).values\ny = result_umap_svd_types[\"type\"].values","2021d49d":"X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, test_size=0.2)\nprint ((X_train.shape),(y_train.shape),(X_test.shape),(y_test.shape))","f8805da6":"def baseline_report(model, X_train, X_test, y_train, y_test, name):\n    strat_k_fold = StratifiedKFold(n_splits=5, shuffle=True)\n    model.fit(X_train, y_train)\n    accuracy     = np.mean(cross_val_score(model, X_train, y_train, cv=strat_k_fold, scoring='accuracy', n_jobs=-1))\n    precision    = np.mean(cross_val_score(model, X_train, y_train, cv=strat_k_fold, scoring='precision_weighted', n_jobs=-1))\n    recall       = np.mean(cross_val_score(model, X_train, y_train, cv=strat_k_fold, scoring='recall_weighted', n_jobs=-1))\n    f1score      = np.mean(cross_val_score(model, X_train, y_train, cv=strat_k_fold, scoring='f1_weighted', n_jobs=-1))\n    y_pred = model.predict(X_test)\n    mcm = multilabel_confusion_matrix(y_test, y_pred)\n    tn = mcm[:, 0, 0]\n    tp = mcm[:, 1, 1]\n    fn = mcm[:, 1, 0]\n    fp = mcm[:, 0, 1]\n    specificities = tn \/ (tn+fp)\n    specificity = (specificities.sum())\/ 16\n\n    df_model = pd.DataFrame({'model'        : [name],\n                             'accuracy'     : [accuracy],\n                             'precision'    : [precision],\n                             'recall'       : [recall],\n                             'f1score'      : [f1score],\n                             'specificity': [specificity]\n                            })   \n    return df_model","5cb09ae8":"models = {'gnb': GaussianNB(),\n          'logit': LogisticRegression(),\n          'knn': KNeighborsClassifier(),\n          'decisiontree': DecisionTreeClassifier(),\n          'randomforest': RandomForestClassifier(),\n          'xgboost': GradientBoostingClassifier(),\n          'MLPC': MLPClassifier()\n         }","7f8f8af9":"# Evaluation of models\nmodels_umap_svd = pd.concat([baseline_report(model, X_train, X_test, y_train, y_test, name) for (name, model) in models.items()])\nmodels_umap_svd.sort_values(by='f1score', axis=0, ascending=False, inplace=True)\nmodels_umap_svd.to_csv(\"models_umap_svd.csv\")\nmodels_umap_svd","c9e0293f":"def sampling_k_elements(group, k=39):\n    if len(group) < k:\n        return group\n    return group.sample(k)\n\nbalanced_umap_svd = result_umap_svd_types.groupby(\"type\").apply(sampling_k_elements).reset_index(drop=True)","30601040":"X = balanced_umap_svd.drop([\"type\",\"enfj\", \"enfp\", \"entj\", \"entp\", \"esfj\", \"esfp\", \"estj\", \"estp\",\"infj\", \"infp\", \"intj\",\n                               \"intp\", \"isfj\", \"isfp\", \"istj\", \"istp\"], axis=1).values\ny = balanced_umap_svd[\"type\"].values","e5ce8a1f":"X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, test_size=0.2)\nprint ((X_train.shape),(y_train.shape),(X_test.shape),(y_test.shape))","547607bb":"def baseline_report(model, X_train, X_test, y_train, y_test, name):\n    strat_k_fold = StratifiedKFold(n_splits=5, shuffle=True)\n    model.fit(X_train, y_train)\n    accuracy     = np.mean(cross_val_score(model, X_train, y_train, cv=strat_k_fold, scoring='accuracy', n_jobs=-1))\n    precision    = np.mean(cross_val_score(model, X_train, y_train, cv=strat_k_fold, scoring='precision_weighted', n_jobs=-1))\n    recall       = np.mean(cross_val_score(model, X_train, y_train, cv=strat_k_fold, scoring='recall_weighted', n_jobs=-1))\n    f1score      = np.mean(cross_val_score(model, X_train, y_train, cv=strat_k_fold, scoring='f1_weighted', n_jobs=-1))\n    y_pred = model.predict(X_test)\n    mcm = multilabel_confusion_matrix(y_test, y_pred)\n    tn = mcm[:, 0, 0]\n    tp = mcm[:, 1, 1]\n    fn = mcm[:, 1, 0]\n    fp = mcm[:, 0, 1]\n    specificities = tn \/ (tn+fp)\n    specificity = (specificities.sum())\/ 16\n\n    df_model = pd.DataFrame({'model'        : [name],\n                             'accuracy'     : [accuracy],\n                             'precision'    : [precision],\n                             'recall'       : [recall],\n                             'f1score'      : [f1score],\n                             'specificity': [specificity]\n                            })   \n    return df_model","8836e5c0":"models = {'gnb': GaussianNB(),\n          'logit': LogisticRegression(),\n          'knn': KNeighborsClassifier(),\n          'decisiontree': DecisionTreeClassifier(),\n          'randomforest': RandomForestClassifier(),\n          'xgboost': GradientBoostingClassifier(),\n          'MLPC': MLPClassifier()\n         }","79faadc6":"# Evaluation of models\nmodels_umap_svd_resampled = pd.concat([baseline_report(model, X_train, X_test, y_train, y_test, name) for (name, model) in models.items()])\nmodels_umap_svd_resampled.sort_values(by='f1score', axis=0, ascending=False, inplace=True)\nmodels_umap_svd_resampled.to_csv(\"models_umap_svd_resampled.csv\")\nmodels_umap_svd_resampled","c76874c2":"result_svd_vec_dimensions  = pd.read_csv(\"..\/input\/2-mbti-preprocessing\/result_svd_vec_dimensions.csv\")\nresult_svd_vec_dimensions.drop([\"Unnamed: 0\"], axis=1, inplace=True)","8d16f431":"result_svd_vec_dimensions.head()","8f974edf":"result_svd_vec_dimensions.shape","57d4ccd5":"X = result_svd_vec_dimensions.drop([\"type\",\"i-e\", \"n-s\", \"t-f\", \"j-p\"], axis=1).values\ny = result_svd_vec_dimensions[\"i-e\"].values","763dea73":"X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, test_size=0.2)\nprint ((X_train.shape),(y_train.shape),(X_test.shape),(y_test.shape))","bdaf4675":"def baseline_report(model, X_train, X_test, y_train, y_test, name):\n    strat_k_fold = StratifiedKFold(n_splits=5, shuffle=True)\n    model.fit(X_train, y_train)\n    accuracy     = np.mean(cross_val_score(model, X_train, y_train, cv=strat_k_fold, scoring='accuracy', n_jobs=-1))\n    precision    = np.mean(cross_val_score(model, X_train, y_train, cv=strat_k_fold, scoring='precision_weighted', n_jobs=-1))\n    recall       = np.mean(cross_val_score(model, X_train, y_train, cv=strat_k_fold, scoring='recall_weighted', n_jobs=-1))\n    f1score      = np.mean(cross_val_score(model, X_train, y_train, cv=strat_k_fold, scoring='f1_weighted', n_jobs=-1))\n    y_pred = model.predict(X_test)\n    tn, fp, fn, tp = confusion_matrix(y_pred, y_test).ravel()\n    specificity = tn \/ (tn+fp)\n\n    df_model = pd.DataFrame({'model'        : [name],\n                             'accuracy'     : [accuracy],\n                             'precision'    : [precision],\n                             'recall'       : [recall],\n                             'f1score'      : [f1score],\n                             'specificity'  : [specificity]\n                            })   \n    return df_model","546a1183":"models = {'gnb': GaussianNB(),\n          'randomforest': RandomForestClassifier(),\n          'xgboost': GradientBoostingClassifier(),\n          'MLPC': MLPClassifier()\n         }","4dd67bb4":"# Evaluation of models\nmodels_ie = pd.concat([baseline_report(model, X_train, X_test, y_train, y_test, name) for (name, model) in models.items()])\nmodels_ie.sort_values(by='f1score', axis=0, ascending=False, inplace=True)\nmodels_ie.to_csv(\"models_ie.csv\")\nmodels_ie","4940bb0c":"xgboost = GradientBoostingClassifier().fit(X_train, y_train)\n\nfig, ax = plt.subplots(figsize=(10,20))\nviz = FeatureImportances(xgboost)\nviz.fit(X, y)\nviz.show()\nviz.show(outpath=\"feature_importance_i-e.png\")\nsns.set_context(\"talk\")\nplt.show()","75cfd9bb":"X = result_svd_vec_dimensions.drop([\"type\",\"i-e\", \"n-s\", \"t-f\", \"j-p\"], axis=1).values\ny = result_svd_vec_dimensions[\"n-s\"].values","ce7a7f03":"X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, test_size=0.2)\nprint ((X_train.shape),(y_train.shape),(X_test.shape),(y_test.shape))","f04f558e":"def baseline_report(model, X_train, X_test, y_train, y_test, name):\n    strat_k_fold = StratifiedKFold(n_splits=5, shuffle=True)\n    model.fit(X_train, y_train)\n    accuracy     = np.mean(cross_val_score(model, X_train, y_train, cv=strat_k_fold, scoring='accuracy', n_jobs=-1))\n    precision    = np.mean(cross_val_score(model, X_train, y_train, cv=strat_k_fold, scoring='precision_weighted', n_jobs=-1))\n    recall       = np.mean(cross_val_score(model, X_train, y_train, cv=strat_k_fold, scoring='recall_weighted', n_jobs=-1))\n    f1score      = np.mean(cross_val_score(model, X_train, y_train, cv=strat_k_fold, scoring='f1_weighted', n_jobs=-1))\n    y_pred = model.predict(X_test)\n    tn, fp, fn, tp = confusion_matrix(y_pred, y_test).ravel()\n    specificity = tn \/ (tn+fp)\n\n    df_model = pd.DataFrame({'model'        : [name],\n                             'accuracy'     : [accuracy],\n                             'precision'    : [precision],\n                             'recall'       : [recall],\n                             'f1score'      : [f1score],\n                             'specificity'  : [specificity]\n                            })   \n    return df_model","e03c06bd":"models = {'gnb': GaussianNB(),\n          'randomforest': RandomForestClassifier(),\n          'xgboost': GradientBoostingClassifier(),\n          'MLPC': MLPClassifier()\n         }","3c8a02b5":"# Evaluation of models\nmodels_ns = pd.concat([baseline_report(model, X_train, X_test, y_train, y_test, name) for (name, model) in models.items()])\nmodels_ns.sort_values(by='f1score', axis=0, ascending=False, inplace=True)\nmodels_ns.to_csv(\"models_ns.csv\")\nmodels_ns","186462c8":"xgboost = GradientBoostingClassifier().fit(X_train, y_train)\n\nfig, ax = plt.subplots(figsize=(10,20))\nviz = FeatureImportances(xgboost)\nviz.fit(X, y)\nviz.show()\nviz.show(outpath=\"feature_importance_n-s.png\")\nsns.set_context(\"talk\")\nplt.show()","7cf1dc1a":"X = result_svd_vec_dimensions.drop([\"type\",\"i-e\", \"n-s\", \"t-f\", \"j-p\"], axis=1).values\ny = result_svd_vec_dimensions[\"t-f\"].values","8b4b8c70":"X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, test_size=0.2)\nprint ((X_train.shape),(y_train.shape),(X_test.shape),(y_test.shape))","7b2e1797":"def baseline_report(model, X_train, X_test, y_train, y_test, name):\n    strat_k_fold = StratifiedKFold(n_splits=5, shuffle=True)\n    model.fit(X_train, y_train)\n    accuracy     = np.mean(cross_val_score(model, X_train, y_train, cv=strat_k_fold, scoring='accuracy', n_jobs=-1))\n    precision    = np.mean(cross_val_score(model, X_train, y_train, cv=strat_k_fold, scoring='precision_weighted', n_jobs=-1))\n    recall       = np.mean(cross_val_score(model, X_train, y_train, cv=strat_k_fold, scoring='recall_weighted', n_jobs=-1))\n    f1score      = np.mean(cross_val_score(model, X_train, y_train, cv=strat_k_fold, scoring='f1_weighted', n_jobs=-1))\n    y_pred = model.predict(X_test)\n    tn, fp, fn, tp = confusion_matrix(y_pred, y_test).ravel()\n    specificity = tn \/ (tn+fp)\n\n    df_model = pd.DataFrame({'model'        : [name],\n                             'accuracy'     : [accuracy],\n                             'precision'    : [precision],\n                             'recall'       : [recall],\n                             'f1score'      : [f1score],\n                             'specificity'  : [specificity]\n                            })   \n    return df_model","b8459880":"models = {'gnb': GaussianNB(),\n          'randomforest': RandomForestClassifier(),\n          'xgboost': GradientBoostingClassifier(),\n          'MLPC': MLPClassifier()\n         }","713608fb":"# Evaluation of models\nmodels_tf = pd.concat([baseline_report(model, X_train, X_test, y_train, y_test, name) for (name, model) in models.items()])\nmodels_tf.sort_values(by='f1score', axis=0, ascending=False, inplace=True)\nmodels_tf.to_csv(\"models_tf.csv\")\nmodels_tf","0636e7c7":"xgboost = GradientBoostingClassifier().fit(X_train, y_train)\n\nfig, ax = plt.subplots(figsize=(10,20))\nviz = FeatureImportances(xgboost)\nviz.fit(X, y)\nviz.show()\nviz.show(outpath=\"feature_importance_t-f.png\")\nsns.set_context(\"talk\")\nplt.show()","e5a67dfd":"X = result_svd_vec_dimensions.drop([\"type\",\"i-e\", \"n-s\", \"t-f\", \"j-p\"], axis=1).values\ny = result_svd_vec_dimensions[\"j-p\"].values","0a7927b1":"X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, test_size=0.2)\nprint ((X_train.shape),(y_train.shape),(X_test.shape),(y_test.shape))","f2ab8ed5":"def baseline_report(model, X_train, X_test, y_train, y_test, name):\n    strat_k_fold = StratifiedKFold(n_splits=5, shuffle=True)\n    model.fit(X_train, y_train)\n    accuracy     = np.mean(cross_val_score(model, X_train, y_train, cv=strat_k_fold, scoring='accuracy', n_jobs=-1))\n    precision    = np.mean(cross_val_score(model, X_train, y_train, cv=strat_k_fold, scoring='precision_weighted', n_jobs=-1))\n    recall       = np.mean(cross_val_score(model, X_train, y_train, cv=strat_k_fold, scoring='recall_weighted', n_jobs=-1))\n    f1score      = np.mean(cross_val_score(model, X_train, y_train, cv=strat_k_fold, scoring='f1_weighted', n_jobs=-1))\n    y_pred = model.predict(X_test)\n    tn, fp, fn, tp = confusion_matrix(y_pred, y_test).ravel()\n    specificity = tn \/ (tn+fp)\n\n    df_model = pd.DataFrame({'model'        : [name],\n                             'accuracy'     : [accuracy],\n                             'precision'    : [precision],\n                             'recall'       : [recall],\n                             'f1score'      : [f1score],\n                             'specificity'  : [specificity]\n                            })   \n    return df_model","f8e4709e":"models = {'gnb': GaussianNB(),\n          'randomforest': RandomForestClassifier(),\n          'xgboost': GradientBoostingClassifier(),\n          'MLPC': MLPClassifier()\n         }","e0ccc1ff":"# Evaluation of models\nmodels_jp = pd.concat([baseline_report(model, X_train, X_test, y_train, y_test, name) for (name, model) in models.items()])\nmodels_jp.sort_values(by='f1score', axis=0, ascending=False, inplace=True)\nmodels_jp.to_csv(\"models_jp.csv\")\nmodels_jp","15923dd3":"xgboost = GradientBoostingClassifier().fit(X_train, y_train)\n\nfig, ax = plt.subplots(figsize=(10,20))\nviz = FeatureImportances(xgboost)\nviz.fit(X, y)\nviz.show()\nviz.show(outpath=\"feature_importance_j-p.png\")\nsns.set_context(\"talk\")\nplt.show()","db07815d":"dimensions = models_ie.iloc[0,4] * models_ns.iloc[0,4] * models_tf.iloc[0,4] * models_jp.iloc[0,4]\ntypes = models_svd.iloc[0,4]\n\nprint(\"F1 Scores:\")\nprint(\"Types =\", types,\"vs\",\"Dimensions =\", dimensions)","9dda799c":"<img src=\"https:\/\/bit.ly\/2VnXWr2\" width=\"100\" align=\"left\">","4148fffc":"##### original sample","3196228d":"## Imports","762454d4":"###### Feature importance","da2aa7a2":"<img src=\"https:\/\/www.nicepng.com\/png\/detail\/148-1486992_discover-the-most-powerful-ways-to-automate-your.png\" width=\"1000\"> ","756818bf":"##### resampled","92ff3caa":"###### Feature importance","29a175b5":"##### resampled","f7e47745":"# Final project: NLP to predict Myers-Briggs Personality Type","b67e41f4":"##### Intuition (N) \u2013 Sensing (S)","8a0841e5":"Before proceeding further to Deep Learning methods and fine tuning of the models previously evalueted, we will try training the models on each 4 dimensions using the methods and sample that worked better.","3a7f820d":"### Using dimensions","d916425a":"##### resampled","3da51431":"So, in the end, the model trained using types predicts better that applying the 4 models for the different dimensions consecutively, unless we are particularly interested in 1 of the personality dimensions.  In that case using the model for that particular dimension would be my recommendation. ","f0c24e72":"#### UMAP on TSVD","c97614a5":"##### original sample","83120b47":"It also seems, attending to feature importances, than words_per_comment and variance_of_word_counts were not relevant in the models' training to predict the types and dimensions. ","9fcef68a":"**Comments**","9eb677e1":"##### original sample","2912fc95":"##### Introversion (I) \u2013 Extroversion (E)","14b4286d":"##### Judging (J) \u2013 Perceiving (P)","698eedbb":"### Using types","3701e683":"##### Thinking (T) \u2013 Feeling (F)","97cf24d9":"From the 6 datasets created in the previous notebook, I will train few models by combining different algorithms (`GaussianNB`, `LogisticRegression`, `KNeighborsClassifier`, `DecisionTreeClassifier`, `RandomForestClassifier`, `GradientBoostingClassifier` and `MLPClassifier`)  for each dataset and each target. Moreover, we will have a version of each with the original dataset size and a second one with a resampled version of it.\n\nThough more metrics, all of them weighted ones, will be analyzed, I will focus on F1 score. In my classification, precision is not more relevant than exhaustivity neither the opposite, plus F1 is much less prompt to overfitting or underfitting issues compared to accuracy, especially considering we will use weighted measures.","6b022d1f":"## 3. Model building and evaluation: Machine Learning","2b240f47":"#### UMAP","839c6a63":"###### Feature importance","5573c2f9":"###### Feature importance","9dc6d057":"#### Truncated SVD with the original sample","be7cba43":"#### Truncated SVD ","d8524ac5":"###### Feature importance"}}