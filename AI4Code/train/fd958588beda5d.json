{"cell_type":{"19a30be6":"code","7afe00f9":"code","496e5ce7":"code","2765d4e8":"code","a100f31c":"code","d275ba90":"code","cad91302":"code","c5b59b25":"code","831ff916":"code","b9196eb4":"markdown","027ed3a0":"markdown","683d2492":"markdown","23c8c526":"markdown","96d73f17":"markdown","44b0e3a6":"markdown","fbdbbaf7":"markdown","6d14fe53":"markdown"},"source":{"19a30be6":"import numpy as np\nimport pandas as pd\nfrom pathlib import Path\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        \nimport matplotlib.pyplot as plt\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.preprocessing import LabelEncoder\n\nfrom sklearn.ensemble import RandomForestClassifier\n\nimport matplotlib.pyplot as plt\n        \ninput_path = Path('\/kaggle\/input\/tabular-playground-series-mar-2021\/')","7afe00f9":"train = pd.read_csv(input_path \/ 'train.csv', index_col='id')\ndisplay(train.head())","496e5ce7":"test = pd.read_csv(input_path \/ 'test.csv', index_col='id')\ndisplay(test.head())","2765d4e8":"submission = pd.read_csv(input_path \/ 'sample_submission.csv', index_col='id')\ndisplay(submission.head())","a100f31c":"for c in train.columns:\n    if train[c].dtype=='object': \n        lbl = LabelEncoder()\n        lbl.fit(list(train[c].values) + list(test[c].values))\n        train[c] = lbl.transform(train[c].values)\n        test[c] = lbl.transform(test[c].values)\n        \ndisplay(train.head())","d275ba90":"target = train.pop('target')\nX_train, X_test, y_train, y_test = train_test_split(train, target, train_size=0.60)","cad91302":"clf = RandomForestClassifier(n_estimators=200, max_depth=7, n_jobs=-1)\nclf.fit(X_train, y_train)\ny_pred = clf.predict_proba(X_test)[:, 1] # This grabs the positive class prediction\nscore = roc_auc_score(y_test, y_pred)\nprint(f'{score:0.5f}') # 0.87323 shows we're doing better than a dummy model","c5b59b25":"plt.figure(figsize=(8,4))\nplt.hist(y_pred[np.where(y_test == 0)], bins=100, alpha=0.75, label='neg class')\nplt.hist(y_pred[np.where(y_test == 1)], bins=100, alpha=0.75, label='pos class')\nplt.legend()\nplt.show()","831ff916":"clf = RandomForestClassifier(n_estimators=200, max_depth=7, n_jobs=-1)\nclf.fit(train, target)\nsubmission['target'] = clf.predict_proba(test)[:, 1]\nsubmission.to_csv('random_forest.csv')","b9196eb4":"# Read in the data files","027ed3a0":"## We need to encode the categoricals.\n\nThere are different strategies to accomplish this, and different approaches will have different performance when using different algorithms.  You may decide to encode features with high cardinality (e.g., more distinct values) diffirently than features with low cardinality. For this starter notebook, we'll use simple encoding.","683d2492":"# Let's train it on all the data and make a submission!","23c8c526":"In this notebook, you will learn how to make your first submission to the [Tabular Playground Series - Mar 2021 competition.](https:\/\/www.kaggle.com\/c\/tabular-playground-series-mar-2021)\n\n# Make the most of this notebook!\n\nYou can use the \"Copy and Edit\" button in the upper right of the page to create your own copy of this notebook and experiment with different models. You can run it as is and then see if you can make improvements.","96d73f17":"## Now you should save your Notebook (blue button in the upper right), and then when that's complete go to the notebook viewer and make a submission to the competition. :-)\n\n## There's lots of room for improvement. What things can you try to get a better score?","44b0e3a6":"# Simple Random Forest\n\nIn previous Tabular Playground Series competition, when the target was continuous, we created a \"naive\" dummy model, that just predicted the average of the target. That approach is less useful when the scoring metric is AUC, since any constant prediction will score 0.5. So we'll skip that this time, and note that we want to score better than 0.5 for our model to be considered better than naive or random.","fbdbbaf7":"## Let's take a look at how the model predicted the various classes\n\nThe graph below shows that the model does well with most of the negative observations, but struggles with many of the positive observations.","6d14fe53":"## Pull out the target, and make a validation split"}}