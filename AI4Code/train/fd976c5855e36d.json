{"cell_type":{"66284de7":"code","3a9c3808":"code","b1330aa0":"code","106d14d5":"code","9401c3a6":"code","3f113722":"code","6d5da022":"code","9378078c":"code","d8207437":"code","5d21bc58":"markdown","c6b62f72":"markdown","9ee14841":"markdown","90244560":"markdown"},"source":{"66284de7":"import numpy as np\nimport os\nimport glob\nimport cv2\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport pickle\nimport keras\nfrom keras.models import Sequential\nfrom keras.utils import to_categorical\nfrom keras.layers import Dense, Input, Conv2D, Flatten, MaxPooling2D, Activation, Dropout, Average\nfrom keras.models import Model\nfrom keras.models import load_model\nfrom keras import optimizers\nfrom keras.metrics import categorical_crossentropy\nfrom keras.callbacks import ModelCheckpoint\nfrom keras import callbacks\nfrom keras import backend as K\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import KFold\nfrom keras.preprocessing.image import ImageDataGenerator\nimport gc\nfrom PIL import Image,ImageFilter,ImageDraw","3a9c3808":"data_dir=os.path.join('..','input')\n\narr_train = ['a','b','c','d','e']\niterator_train = len(arr_train)\npaths_train_all=[]\n\nfor i in range(iterator_train):\n    #print (arr_train[i])\n    dirx= 'training-'+arr_train[i]\n    paths_train_x=glob.glob(os.path.join(data_dir,dirx,'*.png'))\n    paths_train_all=paths_train_all+paths_train_x\n\narr_test = ['a','b','c','d','e','f','auga','augc']\niterator_test = len(arr_test)\npaths_test_all=[]\n\nfor i in range(iterator_test):\n    dirx= 'testing-'+arr_test[i]\n    paths_test_x=glob.glob(os.path.join(data_dir,dirx,'*.png'))\n    paths_test_all=paths_test_all+paths_test_x\n    if arr_test[i]=='f':\n        paths_test_f=glob.glob(os.path.join(data_dir,dirx,'*.JPG'))\n        paths_test_all=paths_test_all+paths_test_f\n\n\npath_label_train_all=[]\nfor i in range(iterator_train):\n    dirx= 'training-'+arr_train[i] + '.csv'\n    paths_label_train = glob.glob(os.path.join(data_dir,dirx))\n    \n    path_label_train_all= path_label_train_all + paths_label_train\nprint (path_label_train_all)\n\n","b1330aa0":"def get_key(path):\n    # seperates the key of an image from the filepath\n    key=path.split(sep=os.sep)[-1]\n    return key\n\ndef get_data(paths_img,path_label=None,resize_dim=32):\n    '''reads images from the filepaths, resizes them (if given), and returns them in a numpy array\n    Args:\n        paths_img: image filepaths\n        path_label: pass image label filepaths while processing training data, defaults to None while processing testing data\n        resize_dim: if given, the image is resized to resize_dim x resize_dim (optional)\n    Returns:\n        X: group of images\n        y: categorical true labels\n    '''\n    X=[] # initialize empty list for resized images\n    for i,path in enumerate(paths_img):\n        img=cv2.imread(path,cv2.IMREAD_GRAYSCALE) # images loaded in color (BGR)\n#         img=cv2.cvtColor(img,cv2.COLOR_BGR2RGB)\n        if resize_dim is not None:\n            img=cv2.resize(img,(resize_dim,resize_dim),interpolation=cv2.INTER_AREA) # resize image to 28x28\n        X.append(np.expand_dims(img,axis=2)) # expand image to 28x28x1 and append to the list.\n#         X.append(img) # expand image to 28x28x1 and append to the list\n        # display progress\n        if i==len(paths_img)-1:\n            end='\\n'\n        else: end='\\r'\n        print('processed {}\/{}'.format(i+1,len(paths_img)),end=end)\n        \n    X=np.array(X) # tranform list to numpy array\n    if  path_label is None:\n        return X\n    else:\n        \n\n        # Concatenate all data into one DataFrame\n        df = pd.DataFrame()\n        l = []\n        for file_ in path_label:\n            df_x = pd.read_csv(file_,index_col=None, header=0)\n            l.append(df_x)\n        df = pd.concat(l)\n        \n        #df = pd.read_csv(path_label[i]) # read labels\n        df=df.set_index('filename') \n        y_label=[df.loc[get_key(path)]['digit'] for path in  paths_img] # get the labels corresponding to the images\n        y=to_categorical(y_label,10) # transfrom integer value to categorical variable\n\n        return X, y\n\ndef create_submission(predictions,keys,path):\n    result = pd.DataFrame(\n        predictions,\n        columns=['label'],\n        index=keys\n        )\n    result.index.name='key'\n    result.to_csv(path, index=True)","106d14d5":"img_size = 32\nX_train_all,y_train_all=get_data(paths_train_all,path_label_train_all,resize_dim=img_size)\nprint (X_train_all.shape)\nprint (y_train_all.shape)\n","9401c3a6":"k=0\ndef image_gaussian_blur(img):\n    img = Image.fromarray(np.uint8(img))\n    blurred_image = img.filter(ImageFilter.GaussianBlur(radius=5))\n    img = np.array(img)\n    return img\n\ndef black_box(img):\n    global z\n    x1=img.shape[0]\/4\n    y1=img.shape[1]\/4\n    img = Image.fromarray(img)\n    rec_draw = ImageDraw.Draw(img)\n    rec_draw.rectangle((x1+z,y1+z,x1+z+25,y1+z+25), fill=(0,0,0), outline=(0, 0, 0))\n    img = np.array(img)\n    z=(z+30)%170\n    return img\n\ndef noisy(img):\n    #np.random.seed(2)\n#   noise_typ == \"s&p\":\n    row,col,ch = img.shape\n    s_vs_p = 1\n    amount = 0.01\n    out = img\n    # Salt mode\n    num_salt = np.ceil(amount * img.size * s_vs_p)\n    coords = [np.random.randint(0, var1 - 1, int(num_salt))\n        for var1 in img.shape[::-1]]\n    out[coords] = 1\n\n    # Pepper mode\n    num_pepper = np.ceil(amount* img.size * (1. - s_vs_p))\n    coords = [np.random.randint(0, var1 - 1, int(num_pepper))\n                for var1 in img.shape]\n    out[coords] = 0\n    return out\n\ndef preprocessed(img):\n    img = img.reshape((1,) + img.shape)\n    print (img.shape)\n    global k\n    if k\/2==0:\n        k=k+1\n        return black_box(img)\n#     if k\/3==0:\n#         k=k+1\n#         return image_gaussian_blur(img)\n    if k\/5==1:\n        k=k+1\n        return noisy(img)\n    \n    return black_box(img)\n    \n    \ndef data_aug(X_train,X_test,y_train,y_test,train_batch_size,test_batch_size):\n    train_datagen = ImageDataGenerator(\n        cval=0,\n        rotation_range=40,\n        width_shift_range=0.2,\n        height_shift_range=0.2,\n        rescale=1.0\/255,\n        shear_range=0.2,\n        zoom_range=0.2,\n        horizontal_flip=True,\n        fill_mode='constant',\n#         preprocessing_function=noisy\n    )\n    test_datagen = ImageDataGenerator(rescale=1.0\/255)\n    train_batch = train_datagen.flow(X_train,y_train,batch_size=train_batch_size)\n    test_batch = test_datagen.flow(X_test,y_test,batch_size=test_batch_size)\n    return (train_batch,test_batch)\n","3f113722":"def custom_model1(img_size=img_size,channels=X_train_all.shape[3]):\n    model = Sequential()\n    input_shape = (img_size,img_size,channels)\n    model.add(Conv2D(32, (3, 3), input_shape=input_shape, padding='same'))\n    model.add(Activation('relu'))\n    model.add(MaxPooling2D(pool_size=(2, 2)))\n\n    model.add(Conv2D(64, (3, 3),padding='same'))\n    model.add(Activation('relu'))\n    model.add(MaxPooling2D(pool_size=(2, 2)))\n\n    model.add(Conv2D(128, (3, 3),padding='same'))\n    model.add(Activation('relu'))\n    model.add(MaxPooling2D(pool_size=(2, 2)))\n\n    model.add(Flatten())\n    model.add(Dense(64))\n    model.add(Activation('relu'))\n    model.add(Dropout(0.2))\n    model.add(Dense(10))\n    model.add(Activation('softmax'))\n    \n    model.compile(optimizers.Adam(lr=.001), loss='categorical_crossentropy', metrics=['accuracy'])\n    # UNCOMMENT THIS TO VIEW THE ARCHITECTURE\n    #model.summary()\n    \n    return model\n\n    \n    \n    return cb","6d5da022":"def callback(tf_log_dir_name='.\/tf-log\/',patience_lr=10):\n    cb = []\n    \"\"\"\n    Tensorboard log callback\n    \"\"\"\n    tb = callbacks.TensorBoard(log_dir=tf_log_dir_name, histogram_freq=0)\n    cb.append(tb)\n    \n    \n    \"\"\"\n    Model-Checkpoint\n    \"\"\"\n    #m = callbacks.ModelCheckpoint(filepath=model_name,monitor='val_loss',mode='auto')\n    #cb.append(m)\n    \n    \"\"\"\n    Reduce Learning Rate\n    \"\"\"\n    #reduce_lr_loss = callbacks.ReduceLROnPlateau(monitor='loss', factor=0.1, patience=patience_lr, verbose=1, epsilon=1e-4, mode='min')\n    #cb.append(reduce_lr_loss)\n    \n    \"\"\"\n    Early Stopping callback\n    \"\"\"\n    #Uncomment for usage\n    # early_stop = callbacks.EarlyStopping(monitor='val_acc', min_delta=0, patience=5, verbose=1, mode='auto',save_best_only=True)\n    # cb.apppend(early_stop)\n    return cb","9378078c":"X_test_all=get_data(paths_test_all,resize_dim=img_size)\nX_test_all=X_test_all\/255","d8207437":"\n\nkfold = KFold(n_splits=10,shuffle=True, random_state=5)\ncvscores = []\nFold = 1\nfor train, val in kfold.split(X_train_all, y_train_all):\n    if Fold==2:\n        break\n    gc.collect()\n    K.clear_session()\n    print ('Fold: ',Fold)\n    \n    X_train = X_train_all[train]\n    X_val = X_train_all[val]\n#     X_train = X_train.astype('float32')\n#     X_val = X_val.astype('float32')\n    y_train = y_train_all[train]\n    y_val = y_train_all[val]\n    \n    # Data Augmentation and Normalization(OPTIONAL) UNCOMMENT THIS FOR AUGMENTATION !!\n    batch_size = 32\n    train_batch, val_batch = data_aug(X_train,X_val,y_train,y_val, batch_size, batch_size)\n    print (train_batch[0][0].shape)\n\n    \n    # Data Normalization only - COMMENT THIS OUT FOR DATA AUGMENTATION\n#     X_train \/= 255\n#     X_val \/= 255\n    \n    \n    # If model checkpoint is used UNCOMMENT THIS\n    #model_name = 'cnn_keras_Fold_'+str(Fold)+'.h5'\n    \n    cb = callback()\n    \n    # create model\n    model = custom_model1(img_size)\n    \n    # Fit the model for without Data Augmentation - COMMENT THIS OUT FOR DATA AUGMENTATION\n#     batch_size=16\n#     epochs = 50\n#     model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=epochs, batch_size=batch_size, callbacks=cb, verbose=2)\n  \n    \n    # Fit generator for Data Augmentation - UNCOMMENT THIS FOR DATA AUGMENTATION\n    #batch_size = 128\n    epochs =  70\n    model.fit_generator(train_batch, validation_data=val_batch, epochs=epochs, \n                        validation_steps=X_val.shape[0]\/\/batch_size, \n                       steps_per_epoch= X_train.shape[0]\/\/batch_size,\n                        callbacks=cb, verbose=1)\n    \n    # Save each fold model\n    model_name = 'cnn_keras_aug_Fold_'+str(Fold)+'.h5'\n    model.save(model_name)    \n    # evaluate the model\n    scores = model.evaluate(X_val, y_val, verbose=0)\n    print(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\n    cvscores.append(scores[1] * 100)    \n    # save the probability prediction of each fold in separate csv file\n    proba = model.predict(X_test_all,batch_size=None,steps=1)\n    labels=[np.argmax(pred) for pred in proba]\n    keys=[get_key(path) for path in paths_test_all ]\n    csv_name= 'submission_CNN_keras_aug_Fold'+str(Fold)+'.csv'\n    create_submission(predictions=labels,keys=keys,path=csv_name)\n    \n    \n    Fold = Fold +1\n\nprint(\"%s: %.2f%%\" % (\"Mean Accuracy: \",np.mean(cvscores)))\nprint(\"%s: %.2f%%\" % (\"Standard Deviation: +\/-\", np.std(cvscores)))","5d21bc58":"**models**","c6b62f72":"**Libraries**","9ee14841":"**image augmentation**","90244560":"** *callbacks* **"}}