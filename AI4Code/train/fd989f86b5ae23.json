{"cell_type":{"a1478103":"code","dd542003":"code","00a029ac":"code","21d91064":"code","1eecbef5":"code","38f08bbf":"code","6cbc8365":"code","e9ad4c39":"code","9199ad3d":"code","fae74202":"code","32c742b8":"code","42e8efb7":"code","54905225":"code","184d36f4":"code","d91c38c4":"code","fabbcd77":"code","da139b5c":"code","c862076f":"code","97d785c7":"code","236ebbc7":"code","0489289e":"code","f0cc9b3f":"code","c9fbfbd0":"code","57825c01":"code","55a53879":"code","cf16ca22":"code","666955f8":"code","c4f82e2c":"code","558d0c47":"code","7ee7e4d4":"code","4901d034":"code","9277ad98":"code","182833ae":"code","84561596":"code","785885e4":"code","5ad0e243":"code","6051c3f6":"code","952a38a3":"code","ee425224":"code","55f75671":"code","8b2f17f7":"code","96e6a541":"code","d94cc471":"code","951b347b":"code","9d0814ab":"code","5e4678c5":"code","e3ef303d":"code","b5d0190f":"code","ffe4ca0a":"code","f541a337":"code","488fbe64":"code","1caa2858":"code","9625a370":"code","e27bf8b0":"code","f3c98d35":"code","f725b4b0":"code","7928fd71":"code","f078e602":"code","87ca3123":"code","c30e16cc":"code","de77e01a":"code","2728e03d":"code","9f9eda55":"code","35fd725e":"code","15e9efc6":"code","a2e0cadf":"code","3b0117ba":"code","604b728b":"code","e2d55cd3":"code","3dd9edd7":"markdown","fbaa226e":"markdown","3925e620":"markdown","40f24549":"markdown","ad2f4ad7":"markdown","33e2c03b":"markdown","745a90b5":"markdown","9d49b83c":"markdown","8a072a86":"markdown","97a1ee45":"markdown","da51da7d":"markdown","50b091f9":"markdown","9a7eeaf9":"markdown","ba1f5337":"markdown","9f751e6d":"markdown","810b9608":"markdown","097ad97f":"markdown","68e8c3b9":"markdown","d56afae8":"markdown","6fd5dc03":"markdown","29ec9195":"markdown","ef52ce8b":"markdown","9a97db83":"markdown","622390ce":"markdown","c5422582":"markdown","37a93613":"markdown","3084a4c5":"markdown"},"source":{"a1478103":"# Import data analysis tools \nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt","dd542003":"# Import the data set\ndf = pd.read_csv(\"\/Users\/ulasarikaya\/Desktop\/Uygulama Workshop\/data\/Bulldozer.csv\")","00a029ac":"# No parse_dates... check dtype of \"saledate\"\ndf.info()","21d91064":"fig, ax = plt.subplots(dpi = 130)\nax.scatter(df[\"saledate\"][:1000], df[\"SalePrice\"][:1000])","1eecbef5":"df.SalePrice.plot.hist()","38f08bbf":"df = pd.read_csv(\"\/Users\/ulasarikaya\/Desktop\/Uygulama Workshop\/data\/Bulldozer.csv\",\n                 low_memory=False,\n                 parse_dates=[\"saledate\"])\ndf.head()","6cbc8365":"# With parse_dates... check dtype of \"saledate\"\ndf.info()","e9ad4c39":"fig, ax = plt.subplots(dpi=130)\nax.scatter(df[\"saledate\"][:1000], df[\"SalePrice\"][:1000])","9199ad3d":"df.head()","fae74202":"df.head().T","32c742b8":"df.saledate.head(20)","42e8efb7":"# Sort DataFrame in date order\ndf.sort_values(by=[\"saledate\"], inplace=True, ascending=True)\ndf.saledate.head(20)","54905225":"# Make a copy of the original DataFrame to perform edits on\ndf_tmp = df.copy()","184d36f4":"# Add datetime parameters for saledate\ndf_tmp[\"saleYear\"] = df_tmp.saledate.dt.year\ndf_tmp[\"saleMonth\"] = df_tmp.saledate.dt.month\ndf_tmp[\"saleDay\"] = df_tmp.saledate.dt.day\ndf_tmp[\"saleDayofweek\"] = df_tmp.saledate.dt.dayofweek\ndf_tmp[\"saleDayofyear\"] = df_tmp.saledate.dt.dayofyear\n\n# Drop original saledate\ndf_tmp.drop(\"saledate\", axis=1, inplace=True)\n\ndf_tmp.head()","d91c38c4":"df_tmp.head().T","fabbcd77":"# Check the different values of different columns\ndf_tmp.Hydraulics.value_counts()","da139b5c":"# Check for missing categories and different datatypes\ndf_tmp.info()","c862076f":"# Check for missing values\ndf_tmp.isna().sum()","97d785c7":"df_tmp.head().T","236ebbc7":"pd.api.types.is_string_dtype(df_tmp[\"UsageBand\"])","0489289e":"# These columns contain strings\nfor label, content in df_tmp.items():\n    if pd.api.types.is_string_dtype(content):\n        print(label)","f0cc9b3f":"# If you're wondering what df.items() does, let's use a dictionary as an example\nrandom_dict = {\"key1\": \"hello\",\n               \"key2\": \"world!\"}\n\nfor key, value in random_dict.items():\n    print(f\"This is a key: {key}\")\n    print(f\"This is a value: {value}\")","c9fbfbd0":"# This will turn all of the string values into category values\nfor label, content in df_tmp.items():\n    if pd.api.types.is_string_dtype(content):\n        df_tmp[label] = content.astype(\"category\").cat.as_ordered()","57825c01":"df_tmp.info()","55a53879":"df_tmp.state.cat.categories","cf16ca22":"df_tmp.state.cat.codes","666955f8":"df_tmp.isnull().sum()\/len(df_tmp)","c4f82e2c":"# Save preprocessed data\ndf_tmp.to_csv(\"train_tmp.csv\",\n              index=False)","558d0c47":"# Import preprocessed data\ndf_tmp = pd.read_csv(\"train_tmp.csv\",\n                     low_memory=False)\ndf_tmp.head().T","7ee7e4d4":"df_tmp.info()","4901d034":"# Check missing values\ndf_tmp.isna().sum()","9277ad98":"for label, content in df_tmp.items():\n    if pd.api.types.is_numeric_dtype(content):\n        print(label)","182833ae":"# Check for which numeric columns have null values\nfor label, content in df_tmp.items():\n    if pd.api.types.is_numeric_dtype(content):\n        if pd.isnull(content).sum():\n            print(label)","84561596":"# Fill numeric rows with the median\nfor label, content in df_tmp.items():\n    if pd.api.types.is_numeric_dtype(content):\n        if pd.isnull(content).sum():\n            # Add a binary column which tells if the data was missing our not\n            df_tmp[label+\"_is_missing\"] = pd.isnull(content)\n            # Fill missing numeric values with median since it's more robust than the mean\n            df_tmp[label] = content.fillna(content.median())","785885e4":"# Check if there's any null values\nfor label, content in df_tmp.items():\n    if pd.api.types.is_numeric_dtype(content):\n        if pd.isnull(content).sum():\n            print(label)","5ad0e243":"# Check to see how many examples were missing\ndf_tmp.auctioneerID_is_missing.value_counts()","6051c3f6":"# Check columns which *aren't* numeric\nfor label, content in df_tmp.items():\n    if not pd.api.types.is_numeric_dtype(content):\n        print(label)","952a38a3":"# Turn categorical variables into numbers\nfor label, content in df_tmp.items():\n    # Check columns which *aren't* numeric\n    if not pd.api.types.is_numeric_dtype(content):\n        # Add binary column to inidicate whether sample had missing value\n        df_tmp[label+\"_is_missing\"] = pd.isnull(content)\n        # We add the +1 because pandas encodes missing categories as -1\n        df_tmp[label] = pd.Categorical(content).codes+1        ","ee425224":"df_tmp.info()","55f75671":"df_tmp.isna().sum()","8b2f17f7":"df_tmp.head().T","96e6a541":"\nfrom sklearn.ensemble import RandomForestRegressor\n# Instantiate model\nmodel = RandomForestRegressor(n_jobs=-1)","d94cc471":"%%time\nmodel.fit(df_tmp.drop(\"SalePrice\", axis=1), df_tmp.SalePrice)","951b347b":"# Score the model\nmodel.score(df_tmp.drop(\"SalePrice\", axis=1), df_tmp.SalePrice)","9d0814ab":"df_tmp.head()","5e4678c5":"df_tmp.saleYear.value_counts()","e3ef303d":"# Split data into training and test\ndf_train = df_tmp[df_tmp.saleYear != 2012]\ndf_test = df_tmp[df_tmp.saleYear==2012]\nlen(df_train), len(df_test)","b5d0190f":"# Split data into X & y\nX_train, y_train = df_train.drop(\"SalePrice\", axis=1), df_train.SalePrice\nX_test, y_test = df_test.drop(\"SalePrice\", axis=1), df_test.SalePrice\n\nX_train.shape, y_train.shape, X_test.shape, y_test.shape","ffe4ca0a":"# Create evaluation function (the competition uses Root Mean Square Log Error)\nfrom sklearn.metrics import mean_squared_log_error, mean_absolute_error\n\ndef rmsle(y_test, y_preds):\n    return np.sqrt(mean_squared_log_error(y_test, y_preds))\n\n# Create function to evaluate our model\ndef show_scores(model):\n    train_preds = model.predict(X_train)\n    test_preds = model.predict(X_test)\n    scores = {\"Training MAE\": mean_absolute_error(y_train, train_preds),\n              \"Test MAE\": mean_absolute_error(y_test, test_preds),\n              \"Training RMSLE\": rmsle(y_train, train_preds),\n              \"Test RMSLE\": rmsle(y_test, test_preds),\n              \"Training R^2\": model.score(X_train, y_train),\n              \"Test R^2\": model.score(X_test, y_test)}\n    return scores","f541a337":"# This takes too long...\n\n# %%time\n# # Retrain a model on training data\n# model.fit(X_train, y_train)\n# show_scores(model)","488fbe64":"len(X_train)","1caa2858":"# Change max samples in RandomForestRegressor\nmodel = RandomForestRegressor(n_jobs=-1,\n                              max_samples=10000)","9625a370":"%%time\n# Cutting down the max number of samples each tree can see improves training time\nmodel.fit(X_train, y_train)","e27bf8b0":"show_scores(model)","f3c98d35":"from sklearn.model_selection import TimeSeriesSplit\ntscv = TimeSeriesSplit(n_splits=5)\nX_temp = np.arange(1,21,1)\nfor train_index, validation_index in tscv.split(X_temp):\n    X_temp_train, X_temp_valid,= X_temp[train_index], X_temp[validation_index], \n    print(X_temp_train, X_temp_valid)","f725b4b0":"%%time\nfrom sklearn.model_selection import RandomizedSearchCV\n# Different RandomForestClassifier hyperparameters\nrf_grid = {\"n_estimators\": np.arange(10, 100, 10),\n           \"max_depth\": [None, 3, 5, 10],\n           \"min_samples_split\": np.arange(2, 20, 2),\n           \"min_samples_leaf\": np.arange(1, 20, 2),\n           \"max_features\": [0.5, 1, \"sqrt\", \"auto\"],\n           \"max_samples\": [10000]}\n\ntscv = TimeSeriesSplit(n_splits=5)\nrs_model = RandomizedSearchCV(RandomForestRegressor(),\n                              param_distributions=rf_grid,\n                              n_iter=10,\n                              cv=tscv,\n                              verbose=True,\n                             n_jobs=-1)\n\nrs_model.fit(X_train, y_train)","7928fd71":"# Find the best parameters from the RandomizedSearch \nrs_model.best_params_","f078e602":"# Evaluate the RandomizedSearch model\nshow_scores(rs_model)","87ca3123":"%%time\n# Most ideal hyperparameters\nideal_model = RandomForestRegressor(n_estimators=100,\n                                    min_samples_leaf=7,\n                                    min_samples_split=4,\n                                    max_features=0.5,\n                                    n_jobs=-1,\n                                    max_depth=None,\n                                    max_samples=None)\nideal_model.fit(X_train, y_train)","c30e16cc":"show_scores(ideal_model)","de77e01a":"# Find feature importance of our best model\nideal_model.feature_importances_","2728e03d":"# Install Seaborn package in current environment (if you don't have it)\n# import sys\n# !conda install --yes --prefix {sys.prefix} seaborn","9f9eda55":"import seaborn as sns\n\n# Helper function for plotting feature importance\ndef plot_features(columns, importances, n=20):\n    df = (pd.DataFrame({\"features\": columns,\n                        \"feature_importance\": importances})\n          .sort_values(\"feature_importance\", ascending=False)\n          .reset_index(drop=True))\n    \n    sns.barplot(x=\"feature_importance\",\n                y=\"features\",\n                data=df[:n],\n                orient=\"h\")","35fd725e":"plot_features(X_train.columns, ideal_model.feature_importances_)","15e9efc6":"sum(ideal_model.feature_importances_)","a2e0cadf":"df.ProductSize.isna().sum()","3b0117ba":"df.ProductSize.value_counts()","604b728b":"df.Turbocharged.value_counts()","e2d55cd3":"df.Thumb.value_counts()","3dd9edd7":"According to the [Kaggle data page](https:\/\/www.kaggle.com\/c\/bluebook-for-bulldozers\/data), the validation set and test set are split according to dates.\n\nThis makes sense since we're working on a time series problem.\n\nE.g. using past events to try and predict future events.\n\nKnowing this, randomly splitting our data into train and test sets using something like `train_test_split()` wouldn't work.\n\nInstead, we split our data into training, validation and test sets using the date each sample occured.\n\nIn our case:\n* Training = all samples up until 2012\n* Test = all samples from 2012\n","fbaa226e":"Why add a binary column indicating whether the data was missing or not?\n\nWe can easily fill all of the missing numeric values in our dataset with the median. However, a numeric value may be missing for a reason. In other words, absence of evidence may be evidence of absence. Adding a binary column which indicates whether the value was missing or not helps to retain this information.","3925e620":"## Feature Importance\n\nSince we've built a model which is able to make predictions. The people you share these predictions with (or yourself) might be curious of what parts of the data led to these predictions.\n\nThis is where **feature importance** comes in. Feature importance seeks to figure out which different attributes of the data were most important when it comes to predicting the **target variable**.\n\nIn our case, after our model learned the patterns in the data, which bulldozer sale attributes were most important for predicting its overall sale price?\n\nBeware: the default feature importances for random forests can lead to non-ideal results.\n\nTo find which features were most important of a machine learning model, a good idea is to search something like \"\\[MODEL NAME\\] feature importance\".\n\nDoing this for our `RandomForestRegressor` leads us to find the `feature_importances_` attribute.\n\nLet's check it out.","40f24549":"With these new hyperparameters as well as using all the samples, we can see an improvement to our models performance.\n\nYou can make a faster model by altering some of the hyperparameters. Particularly by lowering `n_estimators` since each increase in `n_estimators` is basically building another small model.\n\nHowever, lowering of `n_estimators` or altering of other hyperparameters may lead to poorer results.","ad2f4ad7":"### Building an evaluation function\n\nAccording to Kaggle for the Bluebook for Bulldozers competition, [the evaluation function](https:\/\/www.kaggle.com\/c\/bluebook-for-bulldozers\/overview\/evaluation) they use is root mean squared log error (RMSLE).\n\n**RMSLE** = generally you don't care as much if you're off by $10 as much as you'd care if you were off by 10%, you care more about ratios rather than differences. **MAE** (mean absolute error) is more about exact differences.\n\nIt's important to understand the evaluation metric you're going for.\n\nSince Scikit-Learn doesn't have a function built-in for RMSLE, we'll create our own.\n\nWe can do this by taking the square root of Scikit-Learn's [mean_squared_log_error](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.metrics.mean_squared_log_error.html#sklearn.metrics.mean_squared_log_error) (MSLE). MSLE is the same as taking the log of mean squared error (MSE).\n\nWe'll also calculate the MAE and R^2 for fun.","33e2c03b":"In the format it's in, it's still good to be worked with, let's save it to file and reimport it so we can continue on.\n\n### Save Processed Data","745a90b5":"**Question:** Why is this metric not reliable?","9d49b83c":"Now we've got our tools for data analysis ready, we can import the data and start to explore it.","8a072a86":"### Sort DataFrame by saledate\n\nAs we're working on a time series problem and trying to predict future examples given past examples, it makes sense to sort our data by date.","97a1ee45":"## Convert strings to categories\n\nOne way to help turn all of our data into numbers is to convert the columns with the string datatype into a category datatype.\n\nTo do this we can use the [pandas types API](https:\/\/pandas.pydata.org\/pandas-docs\/stable\/reference\/general_utility_functions.html#data-types-related-functionality) which allows us to interact and manipulate the types of data.","da51da7d":"### Add datetime parameters for saledate column\n\nWhy?\n\nSo we can enrich our dataset with as much information as possible.\n\nBecause we imported the data using `read_csv()` and we asked pandas to parse the dates using `parse_dates=[\"saledate\"]`, we can now access the [different datetime attributes](https:\/\/pandas.pydata.org\/pandas-docs\/stable\/reference\/api\/pandas.DatetimeIndex.html) of the `saledate` column.","50b091f9":"Excellent, our processed DataFrame has the columns we added to it but it's still missing values.","9a7eeaf9":"We could add more of these style of columns, such as, whether it was the start or end of a quarter but these will do for now.\n\n**Challenge:** See what other [datetime attributes](https:\/\/pandas.pydata.org\/pandas-docs\/stable\/reference\/api\/pandas.DatetimeIndex.html) you can add to `df_tmp` using a similar technique to what we've used above. Hint: check the bottom of the pandas.DatetimeIndex docs.","ba1f5337":"### Testing our model on a subset (to tune the hyperparameters)\n\nRetraing an entire model would take far too long to continuing experimenting as fast as we want to.\n\nSo what we'll do is take a sample of the training set and tune the hyperparameters on that before training a larger model.\n\nIf you're experiments are taking longer than 10-seconds (give or take how long you have to wait), you should be trying to speed things up. You can speed things up by sampling less data or using a faster computer.","9f751e6d":"Setting `max_samples` to 10000 means every `n_estimator` (default 100) in our `RandomForestRegressor` will only see 10000 random samples from our DataFrame instead of the entire 400,000.\n\nIn other words, we'll be looking at 40x less samples which means we'll get faster computation speeds but we should expect our results to worsen (simple the model has less samples to learn patterns from).","810b9608":"Beautiful, that took far less time than the model with all the data.\n\nWith this, let's try tune some hyperparameters.","097ad97f":"### Make a copy of the original DataFrame\n\nSince we're going to be manipulating the data, we'll make a copy of the original DataFrame and perform our changes there.\n\nThis will keep the original DataFrame in tact if we need it again.","68e8c3b9":"### Filling and turning categorical variables to numbers\n\nNow we've filled the numeric values, we'll do the same with the categorical values at the same time as turning them into numbers.","d56afae8":"## 5. Modelling\n\nWe've explored our dataset a little as well as enriched it with some datetime attributes, now let's try to model.\n\nWhy model so early?\n\nWe know the evaluation metric we're heading towards. We could spend more time doing exploratory data analysis (EDA), finding more out about the data ourselves but what we'll do instead is use a machine learning model to help us do EDA.\n\nRemember, one of the biggest goals of starting any new machine learning project is reducing the time between experiments.\n\nFollowing the [Scikit-Learn machine learning map](https:\/\/scikit-learn.org\/stable\/tutorial\/machine_learning_map\/index.html), we find a [RandomForestRegressor()](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.ensemble.RandomForestRegressor.html#sklearn-ensemble-randomforestregressor) might be a good candidate.","6fd5dc03":"Now all of our data is numeric and there are no missing values, we should be able to build a machine learning model!\n\nLet's reinstantiate our trusty [RandomForestRegressor](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.ensemble.RandomForestRegressor.html).\n\nThis will take a few minutes which is too long for interacting with it. So what we'll do is create a subset of rows to work with.","29ec9195":"All of our data is categorical and thus we can now turn the categories into numbers, however it's still missing values...","ef52ce8b":"### Hyperparameter tuning with RandomizedSearchCV\n\nYou can increase `n_iter` to try more combinations of hyperparameters but in our case, we'll try 20 and see where it gets us.\n\nRemember, we're trying to reduce the amount of time it takes between experiments.","9a97db83":"### Train a model with the best parameters\n\nIn a model I prepared earlier, I tried 100 different combinations of hyperparameters (setting `n_iter` to 100 in `RandomizedSearchCV`) and found the best results came from the ones you see below.\n\n**Note:** This kind of search on my computer (`n_iter` = 100) took ~2-hours. So it's kind of a set and come back later experiment.\n\nWe'll instantiate a new model with these discovered hyperparameters and reset the `max_samples` back to its original value.","622390ce":"### Parsing dates\nWhen working with time series data, it's a good idea to make sure any date data is the format of a [datetime object](https:\/\/docs.python.org\/3\/library\/datetime.html) (a Python data type which encodes specific information about dates).","c5422582":"### Splitting data into train\/test sets","37a93613":"## Fill missing values\n\nFrom our experience with machine learning models. We know two things:\n1. All of our data has to be numerical\n2. There can't be any missing values\n\nAnd as we've seen using `df_tmp.isna().sum()` our data still has plenty of missing values.\n\nLet's fill them.\n\n### Filling numerical values first\n\nWe're going to fill any column with missing values with the median of that column.","3084a4c5":"Depending on your computer making calculations on ~400,000 rows may take a while...\n\nLet's alter the number of samples each `n_estimator` in the [`RandomForestRegressor`](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.ensemble.RandomForestRegressor.html) see's using the `max_samples` parameter."}}