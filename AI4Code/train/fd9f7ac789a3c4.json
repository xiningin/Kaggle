{"cell_type":{"d35ae0be":"code","28cb3117":"code","445d07d9":"code","c309515b":"code","af2bb28c":"code","34ea7fe1":"code","41a27ed5":"code","b2974ef5":"code","98da60c2":"code","35478b50":"markdown","81292e98":"markdown","5172a9ec":"markdown","6e65a4b3":"markdown","131bf628":"markdown","7705a796":"markdown"},"source":{"d35ae0be":"# Import Libraries\nimport os\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport warnings\nimport tensorflow as tf\n\nfrom transformers import RobertaTokenizer, XLMRobertaTokenizer, TFXLMRobertaModel, TFRobertaModel\nfrom kaggle_datasets import KaggleDatasets\n\nwarnings.filterwarnings('ignore')\n\nos.listdir(\"..\/input\/contradictory-my-dear-watson\") #List files in data dir","28cb3117":"def TPUSetup():\n    \"\"\"Configure TPU.\"\"\"\n    print('=========Configuring TPU=========')\n    try:\n        tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n        tf.config.experimental_connect_to_cluster(tpu)\n        tf.tpu.experimental.initialize_tpu_system(tpu)\n        strategy = tf.distribute.experimental.TPUStrategy(tpu)\n        print('=========Finished TPU configuration=========')\n    except ValueError:\n        # default to CPU and single GPU if TPU isn't detected\n        strategy = tf.distribute.get_strategy()\n    print('Number of replicas in sync:', strategy.num_replicas_in_sync)\n    return strategy \n\nstrategy = TPUSetup()\nBATCH_SIZE= 4 * strategy.num_replicas_in_sync #ensures utilization of all tpu cores for training speedup\nAUTO = tf.data.experimental.AUTOTUNE","445d07d9":"CONFIG = {\n    'train_path':'..\/input\/contradictory-my-dear-watson\/train.csv',\n    'test_path':'..\/input\/contradictory-my-dear-watson\/test.csv',\n    'train':{\n        'model_name': 'jplu\/tf-xlm-roberta-large',\n        'batch_size': BATCH_SIZE,\n        'epochs': 15,\n    }\n    \n}","c309515b":"def load_data(path:[str])->pd.DataFrame:\n    \"\"\"Loads files in the given file path.\n    Default path order: [train_path, test_path]\n    \"\"\"\n    train = pd.read_csv(path[0])\n    test = pd.read_csv(path[1])\n    return train, test\n\ntrain_df, test_df = load_data([CONFIG['train_path'], CONFIG['test_path']])\n\n# Exploring the shape of the datasets\nprint(f'Train Data shape {train_df.shape}')\nprint(f'Test Data shape: {test_df.shape}')\ntrain_df.head(5)","af2bb28c":"plt.figure(figsize=(12,6))\nplt.subplot(1,2,1)\ntext_count = train_df['language'].value_counts()\nplt.bar(text_count.index, text_count.values)\nplt.gca().set_xticklabels(text_count.index, rotation='45')\nplt.title('Train_language distributions')\nplt.ylabel('Text Count')\n\nplt.subplot(1,2,2)\ntext_count = test_df['language'].value_counts()\nplt.bar(text_count.index, text_count.values)\nplt.gca().set_xticklabels(text_count.index,rotation='45')\nplt.title('Test_language distributions')","34ea7fe1":"MAX_LEN = 90\ntokenizer = XLMRobertaTokenizer.from_pretrained(CONFIG['train']['model_name'])\n\ndef text_encode(sentence):\n    \"\"\"Encodes an input sentence using the bert tokenizer.\n    \"\"\"\n    tokens = list(tokenizer.tokenize(sentence))\n    tokens.append('[SEP]')\n    return tokenizer.convert_tokens_to_ids(tokens)\n\n\ndef roberta_encode(hypotheses, premises, tokenizer):\n    \"\"\"\n    Takes input_ids, input_masks, and input_type_ids inputs\n    to allow the model to know that the premise and hypothesis are distinct sentences\n    and also to ignore any padding from the tokenizer.\n    the [CLS] token denotes the beginning of the inputs,\n    a [SEP] token denotes the separation between the premise and the hypothesis.\n    Padding ensures all of the inputs to be the same size.\n    \"\"\"\n    num_examples = len(hypotheses)\n    hypothesis = tf.ragged.constant([text_encode(text) for text in np.array(hypotheses)])\n    premise = tf.ragged.constant([text_encode(text) for text in np.array(premises)])\n    \n    cls = [tokenizer.convert_tokens_to_ids(['[CLS]'])] * hypothesis.shape[0]\n    input_ids = tf.concat([cls, hypothesis, premise], axis=-1)\n    input_mask = tf.ones_like(input_ids).to_tensor()\n    \n    type_cls = tf.zeros_like(cls)\n    type_hypothesis = tf.zeros_like(hypothesis)\n    type_premise = tf.zeros_like(premise)\n    input_type_ids = tf.concat([type_cls, type_hypothesis, type_premise], axis=-1).to_tensor()\n    \n    inputs = {\n        'input_ids':input_ids.to_tensor(),\n        'input_mask': input_mask,\n        'input_type_ids': input_type_ids\n    }\n    \n    return inputs\n\ndef build_model(model_name: str):\n    \"\"\"NLI model\"\"\"\n    roberta_encoder = TFXLMRobertaModel.from_pretrained(model_name)\n    input_ids = tf.keras.Input(shape=(MAX_LEN,), dtype=tf.int32, name='input_ids')\n    input_mask = tf.keras.Input(shape=(MAX_LEN,), dtype=tf.int32, name='input_mask')\n    input_type_ids = tf.keras.Input(shape=(MAX_LEN,), dtype=tf.int32, name='input_type_ids')\n    \n    embedding = roberta_encoder([input_ids, input_mask, input_type_ids])[0]\n    output = tf.keras.layers.Dense(3, activation='softmax')(embedding[:, 0, :])\n    \n    model = tf.keras.Model(inputs=[input_ids, input_mask, input_type_ids], outputs=output)\n    model.compile(tf.keras.optimizers.Adam(lr=1e-5),\n                  loss='sparse_categorical_crossentropy',\n                  metrics=['accuracy']\n                 )\n    \n    return model\n    ","41a27ed5":"# Training and test files\ntrain_input = roberta_encode(train_df['hypothesis'].values, train_df['premise'].values, tokenizer)\ntest_input = roberta_encode(test_df['premise'].values, test_df['hypothesis'].values, tokenizer)\n\n# Learning rate Scheduler\ndef build_lrfn(lr_start=0.00001, lr_max=0.00003, \n               lr_min=0.000001, lr_rampup_epochs=3, \n               lr_sustain_epochs=0, lr_exp_decay=.6):\n    lr_max = lr_max * strategy.num_replicas_in_sync\n\n    def lrfn(epoch):\n        if epoch < lr_rampup_epochs:\n            lr = (lr_max - lr_start) \/ lr_rampup_epochs * epoch + lr_start\n        elif epoch < lr_rampup_epochs + lr_sustain_epochs:\n            lr = lr_max\n        else:\n            lr = (lr_max - lr_min) * lr_exp_decay**(epoch - lr_rampup_epochs - lr_sustain_epochs) + lr_min\n        return lr\n    \n    return lrfn\n\n\n_lrfn = build_lrfn()\nlr_schedule = tf.keras.callbacks.LearningRateScheduler(_lrfn, verbose=1)\nCheckpoint=tf.keras.callbacks.ModelCheckpoint(f\"roberta_base.h5\",\n                                              monitor='val_loss',\n                                              verbose=0,\n                                              save_best_only=True,\n                                              save_weights_only=True,\n                                              mode='min'\n                                             )","b2974ef5":"# Creating the model on TPU\nwith strategy.scope():\n    model = build_model(CONFIG['train']['model_name'])\n    model.summary()\n    \nmodel.fit(train_input, \n          train_df['label'].values,\n          epochs=CONFIG['train']['epochs'],\n          verbose = 1,\n          batch_size = CONFIG['train']['batch_size'],\n          validation_split=0.2,\n          callbacks=[Checkpoint]\n         )","98da60c2":"predictions = [np.argmax(i) for i in model.predict(test_input)]\n\n# Generate submission file\nsubmission = test_df['id'].copy().to_frame()\nsubmission['prediction'] = predictions\nsubmission.to_csv('submission.csv', index=False)","35478b50":"## Introduction:\n\n<p style=\"font_size:15\"> Natural language inference(NLI) is the task of determining whether a \u201chypothesis\u201d is true (entailment), false (contradiction), or undetermined (neutral) given a \u201cpremise\u201d.<\/p>\n\n**[Example:](http:\/\/nlpprogress.com\/english\/natural_language_inference.html#:~:text=Natural%20language%20inference%20is%20the,Premise)**\n\n| Premise | Label | Hypothesis |\n| :- | :-: | :- |\n| A soccer game with multiple males playing. | entailement | Some men are playing a sport|\n| A man inspects the uniform of a figure in some East Asian country. | contradiction | The man is sleeping. |\n| An older and younger man smiling. | neutral | An older and younger man smiling. |\n\n\n<br\/><br\/>\n<strong>In summary, NLI determines whether a given text (hypothesis) implies something similar to what is implied in the reference text (premise)<strong>","81292e98":"## Building the Model","5172a9ec":"### TPU configuration","6e65a4b3":"## <font size='5'>Data Preprocessing<\/font>","131bf628":"### Visualizing the Language distribution<font>","7705a796":"There are many great kernels already available that Introduce and analyze the task and data well. Credits to the ones that helped me in building this kernel:\n* [Ana's tutorial notebook](https:\/\/www.kaggle.com\/anasofiauzsoy\/tutorial-notebook)\n* [shahules's kernel](https:\/\/www.kaggle.com\/shahules\/contradiction-xlm-kfold-starter)\n\nIn this notebook I'll train a Roberta Model with TensorFlow and use TPU for acceleration."}}