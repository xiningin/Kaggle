{"cell_type":{"f53d8ef6":"code","3992a57e":"code","8b332a12":"code","bd4500bb":"code","3bd9562c":"code","f23b179d":"code","e0bac27b":"code","dd49a7aa":"code","b2ca56dc":"code","184834e7":"code","ebd0fb6b":"code","aef9ea56":"code","af4468b8":"code","4200bc54":"code","5ba81531":"code","d2561be9":"code","c0daa2eb":"code","46bfd233":"code","24ee8bd8":"code","931eae44":"code","f2ceae20":"code","91bdbf84":"code","d3be1eae":"code","c03c32a1":"code","48b9376d":"code","f4a55653":"code","a0b3d996":"code","589917c2":"code","e0892696":"code","a471e6db":"code","d98851d8":"code","8bf10692":"code","526d31c9":"code","4cf7513c":"code","b486c316":"code","0c82136a":"code","6d03034d":"code","542734c9":"code","342870ef":"code","288067aa":"code","98678b94":"code","0bb01d49":"code","d1fc9071":"code","0dce716c":"code","6c4b11a2":"code","1560a0f8":"code","d37abca9":"code","e119fc72":"code","4f50b9f9":"code","770662ce":"code","8b69673e":"code","d39d2ef7":"code","511224c5":"code","7de0f3c5":"code","faf7d0d8":"code","d73e9806":"code","7470dbe3":"code","d5aa23fa":"code","20c40190":"code","37eab8a3":"code","1a004892":"code","f8638e80":"code","7dd5f1f1":"code","112e6472":"code","79bbefb7":"code","0e998994":"code","fb1d1a5d":"code","bb7c1d8e":"code","7c65f670":"markdown","b23ad7f8":"markdown","30dde400":"markdown","88e65e4f":"markdown","73368645":"markdown","289de0d9":"markdown","a3b7473f":"markdown","9863b318":"markdown","c400df85":"markdown","df64e985":"markdown","e14c8e36":"markdown","8e92240b":"markdown","5df5b600":"markdown","050d192c":"markdown","abb40c1d":"markdown","6ca15853":"markdown"},"source":{"f53d8ef6":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","3992a57e":"#path = '..\/input\/melbourne-housing-market\/Melbourne_housing_FULL.csv' \n# Not working for the above dataset at the moment. Not all the preprocessing needed have been done yet.\n\npath = '..\/input\/melbourne-housing-snapshot\/melb_data.csv'\nraw_data = pd.read_csv(path)","8b332a12":"raw_data.head(5)","bd4500bb":"columns_of_interest = ['Rooms', 'Type', 'Distance', 'Bedroom2', \n                        'Bathroom', 'Car', 'Landsize', 'BuildingArea',\n                        'YearBuilt', 'Lattitude', 'Longtitude', 'Regionname', 'Price']","3bd9562c":"df = raw_data.copy()\ndf.info()","f23b179d":"df = df[columns_of_interest]\ndf.head()","e0bac27b":"column_names = ['Landsize', 'BuildingArea']\ndf_ls_ba = df[column_names]\ndf_ls_ba = df_ls_ba.dropna(axis = 0)\ndf_ls_ba.info()","dd49a7aa":"landsize_buildingarea_ratio = df_ls_ba['Landsize'].median()\/df_ls_ba['BuildingArea'].median()\nlandsize_buildingarea_ratio","b2ca56dc":"df_buildingarea = df.copy()","184834e7":"def remove_nan_buildingarea(row):\n    return row['Landsize']\/landsize_buildingarea_ratio if \\\n                pd.isnull(row['BuildingArea']) else row['BuildingArea']","ebd0fb6b":"df_buildingarea['BuildingArea'] = df_buildingarea.apply(remove_nan_buildingarea, axis = 1)","aef9ea56":"df_buildingarea.head()","af4468b8":"df_buildingarea.info()","4200bc54":"for col in df.columns:\n    print('Column: {}  |  Type: {}'.format(col, type(df[col][0])))\n","5ba81531":"df_region = df_buildingarea.copy()\ndf_region['Regionname'].unique()","d2561be9":"df_region['Regionname'].isnull().sum()","c0daa2eb":"for row in df_region['Regionname']:\n    row = 'Unknown' if row is None else row","46bfd233":"df_region['Regionname'].isnull().sum()","24ee8bd8":"def remove_nan_str(value):\n    if isinstance(value, str):\n        return value\n    else:\n        return 'Unknown' if pd.np.isnan(value) else value","931eae44":"df_region['Regionname'] = df_region['Regionname'].apply(remove_nan_str)\ndf_region['Regionname'].isnull().sum()","f2ceae20":"df_region['Regionname'].unique()","91bdbf84":"df_region['Regionname'].value_counts()","d3be1eae":"regionname_dummies = pd.get_dummies(df_region['Regionname'])\nregionname_dummies.head()","c03c32a1":"#Removes the Unknown column and avoid multicollinearity\n\nif regionname_dummies.columns.contains('Unknown'):\n    regionname_dummies = regionname_dummies.drop(['Unknown'], axis = 1)\nregionname_dummies.head()","48b9376d":"df_region = pd.concat([df_region, regionname_dummies], axis = 1)\ndf_region = df_region.drop(['Regionname'], axis = 1)\ndf_region.head()","f4a55653":"df['Type'].isnull().sum()","a0b3d996":"df['Type'].unique()","589917c2":"type_dummies = pd.get_dummies(df['Type'])\ntype_dummies","e0892696":"type_dummies.sum()","a471e6db":"# Since 'h' is the most seen value, we'll remove it and consider as the default value, again\n# avoiding multicollinearity","d98851d8":"type_dummies = type_dummies.drop(['h'], axis = 1)","8bf10692":"df_type = pd.concat([df_region, type_dummies], axis = 1)\ndf_type = df_type.drop(['Type'], axis = 1)\ndf_type.head()","526d31c9":"df_YearBuilt = df_type.copy()","4cf7513c":"df_YearBuilt['YearBuilt'].median()","b486c316":"df_YearBuilt.info()","0c82136a":"# Checking the possible ages of the data\n\nprint(df_YearBuilt['YearBuilt'].value_counts())","6d03034d":"# Age Groups in Decades\n# Up to:\n# 0, 1, 2, 3, 5, 100, 9999 (Unknown)","542734c9":"import datetime\n\nage_groups = {0, 1, 2, 3, 5, 10, 20, 100}\n\ndef divide_data_by_age_groups(year_built):\n    if pd.isnull(year_built):\n        return 9999\n    age = datetime.datetime.now().year - year_built\n    if (age % 10) >= 5:\n        age_decades = ((age \/\/ 10) + 1)\n    else:\n        age_decades = (age \/\/ 10)\n\n    for group in age_groups:\n        if age_decades <= group:\n            age_decades = group\n            break\n        \n    #print('AGE {} | DECADES {}'.format(age, age_decades))\n    return age_decades","342870ef":"df_YearBuilt['AgeInDecades'] = df_YearBuilt['YearBuilt'].apply(divide_data_by_age_groups)","288067aa":"df_YearBuilt.head(5)","98678b94":"decades_dummies = pd.get_dummies(df_YearBuilt['AgeInDecades'])\ndecades_dummies.head()","0bb01d49":"column_names = ['0_Decades_Old', '1_Decades_Old', '2_Decades_Old', '3_Decades_Old', '100_Decades_Old', 'Unknown_Decades_Old']\ndecades_dummies.columns = column_names\ndecades_dummies.head()","d1fc9071":"decades_dummies = decades_dummies.drop(['Unknown_Decades_Old'], axis = 1)\ndecades_dummies.head()","0dce716c":"df_YearBuilt = df_YearBuilt.drop(['AgeInDecades', 'YearBuilt'], axis = 1)\ndf_YearBuilt.head()","6c4b11a2":"df_YearBuilt = pd.concat([df_YearBuilt, decades_dummies], axis = 1)\ndf_YearBuilt.head()","1560a0f8":"df_Car = df_YearBuilt.copy()\ndf_Car['Car'].isnull().sum()","d37abca9":"# The 62 null values must be filled, let's use the median\ncar_median = df_YearBuilt['Car'].median()\n\ndef remove_nan_car(value):\n    if pd.isnull(value):\n        return car_median\n    else:\n        return value","e119fc72":"print(df_Car['Car'].isnull().sum())\ndf_Car['Car'] = df_Car['Car'].apply(remove_nan_car)\nprint(df_Car['Car'].isnull().sum())","4f50b9f9":"df_Car.head()","770662ce":"df_Car.isnull().sum()","8b69673e":"df_Car.columns.values","d39d2ef7":"column_names = ['Rooms', 'Distance', 'Bedroom2', 'Bathroom', 'Car', 'Landsize',\n       'BuildingArea', 'Lattitude', 'Longtitude',\n       'Eastern Metropolitan', 'Eastern Victoria',\n       'Northern Metropolitan', 'Northern Victoria',\n       'South-Eastern Metropolitan', 'Southern Metropolitan',\n       'Western Metropolitan', 'Western Victoria', 't', 'u',\n       '0_Decades_Old', '1_Decades_Old', '2_Decades_Old', '3_Decades_Old',\n       '100_Decades_Old', 'Price']\n\ndf_Car = df_Car[column_names]\ndf_Car.head()","511224c5":"df_preprocessed = df_Car.copy()\ndf_preprocessed.head()","7de0f3c5":"from sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.ensemble import RandomForestRegressor","faf7d0d8":"raw_data = df_preprocessed.copy()\n\n# Since BuildingArea has been filtered by a factor of the ratio of Landsize vs BuildingArea\n# a nice check is to run the Kernel without this column, and check it's accuracy without it\nraw_data = raw_data.drop('BuildingArea', axis = 1)\n\nraw_data.head()","d73e9806":"y = raw_data['Price']\nraw_data.drop(['Price'], axis = 1)\n\ntrain_X, test_X, train_y, test_y = train_test_split(df_preprocessed, y,train_size=0.8, \n                                                    test_size=0.2, \n                                                    random_state=0)","7470dbe3":"test_X.head(100)","d5aa23fa":"model = RandomForestRegressor(random_state=900)\nmodel.fit(train_X, train_y)\npredictions = model.predict(test_X)","20c40190":"def score_dataset(X_train, X_test, y_train, y_test):\n    model = RandomForestRegressor(random_state=900)\n    model.fit(X_train, y_train)\n    preds = model.predict(X_test)\n    return mean_absolute_error(y_test, preds)","37eab8a3":"score_dataset(train_X,test_X, train_y, test_y)","1a004892":"def get_mae(X, y):\n    # multiple by -1 to make positive MAE score instead of neg value returned as sklearn convention\n    return -1 * cross_val_score(RandomForestRegressor(50), \n                                X, y, \n                                scoring = 'neg_mean_absolute_error').mean()","f8638e80":"get_mae(test_X, test_y)","7dd5f1f1":"## Plotting Predicted vs Real","112e6472":"import matplotlib.pyplot as plt\nimport seaborn as sns; sns.set()","79bbefb7":"diff = test_y - predictions\nplt.plot(diff, 'rd')\nplt.ylabel('Absolute Price Error ($)')\nplt.show()","0e998994":"print(diff.max())\nprint(diff.min())","fb1d1a5d":"diff_percent = ((test_y - predictions)\/test_y)*100\nplt.plot(diff_percent, 'rd')\nplt.ylabel('Relative Price Error (%)')\nplt.show()","bb7c1d8e":"print(\"Minimum error: {}\".format(diff_percent.min()))\nprint(\"Maximum error: {}\".format(diff_percent.max()))\nerror_mean = diff_percent.mean()\nprint(\"Error Mean: {}\".format(diff_percent.mean()))\naccuracy = (1 - diff_percent.mean())*100\nprint(\"Accuracy: {}\".format(accuracy))","7c65f670":"As the above plotting shows, the model has adhered well to the data, and the predictions seems to be good.\n\nMaximum error seems to be of around 100K, but...","b23ad7f8":"## Import the Dataset","30dde400":"## Initiating Machine Learning","88e65e4f":"# Car Column Analisys","73368645":"## Columns reorganization","289de0d9":"## Checkpoint","a3b7473f":"## 'YearBuilt' column Analisys","9863b318":"## Melbourne House Princing 99% Accuracy","c400df85":"... the biggest error is under 3%\n\nAccuracy = 99.51%\n\nI'm very glad and hoping it's right! =]","df64e985":"## Pre Processed Checkpoint","e14c8e36":"## Region Column Analysis","8e92240b":"This is my first Kernel done from start to end on Kaggle after doing the Kaggle courses and some others from Udemy.\n\nThis kernel pre processes the dataset of the Melbourne Housing Dataset and does the prediction using the Random Forest algorithm.\n\nThe maximum calculated error of the algorithm is +\/- 2.8%\nThe prediction mean accuracy is over 99%.\n\nIf you can review it, I would appreciate!\n\nHope you enjoy it.","5df5b600":"## Checking the columns**","050d192c":"## Splitting the dataset","abb40c1d":"## Type Column Analysis","6ca15853":"## Checking the Ratio of Landsize vs BuildingArea"}}