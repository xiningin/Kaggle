{"cell_type":{"9f285386":"code","7d40bdb5":"code","e76bebfe":"code","34ef03ef":"code","7349c473":"markdown","1bad1708":"markdown","1e0f4ff8":"markdown","f98a0438":"markdown","59d3733d":"markdown","fa59bb9f":"markdown","68d1bc3c":"markdown","590b3fa7":"markdown","3022bbba":"markdown","0422b93f":"markdown","69bb98b1":"markdown","69bc95dd":"markdown","a07a01b7":"markdown","1194e975":"markdown","bb7a4f5f":"markdown","56d5b97d":"markdown","fc667c08":"markdown","7d7c0780":"markdown","5baa8896":"markdown","3e27821d":"markdown","4e873df0":"markdown","2a5a10dc":"markdown","b873afaa":"markdown","a3ba6c22":"markdown","a2a9c93e":"markdown","aace36e7":"markdown","475dfe8c":"markdown","4fb90e2d":"markdown","772e12a6":"markdown","17a08d92":"markdown","7909901c":"markdown","249a7dfa":"markdown","dec33146":"markdown","767a05e6":"markdown","864d846b":"markdown","b599b3ca":"markdown","2486dba6":"markdown","5f21e6ae":"markdown","ac70c904":"markdown","70d44474":"markdown","21da0b1c":"markdown","cb31bd1d":"markdown","08953c62":"markdown","27d4dbd8":"markdown","17996831":"markdown","ad2d8f6f":"markdown","b9976c6e":"markdown","e74d0ca2":"markdown","24ed4295":"markdown","1f0c3388":"markdown","b0e96c02":"markdown","e67f155a":"markdown","dd35cc3c":"markdown","d645e59c":"markdown","6068849b":"markdown","da9b6422":"markdown","eaf38c9e":"markdown","f748fc55":"markdown","72b8859c":"markdown","4f8fef37":"markdown","4e159ad9":"markdown","80587e89":"markdown","ed2a53d7":"markdown","0afdbbf4":"markdown","c474b5ce":"markdown","0a4641fa":"markdown","4a85c3ee":"markdown","3096848f":"markdown","7735901f":"markdown"},"source":{"9f285386":"!pip install nilearn\nimport nilearn","7d40bdb5":"from nilearn import plotting\n#Plotting sample anatomical image\nplotting.plot_anat(\"..\/input\/flanker-dataset-sample\/sub-02\/anat\/sub-02_T1w.nii\",vmax=800,dim=0.1)","e76bebfe":"#Plotting sample functional image\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nfrom nilearn import image\nfunc_img = image.load_img(\"..\/input\/flanker-dataset-sample\/sub-02\/func\/sub-02_task-flanker_run-1_bold.nii\")\nprint(func_img.shape)\nfirst_3d_slice = image.index_img(func_img, 0)\nplotting.view_img(first_3d_slice)","34ef03ef":"import pandas as pd\npd.read_csv(\"..\/input\/flanker-dataset-sample\/sub-02\/func\/sub-02_task-flanker_run-1_events.tsv\", sep='\\t')","7349c473":">You can play a simulation of the flanker task in [this awesome site](http:\/\/cognitivefun.net\/test\/6)","1bad1708":"The latter step is usually done using 3 reference points to align,like so:\n\n![](https:\/\/i.imgur.com\/xPjrX7P.png)","1e0f4ff8":"Thus, we adjust the image, using the following algorithm:\n\n`Choose a reference frame in the time series of the scan (usually the first,middle,or last frame),then\n    for each frame\n    apply`[rigid body transformations](https:\/\/en.wikipedia.org\/wiki\/Rigid_transformation)`using 3 Degrees of freedom (pitch,yaw,roll) to align the current frame's volume (in all 3 dimensions) to the reference frame`","f98a0438":"![](https:\/\/www.researchgate.net\/profile\/Jaime-Arias-Almeida\/publication\/318588598\/figure\/fig8\/AS:668595419901962@1536416978531\/fMRI-BOLD-signal-OLKT90-The-BOLD-signal-measures-the-local-changes-in-blood.ppm)","59d3733d":"#### Converting BIDS events Timings To FSL","fa59bb9f":"`.\/bids_to_FSL_timings.sh -t \"task-flanker\" -n 2 -r 3 -e \"incongruent_correct congruent_correct\" -o 1 -d 2`","68d1bc3c":"The *intensity treshold* is set to .2 so that we don't risk to eliminate parts of the brain.\n\nbelow there's a comparison between two extractions \n\n(FI = 0.2 BLUE, FI = 0.7 RED)\n\n(notice the change in the occipital cortex )","590b3fa7":"![](https:\/\/i.imgur.com\/ZnApdmB.gif)","3022bbba":"*Smoothing* is the process of applying a kernel (**FWHM** , measured in mm) over the image, averaging the value of neighboring pixels","0422b93f":"### Method","69bb98b1":"**Output**\n\n*some logging to monitor the process followed by a preview for each file created*\n\n![](https:\/\/i.imgur.com\/foRaW5F.png)","69bc95dd":"#### *This analysis is being run on the 3rd Contrast (incongruent-congruent) , not on resting state comparisons*\n\nAgain,we use a wildcard selector to select all *copes* created (*the contrast between beta weights*):\n\n`ls $PWD\/.gfeat\/cope3.feat\/stats\/cope* | sort -V > copes_list.txt`\n\nThis time , we'll use **Mixed Effects , FLAME 1** because it offers a good balance of weighting.","a07a01b7":"*Motion correction* involves stabilizing the image as the patient might move during the scan\n\n\nThis often results in [Gibbs Ringing Artifacts](https:\/\/mriquestions.com\/gibbs-artifact.html) and inaccuracies in the final [MVPA](https:\/\/www.brainvoyager.com\/bv\/doc\/UsersGuide\/MVPA\/MultiVoxelPatternAnalysisMVPA.html) , as the singular voxel wont be always in the same position during the time series","1194e975":"### 3) Slice-Timing Correction","bb7a4f5f":"### 5) Registration and Normalization","56d5b97d":"![](https:\/\/i.imgur.com\/rO6o6zE.png)","fc667c08":"![](https:\/\/i.imgur.com\/9obhXCZ.gif)","7d7c0780":"### 1.Skullstripping (*BET tool*)","5baa8896":"We thus supply FSL with all the runs of each subjects, using the following wildcard to list their path:\n\n` ls -d $PWD\/sub-??\/func\/*\/ > runs_list.txt`\n\nand subsequently run it using **Fixed effects** weighting","3e27821d":"# First-Level Analysis","4e873df0":"special thanks to https:\/\/andysbrainbook.readthedocs.io\/ for the incredible education available on his site :D.","2a5a10dc":"The dataset is conform to the [Brain Imaging Data Structure](https:\/\/bids.neuroimaging.io\/) standard,and it's available [here](https:\/\/openneuro.org\/datasets\/ds000102\/versions\/00001).\n","b873afaa":"**FSL** is a library of analysis tools for FMRI, MRI and DTI brain imaging data.\nIt contains *Preprocessing* tools,as well as *viewing* (FSLeyes) and *Model Fitting* tools","a3ba6c22":"This dataset was obtained from the OpenfMRI project (http:\/\/www.openfmri.org). Accession #: ds102 Description: Flanker task (event-related)\n\nThe \"NYU Slow Flanker\" dataset comprises data collected from 26 healthy adults (age and sex included in Slow_Flanker_age_sex.txt) while they performed a slow event-related Eriksen Flanker task.\n\nOn each trial (inter-trial interval (ITI) varied between 8 s and 14 s; mean ITI=12 s),participants used one of two buttons on a response pad to indicate the direction of a central arrow in an array of 5 arrows. In congruent trials the flanking arrows pointed in the same direction as the central arrow (e.g., < < < < <), while in more demanding incongruent trials the flanking arrows pointed in the opposite direction (e.g., < < > < <).\n\nSubjects performed two 5-minute blocks, each containing 12 congruent and 12 incongruent trials, presented in a pseudorandom order.\n\nFunctional imaging data were acquired using a research dedicated Siemens Allegra 3.0 T scanner, with a standard Siemens head coil, located at the NYU Center for Brain Imaging.\n\nThis dataset contains 146 contiguous echo planar imaging (EPI) whole-brainfunctional volumes (TR=2000 ms; TE=30 ms; flip angle=80, 40 slices, matrix=64x64; FOV=192 mm; acquisition voxel size=3x3x4mm) during each of the two flanker task blocks. A high-resolution T1-weighted anatomical image was also acquired using a magnetization prepared gradient echo sequence (MPRAGE, TR=2500 ms; TE= 3.93 ms; TI=900 ms; flip angle=8; 176 slices, FOV=256 mm).","a2a9c93e":" *Slice-Timing Correction* is the equivalent of [V-SYNC](https:\/\/en.wikipedia.org\/wiki\/Vsync_(computing)) with computer games ","aace36e7":"`subjXX\/                                    Each subject (subj) has its own enumerated folder\n\u251c\u2500 anat\/                                    anat (anatomical image) contains 3D Brain MRI data\n\u2502  \u251c\u2500 subXX_T1w.nii.gz                      NIfTI-1 T1-weighted compressed data \n\u251c\u2500 func\/                                    func (functional image) contains 4D MRI functional images and their respective events file\n\u2502  \u251c\u2500 subXX_task-flanker_run-X_bold.nii.gz  NIfTI-1 BOLD-weighted compressed data for the X run\n\u2502  \u251c\u2500 subXX_task-flanker_run-X_events.tsv   TSV file containing *onset_time*,*duration_time* and *event_type* columns for the X run`","475dfe8c":"![](https:\/\/i.imgur.com\/mcxqIpe.png)","4fb90e2d":"*We can also note how voxels with a high z-score effectively mimic the expected model behaviour*\n\n![](https:\/\/i.imgur.com\/UJQqbea.png)","772e12a6":"### 2) Motion Correction","17a08d92":"After testing this *Single Subject Analysis*, we're now ready to run it for the 2nd run and at at a *group level* (for all subjects) ","7909901c":"Running the group level analysis,we can see the end result:\n\n**Output**\n\n*tresholded z-stat map layered on top of the MNI512_Brain*\n\n![](https:\/\/i.imgur.com\/rMC4vnw.gif)\n\n\n# End Result\n![](https:\/\/i.imgur.com\/HUkXZIN.png)\n\n### Looking at the end result,we can see particular activity in the Occipital Region and after comparing incongruent vs congruent responses.\n\n### This region of the brain is in fact responsible for the visual function,suggesting ,in fact, an heightened stimulus derived from peripherial perception of the other - completely unrelated and useless - arrows\n\n### This,in turn, supports the theory that we exert very little control over our perception and attention, and that are, thus, in large part,addressed by our unconscious","249a7dfa":"The last one, *Cluster* is based on the notion that different functions in the brain tend to be localized in *functional clusters of neurons* ,at different hierarchical levels.\n\nThis method has its pros and its cons , of course, for example, even though this method cancel out noise (by the same logic of *smoothing*), its 'cluster logic' makes it harder to pinpoint precisely the correlated region, you can find more in [this article](https:\/\/www.ncbi.nlm.nih.gov\/pmc\/articles\/PMC4214144\/) ","dec33146":"# Second Level Analysis","767a05e6":"*First of all , what is the BOLD signal ?*\n\nThe [Blood-oxygen-level-dependent](https:\/\/en.wikipedia.org\/wiki\/Blood-oxygen-level-dependent_imaging) signal, it's a measure of neuronal firing that interprets the amount of oxygen in a particular region of the brain as an approximation of its activity.\n\nThis (sort of) \"spike\" in oxygen has been observed ,on average , to have a consistent shape and that its[ Haemodynamic response](https:\/\/en.wikipedia.org\/wiki\/Haemodynamic_response) can be thus modeled using a [Gamma function](https:\/\/en.wikipedia.org\/wiki\/Gamma_function);","864d846b":"**What is FSL?**","b599b3ca":"![](https:\/\/i.imgur.com\/DyzcGlu.gif)","2486dba6":"# Data Preprocessing","5f21e6ae":"# Third Level Analysis","ac70c904":"To run this analysis , i again had to write a generalized script that you can find [here](https:\/\/github.com\/myreal-exe\/group-level-first-analysis-bids-bash-script) ( *it basically iterates on all subjects the same process we just did* )\n\n`bash run_1stLevel_Analysis.sh -n 2 -s 26 -f 0.2 -i \"01\"`\n\nyea go watch some netflix that'll take a while \u00af\\\\\\_( \u0361\u00b0 \u035c\u0296 \u0361\u00b0)\\_\/\u00af *(5hrs for me \u00e7_\u00e7)*","70d44474":"#### After creating a model for a single subject, we can now extend the inference on all subjects to extrapolate a more generalized constrast map , averaging together within each subject the *beta weights* and *contrast estimates* from the 1st-level analysis.","21da0b1c":"![](https:\/\/s10.gifyu.com\/images\/ezgif-4-3854f77572.gif)","cb31bd1d":"*Skullstripping* is the process of isolating the brain from the skull\/eyes\/neck arteries *ecc..*","08953c62":"![](https:\/\/i.imgur.com\/2agmqVO.gif)","27d4dbd8":"![](https:\/\/i.imgur.com\/T7Q6ull.giff)","17996831":"This time we are instead faced with a selection in the **Post-Stats** tab for different types of *tresholding* to establish the signfiicance of each voxel:\n\n1. **None** : No tresholding required. \n2. **Uncorrected** : Simply allows every voxels whose z-score exceeds the specified treshold\n3. **Voxel** : Uses a GRF-theory-based maximum height thresholding  with thresholding at the level set, using one-tailed testing (*testing if the value, the z-score,  falls on one side of the distribution , in this case the right side from the Z treshold*)\n4. **Cluster** (aguably the most popular) : in this method a *Z treshold* s used to define contiguous clusters. Then each cluster's estimated significance level (z-score) is compared with the *cluster probability threshold* ,that we can specify in FSL (*the higher the treshold the easier it is for neighbouring signficant voxels to be considered as a cluster*)","ad2d8f6f":"This may seem counter-intuitive at first \u2014 *why would you downscale the resolution of the scans ?*\nThe answer is simple, functional scans, as you perhaps noticed , are very noisy, thus in the regions where's the signal there's also a lot of [white noise](https:\/\/en.wikipedia.org\/wiki\/White_noise)\n\nIt follows that, if we average the pixel values , the areas in which there's an actual signal will prevail over the noise,thus canceling out noise and enhancing the signal's pattern","b9976c6e":"unfortunately i couldn't find any generalized converter,so i had quicky learn bash scripting and to come up with one myself [available here](https:\/\/github.com\/myreal-exe\/BIDS-to-FSL-converter-bash-script) after modifying [this](https:\/\/github.com\/andrewjahn\/FSL_Scripts\/blob\/master\/make_FSL_Timings.sh)","e74d0ca2":"# Model Fitting","24ed4295":"**Output**\n\n![](https:\/\/i.imgur.com\/adMeha8.png)","1f0c3388":"But , as with other types of machine learning, there are multiple methods of weighting each data point (*in our case each of the 3 constrast maps for each subject*),and here,in FSL, we can choose one of the following:\n\n1. **Fixed Effects** : every data point is weighted equally ( ***soviet anthem starts playing*** )\n2. **Mixed Effects**: (Ordinary Least Squares): it utilizes minimum squared error (SSE) on the average parameter estimates calculated for each subject to weight more the outliers (points with a bigger deviation);\n3. **Mixed Effects ,FLAME 1**: Weight each subject\u2019s parameter estimate by the variance of that contrast estimate. (low variance will be weighted more)\n4.**Mixed Effects ,FLAME 1+2**: A more rigorous version of FLAME 1. It takes much longer, and is only helpful for analyzing small samples (e.g., 10 subjects or fewer);\n5.**Randomise**: A non-parametric test (for Postmodernists :) )","b0e96c02":"The model we're going to use is a *General Linear Model* (**GLM**) using multiple linear regressors","e67f155a":"The problem of course with *slicing* is that it creates [screen tearing](https:\/\/it.wikipedia.org\/wiki\/Screen_tearing).\n\nthe impact of such effect, though,it's determined by the **TR** (repetition time), aka the *slices' frequency of sampling* ( *the higher the TR , the bigger the impact* ).\n\nFor this particular experiment it's not necessary ,though ,i'll do it anyway \u00f9_\u00f9 (mainly for educational purposes)","dd35cc3c":"Given this approximation, we're going to estimate for each voxel the *beta weights* ($\\beta$, that you can think as the amplitude of the Gamma function) for each BOLD response relative to each type of event in our study so that :  \n\n#### $BOLD$ $\\approx$ $\\beta$$\\gamma$;","d645e59c":"# Dataset Analysis","6068849b":"#### **GLM EQUATION**\n\n$TSS = \\beta_{1}* ModeledBOLD_{1} + \\beta_{2}* ModeledBOLD_{2} ... + C$\n\n`TSS = Time Series Signal`\n\n$\\beta$ `= beta weights`\n\n`ModeledBOLD = Our interpolation of how the BOLD signal in response to each stimulus should look like otained by convolving our Gamma function with each stimulus timing and duration`","da9b6422":"**REFERENCES**\n\n*Kelly, A.M., Uddin, L.Q., Biswal, B.B., Castellanos, F.X., Milham, M.P. (2008). Competition between functional brain networks mediates behavioral variability. Neuroimage, 39(1):527-37*","eaf38c9e":"![](https:\/\/i.imgur.com\/AMHDiP5.gif)","f748fc55":"if we then run the analysis,we get one z threshold map for each contrast in BOLD value for each pair of states :\n\n**Output**\n\n**incongruent-base**\n\n![](https:\/\/i.imgur.com\/yJ0ThAo.png)\n\n**congruent-base**\n\n![](https:\/\/i.imgur.com\/BDrvB6U.png)\n\n**incongruent-congruent**\n\n![](https:\/\/i.imgur.com\/7c0hpTu.png)","72b8859c":"#### In a third level analysis, we use the results from the previous 2nd-level-analysis (the average estimates of all subjects) and test wheter for each type of contrast map the standard error and average estimates are statistically significant to infer a resonable conclusion.","4f8fef37":"*Registration*, instead , is the process of applying the necessary transformations to the anatomical image to achieve *Normalization* and to register (apply) those same transformations to the functional one.The reason being that the anatomical images have higher quality and thus more precise in the alignment","4e159ad9":"### 4) Smoothing","80587e89":"The *MRI scanner* does not scan the brain instantenously as a whole , instead , it scans the brain sequentially,in *slices*","ed2a53d7":"![](https:\/\/i.imgur.com\/8aYLMWp.gif)","0afdbbf4":"![](https:\/\/andysbrainbook.readthedocs.io\/en\/latest\/_images\/GLM_fMRI_Data_FSL.gif)\n\n*source : [andysbrainbook](https:\/\/andysbrainbook.readthedocs.io\/)*","c474b5ce":"Of course, though, since we have to run a staistical analysis on a group study, we have to make sure that our data is [normalized](https:\/\/en.wikipedia.org\/wiki\/Normalization_(statistics)). That means , in our case , to standardize our subject's individual brains (with diffferent sizes and shapes) using a *template brain* , so that every brain region lines up; in this case : the [MNI152 Template](https:\/\/github.com\/Jfortin1\/MNITemplate) brain.\n\nThis is the process of *Normalization*","0a4641fa":"**Design Matrix** \n\n*On the left,there's the high pass filter (only frequencies longer than the red bar will be used to create the model)*\n\n*On the right, instead, we there are 2 time series prediction (time on the vertical axis) of the BOLD response,one for each event (incongruent and congruent)*\n\n![](https:\/\/i.imgur.com\/gIjZkA9.png)","4a85c3ee":"For cleaning and preprocessing the data we're going to use [FSL](https:\/\/fsl.fmrib.ox.ac.uk\/fsl\/fslwiki\/) hosted in a Ubuntu 18.04 client","3096848f":"We're going to:\n\n1) Estimate *weights* for each individual *BOLD* response (for each event type) during the individual time series\n\n2) Fit the model on each voxel\n\n3) Constructing a *statistical map* of *fit* for each voxel","7735901f":"\n![](https:\/\/crnl.readthedocs.io\/_images\/slice_order_1.jpg)"}}