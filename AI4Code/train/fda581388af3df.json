{"cell_type":{"21c864cb":"code","cfbe4cbe":"code","43aca022":"code","e7ce0fb8":"code","8ee4405a":"code","6c7d92f0":"code","55f06e46":"code","b957ab45":"code","395cc06a":"code","219961a6":"code","213f79ed":"code","43431fa8":"code","8e4441ac":"code","c7091d49":"code","85cc1fbf":"code","cee1e328":"code","4670beeb":"code","1b71d1a2":"code","c590cf73":"code","98af6277":"code","82d4897a":"code","baa129f4":"code","5151363f":"code","28a1e263":"code","e55b0e4e":"code","a237f76a":"code","67daa571":"code","7af61524":"code","e6c70ebc":"code","9322029e":"code","ef36665e":"code","0e2ba9bc":"code","fd9920d5":"code","d0dd35ee":"code","c12f2fb0":"code","49678b71":"code","4a10c7c1":"markdown","d912316e":"markdown","472bd4e1":"markdown","7ec0b735":"markdown","439a24e8":"markdown","252ee3ae":"markdown","4fcf5f5f":"markdown","802fe9b8":"markdown","d439d5a3":"markdown","ffc7f9dc":"markdown","b43f08e4":"markdown","6c702ddc":"markdown"},"source":{"21c864cb":"from fastai import *\nfrom fastai.text import *\n\nimport pandas as pd","cfbe4cbe":"bs = 32\npath = Path('..\/input\/innoplexusav\/')","43aca022":"## Lets look at our data first\ntrain = pd.read_csv(path\/'train.csv'); train.head()","e7ce0fb8":"## Language model data\ndata_lm = TextLMDataBunch.from_csv(path, 'train.csv', text_cols='text')","8ee4405a":"## Language model Learner\nlearn_lm = language_model_learner(data_lm, AWD_LSTM, drop_mult=1.0, model_dir='\/tmp\/models')","6c7d92f0":"## Look at some of the text generated by our language model\n\nTEXT = \"The color of the sky is\"\nN_WORDS = 40\nN_SENTENCES = 2\nprint(\"\\n\".join(learn_lm.predict(TEXT, N_WORDS, temperature=0.75) for _ in range(N_SENTENCES)))","55f06e46":"learn_lm.lr_find()\nlearn_lm.recorder.plot()","b957ab45":"lr = 1e-2\/2\nlr *= bs\/48\nlearn_lm.to_fp16(); # converting the model to 1\/2 precision. Helps the model to train faster","395cc06a":"learn_lm.fit_one_cycle(1, lr*10, moms=(0.8,0.7)) # training only the head","219961a6":"learn_lm.unfreeze()\nlearn_lm.fit_one_cycle(10, lr, moms=(0.8,0.7)) # training the complete model","213f79ed":"learn_lm.save('ft_lm') # saving the complete language model","43431fa8":"learn_lm.save_encoder('ft_enc') # saving only the encoder part of the language model","8e4441ac":"# Classifier model data\ndata_clas = TextClasDataBunch.from_csv(path, 'train.csv',test='test.csv', text_cols='text', label_cols='sentiment', vocab=data_lm.vocab, bs=bs)","c7091d49":"# learner for classification\nlearn = text_classifier_learner(data_clas, AWD_LSTM, drop_mult=0.5, model_dir='\/tmp\/models', metrics=[accuracy, FBeta(average='macro',beta=1)])\nlearn.load_encoder('ft_enc')","85cc1fbf":"learn.lr_find()","cee1e328":"learn.recorder.plot()","4670beeb":"learn.fit_one_cycle(1, 2e-2, moms=(0.8,0.7))","1b71d1a2":"learn.save('first')","c590cf73":"## Gradually unfreezing \nlearn.freeze_to(-2)\nlearn.fit_one_cycle(1, slice(1e-2\/(2.6**4),1e-2), moms=(0.8,0.7))","98af6277":"learn.save('2nd')","82d4897a":"# learn.load('2nd')","baa129f4":"learn.freeze_to(-3)\nlearn.fit_one_cycle(1, slice(5e-3\/(2.6**4),5e-3), moms=(0.8,0.7))","5151363f":"learn.save('3rd')","28a1e263":"## training the complete model\nlearn.unfreeze()\nlearn.fit_one_cycle(2, slice(1e-3\/(2.6**4),1e-3), moms=(0.8,0.7))","e55b0e4e":"learn.save('clas')","a237f76a":"test_df = pd.read_csv(path\/'test.csv')\ntest_df.head()","67daa571":"test = TextList.from_csv(path, 'test.csv', cols='text')","7af61524":"learn.data.add_test(test) ","e6c70ebc":"predictions, *_ = learn.get_preds(DatasetType.Test) # making predictions \nlabels = np.argmax(predictions, 1)","9322029e":"labels","ef36665e":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics import f1_score, fbeta_score","0e2ba9bc":"def score(y, y_pred):\n    return fbeta_score(y, y_pred, beta=1, average='macro')\n\ndef evaluate_model(model, X, y):\n    scores = cross_val_score(model, X, y, cv=3, n_jobs=-1)\n    return scores.mean(), scores.std()","fd9920d5":"train = pd.read_csv(path\/'train.csv')\ntrain.head()","d0dd35ee":"X = TfidfVectorizer(ngram_range=(1,3), stop_words='english', max_features=60000).fit_transform(train.text)\ny = train.sentiment","c12f2fb0":"model = LogisticRegression(class_weight='balanced')","49678b71":"evaluate_model(model, X,y)","4a10c7c1":"### Classifier","d912316e":"> Note: Again the *model_dir* is set to '\/tmp\/model\/' for the same reason as above.\n\n> Note: *Metrics* is changed to FBeta because the completions used FBeta as evaluation metric.","472bd4e1":"### Language Model","7ec0b735":"> Note: I am changing the *model_dir* because kaggle does not allow us to save files inside *..\/input\/* (it is in read-only mode)","439a24e8":"**Conclusion:** \n\nBest Score by ULMfit was 0.545760","252ee3ae":"## Please upvote this Kaggle kernel if you find it helpful :)","4fcf5f5f":"## Fastai (Approach - 1)","802fe9b8":"## Logistic Regression (Approach - 2)","d439d5a3":"To make prediction you can either add the test set while creating *TextClasDataBunch* or your can also add it later (what I did here). You can read more about it [here](https:\/\/stackoverflow.com\/questions\/56327207\/not-able-to-predict-output-in-fastai)","ffc7f9dc":"# Innoplexus online hiring Hackathon\n\n### by Ankur Singh","b43f08e4":"**Conclusion:**\n\nLogistic Regression (0.724) beats ULMfit(0.546) by a really big margin. It might not be always true but for the particular dataset, logistic regression performs better than ULMfit.","6c702ddc":"In this kernel, I will showcase my approach of takling [drug sentiment analysis](https:\/\/datahack.analyticsvidhya.com\/contest\/innoplexus-online-hiring-hackathon\/) hosted by [Analytics Vidhya](https:\/\/analyticsvidhya.com). You are free to use any part of the code. See this [medium article](https:\/\/medium.com\/@ankursingh_82471\/text-classification-5e119f23905e) for detailed dicussion.\n\nI will be using 2 different approaches. They are listed as follows:\n- Fastai (ULMfit + TextClasLearner)\n- Logistic Regression\n"}}