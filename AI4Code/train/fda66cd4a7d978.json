{"cell_type":{"84c0f6e9":"code","6f5a5d55":"code","98c94566":"code","07abe27b":"code","27da9225":"code","de0279c4":"code","40087f25":"code","e7a303b8":"code","5e7eb8e2":"code","1bedb7d3":"code","5c1aa212":"code","ac60ed2c":"code","ae5c0a3a":"code","11d19613":"code","2571a386":"code","eb86d813":"code","a7351993":"code","785efaf5":"code","2b96adcd":"code","4b120832":"code","557544e2":"code","0a810acb":"code","06ec8af3":"code","2d0704d8":"code","6ae304c0":"code","58c35983":"code","ad43a6c0":"code","1f10b88b":"code","aa78a8ec":"code","c9193954":"code","03d8ccd4":"markdown","4db71711":"markdown","e017dacb":"markdown","dde71466":"markdown","d9ee39ef":"markdown","6cfcc1b8":"markdown","41b4cf8e":"markdown","e34f0f9b":"markdown","8821ee74":"markdown","46225a6b":"markdown","16259e92":"markdown","da7c390d":"markdown","303ab2a9":"markdown","5b819b6d":"markdown","7f3d5dd6":"markdown","971f8472":"markdown"},"source":{"84c0f6e9":"# Ignore warnings\nimport warnings\nwarnings.filterwarnings('ignore')","6f5a5d55":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib as plt\nimport seaborn as sns\n%matplotlib inline\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","98c94566":"#Reading the DATA\ntrain = pd.read_csv('..\/input\/digit-recognizer\/train.csv')\ntest= pd.read_csv('..\/input\/digit-recognizer\/test.csv')","07abe27b":"train.head()","27da9225":"train.columns","de0279c4":"#Shape of data Train and Test\ntrain.shape,test.shape","40087f25":"sns.countplot(x=train.label ,data=train)","e7a303b8":"X = train.drop('label',axis=1)\ny = train['label']","5e7eb8e2":"#Now we will convert the labels intoOne-Hot representation:\ny=pd.get_dummies(y)","1bedb7d3":"print(y.iloc[:10:])","5c1aa212":"X=X\/255.0\ntest=test\/255.0","ac60ed2c":"#Reshaping the data for further model process\nX = X.values.reshape(-1,28,28,1)\ntest = test.values.reshape(-1,28,28,1)","ae5c0a3a":"#Shape of X,y and Test data\nprint(X.shape)\nprint(y.shape)\nprint(test.shape)","11d19613":"from sklearn.model_selection import train_test_split\n\nX_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.20,random_state=42)","2571a386":"#Setting up the batch_size and epochs\nbatch_size=32\nepochs=20","eb86d813":"from tensorflow import keras\nfrom tensorflow.keras import layers\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Flatten\nfrom keras.layers import Conv2D, MaxPooling2D, BatchNormalization\nfrom tensorflow.keras.layers.experimental import preprocessing","a7351993":"from tensorflow import keras\nfrom tensorflow.keras import layers\n\nmodel = keras.Sequential([\n    layers.Dense(100, activation='relu', input_shape=(28,28,1)),\n    layers.Dropout(0.3),\n    layers.Flatten(),\n    layers.Dense(units=100, activation='relu'),\n    layers.Dense(units=10, activation='softmax')\n])\nmodel.summary()","785efaf5":"model.compile(\n    optimizer='adam',\n    loss='categorical_crossentropy',\n    metrics=['accuracy']\n)\nmodel.optimizer.lr=0.001","2b96adcd":"#Early stopping\nfrom tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\nearly_stopping = EarlyStopping(monitor='val_loss', patience=3,restore_best_weights=True)","4b120832":"rlrp=ReduceLROnPlateau(monitor='val_loss', patience=3,factor=0.5, min_lr=0.00001)","557544e2":"history_model = model.fit(\n    x=X_train,\n    y=y_train,\n    validation_data=(X_test, y_test),\n    batch_size=batch_size,\n    epochs=epochs,\n    shuffle=True,\n    verbose=2,\n    callbacks=[early_stopping,rlrp]\n)","0a810acb":"#Let's plot the curve for loss,val_loss,accuracy,val_accuracy\ndef plot_loss_nd_accuracy(history):\n    history_df=pd.DataFrame(history)\n    history_df.loc[0:,['loss','val_loss']].plot()\n    history_df.loc[0:,['accuracy','val_accuracy']].plot()","06ec8af3":"plot_loss_nd_accuracy(history_model.history)","2d0704d8":"# Building the model\nmodel_cnn = keras.Sequential([\n    layers.InputLayer(input_shape=[28,28,1]),\n    preprocessing.RandomContrast(0.2),\n    preprocessing.RandomTranslation(height_factor=0.1,width_factor=0.1),\n    layers.Conv2D(filters=32, kernel_size=3, activation='relu'),\n    layers.BatchNormalization(axis=1),\n    layers.Conv2D(filters=64, kernel_size=3, activation='relu'),\n    layers.BatchNormalization(axis=1),\n    layers.Conv2D(filters=128, kernel_size=5, activation='relu'),\n    layers.BatchNormalization(axis=1),\n    layers.Conv2D(filters=128, kernel_size=5, activation='relu'),\n    layers.MaxPool2D(),\n    layers.BatchNormalization(axis=1),\n    layers.Flatten(),\n    layers.Dense(units=1024,activation='relu'),\n    layers.Dense(units=10, activation='softmax')\n])","6ae304c0":"model_cnn.summary()","58c35983":"model_cnn.compile(\n    optimizer='adam',\n    loss='categorical_crossentropy',\n    metrics=['accuracy']\n)\nmodel_cnn.optimizer.lr=0.001","ad43a6c0":"model_2 = model_cnn.fit(\n    x=X_train,\n    y=y_train,\n    validation_data=(X_test, y_test),\n    batch_size=batch_size,\n    epochs=epochs,\n    shuffle=True,\n    verbose=2,\n    callbacks=[early_stopping,rlrp]\n)","1f10b88b":"plot_loss_nd_accuracy(model_2.history)","aa78a8ec":"predictions = model_cnn.predict(test)\nresults = predictions.argmax(axis=-1)","c9193954":"result = pd.DataFrame()\nresult['ImageId'] = list(range(1,28001))\nresult['Label'] = results\nresult.to_csv(\"CNN_MODEL.csv\", index = False) ","03d8ccd4":"Each image is 28 pixels in height and 28 pixels in width, for a total of 784 pixels in total. Each pixel has a single pixel-value associated with it, indicating the lightness or darkness of that pixel, with higher numbers meaning darker. This pixel-value is an integer between 0 and 255, inclusive.","4db71711":"Fitting the same data to cnn","e017dacb":"![image.png](attachment:8577d11d-90d3-4dd4-a5f3-f2dead3c26e6.png)","dde71466":"We can see that the validation and training data seem to converge together this time, which implies that we have solved the main causes of overfitting.","d9ee39ef":"# The Model\nI am using a convolutional neural network with the following architecture:\n\n1)InputLayer(28, 28, 1)\n\n2)Preprocessing\n\n3)Conv2D(kernel_size=3, filters=32, activation='relu')\n\n4)BatchNormalization\n\n5)Conv2D(kernel_size=3, filters=64, activation='relu')\n\n6)BatchNormalization\n\n7)Conv2D(kernel_size=5, filters=128, activation='relu')\n\n8)BatchNormalization\n\n9)Conv2D(kernel_size=5, filters=128, activation='relu')\n\n10)MaxPool2D\n\n11)BatchNormalization\n\n12)Flatten\n\n13Dense(units=1024,activation='relu')\n\n14)Dense(units=10, activation='softmax'","6cfcc1b8":"Here we can see that the model converges at around 0.96 val_accuracy score (different results may occur at each run, due to the randomization of the initial parameters).\n\nLet's also take a look at the loss and accuracy values at each epoch. However, since we will do the same for each model in the notebook, we will first create a helper function for it.","41b4cf8e":"**The data files train.csv and test.csv contain gray-scale images of hand-drawn digits, from zero through nine.**","e34f0f9b":"# Submission","8821ee74":"![image.png](attachment:3ee9f2a8-8370-46e2-aca4-6065b6ca93cb.png)","46225a6b":"Here I've scale the data inton 0 and 1.","16259e92":"# Buliding a Convolutional Neural Network (CNN) using Keras\nA convolutional neural network (CNN) is a type of artificial neural network used in image recognition and processing that is specifically designed to process pixel data.\n\nCNNs are used for image classification and recognition because of its high accuracy. ... The CNN follows a hierarchical model which works on building a network, like a funnel, and finally gives out a fully-connected layer where all the neurons are connected to each other and the output is processed.","da7c390d":"# What are neural networks?\n\n***Neural networks reflect the behavior of the human brain, allowing computer programs to recognize patterns and solve common problems in the fields of AI, machine learning, and deep learning.***\n\nNeural networks, also known as artificial neural networks (ANNs) or simulated neural networks (SNNs), are a subset of machine learning and are at the heart of deep learning algorithms. Their name and structure are inspired by the human brain, mimicking the way that biological neurons signal to one another.\n\nArtificial neural networks (ANNs) are comprised of a node layers, containing an input layer, one or more hidden layers, and an output layer. Each node, or artificial neuron, connects to another and has an associated weight and threshold. If the output of any individual node is above the specified threshold value, that node is activated, sending data to the next layer of the network. Otherwise, no data is passed along to the next layer of the network.","303ab2a9":"My traning data for digit recognition","5b819b6d":"# Neural Network with using tensorflow Keras","7f3d5dd6":"Okay! In our training data we have label, Now i  will seperate it into X and y variables:","971f8472":"->Now i will apply train_test_split for spliting the training data into training and testing."}}