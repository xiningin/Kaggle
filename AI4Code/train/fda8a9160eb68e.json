{"cell_type":{"4ccb77a7":"code","d6839f6f":"code","aee4e61a":"code","6ae9016c":"code","1e12f3ba":"code","8d269bc3":"code","4f4d61d0":"code","b3cbd4e2":"code","496343b4":"code","988ce9f7":"code","2dac5d98":"code","88b0023d":"code","3afcc56c":"code","4e923f68":"code","44f00619":"code","a536e8c7":"code","0b86ed1c":"code","a33c4883":"code","9c9e7058":"code","f8d723a9":"code","645dc191":"code","2d10b3c9":"code","4af854dd":"code","3830441c":"code","e42b08fd":"code","ff98279d":"code","4eb75cad":"code","85cb0f9d":"code","80604239":"code","7eac8e9b":"code","fa276a08":"code","720ddb12":"code","2dbea26a":"code","4cdc4fff":"code","d15b4d80":"code","c47e01b2":"code","3b8cf39a":"code","f463b3a2":"code","830585d1":"markdown","3b574584":"markdown"},"source":{"4ccb77a7":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","d6839f6f":"data = pd.read_csv('\/kaggle\/input\/digit-recognizer\/train.csv')","aee4e61a":"data.shape","6ae9016c":"data.columns","1e12f3ba":"input_data = data.drop('label', axis=1)\ninput_data.shape","8d269bc3":"output_data = data['label']\noutput_data.shape","4f4d61d0":"input_data.head(1)","b3cbd4e2":"max(input_data.iloc[0])","496343b4":"from sklearn import preprocessing\nMinMaxScaler = preprocessing.MinMaxScaler()\ninput_data = MinMaxScaler.fit_transform(input_data)","988ce9f7":"#Reshape to required tensor shape in Keras\ninput_data = input_data.reshape(-1, 28, 28, 1)\ninput_data.shape","2dac5d98":"test = pd.read_csv('\/kaggle\/input\/digit-recognizer\/test.csv')","88b0023d":"test_data = MinMaxScaler.fit_transform(test)\ntest_data = test_data.reshape(-1, 28, 28, 1)\ntest_data.shape","3afcc56c":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(input_data, output_data.values, test_size=0.3, random_state=1)","4e923f68":"#To convert target data (One hot encoding #preprocessing)\n# 1 ==> [1 0 0 0 0 0 0 0 0 0]\n# 2 ==> [0 1 0 0 0 0 0 0 0 0]\n# 3 ==> [0 0 1 0 0 0 0 0 0 0]\nfrom keras.utils.np_utils import to_categorical","44f00619":"y_test.shape","a536e8c7":"y_test","0b86ed1c":"X_train.shape","a33c4883":"y_train_cat = to_categorical(y_train, 10)\ny_test_cat = to_categorical(y_test, 10)","9c9e7058":"y_test_cat.shape","f8d723a9":"y_train_cat.shape","645dc191":"y_train_cat","2d10b3c9":"from keras.models import Sequential\nfrom keras.layers import Dense, Conv2D, MaxPool2D, Flatten, Dropout, AveragePooling2D\nimport keras.backend as K","4af854dd":"from keras.preprocessing.image import ImageDataGenerator\n\ngenerator = ImageDataGenerator(#rescale = 1.\/255,\n                               width_shift_range=0.1,\n                               height_shift_range=0.1,\n                               rotation_range = 20,\n                               shear_range = 0.3,\n                               zoom_range = 0.3,\n                               horizontal_flip = True)\ngenerator.fit(X_train)","3830441c":"K.clear_session()\n\nmodel = Sequential()\n\n#model.add(generator())\n\nmodel.add(Conv2D(32, (3, 3), activation='tanh', input_shape=(28, 28, 1), padding=\"SAME\"))\nmodel.add(Conv2D(32, (3, 3), activation='tanh', padding=\"SAME\"))\n#model.add(Conv2D(32, (3, 3), activation='tanh', padding=\"SAME\"))\nmodel.add(AveragePooling2D(pool_size=(2, 2)))\n#model.add(MaxPool2D(pool_size=(2, 2)))\nmodel.add(Dropout(0.3))\n\n#model.add(Conv2D(128, (3, 3), activation='tanh'))\nmodel.add(Conv2D(64, (3, 3), activation='tanh', padding=\"SAME\"))\nmodel.add(Conv2D(64, (3, 3), activation='tanh', padding=\"SAME\"))\nmodel.add(AveragePooling2D(pool_size=(2, 2)))\n#model.add(MaxPool2D(pool_size=(2, 2)))\nmodel.add(Dropout(0.3))\n\nmodel.add(Conv2D(128, (3, 3), activation='tanh', padding=\"SAME\"))\nmodel.add(Conv2D(128, (3, 3), activation='tanh', padding=\"SAME\"))\nmodel.add(AveragePooling2D(pool_size=(2, 2)))\n#model.add(MaxPool2D(pool_size=(2, 2)))\nmodel.add(Dropout(0.2))\nmodel.add(Flatten())\n\nmodel.add(Dense(784, activation='tanh'))\nmodel.add(Dropout(0.2))\nmodel.add(Dense(256, activation='tanh'))\nmodel.add(Dropout(0.2))\nmodel.add(Dense(128, activation='tanh'))\nmodel.add(Dropout(0.2))\n\nmodel.add(Dense(10, activation='softmax'))\n\nmodel.compile(loss='categorical_crossentropy',\n              optimizer='Adamax',\n              metrics=['accuracy'])","e42b08fd":"model.summary()","ff98279d":"# Reduce learning by measuring \"validation accuracy\"\nfrom keras.callbacks import ReduceLROnPlateau\nlearning_rate_reduction = ReduceLROnPlateau(monitor='val_accuracy', \n                                            patience=3, \n                                            verbose=1, \n                                            factor=0.5, \n                                            min_lr=0.00001)","4eb75cad":"#Now training model through augmentation\n#For augmentation please follow the link \"https:\/\/keras.io\/api\/preprocessing\/image\/\"\nK.clear_session()\nhh = model.fit(generator.flow(X_train, y_train_cat, batch_size=64), validation_data=(X_test, y_test_cat),\n          steps_per_epoch=len(X_train) \/ 64, epochs=30, verbose=1, callbacks=[learning_rate_reduction])","85cb0f9d":"import matplotlib.pyplot as plt\n%matplotlib inline\nplt.plot(hh.history['accuracy'])\nplt.plot(hh.history['val_accuracy'])\nplt.legend(['Training', 'Validation'])\nplt.title('Accuracy')\nplt.xlabel('Epochs')","80604239":"# store the weights of trained model on augmented images data\ntrained_weights = model.get_weights()","7eac8e9b":"learning_rate_reduction_2 = ReduceLROnPlateau(monitor='val_accuracy', \n                                            patience=3, \n                                            verbose=1, \n                                            factor=0.3, \n                                            min_lr=0.00001)","fa276a08":"K.clear_session()\n\n# to the previous trained model weights\nmodel.set_weights(trained_weights)\n#Now training on actual data and initial weights are adjusted to already traing model on augmented data\nh = model.fit(X_train, y_train_cat,\n                      batch_size = 64,\n                      validation_data=(X_test, y_test_cat),\n                      epochs=50,\n                      verbose=1, callbacks=[learning_rate_reduction_2])","720ddb12":"plt.plot(h.history['accuracy'])\nplt.plot(h.history['val_accuracy'])\nplt.legend(['Training', 'Validation'])\nplt.title('Accuracy')\nplt.xlabel('Epochs')","2dbea26a":"model.evaluate(X_test, y_test_cat)","4cdc4fff":"predictions = model.predict(test_data)","d15b4d80":"predictions.shape","c47e01b2":"pred =  np.argmax(predictions, axis = 1)\npred.shape","3b8cf39a":"sample = pd.read_csv('\/kaggle\/input\/digit-recognizer\/sample_submission.csv')","f463b3a2":"output = pd.DataFrame({'ImageId': sample.ImageId, 'Label': pred})\noutput.to_csv('my_submission.csv', index=False)\nprint(\"Your submission was successfully saved!\")","830585d1":"K.clear_session() is useful when you're creating multiple models in succession, such as during hyperparameter search or cross-validation. Each model you train adds nodes (potentially numbering in the thousands) to the graph. TensorFlow executes the entire graph whenever you (or Keras) call tf.Session.run() or tf.Tensor.eval(), so your models will become slower and slower to train, and you may also run out of memory. Clearing the session removes all the nodes left over from previous models, freeing memory and preventing slowdown.","3b574584":"## Now we will train second model using initial weights adjusted to previously trained model on augmented images data"}}