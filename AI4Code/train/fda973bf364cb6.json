{"cell_type":{"45473a79":"code","6cf1e6f1":"code","406db2b2":"code","d240dbd7":"code","b7565b80":"code","84287f73":"code","e4b2acc6":"code","48a05ba7":"code","cd55869a":"code","c5c8c2ae":"code","ce7f3a7c":"code","c15032b7":"code","cdd3912c":"code","73f406cb":"code","6033f86a":"code","c8fa5d9f":"code","bf049402":"code","318c27a0":"code","50ed4a5b":"code","0ca95751":"code","8dd6ba4b":"code","23728d84":"code","dc4ed177":"code","54193339":"code","8bd2918b":"code","f864e580":"markdown","ab3f4690":"markdown","60c859bc":"markdown","793bdc26":"markdown","3c94e3b8":"markdown","d7bf6bea":"markdown","3e34effe":"markdown","d375cc40":"markdown","d10ff55b":"markdown","c9881514":"markdown","8c668a1f":"markdown","dbcb580d":"markdown","f7c87f02":"markdown","2e00acae":"markdown","ea6706d1":"markdown"},"source":{"45473a79":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom sklearn import datasets\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.neighbors import KNeighborsClassifier\nimport seaborn as sns\nsns.set()\nimport matplotlib.pyplot as plt\nimport os","6cf1e6f1":"iris_data = pd.read_csv(\"..\/input\/Iris.csv\",index_col='Id')","406db2b2":"iris_data.info()","d240dbd7":"iris_data.describe()","b7565b80":"iris_data.head()","84287f73":"## Label encoding since the algorithms we are going to use do not take non numerical or boolean data as inputs\niris_data.Species.replace({'Iris-setosa':0,'Iris-versicolor':1, 'Iris-virginica':2},inplace=True)","e4b2acc6":"iris_data.head()","48a05ba7":"## null count analysis before modelling to keep check\nimport missingno as msno\np=msno.bar(iris_data)","cd55869a":"sns.countplot(y=iris_data.Species ,data=iris_data)\nplt.xlabel(\"Count of each Target class\")\nplt.ylabel(\"Target classes\")\nplt.show()","c5c8c2ae":"fig,ax = plt.subplots(nrows = 2, ncols=2, figsize=(16,10))\nrow = 0\ncol = 0\nfor i in range(len(iris_data.columns) -1):\n    if col > 1:\n        row += 1\n        col = 0\n    axes = ax[row,col]\n    sns.boxplot(x = iris_data['Species'], y = iris_data[iris_data.columns[i]],ax = axes)\n    col += 1\nplt.tight_layout()\n# plt.title(\"Individual Features by Class\")\nplt.show()","ce7f3a7c":"p=sns.pairplot(iris_data, hue = 'Species')","c15032b7":"plt.figure(figsize=(15,15))\np=sns.heatmap(iris_data.corr(), annot=True,cmap='RdYlGn') ","cdd3912c":"iris_data.hist(figsize=(15,12),bins = 15)\nplt.title(\"Features Distribution\")\nplt.show()","73f406cb":"X = iris_data.drop(['Species'],axis=1)\ny = iris_data.Species","6033f86a":"from sklearn.preprocessing import MinMaxScaler\nscaler = MinMaxScaler()\nX=scaler.fit_transform(X)","c8fa5d9f":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state=20, stratify=y)","bf049402":"knn = KNeighborsClassifier(7)\nknn.fit(X_train,y_train)\nprint(\"Train score before PCA\",knn.score(X_train,y_train),\"%\")\nprint(\"Test score before PCA\",knn.score(X_test,y_test),\"%\")","318c27a0":"from sklearn.decomposition import PCA\npca = PCA()\nX_new = pca.fit_transform(X)","50ed4a5b":"pca.get_covariance()","0ca95751":"explained_variance=pca.explained_variance_ratio_\nexplained_variance","8dd6ba4b":"with plt.style.context('dark_background'):\n    plt.figure(figsize=(6, 4))\n\n    plt.bar(range(4), explained_variance, alpha=0.5, align='center',\n            label='individual explained variance')\n    plt.ylabel('Explained variance ratio')\n    plt.xlabel('Principal components')\n    plt.legend(loc='best')\n    plt.tight_layout()","23728d84":"pca=PCA(n_components=3)\nX_new=pca.fit_transform(X)","dc4ed177":"X_train_new, X_test_new, y_train, y_test = train_test_split(X_new, y, test_size = 0.3, random_state=20, stratify=y)","54193339":"knn_pca = KNeighborsClassifier(7)\nknn_pca.fit(X_train_new,y_train)\nprint(\"Train score after PCA\",knn_pca.score(X_train_new,y_train),\"%\")\nprint(\"Test score after PCA\",knn_pca.score(X_test_new,y_test),\"%\")","8bd2918b":"# Visualising the Test set results\nclassifier = knn_pca\nfrom matplotlib.colors import ListedColormap\nX_set, y_set = X_test_new, y_test\nX1, X2 = np.meshgrid(np.arange(start = X_set[:, 0].min() - 1, stop = X_set[:, 0].max() + 1, step = 0.01),\n                     np.arange(start = X_set[:, 1].min() - 1, stop = X_set[:, 1].max() + 1, step = 0.01))\nplt.contourf(X1, X2, classifier.predict(np.array([X1.ravel(), X2.ravel(),np.zeros((X1.shape[0],X1.shape[1])).ravel()]).T).reshape(X1.shape),\n             alpha = 0.75, cmap = ListedColormap(('pink', 'lightgreen')))\nplt.xlim(X1.min(), X1.max())\nplt.ylim(X2.min(), X2.max())\nfor i, j in enumerate(np.unique(y_set)):\n    plt.scatter(X_set[y_set == j, 0], X_set[y_set == j, 1],\n                c = ListedColormap(('red', 'green'))(i), label = j)\nplt.title('KNN PCA (Test set)')\nplt.xlabel('PC1')\nplt.ylabel('PC2')\nplt.legend()\nplt.show()","f864e580":"# 2. With PCA","ab3f4690":"### Now some deeper explorations!","60c859bc":"### One of the biggest aims of these sort of plots and EDAs are to identify features that are not much helpful in explaining the target outcome. The SepalWidthCm feature seems to be less relevant in explaining the target class as compared to the other features","793bdc26":"# 1. Without PCA","3c94e3b8":"# Bivariate EDA","d7bf6bea":"# Univariate EDA","3e34effe":"#### The  histogram distributions don't seem to be ideal.","d375cc40":"# In this notebook we will take a look at one of the most reknowned dimensionality reduction techniques in machine learning and data science technology. Finally we will dig deep in the methodology so as to understand the exact working of the algorithm easily. \n# **Principle Component Analysis**","d10ff55b":"## How PCA works?","c9881514":"#### Early Insights : \n1. 150 rows\n2. 4 Independent variables to act as factors\n3. All have same units of measurement (cm)\n4. No missing data\n5. Three unique target classes namely : 'Iris-setosa', 'Iris-versicolor' and 'Iris-virginica'\n6. No class imbalance, all target classes have equal number of rows (50 each).","8c668a1f":"Implementing PCA on a 2-D Dataset\n### Step 1: Normalize the data\n\nFirst step is to normalize the data that we have so that PCA works properly. This is done by subtracting the respective means from the numbers in the respective column. So if we have two dimensions X and Y, all X become \ud835\udd01- and all Y become \ud835\udc9a-. This produces a dataset whose mean is zero.\n\n### Step 2: Calculate the covariance matrix\n\nSince the dataset we took is 2-dimensional, this will result in a 2x2 Covariance matrix.\n \n![](https:\/\/s3.amazonaws.com\/files.dezyre.com\/images\/Tutorials\/Covariance+Matrix.JPG) \n\n\nPlease note that Var[X1] = Cov[X1,X1] and Var[X2] = Cov[X2,X2].\n\n### Step 3: Calculate the eigenvalues and eigenvectors\n\nNext step is to calculate the eigenvalues and eigenvectors for the covariance matrix. The same is possible because it is a square matrix. \u019b is an eigenvalue for a matrix A if it is a solution of the characteristic equation:\n\ndet( \u019bI - A ) = 0\n\nWhere, I is the identity matrix of the same dimension as A which is a required condition for the matrix subtraction as well in this case and \u2018det\u2019 is the determinant of the matrix. For each eigenvalue \u019b, a corresponding eigen-vector v, can be found by solving:\n\n( \u019bI - A )v = 0\n\n### Step 4: Choosing components and forming a feature vector:\n\nWe order the eigenvalues from largest to smallest so that it gives us the components in order or significance. Here comes the dimensionality reduction part. If we have a dataset with n variables, then we have the corresponding n eigenvalues and eigenvectors. It turns out that the eigenvector corresponding to the highest eigenvalue is the principal component of the dataset and it is our call as to how many eigenvalues we choose to proceed our analysis with. To reduce the dimensions, we choose the first p eigenvalues and ignore the rest. We do lose out some information in the process, but if the eigenvalues are small, we do not lose much.","dbcb580d":"PCA finds the principal components of data.\n\nIt is often useful to measure data in terms of its principal components rather than on a normal x-y axis. So what are principal components then? They\u2019re the underlying structure in the data. They are the directions where there is the most variance, the directions where the data is most spread out. \n\n\nPCA finds a new set of dimensions (or a set of basis of views) such that all the dimensions are orthogonal (and hence linearly independent) and ranked according to the variance of data along them. It means more important principle\naxis occurs first. (more important = more variance\/more spread out data)\n\nHow does PCA work -\n\n* Calculate the covariance matrix X of data points.\n* Calculate eigen vectors and corresponding eigen values.\n* Sort the eigen vectors according to their eigen values in decreasing order.\n* Choose first k eigen vectors and that will be the new k dimensions.\n* Transform the original n dimensional data points into k dimensions.\n\n\nFor Eigen term details : http:\/\/setosa.io\/ev\/eigenvectors-and-eigenvalues\/","f7c87f02":"# Modelling","2e00acae":"Understanding PCA without visuals is difficult so I would strongly recommend watching this quick video after this notebook implementation.\n## [PCA in detail](https:\/\/www.youtube.com\/watch?v=_UVHneBUBW0&t=2s)","ea6706d1":"Many thanks to :\n* https:\/\/georgemdallas.wordpress.com\/2013\/10\/30\/principal-component-analysis-4-dummies-eigenvectors-eigenvalues-and-dimension-reduction\/\n* https:\/\/medium.com\/@aptrishu\/understanding-principle-component-analysis-e32be0253ef0\n* https:\/\/www.dezyre.com\/data-science-in-python-tutorial\/principal-component-analysis-tutorial"}}