{"cell_type":{"b591ad2b":"code","43e442e0":"code","734a1d22":"code","94be3d4e":"code","bc7973ce":"code","c225e811":"code","00cf82cd":"code","703ecc4f":"code","ff4953b1":"code","d05b1223":"code","551a4acf":"code","d5bf2070":"code","ca2d8f02":"code","f7efff47":"code","f5711e42":"code","0981db84":"code","617746b0":"code","86a38937":"code","7eb45b44":"code","fb6ad2b0":"code","3aa79eb2":"code","77d5378d":"code","28608989":"code","243a38fe":"code","3ae94a0b":"code","5aa582cf":"code","f5f84fdb":"code","deeb2fc0":"code","fe226f86":"code","ca2e477d":"code","c06b0af0":"code","e2f649a0":"code","bd407a38":"code","8d594b59":"code","1971b6a1":"code","4813ae6f":"code","ab194d5e":"code","1cd21735":"code","f3739edb":"code","92bd87c2":"code","38a64358":"code","24c1cc41":"code","e9a5a127":"code","26101bb7":"code","0653c030":"code","118c953a":"code","679e91b7":"code","8c9ac0a5":"code","2bb2b354":"code","69ba9c9a":"code","c4c43817":"code","23d80e7b":"code","53611a6f":"code","d34891fb":"code","05f8cdf1":"code","17833f01":"code","139678e0":"code","45c201a0":"code","400aca5f":"code","1e1ffc52":"code","c47077c1":"code","e06e3d42":"code","2082222c":"code","11d0e990":"code","c90cb3f6":"code","230bbc10":"code","782f1940":"code","7a2f66ce":"code","482e7574":"code","a3a0c558":"code","6e11de9a":"code","ee7c99e6":"code","5c1ac990":"code","01bbf34c":"code","361abc76":"code","001fd861":"code","e0a61d7f":"code","f18c531c":"code","f86671cc":"markdown","3f846e8a":"markdown","504f66e2":"markdown","b9778444":"markdown","55eaaadb":"markdown","a5760967":"markdown","230810de":"markdown","9a307efc":"markdown","3fc72c75":"markdown","eb1acd11":"markdown","18e5444b":"markdown"},"source":{"b591ad2b":"!pip install pendulum","43e442e0":"!pip install fbprophet","734a1d22":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport Geohash\nfrom matplotlib import pyplot as plt\nfrom math import sin, cos, sqrt, atan2, radians\nimport pendulum\nfrom scipy.interpolate import interpn\nfrom sklearn.preprocessing import LabelBinarizer , LabelEncoder, OneHotEncoder, MinMaxScaler\nimport os\nprint(os.listdir(\"..\/input\"))\n\nfrom numpy import split\nfrom numpy import array\nfrom sklearn.metrics import mean_squared_error\n\nimport utils\nimport gc # garbage collector\nimport seaborn as sns\n% matplotlib inline\nplt.style.use('seaborn-whitegrid')","94be3d4e":"import fbprophet\nfbprophet.__version__","bc7973ce":"from keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.layers import Flatten\nfrom keras.layers import LSTM\nfrom keras.layers import RepeatVector\nfrom keras.layers import TimeDistributed","c225e811":"import plotly\nplotly.__version__\nimport plotly.graph_objs as go\nfrom plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\ninit_notebook_mode(connected=True)","00cf82cd":"df = pd.read_csv(\"..\/input\/training.csv\")","703ecc4f":"# check for null values\ndf[df.isnull().any(axis=1)]","ff4953b1":"df.info()","d05b1223":"df.describe()","551a4acf":"df.head()","d5bf2070":"# get the unique hashes\ngeohashes_df = df.groupby('geohash6', as_index=False)\\\n.agg({'day':'count'})\\\n.rename(columns={'day':'count'})\\\n.sort_values(by='count', ascending=False)","ca2d8f02":"print(\"Total encoded hash\",len(geohashes_df))","f7efff47":"geohashes_df.sort_values(by='count', ascending=False).head()","f5711e42":"# find out the distribution of the hash\nax = sns.distplot(geohashes_df['count'], rug=True, hist=False)","0981db84":"%%time\n# decode geohash into lat and long\ngeohashes_df['lat'] = None\ngeohashes_df['lat_err'] = None\ngeohashes_df['long'] = None\ngeohashes_df['long_err'] = None\nfor i in range(len(geohashes_df)):\n    geo_decoded = Geohash.decode_exactly(geohashes_df.loc[i,'geohash6'])\n    geohashes_df.loc[i,'lat'] = geo_decoded[0]\n    geohashes_df.loc[i,'long'] = geo_decoded[1]\n    geohashes_df.loc[i,'lat_err'] = geo_decoded[2]\n    geohashes_df.loc[i,'long_err'] = geo_decoded[3]","617746b0":"# top 10 geo\ngeohashes_df.head(10)","86a38937":"# merge into original df\ndf = df.merge(geohashes_df.drop(columns=['count']), on='geohash6', how='inner')","7eb45b44":"# convert lat and long into float type\ndf['lat'] = df['lat'].astype('float64')\ndf['long'] = df['long'].astype('float64')\ndf['lat_err'] = df['lat_err'].astype('float64')\ndf['long_err'] = df['long_err'].astype('float64')","fb6ad2b0":"# extract hour and minute from timestamp column\ndf[['h','m']] = df['timestamp'].str.split(':',expand=True)\ndf['h'] = df['h'].astype('int64')\ndf['m'] = df['m'].astype('int64')","3aa79eb2":"# extract day of week (DoW) from day\n# since we have no idea about which month is this data were taken so can't\n# be sure the DoW starts on which day\n# but it's good enought we dont need to dig deeper I supposed\n# but in test set clearly this thing needs to be mapped out correcly\ndf['dow'] = df['day'] % 7","77d5378d":"df.head(1)","28608989":"# outlier from day\nzscore = lambda x: (x - x.mean()) \/ x.std()\ndf['zscore_day'] = np.abs(df.groupby('day')['demand'].transform(zscore))\nprint(\"number of suspected outliers from day\", len(df[df['zscore_day'] > 3]))","243a38fe":"# outlier from timestamp\ndf['zscore_timestamp'] = np.abs(df.groupby('timestamp')['demand'].transform(zscore))\nprint(\"number of suspected outliers from timestamp\", len(df[df['zscore_timestamp'] > 3]))","3ae94a0b":"_ = df[(df['zscore_day'] <= 3) & (df['zscore_timestamp'] <= 3)].groupby(['dow','h'], as_index=False)\\\n.agg({'demand':'mean'})\\\n.sort_values(by=['dow','h'])","5aa582cf":"_.head()","f5f84fdb":"data = [\n    go.Heatmap(\n        z=_['demand'].values.reshape((7,24)),\n        x=[str(i) for i in range(24)],\n        y=[str(i) for i in range(7)],\n        #colorscale='Viridis',\n    )\n]\n\nlayout = go.Layout(\n    title='Average Demand in Day of Week x 24 hours',\n    xaxis=dict(\n        title='Hours',\n    ),\n    yaxis=dict(\n        title='Day of Week',\n    ),\n)\n\nfig = go.Figure(data=data, layout=layout)\niplot(fig)","deeb2fc0":"# let's find out since 2018 which month has monday for their first day\nstart = pendulum.datetime(2018, 1, 1)\nend = pendulum.datetime(2019, 5, 22)\n\nperiod = pendulum.period(start, end)\n\nfor dt in period.range('days'):\n    if dt.day_of_week == pendulum.MONDAY and dt.day == 1:\n        print(dt.to_date_string(), \"number of days:\", dt.end_of('month').day-dt.start_of('month').day+1)","fe226f86":"_ = df.groupby('day', as_index=False).agg({'demand':'mean'})","ca2e477d":"x = [i for i in range(31)]\ntrace1 = go.Scatter(\n    x = x,\n    y = _['demand'][0:31],\n    mode = 'lines+markers',\n    name = 'First Month'\n)\ntrace2 = go.Scatter(\n    x = x,\n    y = _['demand'][31:61],\n    mode = 'lines+markers',\n    name = 'Second Month'\n)\n\ndata = [trace1, trace2]\niplot(data)","c06b0af0":"# demand distribution\ndf['demand'].hist(bins=100, figsize=(14,3))\nplt.xlabel('Demand normalized')\nplt.title('Histogram');","e2f649a0":"df.corr()","bd407a38":"def calc_dist(lat1, long1, lat2, long2):\n    # calculate distance between two coordinate using Haversine formula\n    # approximate radius of earth in km\n    R = 6373.0\n    lat1 = radians(lat1)\n    lon1 = radians(long1)\n    lat2 = radians(lat2)\n    lon2 = radians(long2)\n    dlon = lon2 - lon1\n    dlat = lat2 - lat1\n    a = sin(dlat \/ 2)**2 + cos(lat1) * cos(lat2) * sin(dlon \/ 2)**2\n    c = 2 * atan2(sqrt(a), sqrt(1 - a))\n    distance = R * c\n    return distance","8d594b59":"# minimum and maximum latitude\nlat_min, lat_max = df['lat'].min(), df['lat'].max()\n# minimum and maximum longitude\nlong_min, long_max = df['long'].min(), df['long'].max()","1971b6a1":"print(\"Min coordinate\", (lat_min, long_min))\nprint(\"Max coordinate\", (lat_max, long_max))","4813ae6f":"lat_A = lat_max\nlong_A = long_min\nlat_B = lat_min\nlong_B = long_max\ndA = calc_dist(lat_min, long_min, lat_A, long_A)\ndB = calc_dist(lat_min, long_min, lat_B, long_B)\ndiameter = calc_dist(lat_min, long_min, lat_max, long_max)","ab194d5e":"print(\"Circle Area\",np.pi * (diameter\/2)**2)\nprint(\"Square area\", dA*dB)","1cd21735":"# https:\/\/www.openstreetmap.org\/#map=4\/-6.61\/109.16\n# The bounding box in openstreetmap: left, bottom, right, top (min long, min lat, max long, max lat)\nbbox = (89.00, 119.31,-9.15, 10.17) # we'll plot long on x-axis\nsea_map = plt.imread(\"http:\/\/madet.my\/images\/map_sea_trim.png\")","f3739edb":"alpha=0.3\ns=1\nfig, ax = plt.subplots(1, 1, figsize=(16,10))\nax.scatter(df['long'],df['lat'], zorder=1, alpha=alpha, c='r', s=s)\nax.set_xlim((bbox[0], bbox[1]))\nax.set_ylim((bbox[2], bbox[3]))\nax.set_title('SEA Map (partial) with scatter points of demand geolocation')\nax.set_xlabel(\"Longitude\")\nax.set_ylabel(\"Latitude\")\nax.imshow(sea_map, zorder=0, extent=bbox)\nplt.show()","92bd87c2":"%%time\nx = df['lat'].values\ny = df['long'].values\nplt.figure(figsize=(14,8))\ndata , x_e, y_e = np.histogram2d( x, y)\nz = interpn( ( 0.5*(x_e[1:] + x_e[:-1]) , 0.5*(y_e[1:]+y_e[:-1]) ) , \n            data , np.vstack([x,y]).T , method = \"splinef2d\", bounds_error = False)\nidx = z.argsort()\nx, y, z = x[idx], y[idx], z[idx]\nplt.scatter(x, y, c=z, cmap='magma')\nplt.colorbar()\nplt.xlabel('Latitude')\nplt.ylabel('Longitude')\nplt.title('Density of demand by location');","38a64358":"# label encode the geohash6\n# though the best way is probably onehot encode, but we'll leave it to next iteration\nlabelencoder = LabelEncoder()\ndf['geo_encoded'] = labelencoder.fit_transform(df['geohash6'])","24c1cc41":"# @TODO encode the geo to onehot or binary\n# lb = LabelBinarizer(sparse_output=False)\n# lb.fit(df['geo_encoded'])\n# we'll transform geohash6 into onehot\n# lb.transform([212]).shape","e9a5a127":"# select only column we're interested in\ndf = df[['day','h', 'm', 'dow','geo_encoded','demand']]","26101bb7":"Prophet = fbprophet.Prophet","0653c030":"df_copy = df.copy()\ndf_copy['year'] = 2018\ndf_copy['day_'] = df_copy['day'].apply(lambda x : x if x <= 31 else x-31 )\ndf_copy['month'] = df_copy['day'].apply(lambda x : 10 if x <= 31 else 11 )\ndf_copy['s'] = 0\ndf_copy['ds'] = pd.to_datetime(dict(year=df_copy.year, month=df_copy.month, day=df_copy.day_, hour=df_copy.h, minute=df_copy.m))","118c953a":"# remove columns that are not be used in the model\ndf_copy.drop(columns=['day','h','m', 's', 'dow', 'year', 'day_', 's', 'month'], inplace=True)","679e91b7":"# fbprophet expect the y column\ndf_copy.rename(columns={'demand':'y'}, inplace=True)","8c9ac0a5":"df_copy.head()","2bb2b354":"#df_copy.groupby('geo_encoded').agg({'ds':'count'}).sort_values(by='ds', ascending=False).head()","69ba9c9a":"#df_copy = df_copy.sample(frac=0.1, random_state=1)\n# we'll take only one of the geohash area as taking entire dataset is extremely expensive\n# It would take 1h 28min 24s for training\n# so to save time I'll pick first 5 top areas\ndf_copy = df_copy[df_copy['geo_encoded'].isin([232,261,215,275,507])]","c4c43817":"#sort by datetime\ndf_copy = df_copy.sort_values(by=['ds'])","23d80e7b":"split_n = int(0.8*len(df_copy))\ntrain = df_copy.iloc[:split_n,:]\ntest = df_copy.iloc[split_n:,:]","53611a6f":"# mcmc_samples=300, changepoint_prior_scale=0.01 (enable this to get uncertainty bound)\nm = Prophet(seasonality_mode='multiplicative', \\\n            weekly_seasonality=True, \\\n            daily_seasonality=True)","d34891fb":"%%time\nm.add_regressor('geo_encoded', mode='multiplicative')\nm.fit(train)","05f8cdf1":"%%time\n#future = m.make_future_dataframe(periods=5, freq='min')\nfuture = train[['ds','geo_encoded']]\nforecast = m.predict(future)\nrmse = np.sqrt(mean_squared_error(train['y'], forecast['yhat']))\nprint(rmse)","17833f01":"%%time\n#future = m.make_future_dataframe(periods=5, freq='min')\nfuture = test[['ds','geo_encoded']]\nforecast = m.predict(future)\nrmse = np.sqrt(mean_squared_error(test['y'], forecast['yhat']))\nprint(rmse)","139678e0":"future = df_copy[['ds','geo_encoded']]\nforecast = m.predict(future)","45c201a0":"forecast['y'] = df_copy['y'].values\nforecast['yhat_lower'] =  np.clip(forecast.yhat_lower, 0, 1)\nforecast['yhat_upper'] =  np.clip(forecast.yhat_upper, 0, 1)","400aca5f":"f, ax = plt.subplots(figsize=(14, 8))\nfirst = forecast.iloc[:split_n,:]\nax.plot(first.ds, first.y, 'ko', markersize=3)\nax.plot(first.ds, first.yhat, color='steelblue', lw=0.5)\n#ax.fill_between(first.ds, first.yhat_lower, first.yhat_upper, color='steelblue', alpha=0.3)\n\nsecond = forecast.iloc[split_n:,:]\nax.plot(second.ds, second.y, 'ro', markersize=3)\nax.plot(second.ds, second.yhat, color='coral', lw=0.5)\n#ax.fill_between(second.ds, second.yhat_lower, second.yhat_upper, color='coral', alpha=0.3)\nax.axvline(str(second.iloc[0]['ds']), color='0.8', alpha=0.9)\nax.grid(ls=':', lw=0.5)","1e1ffc52":"# clean up \ndel df_copy\ndel forecast\ndel future\ngc.collect()","c47077c1":"# We'll prototype using 100k sample since executing full 4 millions dataset will slow down our step\n# once we have good model we can gradually increase the size and observe the accuracy.\ndf_sample = df.sample(100000, random_state=1)","e06e3d42":"df_sample.info()","2082222c":"df_sample = df_sample.sort_values(by=['day','h','m'])","11d0e990":"df_sample.head()","c90cb3f6":"# normalize features\nscaler = MinMaxScaler(feature_range=(0, 1))\ndf_sample[['day','h','m','dow','geo_encoded']] = scaler.fit_transform(df_sample[['day','h','m','dow','geo_encoded']])","230bbc10":"df_sample.shift(-1-i).values.shape","782f1940":"# frame as supervised learning\n# we're asked to make prediction T+1 to T+4 \n# I choose T-1 to T-16 for 4 hour lagging for our prediction\nX = []\nfor i in range(16):\n    X.append(df_sample.shift(-1-i).fillna(-1).values)\nX=np.array(X)","7a2f66ce":"X.shape","482e7574":"X=X.reshape(X.shape[1],X.shape[0],X.shape[2])","a3a0c558":"# split into train and test\nn_train = int(0.8*len(X))\nX_train = X[:n_train,:,:]\nX_test = X[n_train:,:,:]","6e11de9a":"X_train.shape, X_test.shape","ee7c99e6":"y = df_sample['demand'].values\ny_train = y[:n_train]\ny_test = y[n_train:]","5c1ac990":"model = Sequential()\nmodel.add(LSTM(units=30, return_sequences= True, input_shape=(X_train.shape[1],X_train.shape[2])))\nmodel.add(LSTM(units=30, return_sequences=True))\nmodel.add(LSTM(units=30))\nmodel.add(Dense(units=1))\nmodel.summary()","01bbf34c":"model.compile(optimizer='adam', loss='mean_squared_error')","361abc76":"model.fit(X_train, y_train, epochs=2, batch_size=128)","001fd861":"predicted_value= model.predict(X_test)","e0a61d7f":"mean_squared_error(y_test,predicted_value)","f18c531c":"### @todo ###\n### forward walking ####","f86671cc":"## Plot into Map\nHere we want to understand if we can use external data augmentation from the geo location. But turns out the location were masked into ocean. Unless grab is providing on demand hailing to Atlantist, I'm afraid we can't do much use the geo data for augmentation such as landmarks (point of interest).","3f846e8a":"# Introduction\nThis notebook demonstrate a machine learning model project prototyping from [Grab AI Challengle in Traffic Management](https:\/\/www.aiforsea.com\/traffic-management) as a starting kit.\n\n## Objective\n\n> Economies in Southeast Asia are turning to AI to solve traffic congestion, which hinders mobility and economic growth. The first step in the push towards alleviating traffic congestion is to understand travel demand and travel patterns within the city.\n> \n> Can we accurately forecast travel demand based on historical Grab bookings to predict areas and times with high travel demand?","504f66e2":"### Decoding geohash6\nWe will do a decoding into latitude and longitude back this encoding to find out more information about the the data location.\n> Geohash is a public domain geocoding system invented by Gustavo Niemeyer[1], which encodes a geographic location into a short string of letters and digits. It is a hierarchical spatial data structure which subdivides space into buckets of grid shape, which is one of the many applications of what is known as a Z-order curve, and generally space-filling curves (source: wikipedia).\n\nMore information about the geohash can be found in wikipedia: [wikipedia](https:\/\/en.wikipedia.org\/wiki\/Geohash): \n\n> So geohash6 means that it has 6 characters with the precision of\u2007\u00b10.61km (610m). I suspect landmark or point of interest is already compressed into this single encoding value.","b9778444":"# Baseline Models\n* This is a supervised learning problem where we can throw to the model the features as well as actual values to be learned. \n* Since we have multiple features to be used as forecasting step, it's fall under multivariate forecasting\n* We'll tackle using 2 simple approach here: \n  * Single variate time series forecasting and use parallel training (@todo)\n  * fbprohet multivariate through regressor\n  * Neural network\n  \n  \n* Here what I meant by baseline model is that there is no improvement after first time running the model. In the actual machine learning project one would usually have this similar approach and incrementally fine tune the model to produce better result such as finding best hyperparameter, adding more features, remove outliers, and etc\n* From my experience working with machine learning project major challenge usualy is overfit and imbalance class issues. If we look at the the geohash observation some only got few example so it could potentially overfit and produce far incorrect prediction in production environment. Another thing to observe is outliers, one scenario is sudden spike in certain areas due to a random events. We probably want to exclude this if such spike is not repeatable in seasonality pattern.\n  ","55eaaadb":"### Calculate the bounding area\n* The square area (KM) of the GPS bounding box is $ 1,170 km^2 $ . So I imagine the size close to Penang","a5760967":"### Timestamp and day\nThe timestamp is not a linux timestamp but rather a `hour:minute` form. Another thing we don't have is the context of  month and year. But we'll make an attempt to deduce. Knowing which month and year could help us in forming seasonality or holiday that potentially affecting the prediction","230810de":"# Data Exploratory and wrangling\nFirst step in machine learning is always about understanding the data","9a307efc":"## Fbprophet\nfbprophet is the easiest tool for forecasting where you can model your solution quickly and sometime it's already good enough.\n\n**UPDATE:** I did not include the 5 steps forward forecasting here, but we can easily use manual predict method and incrementally predict T+1 until T+5","3fc72c75":"### Analysis of the Geo data","eb1acd11":"# Neural Network (LSTM)\n\nWe'll try to build a Encoder-Decoder LSTM model for multi-step forecasting with multivariate input data using methods explained in [machinelearningmastery.com](https:\/\/machinelearningmastery.com\/how-to-develop-lstm-models-for-multi-step-time-series-forecasting-of-household-power-consumption\/)\n\nThe final average RMSE is significantly lower than fbprohet. Thought I didn't have time to verify the result. I'll leave it to later time.\n","18e5444b":"### Heatmap analysis\n* We have no context which month the day 0 is\n* But from heatmap I would speculate that the 0 is Monday \n* Since we can observe the stripe on 6th DoW noticably deviating from the rest, this could indicate Sunday\n* Looking at the \"First month\" vs \"second month\" plot, I suspect the dataset were derived from Malaysia and for October and November 2018."}}