{"cell_type":{"0cd2d707":"code","83be6c75":"code","517ee680":"code","f754d964":"code","4a575b3a":"code","12757ba8":"code","02c7041a":"code","d50a96b3":"code","d7749941":"code","94e77118":"code","82dd6257":"code","4990490f":"code","53315711":"code","986f156b":"code","4f5a8697":"code","f86fc25e":"code","7f2159cc":"code","586323cc":"code","658ffe36":"code","122ce1bc":"code","568580b0":"code","e6b05bfa":"code","d725cc66":"code","3bf5e8d7":"code","a40629b3":"code","2034ebb7":"code","117563c2":"code","d38d95f4":"code","ead463a1":"code","5297520f":"code","13da19d7":"code","654f32c1":"code","38314ca9":"code","cab37ebd":"code","52245101":"code","691d6b57":"code","c9feebb8":"code","2e346d03":"code","0caf64f0":"code","43602c6d":"code","c396b8ff":"code","03c82650":"code","f36a16e2":"code","60ef02b2":"code","aa5c6eba":"code","24ee6d62":"code","858e93aa":"code","9f0da22f":"code","fdc81e14":"code","a66b8446":"code","dcfdf3cf":"code","6efb250b":"code","2a3abff0":"code","dfcf3b1c":"code","fc208476":"code","81237253":"code","c559c492":"code","fde385e2":"code","b41558f5":"code","5206723f":"code","f5a2f530":"code","f2c19341":"code","e8c8edd1":"code","61bf3c69":"code","08f35b45":"code","2d892041":"code","9451f14d":"code","1fb2455b":"code","3af821c9":"code","ae0f14ec":"code","07fc9994":"code","61eb3eaf":"code","6ff1f2eb":"code","928b7786":"code","a8e31121":"code","3b5cf5e2":"code","37174fb7":"code","23f59e8e":"code","9e4fabb8":"code","fac22a59":"markdown","2218ae24":"markdown","7480a79f":"markdown","7dab85dd":"markdown","62e42960":"markdown","facfac01":"markdown","ed89b0f0":"markdown","791128fa":"markdown","9341773f":"markdown","ebc28f50":"markdown","3018f26e":"markdown","7cc8ea12":"markdown","66b275f1":"markdown","c3b489d1":"markdown","6cd420c1":"markdown","7dda15d6":"markdown","83f92358":"markdown","9b33a4ab":"markdown","74c9cedc":"markdown","eb7f73cf":"markdown","0f5e53b3":"markdown","756f2216":"markdown","a9c199fe":"markdown","1442e9d7":"markdown","1174f45f":"markdown","bacc2b83":"markdown","b41fd3e8":"markdown","7de74148":"markdown","a55c2bdd":"markdown","a15d75f9":"markdown","2d3ac3e9":"markdown","62ec74da":"markdown","222a487e":"markdown","789b4633":"markdown","2e7ca215":"markdown","0becc612":"markdown","e111175b":"markdown","45a2cee8":"markdown","fa899e20":"markdown","b81e10c7":"markdown","04bc3921":"markdown"},"source":{"0cd2d707":"import numpy as np \n\nimport pandas as pd \n\nimport os\n\nimport seaborn as sns\n\nfrom scipy.stats import chi2_contingency\nfrom scipy.stats import chisquare\nimport scipy.stats as ss\nfrom scipy.stats import randint as sp_randint\nfrom scipy.stats import uniform as sp_uniform\n\nimport matplotlib.pyplot as plt\nplt.style.use('tableau-colorblind10')\ncolor_pal = [x['color'] for x in plt.rcParams['axes.prop_cycle']]\n%matplotlib inline\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nfrom sklearn.preprocessing import minmax_scale\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score,roc_auc_score\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn import preprocessing \nfrom sklearn.model_selection import GridSearchCV,RandomizedSearchCV\nfrom sklearn.model_selection import KFold\n\nimport datetime\n\nfrom lightgbm import LGBMClassifier\n\n\n","83be6c75":"## Function to reduce the DF size\ndef reduce_mem_usage(df, verbose=True):\n    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    start_mem = df.memory_usage().sum() \/ 1024**2    \n    for col in df.columns:\n        col_type = df[col].dtypes\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)    \n    end_mem = df.memory_usage().sum() \/ 1024**2\n    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) \/ start_mem))\n    return df","517ee680":"df_transaction = pd.read_csv(\"..\/input\/ieee-fraud-detection\/train_transaction.csv\")\n\n# Check for Missing  data\ntotal = df_transaction.isnull().sum()\nprint(\"NaN values in Transaction database\",total)\n\n# Unique Values\nUniqueID = df_transaction['TransactionID'].nunique()\nprint(\"No of unique Transaction ID's\",UniqueID)\n\n# Columns in dataframe\nprint(\"Columns in dataframe\",df_transaction.columns)\n\n","f754d964":"# Columns datatypes\nstring_columns = df_transaction.select_dtypes('object').columns.tolist()\nprint(\"Object Columns\",string_columns)\n\nnumeric_columns = df_transaction.select_dtypes(include=np.number).columns.tolist()\nprint(\"Numerical columns \",numeric_columns)","4a575b3a":"# Memory Reduction\ndf_transaction = reduce_mem_usage(df_transaction, verbose=True)\nprint(df_transaction.head(10))","12757ba8":"# Check for Missing Data\n\ndf_transaction = df_transaction.replace(np.nan, 'NaN', regex=True)\n\nlst = string_columns\ndata = df_transaction[df_transaction.columns.intersection(lst)]\n\n","02c7041a":"# Check for M variables since they have a high number of NaN vales\nM = data.filter(regex='^M',axis=1)\nM.head()","d50a96b3":"columns = M.columns\n\n#df_trans = df_transaction.copy()\n\nfor col in columns:   \n    \n        \n    df_trans = df_transaction[df_transaction.isFraud==1][col]  \n    \n    \n    plt.figure(figsize=(14,10))\n    \n    g1 = sns.countplot(x=df_trans)\n    \n    g1.set_title(\"Frequeny of \" + str(col)+ \" by Fraud Values\", fontsize=19)\n \n    g1.set_xticklabels(g1.get_xticklabels(),rotation=45)\n    plt.show()\n  \n    del g1\n    \n    \n    \n    \ndel df_trans \n","d7749941":"# Columns ProductCD, card4, card6\ncolumns = ['ProductCD', 'card4', 'card6']\n\nfor col in columns:\n    \n    df_trans = df_transaction[df_transaction.isFraud==1][col]  \n    \n    \n    plt.figure(figsize=(14,10))\n    \n    g1 = sns.countplot(x=df_trans)\n    \n    g1.set_title(\"Frequeny of \" + str(col)+ \" by Fraud Values\", fontsize=19)\n \n    g1.set_xticklabels(g1.get_xticklabels(),rotation=45)\n    plt.show()\n  \n    del g1","94e77118":"# Columns other than M\ncolumns = ['P_emaildomain', 'R_emaildomain']\n\n\nfor col in columns:\n    \n    df_trans = df_transaction[df_transaction.isFraud==1][col]  \n    \n    \n    plt.figure(figsize=(14,10))\n    \n    g1 = sns.countplot(x=df_trans)\n    \n    g1.set_title(\"Frequeny of \" + str(col)+ \" by Fraud Values\", fontsize=19)\n \n    g1.set_xticklabels(g1.get_xticklabels(),rotation=45)\n    plt.show()\n  \n    del g1\n    ","82dd6257":"# Cramers V for categorical correlations\ndef cramers_v(x, y):\n    x = np.array(x)\n    y = np.array(y)\n    confusion_matrix = pd.crosstab(x, y)\n    chi2 = ss.chi2_contingency(confusion_matrix)[0]\n    n = confusion_matrix.sum().sum()\n    phi2 = chi2\/n\n    r,k = confusion_matrix.shape\n    phi2corr = max(0, phi2-((k-1)*(r-1))\/(n-1))\n    rcorr = r-((r-1)**2)\/(n-1)\n    kcorr = k-((k-1)**2)\/(n-1)\n    return np.sqrt(phi2corr\/min((kcorr-1),(rcorr-1)))\n\n\ncramersv = pd.DataFrame(index=data.columns,columns=data.columns)\ncolumns = data.columns\n\nfor i in range(0,len(columns)):\n    for j in range(0,len(columns)):\n        #print(data[columns[i]].tolist())\n        u = cramers_v(data[columns[i]].tolist(),data[columns[j]].tolist())\n        cramersv.loc[columns[i],columns[j]] = u\n        \ncramersv.fillna(value=np.nan,inplace=True)","4990490f":"plt.figure(figsize=(15,15))\nsns.heatmap(cramersv, annot=True)\nplt.show()","53315711":"# Reduce Memory\ndel cramersv,data","986f156b":"df_transaction.drop(columns=M.columns,inplace=True)\nprint(df_transaction.head(10))\ndel M","4f5a8697":"print(df_transaction.columns)","f86fc25e":"# Too many V variables. We can do a correlation analysis and remove columns that are highly correlated\nV = df_transaction.filter(regex='^V',axis=1)\n\nprint(len(V))","7f2159cc":"# Create correlation matrix\n\nV = V.applymap(float)\ncorr_matrix = V.corr().abs()\n\n\n# Select upper triangle of correlation matrix\nupper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))\n\n# Find index of feature columns with correlation greater than 0.95\nto_drop = [column for column in upper.columns if any(upper[column] > 0.95)]\n\nprint(to_drop)\nHighCorrVDrop = to_drop\n\ndel corr_matrix, upper","586323cc":"# Let's look at other columns\ndf_transaction.drop(columns = V.columns,inplace=True)\ndel V","658ffe36":"# Let's look at transaction amt\n\ndf_transaction['TransactionAmt'] = df_transaction['TransactionAmt'].apply(pd.to_numeric)\nf, ax = plt.subplots(1, 1,figsize=(15,15))\n\nsns.distplot(ax= ax,a = df_transaction[df_transaction['isFraud']==0]['TransactionAmt'], color=\"skyblue\", label=\"is Not Fraud\")\n\n\nsns.distplot(ax=ax,a = df_transaction[df_transaction['isFraud']==1]['TransactionAmt'], color=\"red\", label=\"is Fraud\")\nplt.legend(labels=['is Not Fraud', 'is Fraud'])\nax.set_xlabel(\"Transaction Amt\")\n\nplt.show()\n\n","122ce1bc":"# Data heavily skewed. Apply log of the data\n\nf, ax = plt.subplots(1, 1,figsize=(15,15))\n\nsns.distplot(ax= ax,a = np.log(df_transaction[df_transaction['isFraud']==0]['TransactionAmt']), color=\"skyblue\", label=\"is Not Fraud\")\n\n\nsns.distplot(ax=ax,a = np.log(df_transaction[df_transaction['isFraud']==1]['TransactionAmt']), color=\"red\", label=\"is Fraud\")\nplt.legend(labels=['is Not Fraud', 'is Fraud'])\nax.set_xlabel(\"Log of Transaction Amt\")\nplt.show()","568580b0":"# Find which of the columns have the highest frquency among fake transactions\n\ndf_trans = df_transaction[df_transaction['isFraud']==1]\ndf_tran = df_transaction.copy()\nC = df_transaction.filter(regex='^C',axis=1)\n\n\n\nfor col in C.columns:\n   \n    index_list = df_trans[col].value_counts(ascending=False)[:15].index.to_list()\n    df_tran.loc[df_tran[(~df_tran[col].isin(index_list))].index, col] = \"Others\"\n    \n    \ndel df_trans","e6b05bfa":"columns = C.columns\n\n\nfor col in columns:\n    \n    df_trans = df_tran[df_tran.isFraud==1][col]  \n    \n    \n    plt.figure(figsize=(14,10))\n    \n    g1 = sns.countplot(x=df_trans)\n    \n    g1.set_title(\"Frequeny of \" + str(col)+ \" by Fraud Values\", fontsize=19)\n \n    g1.set_xticklabels(g1.get_xticklabels(),rotation=45)\n    plt.show()\n  \n    del g1\n    \ndel C","d725cc66":"# Find which of the columns have the highest frquency among fake transactions\n# Due to high presence of many features. Let's pick only the top features by frequency for plotting\n\ndf_trans = df_transaction[df_transaction['isFraud']==1]\ndf_tran = df_transaction.copy()\nD = df_transaction.filter(regex='^D',axis=1)\n\n\n\nfor col in D.columns:\n   \n    index_list = df_trans[col].value_counts(ascending=False)[:15].index.to_list()\n    df_tran.loc[df_tran[(~df_tran[col].isin(index_list))].index, col] = \"Others\"\n    \n    \ndel df_trans","3bf5e8d7":"columns = D.columns\n\n\nfor col in columns:\n    \n    df_trans = df_tran[df_tran.isFraud==1][col]  \n    \n    \n    plt.figure(figsize=(14,10))\n    \n    g1 = sns.countplot(x=df_trans)\n    \n    g1.set_title(\"Frequeny of \" + str(col)+ \" by Fraud Values\", fontsize=19)\n \n    g1.set_xticklabels(g1.get_xticklabels(),rotation=45)\n    plt.show()\n  \n    del g1\n    \ndel D\n","a40629b3":"# Find which of the columns have the highest frquency among fake transactions\n# Due to high presence of many features. Let's pick only the top features by frquency for plotting\n\ndf_trans = df_transaction[df_transaction['isFraud']==1]\ndf_tran = df_transaction.copy()\nD = df_transaction[['addr1','addr2','dist1','dist2']]\n\n\n\nfor col in D.columns:\n   \n    index_list = df_trans[col].value_counts(ascending=False)[:15].index.to_list()\n    df_tran.loc[df_tran[(~df_tran[col].isin(index_list))].index, col] = \"Others\"\n    \n    \ndel df_trans","2034ebb7":"columns = D.columns\n\nfor col in columns:\n    \n    df_trans = df_tran[df_tran.isFraud==1][col]  \n    \n    \n    plt.figure(figsize=(14,10))\n    \n    g1 = sns.countplot(x=df_trans)\n    \n    g1.set_title(\"Frequeny of \" + str(col)+ \" by Fraud Values\", fontsize=19)\n \n    g1.set_xticklabels(g1.get_xticklabels(),rotation=45)\n    plt.show()\n  \n    del g1\n    \ndel D","117563c2":"# Find which of the columns have the highest frquency among fake transactions\n# Due to high presence of many features. Let's pick only the top features by frquency for plotting\n\ndf_trans = df_transaction[df_transaction['isFraud']==1]\ndf_tran = df_transaction.copy()\nD = df_transaction[['card1','card2','card3','card5']]\n\n\n\nfor col in D.columns:\n   \n    index_list = df_trans[col].value_counts(ascending=False)[:15].index.to_list()\n    df_tran.loc[df_tran[(~df_tran[col].isin(index_list))].index, col] = \"Others\"\n    \n    \ndel df_trans","d38d95f4":"columns = D.columns\n\nfor col in columns:\n    \n    df_trans = df_tran[df_tran.isFraud==1][col]  \n    \n    \n    plt.figure(figsize=(14,10))\n    \n    g1 = sns.countplot(x=df_trans)\n    \n    g1.set_title(\"Frequeny of \" + str(col)+ \" by Fraud Values\", fontsize=19)\n \n    g1.set_xticklabels(g1.get_xticklabels(),rotation=45)\n    plt.show()\n  \n    del g1\n    \ndel D","ead463a1":"START_DATE = '2017-12-01'\nstartdate = datetime.datetime.strptime(START_DATE, \"%Y-%m-%d\")\ndf_transaction[\"date\"] = df_transaction['TransactionDT'].apply(lambda x: (startdate + datetime.timedelta(seconds=x)))\n\ndf_transaction['Weekdays'] = df_transaction['date'].dt.dayofweek\ndf_transaction['Hours'] = df_transaction['date'].dt.hour\ndf_transaction['Days'] = df_transaction['date'].dt.day\n\n","5297520f":"columns = ['Weekdays','Hours','Days']\n\nfor col in columns:\n    \n    df_trans = df_transaction[df_transaction.isFraud==1][col]  \n    \n    \n    plt.figure(figsize=(14,10))\n    \n    g1 = sns.countplot(x=df_trans)\n    \n    g1.set_title(\"Frequeny of \" + str(col)+ \" by Fraud Values\", fontsize=19)\n \n    g1.set_xticklabels(g1.get_xticklabels(),rotation=45)\n    plt.show()\n  \n    del g1\n    \n","13da19d7":"truncated_df = df_transaction[['TransactionID','isFraud']]\ndel df_transaction","654f32c1":"df_identity = pd.read_csv(\"..\/input\/ieee-fraud-detection\/train_identity.csv\")","38314ca9":"# Name of columns and null values among them\n\nprint(df_identity.columns)\n\nprint(\"NaN values : \",df_identity.isna().sum())","cab37ebd":"# Let's look at the data in the ID subset folder\n\nID = df_identity.filter(regex='^id',axis=1)\nID.head()","52245101":"print(ID.iloc[:,0:10])","691d6b57":"print(ID.iloc[:,10:20])","c9feebb8":"print(ID.iloc[:,20:30])","2e346d03":"print(ID.iloc[:,30:40])","0caf64f0":"# Merge the targets and identity database for plotting purpose\n\ndf_identity = pd.merge(truncated_df,df_identity, how='left',on='TransactionID')\ndf_identity.reset_index(inplace=True)","43602c6d":"columns = ['id_34','id_35','id_36','id_37','id_38']\n\nfor col in columns:\n    \n    df_iden = df_identity[df_identity.isFraud==1][col]  \n    \n    \n    plt.figure(figsize=(14,10))\n    \n    g1 = sns.countplot(x=df_iden)\n    \n    g1.set_title(\"Frequeny of \" + str(col)+ \" by Fraud Values\", fontsize=19)\n \n    g1.set_xticklabels(g1.get_xticklabels(),rotation=45)\n    plt.show()\n  \n    del g1\n    ","c396b8ff":"columns = ['id_12','id_15','id_16','id_28','id_29']\n\nfor col in columns:\n    \n    df_iden = df_identity[df_identity.isFraud==1][col]  \n    \n    \n    plt.figure(figsize=(14,10))\n    \n    g1 = sns.countplot(x=df_iden)\n    \n    g1.set_title(\"Frequeny of \" + str(col)+ \" by Fraud Values\", fontsize=19)\n \n    g1.set_xticklabels(g1.get_xticklabels(),rotation=45)\n    plt.show()\n  \n    del g1\n    \n \n    ","03c82650":"# ID columns with high number of NaN values\n\ncolumns =['id_07', 'id_08', 'id_21', 'id_22', 'id_23', 'id_24', 'id_25', 'id_26', 'id_27']\n\n\nfor col in columns:\n    \n    df_iden = df_identity[df_identity.isFraud==1][col]  \n    \n    \n    plt.figure(figsize=(14,10))\n    \n    g1 = sns.countplot(x=df_iden)\n    \n    g1.set_title(\"Frequeny of \" + str(col)+ \" by Fraud Values\", fontsize=19)\n \n    g1.set_xticklabels(g1.get_xticklabels(),rotation=45)\n    plt.show()\n  \n    del g1","f36a16e2":"# Further analysis of id_23\n\n  \nplt.figure(figsize=(14,10))\n    \ng1 = sns.countplot(x='id_23',hue='isFraud',data=df_identity)\n    \ng1.set_title(\"Frequeny of \" + \"id_23\" + \" by Fraud Values\", fontsize=19)\n \ng1.set_xticklabels(g1.get_xticklabels(),rotation=45)\nplt.show()\n\ndel g1","60ef02b2":"# Find which of the columns have the highest frquency among fake transactions\n# Due to high presence of many features. Let's pick only the top features by frquency for plotting\n\ndf_ident =df_identity[df_identity['isFraud']==1]\ndf_iden = df_identity.copy()\nD = df_identity[['id_30','id_31','id_33','DeviceInfo']]\n\n\n\nfor col in D.columns:\n   \n    index_list = df_ident[col].value_counts(ascending=False)[:15].index.to_list()\n    df_iden.loc[df_iden[(~df_iden[col].isin(index_list))].index, col] = \"Others\"\n    \n    \ndel df_ident","aa5c6eba":"columns = D.columns\n\n\nfor col in columns:\n    \n    df_ident = df_iden[df_iden.isFraud==1][col]  \n    \n    \n    plt.figure(figsize=(14,10))\n    \n    g1 = sns.countplot(x=df_ident)\n    \n    g1.set_title(\"Frequeny of \" + str(col)+ \" by Fraud Values\", fontsize=19)\n \n    g1.set_xticklabels(g1.get_xticklabels(),rotation=45)\n    plt.show()\n  \n    del g1\n    \ndel df_iden\n\n  \n","24ee6d62":"df_trans = pd.read_csv(\"..\/input\/ieee-fraud-detection\/train_transaction.csv\")\ndf_identity = pd.read_csv(\"..\/input\/ieee-fraud-detection\/train_identity.csv\")","858e93aa":"# Data Merging\ndf = pd.merge(df_trans,df_identity, how='left',on='TransactionID')\ndf.reset_index(inplace=True)\ndel df_trans,df_identity","9f0da22f":"df.drop(columns=['index'],inplace=True)","fdc81e14":"def pick_high_freq_only(df,df2 = pd.DataFrame(),second=False):\n    \n    # Find which of the columns have the highest frquency among fake transactions\n    \n    # Device Info\n    d = df[df['isFraud']==1]\n    dnf = df[df['isFraud']==0]\n    columns  = ['DeviceInfo']\n\n\n    for col in columns:\n   \n        index_list1 = d[col].value_counts(ascending=False)[:100].index.to_list()\n        index_list2 = dnf[col].value_counts(ascending=False)[:100].index.to_list()\n    \n        index_list = index_list1 + index_list2\n        df.loc[df[(~df[col].isin(index_list))].index, col] = \"Others\"\n    \n        if second == True:\n            df2.loc[df2[(~df2[col].isin(index_list))].index, col] = \"Others\"\n    \n    \n    del d,dnf\n    \n    # Email Domains\n    d = df[df['isFraud']==1]\n    dnf = df[df['isFraud']==0]\n    columns  = ['P_emaildomain', 'R_emaildomain']\n\n\n    for col in columns:\n   \n        index_list1 = d[col].value_counts(ascending=False)[:20].index.to_list()\n        index_list2 = dnf[col].value_counts(ascending=False)[:20].index.to_list()\n    \n        index_list = index_list1 + index_list2\n        df.loc[df[(~df[col].isin(index_list))].index, col] = \"Others\"\n        \n        if second == True:\n            df2.loc[df2[(~df2[col].isin(index_list))].index, col] = \"Others\"\n            \n    \n    \n    \n    del d,dnf\n    \n    # Selected ID's\n    d = df[df['isFraud']==1]\n    dnf = df[df['isFraud']==0]\n    columns = ['id_30','id_31','id_33']\n\n    for col in columns:\n   \n        index_list1 = d[col].value_counts(ascending=False)[:100].index.to_list()\n        index_list2 = dnf[col].value_counts(ascending=False)[:100].index.to_list()\n    \n        index_list = index_list1 + index_list2\n        df.loc[df[(~df[col].isin(index_list))].index, col] = \"Others\"\n        \n        if second == True:\n            df2.loc[df2[(~df2[col].isin(index_list))].index, col] = \"Others\"\n    \n    \n    del d,dnf\n    \n    return df,df2\n\n\n\n\ndf,_ = pick_high_freq_only(df)\n    \n#print(df)  ","a66b8446":"# Seggregate into features and target\n\nFeatures = df\nFeatures = Features.drop(columns=['isFraud','TransactionID'])\nTarget = df['isFraud'].astype(float)\nTarget = Target.values\nTarget = Target.reshape((len(Target), 1))\n\ndel df","dcfdf3cf":"def FETransactionDT(Features):\n    \n    #print(Features)\n    START_DATE = '2017-12-01'\n    startdate = datetime.datetime.strptime(START_DATE, \"%Y-%m-%d\")\n    Features[\"date\"] = Features['TransactionDT'].apply(lambda x: (startdate + datetime.timedelta(seconds=x)))\n\n    Features['Weekdays'] = Features['date'].dt.dayofweek\n    Features['Hours'] = Features['date'].dt.hour\n    Features['Days'] = Features['date'].dt.day\n\n    Features['Weekdays'] = pd.to_numeric(Features['Weekdays'])\n    Features['Hours'] = pd.to_numeric(Features['Hours'])\n    Features['Days'] = pd.to_numeric(Features['Days'])\n\n    Features.drop(columns=[\"date\"],inplace=True)\n    \n    return Features\n    \nFeatures = FETransactionDT(Features)\n","6efb250b":"def HighNaNcol(Features):\n    \n    HighNaN = []\n    for col in Features.columns:\n    \n        no = Features[col].isna().sum()\n    \n        if no > 0.95 * len(Features):\n            HighNaN.append(col)\n            \n    print(HighNaN)\n    return HighNaN\n\n#HighNaN = HighNaNcol(Features)\n","2a3abff0":"# Features to drop \ncolumns_to_drop = HighCorrVDrop\n\n\n#Features.drop(columns=HighNaN,inplace=True)\nFeatures.drop(columns = columns_to_drop,inplace=True)","dfcf3b1c":"test_size = 0.2\nX_train, X_test, y_train, y_test = train_test_split(Features, Target, test_size=test_size,random_state=32)","fc208476":"def ScaleColumns(X_train,X_test):\n    for col in X_train.columns:\n        if X_train[col].dtype=='object' or X_test[col].dtype=='object': \n            print(col)\n            lbl = preprocessing.LabelEncoder()\n        \n            X_train[col] = X_train[col].astype(str)\n            X_test[col] = X_test[col].astype(str)\n        \n            X_train[col] = lbl.fit_transform(X_train[col])\n            X_test[col] = lbl.transform(X_test[col])  \n        \n        if col == 'TransactionAmt':\n            \n            print(col)\n            SS = preprocessing.StandardScaler()\n        \n            X_train_arr = X_train[col].astype(float).values\n            X_test_arr = X_test[col].astype(float).values   \n        \n            \n            X_train_arr = SS.fit_transform(X_train_arr.reshape(-1,1))\n            X_test_arr = SS.transform(X_test_arr.reshape(-1,1))\n            \n            X_train[col]  = X_train_arr \n            X_test[col]   = X_test_arr\n            \n    return X_train, X_test\n            \nX_train, X_test = ScaleColumns(X_train,X_test)","81237253":"\n#fit_params={\"early_stopping_rounds\":50, \n           # \"eval_metric\" : 'auc', \n           #\"eval_set\" : [(X_test,y_test)],\n           #'eval_names': ['valid'],\n           # 'verbose': 100,\n           #'categorical_feature': 'auto'}\n\n#param_test ={  'n_estimators': [400, 700, 1000],\n  #'colsample_bytree': [0.7, 0.8],\n  # 'max_depth': [15,20,25],\n   #'num_leaves': [50, 100, 200],\n  # 'reg_alpha': [1.1, 1.2, 1.3],\n # 'reg_lambda': [1.1, 1.2, 1.3],\n # 'min_split_gain': [0.3, 0.4],\n #'subsample': [0.7, 0.8, 0.9],\n  # 'subsample_freq': [20]}","c559c492":"#clf = LGBMClassifier(random_state=314, silent=True, metric='None', n_jobs=2)\n#model = RandomizedSearchCV(\n  # estimator=clf, param_distributions=param_test, \n   # scoring='roc_auc',\n   # cv=3,\n    #refit=True,\n   # random_state=314,\n   # verbose=True)","fde385e2":"#model.fit(X_train, y_train, **fit_params)\n#print('Best score reached: {} with params: {} '.format(model.best_score_, model.best_params_))","b41558f5":"\nmodel = LGBMClassifier(subsample_freq= 20,subsample= 0.7, reg_lambda= 1.2, reg_alpha = 1.1, \n                       num_leaves = 200, n_estimators = 700, \n                       min_split_gain = 0.3, max_depth = 25, colsample_bytre = 0.8,random_state=314,n_jobs=2)\n\nmodel.fit(X_train, y_train)","5206723f":"# make predictions for test data\ny_pred = model.predict_proba(X_test)","f5a2f530":"# evaluate predictions\nscore = roc_auc_score(y_test, y_pred[:,1])\nprint(\"ROC_AUC score: %.2f%%\" % (score * 100.0))","f2c19341":"df_train_trans = pd.read_csv(\"..\/input\/ieee-fraud-detection\/train_transaction.csv\")\ndf_train_identity = pd.read_csv(\"..\/input\/ieee-fraud-detection\/train_identity.csv\")\n\n# Data Merging\ndf_train = pd.merge(df_train_trans,df_train_identity , how='left',on='TransactionID')\ndf_train.reset_index(inplace=True)\ndel df_train_trans,df_train_identity","e8c8edd1":"df_test_trans = pd.read_csv(\"..\/input\/ieee-fraud-detection\/test_transaction.csv\")\ndf_test_identity = pd.read_csv(\"..\/input\/ieee-fraud-detection\/test_identity.csv\")\n\n# Data Merging\ndf_test = pd.merge(df_test_trans,df_test_identity , how='left',on='TransactionID')\ndf_test.reset_index(inplace=True)\ndel df_test_trans,df_test_identity","61bf3c69":"df_train.drop(columns=['index'],inplace=True)\ndf_test.drop(columns=['index'],inplace=True)","08f35b45":"df_test.columns = df_test.columns.str.replace(\"-\", \"_\")\nprint(df_test.columns)","2d892041":"df_train,df_test = pick_high_freq_only(df_train,df_test,second=True)","9451f14d":"# Seggregate into features and target\n\nFeatures_train = df_train\nFeatures_train = Features_train.drop(columns=['isFraud'])\nTarget_train = df_train['isFraud']\nTarget_train = Target_train.values\nTarget_train = Target_train.reshape((len(Target_train), 1))\n\ndel df_train","1fb2455b":"Features_test = df_test\n\ndel df_test","3af821c9":"Features_train = FETransactionDT(Features_train)\nFeatures_test = FETransactionDT(Features_test)","ae0f14ec":"HighNaN = HighNaNcol(Features_train)","07fc9994":"# Features to drop \ncolumns_to_drop = HighCorrVDrop\n\n#Features_train.drop(columns=HighNaN,inplace=True)\nFeatures_train.drop(columns = columns_to_drop,inplace=True)\n\n#Features_test.drop(columns=HighNaN,inplace=True)\nFeatures_test.drop(columns = columns_to_drop,inplace=True)","61eb3eaf":"Features_train, Features_test = ScaleColumns(Features_train, Features_test)","6ff1f2eb":"Features_train = Features_train.drop(columns=['TransactionID'])\nFeatures_test_without_ID = Features_test.drop(columns=['TransactionID'])","928b7786":"\nmodel = LGBMClassifier(subsample_freq= 20,subsample= 0.7, reg_lambda= 1.2, reg_alpha = 1.1, \n                       num_leaves = 200, n_estimators = 700, \n                       min_split_gain = 0.3, max_depth = 25, colsample_bytre = 0.8,random_state=314,n_jobs=2)\n\nmodel.fit(Features_train, Target_train)","a8e31121":"# make predictions for test data\ny_pred = model.predict_proba(Features_test_without_ID)\n","3b5cf5e2":"print(y_pred)","37174fb7":"submission = pd.DataFrame()\nsubmission['TransactionID'] = Features_test['TransactionID']\nsubmission['isFraud'] = y_pred[:,1]\n\nprint(submission)\n","23f59e8e":"submission.to_csv('submission.csv', index=False)","9e4fabb8":"def plotImp(model, X , num = 20):\n    feature_imp = pd.DataFrame({'Value':model.feature_importances_,'Feature':X.columns})\n    plt.figure(figsize=(40, 20))\n    sns.set(font_scale = 5)\n    sns.barplot(x=\"Value\", y=\"Feature\", data=feature_imp.sort_values(by=\"Value\", \n                                                        ascending=False)[0:num])\n    plt.title('LightGBM Feature imprtances')\n    plt.tight_layout()\n    \n    plt.show()\n    \nplotImp(model, Features_train , num =20)","fac22a59":"- It would be better if we scale the values of the transaction amount while training","2218ae24":"- TransactionDT\n  - Break datetime into weekdays, hours and days\n  - https:\/\/www.kaggle.com\/c\/ieee-fraud-detection\/discussion\/100400#latest-579480","7480a79f":"- Higher tedency for anonymous IP ratio wise","7dab85dd":"# Exploratory Data Analysis of Transaction Database","62e42960":"# Libraries","facfac01":"- Distance and address columns","ed89b0f0":"- Our First approach would be to perform an exploratory data analysis to better understand the features\n- Then using the EDA, we can perform Feature Engineering\n- Then search for the best hyper-parameters in LightGBM model\n- Then implement our features and target values in LightGBM model\n- Search for feature importances","791128fa":"- C subset columns\n- Due to high presence of many features. Let's pick only the top features by frquency for plotting","9341773f":"- Reference\n  - https:\/\/www.kaggle.com\/kabure\/extensive-eda-and-modeling-xgb-hyperopt\n  - https:\/\/mlfromscratch.com\/gridsearch-keras-sklearn\/#\/\n  - https:\/\/stackoverflow.com\/questions\/53413701\/feature-importance-using-lightgbm\n  - https:\/\/towardsdatascience.com\/the-search-for-categorical-correlation-a1cf7f1888c9\n  - https:\/\/chrisalbon.com\/machine_learning\/feature_selection\/drop_highly_correlated_features\/\n  ","ebc28f50":"- Feature Engineering with TransactionDT","3018f26e":"- RandomSearchCV Search for LightGBM","7cc8ea12":"- Some interesting Features from the graphs regarding the fraud for various features. Now let's look at D i.e days between transactions\n  ","66b275f1":"- Drop columns with high number of NaN values","c3b489d1":"- Not sure of the meaning between the different TimeDelta's, but there seems to be a high frequency for NaN values","6cd420c1":"- Fraud rate has a count on 5th working day and in the evening","7dda15d6":"# Implement model on test set","83f92358":"# Applying information we have learnt and implementing LightGBM on the training set first","9b33a4ab":"- Columns M2 and M3 have a very strong correlation of 0.85.\n- Also from the line plots M2 and M3 share a similar pattern of fraud correlation\n","74c9cedc":"## Categorical Columns","eb7f73cf":"-  Scale the Categorical and Numerical columns","0f5e53b3":"- We then pick the values in the corresponding columns with high frequency from Fraudelent and non Fraudenlent case.\n- This ensures that the prediction works well for the most frequent values in both cases","756f2216":"# Exploratory Data Analysis of Identity Database\n- Here I will be performing EDA only on select columns with understandable data.","a9c199fe":"# Feature Importance Chart","1442e9d7":"<img src=\"https:\/\/i2.wp.com\/computopedia.com\/wp-content\/uploads\/2020\/01\/online-fraud-image.png?fit=399%2C300&ssl=1\">","1174f45f":"- High Frequency of Fraud values among 'M' variables which have missing values","bacc2b83":"- Model fitting using LGBM classifier\n- The hyperparameters are found using a RandomizedSearchCV (commented above)","b41fd3e8":"- Missing values in addr1, addr2, dist1,dist2 have a higher fraud rate as well as in dist 1\n\n","7de74148":"- Among P email domain, gmail.com has a high fraud frequency\n- Among R email domain, gmail.com has a high fraud frequency","a55c2bdd":"- The notebook helped me learn and practice Feature Engineering and Visualization.\n- Let me know if there is any further scope of improvement!","a15d75f9":"- Drop V columns which have high correlation","2d3ac3e9":"- Interesting result in id_23 and id_27\n- id_23 has dist of IP address. There are high frequency values in anonymous (not higher than transparent)\n- id_27 shows 'Found' throughout\n- It would be valid to keep the High NaN columns","62ec74da":"- High number of low frequency values in card1, card2\n- 150 and 185 have a high frequency for card 3 \n- 226 have a high frequency for card 3","222a487e":"- Applying log transformation on data","789b4633":"- Interesting Observations\n  - High Frequency of Fraud in W in ProductCD\n  - High Frequency of Fraud in visa\n  - High Frequency of Fraud among debit cards","2e7ca215":"# Detection of Anomalies in the transactions\n\nThe prediction is based on the column 'isFraud' which contains binary values of 0 and 1.\nOur task is to predict the probability of a fraudulent transaction.\n\nThe data consists of 2 files. One of which contains identity information i.e train_identitiy.csv and the other contains transaction information i.e train_transaction.csv. The similar equivalent is also available in the testing set.\n\nAvailable details of the columns are as mentioned below:\n\n- Transaction Table \n  - TransactionDT: timedelta from a given reference datetime (not an actual timestamp)\n  - TransactionAMT: transaction payment amount in USD\n  - ProductCD: product code, the product for each transaction\n  - card1 - card6: payment card information, such as card type, card category, issue bank, country, etc.\n  - addr: address\n  - dist: distance\n  - P_ and (R__) emaildomain: purchaser and recipient email domain\n  - C1-C14: counting, such as how many addresses are found to be associated with the payment card, etc. The actual meaning is masked.\n  - D1-D15: timedelta, such as days between previous transaction, etc.\n  - M1-M9: match, such as names on card and address, etc.\n  - Vxxx: Vesta engineered rich features, including ranking, counting, and other entity relations.\n  \n\n- Identity Table \n  - DeviceType\n  - DeviceInfo\n  - id12 - id38\n  \n","0becc612":"- For cards 1 - 6 except 4  and 6","e111175b":"Summary\n- Card1,Card2 TransactionDT and TransactionAmt seem to hold a major importance in the model\n- Still scope for Feature Engineering by adjusting the value counts of the categorical variables we would like to hold","45a2cee8":"## Non Categorical Columns","fa899e20":"- High amount of Fraud among low frequency values\n- It would be advisable to take a higher size for prediction\n","b81e10c7":"# Read Data and Data Memory Reduction","04bc3921":"- Feature Engineering\n   - Replace values with low value count with 'Others'\n   - Create columns splitting datetime into components\n   - Drop highly correlatted V columns\n   - Drop columns with high NaN values"}}