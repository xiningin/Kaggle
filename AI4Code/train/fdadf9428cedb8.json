{"cell_type":{"44196d3c":"code","d89a6255":"code","b9141f97":"code","5ce4dd61":"code","18e18859":"code","5a17529b":"code","187be503":"code","9a5d380c":"code","c13d552f":"markdown","a09e67d1":"markdown","33520453":"markdown","3b76181d":"markdown","4eda9ce9":"markdown"},"source":{"44196d3c":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nnp.random.seed(10)\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Required magic to display matplotlib plots in notebooks\n%matplotlib inline\n\nfrom sklearn import metrics\nfrom sklearn.metrics import accuracy_score, confusion_matrix\nfrom sklearn.model_selection import train_test_split","d89a6255":"# Define the sigmoid activator; we ask if we want the sigmoid or its derivative\ndef sigmoid_act(x, der=False):\n    import numpy as np\n    \n    if (der==True) : #derivative of the sigmoid\n        f = 1\/(1+ np.exp(- x))*(1-1\/(1+ np.exp(- x)))\n    else : # sigmoid\n        f = 1\/(1+ np.exp(- x))\n    \n    return f\n\n# We may employ the Rectifier Linear Unit (ReLU)\ndef ReLU_act(x, der=False):\n    import numpy as np\n    \n    if (der == True): # the derivative of the ReLU is the Heaviside Theta\n        f = np.heaviside(x, 1)\n    else :\n        f = np.maximum(x, 0)\n    \n    return f\n\n'''\nArtificial Neural Network Class\n'''\nclass ANN:\n    import numpy as np # linear algebra\n    np.random.seed(10)\n    \n    '''\n    Initialize the ANN;\n    HiddenLayer vector : will contain the Layers' info\n    w, b, phi = (empty) arrays that will contain all the w, b and activation functions for all the Layers\n    mu = cost function\n    eta = a standard learning rate initialization. It can be modified by the 'set_learning_rate' method\n    '''\n    def __init__(self) :\n        self.HiddenLayer = []\n        self.w = []\n        self.b = []\n        self.phi = []\n        self.mu = []\n        self.eta = 1 #set up the proper Learning Rate!!\n    \n    '''\n    add method: to add layers to the network\n    '''\n    def add(self, lay = (4, 'ReLU') ):\n        self.HiddenLayer.append(lay)\n    \n    '''\n    FeedForward method: as explained before. \n    '''\n    @staticmethod\n    def FeedForward(w, b, phi, x):\n        return phi(np.dot(w, x) + b)\n        \n    '''\n    BackPropagation algorithm implementing the Gradient Descent \n    '''\n    def BackPropagation(self, x, z, Y, w, b, phi):\n        self.delta = []\n        \n        # We initialize ausiliar w and b that are used only inside the backpropagation algorithm once called        \n        self.W = []\n        self.B = []\n        \n        # We start computing the LAST error, the one for the OutPut Layer \n        self.delta.append(  (z[len(z)-1] - Y) * phi[len(z)-1](z[len(z)-1], der=True) )\n        \n        '''Now we BACKpropagate'''\n        # We thus compute from next-to-last to first\n        for i in range(0, len(z)-1):\n            self.delta.append( np.dot( self.delta[i], w[len(z)- 1 - i] ) * phi[len(z)- 2 - i](z[len(z)- 2 - i], der=True) )\n        \n        # We have the error array ordered from last to first; we flip it to order it from first to last\n        self.delta = np.flip(self.delta, 0)  \n        \n        # Now we define the delta as the error divided by the number of training samples\n        self.delta = self.delta\/self.X.shape[0] \n        \n        '''GRADIENT DESCENT'''\n        # We start from the first layer that is special, since it is connected to the Input Layer\n        self.W.append( w[0] - self.eta * np.kron(self.delta[0], x).reshape( len(z[0]), x.shape[0] ) )\n        self.B.append( b[0] - self.eta * self.delta[0] )\n        \n        # We now descend for all the other Hidden Layers + OutPut Layer\n        for i in range(1, len(z)):\n            self.W.append( w[i] - self.eta * np.kron(self.delta[i], z[i-1]).reshape(len(z[i]), len(z[i-1])) )\n            self.B.append( b[i] - self.eta * self.delta[i] )\n        \n        # We return the descended parameters w, b\n        return np.array(self.W), np.array(self.B)\n    \n    \n    '''\n    Fit method: it calls FeedForward and Backpropagation methods\n    '''\n    def Fit(self, X_train, Y_train):            \n        print('Start fitting...')\n        '''\n        Input layer\n        '''\n        self.X = X_train\n        self.Y = Y_train\n        \n        '''\n        We now initialize the Network by retrieving the Hidden Layers and concatenating them \n        '''\n        print('Model recap: \\n')\n        print('You are fitting an ANN with the following amount of layers: ', len(self.HiddenLayer))\n        \n        for i in range(0, len(self.HiddenLayer)) :\n            print('Layer ', i+1)\n            print('Number of neurons: ', self.HiddenLayer[i][0])\n            if i==0:\n                # We now try to use the He et al. Initialization from ArXiv:1502.01852\n                self.w.append( np.random.randn(self.HiddenLayer[i][0] , self.X.shape[1])\/np.sqrt(2\/self.X.shape[1]) )\n                self.b.append( np.random.randn(self.HiddenLayer[i][0])\/np.sqrt(2\/self.X.shape[1]))\n                # Old initialization\n                #self.w.append(2 * np.random.rand(self.HiddenLayer[i][0] , self.X.shape[1]) - 0.5)\n                #self.b.append(np.random.rand(self.HiddenLayer[i][0]))\n                \n                # Initialize the Activation function\n                for act in Activation_function.list_act():\n                    if self.HiddenLayer[i][1] == act :\n                        self.phi.append(Activation_function.get_act(act))\n                        print('\\tActivation: ', act)\n\n            else :\n                # We now try to use the He et al. Initialization from ArXiv:1502.01852\n                self.w.append( np.random.randn(self.HiddenLayer[i][0] , self.HiddenLayer[i-1][0] )\/np.sqrt(2\/self.HiddenLayer[i-1][0]))\n                self.b.append( np.random.randn(self.HiddenLayer[i][0])\/np.sqrt(2\/self.HiddenLayer[i-1][0]))\n                # Old initialization\n                #self.w.append(2*np.random.rand(self.HiddenLayer[i][0] , self.HiddenLayer[i-1][0] ) - 0.5)\n                #self.b.append(np.random.rand(self.HiddenLayer[i][0]))\n                \n                # Initialize the Activation function\n                for act in Activation_function.list_act():\n                    if self.HiddenLayer[i][1] == act :\n                        self.phi.append(Activation_function.get_act(act))\n                        print('\\tActivation: ', act)\n            \n        '''\n        Now we start the Loop over the training dataset\n        '''  \n        for I in range(0, self.X.shape[0]): # loop over the training set\n            '''\n            Now we start the feed forward\n            '''  \n            self.z = []\n            \n            self.z.append( self.FeedForward(self.w[0], self.b[0], self.phi[0], self.X[I]) ) # First layers\n            \n            for i in range(1, len(self.HiddenLayer)): #Looping over layers\n                self.z.append( self.FeedForward(self.w[i] , self.b[i], self.phi[i], self.z[i-1] ) )\n        \n            \n            '''\n            Here we backpropagate\n            '''      \n            self.w, self.b  = self.BackPropagation(self.X[I], self.z, self.Y[I], self.w, self.b, self.phi)\n            \n            '''\n            Compute cost function\n            ''' \n            self.mu.append(\n                (1\/2) * np.dot(self.z[len(self.z)-1] - self.Y[I], self.z[len(self.z)-1] - self.Y[I]) \n            )\n            \n        print('Fit done. \\n')\n        \n\n    \n    '''\n    predict method\n    '''\n    def predict(self, X_test):\n        \n        print('Starting predictions...')\n        \n        self.pred = []\n        self.XX = X_test\n        \n        for I in range(0, self.XX.shape[0]): # loop over the training set\n            \n            '''\n            Now we start the feed forward\n            '''  \n            self.z = []\n            \n            self.z.append(self.FeedForward(self.w[0] , self.b[0], self.phi[0], self.XX[I])) #First layer\n            for i in range(1, len(self.HiddenLayer)) : # loop over the layers\n                temp = self.FeedForward(self.w[i] , self.b[i], self.phi[i], self.z[i-1])\n                self.z.append(temp)\n       \n            # Append the prediction;\n            # We now need a binary classifier; we this apply an Heaviside Theta and we set to 0.5 the threshold\n            # if y < 0.5 the output is zero, otherwise is zero\n            self.pred.append( np.heaviside(  self.z[-1] - 0.5, 1)[0] ) # NB: self.z[-1]  is the last element of the self.z list\n        \n        print('Predictions done. \\n')\n\n        return np.array(self.pred)\n   \n    '''\n    We need a method to retrieve the accuracy for each training data to follow the learning of the ANN\n    '''\n    def get_accuracy(self):\n        return np.array(self.mu)\n    # This is the averaged version\n    def get_avg_accuracy(self):\n        import math\n        self.batch_loss = []\n        for i in range(0, 10):\n            self.loss_avg = 0\n            # To set the batch in 10 element\/batch we use math.ceil method\n            # int(math.ceil((self.X.shape[0]-10) \/ 10.0))    - 1\n            for m in range(0, (int(math.ceil((self.X.shape[0]-10) \/ 10.0))   )-1):\n                #self.loss_avg += self.mu[60*i+m]\/60\n                self.loss_avg += self.mu[(int(math.ceil((self.X.shape[0]-10) \/ 10.0)) )*i + m]\/(int(math.ceil((self.X.shape[0]-10) \/ 10.0)) )\n            self.batch_loss.append(self.loss_avg)\n        return np.array(self.batch_loss)\n    \n    '''\n    Method to set the learning rate\n    '''\n    def set_learning_rate(self, et=1):\n        self.eta = et\n        \n        \n'''\nlayers class\n'''\nclass layers :\n    '''\n    Layer method: used to call standar layers to add. \n    Easily generalizable to more general layers (Pooling and Convolutional layers)\n    '''        \n    def layer(p=4, activation = 'ReLU'):\n        return (p, activation)\n\n'''\nActivation functions class\n'''\nclass Activation_function(ANN):\n    import numpy as np\n    \n    def __init__(self) :\n        super().__init__()\n        \n    '''\n    Define the sigmoid activator; we ask if we want the sigmoid or its derivative\n    '''\n    def sigmoid_act(x, der=False):\n        if (der==True) : #derivative of the sigmoid\n            f = 1\/(1+ np.exp(- x))*(1-1\/(1+ np.exp(- x)))\n        else : # sigmoid\n            f = 1\/(1+ np.exp(- x))\n        return f\n\n    '''\n    Define the Rectifier Linear Unit (ReLU)\n    '''\n    def ReLU_act(x, der=False):\n        if (der == True): # the derivative of the ReLU is the Heaviside Theta\n            f = np.heaviside(x, 1)\n        else :\n            f = np.maximum(x, 0)\n        return f\n    \n    def list_act():\n        return ['sigmoid', 'ReLU']\n    \n    def get_act(string = 'ReLU'):\n        if string == 'ReLU':\n            return ReLU_act\n        elif string == 'sigmoid':\n            return sigmoid_act\n        else :\n            return sigmoid_act","b9141f97":"# Input data files are available in the \"..\/input\/\" directory.\ndata = pd.read_csv('..\/input\/web-page-phishing-detection\/phishing_data.csv')\ndata = data.drop(['url', 'ip'], axis=1)\n\ndict_phishing = { \n    'legitimate': 0,\n    'phishing': 1\n}\n\ndict_domain = {\n    'zero' : 0,\n    'one' : 1\n}\n\nfor feature_name in data.columns:\n    if (feature_name != 'status'):\n        data[feature_name] = data[feature_name].apply(lambda x : dict_domain[x.lower()] if type(x) is str and not x.isnumeric() else int(x))\n    else:\n        data[feature_name] = data[feature_name].apply(lambda x : dict_phishing[x.lower()])\n\ndata.head(4)","5ce4dd61":"corr = data.corr()\nnp.fill_diagonal(corr.values, np.nan)\ns = corr.unstack()\nso = s.sort_values(kind=\"quicksort\", ascending=True)\nfeatures_keys = list(filter(lambda x: x[0] != 'status' and x[1] != 'status', so.keys()))\n\nselected_features = set()\nfeature_count = 8\nselected_features_count = 0\ncurrent_index = 0\nwhile (selected_features_count < feature_count and current_index < len(features_keys)):\n    [feature_one, feature_two] = features_keys[current_index]\n    if (feature_one not in selected_features and selected_features_count < feature_count):\n        selected_features.add(feature_one)\n        selected_features_count += 1\n    if (feature_two not in selected_features and selected_features_count < feature_count):\n        selected_features.add(feature_two)\n        selected_features_count += 1\n    current_index += 1\n\nfeature_names = list(selected_features)\nfeatures = data[feature_names].to_numpy()\nlabels = data['status'].to_numpy()\n\nprint('Features selecionadas:')\nprint(', '.join(feature_names))","18e18859":"# split into training and testing sets\nX_train, X_test, Y_train, Y_test = train_test_split(features, labels, test_size=0.30)\n\nprint('Training records:',Y_train.size)\nprint('Test records:',Y_test.size)","5a17529b":"model = None\nmodel = ANN()\n\nmodel.add(layers.layer(16, 'ReLU'))\nmodel.add(layers.layer(8, 'ReLU'))\nmodel.add(layers.layer(2, 'sigmoid'))\n\nmodel.set_learning_rate(0.5)\n\nmodel.Fit(X_train, Y_train)\nacc_val = model.get_accuracy()\nacc_avg_val = model.get_avg_accuracy()\npredictions = model.predict(X_test)","187be503":"plt.figure(figsize=(10,6))\nplt.scatter(np.arange(1, X_train.shape[0]+1), acc_val, alpha=0.3, s=4, label='mu')\nplt.title('Loss for each training data point', fontsize=20)\nplt.xlabel('Training data', fontsize=16)\nplt.ylabel('Loss', fontsize=16)\nplt.show()\n\nplt.figure(figsize=(10,6))\nplt.scatter(np.arange(1, len(acc_avg_val)+1), acc_avg_val, label='mu')\nplt.title('Averege Loss by epoch', fontsize=20)\nplt.xlabel('Training data', fontsize=16)\nplt.ylabel('Loss', fontsize=16)\nplt.show()","9a5d380c":"# Plot the confusion matrix\nprint(predictions)\ncm = confusion_matrix(Y_test, predictions)\ndict_live = { \n    0 : 'Legitimate',\n    1 : 'Phishing'\n}\n\n\ndf_cm = pd.DataFrame(cm, index = [dict_live[i] for i in range(0,2)], columns = [dict_live[i] for i in range(0,2)])\nplt.figure(figsize = (7,7))\nsns.heatmap(df_cm, annot=True, cmap=plt.cm.Blues, fmt='g')\nplt.xlabel(\"Predicted Class\", fontsize=18)\nplt.ylabel(\"True Class\", fontsize=18)\nplt.show()","c13d552f":"# Cybersecurity Data Science\n## Hands-on\n\nO ataque de phishing \u00e9 um crime cibern\u00e9tico no qual um indiv\u00edduo, ou um grupo, tenta se passar por uma institui\u00e7\u00e3o leg\u00edtima, enviando mensagens por e-mail, telefone ou texto para um alvo ou m\u00faltiplos alvos, afim de atra\u00ed-los e fazer com que cedam dados sens\u00edveis, tais como dados de identifica\u00e7\u00e3o pessoais, informa\u00e7\u00f5es banc\u00e1rias e de cr\u00e9dito, senhas, etc.\nO presente trabalho tem como objetivo testar a sele\u00e7\u00e3o de features baseado exclusivamente na correla\u00e7\u00e3o entre elas na aplica\u00e7\u00e3o de t\u00e9cnicas de redes neurais na classifica\u00e7\u00e3o de URLs em duas categorias: phishing ou leg\u00edtimas. Para isso foi utilizado um dataset rotulado e com caracter\u00edsticas j\u00e1 mapeadas em um csv, que pode ser encontrado [aqui](https:\/\/www.kaggle.com\/manishkc06\/web-page-phishing-detection).\n\nCarregando todas as depend\u00eancias necess\u00e1rias:","a09e67d1":"### Carrega o dataset\nCarrega o dataset selecionado, al\u00e9m disso estamos removendo a URL e o IP que s\u00e3o campos que n\u00e3o nos interessa na constru\u00e7\u00e3o do classificador, e alguns campos de interesse que s\u00e3o originalmente strings s\u00e3o mapeados para os valores \\[0,1\\].","33520453":"## Sele\u00e7\u00e3o das features\nComo estrat\u00e9gia para sele\u00e7\u00e3o das features, eu decidi pegar um item de cada par daqueles que possuem menor correla\u00e7\u00e3o dentre todos, at\u00e9 completar oito features, garantindo que o elemento n\u00e3o selecionado de um par n\u00e3o entre na lista.","3b76181d":"C\u00f3digo utilizado no exemplo para a constru\u00e7\u00e3o da rede neural:","4eda9ce9":"## Conclus\u00e3o\nComo podemos observar, selecionando apenas features olhando para um baixo n\u00edvel de correla\u00e7\u00e3o da maneira acima e utilizando a mesma rede neural utilizada no exemplo, resulta em uma baixa acur\u00e1cia e uma quantidade muito alta de erros, logo n\u00e3o \u00e9 uma boa estrat\u00e9gia a ser seguida cegamente."}}