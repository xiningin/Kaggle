{"cell_type":{"0c36f04e":"code","6c2e53e0":"code","c725e204":"code","8b7d712f":"code","e32871ad":"code","c0337ed1":"code","79c14b93":"code","c84fe4d8":"code","b356be65":"code","5f30395a":"code","be2fdc64":"code","a454acfa":"code","52fcffdc":"code","2102d25e":"code","33484b9f":"code","29150dd5":"code","fab1c629":"code","4eb5c321":"code","903f6687":"code","8e70c8be":"code","71c81bb3":"code","68927475":"code","32ab78ff":"code","175da30a":"code","83aac9f4":"code","404c2569":"code","bd593f83":"code","20734d5d":"code","e205f3cc":"code","1ee1efec":"code","54b1c790":"code","601067f6":"code","7cf7079f":"code","e5dc0ee3":"code","46f43517":"code","3e4a6672":"code","876da6f0":"code","1938288c":"code","e42d5f2e":"code","09f1bd5c":"code","810a9c02":"code","a70cf196":"code","254b4d3e":"code","4f073869":"code","7e978c2c":"code","5cc6aac0":"code","5a7e428f":"code","33712c11":"code","77c629cd":"code","ae7c3f4c":"code","d4cfa82c":"code","0c7740a3":"code","a904d43c":"code","3d48b7e1":"code","01c133bf":"code","3933616c":"code","2e768307":"code","3f5689a1":"code","3a331352":"code","450ea865":"code","1627dff8":"code","c405d05d":"code","3c1242ff":"code","36357eec":"code","a2a5851b":"code","1eae4fc4":"code","7005db2d":"code","883f7511":"code","556a2e8e":"code","744922fb":"code","a027df6a":"code","e2a4aca9":"code","ce485c30":"code","f97f0e23":"code","88e9c860":"code","65e7a734":"code","195e94ed":"code","0a7e5bae":"code","19f2da0f":"code","9b2a3278":"code","2149593a":"code","da235000":"code","f699fd1a":"code","b71dc0b3":"code","d0cc19d3":"code","628870ad":"code","57a3e5da":"code","6576b3ac":"code","5d39fea4":"code","e0c2dae6":"markdown","0fcd47a2":"markdown","a46e12ab":"markdown","475af9a0":"markdown","9bc4ff59":"markdown","07d22558":"markdown","c260b952":"markdown","719aaa31":"markdown","d8bd675a":"markdown","dafe3940":"markdown","2f6b271c":"markdown","b7b9c169":"markdown","9f55468a":"markdown","0daeaca1":"markdown","46ca80a1":"markdown","39781a34":"markdown","0bfc92ee":"markdown","018a250c":"markdown","7b885913":"markdown","21fe719a":"markdown","49de9b4e":"markdown","0d00861b":"markdown","5d635556":"markdown","ff0aed32":"markdown","b2699525":"markdown","be3579f1":"markdown","38efe0b7":"markdown","18989ec3":"markdown","3a8eecee":"markdown","435d6bdb":"markdown","16e4c175":"markdown","6e6804d7":"markdown","78ec1591":"markdown","fab293f9":"markdown","a44b39d2":"markdown","409e046c":"markdown","91aa6528":"markdown","1733198c":"markdown","8b135053":"markdown","96f47c6b":"markdown","7298651e":"markdown","97543934":"markdown","095cbdcb":"markdown","07b8f899":"markdown","415afc74":"markdown","e4e44c72":"markdown","c2897181":"markdown","3fa49c3e":"markdown","af25680f":"markdown","bf0ccb44":"markdown","a4cfe686":"markdown","b13fc903":"markdown","67abb28f":"markdown","ce583130":"markdown","3b8d871a":"markdown","0b992413":"markdown","8669d35f":"markdown","c6bf4fcd":"markdown","01dfe6b8":"markdown","e1f3adb5":"markdown","6fc5505a":"markdown","f3b2689b":"markdown","003d6bfc":"markdown","d6040abd":"markdown","321c1dd8":"markdown","d887257a":"markdown","9df5a79b":"markdown","76157031":"markdown","c433a2ca":"markdown","9c02f377":"markdown","429ebcbe":"markdown","730bafd2":"markdown","ec02e8b1":"markdown","b6bdbd8d":"markdown","8e40037f":"markdown","1b831bed":"markdown","f9957cac":"markdown","d5f10845":"markdown"},"source":{"0c36f04e":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","6c2e53e0":"# Suppress Warnings\n\nimport warnings\nwarnings.filterwarnings('ignore')","c725e204":"# Import required libraries\n\nimport numpy as np\nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\nimport statsmodels.api as sm\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\n\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.metrics import r2_score\nfrom sklearn.feature_selection import RFE","8b7d712f":"# Reading the 'day.csv' file into a pandas dataframe called 'bikes'\n\nbikes = pd.read_csv('..\/input\/boom-bikes-data\/day.csv')\n\n# See the first five rows of dataframe 'bikes'\n\nbikes.head()","e32871ad":"# Check the dimensions of dataframe\n\nbikes.shape","c0337ed1":"# Check the column-wise information of dataframe\n\nbikes.info()","79c14b93":"# Check data types of columns\n\nbikes.dtypes","c84fe4d8":"# Checking for the missing values\n\nbikes.isnull().sum()","b356be65":"# Check the statistical desciption of numerical columns of dataframe\n\nbikes.describe()","5f30395a":"# Drop the aforesaid 4 features and view the first five rows of our new dataframe\n\nbikes.drop(['instant', 'dteday', 'casual', 'registered'], axis=1, inplace=True)\nbikes.head()","be2fdc64":"# Create a list of numerical variables only\nnum_vars = ['temp', 'atemp', 'hum', 'windspeed', 'cnt']\n\n# Visualise correlations among these numerical variables by plotting multiple scatter plots\n\nplt.figure(figsize=(15,10))\nsns.pairplot(bikes[num_vars])\nplt.show()","a454acfa":"# Removing variable 'atemp' and look at the first 5 rows\n\nbikes.drop(['atemp'], axis=1, inplace=True)\nbikes.head()","52fcffdc":"# First, we will convert 'season' to categorical string values\n\nbikes['season'] = bikes['season'].map({1:'spring', 2:'summer', 3:'fall', 4:'winter'})","2102d25e":"# Check if the above conversion worked fine\n\nbikes['season'].value_counts()","33484b9f":"# Similarly, we will convert 'weathersit' to categorical string values\n\nbikes['weathersit'] = bikes['weathersit'].map({1:'clear', 2:'mist', 3:'light', 4:'heavy'})","29150dd5":"# Check if the above conversion worked fine\n\nbikes['weathersit'].value_counts()","fab1c629":"# Next, we will convert 'mnth' to categorical string values\n\nbikes['mnth'] = bikes['mnth'].map({1:'Jan', 2:'Feb', 3:'Mar', 4:'Apr', 5:'May', 6:'Jun', 7:'Jul', 8:'Aug', 9:'Sep', 10:'Oct', 11:'Nov', 12:'Dec'})","4eb5c321":"# Check if the above conversion workd fine\n\nbikes['mnth'].value_counts()","903f6687":"# Lastly, we will convert 'weekday' to categorical string values\n\nbikes['weekday'] = bikes['weekday'].map({0:'Sun', 1:'Mon', 2:'Tue', 3:'Wed', 4:'Thu', 5:'Fri', 6:'Sat'})","8e70c8be":"# Check if the above conversion worked fine\n\nbikes['weekday'].value_counts()","71c81bb3":"# Let's again have a look at first five lines of our dataframe\n\nbikes.head()","68927475":"# Separate out the categorical variables\n\ncat_vars = ['season', 'yr', 'mnth', 'holiday', 'weekday', 'workingday', 'weathersit']","32ab78ff":"# Plotting Boxplots for each categorical variables\n\nplt.figure(figsize=(15,20))\nplt.subplots_adjust(hspace=.5)\nfor i, col in enumerate(cat_vars):\n    plt.subplot(4,2,i+1)\n    plt.title(label=\"Boxplot of 'cnt' v\/s \" + col)\n    sns.boxplot(x=col, y='cnt', data=bikes);","175da30a":"# View the correlation matrix of 'bikes' dataframe\n\ncorrelation = bikes.corr()\ncorrelation","83aac9f4":"# To have a better understanding of correlation matrix, we plot the lower half of heatmap of this matrix\n\nmask = np.zeros_like(correlation)\nmask[np.triu_indices_from(mask)] = True\nplt.figure(figsize=(15,10))\nsns.heatmap(correlation, mask=mask, annot=True, cmap='seismic', square=True, center=0)\nplt.yticks(rotation=0);","404c2569":"# Have a look at our dataframe again\n\nbikes.head()","bd593f83":"# Create dummy variables for columns having more than two levels\n\nvarlist = ['season', 'mnth', 'weekday', 'weathersit']","20734d5d":"# Creating dummy variables for above four features and then dropping the original feature as the information \n# contained in it now becomes redundant\n\ndummy1 = pd.get_dummies(bikes[varlist], drop_first=True)\nbikes = pd.concat([bikes, dummy1], axis=1)\nbikes = bikes.drop(varlist, axis=1)\nbikes.head()","e205f3cc":"# Checking the newly formed columns\n\nbikes.columns","1ee1efec":"# Check whether we have created any null values while forming dummy variables\n\nbikes.isnull().sum()","54b1c790":"# Performing test-train split: Train set = 70% and Test set = 30%\n\ndf_train, df_test = train_test_split(bikes, train_size=0.7, test_size=0.3, random_state=100)","601067f6":"# See the dimensions of our train set and test set\n\nprint(df_train.shape)\nprint(df_test.shape)","7cf7079f":"# Creating a list of numerical columns (not including the ones with Yes-No and Dummy variables)\n\nnum_vars = ['temp', 'hum', 'windspeed', 'cnt']","e5dc0ee3":"# Scale the data using MinMax scaling\n\nscaler = MinMaxScaler()\ndf_train[num_vars] = scaler.fit_transform(df_train[num_vars])\ndf_train.head()","46f43517":"## See the statistical description of numerical columns\n\ndf_train[num_vars].describe(percentiles=[.25, .5, .75, .90, .95, .99])","3e4a6672":"# Now let us look at the correlation between variables of train set\n\nplt.figure(figsize=(25,20))\nsns.heatmap(df_train.corr(), annot=True, cmap='coolwarm');","876da6f0":"# Scatter plots of 'cnt' v\/s 'temp'\n\nplt.figure(figsize=(12,5))\nsns.scatterplot(df_train.temp, df_train.cnt);","1938288c":"# Assigning feature variable X and response variable y\n\ny_train = df_train.pop('cnt')\nX_train = df_train","e42d5f2e":"# Check the dimensions of training sets\n\nprint(X_train.shape)\nprint(y_train.shape)","09f1bd5c":"# Check all the columns of dataframe\n\nbikes.columns","810a9c02":"# Build our 1st model\n\n# Add constant\nX_train_lm = sm.add_constant(X_train)\n\nlr1 = sm.OLS(y_train, X_train_lm).fit()\n\n# Check the summary of 1st model\nlr1.summary()","a70cf196":"# Checking VIFs\n\nvif = pd.DataFrame()\nvif['Features'] = X_train.columns\nvif['VIF'] = [variance_inflation_factor(X_train.values, i) for i in range(X_train.shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by='VIF', ascending=False)\nvif ","254b4d3e":"# Dropping variable 'workingday' and updating our 1st model\n\nX = X_train.drop('workingday', axis=1)","4f073869":"# Build 2nd model\n\nX_train_lm = sm.add_constant(X)\n\nlr2 = sm.OLS(y_train, X_train_lm).fit()\n\n# Check the summary of 2nd model\nlr2.summary()","7e978c2c":"# Checking VIFs\n\nvif = pd.DataFrame()\nvif['Features'] = X.columns\nvif['VIF'] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by='VIF', ascending=False)\nvif ","5cc6aac0":"# Dropping variable 'season_spring' and updating our 2nd model\n\nX = X.drop('season_spring', axis=1)","5a7e428f":"# Build 3rd model\n\nX_train_lm = sm.add_constant(X)\n\nlr3 = sm.OLS(y_train, X_train_lm).fit()\n\n# Check the summary of 3rd model\nlr3.summary()","33712c11":"# Checking VIFs\n\nvif = pd.DataFrame()\nvif['Features'] = X.columns\nvif['VIF'] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by='VIF', ascending=False)\nvif ","77c629cd":"# Dropping variable 'mnth_Oct' and updating our 3rd model\n\nX = X.drop('mnth_Oct', axis=1)","ae7c3f4c":"# Build 4th model\n\nX_train_lm = sm.add_constant(X)\n\nlr4 = sm.OLS(y_train, X_train_lm).fit()\n\n# Check the summary of 4th model\nlr4.summary()","d4cfa82c":"# Checking VIFs\n\nvif = pd.DataFrame()\nvif['Features'] = X.columns\nvif['VIF'] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by='VIF', ascending=False)\nvif ","0c7740a3":"# Dropping variable 'weekday_Sat' and updating our 4th model\n\nX = X.drop('weekday_Sat', axis=1)","a904d43c":"# Build 5th model\n\nX_train_lm = sm.add_constant(X)\n\nlr5 = sm.OLS(y_train, X_train_lm).fit()\n\n# Check the summary of 5th model\nlr5.summary()","3d48b7e1":"# Checking VIFs\n\nvif = pd.DataFrame()\nvif['Features'] = X.columns\nvif['VIF'] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by='VIF', ascending=False)\nvif ","01c133bf":"# Dropping variable 'weekday_Thu' and updating our 5th model\n\nX = X.drop('weekday_Thu', axis=1)","3933616c":"# Build 6th model\n\nX_train_lm = sm.add_constant(X)\n\nlr6 = sm.OLS(y_train, X_train_lm).fit()\n\n# Check the summary of 6th model\n\nlr6.summary()","2e768307":"# Checking VIFs\n\nvif = pd.DataFrame()\nvif['Features'] = X.columns\nvif['VIF'] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by='VIF', ascending=False)\nvif ","3f5689a1":"# Predicting on our train set\n\ny_train_pred = lr6.predict(X_train_lm)","3a331352":"# Plotting histogram of residuals (error terms) to validate assumption 2\n\nplt.figure(figsize=(10,8))\nErrors = y_train - y_train_pred\nsns.distplot(Errors, bins=20)\nplt.title(\"Histogram of Error Terms\", fontsize=20)\nplt.xlabel(\"Errors (y_train - y_train_pred)\", fontsize=15);\n","450ea865":"# Calculate the mean or error terms\n\nprint(np.mean(Errors))","1627dff8":"# Plotting scatter plot to validate assumptions 3 & 4\n\nplt.figure(figsize=(12,8))\nsns.regplot(y_train_pred, Errors)\nplt.title(\"Scatter plot of Error Terms\", fontsize=20)\nplt.ylabel(\"Errors (y_train - y_train_pred)\", fontsize=15)\nplt.xlabel(\"Predicted Values\", fontsize=15);","c405d05d":"# Scaling test data using the same columns used while scaling train data\n\nnum_vars = ['temp', 'hum', 'windspeed', 'cnt']\n\ndf_test[num_vars] = scaler.transform(df_test[num_vars])","3c1242ff":"# Verify that all the variables are scaled\n\ndf_test.describe()","36357eec":"# Creating response variable y and predictor variable X from test data\n\ny_test = df_test.pop('cnt')\nX_test = df_test","a2a5851b":"# Check the dimensions\n\nprint(X_test.shape)\nprint(y_test.shape)","1eae4fc4":"# Adding constant\n\nX_test_sm = sm.add_constant(X_test)","7005db2d":"# Dropping the variables that we dropped from training data\n\nX_test_sm = X_test_sm.drop(['workingday', 'season_spring', 'mnth_Oct', 'weekday_Sat', 'weekday_Thu'], axis=1)\n","883f7511":"### Making predictions\n\ny_test_pred = lr6.predict(X_test_sm)","556a2e8e":"# Evaluate the model\n\nr2 = r2_score(y_test, y_test_pred)\nr2","744922fb":"# Next calculate adjusted R-squared using formula: Adj R^2 = (1\u2212(1\u2212R^2)\u2217(n\u22121)\/(n\u2212p\u22121))\n\nn = X_test.shape[0]    # No. of rows of test data\np = X_test.shape[1]    # No. of columns of test data\n\nadj_r2 = (1-(1-r2)*(n-1)\/(n-p-1))\nadj_r2","a027df6a":"# Plot scatter plot between y_test and y_test_pred\n\nplt.figure(figsize=(10,8))\nsns.scatterplot(y_test, y_test_pred)\nplt.title(\"y_test v\/s y_pred\", fontsize=20)\nplt.xlabel(\"y_test\", fontsize=15)\nplt.ylabel(\"y_pred\", fontsize=15);\n","e2a4aca9":"# Instantiating a class for LinearRegression\n\nlm = LinearRegression()","ce485c30":"# Running RFE with the output number of the variable equal to 10\n\nrfe = RFE(lm, 10)\nrfe = rfe.fit(X_train, y_train)","f97f0e23":"# Look at columns and their ranking given by RFE\n\nlist(zip(X_train.columns, rfe.support_, rfe.ranking_))","88e9c860":"# The 10 columns selected by RFE method\n\ncol = X_train.columns[rfe.support_]\ncol","65e7a734":"# Show columns not selected by RFE method\n\nX_train.columns[~rfe.support_]","195e94ed":"# Create train set by only the columns selected by RFE\n\nX_train_rfe = X_train[col]\n\n# Add constant\nX_train_rfe = sm.add_constant(X_train_rfe)\n\nlm1 = sm.OLS(y_train, X_train_rfe).fit()\n\nlm1.summary()","0a7e5bae":"# Checking VIFs\n\nX_train_new = X_train_rfe.drop(['const'], axis=1)\nvif = pd.DataFrame()\nX = X_train_new\nvif['Features'] = X.columns\nvif['VIF'] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by='VIF', ascending=False)\nvif ","19f2da0f":"# Now we first drop the variable 'hum' as it has the highest VIF (15.18)\n\nX = X.drop(['hum'], axis=1)\n\n# Add constant\nX_train_lm = sm.add_constant(X)\n\nlm2 = sm.OLS(y_train, X_train_lm).fit()\n\nlm2.summary()","9b2a3278":"# Checking VIFs again\n\nX_train_new = X_train_lm.drop(['const'], axis=1)\nvif = pd.DataFrame()\nX = X_train_new\nvif['Features'] = X.columns\nvif['VIF'] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by='VIF', ascending=False)\nvif ","2149593a":"# Predicting on our train set\n\ny_train_pred = lm2.predict(X_train_lm)","da235000":"# Plotting histogram of residuals (error terms) to valiate assumption 2\n\nplt.figure(figsize=(10,8))\nErrors = y_train - y_train_pred\nsns.distplot(Errors, bins=20)\nplt.title(\"Histogram of Error Terms\", fontsize=20)\nplt.xlabel(\"Errors (y_train - y_train_pred)\", fontsize=15);\n","f699fd1a":"# Plotting scatter plot to validate assumptions 3 & 4\n\nplt.figure(figsize=(12,8))\nsns.regplot(y_train_pred, Errors)\nplt.title(\"Scatter plot of Error Terms\", fontsize=20)\nplt.ylabel(\"Errors (y_train - y_train_pred)\", fontsize=15)\nplt.xlabel(\"Predicted Values\", fontsize=15);","b71dc0b3":"# Check the dimensions of test data \n\nprint(X_test.shape)\nprint(y_test.shape)","d0cc19d3":"# Selecting all the variables\/columns that were selected while builiding model on training set\nX_test_new = X_test[X_train_new.columns]\n\n# Add constant\nX_test_new = sm.add_constant(X_test_new)","628870ad":"# Making predictions\n\ny_test_pred = lm2.predict(X_test_new)","57a3e5da":"# Evaluate the model: First calculate R-squared\n\nr2 = r2_score(y_test, y_test_pred)\nr2","6576b3ac":"# Next calculate adjusted R-squared using formula: Adj R^2 = (1\u2212(1\u2212R^2)\u2217(n\u22121)\/(n\u2212p\u22121))\n\nn = X_test.shape[0]    # No. of rows of test data\np = X_test.shape[1]    # No. of columns of test data\n\nadj_r2 = (1-(1-r2)*(n-1)\/(n-p-1))\nadj_r2","5d39fea4":"# Plot scatter plot between y_test and y_test_pred\n\nplt.figure(figsize=(10,8))\nsns.scatterplot(y_test, y_test_pred)\nplt.title(\"y_test v\/s y_pred\", fontsize=20)\nplt.xlabel(\"y_test\", fontsize=15)\nplt.ylabel(\"y_pred\", fontsize=15);\n","e0c2dae6":"## <font color=purple>Method 2: Feature Selection using RFE (Recursive Feature Elimination)<\/font>","0fcd47a2":"#### Let us plot a scatter plot between 'cnt' and the numeric variable 'temp' to see this correlation","a46e12ab":"### Visualising Categorical variables","475af9a0":"#### Inferences:\n- From above scatter plots, we observe that '`cnt`' has positive correlation with '`temp`' and '`atemp`'.\n- There is strong collinearity among the variables '`temp`' and '`atemp`'.\n- So, we have to remove any one of them. \n- As '`temp`' is more sensible, we drop '`atemp`'.","9bc4ff59":"#### Inferences\n- We see above that there are no missing\/null values in any of the columns.   \n- Hence, we don't need to do any missing value treatment on our dataframe.","07d22558":"#### Validation of Assumption 3:\n- From above scatterplot, we observe that there is no discernible pattern\n- Thus, we can conclude that ***error terms\/residuals are independent of each other***\n\n#### Validation of Assumption 4:\n- We also observe from above scatter plot that the variation of data points is not increasing significantly. \n- Thus, we can conclude that our ***error terms have constant variance (homoscedasticity)***","c260b952":"### Handling Categorical Variables\n\n- Before visualising categorical variables, we need to do some modifications to some of them.  \n- We need to convert variables '`season`', '`weathersit`', '`mnth`' and '`weekday`' to categorical string values because the numeric values associated with these labels may indicate that there is some order to them, which is actually not the case. Converting them will also help in better visualisation and ease our process of creating dummy variables out of them.","719aaa31":"## <font color='darkgreen'>Step 8: Model Evaluation<\/font>\n#### <font color=purple>(Method 2: RFE)<\/font>","d8bd675a":"#### Inferences:\n\nFrom above list of columns, we see that we have created:\n- 3 dummy variables '`season_spring`','`season_summer`' and '`season_winter`' for 4 seasons\n- 11 dummy variables '`mnth_Aug`', '`mnth_Dec`', '`mnth_Feb`', '`mnth_Jan`', '`mnth_Jul`', '`mnth_Jun`', '`mnth_Mar`', '`mnth_May`', '`mnth_Nov`', '`mnth_Oct`' and '`mnth_Sep`' for 12 months\n- 6 dummy variables '`weekday_Mon`', '`weekday_Sat`', '`weekday_Sun`', '`weekday_Thu`', '`weekday_Tue`' and '`weekday_Wed`' for 7 weekdays\n- 2 dummy variables '`weathersit_light`' and '`weathersit_mist`' for 4 types of weathers (as there are no rentals in heavy weather, number of dummy variables is one less than 3)","dafe3940":"### Data Dictionary  \n\n- **instant**: record index\n- **dteday** : date\n- **season** : season (1:spring, 2:summer, 3:fall, 4:winter)\n- **yr** : year (0: 2018, 1:2019)\n- **mnth** : month ( 1 to 12)\n- **holiday** : whether day is a holiday or not (extracted from http:\/\/dchr.dc.gov\/page\/holiday-schedule)\n- **weekday** : day of the week\n- **workingday** : if day is neither weekend nor holiday is 1, otherwise is 0.\n- **weathersit** : \n\t\t- 1: Clear, Few clouds, Partly cloudy, Partly cloudy\n\t\t- 2: Mist + Cloudy, Mist + Broken clouds, Mist + Few clouds, Mist\n\t\t- 3: Light Snow, Light Rain + Thunderstorm + Scattered clouds, Light Rain + Scattered clouds\n\t\t- 4: Heavy Rain + Ice Pallets + Thunderstorm + Mist, Snow + Fog\n- **temp** : temperature in Celsius\n- **atemp**: feeling temperature in Celsius\n- **hum**: humidity\n- **windspeed**: wind speed\n- **casual**: count of casual users\n- **registered**: count of registered users\n- **cnt**: count of total rental bikes including both casual and registered\t","2f6b271c":"#### Inference:\n- We observe that there are no bikes are rented when the weather is '**heavy**'.","b7b9c169":"#### Inference:\n- We get quite a good R-squared of **0.830**\n- We see from summary that all p-values are very low. \n- So, we will have to check for VIFs","9f55468a":"#### Inferences:\n- R-squared remains same (**0.850**) after dropping '`weekday_Sat`'\n- Still there are many variables with high p-values\n- But again, we will first look at VIFs","0daeaca1":"## <font color='darkgreen'>Step 5: Building a Linear Model<\/font>","46ca80a1":"### Creating Dummy Variables","39781a34":"#### Validation of Assumption 2:\n- From above histogram and calculation, we observe that ***error terms\/residuals are normally distributed with mean approximately zero***\n- Thus, our model will be good to make ***inferences*** on our 'bikes' dataset. Note that we wanted to know which variables are significant in predicting the demand for bikes.\n- This also means that the p-values obtained in our model are reliable.","0bfc92ee":"### Data Quality Check","018a250c":"By looking at the data dictionary and info() above, we conclude that the following features are of no use and can be conveniently dropped:-\n\n- **`instant`**: This feature gives the index of records. For this, we can use the default index given in pandas dataframe.\n\n\n- **`dteday`**: This feature gives the date of record. This information is redundant as there are two other features `mnth` (Month) and `yr` (Year)\n\n\n- **`casual`** & **`registered`**: These two values give count of casual and registered users respectively. However, there is a column `cnt` containing sum of these two columns. As these two will not serve any purpose independently and the same information is already provided in column `cnt`, we drop them.","7b885913":"#### We already have our test data divided into X_test and y_test","21fe719a":"#### Checking for VIFs","49de9b4e":"### Flow of this notebook\n\nThe following are steps involved in problem solving:\n1. Reading, Understanding and Quality Check of the data\n2. Visualising the Data\n3. Data Preparation\n4. Splitting Data into Train-Test and Scaling\n5. Building a Linear Model (this is done using two methods: **Backward Selection** and **Recursive Feature Elimination** and two separate models obtained)\n6. Residual Analysis\n7. Making Predictions\n8. Model Evaluation\n\n**Note**: Steps 5 to 8 will be done twice, first time model built using ***Backward Selection*** and second time for model built using ***RFE***)","0d00861b":"## <font color='darkgreen'>Step 6: Residual Analysis<\/font>\n\n#### <font color=purple>(Method 1: Backward Selection)<\/font>","5d635556":"Here the names are created as under\n- **clear** = 1: Clear, Few clouds, Partly cloudy, Partly cloudy\n- **mist**    = 2: Mist + Cloudy, Mist + Broken clouds, Mist + Few clouds, Mist\n- **light** = 3: Light Snow, Light Rain + Thunderstorm + Scattered clouds, Light Rain + Scattered clouds\n- **heavy** = 4: Heavy Rain + Ice Pallets + Thunderstorm + Mist, Snow + Fog","ff0aed32":"#### Inferences:\n- Categorical variables having two levels: '`yr`', '`holiday`'and '`workingday`' are already having values as **0**s and **1**s. So, we ***don't need to map them***.  \n- However, we need to convert categorical variables having more than two levels to dummy variables, which are '`season`', '`mnth`', '`weekday`', '`weathersit`'","b2699525":"### Conclusion:\n- Both our final models obtained from ***Backward Selection** as well as **RFE** feature selection methods are significant.\n- The common variables significant in predicting demand for sharde bikes are:\n> '`temp`'  \n'`weathersit_light`'  \n'`yr`' and   \n'`windspeed`'\n- These can be suggested to BoomBikes company as the factors most affecting the demand for rental bikes.","be3579f1":"#### Inferences:\n- R-squared is quite high (**0.851**).\n- But, we will also look at VIF values to decide which variable to drop","38efe0b7":"1. The variables significant in predicting the demand for shared bikes are the ones having near to zero p-values:\n\n   - `yr`: whether the year is 2018 or 2019\n   - `holiday`: whether day is a holiday or not\n   - `temp`: temperature in Celsius\n   - `windspeed`: wind speed on a given day\n   - `season_spring`: whether it is spring season or not\n   - `season_winter`: whether it is winter season or not\n   - `mnth_Jul`: whether it is month of Jul or not\n   - `mnth_Sep`: whether it is month of September or not\n   - `weathersit_light`: whether the weather is Light Snow \/ (Light Rain + Thunderstorm + Scattered clouds) or (Light Rain + Scattered clouds)\n\n\n\n2. The extent to which these variables describe the bike demand (we'll see the **top 5 variables** that affect the demand the most):\n\n   - `temp`: a 1 degree celsius increase in temperature leads to increase in rental bikes of **0.4758** units\n   - `weathersit_light`: if the weather is Light Snow \/ (Light Rain + Thunderstorm + Scattered clouds) or (Light Rain + Scattered clouds), then number of rental bikes decreases by **0.2562** units\n   - `yr`: if the year is 2019 instead of 2018, the number of rental bikes increases by **0.2350** units\n   - `windspeed`: if the windspeed will increase by 1 unit, the number of rental bikes will decrease by **0.1325** units\n   - `season_winter`: if the season will be winter, the number of rental bikes will decrease by **0.1032** units\n   ","18989ec3":"### Dropping useless variables ","3a8eecee":"#### Inference:\n- We observe that in our 2nd model, ***all p-values of coefficients are very low***, hence all the coefficients are significant.\n- Also, ***all the VIF values are less than 5***.\n- Hence, we accept this 2nd model '**lm2**', which explains **80.5 %** variation of the data.","435d6bdb":"### Visualising Correlation among variables","16e4c175":"#### Inferences:\n- The R-squared decreases slightly to **0.805**\n- All the p-values are very low\n- Again, we check for VIF values","6e6804d7":"#### First, we build a model using all the variables","78ec1591":"### <font color=purple>(Method 1: Feature Selection using Backward Selection)<\/font>","fab293f9":"### Visualise correlations among the variables in training set","a44b39d2":"### Validation of Assumption 1:\n#### There is a linear relationship between X (indpendent variables) and Y (response variable)\n- From above all visualisations, we observe there is some relation between our response variable (**y**) and many of the independent variables (**Xs**)\n- Thus, we can fit a linear model between them.","409e046c":"### Assumptions of Linear Regression","91aa6528":"#### Inference:\n- In our 2nd model on training set, we got an R-squared of **0.805** and an adjusted R-squared of **0.801**\n- The same model gives an R-squared of **0.788** on test set and an adjusted R-squared of **0.758**\n- The value of probability of F-statistic is **5.33e-171** which is very low.\n- If we would have started with **Null Hypothesis** that `all coefficients are zero` , then from summary of our 2nd model, we see that `none of the coeeficients is zero`.  Thus, we reject our null hypothesis in favour of **Alternate Hypothesis** that our `model is statistically significant`.\n- Hence, we conclude that our prediction model is significant.\n\n\n- Our final Linear Regression equation takes the form:\n\n##  $cnt = 0.2042 + (0.4758 * temp) - (0.2562 * weathersit\\_light) + (0.2350 * yr) - (0.1325 * windspeed) - (0.1032 * season\\_winter) + ...... $ ","1733198c":"#### Inferences:\n- R-squared reduces slightly after dropping '`season_spring`' (**0.850**)\n- Still there are many variables with high p-values\n- But again, we will first look at VIFs","8b135053":"#### Inference:\n- The next variable with highest VIF (**6.29**) and also having a high p-value(**0.858**) is `mnth_Oct`.\n- So, now we rebuild our model by dropping `mnth_Oct`","96f47c6b":"1. The variables significant in predicting the demand for shared bikes are the ones having near to zero p-values:\n\n   - `yr`: whether the year is 2018 or 2019\n   - `holiday`: whether day is a holiday or not\n   - `temp`: temperature in Celsius\n   - `hum`: humidity on a given day\n   - `windspeed`: wind speed on a given day\n   - `season_summer`: whether it is summer season or not\n   - `season_winter`: whether it is winter season or not\n   - `mnth_Sep`: whether it is month of September or not\n   - `weathersit_light`: whether the weather is Light Snow \/ (Light Rain + Thunderstorm + Scattered clouds) or (Light Rain + Scattered clouds)\n\n\n\n2. The extent to which these variables describe the bike demand (we'll see the **top 5 variables** that affect the demand the most):\n\n   - `temp`: a 1 degree celsius increase in temperature leads to increase in rental bikes of **0.4550** units\n   - `weathersit_light`: if the weather is Light Snow \/ (Light Rain + Thunderstorm + Scattered clouds) or (Light Rain + Scattered clouds), then number of rental bikes decreases by **0.2493** units\n   - `yr`: if the year is 2019 instead of 2018, the number of rental bikes increases by **0.2310** units\n   - `windspeed`: if the windspeed will increase by 1 unit, the number of rental bikes will decrease by **0.1876** units\n   - `hum`: if the humidity will increase by 1 unit, the number of rental bikes will decrease by **0.1581** units\n   ","7298651e":"## <font color='brown'>Summary to be provided to BoomBikes Company<\/font>","97543934":"#### Inference:\n- The next variable with high p-value(**0.827**) and low VIF(**1.55**) is `weekday_Thu`.\n- So, now we rebuild our model by dropping `weekday_Thu`","095cbdcb":"We will build a Linear Regression model by making the following assumptions and later, validate them while performing our model building:\n\n1. There is a linear relationship between X (indpendent variables) and Y (response variable)\n2. Error terms are normally distributed with mean zero\n3. Error terms are independent of each other\n4. Error terms have constant variance (homoscedasticity)","07b8f899":"#### Inference:\n- From above heatmap, we observe that '`cnt`' is highly correlated with '`yr`', '`temp`' and '`season_spring`'.","415afc74":"#### Validation of Assumption 3:\n- From above scatterplot, we observe that there is no discernible pattern\n- Thus, we can conclude that ***error terms\/residuals are independent of each other***\n\n#### Validation of Assumption 4:\n- We also observe from above scatter plot that the variation of data points is not increasing significantly. \n- Thus, we can conclude that our ***error terms have constant variance (homoscedasticity)***","e4e44c72":"## <font color='darkgreen'>Step 7: Making Predictions<\/font>\n#### <font color=purple>(Method 2: RFE)<\/font>","c2897181":"#### Inferences:\n- R-squared remains same (**0.850**) after dropping '`weekday_Thu`'\n- Still there are few variables with high p-values\n- So we also look at VIFs","3fa49c3e":"## <font color='darkgreen'>Step 8: Model Evaluation<\/font>\n#### <font color=purple>(Method 1: Backward Selection)<\/font>","af25680f":"## <font color='darkgreen'>Step 5: Building a Linear Model<\/font>","bf0ccb44":"## <font color='darkgreen'>Step 6: Residual Analysis<\/font>\n\n#### <font color=purple>(Method 2: RFE)<\/font>","a4cfe686":"### Visualising the numerical variables","b13fc903":"#### Inference:\n- Now, there are no variables with ***both high p-value and high VIFs*** \n- So, next we look for variables with ***high p-value and low VIFs***\n- The next variable with a high p-value(**0.859**) and low VIF (**1.81**) is is `weekday_Sat`.\n- So, now we rebuild our model by dropping `weekday_Sat`","67abb28f":"### Creating our feature and response variables","ce583130":"### Scaling all the numerical columns","3b8d871a":"#### Dividing Test data into X and y","0b992413":"#### Inferences:\n- R-squared is same after dropping `workingday` (**0.851**)\n- Still there are many variables with high p-values\n- But again, we will first look at VIFs","8669d35f":"## <font color='darkgreen'>Step 1: Reading, Understanding and Quality Check of the data<\/font>","c6bf4fcd":"## <font color='darkgreen'>Step 3: Data Preparation<\/font>","01dfe6b8":"#### Inference:\n- We observe that all the columns have appropriate data types.","e1f3adb5":"### Importing Libraries","6fc5505a":"#### Problem Statement\n\n- A **bike-sharing system** is a service in which bikes are made available for shared use to individuals on a short term basis for a price or free. Many bike share systems allow people to borrow a bike from a \"dock\" which is usually computer-controlled wherein the user enters the payment information, and the system unlocks it. This bike can then be returned to another dock belonging to the same system.\n\n\n- A US bike-sharing provider ***BoomBikes*** has recently suffered considerable ***dips in their revenues*** due to the ongoing Corona pandemic. The company is finding it very difficult to sustain in the current market scenario. So, it has decided to come up with a mindful ***business plan to be able to accelerate its revenue*** as soon as the ongoing lockdown comes to an end, and the economy restores to a healthy state. \n\n\n- In such an attempt, BoomBikes aspires ***to understand the demand for shared bikes among the people after this ongoing quarantine situation ends*** across the nation due to Covid-19. They have planned this to prepare themselves to cater to the people's needs once the situation gets better all around and stand out from other service providers and make huge profits.\n\n\n- They have contracted a consulting company to understand the factors on which the demand for these shared bikes depends. Specifically, they want ***to understand the factors affecting the demand for these shared bikes in the American market***.\n\n\n\n- The company wants to know:\n\n   - **Which variables** are significant in predicting the demand for shared bikes?\n   - **How well those variables describe** the bike demands?\n   \n   \n\n- Based on various meteorological surveys and people's styles, the service provider firm has gathered a large dataset on daily bike demands across the American market based on some factors. \n\n","f3b2689b":"#### Inferences:\n- R-squared remains same **0.850** after dropping '`mnth_Oct`'\n- Still there are many variables with high p-values\n- But again, we will first look at VIFs","003d6bfc":"#### Inference:\n- We get R-squared value of **0.851**\n- The variable `workingday` has both high p-value (**0.395**) and very high VIF (**63.89**)\n- So, we will first drop `workingday`","d6040abd":"### Building model using statsmodels","321c1dd8":"#### Inference:\n- As the values in each column above are gradually increasing, we can conclude that there are no outliers","d887257a":"#### Validation of Assumption 2:\n- From above histogram, we observe that ***error terms\/residuals are normally distributed with mean approximately zero***\n- Thus, our model will be good to make ***inferences*** on our 'bikes' dataset. Note that we wanted to know which variables are significant in predicting the demand for bikes.\n- This also means that the p-values obtained in our model are reliable.","9df5a79b":"#### Inferences:\n- From above heatmap, we observe our target variable '`cnt`' has:\n   - high correlation with variables '`yr`' and '`temp`'\n   - low correlation with '`holiday`', '`workingday`' and '`hum`'","76157031":"## <font color='darkgreen'>Step 4: Splitting Data into Train-Test and Scaling<\/font>","c433a2ca":"## <font color='brown'>Summary to be provided to BoomBikes Company<\/font>","9c02f377":"# <font color= brown> Bike Sharing Assignment<\/font>","429ebcbe":"#### Now, our data is ready for the model to be built.","730bafd2":"#### Inference:\n- The next variable with high VIF (**11.88**) and also having a high p-value (**0.152**) is `season_spring`.\n- So, now we rebuild our model by dropping `season_spring`","ec02e8b1":"## <font color='darkgreen'>Step 7: Making Predictions<\/font>\n#### <font color=purple>(Method 1: Backward Selection)<\/font>","b6bdbd8d":"## <font color=teal>Submitted by: Praveersinh Parmar<\/font>","8e40037f":"#### Inference:\n- In our 6th model on training set, we got an R-squared of **0.850** and an adjusted R-squared of **0.843**\n- The same model gives an R-squared of **0.822** on test set and adjusted R-squared of **0.796**.\n- Thus, our model is working almost same on training as well as test data. It is not underfitting or overfitting.\n- The value of probability of F-statistic is **1.35e-183** which is very low.\n- If we would have started with **Null Hypothesis** that `all coefficients are zero` , then from summary of our 6th model, we see that `none of the coeeficients is zero`.  Thus, we reject our null hypothesis in favour of **Alternate Hypothesis** that our `model is statistically significant`.\n- Hence, we conclude that our prediction model is significant.\n\n\n- Our final Linear Regression equation takes the form:\n\n## $cnt = 0.3117 + (0.4550 * temp) - (0.2493 * weathersit\\_light) + (0.2310 * yr) - (0.1867 * windspeed) - (0.1581 * hum) + (0.1370 * season\\_winter)  + ...... $","1b831bed":"#### Inferences:\n- More bikes are rented in **fall** and **summer** seasons, and from **May** to **October** months\n- More number of bikes were rented in **2019** than in 2018.\n- If the weather is **clear** (or having **few clouds**), then more bikes are rented. No bikes are rented in **heavy** weather.\n\n\n- Rest of the features **`holiday`**, **`workingday`** and **`weekday`** don't show any discernible pattern and hence, it is suggested not to use them for prediction of **`cnt`**. \n\n\n- Overall, the data is well behaved with very few outliers.\n- As we have very less data available, we will not treat outliers.","f9957cac":"#### Inference:\n- We observe that from the 3rd model onwards, ***R-squared is remaining the same, 0.850***\n- Now, if we keep on removing more variables, it doesn't impact the R-squared of model very much\n- Hence, we can stop our exercise and **accept our 6th model 'lr6'**, which explains **85%** variation of the data ","d5f10845":"## <font color='darkgreen'>Step 2: Visualising the Data<\/font>"}}