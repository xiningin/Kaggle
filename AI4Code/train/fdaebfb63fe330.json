{"cell_type":{"3834c48c":"code","d005d1c7":"code","39882a5a":"code","8e3dca9b":"code","ad1f3056":"code","6fb2781c":"code","fce1ea8b":"code","a6663f91":"code","19fe5d8b":"code","b8301378":"code","6a848f3c":"code","cca8ecc8":"code","f6f79205":"code","037e1794":"code","e959c0be":"code","c43cb3b9":"code","5e004cf4":"code","c58506b8":"code","0809f993":"code","08700c3d":"code","7e5d9ca9":"code","9dcb62dd":"code","2bf2f440":"code","e68aba04":"markdown","7cbdc0ff":"markdown","5da15ea8":"markdown","51ea32c8":"markdown","46ecf8f4":"markdown","0b9456b3":"markdown","de9c4526":"markdown","0e5d73b2":"markdown","327e2113":"markdown","ff740adc":"markdown","1cf63dc2":"markdown","d4ff3611":"markdown","a9b24690":"markdown"},"source":{"3834c48c":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","d005d1c7":"from datetime import datetime\nfrom pandas import DataFrame\ndf_original = DataFrame(\n            {\n                \"Name\": \"Maria Maria Maria Maria Jane Carlos\".split(),\n                \"Sample\": [25, 9, 4, 3, 2, 8],\n                \"Date\": [\n                    datetime(2019, 9, 1, 13, 0),\n                    datetime(2019, 9, 1, 13, 5),\n                    datetime(2019, 10, 1, 20, 0),\n                    datetime(2019, 10, 3, 10, 0),\n                    datetime(2019, 12, 2, 12, 0),\n                    datetime(2019, 9, 2, 14, 0),\n                ],\n            }\n        )\ndf_original.set_index('Date',inplace=False)","39882a5a":"df_original","8e3dca9b":"import pandas as pd\ndf_original.groupby(pd.Grouper(key = \"Date\",freq=\"M\")).sum()","ad1f3056":"df = pd.read_excel(\"https:\/\/github.com\/chris1610\/pbpython\/blob\/master\/data\/sample-salesv3.xlsx?raw=True\")\ndf[\"date\"] = pd.to_datetime(df['date'])\ndf.head()","6fb2781c":"import pandas as pd\ndf.groupby(pd.Grouper(key = \"date\",freq=\"M\")).sum()","fce1ea8b":"import pandas as pd\ndf_original.groupby(pd.Grouper(key='Date',freq='5D')).sum()","a6663f91":"import pandas as pd \ndf_original_5D = df_original.groupby(pd.Grouper(key='Date', freq='5D')).sum()\ndf_original_5D[df_original_5D['Sample']!=0]","19fe5d8b":"import pandas as pd \ndf_5D = df.groupby(pd.Grouper(key='date', freq='5D')).sum()\ndf_5D[df_5D['quantity'] >= 700]","b8301378":"import pandas as pd \ndf_5D = df.groupby(pd.Grouper(key='date', freq='5D')).sum()\ndf_5D[df_5D['unit price'] >= 1300]","6a848f3c":"df_original.set_index('Name', inplace=True)","cca8ecc8":"df_original.groupby(pd.Grouper(level= 'Name', axis = 0)).sum()","f6f79205":"df.set_index('date').resample('M')[\"ext price\"].sum()","037e1794":"df.set_index('date').groupby('name')[\"ext price\"].resample(\"M\").sum()","e959c0be":"df.groupby(['name', pd.Grouper(key='date', freq='M')])['ext price'].sum()","c43cb3b9":"df.groupby(['name', pd.Grouper(key='date', freq='A-DEC')])['ext price'].sum()","5e004cf4":"df[[\"ext price\", \"quantity\"]].sum()","c58506b8":"df[\"unit price\"].mean()","0809f993":"df[[\"ext price\", \"quantity\", \"unit price\"]].agg(['sum', 'mean'])","08700c3d":"df.agg({'ext price': ['sum', 'mean'], 'quantity': ['sum', 'mean'], 'unit price': ['mean']})","7e5d9ca9":"get_max = lambda x: x.value_counts(dropna=False).index[0]","9dcb62dd":"df.agg({'ext price': ['sum', 'mean'], 'quantity': ['sum', 'mean'], 'unit price': ['mean'], 'sku': [get_max]})","2bf2f440":"import collections\nf = collections.OrderedDict([('ext price', ['sum', 'mean']), ('quantity', ['sum', 'mean']), ('sku', [get_max])])\ndf.agg(f)","e68aba04":"- TimeSeries Dataframe","7cbdc0ff":"- We will set the freq parameter as 5D here","5da15ea8":"- filter out those 0 from the result","51ea32c8":"- Groupby Level Parameter\n- Let\u2019s set the index of the original dataframe to any of the target column we want to group","46ecf8f4":"- same can be done in better ways","0b9456b3":"- just change the freq parameter to one of the valid offset aliases. For instance, an annual summary using December as the last month","de9c4526":"- agg makes this simpler:","0e5d73b2":"- The results are good but including the sum of the unit price is not really that useful. Fortunately we can pass a dictionary to agg and specify what operations to apply to each column.","327e2113":"- Groupby Date and Time using Resample\n- It is a convenience method for resampling and converting the frequency of any DatetimeIndex, PeriodIndex, or TimedeltaIndex\n- The tricky part about using resample is that it only operates on an index.","ff740adc":"- Set the target column as dataframe index and then group by Index using the level parameter\n\n","1cf63dc2":"- GroupBy Month\n- Parameter key is the Groupby key, which selects the grouping column \n- Freq can be Hourly, Daily, Weekly, Monthly etc. Full specification of available frequency can be found\n- https:\/\/pandas.pydata.org\/pandas-docs\/stable\/user_guide\/timeseries.html#offset-aliases","d4ff3611":"- **New and improved aggregate function**\n- similar to group by\n- get the total of the ext price and quantity column as well as the average of the unit price","a9b24690":"- The aggregate function using a dictionary is useful but one challenge is that it does not preserve order. If you want to make sure your columns are in a specific order, you can use an OrderedDict :"}}