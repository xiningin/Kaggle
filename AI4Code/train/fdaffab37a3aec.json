{"cell_type":{"5307c4cd":"code","eac0c00e":"code","71b925b6":"code","0e703bda":"code","ba53a5c5":"code","b5b7b4ce":"code","9f9ccb92":"code","73c0dd17":"code","763fa2c9":"code","36a8f2ae":"code","0fad7d91":"code","05c8fcfb":"code","4bd965d9":"code","a4895c3a":"code","a0ac2a86":"code","c3feac0c":"code","bd3edbc0":"code","8377276c":"code","4d3fe59e":"code","0c578841":"code","94614ffb":"code","a2f226f8":"code","ed39af32":"code","9fbf5e27":"code","ef7dc6b1":"markdown","193eeca7":"markdown","8dba33b8":"markdown","d704a435":"markdown","fb49ab63":"markdown","d7084e69":"markdown","53428f7c":"markdown","e39930fa":"markdown","6ca82252":"markdown","a3401241":"markdown","30802d29":"markdown","ccf7e543":"markdown","e98a3139":"markdown","460a222b":"markdown","9b89eaa0":"markdown","eb7dff9a":"markdown","d5a8fa92":"markdown","fdb480ed":"markdown","de4b24d7":"markdown","d69663fa":"markdown","ba99c8f0":"markdown","6674a430":"markdown","7fb95f4e":"markdown","dcc36bcd":"markdown","5d1e6228":"markdown"},"source":{"5307c4cd":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","eac0c00e":"data = pd.read_csv('\/kaggle\/input\/heart-failure-clinical-data\/heart_failure_clinical_records_dataset.csv')","71b925b6":"data.head()","0e703bda":"data.shape","ba53a5c5":"!pip install pycaret","b5b7b4ce":"from pycaret.classification import *","9f9ccb92":"exp_clf101 = setup(data = data, target = 'DEATH_EVENT', session_id=123) ","73c0dd17":"compare_models(sort='AUC')","763fa2c9":"rf = create_model('rf')","36a8f2ae":"tuned_rf = tune_model(rf, optimize = 'AUC')","0fad7d91":"plot_model(tuned_rf, plot = 'auc')","05c8fcfb":"plot_model(tuned_rf, plot = 'pr')","4bd965d9":"plot_model(tuned_rf, plot='feature')","a4895c3a":"interpret_model(tuned_rf)","a0ac2a86":"interpret_model(tuned_rf, plot='correlation')","c3feac0c":"interpret_model(tuned_rf, plot = 'reason', observation = 10)","bd3edbc0":"plot_model(tuned_rf, plot = 'confusion_matrix')","8377276c":"evaluate_model(tuned_rf)","4d3fe59e":"final_rf = finalize_model(tuned_rf)","0c578841":"predict_model(final_rf);","94614ffb":"prediction = predict_model(final_rf, data = data)\nprediction.head()","a2f226f8":"save_model(final_rf,'Final RF Model 08Feb2020')","ed39af32":"saved_final_rf = load_model('Final RF Model 08Feb2020')","9fbf5e27":"new_prediction = predict_model(saved_final_rf, data=data)\nnew_prediction.head()","ef7dc6b1":"When a model is created using the create_model() function it uses the default hyperparameters. In order to tune hyperparameters, the tune_model() function is used. This function automatically tunes the hyperparameters of a model on a pre-defined search space and scores it using stratified cross validation. The output prints a score grid that shows Accuracy, AUC, Recall, Precision, F1 and Kappa by fold.\n\n**Note:** tune_model() does not take a trained model object as an input. It instead requires a model name to be passed as an abbreviated string similar to how it is passed in create_model(). All other functions in pycaret.classification require a trained model object as an argument.\n\n*Source: PyCaret Tutorial*","193eeca7":"**Installing PyCaret**\n\n**Pre-requisites **\n* Python 3.x\n* Latest version of pycaret\n* Internet connection to load data from pycaret's repository\n* Basic Knowledge of Binary Classification","8dba33b8":"Model finalization is the last step in the experiment. A normal machine learning workflow in PyCaret starts with setup(), followed by comparing all models using compare_models() and shortlisting a few candidate models (based on the metric of interest) to perform several modeling techniques such as hyperparameter tuning, ensembling etc. \n\nThis workflow will eventually lead you to the best model for use in making predictions on new and unseen data. The finalize_model() function fits the model onto the complete dataset and for the purposes of this analysis we have just used one model i.e. Random Forest based on the AUC metric and we will be finalizing this model for prediction purposes.\n","d704a435":"**Another way to analyze the performance of models is to use the evaluate_model() function which displays a user interface for all of the available plots for a given model. It internally uses the plot_model() function.**","fb49ab63":"**Let's look at the shape of the data**","d7084e69":"The Label and Score columns are added onto the prediction dataset.\nLabel is the prediction and score is the probability of the prediction. \nNotice that predicted results are concatenated to the original dataset while all the transformations are automatically performed in the background.","53428f7c":"# Confusion Matrix","e39930fa":"Once the setup has been succesfully executed it prints the information grid which contains several important pieces of information. Most of the information is related to the pre-processing pipeline which is constructed when setup() is executed. The majority of these features are out of scope for the purposes of this tutorial however a few important things to note at this stage include:\n\n**session_id :** A pseduo-random number distributed as a seed in all functions for later reproducibility. If no session_id is passed, a random number is automatically generated that is distributed to all functions. In this experiment, the session_id is set as 123 for later reproducibility.\n\n**Target Type :** Binary or Multiclass. The Target type is automatically detected and shown. There is no difference in how the experiment is performed for Binary or Multiclass problems. All functionalities are identical.\n\n**Label Encoded :** When the Target variable is of type string (i.e. 'Yes' or 'No') instead of 1 or 0, it automatically encodes the label into 1 and 0 and displays the mapping (0 : No, 1 : Yes) for reference. In this experiment no label encoding is required since the target variable is of type numeric.\n\n**Original Data :** Displays the original shape of the dataset. In this experiment (22800, 24) means 22,800 samples and 24 features including the target column.\n\n**Missing Values :** When there are missing values in the original data this will show as True. For this experiment there are no missing values in the dataset.\n\n**Numeric Features :** The number of features inferred as numeric. In this dataset, 14 out of 24 features are inferred as numeric.\n\n**Categorical Features :** The number of features inferred as categorical. In this dataset, 9 out of 24 features are inferred as categorical.\n\n**Transformed Train Set :** Displays the shape of the transformed training set. Notice that the original shape of (22800, 24) is transformed into (15959, 91) for the transformed train set and the number of features have increased to 91 from 24 due to categorical encoding\n\n**Transformed Test Set :** Displays the shape of the transformed test\/hold-out set. There are 6841 samples in test\/hold-out set. This split is based on the default value of 70\/30 that can be changed using the train_size parameter in setup.\nNotice how a few tasks that are imperative to perform modeling are automatically handled such as missing value imputation (in this case there are no missing values in the training data, but we still need imputers for unseen data), categorical encoding etc. Most of the parameters in setup() are optional and used for customizing the pre-processing pipeline. These parameters are out of scope for this tutorial but as you progress to the intermediate and expert levels, we will cover them in much greater detail.\n\n*Source: PyCaret Tutorial*","6ca82252":"We have now finished the experiment by finalizing the tuned_rf model which is now stored in final_rf variable. We have also used the model stored in final_rf to predict the outcomes. This brings us to the end of our analysis, but one question is still to be asked: What happens when you have more new data to predict? Do you have to go through the entire experiment again? The answer is no, PyCaret's inbuilt function save_model() allows you to save the model along with entire transformation pipeline for later use.","a3401241":"# Getting the data","30802d29":"# Tuning the Model","ccf7e543":"# Saving the Model","e98a3139":"# Loading the Saved Model","460a222b":"# About the Classification Modelling Technique To Predict Heart Failure\n![](https:\/\/www.nhlbi.nih.gov\/sites\/default\/files\/styles\/16x9_crop\/public\/2021-02\/Heart%20failure%20-%20shutterstock_1663310782.jpg?h=8854d737&itok=C_OOM84X)","9b89eaa0":"Once the model is loaded in the environment, you can simply use it to predict on any new data using the same predict_model() function. Below I have applied the loaded model to predict.","eb7dff9a":"# Setting Up Environment","d5a8fa92":"# Finalizing the Model","fdb480ed":"# Plot the Model","de4b24d7":"**Post optimization we can clearly see that the AUC metric improve by 2.5%**","d69663fa":"# Precision Recall Curve","ba99c8f0":"This analysis is primarily for rookies who are just begining their predictive modeling journey using the pycaret.classification Module.\n\nIn this analysis we will learn:\n\n**Getting Data:** How to import data from PyCaret repository\n**Setting up Environment:** How to setup an experiment in PyCaret and get started with building classification models\n**Create Model:** How to create a model, perform stratified cross validation and evaluate classification metrics\n**Tune Model:** How to automatically tune the hyper-parameters of a classification model\n**Plot Model:** How to analyze model performance using various plots\n**Finalize Model:** How to finalize the best model at the end of the experiment\n**Predict Model:** How to make predictions on new \/ unseen data","6674a430":"It's such a breeze to work with PyCaret with a half a line of code it has created 15 classification models that one can choose from based on their choice of metric. From this point onward I will work only with the Random Forest model \"rf\", given that it showed the best result based on my choice of metric.","7fb95f4e":"**Calling the classification model**","dcc36bcd":"**Comparing Model**\n\nComparing all models to evaluate performance is the recommended starting point for modeling once the setup is completed (unless you exactly know what kind of model you need, which is often not the case). This function trains all models in the model library and scores them using stratified cross validation for metric evaluation. The output prints a score grid that shows average Accuracy, AUC, Recall, Precision, F1 and Kappa accross the folds (10 by default) of all the available models in the model library.\n\nI have used the AUC as the metric of choice to sort the model efficacy, you can select your choice of metric.","5d1e6228":"# Feature Importance"}}