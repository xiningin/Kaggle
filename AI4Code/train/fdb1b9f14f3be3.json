{"cell_type":{"32f41b32":"code","81cfd1ba":"code","7b3f65d8":"code","b086ab19":"code","5149a474":"code","76c55d44":"code","b1c4fdfc":"code","715c7c49":"code","a7300cd8":"code","973b4205":"code","74766c54":"code","3f52122b":"code","c5e63aa9":"code","ee302680":"code","965a8435":"code","bbbf7d89":"code","f964a118":"code","b3e18044":"code","afca8fb7":"markdown","d4cbc29f":"markdown","4b15d19d":"markdown","1b99fdbf":"markdown","e978e406":"markdown","cf000f0e":"markdown","108eb329":"markdown","643d0806":"markdown","4bf49764":"markdown","1884241b":"markdown","5e3bd370":"markdown","6dd4345d":"markdown","4fb465ec":"markdown","c645ffa4":"markdown","a55f3aea":"markdown","11ac5fee":"markdown"},"source":{"32f41b32":"import tensorflow as tf\nimport keras \nfrom keras import layers\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport cv2\nimport os\nfrom tqdm import tqdm\nimport re\nfrom keras.preprocessing.image import img_to_array","81cfd1ba":"# GENERATE_RES = 3\n# # 1=32, 2=64, 3=96, 4=128\n# GENERATE_SQUARE = 96\n\n# IMAGE_CHANNELS = 3\n# PREVIEW_ROWS =4 \n# PREVIEW_COLS = 7\n# PREVIEW_MARGIN= 16\n\n# SEED_SIZE = 100\n# # DATA_PATH = 'images\/train'\n\n# EPOCHS = 50\n# BATCH_SIZE = 32\n\n# print(f\"will generate {GENERATE_SQUARE}px square images.\")","7b3f65d8":"# to get the files in proper order\ndef sorted_alphanumeric(data):  \n    convert = lambda text: int(text) if text.isdigit() else text.lower()\n    alphanum_key = lambda key: [convert(c) for c in re.split('([0-9]+)',key)]\n    return sorted(data,key = alphanum_key)\n# defining the size of the image\nSIZE = 128\n_img = []\npath = '..\/input\/child-images\/images\/Dis_data'\nfiles = os.listdir(path)\nfiles = sorted_alphanumeric(files)\nfor i in tqdm(files):    \n        if i == 'seed1000.png':\n            break\n        else:    \n            img = cv2.imread(path + '\/'+i,1)\n            # open cv reads images in BGR format so we have to convert it to RGB\n            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n            #resizing image\n            img = cv2.resize(img, (SIZE, SIZE))\n            img = (img - 127.5) \/ 127.5\n            imh = img.astype(float)\n            _img.append(img_to_array(img))","b086ab19":"def plot_images(sqr = 5):\n    plt.figure(figsize = (10,10))\n    plt.title(\"Real Images\",fontsize = 35)\n    for i in range(sqr * sqr):\n        plt.subplot(sqr,sqr,i+1)\n        plt.imshow(_img[i]*0.5 + 0.5 )\n        plt.xticks([])\n        plt.yticks([])\n\n# to plot images\nplot_images(4)\n    ","5149a474":"batch_size = 16\ndataset=tf.data.Dataset.from_tensor_slices(np.array(_img)).batch(batch_size)","76c55d44":"type(dataset)","b1c4fdfc":"latent_dim = 100\ndef Generator():\n    model = tf.keras.Sequential()\n    model.add(layers.Dense(128*128*3, use_bias=False, input_shape=(latent_dim,)))\n    model.add(layers.Reshape((128,128,3)))\n    # downsampling\n    model.add(tf.keras.layers.Conv2D(128,4, strides=2, padding='same',kernel_initializer='he_normal', use_bias=False))\n    model.add(tf.keras.layers.BatchNormalization())\n    model.add(tf.keras.layers.LeakyReLU())\n    model.add(tf.keras.layers.Conv2D(256,4, strides=2, padding='same',kernel_initializer='he_normal', use_bias=False))\n    model.add(tf.keras.layers.BatchNormalization())\n    model.add(tf.keras.layers.LeakyReLU())\n    model.add(tf.keras.layers.Conv2D(512,4, strides=2, padding='same',kernel_initializer='he_normal', use_bias=False))\n    model.add(tf.keras.layers.LeakyReLU())\n    #upsampling\n    model.add(tf.keras.layers.Conv2DTranspose(512, 4, strides=2,padding='same',kernel_initializer='he_normal',use_bias=False))\n    model.add(tf.keras.layers.BatchNormalization())\n    model.add(tf.keras.layers.LeakyReLU())\n    model.add(tf.keras.layers.Conv2DTranspose(256, 4, strides=2,padding='same',kernel_initializer='he_normal',use_bias=False))\n    model.add(tf.keras.layers.BatchNormalization())\n  \n    model.add(tf.keras.layers.Conv2DTranspose(128, 4, strides=2,padding='same',kernel_initializer='he_normal',use_bias=False))\n    model.add(tf.keras.layers.BatchNormalization())\n    model.add(tf.keras.layers.Conv2DTranspose(3,4,strides = 1, padding = 'same',activation = 'tanh'))\n    # Tanh activation function compress values between -1 and 1. \n    # This is why we compressed our images between -1 and 1 in readImage function.\n    \n\n    return model","715c7c49":"generator = Generator()\ngenerator.summary()","a7300cd8":"def Discriminator():\n    model = tf.keras.models.Sequential()\n    model.add(tf.keras.layers.Input((SIZE, SIZE, 3)))\n    model.add(tf.keras.layers.Conv2D(128,4, strides=2, padding='same',kernel_initializer='he_normal', use_bias=False))\n    model.add(tf.keras.layers.BatchNormalization())\n    model.add(tf.keras.layers.LeakyReLU())\n    model.add(tf.keras.layers.Conv2D(256,4, strides=2, padding='same',kernel_initializer='he_normal', use_bias=False))\n    model.add(tf.keras.layers.BatchNormalization())\n    model.add(tf.keras.layers.LeakyReLU())\n    model.add(tf.keras.layers.Conv2D(512,4, strides=2, padding='same',kernel_initializer='he_normal', use_bias=False))\n    model.add(tf.keras.layers.LeakyReLU())\n    model.add(tf.keras.layers.Flatten())\n    model.add(tf.keras.layers.Dense(1,activation = 'sigmoid'))\n    return model\n  \n","973b4205":"discriminator = Discriminator()\ndiscriminator.summary()","74766c54":"noise = np.random.normal(-1,1,(1,100))\nimg = generator(noise)\nplt.imshow(img[0,:,:,0])\nplt.show()","3f52122b":"optimizer = tf.keras.optimizers.RMSprop(\n        lr=.0001,\n        clipvalue=1.0,\n        decay=1e-8\n    )\n# Loss function for evaluating adversarial loss\ncross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits = True)","c5e63aa9":"# Define the loss function for the generators\n\n    # First argument of loss is real labels\n    # We've labeled our images as 1 (real) because\n    # we're trying to fool discriminator\ndef generator_loss(fake_output):\n    return cross_entropy(tf.ones_like(fake_output),fake_output)\n\n# Define the loss function for the discriminators\ndef discriminator_loss(fake_output, real_output):\n    fake_loss = cross_entropy(tf.zeros_like(fake_output),fake_output)\n    real_loss = cross_entropy(tf.ones_like(real_output),real_output)\n    return fake_loss + real_loss","ee302680":"def plot_generated_images(square = 5, epochs = 0):\n    \n    \n  plt.figure(figsize = (10,10))\n  for i in range(square * square):\n    if(i == square \/\/2):\n        plt.title(\"Generated Image at Epoch:{}\\n\".format(epochs), fontsize = 32, color = 'black')\n    plt.subplot(square, square, i+1)\n    noise = np.random.normal(0,1,(1,latent_dim))\n    img = generator(noise)\n    plt.imshow(np.clip((img[0,...]+1)\/2, 0, 1))\n    \n    plt.xticks([])\n    plt.yticks([])\n    plt.grid()","965a8435":"def train_steps(images):\n    noise = np.random.normal(0,1,(batch_size,latent_dim))\n    with tf.GradientTape() as gen_tape , tf.GradientTape() as disc_tape:\n        # Generator generated images\n        generated_images = generator(noise) #training = True\n        \n        # We've sent our real and fake images to the discriminator\n        # and taken the decisions of it.\n        fake_output = discriminator(generated_images) #training = True\n        real_output = discriminator(images) #training = True\n        \n        # We've computed losses of generator and discriminator\n        gen_loss = generator_loss(fake_output)\n        dis_loss = discriminator_loss(fake_output, real_output)\n        \n        # We've computed gradients of networks and updated variables using those gradients.\n        gradient_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables)    \n        gradient_of_discriminator = disc_tape.gradient(dis_loss, discriminator.trainable_variables)\n    \n    # generator_optimizer\n    optimizer.apply_gradients(zip(gradient_of_generator,generator.trainable_variables))\n    # discriminator_optimizer\n    optimizer.apply_gradients(zip(gradient_of_discriminator, discriminator.trainable_variables))\n\n    return gen_loss, dis_loss","bbbf7d89":"import time\ndef train(epochs,dataset):\n    \n    for epoch in range(epochs):\n        start = time.time()\n        for images in dataset:\n            train_steps(images)\n        print(\"Epoch:{} Time:{}\".format(epoch+1,np.round(time.time() - start),2))  \n            \n        plot_generated_images(4,epoch+1)\n        \n    ","f964a118":"\ntrain(3,dataset)\n# i had train model previously for more than 10 epochs so generated images are quiet good","b3e18044":"generator.save('generator.h5')\ndiscriminator.save(\"discriminator.h5\")","afca8fb7":"### Let's plot image generated by generator before training","d4cbc29f":"### Components of GANs\n<b> Generator <\/b>: Generator are neural network that learns to generate data which resemble with the input distribution. The generator model take fixed dimension random vector from Gaussian distribution as input and generate the sample out of it which resemble with input.\n\n<b> Discriminator <\/b>: Discriminator are simple neural network that distinguish fake and real data.The discriminator model takes an example from the domain as input (real or generated) and predicts a binary class label of real or fake.\n\nGenerative adversarial networks are based on a game theoretic scenario in which the generator network must compete against an adversary. The generator network directly produces samples. Its adversary, the discriminator network, attempts to distinguish between samples drawn from the training data and samples drawn from the generator.","4b15d19d":"# Working of GANs\n<img src = 'https:\/\/cdn.analyticsvidhya.com\/wp-content\/uploads\/2017\/06\/11000153\/g1.jpg'>\n\nFirst of all we take batch of random vector from the Gaussian distribution and generate fake image out of it using generator. Since generator isn't trained so generated image donot resemble with the real input distribution. We take batches of image from the input distribution along with generated fake images and fed it to discriminator so that it learns to distinguish between real and fake images.\nNow, after training discriminator, we take the batch of images that generator generated and fed them through discriminator again (here we donot fed real images), discriminator will provide an output probabilities, these values are then compared with the probability that the generator should generated (ie 1), error is calculated and backpropagated through the generator and the weight are updated.\nThis above process is repeated until generated images resemble with the input distribution.","1b99fdbf":"## Objective: To generate fake faces of human","e978e406":"Generative Adversarial Networks (GANs) are generative models. They are uses unsupervised technique to generate new things. GAN models learns pattern in input data in such a way that they can generate new sample which resemble with the input data. The main aim of generative adversarial network is to match generated distribution with the original data distribution.\n\nGANs are an exciting and rapidly changing field, delivering generative models ability to generate realistic examples across a range of problem domains, most notably in image-to-image translation tasks such as translating photos of summer to winter or day to night,coloring images and in generating fake photos that even human cannot categorized as fake image.\nThis <a href = 'https:\/\/thispersondoesnotexist.com\/'> site <\/a> uses GAN to generate fake human faces which are similar to real human","cf000f0e":"## Load data\nHere I have used face-mask-lite-dataset, out of available 10000 images i have only use 9090 image. I have read image using opencv since opencv reads image in bgr format i have converted it back to rgb format using cvtColor function. These images are resize into 128 by 128 using resize function and are finally converted to array and are appended in empty array","108eb329":"### Defining training steps","643d0806":"## Import necessary Libraries","4bf49764":"## Visailze our images","1884241b":"## Application of GANs\n\n## 1. Generating fake faces\n<img src = \"https:\/\/3qeqpr26caki16dnhd19sv6by6v-wpengine.netdna-ssl.com\/wp-content\/uploads\/2019\/06\/Examples-of-Photorealistic-GAN-Generated-Faces.png\">\n\n## 2. Generate Examples for Image Datasets\n\n## 3. Face Aging\n<img src = 'https:\/\/www.baycare.net\/media\/5076\/botox-aging-face-plastic-surgery.jpg' height = '600px' width = '500px'>\n\n\n\n## 4. Super Resolution\n<img src = 'https:\/\/miro.medium.com\/max\/700\/1*E-JmUwv7zbwjzFm1hJLxPA.png'>\n\n\n## 5. Image-to-Image Translation\n<img src = 'https:\/\/3qeqpr26caki16dnhd19sv6by6v-wpengine.netdna-ssl.com\/wp-content\/uploads\/2019\/06\/Example-of-Photographs-of-Daytime-Cityscapes-to-Nighttime-with-pix2pix.png'>\n\n## 6.  Photos to Emojis\n<img src = 'https:\/\/3qeqpr26caki16dnhd19sv6by6v-wpengine.netdna-ssl.com\/wp-content\/uploads\/2019\/06\/Example-of-Celebrity-Photographs-and-GAN-Generated-Emojis.png'>\n\n\n\n\n## 7.Text to image Translation\n\n<img src = 'https:\/\/3qeqpr26caki16dnhd19sv6by6v-wpengine.netdna-ssl.com\/wp-content\/uploads\/2019\/06\/Example-of-Textual-Descriptions-and-GAN-Generated-Photographs-of-Birds.png'>\n\n\n## 8. Generate Cartoon characters\n<img src = 'https:\/\/3qeqpr26caki16dnhd19sv6by6v-wpengine.netdna-ssl.com\/wp-content\/uploads\/2019\/06\/Example-of-GAN-Generated-Anime-Character-Faces.png'>\n   \n","5e3bd370":"Here, i have defined batch size so that these batches of images can be fed directly to the discriminator network","6dd4345d":"# Discriminator\n\n* Now we'll build our discriminator network.\n* Discriminator network is a CNN based image classifier that classify images real or fake.\n\nHere, discriminator model take 128 by 128 by 3 image that can be real or generated. This input image is downsampled using Convolution layer and is finally flattened and is fed to single neuron so that it can distinguish real and fake image. Since, final layer uses sigmoid function as activation, it output value in between 0 and 1. Here value greater than 0.5 refers to real and less than 0.5 refers to fake image. The output of discriminator is used in training of generator.","4fb465ec":"## function to plot generated images\n","c645ffa4":"## Training","a55f3aea":"### Defining loss function and optimizer ","11ac5fee":"# Generator \nHere, I have defined generator network. It take random vector from normal distribution as input. This random vector is passed through dense layer and is reshaped and is finally fed through Convolution layers. Here, convolution layers does downsampling of our latent vector, after series of convolution batch normalization and leakyrelu layer our downsampled latent vector is upsampled using Conv2DTranspose.\n\nThe final output layer of Generator generate 128 by 128 by 3 image. The final layer of generator uses hyperbolic tangent as activation to squash the value in between -1 and 1. Generator model looks like simple autoencoder model, where input data is downsampled first and is finally upsampled ."}}