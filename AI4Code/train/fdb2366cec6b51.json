{"cell_type":{"09bf8502":"code","46fa9f9f":"code","0173048a":"code","d1a548bf":"code","599cae8c":"code","21e47a7c":"code","871c1826":"code","d63e8ae2":"code","59b5ec96":"code","1a59ed5c":"markdown","b59ea65a":"markdown"},"source":{"09bf8502":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport os\nimport json\nimport librosa\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nimport tensorflow as tf\n\n# make sure ffmpeg is installed as backend.\n!apt install -y ffmpeg\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        #print(os.path.join(dirname, filename))\n        pass\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","46fa9f9f":"# Dataset location\nSOURCE_PATH = '\/kaggle\/input\/gtzan-dataset-music-genre-classification\/Data\/genres_original\/'\n\n# Path to labels and processed data file, json format.\nJSON_PATH = '\/kaggle\/working\/data.json'\n\n# Sampling rate.\nsr = 22050\n\n# Let's make sure all files have the same amount of samples and pick a duration right under 30 seconds.\nTOTAL_SAMPLES = 29 * sr\n\n# The dataset contains 999 files. Lets make it bigger. \n# X amount of slices => X times more training examples.\nNUM_SLICES = 10\nSAMPLES_PER_SLICE = int(TOTAL_SAMPLES \/ NUM_SLICES)","0173048a":"def preprocess_data(source_path, json_path):\n\n    # Let's create a dictionary of labels and processed data.\n    mydict = {\n        \"labels\": [],\n        \"mfcc\": []\n        }\n\n    # Let's browse each file, slice it and generate the 13 band mfcc for each slice.\n    for i, (dirpath, dirnames, filenames) in enumerate(os.walk(source_path)):\n        for file in filenames:\n            # exclude a corrupted wav file that makes everything crash.\n            if os.path.join(dirpath, file) != '\/kaggle\/input\/gtzan-dataset-music-genre-classification\/Data\/genres_original\/jazz\/jazz.00054.wav':\n                song, sr = librosa.load(os.path.join(dirpath, file), duration=29)\n                for s in range(NUM_SLICES):\n                    start_sample = SAMPLES_PER_SLICE * s\n                    end_sample = start_sample + SAMPLES_PER_SLICE\n                    mfcc = librosa.feature.mfcc(y=song[start_sample:end_sample], sr=sr, n_mfcc=13)\n                    mfcc = mfcc.T\n                    mydict[\"labels\"].append(i-1)\n                    mydict[\"mfcc\"].append(mfcc.tolist())\n            else:\n                pass\n\n    # Let's write the dictionary in a json file.    \n    with open(json_path, 'w') as f:\n        json.dump(mydict, f)\n    f.close()","d1a548bf":"def load_data(json_path):\n\n    with open(json_path, 'r') as f:\n        data = json.load(f)\n    f.close()\n\n    # Let's load our data into numpy arrays for TensorFlow compatibility.\n    X = np.array(data[\"mfcc\"])\n    y = np.array(data[\"labels\"])\n\n    return X, y","599cae8c":"def prepare_datasets(inputs, targets, split_size):\n    \n    # Creating a validation set and a test set.\n    inputs_train, inputs_val, targets_train, targets_val = train_test_split(inputs, targets, test_size=split_size)\n    inputs_train, inputs_test, targets_train, targets_test = train_test_split(inputs_train, targets_train, test_size=split_size)\n    \n    # Our CNN model expects 3D input shape.\n    inputs_train = inputs_train[..., np.newaxis]\n    inputs_val = inputs_val[..., np.newaxis]\n    inputs_test = inputs_test[..., np.newaxis]\n    \n    return inputs_train, inputs_val, inputs_test, targets_train, targets_val, targets_test","21e47a7c":"def design_model(input_shape):\n\n    # Let's design the model architecture.\n    model = tf.keras.models.Sequential([\n        \n        tf.keras.layers.Conv2D(32, (3,3), activation='relu', input_shape=input_shape),\n        tf.keras.layers.MaxPooling2D((3,3), strides=(2,2), padding='same'),\n        tf.keras.layers.BatchNormalization(),\n        \n        tf.keras.layers.Conv2D(32, (3,3), activation='relu'),\n        tf.keras.layers.MaxPooling2D((3,3), strides=(2,2), padding='same'),\n        tf.keras.layers.BatchNormalization(),\n        \n        tf.keras.layers.Conv2D(32, (2,2), activation='relu'),\n        tf.keras.layers.MaxPooling2D((3,3), strides=(2,2), padding='same'),\n        tf.keras.layers.BatchNormalization(),\n        tf.keras.layers.Dropout(0.3),\n        \n        tf.keras.layers.Flatten(),\n        tf.keras.layers.Dense(64, activation='relu'), \n        tf.keras.layers.Dense(len(np.unique(targets)), activation='softmax')\n    ])\n\n    return model","871c1826":"def make_prediction(model, X, y, idx):\n    \n    genre_dict = {\n        0 : \"blues\",\n        1 : \"classical\",\n        2 : \"country\",\n        3 : \"disco\",\n        4 : \"hiphop\",\n        5 : \"jazz\",\n        6 : \"metal\",\n        7 : \"pop\",\n        8 : \"reggae\",\n        9 : \"rock\",\n        }\n        \n    predictions = model.predict(X)\n    genre = np.argmax(predictions[idx])\n    \n    print(\"\\n---Now testing the model for one audio file---\\nThe model predicts: {}, and ground truth is: {}.\\n\".format(genre_dict[genre], genre_dict[y[idx]]))\n","d63e8ae2":"def plot_performance(hist):\n    \n    acc = hist.history['acc']\n    val_acc = hist.history['val_acc']\n    loss = hist.history['loss']\n    val_loss = hist.history['val_loss']\n\n    epochs = range(len(acc))\n\n    plt.plot(epochs, acc, 'r', label='Training accuracy')\n    plt.plot(epochs, val_acc, 'b', label='Validation accuracy')\n    plt.title('Training and validation accuracy')\n    plt.legend()\n    plt.figure()\n\n    plt.plot(epochs, loss, 'r', label='Training Loss')\n    plt.plot(epochs, val_loss, 'b', label='Validation Loss')\n    plt.title('Training and validation loss')\n    plt.legend()\n\n    plt.show()","59b5ec96":"if __name__ == \"__main__\":\n\n    preprocess_data(source_path=SOURCE_PATH, json_path=JSON_PATH)\n    \n    inputs, targets = load_data(json_path=JSON_PATH)\n    \n    Xtrain, Xval, Xtest, ytrain, yval, ytest = prepare_datasets(inputs, targets, 0.2)\n\n    input_shape = (Xtrain.shape[1], Xtrain.shape[2], 1)\n    model = design_model(input_shape)\n\n    # Selection of the optimizer, loss type and metrics for performance evaluation.\n    model.compile(optimizer = tf.keras.optimizers.RMSprop(lr=0.001),\n                     loss='sparse_categorical_crossentropy',\n                     metrics = ['acc']\n                     )\n\n    model.summary()\n\n    #Training the model.\n    history = model.fit(Xtrain, ytrain,\n                        validation_data=(Xval, yval),\n                        epochs=30,\n                        batch_size=32\n                        )\n\n    plot_performance(history)\n\n    # Testing the model on never seen before data.\n    make_prediction(model, Xtest, ytest, 24)","1a59ed5c":"\nThe objective of this project is to classify 30 sec audio files by genre using TensorFlow and Librosa. To classify these audio samples in .wav format, we will preprocess them by calculating their MFCC, which is a temporal representation of the energy variations for each perceived frequency band. In this case, we are choosing 13 bands. For improved modularity and clarity, we will create a dictionnary of MFCCs and associated labels in a separate json file.","b59ea65a":"Our model predicts genres with more than 75% accuracy on the validation set."}}