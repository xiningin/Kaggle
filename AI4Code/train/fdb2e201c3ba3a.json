{"cell_type":{"3206e64e":"code","ab4b9c7c":"code","88e718ec":"code","25a680c5":"code","a90115a3":"code","e0af2798":"code","77327f9f":"code","5f392d51":"code","aed22d7b":"code","e329236f":"code","90db7948":"code","99a35713":"code","ffc940be":"code","94b21d5b":"code","06d4dbea":"code","bb8e4fdf":"code","cbd3e2b4":"markdown","2f9cb39f":"markdown","df318ba7":"markdown"},"source":{"3206e64e":"!pip install tensorflow_addons==0.9.1","ab4b9c7c":"from tensorflow.keras.layers import (TimeDistributed, Dropout, BatchNormalization, Flatten, Convolution1D, Activation, Input, Dense, LSTM, Lambda, Bidirectional,\n                                     Add, AveragePooling1D, Multiply, GRU, GRUCell, LSTMCell, SimpleRNNCell, SimpleRNN, TimeDistributed, RNN,\n                                     RepeatVector, Conv1D, MaxPooling1D, Concatenate, GlobalAveragePooling1D, UpSampling1D)\nimport tensorflow as tf\nfrom tensorflow.keras.layers import Layer\nfrom tensorflow.keras import initializers\nfrom tensorflow.keras import regularizers\nfrom tensorflow.keras import constraints\nfrom tensorflow.keras.utils import get_custom_objects\nfrom tensorflow.keras.layers import Activation\n\nfrom tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, Callback, ReduceLROnPlateau, LearningRateScheduler\nfrom tensorflow.keras.losses import binary_crossentropy, categorical_crossentropy, mean_squared_error\nfrom tensorflow.keras.optimizers import Adam, RMSprop, SGD\nfrom tensorflow.keras.utils import Sequence, to_categorical\nfrom tensorflow.keras import losses, models, optimizers\nfrom tensorflow.keras import backend as K\nimport tensorflow as tf\nfrom typing import List, NoReturn, Union, Tuple, Optional, Text, Generic, Callable, Dict\nfrom sklearn.metrics import cohen_kappa_score, accuracy_score, mean_squared_error, f1_score, confusion_matrix, recall_score,precision_score,log_loss\nfrom logging import getLogger, Formatter, StreamHandler, FileHandler, INFO\nfrom sklearn.model_selection import KFold, GroupKFold, StratifiedKFold\nfrom tqdm import tqdm_notebook as tqdm\nfrom contextlib import contextmanager\nfrom joblib import Parallel, delayed\nfrom IPython.display import display\nfrom sklearn import preprocessing\nimport tensorflow_addons as tfa\nimport scipy.stats as stats\nimport random as rn\nimport pandas as pd\nimport numpy as np\nimport scipy as sp\nimport itertools\nimport warnings\nimport time\nimport pywt\nimport os\nimport gc\nfrom datetime import datetime\nimport matplotlib.pylab as plt\nimport seaborn as sns\nimport random\nfrom typing import Tuple\n\nwarnings.simplefilter('ignore')\nwarnings.filterwarnings('ignore')\npd.set_option('display.max_columns', 1000)\npd.set_option('display.max_rows', 500)\n%matplotlib inline","88e718ec":"def plot_cm(y_true, y_pred, title):\n    figsize=(14,14)\n    y_pred = y_pred.astype(int)\n    cm = confusion_matrix(y_true, y_pred, labels=np.unique(y_true))\n    cm_sum = np.sum(cm, axis=1, keepdims=True)\n    cm_perc = cm \/ cm_sum.astype(float) * 100\n    annot = np.empty_like(cm).astype(str)\n    nrows, ncols = cm.shape\n    for i in range(nrows):\n        for j in range(ncols):\n            c = cm[i, j]\n            p = cm_perc[i, j]\n            if i == j:\n                s = cm_sum[i]\n                annot[i, j] = '%.1f%%\\n%d\/%d' % (p, c, s)\n            elif c == 0:\n                annot[i, j] = ''\n            else:\n                annot[i, j] = '%.1f%%\\n%d' % (p, c)\n    cm = pd.DataFrame(cm, index=np.unique(y_true), columns=np.unique(y_true))\n    cm.index.name = 'Actual'\n    cm.columns.name = 'Predicted'\n    fig, ax = plt.subplots(figsize=figsize)\n    plt.title(title)\n    sns.heatmap(cm, cmap= \"YlGnBu\", annot=annot, fmt='', ax=ax)\n\n\ndef seed_everything(seed):\n    random.seed(seed)\n    np.random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    tf.random.set_seed(seed)\n    \ndef lr_schedule(epoch):\n    if epoch < 40:\n        lr = LR\n    elif epoch < 20:\n        lr = LR \/ 10\n    else:\n        lr = LR \/ 100\n    return lr\n\ndef augment(X, y):\n    \n    X = np.vstack((X, np.flip(X, axis=1))) \n    y = np.vstack((y, np.flip(y, axis=1)))\n    \n    return X, y","25a680c5":"def Classifier(shape_):\n    def wave_block(x, filters, kernel_size, n):\n        dilation_rates = [2**i for i in range(n)]\n        x = Conv1D(filters = filters,\n                   kernel_size = 1,\n                   padding = 'same')(x)\n        res_x = x\n        for dilation_rate in dilation_rates:\n            tanh_out = Conv1D(filters = filters,\n                              kernel_size = kernel_size,\n                              padding = 'same', \n                              activation = 'tanh', \n                              dilation_rate = dilation_rate)(x)\n            sigm_out = Conv1D(filters = filters,\n                              kernel_size = kernel_size,\n                              padding = 'same',\n                              activation = 'sigmoid', \n                              dilation_rate = dilation_rate)(x)\n            x = Multiply()([tanh_out, sigm_out])\n            x = Conv1D(filters = filters,\n                       kernel_size = 1,\n                       padding = 'same')(x)\n            res_x = Add()([res_x, x])\n        return res_x\n    \n    inp = Input(shape = (shape_))\n    \n    x = wave_block(inp, 16, 3, 8)\n    x = wave_block(x, 32, 3, 12)\n    x = wave_block(x, 64, 3, 4)\n    x = wave_block(x, 128, 3, 1)\n    #x = Bidirectional(CuDNNGRU(128,return_sequences=True))(x)\n\n    out = Dense(11, activation = 'softmax', name = 'out')(x)\n    \n    model = models.Model(inputs = inp, outputs = out)\n    \n    opt = Adam(lr = LR)\n\n    model.compile(loss = losses.CategoricalCrossentropy(), optimizer = opt, metrics = ['accuracy'])\n\n    return model\n\n# Class to return F1 Macro at the end of each epoch\n# It also enables us to apply early stopping on F1 Macro\nclass Metric(Callback):\n    def __init__(self, model, callbacks, data):\n        super().__init__()\n        self.model = model\n        self.callbacks = callbacks\n        self.data = data\n\n    def on_train_begin(self, logs=None):\n        for callback in self.callbacks:\n            callback.on_train_begin(logs)\n\n    def on_train_end(self, logs=None):\n        for callback in self.callbacks:\n            callback.on_train_end(logs)\n\n    def on_epoch_end(self, batch, logs=None):\n        X_valid, y_valid = self.data[1][0], self.data[1][1]\n        y_valid = np.argmax(y_valid, axis=2).reshape(-1)\n        valid_pred = np.argmax(self.model.predict(X_valid), axis=2).reshape(-1)\n        valid_recall = recall_score(y_valid, valid_pred, average=\"macro\")        \n        valid_precision = precision_score(y_valid, valid_pred, average=\"macro\")        \n        valid_score = f1_score(y_valid, valid_pred, average=\"macro\")        \n        logs['Valid_F1Macro'] = valid_score\n        logs['Valid_Recall'] = valid_recall\n        logs['Valid_Precision'] = valid_precision\n\n        print(f\"Validation F1 Macro {valid_score:1.6f} Validation Recall {valid_recall:1.6f} Validation Precision {valid_precision:1.6f}\")\n        # print(' Train F1 Macro', train_score, 'Validation F1 Macro', valid_score)\n\n        for callback in self.callbacks:\n            callback.on_epoch_end(batch, logs)\n        \n        gc.collect()            ","a90115a3":"# configurations and main hyperparammeters\nEPOCHS = 70\nNNBATCHSIZE = 16\nGROUP_BATCH_SIZE = 4000\nSEED = 321\nLR = 0.001\nSPLITS = 5\nSLIDE = 800\nseed_everything(SEED)","e0af2798":"# Load Data\ntrain = pd.read_csv(\"\/kaggle\/input\/remove-trends-giba\/train_clean_giba.csv\", usecols=[\"signal\",\"open_channels\"], dtype={'signal': np.float32, 'open_channels':np.int32})\ntest  = pd.read_csv(\"\/kaggle\/input\/remove-trends-giba\/test_clean_giba.csv\", usecols=[\"signal\"], dtype={'signal': np.float32})\n\n# 5+8 Augmentation to Create new batch with 10 channels\ntrain['group'] = np.arange(train.shape[0])\/\/500_000\naug_df = train[train[\"group\"] == 5].copy()\naug_df[\"group\"] = 10\n\nfor col in [\"signal\", \"open_channels\"]:\n    aug_df[col] += train[train[\"group\"] == 8][col].values\n\ntrain = train.append(aug_df, sort=False).reset_index(drop=True)\ndel aug_df\ngc.collect()","77327f9f":"# Load OOFs for stacking\ntrain[\"mlp\"] = np.load(\"\/kaggle\/input\/into-the-wild-mlp-regression\/mlp_reg.npz\")['valid']  \ntest[\"mlp\"] = np.load(\"\/kaggle\/input\/into-the-wild-mlp-regression\/mlp_reg.npz\")['test']  \n\ntrain[\"lgb\"] = np.load(\"\/kaggle\/input\/into-the-wild-lgb-regression\/lgb_reg.npz\")['valid']  \ntest[\"lgb\"] = np.load(\"\/kaggle\/input\/into-the-wild-lgb-regression\/lgb_reg.npz\")['test']  \n\nY_train_proba = np.load(\"\/kaggle\/input\/into-the-wild-rfc-classification\/rfc_clf.npz\")[\"valid\"]\nY_test_proba = np.load(\"\/kaggle\/input\/into-the-wild-rfc-classification\/rfc_clf.npz\")[\"test\"]\n\nfor i in range(11):\n    train[f\"rfc_{i}\"] = Y_train_proba[:, i]\n    test[f\"rfc_{i}\"] = Y_test_proba[:, i]\n\ndel Y_train_proba, Y_test_proba\ngc.collect()","5f392d51":"# Standart Scaling\nfor item in ['signal','mlp','lgb']:\n    if item in train.columns:\n        print(item)\n        train_input_mean = train[item].mean()\n        train_input_sigma = train[item].std()\n        train[item]= (train[item] - train_input_mean) \/ train_input_sigma\n        test[item] = (test[item] - train_input_mean) \/ train_input_sigma\n    \n# Batching for train and test\ntrain['batch'] = train.groupby(train.index\/\/GROUP_BATCH_SIZE, sort=False)['signal'].agg(['ngroup']).values\ntrain['batch'] = train['batch'].astype(np.uint16)\n\ntest['batch'] = test.groupby(test.index\/\/100_000, sort=False)['signal'].agg(['ngroup']).values\ntest['batch'] = test['batch'].astype(np.uint16)\n\n# Fill NAs and Select Features\nfeats = [col for col in train.columns if col not in ['index','batch', 'open_channels', 'time', 'group']]\ntrain = train.replace([np.inf, -np.inf], np.nan)\ntest = test.replace([np.inf, -np.inf], np.nan)\n\nfor col in feats:\n    col_mean = pd.concat([train[col], test[col]], axis=0).mean()\n    train[col] = train[col].fillna(col_mean)\n    test[col] = test[col].fillna(col_mean)\n\nprint(feats)","aed22d7b":"def run_cv_model_by_batch(train, test, n_splits, feats, nn_epochs, nn_batch_size):\n    seed_everything(SEED)\n    K.clear_session()\n    config = tf.compat.v1.ConfigProto(intra_op_parallelism_threads=1,inter_op_parallelism_threads=1)\n    sess = tf.compat.v1.Session(graph=tf.compat.v1.get_default_graph(), config=config)\n    tf.compat.v1.keras.backend.set_session(sess)\n    oof_ = np.zeros((len(train), 11))\n    oof_tta = np.zeros((len(train), 11))\n    preds_ = np.zeros((len(test), 11))\n    target = ['open_channels']\n    \n    group = train['batch']\n    kf = GroupKFold(n_splits=5)\n    splits = [x for x in kf.split(train, train[target], group)]\n\n    new_splits = []\n    for sp in splits:\n        new_split = []\n        new_split.append(np.unique(group[sp[0]]))\n        new_split.append(np.unique(group[sp[1]]))\n        new_split.append(sp[1])    \n        new_splits.append(new_split)\n\n    # pivot target columns to transform the net to a multiclass classification estructure (you can also leave it in 1 vector with sparsecategoricalcrossentropy loss function)\n    tr = pd.concat([pd.get_dummies(train.open_channels), train[['batch']]], axis=1)\n    tr.columns = ['target_'+str(i) for i in range(11)] + ['batch']\n    target_cols = ['target_'+str(i) for i in range(11)]\n    train_tr = np.array(list(tr.groupby('batch').apply(lambda x: x[target_cols].values))).astype(np.float32)\n\n    del tr, group\n    gc.collect()\n    \n    train = np.array(list(train.groupby('batch').apply(lambda x: x[feats].values)))\n    test = np.array(list(test.groupby('batch').apply(lambda x: x[feats].values)))\n    test_flip = np.flip(test, axis=1)\n    \n    for n_fold, (tr_idx, val_idx, val_orig_idx) in enumerate(new_splits[0:], start=0):\n        print(\"=\"*11,f'Training Started for Fold {n_fold+1}',\"=\"*11,\"\\n\")\n        train_x, train_y = train[tr_idx], train_tr[tr_idx]\n        valid_x, valid_y = train[val_idx], train_tr[val_idx]\n        \n        train_x, train_y = augment(train_x, train_y)\n        \n        gc.collect()\n                \n        valid_x_flip = np.flip(valid_x, axis=1)\n\n        shape_ = (None, train_x.shape[2])\n        model = Classifier(shape_)\n\n        cp = ModelCheckpoint(f\"model_fold{n_fold+1}.h5\", monitor='Valid_F1Macro', mode='max',save_best_only=True, verbose=1, period=1)\n        cp.set_model(model)\n        #es = EarlyStopping(monitor='Valid_F1Macro',mode='min',restore_best_weights=True,verbose=1,patience=21)\n        #es.set_model(model)\n\n        cb_lr_schedule = LearningRateScheduler(lr_schedule)               \n        metric = Metric(model, [cp], [(train_x, train_y), (valid_x, valid_y)]) # ,es\n        \n        model.fit(train_x, train_y,epochs = nn_epochs,\n                  callbacks = [cb_lr_schedule, metric], # MacroF1(model, [valid_x_sig, valid_x_oof], valid_y)\n                  batch_size = nn_batch_size, verbose = 0,\n                  validation_data = (valid_x, valid_y))\n        \n        model.load_weights(f\"model_fold{n_fold+1}.h5\")\n        preds_f = model.predict(valid_x)\n        preds_f_tta = np.flip(model.predict(valid_x_flip), axis=1)\n\n        f1_score_ = f1_score(np.argmax(valid_y, axis=2).reshape(-1),  np.argmax(preds_f, axis=2).reshape(-1), average = 'macro')\n        print(f'Training fold {n_fold + 1} completed. F1 Macro : {f1_score_ :1.5f}')\n        f1_score_ = f1_score(np.argmax(valid_y, axis=2).reshape(-1),  np.argmax(preds_f + preds_f_tta, axis=2).reshape(-1), average = 'macro')\n        print(f'Training fold {n_fold + 1} completed. F1 Macro with TTA : {f1_score_ :1.5f}','\\n')\n        \n        oof_[val_orig_idx,:] += preds_f.reshape(-1, preds_f.shape[-1])\n        oof_tta[val_orig_idx,:] += preds_f_tta.reshape(-1, preds_f.shape[-1])\n        \n        def sliding_predict(model, x):\n            pred = np.zeros((x.shape[0], 100_000, 11))\n            for begin in range(0, 100_000, SLIDE):\n                end = begin + GROUP_BATCH_SIZE\n                pred[:, begin:end, :] += model.predict(x[:, begin:end, :])\n            return pred\n        \n        te_preds = sliding_predict(model, test)\n        te_preds = te_preds.reshape(-1, te_preds.shape[-1])           \n        preds_ += te_preds\n        \n        te_preds = np.flip(sliding_predict(model, test_flip), axis=1)\n        te_preds = te_preds.reshape(-1, te_preds.shape[-1])           \n        preds_ += te_preds\n\n    # calculate the oof macro f1_score\n    f1_score_ = f1_score(np.argmax(train_tr, axis = 2).reshape(-1),  np.argmax(oof_, axis = 1), average = 'macro') # axis 2 for the 3 Dimension array and axis 1 for the 2 Domension Array (extracting the best class)\n    print(f'Training completed. OOF F1 Macro : {f1_score_:1.5f}')\n    f1_score_ = f1_score(np.argmax(train_tr, axis = 2).reshape(-1),  np.argmax(oof_ + oof_tta, axis = 1), average = 'macro') # axis 2 for the 3 Dimension array and axis 1 for the 2 Domension Array (extracting the best class)\n    print(f'Training completed. OOF F1 Macro with TTA : {f1_score_:1.5f}')\n    return preds_, oof_, oof_tta","e329236f":"preds, oof, oof_tta = run_cv_model_by_batch(train, test, n_splits=5, feats=feats, nn_epochs=EPOCHS, nn_batch_size=NNBATCHSIZE)","90db7948":"oof_df = train[['signal','open_channels']].copy()\noof_df[\"oof\"] = np.argmax(oof+oof_tta, axis=1)\noof_df = oof_df.head(5000_000)\ngc.collect()\n\noof_f1 = f1_score(oof_df['open_channels'],oof_df['oof'],average = 'macro')\noof_recall = recall_score(oof_df['open_channels'],oof_df['oof'],average = 'macro')\noof_precision = precision_score(oof_df['open_channels'],oof_df['oof'],average = 'macro')\noof_loss = log_loss(oof_df[\"open_channels\"], (oof\/2+oof_tta\/2)[:5000_000])\n\nprint(f\"OOF F1 Macro Score: {oof_f1:.6f} - OOF Recall Score: {oof_recall:.6f} - OOF Precision Score: {oof_precision:.6f}, OOF Log Loss Score: {oof_loss:.6f} \")","99a35713":"np.savez_compressed(f'wavenet.npz',valid=oof, test=preds,tta=oof_tta)","ffc940be":"res = 1000 # Resolution of signal plots\nnum_batches = 10\nbatch_size = 500000\n\nplt.figure(figsize=(20,5));\nplt.plot(range(0,oof_df.shape[0],res),oof_df.open_channels[0::res])\nplt.plot(range(0,oof_df.shape[0],res),oof_df.oof[0::res])\nfor i in range(num_batches+1): plt.plot([i*batch_size,i*batch_size],[-5,12.5],'r')\nfor j in range(num_batches): plt.text(j*batch_size+200000,num_batches,str(j+1),size=20)\nplt.xlabel('Row',size=16); plt.ylabel('Signal',size=16); \nplt.title('Training Data Open Channels - 10 batches',size=20)\nplt.show()","94b21d5b":"plot_cm(oof_df['open_channels'],oof_df['oof'],'Confusion Matrix')","06d4dbea":"sample_submission  = pd.read_csv('\/kaggle\/input\/liverpool-ion-switching\/sample_submission.csv', dtype={'time': np.float32})\nsample_submission['open_channels'] = np.argmax(preds, axis=1).astype(int)\nsample_submission.to_csv(f'submission.csv', index=False, float_format='%.4f')\nprint(sample_submission.open_channels.mean())\ndisplay(sample_submission.head())","bb8e4fdf":"let = ['A','B','C','D','E','F','G','H','I','J']\nres = 1000\n\nplt.figure(figsize=(20,5))\nplt.plot(range(0,sample_submission.shape[0],res),sample_submission.open_channels[0::res])\nfor i in range(5): plt.plot([i*500000,i*500000],[-5,12.5],'r')\nfor i in range(21): plt.plot([i*100000,i*100000],[-5,12.5],'r:')\nfor k in range(4): plt.text(k*500000+250000,10,str(k+1),size=20)\nfor k in range(10): plt.text(k*100000+40000,7.5,let[k],size=16)\nplt.title('Test Data Predictions',size=16)\nplt.show()","cbd3e2b4":"## Submission","2f9cb39f":"### Model Training","df318ba7":"## Step by Step"}}