{"cell_type":{"7db57987":"code","5ad737c2":"code","cccbf99d":"code","7ecf2061":"code","473e2cfa":"code","56d698cf":"code","c3e60b59":"code","c35b6061":"code","e7b4e9aa":"code","a6f211fd":"code","5eb97c20":"code","740a91fc":"code","f80c1c06":"code","2fad5473":"code","4c2dcfa8":"code","26471515":"code","acc289f3":"code","414e6516":"code","f507fb0c":"code","038d42cb":"code","bb15f94b":"code","d31a9f7a":"code","03afa058":"code","0578b864":"code","53851aff":"code","86d51954":"code","bfe62d10":"code","1ccc71ce":"code","020a04a1":"code","1b552122":"code","ca805a56":"code","80ac8b42":"code","cceb0f42":"code","da5aecc5":"code","36f0e073":"code","e9241409":"code","bd59fa18":"code","a60e77c5":"code","eebaea60":"code","e493a071":"code","fb7fdc55":"code","0d2ed16e":"code","6dfc917e":"code","e603e444":"code","39576a8b":"code","6f8f6ce6":"code","76079497":"code","a3b3a984":"code","1fcb5baf":"code","df2e0155":"code","7534a72e":"code","f19b0190":"code","af6ea764":"code","67b1d29c":"code","6e3bb5db":"code","26a06b72":"code","0ea082f4":"code","9a5c1bda":"code","82ce2057":"code","2971b476":"code","586de421":"code","dd10c1d2":"code","98a1e860":"markdown","656964f6":"markdown","9677ea2d":"markdown","1cb36479":"markdown","6a1adbd3":"markdown","4b52ed06":"markdown","63f05c6d":"markdown","af0688eb":"markdown","d470dc0e":"markdown","3bbe40a3":"markdown","c1c6d133":"markdown","bfebde1c":"markdown","566bc3a2":"markdown","e2a695d2":"markdown"},"source":{"7db57987":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","5ad737c2":"heart_data=pd.read_csv(\"\/kaggle\/input\/heart-attack-analysis-prediction-dataset\/heart.csv\")","cccbf99d":"heart_data.info()\n","7ecf2061":"heart_data.head()","473e2cfa":"heart_data.describe","56d698cf":"#Size of dataset\nheart_data.shape","c3e60b59":"#Check for null values\nheart_data.isna().sum()","c35b6061":"heart_data.head(10)","e7b4e9aa":"heart_data[\"caa\"].value_counts()","a6f211fd":"sns.pairplot(heart_data)","5eb97c20":"sns.scatterplot(x=\"chol\",y=\"age\",data=heart_data,hue=\"output\")","740a91fc":"data_column=heart_data.drop([\"age\",\"fbs\",\"sex\",\"cp\",\"restecg\",\"exng\",\"slp\",\"caa\",\"thall\",\"output\"],axis=1)\nlist=data_column.columns\nfor i in  list:\n    sns.scatterplot(x=\"age\",y=i,data=heart_data,hue=\"output\")\n    plt.show()\n    \n","f80c1c06":"heart_data.head()","2fad5473":"#en \u00e7ok kad\u0131n m\u0131 erkek i\u00e7in mi tehlikeli\nsns.displot(heart_data,x=\"sex\",hue=\"output\",multiple=\"stack\")","4c2dcfa8":"heart_data[\"sex\"].value_counts()","26471515":"#fbs 120 den fazla olanlar\u0131n(1) ve chol kalp krizine etkisi\n#Use catplot() to combine a countplot() and a FacetGrid. This allows grouping within additional categorical variables. \n#Using catplot() is safer than using FacetGrid directly, as it ensures synchronization of variable order across facets:\nax=sns.catplot(x=\"thall\",hue=\"fbs\",col=\"output\",kind=\"count\",data=heart_data)\n","acc289f3":"hd_columns=heart_data.drop([\"output\",\"age\",\"trtbps\",\"chol\",\"thalachh\",\"oldpeak\"],axis=1)\ncolumns=hd_columns.columns\nfor i in columns:\n    ax=sns.countplot(x=i,hue=\"output\",data=heart_data)\n    plt.show()","414e6516":"max_threshold=heart_data[\"chol\"].quantile(0.99)\nmax_threshold","f507fb0c":"heart_data[heart_data[\"chol\"]>max_threshold]","038d42cb":"min_threshold=heart_data[\"chol\"].quantile(0.01)\nmin_threshold","bb15f94b":"df=heart_data[(heart_data[\"chol\"]<max_threshold) & (heart_data[\"chol\"]>min_threshold)]\ndf","d31a9f7a":"df.sample(10)","03afa058":"df.head()","0578b864":"from sklearn.preprocessing import StandardScaler\ndf_s=df.copy()","53851aff":"X=df_s.drop([\"output\"],axis=1)\nY=df_s[\"output\"]","86d51954":"#split\nfrom sklearn.model_selection import train_test_split\nX_train,X_test,y_train,y_test=train_test_split(X,Y,test_size=0.2,random_state=42)","bfe62d10":"from sklearn.ensemble import RandomForestClassifier\nmodel=RandomForestClassifier(n_estimators=700).fit(X_train,y_train)\nprdes=model.predict(X_test)\n","1ccc71ce":"import matplotlib.pyplot as plt\nfeature_imp=pd.Series(model.feature_importances_,index=X.columns)\nfeature_imp.plot(kind='barh')\n#feature_imp.nlargest(10), if you have lots of features.","020a04a1":"#list=df[[\"age\",\"trtbps\",\"chol\",\"thalachh\"]]#get multiple columns\n#for i in list:\n   # df_s[i]=StandardScaler().fit_transform(df_s[[i]])\n#scaler=StandardScaler()\n#X_s=pd.DataFrame(scaler.fit_transform(X))#dataframesiz array \u015feklinde oluyor.\n    \n    \n\n","1b552122":"X_train","ca805a56":"from sklearn.metrics import classification_report, confusion_matrix\nfrom sklearn.ensemble import RandomForestClassifier\n","80ac8b42":"best_features=feature_imp.nlargest(8).index\nbest_features","cceb0f42":"X_reduced=X[best_features]\nX_reduced\n","da5aecc5":"Xrd_scale=StandardScaler().fit_transform(X_reduced)\nXrd_train,Xrd_test,yrd_train,yrd_test=train_test_split(Xrd_scale,Y,test_size=0.2,random_state=42)","36f0e073":"rd_model=RandomForestClassifier(n_estimators=700).fit(Xrd_train,yrd_train)\nrpreds=rd_model.predict(Xrd_test)","e9241409":"plt.figure(figsize=(10, 10), dpi=400)\nsns.heatmap(X_reduced.corr().abs(), annot=True)\n#dpi \u00e7\u00f6z\u00fcn\u00fcrl\u00fck\n","bd59fa18":"#KNN\nfrom sklearn.neighbors import KNeighborsClassifier\nknn=KNeighborsClassifier(n_neighbors=7)\nknn.fit(Xrd_train,yrd_train)\npreds=knn.predict(Xrd_test)","a60e77c5":"#accuracy\nfrom sklearn.metrics import accuracy_score, f1_score, confusion_matrix\nprint(\"Accuracy for knn : {}\".format(accuracy_score(yrd_test,preds)))\nprint(\"F1 score for knn : {}\".format(f1_score(yrd_test,preds)))\nprint(\"confusion matrix for knn : {}\".format(confusion_matrix(yrd_test,preds)))","eebaea60":"#best k for Knn\n\ntrain_score=[]\ntest_score=[]\n\nfor i in range(1,15):\n    knn=KNeighborsClassifier(n_neighbors=i)\n    knn.fit(Xrd_train,yrd_train)\n    \n    train_score.append(knn.score(Xrd_train,yrd_train))\n    test_score.append(knn.score(Xrd_test,yrd_test))\n\n","e493a071":"plt.figure(figsize=(12,5))\np = sns.lineplot(range(1,15),train_score,marker='*',label='Train Score')\np = sns.lineplot(range(1,15),test_score,marker='o',label='Test Score')\n","fb7fdc55":"from sklearn.neighbors import KNeighborsClassifier\nknn_2=KNeighborsClassifier(n_neighbors=11)\nknn_2.fit(Xrd_train,yrd_train)\npreds=knn_2.predict(Xrd_test)","0d2ed16e":"#try with cross vali\nfrom sklearn.model_selection import cross_val_score\nscores=cross_val_score(knn_2,Xrd_train,yrd_train,cv=10,scoring=\"accuracy\")\nscores\n","6dfc917e":"scores.mean()","e603e444":"from sklearn.model_selection import GridSearchCV\n#In case of classifier like knn the parameter to be tuned is n_neighbors\nparam_grid = {'n_neighbors':np.arange(1,50),'metric':['euclidean','manhattan'],'weights':['uniform','distance']}\nknn_3 = KNeighborsClassifier()\nknn_cv= GridSearchCV(knn_3,param_grid,cv=5,scoring='accuracy')\nknn_cv.fit(Xrd_train,yrd_train)\n\nprint(\"Best Score:\" + str(knn_cv.best_score_))\nprint(\"Best Parameters: \" + str(knn_cv.best_params_))","39576a8b":"cvres = knn_cv.cv_results_\nfor mean_score, params in zip(cvres[\"mean_test_score\"], cvres[\"params\"]):\n    print(np.sqrt(-mean_score), params)","6f8f6ce6":"from sklearn.tree import DecisionTreeClassifier\ndt=DecisionTreeClassifier(random_state=42)","76079497":"parameter={'max_leaf_nodes': range(2, 10), 'max_depth': range(1,7), 'min_samples_split' : [2,3,4]}\ngrid_searchDT = GridSearchCV(dt,parameter,cv=5)\n                             \ngrid_searchDT.fit(Xrd_train,yrd_train)                                  \n#preds3=grid_searchDT.predict(Xrd_test)","a3b3a984":"grid_searchDT.best_params_","1fcb5baf":"grid_searchDT.best_estimator_","df2e0155":"grid_searchDT.best_score_","7534a72e":"#try with cross vali\nfrom sklearn.model_selection import cross_val_score\nscores=cross_val_score(grid_searchDT,Xrd_train,yrd_train,cv=10,scoring=\"accuracy\")\n","f19b0190":"scores.mean()","af6ea764":"tree = DecisionTreeClassifier(max_depth=4, random_state=42,max_leaf_nodes=8,min_samples_split= 2)\ntree.fit(Xrd_train, yrd_train)","67b1d29c":"from sklearn.tree import export_graphviz\nexport_graphviz(tree, out_file=\"tree.dot\", class_names=[\"0\", \"1\"],\n                feature_names=None, impurity=False, filled=True)","6e3bb5db":"import graphviz\n\nwith open(\"tree.dot\") as f:\n    dot_graph = f.read()\ndisplay(graphviz.Source(dot_graph))","26a06b72":"import sklearn\nsklearn.metrics.SCORERS.keys()","0ea082f4":"from sklearn.ensemble import RandomForestClassifier\nrf=RandomForestClassifier(random_state=42)\nparams=[\n       {'n_estimators':[3,10,30],'max_features':[2,4,6,8]},\n       {'bootstrap':[False],'n_estimators':[3,10],'max_features':[2,3,4]}]\n    \n\nparams=GridSearchCV(rf,params,cv=5,scoring='accuracy')\nparams.fit(Xrd_train,yrd_train)\n","9a5c1bda":"params.best_params_","82ce2057":"params.best_estimator_","2971b476":"params.best_score_","586de421":"from sklearn.model_selection import cross_val_score\nscoresRF=cross_val_score(params,Xrd_train,yrd_train,cv=10,scoring=\"accuracy\")","dd10c1d2":"scoresRF.mean()","98a1e860":"* By looking at the scatter plot, we can understand that it will not be efficient to use logistic regression on this dataset.","656964f6":"# Introduction","9677ea2d":"* It is important to understand the data","1cb36479":"* The best k is 11","6a1adbd3":"## feature selection","4b52ed06":"* It is hard to write scatter plot each time for all columns. For loop will be more useful","63f05c6d":"* We do not have any null values.","af0688eb":"# Import Data","d470dc0e":"# Modelling and Fine Tune Model","3bbe40a3":"* we have outliers","c1c6d133":"# Preprocessing","bfebde1c":"# Outlier Detection","566bc3a2":"# Visualization\n* To see distrubution of data better\n* To see outliers, if any\n* What wonder i about data?","e2a695d2":"* Age : Age of the patient\n\n* Sex : Sex of the patient\n\n* exang: exercise induced angina (1 = yes; 0 = no)\n\n* ca: number of major vessels (0-3)\n\n* cp : Chest Pain type chest pain type\n\n   - Value 1: typical angina\n   - Value 2: atypical angina\n   - Value 3: non-anginal pain\n   - Value 4: asymptomatic\n* trtbps : resting blood pressure (in mm Hg)\n\n* chol : cholestoral in mg\/dl fetched via BMI sensor\n\n* fbs : (fasting blood sugar > 120 mg\/dl) (1 = true; 0 = false)\n\n* rest_ecg : resting electrocardiographic results\n\n   - Value 0: normal\n   - Value 1: having ST-T wave abnormality (T wave inversions and\/or ST elevation or depression of > 0.05 mV)\n   - Value 2: showing probable or definite left ventricular hypertrophy by Estes' criteria\n* thalach : maximum heart rate achieved\n\n* target : 0= less chance of heart attack 1= more chance of heart attack"}}