{"cell_type":{"84a9d949":"code","9d45a781":"code","fe79ec89":"code","7f4f8d7f":"code","20be06a8":"code","6c14c572":"markdown","2fccc21e":"markdown","1aa52269":"markdown","626d4136":"markdown","bc5b4246":"markdown","b0165a19":"markdown","505b6808":"markdown","32a378a8":"markdown","49ced678":"markdown","77b2d021":"markdown","3ae8b56e":"markdown"},"source":{"84a9d949":"import numpy as np\nimport torch","9d45a781":"def dist_between(start_logits, end_logits, device='cpu', max_seq_len=128):\n    \"\"\"get dist btw. pred & ground_truth\"\"\"\n\n    linear_func = torch.tensor(np.linspace(0, 1, max_seq_len, endpoint=False), requires_grad=False)\n    linear_func = linear_func.to(device)\n\n    start_pos = (start_logits*linear_func).sum(axis=1)\n    end_pos = (end_logits*linear_func).sum(axis=1)\n\n    diff = end_pos-start_pos\n\n    return diff.sum(axis=0)\/diff.size(0)\n\n\ndef dist_loss(start_logits, end_logits, start_positions, end_positions, device='cpu', max_seq_len=128, scale=1):\n    \"\"\"calculate distance loss between prediction's length & GT's length\n    \n    Input\n    - start_logits ; shape (batch, max_seq_len{128})\n        - logits for start index\n    - end_logits\n        - logits for end index\n    - start_positions ; shape (batch, 1)\n        - start index for GT\n    - end_positions\n        - end index for GT\n    \"\"\"\n    start_logits = torch.nn.Softmax(1)(start_logits) # shape ; (batch, max_seq_len)\n    end_logits = torch.nn.Softmax(1)(end_logits)\n    \n    start_one_hot = torch.nn.functional.one_hot(start_positions, num_classes=max_seq_len).to(device)\n    end_one_hot = torch.nn.functional.one_hot(end_positions, num_classes=max_seq_len).to(device)\n    \n    pred_dist = dist_between(start_logits, end_logits, device, max_seq_len)\n    gt_dist = dist_between(start_one_hot, end_one_hot, device, max_seq_len) # always positive\n    diff = (gt_dist-pred_dist)\n\n    rev_diff_squared = 1-torch.sqrt(diff*diff) # as diff is smaller, make it get closer to the one\n    loss = -torch.log(rev_diff_squared) # by using negative log function, if argument is near zero -> inifinite, near one -> zero\n\n    return loss*scale\n","fe79ec89":"start_logits = torch.zeros(10)\nstart_logits[0] = 1\nstart_logits[1] = 8\nstart_logits[2] = 1\n\nend_logits = torch.zeros(10)\nend_logits[2] = 2\nend_logits[3] = 6\nend_logits[4] = 2\n\nstart_pos = torch.tensor(1)\nend_pos = torch.tensor(8)\n\ndist_loss(\n    torch.unsqueeze(start_logits, 0),\n    torch.unsqueeze(end_logits, 0),\n    torch.unsqueeze(start_pos, 0),\n    torch.unsqueeze(end_pos, 0),\n    max_seq_len=10,\n)","7f4f8d7f":"start_logits = torch.zeros(10)\nstart_logits[0] = 1\nstart_logits[1] = 8\nstart_logits[2] = 1\n\nend_logits = torch.zeros(10)\nend_logits[7] = 1\nend_logits[8] = 8\nend_logits[9] = 1\n\nstart_pos = torch.tensor(1)\nend_pos = torch.tensor(1)\n\ndist_loss(\n    torch.unsqueeze(start_logits, 0),\n    torch.unsqueeze(end_logits, 0),\n    torch.unsqueeze(start_pos, 0),\n    torch.unsqueeze(end_pos, 0),\n    max_seq_len=10,\n)","20be06a8":"start_logits = torch.zeros(10)\nstart_logits[0] = 1\nstart_logits[1] = 8\nstart_logits[2] = 1\n\nend_logits = torch.zeros(10)\nend_logits[7] = 1\nend_logits[8] = 8\nend_logits[9] = 1\n\nstart_pos = torch.tensor(1)\nend_pos = torch.tensor(8)\n\ndist_loss(\n    torch.unsqueeze(start_logits, 0),\n    torch.unsqueeze(end_logits, 0),\n    torch.unsqueeze(start_pos, 0),\n    torch.unsqueeze(end_pos, 0),\n    max_seq_len=10,\n)","6c14c572":"## Example","2fccc21e":"Like [@laevatein](https:\/\/www.kaggle.com\/laevatein\/tweat-the-loss-function-a-bit), I also wanted to give some loss to my model, to make it understand the gap between start\/end index.  \n\nSo I was surprised when I found the similar thought in [@laevatein](https:\/\/www.kaggle.com\/laevatein\/tweat-the-loss-function-a-bit) his cool work.\nhttps:\/\/www.kaggle.com\/laevatein\/tweat-the-loss-function-a-bit\n\nI designed a loss function for give 'distance loss' to my model  \n\n### **Distance** btw. prediction's start~end and ground truth's start~end   \n- if prediction's length is too different with ground truth's, then give it panelty(~infinite \u221e)  \n- if both are similar, then near zero panelty\n\n\nIt might be not perfect of course but, I just share my one and looking forward to improving it by someone's feedback.\n\n### Updated\n- updated the one_hot creation codes in 'dist_loss()' by using torch.nn.functional.one_hot ([@Yu Kang](http:\/\/https:\/\/www.kaggle.com\/karlyukang)'s feedback)","1aa52269":"## Distance Loss","626d4136":"### len(prediction) ~= len(GT)\n- Prediction ; approximately 1~8\n- GT ; 1~8","bc5b4246":"### len(prediction) < len(GT)\n- Prediction ; approximately 1~3\n- GT ; 1~8","b0165a19":"```\nstart_logits, end_logits = model(token_ids,\n                                 token_type_ids=token_type_ids,\n                                 attention_mask=attention_mask)\n\nstart_loss = torch.nn.CrossEntropyLoss()(start_logits, start_positions)\nend_loss = torch.nn.CrossEntropyLoss()(end_logits, end_positions)\n\nidx_loss = (start_loss+end_loss)\n\ndist_loss = utils.dist_loss(\n    start_logits, end_logits,\n    start_positions, end_positions,\n    device, cfg.MAX_SEQ_LEN) \n\ntotal_loss = idx_loss + dist_loss\n```","505b6808":"lineary increased values is 'linear_func' in the code","32a378a8":"## Test with...\n- model ; RoBERTa\n- loss ; crossEntropyLoss(with start_logits, end_logits) + distanceLoss\n- max_seq_len ; 128\n- batch size ; 128\n- learning rate ; 9e-5\n- epoch ; 3\n- scheduler ; cosine warmup scheduler\n- early stopping with validation jaccard score & patience=3\n     - checked 4 times in each epochs\n- 5-fold using Stratified K-fold (sklearn) with random seed (293984)\n\n\n## Result\n\n### Not Using Distance Loss ;\n    - 1 fold ; 0.7007\n    - 2 fold ; 0.7088\n    - 3 fold ; 0.7113\n    - 4 fold ; 0.7070\n    - 5 fold ; 0.7041\n    - avg ; 0.7065\n### Using Distance Loss ; \n    - 1 fold ; 0.7061\n    - 2 fold ; 0.7128\n    - 3 fold ; 0.7139\n    - 4 fold ; 0.7043\n    - 5 fold ; 0.7086\n    - avg ; 0.7091","49ced678":"![](https:\/\/user-images.githubusercontent.com\/8045508\/80869539-22a2a680-8cdc-11ea-85be-d2af6cb7babb.jpeg)","77b2d021":"## Notice\n- distance for the same length decreases, as **max_seq_len** is increasing","3ae8b56e":"### len(prediction) > len(GT)\n- Prediction ; approximately 1~8\n- GT ; 1~1"}}