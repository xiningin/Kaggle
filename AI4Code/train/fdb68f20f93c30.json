{"cell_type":{"14cdb3cb":"code","3555243b":"code","1368c8c8":"code","89e96a1d":"code","2bff6e71":"code","1a350fbc":"code","8fad6db9":"code","c88beef1":"code","844d138e":"code","3f8e2391":"code","8fb509f5":"code","f4f774bb":"code","a9c84f4b":"code","026f1d20":"code","3364e7fa":"code","074d66ea":"code","683430a8":"code","f141a990":"code","82b35dfb":"code","5d00c514":"code","aeecf4cc":"code","55dd4f1e":"code","75c1f535":"code","f63cba09":"code","601037bb":"code","18a6871b":"code","c1e6d237":"code","ecfff6cd":"code","b5322e39":"code","f3565b24":"code","dc95c6b3":"code","add02bde":"code","10bc2af2":"code","075dac84":"code","c7a42513":"code","4e63b740":"code","d8367cb0":"code","064ab47d":"code","b7fb9fb5":"code","eba1558e":"code","414ee561":"code","96a4bbb3":"code","a1de1f5c":"code","1b503846":"code","03364c2d":"markdown","4cdeba00":"markdown","7ede2efc":"markdown","74d348ac":"markdown","69052f80":"markdown","91808d4f":"markdown","863c0cfc":"markdown","0edc84bf":"markdown","a031d90b":"markdown","12e684c7":"markdown","c1883964":"markdown","881e50e9":"markdown","0c47c1e6":"markdown","4198a5f6":"markdown","35d2cf07":"markdown","c2ee2d06":"markdown","b56953dc":"markdown"},"source":{"14cdb3cb":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","3555243b":"import re\nimport string\n\nfrom sklearn.base import TransformerMixin, BaseEstimator\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.calibration import CalibratedClassifierCV\nfrom imblearn.under_sampling import InstanceHardnessThreshold\nfrom sklearn.svm import LinearSVC\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.feature_selection import SelectFromModel\nfrom sklearn.linear_model import LogisticRegression, SGDClassifier\nfrom sklearn.ensemble import VotingClassifier\nfrom sklearn.naive_bayes import MultinomialNB, ComplementNB\nfrom sklearn.svm import LinearSVC\n\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import classification_report\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.utils.multiclass import unique_labels\nfrom sklearn.feature_selection import SelectFromModel\n\nfrom imblearn.pipeline import Pipeline\nimport pickle\n\n\nimport spacy\nnlp = spacy.load(\"en_core_web_lg\")\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns","1368c8c8":"data = pd.read_csv(\"..\/input\/suicide-watch\/Suicide_Detection.csv\")\ndata.head()","89e96a1d":"data.shape","2bff6e71":"#split data for easy training\n\ndata_split = np.array_split(data, 20)\n\ndf = data_split[0].copy()\ndf2 = data_split[1].copy()\ndf3 = data_split[2]\ndf4 = data_split[3]\ndf5 = data_split[4]\ndf6 = data_split[5]","1a350fbc":"df = df.rename(columns = {\"class\": \"target\"})","8fad6db9":"df.head()","c88beef1":"df.shape","844d138e":"df.target.value_counts()","3f8e2391":"df.target.value_counts().plot.barh()","8fb509f5":"class TextPreprocessor(TransformerMixin):\n    def __init__(self, text_attribute):\n        self.text_attribute = text_attribute\n        \n    def transform(self, X, *_):\n        X_copy = X.copy()\n        X_copy[self.text_attribute] = X_copy[self.text_attribute].apply(self._preprocess_text)\n        return X_copy\n    \n    def _preprocess_text(self, text):\n        return self._lemmatize(self._leave_letters_only(self._clean(text)))\n    \n    def _clean(self, text):\n        bad_symbols = '!\"#%&\\'*+,-<=>?[\\\\]^_`{|}~'\n        text_without_symbols = text.translate(str.maketrans('', '', bad_symbols))\n\n        text_without_bad_words = ''\n        for line in text_without_symbols.split('\\n'):\n            if not line.lower().startswith('from:') and not line.lower().endswith('writes:'):\n                text_without_bad_words += line + '\\n'\n\n        clean_text = text_without_bad_words\n        email_regex = r'([a-zA-Z0-9_.+-]+@[a-zA-Z0-9-]+\\.[a-zA-Z0-9-.]+)'\n        regexes_to_remove = [email_regex, r'Subject:', r'Re:']\n        for r in regexes_to_remove:\n            clean_text = re.sub(r, '', clean_text)\n\n        return clean_text\n    \n    def _leave_letters_only(self, text):\n        text_without_punctuation = text.translate(str.maketrans('', '', string.punctuation))\n        return ' '.join(re.findall(\"[a-zA-Z]+\", text_without_punctuation))\n    \n    def _lemmatize(self, text):\n        doc = nlp(text)\n        words = [x.lemma_ for x in [y for y in doc if not y.is_stop and y.pos_ != 'PUNCT' \n                                    and y.pos_ != 'PART' and y.pos_ != 'X']]\n        return ' '.join(words)\n    \n    def fit(self, *_):\n        return self","f4f774bb":"text_preprocessor = TextPreprocessor(text_attribute='text')\ndf_preprocessed = text_preprocessor.transform(df)","a9c84f4b":"train, test = train_test_split(df_preprocessed, test_size=0.3)","026f1d20":"#Vectorize data\n\ntfidf_vectorizer = TfidfVectorizer(analyzer = \"word\", max_features=10000)\n\nX_tfidf_train = tfidf_vectorizer.fit_transform(train['text'])\nX_tfidf_test = tfidf_vectorizer.transform(test['text'])","3364e7fa":"y = train['target']\ny_test = test['target']","074d66ea":"X, y = X_tfidf_train, y\nX_test, y_test = X_tfidf_test, y_test","683430a8":"scaler = MinMaxScaler()\nX_norm = scaler.fit_transform(X.toarray())\nX_test_norm = scaler.transform(X_test.toarray())","f141a990":"lsvc = LinearSVC(C=100, penalty='l1', max_iter=500, dual=False)\nlsvc.fit(X_norm, y)\nfs = SelectFromModel(lsvc, prefit=True)\nX_sel = fs.transform(X_norm)\nX_test_sel = fs.transform(X_test_norm)\n","82b35dfb":"from IPython.display import Markdown, display\n\ndef show_top10_features(classifier, feature_names, categories):\n    for i, category in enumerate(categories):\n        top10 = np.argsort(classifier.coef_[0, i])[-100:]\n        display(Markdown(\"**%s**: %s\" % (category, \", \".join(feature_names[top10]))))","5d00c514":"feature_names = np.array(tfidf_vectorizer.get_feature_names())\nshow_top10_features(lsvc, feature_names, lsvc.classes_)","aeecf4cc":"print(\"New dataset shape: \", X_sel.shape)\nprint(\"Features reducted: \", X_norm.shape[1] - X_sel.shape[1])","55dd4f1e":"# Evaluation matrix \n\n# this snippet was taken from https:\/\/gist.github.com\/shaypal5\/94c53d765083101efc0240d776a23823\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.utils.multiclass import unique_labels\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\ndef print_confusion_matrix(confusion_matrix, \n                           class_names, \n                           figsize = (15,15), \n                           fontsize=12,\n                           ylabel='True label',\n                           xlabel='Predicted label'):\n    \"\"\"Prints a confusion matrix, as returned by sklearn.metrics.confusion_matrix, as a heatmap.\n    \n    Arguments\n    ---------\n    confusion_matrix: numpy.ndarray\n        The numpy.ndarray object returned from a call to sklearn.metrics.confusion_matrix. \n        Similarly constructed ndarrays can also be used.\n    class_names: list\n        An ordered list of class names, in the order they index the given confusion matrix.\n    figsize: tuple\n        A 2-long tuple, the first value determining the horizontal size of the ouputted figure,\n        the second determining the vertical size. Defaults to (10,7).\n    fontsize: int\n        Font size for axes labels. Defaults to 14.\n        \n    Returns\n    -------\n    matplotlib.figure.Figure\n        The resulting confusion matrix figure\n    \"\"\"\n    df_cm = pd.DataFrame(\n        confusion_matrix, index=class_names, columns=class_names, \n    )\n    fig = plt.figure(figsize=figsize)\n    try:\n        heatmap = sns.heatmap(df_cm, annot=True, fmt=\"d\")\n    except ValueError:\n        raise ValueError(\"Confusion matrix values must be integers.\")\n    heatmap.yaxis.set_ticklabels(heatmap.yaxis.get_ticklabels(), rotation=0, ha='right', fontsize=fontsize)\n    heatmap.xaxis.set_ticklabels(heatmap.xaxis.get_ticklabels(), rotation=45, ha='right', fontsize=fontsize)\n    plt.ylabel(ylabel)\n    plt.xlabel(xlabel)","75c1f535":"def evaluate_model(model, X, y, X_test, y_test, target_names=None):\n    scores = cross_val_score(model, X, y, cv=5, scoring='accuracy')\n    scores_test = cross_val_score(model, X_test, y_test, cv=5, scoring='accuracy')\n    \n    print(\"Accuracy: %0.2f (+\/- %0.2f)\" % (scores.mean(), scores.std()))\n    print(\"Accuracy test: %0.2f (+\/- %0.2f)\" % (scores_test.mean(), scores_test.std()))\n    \n    print(\"Test classification report: \")\n    if target_names is None:\n        target_names = model.classes_\n    print(classification_report(y_test, model.predict(X_test), target_names=target_names))\n    print(\"Test confusion matrix: \")\n    print_confusion_matrix(confusion_matrix(y_test, model.predict(X_test)), class_names=target_names)","f63cba09":"mb = MultinomialNB()\nmb.fit(X_sel, y)\nevaluate_model(mb, X_sel, y, X_test_sel, y_test)\n\n\n#save model\npickle.dump(mb, open(\"MultinomialNB_Text_classification\", 'wb')) #90%\n","601037bb":"cb = ComplementNB()\ncb.fit(X_sel, y)\nevaluate_model(cb, X_sel, y, X_test_sel, y_test)\n\n#save model\npickle.dump(cb, open(\"ComplementNB_Text_classification\", 'wb')) #90%\n","18a6871b":"lr = LogisticRegression(multi_class='ovr', solver = 'liblinear', C=10, penalty = 'l2')\nlr.fit(X_sel, y)\nevaluate_model(lr, X_sel, y, X_test_sel, y_test)\n\n#save model\npickle.dump(lr, open(\"LogisticRegression_Text_classification\", 'wb')) #92%\n","c1e6d237":"lsvc = LinearSVC(C=1000, penalty='l1', max_iter=500, dual=False)\nlsvc.fit(X_sel, y)\nevaluate_model(lsvc, X_sel, y, X_test_sel, y_test)\n\n#save model\npickle.dump(lsvc, open(\"LinearSVC_Text_classification\", 'wb')) #90%\n","ecfff6cd":"sgd = SGDClassifier(alpha=.0001, max_iter=50, loss='log',\n                                       penalty=\"elasticnet\", n_jobs=-1)\nsgd.fit(X_sel, y)\nevaluate_model(sgd, X_sel, y, X_test_sel, y_test)\n\n#save model\npickle.dump(sgd, open(\"SGDClassifier_Text_classification\", 'wb')) #91%","b5322e39":"vclf_sgd = VotingClassifier(estimators=[\n         ('lr', LogisticRegression(multi_class='ovr', solver = 'liblinear', C=10, penalty = 'l2')),\n        ('mb', MultinomialNB()),\n        ('sgd', SGDClassifier(alpha=.0001, max_iter=50, loss='log', penalty=\"elasticnet\"))\n], voting='soft', n_jobs=-1)\nvclf_sgd.fit(X_sel, y)\nevaluate_model(vclf_sgd, X_sel, y, X_test_sel, y_test)\n\n#save model\npickle.dump(vclf_sgd, open(\"VotingClassifier_Text_classification\", 'wb')) #93%","f3565b24":"# Text Proccessing\n\nclass TextPreprocessor(BaseEstimator, TransformerMixin):\n    def __init__(self, text_attribute):\n        self.text_attribute = text_attribute\n    \n    def fit(self, X, y=None):\n        return self\n        \n    def transform(self, X, *_):\n        X_copy = X.copy()\n        return X_copy[self.text_attribute].apply(self._preprocess_text)\n    \n    def _preprocess_text(self, text):\n        return self._lemmatize(self._leave_letters_only(self._clean(text)))\n    \n    def _clean(self, text):\n        bad_symbols = '!\"#%&\\'*+,-<=>?[\\\\]^_`{|}~'\n        text_without_symbols = text.translate(str.maketrans('', '', bad_symbols))\n\n        text_without_bad_words = ''\n        for line in text_without_symbols.split('\\n'):\n            if not line.lower().startswith('from:') and not line.lower().endswith('writes:'):\n                text_without_bad_words += line + '\\n'\n\n        clean_text = text_without_bad_words\n        email_regex = r'([a-zA-Z0-9_.+-]+@[a-zA-Z0-9-]+\\.[a-zA-Z0-9-.]+)'\n        regexes_to_remove = [email_regex, r'Subject:', r'Re:']\n        for r in regexes_to_remove:\n            clean_text = re.sub(r, '', clean_text)\n\n        return clean_text\n    \n    def _leave_letters_only(self, text):\n        text_without_punctuation = text.translate(str.maketrans('', '', string.punctuation))\n        return ' '.join(re.findall(\"[a-zA-Z]+\", text_without_punctuation))\n    \n    def _lemmatize(self, text):\n        doc = nlp(text)\n        words = [x.lemma_ for x in [y for y in doc if not y.is_stop and y.pos_ != 'PUNCT' \n                                    and y.pos_ != 'PART' and y.pos_ != 'X']]\n        return ' '.join(words)","dc95c6b3":"class DenseTransformer(TransformerMixin):\n\n    def fit(self, X, y=None, **fit_params):\n        return self\n\n    def transform(self, X, y=None, **fit_params):\n        return X.todense()","add02bde":"#Pipeline\n\ntext_classification_pipeline = Pipeline([\n    ('text_preprocessor', TextPreprocessor(text_attribute='text')),\n    ('vectorizer', TfidfVectorizer(analyzer = \"word\", max_features=10000)),\n    ('todense_converter', DenseTransformer()),\n    ('scaler', MinMaxScaler()),\n    ('classifier', VotingClassifier(estimators=[\n                         ('lr', LogisticRegression(multi_class='ovr', solver = 'liblinear', C=10, penalty = 'l2')),\n                        ('mb', MultinomialNB()),\n                        ('sgd', SGDClassifier(alpha=.0001, max_iter=50, loss='log', penalty=\"elasticnet\"))\n                        ], \n                                    voting='soft', n_jobs=-1))\n     ])","10bc2af2":"#save Pipeline\nfrom joblib import dump\n\n# dump the pipeline model\ndump(text_classification_pipeline, filename=\"Suicide_text_classification.joblib\")","075dac84":"df2.head()","c7a42513":"train, test = train_test_split(df2, test_size=0.3)\n\nX_p = train.drop(columns=['class', \"Unnamed: 0\"])\ny_p = train['class']\n\nX_p_test = test.drop(columns=['class', \"Unnamed: 0\"])\ny_p_test = test['class']","4e63b740":"%%time\npipeline = text_classification_pipeline\npipeline.fit(X_p, y_p)","d8367cb0":"%%time\ny_pred = pipeline.predict(X_p)","064ab47d":"%%time\ny_test_pred = pipeline.predict(X_p_test)","b7fb9fb5":"y_test_pred","eba1558e":"print(classification_report(y_p, y_pred, target_names=pipeline.classes_))\nprint_confusion_matrix(confusion_matrix(y_p, y_pred), class_names=pipeline.classes_, figsize=(5,5), fontsize=12)","414ee561":"print(classification_report(y_p_test, y_test_pred, target_names=pipeline.classes_))\nprint_confusion_matrix(confusion_matrix(y_p_test, y_test_pred), class_names=pipeline.classes_, figsize=(5,5), fontsize=12)","96a4bbb3":"#df3 \n\ndf3.head()\ndf_text = df3.drop(columns=['class', \"Unnamed: 0\"])\ndf_target = df3[\"class\"]","a1de1f5c":"%%time\ny_pred3 = pipeline.predict(df_text)","1b503846":"print(classification_report(df_target, y_pred3, target_names=pipeline.classes_))\nprint_confusion_matrix(confusion_matrix(df_target, y_pred3), class_names=pipeline.classes_, figsize=(5,5), fontsize=12)","03364c2d":"## Feature Selection","4cdeba00":"## Soft Voting","7ede2efc":"## Preprocess Text","74d348ac":"## Import Libraries","69052f80":"### Test Pipeline on other dataset","91808d4f":"## Feature extraction & Split for Train & Test","863c0cfc":"## SGDClaccifier","0edc84bf":"## Predictive Models","a031d90b":"# Suicide Text Classification","12e684c7":"# Create Pipeline","c1883964":"<img src=\"https:\/\/i.ibb.co\/Prrpgxg\/nlp-preproc.png\" \/>","881e50e9":"## Complement Naive Bayes","0c47c1e6":"## Multinomial Naive Bayes","4198a5f6":"## Linear SVC","35d2cf07":"## Logistic Regression","c2ee2d06":"## Load & Preview dataset","b56953dc":"## Feature Scaling"}}