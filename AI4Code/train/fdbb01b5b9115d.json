{"cell_type":{"53cb2fc5":"code","ba99cdc2":"code","2a5934d8":"code","c580b0de":"code","068fab22":"code","0bb6ee6e":"code","c41209a7":"code","5eaaa53c":"code","bf07d7a3":"code","860afa2e":"code","992e46b8":"code","69765c4a":"code","78b4d64c":"code","8e7d7f90":"code","e01ec2fb":"code","ede544a3":"code","8d7c6f60":"code","7030aa09":"code","fb91b2e7":"code","bc00c6ae":"code","54a235c5":"code","5d184fe1":"code","d854ed76":"code","f4030305":"code","6e3d57c6":"code","5b55d932":"code","65f0c64d":"code","86d90fed":"code","57204958":"code","df9b5ff6":"code","38034ba8":"code","8f4d61cf":"code","75fbe92e":"markdown","0d03b2e4":"markdown","bc1b7ea3":"markdown","bc299f5e":"markdown","f427cf42":"markdown","162b845a":"markdown","2d97a889":"markdown","facecda0":"markdown","55698dbc":"markdown","57833301":"markdown"},"source":{"53cb2fc5":"import numpy as np\nfrom PIL import Image\nimport matplotlib.pyplot as plt\n\nimport torch\nimport torch.optim as optim\nfrom torchvision import models\nfrom torchvision import transforms as tf\nimport torch.nn.functional as F","ba99cdc2":"vgg = models.vgg19(pretrained=True).features\n\nfor param in vgg.parameters():\n    param.requires_grad_(False)","2a5934d8":"device = torch.device(\"cpu\")\n\nif torch.cuda.is_available():\n    device = torch.device(\"cuda\")\n    \nvgg.to(device)","c580b0de":"# All pretrained models in PyTorch expect that the pixel ranges \n# of the input images should be normalized in the same way\n# i.e. loaded in the [0, 1] range, and they have to be \n# normalized using this mean and standard deviation given below. \n# This information is available in the PyTorch documentation - https:\/\/pytorch.org\/vision\/stable\/models.html\n# normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n\nmean = (0.485, 0.456, 0.406)\nstd = (0.229, 0.224, 0.225)","068fab22":"def transformation(img):\n    \n    tasks = tf.Compose([tf.Resize(256), \n                        tf.ToTensor(),\n                        tf.Normalize(mean, std)])\n    \n    img = tasks(img)  \n    img = img.unsqueeze(0)    \n    \n    return img","0bb6ee6e":"#content_img = Image.open(\"cat.jpg\").convert('RGB')\ncontent_img = Image.open(\"..\/input\/cat-pic\/cat.jpg\").convert('RGB')\n#style_img   = Image.open(\"starry_night.jpg\").convert('RGB')\nstyle_img   = Image.open(\"..\/input\/starry-night\/starry_night.jpg\").convert('RGB')","c41209a7":"content_img","5eaaa53c":"content_img.size","bf07d7a3":"list(content_img.getdata())","860afa2e":"style_img","992e46b8":"style_img.size","69765c4a":"list(style_img.getdata())","78b4d64c":"content_img = transformation(content_img).to(device)\nstyle_img   = transformation(style_img).to(device)","8e7d7f90":"content_img.shape","e01ec2fb":"content_img","ede544a3":"style_img.shape","8d7c6f60":"style_img","7030aa09":"def tensor_to_image(tensor):\n    # Detach tensor from the computational graph in Pytorch\n    image = tensor.clone().detach()\n    \n    # Convert the tensor to Numpy format \n    # Squeeze function that we invoke on Numpy will get rid of the batch dimension\n    # of size 1 dimensions\n    # The squeeze function in general squeezes out all size 1 dimensions\n\n    image = image.cpu().numpy().squeeze()\n\n    # The tensor representation of our image has channels in the first dimensions\n    # then height and width of the image. We need to perform a transpose operation\n    # so that we get the image in the format where the dimensions are first height\n    # then width and then the number of channels\n    # This is what matplotlib expects\n    image = image.transpose(1, 2, 0)\n    \n    # Multiply pixel values by std deviation and add the mean \n    # so that we normalize the image.\n    image *= np.array(std) + np.array(mean)\n    \n    # Clip all pixel values in the range 0 to 1 and return this image\n    image = image.clip(0, 1)\n    \n    return image","fb91b2e7":"img = tensor_to_image(style_img)\nfig = plt.figure()\nfig.suptitle('Style Image')\nplt.imshow(img)\n\nimg = tensor_to_image(content_img)\nfig = plt.figure()\nfig.suptitle('Content Image')\nplt.imshow(img)","bc00c6ae":"LAYERS_OF_INTEREST = {'0': 'conv1_1', \n                      '5': 'conv2_1',  \n                      '10': 'conv3_1', \n                      '19': 'conv4_1', \n                      '21': 'conv4_2', \n                      '28': 'conv5_1'}","54a235c5":"def apply_model_and_extract_features(image, model):\n    x = image\n\n    features = {}\n    \n    for name, layer in model._modules.items():\n        x = layer(x)\n        \n        if name in LAYERS_OF_INTEREST:\n            features[LAYERS_OF_INTEREST[name]] = x   \n            \n    return features","5d184fe1":"content_img_features = apply_model_and_extract_features(content_img, vgg)\nstyle_img_features   = apply_model_and_extract_features(style_img, vgg)","d854ed76":"content_img_features","f4030305":"style_img_features","6e3d57c6":"def calculate_gram_matrix(tensor):\n    \n    _, channels, height, width = tensor.size()\n    \n    tensor = tensor.view(channels, height * width)    \n    \n    gram_matrix = torch.mm(tensor, tensor.t())\n    \n    gram_matrix = gram_matrix.div(channels * height * width) \n    \n    return gram_matrix","5b55d932":"style_features_gram_matrix = {layer: calculate_gram_matrix(style_img_features[layer]) for layer in \n                                                    style_img_features}\n\nstyle_features_gram_matrix","65f0c64d":"weights = {'conv1_1': 1.0, 'conv2_1': 0.75, 'conv3_1': 0.35,\n           'conv4_1': 0.25, 'conv5_1': 0.15}","86d90fed":"target = content_img.clone().requires_grad_(True).to(device)\n\noptimizer = optim.Adam([target], lr=0.003)","57204958":"plt.figure()\n\nplt.imshow(tensor_to_image(target))","df9b5ff6":"for i in range(1, 2000):\n    \n    target_features = apply_model_and_extract_features(target, vgg)\n\n    content_loss = F.mse_loss (target_features['conv4_2'], content_img_features['conv4_2'])\n    \n    style_loss = 0\n    for layer in weights:\n        \n        target_feature = target_features[layer]\n\n        target_gram_matrix = calculate_gram_matrix(target_feature)\n        style_gram_matrix = style_features_gram_matrix[layer]\n        \n        layer_loss = F.mse_loss (target_gram_matrix, style_gram_matrix)\n        layer_loss *= weights[layer]\n        \n        _, channels, height, width = target_feature.shape\n\n        style_loss += layer_loss  \n    \n    total_loss = 1000000 * style_loss + content_loss\n    \n    if i % 50 == 0:\n        print ('Epoch {}:, Style Loss : {:4f}, Content Loss : {:4f}'.format( i, style_loss, content_loss))\n    \n    optimizer.zero_grad()\n    \n    total_loss.backward()\n    \n    optimizer.step()","38034ba8":"fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 10))\nax1.imshow(tensor_to_image(content_img))\nax2.imshow(tensor_to_image(target))","8f4d61cf":"#End of Code","75fbe92e":"## Performing Style Transfer\n* To transfer the style from one image to the other, we set the weight of every layer used to obtain style features i.e the initial layer of every convolutional block.\n* Define the optimizer function and the target image which is a copy of the content image.","0d03b2e4":" # Implementing Neural Style Transfer in Pytorch\nhttps:\/\/arxiv.org\/pdf\/1508.06576.pdf\nhttps:\/\/www.analyticsvidhya.com\/blog\/2019\/01\/guide-pytorch-neural-networks-case-studies\/","bc1b7ea3":"Feature correlations are given by the Gram matrix G, where every cell (i, j) in G is the inner product between the vectorised feature maps i and j in a layer.","bc299f5e":"### Define a method to convert tensors to images\n* Clone the tensor\n* Convert the tensor to a numpy array and squeeze it to make it 3d\n* Transpose performs CHW -> HWC\n* Normalize the image and clip the floats into the valid range for plotting","f427cf42":"### Start the loss minimization process \n* Run the loop for some large number of iterations. \n* Calculate the content loss (from content image and target) using mse\n* Calculate the style loss (from style image and target) using mse\n* The combined loss is then backpropagated and minimised.\n* Using the minimized loss, the network parameters are updated which further updates the target image.","162b845a":"## Load the images\n* display the images\n* apply the transformations: resize to tensor, and normalization of values. The mean and std values are of the image dataset on which the model was trained on. VGG was trained on Imagenet.","2d97a889":"## Plot Results","facecda0":"## Import libraries","55698dbc":"## Import the pre-trained model VGG19\n\n* get the features portion from VGG19\n* freeze all VGG parameters\n* save model to device\n\n![image.png](attachment:image.png)","57833301":"## Obtain features from the images\n* We need to extract features related to the content or the objects present from the first image and features related to styles and textures from the second.\n\n* Object Related Features : Initial layers of CNNs focus on lines and edges and the important information about objects can be extracted later layers of the network. In the higher layers, the information space is more complex and detailed pixel information gets lost.\n\n* Style Related Features: The original neural style transfer paper suggests correlations between different features in different layers to obtain style information.\n\n* Conv42, present in the 4th convolutional block with a depth of 512 was used to extract content information. The first convolutional layer of every convolutional block in the network, i.e., conv11, conv21, conv31, conv41, and conv51 were used to extract the style features. \n"}}