{"cell_type":{"d3b084fb":"code","cc386a78":"code","1bd3ede8":"code","8c04c3ac":"code","d5effacb":"code","3746bfd2":"code","2a545a4b":"code","75e36579":"code","a6218c63":"code","28477429":"code","2fbe1e3c":"code","99ddcadf":"code","6410d176":"code","f6908a88":"code","4780f36d":"markdown"},"source":{"d3b084fb":"from types import SimpleNamespace\nfrom collections import Counter\nimport os\nimport re\nimport pathlib\nimport subprocess\nimport array\nimport pickle\nimport numpy as np\nimport pandas as pd","cc386a78":"DATASET_VERSION = 'ca-100'\nDATASET_ROOT = f'..\/input\/viquipdia\/{DATASET_VERSION}'\nWORKING_ROOT = f'data\/{DATASET_VERSION}'\nDATASET_PREFIX = 'ca.wiki'","1bd3ede8":"params = SimpleNamespace(\n    window_size = 5,\n    cutoff = 3,\n    maxtokens = 100000,\n    dataset = f'{DATASET_ROOT}\/{DATASET_PREFIX}',\n    working = f'{WORKING_ROOT}\/{DATASET_PREFIX}',\n)","8c04c3ac":"class Vocabulary(object):\n    def __init__(self, pad_token='<pad>', unk_token='<unk>', eos_token='<eos>'):\n        self.token2idx = {}\n        self.idx2token = []\n        self.pad_token = pad_token\n        self.unk_token = unk_token\n        self.eos_token = eos_token\n        if pad_token is not None:\n            self.pad_index = self.add_token(pad_token)\n        if unk_token is not None:\n            self.unk_index = self.add_token(unk_token)\n        if eos_token is not None:\n            self.eos_index = self.add_token(eos_token)\n\n    def add_token(self, token):\n        if token not in self.token2idx:\n            self.idx2token.append(token)\n            self.token2idx[token] = len(self.idx2token) - 1\n        return self.token2idx[token]\n\n    def get_index(self, token):\n        if isinstance(token, str):\n            return self.token2idx.get(token, self.unk_index)\n        else:\n            return [self.token2idx.get(t, self.unk_index) for t in token]\n\n    def __len__(self):\n        return len(self.idx2token)\n\n    def save(self, filename):\n        with open(filename, 'wb') as f:\n            pickle.dump(self.__dict__, f)\n\n    def load(self, filename):\n        with open(filename, 'rb') as f:\n            self.__dict__.update(pickle.load(f))","d5effacb":"class Punctuation:\n    html = re.compile(r'&apos;|&quot;')\n    punctuation = re.compile(r'[^\\w\\s\u00b7]|_')\n    spaces = re.compile(r'\\s+')\n    ela_geminada = re.compile(r'l \u00b7 l')\n\n    def strip(self, s):\n        '''\n        Remove all punctuation characters.\n        '''\n        s = self.html.sub(' ', s)\n        s = self.punctuation.sub(' ', s)\n        s = self.spaces.sub(' ', s).strip()\n        s = self.ela_geminada.sub('l\u00b7l', s)\n        return s","3746bfd2":"def remove_punctuation(input_path, output_path):\n    punc = Punctuation()\n    with open(input_path, 'r', encoding='utf-8') as inpf, open(output_path, 'w', encoding='utf-8') as outf:\n        for line in inpf:\n            line = punc.strip(line)\n            print(line, file=outf)","2a545a4b":"def get_token_counter(file_path):\n    counter = Counter()\n    with open(file_path, 'r', encoding='utf-8') as f:\n        for line in f:\n            line = line.strip()\n            if line:\n                tokens = line.split()\n                counter.update(tokens)\n    return counter","75e36579":"def get_token_vocabulary(token_counter, cutoff=3, maxtokens=None, verbose=1, eos_token=None):\n    vocab = Vocabulary(eos_token=eos_token)\n    total_count = sum(token_counter.values())\n    in_vocab_count = 0\n\n    for token, count in token_counter.most_common(maxtokens):\n        if count >= cutoff:\n            vocab.add_token(token)\n            in_vocab_count += count\n\n    if verbose:\n        OOV_count = total_count - in_vocab_count\n        print('OOV ratio: %.2f%%.' % (100*OOV_count \/ total_count))\n    return vocab","a6218c63":"def get_token_index(file_path, vocab, eos_token=None):\n    index_list = []\n    with open(file_path, 'r', encoding='utf-8') as f:\n        for line in f:\n            line = line.strip()\n            if line:\n                if eos_token is not None:\n                    line += ' ' + eos_token\n                tokens = line.strip().split()\n                index_list.append([vocab.get_index(token) for token in tokens])\n    return index_list","28477429":"def get_data(idx_list, window_size, pad_index=0):\n    input = []\n    target = array.array('I')\n    left_window = window_size \/\/ 2\n    right_window = window_size - left_window - 1\n    for line in idx_list:\n        if len(line) <= window_size \/\/ 2:\n            continue\n        ext_line = [pad_index] * left_window + line + [pad_index] * right_window\n        for i, token_id in enumerate(line):\n            context = array.array('I', ext_line[i:i + left_window] + ext_line[i + left_window + 1:i + window_size])\n            input.append(context)\n            target.append(token_id)\n    return np.array(input, dtype=np.int32), np.array(target, dtype=np.int32)","2fbe1e3c":"def prepare_dataset(params):\n    dataset_prefix = params.dataset\n    working_prefix = params.working\n    cutoff = params.cutoff\n    maxtokens = params.maxtokens\n    window_size = params.window_size\n\n    data = []\n    for part in ['train', 'valid', 'test']:\n        data_filename = f'{dataset_prefix}.{part}.tokens'\n        data_filename_nopunct = f'{working_prefix}.{part}.tokens.nopunct'\n        remove_punctuation(data_filename, data_filename_nopunct)\n\n        if part == 'train':\n            # Basic token statistics\n            token_counter = get_token_counter(data_filename_nopunct)\n            print(f'Number of Tokens: {sum(token_counter.values())}')\n            print(f'Number of different Tokens: {len(token_counter)}')\n            pickle.dump(token_counter, open(f'{data_filename_nopunct}.dic', 'wb'))\n\n            # Token vocabulary\n            token_vocab = get_token_vocabulary(token_counter, cutoff=cutoff, maxtokens=maxtokens)\n            token_vocab.save(f'{working_prefix}.vocab')\n            print(f'Vocabulary size: {len(token_vocab)}')\n\n        # Token indexes\n        train_idx = get_token_index(data_filename_nopunct, token_vocab)\n        print(f'Number of lines ({part}): {len(train_idx)}')\n\n        # Get input and target arrays\n        idata, target = get_data(train_idx, window_size)\n        data.append((idata, target))\n        print(f'Number of samples ({part}): {len(target)}')\n\n        # Save numpy arrays\n        np.savez(f'{working_prefix}.{part}.npz', idata=idata, target=target)\n    return token_vocab, data","99ddcadf":"# Create working dir\npathlib.Path(WORKING_ROOT).mkdir(parents=True, exist_ok=True)","6410d176":"vocab, data = prepare_dataset(params)","f6908a88":"for word in ['ra\u00efm', 'intel\u00b7ligent']:\n    print(f'{word} -> {vocab.get_index(word)}')","4780f36d":"Check the vocabulary with some words with specific Catalan characters as '\u00ef' and 'l\u00b7l'"}}