{"cell_type":{"8e16ae94":"code","04f13d92":"code","93f8b798":"code","1ee87ad1":"code","9d72be54":"code","ef2dc188":"code","7a5165d4":"code","84f99ac4":"code","5c49e639":"code","92743eaf":"code","65306f0d":"code","2f6c8b80":"code","e6c43c1b":"code","61ef1799":"code","28b85d4e":"code","9c19fa8c":"code","a3d35070":"code","12a23c95":"code","ff0336c0":"code","27b46a74":"code","d6ad5734":"code","6893a33f":"code","cd1caa69":"code","9f2d86a7":"code","0b9b45f9":"code","107aa153":"code","ace41ba3":"code","0b7582fc":"code","1e9d8738":"code","766b2791":"code","224272b8":"code","2b1b97ba":"code","dd816375":"code","222d723d":"code","b0a32482":"code","d1fbc0da":"code","6ec0b479":"code","0ae32c50":"code","5f1fc1e9":"code","906e4162":"code","35d0c080":"code","71cefe78":"code","21d46705":"code","9e163f70":"code","2ca54edb":"code","23fb0654":"code","7ec7467e":"markdown","79674605":"markdown","ef8b5aea":"markdown","592cca94":"markdown","c0075416":"markdown","3a81110d":"markdown","1b8ee0f1":"markdown","a5c83c0b":"markdown","e26beaec":"markdown","64a6f27c":"markdown","23ba629c":"markdown","155af510":"markdown","1e9881a3":"markdown","2e5c9792":"markdown","8988f57d":"markdown","e55b3808":"markdown","02649e00":"markdown","445570b5":"markdown","bf34c235":"markdown","94986087":"markdown","11a7d9ef":"markdown","a1bb13db":"markdown","ce7aca79":"markdown","d582025d":"markdown","daa8db79":"markdown","d2810d21":"markdown","8228ebe7":"markdown","0a36e224":"markdown","ca413f5d":"markdown","217d4701":"markdown","cb066eb2":"markdown","10233db8":"markdown","e58c46e5":"markdown","0c6e3898":"markdown","cfe9c51c":"markdown","c1ba9bf1":"markdown","223765f8":"markdown","df869a4f":"markdown","47cd02d3":"markdown","77b70aec":"markdown","018f79eb":"markdown","58217eff":"markdown","c5ddadf8":"markdown","8c6121de":"markdown","ad4547d0":"markdown","6c689487":"markdown","1be0b8cd":"markdown","42cfe1aa":"markdown","9d02ae68":"markdown","991c4437":"markdown","cb64ff64":"markdown","a80d5906":"markdown","279b85b3":"markdown","10bbc468":"markdown","b5e13ac1":"markdown","c59d092f":"markdown","bfbdb1ee":"markdown","6fa313f3":"markdown","ddbe473b":"markdown","6d13d108":"markdown","d06f0a72":"markdown","eb5c3657":"markdown","32336cb2":"markdown","f16c2b4e":"markdown","7e3bf090":"markdown","8ebafa23":"markdown","7bf9e225":"markdown","761aae43":"markdown","aeaf1861":"markdown","4f76b802":"markdown","b5cda43f":"markdown","202acddc":"markdown","bfccf279":"markdown","7192d4a2":"markdown","3ffc73e3":"markdown","d19d5a31":"markdown","397f9fbb":"markdown","2f44e1b4":"markdown","b0643b87":"markdown","662e5519":"markdown","b59c9596":"markdown","1faae693":"markdown","9d076bc0":"markdown","f6315f6c":"markdown"},"source":{"8e16ae94":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport shap # shap values help us understand the importance of features\n\n# import all essential libraries\nimport missingno as msno # missin data visualization\nimport seaborn as sns\nsns.set()\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder, OneHotEncoder\nimport re\nfrom scipy.stats import randint, norm\nfrom scipy.cluster.vq import whiten\n\n# scikit-learn modules and tools\nfrom sklearn.metrics import roc_curve, auc, roc_auc_score, accuracy_score, classification_report\nfrom sklearn.metrics import mean_squared_error as MSE, confusion_matrix, precision_recall_curve\nfrom sklearn.model_selection import train_test_split, cross_val_score, RandomizedSearchCV, GridSearchCV, learning_curve\nfrom sklearn.ensemble import RandomForestClassifier\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Important global variables\nSEED = 10 # guarantees reproducibility of results\nkFold = 5 # 5-fold cross validation for our classifier\ntune_random_forest = 'tuned_gini'  # available_options: {'grid', 'random', 'tuned_gini', 'tuned_entropy', 'untuned'}, default: 'tuned_entropy'","04f13d92":"# load data \ntrain = pd.read_csv('\/kaggle\/input\/titanic\/train.csv') \ntest = pd.read_csv('\/kaggle\/input\/titanic\/test.csv')\n\n# merge train and test\ndata = train.append(test, ignore_index=True)\n\n# save PassengerId for final submission\npassengerId = test.PassengerId\n\n# create indexes to separate data later on\n# this has been proven extremely useful\ntrain_idx = len(train)\ntest_idx = len(data) - len(test)","93f8b798":"# view a few observations to understand what kind of data we have \ndisplay(data.head(2))\n\ndisplay(data.info())\n\ndescription = pd.Series(['incrementing order of passengers',\n                         'Survival (0 = No; 1 = Yes)',\n                         'indication of economical status',\n                         'Name of passenger',\n                         'male or female',\n                         'Age ranging from 0 to 82',\n                         'Number of siblings or spouces', \n                         'Number of parents or children', \n                         'Ticket number', \n                         'Price of ticket',\n                         'Cabin number', \n                         'Embarkation port (Southampton, Cherbourg, Queenstown)'], index = data.columns)\n\nmissing_value_df = pd.DataFrame({'missing_values (#)': data.isnull().sum(),\n                                 'missing_values (%)': round(data.isnull().sum() * 100 \/ len(data),2),\n                                 'description': description\n                                }\n                               )\ndisplay(missing_value_df)\n\n# show visually missing data\nmsno.matrix(data, figsize=(8, 4));","1ee87ad1":"fig, axes = plt.subplots(1, 2, figsize=(10, 3))\nsns.countplot(ax=axes[0], x='Survived', data=data)\naxes[0].set_ylabel('Passengers (#)')\naxes[0].set_xlabel('')\naxes[0].set_xticklabels(['Died', 'Survived'])\n# the semicolon at the end of piechart is used to make text before chart disappear\naxes[1].pie(data.Survived.value_counts(), explode=[0, 0.1], labels=['Died', 'Survived'], autopct='%1.1f%%', shadow=True, startangle=90);","9d72be54":"fig, ax = plt.subplots(figsize=(5, 3))\nsns.pointplot(x='Pclass', y='Survived', hue='Sex', data=data, ax=ax)\n\n# Percentage of male and female who survived from the titanic\ndisplay(train.groupby('Sex')['Survived'].value_counts(normalize=True).round(2))\n\ndisplay(pd.crosstab(data.Sex, data.Survived, margins=True).style.background_gradient(cmap='summer_r'))\ndisplay(pd.crosstab(data.Pclass, data.Survived, margins=True).style.background_gradient(cmap='summer_r'))","ef2dc188":"fig, ax = plt.subplots(figsize=(5, 3))\nsns.pointplot('SibSp', 'Survived', hue='Sex', data=data, ax=ax)\ndisplay(pd.crosstab(data.SibSp, data.Pclass).style.background_gradient(cmap='summer_r'))","7a5165d4":"display(pd.crosstab(data.Parch, data.Pclass).style.background_gradient(cmap='summer_r'))\nfig, ax = plt.subplots(figsize=(5, 3))\nsns.pointplot('Parch', 'Survived', data=data, hue='Pclass', ax=ax);","84f99ac4":"def my_ecdf(dataset):\n    x = np.sort(dataset)\n    y = np.arange(1, len(x)+1)\/len(x)\n    return x,y\n\nage_mean = np.mean(data.Age)\nage_std = np.std(data.Age)\nsamples = np.random.normal(age_mean, age_std, size=100000)\nx_theor, y_theor = my_ecdf(samples)\n\n# Checking normality with PDF\nfig, ax = plt.subplots(1, 3, figsize=(15, 3))\nsns.distplot(data.Age, kde=True, color='b', ax=ax[0], label='empirical', rug=True, hist=False)\nsns.kdeplot(samples, color='r', linestyle='--', ax=ax[0], label='theoretical')\nax[0].legend()\nax[0].set_xlabel('Age')\nax[0].set_ylabel('PDF')\nax[0].set_title('Probability Density Function (PDF)')\nsns.ecdfplot(data=data, x='Age', label='emprical', color='b', ax=ax[1])\nax[1].plot(x_theor, y_theor, label='theoretical', color='r', linestyle='--')\nax[1].legend()\nax[1].set_xlabel('Age')\nax[1].set_ylabel('CDF')\nax[1].set_title('Empirical Cumulative Density Function (ECDF)')\nax[1].set(ylim=[-0.1,1.1])\nsns.ecdfplot(data=data[data['Survived'] == 1], x='Age', color='g', label='survived', ax=ax[2])\nsns.ecdfplot(data=data[data['Survived'] == 0], x='Age', color='r', label='died', ax=ax[2])\nsns.color_palette('husl',3)\nax[2].set_title('Kernel Density Estimate (KDE)')\nax[2].set_xlabel('Age')\nax[2].set_ylabel('CDF')\nax[2].set(ylim=[-0.1,1.1])\nax[2].legend()\nax[2].annotate('children', xy=(14, 0.1), xytext=(30, 0.01),\n            arrowprops=dict(facecolor='black', shrink=0.1))\n\nstats = data.Age.describe()\nskewness = pd.Series(data.Age.skew(), index = [\"skewness\"])\nstats_df = pd.DataFrame(pd.concat([skewness, stats], sort = False), columns = [data.Age.name])\nstats_df = stats_df.reset_index().rename(columns={\"index\":\"summaryStats\"})\ndisplay(stats_df)","5c49e639":"pal = ['red', 'green']        \ng = sns.FacetGrid(train, col='Pclass', row='Sex', palette=pal, hue='Survived', hue_order=[0, 1])\ng.map(sns.histplot, 'Age',  alpha=.3, bins=20)\ng.add_legend();","92743eaf":"# print all unique titles\ndef return_passengers_title(name):\n    # define regular expression\n    reg = ' ([A-Za-z]+)\\.'\n    # this expression tries to find words that have\n    # a whitespace on their left and a dot right after\n    title_search = re.search(reg, name)\n    # If the title exists, extract and return it.\n    if title_search:\n        return title_search.group(1)\n    else:\n        return \"\"\n\ndata['Title'] = data['Name'].apply(return_passengers_title)\n\n# Group titles in a more compact way\ntitle_map = {\n    \"Capt\": \"Officer\",\n    \"Col\": \"Officer\",\n    \"Major\": \"Officer\",\n    \"Jonkheer\": \"Royalty\",\n    \"Don\": \"Royalty\",\n    \"Dona\": \"Royalty\",\n    \"Sir\" : \"Royalty\",\n    \"Dr\": \"Officer\",\n    \"Rev\": \"Officer\",\n    \"Countess\":\"Royalty\",\n    \"Mme\": \"Mrs\",\n    \"Mlle\": \"Miss\",\n    \"Ms\": \"Mrs\",\n    \"Mr\" : \"Mr\",\n    \"Mrs\" : \"Mrs\",\n    \"Miss\" : \"Miss\",\n    \"Master\" : \"Master\",\n    \"Lady\" : \"Royalty\"\n}\n\ndata['Title'] = data.Title.map(title_map)\n\ndisplay(data.groupby(['Title', 'Pclass'])['Age'].agg(['mean', 'median']).round(1))\n\n# the mean of a binary variable returns the group in favor, meaning the survival-rate\ndisplay(data[['Title', 'Survived']].groupby(['Title']).mean())\n\nfig, ax = plt.subplots(1,2,figsize=(10, 3))\nsns.pointplot('Title', 'Survived', data=data, ax=ax[0]);\nsns.pointplot('Title', 'Survived', data=data, hue='Pclass', ax=ax[1]);","65306f0d":"# Filling the missing values in Age with the mean per Pclass per Title\ndata['Age'] = data.groupby(['Pclass', 'Title'])['Age'].apply(lambda x: x.fillna(x.mean()))","2f6c8b80":"fare_mean = np.mean(data.Fare)\nfare_std = np.std(data.Fare)\nsamples = np.random.normal(fare_mean, fare_std, size=100000)\nx_theor, y_theor = my_ecdf(samples)\n\n# Checking normality with PDF\nfig, ax = plt.subplots(1, 3, figsize=(15, 3))\nsns.distplot(data.Fare, kde=True, color='b', ax=ax[0], label='Empirical PDF', rug=True, hist=False)\nsns.kdeplot(samples, color='r', linestyle='--', ax=ax[0], label='Theoretical PDF')\nax[0].legend()\nax[0].set_xlabel('Fare')\nax[0].set_ylabel('PDF')\nax[0].set_title('Probability Density Function (PDF)')\nsns.ecdfplot(data=data, x='Age', label='emprical', color='b', ax=ax[1])\nax[1].plot(x_theor, y_theor, label='theoretical', color='r', linestyle='--')\nax[1].legend()\nax[1].set_xlabel('Fare')\nax[1].set_ylabel('CDF')\nax[1].set_title('Empirical Cumulative Density Function (ECDF)')\nax[1].set(ylim=[-0.1,1.1])\n\nsns.ecdfplot(data=data[data['Survived'] == 1], x='Fare', color='g', label='Survived', ax=ax[2])\nsns.ecdfplot(data=data[data['Survived'] == 0], x='Fare', color='r', label='Died', ax=ax[2])\nsns.color_palette('husl',3)\nax[2].set_title('Kernel Density Estimate (KDE)')\nax[2].set_xlabel('Fare')\nax[2].set_ylabel('CDF')\nax[2].set(ylim=[-0.1,1.1])\nax[2].annotate('cheap', xy=(50, 0.9), xytext=(200, 0.6),\n            arrowprops=dict(facecolor='black', shrink=0.1))\nax[2].legend()\n\ndisplay(data.groupby('Survived')['Fare'].agg(['mean']).round(2))\n#enhanced_stats = pd.concat(data.Age.describe())\nstats = data.Fare.describe()\nskewness = pd.Series(data.Fare.skew(), index = [\"skewness\"])\nstats_df = pd.DataFrame(pd.concat([skewness, stats], sort = False), columns = [data.Fare.name])\nstats_df = stats_df.reset_index().rename(columns={\"index\":\"summaryStats\"})\ndisplay(stats_df)","e6c43c1b":"missing_fare_idx = data.index[data.Fare.isnull()].tolist()[0]\ndisplay(data.iloc[[missing_fare_idx],:])\n\ndisplay(data.groupby(['Pclass'])['Fare'].agg(['min', 'mean', 'median', 'max']).round(1))\n\n# when mean and median differ significantly, it's always better to pick the median\n# because as a statistic is more robust to outliers.\ndata['Fare'] = data.groupby(['Pclass'])['Fare'].apply(lambda x: x.fillna(x.median()))\n\ndisplay(data[data.index == missing_fare_idx])","61ef1799":"display(data[data.Fare == 0].tail())","28b85d4e":"# display the missing values to get a better feeling\ndisplay(data[data.Embarked.isnull()])\n\n# assing to both passengers\ndata.loc[[61], ['Embarked']] = 'S'\ndata.loc[[829], ['Embarked']] = 'S'\n\n# confirm you assigned the right values\ndisplay(data[(data.index == 61) | (data.index == 829)])","9c19fa8c":"fig, ax = plt.subplots(1, 3, figsize=(15, 3))\nax[0].pie(data.Embarked.value_counts(), explode=[0, 0.05, 0.1], \n          labels=['Southampton', 'Cherbourg', 'Queenstown'], autopct='%1.1f%%', shadow=True, startangle=90);\nsns.pointplot(x='Embarked', y='Survived', data=data, hue='Sex', ax=ax[1])\nax[1].set(ylim=[0,1]);\nsns.pointplot(x='Embarked', y='Survived', data=data, hue='Pclass', ax=ax[2])\nax[2].set(ylim=[0,1]);","a3d35070":"sns.factorplot('Pclass', 'Survived', hue='Sex', col='Embarked', data=data);","12a23c95":"# Plot correlation heatmap of original features\nfig, ax = plt.subplots(figsize=(10, 5))\ncorr = data.corr().round(2)\nsns.heatmap(corr, ax=ax, annot=True,cmap='RdYlGn',linewidths=0.2) #data.corr()-->correlation matrix\nax.set_title('Correlation Matrix of Default Features')\ndel corr","ff0336c0":"# From now on I will create and work on a copy of my data so that\n# I can compare the perfomance of my classifier by alternating between\n# the default data and the advanced data\ndata_advanced = data.copy()","27b46a74":"data_advanced['FamilySize'] = data_advanced['SibSp'] + data_advanced['Parch'] + 1\ndisplay(data_advanced.head(2))\nfig, ax = plt.subplots(1, 2, figsize=(10, 3))\nsns.pointplot('FamilySize', 'Survived', hue='Sex', data=data_advanced, ax=ax[0]);\nsns.pointplot('FamilySize', 'Survived', hue='Pclass', data=data_advanced, ax=ax[1]);","d6ad5734":"display(data_advanced[data_advanced.FamilySize == 11])\ndisplay(data_advanced[(data_advanced.FamilySize == 8)])","6893a33f":"age_bin = 10\ndata_advanced['AgeBand'] = pd.qcut(data_advanced['Age'], age_bin)\ndisplay(data_advanced[:train_idx].groupby(['AgeBand'])['Survived'].mean().to_frame().style.background_gradient(cmap='summer_r'))\n\ndisplay(data_advanced[:train_idx].AgeBand.value_counts())\n\nfig, axes = plt.subplots(1, 3, figsize=(15, 3))\nsns.pointplot('AgeBand', 'Survived', data=data_advanced, ax=axes[0])\naxes[0].set(ylim=[-0.1,1.1]);\naxes[0].set_xticklabels('');\nsns.pointplot('AgeBand', 'Survived', data=data_advanced, hue='Sex', ax=axes[1])\naxes[1].set(ylim=[-0.1,1.1]);\naxes[1].set_xticklabels('');\nsns.pointplot('AgeBand', 'Survived', data=data_advanced, hue='Pclass', ax=axes[2])\naxes[2].set(ylim=[-0.1,1.1]);\naxes[2].set_xticklabels('');","cd1caa69":"# Name Length (without the Title)\ndata_advanced['NameLength'] = data_advanced['Name'].apply(len) - data_advanced['Title'].apply(len)\n\n# binning: the reason I picked 4 bins is because it explains well the pattern\n# and the bins are evenly populated thus eliminating biases in the training\n# of the classifier\ndata_advanced['NameLength'] = pd.qcut(data_advanced['NameLength'], 4)\n\ndisplay(data_advanced['NameLength'].value_counts())\n\n# Let's see if people with longer names have higher chances to survive\nfig, axes = plt.subplots(1, 3, figsize=(15, 3))\nsns.pointplot('NameLength', 'Survived', data=data_advanced, hue='Pclass', ax=axes[0])\naxes[0].set(ylim=[-0.1,1.1])\nsns.pointplot('NameLength', 'Survived', data=data_advanced[data_advanced.Sex == 'male'], hue='Pclass', ax=axes[1])\naxes[1].set(ylim=[-0.1,1.1])\naxes[1].set_title('male')\nsns.pointplot('NameLength', 'Survived', data=data_advanced[data_advanced.Sex == 'female'], hue='Pclass', ax=axes[2])\naxes[2].set(ylim=[-0.1,1.1])\naxes[2].set_title('female')","9f2d86a7":"# Create TicketFrequency\nticket_freq = data_advanced.groupby('Ticket').size()\ndata_advanced['TicketFrequency'] = data_advanced['Ticket'].map(ticket_freq)\n\n# Let's see if people with longer names have higher chances to survive\nfig, axes = plt.subplots(1, 3, figsize=(15, 3))\nsns.countplot(x='TicketFrequency', data=data_advanced, ax=axes[0]);\nsns.pointplot('TicketFrequency', 'Survived', data=data_advanced, hue='Sex', ax=axes[1]);\nsns.pointplot('TicketFrequency', 'Survived', data=data_advanced, hue='Pclass', ax=axes[2]);","0b9b45f9":"data_advanced.Fare = data_advanced.Fare \/ data_advanced.TicketFrequency","107aa153":"# add epsilon to the zero Fare to convert it to log\n# without getting Inf\ndata_advanced['logFare'] = data_advanced.Fare\ndata_advanced.loc[data_advanced.logFare <= 0, ['logFare']] = 0.5\ndata_advanced['logFare'] = np.log2(data_advanced.logFare)\n\n# whiten signal (zero mean, unit std)\nfare_df = data_advanced.copy()[['Fare']]\nscaled_fare = StandardScaler().fit_transform(fare_df.values)\nscaled_fare_df = pd.DataFrame(scaled_fare, index=fare_df.index, columns=fare_df.columns)\ndata_advanced['whiteFare'] = scaled_fare_df.Fare\n\n# Checking normality with PDF\nfig, ax = plt.subplots(1, 3, figsize=(15, 3))\nsns.distplot(data_advanced.Fare, kde=True, color='b', ax=ax[0], label='Empirical', rug=True, hist=False)\nsns.kdeplot(samples, color='r', linestyle='--', ax=ax[0], label='Theoretical')\nax[0].legend()\nax[0].set_xlabel('Fare')\nax[0].set_ylabel('PDF')\nax[0].set_title('Skewed PDF')\n\nlog_fare_mean = np.mean(data_advanced.logFare)\nlog_fare_std = np.std(data_advanced.logFare)\nlog_samples = np.random.normal(log_fare_mean, log_fare_std, size=100000)\nx_theor_log, y_theor_log = my_ecdf(log_samples)\n\nsns.distplot(data_advanced.logFare, kde=True, color='g', ax=ax[1], label='Empirical', rug=True, hist=False)\nsns.kdeplot(log_samples, color='r', linestyle='--', ax=ax[1], label='Theoretical')\nax[1].legend()\nax[1].set_xlabel('logFare')\nax[1].set_ylabel('PDF')\nax[1].set_title('log-Skewed PDF')\n\n# draw samples from ideal gaussian distribution\nwhite_samples = np.random.normal(0,1,size=100000)\nx_theor_white, y_theor_white = my_ecdf(white_samples)\n\nsns.distplot(data_advanced.whiteFare, kde=True, color='g', ax=ax[2], label='Empirical', rug=True, hist=False)\nsns.kdeplot(white_samples, color='r', linestyle='--', ax=ax[2], label='Theoretical')\nax[2].legend()\nax[2].set_xlabel('whiteFare')\nax[2].set_ylabel('PDF')\nax[2].set_title('whitened-Skewed PDF')\n\n# Checking normality with CDF\nfig, ax = plt.subplots(1, 3, figsize=(15, 3))\nsns.ecdfplot(data=data_advanced, x='Fare', label='emprical', color='b', ax=ax[0])\nax[0].plot(x_theor, y_theor, label='theoretical', color='r', linestyle='--')\nax[0].legend()\nax[0].set_xlabel('Fare')\nax[0].set_ylabel('CDF')\nax[0].set_title('Skewed CDF')\nax[0].set(ylim=[-0.1,1.1])\n\nsns.ecdfplot(data=data_advanced, x='logFare', label='emprical', color='g', ax=ax[1])\nax[1].plot(x_theor_log, y_theor_log, label='theoretical', color='r', linestyle='--')\nax[1].legend()\nax[1].set_xlabel('logFare')\nax[1].set_ylabel('CDF')\nax[1].set_title('log-Skewed CDF')\nax[1].set(ylim=[-0.1,1.1])\n\nsns.ecdfplot(data=data_advanced, x='whiteFare', label='emprical', color='g', ax=ax[2])\nax[2].plot(x_theor_white, y_theor_white, label='theoretical', color='r', linestyle='--')\nax[2].legend()\nax[2].set_xlabel('whiteFare')\nax[2].set_ylabel('CDF')\nax[2].set_title('whitened-Skewed CDF')\nax[2].set(ylim=[-0.1,1.1])\n\nstats = data_advanced.Fare.describe().round(3)\nstats_log = data_advanced.logFare.describe().round(3)\nstats_white = data_advanced.whiteFare.describe().round(3)\n\nskewness = pd.Series(data_advanced.Fare.skew(), index = [\"skewness\"])\nskewness_log = pd.Series(data_advanced.logFare.skew(), index = [\"skewness\"])\nskewness_white = pd.Series(data_advanced.whiteFare.skew(), index = [\"skewness\"])\n\nstats_df = pd.DataFrame(pd.concat([skewness, stats], sort = False), columns = [data_advanced.Fare.name])\nstats_df = stats_df.reset_index().rename(columns={\"index\":\"summaryStats\"})\n\nstats_log_df = pd.DataFrame(pd.concat([skewness_log, stats_log], sort = False), columns = [data_advanced.logFare.name])\nstats_log_df = stats_log_df.reset_index().rename(columns={\"index\":\"summaryStats\"})\n\nstats_white_df = pd.DataFrame(pd.concat([skewness_white, stats_white], sort = False), columns = [data_advanced.whiteFare.name])\nstats_white_df = stats_white_df.reset_index().rename(columns={\"index\":\"summaryStats\"})\n\nstats_all_df = pd.merge(pd.merge(stats_df, stats_log_df), stats_white_df)\ndisplay(stats_all_df)","ace41ba3":"fare_bin = 13\ndata_advanced['FareCategory'] = pd.qcut(data_advanced.logFare, fare_bin)\ndisplay(data_advanced[:train_idx].groupby(['FareCategory'])['Survived'].mean().to_frame().style.background_gradient(cmap='summer_r'))\n\nfig, axes = plt.subplots(1, 3, figsize=(15, 3))\nsns.pointplot('FareCategory', 'Survived', data=data_advanced, ax=axes[0])\naxes[0].set(ylim=[-0.1,1.1])\naxes[0].set_xticklabels('');\nsns.pointplot('FareCategory', 'Survived', data=data_advanced, hue='Sex', ax=axes[1])\naxes[1].set_xticklabels('');\nsns.pointplot('FareCategory', 'Survived', data=data_advanced, hue='Pclass', ax=axes[2])\naxes[2].set_xticklabels('');","0b7582fc":"data_advanced['IsAlone'] = 1\nis_alone = data_advanced['TicketFrequency'] == 1\ndata_advanced.loc[is_alone, 'IsAlone'] = 1\ndata_advanced.loc[~is_alone, 'IsAlone'] = 0\n\nfig, axes = plt.subplots(1, 2, figsize=(10, 3))\nsns.pointplot('IsAlone', 'Survived', data=data_advanced, hue='Pclass', ax=axes[0])\naxes[0].set_xticklabels(['No', 'Yes']);\naxes[0].set(ylim=[-0.1,1.1])\nsns.pointplot('IsAlone', 'Survived', data=data_advanced, hue='Sex', ax=axes[1])\naxes[1].set(ylim=[-0.1,1.1])\ndisplay(data_advanced[['IsAlone', 'Sex', 'Survived']].groupby(['IsAlone', 'Sex']).mean())\naxes[1].set_xticklabels(['No', 'Yes']);","1e9d8738":"#display(data_advanced.info())\n\n# Save the imputated Age. I will need it to test\n# the consistency of the distribution of survival\n# rate vs age for each gender for test and train data.\nAge = data_advanced.Age\n\n        \nREDUNDANT_FEATURES = [\n    'PassengerId',\n    'Name',\n    'Age',\n    'Ticket',\n    'Fare',\n    'logFare',\n    'whiteFare',\n    'Cabin',\n    'SibSp',\n    'Parch'\n]\n\ndata_advanced = data_advanced.drop(REDUNDANT_FEATURES , axis=1)\n\n# confirm all features are converted to numerical variables\ndisplay(data_advanced.info())\n# show two observations with the engineered features\ndisplay(data_advanced.head(2))","766b2791":"ORDINAL_FEATURES = [\n    'NameLength',\n    'AgeBand',\n    'FareCategory'\n]\n\nfor feature in ORDINAL_FEATURES:        \n    data_advanced[feature] = LabelEncoder().fit_transform(data_advanced[feature])","224272b8":"# One-hot encoding can be achieved either by OneHotEncoder() or pd.get_dummies()\n# We shoot for the pd.get_dummies() but we kept commented the first approach\n# for the sake of completeness.\n\nNOMINAL_FEATURES = [\n    'Sex',\n    'Title',\n    'Embarked'\n]\n\nfor feature in NOMINAL_FEATURES:\n    # if we have N dummy variables, N-1 are enough to describe the original feature\n    # We want the remaining dummy features to be linearly independent\n    data_advanced = pd.concat([data_advanced ,pd.get_dummies(data_advanced[feature], prefix=feature, prefix_sep='_', drop_first=True)], axis=1)\n    data_advanced = data_advanced.drop(feature, axis=1)","2b1b97ba":"print('Data after feature engineering')\ndisplay(data_advanced.head(2))\ndisplay(data_advanced.info())","dd816375":"# Plot correlation heatmap of original features if # features smaller than 20\n# otherwise it takes too long to plot it, and it's so messy that one cannot \n# derive anything out of it\nfeature_size = data_advanced.shape[1] - 1\nprint('feature-size: {}'.format(feature_size))\nif feature_size <= 20:\n    fig, ax = plt.subplots(figsize=(16, 8))\n    corr = data_advanced.corr().round(2)\n    sns.heatmap(corr, ax=ax, annot=True,cmap='RdYlGn',linewidths=0.2) \n    ax.set_title('Correlation Matrix of Engineered Features')\n    del corr","222d723d":"# this will be proven useful later on\nfeature_names = data_advanced.drop(['Survived'], axis=1).columns","b0a32482":"# Pretend there is only one dataset, the train dataset\nX = data_advanced.iloc[:train_idx].drop(['Survived'], axis=1)\ny = data_advanced.iloc[:train_idx]['Survived']\n\n# I use the stratify here to make sure that the data are split in a way that the distribution\n# of 'Survived' is respected both in the y_train and the y_val .\nX_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.25, random_state=SEED, stratify=y)","d1fbc0da":"scaler = StandardScaler()\nscaler.fit(X_train)\nX_train = scaler.transform(X_train)\nX_val = scaler.transform(X_val)","6ec0b479":"def plot_roc_auc(clf, fpr, tpr):\n    ''' Plot receiver operating characteristic curve (ROC). The diagonal dashed\n        line corresponds to a classifier that randomly guesses (ROC=0.5).\n        The higher this value, the greater the accuracy of your classifier\n        \n        Args:\n            clf: classifier\n            fpr: false positive rate calculated by the \"roc_curve\" sklearn metric\n            tpr: true positive rate calculated by the \"roc_curve\" sklearn metric\n    '''\n    roc_auc = auc(fpr, tpr)\n    # plot the false positive rate on the x axis and the true positive rate on the y axis\n    fig, ax = plt.subplots(figsize=(5, 3))\n    ax.plot(fpr, tpr, label='AUC = {:0.3f}'.format(roc_auc))\n    ax.legend(loc=0)\n    ax.plot([0,1], [0,1], ls='--')\n    ax.set_title('ROC: ' + clf.__class__.__name__)\n    ax.set_ylabel('True Positive Rate')\n    ax.set_xlabel('False Positive Rate')\n    plt.show()\n    \ndef plot_precision_and_recall(precision, recall, threshold):\n    ''' Plot precision and recall vs threshold. These two are contradicting,\n        like the bias and the variance of a classifier. However, there is\n        always a point where these two intersect and that is usually the\n        optimal threshold.\n        \n        Args:\n            precision: precision of the classifier\n            recall: recall of the classifier\n            threshold: it represents the value above which\n                       a data point is considered in the positive class\n    '''\n    fig, ax = plt.subplots(figsize=(5, 3))\n    ax.plot(threshold, precision[:-1]*100, \"r-\", label=\"precision\", linewidth=2)\n    ax.plot(threshold, recall[:-1]*100, \"b\", label=\"recall\", linewidth=2)\n    ax.axvline(x=0.5, color='k', linestyle='dashed', label='default=0.5')\n    intersect_ind = np.argwhere(np.diff(np.sign(precision[:-1] - recall[:-1]))).flatten()[0]\n    ax.axvline(x=threshold[intersect_ind], color='g', linestyle='dashed', label='optimal={:.2f}'.format(threshold[intersect_ind]))\n    ax.set_ylabel(\"Percentage (%)\")\n    ax.set_xlabel(\"Threshold\")\n    ax.legend(loc=\"upper right\")\n    ax.set_ylim([0, 100])\n    plt.show()","0ae32c50":"# instantiate random forest\nrf = RandomForestClassifier(oob_score = True, random_state = SEED)\n\n# get cross validation accuracy\ncv_accuracy = cross_val_score(rf, X_train, y_train, cv=kFold, scoring = \"accuracy\")\nmean_cv_accuracy = cv_accuracy.mean()\nstd_cv_accuracy = cv_accuracy.std()\n\n# get cross validation roc_auc\ncv_roc_auc = cross_val_score(rf, X_train, y_train, cv=kFold, scoring = \"roc_auc\")\nmean_cv_roc_auc = np.mean(cv_roc_auc)\n\n# Train classifier\nrf.fit(X_train, y_train)\n\n# Train accuracy is not something we can rely upon. I only include it here\n# to show you how different the two accuracies are\ntrain_accuracy = rf.score(X_train, y_train)\ntest_accuracy = rf.score(X_val, y_val)\n# Calculate OBB score (or generalization accuracy)\nobb_score = rf.oob_score_\n\n# Make predictions\ny_pred_prob = rf.predict_proba(X_val)[:,1]\ny_pred = rf.predict(X_val)\n\n# RMSE\nrmse = MSE(y_val, y_pred) ** 0.5\nroc_score = roc_auc_score(y_val, y_pred_prob)\n\nevaluation = pd.DataFrame(\n    {\n        'train_accuracy': [train_accuracy],\n        'test_accuracy': [test_accuracy],\n        'cv_accuracy_mean':[mean_cv_accuracy],\n        'cv_accuracy_std':[std_cv_accuracy],\n        'obb_score': [obb_score],\n        'rmse':[rmse],\n        'roc_score':[roc_score]\n    }\n)\n\n# Show 10-Fold accuracy score and obb score\ndisplay(evaluation)\n\n# plot recall vs precision \nprecision, recall, threshold = precision_recall_curve(y_val, y_pred_prob)\nplot_precision_and_recall(precision, recall, threshold)\n\n# plot roc curve\nfpr, tpr, thres = roc_curve(y_val, y_pred_prob)\nplot_roc_auc(rf, fpr, tpr)\n\n# Confusion matrix and classification report\nconf_mat_before_tuning = confusion_matrix(y_val, y_pred)\nfig, ax = plt.subplots(figsize=(5, 3))\nsns.heatmap(conf_mat_before_tuning,annot=True,fmt='3.0f',cmap=\"summer\", ax=ax)\nax.set_title('Confusion_matrix', y=1.05, size=15)\n\nprint('-------------------------------------')\nprint('Classification-report (before-tuning)')\nprint('-------------------------------------')\nprint(classification_report(y_val, y_pred, target_names=['Died', 'Survived']))\n\n# print importance of each feature\nimportances = pd.DataFrame({'feature':feature_names,'importance':rf.feature_importances_})\nimportances = importances.sort_values('importance',ascending=False).set_index('feature')\nfig, ax = plt.subplots(figsize=(10, 5))\nsns.barplot(ax=ax, x='importance', y='feature', data=importances.reset_index())\nax.set_title('Random Forest Classifier Mean Feature Importance Between Folds', size=15)\n\n# print shap values of features (another metric for importance)\nX_val_df = pd.DataFrame(X_val, columns=feature_names)\nexplainer = shap.TreeExplainer(rf)\nshap_values = explainer.shap_values(X_val_df)\nfig, ax = plt.subplots(figsize=(3, 3))\nshap.summary_plot(shap_values, X_val_df, plot_type=\"bar\", auto_size_plot=False)","5f1fc1e9":"# Create the random grid parameters\nparam_grid = {'n_estimators': [10, 20, 50, 100, 300, 500, 1000, 1500, 1700, 2000],\n              'max_depth': [2, 5, 7, 8, 9, 10],\n              'max_features': ['auto'],\n              'min_samples_split': [1, 3, 5],\n              'min_samples_leaf': [1, 3, 5],\n              'criterion': ['gini'],\n              'bootstrap': [True]\n              }\n\n# optimal parameters from grid search with gini criterion\noffline_best_params_gini = {'n_estimators': 2000,\n                       'max_depth': 7,\n                       'max_features': 'auto',\n                        'min_samples_split': 3,\n                        'min_samples_leaf': 1,\n                       'criterion': 'gini',\n                       'bootstrap': True\n                      }\n\n# optimal parameters from grid search with entropy criterion\noffline_best_params_entropy = {'n_estimators': 1700,\n                       'max_depth': 8,\n                       'max_features': 'auto',\n                        'min_samples_split': 5,\n                        'min_samples_leaf': 1,\n                       'criterion': 'entropy',\n                       'bootstrap': True\n                      }\n\n# initialize optimal random forest\noptimal_rf = RandomForestClassifier()\nprint('--------[ Optimal Random Forest (Before Tuning) ]----------')\ndisplay(optimal_rf)\nprint('----------------------------------------------------------')\n\nif tune_random_forest == 'grid':\n    print('Grid search started ... (Fetch a coffee ;) ')\n    # Perform 5-fold grid search to retrieve optimal estimator\n    rf_random = GridSearchCV(estimator=RandomForestClassifier(oob_score = True, random_state = SEED),\n                                   param_grid=param_grid,\n                                   scoring='accuracy',\n                                   cv=kFold,\n                                   verbose=4,\n                                   n_jobs=-1)\n    rf_random.fit(X_train, y_train)\n    best_params = rf_random.best_params_\n    optimal_rf = rf_random.best_estimator_\n    print('best parameters:')\n    display(best_params)\n    print('Grid search finished!')\nelif tune_random_forest == 'random':\n    print('Random search started ... ')\n    # Perform 5-fold random search to retrieve optimal estimator\n    rf_random = RandomizedSearchCV(estimator=RandomForestClassifier(oob_score = True, random_state = SEED),\n                                   param_distributions=random_grid,\n                                   n_iter=100,\n                                   cv=kFold,\n                                   verbose=2,\n                                   random_state=SEED,\n                                   n_jobs=-1)\n    rf_random.fit(X_train, y_train)\n    best_params = rf_random.best_params_\n    optimal_rf = rf_random.best_estimator_\n    print('best parameters:')\n    display(best_params)\n    print('Random search finished!')\nelif tune_random_forest == 'tuned_gini':\n    print('Best params with gini criterion (offline):')\n    # calculated from previous online tuning\n    optimal_rf = RandomForestClassifier(**offline_best_params_gini, oob_score=True, random_state = SEED)\nelif tune_random_forest == 'tuned_entropy':\n    print('Best params with entropy criterion (offline):')\n    # calculated from previous online tuning\n    optimal_rf = RandomForestClassifier(**offline_best_params_entropy, oob_score=True, random_state = SEED)\nelse:\n    print('Default parameters:')\n    # default random forest, untuned\n    optimal_rf = RandomForestClassifier(oob_score=True, random_state = SEED)\n\nprint('--------[ Optimal Random Forest (After Tuning) ]----------')\ndisplay(optimal_rf)\nprint('----------------------------------------------------------')","906e4162":"# instantiate random forest and assing it the optimized random forest from\n# previous step\nrf = optimal_rf\n\n# get cross validation accuracy\ncv_accuracy = cross_val_score(rf, X_train, y_train, cv=kFold, scoring = \"accuracy\")\nmean_cv_accuracy = cv_accuracy.mean()\nstd_cv_accuracy = cv_accuracy.std()\n\n# get cross validation roc_auc\ncv_roc_auc = cross_val_score(rf, X_train, y_train, cv=kFold, scoring = \"roc_auc\")\nmean_cv_roc_auc = np.mean(cv_roc_auc)\n\n# Train classifier\nrf.fit(X_train, y_train)\n\n# Train accuracy is not something we can rely upon. I only include it here\n# to show you how different the two accuracies are\ntrain_accuracy = rf.score(X_train, y_train)\ntest_accuracy = rf.score(X_val, y_val)\n# Calculate OBB score (or generalization accuracy)\nobb_score = rf.oob_score_\n\n# Make predictions\ny_pred_prob = rf.predict_proba(X_val)[:,1]\ny_pred = rf.predict(X_val)\n\n# RMSE\nrmse = MSE(y_val, y_pred) ** 0.5\nroc_score = roc_auc_score(y_val, y_pred_prob)\n\nevaluation = pd.DataFrame(\n    {\n        'train_accuracy': [train_accuracy],\n        'test_accuracy': [test_accuracy],\n        'cv_accuracy_mean':[mean_cv_accuracy],\n        'cv_accuracy_std':[std_cv_accuracy],\n        'obb_score': [obb_score],\n        'rmse':[rmse],\n        'roc_score':[roc_score]\n    }\n)\n\n# Show 10-Fold accuracy score and obb score\ndisplay(evaluation)\n\n# plot precision vs recall\nprecision, recall, threshold = precision_recall_curve(y_val, y_pred_prob)\nplot_precision_and_recall(precision, recall, threshold)\n\n# index of optimal threshold\nopt_thres_ind = np.argwhere(np.diff(np.sign(precision[:-1] - recall[:-1]))).flatten()[0]\n# threshold above which recall starts falling aggresively\nopt_thres = threshold[opt_thres_ind]\n\n# plot roc curve\nfpr, tpr, thres = roc_curve(y_val, y_pred_prob)\nplot_roc_auc(rf, fpr, tpr)\n\n# Confusion matrix and classification report\nconf_mat = confusion_matrix(y_val, y_pred)\nfig, ax = plt.subplots(1, 2, figsize=(10, 3))\nsns.heatmap(conf_mat_before_tuning, annot=True,fmt='3.0f',cmap=\"summer\", ax=ax[0])\nax[0].set_title('Confusion matrix (before tuning)', y=1.05, size=15)\nsns.heatmap(conf_mat,annot=True,fmt='3.0f',cmap=\"summer\", ax=ax[1])\nax[1].set_title('Confusion matrix (after tuning)', y=1.05, size=15)\n\nprint('------------------------------------')\nprint('Classification-report (after-tuning)')\nprint('------------------------------------')\nprint(classification_report(y_val, y_pred, target_names=['Died', 'Survived']))","35d0c080":"def plot_learning_curve(classifier, X, y, ax, metric):\n    \"\"\"Returns a plot of learning curve of a classifier.\"\"\"\n        \n    # Create CV training and test scores for various training set sizes\n    train_size, train_scores, test_scores = learning_curve(classifier, X, y, cv = kFold,\n                                                    scoring=metric, n_jobs = -1, \n                                                    train_sizes = np.linspace(0.01, 1.0, 20),\n                                                    random_state = SEED)\n    \n    # Create mean and standard deviations of training set scores\n    train_mean = np.mean(train_scores, axis = 1)\n    train_std = np.std(train_scores, axis = 1)\n\n    # Create mean and standard deviations of test set scores\n    test_mean = np.mean(test_scores, axis = 1)\n    test_std = np.std(test_scores, axis = 1)\n    \n    # Location of labels\n    label_loc = 'lower right'\n    \n    if metric == 'neg_mean_squared_error':\n        train_mean = - train_mean\n        test_mean = - test_mean\n        label_loc = 'upper right'\n\n    # Draw lines\n    ax.plot(train_size, train_mean, \"o-\", color = \"red\",  label = \"training\")\n    ax.plot(train_size, test_mean, \"o-\", color = \"green\", label = \"cross-validation\")\n    \n    # Draw confidence interval +-1std\n    ax.fill_between(train_size, train_mean - train_std, train_mean + train_std, alpha = 0.2, color = \"r\") \n    ax.fill_between(train_size, test_mean - test_std, test_mean + test_std, alpha = 0.2, color = \"g\")\n\n    # Create plot\n    ax.set_xlabel(\"Training Set Size\")\n    ax.set_ylabel(\"{}\".format(metric))\n    ax.legend(loc = label_loc)\n    ax.grid(True)\n    \nrf_classifiers = [RandomForestClassifier(oob_score=True, random_state = SEED),\n                  RandomForestClassifier(**offline_best_params_gini, oob_score=True, random_state = SEED)]\nrf_labels = ['Default RF', 'Gini-tuned RF']\n\n# options: {'accuracy', 'roc_auc', 'neg_mean_squared_error', etc.}\n# default: accuracy\nmetric = 'accuracy'\n\n# Accuracy score\nfig, axes = plt.subplots(1, 2, figsize=(12, 4))\nfor ax, clf, label in zip (axes, rf_classifiers, rf_labels):\n    plot_learning_curve(clf, X, y, ax, 'accuracy')\n    ax.set_title(label, fontsize = 14)","71cefe78":"# Split train and test data manually. Remember our test data come with no label\n# for survival. However, when we merged the two datasets, the 'Survived' values\n# associated with the test rows are set to NaN.\nX_train = data_advanced.iloc[:train_idx].drop(['Survived'], axis=1)\ny_train = data_advanced.iloc[:train_idx]['Survived']\nX_test = data_advanced.iloc[test_idx:].drop(['Survived'], axis=1)\n\n# scale the features\nX_train = StandardScaler().fit_transform(X_train)\nX_test = StandardScaler().fit_transform(X_test)\n\n# retrain the tuned random forest on the entire train dataset\nrf.fit(X_train, y_train)\n\n# make the predictions to be sumbitted as your final solution\ny_pred = rf.predict(X_test).astype('int')\n\n# Do this if you want to enforce the optimal threshold\n# you identified previously. If this is smaller than 0.5\n# it means you will allow your classifier to rescue a \n# few more passengers. Your classifier will be more optimistic.\n# This doesn't mean it will improve your classifier's performance.\n# At least for me it worsened things.\n#y_proba_test = rf.predict_proba(X_test)[:,1]\n#y_pred = (y_proba_test >= opt_thres).astype('int')","21d46705":"# create enchanced test data with  predicted 'Survived' column\ntest_enhanced = test\ntest_enhanced['Survived'] = pd.Series(y_pred)\n\n# Submit your solution\ntest_enhanced[['PassengerId', 'Survived']].to_csv('titanic_predictions.csv', index=False)\nprint(\"Your submission was successfully saved!\")\n\ndisplay(test_enhanced[['PassengerId', 'Survived']].head(10))","9e163f70":"# fill NaN with some dummy number\n# if we don't do this we won't be able\n# to convert the Survived column to int\ndata_advanced.Survived = data_advanced.Survived.fillna(555)\n# convert the 'Survived' from float to int\ndata_advanced.Survived = data_advanced.Survived.astype(int)\n\n# substitute dummy values with your predictions\n# for the subpart of the 'Survived' columns associated\n# with the test data\nfor k in range(test_idx, data_advanced.shape[0]):\n    data_advanced.loc[[k], ['Survived']] = int(y_pred[k-test_idx])\n\n# add a few more columns from the original data that we had neglected\n# so far but we need them now again for our analysis\ndata_advanced['Sex'] = data.Sex\ndata_advanced['Age'] = Age\n\ntrain_advanced = data_advanced[:train_idx]\ntest_advanced = data_advanced[test_idx:]\n\n# Check the ratio of men and women in the two datasets\nfig, axes = plt.subplots(1, 2, figsize=(10, 3))\naxes[0].pie(train_advanced.Sex.value_counts(), explode=[0, 0.1], labels=['Female', 'Male'], autopct='%1.1f%%', shadow=True, startangle=90);\naxes[0].set_title('Train Data Male\/Female')\naxes[1].pie(test_advanced.Sex.value_counts(), explode=[0, 0.1], labels=['Female', 'Male'], autopct='%1.1f%%', shadow=True, startangle=90);\naxes[1].set_title('Test Data Male\/Female')\n\n# We expect the survival rate to be similar on the test data\nfig, axes = plt.subplots(1, 2, figsize=(10, 3))\naxes[0].pie(train_advanced.Survived.value_counts(), explode=[0, 0.1], labels=['Died', 'Survived'], autopct='%1.1f%%', shadow=True, startangle=90);\naxes[0].set_title('Survival rate (training data)')\naxes[1].pie(test_advanced.Survived.value_counts(), explode=[0, 0.1], labels=['Died', 'Survived'], autopct='%1.1f%%', shadow=True, startangle=90);\naxes[1].set_title('Predicted Survival rate (test data)')\n\n\n# Let's see what our classifier predicted for Roderick Chisholm (survived) and Joseph Bruce (died)\nprint('-------------------------------')\nprint('------[ from wikipedia ]-------')\nprint('-------------------------------')\nprint('Roderick Chisholm (survived)')\nprint('Joseph Ismay      (died)')\nprint('-------------------------------')\ndisplay(test.loc[[1157-test_idx, 1263-test_idx], :] )\n\n# https:\/\/en.wikipedia.org\/wiki\/Passengers_of_the_Titanic\nprint('-------------------------------')\nprint('------[ from wikipedia ]-------')\nprint('-------------------------------')\nprint('Ada Sage                 (died)')\nprint('John George Sage         (died)')\nprint('William Henry Sage       (died)')\nprint('John (Annie Bullen) Sage (died)')\nprint('-------------------------------')\ndisplay(test.loc[[1079-test_idx, 1233-test_idx, 1251-test_idx, 1256-test_idx], :] )\n\n# https:\/\/www.encyclopedia-titanica.org\/titanic-victim\/frederick-charles-godwin.html\n# https:\/\/www.encyclopedia-titanica.org\/titanic-victim\/jessie-allis-goodwin.html\nprint('-------------------------------------------')\nprint('------[ from titanica encyclopedia ]-------')\nprint('-------------------------------------------')\nprint('Mr Frederick Charles Godwin      (died)')\nprint('Miss Jessie Allis Mary Goodwin   (died)')\nprint('-------------------------------------------')\ndisplay(test_enhanced.loc[[1030-train_idx, 1031-train_idx], :] )","2ca54edb":"surv_male_train = train_advanced[(train_advanced.Survived == 1) & (train_advanced.Sex == 'male')]\nsurv_female_train = train_advanced[(train_advanced.Survived == 1) & (train_advanced.Sex == 'female')]\ndead_male_train = train_advanced[(train_advanced.Survived == 0) & (train_advanced.Sex == 'male')]\ndead_female_train = train_advanced[(train_advanced.Survived == 0) & (train_advanced.Sex == 'female')]\n\nsurv_male_test = test_advanced[(test_advanced.Survived == 1) & (test_advanced.Sex == 'male')]\nsurv_female_test = test_advanced[(test_advanced.Survived == 1) & (test_advanced.Sex == 'female')]\ndead_male_test = test_advanced[(test_advanced.Survived == 0) & (test_advanced.Sex == 'male')]\ndead_female_test = test_advanced[(test_advanced.Survived == 0) & (test_advanced.Sex == 'female')]\n\n# plot distribution female\/male survivors\nfig, ax = plt.subplots(1, 2, figsize=(10, 3))\nsns.distplot(surv_female_train.Age, kde=True, hist=False, color='b', ax=ax[0], label='Train')\nsns.distplot(surv_female_test.Age, kde=True, hist=False, color='r', ax=ax[0], label='Test')\nax[0].legend()\nax[0].set_title('Women-Survival')\nax[0].set_xlabel('Age')\nax[0].set_ylabel('KDE')\nax[0].set_ylim(0, 0.050);\nsns.distplot(surv_male_train.Age, kde=True, hist=False, color='b', ax=ax[1], label='Train')\nsns.distplot(surv_male_test.Age, kde=True, hist=False, color='r', ax=ax[1], label='Test')\nax[1].legend()\nax[1].set_title('Men-Survival')\nax[1].set_xlabel('Age')\nax[1].set_ylabel('')\nax[1].set_ylim(0, 0.050);\n\n# plot distribution female\/male victims\nfig, ax = plt.subplots(1, 2, figsize=(10, 3))\nsns.distplot(dead_female_train.Age, kde=True, hist=False, color='b', ax=ax[0], label='Train')\nsns.distplot(dead_female_test.Age, kde=True, hist=False, color='r', ax=ax[0], label='Test')\nax[0].legend()\nax[0].set_title('Women-Death')\nax[0].set_xlabel('Age')\nax[0].set_ylabel('KDE')\nax[0].set_ylim(0, 0.050);\nsns.distplot(dead_male_train.Age, kde=True, hist=False, color='b', ax=ax[1], label='Train')\nsns.distplot(dead_male_test.Age, kde=True, hist=False, color='r', ax=ax[1], label='Test')\nax[1].legend()\nax[1].set_title('Men-Death')\nax[1].set_xlabel('Age')\nax[1].set_ylabel('')\nax[1].set_ylim(0, 0.050);","23fb0654":"# train data\npal = ['red', 'green']        \ng = sns.FacetGrid(train_advanced, col='Pclass', row='Sex', palette=pal, hue='Survived', hue_order=[0, 1])\ng.map(sns.histplot, 'Age',  alpha=.3, bins=20)\ng.add_legend()\ng.fig.subplots_adjust(top=0.9) # adjust the Figure in rp\ng.fig.suptitle('Survival Rate Distribution in Age per Sex per Pclass (Train Data)');\n\n# test data\npal = ['red', 'green']        \ng = sns.FacetGrid(test_advanced, col='Pclass', row='Sex', palette=pal, hue='Survived', hue_order=[0, 1])\ng.map(sns.histplot, 'Age',  alpha=.3, bins=20)\ng.add_legend()\ng.fig.subplots_adjust(top=0.9) # adjust the Figure in rp\ng.fig.suptitle('Predicted Survival Rate Distribution in Age per Sex per Pclass (Test Data)');","7ec7467e":"## 3.6 Create _IsAlone_ from _TicketFrequency_","79674605":"## 3.9 One-Hot Encoding of nominal features\nNominal are the features which are grouped in categories, but there is no sense of order in these categories, e.g.: `Title` or `Sex`. Binary variables like `IsAlone` do not need to be one-hot encoded. When a variable takes N different values, then we can have N-1 one-hot dummy variables describing it perfectly.","ef8b5aea":"__Observations__:\n1. From the histograms we observe that 1st and 2nd class women have the greatest chances of survival, while in the 3rd class the chances for survival for women seem 50-50.\n2. From the histograms we observe 2nd and 3rd class men have the lowest survival rate, while 1st class men chances to survive is 50-50.","592cca94":"__Observations__:\n1. Regardless of class, passengers without children or parents seem to have minimum chances of survival.\n2. Class 1 and 2, seem to have a rise in survival rate when this number inreases. Parch for these two classes does not increase beyond 2 and 3 respectively. The way I interpret this, the richer a passenger, the less children they have.\n3. For the 3rd class Parch doesn't seem to affect the survival rate in a positive way. Especially passengers with 4 or 6 children seem to have no chance to survive. It could be that these passengers stayed in the lower decks which flooded first, or they didn't have enough time to reach the lifeboats on time.","c0075416":"__Observations__:\n1. For the default random forest the cross-validation curve reaches an accuracy-plateau at 0.8 and additional training data do not seem to help the generalization of the model. \n2. I cannot understand why the training curve for the default RF is almost stuck to 1. How can it be? Please leave a comment if you have deeper knowledge in learning curves. I don't.\n3. For the tuned RF the validation curve reaches an accuracy-plateau above 0.8 and it seems that if we had more data the training and validation curve would converge into some value definitely higher than 0.8. I think I can now understand why, no matter how hard I tried, my RF could not score higher than 0.79 in the leaderboard.\n4. Trying a different classifier could lead to higher accuracy. I may do this in the future. Faysal has done some decent comparison among different classifiers in [A Comprehensive Guide to Titanic Machine Learning ](https:\/\/www.kaggle.com\/eraaz1\/a-comprehensive-guide-to-titanic-machine-learning).","3a81110d":"__Observations:__\n1. For your information, Kernel Density Funtions are nothing more than a smoothened version of the respective histogram of the variable we are trying to describe that is an approximation of the Probability Density Function of the same variable.\n2. Women-Survival: The train and test distributions look very similar, so we predicted that the female population of the test data will have similar survival rate as the train data.\n3. Women-Death: the test distribution seems slightly left-skewed (negative skeweness), which means we had correctly trained our random forest since it's favoring younger women. Remember, __women and children first__.\n4. Men-Survival: The test distribution is right-skewed (positive skeweness) which means our random forest favored young men. However, we observe that our random forest is very harsh with men between 20 and 50 years old compared to its train-data counterpart. This means our model is too pessimistic for men of that age.\n5. Men-Death: The two distibutions fit very well with the random forest favoring boys younger than 9 years old. This is a good sign but if we compare it with young girls of the same age we can see that our classifier respected first the gender and then the age. This coincides with the importance of each feature in the heatmap as shown in section 5.1.1.","1b8ee0f1":"__Observations__:\n1. First class women have the highest survival rate irrespective of embarkation port.\n2. Third class class passengers embarked from Southampton had the lowest survival rate irrespective of gender.\n3. There is a very strange pattern for first and second class passengers embarked from Queenstown; all women survived and all men died for this group.\n4. My hypothesis that the embarkation port does not impact the survival rate is supported from the factoplots. However, there may be a non-linear relationship between _Embarked_ and _Survived_. Decision trees and random forests are doing a great job identifying such non-linear relationships.","a5c83c0b":"### 3.3.1 Retrieve _NameLength_ from _Name_","e26beaec":"### 4.1 Split the train dataset in train and validation\nI decided to split my train data into 75% train and 25% validation data. I used the `stratify=y` to make sure that the initial distribution of survived and dead passengers is respected in both `y_train` and `y_val`.","64a6f27c":"## 5.1 Random Forest Classifier\nInspired from [Predicting the Survival of Titanic Passengers](https:\/\/towardsdatascience.com\/predicting-the-survival-of-titanic-passengers-30870ccc7e8) by __Niklas Donges__.","23ba629c":"__Important definitions__:\n- _Precision_: When our classifier predicts yes the person survived, how often is it actually correct?\n- _Recall_: When it is indeed yes the person survived, how often does our classifier predict correctly?\n- _F1-score_: Weighted average between precision and recall. Useful when dealing with unbalanced samples.\n- _Accuracy_:  This is only accurate if the model is balanced. It will give inaccurate results if there is a class imbalance.","155af510":"__Observations__:\n1. The column __Survived__ seems to have 418 missing values. This is an artefact of merging the train and the test data. Remember, the test data didn't come with this column on purpose, since this is what we are actually trying to predict.\n2. The column __Age__ has 263 missing values. Quite a lot, but still we will manage to impute them nicely. Be patient!\n3. The column __Fare__ has one missing value. This will be easy to fix.\n4. The column __Cabin__ has 1014 missing values or 77.46%. This column has so many missing values that I personally think it's not worth recovering it. I have read a plethora of interesting approaches where the __Deck__ column is introduced and they impute it by grouping passengers with same tickets and imputing them by random sampling based on the distribution of the existing decks. However, I think that with so few data the statistics are insignificant.\n5. The column __Embarked__ has 2 missing values. Easy-peasy ;)","1e9881a3":"### 2.2.3 Passengers with zero _Fare_","2e5c9792":"__Observations__:\n1. Having relatively small depth guarantees that the individual trees will not suffer from overfitting the train data. For each test-observation our classifier is fed with, each decision tree will give a prediction based on the trained data, and then the random forest will aggregate those predictions from the individual trees and it will deliver a prediction which will be more robust and less biased. That is why random forest is considered an ensemble learning technique. There is no authoritarian tree here deciding on everyone's behalf, they all vote, and the democratic result is supposed to be the most accurate prediction.","8988f57d":"__Observations__:\n1. The majority of the tickets (54.5%) are single tickets. As we had noticed previously, passengers who were alone had less chances to survive irrespecitve of their class and this is confirmed by the third plot.\n2. Passengers with very large families (above 5 members) had less chances to survive. Most likely they were 3rd class passengers living in lower decks far ways from the lifeboats. Moreover, statistically speaking, in a family of 11 with an overall survival rate being 38%, some will definitely die. \n3. One may claim that this feature contains the same information as _FamilySize_. They have similarities indeed, but there are slight differences. It could very well be that some passengers had the same ticket but they were not family. Consider friends, nannies, maids, etc.. Recall Mrs Stone and Miss Icard from section 2.3. So, we will maintain this feature.","e55b3808":"## 8.1 Evaluate predictions against some known passengers\nAs we explained in section 3.1.1 and after some investigation nobody from the Sage and Goodwin families survived. Moreover, Roderick Chisholm had survived, while Joseph Ismay had died. Let's see how well our random forest captured reality.","02649e00":"## 3.5 _FareCategory_ from _Fare_ and _TicketFrequency_","445570b5":"## 1.2 Data Inspection","bf34c235":"## 3.10 Data inspection after EDA and FE","94986087":"## 1.1 Load and merge train and test datasets","11a7d9ef":"## 1.6 Explore _Parch_","a1bb13db":"<a id='section2'><\/a>\n# 2. Data Cleaning and further EDA\nBear in mind that we will mainly try to check how each feature affects the target variable `Survived`. Since, this label exists only for the train data, the analysis mainly focuses on the train data. However, if you see me making comparisons or plotting features against each other but not with the `Survived` then I consider both train and test data to get as many insight as possible. Considering the test data of a problem in the EDA is not cheating. Touching any aspect of the test data during the training of your classifier, that is cheating.","ce7aca79":"__Observations__:\n1. A very good classifier is one that keeps the off-diagonal elements of the confusion matrix as low as possible (minimize false positive and false negatives). They can never be zero. If so, then you have probably overfitted your data.\n2. ROC value is improved which is a sign that the tuning was worth the wait.\n3. Tuning improved significantly the prediction of deaths. This makes sense since our target variable is slightly imbalanced; 62% of the passengers died while only 38% survived. Therefore, our classifer, while being fitted in such data, it learned well how to predict more deaths than survivals.","d582025d":"## 1.3 Survival rate info","daa8db79":"<a id='section3'><\/a>\n# 3. Feature Engineering (FE)\nHere we will modify existing features, but we will also introduce new ones that may be proven very useful for our classifier. Bear in mind that sometimes we may create some artificial features only for the sake of unfolding information and gaining domain knowledge. Therefore, don't be surprised if some of them sink into oblivion.","d2810d21":"__Observations__:\n\n1. Irrespective of which class the passenger belonged to, if they were alone they had less chances to survive.\n2. Hmm, if you were a single man on the Titanic, you had only 15% chances to survive. Scary. On the other hand, women who were alone had only 2% less chances to survive compared to women with company.","8228ebe7":"## 8.2 Analyze KDE of predictions in _Sex_\nHaving guaranteed that female\/male population is similar for the two datasets, and also that there is nothing odd with the predicted survival rate of the test data, let us take a deeper look in the distribution of survival and death rate among women and men. ","0a36e224":"## 3.4 Explore _Ticket_\nMy analysis will be based upon [Titanic Ticket-only study](https:\/\/www.kaggle.com\/pliptor\/titanic-ticket-only-study\/output) from Oscar Takeshita.","ca413f5d":"## 3.1 Create _FamilySize_ from _SibSp_ and _Parch_","217d4701":"## 2.2 Fixing 1 missing value in _Fare_\n### 2.2.1 Study distribution","cb066eb2":"### 5.1.1 Cross validation and OOB on Untuned Random Forest\nWe will use a `kFold` cross validation where the train set will be split in k bins and each one of them will serve as an evaluation bin while the rest are used for training. ","10233db8":"__Observations:__\n\n1. For 1st class passengers, the longer their names, the higher their chances to survive. One could claim that long names are a sign of nobility. This is only speculation. No matter what this feature seems to be a good predictor of survival. \n2. For 1st and 3rd class men, the longer their names, the higher their chances to survive.\n3. 1st and 2nd class women have extremely high survival rate, regardless of their name's length.\n4. This feature clearly has an impact on the survival rate so it will be proven very useful for our classifier.","e58c46e5":"__Observtions__:\n1. We successfully converted the extremely posively skewed distribution of _Fare_ into a version that is slightly negatively skewed (-0.154) but within the boundaries of normality (-1,1). That is great news. \n2. Both in PDF and CDF we can observe that logFare is nearly Gaussian. \n3. The StandardScaler respected the extremely positive skewness of the Fare signal. Therefore, I decided to continue with the _logFare_","0c6e3898":"<a id='section9'><\/a>\n# 9. Room for improvement\n1. Try different binning of features. Or even try not to bin _Age_ or _Fare_. \n2. Use your imagination to engineer and cook up some more interesting features.\n3. Make sure that your model generalizes.\n4. Try to identify better metrics to evaluate your classifier. I was mainly focusing on the confusion matrix and the ROC results. In addition, I was checking the distribution of survival-rate in age for train and test. Maybe there is something even better. Please comment if you have something in mind.\n5. Try different classifiers. It could be that with this combination of features and tuning of the random forest's hyperparameters I have reached some saturation of accuracy.\n6. Perform tuning of random forest in a greater hyperparameter space (GPU utilization would be awesome for this). \n7. Try the VotingClassifier which belongs in the ensemble family of sklearn. Random Forest utilizies multiple weak Decision Trees. VotingClassifier can combine many classifiers of different nature (e.g.: AdaBoost, Gradient Boosting, SVM, NaiveBayes, etc.) and democratically select the one scoring highest for some defined metric. It looks promising.","cfe9c51c":"## 2.4 Correlation matrix of raw features","c1ba9bf1":"### 3.5.2 _FareCategory_ from _logFare_\nI selected a number of bins high enough to both capture all the different spikes of the original signal and maitain the information gain. Inspired by [Titanic - Advanced Feature Engineering Tutorial](https:\/\/www.kaggle.com\/gunesevitan\/titanic-advanced-feature-engineering-tutorial?scriptVersionId=27280410) from Gunes I selected 13 bins. I am not quite sure what's the best practice here. Should one select a large number of bins to maintain the feature's information but at the cost of ending up with lower linear correlation with the target variable, or should one create fewer bins of greater size to strengthen the linear correlation with the target variable, but at the cost of losing information? Please enlighten me if you know the answer.","223765f8":"__ATTENTION__: when the pearson correlation coefficient of two features is close to zero, that doesn't mean that there is no relationship between the two features; it only means that there is no linear relationship between them. A classifier may reveale this non-linear relationship later on. For instance, _FamilySize_ and _Survived_ seem to have a very low correlation coefficient close to 0.02. One would even think to discard _FamilySize_ since it doesn't contain any significant information to the classifier. Surprisingly, _FamilySize_ is ranked 6-7th most important feature for the random forest as shown in section 5.1.1.\n","df869a4f":"## 3.2 Explore _Age_\nI selected a number of bins that guarantees there is a representative group for children who had the highest survival rate according to __women and children first__ doctrine. Inspired by [Titanic - Advanced Feature Engineering Tutorial](https:\/\/www.kaggle.com\/gunesevitan\/titanic-advanced-feature-engineering-tutorial?scriptVersionId=27280410) from Gunes I selected 10 bins.","47cd02d3":"## 1.5 Explore _SibSp_","77b70aec":"## 3.8 Label Encoding of ordinal features\nWe will convert ordinal features into numerical ones. An ordinal feature is one whose sub-categories entail the notion of oder. For instance, `NameLength` is binned into 4 groups, with each group reflecting passengers with longer names than the previous group. So, when we label-encode it the classifier will correctly assume that 0 < 1 < 2 < 3. The same holds for `FareCategory` and `AgeBand`.","018f79eb":"# Introduction\n\nMy intention when I joined the Titanic competition was to practice my machine learning and data science skills. I am not a senior in the field but I am not a rookie either. I wanted to follow a structured pipeline from Exploratory Data Anaysis and Data Cleaning to Feature Engineering and Classification. If your intention is to score 1.0, then you should probably stop reading here. And remember, there is no way a classifier can predict with 100% accuracy. That would mean a diagonal confusion matrix; that's insanity. Having said this, I will not waste your time explaining all the details about the dataset and the competition itself. Many great Kagglers with beautifully written kernels have already covered this. A great inspiration for me was the [Titanic - Advanced Feature Engineering Tutorial](https:\/\/www.kaggle.com\/gunesevitan\/titanic-advanced-feature-engineering-tutorial) notebook from __G\u00fcne\u015f Evitan__. Big thanks G\u00fcne\u015f. There have also been other Kagglers the current notebook is based upon. I try to acknowledge them here and there throughout this kernel, but please, should I have forgotten someone do not hesitate to leave a comment with a link to their kernels. I am only trying to transfer the knowledge I acquired throughout this journey. I'm not trying to get credits for someone else's work.\n\nAlthough I read many interesting kernels where they introduce quite some advanced and clever features, I decided to keep this kernel simple and focus mainly in the structure of a Machine Learning classification problem. I think that if we massage our data too much then we end up overfitting them unavoidably.\n\nThere is definitely plenty of space for improvement. I am looking forward to some cool recommendations.\n\nIf there is something you don't understand or something you believe I have completely missed or misunderstood please poke me with a comment. After all, we are all here to learn.\n\n__Notebook Execution Time__: approximately 5 min (It's slow because a 5-fold cross-validation is executed 6 times in total)\n\n__Notebook leaderboard score__: 0.78468 (gini), 0.78468 (entropy)\n\n\n## Contents of current notebook\n1. [Exploratory Data Analysis (EDA)](#section1)\n2. [Data Cleaning and further EDA](#section2)\n3. [Feature Engineering (FA)](#section3)\n4. [Train\/test split of data](#section4)\n5. [Classification](#section5)\n6. [Make Predictions on True Test Dataset](#section6)\n7. [Submit solution](#section7)\n8. [Analyze and evaluate predictions](#section8)\n9. [Room for improvement](#section9)\n9. [Cheers](#section10)","58217eff":"### 5.1.2 Hyperparameter Tuning with Random Search\nThe hyperparameter space I define here is quite arbitrary. One could define different ranges and perhaps end up with more accurate predictions. The results from the random search cross-validation can never outperform the grid search cross-validation. The only reason one would select the former would be to save time. One could also take the best parameters from the random search, create new ranges around these best parameters, and perform a more extensive grid search cross-validation. I will just remind you one thing; always weight the impact of tuning on the entire pipeline of your project. E.g.: is it worth waiting 2 hours for a thorough hyperparameter tuning to improve the classification accuracy by 0.1%. It really depends on the nature of the problem you are trying to solve. \n\n__Definitions__:\n1. _gini impurity_: it measures the frequency at which any element of the dataset will be mislabelled when it is randomly labeled\n2. _entropy_: is a measure of information that indicates the disorder of the features with the target.\n\nSince the entropy involves the computationally more expensive logarithm, I recommend you to always prefer gini over entropy at the cost of slightly worse performance. Check [Decision Trees: Gini vs Entropy](https:\/\/quantdare.com\/decision-trees-gini-vs-entropy\/) from Pablo Anzar.","c5ddadf8":"### 3.5.1 Individual Fare\nDivide the total Fare with the frequency of appearance of the ticket, to find the individual fare for each passenger. This is important cause it may appear that some passenger paid a lot, when in reality he\/she was under the same ticket with 10 more people.","8c6121de":"__Observations__:\n1. For all titles, 1st class has highest survival rate. \n2. Royalties and Officers appear only on the 1st class with some officers being outliers in the 3rd class and who actually died. Probably he stayed in lower decks of the titanic.\n3. 3rd class is consistently the one with lowest chances of survival regardless of title.\n4. Mr, which is a title for men, has the lowest survival rate amongst all titles, regardless of class. It makes sense according to the __women and children first__ doctrine.\n5. 1st class men had almost double chances to survive compared to men from 2nd and 3rd class. Money counts.\n6. Master title, who based on its average age represent children, has the highest chances of survival both for 1st and 2nd class. Unfortunately this is not the trend for 3rd class children.\n7. Each Title has a different mean age, therefore it makes sense to fill the missing values of the _Age_ column by grouping our data on _Title_ and _Pclass_.\n8. Mean and median have very close values, so I decided to go for the mean. However, if they differred significantly I would definitely go for median which is a statistic more robust to outliers.\n9. I have decided to keep this feature and let my classifier decide whether it's important or not.","ad4547d0":"### 2.1.2 Monitor survival rate among gender and classes","6c689487":"## 1.4 _Sex_ and _Pclass_ impact on _Survived_","1be0b8cd":"<a id='section7'><\/a>\n# 7. Submit solution","42cfe1aa":"<a id='section5'><\/a>\n# 5. Classification\n\nDifferent classifiers could be exploited here from the `scikit-learn` library:\n\n1. DecisionTreeClassifier \n2. LogisticRegression\n3. KNeighborsClassifier \n4. RandomForestClassifier \n5. AdaBoostClassifier \n6. GradientBoostingClassifier \n7. Support Vector Machines \n8. MLPClassifier\n9. GaussianNB\netc.\n\nI decided to explore only the __Random Forest__ classifier for this problem. Random forests in general acieve lower variance than individual Decision Trees, so that is why I decided to give it a shot. You are free to try all of them and pick the one that gives you the best score. One thing worthmentioning here is the evaluation metric for the performance of your classifier. Make sure you select them carefully. I got disappointed so many times scoring well for the validation data but way lower for the real test data on the leaderboard score.","9d02ae68":"## 3.3 Explore _Name_","991c4437":"__Observations__:\n1. Lonely men had almost 60% less chances to survive than women.\n2. Men with one sibling or spouce had almost double chances to survive compared to the loners. I guess having someone to take care of you during a disaster of this magnitude is of great importance.\n3. Women seem to have similar chances of survival with or without siblings or spouces. However, when this number exceeds two, their chances of survival drops. My guess is that this is associated with the class as well. 3rd class passengers tend to have greater families.","cb64ff64":"### 5.1.4 Compare learning curves of tuned and untuned RF\nInspired by [Plotting Learning Curves](https:\/\/scikit-learn.org\/stable\/auto_examples\/model_selection\/plot_learning_curve.html#sphx-glr-auto-examples-model-selection-plot-learning-curve-py). I compare the performance of the default RF with the gini-tuned RF using accuracy as the defined metric.\n\n__Definitions__:\n1. Train learning curve: learning curve calculated from the training dataset that gives an idea of how well the model is learning (see [here](https:\/\/machinelearningmastery.com\/learning-curves-for-diagnosing-machine-learning-model-performance\/)).\n2. Validation learning curve: learning curve calculated from a hold-out validation dataset that gives an idea of how well the model is generalizing (see [here](https:\/\/machinelearningmastery.com\/learning-curves-for-diagnosing-machine-learning-model-performance\/)).\n3. From these plots we can detect how much we can benefit from adding more training data to our classifier.\n","a80d5906":"__Observations__:\n1. There is a negative correlation between _Pclass_ and _Survived_. As we observed above the higher the class, which in this case by higher we mean 1-->2-->3, the lower the survival rate.\n2. There is also a strong negative correlation between _Pclass_ and _Fare_. This makes sense, 3rd class tickets were cheaper than 1st class ones.\n3. There is a slight positive correlation between _Fare_ and _Survived_. As we will show later on, the more a passenger paid for their tickets the higher the chances to survive.","279b85b3":"### 2.2.2 Fill missing value","10bbc468":"<a id='section10'><\/a>\n# 10. Cheers\nThanks for taking the time to go through my notebook. I hope you enjoyed it.\n\nI look forward to everyone's comments and feedback. And if you have any cool tips to help me score a bit higher let me know. The maximum I have achieved so far is 0.79425 which ranked me top 6%. However, I had made some mistake and when I corrected it, I landed back to lower scores and never recovered since then. The intention of this tutorial is educational. All I wanted was to help new kagglers with some coherent structure and methodology. I hope I made it.\n\nBest of luck with the titanic adventure!","b5e13ac1":"# Import all essential libraries","c59d092f":"## 3.11 Correlation Matrix","bfbdb1ee":"## 3.11 Save feature names\nWe will need this when we calculate the importance of each feature later on.","6fa313f3":"### 2.1.3 Extract _Title_ from _Name_\nAlthough it seems awkward to calculate this now, you need to understand that certain features need to be created to unfold hidden information and then we will most likely discard them, or not, it depends on what we will discover.","ddbe473b":"__Observations__:\n1. Up to family-size=4, the survival rate increases with increasing family size irrespective of gender or class.\n2. Sadly, there are two families for which nobody survived. These families consist of 8 and 11 members respectively. We should be very careful before drawing any conclusions yet though. What we are seeing here corresponds to the train data  for which we have labels for the _Survived_ column. As we see below there are still some members for both families for which we don't know whether they are survivors or victims.\n3. Most kagglers drop _SibSp_ and _Parch_ and keep _FamilySize_. However, I think that by doing this we lose one degree of freedom which could be proven very useful for our classifier. Poke me if you disagree. Despite this, I will go with the flow and keep _FamilySize_. ","6d13d108":"## 8.3 Analyze histogram of predictions in _Age_ per _Sex_ per _Pclass_","d06f0a72":"__Observation__:\n1. You can see how different the mean and median are for each class. When these two statistics differ significantly, always trust the median which is more robust to outliers.","eb5c3657":"## 2.3 Fixing 2 missing values in _Embarked_\nSince there are only two missing values it's worth doing some research about these passengers and find out exactly where they embarked from. The two passengers are Miss Amelie Icard, and Mrs George Nelson Stone. Based on [this source](https:\/\/www.encyclopedia-titanica.org\/titanic-survivor\/amelia-icard.html) I found the following about Amelie\n```\nMiss Amelie Icard, 38, was born in Vaucluse, France, where her father Marc Icard lived at Mafs \u00e1 Murs (?). She boarded the Titanic at Southampton as maid to Mrs George Nelson Stone. She travelled on Mrs Stone's ticket (#113572). Personal Maid to Mrs Martha Evelyn Stone\n\nMrs Stone and Miss Icard were rescued by the Carpathia in lifeboat 6. \n```\nAll the information agree with our dataset. And the answer to our question is that they both Embarked from Southampton's port. I could have substituted these missing values with the most frequent values in `Embarked` column, which happens to be _S_. If there were more missing values I would definitely go for it. Something interesting revealed here is that Miss Icard travelled on Mrs Stone's ticket. Therefore, the `Fare` column is associated with the Ticket number it's not fare per passenger. So, many passengers could be under the same ticket. This is worth-rememebering as we may need to modify the `Fare` column or create a new one out of it, more informative for our classifier.","32336cb2":"__Observations__:\n1. 61.6% of the passengers died. This is the result from the training data for which we have labels for the __Survived__ column. If our classifier is good, we expect to to have similar statistics in the predictions of the test dataset.\n2. The problem we have here is an __Imbalanced Classification Problem__: the distribution of examples across the classes is not equal. That is because we have more deaths than survivals. But the imbalance is only slight and not severe. In any case, no matter what, our classifier in the end will favor deaths over survivals. So, don't be surprised.","f16c2b4e":"__Observations:__\n1. The survival rate for women among the three classes looks very similar for both train and test data.\n2. The survival rate for men in the test data is very pessimistic compared to its train counterpart. This makes sense, given that we trained our random forest with a dataset that was biased against men.","7e3bf090":"__Observations__:\n1. _Fare_ is extremely positively skewed. Therefore, _Fare_ is not normally distributed. This doesn't surprise me since the majority of passengers had bought cheap tickets. This is also confirmed by the extremely high skewness (4.37) which exceeds the acceptable boundaries of normality (-1,1).\n2. From the KDE on the right, we confirm that passengers who paid less than 100 had consistently less chances to survive. Check the belly between the two curves representing death-rate in red and survival-rate in green.","8ebafa23":"__Observations__:\n1. The importance generated from the random forest's attribute and the importance from the shap values differ. In all honesty, I don't know which one should I trust more, the importance of the features from the classifier or their shap values? Please enlighten me if you happen to know. Moreover, should I calculate the shap values on the train or on the validation data like I do?\n2. The default untuned Random Forest already has decent performance.\n3. From the classification report, one can notice, that the untuned random forest has better score in all metrics when it comes to predicting who will die. That makes sense because we have more passengers who died than survived in our training data. So, there is some imbalance, and therefore the classifier is biased to favor, or score better when it comes at predicting deaths.\n4. SHAP values describe the influence of the particular feature to the final prediction. The importance of each feature seems to be slightl different than what we estimated using the random forest's attribute. They both agree though on which features are the least important. In a feature reduction process, that is what matters the most.","7bf9e225":"### 2.1.1 Test normality","761aae43":"### 3.5.2 _logFare_ and _whiteFare_ from _Fare_\nMost ML algorithms deal better with normally distributed variables (Gaussian). I therefore transform _Fare_ by taking its logarithm. Before doing that I incremented the 0 Fares by some positive value, to avoid getting -Inf in the logarithm. I also compare the whitening process of log and StandardScaler.","aeaf1861":"__Observations__:\n1. 468 out of 577 men died; that is 81%. \n2. 81 out of 314 women died; that is 26%. \n3. Regardless of gender, the 1st class passengers had greater chances to survive, while 3rd class passengers had really low survival rate. The narrow confidence intervals are a sign we can trust the results. ","4f76b802":"## 3.7 Drop unnecessary data","b5cda43f":"## 2.1 Fix _Age_ \nThis is a continuous variable so we can study its distribution first to gain some insights. For the skewness I got inspired by [A Comprehensive Guide to Titanic Machine Learning](https:\/\/www.kaggle.com\/eraaz1\/a-comprehensive-guide-to-titanic-machine-learning) from Faysal.","202acddc":"__Observations__:\n1. In general, children had greater survival rate among all three classes. I could have modified the threshold here. In 1912 a 16 years old passenger was definitely not considered a child. \n2. Although there is a big gap in survival rate of women and men, the survival rate for children is very close. However, although this is great news for young boys, it seems that the survival rate of girls is worse than adult women.\n3. I cannot see any strong linear relationship of _AgeBand_ with _Survived_, but I will let the classifier decide whether it's an important feature or not. Be aware that sometimes there exist non-linear relationships between two variables that cannot be detected with naked eye. Random forests are verey good in identifying such relationships.","bfccf279":"__Observations:__\n\n1. I predicted that Roderick Chisholm died when in reality he had survived (misclassification).\n2. I correctly predicted that Joseph Ismay died.\n3. I correctly predicted that the 4 remaining members of the Sage family (11 members in total) died.\n4. I correctly predicted that the 2 remaining members of the Goodwin family (6 members in total) died.\n5. From the upper pie charts we observe that females and males have similar distribution on train and test data.\n6. From the lower pie charts we can conclude that the survival rate for the train and test data is very similar which is a good sign for the accuracy of our random forest. However, this can be misleading, as we could have misclassified many passengers and still have similar survival rate. Let's put it like this; if the survival rate for the two datasets was completely different then we would know our classifier was of poor quality. Having no indication of such thing from these pie charts allows us to hope that we did a decent job ;)","7192d4a2":"__Observations__:\n1. The majority of passengers embarked on the Titanic at the Southampton port (70%). \n2. Passengers embarked at Cherbourg had the highest survival rate regardless of their gender.\n3. The survival rate vs embarkation is similar for men and women,with men having consistently 50% less chances to survive for all ports.\n4. Men emabrked from Queenstown had the lowest chances to survive.\n5. There is no significant pattern here. However, I will not drop this feature. I will allow my classifier to decide what is important and what is not.","3ffc73e3":"Let's see who travelled for free on the titanic. A name standing out from the list below is that of Joseph Ismay's. On wikipedia they state the following about him \n```\nJoseph Bruce Ismay was an English businessman who served as chairman and managing director of the White Star Line. In 1912, he came to international attention as the highest-ranking White Star official to survive the sinking of the company's new flagship RMS Titanic, for which he was subject to severe criticism.\n```\nJoseph belongs to the test data which for this competetion is unlabeled. Let's see if we our classifier later on can predict this man's survival. My guess is that it will not cause it seems to be a very lucky incident. He could have forced some woman or child out of the lifeboat. \n\nAnother name standing out is that of Roderick Chisholm. According to wikipedia:\n```\nRoderick Chisholm was a Scottish shipbuilder. He co-designed the RMS Titanic with Thomas Andrews. He died when the Titanic sank on her maiden voyage.\n```\nSo, it makes sense that this person didn't pay for his ticket. What is confusing me though is that, unlike Joseph, Roderick died. And he also belongs to our unlabeled test data. I am so curious to see what my classifier will predict for those two. It doesn't seem very trivial.","d19d5a31":"<a id='section1'><\/a>\n# 1. Exploratory Data Analysis (EDA)","397f9fbb":"<a id='section6'><\/a>\n# 6. Make Predictions on True Test Dataset \nSo far, I have idientified the optimal random forest after tuning the hyperparameters and evaluated its performance on a subset of the train data, called validation data. Now, we will fuse the train and validation data back to the complete __X_train__ dataset which is the advanced version of the original train data`\/kaggle\/input\/titanic\/train.csv`. Then I train the optimal random forest on the entire train set. Finally I make predictions on the unseen test data, which is the advanced version of the unlabeled `\/kaggle\/input\/titanic\/test.csv`.","2f44e1b4":"### 4.2 Feature scaling \nIn this problem we have already converted all our features into numerical values with similar scale. However, for the sake of completeness I make sure that they are all Z-transformed, which means they have zero mean and unit variance. The same scaling should be applied on the validation data for meaningful results according to [1.17.8. Tips on Practical Use](https:\/\/scikit-learn.org\/stable\/modules\/neural_networks_supervised.html#regression).","b0643b87":"__Observations__:\n1. As a general rule, the more a passenger had paid, the greater their chances to survive. This rule holds regardless of gender. However, as with all the other features, women had greater chances for cheap, average or expensive tickets compared to men.\n2. For some strange reason, 2nd class passengers seem to had less chances to survive when they paid more. This may seem irrational, but there is one dimension we are neglecting here and that is how the passengers were distributed among the different decks of the Titanic, and how far away they were from the lifeboats when Titanic started sinking. By the way, if somebody knows how to exploit _Cabin_ please leave a comment. I am not convinced by converting it into a binary _HasCabin_. Moreover, any good solution on how to retrieve Decks would be much appriciated. I am guessing that it must be a comination of the _Ticket_ and _Cabin_ features but not quite sure how exactly to do it. ","662e5519":"__Observations__:\n1. From summary statistics we see that the skewness is 0.40 which is within the normality range (-0.5, 0.5). Therefore, although the distribution of Age is right-skewed (positive skewness), we can claim that it's almost normally distributed.\n2. From the right ECDF we see that average age of passengers who died is is slightly greater than the one who survived. \n3. From the right  ECDF we observe that for passengers below 20, it was more likely to survive than die.","b59c9596":"### 5.1.3 Evaluation of tuned random forest","1faae693":"<a id='section4'><\/a>\n# 4. Train\/test split of data\nAt this point we need to be very careful. Our test data are not labeled. So, they cannot support us in evaluating our classifier. There are two approaches: \n\n1) Work with the entire train dataset and use cross validation predictions for evaluation\n\n2) Split the training data into train and validation. \n\nI will follow the second approach.","9d076bc0":"<a id='section8'><\/a>\n# 8. Analyze and evaluate predictions\nAs far as I can recall this is a chapter that was missing from many kernels. Most kagglers stopped after the prediction step. I decided to go one step beyond and try to analyze the predictions and compare their distribution against the train data.","f6315f6c":"### 2.1.4 Filling 263 NaN values in _Age_ based on _Pclass_ and _Title_"}}