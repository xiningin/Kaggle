{"cell_type":{"c81ab38f":"code","d1e6af39":"code","d9b5f230":"code","8cac0234":"code","471f9a24":"code","61172492":"code","37ea74ca":"code","64002e50":"code","20a3e889":"code","414f38b5":"code","cee8ce0c":"code","aef2fdc4":"code","18a7e362":"code","35750fba":"code","fd2c8451":"code","d6bef5b5":"code","2a657086":"code","52447c75":"code","1167ac79":"markdown","b23ea2af":"markdown","589dfadc":"markdown","0a9684ba":"markdown","f055a390":"markdown","6f04019b":"markdown","7bfa5ce9":"markdown","b5e85b4e":"markdown","299714a0":"markdown","69372e5f":"markdown","320c6e1a":"markdown","c3069e52":"markdown","8f3cca25":"markdown"},"source":{"c81ab38f":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\n# import os\n# for dirname, _, filenames in os.walk('\/kaggle\/input'):\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","d1e6af39":"import tensorflow.compat.v1 as tf\nfrom matplotlib import pyplot as plt\nfrom keras.preprocessing.image import ImageDataGenerator, load_img\n","d9b5f230":"train_data_dir = \"..\/input\/emotion-detection-fer\/train\/\"\ntest_data_dir = \"..\/input\/emotion-detection-fer\/test\/\"\n\nimg_rows, img_cols = 48, 48 # Taken for granted\n\n# Taken for granted\ntrain_datagen = ImageDataGenerator(\n    rescale=1.\/255,     # Our image pixel values are between 0-255 (RGB). Therefore, to make it easier for our model to process, we normalize by 255.\n    zoom_range=0.3,     \n    horizontal_flip=True  # TODO: Don't know why we would want to randomly flip here unless we're trying to generalize better.\n)\n\ntrain_set = train_datagen.flow_from_directory(\n  directory   = train_data_dir,\n  color_mode  = 'grayscale',\n  target_size = (img_rows, img_cols),\n  class_mode  = 'categorical',\n  shuffle     = True\n)","8cac0234":"test_datagen = ImageDataGenerator(\n    rescale=1.\/255,\n    zoom_range=0.3,\n    horizontal_flip=True\n)\n\ntest_set = test_datagen.flow_from_directory(\n    directory    = test_data_dir,\n    color_mode   = 'grayscale',\n    target_size  = (img_rows, img_cols),\n    class_mode   = 'categorical',\n    shuffle      = True\n)","471f9a24":"train_set.class_indices","61172492":"def count_exp(path, set_):\n    dict_ = {}\n    for expression in os.listdir(path):\n        dir_ = path + expression\n        dict_[expression] = len(os.listdir(dir_))\n    df = pd.DataFrame(dict_, index=[set_])\n    return df\n\ntrain_count = count_exp(train_data_dir, 'train')\ntest_count  = count_exp(test_data_dir, 'test')\n\ntrain_count","37ea74ca":"test_count","64002e50":"train_count.transpose().plot(kind = 'bar')","20a3e889":"# train_count.map each row to a sum\ntrain_num = train_count.values.sum()\ntest_num = test_count.values.sum()","414f38b5":"train_count.apply(lambda x: x*100.\/train_num, axis=1)","cee8ce0c":"test_count.apply(lambda x: x*100.0\/test_num, axis=1)","aef2fdc4":"import random\n\ndef rescale_and_plot_images(image_files):\n    for idx, img_path in enumerate(image_files):\n        plt.subplot(3, 3, idx + 1)\n        img = plt.imread(img_path)\n        plt.imshow(img, cmap = 'gray')\n\ndef plot_images(path, emotion):\n    image_paths = []\n    emotion_path = os.path.join(path, emotion)\n    image_paths = [os.path.join(emotion_path, img_png) for img_png in random.sample(os.listdir(emotion_path), 3)]\n    \n    plt.figure(figsize = (10, 10))\n    rescale_and_plot_images(image_paths)\n    \n\nplot_images(train_data_dir, 'happy')\n","18a7e362":"plot_images(train_data_dir, 'fearful')\n","35750fba":"plot_images(train_data_dir, 'neutral')\n","fd2c8451":"plot_images(train_data_dir, 'angry')\n","d6bef5b5":"plot_images(train_data_dir, 'disgusted')\n","2a657086":"plot_images(train_data_dir, 'sad')\n","52447c75":"plot_images(train_data_dir, 'surprised')\n","1167ac79":"### What are the different classes in the training set?","b23ea2af":"This notebook aims to use convolutional neural networks to detect emotion in facial images. The tutorial referenced for this is: https:\/\/exploreai.org\/p\/tensorflow-cnn\n\nData can be found at: https:\/\/www.kaggle.com\/ananthu017\/emotion-detection-fer","589dfadc":"### Viewing some samples in the dataset","0a9684ba":"# Emotion Detection Using CNNs","f055a390":"## Model Building","6f04019b":"## Imports","7bfa5ce9":"### Percentages of each class in each set","b5e85b4e":"### Counts of each class","299714a0":"## Load the Dataset\n\nThis was taken from: https:\/\/github.com\/shekkizh\/TensorflowProjects\/blob\/master\/EmotionDetection\/EmotionDetectorUtils.py[](http:\/\/)","69372e5f":"### Load training dataset. \n\nIt's taken for granted (prior knowledge) that our data is in the shape 48x48. We will be using the Keras ImageDataGenerator to rescale our image. \n\nRescaling - Pixels are in RGB values (0-255) and to make it more convenient for the model to learn, we normalize them.\n\nZoom - Randomly zoom in or zoom out for images\n\nHorizontal Flip - Randomly flip input horizontally. Asssuming this is for better generalization. (what if we set vertical_flip=True) as well?","320c6e1a":"Conclusion is that distributions between train and test sets are relatively the same.","c3069e52":"## Exploratory Data Analysis","8f3cca25":"The samples are images and so we will use pyplot to view the image."}}