{"cell_type":{"e10e6260":"code","5d75cd9b":"code","9e5b8b5f":"code","ebf24b3b":"code","a175dcad":"code","67afa8f2":"code","57c9e827":"code","609d5880":"code","b7e6953a":"code","45a6d07e":"code","74f58552":"code","5a02a710":"code","c5f38598":"code","27d4d340":"code","83ecae58":"code","14a755d2":"code","d4585d82":"code","de85bc88":"code","f3577965":"code","65b13d6e":"code","c8d03d31":"code","4ba7ef35":"code","1cf0f892":"code","2d247efc":"code","382b786d":"code","06e5612e":"code","2abe5ec2":"code","679f5545":"code","e6260ed2":"code","a09f91c3":"code","b131a9ab":"code","8c8702b0":"markdown","c23810fd":"markdown","defe65e9":"markdown","d41435f2":"markdown","e6deb4d3":"markdown","49696172":"markdown","b30db380":"markdown","374a3264":"markdown","a4dc8311":"markdown","e61b0613":"markdown","acaa4788":"markdown","251516c0":"markdown","d0cadf73":"markdown","da733c67":"markdown","adc1ba37":"markdown","022499bb":"markdown","f1138561":"markdown"},"source":{"e10e6260":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","5d75cd9b":"!pwd\n","9e5b8b5f":"os.chdir('..')\nos.listdir()","ebf24b3b":"import os, shutil\n\noriginal_dataset_dir = 'input\/kus-balik-dataset\/TrainDataset\/TrainDataset'\n\nbase_dir = 'kus_ve_balik_sinifi'\nos.mkdir(base_dir)\n\ntrain_dir = os.path.join(base_dir, 'train')\nos.mkdir(train_dir)\nvalidation_dir = os.path.join(base_dir, 'validation')\nos.mkdir(validation_dir)\ntest_dir = os.path.join(base_dir, 'test')\nos.mkdir(test_dir)\n\ntrain_kuslar_dir = os.path.join(train_dir, 'kuslar')\nos.mkdir(train_kuslar_dir)\n\ntrain_baliklar_dir = os.path.join(train_dir, 'baliklar')\nos.mkdir(train_baliklar_dir)\n\nvalidation_kuslar_dir = os.path.join(validation_dir, 'kuslar')\nos.mkdir(validation_kuslar_dir)\n\nvalidation_baliklar_dir = os.path.join(validation_dir, 'baliklar')\nos.mkdir(validation_baliklar_dir)\n\ntest_kuslar_dir = os.path.join(test_dir, 'kuslar')\nos.mkdir(test_kuslar_dir)\n\ntest_baliklar_dir = os.path.join(test_dir, 'baliklar')\nos.mkdir(test_baliklar_dir)\n\nfnames = ['Kus.{}.jpg'.format(i) for i in range(100)]\nfor fname in fnames:\n    src = os.path.join(original_dataset_dir, fname)\n    dst = os.path.join(train_kuslar_dir, fname)\n    shutil.copyfile(src, dst)\n    \nfnames = ['Kus.{}.jpg'.format(i) for i in range(100, 125)]\nfor fname in fnames:\n    src = os.path.join(original_dataset_dir,  fname)\n    dst = os.path.join(validation_kuslar_dir, fname)\n    shutil.copyfile(src, dst)\n    \nfnames = ['Kus.{}.jpg'.format(i) for i in range(125, 150)]\nfor fname in fnames:\n    src = os.path.join(original_dataset_dir, fname)\n    dst = os.path.join(test_kuslar_dir, fname)\n    shutil.copyfile(src, dst)\n    \nfnames = ['Balik.{}.jpg'.format(i) for i in range(100)]\nfor fname in fnames:\n    src = os.path.join(original_dataset_dir, fname)\n    dst = os.path.join(train_baliklar_dir, fname)\n    shutil.copyfile(src, dst)\n    \nfnames = ['Balik.{}.jpg'.format(i) for i in range(100, 125)]\nfor fname in fnames:\n    src = os.path.join(original_dataset_dir, fname)\n    dst = os.path.join(validation_baliklar_dir, fname)\n    shutil.copyfile(src, dst)\n    \nfnames = ['Balik.{}.jpg'.format(i) for i in range(125, 150)]\nfor fname in fnames:\n    src = os.path.join(original_dataset_dir, fname)\n    dst = os.path.join(test_baliklar_dir, fname)\n    shutil.copyfile(src, dst)","a175dcad":"print('Toplam kus train verisi:', len(os.listdir(train_kuslar_dir)))","67afa8f2":"print('Toplam balik train verisi:', len(os.listdir(train_baliklar_dir)))","57c9e827":"print('Toplam kus validation verisi: ', len(os.listdir(validation_kuslar_dir)))","609d5880":"print('Toplam balik validation verisi: ', len(os.listdir(validation_baliklar_dir)))","b7e6953a":"print('Toplam kus test verisi:', len(os.listdir(test_kuslar_dir)))","45a6d07e":"print('Toplam balik test verisi:', len(os.listdir(test_baliklar_dir)))","74f58552":"from keras import layers\nfrom keras import models\n\nmodel = models.Sequential()\nmodel.add(layers.Conv2D(32, (3, 3), activation = 'relu',\n                       input_shape = (150, 150, 3)))\nmodel.add(layers.MaxPooling2D((2, 2)))\nmodel.add(layers.Conv2D(64, (3, 3), activation = 'relu'))\nmodel.add(layers.MaxPooling2D((2, 2)))\nmodel.add(layers.Conv2D(128, (3, 3), activation = 'relu'))\nmodel.add(layers.MaxPooling2D(2, 2))\nmodel.add(layers.Conv2D(128, (3, 3), activation = 'relu'))\nmodel.add(layers.MaxPooling2D(2, 2))\nmodel.add(layers.Flatten())\nmodel.add(layers.Dense(512, activation = 'relu'))\nmodel.add(layers.Dense(1, activation = 'sigmoid'))","5a02a710":"model.summary()","c5f38598":"from keras import optimizers\n\nmodel.compile(loss = 'binary_crossentropy',\n             optimizer=optimizers.RMSprop(lr=1e-4),\n             metrics = ['acc'])","27d4d340":"from keras.preprocessing.image import ImageDataGenerator\n\ntrain_datagen = ImageDataGenerator(rescale = 1.\/255)\ntest_datagen = ImageDataGenerator(rescale = 1.\/255)\n\ntrain_generator = train_datagen.flow_from_directory(\n                    train_dir,\n                    target_size = (150, 150),\n                    batch_size = 20,\n                    class_mode = 'binary')\n\nvalidation_generator = test_datagen.flow_from_directory(\n                        validation_dir,\n                        target_size = (150, 150),\n                        batch_size = 20,\n                        class_mode = 'binary')","83ecae58":"for data_batch, labels_batch in train_generator:\n    print('veri sekli:', data_batch.shape)\n    print('etiket sekli:', labels_batch.shape)\n    break","14a755d2":"history = model.fit_generator(\n                train_generator,\n                steps_per_epoch = 100,\n                epochs = 20,\n                validation_data = validation_generator,\n                validation_steps = 50)","d4585d82":"model.save('kus_ve_balik_sinifi_1.h5')","de85bc88":"import matplotlib.pyplot as plt\n\nacc = history.history['acc']\nval_acc = history.history['val_acc']\nloss = history.history['loss']\nval_loss = history.history['val_loss']\n\nepochs = range(1, len(acc) + 1)\n\nplt.plot(epochs, acc, 'bo', label = 'Training Accuracy')\nplt.plot(epochs, val_acc, 'b', label = 'Validation Accuracy')\nplt.title('Training ve Validation Accuracy')\nplt.legend()\n\nplt.figure()\n\nplt.plot(epochs, loss, 'bo', label = 'Training Loss')\nplt.plot(epochs, val_loss, 'b', label = 'Validation Loss')\nplt.title('Training ve Validation Loss')\nplt.legend()\n\nplt.show()","f3577965":"datagen = ImageDataGenerator(\n                            rotation_range=40,\n                            width_shift_range=0.2,\n                            height_shift_range=0.2,\n                            shear_range=0.2,\n                            zoom_range=0.2,\n                            horizontal_flip=True,\n                            fill_mode='nearest')","65b13d6e":"import matplotlib.pyplot as plt\nfrom keras.preprocessing import image\n\nfnames = [os.path.join(train_kuslar_dir, fname) for fname in os.listdir(train_kuslar_dir)]\n\nimg_path = fnames[20]\n\nimg = image.load_img(img_path, target_size = (150, 150))\n\nx = image.img_to_array(img)\n\nx = x.reshape((1,) + x.shape)\n\ni = 0\nfor batch in datagen.flow(x, batch_size = 1):\n    plt.figure()\n    imgplot = plt.imshow(image.array_to_img(batch[0]))\n    i += 1\n    if i % 4 == 0:\n        break\nplt.show()","c8d03d31":"model = models.Sequential()\nmodel.add(layers.Conv2D(32, (3, 3), activation = 'relu', input_shape = (150, 150, 3)))\nmodel.add(layers.MaxPooling2D((2, 2)))\nmodel.add(layers.Conv2D(64, (3, 3), activation = 'relu'))\nmodel.add(layers.MaxPooling2D((2, 2)))\nmodel.add(layers.Conv2D(128, (3, 3), activation = 'relu'))\nmodel.add(layers.MaxPooling2D((2, 2)))\nmodel.add(layers.Conv2D(128, (3, 3), activation = 'relu'))\nmodel.add(layers.MaxPooling2D((2, 2)))\nmodel.add(layers.Flatten())\nmodel.add(layers.Dropout(0.5))\nmodel.add(layers.Dense(512, activation = 'relu'))\nmodel.add(layers.Dense(1, activation = 'sigmoid'))\n\nmodel.compile(loss = 'binary_crossentropy',\n             optimizer=optimizers.RMSprop(lr=1e-4),\n             metrics=['acc'])","4ba7ef35":"train_datagen = ImageDataGenerator(\n                rescale = 1.\/255,\n                rotation_range=40,\n                width_shift_range=0.2,\n                height_shift_range=0.2,\n                shear_range=0.2,\n                zoom_range=0.2,\n                horizontal_flip=True)\n\ntest_datagen = ImageDataGenerator(rescale=1.\/255)\n\ntrain_generator = train_datagen.flow_from_directory(\n                                train_dir,\n                                target_size = (150, 150),\n                                batch_size = 32,\n                                class_mode='binary')\n\nvalidation_generator = test_datagen.flow_from_directory(\n                                validation_dir,\n                                target_size = (150, 150),\n                                batch_size = 32,\n                                class_mode = 'binary')\n\nhistory = model.fit_generator(\n                            train_generator,\n                            steps_per_epoch = 100,\n                            epochs = 25,\n                            validation_data = validation_generator,\n                            validation_steps = 50)","1cf0f892":"model.save('kus_ve_balik_sinifi_2.h5')","2d247efc":"acc = history.history['acc']\nval_acc = history.history['val_acc']\n\nloss = history.history['loss']\nval_loss = history.history['val_loss']\n\nepochs = range(1, len(acc) + 1)\n\nplt.plot(epochs, acc, 'bo', label='Training Accuracy')\nplt.plot(epochs, val_acc, 'b', label = 'Validation Accuracy')\nplt.title('Training ve Validation Accuracy')\nplt.legend()\n\nplt.figure()\n\nplt.plot(epochs, loss, 'bo', label = 'Training Loss')\nplt.plot(epochs, val_loss, 'b', label='Validation Loss')\nplt.title('Training ve Validation Loss')\nplt.legend()\n\nplt.show()","382b786d":"import os\nprint(os.listdir('input\/vgg16'))","06e5612e":"from keras.applications import VGG16\n\nconv_base = VGG16(weights='input\/vgg16\/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5',\n                 include_top = False,\n                 input_shape = (150, 150, 3))","2abe5ec2":"conv_base.summary()","679f5545":"conv_base.trainable = True\n\nset_trainable = False\nfor layer in conv_base.layers:\n    if layer.name == 'block5_conv1':\n        set_trainable = True\n    if set_trainable:\n        layer.trainable = True\n    else:\n        layer.trainable = False","e6260ed2":"model.compile(loss = 'binary_crossentropy',\n             optimizer=optimizers.RMSprop(lr=1e-5),\n             metrics=['acc'])\n\nhistory = model.fit_generator(\n    train_generator,\n    steps_per_epoch=100,\n    epochs=25,\n    validation_data = validation_generator,\n    validation_steps=50)","a09f91c3":"acc = history.history['acc']\nval_acc = history.history['val_acc']\nloss = history.history['loss']\nval_loss = history.history['val_loss']\n\nepochs = range(1, len(acc) + 1)\n\nplt.plot(epochs, acc, 'bo', label = 'Training Accuracy')\nplt.plot(epochs, val_acc, 'b', label = 'Validation Accuracy')\nplt.title('Training ve Validation Accuracy')\n\nplt.figure()\nplt.plot(epochs, loss, 'bo', label ='Training Loss')\nplt.plot(epochs, val_loss, 'b', label = 'Validation Loss')\nplt.title('Training ve validation loss')\nplt.legend()\n\nplt.show()","b131a9ab":"test_generator = test_datagen.flow_from_directory(\n        test_dir,\n        target_size=(150, 150),\n        batch_size=20,\n        class_mode='binary')\n\ntest_loss, test_acc = model.evaluate_generator(test_generator, steps=50)\nprint('test acc:', test_acc)","8c8702b0":"Modelin grafi\u011fi olu\u015fturulur.","c23810fd":"Model e\u011fitilir.","defe65e9":"Model kaydedilir.","d41435f2":"Data Augmentation burada uygulanm\u0131\u015ft\u0131r.","e6deb4d3":"Data Augmentation uygulan\u0131rken dropout eklenmi\u015ftir.","49696172":"Data Augmentation ile olu\u015fturulmu\u015f resim \u00f6rnekleri g\u00f6sterilir.","b30db380":"Dataset Link: https:\/\/www.dropbox.com\/s\/t37d6sn0g5po4ll\/CNN_Dataset.zip?dl=0","374a3264":"Augmentation uygulanm\u0131\u015f veriler tekrar normalize edilir.","a4dc8311":"Hem ku\u015f hem de bal\u0131k resimlerinden e\u011fitim, test ve validation i\u00e7in resimler se\u00e7ilir.","e61b0613":"Data Augmentation uygulan\u0131r.","acaa4788":"Data augmention ve dropout uyguland\u0131ktan sonra model 7 epoch sonras\u0131nda overfit olmu\u015ftur. Data augmention verisetinin \u00e7e\u015fitlili\u011fini artt\u0131rd\u0131\u011f\u0131 i\u00e7in sistemin overfit olma ihtimalini d\u00fc\u015f\u00fcrm\u00fc\u015ft\u00fcr.","251516c0":"Resimler normalize edilir. (T\u00fcm resimler ayn\u0131 boyuta getirilir.)","d0cadf73":"Model kaydedilir ve model grafi\u011fi olu\u015fturulur.","da733c67":"Bu ilk model dikkate al\u0131nd\u0131\u011f\u0131nda modelin 2.5 epoch ve sonras\u0131nda overfit oldu\u011fu g\u00f6r\u00fcnmektedir. \u00d6zellikle verisetin az olmas\u0131 bir overfit nedenidir.","adc1ba37":"vgg16 \u00f6nceden olu\u015fturulmu\u015f bir a\u011f modelidir. Burada epoch ayar\u0131 denemesi i\u00e7in projeye eklenmi\u015ftir.","022499bb":"Epoch ayar\u0131 modelinin grafi\u011fi olu\u015fturulur.","f1138561":"Augmentation ve dropout kullan\u0131larak olu\u015fturulmu\u015f model e\u011fitilir."}}