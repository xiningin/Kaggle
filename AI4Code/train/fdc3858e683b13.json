{"cell_type":{"4fce41e1":"code","a960c8aa":"code","a944b1f6":"code","a4b39eb5":"code","c1eeb1ce":"code","f9274906":"code","2ffac2e7":"code","18811c92":"code","a25decc8":"code","3a45d658":"code","8d79efd8":"code","9632ed75":"code","25189510":"code","d04d140f":"code","96398742":"code","31f4bb3f":"code","12805d5f":"code","f5e842a5":"code","bb970fcd":"code","ce1edd3e":"code","b25d8674":"code","a23c4cad":"markdown","e42de7bd":"markdown","94185534":"markdown","bcc3b25e":"markdown","c4e05c66":"markdown","d212a15b":"markdown","8ac1dd3e":"markdown","80c933b8":"markdown","8b6193f9":"markdown","23cf1f90":"markdown","2c4fd450":"markdown","d6caaa34":"markdown","675dfd58":"markdown","039e45a9":"markdown","b62c8589":"markdown"},"source":{"4fce41e1":"import sklearn\nimport numpy as np\nfrom sklearn.datasets import load_breast_cancer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nimport pandas as pd\nfrom sklearn.decomposition import PCA\nimport plotly.express as px\npd.set_option('display.max_rows',30)","a960c8aa":"x = load_breast_cancer()['data']\ny = load_breast_cancer()['target']\n\nx_train, x_test, y_train, y_test = train_test_split(x,y, test_size = 0.25, stratify=y)","a944b1f6":"pca = PCA(n_components = 2)\npca = pd.DataFrame(pca.fit_transform(x), columns = ['x_dim', 'y_dim'])\npca['y'] = y\nfig = px.scatter(pca, 'x_dim', 'y_dim', color = 'y')\nfig.show()","a4b39eb5":"clf = LogisticRegression(max_iter = 10000)\nclf.fit(x_train, y_train)","c1eeb1ce":"y_true = y_test\ny_pred = clf.predict(x_test)","f9274906":"def accuracy_score(y_true, y_pred):\n    count =0\n    for yt, yp in zip(y_true, y_pred): \n        if yt==yp:\n            count = count + 1\n    return count\/len(y_pred)\nacc = accuracy_score(y_true, y_pred)\nprint( f'Accuracy of the model is {acc}')","2ffac2e7":"def True_Positive(y_true, y_pred):\n    count = 0\n    for yt, yp in zip(y_true, y_pred):\n        if yt ==1 and yp == 1:  \n            count = count + 1\n    return count\n\ndef True_Negative(y_true, y_pred):\n    count = 0\n    for yt, yp in zip(y_true, y_pred):\n        if yt ==0 and yp == 0:\n            count = count + 1\n    return count\n\ndef False_Positive(y_true, y_pred):\n    count = 0\n    for yt, yp in zip(y_true, y_pred):\n        if yt ==0 and yp == 1:\n            count = count + 1\n    return count\n\ndef False_Negative(y_true, y_pred):\n    count = 0\n    for yt, yp in zip(y_true, y_pred):\n        if yt ==1 and yp == 0:\n            count = count + 1\n    return count","18811c92":"print(f' Number of True Positive {True_Positive(y_true, y_pred)}')\nprint(f' Number of True Negative {True_Negative(y_true, y_pred)}')\nprint(f' Number of False Positive {False_Positive(y_true, y_pred)}')\nprint(f' Number of False Positive {False_Negative(y_true, y_pred)}')","a25decc8":"TP = True_Positive(y_true, y_pred)\nTN = True_Negative(y_true, y_pred)\nFP = False_Positive(y_true, y_pred)\nFN = False_Negative(y_true, y_pred)","3a45d658":"def Precision(y_true, y_pred):\n    TP = True_Positive(y_true, y_pred)\n    FP = False_Positive(y_true, y_pred)\n    prec = TP\/(TP + FP)\n    return prec","8d79efd8":"print(f'We classified {Precision(y_true, y_pred)} % of patients having predicted breast cancer')","9632ed75":"def recall(y_true, y_pred):\n    TP = True_Positive(y_true, y_pred)\n    TN = True_Negative(y_true, y_pred)\n    FP = False_Positive(y_true, y_pred)\n    FN = False_Negative(y_true, y_pred)\n    \n    recal = (TP)\/(TP+ FN)\n    return recal","25189510":"print(f'We classified {recall(y_true, y_pred)} % of patients having Actual breast cancer')","d04d140f":"def Precision_Recall_Curve(y_true):\n    # So we will calculate all precision and recall value with varying threshold, I will vary threshold with 0.01\n    thresholds = np.arange(0,1,0.01)\n    Probabilities_of_prediction =  clf.predict_proba(x_test)\n    precision_list = []\n    recall_list = []\n    # We will calculate y_pred for each threshold \n    for thresh in thresholds:\n        temp_y_pred = []\n        for prob in Probabilities_of_prediction:\n            if prob[1] > thresh:\n                temp_y_pred.append(1)\n            else:\n                temp_y_pred.append(0)\n                \n        # Calculating precision from y_true and y_pred\n        precision_list.append(Precision(y_true, temp_y_pred))\n        recall_list.append(recall(y_true, temp_y_pred))\n    return precision_list,recall_list, thresholds\n\nprecision_list,recall_list, thresholds=  Precision_Recall_Curve(y_true)\n\n# Let's visualize Precision Recall curve\nfig = px.line(precision_list, recall_list)\nfig.update_layout(    title=\"Precision Recall\",\n    xaxis_title=\"Recall\",\n    yaxis_title=\"Precision\",)\nfig.show()","96398742":"# We will pass Precision list and recall list that we created from the last cell,  \n# This function will give output as list of F1 score for each pair of precision and Recall\ndef f1_score(precision_list,recall_list):\n    f1 = []\n    for Prec, Reca in zip(precision_list,recall_list):\n        f1.append(2 * Prec * Reca \/ (Prec + Reca))\n    return f1","31f4bb3f":"f1_score = f1_score(precision_list,recall_list)","12805d5f":"# Let's put all those things in dataframe\ndf = pd.DataFrame()\ndf['Thresholds'] = thresholds\ndf['Precision'] = precision_list\ndf['Recall'] = recall_list\ndf['F1_score'] = f1_score\ndf","f5e842a5":"# Let's plot F1 score versus threshold  \nfig = px.line(df, 'Thresholds', 'F1_score')\nfig.update_layout(    title=\"Threshold- F1 score Curve\",\n    xaxis_title=\"F1 score\",\n    yaxis_title=\"Threshold\",)\nfig.show()","bb970fcd":"\n# Calculates True positive rate\ndef True_Positive_Rate(y_true, y_pred):\n    TP = True_Positive(y_true, y_pred)\n    FN = False_Negative(y_true, y_pred)\n    return TP\/(TP+FN)\n\n# Calculates False positive rate\ndef False_Positive_Rate(y_true, y_pred):\n    TN = True_Negative(y_true, y_pred)\n    FP = False_Positive(y_true, y_pred)\n    return FP\/(FP+TN)\n\n# Calculates TPR and FPR by varying threshold we generated.   \ndef TPR_FPR(y_true):\n    thresholds = np.arange(0,1,0.01)\n    Probabilities_of_prediction =  clf.predict_proba(x_test)\n    TPR = []\n    FPR = []\n    for thresh in thresholds:\n        temp_y_pred = []\n        for prob in Probabilities_of_prediction:\n            if prob[1] > thresh:\n                temp_y_pred.append(1)\n            else:\n                temp_y_pred.append(0)\n                \n        TPR.append(True_Positive_Rate(y_true, temp_y_pred))\n        FPR.append(False_Positive_Rate(y_true, temp_y_pred))\n    \n    return TPR, FPR\n\nTPR, FPR =  TPR_FPR(y_true)","ce1edd3e":"df['TPR'] = TPR\ndf['FPR'] = FPR","b25d8674":"# Let's plot Receiver Operating Characteristic curve \nfig = px.line(df, 'FPR', 'TPR')\nfig.update_layout(title=\"Receiver Operating Characteristic\",\n    xaxis_title=\"FPR\",\n    yaxis_title=\"TPR\",)\nfig.show()\n","a23c4cad":"## 3.7) AUC- Are Under Curve  \n<a id='section_3_7' ><\/a>\nThis is a curve with TPR Vs FPR.  \n\ni) TPR- True Positive rate  \nFormula: TP\/(TP + FN)  \n\nii) FPR- False Positive rate  \nFormula: FP\/(TN+FP)","e42de7bd":"## 3.2) Basic Terms of Confusion Metrics  \n<a id='section_3_2' ><\/a>\nLet's assume breast cancer - 1, without breast cancer - 0\n\ni) True Positive:-  \n> This means, Model predicts 1 and actual class is also 1, Then it is considered as TP  \n\nii) True Negative:-  \n> This means, Model predicts 0 and actual class is also 0, Then it is considered as TN.  \n\niii) False Positive:-  \n> This means, Model predicts 1 but actual class is 0\n\niv) False Negative\n> This means, Model predicts 0 but actual class is 1","94185534":"## 3.1) Accuracy Score  \n<a id='section_3_1' ><\/a>  \nLet's say out of 100 patients of breast cancer, We predicted 90 patients as positive and which is correct, It means   accuracy score of our model is 90\/100 = 90%  \n\nLet's take an example so that we can have conclusion that, Every time accuracy does not represent our model correctly.  \nSay we have 100 sample, out of which we have 90 without breast cancer and 10 with breast cancer. In our skewed dataset let's assume accuracy score is 90%, which means out of 100 patients we classify 90 perfectly. But as we  \nseen our skewed dataset, It may happen that all correctly classified patients are without breast cancer which is large portion of our data.  \nThis means that we have 0% accuracy considering only breast cancer patient.  ","bcc3b25e":"Let's try to find meaning of this  \nPrecision denotes,  how many we able to classify correctly out of predicted positive class..  \nLet's Take an example with sample size 100 and assume some values of TP, TM, FP, FN.  \nTP = 8  \nTN = 80  \nFP = 10  \nFN = 2  \n\nSo according to this assumption we got (8+88)\/(100) = 96% accuracy. to classify breast cancer.  \nLet's check the percentage of patients we classify correctly having breast cancer.  \n = TP\/(TP+FP)  \n = 8\/(8 + 10)  \n = 44.44 %  \n Pretty bad, We only classified 44.44% patiets having breast cancer correctly.  ","c4e05c66":"From this Precision -Recall curve we can choose threshold where we get both high Precision as well high Recall.  \nWhat does mean High Precision and High Recall ?  \n---> High Precision means Low False Positive.  \n---> High Recall means, Low False Negative.   ","d212a15b":"## 3.3) Precision  \n<a id='section_3_3' ><\/a>\nFormula: TP\/(TP+FP)  \nMeaning: How many we correctly classified among the **PREDICTED** positive class.  \n Now come to our example we will check Precision for our dataset.\n","8ac1dd3e":"## 3) Evaluation Methods  \n<a id='section_3' ><\/a>","80c933b8":"## 3.5) Precision Recall Curve  \n<a id='section_3_5' ><\/a>\n\nThere is trade of between Precision and recall, In our case of classifying patients with breast cancer or not.  \nWe don't want False Negative, Means( We classify patient as not having breast cancer even though our patient is having Breast cancer)  ","8b6193f9":"Let's try to find meaning of this  \nRecall denotes,  how many we able to classify correctly out of Actual positive class..  \nLet's Take an example with sample size 100 and assume some values of TP, TM, FP, FN.  \nTP = 8  \nTN = 80  \nFP = 10  \nFN = 2  \n\nSo according to this assumption we got (8+88)\/(100) = 96% accuracy. to classify breast cancer.  \nLet's check percentage of patients we classify correctly among the actual patients having breast cancer.  \n\n = TP\/(TP+FN)  \n = 8\/(8+2)  \n = 80 %  \n \n So we able to classify 80% of the patients having breast cancer.","23cf1f90":"# Table of Content  \n\n* 1) [Import Liabraries](#section_1)  \n* 2) [Create Model](#section_2)  \n* 3) [Evaluation Methods](#section_3)  \n    * 3.1) [Accuracy Score](#section_3_1)  \n    * 3.2) [Basic Terms in the Confusion_Matrix](#section_3_2)  \n    * 3.3) [Precision](#section_3_3)  \n    * 3.4) [Recall](#section_3_4)  \n    * 3.5) [Precision Recall Curve](#section_3_5)  \n    * 3.6) [F1 Score](#section_3_6)  \n    * 3.7) [AUC- Area Under Curve](#section_3_7)\n    ","2c4fd450":"## 3.4) Recall  \n<a id='section_3_4' ><\/a>\n\nFormula: (TP)\/(TP+ FN)  \nMeaning: How many we correctly classified among the **ACTUAL** positive class.  ","d6caaa34":"## 3.6) F1 Score  \n<a id='section_3_6' ><\/a>\nFormula: 2PR\/(P+R)  \nMeaning: Just Harmonic mean of Precision and recall. Better if closer towards 1 and bad if towards 0.  \n","675dfd58":"## 2) Create Model with Logistic Regression  \n<a id='section_2' ><\/a>","039e45a9":"## 1) Import Liabraries  \n<a id='section_1' ><\/a>","b62c8589":"### Visualize the Dataset"}}