{"cell_type":{"326c7e5e":"code","0b343944":"code","bef06dc1":"code","9ac47484":"code","b0d4f7e2":"code","df8bd78a":"code","df0f87e9":"code","957afd36":"code","84dd1cee":"code","75370248":"code","e72da9a1":"code","97dd3a44":"code","3341bbd3":"code","6c49cf02":"code","559b89bf":"code","1428d5a6":"code","ac4dc9d5":"code","110ece09":"code","a185f145":"code","0f595d09":"code","e985edc6":"code","bc8cf500":"code","70dccd91":"code","dd9f276e":"code","864d1f85":"code","2c054eb0":"code","3829f56f":"code","a6519ee4":"code","612dd87c":"code","f7261bba":"code","1ee024d1":"code","8719bb57":"code","e0f8b04a":"code","de68584d":"code","67125b1d":"code","ab14eb0a":"code","03f9b329":"code","69e40362":"code","252bc2a1":"code","e37d59b5":"code","0facf794":"code","f76484f0":"code","178e442d":"code","acb70d7d":"code","b5ba7ff7":"code","6d9368b9":"code","ecb3fa4a":"code","d4ef796e":"code","d6eb20a9":"code","7502f03b":"code","2507b0fc":"code","4e2f3a2f":"code","4f344931":"code","c9f5630a":"code","81d611bd":"code","5510304a":"code","f7d86984":"code","3ee5d9f6":"code","38da0c66":"code","67a23676":"code","3f93c1fa":"code","6eac2485":"code","0a00ccc5":"code","e15f2f0b":"code","8c1e71c5":"code","02c63c95":"code","d93f811e":"code","90e9869e":"code","35149bc0":"code","58d7b161":"code","2d5bc05a":"code","be49a61f":"code","85f22f24":"code","85faab63":"code","7e936e52":"code","3e29f831":"code","dcdf91d8":"code","beb75192":"code","e0485ff2":"code","5677d0aa":"code","5e87f9f5":"code","260d1576":"code","b7878c64":"code","f12c0960":"code","2017e74b":"code","a003b64c":"code","d808b96d":"code","45653e16":"code","9bda4af8":"code","a24a6e4e":"code","37413e20":"code","5c2c1976":"code","ca0ed5f5":"code","c65ebcc6":"code","7521d330":"code","63fabe74":"code","362f3f12":"code","8523b5f6":"code","271eadc0":"code","9a470e1b":"code","7f984e9e":"code","e31849e9":"code","e290ea90":"code","e9d1d1ea":"code","8bf8cd68":"code","d0f6731b":"code","99db24a2":"code","3e1cfc28":"code","d2c4e403":"code","b405f1bf":"code","c7292659":"code","c1e00b6c":"code","52e61f2c":"code","c8d75a27":"code","8af0dbcc":"code","ad73ffc0":"code","6a25c353":"code","0f460997":"code","1547b995":"code","32caa926":"code","1570282b":"code","cffdb4af":"code","eab142a7":"code","8682ccb4":"code","7e19fa67":"code","0e732508":"code","ac1a6d13":"code","6087de93":"code","9c7e4edc":"code","3768e46c":"code","7bcabe58":"code","355a057f":"code","de5abb34":"code","727808d0":"code","375150ae":"code","bbc7caf0":"code","98904d56":"code","d8de27f9":"code","b14f0f59":"code","8703a618":"code","6c747a52":"code","d361cc3d":"code","f08987e2":"code","2024489f":"code","7c28188a":"markdown","b8fa6a7e":"markdown","c2e8cc92":"markdown","6949bed7":"markdown","0b23d429":"markdown","b2ed5ed5":"markdown","1bca73a9":"markdown","d6aec44b":"markdown","2b3a045f":"markdown","a32bb390":"markdown","00ebdca4":"markdown","2048beb0":"markdown","eb6f3352":"markdown","9f46ffe8":"markdown","52aaa623":"markdown","28c197e4":"markdown","f81c5813":"markdown","efd73399":"markdown","d1e92184":"markdown","7dcb1bb9":"markdown","c3679a9f":"markdown","cc0064fa":"markdown","601f991e":"markdown","02b27e0e":"markdown","b2c26468":"markdown","f60ba5ce":"markdown","dd8b3b47":"markdown","8ddd8787":"markdown","cfcb3d70":"markdown","a351a146":"markdown","f767df84":"markdown","aebbe6dc":"markdown","2cd552c8":"markdown","4bc1c5cf":"markdown","3d081c02":"markdown","940e5c2f":"markdown","021357cb":"markdown","764fb3de":"markdown","40b2a8b0":"markdown","d1e41ae0":"markdown","e8f60d5c":"markdown","577d42fd":"markdown","8f066cb0":"markdown","b9416299":"markdown","d9ff840a":"markdown","401963e0":"markdown","f5b15df9":"markdown","1ad92190":"markdown","2f0828ac":"markdown","521529f7":"markdown","570b2014":"markdown","f0554c45":"markdown","f276b3f8":"markdown","5cfac568":"markdown","d167f356":"markdown","adbc5441":"markdown","3bf5b501":"markdown","b0f95b21":"markdown","629044dd":"markdown","868de127":"markdown","277c211e":"markdown","470da044":"markdown","c6066dc4":"markdown","ff6d20a4":"markdown","fbd6ca59":"markdown","225ab5d3":"markdown","42fe6777":"markdown","65a416ca":"markdown","3fb1db62":"markdown","5da8bffb":"markdown","6166f33f":"markdown","d9fb5e2e":"markdown"},"source":{"326c7e5e":"! pip install --upgrade pip\n! pip install selenium\n! pip install webdriver_manager","0b343944":"import pandas as pd\nimport numpy as np\n\nfrom time import sleep\nfrom random import randint\n\n\nfrom bs4 import BeautifulSoup\nfrom selenium import webdriver","bef06dc1":"# Preparing the chromedriver on kaggle\n!apt-get update # to update ubuntu to correctly run apt install\n!apt install -y chromium-chromedriver\n!cp \/usr\/lib\/chromium-browser\/chromedriver \/usr\/bin","9ac47484":"# Setting the chromedriver\nchrome_options = webdriver.ChromeOptions()\nchrome_options.add_argument('--headless')\nchrome_options.add_argument('--no-sandbox')\nchrome_options.add_argument('--disable-dev-shm-usage')\ndriver = webdriver.Chrome('chromedriver',chrome_options=chrome_options)","b0d4f7e2":"resut_list = []","df8bd78a":"# For loop to scraping each page on vivareal website\n\nfor i in range(1,88,1):\n    \n    url = \"https:\/\/www.vivareal.com.br\/venda\/distrito-federal\/brasilia\/?__vt=rpci:a&pagina=\"+str(i)+\"#onde=BR-Distrito_Federal-NULL-Brasilia,BR-Distrito_Federal-NULL-Brasilia-Barrios-Asa_Sul,BR-Distrito_Federal-NULL-Brasilia-Barrios-Asa_Norte,BR-Distrito_Federal-NULL-Aguas_Claras&tipos=apartamento_residencial\"\n    \n    \n    driver.get(url)\n    \n    sleep(5)\n    soup = BeautifulSoup(driver.page_source, 'html.parser')\n    \n    \n    apt_div = soup.find_all('article', class_='property-card__container js-property-card')\n    \n    \n    sleep(randint(2,10))\n    \n    \n    for container in apt_div:\n        \n        resut_dict = {}\n\n        resut_dict['address'] = container.h2.find('span', class_=\"property-card__address\").text\n        resut_dict['titles'] = container.h2.find('span', class_=\"property-card__title js-cardLink js-card-title\").text\n        resut_dict['areas m\u00b2'] = container.ul.find('li', class_=\"property-card__detail-item property-card__detail-area\").text\n        resut_dict['rooms'] = container.ul.find('li', class_=\"property-card__detail-item property-card__detail-room js-property-detail-rooms\").text\n        resut_dict['bathrooms'] = container.ul.find('li', class_=\"property-card__detail-item property-card__detail-bathroom js-property-detail-bathroom\").text\n        resut_dict['garages'] = container.ul.find('li', class_=\"property-card__detail-item property-card__detail-garage js-property-detail-garages\").text\n        resut_dict['condo_fees'] = []\n        resut_dict['price'] = container.section.find('div', class_=\"property-card__price js-property-card-prices js-property-card__price-small\").text\n\n\n        if container.find('strong', class_='js-condo-price')== None:  \n            resut_dict['condo_fees'].append(None)\n        else:\n            resut_dict['condo_fees'].append(container.find('strong', class_='js-condo-price').text)\n\n        for item in container.find_all('li',class_=\"amenities__item\"):\n            resut_dict[item.get_text().lower()] = 1\n\n        \n        resut_list.append(resut_dict)","df0f87e9":"df = pd.DataFrame(resut_list)","957afd36":"df.head()","84dd1cee":"df.to_csv('Brasilia_Apartaments_Raw.csv')","75370248":"# Cleaning the informations and defining the the df column\n\ndf['titles'] = df['titles'].str.replace('\\n','')\ndf['price'] = df['price'].str.replace('.','')\ndf['condo_fees'] = df['condo_fees'].astype(str).str.replace('.','').str.replace('[','').str.replace(']','')\ndf['price'] = df['price'].str.extract('(\\d+)').astype(float)\ndf['condo_fees'] = df['condo_fees'].astype(str).str.extract('(\\d+)').astype(float)\ndf['areas m\u00b2'] = df['areas m\u00b2'].astype(str).str.extract('(\\d+)').astype(float)\ndf['rooms'] = df['rooms'].astype(str).str.extract('(\\d+)').astype(float)\ndf['bathrooms'] = df['bathrooms'].astype(str).str.extract('(\\d+)').astype(float)\ndf['garages'] = df['garages'].astype(str).str.extract('(\\d+)').astype(float)\ndf['condo_fees'] = df['condo_fees'].astype(str).str.extract('(\\d+)').astype(float)","e72da9a1":"df.shape","97dd3a44":"df.head()","3341bbd3":"df.info()","6c49cf02":"# Total missing value\n\nmissing_values_count = df.isnull().sum()\n\nmissing_values_count.head()","559b89bf":"# Total percent missing value\n\ntotal_cells = np.product(df.shape)\ntotal_missing = missing_values_count.sum()\n\n\npercent_missing = (total_missing\/total_cells) * 100\n\nprint(percent_missing)","1428d5a6":"# Checking if the binary features missing value\n\nmissing_values_count =df[df.columns[8:51]].isnull().sum()\n\ntotal_cells = np.product(df[df.columns[8:51]].shape)\ntotal_missing = missing_values_count.sum()\n\n\npercent_missing = (total_missing\/total_cells) * 100\n\nprint(percent_missing)","ac4dc9d5":"# Dropping the column\n\ndf = df.drop(df[df.columns[8:73]], axis=1)\n\ndf.info()","110ece09":"# Percentage of Missing values after drop binary features\n\nmissing_values_count = df.isnull().sum()\n\ntotal_cells = np.product(df.shape)\ntotal_missing = missing_values_count.sum()\n\n\npercent_missing = (total_missing\/total_cells) * 100\n\nprint(percent_missing)","a185f145":"! pip install Consulta-Correios\n! pip install unidecode","0f595d09":"import consulta_correios\nfrom unidecode import unidecode","e985edc6":"def correios_api(address):\n    \n    # Splitting the string from address value. \n    address_call = address.replace('-',\",\").split(',')[:4]\n    # Creating a list that contains all string parts. We are going to ignore the parts smaller than 2 caracteres.\n    l = []\n    for i in address_call:\n        i = unidecode(i).strip()\n        if len(i) > 2:\n            l.append(i)\n    \n    # From the list, we will try a few combinations of parts to sended to the busca cep API.\n    # Prioritizing send more information + Bras\u00edlia\/DF\n    try:\n        call = str(l[0]+\" \"+l[1]+\" \"+l[2]).replace('Brasilia','').replace('DF','')\n        address_df = pd.DataFrame(consulta_correios.busca_cep(str(call+\" Bras\u00edlia\/DF\")))   \n        match = check_address(address_df,l[0],l[1])\n\n        return pd.Series([match['address'], match['neighborhood']])\n        pass\n    except:\n        try:\n            call = str(l[0]+\" \"+l[-1]).replace('Brasilia','').replace('DF','')\n            address_df = pd.DataFrame(consulta_correios.busca_cep(str(call+\" Bras\u00edlia\/DF\")))   \n            match = check_address(address_df,l[0],l[1])\n\n            return pd.Series([match['address'], match['neighborhood']])\n            pass\n        except:\n            try:\n                call = str(l[0]+\" \"+l[1]).replace('Brasilia','').replace('DF','')\n                address_df = pd.DataFrame(consulta_correios.busca_cep(str(call+\" Bras\u00edlia\/DF\")))   \n\n                match = check_address(address_df,l[0],l[1])\n\n                return pd.Series([match['address'], match['neighborhood']])\n                pass\n            except:\n                try:\n                    call = str(l[0]).replace('Brasilia','').replace('DF','')\n                    address_df = pd.DataFrame(consulta_correios.busca_cep(str(call+\" Bras\u00edlia\/DF\")))\n                    match = check_address(address_df,l[0],l[1])\n\n                    return pd.Series([match['address'], match['neighborhood']])\n                    pass\n                except:\n                    return pd.Series([None, None])\n                    pass\n\n","bc8cf500":"def check_address(address_df,l0,l1):\n    \n    # Here we will try to find some combinations of the parts from old address column in the address_df returned from Busca Cep API \n    try:\n        row = address_df.loc[(address_df['address']==l0)&(address_df['neighborhood']==l1)]\n        if len(row)>=1:\n            match = row.reset_index(drop=True).loc[0]\n        return match\n    except:\n        try:\n            row = address_df.loc[(address_df['address']==l0)&(address_df['neighborhood'].str.contains(l1, na=False, case=False))]\n            if len(row)>=1:\n                match = row.reset_index(drop=True).loc[0]\n            return match\n        except:\n            try:\n                row = address_df.loc[address_df['address']==l0]\n                if len(row)>=1:\n                    match = row.reset_index(drop=True).loc[0]\n                return match\n            except:\n                try:\n                    row = address_df[address_df['address'].str.contains(l0, na=False, case=False)]\n                    if len(row) >= 1: \n                        match = row.reset_index(drop=True).loc[0]\n                    return match\n                except:\n                    pass\n    \n","70dccd91":"df['address_'] = ''\ndf['neighborhood'] = ''","dd9f276e":"%%time\n# Running this to functions in all address values from our list.\n\ndf[['address_','neighborhood']] = df.apply(lambda x: correios_api(x[\"address\"]),axis=1)","864d1f85":"df.info()","2c054eb0":"# Defining a list of neighborhood unique values\n\nneighborhood_ = df['neighborhood'].str.strip().dropna().unique().tolist()\n\nneighborhood_","3829f56f":"# Removing parentheses and blank spaces from the list\n\nneighborhood_list = ['']\n\nfor i in neighborhood_:\n    \n    x = i.replace('(','').replace(')','').strip()\n    neighborhood_list.append(x)\n","a6519ee4":"neighborhood_list.remove('')\n\nneighborhood_list","612dd87c":"# Looking in the old address column for neighborhood names and filling neighborhoods values found\n\nfor index, row in df.iterrows():\n    \n    if pd.isna(df['neighborhood'][index]):\n        nan = df['address'][index].replace('-',\",\").split(',')\n        l = []\n        for i in nan:\n            i = unidecode(i)\n            i = i.strip()\n            if len(i) > 1:\n                l.append(i)\n        for i in l:\n            if any(i in s for s in neighborhood_list):\n                df['neighborhood'][index] = i.strip()","f7261bba":"# Checking the remnants\n\ndf[pd.isna(df['neighborhood'])]","1ee024d1":"# Filling the remnants values (Searching the address on Google)\n\ndf.loc[df['address']=='Setor Oeste, Bras\u00edlia - DF','neighborhood']='Setor Oeste'\ndf.loc[df['address']=='Setor Habitacional Samambaia, Bras\u00edlia - DF','neighborhood']='Setor Habitacional Samambaia'","8719bb57":"df[pd.isna(df['neighborhood'])]","e0f8b04a":"# Looking for different names for the same values\n\ndf['neighborhood'].unique()","de68584d":"# Renaming the different values\n\n#Plano piloto\ndf.loc[df['neighborhood']=='Asa Sul','neighborhood']='Asa Sul (Plano Piloto)'\ndf.loc[df['neighborhood']=='Asa Norte','neighborhood']='Asa Norte (Plano Piloto)'\ndf.loc[df['neighborhood']=='Setor Noroeste','neighborhood']='Setor Noroeste (Plano Piloto)'\ndf.loc[df['neighborhood']=='Granja do Torto','neighborhood']='Granja do Torto (Plano Piloto)'\n\n# Aguas Claras\ndf.loc[df['neighborhood']=='Norte','neighborhood']='Norte (Aguas Claras)'\ndf.loc[df['neighborhood']=='Sul','neighborhood']='Sul (Aguas Claras)'\ndf.loc[df['neighborhood']=='Setor Habitacional Arniqueira','neighborhood']='Setor Habitacional Arniqueira (Aguas Claras)'\n\n#Taguatinga\ndf.loc[df['neighborhood']=='Taguatinga Sul','neighborhood']='Taguatinga Sul (Taguatinga)'\ndf.loc[df['neighborhood']=='Taguatinga Norte','neighborhood']='Taguatinga Norte (Taguatinga)'\ndf.loc[df['neighborhood']=='Setor Industrial','neighborhood']='Setor Industrial (Taguatinga)'\ndf.loc[df['neighborhood']=='Taguatinga Centro','neighborhood']='Taguatinga Centro (Taguatinga)'\ndf.loc[df['neighborhood']=='Setor Hoteleiro','neighborhood']='Setor Hoteleiro (Taguatinga)'\n\n#Lago Sul\ndf.loc[df['neighborhood']=='Setor de Habitacoes Individuais Sul','neighborhood']='Setor de Habitacoes Individuais Sul (Lago Sul)'\n\n#Lago Norte\ndf.loc[df['neighborhood']=='Setor de Habitacoes Individuais Norte','neighborhood']='Setor de Habitacoes Individuais Norte (Lago Norte)'\n\n#Samambaia\ndf.loc[df['neighborhood']=='Samambaia Sul','neighborhood']='Samambaia Sul (Samambaia)'\ndf.loc[df['neighborhood']=='Samambaia Norte','neighborhood']='Samambaia Norte (Samambaia)'\ndf.loc[df['neighborhood']=='Setor Habitacional Samambaia','neighborhood']='Samambaia'\n\n#Ceilandia\ndf.loc[df['neighborhood']=='Ceilandia Norte','neighborhood']='Ceilandia Norte (Ceilandia)'\ndf.loc[df['neighborhood']=='Ceilandia Sul','neighborhood']='Ceilandia Sul (Ceilandia)'\n\n#Guara\ndf.loc[df['address'].str.contains('Zona Industrial'),'neighborhood']='Zona Industrial (Guara)'\ndf.loc[df['address'].str.contains('Quadras Econ\u00f4micas'),'neighborhood']='Quadras Economicas Lucio Costa'\ndf.loc[df['neighborhood']=='Setores Complementares','neighborhood']='Setores Complementares (Guara)'\ndf.loc[df['neighborhood']=='Quadras Economicas Lucio Costa','neighborhood']='Quadras Economicas Lucio Costa (Guara)'\ndf.loc[df['neighborhood']=='Zona Industrial','neighborhood']='Zona Industrial (Guara)'\ndf.loc[df['neighborhood']=='Guara II','neighborhood']='Guara II (Guara)'\ndf.loc[df['neighborhood']=='Guara I','neighborhood']='Guara I (Guara)'\n\n#Gama\ndf.loc[df['neighborhood']=='Setor Central','neighborhood']='Setor Central (Gama)'\ndf.loc[df['neighborhood']=='Setor Leste','neighborhood']='Setor Leste (Gama)'\ndf.loc[df['neighborhood']=='Setor Oeste','neighborhood']='Setor Oeste (Gama)'\ndf.loc[df['neighborhood']=='Setor Sul','neighborhood']='Setor Sul (Gama)'\n\n#Areal\ndf.loc[df['neighborhood']=='Areal (Aguas Claras)','neighborhood']='Areal'\n\n#Vicente Pires\ndf.loc[df['address'].str.contains('Setor Habitacional Vicente Pires'),'neighborhood']='Setor Habitacional Vicente Pires'\ndf.loc[df['neighborhood']=='Setor Habitacional Vicente Pires - Trecho 3','neighborhood']='Setor Habitacional Vicente Pires'\ndf.loc[df['neighborhood']=='Setor Habitacional Vicente Pires','neighborhood']='Setor Habitacional Vicente Pires'\n\n#Sobradinho\ndf.loc[df['neighborhood']=='Grande Colorado','neighborhood']='Grande Colorado (Sobradinho)'\ndf.loc[df['neighborhood']=='Setor Habitacional Contagem','neighborhood']='Setor Habitacional Contagem (Sobradinho)'\n\n#S\u00e3o sebastiao\ndf.loc[df['neighborhood']=='Jardins Mangueiral','neighborhood']='Jardins Mangueiral (Sao Sebastiao)'\n\n#sta_maria\ndf.loc[df['neighborhood']=='Setor Meireles','neighborhood']='Setor Meireles (Santa Maria)'\n\n#nucleo bandeirante\ndf.loc[df['neighborhood']=='Setor Placa da Mercedes','neighborhood']='Setor Placa da Mercedes (Nucleo Bandeirante)'\ndf.loc[df['neighborhood']== 'Area de Desenvolvimento Economico','neighborhood']='Area de Desenvolvimento Economico (Nucleo Bandeirante)'\n","67125b1d":"# Defining the AR list with the respective neighborhood\n\n# Plano piloto\nplano_piloto = ['Asa Sul (Plano Piloto)',\n                'Asa Norte (Plano Piloto)',\n                'Setor Noroeste (Plano Piloto)',\n                'Granja do Torto (Plano Piloto)']\n\n# Aguas claras\naguas_claras = ['Norte (Aguas Claras)',\n                'Sul (Aguas Claras)',\n                'Aguas Claras',\n                'Area de Desenvolvimento Economico (Aguas Claras)',\n                'Setor Habitacional Arniqueira (Aguas Claras)']\n\n\n# Taguatinga\ntaguatinga = [ 'Taguatinga',\n            'Taguatinga Centro (Taguatinga)',\n            'Setor Hoteleiro (Taguatinga)',\n            'Taguatinga Sul (Taguatinga)',\n            'Taguatinga Norte (Taguatinga)',\n            'Setor Industrial (Taguatinga)']\n\n# Lago sul\nlago_sul = ['Setor de Habitacoes Individuais Sul (Lago Sul)']\n\n# Lago norte\nlago_norte = ['Setor de Habitacoes Individuais Norte (Lago Norte)']\n        \n# Samambaia\nsamambaia = ['Samambaia Sul (Samambaia)',\n            'Samambaia Norte (Samambaia)',\n            'Samambaia']\n   \n# Ceilandia\n\nceilandia = ['Ceilandia Sul (Ceilandia)',\n        'Condominio Prive Lucena Roriz (Ceilandia)',\n        'Area de Desenvolvimento Economico (Ceilandia)',\n        'Ceilandia Norte (Ceilandia)',\n        'Setor Industrial (Ceilandia)']\n \n# Cruzeiro\ncruzeiro = ['Cruzeiro Novo',\n            'Cruzeiro Velho']\n\n\n# Sudoeste\/Octogonal\nsud_oct = ['Area Octogonal',\n        'Setor Sudoeste']\n        \n# Guara        \nguara = ['Zona Industrial (Guara)',\n        'Quadras Economicas Lucio Costa (Guara)',\n        'Setores Complementares (Guara)',\n        'Zona Industrial (Guara)',\n        'Guara II (Guara)',\n        'Guara I (Guara)']  \n        \n# Gama\ngama = ['Gama',\n    'Setor Oeste (Gama)',\n    'Setor Sul (Gama)',\n    'Ponte Alta Norte (Gama)',\n    'Setor Central (Gama)',\n    'Setor Leste (Gama)']\n\n\n# Brazlandia       \nbrazlandia = ['Setor Norte (Brazlandia)',\n            'Vila Sao Jose (Brazlandia)']       \n   \n# Areal   \narniqueira = ['Areal']\n\n# Riacho Fundo I       \nriacho_fundo = ['Riacho Fundo I']\n\n# Riacho Fundo II\nriacho_fundoII = ['Riacho Fundo II']\n    \n# Recanto das Emas    \nrecanto = ['Recanto das Emas']\n    \n# Park Way\npark_way = ['Nucleo Rural Vargem Bonita (Park Way)']\n\n# Sobradinho\nsobradinho = ['Sobradinho',\n            'Grande Colorado (Sobradinho)',\n            'Setor Habitacional Contagem (Sobradinho)',\n            'Setor Habitacional Fercal (Sobradinho)',\n            'Condominio Imperio dos Nobres (Sobradinho)',\n            'Condominio Vale dos Pinheiros (Sobradinho)']\n\n# Sobradinho II\nsobradinhoII = ['Setor Oeste (Sobradinho II)']\n\n# Vicente pires\nvic_pires = ['Setor Habitacional Vicente Pires']\n\n\n# Nucleo Bandeirante      \nnucleo = ['Nucleo Bandeirante',\n        'Setor Placa da Mercedes (Nucleo Bandeirante)',\n        'Area de Desenvolvimento Economico (Nucleo Bandeirante)',\n        'Setor de Industrias Bernardo Sayao (Nucleo Bandeirante)']\n\n# Santa maria \nsanta_maria = ['Santa Maria',\n            'Residencial Santos Dumont (Santa Maria)',\n            'Setor Meireles (Santa Maria)']\n       \n       \n# Candangolandia  \ncandan = ['Candangolandia']\n    \n# Planaltina\nplanaltina = [\"Condominio Mestre D'Armas (Planaltina)\",\n            \"Fazenda Mestre D'Armas (Etapa III - Planaltina)\"]\n    \n# S\u00e3o sebasti\u00e3o \nsao_sebastiao = ['Crixa (Sao Sebastiao)',\n            'Morro Azul (Sao Sebastiao)',\n            'Complexo Urbanistico Aldeias do Cerrado (Sao Sebastiao)',\n            'Jardins Mangueiral (Sao Sebastiao)',\n            'Setor Residencial Oeste (Sao Sebastiao)',\n            'Joao Candido (Sao Sebastiao',\n            'Sao Bartolomeu (Sao Sebastiao)']\n\n# Jardim Botanico\njard_bot = ['Setor Habitacional Jardim Botanico']\n\n# Paranoa\nparanoa = ['Paranoa']\n\n","ab14eb0a":"# Setting the AR column looking for the value in neighborhood list\n\ndf.loc[df['neighborhood'].isin(plano_piloto),'AR']='Plano Piloto'\ndf.loc[df['neighborhood'].isin(aguas_claras),'AR']='Aguas Claras'\ndf.loc[df['neighborhood'].isin(taguatinga),'AR']='Taguatinga'\ndf.loc[df['neighborhood'].isin(lago_sul),'AR']='Lago Sul'\ndf.loc[df['neighborhood'].isin(lago_norte),'AR']='Lago Norte'\ndf.loc[df['neighborhood'].isin(samambaia),'AR']='Samambaia'\ndf.loc[df['neighborhood'].isin(ceilandia),'AR']='Ceilandia'\ndf.loc[df['neighborhood'].isin(cruzeiro),'AR']='Cruzeiro'\ndf.loc[df['neighborhood'].isin(sud_oct),'AR']='Sudoeste\/Octogonal'\ndf.loc[df['neighborhood'].isin(guara),'AR']='Guara'\ndf.loc[df['neighborhood'].isin(gama),'AR']='Gama'\ndf.loc[df['neighborhood'].isin(brazlandia),'AR']='Brazlandia'\ndf.loc[df['neighborhood'].isin(arniqueira),'AR']='Arniqueira'\ndf.loc[df['neighborhood'].isin(riacho_fundo),'AR']='Riacho Fundo'\ndf.loc[df['neighborhood'].isin(riacho_fundoII),'AR']='Riacho Fundo II'\ndf.loc[df['neighborhood'].isin(recanto),'AR']='Recanto das Emas'\ndf.loc[df['neighborhood'].isin(park_way),'AR']='Park Way'\ndf.loc[df['neighborhood'].isin(sobradinho),'AR']='Sobradinho'\ndf.loc[df['neighborhood'].isin(sobradinhoII),'AR']='Sobradinho II'\ndf.loc[df['neighborhood'].isin(vic_pires),'AR']='Vicente Pires'\ndf.loc[df['neighborhood'].isin(nucleo),'AR']='Nucleo Bandeirante'\ndf.loc[df['neighborhood'].isin(santa_maria),'AR']='Santa Maria'\ndf.loc[df['neighborhood'].isin(candan),'AR']='Candangolandia'\ndf.loc[df['neighborhood'].isin(planaltina),'AR']='Planaltina'\ndf.loc[df['neighborhood'].isin(sao_sebastiao),'AR']='Sao Sebastiao'\ndf.loc[df['neighborhood'].isin(jard_bot),'AR']='Jardim Botanico'","03f9b329":"df.info()","69e40362":"# Wrong addresses\n\ndf = df[df[\"neighborhood\"]!=\"Centro (Sao Sebastiao)\"]\ndf = df[df[\"neighborhood\"]!=\"Del Lago II (Itapoa)\"]\ndf = df[df[\"neighborhood\"]!=\"Setor Habitacional Sol Nascente (Ceilandia)\"]","252bc2a1":"df[pd.isna(df['AR'])]","e37d59b5":"df = df.replace(\"\", np.nan)","0facf794":"# Looking the new address column null values\n\ndf[pd.isnull(df['address_'])]","f76484f0":"# Defining a dataframe with the unique neighborhood and AR names\n\nneighborhood = pd.DataFrame(df['neighborhood'].unique(),columns=['neighborhood']).dropna()\n\nneighborhood['neighborhood'] = neighborhood['neighborhood'].str.replace('(',' - (')\n\nneighborhood.head()","178e442d":"# Splitting  in the neighborhood from AR and creating a unique df column called neighborhood_AR\n\nsplit_neighborhood = neighborhood['neighborhood'].apply(lambda x: pd.Series(x.split('-')))\nneighborhood2 = pd.DataFrame(split_neighborhood[1].unique().tolist(),columns=['neighborhood'])\n\nsplit_neighborhood = split_neighborhood.drop([1], axis=1)\nsplit_neighborhood = split_neighborhood.rename(columns={0: \"neighborhood\"})\n\nsplit_neighborhood = split_neighborhood.append(neighborhood2).dropna()\nsplit_neighborhood = split_neighborhood['neighborhood'].apply(lambda x: pd.Series(x.replace(')','').replace('(','').strip()))\n\nneighborhood_AR = split_neighborhood[0].unique().tolist()","acb70d7d":"# Looking for null addresses in old address columns removing \"DF\" (Federal District, the brazilian federative unit),\n# \"Bras\u00edlia\" (the city) and values from neighborhood_AR dataframe.\n\nfor index, row in df.iterrows():\n    \n    if pd.isna(df['address_'][index]):\n        nan = df['address'][index].replace('DF','').replace('Bras\u00edlia','').replace('Brasilia','').replace('-',\",\").split(',')\n        l = []\n\n        for i in nan:\n            i = unidecode(i)\n            i = i.strip()\n            l.append(i)\n\n        l2 = [x for x in l if x.strip() not in neighborhood_AR]\n        df['address_'][index] = l2[0]\n        \n\n        ","b5ba7ff7":"len(df[pd.isnull(df['address_'])])","6d9368b9":"df.info()","ecb3fa4a":"df = df.drop(columns = ['address'],axis=1)","d4ef796e":"# Rooms\n\ndf[\"rooms\"] = df[\"rooms\"].fillna(df.groupby([\"address_\"])[\"rooms\"].transform(\"median\"))\ndf[\"rooms\"] = df[\"rooms\"].fillna(df.groupby([\"neighborhood\"])[\"rooms\"].transform(\"median\"))\ndf[\"rooms\"] = df[\"rooms\"].fillna(df.groupby([\"AR\"])[\"rooms\"].transform(\"median\"))\n\ndf[\"rooms\"] = df[\"rooms\"].astype(int)","d6eb20a9":"df.info()","7502f03b":"# Bathrooms\n\ndf[\"bathrooms\"] = df[\"bathrooms\"].fillna((df.groupby([\"address_\"])[\"bathrooms\"].transform(\"median\")))\ndf[\"bathrooms\"] = df[\"bathrooms\"].fillna(df.groupby([\"neighborhood\"])[\"bathrooms\"].transform(\"median\"))\ndf[\"bathrooms\"] = df[\"bathrooms\"].fillna(df.groupby([\"AR\"])[\"bathrooms\"].transform(\"median\"))\n\ndf[\"bathrooms\"] = df[\"bathrooms\"].astype(int)","2507b0fc":"df.info()","4e2f3a2f":"# Garages\n\ndf[\"garages\"] = df[\"garages\"].fillna((df.groupby([\"address_\"])[\"garages\"].transform(\"median\")))\ndf[\"garages\"] = df[\"garages\"].fillna(df.groupby([\"neighborhood\"])[\"garages\"].transform(\"median\"))\ndf[\"garages\"] = df[\"garages\"].fillna(df.groupby([\"AR\"])[\"garages\"].transform(\"median\"))\n\ndf[\"garages\"] = df[\"garages\"].fillna(1)\n\ndf[\"garages\"] = df[\"garages\"].astype(int)","4f344931":"df.info()","c9f5630a":"# Condo Fees\n\ndf[\"condo_fees\"] = df[\"condo_fees\"].fillna((df.groupby([\"address_\"])[\"condo_fees\"].transform(\"median\")))\ndf[\"condo_fees\"] = df[\"condo_fees\"].fillna(df.groupby([\"neighborhood\"])[\"condo_fees\"].transform(\"median\"))\ndf[\"condo_fees\"] = df[\"condo_fees\"].fillna(df.groupby([\"AR\"])[\"condo_fees\"].transform(\"median\"))\n","81d611bd":"df.info()","5510304a":"# Value \n\ndf[\"price\"] = df[\"price\"].fillna((df.groupby([\"address_\"])[\"price\"].transform(\"median\")))\ndf[\"price\"] = df[\"price\"].fillna(df.groupby([\"neighborhood\"])[\"price\"].transform(\"median\"))\ndf[\"price\"] = df[\"price\"].fillna(df.groupby([\"AR\"])[\"price\"].transform(\"median\"))\n","f7d86984":"df.info()","3ee5d9f6":"# Dropping columns where all values are null\n\ndf = df.dropna(axis=0)\n\ndf.info()","38da0c66":"df = df.reset_index(drop=True)","67a23676":"from geopy.geocoders import Nominatim\nfrom geopy.exc import GeocoderTimedOut\n\nfrom six.moves import urllib\nimport certifi","3f93c1fa":"def uo(args, **kwargs):\n    return urllib.request.urlopen(args, cafile=certifi.where(), **kwargs)\n\ngeolocator = Nominatim(user_agent='geopy.geocoders.options.default_user_agent = \"my-application\"')\ngeolocator.urlopen = uo","6eac2485":"def lat_long (address,ar,neighborhood):\n    \n    try:\n\n        location = geolocator.geocode(str(address+\" \"+ar+\" Bras\u00edlia Brazil\"),timeout=1)\n\n        if (\"Distrito Federal\" in str(location))&(\"Microrregi\u00e3o\" not in str(location)):\n            return pd.Series([location.latitude,location.longitude])\n        else:\n            pass\n    except:\n            try:\n                location = geolocator.geocode(str(address+\" \"+ar+' Distrito Federal Brazil'),timeout=1)\n                \n                if (\"Distrito Federal\" in str(location))&(\"Microrregi\u00e3o\" not in str(location)):\n                    return pd.Series([location.latitude,location.longitude])\n                else:\n                    pass\n            except:\n                    try:\n                        location = geolocator.geocode(str(address+\" \"+ neighborhood +\" Bras\u00edlia Brazil\"),timeout=1)\n                        \n                        if (\"Distrito Federal\" in str(location))&(\"Microrregi\u00e3o\" not in str(location)):\n                            return pd.Series([location.latitude,location.longitude])\n                        else:\n                            pass\n                    except:\n                            try:\n                                location = geolocator.geocode(str(address+\" \"+neighborhood+' Distrito Federal Brazil'),timeout=1)\n                                \n                                if (\"Distrito Federal\" in str(location))&(\"Microrregi\u00e3o\" not in str(location)):\n                                    return pd.Series([location.latitude,location.longitude])\n                                else:\n                                    pass\n                            except: \n                                    try:\n                                        lst = address.split(\" \")[-5:]\n                                        location = geolocator.geocode(str(str(lst[:-2])+\" \"+ar+\" Bras\u00edlia Brazil\"),timeout=1)\n                                        \n                                        if (\"Distrito Federal\" in str(location))&(\"Microrregi\u00e3o\" not in str(location)):\n                                            return pd.Series([location.latitude,location.longitude])\n                                        else:\n                                            pass\n                                    except:\n                                            try:\n                                                location = geolocator.geocode(str(str(lst[:-2])+\" \"+ar+\" Distrito Federal Brazil\"),timeout=1)\n                                                \n                                                if (\"Distrito Federal\" in str(location))&(\"Microrregi\u00e3o\" not in str(location)):\n                                                    return pd.Series([location.latitude,location.longitude])\n                                                else:\n                                                    pass\n                                            except: \n                                                    try:\n                                                        lst = address.split(\" \")[-5:]\n                                                        location = geolocator.geocode(str(str(lst[:-2])+\" \"+neighborhood+\" Bras\u00edlia Brazil\"),timeout=1)\n\n                                                        if (\"Distrito Federal\" in str(location))&(\"Microrregi\u00e3o\" not in str(location)):\n                                                            return pd.Series([location.latitude,location.longitude])\n                                                        else:\n                                                            pass\n                                                    except:\n                                                            try:\n                                                                location = geolocator.geocode(str(str(lst[:-2])+\" \"+neighborhood+\" Distrito Federal Brazil\"),timeout=1)\n                                                                \n                                                                if (\"Distrito Federal\" in str(location))&(\"Microrregi\u00e3o\" not in str(location)):\n                                                                    return pd.Series([location.latitude,location.longitude])\n                                                                else:\n                                                                    pass\n                                                            except:\n                                                                    pass\n                                                                ","0a00ccc5":"df['latitude'] = ''\ndf['longitude'] = ''","e15f2f0b":"df.info()","8c1e71c5":"%%time\n\ndf[[\"latitude\", \"longitude\"]] = df.apply(lambda x: lat_long(x[\"address_\"],x[\"AR\"],x[\"neighborhood\"]),axis=1)                                       ","02c63c95":"df['latitude'].value_counts(dropna=False)","d93f811e":"# NAN lat long to the mean of address, neighborhood or RA\n\ndf[[\"latitude\", \"longitude\"]] = df[[\"latitude\", \"longitude\"]].fillna(df.groupby([\"address_\"])[[\"latitude\",\"longitude\"]].transform(\"mean\"))  \ndf[[\"latitude\", \"longitude\"]] = df[[\"latitude\", \"longitude\"]].fillna(df.groupby([\"neighborhood\"])[[\"latitude\",\"longitude\"]].transform(\"mean\"))\ndf[[\"latitude\", \"longitude\"]] = df[[\"latitude\", \"longitude\"]].fillna(df.groupby([\"AR\"])[[\"latitude\",\"longitude\"]].transform(\"mean\"))                                                                     ","90e9869e":"df['latitude'].value_counts(dropna = False).head()","35149bc0":"# Neighborhoods without latitude\n \ndf[pd.isna(df['latitude'])]['neighborhood'].value_counts()","58d7b161":"# Address from Nucleo Rural Vargem Bonita (Park Way) without latitude\n \ndf[(pd.isna(df['latitude']))&(df['neighborhood']=='Nucleo Rural Vargem Bonita (Park Way)')]['address_'].value_counts()","2d5bc05a":"df.loc[(df['neighborhood']=='Nucleo Rural Vargem Bonita (Park Way)')&(df['address_']=='Nucleo Rural Vargem Bonita Rua 2'),['latitude','longitude']] = -15.9253737,-47.9460275\n \ndf[(pd.isna(df['latitude']))&(df['neighborhood']=='Nucleo Rural Vargem Bonita (Park Way)')]['address_'].value_counts()","be49a61f":"# Address from Candangolandia awithout latitude and longitude\n\ndf[(pd.isna(df['latitude']))&(df['neighborhood']=='Candangolandia')]['address_'].value_counts()","85f22f24":"df.loc[(df['neighborhood']=='Candangolandia')&(df['address_']=='EQR 5\/7'),['latitude','longitude']] = -15.8514476,-47.9533786\n         \ndf[(pd.isna(df['latitude']))&(df['neighborhood']=='Candangolandia')]['address_'].value_counts()","85faab63":"df[pd.isna(df['latitude'])]","7e936e52":"df = df.dropna(axis=0)","3e29f831":"df = df.reset_index(drop=True)","dcdf91d8":"from math import cos, asin, sqrt, pi","beb75192":"def distance(lat1, lon1, lat2, lon2):\n    p = pi\/180\n    a = 0.5 - cos((lat2-lat1)*p)\/2 + cos(lat1*p) * cos(lat2*p) * (1-cos((lon2-lon1)*p))\/2\n    return 12742 * asin(sqrt(a)) ","e0485ff2":"# Calculating the distance to the downtown of all apartments\n\ndf[\"distance to downtown - km\"]= df.apply(lambda x:distance(x[\"latitude\"],\n                                                 x[\"longitude\"],\n                                                -15.794228,-47.882165), axis=1)","5677d0aa":"df.head()","5e87f9f5":"resut_list = []","260d1576":"url = \"https:\/\/pt.wikipedia.org\/wiki\/Lista_de_regi%C3%B5es_administrativas_do_Distrito_Federal_por_renda_per_capita\"\n    \n\ndriver.get(url)\n    \nsoup = BeautifulSoup(driver.page_source, 'html.parser')\n    \nra_div = soup.find_all('tr')\n    \n\nfor row in ra_div:\n    cell = row.find_all('td')\n    resut_dict = {}\n    if len(cell) == 3:\n        \n        if len(cell[1].find_all('a'))==2:\n            resut_dict['AR']=cell[1].find_all('a')[1].get('title') \n        else:\n            resut_dict['AR']=cell[1].find('a').get('title')\n        \n        resut_dict['Avg. PCI'] = cell[2].find('b').text\n        \n        resut_list.append(resut_dict)","b7878c64":"pci = pd.DataFrame(resut_list)","f12c0960":"# Cleaning the data extracted\n\npci['AR'] = pci['AR'].str.replace(r\"\\(.*\\)\",\"\").str.strip()\npci['AR'] = pci['AR'].apply(unidecode)\npci['Avg. PCI'] = pci['Avg. PCI'].str.replace('.','').str.replace(',','.').astype(float)","2017e74b":"pci.info()","a003b64c":"pci.loc[pci['AR'] == 'Regiao administrativa de Brasilia','AR'] = 'Plano Piloto'","d808b96d":"pci","45653e16":"df['AR'].value_counts(dropna=True)","9bda4af8":"df.shape","a24a6e4e":"# Merging the df and PCI on AR\n\ndf = df.merge(pci, on='AR', how='left')","37413e20":"df.shape","5c2c1976":"# Looking for Avg. PCI null values\n\ndf[pd.isna(df['Avg. PCI'])]['AR'].unique()","ca0ed5f5":"# PCI anrniqueiras 2086.88\n\ndf.loc[df['AR'] == 'Arniqueira','Avg. PCI'] = 2086.88","c65ebcc6":"df[pd.isna(df['Avg. PCI'])]['AR'].unique()","7521d330":"df.head()","63fabe74":"resut_list = []","362f3f12":"url = \"https:\/\/pt.wikipedia.org\/wiki\/Lista_de_regi%C3%B5es_administrativas_do_Distrito_Federal_por_popula%C3%A7%C3%A3o\"\n    \n\ndriver.get(url)\n    \nsoup = BeautifulSoup(driver.page_source, 'html.parser')\n    \nra_div = soup.find_all('tr')\n    \n\nfor row in ra_div:\n    cell=row.find_all('td')\n    resut_dict = {}\n    if len(cell) == 3:\n        \n        if len(cell[1].find_all('a')) == 2:\n            resut_dict['AR'] = cell[1].find_all('a')[1].get('title') \n        else:\n            resut_dict['AR'] = cell[1].find('a').get('title')\n        \n        resut_dict['population'] = cell[2].text\n        \n        resut_list.append(resut_dict)","8523b5f6":"population = pd.DataFrame(resut_list)","271eadc0":"population","9a470e1b":"# Cleaning the data extracted\n\npopulation['AR'] = population['AR'].str.replace(r\"\\(.*\\)\",\"\").str.strip()\npopulation['AR'] = population['AR'].apply(unidecode)\npopulation['population'] = population['population'].apply(unidecode).str.replace('\\n','').str.replace(' ','').astype(int)","7f984e9e":"population.loc[population['AR'] == 'Regiao administrativa de Brasilia','AR']='Plano Piloto'","e31849e9":"population","e290ea90":"population = population.append({\"AR\":'Arniqueira',\"population\":45091},ignore_index=True)\n\npopulation.loc[population['AR'] == 'Aguas Claras',\"population\"]=(148940-45091)","e9d1d1ea":"# Merging the df and Population on AR\n\ndf = df.merge(population, on = 'AR', how = 'left')","8bf8cd68":"df[pd.isna(df['population'])]['AR'].unique()","d0f6731b":"df.head()","99db24a2":"df.to_csv(\"Brazilian_Apartaments_All_Features.csv\")","3e1cfc28":"import folium\nfrom folium.plugins import FastMarkerCluster\n\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt","d2c4e403":"# Creating a map\n\nm = folium.Map(location = [np.median(df['latitude'].tolist()),\n                          np.median(df['longitude'].tolist())],\n                          tiles =  'stamenterrain',\n                          zoom_start = 12)\n\n\n\nfg = folium.FeatureGroup(name ='Apt Info')\n\n\n# Informations from to be shown in the markers\n\ncallback = ('function (row) {' \n                'var marker = L.marker(new L.LatLng(row[0], row[1]));'\n                \"var popup = L.popup({maxWidth: '300'});\"\n                \"const display_text = {text1: row[2],text2:row[3],text3:row[4],text4:row[5]};\"\n                \"var mytext = $(`<div id='mytext' class='display_text' style='width: 100.0%;height: 100.0%;'> ${display_text.text1}<br><br>RA - ${display_text.text2}<br>Address - ${display_text.text3}<br>Price - ${display_text.text4}<\/div>`)[0];\"\n                \"popup.setContent(mytext);\"\n                \"marker.bindPopup(popup);\"\n                'return marker};')\n                             \n# Plotting \n\nm.add_child(FastMarkerCluster(df[['latitude', 'longitude','titles','AR','address_','price']].values.tolist(), callback=callback))\n\n","b405f1bf":"# Drop the 'latitude','longitude','titles','address_' and 'AR'columns\n\ncol = ['price',\n     'rooms',\n     'bathrooms',\n     'garages',\n     'condo_fees',\n     'areas m\u00b2',\n     'distance to downtown - km',\n     'Avg. PCI',\n     'population',\n     'neighborhood'\n    ]\n\ndf = df[col]","c7292659":"import seaborn as sns\nimport matplotlib.pyplot as plt","c1e00b6c":"df.columns","52e61f2c":"cols = df.columns[:-4].tolist()","c8d75a27":"df_n = df[cols]\n\n# Plotting the boxplot of the 5 firsts features for look for inaccuracies and outliers.\n\nl = df_n.columns.values\nncols = int(len(l))\nnrows = 1\n\nfig,ax2d = plt.subplots(nrows,ncols)\nfig.set_size_inches(15,5)\nfig.subplots_adjust(wspace=3)\n\nax = np.ravel(ax2d)\nax[-2].ticklabel_format(style='plain')\nax[-1].ticklabel_format(style='plain')\n\nfor count,i in enumerate(df_n):\n    \n    sns.boxplot(y = df_n[i],ax = ax[count])","8af0dbcc":"# Removing higher outliers from prices\nnew_df = df[df['price']<80000000]\n\n# Removing higher outliers from areas m\u00b2\n \nnew_df = new_df[new_df['areas m\u00b2']<500]\n\n# Removing outliers higher from garages \nnew_df = new_df[new_df['garages']<20]\n\n# Removing  higher outliers from condo_fees \n\nnew_df = new_df[new_df['condo_fees']<5000]\n\n#Probably the rooms equal 20 are equal to 2, let's change that\n\nnew_df.loc[new_df['rooms']==20,'rooms']=2\n","ad73ffc0":"df_n = new_df[cols]\n\n# Ploting the box plot again\n\nl = df_n.columns.values\nncols = int(len(l))\nnrows = 1\n\n\nfig,ax2d = plt.subplots(nrows,ncols)\nfig.set_size_inches(15,6)\nfig.subplots_adjust(wspace=3)\n\n\nax = np.ravel(ax2d)\nax[-1].ticklabel_format(style='plain')\n\nfor count,i in enumerate(df_n):\n    \n    sns.boxplot(y = df_n[i],ax = ax[count])","6a25c353":"new_df.shape","0f460997":"dist = new_df[new_df.columns[:-1]]","1547b995":"# Plotting the distribution to see the curve\n\nfig, axes = plt.subplots(nrows=1, ncols=9)\nfig.set_size_inches(20,2)\nfig.subplots_adjust(hspace=0.5,wspace=0.5)\n\nfor i,column in enumerate(dist.columns):\n    sns.distplot(dist[column], ax=axes[i],kde_kws={'bw': 0.5})","32caa926":"from sklearn import preprocessing","1570282b":"# Normalization and scaling \n\nlog_scale = pd.DataFrame(preprocessing.scale(np.log(dist)))","cffdb4af":"cols_name = dist.columns\nlog_scale.columns = cols_name","eab142a7":"# Plotting again the distribution after normalize and scale\n\nfig, axes = plt.subplots(nrows=1, ncols=9)\nfig.set_size_inches(20,2)\nfig.subplots_adjust(hspace=0.5,wspace=0.5)\n\nfor i,column in enumerate(log_scale.columns):\n    sns.distplot(log_scale[column], ax=axes[i],kde_kws={'bw': 0.5})","8682ccb4":"# Plotting the Heat Map to better understand the feature correlations\n\ncorrmat = log_scale.corr()\nplt.subplots(figsize=(12,9))\nsns.heatmap(corrmat, vmax=0.9, square=True, cmap='coolwarm');","7e19fa67":"df.to_csv(\"Brazilian_Apartaments_Modeling.csv\")","0e732508":"from sklearn.model_selection import train_test_split","ac1a6d13":"# Creating a df with the categorical values converted to dummies\n\nneighborhood = pd.get_dummies(new_df)\nneighborhood = neighborhood[neighborhood.columns[9:]]","6087de93":"# Defining X with the merging df of the log_scale and neighborhood\n\nX = pd.merge(log_scale[log_scale.columns[1:]],neighborhood,how = 'left',left_index = True, right_index = True)\n\n# Defining y as the value without scale or log normalization\n\ny = new_df[new_df.columns[0]]","9c7e4edc":"X_train, X_test, y_train, y_test = train_test_split( X, y, test_size=0.30, random_state=42, shuffle=False)\n\nprint('Train Len - ',len(X_train))\nprint('Test Len - ',len(X_test))","3768e46c":"from sklearn.model_selection import KFold, cross_val_score, train_test_split\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import GridSearchCV\n\nfrom sklearn.linear_model import ElasticNet, Lasso  \nfrom sklearn.kernel_ridge import KernelRidge\nimport xgboost as xgb\nimport lightgbm as lgb \nfrom lightgbm import LGBMRegressor","7bcabe58":"%time\n\npipe = Pipeline([('regression', Lasso())])\n\ngrid_param = [ \n                {'regression':[Lasso()],\n                 'regression__alpha':np.linspace(0.001,10, 10),\n                },\n                {\n                 'regression':[KernelRidge()],\n                 'regression__alpha':np.linspace(0.001,10, 10),\n                 'regression__kernel':['polynomial'],\n                },\n    \n                {'regression':[ElasticNet()],\n                 'regression__alpha':np.linspace(0.001,10, 10),\n                },\n                \n                {'regression':[xgb.XGBRegressor()],\n                 'regression__colsample_bytree':[1],\n                 'regression__max_depth':[10,15,20],\n                 'regression__gamma':np.linspace(0.0,0.5,3),\n                 'regression__n_estimators':[1500,2000,3000],\n                 'regression__learning_rate':np.linspace(0.005,0.05, 3),\n                 'regression__eval_metric':['rmse']\n                },\n                \n                {'regression':[lgb.LGBMRegressor()],\n                 'regression__objective':['regression'],\n                 'regression__max_depth':[10,15,20],\n                 'regression__learning_rate':np.linspace(0.005,0.05, 3),\n                 'regression__n_estimators':[1500,2000,3000],\n                 'regression__feature_fraction':[1],\n                 'regression__metric':['rmse'],\n                 }\n    \n            ]\nn_folds = 10\nkf = KFold(n_folds, shuffle=True, random_state=42).get_n_splits(X_train.values)\ngridsearch = GridSearchCV(pipe, grid_param, verbose=0, n_jobs=-1, cv = kf, scoring='neg_root_mean_squared_error')\nbest_model = gridsearch.fit(X_train,preprocessing.scale(np.log(y_train)))","355a057f":"best_model.best_estimator_","de5abb34":"nan = None\nbest_model = best_model.best_estimator_[0]","727808d0":"model = best_model.fit(X_train,y_train)","375150ae":"model.score(X_train,y_train)","bbc7caf0":"model.score(X_test, y_test)","98904d56":"pd.options.display.float_format = \"{:.2f}\".format\n","d8de27f9":"y_pred = model.predict(X_test)\n\npred = pd.DataFrame(y_pred).reset_index(drop=True)\ntest = pd.DataFrame(y_test).reset_index(drop=True)\n\npd.merge(test,pred,how = 'left',left_index = True, right_index = True).rename(columns = {'value': 'test', 0: 'pred'}, inplace = False)","b14f0f59":"from yellowbrick.regressor import PredictionError","8703a618":"visualizer = PredictionError(model)\n\nvisualizer.fit(X_train, y_train.values)  # Fit the training data to the visualizer\nvisualizer.score(X_test, y_test.values)  # Evaluate the model on the test data\nvisualizer.show()   \n","6c747a52":"import sklearn.metrics as sm","d361cc3d":"test = preprocessing.scale(y_test)\npred = preprocessing.scale(y_pred)\n\nprint(\"Mean absolute error =\", round(sm.mean_absolute_error(test, pred), 5)) \nprint(\"Mean squared error =\", round(np.sqrt(sm.mean_squared_error(test, pred)), 5)) \nprint(\"Median absolute error =\", round(sm.median_absolute_error(test, pred), 5)) \nprint(\"Explain variance score =\", round(sm.explained_variance_score(test, pred), 5))\nprint(\"R2 score =\",round(sm.r2_score(test, pred),5))","f08987e2":"import joblib","2024489f":"joblib.dump(model, 'apt_value_pred_model.sav')","7c28188a":"<a id=\"pred\"><\/a>\n## 10.2 Predictions","b8fa6a7e":"<a id=\"vnv\"><\/a>\n### 6.2.5 Value null values","c2e8cc92":"<a id=\"gl\"><\/a>\n## 7.1 Geo Location","6949bed7":"<a id=\"dars\"><\/a>\n### 6.1.3  Defining Administrative Regions (AR)","0b23d429":"Finally the results! Here we are going to calculate the score on the train and in the test data frame to look for overfitting. Also we will see the predictions as real values, plot the **prediction error** of our model using the **yellow brick** and calculate the metric using **sklearn metrics**. Lastly we are going to save the model using **joblib**.","b2ed5ed5":"# Price Prediction - Bras\u00edlia Apartments","1bca73a9":"<a id=\"p\"><\/a>\n## 7.4 Population","d6aec44b":"<a id=\"rnv\"><\/a>\n### 6.2.1 Room Null Values\n","2b3a045f":"<a id=\"M\"><\/a>\n# 9 Modeling","a32bb390":"<a id=\"Dc\"><\/a>\n# 5 Data Check ","00ebdca4":"<a id=\"c\"><\/a>\n## 8.6 Correlation Plot","2048beb0":"\n<a id=\"mp\"><\/a>\n## 8.1 Map plot","eb6f3352":"<a id=\"dp\"><\/a>\n## 8.3 Distribution Plot","9f46ffe8":"The data that we will use was extraction using Web Scraping technique from Viva Real Website, one of the Brazilians biggest real estate seller websites. We extracted information about apartments from all over Bras\u00edlia, the capital of Brazil. After query about apartments in Bras\u00edlia in the website each container shows:\nThe address, a title, the area in m\u00b2, the number of rooms, number of bathrooms, number of garages, the condo fees, the price and amenidades items as shown in the image below.\n\n<br>\n<br>\n\n\n![Screen%20Shot%202020-12-02%20at%2017.51.15.png](attachment:Screen%20Shot%202020-12-02%20at%2017.51.15.png)\n\n<br>\n<br>\n\nPs: Most of the time those informations are not completely filled for the apartaments owners.","52aaa623":"<a id=\"dex\"><\/a>\n# 4 Data Extraction","28c197e4":"<a id=\"sm\"><\/a>\n## 10.5 Saving the model","f81c5813":"<a id=\"pep\"><\/a>\n## 10.3 Prediction Error Plot","efd73399":"Way better...","d1e92184":"To help us understand and prepare the DataFrame for the modeling we are going to visualize the data.\n\n * Firt, before we drop latitude and longitude, we are going to plot the apartments on the map.\n * Second, we will plot the DF as Box Plot and look for some inaccuracy.\n * Third, for understanding the normalization, we are going to plot the distribution of our continuous variables. If necessary we will normalize and Scale them.\n * Fourth, to visualize the personal correlation we will use a Heat Map.\nAt the end of this session the data will be ready for the modeling.\n","7dcb1bb9":"The Administrative Regions of the Federal District are divisions very similar to the regular cities with their particular administration, address system and neighborhoods.\n\nHere we are going searching on the web to create a list for each AR containing the neighborhoods of each one of them.\n","c3679a9f":"<a id=\"nanv\"><\/a>\n### 6.1.4 New Address Null Values","cc0064fa":"<a id=\"pnv\"><\/a>\n\n## 5.1 Null values","601f991e":" Arniqueiras population was 45091 in 2015 and that was part of Aguas Claras so we have to subtract this value from Aguas claras\n","02b27e0e":"Now that we got all the address information possibles, we will be able to find the more appropriate values for the Apartments characteristics looking for the median of the addresses, neighborhoods or RAs to fill the null values with a high precision. \nIs important to look for these values in this order, because we can prioritize the median of close apartments.","b2c26468":"Web Scraping the Wikipedia getting the AR and PCI","f60ba5ce":"<a id=\"bnv\"><\/a>\n### 6.2.2 Bathrooms Null Values","dd8b3b47":"The main idea of this notebook was accomplished.  We deal with a relatively small data set extracted with Web Scraping and with hard work in the feature engineering, were able to build a very good predictor of prices for apartments in Bras\u00edlia. \nThat was my first regression project and I really wanna know what you guys think about it.\n\n<h3>If you like the notebook, please upvote and leave your feedback in the comments' section !<\/h3>\n\n<br>\nThat's it for now.\n\n<br>\n<br>\n\nCheers \ud83c\udf40\n\n<br>\n<br>\n<br>\n<br>\n\n\n\n\n\n\n\n","8ddd8787":"<a id=\"FE\"><\/a>\n\n# 6 Feature Engineering","cfcb3d70":"<a id=\"nnv\"><\/a>\n### 6.1.2 Neighborhood Null Values","a351a146":"<a id=\"dttbd\"><\/a>\n## 7.2  Distance to the Bras\u00edlia Downtown\n","f767df84":"Now there are 847 missing values in Neighborhood and the new Address column.","aebbe6dc":"<a id=\"CON\"><\/a>\n# 11 Conclusion","2cd552c8":"To choose the best model we will try **Lasso (l1)**, **Kernel Ridge (l2)**, **Elastic NNet**, **Xgb Regressor** or **Lgbm Regressor** . We are going to create a **Pipeline** with **Hyperparameter Tuning** using **Grid Search** and 10 **K-Folds** shuffled. The scored used will be **neg_root_mean_squared_error**. Also for the Grid Search we are going to scale and normalize our dependent variable (y_train).","4bc1c5cf":"<a id=\"DV\"><\/a>\n# 8 Data Visualization and Data Preparation","3d081c02":"Filling in the addresses without latitude and longitude, searching for the address's latitude longitude on Google","940e5c2f":"<a id=\"NF\"><\/a>\n# 7 New Features","021357cb":"The two next functions will do the following steps. \n * The first one will send pieces of the address columns to the Busca CEP API. It will return a dataframe with a few addresses and neighborhoods that correspond with the parts sended. \n * The second function will confirm if the parts sent are in the dataframe returned.\n \n After the second function, confirm the data returned and send back to the first function. The first function will set the new address and the neighborhood values.\n\n","764fb3de":"In general we cannot expect to get exactly correct results from a regression model. What we hope for is that your predictions are overall close to the real values.","40b2a8b0":"<a id=\"R\"><\/a>\n# 10 Results","d1e41ae0":"<a id=\"address\"><\/a>\n## 6.1 Address\n\n\n\n","e8f60d5c":"Here we will try a few combinations of addresses to try to find the correct geo location. We will try sometimes the Address, sometimes the AR plus city or plus federative unit. Aways prioritizing sending more information for the geocode. ","577d42fd":"<a id=\"tts\"><\/a>\n## 9.1 Train Test Split","8f066cb0":"As seen above, the graphs are not similar to a Gaussian distribution. Also the scale of the numbers are very different as we expected. \n\nFor the normalization we will use numpy.log and for scale we will use preprocessing scale.","b9416299":"First of all, we will  WebScrapping the Viva Real website. To do that we are going to use the BeautifulSoup package. I want to thank [Mrs. Adativa](https:\/\/www.kaggle.com\/aliceadativa) for this very helpful notebook published here on [Kaggle](https:\/\/www.kaggle.com\/aliceadativa\/web-scraping-com-python-parte-1). Also, I want to thank [Anki Kumar](https:\/\/www.kaggle.com\/ankikumar) that made a [nice job](https:\/\/www.kaggle.com\/ankikumar\/script-for-extracting) using the Chrome Drive on Kaggle Kernel.","d9ff840a":"<a id=\"pci\"><\/a>\n## 7.3 Per Capita Income (PCI)","401963e0":"<a id=\"scor\"><\/a>\n## 10.1 Score R\u02c62","f5b15df9":"So, we ended with a data frame that has 3114 rows and 51 columns.\n\n","1ad92190":"When we get a return from the tries. We are going to check if the address and neighborhood from address_DF returned are equal to the string parts sent. So then we will return a match to the first function to define the two new columns. \"Address_'' and \"Neighborhood\".\n","2f0828ac":"<a id=\"bp\"><\/a>\n## 8.2 BoxPlot ","521529f7":"<a id=\"ws_vr\"><\/a>\n\n## 4.1 Web Scrapping - Vivareal WebSite ","570b2014":"More than 90% of the amenities items are null and we can't know if these values \u200b\u200bare empty because there is not the feature or they forgot to fill those options.\nSo let's drop it.\n","f0554c45":"To use the geolocation in our model we're going to use the Haversine formula to determines the great-circle distance between the latitude and longitude (lat1, lon1) of the apartaments and the Bras\u00edlia downtown latitude longitude -15.794228, -47.882165 (lat2, lon2).\n\nTo do that, I was lucky to find this [answer](https:\/\/stackoverflow.com\/questions\/27928\/calculate-distance-between-two-latitude-longitude-points-haversine-formula) from Salvador Dali on Stack OverFlow. It seems that he is coding now. Shout-out to Mr. Dali. You are a very talented man!\n\n[Haversine on wiki](https:\/\/en.wikipedia.org\/wiki\/Haversine_formula)","f276b3f8":"<a id=\"ach\"><\/a>\n## 6.2 Apartament characteristics","5cfac568":"The main objective of the project was to work with a messy data set, extracted from the web and even with a relatively small dataset, build a good price predictor with a regression model.\n\nIn this notebook we will use Web Scraping technique on the Villa Real website. We will get a few features from the main page of the site. So, we will try to don't waste any information however we will spend a lot of time in the feature engineering processes to be able to build a regression model that gives us a very good score.\n\n\n:)\n\nHave fun!","d167f356":"Web Scraping the Wikipedia getting the AR and Population","adbc5441":"<a id=\"Methodology\"><\/a>\n# Methodology ","3bf5b501":"The Addresses in Bras\u00edlia is very confusing,  The Federal District of Brazil has 33 Administrative Regions (AR). Each of them has their own address system. People often get confused setting the address and probably fill the address wrong on the Vila real website. So our idea here is organize, splitting from the address column, which the property owner filled, into a new address column, neighborhood and the AR. \n\nWe will send pieces of the address values to the correios API (The Brazilian mail service) to return the address organized in Address  and Neighborhood. After that, we are going to search on the web to find the AR for each neighborhood.\n\nThose features organized and accurate will be very important in the future to fill the apartament characteristics with a high local precision and add new features about the apartaments to our data frame.","b0f95b21":"That's seems a very good model, by the way.\ud83e\udd29","629044dd":"Now, to fill Neighborhood null values, we will create a list of unique Neighborhood values to check if they are in the old address columns.","868de127":"<a id=\"pht\"><\/a>\n## 9.2 Pipeline Hyperparameter Tuning","277c211e":"<a id=\"data\"><\/a>\n# Data ","470da044":"For modeling we are going to create a pipeline that also will tuning our parameters based on a cross validation.\n\nHowever, before that we will split into Train Tests a df where the categorical values are converted into dummy and the numeric values are normalized and scaled except for the price of the apartaments. Where we will keep the original prices to compare the prediction.\n","c6066dc4":"Enriching our features to get a better result we will add a few new features. \n * Geo Location, using the GeoPy library to plot the apartments on the map and calculate the Distance to the Bras\u00edlia Downtown\n * Also, we will Web Scraping on Wikipedia to find Per Capita Income (PCI) and Population of each AR.\n","ff6d20a4":"<a id=\"cfnv\"><\/a>\n### 6.2.4 Condo Fees Null Values","fbd6ca59":"<a id=\"mr\"><\/a>\n## 10.4 Metrics Results","225ab5d3":"<a id=\"ltaps\"><\/a>\n## 8.5 Log Transformation and Preprocessing Scale ","42fe6777":"As said before, the aim of this project is to use WebScraping technique to extract data from the web. Cleaning and organizing the data and creating new features to help us to build a good regression model to price preditionns.\n\n * To clean and organize with the data we are going to use the **Pandas** and **Numpy** libraries \n * We are going to use the **Busca CEP API** to organize the address features. With the neighborhood of the apartaments we will be able to create a new feature called Administrative Region (AR).\n * With Address, Neighborhood and AR we will be able to find the best value to fill null values of the apartments characteristics, looking for the median of the characteristics in the same address, same neighborhood or same AR  \n * After that we are going to use **Geopy** to find the geolocation of the apartaments. \n * Using the geolocation and the **Haversine Formula** We are going to calculate a new feature called Distance to the Bras\u00edlia Downtown.\n * Again we will use **Web Scraping** with **BeautifulSoup Library** in Wikipedia to find Per Capita Income (PCI) and Population of the Administrative Regions. \n * For the Data Visualization we are going to use **Folium Library**, **Fast Marker Cluster plugin**, **Seaborn**  and **Matplotlib**.\n * Before find the best **Linear Regression**, to normalize the data we're use **numpy** and to scale we're going to use **preprocessing** \n * To choose the best model between, **Lasso (l1)**, **Kernel Ridge (l2)**, **Elastic NNet**, **Xgb Regressor** or **Lgbm Regressor**  we are going to create a **Pipeline** with **Hyperparameter Tuning** using **Grid Search** and **K-Folds** shuffed.\n * Finally after finding our model we will plot the **PredictionError** using **Yellowbrick** library and use **Sklearn Metrics** to measure the results of our model.\n \n \n So, let's get it started! :)\n","65a416ca":"<a id=\"introduction\"><\/a>\n\n# Introduction ","3fb1db62":"The same technique will be used here, but now we will fill with 1 the places without references of garage","5da8bffb":" Fill NaN ","6166f33f":"<a id=\"gnv\"><\/a>\n### 6.2.3 Garage Null Values","d9fb5e2e":"<a id=\"an\"><\/a>\n### 6.1.1 Address  and  Neighborhood\n\nAs said before, we are going to organize the addresses splitting the column address. Using the [Busca Cep API](https:\/\/github.com\/arthurfortes\/consulta_correios), created by [Arthur Fortes](https:\/\/github.com\/arthurfortes), we will be able to get the correct address and the neighborhood extracted from the Correios Web site.\nRecently, I've also contributed with Busca Cep API on Github to updating the Web Scraping code.\n\n"}}