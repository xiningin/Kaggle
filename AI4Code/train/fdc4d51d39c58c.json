{"cell_type":{"b0aed4b1":"code","1d2160bf":"code","4f6c05e5":"code","90b47dfa":"code","38e77e9d":"code","1f7362ca":"code","b76240fe":"code","730624f1":"code","ecf42eda":"code","c18e97c9":"code","3583cb2c":"code","19979bca":"code","acd9e46a":"markdown","021cad77":"markdown","2354192b":"markdown","3d6d3fab":"markdown","d50f2b0d":"markdown","8bf9d556":"markdown","bfda992c":"markdown","68745c9d":"markdown","65c549eb":"markdown","7f5dcd1f":"markdown","adfc2e81":"markdown","4f82e9e3":"markdown","acae81f3":"markdown"},"source":{"b0aed4b1":"import json\n\nimport pandas as pd\nimport numpy as np\nimport plotly.express as px\nimport tensorflow.keras.layers as L\nimport tensorflow as tf\nimport tensorflow.keras as keras\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.applications.inception_v3 import InceptionV3\nfrom tensorflow.keras import layers","1d2160bf":"def LOSS_MCRMSE(y_true, y_pred):\n    colwise_mse = tf.reduce_mean(tf.square(y_true - y_pred), axis=1)\n    return tf.reduce_mean(tf.sqrt(colwise_mse), axis=1)\n\ndef gru_layer(hidden_dim, dropout):\n    return L.Bidirectional(L.GRU(\n        hidden_dim, dropout=dropout, return_sequences=True, kernel_initializer='orthogonal'))\ndef MCRMSE(y_true, y_pred):\n    colwise_mse = tf.reduce_mean(tf.square(y_true - y_pred), axis=1)\n    return tf.reduce_mean(tf.sqrt(colwise_mse), axis=1)\ndef lstm_layer(hidden_dim, dropout):\n    return tf.keras.layers.Bidirectional(\n                                tf.keras.layers.LSTM(hidden_dim,\n                                dropout=dropout,\n                                return_sequences=True,\n                                kernel_initializer = 'orthogonal'))\n\ndef preprocess_inputs(df, token2int, cols=['sequence', 'structure', 'predicted_loop_type']):\n    return pandas_list_to_array(\n        df[cols].applymap(lambda seq: [token2int[x] for x in seq])\n    )\n\ndef pandas_list_to_array(df):\n    \"\"\"\n    Input: dataframe of shape (x, y), containing list of length l\n    Return: np.array of shape (x, l, y)\n    \"\"\"\n    \n    return np.transpose(\n        np.array(df.values.tolist()),\n        (0, 2, 1)\n    )","4f6c05e5":"data_dir = '\/kaggle\/input\/stanford-covid-vaccine\/'\npred_cols = ['reactivity', 'deg_Mg_pH10', 'deg_Mg_50C', 'deg_pH10', 'deg_50C']\n\ny_true = tf.random.normal((32, 68, 3))\ny_pred = tf.random.normal((32, 68, 3))\n\n\ntrain = pd.read_json(data_dir + 'train.json', lines=True)\ntest = pd.read_json(data_dir + 'test.json', lines=True)\nsample_df = pd.read_csv(data_dir + 'sample_submission.csv')\n\ntrain = train.query(\"signal_to_noise >= 1\")\n\ntoken2int = {x:i for i, x in enumerate('().ACGUBEHIMSX')}\n\ntrain_inputs = preprocess_inputs(train, token2int)\ntrain_labels = pandas_list_to_array(train[pred_cols])\n\nx_train, x_val, y_train, y_val = train_test_split(\n    train_inputs, train_labels, test_size=.1, random_state=34, stratify=train.SN_filter)\n\npublic_df = test.query(\"seq_length == 107\")\nprivate_df = test.query(\"seq_length == 130\")\n\npublic_inputs = preprocess_inputs(public_df, token2int)\nprivate_inputs = preprocess_inputs(private_df, token2int)","90b47dfa":"def build_model_structure(embed_size=14, seq_len=107, pred_len=68, dropout=0.5, \n                sp_dropout=0.2, embed_dim=200, hidden_dim=256, n_layers=3):\n    inputs = L.Input(shape=(seq_len, 3))\n    embed = L.Embedding(input_dim=embed_size, output_dim=embed_dim)(inputs)\n    \n    reshaped = tf.reshape(\n        embed, shape=(-1, embed.shape[1],  embed.shape[2] * embed.shape[3])\n    )\n    hidden = L.SpatialDropout1D(sp_dropout)(reshaped)\n    \n    for x in range(n_layers):\n        hidden = gru_layer(hidden_dim, dropout)(hidden)\n    \n    # Since we are only making predictions on the first part of each sequence, \n    # we have to truncate it\n    truncated = hidden[:, :pred_len]\n    out = L.Dense(5, activation='linear')(truncated)\n    \n    model = tf.keras.Model(inputs=inputs, outputs=out)\n    model.compile(tf.optimizers.Adam(), loss=MCRMSE)\n    \n    return model","38e77e9d":"keras.backend.clear_session()\nweight_file='\/kaggle\/input\/learned-model\/model.h5'\n#public model\npre_trained_public_model = build_model_structure(seq_len=107, pred_len=107)\npre_trained_public_model.load_weights(weight_file)\n\n\n#private model\npre_trained_private_model = build_model_structure(seq_len=130, pred_len=130)\npre_trained_private_model.load_weights(weight_file)\n# Make all the layers in the pre-trained model non-trainable\nfor layer in pre_trained_public_model.layers:\n    layer.trainable=False\n\n# Make all the layers in the pre-trained model non-trainable\nfor layer in pre_trained_private_model.layers:\n    layer.trainable=False\n       \n# Get the summary\npre_trained_public_model.summary()","1f7362ca":"# Select the last layer\npred_len=68\nlast_layer = pre_trained_public_model.get_layer('dense')\nlast_output = last_layer.output\n\n\n# Add a fully connected layer with 1,024 hidden units and ReLU activation\nnew_layer = lstm_layer(200, dropout=.4)(last_output)\n#x = layers.Dense(1024, activation='relu')(x)\n\n# Add a dropout rate of 0.2\nnew_layer = layers.Dropout(.2)(new_layer)    \n\ntruncated = new_layer[:, :pred_len]\n\nout = tf.keras.layers.Dense(5, activation='linear')(truncated)\ntransferred_model = tf.keras.Model(inputs=pre_trained_public_model.input,outputs=out)\ntransferred_model.compile(tf.optimizers.Adam(), loss=MCRMSE)","b76240fe":"pre_trained_private_model.summary()","730624f1":"# Select the last layer\npred_len=107\nlast_layer = pre_trained_private_model.get_layer('dense_1')\nlast_output = last_layer.output\n\n\n# Add a fully connected layer with 1,024 hidden units and ReLU activation\nnew_layer = lstm_layer(200, dropout=.4)(last_output)\n#x = layers.Dense(1024, activation='relu')(x)\n\n# Add a dropout rate of 0.2\nnew_layer = layers.Dropout(.2)(new_layer)    \n\ntruncated = new_layer[:, :pred_len]\n\nout = tf.keras.layers.Dense(5, activation='linear')(truncated)\ntransferred_pr_model = tf.keras.Model(inputs=pre_trained_private_model.input,outputs=out)\ntransferred_pr_model.compile(tf.optimizers.Adam(), loss=MCRMSE)","ecf42eda":"history = transferred_model.fit(\n    x_train, y_train,\n    validation_data=(x_val, y_val),\n    batch_size=64,\n    epochs=40,\n    verbose=2,\n    callbacks=[\n        tf.keras.callbacks.ReduceLROnPlateau(patience=5),\n        tf.keras.callbacks.ModelCheckpoint('transferred_model.h5')\n    ]\n)","c18e97c9":"fig = px.line(\n    history.history, y=['loss', 'val_loss'],\n    labels={'index': 'epoch', 'value': 'MCRMSE'}, \n    title='Training History')\nfig.show()","3583cb2c":"transferred_model_public = transferred_model\ntransferred_model_private = transferred_pr_model\n\ntransferred_model_public.load_weights('transferred_model.h5')\ntransferred_model_private.load_weights('transferred_model.h5')\n\npublic_preds = transferred_model_public.predict(public_inputs)\nprivate_preds = transferred_model_private.predict(private_inputs)\n\npreds_ls = []\n\nfor df, preds in [(public_df, public_preds), (private_df, private_preds)]:\n    for i, uid in enumerate(df.id):\n        single_pred = preds[i]\n\n        single_df = pd.DataFrame(single_pred, columns=pred_cols)\n        single_df['id_seqpos'] = [f'{uid}_{x}' for x in range(single_df.shape[0])]\n\n        preds_ls.append(single_df)\n\npreds_df = pd.concat(preds_ls)\npreds_df.head()","19979bca":"preds_df.to_csv('preds_df.csv', index=False)","acd9e46a":"**In this notebook, I will be discussing transfer learning. Utilizing already created model and updating it to add an additional layer and running it. This saves time and help us to further configure the model to attain optimization. I have used the solid model developered by @xhlulu @vbmokin. Referenced Notebooks:**\n\nhttps:\/\/www.kaggle.com\/xhlulu\/openvaccine-simple-gru-model <br>\nhttps:\/\/www.kaggle.com\/its7171\/gru-lstm-with-feature-engineering-and-augmentation\n\n**I have borrowed few code fxns from above notebooks. Credit goes to them, please upvote their notebooks as a token of appreciation and their quality work.****","021cad77":"# **Making Predictions**","2354192b":"# **Method to declare model structure - used in transfer learning**","3d6d3fab":"**Public model structure -  model.h5 can  be downloaded from [model.h5](http:\/\/www.kaggle.com\/gagankarora\/learned-model)**","d50f2b0d":"**Utility Functions**","8bf9d556":"# **Model Evaluation**","bfda992c":"# **Model Fitting**","68745c9d":"# **Update to public Model**","65c549eb":"**Please note Last layer is dense_1**","7f5dcd1f":"# **Create public and private model structure**","adfc2e81":"# **Version 1: Problem Statement**\n# **Version 3: Transfer Learning**\n# **Upcoming Version: Transferred model optimization**\n\nHere I will discuss how to attack such problems when you have 0 domain knowledge or literature is too complex to digest :). I have 0 understanding of RNA and its theory. I will try to lay down the thoughts here to make some sense out of the problem. *  \n\n***Introdution to DNA\/RNA:*** I am newbie to the biology terminology so would write down bare minimum definition so to get the understanding of the problem we are trying to solve:\n\n***DNA v\/s RNA:*** DNA encodes all genetic information and is a blueprint from which biological life is creaed. In a raw manner, it can be considered as biological flash drive. RNA on the other hand is reader which decodes the information from the biological flash drive [DNA] and utilized in the process of creating protein\n\n![DNA v\/s RNA](https:\/\/scx1.b-cdn.net\/csz\/news\/800\/2020\/11-newtechnolog.jpg)\n\nThere are three 3 types of RNA: **mRNA**, tRNA and rRNA. In this Data science problem, we are dealing with **mRNA** which is messenger RNA used for copying portion of genetic code and process is called transcription.\n\n**mRNA Sequencing**:method of analyzing the transcriptomes [sum total of mRNA] of disease or biological states.\n\n\n**Problem Statement:**To predict the degrade rate of various location along RNA sequence\n\n***Fields Used in training set:***\n\n1.   index\n2.   id\n3.   sequence [String of **max** size  107]\n4.   structure [String of **max** size 107 ]\n5.   predicted_loop_type [107 in both train\/test]\n6.   signal_to_noise\n7.   SN_filter\n8.   seq_length \n9.   seq_scored [**max**   68]\n10.  reactivity_error [68 items]\n11.  deg_error_Mg_pH10 [68 items]\n12.  deg_error_pH10 [68 items]\n13.  deg_error_Mg_50C [68 items]\n14.  deg_error_50C [68 items]\n15.  reactivity [68 items]\n16.  deg_Mg_pH10 [68 items]\n17.  deg_mH10 [68 items]\n18.  deg_Mg_50C [68 items]\n19.  deg_50C [68 items]\n\n***Fields Used in test set:***\n\n1.   index\n2.   id\n3.   sequence  [String of **max** size  130]\n4.   structure [String of **max** size  130]\n5.   predicted_loop_type [107 in both train\/test]\n6.   seq_length \n7.   seq_scored \n\nData Stat:\n\nAs per description, Stanford scientists have data on 3029 RNA out of which 2400 are used for training and 629 for public test\n\n**Training set**=2400 <br>\n**public set**=629+3005[added new RNAs]=3634\n\nSo just reading the description of the data columns we can derive independent variable\/Features**** X columns and dependent variable  Y columns:\n\n**X:** sequence [string], structure [string], predicticted_loop_type[string] \n\n**Y:** 5 columns: reactivity,\tdeg_Mg_pH10,\tdeg_pH10,\tdeg_Mg_50C\tdeg_50C\n\nLooking at the X - it's a classic problem of NLP.\n\nIn public test, each of 5 column is vector of length 68 whereas for private test each of 5 colums is a vector of length 91.\n\nIn coming days, will be adding more code in the notebook for the actual analysis. \n","4f82e9e3":"# **Update the private model**","acae81f3":"# **Data Loading**"}}