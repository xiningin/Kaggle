{"cell_type":{"f8e76f9f":"code","01d8bc93":"code","9ec4db7d":"code","4f1bd166":"code","2c392092":"code","09b237f0":"code","da8119fe":"code","1fc6c5a0":"code","ccc9c2e2":"code","4df08ddb":"code","b51ca960":"code","e2d96c8f":"code","75f25c87":"code","2e489387":"code","2a42fc21":"code","8da52866":"code","af6622c7":"code","8db0ead2":"code","55550c72":"code","14829e2b":"code","fc85a566":"code","0d9b2402":"code","1668822b":"code","5d3bc05b":"code","bbbf0a73":"code","354a903d":"code","108e30b2":"code","31c39793":"code","4e67d463":"code","029c1d79":"code","28288e10":"code","2d7ae233":"code","627de84f":"markdown","307c02eb":"markdown"},"source":{"f8e76f9f":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport lightgbm as lgb\nfrom sklearn.metrics import mean_squared_error\nfrom tqdm import tqdm\nimport gc","01d8bc93":"def reduce_mem_usage(df, verbose=True):\n    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    start_mem = df.memory_usage().sum() \/ 1024**2    \n    for col in df.columns:\n        col_type = df[col].dtypes\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)    \n    end_mem = df.memory_usage().sum() \/ 1024**2\n    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) \/ start_mem))\n    return df\n#\ndef autocorrelation(ys, t=1):\n    return np.corrcoef(ys[:-t], ys[t:])","9ec4db7d":"#==========================================================================\ndef preprocess_sales(sales, start=1400, upper=1970):\n    if start is not None:\n        print(\"dropping...\")\n        to_drop = [f\"d_{i+1}\" for i in range(start-1)]\n        print(sales.shape)\n        sales.drop(to_drop, axis=1, inplace=True)\n        print(sales.shape)\n    #=======\n    print(\"adding...\")\n    new_columns = ['d_%i'%i for i in range(1942, upper, 1)]\n    for col in new_columns:\n        sales[col] = np.nan\n    print(\"melting...\")\n    sales = sales.melt(id_vars=[\"id\", \"item_id\", \"dept_id\", \"cat_id\", \"store_id\", \"state_id\",\"scale\",\"start\"],\n                        var_name='d', value_name='demand')\n\n    print(\"generating order\")\n    if start is not None:\n        skip = start\n    else:\n        skip = 1\n    sales[\"nb\"] =sales.index \/\/ 42840 + skip\n    return sales\n#===============================================================\ndef preprocess_calendar(calendar):\n    global maps, mods\n    calendar[\"event_name\"] = calendar[\"event_name_1\"]\n    calendar[\"event_type\"] = calendar[\"event_type_1\"]\n\n    map1 = {mod:i for i,mod in enumerate(calendar['event_name'].unique())}\n    calendar['event_name'] = calendar['event_name'].map(map1)\n    map2 = {mod:i for i,mod in enumerate(calendar['event_type'].unique())}\n    calendar['event_type'] = calendar['event_type'].map(map2)\n    calendar['nday'] = calendar['date'].str[-2:].astype(int)\n    maps[\"event_name\"] = map1\n    maps[\"event_type\"] = map2\n    mods[\"event_name\"] = len(map1)\n    mods[\"event_type\"] = len(map2)\n    calendar[\"wday\"] -=1\n    calendar[\"month\"] -=1\n    calendar[\"year\"] -= 2011\n    mods[\"month\"] = 12\n    mods[\"year\"] = 6\n    mods[\"wday\"] = 7\n    mods['snap_CA'] = 2\n    mods['snap_TX'] = 2\n    mods['snap_WI'] = 2\n\n    calendar.drop([\"event_name_1\", \"event_name_2\", \"event_type_1\", \"event_type_2\", \"date\", \"weekday\"], \n                  axis=1, inplace=True)\n    return calendar\n#=========================================================\ndef make_dataset(categorize=False ,start=1400, upper= 1970):\n    global maps, mods\n    print(\"loading calendar...\")\n    calendar = pd.read_csv(\"..\/input\/m5-forecasting-uncertainty\/calendar.csv\")\n    print(\"loading sales...\")\n    sales = pd.read_csv(\"..\/input\/walmartadd\/sales.csv\")\n    cols = [\"item_id\", \"dept_id\", \"cat_id\",\"store_id\",\"state_id\"]\n    if categorize:\n        for col in cols:\n            temp_dct = {mod:i for i, mod in enumerate(sales[col].unique())}\n            mods[col] = len(temp_dct)\n            maps[col] = temp_dct\n        for col in cols:\n            sales[col] = sales[col].map(maps[col])\n        #\n\n    sales =preprocess_sales(sales, start=start, upper= upper)\n    calendar = preprocess_calendar(calendar)\n    calendar = reduce_mem_usage(calendar)\n    print(\"merge with calendar...\")\n    sales = sales.merge(calendar, on='d', how='left')\n    del calendar\n\n    print(\"reordering...\")\n    sales.sort_values(by=[\"id\",\"nb\"], inplace=True)\n    print(\"re-indexing..\")\n    sales.reset_index(inplace=True, drop=True)\n    gc.collect()\n\n    sales['n_week'] = (sales['nb']-1)\/\/7\n    sales[\"nday\"] -= 1\n    mods['nday'] = 31\n    sales = reduce_mem_usage(sales)\n    gc.collect()\n    return sales\n#===============================================================================#","4f1bd166":"%%time\nCATEGORIZE = True;\nSTART = 1400; UPPER = 1970;\nmaps = {}\nmods = {}\nsales = make_dataset(categorize=CATEGORIZE ,start=START, upper= UPPER)","2c392092":"sales[\"x\"] = sales[\"demand\"] \/ sales[\"scale\"]","09b237f0":"LAGS = [28, 35, 42, 49, 56, 63]\nFEATS = []\nfor lag in tqdm(LAGS):\n    sales[f\"x_{lag}\"] = sales.groupby(\"id\")[\"x\"].shift(lag)\n    FEATS.append(f\"x_{lag}\")\n#","da8119fe":"#sales.loc[(sales.start>1844)&(sales.nb>1840)&(sales.nb<1850), ['id','start','nb','demand']]\n#sales.start.max() #1845","1fc6c5a0":"print(sales.shape)\nsales = sales.loc[sales.nb>sales.start]\nprint(sales.shape)","ccc9c2e2":"nb = sales['nb'].values\nMAX_LAG = max(LAGS)\n#tr_mask = np.logical_and(nb>START + MAX_LAG, nb<=1913)\ntr_mask = np.logical_and(nb>START + MAX_LAG, nb<=1941) # SORRY THIS IS FAKE VALIDATION. I DIDN'T THINK IT WOULD HAVE HAD LIFTED UP MY SCORE LIKE THAT\nval_mask = np.logical_and(nb>1913, nb<=1941)\nte_mask = np.logical_and(nb>1941, nb<=1969)","4df08ddb":"scale = sales['scale'].values\nids = sales['id'].values\n#y = sales['demand'].values\n#ys = y \/ scale\nys = sales['x'].values\nZ = sales[FEATS].values","b51ca960":"sv = scale[val_mask]\nse = scale[te_mask]\nids = ids[te_mask]\nids = ids.reshape((-1, 28))","e2d96c8f":"ca = sales[['snap_CA']].values\ntx = sales[['snap_TX']].values\nwi = sales[['snap_WI']].values\nwday = sales[['wday']].values\nmonth = sales[['month']].values\nyear = sales[['year']].values\nevent = sales[['event_name']].values\nnday = sales[['nday']].values","75f25c87":"item = sales[['item_id']].values\ndept = sales[['dept_id']].values\ncat = sales[['cat_id']].values\nstore = sales[['store_id']].values\nstate = sales[['state_id']].values","2e489387":"def make_data(mask):\n    x = {\"snap_CA\":ca[mask], \"snap_TX\":tx[mask], \"snap_WI\":wi[mask], \"wday\":wday[mask], \n         \"month\":month[mask], \"year\":year[mask], \"event\":event[mask], \"nday\":nday[mask], \n         \"item\":item[mask], \"dept\":dept[mask], \"cat\":cat[mask], \"store\":store[mask], \n         \"state\":state[mask], \"num\":Z[mask]}\n    t = ys[mask]\n    return x, t","2a42fc21":"xt, yt = make_data(tr_mask) #train\nxv, yv = make_data(val_mask) # val\nxe, ye = make_data(te_mask) # test","8da52866":"import tensorflow.keras.layers as L\nimport tensorflow.keras.models as M\nimport tensorflow.keras.backend as K\nfrom tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, EarlyStopping\nimport tensorflow as tf","af6622c7":"\n#=====\ndef qloss(y_true, y_pred):\n    # Pinball loss for multiple quantiles\n    qs = [0.005, 0.025, 0.165, 0.250, 0.500, 0.750, 0.835, 0.975, 0.995]\n    q = tf.constant(np.array([qs]), dtype=tf.float32)\n    e = y_true - y_pred\n    v = tf.maximum(q*e, (q-1)*e)\n    return K.mean(v)\n#============================#\ndef make_model(n_in):\n    \n    num = L.Input((n_in,), name=\"num\")\n    \n    ca = L.Input((1,), name=\"snap_CA\")\n    tx = L.Input((1,), name=\"snap_TX\")\n    wi = L.Input((1,), name=\"snap_WI\")\n    wday = L.Input((1,), name=\"wday\")\n    month = L.Input((1,), name=\"month\")\n    year = L.Input((1,), name=\"year\")\n    event = L.Input((1,), name=\"event\")\n    nday = L.Input((1,), name=\"nday\")\n    item = L.Input((1,), name=\"item\")\n    dept = L.Input((1,), name=\"dept\")\n    cat = L.Input((1,), name=\"cat\")\n    store = L.Input((1,), name=\"store\")\n    state = L.Input((1,), name=\"state\")\n    inp = {\"snap_CA\":ca, \"snap_TX\":tx, \"snap_WI\":wi, \"wday\":wday, \n           \"month\":month, \"year\":year, \"event\":event, \"nday\":nday,\n           \"item\":item, \"dept\":dept, \"cat\":cat, \"store\":store, \n           \"state\":state, \"num\":num} \n    #\n    ca_ = L.Embedding(mods[\"snap_CA\"], mods[\"snap_CA\"], name=\"ca_3d\")(ca)\n    tx_ = L.Embedding(mods[\"snap_TX\"], mods[\"snap_TX\"], name=\"tx_3d\")(tx)\n    wi_ = L.Embedding(mods[\"snap_WI\"], mods[\"snap_WI\"], name=\"wi_3d\")(wi)\n    wday_ = L.Embedding(mods[\"wday\"], mods[\"wday\"], name=\"wday_3d\")(wday)\n    month_ = L.Embedding(mods[\"month\"], mods[\"month\"], name=\"month_3d\")(month)\n    year_ = L.Embedding(mods[\"year\"], mods[\"year\"], name=\"year_3d\")(year)\n    event_ = L.Embedding(mods[\"event_name\"], mods[\"event_name\"], name=\"event_3d\")(event)\n    nday_ = L.Embedding(mods[\"nday\"], mods[\"nday\"], name=\"nday_3d\")(nday)\n    item_ = L.Embedding(mods[\"item_id\"], 10, name=\"item_3d\")(item)\n    dept_ = L.Embedding(mods[\"dept_id\"], mods[\"dept_id\"], name=\"dept_3d\")(dept)\n    cat_ = L.Embedding(mods[\"cat_id\"], mods[\"cat_id\"], name=\"cat_3d\")(cat)\n    store_ = L.Embedding(mods[\"store_id\"], mods[\"store_id\"], name=\"store_3d\")(store)\n    state_ = L.Embedding(mods[\"state_id\"], mods[\"state_id\"], name=\"state_3d\")(state)\n    \n    p = [ca_, tx_, wi_, wday_, month_, year_, event_, nday_, item_, dept_, cat_, store_, state_]\n    emb = L.Concatenate(name=\"embds\")(p)\n    context = L.Flatten(name=\"context\")(emb)\n    \n    x = L.Concatenate(name=\"x1\")([context, num])\n    x = L.Dense(500, activation=\"relu\", name=\"d1\")(x)\n    x = L.Dropout(0.3)(x)\n    x = L.Concatenate(name=\"m1\")([x, context])\n    x = L.Dense(500, activation=\"relu\", name=\"d2\")(x)\n    x = L.Dropout(0.3)(x)\n    x = L.Concatenate(name=\"m2\")([x, context])\n    x = L.Dense(500, activation=\"relu\", name=\"d3\")(x)\n    preds = L.Dense(9, activation=\"linear\", name=\"preds\")(x)\n    model = M.Model(inp, preds, name=\"M1\")\n    model.compile(loss=qloss, optimizer=\"adam\")\n    return model","8db0ead2":"net = make_model(len(FEATS))\nckpt = ModelCheckpoint(\"w.h5\", monitor='val_loss', verbose=1, save_best_only=True,mode='min')\nreduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2,\n                              patience=5, min_lr=0.001)\nes = EarlyStopping(monitor='val_loss', patience=3)\nprint(net.summary())","55550c72":"net.fit(xt, yt, batch_size=50_000, epochs=20, validation_data=(xv, yv), callbacks=[ckpt, reduce_lr, es])","14829e2b":"nett = make_model(len(FEATS))\nnett.load_weights(\"w.h5\")","fc85a566":"pv = nett.predict(xv, batch_size=50_000, verbose=1)\npe = nett.predict(xe, batch_size=50_000, verbose=1)","0d9b2402":"nett.evaluate(xv, yv, batch_size=50_000)","1668822b":"pv = pv.reshape((-1, 28, 9))\npe = pe.reshape((-1, 28, 9))","5d3bc05b":"sv = sv.reshape((-1, 28))\nse = se.reshape((-1, 28))","bbbf0a73":"Yv = yv.reshape((-1, 28))","354a903d":"k = np.random.randint(0, 42840)\n#k = np.random.randint(0, 200)\nprint(ids[k, 0])\nplt.plot(np.arange(28, 56), Yv[k], label=\"true\")\nplt.plot(np.arange(28, 56), pv[k ,:, 3], label=\"q25\")\nplt.plot(np.arange(28, 56), pv[k ,:, 4], label=\"q50\")\nplt.plot(np.arange(28, 56), pv[k, :, 5], label=\"q75\")\nplt.legend(loc=\"best\")\nplt.show()","108e30b2":"names = [f\"F{i+1}\" for i in range(28)]","31c39793":"piv = pd.DataFrame(ids[:, 0], columns=[\"id\"])","4e67d463":"QUANTILES = [\"0.005\", \"0.025\", \"0.165\", \"0.250\", \"0.500\", \"0.750\", \"0.835\", \"0.975\", \"0.995\"]\nVALID = []\nEVAL = []\n\nfor i, quantile in tqdm(enumerate(QUANTILES)):\n    t1 = pd.DataFrame(pv[:,:, i]*sv, columns=names)\n    t1 = piv.join(t1)\n    t1[\"id\"] = t1[\"id\"] + f\"_{quantile}_validation\"\n    t2 = pd.DataFrame(pe[:,:, i]*se, columns=names)\n    t2 = piv.join(t2)\n    t2[\"id\"] = t2[\"id\"] + f\"_{quantile}_evaluation\"\n    VALID.append(t1)\n    EVAL.append(t2)\n#============#","029c1d79":"sub = pd.DataFrame()\nsub = sub.append(VALID + EVAL)\ndel VALID, EVAL, t1, t2","28288e10":"sub.head()","2d7ae233":"sub.to_csv(\"submission.csv\", index=False)","627de84f":"### Prediction","307c02eb":"## It is a baseline model. Feel free to add your own FE magic !!!"}}