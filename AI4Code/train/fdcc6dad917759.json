{"cell_type":{"3f1cb863":"code","ddc3c96a":"markdown","bb85ac2c":"markdown"},"source":{"3f1cb863":"import numpy as np\nimport matplotlib.pyplot as plt\n\nplt.rcParams['figure.figsize'] = (10, 8)\n\n# intial parameters\nn_iter = 50\nsz = (n_iter,) # size of array\nx = -0.37727 # truth value (typo in example at top of p. 13 calls this z)\nz = np.random.normal(x,0.1,size=sz) # observations (normal about x, sigma=0.1)\n\nQ = 1e-5 # process variance\n\n# allocate space for arrays\nxhat=np.zeros(sz)      # a posteri estimate of x\nP=np.zeros(sz)         # a posteri error estimate\nxhatminus=np.zeros(sz) # a priori estimate of x\nPminus=np.zeros(sz)    # a priori error estimate\nK=np.zeros(sz)         # gain or blending factor\n\nR = 0.1**2 # estimate of measurement variance, change to see effect\n\n# intial guesses\nxhat[0] = 0.0\nP[0] = 1.0\n\nfor k in range(1,n_iter):\n    # time update\n    xhatminus[k] = xhat[k-1]\n    Pminus[k] = P[k-1]+Q\n\n    # measurement update\n    K[k] = Pminus[k]\/( Pminus[k]+R )\n    xhat[k] = xhatminus[k]+K[k]*(z[k]-xhatminus[k])\n    P[k] = (1-K[k])*Pminus[k]\n\nplt.figure()\nplt.plot(z,'k+',label='noisy measurements')\nplt.plot(xhat,'b-',label='a posteri estimate')\nplt.axhline(x,color='g',label='truth value')\nplt.legend()\nplt.title('Estimate vs. iteration step', fontweight='bold')\nplt.xlabel('Iteration')\nplt.ylabel('Voltage')\n\nplt.figure()\nvalid_iter = range(1,n_iter) # Pminus not valid at step 0\nplt.plot(valid_iter,Pminus[valid_iter],label='a priori error estimate')\nplt.title('Estimated $\\it{\\mathbf{a \\ priori}}$ error vs. iteration step', fontweight='bold')\nplt.xlabel('Iteration')\nplt.ylabel('$(Voltage)^2$')\nplt.setp(plt.gca(),'ylim',[0,.01])\nplt.show()","ddc3c96a":"# Kalman filters\n\n## Background\n\nKalman filters are an advanced technique that provides an efficient way of computing the current state of a statistical process in a way that minimizes mean squared error. They are an example of a **filter**: a statistical technique for reducing the noise in a sequence of pointwise (or continuous; but pointwise in Kalman filters and in all practical applications) observations, generating a value which is close to the true value for some underlying statistical system. For a bit more on filters in general see [this notebook](https:\/\/www.kaggle.com\/residentmario\/denoising-algorithms\/).\n\nKalman filters are one of the more advanced filter algorithms available, but they're also one of the most widespread. They played an important role in the computers used for the Apollo moon landing, and have found continued application in things like cleaning signals on instruments from self-driving cars and the like.\n\nThis notebook contains my notes from [\"An Introduction to the Kalman Filter\"](https:\/\/www.cs.unc.edu\/~welch\/media\/pdf\/kalman_intro.pdf) and the corresponding `scipy` [cookbook example](https:\/\/scipy-cookbook.readthedocs.io\/items\/KalmanFiltering.html).\n\n## Definition\n\nMathematically speaking Kalman filters assume a discrete stochastic process of the form:\n\n$$x_k = Ax_{k \u2013 1} + Bu_{k \u2013 1} + w_{k \u2013 1}$$\n\nWhich is being measured by a $z_k$ such that:\n\n$$z_k = H x_k + v_k$$\n\n$w_k$ and $v_k$ are assumed to be the process noise and the measurement noise respectively, and are assumed to be normally distributed which mean zero and variance $Q$ and $R$, respectively:\n\n$$p(w) \\sim N(0, Q)$$\n$$p(v) \\sim N(0, R)$$\n\nKalman filters differenciate between an a priori estimate $\\hat{x}^-$, and an a posteriori estimate, $\\hat{x}^k$. The a priori estimate is the best guess the filter makes for a value before it is observed, whilst the the a posteriori estimate is its best guess with the additional information of $z_k$.\n\nThese values have error terms and covariance terms:\n\n$$e_k^- = x_k - \\hat{x}_k^-$$\n$$e_k = x_k - \\hat{x}_k$$\n$$P_k^- = E[e_k^- e_k^{-T}]$$\n$$P_k = E[e_k e_k^T]$$\n\nWith all of these dynamics in place, here is the equation the Kalman filter uses to actually model the true value $\\hat{x}_k$:\n\n$$\\hat{x}_k = \\hat{x}_k^- + K(z_k - H\\hat{x}_k^-)$$\n\nThe $z_k - H\\hat{x}_k^-$ term is the **residual**: the difference between the observed value, $z_k$, and the value that the Kalman filter predicted in the absence of the observation, $H\\hat{x}_k^-$. The term $K$ is an $n\\times m$ matrix which is known as the **gain**. K is chosen such that it minimizes $P_k$&mdash;the a posteriori error covariance. K can be solved for analytically by performing a few equation substitutions, then setting $K=0$ and solving for its trace equalling 0 (e.g. finding a minimal point). This has a closed-form solution:\n\n$$K_k = \\frac{P_k^- H^T}{HP_K^- H^T + R}$$\n\n\n## Discussion\n\nThis solution has the following revealing properties:\n\n$$\\lim_{R\\to 0} K_k = \\frac{1}{H} = H^{-1}$$\n$$\\lim_{P_k^- \\to 0} K_k = 0 \\implies \\hat{x}_k = \\hat{x}_k^-$$\n\nIn other words, as the measurement error covariance approaches 0, the residual is weighed less and less, and the measurement value $z_k$ is weighed more and more. Going the other way, the predictor $H \\hat{x}_k^-$ is more trusted, and $z_k$ is less trusted.\n\nWhat this equation is essentially doing, then, is balancing how much trust it places in the observed (measured) value, versus how much trust it places in the modeled value! This is a big part of why Kalman filters have proven so useful. They incorporate a limited form of model accuracy feedback into their design, allowing them to adjust the relative merit of prediction versus observation in an online way.\n\nFrom a computational perspective Kalman filters are advantageous because they are recursive, and do not require recomputing all past information in order to generate an update. This is an advantage over many other filters, which are more computationally intensive. They remind me of recurrent neural networks in this regard, though Kalman filters are obviously orders of magnitude less complex computationally.\n\nNote that the implementation proceeds in two steps. First a time update is computed, resulting in a predicted value $z_k$. Next, a measurement update is computed, which corrects the $z_k$ value using the modeled value. The full runthrough is as follows:\n\n![](https:\/\/i.imgur.com\/SywCSf4.png)\n\nThe values $R$ and $Q$ are important to the model, and must be set prior to run. $R$ can generally be set by observing the phenomenon in an offline manner for a period prior to the introduction of the Kalman filter. $Q$ is a hyperparameter which must be tuned, as it cannot be observed directly. Interestingly enough the paper suggests using *another Kalman filter* to tune $Q$ in an online manner. In a neural learning context that's basically pretraining!\n\nThe Kalman filter presented here is a linear one. Kalman filters may be extended more loosely to non-linear systems using a functional model for $x_k$: $x_k = f(x_{k-1}, u_{k-1}, w_{k-1}$. This form of the Kalman filter is much more ad hoc than the original linear Kalman filter because it doesn't follow probabilistic assumptions as closely (not that those matter). But the Kalman filter can perform reasonably well even when it is reasonably far from correct assumptions, as long the measurement error is relatively low, because it relies on the same sort-of-works-anyway Bayesian principles that power the [naive Bayes classifier](https:\/\/www.kaggle.com\/residentmario\/primer-on-naive-bayes-algorithms\/). For more on that including the derivation read [the paper](https:\/\/www.cs.unc.edu\/~welch\/media\/pdf\/kalman_intro.pdf).","bb85ac2c":"## Demo\n\nHere is the demo, taken directly from the [`scipy` cookbook demo](https:\/\/scipy-cookbook.readthedocs.io\/items\/KalmanFiltering.html):"}}